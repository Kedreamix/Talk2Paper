<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="I2I Translation">
    <meta name="description" content="I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-03-14  Electromyography-Informed Facial Expression Reconstruction for   Physiological-Based Synthesis and Analysis">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>I2I Translation | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-e9c42602c21b81f4a9b9c1e75bf0491a.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">I2I Translation</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/I2I-Translation/">
                                <span class="chip bg-color">I2I Translation</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                I2I Translation
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-03-14
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    11k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    44 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-03-14-更新"><a href="#2025-03-14-更新" class="headerlink" title="2025-03-14 更新"></a>2025-03-14 更新</h1><h2 id="Electromyography-Informed-Facial-Expression-Reconstruction-for-Physiological-Based-Synthesis-and-Analysis"><a href="#Electromyography-Informed-Facial-Expression-Reconstruction-for-Physiological-Based-Synthesis-and-Analysis" class="headerlink" title="Electromyography-Informed Facial Expression Reconstruction for   Physiological-Based Synthesis and Analysis"></a>Electromyography-Informed Facial Expression Reconstruction for   Physiological-Based Synthesis and Analysis</h2><p><strong>Authors:Tim Büchner, Christoph Anders, Orlando Guntinas-Lichius, Joachim Denzler</strong></p>
<p>The relationship between muscle activity and resulting facial expressions is crucial for various fields, including psychology, medicine, and entertainment. The synchronous recording of facial mimicry and muscular activity via surface electromyography (sEMG) provides a unique window into these complex dynamics. Unfortunately, existing methods for facial analysis cannot handle electrode occlusion, rendering them ineffective. Even with occlusion-free reference images of the same person, variations in expression intensity and execution are unmatchable. Our electromyography-informed facial expression reconstruction (EIFER) approach is a novel method to restore faces under sEMG occlusion faithfully in an adversarial manner. We decouple facial geometry and visual appearance (e.g., skin texture, lighting, electrodes) by combining a 3D Morphable Model (3DMM) with neural unpaired image-to-image translation via reference recordings. Then, EIFER learns a bidirectional mapping between 3DMM expression parameters and muscle activity, establishing correspondence between the two domains. We validate the effectiveness of our approach through experiments on a dataset of synchronized sEMG recordings and facial mimicry, demonstrating faithful geometry and appearance reconstruction. Further, we synthesize expressions based on muscle activity and how observed expressions can predict dynamic muscle activity. Consequently, EIFER introduces a new paradigm for facial electromyography, which could be extended to other forms of multi-modal face recordings. </p>
<blockquote>
<p>肌肉活动与产生的面部表情之间的关系对心理学、医学和娱乐等多个领域都至关重要。通过表面肌电图（sEMG）同步记录面部表情和肌肉活动，为这些复杂动态提供了一个独特的窗口。然而，现有的面部分析方法无法处理电极遮挡问题，导致它们无效。即使使用同一人无遮挡的参考图像，表情强度和执行的差异也无法匹配。我们的电生理学启发下的面部表情重建（EIFER）方法是一种采用对抗方式在sEMG遮挡下忠实还原面部的新方法。我们通过结合三维可变形模型（3DMM）和通过参考记录进行神经无配对图像到图像的翻译，来解耦面部几何形状和视觉外观（如皮肤纹理、光照、电极等）。然后，EIFER学习3DMM表情参数与肌肉活动之间的双向映射，在两个领域之间建立对应关系。我们通过同步sEMG记录和面部表情的数据集进行实验，验证了我们的方法的有效性，展示了忠实的几何和外观重建。此外，我们根据肌肉活动合成表情，并探讨观察到的表情如何预测动态肌肉活动。因此，EIFER为面部肌电图引入了新的范式，可以扩展到其他形式的多模式面部记录。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09556v1">PDF</a> Accepted at CVPR 2025, 41 pages, 37 figures, 8 tables</p>
<p><strong>Summary</strong></p>
<p>本文介绍了肌肉活动与面部表情之间的关键关系，并指出其在心理学、医学和娱乐等领域的重要性。文章提出了一种新型的电肌图引导面部表情重建（EIFER）方法，能够在表面肌电图（sEMG）遮挡的情况下恢复面部图像。通过结合三维可变形模型（3DMM）和神经非配对图像到图像的翻译，该方法能够解耦面部几何形状和视觉外观，并通过双向映射学习肌肉活动与面部表情参数之间的对应关系。实验验证表明，该方法在同步sEMG记录和面部表情模仿数据集上具有良好的几何形状和外观重建效果。此外，该方法还能根据肌肉活动合成表情，并根据观察到的表情预测动态肌肉活动。因此，EIFER为面部电肌图研究带来了新的范式，可扩展到其他多模态面部记录形式。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>肌肉活动与面部表情的关系在心理学、医学和娱乐等领域具有重要意义。</li>
<li>现有面部分析方法无法处理电极遮挡问题，导致分析失效。</li>
<li>EIFER方法结合3DMM和神经非配对图像翻译技术，能够在sEMG遮挡的情况下恢复面部图像。</li>
<li>EIFER方法通过双向映射学习肌肉活动与面部表情参数之间的对应关系。</li>
<li>实验验证了EIFER方法在几何形状和外观重建上的有效性。</li>
<li>EIFER能合成基于肌肉活动的表情，并预测动态肌肉活动。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09556">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-5009481d1340ef62d7d77e3f26f54380.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dd2877b8348ca4e8c3ba1857a48617fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-44683405bf33b4a626f2dc9c4ab4543c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dcdf6c36d9844406edea987945752a6e.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="CM-Diff-A-Single-Generative-Network-for-Bidirectional-Cross-Modality-Translation-Diffusion-Model-Between-Infrared-and-Visible-Images"><a href="#CM-Diff-A-Single-Generative-Network-for-Bidirectional-Cross-Modality-Translation-Diffusion-Model-Between-Infrared-and-Visible-Images" class="headerlink" title="CM-Diff: A Single Generative Network for Bidirectional Cross-Modality   Translation Diffusion Model Between Infrared and Visible Images"></a>CM-Diff: A Single Generative Network for Bidirectional Cross-Modality   Translation Diffusion Model Between Infrared and Visible Images</h2><p><strong>Authors:Bin Hu, Chenqiang Gao, Shurui Liu, Junjie Guo, Fang Chen, Fangcen Liu</strong></p>
<p>The image translation method represents a crucial approach for mitigating information deficiencies in the infrared and visible modalities, while also facilitating the enhancement of modality-specific datasets. However, existing methods for infrared and visible image translation either achieve unidirectional modality translation or rely on cycle consistency for bidirectional modality translation, which may result in suboptimal performance. In this work, we present the cross-modality translation diffusion model (CM-Diff) for simultaneously modeling data distributions in both the infrared and visible modalities. We address this challenge by combining translation direction labels for guidance during training with cross-modality feature control. Specifically, we view the establishment of the mapping relationship between the two modalities as the process of learning data distributions and understanding modality differences, achieved through a novel Bidirectional Diffusion Training (BDT) strategy. Additionally, we propose a Statistical Constraint Inference (SCI) strategy to ensure the generated image closely adheres to the data distribution of the target modality. Experimental results demonstrate the superiority of our CM-Diff over state-of-the-art methods, highlighting its potential for generating dual-modality datasets. </p>
<blockquote>
<p>图像转换方法对于弥补红外和可见光模式中的信息缺失至关重要，同时也有助于增强特定模式的数据集。然而，现有的红外和可见图像转换方法要么实现单向模式转换，要么依赖于循环一致性进行双向模式转换，这可能导致性能不佳。在这项工作中，我们提出了跨模态转换扩散模型（CM-Diff），以同时模拟红外和可见光模态的数据分布。我们通过结合训练过程中的翻译方向标签进行引导，以及跨模态特征控制来解决这一挑战。具体来说，我们将建立两种模式之间的映射关系视为学习过程数据分布和理解模式差异的过程，通过一种新的双向扩散训练（BDT）策略来实现。此外，我们提出了一种统计约束推理（SCI）策略，以确保生成的图像紧密符合目标模态的数据分布。实验结果证明了我们的CM-Diff相较于最先进的方法具有优越性，凸显了其生成双模态数据集的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09514v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>该研究提出了一种跨模态翻译扩散模型（CM-Diff），该模型能够同时对红外和可见模态的数据分布进行建模，解决了现有图像翻译方法在红外和可见光图像翻译方面存在的单向模态翻译或依赖于循环一致性进行双向模态翻译的问题。该研究通过结合翻译方向标签进行训练指导以及跨模态特征控制来解决这一挑战。通过采用双向扩散训练（BDT）策略，建立两种模态之间的映射关系，理解和利用模态差异。同时，提出统计约束推理（SCI）策略，确保生成的图像紧密符合目标模态的数据分布。实验结果表明，CM-Diff相较于现有先进方法具有优越性，具有生成双模态数据集的潜力。</p>
<p><strong>要点</strong></p>
<ol>
<li>跨模态翻译扩散模型（CM-Diff）解决了红外和可见模态信息缺失的问题，同时增强了模态特定数据集。</li>
<li>现有图像翻译方法存在单向或依赖于循环一致性的双向模态翻译的问题，而CM-Diff能同时进行双向翻译。</li>
<li>CM-Diff通过结合翻译方向标签进行训练指导以及跨模态特征控制来解决挑战。</li>
<li>采用双向扩散训练（BDT）策略建立两种模态的映射关系，理解和利用模态差异。</li>
<li>CM-Diff采用统计约束推理（SCI）策略，确保生成的图像符合目标模态的数据分布。</li>
<li>实验证明CM-Diff在生成双模态数据集方面表现优异，超过现有方法。</li>
<li>CM-Diff的潜在应用包括改善模态转换的性能、扩充数据集、增强跨模态通信等。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09514">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-99d504687f34b48899dcc9ca73832461.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21315c90871e6474222bf15aab7b3802.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Alias-Free-Latent-Diffusion-Models-Improving-Fractional-Shift-Equivariance-of-Diffusion-Latent-Space"><a href="#Alias-Free-Latent-Diffusion-Models-Improving-Fractional-Shift-Equivariance-of-Diffusion-Latent-Space" class="headerlink" title="Alias-Free Latent Diffusion Models:Improving Fractional Shift   Equivariance of Diffusion Latent Space"></a>Alias-Free Latent Diffusion Models:Improving Fractional Shift   Equivariance of Diffusion Latent Space</h2><p><strong>Authors:Yifan Zhou, Zeqi Xiao, Shuai Yang, Xingang Pan</strong></p>
<p>Latent Diffusion Models (LDMs) are known to have an unstable generation process, where even small perturbations or shifts in the input noise can lead to significantly different outputs. This hinders their applicability in applications requiring consistent results. In this work, we redesign LDMs to enhance consistency by making them shift-equivariant. While introducing anti-aliasing operations can partially improve shift-equivariance, significant aliasing and inconsistency persist due to the unique challenges in LDMs, including 1) aliasing amplification during VAE training and multiple U-Net inferences, and 2) self-attention modules that inherently lack shift-equivariance. To address these issues, we redesign the attention modules to be shift-equivariant and propose an equivariance loss that effectively suppresses the frequency bandwidth of the features in the continuous domain. The resulting alias-free LDM (AF-LDM) achieves strong shift-equivariance and is also robust to irregular warping. Extensive experiments demonstrate that AF-LDM produces significantly more consistent results than vanilla LDM across various applications, including video editing and image-to-image translation. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/SingleZombie/AFLDM">https://github.com/SingleZombie/AFLDM</a> </p>
<blockquote>
<p>潜在扩散模型（LDMs）的生成过程不稳定，即使输入噪声出现微小的扰动或变化，也可能导致输出结果显著不同。这阻碍了它们在需要一致结果的应用中的适用性。在这项工作中，我们通过使模型具有平移等变性来重新设计LDMs，以增强其一致性。虽然引入抗混叠操作可以部分提高平移等变性，但由于LDM中独特的挑战，仍然存在严重的混叠和不一致性，包括1）在VAE训练期间和多个U-Net推断期间的混叠放大，以及2）本质上缺乏平移等变性的自注意力模块。为了解决这些问题，我们重新设计了注意力模块以实现平移等变性，并提出了一种等变性损失，有效地抑制了连续域中特征频率带宽。由此产生的无混叠LDM（AF-LDM）实现了强大的平移等变性，并且对不规则扭曲也具有鲁棒性。大量实验表明，AF-LDM在各种应用中产生的结果比原始LDM更加一致，包括视频编辑和图像到图像的翻译。代码可在此处找到：<a target="_blank" rel="noopener" href="https://github.com/SingleZombie/AFLDM">https://github.com/SingleZombie/AFLDM</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09419v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了潜在扩散模型（LDMs）在生成过程中的不稳定性问题，并针对该问题提出了一种改进方法，即通过设计别名免费潜在扩散模型（AF-LDM）来实现更强的等变性。为了解决LDM面临的特定挑战，如变分自编码器训练中的别名放大和多U-Net推理问题，以及自注意力模块本身缺乏等变性，该研究重新设计了注意力模块并提出了等变性损失来抑制连续域中的特征频率带宽。实验结果证明，AF-LDM在各种应用中表现优于原始LDM，实现了更一致的生成结果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LDMs存在生成过程不稳定的问题，导致输入噪声的小扰动或变化可能导致显著不同的输出。</li>
<li>为提高一致性，研究者提出了AF-LDM模型以增强其等变性。</li>
<li>通过引入抗混叠操作，AF-LDM能够部分改善等变性。然而，由于LDM面临的特定挑战，仍存在显著的混叠和不一致性。</li>
<li>LDM的挑战包括变分自编码器训练中的别名放大问题以及多个U-Net推理过程中出现的问题。此外，其自注意力模块本质上缺乏等变性也是问题所在。对此提出了两种解决方案：重新设计注意力模块以实现等变性，并引入等变性损失来抑制特征频率带宽。这些措施旨在消除别名影响并提高模型的稳健性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09419">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2b1012620b0eba499948e53db16ffe2e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a671201e0c757dbebc4713a57d2bcda.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-898d72dd75d4a3c968cf26cef050c908.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f4598a30be8cdac79f2877ee3991ae5f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6997a1ba7f8d12f60da090caead5a444.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Reangle-A-Video-4D-Video-Generation-as-Video-to-Video-Translation"><a href="#Reangle-A-Video-4D-Video-Generation-as-Video-to-Video-Translation" class="headerlink" title="Reangle-A-Video: 4D Video Generation as Video-to-Video Translation"></a>Reangle-A-Video: 4D Video Generation as Video-to-Video Translation</h2><p><strong>Authors:Hyeonho Jeong, Suhyeon Lee, Jong Chul Ye</strong></p>
<p>We introduce Reangle-A-Video, a unified framework for generating synchronized multi-view videos from a single input video. Unlike mainstream approaches that train multi-view video diffusion models on large-scale 4D datasets, our method reframes the multi-view video generation task as video-to-videos translation, leveraging publicly available image and video diffusion priors. In essence, Reangle-A-Video operates in two stages. (1) Multi-View Motion Learning: An image-to-video diffusion transformer is synchronously fine-tuned in a self-supervised manner to distill view-invariant motion from a set of warped videos. (2) Multi-View Consistent Image-to-Images Translation: The first frame of the input video is warped and inpainted into various camera perspectives under an inference-time cross-view consistency guidance using DUSt3R, generating multi-view consistent starting images. Extensive experiments on static view transport and dynamic camera control show that Reangle-A-Video surpasses existing methods, establishing a new solution for multi-view video generation. We will publicly release our code and data. Project page: <a target="_blank" rel="noopener" href="https://hyeonho99.github.io/reangle-a-video/">https://hyeonho99.github.io/reangle-a-video/</a> </p>
<blockquote>
<p>我们介绍Reangle-A-Video，这是一个从单个输入视频生成同步多视角视频的统一框架。不同于主流方法在大型4D数据集上训练多视角视频扩散模型，我们的方法将多视角视频生成任务重新构建为视频到视频的翻译，利用公开可用的图像和视频扩散先验。本质上，Reangle-A-Video分为两个阶段。（1）多视角运动学习：以自监督的方式同步微调图像到视频扩散转换器，从一组扭曲的视频中提炼出视角不变的运动。（2）多视角一致图像到图像的翻译：输入视频的第一帧在推理时间跨视角一致性指导下扭曲和填充，生成多视角一致的开始图像。关于静态视角传输和动态摄像机控制的广泛实验表明，Reangle-A-Video超越了现有方法，为多视角视频生成建立了新的解决方案。我们将公开发布我们的代码和数据。项目页面：<a target="_blank" rel="noopener" href="https://hyeonho99.github.io/reangle-a-video/">https://hyeonho99.github.io/reangle-a-video/</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09151v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://hyeonho99.github.io/reangle-a-video/">https://hyeonho99.github.io/reangle-a-video/</a></p>
<p><strong>Summary</strong></p>
<p>Reangle-A-Video框架能够从单一输入视频生成同步多视角视频。它采用视频到视频的翻译方式，利用公开可用的图像和视频扩散先验知识，不同于主流在大型4D数据集上训练多视角视频扩散模型的方法。Reangle-A-Video分为两个阶段：多视角运动学习和多视角一致图像到图像的翻译。通过同步自监督方式微调图像到视频扩散转换器，从一组变形视频中提取视图不变的运动。在输入视频的第一帧中，采用跨视角一致性指导生成多视角一致起始图像。实验表明，Reangle-A-Video在静态视角转换和动态摄像机控制上超越了现有方法，为多视角视频生成提供了新的解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Reangle-A-Video是一个从单一输入视频生成同步多视角视频的框架。</li>
<li>它采用视频到视频的翻译方式，利用图像和视频扩散先验知识。</li>
<li>该方法通过两个阶段进行：多视角运动学习和多视角一致图像到图像的翻译。</li>
<li>通过同步自监督方式提取视图不变的运动。</li>
<li>生成多视角一致起始图像时采用了跨视角一致性指导。</li>
<li>Reangle-A-Video在静态和动态视角转换上表现出超越现有方法的效果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09151">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-170cd7be7f5ed78ae237c5de7aecd105.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f78a1bb6134454b0aa5d26d13966d989.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e9c42602c21b81f4a9b9c1e75bf0491a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e6b7950510b9a10e64706e857d8e34f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4cc5663c575eb9a60228af9158e38ef4.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Are-ECGs-enough-Deep-learning-classification-of-cardiac-anomalies-using-only-electrocardiograms"><a href="#Are-ECGs-enough-Deep-learning-classification-of-cardiac-anomalies-using-only-electrocardiograms" class="headerlink" title="Are ECGs enough? Deep learning classification of cardiac anomalies using   only electrocardiograms"></a>Are ECGs enough? Deep learning classification of cardiac anomalies using   only electrocardiograms</h2><p><strong>Authors:Joao D. S. Marques, Arlindo L. Oliveira</strong></p>
<p>Electrocardiography (ECG) is an essential tool for diagnosing multiple cardiac anomalies: it provides valuable clinical insights, while being affordable, fast and available in many settings. However, in the current literature, the role of ECG analysis is often unclear: many approaches either rely on additional imaging modalities, such as Computed Tomography Pulmonary Angiography (CTPA), which may not always be available, or do not effectively generalize across different classification problems. Furthermore, the availability of public ECG datasets is limited and, in practice, these datasets tend to be small, making it essential to optimize learning strategies. In this study, we investigate the performance of multiple neural network architectures in order to assess the impact of various approaches. Moreover, we check whether these practices enhance model generalization when transfer learning is used to translate information learned in larger ECG datasets, such as PTB-XL and CPSC18, to a smaller, more challenging dataset for pulmonary embolism (PE) detection. By leveraging transfer learning, we analyze the extent to which we can improve learning efficiency and predictive performance on limited data. Code available at <a target="_blank" rel="noopener" href="https://github.com/joaodsmarques/Are-ECGs-enough-Deep-Learning-Classifiers">https://github.com/joaodsmarques/Are-ECGs-enough-Deep-Learning-Classifiers</a> . </p>
<blockquote>
<p>心电图（ECG）是诊断多种心脏异常的重要工具：它提供了宝贵的临床见解，同时经济实惠、快速，并在许多环境中都可使用。然而，在目前的文献中，心电图分析的作用往往不明确：许多方法要么依赖于可能并不可用的其他成像模式，例如计算机断层扫描肺动脉造影术（CTPA），要么在不同的分类问题中不能有效地通用。此外，公共心电图数据集的可获得性有限，实际上这些数据集往往很小，因此必须优化学习策略。在这项研究中，我们调查了多种神经网络架构的性能，以评估各种方法的影响。此外，我们还检查了当使用迁移学习将在大规模心电图数据集中学习到的信息（如PTB-XL和CPSC18）迁移到较小但更具挑战性的肺栓塞（PE）检测数据集时，这些实践是否提高了模型的通用性。通过利用迁移学习，我们分析了在有限数据上，我们能提高多少学习效率和预测性能。相关代码可通过以下链接获取：<a target="_blank" rel="noopener" href="https://github.com/joaodsmarques/Are-ECGs-enough-Deep-Learning-Classifiers%EF%BC%88%E4%BB%A5%E4%BD%BF%E7%9C%8B%E5%8F%A4%EF%BC%89">https://github.com/joaodsmarques/Are-ECGs-enough-Deep-Learning-Classifiers。（以供参考）</a>（请以实际网站内容为准）</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08960v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>心电图（ECG）是诊断多种心脏异常的重要工具，具有经济、快速、可在多种环境中使用的优势。然而，现有文献中关于心电图分析的角色往往不明确，许多方法依赖于可能无法始终获得的附加成像模式，如计算机断层扫描肺动脉造影术（CTPA），或者不能有效地在不同分类问题中推广。此外，公开的心电图数据集有限，且在实践中这些数据集往往很小，因此需要优化学习策略。本研究旨在评估多种神经网络架构的性能，并检查这些实践在提高模型泛化能力方面的作用，特别是在使用迁移学习将在大规模心电图数据集中学习的信息转移到较小的、更具挑战性的肺栓塞检测数据集时。通过利用迁移学习，我们分析了在有限数据上提高学习效率和预测性能的可行性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>心电图是诊断心脏异常的重要工具，具有广泛的应用和经济性优势。</li>
<li>当前文献中心电图分析的角色存在不明确性，许多方法依赖于其他成像模式或难以在不同分类问题中推广。</li>
<li>公共心电图数据集有限且实际应用中往往规模较小，需要优化学习策略。</li>
<li>本研究评估了多种神经网络架构的性能以提高模型泛化能力。</li>
<li>研究重点之一是使用迁移学习在大规模心电图数据集中学习的信息转移到小规模数据集上的能力。</li>
<li>利用迁移学习可改善有限数据上的学习效率和预测性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08960">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-7533ed2e67bbfcebeeecbe6d61033031.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-beae897000d09d52e37d256426cefe0b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-99994b4b274486ef7f2fe93fa38211ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b66a0d46ce613c790a74ade10b04846.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Versatile-Multimodal-Controls-for-Whole-Body-Talking-Human-Animation"><a href="#Versatile-Multimodal-Controls-for-Whole-Body-Talking-Human-Animation" class="headerlink" title="Versatile Multimodal Controls for Whole-Body Talking Human Animation"></a>Versatile Multimodal Controls for Whole-Body Talking Human Animation</h2><p><strong>Authors:Zheng Qin, Ruobing Zheng, Yabing Wang, Tianqi Li, Zixin Zhu, Minghui Yang, Ming Yang, Le Wang</strong></p>
<p>Human animation from a single reference image shall be flexible to synthesize whole-body motion for either a headshot or whole-body portrait, where the motions are readily controlled by audio signal and text prompts. This is hard for most existing methods as they only support producing pre-specified head or half-body motion aligned with audio inputs. In this paper, we propose a versatile human animation method, i.e., VersaAnimator, which generates whole-body talking human from arbitrary portrait images, not only driven by audio signal but also flexibly controlled by text prompts. Specifically, we design a text-controlled, audio-driven motion generator that produces whole-body motion representations in 3D synchronized with audio inputs while following textual motion descriptions. To promote natural smooth motion, we propose a code-pose translation module to link VAE codebooks with 2D DWposes extracted from template videos. Moreover, we introduce a multi-modal video diffusion that generates photorealistic human animation from a reference image according to both audio inputs and whole-body motion representations. Extensive experiments show that VersaAnimator outperforms existing methods in visual quality, identity preservation, and audio-lip synchronization. </p>
<blockquote>
<p>从单一参考图像进行的人脸动画应该能够灵活地合成头部特写或全身肖像的整体运动，这些运动可以由音频信号和文字提示轻松控制。这对大多数现有方法来说是非常困难的，因为它们仅支持生成与音频输入对齐的预设头部或半身运动。在本文中，我们提出了一种通用的人脸动画方法，即VersaAnimator，它可以从任意的肖像图像生成全身说话的人脸动画，不仅由音频信号驱动，还可以灵活地由文字提示控制。具体来说，我们设计了一个文本控制的、音频驱动的运动生成器，该生成器能够产生与音频输入同步的全身运动表示（在3D中），同时遵循文本运动描述。为了促进自然流畅的运动，我们提出了一个编码姿态转换模块，用于将VAE代码本与从模板视频中提取的2DDW姿态联系起来。此外，我们引入了一种多模态视频扩散技术，该技术可以根据参考图像、音频输入和全身运动表示生成逼真的人脸动画。大量实验表明，VersaAnimator在视觉质量、身份保留和音频与嘴唇同步方面优于现有方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08714v1">PDF</a> </p>
<p><strong>Summary</strong><br>单参考图像的人像动画可以灵活合成全身动作，无论是头像还是全身肖像，动作都可以通过音频信号和文字提示轻松控制。大多数现有方法仅支持生成与音频输入对齐的预设头部或半身动作，难以实现全身动作的灵活控制。本文提出了一种通用的人像动画方法VersaAnimator，可以从任意肖像图像生成全身动作，不仅由音频信号驱动，还能通过文字提示灵活控制。此方法结合了文本控制、音频驱动的运动生成器，产生与音频输入同步的全身动作表示，并通过模板视频提取的DWposes与VAE代码本的转换模块，实现自然流畅的动作生成。同时引入了多模态视频扩散技术，根据参考图像、音频输入和全身动作表示生成逼真的动画。实验表明，VersaAnimator在视觉质量、身份保留和音频唇同步方面优于现有方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>人像动画可以从单参考图像生成全身动作，包括头部和全身肖像。</li>
<li>动作可以通过音频信号和文字提示灵活控制。</li>
<li>现有方法仅支持预设的头部或半身动作与音频输入对齐，难以实现全身动作的灵活控制。</li>
<li>提出了一种通用的人像动画方法VersaAnimator，能生成与音频输入同步的全身动作表示。</li>
<li>通过模板视频提取的DWposes与VAE代码本的转换模块实现自然流畅的动作生成。</li>
<li>引入了多模态视频扩散技术，提高了动画的逼真度。</li>
<li>VersaAnimator在视觉质量、身份保留和音频唇同步方面优于现有方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08714">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-544e44d51db45d3e5459d544cf44575e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1cadea61e4907cd6a69432076455f3d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b111d2208a9e49f013d5550bffdaa200.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-28fcbcf16b6a3381317a45ec3ee5082f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6dfa8bd672ff361a77bf3c1459b8296e.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Segmentation-Guided-CT-Synthesis-with-Pixel-Wise-Conformal-Uncertainty-Bounds"><a href="#Segmentation-Guided-CT-Synthesis-with-Pixel-Wise-Conformal-Uncertainty-Bounds" class="headerlink" title="Segmentation-Guided CT Synthesis with Pixel-Wise Conformal Uncertainty   Bounds"></a>Segmentation-Guided CT Synthesis with Pixel-Wise Conformal Uncertainty   Bounds</h2><p><strong>Authors:David Vallmanya Poch, Yorick Estievenart, Elnura Zhalieva, Sukanya Patra, Mohammad Yaqub, Souhaib Ben Taieb</strong></p>
<p>Accurate dose calculations in proton therapy rely on high-quality CT images. While planning CTs (pCTs) serve as a reference for dosimetric planning, Cone Beam CT (CBCT) is used throughout Adaptive Radiotherapy (ART) to generate sCTs for improved dose calculations. Despite its lower cost and reduced radiation exposure advantages, CBCT suffers from severe artefacts and poor image quality, making it unsuitable for precise dosimetry. Deep learning-based CBCT-to-CT translation has emerged as a promising approach. Still, existing methods often introduce anatomical inconsistencies and lack reliable uncertainty estimates, limiting their clinical adoption. To bridge this gap, we propose STF-RUE, a novel framework integrating two key components. First, STF, a segmentation-guided CBCT-to-CT translation method that enhances anatomical consistency by leveraging segmentation priors extracted from pCTs. Second, RUE, a conformal prediction method that augments predicted CTs with pixel-wise conformal prediction intervals, providing clinicians with robust reliability indicator. Comprehensive experiments using UNet++ and Fast-DDPM on two benchmark datasets demonstrate that STF-RUE significantly improves translation accuracy, as measured by a novel soft-tissue-focused metric designed for precise dose computation. Additionally, STF-RUE provides better-calibrated uncertainty sets for synthetic CT, reinforcing trust in synthetic CTs. By addressing both anatomical fidelity and uncertainty quantification, STF-RUE marks a crucial step toward safer and more effective adaptive proton therapy. Code is available at <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/cbct2ct_translation-B2D9/">https://anonymous.4open.science/r/cbct2ct_translation-B2D9/</a>. </p>
<blockquote>
<p>质子疗法中的准确剂量计算依赖于高质量CT图像。计划CT（pCT）作为剂量计划的参考，而锥形束CT（CBCT）在自适应放射治疗（ART）中用于生成改进剂量计算的sCT。尽管CBCT具有成本低、辐射暴露优势小等优点，但它存在严重的伪影和图像质量差的问题，因此不适合进行精确的剂量测定。基于深度学习的CBCT-to-CT转换已成为一种有前途的方法。然而，现有方法往往引入解剖结构不一致性，且缺乏可靠的不确定性估计，限制了其在临床上的应用。为了弥补这一差距，我们提出了STF-RUE，这是一个结合了关键组件的新框架。首先，STF是一种受分割指导的CBCT-to-CT转换方法，它通过利用从pCT中提取的分割先验知识来提高解剖结构的一致性。其次，RUE是一种符合预测的方法，它为预测的CT配备像素级的预测间隔，为临床医生提供可靠的可靠性指标。在两项基准数据集上使用UNet++和Fast-DDPM进行的综合实验表明，STF-RUE在新型软组织针对性指标衡量下显著提高翻译精度，该指标专为精确剂量计算而设计。此外，STF-RUE为合成CT提供更好的校准不确定性集，增强了对合成CT的信任。通过解决解剖结构忠实度和不确定性量化问题，STF-RUE标志着自适应质子疗法朝着更安全、更有效的方向迈出了重要一步。代码可在<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/cbct2ct_translation-B2D9/%E6%89%BE%E5%88%B0%E3%80%82">https://anonymous.4open.science/r/cbct2ct_translation-B2D9/找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08515v1">PDF</a> MICCAI 2025 Conference Submission. Follows the required LNCS format.   12 pages including references. Contains 4 figures and 1 table</p>
<p><strong>Summary</strong><br>     质子治疗中的精确剂量计算依赖于高质量的CT图像。在适应性放射治疗（ART）中，锥束CT（CBCT）被用于生成sCT以改进剂量计算，尽管其成本较低且辐射暴露减少，但其图像质量存在严重缺陷，不适宜进行精确剂量测定。深度学习在CBCT到CT的翻译上展现出潜力，但现有方法常带来解剖结构不一致性和缺乏可靠的不确定性估计，限制了其临床应用。本研究提出了STF-RUE框架，该框架结合了两项关键技术，提升了质子治疗的精准度和可靠性。 </p>
<p><strong>Key Takeaways</strong></p>
<pre><code>* CBCT虽有助于剂量计算但存在严重图像质量缺陷问题，对精准放射治疗效果有影响。 
* 当前用于处理CBCT到CT翻译的深度学习技术面临解剖结构不一致性和不确定性估计的挑战。 
* STF-RUE框架通过集成两项关键技术解决了上述问题，包括基于分割引导的CBCT到CT翻译方法和基于一致性预测的可靠性指标估计方法。 
* STF-RUE显著提高了翻译精度和可靠性指标，改善了合成CT的不确定性校准，对质子治疗安全有效性具有关键性影响。这一研究成果已经通过实验验证。 ​​
</code></pre>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08515">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-500efa926383894bb180f86afe434814.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bed1cbeb2a1dfc3c99e78c063d6a5212.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6879ddd99c5e2c361b5e97d3e35d12ef.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Data-Foundations-for-Large-Scale-Multimodal-Clinical-Foundation-Models"><a href="#Data-Foundations-for-Large-Scale-Multimodal-Clinical-Foundation-Models" class="headerlink" title="Data Foundations for Large Scale Multimodal Clinical Foundation Models"></a>Data Foundations for Large Scale Multimodal Clinical Foundation Models</h2><p><strong>Authors:Wei Dai, Peilin Chen, Malinda Lu, Daniel Li, Haowen Wei, Hejie Cui, Paul Pu Liang</strong></p>
<p>Recent advances in clinical AI have enabled remarkable progress across many clinical domains. However, existing benchmarks and models are primarily limited to a small set of modalities and tasks, which hinders the development of large-scale multimodal methods that can make holistic assessments of patient health and well-being. To bridge this gap, we introduce Clinical Large-Scale Integrative Multimodal Benchmark (CLIMB), a comprehensive clinical benchmark unifying diverse clinical data across imaging, language, temporal, and graph modalities. CLIMB comprises 4.51 million patient samples totaling 19.01 terabytes distributed across 2D imaging, 3D video, time series, graphs, and multimodal data. Through extensive empirical evaluation, we demonstrate that multitask pretraining significantly improves performance on understudied domains, achieving up to 29% improvement in ultrasound and 23% in ECG analysis over single-task learning. Pretraining on CLIMB also effectively improves models’ generalization capability to new tasks, and strong unimodal encoder performance translates well to multimodal performance when paired with task-appropriate fusion strategies. Our findings provide a foundation for new architecture designs and pretraining strategies to advance clinical AI research. Code is released at <a target="_blank" rel="noopener" href="https://github.com/DDVD233/climb">https://github.com/DDVD233/climb</a>. </p>
<blockquote>
<p>近年来，临床人工智能的进步在许多临床领域都取得了显著的成效。然而，现有的基准测试和模型主要局限于少数几种模态和任务，这阻碍了能够全面评估患者健康和福祉的大规模多模态方法的发展。为了弥补这一差距，我们引入了临床大规模综合多模态基准测试（CLIMB），这是一个统一了成像、语言、时间和图形等多种临床数据模式的综合临床基准测试。CLIMB包含了跨越二维成像、三维视频、时间序列、图形和多模态数据的共达四千五百一十万患者样本数据，总计十九亿字节数据分布。通过广泛的实证研究，我们证明了多任务预训练能够在缺乏研究的领域里显著提高性能，超声波分析中的提升率高达百分之二十九，心电图分析中达到百分之二十三于单任务学习之上。在CLIMB上进行预训练还能有效提高模型对新任务的泛化能力，强大的单模态编码器性能在与任务适当的融合策略结合时，能很好地转化为多模态性能。我们的研究为新的架构设计和预训练策略提供了基础，以推动临床人工智能研究的发展。相关代码已发布在<a target="_blank" rel="noopener" href="https://github.com/DDVD233/climb%E4%B8%8A%E3%80%82">https://github.com/DDVD233/climb上。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07667v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文主要介绍了临床人工智能的最新进展及多模态方法的局限性，为此提出了一种大规模的多模态临床基准测试（CLIMB），涵盖了图像、语言、时序和图等多种临床数据模态。通过多任务预训练，CLIMB在超声和心电图分析等领域取得了显著的提升效果。此外，CLIMB还为临床人工智能研究提供了新的架构设计和预训练策略的基础。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>临床人工智能在多个领域取得显著进展，但多模态方法的开发受限。</li>
<li>提出了一种新的临床基准测试CLIMB，涵盖多种临床数据模态。</li>
<li>CLIMB包含4.51百万患者样本，总计19.01太字节的数据。</li>
<li>多任务预训练在CLIMB上能显著提升模型性能，特别是在超声和心电图分析领域。</li>
<li>预训练有助于模型对新任务的适应能力。</li>
<li>强大的单模态编码器性能，在配合适当的融合策略时，能取得良好的多模态性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07667">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-719deba1687c0befc6f04e3b1af164af.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a57dd9ae0a0380241033c2aa849e0488.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d84bf8e80c2d110cadd55dc18fd78e76.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-14\./crop_I2I Translation/2503.07667v1/page_3_0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f9816e50733bbd50466a98e12955c36b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-29ef902c04cb92589f4a1a7672a04812.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="LLM-HDR-Bridging-LLM-based-Perception-and-Self-Supervision-for-Unpaired-LDR-to-HDR-Image-Reconstruction"><a href="#LLM-HDR-Bridging-LLM-based-Perception-and-Self-Supervision-for-Unpaired-LDR-to-HDR-Image-Reconstruction" class="headerlink" title="LLM-HDR: Bridging LLM-based Perception and Self-Supervision for Unpaired   LDR-to-HDR Image Reconstruction"></a>LLM-HDR: Bridging LLM-based Perception and Self-Supervision for Unpaired   LDR-to-HDR Image Reconstruction</h2><p><strong>Authors:Hrishav Bakul Barua, Kalin Stefanov, Lemuel Lai En Che, Abhinav Dhall, KokSheik Wong, Ganesh Krishnasamy</strong></p>
<p>The translation of Low Dynamic Range (LDR) to High Dynamic Range (HDR) images is an important computer vision task. There is a significant amount of research utilizing both conventional non-learning methods and modern data-driven approaches, focusing on using both single-exposed and multi-exposed LDR for HDR image reconstruction. However, most current state-of-the-art methods require high-quality paired {LDR,HDR} datasets for model training. In addition, there is limited literature on using unpaired datasets for this task, that is, the model learns a mapping between domains, i.e., {LDR,HDR}. This paper proposes LLM-HDR, a method that integrates the perception of Large Language Models (LLM) into a modified semantic- and cycle-consistent adversarial architecture that utilizes unpaired {LDR,HDR} datasets for training. The method introduces novel artifact- and exposure-aware generators to address visual artifact removal and an encoder and loss to address semantic consistency, another under-explored topic. LLM-HDR is the first to use an LLM for the {LDR,HDR} translation task in a self-supervised setup. The method achieves state-of-the-art performance across several benchmark datasets and reconstructs high-quality HDR images. The official website of this work is available at: <a target="_blank" rel="noopener" href="https://github.com/HrishavBakulBarua/LLM-HDR">https://github.com/HrishavBakulBarua/LLM-HDR</a> </p>
<blockquote>
<p>将低动态范围（LDR）图像翻译成高动态范围（HDR）图像是一项重要的计算机视觉任务。许多研究都利用传统的非学习方法和现代的数据驱动方法，专注于使用单曝光和多曝光的LDR进行HDR图像重建。然而，目前大多数最先进的方法都需要高质量成对的{LDR，HDR}数据集进行模型训练。此外，关于使用非配对数据集进行此任务的文献很少，也就是说，模型在两个领域之间进行映射学习，即{LDR，HDR}。本文提出了LLM-HDR方法，它将大型语言模型（LLM）的感知能力集成到一个经过修改的语义和循环一致的对抗性架构中，该架构利用非配对的{LDR，HDR}数据集进行训练。该方法引入了新型的去伪影和曝光感知生成器来解决视觉伪影去除问题，以及一个解决语义一致性的编码器和损失函数，这是另一个尚未深入探讨的主题。LLM-HDR是第一个在自我监督设置中使用LLM进行{LDR，HDR}翻译任务的方法。该方法在多个基准数据集上实现了最先进的性能，并重建了高质量HDR图像。该工作的官方网站地址为：<a target="_blank" rel="noopener" href="https://github.com/HrishavBakulBarua/LLM-HDR">https://github.com/HrishavBakulBarua/LLM-HDR</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.15068v2">PDF</a> </p>
<p><strong>Summary</strong>：本文提出一种将低动态范围（LDR）图像翻译成高动态范围（HDR）图像的新方法，即LLM-HDR。该方法利用大型语言模型（LLM）感知能力，采用无配对数据集的语义一致性循环对抗架构进行训练。引入新颖的伪影感知和曝光感知生成器解决视觉伪影去除问题，并通过编码器和损失解决语义一致性问题。LLM-HDR首次在自监督设置中利用LLM进行LDR到HDR的翻译任务，实现了跨多个基准数据集的卓越性能，并能重建高质量HDR图像。</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>LLM-HDR是一种新的低动态范围（LDR）到高动态范围（HDR）图像翻译方法。</li>
<li>该方法结合了大型语言模型（LLM）的感知能力进行图像翻译。</li>
<li>LLM-HDR采用无配对数据集的语义一致性循环对抗架构进行训练。</li>
<li>该方法引入了伪影感知和曝光感知生成器以解决视觉伪影去除问题。</li>
<li>LLM-HDR首次在自监督设置中使用LLM进行HDR图像重建。</li>
<li>LLM-HDR实现了跨多个基准数据集的卓越性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.15068">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ed0943f3f67423ae0c92ddaeb82166ed.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c796c8b662950542046b576ea2142a76.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f7d1f0997d46770f2f236f98bb63377.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Towards-Zero-Shot-Multimodal-Machine-Translation"><a href="#Towards-Zero-Shot-Multimodal-Machine-Translation" class="headerlink" title="Towards Zero-Shot Multimodal Machine Translation"></a>Towards Zero-Shot Multimodal Machine Translation</h2><p><strong>Authors:Matthieu Futeral, Cordelia Schmid, Benoît Sagot, Rachel Bawden</strong></p>
<p>Current multimodal machine translation (MMT) systems rely on fully supervised data (i.e models are trained on sentences with their translations and accompanying images). However, this type of data is costly to collect, limiting the extension of MMT to other language pairs for which such data does not exist. In this work, we propose a method to bypass the need for fully supervised data to train MMT systems, using multimodal English data only. Our method, called ZeroMMT, consists in adapting a strong text-only machine translation (MT) model by training it on a mixture of two objectives: visually conditioned masked language modelling and the Kullback-Leibler divergence between the original and new MMT outputs. We evaluate on standard MMT benchmarks and the recently released CoMMuTE, a contrastive benchmark aiming to evaluate how well models use images to disambiguate English sentences. We obtain disambiguation performance close to state-of-the-art MMT models trained additionally on fully supervised examples. To prove that our method generalizes to languages with no fully supervised training data available, we extend the CoMMuTE evaluation dataset to three new languages: Arabic, Russian and Chinese. We further show that we can control the trade-off between disambiguation capabilities and translation fidelity at inference time using classifier-free guidance and without any additional data. Our code, data and trained models are publicly accessible. </p>
<blockquote>
<p>当前的多模态机器翻译（MMT）系统依赖于完全监督的数据（即模型在具有翻译和相应图像的句子上进行训练）。然而，这种类型的数据收集成本很高，限制了MMT在其他没有此类数据存在的语言配对中的扩展。在这项工作中，我们提出了一种方法，绕过对完全监督数据训练MMT系统的需求，仅使用多模态英语数据。我们的方法称为ZeroMMT，通过对强大的纯文本机器翻译（MT）模型进行适应，以两个目标进行训练：视觉条件下的掩码语言建模和原始与新的MMT输出之间的Kullback-Leibler散度。我们在标准MMT基准测试和最近发布的CoMMuTE上进行了评估，CoMMuTE是一个对比基准测试，旨在评估模型使用图像对英文句子进行消歧的能力。我们获得的消歧性能接近在完全监督示例上训练的最新MMT模型。为了证明我们的方法可以推广到没有完全监督训练数据可用的语言，我们将CoMMuTE评估数据集扩展到三种新语言：阿拉伯语、俄语和中文。我们进一步表明，我们可以在推理时间使用无分类器引导和无需任何额外数据的情况下，控制消歧能力和翻译忠实度之间的权衡。我们的代码、数据和训练模型可公开访问。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.13579v2">PDF</a> NAACL 2025 (Findings)</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种不需要全监督数据训练多模态机器翻译系统的方法，名为ZeroMMT。该方法通过训练强大的纯文本机器翻译模型，实现两种目标混合：视觉条件下的掩码语言建模和原始与新型多模态翻译输出之间的Kullback-Leibler散度。评估结果显示，该方法在标准多模态翻译基准测试集和最新发布的旨在评估模型如何利用图像消除英语句子歧义的对比基准测试集CoMMuTE上的表现接近使用全监督示例训练的多模态翻译模型的最先进水平。此外，为了证明该方法可以推广到没有全监督训练数据可用的语言，我们将CoMMuTE评估数据集扩展到了三种新语言：阿拉伯语、俄语和中文。同时，我们还展示了在推理时间，我们可以通过无分类器引导的方式，无需任何额外数据来控制消除歧义能力和翻译忠实度之间的权衡。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前的多模态机器翻译系统依赖于全监督数据，成本高昂且限制了其在没有此类数据的语言对的扩展。</li>
<li>提出了一种名为ZeroMMT的方法，通过训练强大的纯文本机器翻译模型来适应多模态翻译，无需全监督数据。</li>
<li>ZeroMMT方法结合了两种目标：视觉条件下的掩码语言建模和原始与新型多模态翻译输出之间的Kullback-Leibler散度。</li>
<li>在标准多模态翻译基准测试集和CoMMuTE评估数据集上，ZeroMMT的表现接近最先进水平。</li>
<li>ZeroMMT方法可以推广到没有全监督训练数据的语言，如阿拉伯语、俄语和中文。</li>
<li>通过无分类器引导的方式，可以在推理时间控制消除歧义能力和翻译忠实度之间的权衡。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.13579">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4029089afd410537fc1e7b6a76c39b7e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5d2987f3d40fc2b69d037cac28297811.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a114a08cd327cd33dbc03616a84b252.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-29829cea5f32a96518940e4839d7709a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-639dc6f9f753fd6a66ba67eca0370368.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-074533f0c8d39cf0f33b3375a3f5be3d.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Computing-high-dimensional-optimal-transport-by-flow-neural-networks"><a href="#Computing-high-dimensional-optimal-transport-by-flow-neural-networks" class="headerlink" title="Computing high-dimensional optimal transport by flow neural networks"></a>Computing high-dimensional optimal transport by flow neural networks</h2><p><strong>Authors:Chen Xu, Xiuyuan Cheng, Yao Xie</strong></p>
<p>Computing optimal transport (OT) for general high-dimensional data has been a long-standing challenge. Despite much progress, most of the efforts including neural network methods have been focused on the static formulation of the OT problem. The current work proposes to compute the dynamic OT between two arbitrary distributions $P$ and $Q$ by optimizing a flow model, where both distributions are only accessible via finite samples. Our method learns the dynamic OT by finding an invertible flow that minimizes the transport cost. The trained optimal transport flow subsequently allows for performing many downstream tasks, including infinitesimal density ratio estimation (DRE) and domain adaptation by interpolating distributions in the latent space. The effectiveness of the proposed model on high-dimensional data is demonstrated by strong empirical performance on OT baselines, image-to-image translation, and high-dimensional DRE. </p>
<blockquote>
<p>计算通用高维数据的最优传输（OT）一直是一个长期存在的挑战。尽管取得了诸多进展，但包括神经网络方法在内的大多数努力都集中在OT问题的静态公式上。当前的工作提出了一种通过优化流模型来计算两个任意分布$P$和$Q$之间的动态OT的方法，这两个分布只能通过有限样本访问。我们的方法通过学习动态OT，找到一个可逆流，以最小化传输成本。训练后的最优传输流随后可执行许多下游任务，包括无穷小密度比率估计（DRE）和通过潜在空间中的分布插值进行域适应。所提出模型在高维数据上的有效性已通过OT基准测试、图像到图像的翻译和高维DRE的强大实证性能得到证明。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2305.11857v5">PDF</a> Accepted by AISTATS 2025</p>
<p><strong>Summary</strong></p>
<p>本文提出了动态最优传输（OT）的计算方法，用于计算两个任意分布P和Q之间的动态OT。该方法通过优化流模型来学习动态OT，其中两个分布仅通过有限样本访问。训练后的最优传输流可以进行许多下游任务，包括无穷密度比率估计（DRE）和域适应，通过在潜在空间中对分布进行插值。该模型在高维数据上的有效性通过在对OT基线、图像到图像的翻译和高维DRE上的强大经验表现得到证明。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>本文提出了计算两个任意分布间动态最优传输（OT）的方法。</li>
<li>该方法通过优化流模型来学习动态OT。</li>
<li>分布仅通过有限样本进行访问。</li>
<li>训练后的最优传输流可以进行多种下游任务，如无穷密度比率估计（DRE）和域适应。</li>
<li>插值在潜在空间中的分布有助于提高模型的性能。</li>
<li>模型在高维数据上的有效性得到了强有力的实证证明。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2305.11857">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-c60431ecbff06cbaed5fb309ab037c80.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d68d4418286b70b4a62e3d0fcd7c3e04.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-14/I2I%20Translation/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-14/I2I%20Translation/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/I2I-Translation/">
                                    <span class="chip bg-color">I2I Translation</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-14/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-b657b10c8d2e5e2c6f07564b881637c9.jpg" class="responsive-img" alt="视频理解">
                        
                        <span class="card-title">视频理解</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            视频理解 方向最新论文已更新，请持续关注 Update in 2025-03-14  Memory-enhanced Retrieval Augmentation for Long Video Understanding
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-14
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    视频理解
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">视频理解</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-14/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-b5713d02847055507997eb5be3d3f6db.jpg" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-03-14  Membership Inference Attacks fueled by Few-Short Learning to detect   privacy leakage tackling data integrity
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-14
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">23539.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
