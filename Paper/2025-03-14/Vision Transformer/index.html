<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Vision Transformer">
    <meta name="description" content="Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-14  Evaluating Visual Explanations of Attention Maps for Transformer-based   Medical Imaging">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Vision Transformer | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-12251ebb19d0447c881400e74be59b48.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Vision Transformer</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Vision-Transformer/">
                                <span class="chip bg-color">Vision Transformer</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                Vision Transformer
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-14
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    37 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-14-æ›´æ–°"><a href="#2025-03-14-æ›´æ–°" class="headerlink" title="2025-03-14 æ›´æ–°"></a>2025-03-14 æ›´æ–°</h1><h2 id="Evaluating-Visual-Explanations-of-Attention-Maps-for-Transformer-based-Medical-Imaging"><a href="#Evaluating-Visual-Explanations-of-Attention-Maps-for-Transformer-based-Medical-Imaging" class="headerlink" title="Evaluating Visual Explanations of Attention Maps for Transformer-based   Medical Imaging"></a>Evaluating Visual Explanations of Attention Maps for Transformer-based   Medical Imaging</h2><p><strong>Authors:Minjae Chung, Jong Bum Won, Ganghyun Kim, Yujin Kim, Utku Ozbulak</strong></p>
<p>Although Vision Transformers (ViTs) have recently demonstrated superior performance in medical imaging problems, they face explainability issues similar to previous architectures such as convolutional neural networks. Recent research efforts suggest that attention maps, which are part of decision-making process of ViTs can potentially address the explainability issue by identifying regions influencing predictions, especially in models pretrained with self-supervised learning. In this work, we compare the visual explanations of attention maps to other commonly used methods for medical imaging problems. To do so, we employ four distinct medical imaging datasets that involve the identification of (1) colonic polyps, (2) breast tumors, (3) esophageal inflammation, and (4) bone fractures and hardware implants. Through large-scale experiments on the aforementioned datasets using various supervised and self-supervised pretrained ViTs, we find that although attention maps show promise under certain conditions and generally surpass GradCAM in explainability, they are outperformed by transformer-specific interpretability methods. Our findings indicate that the efficacy of attention maps as a method of interpretability is context-dependent and may be limited as they do not consistently provide the comprehensive insights required for robust medical decision-making. </p>
<blockquote>
<p>å°½ç®¡è§†è§‰Transformerï¼ˆViTsï¼‰åœ¨åŒ»å­¦å½±åƒé—®é¢˜ä¸­å±•ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä½†å®ƒä»¬ä»é¢ä¸´ä¸æ—©æœŸæ¶æ„ï¼ˆå¦‚å·ç§¯ç¥ç»ç½‘ç»œï¼‰ç±»ä¼¼çš„è§£é‡Šæ€§é—®é¢˜ã€‚æœ€è¿‘çš„ç ”ç©¶åŠªåŠ›è¡¨æ˜ï¼Œè§†è§‰Transformerçš„å†³ç­–è¿‡ç¨‹ä¸­çš„æ³¨æ„åŠ›åœ°å›¾æœ‰å¯èƒ½é€šè¿‡è¯†åˆ«å½±å“é¢„æµ‹çš„åŒºåŸŸæ¥è§£å†³è§£é‡Šæ€§é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½¿ç”¨è‡ªç›‘ç£å­¦ä¹ è¿›è¡Œé¢„è®­ç»ƒçš„æ¨¡å‹ä¸­ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å°†æ³¨æ„åŠ›åœ°å›¾çš„è§†è§‰è§£é‡Šä¸å…¶ä»–å¸¸ç”¨äºåŒ»å­¦å½±åƒé—®é¢˜çš„æ–¹æ³•è¿›è¡Œæ¯”è¾ƒã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†å››ä¸ªä¸åŒçš„åŒ»å­¦å½±åƒæ•°æ®é›†ï¼Œæ¶‰åŠï¼ˆ1ï¼‰ç»“è‚ æ¯è‚‰ã€ï¼ˆ2ï¼‰ä¹³æˆ¿è‚¿ç˜¤ã€ï¼ˆ3ï¼‰é£Ÿé“ç‚ç—‡å’Œï¼ˆ4ï¼‰éª¨æŠ˜åŠç¡¬ä»¶æ¤å…¥ç‰©çš„è¯†åˆ«ã€‚é€šè¿‡åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè¿›è¡Œå„ç§æœ‰ç›‘ç£å’Œè‡ªç›‘ç£é¢„è®­ç»ƒçš„ViTå®éªŒï¼Œæˆ‘ä»¬å‘ç°å°½ç®¡åœ¨æŸäº›æ¡ä»¶ä¸‹ï¼Œæ³¨æ„åŠ›åœ°å›¾æ˜¾ç¤ºå‡ºå‰æ™¯å¹¶åœ¨ä¸€èˆ¬è§£é‡Šä¸Šè¶…è¿‡äº†GradCAMï¼Œä½†å®ƒä»¬è¿˜æ˜¯è¢«ç‰¹å®šäºå˜å‹å™¨çš„è§£é‡Šæ–¹æ³•æ‰€è¶…è¶Šã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæ³¨æ„åŠ›åœ°å›¾ä½œä¸ºè§£é‡Šæ–¹æ³•çš„æœ‰æ•ˆæ€§æ˜¯ä¾èµ–äºå…·ä½“æƒ…å¢ƒçš„ï¼Œå¹¶ä¸”å¯èƒ½å—åˆ°é™åˆ¶ï¼Œå› ä¸ºå®ƒä»¬å¹¶ä¸èƒ½å§‹ç»ˆæä¾›å…¨é¢çš„è§è§£ä»¥ä¾›åšå‡ºç¨³å¥çš„åŒ»ç–—å†³ç­–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09535v1">PDF</a> Accepted for publication in MICCAI 2024 Workshop on Interpretability   of Machine Intelligence in Medical Image Computing (iMIMIC)</p>
<p><strong>Summary</strong></p>
<p>ViTsåœ¨åŒ»ç–—æˆåƒé—®é¢˜ä¸­å±•ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†é¢ä¸´ä¸CNNç›¸åŒçš„è§£é‡Šæ€§é—®é¢˜ã€‚è¿‘æœŸç ”ç©¶å°è¯•é€šè¿‡æ³¨æ„åŠ›å›¾ï¼ˆä½œä¸ºViTså†³ç­–è¿‡ç¨‹çš„ä¸€éƒ¨åˆ†ï¼‰è§£å†³è¯¥é—®é¢˜ï¼Œèƒ½è¯†åˆ«å½±å“é¢„æµ‹çš„åŒºåŸŸã€‚æœ¬æ–‡å¯¹æ³¨æ„åŠ›å›¾å’Œå…¶ä»–åŒ»ç–—æˆåƒé—®é¢˜è§£é‡Šæ–¹æ³•è¿›è¡Œå¯¹æ¯”ã€‚å®éªŒæ¶‰åŠå››ä¸ªæ•°æ®é›†ï¼Œæ¶µç›–ä¸åŒåŒ»å­¦æƒ…å†µã€‚ç ”ç©¶å‘ç°ï¼Œè™½ç„¶æ³¨æ„åŠ›å›¾åœ¨æŸäº›æ¡ä»¶ä¸‹è¡¨ç°è‰¯å¥½ä¸”æ€»ä½“è¶…è¶ŠGradCAMçš„è§£é‡Šæ€§ï¼Œä½†å®ƒä»¬ä»è¢«ç‰¹å®šäºå˜å‹å™¨çš„è§£é‡Šæ–¹æ³•æ‰€è¶…è¶Šã€‚è¿™è¡¨æ˜æ³¨æ„åŠ›å›¾çš„è§£é‡Šæ•ˆæœå…·æœ‰ä¸Šä¸‹æ–‡ä¾èµ–æ€§ï¼Œå¯èƒ½æ— æ³•æä¾›å…¨é¢è€Œç¨³å¥çš„åŒ»ç–—å†³ç­–æ‰€éœ€æ´å¯Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision Transformers (ViTs) åœ¨åŒ»ç–—æˆåƒé—®é¢˜ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†è§£é‡Šæ€§æ–¹é¢ä»éœ€æ”¹è¿›ã€‚</li>
<li>æ³¨æ„åŠ›å›¾æ˜¯ä¸€ç§è§£å†³ViTsè§£é‡Šæ€§é—®é¢˜çš„æ–¹æ³•ï¼Œå¯è¯†åˆ«å½±å“é¢„æµ‹çš„åŒºåŸŸã€‚</li>
<li>åœ¨å››ä¸ªä¸åŒåŒ»ç–—æ•°æ®é›†ä¸Šçš„å®éªŒæ˜¾ç¤ºï¼Œæ³¨æ„åŠ›å›¾åœ¨æŸäº›æ¡ä»¶ä¸‹è¡¨ç°è‰¯å¥½ä½†ä¸å¤Ÿå…¨é¢ã€‚</li>
<li>æ³¨æ„åŠ›å›¾çš„è§£é‡Šæ•ˆæœå…·æœ‰ä¸Šä¸‹æ–‡ä¾èµ–æ€§ï¼Œå¯èƒ½æ— æ³•æä¾›å…¨é¢çš„æ´å¯Ÿä»¥ä¾›ç¨³å¥åŒ»ç–—å†³ç­–ã€‚</li>
<li>ä¸å…¶ä»–å¸¸è§åŒ»ç–—æˆåƒè§£é‡Šæ–¹æ³•ç›¸æ¯”ï¼Œæ³¨æ„åŠ›å›¾åœ¨æŸäº›æƒ…å†µä¸‹å¯èƒ½ä¸å¦‚ç‰¹å®šäºå˜å‹å™¨çš„è§£é‡Šæ–¹æ³•æœ‰æ•ˆã€‚</li>
<li>å°½ç®¡æ³¨æ„åŠ›å›¾åœ¨æŸäº›æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†ä»éœ€è¿›ä¸€æ­¥ç ”ç©¶ä»¥æ”¹è¿›å…¶åœ¨åŒ»ç–—å›¾åƒè§£é‡Šä¸­çš„æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09535">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-175035fbd831834046c37eab15133751.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6b57fc0a81ca93720ff1db3a2e06760c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1541c158a0579febbe18704599927e6f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-505908cc84dd4111c5b39d1bc8855686.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-09ec19c8400c8e91f81056a8933f9ebf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b6246da52cecb16ccef50e84cbc16e9.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="ForAug-Recombining-Foregrounds-and-Backgrounds-to-Improve-Vision-Transformer-Training-with-Bias-Mitigation"><a href="#ForAug-Recombining-Foregrounds-and-Backgrounds-to-Improve-Vision-Transformer-Training-with-Bias-Mitigation" class="headerlink" title="ForAug: Recombining Foregrounds and Backgrounds to Improve Vision   Transformer Training with Bias Mitigation"></a>ForAug: Recombining Foregrounds and Backgrounds to Improve Vision   Transformer Training with Bias Mitigation</h2><p><strong>Authors:Tobias Christian Nauen, Brian Moser, Federico Raue, Stanislav Frolov, Andreas Dengel</strong></p>
<p>Transformers, particularly Vision Transformers (ViTs), have achieved state-of-the-art performance in large-scale image classification. However, they often require large amounts of data and can exhibit biases that limit their robustness and generalizability. This paper introduces ForAug, a novel data augmentation scheme that addresses these challenges and explicitly includes inductive biases, which commonly are part of the neural network architecture, into the training data. ForAug is constructed by using pretrained foundation models to separate and recombine foreground objects with different backgrounds, enabling fine-grained control over image composition during training. It thus increases the data diversity and effective number of training samples. We demonstrate that training on ForNet, the application of ForAug to ImageNet, significantly improves the accuracy of ViTs and other architectures by up to 4.5 percentage points (p.p.) on ImageNet and 7.3 p.p. on downstream tasks. Importantly, ForAug enables novel ways of analyzing model behavior and quantifying biases. Namely, we introduce metrics for background robustness, foreground focus, center bias, and size bias and show that training on ForNet substantially reduces these biases compared to training on ImageNet. In summary, ForAug provides a valuable tool for analyzing and mitigating biases, enabling the development of more robust and reliable computer vision models. Our code and dataset are publicly available at <a target="_blank" rel="noopener" href="https://github.com/tobna/ForAug">https://github.com/tobna/ForAug</a>. </p>
<blockquote>
<p>Transformerï¼Œç‰¹åˆ«æ˜¯è§†è§‰Transformerï¼ˆViTï¼‰ï¼Œåœ¨å¤§è§„æ¨¡å›¾åƒåˆ†ç±»æ–¹é¢å·²ç»è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå®ƒä»¬é€šå¸¸éœ€è¦å¤§é‡çš„æ•°æ®ï¼Œå¹¶å¯èƒ½è¡¨ç°å‡ºé™åˆ¶å…¶é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›çš„åè§ã€‚æœ¬æ–‡é’ˆå¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹æ•°æ®å¢å¼ºæ–¹æ¡ˆForAugï¼Œå®ƒå°†å½’çº³åè§ï¼ˆé€šå¸¸ä½œä¸ºç¥ç»ç½‘ç»œæ¶æ„çš„ä¸€éƒ¨åˆ†ï¼‰æ˜¾å¼åœ°çº³å…¥è®­ç»ƒæ•°æ®ä¸­ã€‚ForAugé€šè¿‡ä½¿ç”¨é¢„è®­ç»ƒçš„åŸºç¡€æ¨¡å‹æ¥åˆ†ç¦»å’Œé‡æ–°ç»„åˆå‰æ™¯ç‰©ä½“ä¸ä¸åŒçš„èƒŒæ™¯ï¼Œå®ç°å¯¹è®­ç»ƒè¿‡ç¨‹ä¸­å›¾åƒç»„æˆçš„ç²¾ç»†æ§åˆ¶ï¼Œä»è€Œå¢åŠ äº†æ•°æ®å¤šæ ·æ€§å’Œæœ‰æ•ˆçš„è®­ç»ƒæ ·æœ¬æ•°é‡ã€‚æˆ‘ä»¬è¯æ˜ï¼Œåœ¨ForNetä¸Šåº”ç”¨ForAugå¯¹ImageNetè¿›è¡Œè®­ç»ƒï¼Œæ˜¾è‘—æé«˜äº†ViTå’Œå…¶ä»–æ¶æ„çš„å‡†ç¡®æ€§ï¼Œåœ¨ImageNetä¸Šæé«˜äº†é«˜è¾¾4.5ä¸ªç™¾åˆ†ç‚¹ï¼ˆp.p.ï¼‰ï¼Œåœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šæé«˜äº†7.3ä¸ªç™¾åˆ†ç‚¹ã€‚é‡è¦çš„æ˜¯ï¼ŒForAugæä¾›äº†æ–°çš„åˆ†ææ¨¡å‹è¡Œä¸ºå’Œé‡åŒ–åè§çš„æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†èƒŒæ™¯ç¨³å¥æ€§ã€å‰æ™¯ç„¦ç‚¹ã€ä¸­å¿ƒåè§å’Œå¤§å°åè§çš„æŒ‡æ ‡ï¼Œå¹¶è¯æ˜ä¸åœ¨ImageNetä¸Šè¿›è¡Œè®­ç»ƒç›¸æ¯”ï¼Œåœ¨ForNetä¸Šè¿›è¡Œè®­ç»ƒå¯ä»¥å¤§å¤§å‡å°‘è¿™äº›åè§ã€‚æ€»ä¹‹ï¼ŒForAugä¸ºåˆ†æå’Œç¼“è§£åè§æä¾›äº†æœ‰ä»·å€¼çš„å·¥å…·ï¼Œä½¿å¼€å‘æ›´ç¨³å¥ã€æ›´å¯é çš„è®¡ç®—æœºè§†è§‰æ¨¡å‹æˆä¸ºå¯èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/tobna/ForAug%E5%85%AC%E5%BC%80%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/tobna/ForAugå…¬å¼€è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09399v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºForAugçš„æ–°å‹æ•°æ®å¢å¼ºæ–¹æ¡ˆï¼Œæ—¨åœ¨è§£å†³Transformerï¼Œå°¤å…¶æ˜¯Vision Transformersï¼ˆViTsï¼‰åœ¨å¤§è§„æ¨¡å›¾åƒåˆ†ç±»ä¸­é¢ä¸´çš„æŒ‘æˆ˜ã€‚ForAugé€šè¿‡å¼•å…¥å½’çº³åç½®ï¼Œå¢åŠ æ•°æ®å¤šæ ·æ€§å’Œæœ‰æ•ˆè®­ç»ƒæ ·æœ¬æ•°é‡ï¼Œæé«˜äº†æ¨¡å‹çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒForAugè¿˜æä¾›äº†ä¸€ç§åˆ†æå’Œç¼“è§£æ¨¡å‹åè§çš„æ–°æ–¹æ³•ã€‚è®­ç»ƒåœ¨ForNetï¼ˆForAugåœ¨ImageNetä¸Šçš„åº”ç”¨ï¼‰ä¸Šçš„ViTså’Œå…¶ä»–æ¶æ„åœ¨ImageNetä¸Šçš„å‡†ç¡®ç‡æé«˜äº†é«˜è¾¾4.5ä¸ªç™¾åˆ†ç‚¹ï¼ˆp.p.ï¼‰ï¼Œå¹¶åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šæé«˜äº†7.3ä¸ªç™¾åˆ†ç‚¹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ForAugæ˜¯ä¸€ç§æ–°å‹æ•°æ®å¢å¼ºæ–¹æ¡ˆï¼Œæ—¨åœ¨è§£å†³Vision Transformersï¼ˆViTsï¼‰é¢ä¸´çš„æŒ‘æˆ˜ã€‚</li>
<li>ForAugé€šè¿‡å¼•å…¥å½’çº³åç½®æé«˜æ¨¡å‹çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ForAugåˆ©ç”¨é¢„è®­ç»ƒçš„åŸºç¡€æ¨¡å‹æ¥åˆ†ç¦»å’Œé‡ç»„å‰æ™¯ç‰©ä½“ä¸ä¸åŒèƒŒæ™¯ï¼Œä»è€Œå¢åŠ æ•°æ®å¤šæ ·æ€§å’Œæœ‰æ•ˆè®­ç»ƒæ ·æœ¬æ•°é‡ã€‚</li>
<li>è®­ç»ƒåœ¨ForNetä¸Šçš„æ¨¡å‹åœ¨ImageNetä¸Šçš„å‡†ç¡®ç‡æ˜¾è‘—æé«˜ã€‚</li>
<li>ForAugæä¾›åˆ†æå’Œç¼“è§£æ¨¡å‹åè§çš„æ–°æ–¹æ³•ã€‚</li>
<li>ForAugå¼•å…¥èƒŒæ™¯ç¨³å¥æ€§ã€å‰æ™¯ç„¦ç‚¹ã€ä¸­å¿ƒåè§å’Œå¤§å°åè§çš„åº¦é‡æ ‡å‡†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09399">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-81d52c0aa606d460e31558c32d630f67.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-02bd429357cac60d2b97603fc69098f5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d96a18f6fa4aad9ec606d192d2e9d395.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d8393fbce1e4a214919579870284026.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0c842daa42dcaf29346cbe1f6b99876e.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Prompt-to-Restore-Restore-to-Prompt-Cyclic-Prompting-for-Universal-Adverse-Weather-Removal"><a href="#Prompt-to-Restore-Restore-to-Prompt-Cyclic-Prompting-for-Universal-Adverse-Weather-Removal" class="headerlink" title="Prompt to Restore, Restore to Prompt: Cyclic Prompting for Universal   Adverse Weather Removal"></a>Prompt to Restore, Restore to Prompt: Cyclic Prompting for Universal   Adverse Weather Removal</h2><p><strong>Authors:Rongxin Liao, Feng Li, Yanyan Wei, Zenglin Shi, Le Zhang, Huihui Bai, Meng Wang</strong></p>
<p>Universal adverse weather removal (UAWR) seeks to address various weather degradations within a unified framework. Recent methods are inspired by prompt learning using pre-trained vision-language models (e.g., CLIP), leveraging degradation-aware prompts to facilitate weather-free image restoration, yielding significant improvements. In this work, we propose CyclicPrompt, an innovative cyclic prompt approach designed to enhance the effectiveness, adaptability, and generalizability of UAWR. CyclicPrompt Comprises two key components: 1) a composite context prompt that integrates weather-related information and context-aware representations into the network to guide restoration. This prompt differs from previous methods by marrying learnable input-conditional vectors with weather-specific knowledge, thereby improving adaptability across various degradations. 2) The erase-and-paste mechanism, after the initial guided restoration, substitutes weather-specific knowledge with constrained restoration priors, inducing high-quality weather-free concepts into the composite prompt to further fine-tune the restoration process. Therefore, we can form a cyclic â€œPrompt-Restore-Promptâ€ pipeline that adeptly harnesses weather-specific knowledge, textual contexts, and reliable textures. Extensive experiments on synthetic and real-world datasets validate the superior performance of CyclicPrompt. The code is available at: <a target="_blank" rel="noopener" href="https://github.com/RongxinL/CyclicPrompt">https://github.com/RongxinL/CyclicPrompt</a>. </p>
<blockquote>
<p>æ™®éæ¶åŠ£å¤©æ°”å»é™¤ï¼ˆUAWRï¼‰æ—¨åœ¨åœ¨ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶å†…è§£å†³å„ç§å¤©æ°”é€€åŒ–é—®é¢˜ã€‚è¿‘æœŸçš„æ–¹æ³•å—åˆ°é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆä¾‹å¦‚CLIPï¼‰çš„æç¤ºå­¦ä¹ çš„å¯å‘ï¼Œåˆ©ç”¨é€€åŒ–æ„ŸçŸ¥æç¤ºæ¥ä¿ƒè¿›æ— å¤©æ°”å›¾åƒæ¢å¤ï¼Œå–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†CyclicPromptï¼Œè¿™æ˜¯ä¸€ç§åˆ›æ–°çš„å¾ªç¯æç¤ºæ–¹æ³•ï¼Œæ—¨åœ¨æé«˜UAWRçš„æœ‰æ•ˆæ€§ã€é€‚åº”æ€§å’Œé€šç”¨æ€§ã€‚CyclicPromptåŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼š1ï¼‰å¤åˆä¸Šä¸‹æ–‡æç¤ºï¼Œå®ƒå°†å¤©æ°”ç›¸å…³ä¿¡æ¯å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥è¡¨ç¤ºé›†æˆåˆ°ç½‘ç»œä¸­ï¼Œä»¥æŒ‡å¯¼æ¢å¤ã€‚æ­¤æç¤ºä¸ä»¥å‰çš„æ–¹æ³•ä¸åŒï¼Œå®ƒå°†å¯å­¦ä¹ çš„è¾“å…¥æ¡ä»¶å‘é‡ä¸ç‰¹å®šå¤©æ°”çŸ¥è¯†ç›¸ç»“åˆï¼Œä»è€Œæé«˜äº†åœ¨å„ç§é€€åŒ–æƒ…å†µä¸‹çš„é€‚åº”æ€§ã€‚2ï¼‰æ“¦é™¤å’Œç²˜è´´æœºåˆ¶ï¼Œåœ¨åˆå§‹çš„æŒ‡å¯¼æ¢å¤ä¹‹åï¼Œç”¨å—é™åˆ¶çš„ä¿®å¤å…ˆéªŒæ›¿æ¢ç‰¹å®šå¤©æ°”çš„çŸ¥è¯†ï¼Œå°†é«˜è´¨é‡çš„æ— å¤©æ°”æ¦‚å¿µå¼•å…¥å¤åˆæç¤ºä¸­ï¼Œä»¥è¿›ä¸€æ­¥å¾®è°ƒæ¢å¤è¿‡ç¨‹ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥å½¢æˆä¸€ä¸ªå¾ªç¯çš„â€œæç¤º-æ¢å¤-æç¤ºâ€ç®¡é“ï¼Œå·§å¦™åœ°åˆ©ç”¨ç‰¹å®šå¤©æ°”çš„çŸ¥è¯†ã€æ–‡æœ¬ä¸Šä¸‹æ–‡å’Œå¯é çš„çº¹ç†ã€‚åœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒéªŒè¯äº†CyclicPromptçš„ä¼˜è¶Šæ€§èƒ½ã€‚ä»£ç å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/RongxinL/CyclicPrompt%E3%80%82">https://github.com/RongxinL/CyclicPromptã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09013v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ‘˜è¦é’ˆå¯¹æ™®éæ¶åŠ£å¤©æ°”å»é™¤ï¼ˆUAWRï¼‰æŠ€æœ¯çš„æ–°å‘å±•è¿›è¡Œäº†æ¦‚è¿°ã€‚æ–‡ç« æå‡ºäº†CyclicPromptæ–¹æ³•ï¼ŒåŒ…å«å¤åˆä¸Šä¸‹æ–‡æç¤ºå’Œæ“¦é™¤ç²˜è´´æœºåˆ¶ä¸¤å¤§æ ¸å¿ƒç»„ä»¶ï¼Œæ—¨åœ¨æé«˜UAWRçš„æœ‰æ•ˆæ€§ã€é€‚åº”æ€§å’Œæ³›åŒ–æ€§ã€‚è¯¥æ–¹æ³•æ•´åˆå¤©æ°”ç›¸å…³ä¿¡æ¯å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥è¡¨ç¤ºï¼Œå¼•å¯¼å›¾åƒæ¢å¤ï¼Œå¹¶é€šè¿‡å¾ªç¯â€œæç¤º-æ¢å¤-å†æç¤ºâ€ç®¡é“å®ç°é«˜æ€§èƒ½å¤©æ°”æ— å…³çš„æ¦‚å¿µå¼•å…¥å’Œæ¢å¤è¿‡ç¨‹å¾®è°ƒã€‚å·²åœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒéªŒè¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CyclicPromptæ˜¯ä¸€ç§é’ˆå¯¹æ™®éæ¶åŠ£å¤©æ°”å»é™¤ï¼ˆUAWRï¼‰çš„å¾ªç¯æç¤ºæ–¹æ³•ï¼Œæ—¨åœ¨æé«˜UAWRæŠ€æœ¯çš„æ•ˆæœã€é€‚åº”æ€§å’Œæ³›åŒ–æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•åŒ…å«ä¸¤å¤§æ ¸å¿ƒç»„ä»¶ï¼šå¤åˆä¸Šä¸‹æ–‡æç¤ºå’Œæ“¦é™¤ç²˜è´´æœºåˆ¶ã€‚</li>
<li>å¤åˆä¸Šä¸‹æ–‡æç¤ºç»“åˆäº†å¤©æ°”ç›¸å…³ä¿¡æ¯å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥è¡¨ç¤ºï¼Œä»¥å¼•å¯¼å›¾åƒæ¢å¤ã€‚</li>
<li>æ“¦é™¤ç²˜è´´æœºåˆ¶åœ¨åˆå§‹å¼•å¯¼æ¢å¤åï¼Œç”¨å—é™çš„æ¢å¤å…ˆéªŒæ›¿æ¢å¤©æ°”ç‰¹å®šçŸ¥è¯†ï¼Œå°†é«˜è´¨é‡çš„æ— å¤©æ°”æ¦‚å¿µå¼•å…¥å¤åˆæç¤ºï¼Œä»¥è¿›ä¸€æ­¥å¾®è°ƒæ¢å¤è¿‡ç¨‹ã€‚</li>
<li>CyclicPromptå½¢æˆäº†ä¸€ä¸ªå¾ªç¯çš„â€œæç¤º-æ¢å¤-å†æç¤ºâ€ç®¡é“ï¼Œå……åˆ†åˆ©ç”¨å¤©æ°”ç‰¹å®šçŸ¥è¯†ã€æ–‡æœ¬ä¸Šä¸‹æ–‡å’Œå¯é çº¹ç†ã€‚</li>
<li>åœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒï¼ŒéªŒè¯äº†CyclicPromptæ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09013">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-19576b1c1c2857c66be916f03de0b047.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ccb61767de73013f94522a8ce2f08cd1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9e0a0ee2314f43bf82343d74eab0c35f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ac551bffd18e4e60ba6288d24dafbee8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4bd77aeff223f5e3fe0b1aa4897c427e.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Accelerate-3D-Object-Detection-Models-via-Zero-Shot-Attention-Key-Pruning"><a href="#Accelerate-3D-Object-Detection-Models-via-Zero-Shot-Attention-Key-Pruning" class="headerlink" title="Accelerate 3D Object Detection Models via Zero-Shot Attention Key   Pruning"></a>Accelerate 3D Object Detection Models via Zero-Shot Attention Key   Pruning</h2><p><strong>Authors:Lizhen Xu, Xiuxiu Bai, Xiaojun Jia, Jianwu Fang, Shanmin Pang</strong></p>
<p>Query-based methods with dense features have demonstrated remarkable success in 3D object detection tasks. However, the computational demands of these models, particularly with large image sizes and multiple transformer layers, pose significant challenges for efficient running on edge devices. Existing pruning and distillation methods either need retraining or are designed for ViT models, which are hard to migrate to 3D detectors. To address this issue, we propose a zero-shot runtime pruning method for transformer decoders in 3D object detection models. The method, termed tgGBC (trim keys gradually Guided By Classification scores), systematically trims keys in transformer modules based on their importance. We expand the classification score to multiply it with the attention map to get the importance score of each key and then prune certain keys after each transformer layer according to their importance scores. Our method achieves a 1.99x speedup in the transformer decoder of the latest ToC3D model, with only a minimal performance loss of less than 1%. Interestingly, for certain models, our method even enhances their performance. Moreover, we deploy 3D detectors with tgGBC on an edge device, further validating the effectiveness of our method. The code can be found at <a target="_blank" rel="noopener" href="https://github.com/iseri27/tg_gbc">https://github.com/iseri27/tg_gbc</a>. </p>
<blockquote>
<p>åŸºäºæŸ¥è¯¢çš„æ–¹æ³•å’Œå¯†é›†ç‰¹å¾åœ¨3Dç›®æ ‡æ£€æµ‹ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹çš„è®¡ç®—éœ€æ±‚ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤§å›¾åƒå°ºå¯¸å’Œå¤šä¸ªè½¬æ¢å™¨å±‚æ—¶ï¼Œå¯¹äºåœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šè¿›è¡Œé«˜æ•ˆè¿è¡Œæ„æˆäº†é‡å¤§æŒ‘æˆ˜ã€‚ç°æœ‰çš„å‰ªæå’Œè’¸é¦æ–¹æ³•éœ€è¦é‡è®­ï¼Œæˆ–è€…ä¸“ä¸ºViTæ¨¡å‹è®¾è®¡ï¼Œå¾ˆéš¾è¿ç§»åˆ°3Dæ£€æµ‹å™¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äº3Dç›®æ ‡æ£€æµ‹æ¨¡å‹ä¸­è½¬æ¢å™¨è§£ç å™¨çš„é›¶æ—¶è¿è¡Œå‰ªææ–¹æ³•ã€‚è¯¥æ–¹æ³•è¢«ç§°ä¸ºtgGBCï¼ˆåˆ†ç±»åˆ†æ•°é€æ­¥å¼•å¯¼ä¿®å‰ªé”®ï¼‰ï¼Œå®ƒæ ¹æ®é‡è¦æ€§ç³»ç»Ÿåœ°ä¿®å‰ªè½¬æ¢å™¨æ¨¡å—ä¸­çš„é”®ã€‚æˆ‘ä»¬å°†åˆ†ç±»åˆ†æ•°æ‰©å±•åˆ°ä¸æ³¨æ„åŠ›å›¾ç›¸ä¹˜ï¼Œä»¥è·å¾—æ¯ä¸ªé”®çš„é‡è¦æ€§åˆ†æ•°ï¼Œç„¶åæ ¹æ®å…¶é‡è¦æ€§åˆ†æ•°åœ¨æ¯ä¸ªè½¬æ¢å™¨å±‚ä¹‹åä¿®å‰ªæŸäº›é”®ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨æœ€æ–°çš„ToC3Dæ¨¡å‹çš„è½¬æ¢å™¨è§£ç å™¨ä¸­å®ç°äº†1.99å€çš„é€Ÿåº¦æå‡ï¼Œå¹¶ä¸”æ€§èƒ½æŸå¤±å¾®ä¹å…¶å¾®ï¼Œä¸åˆ°1%ã€‚æœ‰è¶£çš„æ˜¯ï¼Œå¯¹äºæŸäº›æ¨¡å‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç”šè‡³æé«˜äº†å®ƒä»¬çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²å¸¦æœ‰tgGBCçš„3Dæ£€æµ‹å™¨ï¼Œè¿›ä¸€æ­¥éªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/iseri27/tg_gbc">https://github.com/iseri27/tg_gbc</a>ä¸­æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08101v2">PDF</a> The code can be found at <a target="_blank" rel="noopener" href="https://github.com/iseri27/tg_gbc">https://github.com/iseri27/tg_gbc</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºtgGBCçš„é›¶è¿è¡Œæ—¶ä¿®å‰ªæ–¹æ³•ï¼Œç”¨äº3Dç›®æ ‡æ£€æµ‹æ¨¡å‹ä¸­çš„è½¬æ¢å™¨è§£ç å™¨ã€‚è¯¥æ–¹æ³•åŸºäºåˆ†ç±»åˆ†æ•°ä¸æ³¨æ„åŠ›å›¾çš„ä¹˜ç§¯æ¥ç¡®å®šé”®çš„é‡è¦æ€§ï¼Œå¹¶é€å±‚ä¿®å‰ªä¸é‡è¦çš„é”®ã€‚æ­¤æ–¹æ³•å®ç°äº†æœ€æ–°ToC3Dæ¨¡å‹çš„å˜å‹å™¨è§£ç å™¨é€Ÿåº¦æå‡1.99å€ï¼Œæ€§èƒ½æŸå¤±ä»…ä¸ºä¸åˆ°ç™¾åˆ†ä¹‹ä¸€ã€‚åŒæ—¶ï¼Œéƒ¨ç½²äº†å¸¦æœ‰tgGBCçš„3Dæ£€æµ‹å™¨åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æŸ¥è¯¢æ–¹æ³•ç»“åˆå¯†é›†ç‰¹å¾åœ¨3Dç›®æ ‡æ£€æµ‹ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†è®¡ç®—éœ€æ±‚å¤§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤§å›¾åƒå’Œå¤šå±‚è½¬æ¢å™¨æ—¶ã€‚</li>
<li>ç°æœ‰ä¿®å‰ªå’Œè’¸é¦æ–¹æ³•éœ€è¦é‡æ–°è®­ç»ƒæˆ–ä¸“ä¸ºViTæ¨¡å‹è®¾è®¡ï¼Œéš¾ä»¥è¿ç§»åˆ°3Dæ£€æµ‹å™¨ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºtgGBCçš„é›¶è¿è¡Œæ—¶ä¿®å‰ªæ–¹æ³•ï¼Œé’ˆå¯¹è½¬æ¢å™¨è§£ç å™¨åœ¨3Dç›®æ ‡æ£€æµ‹æ¨¡å‹ä¸­çš„ä½¿ç”¨ã€‚</li>
<li>tgGBCåŸºäºåˆ†ç±»åˆ†æ•°å’Œæ³¨æ„åŠ›å›¾æ¥è¯„ä¼°æ¯ä¸ªé”®çš„é‡è¦æ€§ï¼Œç„¶åè¿›è¡Œä¿®å‰ªã€‚</li>
<li>æ–¹æ³•å®ç°äº†ToC3Dæ¨¡å‹çš„å˜å‹å™¨è§£ç å™¨é€Ÿåº¦æå‡1.99å€ï¼ŒåŒæ—¶æ€§èƒ½æŸå¤±å¾®å°ã€‚</li>
<li>åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²äº†ä½¿ç”¨tgGBCçš„3Dæ£€æµ‹å™¨ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08101">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-20ad4553d41d28e9178bb94eb6a9267b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fcd960e8fe7aab4b760823fb766eb29e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b6e8d1be338c3e06befe5e87361353df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff785287fc071e6f8215097c93de42d9.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Filter-Images-First-Generate-Instructions-Later-Pre-Instruction-Data-Selection-for-Visual-Instruction-Tuning"><a href="#Filter-Images-First-Generate-Instructions-Later-Pre-Instruction-Data-Selection-for-Visual-Instruction-Tuning" class="headerlink" title="Filter Images First, Generate Instructions Later: Pre-Instruction Data   Selection for Visual Instruction Tuning"></a>Filter Images First, Generate Instructions Later: Pre-Instruction Data   Selection for Visual Instruction Tuning</h2><p><strong>Authors:Bardia Safaei, Faizan Siddiqui, Jiacong Xu, Vishal M. Patel, Shao-Yuan Lo</strong></p>
<p>Visual instruction tuning (VIT) for large vision-language models (LVLMs) requires training on expansive datasets of image-instruction pairs, which can be costly. Recent efforts in VIT data selection aim to select a small subset of high-quality image-instruction pairs, reducing VIT runtime while maintaining performance comparable to full-scale training. However, a major challenge often overlooked is that generating instructions from unlabeled images for VIT is highly expensive. Most existing VIT datasets rely heavily on human annotations or paid services like the GPT API, which limits users with constrained resources from creating VIT datasets for custom applications. To address this, we introduce Pre-Instruction Data Selection (PreSel), a more practical data selection paradigm that directly selects the most beneficial unlabeled images and generates instructions only for the selected images. PreSel first estimates the relative importance of each vision task within VIT datasets to derive task-wise sampling budgets. It then clusters image features within each task, selecting the most representative images with the budget. This approach reduces computational overhead for both instruction generation during VIT data formation and LVLM fine-tuning. By generating instructions for only 15% of the images, PreSel achieves performance comparable to full-data VIT on the LLaVA-1.5 and Vision-Flan datasets. The link to our project page: <a target="_blank" rel="noopener" href="https://bardisafa.github.io/PreSel">https://bardisafa.github.io/PreSel</a> </p>
<blockquote>
<p>è§†è§‰æŒ‡ä»¤è°ƒæ•´ï¼ˆVITï¼‰å¯¹äºå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰éœ€è¦åœ¨åºå¤§çš„å›¾åƒæŒ‡ä»¤å¯¹æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè¿™å¯èƒ½ä¼šå¾ˆæ˜‚è´µã€‚æœ€è¿‘å…³äºVITæ•°æ®é€‰æ‹©çš„åŠªåŠ›æ—¨åœ¨é€‰æ‹©é«˜è´¨é‡å›¾åƒæŒ‡ä»¤å¯¹çš„å­é›†ï¼Œåœ¨ä¿æŒä¸å…¨é¢è®­ç»ƒç›¸å½“çš„æ€§èƒ½çš„åŒæ—¶ï¼Œå‡å°‘VITçš„è¿è¡Œæ—¶é—´ã€‚ç„¶è€Œï¼Œç»å¸¸è¢«å¿½è§†çš„ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜æ˜¯ï¼Œä»æ— æ ‡ç­¾çš„å›¾åƒä¸­ç”ŸæˆVITçš„æŒ‡ä»¤æˆæœ¬éå¸¸é«˜ã€‚å¤§å¤šæ•°ç°æœ‰çš„VITæ•°æ®é›†ä¸¥é‡ä¾èµ–äºäººç±»æ³¨é‡Šæˆ–ä½¿ç”¨å¦‚GPT APIç­‰ä»˜è´¹æœåŠ¡ï¼Œè¿™é™åˆ¶äº†èµ„æºå—é™çš„ç”¨æˆ·ä¸ºå®šåˆ¶åº”ç”¨ç¨‹åºåˆ›å»ºVITæ•°æ®é›†ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†é¢„æŒ‡ä»¤æ•°æ®é€‰æ‹©ï¼ˆPreSelï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ›´å®ç”¨çš„æ•°æ®é€‰æ‹©èŒƒå¼ï¼Œå®ƒç›´æ¥é€‰æ‹©æœ€æœ‰ç›Šçš„æ— æ ‡ç­¾å›¾åƒï¼Œåªä¸ºæ‰€é€‰å›¾åƒç”ŸæˆæŒ‡ä»¤ã€‚PreSelé¦–å…ˆä¼°è®¡VITæ•°æ®é›†ä¸­æ¯ä¸ªè§†è§‰ä»»åŠ¡çš„ç›¸å¯¹é‡è¦æ€§ï¼Œä»¥å¾—å‡ºä»»åŠ¡çº§é‡‡æ ·é¢„ç®—ã€‚ç„¶åï¼Œå®ƒåœ¨æ¯ä¸ªä»»åŠ¡å†…å¯¹å›¾åƒç‰¹å¾è¿›è¡Œèšç±»ï¼Œé€‰æ‹©æœ€å…·ä»£è¡¨æ€§çš„å›¾åƒä»¥é€‚åº”é¢„ç®—ã€‚è¿™ç§æ–¹æ³•å‡å°‘äº†VITæ•°æ®å½¢æˆå’ŒLVLMå¾®è°ƒè¿‡ç¨‹ä¸­æŒ‡ä»¤ç”Ÿæˆçš„è®¡ç®—å¼€é”€ã€‚åªä¸º15%çš„å›¾åƒç”ŸæˆæŒ‡ä»¤ï¼ŒPreSelåœ¨LLaVA-1.5å’ŒVision-Flanæ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¸å…¨æ•°æ®VITç›¸å½“ã€‚æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢é“¾æ¥ï¼š<a target="_blank" rel="noopener" href="https://bardisafa.github.io/PreSel">https://bardisafa.github.io/PreSel</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07591v1">PDF</a> Accepted at Computer Vision and Pattern Recognition Conference (CVPR)   2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„è§†è§‰æŒ‡ä»¤å¾®è°ƒï¼ˆVITï¼‰çš„æŒ‘æˆ˜ã€‚ä¸ºäº†é™ä½æ•°æ®æ”¶é›†æˆæœ¬ï¼Œå½“å‰ç ”ç©¶é›†ä¸­åœ¨é€‰æ‹©é«˜è´¨é‡å›¾åƒæŒ‡ä»¤å¯¹æ•°æ®å­é›†ä¸Šã€‚ä½†é—®é¢˜åœ¨äºç”Ÿæˆå›¾åƒæŒ‡ä»¤éœ€è¦æ ‡æ³¨ï¼Œè€—è´¹äººåŠ›æˆ–å¤§é‡é‡‘é’±ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†Pre-Instruction Data Selectionï¼ˆPreSelï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€‰æ‹©æœ€æœ‰ç›Šçš„æ— æ ‡ç­¾å›¾åƒï¼Œåªä¸ºé€‰ä¸­çš„å›¾åƒç”ŸæˆæŒ‡ä»¤ã€‚PreSelé€šè¿‡ä¼°è®¡æ¯ä¸ªè§†è§‰ä»»åŠ¡çš„é‡è¦æ€§æ¥åˆ¶å®šä»»åŠ¡çº§åˆ«çš„é‡‡æ ·é¢„ç®—ï¼Œç„¶ååœ¨æ¯ä¸ªä»»åŠ¡å†…èšç±»å›¾åƒç‰¹å¾ï¼Œé€‰æ‹©æœ€å…·ä»£è¡¨æ€§çš„å›¾åƒã€‚é€šè¿‡åªä¸ºå›¾åƒçš„15%ç”ŸæˆæŒ‡ä»¤ï¼ŒPreSelåœ¨LLaVA-1.5å’ŒVision-Flanæ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¸å…¨æ•°æ®VITç›¸å½“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VITè®­ç»ƒéœ€è¦å¤§é‡å›¾åƒæŒ‡ä»¤å¯¹æ•°æ®ï¼Œæˆæœ¬é«˜æ˜‚ã€‚</li>
<li>å½“å‰ç ”ç©¶è‡´åŠ›äºé€‰æ‹©é«˜è´¨é‡å›¾åƒæŒ‡ä»¤å¯¹æ•°æ®å­é›†ä»¥é™ä½è®­ç»ƒæˆæœ¬ã€‚</li>
<li>ç”ŸæˆæŒ‡ä»¤éœ€è¦å¤§é‡æ ‡æ³¨æˆ–ä»˜è´¹æœåŠ¡ï¼Œé™åˆ¶äº†èµ„æºå—é™ç”¨æˆ·åˆ›å»ºè‡ªå®šä¹‰VITæ•°æ®é›†çš„èƒ½åŠ›ã€‚</li>
<li>æå‡ºPreSelæ–¹æ³•è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œä¸ºæ— æ ‡ç­¾å›¾åƒé€‰æ‹©æœ€æœ‰ä»·å€¼çš„å›¾åƒç”ŸæˆæŒ‡ä»¤ã€‚</li>
<li>PreSelé¦–å…ˆä¼°è®¡æ¯ä¸ªè§†è§‰ä»»åŠ¡çš„é‡è¦æ€§æ¥åˆ¶å®šé‡‡æ ·é¢„ç®—ã€‚</li>
<li>PreSelé€šè¿‡èšç±»å›¾åƒç‰¹å¾é€‰æ‹©æœ€å…·ä»£è¡¨æ€§çš„å›¾åƒï¼Œé™ä½è®¡ç®—å¼€é”€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07591">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-fa170626bf00ee708fa1e6a942043708.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93ec5db65b826b27d696e3a99ffda4de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e265abccdd703087bfb59e5a7564ce27.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f68ee10db0da8de33097a4f4f36d45f7.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Critic-V-VLM-Critics-Help-Catch-VLM-Errors-in-Multimodal-Reasoning"><a href="#Critic-V-VLM-Critics-Help-Catch-VLM-Errors-in-Multimodal-Reasoning" class="headerlink" title="Critic-V: VLM Critics Help Catch VLM Errors in Multimodal Reasoning"></a>Critic-V: VLM Critics Help Catch VLM Errors in Multimodal Reasoning</h2><p><strong>Authors:Di Zhang, Junxian Li, Jingdi Lei, Xunzhi Wang, Yujie Liu, Zonglin Yang, Jiatong Li, Weida Wang, Suorong Yang, Jianbo Wu, Peng Ye, Wanli Ouyang, Dongzhan Zhou</strong></p>
<p>Vision-language models (VLMs) have shown remarkable advancements in multimodal reasoning tasks. However, they still often generate inaccurate or irrelevant responses due to issues like hallucinated image understandings or unrefined reasoning paths. To address these challenges, we introduce Critic-V, a novel framework inspired by the Actor-Critic paradigm to boost the reasoning capability of VLMs. This framework decouples the reasoning process and critic process by integrating two independent components: the Reasoner, which generates reasoning paths based on visual and textual inputs, and the Critic, which provides constructive critique to refine these paths. In this approach, the Reasoner generates reasoning responses according to text prompts, which can evolve iteratively as a policy based on feedback from the Critic. This interaction process was theoretically driven by a reinforcement learning framework where the Critic offers natural language critiques instead of scalar rewards, enabling more nuanced feedback to boost the Reasonerâ€™s capability on complex reasoning tasks. The Critic model is trained using Direct Preference Optimization (DPO), leveraging a preference dataset of critiques ranked by Rule-based Reward~(RBR) to enhance its critic capabilities. Evaluation results show that the Critic-V framework significantly outperforms existing methods, including GPT-4V, on 5 out of 8 benchmarks, especially regarding reasoning accuracy and efficiency. Combining a dynamic text-based policy for the Reasoner and constructive feedback from the preference-optimized Critic enables a more reliable and context-sensitive multimodal reasoning process. Our approach provides a promising solution to enhance the reliability of VLMs, improving their performance in real-world reasoning-heavy multimodal applications such as autonomous driving and embodied intelligence. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œç”±äºè¯¸å¦‚è™šæ„çš„å›¾åƒç†è§£æˆ–ç²—ç³™çš„æ¨ç†è·¯å¾„ç­‰é—®é¢˜ï¼Œå®ƒä»¬ä»ç„¶ç»å¸¸äº§ç”Ÿä¸å‡†ç¡®æˆ–ä¸ç›¸å…³çš„ååº”ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†Critic-Vï¼Œè¿™æ˜¯ä¸€ä¸ªå—Actor-CriticèŒƒå¼å¯å‘çš„æ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨æå‡VLMsçš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶é€šè¿‡é›†æˆä¸¤ä¸ªç‹¬ç«‹ç»„ä»¶æ¥è§£è€¦æ¨ç†è¿‡ç¨‹å’Œæ‰¹è¯„è¿‡ç¨‹ï¼šReasonerï¼Œå®ƒæ ¹æ®è§†è§‰å’Œæ–‡æœ¬è¾“å…¥ç”Ÿæˆæ¨ç†è·¯å¾„ï¼›ä»¥åŠCriticï¼Œå®ƒæä¾›å»ºè®¾æ€§æ‰¹è¯„ä»¥ä¼˜åŒ–è¿™äº›è·¯å¾„ã€‚åœ¨æ­¤æ–¹æ³•ä¸­ï¼ŒReasoneræ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆæ¨ç†ååº”ï¼Œè¿™äº›ååº”å¯ä»¥åŸºäºæ¥è‡ªCriticçš„åé¦ˆè€Œè¿­ä»£åœ°å‘å±•ä¸ºç­–ç•¥ã€‚è¿™ä¸€äº¤äº’è¿‡ç¨‹æ˜¯ç”±å¼ºåŒ–å­¦ä¹ æ¡†æ¶é©±åŠ¨çš„ï¼Œå…¶ä¸­Criticæä¾›è‡ªç„¶è¯­è¨€æ‰¹è¯„è€Œä¸æ˜¯æ ‡é‡å¥–åŠ±ï¼Œä»è€Œå¯ä»¥æä¾›æ›´å¾®å¦™çš„åé¦ˆï¼Œä»¥æå‡Reasoneråœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šçš„èƒ½åŠ›ã€‚Criticæ¨¡å‹ä½¿ç”¨ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰è¿›è¡Œè®­ç»ƒï¼Œåˆ©ç”¨ç”±åŸºäºè§„åˆ™çš„å¥–åŠ±ï¼ˆRBRï¼‰æ’åçš„è¯„è®ºåå¥½æ•°æ®é›†æ¥å¢å¼ºå…¶æ‰¹è¯„èƒ½åŠ›ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒCritic-Væ¡†æ¶åœ¨8ä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¿‡ç°æœ‰æ–¹æ³•ï¼ŒåŒ…æ‹¬GPT-4Våœ¨å†…ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¨ç†å‡†ç¡®æ€§å’Œæ•ˆç‡æ–¹é¢ã€‚ç»“åˆReasonerçš„åŠ¨æ€åŸºäºæ–‡æœ¬çš„ç­–ç•¥å’Œæ¥è‡ªåå¥½ä¼˜åŒ–Criticçš„å»ºè®¾æ€§åé¦ˆï¼Œå®ç°äº†æ›´å¯é å’Œä¸Šä¸‹æ–‡æ•æ„Ÿçš„å¤šæ¨¡æ€æ¨ç†è¿‡ç¨‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ºæå‡VLMsçš„å¯é æ€§æä¾›äº†æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆï¼Œæ”¹è¿›å…¶åœ¨è‡ªåŠ¨é©¾é©¶å’Œæ™ºèƒ½é›†æˆç­‰ç°å®ä¸–ç•Œæ¨ç†å¯†é›†å‹å¤šæ¨¡æ€åº”ç”¨ä¸­çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.18203v4">PDF</a> 16 pages, 11 figures</p>
<p><strong>Summary</strong><br>      Vision-languageæ¨¡å‹ï¼ˆVLMï¼‰åœ¨å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸­å±•ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œä½†ä»å­˜åœ¨ä¸å‡†ç¡®æˆ–æ— å…³çš„ååº”é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†Critic-Væ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†Actor-Criticç†å¿µæ¥æå‡VLMçš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶æ•´åˆäº†Reasonerå’ŒCriticä¸¤ä¸ªç‹¬ç«‹ç»„ä»¶ï¼Œå‰è€…åŸºäºè§†è§‰å’Œæ–‡æœ¬è¾“å…¥ç”Ÿæˆæ¨ç†è·¯å¾„ï¼Œåè€…æä¾›å»ºè®¾æ€§æ‰¹è¯„ä»¥ä¼˜åŒ–è¿™äº›è·¯å¾„ã€‚Criticæ¨¡å‹é‡‡ç”¨ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰è®­ç»ƒï¼Œåˆ©ç”¨åŸºäºè§„åˆ™çš„å¥–åŠ±ï¼ˆRBRï¼‰å¯¹æ‰¹è¯„è¿›è¡Œæ’åï¼Œä»¥å¢å¼ºå…¶æ‰¹åˆ¤èƒ½åŠ›ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œåœ¨äº”ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒCritic-Væ¡†æ¶åœ¨8ä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¨ç†å‡†ç¡®æ€§å’Œæ•ˆç‡æ–¹é¢ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ºæé«˜VLMçš„å¯é æ€§æä¾›äº†æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆï¼Œæœ‰åŠ©äºæ”¹å–„çœŸå®ä¸–ç•Œå¤šæ¨¡æ€åº”ç”¨ä¸­çš„è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision-language models (VLMs)åœ¨å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸­å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œä½†å­˜åœ¨ä¸å‡†ç¡®æˆ–æ— å…³çš„ååº”é—®é¢˜ã€‚</li>
<li>Critic-Væ¡†æ¶ç»“åˆäº†Actor-Criticç†å¿µï¼Œæ—¨åœ¨æå‡VLMçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>è¯¥æ¡†æ¶åŒ…å«ä¸¤ä¸ªç‹¬ç«‹ç»„ä»¶ï¼šReasonerå’ŒCriticã€‚Reasonerè´Ÿè´£ç”ŸæˆåŸºäºè§†è§‰å’Œæ–‡æœ¬è¾“å…¥çš„æ¨ç†è·¯å¾„ï¼Œè€ŒCriticåˆ™æä¾›å»ºè®¾æ€§æ‰¹è¯„ä»¥ä¼˜åŒ–è¿™äº›è·¯å¾„ã€‚</li>
<li>Criticæ¨¡å‹é‡‡ç”¨ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰è®­ç»ƒï¼Œåˆ©ç”¨åŸºäºè§„åˆ™çš„å¥–åŠ±å¯¹æ‰¹è¯„è¿›è¡Œæ’åã€‚</li>
<li>Critic-Væ¡†æ¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¨ç†å‡†ç¡®æ€§å’Œæ•ˆç‡æ–¹é¢ã€‚</li>
<li>è¯¥æ–¹æ³•å¯¹æé«˜VLMåœ¨çœŸå®ä¸–ç•Œå¤šæ¨¡æ€åº”ç”¨ä¸­çš„å¯é æ€§æä¾›äº†æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.18203">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-722f75d9e130680fe8a09337b9de17cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b6837c2c367cd1a4e7ee3ad4ec2bd8ec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-269d187768ea4ab5aa83a7d89fb133bc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f2278620f94584191480f74c9cf8598c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0bf82c8a9b7c59eb8562c67e28067af8.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="PromptHSI-Universal-Hyperspectral-Image-Restoration-with-Vision-Language-Modulated-Frequency-Adaptation"><a href="#PromptHSI-Universal-Hyperspectral-Image-Restoration-with-Vision-Language-Modulated-Frequency-Adaptation" class="headerlink" title="PromptHSI: Universal Hyperspectral Image Restoration with   Vision-Language Modulated Frequency Adaptation"></a>PromptHSI: Universal Hyperspectral Image Restoration with   Vision-Language Modulated Frequency Adaptation</h2><p><strong>Authors:Chia-Ming Lee, Ching-Heng Cheng, Yu-Fan Lin, Yi-Ching Cheng, Wo-Ting Liao, Fu-En Yang, Yu-Chiang Frank Wang, Chih-Chung Hsu</strong></p>
<p>Recent advances in All-in-One (AiO) RGB image restoration have demonstrated the effectiveness of prompt learning in handling multiple degradations within a single model. However, extending these approaches to hyperspectral image (HSI) restoration is challenging due to the domain gap between RGB and HSI features, information loss in visual prompts under severe composite degradations, and difficulties in capturing HSI-specific degradation patterns via text prompts. In this paper, we propose PromptHSI, the first universal AiO HSI restoration framework that addresses these challenges. By incorporating frequency-aware feature modulation, which utilizes frequency analysis to narrow down the restoration search space and employing vision-language model (VLM)-guided prompt learning, our approach decomposes text prompts into intensity and bias controllers that effectively guide the restoration process while mitigating domain discrepancies. Extensive experiments demonstrate that our unified architecture excels at both fine-grained recovery and global information restoration across diverse degradation scenarios, highlighting its significant potential for practical remote sensing applications. The source code is available at <a target="_blank" rel="noopener" href="https://github.com/chingheng0808/PromptHSI">https://github.com/chingheng0808/PromptHSI</a>. </p>
<blockquote>
<p>åœ¨å…¨æ™¯ä¸€ä½“åŒ–ï¼ˆAiOï¼‰RGBå›¾åƒä¿®å¤çš„æœ€æ–°è¿›å±•ä¸­ï¼Œæç¤ºå­¦ä¹ åœ¨å¤„ç†å•ä¸€æ¨¡å‹å†…çš„å¤šç§é€€åŒ–é—®é¢˜æ–¹é¢çš„æœ‰æ•ˆæ€§å·²ç»å¾—åˆ°äº†éªŒè¯ã€‚ç„¶è€Œï¼Œå°†è¿™äº›æ–¹æ³•æ‰©å±•åˆ°é«˜å…‰è°±å›¾åƒï¼ˆHSIï¼‰ä¿®å¤å´å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºRGBå’ŒHSIç‰¹å¾ä¹‹é—´çš„é¢†åŸŸå·®è·ã€åœ¨ä¸¥é‡å¤åˆé€€åŒ–ä¸‹è§†è§‰æç¤ºçš„ä¿¡æ¯ä¸¢å¤±ï¼Œä»¥åŠé€šè¿‡æ–‡æœ¬æç¤ºæ•è·HSIç‰¹å®šé€€åŒ–æ¨¡å¼çš„å›°éš¾ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†PromptHSIï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªè§£å†³è¿™äº›æŒ‘æˆ˜çš„å…¨æ™¯ä¸€ä½“åŒ–HSIä¿®å¤æ¡†æ¶ã€‚é€šè¿‡ç»“åˆé¢‘ç‡æ„ŸçŸ¥ç‰¹å¾è°ƒåˆ¶ï¼Œåˆ©ç”¨é¢‘ç‡åˆ†æç¼©å°ä¿®å¤æœç´¢ç©ºé—´ï¼Œå¹¶é‡‡ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æŒ‡å¯¼çš„æç¤ºå­¦ä¹ ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å°†æ–‡æœ¬æç¤ºåˆ†è§£ä¸ºå¼ºåº¦å’Œåå·®æ§åˆ¶å™¨ï¼Œæœ‰æ•ˆåœ°æŒ‡å¯¼äº†ä¿®å¤è¿‡ç¨‹ï¼ŒåŒæ—¶å‡è½»äº†é¢†åŸŸå·®å¼‚ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç»Ÿä¸€æ¶æ„åœ¨å¤šç§é€€åŒ–åœºæ™¯ä¸­éƒ½æ“…é•¿ç²¾ç»†æ¢å¤å’Œå…¨å±€ä¿¡æ¯æ¢å¤ï¼Œçªå‡ºäº†å…¶åœ¨å®é™…é¥æ„Ÿåº”ç”¨ä¸­çš„æ˜¾è‘—æ½œåŠ›ã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/chingheng0808/PromptHSI%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/chingheng0808/PromptHSIæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.15922v3">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://chingheng0808.github.io/prompthsiP/static.html">https://chingheng0808.github.io/prompthsiP/static.html</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºPromptHSIçš„é€šç”¨AiOé«˜å…‰è°±å›¾åƒï¼ˆHSIï¼‰æ¢å¤æ¡†æ¶ï¼Œè§£å†³äº†åœ¨RGBå›¾åƒæ¢å¤ä¸­é‡‡ç”¨æç¤ºå­¦ä¹ å¤„ç†å¤šç§é€€åŒ–æ‰€é¢ä¸´çš„æŒ‘æˆ˜ã€‚é€šè¿‡ç»“åˆé¢‘ç‡æ„ŸçŸ¥ç‰¹å¾è°ƒåˆ¶å’Œè§†è§‰è¯­è¨€æ¨¡å‹å¼•å¯¼æç¤ºå­¦ä¹ ï¼Œè¯¥æ¡†æ¶ç¼©å°äº†æ¢å¤çš„æœç´¢ç©ºé—´ï¼Œå¹¶å°†æ–‡æœ¬æç¤ºåˆ†è§£ä¸ºå¼ºåº¦å’Œåå·®æ§åˆ¶å™¨ï¼Œæœ‰æ•ˆæŒ‡å¯¼æ¢å¤è¿‡ç¨‹å¹¶å‡å°‘é¢†åŸŸå·®å¼‚ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶åœ¨ä¸åŒé€€åŒ–åœºæ™¯ä¸­éƒ½èƒ½å®ç°ç²¾ç»†æ¢å¤å’Œå…¨å±€ä¿¡æ¯æ¢å¤ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨é¥æ„Ÿåº”ç”¨ä¸­çš„æ˜¾è‘—æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PromptHSIæ˜¯é¦–ä¸ªé€šç”¨çš„AiOé«˜å…‰è°±å›¾åƒæ¢å¤æ¡†æ¶ã€‚</li>
<li>è¯¥æ¡†æ¶è§£å†³äº†RGBå›¾åƒæ¢å¤åœ¨åº”ç”¨åˆ°é«˜å…‰è°±å›¾åƒæ¢å¤æ—¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚</li>
<li>é€šè¿‡é¢‘ç‡æ„ŸçŸ¥ç‰¹å¾è°ƒåˆ¶ç¼©å°äº†æ¢å¤çš„æœç´¢ç©ºé—´ã€‚</li>
<li>ç»“åˆäº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å¼•å¯¼çš„æç¤ºå­¦ä¹ ã€‚</li>
<li>æ–‡æœ¬æç¤ºè¢«åˆ†è§£ä¸ºå¼ºåº¦å’Œåå·®æ§åˆ¶å™¨ï¼Œæœ‰æ•ˆæŒ‡å¯¼æ¢å¤è¿‡ç¨‹ã€‚</li>
<li>è¯¥æ¡†æ¶èƒ½å¤„ç†ä¸åŒé€€åŒ–åœºæ™¯ä¸‹çš„ç²¾ç»†æ¢å¤å’Œå…¨å±€ä¿¡æ¯æ¢å¤ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.15922">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ff20d27761ecf8a5a353f365ea7d2f67.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e08131e2fd5343dfd056a87045874156.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b5cbbe54ccbe0bad19aae338d4a63687.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad0d750b1dee273cc83580335b6141c5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec184b4523d2a9ad4336a6e456a1e936.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-12251ebb19d0447c881400e74be59b48.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="MambaIRv2-Attentive-State-Space-Restoration"><a href="#MambaIRv2-Attentive-State-Space-Restoration" class="headerlink" title="MambaIRv2: Attentive State Space Restoration"></a>MambaIRv2: Attentive State Space Restoration</h2><p><strong>Authors:Hang Guo, Yong Guo, Yaohua Zha, Yulun Zhang, Wenbo Li, Tao Dai, Shu-Tao Xia, Yawei Li</strong></p>
<p>The Mamba-based image restoration backbones have recently demonstrated significant potential in balancing global reception and computational efficiency. However, the inherent causal modeling limitation of Mamba, where each token depends solely on its predecessors in the scanned sequence, restricts the full utilization of pixels across the image and thus presents new challenges in image restoration. In this work, we propose MambaIRv2, which equips Mamba with the non-causal modeling ability similar to ViTs to reach the attentive state space restoration model. Specifically, the proposed attentive state-space equation allows to attend beyond the scanned sequence and facilitate image unfolding with just one single scan. Moreover, we further introduce a semantic-guided neighboring mechanism to encourage interaction between distant but similar pixels. Extensive experiments show our MambaIRv2 outperforms SRFormer by even 0.35dB PSNR for lightweight SR even with 9.3% less parameters and suppresses HAT on classic SR by up to 0.29dB. Code is available at <a target="_blank" rel="noopener" href="https://github.com/csguoh/MambaIR">https://github.com/csguoh/MambaIR</a>. </p>
<blockquote>
<p>åŸºäºMambaçš„å›¾åƒæ¢å¤éª¨å¹²ç½‘æœ€è¿‘åœ¨å¹³è¡¡å…¨å±€æ¥æ”¶å’Œè®¡ç®—æ•ˆç‡æ–¹é¢è¡¨ç°å‡ºäº†å·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼ŒMambaå›ºæœ‰çš„å› æœå»ºæ¨¡é™åˆ¶ï¼Œå³æ¯ä¸ªä»¤ç‰Œä»…ä¾èµ–äºæ‰«æåºåˆ—ä¸­çš„å…ˆé©±è€…ï¼Œé™åˆ¶äº†è·¨å›¾åƒçš„åƒç´ çš„å……åˆ†åˆ©ç”¨ï¼Œä»è€Œä¸ºå›¾åƒæ¢å¤å¸¦æ¥äº†æ–°çš„æŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†MambaIRv2ï¼Œå®ƒä¸ºMambaé…å¤‡äº†ä¸ViTsç±»ä¼¼çš„éå› æœå»ºæ¨¡èƒ½åŠ›ï¼Œä»¥è¾¾åˆ°å…³æ³¨çŠ¶æ€ç©ºé—´æ¢å¤æ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼Œæ‰€æå‡ºçš„å…³æ³¨çŠ¶æ€ç©ºé—´æ–¹ç¨‹å…è®¸å…³æ³¨æ‰«æåºåˆ—ä¹‹å¤–çš„å†…å®¹ï¼Œå¹¶ä»…é€šè¿‡ä¸€æ¬¡æ‰«æå³å¯ä¿ƒè¿›å›¾åƒå±•å¼€ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¿›ä¸€æ­¥å¼•å…¥äº†ä¸€ç§è¯­ä¹‰å¼•å¯¼é‚»æ¥æœºåˆ¶ï¼Œä»¥é¼“åŠ±è¿œè·ç¦»ä½†ç›¸ä¼¼çš„åƒç´ ä¹‹é—´çš„äº¤äº’ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„MambaIRv2ç”šè‡³è¶…è¶Šäº†SRFormerï¼Œåœ¨è½»é‡çº§SRçš„PSNRä¸Šæé«˜äº†0.35dBï¼ŒåŒæ—¶å‚æ•°å‡å°‘äº†9.3%ã€‚åœ¨ç»å…¸SRä¸ŠæŠ‘åˆ¶äº†HATé«˜è¾¾0.29dBã€‚ä»£ç å¯ä»<a target="_blank" rel="noopener" href="https://github.com/csguoh/MambaIR%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/csguoh/MambaIRè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.15269v2">PDF</a> Accepted by CVPR2025</p>
<p><strong>æ‘˜è¦</strong><br>     åŸºäºMambaçš„å›¾åƒæ¢å¤æ¶æ„åœ¨å¹³è¡¡å…¨å±€æ¥å—å’Œè®¡ç®—æ•ˆç‡æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼ŒMambaå›ºæœ‰çš„å› æœå»ºæ¨¡é™åˆ¶ï¼Œå³æ¯ä¸ªæ ‡è®°ä»…ä¾èµ–äºæ‰«æåºåˆ—ä¸­çš„å‰é©±æ ‡è®°ï¼Œé™åˆ¶äº†å›¾åƒä¸­åƒç´ çš„å……åˆ†åˆ©ç”¨ï¼Œä»è€Œç»™å›¾åƒæ¢å¤å¸¦æ¥äº†æ–°çš„æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æå‡ºäº†MambaIRv2ï¼Œå®ƒé€šè¿‡é…å¤‡éå› æœå»ºæ¨¡èƒ½åŠ›ï¼ˆç±»ä¼¼äºViTsï¼‰æ¥å¢å¼ºMambaçš„åŠŸèƒ½ï¼Œä»¥å®ç°å…³æ³¨çŠ¶æ€ç©ºé—´æ¢å¤æ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼Œæå‡ºçš„çŠ¶æ€ç©ºé—´æ–¹ç¨‹å…è®¸å…³æ³¨æ‰«æåºåˆ—ä¹‹å¤–çš„å†…å®¹ï¼Œå¹¶ä¸”åªéœ€è¦å•æ¬¡æ‰«æå°±èƒ½å®ç°å›¾åƒå±•å¼€ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†è¯­ä¹‰å¼•å¯¼é‚»åŸŸæœºåˆ¶æ¥é¼“åŠ±è¿œè·ç¦»ç›¸ä¼¼åƒç´ ä¹‹é—´çš„äº¤äº’ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„MambaIRv2åœ¨è½»é‡çº§SRä¸Šç”šè‡³è¶…è¿‡äº†SRFormerçš„0.35dB PSNRï¼Œå¹¶ä¸”å‚æ•°å‡å°‘äº†9.3%ï¼Œåœ¨ç»å…¸SRä¸Šæœ€å¤šæ¯”HATé«˜å‡º0.29dBã€‚ä»£ç å¯åœ¨csguoh&#x2F;MambaIRç½‘ç«™ä¸Šæ‰¾åˆ°ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>Mambaå›¾åƒæ¢å¤æ¶æ„å±•ç°å‡ºåœ¨å¹³è¡¡å…¨å±€æ¥å—å’Œè®¡ç®—æ•ˆç‡æ–¹é¢çš„æ½œåŠ›ã€‚</li>
<li>Mambaå­˜åœ¨å› æœå»ºæ¨¡é™åˆ¶ï¼Œé™åˆ¶äº†å›¾åƒä¸­åƒç´ çš„å……åˆ†åˆ©ç”¨ã€‚</li>
<li>æå‡ºMambaIRv2ï¼Œé…å¤‡éå› æœå»ºæ¨¡èƒ½åŠ›ï¼Œä»¥å®ç°å…³æ³¨çŠ¶æ€ç©ºé—´æ¢å¤æ¨¡å‹ã€‚</li>
<li>MambaIRv2å…è®¸å…³æ³¨æ‰«æåºåˆ—ä¹‹å¤–çš„å†…å®¹ï¼Œä¿ƒè¿›å›¾åƒå±•å¼€ã€‚</li>
<li>å¼•å…¥è¯­ä¹‰å¼•å¯¼é‚»åŸŸæœºåˆ¶æ¥åŠ å¼ºåƒç´ é—´çš„äº¤äº’ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºMambaIRv2æ€§èƒ½ä¼˜è¶Šï¼Œè½»é‡çº§SRä¸Šä¼˜äºSRFormerã€‚</li>
<li>ä»£ç å·²å…¬å¼€åœ¨csguoh&#x2F;MambaIRç½‘ç«™ä¸Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.15269">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-85b4eee7706d696a9b95f945057a2bb3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eaff782a7fd6be80e2109a2cce36880d.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-14\./crop_Vision Transformer/2411.15269v2/page_3_0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-55285f29d86493719080728a27ce191b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-30c0077c899e2798f8a5efc94eaed658.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="BiomedCoOp-Learning-to-Prompt-for-Biomedical-Vision-Language-Models"><a href="#BiomedCoOp-Learning-to-Prompt-for-Biomedical-Vision-Language-Models" class="headerlink" title="BiomedCoOp: Learning to Prompt for Biomedical Vision-Language Models"></a>BiomedCoOp: Learning to Prompt for Biomedical Vision-Language Models</h2><p><strong>Authors:Taha Koleilat, Hojat Asgariandehkordi, Hassan Rivaz, Yiming Xiao</strong></p>
<p>Recent advancements in vision-language models (VLMs), such as CLIP, have demonstrated substantial success in self-supervised representation learning for vision tasks. However, effectively adapting VLMs to downstream applications remains challenging, as their accuracy often depends on time-intensive and expertise-demanding prompt engineering, while full model fine-tuning is costly. This is particularly true for biomedical images, which, unlike natural images, typically suffer from limited annotated datasets, unintuitive image contrasts, and nuanced visual features. Recent prompt learning techniques, such as Context Optimization (CoOp) intend to tackle these issues, but still fall short in generalizability. Meanwhile, explorations in prompt learning for biomedical image analysis are still highly limited. In this work, we propose BiomedCoOp, a novel prompt learning framework that enables efficient adaptation of BiomedCLIP for accurate and highly generalizable few-shot biomedical image classification. Our approach achieves effective prompt context learning by leveraging semantic consistency with average prompt ensembles from Large Language Models (LLMs) and knowledge distillation with a statistics-based prompt selection strategy. We conducted comprehensive validation of our proposed framework on 11 medical datasets across 9 modalities and 10 organs against existing state-of-the-art methods, demonstrating significant improvements in both accuracy and generalizability. The code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/HealthX-Lab/BiomedCoOp">https://github.com/HealthX-Lab/BiomedCoOp</a>. </p>
<blockquote>
<p>å…³äºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰çš„æœ€æ–°è¿›å±•åœ¨è§†è§‰ä»»åŠ¡çš„è‡ªç›‘ç£è¡¨ç¤ºå­¦ä¹ ä¸­å–å¾—äº†é‡å¤§æˆåŠŸã€‚ç„¶è€Œï¼Œå°†VLMsæœ‰æ•ˆåœ°é€‚åº”åˆ°ä¸‹æ¸¸åº”ç”¨ä¸­ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå®ƒä»¬çš„å‡†ç¡®åº¦é€šå¸¸ä¾èµ–äºè€—æ—¶ä¸”éœ€è¦ä¸“ä¸šçŸ¥è¯†çš„æç¤ºå·¥ç¨‹ï¼Œè€Œå…¨æ¨¡å‹çš„å¾®è°ƒæˆæœ¬é«˜æ˜‚ã€‚è¿™å¯¹äºç”Ÿç‰©åŒ»å­¦å›¾åƒå°¤å…¶å¦‚æ­¤ï¼Œä¸å¤©ç„¶å›¾åƒä¸åŒï¼Œç”Ÿç‰©åŒ»å­¦å›¾åƒé€šå¸¸å—é™äºæ ‡æ³¨æ•°æ®é›†ã€å›¾åƒå¯¹æ¯”åº¦ä¸å¤Ÿç›´è§‚ä»¥åŠå¾®å¦™çš„è§†è§‰ç‰¹å¾ã€‚æœ€è¿‘çš„æç¤ºå­¦ä¹ æŠ€æœ¯ï¼ˆå¦‚CoOpï¼‰æ—¨åœ¨è§£å†³è¿™äº›é—®é¢˜ï¼Œä½†åœ¨é€šç”¨æ€§æ–¹é¢ä»ç„¶æœ‰æ‰€ä¸è¶³ã€‚åŒæ—¶ï¼Œé’ˆå¯¹ç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†æçš„æç¤ºå­¦ä¹ æ¢ç´¢ä»ç„¶éå¸¸æœ‰é™ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†BiomedCoOpï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„æç¤ºå­¦ä¹ æ¡†æ¶ï¼Œèƒ½å¤Ÿå®ç°BiomedCLIPçš„é«˜æ•ˆé€‚åº”ï¼Œä»¥è¿›è¡Œå‡†ç¡®ä¸”é«˜åº¦é€šç”¨çš„ç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†ç±»ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡åˆ©ç”¨ä¸å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¹³å‡æç¤ºé›†åˆçš„è¯­ä¹‰ä¸€è‡´æ€§ä»¥åŠåŸºäºç»Ÿè®¡çš„æç¤ºé€‰æ‹©ç­–ç•¥çš„çŸ¥è¯†è’¸é¦ï¼Œå®ç°äº†æœ‰æ•ˆçš„æç¤ºä¸Šä¸‹æ–‡å­¦ä¹ ã€‚æˆ‘ä»¬åœ¨è·¨è¶Š9ç§æ¨¡æ€å’Œ10ä¸ªå™¨å®˜çš„11ä¸ªåŒ»å­¦æ•°æ®é›†ä¸Šå¯¹æ‰€æå‡ºçš„æ¡†æ¶è¿›è¡Œäº†å…¨é¢çš„éªŒè¯ï¼Œä¸ç°æœ‰çš„æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œå‡†ç¡®æ€§å’Œé€šç”¨æ€§å‡æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æå‡ã€‚ä»£ç å…¬å¼€åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/HealthX-Lab/BiomedCoOp%E3%80%82">https://github.com/HealthX-Lab/BiomedCoOpã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.15232v2">PDF</a> Accepted to CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºCLIPç­‰è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„æœ€æ–°è¿›å±•ï¼Œåœ¨è‡ªæˆ‘ç›‘ç£è¡¨ç¤ºå­¦ä¹ æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚ç„¶è€Œï¼Œå°†å…¶æœ‰æ•ˆåœ°é€‚åº”ä¸‹æ¸¸åº”ç”¨ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œç‰¹åˆ«æ˜¯å¯¹äºç”Ÿç‰©åŒ»å­¦å›¾åƒè€Œè¨€ã€‚æœ¬æ–‡æå‡ºäº†BiomedCoOpï¼Œä¸€ç§æ–°é¢–çš„æç¤ºå­¦ä¹ æ¡†æ¶ï¼Œèƒ½å¤Ÿå®ç°å‡†ç¡®ä¸”é«˜åº¦é€šç”¨çš„å°‘æ•°ç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†ç±»çš„æ¨¡å‹é€‚é…ã€‚å®ƒé€šè¿‡åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¹³å‡æç¤ºé›†åˆçš„è¯­ä¹‰ä¸€è‡´æ€§ä»¥åŠåŸºäºç»Ÿè®¡çš„æç¤ºé€‰æ‹©ç­–ç•¥è¿›è¡ŒçŸ¥è¯†è’¸é¦ï¼Œå®ç°äº†æœ‰æ•ˆçš„æç¤ºä¸Šä¸‹æ–‡å­¦ä¹ ã€‚ç»è¿‡å¹¿æ³›çš„éªŒè¯ï¼Œä¸ç°æœ‰çš„æœ€æ–°æ–¹æ³•ç›¸æ¯”ï¼ŒBiomedCoOpåœ¨å‡†ç¡®æ€§å’Œé€šç”¨æ€§æ–¹é¢éƒ½å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>VLMså¦‚CLIPåœ¨è‡ªæˆ‘ç›‘ç£è¡¨ç¤ºå­¦ä¹ æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†åœ¨é€‚åº”ä¸‹æ¸¸åº”ç”¨æ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>æç¤ºå·¥ç¨‹æ˜¯VLMsåœ¨ç‰¹å®šä»»åŠ¡ä¸­åº”ç”¨çš„å…³é”®å› ç´ ï¼Œé€šå¸¸éœ€è¦å¤§é‡æ—¶é—´å’Œä¸“ä¸šçŸ¥è¯†ã€‚</li>
<li>é’ˆå¯¹ç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†æçš„ç‰¹å®šæŒ‘æˆ˜ï¼ˆå¦‚æœ‰é™æ³¨é‡Šæ•°æ®é›†ã€ä¸ç›´è§‚çš„å›¾åƒå¯¹æ¯”å’Œå¾®å¦™çš„è§†è§‰ç‰¹å¾ï¼‰ï¼Œç°æœ‰çš„æ–¹æ³•å¦‚CoOpä»æœ‰å¾…æ”¹è¿›ã€‚</li>
<li>BiomedCoOpæ˜¯ä¸€ç§æ–°é¢–çš„æç¤ºå­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡ç»“åˆè¯­ä¹‰ä¸€è‡´æ€§ã€å¹³å‡æç¤ºé›†åˆå’ŒåŸºäºç»Ÿè®¡çš„æç¤ºé€‰æ‹©ç­–ç•¥è¿›è¡ŒçŸ¥è¯†è’¸é¦ï¼Œå®ç°äº†é«˜æ•ˆçš„ç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†ç±»æ¨¡å‹é€‚é…ã€‚</li>
<li>åœ¨æ¶µç›–å¤šç§åŒ»å­¦æ•°æ®é›†ã€æ¨¡æ€å’Œå™¨å®˜çš„å¤§è§„æ¨¡éªŒè¯ä¸­ï¼ŒBiomedCoOpç›¸å¯¹äºç°æœ‰æŠ€æœ¯æ˜¾ç¤ºå‡ºæ›´é«˜çš„å‡†ç¡®æ€§å’Œé€šç”¨æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.15232">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-38e06f0b12f79ae2df2bf415fffa84ef.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d742d08ca799cb2ef67626f8fec6be7e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6bec45fcdc9fbfcf81f31e3126e9fa5c.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-14/Vision%20Transformer/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-14/Vision%20Transformer/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Vision-Transformer/">
                                    <span class="chip bg-color">Vision Transformer</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-14/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-85496aa7a0b91d2d83ea6d5050f46013.jpg" class="responsive-img" alt="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
                        
                        <span class="card-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-14  Dual-Domain Homogeneous Fusion with Cross-Modal Mamba and Progressive   Decoder for 3D Object Detection
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-14
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-14/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-b657b10c8d2e5e2c6f07564b881637c9.jpg" class="responsive-img" alt="è§†é¢‘ç†è§£">
                        
                        <span class="card-title">è§†é¢‘ç†è§£</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-14  Memory-enhanced Retrieval Augmentation for Long Video Understanding
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-14
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    è§†é¢‘ç†è§£
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30930.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
