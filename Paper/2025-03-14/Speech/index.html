<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-14  Everything Can Be Described in Words A Simple Unified Multi-Modal   Framework with Semantic and Temporal Alignment">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-4ab6d35773e0ca0121eba9370d764c97.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-14
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    9.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    37 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-14-æ›´æ–°"><a href="#2025-03-14-æ›´æ–°" class="headerlink" title="2025-03-14 æ›´æ–°"></a>2025-03-14 æ›´æ–°</h1><h2 id="Everything-Can-Be-Described-in-Words-A-Simple-Unified-Multi-Modal-Framework-with-Semantic-and-Temporal-Alignment"><a href="#Everything-Can-Be-Described-in-Words-A-Simple-Unified-Multi-Modal-Framework-with-Semantic-and-Temporal-Alignment" class="headerlink" title="Everything Can Be Described in Words: A Simple Unified Multi-Modal   Framework with Semantic and Temporal Alignment"></a>Everything Can Be Described in Words: A Simple Unified Multi-Modal   Framework with Semantic and Temporal Alignment</h2><p><strong>Authors:Xiaowei Bi, Zheyuan Xu</strong></p>
<p>Long Video Question Answering (LVQA) is challenging due to the need for temporal reasoning and large-scale multimodal data processing. Existing methods struggle with retrieving cross-modal information from long videos, especially when relevant details are sparsely distributed. We introduce UMaT (Unified Multi-modal as Text), a retrieval-augmented generation (RAG) framework that efficiently processes extremely long videos while maintaining cross-modal coherence. UMaT converts visual and auditory data into a unified textual representation, ensuring semantic and temporal alignment. Short video clips are analyzed using a vision-language model, while automatic speech recognition (ASR) transcribes dialogue. These text-based representations are structured into temporally aligned segments, with adaptive filtering to remove redundancy and retain salient details. The processed data is embedded into a vector database, enabling precise retrieval of dispersed yet relevant content. Experiments on a benchmark LVQA dataset show that UMaT outperforms existing methods in multimodal integration, long-form video understanding, and sparse information retrieval. Its scalability and interpretability allow it to process videos over an hour long while maintaining semantic and temporal coherence. These findings underscore the importance of structured retrieval and multimodal synchronization for advancing LVQA and long-form AI systems. </p>
<blockquote>
<p>é•¿è§†é¢‘é—®ç­”ï¼ˆLVQAï¼‰å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå®ƒéœ€è¦æ—¶åºæ¨ç†å’Œå¤§è§„æ¨¡å¤šæ¨¡æ€æ•°æ®å¤„ç†ã€‚ç°æœ‰æ–¹æ³•åœ¨ä»é•¿è§†é¢‘ä¸­æ£€ç´¢è·¨æ¨¡æ€ä¿¡æ¯æ—¶é¢ä¸´å›°éš¾ï¼Œå°¤å…¶æ˜¯å½“ç›¸å…³ç»†èŠ‚ç¨€ç–åˆ†å¸ƒæ—¶ã€‚æˆ‘ä»¬å¼•å…¥äº†UMaTï¼ˆç»Ÿä¸€å¤šæ¨¡æ€æ–‡æœ¬ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å¢å¼ºæ£€ç´¢ç”Ÿæˆï¼ˆRAGï¼‰æ¡†æ¶ï¼Œèƒ½å¤Ÿé«˜æ•ˆå¤„ç†æé•¿çš„è§†é¢‘ï¼ŒåŒæ—¶ä¿æŒè·¨æ¨¡æ€ä¸€è‡´æ€§ã€‚UMaTå°†è§†è§‰å’Œå¬è§‰æ•°æ®è½¬æ¢ä¸ºç»Ÿä¸€çš„æ–‡æœ¬è¡¨ç¤ºï¼Œç¡®ä¿è¯­ä¹‰å’Œæ—¶åºå¯¹é½ã€‚çŸ­è§†é¢‘ç‰‡æ®µä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œåˆ†æï¼Œè€Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰åˆ™è½¬å½•å¯¹è¯ã€‚è¿™äº›åŸºäºæ–‡æœ¬çš„è¡¨ç¤ºè¢«ç»“æ„åŒ–æˆæ—¶åºå¯¹é½çš„ç‰‡æ®µï¼Œé€šè¿‡è‡ªé€‚åº”è¿‡æ»¤å»é™¤å†—ä½™ï¼Œä¿ç•™é‡è¦ç»†èŠ‚ã€‚å¤„ç†è¿‡çš„æ•°æ®åµŒå…¥åˆ°å‘é‡æ•°æ®åº“ä¸­ï¼Œèƒ½å¤Ÿå®ç°åˆ†æ•£ä½†ç›¸å…³å†…å®¹ç²¾ç¡®æ£€ç´¢ã€‚åœ¨åŸºå‡†LVQAæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒUMaTåœ¨å¤šæ¨¡æ€èåˆã€é•¿æ ¼å¼è§†é¢‘ç†è§£å’Œç¨€ç–ä¿¡æ¯æ£€ç´¢æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚å…¶å¯æ‰©å±•æ€§å’Œå¯è§£é‡Šæ€§å…è®¸å…¶å¤„ç†è¶…è¿‡ä¸€å°æ—¶çš„è§†é¢‘ï¼ŒåŒæ—¶ä¿æŒè¯­ä¹‰å’Œæ—¶åºä¸€è‡´æ€§ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†ç»“æ„åŒ–æ£€ç´¢å’Œå¤šæ¨¡æ€åŒæ­¥åœ¨æ¨åŠ¨LVQAå’Œé•¿æ ¼å¼AIç³»ç»Ÿå‘å±•ä¸­çš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09081v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>é•¿è§†é¢‘é—®ç­”ï¼ˆLVQAï¼‰é¢ä¸´æ—¶é—´æ¨ç†å’Œå¤§è§„æ¨¡å¤šæ¨¡æ€æ•°æ®å¤„ç†çš„éœ€æ±‚æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥ä»é•¿è§†é¢‘ä¸­æ£€ç´¢è·¨æ¨¡æ€ä¿¡æ¯ï¼Œå°¤å…¶å½“ç›¸å…³ç»†èŠ‚åˆ†å¸ƒç¨€ç–æ—¶ã€‚æˆ‘ä»¬æ¨å‡ºUMaTï¼ˆç»Ÿä¸€å¤šæ¨¡æ€æ–‡æœ¬ï¼‰ï¼Œä¸€ä¸ªæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ¡†æ¶ï¼Œèƒ½å¤Ÿé«˜æ•ˆå¤„ç†æé•¿è§†é¢‘å¹¶ä¿æŒè·¨æ¨¡æ€ä¸€è‡´æ€§ã€‚UMaTå°†è§†è§‰å’Œå¬è§‰æ•°æ®è½¬åŒ–ä¸ºç»Ÿä¸€æ–‡æœ¬è¡¨ç¤ºï¼Œç¡®ä¿è¯­ä¹‰å’Œæ—¶é—´å¯¹é½ã€‚å®ƒåˆ†æçŸ­è§†é¢‘ç‰‡æ®µä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰è½¬å½•å¯¹è¯ã€‚è¿™äº›åŸºäºæ–‡æœ¬çš„è¡¨ç¤ºè¢«ç»“æ„åŒ–æˆæ—¶é—´å¯¹é½çš„ç‰‡æ®µï¼Œé€šè¿‡è‡ªé€‚åº”è¿‡æ»¤å»é™¤å†—ä½™å¹¶ä¿ç•™é‡è¦ç»†èŠ‚ã€‚å¤„ç†è¿‡çš„æ•°æ®åµŒå…¥å‘é‡æ•°æ®åº“ï¼Œèƒ½ç²¾ç¡®æ£€ç´¢åˆ†æ•£ä½†ç›¸å…³çš„å†…å®¹ã€‚åœ¨åŸºå‡†LVQAæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒUMaTåœ¨å¤šæ¨¡æ€èåˆã€é•¿è§†é¢‘ç†è§£å’Œç¨€ç–ä¿¡æ¯æ£€ç´¢æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚å…¶å¯æ‰©å±•æ€§å’Œå¯è§£é‡Šæ€§å¯å¤„ç†è¶…è¿‡ä¸€å°æ—¶çš„è§†é¢‘ï¼ŒåŒæ—¶ä¿æŒè¯­ä¹‰å’Œæ—¶é—´ä¸€è‡´æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é•¿è§†é¢‘é—®ç­”ï¼ˆLVQAï¼‰éœ€è¦å¤„ç†å¤§è§„æ¨¡å¤šæ¨¡æ€æ•°æ®ï¼Œé¢ä¸´æ—¶é—´æ¨ç†å’Œè·¨æ¨¡æ€ä¿¡æ¯æ£€ç´¢çš„æŒ‘æˆ˜ã€‚</li>
<li>UMaTæ˜¯ä¸€ä¸ªRAGæ¡†æ¶ï¼Œèƒ½é«˜æ•ˆå¤„ç†æé•¿è§†é¢‘å¹¶ç»´æŒè·¨æ¨¡æ€ä¸€è‡´æ€§ã€‚</li>
<li>UMaTå°†è§†è§‰å’Œå¬è§‰æ•°æ®è½¬åŒ–ä¸ºç»Ÿä¸€æ–‡æœ¬è¡¨ç¤ºï¼Œç¡®ä¿è¯­ä¹‰å’Œæ—¶é—´å¯¹é½ã€‚</li>
<li>UMaTé€šè¿‡ç»“æ„åŒ–ä¸ºæ—¶é—´å¯¹é½çš„ç‰‡æ®µï¼Œå¹¶åˆ©ç”¨è‡ªé€‚åº”è¿‡æ»¤æ¥å¤„ç†é•¿è§†é¢‘ä¸­çš„å†—ä½™å’Œå…³é”®ä¿¡æ¯ã€‚</li>
<li>UMaTä½¿ç”¨å‘é‡æ•°æ®åº“è¿›è¡Œæ•°æ®å¤„ç†å’Œå­˜å‚¨ï¼Œèƒ½ç²¾ç¡®æ£€ç´¢åˆ†æ•£ä½†ç›¸å…³çš„å†…å®¹ã€‚</li>
<li>UMaTåœ¨åŸºå‡†LVQAæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå°¤å…¶åœ¨å¤šæ¨¡æ€èåˆã€é•¿è§†é¢‘ç†è§£å’Œç¨€ç–ä¿¡æ¯æ£€ç´¢æ–¹é¢ã€‚</li>
<li>UMaTå…·å¤‡å¯æ‰©å±•æ€§å’Œå¯è§£é‡Šæ€§ï¼Œå¯å¤„ç†è¶…è¿‡ä¸€å°æ—¶çš„è§†é¢‘å†…å®¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09081">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e8d6a40becdda56a9af6016bebe4206f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7d632126210c7544985bf52dc32b618e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2821815a481a85b1ae5016565a24a70c.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="An-Exhaustive-Evaluation-of-TTS-and-VC-based-Data-Augmentation-for-ASR"><a href="#An-Exhaustive-Evaluation-of-TTS-and-VC-based-Data-Augmentation-for-ASR" class="headerlink" title="An Exhaustive Evaluation of TTS- and VC-based Data Augmentation for ASR"></a>An Exhaustive Evaluation of TTS- and VC-based Data Augmentation for ASR</h2><p><strong>Authors:Sewade Ogun, Vincent Colotte, Emmanuel Vincent</strong></p>
<p>Augmenting the training data of automatic speech recognition (ASR) systems with synthetic data generated by text-to-speech (TTS) or voice conversion (VC) has gained popularity in recent years. Several works have demonstrated improvements in ASR performance using this augmentation approach. However, because of the lower diversity of synthetic speech, naively combining synthetic and real data often does not yield the best results. In this work, we leverage recently proposed flow-based TTS&#x2F;VC models allowing greater speech diversity, and assess the respective impact of augmenting various speech attributes on the word error rate (WER) achieved by several ASR models. Pitch augmentation and VC-based speaker augmentation are found to be ineffective in our setup. Jointly augmenting all other attributes reduces the WER of a Conformer-Transducer model by 11% relative on Common Voice and by up to 35% relative on LibriSpeech compared to training on real data only. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œé€šè¿‡æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æˆ–è¯­éŸ³è½¬æ¢ï¼ˆVCï¼‰ç”Ÿæˆåˆæˆæ•°æ®æ¥å¢å¼ºè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿçš„è®­ç»ƒæ•°æ®å·²ç»å˜å¾—éå¸¸æµè¡Œã€‚å‡ é¡¹ä½œå“å·²ç»è¯æ˜äº†ä½¿ç”¨è¿™ç§å¢å¼ºæ–¹æ³•å¯ä»¥æ”¹è¿›ASRæ€§èƒ½ã€‚ç„¶è€Œï¼Œç”±äºåˆæˆè¯­éŸ³çš„å¤šæ ·æ€§è¾ƒä½ï¼Œç®€å•åœ°ç»“åˆåˆæˆæ•°æ®å’ŒçœŸå®æ•°æ®å¾€å¾€ä¸èƒ½äº§ç”Ÿæœ€ä½³ç»“æœã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨æœ€è¿‘æå‡ºçš„åŸºäºæµçš„TTS&#x2F;VCæ¨¡å‹ï¼Œå…è®¸æ›´å¤§çš„è¯­éŸ³å¤šæ ·æ€§ï¼Œå¹¶è¯„ä¼°å¢å¼ºå„ç§è¯­éŸ³å±æ€§å¯¹å¤šä¸ªASRæ¨¡å‹å®ç°çš„å•è¯é”™è¯¯ç‡ï¼ˆWERï¼‰çš„ç›¸åº”å½±å“ã€‚åœ¨æˆ‘ä»¬çš„è®¾ç½®ä¸­ï¼Œå‘ç°éŸ³è°ƒå¢å¼ºå’ŒåŸºäºVCçš„æ¼”è®²è€…å¢å¼ºæ•ˆæœä¸ä½³ã€‚è”åˆå¢å¼ºæ‰€æœ‰å…¶ä»–å±æ€§å¯ä»¥é™ä½Common Voiceä¸ŠConformer-Transduceræ¨¡å‹çš„ç›¸å¯¹WER 11%ï¼Œä¸ä»…åœ¨çœŸå®æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒç›¸æ¯”ï¼ŒLibriSpeechä¸Šçš„ç›¸å¯¹WERå¯é™ä½é«˜è¾¾35%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08954v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>ä½¿ç”¨æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰æˆ–è¯­éŸ³è½¬æ¢ï¼ˆVCï¼‰ç”Ÿæˆåˆæˆæ•°æ®æ¥å¢å¼ºè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿçš„è®­ç»ƒæ•°æ®å·²é€æ¸æˆä¸ºè¿‘å¹´æ¥çš„çƒ­é—¨è¶‹åŠ¿ã€‚å¤šé¡¹ç ”ç©¶è¯æ˜ï¼Œä½¿ç”¨è¿™ç§å¢å¼ºæ–¹æ³•å¯ä»¥æé«˜ASRçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç”±äºåˆæˆè¯­éŸ³çš„å¤šæ ·æ€§è¾ƒä½ï¼Œå•çº¯åœ°å°†åˆæˆæ•°æ®ä¸çœŸå®æ•°æ®ç›¸ç»“åˆå¾€å¾€æ— æ³•è·å¾—æœ€ä½³æ•ˆæœã€‚æœ¬ç ”ç©¶åˆ©ç”¨æœ€æ–°æå‡ºçš„åŸºäºæµçš„TTS&#x2F;VCæ¨¡å‹ï¼Œè¯¥æ¨¡å‹å…è®¸æ›´å¤§çš„è¯­éŸ³å¤šæ ·æ€§ï¼Œå¹¶è¯„ä¼°å¢å¼ºå„ç§è¯­éŸ³å±æ€§å¯¹è¯é”™è¯¯ç‡ï¼ˆWERï¼‰çš„å½±å“ã€‚åœ¨æˆ‘ä»¬çš„è®¾ç½®ä¸­ï¼ŒéŸ³è°ƒå¢å¼ºå’ŒåŸºäºVCçš„è¯´è¯äººå¢å¼ºæ•ˆæœä¸ä½³ã€‚è”åˆå¢å¼ºæ‰€æœ‰å…¶ä»–å±æ€§å¯ä½¿Conformer-Transduceræ¨¡å‹çš„WERåœ¨Common Voiceä¸Šç›¸å¯¹é™ä½11%ï¼Œåœ¨LibriSpeechä¸Šç›¸å¯¹é™ä½é«˜è¾¾35%ï¼Œä¸ä»…ä½¿ç”¨çœŸå®æ•°æ®è¿›è¡Œè®­ç»ƒç›¸æ¯”ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>åˆæˆæ•°æ®å¢å¼ºASRè®­ç»ƒæ•°æ®é€æ¸æµè¡Œï¼Œèƒ½æé«˜ASRæ€§èƒ½ã€‚</li>
<li>å•çº¯ç»“åˆåˆæˆæ•°æ®å’ŒçœŸå®æ•°æ®å¹¶ä¸æ€»èƒ½è·å¾—æœ€ä½³ç»“æœã€‚</li>
<li>åˆ©ç”¨åŸºäºæµçš„TTS&#x2F;VCæ¨¡å‹ï¼Œè¯¥æ¨¡å‹å…è®¸æ›´å¤§çš„è¯­éŸ³å¤šæ ·æ€§ã€‚</li>
<li>è¯„ä¼°äº†å¢å¼ºå„ç§è¯­éŸ³å±æ€§å¯¹WERçš„å½±å“ã€‚</li>
<li>éŸ³è°ƒå¢å¼ºå’ŒåŸºäºVCçš„è¯´è¯äººå¢å¼ºåœ¨ç‰¹å®šè®¾ç½®ä¸‹æ•ˆæœä¸ä½³ã€‚</li>
<li>è”åˆå¢å¼ºæ‰€æœ‰å…¶ä»–å±æ€§å¯æ˜¾è‘—æé«˜ASRæ€§èƒ½ï¼Œç›¸å¯¹é™ä½WERã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08954">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fdc7edc0d135c427bd2e41e97709423c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b266de6838750e8598815df51b591dd7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d823cd588fb95609dcfae114ebcf157c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ab6d35773e0ca0121eba9370d764c97.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd3ade09a1ad7eeb7773864c1e648f0f.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Contextual-Speech-Extraction-Leveraging-Textual-History-as-an-Implicit-Cue-for-Target-Speech-Extraction"><a href="#Contextual-Speech-Extraction-Leveraging-Textual-History-as-an-Implicit-Cue-for-Target-Speech-Extraction" class="headerlink" title="Contextual Speech Extraction: Leveraging Textual History as an Implicit   Cue for Target Speech Extraction"></a>Contextual Speech Extraction: Leveraging Textual History as an Implicit   Cue for Target Speech Extraction</h2><p><strong>Authors:Minsu Kim, Rodrigo Mira, Honglie Chen, Stavros Petridis, Maja Pantic</strong></p>
<p>In this paper, we investigate a novel approach for Target Speech Extraction (TSE), which relies solely on textual context to extract the target speech. We refer to this task as Contextual Speech Extraction (CSE). Unlike traditional TSE methods that rely on pre-recorded enrollment utterances, video of the target speakerâ€™s face, spatial information, or other explicit cues to identify the target stream, our proposed method requires only a few turns of previous dialogue (or monologue) history. This approach is naturally feasible in mobile messaging environments where voice recordings are typically preceded by textual dialogue that can be leveraged implicitly. We present three CSE models and analyze their performances on three datasets. Through our experiments, we demonstrate that even when the model relies purely on dialogue history, it can achieve over 90 % accuracy in identifying the correct target stream with only two previous dialogue turns. Furthermore, we show that by leveraging both textual context and enrollment utterances as cues during training, we further enhance our modelâ€™s flexibility and effectiveness, allowing us to use either cue during inference, or combine both for improved performance. Samples and code available on <a target="_blank" rel="noopener" href="https://miraodasilva.github.io/cse-project-page">https://miraodasilva.github.io/cse-project-page</a> . </p>
<blockquote>
<p>åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ä¸€ç§é’ˆå¯¹ç›®æ ‡è¯­éŸ³æå–ï¼ˆTSEï¼‰çš„æ–°å‹æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä»…ä¾èµ–æ–‡æœ¬ä¸Šä¸‹æ–‡æ¥æå–ç›®æ ‡è¯­éŸ³ã€‚æˆ‘ä»¬å°†æ­¤ä»»åŠ¡ç§°ä¸ºä¸Šä¸‹æ–‡è¯­éŸ³æå–ï¼ˆCSEï¼‰ã€‚ä¸ä¼ ç»Ÿçš„TSEæ–¹æ³•ä¸åŒï¼Œè¿™äº›æ–¹æ³•ä¾èµ–äºé¢„å…ˆå½•åˆ¶çš„æ³¨å†Œè¯è¯­ã€ç›®æ ‡è¯´è¯äººçš„é¢éƒ¨è§†é¢‘ã€ç©ºé—´ä¿¡æ¯æˆ–å…¶ä»–æ˜ç¡®çš„çº¿ç´¢æ¥è¯†åˆ«ç›®æ ‡æµï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•åªéœ€è¦å‰å‡ è½®å¯¹è¯ï¼ˆæˆ–ç‹¬ç™½ï¼‰å†å²ã€‚è¿™ç§æ–¹æ³•åœ¨ç§»åŠ¨æ¶ˆæ¯ç¯å¢ƒä¸­è‡ªç„¶è€Œç„¶åœ°å¯è¡Œï¼Œè¯­éŸ³è®°å½•é€šå¸¸å…ˆäºæ–‡æœ¬å¯¹è¯ï¼Œå¯ä»¥éšæ€§åˆ©ç”¨ã€‚æˆ‘ä»¬æå‡ºäº†ä¸‰ç§CSEæ¨¡å‹ï¼Œå¹¶åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šå¯¹å…¶æ€§èƒ½è¿›è¡Œäº†åˆ†æã€‚é€šè¿‡å®éªŒï¼Œæˆ‘ä»¬è¯æ˜å³ä½¿æ¨¡å‹ä»…ä¾èµ–äºå¯¹è¯å†å²ï¼Œåœ¨ä»…ä½¿ç”¨å‰ä¸¤è½®å¯¹è¯çš„æƒ…å†µä¸‹ï¼Œä¹Ÿå¯ä»¥å®ç°è¶…è¿‡90%çš„æ­£ç¡®è¯†åˆ«ç›®æ ‡æµçš„å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œé€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åˆ©ç”¨æ–‡æœ¬ä¸Šä¸‹æ–‡å’Œæ³¨å†Œè¯è¯­ä½œä¸ºçº¿ç´¢ï¼Œå¯ä»¥è¿›ä¸€æ­¥æé«˜æ¨¡å‹çš„çµæ´»æ€§å’Œæœ‰æ•ˆæ€§ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿåœ¨æ¨ç†è¿‡ç¨‹ä¸­ä½¿ç”¨ä»»ä¸€çº¿ç´¢ï¼Œæˆ–ç»“åˆä¸¤è€…ä»¥æé«˜æ€§èƒ½ã€‚ç›¸å…³æ ·æœ¬å’Œä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://miraodasilva.github.io/cse-project-page%E8%8E%B7%E5%8F%96%E3%80%82">https://miraodasilva.github.io/cse-project-pageè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08798v1">PDF</a> Accepted to ICASSP 2025</p>
<p><strong>Summary</strong>ï¼š<br>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç›®æ ‡è¯­éŸ³æå–æ–¹æ³•â€”â€”ä¸Šä¸‹æ–‡è¯­éŸ³æå–ï¼ˆCSEï¼‰ï¼Œè¯¥æ–¹æ³•ä»…ä¾èµ–äºæ–‡æœ¬ä¸Šä¸‹æ–‡æ¥æå–ç›®æ ‡è¯­éŸ³ï¼Œé€‚ç”¨äºç§»åŠ¨é€šè®¯ç¯å¢ƒä¸‹çš„æ–‡æœ¬å¯¹è¯å’Œå£°éŸ³æ•°æ®çš„æ— ç¼äº¤äº’åœºæ™¯ã€‚ç›¸è¾ƒäºä¼ ç»Ÿçš„ç›®æ ‡è¯­éŸ³æå–æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ— éœ€é¢„å½•çš„æ³¨å†Œè¯­éŸ³ã€ç›®æ ‡è¯´è¯äººçš„é¢éƒ¨è§†é¢‘ã€ç©ºé—´ä¿¡æ¯æˆ–å…¶ä»–æ˜ç¡®çº¿ç´¢ï¼Œä»…ä¾èµ–å¯¹è¯å†å²å³å¯å®ç°è¶…è¿‡90%çš„å‡†ç¡®è¯†åˆ«ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å±•ç¤ºäº†ç»“åˆæ–‡æœ¬ä¸Šä¸‹æ–‡å’Œæ³¨å†Œè¯­éŸ³ä½œä¸ºçº¿ç´¢è¿›è¡Œè®­ç»ƒçš„æ–¹æ³•ï¼Œæé«˜äº†æ¨¡å‹çš„çµæ´»æ€§å’Œæœ‰æ•ˆæ€§ï¼Œå¯åœ¨æ¨ç†è¿‡ç¨‹ä¸­å•ç‹¬ä½¿ç”¨æˆ–ç»“åˆä¸¤ç§çº¿ç´¢å®ç°æ›´ä½³æ€§èƒ½ã€‚è¯¥æ–¹æ³•çš„æ ·å“å’Œä»£ç å¯è®¿é—®[<a target="_blank" rel="noopener" href="https://miraodasilva.github.io/cse-project-page%E4%BA%86%E8%A7%A3%E8%AF%A6%E7%BB%86%E4%BF%A1%E6%81%AF]%E3%80%82">https://miraodasilva.github.io/cse-project-pageäº†è§£è¯¦ç»†ä¿¡æ¯]ã€‚</a></p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç›®æ ‡è¯­éŸ³æå–æ–¹æ³•â€”â€”ä¸Šä¸‹æ–‡è¯­éŸ³æå–ï¼ˆCSEï¼‰ã€‚</li>
<li>è¯¥æ–¹æ³•ä»…ä¾èµ–äºæ–‡æœ¬ä¸Šä¸‹æ–‡è¿›è¡Œç›®æ ‡è¯­éŸ³æå–ã€‚</li>
<li>å¯¹æ¯”ä¼ ç»Ÿæ–¹æ³•ï¼Œæ–°æ–¹æ³•æ— éœ€é¢„å½•çš„æ³¨å†Œè¯­éŸ³ç­‰æ˜ç¡®çº¿ç´¢ï¼Œä»…ä¾èµ–å¯¹è¯å†å²å³å¯å®ç°é«˜å‡†ç¡®è¯†åˆ«ã€‚</li>
<li>æ–°æ–¹æ³•åœ¨å®éªŒä¸­çš„å‡†ç¡®è¯†åˆ«ç‡è¶…è¿‡90%ã€‚</li>
<li>ç ”ç©¶å±•ç¤ºäº†ç»“åˆæ–‡æœ¬ä¸Šä¸‹æ–‡å’Œæ³¨å†Œè¯­éŸ³ä½œä¸ºè®­ç»ƒçº¿ç´¢çš„æ–¹æ³•ï¼Œæé«˜æ¨¡å‹çš„çµæ´»æ€§å’Œæœ‰æ•ˆæ€§ã€‚</li>
<li>æ¨¡å‹å¯ä»¥åœ¨æ¨ç†è¿‡ç¨‹ä¸­ä½¿ç”¨å•ä¸€çº¿ç´¢æˆ–ç»“åˆä¸¤ç§çº¿ç´¢ï¼Œå®ç°æ›´ä½³æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08798">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-18ee554dcc000499b502f31d7365cd93.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50f0b68b383c002dff2c5cf758d4e26b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0e2e2177f57a3f391fc1646c183be065.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fcbceab6ddd676590b24c038ab08330f.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Prompt2LVideos-Exploring-Prompts-for-Understanding-Long-Form-Multimodal-Videos"><a href="#Prompt2LVideos-Exploring-Prompts-for-Understanding-Long-Form-Multimodal-Videos" class="headerlink" title="Prompt2LVideos: Exploring Prompts for Understanding Long-Form Multimodal   Videos"></a>Prompt2LVideos: Exploring Prompts for Understanding Long-Form Multimodal   Videos</h2><p><strong>Authors:Soumya Shamarao Jahagirdar, Jayasree Saha, C V Jawahar</strong></p>
<p>Learning multimodal video understanding typically relies on datasets comprising video clips paired with manually annotated captions. However, this becomes even more challenging when dealing with long-form videos, lasting from minutes to hours, in educational and news domains due to the need for more annotators with subject expertise. Hence, there arises a need for automated solutions. Recent advancements in Large Language Models (LLMs) promise to capture concise and informative content that allows the comprehension of entire videos by leveraging Automatic Speech Recognition (ASR) and Optical Character Recognition (OCR) technologies. ASR provides textual content from audio, while OCR extracts textual content from specific frames. This paper introduces a dataset comprising long-form lectures and news videos. We present baseline approaches to understand their limitations on this dataset and advocate for exploring prompt engineering techniques to comprehend long-form multimodal video datasets comprehensively. </p>
<blockquote>
<p>å­¦ä¹ å¤šæ¨¡æ€è§†é¢‘ç†è§£é€šå¸¸ä¾èµ–äºç”±è§†é¢‘ç‰‡æ®µå’Œæ‰‹åŠ¨æ³¨é‡Šçš„æ ‡é¢˜ç»„æˆçš„æ•°æ®é›†ã€‚ç„¶è€Œï¼Œåœ¨å¤„ç†æ•™è‚²å’Œæ–°é¢†åŸŸçš„é•¿è§†é¢‘æ—¶ï¼Œç”±äºéœ€è¦æ›´å¤šå…·æœ‰ä¸“ä¸šçŸ¥è¯†çš„æ³¨é‡Šè€…ï¼Œè¿™å˜å¾—æ›´åŠ å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œè¿™äº›è§†é¢‘æ—¶é•¿ä»å‡ åˆ†é’Ÿåˆ°å‡ å°æ—¶ä¸ç­‰ã€‚å› æ­¤ï¼Œéœ€è¦è‡ªåŠ¨è§£å†³æ–¹æ¡ˆã€‚æœ€è¿‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥æ˜¾ç¤ºå‡ºèƒ½å¤Ÿæ•æ‰ç®€æ´ä¸”ä¿¡æ¯ä¸°å¯Œçš„å†…å®¹ï¼Œåˆ©ç”¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å’Œå…‰å­¦å­—ç¬¦è¯†åˆ«ï¼ˆOCRï¼‰æŠ€æœ¯ï¼Œé€šè¿‡ç†è§£æ•´ä¸ªè§†é¢‘çš„å†…å®¹å®ç°è§†é¢‘çš„ç†è§£ã€‚ASRä»éŸ³é¢‘ä¸­æä¾›æ–‡æœ¬å†…å®¹ï¼Œè€ŒOCRåˆ™ä»ç‰¹å®šå¸§ä¸­æå–æ–‡æœ¬å†…å®¹ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåŒ…å«é•¿è®²åº§å’Œæ–°é—»è§†é¢‘çš„æ•°æ®é›†ã€‚æˆ‘ä»¬ä»‹ç»äº†åœ¨è¿™ä¸ªæ•°æ®é›†ä¸Šç†è§£å…¶å±€é™æ€§çš„åŸºçº¿æ–¹æ³•ï¼Œå¹¶æå€¡æ¢ç´¢æç¤ºå·¥ç¨‹æŠ€æœ¯ï¼Œä»¥å…¨é¢ç†è§£é•¿å½¢å¼çš„å¤šæ¨¡æ€è§†é¢‘æ•°æ®é›†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08335v1">PDF</a> CVIP 2024</p>
<p><strong>Summary</strong></p>
<p>éšç€é•¿è§†é¢‘å†…å®¹çš„æ™®åŠï¼Œç‰¹åˆ«æ˜¯åœ¨æ•™è‚²å’Œæ–°é—»é¢†åŸŸï¼Œæ‰‹åŠ¨æ ‡æ³¨è§†é¢‘å†…å®¹çš„ä»»åŠ¡å˜å¾—è¶Šæ¥è¶Šå¤æ‚å’Œè€—æ—¶ã€‚å› æ­¤ï¼Œéœ€è¦è‡ªåŠ¨åŒ–è§£å†³æ–¹æ¡ˆã€‚æœ¬æ–‡å¼•å…¥äº†ä¸€ç§åŒ…å«é•¿è®²åº§å’Œæ–°é—»è§†é¢‘çš„æ•°æ®é›†ï¼Œå¹¶æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è‡ªåŠ¨è¯†åˆ«å’Œè¯†åˆ«è§†é¢‘å†…å®¹çš„æŠ€æœ¯ï¼Œä¸ºç†è§£é•¿è§†é¢‘æä¾›äº†æœ‰æ•ˆçš„å·¥å…·ã€‚æ–‡ç« ä¸»å¼ é‡‡ç”¨æç¤ºå·¥ç¨‹æŠ€æœ¯ï¼Œä»¥ä¾¿æ›´å…¨é¢åœ°ç†è§£é•¿æ ¼å¼çš„å¤šæ¨¡æ€è§†é¢‘æ•°æ®é›†ã€‚è¿™ä¸€æŠ€æœ¯åœ¨æ•™è‚²æŠ€æœ¯å’Œäººå·¥æ™ºèƒ½é¢†åŸŸçš„åº”ç”¨æ½œåŠ›å·¨å¤§ã€‚æœ¬æ–‡åŸºäºè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å’Œå…‰å­¦å­—ç¬¦è¯†åˆ«ï¼ˆOCRï¼‰æŠ€æœ¯å®ç°äº†ä»è§†é¢‘éŸ³é¢‘å’Œè§†é¢‘å¸§ä¸­æå–æ–‡æœ¬å†…å®¹çš„ç›®æ ‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>é•¿æ ¼å¼è§†é¢‘åœ¨æ•™è‚²å’Œæ–°é—»é¢†åŸŸçš„éœ€æ±‚å¢é•¿å¯¼è‡´äº†å¯¹è‡ªåŠ¨åŒ–è§£å†³æ–¹æ¡ˆçš„éœ€æ±‚ï¼Œä»¥ç®€åŒ–æ‰‹åŠ¨æ ‡æ³¨è§†é¢‘å†…å®¹çš„ä»»åŠ¡ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç†è§£é•¿æ ¼å¼å¤šæ¨¡æ€è§†é¢‘å†…å®¹æ–¹é¢å…·æœ‰æ½œåŠ›ã€‚</li>
<li>è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å’Œå…‰å­¦å­—ç¬¦è¯†åˆ«ï¼ˆOCRï¼‰æŠ€æœ¯è¢«ç”¨äºä»è§†é¢‘ä¸­æå–æ–‡æœ¬å†…å®¹ã€‚</li>
<li>ASRèƒ½å¤Ÿä»è§†é¢‘éŸ³é¢‘ä¸­æå–æ–‡æœ¬å†…å®¹ï¼Œè€ŒOCRåˆ™ä»ç‰¹å®šçš„è§†é¢‘å¸§ä¸­æå–æ–‡æœ¬ä¿¡æ¯ã€‚</li>
<li>æœ¬æ–‡ä»‹ç»äº†åŒ…å«é•¿è®²åº§å’Œæ–°é—»è§†é¢‘çš„æ•°æ®é›†ï¼Œå¹¶æå‡ºäº†åŸºå‡†æ–¹æ³•æ¥ç†è§£å…¶åœ¨è¯¥æ•°æ®é›†ä¸Šçš„å±€é™æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08335">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fbe6c95c1b4dbdb0ab65d7d1344f1b36.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b657b10c8d2e5e2c6f07564b881637c9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f5023425b67835ffb82a8e4a23f79007.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-25ab7483b438218d293ad49505a415f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8cd36fa92671f3f4be62c16689bde665.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Enhancing-Multilingual-Language-Models-for-Code-Switched-Input-Data"><a href="#Enhancing-Multilingual-Language-Models-for-Code-Switched-Input-Data" class="headerlink" title="Enhancing Multilingual Language Models for Code-Switched Input Data"></a>Enhancing Multilingual Language Models for Code-Switched Input Data</h2><p><strong>Authors:Katherine Xie, Nitya Babbar, Vicky Chen, Yoanna Turura</strong></p>
<p>Code-switching, or alternating between languages within a single conversation, presents challenges for multilingual language models on NLP tasks. This research investigates if pre-training Multilingual BERT (mBERT) on code-switched datasets improves the modelâ€™s performance on critical NLP tasks such as part of speech tagging, sentiment analysis, named entity recognition, and language identification. We use a dataset of Spanglish tweets for pre-training and evaluate the pre-trained model against a baseline model.   Our findings show that our pre-trained mBERT model outperforms or matches the baseline model in the given tasks, with the most significant improvements seen for parts of speech tagging. Additionally, our latent analysis uncovers more homogenous English and Spanish embeddings for language identification tasks, providing insights for future modeling work.   This research highlights potential for adapting multilingual LMs for code-switched input data in order for advanced utility in globalized and multilingual contexts. Future work includes extending experiments to other language pairs, incorporating multiform data, and exploring methods for better understanding context-dependent code-switches. </p>
<blockquote>
<p>ä»£ç åˆ‡æ¢æˆ–åœ¨å•ä¸ªå¯¹è¯ä¸­ä½¿ç”¨å¤šç§è¯­è¨€ï¼Œå¯¹å¤šè¯­è¨€è¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸Šæå‡ºäº†æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æ—¨åœ¨è°ƒæŸ¥é¢„è®­ç»ƒå¤šè¯­è¨€BERTï¼ˆmBERTï¼‰åœ¨ä»£ç åˆ‡æ¢æ•°æ®é›†ä¸Šæ˜¯å¦èƒ½åœ¨å…³é”®çš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼ˆå¦‚è¯æ€§æ ‡æ³¨ã€æƒ…æ„Ÿåˆ†æã€å‘½åå®ä½“è¯†åˆ«å’Œè¯­è¨€è¯†åˆ«ç­‰ï¼‰ä¸Šæé«˜æ¨¡å‹æ€§èƒ½ã€‚æˆ‘ä»¬ä½¿ç”¨è¥¿ç­ç‰™è¯­å’Œè‹±è¯­æ··åˆæ¨æ–‡çš„æ•°æ®é›†è¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶å¯¹é¢„è®­ç»ƒæ¨¡å‹ä¸åŸºçº¿æ¨¡å‹è¿›è¡Œè¯„ä¼°ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé¢„è®­ç»ƒmBERTæ¨¡å‹åœ¨ç»™å®šçš„ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜äºæˆ–ç­‰äºåŸºçº¿æ¨¡å‹ï¼Œå…¶ä¸­è¯æ€§æ ‡æ³¨çš„æ”¹è¿›æœ€ä¸ºæ˜¾è‘—ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ½œåœ¨åˆ†ææ­ç¤ºäº†ç”¨äºè¯­è¨€è¯†åˆ«ä»»åŠ¡çš„æ›´ç»Ÿä¸€çš„è‹±è¯­å’Œè¥¿ç­ç‰™è¯­åµŒå…¥ï¼Œä¸ºæœªæ¥å»ºæ¨¡å·¥ä½œæä¾›äº†è§è§£ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†ä¸ºé€‚åº”ä»£ç åˆ‡æ¢è¾“å…¥æ•°æ®çš„å¤šè¯­è¨€è¯­è¨€æ¨¡å‹åœ¨å…¨çƒåŒ–å’Œå¤šè¯­è¨€ç¯å¢ƒçš„é«˜çº§åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚æœªæ¥çš„å·¥ä½œåŒ…æ‹¬å°†å®éªŒæ‰©å±•åˆ°å…¶ä»–è¯­è¨€å¯¹ã€èå…¥å¤šç§å½¢å¼çš„æ•°æ®ï¼Œå¹¶æ¢ç´¢æ›´å¥½åœ°ç†è§£ä¸Šä¸‹æ–‡ç›¸å…³ä»£ç åˆ‡æ¢çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07990v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>é¢„è®­ç»ƒçš„å¤šè¯­è¨€BERTæ¨¡å‹ï¼ˆmBERTï¼‰åœ¨ä»£ç åˆ‡æ¢æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¯ä»¥æ”¹å–„å…¶åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œå¦‚è¯æ€§æ ‡æ³¨ã€æƒ…æ„Ÿåˆ†æã€å®ä½“å‘½åè¯†åˆ«å’Œè¯­è¨€è¯†åˆ«ç­‰ã€‚æœ¬ç ”ç©¶ä½¿ç”¨è¥¿ç­ç‰™è‹±è¯­æ¨ç‰¹æ•°æ®é›†è¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶ä¸åŸºçº¿æ¨¡å‹è¿›è¡Œè¯„ä¼°ã€‚ç»“æœæ˜¾ç¤ºï¼Œé¢„è®­ç»ƒçš„mBERTæ¨¡å‹åœ¨ç»™å®šä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºæˆ–ç­‰åŒäºåŸºçº¿æ¨¡å‹ï¼Œå°¤å…¶åœ¨è¯æ€§æ ‡æ³¨æ–¹é¢æœ€ä¸ºæ˜¾è‘—ã€‚æ­¤å¤–ï¼Œæ½œåœ¨åˆ†æå‘ç°è‹±è¯­å’Œè¥¿ç­ç‰™è¯­çš„è¯­è¨€è¯†åˆ«ä»»åŠ¡åµŒå…¥æ›´ä¸ºä¸€è‡´ï¼Œä¸ºæœªæ¥çš„å»ºæ¨¡å·¥ä½œæä¾›äº†å¯ç¤ºã€‚æœ¬ç ”ç©¶çªæ˜¾äº†ä¸ºé€‚åº”ä»£ç åˆ‡æ¢è¾“å…¥æ•°æ®çš„å¤šè¯­è¨€è¯­è¨€æ¨¡å‹åœ¨å…¨çƒåŒ–ã€å¤šè¯­è¨€ç¯å¢ƒä¸­çš„æ½œåœ¨åº”ç”¨ã€‚æœªæ¥çš„ç ”ç©¶æ–¹å‘åŒ…æ‹¬æ‰©å±•åˆ°å…¶ä»–è¯­è¨€å¯¹ã€èå…¥å¤šç§å½¢å¼çš„æ•°æ®å’Œæ¢ç´¢ç†è§£ä¸Šä¸‹æ–‡ç›¸å…³ä»£ç åˆ‡æ¢çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»£ç åˆ‡æ¢å¯¹äºå¤šè¯­è¨€è¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸Šæ„æˆæŒ‘æˆ˜ã€‚</li>
<li>é¢„è®­ç»ƒå¤šè¯­è¨€BERTæ¨¡å‹ï¼ˆmBERTï¼‰åœ¨ä»£ç åˆ‡æ¢æ•°æ®é›†ä¸Šå¯ä»¥æ”¹è¿›æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>ä½¿ç”¨è¥¿ç­ç‰™è‹±è¯­æ¨ç‰¹æ•°æ®é›†è¿›è¡Œé¢„è®­ç»ƒè¯„ä¼°ï¼Œæ˜¾ç¤ºé¢„è®­ç»ƒæ¨¡å‹åœ¨å¤šä¸ªä»»åŠ¡ä¸Šä¼˜äºåŸºçº¿æ¨¡å‹ã€‚</li>
<li>è¯æ€§æ ‡æ³¨ä»»åŠ¡çš„æ”¹è¿›æœ€ä¸ºæ˜¾è‘—ã€‚</li>
<li>æ½œåœ¨åˆ†æå‘ç°è‹±è¯­å’Œè¥¿ç­ç‰™è¯­çš„åµŒå…¥ä¸€è‡´æ€§æ›´é«˜ï¼Œæœ‰åŠ©äºè¯­è¨€è¯†åˆ«ä»»åŠ¡ã€‚</li>
<li>é€‚åº”ä»£ç åˆ‡æ¢è¾“å…¥æ•°æ®çš„å¤šè¯­è¨€è¯­è¨€æ¨¡å‹åœ¨å…¨çƒåŒ–ã€å¤šè¯­è¨€ç¯å¢ƒä¸­å…·æœ‰æ½œåœ¨åº”ç”¨ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07990">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-b7c4ac0801f99c20a2e0f5bd6a2606ee.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c61abe5cf4ba2aae27b07826e33a6a3f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0082de16b4cf157e3b2c07e87ea86f4f.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="CS-Dialogue-A-104-Hour-Dataset-of-Spontaneous-Mandarin-English-Code-Switching-Dialogues-for-Speech-Recognition"><a href="#CS-Dialogue-A-104-Hour-Dataset-of-Spontaneous-Mandarin-English-Code-Switching-Dialogues-for-Speech-Recognition" class="headerlink" title="CS-Dialogue: A 104-Hour Dataset of Spontaneous Mandarin-English   Code-Switching Dialogues for Speech Recognition"></a>CS-Dialogue: A 104-Hour Dataset of Spontaneous Mandarin-English   Code-Switching Dialogues for Speech Recognition</h2><p><strong>Authors:Jiaming Zhou, Yujie Guo, Shiwan Zhao, Haoqin Sun, Hui Wang, Jiabei He, Aobo Kong, Shiyao Wang, Xi Yang, Yequan Wang, Yonghua Lin, Yong Qin</strong></p>
<p>Code-switching (CS), the alternation between two or more languages within a single conversation, presents significant challenges for automatic speech recognition (ASR) systems. Existing Mandarin-English code-switching datasets often suffer from limitations in size, spontaneity, and the lack of full-length dialogue recordings with transcriptions, hindering the development of robust ASR models for real-world conversational scenarios. This paper introduces CS-Dialogue, a novel large-scale Mandarin-English code-switching speech dataset comprising 104 hours of spontaneous conversations from 200 speakers. Unlike previous datasets, CS-Dialogue provides full-length dialogue recordings with complete transcriptions, capturing naturalistic code-switching patterns in continuous speech. We describe the data collection and annotation processes, present detailed statistics of the dataset, and establish benchmark ASR performance using state-of-the-art models. Our experiments, using Transformer, Conformer, and Branchformer, demonstrate the challenges of code-switching ASR, and show that existing pre-trained models such as Whisper still have the space to improve. The CS-Dialogue dataset will be made freely available for all academic purposes. </p>
<blockquote>
<p>è¯­è¨€äº¤æ›¿ï¼ˆCSï¼‰æ˜¯æŒ‡åœ¨å•æ¬¡å¯¹è¯ä¸­ä¸¤ä¸ªæˆ–å¤šä¸ªè¯­è¨€ä¹‹é—´çš„äº¤æ›¿ï¼Œè¿™å¯¹è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿæå‡ºäº†é‡å¤§æŒ‘æˆ˜ã€‚ç°æœ‰çš„æ±‰è¯­-è‹±è¯­äº¤æ›¿æ•°æ®é›†é€šå¸¸åœ¨è§„æ¨¡ã€è‡ªå‘æ€§å’Œç¼ºä¹å¸¦æœ‰è½¬å½•çš„å…¨é•¿å¯¹è¯å½•éŸ³ç­‰æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œé˜»ç¢äº†ä¸ºç°å®ä¸–ç•Œçš„å¯¹è¯åœºæ™¯å¼€å‘ç¨³å¥çš„ASRæ¨¡å‹ã€‚æœ¬æ–‡ä»‹ç»äº†CS-Dialogueï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„å¤§è§„æ¨¡æ±‰è¯­-è‹±è¯­äº¤æ›¿è¯­éŸ³æ•°æ®é›†ï¼ŒåŒ…å«æ¥è‡ª200åå‘è¨€äººçš„104å°æ—¶è‡ªå‘å¯¹è¯ã€‚ä¸ä»¥å‰çš„æ•°æ®é›†ä¸åŒï¼ŒCS-Dialogueæä¾›äº†å¸¦æœ‰å®Œæ•´è½¬å½•çš„å…¨é•¿å¯¹è¯å½•éŸ³ï¼Œæ•æ‰è¿ç»­è¯­éŸ³ä¸­çš„è‡ªç„¶è¯­è¨€äº¤æ›¿æ¨¡å¼ã€‚æˆ‘ä»¬æè¿°äº†æ•°æ®æ”¶é›†å’Œæ³¨é‡Šè¿‡ç¨‹ï¼Œç»™å‡ºäº†æ•°æ®é›†çš„è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯ï¼Œå¹¶ä½¿ç”¨æœ€æ–°æ¨¡å‹å»ºç«‹äº†ASRæ€§èƒ½åŸºå‡†ã€‚æˆ‘ä»¬çš„å®éªŒä½¿ç”¨äº†Transformerã€Conformerå’ŒBranchformerï¼Œå±•ç¤ºäº†è¯­è¨€äº¤æ›¿ASRçš„æŒ‘æˆ˜æ€§ï¼Œå¹¶è¡¨æ˜ç°æœ‰çš„é¢„è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚Whisperï¼‰ä»æœ‰æ”¹è¿›ç©ºé—´ã€‚CS-Dialogueæ•°æ®é›†å°†å…è´¹ä¾›æ‰€æœ‰å­¦æœ¯ç”¨é€”ä½¿ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.18913v2">PDF</a> </p>
<p><strong>æ€»ç»“</strong></p>
<p>è¯¥è®ºæ–‡ä»‹ç»äº†CS-Dialogueæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„ã€æ¶µç›–æ™®é€šè¯ä¸è‹±è¯­äº¤æ›¿ä½¿ç”¨ç°è±¡çš„å¤§å‹æ•°æ®é›†ã€‚å…¶ä¸­åŒ…å«æ¥è‡ª200åè¯´è¯è€…çš„104å°æ—¶è‡ªç„¶å¯¹è¯å½•éŸ³ï¼Œå¹¶é™„æœ‰å®Œæ•´è½¬å½•ã€‚è¯¥æ•°æ®é›†è§£å†³äº†ç°æœ‰æ™®é€šè¯-è‹±è¯­äº¤æ›¿ä½¿ç”¨æ•°æ®é›†å¤§å°æœ‰é™ã€ç¼ºä¹è‡ªå‘æ€§åŠå®Œæ•´å¯¹è¯å½•éŸ³çš„é—®é¢˜ï¼Œæœ‰åŠ©äºå¼€å‘é€‚ç”¨äºçœŸå®å¯¹è¯åœºæ™¯çš„ç¨³å¥è¯­éŸ³è¯†åˆ«æ¨¡å‹ã€‚è®ºæ–‡æè¿°äº†æ•°æ®æ”¶é›†ä¸æ ‡æ³¨è¿‡ç¨‹ï¼Œæä¾›äº†æ•°æ®é›†çš„è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯ï¼Œå¹¶ä½¿ç”¨å‰æ²¿æ¨¡å‹å»ºç«‹äº†åŸºå‡†è¯­éŸ³è¯†åˆ«æ€§èƒ½ã€‚å®éªŒè¡¨æ˜ï¼Œç°æœ‰çš„é¢„è®­ç»ƒæ¨¡å‹å¦‚Whisperåœ¨æ™®é€šè¯-è‹±è¯­äº¤æ›¿ä½¿ç”¨çš„è¯­éŸ³è¯†åˆ«ä¸Šä»æœ‰æå‡ç©ºé—´ã€‚CS-Dialogueæ•°æ®é›†å°†å…è´¹ç”¨äºæ‰€æœ‰å­¦æœ¯ç”¨é€”ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>CS-Dialogueæ˜¯é¦–ä¸ªå¤§è§„æ¨¡çš„æ™®é€šè¯-è‹±è¯­äº¤æ›¿ä½¿ç”¨è¯­éŸ³æ•°æ®é›†ï¼ŒåŒ…å«104å°æ—¶çš„è‡ªå‘å¯¹è¯å½•éŸ³å’Œå®Œæ•´è½¬å½•ã€‚</li>
<li>æ•°æ®é›†è§£å†³äº†ç°æœ‰æ™®é€šè¯-è‹±è¯­äº¤æ›¿ä½¿ç”¨æ•°æ®é›†å¤§å°æœ‰é™ã€ç¼ºä¹è‡ªå‘æ€§åŠå®Œæ•´å¯¹è¯è®°å½•çš„é—®é¢˜ã€‚</li>
<li>æ•°æ®é›†æœ‰åŠ©äºå¼€å‘é€‚ç”¨äºçœŸå®å¯¹è¯åœºæ™¯çš„ç¨³å¥è¯­éŸ³è¯†åˆ«æ¨¡å‹ã€‚</li>
<li>è®ºæ–‡æè¿°äº†æ•°æ®æ”¶é›†ä¸æ ‡æ³¨æµç¨‹ï¼Œå¹¶æä¾›äº†è¯¦ç»†çš„ç»Ÿè®¡æ•°æ®ã€‚</li>
<li>ä½¿ç”¨Transformerã€Conformerå’ŒBranchformerç­‰å…ˆè¿›æ¨¡å‹å»ºç«‹çš„åŸºå‡†è¯­éŸ³è¯†åˆ«æ€§èƒ½è¡¨æ˜äº†ç°æœ‰çš„é¢„è®­ç»ƒæ¨¡å‹åœ¨å¤„ç†æ™®é€šè¯-è‹±è¯­äº¤æ›¿ä½¿ç”¨çš„è¯­éŸ³è¯†åˆ«æ—¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œç°æœ‰çš„é¢„è®­ç»ƒæ¨¡å‹å¦‚Whisperä»æœ‰æ”¹è¿›ç©ºé—´ã€‚</li>
<li>CS-Dialogueæ•°æ®é›†å°†ä¾›æ‰€æœ‰å­¦æœ¯ç”¨é€”å…è´¹ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.18913">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a5d41c0acd651035fb457987bf0cbd52.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48a9ec32dcf88f8c93bd03b14049382c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7f35b68213455057b5ddfdb72d5aef37.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-708d7cb6863592ae4ca5a03617a80d11.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab3f45528557a7893dd8c474d60eaaef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8050fcc384ae9f363eee6e31f83bb6f8.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="HumanVBench-Exploring-Human-Centric-Video-Understanding-Capabilities-of-MLLMs-with-Synthetic-Benchmark-Data"><a href="#HumanVBench-Exploring-Human-Centric-Video-Understanding-Capabilities-of-MLLMs-with-Synthetic-Benchmark-Data" class="headerlink" title="HumanVBench: Exploring Human-Centric Video Understanding Capabilities of   MLLMs with Synthetic Benchmark Data"></a>HumanVBench: Exploring Human-Centric Video Understanding Capabilities of   MLLMs with Synthetic Benchmark Data</h2><p><strong>Authors:Ting Zhou, Daoyuan Chen, Qirui Jiao, Bolin Ding, Yaliang Li, Ying Shen</strong></p>
<p>In the domain of Multimodal Large Language Models (MLLMs), achieving human-centric video understanding remains a formidable challenge. Existing benchmarks primarily emphasize object and action recognition, often neglecting the intricate nuances of human emotions, behaviors, and speech-visual alignment within video content. We present HumanVBench, an innovative benchmark meticulously crafted to bridge these gaps in the evaluation of video MLLMs. HumanVBench comprises 16 carefully designed tasks that explore two primary dimensions: inner emotion and outer manifestations, spanning static and dynamic, basic and complex, as well as single-modal and cross-modal aspects. With two advanced automated pipelines for video annotation and distractor-included QA generation, HumanVBench utilizes diverse state-of-the-art (SOTA) techniques to streamline benchmark data synthesis and quality assessment, minimizing human annotation dependency tailored to human-centric multimodal attributes. A comprehensive evaluation across 22 SOTA video MLLMs reveals notable limitations in current performance, especially in cross-modal and emotion perception, underscoring the necessity for further refinement toward achieving more human-like understanding. HumanVBench is open-sourced to facilitate future advancements and real-world applications in video MLLMs. </p>
<blockquote>
<p>åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰é¢†åŸŸï¼Œå®ç°ä»¥äººç±»ä¸ºä¸­å¿ƒçš„è§†é¢‘ç†è§£ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•ä¸»è¦å¼ºè°ƒå¯¹è±¡å’ŒåŠ¨ä½œè¯†åˆ«ï¼Œå¾€å¾€å¿½è§†äº†è§†é¢‘å†…å®¹ä¸­äººç±»æƒ…ç»ªã€è¡Œä¸ºå’Œè¯­éŸ³è§†è§‰å¯¹é½çš„ç»†å¾®å·®åˆ«ã€‚æˆ‘ä»¬æ¨å‡ºäº†HumanVBenchï¼Œè¿™æ˜¯ä¸€ä¸ªç²¾å¿ƒè®¾è®¡çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨å¼¥è¡¥è§†é¢‘MLLMsè¯„ä¼°ä¸­çš„è¿™äº›å·®è·ã€‚HumanVBenchåŒ…å«16ä¸ªç²¾å¿ƒè®¾è®¡çš„ä»»åŠ¡ï¼Œæ¢ç´¢ä¸¤ä¸ªä¸»è¦ç»´åº¦ï¼šå†…åœ¨æƒ…ç»ªå’Œå¤–åœ¨è¡¨ç°ï¼Œæ¶µç›–é™æ€å’ŒåŠ¨æ€ã€åŸºæœ¬å’Œå¤æ‚ï¼Œä»¥åŠå•æ¨¡æ€å’Œè·¨æ¨¡æ€æ–¹é¢ã€‚HumanVBenchä½¿ç”¨å…ˆè¿›çš„è‡ªåŠ¨åŒ–ç®¡é“è¿›è¡Œè§†é¢‘æ ‡æ³¨å’ŒåŒ…å«å¹²æ‰°é¡¹çš„QAç”Ÿæˆï¼Œåˆ©ç”¨å¤šç§æœ€æ–°æŠ€æœ¯ä¼˜åŒ–åŸºå‡†æ•°æ®åˆæˆå’Œè´¨é‡è¯„ä¼°ï¼Œå‡å°‘å¯¹äººç±»æ ‡æ³¨çš„ä¾èµ–ï¼Œä¸“é—¨é¢å‘ä»¥äººç±»ä¸ºä¸­å¿ƒçš„å¤šæ¨¡æ€å±æ€§ã€‚å¯¹22ä¸ªæœ€æ–°è§†é¢‘MLLMsçš„ç»¼åˆè¯„ä¼°æ˜¾ç¤ºï¼Œå½“å‰æ€§èƒ½å­˜åœ¨æ˜¾è‘—å±€é™ï¼Œç‰¹åˆ«æ˜¯åœ¨è·¨æ¨¡æ€å’Œæƒ…æ„Ÿæ„ŸçŸ¥æ–¹é¢ï¼Œè¿™å¼ºè°ƒäº†å¯¹è¿›ä¸€æ­¥æ”¹è¿›å®ç°æ›´äººæ€§åŒ–çš„ç†è§£çš„å¿…è¦æ€§ã€‚HumanVBenchå¼€æºï¼Œä»¥ä¿ƒè¿›è§†é¢‘MLLMsçš„æœªæ¥å‘å±•å’Œå®é™…åº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17574v2">PDF</a> 22 pages, 23 figures, 7 tables</p>
<p><strong>Summary</strong></p>
<p>åœ¨å¤šåª’ä½“å¤§å‹è¯­è¨€æ¨¡å‹é¢†åŸŸï¼Œå®ç°ä»¥äººç±»ä¸ºä¸­å¿ƒçš„è§†é¢‘ç†è§£æ˜¯ä¸€é¡¹æŒ‘æˆ˜ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•ä¸»è¦ä¾§é‡äºç‰©ä½“å’ŒåŠ¨ä½œè¯†åˆ«ï¼Œå¿½è§†äº†äººç±»æƒ…ç»ªã€è¡Œä¸ºå’Œè§†é¢‘å†…å®¹ä¸­è¯­éŸ³è§†è§‰å¯¹é½çš„ç»†å¾®å·®åˆ«ã€‚æˆ‘ä»¬æ¨å‡ºHumanVBenchåŸºå‡†æµ‹è¯•ï¼Œä»¥å¼¥è¡¥è¿™äº›è¯„ä¼°å·®è·ã€‚HumanVBenchåŒ…å«16é¡¹ç²¾å¿ƒè®¾è®¡çš„ä»»åŠ¡ï¼Œæ¢ç´¢å†…åœ¨æƒ…æ„Ÿå’Œå¤–åœ¨è¡¨ç°ä¸¤ä¸ªä¸»è¦ç»´åº¦ï¼Œæ¶µç›–é™æ€å’ŒåŠ¨æ€ã€åŸºæœ¬å’Œå¤æ‚ï¼Œä»¥åŠå•æ¨¡æ€å’Œè·¨æ¨¡æ€æ–¹é¢ã€‚åˆ©ç”¨ä¸¤ä¸ªå…ˆè¿›çš„è‡ªåŠ¨åŒ–ç®¡é“è¿›è¡Œè§†é¢‘æ ‡æ³¨å’ŒåŒ…å«å¹²æ‰°é¡¹çš„QAç”Ÿæˆï¼ŒHumanVBenchåˆ©ç”¨å¤šç§æœ€æ–°æŠ€æœ¯ç®€åŒ–åŸºå‡†æµ‹è¯•æ•°æ®åˆæˆå’Œè´¨é‡è¯„ä¼°ï¼Œå‡å°‘äººå·¥æ ‡æ³¨çš„ä¾èµ–ï¼Œé’ˆå¯¹ä»¥äººä¸ºä¸­å¿ƒçš„å¤šæ¨¡æ€å±æ€§å®šåˆ¶ã€‚å¯¹22ä¸ªæœ€æ–°è§†é¢‘å¤šåª’ä½“è¯­è¨€æ¨¡å‹çš„å…¨é¢è¯„ä¼°æ˜¾ç¤ºï¼Œåœ¨è·¨æ¨¡æ€å’Œæƒ…æ„Ÿæ„ŸçŸ¥æ–¹é¢å­˜åœ¨æ˜¾è‘—å±€é™æ€§ï¼Œå¼ºè°ƒéœ€è¦è¿›ä¸€æ­¥æ”¹è¿›ä»¥å®ç°æ›´äººæ€§åŒ–çš„ç†è§£ã€‚HumanVBenchå·²å¼€æºï¼Œä»¥ä¿ƒè¿›è§†é¢‘å¤šåª’ä½“è¯­è¨€æ¨¡å‹çš„æœªæ¥å‘å±•å’Œå®é™…åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Human-centric video understanding remains a challenge in Multimodal Large Language Models (MLLMs).</li>
<li>ç°æœ‰åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨ç‰©ä½“å’ŒåŠ¨ä½œè¯†åˆ«ï¼Œå¿½è§†äº†äººç±»æƒ…ç»ªã€è¡Œä¸ºå’Œè¯­éŸ³è§†è§‰å¯¹é½çš„ç»†å¾®å·®åˆ«ã€‚</li>
<li>HumanVBenchåŸºå‡†æµ‹è¯•åŒ…å«16é¡¹ä»»åŠ¡ï¼Œæ¢ç´¢å†…åœ¨æƒ…æ„Ÿå’Œå¤–åœ¨è¡¨ç°ä¸¤ä¸ªä¸»è¦ç»´åº¦ã€‚</li>
<li>HumanVBenchåˆ©ç”¨è‡ªåŠ¨åŒ–ç®¡é“è¿›è¡Œè§†é¢‘æ ‡æ³¨å’ŒQAç”Ÿæˆï¼Œä»¥å‡å°‘äººå·¥æ ‡æ³¨çš„ä¾èµ–ã€‚</li>
<li>è·¨æ¨¡æ€å’Œæƒ…æ„Ÿæ„ŸçŸ¥æ˜¯å½“å‰å¤šåª’ä½“è¯­è¨€æ¨¡å‹åœ¨è§†é¢‘ç†è§£æ–¹é¢çš„æ˜¾è‘—å±€é™æ€§ã€‚</li>
<li>HumanVBenchå·²å¼€æºï¼Œä»¥ä¿ƒè¿›è§†é¢‘å¤šåª’ä½“è¯­è¨€æ¨¡å‹çš„æœªæ¥å‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17574">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9aa31fd256e4e1214999c2c0675e76f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b7dcbd9fac21bc08fff550b2f932ed8b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c27fbacceb72e10420ea5305cd6a90f.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="SemTalk-Holistic-Co-speech-Motion-Generation-with-Frame-level-Semantic-Emphasis"><a href="#SemTalk-Holistic-Co-speech-Motion-Generation-with-Frame-level-Semantic-Emphasis" class="headerlink" title="SemTalk: Holistic Co-speech Motion Generation with Frame-level Semantic   Emphasis"></a>SemTalk: Holistic Co-speech Motion Generation with Frame-level Semantic   Emphasis</h2><p><strong>Authors:Xiangyue Zhang, Jianfang Li, Jiaxu Zhang, Ziqiang Dang, Jianqiang Ren, Liefeng Bo, Zhigang Tu</strong></p>
<p>A good co-speech motion generation cannot be achieved without a careful integration of common rhythmic motion and rare yet essential semantic motion. In this work, we propose SemTalk for holistic co-speech motion generation with frame-level semantic emphasis. Our key insight is to separately learn base motions and sparse motions, and then adaptively fuse them. In particular, coarse2fine cross-attention module and rhythmic consistency learning are explored to establish rhythm-related base motion, ensuring a coherent foundation that synchronizes gestures with the speech rhythm. Subsequently, semantic emphasis learning is designed to generate semantic-aware sparse motion, focusing on frame-level semantic cues. Finally, to integrate sparse motion into the base motion and generate semantic-emphasized co-speech gestures, we further leverage a learned semantic score for adaptive synthesis. Qualitative and quantitative comparisons on two public datasets demonstrate that our method outperforms the state-of-the-art, delivering high-quality co-speech motion with enhanced semantic richness over a stable base motion. </p>
<blockquote>
<p>è‰¯å¥½çš„ååŒè¯­éŸ³åŠ¨ä½œç”Ÿæˆç¦»ä¸å¼€å¸¸è§çš„èŠ‚å¥åŠ¨ä½œå’Œç½•è§ä½†å¿…è¦çš„è¯­ä¹‰åŠ¨ä½œçš„ä»”ç»†èåˆã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ç”¨äºæ•´ä½“ååŒè¯­éŸ³åŠ¨ä½œç”Ÿæˆçš„SemTalkæ–¹æ³•ï¼Œå…·æœ‰å¸§çº§è¯­ä¹‰å¼ºè°ƒã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯åˆ†åˆ«å­¦ä¹ åŸºç¡€åŠ¨ä½œå’Œç¨€ç–åŠ¨ä½œï¼Œç„¶åè‡ªé€‚åº”åœ°èåˆå®ƒä»¬ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬æ¢ç´¢äº†coarse2fineäº¤å‰æ³¨æ„åŠ›æ¨¡å—å’ŒèŠ‚å¥ä¸€è‡´æ€§å­¦ä¹ æ¥å»ºç«‹ä¸èŠ‚å¥ç›¸å…³çš„åŸºæœ¬åŠ¨ä½œï¼Œç¡®ä¿ä¸è¯­éŸ³èŠ‚å¥åŒæ­¥çš„æ‰‹åŠ¿æœ‰ä¸€ä¸ªè¿è´¯çš„åŸºç¡€ã€‚éšåï¼Œè®¾è®¡è¯­ä¹‰å¼ºè°ƒå­¦ä¹ æ¥ç”Ÿæˆå…·æœ‰è¯­ä¹‰æ„ŸçŸ¥çš„ç¨€ç–åŠ¨ä½œï¼Œä¾§é‡äºå¸§çº§è¯­ä¹‰çº¿ç´¢ã€‚æœ€åï¼Œä¸ºäº†å°†ç¨€ç–åŠ¨ä½œèå…¥åŸºç¡€åŠ¨ä½œä¸­ï¼Œç”Ÿæˆå…·æœ‰è¯­ä¹‰å¼ºè°ƒçš„ååŒè¯­éŸ³æ‰‹åŠ¿ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥åˆ©ç”¨å­¦ä¹ å¾—åˆ°çš„è¯­ä¹‰åˆ†æ•°è¿›è¡Œè‡ªé€‚åº”åˆæˆã€‚åœ¨ä¸¤é¡¹å…¬å…±æ•°æ®é›†ä¸Šçš„å®šæ€§å’Œå®šé‡æ¯”è¾ƒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œæä¾›é«˜è´¨é‡çš„ååŒè¯­éŸ³åŠ¨ä½œï¼Œåœ¨ç¨³å®šçš„åŸºç¡€åŠ¨ä½œä¸Šå¢åŠ äº†è¯­ä¹‰ä¸°å¯Œæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16563v3">PDF</a> 11 pages, 8 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºSemTalkçš„ååŒè¯­éŸ³è¿åŠ¨ç”Ÿæˆæ–¹æ³•ï¼Œè¯¥æ–¹æ³•èåˆäº†å¸¸è§„èŠ‚å¥è¿åŠ¨å’Œç½•è§ä½†é‡è¦çš„è¯­ä¹‰è¿åŠ¨ã€‚é€šè¿‡åˆ†åˆ«å­¦ä¹ åŸºç¡€è¿åŠ¨å’Œç¨€ç–è¿åŠ¨ï¼Œç„¶åè‡ªé€‚åº”åœ°èåˆå®ƒä»¬ã€‚é‡‡ç”¨coarse2fineäº¤å‰æ³¨æ„åŠ›æ¨¡å—å’ŒèŠ‚å¥ä¸€è‡´æ€§å­¦ä¹ æ¥å»ºç«‹ä¸èŠ‚å¥ç›¸å…³çš„åŸºç¡€è¿åŠ¨ï¼Œç¡®ä¿æ‰‹åŠ¿ä¸è¯­éŸ³èŠ‚å¥çš„åŒæ­¥ã€‚æ­¤å¤–ï¼Œè®¾è®¡äº†è¯­ä¹‰å¼ºè°ƒå­¦ä¹ æ¥ç”Ÿæˆå…·æœ‰è¯­ä¹‰æ„è¯†çš„ç¨€ç–è¿åŠ¨ï¼Œé‡ç‚¹å…³æ³¨å¸§çº§è¯­ä¹‰çº¿ç´¢ã€‚æœ€åï¼Œé€šè¿‡è‡ªé€‚åº”åˆæˆå°†ç¨€ç–è¿åŠ¨èå…¥åŸºç¡€è¿åŠ¨ï¼Œç”Ÿæˆå…·æœ‰è¯­ä¹‰å¼ºè°ƒçš„ååŒè¯­éŸ³æ‰‹åŠ¿ã€‚æœ¬æ–‡æ–¹æ³•åœ¨ä¸¤ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œç”Ÿæˆäº†é«˜è´¨é‡ä¸”è¯­ä¹‰ä¸°å¯Œçš„ååŒè¯­éŸ³è¿åŠ¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ååŒè¯­éŸ³è¿åŠ¨ç”Ÿæˆéœ€è¦èåˆå¸¸è§„èŠ‚å¥è¿åŠ¨å’Œè¯­ä¹‰è¿åŠ¨ã€‚</li>
<li>æå‡ºSemTalkæ–¹æ³•ï¼Œåˆ†åˆ«å­¦ä¹ åŸºç¡€è¿åŠ¨å’Œç¨€ç–è¿åŠ¨ï¼Œå¹¶è‡ªé€‚åº”èåˆã€‚</li>
<li>é‡‡ç”¨coarse2fineäº¤å‰æ³¨æ„åŠ›æ¨¡å—å»ºç«‹ä¸è¯­éŸ³èŠ‚å¥ç›¸å…³çš„åŸºç¡€è¿åŠ¨ã€‚</li>
<li>è¯­ä¹‰å¼ºè°ƒå­¦ä¹ ç”¨äºç”Ÿæˆå¸§çº§è¯­ä¹‰çº¿ç´¢çš„ç¨€ç–è¿åŠ¨ã€‚</li>
<li>é€šè¿‡è‡ªé€‚åº”åˆæˆå°†ç¨€ç–è¿åŠ¨èå…¥åŸºç¡€è¿åŠ¨ã€‚</li>
<li>æ–¹æ³•åœ¨å…¬å…±æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç”Ÿæˆé«˜è´¨é‡ä¸”è¯­ä¹‰ä¸°å¯Œçš„ååŒè¯­éŸ³è¿åŠ¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16563">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c6d57512a50872a657af04855f7bd13e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7bdea21775e414743083889bac9441db.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3893daeab405fe5406b44cc111d176a6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-299f2d0ec51f64a4202160b96b71ef26.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6632f41b5354ba80da2d8cb01d69bb63.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Biodenoising-Animal-Vocalization-Denoising-without-Access-to-Clean-Data"><a href="#Biodenoising-Animal-Vocalization-Denoising-without-Access-to-Clean-Data" class="headerlink" title="Biodenoising: Animal Vocalization Denoising without Access to Clean Data"></a>Biodenoising: Animal Vocalization Denoising without Access to Clean Data</h2><p><strong>Authors:Marius Miron, Sara Keen, Jen-Yu Liu, Benjamin Hoffman, Masato Hagiwara, Olivier Pietquin, Felix Effenberger, Maddie Cusimano</strong></p>
<p>Animal vocalization denoising is a task similar to human speech enhancement, which is relatively well-studied. In contrast to the latter, it comprises a higher diversity of sound production mechanisms and recording environments, and this higher diversity is a challenge for existing models. Adding to the challenge and in contrast to speech, we lack large and diverse datasets comprising clean vocalizations. As a solution we use as training data pseudo-clean targets, i.e. pre-denoised vocalizations, and segments of background noise without a vocalization. We propose a train set derived from bioacoustics datasets and repositories representing diverse species, acoustic environments, geographic regions. Additionally, we introduce a non-overlapping benchmark set comprising clean vocalizations from different taxa and noise samples. We show that that denoising models (demucs, CleanUNet) trained on pseudo-clean targets obtained with speech enhancement models achieve competitive results on the benchmarking set. We publish data, code, libraries, and demos at <a target="_blank" rel="noopener" href="https://earthspecies.github.io/biodenoising/">https://earthspecies.github.io/biodenoising/</a>. </p>
<blockquote>
<p>åŠ¨ç‰©å‘å£°å»å™ªä¸äººç±»è¯­éŸ³å¢å¼ºä»»åŠ¡ç±»ä¼¼ï¼Œå·²ç»å¾—åˆ°äº†è¾ƒä¸ºå……åˆ†çš„ç ”ç©¶ã€‚ç„¶è€Œï¼Œä¸ä¹‹ç›¸æ¯”ï¼ŒåŠ¨ç‰©å‘å£°å»å™ªåŒ…å«äº†æ›´å¤šæ ·åŒ–çš„å£°éŸ³äº§ç”Ÿæœºåˆ¶å’Œå½•éŸ³ç¯å¢ƒï¼Œè¿™ç»™ç°æœ‰æ¨¡å‹å¸¦æ¥äº†æ›´å¤§çš„æŒ‘æˆ˜ã€‚é™¤äº†è¿™äº›æŒ‘æˆ˜ä¹‹å¤–ï¼Œä¸è¯­éŸ³ä¸åŒï¼Œæˆ‘ä»¬è¿˜ç¼ºä¹åŒ…å«æ¸…æ™°å‘å£°çš„å¤§å‹å¤šæ ·åŒ–æ•°æ®é›†ã€‚ä½œä¸ºè§£å†³æ–¹æ¡ˆï¼Œæˆ‘ä»¬ä½¿ç”¨ä¼ªæ¸…æ´ç›®æ ‡ä½œä¸ºè®­ç»ƒæ•°æ®ï¼Œå³é¢„å»å™ªå‘å£°å’Œæ— å‘å£°çš„èƒŒæ™¯å™ªå£°ç‰‡æ®µã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŸºäºç”Ÿç‰©å£°å­¦æ•°æ®é›†å’Œå­˜å‚¨åº“çš„è®­ç»ƒé›†ï¼Œè¿™äº›æ•°æ®é›†æ¶µç›–äº†å¤šç§ç‰©ç§ã€å£°å­¦ç¯å¢ƒå’Œåœ°ç†åŒºåŸŸã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªä¸åŒ…å«é‡å çš„åŸºå‡†æµ‹è¯•é›†ï¼Œå…¶ä¸­åŒ…æ‹¬æ¥è‡ªä¸åŒåˆ†ç±»ç¾¤ç»„çš„æ¸…æ´å‘å£°å’Œå™ªå£°æ ·æœ¬ã€‚æˆ‘ä»¬è¯æ˜ï¼Œä½¿ç”¨è¯­éŸ³å¢å¼ºæ¨¡å‹è·å¾—çš„ä¼ªæ¸…æ´ç›®æ ‡è¿›è¡Œè®­ç»ƒçš„å»å™ªæ¨¡å‹ï¼ˆå¦‚demucså’ŒCleanUNetï¼‰åœ¨åŸºå‡†æµ‹è¯•é›†ä¸Šå–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://earthspecies.github.io/biodenoising/%E5%8F%91%E5%B8%83%E6%95%B0%E6%8D%AE%E3%80%81%E4%BB%A3%E7%A0%81%E3%80%81%E5%BA%93%E5%92%8C%E6%BC%94%E7%A4%BA%E3%80%82">https://earthspecies.github.io/biodenoising/å‘å¸ƒæ•°æ®ã€ä»£ç ã€åº“å’Œæ¼”ç¤ºã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.03427v3">PDF</a> 5 pages, 2 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŠ¨ç‰©å£°éŸ³å»å™ªä»»åŠ¡é¢ä¸´çš„æŒ‘æˆ˜å’Œè§£å†³æ–¹æ¡ˆã€‚ç”±äºåŠ¨ç‰©å£°éŸ³äº§ç”Ÿæœºåˆ¶å’Œå½•éŸ³ç¯å¢ƒå¤šæ ·ï¼Œç°æœ‰æ¨¡å‹éš¾ä»¥åº”å¯¹ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿä½¿ç”¨ä¼ªæ¸…æ´ç›®æ ‡ä½œä¸ºè®­ç»ƒæ•°æ®ï¼Œå¹¶å¼•å…¥ç”Ÿç‰©å£°å­¦æ•°æ®é›†å’Œä»£è¡¨ä¸åŒç‰©ç§ã€å£°å­¦ç¯å¢ƒå’Œåœ°ç†åŒºåŸŸçš„å­˜å‚¨åº“æ„å»ºè®­ç»ƒé›†ã€‚æ­¤å¤–ï¼Œä»–ä»¬è¿˜æ¨å‡ºä¸€ä¸ªåŒ…å«ä¸åŒç¨ç§æ¸…æ´å‘å£°å’Œå™ªå£°æ ·æœ¬çš„éé‡å åŸºå‡†æµ‹è¯•é›†ã€‚é€šè¿‡è®­ç»ƒä¼ªæ¸…æ´ç›®æ ‡ä¸Šçš„å»å™ªæ¨¡å‹ï¼Œå¦‚demucså’ŒCleanUNetï¼Œåœ¨åŸºå‡†æµ‹è¯•é›†ä¸Šå–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚ç›¸å…³æ•°æ®å’Œä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://earthspecies.github.io/biodenoising/%E4%B8%8A%E3%80%82">https://earthspecies.github.io/biodenoising/ä¸Šã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŠ¨ç‰©å£°éŸ³å»å™ªä¸äººç±»è¯­éŸ³å¢å¼ºç±»ä¼¼ï¼Œä½†é¢ä¸´æ›´é«˜çš„å£°éŸ³å¤šæ ·æ€§å’Œç¼ºä¹å¤§å‹å¤šå…ƒæ¸…æ´å‘å£°æ•°æ®é›†æŒ‘æˆ˜ã€‚</li>
<li>ä¸ºè§£å†³æ•°æ®ç¼ºä¹é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿä½¿ç”¨ä¼ªæ¸…æ´ç›®æ ‡å’ŒèƒŒæ™¯å™ªå£°ä½œä¸ºè®­ç»ƒæ•°æ®ã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿæ„å»ºäº†ä¸€ä¸ªåŒ…å«å¤šç§ç‰©ç§ã€å£°å­¦ç¯å¢ƒå’Œåœ°ç†åŒºåŸŸçš„è®­ç»ƒé›†ã€‚</li>
<li>å¼•å…¥éé‡å åŸºå‡†æµ‹è¯•é›†ï¼ŒåŒ…å«æ¸…æ´å‘å£°å’Œå™ªå£°æ ·æœ¬ã€‚</li>
<li>åŸºäºä¼ªæ¸…æ´ç›®æ ‡çš„å»å™ªæ¨¡å‹ï¼ˆå¦‚demucså’ŒCleanUNetï¼‰åœ¨åŸºå‡†æµ‹è¯•é›†ä¸Šè¡¨ç°è‰¯å¥½ã€‚</li>
<li>è¯¥ç ”ç©¶å›¢é˜Ÿæä¾›äº†åŠ¨ç‰©å£°éŸ³å»å™ªçš„æ•°æ®é›†ã€ä»£ç åº“å’Œæ¼”ç¤ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.03427">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b76f5fe4ab822a368ca1f34036129d09.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-568202a7d1ee5f05860fbb60daf870a3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6369b484852ea57384b745db07b08f17.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="M2R-Whisper-Multi-stage-and-Multi-scale-Retrieval-Augmentation-for-Enhancing-Whisper"><a href="#M2R-Whisper-Multi-stage-and-Multi-scale-Retrieval-Augmentation-for-Enhancing-Whisper" class="headerlink" title="M2R-Whisper: Multi-stage and Multi-scale Retrieval Augmentation for   Enhancing Whisper"></a>M2R-Whisper: Multi-stage and Multi-scale Retrieval Augmentation for   Enhancing Whisper</h2><p><strong>Authors:Jiaming Zhou, Shiwan Zhao, Jiabei He, Hui Wang, Wenjia Zeng, Yong Chen, Haoqin Sun, Aobo Kong, Yong Qin</strong></p>
<p>State-of-the-art models like OpenAIâ€™s Whisper exhibit strong performance in multilingual automatic speech recognition (ASR), but they still face challenges in accurately recognizing diverse subdialects. In this paper, we propose M2R-whisper, a novel multi-stage and multi-scale retrieval augmentation approach designed to enhance ASR performance in low-resource settings. Building on the principles of in-context learning (ICL) and retrieval-augmented techniques, our method employs sentence-level ICL in the pre-processing stage to harness contextual information, while integrating token-level k-Nearest Neighbors (kNN) retrieval as a post-processing step to further refine the final output distribution. By synergistically combining sentence-level and token-level retrieval strategies, M2R-whisper effectively mitigates various types of recognition errors. Experiments conducted on Mandarin and subdialect datasets, including AISHELL-1 and KeSpeech, demonstrate substantial improvements in ASR accuracy, all achieved without any parameter updates. </p>
<blockquote>
<p>å½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œå¦‚OpenAIçš„Whisperï¼Œåœ¨å¤šè¯­ç§è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†å®ƒä»¬åœ¨è¯†åˆ«å¤šæ ·çš„æ¬¡æ–¹è¨€æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†M2R-whisperï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å¤šé˜¶æ®µå¤šå°ºåº¦æ£€ç´¢å¢å¼ºæ–¹æ³•ï¼Œæ—¨åœ¨å¢å¼ºä½èµ„æºç¯å¢ƒä¸‹çš„ASRæ€§èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŸºäºä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰å’Œæ£€ç´¢å¢å¼ºæŠ€æœ¯ï¼Œåœ¨é¢„å¤„ç†é˜¶æ®µé‡‡ç”¨å¥å­çº§ICLæ¥åˆ©ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ŒåŒæ—¶åœ¨åå¤„ç†æ­¥éª¤ä¸­é›†æˆåŸºäºæ ‡è®°çš„kè¿‘é‚»ï¼ˆkNNï¼‰æ£€ç´¢ï¼Œä»¥è¿›ä¸€æ­¥ä¼˜åŒ–æœ€ç»ˆçš„è¾“å‡ºåˆ†å¸ƒã€‚é€šè¿‡ååŒç»“åˆå¥å­çº§å’Œæ ‡è®°çº§æ£€ç´¢ç­–ç•¥ï¼ŒM2R-whisperæœ‰æ•ˆåœ°å‡è½»äº†å„ç§ç±»å‹çš„è¯†åˆ«é”™è¯¯ã€‚åœ¨åŒ…æ‹¬AISHELL-1å’ŒKeSpeechåœ¨å†…çš„æ™®é€šè¯å’Œæ¬¡æ–¹è¨€æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒASRå‡†ç¡®ç‡å¾—åˆ°äº†æ˜¾è‘—æé«˜ï¼Œæ‰€æœ‰è¿™äº›æ”¹è¿›éƒ½æ²¡æœ‰æ›´æ–°ä»»ä½•å‚æ•°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.11889v3">PDF</a> Accepted by ICASSP 2025, oral</p>
<p><strong>Summary</strong></p>
<p>M2R-whisperæ˜¯ä¸€ç§é’ˆå¯¹ä½èµ„æºç¯å¢ƒä¸‹çš„å¤šè¯­ç§è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ€§èƒ½æå‡çš„æ–°æ–¹æ³•ã€‚å®ƒç»“åˆäº†ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰å’Œæ£€ç´¢å¢å¼ºæŠ€æœ¯ï¼Œé‡‡ç”¨å¤šé˜¶æ®µå¤šå°ºåº¦çš„ç­–ç•¥ï¼Œåœ¨é¢„å¤„ç†é˜¶æ®µåˆ©ç”¨å¥å­çº§åˆ«çš„ICLæ¥åˆ©ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¹¶åœ¨åå¤„ç†é˜¶æ®µé‡‡ç”¨åŸºäºtokençš„kè¿‘é‚»ï¼ˆkNNï¼‰æ£€ç´¢æŠ€æœ¯è¿›ä¸€æ­¥ä¼˜åŒ–è¾“å‡ºç»“æœåˆ†å¸ƒã€‚æ­¤æ–¹æ³•å¯æœ‰æ•ˆå‡è½»å„ç±»è¯†åˆ«é”™è¯¯ï¼Œé€šè¿‡ä¸­è‹±æ–‡çš„å¯¹ç…§å®éªŒéªŒè¯äº†å…¶åœ¨ä¸åŒæ–¹è¨€å£éŸ³æ•°æ®é›†ä¸Šçš„æ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>M2R-whisperæ—¨åœ¨å¢å¼ºä½èµ„æºç¯å¢ƒä¸‹çš„å¤šè¯­ç§è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•ç»“åˆäº†ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰å’Œæ£€ç´¢å¢å¼ºæŠ€æœ¯ã€‚</li>
<li>M2R-whisperé‡‡ç”¨å¤šé˜¶æ®µå¤šå°ºåº¦çš„ç­–ç•¥ï¼ŒåŒ…æ‹¬é¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ã€‚</li>
<li>é¢„å¤„ç†é˜¶æ®µé‡‡ç”¨å¥å­çº§åˆ«çš„ICLä»¥åˆ©ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚</li>
<li>åå¤„ç†é˜¶æ®µä½¿ç”¨åŸºäºtokençš„kè¿‘é‚»ï¼ˆkNNï¼‰æ£€ç´¢æŠ€æœ¯è¿›ä¸€æ­¥ä¼˜åŒ–è¾“å‡ºç»“æœåˆ†å¸ƒã€‚</li>
<li>M2R-whisperèƒ½å¤Ÿæœ‰æ•ˆå‡è½»å„ç±»è¯†åˆ«é”™è¯¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.11889">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-157a5f6ade70dd426a6d5ff474549701.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-213a5fb72883626f9098e0cfd082cb00.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-62fe17f43a8c8545694fa2482d74ec51.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f0ad7ce499f8ab419cca5e3545c90775.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-38be93e10552c028e4dd8c6eaa9b2e10.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-14/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-14/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-14/GAN/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-3dc2e9f11edafb6e7d77773500cc4b25.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-14  Revealing Unintentional Information Leakage in Low-Dimensional Facial   Portrait Representations
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-14
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-14/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-65ddb79f369ca600a59a6227f48da491.jpg" class="responsive-img" alt="æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ ">
                        
                        <span class="card-title">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹  æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-14  Patch-Wise Hypergraph Contrastive Learning with Dual Normal Distribution   Weighting for Multi-Domain Stain Transfer
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-14
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ 
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">17012.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
