<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech 方向最新论文已更新，请持续关注 Update in 2025-03-14  Everything Can Be Described in Words A Simple Unified Multi-Modal   Framework with Semantic and Temporal Alignment">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-4ab6d35773e0ca0121eba9370d764c97.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-03-14
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-03-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    9.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    37 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-03-14-更新"><a href="#2025-03-14-更新" class="headerlink" title="2025-03-14 更新"></a>2025-03-14 更新</h1><h2 id="Everything-Can-Be-Described-in-Words-A-Simple-Unified-Multi-Modal-Framework-with-Semantic-and-Temporal-Alignment"><a href="#Everything-Can-Be-Described-in-Words-A-Simple-Unified-Multi-Modal-Framework-with-Semantic-and-Temporal-Alignment" class="headerlink" title="Everything Can Be Described in Words: A Simple Unified Multi-Modal   Framework with Semantic and Temporal Alignment"></a>Everything Can Be Described in Words: A Simple Unified Multi-Modal   Framework with Semantic and Temporal Alignment</h2><p><strong>Authors:Xiaowei Bi, Zheyuan Xu</strong></p>
<p>Long Video Question Answering (LVQA) is challenging due to the need for temporal reasoning and large-scale multimodal data processing. Existing methods struggle with retrieving cross-modal information from long videos, especially when relevant details are sparsely distributed. We introduce UMaT (Unified Multi-modal as Text), a retrieval-augmented generation (RAG) framework that efficiently processes extremely long videos while maintaining cross-modal coherence. UMaT converts visual and auditory data into a unified textual representation, ensuring semantic and temporal alignment. Short video clips are analyzed using a vision-language model, while automatic speech recognition (ASR) transcribes dialogue. These text-based representations are structured into temporally aligned segments, with adaptive filtering to remove redundancy and retain salient details. The processed data is embedded into a vector database, enabling precise retrieval of dispersed yet relevant content. Experiments on a benchmark LVQA dataset show that UMaT outperforms existing methods in multimodal integration, long-form video understanding, and sparse information retrieval. Its scalability and interpretability allow it to process videos over an hour long while maintaining semantic and temporal coherence. These findings underscore the importance of structured retrieval and multimodal synchronization for advancing LVQA and long-form AI systems. </p>
<blockquote>
<p>长视频问答（LVQA）具有挑战性，因为它需要时序推理和大规模多模态数据处理。现有方法在从长视频中检索跨模态信息时面临困难，尤其是当相关细节稀疏分布时。我们引入了UMaT（统一多模态文本），这是一种增强检索生成（RAG）框架，能够高效处理极长的视频，同时保持跨模态一致性。UMaT将视觉和听觉数据转换为统一的文本表示，确保语义和时序对齐。短视频片段使用视觉语言模型进行分析，而自动语音识别（ASR）则转录对话。这些基于文本的表示被结构化成时序对齐的片段，通过自适应过滤去除冗余，保留重要细节。处理过的数据嵌入到向量数据库中，能够实现分散但相关内容精确检索。在基准LVQA数据集上的实验表明，UMaT在多模态融合、长格式视频理解和稀疏信息检索方面优于现有方法。其可扩展性和可解释性允许其处理超过一小时的视频，同时保持语义和时序一致性。这些发现强调了结构化检索和多模态同步在推动LVQA和长格式AI系统发展中的重要性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09081v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>长视频问答（LVQA）面临时间推理和大规模多模态数据处理的需求挑战。现有方法难以从长视频中检索跨模态信息，尤其当相关细节分布稀疏时。我们推出UMaT（统一多模态文本），一个检索增强生成（RAG）框架，能够高效处理极长视频并保持跨模态一致性。UMaT将视觉和听觉数据转化为统一文本表示，确保语义和时间对齐。它分析短视频片段使用视觉语言模型，自动语音识别（ASR）转录对话。这些基于文本的表示被结构化成时间对齐的片段，通过自适应过滤去除冗余并保留重要细节。处理过的数据嵌入向量数据库，能精确检索分散但相关的内容。在基准LVQA数据集上的实验表明，UMaT在多模态融合、长视频理解和稀疏信息检索方面优于现有方法。其可扩展性和可解释性可处理超过一小时的视频，同时保持语义和时间一致性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>长视频问答（LVQA）需要处理大规模多模态数据，面临时间推理和跨模态信息检索的挑战。</li>
<li>UMaT是一个RAG框架，能高效处理极长视频并维持跨模态一致性。</li>
<li>UMaT将视觉和听觉数据转化为统一文本表示，确保语义和时间对齐。</li>
<li>UMaT通过结构化为时间对齐的片段，并利用自适应过滤来处理长视频中的冗余和关键信息。</li>
<li>UMaT使用向量数据库进行数据处理和存储，能精确检索分散但相关的内容。</li>
<li>UMaT在基准LVQA数据集上的表现优于现有方法，尤其在多模态融合、长视频理解和稀疏信息检索方面。</li>
<li>UMaT具备可扩展性和可解释性，可处理超过一小时的视频内容。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09081">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-e8d6a40becdda56a9af6016bebe4206f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7d632126210c7544985bf52dc32b618e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2821815a481a85b1ae5016565a24a70c.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="An-Exhaustive-Evaluation-of-TTS-and-VC-based-Data-Augmentation-for-ASR"><a href="#An-Exhaustive-Evaluation-of-TTS-and-VC-based-Data-Augmentation-for-ASR" class="headerlink" title="An Exhaustive Evaluation of TTS- and VC-based Data Augmentation for ASR"></a>An Exhaustive Evaluation of TTS- and VC-based Data Augmentation for ASR</h2><p><strong>Authors:Sewade Ogun, Vincent Colotte, Emmanuel Vincent</strong></p>
<p>Augmenting the training data of automatic speech recognition (ASR) systems with synthetic data generated by text-to-speech (TTS) or voice conversion (VC) has gained popularity in recent years. Several works have demonstrated improvements in ASR performance using this augmentation approach. However, because of the lower diversity of synthetic speech, naively combining synthetic and real data often does not yield the best results. In this work, we leverage recently proposed flow-based TTS&#x2F;VC models allowing greater speech diversity, and assess the respective impact of augmenting various speech attributes on the word error rate (WER) achieved by several ASR models. Pitch augmentation and VC-based speaker augmentation are found to be ineffective in our setup. Jointly augmenting all other attributes reduces the WER of a Conformer-Transducer model by 11% relative on Common Voice and by up to 35% relative on LibriSpeech compared to training on real data only. </p>
<blockquote>
<p>近年来，通过文本到语音（TTS）或语音转换（VC）生成合成数据来增强自动语音识别（ASR）系统的训练数据已经变得非常流行。几项作品已经证明了使用这种增强方法可以改进ASR性能。然而，由于合成语音的多样性较低，简单地结合合成数据和真实数据往往不能产生最佳结果。在这项工作中，我们利用最近提出的基于流的TTS&#x2F;VC模型，允许更大的语音多样性，并评估增强各种语音属性对多个ASR模型实现的单词错误率（WER）的相应影响。在我们的设置中，发现音调增强和基于VC的演讲者增强效果不佳。联合增强所有其他属性可以降低Common Voice上Conformer-Transducer模型的相对WER 11%，与仅在真实数据上进行训练相比，LibriSpeech上的相对WER可降低高达35%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08954v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>使用文本转语音（TTS）或语音转换（VC）生成合成数据来增强自动语音识别（ASR）系统的训练数据已逐渐成为近年来的热门趋势。多项研究证明，使用这种增强方法可以提高ASR的性能。然而，由于合成语音的多样性较低，单纯地将合成数据与真实数据相结合往往无法获得最佳效果。本研究利用最新提出的基于流的TTS&#x2F;VC模型，该模型允许更大的语音多样性，并评估增强各种语音属性对词错误率（WER）的影响。在我们的设置中，音调增强和基于VC的说话人增强效果不佳。联合增强所有其他属性可使Conformer-Transducer模型的WER在Common Voice上相对降低11%，在LibriSpeech上相对降低高达35%，与仅使用真实数据进行训练相比。</p>
<p><strong>要点</strong></p>
<ol>
<li>合成数据增强ASR训练数据逐渐流行，能提高ASR性能。</li>
<li>单纯结合合成数据和真实数据并不总能获得最佳结果。</li>
<li>利用基于流的TTS&#x2F;VC模型，该模型允许更大的语音多样性。</li>
<li>评估了增强各种语音属性对WER的影响。</li>
<li>音调增强和基于VC的说话人增强在特定设置下效果不佳。</li>
<li>联合增强所有其他属性可显著提高ASR性能，相对降低WER。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08954">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-fdc7edc0d135c427bd2e41e97709423c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b266de6838750e8598815df51b591dd7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d823cd588fb95609dcfae114ebcf157c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ab6d35773e0ca0121eba9370d764c97.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd3ade09a1ad7eeb7773864c1e648f0f.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Contextual-Speech-Extraction-Leveraging-Textual-History-as-an-Implicit-Cue-for-Target-Speech-Extraction"><a href="#Contextual-Speech-Extraction-Leveraging-Textual-History-as-an-Implicit-Cue-for-Target-Speech-Extraction" class="headerlink" title="Contextual Speech Extraction: Leveraging Textual History as an Implicit   Cue for Target Speech Extraction"></a>Contextual Speech Extraction: Leveraging Textual History as an Implicit   Cue for Target Speech Extraction</h2><p><strong>Authors:Minsu Kim, Rodrigo Mira, Honglie Chen, Stavros Petridis, Maja Pantic</strong></p>
<p>In this paper, we investigate a novel approach for Target Speech Extraction (TSE), which relies solely on textual context to extract the target speech. We refer to this task as Contextual Speech Extraction (CSE). Unlike traditional TSE methods that rely on pre-recorded enrollment utterances, video of the target speaker’s face, spatial information, or other explicit cues to identify the target stream, our proposed method requires only a few turns of previous dialogue (or monologue) history. This approach is naturally feasible in mobile messaging environments where voice recordings are typically preceded by textual dialogue that can be leveraged implicitly. We present three CSE models and analyze their performances on three datasets. Through our experiments, we demonstrate that even when the model relies purely on dialogue history, it can achieve over 90 % accuracy in identifying the correct target stream with only two previous dialogue turns. Furthermore, we show that by leveraging both textual context and enrollment utterances as cues during training, we further enhance our model’s flexibility and effectiveness, allowing us to use either cue during inference, or combine both for improved performance. Samples and code available on <a target="_blank" rel="noopener" href="https://miraodasilva.github.io/cse-project-page">https://miraodasilva.github.io/cse-project-page</a> . </p>
<blockquote>
<p>在本文中，我们研究了一种针对目标语音提取（TSE）的新型方法，该方法仅依赖文本上下文来提取目标语音。我们将此任务称为上下文语音提取（CSE）。与传统的TSE方法不同，这些方法依赖于预先录制的注册话语、目标说话人的面部视频、空间信息或其他明确的线索来识别目标流，我们提出的方法只需要前几轮对话（或独白）历史。这种方法在移动消息环境中自然而然地可行，语音记录通常先于文本对话，可以隐性利用。我们提出了三种CSE模型，并在三个数据集上对其性能进行了分析。通过实验，我们证明即使模型仅依赖于对话历史，在仅使用前两轮对话的情况下，也可以实现超过90%的正确识别目标流的准确率。此外，我们还表明，通过在训练过程中利用文本上下文和注册话语作为线索，可以进一步提高模型的灵活性和有效性，使我们能够在推理过程中使用任一线索，或结合两者以提高性能。相关样本和代码可通过<a target="_blank" rel="noopener" href="https://miraodasilva.github.io/cse-project-page%E8%8E%B7%E5%8F%96%E3%80%82">https://miraodasilva.github.io/cse-project-page获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08798v1">PDF</a> Accepted to ICASSP 2025</p>
<p><strong>Summary</strong>：<br>本文提出了一种新的目标语音提取方法——上下文语音提取（CSE），该方法仅依赖于文本上下文来提取目标语音，适用于移动通讯环境下的文本对话和声音数据的无缝交互场景。相较于传统的目标语音提取方法，该方法无需预录的注册语音、目标说话人的面部视频、空间信息或其他明确线索，仅依赖对话历史即可实现超过90%的准确识别。此外，研究还展示了结合文本上下文和注册语音作为线索进行训练的方法，提高了模型的灵活性和有效性，可在推理过程中单独使用或结合两种线索实现更佳性能。该方法的样品和代码可访问[<a target="_blank" rel="noopener" href="https://miraodasilva.github.io/cse-project-page%E4%BA%86%E8%A7%A3%E8%AF%A6%E7%BB%86%E4%BF%A1%E6%81%AF]%E3%80%82">https://miraodasilva.github.io/cse-project-page了解详细信息]。</a></p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>论文提出了一种新的目标语音提取方法——上下文语音提取（CSE）。</li>
<li>该方法仅依赖于文本上下文进行目标语音提取。</li>
<li>对比传统方法，新方法无需预录的注册语音等明确线索，仅依赖对话历史即可实现高准确识别。</li>
<li>新方法在实验中的准确识别率超过90%。</li>
<li>研究展示了结合文本上下文和注册语音作为训练线索的方法，提高模型的灵活性和有效性。</li>
<li>模型可以在推理过程中使用单一线索或结合两种线索，实现更佳性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08798">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-18ee554dcc000499b502f31d7365cd93.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50f0b68b383c002dff2c5cf758d4e26b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0e2e2177f57a3f391fc1646c183be065.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fcbceab6ddd676590b24c038ab08330f.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Prompt2LVideos-Exploring-Prompts-for-Understanding-Long-Form-Multimodal-Videos"><a href="#Prompt2LVideos-Exploring-Prompts-for-Understanding-Long-Form-Multimodal-Videos" class="headerlink" title="Prompt2LVideos: Exploring Prompts for Understanding Long-Form Multimodal   Videos"></a>Prompt2LVideos: Exploring Prompts for Understanding Long-Form Multimodal   Videos</h2><p><strong>Authors:Soumya Shamarao Jahagirdar, Jayasree Saha, C V Jawahar</strong></p>
<p>Learning multimodal video understanding typically relies on datasets comprising video clips paired with manually annotated captions. However, this becomes even more challenging when dealing with long-form videos, lasting from minutes to hours, in educational and news domains due to the need for more annotators with subject expertise. Hence, there arises a need for automated solutions. Recent advancements in Large Language Models (LLMs) promise to capture concise and informative content that allows the comprehension of entire videos by leveraging Automatic Speech Recognition (ASR) and Optical Character Recognition (OCR) technologies. ASR provides textual content from audio, while OCR extracts textual content from specific frames. This paper introduces a dataset comprising long-form lectures and news videos. We present baseline approaches to understand their limitations on this dataset and advocate for exploring prompt engineering techniques to comprehend long-form multimodal video datasets comprehensively. </p>
<blockquote>
<p>学习多模态视频理解通常依赖于由视频片段和手动注释的标题组成的数据集。然而，在处理教育和新领域的长视频时，由于需要更多具有专业知识的注释者，这变得更加具有挑战性，这些视频时长从几分钟到几小时不等。因此，需要自动解决方案。最近的大型语言模型（LLM）的进步显示出能够捕捉简洁且信息丰富的内容，利用自动语音识别（ASR）和光学字符识别（OCR）技术，通过理解整个视频的内容实现视频的理解。ASR从音频中提供文本内容，而OCR则从特定帧中提取文本内容。本文介绍了一个包含长讲座和新闻视频的数据集。我们介绍了在这个数据集上理解其局限性的基线方法，并提倡探索提示工程技术，以全面理解长形式的多模态视频数据集。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08335v1">PDF</a> CVIP 2024</p>
<p><strong>Summary</strong></p>
<p>随着长视频内容的普及，特别是在教育和新闻领域，手动标注视频内容的任务变得越来越复杂和耗时。因此，需要自动化解决方案。本文引入了一种包含长讲座和新闻视频的数据集，并探讨了如何利用大型语言模型（LLMs）自动识别和识别视频内容的技术，为理解长视频提供了有效的工具。文章主张采用提示工程技术，以便更全面地理解长格式的多模态视频数据集。这一技术在教育技术和人工智能领域的应用潜力巨大。本文基于自动语音识别（ASR）和光学字符识别（OCR）技术实现了从视频音频和视频帧中提取文本内容的目标。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>长格式视频在教育和新闻领域的需求增长导致了对自动化解决方案的需求，以简化手动标注视频内容的任务。</li>
<li>大型语言模型（LLMs）在理解长格式多模态视频内容方面具有潜力。</li>
<li>自动语音识别（ASR）和光学字符识别（OCR）技术被用于从视频中提取文本内容。</li>
<li>ASR能够从视频音频中提取文本内容，而OCR则从特定的视频帧中提取文本信息。</li>
<li>本文介绍了包含长讲座和新闻视频的数据集，并提出了基准方法来理解其在该数据集上的局限性。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08335">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-fbe6c95c1b4dbdb0ab65d7d1344f1b36.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b657b10c8d2e5e2c6f07564b881637c9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f5023425b67835ffb82a8e4a23f79007.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-25ab7483b438218d293ad49505a415f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8cd36fa92671f3f4be62c16689bde665.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Enhancing-Multilingual-Language-Models-for-Code-Switched-Input-Data"><a href="#Enhancing-Multilingual-Language-Models-for-Code-Switched-Input-Data" class="headerlink" title="Enhancing Multilingual Language Models for Code-Switched Input Data"></a>Enhancing Multilingual Language Models for Code-Switched Input Data</h2><p><strong>Authors:Katherine Xie, Nitya Babbar, Vicky Chen, Yoanna Turura</strong></p>
<p>Code-switching, or alternating between languages within a single conversation, presents challenges for multilingual language models on NLP tasks. This research investigates if pre-training Multilingual BERT (mBERT) on code-switched datasets improves the model’s performance on critical NLP tasks such as part of speech tagging, sentiment analysis, named entity recognition, and language identification. We use a dataset of Spanglish tweets for pre-training and evaluate the pre-trained model against a baseline model.   Our findings show that our pre-trained mBERT model outperforms or matches the baseline model in the given tasks, with the most significant improvements seen for parts of speech tagging. Additionally, our latent analysis uncovers more homogenous English and Spanish embeddings for language identification tasks, providing insights for future modeling work.   This research highlights potential for adapting multilingual LMs for code-switched input data in order for advanced utility in globalized and multilingual contexts. Future work includes extending experiments to other language pairs, incorporating multiform data, and exploring methods for better understanding context-dependent code-switches. </p>
<blockquote>
<p>代码切换或在单个对话中使用多种语言，对多语言语言模型在自然语言处理任务上提出了挑战。本研究旨在调查预训练多语言BERT（mBERT）在代码切换数据集上是否能在关键的自然语言处理任务（如词性标注、情感分析、命名实体识别和语言识别等）上提高模型性能。我们使用西班牙语和英语混合推文的数据集进行预训练，并对预训练模型与基线模型进行评估。我们的研究结果表明，预训练mBERT模型在给定的任务上表现优于或等于基线模型，其中词性标注的改进最为显著。此外，我们的潜在分析揭示了用于语言识别任务的更统一的英语和西班牙语嵌入，为未来建模工作提供了见解。该研究强调了为适应代码切换输入数据的多语言语言模型在全球化和多语言环境的高级应用中的潜力。未来的工作包括将实验扩展到其他语言对、融入多种形式的数据，并探索更好地理解上下文相关代码切换的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07990v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>预训练的多语言BERT模型（mBERT）在代码切换数据集上进行训练，可以改善其在自然语言处理任务上的性能，如词性标注、情感分析、实体命名识别和语言识别等。本研究使用西班牙英语推特数据集进行预训练，并与基线模型进行评估。结果显示，预训练的mBERT模型在给定任务上的表现优于或等同于基线模型，尤其在词性标注方面最为显著。此外，潜在分析发现英语和西班牙语的语言识别任务嵌入更为一致，为未来的建模工作提供了启示。本研究突显了为适应代码切换输入数据的多语言语言模型在全球化、多语言环境中的潜在应用。未来的研究方向包括扩展到其他语言对、融入多种形式的数据和探索理解上下文相关代码切换的方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>代码切换对于多语言语言模型在自然语言处理任务上构成挑战。</li>
<li>预训练多语言BERT模型（mBERT）在代码切换数据集上可以改进模型在自然语言处理任务上的性能。</li>
<li>使用西班牙英语推特数据集进行预训练评估，显示预训练模型在多个任务上优于基线模型。</li>
<li>词性标注任务的改进最为显著。</li>
<li>潜在分析发现英语和西班牙语的嵌入一致性更高，有助于语言识别任务。</li>
<li>适应代码切换输入数据的多语言语言模型在全球化、多语言环境中具有潜在应用价值。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07990">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-b7c4ac0801f99c20a2e0f5bd6a2606ee.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c61abe5cf4ba2aae27b07826e33a6a3f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0082de16b4cf157e3b2c07e87ea86f4f.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="CS-Dialogue-A-104-Hour-Dataset-of-Spontaneous-Mandarin-English-Code-Switching-Dialogues-for-Speech-Recognition"><a href="#CS-Dialogue-A-104-Hour-Dataset-of-Spontaneous-Mandarin-English-Code-Switching-Dialogues-for-Speech-Recognition" class="headerlink" title="CS-Dialogue: A 104-Hour Dataset of Spontaneous Mandarin-English   Code-Switching Dialogues for Speech Recognition"></a>CS-Dialogue: A 104-Hour Dataset of Spontaneous Mandarin-English   Code-Switching Dialogues for Speech Recognition</h2><p><strong>Authors:Jiaming Zhou, Yujie Guo, Shiwan Zhao, Haoqin Sun, Hui Wang, Jiabei He, Aobo Kong, Shiyao Wang, Xi Yang, Yequan Wang, Yonghua Lin, Yong Qin</strong></p>
<p>Code-switching (CS), the alternation between two or more languages within a single conversation, presents significant challenges for automatic speech recognition (ASR) systems. Existing Mandarin-English code-switching datasets often suffer from limitations in size, spontaneity, and the lack of full-length dialogue recordings with transcriptions, hindering the development of robust ASR models for real-world conversational scenarios. This paper introduces CS-Dialogue, a novel large-scale Mandarin-English code-switching speech dataset comprising 104 hours of spontaneous conversations from 200 speakers. Unlike previous datasets, CS-Dialogue provides full-length dialogue recordings with complete transcriptions, capturing naturalistic code-switching patterns in continuous speech. We describe the data collection and annotation processes, present detailed statistics of the dataset, and establish benchmark ASR performance using state-of-the-art models. Our experiments, using Transformer, Conformer, and Branchformer, demonstrate the challenges of code-switching ASR, and show that existing pre-trained models such as Whisper still have the space to improve. The CS-Dialogue dataset will be made freely available for all academic purposes. </p>
<blockquote>
<p>语言交替（CS）是指在单次对话中两个或多个语言之间的交替，这对自动语音识别（ASR）系统提出了重大挑战。现有的汉语-英语交替数据集通常在规模、自发性和缺乏带有转录的全长对话录音等方面存在局限性，阻碍了为现实世界的对话场景开发稳健的ASR模型。本文介绍了CS-Dialogue，这是一个新的大规模汉语-英语交替语音数据集，包含来自200名发言人的104小时自发对话。与以前的数据集不同，CS-Dialogue提供了带有完整转录的全长对话录音，捕捉连续语音中的自然语言交替模式。我们描述了数据收集和注释过程，给出了数据集的详细统计信息，并使用最新模型建立了ASR性能基准。我们的实验使用了Transformer、Conformer和Branchformer，展示了语言交替ASR的挑战性，并表明现有的预训练模型（如Whisper）仍有改进空间。CS-Dialogue数据集将免费供所有学术用途使用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.18913v2">PDF</a> </p>
<p><strong>总结</strong></p>
<p>该论文介绍了CS-Dialogue数据集，这是一个大规模的、涵盖普通话与英语交替使用现象的大型数据集。其中包含来自200名说话者的104小时自然对话录音，并附有完整转录。该数据集解决了现有普通话-英语交替使用数据集大小有限、缺乏自发性及完整对话录音的问题，有助于开发适用于真实对话场景的稳健语音识别模型。论文描述了数据收集与标注过程，提供了数据集的详细统计信息，并使用前沿模型建立了基准语音识别性能。实验表明，现有的预训练模型如Whisper在普通话-英语交替使用的语音识别上仍有提升空间。CS-Dialogue数据集将免费用于所有学术用途。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>CS-Dialogue是首个大规模的普通话-英语交替使用语音数据集，包含104小时的自发对话录音和完整转录。</li>
<li>数据集解决了现有普通话-英语交替使用数据集大小有限、缺乏自发性及完整对话记录的问题。</li>
<li>数据集有助于开发适用于真实对话场景的稳健语音识别模型。</li>
<li>论文描述了数据收集与标注流程，并提供了详细的统计数据。</li>
<li>使用Transformer、Conformer和Branchformer等先进模型建立的基准语音识别性能表明了现有的预训练模型在处理普通话-英语交替使用的语音识别时面临的挑战。</li>
<li>实验结果显示，现有的预训练模型如Whisper仍有改进空间。</li>
<li>CS-Dialogue数据集将供所有学术用途免费使用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.18913">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a5d41c0acd651035fb457987bf0cbd52.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48a9ec32dcf88f8c93bd03b14049382c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7f35b68213455057b5ddfdb72d5aef37.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-708d7cb6863592ae4ca5a03617a80d11.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab3f45528557a7893dd8c474d60eaaef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8050fcc384ae9f363eee6e31f83bb6f8.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="HumanVBench-Exploring-Human-Centric-Video-Understanding-Capabilities-of-MLLMs-with-Synthetic-Benchmark-Data"><a href="#HumanVBench-Exploring-Human-Centric-Video-Understanding-Capabilities-of-MLLMs-with-Synthetic-Benchmark-Data" class="headerlink" title="HumanVBench: Exploring Human-Centric Video Understanding Capabilities of   MLLMs with Synthetic Benchmark Data"></a>HumanVBench: Exploring Human-Centric Video Understanding Capabilities of   MLLMs with Synthetic Benchmark Data</h2><p><strong>Authors:Ting Zhou, Daoyuan Chen, Qirui Jiao, Bolin Ding, Yaliang Li, Ying Shen</strong></p>
<p>In the domain of Multimodal Large Language Models (MLLMs), achieving human-centric video understanding remains a formidable challenge. Existing benchmarks primarily emphasize object and action recognition, often neglecting the intricate nuances of human emotions, behaviors, and speech-visual alignment within video content. We present HumanVBench, an innovative benchmark meticulously crafted to bridge these gaps in the evaluation of video MLLMs. HumanVBench comprises 16 carefully designed tasks that explore two primary dimensions: inner emotion and outer manifestations, spanning static and dynamic, basic and complex, as well as single-modal and cross-modal aspects. With two advanced automated pipelines for video annotation and distractor-included QA generation, HumanVBench utilizes diverse state-of-the-art (SOTA) techniques to streamline benchmark data synthesis and quality assessment, minimizing human annotation dependency tailored to human-centric multimodal attributes. A comprehensive evaluation across 22 SOTA video MLLMs reveals notable limitations in current performance, especially in cross-modal and emotion perception, underscoring the necessity for further refinement toward achieving more human-like understanding. HumanVBench is open-sourced to facilitate future advancements and real-world applications in video MLLMs. </p>
<blockquote>
<p>在多模态大型语言模型（MLLMs）领域，实现以人类为中心的视频理解仍然是一个巨大的挑战。现有的基准测试主要强调对象和动作识别，往往忽视了视频内容中人类情绪、行为和语音视觉对齐的细微差别。我们推出了HumanVBench，这是一个精心设计的基准测试，旨在弥补视频MLLMs评估中的这些差距。HumanVBench包含16个精心设计的任务，探索两个主要维度：内在情绪和外在表现，涵盖静态和动态、基本和复杂，以及单模态和跨模态方面。HumanVBench使用先进的自动化管道进行视频标注和包含干扰项的QA生成，利用多种最新技术优化基准数据合成和质量评估，减少对人类标注的依赖，专门面向以人类为中心的多模态属性。对22个最新视频MLLMs的综合评估显示，当前性能存在显著局限，特别是在跨模态和情感感知方面，这强调了对进一步改进实现更人性化的理解的必要性。HumanVBench开源，以促进视频MLLMs的未来发展和实际应用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17574v2">PDF</a> 22 pages, 23 figures, 7 tables</p>
<p><strong>Summary</strong></p>
<p>在多媒体大型语言模型领域，实现以人类为中心的视频理解是一项挑战。现有的基准测试主要侧重于物体和动作识别，忽视了人类情绪、行为和视频内容中语音视觉对齐的细微差别。我们推出HumanVBench基准测试，以弥补这些评估差距。HumanVBench包含16项精心设计的任务，探索内在情感和外在表现两个主要维度，涵盖静态和动态、基本和复杂，以及单模态和跨模态方面。利用两个先进的自动化管道进行视频标注和包含干扰项的QA生成，HumanVBench利用多种最新技术简化基准测试数据合成和质量评估，减少人工标注的依赖，针对以人为中心的多模态属性定制。对22个最新视频多媒体语言模型的全面评估显示，在跨模态和情感感知方面存在显著局限性，强调需要进一步改进以实现更人性化的理解。HumanVBench已开源，以促进视频多媒体语言模型的未来发展和实际应用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Human-centric video understanding remains a challenge in Multimodal Large Language Models (MLLMs).</li>
<li>现有基准测试主要关注物体和动作识别，忽视了人类情绪、行为和语音视觉对齐的细微差别。</li>
<li>HumanVBench基准测试包含16项任务，探索内在情感和外在表现两个主要维度。</li>
<li>HumanVBench利用自动化管道进行视频标注和QA生成，以减少人工标注的依赖。</li>
<li>跨模态和情感感知是当前多媒体语言模型在视频理解方面的显著局限性。</li>
<li>HumanVBench已开源，以促进视频多媒体语言模型的未来发展。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17574">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9aa31fd256e4e1214999c2c0675e76f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b7dcbd9fac21bc08fff550b2f932ed8b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c27fbacceb72e10420ea5305cd6a90f.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="SemTalk-Holistic-Co-speech-Motion-Generation-with-Frame-level-Semantic-Emphasis"><a href="#SemTalk-Holistic-Co-speech-Motion-Generation-with-Frame-level-Semantic-Emphasis" class="headerlink" title="SemTalk: Holistic Co-speech Motion Generation with Frame-level Semantic   Emphasis"></a>SemTalk: Holistic Co-speech Motion Generation with Frame-level Semantic   Emphasis</h2><p><strong>Authors:Xiangyue Zhang, Jianfang Li, Jiaxu Zhang, Ziqiang Dang, Jianqiang Ren, Liefeng Bo, Zhigang Tu</strong></p>
<p>A good co-speech motion generation cannot be achieved without a careful integration of common rhythmic motion and rare yet essential semantic motion. In this work, we propose SemTalk for holistic co-speech motion generation with frame-level semantic emphasis. Our key insight is to separately learn base motions and sparse motions, and then adaptively fuse them. In particular, coarse2fine cross-attention module and rhythmic consistency learning are explored to establish rhythm-related base motion, ensuring a coherent foundation that synchronizes gestures with the speech rhythm. Subsequently, semantic emphasis learning is designed to generate semantic-aware sparse motion, focusing on frame-level semantic cues. Finally, to integrate sparse motion into the base motion and generate semantic-emphasized co-speech gestures, we further leverage a learned semantic score for adaptive synthesis. Qualitative and quantitative comparisons on two public datasets demonstrate that our method outperforms the state-of-the-art, delivering high-quality co-speech motion with enhanced semantic richness over a stable base motion. </p>
<blockquote>
<p>良好的协同语音动作生成离不开常见的节奏动作和罕见但必要的语义动作的仔细融合。在这项工作中，我们提出了用于整体协同语音动作生成的SemTalk方法，具有帧级语义强调。我们的关键见解是分别学习基础动作和稀疏动作，然后自适应地融合它们。特别是，我们探索了coarse2fine交叉注意力模块和节奏一致性学习来建立与节奏相关的基本动作，确保与语音节奏同步的手势有一个连贯的基础。随后，设计语义强调学习来生成具有语义感知的稀疏动作，侧重于帧级语义线索。最后，为了将稀疏动作融入基础动作中，生成具有语义强调的协同语音手势，我们进一步利用学习得到的语义分数进行自适应合成。在两项公共数据集上的定性和定量比较表明，我们的方法优于现有技术，提供高质量的协同语音动作，在稳定的基础动作上增加了语义丰富性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16563v3">PDF</a> 11 pages, 8 figures</p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为SemTalk的协同语音运动生成方法，该方法融合了常规节奏运动和罕见但重要的语义运动。通过分别学习基础运动和稀疏运动，然后自适应地融合它们。采用coarse2fine交叉注意力模块和节奏一致性学习来建立与节奏相关的基础运动，确保手势与语音节奏的同步。此外，设计了语义强调学习来生成具有语义意识的稀疏运动，重点关注帧级语义线索。最后，通过自适应合成将稀疏运动融入基础运动，生成具有语义强调的协同语音手势。本文方法在两个公共数据集上的表现优于现有技术，生成了高质量且语义丰富的协同语音运动。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>协同语音运动生成需要融合常规节奏运动和语义运动。</li>
<li>提出SemTalk方法，分别学习基础运动和稀疏运动，并自适应融合。</li>
<li>采用coarse2fine交叉注意力模块建立与语音节奏相关的基础运动。</li>
<li>语义强调学习用于生成帧级语义线索的稀疏运动。</li>
<li>通过自适应合成将稀疏运动融入基础运动。</li>
<li>方法在公共数据集上表现优异，生成高质量且语义丰富的协同语音运动。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16563">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c6d57512a50872a657af04855f7bd13e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7bdea21775e414743083889bac9441db.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3893daeab405fe5406b44cc111d176a6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-299f2d0ec51f64a4202160b96b71ef26.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6632f41b5354ba80da2d8cb01d69bb63.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Biodenoising-Animal-Vocalization-Denoising-without-Access-to-Clean-Data"><a href="#Biodenoising-Animal-Vocalization-Denoising-without-Access-to-Clean-Data" class="headerlink" title="Biodenoising: Animal Vocalization Denoising without Access to Clean Data"></a>Biodenoising: Animal Vocalization Denoising without Access to Clean Data</h2><p><strong>Authors:Marius Miron, Sara Keen, Jen-Yu Liu, Benjamin Hoffman, Masato Hagiwara, Olivier Pietquin, Felix Effenberger, Maddie Cusimano</strong></p>
<p>Animal vocalization denoising is a task similar to human speech enhancement, which is relatively well-studied. In contrast to the latter, it comprises a higher diversity of sound production mechanisms and recording environments, and this higher diversity is a challenge for existing models. Adding to the challenge and in contrast to speech, we lack large and diverse datasets comprising clean vocalizations. As a solution we use as training data pseudo-clean targets, i.e. pre-denoised vocalizations, and segments of background noise without a vocalization. We propose a train set derived from bioacoustics datasets and repositories representing diverse species, acoustic environments, geographic regions. Additionally, we introduce a non-overlapping benchmark set comprising clean vocalizations from different taxa and noise samples. We show that that denoising models (demucs, CleanUNet) trained on pseudo-clean targets obtained with speech enhancement models achieve competitive results on the benchmarking set. We publish data, code, libraries, and demos at <a target="_blank" rel="noopener" href="https://earthspecies.github.io/biodenoising/">https://earthspecies.github.io/biodenoising/</a>. </p>
<blockquote>
<p>动物发声去噪与人类语音增强任务类似，已经得到了较为充分的研究。然而，与之相比，动物发声去噪包含了更多样化的声音产生机制和录音环境，这给现有模型带来了更大的挑战。除了这些挑战之外，与语音不同，我们还缺乏包含清晰发声的大型多样化数据集。作为解决方案，我们使用伪清洁目标作为训练数据，即预去噪发声和无发声的背景噪声片段。我们提出了一个基于生物声学数据集和存储库的训练集，这些数据集涵盖了多种物种、声学环境和地理区域。此外，我们还引入了一个不包含重叠的基准测试集，其中包括来自不同分类群组的清洁发声和噪声样本。我们证明，使用语音增强模型获得的伪清洁目标进行训练的去噪模型（如demucs和CleanUNet）在基准测试集上取得了具有竞争力的结果。我们在<a target="_blank" rel="noopener" href="https://earthspecies.github.io/biodenoising/%E5%8F%91%E5%B8%83%E6%95%B0%E6%8D%AE%E3%80%81%E4%BB%A3%E7%A0%81%E3%80%81%E5%BA%93%E5%92%8C%E6%BC%94%E7%A4%BA%E3%80%82">https://earthspecies.github.io/biodenoising/发布数据、代码、库和演示。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.03427v3">PDF</a> 5 pages, 2 tables</p>
<p><strong>Summary</strong></p>
<p>本文介绍了动物声音去噪任务面临的挑战和解决方案。由于动物声音产生机制和录音环境多样，现有模型难以应对。为解决此问题，研究团队使用伪清洁目标作为训练数据，并引入生物声学数据集和代表不同物种、声学环境和地理区域的存储库构建训练集。此外，他们还推出一个包含不同税种清洁发声和噪声样本的非重叠基准测试集。通过训练伪清洁目标上的去噪模型，如demucs和CleanUNet，在基准测试集上取得了具有竞争力的结果。相关数据和代码已发布在<a target="_blank" rel="noopener" href="https://earthspecies.github.io/biodenoising/%E4%B8%8A%E3%80%82">https://earthspecies.github.io/biodenoising/上。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>动物声音去噪与人类语音增强类似，但面临更高的声音多样性和缺乏大型多元清洁发声数据集挑战。</li>
<li>为解决数据缺乏问题，研究团队使用伪清洁目标和背景噪声作为训练数据。</li>
<li>研究团队构建了一个包含多种物种、声学环境和地理区域的训练集。</li>
<li>引入非重叠基准测试集，包含清洁发声和噪声样本。</li>
<li>基于伪清洁目标的去噪模型（如demucs和CleanUNet）在基准测试集上表现良好。</li>
<li>该研究团队提供了动物声音去噪的数据集、代码库和演示。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.03427">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-b76f5fe4ab822a368ca1f34036129d09.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-568202a7d1ee5f05860fbb60daf870a3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6369b484852ea57384b745db07b08f17.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="M2R-Whisper-Multi-stage-and-Multi-scale-Retrieval-Augmentation-for-Enhancing-Whisper"><a href="#M2R-Whisper-Multi-stage-and-Multi-scale-Retrieval-Augmentation-for-Enhancing-Whisper" class="headerlink" title="M2R-Whisper: Multi-stage and Multi-scale Retrieval Augmentation for   Enhancing Whisper"></a>M2R-Whisper: Multi-stage and Multi-scale Retrieval Augmentation for   Enhancing Whisper</h2><p><strong>Authors:Jiaming Zhou, Shiwan Zhao, Jiabei He, Hui Wang, Wenjia Zeng, Yong Chen, Haoqin Sun, Aobo Kong, Yong Qin</strong></p>
<p>State-of-the-art models like OpenAI’s Whisper exhibit strong performance in multilingual automatic speech recognition (ASR), but they still face challenges in accurately recognizing diverse subdialects. In this paper, we propose M2R-whisper, a novel multi-stage and multi-scale retrieval augmentation approach designed to enhance ASR performance in low-resource settings. Building on the principles of in-context learning (ICL) and retrieval-augmented techniques, our method employs sentence-level ICL in the pre-processing stage to harness contextual information, while integrating token-level k-Nearest Neighbors (kNN) retrieval as a post-processing step to further refine the final output distribution. By synergistically combining sentence-level and token-level retrieval strategies, M2R-whisper effectively mitigates various types of recognition errors. Experiments conducted on Mandarin and subdialect datasets, including AISHELL-1 and KeSpeech, demonstrate substantial improvements in ASR accuracy, all achieved without any parameter updates. </p>
<blockquote>
<p>当前最先进的模型，如OpenAI的Whisper，在多语种自动语音识别（ASR）方面表现出强大的性能，但它们在识别多样的次方言方面仍面临挑战。在本文中，我们提出了M2R-whisper，这是一种新型的多阶段多尺度检索增强方法，旨在增强低资源环境下的ASR性能。我们的方法基于上下文学习（ICL）和检索增强技术，在预处理阶段采用句子级ICL来利用上下文信息，同时在后处理步骤中集成基于标记的k近邻（kNN）检索，以进一步优化最终的输出分布。通过协同结合句子级和标记级检索策略，M2R-whisper有效地减轻了各种类型的识别错误。在包括AISHELL-1和KeSpeech在内的普通话和次方言数据集上进行的实验表明，ASR准确率得到了显著提高，所有这些改进都没有更新任何参数。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.11889v3">PDF</a> Accepted by ICASSP 2025, oral</p>
<p><strong>Summary</strong></p>
<p>M2R-whisper是一种针对低资源环境下的多语种自动语音识别（ASR）性能提升的新方法。它结合了上下文学习（ICL）和检索增强技术，采用多阶段多尺度的策略，在预处理阶段利用句子级别的ICL来利用上下文信息，并在后处理阶段采用基于token的k近邻（kNN）检索技术进一步优化输出结果分布。此方法可有效减轻各类识别错误，通过中英文的对照实验验证了其在不同方言口音数据集上的性能提升。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>M2R-whisper旨在增强低资源环境下的多语种自动语音识别（ASR）性能。</li>
<li>该方法结合了上下文学习（ICL）和检索增强技术。</li>
<li>M2R-whisper采用多阶段多尺度的策略，包括预处理和后处理步骤。</li>
<li>预处理阶段采用句子级别的ICL以利用上下文信息。</li>
<li>后处理阶段使用基于token的k近邻（kNN）检索技术进一步优化输出结果分布。</li>
<li>M2R-whisper能够有效减轻各类识别错误。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.11889">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-157a5f6ade70dd426a6d5ff474549701.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-213a5fb72883626f9098e0cfd082cb00.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-62fe17f43a8c8545694fa2482d74ec51.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f0ad7ce499f8ab419cca5e3545c90775.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-38be93e10552c028e4dd8c6eaa9b2e10.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-14/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-14/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-14/GAN/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-3dc2e9f11edafb6e7d77773500cc4b25.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN 方向最新论文已更新，请持续关注 Update in 2025-03-14  Revealing Unintentional Information Leakage in Low-Dimensional Facial   Portrait Representations
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-14
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-14/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-65ddb79f369ca600a59a6227f48da491.jpg" class="responsive-img" alt="无监督/半监督/对比学习">
                        
                        <span class="card-title">无监督/半监督/对比学习</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            无监督/半监督/对比学习 方向最新论文已更新，请持续关注 Update in 2025-03-14  Patch-Wise Hypergraph Contrastive Learning with Dual Normal Distribution   Weighting for Multi-Domain Stain Transfer
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-14
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    无监督/半监督/对比学习
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">无监督/半监督/对比学习</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">17012.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
