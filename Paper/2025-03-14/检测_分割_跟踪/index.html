<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="检测/分割/跟踪">
    <meta name="description" content="检测/分割/跟踪 方向最新论文已更新，请持续关注 Update in 2025-03-14  Dual-Domain Homogeneous Fusion with Cross-Modal Mamba and Progressive   Decoder for 3D Object Detection">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>检测/分割/跟踪 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-85496aa7a0b91d2d83ea6d5050f46013.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">检测/分割/跟踪</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                <span class="chip bg-color">检测/分割/跟踪</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                检测/分割/跟踪
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-03-14
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-03-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    6.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    25 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-03-14-更新"><a href="#2025-03-14-更新" class="headerlink" title="2025-03-14 更新"></a>2025-03-14 更新</h1><h2 id="Dual-Domain-Homogeneous-Fusion-with-Cross-Modal-Mamba-and-Progressive-Decoder-for-3D-Object-Detection"><a href="#Dual-Domain-Homogeneous-Fusion-with-Cross-Modal-Mamba-and-Progressive-Decoder-for-3D-Object-Detection" class="headerlink" title="Dual-Domain Homogeneous Fusion with Cross-Modal Mamba and Progressive   Decoder for 3D Object Detection"></a>Dual-Domain Homogeneous Fusion with Cross-Modal Mamba and Progressive   Decoder for 3D Object Detection</h2><p><strong>Authors:Xuzhong Hu, Zaipeng Duan, Pei An, Jun zhang, Jie Ma</strong></p>
<p>Fusing LiDAR point cloud features and image features in a homogeneous BEV space has been widely adopted for 3D object detection in autonomous driving. However, such methods are limited by the excessive compression of multi-modal features. While some works explore feature fusion in dense voxel spaces, they suffer from high computational costs and inefficiencies in query generation. To address these limitations, we propose a Dual-Domain Homogeneous Fusion network (DDHFusion), which leverages the complementary advantages of both BEV and voxel domains while mitigating their respective drawbacks. Specifically, we first transform image features into BEV and sparse voxel spaces using LSS and our proposed semantic-aware feature sampling module which can significantly reduces computational overhead by filtering unimportant voxels. For feature encoding, we design two networks for BEV and voxel feature fusion, incorporating novel cross-modal voxel and BEV Mamba blocks to resolve feature misalignment and enable efficient yet comprehensive scene perception. The output voxel features are injected into the BEV space to compensate for the loss of 3D details caused by height compression. For feature decoding, a progressive query generation module is implemented in the BEV domain to alleviate false negatives during query selection caused by feature compression and small object sizes. Finally, a progressive decoder can sequentially aggregate not only context-rich BEV features but also geometry-aware voxel features, ensuring more precise confidence prediction and bounding box regression. On the NuScenes dataset, DDHfusion achieves state-of-the-art performance, and further experiments demonstrate its superiority over other homogeneous fusion methods. </p>
<blockquote>
<p>在自动驾驶的3D目标检测中，将激光雷达点云特征和图像特征融合在同一个地面高程视图（BEV）空间中已被广泛采用。然而，这些方法受到多模态特征过度压缩的限制。虽然有些研究在密集体素空间中探索特征融合，但它们面临着计算成本高昂和查询生成效率低下的问题。为了解决这些限制，我们提出了一个双域同态融合网络（DDHFusion），它结合了BEV和体素域各自的优点，同时缓解了各自的缺点。具体来说，我们首先使用LSS和我们提出的语义感知特征采样模块将图像特征转换为BEV和稀疏体素空间，这可以通过过滤掉不重要的体素来大大减少计算开销。对于特征编码，我们为BEV和体素特征融合设计了两个网络，并引入了新型的跨模态体素和BEV Mamba块来解决特征错位问题，并实现高效而全面的场景感知。输出的体素特征被注入到BEV空间以弥补因高度压缩而损失的3D细节。对于特征解码，我们在BEV域中实现了渐进式查询生成模块，以缓解因特征压缩和小对象大小而在查询选择时产生的假阴性。最后，一个渐进式解码器可以顺序地聚合丰富的BEV特征和几何感知的体素特征，确保更精确的信心预测和边界框回归。在NuScenes数据集上，DDHFusion达到了最先进的性能，进一步的实验证明了它在其他同态融合方法上的优越性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08992v1">PDF</a> 13 pages, 9 figures</p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为DDHFusion的双域同构融合网络，该网络融合激光雷达点云特征和图像特征于统一的多模态BEV空间中，实现更精准的自主驾驶场景中的三维目标检测。其核心思想是通过构建适应图像特征转至BEV空间和稀疏体素空间的转换机制，设计创新的特征编码和特征解码网络，克服单一域融合方法的局限性，从而在车辆自主驾驶中实现高效全面的场景感知。在NuScenes数据集上的实验表明，DDHFusion方法达到领先水平。</p>
<p><strong>Key Takeaways</strong></p>
<p>以下是文本中提到的七个关键见解：</p>
<ol>
<li>DDHFusion网络采用双域同构融合策略，融合激光雷达点云特征和图像特征于统一的多模态BEV空间中。</li>
<li>提出利用LSS及语义感知特征采样模块实现图像特征至BEV和稀疏体素空间的转换，显著减少计算负担。</li>
<li>设计创新的特征编码网络，包括跨模态体素和BEV Mamba块，解决特征不对准问题并实现高效全面的场景感知。</li>
<li>输出体素特征注入到BEV空间以弥补因高度压缩导致的三维细节损失。</li>
<li>采用渐进式查询生成模块在BEV域中实现特征解码，减轻查询选择中的误判并应对小物体检测挑战。</li>
<li>通过渐进式解码器实现上下文丰富的BEV特征与几何感知体素特征的聚合，提高置信度预测和边界框回归的精度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08992">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-14\./crop_检测_分割_跟踪/2503.08992v1/page_0_0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f27831995ed3da335ed8ed34e7a39d4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6557828560719f73b982a03776940e7a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0df3871516458a1a9d77df9ab33288e6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b01790f2c5325334b11e9ddfd399ab86.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48515ad3eece21b29d3a884ffb7d3cf6.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="SegDesicNet-Lightweight-Semantic-Segmentation-in-Remote-Sensing-with-Geo-Coordinate-Embeddings-for-Domain-Adaptation"><a href="#SegDesicNet-Lightweight-Semantic-Segmentation-in-Remote-Sensing-with-Geo-Coordinate-Embeddings-for-Domain-Adaptation" class="headerlink" title="SegDesicNet: Lightweight Semantic Segmentation in Remote Sensing with   Geo-Coordinate Embeddings for Domain Adaptation"></a>SegDesicNet: Lightweight Semantic Segmentation in Remote Sensing with   Geo-Coordinate Embeddings for Domain Adaptation</h2><p><strong>Authors:Sachin Verma, Frank Lindseth, Gabriel Kiss</strong></p>
<p>Semantic segmentation is essential for analyzing highdefinition remote sensing images (HRSIs) because it allows the precise classification of objects and regions at the pixel level. However, remote sensing data present challenges owing to geographical location, weather, and environmental variations, making it difficult for semantic segmentation models to generalize across diverse scenarios. Existing methods are often limited to specific data domains and require expert annotators and specialized equipment for semantic labeling. In this study, we propose a novel unsupervised domain adaptation technique for remote sensing semantic segmentation by utilizing geographical coordinates that are readily accessible in remote sensing setups as metadata in a dataset. To bridge the domain gap, we propose a novel approach that considers the combination of an image&#39;s location encoding trait and the spherical nature of Earth&#39;s surface. Our proposed SegDesicNet module regresses the GRID positional encoding of the geo coordinates projected over the unit sphere to obtain the domain loss. Our experimental results demonstrate that the proposed SegDesicNet outperforms state of the art domain adaptation methods in remote sensing image segmentation, achieving an improvement of approximately ~6% in the mean intersection over union (MIoU) with a ~ 27% drop in parameter count on benchmarked subsets of the publicly available FLAIR #1 dataset. We also benchmarked our method performance on the custom split of the ISPRS Potsdam dataset. Our algorithm seeks to reduce the modeling disparity between artificial neural networks and human comprehension of the physical world, making the technology more human centric and scalable. </p>
<blockquote>
<p>语义分割对于分析高分辨率遥感图像（HRSI）至关重要，因为它可以在像素级别对对象和区域进行精确分类。然而，由于地理位置、天气和环境变化的影响，遥感数据存在挑战，使得语义分割模型难以在多种场景中进行推广。现有方法通常局限于特定数据域，需要专家注释器和专用设备进行语义标记。在本研究中，我们提出了一种新型的遥感语义分割无监督域适应技术，该技术利用遥感设置中可轻松访问的地理坐标作为数据集中的元数据。为了缩小域差距，我们提出了一种新方法，考虑图像位置编码特征与地球表面球形特性的结合。我们提出的SegDesicNet模块对投影视于单位球体上的地理坐标进行GRID位置编码，以获得域损失。实验结果表明，SegDesicNet在遥感图像分割方面的性能超过了先进的域适应方法，在公开可用的FLAIR # 1数据集的标准子集上，平均交并比（MIoU）提高了约6%，参数计数减少了约27%。我们还对ISPRS Potsdam数据集的自定分割方法进行了基准测试。我们的算法旨在减少人工神经网络与人类对物理世界的理解之间的建模差异，使技术更加以人为中心并可扩展。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08290v1">PDF</a> <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/WACV2025/papers/Verma_SegDesicNet_Lightweight_Semantic_Segmentation_in_Remote_Sensing_with_Geo-Coordinate_Embeddings_WACV_2025_paper.pdf">https://openaccess.thecvf.com/content/WACV2025/papers/Verma_SegDesicNet_Lightweight_Semantic_Segmentation_in_Remote_Sensing_with_Geo-Coordinate_Embeddings_WACV_2025_paper.pdf</a></p>
<p><strong>Summary</strong></p>
<p>本文探讨了语义分割在高分辨率遥感图像分析中的重要性及其所面临的挑战。为解决领域差异问题，提出了一种基于地理坐标的无监督域适应技术，利用遥感设置中的元数据（即地理位置编码特性）来缩小领域差距。实验结果表明，SegDesicNet模块在遥感图像分割的域适应方法中表现优异，提高了平均交并比（MIoU）约6%，参数数量减少了约27%。该算法旨在减少人工神经网络与人类对物理世界的理解之间的建模差异，使技术更加以人为中心并具备可扩展性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语义分割在高分辨率遥感图像分析中非常重要，可以实现像素级别的对象分类。</li>
<li>遥感数据因地理位置、天气和环境变化而带来挑战，使得语义分割模型难以在不同场景中进行泛化。</li>
<li>现有方法常常局限于特定数据域，需要专家标注和专门设备进行语义标注。</li>
<li>提出了一种基于地理坐标的无监督域适应技术，利用遥感设置中的元数据来解决领域差异问题。</li>
<li>SegDesicNet模块结合图像的地理位置编码特性和地球表面的球形特性，通过回归在球体上投影的地理坐标的GRID位置编码来获得域损失。</li>
<li>实验结果表明，SegDesicNet在遥感图像分割的域适应方法中表现优异，提高了平均交并比约6%。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08290">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-5569660a87a46e080703c1f8d3e31578.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d16d33194aec0c883572a405df984d1b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9cca623faced14bef73fe8c7f3956fdf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-79993605caaac9fae3ebeb7e7e965246.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-981a2d517b3fa03e2075462996681c0b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-96d1505f9a91831c2cb502f69967bb55.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6089824ad79aeedf6a4b9311eecffd0e.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Accelerate-3D-Object-Detection-Models-via-Zero-Shot-Attention-Key-Pruning"><a href="#Accelerate-3D-Object-Detection-Models-via-Zero-Shot-Attention-Key-Pruning" class="headerlink" title="Accelerate 3D Object Detection Models via Zero-Shot Attention Key   Pruning"></a>Accelerate 3D Object Detection Models via Zero-Shot Attention Key   Pruning</h2><p><strong>Authors:Lizhen Xu, Xiuxiu Bai, Xiaojun Jia, Jianwu Fang, Shanmin Pang</strong></p>
<p>Query-based methods with dense features have demonstrated remarkable success in 3D object detection tasks. However, the computational demands of these models, particularly with large image sizes and multiple transformer layers, pose significant challenges for efficient running on edge devices. Existing pruning and distillation methods either need retraining or are designed for ViT models, which are hard to migrate to 3D detectors. To address this issue, we propose a zero-shot runtime pruning method for transformer decoders in 3D object detection models. The method, termed tgGBC (trim keys gradually Guided By Classification scores), systematically trims keys in transformer modules based on their importance. We expand the classification score to multiply it with the attention map to get the importance score of each key and then prune certain keys after each transformer layer according to their importance scores. Our method achieves a 1.99x speedup in the transformer decoder of the latest ToC3D model, with only a minimal performance loss of less than 1%. Interestingly, for certain models, our method even enhances their performance. Moreover, we deploy 3D detectors with tgGBC on an edge device, further validating the effectiveness of our method. The code can be found at <a target="_blank" rel="noopener" href="https://github.com/iseri27/tg_gbc">https://github.com/iseri27/tg_gbc</a>. </p>
<blockquote>
<p>基于查询的方法和密集特征在3D对象检测任务中取得了显著的成功。然而，这些模型，特别是在处理大图像尺寸和多个转换器层时，对计算需求极高，这给在边缘设备上高效运行带来了重大挑战。现有的修剪和蒸馏方法都需要重新训练，或者专为ViT模型设计，很难迁移到3D检测器上。为了解决这一问题，我们提出了一种用于3D对象检测模型中变压器解码器的零拍摄运行时修剪方法。该方法被称为tgGBC（通过分类分数逐步引导修剪关键），它根据重要性系统地修剪变压器模块中的关键。我们将分类分数扩展到与注意力图相乘，以获得每个关键的重要性分数，然后根据其重要性分数在每层变压器之后修剪某些关键。我们的方法在最新的ToC3D模型的变压器解码器上实现了1.99倍的加速，性能损失仅低于1%。有趣的是，对于某些模型，我们的方法甚至提高了其性能。此外，我们在边缘设备上部署了带有tgGBC的3D检测器，进一步验证了我们的方法的有效性。代码可在<a target="_blank" rel="noopener" href="https://github.com/iseri27/tg_gbc">https://github.com/iseri27/tg_gbc</a>找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08101v2">PDF</a> The code can be found at <a target="_blank" rel="noopener" href="https://github.com/iseri27/tg_gbc">https://github.com/iseri27/tg_gbc</a></p>
<p><strong>Summary</strong><br>     基于查询的方法和密集特征在3D对象检测任务中取得了显著的成功，但其计算需求，特别是大型图像尺寸和多个transformer层，对边缘设备的运行效率提出了挑战。为解决这一问题，我们提出了一种名为tgGBC的零镜头运行时修剪方法，该方法基于分类分数引导逐步修剪transformer解码器中的关键元素。通过扩展分类分数并与注意力图相乘，得到每个关键元素的重要性分数，然后根据重要性分数逐层修剪关键元素。该方法在最新的ToC3D模型的transformer解码器中实现了1.99倍的速度提升，性能损失微乎其微（不到1%），甚至在某些模型中提高了性能。此外，我们在边缘设备上部署了带有tgGBC的3D检测器，进一步验证了该方法的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>查询方法和密集特征在3D对象检测中的成功应用。</li>
<li>计算需求，特别是大型图像尺寸和复杂模型结构，对边缘设备的运行效率构成挑战。</li>
<li>提出了一种名为tgGBC的零镜头运行时修剪方法，针对transformer解码器进行优化。</li>
<li>基于分类分数逐步修剪关键元素，实现模型加速。</li>
<li>在最新模型的transformer解码器中实现了显著的速度提升，同时保持较低的性能损失。</li>
<li>tgGBC方法能够提高某些模型的性能。</li>
<li>边缘设备上的部署验证了tgGBC方法的有效性。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08101">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-20ad4553d41d28e9178bb94eb6a9267b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fcd960e8fe7aab4b760823fb766eb29e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b6e8d1be338c3e06befe5e87361353df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff785287fc071e6f8215097c93de42d9.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Hierarchical-Cross-Modal-Alignment-for-Open-Vocabulary-3D-Object-Detection"><a href="#Hierarchical-Cross-Modal-Alignment-for-Open-Vocabulary-3D-Object-Detection" class="headerlink" title="Hierarchical Cross-Modal Alignment for Open-Vocabulary 3D Object   Detection"></a>Hierarchical Cross-Modal Alignment for Open-Vocabulary 3D Object   Detection</h2><p><strong>Authors:Youjun Zhao, Jiaying Lin, Rynson W. H. Lau</strong></p>
<p>Open-vocabulary 3D object detection (OV-3DOD) aims at localizing and classifying novel objects beyond closed sets. The recent success of vision-language models (VLMs) has demonstrated their remarkable capabilities to understand open vocabularies. Existing works that leverage VLMs for 3D object detection (3DOD) generally resort to representations that lose the rich scene context required for 3D perception. To address this problem, we propose in this paper a hierarchical framework, named HCMA, to simultaneously learn local object and global scene information for OV-3DOD. Specifically, we first design a Hierarchical Data Integration (HDI) approach to obtain coarse-to-fine 3D-image-text data, which is fed into a VLM to extract object-centric knowledge. To facilitate the association of feature hierarchies, we then propose an Interactive Cross-Modal Alignment (ICMA) strategy to establish effective intra-level and inter-level feature connections. To better align features across different levels, we further propose an Object-Focusing Context Adjustment (OFCA) module to refine multi-level features by emphasizing object-related features. Extensive experiments demonstrate that the proposed method outperforms SOTA methods on the existing OV-3DOD benchmarks. It also achieves promising OV-3DOD results even without any 3D annotations. </p>
<blockquote>
<p>开放词汇表下的三维物体检测（OV-3DOD）旨在定位并分类超出封闭集合范围的新物体。视觉语言模型（VLM）的近期成功展现出了它们理解开放词汇表的卓越能力。现有的利用VLM进行三维物体检测（3DOD）的工作通常依赖于丢失了用于三维感知所需丰富场景上下文的表示。为了解决这个问题，本文提出了一种层次化框架HCMA，可以同时学习局部物体和全局场景信息用于OV-3DOD。具体来说，我们首先设计了一种层次化数据集成（HDI）方法，从粗到细地获取三维图像文本数据，并将其输入到VLM中以提取以物体为中心的知识。为了促进特征层次的关联，我们随后提出了一种交互式跨模态对齐（ICMA）策略，以建立有效的内部和外部特征连接。为了更好地对齐不同层次的特征，我们进一步提出了一个物体聚焦上下文调整（OFCA）模块，通过强调与物体相关的特征来优化多层次特征。大量实验表明，所提出的方法在现有的OV-3DOD基准测试中优于最新方法。即使没有任何三维标注，它也实现了有前景的OV-3DOD结果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07593v1">PDF</a> AAAI 2025 (Extented Version). Project Page:   <a target="_blank" rel="noopener" href="https://youjunzhao.github.io/HCMA/">https://youjunzhao.github.io/HCMA/</a></p>
<p><strong>Summary</strong></p>
<p>本文提出了一种面向开放词汇3D对象检测的层次化框架HCMA，旨在同时学习局部对象信息和全局场景信息。通过设计分层数据集成（HDI）方法获取粗到细的3D图像文本数据，利用视觉语言模型（VLM）提取对象中心知识。为了建立特征层次之间的有效联系，提出了交互式跨模态对齐（ICMA）策略，并进一步优化了多级别特征，通过强调与对象相关的特征来实现对象聚焦上下文调整（OFCA）模块。实验表明，该方法在现有开放词汇3D对象检测基准测试中优于现有方法，并且在没有3D注释的情况下也取得了有前景的结果。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>开放词汇3D对象检测（OV-3DOD）旨在定位并分类超出封闭集的新对象。</li>
<li>现有利用视觉语言模型（VLM）进行3D对象检测（3DOD）的方法通常丢失了用于3D感知的丰富场景上下文信息。</li>
<li>提出的层次化框架HCMA能同时学习局部对象信息和全局场景信息。</li>
<li>分层数据集成（HDI）方法用于获取粗到细的3D图像文本数据。</li>
<li>交互式跨模态对齐（ICMA）策略有助于建立特征层次之间的有效联系。</li>
<li>对象聚焦上下文调整（OFCA）模块用于优化多级别特征，强调与对象相关的特征。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07593">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d4f5ab4abbec971f46d3507297b52295.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd9430c977f90904781a4dba09671659.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f790d754c5a053408bf5923e3f230f2e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5b185ba6a4331a1daaca4f36f9dd95b3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4872f391ec20a5bf8c2b344344a4dc91.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-85496aa7a0b91d2d83ea6d5050f46013.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="PointDiffuse-A-Dual-Conditional-Diffusion-Model-for-Enhanced-Point-Cloud-Semantic-Segmentation"><a href="#PointDiffuse-A-Dual-Conditional-Diffusion-Model-for-Enhanced-Point-Cloud-Semantic-Segmentation" class="headerlink" title="PointDiffuse: A Dual-Conditional Diffusion Model for Enhanced Point   Cloud Semantic Segmentation"></a>PointDiffuse: A Dual-Conditional Diffusion Model for Enhanced Point   Cloud Semantic Segmentation</h2><p><strong>Authors:Yong He, Hongshan Yu, Mingtao Feng, Tongjia Chen, Zechuan Li, Anwaar Ulhaq, Saeed Anwar, Ajmal Saeed Mian</strong></p>
<p>Diffusion probabilistic models are traditionally used to generate colors at fixed pixel positions in 2D images. Building on this, we extend diffusion models to point cloud semantic segmentation, where point positions also remain fixed, and the diffusion model generates point labels instead of colors. To accelerate the denoising process in reverse diffusion, we introduce a noisy label embedding mechanism. This approach integrates semantic information into the noisy label, providing an initial semantic reference that improves the reverse diffusion efficiency. Additionally, we propose a point frequency transformer that enhances the adjustment of high-level context in point clouds. To reduce computational complexity, we introduce the position condition into MLP and propose denoising PointNet to process the high-resolution point cloud without sacrificing geometric details. Finally, we integrate the proposed noisy label embedding, point frequency transformer and denoising PointNet in our proposed dual conditional diffusion model-based network (PointDiffuse) to perform large-scale point cloud semantic segmentation. Extensive experiments on five benchmarks demonstrate the superiority of PointDiffuse, achieving the state-of-the-art mIoU of 74.2% on S3DIS Area 5, 81.2% on S3DIS 6-fold and 64.8% on SWAN dataset. </p>
<blockquote>
<p>传统上，扩散概率模型被用于在二维图像的固定像素位置生成颜色。在此基础上，我们将扩散模型扩展到点云语义分割，其中点位置也是固定的，扩散模型生成点标签而不是颜色。为了加速反向扩散中的去噪过程，我们引入了带噪声标签嵌入机制。这种方法将语义信息集成到带噪声的标签中，提供了初始语义参考，提高了反向扩散的效率。此外，我们提出了一种点频变压器，它增强了点云中高级上下文的调整。为了降低计算复杂度，我们将位置条件引入多层感知器，并提出去噪PointNet处理高分辨率点云而不损失几何细节。最后，我们将所提出的带噪声标签嵌入、点频变压器和去噪PointNet集成到我们所提出的基于双条件扩散模型的网络（PointDiffuse）中，执行大规模点云语义分割。在五组基准测试上的广泛实验证明了PointDiffuse的优越性，在S3DIS Area 5上实现了最新的mIoU为74.2%，在S3DIS 6倍交叉验证上达到了81.2%，在SWAN数据集上达到了64.8%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06094v2">PDF</a> 8 pages, 3 figures, 7 tables</p>
<p><strong>Summary</strong>：<br>本文介绍了将扩散概率模型应用于点云语义分割的方法。通过引入噪声标签嵌入机制和点频变换器，提高了反向扩散过程的效率和点云上下文调整的准确性。同时，提出了降噪PointNet，可在不损失几何细节的情况下处理高分辨率点云。最终，将这些方法整合到提出的双条件扩散模型网络PointDiffuse中，实现了大规模点云语义分割的优异性能，在多个数据集上达到了最新水平的mIoU。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>扩散概率模型被应用于点云语义分割，生成点标签而非颜色。</li>
<li>引入噪声标签嵌入机制，将语义信息集成到噪声标签中，提高反向扩散效率。</li>
<li>提出点频变换器，改善点云中高级上下文的调整。</li>
<li>为了降低计算复杂度，将位置条件引入MLP，并提出降噪PointNet处理高分辨率点云，不损失几何细节。</li>
<li>整合噪声标签嵌入、点频变换器和降噪PointNet到PointDiffuse网络中。</li>
<li>PointDiffuse在多个数据集上实现了优异的性能，达到了最新水平的mIoU。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06094">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-a91a69c6de783e3e45946f008dfbdd77.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6cfaec9582b08dc0cd3089d9c2a565a4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ede1d4c8150dd0ccf47ff69c90080654.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="PerSense-Personalized-Instance-Segmentation-in-Dense-Images"><a href="#PerSense-Personalized-Instance-Segmentation-in-Dense-Images" class="headerlink" title="PerSense: Personalized Instance Segmentation in Dense Images"></a>PerSense: Personalized Instance Segmentation in Dense Images</h2><p><strong>Authors:Muhammad Ibraheem Siddiqui, Muhammad Umer Sheikh, Hassan Abid, Muhammad Haris Khan</strong></p>
<p>The emergence of foundational models has significantly advanced segmentation approaches. However, existing models still face challenges in automatically segmenting personalized instances in dense scenarios, where severe occlusions, scale variations, and background clutter hinder precise instance delineation. To address this, we propose PerSense, an end-to-end, training-free, and model-agnostic one-shot framework for personalized instance segmentation in dense images. We start with developing a new baseline capable of automatically generating instance-level point prompts via proposing a novel Instance Detection Module (IDM) that leverages density maps, encapsulating spatial distribution of objects in an image. To reduce false positives, we design the Point Prompt Selection Module (PPSM), which refines the output of IDM based on an adaptive threshold. Both IDM and PPSM seamlessly integrate into our model-agnostic framework. Furthermore, we introduce a feedback mechanism which enables PerSense to improve the accuracy of density maps by automating the exemplar selection process for density map generation. Finally, to promote algorithmic advances and effective tools for this relatively underexplored task, we introduce PerSense-D, an evaluation benchmark exclusive to personalized instance segmentation in dense images. Our extensive experiments establish PerSense superiority in dense scenarios compared to SOTA approaches. Additionally, our qualitative findings demonstrate the adaptability of our framework to images captured in-the-wild. </p>
<blockquote>
<p>基础模型的兴起已经显著推动了分割方法的发展。然而，现有模型在自动分割密集场景中个性化实例时仍面临挑战，其中严重的遮挡、尺度变化和背景杂乱阻碍了精确实例的划定。为了解决这个问题，我们提出了PerSense，这是一个端到端、无需训练、与模型无关的一次性框架，用于密集图像中的个性化实例分割。我们首先从开发一种能够自动生成实例级点提示的新基线开始，通过提出一种新的实例检测模块（IDM）来利用密度图，封装图像中对象的空间分布。为了减少误报，我们设计了点提示选择模块（PPSM），它基于自适应阈值对IDM的输出进行细化。IDM和PPSM无缝集成到我们的与模型无关框架中。此外，我们引入了一种反馈机制，使PerSense能够改进密度图的准确性，通过自动化密度图生成的样本选择过程。最后，为了推动这一相对未被充分探索的任务的算法进步和有效工具的出现，我们推出了PerSense-D，这是一个专门针对密集图像中个性化实例分割的评估基准。我们的大量实验表明，PerSense在密集场景中的表现优于最先进的方法。此外，我们的定性结果证明了我们的框架对野外拍摄图像的适应性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.13518v3">PDF</a> Technical report of PerSense</p>
<p><strong>Summary</strong></p>
<p>一种名为PerSense的端到端、无需训练、模型无关的个性化实例分割框架被提出，用于密集图像中的个性化实例分割。通过开发新的基准线技术，实现自动生成实例级点提示，并采用密度图来体现图像中物体的空间分布。此外，引入反馈机制自动选择样本以改进密度图的准确性，并推出PerSense-D评估基准，以促进该相对较少的任务的算法进步和有效工具的开发。实验证明，PerSense在密集场景中的表现优于现有先进技术。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了一种新的个性化实例分割框架PerSense，适用于密集图像。</li>
<li>开发了一种自动生成实例级点提示的新方法，通过实例检测模块（IDM）利用密度图体现物体空间分布。</li>
<li>引入了点提示选择模块（PPSM）以减少误报，并基于自适应阈值对IDM输出进行精细化处理。</li>
<li>PerSense框架无缝集成了IDM和PPSM。</li>
<li>引入反馈机制，自动选择样本改进密度图的准确性。</li>
<li>推出了PerSense-D评估基准，以促进个性化实例分割任务的算法进步和工具开发。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.13518">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-166e832903789c6f8412b64f452e3089.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-076a20d0f9bc70d46ee1a6540a0e5a9c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5d19c4adad211402a1fdf55ff4216c2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-94d2f2e3ba6ead6a971699612ab3c11a.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-14/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-14/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                    <span class="chip bg-color">检测/分割/跟踪</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-14/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-65ddb79f369ca600a59a6227f48da491.jpg" class="responsive-img" alt="无监督/半监督/对比学习">
                        
                        <span class="card-title">无监督/半监督/对比学习</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            无监督/半监督/对比学习 方向最新论文已更新，请持续关注 Update in 2025-03-14  Patch-Wise Hypergraph Contrastive Learning with Dual Normal Distribution   Weighting for Multi-Domain Stain Transfer
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-14
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    无监督/半监督/对比学习
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">无监督/半监督/对比学习</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-14/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-12251ebb19d0447c881400e74be59b48.jpg" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer 方向最新论文已更新，请持续关注 Update in 2025-03-14  Evaluating Visual Explanations of Attention Maps for Transformer-based   Medical Imaging
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-14
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">15444.6k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
