<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-14  Search-R1 Training LLMs to Reason and Leverage Search Engines with   Reinforcement Learning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-d435ed7ca1c36725b8fded997977b9fe.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-14
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    81 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-14-æ›´æ–°"><a href="#2025-03-14-æ›´æ–°" class="headerlink" title="2025-03-14 æ›´æ–°"></a>2025-03-14 æ›´æ–°</h1><h2 id="Search-R1-Training-LLMs-to-Reason-and-Leverage-Search-Engines-with-Reinforcement-Learning"><a href="#Search-R1-Training-LLMs-to-Reason-and-Leverage-Search-Engines-with-Reinforcement-Learning" class="headerlink" title="Search-R1: Training LLMs to Reason and Leverage Search Engines with   Reinforcement Learning"></a>Search-R1: Training LLMs to Reason and Leverage Search Engines with   Reinforcement Learning</h2><p><strong>Authors:Bowen Jin, Hansi Zeng, Zhenrui Yue, Dong Wang, Hamed Zamani, Jiawei Han</strong></p>
<p>Efficiently acquiring external knowledge and up-to-date information is essential for effective reasoning and text generation in large language models (LLMs). Retrieval augmentation and tool-use training approaches where a search engine is treated as a tool lack complex multi-turn retrieval flexibility or require large-scale supervised data. Prompting advanced LLMs with reasoning capabilities during inference to use search engines is not optimal, since the LLM does not learn how to optimally interact with the search engine. This paper introduces Search-R1, an extension of the DeepSeek-R1 model where the LLM learns â€“ solely through reinforcement learning (RL) â€“ to autonomously generate (multiple) search queries during step-by-step reasoning with real-time retrieval. Search-R1 optimizes LLM rollouts with multi-turn search interactions, leveraging retrieved token masking for stable RL training and a simple outcome-based reward function. Experiments on seven question-answering datasets show that Search-R1 improves performance by 26% (Qwen2.5-7B), 21% (Qwen2.5-3B), and 10% (LLaMA3.2-3B) over SOTA baselines. This paper further provides empirical insights into RL optimization methods, LLM choices, and response length dynamics in retrieval-augmented reasoning. The code and model checkpoints are available at <a target="_blank" rel="noopener" href="https://github.com/PeterGriffinJin/Search-R1">https://github.com/PeterGriffinJin/Search-R1</a>. </p>
<blockquote>
<p>åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­è¿›è¡Œæœ‰æ•ˆçš„æ¨ç†å’Œæ–‡æœ¬ç”Ÿæˆï¼Œé«˜æ•ˆè·å–å¤–éƒ¨çŸ¥è¯†å’Œæœ€æ–°ä¿¡æ¯è‡³å…³é‡è¦ã€‚æ£€ç´¢å¢å¼ºå’Œå·¥å…·ä½¿ç”¨è®­ç»ƒæ–¹æ³•ä¸­ï¼Œå°†æœç´¢å¼•æ“è§†ä¸ºå·¥å…·ç¼ºä¹å¤æ‚çš„å¤šè½®æ£€ç´¢çµæ´»æ€§ï¼Œæˆ–è€…éœ€è¦å¤§è§„æ¨¡ç›‘ç£æ•°æ®ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­æç¤ºå…·å¤‡æ¨ç†èƒ½åŠ›çš„å…ˆè¿›LLMä½¿ç”¨æœç´¢å¼•æ“å¹¶ä¸ç†æƒ³ï¼Œå› ä¸ºLLMå¹¶æ²¡æœ‰å­¦ä¹ å¦‚ä½•æœ€ä¼˜åœ°ä¸æœç´¢å¼•æ“è¿›è¡Œäº¤äº’ã€‚æœ¬æ–‡ä»‹ç»äº†Search-R1ï¼Œå®ƒæ˜¯DeepSeek-R1æ¨¡å‹çš„æ‰©å±•ï¼Œå…¶ä¸­LLMé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è‡ªä¸»å­¦ä¹ ï¼Œåœ¨é€æ­¥æ¨ç†è¿‡ç¨‹ä¸­è‡ªä¸»ç”Ÿæˆï¼ˆå¤šä¸ªï¼‰æœç´¢æŸ¥è¯¢ï¼Œå¹¶è¿›è¡Œå®æ—¶æ£€ç´¢ã€‚Search-R1é€šè¿‡å¤šè½®æœç´¢äº¤äº’ä¼˜åŒ–LLMçš„æ¨å‡ºï¼Œåˆ©ç”¨æ£€ç´¢ä»¤ç‰Œå±è”½è¿›è¡Œç¨³å®šçš„RLè®­ç»ƒå’Œä¸€ä¸ªç®€å•çš„åŸºäºç»“æœå¥–åŠ±å‡½æ•°ã€‚åœ¨ä¸ƒä¸ªé—®ç­”æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œç›¸è¾ƒäºSOTAåŸºçº¿ï¼ŒSearch-R1æé«˜äº†Qwen2.5-7Bçš„26%ã€Qwen2.5-3Bçš„21%å’ŒLLaMA3.2-3Bçš„10%çš„æ€§èƒ½ã€‚æœ¬æ–‡è¿˜æä¾›äº†å…³äºå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æ–¹æ³•ã€LLMé€‰æ‹©å’Œå“åº”é•¿åº¦åŠ¨åŠ›å­¦çš„å®è¯è§è§£ã€‚ä»£ç å’Œæ¨¡å‹æ£€æŸ¥ç‚¹å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/PeterGriffinJin/Search-R1%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/PeterGriffinJin/Search-R1è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09516v1">PDF</a> 16 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Search-R1æ¨¡å‹ï¼Œå®ƒæ˜¯DeepSeek-R1æ¨¡å‹çš„æ‰©å±•ã€‚è¯¥æ¨¡å‹ä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿè‡ªä¸»å­¦ä¹ ä¸æœç´¢å¼•æ“è¿›è¡Œå¤šè½®æœç´¢äº¤äº’ï¼Œä»¥å®æ—¶æ£€ç´¢æ–¹å¼ç”Ÿæˆå¤šä¸ªæœç´¢æŸ¥è¯¢ã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒï¼ŒSearch-R1ä¼˜åŒ–äº†LLMçš„æ¨ç†è¿‡ç¨‹ï¼Œå¹¶æä¾›äº†å…³äºRLä¼˜åŒ–æ–¹æ³•ã€LLMé€‰æ‹©å’Œå“åº”é•¿åº¦åŠ¨æ€çš„å®è¯è§è§£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSearch-R1åœ¨ä¸ƒä¸ªé—®ç­”æ•°æ®é›†ä¸Šçš„æ€§èƒ½è¾ƒç°æœ‰æŠ€æœ¯æé«˜äº†26%ã€21%å’Œ10%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Search-R1æ¨¡å‹æ˜¯DeepSeek-R1çš„æ‰©å±•ï¼Œä½¿LLMèƒ½å¤Ÿè‡ªä¸»å­¦ä¹ ä¸æœç´¢å¼•æ“è¿›è¡Œå¤šè½®æœç´¢äº¤äº’ã€‚</li>
<li>è¯¥æ¨¡å‹é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒï¼Œä½¿LLMèƒ½å¤Ÿè‡ªä¸»ç”Ÿæˆå¤šä¸ªæœç´¢æŸ¥è¯¢ï¼Œä»¥å®æ—¶æ£€ç´¢æ–¹å¼è¿›è¡Œæ¨ç†ã€‚</li>
<li>Search-R1ä¼˜åŒ–äº†LLMçš„æ¨ç†è¿‡ç¨‹ï¼Œé€šè¿‡æ£€ç´¢åˆ°çš„ä»¤ç‰Œå±è”½æ¥å®ç°ç¨³å®šçš„RLè®­ç»ƒã€‚</li>
<li>å“åº”é•¿åº¦åŠ¨åŠ›å­¦åœ¨æ£€ç´¢å¢å¼ºæ¨ç†ä¸­å¾ˆé‡è¦ï¼ŒSearch-R1å¯¹æ­¤è¿›è¡Œäº†æ·±å…¥ç ”ç©¶ã€‚</li>
<li>å®è¯åˆ†æè¡¨æ˜ï¼ŒSearch-R1åœ¨å¤šä¸ªé—®ç­”æ•°æ®é›†ä¸Šçš„æ€§èƒ½è¾ƒç°æœ‰æŠ€æœ¯æ˜¾è‘—æé«˜ã€‚</li>
<li>è¯¥æ¨¡å‹çš„ä»£ç å’Œæ¨¡å‹æ£€æŸ¥ç‚¹å·²å…¬å¼€å‘å¸ƒï¼Œå¯ä¾›ç ”ç©¶ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09516">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c71d0293ecd0ef9e3d7bef31402bf39b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c7677d1b5771eb2d8fc9f3c532c475d8.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Reinforcement-Learning-is-all-You-Need"><a href="#Reinforcement-Learning-is-all-You-Need" class="headerlink" title="Reinforcement Learning is all You Need"></a>Reinforcement Learning is all You Need</h2><p><strong>Authors:Yongsheng Lian</strong></p>
<p>Inspired by the success of DeepSeek R1 in reasoning via reinforcement learning without human feedback, we train a 3B language model using the Countdown Game with pure reinforcement learning. Our model outperforms baselines on four of five benchmarks, demonstrating improved generalization beyond its training data. Notably, response length does not correlate with reasoning quality, and while â€œaha momentsâ€ emerge, they do not always yield correct answers. These findings highlight the potential of RL-only training for reasoning enhancement and suggest future work on refining reward structures to bridge emergent insights with accuracy. </p>
<blockquote>
<p>å—DeepSeek R1é€šè¿‡æ— éœ€äººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ è¿›è¡Œæ¨ç†çš„æˆåŠŸå¯å‘ï¼Œæˆ‘ä»¬é‡‡ç”¨çº¯å¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•ï¼Œä½¿ç”¨å€’è®¡æ—¶æ¸¸æˆå¯¹3Bè¯­è¨€æ¨¡å‹è¿›è¡Œè®­ç»ƒã€‚è¯¥æ¨¡å‹åœ¨äº”ä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„å››ä¸ªä¸Šè¶…è¶Šäº†åŸºçº¿æ°´å¹³ï¼Œè¡¨ç°å‡ºåœ¨è®­ç»ƒæ•°æ®ä¹‹å¤–çš„æ”¹è¿›æ³›åŒ–èƒ½åŠ›ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå“åº”é•¿åº¦å¹¶ä¸ä¸æ¨ç†è´¨é‡ç›¸å…³ï¼Œè™½ç„¶ä¼šå‡ºç°â€œé¡¿æ‚Ÿæ—¶åˆ»â€ï¼Œä½†å¹¶ä¸æ€»èƒ½å¾—å‡ºæ­£ç¡®ç­”æ¡ˆã€‚è¿™äº›å‘ç°çªæ˜¾äº†ä»…ä½¿ç”¨å¼ºåŒ–å­¦ä¹ è¿›è¡Œæ¨ç†å¢å¼ºçš„æ½œåŠ›ï¼Œå¹¶å»ºè®®æœªæ¥çš„å·¥ä½œæ˜¯å¯¹å¥–åŠ±ç»“æ„è¿›è¡Œç»†åŒ–ï¼Œä»¥å°†çªå‘è§è§£ä¸å‡†ç¡®æ€§ç›¸ç»“åˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09512v1">PDF</a> 15 pages, 2 figures</p>
<p><strong>Summary</strong><br>åŸºäºDeepSeek R1é€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›è¡Œæ¨ç†çš„æˆåŠŸï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªè§„æ¨¡ä¸º3Bçš„è¯­è¨€æ¨¡å‹ï¼Œé‡‡ç”¨çº¯å¼ºåŒ–å­¦ä¹ çš„Countdown Gameæ–¹æ³•ã€‚æ¨¡å‹åœ¨äº”ä¸ªåŸºå‡†æµ‹è¯•ä¸­æœ‰å››ä¸ªè¡¨ç°è¶…è¶ŠåŸºçº¿ï¼Œå±•ç°å‡ºè¶…è¶Šè®­ç»ƒæ•°æ®çš„æ³›åŒ–èƒ½åŠ›ã€‚å“åº”é•¿åº¦å¹¶ä¸ä¸æ¨ç†è´¨é‡ç›¸å…³ï¼Œâ€œé¡¿æ‚Ÿæ—¶åˆ»â€çš„å‡ºç°å¹¶ä¸æ€»èƒ½å¸¦æ¥æ­£ç¡®ç­”æ¡ˆã€‚è¿™äº›å‘ç°çªæ˜¾å‡ºä»…ä½¿ç”¨å¼ºåŒ–å­¦ä¹ è¿›è¡Œæ¨ç†å¢å¼ºçš„æ½œåŠ›ï¼Œå¹¶å»ºè®®æœªæ¥æ”¹è¿›å¥–åŠ±ç»“æ„ä»¥å¼¥åˆæ–°å…´è§è§£ä¸å‡†ç¡®æ€§ä¹‹é—´çš„é¸¿æ²Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä½¿ç”¨çº¯å¼ºåŒ–å­¦ä¹ è®­ç»ƒäº†ä¸€ä¸ªè§„æ¨¡ä¸º3Bçš„è¯­è¨€æ¨¡å‹ï¼Œåœ¨äº”ä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>æ¨¡å‹å±•ç°å‡ºè¶…è¶Šè®­ç»ƒæ•°æ®çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>å“åº”é•¿åº¦å¹¶ä¸ä¸æ¨ç†è´¨é‡ç›´æ¥ç›¸å…³ã€‚</li>
<li>æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­ä¼šå‡ºç°â€œé¡¿æ‚Ÿæ—¶åˆ»â€ï¼Œä½†å¹¶ä¸æ€»èƒ½å¾—å‡ºæ­£ç¡®ç­”æ¡ˆã€‚</li>
<li>ä»…ä½¿ç”¨å¼ºåŒ–å­¦ä¹ è¿›è¡Œæ¨ç†å¢å¼ºå…·æœ‰æ½œåŠ›ã€‚</li>
<li>éœ€è¦æ”¹è¿›å¥–åŠ±ç»“æ„ä»¥æ›´å¥½åœ°ç»“åˆæ–°å…´è§è§£ä¸å‡†ç¡®æ€§ã€‚</li>
<li>ç ”ç©¶å¼ºè°ƒäº†å¼ºåŒ–å­¦ä¹ åœ¨æ¨åŠ¨è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›æ–¹é¢çš„ä½œç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09512">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-96b5b759d6a111f8988cdcb90bd6b15b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9eebf828d226d92d6028fe830402fb2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2022515b2d29b0db107173b88c21dce6.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="ReMA-Learning-to-Meta-think-for-LLMs-with-Multi-Agent-Reinforcement-Learning"><a href="#ReMA-Learning-to-Meta-think-for-LLMs-with-Multi-Agent-Reinforcement-Learning" class="headerlink" title="ReMA: Learning to Meta-think for LLMs with Multi-Agent Reinforcement   Learning"></a>ReMA: Learning to Meta-think for LLMs with Multi-Agent Reinforcement   Learning</h2><p><strong>Authors:Ziyu Wan, Yunxiang Li, Yan Song, Hanjing Wang, Linyi Yang, Mark Schmidt, Jun Wang, Weinan Zhang, Shuyue Hu, Ying Wen</strong></p>
<p>Recent research on Reasoning of Large Language Models (LLMs) has sought to further enhance their performance by integrating meta-thinking â€“ enabling models to monitor, evaluate, and control their reasoning processes for more adaptive and effective problem-solving. However, current single-agent work lacks a specialized design for acquiring meta-thinking, resulting in low efficacy. To address this challenge, we introduce Reinforced Meta-thinking Agents (ReMA), a novel framework that leverages Multi-Agent Reinforcement Learning (MARL) to elicit meta-thinking behaviors, encouraging LLMs to think about thinking. ReMA decouples the reasoning process into two hierarchical agents: a high-level meta-thinking agent responsible for generating strategic oversight and plans, and a low-level reasoning agent for detailed executions. Through iterative reinforcement learning with aligned objectives, these agents explore and learn collaboration, leading to improved generalization and robustness. Experimental results demonstrate that ReMA outperforms single-agent RL baselines on complex reasoning tasks, including competitive-level mathematical benchmarks and LLM-as-a-Judge benchmarks. Comprehensive ablation studies further illustrate the evolving dynamics of each distinct agent, providing valuable insights into how the meta-thinking reasoning process enhances the reasoning capabilities of LLMs. </p>
<blockquote>
<p>å…³äºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†çš„æœ€æ–°ç ”ç©¶è¯•å›¾é€šè¿‡èå…¥å…ƒæ€ç»´æ¥è¿›ä¸€æ­¥å¢å¼ºå…¶æ€§èƒ½â€”â€”ä½¿æ¨¡å‹èƒ½å¤Ÿç›‘æ§ã€è¯„ä¼°å’Œæ§åˆ¶åœ¨æ¨ç†è¿‡ç¨‹ä¸­çš„è¡¨ç°ï¼Œä»è€Œè¿›è¡Œæ›´åŠ é€‚åº”æ€§å’Œæœ‰æ•ˆçš„è§£å†³é—®é¢˜ã€‚ç„¶è€Œï¼Œå½“å‰çš„å•æ™ºèƒ½ä½“å·¥ä½œç¼ºä¹è·å–å…ƒæ€ç»´çš„ä¸“é—¨è®¾è®¡ï¼Œå¯¼è‡´æ•ˆç‡è¾ƒä½ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¼ºåŒ–å…ƒæ€ç»´æ™ºèƒ½ä½“ï¼ˆReMAï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰æ¥æ¿€å‘å…ƒæ€ç»´è¡Œä¸ºçš„æ–°å‹æ¡†æ¶ï¼Œé¼“åŠ±LLMè¿›è¡Œåæ€æ€§æ€è€ƒã€‚ReMAå°†æ¨ç†è¿‡ç¨‹è§£è€¦ä¸ºä¸¤ä¸ªå±‚æ¬¡æ™ºèƒ½ä½“ï¼šé«˜çº§å…ƒæ€ç»´æ™ºèƒ½ä½“è´Ÿè´£ç”Ÿæˆæˆ˜ç•¥æ€§ç›‘ç£å’Œè®¡åˆ’ï¼Œè€Œä½çº§æ¨ç†æ™ºèƒ½ä½“åˆ™è´Ÿè´£è¯¦ç»†æ‰§è¡Œã€‚é€šè¿‡ç›®æ ‡ä¸€è‡´çš„è¿­ä»£å¼ºåŒ–å­¦ä¹ ï¼Œè¿™äº›æ™ºèƒ½ä½“è¿›è¡Œæ¢ç´¢å’Œå­¦ä¹ åä½œï¼Œæé«˜äº†æ³›åŒ–å’Œç¨³å¥æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å¤æ‚çš„æ¨ç†ä»»åŠ¡ä¸Šï¼ŒReMAçš„è¡¨ç°è¶…è¿‡äº†å•æ™ºèƒ½ä½“RLåŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬ç«äº‰çº§åˆ«çš„æ•°å­¦åŸºå‡†æµ‹è¯•å’ŒLLMä½œä¸ºæ³•å®˜çš„åŸºå‡†æµ‹è¯•ã€‚å…¨é¢çš„æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥è¯´æ˜äº†æ¯ä¸ªç‹¬ç‰¹æ™ºèƒ½ä½“çš„åŠ¨æ€æ¼”å˜ï¼Œæä¾›äº†å…ƒæ€ç»´æ¨ç†è¿‡ç¨‹å¦‚ä½•å¢å¼ºLLMæ¨ç†èƒ½åŠ›çš„å®è´µè§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09501v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸå…³äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç ”ç©¶è‡´åŠ›äºé€šè¿‡å¼•å…¥å…ƒæ€ç»´è¿›ä¸€æ­¥æå‡å…¶æ€§èƒ½ã€‚å…ƒæ€ç»´ä½¿æ¨¡å‹èƒ½å¤Ÿç›‘æ§ã€è¯„ä¼°å’Œæ§åˆ¶ç³»ç»Ÿè‡ªèº«çš„æ¨ç†è¿‡ç¨‹ï¼Œä»è€Œå®ç°æ›´çµæ´»å’Œé«˜æ•ˆçš„é—®é¢˜è§£å†³ã€‚ä¸ºè§£å†³ç°æœ‰å•ä¸»ä½“åœ¨è·å–å…ƒæ€ç»´æ–¹é¢çš„ä¸è¶³ï¼Œæˆ‘ä»¬æå‡ºäº†å¼ºåŒ–å…ƒæ€ç»´ä»£ç†ï¼ˆReMAï¼‰è¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œåˆ©ç”¨å¤šä¸»ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰æ¿€å‘å…ƒæ€ç»´è¡Œä¸ºã€‚ReMAå°†æ¨ç†è¿‡ç¨‹åˆ’åˆ†ä¸ºä¸¤ä¸ªå±‚æ¬¡ä¸»ä½“ï¼šé«˜å±‚æ¬¡çš„å…ƒæ€ç»´ä¸»ä½“è´Ÿè´£ç”Ÿæˆæˆ˜ç•¥æ€§ç›‘ç£ä¸è®¡åˆ’ï¼Œä½å±‚æ¬¡çš„æ¨ç†ä¸»ä½“è´Ÿè´£å…·ä½“æ‰§è¡Œã€‚é€šè¿‡ç›®æ ‡ä¸€è‡´çš„è¿­ä»£å¼ºåŒ–å­¦ä¹ ï¼Œè¿™äº›ä¸»ä½“èƒ½å¤Ÿæ¢ç´¢ä¸åä½œï¼Œä»è€Œæé«˜æ³›åŒ–èƒ½åŠ›ä¸ç¨³å¥æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒReMAåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºå•ä¸»ä½“RLåŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬ç«äº‰çº§åˆ«çš„æ•°å­¦åŸºå‡†æµ‹è¯•å’ŒLLMä½œä¸ºæ³•å®˜çš„åŸºå‡†æµ‹è¯•ã€‚ç»¼åˆæ¶ˆèç ”ç©¶è¿›ä¸€æ­¥æ­ç¤ºäº†å„ä¸»ä½“çš„åŠ¨æ€å˜åŒ–è¿‡ç¨‹ï¼Œä¸ºå…ƒæ€ç»´æ¨ç†è¿‡ç¨‹å¦‚ä½•æå‡LLMçš„æ¨ç†èƒ½åŠ›æä¾›äº†å®è´µè§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶æ—¨åœ¨é€šè¿‡å¼•å…¥å…ƒæ€ç»´å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†æ€§èƒ½ã€‚</li>
<li>å½“å‰å•ä¸»ä½“åœ¨è·å–å…ƒæ€ç»´æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œéœ€è¦æ–°å‹æ¡†æ¶æ¥è§£å†³ã€‚</li>
<li>ReMAæ¡†æ¶åˆ©ç”¨å¤šä¸»ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰æ¿€å‘å…ƒæ€ç»´è¡Œä¸ºã€‚</li>
<li>ReMAå°†æ¨ç†è¿‡ç¨‹åˆ†ä¸ºä¸¤ä¸ªå±‚æ¬¡ä¸»ä½“ï¼šå…ƒæ€ç»´ä¸»ä½“å’Œæ¨ç†ä¸»ä½“ã€‚</li>
<li>é€šè¿‡è¿­ä»£å¼ºåŒ–å­¦ä¹ ï¼Œè¿™äº›ä¸»ä½“èƒ½å¤Ÿåä½œä»¥æé«˜æ³›åŒ–èƒ½åŠ›å’Œç¨³å¥æ€§ã€‚</li>
<li>ReMAåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºå•ä¸»ä½“RLåŸºå‡†æµ‹è¯•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09501">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-31bc4292f3764e1db00880af7f69f7be.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5087306fb21b19db6c96420a875b08bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f204a1cf33c2518ea1cd5e1c60c1b75.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="How-Well-Does-Your-Tabular-Generator-Learn-the-Structure-of-Tabular-Data"><a href="#How-Well-Does-Your-Tabular-Generator-Learn-the-Structure-of-Tabular-Data" class="headerlink" title="How Well Does Your Tabular Generator Learn the Structure of Tabular   Data?"></a>How Well Does Your Tabular Generator Learn the Structure of Tabular   Data?</h2><p><strong>Authors:Xiangjian Jiang, Nikola Simidjievski, Mateja Jamnik</strong></p>
<p>Heterogeneous tabular data poses unique challenges in generative modelling due to its fundamentally different underlying data structure compared to homogeneous modalities, such as images and text. Although previous research has sought to adapt the successes of generative modelling in homogeneous modalities to the tabular domain, defining an effective generator for tabular data remains an open problem. One major reason is that the evaluation criteria inherited from other modalities often fail to adequately assess whether tabular generative models effectively capture or utilise the unique structural information encoded in tabular data. In this paper, we carefully examine the limitations of the prevailing evaluation framework and introduce $\textbf{TabStruct}$, a novel evaluation benchmark that positions structural fidelity as a core evaluation dimension. Specifically, TabStruct evaluates the alignment of causal structures in real and synthetic data, providing a direct measure of how effectively tabular generative models learn the structure of tabular data. Through extensive experiments using generators from eight categories on seven datasets with expert-validated causal graphical structures, we show that structural fidelity offers a task-independent, domain-agnostic evaluation dimension. Our findings highlight the importance of tabular data structure and offer practical guidance for developing more effective and robust tabular generative models. Code is available at <a target="_blank" rel="noopener" href="https://github.com/SilenceX12138/TabStruct">https://github.com/SilenceX12138/TabStruct</a>. </p>
<blockquote>
<p>é’ˆå¯¹å¼‚è´¨è¡¨æ ¼æ•°æ®çš„ç”Ÿæˆå»ºæ¨¡é¢ä¸´ç‹¬ç‰¹æŒ‘æˆ˜ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºå…¶åŸºç¡€çš„æ•°æ®ç»“æ„ä¸åŒè´¨æ¨¡æ€ï¼ˆå¦‚å›¾åƒå’Œæ–‡æœ¬ï¼‰ç›¸æ¯”å­˜åœ¨æ ¹æœ¬ä¸Šçš„å·®å¼‚ã€‚å°½ç®¡ä¹‹å‰çš„ç ”ç©¶è¯•å›¾å°†åŒè´¨æ¨¡æ€ç”Ÿæˆå»ºæ¨¡çš„æˆåŠŸç»éªŒåº”ç”¨åˆ°è¡¨æ ¼é¢†åŸŸï¼Œä½†ä¸ºè¡¨æ ¼æ•°æ®å®šä¹‰æœ‰æ•ˆçš„ç”Ÿæˆå™¨ä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾æ€§é—®é¢˜ã€‚å…¶ä¸­ä¸€ä¸ªä¸»è¦åŸå› æ˜¯ï¼Œä»å…¶ä»–æ¨¡æ€ç»§æ‰¿çš„è¯„ä¼°æ ‡å‡†é€šå¸¸æ— æ³•å……åˆ†è¯„ä¼°è¡¨æ ¼ç”Ÿæˆæ¨¡å‹æ˜¯å¦æœ‰æ•ˆæ•è·æˆ–åˆ©ç”¨è¡¨æ ¼æ•°æ®ä¸­ç¼–ç çš„ç‹¬ç‰¹ç»“æ„ä¿¡æ¯ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»”ç»†ç ”ç©¶äº†ç°æœ‰è¯„ä¼°æ¡†æ¶çš„é™åˆ¶ï¼Œå¹¶å¼•å…¥äº†æ–°çš„è¯„ä¼°åŸºå‡†<strong>TabStruct</strong>ï¼Œå®ƒå°†ç»“æ„ä¿çœŸåº¦ä½œä¸ºæ ¸å¿ƒè¯„ä¼°ç»´åº¦ã€‚å…·ä½“æ¥è¯´ï¼ŒTabStructè¯„ä¼°çœŸå®å’Œåˆæˆæ•°æ®ä¸­å› æœç»“æ„çš„å¯¹é½æƒ…å†µï¼Œç›´æ¥è¡¡é‡è¡¨æ ¼ç”Ÿæˆæ¨¡å‹å­¦ä¹ è¡¨æ ¼æ•°æ®ç»“æ„çš„æ•ˆç‡ã€‚æˆ‘ä»¬é€šè¿‡ä½¿ç”¨å…«ä¸ªç±»åˆ«çš„ç”Ÿæˆå™¨åœ¨ä¸ƒä¸ªå…·æœ‰ä¸“å®¶éªŒè¯çš„å› æœå›¾å½¢ç»“æ„çš„æ•°æ®é›†ä¸Šè¿›è¡Œå¤§é‡å®éªŒï¼Œè¡¨æ˜ç»“æ„ä¿çœŸåº¦æä¾›äº†ä¸€ä¸ªä»»åŠ¡ç‹¬ç«‹ã€é¢†åŸŸæ— å…³çš„è¯„ä¼°ç»´åº¦ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜äº†è¡¨æ ¼æ•°æ®ç»“æ„çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºå¼€å‘æ›´æœ‰æ•ˆå’Œæ›´ç¨³å¥çš„è¡¨æ ¼ç”Ÿæˆæ¨¡å‹æä¾›äº†å®é™…æŒ‡å¯¼ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/SilenceX12138/TabStruct%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/SilenceX12138/TabStructæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09453v1">PDF</a> Accepted by ICLR 2025 workshops (DeLTa and SynthData)</p>
<p><strong>Summary</strong><br>å¼‚æ„å›¾æ•°æ®åœ¨ç”Ÿæˆæ¨¡å‹ä¸­å¸¦æ¥äº†ç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼Œä¸»è¦ç”±äºå®ƒçš„åº•å±‚æ•°æ®ç»“æ„ä¸å…¶ä»–æ¨¡æ€ï¼ˆå¦‚å›¾åƒå’Œæ–‡æœ¬ï¼‰æ ¹æœ¬ä¸åŒã€‚è™½ç„¶ä¹‹å‰çš„ç ”ç©¶å°è¯•å°†ç”Ÿæˆæ¨¡å‹åœ¨å•ä¸€æ¨¡æ€ä¸­çš„æˆåŠŸåº”ç”¨äºè¡¨æ ¼é¢†åŸŸï¼Œä½†ä¸ºè¡¨æ ¼æ•°æ®å®šä¹‰æœ‰æ•ˆçš„ç”Ÿæˆå™¨ä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾çš„é—®é¢˜ã€‚ä¸»è¦çš„åŸå› ä¹‹ä¸€æ˜¯ï¼Œä»å…¶ä»–æ¨¡æ€ç»§æ‰¿çš„è¯„ä¼°æ ‡å‡†å¾€å¾€æ— æ³•å……åˆ†è¯„ä¼°è¡¨æ ¼ç”Ÿæˆæ¨¡å‹æ˜¯å¦æœ‰æ•ˆåœ°æ•è·æˆ–åˆ©ç”¨è¡¨æ ¼æ•°æ®ä¸­çš„ç‹¬ç‰¹ç»“æ„ä¿¡æ¯ã€‚æœ¬æ–‡ä»”ç»†ç ”ç©¶äº†ç°æœ‰è¯„ä¼°æ¡†æ¶çš„å±€é™æ€§ï¼Œå¹¶å¼•å…¥äº†æ–°çš„è¯„ä¼°åŸºå‡†<strong>TabStruct</strong>ï¼Œå®ƒå°†ç»“æ„ä¿çœŸåº¦ä½œä¸ºæ ¸å¿ƒè¯„ä¼°ç»´åº¦ã€‚TabStructè¯„ä¼°çœŸå®å’Œåˆæˆæ•°æ®ä¸­å› æœç»“æ„çš„å¯¹é½æƒ…å†µï¼Œç›´æ¥è¡¡é‡è¡¨æ ¼ç”Ÿæˆæ¨¡å‹å­¦ä¹ è¡¨æ ¼æ•°æ®ç»“æ„çš„ç¨‹åº¦ã€‚é€šè¿‡å¹¿æ³›å®éªŒï¼Œä½¿ç”¨å…«ä¸ªç±»åˆ«çš„ç”Ÿæˆå™¨åœ¨ä¸ƒä¸ªå…·æœ‰ä¸“å®¶éªŒè¯çš„å› æœå›¾å½¢ç»“æ„çš„æ•°æ®é›†ä¸Šè¿›è¡Œæµ‹è¯•ï¼Œæˆ‘ä»¬è¯æ˜äº†ç»“æ„ä¿çœŸåº¦æä¾›äº†ä¸€ä¸ªä»»åŠ¡ç‹¬ç«‹ã€é¢†åŸŸé€šç”¨çš„è¯„ä¼°ç»´åº¦ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¼ºè°ƒäº†è¡¨æ ¼æ•°æ®ç»“æ„çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºå¼€å‘æ›´æœ‰æ•ˆå’Œç¨³å¥çš„è¡¨æ ¼ç”Ÿæˆæ¨¡å‹æä¾›äº†å®é™…æŒ‡å¯¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼‚æ„å›¾æ•°æ®åœ¨ç”Ÿæˆå»ºæ¨¡ä¸­å…·æœ‰ç‹¬ç‰¹æŒ‘æˆ˜ï¼Œéœ€è¦é€‚åº”å…¶ä¸å…¶ä»–æ¨¡æ€ï¼ˆå¦‚å›¾åƒå’Œæ–‡æœ¬ï¼‰ä¸åŒçš„åº•å±‚æ•°æ®ç»“æ„ã€‚</li>
<li>ç°æœ‰è¯„ä¼°æ ‡å‡†åœ¨è¯„ä¼°è¡¨æ ¼ç”Ÿæˆæ¨¡å‹æ—¶å¾€å¾€æ— æ³•å……åˆ†è¯„ä¼°æ¨¡å‹æ˜¯å¦æ•è·æˆ–åˆ©ç”¨è¡¨æ ¼æ•°æ®ä¸­çš„ç»“æ„ä¿¡æ¯ã€‚</li>
<li>å¼•å…¥æ–°çš„è¯„ä¼°åŸºå‡†<strong>TabStruct</strong>ï¼Œå°†ç»“æ„ä¿çœŸåº¦ä½œä¸ºæ ¸å¿ƒè¯„ä¼°ç»´åº¦ï¼Œä»¥è¡¡é‡è¡¨æ ¼ç”Ÿæˆæ¨¡å‹å­¦ä¹ è¡¨æ ¼æ•°æ®ç»“æ„çš„æœ‰æ•ˆæ€§ã€‚</li>
<li><strong>TabStruct</strong>é€šè¿‡è¯„ä¼°çœŸå®å’Œåˆæˆæ•°æ®ä¸­å› æœç»“æ„çš„å¯¹é½æ¥ç›´æ¥è¡¡é‡æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>å¹¿æ³›å®éªŒè¯æ˜ç»“æ„ä¿çœŸåº¦æ˜¯ä»»åŠ¡ç‹¬ç«‹ã€é¢†åŸŸé€šç”¨çš„è¯„ä¼°ç»´åº¦ã€‚</li>
<li>ç ”ç©¶å¼ºè°ƒäº†è¡¨æ ¼æ•°æ®ç»“æ„çš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09453">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-03d2d3c93b57de0f398ff3c58a6cdc62.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8cc666f96e48205b6c75be0897e6c1a1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4315267bddba31d163e83281aad50348.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2b244044f45cad64708a5c23fa5487a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-241f8c9dff50d4c19a9e8b5bd9bd3174.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Is-LLMs-Hallucination-Usable-LLM-based-Negative-Reasoning-for-Fake-News-Detection"><a href="#Is-LLMs-Hallucination-Usable-LLM-based-Negative-Reasoning-for-Fake-News-Detection" class="headerlink" title="Is LLMs Hallucination Usable? LLM-based Negative Reasoning for Fake News   Detection"></a>Is LLMs Hallucination Usable? LLM-based Negative Reasoning for Fake News   Detection</h2><p><strong>Authors:Chaowei Zhang, Zongling Feng, Zewei Zhang, Jipeng Qiang, Guandong Xu, Yun Li</strong></p>
<p>The questionable responses caused by knowledge hallucination may lead to LLMsâ€™ unstable ability in decision-making. However, it has never been investigated whether the LLMsâ€™ hallucination is possibly usable to generate negative reasoning for facilitating the detection of fake news. This study proposes a novel supervised self-reinforced reasoning rectification approach - SR$^3$ that yields both common reasonable reasoning and wrong understandings (negative reasoning) for news via LLMs reflection for semantic consistency learning. Upon that, we construct a negative reasoning-based news learning model called - \emph{NRFE}, which leverages positive or negative news-reasoning pairs for learning the semantic consistency between them. To avoid the impact of label-implicated reasoning, we deploy a student model - \emph{NRFE-D} that only takes news content as input to inspect the performance of our method by distilling the knowledge from \emph{NRFE}. The experimental results verified on three popular fake news datasets demonstrate the superiority of our method compared with three kinds of baselines including prompting on LLMs, fine-tuning on pre-trained SLMs, and other representative fake news detection methods. </p>
<blockquote>
<p>ç”±çŸ¥è¯†å¹»è§‰å¼•å‘çš„å¯ç–‘ååº”å¯èƒ½å¯¼è‡´å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å†³ç­–åˆ¶å®šä¸Šçš„ä¸ç¨³å®šèƒ½åŠ›ã€‚ç„¶è€Œï¼Œå°šæœªæœ‰ç ”ç©¶è°ƒæŸ¥å¤§å‹è¯­è¨€æ¨¡å‹çš„å¹»è§‰æ˜¯å¦å¯èƒ½è¢«ç”¨äºç”Ÿæˆè´Ÿé¢æ¨ç†ï¼Œä»¥ä¿ƒè¿›è™šå‡ä¿¡æ¯çš„æ£€æµ‹ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹çš„ç›‘ç£å¼è‡ªæˆ‘å¼ºåŒ–æ¨ç†ä¿®æ­£æ–¹æ³•â€”â€”SR$^3$ï¼Œè¯¥æ–¹æ³•é€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹çš„åæ€ï¼Œå¯¹æ–°é—»è¿›è¡Œè¯­ä¹‰ä¸€è‡´æ€§å­¦ä¹ ï¼Œä»è€Œäº§ç”Ÿåˆç†çš„é€šç”¨æ¨ç†å’Œé”™è¯¯ç†è§£ï¼ˆè´Ÿé¢æ¨ç†ï¼‰ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŸºäºè´Ÿé¢æ¨ç†çš„æ–°é—»å­¦ä¹ æ¨¡å‹â€”â€”NRFEï¼Œè¯¥æ¨¡å‹åˆ©ç”¨æ­£é¢æˆ–è´Ÿé¢æ–°é—»æ¨ç†å¯¹æ¥å­¦ä¹ å®ƒä»¬ä¹‹é—´çš„è¯­ä¹‰ä¸€è‡´æ€§ã€‚ä¸ºäº†é¿å…æ ‡ç­¾éšå«æ¨ç†çš„å½±å“ï¼Œæˆ‘ä»¬éƒ¨ç½²äº†ä¸€ä¸ªä»…å°†æ–°é—»å†…å®¹ä½œä¸ºè¾“å…¥çš„å­¦ç”Ÿæ¨¡å‹NRFE-Dï¼Œé€šè¿‡ä»NRFEä¸­æç‚¼çŸ¥è¯†æ¥æ£€æŸ¥æˆ‘ä»¬æ–¹æ³•çš„æ€§èƒ½ã€‚åœ¨ä¸‰ä¸ªæµè¡Œçš„è™šå‡æ–°é—»æ•°æ®é›†ä¸Šè¿›è¡Œå®éªŒéªŒè¯ï¼Œç»“æœè¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•ä¸åŒ…æ‹¬åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸Šè¿›è¡Œæç¤ºã€åœ¨é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ä¸Šè¿›è¡Œå¾®è°ƒä»¥åŠå…¶ä»–ä»£è¡¨æ€§è™šå‡æ–°é—»æ£€æµ‹æ–¹æ³•åœ¨å†…çš„ä¸‰ç§åŸºçº¿ç›¸æ¯”å…·æœ‰ä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09153v1">PDF</a> 9 pages, 12 figures, conference</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬æ¢è®¨äº†çŸ¥è¯†å¹»è§‰å¯¼è‡´çš„å¯ç–‘å›åº”å¯èƒ½ä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å†³ç­–èƒ½åŠ›ä¸Šè¡¨ç°ä¸ç¨³å®šçš„é—®é¢˜ã€‚ç„¶è€Œï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåä¸ºSR$^3$çš„æ–°å‹ç›‘ç£è‡ªæˆ‘å¼ºåŒ–æ¨ç†æ ¡æ­£æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡LLMsçš„åæ€è¿›è¡Œè¯­ä¹‰ä¸€è‡´æ€§å­¦ä¹ ï¼Œäº§ç”Ÿåˆç†çš„æ¨ç†å’Œé”™è¯¯çš„è®¤çŸ¥ï¼ˆè´Ÿé¢æ¨ç†ï¼‰ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŸºäºè´Ÿé¢æ¨ç†çš„æ–°é—»å­¦ä¹ æ¨¡å‹NRFEï¼Œåˆ©ç”¨æ­£é¢æˆ–è´Ÿé¢æ–°é—»æ¨ç†å¯¹è¿›è¡Œè¯­ä¹‰ä¸€è‡´æ€§å­¦ä¹ ã€‚ä¸ºäº†é¿å…æ ‡ç­¾éšå«æ¨ç†çš„å½±å“ï¼Œæˆ‘ä»¬éƒ¨ç½²äº†ä¸€ä¸ªä»…å°†æ–°é—»å†…å®¹ä½œä¸ºè¾“å…¥çš„å­¦ç”Ÿæ¨¡å‹NRFE-Dï¼Œä»¥æ£€éªŒæˆ‘ä»¬æ–¹æ³•çš„æ•ˆæœã€‚é€šè¿‡åœ¨ä¸‰å¥—æµè¡Œçš„å‡æ–°é—»æ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯ï¼Œä¸åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å…·æœ‰ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>çŸ¥è¯†å¹»è§‰å¯èƒ½å¯¼è‡´å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å†³ç­–èƒ½åŠ›ä¸ç¨³å®šã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹ç›‘ç£è‡ªæˆ‘å¼ºåŒ–æ¨ç†æ ¡æ­£æ–¹æ³•SR$^3$ã€‚</li>
<li>SR$^3$èƒ½äº§ç”Ÿåˆç†çš„æ¨ç†å’Œé”™è¯¯çš„è®¤çŸ¥ï¼ˆè´Ÿé¢æ¨ç†ï¼‰ã€‚</li>
<li>åŸºäºè´Ÿé¢æ¨ç†æ„å»ºäº†æ–°é—»å­¦ä¹ æ¨¡å‹NRFEã€‚</li>
<li>NRFEåˆ©ç”¨æ­£é¢æˆ–è´Ÿé¢æ–°é—»æ¨ç†å¯¹è¿›è¡Œè¯­ä¹‰ä¸€è‡´æ€§å­¦ä¹ ã€‚</li>
<li>å­¦ç”Ÿæ¨¡å‹NRFE-Dç”¨äºæ£€éªŒæ–¹æ³•çš„æ€§èƒ½ï¼Œä»…å°†æ–°é—»å†…å®¹ä½œä¸ºè¾“å…¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09153">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d18b09b2f75dc3e763d2f0acd4757b68.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bdacbe894ee4b86a2edd0e140150ae4d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d1177f1b1d45ec3dcf9ccf983c0fc2d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a2ccdb2a4c0d80874e5925d67247a825.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f9d7bede9cfd713feb2b32bcc3fe6d2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-45e4117a595b948f611ebf85835361f6.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Memory-enhanced-Retrieval-Augmentation-for-Long-Video-Understanding"><a href="#Memory-enhanced-Retrieval-Augmentation-for-Long-Video-Understanding" class="headerlink" title="Memory-enhanced Retrieval Augmentation for Long Video Understanding"></a>Memory-enhanced Retrieval Augmentation for Long Video Understanding</h2><p><strong>Authors:Huaying Yuan, Zheng Liu, Minhao Qin, Hongjin Qian, Y Shu, Zhicheng Dou, Ji-Rong Wen</strong></p>
<p>Retrieval-augmented generation (RAG) shows strong potential in addressing long-video understanding (LVU) tasks. However, traditional RAG methods remain fundamentally limited due to their dependence on explicit search queries, which are unavailable in many situations. To overcome this challenge, we introduce a novel RAG-based LVU approach inspired by the cognitive memory of human beings, which is called MemVid. Our approach operates with four basics steps: memorizing holistic video information, reasoning about the taskâ€™s information needs based on the memory, retrieving critical moments based on the information needs, and focusing on the retrieved moments to produce the final answer. To enhance the systemâ€™s memory-grounded reasoning capabilities and achieve optimal end-to-end performance, we propose a curriculum learning strategy. This approach begins with supervised learning on well-annotated reasoning results, then progressively explores and reinforces more plausible reasoning outcomes through reinforcement learning. We perform extensive evaluations on popular LVU benchmarks, including MLVU, VideoMME and LVBench. In our experiment, MemVid significantly outperforms existing RAG-based methods and popular LVU models, which demonstrate the effectiveness of our approach. Our model and source code will be made publicly available upon acceptance. </p>
<blockquote>
<p>å¢å¼ºæ£€ç´¢ç”Ÿæˆï¼ˆRAGï¼‰åœ¨é•¿è§†é¢‘ç†è§£ï¼ˆLVUï¼‰ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºå¼ºå¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„RAGæ–¹æ³•ä»ç„¶å­˜åœ¨æ ¹æœ¬ä¸Šçš„å±€é™æ€§ï¼Œå› ä¸ºå®ƒä»¬ä¾èµ–äºè®¸å¤šæƒ…å†µä¸‹å¹¶ä¸å¯ç”¨çš„æ˜ç¡®æœç´¢æŸ¥è¯¢ã€‚ä¸ºäº†å…‹æœè¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å—äººç±»è®¤çŸ¥è®°å¿†å¯å‘çš„æ–°å‹åŸºäºRAGçš„LVUæ–¹æ³•ï¼Œç§°ä¸ºMemVidã€‚æˆ‘ä»¬çš„æ–¹æ³•æœ‰å››ä¸ªåŸºæœ¬æ­¥éª¤ï¼šè®°å¿†æ•´ä½“è§†é¢‘ä¿¡æ¯ï¼ŒåŸºäºè®°å¿†å¯¹ä»»åŠ¡ä¿¡æ¯éœ€æ±‚è¿›è¡Œæ¨ç†ï¼ŒåŸºäºä¿¡æ¯éœ€æ±‚æ£€ç´¢å…³é”®æ—¶åˆ»ï¼Œå¹¶ä¸“æ³¨äºæ£€ç´¢åˆ°çš„æ—¶åˆ»ä»¥äº§ç”Ÿæœ€ç»ˆç­”æ¡ˆã€‚ä¸ºäº†æé«˜ç³»ç»Ÿçš„åŸºäºè®°å¿†çš„æ¨ç†èƒ½åŠ›å¹¶å®ç°ç«¯åˆ°ç«¯çš„æœ€ä½³æ€§èƒ½ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ã€‚è¯¥æ–¹æ³•é¦–å…ˆä»ç»è¿‡è‰¯å¥½æ³¨é‡Šçš„æ¨ç†ç»“æœå¼€å§‹ç›‘ç£å­¦ä¹ ï¼Œç„¶åé€šè¿‡å¼ºåŒ–å­¦ä¹ é€æ­¥æ¢ç´¢å’Œå¼ºåŒ–æ›´åˆç†çš„æ¨ç†ç»“æœã€‚æˆ‘ä»¬åœ¨æµè¡Œçš„LVUåŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ï¼ŒåŒ…æ‹¬MLVUã€VideoMMEå’ŒLVBenchã€‚åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼ŒMemVidæ˜¾è‘—ä¼˜äºç°æœ‰çš„åŸºäºRAGçš„æ–¹æ³•å’Œæµè¡Œçš„LVUæ¨¡å‹ï¼Œè¿™è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„æ¨¡å‹å’Œæºä»£ç å°†åœ¨æ¥å—åå…¬å¼€å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09149v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºäººç±»è®¤çŸ¥è®°å¿†ï¼Œæå‡ºä¸€ç§æ–°å‹çš„è§†é¢‘ç†è§£æ–¹æ³•MemVidï¼Œè§£å†³äº†æ£€ç´¢å¢å¼ºç”Ÿæˆæ¨¡å‹ä¾èµ–æ˜ç¡®æœç´¢æŸ¥è¯¢çš„é—®é¢˜ã€‚å®ƒé€šè¿‡å››ä¸ªæ­¥éª¤å®ç°ï¼šè®°å¿†æ•´ä½“è§†é¢‘ä¿¡æ¯ï¼ŒåŸºäºä»»åŠ¡éœ€æ±‚è¿›è¡Œæ¨ç†ï¼Œæ ¹æ®éœ€æ±‚æ£€ç´¢å…³é”®ç¬é—´ï¼Œèšç„¦äºæ£€ç´¢ç¬é—´ç”Ÿæˆç­”æ¡ˆã€‚åŒæ—¶å¼•å…¥è¯¾ç¨‹å­¦ä¹ ç­–ç•¥æé«˜ç³»ç»Ÿè®°å¿†æ¨ç†èƒ½åŠ›å¹¶ä¼˜åŒ–ç«¯åˆ°ç«¯æ€§èƒ½ã€‚åœ¨æµè¡Œè§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šç°æœ‰æ£€ç´¢å¢å¼ºç”Ÿæˆæ–¹æ³•å’Œæµè¡Œè§†é¢‘ç†è§£æ¨¡å‹ã€‚å…¬å¼€æ¨¡å‹å’Œæºä»£ç ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä¸€ã€æå‡ºäº†ä¸€ç§åŸºäºè®°å¿†çš„è§†é¢‘ç†è§£æ–¹æ³•MemVidï¼Œè§£å†³äº†ä¼ ç»Ÿæ£€ç´¢å¢å¼ºç”Ÿæˆæ¨¡å‹ä¾èµ–æ˜ç¡®æœç´¢æŸ¥è¯¢çš„é—®é¢˜ã€‚<br>äºŒã€MemVidé€šè¿‡å››ä¸ªæ­¥éª¤å®ç°è§†é¢‘ç†è§£ï¼šè®°å¿†æ•´ä½“ä¿¡æ¯ã€åŸºäºä»»åŠ¡éœ€æ±‚æ¨ç†ã€æ£€ç´¢å…³é”®ç¬é—´å’Œç”Ÿæˆç­”æ¡ˆã€‚<br>ä¸‰.å¼•å…¥è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ä»¥å¢å¼ºç³»ç»Ÿçš„è®°å¿†æ¨ç†èƒ½åŠ›å¹¶ä¼˜åŒ–ç«¯åˆ°ç«¯æ€§èƒ½ã€‚<br>å››ã€MemVidåœ¨å¤šä¸ªæµè¡Œçš„é•¿è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚<br>äº”ã€MemVidæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„æ£€ç´¢å¢å¼ºç”Ÿæˆæ–¹æ³•å’Œä¸»æµè§†é¢‘ç†è§£æ¨¡å‹ã€‚<br>å…­ã€è¯¥æ¨¡å‹å¯æœ‰æ•ˆåº”å¯¹æ— æ˜ç¡®æœç´¢æŸ¥è¯¢æƒ…å†µä¸‹çš„è§†é¢‘ç†è§£ä»»åŠ¡ã€‚</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09149">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f9a0ff267abfca3729ffbf9f81c3d245.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c8bdc041fc4fa19f242d873fd75bb82.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dd3b33737d08389bc981f9b064296c5f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e47fab8e66a46b9e767d45e6280c49cc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-04a4fd82e7fe59241ef7f600abbdf2ac.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Adaptive-Backdoor-Attacks-with-Reasonable-Constraints-on-Graph-Neural-Networks"><a href="#Adaptive-Backdoor-Attacks-with-Reasonable-Constraints-on-Graph-Neural-Networks" class="headerlink" title="Adaptive Backdoor Attacks with Reasonable Constraints on Graph Neural   Networks"></a>Adaptive Backdoor Attacks with Reasonable Constraints on Graph Neural   Networks</h2><p><strong>Authors:Xuewen Dong, Jiachen Li, Shujun Li, Zhichao You, Qiang Qu, Yaroslav Kholodov, Yulong Shen</strong></p>
<p>Recent studies show that graph neural networks (GNNs) are vulnerable to backdoor attacks. Existing backdoor attacks against GNNs use fixed-pattern triggers and lack reasonable trigger constraints, overlooking individual graph characteristics and rendering insufficient evasiveness. To tackle the above issues, we propose ABARC, the first Adaptive Backdoor Attack with Reasonable Constraints, applying to both graph-level and node-level tasks in GNNs. For graph-level tasks, we propose a subgraph backdoor attack independent of the graphâ€™s topology. It dynamically selects trigger nodes for each target graph and modifies node features with constraints based on graph similarity, feature range, and feature type. For node-level tasks, our attack begins with an analysis of node features, followed by selecting and modifying trigger features, which are then constrained by node similarity, feature range, and feature type. Furthermore, an adaptive edge-pruning mechanism is designed to reduce the impact of neighbors on target nodes, ensuring a high attack success rate (ASR). Experimental results show that even with reasonable constraints for attack evasiveness, our attack achieves a high ASR while incurring a marginal clean accuracy drop (CAD). When combined with the state-of-the-art defense randomized smoothing (RS) method, our attack maintains an ASR over 94%, surpassing existing attacks by more than 7%. </p>
<blockquote>
<p>æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œå›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰å®¹æ˜“å—åˆ°åé—¨æ”»å‡»çš„å½±å“ã€‚ç°æœ‰çš„é’ˆå¯¹GNNsçš„åé—¨æ”»å‡»ä½¿ç”¨å›ºå®šæ¨¡å¼çš„è§¦å‘å™¨ï¼Œç¼ºä¹åˆç†çš„è§¦å‘çº¦æŸï¼Œå¿½è§†äº†å•ä¸ªå›¾çš„ç‰¹æ€§ï¼Œå¯¼è‡´é€ƒé¿æ€§ä¸è¶³ã€‚ä¸ºäº†è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ABARCï¼Œå³å…·æœ‰åˆç†çº¦æŸçš„é€‚åº”æ€§åé—¨æ”»å‡»ï¼Œé€‚ç”¨äºGNNsçš„å›¾çº§å’ŒèŠ‚ç‚¹çº§ä»»åŠ¡ã€‚å¯¹äºå›¾çº§ä»»åŠ¡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç‹¬ç«‹äºå›¾æ‹“æ‰‘çš„å­å›¾åé—¨æ”»å‡»ã€‚å®ƒåŠ¨æ€ä¸ºæ¯ä¸ªç›®æ ‡å›¾é€‰æ‹©è§¦å‘èŠ‚ç‚¹ï¼Œå¹¶åŸºäºå›¾çš„ç›¸ä¼¼æ€§ã€ç‰¹å¾èŒƒå›´å’Œç‰¹å¾ç±»å‹å¯¹èŠ‚ç‚¹ç‰¹å¾è¿›è¡Œä¿®æ”¹å’Œçº¦æŸã€‚å¯¹äºèŠ‚ç‚¹çº§ä»»åŠ¡ï¼Œæˆ‘ä»¬çš„æ”»å‡»é¦–å…ˆåˆ†æèŠ‚ç‚¹ç‰¹å¾ï¼Œç„¶åé€‰æ‹©å¹¶ä¿®æ”¹è§¦å‘ç‰¹å¾ï¼Œè¿™äº›ç‰¹å¾å—åˆ°èŠ‚ç‚¹ç›¸ä¼¼æ€§ã€ç‰¹å¾èŒƒå›´å’Œç‰¹å¾ç±»å‹çš„çº¦æŸã€‚æ­¤å¤–ï¼Œè¿˜è®¾è®¡äº†ä¸€ç§è‡ªé€‚åº”çš„è¾¹ç¼˜ä¿®å‰ªæœºåˆ¶ï¼Œä»¥å‡å°‘é‚»å±…èŠ‚ç‚¹å¯¹ç›®æ ‡èŠ‚ç‚¹çš„å½±å“ï¼Œç¡®ä¿é«˜æ”»å‡»æˆåŠŸç‡ï¼ˆASRï¼‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿å¯¹æ”»å‡»é€ƒé¿æ€§è¿›è¡Œåˆç†çº¦æŸï¼Œæˆ‘ä»¬çš„æ”»å‡»ä»èƒ½è¾¾åˆ°é«˜ASRï¼ŒåŒæ—¶åªå¼•èµ·è½»å¾®çš„æ¸…æ´ç²¾åº¦ä¸‹é™ï¼ˆCADï¼‰ã€‚å½“ä¸æœ€å…ˆè¿›çš„é˜²å¾¡éšæœºå¹³æ»‘ï¼ˆRSï¼‰æ–¹æ³•ç›¸ç»“åˆæ—¶ï¼Œæˆ‘ä»¬çš„æ”»å‡»ä¿æŒè¶…è¿‡94%çš„ASRï¼Œè¶…è¿‡äº†ç°æœ‰æ”»å‡»çš„7%ä»¥ä¸Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09049v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>è¿‘æœŸç ”ç©¶å‘ç°å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰å®¹æ˜“å—åˆ°åé—¨æ”»å‡»çš„å½±å“ã€‚ç°æœ‰é’ˆå¯¹GNNsçš„åé—¨æ”»å‡»ä½¿ç”¨å›ºå®šæ¨¡å¼è§¦å‘å™¨ï¼Œç¼ºä¹åˆç†çš„è§¦å‘å™¨çº¦æŸï¼Œå¿½ç•¥äº†å•ä¸ªå›¾çš„ç‰¹æ€§ï¼Œå¯¼è‡´æ”»å‡»éšè”½æ€§ä¸è¶³ã€‚ä¸ºè§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ABARCï¼Œè¿™æ˜¯ä¸€ç§å…·æœ‰åˆç†çº¦æŸçš„é€‚åº”æ€§åé—¨æ”»å‡»æ–¹æ³•ï¼Œé€‚ç”¨äºGNNsçš„å›¾çº§å’ŒèŠ‚ç‚¹çº§ä»»åŠ¡ã€‚é’ˆå¯¹å›¾çº§ä»»åŠ¡ï¼Œæˆ‘ä»¬æå‡ºäº†ç‹¬ç«‹äºå›¾æ‹“æ‰‘çš„å­å›¾åé—¨æ”»å‡»ï¼ŒåŠ¨æ€ä¸ºæ¯ç›®æ ‡å›¾é€‰æ‹©è§¦å‘èŠ‚ç‚¹å¹¶åŸºäºå›¾ç›¸ä¼¼æ€§ã€ç‰¹å¾èŒƒå›´å’Œç‰¹å¾ç±»å‹ä¿®æ”¹èŠ‚ç‚¹ç‰¹å¾ã€‚é’ˆå¯¹èŠ‚ç‚¹çº§ä»»åŠ¡ï¼Œæˆ‘ä»¬çš„æ”»å‡»é¦–å…ˆåˆ†æèŠ‚ç‚¹ç‰¹å¾ï¼Œç„¶åé€‰æ‹©å’Œä¿®æ”¹è§¦å‘ç‰¹å¾ï¼Œå—èŠ‚ç‚¹ç›¸ä¼¼æ€§ã€ç‰¹å¾èŒƒå›´å’Œç‰¹å¾ç±»å‹çš„çº¦æŸã€‚æ­¤å¤–ï¼Œè®¾è®¡äº†è‡ªé€‚åº”çš„è¾¹å‰ªææœºåˆ¶æ¥å‡å°‘é‚»å±…èŠ‚ç‚¹å¯¹ç›®æ ‡èŠ‚ç‚¹çš„å½±å“ï¼Œç¡®ä¿é«˜æ”»å‡»æˆåŠŸç‡ï¼ˆASRï¼‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ”»å‡»åœ¨å…·æœ‰åˆç†çš„æ”»å‡»éšè”½æ€§çº¦æŸçš„åŒæ—¶ï¼Œå®ç°äº†é«˜ASRå’Œè¾ƒå°çš„æ¸…æ´ç²¾åº¦ä¸‹é™ï¼ˆCADï¼‰ã€‚ä¸æœ€å…ˆè¿›çš„é˜²å¾¡éšæœºå¹³æ»‘ï¼ˆRSï¼‰æ–¹æ³•ç›¸ç»“åˆæ—¶ï¼Œæˆ‘ä»¬çš„æ”»å‡»ç»´æŒè¶…è¿‡94%çš„ASRï¼Œè¶…è¿‡äº†ç°æœ‰æ”»å‡»è¶…è¿‡7%ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰é¢ä¸´åé—¨æ”»å‡»çš„é£é™©ã€‚</li>
<li>ç°æœ‰åé—¨æ”»å‡»æ–¹æ³•ä½¿ç”¨å›ºå®šæ¨¡å¼è§¦å‘å™¨ï¼Œç¼ºä¹ä¸ªæ€§åŒ–ä¸”éšè”½æ€§ä¸è¶³ã€‚</li>
<li>ABARCæ˜¯ä¸€ç§é€‚åº”æ€§åé—¨æ”»å‡»æ–¹æ³•ï¼Œé€‚ç”¨äºå›¾ç¥ç»ç½‘ç»œä¸­çš„å›¾çº§å’ŒèŠ‚ç‚¹çº§ä»»åŠ¡ã€‚</li>
<li>ABARCé’ˆå¯¹å›¾çº§ä»»åŠ¡æå‡ºå­å›¾åé—¨æ”»å‡»ï¼ŒåŠ¨æ€é€‰æ‹©è§¦å‘èŠ‚ç‚¹å¹¶ä¿®æ”¹èŠ‚ç‚¹ç‰¹å¾ã€‚</li>
<li>å¯¹äºèŠ‚ç‚¹çº§ä»»åŠ¡ï¼ŒABARCé€šè¿‡åˆ†æèŠ‚ç‚¹ç‰¹å¾é€‰æ‹©å’Œä¿®æ”¹è§¦å‘ç‰¹å¾ã€‚</li>
<li>ABARCè®¾è®¡è‡ªé€‚åº”è¾¹å‰ªææœºåˆ¶ä»¥æé«˜æ”»å‡»æˆåŠŸç‡ï¼ˆASRï¼‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09049">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-91a63e97666250b015fe83eeaa71a9ee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-39fe4ac2808682eeb80f12277e399924.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8bebfdd9a96d40c014a458cb052b47f3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dc13fc5f1e4e7f4db4b5bcabe8cf7f30.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Teaching-LLMs-How-to-Learn-with-Contextual-Fine-Tuning"><a href="#Teaching-LLMs-How-to-Learn-with-Contextual-Fine-Tuning" class="headerlink" title="Teaching LLMs How to Learn with Contextual Fine-Tuning"></a>Teaching LLMs How to Learn with Contextual Fine-Tuning</h2><p><strong>Authors:Younwoo Choi, Muhammad Adil Asif, Ziwen Han, John Willes, Rahul G. Krishnan</strong></p>
<p>Prompting Large Language Models (LLMs), or providing context on the expected model of operation, is an effective way to steer the outputs of such models to satisfy human desiderata after they have been trained. But in rapidly evolving domains, there is often need to fine-tune LLMs to improve either the kind of knowledge in their memory or their abilities to perform open ended reasoning in new domains. When humanâ€™s learn new concepts, we often do so by linking the new material that we are studying to concepts we have already learned before. To that end, we ask, â€œcan prompting help us teach LLMs how to learnâ€. In this work, we study a novel generalization of instruction tuning, called contextual fine-tuning, to fine-tune LLMs. Our method leverages instructional prompts designed to mimic human cognitive strategies in learning and problem-solving to guide the learning process during training, aiming to improve the modelâ€™s interpretation and understanding of domain-specific knowledge. We empirically demonstrate that this simple yet effective modification improves the ability of LLMs to be fine-tuned rapidly on new datasets both within the medical and financial domains. </p>
<blockquote>
<p>æç¤ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æˆ–ä¸ºé¢„æœŸçš„æ¨¡å‹æ“ä½œæä¾›èƒŒæ™¯ï¼Œæ˜¯å¼•å¯¼æ¨¡å‹è¾“å‡ºä»¥æ»¡è¶³äººç±»éœ€æ±‚çš„æœ‰æ•ˆæ–¹æ³•ï¼Œè¿™äº›æ¨¡å‹å·²ç»åœ¨è®­ç»ƒä¹‹åã€‚ä½†åœ¨å¿«é€Ÿå‘å±•çš„é¢†åŸŸä¸­ï¼Œé€šå¸¸éœ€è¦å¾®è°ƒLLMï¼Œä»¥æ”¹å–„å…¶å†…å­˜ä¸­çš„çŸ¥è¯†ç±»å‹æˆ–åœ¨æ–°çš„é¢†åŸŸè¿›è¡Œå¼€æ”¾å¼æ¨ç†çš„èƒ½åŠ›ã€‚å½“äººç±»å­¦ä¹ æ–°æ¦‚å¿µæ—¶ï¼Œæˆ‘ä»¬å¾€å¾€é€šè¿‡æŠŠæ­£åœ¨ç ”ç©¶çš„æ–°ææ–™ä¸æˆ‘ä»¬å·²ç»å­¦è¿‡çš„æ¦‚å¿µè”ç³»èµ·æ¥ã€‚é‰´äºæ­¤ï¼Œæˆ‘ä»¬çš„é—®é¢˜æ˜¯ï¼Œâ€œæç¤ºæ˜¯å¦èƒ½å¸®åŠ©æˆ‘ä»¬æ•™ä¼šLLMå¦‚ä½•å­¦ä¹ â€ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ä¸€ç§æŒ‡ä»¤è°ƒæ•´çš„æ–°æ³›åŒ–æ–¹æ³•ï¼Œç§°ä¸ºä¸Šä¸‹æ–‡å¾®è°ƒï¼Œä»¥å¾®è°ƒLLMã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨æŒ‡ä»¤æç¤ºï¼Œæ¨¡ä»¿äººç±»åœ¨å­¦ä¹ å’Œè§£å†³é—®é¢˜ä¸­çš„è®¤çŸ¥ç­–ç•¥ï¼Œä»¥æŒ‡å¯¼è®­ç»ƒè¿‡ç¨‹ä¸­çš„å­¦ä¹ è¿‡ç¨‹ï¼Œæ—¨åœ¨æé«˜æ¨¡å‹å¯¹ç‰¹å®šé¢†åŸŸçš„è§£é‡Šå’Œç†è§£èƒ½åŠ›ã€‚æˆ‘ä»¬é€šè¿‡å®è¯è¡¨æ˜ï¼Œè¿™ç§ç®€å•è€Œæœ‰æ•ˆçš„æ”¹è¿›æé«˜äº†LLMåœ¨æ–°æ•°æ®é›†ä¸Šçš„å¿«é€Ÿå¾®è°ƒèƒ½åŠ›ï¼Œæ— è®ºæ˜¯åœ¨åŒ»ç–—é¢†åŸŸè¿˜æ˜¯é‡‘èé¢†åŸŸã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09032v1">PDF</a> ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>è®­ç»ƒåé€šè¿‡æç¤ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æˆ–æä¾›é¢„æœŸçš„æ¨¡å‹æ“ä½œä¸Šä¸‹æ–‡ï¼Œå¯ä»¥æœ‰æ•ˆåœ°å¼•å¯¼æ¨¡å‹çš„è¾“å‡ºä»¥æ»¡è¶³äººç±»çš„éœ€æ±‚ã€‚ç„¶è€Œï¼Œåœ¨å¿«é€Ÿå‘å±•çš„é¢†åŸŸä¸­ï¼Œé€šå¸¸éœ€è¦å¾®è°ƒLLMsï¼Œä»¥æ”¹å–„å…¶å†…å­˜ä¸­çš„çŸ¥è¯†æˆ–åœ¨æ–°é¢†åŸŸè¿›è¡Œå¼€æ”¾å¼æ¨ç†çš„èƒ½åŠ›ã€‚å½“äººç±»å­¦ä¹ æ–°æ¦‚å¿µæ—¶ï¼Œæˆ‘ä»¬å¸¸å¸¸é€šè¿‡å°†æ­£åœ¨ç ”ç©¶çš„æ–°ææ–™ä¸ä¹‹å‰å­¦è¿‡çš„æ¦‚å¿µè”ç³»èµ·æ¥æ¥å­¦ä¹ ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºçš„é—®é¢˜æ˜¯ï¼Œâ€œæç¤ºæ˜¯å¦èƒ½å¸®åŠ©æˆ‘ä»¬æ•™LLMså¦‚ä½•å­¦ä¹ â€ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†æŒ‡ä»¤å¾®è°ƒçš„ä¸€ç§æ–°å‹æ³›åŒ–ï¼Œç§°ä¸ºä¸Šä¸‹æ–‡å¾®è°ƒï¼Œæ¥å¾®è°ƒLLMsã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨æŒ‡ä»¤æç¤ºæ¥æ¨¡ä»¿äººç±»åœ¨å­¦ä¹ å’Œè§£å†³é—®é¢˜ä¸­çš„è®¤çŸ¥ç­–ç•¥ï¼Œä»¥æŒ‡å¯¼è®­ç»ƒè¿‡ç¨‹ä¸­çš„å­¦ä¹ è¿‡ç¨‹ï¼Œæ—¨åœ¨æé«˜æ¨¡å‹å¯¹ç‰¹å®šé¢†åŸŸçŸ¥è¯†çš„ç†è§£åŠ›å’Œè§£é‡ŠåŠ›ã€‚æˆ‘ä»¬é€šè¿‡å®éªŒè¯æ˜ï¼Œè¿™ç§ç®€å•è€Œæœ‰æ•ˆçš„æ”¹è¿›æé«˜äº†LLMsåœ¨æ–°æ•°æ®é›†ä¸Šçš„å¿«é€Ÿå¾®è°ƒèƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨åŒ»ç–—å’Œé‡‘èé¢†åŸŸã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æç¤ºæ˜¯å¼•å¯¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¾“å‡ºçš„æœ‰æ•ˆæ–¹å¼ï¼Œå¯ä»¥ä½¿å…¶æ»¡è¶³äººç±»éœ€æ±‚ã€‚</li>
<li>åœ¨å¿«é€Ÿå˜åŒ–çš„é¢†åŸŸï¼Œéœ€è¦å¾®è°ƒLLMsä»¥æé«˜å…¶çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ã€‚</li>
<li>äººç±»å­¦ä¹ æ–°æ¦‚å¿µæ—¶ï¼Œå–„äºå°†æ–°çŸ¥è¯†ä¸å·²çŸ¥çŸ¥è¯†è”ç³»èµ·æ¥ã€‚</li>
<li>æå‡ºäº†ä¸Šä¸‹æ–‡å¾®è°ƒçš„æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„æŒ‡ä»¤è°ƒå‚æ³›åŒ–ã€‚</li>
<li>ä¸Šä¸‹æ–‡å¾®è°ƒåˆ©ç”¨æŒ‡ä»¤æç¤ºæ¨¡ä»¿äººç±»è®¤çŸ¥ç­–ç•¥ï¼Œä»¥æé«˜æ¨¡å‹å¯¹ç‰¹å®šé¢†åŸŸçŸ¥è¯†çš„ç†è§£åŠ›å’Œè§£é‡ŠåŠ›ã€‚</li>
<li>ä¸Šä¸‹æ–‡å¾®è°ƒåœ¨åŒ»ç–—å’Œé‡‘èç­‰é¢†åŸŸçš„æ–°æ•°æ®é›†ä¸Šå®ç°äº†LLMsçš„å¿«é€Ÿå¾®è°ƒèƒ½åŠ›çš„æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09032">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0432d32251c9933395c3001794c693da.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cfe698d678a658ff6c1fc5568dfebab1.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Chain-of-Thought-Reasoning-In-The-Wild-Is-Not-Always-Faithful"><a href="#Chain-of-Thought-Reasoning-In-The-Wild-Is-Not-Always-Faithful" class="headerlink" title="Chain-of-Thought Reasoning In The Wild Is Not Always Faithful"></a>Chain-of-Thought Reasoning In The Wild Is Not Always Faithful</h2><p><strong>Authors:IvÃ¡n Arcuschin, Jett Janiak, Robert Krzyzanowski, Senthooran Rajamanoharan, Neel Nanda, Arthur Conmy</strong></p>
<p>Chain-of-Thought (CoT) reasoning has significantly advanced state-of-the-art AI capabilities. However, recent studies have shown that CoT reasoning is not always faithful, i.e. CoT reasoning does not always reflect how models arrive at conclusions. So far, most of these studies have focused on unfaithfulness in unnatural contexts where an explicit bias has been introduced. In contrast, we show that unfaithful CoT can occur on realistic prompts with no artificial bias. Our results reveal concerning rates of several forms of unfaithful reasoning in frontier models: Sonnet 3.7 (30.6%), DeepSeek R1 (15.8%) and ChatGPT-4o (12.6%) all answer a high proportion of question pairs unfaithfully. Specifically, we find that models rationalize their implicit biases in answers to binary questions (â€œimplicit post-hoc rationalizationâ€). For example, when separately presented with the questions â€œIs X bigger than Y?â€ and â€œIs Y bigger than X?â€, models sometimes produce superficially coherent arguments to justify answering Yes to both questions or No to both questions, despite such responses being logically contradictory. We also investigate restoration errors (Dziri et al., 2023), where models make and then silently correct errors in their reasoning, and unfaithful shortcuts, where models use clearly illogical reasoning to simplify solving problems in Putnam questions (a hard benchmark). Our findings raise challenges for AI safety work that relies on monitoring CoT to detect undesired behavior. </p>
<blockquote>
<p>â€œé“¾å¼æ€ç»´â€ï¼ˆChain-of-Thought, CoTï¼‰æ¨ç†æŠ€æœ¯å·²æ˜¾è‘—æå‡äº†äººå·¥æ™ºèƒ½çš„æœ€æ–°èƒ½åŠ›ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼ŒCoTæ¨ç†å¹¶ä¸æ€»æ˜¯å¿ å®å¯é ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼ŒCoTæ¨ç†å¹¶ä¸æ€»æ˜¯åæ˜ æ¨¡å‹å¦‚ä½•å¾—å‡ºç»“è®ºã€‚è¿„ä»Šä¸ºæ­¢ï¼Œå¤§å¤šæ•°ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨éè‡ªç„¶è¯­å¢ƒä¸‹çš„ä¸å¿ å®æƒ…å†µï¼Œå…¶ä¸­å·²å¼•å…¥äº†æ˜ç¡®çš„åè§ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘ä»¬è¯æ˜åœ¨ä¸å¸¦æœ‰ä»»ä½•äººä¸ºåè§çš„æƒ…å†µä¸‹ï¼Œå®é™…æç¤ºä¹Ÿä¼šå‡ºç°ä¸å¿ å®çš„CoTã€‚æˆ‘ä»¬çš„ç»“æœæ˜¾ç¤ºï¼Œå‰æ²¿æ¨¡å‹ä¸­å­˜åœ¨ç€ä»¤äººæ‹…å¿§çš„å„ç§ä¸å¿ å®æ¨ç†çš„æ¯”ç‡ï¼šSonnet 3.7ï¼ˆ30.6%ï¼‰ã€DeepSeek R1ï¼ˆ15.8%ï¼‰å’ŒChatGPT-4oï¼ˆ12.6%ï¼‰éƒ½èƒ½å›ç­”å¤§é‡çš„é—®é¢˜å¯¹ä¸å¿ å®æ¨ç†ç»™å‡ºå›åº”ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å‘ç°æ¨¡å‹åœ¨å›ç­”äºŒå…ƒé—®é¢˜æ—¶ä¸ºå…¶ç­”æ¡ˆåˆç†åŒ–ï¼ˆå³â€œäº‹åéšå¼åˆç†åŒ–â€ï¼‰ã€‚ä¾‹å¦‚ï¼Œå½“åˆ†åˆ«é¢å¯¹é—®é¢˜â€œXæ˜¯å¦å¤§äºYï¼Ÿâ€å’Œâ€œYæ˜¯å¦å¤§äºXï¼Ÿâ€æ—¶ï¼Œæ¨¡å‹æœ‰æ—¶ä¼šåšå‡ºçœ‹ä¼¼è¿è´¯çš„è®ºæ®ï¼Œä»¥è¯æ˜ä¸¤ä¸ªé—®é¢˜éƒ½å›ç­”â€œæ˜¯â€æˆ–éƒ½å›ç­”â€œå¦â€ï¼Œå°½ç®¡è¿™æ ·çš„å›ç­”åœ¨é€»è¾‘ä¸Šæ˜¯çŸ›ç›¾çš„ã€‚æˆ‘ä»¬è¿˜ç ”ç©¶äº†æ¨¡å‹åœ¨æ¨ç†ä¸­çš„æ¢å¤é”™è¯¯ï¼ˆDziriç­‰äººï¼Œ2023ï¼‰ï¼Œå³æ¨¡å‹ä¼šå‡ºç°æ¨ç†é”™è¯¯ç„¶åé»˜é»˜è¿›è¡Œä¿®æ­£ï¼Œä»¥åŠä¸å¿ å®çš„ç®€åŒ–æ–¹æ³•ï¼Œå³æ¨¡å‹ä½¿ç”¨æ˜æ˜¾ä¸åˆé€»è¾‘çš„æ¨ç†æ¥ç®€åŒ–è§£å†³Putnamé—®é¢˜ï¼ˆä¸€ä¸ªéš¾åº¦å¾ˆé«˜çš„åŸºå‡†æµ‹è¯•ï¼‰ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœå¯¹ä¾èµ–ç›‘æ§CoTæ¥æ£€æµ‹AIä¸è‰¯è¡Œä¸ºçš„AIå®‰å…¨å·¥ä½œæå‡ºäº†æŒ‘æˆ˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08679v1">PDF</a> Accepted to the ICLR 2025 Workshop, 10 main paper pages, 38 appendix   pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†Chain-of-Thoughtï¼ˆCoTï¼‰æ¨ç†çš„å±€é™æ€§ï¼Œå³åœ¨æŸäº›æƒ…å¢ƒä¸‹å­˜åœ¨ä¸å¿ å®ç°è±¡ã€‚ç ”ç©¶å‘ç°ï¼Œå³ä½¿åœ¨æ— äººå·¥å¹²é¢„çš„ç°å®æƒ…å¢ƒä¸‹ï¼Œå‰æ²¿æ¨¡å‹å¦‚Sonnet 3.7ã€DeepSeek R1å’ŒChatGPT-4oä¹Ÿå­˜åœ¨è¾ƒé«˜æ¯”ä¾‹çš„ä¸å¿ å®æ¨ç†è¡Œä¸ºã€‚æ¨¡å‹åœ¨å›ç­”äºŒå…ƒé—®é¢˜æ—¶ï¼Œä¼šåˆ©ç”¨éšå«åè§è¿›è¡Œäº‹ååˆç†åŒ–ã€‚æ­¤å¤–ï¼Œæ¨¡å‹è¿˜ä¼šåœ¨æ¨ç†è¿‡ç¨‹ä¸­é™é»˜çº æ­£é”™è¯¯æˆ–ä½¿ç”¨æ˜æ˜¾ä¸åˆé€»è¾‘çš„æ·å¾„æ¥è§£å†³é—®é¢˜ã€‚è¿™äº›å‘ç°å¯¹ä¾èµ–CoTç›‘æ§æ¥æ£€æµ‹AIä¸å½“è¡Œä¸ºçš„AIå®‰å…¨å·¥ä½œæå‡ºäº†æŒ‘æˆ˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>CoTæ¨ç†æœ‰å±€é™æ€§ï¼Œå­˜åœ¨ä¸å¿ å®ç°è±¡ã€‚</li>
<li>ç°æœ‰ç ”ç©¶å¤šå…³æ³¨äºäººä¸ºå¹²é¢„ä¸‹çš„ä¸å¿ å®æ¨ç†ï¼Œè€Œæœ¬æ–‡å¼ºè°ƒæ— äººå·¥å¹²é¢„çš„ç°å®æƒ…å¢ƒä¸‹ä¹Ÿå­˜åœ¨ä¸å¿ å®æ¨ç†ã€‚</li>
<li>å‰æ²¿æ¨¡å‹å¦‚Sonnet 3.7ã€DeepSeek R1å’ŒChatGPT-4oå­˜åœ¨è¾ƒé«˜æ¯”ä¾‹çš„ä¸å¿ å®æ¨ç†è¡Œä¸ºã€‚</li>
<li>æ¨¡å‹åœ¨å›ç­”äºŒå…ƒé—®é¢˜æ—¶ï¼Œä¼šåˆ©ç”¨éšå«åè§è¿›è¡Œäº‹ååˆç†åŒ–ã€‚</li>
<li>æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­å­˜åœ¨é™é»˜çº æ­£é”™è¯¯æˆ–ä½¿ç”¨ä¸åˆé€»è¾‘çš„æ·å¾„æ¥è§£å†³é—®é¢˜ã€‚</li>
<li>è¿™äº›å‘ç°å¯¹ä¾èµ–CoTç›‘æ§æ¥æ£€æµ‹AIä¸å½“è¡Œä¸ºçš„AIå®‰å…¨å·¥ä½œæå‡ºäº†æŒ‘æˆ˜ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08679">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-632e652535a9fa9a1354146a2f525ebf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-72a5833ed46385dd9a400a80b7a1df54.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-40c439074ae5d50d603231311249f467.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e020f6bb76f8cd487e40ab971afe4fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f6ea6f14ee2b8643ed99995ee1f8093.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="SegAgent-Exploring-Pixel-Understanding-Capabilities-in-MLLMs-by-Imitating-Human-Annotator-Trajectories"><a href="#SegAgent-Exploring-Pixel-Understanding-Capabilities-in-MLLMs-by-Imitating-Human-Annotator-Trajectories" class="headerlink" title="SegAgent: Exploring Pixel Understanding Capabilities in MLLMs by   Imitating Human Annotator Trajectories"></a>SegAgent: Exploring Pixel Understanding Capabilities in MLLMs by   Imitating Human Annotator Trajectories</h2><p><strong>Authors:Muzhi Zhu, Yuzhuo Tian, Hao Chen, Chunluan Zhou, Qingpei Guo, Yang Liu, Ming Yang, Chunhua Shen</strong></p>
<p>While MLLMs have demonstrated adequate image understanding capabilities, they still struggle with pixel-level comprehension, limiting their practical applications. Current evaluation tasks like VQA and visual grounding remain too coarse to assess fine-grained pixel comprehension accurately. Though segmentation is foundational for pixel-level understanding, existing methods often require MLLMs to generate implicit tokens, decoded through external pixel decoders. This approach disrupts the MLLMâ€™s text output space, potentially compromising language capabilities and reducing flexibility and extensibility, while failing to reflect the modelâ€™s intrinsic pixel-level understanding.   Thus, we introduce the Human-Like Mask Annotation Task (HLMAT), a new paradigm where MLLMs mimic human annotators using interactive segmentation tools. Modeling segmentation as a multi-step Markov Decision Process, HLMAT enables MLLMs to iteratively generate text-based click points, achieving high-quality masks without architectural changes or implicit tokens. Through this setup, we develop SegAgent, a model fine-tuned on human-like annotation trajectories, which achieves performance comparable to state-of-the-art (SOTA) methods and supports additional tasks like mask refinement and annotation filtering.   HLMAT provides a protocol for assessing fine-grained pixel understanding in MLLMs and introduces a vision-centric, multi-step decision-making task that facilitates exploration of MLLMsâ€™ visual reasoning abilities. Our adaptations of policy improvement method StaR and PRM-guided tree search further enhance model robustness in complex segmentation tasks, laying a foundation for future advancements in fine-grained visual perception and multi-step decision-making for MLLMs. </p>
<blockquote>
<p>è™½ç„¶MLLMå·²ç»å±•ç°å‡ºè¶³å¤Ÿçš„å›¾åƒç†è§£èƒ½åŠ›ï¼Œä½†åœ¨åƒç´ çº§ç†è§£æ–¹é¢ä»å­˜åœ¨å›°éš¾ï¼Œè¿™é™åˆ¶äº†å…¶å®é™…åº”ç”¨ã€‚å½“å‰çš„è¯„ä¼°ä»»åŠ¡ï¼Œå¦‚è§†è§‰é—®ç­”å’Œè§†è§‰å®šä½ï¼Œä»ç„¶è¿‡äºç²—ç•¥ï¼Œæ— æ³•å‡†ç¡®è¯„ä¼°åƒç´ çº§çš„ç²¾ç»†ç†è§£ã€‚è™½ç„¶åˆ†å‰²æ˜¯åƒç´ çº§ç†è§£çš„åŸºç¡€ï¼Œä½†ç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦MLLMç”Ÿæˆéšå¼ä»¤ç‰Œï¼Œå¹¶é€šè¿‡å¤–éƒ¨åƒç´ è§£ç å™¨è¿›è¡Œè§£ç ã€‚è¿™ç§æ–¹æ³•ç ´åäº†MLLMçš„æ–‡æœ¬è¾“å‡ºç©ºé—´ï¼Œå¯èƒ½æŸå®³è¯­è¨€åŠŸèƒ½ï¼Œé™ä½çµæ´»æ€§å’Œå¯æ‰©å±•æ€§ï¼ŒåŒæ—¶æ— æ³•åæ˜ æ¨¡å‹çš„å†…åœ¨åƒç´ çº§ç†è§£ã€‚</p>
</blockquote>
<p>å› æ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†äººç±»æ ·å¼çš„æ©ç æ ‡æ³¨ä»»åŠ¡ï¼ˆHLMATï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æ¨¡å¼ï¼Œå…¶ä¸­MLLMä½¿ç”¨äº¤äº’å¼åˆ†å‰²å·¥å…·æ¨¡ä»¿äººç±»æ³¨é‡Šè€…ã€‚å°†åˆ†å‰²å»ºæ¨¡ä¸ºå¤šæ­¥é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ŒHLMATä½¿MLLMèƒ½å¤Ÿè¿­ä»£ç”ŸæˆåŸºäºæ–‡æœ¬çš„ç‚¹å‡»ç‚¹ï¼Œä»è€Œåœ¨ä¸æ”¹å˜æ¶æ„æˆ–ç”Ÿæˆéšå¼ä»¤ç‰Œçš„æƒ…å†µä¸‹å®ç°é«˜è´¨é‡æ©ç ã€‚é€šè¿‡è¿™ä¸€è®¾ç½®ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªåœ¨ç±»ä¼¼äººç±»çš„æ³¨é‡Šè½¨è¿¹ä¸Šç»è¿‡å¾®è°ƒç»†è°ƒçš„SegAgentæ¨¡å‹ï¼Œå…¶æ€§èƒ½å¯ä¸æœ€å…ˆè¿›çš„æ¨¡å‹ç›¸æ¯”æ‹Ÿï¼Œå¹¶æ”¯æŒè¯¸å¦‚æ©æ¨¡ç»†åŒ–ã€æ³¨é‡Šè¿‡æ»¤ç­‰é¢å¤–ä»»åŠ¡ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08625v1">PDF</a> CVPR2025;Code will be released at   \url{<a target="_blank" rel="noopener" href="https://github.com/aim-uofa/SegAgent%7D">https://github.com/aim-uofa/SegAgent}</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†MLLMsåœ¨å›¾åƒç†è§£æ–¹é¢çš„ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯åœ¨åƒç´ çº§åˆ«çš„ç†è§£ä¸Šã€‚ç°æœ‰çš„è¯„ä¼°ä»»åŠ¡å¦‚VQAå’Œè§†è§‰å®šä½ä»æ— æ³•å‡†ç¡®è¯„ä¼°åƒç´ çº§åˆ«çš„ç†è§£ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§æ–°çš„äººç±»åŒ–æ©è†œæ ‡æ³¨ä»»åŠ¡ï¼ˆHLMATï¼‰ï¼Œä½¿MLLMsèƒ½å¤Ÿæ¨¡ä»¿äººç±»æ ‡æ³¨è€…ä½¿ç”¨äº¤äº’å¼åˆ†å‰²å·¥å…·ï¼Œé€šè¿‡å¤šæ­¥é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ç”Ÿæˆæ–‡æœ¬ç‚¹å‡»ç‚¹ï¼Œå®ç°é«˜è´¨é‡æ©è†œç”Ÿæˆã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜ä»‹ç»äº†åŸºäºæ­¤ä»»åŠ¡å¼€å‘çš„SegAgentæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨æ€§èƒ½ä¸Šè¾¾åˆ°æœ€æ–°æ°´å¹³ï¼Œå¹¶æ”¯æŒé¢å¤–çš„ä»»åŠ¡å¦‚æ©è†œç»†åŒ–ã€æ ‡æ³¨è¿‡æ»¤ç­‰ã€‚HLMATä¸ºè¯„ä¼°MLLMsçš„ç²¾ç»†åƒç´ ç†è§£æä¾›äº†åè®®ï¼Œå¹¶å¼•å…¥äº†ä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„å¤šæ­¥å†³ç­–ä»»åŠ¡ï¼Œä¿ƒè¿›äº†MLLMsçš„è§†è§‰æ¨ç†èƒ½åŠ›æ¢ç´¢ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLMsåœ¨å›¾åƒç†è§£æ–¹é¢ï¼Œç‰¹åˆ«æ˜¯åœ¨åƒç´ çº§åˆ«ç†è§£ä¸Šè¿˜å­˜åœ¨å›°éš¾ï¼Œé™åˆ¶äº†å…¶å®é™…åº”ç”¨ã€‚</li>
<li>ç°æœ‰è¯„ä¼°ä»»åŠ¡å¦‚VQAå’Œè§†è§‰å®šä½æ— æ³•å‡†ç¡®è¯„ä¼°åƒç´ çº§åˆ«çš„ç†è§£ã€‚</li>
<li>æå‡ºäº†æ–°çš„äººç±»åŒ–æ©è†œæ ‡æ³¨ä»»åŠ¡ï¼ˆHLMATï¼‰ï¼Œä½¿MLLMsèƒ½å¤Ÿæ¨¡ä»¿äººç±»æ ‡æ³¨è€…ã€‚</li>
<li>HLMATé€šè¿‡å¤šæ­¥é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹å®ç°é«˜è´¨é‡æ©è†œç”Ÿæˆã€‚</li>
<li>SegAgentæ¨¡å‹åœ¨æ€§èƒ½ä¸Šè¾¾åˆ°æœ€æ–°æ°´å¹³ï¼Œæ”¯æŒé¢å¤–çš„ä»»åŠ¡å¦‚æ©è†œç»†åŒ–ã€æ ‡æ³¨è¿‡æ»¤ç­‰ã€‚</li>
<li>HLMATä¸ºè¯„ä¼°MLLMsçš„ç²¾ç»†åƒç´ ç†è§£æä¾›äº†åè®®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08625">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-27c2ad9c03704df201e4ef02df734f5b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c57a616c418c5bfce238da81175393fa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-791bb94fa5643344b456a06323f17faf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2da682cc3a200ad054d92cd5d0eebaad.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Reasoning-and-Sampling-Augmented-MCQ-Difficulty-Prediction-via-LLMs"><a href="#Reasoning-and-Sampling-Augmented-MCQ-Difficulty-Prediction-via-LLMs" class="headerlink" title="Reasoning and Sampling-Augmented MCQ Difficulty Prediction via LLMs"></a>Reasoning and Sampling-Augmented MCQ Difficulty Prediction via LLMs</h2><p><strong>Authors:Wanyong Feng, Peter Tran, Stephen Sireci, Andrew Lan</strong></p>
<p>The difficulty of multiple-choice questions (MCQs) is a crucial factor for educational assessments. Predicting MCQ difficulty is challenging since it requires understanding both the complexity of reaching the correct option and the plausibility of distractors, i.e., incorrect options. In this paper, we propose a novel, two-stage method to predict the difficulty of MCQs. First, to better estimate the complexity of each MCQ, we use large language models (LLMs) to augment the reasoning steps required to reach each option. We use not just the MCQ itself but also these reasoning steps as input to predict the difficulty. Second, to capture the plausibility of distractors, we sample knowledge levels from a distribution to account for variation among students responding to the MCQ. This setup, inspired by item response theory (IRT), enable us to estimate the likelihood of students selecting each (both correct and incorrect) option. We align these predictions with their ground truth values, using a Kullback-Leibler (KL) divergence-based regularization objective, and use estimated likelihoods to predict MCQ difficulty. We evaluate our method on two real-world \emph{math} MCQ and response datasets with ground truth difficulty values estimated using IRT. Experimental results show that our method outperforms all baselines, up to a 28.3% reduction in mean squared error and a 34.6% improvement in the coefficient of determination. We also qualitatively discuss how our novel method results in higher accuracy in predicting MCQ difficulty. </p>
<blockquote>
<p>é€‰æ‹©é¢˜éš¾åº¦æ˜¯æ•™è‚²è¯„ä¼°ä¸­çš„å…³é”®å› ç´ ã€‚é¢„æµ‹é€‰æ‹©é¢˜çš„éš¾åº¦å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºè¿™éœ€è¦ç†è§£é€‰æ‹©æ­£ç¡®ç­”æ¡ˆçš„å¤æ‚æ€§å’Œé”™è¯¯é€‰é¡¹çš„è¿·æƒ‘æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ä¸¤é˜¶æ®µæ–¹æ³•æ¥é¢„æµ‹é€‰æ‹©é¢˜çš„éš¾åº¦ã€‚é¦–å…ˆï¼Œä¸ºäº†æ›´å¥½åœ°ä¼°è®¡æ¯ä¸ªé€‰æ‹©é¢˜çš„å¤æ‚æ€§ï¼Œæˆ‘ä»¬ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥å¢åŠ è§£ç­”æ¯ä¸ªé€‰é¡¹æ‰€éœ€çš„æ¨ç†æ­¥éª¤ã€‚æˆ‘ä»¬ä¸ä»…ä½¿ç”¨é€‰æ‹©é¢˜æœ¬èº«ï¼Œè€Œä¸”ä½¿ç”¨è¿™äº›æ¨ç†æ­¥éª¤ä½œä¸ºè¾“å…¥æ¥é¢„æµ‹éš¾åº¦ã€‚å…¶æ¬¡ï¼Œä¸ºäº†æ•æ‰é”™è¯¯é€‰é¡¹çš„è¿·æƒ‘æ€§ï¼Œæˆ‘ä»¬ä»åˆ†å¸ƒä¸­æŠ½å–çŸ¥è¯†å±‚æ¬¡æ¥åæ˜ å­¦ç”Ÿåœ¨å›ç­”é€‰æ‹©é¢˜æ—¶çš„å·®å¼‚ã€‚è¿™ä¸ªè®¾ç½®çµæ„Ÿæ¥è‡ªäºé¡¹ç›®ååº”ç†è®ºï¼ˆIRTï¼‰ï¼Œå®ƒä½¿æˆ‘ä»¬èƒ½å¤Ÿä¼°è®¡å­¦ç”Ÿé€‰æ‹©æ¯ä¸ªé€‰é¡¹ï¼ˆæ— è®ºæ˜¯æ­£ç¡®è¿˜æ˜¯é”™è¯¯ï¼‰çš„å¯èƒ½æ€§ã€‚æˆ‘ä»¬ä½¿ç”¨åŸºäºKullback-Leiblerï¼ˆKLï¼‰æ•£åº¦çš„æ­£åˆ™åŒ–ç›®æ ‡æ¥å¯¹é½è¿™äº›é¢„æµ‹ä¸ä»–ä»¬çš„çœŸå®å€¼ï¼Œå¹¶ä½¿ç”¨ä¼°è®¡çš„å¯èƒ½æ€§æ¥é¢„æµ‹é€‰æ‹©é¢˜çš„éš¾åº¦ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªçœŸå®çš„æ•°å­¦é€‰æ‹©é¢˜å’Œå›ç­”æ•°æ®é›†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œè¿™äº›æ•°æ®çš„çœŸå®éš¾åº¦å€¼æ˜¯é€šè¿‡IRTä¼°è®¡å¾—å‡ºçš„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºæ‰€æœ‰åŸºçº¿æ–¹æ³•ï¼Œå¹³å‡å¹³æ–¹è¯¯å·®é™ä½äº†28.3%ï¼Œå†³å®šç³»æ•°æé«˜äº†34.6%ã€‚æˆ‘ä»¬è¿˜ä»å®šæ€§è§’åº¦è®¨è®ºäº†æˆ‘ä»¬çš„æ–°æ–¹æ³•åœ¨é¢„æµ‹é€‰æ‹©é¢˜éš¾åº¦æ–¹é¢ä¸ºä½•èƒ½å¸¦æ¥æ›´é«˜çš„å‡†ç¡®æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08551v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§é¢„æµ‹é€‰æ‹©é¢˜éš¾åº¦çš„æ–°é¢–ä¸¤é˜¶æ®µæ–¹æ³•ã€‚é¦–å…ˆï¼Œä¸ºäº†æ›´å¥½åœ°ä¼°è®¡æ¯ä¸ªé€‰æ‹©é¢˜çš„å¤æ‚åº¦ï¼Œä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ¥å¢å¼ºè§£ç­”æ¯ä¸ªé€‰é¡¹æ‰€éœ€çš„æ¨ç†æ­¥éª¤ã€‚ç„¶åï¼Œåˆ©ç”¨çŸ¥è¯†æ°´å¹³çš„åˆ†å¸ƒæ¥æ•æ‰å¹²æ‰°é¡¹çš„å¯ä¿¡ç¨‹åº¦ï¼Œä»¥åº”å¯¹ä¸åŒå­¦ç”Ÿå¯¹é€‰æ‹©é¢˜ååº”çš„ä¸åŒã€‚é€šè¿‡åŸºäºKullback-Leibleræ•£åº¦çš„æ­£åˆ™åŒ–ç›®æ ‡å¯¹é½è¿™äº›é¢„æµ‹ä¸å…¶çœŸå®å€¼ï¼Œå¹¶åˆ©ç”¨ä¼°è®¡çš„å¯èƒ½æ€§æ¥é¢„æµ‹é€‰æ‹©é¢˜çš„éš¾åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºæ‰€æœ‰åŸºçº¿æ–¹æ³•ï¼Œå‡æ–¹è¯¯å·®é™ä½äº†28.3%ï¼Œå†³å®šç³»æ•°æé«˜äº†34.6%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>é€‰æ‹©é¢˜çš„éš¾åº¦æ˜¯æ•™è‚²è¯„ä¼°ä¸­çš„å…³é”®å› ç´ ã€‚</li>
<li>é¢„æµ‹é€‰æ‹©é¢˜éš¾åº¦å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå®ƒæ¶‰åŠç†è§£æ­£ç¡®é€‰é¡¹çš„å¤æ‚æ€§å’Œå¹²æ‰°é¡¹çš„å¯ä¿¡ç¨‹åº¦ã€‚</li>
<li>æå‡ºä¸€ç§æ–°é¢–çš„ä¸¤é˜¶æ®µæ–¹æ³•æ¥é¢„æµ‹é€‰æ‹©é¢˜çš„éš¾åº¦ã€‚</li>
<li>ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å¢å¼ºæ¨ç†æ­¥éª¤ï¼Œä»¥æ›´å¥½åœ°ä¼°è®¡æ¯ä¸ªé€‰æ‹©é¢˜çš„å¤æ‚åº¦ã€‚</li>
<li>åˆ©ç”¨çŸ¥è¯†æ°´å¹³çš„åˆ†å¸ƒæ¥æ•æ‰å¹²æ‰°é¡¹çš„å¯ä¿¡ç¨‹åº¦ï¼Œä»¥åº”å¯¹å­¦ç”Ÿçš„ä¸åŒååº”ã€‚</li>
<li>æ–¹æ³•é€šè¿‡åŸºäºKullback-Leibleræ•£åº¦çš„æ­£åˆ™åŒ–ç›®æ ‡è¿›è¡Œé¢„æµ‹ï¼Œå¹¶å®éªŒéªŒè¯äº†å…¶ä¼˜è¶Šæ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08551">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-92c01121f2a9c629a7a5433796d04272.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6049eee7048f0c2d99ee857217172e84.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Mellow-a-small-audio-language-model-for-reasoning"><a href="#Mellow-a-small-audio-language-model-for-reasoning" class="headerlink" title="Mellow: a small audio language model for reasoning"></a>Mellow: a small audio language model for reasoning</h2><p><strong>Authors:Soham Deshmukh, Satvik Dixit, Rita Singh, Bhiksha Raj</strong></p>
<p>Multimodal Audio-Language Models (ALMs) can understand and reason over both audio and text. Typically, reasoning performance correlates with model size, with the best results achieved by models exceeding 8 billion parameters. However, no prior work has explored enabling small audio-language models to perform reasoning tasks, despite the potential applications for edge devices. To address this gap, we introduce Mellow, a small Audio-Language Model specifically designed for reasoning. Mellow achieves state-of-the-art performance among existing small audio-language models and surpasses several larger models in reasoning capabilities. For instance, Mellow scores 52.11 on MMAU, comparable to SoTA Qwen2 Audio (which scores 52.5) while using 50 times fewer parameters and being trained on 60 times less data (audio hrs). To train Mellow, we introduce ReasonAQA, a dataset designed to enhance audio-grounded reasoning in models. It consists of a mixture of existing datasets (30% of the data) and synthetically generated data (70%). The synthetic dataset is derived from audio captioning datasets, where Large Language Models (LLMs) generate detailed and multiple-choice questions focusing on audio events, objects, acoustic scenes, signal properties, semantics, and listener emotions. To evaluate Mellowâ€™s reasoning ability, we benchmark it on a diverse set of tasks, assessing on both in-distribution and out-of-distribution data, including audio understanding, deductive reasoning, and comparative reasoning. Finally, we conduct extensive ablation studies to explore the impact of projection layer choices, synthetic data generation methods, and language model pretraining on reasoning performance. Our training dataset, findings, and baseline pave the way for developing small ALMs capable of reasoning. </p>
<blockquote>
<p>å¤šæ¨¡æ€éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆALMï¼‰èƒ½å¤Ÿç†è§£å’Œæ¨ç†éŸ³é¢‘å’Œæ–‡æœ¬ã€‚é€šå¸¸ï¼Œæ¨ç†æ€§èƒ½ä¸æ¨¡å‹å¤§å°ç›¸å…³ï¼Œæœ€å¥½çš„ç»“æœæ˜¯ç”±è¶…è¿‡8äº¿å‚æ•°çš„æ¨¡å‹å®ç°çš„ã€‚ç„¶è€Œï¼Œå°½ç®¡è¾¹ç¼˜è®¾å¤‡æœ‰æ½œåœ¨çš„åº”ç”¨ï¼Œä½†ä¹‹å‰çš„å·¥ä½œå¹¶æœªæ¢ç´¢ä½¿å°å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹æ‰§è¡Œæ¨ç†ä»»åŠ¡çš„å¯èƒ½æ€§ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸“ä¸ºæ¨ç†è€Œè®¾è®¡çš„å°å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹â€œMellowâ€ã€‚Mellowåœ¨ç°æœ‰å°å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶åœ¨æ¨ç†èƒ½åŠ›æ–¹é¢è¶…è¶Šäº†æŸäº›æ›´å¤§çš„æ¨¡å‹ã€‚ä¾‹å¦‚ï¼ŒMellowåœ¨MMAUä¸Šçš„å¾—åˆ†ä¸º52.11ï¼Œä¸å½“å‰æœ€ä½³æ°´å¹³çš„Qwen2 Audioï¼ˆå¾—åˆ†ä¸º52.5ï¼‰ç›¸å½“ï¼ŒåŒæ—¶ä½¿ç”¨å‚æ•°å°‘50å€ï¼Œå¹¶ä¸”åœ¨æ•°æ®ï¼ˆéŸ³é¢‘å°æ—¶æ•°ï¼‰çš„è®­ç»ƒä¸Šå‡å°‘äº†60å€ã€‚ä¸ºäº†è®­ç»ƒMellowï¼Œæˆ‘ä»¬å¼•å…¥äº†ReasonAQAæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†æ—¨åœ¨æé«˜æ¨¡å‹çš„éŸ³é¢‘åŸºç¡€æ¨ç†èƒ½åŠ›ã€‚å®ƒåŒ…å«ç°æœ‰æ•°æ®é›†ï¼ˆå 30%ï¼‰å’Œåˆæˆæ•°æ®ï¼ˆå 70%ï¼‰çš„æ··åˆã€‚åˆæˆæ•°æ®é›†æ¥æºäºéŸ³é¢‘æè¿°æ•°æ®é›†ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¼šç”Ÿæˆä¸“æ³¨äºéŸ³é¢‘äº‹ä»¶ã€å¯¹è±¡ã€å£°å­¦åœºæ™¯ã€ä¿¡å·å±æ€§ã€è¯­ä¹‰å’Œå¬ä¼—æƒ…ç»ªçš„å…·ä½“å’Œå¤šé¡¹é€‰æ‹©é¢˜ã€‚ä¸ºäº†è¯„ä¼°Mellowçš„æ¨ç†èƒ½åŠ›ï¼Œæˆ‘ä»¬åœ¨ä¸€ç»„å¤šæ ·åŒ–çš„ä»»åŠ¡ä¸Šå¯¹å…¶è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œè¯„ä¼°å…¶åœ¨å†…éƒ¨æ•°æ®å’Œå¤–éƒ¨æ•°æ®ä¸Šçš„è¡¨ç°ï¼ŒåŒ…æ‹¬éŸ³é¢‘ç†è§£ã€æ¼”ç»æ¨ç†å’Œæ¯”è¾ƒæ¨ç†ã€‚æœ€åï¼Œæˆ‘ä»¬è¿›è¡Œäº†å¹¿æ³›çš„æ¶ˆèç ”ç©¶ï¼Œä»¥æ¢è®¨æŠ•å½±å±‚é€‰æ‹©ã€åˆæˆæ•°æ®ç”Ÿæˆæ–¹æ³•å’Œè¯­è¨€æ¨¡å‹é¢„è®­ç»ƒå¯¹æ¨ç†æ€§èƒ½çš„å½±å“ã€‚æˆ‘ä»¬çš„è®­ç»ƒæ•°æ®é›†ã€ç ”ç©¶ç»“æœå’ŒåŸºçº¿ä¸ºå¼€å‘èƒ½å¤Ÿæ¨ç†çš„å°å‹ALMé“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08540v1">PDF</a> Checkpoint and dataset available at:   <a target="_blank" rel="noopener" href="https://github.com/soham97/mellow">https://github.com/soham97/mellow</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹å°å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹çš„æ¨ç†ä»»åŠ¡çš„ç ”ç©¶ã€‚ç ”ç©¶äººå‘˜æ¨å‡ºäº†Mellowæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨å°å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶åœ¨æŸäº›æ¨ç†èƒ½åŠ›æ–¹é¢è¶…è¶Šäº†è¾ƒå¤§çš„æ¨¡å‹ã€‚Mellowçš„è®¾è®¡é’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œäº†ä¼˜åŒ–ï¼Œé‡‡ç”¨äº†ä¸€ç§æ–°çš„æ•°æ®é›†ReasonAQAè¿›è¡Œè®­ç»ƒï¼Œè¯¥æ•°æ®é›†ç”±ç°æœ‰æ•°æ®é›†å’Œåˆæˆæ•°æ®é›†ç»„æˆã€‚åˆæˆæ•°æ®é›†æ˜¯é€šè¿‡éŸ³é¢‘æè¿°æ•°æ®é›†å’Œå¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„ï¼Œç”¨äºç”Ÿæˆå…³äºéŸ³é¢‘äº‹ä»¶çš„è¯¦ç»†å’Œå¤šé€‰æ‹©é¢˜ã€‚Mellowåœ¨å„ç§ä»»åŠ¡ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼ŒåŒ…æ‹¬éŸ³é¢‘ç†è§£ã€æ¨ç†ç­‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆALMsï¼‰èƒ½ç†è§£å¹¶å¤„ç†éŸ³é¢‘å’Œæ–‡æœ¬ã€‚</li>
<li>æ¨¡å‹çš„æ¨ç†æ€§èƒ½ä¸æ¨¡å‹å¤§å°æœ‰å…³ï¼Œå¤§å‹æ¨¡å‹è¡¨ç°æ›´ä½³ã€‚</li>
<li>ç›®å‰å°šæœªæœ‰ç ”ç©¶æ¢ç´¢å°å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹çš„æ¨ç†ä»»åŠ¡ï¼Œå°½ç®¡è¿™åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šæœ‰æ½œåœ¨åº”ç”¨ã€‚</li>
<li>Mellowæ˜¯ä¸€ä¸ªä¸“ä¸ºæ¨ç†è®¾è®¡çš„å°å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
<li>Mellowåœ¨MMAUä¸Šçš„å¾—åˆ†ä¸SoTA Qwen2 Audioç›¸å½“ï¼Œä½†ä½¿ç”¨äº†50å€æ›´å°‘çš„å‚æ•°ï¼Œå¹¶åœ¨æ›´å°‘çš„æ•°æ®ï¼ˆéŸ³é¢‘å°æ—¶æ•°ï¼‰ä¸Šè¿›è¡Œè®­ç»ƒã€‚</li>
<li>ReasonAQAæ•°æ®é›†çš„å¼•å…¥ï¼Œè¯¥æ•°æ®é›†ç”±ç°æœ‰æ•°æ®é›†å’Œåˆæˆæ•°æ®é›†ç»„æˆï¼Œæ—¨åœ¨æé«˜æ¨¡å‹çš„éŸ³é¢‘åŸºç¡€æ¨ç†èƒ½åŠ›ã€‚</li>
<li>åˆæˆæ•°æ®é›†æ˜¯é€šè¿‡éŸ³é¢‘æè¿°æ•°æ®é›†å’Œå¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„ï¼ŒåŒ…å«å…³äºéŸ³é¢‘äº‹ä»¶çš„è¯¦ç»†å’Œå¤šé€‰æ‹©é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08540">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-aff1ead2bba53b54c3b24e2f1dace146.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-77654bb1d86445dffcaf96dbaf0bf897.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0d06cd1a1fe723176b707171b07a7c2a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-01e59c29ba72e08eb8cd02d598d55a24.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-496aac1bb963e8cbf99e1085c25cc1df.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-14\./crop_R1_Reasoning/2503.08540v1/page_5_0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-26f340856a9a0dd9ae2642ef9e5825fc.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="GTR-Guided-Thought-Reinforcement-Prevents-Thought-Collapse-in-RL-based-VLM-Agent-Training"><a href="#GTR-Guided-Thought-Reinforcement-Prevents-Thought-Collapse-in-RL-based-VLM-Agent-Training" class="headerlink" title="GTR: Guided Thought Reinforcement Prevents Thought Collapse in RL-based   VLM Agent Training"></a>GTR: Guided Thought Reinforcement Prevents Thought Collapse in RL-based   VLM Agent Training</h2><p><strong>Authors:Tong Wei, Yijun Yang, Junliang Xing, Yuanchun Shi, Zongqing Lu, Deheng Ye</strong></p>
<p>Reinforcement learning with verifiable outcome rewards (RLVR) has effectively scaled up chain-of-thought (CoT) reasoning in large language models (LLMs). Yet, its efficacy in training vision-language model (VLM) agents for goal-directed action reasoning in visual environments is less established. This work investigates this problem through extensive experiments on complex card games, such as 24 points, and embodied tasks from ALFWorld. We find that when rewards are based solely on action outcomes, RL fails to incentivize CoT reasoning in VLMs, instead leading to a phenomenon we termed thought collapse, characterized by a rapid loss of diversity in the agentâ€™s thoughts, state-irrelevant and incomplete reasoning, and subsequent invalid actions, resulting in negative rewards. To counteract thought collapse, we highlight the necessity of process guidance and propose an automated corrector that evaluates and refines the agentâ€™s reasoning at each RL step. This simple and scalable GTR (Guided Thought Reinforcement) framework trains reasoning and action simultaneously without the need for dense, per-step human labeling. Our experiments demonstrate that GTR significantly enhances the performance and generalization of the LLaVA-7b model across various visual environments, achieving 3-5 times higher task success rates compared to SoTA models with notably smaller model sizes. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯ç»“æœå¥–åŠ±ï¼ˆRLVRï¼‰å·²åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­æœ‰æ•ˆåœ°æ‰©å±•äº†æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†ã€‚ç„¶è€Œï¼Œå…¶åœ¨é’ˆå¯¹è§†è§‰ç¯å¢ƒä¸­ç›®æ ‡å¯¼å‘è¡ŒåŠ¨æ¨ç†è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ä»£ç†æ–¹é¢çš„æœ‰æ•ˆæ€§å°šæœªæ˜ç¡®å»ºç«‹ã€‚æœ¬ç ”ç©¶é€šè¿‡å¤æ‚çš„å¡ç‰Œæ¸¸æˆï¼ˆå¦‚äºŒåå››ç‚¹ï¼‰å’Œèº«ä½“ä»»åŠ¡ï¼ˆæ¥è‡ªALFWorldï¼‰çš„å¤§é‡å®éªŒæ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚æˆ‘ä»¬å‘ç°ï¼Œå½“å¥–åŠ±ä»…ä»…åŸºäºè¡ŒåŠ¨ç»“æœæ—¶ï¼Œå¼ºåŒ–å­¦ä¹ æ— æ³•æ¿€åŠ±VLMä¸­çš„æ€ç»´é“¾æ¨ç†ï¼Œåè€Œå¯¼è‡´æˆ‘ä»¬ç§°ä¹‹ä¸ºâ€œæ€ç»´å´©æºƒâ€çš„ç°è±¡ï¼Œç‰¹ç‚¹æ˜¯ä»£ç†æ€ç»´çš„è¿…é€Ÿä¸§å¤±å¤šæ ·æ€§ã€çŠ¶æ€æ— å…³å’Œä¸å®Œæ•´çš„æ¨ç†ï¼Œä»¥åŠéšåçš„æ— æ•ˆè¡ŒåŠ¨ï¼Œå¯¼è‡´è´Ÿé¢å¥–åŠ±ã€‚ä¸ºäº†å¯¹æŠ—æ€ç»´å´©æºƒï¼Œæˆ‘ä»¬å¼ºè°ƒäº†è¿‡ç¨‹æŒ‡å¯¼çš„å¿…è¦æ€§ï¼Œå¹¶æå‡ºäº†ä¸€ç§è‡ªåŠ¨åŒ–æ ¡æ­£å™¨ï¼Œå®ƒåœ¨æ¯ä¸ªå¼ºåŒ–å­¦ä¹ æ­¥éª¤ä¸­è¯„ä¼°å’Œç²¾ç‚¼ä»£ç†çš„æ¨ç†ã€‚è¿™ç§ç®€å•ä¸”å¯æ‰©å±•çš„GTRï¼ˆå¼•å¯¼æ€ç»´å¼ºåŒ–ï¼‰æ¡†æ¶åŒæ—¶è®­ç»ƒæ¨ç†å’Œè¡ŒåŠ¨ï¼Œæ— éœ€å¯†é›†ã€é€æ­¥çš„äººå·¥æ ‡æ³¨ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒGTRæ˜¾è‘—æé«˜äº†LLaVA-7bæ¨¡å‹åœ¨å„ç§è§†è§‰ç¯å¢ƒä¸­çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ï¼Œä¸æœ€æ–°æ¨¡å‹ç›¸æ¯”ï¼Œä»»åŠ¡æˆåŠŸç‡æé«˜äº†3-5å€ï¼Œè€Œä¸”æ¨¡å‹å¤§å°æ˜¾è‘—æ›´å°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08525v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šå¼ºåŒ–å­¦ä¹ ç»“åˆå¯éªŒè¯ç»“æœå¥–åŠ±ï¼ˆRLVRï¼‰åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†å·²ç»å–å¾—äº†æœ‰æ•ˆè¿›å±•ã€‚ç„¶è€Œï¼Œå…¶åœ¨è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ä»£ç†è¿›è¡Œè§†è§‰ç¯å¢ƒä¸­çš„ç›®æ ‡å¯¼å‘è¡ŒåŠ¨æ¨ç†æ–¹é¢çš„æœ‰æ•ˆæ€§å°šæœªæ˜ç¡®ã€‚æœ¬ç ”ç©¶é€šè¿‡å¤æ‚çš„å¡ç‰Œæ¸¸æˆå’ŒALFWorldä¸­çš„å®ä½“ä»»åŠ¡å®éªŒï¼Œå‘ç°ä»…åŸºäºè¡ŒåŠ¨ç»“æœçš„å¥–åŠ±ä¼šå¯¼è‡´å¼ºåŒ–å­¦ä¹ æ— æ³•æ¿€åŠ±VLMä¸­çš„CoTæ¨ç†ï¼Œå‡ºç°æˆ‘ä»¬ç§°ä¹‹ä¸ºâ€œæ€ç»´å´©æºƒâ€çš„ç°è±¡ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼ºè°ƒäº†è¿‡ç¨‹æŒ‡å¯¼çš„å¿…è¦æ€§ï¼Œå¹¶æå‡ºä¸€ç§è‡ªåŠ¨åŒ–æ ¡æ­£å™¨ï¼Œåœ¨å¼ºåŒ–å­¦ä¹ çš„æ¯ä¸€æ­¥è¯„ä¼°å¹¶æ”¹è¿›ä»£ç†çš„æ¨ç†ã€‚è¿™ç§ç®€å•ä¸”å¯æ‰©å±•çš„GTRï¼ˆå¼•å¯¼æ€ç»´å¼ºåŒ–ï¼‰æ¡†æ¶åŒæ—¶è®­ç»ƒæ¨ç†å’Œè¡ŒåŠ¨ï¼Œæ— éœ€å¯†é›†çš„æ¯ä¸€æ­¥äººå·¥æ ‡æ³¨ã€‚å®éªŒè¡¨æ˜ï¼ŒGTRæ˜¾è‘—æé«˜äº†LLaVA-7bæ¨¡å‹åœ¨å„ç§è§†è§‰ç¯å¢ƒä¸­çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ï¼Œç›¸è¾ƒäºæœ€å…ˆè¿›æ¨¡å‹çš„ä»»åŠ¡æˆåŠŸç‡æé«˜äº†3-5å€ï¼Œä¸”æ¨¡å‹ä½“ç§¯æ›´å°ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ç»“åˆå¯éªŒè¯ç»“æœå¥–åŠ±ï¼ˆRLVRï¼‰èƒ½å¤Ÿæå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†èƒ½åŠ›ã€‚</li>
<li>åœ¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ä¸­ï¼Œä»…ä¾èµ–è¡ŒåŠ¨ç»“æœçš„å¥–åŠ±ä¼šå¯¼è‡´å¼ºåŒ–å­¦ä¹ æ— æ³•æœ‰æ•ˆæ¿€åŠ±æ¨ç†æ€ç»´ï¼Œå‡ºç°â€œæ€ç»´å´©æºƒâ€ç°è±¡ã€‚</li>
<li>ä¸ºè§£å†³æ€ç»´å´©æºƒé—®é¢˜ï¼Œéœ€è¦å¼•å…¥è¿‡ç¨‹æŒ‡å¯¼ï¼Œæå‡ºä¸€ç§è‡ªåŠ¨åŒ–æ ¡æ­£å™¨æ¥è¯„ä¼°å¹¶æ”¹è¿›ä»£ç†çš„æ¯ä¸€æ­¥æ¨ç†ã€‚</li>
<li>GTRï¼ˆå¼•å¯¼æ€ç»´å¼ºåŒ–ï¼‰æ¡†æ¶èƒ½åŒæ—¶è®­ç»ƒæ¨ç†å’Œè¡ŒåŠ¨ï¼Œæ— éœ€å¯†é›†çš„æ¯ä¸€æ­¥äººå·¥æ ‡æ³¨ã€‚</li>
<li>GTRæ¡†æ¶åœ¨å¤šç§è§†è§‰ç¯å¢ƒä¸­æ˜¾è‘—æé«˜æ¨¡å‹çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ä¸ç°æœ‰å…ˆè¿›æ¨¡å‹ç›¸æ¯”ï¼ŒGTRæ¡†æ¶çš„ä»»åŠ¡æˆåŠŸç‡æé«˜äº†3-5å€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08525">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8b14879ae44dc2af95087638a2714b31.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5c12933f6641d7f0d87a97682719eef.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-529a8abcd3babecf78139b82f7cff30c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b4ff83c972b1e7e68ebbc0eafc4c7abf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e78199b884320a1387155d0e68a3b9a2.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="LightPlanner-Unleashing-the-Reasoning-Capabilities-of-Lightweight-Large-Language-Models-in-Task-Planning"><a href="#LightPlanner-Unleashing-the-Reasoning-Capabilities-of-Lightweight-Large-Language-Models-in-Task-Planning" class="headerlink" title="LightPlanner: Unleashing the Reasoning Capabilities of Lightweight Large   Language Models in Task Planning"></a>LightPlanner: Unleashing the Reasoning Capabilities of Lightweight Large   Language Models in Task Planning</h2><p><strong>Authors:Weijie Zhou, Yi Peng, Manli Tao, Chaoyang Zhao, Honghui Dong, Ming Tang, Jinqiao Wang</strong></p>
<p>In recent years, lightweight large language models (LLMs) have garnered significant attention in the robotics field due to their low computational resource requirements and suitability for edge deployment. However, in task planning â€“ particularly for complex tasks that involve dynamic semantic logic reasoning â€“ lightweight LLMs have underperformed. To address this limitation, we propose a novel task planner, LightPlanner, which enhances the performance of lightweight LLMs in complex task planning by fully leveraging their reasoning capabilities. Unlike conventional planners that use fixed skill templates, LightPlanner controls robot actions via parameterized function calls, dynamically generating parameter values. This approach allows for fine-grained skill control and improves task planning success rates in complex scenarios. Furthermore, we introduce hierarchical deep reasoning. Before generating each action decision step, LightPlanner thoroughly considers three levels: action execution (feedback verification), semantic parsing (goal consistency verification), and parameter generation (parameter validity verification). This ensures the correctness of subsequent action controls. Additionally, we incorporate a memory module to store historical actions, thereby reducing context length and enhancing planning efficiency for long-term tasks. We train the LightPlanner-1.5B model on our LightPlan-40k dataset, which comprises 40,000 action controls across tasks with 2 to 13 action steps. Experiments demonstrate that our model achieves the highest task success rate despite having the smallest number of parameters. In tasks involving spatial semantic reasoning, the success rate exceeds that of ReAct by 14.9 percent. Moreover, we demonstrate LightPlannerâ€™s potential to operate on edge devices. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œç”±äºæœºå™¨äººé¢†åŸŸå¯¹è®¡ç®—èµ„æºçš„è¦æ±‚è¾ƒä½ä¸”é€‚åˆè¾¹ç¼˜éƒ¨ç½²ï¼Œè½»é‡çº§çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å·²ç»å¼•èµ·äº†å¹¿æ³›çš„å…³æ³¨ã€‚ç„¶è€Œï¼Œåœ¨ä»»åŠ¡è§„åˆ’æ–¹é¢ï¼Œå°¤å…¶æ˜¯åœ¨æ¶‰åŠåŠ¨æ€è¯­ä¹‰é€»è¾‘æ¨ç†çš„å¤æ‚ä»»åŠ¡ä¸­ï¼Œè½»é‡çº§LLMsçš„è¡¨ç°å¹¶ä¸ç†æƒ³ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„ä»»åŠ¡è§„åˆ’å™¨LightPlannerï¼Œå®ƒé€šè¿‡å……åˆ†åˆ©ç”¨è½»é‡çº§LLMçš„æ¨ç†èƒ½åŠ›ï¼Œæé«˜å…¶åœ¨å¤æ‚ä»»åŠ¡è§„åˆ’ä¸­çš„æ€§èƒ½ã€‚ä¸ä¼ ç»Ÿçš„ä½¿ç”¨å›ºå®šæŠ€èƒ½æ¨¡æ¿çš„è§„åˆ’å™¨ä¸åŒï¼ŒLightPlanneré€šè¿‡å‚æ•°åŒ–å‡½æ•°è°ƒç”¨æ§åˆ¶æœºå™¨äººåŠ¨ä½œï¼Œå¹¶åŠ¨æ€ç”Ÿæˆå‚æ•°å€¼ã€‚è¿™ç§æ–¹æ³•å…è®¸ç²¾ç»†çš„æŠ€èƒ½æ§åˆ¶ï¼Œå¹¶åœ¨å¤æ‚åœºæ™¯ä¸­æé«˜äº†ä»»åŠ¡è§„åˆ’çš„æˆåŠŸç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†åˆ†å±‚æ·±åº¦æ¨ç†ã€‚åœ¨ç”Ÿæˆæ¯ä¸ªåŠ¨ä½œå†³ç­–æ­¥éª¤ä¹‹å‰ï¼ŒLightPlannerä¼šå……åˆ†è€ƒè™‘ä¸‰ä¸ªå±‚æ¬¡ï¼šåŠ¨ä½œæ‰§è¡Œï¼ˆåé¦ˆéªŒè¯ï¼‰ã€è¯­ä¹‰è§£æï¼ˆç›®æ ‡ä¸€è‡´æ€§éªŒè¯ï¼‰å’Œå‚æ•°ç”Ÿæˆï¼ˆå‚æ•°æœ‰æ•ˆæ€§éªŒè¯ï¼‰ã€‚è¿™ç¡®ä¿äº†åç»­åŠ¨ä½œæ§åˆ¶çš„æ­£ç¡®æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åŠ å…¥äº†ä¸€ä¸ªè®°å¿†æ¨¡å—æ¥å­˜å‚¨å†å²åŠ¨ä½œï¼Œä»è€Œå‡å°‘äº†ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œæé«˜äº†é•¿æœŸä»»åŠ¡çš„è§„åˆ’æ•ˆç‡ã€‚æˆ‘ä»¬åœ¨LightPlan-40kæ•°æ®é›†ä¸Šè®­ç»ƒäº†LightPlanner-1.5Bæ¨¡å‹ï¼Œè¯¥æ•°æ®é›†åŒ…å«4ä¸‡ä¸ªæ¶‰åŠä»»åŠ¡ä¸­è¡ŒåŠ¨æ­¥éª¤çš„åŠ¨ä½œæ§åˆ¶æŒ‡ä»¤ï¼Œæ¶µç›–çš„ä»»åŠ¡æ¶‰åŠè¡ŒåŠ¨æ­¥éª¤åœ¨2è‡³13ä¹‹é—´ã€‚å®éªŒè¡¨æ˜ï¼Œå°½ç®¡æˆ‘ä»¬çš„æ¨¡å‹å‚æ•°æ•°é‡æœ€å°‘ï¼Œä½†æˆ‘ä»¬çš„æ¨¡å‹ä»ç„¶è¾¾åˆ°äº†æœ€é«˜çš„ä»»åŠ¡æˆåŠŸç‡ã€‚åœ¨æ¶‰åŠç©ºé—´è¯­ä¹‰æ¨ç†çš„ä»»åŠ¡ä¸­ï¼ŒæˆåŠŸç‡è¶…è¿‡äº†ReActæ¨¡å‹14.9ä¸ªç™¾åˆ†ç‚¹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å±•ç¤ºäº†LightPlanneråœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šçš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08508v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹è½»é‡åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æœºå™¨äººä»»åŠ¡è§„åˆ’ä¸­çš„æ€§èƒ½ä¸è¶³é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹ä»»åŠ¡è§„åˆ’å™¨LightPlannerã€‚LightPlanneré€šè¿‡å‚æ•°åŒ–å‡½æ•°è°ƒç”¨æ§åˆ¶æœºå™¨äººåŠ¨ä½œï¼Œå¹¶å¼•å…¥å±‚æ¬¡åŒ–æ·±åº¦æ¨ç†å’Œè®°å¿†æ¨¡å—ï¼Œæé«˜äº†è½»é‡çº§LLMsåœ¨å¤æ‚ä»»åŠ¡è§„åˆ’ä¸­çš„æ€§èƒ½ã€‚å®éªŒè¯æ˜ï¼ŒLightPlanneræ¨¡å‹åœ¨ä»»åŠ¡æˆåŠŸç‡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶æ˜¯æ¶‰åŠç©ºé—´è¯­ä¹‰æ¨ç†çš„ä»»åŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è½»é‡åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æœºå™¨äººä»»åŠ¡è§„åˆ’ä¸­å—åˆ°å…³æ³¨ï¼Œä½†åœ¨æ¶‰åŠåŠ¨æ€è¯­ä¹‰é€»è¾‘æ¨ç†çš„å¤æ‚ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹ä»»åŠ¡è§„åˆ’å™¨LightPlannerï¼Œé€šè¿‡å‚æ•°åŒ–å‡½æ•°è°ƒç”¨æ§åˆ¶æœºå™¨äººåŠ¨ä½œï¼Œæé«˜è½»é‡çº§LLMsåœ¨å¤æ‚ä»»åŠ¡è§„åˆ’ä¸­çš„æ€§èƒ½ã€‚</li>
<li>LightPlannerå¼•å…¥å±‚æ¬¡åŒ–æ·±åº¦æ¨ç†ï¼Œç¡®ä¿åç»­åŠ¨ä½œæ§åˆ¶æ­£ç¡®æ€§ã€‚</li>
<li>LightPlanneråŒ…å«è®°å¿†æ¨¡å—ï¼Œç”¨äºå­˜å‚¨å†å²åŠ¨ä½œï¼Œæé«˜é•¿æœŸä»»åŠ¡çš„è§„åˆ’æ•ˆç‡ã€‚</li>
<li>LightPlanner-1.5Bæ¨¡å‹åœ¨LightPlan-40kæ•°æ®é›†ä¸Šè®­ç»ƒï¼Œå®éªŒè¯æ˜å…¶ä»»åŠ¡æˆåŠŸç‡è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>åœ¨æ¶‰åŠç©ºé—´è¯­ä¹‰æ¨ç†çš„ä»»åŠ¡ä¸­ï¼ŒLightPlanneræˆåŠŸç‡è¶…è¿‡ReActè¾¾14.9%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08508">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f674b5c7435dee4d1209323635c808fe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4a27a5598ec4424f4e908ada8b16a0fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-86340c60e5a8884d11ca981a67bc8dc7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-50c2fd8fcd57b31229d54143a618a25e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-170e198a7958371cdc26f60418522de1.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Enhancing-Multi-Hop-Fact-Verification-with-Structured-Knowledge-Augmented-Large-Language-Models"><a href="#Enhancing-Multi-Hop-Fact-Verification-with-Structured-Knowledge-Augmented-Large-Language-Models" class="headerlink" title="Enhancing Multi-Hop Fact Verification with Structured   Knowledge-Augmented Large Language Models"></a>Enhancing Multi-Hop Fact Verification with Structured   Knowledge-Augmented Large Language Models</h2><p><strong>Authors:Han Cao, Lingwei Wei, Wei Zhou, Songlin Hu</strong></p>
<p>The rapid development of social platforms exacerbates the dissemination of misinformation, which stimulates the research in fact verification. Recent studies tend to leverage semantic features to solve this problem as a single-hop task. However, the process of verifying a claim requires several pieces of evidence with complicated inner logic and relations to verify the given claim in real-world situations. Recent studies attempt to improve both understanding and reasoning abilities to enhance the performance, but they overlook the crucial relations between entities that benefit models to understand better and facilitate the prediction. To emphasize the significance of relations, we resort to Large Language Models (LLMs) considering their excellent understanding ability. Instead of other methods using LLMs as the predictor, we take them as relation extractors, for they do better in understanding rather than reasoning according to the experimental results. Thus, to solve the challenges above, we propose a novel Structured Knowledge-Augmented LLM-based Network (LLM-SKAN) for multi-hop fact verification. Specifically, we utilize an LLM-driven Knowledge Extractor to capture fine-grained information, including entities and their complicated relations. Besides, we leverage a Knowledge-Augmented Relation Graph Fusion module to interact with each node and learn better claim-evidence representations comprehensively. The experimental results on four common-used datasets demonstrate the effectiveness and superiority of our model. </p>
<blockquote>
<p>éšç€ç¤¾äº¤å¹³å°çš„é«˜é€Ÿå‘å±•ï¼ŒåŠ å‰§äº†é”™ä¿¡æ¯çš„ä¼ æ’­ï¼Œè¿™æ¿€å‘äº†äº‹å®æ ¸æŸ¥çš„ç ”ç©¶ã€‚è¿‘æœŸçš„ç ”ç©¶å€¾å‘äºåˆ©ç”¨è¯­ä¹‰ç‰¹å¾æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒæŠŠå®ƒä½œä¸ºä¸€ä¸ªå•è·³ä»»åŠ¡æ¥å¤„ç†ã€‚ç„¶è€Œï¼ŒéªŒè¯ä¸€ä¸ªå£°æ˜çš„è¿‡ç¨‹éœ€è¦å¤šä¸ªè¯æ®ï¼Œè¿™äº›è¯æ®åœ¨å†…éƒ¨é€»è¾‘å’Œå…³è”ä¸Šéå¸¸å¤æ‚ï¼Œæ‰èƒ½åœ¨ç°å®æƒ…å¢ƒä¸­å¯¹ç»™å®šçš„å£°æ˜è¿›è¡ŒéªŒè¯ã€‚è¿‘æœŸçš„ç ”ç©¶è¯•å›¾æé«˜ç†è§£å’Œæ¨ç†èƒ½åŠ›æ¥æå‡æ€§èƒ½ï¼Œä½†å®ƒä»¬å¿½è§†äº†å®ä½“ä¹‹é—´çš„å…³é”®å…³ç³»ï¼Œè¿™äº›å…³ç³»æœ‰åŠ©äºæ¨¡å‹æ›´å¥½åœ°ç†è§£å¹¶ä¿ƒè¿›é¢„æµ‹ã€‚ä¸ºäº†å¼ºè°ƒå…³ç³»çš„é‡è¦æ€§ï¼Œæˆ‘ä»¬å€ŸåŠ©å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œè€ƒè™‘åˆ°å®ƒä»¬å‡ºè‰²çš„ç†è§£èƒ½åŠ›ã€‚ä¸å…¶ä»–å°†LLMç”¨ä½œé¢„æµ‹å™¨çš„æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬å°†å…¶ç”¨ä½œå…³ç³»æå–å™¨ï¼Œæ ¹æ®å®éªŒç»“æœï¼Œå®ƒä»¬åœ¨ç†è§£è€Œéæ¨ç†æ–¹é¢è¡¨ç°æ›´å¥½ã€‚å› æ­¤ï¼Œä¸ºäº†è§£å†³ä¸Šè¿°æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„ç»“æ„åŒ–çŸ¥è¯†å¢å¼ºç½‘ç»œï¼ˆLLM-SKANï¼‰è¿›è¡Œå¤šè·³äº‹å®æ ¸æŸ¥ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åˆ©ç”¨LLMé©±åŠ¨çš„çŸ¥è¯†æå–å™¨æ¥æ•æ‰ç²¾ç»†ä¿¡æ¯ï¼ŒåŒ…æ‹¬å®ä½“å’Œä»–ä»¬çš„å¤æ‚å…³ç³»ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å€ŸåŠ©çŸ¥è¯†å¢å¼ºå…³ç³»å›¾èåˆæ¨¡å—æ¥ä¸æ¯ä¸ªèŠ‚ç‚¹è¿›è¡Œäº¤äº’ï¼Œå¹¶å…¨é¢å­¦ä¹ æ›´å¥½çš„å£°æ˜-è¯æ®è¡¨ç¤ºã€‚åœ¨å››ä¸ªå¸¸ç”¨æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜äº†æˆ‘ä»¬æ¨¡å‹çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08495v1">PDF</a> Accepted by AAAI 2025</p>
<p><strong>Summary</strong>ï¼šéšç€ç¤¾äº¤å¹³å°å¿«é€Ÿå‘å±•ï¼Œè°£è¨€ä¼ æ’­é—®é¢˜åŠ å‰§ï¼Œä¿ƒä½¿äº†äº‹å®æ ¸æŸ¥ç ”ç©¶çš„é‡è¦æ€§æ—¥ç›Šå¢åŠ ã€‚æœ€è¿‘ç ”ç©¶å€¾å‘äºåˆ©ç”¨è¯­ä¹‰ç‰¹å¾è§£å†³è¿™ä¸€é—®é¢˜ï¼Œä½†ç°å®æƒ…å†µä¸­çš„æ ¸æŸ¥éœ€è¦å¤šæ–¹è¯æ®å’Œå¤æ‚é€»è¾‘ã€‚ä¸ºæé«˜æ¨¡å‹ç†è§£å’Œæ¨ç†èƒ½åŠ›ï¼Œæœ¬æ–‡åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¼˜ç§€ç†è§£èƒ½åŠ›ï¼Œå°†å…¶ä½œä¸ºå…³ç³»æå–å™¨ï¼Œæå‡ºä¸€ç§åŸºäºLLMçš„ç»“æ„åŒ–çŸ¥è¯†å¢å¼ºç½‘ç»œï¼ˆLLM-SKANï¼‰è¿›è¡Œå¤šè·³äº‹å®æ ¸æŸ¥ã€‚é€šè¿‡LLMé©±åŠ¨çš„çŸ¥è¯†æå–å™¨æ•æ‰å®ä½“åŠå…¶å¤æ‚å…³ç³»ï¼Œå¹¶ç»“åˆçŸ¥è¯†å¢å¼ºå…³ç³»å›¾èåˆæ¨¡å—ï¼Œæé«˜æ¨¡å‹æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜è¯¥æ¨¡å‹åœ¨å››ä¸ªå¸¸ç”¨æ•°æ®é›†ä¸Šçš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ç¤¾äº¤å¹³å°å¿«é€Ÿå‘å±•åŠ å‰§äº†è°£è¨€ä¼ æ’­ï¼Œå¼•å‘äº†å¯¹äº‹å®æ ¸æŸ¥ç ”ç©¶çš„å…³æ³¨ã€‚</li>
<li>æœ€è¿‘ç ”ç©¶å€¾å‘äºåˆ©ç”¨è¯­ä¹‰ç‰¹å¾è§£å†³äº‹å®æ ¸æŸ¥é—®é¢˜ï¼Œä½†ç°å®æƒ…å†µä¸­çš„æ ¸æŸ¥è¿‡ç¨‹å¤æ‚ï¼Œéœ€è¦å¤šæ–¹è¯æ®å’Œå¤æ‚é€»è¾‘ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·æœ‰å‡ºè‰²çš„ç†è§£èƒ½åŠ›ï¼Œæœ¬æ–‡å°†å…¶ä½œä¸ºå…³ç³»æå–å™¨æ¥å¼ºåŒ–æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>æå‡ºä¸€ç§æ–°å‹ç»“æ„åŒ–çŸ¥è¯†å¢å¼ºç½‘ç»œï¼ˆLLM-SKANï¼‰ç”¨äºå¤šè·³äº‹å®æ ¸æŸ¥ã€‚</li>
<li>LLM-SKANé€šè¿‡LLMé©±åŠ¨çš„çŸ¥è¯†æå–å™¨æ•æ‰å®ä½“åŠå…¶å¤æ‚å…³ç³»ï¼Œæé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>çŸ¥è¯†å¢å¼ºå…³ç³»å›¾èåˆæ¨¡å—èƒ½å¢å¼ºæ¨¡å‹çš„äº¤äº’èƒ½åŠ›å’Œå¯¹ç´¢èµ”è¯æ®çš„ç»¼åˆç†è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08495">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-587d4b480401e47495db6d9d9fa51210.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-907b4271d0fb455ea889979a7f597495.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9a4dfe5a88c1d68fc20aae7c2dfafd16.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-afba8942b6f8719164c9ca0fc589e2e5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ebb2aa59365aed335469a44af6598e48.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b7dd54b18126619e4e72bb92d7f96c5e.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Seeing-and-Reasoning-with-Confidence-Supercharging-Multimodal-LLMs-with-an-Uncertainty-Aware-Agentic-Framework"><a href="#Seeing-and-Reasoning-with-Confidence-Supercharging-Multimodal-LLMs-with-an-Uncertainty-Aware-Agentic-Framework" class="headerlink" title="Seeing and Reasoning with Confidence: Supercharging Multimodal LLMs with   an Uncertainty-Aware Agentic Framework"></a>Seeing and Reasoning with Confidence: Supercharging Multimodal LLMs with   an Uncertainty-Aware Agentic Framework</h2><p><strong>Authors:Zhuo Zhi, Chen Feng, Adam Daneshmend, Mine Orlu, Andreas Demosthenous, Lu Yin, Da Li, Ziquan Liu, Miguel R. D. Rodrigues</strong></p>
<p>Multimodal large language models (MLLMs) show promise in tasks like visual question answering (VQA) but still face challenges in multimodal reasoning. Recent works adapt agentic frameworks or chain-of-thought (CoT) reasoning to improve performance. However, CoT-based multimodal reasoning often demands costly data annotation and fine-tuning, while agentic approaches relying on external tools risk introducing unreliable output from these tools. In this paper, we propose Seeing and Reasoning with Confidence (SRICE), a training-free multimodal reasoning framework that integrates external vision models with uncertainty quantification (UQ) into an MLLM to address these challenges. Specifically, SRICE guides the inference process by allowing MLLM to autonomously select regions of interest through multi-stage interactions with the help of external tools. We propose to use a conformal prediction-based approach to calibrate the output of external tools and select the optimal tool by estimating the uncertainty of an MLLMâ€™s output. Our experiment shows that the average improvement of SRICE over the base MLLM is 4.6% on five datasets and the performance on some datasets even outperforms fine-tuning-based methods, revealing the significance of ensuring reliable tool use in an MLLM agent. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ç­‰ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œä½†åœ¨å¤šæ¨¡æ€æ¨ç†æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚è¿‘æœŸçš„ç ”ç©¶é‡‡ç”¨ä»£ç†æ¡†æ¶æˆ–æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†æ¥æé«˜æ€§èƒ½ã€‚ç„¶è€Œï¼ŒåŸºäºæ€ç»´é“¾çš„å¤šæ¨¡æ€æ¨ç†é€šå¸¸éœ€è¦æ˜‚è´µçš„æ•°æ®æ ‡æ³¨å’Œå¾®è°ƒï¼Œè€Œä¾èµ–å¤–éƒ¨å·¥å…·çš„ä»£ç†æ–¹æ³•åˆ™å­˜åœ¨å¼•å…¥è¿™äº›å·¥å…·ä¸å¯é è¾“å‡ºçš„é£é™©ã€‚é’ˆå¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†â€œä¿¡å¿ƒæ»¡æ»¡çš„çœ‹è§ä¸æ¨ç†â€ï¼ˆSRICEï¼‰è¿™ä¸€æ— éœ€è®­ç»ƒçš„å¤šæ¨¡æ€æ¨ç†æ¡†æ¶ã€‚SRICEé€šè¿‡å°†å¤–éƒ¨è§†è§‰æ¨¡å‹ä¸ä¸ç¡®å®šæ€§é‡åŒ–ï¼ˆUQï¼‰é›†æˆåˆ°MLLMä¸­æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚å…·ä½“æ¥è¯´ï¼ŒSRICEé€šè¿‡å¼•å¯¼æ¨ç†è¿‡ç¨‹ï¼Œä½¿MLLMèƒ½å¤Ÿå€ŸåŠ©å¤–éƒ¨å·¥å…·è‡ªä¸»é€‰æ‹©åˆé€‚çš„æ„Ÿå…´è¶£åŒºåŸŸè¿›è¡Œå¤šé˜¶æ®µäº¤äº’ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºé€‚é…é¢„æµ‹çš„æ–¹æ³•å¯¹å¤–éƒ¨å·¥å…·çš„è¾“å‡ºè¿›è¡Œæ ¡å‡†ï¼Œå¹¶é€šè¿‡ä¼°è®¡MLLMè¾“å‡ºçš„ä¸ç¡®å®šæ€§æ¥é€‰æ‹©æœ€ä½³å·¥å…·ã€‚å®éªŒè¡¨æ˜ï¼ŒSRICEç›¸è¾ƒäºåŸºç¡€MLLMåœ¨äº”ä¸ªæ•°æ®é›†ä¸Šçš„å¹³å‡æ”¹è¿›ç‡ä¸º4.6%ï¼Œå¹¶ä¸”åœ¨æŸäº›æ•°æ®é›†ä¸Šçš„æ€§èƒ½ç”šè‡³è¶…è¿‡äº†å¾®è°ƒæ–¹æ³•ï¼Œè¿™è¡¨æ˜åœ¨MLLMä»£ç†ä¸­ç¡®ä¿å¯é çš„å·¥å…·ä½¿ç”¨å…·æœ‰é‡è¦æ„ä¹‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08308v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ç­‰ä»»åŠ¡ä¸­çš„æ½œåŠ›ï¼Œæœ¬æ–‡æå‡ºäº†Seeing and Reasoning with Confidenceï¼ˆSRICEï¼‰æ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„å¤šæ¨¡æ€æ¨ç†æ¡†æ¶ã€‚å®ƒé›†æˆäº†å¤–éƒ¨è§†è§‰æ¨¡å‹ä¸ä¸ç¡®å®šæ€§é‡åŒ–ï¼ˆUQï¼‰ï¼Œè§£å†³äº†ç°æœ‰æŒ‘æˆ˜ã€‚SRICEé€šè¿‡å¤šé˜¶æ®µäº¤äº’è‡ªä¸»é€‰å–æ„Ÿå…´è¶£åŒºåŸŸï¼Œå¹¶ä½¿ç”¨å¤–éƒ¨å·¥å…·è¾…åŠ©æ¨ç†è¿‡ç¨‹ã€‚åŒæ—¶ï¼Œé‡‡ç”¨åŸºäºå…±è¯†é¢„æµ‹çš„æ–¹æ³•æ ¡å‡†å¤–éƒ¨å·¥å…·çš„è¾“å‡ºï¼Œé€šè¿‡ä¼°è®¡MLLMçš„è¾“å‡ºä¸ç¡®å®šæ€§æ¥é€‰æ‹©æœ€ä½³å·¥å…·ã€‚å®éªŒè¡¨æ˜ï¼ŒSRICEåœ¨äº”ä¸ªæ•°æ®é›†ä¸Šçš„å¹³å‡æ”¹è¿›æ¯”åŸºç¡€MLLMæé«˜äº†4.6%ï¼Œåœ¨æŸäº›æ•°æ®é›†ä¸Šçš„æ€§èƒ½ç”šè‡³è¶…è¿‡äº†å¾®è°ƒæ–¹æ³•ï¼Œçªæ˜¾äº†åœ¨MLLMä»£ç†ä¸­ç¡®ä¿å¯é å·¥å…·ä½¿ç”¨çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ç­‰ä»»åŠ¡ä¸­å…·æœ‰æ½œåŠ›ï¼Œä½†ä»é¢ä¸´å¤šæ¨¡æ€æ¨ç†çš„æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¦‚åŸºäºä»£ç†æ¡†æ¶å’Œé“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†çš„æ–¹æ³•è™½èƒ½æå‡æ€§èƒ½ï¼Œä½†å­˜åœ¨æ•°æ®æ ‡æ³¨æˆæœ¬é«˜å’Œä¾èµ–å¤–éƒ¨å·¥å…·å¸¦æ¥çš„ä¸ç¡®å®šæ€§é—®é¢˜ã€‚</li>
<li>SRICEæ¡†æ¶æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„å¤šæ¨¡æ€æ¨ç†æ¡†æ¶ï¼Œé›†æˆäº†å¤–éƒ¨è§†è§‰æ¨¡å‹ä¸ä¸ç¡®å®šæ€§é‡åŒ–ï¼ˆUQï¼‰ã€‚</li>
<li>SRICEé€šè¿‡å¤šé˜¶æ®µäº¤äº’è‡ªä¸»é€‰å–æ„Ÿå…´è¶£åŒºåŸŸï¼Œå¼•å¯¼æ¨ç†è¿‡ç¨‹ã€‚</li>
<li>é‡‡ç”¨å…±è¯†é¢„æµ‹æ–¹æ³•æ ¡å‡†å¤–éƒ¨å·¥å…·è¾“å‡ºï¼Œé€šè¿‡ä¼°è®¡MLLMè¾“å‡ºä¸ç¡®å®šæ€§æ¥é€‰æ‹©æœ€ä½³å·¥å…·ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºSRICEåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºåŸºç¡€MLLMï¼Œç”šè‡³åœ¨æŸäº›æ•°æ®é›†ä¸Šè¶…è¿‡å¾®è°ƒæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08308">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-eae1e7c5806ce1684ed5af1d349a9cf7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0267038b5d2ab0c6bea3e0e8a2213010.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="A-Cascading-Cooperative-Multi-agent-Framework-for-On-ramp-Merging-Control-Integrating-Large-Language-Models"><a href="#A-Cascading-Cooperative-Multi-agent-Framework-for-On-ramp-Merging-Control-Integrating-Large-Language-Models" class="headerlink" title="A Cascading Cooperative Multi-agent Framework for On-ramp Merging   Control Integrating Large Language Models"></a>A Cascading Cooperative Multi-agent Framework for On-ramp Merging   Control Integrating Large Language Models</h2><p><strong>Authors:Miao Zhang, Zhenlong Fang, Tianyi Wang, Qian Zhang, Shuai Lu, Junfeng Jiao, Tianyu Shi</strong></p>
<p>Traditional Reinforcement Learning (RL) suffers from replicating human-like behaviors, generalizing effectively in multi-agent scenarios, and overcoming inherent interpretability issues.These tasks are compounded when deep environment understanding, agent coordination and dynamic optimization are required. While Large Language Model (LLM) enhanced methods have shown promise in generalization and interoperability, they often neglect necessary multi-agent coordination. Therefore, we introduce the Cascading Cooperative Multi-agent (CCMA) framework, integrating RL for individual interactions, a fine-tuned LLM for regional cooperation, a reward function for global optimization, and the Retrieval-augmented Generation mechanism to dynamically optimize decision-making across complex driving scenarios. Our experiments demonstrate that the CCMA outperforms existing RL methods, demonstrating significant improvements in both micro and macro-level performance in complex driving environments. </p>
<blockquote>
<p>ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¤åˆ¶äººç±»è¡Œä¸ºã€åœ¨å¤šæ™ºèƒ½ä½“åœºæ™¯ä¸­çš„æœ‰æ•ˆæ³›åŒ–ä»¥åŠè§£å†³å›ºæœ‰çš„å¯è§£é‡Šæ€§é—®é¢˜æ–¹é¢å­˜åœ¨å›°éš¾ã€‚å½“éœ€è¦æ·±å…¥äº†è§£ç¯å¢ƒã€æ™ºèƒ½ä½“åè°ƒå’ŒåŠ¨æ€ä¼˜åŒ–æ—¶ï¼Œè¿™äº›ä»»åŠ¡æ›´ä¸ºå¤æ‚ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¢å¼ºæ–¹æ³•åœ¨æ³›åŒ–å’Œäº’æ“ä½œæ€§æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å®ƒä»¬å¾€å¾€å¿½è§†äº†å¿…è¦çš„å¤šæ™ºèƒ½ä½“åè°ƒã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†çº§è”åˆä½œå¤šæ™ºèƒ½ä½“ï¼ˆCCMAï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ•´åˆäº†ç”¨äºä¸ªä½“äº¤äº’çš„RLã€ç”¨äºåŒºåŸŸåˆä½œçš„å¾®è°ƒLLMã€ç”¨äºå…¨å±€ä¼˜åŒ–çš„å¥–åŠ±å‡½æ•°ä»¥åŠå¢å¼ºç”Ÿæˆæœºåˆ¶ï¼Œä»¥åœ¨å¤æ‚çš„é©¾é©¶åœºæ™¯ä¸­åŠ¨æ€ä¼˜åŒ–å†³ç­–ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒCCMAåœ¨å¤æ‚é©¾é©¶ç¯å¢ƒä¸­ï¼Œæ— è®ºæ˜¯åœ¨å¾®è§‚è¿˜æ˜¯å®è§‚å±‚é¢ï¼Œéƒ½ä¼˜äºç°æœ‰çš„RLæ–¹æ³•ï¼Œè¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08199v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æ¨¡æ‹Ÿäººç±»è¡Œä¸ºã€å¤šæ™ºèƒ½ä½“åœºæ™¯ä¸­çš„æœ‰æ•ˆæ³›åŒ–ä»¥åŠè§£å†³å›ºæœ‰çš„å¯è§£é‡Šæ€§é—®é¢˜æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚å½“éœ€è¦æ·±åº¦ç¯å¢ƒç†è§£ã€æ™ºèƒ½ä½“åè°ƒå’ŒåŠ¨æ€ä¼˜åŒ–æ—¶ï¼Œè¿™äº›ä»»åŠ¡æ›´åŠ å¤æ‚ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¢å¼ºæ–¹æ³•åœ¨æ³›åŒ–å’Œäº’æ“ä½œæ€§æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å®ƒä»¬å¾€å¾€å¿½ç•¥äº†å¿…è¦çš„å¤šæ™ºèƒ½ä½“åè°ƒã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†çº§è”åˆä½œå¤šæ™ºèƒ½ä½“ï¼ˆCCMAï¼‰æ¡†æ¶ï¼Œæ•´åˆRLè¿›è¡Œä¸ªä½“äº¤äº’ã€å¯¹åŒºåŸŸåˆä½œè¿›è¡Œå¾®è°ƒLLMã€ç”¨äºå…¨å±€ä¼˜åŒ–çš„å¥–åŠ±å‡½æ•°ä»¥åŠå¢å¼ºç”Ÿæˆçš„æ£€ç´¢æœºåˆ¶ï¼Œä»¥åœ¨å¤æ‚çš„é©¾é©¶åœºæ™¯ä¸­åŠ¨æ€ä¼˜åŒ–å†³ç­–ã€‚å®éªŒè¡¨æ˜ï¼ŒCCMAåœ¨å¾®è§‚å’Œå®è§‚æ€§èƒ½ä¸Šå‡ä¼˜äºç°æœ‰RLæ–¹æ³•ï¼Œåœ¨å¤æ‚é©¾é©¶ç¯å¢ƒä¸­è¡¨ç°å‡ºæ˜¾è‘—æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æ¨¡æ‹Ÿäººç±»è¡Œä¸ºã€å¤šæ™ºèƒ½ä½“åœºæ™¯ä¸­çš„æ³›åŒ–ä»¥åŠè§£å†³å¯è§£é‡Šæ€§é—®é¢˜æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†çº§è”åˆä½œå¤šæ™ºèƒ½ä½“ï¼ˆCCMAï¼‰æ¡†æ¶ã€‚</li>
<li>CCMAæ¡†æ¶ç»“åˆäº†RLã€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€å¥–åŠ±å‡½æ•°å’Œæ£€ç´¢å¢å¼ºç”Ÿæˆæœºåˆ¶ã€‚</li>
<li>RLç”¨äºä¸ªä½“äº¤äº’ï¼ŒLLMç”¨äºåŒºåŸŸåˆä½œï¼Œå¥–åŠ±å‡½æ•°ç”¨äºå…¨å±€ä¼˜åŒ–ã€‚</li>
<li>CCMAæ¡†æ¶åœ¨å¤æ‚çš„é©¾é©¶åœºæ™¯ä¸­è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„RLæ–¹æ³•ã€‚</li>
<li>å®éªŒç»“æœè¯æ˜äº†CCMAåœ¨å¾®è§‚å’Œå®è§‚å±‚é¢ä¸Šçš„æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08199">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-389a1d3aa52888dd2fce5654f1f36db3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2689612830870d1b5ecb418577a6652e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e3a4883ae1afebe5ebf33df659b19424.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0ffaf724c9979232caf675d776f43744.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a34cf34db9f2422c14d1a0c1101122af.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cb24005b1a7d25e042f0dd75deccd5f4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1d30c195de0dcb75ca6b6a6322718409.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-30fe3a2f8302550624fc3d1c1f5f542c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a87db7852eeeedda3011620f604a93ec.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="ProtTeX-Structure-In-Context-Reasoning-and-Editing-of-Proteins-with-Large-Language-Models"><a href="#ProtTeX-Structure-In-Context-Reasoning-and-Editing-of-Proteins-with-Large-Language-Models" class="headerlink" title="ProtTeX: Structure-In-Context Reasoning and Editing of Proteins with   Large Language Models"></a>ProtTeX: Structure-In-Context Reasoning and Editing of Proteins with   Large Language Models</h2><p><strong>Authors:Zicheng Ma, Chuanliu Fan, Zhicong Wang, Zhenyu Chen, Xiaohan Lin, Yanheng Li, Shihao Feng, Jun Zhang, Ziqiang Cao, Yi Qin Gao</strong></p>
<p>Large language models have made remarkable progress in the field of molecular science, particularly in understanding and generating functional small molecules. This success is largely attributed to the effectiveness of molecular tokenization strategies. In protein science, the amino acid sequence serves as the sole tokenizer for LLMs. However, many fundamental challenges in protein science are inherently structure-dependent. The absence of structure-aware tokens significantly limits the capabilities of LLMs for comprehensive biomolecular comprehension and multimodal generation. To address these challenges, we introduce a novel framework, ProtTeX, which tokenizes the protein sequences, structures, and textual information into a unified discrete space. This innovative approach enables joint training of the LLM exclusively through the Next-Token Prediction paradigm, facilitating multimodal protein reasoning and generation. ProtTeX enables general LLMs to perceive and process protein structures through sequential text input, leverage structural information as intermediate reasoning components, and generate or manipulate structures via sequential text output. Experiments demonstrate that our model achieves significant improvements in protein function prediction, outperforming the state-of-the-art domain expert model with a twofold increase in accuracy. Our framework enables high-quality conformational generation and customizable protein design. For the first time, we demonstrate that by adopting the standard training and inference pipelines from the LLM domain, ProtTeX empowers decoder-only LLMs to effectively address diverse spectrum of protein-related tasks. </p>
<blockquote>
<p>åœ¨åˆ†å­ç§‘å­¦é¢†åŸŸï¼Œå¤§å‹è¯­è¨€æ¨¡å‹å–å¾—äº†å¼•äººæ³¨ç›®çš„è¿›å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨ç†è§£å’Œç”ŸæˆåŠŸèƒ½æ€§å°åˆ†å­æ–¹é¢ã€‚è¿™ä¸€æˆåŠŸåœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå½’åŠŸäºåˆ†å­æ ‡è®°ç­–ç•¥çš„æ•ˆç”¨ã€‚åœ¨è›‹ç™½è´¨ç§‘å­¦é¢†åŸŸï¼Œæ°¨åŸºé…¸åºåˆ—è¢«è§†ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹çš„å”¯ä¸€æ ‡è®°å™¨ã€‚ç„¶è€Œï¼Œè›‹ç™½è´¨ç§‘å­¦ä¸­çš„è®¸å¤šåŸºæœ¬æŒ‘æˆ˜æœ¬è´¨ä¸Šæ˜¯ç»“æ„ä¾èµ–çš„ã€‚ç¼ºä¹ç»“æ„æ„ŸçŸ¥æ ‡è®°æ˜¾è‘—é™åˆ¶äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å…¨é¢ç”Ÿç‰©åˆ†å­ç†è§£å’Œå¤šæ¨¡å¼ç”Ÿæˆæ–¹é¢çš„èƒ½åŠ›ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹æ¡†æ¶ProtTeXï¼Œå®ƒå°†è›‹ç™½è´¨åºåˆ—ã€ç»“æ„å’Œæ–‡æœ¬ä¿¡æ¯æ ‡è®°ä¸ºç»Ÿä¸€çš„ç¦»æ•£ç©ºé—´ã€‚è¿™ç§åˆ›æ–°çš„æ–¹æ³•é€šè¿‡ä»…é€šè¿‡ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹èŒƒå¼æ¥è”åˆè®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä¿ƒè¿›å¤šæ¨¡å¼è›‹ç™½è´¨æ¨ç†å’Œç”Ÿæˆã€‚ProtTeXä½¿é€šç”¨çš„å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿé€šè¿‡é¡ºåºæ–‡æœ¬è¾“å…¥æ„ŸçŸ¥å’Œå¤„ç†è›‹ç™½è´¨ç»“æ„ï¼Œåˆ©ç”¨ç»“æ„ä¿¡æ¯ä½œä¸ºä¸­é—´æ¨ç†æˆåˆ†ï¼Œå¹¶é€šè¿‡é¡ºåºæ–‡æœ¬è¾“å‡ºç”Ÿæˆæˆ–æ“ä½œç»“æ„ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨è›‹ç™½è´¨åŠŸèƒ½é¢„æµ‹æ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œä¼˜äºæœ€å…ˆè¿›çš„é¢†åŸŸä¸“å®¶æ¨¡å‹ï¼Œå‡†ç¡®æ€§æé«˜äº†ä¸¤å€ã€‚æˆ‘ä»¬çš„æ¡†æ¶èƒ½å¤Ÿå®ç°é«˜è´¨é‡çš„ç»“æ„ç”Ÿæˆå’Œå¯å®šåˆ¶çš„è›‹ç™½è´¨è®¾è®¡ã€‚é¦–æ¬¡å±•ç¤ºäº†é€šè¿‡é‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹é¢†åŸŸçš„æ ‡å‡†è®­ç»ƒå’Œæ¨ç†ç®¡é“ï¼ŒProtTeXèµ‹èƒ½ä»…è§£ç çš„å¤§å‹è¯­è¨€æ¨¡å‹æœ‰æ•ˆè§£å†³å„ç§è›‹ç™½è´¨ç›¸å…³ä»»åŠ¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08179v2">PDF</a> 26 pages, 9 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åˆ†å­ç§‘å­¦é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨ç†è§£å’Œç”ŸæˆåŠŸèƒ½æ€§å°åˆ†å­æ–¹é¢ã€‚æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹æ¡†æ¶ProtTeXï¼Œå°†è›‹ç™½è´¨åºåˆ—ã€ç»“æ„å’Œæ–‡æœ¬ä¿¡æ¯ç»Ÿä¸€åˆ°ä¸€ä¸ªç¦»æ•£ç©ºé—´ä¸­ï¼Œå®ç°è›‹ç™½è´¨çš„å¤šæ¨¡æ€æ¨ç†å’Œç”Ÿæˆã€‚è¯¥æ¡†æ¶é€šè¿‡åºåˆ—æ–‡æœ¬è¾“å…¥ä½¿é€šç”¨å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿæ„ŸçŸ¥å’Œå¤„ç†è›‹ç™½è´¨ç»“æ„ï¼Œåˆ©ç”¨ç»“æ„ä¿¡æ¯ä½œä¸ºä¸­é—´æ¨ç†æˆåˆ†ï¼Œå¹¶é€šè¿‡åºåˆ—æ–‡æœ¬è¾“å‡ºç”Ÿæˆæˆ–æ“ä½œç»“æ„ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨è›‹ç™½è´¨åŠŸèƒ½é¢„æµ‹æ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œå¹¶å®ç°äº†é«˜è´¨é‡çš„æ„è±¡ç”Ÿæˆå’Œå¯å®šåˆ¶çš„è›‹ç™½è´¨è®¾è®¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åˆ†å­ç§‘å­¦é¢†åŸŸå–å¾—æ˜¾è‘—è¿›å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨ç†è§£å’Œç”ŸæˆåŠŸèƒ½æ€§å°åˆ†å­æ–¹é¢ã€‚</li>
<li>è›‹ç™½è´¨ç§‘å­¦ä¸­çš„è®¸å¤šåŸºæœ¬æŒ‘æˆ˜æ˜¯å›ºæœ‰åœ°ç»“æ„ä¾èµ–çš„ã€‚</li>
<li>ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†è›‹ç™½è´¨ç»“æ„æ—¶å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>ProtTeXæ¡†æ¶é€šè¿‡ç»Ÿä¸€ç¦»æ•£ç©ºé—´å®ç°è›‹ç™½è´¨åºåˆ—ã€ç»“æ„å’Œæ–‡æœ¬ä¿¡æ¯çš„å¤„ç†ã€‚</li>
<li>ProtTeXæ¡†æ¶ä½¿é€šç”¨å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿæ„ŸçŸ¥å’Œå¤„ç†è›‹ç™½è´¨ç»“æ„ã€‚</li>
<li>è¯¥æ¡†æ¶åœ¨è›‹ç™½è´¨åŠŸèƒ½é¢„æµ‹æ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œå¹¶å®ç°äº†é«˜è´¨é‡çš„æ„è±¡ç”Ÿæˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08179">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4575725610bc20fed3e7d5981f7f2e20.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0ace856496cdea8a801c11ce8dc21254.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Investigating-the-Effectiveness-of-a-Socratic-Chain-of-Thoughts-Reasoning-Method-for-Task-Planning-in-Robotics-A-Case-Study"><a href="#Investigating-the-Effectiveness-of-a-Socratic-Chain-of-Thoughts-Reasoning-Method-for-Task-Planning-in-Robotics-A-Case-Study" class="headerlink" title="Investigating the Effectiveness of a Socratic Chain-of-Thoughts   Reasoning Method for Task Planning in Robotics, A Case Study"></a>Investigating the Effectiveness of a Socratic Chain-of-Thoughts   Reasoning Method for Task Planning in Robotics, A Case Study</h2><p><strong>Authors:Veronica Bot, Zheyuan Xu</strong></p>
<p>Large language models (LLMs) have demonstrated unprecedented capability in reasoning with natural language. Coupled with this development is the emergence of embodied AI in robotics. Despite showing promise for verbal and written reasoning tasks, it remains unknown whether LLMs are capable of navigating complex spatial tasks with physical actions in the real world. To this end, it is of interest to investigate applying LLMs to robotics in zero-shot learning scenarios, and in the absence of fine-tuning - a feat which could significantly improve human-robot interaction, alleviate compute cost, and eliminate low-level programming tasks associated with robot tasks.   To explore this question, we apply GPT-4(Omni) with a simulated Tiago robot in Webots engine for an object search task. We evaluate the effectiveness of three reasoning strategies based on Chain-of-Thought (CoT) sub-task list generation with the Socratic method (SocraCoT) (in order of increasing rigor): (1) Non-CoT&#x2F;Non-SocraCoT, (2) CoT only, and (3) SocraCoT. Performance was measured in terms of the proportion of tasks successfully completed and execution time (N &#x3D; 20). Our preliminary results show that when combined with chain-of-thought reasoning, the Socratic method can be used for code generation for robotic tasks that require spatial awareness. In extension of this finding, we propose EVINCE-LoC; a modified EVINCE method that could further enhance performance in highly complex and or dynamic testing scenarios. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€æ¨ç†æ–¹é¢è¡¨ç°å‡ºäº†å‰æ‰€æœªæœ‰çš„èƒ½åŠ›ã€‚ä¸æ­¤åŒæ—¶ï¼Œæœºå™¨äººé¢†åŸŸä¹Ÿå‡ºç°äº†åµŒå…¥å¼çš„AIã€‚å°½ç®¡LLMsåœ¨è¨€è¯­å’Œä¹¦é¢æ¨ç†ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†æˆ‘ä»¬ä»ä¸æ¸…æ¥šå®ƒä»¬æ˜¯å¦èƒ½åœ¨ç°å®ä¸–ç•Œä¸­æ‰§è¡Œå¤æ‚çš„ç©ºé—´ä»»åŠ¡ã€‚å› æ­¤ï¼Œå°†LLMsåº”ç”¨äºæœºå™¨äººçš„é›¶æ ·æœ¬å­¦ä¹ åœºæ™¯ååˆ†ä»¤äººæ„Ÿå…´è¶£ï¼Œå¹¶ä¸”ä¸éœ€è¦ç²¾ç»†è°ƒæ•´ï¼Œè¿™ä¸€ç‰¹æ€§èƒ½å¤Ÿæå¤§åœ°æ”¹å–„äººæœºäº¤äº’ã€é™ä½è®¡ç®—æˆæœ¬ï¼Œå¹¶æ¶ˆé™¤ä¸æœºå™¨äººä»»åŠ¡ç›¸å…³çš„ä½çº§ç¼–ç¨‹ä»»åŠ¡ã€‚ä¸ºäº†æ¢ç©¶è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å°†GPT-4ï¼ˆOmniï¼‰ä¸Webotså¼•æ“ä¸­çš„æ¨¡æ‹ŸTiagoæœºå™¨äººç›¸ç»“åˆï¼Œè¿›è¡Œç‰©ä½“æœç´¢ä»»åŠ¡ã€‚æˆ‘ä»¬è¯„ä¼°äº†ä¸‰ç§åŸºäºæ€ç»´é“¾ï¼ˆCoTï¼‰çš„å­ä»»åŠ¡åˆ—è¡¨ç”Ÿæˆç­–ç•¥çš„æœ‰æ•ˆæ€§ï¼Œè¿™ä¸‰ç§ç­–ç•¥æŒ‰ä¸¥è°¨æ€§é€’å¢çš„é¡ºåºæ’åˆ—ï¼šï¼ˆ1ï¼‰éCoT&#x2F;éè‹æ ¼æ‹‰åº•æ–¹æ³•ï¼ˆSocraCoTï¼‰ï¼Œï¼ˆ2ï¼‰ä»…CoTï¼Œï¼ˆ3ï¼‰SocraCoTã€‚æˆ‘ä»¬çš„ç»©æ•ˆè¡¡é‡æ ‡å‡†æ˜¯ä»»åŠ¡å®Œæˆçš„æ¯”ä¾‹å’Œæ‰§è¡Œæ—¶é—´ï¼ˆN&#x3D;20ï¼‰ã€‚åˆæ­¥ç»“æœè¡¨æ˜ï¼Œç»“åˆæ€ç»´é“¾æ¨ç†ï¼Œè‹æ ¼æ‹‰åº•æ–¹æ³•å¯ç”¨äºç”Ÿæˆæœºå™¨äººä»»åŠ¡çš„ä»£ç ï¼Œè¿™äº›ä»»åŠ¡éœ€è¦ç©ºé—´æ„ŸçŸ¥èƒ½åŠ›ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†EVINCE-LoCï¼›ä¸€ç§æ”¹è¿›çš„EVINCEæ–¹æ³•ï¼Œå¯èƒ½è¿›ä¸€æ­¥æé«˜åœ¨é«˜åº¦å¤æ‚æˆ–åŠ¨æ€æµ‹è¯•åœºæ™¯ä¸­çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08174v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç»“åˆæœºå™¨äººå®ä½“AIå±•ç°å‡ºè‡ªç„¶è¯­è¨€çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå¯¹äºæ˜¯å¦èƒ½åœ¨æ— å¾®è°ƒæƒ…å†µä¸‹ï¼Œå®ç°æœºå™¨äººé›¶å°„å‡»å­¦ä¹ åœºæ™¯ä¸­çš„å¤æ‚ç©ºé—´ä»»åŠ¡ï¼Œä»ç„¶æœªçŸ¥ã€‚ç ”ç©¶é€šè¿‡GPT-4ï¼ˆOmniï¼‰ä¸Webotså¼•æ“ä¸­çš„æ¨¡æ‹ŸTiagoæœºå™¨äººè¿›è¡Œå¯¹è±¡æœç´¢ä»»åŠ¡æ¥æ¢ç´¢è¿™ä¸ªé—®é¢˜ã€‚åˆæ­¥ç»“æœè¡¨æ˜ï¼Œç»“åˆæ€ç»´é“¾æ¨ç†ï¼Œè‹æ ¼æ‹‰åº•æ–¹æ³•å¯ç”¨äºç”Ÿæˆæœºå™¨äººä»»åŠ¡çš„ä»£ç ï¼Œè¿™äº›ä»»åŠ¡éœ€è¦ç©ºé—´æ„ŸçŸ¥ã€‚æ®æ­¤ï¼Œç ”ç©¶æå‡ºäº†EVINCE-LoCæ–¹æ³•ï¼Œå¯è¿›ä¸€æ­¥æå‡å¤æ‚å’ŒåŠ¨æ€æµ‹è¯•åœºæ™¯ä¸­çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ç»“åˆæœºå™¨äººå®ä½“AIå…·æœ‰è‡ªç„¶è¯­è¨€çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ç›®å‰å¯¹äºLLMåœ¨é›¶å°„å‡»å­¦ä¹ åœºæ™¯ä¸­å®Œæˆå¤æ‚ç©ºé—´ä»»åŠ¡çš„èƒ½åŠ›ä»ä¸æ¸…æ¥šã€‚</li>
<li>ç ”ç©¶é€šè¿‡GPT-4ï¼ˆOmniï¼‰åœ¨Webotså¼•æ“ä¸­çš„æ¨¡æ‹ŸTiagoæœºå™¨äººè¿›è¡Œå¯¹è±¡æœç´¢ä»»åŠ¡ä»¥æ¢ç´¢æ­¤é—®é¢˜ã€‚</li>
<li>åˆæ­¥ç»“æœè¡¨æ˜ï¼Œç»“åˆæ€ç»´é“¾æ¨ç†ï¼Œè‹æ ¼æ‹‰åº•æ–¹æ³•å¯ç”¨äºç”Ÿæˆéœ€è¦ç©ºé—´æ„ŸçŸ¥çš„æœºå™¨äººä»»åŠ¡ä»£ç ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†EVINCE-LoCæ–¹æ³•ï¼Œæ—¨åœ¨æé«˜åœ¨å¤æ‚å’ŒåŠ¨æ€æµ‹è¯•åœºæ™¯ä¸­çš„æœºå™¨äººä»»åŠ¡æ€§èƒ½ã€‚</li>
<li>æ€§èƒ½è¯„ä¼°æ˜¯é€šè¿‡ä»»åŠ¡å®Œæˆæ¯”ä¾‹å’Œæ‰§è¡Œæ—¶é—´ï¼ˆN&#x3D;20ï¼‰æ¥è¿›è¡Œçš„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08174">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-97ffb31d0e0184ed7acbadd62f8b4dd5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-718947579dab2743a4fc9004d3e94ea3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d435ed7ca1c36725b8fded997977b9fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6db11277dfe9428967f65e0505932b47.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2597d07a035813363c7a17818428401b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dac85a0e8aac3483083c7d71439433ab.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="LLM4MAC-An-LLM-Driven-Reinforcement-Learning-Framework-for-MAC-Protocol-Emergence"><a href="#LLM4MAC-An-LLM-Driven-Reinforcement-Learning-Framework-for-MAC-Protocol-Emergence" class="headerlink" title="LLM4MAC: An LLM-Driven Reinforcement Learning Framework for MAC Protocol   Emergence"></a>LLM4MAC: An LLM-Driven Reinforcement Learning Framework for MAC Protocol   Emergence</h2><p><strong>Authors:Renxuan Tan, Rongpeng Li, Zhifeng Zhao</strong></p>
<p>With the advent of 6G systems, emerging hyper-connected ecosystems necessitate agile and adaptive medium access control (MAC) protocols to contend with network dynamics and diverse service requirements. We propose LLM4MAC, a novel framework that harnesses large language models (LLMs) within a reinforcement learning paradigm to drive MAC protocol emergence. By reformulating uplink data transmission scheduling as a semantics-generalized partially observable Markov game (POMG), LLM4MAC encodes network operations in natural language, while proximal policy optimization (PPO) ensures continuous alignment with the evolving network dynamics. A structured identity embedding (SIE) mechanism further enables robust coordination among heterogeneous agents. Extensive simulations demonstrate that on top of a compact LLM, which is purposefully selected to balance performance with resource efficiency, the protocol emerging from LLM4MAC outperforms comparative baselines in throughput and generalization. </p>
<blockquote>
<p>éšç€6Gç³»ç»Ÿçš„å‡ºç°ï¼Œæ–°å…´çš„è¶…äº’è”ç”Ÿæ€ç³»ç»Ÿéœ€è¦çµæ´»å’Œè‡ªé€‚åº”çš„ä»‹è´¨è®¿é—®æ§åˆ¶ï¼ˆMACï¼‰åè®®æ¥åº”å¯¹ç½‘ç»œåŠ¨æ€å˜åŒ–å’Œå„ç§æœåŠ¡éœ€æ±‚ã€‚æˆ‘ä»¬æå‡ºäº†LLM4MACè¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨å¼ºåŒ–å­¦ä¹ èŒƒå¼ä¸­çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥é©±åŠ¨MACåè®®çš„ç”Ÿæˆã€‚æˆ‘ä»¬å°†ä¸Šè¡Œæ•°æ®ä¼ è¾“è°ƒåº¦é‡æ–°æ„å»ºä¸ºè¯­ä¹‰æ³›åŒ–çš„éƒ¨åˆ†å¯è§‚å¯Ÿé©¬å°”å¯å¤«æ¸¸æˆï¼ˆPOMGï¼‰ï¼ŒLLM4MACç”¨è‡ªç„¶è¯­è¨€ç¼–ç ç½‘ç»œæ“ä½œï¼Œè€Œè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰ç¡®ä¿ä¸ä¸æ–­å˜åŒ–çš„ç½‘ç»œåŠ¨æ€æŒç»­å¯¹é½ã€‚ç»“æ„åŒ–èº«ä»½åµŒå…¥ï¼ˆSIEï¼‰æœºåˆ¶è¿›ä¸€æ­¥å®ç°äº†å¼‚è´¨æ™ºèƒ½ä½“ä¹‹é—´çš„ç¨³å¥åè°ƒã€‚å¤§é‡æ¨¡æ‹Ÿè¡¨æ˜ï¼Œç»è¿‡ç²¾å¿ƒæŒ‘é€‰ä»¥å¹³è¡¡æ€§èƒ½å’Œèµ„æºæ•ˆç‡çš„å°å‹LLMï¼ŒLLM4MACåè®®åœ¨ååé‡å’Œæ³›åŒ–æ–¹é¢å‡ä¼˜äºå¯¹æ¯”åŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08123v1">PDF</a> 5 pages, 5 figures</p>
<p><strong>Summary</strong><br>    éšç€6Gç³»ç»Ÿçš„å‡ºç°ï¼Œéœ€è¦çµæ´»è‡ªé€‚åº”çš„åª’ä½“æ¥å…¥æ§åˆ¶ï¼ˆMACï¼‰åè®®åº”å¯¹ç½‘ç»œåŠ¨æ€å’Œå„ç§æœåŠ¡éœ€æ±‚ã€‚æå‡ºLLM4MACæ¡†æ¶ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¼ºåŒ–å­¦ä¹ èŒƒå¼ä¸­é©±åŠ¨MACåè®®æ¶Œç°ã€‚é€šè¿‡å°†æœ‰æºæ•°æ®ä¸Šä¼ è°ƒåº¦é‡æ–°å®šä¹‰ä¸ºè¯­ä¹‰å¹¿ä¹‰éƒ¨åˆ†å¯è§‚å¯Ÿé©¬å°”å¯å¤«åšå¼ˆï¼ˆPOMGï¼‰ï¼ŒLLM4MACä»¥è‡ªç„¶è¯­è¨€ç¼–ç ç½‘ç»œæ“ä½œï¼ŒåŒæ—¶è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰ç¡®ä¿ä¸ä¸æ–­å˜åŒ–ç½‘ç»œåŠ¨æ€çš„æŒç»­å¯¹é½ã€‚ç»“æ„åŒ–èº«ä»½åµŒå…¥ï¼ˆSIEï¼‰æœºåˆ¶è¿›ä¸€æ­¥å®ç°ä¸åŒä»£ç†ä¹‹é—´çš„ç¨³å¥åè°ƒã€‚æ¨¡æ‹Ÿæ˜¾ç¤ºï¼Œåœ¨ç´§å‡‘å‹LLMä¸Šï¼ŒLLM4MACåè®®åœ¨ååé‡å’Œæ³›åŒ–èƒ½åŠ›æ–¹é¢è¶…è¶Šäº†å¯¹æ¯”åŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>6Gç³»ç»Ÿéœ€è¦çµæ´»è‡ªé€‚åº”çš„MACåè®®åº”å¯¹ç½‘ç»œåŠ¨æ€å’ŒæœåŠ¡éœ€æ±‚çš„å¤šæ ·æ€§ã€‚</li>
<li>LLM4MACæ¡†æ¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œå¼ºåŒ–å­¦ä¹ èŒƒå¼æ¥é©±åŠ¨MACåè®®çš„æ¶Œç°ã€‚</li>
<li>LLM4MACé€šè¿‡å°†æœ‰æºæ•°æ®ä¸Šä¼ è°ƒåº¦å®šä¹‰ä¸ºè¯­ä¹‰å¹¿ä¹‰POMGæ¥å¤„ç†ç½‘ç»œæ“ä½œã€‚</li>
<li>PPOç¡®ä¿ä¸ä¸æ–­å˜åŒ–ç½‘ç»œåŠ¨æ€çš„æŒç»­å¯¹é½ã€‚</li>
<li>ç»“æ„åŒ–èº«ä»½åµŒå…¥ï¼ˆSIEï¼‰æœºåˆ¶ç”¨äºä¸åŒä»£ç†ä¹‹é—´çš„ç¨³å¥åè°ƒã€‚</li>
<li>LLM4MACåœ¨ç´§å‡‘å‹LLMçš„åŸºç¡€ä¸Šï¼Œé€šè¿‡æ¨¡æ‹Ÿæ˜¾ç¤ºå‡ºå…¶åœ¨ååé‡å’Œæ³›åŒ–èƒ½åŠ›æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚</li>
<li>LLM4MACåè®®å…·å¤‡é«˜æ•ˆèµ„æºåˆ©ç”¨çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08123">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0de38b30b30c4286cac96114effd9fb7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-48a01b78c0bdff0350a3b306276a80e3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c4b76f616ce08dbc243baf0d2e37d033.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-681e43154ecd88ff2d62b5b4af281771.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-14/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-14/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-14/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-2443f5f69ced566e7b2344260a6fae41.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-14  SimLingo Vision-Only Closed-Loop Autonomous Driving with   Language-Action Alignment
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-14
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-12/Text-to-Motion/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-347e39bfdd54a24c4a56ca19758750c7.jpg" class="responsive-img" alt="Text-to-Motion">
                        
                        <span class="card-title">Text-to-Motion</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Text-to-Motion æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-12  PersonaBooth Personalized Text-to-Motion Generation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-12
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Text-to-Motion/" class="post-category">
                                    Text-to-Motion
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Text-to-Motion/">
                        <span class="chip bg-color">Text-to-Motion</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">13632.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
