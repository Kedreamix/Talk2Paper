<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="医学图像">
    <meta name="description" content="医学图像 方向最新论文已更新，请持续关注 Update in 2025-03-05  MobileViM A Light-weight and Dimension-independent Vision Mamba for 3D   Medical Image Analysis">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>医学图像 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-ce4cf8702947ea6ba2c02dc49aa37123.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">医学图像</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">医学图像</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                医学图像
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-03-05
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    10.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    42 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-03-05-更新"><a href="#2025-03-05-更新" class="headerlink" title="2025-03-05 更新"></a>2025-03-05 更新</h1><h2 id="MobileViM-A-Light-weight-and-Dimension-independent-Vision-Mamba-for-3D-Medical-Image-Analysis"><a href="#MobileViM-A-Light-weight-and-Dimension-independent-Vision-Mamba-for-3D-Medical-Image-Analysis" class="headerlink" title="MobileViM: A Light-weight and Dimension-independent Vision Mamba for 3D   Medical Image Analysis"></a>MobileViM: A Light-weight and Dimension-independent Vision Mamba for 3D   Medical Image Analysis</h2><p><strong>Authors:Wei Dai, Steven Wang, Jun Liu</strong></p>
<p>Efficient evaluation of three-dimensional (3D) medical images is crucial for diagnostic and therapeutic practices in healthcare. Recent years have seen a substantial uptake in applying deep learning and computer vision to analyse and interpret medical images. Traditional approaches, such as convolutional neural networks (CNNs) and vision transformers (ViTs), face significant computational challenges, prompting the need for architectural advancements. Recent efforts have led to the introduction of novel architectures like the &#96;&#96;Mamba’’ model as alternative solutions to traditional CNNs or ViTs. The Mamba model excels in the linear processing of one-dimensional data with low computational demands. However, Mamba’s potential for 3D medical image analysis remains underexplored and could face significant computational challenges as the dimension increases. This manuscript presents MobileViM, a streamlined architecture for efficient segmentation of 3D medical images. In the MobileViM network, we invent a new dimension-independent mechanism and a dual-direction traversing approach to incorporate with a vision-Mamba-based framework. MobileViM also features a cross-scale bridging technique to improve efficiency and accuracy across various medical imaging modalities. With these enhancements, MobileViM achieves segmentation speeds exceeding 90 frames per second (FPS) on a single graphics processing unit (i.e., NVIDIA RTX 4090). This performance is over 24 FPS faster than the state-of-the-art deep learning models for processing 3D images with the same computational resources. In addition, experimental evaluations demonstrate that MobileViM delivers superior performance, with Dice similarity scores reaching 92.72%, 86.69%, 80.46%, and 77.43% for PENGWIN, BraTS2024, ATLAS, and Toothfairy2 datasets, respectively, which significantly surpasses existing models. </p>
<blockquote>
<p>在医疗保健领域，对三维（3D）医学图像的有效评估对于诊断和治疗实践至关重要。近年来，深度学习和计算机视觉在医学图像分析和解释方面的应用显著增加。传统方法，如卷积神经网络（CNNs）和视觉变压器（ViTs），面临着重大计算挑战，促使需要进行架构改进。近期的研究工作引入了新型架构，如“Mamba”模型，作为传统CNNs或ViTs的替代解决方案。“Mamba”模型在处理一维数据的线性处理方面表现出色，计算需求较低。然而，Mamba在3D医学图像分析方面的潜力尚未得到充分探索，随着维度的增加，可能会面临重大的计算挑战。本文提出了MobileViM，这是一个用于高效分割3D医学图像的简化架构。在MobileViM网络中，我们发明了一种新的维度独立机制和一种双向遍历方法，将其纳入基于视觉Mamba的框架中。MobileViM还采用跨尺度桥梁技术，以提高不同医学成像模式的效率和准确性。通过这些增强功能，MobileViM在单个图形处理单元（例如NVIDIA RTX 4090）上实现了超过每秒90帧（FPS）的分割速度。此性能比使用相同计算资源的最新深度学习模型处理3D图像的速度快24 FPS以上。此外，实验评估表明，MobileViM的性能卓越，在PENGWIN、BraTS2024、ATLAS和Toothfairy2数据集上的Dice相似度得分分别达到92.72%、86.69%、80.46%和77.43%，显著超越了现有模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13524v2">PDF</a> The co-authors have not approved its submission to arXiv</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种针对三维医学图像高效分割的模型MobileViM。该模型引入了一种新的维度独立机制和双向遍历方法，与基于视觉的Mamba框架相结合。MobileViM还采用跨尺度桥接技术，以提高不同医学成像模态的效率和准确性。相较于其他先进深度学习模型，MobileViM在处理三维图像时速度更快，性能更优。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>三维医学图像评估在医疗诊断和治疗实践中至关重要。</li>
<li>深度学习在计算机视觉在医学图像分析和解释中得到广泛应用。</li>
<li>传统方法如卷积神经网络（CNNs）和视觉转换器（ViTs）面临计算挑战，需要架构改进。</li>
<li>Mamba模型在低计算需求的一维数据处理中表现出色，但在三维医学图像分析方面的潜力尚未得到充分探索。</li>
<li>MobileViM是一个针对三维医学图像高效分割的流线型架构。</li>
<li>MobileViM采用新的维度独立机制和双向遍历方法与视觉Mamba框架相结合，提高了效率和准确性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13524">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-478c4a069cb045964697c54dfba709cc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce4cf8702947ea6ba2c02dc49aa37123.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e42796ec18c7321bee4b45d36a1c3825.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9cd6b470a040419562bc91eea89ac5f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a5f77b8f330ea05bdd1df377c514ad9.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="SCC-YOLO-An-Improved-Object-Detector-for-Assisting-in-Brain-Tumor-Diagnosis"><a href="#SCC-YOLO-An-Improved-Object-Detector-for-Assisting-in-Brain-Tumor-Diagnosis" class="headerlink" title="SCC-YOLO: An Improved Object Detector for Assisting in Brain Tumor   Diagnosis"></a>SCC-YOLO: An Improved Object Detector for Assisting in Brain Tumor   Diagnosis</h2><p><strong>Authors:Runci Bai, Guibao Xu, Yanze Shi</strong></p>
<p>Brain tumors can lead to neurological dysfunction, cognitive and psychological changes, increased intracranial pressure, and seizures, posing significant risks to health. The You Only Look Once (YOLO) series has shown superior accuracy in medical imaging object detection. This paper presents a novel SCC-YOLO architecture that integrates the SCConv module into YOLOv9. The SCConv module optimizes convolutional efficiency by reducing spatial and channel redundancy, enhancing image feature learning. We examine the effects of different attention mechanisms with YOLOv9 for brain tumor detection using the Br35H dataset and our custom dataset (Brain_Tumor_Dataset). Results indicate that SCC-YOLO improved mAP50 by 0.3% on the Br35H dataset and by 0.5% on our custom dataset compared to YOLOv9. SCC-YOLO achieves state-of-the-art performance in brain tumor detection. </p>
<blockquote>
<p>脑肿瘤可能导致神经功能障碍、认知和心理学变化、颅内压升高和癫痫发作，对健康构成重大风险。You Only Look Once（YOLO）系列已在医学影像物体检测中展现出卓越的准确性。本文提出了一种新型的SCC-YOLO架构，它将SCConv模块整合到YOLOv9中。SCConv模块通过减少空间冗余和通道冗余来优化卷积效率，增强图像特征学习。我们使用Br35H数据集和自定义数据集（Brain_Tumor_Dataset）来检验不同注意力机制在YOLOv9检测脑肿瘤方面的效果。结果表明，与YOLOv9相比，SCC-YOLO在Br35H数据集上的mAP50提高了0.3%，在我们的自定义数据集上提高了0.5%。SCC-YOLO在脑肿瘤检测方面达到了最先进的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.03836v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种新型的SCC-YOLO架构，它将SCConv模块集成到YOLOv9中以提高医学图像中目标检测的准确性。实验结果显示，SCC-YOLO在Br35H数据集和自定义数据集上的表现均优于YOLOv9，特别是在脑肿瘤检测方面取得了最先进的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>脑肿瘤可能导致神经功能障碍、认知和心理变化、颅内压增加和癫痫发作，对健康构成重大风险。</li>
<li>YOLO系列在医学成像目标检测中展现了较高的准确性。</li>
<li>本文提出了一种新型的SCC-YOLO架构，集成了SCConv模块以优化卷积效率，增强图像特征学习。</li>
<li>SCC-YOLO在Br35H数据集和自定义的Brain_Tumor_Dataset数据集上进行了实验验证。</li>
<li>实验结果显示，与YOLOv9相比，SCC-YOLO在mAP50指标上有所提升，其中在自定义数据集上的提升更为明显。</li>
<li>SCC-YOLO在脑肿瘤检测方面取得了最先进的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.03836">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8e57a99cb01845266804066bfb55d8d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-40439e09d849d6c7d2da475668a69610.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-716e93a001815ef004af7c8c02d3454f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0bef9c3287cc19861a65f2cd40a73886.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0951837b4439609123821811c2093d2f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f9221bc3cd5a48c4a20bf66b756658c.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="CT-Mamba-A-Hybrid-Convolutional-State-Space-Model-for-Low-Dose-CT-Denoising"><a href="#CT-Mamba-A-Hybrid-Convolutional-State-Space-Model-for-Low-Dose-CT-Denoising" class="headerlink" title="CT-Mamba: A Hybrid Convolutional State Space Model for Low-Dose CT   Denoising"></a>CT-Mamba: A Hybrid Convolutional State Space Model for Low-Dose CT   Denoising</h2><p><strong>Authors:Linxuan Li, Wenjia Wei, Luyao Yang, Wenwen Zhang, Jiashu Dong, Yahua Liu, Hongshi Huang, Wei Zhao</strong></p>
<p>Low-dose CT (LDCT) significantly reduces the radiation dose received by patients, however, dose reduction introduces additional noise and artifacts. Currently, denoising methods based on convolutional neural networks (CNNs) face limitations in long-range modeling capabilities, while Transformer-based denoising methods, although capable of powerful long-range modeling, suffer from high computational complexity. Furthermore, the denoised images predicted by deep learning-based techniques inevitably exhibit differences in noise distribution compared to normal-dose CT (NDCT) images, which can also impact the final image quality and diagnostic outcomes. This paper proposes CT-Mamba, a hybrid convolutional State Space Model for LDCT image denoising. The model combines the local feature extraction advantages of CNNs with Mamba’s strength in capturing long-range dependencies, enabling it to capture both local details and global context. Additionally, we introduce an innovative spatially coherent ‘Z’-shaped scanning scheme to ensure spatial continuity between adjacent pixels in the image. We design a Mamba-driven deep noise power spectrum (NPS) loss function to guide model training, ensuring that the noise texture of the denoised LDCT images closely resembles that of NDCT images, thereby enhancing overall image quality and diagnostic value. Experimental results have demonstrated that CT-Mamba performs excellently in reducing noise in LDCT images, enhancing detail preservation, and optimizing noise texture distribution, and exhibits higher statistical similarity with the radiomics features of NDCT images. The proposed CT-Mamba demonstrates outstanding performance in LDCT denoising and holds promise as a representative approach for applying the Mamba framework to LDCT denoising tasks. Our code will be made available after the paper is officially published: <a target="_blank" rel="noopener" href="https://github.com/zy2219105/CT-Mamba/">https://github.com/zy2219105/CT-Mamba/</a>. </p>
<blockquote>
<p>低剂量CT（LDCT）显著减少了患者接受的辐射剂量，然而，剂量的减少会引入额外的噪声和伪影。目前，基于卷积神经网络（CNNs）的降噪方法在长期建模能力方面存在局限性，而基于Transformer的降噪方法虽然具有强大的长期建模能力，但计算复杂度较高。此外，基于深度学习技术预测的降噪图像与正常剂量CT（NDCT）图像相比，噪声分布不可避免地存在差异，这也可能影响最终的图像质量和诊断结果。本文提出了CT-Mamba，一种用于LDCT图像降噪的混合卷积状态空间模型。该模型结合了CNNs提取局部特征的优势和Mamba在捕捉长期依赖关系方面的实力，使其能够捕捉局部细节和全局上下文。此外，我们引入了一种创新的空间连贯的“Z”形扫描方案，以确保图像中相邻像素之间的空间连续性。我们设计了一种以Mamba驱动的深度噪声功率谱（NPS）损失函数来指导模型训练，确保降噪LDCT图像的噪声纹理与NDCT图像相似，从而提高整体图像质量和诊断价值。实验结果表明，CT-Mamba在降低LDCT图像噪声、增强细节保留和优化噪声纹理分布方面表现出色，与NDCT图像的放射学特征具有更高的统计相似性。所提出的CT-Mamba在LDCT降噪方面表现出卓越的性能，并有望作为将Mamba框架应用于LDCT降噪任务的一种代表性方法。我们的代码将在论文正式发表后提供：<a target="_blank" rel="noopener" href="https://github.com/zy2219105/CT-Mamba/">https://github.com/zy2219105/CT-Mamba/</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.07930v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于卷积神经网络与Mamba模型的混合CT图像去噪方法能有效减少低剂量CT（LDCT）图像中的噪声与伪影，同时兼顾局部细节与全局上下文信息。新方法结合了CNN的局部特征提取优势与Mamba模型的长程依赖性捕捉能力，并提出了一种新颖的“Z”字形扫描方案，以确保图像中相邻像素的空间连续性。此外，通过设计基于Mamba的深度噪声功率谱（NPS）损失函数，训练模型以生成与常规剂量CT（NDCT）图像噪声纹理相似的去噪LDCT图像，从而提高整体图像质量和诊断价值。实验结果显示，该新方法在降低LDCT图像噪声、增强细节保留以及优化噪声纹理分布方面表现出卓越性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>低剂量CT（LDCT）中使用了新型的图像去噪方法以减轻噪声和伪影问题。</li>
<li>该方法结合了卷积神经网络（CNN）与Mamba模型的优势，提高了长程建模能力同时降低了计算复杂性。</li>
<li>创新性地采用“Z”字形扫描方案确保图像的空间连续性。</li>
<li>引入了深度噪声功率谱（NPS）损失函数，确保去噪后的LDCT图像噪声纹理接近常规剂量CT（NDCT）图像。</li>
<li>实验结果显示该方法在降低噪声、保留细节和优化噪声纹理方面效果显著。</li>
<li>该方法有望代表将Mamba框架应用于LDCT去噪任务的一种典型方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.07930">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f82343b6f9b62451565796de39d0aec6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4464e404c3ac60db594159995ef9f4f8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8c1b0fc322a82994bf2bac29ae717243.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a17ec349251df4219a4379ade78dd45.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dacff271adf6830efa2c8de5c4cbe6a6.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Learning-General-Purpose-Biomedical-Volume-Representations-using-Randomized-Synthesis"><a href="#Learning-General-Purpose-Biomedical-Volume-Representations-using-Randomized-Synthesis" class="headerlink" title="Learning General-Purpose Biomedical Volume Representations using   Randomized Synthesis"></a>Learning General-Purpose Biomedical Volume Representations using   Randomized Synthesis</h2><p><strong>Authors:Neel Dey, Benjamin Billot, Hallee E. Wong, Clinton J. Wang, Mengwei Ren, P. Ellen Grant, Adrian V. Dalca, Polina Golland</strong></p>
<p>Current volumetric biomedical foundation models struggle to generalize as public 3D datasets are small and do not cover the broad diversity of medical procedures, conditions, anatomical regions, and imaging protocols. We address this by creating a representation learning method that instead anticipates strong domain shifts at training time itself. We first propose a data engine that synthesizes highly variable training samples that would enable generalization to new biomedical contexts. To then train a single 3D network for any voxel-level task, we develop a contrastive learning method that pretrains the network to be stable against nuisance imaging variation simulated by the data engine, a key inductive bias for generalization. This network’s features can be used as robust representations of input images for downstream tasks and its weights provide a strong, dataset-agnostic initialization for finetuning on new datasets. As a result, we set new standards across both multimodality registration and few-shot segmentation, a first for any 3D biomedical vision model, all without (pre-)training on any existing dataset of real images. </p>
<blockquote>
<p>当前的三维生物医学基础模型在泛化方面存在困难，因为公共的3D数据集规模较小，并不能覆盖广泛的医疗程序、状况、解剖区域和成像协议。我们通过创建一种表示学习方法来解决这个问题，该方法能够在训练时本身就能预测到强烈的领域变化。我们首先提出了一个数据引擎，该引擎能够合成高度可变的训练样本，以实现向新生物医学环境的泛化。为了针对任何体素级任务训练单个的3D网络，我们开发了一种对比学习方法，该方法能够训练网络以对抗由数据引擎模拟的干扰成像变化，这是泛化的关键归纳偏置。该网络的特征可以作为下游任务的稳健图像表示，其权重为在新数据集上进行微调提供了强大且独立于数据集之外的初始化。因此，我们在多模态注册和少样本分割方面设定了新的标准，这是首次有3D生物医学视觉模型能做到这一点，并且全程未使用任何现有真实图像数据集进行（预）训练。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.02372v2">PDF</a> ICLR 2025: International Conference on Learning Representations. Code   and model weights available at <a target="_blank" rel="noopener" href="https://github.com/neel-dey/anatomix">https://github.com/neel-dey/anatomix</a>.   Keywords: synthetic data, representation learning, medical image analysis,   image registration, image segmentation</p>
<p><strong>Summary</strong></p>
<p>该文提出一种解决生物医学图像领域模型泛化能力弱的问题的方法。针对公共3D数据集规模小、涵盖医疗程序、状况、解剖区域和成像协议多样性不足的问题，研究团队创建了一种表征学习方法，该方法能够在训练时自身预测强领域漂移。他们首先提出一个数据引擎，合成高度可变的训练样本，以实现对新生物医学环境的泛化。接着，为了对任何体素级任务进行训练，他们开发了一种对比学习方法，该方法对由数据引擎模拟的干扰成像变化进行预训练，形成对泛化的关键归纳偏置。该网络的特征可用于下游任务的稳健表示，其权重为在新数据集上进行微调提供了强大的、独立于数据集初始化。最终，该研究在多模态注册和少样本分割任务上设定了新的标准，成为首个无需在真实图像现有数据集上进行（预）训练的3D生物医学视觉模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前生物医学图像领域的模型泛化能力受限，主要由于公共数据集规模小且多样性不足。</li>
<li>研究团队提出一种表征学习方法，能在训练时预测强领域漂移。</li>
<li>通过数据引擎合成高度可变的训练样本，以提高模型对新生物医学环境的泛化能力。</li>
<li>采用对比学习方法进行预训练，增强模型对干扰成像变化的稳定性。</li>
<li>该网络特征适用于下游任务，并提供稳健的图像表示。</li>
<li>模型权重为在新数据集上的微调提供了强大的初始化，且独立于数据集。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.02372">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c8ccbfddae24e54ee188d2aaa59b337c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8829a891f98535325355d33e316e5675.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c296a2b6e427afb423e81fadfd0733cc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eff7484428edeb2c68f2c070333526f4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-62bc1811ed772a16504ff90639918a42.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1f2a62ad11af4a292fdc90828afd524c.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Unleashing-the-Potential-of-Vision-Language-Pre-Training-for-3D-Zero-Shot-Lesion-Segmentation-via-Mask-Attribute-Alignment"><a href="#Unleashing-the-Potential-of-Vision-Language-Pre-Training-for-3D-Zero-Shot-Lesion-Segmentation-via-Mask-Attribute-Alignment" class="headerlink" title="Unleashing the Potential of Vision-Language Pre-Training for 3D   Zero-Shot Lesion Segmentation via Mask-Attribute Alignment"></a>Unleashing the Potential of Vision-Language Pre-Training for 3D   Zero-Shot Lesion Segmentation via Mask-Attribute Alignment</h2><p><strong>Authors:Yankai Jiang, Wenhui Lei, Xiaofan Zhang, Shaoting Zhang</strong></p>
<p>Recent advancements in medical vision-language pre-training models have driven significant progress in zero-shot disease recognition. However, transferring image-level knowledge to pixel-level tasks, such as lesion segmentation in 3D CT scans, remains a critical challenge. Due to the complexity and variability of pathological visual characteristics, existing methods struggle to align fine-grained lesion features not encountered during training with disease-related textual representations. In this paper, we present Malenia, a novel multi-scale lesion-level mask-attribute alignment framework, specifically designed for 3D zero-shot lesion segmentation. Malenia improves the compatibility between mask representations and their associated elemental attributes, explicitly linking the visual features of unseen lesions with the extensible knowledge learned from previously seen ones. Furthermore, we design a Cross-Modal Knowledge Injection module to enhance both visual and textual features with mutually beneficial information, effectively guiding the generation of segmentation results. Comprehensive experiments across three datasets and 12 lesion categories validate the superior performance of Malenia. </p>
<blockquote>
<p>近年来，医学视觉语言预训练模型的进步推动了零样本疾病识别的显著发展。然而，将图像级别的知识转移到像素级别的任务，如在3D CT扫描中的病灶分割，仍然是一个关键挑战。由于病理视觉特征的复杂性和可变性，现有方法很难将训练期间未遇到的精细病灶特征与疾病相关的文本表示进行对齐。在本文中，我们提出了Malenia，这是一种新型的多尺度病灶级别掩膜属性对齐框架，专为3D零样本病灶分割设计。Malenia提高了掩膜表示与其相关元素属性之间的兼容性，明确地将未见过的病灶的视觉特征与从已见过的病灶中学到的可扩展知识联系起来。此外，我们设计了一个跨模态知识注入模块，以增强视觉和文本特征的相互有益信息，有效指导生成分割结果。在三个数据集和12个病灶类别的综合实验验证了Malenia的卓越性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.15744v2">PDF</a> Accepted as ICLR 2025 conference paper</p>
<p><strong>Summary</strong><br>医学视觉语言预训练模型的最新进展推动了零样本疾病识别的显著进步。然而，将图像级别的知识转移到像素级别的任务，如3D CT扫描中的病灶分割，仍是一个关键挑战。本文提出一种名为Malenia的多尺度病灶级别掩膜属性对齐框架，专门用于3D零样本病灶分割。该框架提高了掩膜表示与其相关基础属性之间的兼容性，将未见病灶的视觉特征与从已见病灶中学到的可扩展知识明确联系起来。此外，设计了一种跨模态知识注入模块，以增强视觉和文本特征的相互补充信息，有效指导生成分割结果。在三个数据集和12个病灶类别的综合实验中验证了Malenia的卓越性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>医学视觉语言预训练模型的最新进展推动了零样本疾病识别的进步。</li>
<li>图像级别的知识转移到像素级别的任务（如病灶分割）仍然是一个挑战。</li>
<li>Malenia框架用于3D零样本病灶分割，实现多尺度病灶级别掩膜属性对齐。</li>
<li>Malenia提高了掩膜表示与基础属性之间的兼容性，关联未见与已见病灶特征。</li>
<li>跨模态知识注入模块增强视觉和文本特征的互补信息。</li>
<li>Malenia在多个数据集和不同病灶类别上表现出卓越性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.15744">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-a66de1f1deef3aa8acc368b5f35ed1c9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1263f7f248053b7fc9bc2f10276c5687.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-79eb0f09b6df26e444bcb1f047e947eb.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Improved-Baselines-with-Synchronized-Encoding-for-Universal-Medical-Image-Segmentation"><a href="#Improved-Baselines-with-Synchronized-Encoding-for-Universal-Medical-Image-Segmentation" class="headerlink" title="Improved Baselines with Synchronized Encoding for Universal Medical   Image Segmentation"></a>Improved Baselines with Synchronized Encoding for Universal Medical   Image Segmentation</h2><p><strong>Authors:Sihan Yang, Xuande Mi, Jiadong Feng, Haixia Bi, Hai Zhang, Jian Sun</strong></p>
<p>Large foundation models, known for their strong zero-shot generalization capabilities, can be applied to a wide range of downstream tasks. However, developing foundation models for medical image segmentation poses a significant challenge due to the domain gap between natural and medical images. While fine-tuning techniques based on the Segment Anything Model (SAM) have been explored, they primarily focus on scaling up data or refining inference strategies without incorporating domain-specific architectural designs, limiting their zero-shot performance. To optimize segmentation performance under standard inference settings and provide a strong baseline for future research, we introduce SyncSAM, which employs a synchronized dual-branch encoder that integrates convolution and Transformer features in a synchronized manner to enhance medical image encoding, and a multi-scale dual-branch decoder to preserve image details. SyncSAM is trained on two of the largest medical image segmentation datasets, SA-Med2D-20M and IMed-361M, resulting in a series of pre-trained models for universal medical image segmentation. Experimental results demonstrate that SyncSAM not only achieves state-of-the-art performance on test sets but also exhibits strong zero-shot capabilities on unseen datasets. The code and model weights are available at <a target="_blank" rel="noopener" href="https://github.com/Hhankyangg/SyncSAM">https://github.com/Hhankyangg/SyncSAM</a>. </p>
<blockquote>
<p>大型基础模型以其强大的零样本泛化能力而著称，可广泛应用于各种下游任务。然而，针对医学图像分割开发基础模型是一个巨大的挑战，因为自然图像和医学图像之间存在领域差距。虽然基于Segment Anything Model（SAM）的微调技术已被探索，但它们主要关注扩大数据规模或改进推理策略，而没有结合领域特定的架构设计，从而限制了其零样本性能。为了优化标准推理设置下的分割性能，并为未来的研究提供强大的基线，我们引入了SyncSAM。SyncSAM采用同步双分支编码器，以同步方式集成卷积和Transformer特征，增强医学图像编码；并采用多尺度双分支解码器，保留图像细节。SyncSAM在两大医学图像分割数据集SA-Med2D-20M和IMed-361M上进行训练，生成了一系列用于通用医学图像分割的预训练模型。实验结果表明，SyncSAM不仅在测试集上达到最新性能，而且在未见数据集上表现出强大的零样本能力。相关代码和模型权重可在<a target="_blank" rel="noopener" href="https://github.com/Hhankyangg/SyncSAM%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Hhankyangg/SyncSAM获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.09886v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型基础模型在医学图像分割中具有强大的零样本泛化能力，但面临自然图像与医学图像领域差异的挑战。SyncSAM模型通过同步双分支编码器和多尺度双分支解码器来增强医学图像的编码和细节保留。它在两个最大的医学图像分割数据集上进行训练，实现了预训练模型的通用医学图像分割。实验结果表明，SyncSAM在测试集上达到了最先进的性能，并在未见数据集上展现出强大的零样本能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型基础模型具备广泛的应用潜力，特别是在医学图像分割领域。</li>
<li>医学图像分割面临自然图像与医学图像领域差异的挑战。</li>
<li>SyncSAM模型通过同步双分支编码器集成卷积和Transformer特征，以提高医学图像编码效果。</li>
<li>SyncSAM采用多尺度双分支解码器，旨在保留图像细节。</li>
<li>SyncSAM模型在多个医学图像分割数据集上进行训练，提供预训练模型用于通用医学图像分割。</li>
<li>实验结果显示SyncSAM达到了最先进的性能，并在未见数据集上展现出强大的零样本能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.09886">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-df46be8b466facf8ba6a6d977ec6624d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b7edef4722fd0880755c51e3ce041f21.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-775c002212b3f8748cc976c4ab39c75c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-545bee0888f2e398ba83261f44f8d9aa.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Real-Time-Image-Analysis-Software-Suitable-for-Resource-Constrained-Computing"><a href="#Real-Time-Image-Analysis-Software-Suitable-for-Resource-Constrained-Computing" class="headerlink" title="Real-Time Image Analysis Software Suitable for Resource-Constrained   Computing"></a>Real-Time Image Analysis Software Suitable for Resource-Constrained   Computing</h2><p><strong>Authors:Alexandre Matov</strong></p>
<p>Methods: We have developed a software suite (DataSet Tracker) for real-time analysis designed to run on computers, smartphones, and smart glasses hardware and suitable for resource-constrained, on-the-fly computing in microscopes without internet connectivity; a demo is available for viewing at datasetanalysis.com. Our objective is to present the community with an integrated, easy to use by all, tool for resolving the complex dynamics of the cytoskeletal meshworks, intracytoplasmic membranous networks, and vesicle trafficking. Our software is optimized for resource-constrained computing and can be installed even on microscopes without internet connectivity.   Results: Our computational platform can provide high-content analyses and functional secondary screening of novel compounds that are in the process of approval, or at a pre-clinical stage of development, and putative combination therapies based on FDA-approved drugs. Importantly, dissecting the mechanisms of drug action with quantitative detail will allow the design of drugs that impede relapse and optimal dose regimens with minimal harmful side effects by carefully exploiting disease-specific aberrations.   Conclusions: DataSet Tracker, the real-time optical flow feature tracking software presented in this contribution, can serve as the base module of an integrated platform of existing and future algorithms for real-time cellular analysis. The computational assay we propose could successfully be applied to evaluate treatment strategies for any human organ. It is our goal to have this integrated tool approved for use in the clinical practice. </p>
<blockquote>
<p><strong>方法</strong>：我们开发了一套实时分析软件套件（DataSet Tracker），可在计算机、智能手机和智能眼镜等硬件上运行，适用于无网络连接显微镜下的资源受限、即时计算。可以在datasetanalysis.com上查看演示版。我们的目标是向研究群体提供一个集成工具，该工具易于所有人使用，可解决细胞骨架网格、胞质内膜网络和囊泡转运的复杂动态问题。我们的软件针对资源受限的计算进行了优化，甚至可以在没有互联网连接的显微镜上安装使用。</p>
</blockquote>
<p><strong>结果</strong>：我们的计算平台可以提供高内涵分析以及新药筛选的功能二次筛选，这些新药正处于审批过程中或处于开发预临床阶段，以及基于FDA批准药物的组合疗法。重要的是，通过定量细节分析药物作用机制，将能够设计出阻止复发的药物，并借助精准地利用疾病特异性异常来制定最佳治疗方案和最小有害副作用的剂量方案。</p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.15735v7">PDF</a> </p>
<p><strong>Summary</strong><br>本文介绍了一种用于实时分析的软件套件DataSet Tracker，适用于资源受限环境下无网络连接显微镜的在线计算。软件具有优化资源消耗的特点，旨在为社区提供一个综合工具，解决细胞骨架网格、细胞内膜网络和囊泡运输的复杂动态问题。该软件可应用于新药筛选和药物作用机制定量研究，有助于设计防止复发的药物并制定最佳剂量方案。DataSet Tracker可以作为现有和未来算法的集成平台的基础模块，用于实时细胞分析，成功应用于人类器官治疗策略评估，有望在临床实践中得到应用。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>开发了一种实时分析软件套件DataSet Tracker，可在计算机、智能手机和智能眼镜硬件上运行。</li>
<li>适用于资源受限、无网络连接显微镜的在线计算环境。</li>
<li>软件旨在解决细胞骨架网格、细胞内膜网络和囊泡运输等领域的复杂动态问题。</li>
<li>DataSet Tracker能进行高内容分析并对新药和组合疗法进行功能性二级筛选。</li>
<li>软件能定量研究药物作用机制，有助于设计防止复发的药物并制定最佳剂量方案。</li>
<li>DataSet Tracker可作为集成平台的基石模块，用于实时细胞分析，适用于评估任何人类器官的治疗策略。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.15735">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-94f1386177611dc8371b3965f53e31eb.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="HDKD-Hybrid-Data-Efficient-Knowledge-Distillation-Network-for-Medical-Image-Classification"><a href="#HDKD-Hybrid-Data-Efficient-Knowledge-Distillation-Network-for-Medical-Image-Classification" class="headerlink" title="HDKD: Hybrid Data-Efficient Knowledge Distillation Network for Medical   Image Classification"></a>HDKD: Hybrid Data-Efficient Knowledge Distillation Network for Medical   Image Classification</h2><p><strong>Authors:Omar S. EL-Assiouti, Ghada Hamed, Dina Khattab, Hala M. Ebied</strong></p>
<p>Vision Transformers (ViTs) have achieved significant advancement in computer vision tasks due to their powerful modeling capacity. However, their performance notably degrades when trained with insufficient data due to lack of inherent inductive biases. Distilling knowledge and inductive biases from a Convolutional Neural Network (CNN) teacher has emerged as an effective strategy for enhancing the generalization of ViTs on limited datasets. Previous approaches to Knowledge Distillation (KD) have pursued two primary paths: some focused solely on distilling the logit distribution from CNN teacher to ViT student, neglecting the rich semantic information present in intermediate features due to the structural differences between them. Others integrated feature distillation along with logit distillation, yet this introduced alignment operations that limits the amount of knowledge transferred due to mismatched architectures and increased the computational overhead. To this end, this paper presents Hybrid Data-efficient Knowledge Distillation (HDKD) paradigm which employs a CNN teacher and a hybrid student. The choice of hybrid student serves two main aspects. First, it leverages the strengths of both convolutions and transformers while sharing the convolutional structure with the teacher model. Second, this shared structure enables the direct application of feature distillation without any information loss or additional computational overhead. Additionally, we propose an efficient light-weight convolutional block named Mobile Channel-Spatial Attention (MBCSA), which serves as the primary convolutional block in both teacher and student models. Extensive experiments on two medical public datasets showcase the superiority of HDKD over other state-of-the-art models and its computational efficiency. Source code at: <a target="_blank" rel="noopener" href="https://github.com/omarsherif200/HDKD">https://github.com/omarsherif200/HDKD</a> </p>
<blockquote>
<p>视觉Transformer（ViTs）由于其强大的建模能力，在计算机视觉任务中取得了显著的进展。然而，在数据不足的情况下进行训练时，由于其缺乏固有的归纳偏置，其性能会显著下降。从卷积神经网络（CNN）教师中提炼知识和归纳偏置，已成为提高ViT在有限数据集上泛化能力的有效策略。知识蒸馏（KD）的先前方法主要追求两种途径：一些方法专注于从CNN教师蒸馏logit分布到ViT学生，忽视了由于结构差异而存在于中间特征中的丰富语义信息。另一些方法结合了特征蒸馏和logit蒸馏，但这引入了对齐操作，由于架构不匹配而限制了知识转移的量并增加了计算开销。为此，本文提出了混合数据高效知识蒸馏（HDKD）范式，该范式采用CNN教师和混合学生。选择混合学生有两个主要方面。首先，它结合了卷积和变压器的优点，同时与教师的模型共享卷积结构。其次，这种共享结构使特征蒸馏能够直接应用，没有任何信息损失或额外的计算开销。此外，我们提出了一种高效的轻量级卷积块，名为移动通道空间注意力（MBCSA），它作为教师和学生模型中主要的卷积块。在两个医学公共数据集上的大量实验表明，HDKD优于其他最先进模型且计算效率更高。源代码位于：<a target="_blank" rel="noopener" href="https://github.com/omarsherif200/HDKD">https://github.com/omarsherif200/HDKD</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.07516v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了Vision Transformers（ViTs）在面临数据不足时性能下降的问题，以及通过从卷积神经网络（CNN）教师模型中蒸馏知识和归纳偏置来增强ViT学生在有限数据集上的泛化能力的方法。文章提出了一种新型的混合数据高效知识蒸馏（HDKD）范式，它采用CNN教师模型和混合学生模型，通过共享结构实现了特征蒸馏的直接应用，提高了知识转移的效率，同时降低了计算开销。此外，文章还提出了一种轻量级的卷积块——Mobile Channel-Spatial Attention（MBCSA），它在教师和学生模型中都起到了关键作用。在医疗公共数据集上的实验表明，HDKD优于其他先进模型，并具有计算效率高的优势。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision Transformers (ViTs) 面临数据不足时性能下降的问题。</li>
<li>知识蒸馏是一种提高ViT在有限数据集上泛化能力的有效策略。</li>
<li>以往的知识蒸馏（KD）方法主要追求两种路径：一种只关注从CNN教师模型到ViT学生模型的logit分布蒸馏，忽视了由于结构差异而存在的中间特征中的丰富语义信息；另一种结合了特征蒸馏和logit蒸馏，但引入了由于架构不匹配而限制知识传递的对齐操作，并增加了计算开销。</li>
<li>本文提出了Hybrid Data-efficient Knowledge Distillation (HDKD) 范式，采用CNN教师模型和混合学生模型，通过共享结构实现特征蒸馏的直接应用。</li>
<li>HDKD采用轻量级的卷积块Mobile Channel-Spatial Attention (MBCSA)，在教师和学生模型中都起到了关键作用。</li>
<li>在医疗公共数据集上的实验表明，HDKD优于其他先进模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.07516">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-7e71ca9e34d27fc62cd6072dfebb598d.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="MERIT-Multi-view-evidential-learning-for-reliable-and-interpretable-liver-fibrosis-staging"><a href="#MERIT-Multi-view-evidential-learning-for-reliable-and-interpretable-liver-fibrosis-staging" class="headerlink" title="MERIT: Multi-view evidential learning for reliable and interpretable   liver fibrosis staging"></a>MERIT: Multi-view evidential learning for reliable and interpretable   liver fibrosis staging</h2><p><strong>Authors:Yuanye Liu, Zheyao Gao, Nannan Shi, Fuping Wu, Yuxin Shi, Qingchao Chen, Xiahai Zhuang</strong></p>
<p>Accurate staging of liver fibrosis from magnetic resonance imaging (MRI) is crucial in clinical practice. While conventional methods often focus on a specific sub-region, multi-view learning captures more information by analyzing multiple patches simultaneously. However, previous multi-view approaches could not typically calculate uncertainty by nature, and they generally integrate features from different views in a black-box fashion, hence compromising reliability as well as interpretability of the resulting models. In this work, we propose a new multi-view method based on evidential learning, referred to as MERIT, which tackles the two challenges in a unified framework. MERIT enables uncertainty quantification of the predictions to enhance reliability, and employs a logic-based combination rule to improve interpretability. Specifically, MERIT models the prediction from each sub-view as an opinion with quantified uncertainty under the guidance of the subjective logic theory. Furthermore, a distribution-aware base rate is introduced to enhance performance, particularly in scenarios involving class distribution shifts. Finally, MERIT adopts a feature-specific combination rule to explicitly fuse multi-view predictions, thereby enhancing interpretability. Results have showcased the effectiveness of the proposed MERIT, highlighting the reliability and offering both ad-hoc and post-hoc interpretability. They also illustrate that MERIT can elucidate the significance of each view in the decision-making process for liver fibrosis staging. Our code has be released via <a target="_blank" rel="noopener" href="https://github.com/HenryLau7/MERIT">https://github.com/HenryLau7/MERIT</a>. </p>
<blockquote>
<p>肝脏纤维化的磁共振成像（MRI）精确分期在临床实践中至关重要。传统方法往往专注于特定子区域，而多视角学习通过同时分析多个补丁来捕获更多信息。然而，先前的多视角方法通常不能自然地计算不确定性，它们一般以黑箱方式整合不同视角的特征，从而损害了模型的可靠性和解释性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.02918v2">PDF</a> Accepted by Medical Image Analysis</p>
<p><strong>摘要</strong></p>
<p>基于磁共振成像对肝纤维化进行准确的分期在临床实践中非常重要。传统方法通常只关注特定子区域，而多视角学习通过同时分析多个补丁捕获更多信息。然而，以前的多视角方法通常无法计算不确定性，它们通常以黑箱方式整合不同视角的特征，从而损害模型的可靠性和解释性。本研究提出了一种基于证据学习的新多视角方法，称为MERIT，该方法在统一框架内解决了这两个挑战。MERIT能够对预测进行不确定性量化，以提高可靠性，并采用基于逻辑的组合规则来提高解释性。具体来说，MERIT将每个子视角的预测建模为具有量化不确定性的观点，在主观逻辑理论的指导下进行。此外，引入了一种感知分布的基线率，以提高性能，特别是在涉及类别分布转移的场景中。最后，MERIT采用特征特定的组合规则来显式融合多视角预测，从而提高了解释性。结果展示了所提出MERIT的有效性，突出了其可靠性和提供了专项及事后的解释性。他们还表明，MERIT可以阐明决策过程中每个视角对肝纤维化分期的重要性。我们的代码已通过<a target="_blank" rel="noopener" href="https://github.com/HenryLau7/MERIT%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/HenryLau7/MERIT发布。</a></p>
<p><strong>关键见解</strong></p>
<ol>
<li>多视角学习方法在肝纤维化MRI分期中至关重要，能够捕获更多信息。</li>
<li>以往的多视角方法无法计算不确定性，影响模型可靠性和解释性。</li>
<li>MERIT方法基于证据学习，统一解决不确定性和解释性问题。</li>
<li>MERIT将预测建模为具有量化不确定性的观点，提高决策可靠性。</li>
<li>MERIT引入分布感知基线率，优化类别分布转移场景中的性能。</li>
<li>MERIT采用特征特定组合规则，显式融合多视角预测，提高解释性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.02918">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7d245294d2efba3fbb2be550bf369f50.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f3540980b296a4fc96511cea600767f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-36d88e6ceac5f6a5423e2b7339cc3cee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ffa06f09353eaf645adaa4f182f8395.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-833142ac404ae3266dbe7c2c4b63d3b3.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="CromSS-Cross-modal-pre-training-with-noisy-labels-for-remote-sensing-image-segmentation"><a href="#CromSS-Cross-modal-pre-training-with-noisy-labels-for-remote-sensing-image-segmentation" class="headerlink" title="CromSS: Cross-modal pre-training with noisy labels for remote sensing   image segmentation"></a>CromSS: Cross-modal pre-training with noisy labels for remote sensing   image segmentation</h2><p><strong>Authors:Chenying Liu, Conrad Albrecht, Yi Wang, Xiao Xiang Zhu</strong></p>
<p>We explore the potential of large-scale noisily labeled data to enhance feature learning by pretraining semantic segmentation models within a multi-modal framework for geospatial applications. We propose a novel Cross-modal Sample Selection (CromSS) method, a weakly supervised pretraining strategy designed to improve feature representations through cross-modal consistency and noise mitigation techniques. Unlike conventional pretraining approaches, CromSS exploits massive amounts of noisy and easy-to-come-by labels for improved feature learning beneficial to semantic segmentation tasks. We investigate middle and late fusion strategies to optimize the multi-modal pretraining architecture design. We also introduce a cross-modal sample selection module to mitigate the adverse effects of label noise, which employs a cross-modal entangling strategy to refine the estimated confidence masks within each modality to guide the sampling process. Additionally, we introduce a spatial-temporal label smoothing technique to counteract overconfidence for enhanced robustness against noisy labels. To validate our approach, we assembled the multi-modal dataset, NoLDO-S12, which consists of a large-scale noisy label subset from Google’s Dynamic World (DW) dataset for pretraining and two downstream subsets with high-quality labels from Google DW and OpenStreetMap (OSM) for transfer learning. Experimental results on two downstream tasks and the publicly available DFC2020 dataset demonstrate that when effectively utilized, the low-cost noisy labels can significantly enhance feature learning for segmentation tasks. All data, code, and pretrained weights will be made publicly available. </p>
<blockquote>
<p>我们探索大规模噪声标记数据在地理应用的多模态框架内通过预训练语义分割模型以增强特征学习的潜力。我们提出了一种新颖的跨模态样本选择（CromSS）方法，这是一种弱监督预训练策略，旨在通过跨模态一致性和降噪技术改进特征表示。与传统的预训练方法不同，CromSS利用大量的噪声和易获得的标签来改进特征学习，这对语义分割任务有益。我们研究了中间和后期融合策略来优化多模态预训练架构设计。我们还引入了一个跨模态样本选择模块来缓解标签噪声的不利影响，该模块采用跨模态纠缠策略来细化每个模态内的估计置信掩码，以指导采样过程。此外，我们引入了一种时空标签平滑技术来对抗过度自信，以增强对噪声标签的稳健性。为了验证我们的方法，我们创建了多模态数据集NoLDO-S12，它由来自Google动态世界（DW）数据集的大规模噪声标签子集组成，用于预训练，以及两个来自Google DW和OpenStreetMap（OSM）的高质量标签的下游子集用于迁移学习。在两个下游任务和公开的DFC2020数据集上的实验结果表明，当有效利用时，低成本噪声标签可以显著增强分割任务的特征学习。所有数据、代码和预训练权重将公开可用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.01217v2">PDF</a> The 1st short version was accepted as an oral presentation by ICLR   2024 ML4RS workshop. The 2nd extended version is being under review</p>
<p><strong>摘要</strong></p>
<p>本研究探讨大规模噪声标签数据在地理应用中的潜力，通过多模态框架预训练语义分割模型以增强特征学习。提出一种新颖的跨模态样本选择（CromSS）方法，这是一种弱监督预训练策略，旨在通过跨模态一致性和噪声抑制技术改进特征表示。不同于传统预训练方式，CromSS利用大量易于获取且带有噪声的标签来改善特征学习，对语义分割任务有益。研究中探讨了中期和后期融合策略以优化多模态预训练架构设计。还引入跨模态样本选择模块来减轻标签噪声的不利影响，采用跨模态纠缠策略来优化每个模态的估计置信度掩膜，以指导采样过程。此外，引入空间时间标签平滑技术来对抗过度自信，提高对噪声标签的稳健性。为验证方法，我们构建了多模态数据集NoLDO-S12，其中包括来自Google动态世界（DW）数据集的大规模噪声标签子集用于预训练，以及来自Google DW和OpenStreetMap（OSM）的高质量标签下游子集用于迁移学习。在两个下游任务和公开的DFC2020数据集上的实验结果表明，当有效利用时，低成本噪声标签可以显著增强分割任务的特征学习。所有数据、代码和预训练权重将公开可用。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>探索大规模噪声标签数据在地理应用语义分割中的潜力。</li>
<li>提出一种新颖的跨模态样本选择（CromSS）方法，结合弱监督预训练来改善特征学习。</li>
<li>引入跨模态样本选择模块来减轻噪声标签的负面影响。</li>
<li>采用空间时间标签平滑技术来提高模型对噪声标签的稳健性。</li>
<li>通过实验验证，利用噪声标签能显著增强语义分割任务的特征学习。</li>
<li>公开了多模态数据集NoLDO-S12，供研究使用。</li>
<li>该方法的数据、代码和预训练权重均可公开获取。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.01217">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-47edb857dce04d4a392fc91a7d1fe86a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6734a868131bf6d497e8f178f99e153c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-56b0f545b688e36318b90529087a9613.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-201e333ea8bca7501cda4056c84b9eee.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c5c2a0f4fce9a88df6783e548fff753a.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-05/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-05/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">医学图像</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-05/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-0fc8eaf8d5cded4d6217ebce5f6378e3.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS 方向最新论文已更新，请持续关注 Update in 2025-03-05  CUIfy the XR An Open-Source Package to Embed LLM-powered Conversational   Agents in XR
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-05
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-05/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-59a4cc9492a290d548413adf31be9e93.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-03-05  A Dual-Purpose Framework for Backdoor Defense and Backdoor Amplification   in Diffusion Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-05
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">24801.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
