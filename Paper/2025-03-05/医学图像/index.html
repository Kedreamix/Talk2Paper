<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-05  MobileViM A Light-weight and Dimension-independent Vision Mamba for 3D   Medical Image Analysis">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-ce4cf8702947ea6ba2c02dc49aa37123.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-05
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    10.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    42 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-05-æ›´æ–°"><a href="#2025-03-05-æ›´æ–°" class="headerlink" title="2025-03-05 æ›´æ–°"></a>2025-03-05 æ›´æ–°</h1><h2 id="MobileViM-A-Light-weight-and-Dimension-independent-Vision-Mamba-for-3D-Medical-Image-Analysis"><a href="#MobileViM-A-Light-weight-and-Dimension-independent-Vision-Mamba-for-3D-Medical-Image-Analysis" class="headerlink" title="MobileViM: A Light-weight and Dimension-independent Vision Mamba for 3D   Medical Image Analysis"></a>MobileViM: A Light-weight and Dimension-independent Vision Mamba for 3D   Medical Image Analysis</h2><p><strong>Authors:Wei Dai, Steven Wang, Jun Liu</strong></p>
<p>Efficient evaluation of three-dimensional (3D) medical images is crucial for diagnostic and therapeutic practices in healthcare. Recent years have seen a substantial uptake in applying deep learning and computer vision to analyse and interpret medical images. Traditional approaches, such as convolutional neural networks (CNNs) and vision transformers (ViTs), face significant computational challenges, prompting the need for architectural advancements. Recent efforts have led to the introduction of novel architectures like the &#96;&#96;Mambaâ€™â€™ model as alternative solutions to traditional CNNs or ViTs. The Mamba model excels in the linear processing of one-dimensional data with low computational demands. However, Mambaâ€™s potential for 3D medical image analysis remains underexplored and could face significant computational challenges as the dimension increases. This manuscript presents MobileViM, a streamlined architecture for efficient segmentation of 3D medical images. In the MobileViM network, we invent a new dimension-independent mechanism and a dual-direction traversing approach to incorporate with a vision-Mamba-based framework. MobileViM also features a cross-scale bridging technique to improve efficiency and accuracy across various medical imaging modalities. With these enhancements, MobileViM achieves segmentation speeds exceeding 90 frames per second (FPS) on a single graphics processing unit (i.e., NVIDIA RTX 4090). This performance is over 24 FPS faster than the state-of-the-art deep learning models for processing 3D images with the same computational resources. In addition, experimental evaluations demonstrate that MobileViM delivers superior performance, with Dice similarity scores reaching 92.72%, 86.69%, 80.46%, and 77.43% for PENGWIN, BraTS2024, ATLAS, and Toothfairy2 datasets, respectively, which significantly surpasses existing models. </p>
<blockquote>
<p>åœ¨åŒ»ç–—ä¿å¥é¢†åŸŸï¼Œå¯¹ä¸‰ç»´ï¼ˆ3Dï¼‰åŒ»å­¦å›¾åƒçš„æœ‰æ•ˆè¯„ä¼°å¯¹äºè¯Šæ–­å’Œæ²»ç–—å®è·µè‡³å…³é‡è¦ã€‚è¿‘å¹´æ¥ï¼Œæ·±åº¦å­¦ä¹ å’Œè®¡ç®—æœºè§†è§‰åœ¨åŒ»å­¦å›¾åƒåˆ†æå’Œè§£é‡Šæ–¹é¢çš„åº”ç”¨æ˜¾è‘—å¢åŠ ã€‚ä¼ ç»Ÿæ–¹æ³•ï¼Œå¦‚å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰å’Œè§†è§‰å˜å‹å™¨ï¼ˆViTsï¼‰ï¼Œé¢ä¸´ç€é‡å¤§è®¡ç®—æŒ‘æˆ˜ï¼Œä¿ƒä½¿éœ€è¦è¿›è¡Œæ¶æ„æ”¹è¿›ã€‚è¿‘æœŸçš„ç ”ç©¶å·¥ä½œå¼•å…¥äº†æ–°å‹æ¶æ„ï¼Œå¦‚â€œMambaâ€æ¨¡å‹ï¼Œä½œä¸ºä¼ ç»ŸCNNsæˆ–ViTsçš„æ›¿ä»£è§£å†³æ–¹æ¡ˆã€‚â€œMambaâ€æ¨¡å‹åœ¨å¤„ç†ä¸€ç»´æ•°æ®çš„çº¿æ€§å¤„ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œè®¡ç®—éœ€æ±‚è¾ƒä½ã€‚ç„¶è€Œï¼ŒMambaåœ¨3DåŒ»å­¦å›¾åƒåˆ†ææ–¹é¢çš„æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ï¼Œéšç€ç»´åº¦çš„å¢åŠ ï¼Œå¯èƒ½ä¼šé¢ä¸´é‡å¤§çš„è®¡ç®—æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†MobileViMï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºé«˜æ•ˆåˆ†å‰²3DåŒ»å­¦å›¾åƒçš„ç®€åŒ–æ¶æ„ã€‚åœ¨MobileViMç½‘ç»œä¸­ï¼Œæˆ‘ä»¬å‘æ˜äº†ä¸€ç§æ–°çš„ç»´åº¦ç‹¬ç«‹æœºåˆ¶å’Œä¸€ç§åŒå‘éå†æ–¹æ³•ï¼Œå°†å…¶çº³å…¥åŸºäºè§†è§‰Mambaçš„æ¡†æ¶ä¸­ã€‚MobileViMè¿˜é‡‡ç”¨è·¨å°ºåº¦æ¡¥æ¢æŠ€æœ¯ï¼Œä»¥æé«˜ä¸åŒåŒ»å­¦æˆåƒæ¨¡å¼çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚é€šè¿‡è¿™äº›å¢å¼ºåŠŸèƒ½ï¼ŒMobileViMåœ¨å•ä¸ªå›¾å½¢å¤„ç†å•å…ƒï¼ˆä¾‹å¦‚NVIDIA RTX 4090ï¼‰ä¸Šå®ç°äº†è¶…è¿‡æ¯ç§’90å¸§ï¼ˆFPSï¼‰çš„åˆ†å‰²é€Ÿåº¦ã€‚æ­¤æ€§èƒ½æ¯”ä½¿ç”¨ç›¸åŒè®¡ç®—èµ„æºçš„æœ€æ–°æ·±åº¦å­¦ä¹ æ¨¡å‹å¤„ç†3Då›¾åƒçš„é€Ÿåº¦å¿«24 FPSä»¥ä¸Šã€‚æ­¤å¤–ï¼Œå®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒMobileViMçš„æ€§èƒ½å“è¶Šï¼Œåœ¨PENGWINã€BraTS2024ã€ATLASå’ŒToothfairy2æ•°æ®é›†ä¸Šçš„Diceç›¸ä¼¼åº¦å¾—åˆ†åˆ†åˆ«è¾¾åˆ°92.72%ã€86.69%ã€80.46%å’Œ77.43%ï¼Œæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13524v2">PDF</a> The co-authors have not approved its submission to arXiv</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹ä¸‰ç»´åŒ»å­¦å›¾åƒé«˜æ•ˆåˆ†å‰²çš„æ¨¡å‹MobileViMã€‚è¯¥æ¨¡å‹å¼•å…¥äº†ä¸€ç§æ–°çš„ç»´åº¦ç‹¬ç«‹æœºåˆ¶å’ŒåŒå‘éå†æ–¹æ³•ï¼Œä¸åŸºäºè§†è§‰çš„Mambaæ¡†æ¶ç›¸ç»“åˆã€‚MobileViMè¿˜é‡‡ç”¨è·¨å°ºåº¦æ¡¥æ¥æŠ€æœ¯ï¼Œä»¥æé«˜ä¸åŒåŒ»å­¦æˆåƒæ¨¡æ€çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚ç›¸è¾ƒäºå…¶ä»–å…ˆè¿›æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ŒMobileViMåœ¨å¤„ç†ä¸‰ç»´å›¾åƒæ—¶é€Ÿåº¦æ›´å¿«ï¼Œæ€§èƒ½æ›´ä¼˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¸‰ç»´åŒ»å­¦å›¾åƒè¯„ä¼°åœ¨åŒ»ç–—è¯Šæ–­å’Œæ²»ç–—å®è·µä¸­è‡³å…³é‡è¦ã€‚</li>
<li>æ·±åº¦å­¦ä¹ åœ¨è®¡ç®—æœºè§†è§‰åœ¨åŒ»å­¦å›¾åƒåˆ†æå’Œè§£é‡Šä¸­å¾—åˆ°å¹¿æ³›åº”ç”¨ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•å¦‚å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰å’Œè§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰é¢ä¸´è®¡ç®—æŒ‘æˆ˜ï¼Œéœ€è¦æ¶æ„æ”¹è¿›ã€‚</li>
<li>Mambaæ¨¡å‹åœ¨ä½è®¡ç®—éœ€æ±‚çš„ä¸€ç»´æ•°æ®å¤„ç†ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ä¸‰ç»´åŒ»å­¦å›¾åƒåˆ†ææ–¹é¢çš„æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚</li>
<li>MobileViMæ˜¯ä¸€ä¸ªé’ˆå¯¹ä¸‰ç»´åŒ»å­¦å›¾åƒé«˜æ•ˆåˆ†å‰²çš„æµçº¿å‹æ¶æ„ã€‚</li>
<li>MobileViMé‡‡ç”¨æ–°çš„ç»´åº¦ç‹¬ç«‹æœºåˆ¶å’ŒåŒå‘éå†æ–¹æ³•ä¸è§†è§‰Mambaæ¡†æ¶ç›¸ç»“åˆï¼Œæé«˜äº†æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13524">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-478c4a069cb045964697c54dfba709cc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce4cf8702947ea6ba2c02dc49aa37123.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e42796ec18c7321bee4b45d36a1c3825.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9cd6b470a040419562bc91eea89ac5f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a5f77b8f330ea05bdd1df377c514ad9.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="SCC-YOLO-An-Improved-Object-Detector-for-Assisting-in-Brain-Tumor-Diagnosis"><a href="#SCC-YOLO-An-Improved-Object-Detector-for-Assisting-in-Brain-Tumor-Diagnosis" class="headerlink" title="SCC-YOLO: An Improved Object Detector for Assisting in Brain Tumor   Diagnosis"></a>SCC-YOLO: An Improved Object Detector for Assisting in Brain Tumor   Diagnosis</h2><p><strong>Authors:Runci Bai, Guibao Xu, Yanze Shi</strong></p>
<p>Brain tumors can lead to neurological dysfunction, cognitive and psychological changes, increased intracranial pressure, and seizures, posing significant risks to health. The You Only Look Once (YOLO) series has shown superior accuracy in medical imaging object detection. This paper presents a novel SCC-YOLO architecture that integrates the SCConv module into YOLOv9. The SCConv module optimizes convolutional efficiency by reducing spatial and channel redundancy, enhancing image feature learning. We examine the effects of different attention mechanisms with YOLOv9 for brain tumor detection using the Br35H dataset and our custom dataset (Brain_Tumor_Dataset). Results indicate that SCC-YOLO improved mAP50 by 0.3% on the Br35H dataset and by 0.5% on our custom dataset compared to YOLOv9. SCC-YOLO achieves state-of-the-art performance in brain tumor detection. </p>
<blockquote>
<p>è„‘è‚¿ç˜¤å¯èƒ½å¯¼è‡´ç¥ç»åŠŸèƒ½éšœç¢ã€è®¤çŸ¥å’Œå¿ƒç†å­¦å˜åŒ–ã€é¢…å†…å‹å‡é«˜å’Œç™«ç—«å‘ä½œï¼Œå¯¹å¥åº·æ„æˆé‡å¤§é£é™©ã€‚You Only Look Onceï¼ˆYOLOï¼‰ç³»åˆ—å·²åœ¨åŒ»å­¦å½±åƒç‰©ä½“æ£€æµ‹ä¸­å±•ç°å‡ºå“è¶Šçš„å‡†ç¡®æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„SCC-YOLOæ¶æ„ï¼Œå®ƒå°†SCConvæ¨¡å—æ•´åˆåˆ°YOLOv9ä¸­ã€‚SCConvæ¨¡å—é€šè¿‡å‡å°‘ç©ºé—´å†—ä½™å’Œé€šé“å†—ä½™æ¥ä¼˜åŒ–å·ç§¯æ•ˆç‡ï¼Œå¢å¼ºå›¾åƒç‰¹å¾å­¦ä¹ ã€‚æˆ‘ä»¬ä½¿ç”¨Br35Hæ•°æ®é›†å’Œè‡ªå®šä¹‰æ•°æ®é›†ï¼ˆBrain_Tumor_Datasetï¼‰æ¥æ£€éªŒä¸åŒæ³¨æ„åŠ›æœºåˆ¶åœ¨YOLOv9æ£€æµ‹è„‘è‚¿ç˜¤æ–¹é¢çš„æ•ˆæœã€‚ç»“æœè¡¨æ˜ï¼Œä¸YOLOv9ç›¸æ¯”ï¼ŒSCC-YOLOåœ¨Br35Hæ•°æ®é›†ä¸Šçš„mAP50æé«˜äº†0.3%ï¼Œåœ¨æˆ‘ä»¬çš„è‡ªå®šä¹‰æ•°æ®é›†ä¸Šæé«˜äº†0.5%ã€‚SCC-YOLOåœ¨è„‘è‚¿ç˜¤æ£€æµ‹æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.03836v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„SCC-YOLOæ¶æ„ï¼Œå®ƒå°†SCConvæ¨¡å—é›†æˆåˆ°YOLOv9ä¸­ä»¥æé«˜åŒ»å­¦å›¾åƒä¸­ç›®æ ‡æ£€æµ‹çš„å‡†ç¡®æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSCC-YOLOåœ¨Br35Hæ•°æ®é›†å’Œè‡ªå®šä¹‰æ•°æ®é›†ä¸Šçš„è¡¨ç°å‡ä¼˜äºYOLOv9ï¼Œç‰¹åˆ«æ˜¯åœ¨è„‘è‚¿ç˜¤æ£€æµ‹æ–¹é¢å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è„‘è‚¿ç˜¤å¯èƒ½å¯¼è‡´ç¥ç»åŠŸèƒ½éšœç¢ã€è®¤çŸ¥å’Œå¿ƒç†å˜åŒ–ã€é¢…å†…å‹å¢åŠ å’Œç™«ç—«å‘ä½œï¼Œå¯¹å¥åº·æ„æˆé‡å¤§é£é™©ã€‚</li>
<li>YOLOç³»åˆ—åœ¨åŒ»å­¦æˆåƒç›®æ ‡æ£€æµ‹ä¸­å±•ç°äº†è¾ƒé«˜çš„å‡†ç¡®æ€§ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„SCC-YOLOæ¶æ„ï¼Œé›†æˆäº†SCConvæ¨¡å—ä»¥ä¼˜åŒ–å·ç§¯æ•ˆç‡ï¼Œå¢å¼ºå›¾åƒç‰¹å¾å­¦ä¹ ã€‚</li>
<li>SCC-YOLOåœ¨Br35Hæ•°æ®é›†å’Œè‡ªå®šä¹‰çš„Brain_Tumor_Datasetæ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒéªŒè¯ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¸YOLOv9ç›¸æ¯”ï¼ŒSCC-YOLOåœ¨mAP50æŒ‡æ ‡ä¸Šæœ‰æ‰€æå‡ï¼Œå…¶ä¸­åœ¨è‡ªå®šä¹‰æ•°æ®é›†ä¸Šçš„æå‡æ›´ä¸ºæ˜æ˜¾ã€‚</li>
<li>SCC-YOLOåœ¨è„‘è‚¿ç˜¤æ£€æµ‹æ–¹é¢å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.03836">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8e57a99cb01845266804066bfb55d8d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-40439e09d849d6c7d2da475668a69610.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-716e93a001815ef004af7c8c02d3454f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0bef9c3287cc19861a65f2cd40a73886.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0951837b4439609123821811c2093d2f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f9221bc3cd5a48c4a20bf66b756658c.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="CT-Mamba-A-Hybrid-Convolutional-State-Space-Model-for-Low-Dose-CT-Denoising"><a href="#CT-Mamba-A-Hybrid-Convolutional-State-Space-Model-for-Low-Dose-CT-Denoising" class="headerlink" title="CT-Mamba: A Hybrid Convolutional State Space Model for Low-Dose CT   Denoising"></a>CT-Mamba: A Hybrid Convolutional State Space Model for Low-Dose CT   Denoising</h2><p><strong>Authors:Linxuan Li, Wenjia Wei, Luyao Yang, Wenwen Zhang, Jiashu Dong, Yahua Liu, Hongshi Huang, Wei Zhao</strong></p>
<p>Low-dose CT (LDCT) significantly reduces the radiation dose received by patients, however, dose reduction introduces additional noise and artifacts. Currently, denoising methods based on convolutional neural networks (CNNs) face limitations in long-range modeling capabilities, while Transformer-based denoising methods, although capable of powerful long-range modeling, suffer from high computational complexity. Furthermore, the denoised images predicted by deep learning-based techniques inevitably exhibit differences in noise distribution compared to normal-dose CT (NDCT) images, which can also impact the final image quality and diagnostic outcomes. This paper proposes CT-Mamba, a hybrid convolutional State Space Model for LDCT image denoising. The model combines the local feature extraction advantages of CNNs with Mambaâ€™s strength in capturing long-range dependencies, enabling it to capture both local details and global context. Additionally, we introduce an innovative spatially coherent â€˜Zâ€™-shaped scanning scheme to ensure spatial continuity between adjacent pixels in the image. We design a Mamba-driven deep noise power spectrum (NPS) loss function to guide model training, ensuring that the noise texture of the denoised LDCT images closely resembles that of NDCT images, thereby enhancing overall image quality and diagnostic value. Experimental results have demonstrated that CT-Mamba performs excellently in reducing noise in LDCT images, enhancing detail preservation, and optimizing noise texture distribution, and exhibits higher statistical similarity with the radiomics features of NDCT images. The proposed CT-Mamba demonstrates outstanding performance in LDCT denoising and holds promise as a representative approach for applying the Mamba framework to LDCT denoising tasks. Our code will be made available after the paper is officially published: <a target="_blank" rel="noopener" href="https://github.com/zy2219105/CT-Mamba/">https://github.com/zy2219105/CT-Mamba/</a>. </p>
<blockquote>
<p>ä½å‰‚é‡CTï¼ˆLDCTï¼‰æ˜¾è‘—å‡å°‘äº†æ‚£è€…æ¥å—çš„è¾å°„å‰‚é‡ï¼Œç„¶è€Œï¼Œå‰‚é‡çš„å‡å°‘ä¼šå¼•å…¥é¢å¤–çš„å™ªå£°å’Œä¼ªå½±ã€‚ç›®å‰ï¼ŒåŸºäºå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰çš„é™å™ªæ–¹æ³•åœ¨é•¿æœŸå»ºæ¨¡èƒ½åŠ›æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œè€ŒåŸºäºTransformerçš„é™å™ªæ–¹æ³•è™½ç„¶å…·æœ‰å¼ºå¤§çš„é•¿æœŸå»ºæ¨¡èƒ½åŠ›ï¼Œä½†è®¡ç®—å¤æ‚åº¦è¾ƒé«˜ã€‚æ­¤å¤–ï¼ŒåŸºäºæ·±åº¦å­¦ä¹ æŠ€æœ¯é¢„æµ‹çš„é™å™ªå›¾åƒä¸æ­£å¸¸å‰‚é‡CTï¼ˆNDCTï¼‰å›¾åƒç›¸æ¯”ï¼Œå™ªå£°åˆ†å¸ƒä¸å¯é¿å…åœ°å­˜åœ¨å·®å¼‚ï¼Œè¿™ä¹Ÿå¯èƒ½å½±å“æœ€ç»ˆçš„å›¾åƒè´¨é‡å’Œè¯Šæ–­ç»“æœã€‚æœ¬æ–‡æå‡ºäº†CT-Mambaï¼Œä¸€ç§ç”¨äºLDCTå›¾åƒé™å™ªçš„æ··åˆå·ç§¯çŠ¶æ€ç©ºé—´æ¨¡å‹ã€‚è¯¥æ¨¡å‹ç»“åˆäº†CNNsæå–å±€éƒ¨ç‰¹å¾çš„ä¼˜åŠ¿å’ŒMambaåœ¨æ•æ‰é•¿æœŸä¾èµ–å…³ç³»æ–¹é¢çš„å®åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿæ•æ‰å±€éƒ¨ç»†èŠ‚å’Œå…¨å±€ä¸Šä¸‹æ–‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åˆ›æ–°çš„ç©ºé—´è¿è´¯çš„â€œZâ€å½¢æ‰«ææ–¹æ¡ˆï¼Œä»¥ç¡®ä¿å›¾åƒä¸­ç›¸é‚»åƒç´ ä¹‹é—´çš„ç©ºé—´è¿ç»­æ€§ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ç§ä»¥Mambaé©±åŠ¨çš„æ·±åº¦å™ªå£°åŠŸç‡è°±ï¼ˆNPSï¼‰æŸå¤±å‡½æ•°æ¥æŒ‡å¯¼æ¨¡å‹è®­ç»ƒï¼Œç¡®ä¿é™å™ªLDCTå›¾åƒçš„å™ªå£°çº¹ç†ä¸NDCTå›¾åƒç›¸ä¼¼ï¼Œä»è€Œæé«˜æ•´ä½“å›¾åƒè´¨é‡å’Œè¯Šæ–­ä»·å€¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCT-Mambaåœ¨é™ä½LDCTå›¾åƒå™ªå£°ã€å¢å¼ºç»†èŠ‚ä¿ç•™å’Œä¼˜åŒ–å™ªå£°çº¹ç†åˆ†å¸ƒæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä¸NDCTå›¾åƒçš„æ”¾å°„å­¦ç‰¹å¾å…·æœ‰æ›´é«˜çš„ç»Ÿè®¡ç›¸ä¼¼æ€§ã€‚æ‰€æå‡ºçš„CT-Mambaåœ¨LDCTé™å™ªæ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œå¹¶æœ‰æœ›ä½œä¸ºå°†Mambaæ¡†æ¶åº”ç”¨äºLDCTé™å™ªä»»åŠ¡çš„ä¸€ç§ä»£è¡¨æ€§æ–¹æ³•ã€‚æˆ‘ä»¬çš„ä»£ç å°†åœ¨è®ºæ–‡æ­£å¼å‘è¡¨åæä¾›ï¼š<a target="_blank" rel="noopener" href="https://github.com/zy2219105/CT-Mamba/">https://github.com/zy2219105/CT-Mamba/</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.07930v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå·ç§¯ç¥ç»ç½‘ç»œä¸Mambaæ¨¡å‹çš„æ··åˆCTå›¾åƒå»å™ªæ–¹æ³•èƒ½æœ‰æ•ˆå‡å°‘ä½å‰‚é‡CTï¼ˆLDCTï¼‰å›¾åƒä¸­çš„å™ªå£°ä¸ä¼ªå½±ï¼ŒåŒæ—¶å…¼é¡¾å±€éƒ¨ç»†èŠ‚ä¸å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚æ–°æ–¹æ³•ç»“åˆäº†CNNçš„å±€éƒ¨ç‰¹å¾æå–ä¼˜åŠ¿ä¸Mambaæ¨¡å‹çš„é•¿ç¨‹ä¾èµ–æ€§æ•æ‰èƒ½åŠ›ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°é¢–çš„â€œZâ€å­—å½¢æ‰«ææ–¹æ¡ˆï¼Œä»¥ç¡®ä¿å›¾åƒä¸­ç›¸é‚»åƒç´ çš„ç©ºé—´è¿ç»­æ€§ã€‚æ­¤å¤–ï¼Œé€šè¿‡è®¾è®¡åŸºäºMambaçš„æ·±åº¦å™ªå£°åŠŸç‡è°±ï¼ˆNPSï¼‰æŸå¤±å‡½æ•°ï¼Œè®­ç»ƒæ¨¡å‹ä»¥ç”Ÿæˆä¸å¸¸è§„å‰‚é‡CTï¼ˆNDCTï¼‰å›¾åƒå™ªå£°çº¹ç†ç›¸ä¼¼çš„å»å™ªLDCTå›¾åƒï¼Œä»è€Œæé«˜æ•´ä½“å›¾åƒè´¨é‡å’Œè¯Šæ–­ä»·å€¼ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–°æ–¹æ³•åœ¨é™ä½LDCTå›¾åƒå™ªå£°ã€å¢å¼ºç»†èŠ‚ä¿ç•™ä»¥åŠä¼˜åŒ–å™ªå£°çº¹ç†åˆ†å¸ƒæ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä½å‰‚é‡CTï¼ˆLDCTï¼‰ä¸­ä½¿ç”¨äº†æ–°å‹çš„å›¾åƒå»å™ªæ–¹æ³•ä»¥å‡è½»å™ªå£°å’Œä¼ªå½±é—®é¢˜ã€‚</li>
<li>è¯¥æ–¹æ³•ç»“åˆäº†å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ä¸Mambaæ¨¡å‹çš„ä¼˜åŠ¿ï¼Œæé«˜äº†é•¿ç¨‹å»ºæ¨¡èƒ½åŠ›åŒæ—¶é™ä½äº†è®¡ç®—å¤æ‚æ€§ã€‚</li>
<li>åˆ›æ–°æ€§åœ°é‡‡ç”¨â€œZâ€å­—å½¢æ‰«ææ–¹æ¡ˆç¡®ä¿å›¾åƒçš„ç©ºé—´è¿ç»­æ€§ã€‚</li>
<li>å¼•å…¥äº†æ·±åº¦å™ªå£°åŠŸç‡è°±ï¼ˆNPSï¼‰æŸå¤±å‡½æ•°ï¼Œç¡®ä¿å»å™ªåçš„LDCTå›¾åƒå™ªå£°çº¹ç†æ¥è¿‘å¸¸è§„å‰‚é‡CTï¼ˆNDCTï¼‰å›¾åƒã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•åœ¨é™ä½å™ªå£°ã€ä¿ç•™ç»†èŠ‚å’Œä¼˜åŒ–å™ªå£°çº¹ç†æ–¹é¢æ•ˆæœæ˜¾è‘—ã€‚</li>
<li>è¯¥æ–¹æ³•æœ‰æœ›ä»£è¡¨å°†Mambaæ¡†æ¶åº”ç”¨äºLDCTå»å™ªä»»åŠ¡çš„ä¸€ç§å…¸å‹æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.07930">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f82343b6f9b62451565796de39d0aec6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4464e404c3ac60db594159995ef9f4f8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8c1b0fc322a82994bf2bac29ae717243.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a17ec349251df4219a4379ade78dd45.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dacff271adf6830efa2c8de5c4cbe6a6.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Learning-General-Purpose-Biomedical-Volume-Representations-using-Randomized-Synthesis"><a href="#Learning-General-Purpose-Biomedical-Volume-Representations-using-Randomized-Synthesis" class="headerlink" title="Learning General-Purpose Biomedical Volume Representations using   Randomized Synthesis"></a>Learning General-Purpose Biomedical Volume Representations using   Randomized Synthesis</h2><p><strong>Authors:Neel Dey, Benjamin Billot, Hallee E. Wong, Clinton J. Wang, Mengwei Ren, P. Ellen Grant, Adrian V. Dalca, Polina Golland</strong></p>
<p>Current volumetric biomedical foundation models struggle to generalize as public 3D datasets are small and do not cover the broad diversity of medical procedures, conditions, anatomical regions, and imaging protocols. We address this by creating a representation learning method that instead anticipates strong domain shifts at training time itself. We first propose a data engine that synthesizes highly variable training samples that would enable generalization to new biomedical contexts. To then train a single 3D network for any voxel-level task, we develop a contrastive learning method that pretrains the network to be stable against nuisance imaging variation simulated by the data engine, a key inductive bias for generalization. This networkâ€™s features can be used as robust representations of input images for downstream tasks and its weights provide a strong, dataset-agnostic initialization for finetuning on new datasets. As a result, we set new standards across both multimodality registration and few-shot segmentation, a first for any 3D biomedical vision model, all without (pre-)training on any existing dataset of real images. </p>
<blockquote>
<p>å½“å‰çš„ä¸‰ç»´ç”Ÿç‰©åŒ»å­¦åŸºç¡€æ¨¡å‹åœ¨æ³›åŒ–æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå› ä¸ºå…¬å…±çš„3Dæ•°æ®é›†è§„æ¨¡è¾ƒå°ï¼Œå¹¶ä¸èƒ½è¦†ç›–å¹¿æ³›çš„åŒ»ç–—ç¨‹åºã€çŠ¶å†µã€è§£å‰–åŒºåŸŸå’Œæˆåƒåè®®ã€‚æˆ‘ä»¬é€šè¿‡åˆ›å»ºä¸€ç§è¡¨ç¤ºå­¦ä¹ æ–¹æ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨è®­ç»ƒæ—¶æœ¬èº«å°±èƒ½é¢„æµ‹åˆ°å¼ºçƒˆçš„é¢†åŸŸå˜åŒ–ã€‚æˆ‘ä»¬é¦–å…ˆæå‡ºäº†ä¸€ä¸ªæ•°æ®å¼•æ“ï¼Œè¯¥å¼•æ“èƒ½å¤Ÿåˆæˆé«˜åº¦å¯å˜çš„è®­ç»ƒæ ·æœ¬ï¼Œä»¥å®ç°å‘æ–°ç”Ÿç‰©åŒ»å­¦ç¯å¢ƒçš„æ³›åŒ–ã€‚ä¸ºäº†é’ˆå¯¹ä»»ä½•ä½“ç´ çº§ä»»åŠ¡è®­ç»ƒå•ä¸ªçš„3Dç½‘ç»œï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§å¯¹æ¯”å­¦ä¹ æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿè®­ç»ƒç½‘ç»œä»¥å¯¹æŠ—ç”±æ•°æ®å¼•æ“æ¨¡æ‹Ÿçš„å¹²æ‰°æˆåƒå˜åŒ–ï¼Œè¿™æ˜¯æ³›åŒ–çš„å…³é”®å½’çº³åç½®ã€‚è¯¥ç½‘ç»œçš„ç‰¹å¾å¯ä»¥ä½œä¸ºä¸‹æ¸¸ä»»åŠ¡çš„ç¨³å¥å›¾åƒè¡¨ç¤ºï¼Œå…¶æƒé‡ä¸ºåœ¨æ–°æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒæä¾›äº†å¼ºå¤§ä¸”ç‹¬ç«‹äºæ•°æ®é›†ä¹‹å¤–çš„åˆå§‹åŒ–ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åœ¨å¤šæ¨¡æ€æ³¨å†Œå’Œå°‘æ ·æœ¬åˆ†å‰²æ–¹é¢è®¾å®šäº†æ–°çš„æ ‡å‡†ï¼Œè¿™æ˜¯é¦–æ¬¡æœ‰3Dç”Ÿç‰©åŒ»å­¦è§†è§‰æ¨¡å‹èƒ½åšåˆ°è¿™ä¸€ç‚¹ï¼Œå¹¶ä¸”å…¨ç¨‹æœªä½¿ç”¨ä»»ä½•ç°æœ‰çœŸå®å›¾åƒæ•°æ®é›†è¿›è¡Œï¼ˆé¢„ï¼‰è®­ç»ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.02372v2">PDF</a> ICLR 2025: International Conference on Learning Representations. Code   and model weights available at <a target="_blank" rel="noopener" href="https://github.com/neel-dey/anatomix">https://github.com/neel-dey/anatomix</a>.   Keywords: synthetic data, representation learning, medical image analysis,   image registration, image segmentation</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æå‡ºä¸€ç§è§£å†³ç”Ÿç‰©åŒ»å­¦å›¾åƒé¢†åŸŸæ¨¡å‹æ³›åŒ–èƒ½åŠ›å¼±çš„é—®é¢˜çš„æ–¹æ³•ã€‚é’ˆå¯¹å…¬å…±3Dæ•°æ®é›†è§„æ¨¡å°ã€æ¶µç›–åŒ»ç–—ç¨‹åºã€çŠ¶å†µã€è§£å‰–åŒºåŸŸå’Œæˆåƒåè®®å¤šæ ·æ€§ä¸è¶³çš„é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿåˆ›å»ºäº†ä¸€ç§è¡¨å¾å­¦ä¹ æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨è®­ç»ƒæ—¶è‡ªèº«é¢„æµ‹å¼ºé¢†åŸŸæ¼‚ç§»ã€‚ä»–ä»¬é¦–å…ˆæå‡ºä¸€ä¸ªæ•°æ®å¼•æ“ï¼Œåˆæˆé«˜åº¦å¯å˜çš„è®­ç»ƒæ ·æœ¬ï¼Œä»¥å®ç°å¯¹æ–°ç”Ÿç‰©åŒ»å­¦ç¯å¢ƒçš„æ³›åŒ–ã€‚æ¥ç€ï¼Œä¸ºäº†å¯¹ä»»ä½•ä½“ç´ çº§ä»»åŠ¡è¿›è¡Œè®­ç»ƒï¼Œä»–ä»¬å¼€å‘äº†ä¸€ç§å¯¹æ¯”å­¦ä¹ æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯¹ç”±æ•°æ®å¼•æ“æ¨¡æ‹Ÿçš„å¹²æ‰°æˆåƒå˜åŒ–è¿›è¡Œé¢„è®­ç»ƒï¼Œå½¢æˆå¯¹æ³›åŒ–çš„å…³é”®å½’çº³åç½®ã€‚è¯¥ç½‘ç»œçš„ç‰¹å¾å¯ç”¨äºä¸‹æ¸¸ä»»åŠ¡çš„ç¨³å¥è¡¨ç¤ºï¼Œå…¶æƒé‡ä¸ºåœ¨æ–°æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒæä¾›äº†å¼ºå¤§çš„ã€ç‹¬ç«‹äºæ•°æ®é›†åˆå§‹åŒ–ã€‚æœ€ç»ˆï¼Œè¯¥ç ”ç©¶åœ¨å¤šæ¨¡æ€æ³¨å†Œå’Œå°‘æ ·æœ¬åˆ†å‰²ä»»åŠ¡ä¸Šè®¾å®šäº†æ–°çš„æ ‡å‡†ï¼Œæˆä¸ºé¦–ä¸ªæ— éœ€åœ¨çœŸå®å›¾åƒç°æœ‰æ•°æ®é›†ä¸Šè¿›è¡Œï¼ˆé¢„ï¼‰è®­ç»ƒçš„3Dç”Ÿç‰©åŒ»å­¦è§†è§‰æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰ç”Ÿç‰©åŒ»å­¦å›¾åƒé¢†åŸŸçš„æ¨¡å‹æ³›åŒ–èƒ½åŠ›å—é™ï¼Œä¸»è¦ç”±äºå…¬å…±æ•°æ®é›†è§„æ¨¡å°ä¸”å¤šæ ·æ€§ä¸è¶³ã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿæå‡ºä¸€ç§è¡¨å¾å­¦ä¹ æ–¹æ³•ï¼Œèƒ½åœ¨è®­ç»ƒæ—¶é¢„æµ‹å¼ºé¢†åŸŸæ¼‚ç§»ã€‚</li>
<li>é€šè¿‡æ•°æ®å¼•æ“åˆæˆé«˜åº¦å¯å˜çš„è®­ç»ƒæ ·æœ¬ï¼Œä»¥æé«˜æ¨¡å‹å¯¹æ–°ç”Ÿç‰©åŒ»å­¦ç¯å¢ƒçš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>é‡‡ç”¨å¯¹æ¯”å­¦ä¹ æ–¹æ³•è¿›è¡Œé¢„è®­ç»ƒï¼Œå¢å¼ºæ¨¡å‹å¯¹å¹²æ‰°æˆåƒå˜åŒ–çš„ç¨³å®šæ€§ã€‚</li>
<li>è¯¥ç½‘ç»œç‰¹å¾é€‚ç”¨äºä¸‹æ¸¸ä»»åŠ¡ï¼Œå¹¶æä¾›ç¨³å¥çš„å›¾åƒè¡¨ç¤ºã€‚</li>
<li>æ¨¡å‹æƒé‡ä¸ºåœ¨æ–°æ•°æ®é›†ä¸Šçš„å¾®è°ƒæä¾›äº†å¼ºå¤§çš„åˆå§‹åŒ–ï¼Œä¸”ç‹¬ç«‹äºæ•°æ®é›†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.02372">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c8ccbfddae24e54ee188d2aaa59b337c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8829a891f98535325355d33e316e5675.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c296a2b6e427afb423e81fadfd0733cc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eff7484428edeb2c68f2c070333526f4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-62bc1811ed772a16504ff90639918a42.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1f2a62ad11af4a292fdc90828afd524c.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Unleashing-the-Potential-of-Vision-Language-Pre-Training-for-3D-Zero-Shot-Lesion-Segmentation-via-Mask-Attribute-Alignment"><a href="#Unleashing-the-Potential-of-Vision-Language-Pre-Training-for-3D-Zero-Shot-Lesion-Segmentation-via-Mask-Attribute-Alignment" class="headerlink" title="Unleashing the Potential of Vision-Language Pre-Training for 3D   Zero-Shot Lesion Segmentation via Mask-Attribute Alignment"></a>Unleashing the Potential of Vision-Language Pre-Training for 3D   Zero-Shot Lesion Segmentation via Mask-Attribute Alignment</h2><p><strong>Authors:Yankai Jiang, Wenhui Lei, Xiaofan Zhang, Shaoting Zhang</strong></p>
<p>Recent advancements in medical vision-language pre-training models have driven significant progress in zero-shot disease recognition. However, transferring image-level knowledge to pixel-level tasks, such as lesion segmentation in 3D CT scans, remains a critical challenge. Due to the complexity and variability of pathological visual characteristics, existing methods struggle to align fine-grained lesion features not encountered during training with disease-related textual representations. In this paper, we present Malenia, a novel multi-scale lesion-level mask-attribute alignment framework, specifically designed for 3D zero-shot lesion segmentation. Malenia improves the compatibility between mask representations and their associated elemental attributes, explicitly linking the visual features of unseen lesions with the extensible knowledge learned from previously seen ones. Furthermore, we design a Cross-Modal Knowledge Injection module to enhance both visual and textual features with mutually beneficial information, effectively guiding the generation of segmentation results. Comprehensive experiments across three datasets and 12 lesion categories validate the superior performance of Malenia. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼ŒåŒ»å­¦è§†è§‰è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹çš„è¿›æ­¥æ¨åŠ¨äº†é›¶æ ·æœ¬ç–¾ç—…è¯†åˆ«çš„æ˜¾è‘—å‘å±•ã€‚ç„¶è€Œï¼Œå°†å›¾åƒçº§åˆ«çš„çŸ¥è¯†è½¬ç§»åˆ°åƒç´ çº§åˆ«çš„ä»»åŠ¡ï¼Œå¦‚åœ¨3D CTæ‰«æä¸­çš„ç—…ç¶åˆ†å‰²ï¼Œä»ç„¶æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚ç”±äºç—…ç†è§†è§‰ç‰¹å¾çš„å¤æ‚æ€§å’Œå¯å˜æ€§ï¼Œç°æœ‰æ–¹æ³•å¾ˆéš¾å°†è®­ç»ƒæœŸé—´æœªé‡åˆ°çš„ç²¾ç»†ç—…ç¶ç‰¹å¾ä¸ç–¾ç—…ç›¸å…³çš„æ–‡æœ¬è¡¨ç¤ºè¿›è¡Œå¯¹é½ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Maleniaï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å¤šå°ºåº¦ç—…ç¶çº§åˆ«æ©è†œå±æ€§å¯¹é½æ¡†æ¶ï¼Œä¸“ä¸º3Dé›¶æ ·æœ¬ç—…ç¶åˆ†å‰²è®¾è®¡ã€‚Maleniaæé«˜äº†æ©è†œè¡¨ç¤ºä¸å…¶ç›¸å…³å…ƒç´ å±æ€§ä¹‹é—´çš„å…¼å®¹æ€§ï¼Œæ˜ç¡®åœ°å°†æœªè§è¿‡çš„ç—…ç¶çš„è§†è§‰ç‰¹å¾ä¸ä»å·²è§è¿‡çš„ç—…ç¶ä¸­å­¦åˆ°çš„å¯æ‰©å±•çŸ¥è¯†è”ç³»èµ·æ¥ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªè·¨æ¨¡æ€çŸ¥è¯†æ³¨å…¥æ¨¡å—ï¼Œä»¥å¢å¼ºè§†è§‰å’Œæ–‡æœ¬ç‰¹å¾çš„ç›¸äº’æœ‰ç›Šä¿¡æ¯ï¼Œæœ‰æ•ˆæŒ‡å¯¼ç”Ÿæˆåˆ†å‰²ç»“æœã€‚åœ¨ä¸‰ä¸ªæ•°æ®é›†å’Œ12ä¸ªç—…ç¶ç±»åˆ«çš„ç»¼åˆå®éªŒéªŒè¯äº†Maleniaçš„å“è¶Šæ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.15744v2">PDF</a> Accepted as ICLR 2025 conference paper</p>
<p><strong>Summary</strong><br>åŒ»å­¦è§†è§‰è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹çš„æœ€æ–°è¿›å±•æ¨åŠ¨äº†é›¶æ ·æœ¬ç–¾ç—…è¯†åˆ«çš„æ˜¾è‘—è¿›æ­¥ã€‚ç„¶è€Œï¼Œå°†å›¾åƒçº§åˆ«çš„çŸ¥è¯†è½¬ç§»åˆ°åƒç´ çº§åˆ«çš„ä»»åŠ¡ï¼Œå¦‚3D CTæ‰«æä¸­çš„ç—…ç¶åˆ†å‰²ï¼Œä»æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºä¸€ç§åä¸ºMaleniaçš„å¤šå°ºåº¦ç—…ç¶çº§åˆ«æ©è†œå±æ€§å¯¹é½æ¡†æ¶ï¼Œä¸“é—¨ç”¨äº3Dé›¶æ ·æœ¬ç—…ç¶åˆ†å‰²ã€‚è¯¥æ¡†æ¶æé«˜äº†æ©è†œè¡¨ç¤ºä¸å…¶ç›¸å…³åŸºç¡€å±æ€§ä¹‹é—´çš„å…¼å®¹æ€§ï¼Œå°†æœªè§ç—…ç¶çš„è§†è§‰ç‰¹å¾ä¸ä»å·²è§ç—…ç¶ä¸­å­¦åˆ°çš„å¯æ‰©å±•çŸ¥è¯†æ˜ç¡®è”ç³»èµ·æ¥ã€‚æ­¤å¤–ï¼Œè®¾è®¡äº†ä¸€ç§è·¨æ¨¡æ€çŸ¥è¯†æ³¨å…¥æ¨¡å—ï¼Œä»¥å¢å¼ºè§†è§‰å’Œæ–‡æœ¬ç‰¹å¾çš„ç›¸äº’è¡¥å……ä¿¡æ¯ï¼Œæœ‰æ•ˆæŒ‡å¯¼ç”Ÿæˆåˆ†å‰²ç»“æœã€‚åœ¨ä¸‰ä¸ªæ•°æ®é›†å’Œ12ä¸ªç—…ç¶ç±»åˆ«çš„ç»¼åˆå®éªŒä¸­éªŒè¯äº†Maleniaçš„å“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦è§†è§‰è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹çš„æœ€æ–°è¿›å±•æ¨åŠ¨äº†é›¶æ ·æœ¬ç–¾ç—…è¯†åˆ«çš„è¿›æ­¥ã€‚</li>
<li>å›¾åƒçº§åˆ«çš„çŸ¥è¯†è½¬ç§»åˆ°åƒç´ çº§åˆ«çš„ä»»åŠ¡ï¼ˆå¦‚ç—…ç¶åˆ†å‰²ï¼‰ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚</li>
<li>Maleniaæ¡†æ¶ç”¨äº3Dé›¶æ ·æœ¬ç—…ç¶åˆ†å‰²ï¼Œå®ç°å¤šå°ºåº¦ç—…ç¶çº§åˆ«æ©è†œå±æ€§å¯¹é½ã€‚</li>
<li>Maleniaæé«˜äº†æ©è†œè¡¨ç¤ºä¸åŸºç¡€å±æ€§ä¹‹é—´çš„å…¼å®¹æ€§ï¼Œå…³è”æœªè§ä¸å·²è§ç—…ç¶ç‰¹å¾ã€‚</li>
<li>è·¨æ¨¡æ€çŸ¥è¯†æ³¨å…¥æ¨¡å—å¢å¼ºè§†è§‰å’Œæ–‡æœ¬ç‰¹å¾çš„äº’è¡¥ä¿¡æ¯ã€‚</li>
<li>Maleniaåœ¨å¤šä¸ªæ•°æ®é›†å’Œä¸åŒç—…ç¶ç±»åˆ«ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.15744">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-a66de1f1deef3aa8acc368b5f35ed1c9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1263f7f248053b7fc9bc2f10276c5687.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-79eb0f09b6df26e444bcb1f047e947eb.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Improved-Baselines-with-Synchronized-Encoding-for-Universal-Medical-Image-Segmentation"><a href="#Improved-Baselines-with-Synchronized-Encoding-for-Universal-Medical-Image-Segmentation" class="headerlink" title="Improved Baselines with Synchronized Encoding for Universal Medical   Image Segmentation"></a>Improved Baselines with Synchronized Encoding for Universal Medical   Image Segmentation</h2><p><strong>Authors:Sihan Yang, Xuande Mi, Jiadong Feng, Haixia Bi, Hai Zhang, Jian Sun</strong></p>
<p>Large foundation models, known for their strong zero-shot generalization capabilities, can be applied to a wide range of downstream tasks. However, developing foundation models for medical image segmentation poses a significant challenge due to the domain gap between natural and medical images. While fine-tuning techniques based on the Segment Anything Model (SAM) have been explored, they primarily focus on scaling up data or refining inference strategies without incorporating domain-specific architectural designs, limiting their zero-shot performance. To optimize segmentation performance under standard inference settings and provide a strong baseline for future research, we introduce SyncSAM, which employs a synchronized dual-branch encoder that integrates convolution and Transformer features in a synchronized manner to enhance medical image encoding, and a multi-scale dual-branch decoder to preserve image details. SyncSAM is trained on two of the largest medical image segmentation datasets, SA-Med2D-20M and IMed-361M, resulting in a series of pre-trained models for universal medical image segmentation. Experimental results demonstrate that SyncSAM not only achieves state-of-the-art performance on test sets but also exhibits strong zero-shot capabilities on unseen datasets. The code and model weights are available at <a target="_blank" rel="noopener" href="https://github.com/Hhankyangg/SyncSAM">https://github.com/Hhankyangg/SyncSAM</a>. </p>
<blockquote>
<p>å¤§å‹åŸºç¡€æ¨¡å‹ä»¥å…¶å¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›è€Œè‘—ç§°ï¼Œå¯å¹¿æ³›åº”ç”¨äºå„ç§ä¸‹æ¸¸ä»»åŠ¡ã€‚ç„¶è€Œï¼Œé’ˆå¯¹åŒ»å­¦å›¾åƒåˆ†å‰²å¼€å‘åŸºç¡€æ¨¡å‹æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ï¼Œå› ä¸ºè‡ªç„¶å›¾åƒå’ŒåŒ»å­¦å›¾åƒä¹‹é—´å­˜åœ¨é¢†åŸŸå·®è·ã€‚è™½ç„¶åŸºäºSegment Anything Modelï¼ˆSAMï¼‰çš„å¾®è°ƒæŠ€æœ¯å·²è¢«æ¢ç´¢ï¼Œä½†å®ƒä»¬ä¸»è¦å…³æ³¨æ‰©å¤§æ•°æ®è§„æ¨¡æˆ–æ”¹è¿›æ¨ç†ç­–ç•¥ï¼Œè€Œæ²¡æœ‰ç»“åˆé¢†åŸŸç‰¹å®šçš„æ¶æ„è®¾è®¡ï¼Œä»è€Œé™åˆ¶äº†å…¶é›¶æ ·æœ¬æ€§èƒ½ã€‚ä¸ºäº†ä¼˜åŒ–æ ‡å‡†æ¨ç†è®¾ç½®ä¸‹çš„åˆ†å‰²æ€§èƒ½ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›å¼ºå¤§çš„åŸºçº¿ï¼Œæˆ‘ä»¬å¼•å…¥äº†SyncSAMã€‚SyncSAMé‡‡ç”¨åŒæ­¥åŒåˆ†æ”¯ç¼–ç å™¨ï¼Œä»¥åŒæ­¥æ–¹å¼é›†æˆå·ç§¯å’ŒTransformerç‰¹å¾ï¼Œå¢å¼ºåŒ»å­¦å›¾åƒç¼–ç ï¼›å¹¶é‡‡ç”¨å¤šå°ºåº¦åŒåˆ†æ”¯è§£ç å™¨ï¼Œä¿ç•™å›¾åƒç»†èŠ‚ã€‚SyncSAMåœ¨ä¸¤å¤§åŒ»å­¦å›¾åƒåˆ†å‰²æ•°æ®é›†SA-Med2D-20Må’ŒIMed-361Mä¸Šè¿›è¡Œè®­ç»ƒï¼Œç”Ÿæˆäº†ä¸€ç³»åˆ—ç”¨äºé€šç”¨åŒ»å­¦å›¾åƒåˆ†å‰²çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSyncSAMä¸ä»…åœ¨æµ‹è¯•é›†ä¸Šè¾¾åˆ°æœ€æ–°æ€§èƒ½ï¼Œè€Œä¸”åœ¨æœªè§æ•°æ®é›†ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬èƒ½åŠ›ã€‚ç›¸å…³ä»£ç å’Œæ¨¡å‹æƒé‡å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Hhankyangg/SyncSAM%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Hhankyangg/SyncSAMè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.09886v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹åŸºç¡€æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­å…·æœ‰å¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œä½†é¢ä¸´è‡ªç„¶å›¾åƒä¸åŒ»å­¦å›¾åƒé¢†åŸŸå·®å¼‚çš„æŒ‘æˆ˜ã€‚SyncSAMæ¨¡å‹é€šè¿‡åŒæ­¥åŒåˆ†æ”¯ç¼–ç å™¨å’Œå¤šå°ºåº¦åŒåˆ†æ”¯è§£ç å™¨æ¥å¢å¼ºåŒ»å­¦å›¾åƒçš„ç¼–ç å’Œç»†èŠ‚ä¿ç•™ã€‚å®ƒåœ¨ä¸¤ä¸ªæœ€å¤§çš„åŒ»å­¦å›¾åƒåˆ†å‰²æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå®ç°äº†é¢„è®­ç»ƒæ¨¡å‹çš„é€šç”¨åŒ»å­¦å›¾åƒåˆ†å‰²ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSyncSAMåœ¨æµ‹è¯•é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶åœ¨æœªè§æ•°æ®é›†ä¸Šå±•ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹åŸºç¡€æ¨¡å‹å…·å¤‡å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²é¢†åŸŸã€‚</li>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²é¢ä¸´è‡ªç„¶å›¾åƒä¸åŒ»å­¦å›¾åƒé¢†åŸŸå·®å¼‚çš„æŒ‘æˆ˜ã€‚</li>
<li>SyncSAMæ¨¡å‹é€šè¿‡åŒæ­¥åŒåˆ†æ”¯ç¼–ç å™¨é›†æˆå·ç§¯å’ŒTransformerç‰¹å¾ï¼Œä»¥æé«˜åŒ»å­¦å›¾åƒç¼–ç æ•ˆæœã€‚</li>
<li>SyncSAMé‡‡ç”¨å¤šå°ºåº¦åŒåˆ†æ”¯è§£ç å™¨ï¼Œæ—¨åœ¨ä¿ç•™å›¾åƒç»†èŠ‚ã€‚</li>
<li>SyncSAMæ¨¡å‹åœ¨å¤šä¸ªåŒ»å­¦å›¾åƒåˆ†å‰²æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæä¾›é¢„è®­ç»ƒæ¨¡å‹ç”¨äºé€šç”¨åŒ»å­¦å›¾åƒåˆ†å‰²ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºSyncSAMè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶åœ¨æœªè§æ•°æ®é›†ä¸Šå±•ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.09886">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-df46be8b466facf8ba6a6d977ec6624d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b7edef4722fd0880755c51e3ce041f21.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-775c002212b3f8748cc976c4ab39c75c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-545bee0888f2e398ba83261f44f8d9aa.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Real-Time-Image-Analysis-Software-Suitable-for-Resource-Constrained-Computing"><a href="#Real-Time-Image-Analysis-Software-Suitable-for-Resource-Constrained-Computing" class="headerlink" title="Real-Time Image Analysis Software Suitable for Resource-Constrained   Computing"></a>Real-Time Image Analysis Software Suitable for Resource-Constrained   Computing</h2><p><strong>Authors:Alexandre Matov</strong></p>
<p>Methods: We have developed a software suite (DataSet Tracker) for real-time analysis designed to run on computers, smartphones, and smart glasses hardware and suitable for resource-constrained, on-the-fly computing in microscopes without internet connectivity; a demo is available for viewing at datasetanalysis.com. Our objective is to present the community with an integrated, easy to use by all, tool for resolving the complex dynamics of the cytoskeletal meshworks, intracytoplasmic membranous networks, and vesicle trafficking. Our software is optimized for resource-constrained computing and can be installed even on microscopes without internet connectivity.   Results: Our computational platform can provide high-content analyses and functional secondary screening of novel compounds that are in the process of approval, or at a pre-clinical stage of development, and putative combination therapies based on FDA-approved drugs. Importantly, dissecting the mechanisms of drug action with quantitative detail will allow the design of drugs that impede relapse and optimal dose regimens with minimal harmful side effects by carefully exploiting disease-specific aberrations.   Conclusions: DataSet Tracker, the real-time optical flow feature tracking software presented in this contribution, can serve as the base module of an integrated platform of existing and future algorithms for real-time cellular analysis. The computational assay we propose could successfully be applied to evaluate treatment strategies for any human organ. It is our goal to have this integrated tool approved for use in the clinical practice. </p>
<blockquote>
<p><strong>æ–¹æ³•</strong>ï¼šæˆ‘ä»¬å¼€å‘äº†ä¸€å¥—å®æ—¶åˆ†æè½¯ä»¶å¥—ä»¶ï¼ˆDataSet Trackerï¼‰ï¼Œå¯åœ¨è®¡ç®—æœºã€æ™ºèƒ½æ‰‹æœºå’Œæ™ºèƒ½çœ¼é•œç­‰ç¡¬ä»¶ä¸Šè¿è¡Œï¼Œé€‚ç”¨äºæ— ç½‘ç»œè¿æ¥æ˜¾å¾®é•œä¸‹çš„èµ„æºå—é™ã€å³æ—¶è®¡ç®—ã€‚å¯ä»¥åœ¨datasetanalysis.comä¸ŠæŸ¥çœ‹æ¼”ç¤ºç‰ˆã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯å‘ç ”ç©¶ç¾¤ä½“æä¾›ä¸€ä¸ªé›†æˆå·¥å…·ï¼Œè¯¥å·¥å…·æ˜“äºæ‰€æœ‰äººä½¿ç”¨ï¼Œå¯è§£å†³ç»†èƒéª¨æ¶ç½‘æ ¼ã€èƒè´¨å†…è†œç½‘ç»œå’Œå›Šæ³¡è½¬è¿çš„å¤æ‚åŠ¨æ€é—®é¢˜ã€‚æˆ‘ä»¬çš„è½¯ä»¶é’ˆå¯¹èµ„æºå—é™çš„è®¡ç®—è¿›è¡Œäº†ä¼˜åŒ–ï¼Œç”šè‡³å¯ä»¥åœ¨æ²¡æœ‰äº’è”ç½‘è¿æ¥çš„æ˜¾å¾®é•œä¸Šå®‰è£…ä½¿ç”¨ã€‚</p>
</blockquote>
<p><strong>ç»“æœ</strong>ï¼šæˆ‘ä»¬çš„è®¡ç®—å¹³å°å¯ä»¥æä¾›é«˜å†…æ¶µåˆ†æä»¥åŠæ–°è¯ç­›é€‰çš„åŠŸèƒ½äºŒæ¬¡ç­›é€‰ï¼Œè¿™äº›æ–°è¯æ­£å¤„äºå®¡æ‰¹è¿‡ç¨‹ä¸­æˆ–å¤„äºå¼€å‘é¢„ä¸´åºŠé˜¶æ®µï¼Œä»¥åŠåŸºäºFDAæ‰¹å‡†è¯ç‰©çš„ç»„åˆç–—æ³•ã€‚é‡è¦çš„æ˜¯ï¼Œé€šè¿‡å®šé‡ç»†èŠ‚åˆ†æè¯ç‰©ä½œç”¨æœºåˆ¶ï¼Œå°†èƒ½å¤Ÿè®¾è®¡å‡ºé˜»æ­¢å¤å‘çš„è¯ç‰©ï¼Œå¹¶å€ŸåŠ©ç²¾å‡†åœ°åˆ©ç”¨ç–¾ç—…ç‰¹å¼‚æ€§å¼‚å¸¸æ¥åˆ¶å®šæœ€ä½³æ²»ç–—æ–¹æ¡ˆå’Œæœ€å°æœ‰å®³å‰¯ä½œç”¨çš„å‰‚é‡æ–¹æ¡ˆã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.15735v7">PDF</a> </p>
<p><strong>Summary</strong><br>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç”¨äºå®æ—¶åˆ†æçš„è½¯ä»¶å¥—ä»¶DataSet Trackerï¼Œé€‚ç”¨äºèµ„æºå—é™ç¯å¢ƒä¸‹æ— ç½‘ç»œè¿æ¥æ˜¾å¾®é•œçš„åœ¨çº¿è®¡ç®—ã€‚è½¯ä»¶å…·æœ‰ä¼˜åŒ–èµ„æºæ¶ˆè€—çš„ç‰¹ç‚¹ï¼Œæ—¨åœ¨ä¸ºç¤¾åŒºæä¾›ä¸€ä¸ªç»¼åˆå·¥å…·ï¼Œè§£å†³ç»†èƒéª¨æ¶ç½‘æ ¼ã€ç»†èƒå†…è†œç½‘ç»œå’Œå›Šæ³¡è¿è¾“çš„å¤æ‚åŠ¨æ€é—®é¢˜ã€‚è¯¥è½¯ä»¶å¯åº”ç”¨äºæ–°è¯ç­›é€‰å’Œè¯ç‰©ä½œç”¨æœºåˆ¶å®šé‡ç ”ç©¶ï¼Œæœ‰åŠ©äºè®¾è®¡é˜²æ­¢å¤å‘çš„è¯ç‰©å¹¶åˆ¶å®šæœ€ä½³å‰‚é‡æ–¹æ¡ˆã€‚DataSet Trackerå¯ä»¥ä½œä¸ºç°æœ‰å’Œæœªæ¥ç®—æ³•çš„é›†æˆå¹³å°çš„åŸºç¡€æ¨¡å—ï¼Œç”¨äºå®æ—¶ç»†èƒåˆ†æï¼ŒæˆåŠŸåº”ç”¨äºäººç±»å™¨å®˜æ²»ç–—ç­–ç•¥è¯„ä¼°ï¼Œæœ‰æœ›åœ¨ä¸´åºŠå®è·µä¸­å¾—åˆ°åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¼€å‘äº†ä¸€ç§å®æ—¶åˆ†æè½¯ä»¶å¥—ä»¶DataSet Trackerï¼Œå¯åœ¨è®¡ç®—æœºã€æ™ºèƒ½æ‰‹æœºå’Œæ™ºèƒ½çœ¼é•œç¡¬ä»¶ä¸Šè¿è¡Œã€‚</li>
<li>é€‚ç”¨äºèµ„æºå—é™ã€æ— ç½‘ç»œè¿æ¥æ˜¾å¾®é•œçš„åœ¨çº¿è®¡ç®—ç¯å¢ƒã€‚</li>
<li>è½¯ä»¶æ—¨åœ¨è§£å†³ç»†èƒéª¨æ¶ç½‘æ ¼ã€ç»†èƒå†…è†œç½‘ç»œå’Œå›Šæ³¡è¿è¾“ç­‰é¢†åŸŸçš„å¤æ‚åŠ¨æ€é—®é¢˜ã€‚</li>
<li>DataSet Trackerèƒ½è¿›è¡Œé«˜å†…å®¹åˆ†æå¹¶å¯¹æ–°è¯å’Œç»„åˆç–—æ³•è¿›è¡ŒåŠŸèƒ½æ€§äºŒçº§ç­›é€‰ã€‚</li>
<li>è½¯ä»¶èƒ½å®šé‡ç ”ç©¶è¯ç‰©ä½œç”¨æœºåˆ¶ï¼Œæœ‰åŠ©äºè®¾è®¡é˜²æ­¢å¤å‘çš„è¯ç‰©å¹¶åˆ¶å®šæœ€ä½³å‰‚é‡æ–¹æ¡ˆã€‚</li>
<li>DataSet Trackerå¯ä½œä¸ºé›†æˆå¹³å°çš„åŸºçŸ³æ¨¡å—ï¼Œç”¨äºå®æ—¶ç»†èƒåˆ†æï¼Œé€‚ç”¨äºè¯„ä¼°ä»»ä½•äººç±»å™¨å®˜çš„æ²»ç–—ç­–ç•¥ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.15735">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-94f1386177611dc8371b3965f53e31eb.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="HDKD-Hybrid-Data-Efficient-Knowledge-Distillation-Network-for-Medical-Image-Classification"><a href="#HDKD-Hybrid-Data-Efficient-Knowledge-Distillation-Network-for-Medical-Image-Classification" class="headerlink" title="HDKD: Hybrid Data-Efficient Knowledge Distillation Network for Medical   Image Classification"></a>HDKD: Hybrid Data-Efficient Knowledge Distillation Network for Medical   Image Classification</h2><p><strong>Authors:Omar S. EL-Assiouti, Ghada Hamed, Dina Khattab, Hala M. Ebied</strong></p>
<p>Vision Transformers (ViTs) have achieved significant advancement in computer vision tasks due to their powerful modeling capacity. However, their performance notably degrades when trained with insufficient data due to lack of inherent inductive biases. Distilling knowledge and inductive biases from a Convolutional Neural Network (CNN) teacher has emerged as an effective strategy for enhancing the generalization of ViTs on limited datasets. Previous approaches to Knowledge Distillation (KD) have pursued two primary paths: some focused solely on distilling the logit distribution from CNN teacher to ViT student, neglecting the rich semantic information present in intermediate features due to the structural differences between them. Others integrated feature distillation along with logit distillation, yet this introduced alignment operations that limits the amount of knowledge transferred due to mismatched architectures and increased the computational overhead. To this end, this paper presents Hybrid Data-efficient Knowledge Distillation (HDKD) paradigm which employs a CNN teacher and a hybrid student. The choice of hybrid student serves two main aspects. First, it leverages the strengths of both convolutions and transformers while sharing the convolutional structure with the teacher model. Second, this shared structure enables the direct application of feature distillation without any information loss or additional computational overhead. Additionally, we propose an efficient light-weight convolutional block named Mobile Channel-Spatial Attention (MBCSA), which serves as the primary convolutional block in both teacher and student models. Extensive experiments on two medical public datasets showcase the superiority of HDKD over other state-of-the-art models and its computational efficiency. Source code at: <a target="_blank" rel="noopener" href="https://github.com/omarsherif200/HDKD">https://github.com/omarsherif200/HDKD</a> </p>
<blockquote>
<p>è§†è§‰Transformerï¼ˆViTsï¼‰ç”±äºå…¶å¼ºå¤§çš„å»ºæ¨¡èƒ½åŠ›ï¼Œåœ¨è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„è¿›å±•ã€‚ç„¶è€Œï¼Œåœ¨æ•°æ®ä¸è¶³çš„æƒ…å†µä¸‹è¿›è¡Œè®­ç»ƒæ—¶ï¼Œç”±äºå…¶ç¼ºä¹å›ºæœ‰çš„å½’çº³åç½®ï¼Œå…¶æ€§èƒ½ä¼šæ˜¾è‘—ä¸‹é™ã€‚ä»å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ•™å¸ˆä¸­æç‚¼çŸ¥è¯†å’Œå½’çº³åç½®ï¼Œå·²æˆä¸ºæé«˜ViTåœ¨æœ‰é™æ•°æ®é›†ä¸Šæ³›åŒ–èƒ½åŠ›çš„æœ‰æ•ˆç­–ç•¥ã€‚çŸ¥è¯†è’¸é¦ï¼ˆKDï¼‰çš„å…ˆå‰æ–¹æ³•ä¸»è¦è¿½æ±‚ä¸¤ç§é€”å¾„ï¼šä¸€äº›æ–¹æ³•ä¸“æ³¨äºä»CNNæ•™å¸ˆè’¸é¦logitåˆ†å¸ƒåˆ°ViTå­¦ç”Ÿï¼Œå¿½è§†äº†ç”±äºç»“æ„å·®å¼‚è€Œå­˜åœ¨äºä¸­é—´ç‰¹å¾ä¸­çš„ä¸°å¯Œè¯­ä¹‰ä¿¡æ¯ã€‚å¦ä¸€äº›æ–¹æ³•ç»“åˆäº†ç‰¹å¾è’¸é¦å’Œlogitè’¸é¦ï¼Œä½†è¿™å¼•å…¥äº†å¯¹é½æ“ä½œï¼Œç”±äºæ¶æ„ä¸åŒ¹é…è€Œé™åˆ¶äº†çŸ¥è¯†è½¬ç§»çš„é‡å¹¶å¢åŠ äº†è®¡ç®—å¼€é”€ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†æ··åˆæ•°æ®é«˜æ•ˆçŸ¥è¯†è’¸é¦ï¼ˆHDKDï¼‰èŒƒå¼ï¼Œè¯¥èŒƒå¼é‡‡ç”¨CNNæ•™å¸ˆå’Œæ··åˆå­¦ç”Ÿã€‚é€‰æ‹©æ··åˆå­¦ç”Ÿæœ‰ä¸¤ä¸ªä¸»è¦æ–¹é¢ã€‚é¦–å…ˆï¼Œå®ƒç»“åˆäº†å·ç§¯å’Œå˜å‹å™¨çš„ä¼˜ç‚¹ï¼ŒåŒæ—¶ä¸æ•™å¸ˆçš„æ¨¡å‹å…±äº«å·ç§¯ç»“æ„ã€‚å…¶æ¬¡ï¼Œè¿™ç§å…±äº«ç»“æ„ä½¿ç‰¹å¾è’¸é¦èƒ½å¤Ÿç›´æ¥åº”ç”¨ï¼Œæ²¡æœ‰ä»»ä½•ä¿¡æ¯æŸå¤±æˆ–é¢å¤–çš„è®¡ç®—å¼€é”€ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„è½»é‡çº§å·ç§¯å—ï¼Œåä¸ºç§»åŠ¨é€šé“ç©ºé—´æ³¨æ„åŠ›ï¼ˆMBCSAï¼‰ï¼Œå®ƒä½œä¸ºæ•™å¸ˆå’Œå­¦ç”Ÿæ¨¡å‹ä¸­ä¸»è¦çš„å·ç§¯å—ã€‚åœ¨ä¸¤ä¸ªåŒ»å­¦å…¬å…±æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒHDKDä¼˜äºå…¶ä»–æœ€å…ˆè¿›æ¨¡å‹ä¸”è®¡ç®—æ•ˆç‡æ›´é«˜ã€‚æºä»£ç ä½äºï¼š<a target="_blank" rel="noopener" href="https://github.com/omarsherif200/HDKD">https://github.com/omarsherif200/HDKD</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.07516v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Vision Transformersï¼ˆViTsï¼‰åœ¨é¢ä¸´æ•°æ®ä¸è¶³æ—¶æ€§èƒ½ä¸‹é™çš„é—®é¢˜ï¼Œä»¥åŠé€šè¿‡ä»å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ•™å¸ˆæ¨¡å‹ä¸­è’¸é¦çŸ¥è¯†å’Œå½’çº³åç½®æ¥å¢å¼ºViTå­¦ç”Ÿåœ¨æœ‰é™æ•°æ®é›†ä¸Šçš„æ³›åŒ–èƒ½åŠ›çš„æ–¹æ³•ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§æ–°å‹çš„æ··åˆæ•°æ®é«˜æ•ˆçŸ¥è¯†è’¸é¦ï¼ˆHDKDï¼‰èŒƒå¼ï¼Œå®ƒé‡‡ç”¨CNNæ•™å¸ˆæ¨¡å‹å’Œæ··åˆå­¦ç”Ÿæ¨¡å‹ï¼Œé€šè¿‡å…±äº«ç»“æ„å®ç°äº†ç‰¹å¾è’¸é¦çš„ç›´æ¥åº”ç”¨ï¼Œæé«˜äº†çŸ¥è¯†è½¬ç§»çš„æ•ˆç‡ï¼ŒåŒæ—¶é™ä½äº†è®¡ç®—å¼€é”€ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜æå‡ºäº†ä¸€ç§è½»é‡çº§çš„å·ç§¯å—â€”â€”Mobile Channel-Spatial Attentionï¼ˆMBCSAï¼‰ï¼Œå®ƒåœ¨æ•™å¸ˆå’Œå­¦ç”Ÿæ¨¡å‹ä¸­éƒ½èµ·åˆ°äº†å…³é”®ä½œç”¨ã€‚åœ¨åŒ»ç–—å…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒHDKDä¼˜äºå…¶ä»–å…ˆè¿›æ¨¡å‹ï¼Œå¹¶å…·æœ‰è®¡ç®—æ•ˆç‡é«˜çš„ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision Transformers (ViTs) é¢ä¸´æ•°æ®ä¸è¶³æ—¶æ€§èƒ½ä¸‹é™çš„é—®é¢˜ã€‚</li>
<li>çŸ¥è¯†è’¸é¦æ˜¯ä¸€ç§æé«˜ViTåœ¨æœ‰é™æ•°æ®é›†ä¸Šæ³›åŒ–èƒ½åŠ›çš„æœ‰æ•ˆç­–ç•¥ã€‚</li>
<li>ä»¥å¾€çš„çŸ¥è¯†è’¸é¦ï¼ˆKDï¼‰æ–¹æ³•ä¸»è¦è¿½æ±‚ä¸¤ç§è·¯å¾„ï¼šä¸€ç§åªå…³æ³¨ä»CNNæ•™å¸ˆæ¨¡å‹åˆ°ViTå­¦ç”Ÿæ¨¡å‹çš„logitåˆ†å¸ƒè’¸é¦ï¼Œå¿½è§†äº†ç”±äºç»“æ„å·®å¼‚è€Œå­˜åœ¨çš„ä¸­é—´ç‰¹å¾ä¸­çš„ä¸°å¯Œè¯­ä¹‰ä¿¡æ¯ï¼›å¦ä¸€ç§ç»“åˆäº†ç‰¹å¾è’¸é¦å’Œlogitè’¸é¦ï¼Œä½†å¼•å…¥äº†ç”±äºæ¶æ„ä¸åŒ¹é…è€Œé™åˆ¶çŸ¥è¯†ä¼ é€’çš„å¯¹é½æ“ä½œï¼Œå¹¶å¢åŠ äº†è®¡ç®—å¼€é”€ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†Hybrid Data-efficient Knowledge Distillation (HDKD) èŒƒå¼ï¼Œé‡‡ç”¨CNNæ•™å¸ˆæ¨¡å‹å’Œæ··åˆå­¦ç”Ÿæ¨¡å‹ï¼Œé€šè¿‡å…±äº«ç»“æ„å®ç°ç‰¹å¾è’¸é¦çš„ç›´æ¥åº”ç”¨ã€‚</li>
<li>HDKDé‡‡ç”¨è½»é‡çº§çš„å·ç§¯å—Mobile Channel-Spatial Attention (MBCSA)ï¼Œåœ¨æ•™å¸ˆå’Œå­¦ç”Ÿæ¨¡å‹ä¸­éƒ½èµ·åˆ°äº†å…³é”®ä½œç”¨ã€‚</li>
<li>åœ¨åŒ»ç–—å…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒHDKDä¼˜äºå…¶ä»–å…ˆè¿›æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.07516">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7e71ca9e34d27fc62cd6072dfebb598d.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="MERIT-Multi-view-evidential-learning-for-reliable-and-interpretable-liver-fibrosis-staging"><a href="#MERIT-Multi-view-evidential-learning-for-reliable-and-interpretable-liver-fibrosis-staging" class="headerlink" title="MERIT: Multi-view evidential learning for reliable and interpretable   liver fibrosis staging"></a>MERIT: Multi-view evidential learning for reliable and interpretable   liver fibrosis staging</h2><p><strong>Authors:Yuanye Liu, Zheyao Gao, Nannan Shi, Fuping Wu, Yuxin Shi, Qingchao Chen, Xiahai Zhuang</strong></p>
<p>Accurate staging of liver fibrosis from magnetic resonance imaging (MRI) is crucial in clinical practice. While conventional methods often focus on a specific sub-region, multi-view learning captures more information by analyzing multiple patches simultaneously. However, previous multi-view approaches could not typically calculate uncertainty by nature, and they generally integrate features from different views in a black-box fashion, hence compromising reliability as well as interpretability of the resulting models. In this work, we propose a new multi-view method based on evidential learning, referred to as MERIT, which tackles the two challenges in a unified framework. MERIT enables uncertainty quantification of the predictions to enhance reliability, and employs a logic-based combination rule to improve interpretability. Specifically, MERIT models the prediction from each sub-view as an opinion with quantified uncertainty under the guidance of the subjective logic theory. Furthermore, a distribution-aware base rate is introduced to enhance performance, particularly in scenarios involving class distribution shifts. Finally, MERIT adopts a feature-specific combination rule to explicitly fuse multi-view predictions, thereby enhancing interpretability. Results have showcased the effectiveness of the proposed MERIT, highlighting the reliability and offering both ad-hoc and post-hoc interpretability. They also illustrate that MERIT can elucidate the significance of each view in the decision-making process for liver fibrosis staging. Our code has be released via <a target="_blank" rel="noopener" href="https://github.com/HenryLau7/MERIT">https://github.com/HenryLau7/MERIT</a>. </p>
<blockquote>
<p>è‚è„çº¤ç»´åŒ–çš„ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ç²¾ç¡®åˆ†æœŸåœ¨ä¸´åºŠå®è·µä¸­è‡³å…³é‡è¦ã€‚ä¼ ç»Ÿæ–¹æ³•å¾€å¾€ä¸“æ³¨äºç‰¹å®šå­åŒºåŸŸï¼Œè€Œå¤šè§†è§’å­¦ä¹ é€šè¿‡åŒæ—¶åˆ†æå¤šä¸ªè¡¥ä¸æ¥æ•è·æ›´å¤šä¿¡æ¯ã€‚ç„¶è€Œï¼Œå…ˆå‰çš„å¤šè§†è§’æ–¹æ³•é€šå¸¸ä¸èƒ½è‡ªç„¶åœ°è®¡ç®—ä¸ç¡®å®šæ€§ï¼Œå®ƒä»¬ä¸€èˆ¬ä»¥é»‘ç®±æ–¹å¼æ•´åˆä¸åŒè§†è§’çš„ç‰¹å¾ï¼Œä»è€ŒæŸå®³äº†æ¨¡å‹çš„å¯é æ€§å’Œè§£é‡Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.02918v2">PDF</a> Accepted by Medical Image Analysis</p>
<p><strong>æ‘˜è¦</strong></p>
<p>åŸºäºç£å…±æŒ¯æˆåƒå¯¹è‚çº¤ç»´åŒ–è¿›è¡Œå‡†ç¡®çš„åˆ†æœŸåœ¨ä¸´åºŠå®è·µä¸­éå¸¸é‡è¦ã€‚ä¼ ç»Ÿæ–¹æ³•é€šå¸¸åªå…³æ³¨ç‰¹å®šå­åŒºåŸŸï¼Œè€Œå¤šè§†è§’å­¦ä¹ é€šè¿‡åŒæ—¶åˆ†æå¤šä¸ªè¡¥ä¸æ•è·æ›´å¤šä¿¡æ¯ã€‚ç„¶è€Œï¼Œä»¥å‰çš„å¤šè§†è§’æ–¹æ³•é€šå¸¸æ— æ³•è®¡ç®—ä¸ç¡®å®šæ€§ï¼Œå®ƒä»¬é€šå¸¸ä»¥é»‘ç®±æ–¹å¼æ•´åˆä¸åŒè§†è§’çš„ç‰¹å¾ï¼Œä»è€ŒæŸå®³æ¨¡å‹çš„å¯é æ€§å’Œè§£é‡Šæ€§ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºè¯æ®å­¦ä¹ çš„æ–°å¤šè§†è§’æ–¹æ³•ï¼Œç§°ä¸ºMERITï¼Œè¯¥æ–¹æ³•åœ¨ç»Ÿä¸€æ¡†æ¶å†…è§£å†³äº†è¿™ä¸¤ä¸ªæŒ‘æˆ˜ã€‚MERITèƒ½å¤Ÿå¯¹é¢„æµ‹è¿›è¡Œä¸ç¡®å®šæ€§é‡åŒ–ï¼Œä»¥æé«˜å¯é æ€§ï¼Œå¹¶é‡‡ç”¨åŸºäºé€»è¾‘çš„ç»„åˆè§„åˆ™æ¥æé«˜è§£é‡Šæ€§ã€‚å…·ä½“æ¥è¯´ï¼ŒMERITå°†æ¯ä¸ªå­è§†è§’çš„é¢„æµ‹å»ºæ¨¡ä¸ºå…·æœ‰é‡åŒ–ä¸ç¡®å®šæ€§çš„è§‚ç‚¹ï¼Œåœ¨ä¸»è§‚é€»è¾‘ç†è®ºçš„æŒ‡å¯¼ä¸‹è¿›è¡Œã€‚æ­¤å¤–ï¼Œå¼•å…¥äº†ä¸€ç§æ„ŸçŸ¥åˆ†å¸ƒçš„åŸºçº¿ç‡ï¼Œä»¥æé«˜æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¶‰åŠç±»åˆ«åˆ†å¸ƒè½¬ç§»çš„åœºæ™¯ä¸­ã€‚æœ€åï¼ŒMERITé‡‡ç”¨ç‰¹å¾ç‰¹å®šçš„ç»„åˆè§„åˆ™æ¥æ˜¾å¼èåˆå¤šè§†è§’é¢„æµ‹ï¼Œä»è€Œæé«˜äº†è§£é‡Šæ€§ã€‚ç»“æœå±•ç¤ºäº†æ‰€æå‡ºMERITçš„æœ‰æ•ˆæ€§ï¼Œçªå‡ºäº†å…¶å¯é æ€§å’Œæä¾›äº†ä¸“é¡¹åŠäº‹åçš„è§£é‡Šæ€§ã€‚ä»–ä»¬è¿˜è¡¨æ˜ï¼ŒMERITå¯ä»¥é˜æ˜å†³ç­–è¿‡ç¨‹ä¸­æ¯ä¸ªè§†è§’å¯¹è‚çº¤ç»´åŒ–åˆ†æœŸçš„é‡è¦æ€§ã€‚æˆ‘ä»¬çš„ä»£ç å·²é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/HenryLau7/MERIT%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/HenryLau7/MERITå‘å¸ƒã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤šè§†è§’å­¦ä¹ æ–¹æ³•åœ¨è‚çº¤ç»´åŒ–MRIåˆ†æœŸä¸­è‡³å…³é‡è¦ï¼Œèƒ½å¤Ÿæ•è·æ›´å¤šä¿¡æ¯ã€‚</li>
<li>ä»¥å¾€çš„å¤šè§†è§’æ–¹æ³•æ— æ³•è®¡ç®—ä¸ç¡®å®šæ€§ï¼Œå½±å“æ¨¡å‹å¯é æ€§å’Œè§£é‡Šæ€§ã€‚</li>
<li>MERITæ–¹æ³•åŸºäºè¯æ®å­¦ä¹ ï¼Œç»Ÿä¸€è§£å†³ä¸ç¡®å®šæ€§å’Œè§£é‡Šæ€§é—®é¢˜ã€‚</li>
<li>MERITå°†é¢„æµ‹å»ºæ¨¡ä¸ºå…·æœ‰é‡åŒ–ä¸ç¡®å®šæ€§çš„è§‚ç‚¹ï¼Œæé«˜å†³ç­–å¯é æ€§ã€‚</li>
<li>MERITå¼•å…¥åˆ†å¸ƒæ„ŸçŸ¥åŸºçº¿ç‡ï¼Œä¼˜åŒ–ç±»åˆ«åˆ†å¸ƒè½¬ç§»åœºæ™¯ä¸­çš„æ€§èƒ½ã€‚</li>
<li>MERITé‡‡ç”¨ç‰¹å¾ç‰¹å®šç»„åˆè§„åˆ™ï¼Œæ˜¾å¼èåˆå¤šè§†è§’é¢„æµ‹ï¼Œæé«˜è§£é‡Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.02918">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7d245294d2efba3fbb2be550bf369f50.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f3540980b296a4fc96511cea600767f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-36d88e6ceac5f6a5423e2b7339cc3cee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ffa06f09353eaf645adaa4f182f8395.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-833142ac404ae3266dbe7c2c4b63d3b3.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="CromSS-Cross-modal-pre-training-with-noisy-labels-for-remote-sensing-image-segmentation"><a href="#CromSS-Cross-modal-pre-training-with-noisy-labels-for-remote-sensing-image-segmentation" class="headerlink" title="CromSS: Cross-modal pre-training with noisy labels for remote sensing   image segmentation"></a>CromSS: Cross-modal pre-training with noisy labels for remote sensing   image segmentation</h2><p><strong>Authors:Chenying Liu, Conrad Albrecht, Yi Wang, Xiao Xiang Zhu</strong></p>
<p>We explore the potential of large-scale noisily labeled data to enhance feature learning by pretraining semantic segmentation models within a multi-modal framework for geospatial applications. We propose a novel Cross-modal Sample Selection (CromSS) method, a weakly supervised pretraining strategy designed to improve feature representations through cross-modal consistency and noise mitigation techniques. Unlike conventional pretraining approaches, CromSS exploits massive amounts of noisy and easy-to-come-by labels for improved feature learning beneficial to semantic segmentation tasks. We investigate middle and late fusion strategies to optimize the multi-modal pretraining architecture design. We also introduce a cross-modal sample selection module to mitigate the adverse effects of label noise, which employs a cross-modal entangling strategy to refine the estimated confidence masks within each modality to guide the sampling process. Additionally, we introduce a spatial-temporal label smoothing technique to counteract overconfidence for enhanced robustness against noisy labels. To validate our approach, we assembled the multi-modal dataset, NoLDO-S12, which consists of a large-scale noisy label subset from Googleâ€™s Dynamic World (DW) dataset for pretraining and two downstream subsets with high-quality labels from Google DW and OpenStreetMap (OSM) for transfer learning. Experimental results on two downstream tasks and the publicly available DFC2020 dataset demonstrate that when effectively utilized, the low-cost noisy labels can significantly enhance feature learning for segmentation tasks. All data, code, and pretrained weights will be made publicly available. </p>
<blockquote>
<p>æˆ‘ä»¬æ¢ç´¢å¤§è§„æ¨¡å™ªå£°æ ‡è®°æ•°æ®åœ¨åœ°ç†åº”ç”¨çš„å¤šæ¨¡æ€æ¡†æ¶å†…é€šè¿‡é¢„è®­ç»ƒè¯­ä¹‰åˆ†å‰²æ¨¡å‹ä»¥å¢å¼ºç‰¹å¾å­¦ä¹ çš„æ½œåŠ›ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„è·¨æ¨¡æ€æ ·æœ¬é€‰æ‹©ï¼ˆCromSSï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§å¼±ç›‘ç£é¢„è®­ç»ƒç­–ç•¥ï¼Œæ—¨åœ¨é€šè¿‡è·¨æ¨¡æ€ä¸€è‡´æ€§å’Œé™å™ªæŠ€æœ¯æ”¹è¿›ç‰¹å¾è¡¨ç¤ºã€‚ä¸ä¼ ç»Ÿçš„é¢„è®­ç»ƒæ–¹æ³•ä¸åŒï¼ŒCromSSåˆ©ç”¨å¤§é‡çš„å™ªå£°å’Œæ˜“è·å¾—çš„æ ‡ç­¾æ¥æ”¹è¿›ç‰¹å¾å­¦ä¹ ï¼Œè¿™å¯¹è¯­ä¹‰åˆ†å‰²ä»»åŠ¡æœ‰ç›Šã€‚æˆ‘ä»¬ç ”ç©¶äº†ä¸­é—´å’ŒåæœŸèåˆç­–ç•¥æ¥ä¼˜åŒ–å¤šæ¨¡æ€é¢„è®­ç»ƒæ¶æ„è®¾è®¡ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªè·¨æ¨¡æ€æ ·æœ¬é€‰æ‹©æ¨¡å—æ¥ç¼“è§£æ ‡ç­¾å™ªå£°çš„ä¸åˆ©å½±å“ï¼Œè¯¥æ¨¡å—é‡‡ç”¨è·¨æ¨¡æ€çº ç¼ ç­–ç•¥æ¥ç»†åŒ–æ¯ä¸ªæ¨¡æ€å†…çš„ä¼°è®¡ç½®ä¿¡æ©ç ï¼Œä»¥æŒ‡å¯¼é‡‡æ ·è¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ—¶ç©ºæ ‡ç­¾å¹³æ»‘æŠ€æœ¯æ¥å¯¹æŠ—è¿‡åº¦è‡ªä¿¡ï¼Œä»¥å¢å¼ºå¯¹å™ªå£°æ ‡ç­¾çš„ç¨³å¥æ€§ã€‚ä¸ºäº†éªŒè¯æˆ‘ä»¬çš„æ–¹æ³•ï¼Œæˆ‘ä»¬åˆ›å»ºäº†å¤šæ¨¡æ€æ•°æ®é›†NoLDO-S12ï¼Œå®ƒç”±æ¥è‡ªGoogleåŠ¨æ€ä¸–ç•Œï¼ˆDWï¼‰æ•°æ®é›†çš„å¤§è§„æ¨¡å™ªå£°æ ‡ç­¾å­é›†ç»„æˆï¼Œç”¨äºé¢„è®­ç»ƒï¼Œä»¥åŠä¸¤ä¸ªæ¥è‡ªGoogle DWå’ŒOpenStreetMapï¼ˆOSMï¼‰çš„é«˜è´¨é‡æ ‡ç­¾çš„ä¸‹æ¸¸å­é›†ç”¨äºè¿ç§»å­¦ä¹ ã€‚åœ¨ä¸¤ä¸ªä¸‹æ¸¸ä»»åŠ¡å’Œå…¬å¼€çš„DFC2020æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œå½“æœ‰æ•ˆåˆ©ç”¨æ—¶ï¼Œä½æˆæœ¬å™ªå£°æ ‡ç­¾å¯ä»¥æ˜¾è‘—å¢å¼ºåˆ†å‰²ä»»åŠ¡çš„ç‰¹å¾å­¦ä¹ ã€‚æ‰€æœ‰æ•°æ®ã€ä»£ç å’Œé¢„è®­ç»ƒæƒé‡å°†å…¬å¼€å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.01217v2">PDF</a> The 1st short version was accepted as an oral presentation by ICLR   2024 ML4RS workshop. The 2nd extended version is being under review</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬ç ”ç©¶æ¢è®¨å¤§è§„æ¨¡å™ªå£°æ ‡ç­¾æ•°æ®åœ¨åœ°ç†åº”ç”¨ä¸­çš„æ½œåŠ›ï¼Œé€šè¿‡å¤šæ¨¡æ€æ¡†æ¶é¢„è®­ç»ƒè¯­ä¹‰åˆ†å‰²æ¨¡å‹ä»¥å¢å¼ºç‰¹å¾å­¦ä¹ ã€‚æå‡ºä¸€ç§æ–°é¢–çš„è·¨æ¨¡æ€æ ·æœ¬é€‰æ‹©ï¼ˆCromSSï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§å¼±ç›‘ç£é¢„è®­ç»ƒç­–ç•¥ï¼Œæ—¨åœ¨é€šè¿‡è·¨æ¨¡æ€ä¸€è‡´æ€§å’Œå™ªå£°æŠ‘åˆ¶æŠ€æœ¯æ”¹è¿›ç‰¹å¾è¡¨ç¤ºã€‚ä¸åŒäºä¼ ç»Ÿé¢„è®­ç»ƒæ–¹å¼ï¼ŒCromSSåˆ©ç”¨å¤§é‡æ˜“äºè·å–ä¸”å¸¦æœ‰å™ªå£°çš„æ ‡ç­¾æ¥æ”¹å–„ç‰¹å¾å­¦ä¹ ï¼Œå¯¹è¯­ä¹‰åˆ†å‰²ä»»åŠ¡æœ‰ç›Šã€‚ç ”ç©¶ä¸­æ¢è®¨äº†ä¸­æœŸå’ŒåæœŸèåˆç­–ç•¥ä»¥ä¼˜åŒ–å¤šæ¨¡æ€é¢„è®­ç»ƒæ¶æ„è®¾è®¡ã€‚è¿˜å¼•å…¥è·¨æ¨¡æ€æ ·æœ¬é€‰æ‹©æ¨¡å—æ¥å‡è½»æ ‡ç­¾å™ªå£°çš„ä¸åˆ©å½±å“ï¼Œé‡‡ç”¨è·¨æ¨¡æ€çº ç¼ ç­–ç•¥æ¥ä¼˜åŒ–æ¯ä¸ªæ¨¡æ€çš„ä¼°è®¡ç½®ä¿¡åº¦æ©è†œï¼Œä»¥æŒ‡å¯¼é‡‡æ ·è¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œå¼•å…¥ç©ºé—´æ—¶é—´æ ‡ç­¾å¹³æ»‘æŠ€æœ¯æ¥å¯¹æŠ—è¿‡åº¦è‡ªä¿¡ï¼Œæé«˜å¯¹å™ªå£°æ ‡ç­¾çš„ç¨³å¥æ€§ã€‚ä¸ºéªŒè¯æ–¹æ³•ï¼Œæˆ‘ä»¬æ„å»ºäº†å¤šæ¨¡æ€æ•°æ®é›†NoLDO-S12ï¼Œå…¶ä¸­åŒ…æ‹¬æ¥è‡ªGoogleåŠ¨æ€ä¸–ç•Œï¼ˆDWï¼‰æ•°æ®é›†çš„å¤§è§„æ¨¡å™ªå£°æ ‡ç­¾å­é›†ç”¨äºé¢„è®­ç»ƒï¼Œä»¥åŠæ¥è‡ªGoogle DWå’ŒOpenStreetMapï¼ˆOSMï¼‰çš„é«˜è´¨é‡æ ‡ç­¾ä¸‹æ¸¸å­é›†ç”¨äºè¿ç§»å­¦ä¹ ã€‚åœ¨ä¸¤ä¸ªä¸‹æ¸¸ä»»åŠ¡å’Œå…¬å¼€çš„DFC2020æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œå½“æœ‰æ•ˆåˆ©ç”¨æ—¶ï¼Œä½æˆæœ¬å™ªå£°æ ‡ç­¾å¯ä»¥æ˜¾è‘—å¢å¼ºåˆ†å‰²ä»»åŠ¡çš„ç‰¹å¾å­¦ä¹ ã€‚æ‰€æœ‰æ•°æ®ã€ä»£ç å’Œé¢„è®­ç»ƒæƒé‡å°†å…¬å¼€å¯ç”¨ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ¢ç´¢å¤§è§„æ¨¡å™ªå£°æ ‡ç­¾æ•°æ®åœ¨åœ°ç†åº”ç”¨è¯­ä¹‰åˆ†å‰²ä¸­çš„æ½œåŠ›ã€‚</li>
<li>æå‡ºä¸€ç§æ–°é¢–çš„è·¨æ¨¡æ€æ ·æœ¬é€‰æ‹©ï¼ˆCromSSï¼‰æ–¹æ³•ï¼Œç»“åˆå¼±ç›‘ç£é¢„è®­ç»ƒæ¥æ”¹å–„ç‰¹å¾å­¦ä¹ ã€‚</li>
<li>å¼•å…¥è·¨æ¨¡æ€æ ·æœ¬é€‰æ‹©æ¨¡å—æ¥å‡è½»å™ªå£°æ ‡ç­¾çš„è´Ÿé¢å½±å“ã€‚</li>
<li>é‡‡ç”¨ç©ºé—´æ—¶é—´æ ‡ç­¾å¹³æ»‘æŠ€æœ¯æ¥æé«˜æ¨¡å‹å¯¹å™ªå£°æ ‡ç­¾çš„ç¨³å¥æ€§ã€‚</li>
<li>é€šè¿‡å®éªŒéªŒè¯ï¼Œåˆ©ç”¨å™ªå£°æ ‡ç­¾èƒ½æ˜¾è‘—å¢å¼ºè¯­ä¹‰åˆ†å‰²ä»»åŠ¡çš„ç‰¹å¾å­¦ä¹ ã€‚</li>
<li>å…¬å¼€äº†å¤šæ¨¡æ€æ•°æ®é›†NoLDO-S12ï¼Œä¾›ç ”ç©¶ä½¿ç”¨ã€‚</li>
<li>è¯¥æ–¹æ³•çš„æ•°æ®ã€ä»£ç å’Œé¢„è®­ç»ƒæƒé‡å‡å¯å…¬å¼€è·å–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.01217">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-47edb857dce04d4a392fc91a7d1fe86a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6734a868131bf6d497e8f178f99e153c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-56b0f545b688e36318b90529087a9613.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-201e333ea8bca7501cda4056c84b9eee.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c5c2a0f4fce9a88df6783e548fff753a.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-05/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-05/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-05/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-0fc8eaf8d5cded4d6217ebce5f6378e3.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-05  CUIfy the XR An Open-Source Package to Embed LLM-powered Conversational   Agents in XR
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-05
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-05/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-59a4cc9492a290d548413adf31be9e93.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-05  A Dual-Purpose Framework for Backdoor Defense and Backdoor Amplification   in Diffusion Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-05
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">24801.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
