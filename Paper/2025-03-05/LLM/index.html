<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-05  ECLeKTic a Novel Challenge Set for Evaluation of Cross-Lingual   Knowledge Transfer">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-1807107d98319b98694e460d22bd098d.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-05
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    84 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-05-æ›´æ–°"><a href="#2025-03-05-æ›´æ–°" class="headerlink" title="2025-03-05 æ›´æ–°"></a>2025-03-05 æ›´æ–°</h1><h2 id="ECLeKTic-a-Novel-Challenge-Set-for-Evaluation-of-Cross-Lingual-Knowledge-Transfer"><a href="#ECLeKTic-a-Novel-Challenge-Set-for-Evaluation-of-Cross-Lingual-Knowledge-Transfer" class="headerlink" title="ECLeKTic: a Novel Challenge Set for Evaluation of Cross-Lingual   Knowledge Transfer"></a>ECLeKTic: a Novel Challenge Set for Evaluation of Cross-Lingual   Knowledge Transfer</h2><p><strong>Authors:Omer Goldman, Uri Shaham, Dan Malkin, Sivan Eiger, Avinatan Hassidim, Yossi Matias, Joshua Maynez, Adi Mayrav Gilady, Jason Riesa, Shruti Rijhwani, Laura Rimell, Idan Szpektor, Reut Tsarfaty, Matan Eyal</strong></p>
<p>To achieve equitable performance across languages, multilingual large language models (LLMs) must be able to abstract knowledge beyond the language in which it was acquired. However, the current literature lacks reliable ways to measure LLMsâ€™ capability of cross-lingual knowledge transfer. To that end, we present ECLeKTic, a multilingual closed-book QA (CBQA) dataset that Evaluates Cross-Lingual Knowledge Transfer in a simple, black-box manner. We detected information with uneven coverage across languages by controlling for presence and absence of Wikipedia articles in 12 languages. We generated knowledge-seeking questions in a source language, for which the answer appears in a relevant Wikipedia article and translated them to all other 11 languages, for which the respective Wikipedias lack equivalent articles. Assuming that Wikipedia reflects the prominent knowledge in the LLMâ€™s training data, to solve ECLeKTicâ€™s CBQA task the model is required to transfer knowledge between languages. Experimenting with 8 LLMs, we show that SOTA models struggle to effectively share knowledge across, languages even if they can predict the answer well for queries in the same language the knowledge was acquired in. </p>
<blockquote>
<p>ä¸ºäº†å®ç°ä¸åŒè¯­è¨€çš„å…¬å¹³æ€§èƒ½ï¼Œå¤šè¯­è¨€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¿…é¡»èƒ½å¤ŸæŠ½è±¡è·å–è¯­è¨€ä¹‹å¤–çš„çŸ¥è¯†ã€‚ç„¶è€Œï¼Œç°æœ‰æ–‡çŒ®ç¼ºä¹å¯é çš„æ–¹æ³•æ¥è¡¡é‡LLMè·¨è¯­è¨€çŸ¥è¯†è½¬ç§»çš„èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ECLeKTicï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šè¯­è¨€å°é—­é—®ç­”ï¼ˆCBQAï¼‰æ•°æ®é›†ï¼Œä»¥ç®€å•ã€é»‘ç®±çš„æ–¹å¼è¯„ä¼°è·¨è¯­è¨€çŸ¥è¯†çš„è½¬ç§»ã€‚æˆ‘ä»¬é€šè¿‡æ§åˆ¶12ç§è¯­è¨€çš„ç»´åŸºç™¾ç§‘æ–‡ç« çš„æœ‰æ— ï¼Œæ£€æµ‹äº†è¯­è¨€é—´ä¿¡æ¯è¦†ç›–çš„ä¸å‡åŒ€æ€§ã€‚æˆ‘ä»¬åœ¨æºè¯­è¨€ä¸­ç”Ÿæˆå¯»æ±‚çŸ¥è¯†çš„é—®é¢˜ï¼Œè¿™äº›é—®é¢˜çš„ç­”æ¡ˆå‡ºç°åœ¨ç›¸å…³çš„ç»´åŸºç™¾ç§‘æ–‡ç« ä¸­ï¼Œç„¶åå°†å®ƒä»¬ç¿»è¯‘åˆ°å…¶ä»–æ‰€æœ‰11ç§è¯­è¨€ï¼Œå¯¹äºè¿™äº›è¯­è¨€ï¼Œç›¸åº”çš„ç»´åŸºç™¾ç§‘ç¼ºä¹ç­‰ä»·çš„æ–‡ç« ã€‚å‡è®¾ç»´åŸºç™¾ç§‘åæ˜ äº†LLMè®­ç»ƒæ•°æ®ä¸­çš„çªå‡ºçŸ¥è¯†ï¼Œè¦è§£å†³ECLeKTicçš„CBQAä»»åŠ¡ï¼Œæ¨¡å‹éœ€è¦åœ¨è¯­è¨€ä¹‹é—´è¿›è¡ŒçŸ¥è¯†è½¬ç§»ã€‚é€šè¿‡å¯¹8ç§LLMè¿›è¡Œå®éªŒï¼Œæˆ‘ä»¬å‘ç°å³ä½¿å¯¹äºåœ¨å…¶è·å–çŸ¥è¯†çš„åŒä¸€è¯­è¨€ä¸­çš„æŸ¥è¯¢èƒ½å¤Ÿå¾ˆå¥½åœ°é¢„æµ‹ç­”æ¡ˆï¼ŒSOTAæ¨¡å‹åœ¨è·¨è¯­è¨€å…±äº«çŸ¥è¯†æ–¹é¢ä»ç„¶é¢ä¸´å›°éš¾ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.21228v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LLMåœ¨è¯­è¨€é—´çš„è¡¨ç°éœ€å¹³ç­‰ï¼Œéœ€è¦è¶…è¶Šè¯­è¨€æœ¬èº«è¿›è¡ŒçŸ¥è¯†æŠ½è±¡ã€‚ç„¶è€Œï¼Œå½“å‰æ–‡çŒ®ç¼ºä¹å¯é çš„æ–¹æ³•æ¥æµ‹é‡LLMè·¨è¯­è¨€çŸ¥è¯†è½¬ç§»çš„èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ECLeKTicæ•°æ®é›†ï¼Œä»¥ç®€å•ç›´è§‚çš„æ–¹å¼è¯„ä¼°è·¨è¯­è¨€çš„çŸ¥è¯†è½¬ç§»èƒ½åŠ›ã€‚é€šè¿‡å¯¹è¯­è¨€ä¹‹é—´ä¿¡æ¯è¦†ç›–ä¸å‡çš„æƒ…å†µè¿›è¡Œç ”ç©¶ï¼Œæˆ‘ä»¬é€šè¿‡æ§åˆ¶èµ„æ–™çš„æ–¹å¼æ£€éªŒäº†çŸ¥è¯†çš„è¿ç§»ã€‚å®éªŒä¸­æ¶‰åŠå¤šä¸ªLLMï¼Œæˆ‘ä»¬å‘ç°å…ˆè¿›æ¨¡å‹åœ¨è·¨è¯­è¨€ç¯å¢ƒä¸‹éš¾ä»¥æœ‰æ•ˆå…±äº«çŸ¥è¯†ã€‚å³ä½¿å®ƒä»¬åœ¨è·å–çŸ¥è¯†çš„åŒä¸€è¯­è¨€ä¸­è¿›è¡ŒæŸ¥è¯¢é¢„æµ‹è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨è·¨è¯­è¨€ç¯å¢ƒä¸‹ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLMè¦å®ç°ä¸åŒè¯­è¨€é—´çš„å…¬å¹³è¡¨ç°ï¼Œå¿…é¡»è¶…è¶Šè¯­è¨€æœ¬èº«è¿›è¡ŒçŸ¥è¯†æŠ½è±¡ã€‚</li>
<li>å½“å‰ç¼ºä¹è¯„ä¼°LLMè·¨è¯­è¨€çŸ¥è¯†è½¬ç§»èƒ½åŠ›çš„å¯é æ–¹æ³•ã€‚</li>
<li>ECLeKTicæ•°æ®é›†ç”¨äºç®€å•ç›´è§‚åœ°è¯„ä¼°è·¨è¯­è¨€çš„çŸ¥è¯†è½¬ç§»èƒ½åŠ›ã€‚</li>
<li>ä¸åŒè¯­è¨€ä¹‹é—´çš„ä¿¡æ¯è¦†ç›–ä¸å‡å½±å“çŸ¥è¯†çš„è¿ç§»ã€‚</li>
<li>LLMåœ¨è·¨è¯­è¨€ç¯å¢ƒä¸‹éš¾ä»¥æœ‰æ•ˆå…±äº«çŸ¥è¯†ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.21228">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cc9b554d9a42a9662cdbd0301a5980d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a8fd04be3270256fe636720ea0fae062.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dc819cc8565e76808159d5cbd05fcf09.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76518c26e33f3dc0b0527b001749fa3b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bbd7bfbfce8508a172797f8f691c3762.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8bdbe0d6627a32d0f7372675b707af60.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="CAMEx-Curvature-aware-Merging-of-Experts"><a href="#CAMEx-Curvature-aware-Merging-of-Experts" class="headerlink" title="CAMEx: Curvature-aware Merging of Experts"></a>CAMEx: Curvature-aware Merging of Experts</h2><p><strong>Authors:Dung V. Nguyen, Minh H. Nguyen, Luc Q. Nguyen, Rachel S. Y. Teo, Tan M. Nguyen, Linh Duy Tran</strong></p>
<p>Existing methods for merging experts during model training and fine-tuning predominantly rely on Euclidean geometry, which assumes a flat parameter space. This assumption can limit the modelâ€™s generalization ability, especially during the pre-training phase, where the parameter manifold might exhibit more complex curvature. Curvature-aware merging methods typically require additional information and computational resources to approximate the Fisher Information Matrix, adding memory overhead. In this paper, we introduce CAMEx (Curvature-Aware Merging of Experts), a novel expert merging protocol that incorporates natural gradients to account for the non-Euclidean curvature of the parameter manifold. By leveraging natural gradients, CAMEx adapts more effectively to the structure of the parameter space, improving alignment between model updates and the manifoldâ€™s geometry. This approach enhances both pre-training and fine-tuning, resulting in better optimization trajectories and improved generalization without the substantial memory overhead typically associated with curvature-aware methods. Our contributions are threefold: (1) CAMEx significantly outperforms traditional Euclidean-based expert merging techniques across various natural language processing tasks, leading to enhanced performance during pre-training and fine-tuning; (2) we introduce a dynamic merging architecture that optimizes resource utilization, achieving high performance while reducing computational costs, facilitating efficient scaling of large language models; and (3) we provide both theoretical and empirical evidence to demonstrate the efficiency of our proposed method. The code is publicly available at: <a target="_blank" rel="noopener" href="https://github.com/kpup1710/CAMEx">https://github.com/kpup1710/CAMEx</a>. </p>
<blockquote>
<p>ç°æœ‰çš„æ¨¡å‹è®­ç»ƒå’Œå¾®è°ƒè¿‡ç¨‹ä¸­çš„ä¸“å®¶èåˆæ–¹æ³•ä¸»è¦ä¾èµ–äºæ¬§å‡ é‡Œå¾—å‡ ä½•ï¼Œè¿™å‡è®¾äº†å‚æ•°ç©ºé—´æ˜¯å¹³å¦çš„ã€‚è¿™ä¸ªå‡è®¾å¯èƒ½ä¼šé™åˆ¶æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨é¢„è®­ç»ƒé˜¶æ®µï¼Œå‚æ•°æµå½¢å¯èƒ½è¡¨ç°å‡ºæ›´å¤æ‚çš„æ›²ç‡ã€‚è€ƒè™‘åˆ°æ›²ç‡çš„èåˆæ–¹æ³•é€šå¸¸éœ€è¦é¢å¤–çš„ä¿¡æ¯å’Œè®¡ç®—èµ„æºæ¥è¿‘ä¼¼è´¹èˆå°”ä¿¡æ¯çŸ©é˜µï¼Œå¢åŠ äº†å†…å­˜å¼€é”€ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†CAMExï¼ˆåŸºäºæ›²ç‡çš„ä¸“å®¶èåˆï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„ä¸“å®¶èåˆåè®®ï¼Œå®ƒé‡‡ç”¨è‡ªç„¶æ¢¯åº¦æ¥è€ƒè™‘å‚æ•°æµå½¢çš„éæ¬§å‡ é‡Œå¾—æ›²ç‡ã€‚é€šè¿‡åˆ©ç”¨è‡ªç„¶æ¢¯åº¦ï¼ŒCAMExæ›´æœ‰æ•ˆåœ°é€‚åº”äº†å‚æ•°ç©ºé—´çš„ç»“æ„ï¼Œæ”¹è¿›äº†æ¨¡å‹æ›´æ–°ä¸æµå½¢å‡ ä½•ä¹‹é—´çš„å¯¹é½ã€‚è¿™ç§æ–¹æ³•æé«˜äº†é¢„è®­ç»ƒå’Œå¾®è°ƒçš„æ•ˆæœï¼Œå¸¦æ¥äº†æ›´å¥½çš„ä¼˜åŒ–è½¨è¿¹å’Œæ”¹è¿›çš„æ³›åŒ–èƒ½åŠ›ï¼Œè€Œä¸”æ²¡æœ‰ä¸æ›²ç‡æ„ŸçŸ¥æ–¹æ³•ç›¸å…³çš„å·¨å¤§å†…å­˜å¼€é”€ã€‚æˆ‘ä»¬çš„è´¡çŒ®æœ‰ä¸‰ç‚¹ï¼šï¼ˆ1ï¼‰CAMExåœ¨å„ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„åŸºäºæ¬§å‡ é‡Œå¾—çš„ä¸“å®¶èåˆæŠ€æœ¯ï¼Œåœ¨é¢„è®­ç»ƒå’Œå¾®è°ƒè¿‡ç¨‹ä¸­æé«˜äº†æ€§èƒ½ï¼›ï¼ˆ2ï¼‰æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŠ¨æ€èåˆæ¶æ„ï¼Œä¼˜åŒ–äº†èµ„æºåˆ©ç”¨ï¼Œåœ¨é™ä½è®¡ç®—æˆæœ¬çš„åŒæ—¶å®ç°äº†é«˜æ€§èƒ½ï¼Œä¿ƒè¿›äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ•ˆç‡æ‰©å±•ï¼›ï¼ˆ3ï¼‰æˆ‘ä»¬æä¾›äº†ç†è®ºå’Œå®è¯è¯æ®ï¼Œè¯æ˜äº†æˆ‘ä»¬æå‡ºçš„æ–¹æ³•çš„æ•ˆç‡ã€‚ä»£ç å…¬å¼€åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/kpup1710/CAMEx%E3%80%82">https://github.com/kpup1710/CAMExã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.18821v2">PDF</a> 10 pages, 5 Figures, 7 Tables. Published at ICLR 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºäº†CAMExï¼ˆåŸºäºæ›²ç‡çš„ä¸“å®¶åˆå¹¶ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„ä¸“å®¶åˆå¹¶åè®®ï¼Œå®ƒé‡‡ç”¨è‡ªç„¶æ¢¯åº¦æ¥åº”å¯¹å‚æ•°æµå½¢éæ¬§å‡ é‡Œå¾—æ›²ç‡çš„é—®é¢˜ã€‚é€šè¿‡åˆ©ç”¨è‡ªç„¶æ¢¯åº¦ï¼ŒCAMExæ›´æœ‰æ•ˆåœ°é€‚åº”äº†å‚æ•°ç©ºé—´çš„ç»“æ„ï¼Œæ”¹å–„äº†æ¨¡å‹æ›´æ–°ä¸æµå½¢å‡ ä½•ä¹‹é—´çš„å¯¹é½ã€‚æ­¤æ–¹æ³•åœ¨é¢„è®­ç»ƒå’Œå¾®è°ƒä¸­éƒ½è¡¨ç°å‡ºè‰²ï¼Œèƒ½åœ¨ä¸å¢åŠ å¤§é‡å†…å­˜å¼€é”€çš„æƒ…å†µä¸‹ä¼˜åŒ–è½¨è¿¹å¹¶æé«˜æ³›åŒ–èƒ½åŠ›ã€‚æœ¬æ–‡çš„ä¸»è¦è´¡çŒ®åŒ…æ‹¬ï¼šCAMExåœ¨å¤šç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„æ¬§å‡ é‡Œå¾—ä¸“å®¶åˆå¹¶æŠ€æœ¯ï¼›æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŠ¨æ€åˆå¹¶æ¶æ„ï¼Œåœ¨ä¼˜åŒ–èµ„æºåˆ©ç”¨çš„åŒæ—¶é™ä½äº†è®¡ç®—æˆæœ¬ï¼Œä¿ƒè¿›äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ•ˆç‡æ‰©å±•ï¼›æˆ‘ä»¬æä¾›äº†ç†è®ºå’Œå®è¯è¯æ®æ¥è¯æ˜æˆ‘ä»¬æ–¹æ³•çš„æ•ˆç‡ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>CAMExå¼•å…¥äº†ä¸€ç§æ–°çš„ä¸“å®¶åˆå¹¶æ–¹æ³•ï¼Œè€ƒè™‘å‚æ•°æµå½¢çš„éæ¬§å‡ é‡Œå¾—æ›²ç‡ï¼Œé€šè¿‡è‡ªç„¶æ¢¯åº¦è¿›è¡Œæ›´æœ‰æ•ˆçš„æ¨¡å‹æ›´æ–°ã€‚</li>
<li>CAMExåœ¨é¢„è®­ç»ƒå’Œå¾®è°ƒé˜¶æ®µéƒ½è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œæé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ä¸ä¼ ç»Ÿçš„æ¬§å‡ é‡Œå¾—ä¸“å®¶åˆå¹¶æŠ€æœ¯ç›¸æ¯”ï¼ŒCAMExåœ¨å„ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸Šè¡¨ç°æ›´ä¼˜ç§€ã€‚</li>
<li>åŠ¨æ€åˆå¹¶æ¶æ„çš„ä¼˜åŒ–èµ„æºåˆ©ç”¨ï¼Œé™ä½äº†è®¡ç®—æˆæœ¬ï¼Œæœ‰åˆ©äºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ•ˆç‡æ‰©å±•ã€‚</li>
<li>æœ¬æ–‡æä¾›äº†ç†è®ºå’Œå®è¯è¯æ®æ¥è¯æ˜CAMExæ–¹æ³•çš„é«˜æ•ˆæ€§ã€‚</li>
<li>CAMExçš„ä»£ç å·²å…¬å¼€å‘å¸ƒåœ¨GitHubä¸Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.18821">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-665e36d8933075b91c8f5b8399041336.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae523b59fce287fa6c3d9026d4951405.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e0287f1c36eba563d31dff62355a0449.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce855c8576ae8f3369f6a44ffe1dbfb9.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Chain-of-Draft-Thinking-Faster-by-Writing-Less"><a href="#Chain-of-Draft-Thinking-Faster-by-Writing-Less" class="headerlink" title="Chain of Draft: Thinking Faster by Writing Less"></a>Chain of Draft: Thinking Faster by Writing Less</h2><p><strong>Authors:Silei Xu, Wenhao Xie, Lingxiao Zhao, Pengcheng He</strong></p>
<p>Large Language Models (LLMs) have demonstrated remarkable performance in solving complex reasoning tasks through mechanisms like Chain-of-Thought (CoT) prompting, which emphasizes verbose, step-by-step reasoning. However, humans typically employ a more efficient strategy: drafting concise intermediate thoughts that capture only essential information. In this work, we propose Chain of Draft (CoD), a novel paradigm inspired by human cognitive processes, where LLMs generate minimalistic yet informative intermediate reasoning outputs while solving tasks. By reducing verbosity and focusing on critical insights, CoD matches or surpasses CoT in accuracy while using as little as only 7.6% of the tokens, significantly reducing cost and latency across various reasoning tasks. Our code and data are available at <a target="_blank" rel="noopener" href="https://github.com/sileix/chain-of-draft">https://github.com/sileix/chain-of-draft</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡æ€ç»´é“¾ï¼ˆCoTï¼‰æç¤ºç­‰æœºåˆ¶ï¼Œåœ¨è§£å†³å¤æ‚æ¨ç†ä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œå¼ºè°ƒè¯¦ç»†ã€é€æ­¥æ¨ç†ã€‚ç„¶è€Œï¼Œäººç±»é€šå¸¸é‡‡ç”¨æ›´æœ‰æ•ˆçš„ç­–ç•¥ï¼šèµ·è‰ç®€æ´çš„ä¸­é—´æ€æƒ³ï¼Œåªæ•æ‰å…³é”®ä¿¡æ¯ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†å—äººç±»è®¤çŸ¥è¿‡ç¨‹å¯å‘çš„å…¨æ–°èŒƒå¼â€”â€”è‰ç¨¿é“¾ï¼ˆCoDï¼‰ã€‚åœ¨æ­¤èŒƒå¼ä¸‹ï¼ŒLLMåœ¨è§£å†³é—®é¢˜æ—¶ä¼šç”Ÿæˆç®€æ´è€Œå¯Œæœ‰ä¿¡æ¯é‡çš„ä¸­é—´æ¨ç†è¾“å‡ºã€‚é€šè¿‡å‡å°‘å†—ä½™å¹¶ä¸“æ³¨äºå…³é”®è§è§£ï¼ŒCoDåœ¨å‡†ç¡®æ€§æ–¹é¢ä¸CoTç›¸åŒ¹é…æˆ–è¶…è¶Šï¼ŒåŒæ—¶ä»…ä½¿ç”¨7.6%çš„ç¬¦å·ï¼Œæ˜¾è‘—é™ä½äº†å„ç§æ¨ç†ä»»åŠ¡çš„æˆæœ¬å’Œå»¶è¿Ÿã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®åœ¨<a target="_blank" rel="noopener" href="https://github.com/sileix/chain-of-draft%E5%8F%AF%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/sileix/chain-of-draftå¯å…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.18600v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡å¦‚æ€ç»´é“¾ï¼ˆCoTï¼‰æç¤ºç­‰æœºåˆ¶åœ¨è§£å†³å¤æ‚æ¨ç†ä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚ç„¶è€Œï¼Œäººç±»é€šå¸¸é‡‡ç”¨æ›´é«˜æ•ˆçš„ç­–ç•¥ï¼Œå³æ’°å†™ç®€æ´çš„ä¸­é—´æƒ³æ³•ï¼Œä»…æ•æ‰å…³é”®ä¿¡æ¯ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§æ–°çš„èŒƒå¼â€”â€”æ€ç»´é“¾è‰æ¡ˆï¼ˆCoDï¼‰ï¼Œå®ƒå—åˆ°äººç±»è®¤çŸ¥è¿‡ç¨‹çš„å¯å‘ï¼Œä½¿LLMåœ¨è§£å†³é—®é¢˜æ—¶èƒ½å¤Ÿç”Ÿæˆç®€æ´è€Œå¯Œæœ‰ä¿¡æ¯é‡çš„ä¸­é—´æ¨ç†è¾“å‡ºã€‚é€šè¿‡å‡å°‘å†—ä½™ä¿¡æ¯å¹¶ä¸“æ³¨äºå…³é”®è§è§£ï¼ŒCoDåœ¨å‡†ç¡®æ€§æ–¹é¢ä¸CoTç›¸åŒ¹é…æˆ–æ›´èƒœä¸€ç­¹ï¼ŒåŒæ—¶ä»…ä½¿ç”¨7.6%çš„æ ‡è®°ï¼Œæ˜¾è‘—é™ä½äº†å„ç§æ¨ç†ä»»åŠ¡çš„æˆæœ¬å’Œå»¶è¿Ÿã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡æ€ç»´é“¾ï¼ˆCoTï¼‰æç¤ºè§£å†³å¤æ‚æ¨ç†ä»»åŠ¡è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>äººç±»åœ¨è§£å†³æ¨ç†é—®é¢˜æ—¶é€šå¸¸é‡‡ç”¨ç®€æ´çš„ä¸­é—´æ€è€ƒç­–ç•¥ï¼Œä»…å…³æ³¨å…³é”®ä¿¡æ¯ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„èŒƒå¼â€”â€”æ€ç»´é“¾è‰æ¡ˆï¼ˆCoDï¼‰ï¼Œå®ƒå—åˆ°äººç±»è®¤çŸ¥è¿‡ç¨‹çš„å¯å‘ã€‚</li>
<li>CoDä½¿LLMèƒ½å¤Ÿç”Ÿæˆç®€æ´è€Œå¯Œæœ‰ä¿¡æ¯é‡çš„ä¸­é—´æ¨ç†è¾“å‡ºã€‚</li>
<li>CoDåœ¨å‡†ç¡®æ€§æ–¹é¢ä¸CoTç›¸åŒ¹é…ï¼Œå¹¶åœ¨æŸäº›æƒ…å†µä¸‹è¡¨ç°æ›´ä¼˜ç§€ã€‚</li>
<li>CoDæ˜¾è‘—é™ä½äº†æ¨ç†ä»»åŠ¡çš„æˆæœ¬å’Œå»¶è¿Ÿï¼Œä»…ä½¿ç”¨7.6%çš„æ ‡è®°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.18600">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2788dffc365bcc881abe1aa271ca1840.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dda05ec3b655f81dace14f4a30e2e0a1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cfba2aa961945ad62950b27c49191265.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9a95f3b83cb59ad72cf11b240c32de8f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d68cf434c2ef4bc644c43c1483164ffd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff75cece2428502bf7cf31c49d7598c4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a41fa6f8c049253ca7c90158a7008309.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="InductionBench-LLMs-Fail-in-the-Simplest-Complexity-Class"><a href="#InductionBench-LLMs-Fail-in-the-Simplest-Complexity-Class" class="headerlink" title="InductionBench: LLMs Fail in the Simplest Complexity Class"></a>InductionBench: LLMs Fail in the Simplest Complexity Class</h2><p><strong>Authors:Wenyue Hua, Tyler Wong, Sun Fei, Liangming Pan, Adam Jardine, William Yang Wang</strong></p>
<p>Large language models (LLMs) have shown remarkable improvements in reasoning and many existing benchmarks have been addressed by models such as o1 and o3 either fully or partially. However, a majority of these benchmarks emphasize deductive reasoning, including mathematical and coding tasks in which rules such as mathematical axioms or programming syntax are clearly defined, based on which LLMs can plan and apply these rules to arrive at a solution. In contrast, inductive reasoning, where one infers the underlying rules from observed data, remains less explored. Such inductive processes lie at the heart of scientific discovery, as they enable researchers to extract general principles from empirical observations. To assess whether LLMs possess this capacity, we introduce InductionBench, a new benchmark designed to evaluate the inductive reasoning ability of LLMs. Our experimental findings reveal that even the most advanced models available struggle to master the simplest complexity classes within the subregular hierarchy of functions, highlighting a notable deficiency in current LLMsâ€™ inductive reasoning capabilities. Coda and data are available <a target="_blank" rel="noopener" href="https://github.com/Wenyueh/inductive_reasoning_benchmark">https://github.com/Wenyueh/inductive_reasoning_benchmark</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œç°æœ‰çš„è®¸å¤šåŸºå‡†æµ‹è¯•ï¼ˆå¦‚o1å’Œo3ï¼‰å·²ç»è¢«è¿™äº›æ¨¡å‹å®Œå…¨æˆ–éƒ¨åˆ†è§£å†³ã€‚ç„¶è€Œï¼Œè¿™äº›åŸºå‡†æµ‹è¯•ä¸­å¤§å¤šæ•°éƒ½ä¾§é‡äºæ¼”ç»æ¨ç†ï¼ŒåŒ…æ‹¬æ•°å­¦å’Œç¼–ç ä»»åŠ¡ï¼Œè¿™äº›ä»»åŠ¡ä¸­çš„è§„åˆ™ï¼ˆå¦‚æ•°å­¦å…¬ç†æˆ–ç¼–ç¨‹è¯­æ³•ï¼‰æ˜¯æ˜ç¡®å®šä¹‰çš„ï¼ŒLLMå¯ä»¥è§„åˆ’å¹¶åº”ç”¨è¿™äº›è§„åˆ™æ¥å¾—å‡ºè§£å†³æ–¹æ¡ˆã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå½’çº³æ¨ç†çš„ç ”ç©¶è¾ƒå°‘ï¼Œå½’çº³æ¨ç†æ˜¯ä»è§‚å¯Ÿåˆ°çš„æ•°æ®ä¸­æ¨æ–­å‡ºæ½œåœ¨è§„åˆ™çš„è¿‡ç¨‹ã€‚è¿™æ ·çš„å½’çº³è¿‡ç¨‹å¤„äºç§‘å­¦å‘ç°çš„æ ¸å¿ƒï¼Œå› ä¸ºå®ƒä»¬ä½¿ç ”ç©¶äººå‘˜èƒ½å¤Ÿä»å®è¯è§‚å¯Ÿä¸­æå–ä¸€èˆ¬åŸåˆ™ã€‚ä¸ºäº†è¯„ä¼°LLMæ˜¯å¦å…·å¤‡è¿™ç§èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼•å…¥äº†InductionBenchï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°LLMçš„å½’çº³æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿åœ¨åŠŸèƒ½å­æ­£åˆ™å±‚æ¬¡ç»“æ„ä¸­æœ€ç®€å•çš„å¤æ‚æ€§ç±»åˆ«ä¸­ï¼Œæœ€å…ˆè¿›çš„æ¨¡å‹ä¹Ÿå¾ˆéš¾æŒæ¡ï¼Œè¿™çªæ˜¾äº†å½“å‰LLMå½’çº³æ¨ç†èƒ½åŠ›çš„æ˜¾è‘—ä¸è¶³ã€‚ç›¸å…³ä»£ç å’Œæ•°æ®å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/Wenyueh/inductive_reasoning_benchmark%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Wenyueh/inductive_reasoning_benchmarkè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.15823v3">PDF</a> 24 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†æ–¹é¢å–å¾—æ˜¾è‘—è¿›æ­¥ï¼Œç°æœ‰è®¸å¤šåŸºå‡†æµ‹è¯•å·²è¢«æ¨¡å‹å¦‚o1å’Œo3éƒ¨åˆ†æˆ–å®Œå…¨è§£å†³ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°åŸºå‡†æµ‹è¯•ä¾§é‡äºæ˜ç¡®çš„è§„åˆ™å®šä¹‰ï¼Œå¦‚æ•°å­¦å…¬ç†æˆ–ç¼–ç¨‹è¯­æ³•ï¼ŒLLMå¯ä»¥é€šè¿‡åº”ç”¨è¿™äº›è§„åˆ™æ¥è§£å†³é—®é¢˜ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå…³äºä»è§‚å¯Ÿåˆ°çš„æ•°æ®ä¸­æ¨æ–­æ½œåœ¨è§„åˆ™çš„å½’çº³æ¨ç†ç ”ç©¶è¾ƒå°‘ã€‚ä¸ºäº†è¯„ä¼°LLMçš„è¿™ç§èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼•å…¥äº†InductionBenchï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°LLMçš„å½’çº³æ¨ç†èƒ½åŠ›ã€‚å®éªŒå‘ç°ï¼Œæœ€å…ˆè¿›çš„æ¨¡å‹åœ¨å‡½æ•°æ¬¡æ­£è§„å±‚æ¬¡ç»“æ„ä¸­æœ€ç®€å•çš„å¤æ‚æ€§ç±»åˆ«ä¸­ä¹Ÿå­˜åœ¨å›°éš¾ï¼Œè¿™è¡¨æ˜å½“å‰LLMçš„å½’çº³æ¨ç†èƒ½åŠ›å­˜åœ¨æ˜æ˜¾ä¸è¶³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨æ¨ç†æ–¹é¢å–å¾—æ˜¾è‘—è¿›æ­¥ï¼Œå¹¶å·²åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>ç°æœ‰åŸºå‡†æµ‹è¯•å¤§å¤šä¾§é‡äºæ˜ç¡®çš„è§„åˆ™å®šä¹‰çš„æ¨ç†ï¼Œå¦‚æ•°å­¦å…¬ç†æˆ–ç¼–ç¨‹è¯­æ³•ã€‚</li>
<li>å½’çº³æ¨ç†æ˜¯ä¸€ä¸ªç›¸å¯¹è¾ƒæ–°çš„ç ”ç©¶é¢†åŸŸï¼Œå°¤å…¶æ˜¯ä¸LLMçš„èƒ½åŠ›è¯„ä¼°æœ‰å…³ã€‚</li>
<li>InductionBenchæ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°LLMçš„å½’çº³æ¨ç†èƒ½åŠ›ã€‚</li>
<li>æœ€å…ˆè¿›çš„LLMåœ¨å½’çº³æ¨ç†æ–¹é¢ä»å­˜åœ¨æ˜æ˜¾ä¸è¶³ï¼Œå³ä½¿åœ¨ç®€å•çš„å¤æ‚æ€§ç±»åˆ«ä¸­ä¹Ÿé¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>æ•°æ®å’Œä»£ç å¯é€šè¿‡ç‰¹å®šé“¾æ¥è·å–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.15823">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f60cd8d599953a81344e74d79d703b78.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-30738bf8f120d736dad468669149001f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1807107d98319b98694e460d22bd098d.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="SURGE-On-the-Potential-of-Large-Language-Models-as-General-Purpose-Surrogate-Code-Executors"><a href="#SURGE-On-the-Potential-of-Large-Language-Models-as-General-Purpose-Surrogate-Code-Executors" class="headerlink" title="SURGE: On the Potential of Large Language Models as General-Purpose   Surrogate Code Executors"></a>SURGE: On the Potential of Large Language Models as General-Purpose   Surrogate Code Executors</h2><p><strong>Authors:Bohan Lyu, Siqiao Huang, Zichen Liang</strong></p>
<p>Neural surrogate models have emerged as powerful and efficient tools in data mining. Meanwhile, large language models (LLMs) have demonstrated remarkable capabilities in code-related tasks. We investigate a novel application: using LLMs as surrogate models for code execution prediction. Given LLMsâ€™ unique ability to understand and process diverse programs, they present a promising direction for building general-purpose surrogate models. To systematically investigate this capability, we introduce SURGE, a comprehensive benchmark with $1160$ problems covering $8$ key aspects: multi-language programming tasks, competition-level programming problems, repository-level code analysis, high-cost scientific computing, time-complexity-intensive algorithms, buggy code analysis, programs dependent on specific compilers or execution environments, and formal mathematical proof verification. Through extensive empirical analysis of $21$ open-source and proprietary LLMs, we examine scaling laws, data efficiency, and predictive accuracy. Our findings reveal important insights about the feasibility of LLMs as efficient surrogates for computational processes, with implications for automated software testing, program analysis, and computational resource optimization in data mining applications. Code and dataset are released at <a target="_blank" rel="noopener" href="https://github.com/Imbernoulli/SURGE">https://github.com/Imbernoulli/SURGE</a>. </p>
<blockquote>
<p>ç¥ç»ä»£ç†æ¨¡å‹å·²ç»åœ¨æ•°æ®æŒ–æ˜ä¸­å±•ç°å‡ºå¼ºå¤§ä¸”é«˜æ•ˆçš„å·¥å…·èƒ½åŠ›ã€‚ä¸æ­¤åŒæ—¶ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä»£ç ç›¸å…³ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†æ˜¾è‘—çš„èƒ½åŠ›ã€‚æˆ‘ä»¬ç ”ç©¶äº†ä¸€ä¸ªæ–°åº”ç”¨ï¼šä½¿ç”¨LLMä½œä¸ºä»£ç æ‰§è¡Œé¢„æµ‹çš„ä»£ç†æ¨¡å‹ã€‚é‰´äºLLMç†è§£å’Œå¤„ç†å„ç§ç¨‹åºçš„ç‹¬ç‰¹èƒ½åŠ›ï¼Œå®ƒä»¬ä¸ºæ„å»ºé€šç”¨ä»£ç†æ¨¡å‹æä¾›äº†ä¸€ä¸ªæœ‰å‰æ™¯çš„æ–¹å‘ã€‚ä¸ºäº†ç³»ç»Ÿåœ°ç ”ç©¶è¿™ä¸€èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼•å…¥äº†SURGEï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«1160ä¸ªé—®é¢˜çš„ç»¼åˆåŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–8ä¸ªå…³é”®æ–¹é¢ï¼šå¤šè¯­è¨€ç¼–ç¨‹ä»»åŠ¡ã€ç«èµ›çº§ç¼–ç¨‹é—®é¢˜ã€ä»“åº“çº§ä»£ç åˆ†æã€é«˜æˆæœ¬ç§‘å­¦è®¡ç®—ã€æ—¶é—´å¤æ‚åº¦å¯†é›†ç®—æ³•ã€é”™è¯¯ä»£ç åˆ†æã€ä¾èµ–äºç‰¹å®šç¼–è¯‘å™¨æˆ–æ‰§è¡Œç¯å¢ƒçš„ç¨‹åºä»¥åŠå½¢å¼åŒ–æ•°å­¦è¯æ˜éªŒè¯ã€‚é€šè¿‡å¯¹21ä¸ªå¼€æºå’Œä¸“æœ‰LLMè¿›è¡Œå¹¿æ³›çš„å®è¯åˆ†æï¼Œæˆ‘ä»¬ç ”ç©¶äº†è§„æ¨¡å®šå¾‹ã€æ•°æ®æ•ˆç‡å’Œé¢„æµ‹ç²¾åº¦ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœæ­ç¤ºäº†LLMä½œä¸ºè®¡ç®—è¿‡ç¨‹çš„é«˜æ•ˆä»£ç†çš„å¯è¡Œæ€§ï¼Œä¸ºè‡ªåŠ¨åŒ–è½¯ä»¶æµ‹è¯•ã€ç¨‹åºåˆ†æå’Œæ•°æ®æŒ–æ˜åº”ç”¨ä¸­çš„è®¡ç®—èµ„æºä¼˜åŒ–æä¾›äº†å¯ç¤ºã€‚ä»£ç å’Œæ•°æ®é›†å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/Imbernoulli/SURGE%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/Imbernoulli/SURGEå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.11167v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ç¥ç»ç½‘ç»œä»£ç†æ¨¡å‹åœ¨æ•°æ®æŒ–æ˜ä¸­å±•ç°å‡ºå¼ºå¤§ä¸”é«˜æ•ˆçš„å·¥å…·èƒ½åŠ›ã€‚åŒæ—¶ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä»£ç ç›¸å…³ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ã€‚æœ¬ç ”ç©¶æ¢ç´¢äº†ä¸€ç§æ–°å‹åº”ç”¨ï¼šå°†LLMç”¨ä½œä»£ç æ‰§è¡Œé¢„æµ‹çš„ä»£ç†æ¨¡å‹ã€‚é‰´äºLLMç†è§£å’Œå¤„ç†å¤šæ ·ç¨‹åºçš„èƒ½åŠ›ï¼Œå®ƒä»¬ä¸ºæ„å»ºé€šç”¨ä»£ç†æ¨¡å‹æä¾›äº†æœ‰å‰æ™¯çš„æ–¹å‘ã€‚ä¸ºäº†ç³»ç»Ÿåœ°ç ”ç©¶è¿™ä¸€èƒ½åŠ›ï¼Œæˆ‘ä»¬æ¨å‡ºäº†SURGEç»¼åˆåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«æ¶µç›–å…«ä¸ªå…³é”®æ–¹é¢çš„1160ä¸ªé—®é¢˜ï¼šå¤šè¯­è¨€ç¼–ç¨‹ä»»åŠ¡ã€ç«èµ›çº§ç¼–ç¨‹é—®é¢˜ã€ä»“åº“çº§ä»£ç åˆ†æã€é«˜æˆæœ¬ç§‘å­¦è®¡ç®—ã€æ—¶é—´å¤æ‚åº¦å¯†é›†ç®—æ³•ã€é”™è¯¯ä»£ç åˆ†æã€ç‰¹å®šç¼–è¯‘å™¨æˆ–æ‰§è¡Œç¯å¢ƒä¾èµ–çš„ç¨‹åºä»¥åŠå½¢å¼åŒ–æ•°å­¦è¯æ˜éªŒè¯ã€‚é€šè¿‡å¯¹21ä¸ªå¼€æºå’Œä¸“æœ‰LLMçš„å¹¿æ³›å®è¯åˆ†æï¼Œæˆ‘ä»¬ç ”ç©¶äº†è§„æ¨¡å®šå¾‹ã€æ•°æ®æ•ˆç‡å’Œé¢„æµ‹ç²¾åº¦ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒLLMä½œä¸ºè®¡ç®—è¿‡ç¨‹çš„ä»£ç†æ˜¯å¯è¡Œçš„ï¼Œå¯¹è‡ªåŠ¨åŒ–è½¯ä»¶æµ‹è¯•ã€ç¨‹åºåˆ†æå’Œæ•°æ®æŒ–æ˜åº”ç”¨çš„è®¡ç®—èµ„æºä¼˜åŒ–å…·æœ‰é‡è¦å½±å“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¥ç»ç½‘ç»œä»£ç†æ¨¡å‹åœ¨æ•°æ®æŒ–æ˜ä¸­è¡¨ç°å¼ºå¤§ä¸”é«˜æ•ˆã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä»£ç ç›¸å…³ä»»åŠ¡ä¸­å…·æœ‰å“è¶Šèƒ½åŠ›ã€‚</li>
<li>LLMå¯åº”ç”¨äºä»£ç æ‰§è¡Œé¢„æµ‹ï¼Œä½œä¸ºé€šç”¨ä»£ç†æ¨¡å‹å…·æœ‰æ½œåŠ›ã€‚</li>
<li>æ¨å‡ºSURGEç»¼åˆåŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–å¤šä¸ªå…³é”®æ–¹é¢çš„ç¼–ç¨‹é—®é¢˜ã€‚</li>
<li>é€šè¿‡å®è¯åˆ†æç ”ç©¶äº†LLMçš„è§„æ¨¡å®šå¾‹ã€æ•°æ®æ•ˆç‡å’Œé¢„æµ‹ç²¾åº¦ã€‚</li>
<li>LLMä½œä¸ºè®¡ç®—è¿‡ç¨‹çš„ä»£ç†å…·æœ‰å¯è¡Œæ€§ï¼Œå¯¹è‡ªåŠ¨åŒ–è½¯ä»¶æµ‹è¯•å’Œç¨‹åºåˆ†ææœ‰é‡è¦å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.11167">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-1f1ce9374082d57897067717fc9103be.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-49ca97d2f9251e6c28ac6c317fb0aa07.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-96c2c8b8ea814befa4b3c7c944ef7c32.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-46196c8d66b2eed4a08cd42ea5476a9e.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Preconditioned-Inexact-Stochastic-ADMM-for-Deep-Model"><a href="#Preconditioned-Inexact-Stochastic-ADMM-for-Deep-Model" class="headerlink" title="Preconditioned Inexact Stochastic ADMM for Deep Model"></a>Preconditioned Inexact Stochastic ADMM for Deep Model</h2><p><strong>Authors:Shenglong Zhou, Ouya Wang, Ziyan Luo, Yongxu Zhu, Geoffrey Ye Li</strong></p>
<p>The recent advancement of foundation models (FMs) has brought about a paradigm shift, revolutionizing various sectors worldwide. The popular optimizers used to train these models are stochastic gradient descent-based algorithms, which face inherent limitations, such as slow convergence and stringent assumptions for convergence. In particular, data heterogeneity arising from distributed settings poses significant challenges to their theoretical and numerical performance. This paper develops an algorithm, PISA ({P}reconditioned {I}nexact {S}tochastic {A}lternating Direction Method of Multipliers), which enables scalable parallel computing and supports various second-moment schemes. Grounded in rigorous theoretical guarantees, the algorithm converges under the sole assumption of Lipschitz continuity of the gradient, thereby removing the need for other conditions commonly imposed by stochastic methods. This capability enables PISA to tackle the challenge of data heterogeneity effectively. Comprehensive experimental evaluations for training or fine-tuning diverse FMs, including vision models, large language models, reinforcement learning models, generative adversarial networks, and recurrent neural networks, demonstrate its superior numerical performance compared to various state-of-the-art optimizers. </p>
<blockquote>
<p>è¿‘æœŸçš„é¢„è®­ç»ƒæ¨¡å‹ï¼ˆFMsï¼‰çš„å‘å±•å¸¦æ¥äº†èŒƒå¼è½¬å˜ï¼Œä¸ºå…¨çƒçš„å¤šä¸ªè¡Œä¸šå¸¦æ¥äº†é©å‘½æ€§çš„å˜é©ã€‚ç”¨äºè®­ç»ƒè¿™äº›æ¨¡å‹çš„æµè¡Œä¼˜åŒ–å™¨æ˜¯åŸºäºéšæœºæ¢¯åº¦ä¸‹é™ç®—æ³•çš„ç®—æ³•ï¼Œè¿™äº›ç®—æ³•é¢ä¸´ç€å›ºæœ‰çš„å±€é™æ€§ï¼Œå¦‚æ”¶æ•›é€Ÿåº¦æ…¢å’Œå¯¹æ”¶æ•›çš„ä¸¥æ ¼å‡è®¾ã€‚ç‰¹åˆ«æ˜¯åˆ†å¸ƒå¼è®¾ç½®ä¸­äº§ç”Ÿçš„æ•°æ®å¼‚æ„æ€§å¯¹å…¶ç†è®ºå’Œæ•°å€¼æ€§èƒ½æå‡ºäº†é‡å¤§æŒ‘æˆ˜ã€‚æœ¬æ–‡å¼€å‘äº†ä¸€ç§ç®—æ³•PISAï¼ˆé¢„æ¡ä»¶è¿‘ä¼¼éšæœºäº¤æ›¿æ–¹å‘ä¹˜æ•°æ³•ï¼‰ï¼Œå¯å®ç°å¯æ‰©å±•çš„å¹¶è¡Œè®¡ç®—å¹¶æ”¯æŒå¤šç§äºŒé˜¶çŸ©æ–¹æ¡ˆã€‚è¯¥ç®—æ³•å»ºç«‹åœ¨ä¸¥æ ¼çš„ç†è®ºä¿è¯ä¹‹ä¸Šï¼Œåªåœ¨æ¢¯åº¦å…·æœ‰Lipschitzè¿ç»­æ€§çš„å‡è®¾ä¸‹æ”¶æ•›ï¼Œä»è€Œæ¶ˆé™¤äº†éšæœºæ–¹æ³•é€šå¸¸æ–½åŠ çš„å…¶å®ƒæ¡ä»¶ã€‚è¿™ç§èƒ½åŠ›ä½¿å¾—PISAèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°è§£å†³æ•°æ®å¼‚æ„æ€§çš„æŒ‘æˆ˜ã€‚å¯¹äºè®­ç»ƒæˆ–å¾®è°ƒå„ç§é¢„è®­ç»ƒæ¨¡å‹çš„ç»¼åˆå®éªŒè¯„ä¼°ï¼ŒåŒ…æ‹¬è§†è§‰æ¨¡å‹ã€å¤§å‹è¯­è¨€æ¨¡å‹ã€å¼ºåŒ–å­¦ä¹ æ¨¡å‹ã€ç”Ÿæˆå¯¹æŠ—ç½‘ç»œå’Œå¾ªç¯ç¥ç»ç½‘ç»œç­‰ï¼Œè¯æ˜äº†å…¶ç›¸è¾ƒäºå„ç§æœ€å…ˆè¿›çš„ä¼˜åŒ–å™¨çš„ä¼˜è¶Šæ•°å€¼æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.10784v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡æ¨¡å‹ï¼ˆFMsï¼‰çš„æœ€æ–°è¿›å±•å¸¦æ¥äº†èŒƒå¼è½¬å˜ï¼Œæ­£åœ¨å…¨çƒèŒƒå›´å†…å˜é©å„ä¸ªé¢†åŸŸã€‚å°½ç®¡å½“å‰ç”¨äºè®­ç»ƒæ¨¡å‹çš„ä¼˜åŒ–å™¨å¤§å¤šä¸ºåŸºäºéšæœºæ¢¯åº¦ä¸‹é™ç®—æ³•ï¼Œä½†å…¶å­˜åœ¨å›ºæœ‰çš„å±€é™æ€§ï¼Œå¦‚æ”¶æ•›é€Ÿåº¦æ…¢å’Œå‡è®¾æ¡ä»¶ä¸¥æ ¼ç­‰ã€‚æ•°æ®å¼‚æ„æ€§å¸¦æ¥çš„æŒ‘æˆ˜ä¸¥é‡å½±å“äº†å®ƒä»¬çš„ç†è®ºå’Œæ•°å€¼æ€§èƒ½ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç®—æ³•PISAï¼Œè¯¥ç®—æ³•å¯å®ç°å¯æ‰©å±•çš„å¹¶è¡Œè®¡ç®—å¹¶æ”¯æŒå¤šç§äºŒé˜¶çŸ©æ–¹æ¡ˆã€‚åŸºäºä¸¥æ ¼çš„ç†è®ºä¿è¯ï¼Œè¯¥ç®—æ³•ä»…åœ¨æ¢¯åº¦æ»¡è¶³Lipschitzè¿ç»­æ€§å‡è®¾ä¸‹æ”¶æ•›ï¼Œæ— éœ€å…¶ä»–éšæœºæ–¹æ³•å¸¸è§çš„æ¡ä»¶é™åˆ¶ã€‚è¿™ä½¿å¾—PISAèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åº”å¯¹æ•°æ®å¼‚æ„æ€§çš„æŒ‘æˆ˜ã€‚å¯¹å¤šç§FMsï¼ˆåŒ…æ‹¬è§†è§‰æ¨¡å‹ã€å¤§å‹è¯­è¨€æ¨¡å‹ã€å¼ºåŒ–å­¦ä¹ æ¨¡å‹ã€ç”Ÿæˆå¯¹æŠ—ç½‘ç»œå’Œå¾ªç¯ç¥ç»ç½‘ç»œç­‰ï¼‰çš„å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼Œå…¶æ•°å€¼æ€§èƒ½ä¼˜äºå„ç§æœ€å…ˆè¿›çš„ä¼˜åŒ–å™¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§è§„æ¨¡æ¨¡å‹ï¼ˆFMsï¼‰çš„è¿›å±•å¸¦æ¥äº†å„é¢†åŸŸå˜é©ã€‚</li>
<li>å½“å‰ä¼˜åŒ–å™¨å­˜åœ¨æ”¶æ•›é€Ÿåº¦æ…¢å’Œå‡è®¾æ¡ä»¶ä¸¥æ ¼ç­‰é—®é¢˜ã€‚</li>
<li>æ•°æ®å¼‚æ„æ€§å¯¹ä¼˜åŒ–å™¨çš„ç†è®ºå’Œæ•°å€¼æ€§èƒ½å¸¦æ¥äº†æŒ‘æˆ˜ã€‚</li>
<li>PISAç®—æ³•å…·æœ‰å¯æ‰©å±•çš„å¹¶è¡Œè®¡ç®—èƒ½åŠ›ï¼Œå¹¶æ”¯æŒå¤šç§äºŒé˜¶çŸ©æ–¹æ¡ˆã€‚</li>
<li>PISAåŸºäºä¸¥æ ¼çš„ç†è®ºä¿è¯ï¼Œåœ¨æ¢¯åº¦æ»¡è¶³Lipschitzè¿ç»­æ€§å‡è®¾ä¸‹æ”¶æ•›ã€‚</li>
<li>PISAèƒ½æœ‰æ•ˆåº”å¯¹æ•°æ®å¼‚æ„æ€§çš„æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.10784">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-76a6b3297f2b4fba0f98c8d3322c33cc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7606932a9d2546286ab0b961ac2c40a4.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Gradient-Based-Multi-Objective-Deep-Learning-Algorithms-Theories-Applications-and-Beyond"><a href="#Gradient-Based-Multi-Objective-Deep-Learning-Algorithms-Theories-Applications-and-Beyond" class="headerlink" title="Gradient-Based Multi-Objective Deep Learning: Algorithms, Theories,   Applications, and Beyond"></a>Gradient-Based Multi-Objective Deep Learning: Algorithms, Theories,   Applications, and Beyond</h2><p><strong>Authors:Weiyu Chen, Xiaoyuan Zhang, Baijiong Lin, Xi Lin, Han Zhao, Qingfu Zhang, James T. Kwok</strong></p>
<p>Multi-objective optimization (MOO) in deep learning aims to simultaneously optimize multiple conflicting objectives, a challenge frequently encountered in areas like multi-task learning and multi-criteria learning. Recent advancements in gradient-based MOO methods have enabled the discovery of diverse types of solutions, ranging from a single balanced solution to finite or even infinite Pareto sets, tailored to user needs. These developments have broad applications across domains such as reinforcement learning, computer vision, recommendation systems, and large language models. This survey provides the first comprehensive review of gradient-based MOO in deep learning, covering algorithms, theories, and practical applications. By unifying various approaches and identifying critical challenges, it serves as a foundational resource for driving innovation in this evolving field. A comprehensive list of MOO algorithms in deep learning is available at <a target="_blank" rel="noopener" href="https://github.com/Baijiong-Lin/Awesome-Multi-Objective-Deep-Learning">https://github.com/Baijiong-Lin/Awesome-Multi-Objective-Deep-Learning</a>. </p>
<blockquote>
<p>æ·±åº¦å­¦ä¹ ä¸­çš„å¤šç›®æ ‡ä¼˜åŒ–ï¼ˆMOOï¼‰æ—¨åœ¨åŒæ—¶ä¼˜åŒ–å¤šä¸ªç›¸äº’å†²çªçš„ç›®æ ‡ï¼Œè¿™åœ¨å¤šä»»åŠ¡å­¦ä¹ å’Œå¤šæ ‡å‡†å­¦ä¹ ç­‰é¢†åŸŸä¸­ç»å¸¸é‡åˆ°æŒ‘æˆ˜ã€‚åŸºäºæ¢¯åº¦çš„MOOæ–¹æ³•çš„æœ€æ–°è¿›å±•å·²ç»èƒ½å¤Ÿå‘ç°å¤šç§è§£å†³æ–¹æ¡ˆï¼Œä»å•ä¸€å¹³è¡¡è§£å†³æ–¹æ¡ˆåˆ°æœ‰é™ç”šè‡³æ— é™çš„å¸•ç´¯æ‰˜é›†ï¼Œè¿™äº›è§£å†³æ–¹æ¡ˆéƒ½ç¬¦åˆç”¨æˆ·çš„éœ€æ±‚ã€‚è¿™äº›å‘å±•åœ¨å¼ºåŒ–å­¦ä¹ ã€è®¡ç®—æœºè§†è§‰ã€æ¨èç³»ç»Ÿå’Œå¤§å‹è¯­è¨€æ¨¡å‹ç­‰é¢†åŸŸæœ‰å¹¿æ³›åº”ç”¨ã€‚æœ¬æ–‡æä¾›äº†åŸºäºæ¢¯åº¦çš„æ·±åº¦å­¦ä¹ MOOçš„é¦–ä»½å…¨é¢ç»¼è¿°ï¼Œæ¶µç›–äº†ç®—æ³•ã€ç†è®ºå’Œå®è·µåº”ç”¨ã€‚é€šè¿‡ç»Ÿä¸€å„ç§æ–¹æ³•å’Œç¡®å®šå…³é”®æŒ‘æˆ˜ï¼Œå®ƒæˆä¸ºæ¨åŠ¨è¿™ä¸€ä¸æ–­å‘å±•çš„é¢†åŸŸåˆ›æ–°çš„åŸºç¡€èµ„æºã€‚æ·±åº¦å­¦ä¹ ä¸­çš„MOOç®—æ³•çš„ç»¼åˆåˆ—è¡¨å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Baijiong-Lin/Awesome-Multi-Objective-Deep-Learning">https://github.com/Baijiong-Lin/Awesome-Multi-Objective-Deep-Learning</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.10945v2">PDF</a> </p>
<p><strong>Summary</strong><br>     æ·±åº¦å­¦ä¹ ä¸­çš„å¤šç›®æ ‡ä¼˜åŒ–ï¼ˆMOOï¼‰æ—¨åœ¨åŒæ—¶ä¼˜åŒ–å¤šä¸ªå†²çªç›®æ ‡ï¼Œè¿™åœ¨å¤šä»»åŠ¡å­¦ä¹ å’Œå¤šæ ‡å‡†å­¦ä¹ ä¸­ç»å¸¸é‡åˆ°ã€‚è¿‘æœŸåŸºäºæ¢¯åº¦çš„MOOæ–¹æ³•çš„å‘å±•ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿå‘ç°å„ç§ç±»å‹çš„è§£å†³æ–¹æ¡ˆï¼Œä»å•ä¸€å¹³è¡¡è§£å†³æ–¹æ¡ˆåˆ°æœ‰é™çš„ç”šè‡³æ— é™çš„å¸•ç´¯æ‰˜é›†ï¼Œæ»¡è¶³ä¸åŒéœ€æ±‚ã€‚è¯¥æ–¹æ³•å¹¿æ³›åº”ç”¨äºå¼ºåŒ–å­¦ä¹ ã€è®¡ç®—æœºè§†è§‰ã€æ¨èç³»ç»Ÿå’Œå¤§è¯­è¨€æ¨¡å‹ç­‰é¢†åŸŸã€‚è¿™ç¯‡ç»¼è¿°æ–‡ç« é¦–æ¬¡å…¨é¢å›é¡¾äº†æ·±åº¦å­¦ä¹ ä¸­çš„åŸºäºæ¢¯åº¦çš„MOOï¼Œæ¶µç›–äº†ç®—æ³•ã€ç†è®ºå’Œåº”ç”¨ã€‚å®ƒä¸ºæ¨åŠ¨è¿™ä¸€æ–°å…´é¢†åŸŸçš„åˆ›æ–°æä¾›äº†åŸºç¡€èµ„æºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šç›®æ ‡ä¼˜åŒ–ï¼ˆMOOï¼‰åœ¨æ·±åº¦å­¦ä¹ ä¸­ç”¨äºåŒæ—¶ä¼˜åŒ–å¤šä¸ªå†²çªç›®æ ‡ã€‚</li>
<li>åŸºäºæ¢¯åº¦çš„MOOæ–¹æ³•èƒ½å‘ç°å¤šç§ç±»å‹çš„è§£å†³æ–¹æ¡ˆï¼Œæ»¡è¶³ä¸åŒç”¨æˆ·éœ€æ±‚ã€‚</li>
<li>è¯¥æ–¹æ³•å¹¿æ³›åº”ç”¨äºå¼ºåŒ–å­¦ä¹ ã€è®¡ç®—æœºè§†è§‰ã€æ¨èç³»ç»Ÿå’Œå¤§è¯­è¨€æ¨¡å‹ã€‚</li>
<li>ç»¼è¿°æ–‡ç« å…¨é¢ä»‹ç»äº†æ·±åº¦å­¦ä¹ ä¸­çš„åŸºäºæ¢¯åº¦çš„MOOï¼ŒåŒ…æ‹¬ç®—æ³•ã€ç†è®ºå’Œåº”ç”¨ã€‚</li>
<li>è¯¥æ–‡ç« ä¸ºé©±åŠ¨MOOé¢†åŸŸåˆ›æ–°æä¾›äº†åŸºç¡€èµ„æºã€‚</li>
<li>æ–‡ç« ä¸­æåˆ°çš„æŒ‘æˆ˜å¯¹äºæœªæ¥ç ”ç©¶å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.10945">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4407b1694a4871873b754d5a332849e8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-22852f435352b776369e8d7b54def5d5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e52fd007f6c6342e5b2d0729d257875b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7790caf9da8b544f851b2a1826fc2cd8.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Monet-Mixture-of-Monosemantic-Experts-for-Transformers"><a href="#Monet-Mixture-of-Monosemantic-Experts-for-Transformers" class="headerlink" title="Monet: Mixture of Monosemantic Experts for Transformers"></a>Monet: Mixture of Monosemantic Experts for Transformers</h2><p><strong>Authors:Jungwoo Park, Young Jin Ahn, Kee-Eung Kim, Jaewoo Kang</strong></p>
<p>Understanding the internal computations of large language models (LLMs) is crucial for aligning them with human values and preventing undesirable behaviors like toxic content generation. However, mechanistic interpretability is hindered by polysemanticity â€“ where individual neurons respond to multiple, unrelated concepts. While Sparse Autoencoders (SAEs) have attempted to disentangle these features through sparse dictionary learning, they have compromised LLM performance due to reliance on post-hoc reconstruction loss. To address this issue, we introduce Mixture of Monosemantic Experts for Transformers (Monet) architecture, which incorporates sparse dictionary learning directly into end-to-end Mixture-of-Experts pretraining. Our novel expert decomposition method enables scaling the expert count to 262,144 per layer while total parameters scale proportionally to the square root of the number of experts. Our analyses demonstrate mutual exclusivity of knowledge across experts and showcase the parametric knowledge encapsulated within individual experts. Moreover, Monet allows knowledge manipulation over domains, languages, and toxicity mitigation without degrading general performance. Our pursuit of transparent LLMs highlights the potential of scaling expert counts to enhance mechanistic interpretability and directly resect the internal knowledge to fundamentally adjust model behavior. The source code and pretrained checkpoints are available at <a target="_blank" rel="noopener" href="https://github.com/dmis-lab/Monet">https://github.com/dmis-lab/Monet</a>. </p>
<blockquote>
<p>ç†è§£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å†…éƒ¨è®¡ç®—å¯¹äºå°†å…¶ä¸äººç±»ä»·å€¼è§‚å¯¹é½å¹¶é˜²æ­¢ç”Ÿæˆæœ‰æ¯’å†…å®¹ç­‰ä¸å½“è¡Œä¸ºè‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå¤šä¹‰æ€§ï¼ˆå³å•ä¸ªç¥ç»å…ƒå¯¹å¤šä¸ªä¸ç›¸å…³æ¦‚å¿µçš„å“åº”ï¼‰é˜»ç¢äº†æœºæ¢°è§£é‡Šæ€§ã€‚ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨ï¼ˆSAEï¼‰æ›¾è¯•å›¾é€šè¿‡ç¨€ç–å­—å…¸å­¦ä¹ æ¥è§£å¼€è¿™äº›ç‰¹å¾ï¼Œä½†ç”±äºä¾èµ–äº‹åé‡å»ºæŸå¤±è€ŒæŸå®³äº†LLMçš„æ€§èƒ½ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†â€œTransformerçš„å•è¯­ä¹‰ä¸“å®¶æ··åˆä½“â€ï¼ˆMonetï¼‰æ¶æ„ï¼Œå®ƒå°†ç¨€ç–å­—å…¸å­¦ä¹ ç›´æ¥é›†æˆåˆ°ç«¯åˆ°ç«¯çš„ä¸“å®¶æ··åˆé¢„è®­ç»ƒè¿‡ç¨‹ä¸­ã€‚æˆ‘ä»¬æ–°é¢–çš„ä¸“å®¶åˆ†è§£æ–¹æ³•èƒ½å¤Ÿå®ç°æ¯å±‚ä¸“å®¶æ•°é‡è¾¾åˆ°262,144ä¸ªï¼ŒåŒæ—¶æ€»å‚æ•°æ•°é‡ä¸ä¸“å®¶æ•°é‡çš„å¹³æ–¹æ ¹æˆæ¯”ä¾‹å¢é•¿ã€‚æˆ‘ä»¬çš„åˆ†æè¯æ˜äº†ä¸“å®¶ä¹‹é—´çŸ¥è¯†çš„ç›¸äº’ç‹¬ç«‹æ€§ï¼Œå¹¶å±•ç¤ºäº†å•ä¸ªä¸“å®¶æ‰€åŒ…å«çš„å‚æ•°çŸ¥è¯†ã€‚æ­¤å¤–ï¼ŒMonetå…è®¸åœ¨åŸŸã€è¯­è¨€å’Œæ¯’æ€§æ–¹é¢è¿›è¡ŒçŸ¥è¯†æ“æ§ï¼ŒåŒæ—¶ä¸ä¼šé™ä½æ•´ä½“æ€§èƒ½ã€‚æˆ‘ä»¬å¯¹é€æ˜LLMçš„è¿½æ±‚çªæ˜¾äº†å¢åŠ ä¸“å®¶æ•°é‡ä»¥æå‡æœºæ¢°è§£é‡Šæ€§çš„æ½œåŠ›ï¼Œå¹¶å¯ç›´æ¥åˆ‡é™¤å†…éƒ¨çŸ¥è¯†ä»¥ä»æ ¹æœ¬ä¸Šè°ƒæ•´æ¨¡å‹è¡Œä¸ºã€‚æºä»£ç å’Œé¢„è®­ç»ƒæ£€æŸ¥ç‚¹å¯è®¿é—®<a target="_blank" rel="noopener" href="https://github.com/dmis-lab/Monet%E3%80%82">https://github.com/dmis-lab/Monetã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04139v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å†…éƒ¨è®¡ç®—ç†è§£å¯¹äºä¸äººç±»ä»·å€¼è§‚å¯¹é½åŠé˜²æ­¢ç”Ÿæˆæœ‰æ¯’å†…å®¹ç­‰ä¸å¯å–è¡Œä¸ºè‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç”±äºç¥ç»å…ƒçš„å¤šä¹‰æ€§ï¼ˆå³å•ä¸ªç¥ç»å…ƒå¯¹å¤šä¸ªæ— å…³æ¦‚å¿µçš„å“åº”ï¼‰ï¼Œæœºæ¢°è§£é‡Šæ€§å—é˜»ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥Monetæ¶æ„ï¼Œå°†ç¨€ç–å­—å…¸å­¦ä¹ ç›´æ¥èå…¥ç«¯åˆ°ç«¯çš„ä¸“å®¶æ··åˆé¢„è®­ç»ƒã€‚Monetçš„æ–°å‹ä¸“å®¶åˆ†è§£æ³•å¯å®ç°æ¯å±‚ä¸“å®¶æ•°é‡å¢è‡³262,144ä¸ªï¼ŒåŒæ—¶æ€»å‚æ•°ä¸ä¸“å®¶æ•°é‡çš„å¹³æ–¹æ ¹æˆæ¯”ä¾‹å¢é•¿ã€‚åˆ†ææ˜¾ç¤ºï¼Œä¸“å®¶é—´çš„çŸ¥è¯†ç›¸äº’ç‹¬ç«‹ï¼Œå±•ç¤ºäº†å•ä¸ªä¸“å®¶å†…éƒ¨çš„å‚æ•°çŸ¥è¯†ã€‚æ­¤å¤–ï¼ŒMonetå¯å®ç°è·¨é¢†åŸŸã€è¯­è¨€çš„çŸ¥è¯†æ“æ§åŠæ¯’æ€§ç¼“è§£ï¼Œä¸”ä¸å½±å“æ•´ä½“æ€§èƒ½ã€‚æˆ‘ä»¬å¯¹é€æ˜LLMçš„è¿½æ±‚çªæ˜¾äº†å¢åŠ ä¸“å®¶æ•°é‡ä»¥æå‡æœºæ¢°è§£é‡Šæ€§çš„æ½œåŠ›ï¼Œå¹¶å¯ç›´æ¥è°ƒæ•´å†…éƒ¨çŸ¥è¯†ä»¥æ”¹å˜æ¨¡å‹è¡Œä¸ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç†è§£LLMçš„å†…éƒ¨è®¡ç®—å¯¹äºä¸äººç±»ä»·å€¼è§‚å¯¹é½å’Œé˜²æ­¢ä¸è‰¯è¡Œä¸ºè‡³å…³é‡è¦ã€‚</li>
<li>ç¥ç»å…ƒçš„å¤šä¹‰æ€§é˜»ç¢äº†æœºæ¢°è§£é‡Šæ€§ã€‚</li>
<li>Sparse Autoencodersï¼ˆSAEï¼‰å°è¯•é€šè¿‡ç¨€ç–å­—å…¸å­¦ä¹ è§£å¼€è¿™äº›ç‰¹å¾ï¼Œä½†å½±å“äº†LLMæ€§èƒ½ã€‚</li>
<li>å¼•å…¥Monetæ¶æ„ï¼Œå°†ç¨€ç–å­—å…¸å­¦ä¹ ç›´æ¥èå…¥ä¸“å®¶æ··åˆé¢„è®­ç»ƒçš„ç«¯åˆ°ç«¯è¿‡ç¨‹ã€‚</li>
<li>Monetçš„æ–°å‹ä¸“å®¶åˆ†è§£æ³•å…è®¸æ¯å±‚ä¸“å®¶æ•°é‡å¤§å¹…å¢åŠ ã€‚</li>
<li>ä¸“å®¶é—´çš„çŸ¥è¯†ç›¸äº’ç‹¬ç«‹ï¼Œå±•ç¤ºäº†å•ä¸ªä¸“å®¶çš„å‚æ•°çŸ¥è¯†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.04139">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-98e2d040d4283c73ac76f00b755bf246.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b5950e0b2ac2df741f831b6736f45726.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89f8b96b14d87014d76a7f04f96333ae.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Controllable-Context-Sensitivity-and-the-Knob-Behind-It"><a href="#Controllable-Context-Sensitivity-and-the-Knob-Behind-It" class="headerlink" title="Controllable Context Sensitivity and the Knob Behind It"></a>Controllable Context Sensitivity and the Knob Behind It</h2><p><strong>Authors:Julian Minder, Kevin Du, Niklas Stoehr, Giovanni Monea, Chris Wendler, Robert West, Ryan Cotterell</strong></p>
<p>When making predictions, a language model must trade off how much it relies on its context vs. its prior knowledge. Choosing how sensitive the model is to its context is a fundamental functionality, as it enables the model to excel at tasks like retrieval-augmented generation and question-answering. In this paper, we search for a knob which controls this sensitivity, determining whether language models answer from the context or their prior knowledge. To guide this search, we design a task for controllable context sensitivity. In this task, we first feed the model a context (Paris is in England) and a question (Where is Paris?); we then instruct the model to either use its prior or contextual knowledge and evaluate whether it generates the correct answer for both intents (either France or England). When fine-tuned on this task, instruction-tuned versions of Llama-3.1, Mistral-v0.3, and Gemma-2 can solve it with high accuracy (85-95%). Analyzing these high-performing models, we narrow down which layers may be important to context sensitivity using a novel linear time algorithm. Then, in each model, we identify a 1-D subspace in a single layer that encodes whether the model follows context or prior knowledge. Interestingly, while we identify this subspace in a fine-tuned model, we find that the exact same subspace serves as an effective knob in not only that model but also non-fine-tuned instruct and base models of that model family. Finally, we show a strong correlation between a modelâ€™s performance and how distinctly it separates context-agreeing from context-ignoring answers in this subspace. These results suggest a single subspace facilitates how the model chooses between context and prior knowledge, hinting at a simple fundamental mechanism that controls this behavior. </p>
<blockquote>
<p>åœ¨åšå‡ºé¢„æµ‹æ—¶ï¼Œè¯­è¨€æ¨¡å‹å¿…é¡»åœ¨ä¾èµ–ä¸Šä¸‹æ–‡å’Œå…ˆå‰çŸ¥è¯†ä¹‹é—´è¿›è¡Œæƒè¡¡ã€‚é€‰æ‹©æ¨¡å‹å¯¹ä¸Šä¸‹æ–‡çš„æ•æ„Ÿç¨‹åº¦æ˜¯ä¸€ä¸ªåŸºæœ¬åŠŸèƒ½ï¼Œå› ä¸ºè¿™ä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨è¯¸å¦‚æ£€ç´¢å¢å¼ºç”Ÿæˆå’Œé—®ç­”ç­‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¯»æ‰¾ä¸€ä¸ªæ§åˆ¶è¿™ç§æ•æ„Ÿæ€§çš„æ—‹é’®ï¼Œä»¥ç¡®å®šè¯­è¨€æ¨¡å‹æ˜¯ä»ä¸Šä¸‹æ–‡è¿˜æ˜¯å…ˆå‰çŸ¥è¯†ä¸­å¾—å‡ºç­”æ¡ˆã€‚ä¸ºäº†æŒ‡å¯¼è¿™æ¬¡æœç´¢ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå¯æ§ä¸Šä¸‹æ–‡æ•æ„Ÿåº¦çš„ä»»åŠ¡ã€‚åœ¨æ­¤ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆå‘æ¨¡å‹æä¾›ä¸Šä¸‹æ–‡ï¼ˆä¾‹å¦‚â€œå·´é»åœ¨è‹±å›½â€ï¼‰å’Œé—®é¢˜ï¼ˆâ€œå·´é»åœ¨å“ªé‡Œï¼Ÿâ€ï¼‰ï¼›ç„¶åæŒ‡ç¤ºæ¨¡å‹ä½¿ç”¨å…¶å…ˆå‰çš„çŸ¥è¯†æˆ–ä¸Šä¸‹æ–‡çŸ¥è¯†ï¼Œå¹¶è¯„ä¼°å®ƒæ˜¯å¦èƒ½å¤Ÿé’ˆå¯¹ä¸¤ç§æ„å›¾ï¼ˆæ³•å›½æˆ–è‹±å›½ï¼‰ç»™å‡ºæ­£ç¡®çš„ç­”æ¡ˆã€‚åœ¨æ­¤ä»»åŠ¡ä¸Šè¿›è¡Œå¾®è°ƒåï¼ŒLlama-3.1ã€Mistral-v0.3å’ŒGemma-2çš„æŒ‡ä»¤è°ƒæ•´ç‰ˆå¯ä»¥ä»¥é«˜å‡†ç¡®ç‡ï¼ˆ85-95%ï¼‰è§£å†³æ­¤é—®é¢˜ã€‚é€šè¿‡åˆ†æè¿™äº›é«˜æ€§èƒ½æ¨¡å‹ï¼Œæˆ‘ä»¬åˆ©ç”¨ä¸€ç§æ–°é¢–çš„çº¿æ€§æ—¶é—´ç®—æ³•ç¼©å°äº†å¯¹ä¸Šä¸‹æ–‡æ•æ„Ÿåº¦è‡³å…³é‡è¦çš„å±‚ã€‚ç„¶åï¼Œåœ¨æ¯ä¸ªæ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬åœ¨å•å±‚ä¸­è¯†åˆ«å‡ºä¸€ä¸ªä¸€ç»´å­ç©ºé—´ï¼Œè¯¥å­ç©ºé—´ç¼–ç æ¨¡å‹æ˜¯éµå¾ªä¸Šä¸‹æ–‡è¿˜æ˜¯å…ˆå‰çŸ¥è¯†ã€‚æœ‰è¶£çš„æ˜¯ï¼Œè™½ç„¶æˆ‘ä»¬åœ¨ç»è¿‡ç²¾ç»†è°ƒæ•´çš„æ¨¡å‹ä¸­ç¡®å®šäº†æ­¤å­ç©ºé—´ï¼Œä½†æˆ‘ä»¬å‘ç°è¯¥å­ç©ºé—´ä¸ä»…åœ¨è¯¥æ¨¡å‹ä¸­æœ‰æ•ˆï¼Œè€Œä¸”åœ¨è¯¥æ¨¡å‹ç³»åˆ—çš„å…¶ä»–æœªç»è¿‡ç²¾ç»†è°ƒæ•´å’ŒæŒ‡ä»¤çš„æ¨¡å‹ä¸­åŒæ ·æœ‰æ•ˆã€‚æœ€åï¼Œæˆ‘ä»¬å‘ç°åœ¨æ­¤å­ç©ºé—´ä¸­ï¼Œæ¨¡å‹çš„æ€§èƒ½ä¸åŒºåˆ†ä¸Šä¸‹æ–‡ä¸€è‡´å’Œå¿½ç•¥ä¸Šä¸‹æ–‡çš„ç­”æ¡ˆä¹‹é—´å­˜åœ¨å¾ˆå¼ºçš„ç›¸å…³æ€§ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œå•ä¸ªå­ç©ºé—´ä¿ƒè¿›äº†æ¨¡å‹åœ¨ä¸Šä¸‹æ–‡å’Œå…ˆå‰çŸ¥è¯†ä¹‹é—´çš„é€‰æ‹©ï¼Œæš—ç¤ºäº†ä¸€ä¸ªç®€å•çš„åŸºæœ¬æœºåˆ¶å¯ä»¥æ§åˆ¶è¿™ç§è¡Œä¸ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.07404v2">PDF</a> Published as a conference paper at ICLR 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†è¯­è¨€æ¨¡å‹åœ¨é¢„æµ‹æ—¶å¦‚ä½•å¹³è¡¡ä¸Šä¸‹æ–‡ä¾èµ–ä¸å…ˆéªŒçŸ¥è¯†ä¾èµ–çš„æƒè¡¡é—®é¢˜ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§æ§åˆ¶è¯­å¢ƒæ•æ„Ÿåº¦çš„ä»»åŠ¡ï¼Œé€šè¿‡è®¾è®¡ç‰¹å®šä»»åŠ¡æ¥æŒ‡å¯¼æ¨¡å‹å¦‚ä½•é€‰æ‹©ä½¿ç”¨ä¸Šä¸‹æ–‡çŸ¥è¯†æˆ–å…ˆéªŒçŸ¥è¯†ã€‚ç ”ç©¶å‘ç°ï¼Œé€šè¿‡å¾®è°ƒä»»åŠ¡ï¼ŒæŸäº›æ¨¡å‹èƒ½å¤Ÿå‡†ç¡®åœ°åœ¨ä¸åŒæƒ…å¢ƒä¸‹é€‰æ‹©ä½¿ç”¨ä¸Šä¸‹æ–‡çŸ¥è¯†æˆ–å…ˆéªŒçŸ¥è¯†ï¼Œå¹¶è¡¨ç°å‡ºé«˜å‡†ç¡®ç‡ã€‚è¿›ä¸€æ­¥çš„åˆ†ææ­ç¤ºäº†æ§åˆ¶è¯­å¢ƒæ•æ„Ÿåº¦çš„å…³é”®å±‚çº§å’Œå­ç©ºé—´ï¼Œè¿™äº›å­ç©ºé—´åœ¨ä¸åŒæ¨¡å‹å’Œæƒ…å¢ƒä¸‹å‡æœ‰æ•ˆã€‚ç ”ç©¶è¿˜å‘ç°æ¨¡å‹æ€§èƒ½ä¸åœ¨å­ç©ºé—´ä¸­åŒºåˆ†ä¸Šä¸‹æ–‡ä¸€è‡´å’Œå¿½ç•¥ä¸Šä¸‹æ–‡çš„ç­”æ¡ˆçš„èƒ½åŠ›ä¹‹é—´å­˜åœ¨å¼ºçƒˆå…³è”ã€‚è¿™äº›ç»“æœæ­ç¤ºäº†å•ä¸€å­ç©ºé—´å¦‚ä½•å¸®åŠ©æ¨¡å‹åœ¨ä¸Šä¸‹æ–‡å’Œå…ˆéªŒçŸ¥è¯†ä¹‹é—´åšå‡ºé€‰æ‹©ï¼Œå¹¶æš—ç¤ºäº†ä¸€ç§æ§åˆ¶è¿™ç§è¡Œä¸ºçš„åŸºæœ¬æœºåˆ¶ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è¯­è¨€æ¨¡å‹åœ¨é¢„æµ‹æ—¶éœ€æƒè¡¡ä¸Šä¸‹æ–‡ä¾èµ–ä¸å…ˆéªŒçŸ¥è¯†ä¾èµ–ã€‚</li>
<li>é€šè¿‡è®¾è®¡ç‰¹å®šä»»åŠ¡æ¥æŒ‡å¯¼æ¨¡å‹å¦‚ä½•é€‰æ‹©ä½¿ç”¨ä¸Šä¸‹æ–‡çŸ¥è¯†æˆ–å…ˆéªŒçŸ¥è¯†ã€‚</li>
<li>é«˜æ€§èƒ½æ¨¡å‹èƒ½å¤Ÿé€šè¿‡å¾®è°ƒä»»åŠ¡æ¥å‡†ç¡®åœ°åœ¨ä¸åŒæƒ…å¢ƒä¸‹é€‰æ‹©ä½¿ç”¨ä¸Šä¸‹æ–‡çŸ¥è¯†æˆ–å…ˆéªŒçŸ¥è¯†ã€‚</li>
<li>æ­ç¤ºäº†æ§åˆ¶è¯­å¢ƒæ•æ„Ÿåº¦çš„å…³é”®å±‚çº§å’Œå­ç©ºé—´ã€‚</li>
<li>è¿™äº›å­ç©ºé—´ä¸ä»…é€‚ç”¨äºå¾®è°ƒè¿‡çš„æ¨¡å‹ï¼Œä¹Ÿé€‚ç”¨äºæœªç»è¿‡è°ƒæ•™çš„æ¨¡å‹å’Œè¯¥æ¨¡å‹å®¶æ—çš„åŸºç¡€æ¨¡å‹ã€‚</li>
<li>æ¨¡å‹æ€§èƒ½ä¸å­ç©ºé—´ä¸­åŒºåˆ†ä¸Šä¸‹æ–‡ä¸€è‡´æ€§å’Œå¿½ç•¥ä¸Šä¸‹æ–‡çš„ç­”æ¡ˆçš„èƒ½åŠ›ä¹‹é—´å­˜åœ¨å¼ºçƒˆå…³è”ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.07404">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-67195903ce13f5efee69fb320e202524.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2687419b9cb79a95096dc67514c2eda3.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="CUIfy-the-XR-An-Open-Source-Package-to-Embed-LLM-powered-Conversational-Agents-in-XR"><a href="#CUIfy-the-XR-An-Open-Source-Package-to-Embed-LLM-powered-Conversational-Agents-in-XR" class="headerlink" title="CUIfy the XR: An Open-Source Package to Embed LLM-powered Conversational   Agents in XR"></a>CUIfy the XR: An Open-Source Package to Embed LLM-powered Conversational   Agents in XR</h2><p><strong>Authors:Kadir Burak Buldu, SÃ¼leyman Ã–zdel, Ka Hei Carrie Lau, Mengdi Wang, Daniel Saad, Sofie SchÃ¶nborn, Auxane Boch, Enkelejda Kasneci, Efe Bozkir</strong></p>
<p>Recent developments in computer graphics, machine learning, and sensor technologies enable numerous opportunities for extended reality (XR) setups for everyday life, from skills training to entertainment. With large corporations offering affordable consumer-grade head-mounted displays (HMDs), XR will likely become pervasive, and HMDs will develop as personal devices like smartphones and tablets. However, having intelligent spaces and naturalistic interactions in XR is as important as technological advances so that users grow their engagement in virtual and augmented spaces. To this end, large language model (LLM)â€“powered non-player characters (NPCs) with speech-to-text (STT) and text-to-speech (TTS) models bring significant advantages over conventional or pre-scripted NPCs for facilitating more natural conversational user interfaces (CUIs) in XR. This paper provides the community with an open-source, customizable, extendable, and privacy-aware Unity package, CUIfy, that facilitates speech-based NPC-user interaction with widely used LLMs, STT, and TTS models. Our package also supports multiple LLM-powered NPCs per environment and minimizes latency between different computational models through streaming to achieve usable interactions between users and NPCs. We publish our source code in the following repository: <a target="_blank" rel="noopener" href="https://gitlab.lrz.de/hctl/cuify">https://gitlab.lrz.de/hctl/cuify</a> </p>
<blockquote>
<p>æœ€è¿‘çš„è®¡ç®—æœºå›¾å½¢å­¦ã€æœºå™¨å­¦ä¹ å’Œä¼ æ„Ÿå™¨æŠ€æœ¯çš„å‘å±•ä¸ºæ‰©å±•ç°å®ï¼ˆXRï¼‰åœ¨æ—¥å¸¸ç”Ÿæ´»ä¸­çš„è¿ç”¨æä¾›äº†ä¼—å¤šæœºä¼šï¼Œæ— è®ºæ˜¯æŠ€èƒ½åŸ¹è®­è¿˜æ˜¯å¨±ä¹ã€‚éšç€å¤§å‹ä¼ä¸šæä¾›ç»æµå®æƒ çš„æ¶ˆè´¹çº§å¤´æˆ´æ˜¾ç¤ºå™¨ï¼ˆHMDï¼‰ï¼ŒXRå¾ˆå¯èƒ½å˜å¾—æ™®åŠï¼ŒHMDå°†å‘å±•æˆä¸ºåƒæ™ºèƒ½æ‰‹æœºå’Œå¹³æ¿ç”µè„‘ä¸€æ ·çš„ä¸ªäººè®¾å¤‡ã€‚ç„¶è€Œï¼Œæ‹¥æœ‰æ™ºèƒ½ç©ºé—´å’Œè‡ªç„¶äº¤äº’çš„XRä¸ç§‘æŠ€è¿›æ­¥ä¸€æ ·é‡è¦ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿåœ¨è™šæ‹Ÿå’Œå¢å¼ºç©ºé—´ä¸­å¢åŠ å‚ä¸åº¦ã€‚ä¸ºæ­¤ï¼Œä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„éç©å®¶è§’è‰²ï¼ˆNPCï¼‰é€šè¿‡è¯­éŸ³è½¬æ–‡æœ¬ï¼ˆSTTï¼‰å’Œæ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹ï¼Œä¸ºXRä¸­æ›´è‡ªç„¶çš„å¯¹è¯å¼ç”¨æˆ·ç•Œé¢ï¼ˆCUIï¼‰å¸¦æ¥äº†ç›¸è¾ƒäºä¼ ç»Ÿæˆ–é¢„è®¾NPCçš„æ˜¾è‘—ä¼˜åŠ¿ã€‚æœ¬æ–‡ä¸ºç¤¾åŒºæä¾›äº†ä¸€ä¸ªå¼€æºã€å¯å®šåˆ¶ã€å¯æ‰©å±•ä¸”æ³¨é‡éšç§çš„Unityè½¯ä»¶åŒ…CUIfyï¼Œå®ƒä¿ƒè¿›äº†åŸºäºè¯­éŸ³çš„NPC-ç”¨æˆ·äº¤äº’ï¼Œå¹¿æ³›ä½¿ç”¨äº†LLMã€STTå’ŒTTSæ¨¡å‹ã€‚æˆ‘ä»¬çš„è½¯ä»¶åŒ…è¿˜æ”¯æŒæ¯ä¸ªç¯å¢ƒå¤šä¸ªLLMé©±åŠ¨çš„NPCï¼Œå¹¶é€šè¿‡æµå¼ä¼ è¾“å‡å°‘ä¸åŒè®¡ç®—æ¨¡å‹ä¹‹é—´çš„å»¶è¿Ÿï¼Œä»è€Œå®ç°ç”¨æˆ·ä¸NPCä¹‹é—´çš„å¯ç”¨äº¤äº’ã€‚æˆ‘ä»¬åœ¨ä»¥ä¸‹ä»“åº“ä¸­å‘å¸ƒæºä»£ç ï¼š<a target="_blank" rel="noopener" href="https://gitlab.lrz.de/hctl/cuify">https://gitlab.lrz.de/hctl/cuify</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.04671v3">PDF</a> 7th IEEE International Conference on Artificial Intelligence &amp;   eXtended and Virtual Reality (IEEE AIxVR 2025)</p>
<p><strong>Summary</strong></p>
<p>éšç€è®¡ç®—æœºå›¾å½¢å­¦ã€æœºå™¨å­¦ä¹ å’Œä¼ æ„Ÿå™¨æŠ€æœ¯çš„æœ€æ–°å‘å±•ï¼Œæ‰©å±•ç°å®ï¼ˆXRï¼‰è®¾ç½®åœ¨æ—¥å¸¸ç”Ÿæ´»ä¸­çš„æœºä¼šæ—¥ç›Šå¢å¤šï¼Œä»æŠ€èƒ½åŸ¹è®­åˆ°å¨±ä¹çš†å¯åº”ç”¨ã€‚å¤§å‹ä¼ä¸šæ¨å‡ºçš„ç»æµå®æƒ çš„æ¶ˆè´¹çº§å¤´æˆ´å¼æ˜¾ç¤ºå™¨ï¼ˆHMDsï¼‰å°†ä½¿XRæ™®åŠï¼ŒHMDså°†åƒæ™ºèƒ½æ‰‹æœºå’Œå¹³æ¿ç”µè„‘ä¸€æ ·æˆä¸ºä¸ªäººè®¾å¤‡ã€‚ç„¶è€Œï¼Œå®ç°XRä¸­çš„æ™ºèƒ½ç©ºé—´å’Œè‡ªç„¶äº¤äº’ä¸ç§‘æŠ€è¿›æ­¥åŒæ ·é‡è¦ï¼Œä»¥ä½¿ç”¨æˆ·å¢åŠ å¯¹è™šæ‹Ÿå’Œå¢å¼ºç©ºé—´çš„å‚ä¸åº¦ã€‚ä¸ºæ­¤ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„éç©å®¶è§’è‰²ï¼ˆNPCsï¼‰ç»“åˆè¯­éŸ³è¯†åˆ«å’Œæ–‡æœ¬ç”ŸæˆæŠ€æœ¯ï¼Œå¯ä¸ºXRä¸­çš„ç”¨æˆ·å¸¦æ¥æ›´è‡ªç„¶çš„è¯­è¨€ç”¨æˆ·ç•Œé¢ï¼ˆCUIï¼‰ã€‚æœ¬æ–‡å‘ç¤¾åŒºæä¾›äº†ä¸€ä¸ªå¼€æºã€å¯å®šåˆ¶ã€å¯æ‰©å±•ä¸”æ³¨é‡éšç§çš„Unityè½¯ä»¶åŒ…CUIfyï¼Œå®ƒä¿ƒè¿›äº†åŸºäºè¯­éŸ³çš„NPC-ç”¨æˆ·ä¸å¹¿æ³›ä½¿ç”¨çš„LLMã€è¯­éŸ³è¯†åˆ«å’Œæ–‡æœ¬ç”Ÿæˆæ¨¡å‹çš„äº¤äº’ã€‚æˆ‘ä»¬çš„è½¯ä»¶åŒ…è¿˜æ”¯æŒæ¯ä¸ªç¯å¢ƒå¤šä¸ªLLMé©±åŠ¨çš„NPCï¼Œå¹¶é€šè¿‡æµæŠ€æœ¯æœ€å°åŒ–ä¸åŒè®¡ç®—æ¨¡å‹ä¹‹é—´çš„å»¶è¿Ÿï¼Œä»¥å®ç°ç”¨æˆ·å’ŒNPCä¹‹é—´çš„å¯ç”¨äº¤äº’ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©å±•ç°å®ï¼ˆXRï¼‰åœ¨æ—¥å¸¸ç”Ÿæ´»ä¸­çš„æœºä¼šå¢å¤šï¼Œæ¶‰åŠæŠ€èƒ½åŸ¹è®­ã€å¨±ä¹ç­‰å¤šä¸ªé¢†åŸŸã€‚</li>
<li>å¤´æˆ´å¼æ˜¾ç¤ºå™¨ï¼ˆHMDsï¼‰å°†æˆä¸ºä¸ªäººè®¾å¤‡ï¼Œå¦‚åŒæ™ºèƒ½æ‰‹æœºå’Œå¹³æ¿ç”µè„‘ã€‚</li>
<li>æ™ºèƒ½ç©ºé—´å’Œè‡ªç„¶äº¤äº’åœ¨XRä¸­çš„é‡è¦æ€§ä¸äºšäºç§‘æŠ€è¿›æ­¥ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„éç©å®¶è§’è‰²ï¼ˆNPCsï¼‰ç»“åˆè¯­éŸ³è¯†åˆ«å’Œæ–‡æœ¬ç”ŸæˆæŠ€æœ¯ï¼Œå¯å®ç°æ›´è‡ªç„¶çš„è¯­è¨€ç”¨æˆ·ç•Œé¢ï¼ˆCUIï¼‰ã€‚</li>
<li>æœ¬æ–‡æä¾›äº†ä¸€ä¸ªå¼€æºçš„Unityè½¯ä»¶åŒ…CUIfyï¼Œæ”¯æŒåŸºäºè¯­éŸ³çš„NPC-ç”¨æˆ·äº¤äº’ã€‚</li>
<li>CUIfyæ”¯æŒå¤šä¸ªLLMé©±åŠ¨çš„NPCåœ¨åŒä¸€ç¯å¢ƒå†…ï¼Œæé«˜äº¤äº’å¤šæ ·æ€§ã€‚</li>
<li>é€šè¿‡æµæŠ€æœ¯æœ€å°åŒ–è®¡ç®—æ¨¡å‹é—´çš„å»¶è¿Ÿï¼Œæé«˜ç”¨æˆ·ä¸NPCä¹‹é—´çš„äº¤äº’ä½“éªŒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.04671">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b0ebd2f9422e2d2b3a61905fe37529e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-90d7d8da6b3bb3b9a79de2dd574a8070.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-273f6ac1d026cc6465dee7b4dc46e0d1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b7298009103c19166d86615b5e16a162.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Relaxed-Recursive-Transformers-Effective-Parameter-Sharing-with-Layer-wise-LoRA"><a href="#Relaxed-Recursive-Transformers-Effective-Parameter-Sharing-with-Layer-wise-LoRA" class="headerlink" title="Relaxed Recursive Transformers: Effective Parameter Sharing with   Layer-wise LoRA"></a>Relaxed Recursive Transformers: Effective Parameter Sharing with   Layer-wise LoRA</h2><p><strong>Authors:Sangmin Bae, Adam Fisch, Hrayr Harutyunyan, Ziwei Ji, Seungyeon Kim, Tal Schuster</strong></p>
<p>Large language models (LLMs) are expensive to deploy. Parameter sharing offers a possible path towards reducing their size and cost, but its effectiveness in modern LLMs remains fairly limited. In this work, we revisit â€œlayer tyingâ€ as form of parameter sharing in Transformers, and introduce novel methods for converting existing LLMs into smaller â€œRecursive Transformersâ€ that share parameters across layers, with minimal loss of performance. Here, our Recursive Transformers are efficiently initialized from standard pretrained Transformers, but only use a single block of unique layers that is then repeated multiple times in a loop. We further improve performance by introducing Relaxed Recursive Transformers that add flexibility to the layer tying constraint via depth-wise low-rank adaptation (LoRA) modules, yet still preserve the compactness of the overall model. We show that our recursive models (e.g., recursive Gemma 1B) outperform both similar-sized vanilla pretrained models (such as TinyLlama 1.1B and Pythia 1B) and knowledge distillation baselines â€“ and can even recover most of the performance of the original â€œfull-sizeâ€ model (e.g., Gemma 2B with no shared parameters). Finally, we propose Continuous Depth-wise Batching, a promising new inference paradigm enabled by the Recursive Transformer when paired with early exiting. In a theoretical analysis, we show that this has the potential to lead to significant (2-3x) gains in inference throughput. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„éƒ¨ç½²æˆæœ¬å¾ˆé«˜ã€‚å‚æ•°å…±äº«ä¸ºå®ç°å‡å°‘å…¶è§„æ¨¡å’Œæˆæœ¬çš„å¯èƒ½è·¯å¾„æä¾›äº†æœºä¼šï¼Œä½†åœ¨ç°ä»£LLMä¸­çš„æœ‰æ•ˆæ€§ä»ç„¶ç›¸å½“æœ‰é™ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é‡æ–°ç ”ç©¶äº†Transformerä¸­çš„å‚æ•°å…±äº«å½¢å¼â€œå±‚ç»‘å®šâ€ï¼Œå¹¶ä»‹ç»äº†å°†ç°æœ‰LLMè½¬æ¢ä¸ºè¾ƒå°çš„â€œé€’å½’Transformerâ€çš„æ–°æ–¹æ³•ï¼Œè¿™äº›é€’å½’Transformeråœ¨å±‚ä¹‹é—´å…±äº«å‚æ•°ï¼ŒåŒæ—¶æ€§èƒ½æŸå¤±æœ€å°ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬çš„é€’å½’Transformeræ˜¯ä»æ ‡å‡†é¢„è®­ç»ƒTransformeræœ‰æ•ˆåˆå§‹åŒ–çš„ï¼Œä½†åªä½¿ç”¨å•ä¸ªå”¯ä¸€å±‚å—ï¼Œç„¶ååœ¨å¾ªç¯ä¸­å¤šæ¬¡é‡å¤ã€‚é€šè¿‡å¼•å…¥çµæ´»çš„é€’å½’Transformerï¼Œæˆ‘ä»¬åœ¨å±‚ç»‘å®šçº¦æŸä¸­æ·»åŠ çµæ´»æ€§ï¼Œé€šè¿‡æ·±åº¦ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰æ¨¡å—ä»ç„¶ä¿æŒæ•´ä½“æ¨¡å‹çš„ç´§å‡‘æ€§ï¼Œè¿›ä¸€æ­¥æé«˜äº†æ€§èƒ½ã€‚æˆ‘ä»¬å±•ç¤ºäº†æˆ‘ä»¬çš„é€’å½’æ¨¡å‹ï¼ˆä¾‹å¦‚ï¼Œé€’å½’Gemma 1Bï¼‰ä¼˜äºç±»ä¼¼å¤§å°çš„æ™®é€šé¢„è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚TinyLlama 1.1Bå’ŒPythia 1Bï¼‰å’ŒçŸ¥è¯†è’¸é¦åŸºçº¿â€”â€”ç”šè‡³å¯ä»¥æ¢å¤åŸå§‹â€œå…¨å°ºå¯¸â€æ¨¡å‹çš„å¤§éƒ¨åˆ†æ€§èƒ½ï¼ˆä¾‹å¦‚ï¼Œæ²¡æœ‰å…±äº«å‚æ•°çš„Gemma 2Bï¼‰ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†è¿ç»­æ·±åº¦åˆ†æ‰¹å¤„ç†ï¼Œè¿™æ˜¯ç”±é€’å½’Transformerä¸æ—©æœŸé€€å‡ºç›¸ç»“åˆè€Œå®ç°çš„å…·æœ‰å‰æ™¯çš„æ–°æ¨ç†èŒƒå¼ã€‚åœ¨ç†è®ºåˆ†æä¸­ï¼Œæˆ‘ä»¬è¯æ˜äº†è¿™æœ‰å¯èƒ½å¯¼è‡´æ¨ç†ååé‡äº§ç”Ÿæ˜¾è‘—ï¼ˆ2-3å€ï¼‰çš„æå‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.20672v3">PDF</a> ICLR 2025; 49 pages, 17 figures, 19 tables</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰éƒ¨ç½²æˆæœ¬é«˜æ˜‚ã€‚æœ¬ç ”ç©¶é‡æ–°å®¡è§†äº†Transformerä¸­çš„å‚æ•°å…±äº«æ–¹å¼â€œå±‚ç»‘å®šâ€ï¼Œå¹¶å¼•å…¥äº†å°†ç°æœ‰LLMè½¬æ¢ä¸ºè¾ƒå°çš„â€œé€’å½’Transformerâ€çš„æ–°æ–¹æ³•ï¼Œè¿™äº›é€’å½’Transformeråœ¨å±‚ä¹‹é—´å…±äº«å‚æ•°ï¼ŒåŒæ—¶æ€§èƒ½æŸå¤±æœ€å°åŒ–ã€‚é€šè¿‡æ ‡å‡†é¢„è®­ç»ƒTransformeræœ‰æ•ˆåœ°åˆå§‹åŒ–é€’å½’Transformerï¼Œå¹¶ä»…ä½¿ç”¨ä¸€ä¸ªç‹¬ç‰¹çš„å›¾å±‚å—ï¼Œç„¶ååœ¨å¾ªç¯ä¸­å¤šæ¬¡é‡å¤ã€‚é€šè¿‡å¼•å…¥çµæ´»çš„é€’å½’Transformerï¼Œå³é€šè¿‡æ·±åº¦ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰æ¨¡å—å¢åŠ å±‚ç»‘å®šçº¦æŸçš„çµæ´»æ€§ï¼Œä»ä¿ç•™äº†æ¨¡å‹çš„ç´§å‡‘æ€§ã€‚é€’å½’æ¨¡å‹ï¼ˆä¾‹å¦‚é€’å½’Gemma 1Bï¼‰è¡¨ç°å‡ºä¼˜äºç±»ä¼¼å¤§å°çš„æ™®é€šé¢„è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚TinyLlama 1.1Bå’ŒPythia 1Bï¼‰å’ŒçŸ¥è¯†è’¸é¦åŸºçº¿çš„èƒ½åŠ›ï¼Œç”šè‡³å¯ä»¥æ¢å¤å¤§éƒ¨åˆ†åŸå§‹â€œå…¨å°ºå¯¸â€æ¨¡å‹ï¼ˆå¦‚æ²¡æœ‰å…±äº«å‚æ•°çš„Gemma 2Bï¼‰çš„æ€§èƒ½ã€‚æœ€åï¼Œæå‡ºäº†ç”±é€’å½’Transformerä¸æ—©æœŸé€€å‡ºç›¸ç»“åˆå®ç°çš„å…¨æ–°æ¨ç†æ¨¡å¼â€”â€”è¿ç»­æ·±åº¦åˆ†æ‰¹ï¼Œç†è®ºä¸Šæœ‰æ½œåŠ›å®ç°æ¨ç†ååé‡çš„æ˜¾è‘—ï¼ˆ2-3å€ï¼‰æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶æ¢è®¨äº†å¦‚ä½•ä½¿ç”¨å‚æ•°å…±äº«æ¥å‡å°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å°ºå¯¸å’Œé™ä½æˆæœ¬ã€‚</li>
<li>æå‡ºäº†â€œé€’å½’Transformerâ€æ¨¡å‹ï¼Œé€šè¿‡å±‚ç»‘å®šå®ç°å‚æ•°å…±äº«ï¼Œå¹¶æœ€å°åŒ–æ€§èƒ½æŸå¤±ã€‚</li>
<li>é€’å½’Transformeré€šè¿‡æ ‡å‡†é¢„è®­ç»ƒTransformerè¿›è¡Œé«˜æ•ˆåˆå§‹åŒ–ï¼Œä¸”ä»…ä½¿ç”¨ä¸€ä¸ªç‹¬ç‰¹å›¾å±‚å—è¿›è¡Œå¤šæ¬¡é‡å¤ã€‚</li>
<li>é€šè¿‡å¼•å…¥çµæ´»çš„é€’å½’Transformerå’Œæ·±åº¦ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰æ¨¡å—ï¼Œæé«˜äº†æ¨¡å‹æ€§èƒ½å¹¶ä¿æŒäº†å…¶ç´§å‡‘æ€§ã€‚</li>
<li>é€’å½’æ¨¡å‹åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†ç±»ä¼¼çš„é¢„è®­ç»ƒæ¨¡å‹å’ŒçŸ¥è¯†è’¸é¦åŸºçº¿ï¼Œå¹¶æ¥è¿‘å…¨å°ºå¯¸æ¨¡å‹çš„è¡¨ç°ã€‚</li>
<li>ç»“åˆæ—©æœŸé€€å‡ºæœºåˆ¶ï¼Œæå‡ºäº†æ–°çš„æ¨ç†æ¨¡å¼â€”â€”è¿ç»­æ·±åº¦åˆ†æ‰¹ï¼Œæœ‰æœ›æ˜¾è‘—æé«˜æ¨ç†æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.20672">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-b6865a9661b7dbb40456a857c99769e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-03d1482a558dfbf22aaab98782f6fc91.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fed3902a230970a56b83b9b877209a71.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9ddb3b1f04877fb02ad8be7e50715109.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eaf858d3bc72062db466888df68fa2e7.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Can-Knowledge-Editing-Really-Correct-Hallucinations"><a href="#Can-Knowledge-Editing-Really-Correct-Hallucinations" class="headerlink" title="Can Knowledge Editing Really Correct Hallucinations?"></a>Can Knowledge Editing Really Correct Hallucinations?</h2><p><strong>Authors:Baixiang Huang, Canyu Chen, Xiongxiao Xu, Ali Payani, Kai Shu</strong></p>
<p>Large Language Models (LLMs) suffer from hallucinations, referring to the non-factual information in generated content, despite their superior capacities across tasks. Meanwhile, knowledge editing has been developed as a new popular paradigm to correct erroneous factual knowledge encoded in LLMs with the advantage of avoiding retraining from scratch. However, a common issue of existing evaluation datasets for knowledge editing is that they do not ensure that LLMs actually generate hallucinated answers to the evaluation questions before editing. When LLMs are evaluated on such datasets after being edited by different techniques, it is hard to directly adopt the performance to assess the effectiveness of different knowledge editing methods in correcting hallucinations. Thus, the fundamental question remains insufficiently validated: Can knowledge editing really correct hallucinations in LLMs? We proposed HalluEditBench to holistically benchmark knowledge editing methods in correcting real-world hallucinations. First, we rigorously construct a massive hallucination dataset with 9 domains, 26 topics and more than 6,000 hallucinations. Then, we assess the performance of knowledge editing methods in a holistic way on five dimensions including Efficacy, Generalization, Portability, Locality, and Robustness. Through HalluEditBench, we have provided new insights into the potentials and limitations of different knowledge editing methods in correcting hallucinations, which could inspire future improvements and facilitate progress in the field of knowledge editing. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å­˜åœ¨è™šæ„é—®é¢˜ï¼Œå³ç”Ÿæˆå†…å®¹ä¸­çš„éäº‹å®ä¿¡æ¯ï¼Œå°½ç®¡å…¶åœ¨å„é¡¹ä»»åŠ¡ä¸­çš„èƒ½åŠ›è¶…ç¾¤ã€‚åŒæ—¶ï¼ŒçŸ¥è¯†ç¼–è¾‘ä½œä¸ºä¸€ç§æ–°çš„æµè¡ŒèŒƒå¼ï¼Œä»¥å…¶é¿å…ä»å¤´å¼€å§‹é‡æ–°è®­ç»ƒçš„ä¼˜åŠ¿ï¼Œè¢«å¼€å‘å‡ºæ¥ä»¥çº æ­£ç¼–ç åœ¨LLMä¸­çš„é”™è¯¯äº‹å®çŸ¥è¯†ã€‚ç„¶è€Œï¼Œç°æœ‰çŸ¥è¯†ç¼–è¾‘è¯„ä¼°æ•°æ®é›†çš„ä¸€ä¸ªæ™®éé—®é¢˜æ˜¯ï¼Œå®ƒä»¬å¹¶ä¸èƒ½ç¡®ä¿LLMåœ¨ç¼–è¾‘å‰å®é™…ä¸Šç”Ÿæˆäº†è™šæ„çš„ç­”æ¡ˆã€‚å½“åœ¨å„ç§æŠ€æœ¯ç¼–è¾‘åçš„LLMä¸Šä½¿ç”¨æ­¤ç±»æ•°æ®é›†è¿›è¡Œè¯„ä¼°æ—¶ï¼Œå¾ˆéš¾ç›´æ¥é‡‡ç”¨å…¶æ€§èƒ½æ¥è¯„ä¼°ä¸åŒçŸ¥è¯†ç¼–è¾‘æ–¹æ³•åœ¨çº æ­£è™šæ„å†…å®¹æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚å› æ­¤ï¼Œä¸€ä¸ªåŸºæœ¬çš„é—®é¢˜ä»æœªå¾—åˆ°å……åˆ†éªŒè¯ï¼šçŸ¥è¯†ç¼–è¾‘çœŸçš„èƒ½å¤Ÿçº æ­£LLMä¸­çš„è™šæ„å†…å®¹å—ï¼Ÿæˆ‘ä»¬æå‡ºäº†HalluEditBenchï¼Œä»¥å…¨é¢è¯„ä¼°çŸ¥è¯†ç¼–è¾‘æ–¹æ³•åœ¨çº æ­£ç°å®ä¸–ç•Œä¸­çš„è™šæ„å†…å®¹æ–¹é¢çš„è¡¨ç°ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä¸¥æ ¼æ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡è™šæ„æ•°æ®é›†ï¼ŒåŒ…å«9ä¸ªé¢†åŸŸã€26ä¸ªä¸»é¢˜å’Œè¶…è¿‡6000ä¸ªè™šæ„æ¡ˆä¾‹ã€‚ç„¶åï¼Œæˆ‘ä»¬ä»äº”ä¸ªç»´åº¦å…¨é¢è¯„ä¼°äº†çŸ¥è¯†ç¼–è¾‘æ–¹æ³•çš„è¡¨ç°ï¼ŒåŒ…æ‹¬æ•ˆç‡ã€æ³›åŒ–èƒ½åŠ›ã€å¯ç§»æ¤æ€§ã€å±€éƒ¨æ€§å’Œç¨³å¥æ€§ã€‚é€šè¿‡HalluEditBenchï¼Œæˆ‘ä»¬å¯¹ä¸åŒçŸ¥è¯†ç¼–è¾‘æ–¹æ³•åœ¨çº æ­£è™šæ„å†…å®¹æ–¹é¢çš„æ½œåŠ›ä¸å±€é™æ€§æœ‰äº†å…¨æ–°çš„è®¤è¯†ï¼Œè¿™å¯ä»¥æ¿€å‘æœªæ¥çš„æ”¹è¿›ï¼Œå¹¶æ¨åŠ¨çŸ¥è¯†ç¼–è¾‘é¢†åŸŸçš„å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.16251v3">PDF</a> ICLR 2025. Main paper: 10 pages; total: 34 pages (including   appendix). The first two authors contributed equally to this work. Code,   data, results, and additional resources are available on the project website:   <a target="_blank" rel="noopener" href="https://llm-editing.github.io/">https://llm-editing.github.io</a></p>
<p><strong>Summary</strong><br>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å­˜åœ¨ç”Ÿæˆå†…å®¹ä¸­çš„éäº‹å®ä¿¡æ¯é—®é¢˜ï¼Œå³æ‰€è°“çš„â€œå¹»è§‰â€ã€‚ä¸ºçº æ­£ç¼–ç åœ¨LLMä¸­çš„é”™è¯¯äº‹å®çŸ¥è¯†ï¼ŒçŸ¥è¯†ç¼–è¾‘ä½œä¸ºä¸€ç§æ–°å‹èŒƒå¼å¾—åˆ°å‘å±•ï¼Œå…¶ä¼˜åŠ¿åœ¨äºé¿å…ä»å¤´å¼€å§‹é‡æ–°è®­ç»ƒã€‚ç„¶è€Œï¼Œç°æœ‰è¯„ä¼°æ•°æ®é›†æ™®éå­˜åœ¨ä¸€ä¸ªé—®é¢˜ï¼Œå³å®ƒä»¬æ— æ³•ä¿è¯LLMåœ¨ç¼–è¾‘å‰å®é™…ä¸Šç”Ÿæˆäº†å¹»è§‰ç­”æ¡ˆã€‚å½“åœ¨å„ç§çŸ¥è¯†ç¼–è¾‘æŠ€æœ¯è¯„ä¼°æ•°æ®é›†ä¸Šè¯„ä¼°LLMæ—¶ï¼Œå¾ˆéš¾ç›´æ¥é‡‡ç”¨æ€§èƒ½è¯„ä¼°æ¥æ£€éªŒä¸åŒçŸ¥è¯†ç¼–è¾‘æ–¹æ³•åœ¨çº æ­£å¹»è§‰æ–¹é¢çš„æ•ˆæœã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºHalluEditBenchï¼Œå…¨é¢è¯„ä¼°çŸ¥è¯†ç¼–è¾‘æ–¹æ³•åœ¨çº æ­£ç°å®å¹»è§‰æ–¹é¢çš„èƒ½åŠ›ã€‚æˆ‘ä»¬ä¸¥æ ¼æ„å»ºäº†åŒ…å«9ä¸ªé¢†åŸŸã€26ä¸ªä¸»é¢˜å’Œ6000å¤šä¸ªå¹»è§‰çš„å¤§è§„æ¨¡å¹»è§‰æ•°æ®é›†ï¼Œä»æ•ˆç‡ã€é€šç”¨æ€§ã€å¯ç§»æ¤æ€§ã€å±€éƒ¨æ€§å’Œç¨³å¥æ€§äº”ä¸ªç»´åº¦å…¨é¢è¯„ä¼°çŸ¥è¯†ç¼–è¾‘æ–¹æ³•çš„æ€§èƒ½ã€‚HalluEditBenchä¸ºä¸åŒçŸ¥è¯†ç¼–è¾‘æ–¹æ³•åœ¨çº æ­£å¹»è§‰æ–¹é¢çš„æ½œåŠ›å’Œå±€é™æ€§æä¾›äº†æ–°çš„è§è§£ï¼Œæœ‰æœ›æ¿€å‘æœªæ¥æ”¹è¿›å¹¶æ¨åŠ¨çŸ¥è¯†ç¼–è¾‘é¢†åŸŸçš„è¿›æ­¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså­˜åœ¨ç”Ÿæˆéäº‹å®ä¿¡æ¯çš„å¹»è§‰é—®é¢˜ã€‚</li>
<li>çŸ¥è¯†ç¼–è¾‘ä½œä¸ºä¸€ç§æ–°å…´èŒƒå¼ï¼Œæ—¨åœ¨çº æ­£LLMsä¸­çš„é”™è¯¯äº‹å®çŸ¥è¯†ï¼Œä¸”é¿å…ä»å¤´å¼€å§‹é‡æ–°è®­ç»ƒã€‚</li>
<li>ç°æœ‰è¯„ä¼°æ•°æ®é›†åœ¨è¯„ä¼°çŸ¥è¯†ç¼–è¾‘æ–¹æ³•æ—¶å­˜åœ¨ç¼ºé™·ï¼Œæ— æ³•ä¿è¯LLMsåœ¨ç¼–è¾‘å‰ç”Ÿæˆå¹»è§‰ç­”æ¡ˆã€‚</li>
<li>HalluEditBenchæ—¨åœ¨å…¨é¢è¯„ä¼°çŸ¥è¯†ç¼–è¾‘æ–¹æ³•åœ¨çº æ­£ç°å®å¹»è§‰æ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>HalluEditBenchåŒ…æ‹¬ä¸€ä¸ªå¤§è§„æ¨¡å¹»è§‰æ•°æ®é›†ï¼Œæ¶µç›–å¤šä¸ªé¢†åŸŸå’Œä¸»é¢˜ã€‚</li>
<li>çŸ¥è¯†ç¼–è¾‘æ–¹æ³•çš„è¯„ä¼°æ¶‰åŠäº”ä¸ªç»´åº¦ï¼šæ•ˆç‡ã€é€šç”¨æ€§ã€å¯ç§»æ¤æ€§ã€å±€éƒ¨æ€§å’Œç¨³å¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.16251">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-699b6998702dcc595fbfcac354656974.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70b5448e9d41b95fcce320893d2f5825.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e2f924af73399a4fea41a87c8b776d93.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b9c3fefa191f265eb644a65d4fe54ef8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a2a381376286dc84302b374ddcd619fc.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="LLMOPT-Learning-to-Define-and-Solve-General-Optimization-Problems-from-Scratch"><a href="#LLMOPT-Learning-to-Define-and-Solve-General-Optimization-Problems-from-Scratch" class="headerlink" title="LLMOPT: Learning to Define and Solve General Optimization Problems from   Scratch"></a>LLMOPT: Learning to Define and Solve General Optimization Problems from   Scratch</h2><p><strong>Authors:Caigao Jiang, Xiang Shu, Hong Qian, Xingyu Lu, Jun Zhou, Aimin Zhou, Yang Yu</strong></p>
<p>Optimization problems are prevalent across various scenarios. Formulating and then solving optimization problems described by natural language often requires highly specialized human expertise, which could block the widespread application of optimization-based decision making. To automate problem formulation and solving, leveraging large language models (LLMs) has emerged as a potential way. However, this kind of approach suffers from the issue of optimization generalization. Namely, the accuracy of most current LLM-based methods and the generality of optimization problem types that they can model are still limited. In this paper, we propose a unified learning-based framework called LLMOPT to boost optimization generalization. Starting from the natural language descriptions of optimization problems and a pre-trained LLM, LLMOPT constructs the introduced five-element formulation as a universal model for learning to define diverse optimization problem types. Then, LLMOPT employs the multi-instruction tuning to enhance both problem formalization and solver code generation accuracy and generality. After that, to prevent hallucinations in LLMs, such as sacrificing solving accuracy to avoid execution errors, the model alignment and self-correction mechanism are adopted in LLMOPT. We evaluate the optimization generalization ability of LLMOPT and compared methods across six real-world datasets covering roughly 20 fields such as health, environment, energy and manufacturing, etc. Extensive experiment results show that LLMOPT is able to model various optimization problem types such as linear&#x2F;nonlinear programming, mixed integer programming, and combinatorial optimization, and achieves a notable 11.08% average solving accuracy improvement compared with the state-of-the-art methods. The code is available at <a target="_blank" rel="noopener" href="https://github.com/caigaojiang/LLMOPT">https://github.com/caigaojiang/LLMOPT</a>. </p>
<blockquote>
<p>ä¼˜åŒ–é—®é¢˜åœ¨å„ç§åœºæ™¯ä¸­æ™®éå­˜åœ¨ã€‚é€šè¿‡è‡ªç„¶è¯­è¨€æè¿°å¹¶ç„¶åè§£å†³ä¼˜åŒ–é—®é¢˜é€šå¸¸éœ€è¦é«˜åº¦ä¸“ä¸šåŒ–çš„äººç±»ä¸“å®¶çŸ¥è¯†ï¼Œè¿™å¯èƒ½ä¼šé˜»ç¢åŸºäºä¼˜åŒ–çš„å†³ç­–åˆ¶å®šçš„å¹¿æ³›åº”ç”¨ã€‚ä¸ºäº†è‡ªåŠ¨è¿›è¡Œé—®é¢˜åˆ¶å®šå’Œè§£å†³ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»æˆä¸ºä¸€ç§å¯èƒ½çš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œè¿™ç§ç±»å‹çš„æ–¹æ³•å­˜åœ¨ä¼˜åŒ–æ³›åŒ–çš„é—®é¢˜ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå½“å‰å¤§å¤šæ•°åŸºäºLLMçš„æ–¹æ³•çš„å‡†ç¡®åº¦ä»¥åŠå®ƒä»¬èƒ½å¤Ÿå»ºæ¨¡çš„ä¼˜åŒ–é—®é¢˜ç±»å‹çš„æ™®éæ€§ä»ç„¶æœ‰é™ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„åŸºäºå­¦ä¹ çš„æ¡†æ¶ï¼Œç§°ä¸ºLLMOPTï¼Œä»¥æå‡ä¼˜åŒ–æ³›åŒ–èƒ½åŠ›ã€‚LLMOPTä»ä¼˜åŒ–é—®é¢˜çš„è‡ªç„¶è¯­è¨€æè¿°å’Œé¢„è®­ç»ƒLLMå¼€å§‹ï¼Œæ„å»ºäº†ä¸€ä¸ªäº”å…ƒç´ å…¬å¼ä½œä¸ºå­¦ä¹ å®šä¹‰å¤šç§ä¼˜åŒ–é—®é¢˜ç±»å‹çš„é€šç”¨æ¨¡å‹ã€‚ç„¶åï¼ŒLLMOPTé‡‡ç”¨å¤šæŒ‡ä»¤è°ƒæ•´ï¼Œä»¥æé«˜é—®é¢˜å½¢å¼åŒ–å’Œæ±‚è§£å™¨ä»£ç ç”Ÿæˆçš„å‡†ç¡®æ€§å’Œæ™®éæ€§ã€‚ä¹‹åï¼Œä¸ºäº†é˜²æ­¢LLMä¸­çš„å¹»è§‰ï¼Œä¾‹å¦‚åœ¨é¿å…æ‰§è¡Œé”™è¯¯çš„æƒ…å†µä¸‹ç‰ºç‰²è§£å†³ç²¾åº¦ï¼ŒLLMOPTä¸­é‡‡ç”¨äº†æ¨¡å‹å¯¹é½å’Œè‡ªæ ¡æ­£æœºåˆ¶ã€‚æˆ‘ä»¬åœ¨æ¶µç›–å¤§çº¦20ä¸ªé¢†åŸŸï¼ˆå¦‚å¥åº·ã€ç¯å¢ƒã€èƒ½æºå’Œåˆ¶é€ ç­‰ï¼‰çš„å…­ä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šè¯„ä¼°äº†LLMOPTå’Œå¯¹æ¯”æ–¹æ³•çš„ä¼˜åŒ–æ³›åŒ–èƒ½åŠ›ã€‚å¤§é‡çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒLLMOPTèƒ½å¤Ÿå»ºæ¨¡å¤šç§ä¼˜åŒ–é—®é¢˜ç±»å‹ï¼Œå¦‚çº¿æ€§&#x2F;éçº¿æ€§è§„åˆ’ã€æ··åˆæ•´æ•°è§„åˆ’å’Œç»„åˆä¼˜åŒ–ç­‰ï¼Œä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œå®ç°äº†11.08%çš„å¹³å‡è§£å†³ç²¾åº¦æ”¹è¿›ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/caigaojiang/LLMOPT%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/caigaojiang/LLMOPTä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.13213v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>ä¼˜åŒ–é—®é¢˜å¹¿æ³›å­˜åœ¨äºå„ç§åœºæ™¯ä¸­ã€‚è‡ªç„¶è¯­è¨€æè¿°ä¼˜åŒ–é—®é¢˜çš„å½¢æˆå’Œè§£å†³éœ€è¦é«˜åº¦ä¸“ä¸šåŒ–çš„äººç±»ä¸“ä¸šçŸ¥è¯†ï¼Œè¿™é˜»ç¢äº†åŸºäºä¼˜åŒ–çš„å†³ç­–åˆ¶å®šçš„å¹¿æ³›åº”ç”¨ã€‚ä¸ºäº†è‡ªåŠ¨åŒ–é—®é¢˜å½¢æˆå’Œè§£å†³ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»æˆä¸ºä¸€ç§å¯èƒ½çš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•é¢ä¸´ç€ä¼˜åŒ–æ³›åŒ–çš„é—®é¢˜ã€‚å³å½“å‰å¤§å¤šæ•°åŸºäºLLMçš„æ–¹æ³•çš„å‡†ç¡®æ€§å’Œå®ƒä»¬èƒ½å¤Ÿå»ºæ¨¡çš„ä¼˜åŒ–é—®é¢˜ç±»å‹çš„æ™®éæ€§ä»ç„¶å—åˆ°é™åˆ¶ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåä¸ºLLMOPTçš„ç»Ÿä¸€å­¦ä¹ æ¡†æ¶ï¼Œä»¥æé«˜ä¼˜åŒ–æ³›åŒ–èƒ½åŠ›ã€‚LLMOPTä»ä¼˜åŒ–é—®é¢˜çš„è‡ªç„¶è¯­è¨€æè¿°å’Œä¸€ä¸ªé¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹å¼€å§‹ï¼Œæ„å»ºäº†äº”å…ƒç´ å…¬å¼ä½œä¸ºå­¦ä¹ å®šä¹‰å¤šç§ä¼˜åŒ–é—®é¢˜ç±»å‹çš„é€šç”¨æ¨¡å‹ã€‚ç„¶åï¼ŒLLMOPTé‡‡ç”¨å¤šæŒ‡ä»¤è°ƒæ•´å¢å¼ºé—®é¢˜å½¢å¼åŒ–å’Œæ±‚è§£å™¨ä»£ç ç”Ÿæˆå‡†ç¡®æ€§å’Œæ™®éæ€§ã€‚ä¹‹åï¼Œä¸ºäº†é˜²æ­¢LLMä¸­çš„å¹»è§‰ï¼Œå¦‚ç‰ºç‰²æ±‚è§£ç²¾åº¦ä»¥é¿å…æ‰§è¡Œé”™è¯¯ï¼Œåœ¨LLMOPTä¸­é‡‡ç”¨äº†æ¨¡å‹å¯¹é½å’Œè‡ªæˆ‘æ ¡æ­£æœºåˆ¶ã€‚æˆ‘ä»¬åœ¨æ¶µç›–å¤§çº¦20ä¸ªé¢†åŸŸçš„å…­ä¸ªçœŸå®æ•°æ®é›†ä¸Šè¯„ä¼°äº†LLMOPTå’Œå¯¹æ¯”æ–¹æ³•çš„ä¼˜åŒ–æ³›åŒ–èƒ½åŠ›ï¼Œå¦‚å¥åº·ã€ç¯å¢ƒã€èƒ½æºå’Œåˆ¶é€ ç­‰ã€‚å¤§é‡å®éªŒç»“æœè¡¨æ˜ï¼ŒLLMOPTèƒ½å¤Ÿæ¨¡æ‹Ÿå¤šç§ä¼˜åŒ–é—®é¢˜ç±»å‹ï¼Œå¦‚çº¿æ€§&#x2F;éçº¿æ€§è§„åˆ’ã€æ··åˆæ•´æ•°è§„åˆ’å’Œç»„åˆä¼˜åŒ–ï¼Œä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œå¹³å‡æ±‚è§£ç²¾åº¦æé«˜äº†11.08%ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/caigaojiang/LLMOPT%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/caigaojiang/LLMOPTä¸Šæ‰¾åˆ°ã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯ç”¨äºè‡ªåŠ¨åŒ–ä¼˜åŒ–é—®é¢˜çš„å½¢æˆå’Œè§£å†³ï¼Œä½†å­˜åœ¨ä¼˜åŒ–æ³›åŒ–çš„é—®é¢˜ã€‚</li>
<li>LLMOPTæ¡†æ¶é€šè¿‡æ„å»ºäº”å…ƒç´ å…¬å¼æ¥æé«˜ä¼˜åŒ–æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶å¤„ç†å¤šç§ä¼˜åŒ–é—®é¢˜ç±»å‹ã€‚</li>
<li>LLMOPTé‡‡ç”¨å¤šæŒ‡ä»¤è°ƒæ•´å¢å¼ºé—®é¢˜å’Œæ±‚è§£å™¨ä»£ç ç”Ÿæˆçš„å‡†ç¡®æ€§å’Œæ™®éæ€§ã€‚</li>
<li>LLMOPTé‡‡ç”¨æ¨¡å‹å¯¹é½å’Œè‡ªæˆ‘æ ¡æ­£æœºåˆ¶ä»¥é˜²æ­¢LLMä¸­çš„å¹»è§‰ã€‚</li>
<li>åœ¨å…­ä¸ªçœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒLLMOPTåœ¨å¤šç§ä¼˜åŒ–é—®é¢˜ç±»å‹ä¸Šå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œå¹³å‡æ±‚è§£ç²¾åº¦è¾ƒç°æœ‰æ–¹æ³•æé«˜äº†11.08%ã€‚</li>
<li>LLMOPTä»£ç åº“å…¬å¼€å¯ç”¨ï¼Œä¾¿äºè¿›ä¸€æ­¥ç ”ç©¶å’Œåº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.13213">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a58f725a776d87fa7e0dcd5e747552c8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-186e99627097a75f5d449f686d2d64f2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2f424ae9446a6605b7962b66e9af18f3.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Facilitating-Multi-turn-Function-Calling-for-LLMs-via-Compositional-Instruction-Tuning"><a href="#Facilitating-Multi-turn-Function-Calling-for-LLMs-via-Compositional-Instruction-Tuning" class="headerlink" title="Facilitating Multi-turn Function Calling for LLMs via Compositional   Instruction Tuning"></a>Facilitating Multi-turn Function Calling for LLMs via Compositional   Instruction Tuning</h2><p><strong>Authors:Mingyang Chen, Haoze Sun, Tianpeng Li, Fan Yang, Hao Liang, Keer Lu, Bin Cui, Wentao Zhang, Zenan Zhou, Weipeng Chen</strong></p>
<p>Large Language Models (LLMs) have exhibited significant potential in performing diverse tasks, including the ability to call functions or use external tools to enhance their performance. While current research on function calling by LLMs primarily focuses on single-turn interactions, this paper addresses the overlooked necessity for LLMs to engage in multi-turn function callingâ€“critical for handling compositional, real-world queries that require planning with functions but not only use functions. To facilitate this, we introduce an approach, BUTTON, which generates synthetic compositional instruction tuning data via bottom-up instruction construction and top-down trajectory generation. In the bottom-up phase, we generate simple atomic tasks based on real-world scenarios and build compositional tasks using heuristic strategies based on atomic tasks. Corresponding function definitions are then synthesized for these compositional tasks. The top-down phase features a multi-agent environment where interactions among simulated humans, assistants, and tools are utilized to gather multi-turn function calling trajectories. This approach ensures task compositionality and allows for effective function and trajectory generation by examining atomic tasks within compositional tasks. We produce a dataset BUTTONInstruct comprising 8k data points and demonstrate its effectiveness through extensive experiments across various LLMs. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ‰§è¡Œå¤šæ ·åŒ–ä»»åŠ¡æ—¶è¡¨ç°å‡ºäº†å·¨å¤§çš„æ½œåŠ›ï¼ŒåŒ…æ‹¬è°ƒç”¨å‡½æ•°æˆ–ä½¿ç”¨å¤–éƒ¨å·¥å…·æ¥æå‡æ€§èƒ½çš„èƒ½åŠ›ã€‚è™½ç„¶ç›®å‰å…³äºLLMçš„å‡½æ•°è°ƒç”¨ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å•è½®äº¤äº’ä¸Šï¼Œä½†æœ¬æ–‡å¼ºè°ƒäº†LLMè¿›è¡Œå¤šè½®å‡½æ•°è°ƒç”¨çš„å¿…è¦æ€§â€”â€”è¿™å¯¹äºå¤„ç†éœ€è¦è§„åˆ’å‡½æ•°è€Œéä»…ä½¿ç”¨å‡½æ•°çš„ç»„åˆã€ç°å®ä¸–ç•ŒæŸ¥è¯¢è‡³å…³é‡è¦ã€‚ä¸ºäº†ä¿ƒè¿›è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–¹æ³•â€”â€”BUTTONï¼Œå®ƒé€šè¿‡è‡ªä¸‹è€Œä¸Šçš„æŒ‡ä»¤æ„å»ºå’Œè‡ªä¸Šè€Œä¸‹çš„è½¨è¿¹ç”Ÿæˆï¼Œç”Ÿæˆåˆæˆç»„åˆæŒ‡ä»¤å¾®è°ƒæ•°æ®ã€‚åœ¨è‡ªä¸‹è€Œä¸Šçš„é˜¶æ®µï¼Œæˆ‘ä»¬åŸºäºç°å®åœºæ™¯ç”Ÿæˆç®€å•çš„åŸå­ä»»åŠ¡ï¼Œå¹¶ä½¿ç”¨åŸºäºåŸå­ä»»åŠ¡çš„å¯å‘å¼ç­–ç•¥æ„å»ºç»„åˆä»»åŠ¡ã€‚ç„¶åä¸ºè¿™äº›ç»„åˆä»»åŠ¡åˆæˆç›¸åº”çš„å‡½æ•°å®šä¹‰ã€‚è‡ªä¸Šè€Œä¸‹é˜¶æ®µçš„ç‰¹ç‚¹æ˜¯å¤šæ™ºèƒ½ä½“ç¯å¢ƒï¼Œå…¶ä¸­æ¨¡æ‹Ÿäººç±»ã€åŠ©ç†å’Œå·¥å…·ä¹‹é—´çš„äº¤äº’ç”¨äºæ”¶é›†å¤šè½®å‡½æ•°è°ƒç”¨è½¨è¿¹ã€‚è¿™ç§æ–¹æ³•ç¡®ä¿äº†ä»»åŠ¡çš„ç»„åˆæ€§ï¼Œå¹¶å…è®¸é€šè¿‡æ£€æŸ¥ç»„åˆä»»åŠ¡ä¸­çš„åŸå­ä»»åŠ¡æ¥è¿›è¡Œæœ‰æ•ˆçš„å‡½æ•°å’Œè½¨è¿¹ç”Ÿæˆã€‚æˆ‘ä»¬åˆ¶ä½œäº†ä¸€ä¸ªåŒ…å«8000ä¸ªæ•°æ®ç‚¹çš„BUTTONInstructæ•°æ®é›†ï¼Œå¹¶é€šè¿‡åœ¨å„ç§LLMä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.12952v2">PDF</a> Accepted to ICLR 2025</p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·æœ‰è°ƒç”¨å‡½æ•°æˆ–ä½¿ç”¨å¤–éƒ¨å·¥å…·æå‡æ€§èƒ½çš„èƒ½åŠ›ï¼Œç›®å‰ç ”ç©¶ä¸»è¦å…³æ³¨å•è½®äº¤äº’ä¸­çš„å‡½æ•°è°ƒç”¨ã€‚æœ¬æ–‡ä¸ºè§£å†³å¤„ç†éœ€è¦è§„åˆ’å‡½æ•°è€Œéä»…ä½¿ç”¨å‡½æ•°çš„ç»„åˆå¼ã€ç°å®ä¸–ç•ŒæŸ¥è¯¢çš„éœ€æ±‚ï¼Œæå‡ºäº†ä¸€ç§åä¸ºBUTTONçš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡è‡ªä¸‹è€Œä¸Šçš„æŒ‡ä»¤æ„å»ºå’Œè‡ªä¸Šè€Œä¸‹çš„è½¨è¿¹ç”Ÿæˆï¼Œç”Ÿæˆåˆæˆç»„åˆæŒ‡ä»¤è°ƒæ•´æ•°æ®ã€‚é¦–å…ˆï¼ŒåŸºäºçœŸå®åœºæ™¯ç”Ÿæˆç®€å•åŸå­ä»»åŠ¡ï¼Œå¹¶åˆ©ç”¨å¯å‘å¼ç­–ç•¥æ„å»ºç»„åˆä»»åŠ¡åŠç›¸åº”å‡½æ•°å®šä¹‰ã€‚æ¥ç€ï¼Œåœ¨å¤šæ™ºèƒ½ä½“ç¯å¢ƒä¸­æ¨¡æ‹Ÿäººç±»ã€åŠ©ç†å’Œå·¥å…·ä¹‹é—´çš„äº¤äº’ï¼Œæ”¶é›†å¤šè½®å‡½æ•°è°ƒç”¨è½¨è¿¹ã€‚æ­¤æ–¹æ³•ç¡®ä¿äº†ä»»åŠ¡çš„å¯ç»„åˆæ€§ï¼Œå¹¶å…è®¸é€šè¿‡æ£€æŸ¥ç»„åˆä»»åŠ¡ä¸­çš„åŸå­ä»»åŠ¡æ¥æœ‰æ•ˆåœ°ç”ŸæˆåŠŸèƒ½å’Œè½¨è¿¹ã€‚æˆ‘ä»¬åˆ¶ä½œäº†åŒ…å«8kæ•°æ®ç‚¹çš„BUTTONInstructæ•°æ®é›†ï¼Œå¹¶é€šè¿‡è·¨å¤šç§LLMçš„å¹¿æ³›å®éªŒè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså…·å¤‡è°ƒç”¨å‡½æ•°å’Œå¤–éƒ¨å·¥å…·çš„èƒ½åŠ›ï¼Œç›®å‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å•è½®äº¤äº’ä¸­çš„å‡½æ•°è°ƒç”¨ã€‚</li>
<li>æœ¬æ–‡å¼ºè°ƒå¤„ç†ç»„åˆå¼ã€ç°å®ä¸–ç•ŒæŸ¥è¯¢çš„éœ€æ±‚ï¼Œè¿™äº›æŸ¥è¯¢éœ€è¦LLMsè¿›è¡Œå¤šè½®å‡½æ•°è°ƒç”¨ã€‚</li>
<li>BUTTONæ–¹æ³•ç”¨äºç”Ÿæˆåˆæˆç»„åˆæŒ‡ä»¤è°ƒæ•´æ•°æ®ï¼ŒåŒ…å«è‡ªä¸‹è€Œä¸Šçš„æŒ‡ä»¤æ„å»ºå’Œè‡ªä¸Šè€Œä¸‹çš„è½¨è¿¹ç”Ÿæˆä¸¤ä¸ªé˜¶æ®µã€‚</li>
<li>åœ¨BOTTOMé˜¶æ®µï¼ŒåŸºäºçœŸå®åœºæ™¯ç”ŸæˆåŸå­ä»»åŠ¡ï¼Œå¹¶ä½¿ç”¨å¯å‘å¼ç­–ç•¥æ„å»ºç»„åˆä»»åŠ¡å’Œç›¸åº”å‡½æ•°å®šä¹‰ã€‚</li>
<li>åœ¨TOPé˜¶æ®µï¼Œåˆ©ç”¨å¤šæ™ºèƒ½ä½“ç¯å¢ƒæ¨¡æ‹Ÿäººç±»ã€åŠ©ç†å’Œå·¥å…·ä¹‹é—´çš„äº¤äº’ï¼Œç”Ÿæˆå¤šè½®å‡½æ•°è°ƒç”¨è½¨è¿¹ã€‚</li>
<li>BUTTONæ–¹æ³•ç¡®ä¿äº†ä»»åŠ¡çš„å¯ç»„åˆæ€§ï¼Œå¹¶èƒ½æœ‰æ•ˆç”ŸæˆåŠŸèƒ½å’Œè½¨è¿¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.12952">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8aa6cad0b0ca2156f09d06b4e324b7bb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cde380922f9c2ded4ad4f035b35eb9dc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cf3aef7efa34d97c717e01ab44b6b114.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2a8a53b0e6cd0570dccc37a703815974.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Bypassing-the-Exponential-Dependency-Looped-Transformers-Efficiently-Learn-In-context-by-Multi-step-Gradient-Descent"><a href="#Bypassing-the-Exponential-Dependency-Looped-Transformers-Efficiently-Learn-In-context-by-Multi-step-Gradient-Descent" class="headerlink" title="Bypassing the Exponential Dependency: Looped Transformers Efficiently   Learn In-context by Multi-step Gradient Descent"></a>Bypassing the Exponential Dependency: Looped Transformers Efficiently   Learn In-context by Multi-step Gradient Descent</h2><p><strong>Authors:Bo Chen, Xiaoyu Li, Yingyu Liang, Zhenmei Shi, Zhao Song</strong></p>
<p>In-context learning has been recognized as a key factor in the success of Large Language Models (LLMs). It refers to the modelâ€™s ability to learn patterns on the fly from provided in-context examples in the prompt during inference. Previous studies have demonstrated that the Transformer architecture used in LLMs can implement a single-step gradient descent update by processing in-context examples in a single forward pass. Recent work has further shown that, during in-context learning, a looped Transformer can implement multi-step gradient descent updates in forward passes. However, their theoretical results require an exponential number of in-context examples, $n &#x3D; \exp(\Omega(T))$, where $T$ is the number of loops or passes, to achieve a reasonably low error. In this paper, we study linear looped Transformers in-context learning on linear vector generation tasks. We show that linear looped Transformers can implement multi-step gradient descent efficiently for in-context learning. Our results demonstrate that as long as the input data has a constant condition number, e.g., $n &#x3D; O(d)$, the linear looped Transformers can achieve a small error by multi-step gradient descent during in-context learning. Furthermore, our preliminary experiments validate our theoretical analysis. Our findings reveal that the Transformer architecture possesses a stronger in-context learning capability than previously understood, offering new insights into the mechanisms behind LLMs and potentially guiding the better design of efficient inference algorithms for LLMs. </p>
<blockquote>
<p>ä¸Šä¸‹æ–‡å­¦ä¹ å·²è¢«è®¤ä¸ºæ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æˆåŠŸçš„å…³é”®å› ç´ ã€‚å®ƒæŒ‡çš„æ˜¯æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­ä»æä¾›çš„ä¸Šä¸‹æ–‡ç¤ºä¾‹ä¸­å³æ—¶å­¦ä¹ æ¨¡å¼çš„èƒ½åŠ›ã€‚å…ˆå‰çš„ç ”ç©¶è¡¨æ˜ï¼ŒLLMä¸­ä½¿ç”¨çš„Transformeræ¶æ„å¯ä»¥é€šè¿‡ä¸€æ¬¡å‰å‘ä¼ é€’å¤„ç†ä¸Šä¸‹æ–‡ç¤ºä¾‹æ¥å®ç°å•æ­¥æ¢¯åº¦ä¸‹é™æ›´æ–°ã€‚æœ€è¿‘çš„å·¥ä½œè¿›ä¸€æ­¥è¡¨æ˜ï¼Œåœ¨ä¸Šä¸‹æ–‡å­¦ä¹ è¿‡ç¨‹ä¸­ï¼Œå¾ªç¯Transformerå¯ä»¥åœ¨å‰å‘ä¼ é€’ä¸­å®ç°å¤šæ­¥æ¢¯åº¦ä¸‹é™æ›´æ–°ã€‚ç„¶è€Œï¼Œä»–ä»¬çš„ç†è®ºç»“æœéœ€è¦å‘ˆæŒ‡æ•°å¢é•¿çš„ä¸Šä¸‹æ–‡ç¤ºä¾‹æ•°é‡ï¼Œå³$n &#x3D; \exp(\Omega(T))$ï¼Œå…¶ä¸­$T$æ˜¯å¾ªç¯æˆ–ä¼ é€’çš„æ¬¡æ•°ï¼Œæ‰èƒ½è¾¾åˆ°åˆç†çš„ä½è¯¯å·®ç‡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†çº¿æ€§å‘é‡ç”Ÿæˆä»»åŠ¡ä¸­çº¿æ€§å¾ªç¯Transformerçš„ä¸Šä¸‹æ–‡å­¦ä¹ ã€‚æˆ‘ä»¬å±•ç¤ºäº†çº¿æ€§å¾ªç¯Transformerå¯ä»¥æœ‰æ•ˆåœ°å®ç°å¤šæ­¥æ¢¯åº¦ä¸‹é™ï¼Œç”¨äºä¸Šä¸‹æ–‡å­¦ä¹ ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œåªè¦è¾“å…¥æ•°æ®å…·æœ‰æ’å®šçš„æ¡ä»¶æ•°ï¼Œä¾‹å¦‚$n &#x3D; O(d)$ï¼Œçº¿æ€§å¾ªç¯Transformerå°±å¯ä»¥é€šè¿‡å¤šæ­¥æ¢¯åº¦ä¸‹é™åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­å®ç°å°è¯¯å·®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„åˆæ­¥å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„ç†è®ºåˆ†æã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒTransformeræ¶æ„çš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›æ¯”ä»¥å‰æ‰€ç†è§£çš„è¦å¼ºï¼Œè¿™ä¸ºLLMçš„æœºåˆ¶æä¾›äº†æ–°çš„è§è§£ï¼Œå¹¶å¯èƒ½ä¸ºLLMè®¾è®¡æ›´æœ‰æ•ˆçš„æ¨ç†ç®—æ³•æä¾›æŒ‡å¯¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.11268v2">PDF</a> AIStats 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†çº¿æ€§å¾ªç¯Transformeråœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­çš„è¡¨ç°ï¼Œç‰¹åˆ«æ˜¯åœ¨çº¿æ€§å‘é‡ç”Ÿæˆä»»åŠ¡ä¸Šã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œçº¿æ€§å¾ªç¯Transformerå¯ä»¥æœ‰æ•ˆåœ°å®ç°å¤šæ­¥æ¢¯åº¦ä¸‹é™ï¼Œå³ä½¿åœ¨è¾“å…¥æ•°æ®å…·æœ‰æ’å®šæ¡ä»¶æ•°çš„æƒ…å†µä¸‹ï¼Œä¹Ÿèƒ½é€šè¿‡å¤šæ­¥æ¢¯åº¦ä¸‹é™å®ç°ä½è¯¯å·®çš„ä¸Šä¸‹æ–‡å­¦ä¹ ã€‚åˆæ­¥å®éªŒéªŒè¯äº†ç†è®ºåˆ†æçš„æœ‰æ•ˆæ€§ï¼Œæ­ç¤ºäº†Transformeræ¶æ„æ¯”å…ˆå‰æ‰€ç†è§£çš„æ‹¥æœ‰æ›´å¼ºå¤§çš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ï¼Œä¸ºLLMçš„æœºåˆ¶å’Œé«˜æ•ˆæ¨ç†ç®—æ³•çš„è®¾è®¡æä¾›äº†æ–°è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>In-context learningæ˜¯LLMæˆåŠŸçš„å…³é”®å› ç´ ï¼ŒæŒ‡æ¨¡å‹ä»æç¤ºä¸­çš„ä¸Šä¸‹æ–‡ç¤ºä¾‹å®æ—¶å­¦ä¹ æ¨¡å¼çš„èƒ½åŠ›ã€‚</li>
<li>ä¹‹å‰çš„ç ”ç©¶è¡¨æ˜ï¼ŒTransformeræ¶æ„å¯ä»¥é€šè¿‡å•æ¬¡å‰å‘ä¼ é€’å®ç°å•æ­¥æ¢¯åº¦ä¸‹é™æ›´æ–°ã€‚</li>
<li>æœ€æ–°å·¥ä½œæ˜¾ç¤ºï¼Œåœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­ï¼Œå¾ªç¯Transformerå¯ä»¥å®æ–½å¤šæ­¥æ¢¯åº¦ä¸‹é™æ›´æ–°ã€‚</li>
<li>ç†è®ºç»“æœéœ€è¦æŒ‡æ•°çº§çš„ä¸Šä¸‹æ–‡ç¤ºä¾‹æ•°é‡æ¥è¾¾åˆ°è¾ƒä½çš„è¯¯å·®ã€‚</li>
<li>çº¿æ€§å¾ªç¯Transformeråœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­å¯ä»¥é«˜æ•ˆå®ç°å¤šæ­¥æ¢¯åº¦ä¸‹é™ã€‚</li>
<li>å½“è¾“å…¥æ•°æ®å…·æœ‰æ’å®šæ¡ä»¶æ•°æ—¶ï¼Œçº¿æ€§å¾ªç¯Transformerå¯ä»¥é€šè¿‡å¤šæ­¥æ¢¯åº¦ä¸‹é™å®ç°å°è¯¯å·®çš„ä¸Šä¸‹æ–‡å­¦ä¹ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.11268">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e33c2db9988f92fcadaaf42facc0c9e9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-19d63db9fba1cde7cbffbb444baca8d7.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="From-Tokens-to-Words-On-the-Inner-Lexicon-of-LLMs"><a href="#From-Tokens-to-Words-On-the-Inner-Lexicon-of-LLMs" class="headerlink" title="From Tokens to Words: On the Inner Lexicon of LLMs"></a>From Tokens to Words: On the Inner Lexicon of LLMs</h2><p><strong>Authors:Guy Kaplan, Matanel Oren, Yuval Reif, Roy Schwartz</strong></p>
<p>Natural language is composed of words, but modern large language models (LLMs) process sub-words as input. A natural question raised by this discrepancy is whether LLMs encode words internally, and if so how. We present evidence that LLMs engage in an intrinsic detokenization process, where sub-word sequences are combined into coherent whole-word representations at their last token. Our experiments show that this process primarily takes place within the early and middle layers of the model. We further demonstrate its robustness to arbitrary splits (e.g., â€œcatsâ€ to â€œcaâ€ and â€œtsâ€), typos, and importantly-to out-of-vocabulary words: when feeding the last token internal representations of such words to the model as input, it can â€œunderstandâ€ them as the complete word despite never seeing such representations as input during training. Our findings suggest that LLMs maintain a latent vocabulary beyond the tokenizerâ€™s scope. These insights provide a practical, finetuning-free application for expanding the vocabulary of pre-trained models. By enabling the addition of new vocabulary words, we reduce input length and inference iterations, which reduces both space and model latency, with little to no loss in model accuracy. </p>
<blockquote>
<p>è‡ªç„¶è¯­è¨€ç”±å•è¯ç»„æˆï¼Œä½†ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å°†å­è¯ä½œä¸ºè¾“å…¥è¿›è¡Œå¤„ç†ã€‚ç”±æ­¤äº§ç”Ÿçš„ä¸€ä¸ªè‡ªç„¶é—®é¢˜æ˜¯ï¼ŒLLMæ˜¯å¦åœ¨å†…éƒ¨å¯¹å•è¯è¿›è¡Œç¼–ç ï¼Œå¦‚æœæ˜¯çš„è¯æ˜¯å¦‚ä½•è¿›è¡Œçš„ã€‚æˆ‘ä»¬æä¾›è¯æ®è¡¨æ˜ï¼ŒLLMç»å†äº†å†…åœ¨çš„ç»†èŠ‚æ¶ˆè§£è¿‡ç¨‹ï¼Œå…¶ä¸­å­è¯åºåˆ—åœ¨æœ€åä¸€ä¸ªè¯å¤„ç»„åˆæˆè¿è´¯çš„æ•´è¯è¡¨ç¤ºã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œè¿™ä¸ªè¿‡ç¨‹ä¸»è¦å‘ç”Ÿåœ¨æ¨¡å‹çš„æ—©æœŸå’Œä¸­æœŸå±‚ã€‚æˆ‘ä»¬è¿˜è¯æ˜äº†å®ƒå¯¹ä»»æ„æ‹†åˆ†ï¼ˆä¾‹å¦‚ï¼Œâ€œcatsâ€åˆ°â€œcaâ€å’Œâ€œtsâ€ï¼‰ã€æ‹¼å†™é”™è¯¯ä»¥åŠæœ€é‡è¦çš„è¶…å‡ºè¯æ±‡è¡¨çš„å•è¯çš„ç¨³å¥æ€§ï¼šå½“å°†è¿™äº›å•è¯çš„æœ€åä¸€ä¸ªå†…éƒ¨è¡¨ç¤ºä½œä¸ºè¾“å…¥æä¾›ç»™æ¨¡å‹æ—¶ï¼Œå³ä½¿å®ƒåœ¨è®­ç»ƒæœŸé—´ä»æœªè§è¿‡è¿™æ ·çš„è¡¨ç¤ºä½œä¸ºè¾“å…¥ï¼Œå®ƒä¹Ÿå¯ä»¥å°†å®ƒä»¬â€œç†è§£â€ä¸ºå®Œæ•´çš„å•è¯ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒLLMåœ¨ä»¤ç‰ŒåŒ–å™¨èŒƒå›´ä¹‹å¤–ç»´æŒäº†ä¸€ä¸ªæ½œåœ¨è¯æ±‡ã€‚è¿™äº›è§è§£æä¾›äº†ä¸€ç§å®ç”¨ã€æ— éœ€å¾®è°ƒå³å¯æ‰©å±•é¢„è®­ç»ƒæ¨¡å‹è¯æ±‡é‡çš„åº”ç”¨ã€‚é€šè¿‡æ·»åŠ æ–°è¯æ±‡å•è¯çš„èƒ½åŠ›ï¼Œæˆ‘ä»¬å‡å°‘äº†è¾“å…¥é•¿åº¦å’Œæ¨ç†è¿­ä»£æ¬¡æ•°ï¼Œä»è€Œåœ¨å‡ ä¹ä¸æŸå¤±æ¨¡å‹ç²¾åº¦çš„æƒ…å†µä¸‹å‡å°‘äº†ç©ºé—´å’Œæ¨¡å‹å»¶è¿Ÿã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.05864v4">PDF</a> Accepted to the International Conference on Learning Representations   (ICLR) 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†è‡ªç„¶è¯­è¨€æ—¶ï¼Œè™½ç„¶è¾“å…¥çš„æ˜¯å­è¯ï¼Œä½†å†…åœ¨è¿›è¡Œäº†è¯æ±‡çš„åˆå¹¶ä¸ç»„åˆï¼Œå½¢æˆè¿è´¯çš„è¯æ±‡è¡¨ç¤ºã€‚å®éªŒæ˜¾ç¤ºè¿™ä¸€è¿‡ç¨‹ä¸»è¦åœ¨æ¨¡å‹çš„å‰å‡ å±‚è¿›è¡Œï¼Œä¸”å¯¹ä»»æ„æ‹†åˆ†ã€æ‹¼å†™é”™è¯¯åŠè¶…å‡ºè¯æ±‡è¡¨çš„è¯æ±‡å…·æœ‰ç¨³å¥æ€§ã€‚æ¨¡å‹èƒ½åœ¨æ— éœ€é¢å¤–è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œå°†æœ€åä¸€é¡¹è¯æ±‡çš„å†…éƒ¨è¡¨ç¤ºä½œä¸ºè¾“å…¥è¿›è¡Œè¯†åˆ«ã€‚è¿™äº›å‘ç°æ˜¾ç¤ºLLMå…·å¤‡æ½œåœ¨çš„è¯æ±‡åº“æ‰©å±•èƒ½åŠ›ï¼Œå¯æé«˜æ¨¡å‹æ•ˆç‡å¹¶å‡å°‘æŸå¤±ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMå…·æœ‰å†…åœ¨è¯æ±‡åˆå¹¶è¿‡ç¨‹ï¼Œå°†å­è¯åºåˆ—ç»„åˆæˆå®Œæ•´çš„è¯æ±‡è¡¨ç¤ºã€‚</li>
<li>è¯¥è¿‡ç¨‹ä¸»è¦åœ¨æ¨¡å‹çš„å‰å‡ å±‚å‘ç”Ÿï¼Œæ¶‰åŠè¯æ±‡çš„è¿è´¯è¡¨ç¤ºã€‚</li>
<li>LLMå¯¹è¯æ±‡çš„ä»»æ„æ‹†åˆ†ã€æ‹¼å†™é”™è¯¯åŠè¶…å‡ºè¯æ±‡è¡¨çš„è¯æ±‡å…·æœ‰ç¨³å¥æ€§ã€‚</li>
<li>LLMèƒ½å¤Ÿè¯†åˆ«å¹¶ç†è§£è®­ç»ƒæ—¶æœªæ›¾æ¥è§¦è¿‡çš„è¯æ±‡å†…éƒ¨è¡¨ç¤ºã€‚</li>
<li>LLMå…·å¤‡æ½œåœ¨çš„è¯æ±‡åº“æ‰©å±•èƒ½åŠ›ï¼Œå¯è¶…è¶Šåˆ†è¯å™¨çš„èŒƒå›´ã€‚</li>
<li>é€šè¿‡æ‰©å±•è¯æ±‡é‡ï¼Œå¯ä»¥å‡å°‘è¾“å…¥é•¿åº¦å’Œæ¨ç†è¿­ä»£ï¼Œä»è€Œå‡å°‘ç©ºé—´éœ€æ±‚å’Œæ¨¡å‹å»¶è¿Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.05864">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-833224c02122da8f44e4267c8bb1d32c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cf3e23ced5bcccae1e5e69d1580ae04b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2c402ef73478477333cc7d409133d8cd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2c79d9c5543d9ecf6b95645829683e2a.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="TRACE-Temporal-Grounding-Video-LLM-via-Causal-Event-Modeling"><a href="#TRACE-Temporal-Grounding-Video-LLM-via-Causal-Event-Modeling" class="headerlink" title="TRACE: Temporal Grounding Video LLM via Causal Event Modeling"></a>TRACE: Temporal Grounding Video LLM via Causal Event Modeling</h2><p><strong>Authors:Yongxin Guo, Jingyu Liu, Mingda Li, Qingbin Liu, Xi Chen, Xiaoying Tang</strong></p>
<p>Video Temporal Grounding (VTG) is a crucial capability for video understanding models and plays a vital role in downstream tasks such as video browsing and editing. To effectively handle various tasks simultaneously and enable zero-shot prediction, there is a growing trend in employing video LLMs for VTG tasks. However, current video LLM-based methods rely exclusively on natural language generation, lacking the ability to model the clear structure inherent in videos, which restricts their effectiveness in tackling VTG tasks. To address this issue, this paper first formally introduces causal event modeling framework, which represents video LLM outputs as sequences of events, and predict the current event using previous events, video inputs, and textural instructions. Each event consists of three components: timestamps, salient scores, and textual captions. We then propose a novel task-interleaved video LLM called TRACE to effectively implement the causal event modeling framework in practice. The TRACE process visual frames, timestamps, salient scores, and text as distinct tasks, employing various encoders and decoding heads for each. Task tokens are arranged in an interleaved sequence according to the causal event modeling frameworkâ€™s formulation. Extensive experiments on various VTG tasks and datasets demonstrate the superior performance of TRACE compared to state-of-the-art video LLMs. Our model and code are available at <a target="_blank" rel="noopener" href="https://github.com/gyxxyg/TRACE">https://github.com/gyxxyg/TRACE</a>. </p>
<blockquote>
<p>è§†é¢‘æ—¶åºå®šä½ï¼ˆVTGï¼‰æ˜¯è§†é¢‘ç†è§£æ¨¡å‹çš„å…³é”®èƒ½åŠ›ï¼Œå¹¶åœ¨è§†é¢‘æµè§ˆå’Œç¼–è¾‘ç­‰ä¸‹æ¸¸ä»»åŠ¡ä¸­èµ·åˆ°è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ä¸ºäº†æœ‰æ•ˆåœ°åŒæ—¶å¤„ç†å„ç§ä»»åŠ¡å¹¶å®ç°é›¶æ ·æœ¬é¢„æµ‹ï¼Œè¶Šæ¥è¶Šå¤šçš„è¶‹åŠ¿æ˜¯é‡‡ç”¨è§†é¢‘LLMè¿›è¡ŒVTGä»»åŠ¡ã€‚ç„¶è€Œï¼ŒåŸºäºå½“å‰è§†é¢‘LLMçš„æ–¹æ³•ä»…ä¾èµ–äºè‡ªç„¶è¯­è¨€ç”Ÿæˆï¼Œç¼ºä¹å»ºæ¨¡è§†é¢‘å†…åœ¨æ¸…æ™°ç»“æ„çš„èƒ½åŠ›ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨å¤„ç†VTGä»»åŠ¡æ—¶çš„æœ‰æ•ˆæ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡é¦–å…ˆæ­£å¼å¼•å…¥äº†å› æœäº‹ä»¶å»ºæ¨¡æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†è§†é¢‘LLMè¾“å‡ºè¡¨ç¤ºä¸ºäº‹ä»¶åºåˆ—ï¼Œå¹¶ä½¿ç”¨ä»¥å‰çš„äº‹ä»¶ã€è§†é¢‘è¾“å…¥å’Œæ–‡æœ¬æŒ‡ä»¤æ¥é¢„æµ‹å½“å‰äº‹ä»¶ã€‚æ¯ä¸ªäº‹ä»¶ç”±ä¸‰ä¸ªç»„ä»¶ç»„æˆï¼šæ—¶é—´æˆ³ã€çªå‡ºå¾—åˆ†å’Œæ–‡æœ¬æ ‡é¢˜ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ä»»åŠ¡äº¤ç»‡è§†é¢‘LLMï¼Œç§°ä¸ºTRACEï¼Œä»¥æœ‰æ•ˆåœ°åœ¨å®é™…ä¸­å®æ–½å› æœäº‹ä»¶å»ºæ¨¡æ¡†æ¶ã€‚TRACEå°†è§†è§‰å¸§ã€æ—¶é—´æˆ³ã€çªå‡ºå¾—åˆ†å’Œæ–‡æœ¬è§†ä¸ºä¸åŒçš„ä»»åŠ¡ï¼Œå¹¶ä¸ºæ¯ä¸ªä»»åŠ¡ä½¿ç”¨å„ç§ç¼–ç å™¨å’Œè§£ç å¤´ã€‚ä»»åŠ¡æ ‡è®°æ ¹æ®å› æœäº‹ä»¶å»ºæ¨¡æ¡†æ¶çš„å…¬å¼è¿›è¡Œäº¤é”™åºåˆ—å®‰æ’ã€‚åœ¨å„ç§VTGä»»åŠ¡å’Œæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒTRACEçš„æ€§èƒ½ä¼˜äºæœ€å…ˆè¿›çš„è§†é¢‘LLMã€‚æˆ‘ä»¬çš„æ¨¡å‹å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/gyxxyg/TRACE%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/gyxxyg/TRACEä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.05643v3">PDF</a> ICLR 2025</p>
<p><strong>Summary</strong>ï¼šè§†é¢‘æ—¶åºå®šä½ï¼ˆVTGï¼‰æ˜¯è§†é¢‘ç†è§£æ¨¡å‹ä¸­çš„æ ¸å¿ƒèƒ½åŠ›ï¼Œåœ¨è§†é¢‘æµè§ˆå’Œç¼–è¾‘ç­‰ä¸‹æ¸¸ä»»åŠ¡ä¸­æ‰®æ¼”é‡è¦è§’è‰²ã€‚ç°æœ‰åŸºäºè§†é¢‘LLMçš„æ–¹æ³•ä¸»è¦ä¾èµ–è‡ªç„¶è¯­è¨€ç”Ÿæˆï¼Œç¼ºä¹å»ºæ¨¡è§†é¢‘å†…åœ¨ç»“æ„çš„èƒ½åŠ›ï¼Œé™åˆ¶äº†å…¶åœ¨VTGä»»åŠ¡ä¸Šçš„æ•ˆæœã€‚æœ¬æ–‡å¼•å…¥å› æœäº‹ä»¶å»ºæ¨¡æ¡†æ¶ï¼Œå°†è§†é¢‘LLMè¾“å‡ºè¡¨ç¤ºä¸ºäº‹ä»¶åºåˆ—ï¼Œå¹¶ä½¿ç”¨å…ˆå‰äº‹ä»¶ã€è§†é¢‘è¾“å…¥å’Œæ–‡æœ¬æŒ‡ä»¤é¢„æµ‹å½“å‰äº‹ä»¶ã€‚æå‡ºä»»åŠ¡äº¤ç»‡çš„è§†é¢‘LLMï¼ˆTRACEï¼‰ï¼Œå®ç°å› æœäº‹ä»¶å»ºæ¨¡æ¡†æ¶ã€‚TRACEå¯¹è§†è§‰å¸§ã€æ—¶é—´æˆ³ã€æ˜¾è‘—åˆ†æ•°å’Œæ–‡æœ¬è¿›è¡Œä¸åŒä»»åŠ¡å¤„ç†ï¼Œæ ¹æ®å› æœäº‹ä»¶å»ºæ¨¡æ¡†æ¶çš„å…¬å¼å®‰æ’ä»»åŠ¡æ ‡è®°ã€‚åœ¨å¤šä¸ªVTGä»»åŠ¡å’Œæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜TRACEä¼˜äºç°æœ‰è§†é¢‘LLMã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è§†é¢‘æ—¶åºå®šä½ï¼ˆVTGï¼‰åœ¨è§†é¢‘ç†è§£æ¨¡å‹ä¸­å æ®é‡è¦åœ°ä½ï¼Œå¯¹è§†é¢‘æµè§ˆå’Œç¼–è¾‘ç­‰ä¸‹æ¸¸ä»»åŠ¡è‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰åŸºäºè§†é¢‘LLMçš„æ–¹æ³•ä¸»è¦ä¾èµ–è‡ªç„¶è¯­è¨€ç”Ÿæˆï¼Œç¼ºä¹å»ºæ¨¡è§†é¢‘å†…åœ¨ç»“æ„çš„èƒ½åŠ›ã€‚</li>
<li>å¼•å…¥å› æœäº‹ä»¶å»ºæ¨¡æ¡†æ¶ï¼Œå°†è§†é¢‘LLMè¾“å‡ºè¡¨ç¤ºä¸ºäº‹ä»¶åºåˆ—ï¼ŒåŒ…æ‹¬æ—¶é—´æˆ³ã€æ˜¾è‘—åˆ†æ•°å’Œæ–‡æœ¬æè¿°ã€‚</li>
<li>æå‡ºä»»åŠ¡äº¤ç»‡çš„è§†é¢‘LLMï¼ˆTRACEï¼‰æ¥å®æ–½å› æœäº‹ä»¶å»ºæ¨¡æ¡†æ¶ã€‚</li>
<li>TRACEå¯¹è§†è§‰å¸§ã€æ—¶é—´æˆ³ã€æ˜¾è‘—åˆ†æ•°å’Œæ–‡æœ¬è¿›è¡Œä¸åŒä»»åŠ¡å¤„ç†ï¼Œä½¿ç”¨å„ç§ç¼–ç å™¨å’Œè§£ç å¤´ã€‚</li>
<li>æ ¹æ®å› æœäº‹ä»¶å»ºæ¨¡æ¡†æ¶çš„å…¬å¼å®‰æ’ä»»åŠ¡æ ‡è®°ï¼Œå®ç°æ›´å¥½çš„VTGä»»åŠ¡æ€§èƒ½ã€‚</li>
<li>åœ¨å¤šä¸ªVTGä»»åŠ¡å’Œæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜TRACEä¼˜äºç°æœ‰è§†é¢‘LLMã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.05643">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-7ad5c3f01c82678d71f1b3fbba428363.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9398f4260d04b744bc1752d9a3e23c43.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c3054ce3bd467d30e57ae44e665e1cfa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-53bc59b71f69216937337cce5c5a6619.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0f8e1ffdbcd5503dfe0d5807289fbb55.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="FoodMLLM-JP-Leveraging-Multimodal-Large-Language-Models-for-Japanese-Recipe-Generation"><a href="#FoodMLLM-JP-Leveraging-Multimodal-Large-Language-Models-for-Japanese-Recipe-Generation" class="headerlink" title="FoodMLLM-JP: Leveraging Multimodal Large Language Models for Japanese   Recipe Generation"></a>FoodMLLM-JP: Leveraging Multimodal Large Language Models for Japanese   Recipe Generation</h2><p><strong>Authors:Yuki Imajuku, Yoko Yamakata, Kiyoharu Aizawa</strong></p>
<p>Research on food image understanding using recipe data has been a long-standing focus due to the diversity and complexity of the data. Moreover, food is inextricably linked to peopleâ€™s lives, making it a vital research area for practical applications such as dietary management. Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities, not only in their vast knowledge but also in their ability to handle languages naturally. While English is predominantly used, they can also support multiple languages including Japanese. This suggests that MLLMs are expected to significantly improve performance in food image understanding tasks. We fine-tuned open MLLMs LLaVA-1.5 and Phi-3 Vision on a Japanese recipe dataset and benchmarked their performance against the closed model GPT-4o. We then evaluated the content of generated recipes, including ingredients and cooking procedures, using 5,000 evaluation samples that comprehensively cover Japanese food culture. Our evaluation demonstrates that the open models trained on recipe data outperform GPT-4o, the current state-of-the-art model, in ingredient generation. Our model achieved F1 score of 0.531, surpassing GPT-4oâ€™s F1 score of 0.481, indicating a higher level of accuracy. Furthermore, our model exhibited comparable performance to GPT-4o in generating cooking procedure text. </p>
<blockquote>
<p>ä½¿ç”¨é£Ÿè°±æ•°æ®è¿›è¡Œé£Ÿå“å›¾åƒç†è§£çš„ç ”ç©¶ä¸€ç›´æ˜¯ç„¦ç‚¹ï¼Œä¸»è¦ç”±äºæ•°æ®çš„å¤šæ ·æ€§å’Œå¤æ‚æ€§ã€‚æ­¤å¤–ï¼Œé£Ÿå“ä¸äººä»¬çš„ç”Ÿæ´»ç´§å¯†ç›¸è¿ï¼Œä½¿å¾—å…¶åœ¨é¥®é£Ÿç®¡ç†ç­‰å®é™…åº”ç”¨ä¸­æˆä¸ºé‡è¦ç ”ç©¶é¢†åŸŸã€‚è¿‘æœŸå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è¿›å±•è¡¨ç°å‡ºäº†æ˜¾è‘—çš„å®åŠ›ï¼Œä¸ä»…åœ¨äºå…¶ä¸°å¯Œçš„çŸ¥è¯†ï¼Œè¿˜åœ¨äºå…¶è‡ªç„¶å¤„ç†è¯­è¨€çš„èƒ½åŠ›ã€‚è™½ç„¶ä¸»è¦æ˜¯è‹±è¯­ä½¿ç”¨ä¸ºä¸»ï¼Œä½†å®ƒä»¬ä¹Ÿæ”¯æŒå¤šç§è¯­è¨€ï¼ŒåŒ…æ‹¬æ—¥è¯­ã€‚è¿™è¡¨æ˜MLLMsæœ‰æœ›åœ¨é£Ÿå“å›¾åƒç†è§£ä»»åŠ¡ä¸­æ˜¾è‘—æé«˜æ€§èƒ½ã€‚æˆ‘ä»¬å¯¹å…¬å¼€MLLMs LLaVA-1.5å’ŒPhi-3 Visionè¿›è¡Œäº†å¾®è°ƒï¼Œä½¿ç”¨æ—¥æœ¬é£Ÿè°±æ•°æ®é›†è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œä¸å°é—­æ¨¡å‹GPT-4oçš„æ€§èƒ½è¿›è¡Œäº†æ¯”è¾ƒã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨å…¨é¢è¦†ç›–æ—¥æœ¬é£Ÿå“æ–‡åŒ–çš„5000ä¸ªè¯„ä¼°æ ·æœ¬ï¼Œå¯¹ç”Ÿæˆé£Ÿè°±çš„å†…å®¹ï¼ˆåŒ…æ‹¬é£Ÿæå’Œçƒ¹é¥ªç¨‹åºï¼‰è¿›è¡Œäº†è¯„ä¼°ã€‚æˆ‘ä»¬çš„è¯„ä¼°è¡¨æ˜ï¼Œåœ¨é£Ÿæç”Ÿæˆæ–¹é¢ï¼ŒåŸºäºé£Ÿè°±æ•°æ®è®­ç»ƒçš„å…¬å¼€æ¨¡å‹ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹GPT-4oã€‚æˆ‘ä»¬çš„æ¨¡å‹F1åˆ†æ•°è¾¾åˆ°0.531ï¼Œè¶…è¿‡äº†GPT-4oçš„0.481ï¼Œæ˜¾ç¤ºå‡ºæ›´é«˜çš„å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨ç”Ÿæˆçƒ¹é¥ªè¿‡ç¨‹æ–‡æœ¬æ–¹é¢çš„æ€§èƒ½ä¸GPT-4oç›¸å½“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.18459v2">PDF</a> 15 pages, 5 figures. We found errors in the calculation of evaluation   metrics, which were corrected in this version with   $\color{blue}{\text{modifications highlighted in blue}}$. Please also see the   Appendix</p>
<p><strong>Summary</strong>ï¼š<br>åˆ©ç”¨é£Ÿè°±æ•°æ®ç ”ç©¶é£Ÿå“å›¾åƒè¯†åˆ«ä¸€ç›´æ˜¯ç ”ç©¶çš„é‡ç‚¹ï¼Œç”±äºæ•°æ®çš„å¤šæ ·æ€§å’Œå¤æ‚æ€§ã€‚è¿‘æœŸå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨é£Ÿå“å›¾åƒè¯†åˆ«ä»»åŠ¡ä¸Šå±•ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œä¸ä»…èƒ½å¤„ç†å¤šç§è¯­è¨€åŒ…æ‹¬æ—¥è¯­ï¼Œè€Œä¸”åœ¨é£Ÿè°±æ•°æ®ä¸Šçš„è¡¨ç°ä¼˜å¼‚ã€‚æœ¬ç ”ç©¶å¯¹LLaVA-1.5å’ŒPhi-3 Visionä¸¤ä¸ªå¼€æºæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå¹¶åœ¨æ—¥æœ¬é£Ÿè°±æ•°æ®é›†ä¸Šè¿›è¡Œæ€§èƒ½è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºå…¶åœ¨é£Ÿæç”Ÿæˆæ–¹é¢ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹GPT-4oï¼ŒF1åˆ†æ•°è¾¾åˆ°0.531ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>é£Ÿå“å›¾åƒè¯†åˆ«ç ”ç©¶å› æ•°æ®çš„å¤šæ ·æ€§å’Œå¤æ‚æ€§è€ŒæŒç»­å—åˆ°å…³æ³¨ã€‚</li>
<li>å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨é£Ÿå“å›¾åƒè¯†åˆ«ä»»åŠ¡ä¸Šå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚</li>
<li>MLLMsä¸ä»…èƒ½å¤„ç†è‹±è¯­ï¼Œä¹Ÿå¯æ”¯æŒåŒ…æ‹¬æ—¥è¯­åœ¨å†…çš„å¤šç§è¯­è¨€ã€‚</li>
<li>å¼€æºæ¨¡å‹LLaVA-1.5å’ŒPhi-3 Visionåœ¨æ—¥æœ¬é£Ÿè°±æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹GPT-4oã€‚</li>
<li>åœ¨é£Ÿæç”Ÿæˆæ–¹é¢ï¼ŒLLaVA-1.5å’ŒPhi-3 Visionçš„F1åˆ†æ•°è¾¾åˆ°0.531ï¼Œé«˜äºGPT-4oçš„0.481ã€‚</li>
<li>åœ¨ç”Ÿæˆçƒ¹é¥ªæµç¨‹æ–‡æœ¬æ–¹é¢ï¼ŒLLaVA-1.5å’ŒPhi-3 Visionä¸GPT-4oè¡¨ç°ç›¸å½“ã€‚</li>
<li>ç ”ç©¶ç»“æœè¡¨æ˜å¼€æºæ¨¡å‹åœ¨é£Ÿå“å›¾åƒè¯†åˆ«ä»»åŠ¡ä¸­çš„æ€§èƒ½æå‡æœ‰æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.18459">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fe503e25f9c7c531e458d10052ed1b71.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-558f63e9a3571e95f09190f4b1b2ae6d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e85542e92f244e41d6bddf57874e94e8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8f3056cae3f654ecc83b8299d96f720e.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="StarVid-Enhancing-Semantic-Alignment-in-Video-Diffusion-Models-via-Spatial-and-SynTactic-Guided-Attention-Refocusing"><a href="#StarVid-Enhancing-Semantic-Alignment-in-Video-Diffusion-Models-via-Spatial-and-SynTactic-Guided-Attention-Refocusing" class="headerlink" title="StarVid: Enhancing Semantic Alignment in Video Diffusion Models via   Spatial and SynTactic Guided Attention Refocusing"></a>StarVid: Enhancing Semantic Alignment in Video Diffusion Models via   Spatial and SynTactic Guided Attention Refocusing</h2><p><strong>Authors:Yuanhang Li, Qi Mao, Lan Chen, Zhen Fang, Lei Tian, Xinyan Xiao, Libiao Jin, Hua Wu</strong></p>
<p>Recent advances in text-to-video (T2V) generation with diffusion models have garnered significant attention. However, they typically perform well in scenes with a single object and motion, struggling in compositional scenarios with multiple objects and distinct motions to accurately reflect the semantic content of text prompts. To address these challenges, we propose \textbf{StarVid}, a plug-and-play, training-free method that improves semantic alignment between multiple subjects, their motions, and text prompts in T2V models. StarVid first leverages the spatial reasoning capabilities of large language models (LLMs) for two-stage motion trajectory planning based on text prompts. Such trajectories serve as spatial priors, guiding a spatial-aware loss to refocus cross-attention (CA) maps into distinctive regions. Furthermore, we propose a syntax-guided contrastive constraint to strengthen the correlation between the CA maps of verbs and their corresponding nouns, enhancing motion-subject binding. Both qualitative and quantitative evaluations demonstrate that the proposed framework significantly outperforms baseline methods, delivering videos of higher quality with improved semantic consistency. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œæ–‡æœ¬åˆ°è§†é¢‘ï¼ˆT2Vï¼‰ç”Ÿæˆä¸­çš„æ‰©æ•£æ¨¡å‹çš„è¿›å±•å¼•èµ·äº†äººä»¬çš„å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨å•ä¸€ç‰©ä½“å’Œè¿åŠ¨çš„åœºæ™¯ä¸­è¡¨ç°è¾ƒå¥½ï¼Œä½†åœ¨å¤šä¸ªç‰©ä½“å’Œä¸åŒè¿åŠ¨çš„ç»„åˆåœºæ™¯ä¸­ï¼Œéš¾ä»¥å‡†ç¡®åæ˜ æ–‡æœ¬æç¤ºçš„è¯­ä¹‰å†…å®¹ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†<strong>StarVid</strong>ï¼Œè¿™æ˜¯ä¸€ç§å³æ’å³ç”¨ã€æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜T2Væ¨¡å‹ä¸­å¤šä¸ªä¸»é¢˜ã€å…¶è¿åŠ¨ä¸æ–‡æœ¬æç¤ºä¹‹é—´çš„è¯­ä¹‰å¯¹é½ã€‚StarVidé¦–å…ˆåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œè¿›è¡ŒåŸºäºæ–‡æœ¬æç¤ºçš„ä¸¤é˜¶æ®µè¿åŠ¨è½¨è¿¹è§„åˆ’ã€‚è¿™äº›è½¨è¿¹ä½œä¸ºç©ºé—´å…ˆéªŒï¼Œå¼•å¯¼ç©ºé—´æ„ŸçŸ¥æŸå¤±æ¥é‡æ–°å®šä½äº¤å‰æ³¨æ„åŠ›ï¼ˆCAï¼‰åœ°å›¾åˆ°ä¸åŒçš„åŒºåŸŸã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†è¯­æ³•æŒ‡å¯¼çš„å¯¹æ¯”çº¦æŸï¼Œä»¥åŠ å¼ºåŠ¨è¯å’Œå¯¹åº”åè¯çš„CAåœ°å›¾ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œå¢å¼ºè¿åŠ¨ä¸»é¢˜ç»‘å®šã€‚å®šæ€§å’Œå®šé‡è¯„ä¼°å‡è¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ¡†æ¶æ˜¾è‘—ä¼˜äºåŸºå‡†æ–¹æ³•ï¼Œç”Ÿæˆçš„è§†é¢‘è´¨é‡æ›´é«˜ï¼Œè¯­ä¹‰ä¸€è‡´æ€§æ›´å¼ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.15259v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬åˆ°è§†é¢‘ï¼ˆT2Vï¼‰ç”Ÿæˆé¢†åŸŸè¿‘æœŸåˆ©ç”¨æ‰©æ•£æ¨¡å‹å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨å¤„ç†åŒ…å«å¤šä¸ªå¯¹è±¡å’Œä¸åŒè¿åŠ¨çš„åœºæ™¯æ—¶ä»é¢ä¸´æŒ‘æˆ˜ï¼Œéš¾ä»¥å‡†ç¡®åæ˜ æ–‡æœ¬æç¤ºçš„è¯­ä¹‰å†…å®¹ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæå‡ºä¸€ç§åä¸ºStarVidçš„å³æ’å³ç”¨ã€æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œå¯æå‡T2Væ¨¡å‹ä¸­å¤šä¸ªä¸»é¢˜ã€å…¶è¿åŠ¨å’Œæ–‡æœ¬æç¤ºä¹‹é—´çš„è¯­ä¹‰å¯¹é½ã€‚StarVidé¦–å…ˆåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›ï¼ŒåŸºäºæ–‡æœ¬æç¤ºè¿›è¡Œä¸¤é˜¶æ®µè¿åŠ¨è½¨è¿¹è§„åˆ’ã€‚è¿™äº›è½¨è¿¹ä½œä¸ºç©ºé—´å…ˆéªŒï¼Œå¼•å¯¼ç©ºé—´æ„ŸçŸ¥æŸå¤±ï¼Œå°†äº¤å‰æ³¨æ„åŠ›ï¼ˆCAï¼‰åœ°å›¾é‡æ–°å®šä½åˆ°ä¸åŒåŒºåŸŸã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºä¸€ç§è¯­æ³•æŒ‡å¯¼çš„å¯¹æ¯”çº¦æŸï¼Œå¢å¼ºåŠ¨è¯å’Œå¯¹åº”åè¯çš„CAåœ°å›¾ä¹‹é—´çš„å…³è”æ€§ï¼Œå¢å¼ºè¿åŠ¨ä¸»é¢˜ç»‘å®šã€‚è¯„ä¼°å’Œå®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºåŸºå‡†æ–¹æ³•ï¼Œç”Ÿæˆé«˜è´¨é‡ä¸”è¯­ä¹‰ä¸€è‡´çš„è§†é¢‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆé¢†åŸŸå­˜åœ¨æŒ‘æˆ˜ï¼šå¤„ç†åŒ…å«å¤šä¸ªå¯¹è±¡å’Œä¸åŒè¿åŠ¨çš„åœºæ™¯æ—¶éš¾ä»¥å‡†ç¡®åæ˜ æ–‡æœ¬æç¤ºçš„è¯­ä¹‰å†…å®¹ã€‚</li>
<li>StarVidæ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œå¯æ”¹å–„T2Væ¨¡å‹ä¸­è¯­ä¹‰å¯¹é½é—®é¢˜ã€‚</li>
<li>StarVidåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›è¿›è¡Œä¸¤é˜¶æ®µè¿åŠ¨è½¨è¿¹è§„åˆ’ã€‚</li>
<li>ç©ºé—´å…ˆéªŒå’Œäº¤å‰æ³¨æ„åŠ›åœ°å›¾çš„é‡å®šä½æ˜¯æé«˜è§†é¢‘è´¨é‡çš„å…³é”®ã€‚</li>
<li>è¯­æ³•æŒ‡å¯¼çš„å¯¹æ¯”çº¦æŸå¢å¼ºäº†åŠ¨è¯å’Œåè¯ä¹‹é—´çš„å…³è”æ€§ï¼Œæé«˜è¿åŠ¨ä¸»é¢˜ç»‘å®šã€‚</li>
<li>ä¸åŸºå‡†æ–¹æ³•ç›¸æ¯”ï¼ŒStarVidæ˜¾è‘—æé«˜äº†è§†é¢‘ç”Ÿæˆçš„è¯­ä¹‰ä¸€è‡´æ€§å’Œè´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.15259">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-326ba55ff57d6ba5a8b130ac7b8de2a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5474bde42c31899919fc6cd935200c90.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4841bfa2782963256f00fc6c4f17d5ed.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bfc7ab27cff27b067e582c2bbb1053c6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a80850315768456e729a338ea89d876e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c9677a2d5e36d7abb093307b7184cf7a.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="PAPILLON-Efficient-and-Stealthy-Fuzz-Testing-Powered-Jailbreaks-for-LLMs"><a href="#PAPILLON-Efficient-and-Stealthy-Fuzz-Testing-Powered-Jailbreaks-for-LLMs" class="headerlink" title="PAPILLON: Efficient and Stealthy Fuzz Testing-Powered Jailbreaks for   LLMs"></a>PAPILLON: Efficient and Stealthy Fuzz Testing-Powered Jailbreaks for   LLMs</h2><p><strong>Authors:Xueluan Gong, Mingzhe Li, Yilin Zhang, Fengyuan Ran, Chen Chen, Yanjiao Chen, Qian Wang, Kwok-Yan Lam</strong></p>
<p>Large Language Models (LLMs) have excelled in various tasks but are still vulnerable to jailbreaking attacks, where attackers create jailbreak prompts to mislead the model to produce harmful or offensive content. Current jailbreak methods either rely heavily on manually crafted templates, which pose challenges in scalability and adaptability, or struggle to generate semantically coherent prompts, making them easy to detect. Additionally, most existing approaches involve lengthy prompts, leading to higher query costs. In this paper, to remedy these challenges, we introduce a novel jailbreaking attack framework called PAPILLON, which is an automated, black-box jailbreaking attack framework that adapts the black-box fuzz testing approach with a series of customized designs. Instead of relying on manually crafted templates,PAPILLON starts with an empty seed pool, removing the need to search for any related jailbreaking templates. We also develop three novel question-dependent mutation strategies using an LLM helper to generate prompts that maintain semantic coherence while significantly reducing their length. Additionally, we implement a two-level judge module to accurately detect genuine successful jailbreaks. We evaluated PAPILLON on 7 representative LLMs and compared it with 5 state-of-the-art jailbreaking attack strategies. For proprietary LLM APIs, such as GPT-3.5 turbo, GPT-4, and Gemini-Pro, PAPILLONs achieves attack success rates of over 90%, 80%, and 74%, respectively, exceeding existing baselines by more than 60%. Additionally, PAPILLON can maintain high semantic coherence while significantly reducing the length of jailbreak prompts. When targeting GPT-4, PAPILLON can achieve over 78% attack success rate even with 100 tokens. Moreover, PAPILLON demonstrates transferability and is robust to state-of-the-art defenses. Code: <a target="_blank" rel="noopener" href="https://github.com/aaFrostnova/Papillon">https://github.com/aaFrostnova/Papillon</a> </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ä»å®¹æ˜“å—åˆ°è¶Šç‹±æ”»å‡»çš„å½±å“ã€‚æ”»å‡»è€…ä¼šåˆ¶é€ è¶Šç‹±æç¤ºæ¥è¯¯å¯¼æ¨¡å‹ï¼Œäº§ç”Ÿæœ‰å®³æˆ–å†’çŠ¯æ€§çš„å†…å®¹ã€‚å½“å‰çš„è¶Šç‹±æ–¹æ³•è¦ä¹ˆä¸¥é‡ä¾èµ–æ‰‹å·¥åˆ¶ä½œçš„æ¨¡æ¿ï¼Œè¿™åœ¨å¯æ‰©å±•æ€§å’Œé€‚åº”æ€§æ–¹é¢å¸¦æ¥æŒ‘æˆ˜ï¼Œè¦ä¹ˆéš¾ä»¥ç”Ÿæˆè¯­ä¹‰è¿è´¯çš„æç¤ºï¼Œä½¿å…¶å®¹æ˜“è¢«æ£€æµ‹ã€‚æ­¤å¤–ï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•éƒ½éœ€è¦å†—é•¿çš„æç¤ºï¼Œå¯¼è‡´æŸ¥è¯¢æˆæœ¬è¾ƒé«˜ã€‚</p>
</blockquote>
<p>é’ˆå¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºPAPILLONçš„æ–°å‹è¶Šç‹±æ”»å‡»æ¡†æ¶ã€‚å®ƒæ˜¯ä¸€ä¸ªè‡ªåŠ¨åŒ–ã€é»‘ç®±è¶Šç‹±æ”»å‡»æ¡†æ¶ï¼Œé‡‡ç”¨é»‘ç®±æ¨¡ç³Šæµ‹è¯•æ–¹æ³•ä¸ä¸€ç³»åˆ—å®šåˆ¶è®¾è®¡ç›¸ç»“åˆã€‚ä¸ä¾èµ–æ‰‹å·¥åˆ¶ä½œçš„æ¨¡æ¿ä¸åŒï¼ŒPAPILLONä»ä¸€ä¸ªç©ºçš„ç§å­æ± å¼€å§‹ï¼Œæ— éœ€æœç´¢ä»»ä½•ç›¸å…³çš„è¶Šç‹±æ¨¡æ¿ã€‚æˆ‘ä»¬è¿˜åˆ©ç”¨LLMåŠ©æ‰‹å¼€å‘äº†ä¸‰ç§æ–°é¢–çš„é—®é¢˜ç›¸å…³å˜å¼‚ç­–ç•¥ï¼Œç”Ÿæˆä¿æŒè¯­ä¹‰è¿è´¯æ€§åŒæ—¶æ˜¾è‘—ç¼©çŸ­é•¿åº¦çš„æç¤ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å®ç°äº†ä¸€ä¸ªä¸¤çº§åˆ¤æ–­æ¨¡å—ï¼Œä»¥å‡†ç¡®æ£€æµ‹çœŸæ­£çš„æˆåŠŸè¶Šç‹±ã€‚</p>
<p>æˆ‘ä»¬åœ¨7ä¸ªä»£è¡¨æ€§LLMä¸Šè¯„ä¼°äº†PAPILLONï¼Œå¹¶ä¸5ç§æœ€å…ˆè¿›çš„è¶Šç‹±æ”»å‡»ç­–ç•¥è¿›è¡Œäº†æ¯”è¾ƒã€‚å¯¹äºä¸“æœ‰LLM APIï¼Œå¦‚GPT-3.5 turboã€GPT-4å’ŒGemini-Proï¼ŒPAPILLONçš„æ”»å‡»æˆåŠŸç‡åˆ†åˆ«è¶…è¿‡90%ã€80%å’Œ74%ï¼Œè¶…è¿‡ç°æœ‰åŸºçº¿60%ä»¥ä¸Šã€‚æ­¤å¤–ï¼ŒPAPILLONåœ¨ä¿æŒé«˜è¯­ä¹‰è¿è´¯æ€§çš„åŒæ—¶ï¼Œèƒ½æ˜¾è‘—ç¼©çŸ­è¶Šç‹±æç¤ºçš„é•¿åº¦ã€‚å½“é’ˆå¯¹GPT-4æ—¶ï¼Œå³ä½¿åœ¨100ä¸ªä»¤ç‰Œçš„æƒ…å†µä¸‹ï¼ŒPAPILLONçš„æ”»å‡»æˆåŠŸç‡ä¹Ÿèƒ½è¾¾åˆ°78%ä»¥ä¸Šã€‚è€Œä¸”ï¼ŒPAPILLONå…·æœ‰å¯è¿ç§»æ€§ï¼Œå¯¹æœ€å…ˆè¿›çš„é˜²å¾¡æªæ–½è¡¨ç°å‡ºç¨³å¥æ€§ã€‚</p>
<p>ä»£ç ï¼š<a target="_blank" rel="noopener" href="https://github.com/aaFrostnova/Papillon">https://github.com/aaFrostnova/Papillon</a></p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.14866v5">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>LLMsè™½ç„¶æ“…é•¿å„ç§ä»»åŠ¡ï¼Œä½†ä»æ˜“å—è¶Šç‹±æ”»å‡»çš„å½±å“ã€‚ç°æœ‰è¶Šç‹±æ–¹æ³•è¦ä¹ˆä¾èµ–äºæ‰‹å·¥åˆ¶ä½œçš„æ¨¡æ¿ï¼Œé¢ä¸´å¯æ‰©å±•æ€§å’Œé€‚åº”æ€§çš„æŒ‘æˆ˜ï¼Œè¦ä¹ˆéš¾ä»¥ç”Ÿæˆè¯­ä¹‰è¿è´¯çš„æç¤ºï¼Œå®¹æ˜“è¢«æ£€æµ‹ã€‚é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºPAPILLONçš„è‡ªåŠ¨åŒ–é»‘ç®±è¶Šç‹±æ”»å‡»æ¡†æ¶ï¼Œé‡‡ç”¨é»‘ç®±æ¨¡ç³Šæµ‹è¯•æ–¹æ³•ä¸ä¸€ç³»åˆ—å®šåˆ¶è®¾è®¡ç›¸ç»“åˆã€‚PAPILLONé€šè¿‡è‡ªåŠ¨åŒ–ç”Ÿæˆè¯­ä¹‰è¿è´¯ä¸”ç®€çŸ­çš„å†…å®¹ç»•è¿‡ç°æœ‰LLMé˜²çº¿å®æ–½æ”»å‡»ï¼Œæ˜¾è‘—æé«˜äº†è¶Šç‹±æˆåŠŸç‡å¹¶é™ä½äº†æŸ¥è¯¢æˆæœ¬ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>LLMså°½ç®¡åœ¨å„ç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ä»é¢ä¸´è¶Šç‹±æ”»å‡»çš„é£é™©ã€‚</li>
<li>å½“å‰è¶Šç‹±æ–¹æ³•é¢ä¸´çš„æŒ‘æˆ˜åŒ…æ‹¬ç¼ºä¹å¯æ‰©å±•æ€§å’Œé€‚åº”æ€§ã€éš¾ä»¥ç”Ÿæˆè¯­ä¹‰è¿è´¯çš„æç¤ºä»¥åŠé«˜æŸ¥è¯¢æˆæœ¬ã€‚</li>
<li>PAPILLONæ˜¯ä¸€ç§æ–°å‹è‡ªåŠ¨åŒ–é»‘ç®±è¶Šç‹±æ”»å‡»æ¡†æ¶ï¼Œæ— éœ€æœç´¢ç›¸å…³è¶Šç‹±æ¨¡æ¿ã€‚</li>
<li>PAPILLONé‡‡ç”¨ä¸‰ç§æ–°å‹é—®é¢˜ç›¸å…³çš„å˜å¼‚ç­–ç•¥ç”Ÿæˆç®€çŸ­ä¸”è¯­ä¹‰è¿è´¯çš„æç¤ºã€‚</li>
<li>PAPILLONå®æ–½äº†ä¸€ä¸ªä¸¤çº§åˆ¤æ–­æ¨¡å—æ¥å‡†ç¡®æ£€æµ‹çœŸæ­£çš„æˆåŠŸè¶Šç‹±ã€‚</li>
<li>åœ¨ä»£è¡¨æ€§LLMsä¸Šè¯„ä¼°ï¼ŒPAPILLONå¯¹ä¸“æœ‰LLM APIï¼ˆå¦‚GPT-3.5 turboã€GPT-4å’ŒGemini-Proï¼‰çš„è¶Šç‹±æˆåŠŸç‡è¶…è¿‡ç°æœ‰åŸºçº¿60%ä»¥ä¸Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.14866">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-92b67fd7135b82ba9b64c119a2718fe7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6769c91fc3a74c6211ccce60b9190d53.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a5923eb5e2352654106d6481c976f91.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-91c53997329247b01a036b2ccac4ebcc.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-05/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-05/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-05/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-3de82161d67aeb20a744e490e5639f7c.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-05  FACT-AUDIT An Adaptive Multi-Agent Framework for Dynamic Fact-Checking   Evaluation of Large Language Models
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-05
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-04/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-beed8e9da414ec2166ae132d50ab2e69.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-04  ARTalk Speech-Driven 3D Head Animation via Autoregressive Model
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-04
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">31987.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
