<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-03-05  Learning to Learn Weight Generation via Trajectory Diffusion">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-359b7d32c8d2e3115557c0e7d2f42cbd.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-03-05
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-03-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    32 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-03-05-更新"><a href="#2025-03-05-更新" class="headerlink" title="2025-03-05 更新"></a>2025-03-05 更新</h1><h2 id="Learning-to-Learn-Weight-Generation-via-Trajectory-Diffusion"><a href="#Learning-to-Learn-Weight-Generation-via-Trajectory-Diffusion" class="headerlink" title="Learning to Learn Weight Generation via Trajectory Diffusion"></a>Learning to Learn Weight Generation via Trajectory Diffusion</h2><p><strong>Authors:Yunchuan Guan, Yu Liu, Ke Zhou, Zhiqi Shen, Serge Belongie, Jenq-Neng Hwang, Lei Li</strong></p>
<p>Diffusion-based algorithms have emerged as promising techniques for weight generation, particularly in scenarios like multi-task learning that require frequent weight updates. However, existing solutions suffer from limited cross-task transferability. In addition, they only utilize optimal weights as training samples, ignoring the value of other weights in the optimization process. To address these issues, we propose Lt-Di, which integrates the diffusion algorithm with meta-learning to generate weights for unseen tasks. Furthermore, we extend the vanilla diffusion algorithm into a trajectory diffusion algorithm to utilize other weights along the optimization trajectory. Trajectory diffusion decomposes the entire diffusion chain into multiple shorter ones, improving training and inference efficiency. We analyze the convergence properties of the weight generation paradigm and improve convergence efficiency without additional time overhead. Our experiments demonstrate Lt-Di’s higher accuracy while reducing computational overhead across various tasks, including zero-shot and few-shot learning, multi-domain generalization, and large-scale language model fine-tuning.Our code is released at <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/Lt-Di-0E51">https://anonymous.4open.science/r/Lt-Di-0E51</a>. </p>
<blockquote>
<p>基于扩散的算法已成为权重生成的有前途的技术，特别是在需要频繁权重更新的多任务学习等场景中。然而，现有解决方案存在跨任务迁移能力有限的不足。此外，它们仅利用最优权重作为训练样本，忽略了优化过程中其他权重的价值。为了解决这些问题，我们提出了Lt-Di，它将扩散算法与元学习相结合，以生成未见任务的权重。此外，我们将普通的扩散算法扩展为轨迹扩散算法，以利用优化轨迹上的其他权重。轨迹扩散将整个扩散链分解为多个较短的链，提高了训练和推理效率。我们分析了权重生成范式的收敛属性，提高了收敛效率，而没有增加额外的时间开销。我们的实验表明，Lt-Di在各种任务中具有较高的准确性，同时减少了计算开销，包括零样本和少样本学习、多域泛化以及大规模语言模型的微调。我们的代码发布在：<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/Lt-Di-0E51%E3%80%82">https://anonymous.4open.science/r/Lt-Di-0E51。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01117v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>扩散算法结合元学习生成权重，解决多任务学习中的跨任务迁移问题，并扩展为轨迹扩散算法，利用优化过程中的其他权重。分析权重生成范式的收敛性质，提高收敛效率且无需额外时间开销。实验证明，Lt-Di在多种任务上表现更精确，包括零样本和少样本学习、跨域泛化和大规模语言模型微调。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散算法在多任务学习中展现出生成权重的潜力。</li>
<li>现有解决方案存在跨任务迁移的局限性。</li>
<li>Lt-Di结合扩散算法与元学习，生成未见任务的权重。</li>
<li>轨迹扩散算法利用优化过程中的其他权重。</li>
<li>Lt-Di提高了训练效率和推理效率。</li>
<li>Lt-Di提高了权重生成范式的收敛效率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01117">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c754ede68d43ec54006e55d61d4fb4dd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a166195fcd2db168a50d21d1d59850b3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e4c91ab2b9694eb4eb2032630d28a4d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-629046bf796e392d79c1b06bc63c7a47.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Text-driven-Adaptation-of-Foundation-Models-for-Few-shot-Surgical-Workflow-Analysis"><a href="#Text-driven-Adaptation-of-Foundation-Models-for-Few-shot-Surgical-Workflow-Analysis" class="headerlink" title="Text-driven Adaptation of Foundation Models for Few-shot Surgical   Workflow Analysis"></a>Text-driven Adaptation of Foundation Models for Few-shot Surgical   Workflow Analysis</h2><p><strong>Authors:Tingxuan Chen, Kun Yuan, Vinkle Srivastav, Nassir Navab, Nicolas Padoy</strong></p>
<p>Purpose: Surgical workflow analysis is crucial for improving surgical efficiency and safety. However, previous studies rely heavily on large-scale annotated datasets, posing challenges in cost, scalability, and reliance on expert annotations. To address this, we propose Surg-FTDA (Few-shot Text-driven Adaptation), designed to handle various surgical workflow analysis tasks with minimal paired image-label data.   Methods: Our approach has two key components. First, Few-shot selection-based modality alignment selects a small subset of images and aligns their embeddings with text embeddings from the downstream task, bridging the modality gap. Second, Text-driven adaptation leverages only text data to train a decoder, eliminating the need for paired image-text data. This decoder is then applied to aligned image embeddings, enabling image-related tasks without explicit image-text pairs.   Results: We evaluate our approach to generative tasks (image captioning) and discriminative tasks (triplet recognition and phase recognition). Results show that Surg-FTDA outperforms baselines and generalizes well across downstream tasks.   Conclusion: We propose a text-driven adaptation approach that mitigates the modality gap and handles multiple downstream tasks in surgical workflow analysis, with minimal reliance on large annotated datasets. The code and dataset will be released in <a target="_blank" rel="noopener" href="https://github.com/CAMMA-public/Surg-FTDA">https://github.com/CAMMA-public/Surg-FTDA</a> </p>
<blockquote>
<p>目的：手术工作流程分析对于提高手术效率和安全性至关重要。然而，之前的研究严重依赖于大规模标注数据集，这带来了成本、可扩展性和对专家标注的依赖等方面的挑战。为了解决这一问题，我们提出了Surg-FTDA（Few-shot文本驱动适应）方法，旨在用最少配对的图像标签数据来处理各种手术工作流程分析任务。</p>
</blockquote>
<p>方法：我们的方法有两个关键组成部分。首先，基于Few-shot选择的模态对齐选择一小部分图像，并将它们的嵌入与下游任务的文本嵌入对齐，从而弥合了模态之间的差距。其次，文本驱动适应仅利用文本数据来训练解码器，从而消除了对配对图像文本数据的需求。然后，将这个解码器应用于对齐的图像嵌入，从而能够在没有明确的图像文本对的情况下执行图像相关任务。</p>
<p>结果：我们对生成任务（图像字幕）和鉴别任务（三元组识别和阶段识别）进行了评估。结果表明，Surg-FTDA优于基线，并在下游任务中具有良好的泛化能力。</p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09555v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为Surg-FTDA（Few-shot文本驱动适应）的方法，用于改进手术流程分析任务的效率和安全性。该方法通过少量配对图像标签数据，解决大规模标注数据集带来的成本、可扩展性和依赖专家标注的问题。通过文本驱动适应策略，该方法能够利用文本数据训练解码器，无需配对图像文本数据即可完成图像相关任务。实验结果表明，Surg-FTDA在生成任务（图像描述）和判别任务（三元组识别和阶段识别）中均表现优异，并能很好地应用于不同下游任务。代码和数据集将在<a target="_blank" rel="noopener" href="https://github.com/CAMMA-public/Surg-FTDA%E5%85%AC%E5%BC%80%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/CAMMA-public/Surg-FTDA公开发布。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Surg-FTDA旨在解决手术流程分析中的效率和安全问题，特别是对数据标注的挑战性。</li>
<li>方法包括两个关键部分：基于少数选择的模态对齐和文本驱动适应策略。</li>
<li>通过选择少量图像并与其下游任务的文本嵌入进行对齐，缩小了模态差距。</li>
<li>仅使用文本数据训练解码器，无需配对图像文本数据即可完成图像相关任务。</li>
<li>Surg-FTDA在生成任务和判别任务上表现优异，具有广泛的应用潜力。</li>
<li>代码和数据集将在公共存储库中进行共享以供进一步研究。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09555">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-22d55c0c82e5d703b349f9be11191c90.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-741f3496eb39e29b1d523f763f7297d8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-964423334114aa83cdedea9c263296c6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d6cd0dd712f9125aeed779a98e5312ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c97290226d910425b794303da5958fdd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f7713590a82ff60cd156881b244c56e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-359b7d32c8d2e3115557c0e7d2f42cbd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6de8039cc24d7d8ab1286ebf897e9b70.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2cbc91f323f3004945d059d79884d39f.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="On-the-Generalization-and-Adaptation-Ability-of-Machine-Generated-Text-Detectors-in-Academic-Writing"><a href="#On-the-Generalization-and-Adaptation-Ability-of-Machine-Generated-Text-Detectors-in-Academic-Writing" class="headerlink" title="On the Generalization and Adaptation Ability of Machine-Generated Text   Detectors in Academic Writing"></a>On the Generalization and Adaptation Ability of Machine-Generated Text   Detectors in Academic Writing</h2><p><strong>Authors:Yule Liu, Zhiyuan Zhong, Yifan Liao, Zhen Sun, Jingyi Zheng, Jiaheng Wei, Qingyuan Gong, Fenghua Tong, Yang Chen, Yang Zhang, Xinlei He</strong></p>
<p>The rising popularity of large language models (LLMs) has raised concerns about machine-generated text (MGT), particularly in academic settings, where issues like plagiarism and misinformation are prevalent. As a result, developing a highly generalizable and adaptable MGT detection system has become an urgent priority. Given that LLMs are most commonly misused in academic writing, this work investigates the generalization and adaptation capabilities of MGT detectors in three key aspects specific to academic writing: First, we construct MGT-Acedemic, a large-scale dataset comprising over 336M tokens and 749K samples. MGT-Acedemic focuses on academic writing, featuring human-written texts (HWTs) and MGTs across STEM, Humanities, and Social Sciences, paired with an extensible code framework for efficient benchmarking. Second, we benchmark the performance of various detectors for binary classification and attribution tasks in both in-domain and cross-domain settings. This benchmark reveals the often-overlooked challenges of attribution tasks. Third, we introduce a novel attribution task where models have to adapt to new classes over time without (or with very limited) access to prior training data in both few-shot and many-shot scenarios. We implement eight different adapting techniques to improve the performance and highlight the inherent complexity of the task. Our findings provide insights into the generalization and adaptation ability of MGT detectors across diverse scenarios and lay the foundation for building robust, adaptive detection systems. The code framework is available at <a target="_blank" rel="noopener" href="https://github.com/Y-L-LIU/MGTBench-2.0">https://github.com/Y-L-LIU/MGTBench-2.0</a>. </p>
<blockquote>
<p>随着大型语言模型（LLM）的日益普及，人们开始关注机器生成文本（MGT）的问题，特别是在学术环境中，抄袭和误信息等问题的普遍存在。因此，开发一个高度通用化和适应性的MGT检测系统已成为一项紧迫的任务。鉴于LLM在学术写作中最常被滥用，本研究从学术写作的角度出发，探讨了MGT检测器在三个方面的通用性和适应性能力：首先，我们构建了MGT-Acedemic数据集，该数据集包含超过3.36亿个标记和74.9万个样本的大规模数据集。MGT-Acedemic专注于学术写作领域，涵盖了科学、工程、文学和人文学科的文本（人类原创文本）和MGT，以及与可伸缩代码框架配合进行高效基准测试的功能。其次，我们对多种检测器进行了基准测试，这些检测器要在本地域和跨地域设置中进行二进制分类和归属任务分类任务。这个基准测试揭示出归属任务分类挑战被经常被忽视。最后，我们引入了一项新颖的归属任务分类测试，测试模型在时间推移中对新类别的适应力在没有或者很少有先前的训练数据的情况下对这两种情景进行短时长（少样本和多样本）进行推演预测的能力。我们实施了八种不同的适应技术来提高性能并突出任务的固有复杂性。我们的研究为深入了解MGT检测器在不同场景下的通用性和适应能力提供了见解，并为构建稳健的适应性检测系统奠定了基础。代码框架可在<a target="_blank" rel="noopener" href="https://github.com/Y-L-LIU/MGTBench-2.0%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Y-L-LIU/MGTBench-2.0获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17242v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文关注大型语言模型（LLMs）在学术写作中的滥用问题，特别是机器生成文本（MGT）的检测。研究构建了针对学术写作的MGT-Acedemic数据集，对多种检测器进行基准测试，并引入了一种新的归属任务。研究发现检测器在不同场景下的通用性和适应性挑战，并提出了多种适应技术以提高性能。这为构建稳健、自适应的检测系统提供了见解。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）在学术写作中的滥用引发了对机器生成文本（MGT）的关注，特别是在存在抄袭和误导信息问题的学术环境中。</li>
<li>构建了专注于学术写作的MGT-Acedemic数据集，包含超过3.36亿个标记和74.9万个样本，涵盖STEM、人文和社会科学领域的人类撰写文本（HWTs）和MGTs。</li>
<li>对多种检测器进行基准测试，包括二进制分类和归属任务，在内部和跨领域环境中均表现出挑战。</li>
<li>引入了一种新的归属任务，要求模型在少量或没有先前训练数据的情况下，随着时间的推移适应新类别。</li>
<li>实施八种不同的适应技术以提高性能，突显了任务的固有复杂性。</li>
<li>研究结果提供了关于MGT检测器在不同场景下的通用性和适应能力的见解。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17242">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-c4fcb6e8d4659cf262685b104d024637.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a61840c1d461c9fffd517feb8b09f349.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-09ae46c2c5a6a6b0a8c6e3c19ec85db8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c04ec85d95acf51bd29207e253b51d7b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-468fb30c0dfecd986aeb542c11d1d4a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19bc05afa534f1a1c4cf58276c5170a8.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Dist-Loss-Enhancing-Regression-in-Few-Shot-Region-through-Distribution-Distance-Constraint"><a href="#Dist-Loss-Enhancing-Regression-in-Few-Shot-Region-through-Distribution-Distance-Constraint" class="headerlink" title="Dist Loss: Enhancing Regression in Few-Shot Region through Distribution   Distance Constraint"></a>Dist Loss: Enhancing Regression in Few-Shot Region through Distribution   Distance Constraint</h2><p><strong>Authors:Guangkun Nie, Gongzheng Tang, Shenda Hong</strong></p>
<p>Imbalanced data distributions are prevalent in real-world scenarios, posing significant challenges in both imbalanced classification and imbalanced regression tasks. They often cause deep learning models to overfit in areas of high sample density (many-shot regions) while underperforming in areas of low sample density (few-shot regions). This characteristic restricts the utility of deep learning models in various sectors, notably healthcare, where areas with few-shot data hold greater clinical relevance. While recent studies have shown the benefits of incorporating distribution information in imbalanced classification tasks, such strategies are rarely explored in imbalanced regression. In this paper, we address this issue by introducing a novel loss function, termed Dist Loss, designed to minimize the distribution distance between the model’s predictions and the target labels in a differentiable manner, effectively integrating distribution information into model training. Dist Loss enables deep learning models to regularize their output distribution during training, effectively enhancing their focus on few-shot regions. We have conducted extensive experiments across three datasets spanning computer vision and healthcare: IMDB-WIKI-DIR, AgeDB-DIR, and ECG-Ka-DIR. The results demonstrate that Dist Loss effectively mitigates the negative impact of imbalanced data distribution on model performance, achieving state-of-the-art results in sparse data regions. Furthermore, Dist Loss is easy to integrate, complementing existing methods. </p>
<blockquote>
<p>不平衡数据分布在实际场景中普遍存在，给不平衡分类和不平衡回归任务带来了重大挑战。它们经常导致深度学习模型在高样本密度区域（多镜头区域）过拟合，而在低样本密度区域（小样本区域）表现不佳。这种特性限制了深度学习模型在各个领域的实用性，特别是在医疗保健领域，小样本数据区域具有更大的临床意义。虽然最近的研究已经显示了将分布信息融入不平衡分类任务中的好处，但在不平衡回归中很少探索此类策略。在本文中，我们通过引入一种新型损失函数来解决这个问题，称为Dist Loss，旨在以可区分的方式最小化模型预测与目标标签之间的分布距离，有效地将分布信息融入模型训练。Dist Loss使深度学习模型能够在训练过程中规范其输出分布，从而有效提高它们对小样本区域的关注。我们在跨越计算机视觉和医疗保健的三个数据集上进行了广泛实验：IMDB-WIKI-DIR、AgeDB-DIR和ECG-Ka-DIR。结果表明，Dist Loss有效地减轻了不平衡数据分布对模型性能的负面影响，在稀疏数据区域实现了最新技术成果。此外，Dist Loss易于集成，可以弥补现有方法的不足。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.15216v2">PDF</a> </p>
<p><strong>Summary</strong><br>深度学习模型在处理不平衡数据分布时面临挑战，特别是在少样本区域表现不佳。本文提出一种新型损失函数——Dist Loss，通过最小化模型预测与目标标签之间的分布距离，将分布信息融入模型训练，提高模型在少样本区域的关注度。实验结果表明，Dist Loss能有效缓解不平衡数据分布对模型性能的负面影响，达到稀疏数据区域的最佳效果，且易于集成。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>不平衡数据分布在现实场景中普遍存在，对深度学习的分类和回归任务都带来挑战。</li>
<li>深度学习模型在高样本密度区域容易过拟合，而在低样本密度区域表现不佳。</li>
<li>本文提出了一种新型的损失函数——Dist Loss，旨在最小化模型预测与目标标签之间的分布距离。</li>
<li>Dist Loss能够使得深度模型在训练过程中规范其输出分布，提高对少样本区域的关注度。</li>
<li>实验表明，Dist Loss能有效缓解不平衡数据分布对模型性能的负面影响。</li>
<li>Dist Loss在图像和医疗数据集的测试中表现优异，达到了先进的效果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.15216">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-23e17600b50393b75e7062275d4a3d75.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0e42441625531801b0ca8db53946cdf8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a6b33ffef72f0152ca7ef1894df27945.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a4b1abb7af5ca6233459bcb94a6a1d1a.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Learning-General-Purpose-Biomedical-Volume-Representations-using-Randomized-Synthesis"><a href="#Learning-General-Purpose-Biomedical-Volume-Representations-using-Randomized-Synthesis" class="headerlink" title="Learning General-Purpose Biomedical Volume Representations using   Randomized Synthesis"></a>Learning General-Purpose Biomedical Volume Representations using   Randomized Synthesis</h2><p><strong>Authors:Neel Dey, Benjamin Billot, Hallee E. Wong, Clinton J. Wang, Mengwei Ren, P. Ellen Grant, Adrian V. Dalca, Polina Golland</strong></p>
<p>Current volumetric biomedical foundation models struggle to generalize as public 3D datasets are small and do not cover the broad diversity of medical procedures, conditions, anatomical regions, and imaging protocols. We address this by creating a representation learning method that instead anticipates strong domain shifts at training time itself. We first propose a data engine that synthesizes highly variable training samples that would enable generalization to new biomedical contexts. To then train a single 3D network for any voxel-level task, we develop a contrastive learning method that pretrains the network to be stable against nuisance imaging variation simulated by the data engine, a key inductive bias for generalization. This network’s features can be used as robust representations of input images for downstream tasks and its weights provide a strong, dataset-agnostic initialization for finetuning on new datasets. As a result, we set new standards across both multimodality registration and few-shot segmentation, a first for any 3D biomedical vision model, all without (pre-)training on any existing dataset of real images. </p>
<blockquote>
<p>当前体积生物医学基础模型难以推广，因为公共3D数据集规模较小，并未涵盖广泛的医疗程序、状况、解剖区域和成像协议多样性。我们通过创建一种表示学习方法来解决这个问题，该方法能够在训练时本身就能预测到强烈的领域偏移。我们首先提出一个数据引擎，合成高度可变的训练样本，以实现向新生物医学环境的推广。为了针对任何体素级任务训练单个3D网络，我们开发了一种对比学习方法，该方法对由数据引擎模拟的干扰成像变化进行预训练网络，这是实现泛化的关键归纳偏置。该网络的特征可作为下游任务的稳健表示，其权重为在新数据集上进行微调提供了强大且独立于数据集初始化。因此，我们在多模态注册和少镜头分割方面设定了新的标准，这是3D生物医学视觉模型的首创，且无需在任何现有真实图像数据集上进行（预）训练。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.02372v2">PDF</a> ICLR 2025: International Conference on Learning Representations. Code   and model weights available at <a target="_blank" rel="noopener" href="https://github.com/neel-dey/anatomix">https://github.com/neel-dey/anatomix</a>.   Keywords: synthetic data, representation learning, medical image analysis,   image registration, image segmentation</p>
<p><strong>Summary</strong></p>
<p>本文提出一种解决生物医学领域模型泛化能力弱的问题的方法。针对公共3D数据集规模小且不能覆盖广泛医疗程序、状况、解剖区域和成像协议的问题，文章提出了一种表示学习方法，该方法能够在训练时自身预期到强领域漂移。通过创建一个数据引擎来合成高度可变的训练样本，使模型能够推广到新的生物医学环境。同时，开发了一种对比学习方法来训练一个用于任何体素级别任务的单一3D网络。该网络能够在数据引擎模拟的干扰成像变化面前保持稳定，是一种关键的一般化归纳偏见。该网络的特征可用于下游任务的稳健表示，其权重为在新的数据集上进行微调提供了强大且独立于数据集的基础初始化。这种方法在跨模态注册和少样本分割方面都达到了新的标准，成为首个不需要在真实图像数据集上进行预训练的3D生物医学视觉模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前生物医学领域的体积模型泛化能力受限，主要由于公共3D数据集规模小且缺乏多样性。</li>
<li>提出一种数据引擎，能够合成高度可变的训练样本以改善模型的泛化能力。</li>
<li>采用对比学习方法训练单一3D网络，用于处理各种体素级别任务。</li>
<li>网络能在模拟的干扰成像变化面前保持稳定，这是提高模型泛化能力的重要归纳偏见。</li>
<li>该网络特征可作为下游任务的稳健表示，其权重为在新的数据集上进行微调提供了强大的初始化。</li>
<li>此方法在多模态注册和少样本分割方面达到新的标准。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.02372">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-c8ccbfddae24e54ee188d2aaa59b337c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8829a891f98535325355d33e316e5675.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c296a2b6e427afb423e81fadfd0733cc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-eff7484428edeb2c68f2c070333526f4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-62bc1811ed772a16504ff90639918a42.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1f2a62ad11af4a292fdc90828afd524c.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Data-adaptive-Differentially-Private-Prompt-Synthesis-for-In-Context-Learning"><a href="#Data-adaptive-Differentially-Private-Prompt-Synthesis-for-In-Context-Learning" class="headerlink" title="Data-adaptive Differentially Private Prompt Synthesis for In-Context   Learning"></a>Data-adaptive Differentially Private Prompt Synthesis for In-Context   Learning</h2><p><strong>Authors:Fengyu Gao, Ruida Zhou, Tianhao Wang, Cong Shen, Jing Yang</strong></p>
<p>Large Language Models (LLMs) rely on the contextual information embedded in examples&#x2F;demonstrations to perform in-context learning (ICL). To mitigate the risk of LLMs potentially leaking private information contained in examples in the prompt, we introduce a novel data-adaptive differentially private algorithm called AdaDPSyn to generate synthetic examples from the private dataset and then use these synthetic examples to perform ICL. The objective of AdaDPSyn is to adaptively adjust the noise level in the data synthesis mechanism according to the inherent statistical properties of the data, thereby preserving high ICL accuracy while maintaining formal differential privacy guarantees. A key innovation in AdaDPSyn is the Precision-Focused Iterative Radius Reduction technique, which dynamically refines the aggregation radius - the scope of data grouping for noise addition - based on patterns observed in data clustering, thereby minimizing the amount of additive noise. We conduct extensive experiments on standard benchmarks and compare AdaDPSyn with DP few-shot generation algorithm (Tang et al., 2023). The experiments demonstrate that AdaDPSyn not only outperforms DP few-shot generation, but also maintains high accuracy levels close to those of non-private baselines, providing an effective solution for ICL with privacy protection. </p>
<blockquote>
<p>大型语言模型（LLM）依赖于嵌入在示例&#x2F;演示中的上下文信息进行上下文学习（ICL）。为了减轻LLM可能在提示中的示例中泄露私人信息的风险，我们引入了一种新型的数据自适应差分隐私算法AdaDPSyn，从私有数据集中生成合成示例，然后使用这些合成示例进行ICL。AdaDPSyn的目标是根据数据的固有统计属性自适应地调整数据合成机制中的噪声水平，从而在保持正式差分隐私保证的同时，保持高ICL精度。AdaDPSyn的一个关键创新点是精准聚焦的迭代半径缩减技术，该技术根据在数据聚类中观察到的模式动态调整聚合半径（噪声添加的数据分组范围），从而最小化添加的噪声量。我们在标准基准测试上进行了大量实验，将AdaDPSyn与DP少样本生成算法（Tang等人，2023）进行了比较。实验表明，AdaDPSyn不仅优于DP少样本生成，而且保持接近非私有基准测试的高精度水平，为带有隐私保护的ICL提供了有效解决方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.12085v2">PDF</a> Accepted to ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>大型语言模型依赖示例中的上下文信息进行上下文学习。为缓解示例中可能泄露的私人信息风险，提出了一种新型的数据自适应差分隐私算法AdaDPSyn，用于从私有数据集中生成合成示例，并使用这些合成示例进行上下文学习。AdaDPSyn能自适应调整数据合成机制中的噪声水平，以适应数据的内在统计特性，从而在保持高上下文学习精度的同时，确保正式的差分隐私保证。其关键创新在于精准聚焦的迭代半径缩减技术，该技术根据数据聚类中观察到的模式动态调整聚合半径，从而最小化添加的噪声量。实验表明，AdaDPSyn不仅优于DP的少样本生成算法，而且保持接近非私有基准线的精度，为带有隐私保护的上下文学习提供了有效解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型依赖示例的上下文信息进行学习。</li>
<li>AdaDPSyn是一种数据自适应的差分隐私算法，用于生成合成示例以进行上下文学习。</li>
<li>AdaDPSyn能根据数据的统计特性自适应调整噪声水平。</li>
<li>AdaDPSyn采用精准聚焦的迭代半径缩减技术，动态调整聚合半径以最小化噪声。</li>
<li>AdaDPSyn在保持高上下文学习精度的同时，确保差分隐私。</li>
<li>与现有的DP少样本生成算法相比，AdaDPSyn表现出更好的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.12085">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c1b2004c529bbbecc06bc7838dd56d23.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0c01121ee1a009a8ec5922597e9a4204.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-68a5cf9d73f581ebfb6598032631ec27.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Snuffy-Efficient-Whole-Slide-Image-Classifier"><a href="#Snuffy-Efficient-Whole-Slide-Image-Classifier" class="headerlink" title="Snuffy: Efficient Whole Slide Image Classifier"></a>Snuffy: Efficient Whole Slide Image Classifier</h2><p><strong>Authors:Hossein Jafarinia, Alireza Alipanah, Danial Hamdi, Saeed Razavi, Nahal Mirzaie, Mohammad Hossein Rohban</strong></p>
<p>Whole Slide Image (WSI) classification with multiple instance learning (MIL) in digital pathology faces significant computational challenges. Current methods mostly rely on extensive self-supervised learning (SSL) for satisfactory performance, requiring long training periods and considerable computational resources. At the same time, no pre-training affects performance due to domain shifts from natural images to WSIs. We introduce Snuffy architecture, a novel MIL-pooling method based on sparse transformers that mitigates performance loss with limited pre-training and enables continual few-shot pre-training as a competitive option. Our sparsity pattern is tailored for pathology and is theoretically proven to be a universal approximator with the tightest probabilistic sharp bound on the number of layers for sparse transformers, to date. We demonstrate Snuffy’s effectiveness on CAMELYON16 and TCGA Lung cancer datasets, achieving superior WSI and patch-level accuracies. The code is available on <a target="_blank" rel="noopener" href="https://github.com/jafarinia/snuffy">https://github.com/jafarinia/snuffy</a>. </p>
<blockquote>
<p>在全数字病理学中，基于多实例学习（MIL）的全幻灯片图像（WSI）分类面临着巨大的计算挑战。当前的方法大多依赖于广泛的自监督学习（SSL）以获得令人满意的效果，这需要长时间的训练和大量的计算资源。同时，由于没有预训练会受到来自自然图像到WSI领域转变的影响。我们引入了Snuffy架构，这是一种基于稀疏变压器的新型MIL池方法，它通过有限的预训练减轻了性能损失，并能够实现持续的少量预训练作为一种具有竞争力的选择。我们的稀疏模式是针对病理学量身定制的，从理论上证明了它是迄今为止稀疏变压器层数最紧的概率尖锐界限的通用逼近器。我们在CAMELYON16和TCGA肺癌数据集上展示了Snuffy的有效性，实现了优越的全幻灯片图像和补丁级别的准确性。代码可在<a target="_blank" rel="noopener" href="https://github.com/jafarinia/snuffy%E4%B8%8A%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/jafarinia/snuffy上获得。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.08258v3">PDF</a> Accepted for ECCV 2024</p>
<p><strong>Summary</strong></p>
<p>本文介绍了基于稀疏变换器的新型MIL池化方法——Snuffy架构，该架构解决了数字病理学中全幻灯片图像（WSI）分类面临的挑战。传统的自监督学习方法需要大量时间和计算资源，而Snuffy架构通过引入稀疏变换器技术实现了更高效的训练过程，并具有出色的少样本预训练能力。实验结果表明，Snuffy架构在CAMELYON16和TCGA肺癌数据集上实现了更高的WSI和补丁级别的准确性。代码已发布在GitHub上。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Snuffy架构解决了数字病理学中WSI分类面临的计算挑战。</li>
<li>当前方法大多依赖大量自监督学习（SSL），训练时间长且计算资源消耗大。</li>
<li>Snuffy架构引入稀疏变换器技术，提高训练效率并具备出色的少样本预训练能力。</li>
<li>Snuffy架构实现了个性化稀疏模式，适用于病理学领域，且在稀疏变换器的层数上具有理论证明的最紧概率尖锐界。</li>
<li>在CAMELYON16和TCGA肺癌数据集上，Snuffy架构实现了较高的WSI和补丁级别准确性。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.08258">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-de715e8102c75e4c84c4d4abc58972b8.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Leveraging-Vision-Language-Models-for-Specialized-Agricultural-Tasks"><a href="#Leveraging-Vision-Language-Models-for-Specialized-Agricultural-Tasks" class="headerlink" title="Leveraging Vision Language Models for Specialized Agricultural Tasks"></a>Leveraging Vision Language Models for Specialized Agricultural Tasks</h2><p><strong>Authors:Muhammad Arbab Arshad, Talukder Zaki Jubery, Tirtho Roy, Rim Nassiri, Asheesh K. Singh, Arti Singh, Chinmay Hegde, Baskar Ganapathysubramanian, Aditya Balu, Adarsh Krishnamurthy, Soumik Sarkar</strong></p>
<p>As Vision Language Models (VLMs) become increasingly accessible to farmers and agricultural experts, there is a growing need to evaluate their potential in specialized tasks. We present AgEval, a comprehensive benchmark for assessing VLMs’ capabilities in plant stress phenotyping, offering a solution to the challenge of limited annotated data in agriculture. Our study explores how general-purpose VLMs can be leveraged for domain-specific tasks with only a few annotated examples, providing insights into their behavior and adaptability. AgEval encompasses 12 diverse plant stress phenotyping tasks, evaluating zero-shot and few-shot in-context learning performance of state-of-the-art models including Claude, GPT, Gemini, and LLaVA. Our results demonstrate VLMs’ rapid adaptability to specialized tasks, with the best-performing model showing an increase in F1 scores from 46.24% to 73.37% in 8-shot identification. To quantify performance disparities across classes, we introduce metrics such as the coefficient of variation (CV), revealing that VLMs’ training impacts classes differently, with CV ranging from 26.02% to 58.03%. We also find that strategic example selection enhances model reliability, with exact category examples improving F1 scores by 15.38% on average. AgEval establishes a framework for assessing VLMs in agricultural applications, offering valuable benchmarks for future evaluations. Our findings suggest that VLMs, with minimal few-shot examples, show promise as a viable alternative to traditional specialized models in plant stress phenotyping, while also highlighting areas for further refinement. Results and benchmark details are available at: <a target="_blank" rel="noopener" href="https://github.com/arbab-ml/AgEval">https://github.com/arbab-ml/AgEval</a> </p>
<blockquote>
<p>随着视觉语言模型（VLMs）越来越被农民和农业专家所接触，评估它们在专业任务中的潜力变得日益重要。我们推出了AgEval，这是一个用于评估VLMs在植物胁迫表型分析能力的综合基准测试，为解决农业中标注数据有限的问题提供了解决方案。我们的研究探讨了通用VLMs如何利用少量的标注示例来完成特定领域的任务，深入了解其行为和适应性。AgEval涵盖了12个多样化的植物胁迫表型分析任务，评估了包括Claude、GPT、Gemini和LLaVA等最新模型的无监督和少量监督的上下文学习能力表现。我们的结果表明，VLMs能够迅速适应专业任务，最佳性能的模型在8次拍摄的识别中F1得分从46.24%提高到73.37%。为了量化各类之间的性能差异，我们引入了变异系数（CV）这一指标，揭示VLMs的训练对不同类别的影响不同，CV范围从26.02%到58.03%。我们还发现，战略性的例子选择可以提高模型的可靠性，精确类别的例子平均提高了F1得分15.38%。AgEval建立了评估VLMs在农业应用中的基准测试框架，为未来的评估提供了有价值的参考。我们的研究结果表明，利用最少的几次拍摄示例，VLMs在植物胁迫表型分析中显示出作为传统专业模型的可行替代方案的潜力，同时也突出了需要进一步改进的领域。结果和基准测试详情可在：<a target="_blank" rel="noopener" href="https://github.com/arbab-ml/AgEval%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/arbab-ml/AgEval找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.19617v2">PDF</a> Published at WACV 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了Vision Language Models（VLMs）在农业领域的潜力评估。为评估VLMs在植物压力表现分析方面的能力，提出了AgEval这一综合性基准测试。研究探讨了通用VLMs如何在只需少量标注样本的情况下，就能适应特定任务。AgEval涵盖了多样化的植物压力表现分析任务，评估了最前沿模型如Claude、GPT等的零样本和少样本上下文学习能力表现。结果显示VLMs能快速适应专项任务，最佳模型在8次射击识别中的F1得分从46.24%提高到73.37%。同时，通过引入变异系数等度量标准，揭示了不同类别间性能差异的问题。研究还发现，通过选择策略性示例能提高模型可靠性。AgEval为评估VLMs在农业应用中的表现提供了宝贵基准，表明VLMs在植物压力表现分析方面显示出巨大的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision Language Models (VLMs) 在农业领域的植物压力表现分析中具有潜力评估价值。</li>
<li>AgEval是评估VLMs在植物压力表现分析方面的综合性基准测试。</li>
<li>VLMs可以在只需少量标注样本的情况下适应特定任务。</li>
<li>AgEval涵盖了多样化的植物压力表现分析任务，并评估了多种最前沿模型的表现。</li>
<li>VLMs能快速适应专项任务，并通过战略示例选择提高模型可靠性。</li>
<li>不同类别间存在性能差异，引入度量标准如变异系数以量化这种差异。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.19617">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d15cc82c7f8b9f1758f5abaf84baf770.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a8022a7665b2e1652f4ad1827f07c5fa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-28033b52ca1647158f18a81cabcc26b1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3f340833d5f714f8e173f335ab2ad99b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cebeeb0a109b3fec821d56d1b20e1d79.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f710258232606af9b1e3a3d92bf032d.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-05/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-05/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-05/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-ae67c44cc8589d871fe1d536da07d97a.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-03-05  DynRefer Delving into Region-level Multimodal Tasks via Dynamic   Resolution
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-05
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-05/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-3de82161d67aeb20a744e490e5639f7c.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-03-05  FACT-AUDIT An Adaptive Multi-Agent Framework for Dynamic Fact-Checking   Evaluation of Large Language Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-05
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">17124.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
