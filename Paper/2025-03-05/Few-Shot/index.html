<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-05  Learning to Learn Weight Generation via Trajectory Diffusion">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-359b7d32c8d2e3115557c0e7d2f42cbd.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-05
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    32 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-05-æ›´æ–°"><a href="#2025-03-05-æ›´æ–°" class="headerlink" title="2025-03-05 æ›´æ–°"></a>2025-03-05 æ›´æ–°</h1><h2 id="Learning-to-Learn-Weight-Generation-via-Trajectory-Diffusion"><a href="#Learning-to-Learn-Weight-Generation-via-Trajectory-Diffusion" class="headerlink" title="Learning to Learn Weight Generation via Trajectory Diffusion"></a>Learning to Learn Weight Generation via Trajectory Diffusion</h2><p><strong>Authors:Yunchuan Guan, Yu Liu, Ke Zhou, Zhiqi Shen, Serge Belongie, Jenq-Neng Hwang, Lei Li</strong></p>
<p>Diffusion-based algorithms have emerged as promising techniques for weight generation, particularly in scenarios like multi-task learning that require frequent weight updates. However, existing solutions suffer from limited cross-task transferability. In addition, they only utilize optimal weights as training samples, ignoring the value of other weights in the optimization process. To address these issues, we propose Lt-Di, which integrates the diffusion algorithm with meta-learning to generate weights for unseen tasks. Furthermore, we extend the vanilla diffusion algorithm into a trajectory diffusion algorithm to utilize other weights along the optimization trajectory. Trajectory diffusion decomposes the entire diffusion chain into multiple shorter ones, improving training and inference efficiency. We analyze the convergence properties of the weight generation paradigm and improve convergence efficiency without additional time overhead. Our experiments demonstrate Lt-Diâ€™s higher accuracy while reducing computational overhead across various tasks, including zero-shot and few-shot learning, multi-domain generalization, and large-scale language model fine-tuning.Our code is released at <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/Lt-Di-0E51">https://anonymous.4open.science/r/Lt-Di-0E51</a>. </p>
<blockquote>
<p>åŸºäºæ‰©æ•£çš„ç®—æ³•å·²æˆä¸ºæƒé‡ç”Ÿæˆçš„æœ‰å‰é€”çš„æŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦é¢‘ç¹æƒé‡æ›´æ–°çš„å¤šä»»åŠ¡å­¦ä¹ ç­‰åœºæ™¯ä¸­ã€‚ç„¶è€Œï¼Œç°æœ‰è§£å†³æ–¹æ¡ˆå­˜åœ¨è·¨ä»»åŠ¡è¿ç§»èƒ½åŠ›æœ‰é™çš„ä¸è¶³ã€‚æ­¤å¤–ï¼Œå®ƒä»¬ä»…åˆ©ç”¨æœ€ä¼˜æƒé‡ä½œä¸ºè®­ç»ƒæ ·æœ¬ï¼Œå¿½ç•¥äº†ä¼˜åŒ–è¿‡ç¨‹ä¸­å…¶ä»–æƒé‡çš„ä»·å€¼ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Lt-Diï¼Œå®ƒå°†æ‰©æ•£ç®—æ³•ä¸å…ƒå­¦ä¹ ç›¸ç»“åˆï¼Œä»¥ç”Ÿæˆæœªè§ä»»åŠ¡çš„æƒé‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†æ™®é€šçš„æ‰©æ•£ç®—æ³•æ‰©å±•ä¸ºè½¨è¿¹æ‰©æ•£ç®—æ³•ï¼Œä»¥åˆ©ç”¨ä¼˜åŒ–è½¨è¿¹ä¸Šçš„å…¶ä»–æƒé‡ã€‚è½¨è¿¹æ‰©æ•£å°†æ•´ä¸ªæ‰©æ•£é“¾åˆ†è§£ä¸ºå¤šä¸ªè¾ƒçŸ­çš„é“¾ï¼Œæé«˜äº†è®­ç»ƒå’Œæ¨ç†æ•ˆç‡ã€‚æˆ‘ä»¬åˆ†æäº†æƒé‡ç”ŸæˆèŒƒå¼çš„æ”¶æ•›å±æ€§ï¼Œæé«˜äº†æ”¶æ•›æ•ˆç‡ï¼Œè€Œæ²¡æœ‰å¢åŠ é¢å¤–çš„æ—¶é—´å¼€é”€ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒLt-Diåœ¨å„ç§ä»»åŠ¡ä¸­å…·æœ‰è¾ƒé«˜çš„å‡†ç¡®æ€§ï¼ŒåŒæ—¶å‡å°‘äº†è®¡ç®—å¼€é”€ï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬å­¦ä¹ ã€å¤šåŸŸæ³›åŒ–ä»¥åŠå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„å¾®è°ƒã€‚æˆ‘ä»¬çš„ä»£ç å‘å¸ƒåœ¨ï¼š<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/Lt-Di-0E51%E3%80%82">https://anonymous.4open.science/r/Lt-Di-0E51ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01117v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£ç®—æ³•ç»“åˆå…ƒå­¦ä¹ ç”Ÿæˆæƒé‡ï¼Œè§£å†³å¤šä»»åŠ¡å­¦ä¹ ä¸­çš„è·¨ä»»åŠ¡è¿ç§»é—®é¢˜ï¼Œå¹¶æ‰©å±•ä¸ºè½¨è¿¹æ‰©æ•£ç®—æ³•ï¼Œåˆ©ç”¨ä¼˜åŒ–è¿‡ç¨‹ä¸­çš„å…¶ä»–æƒé‡ã€‚åˆ†ææƒé‡ç”ŸæˆèŒƒå¼çš„æ”¶æ•›æ€§è´¨ï¼Œæé«˜æ”¶æ•›æ•ˆç‡ä¸”æ— éœ€é¢å¤–æ—¶é—´å¼€é”€ã€‚å®éªŒè¯æ˜ï¼ŒLt-Diåœ¨å¤šç§ä»»åŠ¡ä¸Šè¡¨ç°æ›´ç²¾ç¡®ï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬å­¦ä¹ ã€è·¨åŸŸæ³›åŒ–å’Œå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹å¾®è°ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£ç®—æ³•åœ¨å¤šä»»åŠ¡å­¦ä¹ ä¸­å±•ç°å‡ºç”Ÿæˆæƒé‡çš„æ½œåŠ›ã€‚</li>
<li>ç°æœ‰è§£å†³æ–¹æ¡ˆå­˜åœ¨è·¨ä»»åŠ¡è¿ç§»çš„å±€é™æ€§ã€‚</li>
<li>Lt-Diç»“åˆæ‰©æ•£ç®—æ³•ä¸å…ƒå­¦ä¹ ï¼Œç”Ÿæˆæœªè§ä»»åŠ¡çš„æƒé‡ã€‚</li>
<li>è½¨è¿¹æ‰©æ•£ç®—æ³•åˆ©ç”¨ä¼˜åŒ–è¿‡ç¨‹ä¸­çš„å…¶ä»–æƒé‡ã€‚</li>
<li>Lt-Diæé«˜äº†è®­ç»ƒæ•ˆç‡å’Œæ¨ç†æ•ˆç‡ã€‚</li>
<li>Lt-Diæé«˜äº†æƒé‡ç”ŸæˆèŒƒå¼çš„æ”¶æ•›æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01117">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c754ede68d43ec54006e55d61d4fb4dd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a166195fcd2db168a50d21d1d59850b3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e4c91ab2b9694eb4eb2032630d28a4d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-629046bf796e392d79c1b06bc63c7a47.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Text-driven-Adaptation-of-Foundation-Models-for-Few-shot-Surgical-Workflow-Analysis"><a href="#Text-driven-Adaptation-of-Foundation-Models-for-Few-shot-Surgical-Workflow-Analysis" class="headerlink" title="Text-driven Adaptation of Foundation Models for Few-shot Surgical   Workflow Analysis"></a>Text-driven Adaptation of Foundation Models for Few-shot Surgical   Workflow Analysis</h2><p><strong>Authors:Tingxuan Chen, Kun Yuan, Vinkle Srivastav, Nassir Navab, Nicolas Padoy</strong></p>
<p>Purpose: Surgical workflow analysis is crucial for improving surgical efficiency and safety. However, previous studies rely heavily on large-scale annotated datasets, posing challenges in cost, scalability, and reliance on expert annotations. To address this, we propose Surg-FTDA (Few-shot Text-driven Adaptation), designed to handle various surgical workflow analysis tasks with minimal paired image-label data.   Methods: Our approach has two key components. First, Few-shot selection-based modality alignment selects a small subset of images and aligns their embeddings with text embeddings from the downstream task, bridging the modality gap. Second, Text-driven adaptation leverages only text data to train a decoder, eliminating the need for paired image-text data. This decoder is then applied to aligned image embeddings, enabling image-related tasks without explicit image-text pairs.   Results: We evaluate our approach to generative tasks (image captioning) and discriminative tasks (triplet recognition and phase recognition). Results show that Surg-FTDA outperforms baselines and generalizes well across downstream tasks.   Conclusion: We propose a text-driven adaptation approach that mitigates the modality gap and handles multiple downstream tasks in surgical workflow analysis, with minimal reliance on large annotated datasets. The code and dataset will be released in <a target="_blank" rel="noopener" href="https://github.com/CAMMA-public/Surg-FTDA">https://github.com/CAMMA-public/Surg-FTDA</a> </p>
<blockquote>
<p>ç›®çš„ï¼šæ‰‹æœ¯å·¥ä½œæµç¨‹åˆ†æå¯¹äºæé«˜æ‰‹æœ¯æ•ˆç‡å’Œå®‰å…¨æ€§è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œä¹‹å‰çš„ç ”ç©¶ä¸¥é‡ä¾èµ–äºå¤§è§„æ¨¡æ ‡æ³¨æ•°æ®é›†ï¼Œè¿™å¸¦æ¥äº†æˆæœ¬ã€å¯æ‰©å±•æ€§å’Œå¯¹ä¸“å®¶æ ‡æ³¨çš„ä¾èµ–ç­‰æ–¹é¢çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Surg-FTDAï¼ˆFew-shotæ–‡æœ¬é©±åŠ¨é€‚åº”ï¼‰æ–¹æ³•ï¼Œæ—¨åœ¨ç”¨æœ€å°‘é…å¯¹çš„å›¾åƒæ ‡ç­¾æ•°æ®æ¥å¤„ç†å„ç§æ‰‹æœ¯å·¥ä½œæµç¨‹åˆ†æä»»åŠ¡ã€‚</p>
</blockquote>
<p>æ–¹æ³•ï¼šæˆ‘ä»¬çš„æ–¹æ³•æœ‰ä¸¤ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ã€‚é¦–å…ˆï¼ŒåŸºäºFew-shoté€‰æ‹©çš„æ¨¡æ€å¯¹é½é€‰æ‹©ä¸€å°éƒ¨åˆ†å›¾åƒï¼Œå¹¶å°†å®ƒä»¬çš„åµŒå…¥ä¸ä¸‹æ¸¸ä»»åŠ¡çš„æ–‡æœ¬åµŒå…¥å¯¹é½ï¼Œä»è€Œå¼¥åˆäº†æ¨¡æ€ä¹‹é—´çš„å·®è·ã€‚å…¶æ¬¡ï¼Œæ–‡æœ¬é©±åŠ¨é€‚åº”ä»…åˆ©ç”¨æ–‡æœ¬æ•°æ®æ¥è®­ç»ƒè§£ç å™¨ï¼Œä»è€Œæ¶ˆé™¤äº†å¯¹é…å¯¹å›¾åƒæ–‡æœ¬æ•°æ®çš„éœ€æ±‚ã€‚ç„¶åï¼Œå°†è¿™ä¸ªè§£ç å™¨åº”ç”¨äºå¯¹é½çš„å›¾åƒåµŒå…¥ï¼Œä»è€Œèƒ½å¤Ÿåœ¨æ²¡æœ‰æ˜ç¡®çš„å›¾åƒæ–‡æœ¬å¯¹çš„æƒ…å†µä¸‹æ‰§è¡Œå›¾åƒç›¸å…³ä»»åŠ¡ã€‚</p>
<p>ç»“æœï¼šæˆ‘ä»¬å¯¹ç”Ÿæˆä»»åŠ¡ï¼ˆå›¾åƒå­—å¹•ï¼‰å’Œé‰´åˆ«ä»»åŠ¡ï¼ˆä¸‰å…ƒç»„è¯†åˆ«å’Œé˜¶æ®µè¯†åˆ«ï¼‰è¿›è¡Œäº†è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼ŒSurg-FTDAä¼˜äºåŸºçº¿ï¼Œå¹¶åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09555v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºSurg-FTDAï¼ˆFew-shotæ–‡æœ¬é©±åŠ¨é€‚åº”ï¼‰çš„æ–¹æ³•ï¼Œç”¨äºæ”¹è¿›æ‰‹æœ¯æµç¨‹åˆ†æä»»åŠ¡çš„æ•ˆç‡å’Œå®‰å…¨æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡å°‘é‡é…å¯¹å›¾åƒæ ‡ç­¾æ•°æ®ï¼Œè§£å†³å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®é›†å¸¦æ¥çš„æˆæœ¬ã€å¯æ‰©å±•æ€§å’Œä¾èµ–ä¸“å®¶æ ‡æ³¨çš„é—®é¢˜ã€‚é€šè¿‡æ–‡æœ¬é©±åŠ¨é€‚åº”ç­–ç•¥ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåˆ©ç”¨æ–‡æœ¬æ•°æ®è®­ç»ƒè§£ç å™¨ï¼Œæ— éœ€é…å¯¹å›¾åƒæ–‡æœ¬æ•°æ®å³å¯å®Œæˆå›¾åƒç›¸å…³ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSurg-FTDAåœ¨ç”Ÿæˆä»»åŠ¡ï¼ˆå›¾åƒæè¿°ï¼‰å’Œåˆ¤åˆ«ä»»åŠ¡ï¼ˆä¸‰å…ƒç»„è¯†åˆ«å’Œé˜¶æ®µè¯†åˆ«ï¼‰ä¸­å‡è¡¨ç°ä¼˜å¼‚ï¼Œå¹¶èƒ½å¾ˆå¥½åœ°åº”ç”¨äºä¸åŒä¸‹æ¸¸ä»»åŠ¡ã€‚ä»£ç å’Œæ•°æ®é›†å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/CAMMA-public/Surg-FTDA%E5%85%AC%E5%BC%80%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/CAMMA-public/Surg-FTDAå…¬å¼€å‘å¸ƒã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Surg-FTDAæ—¨åœ¨è§£å†³æ‰‹æœ¯æµç¨‹åˆ†æä¸­çš„æ•ˆç‡å’Œå®‰å…¨é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯å¯¹æ•°æ®æ ‡æ³¨çš„æŒ‘æˆ˜æ€§ã€‚</li>
<li>æ–¹æ³•åŒ…æ‹¬ä¸¤ä¸ªå…³é”®éƒ¨åˆ†ï¼šåŸºäºå°‘æ•°é€‰æ‹©çš„æ¨¡æ€å¯¹é½å’Œæ–‡æœ¬é©±åŠ¨é€‚åº”ç­–ç•¥ã€‚</li>
<li>é€šè¿‡é€‰æ‹©å°‘é‡å›¾åƒå¹¶ä¸å…¶ä¸‹æ¸¸ä»»åŠ¡çš„æ–‡æœ¬åµŒå…¥è¿›è¡Œå¯¹é½ï¼Œç¼©å°äº†æ¨¡æ€å·®è·ã€‚</li>
<li>ä»…ä½¿ç”¨æ–‡æœ¬æ•°æ®è®­ç»ƒè§£ç å™¨ï¼Œæ— éœ€é…å¯¹å›¾åƒæ–‡æœ¬æ•°æ®å³å¯å®Œæˆå›¾åƒç›¸å…³ä»»åŠ¡ã€‚</li>
<li>Surg-FTDAåœ¨ç”Ÿæˆä»»åŠ¡å’Œåˆ¤åˆ«ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚</li>
<li>ä»£ç å’Œæ•°æ®é›†å°†åœ¨å…¬å…±å­˜å‚¨åº“ä¸­è¿›è¡Œå…±äº«ä»¥ä¾›è¿›ä¸€æ­¥ç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09555">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-22d55c0c82e5d703b349f9be11191c90.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-741f3496eb39e29b1d523f763f7297d8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-964423334114aa83cdedea9c263296c6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d6cd0dd712f9125aeed779a98e5312ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c97290226d910425b794303da5958fdd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f7713590a82ff60cd156881b244c56e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-359b7d32c8d2e3115557c0e7d2f42cbd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6de8039cc24d7d8ab1286ebf897e9b70.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2cbc91f323f3004945d059d79884d39f.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="On-the-Generalization-and-Adaptation-Ability-of-Machine-Generated-Text-Detectors-in-Academic-Writing"><a href="#On-the-Generalization-and-Adaptation-Ability-of-Machine-Generated-Text-Detectors-in-Academic-Writing" class="headerlink" title="On the Generalization and Adaptation Ability of Machine-Generated Text   Detectors in Academic Writing"></a>On the Generalization and Adaptation Ability of Machine-Generated Text   Detectors in Academic Writing</h2><p><strong>Authors:Yule Liu, Zhiyuan Zhong, Yifan Liao, Zhen Sun, Jingyi Zheng, Jiaheng Wei, Qingyuan Gong, Fenghua Tong, Yang Chen, Yang Zhang, Xinlei He</strong></p>
<p>The rising popularity of large language models (LLMs) has raised concerns about machine-generated text (MGT), particularly in academic settings, where issues like plagiarism and misinformation are prevalent. As a result, developing a highly generalizable and adaptable MGT detection system has become an urgent priority. Given that LLMs are most commonly misused in academic writing, this work investigates the generalization and adaptation capabilities of MGT detectors in three key aspects specific to academic writing: First, we construct MGT-Acedemic, a large-scale dataset comprising over 336M tokens and 749K samples. MGT-Acedemic focuses on academic writing, featuring human-written texts (HWTs) and MGTs across STEM, Humanities, and Social Sciences, paired with an extensible code framework for efficient benchmarking. Second, we benchmark the performance of various detectors for binary classification and attribution tasks in both in-domain and cross-domain settings. This benchmark reveals the often-overlooked challenges of attribution tasks. Third, we introduce a novel attribution task where models have to adapt to new classes over time without (or with very limited) access to prior training data in both few-shot and many-shot scenarios. We implement eight different adapting techniques to improve the performance and highlight the inherent complexity of the task. Our findings provide insights into the generalization and adaptation ability of MGT detectors across diverse scenarios and lay the foundation for building robust, adaptive detection systems. The code framework is available at <a target="_blank" rel="noopener" href="https://github.com/Y-L-LIU/MGTBench-2.0">https://github.com/Y-L-LIU/MGTBench-2.0</a>. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ—¥ç›Šæ™®åŠï¼Œäººä»¬å¼€å§‹å…³æ³¨æœºå™¨ç”Ÿæˆæ–‡æœ¬ï¼ˆMGTï¼‰çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å­¦æœ¯ç¯å¢ƒä¸­ï¼ŒæŠ„è¢­å’Œè¯¯ä¿¡æ¯ç­‰é—®é¢˜çš„æ™®éå­˜åœ¨ã€‚å› æ­¤ï¼Œå¼€å‘ä¸€ä¸ªé«˜åº¦é€šç”¨åŒ–å’Œé€‚åº”æ€§çš„MGTæ£€æµ‹ç³»ç»Ÿå·²æˆä¸ºä¸€é¡¹ç´§è¿«çš„ä»»åŠ¡ã€‚é‰´äºLLMåœ¨å­¦æœ¯å†™ä½œä¸­æœ€å¸¸è¢«æ»¥ç”¨ï¼Œæœ¬ç ”ç©¶ä»å­¦æœ¯å†™ä½œçš„è§’åº¦å‡ºå‘ï¼Œæ¢è®¨äº†MGTæ£€æµ‹å™¨åœ¨ä¸‰ä¸ªæ–¹é¢çš„é€šç”¨æ€§å’Œé€‚åº”æ€§èƒ½åŠ›ï¼šé¦–å…ˆï¼Œæˆ‘ä»¬æ„å»ºäº†MGT-Acedemicæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«è¶…è¿‡3.36äº¿ä¸ªæ ‡è®°å’Œ74.9ä¸‡ä¸ªæ ·æœ¬çš„å¤§è§„æ¨¡æ•°æ®é›†ã€‚MGT-Acedemicä¸“æ³¨äºå­¦æœ¯å†™ä½œé¢†åŸŸï¼Œæ¶µç›–äº†ç§‘å­¦ã€å·¥ç¨‹ã€æ–‡å­¦å’Œäººæ–‡å­¦ç§‘çš„æ–‡æœ¬ï¼ˆäººç±»åŸåˆ›æ–‡æœ¬ï¼‰å’ŒMGTï¼Œä»¥åŠä¸å¯ä¼¸ç¼©ä»£ç æ¡†æ¶é…åˆè¿›è¡Œé«˜æ•ˆåŸºå‡†æµ‹è¯•çš„åŠŸèƒ½ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¯¹å¤šç§æ£€æµ‹å™¨è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œè¿™äº›æ£€æµ‹å™¨è¦åœ¨æœ¬åœ°åŸŸå’Œè·¨åœ°åŸŸè®¾ç½®ä¸­è¿›è¡ŒäºŒè¿›åˆ¶åˆ†ç±»å’Œå½’å±ä»»åŠ¡åˆ†ç±»ä»»åŠ¡ã€‚è¿™ä¸ªåŸºå‡†æµ‹è¯•æ­ç¤ºå‡ºå½’å±ä»»åŠ¡åˆ†ç±»æŒ‘æˆ˜è¢«ç»å¸¸è¢«å¿½è§†ã€‚æœ€åï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€é¡¹æ–°é¢–çš„å½’å±ä»»åŠ¡åˆ†ç±»æµ‹è¯•ï¼Œæµ‹è¯•æ¨¡å‹åœ¨æ—¶é—´æ¨ç§»ä¸­å¯¹æ–°ç±»åˆ«çš„é€‚åº”åŠ›åœ¨æ²¡æœ‰æˆ–è€…å¾ˆå°‘æœ‰å…ˆå‰çš„è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹å¯¹è¿™ä¸¤ç§æƒ…æ™¯è¿›è¡ŒçŸ­æ—¶é•¿ï¼ˆå°‘æ ·æœ¬å’Œå¤šæ ·æœ¬ï¼‰è¿›è¡Œæ¨æ¼”é¢„æµ‹çš„èƒ½åŠ›ã€‚æˆ‘ä»¬å®æ–½äº†å…«ç§ä¸åŒçš„é€‚åº”æŠ€æœ¯æ¥æé«˜æ€§èƒ½å¹¶çªå‡ºä»»åŠ¡çš„å›ºæœ‰å¤æ‚æ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºæ·±å…¥äº†è§£MGTæ£€æµ‹å™¨åœ¨ä¸åŒåœºæ™¯ä¸‹çš„é€šç”¨æ€§å’Œé€‚åº”èƒ½åŠ›æä¾›äº†è§è§£ï¼Œå¹¶ä¸ºæ„å»ºç¨³å¥çš„é€‚åº”æ€§æ£€æµ‹ç³»ç»Ÿå¥ å®šäº†åŸºç¡€ã€‚ä»£ç æ¡†æ¶å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Y-L-LIU/MGTBench-2.0%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Y-L-LIU/MGTBench-2.0è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17242v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å…³æ³¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å­¦æœ¯å†™ä½œä¸­çš„æ»¥ç”¨é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯æœºå™¨ç”Ÿæˆæ–‡æœ¬ï¼ˆMGTï¼‰çš„æ£€æµ‹ã€‚ç ”ç©¶æ„å»ºäº†é’ˆå¯¹å­¦æœ¯å†™ä½œçš„MGT-Acedemicæ•°æ®é›†ï¼Œå¯¹å¤šç§æ£€æµ‹å™¨è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°çš„å½’å±ä»»åŠ¡ã€‚ç ”ç©¶å‘ç°æ£€æµ‹å™¨åœ¨ä¸åŒåœºæ™¯ä¸‹çš„é€šç”¨æ€§å’Œé€‚åº”æ€§æŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†å¤šç§é€‚åº”æŠ€æœ¯ä»¥æé«˜æ€§èƒ½ã€‚è¿™ä¸ºæ„å»ºç¨³å¥ã€è‡ªé€‚åº”çš„æ£€æµ‹ç³»ç»Ÿæä¾›äº†è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å­¦æœ¯å†™ä½œä¸­çš„æ»¥ç”¨å¼•å‘äº†å¯¹æœºå™¨ç”Ÿæˆæ–‡æœ¬ï¼ˆMGTï¼‰çš„å…³æ³¨ï¼Œç‰¹åˆ«æ˜¯åœ¨å­˜åœ¨æŠ„è¢­å’Œè¯¯å¯¼ä¿¡æ¯é—®é¢˜çš„å­¦æœ¯ç¯å¢ƒä¸­ã€‚</li>
<li>æ„å»ºäº†ä¸“æ³¨äºå­¦æœ¯å†™ä½œçš„MGT-Acedemicæ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡3.36äº¿ä¸ªæ ‡è®°å’Œ74.9ä¸‡ä¸ªæ ·æœ¬ï¼Œæ¶µç›–STEMã€äººæ–‡å’Œç¤¾ä¼šç§‘å­¦é¢†åŸŸçš„äººç±»æ’°å†™æ–‡æœ¬ï¼ˆHWTsï¼‰å’ŒMGTsã€‚</li>
<li>å¯¹å¤šç§æ£€æµ‹å™¨è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬äºŒè¿›åˆ¶åˆ†ç±»å’Œå½’å±ä»»åŠ¡ï¼Œåœ¨å†…éƒ¨å’Œè·¨é¢†åŸŸç¯å¢ƒä¸­å‡è¡¨ç°å‡ºæŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„å½’å±ä»»åŠ¡ï¼Œè¦æ±‚æ¨¡å‹åœ¨å°‘é‡æˆ–æ²¡æœ‰å…ˆå‰è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹ï¼Œéšç€æ—¶é—´çš„æ¨ç§»é€‚åº”æ–°ç±»åˆ«ã€‚</li>
<li>å®æ–½å…«ç§ä¸åŒçš„é€‚åº”æŠ€æœ¯ä»¥æé«˜æ€§èƒ½ï¼Œçªæ˜¾äº†ä»»åŠ¡çš„å›ºæœ‰å¤æ‚æ€§ã€‚</li>
<li>ç ”ç©¶ç»“æœæä¾›äº†å…³äºMGTæ£€æµ‹å™¨åœ¨ä¸åŒåœºæ™¯ä¸‹çš„é€šç”¨æ€§å’Œé€‚åº”èƒ½åŠ›çš„è§è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17242">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c4fcb6e8d4659cf262685b104d024637.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a61840c1d461c9fffd517feb8b09f349.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-09ae46c2c5a6a6b0a8c6e3c19ec85db8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c04ec85d95acf51bd29207e253b51d7b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-468fb30c0dfecd986aeb542c11d1d4a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19bc05afa534f1a1c4cf58276c5170a8.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Dist-Loss-Enhancing-Regression-in-Few-Shot-Region-through-Distribution-Distance-Constraint"><a href="#Dist-Loss-Enhancing-Regression-in-Few-Shot-Region-through-Distribution-Distance-Constraint" class="headerlink" title="Dist Loss: Enhancing Regression in Few-Shot Region through Distribution   Distance Constraint"></a>Dist Loss: Enhancing Regression in Few-Shot Region through Distribution   Distance Constraint</h2><p><strong>Authors:Guangkun Nie, Gongzheng Tang, Shenda Hong</strong></p>
<p>Imbalanced data distributions are prevalent in real-world scenarios, posing significant challenges in both imbalanced classification and imbalanced regression tasks. They often cause deep learning models to overfit in areas of high sample density (many-shot regions) while underperforming in areas of low sample density (few-shot regions). This characteristic restricts the utility of deep learning models in various sectors, notably healthcare, where areas with few-shot data hold greater clinical relevance. While recent studies have shown the benefits of incorporating distribution information in imbalanced classification tasks, such strategies are rarely explored in imbalanced regression. In this paper, we address this issue by introducing a novel loss function, termed Dist Loss, designed to minimize the distribution distance between the modelâ€™s predictions and the target labels in a differentiable manner, effectively integrating distribution information into model training. Dist Loss enables deep learning models to regularize their output distribution during training, effectively enhancing their focus on few-shot regions. We have conducted extensive experiments across three datasets spanning computer vision and healthcare: IMDB-WIKI-DIR, AgeDB-DIR, and ECG-Ka-DIR. The results demonstrate that Dist Loss effectively mitigates the negative impact of imbalanced data distribution on model performance, achieving state-of-the-art results in sparse data regions. Furthermore, Dist Loss is easy to integrate, complementing existing methods. </p>
<blockquote>
<p>ä¸å¹³è¡¡æ•°æ®åˆ†å¸ƒåœ¨å®é™…åœºæ™¯ä¸­æ™®éå­˜åœ¨ï¼Œç»™ä¸å¹³è¡¡åˆ†ç±»å’Œä¸å¹³è¡¡å›å½’ä»»åŠ¡å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚å®ƒä»¬ç»å¸¸å¯¼è‡´æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨é«˜æ ·æœ¬å¯†åº¦åŒºåŸŸï¼ˆå¤šé•œå¤´åŒºåŸŸï¼‰è¿‡æ‹Ÿåˆï¼Œè€Œåœ¨ä½æ ·æœ¬å¯†åº¦åŒºåŸŸï¼ˆå°æ ·æœ¬åŒºåŸŸï¼‰è¡¨ç°ä¸ä½³ã€‚è¿™ç§ç‰¹æ€§é™åˆ¶äº†æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨å„ä¸ªé¢†åŸŸçš„å®ç”¨æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»ç–—ä¿å¥é¢†åŸŸï¼Œå°æ ·æœ¬æ•°æ®åŒºåŸŸå…·æœ‰æ›´å¤§çš„ä¸´åºŠæ„ä¹‰ã€‚è™½ç„¶æœ€è¿‘çš„ç ”ç©¶å·²ç»æ˜¾ç¤ºäº†å°†åˆ†å¸ƒä¿¡æ¯èå…¥ä¸å¹³è¡¡åˆ†ç±»ä»»åŠ¡ä¸­çš„å¥½å¤„ï¼Œä½†åœ¨ä¸å¹³è¡¡å›å½’ä¸­å¾ˆå°‘æ¢ç´¢æ­¤ç±»ç­–ç•¥ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å¼•å…¥ä¸€ç§æ–°å‹æŸå¤±å‡½æ•°æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç§°ä¸ºDist Lossï¼Œæ—¨åœ¨ä»¥å¯åŒºåˆ†çš„æ–¹å¼æœ€å°åŒ–æ¨¡å‹é¢„æµ‹ä¸ç›®æ ‡æ ‡ç­¾ä¹‹é—´çš„åˆ†å¸ƒè·ç¦»ï¼Œæœ‰æ•ˆåœ°å°†åˆ†å¸ƒä¿¡æ¯èå…¥æ¨¡å‹è®­ç»ƒã€‚Dist Lossä½¿æ·±åº¦å­¦ä¹ æ¨¡å‹èƒ½å¤Ÿåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è§„èŒƒå…¶è¾“å‡ºåˆ†å¸ƒï¼Œä»è€Œæœ‰æ•ˆæé«˜å®ƒä»¬å¯¹å°æ ·æœ¬åŒºåŸŸçš„å…³æ³¨ã€‚æˆ‘ä»¬åœ¨è·¨è¶Šè®¡ç®—æœºè§†è§‰å’ŒåŒ»ç–—ä¿å¥çš„ä¸‰ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒï¼šIMDB-WIKI-DIRã€AgeDB-DIRå’ŒECG-Ka-DIRã€‚ç»“æœè¡¨æ˜ï¼ŒDist Lossæœ‰æ•ˆåœ°å‡è½»äº†ä¸å¹³è¡¡æ•°æ®åˆ†å¸ƒå¯¹æ¨¡å‹æ€§èƒ½çš„è´Ÿé¢å½±å“ï¼Œåœ¨ç¨€ç–æ•°æ®åŒºåŸŸå®ç°äº†æœ€æ–°æŠ€æœ¯æˆæœã€‚æ­¤å¤–ï¼ŒDist Lossæ˜“äºé›†æˆï¼Œå¯ä»¥å¼¥è¡¥ç°æœ‰æ–¹æ³•çš„ä¸è¶³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.15216v2">PDF</a> </p>
<p><strong>Summary</strong><br>æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨å¤„ç†ä¸å¹³è¡¡æ•°æ®åˆ†å¸ƒæ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å°‘æ ·æœ¬åŒºåŸŸè¡¨ç°ä¸ä½³ã€‚æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹æŸå¤±å‡½æ•°â€”â€”Dist Lossï¼Œé€šè¿‡æœ€å°åŒ–æ¨¡å‹é¢„æµ‹ä¸ç›®æ ‡æ ‡ç­¾ä¹‹é—´çš„åˆ†å¸ƒè·ç¦»ï¼Œå°†åˆ†å¸ƒä¿¡æ¯èå…¥æ¨¡å‹è®­ç»ƒï¼Œæé«˜æ¨¡å‹åœ¨å°‘æ ·æœ¬åŒºåŸŸçš„å…³æ³¨åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDist Lossèƒ½æœ‰æ•ˆç¼“è§£ä¸å¹³è¡¡æ•°æ®åˆ†å¸ƒå¯¹æ¨¡å‹æ€§èƒ½çš„è´Ÿé¢å½±å“ï¼Œè¾¾åˆ°ç¨€ç–æ•°æ®åŒºåŸŸçš„æœ€ä½³æ•ˆæœï¼Œä¸”æ˜“äºé›†æˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¸å¹³è¡¡æ•°æ®åˆ†å¸ƒåœ¨ç°å®åœºæ™¯ä¸­æ™®éå­˜åœ¨ï¼Œå¯¹æ·±åº¦å­¦ä¹ çš„åˆ†ç±»å’Œå›å½’ä»»åŠ¡éƒ½å¸¦æ¥æŒ‘æˆ˜ã€‚</li>
<li>æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨é«˜æ ·æœ¬å¯†åº¦åŒºåŸŸå®¹æ˜“è¿‡æ‹Ÿåˆï¼Œè€Œåœ¨ä½æ ·æœ¬å¯†åº¦åŒºåŸŸè¡¨ç°ä¸ä½³ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„æŸå¤±å‡½æ•°â€”â€”Dist Lossï¼Œæ—¨åœ¨æœ€å°åŒ–æ¨¡å‹é¢„æµ‹ä¸ç›®æ ‡æ ‡ç­¾ä¹‹é—´çš„åˆ†å¸ƒè·ç¦»ã€‚</li>
<li>Dist Lossèƒ½å¤Ÿä½¿å¾—æ·±åº¦æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è§„èŒƒå…¶è¾“å‡ºåˆ†å¸ƒï¼Œæé«˜å¯¹å°‘æ ·æœ¬åŒºåŸŸçš„å…³æ³¨åº¦ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒDist Lossèƒ½æœ‰æ•ˆç¼“è§£ä¸å¹³è¡¡æ•°æ®åˆ†å¸ƒå¯¹æ¨¡å‹æ€§èƒ½çš„è´Ÿé¢å½±å“ã€‚</li>
<li>Dist Lossåœ¨å›¾åƒå’ŒåŒ»ç–—æ•°æ®é›†çš„æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¾¾åˆ°äº†å…ˆè¿›çš„æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.15216">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-23e17600b50393b75e7062275d4a3d75.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0e42441625531801b0ca8db53946cdf8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a6b33ffef72f0152ca7ef1894df27945.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a4b1abb7af5ca6233459bcb94a6a1d1a.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Learning-General-Purpose-Biomedical-Volume-Representations-using-Randomized-Synthesis"><a href="#Learning-General-Purpose-Biomedical-Volume-Representations-using-Randomized-Synthesis" class="headerlink" title="Learning General-Purpose Biomedical Volume Representations using   Randomized Synthesis"></a>Learning General-Purpose Biomedical Volume Representations using   Randomized Synthesis</h2><p><strong>Authors:Neel Dey, Benjamin Billot, Hallee E. Wong, Clinton J. Wang, Mengwei Ren, P. Ellen Grant, Adrian V. Dalca, Polina Golland</strong></p>
<p>Current volumetric biomedical foundation models struggle to generalize as public 3D datasets are small and do not cover the broad diversity of medical procedures, conditions, anatomical regions, and imaging protocols. We address this by creating a representation learning method that instead anticipates strong domain shifts at training time itself. We first propose a data engine that synthesizes highly variable training samples that would enable generalization to new biomedical contexts. To then train a single 3D network for any voxel-level task, we develop a contrastive learning method that pretrains the network to be stable against nuisance imaging variation simulated by the data engine, a key inductive bias for generalization. This networkâ€™s features can be used as robust representations of input images for downstream tasks and its weights provide a strong, dataset-agnostic initialization for finetuning on new datasets. As a result, we set new standards across both multimodality registration and few-shot segmentation, a first for any 3D biomedical vision model, all without (pre-)training on any existing dataset of real images. </p>
<blockquote>
<p>å½“å‰ä½“ç§¯ç”Ÿç‰©åŒ»å­¦åŸºç¡€æ¨¡å‹éš¾ä»¥æ¨å¹¿ï¼Œå› ä¸ºå…¬å…±3Dæ•°æ®é›†è§„æ¨¡è¾ƒå°ï¼Œå¹¶æœªæ¶µç›–å¹¿æ³›çš„åŒ»ç–—ç¨‹åºã€çŠ¶å†µã€è§£å‰–åŒºåŸŸå’Œæˆåƒåè®®å¤šæ ·æ€§ã€‚æˆ‘ä»¬é€šè¿‡åˆ›å»ºä¸€ç§è¡¨ç¤ºå­¦ä¹ æ–¹æ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨è®­ç»ƒæ—¶æœ¬èº«å°±èƒ½é¢„æµ‹åˆ°å¼ºçƒˆçš„é¢†åŸŸåç§»ã€‚æˆ‘ä»¬é¦–å…ˆæå‡ºä¸€ä¸ªæ•°æ®å¼•æ“ï¼Œåˆæˆé«˜åº¦å¯å˜çš„è®­ç»ƒæ ·æœ¬ï¼Œä»¥å®ç°å‘æ–°ç”Ÿç‰©åŒ»å­¦ç¯å¢ƒçš„æ¨å¹¿ã€‚ä¸ºäº†é’ˆå¯¹ä»»ä½•ä½“ç´ çº§ä»»åŠ¡è®­ç»ƒå•ä¸ª3Dç½‘ç»œï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§å¯¹æ¯”å­¦ä¹ æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯¹ç”±æ•°æ®å¼•æ“æ¨¡æ‹Ÿçš„å¹²æ‰°æˆåƒå˜åŒ–è¿›è¡Œé¢„è®­ç»ƒç½‘ç»œï¼Œè¿™æ˜¯å®ç°æ³›åŒ–çš„å…³é”®å½’çº³åç½®ã€‚è¯¥ç½‘ç»œçš„ç‰¹å¾å¯ä½œä¸ºä¸‹æ¸¸ä»»åŠ¡çš„ç¨³å¥è¡¨ç¤ºï¼Œå…¶æƒé‡ä¸ºåœ¨æ–°æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒæä¾›äº†å¼ºå¤§ä¸”ç‹¬ç«‹äºæ•°æ®é›†åˆå§‹åŒ–ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åœ¨å¤šæ¨¡æ€æ³¨å†Œå’Œå°‘é•œå¤´åˆ†å‰²æ–¹é¢è®¾å®šäº†æ–°çš„æ ‡å‡†ï¼Œè¿™æ˜¯3Dç”Ÿç‰©åŒ»å­¦è§†è§‰æ¨¡å‹çš„é¦–åˆ›ï¼Œä¸”æ— éœ€åœ¨ä»»ä½•ç°æœ‰çœŸå®å›¾åƒæ•°æ®é›†ä¸Šè¿›è¡Œï¼ˆé¢„ï¼‰è®­ç»ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.02372v2">PDF</a> ICLR 2025: International Conference on Learning Representations. Code   and model weights available at <a target="_blank" rel="noopener" href="https://github.com/neel-dey/anatomix">https://github.com/neel-dey/anatomix</a>.   Keywords: synthetic data, representation learning, medical image analysis,   image registration, image segmentation</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§è§£å†³ç”Ÿç‰©åŒ»å­¦é¢†åŸŸæ¨¡å‹æ³›åŒ–èƒ½åŠ›å¼±çš„é—®é¢˜çš„æ–¹æ³•ã€‚é’ˆå¯¹å…¬å…±3Dæ•°æ®é›†è§„æ¨¡å°ä¸”ä¸èƒ½è¦†ç›–å¹¿æ³›åŒ»ç–—ç¨‹åºã€çŠ¶å†µã€è§£å‰–åŒºåŸŸå’Œæˆåƒåè®®çš„é—®é¢˜ï¼Œæ–‡ç« æå‡ºäº†ä¸€ç§è¡¨ç¤ºå­¦ä¹ æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨è®­ç»ƒæ—¶è‡ªèº«é¢„æœŸåˆ°å¼ºé¢†åŸŸæ¼‚ç§»ã€‚é€šè¿‡åˆ›å»ºä¸€ä¸ªæ•°æ®å¼•æ“æ¥åˆæˆé«˜åº¦å¯å˜çš„è®­ç»ƒæ ·æœ¬ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ¨å¹¿åˆ°æ–°çš„ç”Ÿç‰©åŒ»å­¦ç¯å¢ƒã€‚åŒæ—¶ï¼Œå¼€å‘äº†ä¸€ç§å¯¹æ¯”å­¦ä¹ æ–¹æ³•æ¥è®­ç»ƒä¸€ä¸ªç”¨äºä»»ä½•ä½“ç´ çº§åˆ«ä»»åŠ¡çš„å•ä¸€3Dç½‘ç»œã€‚è¯¥ç½‘ç»œèƒ½å¤Ÿåœ¨æ•°æ®å¼•æ“æ¨¡æ‹Ÿçš„å¹²æ‰°æˆåƒå˜åŒ–é¢å‰ä¿æŒç¨³å®šï¼Œæ˜¯ä¸€ç§å…³é”®çš„ä¸€èˆ¬åŒ–å½’çº³åè§ã€‚è¯¥ç½‘ç»œçš„ç‰¹å¾å¯ç”¨äºä¸‹æ¸¸ä»»åŠ¡çš„ç¨³å¥è¡¨ç¤ºï¼Œå…¶æƒé‡ä¸ºåœ¨æ–°çš„æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒæä¾›äº†å¼ºå¤§ä¸”ç‹¬ç«‹äºæ•°æ®é›†çš„åŸºç¡€åˆå§‹åŒ–ã€‚è¿™ç§æ–¹æ³•åœ¨è·¨æ¨¡æ€æ³¨å†Œå’Œå°‘æ ·æœ¬åˆ†å‰²æ–¹é¢éƒ½è¾¾åˆ°äº†æ–°çš„æ ‡å‡†ï¼Œæˆä¸ºé¦–ä¸ªä¸éœ€è¦åœ¨çœŸå®å›¾åƒæ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒçš„3Dç”Ÿç‰©åŒ»å­¦è§†è§‰æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰ç”Ÿç‰©åŒ»å­¦é¢†åŸŸçš„ä½“ç§¯æ¨¡å‹æ³›åŒ–èƒ½åŠ›å—é™ï¼Œä¸»è¦ç”±äºå…¬å…±3Dæ•°æ®é›†è§„æ¨¡å°ä¸”ç¼ºä¹å¤šæ ·æ€§ã€‚</li>
<li>æå‡ºä¸€ç§æ•°æ®å¼•æ“ï¼Œèƒ½å¤Ÿåˆæˆé«˜åº¦å¯å˜çš„è®­ç»ƒæ ·æœ¬ä»¥æ”¹å–„æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>é‡‡ç”¨å¯¹æ¯”å­¦ä¹ æ–¹æ³•è®­ç»ƒå•ä¸€3Dç½‘ç»œï¼Œç”¨äºå¤„ç†å„ç§ä½“ç´ çº§åˆ«ä»»åŠ¡ã€‚</li>
<li>ç½‘ç»œèƒ½åœ¨æ¨¡æ‹Ÿçš„å¹²æ‰°æˆåƒå˜åŒ–é¢å‰ä¿æŒç¨³å®šï¼Œè¿™æ˜¯æé«˜æ¨¡å‹æ³›åŒ–èƒ½åŠ›çš„é‡è¦å½’çº³åè§ã€‚</li>
<li>è¯¥ç½‘ç»œç‰¹å¾å¯ä½œä¸ºä¸‹æ¸¸ä»»åŠ¡çš„ç¨³å¥è¡¨ç¤ºï¼Œå…¶æƒé‡ä¸ºåœ¨æ–°çš„æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒæä¾›äº†å¼ºå¤§çš„åˆå§‹åŒ–ã€‚</li>
<li>æ­¤æ–¹æ³•åœ¨å¤šæ¨¡æ€æ³¨å†Œå’Œå°‘æ ·æœ¬åˆ†å‰²æ–¹é¢è¾¾åˆ°æ–°çš„æ ‡å‡†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.02372">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-c8ccbfddae24e54ee188d2aaa59b337c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8829a891f98535325355d33e316e5675.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c296a2b6e427afb423e81fadfd0733cc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-eff7484428edeb2c68f2c070333526f4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-62bc1811ed772a16504ff90639918a42.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1f2a62ad11af4a292fdc90828afd524c.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Data-adaptive-Differentially-Private-Prompt-Synthesis-for-In-Context-Learning"><a href="#Data-adaptive-Differentially-Private-Prompt-Synthesis-for-In-Context-Learning" class="headerlink" title="Data-adaptive Differentially Private Prompt Synthesis for In-Context   Learning"></a>Data-adaptive Differentially Private Prompt Synthesis for In-Context   Learning</h2><p><strong>Authors:Fengyu Gao, Ruida Zhou, Tianhao Wang, Cong Shen, Jing Yang</strong></p>
<p>Large Language Models (LLMs) rely on the contextual information embedded in examples&#x2F;demonstrations to perform in-context learning (ICL). To mitigate the risk of LLMs potentially leaking private information contained in examples in the prompt, we introduce a novel data-adaptive differentially private algorithm called AdaDPSyn to generate synthetic examples from the private dataset and then use these synthetic examples to perform ICL. The objective of AdaDPSyn is to adaptively adjust the noise level in the data synthesis mechanism according to the inherent statistical properties of the data, thereby preserving high ICL accuracy while maintaining formal differential privacy guarantees. A key innovation in AdaDPSyn is the Precision-Focused Iterative Radius Reduction technique, which dynamically refines the aggregation radius - the scope of data grouping for noise addition - based on patterns observed in data clustering, thereby minimizing the amount of additive noise. We conduct extensive experiments on standard benchmarks and compare AdaDPSyn with DP few-shot generation algorithm (Tang et al., 2023). The experiments demonstrate that AdaDPSyn not only outperforms DP few-shot generation, but also maintains high accuracy levels close to those of non-private baselines, providing an effective solution for ICL with privacy protection. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¾èµ–äºåµŒå…¥åœ¨ç¤ºä¾‹&#x2F;æ¼”ç¤ºä¸­çš„ä¸Šä¸‹æ–‡ä¿¡æ¯è¿›è¡Œä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ã€‚ä¸ºäº†å‡è½»LLMå¯èƒ½åœ¨æç¤ºä¸­çš„ç¤ºä¾‹ä¸­æ³„éœ²ç§äººä¿¡æ¯çš„é£é™©ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹çš„æ•°æ®è‡ªé€‚åº”å·®åˆ†éšç§ç®—æ³•AdaDPSynï¼Œä»ç§æœ‰æ•°æ®é›†ä¸­ç”Ÿæˆåˆæˆç¤ºä¾‹ï¼Œç„¶åä½¿ç”¨è¿™äº›åˆæˆç¤ºä¾‹è¿›è¡ŒICLã€‚AdaDPSynçš„ç›®æ ‡æ˜¯æ ¹æ®æ•°æ®çš„å›ºæœ‰ç»Ÿè®¡å±æ€§è‡ªé€‚åº”åœ°è°ƒæ•´æ•°æ®åˆæˆæœºåˆ¶ä¸­çš„å™ªå£°æ°´å¹³ï¼Œä»è€Œåœ¨ä¿æŒæ­£å¼å·®åˆ†éšç§ä¿è¯çš„åŒæ—¶ï¼Œä¿æŒé«˜ICLç²¾åº¦ã€‚AdaDPSynçš„ä¸€ä¸ªå…³é”®åˆ›æ–°ç‚¹æ˜¯ç²¾å‡†èšç„¦çš„è¿­ä»£åŠå¾„ç¼©å‡æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯æ ¹æ®åœ¨æ•°æ®èšç±»ä¸­è§‚å¯Ÿåˆ°çš„æ¨¡å¼åŠ¨æ€è°ƒæ•´èšåˆåŠå¾„ï¼ˆå™ªå£°æ·»åŠ çš„æ•°æ®åˆ†ç»„èŒƒå›´ï¼‰ï¼Œä»è€Œæœ€å°åŒ–æ·»åŠ çš„å™ªå£°é‡ã€‚æˆ‘ä»¬åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼Œå°†AdaDPSynä¸DPå°‘æ ·æœ¬ç”Ÿæˆç®—æ³•ï¼ˆTangç­‰äººï¼Œ2023ï¼‰è¿›è¡Œäº†æ¯”è¾ƒã€‚å®éªŒè¡¨æ˜ï¼ŒAdaDPSynä¸ä»…ä¼˜äºDPå°‘æ ·æœ¬ç”Ÿæˆï¼Œè€Œä¸”ä¿æŒæ¥è¿‘éç§æœ‰åŸºå‡†æµ‹è¯•çš„é«˜ç²¾åº¦æ°´å¹³ï¼Œä¸ºå¸¦æœ‰éšç§ä¿æŠ¤çš„ICLæä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.12085v2">PDF</a> Accepted to ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ä¾èµ–ç¤ºä¾‹ä¸­çš„ä¸Šä¸‹æ–‡ä¿¡æ¯è¿›è¡Œä¸Šä¸‹æ–‡å­¦ä¹ ã€‚ä¸ºç¼“è§£ç¤ºä¾‹ä¸­å¯èƒ½æ³„éœ²çš„ç§äººä¿¡æ¯é£é™©ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çš„æ•°æ®è‡ªé€‚åº”å·®åˆ†éšç§ç®—æ³•AdaDPSynï¼Œç”¨äºä»ç§æœ‰æ•°æ®é›†ä¸­ç”Ÿæˆåˆæˆç¤ºä¾‹ï¼Œå¹¶ä½¿ç”¨è¿™äº›åˆæˆç¤ºä¾‹è¿›è¡Œä¸Šä¸‹æ–‡å­¦ä¹ ã€‚AdaDPSynèƒ½è‡ªé€‚åº”è°ƒæ•´æ•°æ®åˆæˆæœºåˆ¶ä¸­çš„å™ªå£°æ°´å¹³ï¼Œä»¥é€‚åº”æ•°æ®çš„å†…åœ¨ç»Ÿè®¡ç‰¹æ€§ï¼Œä»è€Œåœ¨ä¿æŒé«˜ä¸Šä¸‹æ–‡å­¦ä¹ ç²¾åº¦çš„åŒæ—¶ï¼Œç¡®ä¿æ­£å¼çš„å·®åˆ†éšç§ä¿è¯ã€‚å…¶å…³é”®åˆ›æ–°åœ¨äºç²¾å‡†èšç„¦çš„è¿­ä»£åŠå¾„ç¼©å‡æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯æ ¹æ®æ•°æ®èšç±»ä¸­è§‚å¯Ÿåˆ°çš„æ¨¡å¼åŠ¨æ€è°ƒæ•´èšåˆåŠå¾„ï¼Œä»è€Œæœ€å°åŒ–æ·»åŠ çš„å™ªå£°é‡ã€‚å®éªŒè¡¨æ˜ï¼ŒAdaDPSynä¸ä»…ä¼˜äºDPçš„å°‘æ ·æœ¬ç”Ÿæˆç®—æ³•ï¼Œè€Œä¸”ä¿æŒæ¥è¿‘éç§æœ‰åŸºå‡†çº¿çš„ç²¾åº¦ï¼Œä¸ºå¸¦æœ‰éšç§ä¿æŠ¤çš„ä¸Šä¸‹æ–‡å­¦ä¹ æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ä¾èµ–ç¤ºä¾‹çš„ä¸Šä¸‹æ–‡ä¿¡æ¯è¿›è¡Œå­¦ä¹ ã€‚</li>
<li>AdaDPSynæ˜¯ä¸€ç§æ•°æ®è‡ªé€‚åº”çš„å·®åˆ†éšç§ç®—æ³•ï¼Œç”¨äºç”Ÿæˆåˆæˆç¤ºä¾‹ä»¥è¿›è¡Œä¸Šä¸‹æ–‡å­¦ä¹ ã€‚</li>
<li>AdaDPSynèƒ½æ ¹æ®æ•°æ®çš„ç»Ÿè®¡ç‰¹æ€§è‡ªé€‚åº”è°ƒæ•´å™ªå£°æ°´å¹³ã€‚</li>
<li>AdaDPSyné‡‡ç”¨ç²¾å‡†èšç„¦çš„è¿­ä»£åŠå¾„ç¼©å‡æŠ€æœ¯ï¼ŒåŠ¨æ€è°ƒæ•´èšåˆåŠå¾„ä»¥æœ€å°åŒ–å™ªå£°ã€‚</li>
<li>AdaDPSynåœ¨ä¿æŒé«˜ä¸Šä¸‹æ–‡å­¦ä¹ ç²¾åº¦çš„åŒæ—¶ï¼Œç¡®ä¿å·®åˆ†éšç§ã€‚</li>
<li>ä¸ç°æœ‰çš„DPå°‘æ ·æœ¬ç”Ÿæˆç®—æ³•ç›¸æ¯”ï¼ŒAdaDPSynè¡¨ç°å‡ºæ›´å¥½çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.12085">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c1b2004c529bbbecc06bc7838dd56d23.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0c01121ee1a009a8ec5922597e9a4204.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-68a5cf9d73f581ebfb6598032631ec27.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Snuffy-Efficient-Whole-Slide-Image-Classifier"><a href="#Snuffy-Efficient-Whole-Slide-Image-Classifier" class="headerlink" title="Snuffy: Efficient Whole Slide Image Classifier"></a>Snuffy: Efficient Whole Slide Image Classifier</h2><p><strong>Authors:Hossein Jafarinia, Alireza Alipanah, Danial Hamdi, Saeed Razavi, Nahal Mirzaie, Mohammad Hossein Rohban</strong></p>
<p>Whole Slide Image (WSI) classification with multiple instance learning (MIL) in digital pathology faces significant computational challenges. Current methods mostly rely on extensive self-supervised learning (SSL) for satisfactory performance, requiring long training periods and considerable computational resources. At the same time, no pre-training affects performance due to domain shifts from natural images to WSIs. We introduce Snuffy architecture, a novel MIL-pooling method based on sparse transformers that mitigates performance loss with limited pre-training and enables continual few-shot pre-training as a competitive option. Our sparsity pattern is tailored for pathology and is theoretically proven to be a universal approximator with the tightest probabilistic sharp bound on the number of layers for sparse transformers, to date. We demonstrate Snuffyâ€™s effectiveness on CAMELYON16 and TCGA Lung cancer datasets, achieving superior WSI and patch-level accuracies. The code is available on <a target="_blank" rel="noopener" href="https://github.com/jafarinia/snuffy">https://github.com/jafarinia/snuffy</a>. </p>
<blockquote>
<p>åœ¨å…¨æ•°å­—ç—…ç†å­¦ä¸­ï¼ŒåŸºäºå¤šå®ä¾‹å­¦ä¹ ï¼ˆMILï¼‰çš„å…¨å¹»ç¯ç‰‡å›¾åƒï¼ˆWSIï¼‰åˆ†ç±»é¢ä¸´ç€å·¨å¤§çš„è®¡ç®—æŒ‘æˆ˜ã€‚å½“å‰çš„æ–¹æ³•å¤§å¤šä¾èµ–äºå¹¿æ³›çš„è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰ä»¥è·å¾—ä»¤äººæ»¡æ„çš„æ•ˆæœï¼Œè¿™éœ€è¦é•¿æ—¶é—´çš„è®­ç»ƒå’Œå¤§é‡çš„è®¡ç®—èµ„æºã€‚åŒæ—¶ï¼Œç”±äºæ²¡æœ‰é¢„è®­ç»ƒä¼šå—åˆ°æ¥è‡ªè‡ªç„¶å›¾åƒåˆ°WSIé¢†åŸŸè½¬å˜çš„å½±å“ã€‚æˆ‘ä»¬å¼•å…¥äº†Snuffyæ¶æ„ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºç¨€ç–å˜å‹å™¨çš„æ–°å‹MILæ± æ–¹æ³•ï¼Œå®ƒé€šè¿‡æœ‰é™çš„é¢„è®­ç»ƒå‡è½»äº†æ€§èƒ½æŸå¤±ï¼Œå¹¶èƒ½å¤Ÿå®ç°æŒç»­çš„å°‘é‡é¢„è®­ç»ƒä½œä¸ºä¸€ç§å…·æœ‰ç«äº‰åŠ›çš„é€‰æ‹©ã€‚æˆ‘ä»¬çš„ç¨€ç–æ¨¡å¼æ˜¯é’ˆå¯¹ç—…ç†å­¦é‡èº«å®šåˆ¶çš„ï¼Œä»ç†è®ºä¸Šè¯æ˜äº†å®ƒæ˜¯è¿„ä»Šä¸ºæ­¢ç¨€ç–å˜å‹å™¨å±‚æ•°æœ€ç´§çš„æ¦‚ç‡å°–é”ç•Œé™çš„é€šç”¨é€¼è¿‘å™¨ã€‚æˆ‘ä»¬åœ¨CAMELYON16å’ŒTCGAè‚ºç™Œæ•°æ®é›†ä¸Šå±•ç¤ºäº†Snuffyçš„æœ‰æ•ˆæ€§ï¼Œå®ç°äº†ä¼˜è¶Šçš„å…¨å¹»ç¯ç‰‡å›¾åƒå’Œè¡¥ä¸çº§åˆ«çš„å‡†ç¡®æ€§ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/jafarinia/snuffy%E4%B8%8A%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/jafarinia/snuffyä¸Šè·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.08258v3">PDF</a> Accepted for ECCV 2024</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºç¨€ç–å˜æ¢å™¨çš„æ–°å‹MILæ± åŒ–æ–¹æ³•â€”â€”Snuffyæ¶æ„ï¼Œè¯¥æ¶æ„è§£å†³äº†æ•°å­—ç—…ç†å­¦ä¸­å…¨å¹»ç¯ç‰‡å›¾åƒï¼ˆWSIï¼‰åˆ†ç±»é¢ä¸´çš„æŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•éœ€è¦å¤§é‡æ—¶é—´å’Œè®¡ç®—èµ„æºï¼Œè€ŒSnuffyæ¶æ„é€šè¿‡å¼•å…¥ç¨€ç–å˜æ¢å™¨æŠ€æœ¯å®ç°äº†æ›´é«˜æ•ˆçš„è®­ç»ƒè¿‡ç¨‹ï¼Œå¹¶å…·æœ‰å‡ºè‰²çš„å°‘æ ·æœ¬é¢„è®­ç»ƒèƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSnuffyæ¶æ„åœ¨CAMELYON16å’ŒTCGAè‚ºç™Œæ•°æ®é›†ä¸Šå®ç°äº†æ›´é«˜çš„WSIå’Œè¡¥ä¸çº§åˆ«çš„å‡†ç¡®æ€§ã€‚ä»£ç å·²å‘å¸ƒåœ¨GitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Snuffyæ¶æ„è§£å†³äº†æ•°å­—ç—…ç†å­¦ä¸­WSIåˆ†ç±»é¢ä¸´çš„è®¡ç®—æŒ‘æˆ˜ã€‚</li>
<li>å½“å‰æ–¹æ³•å¤§å¤šä¾èµ–å¤§é‡è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰ï¼Œè®­ç»ƒæ—¶é—´é•¿ä¸”è®¡ç®—èµ„æºæ¶ˆè€—å¤§ã€‚</li>
<li>Snuffyæ¶æ„å¼•å…¥ç¨€ç–å˜æ¢å™¨æŠ€æœ¯ï¼Œæé«˜è®­ç»ƒæ•ˆç‡å¹¶å…·å¤‡å‡ºè‰²çš„å°‘æ ·æœ¬é¢„è®­ç»ƒèƒ½åŠ›ã€‚</li>
<li>Snuffyæ¶æ„å®ç°äº†ä¸ªæ€§åŒ–ç¨€ç–æ¨¡å¼ï¼Œé€‚ç”¨äºç—…ç†å­¦é¢†åŸŸï¼Œä¸”åœ¨ç¨€ç–å˜æ¢å™¨çš„å±‚æ•°ä¸Šå…·æœ‰ç†è®ºè¯æ˜çš„æœ€ç´§æ¦‚ç‡å°–é”ç•Œã€‚</li>
<li>åœ¨CAMELYON16å’ŒTCGAè‚ºç™Œæ•°æ®é›†ä¸Šï¼ŒSnuffyæ¶æ„å®ç°äº†è¾ƒé«˜çš„WSIå’Œè¡¥ä¸çº§åˆ«å‡†ç¡®æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.08258">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-de715e8102c75e4c84c4d4abc58972b8.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Leveraging-Vision-Language-Models-for-Specialized-Agricultural-Tasks"><a href="#Leveraging-Vision-Language-Models-for-Specialized-Agricultural-Tasks" class="headerlink" title="Leveraging Vision Language Models for Specialized Agricultural Tasks"></a>Leveraging Vision Language Models for Specialized Agricultural Tasks</h2><p><strong>Authors:Muhammad Arbab Arshad, Talukder Zaki Jubery, Tirtho Roy, Rim Nassiri, Asheesh K. Singh, Arti Singh, Chinmay Hegde, Baskar Ganapathysubramanian, Aditya Balu, Adarsh Krishnamurthy, Soumik Sarkar</strong></p>
<p>As Vision Language Models (VLMs) become increasingly accessible to farmers and agricultural experts, there is a growing need to evaluate their potential in specialized tasks. We present AgEval, a comprehensive benchmark for assessing VLMsâ€™ capabilities in plant stress phenotyping, offering a solution to the challenge of limited annotated data in agriculture. Our study explores how general-purpose VLMs can be leveraged for domain-specific tasks with only a few annotated examples, providing insights into their behavior and adaptability. AgEval encompasses 12 diverse plant stress phenotyping tasks, evaluating zero-shot and few-shot in-context learning performance of state-of-the-art models including Claude, GPT, Gemini, and LLaVA. Our results demonstrate VLMsâ€™ rapid adaptability to specialized tasks, with the best-performing model showing an increase in F1 scores from 46.24% to 73.37% in 8-shot identification. To quantify performance disparities across classes, we introduce metrics such as the coefficient of variation (CV), revealing that VLMsâ€™ training impacts classes differently, with CV ranging from 26.02% to 58.03%. We also find that strategic example selection enhances model reliability, with exact category examples improving F1 scores by 15.38% on average. AgEval establishes a framework for assessing VLMs in agricultural applications, offering valuable benchmarks for future evaluations. Our findings suggest that VLMs, with minimal few-shot examples, show promise as a viable alternative to traditional specialized models in plant stress phenotyping, while also highlighting areas for further refinement. Results and benchmark details are available at: <a target="_blank" rel="noopener" href="https://github.com/arbab-ml/AgEval">https://github.com/arbab-ml/AgEval</a> </p>
<blockquote>
<p>éšç€è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è¶Šæ¥è¶Šè¢«å†œæ°‘å’Œå†œä¸šä¸“å®¶æ‰€æ¥è§¦ï¼Œè¯„ä¼°å®ƒä»¬åœ¨ä¸“ä¸šä»»åŠ¡ä¸­çš„æ½œåŠ›å˜å¾—æ—¥ç›Šé‡è¦ã€‚æˆ‘ä»¬æ¨å‡ºäº†AgEvalï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°VLMsåœ¨æ¤ç‰©èƒè¿«è¡¨å‹åˆ†æèƒ½åŠ›çš„ç»¼åˆåŸºå‡†æµ‹è¯•ï¼Œä¸ºè§£å†³å†œä¸šä¸­æ ‡æ³¨æ•°æ®æœ‰é™çš„é—®é¢˜æä¾›äº†è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬çš„ç ”ç©¶æ¢è®¨äº†é€šç”¨VLMså¦‚ä½•åˆ©ç”¨å°‘é‡çš„æ ‡æ³¨ç¤ºä¾‹æ¥å®Œæˆç‰¹å®šé¢†åŸŸçš„ä»»åŠ¡ï¼Œæ·±å…¥äº†è§£å…¶è¡Œä¸ºå’Œé€‚åº”æ€§ã€‚AgEvalæ¶µç›–äº†12ä¸ªå¤šæ ·åŒ–çš„æ¤ç‰©èƒè¿«è¡¨å‹åˆ†æä»»åŠ¡ï¼Œè¯„ä¼°äº†åŒ…æ‹¬Claudeã€GPTã€Geminiå’ŒLLaVAç­‰æœ€æ–°æ¨¡å‹çš„æ— ç›‘ç£å’Œå°‘é‡ç›‘ç£çš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›è¡¨ç°ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒVLMsèƒ½å¤Ÿè¿…é€Ÿé€‚åº”ä¸“ä¸šä»»åŠ¡ï¼Œæœ€ä½³æ€§èƒ½çš„æ¨¡å‹åœ¨8æ¬¡æ‹æ‘„çš„è¯†åˆ«ä¸­F1å¾—åˆ†ä»46.24%æé«˜åˆ°73.37%ã€‚ä¸ºäº†é‡åŒ–å„ç±»ä¹‹é—´çš„æ€§èƒ½å·®å¼‚ï¼Œæˆ‘ä»¬å¼•å…¥äº†å˜å¼‚ç³»æ•°ï¼ˆCVï¼‰è¿™ä¸€æŒ‡æ ‡ï¼Œæ­ç¤ºVLMsçš„è®­ç»ƒå¯¹ä¸åŒç±»åˆ«çš„å½±å“ä¸åŒï¼ŒCVèŒƒå›´ä»26.02%åˆ°58.03%ã€‚æˆ‘ä»¬è¿˜å‘ç°ï¼Œæˆ˜ç•¥æ€§çš„ä¾‹å­é€‰æ‹©å¯ä»¥æé«˜æ¨¡å‹çš„å¯é æ€§ï¼Œç²¾ç¡®ç±»åˆ«çš„ä¾‹å­å¹³å‡æé«˜äº†F1å¾—åˆ†15.38%ã€‚AgEvalå»ºç«‹äº†è¯„ä¼°VLMsåœ¨å†œä¸šåº”ç”¨ä¸­çš„åŸºå‡†æµ‹è¯•æ¡†æ¶ï¼Œä¸ºæœªæ¥çš„è¯„ä¼°æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåˆ©ç”¨æœ€å°‘çš„å‡ æ¬¡æ‹æ‘„ç¤ºä¾‹ï¼ŒVLMsåœ¨æ¤ç‰©èƒè¿«è¡¨å‹åˆ†æä¸­æ˜¾ç¤ºå‡ºä½œä¸ºä¼ ç»Ÿä¸“ä¸šæ¨¡å‹çš„å¯è¡Œæ›¿ä»£æ–¹æ¡ˆçš„æ½œåŠ›ï¼ŒåŒæ—¶ä¹Ÿçªå‡ºäº†éœ€è¦è¿›ä¸€æ­¥æ”¹è¿›çš„é¢†åŸŸã€‚ç»“æœå’ŒåŸºå‡†æµ‹è¯•è¯¦æƒ…å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/arbab-ml/AgEval%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/arbab-ml/AgEvalæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.19617v2">PDF</a> Published at WACV 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Vision Language Modelsï¼ˆVLMsï¼‰åœ¨å†œä¸šé¢†åŸŸçš„æ½œåŠ›è¯„ä¼°ã€‚ä¸ºè¯„ä¼°VLMsåœ¨æ¤ç‰©å‹åŠ›è¡¨ç°åˆ†ææ–¹é¢çš„èƒ½åŠ›ï¼Œæå‡ºäº†AgEvalè¿™ä¸€ç»¼åˆæ€§åŸºå‡†æµ‹è¯•ã€‚ç ”ç©¶æ¢è®¨äº†é€šç”¨VLMså¦‚ä½•åœ¨åªéœ€å°‘é‡æ ‡æ³¨æ ·æœ¬çš„æƒ…å†µä¸‹ï¼Œå°±èƒ½é€‚åº”ç‰¹å®šä»»åŠ¡ã€‚AgEvalæ¶µç›–äº†å¤šæ ·åŒ–çš„æ¤ç‰©å‹åŠ›è¡¨ç°åˆ†æä»»åŠ¡ï¼Œè¯„ä¼°äº†æœ€å‰æ²¿æ¨¡å‹å¦‚Claudeã€GPTç­‰çš„é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›è¡¨ç°ã€‚ç»“æœæ˜¾ç¤ºVLMsèƒ½å¿«é€Ÿé€‚åº”ä¸“é¡¹ä»»åŠ¡ï¼Œæœ€ä½³æ¨¡å‹åœ¨8æ¬¡å°„å‡»è¯†åˆ«ä¸­çš„F1å¾—åˆ†ä»46.24%æé«˜åˆ°73.37%ã€‚åŒæ—¶ï¼Œé€šè¿‡å¼•å…¥å˜å¼‚ç³»æ•°ç­‰åº¦é‡æ ‡å‡†ï¼Œæ­ç¤ºäº†ä¸åŒç±»åˆ«é—´æ€§èƒ½å·®å¼‚çš„é—®é¢˜ã€‚ç ”ç©¶è¿˜å‘ç°ï¼Œé€šè¿‡é€‰æ‹©ç­–ç•¥æ€§ç¤ºä¾‹èƒ½æé«˜æ¨¡å‹å¯é æ€§ã€‚AgEvalä¸ºè¯„ä¼°VLMsåœ¨å†œä¸šåº”ç”¨ä¸­çš„è¡¨ç°æä¾›äº†å®è´µåŸºå‡†ï¼Œè¡¨æ˜VLMsåœ¨æ¤ç‰©å‹åŠ›è¡¨ç°åˆ†ææ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision Language Models (VLMs) åœ¨å†œä¸šé¢†åŸŸçš„æ¤ç‰©å‹åŠ›è¡¨ç°åˆ†æä¸­å…·æœ‰æ½œåŠ›è¯„ä¼°ä»·å€¼ã€‚</li>
<li>AgEvalæ˜¯è¯„ä¼°VLMsåœ¨æ¤ç‰©å‹åŠ›è¡¨ç°åˆ†ææ–¹é¢çš„ç»¼åˆæ€§åŸºå‡†æµ‹è¯•ã€‚</li>
<li>VLMså¯ä»¥åœ¨åªéœ€å°‘é‡æ ‡æ³¨æ ·æœ¬çš„æƒ…å†µä¸‹é€‚åº”ç‰¹å®šä»»åŠ¡ã€‚</li>
<li>AgEvalæ¶µç›–äº†å¤šæ ·åŒ–çš„æ¤ç‰©å‹åŠ›è¡¨ç°åˆ†æä»»åŠ¡ï¼Œå¹¶è¯„ä¼°äº†å¤šç§æœ€å‰æ²¿æ¨¡å‹çš„è¡¨ç°ã€‚</li>
<li>VLMsèƒ½å¿«é€Ÿé€‚åº”ä¸“é¡¹ä»»åŠ¡ï¼Œå¹¶é€šè¿‡æˆ˜ç•¥ç¤ºä¾‹é€‰æ‹©æé«˜æ¨¡å‹å¯é æ€§ã€‚</li>
<li>ä¸åŒç±»åˆ«é—´å­˜åœ¨æ€§èƒ½å·®å¼‚ï¼Œå¼•å…¥åº¦é‡æ ‡å‡†å¦‚å˜å¼‚ç³»æ•°ä»¥é‡åŒ–è¿™ç§å·®å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.19617">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d15cc82c7f8b9f1758f5abaf84baf770.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a8022a7665b2e1652f4ad1827f07c5fa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-28033b52ca1647158f18a81cabcc26b1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3f340833d5f714f8e173f335ab2ad99b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cebeeb0a109b3fec821d56d1b20e1d79.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f710258232606af9b1e3a3d92bf032d.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-05/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-05/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-05/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-ae67c44cc8589d871fe1d536da07d97a.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-05  DynRefer Delving into Region-level Multimodal Tasks via Dynamic   Resolution
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-05
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-05/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-3de82161d67aeb20a744e490e5639f7c.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-05  FACT-AUDIT An Adaptive Multi-Agent Framework for Dynamic Fact-Checking   Evaluation of Large Language Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-05
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">17124.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
