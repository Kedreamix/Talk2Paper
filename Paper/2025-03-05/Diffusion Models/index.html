<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-05  A Dual-Purpose Framework for Backdoor Defense and Backdoor Amplification   in Diffusion Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-59a4cc9492a290d548413adf31be9e93.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-05
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    85 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-05-æ›´æ–°"><a href="#2025-03-05-æ›´æ–°" class="headerlink" title="2025-03-05 æ›´æ–°"></a>2025-03-05 æ›´æ–°</h1><h2 id="A-Dual-Purpose-Framework-for-Backdoor-Defense-and-Backdoor-Amplification-in-Diffusion-Models"><a href="#A-Dual-Purpose-Framework-for-Backdoor-Defense-and-Backdoor-Amplification-in-Diffusion-Models" class="headerlink" title="A Dual-Purpose Framework for Backdoor Defense and Backdoor Amplification   in Diffusion Models"></a>A Dual-Purpose Framework for Backdoor Defense and Backdoor Amplification   in Diffusion Models</h2><p><strong>Authors:Vu Tuan Truong, Long Bao Le</strong></p>
<p>Diffusion models have emerged as state-of-the-art generative frameworks, excelling in producing high-quality multi-modal samples. However, recent studies have revealed their vulnerability to backdoor attacks, where backdoored models generate specific, undesirable outputs called backdoor target (e.g., harmful images) when a pre-defined trigger is embedded to their inputs. In this paper, we propose PureDiffusion, a dual-purpose framework that simultaneously serves two contrasting roles: backdoor defense and backdoor attack amplification. For defense, we introduce two novel loss functions to invert backdoor triggers embedded in diffusion models. The first leverages trigger-induced distribution shifts across multiple timesteps of the diffusion process, while the second exploits the denoising consistency effect when a backdoor is activated. Once an accurate trigger inversion is achieved, we develop a backdoor detection method that analyzes both the inverted trigger and the generated backdoor targets to identify backdoor attacks. In terms of attack amplification with the role of an attacker, we describe how our trigger inversion algorithm can be used to reinforce the original trigger embedded in the backdoored diffusion model. This significantly boosts attack performance while reducing the required backdoor training time. Experimental results demonstrate that PureDiffusion achieves near-perfect detection accuracy, outperforming existing defenses by a large margin, particularly against complex trigger patterns. Additionally, in an attack scenario, our attack amplification approach elevates the attack success rate (ASR) of existing backdoor attacks to nearly 100% while reducing training time by up to 20x. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹å·²ç»ä½œä¸ºæœ€å…ˆè¿›çš„ç”Ÿæˆæ¡†æ¶å‡ºç°ï¼Œæ“…é•¿ç”Ÿæˆé«˜è´¨é‡çš„å¤šæ¨¡å¼æ ·æœ¬ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œå®ƒä»¬å®¹æ˜“å—åˆ°åé—¨æ”»å‡»çš„å½±å“ï¼Œåé—¨æ¨¡å‹ä¼šåœ¨è¾“å…¥åµŒå…¥é¢„å®šä¹‰è§¦å‘å™¨æ—¶ï¼Œç”Ÿæˆç‰¹å®šçš„ã€ä¸å¸Œæœ›å‡ºç°çš„è¾“å‡ºï¼Œç§°ä¸ºåé—¨ç›®æ ‡ï¼ˆä¾‹å¦‚ï¼Œæœ‰å®³å›¾åƒï¼‰ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†PureDiffusionï¼Œè¿™æ˜¯ä¸€ä¸ªåŒé‡ç”¨é€”çš„æ¡†æ¶ï¼Œå¯ä»¥åŒæ—¶æ‰®æ¼”ä¸¤ä¸ªç›¸åçš„è§’è‰²ï¼šåé—¨é˜²å¾¡å’Œåé—¨æ”»å‡»å¢å¼ºã€‚åœ¨é˜²å¾¡æ–¹é¢ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸¤ç§æ–°å‹æŸå¤±å‡½æ•°æ¥åè½¬åµŒå…¥åœ¨æ‰©æ•£æ¨¡å‹ä¸­çš„åé—¨è§¦å‘å™¨ã€‚ç¬¬ä¸€ç§åˆ©ç”¨è§¦å‘å¼•èµ·çš„æ‰©æ•£è¿‡ç¨‹ä¸­å¤šä¸ªæ—¶é—´æ­¥çš„åˆ†å¸ƒå˜åŒ–ï¼Œç¬¬äºŒç§åˆ™åˆ©ç”¨åé—¨è¢«æ¿€æ´»æ—¶çš„å»å™ªä¸€è‡´æ€§æ•ˆåº”ã€‚ä¸€æ—¦å®ç°äº†å‡†ç¡®çš„è§¦å‘å™¨åè½¬ï¼Œæˆ‘ä»¬å°±å¼€å‘äº†ä¸€ç§åé—¨æ£€æµ‹æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ†æåè½¬çš„è§¦å‘å™¨å’Œç”Ÿæˆçš„åé—¨ç›®æ ‡ï¼Œä»¥è¯†åˆ«åé—¨æ”»å‡»ã€‚åœ¨ä½œä¸ºæ”»å‡»è€…çš„è§’è‰²è¿›è¡Œæ”»å‡»å¢å¼ºæ–¹é¢ï¼Œæˆ‘ä»¬æè¿°äº†å¦‚ä½•ä½¿ç”¨æˆ‘ä»¬çš„è§¦å‘å™¨åè½¬ç®—æ³•æ¥åŠ å¼ºåµŒå…¥åé—¨æ‰©æ•£æ¨¡å‹ä¸­çš„åŸå§‹è§¦å‘å™¨ã€‚è¿™æ˜¾è‘—æé«˜äº†æ”»å‡»æ€§èƒ½ï¼ŒåŒæ—¶å‡å°‘äº†æ‰€éœ€çš„åé—¨è®­ç»ƒæ—¶é—´ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPureDiffusionå®ç°äº†è¿‘ä¹å®Œç¾çš„æ£€æµ‹ç²¾åº¦ï¼Œå¤§å¤§ä¼˜äºç°æœ‰çš„é˜²å¾¡æ‰‹æ®µï¼Œå°¤å…¶æ˜¯å¯¹å¤æ‚çš„è§¦å‘æ¨¡å¼ã€‚æ­¤å¤–ï¼Œåœ¨æ”»å‡»åœºæ™¯ä¸­ï¼Œæˆ‘ä»¬çš„æ”»å‡»å¢å¼ºæ–¹æ³•å°†ç°æœ‰åé—¨æ”»å‡»çš„æ”»å‡»æˆåŠŸç‡ï¼ˆASRï¼‰æé«˜åˆ°æ¥è¿‘100%ï¼ŒåŒæ—¶å‡å°‘è®­ç»ƒæ—¶é—´é«˜è¾¾20å€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.19047v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ‰©æ•£æ¨¡å‹ä½œä¸ºå½“å‰æœ€å…ˆè¿›çš„ç”Ÿæˆæ¡†æ¶ï¼Œèƒ½å¤Ÿäº§ç”Ÿé«˜è´¨é‡çš„å¤šæ¨¡å¼æ ·æœ¬ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œå®ƒä»¬å®¹æ˜“å—åˆ°åé—¨æ”»å‡»çš„å½±å“ï¼Œå…¶ä¸­åé—¨æ¨¡å‹ä¼šåœ¨è¾“å…¥åµŒå…¥é¢„å®šä¹‰è§¦å‘å™¨æ—¶ç”Ÿæˆç‰¹å®šçš„ã€ä¸å¸Œæœ›çš„è¾“å‡ºï¼Œç§°ä¸ºåé—¨ç›®æ ‡ï¼ˆä¾‹å¦‚ï¼Œæœ‰å®³å›¾åƒï¼‰ã€‚æœ¬æ–‡æå‡ºäº†PureDiffusionï¼Œä¸€ä¸ªå…¼å…·é˜²å¾¡å’Œæ”»å‡»åŠŸèƒ½çš„åŒé‡ç”¨é€”æ¡†æ¶ã€‚åœ¨é˜²å¾¡æ–¹é¢ï¼Œæˆ‘ä»¬å¼•å…¥ä¸¤ç§æ–°å‹æŸå¤±å‡½æ•°æ¥åè½¬åµŒå…¥åœ¨æ‰©æ•£æ¨¡å‹ä¸­çš„åé—¨è§¦å‘å™¨ã€‚ç¬¬ä¸€ç§åˆ©ç”¨è§¦å‘å™¨åœ¨æ‰©æ•£è¿‡ç¨‹çš„å¤šä¸ªæ—¶é—´æ­¥é•¿ä¸­å¼•èµ·çš„åˆ†å¸ƒå˜åŒ–ï¼Œç¬¬äºŒç§åˆ™åˆ©ç”¨åé—¨æ¿€æ´»æ—¶çš„å»å™ªä¸€è‡´æ€§æ•ˆåº”ã€‚å®ç°å‡†ç¡®çš„è§¦å‘å™¨åè½¬åï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§åé—¨æ£€æµ‹æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ†æåè½¬çš„è§¦å‘å™¨å’Œç”Ÿæˆçš„åé—¨ç›®æ ‡ï¼Œä»¥è¯†åˆ«åé—¨æ”»å‡»ã€‚ä½œä¸ºæ”»å‡»è€…çš„è§’è‰²è¿›è¡Œæ”»å‡»æ”¾å¤§æ—¶ï¼Œæˆ‘ä»¬æè¿°äº†å¦‚ä½•åˆ©ç”¨æˆ‘ä»¬çš„è§¦å‘å™¨åè½¬ç®—æ³•åŠ å¼ºåµŒå…¥åœ¨å—åé—¨æ§åˆ¶çš„æ‰©æ•£æ¨¡å‹ä¸­çš„åŸå§‹è§¦å‘å™¨ã€‚è¿™å¤§å¤§æé«˜äº†æ”»å‡»æ€§èƒ½ï¼ŒåŒæ—¶å‡å°‘äº†æ‰€éœ€çš„åé—¨è®­ç»ƒæ—¶é—´ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPureDiffusionå®ç°äº†è¿‘ä¹å®Œç¾çš„æ£€æµ‹å‡†ç¡®ç‡ï¼Œå¤§å¤§ä¼˜äºç°æœ‰é˜²å¾¡æ‰‹æ®µï¼Œå°¤å…¶æ˜¯å¯¹å¤æ‚è§¦å‘æ¨¡å¼ã€‚æ­¤å¤–ï¼Œåœ¨æ”»å‡»åœºæ™¯ä¸­ï¼Œæˆ‘ä»¬çš„æ”»å‡»æ”¾å¤§æ–¹æ³•å¯å°†ç°æœ‰åé—¨æ”»å‡»çš„æˆåŠŸç‡æé«˜åˆ°è¿‘100%ï¼ŒåŒæ—¶å‡å°‘è®­ç»ƒæ—¶é—´é«˜è¾¾20å€ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹è™½ç„¶èƒ½ç”Ÿæˆé«˜è´¨é‡å¤šæ¨¡å¼æ ·æœ¬ï¼Œä½†æ˜“å—åé—¨æ”»å‡»å½±å“ï¼Œèƒ½ç”Ÿæˆç‰¹å®šä¸å¸Œæœ›çš„è¾“å‡ºã€‚</li>
<li>PureDiffusionæ¡†æ¶å…¼å…·é˜²å¾¡å’Œæ”»å‡»åŠŸèƒ½ã€‚</li>
<li>å¼•å…¥ä¸¤ç§æ–°å‹æŸå¤±å‡½æ•°æ¥åè½¬åé—¨è§¦å‘å™¨ã€‚</li>
<li>åˆ©ç”¨è§¦å‘å™¨åœ¨æ‰©æ•£è¿‡ç¨‹ä¸­çš„åˆ†å¸ƒå˜åŒ–å’Œå»å™ªä¸€è‡´æ€§æ•ˆåº”è¿›è¡Œé˜²å¾¡ã€‚</li>
<li>å¼€å‘äº†ä¸€ç§åˆ†æåè½¬è§¦å‘å™¨å’Œåé—¨ç›®æ ‡ä»¥è¯†åˆ«åé—¨æ”»å‡»çš„æ–¹æ³•ã€‚</li>
<li>è§¦å‘å™¨åè½¬ç®—æ³•å¯ç”¨äºåŠ å¼ºåŸå§‹åµŒå…¥çš„è§¦å‘å™¨ï¼Œæé«˜æ”»å‡»æ€§èƒ½å¹¶å‡å°‘è®­ç»ƒæ—¶é—´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.19047">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b47d901bc0c795916570104600e28910.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0345f3cbcf9cba197f6bab1bafda9d7e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ff63ef0614e2464202d9397e4dfa2c10.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1a9c4e64258811e07ee662b42aea6a43.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Fast-Direct-Query-Efficient-Online-Black-box-Guidance-for-Diffusion-model-Target-Generation"><a href="#Fast-Direct-Query-Efficient-Online-Black-box-Guidance-for-Diffusion-model-Target-Generation" class="headerlink" title="Fast Direct: Query-Efficient Online Black-box Guidance for   Diffusion-model Target Generation"></a>Fast Direct: Query-Efficient Online Black-box Guidance for   Diffusion-model Target Generation</h2><p><strong>Authors:Kim Yong Tan, Yueming Lyu, Ivor Tsang, Yew-Soon Ong</strong></p>
<p>Guided diffusion-model generation is a promising direction for customizing the generation process of a pre-trained diffusion-model to address the specific downstream tasks. Existing guided diffusion models either rely on training of the guidance model with pre-collected datasets or require the objective functions to be differentiable. However, for most real-world tasks, the offline datasets are often unavailable, and their objective functions are often not differentiable, such as image generation with human preferences, molecular generation for drug discovery, and material design. Thus, we need an $\textbf{online}$ algorithm capable of collecting data during runtime and supporting a $\textbf{black-box}$ objective function. Moreover, the $\textbf{query efficiency}$ of the algorithm is also critical because the objective evaluation of the query is often expensive in the real-world scenarios. In this work, we propose a novel and simple algorithm, $\textbf{Fast Direct}$, for query-efficient online black-box target generation. Our Fast Direct builds a pseudo-target on the data manifold to update the noise sequence of the diffusion model with a universal direction, which is promising to perform query-efficient guided generation. Extensive experiments on twelve high-resolution ($\small {1024 \times 1024}$) image target generation tasks and six 3D-molecule target generation tasks show $\textbf{6}\times$ up to $\textbf{10}\times$ query efficiency improvement and $\textbf{11}\times$ up to $\textbf{44}\times$ query efficiency improvement, respectively. Our implementation is publicly available at: <a target="_blank" rel="noopener" href="https://github.com/kimyong95/guide-stable-diffusion/tree/fast-direct">https://github.com/kimyong95/guide-stable-diffusion/tree/fast-direct</a> </p>
<blockquote>
<p>å¼•å¯¼æ‰©æ•£æ¨¡å‹ç”Ÿæˆæ˜¯å®šåˆ¶é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆè¿‡ç¨‹ä»¥åº”å¯¹ç‰¹å®šä¸‹æ¸¸ä»»åŠ¡çš„ä¸€ä¸ªå…·æœ‰å‰æ™¯çš„æ–¹å‘ã€‚ç°æœ‰çš„å¼•å¯¼æ‰©æ•£æ¨¡å‹è¦ä¹ˆä¾èµ–äºä½¿ç”¨é¢„å…ˆæ”¶é›†çš„æ•°æ®é›†å¯¹å¼•å¯¼æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œè¦ä¹ˆéœ€è¦ç›®æ ‡å‡½æ•°å¯å¾®ã€‚ç„¶è€Œï¼Œå¯¹äºå¤§å¤šæ•°ç°å®ä¸–ç•Œä»»åŠ¡è€Œè¨€ï¼Œç¦»çº¿æ•°æ®é›†é€šå¸¸ä¸å¯ç”¨ï¼Œå¹¶ä¸”å…¶ç›®æ ‡å‡½æ•°é€šå¸¸ä¸å¯å¾®ï¼Œä¾‹å¦‚å…·æœ‰äººç±»åå¥½çš„å›¾åƒç”Ÿæˆã€ç”¨äºè¯ç‰©å‘ç°çš„åˆ†å­ç”Ÿæˆä»¥åŠææ–™è®¾è®¡ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ç§èƒ½å¤Ÿåœ¨è¿è¡Œæ—¶æ”¶é›†æ•°æ®å¹¶æ”¯æŒé»‘ç®±ç›®æ ‡å‡½æ•°çš„åœ¨çº¿ç®—æ³•ã€‚æ­¤å¤–ï¼Œç®—æ³•çš„æŸ¥è¯¢æ•ˆç‡ä¹Ÿè‡³å…³é‡è¦ï¼Œå› ä¸ºåœ¨ç°å®åœºæ™¯ä¸­ï¼Œç›®æ ‡æŸ¥è¯¢çš„è¯„ä¼°å¾€å¾€æˆæœ¬é«˜æ˜‚ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–ä¸”ç®€å•çš„ç®—æ³•â€œFast Directâ€ï¼Œç”¨äºæŸ¥è¯¢é«˜æ•ˆçš„åœ¨çº¿é»‘ç®±ç›®æ ‡ç”Ÿæˆã€‚æˆ‘ä»¬çš„Fast Directåœ¨æ•°æ®æµå½¢ä¸Šæ„å»ºä¼ªç›®æ ‡ï¼Œä»¥é€šç”¨æ–¹å‘æ›´æ–°æ‰©æ•£æ¨¡å‹çš„å™ªå£°åºåˆ—ï¼Œæœ‰æœ›æ‰§è¡ŒæŸ¥è¯¢é«˜æ•ˆçš„å¼•å¯¼ç”Ÿæˆã€‚åœ¨åäºŒä¸ªé«˜åˆ†è¾¨ç‡ï¼ˆ$1024 \times 1024$ï¼‰å›¾åƒç›®æ ‡ç”Ÿæˆä»»åŠ¡å’Œå…­ä¸ª3Dåˆ†å­ç›®æ ‡ç”Ÿæˆä»»åŠ¡ä¸Šçš„å¤§é‡å®éªŒæ˜¾ç¤ºï¼ŒæŸ¥è¯¢æ•ˆç‡æé«˜äº†6å€è‡³10å€å’Œæé«˜äº†é«˜è¾¾é«˜è¾¾é«˜è¾¾é«˜è¾¾é«˜è¾¾é«˜è¾¾é«˜è¾¾é«˜è¾¾é«˜è¾¾é«˜è¾¾é«˜è¾¾é«˜è¾¾é«˜è¾¾é«˜è¾¾é«˜è¾¾é«˜è¾¾é«˜è¾¾é«˜è¾¾é«˜è¾¾é«˜è¾¾é«˜è¾¾é«˜è¾¾é«˜è¾¾é«˜è¾¾é«˜è¾¾é«˜è¾¾é«˜è¾¾é«˜è¾¾é«˜è¾¾é«˜è¾¾æé«˜æ•ˆç‡å¤§å¤§æé«˜å€å’Œæ”¹è¿›æ”¹å–„äº†æ¯”ä¾‹4ï¼…ã€‚æˆ‘ä»¬çš„å®ç°å…¬å¼€å¯ç”¨åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/kimyong95/guide-stable-diffusion/tree/fast-direct">é“¾æ¥åœ°å€</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01692v4">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>æ‰©æ•£æ¨¡å‹çš„åœ¨çº¿é»‘ç›’ç›®æ ‡ç”Ÿæˆã€‚ç°æœ‰å¼•å¯¼æ‰©æ•£æ¨¡å‹æ–¹æ³•ä¾èµ–é¢„æ”¶é›†æ•°æ®é›†è¿›è¡Œè®­ç»ƒæˆ–è¦æ±‚ç›®æ ‡å‡½æ•°å¯å¾®ã€‚ç„¶è€Œï¼Œå¯¹äºå¤§å¤šæ•°ç°å®ä¸–ç•Œä»»åŠ¡ï¼Œç¦»çº¿æ•°æ®é›†å¸¸ä¸å¯è·å–ä¸”å…¶ç›®æ ‡å‡½æ•°å¸¸ä¸å¯å¾®ï¼Œå¦‚åŸºäºäººç±»åå¥½ç”Ÿæˆå›¾åƒã€è¯ç‰©å‘ç°ä¸­çš„åˆ†å­ç”ŸæˆåŠææ–™è®¾è®¡ã€‚å› æ­¤éœ€è¦èƒ½åœ¨çº¿è¿è¡Œæ—¶æ”¶é›†æ•°æ®å¹¶æ”¯æŒé»‘ç›’ç›®æ ‡å‡½æ•°çš„ç®—æ³•ã€‚æŸ¥è¯¢æ•ˆç‡ä¹Ÿè‡³å…³é‡è¦ï¼Œå› ä¸ºç°å®åœºæ™¯ä¸­ç›®æ ‡è¯„ä¼°å¾€å¾€æ˜‚è´µã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç®€å•è€Œé«˜æ•ˆçš„ç®—æ³•â€œFast Directâ€ï¼Œç”¨äºåœ¨çº¿é»‘ç›’ç›®æ ‡ç”Ÿæˆã€‚Fast Directåœ¨æ•°æ®æµå½¢ä¸Šæ„å»ºä¼ªç›®æ ‡ï¼Œä»¥é€šç”¨æ–¹å‘æ›´æ–°æ‰©æ•£æ¨¡å‹çš„å™ªå£°åºåˆ—ï¼Œæœ‰æœ›è¿›è¡Œé«˜æ•ˆçš„æŸ¥è¯¢å¼•å¯¼ç”Ÿæˆã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨é«˜åˆ†è¾¨ç‡å›¾åƒç›®æ ‡ç”Ÿæˆä»»åŠ¡å’Œä¸‰ç»´åˆ†å­ç›®æ ‡ç”Ÿæˆä»»åŠ¡ä¸­ï¼ŒæŸ¥è¯¢æ•ˆç‡åˆ†åˆ«æé«˜äº†6è‡³10å€å’Œé«˜è¾¾44å€ã€‚ä»£ç å…¬å¼€äºï¼š<a target="_blank" rel="noopener" href="https://github.com/kimyong95/guide-stable-diffusion/tree/fast-direct">é“¾æ¥åœ°å€</a>ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¼•å¯¼æ‰©æ•£æ¨¡å‹ç”Ÿæˆæ˜¯ä¸€ä¸ªé’ˆå¯¹é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„å®šåˆ¶åŒ–ç”Ÿæˆè¿‡ç¨‹çš„æ–¹å‘ï¼Œç”¨äºè§£å†³ç‰¹å®šçš„ä¸‹æ¸¸ä»»åŠ¡ã€‚</li>
<li>ç°æœ‰å¼•å¯¼æ‰©æ•£æ¨¡å‹æ–¹æ³•ä¾èµ–äºé¢„æ”¶é›†æ•°æ®é›†çš„è®­ç»ƒæˆ–è¦æ±‚ç›®æ ‡å‡½æ•°å¯å¾®ï¼Œä½†åœ¨ç°å®ä»»åŠ¡ä¸­è¿™ä¸¤ç‚¹å¸¸æ— æ³•æ»¡è¶³ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ç®€å•è€Œé«˜æ•ˆçš„ç®—æ³•â€œFast Directâ€ï¼Œæ”¯æŒåœ¨çº¿é»‘ç›’ç›®æ ‡ç”Ÿæˆã€‚</li>
<li>Fast Directç®—æ³•åœ¨æ•°æ®æµå½¢ä¸Šæ„å»ºä¼ªç›®æ ‡ï¼Œä»¥é€šç”¨æ–¹å‘æ›´æ–°æ‰©æ•£æ¨¡å‹çš„å™ªå£°åºåˆ—ï¼Œå®ç°æŸ¥è¯¢æ•ˆç‡çš„æé«˜ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒFast Directåœ¨å›¾åƒå’Œåˆ†å­ç”Ÿæˆä»»åŠ¡ä¸­å®ç°äº†æ˜¾è‘—çš„æŸ¥è¯¢æ•ˆç‡æå‡ã€‚</li>
<li>è¯¥ç®—æ³•é€‚ç”¨äºå¤šç§ä»»åŠ¡ç±»å‹ï¼ŒåŒ…æ‹¬å›¾åƒç”Ÿæˆã€åˆ†å­ç”Ÿæˆå’Œææ–™è®¾è®¡ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01692">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-16c1552bb779bed6aa7b94b9ce756e19.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4731e5cbbe45cfbf74389e00db11c3ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-984bf365b19c8f44f7f44c4f877ff087.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Slot-Guided-Adaptation-of-Pre-trained-Diffusion-Models-for-Object-Centric-Learning-and-Compositional-Generation"><a href="#Slot-Guided-Adaptation-of-Pre-trained-Diffusion-Models-for-Object-Centric-Learning-and-Compositional-Generation" class="headerlink" title="Slot-Guided Adaptation of Pre-trained Diffusion Models for   Object-Centric Learning and Compositional Generation"></a>Slot-Guided Adaptation of Pre-trained Diffusion Models for   Object-Centric Learning and Compositional Generation</h2><p><strong>Authors:Adil Kaan Akan, Yucel Yemez</strong></p>
<p>We present SlotAdapt, an object-centric learning method that combines slot attention with pretrained diffusion models by introducing adapters for slot-based conditioning. Our method preserves the generative power of pretrained diffusion models, while avoiding their text-centric conditioning bias. We also incorporate an additional guidance loss into our architecture to align cross-attention from adapter layers with slot attention. This enhances the alignment of our model with the objects in the input image without using external supervision. Experimental results show that our method outperforms state-of-the-art techniques in object discovery and image generation tasks across multiple datasets, including those with real images. Furthermore, we demonstrate through experiments that our method performs remarkably well on complex real-world images for compositional generation, in contrast to other slot-based generative methods in the literature. The project page can be found at <a target="_blank" rel="noopener" href="https://kaanakan.github.io/SlotAdapt/">https://kaanakan.github.io/SlotAdapt/</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†SlotAdaptï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆæ’æ§½æ³¨æ„åŠ›å’Œé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„é¢å‘å¯¹è±¡çš„å­¦ä¹ æ–¹æ³•ï¼Œå®ƒé€šè¿‡å¼•å…¥é€‚é…å™¨æ¥å®ç°åŸºäºæ’æ§½çš„æ¡ä»¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¿ç•™äº†é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ï¼ŒåŒæ—¶é¿å…äº†å…¶é¢å‘æ–‡æœ¬çš„æ¡ä»¶åå·®ã€‚æˆ‘ä»¬è¿˜å°†é¢å¤–çš„æŒ‡å¯¼æŸå¤±çº³å…¥æˆ‘ä»¬çš„æ¶æ„ï¼Œä»¥è°ƒæ•´é€‚é…å™¨å±‚çš„äº¤å‰æ³¨æ„åŠ›ä¸æ’æ§½æ³¨æ„åŠ›ã€‚è¿™æé«˜äº†æˆ‘ä»¬çš„æ¨¡å‹ä¸è¾“å…¥å›¾åƒä¸­çš„å¯¹è±¡çš„å¯¹é½åº¦ï¼Œè€Œæ— éœ€ä½¿ç”¨å¤–éƒ¨ç›‘ç£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„ç‰©ä½“å‘ç°å’Œå›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šçš„è¡¨ç°å‡ä¼˜äºæœ€å…ˆè¿›çš„æŠ€æœ¯ï¼ŒåŒ…æ‹¬çœŸå®å›¾åƒæ•°æ®é›†ã€‚æ­¤å¤–ï¼Œé€šè¿‡å®éªŒè¯æ˜ï¼Œä¸å…¶ä»–åŸºäºæ’æ§½çš„ç”Ÿæˆæ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤æ‚çœŸå®å›¾åƒçš„åˆæˆç”Ÿæˆæ–¹é¢è¡¨ç°å°¤ä¸ºå‡ºè‰²ã€‚é¡¹ç›®é¡µé¢ä½äº<a target="_blank" rel="noopener" href="https://kaanakan.github.io/SlotAdapt/%E3%80%82">https://kaanakan.github.io/SlotAdapt/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.15878v3">PDF</a> Accepted to ICLR2025. Project page:   <a target="_blank" rel="noopener" href="https://kaanakan.github.io/SlotAdapt/">https://kaanakan.github.io/SlotAdapt/</a></p>
<p><strong>Summary</strong></p>
<p>SlotAdaptç»“åˆæ§½ä½æ³¨æ„åŠ›å’Œé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡å¼•å…¥é€‚é…å™¨å®ç°æ§½ä½æ¡ä»¶åŒ–ï¼Œæå‡äº†å›¾åƒç”Ÿæˆå’Œç‰©ä½“å‘ç°ä»»åŠ¡çš„æ•ˆæœã€‚è¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶æ“…é•¿å¤„ç†å¤æ‚çœŸå®å›¾åƒçš„ç»„åˆç”Ÿæˆä»»åŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SlotAdaptç»“åˆäº†æ§½ä½æ³¨æ„åŠ›å’Œé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡å¼•å…¥é€‚é…å™¨å®ç°äº†åŸºäºæ§½ä½çš„æ¡ä»¶åŒ–ã€‚</li>
<li>è¯¥æ–¹æ³•ä¿ç•™äº†é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ï¼Œå¹¶é¿å…äº†æ–‡æœ¬ä¸­å¿ƒåŒ–çš„æ¡ä»¶åŒ–åè§ã€‚</li>
<li>SlotAdapté€šè¿‡é¢å¤–çš„æŒ‡å¯¼æŸå¤±å¢å¼ºäº†æ¨¡å‹ä¸è¾“å…¥å›¾åƒä¸­ç‰©ä½“çš„å¯¹é½åº¦ï¼Œæ— éœ€å¤–éƒ¨ç›‘ç£ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šï¼ŒSlotAdaptåœ¨ç‰©ä½“å‘ç°å’Œå›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šè¡¨ç°å‡ºè¶…è¶Šç°æœ‰æŠ€æœ¯æ°´å¹³çš„æ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å¤æ‚çœŸå®å›¾åƒçš„ç»„åˆç”Ÿæˆä»»åŠ¡ä¸Šè¡¨ç°å°¤ä¸ºå‡ºè‰²ã€‚</li>
<li>SlotAdapté€‚ç”¨äºå¤šç§å›¾åƒç”Ÿæˆåœºæ™¯ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.15878">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a333f39d3ca0541ed298b536aa8c2e1d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4815807fe36efe3507b7ea321af80910.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="StochSync-Stochastic-Diffusion-Synchronization-for-Image-Generation-in-Arbitrary-Spaces"><a href="#StochSync-Stochastic-Diffusion-Synchronization-for-Image-Generation-in-Arbitrary-Spaces" class="headerlink" title="StochSync: Stochastic Diffusion Synchronization for Image Generation in   Arbitrary Spaces"></a>StochSync: Stochastic Diffusion Synchronization for Image Generation in   Arbitrary Spaces</h2><p><strong>Authors:Kyeongmin Yeo, Jaihoon Kim, Minhyuk Sung</strong></p>
<p>We propose a zero-shot method for generating images in arbitrary spaces (e.g., a sphere for 360{\deg} panoramas and a mesh surface for texture) using a pretrained image diffusion model. The zero-shot generation of various visual content using a pretrained image diffusion model has been explored mainly in two directions. First, Diffusion Synchronization-performing reverse diffusion processes jointly across different projected spaces while synchronizing them in the target space-generates high-quality outputs when enough conditioning is provided, but it struggles in its absence. Second, Score Distillation Sampling-gradually updating the target space data through gradient descent-results in better coherence but often lacks detail. In this paper, we reveal for the first time the interconnection between these two methods while highlighting their differences. To this end, we propose StochSync, a novel approach that combines the strengths of both, enabling effective performance with weak conditioning. Our experiments demonstrate that StochSync provides the best performance in 360{\deg} panorama generation (where image conditioning is not given), outperforming previous finetuning-based methods, and also delivers comparable results in 3D mesh texturing (where depth conditioning is provided) with previous methods. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºé¢„è®­ç»ƒå›¾åƒæ‰©æ•£æ¨¡å‹çš„é›¶æ ·æœ¬æ–¹æ³•åœ¨ä»»æ„ç©ºé—´ï¼ˆä¾‹å¦‚ï¼Œç”¨äº360Â°å…¨æ™¯çš„çƒä½“æˆ–ç”¨äºçº¹ç†çš„ç½‘æ ¼è¡¨é¢ï¼‰ç”Ÿæˆå›¾åƒã€‚ä½¿ç”¨é¢„è®­ç»ƒçš„å›¾åƒæ‰©æ•£æ¨¡å‹è¿›è¡Œå„ç§è§†è§‰å†…å®¹çš„é›¶æ ·æœ¬ç”Ÿæˆä¸»è¦æ¢ç´¢äº†ä¸¤ä¸ªæ–¹å‘ã€‚é¦–å…ˆï¼Œæ‰©æ•£åŒæ­¥æ³•é€šè¿‡åœ¨ä¸åŒçš„æŠ•å½±ç©ºé—´ä¸Šæ‰§è¡Œåå‘æ‰©æ•£è¿‡ç¨‹å¹¶åœ¨ç›®æ ‡ç©ºé—´ä¸­åŒæ­¥å®ƒä»¬æ¥ç”Ÿæˆé«˜è´¨é‡è¾“å‡ºï¼Œä½†åœ¨ç¼ºå°‘è¶³å¤Ÿæ¡ä»¶çš„æƒ…å†µä¸‹ä¼šé‡åˆ°å›°éš¾ã€‚å…¶æ¬¡ï¼Œè¯„åˆ†è’¸é¦é‡‡æ ·æ³•é€šè¿‡æ¢¯åº¦ä¸‹é™é€æ­¥æ›´æ–°ç›®æ ‡ç©ºé—´æ•°æ®ï¼Œè™½ç„¶èƒ½å¤Ÿä¿è¯æ›´å¥½çš„è¿è´¯æ€§ï¼Œä½†å¾€å¾€ç¼ºä¹ç»†èŠ‚ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é¦–æ¬¡æ­ç¤ºäº†è¿™ä¸¤ç§æ–¹æ³•ä¹‹é—´çš„ç›¸äº’è”ç³»ï¼ŒåŒæ—¶å¼ºè°ƒäº†å®ƒä»¬çš„å·®å¼‚ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†StochSyncè¿™ä¸€æ–°æ–¹æ³•ï¼Œå®ƒç»“åˆäº†è¿™ä¸¤ç§æ–¹æ³•çš„ä¼˜ç‚¹ï¼Œèƒ½å¤Ÿåœ¨å¼±æ¡ä»¶ä¸‹è¿›è¡Œæœ‰æ•ˆæ€§èƒ½è¡¨ç°ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒStochSyncåœ¨æ— å›¾åƒæ¡ä»¶çš„æƒ…å†µä¸‹è¡¨ç°å‡ºæœ€ä½³çš„360Â°å…¨æ™¯ç”Ÿæˆæ€§èƒ½ï¼Œè¶…è¶Šäº†åŸºäºå¾®è°ƒçš„æ–¹æ³•ï¼Œå¹¶ä¸”åœ¨æä¾›æ·±åº¦æ¡ä»¶çš„æƒ…å†µä¸‹ï¼Œå…¶åœ¨3Dç½‘æ ¼çº¹ç†ç”Ÿæˆæ–¹é¢çš„ç»“æœä¹Ÿä¸ä¹‹å‰çš„æ–¹æ³•ç›¸å½“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.15445v2">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://stochsync.github.io/">https://stochsync.github.io/</a> (ICLR 2025)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºé¢„è®­ç»ƒå›¾åƒæ‰©æ•£æ¨¡å‹çš„é›¶æ ·æœ¬æ–¹æ³•ï¼Œç”¨äºåœ¨ä»»æ„ç©ºé—´ï¼ˆå¦‚çƒä½“å…¨æ™¯å›¾å’Œç½‘æ ¼è¡¨é¢çº¹ç†ï¼‰ç”Ÿæˆå›¾åƒã€‚æ–‡ç« æ¢è®¨äº†ä¸¤ç§ä¸»è¦çš„é›¶æ ·æœ¬ç”Ÿæˆæ–¹å‘ï¼šä¸€æ˜¯é€šè¿‡åŒæ­¥åå‘æ‰©æ•£è¿‡ç¨‹åœ¨ç›®æ ‡ç©ºé—´ä¸­åŒæ­¥ç”Ÿæˆé«˜è´¨é‡è¾“å‡ºï¼›äºŒæ˜¯åœ¨ç›®æ ‡ç©ºé—´ä¸­é€æ­¥æ›´æ–°æ•°æ®ä»¥è·å¾—æ›´å¥½çš„è¿è´¯æ€§ã€‚æœ¬æ–‡é¦–æ¬¡æ­ç¤ºäº†è¿™ä¸¤ç§æ–¹æ³•çš„ç›¸äº’è”ç³»å’Œå·®å¼‚ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•StochSyncï¼Œç»“åˆäº†è¿™ä¸¤ç§æ–¹æ³•çš„ä¼˜ç‚¹ï¼Œåœ¨å¼±æ¡ä»¶ä¸‹å®ç°äº†æœ‰æ•ˆçš„æ€§èƒ½ã€‚å®éªŒè¡¨æ˜ï¼ŒStochSyncåœ¨å…¨æ™¯å›¾ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºæœ€ä½³æ€§èƒ½ï¼Œä¼˜äºåŸºäºå¾®è°ƒçš„æ–¹æ³•ï¼ŒåŒæ—¶åœ¨æä¾›æ·±åº¦æ¡ä»¶çš„ä¸‰ç»´ç½‘æ ¼çº¹ç†ç”Ÿæˆä¸­è¡¨ç°è‰¯å¥½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§åŸºäºé¢„è®­ç»ƒå›¾åƒæ‰©æ•£æ¨¡å‹çš„é›¶æ ·æœ¬æ–¹æ³•åœ¨ä»»æ„ç©ºé—´ç”Ÿæˆå›¾åƒã€‚</li>
<li>æ¢è®¨äº†ä¸¤ç§ä¸»è¦çš„é›¶æ ·æœ¬ç”Ÿæˆæ–¹å‘ï¼šDiffusion Synchronizationå’ŒScore Distillation Samplingï¼Œå¹¶æ­ç¤ºäº†å®ƒä»¬çš„å·®å¼‚å’Œç›¸äº’è”ç³»ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•StochSyncï¼Œç»“åˆäº†Diffusion Synchronizationå’ŒScore Distillation Samplingçš„ä¼˜ç‚¹ã€‚</li>
<li>åœ¨å…¨æ™¯å›¾ç”Ÿæˆæ–¹é¢ï¼ŒStochSyncè¡¨ç°å‡ºæœ€ä½³æ€§èƒ½ï¼Œä¼˜äºåŸºäºå¾®è°ƒçš„æ–¹æ³•ã€‚</li>
<li>åœ¨æä¾›æ·±åº¦æ¡ä»¶çš„ä¸‰ç»´ç½‘æ ¼çº¹ç†ç”Ÿæˆä¸­ï¼ŒStochSyncå…·æœ‰è‰¯å¥½çš„è¡¨ç°ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨å¼±æ¡ä»¶ä¸‹å®ç°æœ‰æ•ˆæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.15445">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-64aab9f2202744790bee3d7a2ff682df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93ab5f8ef50d286503a9e831057f6b6d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4e1c3aa66004fb8455bbccbdacdbb740.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="The-Superposition-of-Diffusion-Models-Using-the-Ito-Density-Estimator"><a href="#The-Superposition-of-Diffusion-Models-Using-the-Ito-Density-Estimator" class="headerlink" title="The Superposition of Diffusion Models Using the ItÃ´ Density Estimator"></a>The Superposition of Diffusion Models Using the ItÃ´ Density Estimator</h2><p><strong>Authors:Marta Skreta, Lazar Atanackovic, Avishek Joey Bose, Alexander Tong, Kirill Neklyudov</strong></p>
<p>The Cambrian explosion of easily accessible pre-trained diffusion models suggests a demand for methods that combine multiple different pre-trained diffusion models without incurring the significant computational burden of re-training a larger combined model. In this paper, we cast the problem of combining multiple pre-trained diffusion models at the generation stage under a novel proposed framework termed superposition. Theoretically, we derive superposition from rigorous first principles stemming from the celebrated continuity equation and design two novel algorithms tailor-made for combining diffusion models in SuperDiff. SuperDiff leverages a new scalable It^o density estimator for the log likelihood of the diffusion SDE which incurs no additional overhead compared to the well-known Hutchinsonâ€™s estimator needed for divergence calculations. We demonstrate that SuperDiff is scalable to large pre-trained diffusion models as superposition is performed solely through composition during inference, and also enjoys painless implementation as it combines different pre-trained vector fields through an automated re-weighting scheme. Notably, we show that SuperDiff is efficient during inference time, and mimics traditional composition operators such as the logical OR and the logical AND. We empirically demonstrate the utility of using SuperDiff for generating more diverse images on CIFAR-10, more faithful prompt conditioned image editing using Stable Diffusion, as well as improved conditional molecule generation and unconditional de novo structure design of proteins. <a target="_blank" rel="noopener" href="https://github.com/necludov/super-diffusion">https://github.com/necludov/super-diffusion</a> </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹çš„æ¶Œç°éå¸¸å®¹æ˜“è·å¾—çš„é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹æ­ç¤ºäº†å¯¹äºç»“åˆå¤šç§ä¸åŒçš„é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„éœ€æ±‚ï¼Œè€Œä¸å¿…æ‰¿å—é‡æ–°è®­ç»ƒæ›´å¤§çš„ç»„åˆæ¨¡å‹å¸¦æ¥çš„å·¨å¤§è®¡ç®—è´Ÿæ‹…ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»“åˆå¤šä¸ªé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„é—®é¢˜ï¼Œè¯¥é—®é¢˜åœ¨ç”Ÿæˆé˜¶æ®µä½äºæ–°æå‡ºçš„æ¡†æ¶ä¸‹ï¼Œç§°ä¸ºå åŠ æ³•ã€‚ç†è®ºä¸Šï¼Œæˆ‘ä»¬ä»è‘—åçš„è¿ç»­æ€§æ–¹ç¨‹å¾—å‡ºäº†ä¸¥æ ¼çš„ç¬¬ä¸€åŸç†æ¨å¯¼å‡ºçš„å åŠ æ³•ï¼Œå¹¶é’ˆå¯¹SuperDiffä¸­çš„æ‰©æ•£æ¨¡å‹è®¾è®¡äº†ä¸¤ç§æ–°é¢–ç®—æ³•ã€‚SuperDiffåˆ©ç”¨ä¸€ç§æ–°çš„å¯æ‰©å±•çš„It^oå¯†åº¦ä¼°è®¡å™¨æ¥è®¡ç®—æ‰©æ•£éšæœºå¾®åˆ†æ–¹ç¨‹çš„å¯¹æ•°ä¼¼ç„¶å€¼ï¼Œä¸ç”¨äºå‘æ•£è®¡ç®—çš„ä¼—æ‰€å‘¨çŸ¥çš„Hutchinsonä¼°è®¡å™¨ç›¸æ¯”ï¼Œæ— éœ€é¢å¤–çš„å¼€é”€ã€‚æˆ‘ä»¬è¯æ˜äº†SuperDiffèƒ½å¤Ÿæ‰©å±•åˆ°å¤§å‹é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ï¼Œå› ä¸ºå åŠ ä»…åœ¨æ¨ç†è¿‡ç¨‹ä¸­é€šè¿‡ç»„åˆå®ç°ï¼Œå¹¶ä¸”é€šè¿‡è‡ªåŠ¨é‡æ–°åŠ æƒæ–¹æ¡ˆç»„åˆä¸åŒçš„é¢„è®­ç»ƒå‘é‡åœºï¼Œå®ç°èµ·æ¥æ¯«ä¸è´¹åŠ›ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬å±•ç¤ºäº†SuperDiffåœ¨æ¨ç†æ—¶é—´çš„é«˜æ•ˆæ€§ï¼Œå¹¶æ¨¡ä»¿äº†ä¼ ç»Ÿçš„ç»„åˆè¿ç®—ç¬¦ï¼Œå¦‚é€»è¾‘æˆ–å’Œé€»è¾‘ä¸ã€‚æˆ‘ä»¬é€šè¿‡ç»éªŒè¯æ˜äº†åœ¨CIFAR-10ä¸Šç”Ÿæˆæ›´å¤šä¸åŒå›¾åƒã€ä½¿ç”¨ç¨³å®šæ‰©æ•£è¿›è¡Œæ›´çœŸå®çš„æç¤ºæ¡ä»¶å›¾åƒç¼–è¾‘ä»¥åŠæ”¹è¿›æ¡ä»¶åˆ†å­ç”Ÿæˆå’Œæ— æ¡ä»¶è›‹ç™½è´¨å…¨æ–°ç»“æ„è®¾è®¡çš„å®ç”¨æ€§ã€‚è¯¦æƒ…è¯·è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/necludov/super-diffusion">https://github.com/necludov/super-diffusion</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17762v2">PDF</a> Accepted as a Spotlight Presentation at the International Conference   on Learning Representations 2025</p>
<p><strong>Summary</strong><br>     è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºSuperDiffçš„æ–°æ¡†æ¶ï¼Œç”¨äºåœ¨ç”Ÿæˆé˜¶æ®µç»„åˆå¤šä¸ªé¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ã€‚SuperDiffåˆ©ç”¨å¯æ‰©å±•çš„ItÅå¯†åº¦ä¼°è®¡å™¨è®¡ç®—æ‰©æ•£éšæœºå¾®åˆ†æ–¹ç¨‹çš„å¯¹æ•°ä¼¼ç„¶å€¼ï¼Œæ— éœ€é¢å¤–çš„å¼€é”€ã€‚SuperDiffé€šè¿‡ç»„åˆä¸åŒçš„é¢„è®­ç»ƒå‘é‡åœºå®ç°è‡ªåŠ¨é‡æ–°åŠ æƒï¼Œå¯è½»æ¾å®æ–½ï¼Œä¸”æ¨ç†æ•ˆç‡é«˜ã€‚æ­¤å¤–ï¼ŒSuperDiffèƒ½å¤Ÿç”Ÿæˆæ›´å¤šæ ·åŒ–çš„å›¾åƒï¼Œæ›´å¿ å®äºæç¤ºæ¡ä»¶çš„å›¾åƒç¼–è¾‘ï¼Œä»¥åŠæ”¹è¿›çš„æ¡ä»¶åˆ†å­ç”Ÿæˆå’Œæ— æ¡ä»¶è›‹ç™½è´¨è®¾è®¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SuperDiffæ¡†æ¶å…è®¸ç»„åˆå¤šä¸ªé¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ï¼Œæ— éœ€æ‰¿æ‹…é‡æ–°è®­ç»ƒå¤§å‹ç»„åˆæ¨¡å‹çš„é‡å¤§è®¡ç®—è´Ÿæ‹…ã€‚</li>
<li>SuperDiffåˆ©ç”¨ItÅå¯†åº¦ä¼°è®¡å™¨è®¡ç®—æ‰©æ•£éšæœºå¾®åˆ†æ–¹ç¨‹çš„å¯¹æ•°ä¼¼ç„¶å€¼ï¼Œå…·æœ‰å¯æ‰©å±•æ€§ã€‚</li>
<li>é€šè¿‡ç»„åˆä¸åŒçš„é¢„è®­ç»ƒå‘é‡åœºï¼ŒSuperDiffå®ç°äº†è‡ªåŠ¨é‡æ–°åŠ æƒï¼Œä½¿å¾—å®æ–½æ›´ä¸ºè½»æ¾ã€‚</li>
<li>SuperDiffåœ¨æ¨ç†é˜¶æ®µæ•ˆç‡é«˜ï¼Œæ¨¡ä»¿äº†é€»è¾‘ORå’ŒANDç­‰ä¼ ç»Ÿçš„ç»„åˆè¿ç®—ç¬¦ã€‚</li>
<li>SuperDiffèƒ½å¤Ÿç”Ÿæˆæ›´å¤šæ ·åŒ–çš„å›¾åƒï¼Œå¦‚åœ¨CIFAR-10ä¸Šçš„è¡¨ç°ã€‚</li>
<li>SuperDiffèƒ½å¤Ÿæ›´å¿ å®äºæç¤ºæ¡ä»¶çš„å›¾åƒç¼–è¾‘ï¼Œå¦‚åœ¨Stable Diffusionä¸­çš„åº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17762">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b19baefb484e7a64f9f78cf1b1efecb6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bff0c8e9945edc75c4df69534c65dab5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-86e5f9392a0e8673934433b5fa87e585.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Autoregressive-Video-Generation-without-Vector-Quantization"><a href="#Autoregressive-Video-Generation-without-Vector-Quantization" class="headerlink" title="Autoregressive Video Generation without Vector Quantization"></a>Autoregressive Video Generation without Vector Quantization</h2><p><strong>Authors:Haoge Deng, Ting Pan, Haiwen Diao, Zhengxiong Luo, Yufeng Cui, Huchuan Lu, Shiguang Shan, Yonggang Qi, Xinlong Wang</strong></p>
<p>This paper presents a novel approach that enables autoregressive video generation with high efficiency. We propose to reformulate the video generation problem as a non-quantized autoregressive modeling of temporal frame-by-frame prediction and spatial set-by-set prediction. Unlike raster-scan prediction in prior autoregressive models or joint distribution modeling of fixed-length tokens in diffusion models, our approach maintains the causal property of GPT-style models for flexible in-context capabilities, while leveraging bidirectional modeling within individual frames for efficiency. With the proposed approach, we train a novel video autoregressive model without vector quantization, termed NOVA. Our results demonstrate that NOVA surpasses prior autoregressive video models in data efficiency, inference speed, visual fidelity, and video fluency, even with a much smaller model capacity, i.e., 0.6B parameters. NOVA also outperforms state-of-the-art image diffusion models in text-to-image generation tasks, with a significantly lower training cost. Additionally, NOVA generalizes well across extended video durations and enables diverse zero-shot applications in one unified model. Code and models are publicly available at <a target="_blank" rel="noopener" href="https://github.com/baaivision/NOVA">https://github.com/baaivision/NOVA</a>. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°è¿›è¡Œè‡ªå›å½’è§†é¢‘ç”Ÿæˆã€‚æˆ‘ä»¬æè®®å°†è§†é¢‘ç”Ÿæˆé—®é¢˜é‡æ–°è¡¨è¿°ä¸ºæœªé‡åŒ–çš„è‡ªå›å½’å»ºæ¨¡ï¼ŒåŒ…æ‹¬æ—¶é—´ä¸Šçš„é€å¸§é¢„æµ‹å’Œç©ºé—´ä¸Šçš„é›†é›†åˆé¢„æµ‹ã€‚ä¸å…ˆå‰è‡ªå›å½’æ¨¡å‹ä¸­çš„æ …æ ¼æ‰«æé¢„æµ‹æˆ–æ‰©æ•£æ¨¡å‹ä¸­çš„å›ºå®šé•¿åº¦æ ‡è®°çš„è”åˆåˆ†å¸ƒå»ºæ¨¡ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¿æŒäº†GPTé£æ ¼æ¨¡å‹çš„å› æœç‰¹æ€§ï¼Œä»¥å®ç°çµæ´»çš„ä¸Šä¸‹æ–‡åŠŸèƒ½ï¼ŒåŒæ—¶åˆ©ç”¨å•ä¸ªå¸§å†…çš„åŒå‘å»ºæ¨¡æ¥æé«˜æ•ˆç‡ã€‚é€šè¿‡è¯¥æ–¹æ³•ï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ç§æ— éœ€å‘é‡é‡åŒ–çš„æ–°å‹è§†é¢‘è‡ªå›å½’æ¨¡å‹ï¼Œç§°ä¸ºNOVAã€‚ç»“æœè¡¨æ˜ï¼ŒNOVAåœ¨æ•°æ®æ•ˆç‡ã€æ¨ç†é€Ÿåº¦ã€è§†è§‰ä¿çœŸåº¦å’Œè§†é¢‘æµç•…æ€§æ–¹é¢è¶…è¶Šäº†å…ˆå‰çš„è‡ªå›å½’è§†é¢‘æ¨¡å‹ï¼Œå³ä½¿æ¨¡å‹å®¹é‡å°å¾—å¤šï¼Œä¹Ÿåªæœ‰0.6Bå‚æ•°ã€‚æ­¤å¤–ï¼ŒNOVAåœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºæœ€å…ˆè¿›çš„å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œå¹¶ä¸”è®­ç»ƒæˆæœ¬æ˜¾è‘—é™ä½ã€‚NOVAè¿˜èƒ½å¾ˆå¥½åœ°é€‚åº”é•¿æ—¶é—´çš„è§†é¢‘ï¼Œå¹¶åœ¨ä¸€ä¸ªç»Ÿä¸€çš„æ¨¡å‹ä¸­å®ç°äº†å¤šæ ·åŒ–çš„é›¶æ ·æœ¬åº”ç”¨ã€‚ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/baaivision/NOVA%E4%B8%8A%E5%85%AC%E5%BC%80%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/baaivision/NOVAä¸Šå…¬å¼€è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14169v2">PDF</a> Accepted to ICLR 2025. Project page at   <a target="_blank" rel="noopener" href="https://github.com/baaivision/NOVA">https://github.com/baaivision/NOVA</a></p>
<p><strong>Summary</strong><br>æ‰©æ•£æ¨¡å‹çš„æ–°æ–¹æ³•å¯ä»¥å®ç°é«˜æ•ˆè‡ªå›å½’è§†é¢‘ç”Ÿæˆã€‚è¯¥ç ”ç©¶å°†è§†é¢‘ç”Ÿæˆé—®é¢˜é‡æ–°å®šä¹‰ä¸ºéé‡åŒ–è‡ªå›å½’å»ºæ¨¡ï¼ŒåŒ…æ‹¬æ—¶é—´å¸§é¢„æµ‹å’Œç©ºæ—¶é›†åˆé¢„æµ‹ã€‚ä¸å…ˆå‰çš„è‡ªå›å½’æ¨¡å‹ä¸­çš„å…‰æ …æ‰«æé¢„æµ‹æˆ–æ‰©æ•£æ¨¡å‹çš„å›ºå®šé•¿åº¦æ ‡è®°çš„è”åˆåˆ†å¸ƒå»ºæ¨¡ä¸åŒï¼Œè¯¥æ–¹æ³•ä¿ç•™äº†GPTé£æ ¼çš„æ¨¡å‹çš„ä¸Šä¸‹æ–‡çµæ´»æ€§ï¼ŒåŒæ—¶åˆ©ç”¨å•ä¸ªå¸§å†…çš„åŒå‘å»ºæ¨¡æé«˜æ•ˆç‡ã€‚è¯¥æ–¹æ³•è®­ç»ƒäº†ä¸€ç§åä¸ºNOVAçš„æ–°å‹è§†é¢‘è‡ªå›å½’æ¨¡å‹ï¼Œæ— éœ€çŸ¢é‡é‡åŒ–ã€‚ç»“æœè¯æ˜ï¼ŒNOVAåœ¨æ•°æ®æ•ˆç‡ã€æ¨ç†é€Ÿåº¦ã€è§†è§‰ä¿çœŸåº¦å’Œè§†é¢‘æµç•…æ€§æ–¹é¢è¶…è¶Šäº†å…ˆå‰çš„è‡ªå›å½’è§†é¢‘æ¨¡å‹ï¼Œå³ä½¿æ¨¡å‹å®¹é‡è¾ƒå°ï¼ˆå³0.6äº¿å‚æ•°ï¼‰ã€‚NOVAè¿˜ä¼˜äºæœ€å…ˆè¿›çš„å›¾åƒæ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå¹¶ä¸”è®­ç»ƒæˆæœ¬æ˜¾è‘—é™ä½ã€‚æ­¤å¤–ï¼ŒNOVAåœ¨æ‰©å±•è§†é¢‘æ—¶é•¿æ–¹é¢å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨ä¸€ä¸ªç»Ÿä¸€æ¨¡å‹ä¸­å®ç°äº†å¤šæ ·åŒ–çš„é›¶æ ·æœ¬åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>1.è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹è‡ªå›å½’è§†é¢‘ç”Ÿæˆæ–¹æ³•ï¼ŒåŸºäºéé‡åŒ–è‡ªå›å½’å»ºæ¨¡ã€‚<br>2.è¯¥ç ”ç©¶å°†è§†é¢‘ç”Ÿæˆé—®é¢˜åˆ†è§£ä¸ºæ—¶é—´å¸§é¢„æµ‹å’Œç©ºæ—¶é›†åˆé¢„æµ‹ã€‚<br>3.æ–°æ–¹æ³•ä¿ç•™äº†GPTé£æ ¼æ¨¡å‹çš„ä¸Šä¸‹æ–‡çµæ´»æ€§ï¼Œå¹¶ç»“åˆäº†å¸§å†…çš„åŒå‘å»ºæ¨¡ä»¥æé«˜æ•ˆç‡ã€‚<br>4.å¼•å…¥äº†ä¸€ç§æ–°å‹è§†é¢‘è‡ªå›å½’æ¨¡å‹NOVAï¼Œæ— éœ€çŸ¢é‡é‡åŒ–ã€‚<br>5.NOVAåœ¨æ•°æ®æ•ˆç‡ã€æ¨ç†é€Ÿåº¦ã€è§†è§‰ä¿çœŸåº¦å’Œè§†é¢‘æµç•…æ€§æ–¹é¢è¶…è¶Šäº†å…¶ä»–è‡ªå›å½’è§†é¢‘æ¨¡å‹ã€‚<br>6.NOVAåœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºæœ€å…ˆè¿›çš„å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œä¸”è®­ç»ƒæˆæœ¬è¾ƒä½ã€‚</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14169">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-39ce9d1bd2f2b9f9c35c2231e9a2cfe5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-103e52e906eb991c230dc43edbeedb55.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-39b4ca274e85d1612b4b947cc85a153f.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="V2X-R-Cooperative-LiDAR-4D-Radar-Fusion-for-3D-Object-Detection-with-Denoising-Diffusion"><a href="#V2X-R-Cooperative-LiDAR-4D-Radar-Fusion-for-3D-Object-Detection-with-Denoising-Diffusion" class="headerlink" title="V2X-R: Cooperative LiDAR-4D Radar Fusion for 3D Object Detection with   Denoising Diffusion"></a>V2X-R: Cooperative LiDAR-4D Radar Fusion for 3D Object Detection with   Denoising Diffusion</h2><p><strong>Authors:Xun Huang, Jinlong Wang, Qiming Xia, Siheng Chen, Bisheng Yang, Xin Li, Cheng Wang, Chenglu Wen</strong></p>
<p>Current Vehicle-to-Everything (V2X) systems have significantly enhanced 3D object detection using LiDAR and camera data. However, these methods suffer from performance degradation in adverse weather conditions. The weatherrobust 4D radar provides Doppler and additional geometric information, raising the possibility of addressing this challenge. To this end, we present V2X-R, the first simulated V2X dataset incorporating LiDAR, camera, and 4D radar. V2X-R contains 12,079 scenarios with 37,727 frames of LiDAR and 4D radar point clouds, 150,908 images, and 170,859 annotated 3D vehicle bounding boxes. Subsequently, we propose a novel cooperative LiDAR-4D radar fusion pipeline for 3D object detection and implement it with various fusion strategies. To achieve weather-robust detection, we additionally propose a Multi-modal Denoising Diffusion (MDD) module in our fusion pipeline. MDD utilizes weather-robust 4D radar feature as a condition to prompt the diffusion model to denoise noisy LiDAR features. Experiments show that our LiDAR-4D radar fusion pipeline demonstrates superior performance in the V2X-R dataset. Over and above this, our MDD module further improved the performance of basic fusion model by up to 5.73%&#x2F;6.70% in foggy&#x2F;snowy conditions with barely disrupting normal performance. The dataset and code will be publicly available at: <a target="_blank" rel="noopener" href="https://github.com/ylwhxht/V2X-R">https://github.com/ylwhxht/V2X-R</a>. </p>
<blockquote>
<p>å½“å‰çš„è½¦å¯¹å¤–ç•Œï¼ˆV2Xï¼‰ç³»ç»Ÿå·²ç»é€šè¿‡æ¿€å…‰é›·è¾¾å’Œç›¸æœºæ•°æ®æ˜¾è‘—å¢å¼ºäº†3Då¯¹è±¡æ£€æµ‹åŠŸèƒ½ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨æ¶åŠ£å¤©æ°”æ¡ä»¶ä¸‹ä¼šå‡ºç°æ€§èƒ½ä¸‹é™ã€‚å¤©æ°”ç¨³å®šçš„4Dé›·è¾¾æä¾›äº†å¤šæ™®å‹’å’Œé¢å¤–çš„å‡ ä½•ä¿¡æ¯ï¼Œä¸ºè§£å†³è¿™ä¸€æŒ‘æˆ˜æä¾›äº†å¯èƒ½æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†V2X-Rï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç»“åˆäº†æ¿€å…‰é›·è¾¾ã€ç›¸æœºå’Œ4Dé›·è¾¾çš„æ¨¡æ‹ŸV2Xæ•°æ®é›†ã€‚V2X-RåŒ…å«12,079ä¸ªåœºæ™¯ï¼Œå…¶ä¸­åŒ…æ‹¬æ¿€å…‰é›·è¾¾å’Œ4Dé›·è¾¾ç‚¹äº‘37,727å¸§ã€å›¾åƒ150,908å¼ ä»¥åŠæ ‡æ³¨çš„3Dè½¦è¾†è¾¹ç•Œæ¡†170,859ä¸ªã€‚éšåï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äº3Då¯¹è±¡æ£€æµ‹çš„æ–°å‹åˆä½œæ¿€å…‰é›·è¾¾-4Dé›·è¾¾èåˆç®¡é“ï¼Œå¹¶é‡‡ç”¨äº†å¤šç§èåˆç­–ç•¥æ¥å®ç°å®ƒã€‚ä¸ºäº†å®ç°å¤©æ°”ç¨³å®šçš„æ£€æµ‹ï¼Œæˆ‘ä»¬åœ¨èåˆç®¡é“ä¸­é¢å¤–æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€å»å™ªæ‰©æ•£ï¼ˆMDDï¼‰æ¨¡å—ã€‚MDDåˆ©ç”¨å¤©æ°”ç¨³å®šçš„4Dé›·è¾¾ç‰¹å¾ä½œä¸ºæ¡ä»¶ï¼Œæç¤ºæ‰©æ•£æ¨¡å‹å¯¹å˜ˆæ‚çš„æ¿€å…‰é›·è¾¾ç‰¹å¾è¿›è¡Œå»å™ªã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¿€å…‰é›·è¾¾-4Dé›·è¾¾èåˆç®¡é“åœ¨V2X-Ræ•°æ®é›†ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œæˆ‘ä»¬çš„MDDæ¨¡å—è¿›ä¸€æ­¥æé«˜äº†åŸºæœ¬èåˆæ¨¡å‹åœ¨é›¾å¤©å’Œé›ªå¤©çš„æ€§èƒ½ï¼ŒåŒæ—¶å‡ ä¹ä¸å½±å“æ­£å¸¸æ€§èƒ½ã€‚æ•°æ®é›†å’Œä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/ylwhxht/V2X-R%E4%B8%8A%E5%85%AC%E5%BC%80%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/ylwhxht/V2X-Rä¸Šå…¬å¼€æä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.08402v3">PDF</a> Accepted by CVPR2025</p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶é’ˆå¯¹ç°æœ‰è½¦è¾†åˆ°ä¸‡ç‰©ï¼ˆV2Xï¼‰ç³»ç»Ÿä¸­ä½¿ç”¨çš„LiDARå’Œæ‘„åƒæœºæ•°æ®åœ¨æ¶åŠ£å¤©æ°”æ¡ä»¶ä¸‹æ€§èƒ½ä¸‹é™çš„é—®é¢˜ï¼Œå¼•å…¥äº†4Dé›·è¾¾æ•°æ®ã€‚ç ”ç©¶å›¢é˜Ÿåˆ›å»ºäº†é¦–ä¸ªé›†æˆLiDARã€æ‘„åƒæœºå’Œ4Dé›·è¾¾çš„æ¨¡æ‹ŸV2Xæ•°æ®é›†V2X-Rã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§æ–°å‹çš„åŸºäºLiDARä¸4Dé›·è¾¾çš„åˆä½œèåˆç®¡é“ï¼Œç”¨äº3Då¯¹è±¡æ£€æµ‹ï¼Œå¹¶å®ç°äº†å¤šç§èåˆç­–ç•¥ã€‚ä¸ºæé«˜å¤©æ°”é€‚åº”æ€§æ£€æµ‹æ€§èƒ½ï¼Œè¯¥å›¢é˜Ÿåœ¨èåˆç®¡é“ä¸­å¼•å…¥äº†å¤šæ¨¡å¼å»å™ªæ‰©æ•£ï¼ˆMDDï¼‰æ¨¡å—ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥èåˆç®¡é“åœ¨V2X-Ræ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œè€ŒMDDæ¨¡å—è¿›ä¸€æ­¥æé«˜äº†åŸºæœ¬èåˆæ¨¡å‹åœ¨é›¾å¤©å’Œé›ªå¤©çš„æ€§èƒ½ï¼ŒåŒæ—¶å‡ ä¹ä¸å½±å“æ­£å¸¸æ€§èƒ½ã€‚æ•°æ®é›†å’Œä»£ç å·²å…¬å¼€å‘å¸ƒåœ¨GitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰V2Xç³»ç»Ÿè™½åˆ©ç”¨LiDARå’Œæ‘„åƒæœºæ•°æ®å®ç°äº†3Då¯¹è±¡æ£€æµ‹çš„æ˜¾è‘—å¢å¼ºï¼Œä½†åœ¨æ¶åŠ£å¤©æ°”æ¡ä»¶ä¸‹æ€§èƒ½ä¸‹é™ã€‚</li>
<li>4Dé›·è¾¾æä¾›å¤šæ™®å‹’å’Œé¢å¤–çš„å‡ ä½•ä¿¡æ¯ï¼Œæœ‰åŠ©äºè§£å†³æ¶åŠ£å¤©æ°”ä¸‹çš„æ£€æµ‹æŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥äº†é¦–ä¸ªé›†æˆLiDARã€æ‘„åƒæœºå’Œ4Dé›·è¾¾çš„æ¨¡æ‹ŸV2Xæ•°æ®é›†V2X-Rã€‚</li>
<li>æå‡ºäº†æ–°å‹çš„åŸºäºLiDARä¸4Dé›·è¾¾çš„åˆä½œèåˆç®¡é“ï¼Œç”¨äº3Då¯¹è±¡æ£€æµ‹ï¼Œå¹¶å®ç°äº†å¤šç§èåˆç­–ç•¥ã€‚</li>
<li>å¤šæ¨¡å¼å»å™ªæ‰©æ•£ï¼ˆMDDï¼‰æ¨¡å—è¢«å¼•å…¥ä»¥æé«˜èåˆç®¡é“çš„å¤©æ°”é€‚åº”æ€§æ£€æµ‹æ€§èƒ½ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼ŒLiDAR-4Dé›·è¾¾èåˆç®¡é“åœ¨V2X-Ræ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.08402">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-e9bc3a6c16e9700cbc864ec2358bd9be.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-83c296cbea80b277891cdbd2e130c0f7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4331b64422430d5b8389da3fa7397789.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-06920c780ad59a7a8423ae6ffafa0cdb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19fedd2365742e611bbdc6df65edef3f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-95311d4ef225749d92448be876d6cfb8.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="OFER-Occluded-Face-Expression-Reconstruction"><a href="#OFER-Occluded-Face-Expression-Reconstruction" class="headerlink" title="OFER: Occluded Face Expression Reconstruction"></a>OFER: Occluded Face Expression Reconstruction</h2><p><strong>Authors:Pratheba Selvaraju, Victoria Fernandez Abrevaya, Timo Bolkart, Rick Akkerman, Tianyu Ding, Faezeh Amjadi, Ilya Zharkov</strong></p>
<p>Reconstructing 3D face models from a single image is an inherently ill-posed problem, which becomes even more challenging in the presence of occlusions. In addition to fewer available observations, occlusions introduce an extra source of ambiguity where multiple reconstructions can be equally valid. Despite the ubiquity of the problem, very few methods address its multi-hypothesis nature. In this paper we introduce OFER, a novel approach for single-image 3D face reconstruction that can generate plausible, diverse, and expressive 3D faces, even under strong occlusions. Specifically, we train two diffusion models to generate the shape and expression coefficients of a face parametric model, conditioned on the input image. This approach captures the multi-modal nature of the problem, generating a distribution of solutions as output. However, to maintain consistency across diverse expressions, the challenge is to select the best matching shape. To achieve this, we propose a novel ranking mechanism that sorts the outputs of the shape diffusion network based on predicted shape accuracy scores. We evaluate our method using standard benchmarks and introduce CO-545, a new protocol and dataset designed to assess the accuracy of expressive faces under occlusion. Our results show improved performance over occlusion-based methods, while also enabling the generation of diverse expressions for a given image. </p>
<blockquote>
<p>ä»å•å¹…å›¾åƒé‡å»º3Däººè„¸æ¨¡å‹æ˜¯ä¸€ä¸ªæœ¬è´¨ä¸Šä¸é€‚å®šçš„é—®é¢˜ï¼Œåœ¨å­˜åœ¨é®æŒ¡çš„æƒ…å†µä¸‹ï¼Œè¿™ä¸ªé—®é¢˜å˜å¾—æ›´åŠ å…·æœ‰æŒ‘æˆ˜æ€§ã€‚é™¤äº†å¯ç”¨çš„è§‚å¯Ÿæ•°æ®è¾ƒå°‘ä¹‹å¤–ï¼Œé®æŒ¡å¼•å…¥äº†é¢å¤–çš„æ¨¡ç³Šæ€§æ¥æºï¼Œå…¶ä¸­å¤šç§é‡å»ºå¯èƒ½æ˜¯åŒæ ·æœ‰æ•ˆçš„ã€‚å°½ç®¡è¿™ä¸ªé—®é¢˜æ™®éå­˜åœ¨ï¼Œä½†å¾ˆå°‘æœ‰æ–¹æ³•è§£å†³å…¶å¤šå‡è®¾æ€§è´¨ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†OFERï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºå•å›¾åƒ3Däººè„¸é‡å»ºçš„æ–°æ–¹æ³•ï¼Œå¯ä»¥ç”Ÿæˆåˆç†ã€å¤šæ ·ã€å¯Œæœ‰è¡¨ç°åŠ›çš„3Däººè„¸ï¼Œå³ä½¿åœ¨å¼ºé®æŒ¡ä¸‹ä¹Ÿæ˜¯å¦‚æ­¤ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸¤ä¸ªæ‰©æ•£æ¨¡å‹æ¥ç”Ÿæˆäººè„¸å‚æ•°æ¨¡å‹çš„å½¢çŠ¶å’Œè¡¨æƒ…ç³»æ•°ï¼Œä»¥è¾“å…¥å›¾åƒä¸ºæ¡ä»¶ã€‚è¿™ç§æ–¹æ³•æ•æ‰äº†é—®é¢˜çš„å¤šæ¨¡æ€æ€§è´¨ï¼Œç”Ÿæˆä¸€ç³»åˆ—è§£å†³æ–¹æ¡ˆä½œä¸ºè¾“å‡ºã€‚ç„¶è€Œï¼Œä¸ºäº†ä¿æŒå„ç§è¡¨æƒ…çš„ä¸€è‡´æ€§ï¼ŒæŒ‘æˆ˜åœ¨äºé€‰æ‹©æœ€ä½³åŒ¹é…çš„å½¢çŠ¶ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ’åæœºåˆ¶ï¼Œæ ¹æ®é¢„æµ‹çš„å½¢çŠ¶å‡†ç¡®åº¦å¾—åˆ†å¯¹å½¢çŠ¶æ‰©æ•£ç½‘ç»œçš„è¾“å‡ºè¿›è¡Œæ’åºã€‚æˆ‘ä»¬ä½¿ç”¨æ ‡å‡†åŸºå‡†å¯¹æˆ‘ä»¬çš„æ–¹æ³•è¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶å¼•å…¥äº†CO-545ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„åè®®å’Œæ•°æ®é›†ï¼Œæ—¨åœ¨è¯„ä¼°é®æŒ¡ä¸‹è¡¨æƒ…è„¸çš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬çš„ç»“æœæ˜¾ç¤ºäº†åœ¨åŸºäºé®æŒ¡çš„æ–¹æ³•ä¸Šçš„æ€§èƒ½æå‡ï¼ŒåŒæ—¶ä¸ºç»™å®šå›¾åƒç”Ÿæˆäº†å¤šæ ·åŒ–çš„è¡¨æƒ…ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.21629v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºOFERçš„æ–°å‹å•å›¾åƒä¸‰ç»´é¢éƒ¨é‡å»ºæ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆé€¼çœŸçš„ã€å¤šæ ·åŒ–çš„ã€å¯Œæœ‰è¡¨ç°åŠ›çš„ä¸‰ç»´é¢éƒ¨æ¨¡å‹ï¼Œå³ä½¿åœ¨å¼ºé®æŒ¡æ¡ä»¶ä¸‹ä¹Ÿèƒ½å¦‚æ­¤ã€‚é€šè¿‡è®­ç»ƒä¸¤ä¸ªæ‰©æ•£æ¨¡å‹æ¥ç”Ÿæˆé¢éƒ¨å‚æ•°æ¨¡å‹çš„å½¢çŠ¶å’Œè¡¨æƒ…ç³»æ•°ï¼Œä»¥è¾“å…¥å›¾åƒä¸ºæ¡ä»¶ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæ•æ‰é—®é¢˜çš„å¤šæ¨¡æ€æ€§è´¨ï¼Œç”Ÿæˆè§£å†³æ–¹æ¡ˆçš„åˆ†å¸ƒã€‚ä¸ºäº†ä¿æŒå„ç§è¡¨æƒ…çš„ä¸€è‡´æ€§ï¼ŒæŒ‘æˆ˜åœ¨äºé€‰æ‹©æœ€ä½³åŒ¹é…çš„å½¢çŠ¶ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ’åºæœºåˆ¶ï¼Œæ ¹æ®é¢„æµ‹çš„å‡†ç¡®å½¢çŠ¶åˆ†æ•°å¯¹å½¢çŠ¶æ‰©æ•£ç½‘ç»œçš„è¾“å‡ºè¿›è¡Œæ’åºã€‚ä½¿ç”¨æ ‡å‡†åŸºå‡†è¿›è¡Œäº†è¯„ä¼°ã€‚ç»“æœæ˜¾ç¤ºä¸é®æŒ¡çš„æ–¹æ³•ç›¸æ¯”å…·æœ‰æ›´é«˜çš„æ€§èƒ½ï¼Œå¹¶èƒ½å¤Ÿåœ¨ç»™å®šçš„å›¾åƒä¸Šç”Ÿæˆå¤šç§è¡¨æƒ…ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OFERæ˜¯ä¸€ç§ç”¨äºä»å•å›¾åƒè¿›è¡Œä¸‰ç»´é¢éƒ¨é‡å»ºçš„æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨é®æŒ¡æ¡ä»¶ä¸‹ç”Ÿæˆé€¼çœŸã€å¤šæ ·åŒ–å’Œå¯Œæœ‰è¡¨ç°åŠ›çš„ä¸‰ç»´é¢éƒ¨æ¨¡å‹ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡è®­ç»ƒä¸¤ä¸ªæ‰©æ•£æ¨¡å‹æ¥ç”Ÿæˆé¢éƒ¨å‚æ•°æ¨¡å‹çš„å½¢çŠ¶å’Œè¡¨æƒ…ç³»æ•°ï¼Œä»¥è¾“å…¥å›¾åƒä¸ºæ¡ä»¶ï¼Œæ•æ‰é—®é¢˜çš„å¤šæ¨¡æ€æ€§è´¨ã€‚</li>
<li>ä¸ºäº†ä¿æŒå„ç§è¡¨æƒ…çš„ä¸€è‡´æ€§ï¼Œæå‡ºäº†ä¸€ä¸ªæ–°é¢–çš„æ’åºæœºåˆ¶æ¥æ ¹æ®é¢„æµ‹çš„å‡†ç¡®å½¢çŠ¶åˆ†æ•°å¯¹å½¢çŠ¶æ‰©æ•£ç½‘ç»œçš„è¾“å‡ºè¿›è¡Œæ’åºã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨æ ‡å‡†åŸºå‡†ä¸Šçš„è¯„ä¼°è¡¨ç°å‡ºè‰²ï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªæ–°çš„åè®®å’Œæ•°æ®é›†CO-545æ¥è¯„ä¼°é®æŒ¡æ¡ä»¶ä¸‹çš„è¡¨æƒ…å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.21629">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6d99a207d9ba2b3dc8bc30419ca9c7e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6d9ce9edb53bed47c9d8692789b9a7fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d361296c237bf1f826082f50f180ccf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9639c65e0159b28caae6b51ed5e7a281.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-603b2aee5e81c5854f928f896b994576.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Probing-the-Latent-Hierarchical-Structure-of-Data-via-Diffusion-Models"><a href="#Probing-the-Latent-Hierarchical-Structure-of-Data-via-Diffusion-Models" class="headerlink" title="Probing the Latent Hierarchical Structure of Data via Diffusion Models"></a>Probing the Latent Hierarchical Structure of Data via Diffusion Models</h2><p><strong>Authors:Antonio Sclocchi, Alessandro Favero, Noam Itzhak Levi, Matthieu Wyart</strong></p>
<p>High-dimensional data must be highly structured to be learnable. Although the compositional and hierarchical nature of data is often put forward to explain learnability, quantitative measurements establishing these properties are scarce. Likewise, accessing the latent variables underlying such a data structure remains a challenge. In this work, we show that forward-backward experiments in diffusion-based models, where data is noised and then denoised to generate new samples, are a promising tool to probe the latent structure of data. We predict in simple hierarchical models that, in this process, changes in data occur by correlated chunks, with a length scale that diverges at a noise level where a phase transition is known to take place. Remarkably, we confirm this prediction in both text and image datasets using state-of-the-art diffusion models. Our results show how latent variable changes manifest in the data and establish how to measure these effects in real data using diffusion models. </p>
<blockquote>
<p>é«˜ç»´æ•°æ®å¿…é¡»é«˜åº¦ç»“æ„åŒ–æ‰èƒ½è¿›è¡Œå­¦ä¹ ã€‚è™½ç„¶æ•°æ®çš„ç»„åˆå’Œåˆ†å±‚æ€§è´¨ç»å¸¸è¢«ç”¨æ¥è§£é‡Šå­¦ä¹ æ€§ï¼Œä½†æ˜¯å»ºç«‹è¿™äº›å±æ€§çš„å®šé‡æµ‹é‡å´å¾ˆå°‘è§ã€‚åŒæ ·ï¼Œè®¿é—®è¿™ç§æ•°æ®ç»“æ„èƒŒåçš„æ½œåœ¨å˜é‡ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†åŸºäºæ‰©æ•£æ¨¡å‹çš„æ­£å‘åå‘å®éªŒï¼Œå…¶ä¸­æ•°æ®è¢«åŠ å…¥å™ªå£°ç„¶åå†å»å™ªå£°ä»¥ç”Ÿæˆæ–°æ ·æœ¬ï¼Œæ˜¯æ¢ç´¢æ•°æ®æ½œåœ¨ç»“æ„çš„æœ‰å‰é€”çš„å·¥å…·ã€‚æˆ‘ä»¬åœ¨ç®€å•çš„åˆ†å±‚æ¨¡å‹ä¸­é¢„æµ‹ï¼Œåœ¨æ­¤è¿‡ç¨‹ä¸­ï¼Œæ•°æ®çš„å˜åŒ–ä¼šä»¥ç›¸å…³å—çš„å½¢å¼å‘ç”Ÿï¼Œé•¿åº¦å°ºåº¦ä¼šåœ¨ä¸€ä¸ªå·²çŸ¥å‘ç”Ÿç›¸å˜çš„å™ªå£°æ°´å¹³å¤„å‘æ•£ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬åœ¨æ–‡æœ¬å’Œå›¾åƒæ•°æ®é›†ä¸Šéƒ½ä½¿ç”¨æœ€å…ˆè¿›çš„æ‰©æ•£æ¨¡å‹è¯å®äº†è¿™ä¸€é¢„æµ‹ã€‚æˆ‘ä»¬çš„ç»“æœå±•ç¤ºäº†æ½œåœ¨å˜é‡å˜åŒ–å¦‚ä½•åœ¨æ•°æ®ä¸­è¡¨ç°å‡ºæ¥ï¼Œå¹¶å»ºç«‹äº†å¦‚ä½•ä½¿ç”¨æ‰©æ•£æ¨¡å‹åœ¨çœŸå®æ•°æ®ä¸­æµ‹é‡è¿™äº›å½±å“çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.13770v2">PDF</a> 10 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æŒ‡å‡ºé«˜ç»´æ•°æ®éœ€è¦é«˜åº¦ç»“æ„åŒ–æ‰èƒ½è¿›è¡Œå­¦ä¹ ã€‚è™½ç„¶æ•°æ®çš„ç»„åˆå’Œå±‚æ¬¡ç»“æ„å¸¸è¢«ç”¨æ¥è§£é‡Šå…¶å¯å­¦ä¹ æ€§ï¼Œä½†å¾ˆå°‘æœ‰å®šé‡æµ‹é‡æ¥éªŒè¯è¿™äº›å±æ€§ã€‚æ­¤å¤–ï¼Œè®¿é—®è¿™äº›æ•°æ®ç»“æ„èƒŒåçš„æ½œåœ¨å˜é‡ä¹Ÿæ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶å±•ç¤ºäº†åŸºäºæ‰©æ•£çš„æ¨¡å‹ä¸­çš„æ­£å‘åå‘å®éªŒæ˜¯æ¢ç´¢æ•°æ®æ½œåœ¨ç»“æ„çš„æœ‰å‰é€”çš„å·¥å…·ã€‚æˆ‘ä»¬é¢„æµ‹åœ¨ç®€å•å±‚æ¬¡æ¨¡å‹ä¸­ï¼Œæ•°æ®åœ¨æ­¤è¿‡ç¨‹ä¸­ä¼šæŒ‰ç›¸å…³å—å‘ç”Ÿå˜åŒ–ï¼Œé•¿åº¦å°ºåº¦ä¼šåœ¨å™ªå£°æ°´å¹³å¤„å‘æ•£ï¼Œæ­¤å¤„å·²çŸ¥ä¼šå‘ç”Ÿç›¸å˜ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæˆ‘ä»¬åœ¨æ–‡æœ¬å’Œå›¾åƒæ•°æ®é›†ä¸­ä½¿ç”¨æœ€å…ˆè¿›çš„æ‰©æ•£æ¨¡å‹è¯å®äº†è¿™ä¸€é¢„æµ‹ã€‚æœ¬ç ”ç©¶å±•ç¤ºäº†æ½œåœ¨å˜é‡å¦‚ä½•åœ¨æ•°æ®ä¸­å‘ç”Ÿå˜åŒ–ï¼Œå¹¶å»ºç«‹äº†å¦‚ä½•åœ¨çœŸå®æ•°æ®ä¸­ä½¿ç”¨æ‰©æ•£æ¨¡å‹æµ‹é‡è¿™äº›æ•ˆåº”çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é«˜ç»´æ•°æ®éœ€è¦é«˜åº¦ç»“æ„åŒ–ä»¥ä¾¿å­¦ä¹ ã€‚</li>
<li>æ•°æ®çš„ç»„åˆå’Œå±‚æ¬¡ç»“æ„å¯¹äºè§£é‡Šå…¶å¯å­¦ä¹ æ€§å¾ˆé‡è¦ï¼Œä½†ç¼ºä¹å®šé‡æµ‹é‡ã€‚</li>
<li>è®¿é—®æ•°æ®ç»“æ„çš„æ½œåœ¨å˜é‡æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹çš„â€œæ­£å‘åå‘å®éªŒâ€æœ‰åŠ©äºæ¢ç´¢æ•°æ®çš„æ½œåœ¨ç»“æ„ã€‚</li>
<li>åœ¨ç®€å•å±‚æ¬¡æ¨¡å‹ä¸­ï¼Œæ•°æ®åœ¨ç‰¹å®šå™ªå£°æ°´å¹³ä¸‹ä¼šå‘ç”Ÿç›¸å˜ï¼Œå¹¶æŒ‰ç›¸å…³å—å˜åŒ–ã€‚è¿™ä¸€é¢„æµ‹åœ¨æ–‡æœ¬å’Œå›¾åƒæ•°æ®é›†ä¸­å¾—åˆ°è¯å®ã€‚</li>
<li>ç ”ç©¶å±•ç¤ºäº†æ½œåœ¨å˜é‡å¦‚ä½•åœ¨æ•°æ®ä¸­å‘ç”Ÿå˜åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.13770">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f7e881b16c293150ee1b0e21415a78d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6790fb189cf6be06c935ac198847a8fc.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Improving-Long-Text-Alignment-for-Text-to-Image-Diffusion-Models"><a href="#Improving-Long-Text-Alignment-for-Text-to-Image-Diffusion-Models" class="headerlink" title="Improving Long-Text Alignment for Text-to-Image Diffusion Models"></a>Improving Long-Text Alignment for Text-to-Image Diffusion Models</h2><p><strong>Authors:Luping Liu, Chao Du, Tianyu Pang, Zehan Wang, Chongxuan Li, Dong Xu</strong></p>
<p>The rapid advancement of text-to-image (T2I) diffusion models has enabled them to generate unprecedented results from given texts. However, as text inputs become longer, existing encoding methods like CLIP face limitations, and aligning the generated images with long texts becomes challenging. To tackle these issues, we propose LongAlign, which includes a segment-level encoding method for processing long texts and a decomposed preference optimization method for effective alignment training. For segment-level encoding, long texts are divided into multiple segments and processed separately. This method overcomes the maximum input length limits of pretrained encoding models. For preference optimization, we provide decomposed CLIP-based preference models to fine-tune diffusion models. Specifically, to utilize CLIP-based preference models for T2I alignment, we delve into their scoring mechanisms and find that the preference scores can be decomposed into two components: a text-relevant part that measures T2I alignment and a text-irrelevant part that assesses other visual aspects of human preference. Additionally, we find that the text-irrelevant part contributes to a common overfitting problem during fine-tuning. To address this, we propose a reweighting strategy that assigns different weights to these two components, thereby reducing overfitting and enhancing alignment. After fine-tuning $512 \times 512$ Stable Diffusion (SD) v1.5 for about 20 hours using our method, the fine-tuned SD outperforms stronger foundation models in T2I alignment, such as PixArt-$\alpha$ and Kandinsky v2.2. The code is available at <a target="_blank" rel="noopener" href="https://github.com/luping-liu/LongAlign">https://github.com/luping-liu/LongAlign</a>. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹çš„å¿«é€Ÿå‘å±•ä½¿å…¶èƒ½å¤Ÿæ ¹æ®ç»™å®šæ–‡æœ¬ç”Ÿæˆå‰æ‰€æœªæœ‰çš„ç»“æœã€‚ç„¶è€Œï¼Œéšç€æ–‡æœ¬è¾“å…¥çš„åŠ é•¿ï¼Œç°æœ‰çš„ç¼–ç æ–¹æ³•ï¼ˆå¦‚CLIPï¼‰é¢ä¸´å±€é™ï¼Œå°†ç”Ÿæˆçš„å›¾åƒä¸é•¿æ–‡æœ¬å¯¹é½å˜å¾—å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†LongAlignæ–¹æ³•ï¼Œå®ƒåŒ…æ‹¬ä¸€ç§ç”¨äºå¤„ç†é•¿æ–‡æœ¬çš„ç‰‡æ®µçº§ç¼–ç æ–¹æ³•å’Œä¸€ç§æœ‰æ•ˆçš„å¯¹é½è®­ç»ƒåˆ†è§£åå¥½ä¼˜åŒ–æ–¹æ³•ã€‚å¯¹äºç‰‡æ®µçº§ç¼–ç ï¼Œé•¿æ–‡æœ¬è¢«åˆ†å‰²æˆå¤šä¸ªç‰‡æ®µå¹¶åˆ†åˆ«è¿›è¡Œå¤„ç†ã€‚è¿™ç§æ–¹æ³•å…‹æœäº†é¢„è®­ç»ƒç¼–ç æ¨¡å‹çš„æœ€å¤§è¾“å…¥é•¿åº¦é™åˆ¶ã€‚å¯¹äºåå¥½ä¼˜åŒ–ï¼Œæˆ‘ä»¬æä¾›äº†åŸºäºCLIPçš„åå¥½æ¨¡å‹æ¥å¾®è°ƒæ‰©æ•£æ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼Œä¸ºäº†å°†CLIPåŸºäºçš„åå¥½æ¨¡å‹ç”¨äºT2Iå¯¹é½ï¼Œæˆ‘ä»¬æ·±å…¥ç ”ç©¶äº†å…¶è¯„åˆ†æœºåˆ¶ï¼Œå¹¶å‘ç°åå¥½åˆ†æ•°å¯ä»¥åˆ†è§£ä¸ºä¸¤ä¸ªéƒ¨åˆ†ï¼šä¸€ä¸ªä¸æ–‡æœ¬ç›¸å…³çš„éƒ¨åˆ†ï¼Œç”¨äºè¡¡é‡T2Iå¯¹é½æƒ…å†µï¼›å¦ä¸€ä¸ªä¸æ–‡æœ¬æ— å…³çš„éƒ¨åˆ†ï¼Œç”¨äºè¯„ä¼°äººç±»åå¥½çš„å…¶ä»–è§†è§‰æ–¹é¢ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°ä¸æ–‡æœ¬æ— å…³çš„éƒ¨åˆ†ä¼šå¯¼è‡´å¾®è°ƒè¿‡ç¨‹ä¸­çš„å¸¸è§è¿‡æ‹Ÿåˆé—®é¢˜ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é‡æ–°åŠ æƒç­–ç•¥ï¼Œä¸ºè¿™ä¸¤ä¸ªéƒ¨åˆ†åˆ†é…ä¸åŒçš„æƒé‡ï¼Œä»è€Œå‡å°‘è¿‡æ‹Ÿåˆå¹¶å¢å¼ºå¯¹é½æ•ˆæœã€‚ä½¿ç”¨æˆ‘ä»¬çš„æ–¹æ³•å¯¹$512 \times 512$çš„Stable Diffusionï¼ˆSDï¼‰v1.5å¾®è°ƒçº¦20å°æ—¶åï¼Œç»è¿‡å¾®è°ƒçš„SDåœ¨T2Iå¯¹é½æ–¹é¢çš„è¡¨ç°è¶…è¿‡äº†æ›´å¼ºçš„åŸºç¡€æ¨¡å‹ï¼Œå¦‚PixArt-$\alpha$å’ŒKandinsky v2.2ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/luping-liu/LongAlign%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/luping-liu/LongAlignæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.11817v2">PDF</a> </p>
<p><strong>Summary</strong><br>     æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹çš„å¿«é€Ÿå‘å±•ä¸ºç»™å®šæ–‡æœ¬ç”Ÿæˆå›¾åƒæä¾›äº†å‰æ‰€æœªæœ‰çš„ç»“æœã€‚ç„¶è€Œï¼Œéšç€æ–‡æœ¬è¾“å…¥çš„å¢é•¿ï¼Œç°æœ‰çš„ç¼–ç æ–¹æ³•å¦‚CLIPé¢ä¸´æŒ‘æˆ˜ï¼Œéš¾ä»¥å°†ç”Ÿæˆçš„å›¾åƒä¸é•¿æ–‡æœ¬å¯¹é½ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºLongAlignæ–¹æ³•ï¼ŒåŒ…æ‹¬åˆ†æ®µçº§ç¼–ç å’Œåˆ†è§£åå¥½ä¼˜åŒ–ã€‚åˆ†æ®µç¼–ç å…‹æœé¢„è®­ç»ƒç¼–ç æ¨¡å‹çš„æœ€å¤§è¾“å…¥é•¿åº¦é™åˆ¶ï¼›åˆ†è§£åå¥½ä¼˜åŒ–åˆ™é€šè¿‡æä¾›åˆ†è§£çš„CLIPåå¥½æ¨¡å‹æ¥å¾®è°ƒæ‰©æ•£æ¨¡å‹ã€‚æˆ‘ä»¬å‘ç°CLIPåå¥½å¾—åˆ†å¯åˆ†è§£ä¸ºè¡¡é‡T2Iå¯¹é½çš„æ–‡æœ¬ç›¸å…³éƒ¨åˆ†å’Œè¯„ä¼°äººç±»åå¥½å…¶ä»–è§†è§‰æ–¹é¢çš„æ–‡æœ¬ä¸ç›¸å…³éƒ¨åˆ†ã€‚ä¸ºè§£å†³æ–‡æœ¬ä¸ç›¸å…³éƒ¨åˆ†å¯¼è‡´çš„è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºé‡æ–°åŠ æƒç­–ç•¥ï¼Œå‡å°‘è¿‡æ‹Ÿåˆï¼Œæé«˜å¯¹é½æ•ˆæœã€‚ä½¿ç”¨LongAlignæ–¹æ³•å¾®è°ƒStable Diffusionçº¦20å°æ—¶åï¼Œå…¶T2Iå¯¹é½æ•ˆæœè¶…è¿‡æ›´å¼ºåŸºå‡†æ¨¡å‹ï¼Œå¦‚PixArt-Î±å’ŒKandinsky v2.2ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹èƒ½åŸºäºç»™å®šæ–‡æœ¬ç”Ÿæˆå›¾åƒï¼Œä¸”æ•ˆæœå‰æ‰€æœªæœ‰ã€‚</li>
<li>éšç€æ–‡æœ¬è¾“å…¥å¢é•¿ï¼Œç°æœ‰ç¼–ç æ–¹æ³•å¦‚CLIPåœ¨å¤„ç†é•¿æ–‡æœ¬ä¸å›¾åƒå¯¹é½æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>LongAlignæ–¹æ³•åŒ…æ‹¬åˆ†æ®µçº§ç¼–ç å’Œåˆ†è§£åå¥½ä¼˜åŒ–ï¼Œä»¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚</li>
<li>åˆ†æ®µçº§ç¼–ç å…‹æœé¢„è®­ç»ƒç¼–ç æ¨¡å‹çš„æœ€å¤§è¾“å…¥é•¿åº¦é™åˆ¶ã€‚</li>
<li>åˆ†è§£åå¥½ä¼˜åŒ–åˆ©ç”¨CLIPåå¥½æ¨¡å‹å¾®è°ƒæ‰©æ•£æ¨¡å‹ï¼Œæé«˜å…¶T2Iå¯¹é½æ•ˆæœã€‚</li>
<li>CLIPåå¥½å¾—åˆ†å¯åˆ†è§£ä¸ºæ–‡æœ¬ç›¸å…³å’Œæ–‡æœ¬ä¸ç›¸å…³ä¸¤éƒ¨åˆ†ï¼Œå…¶ä¸­æ–‡æœ¬ä¸ç›¸å…³éƒ¨åˆ†å¯èƒ½å¯¼è‡´è¿‡æ‹Ÿåˆé—®é¢˜ã€‚</li>
<li>é€šè¿‡é‡æ–°åŠ æƒç­–ç•¥ï¼ŒLongAlignæ–¹æ³•å‡å°‘è¿‡æ‹Ÿåˆï¼Œæé«˜å›¾åƒä¸æ–‡æœ¬çš„å¯¹é½æ•ˆæœï¼Œä¸”åœ¨T2Iå¯¹é½ä»»åŠ¡ä¸­è¡¨ç°è¶…è¶ŠæŸäº›åŸºå‡†æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.11817">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6c3150e6378eae8de3e1259454b32093.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-39fbe63f3f3bf791638e3404a44b607a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-93452cb6a6338b51a908898edf701e66.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c463b15b743cbcfa2ede0368be787e10.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="CtrLoRA-An-Extensible-and-Efficient-Framework-for-Controllable-Image-Generation"><a href="#CtrLoRA-An-Extensible-and-Efficient-Framework-for-Controllable-Image-Generation" class="headerlink" title="CtrLoRA: An Extensible and Efficient Framework for Controllable Image   Generation"></a>CtrLoRA: An Extensible and Efficient Framework for Controllable Image   Generation</h2><p><strong>Authors:Yifeng Xu, Zhenliang He, Shiguang Shan, Xilin Chen</strong></p>
<p>Recently, large-scale diffusion models have made impressive progress in text-to-image (T2I) generation. To further equip these T2I models with fine-grained spatial control, approaches like ControlNet introduce an extra network that learns to follow a condition image. However, for every single condition type, ControlNet requires independent training on millions of data pairs with hundreds of GPU hours, which is quite expensive and makes it challenging for ordinary users to explore and develop new types of conditions. To address this problem, we propose the CtrLoRA framework, which trains a Base ControlNet to learn the common knowledge of image-to-image generation from multiple base conditions, along with condition-specific LoRAs to capture distinct characteristics of each condition. Utilizing our pretrained Base ControlNet, users can easily adapt it to new conditions, requiring as few as 1,000 data pairs and less than one hour of single-GPU training to obtain satisfactory results in most scenarios. Moreover, our CtrLoRA reduces the learnable parameters by 90% compared to ControlNet, significantly lowering the threshold to distribute and deploy the model weights. Extensive experiments on various types of conditions demonstrate the efficiency and effectiveness of our method. Codes and model weights will be released at <a target="_blank" rel="noopener" href="https://github.com/xyfJASON/ctrlora">https://github.com/xyfJASON/ctrlora</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œå¤§è§„æ¨¡æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰ç”Ÿæˆæ–¹é¢å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„è¿›å±•ã€‚ä¸ºäº†è¿›ä¸€æ­¥å¢å¼ºè¿™äº›T2Iæ¨¡å‹çš„ç»†ç²’åº¦ç©ºé—´æ§åˆ¶åŠ›ï¼ŒControlNetç­‰æ–¹æ³•å¼•å…¥äº†ä¸€ä¸ªé¢å¤–çš„ç½‘ç»œï¼Œå­¦ä¹ è·Ÿéšæ¡ä»¶å›¾åƒã€‚ç„¶è€Œï¼Œå¯¹äºæ¯ä¸€ç§æ¡ä»¶ç±»å‹ï¼ŒControlNetéœ€è¦åœ¨æ•°ç™¾ä¸‡å¯¹æ•°æ®å¯¹ä¸Šç‹¬ç«‹è®­ç»ƒï¼Œå¹¶éœ€è¦æ•°ç™¾å°æ—¶çš„GPUæ—¶é—´ï¼Œè¿™ç›¸å½“æ˜‚è´µï¼Œä½¿å¾—æ™®é€šç”¨æˆ·éš¾ä»¥æ¢ç´¢å’Œå¼€å‘æ–°çš„æ¡ä»¶ç±»å‹ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†CtrLoRAæ¡†æ¶ï¼Œè¯¥æ¡†æ¶è®­ç»ƒåŸºç¡€ControlNetæ¥å­¦ä¹ å›¾åƒåˆ°å›¾åƒç”Ÿæˆçš„é€šç”¨çŸ¥è¯†ï¼Œä»å¤šç§åŸºç¡€æ¡ä»¶ä¸­å­¦ä¹ ï¼Œä»¥åŠç‰¹å®šæ¡ä»¶çš„LoRAsæ¥æ•æ‰æ¯ä¸ªæ¡ä»¶çš„ç‹¬ç‰¹ç‰¹å¾ã€‚åˆ©ç”¨æˆ‘ä»¬é¢„è®­ç»ƒçš„åŸºç¡€ControlNetï¼Œç”¨æˆ·å¯ä»¥è½»æ¾é€‚åº”æ–°æ¡ä»¶ï¼Œåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œä»…éœ€1000å¯¹æ•°æ®å¯¹å’Œä¸åˆ°ä¸€ä¸ªå°æ—¶çš„å•GPUè®­ç»ƒå³å¯è·å¾—ä»¤äººæ»¡æ„çš„ç»“æœã€‚æ­¤å¤–ï¼Œä¸ControlNetç›¸æ¯”ï¼Œæˆ‘ä»¬çš„CtrLoRAå‡å°‘äº†90%çš„å¯å­¦ä¹ å‚æ•°ï¼Œå¤§å¤§é™ä½äº†åˆ†å¸ƒå’Œéƒ¨ç½²æ¨¡å‹æƒé‡é—¨æ§›ã€‚å¯¹å„ç§æ¡ä»¶çš„å¹¿æ³›å®éªŒè¯æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„é«˜æ•ˆæ€§å’Œæœ‰æ•ˆæ€§ã€‚ä»£ç å’Œæ¨¡å‹æƒé‡å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/xyfJASON/ctrlora%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/xyfJASON/ctrloraå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.09400v2">PDF</a> ICLR 2025. Code: <a target="_blank" rel="noopener" href="https://github.com/xyfJASON/ctrlora">https://github.com/xyfJASON/ctrlora</a></p>
<p><strong>Summary</strong></p>
<p>å¤§å‹æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬è½¬å›¾åƒç”Ÿæˆé¢†åŸŸå–å¾—æ˜¾è‘—è¿›å±•ã€‚ä¸ºè§£å†³ç°æœ‰æ¨¡å‹å¦‚ControlNetåœ¨ç©ºé—´æ§åˆ¶ä¸Šçš„ç²¾ç»†ç²’åº¦é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºCtrLoRAæ¡†æ¶ï¼Œé€šè¿‡è®­ç»ƒåŸºç¡€ControlNetå­¦ä¹ å¤šç§åŸºç¡€æ¡ä»¶ä¸‹çš„å›¾åƒåˆ°å›¾åƒçš„ç”Ÿæˆé€šç”¨çŸ¥è¯†ï¼Œä»¥åŠé’ˆå¯¹ç‰¹å®šæ¡ä»¶çš„LoRAæ¥æ•æ‰æ¯ä¸ªæ¡ä»¶çš„ç‹¬ç‰¹ç‰¹å¾ã€‚è¿™ä½¿å¾—ç”¨æˆ·èƒ½å¤Ÿè½»æ¾é€‚åº”æ–°æ¡ä»¶ï¼Œåªéœ€å°‘é‡æ•°æ®å¯¹å’Œè¾ƒçŸ­çš„å•GPUè®­ç»ƒæ—¶é—´å³å¯è·å¾—æ»¡æ„ç»“æœã€‚ä¸ControlNetç›¸æ¯”ï¼ŒCtrLoRAæ˜¾è‘—å‡å°‘äº†å¯å­¦ä¹ å‚æ•°ï¼Œé™ä½äº†æ¨¡å‹æƒé‡åˆ†å¸ƒå’Œéƒ¨ç½²çš„é—¨æ§›ã€‚å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é«˜æ•ˆä¸”æœ‰æ•ˆã€‚ä»£ç å’Œæ¨¡å‹æƒé‡å°†åœ¨[ç½‘å€]å‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬è½¬å›¾åƒç”Ÿæˆé¢†åŸŸå–å¾—æ˜¾è‘—è¿›å±•ã€‚</li>
<li>ControlNetéœ€è¦ç‹¬ç«‹è®­ç»ƒå¤§é‡æ•°æ®å¯¹ï¼Œå¯¹äºæ™®é€šç”¨æˆ·æ¥è¯´æ¢ç´¢å’Œå¼€å‘æ–°æ¡ä»¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>CtrLoRAæ¡†æ¶é€šè¿‡è®­ç»ƒåŸºç¡€ControlNetå­¦ä¹ é€šç”¨çŸ¥è¯†ï¼Œå¹¶é€šè¿‡æ¡ä»¶ç‰¹å®šçš„LoRAsæ•æ‰æ¯ä¸ªæ¡ä»¶çš„ç‹¬ç‰¹ç‰¹å¾æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>ç”¨æˆ·å¯ä»¥è½»æ¾é€‚åº”æ–°æ¡ä»¶ï¼Œåªéœ€å°‘é‡æ•°æ®å¯¹å’Œè¾ƒçŸ­çš„å•GPUè®­ç»ƒæ—¶é—´å³å¯è·å¾—æ»¡æ„ç»“æœã€‚</li>
<li>CtrLoRAæ˜¾è‘—å‡å°‘äº†ä¸ControlNetç›¸æ¯”çš„å¯å­¦ä¹ å‚æ•°ã€‚</li>
<li>CtrLoRAé™ä½äº†æ¨¡å‹æƒé‡åˆ†å¸ƒå’Œéƒ¨ç½²çš„é—¨æ§›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.09400">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-87495d031fcfb10c111d83d9db725b80.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e954c55a30e6ee92e5d6b9577284151e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1600d319b2da860cc3f5042e18f08972.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b8262e9eb0d45dec0e7ac61dcea220c3.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="ViBiDSampler-Enhancing-Video-Interpolation-Using-Bidirectional-Diffusion-Sampler"><a href="#ViBiDSampler-Enhancing-Video-Interpolation-Using-Bidirectional-Diffusion-Sampler" class="headerlink" title="ViBiDSampler: Enhancing Video Interpolation Using Bidirectional   Diffusion Sampler"></a>ViBiDSampler: Enhancing Video Interpolation Using Bidirectional   Diffusion Sampler</h2><p><strong>Authors:Serin Yang, Taesung Kwon, Jong Chul Ye</strong></p>
<p>Recent progress in large-scale text-to-video (T2V) and image-to-video (I2V) diffusion models has greatly enhanced video generation, especially in terms of keyframe interpolation. However, current image-to-video diffusion models, while powerful in generating videos from a single conditioning frame, need adaptation for two-frame (start &amp; end) conditioned generation, which is essential for effective bounded interpolation. Unfortunately, existing approaches that fuse temporally forward and backward paths in parallel often suffer from off-manifold issues, leading to artifacts or requiring multiple iterative re-noising steps. In this work, we introduce a novel, bidirectional sampling strategy to address these off-manifold issues without requiring extensive re-noising or fine-tuning. Our method employs sequential sampling along both forward and backward paths, conditioned on the start and end frames, respectively, ensuring more coherent and on-manifold generation of intermediate frames. Additionally, we incorporate advanced guidance techniques, CFG++ and DDS, to further enhance the interpolation process. By integrating these, our method achieves state-of-the-art performance, efficiently generating high-quality, smooth videos between keyframes. On a single 3090 GPU, our method can interpolate 25 frames at 1024 x 576 resolution in just 195 seconds, establishing it as a leading solution for keyframe interpolation. </p>
<blockquote>
<p>è¿‘æœŸå¤§è§„æ¨¡æ–‡æœ¬åˆ°è§†é¢‘ï¼ˆT2Vï¼‰å’Œå›¾åƒåˆ°è§†é¢‘ï¼ˆI2Vï¼‰æ‰©æ•£æ¨¡å‹çš„è¿›å±•æå¤§åœ°æ¨åŠ¨äº†è§†é¢‘ç”Ÿæˆï¼Œå°¤å…¶åœ¨å…³é”®å¸§æ’å€¼æ–¹é¢ã€‚ç„¶è€Œï¼Œå½“å‰çš„å›¾åƒåˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹è™½ç„¶èƒ½å¤Ÿä»å•ä¸ªæ¡ä»¶å¸§ç”Ÿæˆè§†é¢‘ï¼Œä½†éœ€è¦è¿›è¡Œä¸¤å¸§ï¼ˆå¼€å§‹å’Œç»“æŸï¼‰æ¡ä»¶ç”Ÿæˆé€‚åº”ï¼Œè¿™å¯¹äºæœ‰æ•ˆçš„æœ‰ç•Œæ’å€¼è‡³å…³é‡è¦ã€‚ä¸å¹¸çš„æ˜¯ï¼Œç°æœ‰æ–¹æ³•ç»å¸¸åœ¨å¹¶è¡Œèåˆæ—¶é—´æ­£å‘å’Œåå‘è·¯å¾„æ—¶é‡åˆ°æµå½¢å¤–é—®é¢˜ï¼Œå¯¼è‡´å‡ºç°ä¼ªå½±æˆ–éœ€è¦å¤šæ¬¡è¿­ä»£å»å™ªæ­¥éª¤ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„åŒå‘é‡‡æ ·ç­–ç•¥æ¥è§£å†³è¿™äº›æµå½¢å¤–é—®é¢˜ï¼Œè€Œæ— éœ€è¿›è¡Œå¹¿æ³›çš„é‡å™ªå£°å¤„ç†æˆ–å¾®è°ƒã€‚æˆ‘ä»¬çš„æ–¹æ³•æ²¿ç€æ­£å‘å’Œåå‘è·¯å¾„è¿›è¡Œé¡ºåºé‡‡æ ·ï¼Œåˆ†åˆ«ä»¥å¼€å§‹å¸§å’Œç»“æŸå¸§ä¸ºæ¡ä»¶ï¼Œç¡®ä¿ç”Ÿæˆä¸­é—´å¸§æ›´åŠ è¿è´¯ä¸”åœ¨æµå½¢å†…ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ç»“åˆäº†å…ˆè¿›çš„å¼•å¯¼æŠ€æœ¯CFG++å’ŒDDSæ¥è¿›ä¸€æ­¥å¢å¼ºæ’å€¼è¿‡ç¨‹ã€‚é€šè¿‡æ•´åˆè¿™äº›æŠ€æœ¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œèƒ½å¤Ÿé«˜æ•ˆç”Ÿæˆé«˜è´¨é‡ã€å¹³æ»‘çš„è§†é¢‘ã€‚åœ¨å•ä¸ª3090 GPUä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥åœ¨ä»…195ç§’å†…æ’å€¼å‡ºåˆ†è¾¨ç‡ä¸º1024 x 576çš„25å¸§ï¼Œä½¿å…¶æˆä¸ºå…³é”®å¸§æ’å€¼çš„é¢†å…ˆè§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.05651v3">PDF</a> ICLR 2025; Project page: <a target="_blank" rel="noopener" href="https://vibidsampler.github.io/">https://vibidsampler.github.io/</a></p>
<p><strong>Summary</strong><br>     è¿‘æœŸæ–‡æœ¬åˆ°è§†é¢‘ï¼ˆT2Vï¼‰å’Œå›¾åƒåˆ°è§†é¢‘ï¼ˆI2Vï¼‰æ‰©æ•£æ¨¡å‹çš„è¿›å±•æå¤§ä¿ƒè¿›äº†è§†é¢‘ç”Ÿæˆï¼Œç‰¹åˆ«æ˜¯åœ¨å…³é”®å¸§æ’å€¼æ–¹é¢ã€‚ç„¶è€Œï¼Œå½“å‰å›¾åƒåˆ°è§†é¢‘çš„æ‰©æ•£æ¨¡å‹è™½ç„¶èƒ½ä»å•ä¸ªæ¡ä»¶å¸§ç”Ÿæˆè§†é¢‘ï¼Œä½†åœ¨ä¸¤å¸§ï¼ˆå¼€å§‹å’Œç»“æŸï¼‰æ¡ä»¶ä¸‹çš„ç”Ÿæˆä»éœ€è¦æ”¹è¿›ã€‚æœ¬æ–‡å¼•å…¥äº†ä¸€ç§æ–°çš„åŒå‘é‡‡æ ·ç­–ç•¥ï¼Œè§£å†³äº†ç¦»æµé—®é¢˜ï¼Œæé«˜äº†å…³é”®å¸§æ’å€¼çš„è´¨é‡ï¼Œä¸éœ€è¦å¤§é‡é‡æ–°å»å™ªæˆ–å¾®è°ƒã€‚æ–¹æ³•æ²¿æ­£å‘å’Œåå‘è·¯å¾„è¿›è¡Œé¡ºåºé‡‡æ ·ï¼Œåˆ†åˆ«ä»¥å¼€å§‹å’Œç»“æŸå¸§ä¸ºæ¡ä»¶ï¼Œä¿è¯ä¸­é—´å¸§çš„ç”Ÿæˆæ›´åŠ è¿è´¯ä¸”åœ¨æµå½¢ä¸Šã€‚ç»“åˆå…ˆè¿›çš„å¼•å¯¼æŠ€æœ¯CFG++å’ŒDDSï¼Œè¿›ä¸€æ­¥æé«˜äº†æ’å€¼è¿‡ç¨‹çš„æ•ˆæœã€‚æ­¤æ–¹æ³•è¾¾åˆ°äº†é¢†å…ˆæ°´å¹³ï¼Œèƒ½åœ¨å•ä¸ª3090 GPUä¸Šé«˜æ•ˆç”Ÿæˆé«˜è´¨é‡ã€å¹³æ»‘çš„è§†é¢‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹æ–‡æœ¬åˆ°è§†é¢‘å’Œå›¾åƒåˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹çš„æœ€æ–°è¿›å±•å·²æ˜¾è‘—æé«˜è§†é¢‘ç”Ÿæˆèƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨å…³é”®å¸§æ’å€¼æ–¹é¢ã€‚</li>
<li>å½“å‰å›¾åƒåˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹éœ€è¦åœ¨ä¸¤å¸§ï¼ˆå¼€å§‹å’Œç»“æŸï¼‰æ¡ä»¶ä¸‹è¿›è¡Œç”Ÿæˆæ”¹è¿›ã€‚</li>
<li>å¼•å…¥æ–°çš„åŒå‘é‡‡æ ·ç­–ç•¥è§£å†³ç¦»æµé—®é¢˜ï¼Œæé«˜å…³é”®å¸§æ’å€¼è´¨é‡ã€‚</li>
<li>é¡ºåºé‡‡æ ·æ²¿æ­£å‘å’Œåå‘è·¯å¾„è¿›è¡Œï¼Œä»¥å¼€å§‹å’Œç»“æŸå¸§ä¸ºæ¡ä»¶ï¼Œç¡®ä¿ä¸­é—´å¸§çš„è¿è´¯æ€§å’Œåœ¨æµå½¢ä¸Šã€‚</li>
<li>ç»“åˆå…ˆè¿›çš„å¼•å¯¼æŠ€æœ¯CFG++å’ŒDDSå¢å¼ºæ’å€¼è¿‡ç¨‹ã€‚</li>
<li>æ–¹æ³•è¾¾åˆ°é¢†å…ˆæ°´å¹³ï¼Œé«˜æ•ˆç”Ÿæˆé«˜è´¨é‡ã€å¹³æ»‘çš„è§†é¢‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.05651">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f3bfde46fa317dab10869b336396ece3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5fd361526fbaf70019f19565c8c3c0ee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e622b9382ef149bf94a1fa599dac93d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc1cb8b1d5da0b68fa18afc1c46a8924.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Image-Watermarks-are-Removable-Using-Controllable-Regeneration-from-Clean-Noise"><a href="#Image-Watermarks-are-Removable-Using-Controllable-Regeneration-from-Clean-Noise" class="headerlink" title="Image Watermarks are Removable Using Controllable Regeneration from   Clean Noise"></a>Image Watermarks are Removable Using Controllable Regeneration from   Clean Noise</h2><p><strong>Authors:Yepeng Liu, Yiren Song, Hai Ci, Yu Zhang, Haofan Wang, Mike Zheng Shou, Yuheng Bu</strong></p>
<p>Image watermark techniques provide an effective way to assert ownership, deter misuse, and trace content sources, which has become increasingly essential in the era of large generative models. A critical attribute of watermark techniques is their robustness against various manipulations. In this paper, we introduce a watermark removal approach capable of effectively nullifying state-of-the-art watermarking techniques. Our primary insight involves regenerating the watermarked image starting from a clean Gaussian noise via a controllable diffusion model, utilizing the extracted semantic and spatial features from the watermarked image. The semantic control adapter and the spatial control network are specifically trained to control the denoising process towards ensuring image quality and enhancing consistency between the cleaned image and the original watermarked image. To achieve a smooth trade-off between watermark removal performance and image consistency, we further propose an adjustable and controllable regeneration scheme. This scheme adds varying numbers of noise steps to the latent representation of the watermarked image, followed by a controlled denoising process starting from this noisy latent representation. As the number of noise steps increases, the latent representation progressively approaches clean Gaussian noise, facilitating the desired trade-off. We apply our watermark removal methods across various watermarking techniques, and the results demonstrate that our methods offer superior visual consistency&#x2F;quality and enhanced watermark removal performance compared to existing regeneration approaches. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/yepengliu/CtrlRegen">https://github.com/yepengliu/CtrlRegen</a>. </p>
<blockquote>
<p>å›¾åƒæ°´å°æŠ€æœ¯ä¸ºå£°æ˜æ‰€æœ‰æƒã€é˜»æ­¢æ»¥ç”¨å’Œè¿½è¸ªå†…å®¹æ¥æºæä¾›äº†ä¸€ç§æœ‰æ•ˆé€”å¾„ï¼Œè¿™åœ¨å¤§å‹ç”Ÿæˆæ¨¡å‹æ—¶ä»£å˜å¾—æ—¥ç›Šé‡è¦ã€‚æ°´å°æŠ€æœ¯çš„å…³é”®å±æ€§æ˜¯å®ƒä»¬å¯¹å„ç§æ“ä½œçš„é²æ£’æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§èƒ½å¤Ÿæœ‰æ•ˆæ¶ˆé™¤æœ€å…ˆè¿›æ°´å°æŠ€æœ¯çš„æ°´å°å»é™¤æ–¹æ³•ã€‚æˆ‘ä»¬çš„ä¸»è¦è§è§£æ˜¯é€šè¿‡å¯æ§çš„æ‰©æ•£æ¨¡å‹ï¼Œä»å¹²å‡€çš„é«˜æ–¯å™ªå£°å¼€å§‹é‡æ–°ç”Ÿæˆæ°´å°å›¾åƒï¼Œåˆ©ç”¨ä»æ°´å°å›¾åƒä¸­æå–çš„è¯­ä¹‰å’Œç©ºé—´ç‰¹å¾ã€‚è¯­ä¹‰æ§åˆ¶é€‚é…å™¨å’Œç©ºé—´æ§åˆ¶ç½‘ç»œç»è¿‡ä¸“é—¨è®­ç»ƒï¼Œä»¥æ§åˆ¶å»å™ªè¿‡ç¨‹ï¼Œç¡®ä¿å›¾åƒè´¨é‡ï¼Œå¹¶å¢å¼ºæ¸…æ´å›¾åƒä¸åŸå§‹æ°´å°å›¾åƒä¹‹é—´çš„ä¸€è‡´æ€§ã€‚ä¸ºäº†å®ç°æ°´å°å»é™¤æ€§èƒ½å’Œå›¾åƒä¸€è‡´æ€§ä¹‹é—´çš„å¹³ç¨³æƒè¡¡ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†å¯è°ƒå¯æ§çš„å†ç”Ÿæ–¹æ¡ˆã€‚è¯¥æ–¹æ¡ˆå‘æ°´å°å›¾åƒçš„æ½œåœ¨è¡¨ç¤ºæ·»åŠ ä¸åŒæ•°é‡çš„å™ªå£°æ­¥éª¤ï¼Œç„¶åä»è¿™ä¸ªå™ªå£°çš„æ½œåœ¨è¡¨ç¤ºå¼€å§‹è¿›è¡Œæ§åˆ¶å»å™ªè¿‡ç¨‹ã€‚éšç€å™ªå£°æ­¥éª¤æ•°é‡çš„å¢åŠ ï¼Œæ½œåœ¨è¡¨ç¤ºé€æ¸æ¥è¿‘å¹²å‡€çš„é«˜æ–¯å™ªå£°ï¼Œå®ç°äº†æ‰€éœ€çš„æƒè¡¡ã€‚æˆ‘ä»¬åœ¨å„ç§æ°´å°æŠ€æœ¯ä¸­åº”ç”¨äº†æˆ‘ä»¬çš„æ°´å°å»é™¤æ–¹æ³•ï¼Œç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰çš„å†ç”Ÿæ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è§†è§‰ä¸€è‡´æ€§&#x2F;è´¨é‡å’Œæ°´å°å»é™¤æ€§èƒ½æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/yepengliu/CtrlRegen%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/yepengliu/CtrlRegenæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.05470v2">PDF</a> ICLR2025</p>
<p><strong>Summary</strong></p>
<p>é’ˆå¯¹å¤§å‹ç”Ÿæˆæ¨¡å‹æ—¶ä»£ç‰ˆæƒä¿æŠ¤çš„éœ€æ±‚ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„æ°´å°ç§»é™¤æ–¹æ³•ã€‚è¯¥æ–¹æ³•åŸºäºå¯æ§çš„æ‰©æ•£æ¨¡å‹ï¼Œä»æ¸…æ´çš„é«˜æ–¯å™ªå£°å¼€å§‹é‡æ–°ç”Ÿæˆæ°´å°å›¾åƒï¼Œåˆ©ç”¨æå–çš„è¯­ä¹‰å’Œç©ºé—´ç‰¹å¾è¿›è¡Œè®­ç»ƒå’Œä¼˜åŒ–ã€‚é€šè¿‡è°ƒæ•´å™ªå£°æ­¥æ•°ï¼Œå®ç°äº†æ°´å°ç§»é™¤ä¸å›¾åƒä¸€è‡´æ€§ä¹‹é—´çš„å¹³è¡¡ã€‚ç›¸è¾ƒäºç°æœ‰æ–¹æ³•ï¼Œæœ¬æ–‡æå‡ºçš„æ–¹æ³•åœ¨è§†è§‰ä¸€è‡´æ€§å’Œæ°´å°ç§»é™¤æ€§èƒ½ä¸Šè¡¨ç°æ›´ä¼˜ç§€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒæ°´å°æŠ€æœ¯çš„é‡è¦æ€§åœ¨äºå¯¹å†…å®¹çš„æº¯æºã€é˜²æ­¢è¯¯ç”¨å’Œä¸»å¼ æ‰€æœ‰æƒã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ°´å°ç§»é™¤æ–¹æ³•ï¼ŒåŸºäºå¯æ§çš„æ‰©æ•£æ¨¡å‹ä»æ¸…æ´çš„é«˜æ–¯å™ªå£°é‡æ–°ç”Ÿæˆæ°´å°å›¾åƒã€‚</li>
<li>é€šè¿‡è¯­ä¹‰æ§åˆ¶é€‚é…å™¨å’Œç©ºé—´æ§åˆ¶ç½‘ç»œï¼Œç¡®ä¿å›¾åƒè´¨é‡å’Œä¸åŸå§‹æ°´å°å›¾åƒçš„ä¸€è‡´æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§å¯è°ƒæ•´çš„æ§åˆ¶å†ç”Ÿæ–¹æ¡ˆï¼Œé€šè¿‡å¢åŠ å™ªå£°æ­¥æ•°å®ç°æ°´å°ç§»é™¤ä¸å›¾åƒä¸€è‡´æ€§ä¹‹é—´çš„å¹³è¡¡ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å„ç§æ°´å°æŠ€æœ¯ä¸Šçš„åº”ç”¨å‡å–å¾—äº†è‰¯å¥½æ•ˆæœï¼Œç›¸è¾ƒäºç°æœ‰æ–¹æ³•å…·æœ‰ä¼˜è¶Šçš„æ€§èƒ½ã€‚</li>
<li>è¯¥ç ”ç©¶çš„ä»£ç å·²å…¬å¼€åœ¨GitHubä¸Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.05470">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-e8fc55ffae4533b953c7c407aa2835c2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-83ef17f486a92c0e1e2631b22a3116f9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ef32afdfca8ff5ed12ee1566bcafd6a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-86354b7504b0a0e01945c4df0799cda1.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="FedBiP-Heterogeneous-One-Shot-Federated-Learning-with-Personalized-Latent-Diffusion-Models"><a href="#FedBiP-Heterogeneous-One-Shot-Federated-Learning-with-Personalized-Latent-Diffusion-Models" class="headerlink" title="FedBiP: Heterogeneous One-Shot Federated Learning with Personalized   Latent Diffusion Models"></a>FedBiP: Heterogeneous One-Shot Federated Learning with Personalized   Latent Diffusion Models</h2><p><strong>Authors:Haokun Chen, Hang Li, Yao Zhang, Jinhe Bi, Gengyuan Zhang, Yueqi Zhang, Philip Torr, Jindong Gu, Denis Krompass, Volker Tresp</strong></p>
<p>One-Shot Federated Learning (OSFL), a special decentralized machine learning paradigm, has recently gained significant attention. OSFL requires only a single round of client data or model upload, which reduces communication costs and mitigates privacy threats compared to traditional FL. Despite these promising prospects, existing methods face challenges due to client data heterogeneity and limited data quantity when applied to real-world OSFL systems. Recently, Latent Diffusion Models (LDM) have shown remarkable advancements in synthesizing high-quality images through pretraining on large-scale datasets, thereby presenting a potential solution to overcome these issues. However, directly applying pretrained LDM to heterogeneous OSFL results in significant distribution shifts in synthetic data, leading to performance degradation in classification models trained on such data. This issue is particularly pronounced in rare domains, such as medical imaging, which are underrepresented in LDMâ€™s pretraining data. To address this challenge, we propose Federated Bi-Level Personalization (FedBiP), which personalizes the pretrained LDM at both instance-level and concept-level. Hereby, FedBiP synthesizes images following the clientâ€™s local data distribution without compromising the privacy regulations. FedBiP is also the first approach to simultaneously address feature space heterogeneity and client data scarcity in OSFL. Our method is validated through extensive experiments on three OSFL benchmarks with feature space heterogeneity, as well as on challenging medical and satellite image datasets with label heterogeneity. The results demonstrate the effectiveness of FedBiP, which substantially outperforms other OSFL methods. </p>
<blockquote>
<p>ä¸€æ¬¡æ€§è”é‚¦å­¦ä¹ ï¼ˆOSFLï¼‰æ˜¯ä¸€ç§ç‰¹æ®Šçš„å»ä¸­å¿ƒåŒ–æœºå™¨å­¦ä¹ èŒƒå¼ï¼Œæœ€è¿‘å¼•èµ·äº†å¹¿æ³›çš„å…³æ³¨ã€‚OSFLåªéœ€è¦ä¸€è½®å®¢æˆ·ç«¯æ•°æ®æˆ–æ¨¡å‹ä¸Šä¼ ï¼Œä¸ä¼ ç»Ÿçš„è”é‚¦å­¦ä¹ ç›¸æ¯”ï¼Œè¿™é™ä½äº†é€šä¿¡æˆæœ¬å¹¶å‡è½»äº†éšç§å¨èƒã€‚å°½ç®¡å‰æ™¯å……æ»¡å¸Œæœ›ï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨åº”ç”¨äºç°å®ä¸–ç•ŒOSFLç³»ç»Ÿæ—¶ï¼Œé¢ä¸´ç€å®¢æˆ·æ•°æ®å¼‚è´¨æ€§å’Œæ•°æ®é‡æœ‰é™çš„æŒ‘æˆ˜ã€‚</p>
</blockquote>
<p>æœ€è¿‘ï¼Œæ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰åœ¨é€šè¿‡å¤§è§„æ¨¡æ•°æ®é›†è¿›è¡Œé¢„è®­ç»ƒåï¼Œåœ¨åˆæˆé«˜è´¨é‡å›¾åƒæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä¸ºè§£å†³è¿™äº›é—®é¢˜æä¾›äº†æ½œåœ¨è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œç›´æ¥å°†é¢„è®­ç»ƒçš„LDMåº”ç”¨äºå¼‚è´¨OSFLä¼šå¯¼è‡´åˆæˆæ•°æ®åˆ†å¸ƒçš„é‡å¤§å˜åŒ–ï¼Œå¯¼è‡´åœ¨ç”±æ­¤ç±»æ•°æ®è®­ç»ƒçš„åˆ†ç±»æ¨¡å‹æ€§èƒ½ä¸‹é™ã€‚è¿™ä¸€é—®é¢˜åœ¨LDMé¢„è®­ç»ƒæ•°æ®ä»£è¡¨æ€§ä¸è¶³çš„ç½•è§é¢†åŸŸï¼ˆå¦‚åŒ»å­¦å½±åƒï¼‰ä¸­å°¤å…¶ä¸¥é‡ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.04810v2">PDF</a> CVPR 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>OSFLï¼ˆä¸€æ¬¡è”é‚¦å­¦ä¹ ï¼‰æ˜¯ä¸€ç§ç‰¹æ®Šçš„åˆ†å¸ƒå¼æœºå™¨å­¦ä¹ èŒƒå¼ï¼Œå› ä»…éœ€ä¸€è½®å®¢æˆ·ç«¯æ•°æ®æˆ–æ¨¡å‹ä¸Šä¼ è€Œå—åˆ°å…³æ³¨ã€‚ä¸ä¼ ç»Ÿçš„è”é‚¦å­¦ä¹ ç›¸æ¯”ï¼ŒOSFLé™ä½äº†é€šä¿¡æˆæœ¬å¹¶å‡è½»äº†éšç§å¨èƒã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•åœ¨åº”ç”¨åˆ°ç°å®ä¸–ç•ŒOSFLç³»ç»Ÿæ—¶é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ï¼Œå¦‚å®¢æˆ·æ•°æ®å¼‚è´¨æ€§å’Œæ•°æ®é‡æœ‰é™ç­‰ã€‚æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰åœ¨é¢„è®­ç»ƒå¤§è§„æ¨¡æ•°æ®é›†ä¸Šè¡¨ç°å‡ºåˆæˆé«˜è´¨é‡å›¾åƒçš„å·¨å¤§æ½œåŠ›ï¼Œä½†ç›´æ¥åº”ç”¨äºå¼‚è´¨çš„OSFLä¼šäº§ç”Ÿæ˜¾è‘—çš„åˆ†å¸ƒåç§»ã€‚é’ˆå¯¹æ­¤é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†è”é‚¦åŒå±‚ä¸ªæ€§åŒ–ï¼ˆFedBiPï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨å®ä¾‹å±‚é¢å’Œæ¦‚å¿µå±‚é¢å¯¹é¢„è®­ç»ƒçš„LDMè¿›è¡Œä¸ªæ€§åŒ–è°ƒæ•´ã€‚FedBiPèƒ½å¤Ÿæ ¹æ®å®¢æˆ·ç«¯çš„æœ¬åœ°æ•°æ®åˆ†å¸ƒåˆæˆå›¾åƒï¼ŒåŒæ—¶éµå®ˆéšç§è§„å®šã€‚æ­¤å¤–ï¼ŒFedBiPè¿˜æ˜¯é¦–ä¸ªåŒæ—¶è§£å†³OSFLä¸­çš„ç‰¹å¾ç©ºé—´å¼‚è´¨æ€§å’Œå®¢æˆ·æ•°æ®ç¨€ç¼ºæ€§çš„æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFedBiPåœ¨å…·æœ‰ç‰¹å¾ç©ºé—´å¼‚è´¨æ€§çš„ä¸‰ä¸ªOSFLåŸºå‡†æµ‹è¯•ä»¥åŠå…·æœ‰æ ‡ç­¾å¼‚è´¨æ€§çš„åŒ»ç–—å’Œå«æ˜Ÿå›¾åƒæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>OSFLä»…éœ€è¦ä¸€è½®å®¢æˆ·ç«¯æ•°æ®æˆ–æ¨¡å‹ä¸Šä¼ ï¼Œé™ä½äº†é€šä¿¡æˆæœ¬å’Œéšç§å¨èƒã€‚</li>
<li>LDMåœ¨åˆæˆé«˜è´¨é‡å›¾åƒæ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œä½†ç›´æ¥åº”ç”¨äºå¼‚è´¨OSFLä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚</li>
<li>FedBiPæ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ï¼Œé€šè¿‡ä¸ªæ€§åŒ–é¢„è®­ç»ƒçš„LDMä»¥é€‚åº”å®¢æˆ·ç«¯çš„æœ¬åœ°æ•°æ®åˆ†å¸ƒã€‚</li>
<li>FedBiPæ˜¯é¦–ä¸ªåŒæ—¶è§£å†³OSFLä¸­çš„ç‰¹å¾ç©ºé—´å¼‚è´¨æ€§å’Œå®¢æˆ·æ•°æ®ç¨€ç¼ºæ€§çš„æ–¹æ³•ã€‚</li>
<li>FedBiPåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åŒ»ç–—å’Œå«æ˜Ÿå›¾åƒæ•°æ®é›†ä¸Šã€‚</li>
<li>FedBiPçš„åˆæˆå›¾åƒéµå¾ªéšç§è§„å®šï¼Œæ»¡è¶³è”é‚¦å­¦ä¹ çš„éšç§ä¿æŠ¤éœ€æ±‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.04810">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b343848eae5d3a0637bd307ab396c7bc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8bee296d3bbc830242faff0b2d9cad84.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e247307f7d1de71d56034e0ec2be99fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db23b150ea213e082b0a77955f16232d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ac83a30627a5e57d72abfc8e309ddc7.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Diffusion-State-Guided-Projected-Gradient-for-Inverse-Problems"><a href="#Diffusion-State-Guided-Projected-Gradient-for-Inverse-Problems" class="headerlink" title="Diffusion State-Guided Projected Gradient for Inverse Problems"></a>Diffusion State-Guided Projected Gradient for Inverse Problems</h2><p><strong>Authors:Rayhan Zirvi, Bahareh Tolooshams, Anima Anandkumar</strong></p>
<p>Recent advancements in diffusion models have been effective in learning data priors for solving inverse problems. They leverage diffusion sampling steps for inducing a data prior while using a measurement guidance gradient at each step to impose data consistency. For general inverse problems, approximations are needed when an unconditionally trained diffusion model is used since the measurement likelihood is intractable, leading to inaccurate posterior sampling. In other words, due to their approximations, these methods fail to preserve the generation process on the data manifold defined by the diffusion prior, leading to artifacts in applications such as image restoration. To enhance the performance and robustness of diffusion models in solving inverse problems, we propose Diffusion State-Guided Projected Gradient (DiffStateGrad), which projects the measurement gradient onto a subspace that is a low-rank approximation of an intermediate state of the diffusion process. DiffStateGrad, as a module, can be added to a wide range of diffusion-based inverse solvers to improve the preservation of the diffusion process on the prior manifold and filter out artifact-inducing components. We highlight that DiffStateGrad improves the robustness of diffusion models in terms of the choice of measurement guidance step size and noise while improving the worst-case performance. Finally, we demonstrate that DiffStateGrad improves upon the state-of-the-art on linear and nonlinear image restoration inverse problems. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/neuraloperator/DiffStateGrad">https://github.com/neuraloperator/DiffStateGrad</a>. </p>
<blockquote>
<p>æœ€è¿‘æ‰©æ•£æ¨¡å‹çš„è¿›å±•åœ¨å­¦ä¹ æ•°æ®å…ˆéªŒä»¥è§£å†³åé—®é¢˜æ–¹é¢éå¸¸æœ‰æ•ˆã€‚ä»–ä»¬åˆ©ç”¨æ‰©æ•£é‡‡æ ·æ­¥éª¤æ¥è¯±å¯¼æ•°æ®å…ˆéªŒï¼ŒåŒæ—¶åœ¨æ¯ä¸€æ­¥ä½¿ç”¨æµ‹é‡æŒ‡å¯¼æ¢¯åº¦æ¥æ–½åŠ æ•°æ®ä¸€è‡´æ€§ã€‚å¯¹äºä¸€èˆ¬åé—®é¢˜ï¼Œå½“ä½¿ç”¨æ— æ¡ä»¶è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹æ—¶ï¼Œéœ€è¦è¿‘ä¼¼å¤„ç†ï¼Œå› ä¸ºæµ‹é‡å¯èƒ½æ€§éš¾ä»¥å¤„ç†ï¼Œå¯¼è‡´åéªŒé‡‡æ ·ä¸å‡†ç¡®ã€‚æ¢å¥è¯è¯´ï¼Œç”±äºè¿™äº›è¿‘ä¼¼ï¼Œè¿™äº›æ–¹æ³•æ— æ³•ä¿ç•™ç”±æ‰©æ•£å…ˆéªŒå®šä¹‰çš„æ•°æ®æµå½¢ä¸Šçš„ç”Ÿæˆè¿‡ç¨‹ï¼Œå¯¼è‡´å›¾åƒæ¢å¤ç­‰åº”ç”¨ä¸­çš„ä¼ªå½±ã€‚ä¸ºäº†æé«˜æ‰©æ•£æ¨¡å‹è§£å†³åé—®é¢˜çš„æ€§èƒ½å’Œç¨³å¥æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†æ‰©æ•£çŠ¶æ€å¼•å¯¼æŠ•å½±æ¢¯åº¦ï¼ˆDiffStateGradï¼‰ï¼Œå®ƒå°†æµ‹é‡æ¢¯åº¦æŠ•å½±åˆ°æ‰©æ•£è¿‡ç¨‹ä¸­é—´çŠ¶æ€çš„ä½ç§©è¿‘ä¼¼å­ç©ºé—´ä¸Šã€‚DiffStateGradä½œä¸ºä¸€ä¸ªæ¨¡å—ï¼Œå¯ä»¥æ·»åŠ åˆ°å¹¿æ³›çš„åŸºäºæ‰©æ•£çš„åé—®é¢˜æ±‚è§£å™¨ä¸­ï¼Œä»¥æé«˜å¯¹å…ˆéªŒæµå½¢ä¸Šæ‰©æ•£è¿‡ç¨‹çš„ä¿ç•™èƒ½åŠ›ï¼Œå¹¶è¿‡æ»¤æ‰äº§ç”Ÿä¼ªå½±çš„ç»„ä»¶ã€‚æˆ‘ä»¬å¼ºè°ƒï¼ŒDiffStateGradæé«˜äº†æ‰©æ•£æ¨¡å‹åœ¨é€‰æ‹©æµ‹é‡æŒ‡å¯¼æ­¥é•¿å’Œå™ªå£°æ–¹é¢çš„ç¨³å¥æ€§ï¼ŒåŒæ—¶æé«˜äº†æœ€åæƒ…å†µä¸‹çš„æ€§èƒ½ã€‚æœ€åï¼Œæˆ‘ä»¬è¯æ˜DiffStateGradåœ¨è§£å†³çº¿æ€§å’Œéçº¿æ€§å›¾åƒæ¢å¤åé—®é¢˜ä¸Šä¼˜äºç°æœ‰æŠ€æœ¯ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨[<a target="_blank" rel="noopener" href="https://github.com/neuraloperator/DiffStateGrad%E6%89%BE%E5%88%B0%E3%80%82]">https://github.com/neuraloperator/DiffStateGradæ‰¾åˆ°ã€‚]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.03463v3">PDF</a> Published as a conference paper at ICLR 2025. RZ and BT have equal   contributions</p>
<p><strong>Summary</strong><br>     è¿‘æœŸæ‰©æ•£æ¨¡å‹åœ¨è§£å†³åé—®é¢˜æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œå­¦ä¹ æ•°æ®å…ˆéªŒå¹¶åˆ©ç”¨æ‰©æ•£é‡‡æ ·æ­¥éª¤å’Œæµ‹é‡æŒ‡å¯¼æ¢¯åº¦ã€‚ç„¶è€Œï¼Œæ— æ¡ä»¶è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹åœ¨å¤„ç†åé—®é¢˜æ—¶éœ€è¦ä½¿ç”¨è¿‘ä¼¼æ–¹æ³•ï¼Œå¯¼è‡´æµ‹é‡å¯èƒ½æ€§ä¸å¯è¿½è¸ªå’Œå‡†ç¡®æ€§ä¸‹é™ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼ŒDiffusion State-Guided Projected Gradient (DiffStateGrad)ï¼Œèƒ½å¤Ÿæå‡æ‰©æ•£æ¨¡å‹åœ¨è§£å†³åé—®é¢˜æ—¶çš„æ€§èƒ½å’Œé²æ£’æ€§ã€‚DiffStateGradé€šè¿‡å°†æµ‹é‡æ¢¯åº¦æŠ•å½±åˆ°æ‰©æ•£è¿‡ç¨‹ä¸­é—´çŠ¶æ€çš„ä½ç§©è¿‘ä¼¼å­ç©ºé—´ï¼Œæé«˜äº†æ•°æ®æµå½¢ä¸Šçš„æ‰©æ•£è¿‡ç¨‹ä¿æŠ¤å¹¶è¿‡æ»¤æ‰äº†å¯¼è‡´å‡è±¡çš„ç»„ä»¶ã€‚æœ€åé€šè¿‡å®éªŒç»“æœè¯æ˜ï¼ŒDiffStateGradåœ¨çº¿æ€§å’Œéçº¿æ€§å›¾åƒä¿®å¤åé—®é¢˜ä¸Šè¾¾åˆ°äº†å…ˆè¿›æ°´å¹³ã€‚æ›´å¤šè¯¦æƒ…å¯è®¿é—®GitHubé“¾æ¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨è§£å†³åé—®é¢˜æ–¹é¢å±•ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œèƒ½å¤Ÿé€šè¿‡å­¦ä¹ æ•°æ®å…ˆéªŒè¿›è¡Œåé—®é¢˜çš„æ±‚è§£ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åœ¨å¤„ç†åé—®é¢˜æ—¶éœ€è¦ä½¿ç”¨è¿‘ä¼¼æ–¹æ³•ï¼Œå¯¼è‡´æµ‹é‡å¯èƒ½æ€§çš„è¿½è¸ªå›°éš¾å¹¶å¯èƒ½å¯¼è‡´å‡†ç¡®æ€§ä¸‹é™ã€‚</li>
<li>æå‡ºçš„DiffStateGradæ–¹æ³•é€šè¿‡å°†æµ‹é‡æ¢¯åº¦æŠ•å½±åˆ°æ‰©æ•£è¿‡ç¨‹çš„ä¸­é—´çŠ¶æ€å­ç©ºé—´ï¼Œæå‡äº†æ‰©æ•£æ¨¡å‹åœ¨è§£å†³åé—®é¢˜æ—¶çš„æ€§èƒ½ã€‚</li>
<li>DiffStateGradä½œä¸ºä¸€ç§æ¨¡å—ï¼Œå¯å¹¿æ³›åº”ç”¨äºå„ç§åŸºäºæ‰©æ•£çš„åé—®é¢˜æ±‚è§£å™¨ä¸­ä»¥æ”¹è¿›å…ˆéªŒæµå½¢çš„ä¿æŠ¤æ€§èƒ½å¹¶è¿‡æ»¤æ‰äº§ç”Ÿå‡è±¡çš„ç»„ä»¶ã€‚</li>
<li>DiffStateGradæé«˜äº†æ‰©æ•£æ¨¡å‹åœ¨é€‰æ‹©æµ‹é‡æŒ‡å¯¼æ­¥éª¤å¤§å°å’Œå™ªå£°æ–¹é¢çš„é²æ£’æ€§ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒDiffStateGradåœ¨çº¿æ€§å’Œéçº¿æ€§å›¾åƒä¿®å¤åé—®é¢˜ä¸Šä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.03463">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-0c3b103583e1e35bfcd015a133674b7c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1ae0b5590baf824e5ae43aac9c99a51a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-80d50ea2301487db0de96f84525934bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d73a08f0b1ddc496f7fb4e752bfdf443.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d56557d6bd577bafba3c251e3160f84.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c5ff6bc79d9e77536d65330d42581135.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="LANTERN-Accelerating-Visual-Autoregressive-Models-with-Relaxed-Speculative-Decoding"><a href="#LANTERN-Accelerating-Visual-Autoregressive-Models-with-Relaxed-Speculative-Decoding" class="headerlink" title="LANTERN: Accelerating Visual Autoregressive Models with Relaxed   Speculative Decoding"></a>LANTERN: Accelerating Visual Autoregressive Models with Relaxed   Speculative Decoding</h2><p><strong>Authors:Doohyuk Jang, Sihwan Park, June Yong Yang, Yeonsung Jung, Jihun Yun, Souvik Kundu, Sung-Yub Kim, Eunho Yang</strong></p>
<p>Auto-Regressive (AR) models have recently gained prominence in image generation, often matching or even surpassing the performance of diffusion models. However, one major limitation of AR models is their sequential nature, which processes tokens one at a time, slowing down generation compared to models like GANs or diffusion-based methods that operate more efficiently. While speculative decoding has proven effective for accelerating LLMs by generating multiple tokens in a single forward, its application in visual AR models remains largely unexplored. In this work, we identify a challenge in this setting, which we term \textit{token selection ambiguity}, wherein visual AR models frequently assign uniformly low probabilities to tokens, hampering the performance of speculative decoding. To overcome this challenge, we propose a relaxed acceptance condition referred to as LANTERN that leverages the interchangeability of tokens in latent space. This relaxation restores the effectiveness of speculative decoding in visual AR models by enabling more flexible use of candidate tokens that would otherwise be prematurely rejected. Furthermore, by incorporating a total variation distance bound, we ensure that these speed gains are achieved without significantly compromising image quality or semantic coherence. Experimental results demonstrate the efficacy of our method in providing a substantial speed-up over speculative decoding. In specific, compared to a na&quot;ive application of the state-of-the-art speculative decoding, LANTERN increases speed-ups by $\mathbf{1.75}\times$ and $\mathbf{1.82}\times$, as compared to greedy decoding and random sampling, respectively, when applied to LlamaGen, a contemporary visual AR model. The code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/jadohu/LANTERN">https://github.com/jadohu/LANTERN</a>. </p>
<blockquote>
<p>è‡ªå›å½’ï¼ˆARï¼‰æ¨¡å‹æœ€è¿‘åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸè·å¾—äº†æ˜¾è‘—çš„é‡è¦æ€§ï¼Œå…¶æ€§èƒ½é€šå¸¸ä¸æ‰©æ•£æ¨¡å‹ç›¸åŒ¹é…ç”šè‡³æ›´èƒœä¸€ç­¹ã€‚ç„¶è€Œï¼ŒARæ¨¡å‹çš„ä¸€ä¸ªä¸»è¦å±€é™æ€§åœ¨äºå®ƒä»¬çš„é¡ºåºæ€§ï¼Œå³å®ƒä»¬ä¸€æ¬¡åªå¤„ç†ä¸€ä¸ªæ ‡è®°ç¬¦å·ï¼ˆtokensï¼‰ï¼Œä¸ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰æˆ–åŸºäºæ‰©æ•£çš„æ–¹æ³•ç›¸æ¯”ï¼Œé™ä½äº†ç”Ÿæˆæ•ˆç‡ã€‚å°½ç®¡æ¨æµ‹è§£ç ï¼ˆspeculative decodingï¼‰å·²è¢«è¯æ˜å¯ä»¥é€šè¿‡å•æ¬¡å‰å‘ç”Ÿæˆå¤šä¸ªæ ‡è®°ç¬¦å·æ¥åŠ é€Ÿå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œä½†å…¶åœ¨è§†è§‰ARæ¨¡å‹ä¸­çš„åº”ç”¨ä»é²œæœ‰ç ”ç©¶ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç¡®å®šäº†è¿™ä¸€è®¾ç½®ä¸­çš„ä¸€ä¸ªæŒ‘æˆ˜ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºâ€œæ ‡è®°ç¬¦å·é€‰æ‹©æ­§ä¹‰â€ï¼ˆtoken selection ambiguityï¼‰ï¼Œå…¶ä¸­è§†è§‰ARæ¨¡å‹ç»å¸¸ä¸ºæ ‡è®°ç¬¦å·åˆ†é…ç»Ÿä¸€è¾ƒä½çš„æ¦‚ç‡ï¼Œé˜»ç¢äº†æ¨æµ‹è§£ç çš„æ€§èƒ½ã€‚ä¸ºäº†å…‹æœè¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è½»æ¾çš„æ¥å—æ¡ä»¶ï¼Œç§°ä¸ºLANTERNï¼Œå®ƒåˆ©ç”¨æ½œåœ¨ç©ºé—´ä¸­æ ‡è®°ç¬¦å·çš„å¯äº’æ¢æ€§ã€‚è¿™ç§æ”¾æ¾æ¢å¤äº†æ¨æµ‹è§£ç åœ¨è§†è§‰ARæ¨¡å‹ä¸­çš„æœ‰æ•ˆæ€§ï¼Œé€šè¿‡æ›´çµæ´»åœ°åˆ©ç”¨å€™é€‰æ ‡è®°ç¬¦å·ï¼ˆè¿™äº›æ ‡è®°ç¬¦å·é€šå¸¸ä¼šè¢«è¿‡æ—©åœ°æ‹’ç»ï¼‰ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¼•å…¥æ€»å˜å·®è·ç¦»ç•Œé™ï¼ˆtotal variation distance boundï¼‰ï¼Œæˆ‘ä»¬ç¡®ä¿äº†è¿™äº›é€Ÿåº¦æå‡æ˜¯åœ¨ä¸æŸå®³å›¾åƒè´¨é‡æˆ–è¯­ä¹‰è¿è´¯æ€§çš„å‰æä¸‹å®ç°çš„ã€‚å®éªŒç»“æœè¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•å¯¹äºä¸æ¨æµ‹è§£ç ç›¸æ¯”å…·æœ‰æ˜¾è‘—çš„é€Ÿåº¦æå‡æ•ˆæœã€‚å…·ä½“æ¥è¯´ï¼Œä¸å¯¹æœ€æ–°æ¨æµ‹è§£ç æŠ€æœ¯çš„ç®€å•åº”ç”¨ç›¸æ¯”ï¼ŒLANTERNåœ¨åº”ç”¨äºå½“ä»£è§†è§‰ARæ¨¡å‹LlamaGenæ—¶ï¼Œç›¸å¯¹äºè´ªå¿ƒè§£ç å’Œéšæœºé‡‡æ ·åˆ†åˆ«æé«˜äº†$\mathbf{1.75}\times$å’Œ$\mathbf{1.82}\times$çš„é€Ÿåº¦ã€‚ç›¸å…³ä»£ç å·²å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/jadohu/LANTERN%E4%B8%8A%E3%80%82">https://github.com/jadohu/LANTERNä¸Šã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.03355v3">PDF</a> 30 pages, 13 figures, Accepted to ICLR 2025 (poster)</p>
<p><strong>Summary</strong><br>     è¿‘æœŸè‡ªå›å½’ï¼ˆARï¼‰æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸå—åˆ°å¹¿æ³›å…³æ³¨ï¼Œæ€§èƒ½ç”šè‡³è¶…è¿‡äº†æ‰©æ•£æ¨¡å‹ã€‚ç„¶è€Œï¼Œå…¶é€æ¬¡ç”Ÿæˆæ ‡è®°çš„å›ºæœ‰ç‰¹æ€§é™åˆ¶äº†å…¶ç”Ÿæˆé€Ÿåº¦ã€‚ä¸ºåŠ é€Ÿè§†è§‰ARæ¨¡å‹ï¼Œç ”ç©¶è€…æå‡ºæŠ•æœºè§£ç æ–¹æ³•ï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­é¢ä¸´æ ‡è®°é€‰æ‹©æ¨¡ç³Šçš„é—®é¢˜ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæœ¬ç ”ç©¶æå‡ºä¸€ç§åˆ©ç”¨æ½œåœ¨ç©ºé—´ä¸­æ ‡è®°å¯äº’æ¢æ€§çš„å®½æ¾æ¥å—æ¡ä»¶ï¼ˆLANTERNï¼‰ã€‚æ­¤æ–¹æ³•æ¢å¤äº†è§†è§‰ARæ¨¡å‹ä¸­æŠ•æœºè§£ç çš„æœ‰æ•ˆæ€§ï¼Œé€šè¿‡é‡‡ç”¨æ ‡è®°æ€»å˜åŒ–è·ç¦»ç•Œé™ç¡®ä¿é€Ÿåº¦æå‡ä¸ä¼šæŸå®³å›¾åƒè´¨é‡æˆ–è¯­ä¹‰è¿è´¯æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç›¸è¾ƒäºæ ‡å‡†æŠ•æœºè§£ç æ–¹æ³•ï¼ŒLANTERNåˆ†åˆ«æé«˜äº†1.75å€å’Œ1.82å€çš„åŠ é€Ÿæ•ˆæœã€‚ä»£ç å·²å…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªå›å½’ï¼ˆARï¼‰æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆä¸­å—åˆ°é‡è§†ï¼Œä½†ç”Ÿæˆé€Ÿåº¦å—é™äºå…¶é€æ¬¡ç”Ÿæˆæ ‡è®°çš„å›ºæœ‰ç‰¹æ€§ã€‚</li>
<li>æŠ•æœºè§£ç æ–¹æ³•ç”¨äºåŠ é€Ÿè§†è§‰ARæ¨¡å‹ï¼Œä½†é¢ä¸´æ ‡è®°é€‰æ‹©æ¨¡ç³Šçš„æŒ‘æˆ˜ã€‚</li>
<li>LANTERNæ–¹æ³•åˆ©ç”¨æ½œåœ¨ç©ºé—´ä¸­æ ‡è®°çš„å¯äº’æ¢æ€§æ¥è§£å†³æ­¤é—®é¢˜ï¼Œæ¢å¤äº†æŠ•æœºè§£ç çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>LANTERNé€šè¿‡é‡‡ç”¨æ ‡è®°æ€»å˜åŒ–è·ç¦»ç•Œé™ç¡®ä¿åœ¨æé«˜é€Ÿåº¦çš„åŒæ—¶ä¸æŸå®³å›¾åƒè´¨é‡æˆ–è¯­ä¹‰è¿è´¯æ€§ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œç›¸è¾ƒäºæ ‡å‡†æŠ•æœºè§£ç æ–¹æ³•ï¼ŒLANTERNæ˜¾è‘—æé«˜äº†åŠ é€Ÿæ•ˆæœã€‚</li>
<li>ä»£ç å·²å…¬å¼€ä¾›å…¬ä¼—ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.03355">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-025436d7d92a218b108ba7dc5d19d306.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ef31211c86746dec59b65e53ecd5fe14.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2cf93aa4e6d5f675637af40c4994dfa6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-59a4cc9492a290d548413adf31be9e93.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7d0acc9d9aba839f20cb49b68b2bc385.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Erase-then-Redraw-A-Novel-Data-Augmentation-Approach-for-Free-Space-Detection-Using-Diffusion-Model"><a href="#Erase-then-Redraw-A-Novel-Data-Augmentation-Approach-for-Free-Space-Detection-Using-Diffusion-Model" class="headerlink" title="Erase, then Redraw: A Novel Data Augmentation Approach for Free Space   Detection Using Diffusion Model"></a>Erase, then Redraw: A Novel Data Augmentation Approach for Free Space   Detection Using Diffusion Model</h2><p><strong>Authors:Fulong Ma, Weiqing Qi, Guoyang Zhao, Ming Liu, Jun Ma</strong></p>
<p>Data augmentation is one of the most common tools in deep learning, underpinning many recent advances including tasks such as classification, detection, and semantic segmentation. The standard approach to data augmentation involves simple transformations like rotation and flipping to generate new images. However, these new images often lack diversity along the main semantic dimensions within the data. Traditional data augmentation methods cannot alter high-level semantic attributes such as the presence of vehicles, trees, and buildings in a scene to enhance data diversity. In recent years, the rapid development of generative models has injected new vitality into the field of data augmentation. In this paper, we address the lack of diversity in data augmentation for road detection task by using a pre-trained text-to-image diffusion model to parameterize image-to-image transformations. Our method involves editing images using these diffusion models to change their semantics. In essence, we achieve this goal by erasing instances of real objects from the original dataset and generating new instances with similar semantics in the erased regions using the diffusion model, thereby expanding the original dataset. We evaluate our approach on the KITTI road dataset and achieve the best results compared to other data augmentation methods, which demonstrates the effectiveness of our proposed development. </p>
<blockquote>
<p>æ•°æ®å¢å¼ºæ˜¯æ·±åº¦å­¦ä¹ ä¸­æœ€å¸¸è§çš„å·¥å…·ä¹‹ä¸€ï¼Œä¸ºåŒ…æ‹¬åˆ†ç±»ã€æ£€æµ‹å’Œè¯­ä¹‰åˆ†å‰²ç­‰ä»»åŠ¡åœ¨å†…çš„è®¸å¤šæœ€æ–°è¿›å±•æä¾›äº†æ”¯æŒã€‚ä¼ ç»Ÿçš„æ•°æ®å¢å¼ºæ–¹æ³•é€šå¸¸æ¶‰åŠç®€å•çš„è½¬æ¢ï¼Œå¦‚æ—‹è½¬å’Œç¿»è½¬ä»¥ç”Ÿæˆæ–°å›¾åƒã€‚ç„¶è€Œï¼Œè¿™äº›æ–°å›¾åƒåœ¨æ•°æ®çš„ä¸»è¦è¯­ä¹‰ç»´åº¦ä¸Šé€šå¸¸ç¼ºä¹å¤šæ ·æ€§ã€‚ä¼ ç»Ÿæ•°æ®å¢å¼ºæ–¹æ³•æ— æ³•æ”¹å˜é«˜çº§è¯­ä¹‰å±æ€§ï¼Œä¾‹å¦‚åœºæ™¯ä¸­çš„è½¦è¾†ã€æ ‘æœ¨å’Œå»ºç­‘ç‰©çš„å­˜åœ¨ï¼Œä»¥å¢å¼ºæ•°æ®å¤šæ ·æ€§ã€‚è¿‘å¹´æ¥ï¼Œç”Ÿæˆæ¨¡å‹çš„å¿«é€Ÿå‘å±•ä¸ºæ•°æ®å¢å¼ºé¢†åŸŸæ³¨å…¥äº†æ–°æ´»åŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡ä½¿ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹æ¥å‚æ•°åŒ–å›¾åƒåˆ°å›¾åƒçš„è½¬æ¢ï¼Œè§£å†³äº†é“è·¯æ£€æµ‹ä»»åŠ¡ä¸­æ•°æ®å¢å¼ºç¼ºä¹å¤šæ ·æ€§çš„é—®é¢˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ¶‰åŠä½¿ç”¨è¿™äº›æ‰©æ•£æ¨¡å‹ç¼–è¾‘å›¾åƒï¼Œä»¥æ”¹å˜å…¶è¯­ä¹‰ã€‚æœ¬è´¨ä¸Šï¼Œæˆ‘ä»¬é€šè¿‡ä»åŸå§‹æ•°æ®é›†ä¸­åˆ é™¤çœŸå®å¯¹è±¡çš„å®ä¾‹ï¼Œå¹¶åˆ©ç”¨æ‰©æ•£æ¨¡å‹åœ¨åˆ é™¤çš„åŒºåŸŸç”Ÿæˆå…·æœ‰ç›¸ä¼¼è¯­ä¹‰çš„æ–°å®ä¾‹ï¼Œä»è€Œæ‰©å±•åŸå§‹æ•°æ®é›†ã€‚æˆ‘ä»¬åœ¨KITTIé“è·¯æ•°æ®é›†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œä¸å…¶ä»–æ•°æ®å¢å¼ºæ–¹æ³•ç›¸æ¯”å–å¾—äº†æœ€ä½³ç»“æœï¼Œè¿™è¯æ˜äº†æˆ‘ä»¬æ‰€æå‡ºçš„å‘å±•çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.20164v2">PDF</a> </p>
<p><strong>Summary</strong><br>æ•°æ®å¢å¼ºæ˜¯æ·±åº¦å­¦ä¹ ä¸­çš„å¸¸ç”¨å·¥å…·ï¼Œå¯¹äºåˆ†ç±»ã€æ£€æµ‹å’Œè¯­ä¹‰åˆ†å‰²ç­‰ä»»åŠ¡èµ·åˆ°äº†é‡è¦çš„æ¨åŠ¨ä½œç”¨ã€‚ä¼ ç»Ÿæ•°æ®å¢å¼ºæ–¹æ³•ä¸»è¦é€šè¿‡ç®€å•å˜æ¢ç”Ÿæˆæ–°å›¾åƒï¼Œä½†ç¼ºä¹å¤šæ ·æ€§ã€‚æœ¬æ–‡åˆ©ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹è¿›è¡Œå›¾åƒåˆ°å›¾åƒçš„å‚æ•°åŒ–è½¬æ¢ï¼Œè§£å†³é“è·¯æ£€æµ‹ä»»åŠ¡ä¸­æ•°æ®å¢å¼ºç¼ºä¹å¤šæ ·æ€§çš„é—®é¢˜ã€‚é€šè¿‡æ“¦é™¤åŸå§‹æ•°æ®é›†ä¸­çš„å®é™…å¯¹è±¡å¹¶ç”¨æ‰©æ•£æ¨¡å‹åœ¨æ“¦é™¤åŒºåŸŸç”Ÿæˆå…·æœ‰ç›¸ä¼¼è¯­ä¹‰çš„æ–°å®ä¾‹ï¼Œä»è€Œæ‰©å±•åŸå§‹æ•°æ®é›†ã€‚åœ¨KITTIé“è·¯æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ•°æ®å¢å¼ºåœ¨æ·±åº¦å­¦ä¹ ä¸­çš„é‡è¦ä½œç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨åˆ†ç±»ã€æ£€æµ‹å’Œè¯­ä¹‰åˆ†å‰²ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚</li>
<li>ä¼ ç»Ÿæ•°æ®å¢å¼ºæ–¹æ³•ä¸»è¦é€šè¿‡ç®€å•å˜æ¢ç”Ÿæˆæ–°å›¾åƒï¼Œä½†è¿™ç§æ–¹æ³•ç”Ÿæˆçš„å›¾åƒåœ¨è¯­ä¹‰ç»´åº¦ä¸Šç¼ºä¹å¤šæ ·æ€§ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åœ¨æ•°æ®å¢å¼ºä¸­çš„æ½œåŠ›ï¼Œèƒ½å¤Ÿæ”¹å˜å›¾åƒçš„é«˜çº§è¯­ä¹‰å±æ€§ï¼Œæé«˜æ•°æ®å¤šæ ·æ€§ã€‚</li>
<li>æœ¬æ–‡ä½¿ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹è¿›è¡Œå›¾åƒåˆ°å›¾åƒçš„å‚æ•°åŒ–è½¬æ¢ï¼Œè§£å†³é“è·¯æ£€æµ‹ä»»åŠ¡ä¸­æ•°æ®å¢å¼ºç¼ºä¹å¤šæ ·æ€§çš„æŒ‘æˆ˜ã€‚</li>
<li>æ–¹æ³•é€šè¿‡æ“¦é™¤åŸå§‹æ•°æ®é›†ä¸­çš„å¯¹è±¡å¹¶ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆæ–°å®ä¾‹æ¥æ‰©å±•æ•°æ®é›†ã€‚</li>
<li>åœ¨KITTIé“è·¯æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ•°æ®å¢å¼ºæ–¹é¢å–å¾—äº†æœ€ä½³æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.20164">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a96ee1f947f5353caf23d4150cb4dfd5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b5baa2a152c0748e4899e40eee8ab70.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6dfe61987c09ae18a57f1a082e2c4d99.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45552786e93ed2029575050ff9dbb408.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4e7442fc084ec113344eafa7ef814cb4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d86cdc04787a9c0222a06562eaa300c.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Score-Forgetting-Distillation-A-Swift-Data-Free-Method-for-Machine-Unlearning-in-Diffusion-Models"><a href="#Score-Forgetting-Distillation-A-Swift-Data-Free-Method-for-Machine-Unlearning-in-Diffusion-Models" class="headerlink" title="Score Forgetting Distillation: A Swift, Data-Free Method for Machine   Unlearning in Diffusion Models"></a>Score Forgetting Distillation: A Swift, Data-Free Method for Machine   Unlearning in Diffusion Models</h2><p><strong>Authors:Tianqi Chen, Shujian Zhang, Mingyuan Zhou</strong></p>
<p>The machine learning community is increasingly recognizing the importance of fostering trust and safety in modern generative AI (GenAI) models. We posit machine unlearning (MU) as a crucial foundation for developing safe, secure, and trustworthy GenAI models. Traditional MU methods often rely on stringent assumptions and require access to real data. This paper introduces Score Forgetting Distillation (SFD), an innovative MU approach that promotes the forgetting of undesirable information in diffusion models by aligning the conditional scores of â€œunsafeâ€ classes or concepts with those of â€œsafeâ€ ones. To eliminate the need for real data, our SFD framework incorporates a score-based MU loss into the score distillation objective of a pretrained diffusion model. This serves as a regularization term that preserves desired generation capabilities while enabling the production of synthetic data through a one-step generator. Our experiments on pretrained label-conditional and text-to-image diffusion models demonstrate that our method effectively accelerates the forgetting of target classes or concepts during generation, while preserving the quality of other classes or concepts. This unlearned and distilled diffusion not only pioneers a novel concept in MU but also accelerates the generation speed of diffusion models. Our experiments and studies on a range of diffusion models and datasets confirm that our approach is generalizable, effective, and advantageous for MU in diffusion models. Code is available at <a target="_blank" rel="noopener" href="https://github.com/tqch/score-forgetting-distillation">https://github.com/tqch/score-forgetting-distillation</a>. ($\textbf{Warning:}$ This paper contains sexually explicit imagery, discussions of pornography, racially-charged terminology, and other content that some readers may find disturbing, distressing, and&#x2F;or offensive.) </p>
<blockquote>
<p>æœºå™¨å­¦ä¹ é¢†åŸŸè¶Šæ¥è¶Šè®¤è¯†åˆ°åŸ¹å…»ç°ä»£ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆGenAIï¼‰æ¨¡å‹ä¸­çš„ä¿¡ä»»å’Œå®‰å…¨æ€§è‡³å…³é‡è¦ã€‚æˆ‘ä»¬è®¤ä¸ºæœºå™¨é—å¿˜ï¼ˆMUï¼‰æ˜¯å¼€å‘å®‰å…¨ã€å¯é ã€å¯ä¿¡çš„GenAIæ¨¡å‹çš„é‡è¦åŸºç¡€ã€‚ä¼ ç»Ÿçš„MUæ–¹æ³•å¾€å¾€ä¾èµ–äºä¸¥æ ¼çš„å‡è®¾å¹¶éœ€è¦è®¿é—®çœŸå®æ•°æ®ã€‚æœ¬æ–‡ä»‹ç»äº†Score Forgetting Distillationï¼ˆSFDï¼‰è¿™ä¸€åˆ›æ–°çš„MUæ–¹æ³•ï¼Œå®ƒé€šè¿‡ä½¿â€œä¸å®‰å…¨â€ç±»åˆ«æˆ–æ¦‚å¿µçš„æ¡ä»¶åˆ†æ•°ä¸â€œå®‰å…¨â€ç±»åˆ«æˆ–æ¦‚å¿µçš„æ¡ä»¶åˆ†æ•°ä¿æŒä¸€è‡´ï¼Œä¿ƒè¿›åœ¨æ‰©æ•£æ¨¡å‹ä¸­é—å¿˜ä¸éœ€è¦çš„ä¿¡æ¯ã€‚ä¸ºäº†æ¶ˆé™¤å¯¹çœŸå®æ•°æ®çš„éœ€æ±‚ï¼Œæˆ‘ä»¬çš„SFDæ¡†æ¶å°†åŸºäºåˆ†æ•°çš„MUæŸå¤±çº³å…¥é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„åˆ†æ•°è’¸é¦ç›®æ ‡ä¸­ã€‚è¿™ä½œä¸ºæ­£åˆ™åŒ–é¡¹ï¼Œæ—¢ä¿ç•™äº†æ‰€éœ€çš„ç”Ÿæˆèƒ½åŠ›ï¼Œåˆé€šè¿‡ä¸€æ­¥ç”Ÿæˆå™¨äº§ç”Ÿäº†åˆæˆæ•°æ®ã€‚æˆ‘ä»¬åœ¨é¢„è®­ç»ƒçš„æ ‡ç­¾æ¡ä»¶å’Œæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æœ‰æ•ˆåœ°åŠ é€Ÿäº†ç›®æ ‡ç±»åˆ«æˆ–æ¦‚å¿µçš„é—å¿˜è¿‡ç¨‹ï¼ŒåŒæ—¶ä¿æŒäº†å…¶ä»–ç±»åˆ«æˆ–æ¦‚å¿µçš„è´¨é‡ã€‚è¿™ç§æœªè¢«å­¦ä¹ å’Œæç‚¼çš„æ‰©æ•£ä¸ä»…å¼€åˆ›äº†MUé¢†åŸŸçš„æ–°æ¦‚å¿µï¼Œè¿˜åŠ å¿«äº†æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆé€Ÿåº¦ã€‚æˆ‘ä»¬åœ¨ä¸€ç³»åˆ—æ‰©æ•£æ¨¡å‹å’Œæ•°æ®é›†ä¸Šçš„å®éªŒå’Œç ”ç©¶è¯å®ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¯é€šç”¨çš„ã€æœ‰æ•ˆçš„ï¼Œå¯¹äºæ‰©æ•£æ¨¡å‹ä¸­çš„MUå…·æœ‰ä¼˜åŠ¿ã€‚ä»£ç å¯è®¿é—®äº <a target="_blank" rel="noopener" href="https://github.com/tqch/score-forgetting-distillation">https://github.com/tqch/score-forgetting-distillation</a> ã€‚ï¼ˆè­¦å‘Šï¼šæœ¬æ–‡åŒ…å«è‰²æƒ…å†…å®¹ã€å…³äºè‰²æƒ…è®¨è®ºã€ç§æ—æ€§æœ¯è¯­å’Œå…¶ä»–ä¸€äº›è¯»è€…å¯èƒ½è®¤ä¸ºä»¤äººä¸å®‰ã€ä»¤äººç—›è‹¦å’Œ&#x2F;æˆ–å†’çŠ¯çš„å†…å®¹ã€‚ï¼‰</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.11219v3">PDF</a> ICLR 2025</p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬æ–‡æå‡ºä¸€ç§åä¸ºScore Forgetting Distillationï¼ˆSFDï¼‰çš„æ–°å‹æœºå™¨æ— å­¦ä¹ ï¼ˆMUï¼‰æ–¹æ³•ï¼Œæ—¨åœ¨ä¿ƒè¿›æ‰©æ•£æ¨¡å‹ä¸­ä¸è‰¯ä¿¡æ¯çš„é—å¿˜ã€‚è¯¥æ–¹æ³•é€šè¿‡å¯¹â€œä¸å®‰å…¨â€ç±»åˆ«æˆ–æ¦‚å¿µçš„æ¡ä»¶åˆ†æ•°ä¸â€œå®‰å…¨â€ç±»åˆ«æˆ–æ¦‚å¿µçš„åˆ†æ•°å¯¹é½ï¼Œæ— éœ€çœŸå®æ•°æ®å³å¯å®ç°é—å¿˜ã€‚é€šè¿‡åœ¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ä¸­èå…¥åŸºäºåˆ†æ•°çš„MUæŸå¤±ï¼ŒSFDæ¡†æ¶åœ¨ä¿æŒç”Ÿæˆèƒ½åŠ›çš„åŒæ—¶ï¼Œé€šè¿‡ä¸€æ­¥ç”Ÿæˆå™¨äº§ç”Ÿåˆæˆæ•°æ®ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­æœ‰æ•ˆåŠ é€Ÿç›®æ ‡ç±»åˆ«æˆ–æ¦‚å¿µçš„é—å¿˜ï¼ŒåŒæ—¶ä¿æŒå…¶ä»–ç±»åˆ«æˆ–æ¦‚å¿µçš„è´¨é‡ã€‚æ­¤æ— å­¦ä¹ å’Œè’¸é¦çš„æ‰©æ•£æ¨¡å‹ä¸ä»…å¼€åˆ›äº†MUçš„æ–°æ¦‚å¿µï¼Œè¿˜åŠ é€Ÿäº†æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆé€Ÿåº¦ã€‚å®éªŒå’Œç ”ç©¶è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§æ‰©æ•£æ¨¡å‹å’Œæ•°æ®é›†ä¸Šå…·æœ‰é€šç”¨æ€§ã€æœ‰æ•ˆæ€§å’Œä¼˜åŠ¿ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æœºå™¨å­¦ä¹ é¢†åŸŸæ­£æ—¥ç›Šè®¤è¯†åˆ°åœ¨ç°ä»£ç”Ÿæˆå¼AIï¼ˆGenAIï¼‰æ¨¡å‹ä¸­åŸ¹å…»ä¿¡ä»»å’Œå®‰å…¨çš„é‡è¦æ€§ã€‚</li>
<li>æœºå™¨æ— å­¦ä¹ ï¼ˆMUï¼‰è¢«è§†ä¸ºå¼€å‘å®‰å…¨ã€å¯é å’Œå¯ä¿¡çš„GenAIæ¨¡å‹çš„å…³é”®åŸºç¡€ã€‚</li>
<li>ä¼ ç»ŸMUæ–¹æ³•é€šå¸¸ä¾èµ–äºä¸¥æ ¼çš„å‡è®¾å¹¶éœ€è¦çœŸå®æ•°æ®çš„è®¿é—®ã€‚</li>
<li>æœ¬æ–‡ä»‹ç»Score Forgetting Distillationï¼ˆSFDï¼‰ï¼Œä¸€ç§æ–°å‹çš„MUæ–¹æ³•ï¼Œå®ƒé€šè¿‡å¯¹é½â€œä¸å®‰å…¨â€ç±»åˆ«æˆ–æ¦‚å¿µçš„æ¡ä»¶åˆ†æ•°æ¥ä¿ƒè¿›æ‰©æ•£æ¨¡å‹ä¸­ä¸è‰¯ä¿¡æ¯çš„é—å¿˜ã€‚</li>
<li>SFDæ¡†æ¶èå…¥åŸºäºåˆ†æ•°çš„MUæŸå¤±ï¼Œæ— éœ€çœŸå®æ•°æ®ï¼Œå³å¯åœ¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ä¸­å®ç°é—å¿˜ï¼ŒåŒæ—¶ä¿æŒç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒSFDæ–¹æ³•æœ‰æ•ˆåŠ é€Ÿç›®æ ‡ç±»åˆ«æˆ–æ¦‚å¿µçš„é—å¿˜è¿‡ç¨‹ï¼ŒåŒæ—¶ä¿æŒå…¶ä»–ç±»åˆ«æˆ–æ¦‚å¿µçš„è´¨é‡ã€‚</li>
<li>æ­¤æ— å­¦ä¹ å’Œè’¸é¦çš„æ‰©æ•£æ¨¡å‹ä¸ä»…æ¨åŠ¨äº†MUé¢†åŸŸçš„æ–°æ¦‚å¿µï¼Œè¿˜æé«˜äº†æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆé€Ÿåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.11219">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1cee268ba1c31024cb3a1fe22dbd8e63.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e0649c4c6bd36a3314fcac62afbe509b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6a5961c7b0d4a52a3bb6366f95ebc9ab.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="SPDiffusion-Semantic-Protection-Diffusion-Models-for-Multi-concept-Text-to-image-Generation"><a href="#SPDiffusion-Semantic-Protection-Diffusion-Models-for-Multi-concept-Text-to-image-Generation" class="headerlink" title="SPDiffusion: Semantic Protection Diffusion Models for Multi-concept   Text-to-image Generation"></a>SPDiffusion: Semantic Protection Diffusion Models for Multi-concept   Text-to-image Generation</h2><p><strong>Authors:Yang Zhang, Rui Zhang, Xuecheng Nie, Haochen Li, Jikun Chen, Yifan Hao, Xin Zhang, Luoqi Liu, Ling Li</strong></p>
<p>Recent text-to-image models have achieved impressive results in generating high-quality images. However, when tasked with multi-concept generation creating images that contain multiple characters or objects, existing methods often suffer from semantic entanglement, including concept entanglement and improper attribute binding, leading to significant text-image inconsistency. We identify that semantic entanglement arises when certain regions of the latent features attend to incorrect concept and attribute tokens. In this work, we propose the Semantic Protection Diffusion Model (SPDiffusion) to address both concept entanglement and improper attribute binding using only a text prompt as input. The SPDiffusion framework introduces a novel concept region extraction method SP-Extraction to resolve region entanglement in cross-attention, along with SP-Attn, which protects concept regions from the influence of irrelevant attributes and concepts. To evaluate our method, we test it on existing benchmarks, where SPDiffusion achieves state-of-the-art results, demonstrating its effectiveness. </p>
<blockquote>
<p>è¿‘æœŸçš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹åœ¨ç”Ÿæˆé«˜è´¨é‡å›¾åƒæ–¹é¢å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœã€‚ç„¶è€Œï¼Œå½“é¢å¯¹å¤šæ¦‚å¿µç”Ÿæˆä»»åŠ¡ï¼Œå³ç”ŸæˆåŒ…å«å¤šä¸ªå­—ç¬¦æˆ–å¯¹è±¡çš„å›¾åƒæ—¶ï¼Œç°æœ‰æ–¹æ³•ç»å¸¸é­å—è¯­ä¹‰çº ç¼ çš„é—®é¢˜ï¼ŒåŒ…æ‹¬æ¦‚å¿µçº ç¼ å’Œä¸å½“å±æ€§ç»‘å®šï¼Œå¯¼è‡´æ˜¾è‘—çš„æ–‡æœ¬-å›¾åƒä¸ä¸€è‡´ã€‚æˆ‘ä»¬ç¡®å®šï¼Œå½“æ½œåœ¨ç‰¹å¾ä¸­çš„æŸäº›åŒºåŸŸå…³æ³¨é”™è¯¯çš„æ¦‚å¿µå’Œå±æ€§ä»¤ç‰Œæ—¶ï¼Œå°±ä¼šå‡ºç°è¯­ä¹‰çº ç¼ ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†è¯­ä¹‰ä¿æŠ¤æ‰©æ•£æ¨¡å‹ï¼ˆSPDiffusionï¼‰ï¼Œä»…ä½¿ç”¨æ–‡æœ¬æç¤ºä½œä¸ºè¾“å…¥æ¥è§£å†³æ¦‚å¿µçº ç¼ å’Œä¸å½“å±æ€§ç»‘å®šé—®é¢˜ã€‚SPDiffusionæ¡†æ¶å¼•å…¥äº†ä¸€ç§æ–°çš„æ¦‚å¿µåŒºåŸŸæå–æ–¹æ³•SP-Extractionï¼Œä»¥è§£å†³è·¨æ³¨æ„åŠ›ä¸­çš„åŒºåŸŸçº ç¼ é—®é¢˜ï¼Œä»¥åŠSP-Attnï¼Œå®ƒä¿æŠ¤æ¦‚å¿µåŒºåŸŸå…å—æ— å…³å±æ€§å’Œæ¦‚å¿µçš„å½±å“ã€‚ä¸ºäº†è¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•ï¼Œæˆ‘ä»¬åœ¨ç°æœ‰åŸºå‡†æµ‹è¯•ä¸Šå¯¹å…¶è¿›è¡Œäº†æµ‹è¯•ï¼ŒSPDiffusionå–å¾—äº†æœ€æ–°ç»“æœï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.01327v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºSemantic Protection Diffusion Modelï¼ˆSPDiffusionï¼‰çš„æ–‡æœ¬è½¬å›¾åƒæ¨¡å‹ï¼Œç”¨äºè§£å†³å¤šæ¦‚å¿µç”Ÿæˆæ—¶çš„è¯­ä¹‰çº ç¼ å’Œä¸é€‚å½“å±æ€§ç»‘å®šé—®é¢˜ã€‚è¯¥æ¨¡å‹é€šè¿‡å¼•å…¥SP-Extractionå’ŒSP-Attnæœºåˆ¶ï¼Œæœ‰æ•ˆä¿æŠ¤æ¦‚å¿µåŒºåŸŸå…å—æ— å…³å±æ€§å’Œæ¦‚å¿µçš„å½±å“ï¼Œå®ç°è·¨æ³¨æ„åŠ›åŒºåŸŸçº ç¼ çš„è§£å†³ã€‚åœ¨ç°æœ‰åŸºå‡†æµ‹è¯•ä¸­ï¼ŒSPDiffusionå–å¾—äº†æœ€å…ˆè¿›çš„æˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬è½¬å›¾åƒæ¨¡å‹åœ¨å¤šæ¦‚å¿µç”Ÿæˆæ—¶é¢ä¸´è¯­ä¹‰çº ç¼ é—®é¢˜ã€‚</li>
<li>è¯­ä¹‰çº ç¼ æºäºç‰¹å¾åŒºåŸŸçš„æ³¨æ„åŠ›ä¸æ¦‚å¿µæˆ–å±æ€§æ ‡è®°çš„ä¸åŒ¹é…ã€‚</li>
<li>SPDiffusionæ¨¡å‹æ—¨åœ¨è§£å†³æ¦‚å¿µçº ç¼ å’Œä¸é€‚å½“å±æ€§ç»‘å®šé—®é¢˜ã€‚</li>
<li>SPDiffusionå¼•å…¥SP-Extractionæ–¹æ³•ï¼Œè§£å†³è·¨æ³¨æ„åŠ›åŒºåŸŸçº ç¼ ã€‚</li>
<li>SPDiffusioné‡‡ç”¨SP-Attnæœºåˆ¶ï¼Œä¿æŠ¤æ¦‚å¿µåŒºåŸŸå…å—æ— å…³å±æ€§å’Œæ¦‚å¿µçš„å½±å“ã€‚</li>
<li>åœ¨ç°æœ‰åŸºå‡†æµ‹è¯•ä¸­ï¼ŒSPDiffusionæ¨¡å‹è¡¨ç°å“è¶Šï¼Œè¾¾åˆ°æœ€æ–°æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.01327">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c5373909767e7547394f54de18a73f68.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b6aaf88cff0da30df22e1e3fd8544fe0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-029871c9b641a165d359cdace78ddc9f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c96a400d9a10713a9a9dcd3a749f643a.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Speed-accuracy-relations-for-the-diffusion-models-Wisdom-from-nonequilibrium-thermodynamics-and-optimal-transport"><a href="#Speed-accuracy-relations-for-the-diffusion-models-Wisdom-from-nonequilibrium-thermodynamics-and-optimal-transport" class="headerlink" title="Speed-accuracy relations for the diffusion models: Wisdom from   nonequilibrium thermodynamics and optimal transport"></a>Speed-accuracy relations for the diffusion models: Wisdom from   nonequilibrium thermodynamics and optimal transport</h2><p><strong>Authors:Kotaro Ikeda, Tomoya Uda, Daisuke Okanohara, Sosuke Ito</strong></p>
<p>We discuss a connection between a generative model, called the diffusion model, and nonequilibrium thermodynamics for the Fokker-Planck equation, called stochastic thermodynamics. Based on the techniques of stochastic thermodynamics, we derive the speed-accuracy relations for the diffusion models, which are inequalities that relate the accuracy of data generation to the entropy production rate, which can be interpreted as the speed of the diffusion dynamics in the absence of the non-conservative force. From a stochastic thermodynamic perspective, our results provide a quantitative insight into how best to generate data in diffusion models. The optimal learning protocol is introduced by the geodesic of space of the 2-Wasserstein distance in optimal transport theory. We numerically illustrate the validity of the speed-accuracy relations for the diffusion models with different noise schedules and the different data. We numerically discuss our results for the optimal and suboptimal learning protocols. We also show the inaccurate data generation due to the non-conservative force, and the applicability of our results to data generation from the real-world image datasets. </p>
<blockquote>
<p>æˆ‘ä»¬æ¢è®¨äº†ä¸€ç§ç”Ÿæˆæ¨¡å‹â€”â€”æ‰©æ•£æ¨¡å‹ä¸éå¹³è¡¡æ€çƒ­åŠ›å­¦å’ŒFokker-Planckæ–¹ç¨‹çš„éšæœºçƒ­åŠ›å­¦ä¹‹é—´çš„è”ç³»ã€‚åŸºäºéšæœºçƒ­åŠ›å­¦çš„æŠ€æœ¯ï¼Œæˆ‘ä»¬æ¨å¯¼å‡ºäº†æ‰©æ•£æ¨¡å‹çš„é€Ÿåº¦-ç²¾åº¦å…³ç³»ï¼Œè¿™äº›å…³ç³»æ˜¯ä¸ç­‰å¼ï¼Œæè¿°äº†æ•°æ®ç”Ÿæˆçš„ç²¾åº¦ä¸ç†µäº§ç”Ÿç‡ä¹‹é—´çš„è”ç³»ï¼Œå¯ä»¥è§£é‡Šä¸ºåœ¨æ²¡æœ‰éä¿å®ˆåŠ›çš„æƒ…å†µä¸‹æ‰©æ•£åŠ¨åŠ›å­¦çš„é€Ÿåº¦ã€‚ä»éšæœºçƒ­åŠ›å­¦çš„è§’åº¦æ¥çœ‹ï¼Œæˆ‘ä»¬çš„ç»“æœæä¾›äº†åœ¨æ‰©æ•£æ¨¡å‹ä¸­å¦‚ä½•æœ€ä½³ç”Ÿæˆæ•°æ®çš„å®šé‡è§è§£ã€‚æœ€ä¼˜å­¦ä¹ åè®®ç”±æœ€ä¼˜ä¼ è¾“ç†è®ºä¸­çš„2-Wassersteinè·ç¦»çš„ç©ºé—´æµ‹åœ°çº¿å¼•å…¥ã€‚æˆ‘ä»¬é€šè¿‡æ•°å€¼è¯´æ˜äº†ä¸åŒå™ªå£°å®‰æ’å’Œæ•°æ®ä¸‹æ‰©æ•£æ¨¡å‹çš„é€Ÿåº¦-ç²¾åº¦å…³ç³»çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬æ•°å€¼è®¨è®ºäº†æœ€ä¼˜å’Œæ¬¡ä¼˜å­¦ä¹ åè®®çš„ç»“æœã€‚æˆ‘ä»¬è¿˜å±•ç¤ºäº†ç”±äºéä¿å®ˆåŠ›å¯¼è‡´çš„æ•°æ®ç”Ÿæˆä¸å‡†ç¡®ï¼Œä»¥åŠæˆ‘ä»¬çš„ç»“æœå¯¹ç°å®ä¸–ç•Œå›¾åƒæ•°æ®é›†çš„æ•°æ®ç”Ÿæˆçš„é€‚ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.04495v4">PDF</a> 36 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æ‰©æ•£æ¨¡å‹ä¸åŸºäºFokker-Planckæ–¹ç¨‹çš„éšæœºçƒ­åŠ›å­¦ä¹‹é—´çš„è”ç³»ã€‚é€šè¿‡éšæœºçƒ­åŠ›å­¦æŠ€æœ¯ï¼Œæˆ‘ä»¬æ¨å¯¼å‡ºæ‰©æ•£æ¨¡å‹çš„é€Ÿåº¦-ç²¾åº¦å…³ç³»ä¸ç­‰å¼ï¼Œè¯¥ä¸ç­‰å¼å°†æ•°æ®é‡‡é›†çš„å‡†ç¡®æ€§å…³è”åˆ°ç†µäº§ç”Ÿç‡ä¸Šï¼Œå¯ä»¥è§£è¯»ä¸ºåœ¨æ²¡æœ‰éä¿å®ˆåŠ›ä½œç”¨ä¸‹çš„æ‰©æ•£åŠ¨åŠ›é€Ÿåº¦ã€‚æœ¬æ–‡ç»“æœä»éšæœºçƒ­åŠ›å­¦çš„è§’åº¦ä¸ºæ‰©æ•£æ¨¡å‹ä¸­çš„æ•°æ®é‡‡é›†æä¾›äº†å®šé‡è§è§£ã€‚æ­¤å¤–ï¼Œå¼•å…¥æœ€ä¼˜å­¦ä¹ åè®®â€”â€”æœ€ä¼˜ä¼ è¾“ç†è®ºä¸­çš„2-Wassersteinè·ç¦»ç©ºé—´æµ‹åœ°çº¿ã€‚æˆ‘ä»¬å¯¹ä¸åŒå™ªå£°è®¡åˆ’å’Œæ•°æ®çš„æ‰©æ•£æ¨¡å‹çš„é€Ÿåº¦-ç²¾åº¦å…³ç³»è¿›è¡Œäº†æ•°å€¼éªŒè¯ï¼Œå¹¶è®¨è®ºäº†æœ€ä¼˜å’Œæ¬¡ä¼˜å­¦ä¹ åè®®çš„ç»“æœã€‚åŒæ—¶ï¼Œå±•ç¤ºäº†å› éä¿å®ˆåŠ›å¯¼è‡´çš„æ•°æ®ç”Ÿæˆä¸å‡†ç¡®ï¼Œä»¥åŠæˆ‘ä»¬çš„ç»“æœå¯¹ç°å®ä¸–ç•Œå›¾åƒæ•°æ®é›†çš„æ•°æ®ç”Ÿæˆé€‚ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹ä¸éšæœºçƒ­åŠ›å­¦å­˜åœ¨è”ç³»ï¼Œå¯ç”¨äºæ¨å¯¼é€Ÿåº¦-ç²¾åº¦å…³ç³»ä¸ç­‰å¼ã€‚</li>
<li>é€Ÿåº¦-ç²¾åº¦å…³ç³»ä¸ç­‰å¼å…³è”äº†æ•°æ®ç”Ÿæˆçš„å‡†ç¡®æ€§å’Œç†µäº§ç”Ÿç‡ï¼Œåæ˜ äº†æ‰©æ•£åŠ¨åŠ›å­¦çš„é€Ÿåº¦ã€‚</li>
<li>ä»éšæœºçƒ­åŠ›å­¦è§†è§’ï¼Œä¸ºæ‰©æ•£æ¨¡å‹ä¸­çš„æ•°æ®ç”Ÿæˆæä¾›äº†å®šé‡è§è§£ã€‚</li>
<li>å¼•å…¥æœ€ä¼˜å­¦ä¹ åè®®ï¼ŒåŸºäºæœ€ä¼˜ä¼ è¾“ç†è®ºä¸­çš„2-Wassersteinè·ç¦»ç©ºé—´æµ‹åœ°çº¿ã€‚</li>
<li>é€šè¿‡æ•°å€¼éªŒè¯ï¼Œä¸åŒå™ªå£°è®¡åˆ’å’Œæ•°æ®çš„æ‰©æ•£æ¨¡å‹çš„é€Ÿåº¦-ç²¾åº¦å…³ç³»å¾—åˆ°è¯å®ã€‚</li>
<li>æ¢è®¨äº†æœ€ä¼˜å’Œæ¬¡ä¼˜å­¦ä¹ åè®®çš„ç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.04495">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-54cc77c16b619217240d0ebe9103a771.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f5ad23264d965e956a5ddd48c372473.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-05/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-05/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-05/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-ce4cf8702947ea6ba2c02dc49aa37123.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-05  MobileViM A Light-weight and Dimension-independent Vision Mamba for 3D   Medical Image Analysis
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-05
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-05/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-c272fa6ee6403d416b9f8ff448e3a0d8.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-05  GS-CPR Efficient Camera Pose Refinement via 3D Gaussian Splatting
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-05
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">28051.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
