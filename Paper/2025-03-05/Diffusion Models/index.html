<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-03-05  A Dual-Purpose Framework for Backdoor Defense and Backdoor Amplification   in Diffusion Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-59a4cc9492a290d548413adf31be9e93.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-03-05
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    20.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    85 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-03-05-更新"><a href="#2025-03-05-更新" class="headerlink" title="2025-03-05 更新"></a>2025-03-05 更新</h1><h2 id="A-Dual-Purpose-Framework-for-Backdoor-Defense-and-Backdoor-Amplification-in-Diffusion-Models"><a href="#A-Dual-Purpose-Framework-for-Backdoor-Defense-and-Backdoor-Amplification-in-Diffusion-Models" class="headerlink" title="A Dual-Purpose Framework for Backdoor Defense and Backdoor Amplification   in Diffusion Models"></a>A Dual-Purpose Framework for Backdoor Defense and Backdoor Amplification   in Diffusion Models</h2><p><strong>Authors:Vu Tuan Truong, Long Bao Le</strong></p>
<p>Diffusion models have emerged as state-of-the-art generative frameworks, excelling in producing high-quality multi-modal samples. However, recent studies have revealed their vulnerability to backdoor attacks, where backdoored models generate specific, undesirable outputs called backdoor target (e.g., harmful images) when a pre-defined trigger is embedded to their inputs. In this paper, we propose PureDiffusion, a dual-purpose framework that simultaneously serves two contrasting roles: backdoor defense and backdoor attack amplification. For defense, we introduce two novel loss functions to invert backdoor triggers embedded in diffusion models. The first leverages trigger-induced distribution shifts across multiple timesteps of the diffusion process, while the second exploits the denoising consistency effect when a backdoor is activated. Once an accurate trigger inversion is achieved, we develop a backdoor detection method that analyzes both the inverted trigger and the generated backdoor targets to identify backdoor attacks. In terms of attack amplification with the role of an attacker, we describe how our trigger inversion algorithm can be used to reinforce the original trigger embedded in the backdoored diffusion model. This significantly boosts attack performance while reducing the required backdoor training time. Experimental results demonstrate that PureDiffusion achieves near-perfect detection accuracy, outperforming existing defenses by a large margin, particularly against complex trigger patterns. Additionally, in an attack scenario, our attack amplification approach elevates the attack success rate (ASR) of existing backdoor attacks to nearly 100% while reducing training time by up to 20x. </p>
<blockquote>
<p>扩散模型已经作为最先进的生成框架出现，擅长生成高质量的多模式样本。然而，最近的研究表明，它们容易受到后门攻击的影响，后门模型会在输入嵌入预定义触发器时，生成特定的、不希望出现的输出，称为后门目标（例如，有害图像）。在本文中，我们提出了PureDiffusion，这是一个双重用途的框架，可以同时扮演两个相反的角色：后门防御和后门攻击增强。在防御方面，我们介绍了两种新型损失函数来反转嵌入在扩散模型中的后门触发器。第一种利用触发引起的扩散过程中多个时间步的分布变化，第二种则利用后门被激活时的去噪一致性效应。一旦实现了准确的触发器反转，我们就开发了一种后门检测方法，该方法分析反转的触发器和生成的后门目标，以识别后门攻击。在作为攻击者的角色进行攻击增强方面，我们描述了如何使用我们的触发器反转算法来加强嵌入后门扩散模型中的原始触发器。这显著提高了攻击性能，同时减少了所需的后门训练时间。实验结果表明，PureDiffusion实现了近乎完美的检测精度，大大优于现有的防御手段，尤其是对复杂的触发模式。此外，在攻击场景中，我们的攻击增强方法将现有后门攻击的攻击成功率（ASR）提高到接近100%，同时减少训练时间高达20倍。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.19047v2">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>扩散模型作为当前最先进的生成框架，能够产生高质量的多模式样本。然而，最近的研究表明，它们容易受到后门攻击的影响，其中后门模型会在输入嵌入预定义触发器时生成特定的、不希望的输出，称为后门目标（例如，有害图像）。本文提出了PureDiffusion，一个兼具防御和攻击功能的双重用途框架。在防御方面，我们引入两种新型损失函数来反转嵌入在扩散模型中的后门触发器。第一种利用触发器在扩散过程的多个时间步长中引起的分布变化，第二种则利用后门激活时的去噪一致性效应。实现准确的触发器反转后，我们开发了一种后门检测方法，该方法分析反转的触发器和生成的后门目标，以识别后门攻击。作为攻击者的角色进行攻击放大时，我们描述了如何利用我们的触发器反转算法加强嵌入在受后门控制的扩散模型中的原始触发器。这大大提高了攻击性能，同时减少了所需的后门训练时间。实验结果表明，PureDiffusion实现了近乎完美的检测准确率，大大优于现有防御手段，尤其是对复杂触发模式。此外，在攻击场景中，我们的攻击放大方法可将现有后门攻击的成功率提高到近100%，同时减少训练时间高达20倍。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>扩散模型虽然能生成高质量多模式样本，但易受后门攻击影响，能生成特定不希望的输出。</li>
<li>PureDiffusion框架兼具防御和攻击功能。</li>
<li>引入两种新型损失函数来反转后门触发器。</li>
<li>利用触发器在扩散过程中的分布变化和去噪一致性效应进行防御。</li>
<li>开发了一种分析反转触发器和后门目标以识别后门攻击的方法。</li>
<li>触发器反转算法可用于加强原始嵌入的触发器，提高攻击性能并减少训练时间。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.19047">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b47d901bc0c795916570104600e28910.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0345f3cbcf9cba197f6bab1bafda9d7e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ff63ef0614e2464202d9397e4dfa2c10.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1a9c4e64258811e07ee662b42aea6a43.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Fast-Direct-Query-Efficient-Online-Black-box-Guidance-for-Diffusion-model-Target-Generation"><a href="#Fast-Direct-Query-Efficient-Online-Black-box-Guidance-for-Diffusion-model-Target-Generation" class="headerlink" title="Fast Direct: Query-Efficient Online Black-box Guidance for   Diffusion-model Target Generation"></a>Fast Direct: Query-Efficient Online Black-box Guidance for   Diffusion-model Target Generation</h2><p><strong>Authors:Kim Yong Tan, Yueming Lyu, Ivor Tsang, Yew-Soon Ong</strong></p>
<p>Guided diffusion-model generation is a promising direction for customizing the generation process of a pre-trained diffusion-model to address the specific downstream tasks. Existing guided diffusion models either rely on training of the guidance model with pre-collected datasets or require the objective functions to be differentiable. However, for most real-world tasks, the offline datasets are often unavailable, and their objective functions are often not differentiable, such as image generation with human preferences, molecular generation for drug discovery, and material design. Thus, we need an $\textbf{online}$ algorithm capable of collecting data during runtime and supporting a $\textbf{black-box}$ objective function. Moreover, the $\textbf{query efficiency}$ of the algorithm is also critical because the objective evaluation of the query is often expensive in the real-world scenarios. In this work, we propose a novel and simple algorithm, $\textbf{Fast Direct}$, for query-efficient online black-box target generation. Our Fast Direct builds a pseudo-target on the data manifold to update the noise sequence of the diffusion model with a universal direction, which is promising to perform query-efficient guided generation. Extensive experiments on twelve high-resolution ($\small {1024 \times 1024}$) image target generation tasks and six 3D-molecule target generation tasks show $\textbf{6}\times$ up to $\textbf{10}\times$ query efficiency improvement and $\textbf{11}\times$ up to $\textbf{44}\times$ query efficiency improvement, respectively. Our implementation is publicly available at: <a target="_blank" rel="noopener" href="https://github.com/kimyong95/guide-stable-diffusion/tree/fast-direct">https://github.com/kimyong95/guide-stable-diffusion/tree/fast-direct</a> </p>
<blockquote>
<p>引导扩散模型生成是定制预训练扩散模型的生成过程以应对特定下游任务的一个具有前景的方向。现有的引导扩散模型要么依赖于使用预先收集的数据集对引导模型进行训练，要么需要目标函数可微。然而，对于大多数现实世界任务而言，离线数据集通常不可用，并且其目标函数通常不可微，例如具有人类偏好的图像生成、用于药物发现的分子生成以及材料设计。因此，我们需要一种能够在运行时收集数据并支持黑箱目标函数的在线算法。此外，算法的查询效率也至关重要，因为在现实场景中，目标查询的评估往往成本高昂。在这项工作中，我们提出了一种新颖且简单的算法“Fast Direct”，用于查询高效的在线黑箱目标生成。我们的Fast Direct在数据流形上构建伪目标，以通用方向更新扩散模型的噪声序列，有望执行查询高效的引导生成。在十二个高分辨率（$1024 \times 1024$）图像目标生成任务和六个3D分子目标生成任务上的大量实验显示，查询效率提高了6倍至10倍和提高了高达高达高达高达高达高达高达高达高达高达高达高达高达高达高达高达高达高达高达高达高达高达高达高达高达高达高达高达高达高达提高效率大大提高倍和改进改善了比例4％。我们的实现公开可用在：<a target="_blank" rel="noopener" href="https://github.com/kimyong95/guide-stable-diffusion/tree/fast-direct">链接地址</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01692v4">PDF</a> </p>
<p><strong>摘要</strong><br>扩散模型的在线黑盒目标生成。现有引导扩散模型方法依赖预收集数据集进行训练或要求目标函数可微。然而，对于大多数现实世界任务，离线数据集常不可获取且其目标函数常不可微，如基于人类偏好生成图像、药物发现中的分子生成及材料设计。因此需要能在线运行时收集数据并支持黑盒目标函数的算法。查询效率也至关重要，因为现实场景中目标评估往往昂贵。本研究提出了一种简单而高效的算法“Fast Direct”，用于在线黑盒目标生成。Fast Direct在数据流形上构建伪目标，以通用方向更新扩散模型的噪声序列，有望进行高效的查询引导生成。实验表明，在高分辨率图像目标生成任务和三维分子目标生成任务中，查询效率分别提高了6至10倍和高达44倍。代码公开于：<a target="_blank" rel="noopener" href="https://github.com/kimyong95/guide-stable-diffusion/tree/fast-direct">链接地址</a>。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>引导扩散模型生成是一个针对预训练扩散模型的定制化生成过程的方向，用于解决特定的下游任务。</li>
<li>现有引导扩散模型方法依赖于预收集数据集的训练或要求目标函数可微，但在现实任务中这两点常无法满足。</li>
<li>提出了一种简单而高效的算法“Fast Direct”，支持在线黑盒目标生成。</li>
<li>Fast Direct算法在数据流形上构建伪目标，以通用方向更新扩散模型的噪声序列，实现查询效率的提高。</li>
<li>实验证明，Fast Direct在图像和分子生成任务中实现了显著的查询效率提升。</li>
<li>该算法适用于多种任务类型，包括图像生成、分子生成和材料设计等。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01692">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-16c1552bb779bed6aa7b94b9ce756e19.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4731e5cbbe45cfbf74389e00db11c3ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-984bf365b19c8f44f7f44c4f877ff087.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Slot-Guided-Adaptation-of-Pre-trained-Diffusion-Models-for-Object-Centric-Learning-and-Compositional-Generation"><a href="#Slot-Guided-Adaptation-of-Pre-trained-Diffusion-Models-for-Object-Centric-Learning-and-Compositional-Generation" class="headerlink" title="Slot-Guided Adaptation of Pre-trained Diffusion Models for   Object-Centric Learning and Compositional Generation"></a>Slot-Guided Adaptation of Pre-trained Diffusion Models for   Object-Centric Learning and Compositional Generation</h2><p><strong>Authors:Adil Kaan Akan, Yucel Yemez</strong></p>
<p>We present SlotAdapt, an object-centric learning method that combines slot attention with pretrained diffusion models by introducing adapters for slot-based conditioning. Our method preserves the generative power of pretrained diffusion models, while avoiding their text-centric conditioning bias. We also incorporate an additional guidance loss into our architecture to align cross-attention from adapter layers with slot attention. This enhances the alignment of our model with the objects in the input image without using external supervision. Experimental results show that our method outperforms state-of-the-art techniques in object discovery and image generation tasks across multiple datasets, including those with real images. Furthermore, we demonstrate through experiments that our method performs remarkably well on complex real-world images for compositional generation, in contrast to other slot-based generative methods in the literature. The project page can be found at <a target="_blank" rel="noopener" href="https://kaanakan.github.io/SlotAdapt/">https://kaanakan.github.io/SlotAdapt/</a>. </p>
<blockquote>
<p>我们提出了SlotAdapt，这是一种结合插槽注意力和预训练扩散模型的面向对象的学习方法，它通过引入适配器来实现基于插槽的条件。我们的方法保留了预训练扩散模型的生成能力，同时避免了其面向文本的条件偏差。我们还将额外的指导损失纳入我们的架构，以调整适配器层的交叉注意力与插槽注意力。这提高了我们的模型与输入图像中的对象的对齐度，而无需使用外部监督。实验结果表明，我们的方法在多个数据集上的物体发现和图像生成任务上的表现均优于最先进的技术，包括真实图像数据集。此外，通过实验证明，与其他基于插槽的生成方法相比，我们的方法在复杂真实图像的合成生成方面表现尤为出色。项目页面位于<a target="_blank" rel="noopener" href="https://kaanakan.github.io/SlotAdapt/%E3%80%82">https://kaanakan.github.io/SlotAdapt/。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.15878v3">PDF</a> Accepted to ICLR2025. Project page:   <a target="_blank" rel="noopener" href="https://kaanakan.github.io/SlotAdapt/">https://kaanakan.github.io/SlotAdapt/</a></p>
<p><strong>Summary</strong></p>
<p>SlotAdapt结合槽位注意力和预训练扩散模型，通过引入适配器实现槽位条件化，提升了图像生成和物体发现任务的效果。该方法在多个数据集上表现优异，尤其擅长处理复杂真实图像的组合生成任务。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SlotAdapt结合了槽位注意力和预训练扩散模型，通过引入适配器实现了基于槽位的条件化。</li>
<li>该方法保留了预训练扩散模型的生成能力，并避免了文本中心化的条件化偏见。</li>
<li>SlotAdapt通过额外的指导损失增强了模型与输入图像中物体的对齐度，无需外部监督。</li>
<li>在多个数据集上，SlotAdapt在物体发现和图像生成任务上表现出超越现有技术水平的性能。</li>
<li>该方法在复杂真实图像的组合生成任务上表现尤为出色。</li>
<li>SlotAdapt适用于多种图像生成场景，具有广泛的应用前景。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.15878">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a333f39d3ca0541ed298b536aa8c2e1d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4815807fe36efe3507b7ea321af80910.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="StochSync-Stochastic-Diffusion-Synchronization-for-Image-Generation-in-Arbitrary-Spaces"><a href="#StochSync-Stochastic-Diffusion-Synchronization-for-Image-Generation-in-Arbitrary-Spaces" class="headerlink" title="StochSync: Stochastic Diffusion Synchronization for Image Generation in   Arbitrary Spaces"></a>StochSync: Stochastic Diffusion Synchronization for Image Generation in   Arbitrary Spaces</h2><p><strong>Authors:Kyeongmin Yeo, Jaihoon Kim, Minhyuk Sung</strong></p>
<p>We propose a zero-shot method for generating images in arbitrary spaces (e.g., a sphere for 360{\deg} panoramas and a mesh surface for texture) using a pretrained image diffusion model. The zero-shot generation of various visual content using a pretrained image diffusion model has been explored mainly in two directions. First, Diffusion Synchronization-performing reverse diffusion processes jointly across different projected spaces while synchronizing them in the target space-generates high-quality outputs when enough conditioning is provided, but it struggles in its absence. Second, Score Distillation Sampling-gradually updating the target space data through gradient descent-results in better coherence but often lacks detail. In this paper, we reveal for the first time the interconnection between these two methods while highlighting their differences. To this end, we propose StochSync, a novel approach that combines the strengths of both, enabling effective performance with weak conditioning. Our experiments demonstrate that StochSync provides the best performance in 360{\deg} panorama generation (where image conditioning is not given), outperforming previous finetuning-based methods, and also delivers comparable results in 3D mesh texturing (where depth conditioning is provided) with previous methods. </p>
<blockquote>
<p>我们提出了一种基于预训练图像扩散模型的零样本方法在任意空间（例如，用于360°全景的球体或用于纹理的网格表面）生成图像。使用预训练的图像扩散模型进行各种视觉内容的零样本生成主要探索了两个方向。首先，扩散同步法通过在不同的投影空间上执行反向扩散过程并在目标空间中同步它们来生成高质量输出，但在缺少足够条件的情况下会遇到困难。其次，评分蒸馏采样法通过梯度下降逐步更新目标空间数据，虽然能够保证更好的连贯性，但往往缺乏细节。在本文中，我们首次揭示了这两种方法之间的相互联系，同时强调了它们的差异。为此，我们提出了StochSync这一新方法，它结合了这两种方法的优点，能够在弱条件下进行有效性能表现。我们的实验表明，StochSync在无图像条件的情况下表现出最佳的360°全景生成性能，超越了基于微调的方法，并且在提供深度条件的情况下，其在3D网格纹理生成方面的结果也与之前的方法相当。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.15445v2">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://stochsync.github.io/">https://stochsync.github.io/</a> (ICLR 2025)</p>
<p><strong>Summary</strong></p>
<p>本文提出一种基于预训练图像扩散模型的零样本方法，用于在任意空间（如球体全景图和网格表面纹理）生成图像。文章探讨了两种主要的零样本生成方向：一是通过同步反向扩散过程在目标空间中同步生成高质量输出；二是在目标空间中逐步更新数据以获得更好的连贯性。本文首次揭示了这两种方法的相互联系和差异，并提出了一种新的方法StochSync，结合了这两种方法的优点，在弱条件下实现了有效的性能。实验表明，StochSync在全景图生成方面表现出最佳性能，优于基于微调的方法，同时在提供深度条件的三维网格纹理生成中表现良好。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了一种基于预训练图像扩散模型的零样本方法在任意空间生成图像。</li>
<li>探讨了两种主要的零样本生成方向：Diffusion Synchronization和Score Distillation Sampling，并揭示了它们的差异和相互联系。</li>
<li>提出了一种新的方法StochSync，结合了Diffusion Synchronization和Score Distillation Sampling的优点。</li>
<li>在全景图生成方面，StochSync表现出最佳性能，优于基于微调的方法。</li>
<li>在提供深度条件的三维网格纹理生成中，StochSync具有良好的表现。</li>
<li>该方法能够在弱条件下实现有效性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.15445">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-64aab9f2202744790bee3d7a2ff682df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93ab5f8ef50d286503a9e831057f6b6d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4e1c3aa66004fb8455bbccbdacdbb740.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="The-Superposition-of-Diffusion-Models-Using-the-Ito-Density-Estimator"><a href="#The-Superposition-of-Diffusion-Models-Using-the-Ito-Density-Estimator" class="headerlink" title="The Superposition of Diffusion Models Using the Itô Density Estimator"></a>The Superposition of Diffusion Models Using the Itô Density Estimator</h2><p><strong>Authors:Marta Skreta, Lazar Atanackovic, Avishek Joey Bose, Alexander Tong, Kirill Neklyudov</strong></p>
<p>The Cambrian explosion of easily accessible pre-trained diffusion models suggests a demand for methods that combine multiple different pre-trained diffusion models without incurring the significant computational burden of re-training a larger combined model. In this paper, we cast the problem of combining multiple pre-trained diffusion models at the generation stage under a novel proposed framework termed superposition. Theoretically, we derive superposition from rigorous first principles stemming from the celebrated continuity equation and design two novel algorithms tailor-made for combining diffusion models in SuperDiff. SuperDiff leverages a new scalable It^o density estimator for the log likelihood of the diffusion SDE which incurs no additional overhead compared to the well-known Hutchinson’s estimator needed for divergence calculations. We demonstrate that SuperDiff is scalable to large pre-trained diffusion models as superposition is performed solely through composition during inference, and also enjoys painless implementation as it combines different pre-trained vector fields through an automated re-weighting scheme. Notably, we show that SuperDiff is efficient during inference time, and mimics traditional composition operators such as the logical OR and the logical AND. We empirically demonstrate the utility of using SuperDiff for generating more diverse images on CIFAR-10, more faithful prompt conditioned image editing using Stable Diffusion, as well as improved conditional molecule generation and unconditional de novo structure design of proteins. <a target="_blank" rel="noopener" href="https://github.com/necludov/super-diffusion">https://github.com/necludov/super-diffusion</a> </p>
<blockquote>
<p>扩散模型的涌现非常容易获得的预训练扩散模型揭示了对于结合多种不同的预训练扩散模型的需求，而不必承受重新训练更大的组合模型带来的巨大计算负担。在本文中，我们提出了一种结合多个预训练扩散模型的问题，该问题在生成阶段位于新提出的框架下，称为叠加法。理论上，我们从著名的连续性方程得出了严格的第一原理推导出的叠加法，并针对SuperDiff中的扩散模型设计了两种新颖算法。SuperDiff利用一种新的可扩展的It^o密度估计器来计算扩散随机微分方程的对数似然值，与用于发散计算的众所周知的Hutchinson估计器相比，无需额外的开销。我们证明了SuperDiff能够扩展到大型预训练扩散模型，因为叠加仅在推理过程中通过组合实现，并且通过自动重新加权方案组合不同的预训练向量场，实现起来毫不费力。值得注意的是，我们展示了SuperDiff在推理时间的高效性，并模仿了传统的组合运算符，如逻辑或和逻辑与。我们通过经验证明了在CIFAR-10上生成更多不同图像、使用稳定扩散进行更真实的提示条件图像编辑以及改进条件分子生成和无条件蛋白质全新结构设计的实用性。详情请访问：<a target="_blank" rel="noopener" href="https://github.com/necludov/super-diffusion">https://github.com/necludov/super-diffusion</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17762v2">PDF</a> Accepted as a Spotlight Presentation at the International Conference   on Learning Representations 2025</p>
<p><strong>Summary</strong><br>     这篇论文提出了一种名为SuperDiff的新框架，用于在生成阶段组合多个预训练的扩散模型。SuperDiff利用可扩展的Itō密度估计器计算扩散随机微分方程的对数似然值，无需额外的开销。SuperDiff通过组合不同的预训练向量场实现自动重新加权，可轻松实施，且推理效率高。此外，SuperDiff能够生成更多样化的图像，更忠实于提示条件的图像编辑，以及改进的条件分子生成和无条件蛋白质设计。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SuperDiff框架允许组合多个预训练的扩散模型，无需承担重新训练大型组合模型的重大计算负担。</li>
<li>SuperDiff利用Itō密度估计器计算扩散随机微分方程的对数似然值，具有可扩展性。</li>
<li>通过组合不同的预训练向量场，SuperDiff实现了自动重新加权，使得实施更为轻松。</li>
<li>SuperDiff在推理阶段效率高，模仿了逻辑OR和AND等传统的组合运算符。</li>
<li>SuperDiff能够生成更多样化的图像，如在CIFAR-10上的表现。</li>
<li>SuperDiff能够更忠实于提示条件的图像编辑，如在Stable Diffusion中的应用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17762">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b19baefb484e7a64f9f78cf1b1efecb6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bff0c8e9945edc75c4df69534c65dab5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-86e5f9392a0e8673934433b5fa87e585.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Autoregressive-Video-Generation-without-Vector-Quantization"><a href="#Autoregressive-Video-Generation-without-Vector-Quantization" class="headerlink" title="Autoregressive Video Generation without Vector Quantization"></a>Autoregressive Video Generation without Vector Quantization</h2><p><strong>Authors:Haoge Deng, Ting Pan, Haiwen Diao, Zhengxiong Luo, Yufeng Cui, Huchuan Lu, Shiguang Shan, Yonggang Qi, Xinlong Wang</strong></p>
<p>This paper presents a novel approach that enables autoregressive video generation with high efficiency. We propose to reformulate the video generation problem as a non-quantized autoregressive modeling of temporal frame-by-frame prediction and spatial set-by-set prediction. Unlike raster-scan prediction in prior autoregressive models or joint distribution modeling of fixed-length tokens in diffusion models, our approach maintains the causal property of GPT-style models for flexible in-context capabilities, while leveraging bidirectional modeling within individual frames for efficiency. With the proposed approach, we train a novel video autoregressive model without vector quantization, termed NOVA. Our results demonstrate that NOVA surpasses prior autoregressive video models in data efficiency, inference speed, visual fidelity, and video fluency, even with a much smaller model capacity, i.e., 0.6B parameters. NOVA also outperforms state-of-the-art image diffusion models in text-to-image generation tasks, with a significantly lower training cost. Additionally, NOVA generalizes well across extended video durations and enables diverse zero-shot applications in one unified model. Code and models are publicly available at <a target="_blank" rel="noopener" href="https://github.com/baaivision/NOVA">https://github.com/baaivision/NOVA</a>. </p>
<blockquote>
<p>本文提出了一种新的方法，能够高效地进行自回归视频生成。我们提议将视频生成问题重新表述为未量化的自回归建模，包括时间上的逐帧预测和空间上的集集合预测。与先前自回归模型中的栅格扫描预测或扩散模型中的固定长度标记的联合分布建模不同，我们的方法保持了GPT风格模型的因果特性，以实现灵活的上下文功能，同时利用单个帧内的双向建模来提高效率。通过该方法，我们训练了一种无需向量量化的新型视频自回归模型，称为NOVA。结果表明，NOVA在数据效率、推理速度、视觉保真度和视频流畅性方面超越了先前的自回归视频模型，即使模型容量小得多，也只有0.6B参数。此外，NOVA在文本到图像生成任务上的表现优于最先进的图像扩散模型，并且训练成本显著降低。NOVA还能很好地适应长时间的视频，并在一个统一的模型中实现了多样化的零样本应用。代码和模型可在<a target="_blank" rel="noopener" href="https://github.com/baaivision/NOVA%E4%B8%8A%E5%85%AC%E5%BC%80%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/baaivision/NOVA上公开获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14169v2">PDF</a> Accepted to ICLR 2025. Project page at   <a target="_blank" rel="noopener" href="https://github.com/baaivision/NOVA">https://github.com/baaivision/NOVA</a></p>
<p><strong>Summary</strong><br>扩散模型的新方法可以实现高效自回归视频生成。该研究将视频生成问题重新定义为非量化自回归建模，包括时间帧预测和空时集合预测。与先前的自回归模型中的光栅扫描预测或扩散模型的固定长度标记的联合分布建模不同，该方法保留了GPT风格的模型的上下文灵活性，同时利用单个帧内的双向建模提高效率。该方法训练了一种名为NOVA的新型视频自回归模型，无需矢量量化。结果证明，NOVA在数据效率、推理速度、视觉保真度和视频流畅性方面超越了先前的自回归视频模型，即使模型容量较小（即0.6亿参数）。NOVA还优于最先进的图像扩散模型在文本到图像生成任务中的表现，并且训练成本显著降低。此外，NOVA在扩展视频时长方面具有良好的泛化能力，在一个统一模型中实现了多样化的零样本应用。</p>
<p><strong>Key Takeaways</strong></p>
<p>1.该研究提出了一种新型自回归视频生成方法，基于非量化自回归建模。<br>2.该研究将视频生成问题分解为时间帧预测和空时集合预测。<br>3.新方法保留了GPT风格模型的上下文灵活性，并结合了帧内的双向建模以提高效率。<br>4.引入了一种新型视频自回归模型NOVA，无需矢量量化。<br>5.NOVA在数据效率、推理速度、视觉保真度和视频流畅性方面超越了其他自回归视频模型。<br>6.NOVA在文本到图像生成任务中表现优于最先进的图像扩散模型，且训练成本较低。</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14169">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-39ce9d1bd2f2b9f9c35c2231e9a2cfe5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-103e52e906eb991c230dc43edbeedb55.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-39b4ca274e85d1612b4b947cc85a153f.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="V2X-R-Cooperative-LiDAR-4D-Radar-Fusion-for-3D-Object-Detection-with-Denoising-Diffusion"><a href="#V2X-R-Cooperative-LiDAR-4D-Radar-Fusion-for-3D-Object-Detection-with-Denoising-Diffusion" class="headerlink" title="V2X-R: Cooperative LiDAR-4D Radar Fusion for 3D Object Detection with   Denoising Diffusion"></a>V2X-R: Cooperative LiDAR-4D Radar Fusion for 3D Object Detection with   Denoising Diffusion</h2><p><strong>Authors:Xun Huang, Jinlong Wang, Qiming Xia, Siheng Chen, Bisheng Yang, Xin Li, Cheng Wang, Chenglu Wen</strong></p>
<p>Current Vehicle-to-Everything (V2X) systems have significantly enhanced 3D object detection using LiDAR and camera data. However, these methods suffer from performance degradation in adverse weather conditions. The weatherrobust 4D radar provides Doppler and additional geometric information, raising the possibility of addressing this challenge. To this end, we present V2X-R, the first simulated V2X dataset incorporating LiDAR, camera, and 4D radar. V2X-R contains 12,079 scenarios with 37,727 frames of LiDAR and 4D radar point clouds, 150,908 images, and 170,859 annotated 3D vehicle bounding boxes. Subsequently, we propose a novel cooperative LiDAR-4D radar fusion pipeline for 3D object detection and implement it with various fusion strategies. To achieve weather-robust detection, we additionally propose a Multi-modal Denoising Diffusion (MDD) module in our fusion pipeline. MDD utilizes weather-robust 4D radar feature as a condition to prompt the diffusion model to denoise noisy LiDAR features. Experiments show that our LiDAR-4D radar fusion pipeline demonstrates superior performance in the V2X-R dataset. Over and above this, our MDD module further improved the performance of basic fusion model by up to 5.73%&#x2F;6.70% in foggy&#x2F;snowy conditions with barely disrupting normal performance. The dataset and code will be publicly available at: <a target="_blank" rel="noopener" href="https://github.com/ylwhxht/V2X-R">https://github.com/ylwhxht/V2X-R</a>. </p>
<blockquote>
<p>当前的车对外界（V2X）系统已经通过激光雷达和相机数据显著增强了3D对象检测功能。然而，这些方法在恶劣天气条件下会出现性能下降。天气稳定的4D雷达提供了多普勒和额外的几何信息，为解决这一挑战提供了可能性。为此，我们推出了V2X-R，这是第一个结合了激光雷达、相机和4D雷达的模拟V2X数据集。V2X-R包含12,079个场景，其中包括激光雷达和4D雷达点云37,727帧、图像150,908张以及标注的3D车辆边界框170,859个。随后，我们提出了一种用于3D对象检测的新型合作激光雷达-4D雷达融合管道，并采用了多种融合策略来实现它。为了实现天气稳定的检测，我们在融合管道中额外提出了一种多模态去噪扩散（MDD）模块。MDD利用天气稳定的4D雷达特征作为条件，提示扩散模型对嘈杂的激光雷达特征进行去噪。实验表明，我们的激光雷达-4D雷达融合管道在V2X-R数据集上表现出卓越的性能。除此之外，我们的MDD模块进一步提高了基本融合模型在雾天和雪天的性能，同时几乎不影响正常性能。数据集和代码将在<a target="_blank" rel="noopener" href="https://github.com/ylwhxht/V2X-R%E4%B8%8A%E5%85%AC%E5%BC%80%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/ylwhxht/V2X-R上公开提供。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.08402v3">PDF</a> Accepted by CVPR2025</p>
<p><strong>Summary</strong><br>     本研究针对现有车辆到万物（V2X）系统中使用的LiDAR和摄像机数据在恶劣天气条件下性能下降的问题，引入了4D雷达数据。研究团队创建了首个集成LiDAR、摄像机和4D雷达的模拟V2X数据集V2X-R。此外，还提出了一种新型的基于LiDAR与4D雷达的合作融合管道，用于3D对象检测，并实现了多种融合策略。为提高天气适应性检测性能，该团队在融合管道中引入了多模式去噪扩散（MDD）模块。实验表明，该融合管道在V2X-R数据集上的性能优于传统方法，而MDD模块进一步提高了基本融合模型在雾天和雪天的性能，同时几乎不影响正常性能。数据集和代码已公开发布在GitHub上。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前V2X系统虽利用LiDAR和摄像机数据实现了3D对象检测的显著增强，但在恶劣天气条件下性能下降。</li>
<li>4D雷达提供多普勒和额外的几何信息，有助于解决恶劣天气下的检测挑战。</li>
<li>引入了首个集成LiDAR、摄像机和4D雷达的模拟V2X数据集V2X-R。</li>
<li>提出了新型的基于LiDAR与4D雷达的合作融合管道，用于3D对象检测，并实现了多种融合策略。</li>
<li>多模式去噪扩散（MDD）模块被引入以提高融合管道的天气适应性检测性能。</li>
<li>实验显示，LiDAR-4D雷达融合管道在V2X-R数据集上的表现优于传统方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.08402">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-e9bc3a6c16e9700cbc864ec2358bd9be.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-83c296cbea80b277891cdbd2e130c0f7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4331b64422430d5b8389da3fa7397789.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-06920c780ad59a7a8423ae6ffafa0cdb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19fedd2365742e611bbdc6df65edef3f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-95311d4ef225749d92448be876d6cfb8.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="OFER-Occluded-Face-Expression-Reconstruction"><a href="#OFER-Occluded-Face-Expression-Reconstruction" class="headerlink" title="OFER: Occluded Face Expression Reconstruction"></a>OFER: Occluded Face Expression Reconstruction</h2><p><strong>Authors:Pratheba Selvaraju, Victoria Fernandez Abrevaya, Timo Bolkart, Rick Akkerman, Tianyu Ding, Faezeh Amjadi, Ilya Zharkov</strong></p>
<p>Reconstructing 3D face models from a single image is an inherently ill-posed problem, which becomes even more challenging in the presence of occlusions. In addition to fewer available observations, occlusions introduce an extra source of ambiguity where multiple reconstructions can be equally valid. Despite the ubiquity of the problem, very few methods address its multi-hypothesis nature. In this paper we introduce OFER, a novel approach for single-image 3D face reconstruction that can generate plausible, diverse, and expressive 3D faces, even under strong occlusions. Specifically, we train two diffusion models to generate the shape and expression coefficients of a face parametric model, conditioned on the input image. This approach captures the multi-modal nature of the problem, generating a distribution of solutions as output. However, to maintain consistency across diverse expressions, the challenge is to select the best matching shape. To achieve this, we propose a novel ranking mechanism that sorts the outputs of the shape diffusion network based on predicted shape accuracy scores. We evaluate our method using standard benchmarks and introduce CO-545, a new protocol and dataset designed to assess the accuracy of expressive faces under occlusion. Our results show improved performance over occlusion-based methods, while also enabling the generation of diverse expressions for a given image. </p>
<blockquote>
<p>从单幅图像重建3D人脸模型是一个本质上不适定的问题，在存在遮挡的情况下，这个问题变得更加具有挑战性。除了可用的观察数据较少之外，遮挡引入了额外的模糊性来源，其中多种重建可能是同样有效的。尽管这个问题普遍存在，但很少有方法解决其多假设性质。在本文中，我们介绍了OFER，这是一种用于单图像3D人脸重建的新方法，可以生成合理、多样、富有表现力的3D人脸，即使在强遮挡下也是如此。具体来说，我们训练了两个扩散模型来生成人脸参数模型的形状和表情系数，以输入图像为条件。这种方法捕捉了问题的多模态性质，生成一系列解决方案作为输出。然而，为了保持各种表情的一致性，挑战在于选择最佳匹配的形状。为了实现这一点，我们提出了一种新的排名机制，根据预测的形状准确度得分对形状扩散网络的输出进行排序。我们使用标准基准对我们的方法进行了评估，并引入了CO-545，这是一个新的协议和数据集，旨在评估遮挡下表情脸的准确性。我们的结果显示了在基于遮挡的方法上的性能提升，同时为给定图像生成了多样化的表情。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.21629v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种名为OFER的新型单图像三维面部重建方法，该方法能够生成逼真的、多样化的、富有表现力的三维面部模型，即使在强遮挡条件下也能如此。通过训练两个扩散模型来生成面部参数模型的形状和表情系数，以输入图像为条件。该方法能够捕捉问题的多模态性质，生成解决方案的分布。为了保持各种表情的一致性，挑战在于选择最佳匹配的形状。为此，提出了一种新的排序机制，根据预测的准确形状分数对形状扩散网络的输出进行排序。使用标准基准进行了评估。结果显示与遮挡的方法相比具有更高的性能，并能够在给定的图像上生成多种表情。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OFER是一种用于从单图像进行三维面部重建的方法，能够在遮挡条件下生成逼真、多样化和富有表现力的三维面部模型。</li>
<li>该方法通过训练两个扩散模型来生成面部参数模型的形状和表情系数，以输入图像为条件，捕捉问题的多模态性质。</li>
<li>为了保持各种表情的一致性，提出了一个新颖的排序机制来根据预测的准确形状分数对形状扩散网络的输出进行排序。</li>
<li>该方法在标准基准上的评估表现出色，并引入了一个新的协议和数据集CO-545来评估遮挡条件下的表情准确性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.21629">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6d99a207d9ba2b3dc8bc30419ca9c7e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6d9ce9edb53bed47c9d8692789b9a7fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d361296c237bf1f826082f50f180ccf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9639c65e0159b28caae6b51ed5e7a281.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-603b2aee5e81c5854f928f896b994576.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Probing-the-Latent-Hierarchical-Structure-of-Data-via-Diffusion-Models"><a href="#Probing-the-Latent-Hierarchical-Structure-of-Data-via-Diffusion-Models" class="headerlink" title="Probing the Latent Hierarchical Structure of Data via Diffusion Models"></a>Probing the Latent Hierarchical Structure of Data via Diffusion Models</h2><p><strong>Authors:Antonio Sclocchi, Alessandro Favero, Noam Itzhak Levi, Matthieu Wyart</strong></p>
<p>High-dimensional data must be highly structured to be learnable. Although the compositional and hierarchical nature of data is often put forward to explain learnability, quantitative measurements establishing these properties are scarce. Likewise, accessing the latent variables underlying such a data structure remains a challenge. In this work, we show that forward-backward experiments in diffusion-based models, where data is noised and then denoised to generate new samples, are a promising tool to probe the latent structure of data. We predict in simple hierarchical models that, in this process, changes in data occur by correlated chunks, with a length scale that diverges at a noise level where a phase transition is known to take place. Remarkably, we confirm this prediction in both text and image datasets using state-of-the-art diffusion models. Our results show how latent variable changes manifest in the data and establish how to measure these effects in real data using diffusion models. </p>
<blockquote>
<p>高维数据必须高度结构化才能进行学习。虽然数据的组合和分层性质经常被用来解释学习性，但是建立这些属性的定量测量却很少见。同样，访问这种数据结构背后的潜在变量仍然是一个挑战。在这项工作中，我们展示了基于扩散模型的正向反向实验，其中数据被加入噪声然后再去噪声以生成新样本，是探索数据潜在结构的有前途的工具。我们在简单的分层模型中预测，在此过程中，数据的变化会以相关块的形式发生，长度尺度会在一个已知发生相变的噪声水平处发散。值得注意的是，我们在文本和图像数据集上都使用最先进的扩散模型证实了这一预测。我们的结果展示了潜在变量变化如何在数据中表现出来，并建立了如何使用扩散模型在真实数据中测量这些影响的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.13770v2">PDF</a> 10 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>本文指出高维数据需要高度结构化才能进行学习。虽然数据的组合和层次结构常被用来解释其可学习性，但很少有定量测量来验证这些属性。此外，访问这些数据结构背后的潜在变量也是一个挑战。本研究展示了基于扩散的模型中的正向反向实验是探索数据潜在结构的有前途的工具。我们预测在简单层次模型中，数据在此过程中会按相关块发生变化，长度尺度会在噪声水平处发散，此处已知会发生相变。令人惊讶的是，我们在文本和图像数据集中使用最先进的扩散模型证实了这一预测。本研究展示了潜在变量如何在数据中发生变化，并建立了如何在真实数据中使用扩散模型测量这些效应的方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>高维数据需要高度结构化以便学习。</li>
<li>数据的组合和层次结构对于解释其可学习性很重要，但缺乏定量测量。</li>
<li>访问数据结构的潜在变量是一个挑战。</li>
<li>扩散模型的“正向反向实验”有助于探索数据的潜在结构。</li>
<li>在简单层次模型中，数据在特定噪声水平下会发生相变，并按相关块变化。这一预测在文本和图像数据集中得到证实。</li>
<li>研究展示了潜在变量如何在数据中发生变化。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.13770">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-f7e881b16c293150ee1b0e21415a78d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6790fb189cf6be06c935ac198847a8fc.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Improving-Long-Text-Alignment-for-Text-to-Image-Diffusion-Models"><a href="#Improving-Long-Text-Alignment-for-Text-to-Image-Diffusion-Models" class="headerlink" title="Improving Long-Text Alignment for Text-to-Image Diffusion Models"></a>Improving Long-Text Alignment for Text-to-Image Diffusion Models</h2><p><strong>Authors:Luping Liu, Chao Du, Tianyu Pang, Zehan Wang, Chongxuan Li, Dong Xu</strong></p>
<p>The rapid advancement of text-to-image (T2I) diffusion models has enabled them to generate unprecedented results from given texts. However, as text inputs become longer, existing encoding methods like CLIP face limitations, and aligning the generated images with long texts becomes challenging. To tackle these issues, we propose LongAlign, which includes a segment-level encoding method for processing long texts and a decomposed preference optimization method for effective alignment training. For segment-level encoding, long texts are divided into multiple segments and processed separately. This method overcomes the maximum input length limits of pretrained encoding models. For preference optimization, we provide decomposed CLIP-based preference models to fine-tune diffusion models. Specifically, to utilize CLIP-based preference models for T2I alignment, we delve into their scoring mechanisms and find that the preference scores can be decomposed into two components: a text-relevant part that measures T2I alignment and a text-irrelevant part that assesses other visual aspects of human preference. Additionally, we find that the text-irrelevant part contributes to a common overfitting problem during fine-tuning. To address this, we propose a reweighting strategy that assigns different weights to these two components, thereby reducing overfitting and enhancing alignment. After fine-tuning $512 \times 512$ Stable Diffusion (SD) v1.5 for about 20 hours using our method, the fine-tuned SD outperforms stronger foundation models in T2I alignment, such as PixArt-$\alpha$ and Kandinsky v2.2. The code is available at <a target="_blank" rel="noopener" href="https://github.com/luping-liu/LongAlign">https://github.com/luping-liu/LongAlign</a>. </p>
<blockquote>
<p>文本到图像（T2I）扩散模型的快速发展使其能够根据给定文本生成前所未有的结果。然而，随着文本输入的加长，现有的编码方法（如CLIP）面临局限，将生成的图像与长文本对齐变得具有挑战性。为了解决这些问题，我们提出了LongAlign方法，它包括一种用于处理长文本的片段级编码方法和一种有效的对齐训练分解偏好优化方法。对于片段级编码，长文本被分割成多个片段并分别进行处理。这种方法克服了预训练编码模型的最大输入长度限制。对于偏好优化，我们提供了基于CLIP的偏好模型来微调扩散模型。具体来说，为了将CLIP基于的偏好模型用于T2I对齐，我们深入研究了其评分机制，并发现偏好分数可以分解为两个部分：一个与文本相关的部分，用于衡量T2I对齐情况；另一个与文本无关的部分，用于评估人类偏好的其他视觉方面。此外，我们发现与文本无关的部分会导致微调过程中的常见过拟合问题。针对这一问题，我们提出了一种重新加权策略，为这两个部分分配不同的权重，从而减少过拟合并增强对齐效果。使用我们的方法对$512 \times 512$的Stable Diffusion（SD）v1.5微调约20小时后，经过微调的SD在T2I对齐方面的表现超过了更强的基础模型，如PixArt-$\alpha$和Kandinsky v2.2。代码可在<a target="_blank" rel="noopener" href="https://github.com/luping-liu/LongAlign%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/luping-liu/LongAlign找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.11817v2">PDF</a> </p>
<p><strong>Summary</strong><br>     文本到图像（T2I）扩散模型的快速发展为给定文本生成图像提供了前所未有的结果。然而，随着文本输入的增长，现有的编码方法如CLIP面临挑战，难以将生成的图像与长文本对齐。为此，我们提出LongAlign方法，包括分段级编码和分解偏好优化。分段编码克服预训练编码模型的最大输入长度限制；分解偏好优化则通过提供分解的CLIP偏好模型来微调扩散模型。我们发现CLIP偏好得分可分解为衡量T2I对齐的文本相关部分和评估人类偏好其他视觉方面的文本不相关部分。为解决文本不相关部分导致的过拟合问题，我们提出重新加权策略，减少过拟合，提高对齐效果。使用LongAlign方法微调Stable Diffusion约20小时后，其T2I对齐效果超过更强基准模型，如PixArt-α和Kandinsky v2.2。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文本到图像（T2I）扩散模型能基于给定文本生成图像，且效果前所未有。</li>
<li>随着文本输入增长，现有编码方法如CLIP在处理长文本与图像对齐时面临挑战。</li>
<li>LongAlign方法包括分段级编码和分解偏好优化，以解决这些挑战。</li>
<li>分段级编码克服预训练编码模型的最大输入长度限制。</li>
<li>分解偏好优化利用CLIP偏好模型微调扩散模型，提高其T2I对齐效果。</li>
<li>CLIP偏好得分可分解为文本相关和文本不相关两部分，其中文本不相关部分可能导致过拟合问题。</li>
<li>通过重新加权策略，LongAlign方法减少过拟合，提高图像与文本的对齐效果，且在T2I对齐任务中表现超越某些基准模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.11817">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6c3150e6378eae8de3e1259454b32093.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-39fbe63f3f3bf791638e3404a44b607a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-93452cb6a6338b51a908898edf701e66.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c463b15b743cbcfa2ede0368be787e10.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="CtrLoRA-An-Extensible-and-Efficient-Framework-for-Controllable-Image-Generation"><a href="#CtrLoRA-An-Extensible-and-Efficient-Framework-for-Controllable-Image-Generation" class="headerlink" title="CtrLoRA: An Extensible and Efficient Framework for Controllable Image   Generation"></a>CtrLoRA: An Extensible and Efficient Framework for Controllable Image   Generation</h2><p><strong>Authors:Yifeng Xu, Zhenliang He, Shiguang Shan, Xilin Chen</strong></p>
<p>Recently, large-scale diffusion models have made impressive progress in text-to-image (T2I) generation. To further equip these T2I models with fine-grained spatial control, approaches like ControlNet introduce an extra network that learns to follow a condition image. However, for every single condition type, ControlNet requires independent training on millions of data pairs with hundreds of GPU hours, which is quite expensive and makes it challenging for ordinary users to explore and develop new types of conditions. To address this problem, we propose the CtrLoRA framework, which trains a Base ControlNet to learn the common knowledge of image-to-image generation from multiple base conditions, along with condition-specific LoRAs to capture distinct characteristics of each condition. Utilizing our pretrained Base ControlNet, users can easily adapt it to new conditions, requiring as few as 1,000 data pairs and less than one hour of single-GPU training to obtain satisfactory results in most scenarios. Moreover, our CtrLoRA reduces the learnable parameters by 90% compared to ControlNet, significantly lowering the threshold to distribute and deploy the model weights. Extensive experiments on various types of conditions demonstrate the efficiency and effectiveness of our method. Codes and model weights will be released at <a target="_blank" rel="noopener" href="https://github.com/xyfJASON/ctrlora">https://github.com/xyfJASON/ctrlora</a>. </p>
<blockquote>
<p>最近，大规模扩散模型在文本到图像（T2I）生成方面取得了令人印象深刻的进展。为了进一步增强这些T2I模型的细粒度空间控制力，ControlNet等方法引入了一个额外的网络，学习跟随条件图像。然而，对于每一种条件类型，ControlNet需要在数百万对数据对上独立训练，并需要数百小时的GPU时间，这相当昂贵，使得普通用户难以探索和开发新的条件类型。为了解决这个问题，我们提出了CtrLoRA框架，该框架训练基础ControlNet来学习图像到图像生成的通用知识，从多种基础条件中学习，以及特定条件的LoRAs来捕捉每个条件的独特特征。利用我们预训练的基础ControlNet，用户可以轻松适应新条件，在大多数情况下，仅需1000对数据对和不到一个小时的单GPU训练即可获得令人满意的结果。此外，与ControlNet相比，我们的CtrLoRA减少了90%的可学习参数，大大降低了分布和部署模型权重门槛。对各种条件的广泛实验证明了我们方法的高效性和有效性。代码和模型权重将在<a target="_blank" rel="noopener" href="https://github.com/xyfJASON/ctrlora%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/xyfJASON/ctrlora发布。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.09400v2">PDF</a> ICLR 2025. Code: <a target="_blank" rel="noopener" href="https://github.com/xyfJASON/ctrlora">https://github.com/xyfJASON/ctrlora</a></p>
<p><strong>Summary</strong></p>
<p>大型扩散模型在文本转图像生成领域取得显著进展。为解决现有模型如ControlNet在空间控制上的精细粒度问题，我们提出CtrLoRA框架，通过训练基础ControlNet学习多种基础条件下的图像到图像的生成通用知识，以及针对特定条件的LoRA来捕捉每个条件的独特特征。这使得用户能够轻松适应新条件，只需少量数据对和较短的单GPU训练时间即可获得满意结果。与ControlNet相比，CtrLoRA显著减少了可学习参数，降低了模型权重分布和部署的门槛。实验证明，我们的方法高效且有效。代码和模型权重将在[网址]发布。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型扩散模型在文本转图像生成领域取得显著进展。</li>
<li>ControlNet需要独立训练大量数据对，对于普通用户来说探索和开发新条件具有挑战性。</li>
<li>CtrLoRA框架通过训练基础ControlNet学习通用知识，并通过条件特定的LoRAs捕捉每个条件的独特特征来解决这一问题。</li>
<li>用户可以轻松适应新条件，只需少量数据对和较短的单GPU训练时间即可获得满意结果。</li>
<li>CtrLoRA显著减少了与ControlNet相比的可学习参数。</li>
<li>CtrLoRA降低了模型权重分布和部署的门槛。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.09400">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-87495d031fcfb10c111d83d9db725b80.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e954c55a30e6ee92e5d6b9577284151e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1600d319b2da860cc3f5042e18f08972.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b8262e9eb0d45dec0e7ac61dcea220c3.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="ViBiDSampler-Enhancing-Video-Interpolation-Using-Bidirectional-Diffusion-Sampler"><a href="#ViBiDSampler-Enhancing-Video-Interpolation-Using-Bidirectional-Diffusion-Sampler" class="headerlink" title="ViBiDSampler: Enhancing Video Interpolation Using Bidirectional   Diffusion Sampler"></a>ViBiDSampler: Enhancing Video Interpolation Using Bidirectional   Diffusion Sampler</h2><p><strong>Authors:Serin Yang, Taesung Kwon, Jong Chul Ye</strong></p>
<p>Recent progress in large-scale text-to-video (T2V) and image-to-video (I2V) diffusion models has greatly enhanced video generation, especially in terms of keyframe interpolation. However, current image-to-video diffusion models, while powerful in generating videos from a single conditioning frame, need adaptation for two-frame (start &amp; end) conditioned generation, which is essential for effective bounded interpolation. Unfortunately, existing approaches that fuse temporally forward and backward paths in parallel often suffer from off-manifold issues, leading to artifacts or requiring multiple iterative re-noising steps. In this work, we introduce a novel, bidirectional sampling strategy to address these off-manifold issues without requiring extensive re-noising or fine-tuning. Our method employs sequential sampling along both forward and backward paths, conditioned on the start and end frames, respectively, ensuring more coherent and on-manifold generation of intermediate frames. Additionally, we incorporate advanced guidance techniques, CFG++ and DDS, to further enhance the interpolation process. By integrating these, our method achieves state-of-the-art performance, efficiently generating high-quality, smooth videos between keyframes. On a single 3090 GPU, our method can interpolate 25 frames at 1024 x 576 resolution in just 195 seconds, establishing it as a leading solution for keyframe interpolation. </p>
<blockquote>
<p>近期大规模文本到视频（T2V）和图像到视频（I2V）扩散模型的进展极大地推动了视频生成，尤其在关键帧插值方面。然而，当前的图像到视频扩散模型虽然能够从单个条件帧生成视频，但需要进行两帧（开始和结束）条件生成适应，这对于有效的有界插值至关重要。不幸的是，现有方法经常在并行融合时间正向和反向路径时遇到流形外问题，导致出现伪影或需要多次迭代去噪步骤。在这项工作中，我们引入了一种新的双向采样策略来解决这些流形外问题，而无需进行广泛的重噪声处理或微调。我们的方法沿着正向和反向路径进行顺序采样，分别以开始帧和结束帧为条件，确保生成中间帧更加连贯且在流形内。此外，我们还结合了先进的引导技术CFG++和DDS来进一步增强插值过程。通过整合这些技术，我们的方法实现了最先进的性能，能够高效生成高质量、平滑的视频。在单个3090 GPU上，我们的方法可以在仅195秒内插值出分辨率为1024 x 576的25帧，使其成为关键帧插值的领先解决方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.05651v3">PDF</a> ICLR 2025; Project page: <a target="_blank" rel="noopener" href="https://vibidsampler.github.io/">https://vibidsampler.github.io/</a></p>
<p><strong>Summary</strong><br>     近期文本到视频（T2V）和图像到视频（I2V）扩散模型的进展极大促进了视频生成，特别是在关键帧插值方面。然而，当前图像到视频的扩散模型虽然能从单个条件帧生成视频，但在两帧（开始和结束）条件下的生成仍需要改进。本文引入了一种新的双向采样策略，解决了离流问题，提高了关键帧插值的质量，不需要大量重新去噪或微调。方法沿正向和反向路径进行顺序采样，分别以开始和结束帧为条件，保证中间帧的生成更加连贯且在流形上。结合先进的引导技术CFG++和DDS，进一步提高了插值过程的效果。此方法达到了领先水平，能在单个3090 GPU上高效生成高质量、平滑的视频。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型文本到视频和图像到视频扩散模型的最新进展已显著提高视频生成能力，特别是在关键帧插值方面。</li>
<li>当前图像到视频扩散模型需要在两帧（开始和结束）条件下进行生成改进。</li>
<li>引入新的双向采样策略解决离流问题，提高关键帧插值质量。</li>
<li>顺序采样沿正向和反向路径进行，以开始和结束帧为条件，确保中间帧的连贯性和在流形上。</li>
<li>结合先进的引导技术CFG++和DDS增强插值过程。</li>
<li>方法达到领先水平，高效生成高质量、平滑的视频。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.05651">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f3bfde46fa317dab10869b336396ece3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5fd361526fbaf70019f19565c8c3c0ee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e622b9382ef149bf94a1fa599dac93d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc1cb8b1d5da0b68fa18afc1c46a8924.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Image-Watermarks-are-Removable-Using-Controllable-Regeneration-from-Clean-Noise"><a href="#Image-Watermarks-are-Removable-Using-Controllable-Regeneration-from-Clean-Noise" class="headerlink" title="Image Watermarks are Removable Using Controllable Regeneration from   Clean Noise"></a>Image Watermarks are Removable Using Controllable Regeneration from   Clean Noise</h2><p><strong>Authors:Yepeng Liu, Yiren Song, Hai Ci, Yu Zhang, Haofan Wang, Mike Zheng Shou, Yuheng Bu</strong></p>
<p>Image watermark techniques provide an effective way to assert ownership, deter misuse, and trace content sources, which has become increasingly essential in the era of large generative models. A critical attribute of watermark techniques is their robustness against various manipulations. In this paper, we introduce a watermark removal approach capable of effectively nullifying state-of-the-art watermarking techniques. Our primary insight involves regenerating the watermarked image starting from a clean Gaussian noise via a controllable diffusion model, utilizing the extracted semantic and spatial features from the watermarked image. The semantic control adapter and the spatial control network are specifically trained to control the denoising process towards ensuring image quality and enhancing consistency between the cleaned image and the original watermarked image. To achieve a smooth trade-off between watermark removal performance and image consistency, we further propose an adjustable and controllable regeneration scheme. This scheme adds varying numbers of noise steps to the latent representation of the watermarked image, followed by a controlled denoising process starting from this noisy latent representation. As the number of noise steps increases, the latent representation progressively approaches clean Gaussian noise, facilitating the desired trade-off. We apply our watermark removal methods across various watermarking techniques, and the results demonstrate that our methods offer superior visual consistency&#x2F;quality and enhanced watermark removal performance compared to existing regeneration approaches. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/yepengliu/CtrlRegen">https://github.com/yepengliu/CtrlRegen</a>. </p>
<blockquote>
<p>图像水印技术为声明所有权、阻止滥用和追踪内容来源提供了一种有效途径，这在大型生成模型时代变得日益重要。水印技术的关键属性是它们对各种操作的鲁棒性。在本文中，我们介绍了一种能够有效消除最先进水印技术的水印去除方法。我们的主要见解是通过可控的扩散模型，从干净的高斯噪声开始重新生成水印图像，利用从水印图像中提取的语义和空间特征。语义控制适配器和空间控制网络经过专门训练，以控制去噪过程，确保图像质量，并增强清洁图像与原始水印图像之间的一致性。为了实现水印去除性能和图像一致性之间的平稳权衡，我们进一步提出了可调可控的再生方案。该方案向水印图像的潜在表示添加不同数量的噪声步骤，然后从这个噪声的潜在表示开始进行控制去噪过程。随着噪声步骤数量的增加，潜在表示逐渐接近干净的高斯噪声，实现了所需的权衡。我们在各种水印技术中应用了我们的水印去除方法，结果表明，与现有的再生方法相比，我们的方法在视觉一致性&#x2F;质量和水印去除性能方面表现出色。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/yepengliu/CtrlRegen%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/yepengliu/CtrlRegen找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.05470v2">PDF</a> ICLR2025</p>
<p><strong>Summary</strong></p>
<p>针对大型生成模型时代版权保护的需求，本文提出了一种高效的水印移除方法。该方法基于可控的扩散模型，从清洁的高斯噪声开始重新生成水印图像，利用提取的语义和空间特征进行训练和优化。通过调整噪声步数，实现了水印移除与图像一致性之间的平衡。相较于现有方法，本文提出的方法在视觉一致性和水印移除性能上表现更优秀。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>图像水印技术的重要性在于对内容的溯源、防止误用和主张所有权。</li>
<li>本文提出了一种新的水印移除方法，基于可控的扩散模型从清洁的高斯噪声重新生成水印图像。</li>
<li>通过语义控制适配器和空间控制网络，确保图像质量和与原始水印图像的一致性。</li>
<li>提出了一种可调整的控制再生方案，通过增加噪声步数实现水印移除与图像一致性之间的平衡。</li>
<li>该方法在各种水印技术上的应用均取得了良好效果，相较于现有方法具有优越的性能。</li>
<li>该研究的代码已公开在GitHub上。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.05470">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-e8fc55ffae4533b953c7c407aa2835c2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-83ef17f486a92c0e1e2631b22a3116f9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ef32afdfca8ff5ed12ee1566bcafd6a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-86354b7504b0a0e01945c4df0799cda1.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="FedBiP-Heterogeneous-One-Shot-Federated-Learning-with-Personalized-Latent-Diffusion-Models"><a href="#FedBiP-Heterogeneous-One-Shot-Federated-Learning-with-Personalized-Latent-Diffusion-Models" class="headerlink" title="FedBiP: Heterogeneous One-Shot Federated Learning with Personalized   Latent Diffusion Models"></a>FedBiP: Heterogeneous One-Shot Federated Learning with Personalized   Latent Diffusion Models</h2><p><strong>Authors:Haokun Chen, Hang Li, Yao Zhang, Jinhe Bi, Gengyuan Zhang, Yueqi Zhang, Philip Torr, Jindong Gu, Denis Krompass, Volker Tresp</strong></p>
<p>One-Shot Federated Learning (OSFL), a special decentralized machine learning paradigm, has recently gained significant attention. OSFL requires only a single round of client data or model upload, which reduces communication costs and mitigates privacy threats compared to traditional FL. Despite these promising prospects, existing methods face challenges due to client data heterogeneity and limited data quantity when applied to real-world OSFL systems. Recently, Latent Diffusion Models (LDM) have shown remarkable advancements in synthesizing high-quality images through pretraining on large-scale datasets, thereby presenting a potential solution to overcome these issues. However, directly applying pretrained LDM to heterogeneous OSFL results in significant distribution shifts in synthetic data, leading to performance degradation in classification models trained on such data. This issue is particularly pronounced in rare domains, such as medical imaging, which are underrepresented in LDM’s pretraining data. To address this challenge, we propose Federated Bi-Level Personalization (FedBiP), which personalizes the pretrained LDM at both instance-level and concept-level. Hereby, FedBiP synthesizes images following the client’s local data distribution without compromising the privacy regulations. FedBiP is also the first approach to simultaneously address feature space heterogeneity and client data scarcity in OSFL. Our method is validated through extensive experiments on three OSFL benchmarks with feature space heterogeneity, as well as on challenging medical and satellite image datasets with label heterogeneity. The results demonstrate the effectiveness of FedBiP, which substantially outperforms other OSFL methods. </p>
<blockquote>
<p>一次性联邦学习（OSFL）是一种特殊的去中心化机器学习范式，最近引起了广泛的关注。OSFL只需要一轮客户端数据或模型上传，与传统的联邦学习相比，这降低了通信成本并减轻了隐私威胁。尽管前景充满希望，但现有方法在应用于现实世界OSFL系统时，面临着客户数据异质性和数据量有限的挑战。</p>
</blockquote>
<p>最近，潜在扩散模型（LDM）在通过大规模数据集进行预训练后，在合成高质量图像方面取得了显著进展，为解决这些问题提供了潜在解决方案。然而，直接将预训练的LDM应用于异质OSFL会导致合成数据分布的重大变化，导致在由此类数据训练的分类模型性能下降。这一问题在LDM预训练数据代表性不足的罕见领域（如医学影像）中尤其严重。</p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.04810v2">PDF</a> CVPR 2025</p>
<p><strong>摘要</strong></p>
<p>OSFL（一次联邦学习）是一种特殊的分布式机器学习范式，因仅需一轮客户端数据或模型上传而受到关注。与传统的联邦学习相比，OSFL降低了通信成本并减轻了隐私威胁。然而，现有方法在应用到现实世界OSFL系统时面临诸多挑战，如客户数据异质性和数据量有限等。潜在扩散模型（LDM）在预训练大规模数据集上表现出合成高质量图像的巨大潜力，但直接应用于异质的OSFL会产生显著的分布偏移。针对此问题，本文提出了联邦双层个性化（FedBiP）方法，该方法在实例层面和概念层面对预训练的LDM进行个性化调整。FedBiP能够根据客户端的本地数据分布合成图像，同时遵守隐私规定。此外，FedBiP还是首个同时解决OSFL中的特征空间异质性和客户数据稀缺性的方法。实验结果表明，FedBiP在具有特征空间异质性的三个OSFL基准测试以及具有标签异质性的医疗和卫星图像数据集上表现优异。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>OSFL仅需要一轮客户端数据或模型上传，降低了通信成本和隐私威胁。</li>
<li>LDM在合成高质量图像方面表现出显著优势，但直接应用于异质OSFL会导致性能下降。</li>
<li>FedBiP旨在解决这一问题，通过个性化预训练的LDM以适应客户端的本地数据分布。</li>
<li>FedBiP是首个同时解决OSFL中的特征空间异质性和客户数据稀缺性的方法。</li>
<li>FedBiP在多个基准测试集上表现优异，特别是在具有挑战性的医疗和卫星图像数据集上。</li>
<li>FedBiP的合成图像遵循隐私规定，满足联邦学习的隐私保护需求。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.04810">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b343848eae5d3a0637bd307ab396c7bc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8bee296d3bbc830242faff0b2d9cad84.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e247307f7d1de71d56034e0ec2be99fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db23b150ea213e082b0a77955f16232d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ac83a30627a5e57d72abfc8e309ddc7.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Diffusion-State-Guided-Projected-Gradient-for-Inverse-Problems"><a href="#Diffusion-State-Guided-Projected-Gradient-for-Inverse-Problems" class="headerlink" title="Diffusion State-Guided Projected Gradient for Inverse Problems"></a>Diffusion State-Guided Projected Gradient for Inverse Problems</h2><p><strong>Authors:Rayhan Zirvi, Bahareh Tolooshams, Anima Anandkumar</strong></p>
<p>Recent advancements in diffusion models have been effective in learning data priors for solving inverse problems. They leverage diffusion sampling steps for inducing a data prior while using a measurement guidance gradient at each step to impose data consistency. For general inverse problems, approximations are needed when an unconditionally trained diffusion model is used since the measurement likelihood is intractable, leading to inaccurate posterior sampling. In other words, due to their approximations, these methods fail to preserve the generation process on the data manifold defined by the diffusion prior, leading to artifacts in applications such as image restoration. To enhance the performance and robustness of diffusion models in solving inverse problems, we propose Diffusion State-Guided Projected Gradient (DiffStateGrad), which projects the measurement gradient onto a subspace that is a low-rank approximation of an intermediate state of the diffusion process. DiffStateGrad, as a module, can be added to a wide range of diffusion-based inverse solvers to improve the preservation of the diffusion process on the prior manifold and filter out artifact-inducing components. We highlight that DiffStateGrad improves the robustness of diffusion models in terms of the choice of measurement guidance step size and noise while improving the worst-case performance. Finally, we demonstrate that DiffStateGrad improves upon the state-of-the-art on linear and nonlinear image restoration inverse problems. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/neuraloperator/DiffStateGrad">https://github.com/neuraloperator/DiffStateGrad</a>. </p>
<blockquote>
<p>最近扩散模型的进展在学习数据先验以解决反问题方面非常有效。他们利用扩散采样步骤来诱导数据先验，同时在每一步使用测量指导梯度来施加数据一致性。对于一般反问题，当使用无条件训练的扩散模型时，需要近似处理，因为测量可能性难以处理，导致后验采样不准确。换句话说，由于这些近似，这些方法无法保留由扩散先验定义的数据流形上的生成过程，导致图像恢复等应用中的伪影。为了提高扩散模型解决反问题的性能和稳健性，我们提出了扩散状态引导投影梯度（DiffStateGrad），它将测量梯度投影到扩散过程中间状态的低秩近似子空间上。DiffStateGrad作为一个模块，可以添加到广泛的基于扩散的反问题求解器中，以提高对先验流形上扩散过程的保留能力，并过滤掉产生伪影的组件。我们强调，DiffStateGrad提高了扩散模型在选择测量指导步长和噪声方面的稳健性，同时提高了最坏情况下的性能。最后，我们证明DiffStateGrad在解决线性和非线性图像恢复反问题上优于现有技术。我们的代码可在[<a target="_blank" rel="noopener" href="https://github.com/neuraloperator/DiffStateGrad%E6%89%BE%E5%88%B0%E3%80%82]">https://github.com/neuraloperator/DiffStateGrad找到。]</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.03463v3">PDF</a> Published as a conference paper at ICLR 2025. RZ and BT have equal   contributions</p>
<p><strong>Summary</strong><br>     近期扩散模型在解决反问题方面表现出强大的能力，学习数据先验并利用扩散采样步骤和测量指导梯度。然而，无条件训练的扩散模型在处理反问题时需要使用近似方法，导致测量可能性不可追踪和准确性下降。本文提出了一种新的方法，Diffusion State-Guided Projected Gradient (DiffStateGrad)，能够提升扩散模型在解决反问题时的性能和鲁棒性。DiffStateGrad通过将测量梯度投影到扩散过程中间状态的低秩近似子空间，提高了数据流形上的扩散过程保护并过滤掉了导致假象的组件。最后通过实验结果证明，DiffStateGrad在线性和非线性图像修复反问题上达到了先进水平。更多详情可访问GitHub链接。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型在解决反问题方面展现出强大的能力，能够通过学习数据先验进行反问题的求解。</li>
<li>扩散模型在处理反问题时需要使用近似方法，导致测量可能性的追踪困难并可能导致准确性下降。</li>
<li>提出的DiffStateGrad方法通过将测量梯度投影到扩散过程的中间状态子空间，提升了扩散模型在解决反问题时的性能。</li>
<li>DiffStateGrad作为一种模块，可广泛应用于各种基于扩散的反问题求解器中以改进先验流形的保护性能并过滤掉产生假象的组件。</li>
<li>DiffStateGrad提高了扩散模型在选择测量指导步骤大小和噪声方面的鲁棒性。</li>
<li>实验结果表明，DiffStateGrad在线性和非线性图像修复反问题上优于现有技术。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.03463">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-0c3b103583e1e35bfcd015a133674b7c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1ae0b5590baf824e5ae43aac9c99a51a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-80d50ea2301487db0de96f84525934bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d73a08f0b1ddc496f7fb4e752bfdf443.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d56557d6bd577bafba3c251e3160f84.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c5ff6bc79d9e77536d65330d42581135.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="LANTERN-Accelerating-Visual-Autoregressive-Models-with-Relaxed-Speculative-Decoding"><a href="#LANTERN-Accelerating-Visual-Autoregressive-Models-with-Relaxed-Speculative-Decoding" class="headerlink" title="LANTERN: Accelerating Visual Autoregressive Models with Relaxed   Speculative Decoding"></a>LANTERN: Accelerating Visual Autoregressive Models with Relaxed   Speculative Decoding</h2><p><strong>Authors:Doohyuk Jang, Sihwan Park, June Yong Yang, Yeonsung Jung, Jihun Yun, Souvik Kundu, Sung-Yub Kim, Eunho Yang</strong></p>
<p>Auto-Regressive (AR) models have recently gained prominence in image generation, often matching or even surpassing the performance of diffusion models. However, one major limitation of AR models is their sequential nature, which processes tokens one at a time, slowing down generation compared to models like GANs or diffusion-based methods that operate more efficiently. While speculative decoding has proven effective for accelerating LLMs by generating multiple tokens in a single forward, its application in visual AR models remains largely unexplored. In this work, we identify a challenge in this setting, which we term \textit{token selection ambiguity}, wherein visual AR models frequently assign uniformly low probabilities to tokens, hampering the performance of speculative decoding. To overcome this challenge, we propose a relaxed acceptance condition referred to as LANTERN that leverages the interchangeability of tokens in latent space. This relaxation restores the effectiveness of speculative decoding in visual AR models by enabling more flexible use of candidate tokens that would otherwise be prematurely rejected. Furthermore, by incorporating a total variation distance bound, we ensure that these speed gains are achieved without significantly compromising image quality or semantic coherence. Experimental results demonstrate the efficacy of our method in providing a substantial speed-up over speculative decoding. In specific, compared to a na&quot;ive application of the state-of-the-art speculative decoding, LANTERN increases speed-ups by $\mathbf{1.75}\times$ and $\mathbf{1.82}\times$, as compared to greedy decoding and random sampling, respectively, when applied to LlamaGen, a contemporary visual AR model. The code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/jadohu/LANTERN">https://github.com/jadohu/LANTERN</a>. </p>
<blockquote>
<p>自回归（AR）模型最近在图像生成领域获得了显著的重要性，其性能通常与扩散模型相匹配甚至更胜一筹。然而，AR模型的一个主要局限性在于它们的顺序性，即它们一次只处理一个标记符号（tokens），与生成对抗网络（GANs）或基于扩散的方法相比，降低了生成效率。尽管推测解码（speculative decoding）已被证明可以通过单次前向生成多个标记符号来加速大型语言模型（LLMs），但其在视觉AR模型中的应用仍鲜有研究。在这项工作中，我们确定了这一设置中的一个挑战，我们称之为“标记符号选择歧义”（token selection ambiguity），其中视觉AR模型经常为标记符号分配统一较低的概率，阻碍了推测解码的性能。为了克服这一挑战，我们提出了一种轻松的接受条件，称为LANTERN，它利用潜在空间中标记符号的可互换性。这种放松恢复了推测解码在视觉AR模型中的有效性，通过更灵活地利用候选标记符号（这些标记符号通常会被过早地拒绝）。此外，通过引入总变差距离界限（total variation distance bound），我们确保了这些速度提升是在不损害图像质量或语义连贯性的前提下实现的。实验结果表明我们的方法对于与推测解码相比具有显著的速度提升效果。具体来说，与对最新推测解码技术的简单应用相比，LANTERN在应用于当代视觉AR模型LlamaGen时，相对于贪心解码和随机采样分别提高了$\mathbf{1.75}\times$和$\mathbf{1.82}\times$的速度。相关代码已公开在<a target="_blank" rel="noopener" href="https://github.com/jadohu/LANTERN%E4%B8%8A%E3%80%82">https://github.com/jadohu/LANTERN上。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.03355v3">PDF</a> 30 pages, 13 figures, Accepted to ICLR 2025 (poster)</p>
<p><strong>Summary</strong><br>     近期自回归（AR）模型在图像生成领域受到广泛关注，性能甚至超过了扩散模型。然而，其逐次生成标记的固有特性限制了其生成速度。为加速视觉AR模型，研究者提出投机解码方法，但在实际应用中面临标记选择模糊的问题。为解决此问题，本研究提出一种利用潜在空间中标记可互换性的宽松接受条件（LANTERN）。此方法恢复了视觉AR模型中投机解码的有效性，通过采用标记总变化距离界限确保速度提升不会损害图像质量或语义连贯性。实验结果表明，相较于标准投机解码方法，LANTERN分别提高了1.75倍和1.82倍的加速效果。代码已公开。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自回归（AR）模型在图像生成中受到重视，但生成速度受限于其逐次生成标记的固有特性。</li>
<li>投机解码方法用于加速视觉AR模型，但面临标记选择模糊的挑战。</li>
<li>LANTERN方法利用潜在空间中标记的可互换性来解决此问题，恢复了投机解码的有效性。</li>
<li>LANTERN通过采用标记总变化距离界限确保在提高速度的同时不损害图像质量或语义连贯性。</li>
<li>实验结果表明，相较于标准投机解码方法，LANTERN显著提高了加速效果。</li>
<li>代码已公开供公众使用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.03355">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-025436d7d92a218b108ba7dc5d19d306.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ef31211c86746dec59b65e53ecd5fe14.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2cf93aa4e6d5f675637af40c4994dfa6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-59a4cc9492a290d548413adf31be9e93.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7d0acc9d9aba839f20cb49b68b2bc385.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Erase-then-Redraw-A-Novel-Data-Augmentation-Approach-for-Free-Space-Detection-Using-Diffusion-Model"><a href="#Erase-then-Redraw-A-Novel-Data-Augmentation-Approach-for-Free-Space-Detection-Using-Diffusion-Model" class="headerlink" title="Erase, then Redraw: A Novel Data Augmentation Approach for Free Space   Detection Using Diffusion Model"></a>Erase, then Redraw: A Novel Data Augmentation Approach for Free Space   Detection Using Diffusion Model</h2><p><strong>Authors:Fulong Ma, Weiqing Qi, Guoyang Zhao, Ming Liu, Jun Ma</strong></p>
<p>Data augmentation is one of the most common tools in deep learning, underpinning many recent advances including tasks such as classification, detection, and semantic segmentation. The standard approach to data augmentation involves simple transformations like rotation and flipping to generate new images. However, these new images often lack diversity along the main semantic dimensions within the data. Traditional data augmentation methods cannot alter high-level semantic attributes such as the presence of vehicles, trees, and buildings in a scene to enhance data diversity. In recent years, the rapid development of generative models has injected new vitality into the field of data augmentation. In this paper, we address the lack of diversity in data augmentation for road detection task by using a pre-trained text-to-image diffusion model to parameterize image-to-image transformations. Our method involves editing images using these diffusion models to change their semantics. In essence, we achieve this goal by erasing instances of real objects from the original dataset and generating new instances with similar semantics in the erased regions using the diffusion model, thereby expanding the original dataset. We evaluate our approach on the KITTI road dataset and achieve the best results compared to other data augmentation methods, which demonstrates the effectiveness of our proposed development. </p>
<blockquote>
<p>数据增强是深度学习中最常见的工具之一，为包括分类、检测和语义分割等任务在内的许多最新进展提供了支持。传统的数据增强方法通常涉及简单的转换，如旋转和翻转以生成新图像。然而，这些新图像在数据的主要语义维度上通常缺乏多样性。传统数据增强方法无法改变高级语义属性，例如场景中的车辆、树木和建筑物的存在，以增强数据多样性。近年来，生成模型的快速发展为数据增强领域注入了新活力。在本文中，我们通过使用预训练的文本到图像扩散模型来参数化图像到图像的转换，解决了道路检测任务中数据增强缺乏多样性的问题。我们的方法涉及使用这些扩散模型编辑图像，以改变其语义。本质上，我们通过从原始数据集中删除真实对象的实例，并利用扩散模型在删除的区域生成具有相似语义的新实例，从而扩展原始数据集。我们在KITTI道路数据集上评估了我们的方法，与其他数据增强方法相比取得了最佳结果，这证明了我们所提出的发展的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.20164v2">PDF</a> </p>
<p><strong>Summary</strong><br>数据增强是深度学习中的常用工具，对于分类、检测和语义分割等任务起到了重要的推动作用。传统数据增强方法主要通过简单变换生成新图像，但缺乏多样性。本文利用预训练的文本到图像扩散模型进行图像到图像的参数化转换，解决道路检测任务中数据增强缺乏多样性的问题。通过擦除原始数据集中的实际对象并用扩散模型在擦除区域生成具有相似语义的新实例，从而扩展原始数据集。在KITTI道路数据集上的实验结果证明了该方法的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>数据增强在深度学习中的重要作用，特别是在分类、检测和语义分割任务中的应用。</li>
<li>传统数据增强方法主要通过简单变换生成新图像，但这种方法生成的图像在语义维度上缺乏多样性。</li>
<li>扩散模型在数据增强中的潜力，能够改变图像的高级语义属性，提高数据多样性。</li>
<li>本文使用预训练的文本到图像扩散模型进行图像到图像的参数化转换，解决道路检测任务中数据增强缺乏多样性的挑战。</li>
<li>方法通过擦除原始数据集中的对象并用扩散模型生成新实例来扩展数据集。</li>
<li>在KITTI道路数据集上的实验结果表明，该方法在数据增强方面取得了最佳效果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.20164">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a96ee1f947f5353caf23d4150cb4dfd5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b5baa2a152c0748e4899e40eee8ab70.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6dfe61987c09ae18a57f1a082e2c4d99.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45552786e93ed2029575050ff9dbb408.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4e7442fc084ec113344eafa7ef814cb4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d86cdc04787a9c0222a06562eaa300c.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Score-Forgetting-Distillation-A-Swift-Data-Free-Method-for-Machine-Unlearning-in-Diffusion-Models"><a href="#Score-Forgetting-Distillation-A-Swift-Data-Free-Method-for-Machine-Unlearning-in-Diffusion-Models" class="headerlink" title="Score Forgetting Distillation: A Swift, Data-Free Method for Machine   Unlearning in Diffusion Models"></a>Score Forgetting Distillation: A Swift, Data-Free Method for Machine   Unlearning in Diffusion Models</h2><p><strong>Authors:Tianqi Chen, Shujian Zhang, Mingyuan Zhou</strong></p>
<p>The machine learning community is increasingly recognizing the importance of fostering trust and safety in modern generative AI (GenAI) models. We posit machine unlearning (MU) as a crucial foundation for developing safe, secure, and trustworthy GenAI models. Traditional MU methods often rely on stringent assumptions and require access to real data. This paper introduces Score Forgetting Distillation (SFD), an innovative MU approach that promotes the forgetting of undesirable information in diffusion models by aligning the conditional scores of “unsafe” classes or concepts with those of “safe” ones. To eliminate the need for real data, our SFD framework incorporates a score-based MU loss into the score distillation objective of a pretrained diffusion model. This serves as a regularization term that preserves desired generation capabilities while enabling the production of synthetic data through a one-step generator. Our experiments on pretrained label-conditional and text-to-image diffusion models demonstrate that our method effectively accelerates the forgetting of target classes or concepts during generation, while preserving the quality of other classes or concepts. This unlearned and distilled diffusion not only pioneers a novel concept in MU but also accelerates the generation speed of diffusion models. Our experiments and studies on a range of diffusion models and datasets confirm that our approach is generalizable, effective, and advantageous for MU in diffusion models. Code is available at <a target="_blank" rel="noopener" href="https://github.com/tqch/score-forgetting-distillation">https://github.com/tqch/score-forgetting-distillation</a>. ($\textbf{Warning:}$ This paper contains sexually explicit imagery, discussions of pornography, racially-charged terminology, and other content that some readers may find disturbing, distressing, and&#x2F;or offensive.) </p>
<blockquote>
<p>机器学习领域越来越认识到培养现代生成式人工智能（GenAI）模型中的信任和安全性至关重要。我们认为机器遗忘（MU）是开发安全、可靠、可信的GenAI模型的重要基础。传统的MU方法往往依赖于严格的假设并需要访问真实数据。本文介绍了Score Forgetting Distillation（SFD）这一创新的MU方法，它通过使“不安全”类别或概念的条件分数与“安全”类别或概念的条件分数保持一致，促进在扩散模型中遗忘不需要的信息。为了消除对真实数据的需求，我们的SFD框架将基于分数的MU损失纳入预训练扩散模型的分数蒸馏目标中。这作为正则化项，既保留了所需的生成能力，又通过一步生成器产生了合成数据。我们在预训练的标签条件和文本到图像扩散模型上的实验表明，我们的方法有效地加速了目标类别或概念的遗忘过程，同时保持了其他类别或概念的质量。这种未被学习和提炼的扩散不仅开创了MU领域的新概念，还加快了扩散模型的生成速度。我们在一系列扩散模型和数据集上的实验和研究证实，我们的方法是通用的、有效的，对于扩散模型中的MU具有优势。代码可访问于 <a target="_blank" rel="noopener" href="https://github.com/tqch/score-forgetting-distillation">https://github.com/tqch/score-forgetting-distillation</a> 。（警告：本文包含色情内容、关于色情讨论、种族性术语和其他一些读者可能认为令人不安、令人痛苦和&#x2F;或冒犯的内容。）</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.11219v3">PDF</a> ICLR 2025</p>
<p><strong>摘要</strong><br>    本文提出一种名为Score Forgetting Distillation（SFD）的新型机器无学习（MU）方法，旨在促进扩散模型中不良信息的遗忘。该方法通过对“不安全”类别或概念的条件分数与“安全”类别或概念的分数对齐，无需真实数据即可实现遗忘。通过在预训练的扩散模型中融入基于分数的MU损失，SFD框架在保持生成能力的同时，通过一步生成器产生合成数据。实验证明，该方法在生成过程中有效加速目标类别或概念的遗忘，同时保持其他类别或概念的质量。此无学习和蒸馏的扩散模型不仅开创了MU的新概念，还加速了扩散模型的生成速度。实验和研究表明，该方法在多种扩散模型和数据集上具有通用性、有效性和优势。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>机器学习领域正日益认识到在现代生成式AI（GenAI）模型中培养信任和安全的重要性。</li>
<li>机器无学习（MU）被视为开发安全、可靠和可信的GenAI模型的关键基础。</li>
<li>传统MU方法通常依赖于严格的假设并需要真实数据的访问。</li>
<li>本文介绍Score Forgetting Distillation（SFD），一种新型的MU方法，它通过对齐“不安全”类别或概念的条件分数来促进扩散模型中不良信息的遗忘。</li>
<li>SFD框架融入基于分数的MU损失，无需真实数据，即可在预训练的扩散模型中实现遗忘，同时保持生成能力。</li>
<li>实验证明，SFD方法有效加速目标类别或概念的遗忘过程，同时保持其他类别或概念的质量。</li>
<li>此无学习和蒸馏的扩散模型不仅推动了MU领域的新概念，还提高了扩散模型的生成速度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.11219">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-1cee268ba1c31024cb3a1fe22dbd8e63.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e0649c4c6bd36a3314fcac62afbe509b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6a5961c7b0d4a52a3bb6366f95ebc9ab.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="SPDiffusion-Semantic-Protection-Diffusion-Models-for-Multi-concept-Text-to-image-Generation"><a href="#SPDiffusion-Semantic-Protection-Diffusion-Models-for-Multi-concept-Text-to-image-Generation" class="headerlink" title="SPDiffusion: Semantic Protection Diffusion Models for Multi-concept   Text-to-image Generation"></a>SPDiffusion: Semantic Protection Diffusion Models for Multi-concept   Text-to-image Generation</h2><p><strong>Authors:Yang Zhang, Rui Zhang, Xuecheng Nie, Haochen Li, Jikun Chen, Yifan Hao, Xin Zhang, Luoqi Liu, Ling Li</strong></p>
<p>Recent text-to-image models have achieved impressive results in generating high-quality images. However, when tasked with multi-concept generation creating images that contain multiple characters or objects, existing methods often suffer from semantic entanglement, including concept entanglement and improper attribute binding, leading to significant text-image inconsistency. We identify that semantic entanglement arises when certain regions of the latent features attend to incorrect concept and attribute tokens. In this work, we propose the Semantic Protection Diffusion Model (SPDiffusion) to address both concept entanglement and improper attribute binding using only a text prompt as input. The SPDiffusion framework introduces a novel concept region extraction method SP-Extraction to resolve region entanglement in cross-attention, along with SP-Attn, which protects concept regions from the influence of irrelevant attributes and concepts. To evaluate our method, we test it on existing benchmarks, where SPDiffusion achieves state-of-the-art results, demonstrating its effectiveness. </p>
<blockquote>
<p>近期的文本到图像模型在生成高质量图像方面取得了令人印象深刻的结果。然而，当面对多概念生成任务，即生成包含多个字符或对象的图像时，现有方法经常遭受语义纠缠的问题，包括概念纠缠和不当属性绑定，导致显著的文本-图像不一致。我们确定，当潜在特征中的某些区域关注错误的概念和属性令牌时，就会出现语义纠缠。在这项工作中，我们提出了语义保护扩散模型（SPDiffusion），仅使用文本提示作为输入来解决概念纠缠和不当属性绑定问题。SPDiffusion框架引入了一种新的概念区域提取方法SP-Extraction，以解决跨注意力中的区域纠缠问题，以及SP-Attn，它保护概念区域免受无关属性和概念的影响。为了评估我们的方法，我们在现有基准测试上对其进行了测试，SPDiffusion取得了最新结果，证明了其有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.01327v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为Semantic Protection Diffusion Model（SPDiffusion）的文本转图像模型，用于解决多概念生成时的语义纠缠和不适当属性绑定问题。该模型通过引入SP-Extraction和SP-Attn机制，有效保护概念区域免受无关属性和概念的影响，实现跨注意力区域纠缠的解决。在现有基准测试中，SPDiffusion取得了最先进的成果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文本转图像模型在多概念生成时面临语义纠缠问题。</li>
<li>语义纠缠源于特征区域的注意力与概念或属性标记的不匹配。</li>
<li>SPDiffusion模型旨在解决概念纠缠和不适当属性绑定问题。</li>
<li>SPDiffusion引入SP-Extraction方法，解决跨注意力区域纠缠。</li>
<li>SPDiffusion采用SP-Attn机制，保护概念区域免受无关属性和概念的影响。</li>
<li>在现有基准测试中，SPDiffusion模型表现卓越，达到最新水平。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.01327">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c5373909767e7547394f54de18a73f68.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b6aaf88cff0da30df22e1e3fd8544fe0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-029871c9b641a165d359cdace78ddc9f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c96a400d9a10713a9a9dcd3a749f643a.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Speed-accuracy-relations-for-the-diffusion-models-Wisdom-from-nonequilibrium-thermodynamics-and-optimal-transport"><a href="#Speed-accuracy-relations-for-the-diffusion-models-Wisdom-from-nonequilibrium-thermodynamics-and-optimal-transport" class="headerlink" title="Speed-accuracy relations for the diffusion models: Wisdom from   nonequilibrium thermodynamics and optimal transport"></a>Speed-accuracy relations for the diffusion models: Wisdom from   nonequilibrium thermodynamics and optimal transport</h2><p><strong>Authors:Kotaro Ikeda, Tomoya Uda, Daisuke Okanohara, Sosuke Ito</strong></p>
<p>We discuss a connection between a generative model, called the diffusion model, and nonequilibrium thermodynamics for the Fokker-Planck equation, called stochastic thermodynamics. Based on the techniques of stochastic thermodynamics, we derive the speed-accuracy relations for the diffusion models, which are inequalities that relate the accuracy of data generation to the entropy production rate, which can be interpreted as the speed of the diffusion dynamics in the absence of the non-conservative force. From a stochastic thermodynamic perspective, our results provide a quantitative insight into how best to generate data in diffusion models. The optimal learning protocol is introduced by the geodesic of space of the 2-Wasserstein distance in optimal transport theory. We numerically illustrate the validity of the speed-accuracy relations for the diffusion models with different noise schedules and the different data. We numerically discuss our results for the optimal and suboptimal learning protocols. We also show the inaccurate data generation due to the non-conservative force, and the applicability of our results to data generation from the real-world image datasets. </p>
<blockquote>
<p>我们探讨了一种生成模型——扩散模型与非平衡态热力学和Fokker-Planck方程的随机热力学之间的联系。基于随机热力学的技术，我们推导出了扩散模型的速度-精度关系，这些关系是不等式，描述了数据生成的精度与熵产生率之间的联系，可以解释为在没有非保守力的情况下扩散动力学的速度。从随机热力学的角度来看，我们的结果提供了在扩散模型中如何最佳生成数据的定量见解。最优学习协议由最优传输理论中的2-Wasserstein距离的空间测地线引入。我们通过数值说明了不同噪声安排和数据下扩散模型的速度-精度关系的有效性。我们数值讨论了最优和次优学习协议的结果。我们还展示了由于非保守力导致的数据生成不准确，以及我们的结果对现实世界图像数据集的数据生成的适用性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.04495v4">PDF</a> 36 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>本文探讨了扩散模型与基于Fokker-Planck方程的随机热力学之间的联系。通过随机热力学技术，我们推导出扩散模型的速度-精度关系不等式，该不等式将数据采集的准确性关联到熵产生率上，可以解读为在没有非保守力作用下的扩散动力速度。本文结果从随机热力学的角度为扩散模型中的数据采集提供了定量见解。此外，引入最优学习协议——最优传输理论中的2-Wasserstein距离空间测地线。我们对不同噪声计划和数据的扩散模型的速度-精度关系进行了数值验证，并讨论了最优和次优学习协议的结果。同时，展示了因非保守力导致的数据生成不准确，以及我们的结果对现实世界图像数据集的数据生成适用性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型与随机热力学存在联系，可用于推导速度-精度关系不等式。</li>
<li>速度-精度关系不等式关联了数据生成的准确性和熵产生率，反映了扩散动力学的速度。</li>
<li>从随机热力学视角，为扩散模型中的数据生成提供了定量见解。</li>
<li>引入最优学习协议，基于最优传输理论中的2-Wasserstein距离空间测地线。</li>
<li>通过数值验证，不同噪声计划和数据的扩散模型的速度-精度关系得到证实。</li>
<li>探讨了最优和次优学习协议的结果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.04495">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-54cc77c16b619217240d0ebe9103a771.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f5ad23264d965e956a5ddd48c372473.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-05/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-05/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-05/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-ce4cf8702947ea6ba2c02dc49aa37123.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-03-05  MobileViM A Light-weight and Dimension-independent Vision Mamba for 3D   Medical Image Analysis
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-05
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-05/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-c272fa6ee6403d416b9f8ff448e3a0d8.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2025-03-05  GS-CPR Efficient Camera Pose Refinement via 3D Gaussian Splatting
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-05
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">28051.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
