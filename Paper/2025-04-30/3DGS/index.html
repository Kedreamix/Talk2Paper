<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="3DGS">
    <meta name="description" content="3DGS 方向最新论文已更新，请持续关注 Update in 2025-04-30  Mesh-Learner Texturing Mesh with Spherical Harmonics">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>3DGS | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-c3cff7ca5b878ce125255dbb6e2ea578.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">3DGS</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/3DGS/">
                                <span class="chip bg-color">3DGS</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                3DGS
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-04-30
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-01
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    9.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    37 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-04-30-更新"><a href="#2025-04-30-更新" class="headerlink" title="2025-04-30 更新"></a>2025-04-30 更新</h1><h2 id="Mesh-Learner-Texturing-Mesh-with-Spherical-Harmonics"><a href="#Mesh-Learner-Texturing-Mesh-with-Spherical-Harmonics" class="headerlink" title="Mesh-Learner: Texturing Mesh with Spherical Harmonics"></a>Mesh-Learner: Texturing Mesh with Spherical Harmonics</h2><p><strong>Authors:Yunfei Wan, Jianheng Liu, Jiarong Lin, Fu Zhang</strong></p>
<p>In this paper, we present a 3D reconstruction and rendering framework termed Mesh-Learner that is natively compatible with traditional rasterization pipelines. It integrates mesh and spherical harmonic (SH) texture (i.e., texture filled with SH coefficients) into the learning process to learn each mesh s view-dependent radiance end-to-end. Images are rendered by interpolating surrounding SH Texels at each pixel s sampling point using a novel interpolation method. Conversely, gradients from each pixel are back-propagated to the related SH Texels in SH textures. Mesh-Learner exploits graphic features of rasterization pipeline (texture sampling, deferred rendering) to render, which makes Mesh-Learner naturally compatible with tools (e.g., Blender) and tasks (e.g., 3D reconstruction, scene rendering, reinforcement learning for robotics) that are based on rasterization pipelines. Our system can train vast, unlimited scenes because we transfer only the SH textures within the frustum to the GPU for training. At other times, the SH textures are stored in CPU RAM, which results in moderate GPU memory usage. The rendering results on interpolation and extrapolation sequences in the Replica and FAST-LIVO2 datasets achieve state-of-the-art performance compared to existing state-of-the-art methods (e.g., 3D Gaussian Splatting and M2-Mapping). To benefit the society, the code will be available at <a target="_blank" rel="noopener" href="https://github.com/hku-mars/Mesh-Learner">https://github.com/hku-mars/Mesh-Learner</a>. </p>
<blockquote>
<p>本文介绍了一个名为Mesh-Learner的3D重建和渲染框架，它与传统光栅化管道原生兼容。它通过整合网格和球面谐波（SH）纹理（即填充有SH系数的纹理）到学习过程中，以端到端的方式学习每个网格的视角相关辐射率。图像是通过一种新型插值方法，在每个像素的采样点插值周围的SH纹理元素来呈现的。相反，每个像素的梯度会反向传播到相关的SH纹理中的SH纹理元素。Mesh-Learner利用了光栅化管道（纹理采样、延迟渲染）的图形特征来进行渲染，这使得Mesh-Learner自然地与基于光栅化管道的工具（例如Blender）和任务（例如3D重建、场景渲染、机器人强化学习）兼容。我们的系统可以训练大量无限的场景，因为我们将仅将视锥内的SH纹理传输到GPU进行训练。其他时候，SH纹理存储在CPU RAM中，这导致GPU内存使用适中。在Replica和FAST-LIVO2数据集上的插值和外推序列的渲染结果达到了与现有最先进的方法（例如3D高斯溅射和M2-Mapping）相比的最佳性能。为了造福社会，代码将在<a target="_blank" rel="noopener" href="https://github.com/hku-mars/Mesh-Learner%E4%B8%8A%E6%8F%9B%E4%BA%8C%E3%80%82">https://github.com/hku-mars/Mesh-Learner上提供。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.19938v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种名为Mesh-Learner的3D重建和渲染框架，它与传统光栅化管道兼容。该框架结合了网格和球面谐波纹理，通过一种新型插值方法对周围SH Texels进行插值渲染图像，同时反向传播像素的梯度至相关的SH Texels。Mesh-Learner利用光栅化管道（纹理采样、延迟渲染）的图形特性进行渲染，使其自然地兼容基于光栅化管道的工具和任务。该系统的GPU内存使用适中，因为仅在视锥内传输SH纹理以进行训练，可训练大规模、无限的场景。在Replica和FAST-LIVO2数据集上的插值和外推序列的渲染结果达到了与现有先进方法相比的最佳性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Mesh-Learner框架结合了网格和球面谐波（SH）纹理，支持与传统光栅化管道集成。</li>
<li>通过新型插值方法渲染图像，插值周围SH Texels。</li>
<li>反向传播像素的梯度至SH纹理中的相关Texels。</li>
<li>利用光栅化管道的图形特性进行渲染，与基于该管道的工具和任务自然兼容。</li>
<li>GPU内存使用适中，可训练大规模、无限的场景。</li>
<li>在特定数据集上的渲染结果达到最佳性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.19938">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-20c7de97f2a77fa99bc88a5af1b3a67f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-337bc6d2fd66e3505eccbe6f39b24c64.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b2ab021890874788265b584a3fa44d4f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1c6c7ff641e2e607d996db2ab0a9b6f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d6d1021eae5c93406c4df34991ace5da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-03b50012df314884f14ce933b07fb8e9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-34b46b6850ffe46f3c9db3b96a63ce56.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="CE-NPBG-Connectivity-Enhanced-Neural-Point-Based-Graphics-for-Novel-View-Synthesis-in-Autonomous-Driving-Scenes"><a href="#CE-NPBG-Connectivity-Enhanced-Neural-Point-Based-Graphics-for-Novel-View-Synthesis-in-Autonomous-Driving-Scenes" class="headerlink" title="CE-NPBG: Connectivity Enhanced Neural Point-Based Graphics for Novel   View Synthesis in Autonomous Driving Scenes"></a>CE-NPBG: Connectivity Enhanced Neural Point-Based Graphics for Novel   View Synthesis in Autonomous Driving Scenes</h2><p><strong>Authors:Mohammad Altillawi, Fengyi Shen, Liudi Yang, Sai Manoj Prakhya, Ziyuan Liu</strong></p>
<p>Current point-based approaches encounter limitations in scalability and rendering quality when using large 3D point cloud maps because using them directly for novel view synthesis (NVS) leads to degraded visualizations. We identify the primary issue behind these low-quality renderings as a visibility mismatch between geometry and appearance, stemming from using these two modalities together. To address this problem, we present CE-NPBG, a new approach for novel view synthesis (NVS) in large-scale autonomous driving scenes. Our method is a neural point-based technique that leverages two modalities: posed images (cameras) and synchronized raw 3D point clouds (LiDAR). We first employ a connectivity relationship graph between appearance and geometry, which retrieves points from a large 3D point cloud map observed from the current camera perspective and uses them for rendering. By leveraging this connectivity, our method significantly improves rendering quality and enhances run-time and scalability by using only a small subset of points from the large 3D point cloud map. Our approach associates neural descriptors with the points and uses them to synthesize views. To enhance the encoding of these descriptors and elevate rendering quality, we propose a joint adversarial and point rasterization training. During training, we pair an image-synthesizer network with a multi-resolution discriminator. At inference, we decouple them and use the image-synthesizer to generate novel views. We also integrate our proposal into the recent 3D Gaussian Splatting work to highlight its benefits for improved rendering and scalability. </p>
<blockquote>
<p>当前基于点的方法在利用大规模3D点云地图进行新型视图合成（NVS）时，会遇到可扩展性和渲染质量方面的局限，直接使用它们会导致可视化效果下降。我们确定了这些低质量渲染背后的主要问题，即几何和外观之间的可见度不匹配，源于这两种模式的组合使用。为了解决这一问题，我们提出了CE-NPBG，这是一种用于大规模自动驾驶场景的新型视图合成（NVS）的方法。我们的方法是一种基于神经点的技术，利用两种模式：姿态图像（相机）和同步原始3D点云（激光雷达）。我们首先采用外观和几何之间的关联关系图，从当前相机视角观察的大规模3D点云地图中检索点，并将其用于渲染。通过利用这种关联性，我们的方法仅使用大规模3D点云地图中的一小部分点，就能显著提高渲染质量，并增强运行时间和可扩展性。我们的方法将神经描述符与点相关联，并使用它们来合成视图。为了提高这些描述符的编码和提升渲染质量，我们提出了一种联合对抗性和点栅格化训练。在训练过程中，我们将图像合成器网络与多分辨率鉴别器配对。在推理过程中，我们将它们解耦，并使用图像合成器生成新型视图。我们还将我们的提案整合到最近的3D高斯拼贴工作中，以突出其在改进渲染和可扩展性方面的优势。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.19557v1">PDF</a> Accepted in 2025 IEEE&#x2F;CVF Conference on Computer Vision and Pattern   Recognition Workshops (CVPRW)</p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为CE-NPBG的新方法，用于解决大规模自动驾驶场景中基于点的新视角合成（NVS）的渲染质量和效率问题。该方法结合了图像（相机）和同步原始三维点云（激光雷达）两种模式，通过建立几何与外观之间的连接关系图，从大规模三维点云地图中检索当前相机视角的点进行渲染。通过利用这种连接关系，该方法仅使用大规模三维点云地图的小部分点，显著提高了渲染质量和运行效率。该研究还引入了神经描述符与点的关联，用于合成视图，并提出联合对抗和点栅格化训练，以提高描述符的编码和渲染质量。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前点基方法在大规模三维点云地图的直接应用中存在渲染质量和效率的问题。</li>
<li>问题核心在于几何和外观之间的可见性不匹配。</li>
<li>CE-NPBG是一种新视角合成（NVS）方法，结合了图像（相机）和同步原始三维点云（激光雷达）两种模式。</li>
<li>通过建立连接关系图，从大规模三维点云地图中检索当前相机视角的点进行渲染，提高了渲染质量和效率。</li>
<li>方法结合了神经描述符与点的关联，用于合成视图。</li>
<li>研究引入了联合对抗和点栅格化训练，以提高描述符的编码和渲染质量。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.19557">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c6f439d4ca5bcc5b658fdca6a914a234.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-39e852456f825c942db9f6ddbf34c1aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-971a53ec35c2d22b042547e75dec62be.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b62e24101e8e525e3824674a27a3118f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d40bc642b3394d2c496043b5f59ddb30.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ca3cfd45d0d6fb4aae1f2f2aa93bc198.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="GSFF-SLAM-3D-Semantic-Gaussian-Splatting-SLAM-via-Feature-Field"><a href="#GSFF-SLAM-3D-Semantic-Gaussian-Splatting-SLAM-via-Feature-Field" class="headerlink" title="GSFF-SLAM: 3D Semantic Gaussian Splatting SLAM via Feature Field"></a>GSFF-SLAM: 3D Semantic Gaussian Splatting SLAM via Feature Field</h2><p><strong>Authors:Zuxing Lu, Xin Yuan, Shaowen Yang, Jingyu Liu, Jiawei Wang, Changyin Sun</strong></p>
<p>Semantic-aware 3D scene reconstruction is essential for autonomous robots to perform complex interactions. Semantic SLAM, an online approach, integrates pose tracking, geometric reconstruction, and semantic mapping into a unified framework, shows significant potential. However, existing systems, which rely on 2D ground truth priors for supervision, are often limited by the sparsity and noise of these signals in real-world environments. To address this challenge, we propose GSFF-SLAM, a novel dense semantic SLAM system based on 3D Gaussian Splatting that leverages feature fields to achieve joint rendering of appearance, geometry, and N-dimensional semantic features. By independently optimizing feature gradients, our method supports semantic reconstruction using various forms of 2D priors, particularly sparse and noisy signals. Experimental results demonstrate that our approach outperforms previous methods in both tracking accuracy and photorealistic rendering quality. When utilizing 2D ground truth priors, GSFF-SLAM achieves state-of-the-art semantic segmentation performance with 95.03% mIoU, while achieving up to 2.9$\times$ speedup with only marginal performance degradation. </p>
<blockquote>
<p>语义感知的3D场景重建对于自主机器人执行复杂交互至关重要。语义SLAM作为一种在线方法，将姿态跟踪、几何重建和语义映射整合到一个统一框架中，显示出巨大的潜力。然而，现有系统通常依赖于2D真实先验进行监督，这在真实世界环境中受到这些信号稀疏性和噪声的限制。为了应对这一挑战，我们提出了GSFF-SLAM，这是一种新型的基于3D高斯填充技术的密集语义SLAM系统。它利用特征场实现外观、几何和N维语义特征的联合渲染。通过独立优化特征梯度，我们的方法支持使用各种形式的2D先验进行语义重建，尤其是稀疏和噪声信号。实验结果表明，我们的方法在跟踪精度和照片级渲染质量方面都优于以前的方法。当利用2D真实先验时，GSFF-SLAM达到了最先进的语义分割性能，mIoU为95.03%，同时实现了高达2.9倍的加速，性能仅略有下降。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.19409v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探讨了自主机器人在进行复杂交互时，语义感知的3D场景重建的重要性。提出了一种新型的密集语义SLAM系统GSFF-SLAM，该系统基于3D高斯斑点技术，利用特征场实现外观、几何和N维语义特征的联合渲染。该方法可独立优化特征梯度，支持使用各种形式的2D先验进行语义重建，特别是在稀疏和噪声信号环境下表现优异。实验结果显示，该方法在跟踪精度和照片级渲染质量上均优于先前方法，利用2D地面真实先验时，实现95.03%的mIoU语义分割性能，同时实现2.9倍的速度提升，且性能损失较小。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语义感知的3D场景重建对自主机器人进行复杂交互至关重要。</li>
<li>现有的语义SLAM系统通常依赖于2D地面真实先验进行监督，这在真实世界环境中受到信号稀疏和噪声的限制。</li>
<li>GSFF-SLAM是一种新型的密集语义SLAM系统，基于3D高斯斑点技术。</li>
<li>GSFF-SLAM利用特征场实现外观、几何和N维语义特征的联合渲染。</li>
<li>该方法可独立优化特征梯度，支持使用各种形式的2D先验进行语义重建。</li>
<li>实验结果显示GSFF-SLAM在跟踪精度和渲染质量上优于其他方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.19409">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-d1d98acfc8378c57177b2d1be6b292a0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4758e81dfbe2a403fedfa271b2c04cc1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c3cff7ca5b878ce125255dbb6e2ea578.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Rendering-Anywhere-You-See-Renderability-Field-guided-Gaussian-Splatting"><a href="#Rendering-Anywhere-You-See-Renderability-Field-guided-Gaussian-Splatting" class="headerlink" title="Rendering Anywhere You See: Renderability Field-guided Gaussian   Splatting"></a>Rendering Anywhere You See: Renderability Field-guided Gaussian   Splatting</h2><p><strong>Authors:Xiaofeng Jin, Yan Fang, Matteo Frosi, Jianfei Ge, Jiangjian Xiao, Matteo Matteucci</strong></p>
<p>Scene view synthesis, which generates novel views from limited perspectives, is increasingly vital for applications like virtual reality, augmented reality, and robotics. Unlike object-based tasks, such as generating 360{\deg} views of a car, scene view synthesis handles entire environments where non-uniform observations pose unique challenges for stable rendering quality. To address this issue, we propose a novel approach: renderability field-guided gaussian splatting (RF-GS). This method quantifies input inhomogeneity through a renderability field, guiding pseudo-view sampling to enhanced visual consistency. To ensure the quality of wide-baseline pseudo-views, we train an image restoration model to map point projections to visible-light styles. Additionally, our validated hybrid data optimization strategy effectively fuses information of pseudo-view angles and source view textures. Comparative experiments on simulated and real-world data show that our method outperforms existing approaches in rendering stability. </p>
<blockquote>
<p>场景视图合成（Scene view synthesis）是从有限视角生成新颖视角的技术，在虚拟现实、增强现实和机器人等领域的应用越来越重要。与基于物体的任务（如生成汽车360度视图）不同，场景视图合成处理的是整个环境，其中非均匀观察给稳定渲染质量带来了独特挑战。为了解决这个问题，我们提出了一种新方法：可渲染性场引导的高斯涂斑法（RF-GS）。该方法通过可渲染性场量化输入的不均匀性，引导伪视图采样以提高视觉一致性。为了确保宽基线伪视图的质量，我们训练了一个图像恢复模型，将点投影映射到可见光风格。此外，我们经过验证的混合数据优化策略有效地融合了伪视角和源视图纹理的信息。在模拟和真实世界数据上的对比实验表明，我们的方法在渲染稳定性方面优于现有方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.19261v1">PDF</a> 8 pages,8 figures</p>
<p><strong>Summary</strong><br>    本文提出了基于渲染场引导的高斯贴图方法（RF-GS），用于解决场景视角合成中遇到的环境非均匀性问题，提升了视觉效果的一致性。采用混合数据优化策略，结合伪视角角度和源视角纹理信息，提高渲染稳定性。实验证明该方法在模拟和真实数据上均优于现有方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文章提出了场景视角合成中的非均匀性问题，即处理整个环境时面临的挑战。</li>
<li>提出了一种新的方法：基于渲染场引导的高斯贴图（RF-GS），以改善视觉效果的一致性。</li>
<li>通过量化输入的不均匀性，生成一个渲染场来指导伪视角采样。</li>
<li>采用图像恢复模型，将点投影映射到可见光风格，确保宽基线伪视角的质量。</li>
<li>采用验证过的混合数据优化策略，融合伪视角角度和源视角纹理信息。</li>
<li>实验结果表明，该方法在模拟和真实数据上的渲染稳定性优于现有方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.19261">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-c27e99af7b0b6faaf06338dc9bca3baa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df3c2d343f462cbc51c3607b05c8381e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f2a064fb87f648ada1e8eaab64f679e9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a36ad87fe2c4b14ce4e0124d55ab39dc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-92fdc2a95f2397e28752e8964c8aab12.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e2289e2d3fc590ec6604993e436f6f1c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-67256dba78ca7e2520dd11cf8fb4e380.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="IM-Portrait-Learning-3D-aware-Video-Diffusion-for-PhotorealisticTalking-Heads-from-Monocular-Videos"><a href="#IM-Portrait-Learning-3D-aware-Video-Diffusion-for-PhotorealisticTalking-Heads-from-Monocular-Videos" class="headerlink" title="IM-Portrait: Learning 3D-aware Video Diffusion for PhotorealisticTalking   Heads from Monocular Videos"></a>IM-Portrait: Learning 3D-aware Video Diffusion for PhotorealisticTalking   Heads from Monocular Videos</h2><p><strong>Authors:Yuan Li, Ziqian Bai, Feitong Tan, Zhaopeng Cui, Sean Fanello, Yinda Zhang</strong></p>
<p>We propose a novel 3D-aware diffusion-based method for generating photorealistic talking head videos directly from a single identity image and explicit control signals (e.g., expressions). Our method generates Multiplane Images (MPIs) that ensure geometric consistency, making them ideal for immersive viewing experiences like binocular videos for VR headsets. Unlike existing methods that often require a separate stage or joint optimization to reconstruct a 3D representation (such as NeRF or 3D Gaussians), our approach directly generates the final output through a single denoising process, eliminating the need for post-processing steps to render novel views efficiently. To effectively learn from monocular videos, we introduce a training mechanism that reconstructs the output MPI randomly in either the target or the reference camera space. This approach enables the model to simultaneously learn sharp image details and underlying 3D information. Extensive experiments demonstrate the effectiveness of our method, which achieves competitive avatar quality and novel-view rendering capabilities, even without explicit 3D reconstruction or high-quality multi-view training data. </p>
<blockquote>
<p>我们提出了一种新型的基于三维感知扩散的方法，直接从单张身份图像和明确的控制信号（如表情）生成逼真的说话人头视频。我们的方法生成多平面图像（MPIs），确保几何一致性，使其成为理想的选择，可用于虚拟现实头盔显示器的双目视频等沉浸式观看体验。与通常需要单独阶段或联合优化来重建三维表示（如NeRF或三维高斯）的现有方法不同，我们的方法通过单个去噪过程直接生成最终输出，无需进行后期处理步骤即可有效地渲染新颖视图。为了有效地从单目视频中学习，我们引入了一种训练机制，该机制可以在目标相机空间或参考相机空间中随机重建输出MPI。这种方法使模型能够同时学习尖锐的图像细节和潜在的三维信息。大量实验证明了我们方法的有效性，即使在缺乏明确的三维重建或高质量多视角训练数据的情况下，我们的方法也能达到竞争性的化身质量和新颖的视图渲染能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.19165v1">PDF</a> CVPR2025; project page:   <a target="_blank" rel="noopener" href="https://y-u-a-n-l-i.github.io/projects/IM-Portrait/">https://y-u-a-n-l-i.github.io/projects/IM-Portrait/</a></p>
<p><strong>Summary</strong><br>新一代三维感知扩散方法可直接从单一身份图像和控制信号（如表情）生成逼真的动画头像视频。该方法生成多平面图像（MPIs），确保几何一致性，适用于虚拟现实头戴设备的沉浸式观看体验。与其他方法不同，无需额外的阶段或联合优化来重建三维表示，通过单一的降噪过程直接生成最终输出，有效避免复杂的后期处理步骤以实现新视角的高效渲染。引入了一种训练机制，能在目标相机空间或参考相机空间中重建输出MPI，使模型能够同时学习清晰图像细节和底层三维信息。实验证明该方法的有效性，即使在无需明确的三维重建或高质量多视角训练数据的情况下，也能实现具有竞争力的角色质量和新视角的渲染能力。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>提出了一种新型的三维感知扩散方法，能够从单一身份图像和控制信号生成动画头像视频。</li>
<li>生成的多平面图像（MPIs）保证了几何一致性，适合虚拟现实沉浸式体验。</li>
<li>与其他方法不同，该方法通过单一的降噪过程直接生成最终输出，无需额外的阶段或联合优化，简化了流程。</li>
<li>引入了一种训练机制，能在目标相机空间或参考相机空间中重建MPI，促进模型同时学习图像细节和底层三维信息。</li>
<li>方法具有高效的渲染能力和竞争力强的角色质量，即使在没有明确的三维重建或高质量多视角训练数据的情况下也能表现出色。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.19165">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d2ff881d1ab0d0b4aff210c5b6de48d7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9d1d8a2d3376eb8bcec01d470d25c2b5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6a3605fcb2c83be4794d477ce4c0b058.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-37b87189576c03cf50993939fe028f40.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="TransparentGS-Fast-Inverse-Rendering-of-Transparent-Objects-with-Gaussians"><a href="#TransparentGS-Fast-Inverse-Rendering-of-Transparent-Objects-with-Gaussians" class="headerlink" title="TransparentGS: Fast Inverse Rendering of Transparent Objects with   Gaussians"></a>TransparentGS: Fast Inverse Rendering of Transparent Objects with   Gaussians</h2><p><strong>Authors:Letian Huang, Dongwei Ye, Jialin Dan, Chengzhi Tao, Huiwen Liu, Kun Zhou, Bo Ren, Yuanqi Li, Yanwen Guo, Jie Guo</strong></p>
<p>The emergence of neural and Gaussian-based radiance field methods has led to considerable advancements in novel view synthesis and 3D object reconstruction. Nonetheless, specular reflection and refraction continue to pose significant challenges due to the instability and incorrect overfitting of radiance fields to high-frequency light variations. Currently, even 3D Gaussian Splatting (3D-GS), as a powerful and efficient tool, falls short in recovering transparent objects with nearby contents due to the existence of apparent secondary ray effects. To address this issue, we propose TransparentGS, a fast inverse rendering pipeline for transparent objects based on 3D-GS. The main contributions are three-fold. Firstly, an efficient representation of transparent objects, transparent Gaussian primitives, is designed to enable specular refraction through a deferred refraction strategy. Secondly, we leverage Gaussian light field probes (GaussProbe) to encode both ambient light and nearby contents in a unified framework. Thirdly, a depth-based iterative probes query (IterQuery) algorithm is proposed to reduce the parallax errors in our probe-based framework. Experiments demonstrate the speed and accuracy of our approach in recovering transparent objects from complex environments, as well as several applications in computer graphics and vision. </p>
<blockquote>
<p>神经和高斯基辐射场方法的出现为新型视图合成和3D对象重建带来了重大进展。然而，由于辐射场对高频光变化的不稳定性和不正确的过度拟合，镜面反射和折射仍然构成了重大挑战。目前，即使3D高斯Splatting（3D-GS）作为一种强大而有效的工具，由于明显的二次射线效应的存在，在恢复附近内容的透明物体时也相形见绌。为了解决这一问题，我们提出了基于3D-GS的透明对象快速逆向渲染管道TransparentGS。主要贡献有三点。首先，设计了一种透明对象的高效表示方法，即透明高斯基元，以实现通过延迟折射策略的镜面折射。其次，我们利用高斯光场探针（GaussProbe）将环境光和附近内容编码到统一框架中。第三，提出了一种基于深度的迭代探针查询（IterQuery）算法，以减少我们基于探针的框架中的视差误差。实验证明了我们方法在复杂环境中恢复透明对象的速度和准确性，以及在计算机图形和视觉中的一些应用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.18768v1">PDF</a> accepted by SIGGRAPH 2025;   <a target="_blank" rel="noopener" href="https://letianhuang.github.io/transparentgs/">https://letianhuang.github.io/transparentgs/</a></p>
<p><strong>Summary</strong></p>
<p>神经网络和基于高斯的光场方法的发展极大地推动了新型视图合成和三维物体重建的进步。然而，由于高频光照变化下辐射场的稳定性及过拟合问题，光反射和折射仍存在巨大挑战。尽管强大的工具如三维高斯渲染技术（3D-GS）在复原透明物体时表现优秀，但由于存在明显的二次射线效应，难以应对物体周围的场景内容。为解决此问题，我们提出了基于透明高斯原语（TransparentGS）的快速逆向渲染管线。主要贡献有三点：设计了一种透明的物体表示方法——透明高斯原语，通过延迟折射策略实现镜面折射；利用高斯光场探针（GaussProbe）将环境光和周围场景内容编码进统一框架；提出了基于深度的迭代探针查询算法（IterQuery），减少探针基础上的框架的视差误差。实验证明，我们的方法能快速准确地从复杂环境中恢复透明物体，在计算机图形和视觉中有广泛应用。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>神经网络和高斯光场方法推动了视图合成和三维重建的进步。</li>
<li>光反射和折射存在挑战，主要由于辐射场对高频光照变化的稳定性和过拟合问题。</li>
<li>三维高斯渲染技术（3D-GS）在处理透明物体时面临二次射线效应的挑战。</li>
<li>TransparentGS方法通过透明高斯原语设计、GaussProbe和IterQuery算法解决了这些问题。</li>
<li>TransparentGS能快速准确地从复杂环境中恢复透明物体，具有广泛的应用前景。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.18768">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-edc2c87c72e640750978e6623d083b1a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-702c739dbbca3238e7bfcc8a10ad18b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-241c43ee5b7b3975ffdf3b4ef952353a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fe08844b608482787b82c50f4b759b16.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9fd880c89557d8b275f93901f2fa6f86.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="RGS-DR-Reflective-Gaussian-Surfels-with-Deferred-Rendering-for-Shiny-Objects"><a href="#RGS-DR-Reflective-Gaussian-Surfels-with-Deferred-Rendering-for-Shiny-Objects" class="headerlink" title="RGS-DR: Reflective Gaussian Surfels with Deferred Rendering for Shiny   Objects"></a>RGS-DR: Reflective Gaussian Surfels with Deferred Rendering for Shiny   Objects</h2><p><strong>Authors:Georgios Kouros, Minye Wu, Tinne Tuytelaars</strong></p>
<p>We introduce RGS-DR, a novel inverse rendering method for reconstructing and rendering glossy and reflective objects with support for flexible relighting and scene editing. Unlike existing methods (e.g., NeRF and 3D Gaussian Splatting), which struggle with view-dependent effects, RGS-DR utilizes a 2D Gaussian surfel representation to accurately estimate geometry and surface normals, an essential property for high-quality inverse rendering. Our approach explicitly models geometric and material properties through learnable primitives rasterized into a deferred shading pipeline, effectively reducing rendering artifacts and preserving sharp reflections. By employing a multi-level cube mipmap, RGS-DR accurately approximates environment lighting integrals, facilitating high-quality reconstruction and relighting. A residual pass with spherical-mipmap-based directional encoding further refines the appearance modeling. Experiments demonstrate that RGS-DR achieves high-quality reconstruction and rendering quality for shiny objects, often outperforming reconstruction-exclusive state-of-the-art methods incapable of relighting. </p>
<blockquote>
<p>我们介绍了RGS-DR，这是一种新型逆向渲染方法，用于重建和渲染具有光泽和反射特性的物体，支持灵活的重新打光和场景编辑。与现有方法（例如NeRF和3D高斯拼贴）相比，它们在处理与视图相关的效果时遇到困难，而RGS-DR使用2D高斯surfel表示法来准确估计几何和表面法线，这是高质量逆向渲染的基本属性。我们的方法通过可学习的原始元素显式建模几何和材料属性，并将其渲染到延迟着色管道中，这有效地减少了渲染伪影并保持了锐利的反射。通过采用多层次立方体mipmap，RGS-DR能够准确近似环境光照积分，从而实现高质量的重建和重新打光。基于球形mipmap的方向编码的残差传递进一步改进了外观建模。实验表明，RGS-DR在重建和渲染光泽物体方面达到了高质量，通常优于那些无法进行重新打光的仅用于重建的最先进方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.18468v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>RGS-DR是一种新型逆向渲染方法，能重建和渲染具有光泽和反射特性的物体，支持灵活的重新照明和场景编辑。该方法采用2D高斯surfel表示法准确估计几何和表面法线，通过可学习的原始元素显式建模几何和材质属性，并融入延迟着色管道，有效减少渲染伪影，保留锐利反射。通过采用多层立方体mipmap，RGS-DR准确近似环境光照积分，实现高质量重建和重新照明。使用基于球形mipmap的方向编码的残差传递进一步改进了外观建模。实验表明，RGS-DR在重建和渲染光泽物体方面达到高质量，往往超越只能重建而不能重新照明的最新技术方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RGS-DR是一种用于重建和渲染具有光泽和反射特性的物体的新型逆向渲染方法。</li>
<li>该方法采用2D高斯surfel表示法准确估计几何和表面法线，这是高质量逆向渲染的关键属性。</li>
<li>RGS-DR通过可学习的原始元素显式建模几何和材质属性，并融入延迟着色管道，以提高渲染质量。</li>
<li>采用多层立方体mipmap，RGS-DR能准确近似环境光照积分，实现高质量重建和重新照明。</li>
<li>残差传递和基于球形mipmap的方向编码进一步改进了RGS-DR的外观建模。</li>
<li>RGS-DR在重建和渲染光泽物体方面表现出色，往往超越现有技术。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.18468">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-3ee68b5ed0b31156461008b3d74e2904.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5cbe5993b0e82f86f21ec2593ea7f2ee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21a84f71db644f83249586af03728709.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="3D-Gaussian-Inpainting-with-Depth-Guided-Cross-View-Consistency"><a href="#3D-Gaussian-Inpainting-with-Depth-Guided-Cross-View-Consistency" class="headerlink" title="3D Gaussian Inpainting with Depth-Guided Cross-View Consistency"></a>3D Gaussian Inpainting with Depth-Guided Cross-View Consistency</h2><p><strong>Authors:Sheng-Yu Huang, Zi-Ting Chou, Yu-Chiang Frank Wang</strong></p>
<p>When performing 3D inpainting using novel-view rendering methods like Neural Radiance Field (NeRF) or 3D Gaussian Splatting (3DGS), how to achieve texture and geometry consistency across camera views has been a challenge. In this paper, we propose a framework of 3D Gaussian Inpainting with Depth-Guided Cross-View Consistency (3DGIC) for cross-view consistent 3D inpainting. Guided by the rendered depth information from each training view, our 3DGIC exploits background pixels visible across different views for updating the inpainting mask, allowing us to refine the 3DGS for inpainting purposes.Through extensive experiments on benchmark datasets, we confirm that our 3DGIC outperforms current state-of-the-art 3D inpainting methods quantitatively and qualitatively. </p>
<blockquote>
<p>在使用如神经辐射场（NeRF）或3D高斯拼贴（3DGS）等新型视图渲染方法进行3D补全时，如何在不同相机视角间实现纹理和几何一致性一直是一个挑战。在本文中，我们提出了一个名为“带有深度引导跨视图一致性（3DGIC）的3D高斯补全”的框架，用于实现跨视图一致的3D补全。我们的方法通过利用来自每个训练视图的渲染深度信息作为指导，利用在不同视图中可见的背景像素来更新补全掩膜，从而允许我们改进用于补全的3DGS。通过对基准数据集的大量实验，我们证实我们的3DGIC在数量和质量上都优于当前最先进的3D补全方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.11801v2">PDF</a> Accepted to CVPR 2025. For project page, see   <a target="_blank" rel="noopener" href="https://peterjohnsonhuang.github.io/3dgic-pages">https://peterjohnsonhuang.github.io/3dgic-pages</a></p>
<p><strong>Summary</strong></p>
<p>本文提出了一种基于深度引导的跨视图一致性3D高斯补全（3DGIC）框架，用于实现跨视图一致的3D补全。该框架利用从每个训练视图渲染的深度信息来指导背景像素的可见性，从而更新补全掩膜，实现对3DGS的细化，以实现补全目的。实验证明，该框架在基准数据集上的表现优于现有的3D补全方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>3DGIC框架实现了跨视图一致的3D补全。</li>
<li>该框架利用渲染的深度信息来指导背景像素的可见性。</li>
<li>3DGIC通过更新补全掩膜来细化3DGS，以实现更精确的补全。</li>
<li>3DGIC框架在基准数据集上的表现优于现有方法。</li>
<li>该方法在处理3D纹理和几何一致性方面遇到的挑战时表现出优势。</li>
<li>提出的框架适用于使用NeRF或3DGS等新型视图渲染方法的3D补全任务。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.11801">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-0ab49b78e681ebc866a1c8a79cb48f55.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-093bef097eb0b1f5f8e7dcd7467abdc8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-144e7b763e33bb9fcf3959c55712ba83.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-81a788d424a93cc30827a27bbeb0e989.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="CrossView-GS-Cross-view-Gaussian-Splatting-For-Large-scale-Scene-Reconstruction"><a href="#CrossView-GS-Cross-view-Gaussian-Splatting-For-Large-scale-Scene-Reconstruction" class="headerlink" title="CrossView-GS: Cross-view Gaussian Splatting For Large-scale Scene   Reconstruction"></a>CrossView-GS: Cross-view Gaussian Splatting For Large-scale Scene   Reconstruction</h2><p><strong>Authors:Chenhao Zhang, Yuanping Cao, Lei Zhang</strong></p>
<p>3D Gaussian Splatting (3DGS) leverages densely distributed Gaussian primitives for high-quality scene representation and reconstruction. While existing 3DGS methods perform well in scenes with minor view variation, large view changes from cross-view data pose optimization challenges for these methods. To address these issues, we propose a novel cross-view Gaussian Splatting method for large-scale scene reconstruction based on multi-branch construction and fusion. Our method independently reconstructs models from different sets of views as multiple independent branches to establish the baselines of Gaussian distribution, providing reliable priors for cross-view reconstruction during initialization and densification. Specifically, a gradient-aware regularization strategy is introduced to mitigate smoothing issues caused by significant view disparities. Additionally, a unique Gaussian supplementation strategy is utilized to incorporate complementary information of multi-branch into the cross-view model. Extensive experiments on benchmark datasets demonstrate that our method achieves superior performance in novel view synthesis compared to state-of-the-art methods. </p>
<blockquote>
<p>3D高斯摊铺（3DGS）利用密集分布的Gaussian基本单位来表示和重建高质量的场景。现有的三维GS方法在轻微视角变化的场景中表现良好，但从跨视角数据中获取的大视角变化为这些方法带来了优化挑战。为了解决这些问题，我们提出了一种基于多分支构建和融合的大场景重建的跨视图高斯摊铺新方法。我们的方法从不同视角的独立数据集重建模型，建立多个独立分支来构建高斯分布的基线，为初始化和密集化过程中的跨视图重建提供可靠的先验知识。具体来说，引入了一种梯度感知的正则化策略，以缓解由显著视角差异引起的平滑问题。此外，还采用了一种独特的Gaussian补充策略，将多分支的互补信息融入跨视图模型中。在基准数据集上的广泛实验表明，与最先进的方法相比，我们的方法在新型视图合成方面取得了卓越的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01695v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了基于密集分布的高斯基元的三维高斯融合（3DGS）技术用于高质量的场景表示和重建。针对现有方法在跨视角数据存在较大视角变化时的优化挑战，提出了一种基于多分支构建和融合的新型跨视角高斯融合方法，用于大规模场景重建。该方法通过从不同视角独立重建模型作为多个独立分支，建立高斯分布的基准，为跨视角重建提供可靠的先验信息。引入梯度感知正则化策略以缓解显著视角差异导致的平滑问题，并采用独特的高斯补充策略将多分支的互补信息融入跨视角模型中。在基准数据集上的广泛实验表明，该方法在新型视角合成方面相较于现有先进技术表现出卓越性能。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>3DGS技术利用密集分布的高斯基元进行高质量的场景表示和重建。</li>
<li>针对现有方法的不足，提出了一种基于多分支构建和融合的新型跨视角高斯融合方法。</li>
<li>独立分支以高斯分布基准形式建立，为跨视角重建提供可靠先验信息。</li>
<li>采用梯度感知正则化策略缓解显著视角差异造成的平滑问题。</li>
<li>高斯补充策略用于整合多分支的互补信息到跨视角模型中。</li>
<li>方法在基准数据集上的实验表现优越，特别是在新型视角合成方面。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01695">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-43072bdcbb371443c3e110183e2f7509.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-20bbb3ee2a838ebf3ffe4a1e5e9d31c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-833e8596a1fa74894350afde5e892e0a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-81c1a76b223db25dbe55c7e47a318a32.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b91a86f8866a859c9c048844ea36586c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7ab3a78f7c0f17e5176f5735623f8d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9fc1d39b1d54da65971608b0ca427b8b.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Wonderland-Navigating-3D-Scenes-from-a-Single-Image"><a href="#Wonderland-Navigating-3D-Scenes-from-a-Single-Image" class="headerlink" title="Wonderland: Navigating 3D Scenes from a Single Image"></a>Wonderland: Navigating 3D Scenes from a Single Image</h2><p><strong>Authors:Hanwen Liang, Junli Cao, Vidit Goel, Guocheng Qian, Sergei Korolev, Demetri Terzopoulos, Konstantinos N. Plataniotis, Sergey Tulyakov, Jian Ren</strong></p>
<p>How can one efficiently generate high-quality, wide-scope 3D scenes from arbitrary single images? Existing methods suffer several drawbacks, such as requiring multi-view data, time-consuming per-scene optimization, distorted geometry in occluded areas, and low visual quality in backgrounds. Our novel 3D scene reconstruction pipeline overcomes these limitations to tackle the aforesaid challenge. Specifically, we introduce a large-scale reconstruction model that leverages latents from a video diffusion model to predict 3D Gaussian Splattings of scenes in a feed-forward manner. The video diffusion model is designed to create videos precisely following specified camera trajectories, allowing it to generate compressed video latents that encode multi-view information while maintaining 3D consistency. We train the 3D reconstruction model to operate on the video latent space with a progressive learning strategy, enabling the efficient generation of high-quality, wide-scope, and generic 3D scenes. Extensive evaluations across various datasets affirm that our model significantly outperforms existing single-view 3D scene generation methods, especially with out-of-domain images. Thus, we demonstrate for the first time that a 3D reconstruction model can effectively be built upon the latent space of a diffusion model in order to realize efficient 3D scene generation. </p>
<blockquote>
<p>如何从任意单张图像高效生成高质量、大范围的三维场景？现有方法存在诸多缺点，例如需要多视角数据、耗时的场景优化、遮挡区域的几何失真以及背景视觉质量低下。我们的新型三维场景重建流程克服了这些限制，以应对上述挑战。具体来说，我们引入了一种大规模重建模型，该模型以视频扩散模型的潜在特征为基础，以预测场景的三维高斯Splattings。视频扩散模型被设计为遵循指定的相机轨迹创建视频，从而生成压缩的视频潜在特征，这些特征编码了多视角信息，同时保持了三维一致性。我们采用渐进学习策略对三维重建模型进行训练，使其能够在视频潜在空间上运行，从而能够高效生成高质量、大范围、通用的三维场景。在多个数据集上的广泛评估证实，我们的模型在单视图三维场景生成方面显著优于现有方法，尤其是在域外图像上。因此，我们首次证明，可以在扩散模型的潜在空间上建立三维重建模型，以实现高效的三维场景生成。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12091v2">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://snap-research.github.io/wonderland/">https://snap-research.github.io/wonderland/</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种基于视频扩散模型的3D场景重建方法，解决了现有方法的局限性，能够高效生成高质量、宽视野的3D场景。通过引入大规模重建模型，利用视频扩散模型的潜在信息预测场景的3D高斯Splattings。该模型可生成高质量、宽视野和通用的3D场景，并在各种数据集上的评估表现优异。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>引入了一种新的基于视频扩散模型的3D场景重建方法。</li>
<li>该方法通过预测场景的3D高斯Splattings来生成高质量的3D场景。</li>
<li>视频扩散模型的设计可生成遵循指定相机轨迹的视频潜在信息，同时保持3D一致性。</li>
<li>训练重建模型在视频潜在空间上以渐进学习策略进行操作，以实现高效的3D场景生成。</li>
<li>模型能生成宽视野和通用场景，在各种数据集上的表现优于现有单视图3D场景生成方法。</li>
<li>该模型首次展示了在扩散模型的潜在空间上构建有效的3D重建模型以实现高效的场景生成。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12091">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-e6a2d95694980601963f3bccbda83423.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cb22c485b79e1e1879dd15b2d1a685e0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8497b85ef44881476146de4b318058d5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9248f46536c06ac8e68f22d2002853b0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-30/3DGS/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-30/3DGS/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/3DGS/">
                                    <span class="chip bg-color">3DGS</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-30/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-9d1d8a2d3376eb8bcec01d470d25c2b5.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2025-04-30  Joint Optimization of Neural Radiance Fields and Continuous Camera   Motion from a Monocular Video
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-30
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-30/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-19245332300110d488b93963ce43368b.jpg" class="responsive-img" alt="元宇宙/虚拟人">
                        
                        <span class="card-title">元宇宙/虚拟人</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            元宇宙/虚拟人 方向最新论文已更新，请持续关注 Update in 2025-04-30  Real-time High-fidelity Gaussian Human Avatars with Position-based   Interpolation of Spatially Distributed MLPs
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-30
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/" class="post-category">
                                    元宇宙/虚拟人
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                        <span class="chip bg-color">元宇宙/虚拟人</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">17012.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
