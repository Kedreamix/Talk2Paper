<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-30  DEEMO De-identity Multimodal Emotion Recognition and Reasoning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-ee0ea70a875db937618271d0caa1b451.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-30
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    82 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-30-æ›´æ–°"><a href="#2025-04-30-æ›´æ–°" class="headerlink" title="2025-04-30 æ›´æ–°"></a>2025-04-30 æ›´æ–°</h1><h2 id="DEEMO-De-identity-Multimodal-Emotion-Recognition-and-Reasoning"><a href="#DEEMO-De-identity-Multimodal-Emotion-Recognition-and-Reasoning" class="headerlink" title="DEEMO: De-identity Multimodal Emotion Recognition and Reasoning"></a>DEEMO: De-identity Multimodal Emotion Recognition and Reasoning</h2><p><strong>Authors:Deng Li, Bohao Xing, Xin Liu, Baiqiang Xia, Bihan Wen, Heikki KÃ¤lviÃ¤inen</strong></p>
<p>Emotion understanding is a critical yet challenging task. Most existing approaches rely heavily on identity-sensitive information, such as facial expressions and speech, which raises concerns about personal privacy. To address this, we introduce the De-identity Multimodal Emotion Recognition and Reasoning (DEEMO), a novel task designed to enable emotion understanding using de-identified video and audio inputs. The DEEMO dataset consists of two subsets: DEEMO-NFBL, which includes rich annotations of Non-Facial Body Language (NFBL), and DEEMO-MER, an instruction dataset for Multimodal Emotion Recognition and Reasoning using identity-free cues. This design supports emotion understanding without compromising identity privacy. In addition, we propose DEEMO-LLaMA, a Multimodal Large Language Model (MLLM) that integrates de-identified audio, video, and textual information to enhance both emotion recognition and reasoning. Extensive experiments show that DEEMO-LLaMA achieves state-of-the-art performance on both tasks, outperforming existing MLLMs by a significant margin, achieving 74.49% accuracy and 74.45% F1-score in de-identity emotion recognition, and 6.20 clue overlap and 7.66 label overlap in de-identity emotion reasoning. Our work contributes to ethical AI by advancing privacy-preserving emotion understanding and promoting responsible affective computing. </p>
<blockquote>
<p>æƒ…æ„Ÿç†è§£æ˜¯ä¸€é¡¹è‡³å…³é‡è¦ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚ç°æœ‰çš„å¤§å¤šæ•°æ–¹æ³•éƒ½ä¸¥é‡ä¾èµ–äºèº«ä»½æ•æ„Ÿä¿¡æ¯ï¼Œå¦‚é¢éƒ¨è¡¨æƒ…å’Œè¯­éŸ³ï¼Œè¿™å¼•å‘äº†å…³äºä¸ªäººéšç§çš„æ‹…å¿§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†å»èº«ä»½åŒ–å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«å’Œæ¨ç†ï¼ˆDEEMOï¼‰è¿™ä¸€æ–°ä»»åŠ¡ï¼Œæ—¨åœ¨ä½¿ç”¨å»èº«ä»½åŒ–çš„è§†é¢‘å’ŒéŸ³é¢‘è¾“å…¥æ¥å®ç°æƒ…æ„Ÿç†è§£ã€‚DEEMOæ•°æ®é›†ç”±ä¸¤ä¸ªå­é›†ç»„æˆï¼šDEEMO-NFBLï¼ŒåŒ…å«ä¸°å¯Œçš„éé¢éƒ¨è‚¢ä½“è¯­è¨€ï¼ˆNFBLï¼‰æ³¨é‡Šï¼›ä»¥åŠDEEMO-MERï¼Œè¿™æ˜¯ä¸€ä¸ªä½¿ç”¨æ— èº«ä»½æç¤ºè¿›è¡Œå¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«å’Œæ¨ç†çš„æŒ‡ä»¤æ•°æ®é›†ã€‚è¿™ç§è®¾è®¡å¯ä»¥åœ¨ä¸æŸå®³èº«ä»½éšç§çš„æƒ…å†µä¸‹æ”¯æŒæƒ…æ„Ÿç†è§£ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†DEEMO-LLaMAï¼Œè¿™æ˜¯ä¸€ç§å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œå®ƒæ•´åˆäº†å»èº«ä»½åŒ–çš„éŸ³é¢‘ã€è§†é¢‘å’Œæ–‡æœ¬ä¿¡æ¯ï¼Œä»¥å¢å¼ºæƒ…æ„Ÿè¯†åˆ«å’Œæ¨ç†ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDEEMO-LLaMAåœ¨è¿™ä¸¤é¡¹ä»»åŠ¡ä¸Šéƒ½è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„MLLMsï¼Œåœ¨å»èº«ä»½æƒ…æ„Ÿè¯†åˆ«æ–¹é¢è¾¾åˆ°74.49%çš„å‡†ç¡®ç‡å’Œ74.45%çš„F1å¾—åˆ†ï¼Œåœ¨å»èº«ä»½æƒ…æ„Ÿæ¨ç†æ–¹é¢è¾¾åˆ°6.20çš„çº¿ç´¢é‡å ç‡å’Œ7.66çš„æ ‡ç­¾é‡å ç‡ã€‚æˆ‘ä»¬çš„å·¥ä½œé€šè¿‡æ¨è¿›ä¿æŠ¤éšç§çš„æƒ…æ„Ÿç†è§£å’Œä¿ƒè¿›è´Ÿè´£ä»»çš„æƒ…æ„Ÿè®¡ç®—ï¼Œä¸ºä¼¦ç†äººå·¥æ™ºèƒ½åšå‡ºè´¡çŒ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.19549v1">PDF</a> </p>
<p><strong>Summary</strong><br>     è¯¥ç ”ç©¶æå‡ºäº†ä¸€é¡¹æ–°çš„ä»»åŠ¡â€”â€”å»èº«ä»½å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«å’Œæ¨ç†ï¼ˆDEEMOï¼‰ï¼Œæ—¨åœ¨ä½¿ç”¨å»èº«ä»½çš„è§†é¢‘å’ŒéŸ³é¢‘è¾“å…¥è¿›è¡Œæƒ…æ„Ÿç†è§£ï¼Œä»¥è§£å†³ç°æœ‰æ–¹æ³•è¿‡äºä¾èµ–èº«ä»½æ•æ„Ÿä¿¡æ¯çš„é—®é¢˜ã€‚è¯¥æ•°æ®é›†åŒ…å«ä¸¤ä¸ªå­é›†ï¼Œåˆ†åˆ«å…³æ³¨éé¢éƒ¨è‚¢ä½“è¯­è¨€ï¼ˆNFBLï¼‰å’Œå¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«å’Œæ¨ç†ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ä»¥å¢å¼ºæƒ…æ„Ÿè¯†åˆ«å’Œæ¨ç†ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å»èº«ä»½æƒ…æ„Ÿè¯†åˆ«å’Œæ¨ç†ä»»åŠ¡ä¸Šå‡è¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œå¯¹æ¨è¿›éšç§ä¿æŠ¤çš„æƒ…æ„Ÿç†è§£å’Œè´Ÿè´£ä»»çš„æƒ…æ„Ÿè®¡ç®—å…·æœ‰è´¡çŒ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç ”ç©¶å¼•å…¥äº†å»èº«ä»½å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«å’Œæ¨ç†ä»»åŠ¡ï¼ˆDEEMOï¼‰ï¼Œæ—¨åœ¨è§£å†³ä¾èµ–èº«ä»½æ•æ„Ÿä¿¡æ¯çš„æƒ…æ„Ÿç†è§£çš„é—®é¢˜ã€‚</li>
<li>DEEMOæ•°æ®é›†åŒ…å«ä¸¤ä¸ªå­é›†ï¼šå…³æ³¨éé¢éƒ¨è‚¢ä½“è¯­è¨€çš„DEEMO-NFBLå’Œç”¨äºå¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«çš„DEEMO-MERã€‚</li>
<li>æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰DEEMOLLaMAï¼Œé›†æˆå»èº«ä»½éŸ³é¢‘ã€è§†é¢‘å’Œæ–‡æœ¬ä¿¡æ¯ï¼Œå¢å¼ºæƒ…æ„Ÿè¯†åˆ«å’Œæ¨ç†ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºDEEMOLLaMAåœ¨å»èº«ä»½æƒ…æ„Ÿè¯†åˆ«å’Œæ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.19549">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ef82a44ca1f97add7ab4c3da2a47a368.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c1fc19665aeaa5cbae8038bf9ec45c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b62ba7a578626ac9dcefddbe070d1ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-71bbd8b45ff99960170648dd6515acf9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e179c322c6e7c71764844bf3cd7da20.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f434c9b3bb26f69965e33fb10b185f33.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="LR-IAD-Mask-Free-Industrial-Anomaly-Detection-with-Logical-Reasoning"><a href="#LR-IAD-Mask-Free-Industrial-Anomaly-Detection-with-Logical-Reasoning" class="headerlink" title="LR-IAD:Mask-Free Industrial Anomaly Detection with Logical Reasoning"></a>LR-IAD:Mask-Free Industrial Anomaly Detection with Logical Reasoning</h2><p><strong>Authors:Peijian Zeng, Feiyan Pang, Zhanbo Wang, Aimin Yang</strong></p>
<p>Industrial Anomaly Detection (IAD) is critical for ensuring product quality by identifying defects. Traditional methods such as feature embedding and reconstruction-based approaches require large datasets and struggle with scalability. Existing vision-language models (VLMs) and Multimodal Large Language Models (MLLMs) address some limitations but rely on mask annotations, leading to high implementation costs and false positives. Additionally, industrial datasets like MVTec-AD and VisA suffer from severe class imbalance, with defect samples constituting only 23.8% and 11.1% of total data respectively. To address these challenges, we propose a reward function that dynamically prioritizes rare defect patterns during training to handle class imbalance. We also introduce a mask-free reasoning framework using Chain of Thought (CoT) and Group Relative Policy Optimization (GRPO) mechanisms, enabling anomaly detection directly from raw images without annotated masks. This approach generates interpretable step-by-step explanations for defect localization. Our method achieves state-of-the-art performance, outperforming prior approaches by 36% in accuracy on MVTec-AD and 16% on VisA. By eliminating mask dependency and reducing costs while providing explainable outputs, this work advances industrial anomaly detection and supports scalable quality control in manufacturing. Code to reproduce the experiment is available at <a target="_blank" rel="noopener" href="https://github.com/LilaKen/LR-IAD">https://github.com/LilaKen/LR-IAD</a>. </p>
<blockquote>
<p>å·¥ä¸šå¼‚å¸¸æ£€æµ‹ï¼ˆIADï¼‰å¯¹äºé€šè¿‡è¯†åˆ«ç¼ºé™·æ¥ä¿è¯äº§å“è´¨é‡è‡³å…³é‡è¦ã€‚ä¼ ç»Ÿçš„æ–¹æ³•ï¼Œå¦‚åŸºäºç‰¹å¾åµŒå…¥å’Œé‡å»ºçš„æ–¹æ³•ï¼Œéœ€è¦å¤§è§„æ¨¡çš„æ•°æ®é›†ï¼Œå¹¶ä¸”åœ¨å¯æ‰©å±•æ€§æ–¹é¢å­˜åœ¨å›°éš¾ã€‚ç°æœ‰çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰è§£å†³äº†ä¸€äº›é™åˆ¶ï¼Œä½†å®ƒä»¬ä¾èµ–äºæ©è†œæ³¨é‡Šï¼Œå¯¼è‡´å®æ–½æˆæœ¬é«˜å’Œå‡é˜³æ€§ç»“æœå¤šã€‚æ­¤å¤–ï¼Œå·¥ä¸šæ•°æ®é›†å¦‚MVTec-ADå’ŒVisAå­˜åœ¨ä¸¥é‡çš„ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼Œç¼ºé™·æ ·æœ¬ä»…å æ€»æ•°æ•°æ®çš„23.8%å’Œ11.1%ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¥–åŠ±å‡½æ•°ï¼Œè¯¥å‡½æ•°åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŠ¨æ€ä¼˜å…ˆå¤„ç†ç½•è§çš„ç¼ºé™·æ¨¡å¼ï¼Œä»¥å¤„ç†ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§æ— æ©è†œæ¨ç†æ¡†æ¶ï¼Œä½¿ç”¨æ€ç»´é“¾ï¼ˆCoTï¼‰å’Œç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰æœºåˆ¶ï¼Œèƒ½å¤Ÿç›´æ¥ä»åŸå§‹å›¾åƒä¸­è¿›è¡Œå¼‚å¸¸æ£€æµ‹ï¼Œæ— éœ€æ³¨é‡Šæ©è†œã€‚è¯¥æ–¹æ³•ç”Ÿæˆäº†å¯è§£é‡Šçš„ã€æŒ‰æ­¥éª¤è¿›è¡Œçš„ç¼ºé™·å®šä½è§£é‡Šã€‚æˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„è¡¨ç°ï¼Œåœ¨MVTec-ADä¸Šå‡†ç¡®åº¦æé«˜äº†36%ï¼Œåœ¨VisAä¸Šæé«˜äº†16%ã€‚é€šè¿‡æ¶ˆé™¤å¯¹æ©è†œçš„ä¾èµ–ï¼Œé™ä½æˆæœ¬ï¼ŒåŒæ—¶æä¾›å¯è§£é‡Šçš„è¾“å‡ºï¼Œè¿™é¡¹å·¥ä½œæ¨åŠ¨äº†å·¥ä¸šå¼‚å¸¸æ£€æµ‹çš„å‘å±•ï¼Œæ”¯æŒåˆ¶é€ ä¸šçš„å¯æ‰©å±•è´¨é‡æ§åˆ¶ã€‚å®éªŒé‡ç°çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/LilaKen/LR-IAD">https://github.com/LilaKen/LR-IAD</a>ä¸­æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.19524v1">PDF</a> 10 pages</p>
<p><strong>Summary</strong><br>     è¯¥ç ”ç©¶è§£å†³äº†å·¥ä¸šå¼‚å¸¸æ£€æµ‹ä¸­çš„å…³é”®é—®é¢˜ï¼ŒåŒ…æ‹¬æ•°æ®é›†è§„æ¨¡ã€æ ‡æ³¨æˆæœ¬ã€ç±»ä¸å¹³è¡¡å’Œè§£é‡Šæ€§ç¼ºå¤±ç­‰æŒ‘æˆ˜ã€‚é€šè¿‡å¼•å…¥å¥–åŠ±å‡½æ•°ã€æ— æ©è†œæ¨ç†æ¡†æ¶å’Œç›¸å¯¹ç­–ç•¥ä¼˜åŒ–æœºåˆ¶ï¼Œè¯¥ç ”ç©¶å®ç°äº†ç›´æ¥å¯¹åŸå§‹å›¾åƒè¿›è¡Œå¼‚å¸¸æ£€æµ‹å¹¶å®ç°äº†æ­¥éª¤åŒ–çš„ç¼ºé™·å®šä½è§£é‡Šã€‚è¯¥æ–¹æ³•åœ¨MVTec-ADå’ŒVisAæ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡è¾¾åˆ°äº†ä¸šç•Œé¢†å…ˆæ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶é’ˆå¯¹å·¥ä¸šå¼‚å¸¸æ£€æµ‹ä¸­çš„å…³é”®é—®é¢˜ï¼ŒåŒ…æ‹¬æ•°æ®é›†è§„æ¨¡ã€ç±»ä¸å¹³è¡¡ç­‰æŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥å¥–åŠ±å‡½æ•°ä»¥å¤„ç†ç±»ä¸å¹³è¡¡é—®é¢˜ï¼ŒåŠ¨æ€ä¼˜å…ˆåŒ–ç½•è§ç¼ºé™·æ¨¡å¼ã€‚</li>
<li>æå‡ºæ— æ©è†œæ¨ç†æ¡†æ¶ï¼Œåˆ©ç”¨Chain of Thought (CoT)å’ŒGroup Relative Policy Optimization (GRPO)æœºåˆ¶ï¼Œå®ç°ç›´æ¥å¯¹åŸå§‹å›¾åƒè¿›è¡Œå¼‚å¸¸æ£€æµ‹ã€‚</li>
<li>æ–¹æ³•ç”Ÿæˆå¯è§£é‡Šçš„æ­¥éª¤åŒ–ç¼ºé™·å®šä½ï¼Œæé«˜äº†å¼‚å¸¸æ£€æµ‹çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚</li>
<li>åœ¨MVTec-ADå’ŒVisAæ•°æ®é›†ä¸Šå®ç°äº†ä¸šç•Œé¢†å…ˆçš„æ€§èƒ½ï¼Œåˆ†åˆ«æé«˜äº†36%å’Œ16%çš„å‡†ç¡®ç‡ã€‚</li>
<li>æ¶ˆé™¤äº†å¯¹æ©è†œçš„ä¾èµ–ï¼Œé™ä½äº†å®æ–½æˆæœ¬ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.19524">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fbf9948c18c379cb28ecabcaaba33c7e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3d721435e09d05183cc6c0bd4d568389.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5e9827cebd8bab724dd3ef85bab1490c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ca02818e5b42d581102497d86be25b2e.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Improving-Reasoning-Performance-in-Large-Language-Models-via-Representation-Engineering"><a href="#Improving-Reasoning-Performance-in-Large-Language-Models-via-Representation-Engineering" class="headerlink" title="Improving Reasoning Performance in Large Language Models via   Representation Engineering"></a>Improving Reasoning Performance in Large Language Models via   Representation Engineering</h2><p><strong>Authors:Bertram HÃ¸jer, Oliver Jarvis, Stefan Heinrich</strong></p>
<p>Recent advancements in large language models (LLMs) have resulted in increasingly anthropomorphic language concerning the ability of LLMs to reason. Whether reasoning in LLMs should be understood to be inherently different is, however, widely debated. We propose utilizing a representation engineering approach wherein model activations are read from the residual stream of an LLM when processing a reasoning task. The activations are used to derive a control vector that is applied to the model as an inference-time intervention, modulating the representational space of the model, to improve performance on the specified task. We publish the code for deriving control vectors and analyzing model representations. The method allows us to improve performance on reasoning benchmarks and assess how control vectors influence the final logit distribution of a model via metrics such as KL divergence and entropy. We apply control vectors to Mistral-7B-Instruct and a range of Pythia models on an inductive, a deductive and mathematical reasoning task. We show that an LLM can, to a certain degree, be controlled to improve its perceived reasoning ability by modulating activations. The intervention is dependent upon the ability to reliably extract the modelâ€™s typical state when correctly solving a task. Our results suggest that reasoning performance can be modulated in the same manner as other information-processing tasks performed by LLMs and demonstrate that we are capable of improving performance on specific tasks via a simple intervention on the residual stream with no additional training. </p>
<blockquote>
<p>æœ€è¿‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥å¯¼è‡´äº†è¶Šæ¥è¶Šå¤šå…³äºLLMæ¨ç†èƒ½åŠ›çš„æ‹ŸäººåŒ–è¯­è¨€ã€‚ç„¶è€Œï¼Œå…³äºæ˜¯å¦åº”ç†è§£LLMä¸­çš„æ¨ç†èƒ½åŠ›å…·æœ‰æœ¬è´¨å·®å¼‚ï¼Œå­˜åœ¨å¹¿æ³›äº‰è®®ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§è¡¨ç¤ºå·¥ç¨‹æ–¹æ³•ï¼Œåœ¨è¯¥æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬ä»LLMçš„æ®‹å·®æµä¸­è¯»å–æ¨¡å‹æ¿€æ´»ï¼Œå¹¶å°†å…¶åº”ç”¨äºå¤„ç†æ¨ç†ä»»åŠ¡æ—¶ã€‚è¿™äº›æ¿€æ´»è¢«ç”¨æ¥æ¨å¯¼ä¸€ä¸ªæ§åˆ¶å‘é‡ï¼Œè¯¥å‘é‡ä½œä¸ºæ¨ç†æ—¶é—´å¹²é¢„åº”ç”¨äºæ¨¡å‹ï¼Œè°ƒèŠ‚æ¨¡å‹çš„è¡¨ç¤ºç©ºé—´ï¼Œä»¥æé«˜åœ¨æŒ‡å®šä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚æˆ‘ä»¬å‘å¸ƒäº†ç”¨äºæ¨å¯¼æ§åˆ¶å‘é‡å’Œåˆ†ææ¨¡å‹è¡¨ç¤ºçš„ä»£ç ã€‚è¯¥æ–¹æ³•å…è®¸æˆ‘ä»¬åœ¨æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šæé«˜æ€§èƒ½ï¼Œå¹¶é€šè¿‡KLæ•£åº¦å’Œç†µç­‰åº¦é‡æ ‡å‡†è¯„ä¼°æ§åˆ¶å‘é‡å¦‚ä½•å½±å“æ¨¡å‹çš„æœ€ç»ˆé€»è¾‘åˆ†å¸ƒã€‚æˆ‘ä»¬å°†æ§åˆ¶å‘é‡åº”ç”¨äºMistral-7B-Instructå’Œä¸€ç³»åˆ—Pythiaæ¨¡å‹ï¼Œè¿›è¡Œå½’çº³ã€æ¼”ç»å’Œæ•°å­¦æ¨ç†ä»»åŠ¡ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œé€šè¿‡è°ƒèŠ‚æ¿€æ´»ï¼Œå¯ä»¥åœ¨ä¸€å®šç¨‹åº¦ä¸Šæ§åˆ¶LLMæé«˜å…¶è¢«è§†ä¸ºçš„æ¨ç†èƒ½åŠ›ã€‚è¿™ç§å¹²é¢„å–å†³äºåœ¨æ­£ç¡®å®Œæˆä»»åŠ¡æ—¶å¯é åœ°æå–æ¨¡å‹å…¸å‹çŠ¶æ€çš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œä¸LLMæ‰§è¡Œçš„å…¶ä»–ä¿¡æ¯å¤„ç†ä»»åŠ¡ç±»ä¼¼ï¼Œæ¨ç†æ€§èƒ½å¯ä»¥ä»¥ç›¸åŒçš„æ–¹å¼è¿›è¡Œè°ƒèŠ‚ï¼Œå¹¶ä¸”æˆ‘ä»¬èƒ½å¤Ÿé€šè¿‡åœ¨æ®‹å·®æµä¸Šè¿›è¡Œç®€å•å¹²é¢„ï¼ˆæ— éœ€é¢å¤–è®­ç»ƒï¼‰æ¥æé«˜ç‰¹å®šä»»åŠ¡çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.19483v1">PDF</a> Has been accepted at â€œThe Thirteenth International Conference on   Learning Representations (ICLR 2025)â€ Link to publication:   <a target="_blank" rel="noopener" href="https://openreview.net/forum?id=IssPhpUsKt">https://openreview.net/forum?id=IssPhpUsKt</a></p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æœ€æ–°è¿›å±•æ˜¾ç¤ºå…¶åœ¨æ¨ç†èƒ½åŠ›æ–¹é¢è¶Šæ¥è¶Šå…·æœ‰äººç±»ç‰¹å¾ã€‚ç„¶è€Œï¼Œå…³äºLLMsçš„æ¨ç†èƒ½åŠ›æ˜¯å¦åº”è¢«è§†ä¸ºæœ¬è´¨ä¸Šä¸åŒçš„è§‚ç‚¹å­˜åœ¨å¹¿æ³›äº‰è®®ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§è¡¨å¾å·¥ç¨‹æ–¹æ³•ï¼Œå³ä»LLMçš„æ®‹å·®æµä¸­è¯»å–æ¨¡å‹æ¿€æ´»ï¼Œç”¨äºç”Ÿæˆæ§åˆ¶å‘é‡ã€‚åœ¨æ¨ç†ä»»åŠ¡å¤„ç†è¿‡ç¨‹ä¸­ï¼Œå°†æ§åˆ¶å‘é‡åº”ç”¨äºæ¨¡å‹ï¼Œä½œä¸ºæ¨ç†æ—¶é—´å¹²é¢„æªæ–½ï¼Œå¯¹æ¨¡å‹çš„è¡¨ç¤ºç©ºé—´è¿›è¡Œè°ƒåˆ¶ï¼Œä»¥æé«˜ç‰¹å®šä»»åŠ¡çš„æ€§èƒ½ã€‚æœ¬æ–‡å…¬å¸ƒäº†ç”Ÿæˆæ§åˆ¶å‘é‡å’Œåˆ†ææ¨¡å‹è¡¨å¾çš„ä»£ç ã€‚è¯¥æ–¹æ³•å…è®¸æˆ‘ä»¬åœ¨æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šæé«˜æ€§èƒ½ï¼Œå¹¶é€šè¿‡KLæ•£åº¦å’Œç†µç­‰æŒ‡æ ‡è¯„ä¼°æ§åˆ¶å‘é‡å¦‚ä½•å½±å“æ¨¡å‹çš„æœ€ç»ˆé€»è¾‘åˆ†å¸ƒã€‚æˆ‘ä»¬åœ¨Mistral-7B-Instructå’Œä¸€ç³»åˆ—Pythiaæ¨¡å‹ä¸Šåº”ç”¨äº†æ§åˆ¶å‘é‡ï¼Œä»¥è¿›è¡Œå½’çº³ã€æ¼”ç»å’Œæ•°å­¦æ¨ç†ä»»åŠ¡ã€‚ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡è°ƒåˆ¶æ¿€æ´»ï¼Œå¯ä»¥åœ¨ä¸€å®šç¨‹åº¦ä¸Šæ§åˆ¶LLMæé«˜å…¶è¢«æ„ŸçŸ¥çš„æ¨ç†èƒ½åŠ›ã€‚è¿™ç§å¹²é¢„ä¾èµ–äºåœ¨æ­£ç¡®å®Œæˆä»»åŠ¡æ—¶å¯é æå–æ¨¡å‹å…¸å‹çŠ¶æ€çš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå¯ä»¥é€šè¿‡å¯¹æ®‹å·®æµçš„ç®€å•å¹²é¢„ï¼Œåœ¨ä¸éœ€è¦é¢å¤–è®­ç»ƒçš„æƒ…å†µä¸‹æé«˜ç‰¹å®šä»»åŠ¡çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†èƒ½åŠ›æ—¥ç›Šå…·æœ‰äººç±»ç‰¹å¾ï¼Œä½†å…³äºå…¶æœ¬è´¨å·®å¼‚å­˜åœ¨äº‰è®®ã€‚</li>
<li>æå‡ºäº†ä¸€ç§è¡¨å¾å·¥ç¨‹æ–¹æ³•ï¼Œé€šè¿‡æ¨¡å‹æ¿€æ´»ç”Ÿæˆæ§åˆ¶å‘é‡ï¼Œä»¥æé«˜LLMåœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>æ§åˆ¶å‘é‡å¯åº”ç”¨äºæ”¹å–„æ¨ç†åŸºå‡†æµ‹è¯•çš„æ€§èƒ½ã€‚</li>
<li>é€šè¿‡åº¦é‡å¦‚KLæ•£åº¦å’Œç†µçš„æŒ‡æ ‡ï¼Œè¯„ä¼°äº†æ§åˆ¶å‘é‡å¯¹æ¨¡å‹é€»è¾‘åˆ†å¸ƒçš„å½±å“ã€‚</li>
<li>åœ¨å¤šç§æ¨¡å‹å’Œä»»åŠ¡ä¸ŠéªŒè¯äº†æ§åˆ¶å‘é‡çš„åº”ç”¨æ•ˆæœã€‚</li>
<li>å¯é æå–æ¨¡å‹å…¸å‹çŠ¶æ€çš„èƒ½åŠ›æ˜¯è¿›è¡Œæœ‰æ•ˆå¹²é¢„çš„å…³é”®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.19483">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-53ea71d9039333ef25b6db358cdf4ab4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3fcc39a966d6b2db968a719c3245f7ee.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="LLMs-for-Engineering-Teaching-Models-to-Design-High-Powered-Rockets"><a href="#LLMs-for-Engineering-Teaching-Models-to-Design-High-Powered-Rockets" class="headerlink" title="LLMs for Engineering: Teaching Models to Design High Powered Rockets"></a>LLMs for Engineering: Teaching Models to Design High Powered Rockets</h2><p><strong>Authors:Toby Simonds</strong></p>
<p>Large Language Models (LLMs) have transformed software engineering, but their application to physical engineering domains remains underexplored. This paper evaluates LLMsâ€™ capabilities in high-powered rocketry design through RocketBench, a benchmark connecting LLMs to high-fidelity rocket simulations. We test models on two increasingly complex design tasks: target altitude optimization and precision landing challenges. Our findings reveal that while state-of-the-art LLMs demonstrate strong baseline engineering knowledge, they struggle to iterate on their designs when given simulation results and ultimately plateau below human performance levels. However, when enhanced with reinforcement learning (RL), we show that a 7B parameter model outperforms both SoTA foundation models and human experts. This research demonstrates that RL-trained LLMs can serve as effective tools for complex engineering optimization, potentially transforming engineering domains beyond software development. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»æ”¹å˜äº†è½¯ä»¶å·¥ç¨‹çš„é¢è²Œï¼Œä½†å®ƒä»¬åœ¨ç‰©ç†å·¥ç¨‹é¢†åŸŸçš„åº”ç”¨ä»ç„¶è¢«æ¢ç´¢å¾—ä¸å¤Ÿã€‚æœ¬æ–‡é€šè¿‡RocketBenchè¿™ä¸€å°†LLMä¸é«˜ç²¾åº¦ç«ç®­æ¨¡æ‹Ÿç›¸ç»“åˆçš„åŸºå‡†æµ‹è¯•ï¼Œè¯„ä¼°äº†LLMåœ¨é«˜æ€§èƒ½ç«ç®­è®¾è®¡æ–¹é¢çš„èƒ½åŠ›ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªæ—¥ç›Šå¤æ‚çš„è®¾è®¡ä»»åŠ¡ä¸Šæµ‹è¯•äº†è¿™äº›æ¨¡å‹ï¼šç›®æ ‡é«˜åº¦ä¼˜åŒ–å’Œç²¾ç¡®ç€é™†æŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œè™½ç„¶æœ€æ–°LLMæ˜¾ç¤ºå‡ºå¼ºå¤§çš„åŸºç¡€å·¥ç¨‹çŸ¥è¯†ï¼Œä½†å®ƒä»¬åœ¨æ¨¡æ‹Ÿç»“æœæŒ‡å¯¼ä¸‹è¿›è¡Œè®¾è®¡è¿­ä»£æ—¶é¢ä¸´å›°éš¾ï¼Œæœ€ç»ˆè¡¨ç°æ°´å¹³ä½äºäººç±»è¡¨ç°æ°´å¹³ã€‚ç„¶è€Œï¼Œå½“ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›è¡Œå¢å¼ºæ—¶ï¼Œæˆ‘ä»¬è¯æ˜äº†ä¸€ä¸ª7Bå‚æ•°æ¨¡å‹çš„è¡¨ç°è¶…è¿‡äº†æœ€æ–°å‰æ²¿åŸºç¡€æ¨¡å‹å’Œäººç±»ä¸“å®¶ã€‚è¿™é¡¹ç ”ç©¶è¡¨æ˜ï¼Œç»è¿‡RLè®­ç»ƒçš„LLMå¯ä»¥ä½œä¸ºå¤æ‚å·¥ç¨‹ä¼˜åŒ–çš„æœ‰æ•ˆå·¥å…·ï¼Œæœ‰å¯èƒ½æ”¹å˜è½¯ä»¶å¼€å‘ä»¥å¤–çš„å·¥ç¨‹é¢†åŸŸã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.19394v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è½¯ä»¶å·¥ç¨‹ä¸­çš„åº”ç”¨å·²ç»æˆç†Ÿï¼Œä½†åœ¨ç‰©ç†å·¥ç¨‹é¢†åŸŸçš„åº”ç”¨ä»å¾…æ¢ç´¢ã€‚æœ¬æ–‡é€šè¿‡RocketBenchåŸºå‡†æµ‹è¯•è¯„ä¼°äº†LLMsåœ¨é«˜åŠŸç‡ç«ç®­è®¾è®¡æ–¹é¢çš„èƒ½åŠ›ï¼Œè¯¥åŸºå‡†æµ‹è¯•å°†LLMsä¸é«˜ç²¾åº¦ç«ç®­æ¨¡æ‹Ÿç›¸è¿æ¥ã€‚æµ‹è¯•æ¨¡å‹åœ¨ä¸¤ä¸ªé€æ¸å¤æ‚çš„è®¾è®¡ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼šç›®æ ‡é«˜åº¦ä¼˜åŒ–å’Œç²¾ç¡®ç€é™†æŒ‘æˆ˜ã€‚ç ”ç©¶å‘ç°ï¼Œè™½ç„¶ç°æœ‰æœ€å…ˆè¿›çš„LLMså±•ç°å‡ºå¼ºå¤§çš„åŸºç¡€å·¥ç¨‹çŸ¥è¯†ï¼Œä½†å½“æ¨¡æ‹Ÿç»“æœåé¦ˆæ—¶ï¼Œå®ƒä»¬åœ¨è®¾è®¡è¿­ä»£ä¸Šé‡åˆ°å›°éš¾ï¼Œæœ€ç»ˆæ€§èƒ½æœªèƒ½è¶…è¶Šäººç±»æ°´å¹³ã€‚ç„¶è€Œï¼Œå½“ç»“åˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›è¡Œè®­ç»ƒåï¼Œä¸€ä¸ªæ‹¥æœ‰7äº¿å‚æ•°çš„æ¨¡å‹è¶…è¶Šäº†å½“å‰é¡¶å°–çš„åŸºç¡€æ¨¡å‹å’Œäººç±»ä¸“å®¶ã€‚ç ”ç©¶è¯æ˜ï¼Œç»è¿‡RLè®­ç»ƒçš„LLMså¯æˆä¸ºå¤æ‚çš„å·¥ç¨‹ä¼˜åŒ–å·¥å…·ï¼Œæœ‰æœ›ä¸ºè¶…è¶Šè½¯ä»¶å¼€å‘çš„å·¥ç¨‹é¢†åŸŸå¸¦æ¥å˜é©ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç‰©ç†å·¥ç¨‹é¢†åŸŸçš„åº”ç”¨å°šå¾…å……åˆ†æ¢ç´¢ã€‚</li>
<li>RocketBenchåŸºå‡†æµ‹è¯•ç”¨äºè¯„ä¼°LLMsåœ¨é«˜åŠŸç‡ç«ç®­è®¾è®¡æ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>LLMså±•ç°å‡ºå¼ºå¤§çš„åŸºç¡€å·¥ç¨‹çŸ¥è¯†ï¼Œä½†åœ¨è®¾è®¡è¿­ä»£å’Œåé¦ˆæ–¹é¢å­˜åœ¨å±€é™ã€‚</li>
<li>ç»“åˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒçš„LLMsæ€§èƒ½è¶…è¶Šå½“å‰é¡¶å°–çš„åŸºç¡€æ¨¡å‹å’Œäººç±»ä¸“å®¶ã€‚</li>
<li>LLMsåœ¨é«˜ç²¾åº¦ç«ç®­æ¨¡æ‹Ÿä¸­çš„ä¼˜åŒ–å·¥å…·æ½œåŠ›å·¨å¤§ã€‚</li>
<li>LLMsåœ¨å¤æ‚å·¥ç¨‹ä¼˜åŒ–æ–¹é¢å…·æœ‰é‡è¦æ„ä¹‰ï¼Œå°¤å…¶åœ¨è½¯ä»¶ä¹‹å¤–çš„é¢†åŸŸã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.19394">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-3c5fb1ed8defa27e7787cb08c5f2db19.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f3ea181c49e2c7bbe1af35cd0fee256.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-88a7af346ad180344aa60e089e07d99d.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="SPC-Evolving-Self-Play-Critic-via-Adversarial-Games-for-LLM-Reasoning"><a href="#SPC-Evolving-Self-Play-Critic-via-Adversarial-Games-for-LLM-Reasoning" class="headerlink" title="SPC: Evolving Self-Play Critic via Adversarial Games for LLM Reasoning"></a>SPC: Evolving Self-Play Critic via Adversarial Games for LLM Reasoning</h2><p><strong>Authors:Jiaqi Chen, Bang Zhang, Ruotian Ma, Peisong Wang, Xiaodan Liang, Zhaopeng Tu, Xiaolong Li, Kwan-Yee K. Wong</strong></p>
<p>Evaluating the step-by-step reliability of large language model (LLM) reasoning, such as Chain-of-Thought, remains challenging due to the difficulty and cost of obtaining high-quality step-level supervision. In this paper, we introduce Self-Play Critic (SPC), a novel approach where a critic model evolves its ability to assess reasoning steps through adversarial self-play games, eliminating the need for manual step-level annotation. SPC involves fine-tuning two copies of a base model to play two roles, namely a â€œsneaky generatorâ€ that deliberately produces erroneous steps designed to be difficult to detect, and a â€œcriticâ€ that analyzes the correctness of reasoning steps. These two models engage in an adversarial game in which the generator aims to fool the critic, while the critic model seeks to identify the generatorâ€™s errors. Using reinforcement learning based on the game outcomes, the models iteratively improve; the winner of each confrontation receives a positive reward and the loser receives a negative reward, driving continuous self-evolution. Experiments on three reasoning process benchmarks (ProcessBench, PRM800K, DeltaBench) demonstrate that our SPC progressively enhances its error detection capabilities (e.g., accuracy increases from 70.8% to 77.7% on ProcessBench) and surpasses strong baselines, including distilled R1 model. Furthermore, applying SPC to guide the test-time search of diverse LLMs significantly improves their mathematical reasoning performance on MATH500 and AIME2024, outperforming state-of-the-art process reward models. </p>
<blockquote>
<p>è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†çš„é€æ­¥å¯é æ€§ï¼Œå¦‚æ€ç»´é“¾ï¼Œä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºè·å¾—é«˜è´¨é‡æ­¥éª¤çº§ç›‘ç£çš„éš¾åº¦å’Œæˆæœ¬å¾ˆé«˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†è‡ªæˆ‘æ¸¸æˆè¯„è®ºå®¶ï¼ˆSPCï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œè¯„è®ºå®¶æ¨¡å‹é€šè¿‡å¯¹æŠ—æ€§çš„è‡ªæˆ‘æ¸¸æˆæ¥è¿›åŒ–å…¶è¯„ä¼°æ¨ç†æ­¥éª¤çš„èƒ½åŠ›ï¼Œä»è€Œæ¶ˆé™¤äº†å¯¹æ‰‹åŠ¨æ­¥éª¤çº§æ³¨é‡Šçš„éœ€æ±‚ã€‚SPCæ¶‰åŠå¾®è°ƒåŸºç¡€æ¨¡å‹çš„ä¸¤ä¸ªå‰¯æœ¬ï¼Œä»¥æ‰®æ¼”ä¸¤ä¸ªè§’è‰²ï¼Œä¸€ä¸ªæ˜¯â€œç‹¡çŒ¾çš„ç”Ÿæˆå™¨â€ï¼Œå®ƒæ•…æ„äº§ç”Ÿéš¾ä»¥æ£€æµ‹çš„é”™è¯¯æ­¥éª¤ï¼Œå¦ä¸€ä¸ªæ˜¯â€œè¯„è®ºå®¶â€ï¼Œå®ƒåˆ†ææ¨ç†æ­¥éª¤çš„æ­£ç¡®æ€§ã€‚è¿™ä¸¤ä¸ªæ¨¡å‹è¿›è¡Œäº†ä¸€åœºå¯¹æŠ—æ€§æ¸¸æˆï¼Œå…¶ä¸­ç”Ÿæˆå™¨çš„ç›®æ ‡æ˜¯æ¬ºéª—è¯„è®ºå®¶ï¼Œè€Œè¯„è®ºå®¶æ¨¡å‹çš„ç›®æ ‡æ˜¯è¯†åˆ«ç”Ÿæˆå™¨çš„é”™è¯¯ã€‚åŸºäºæ¸¸æˆç»“æœçš„å¼ºåŒ–å­¦ä¹ ï¼Œæ¨¡å‹ä¼šè¿›è¡Œè¿­ä»£æ”¹è¿›ï¼›æ¯æ¬¡å¯¹æŠ—çš„è·èƒœè€…ä¼šè·å¾—æ­£é¢å¥–åŠ±ï¼Œè€Œå¤±è´¥è€…ä¼šè·å¾—è´Ÿé¢å¥–åŠ±ï¼Œä»è€Œæ¨åŠ¨æ¨¡å‹çš„æŒç»­è‡ªæˆ‘è¿›åŒ–ã€‚åœ¨ä¸‰ä¸ªæ¨ç†è¿‡ç¨‹åŸºå‡†æµ‹è¯•ï¼ˆProcessBenchã€PRM800Kã€DeltaBenchï¼‰ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„SPCé€æ­¥æé«˜äº†å…¶é”™è¯¯æ£€æµ‹èƒ½åŠ›ï¼ˆä¾‹å¦‚ï¼Œåœ¨ProcessBenchä¸Šçš„å‡†ç¡®ç‡ä»70.8%æé«˜åˆ°77.7%ï¼‰ï¼Œå¹¶è¶…è¶Šäº†å¼ºå¤§çš„åŸºçº¿ï¼ŒåŒ…æ‹¬è’¸é¦R1æ¨¡å‹ã€‚æ­¤å¤–ï¼Œå°†SPCåº”ç”¨äºæµ‹è¯•æ—¶å¼•å¯¼å¤šç§LLMçš„æœç´¢ï¼Œæ˜¾è‘—æé«˜äº†å®ƒä»¬åœ¨MATH500å’ŒAIME2024ä¸Šçš„æ•°å­¦æ¨ç†æ€§èƒ½ï¼Œè¶…è¶Šäº†æœ€æ–°çš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>ä¸­æ–‡ç®€åŒ–ç‰ˆ</strong></p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.19162v1">PDF</a> Project: <a target="_blank" rel="noopener" href="https://chen-judge.github.io/SPC/">https://chen-judge.github.io/SPC/</a></p>
<p><strong>Summary</strong></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºSelf-Play Criticï¼ˆSPCï¼‰çš„æ–°æ–¹æ³•ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¦‚Chain-of-Thoughtç­‰çš„é€æ­¥æ¨ç†å¯é æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡å¯¹æŠ—æ€§è‡ªæˆ‘æ¸¸æˆè®©è¯„åˆ¤æ¨¡å‹è¿›åŒ–ï¼Œä»è€Œä¸éœ€è¦æ‰‹åŠ¨æ­¥éª¤çº§åˆ«çš„æ ‡æ³¨ã€‚SPCé€šè¿‡å¾®è°ƒåŸºç¡€æ¨¡å‹çš„ä¸¤ä¸ªå‰¯æœ¬ï¼Œåˆ†åˆ«æ‰®æ¼”â€œç‹¡çŒ¾çš„ç”Ÿæˆå™¨â€å’Œâ€œè¯„åˆ¤è€…â€çš„è§’è‰²ï¼Œè¿›è¡Œå¯¹æŠ—æ¸¸æˆã€‚ç”Ÿæˆå™¨æ•…æ„äº§ç”Ÿé”™è¯¯çš„æ­¥éª¤ï¼Œè€Œè¯„åˆ¤è€…åˆ™åˆ†ææ¨ç†æ­¥éª¤çš„æ­£ç¡®æ€§ã€‚åŸºäºæ¸¸æˆç»“æœï¼Œä½¿ç”¨å¼ºåŒ–å­¦ä¹ è¿›è¡Œè¿­ä»£æ”¹è¿›ã€‚å®éªŒè¡¨æ˜ï¼ŒSPCé€æ¸æé«˜äº†é”™è¯¯æ£€æµ‹èƒ½åŠ›ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†å…¶ä»–æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡å¼•å…¥äº†Self-Play Criticï¼ˆSPCï¼‰æ–¹æ³•ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„é€æ­¥æ¨ç†å¯é æ€§ã€‚</li>
<li>SPCé€šè¿‡ä¸¤ä¸ªæ¨¡å‹â€”â€”ç”Ÿæˆå™¨å’Œè¯„åˆ¤è€…â€”â€”ä¹‹é—´çš„å¯¹æŠ—æ€§æ¸¸æˆè¿›åŒ–æ¨¡å‹èƒ½åŠ›ã€‚</li>
<li>ç”Ÿæˆå™¨æ—¨åœ¨äº§ç”Ÿéš¾ä»¥æ£€æµ‹çš„é”™è¯¯æ­¥éª¤ï¼Œè€Œè¯„åˆ¤è€…åˆ™åˆ†ææ¨ç†æ­¥éª¤çš„æ­£ç¡®æ€§ã€‚</li>
<li>ä½¿ç”¨å¼ºåŒ–å­¦ä¹ åŸºäºæ¸¸æˆç»“æœå¯¹æ¨¡å‹è¿›è¡Œè¿­ä»£æ”¹è¿›ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒSPCèƒ½æé«˜é”™è¯¯æ£€æµ‹èƒ½åŠ›ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚<br>6.SPCå¯ç”¨äºæŒ‡å¯¼ä¸åŒå¤§å‹è¯­è¨€æ¨¡å‹çš„æµ‹è¯•æ—¶é—´æœç´¢ï¼Œæé«˜æ•°å­¦æ¨ç†æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.19162">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-16b85a870914d726901b8a0154b1c8aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d67e72444e159abd5b79776a81aea74a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e10911a6feca983f96417b352d2009ed.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="ChiseLLM-Unleashing-the-Power-of-Reasoning-LLMs-for-Chisel-Agile-Hardware-Development"><a href="#ChiseLLM-Unleashing-the-Power-of-Reasoning-LLMs-for-Chisel-Agile-Hardware-Development" class="headerlink" title="ChiseLLM: Unleashing the Power of Reasoning LLMs for Chisel Agile   Hardware Development"></a>ChiseLLM: Unleashing the Power of Reasoning LLMs for Chisel Agile   Hardware Development</h2><p><strong>Authors:Bowei Wang, Jiaran Gao, Yelai Feng, Renzhi Chen, Shanshan Li, Lei Wang</strong></p>
<p>The growing demand for Domain-Specific Architecture (DSA) has driven the development of Agile Hardware Development Methodology (AHDM). Hardware Construction Language (HCL) like Chisel offers high-level abstraction features, making it an ideal language for HCL-Based AHDM. While Large Language Models (LLMs) excel in code generation tasks, they still face challenges with Chisel generation, particularly regarding syntax correctness and design variability. Recent reasoning models have significantly enhanced code generation capabilities through test-time scaling techniques. However, we found that reasoning models without domain adaptation cannot bring substantial benefits to Chisel code generation tasks. This paper presents ChiseLLM, a solution comprising data processing and transformation, prompt-guided reasoning trace synthesis, and domain-adapted model training. We constructed high-quality datasets from public RTL code resources and guided the model to adopt structured thinking patterns through prompt enhancement methods. Experiments demonstrate that our ChiseLLM-7B and ChiseLLM-32B models improved syntax correctness by 18.85% and 26.32% respectively over base models, while increasing variability design ability by 47.58% compared to baseline reasoning models. Our datasets and models are publicly available, providing high-performance, cost-effective models for HCL-Based AHDM, and offering an effective baseline for future research. Github repository: <a target="_blank" rel="noopener" href="https://github.com/observerw/ChiseLLM">https://github.com/observerw/ChiseLLM</a> </p>
<blockquote>
<p>éšç€ç‰¹å®šé¢†åŸŸæ¶æ„ï¼ˆDSAï¼‰éœ€æ±‚çš„ä¸æ–­å¢é•¿ï¼Œæ•æ·ç¡¬ä»¶å¼€å‘æ–¹æ³•è®ºï¼ˆAHDMï¼‰çš„å‘å±•å¾—åˆ°äº†æ¨åŠ¨ã€‚ç¡¬ä»¶æ„é€ è¯­è¨€ï¼ˆHCLï¼‰å¦‚Chiselæä¾›äº†é«˜çº§æŠ½è±¡ç‰¹æ€§ï¼Œä½¿å…¶æˆä¸ºåŸºäºHCLçš„AHDMçš„ç†æƒ³è¯­è¨€ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä»£ç ç”Ÿæˆä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨Chiselç”Ÿæˆæ–¹é¢ä»é¢ä¸´è¯­æ³•æ­£ç¡®æ€§å’Œè®¾è®¡å¯å˜æ€§çš„æŒ‘æˆ˜ã€‚æœ€è¿‘çš„æ¨ç†æ¨¡å‹é€šè¿‡æµ‹è¯•æ—¶ç¼©æ”¾æŠ€æœ¯æ˜¾è‘—å¢å¼ºäº†ä»£ç ç”Ÿæˆèƒ½åŠ›ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å‘ç°æœªç»é¢†åŸŸé€‚åº”çš„æ¨ç†æ¨¡å‹ä¸èƒ½ç»™Chiselä»£ç ç”Ÿæˆä»»åŠ¡å¸¦æ¥å®è´¨æ€§å¥½å¤„ã€‚</p>
</blockquote>
<p>æœ¬æ–‡æå‡ºäº†ChiseLLMè§£å†³æ–¹æ¡ˆï¼ŒåŒ…æ‹¬æ•°æ®å¤„ç†å’Œè½¬æ¢ã€æç¤ºå¼•å¯¼æ¨ç†è½¨è¿¹åˆæˆå’Œé¢†åŸŸé€‚åº”æ¨¡å‹è®­ç»ƒã€‚æˆ‘ä»¬ä»å…¬å…±RTLä»£ç èµ„æºä¸­æ„å»ºäº†é«˜è´¨é‡æ•°æ®é›†ï¼Œå¹¶é€šè¿‡æç¤ºå¢å¼ºæ–¹æ³•æŒ‡å¯¼æ¨¡å‹é‡‡ç”¨ç»“æ„åŒ–æ€ç»´æ¨¡å¼ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ChiseLLM-7Bå’ŒChiseLLM-32Bæ¨¡å‹ç›¸è¾ƒäºåŸºå‡†æ¨¡å‹ï¼Œè¯­æ³•æ­£ç¡®æ€§åˆ†åˆ«æé«˜äº†18.85%å’Œ26.32%ï¼ŒåŒæ—¶ç›¸è¾ƒäºåŸºçº¿æ¨ç†æ¨¡å‹ï¼Œæé«˜äº†47.58%çš„è®¾è®¡èƒ½åŠ›å¯å˜åº¦ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œæ¨¡å‹æ˜¯å…¬å¼€çš„ï¼Œä¸ºåŸºäºHCLçš„AHDMæä¾›äº†é«˜æ€§èƒ½ã€ç»æµå®æƒ çš„æ¨¡å‹ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æœ‰æ•ˆçš„åŸºå‡†çº¿ã€‚Githubä»“åº“åœ°å€ï¼š<a target="_blank" rel="noopener" href="https://github.com/observerw/ChiseLLM">https://github.com/observerw/ChiseLLM</a></p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.19144v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šéšç€é¢†åŸŸç‰¹å®šæ¶æ„ï¼ˆDSAï¼‰éœ€æ±‚çš„å¢é•¿ï¼Œæ•æ·ç¡¬ä»¶å¼€å‘æ–¹æ³•è®ºï¼ˆAHDMï¼‰å¾—åˆ°å‘å±•ã€‚ç¡¬ä»¶æ„é€ è¯­è¨€ï¼ˆHCLï¼‰å¦‚Chiselå…·æœ‰é«˜çº§æŠ½è±¡ç‰¹æ€§ï¼Œé€‚åˆç”¨äºHCL-Based AHDMã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä»£ç ç”Ÿæˆä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨Chiselç”Ÿæˆæ–¹é¢ä»é¢ä¸´è¯­æ³•æ­£ç¡®æ€§å’Œè®¾è®¡å¯å˜æ€§çš„æŒ‘æˆ˜ã€‚æœ€æ–°æ¨ç†æ¨¡å‹é€šè¿‡æµ‹è¯•æ—¶ç¼©æ”¾æŠ€æœ¯æ˜¾è‘—å¢å¼ºäº†ä»£ç ç”Ÿæˆèƒ½åŠ›ï¼Œä½†ç¼ºä¹é¢†åŸŸé€‚åº”æ€§çš„æ¨ç†æ¨¡å‹å¯¹Chiselä»£ç ç”Ÿæˆä»»åŠ¡çš„ç›Šå¤„æœ‰é™ã€‚æœ¬æ–‡æå‡ºChiseLLMï¼ŒåŒ…å«æ•°æ®å¤„ç†å’Œè½¬æ¢ã€æç¤ºå¼•å¯¼æ¨ç†è·Ÿè¸ªåˆæˆå’Œé¢†åŸŸé€‚åº”æ¨¡å‹è®­ç»ƒã€‚é€šè¿‡æ„å»ºé«˜è´¨é‡æ•°æ®é›†å’Œå¼•å¯¼æ¨¡å‹é‡‡ç”¨ç»“æ„åŒ–æ€ç»´ï¼Œå®éªŒè¡¨æ˜ChiseLLMæ¨¡å‹åœ¨è¯­æ³•æ­£ç¡®æ€§å’Œè®¾è®¡å¯å˜æ€§æ–¹é¢ä¼˜äºåŸºç¡€æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>Domain-Specific Architecture (DSA)çš„å¢é•¿æ¨åŠ¨äº†æ•æ·ç¡¬ä»¶å¼€å‘æ–¹æ³•è®ºï¼ˆAHDMï¼‰çš„å‘å±•ã€‚</li>
<li>ç¡¬ä»¶æ„é€ è¯­è¨€ï¼ˆHCLï¼‰å¦‚Chiselå…·æœ‰é«˜çº§æŠ½è±¡ç‰¹æ€§ï¼Œé€‚åˆç”¨äºHCL-Based AHDMã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨Chiselä»£ç ç”Ÿæˆæ–¹é¢é¢ä¸´è¯­æ³•æ­£ç¡®æ€§å’Œè®¾è®¡å¯å˜æ€§çš„æŒ‘æˆ˜ã€‚</li>
<li>æœ€æ–°æ¨ç†æ¨¡å‹é€šè¿‡æµ‹è¯•æ—¶ç¼©æ”¾æŠ€æœ¯æé«˜äº†ä»£ç ç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>ç¼ºä¹é¢†åŸŸé€‚åº”æ€§çš„æ¨ç†æ¨¡å‹åœ¨Chiselä»£ç ç”Ÿæˆä»»åŠ¡ä¸Šçš„æ•ˆç›Šæœ‰é™ã€‚</li>
<li>ChiseLLMé€šè¿‡æ•°æ®å¤„ç†å’Œè½¬æ¢ã€æç¤ºå¼•å¯¼æ¨ç†è·Ÿè¸ªåˆæˆå’Œé¢†åŸŸé€‚åº”æ¨¡å‹è®­ç»ƒç­‰æ–¹æ³•æ¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.19144">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d17b5d1322f7ccd7c2858663bfc54a69.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7761ef64a7c1a6643623c30b05d50d11.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc185735ef98496edd2bb7c6a18a3799.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bd1b3e8b0896462f58acc06f7e4bfeb6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f5d02e8c4edeb08999b92a717353c235.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c679d98fd69994ad627045ac796a28a7.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Efficient-Reasoning-for-LLMs-through-Speculative-Chain-of-Thought"><a href="#Efficient-Reasoning-for-LLMs-through-Speculative-Chain-of-Thought" class="headerlink" title="Efficient Reasoning for LLMs through Speculative Chain-of-Thought"></a>Efficient Reasoning for LLMs through Speculative Chain-of-Thought</h2><p><strong>Authors:Jikai Wang, Juntao Li, Lijun Wu, Min Zhang</strong></p>
<p>Large reasoning language models such as OpenAI-o1 and Deepseek-R1 have recently attracted widespread attention due to their impressive task-solving abilities. However, the enormous model size and the generation of lengthy thought chains introduce significant reasoning costs and response latency. Existing methods for efficient reasoning mainly focus on reducing the number of model parameters or shortening the chain-of-thought length. In this paper, we introduce Speculative Chain-of-Thought (SCoT), which reduces reasoning latency from another perspective by accelerated average reasoning speed through large and small model collaboration. SCoT conducts thought-level drafting using a lightweight draft model. Then it selects the best CoT draft and corrects the error cases with the target model. The proposed thinking behavior alignment improves the efficiency of drafting and the draft selection strategy maintains the prediction accuracy for complex problems. Experimental results on GSM8K, MATH, GaoKao, CollegeMath and Olympiad datasets show that SCoT reduces reasoning latency by 48%$\sim$66% for Deepseek-R1-Distill-Qwen-32B while achieving near-target-model-level performance. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/Jikai0Wang/Speculative_CoT">https://github.com/Jikai0Wang/Speculative_CoT</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼ŒOpenAI-o1å’ŒDeepseek-R1ç­‰å¤§å‹æ¨ç†è¯­è¨€æ¨¡å‹å› å…¶ä»¤äººå°è±¡æ·±åˆ»çš„ä»»åŠ¡è§£å†³èƒ½åŠ›è€Œå¼•èµ·äº†å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œåºå¤§çš„æ¨¡å‹è§„æ¨¡å’Œäº§ç”Ÿå†—é•¿çš„æ€ç»´é“¾ï¼Œå¯¼è‡´æ¨ç†æˆæœ¬è¾ƒé«˜å’Œå“åº”å»¶è¿Ÿã€‚ç°æœ‰çš„é«˜æ•ˆæ¨ç†æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨å‡å°‘æ¨¡å‹å‚æ•°æ•°é‡æˆ–ç¼©çŸ­æ€ç»´é“¾é•¿åº¦ä¸Šã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºå¤§æ¨¡å‹å’Œå°æ¨¡å‹åä½œçš„æ¨æµ‹æ€ç»´é“¾ï¼ˆSpeculative Chain-of-Thoughtï¼Œç®€ç§°SCoTï¼‰ã€‚SCoTä»å¦ä¸€ä¸ªè§’åº¦é€šè¿‡åŠ é€Ÿå¹³å‡æ¨ç†é€Ÿåº¦æ¥å‡å°‘æ¨ç†å»¶è¿Ÿã€‚SCoTä½¿ç”¨è½»é‡çº§è‰ç¨¿æ¨¡å‹è¿›è¡Œæ€ç»´å±‚æ¬¡çš„è‰ç¨¿æ’°å†™ï¼Œç„¶åé€‰æ‹©æœ€ä½³çš„CoTè‰ç¨¿å¹¶ä½¿ç”¨ç›®æ ‡æ¨¡å‹ä¿®æ­£é”™è¯¯æƒ…å†µã€‚æå‡ºçš„æ€è€ƒè¡Œä¸ºå¯¹é½æ–¹å¼æé«˜äº†è‰ç¨¿çš„æ•ˆç‡ï¼Œè€Œè‰ç¨¿é€‰æ‹©ç­–ç•¥åˆ™ä¿æŒäº†å¯¹å¤æ‚é—®é¢˜çš„é¢„æµ‹å‡†ç¡®æ€§ã€‚åœ¨GSM8Kã€MATHã€é«˜è€ƒã€å¤§å­¦æ•°å­¦å’Œå¥¥æ—åŒ¹å…‹æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒSCoTåœ¨Deepseek-R1-Distill-Qwen-32Bä¸Šå°†æ¨ç†å»¶è¿Ÿé™ä½äº†48%~66%ï¼ŒåŒæ—¶å®ç°äº†æ¥è¿‘ç›®æ ‡æ¨¡å‹çº§åˆ«çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Jikai0Wang/Speculative_CoT%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Jikai0Wang/Speculative_CoTæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.19095v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§å‹æ¨ç†è¯­è¨€æ¨¡å‹å¦‚OpenAI-o1å’ŒDeepseek-R1å¤‡å—å…³æ³¨ï¼Œä½†å…¶å·¨å¤§çš„æ¨¡å‹è§„æ¨¡å’Œå¤æ‚çš„æ€ç»´é“¾ç”Ÿæˆå¸¦æ¥é«˜æ˜‚çš„æ¨ç†æˆæœ¬å’Œå“åº”å»¶è¿Ÿã€‚ç°æœ‰æé«˜æ¨ç†æ•ˆç‡çš„æ–¹æ³•ä¸»è¦å…³æ³¨å‡å°‘æ¨¡å‹å‚æ•°æˆ–ç¼©çŸ­æ€ç»´é“¾é•¿åº¦ã€‚æœ¬æ–‡æå‡ºSpeculative Chain-of-Thoughtï¼ˆSCoTï¼‰ï¼Œé€šè¿‡å¤§å°æ¨¡å‹åä½œåŠ é€Ÿå¹³å‡æ¨ç†é€Ÿåº¦ï¼Œä»å¦ä¸€ä¸ªè§’åº¦é™ä½æ¨ç†å»¶è¿Ÿã€‚SCoTåˆ©ç”¨è½»é‡çº§è‰ç¨¿æ¨¡å‹è¿›è¡Œæ€ç»´å±‚æ¬¡çš„è‰ç¨¿ï¼Œç„¶åé€‰æ‹©æœ€ä½³çš„CoTè‰ç¨¿å¹¶ç”¨ç›®æ ‡æ¨¡å‹ä¿®æ­£é”™è¯¯æƒ…å†µã€‚è¿™ç§æ€è€ƒè¡Œä¸ºå¯¹é½æ–¹å¼æé«˜äº†è‰ç¨¿çš„æ•ˆç‡ï¼Œè€Œè‰ç¨¿é€‰æ‹©ç­–ç•¥åˆ™ä¿æŒäº†å¯¹å¤æ‚é—®é¢˜çš„é¢„æµ‹å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSCoTåœ¨GSM8Kã€MATHã€é«˜è€ƒã€å¤§å­¦æ•°å­¦å’Œå¥¥æ—åŒ¹å…‹æ•°æ®é›†ä¸Šï¼Œå°†Deepseek-R1-Distill-Qwen-32Bçš„æ¨ç†å»¶è¿Ÿé™ä½äº†48%~66%ï¼ŒåŒæ—¶å®ç°äº†æ¥è¿‘ç›®æ ‡æ¨¡å‹çº§åˆ«çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹æ¨ç†è¯­è¨€æ¨¡å‹é¢ä¸´æ¨ç†æˆæœ¬å’Œå“åº”å»¶è¿Ÿçš„é—®é¢˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨å‡å°‘æ¨¡å‹å‚æ•°æˆ–ç¼©çŸ­æ€ç»´é“¾é•¿åº¦ä»¥æé«˜æ¨ç†æ•ˆç‡ã€‚</li>
<li>æœ¬æ–‡æå‡ºçš„Speculative Chain-of-Thoughtï¼ˆSCoTï¼‰é€šè¿‡å¤§å°æ¨¡å‹åä½œæ¥åŠ é€Ÿå¹³å‡æ¨ç†é€Ÿåº¦ã€‚</li>
<li>SCoTåˆ©ç”¨è½»é‡çº§è‰ç¨¿æ¨¡å‹è¿›è¡Œæ€ç»´å±‚æ¬¡çš„è‰ç¨¿ï¼Œå¹¶é€‰æ‹©æœ€ä½³è‰ç¨¿æ¥æé«˜æ•ˆç‡ã€‚</li>
<li>SCoTé€šè¿‡ç›®æ ‡æ¨¡å‹ä¿®æ­£é”™è¯¯ï¼Œå®ç°æ€è€ƒè¡Œä¸ºå¯¹é½å’Œè‰ç¨¿é€‰æ‹©ç­–ç•¥ï¼Œä¿æŒå¯¹å¤æ‚é—®é¢˜çš„é¢„æµ‹å‡†ç¡®æ€§ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒSCoTåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šæ˜¾è‘—é™ä½äº†æ¨ç†å»¶è¿Ÿï¼Œå¹¶å®ç°äº†æ¥è¿‘ç›®æ ‡æ¨¡å‹çº§åˆ«çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.19095">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0a1521b177f00da21c47639d6f7e6e9f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6869038442755bfffcbecacf5d08d08d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cac6b0b2c3b23d44f9fb45fd0f426518.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d750ea9bc62226788ebde4661c9cf01b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d20fb5d57d55d9df1425931fc2e47b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-85e5325a1dcee7c967f3adf7d57525d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76a3b0e9c28d603de858c41f6ca27532.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f6eb62e88b440f286e7345632aba10c.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="CipherBank-Exploring-the-Boundary-of-LLM-Reasoning-Capabilities-through-Cryptography-Challenges"><a href="#CipherBank-Exploring-the-Boundary-of-LLM-Reasoning-Capabilities-through-Cryptography-Challenges" class="headerlink" title="CipherBank: Exploring the Boundary of LLM Reasoning Capabilities through   Cryptography Challenges"></a>CipherBank: Exploring the Boundary of LLM Reasoning Capabilities through   Cryptography Challenges</h2><p><strong>Authors:Yu Li, Qizhi Pei, Mengyuan Sun, Honglin Lin, Chenlin Ming, Xin Gao, Jiang Wu, Conghui He, Lijun Wu</strong></p>
<p>Large language models (LLMs) have demonstrated remarkable capabilities, especially the recent advancements in reasoning, such as o1 and o3, pushing the boundaries of AI. Despite these impressive achievements in mathematics and coding, the reasoning abilities of LLMs in domains requiring cryptographic expertise remain underexplored. In this paper, we introduce CipherBank, a comprehensive benchmark designed to evaluate the reasoning capabilities of LLMs in cryptographic decryption tasks. CipherBank comprises 2,358 meticulously crafted problems, covering 262 unique plaintexts across 5 domains and 14 subdomains, with a focus on privacy-sensitive and real-world scenarios that necessitate encryption. From a cryptographic perspective, CipherBank incorporates 3 major categories of encryption methods, spanning 9 distinct algorithms, ranging from classical ciphers to custom cryptographic techniques. We evaluate state-of-the-art LLMs on CipherBank, e.g., GPT-4o, DeepSeek-V3, and cutting-edge reasoning-focused models such as o1 and DeepSeek-R1. Our results reveal significant gaps in reasoning abilities not only between general-purpose chat LLMs and reasoning-focused LLMs but also in the performance of current reasoning-focused models when applied to classical cryptographic decryption tasks, highlighting the challenges these models face in understanding and manipulating encrypted data. Through detailed analysis and error investigations, we provide several key observations that shed light on the limitations and potential improvement areas for LLMs in cryptographic reasoning. These findings underscore the need for continuous advancements in LLM reasoning capabilities. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»å±•ç°å‡ºä»¤äººç©ç›®çš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯æœ€è¿‘åœ¨æ¨ç†æ–¹é¢çš„è¿›å±•ï¼Œå¦‚o1å’Œo3ï¼Œæ¨åŠ¨äº†äººå·¥æ™ºèƒ½çš„è¾¹ç•Œã€‚å°½ç®¡åœ¨æ•°å­¦å’Œç¼–ç æ–¹é¢å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„æˆå°±ï¼Œä½†åœ¨éœ€è¦å¯†ç å­¦ä¸“ä¸šçŸ¥è¯†é¢†åŸŸçš„æ¨ç†èƒ½åŠ›ä»ç„¶è¢«ä½ä¼°ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†CipherBankï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°LLMåœ¨å¯†ç è§£å¯†ä»»åŠ¡ä¸­çš„æ¨ç†èƒ½åŠ›çš„ç»¼åˆåŸºå‡†æµ‹è¯•ã€‚CipherBankåŒ…å«2358ä¸ªç²¾å¿ƒè®¾è®¡çš„å¯†ç é—®é¢˜ï¼Œæ¶µç›–äº”ä¸ªé¢†åŸŸå’Œåå››ä¸ªå­é¢†åŸŸçš„262ä¸ªç‹¬ç‰¹æ˜æ–‡ï¼Œé‡ç‚¹å…³æ³¨éœ€è¦åŠ å¯†çš„éšç§æ•æ„Ÿå’Œç°å®ä¸–ç•Œåœºæ™¯ã€‚ä»å¯†ç å­¦çš„è§’åº¦æ¥çœ‹ï¼ŒCipherBankç»“åˆäº†ä¸‰ç§ä¸»è¦çš„åŠ å¯†æ–¹æ³•ï¼Œæ¶µç›–ä¹ç§ä¸åŒçš„ç®—æ³•ï¼Œä»å¤å…¸å¯†ç åˆ°è‡ªå®šä¹‰å¯†ç æŠ€æœ¯ã€‚æˆ‘ä»¬åœ¨CipherBankä¸Šè¯„ä¼°äº†æœ€å…ˆè¿›çš„LLMï¼Œå¦‚GPT-4oã€DeepSeek-V3ä»¥åŠä¸“æ³¨äºæ¨ç†çš„æ¨¡å‹o1å’ŒDeepSeek-R1ç­‰ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œä¸ä»…åœ¨é€šç”¨èŠå¤©LLMå’Œä¸“æ³¨äºæ¨ç†çš„LLMä¹‹é—´å­˜åœ¨æ¨ç†èƒ½åŠ›çš„å·¨å¤§å·®è·ï¼Œè€Œä¸”åœ¨å°†å½“å‰ä¸“æ³¨äºæ¨ç†çš„æ¨¡å‹åº”ç”¨äºå¤å…¸å¯†ç è§£å¯†ä»»åŠ¡æ—¶ä¹Ÿå­˜åœ¨æ€§èƒ½å·®è·ï¼Œè¿™çªæ˜¾äº†è¿™äº›æ¨¡å‹åœ¨ç†è§£å’Œæ“ä½œåŠ å¯†æ•°æ®æ—¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚é€šè¿‡è¯¦ç»†åˆ†æå’Œé”™è¯¯è°ƒæŸ¥ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€äº›å…³é”®è§‚å¯Ÿç»“æœï¼Œè¿™äº›ç»“æœæ­ç¤ºäº†LLMåœ¨å¯†ç å­¦æ¨ç†æ–¹é¢çš„å±€é™æ€§å’Œæ½œåœ¨çš„æ”¹è¿›æ–¹å‘ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†LLMæ¨ç†èƒ½åŠ›æŒç»­è¿›æ­¥çš„å¿…è¦æ€§å’Œé‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.19093v1">PDF</a> Work in progress</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†èƒ½åŠ›æ–¹é¢å±•ç°å‡ºæ˜¾è‘—çš„ä¼˜åŠ¿ï¼Œç‰¹åˆ«æ˜¯åœ¨o1å’Œo3ç­‰é¢†åŸŸçš„è¿›æ­¥æ¨åŠ¨äº†äººå·¥æ™ºèƒ½çš„è¾¹ç•Œã€‚å°½ç®¡åœ¨æ•°å­¦å’Œç¼–ç æ–¹é¢å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„æˆå°±ï¼Œä½†åœ¨éœ€è¦åŠ å¯†ä¸“ä¸šçŸ¥è¯†çš„é¢†åŸŸä¸­ï¼ŒLLMsçš„æ¨ç†èƒ½åŠ›ä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºCipherBankçš„ç»¼åˆæ€§åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°LLMsåœ¨åŠ å¯†è§£å¯†ä»»åŠ¡ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚CipherBankåŒ…å«2,358ä¸ªç²¾å¿ƒæ„å»ºçš„é—®é¢˜ï¼Œæ¶µç›–262ä¸ªå”¯ä¸€æ˜æ–‡ï¼Œæ¶‰åŠ5ä¸ªé¢†åŸŸå’Œ14ä¸ªå­é¢†åŸŸï¼Œé‡ç‚¹å…³æ³¨éœ€è¦åŠ å¯†çš„éšç§æ•æ„Ÿå’Œç°å®ä¸–ç•Œåœºæ™¯ã€‚ä»åŠ å¯†çš„è§’åº¦æ¥çœ‹ï¼ŒCipherBankçº³å…¥äº†ä¸‰å¤§åŠ å¯†æ–¹æ³•ï¼Œæ¶µç›–9ç§ä¸åŒçš„ç®—æ³•ï¼Œä»ç»å…¸å¯†ç åˆ°è‡ªå®šä¹‰åŠ å¯†æŠ€æœ¯ä¸ç­‰ã€‚æˆ‘ä»¬è¯„ä¼°äº†æœ€æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚GPT-4oã€DeepSeek-V3ä»¥åŠé¢å‘æ¨ç†çš„æ¨¡å‹o1å’ŒDeepSeek-R1ï¼‰åœ¨CipherBankä¸Šçš„è¡¨ç°ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œä¸ä»…åœ¨é€šç”¨èŠå¤©LLMå’Œé¢å‘æ¨ç†çš„LLMä¹‹é—´å­˜åœ¨æ¨ç†èƒ½åŠ›çš„å·®è·ï¼Œè€Œä¸”åœ¨å°†å½“å‰é¢å‘æ¨ç†çš„æ¨¡å‹åº”ç”¨äºç»å…¸åŠ å¯†è§£å¯†ä»»åŠ¡æ—¶ï¼Œå…¶æ€§èƒ½ä¹Ÿè¡¨ç°å‡ºæ˜¾è‘—ä¸è¶³ï¼Œè¿™è¡¨æ˜è¿™äº›æ¨¡å‹åœ¨ç†è§£å’Œæ“ä½œåŠ å¯†æ•°æ®æ–¹é¢é¢ä¸´ç€æŒ‘æˆ˜ã€‚é€šè¿‡è¯¦ç»†åˆ†æå’Œé”™è¯¯è°ƒæŸ¥ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€äº›å…³é”®è§‚å¯Ÿç»“æœï¼Œæ­ç¤ºäº†LLMsåœ¨åŠ å¯†æ¨ç†æ–¹é¢çš„å±€é™æ€§å’Œæ½œåœ¨æ”¹è¿›æ–¹å‘ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†LLMæ¨ç†èƒ½åŠ›æŒç»­å‘å±•çš„å¿…è¦æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†æ–¹é¢å±•ç°å‡ºæ˜¾è‘—çš„ä¼˜åŠ¿ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°å­¦å’Œç¼–ç é¢†åŸŸã€‚</li>
<li>åœ¨éœ€è¦åŠ å¯†ä¸“ä¸šçŸ¥è¯†çš„é¢†åŸŸä¸­ï¼ŒLLMsçš„æ¨ç†èƒ½åŠ›ä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ã€‚</li>
<li>CipherBankåŸºå‡†æµ‹è¯•ç”¨äºè¯„ä¼°LLMsåœ¨åŠ å¯†è§£å¯†ä»»åŠ¡ä¸­çš„æ¨ç†èƒ½åŠ›ï¼ŒåŒ…å«å¤šç§ç²¾å¿ƒæ„å»ºçš„é—®é¢˜ï¼Œè¦†ç›–å¤šä¸ªé¢†åŸŸå’Œç®—æ³•ã€‚</li>
<li>è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œç°æœ‰LLMsåœ¨ç»å…¸åŠ å¯†è§£å¯†ä»»åŠ¡ä¸­çš„æ€§èƒ½å­˜åœ¨æ˜¾è‘—å·®è·å’Œä¸è¶³ã€‚</li>
<li>åˆ†ææ­ç¤ºäº†LLMsåœ¨ç†è§£å’Œæ“ä½œåŠ å¯†æ•°æ®æ–¹é¢çš„å±€é™æ€§ã€‚</li>
<li>éœ€è¦æŒç»­æ”¹è¿›å’Œå‘å±•LLMçš„æ¨ç†èƒ½åŠ›ï¼Œä»¥åº”å¯¹åŠ å¯†é¢†åŸŸçš„æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.19093">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e2c9cd75224b8fd298e62ca3a486a1bd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c07d4c89e3dc9466fd8aea1a0022e950.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fa13c032ed414b097ab0f36155098a66.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d2f7ef6cb2bc3a072477d1dff13c170d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e49c245ee7845fee9674ccfec08952f7.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="MediAug-Exploring-Visual-Augmentation-in-Medical-Imaging"><a href="#MediAug-Exploring-Visual-Augmentation-in-Medical-Imaging" class="headerlink" title="MediAug: Exploring Visual Augmentation in Medical Imaging"></a>MediAug: Exploring Visual Augmentation in Medical Imaging</h2><p><strong>Authors:Xuyin Qi, Zeyu Zhang, Canxuan Gang, Hao Zhang, Lei Zhang, Zhiwei Zhang, Yang Zhao</strong></p>
<p>Data augmentation is essential in medical imaging for improving classification accuracy, lesion detection, and organ segmentation under limited data conditions. However, two significant challenges remain. First, a pronounced domain gap between natural photographs and medical images can distort critical disease features. Second, augmentation studies in medical imaging are fragmented and limited to single tasks or architectures, leaving the benefits of advanced mix-based strategies unclear. To address these challenges, we propose a unified evaluation framework with six mix-based augmentation methods integrated with both convolutional and transformer backbones on brain tumour MRI and eye disease fundus datasets. Our contributions are threefold. (1) We introduce MediAug, a comprehensive and reproducible benchmark for advanced data augmentation in medical imaging. (2) We systematically evaluate MixUp, YOCO, CropMix, CutMix, AugMix, and SnapMix with ResNet-50 and ViT-B backbones. (3) We demonstrate through extensive experiments that MixUp yields the greatest improvement on the brain tumor classification task for ResNet-50 with 79.19% accuracy and SnapMix yields the greatest improvement for ViT-B with 99.44% accuracy, and that YOCO yields the greatest improvement on the eye disease classification task for ResNet-50 with 91.60% accuracy and CutMix yields the greatest improvement for ViT-B with 97.94% accuracy. Code will be available at <a target="_blank" rel="noopener" href="https://github.com/AIGeeksGroup/MediAug">https://github.com/AIGeeksGroup/MediAug</a>. </p>
<blockquote>
<p>æ•°æ®å¢å¼ºåœ¨åŒ»å­¦æˆåƒä¸­è‡³å…³é‡è¦ï¼Œæœ‰åŠ©äºæé«˜åˆ†ç±»ç²¾åº¦ã€ç—…å˜æ£€æµ‹å’Œåœ¨æœ‰é™æ•°æ®æ¡ä»¶ä¸‹çš„å™¨å®˜åˆ†å‰²ã€‚ç„¶è€Œï¼Œä»å­˜åœ¨ä¸¤ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚é¦–å…ˆï¼Œè‡ªç„¶ç…§ç‰‡å’ŒåŒ»å­¦å›¾åƒä¹‹é—´æ˜æ˜¾çš„é¢†åŸŸå·®è·å¯èƒ½ä¼šæ‰­æ›²å…³é”®ç–¾ç—…ç‰¹å¾ã€‚å…¶æ¬¡ï¼ŒåŒ»å­¦æˆåƒä¸­çš„å¢å¼ºç ”ç©¶é›¶æ•£ä¸”ä»…é™äºå•ä¸€ä»»åŠ¡æˆ–æ¶æ„ï¼Œä½¿å¾—æ··åˆç­–ç•¥çš„ä¼˜åŠ¿å°šä¸æ¸…æ¥šã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„è¯„ä¼°æ¡†æ¶ï¼Œé›†æˆäº†å…­ç§åŸºäºæ··åˆçš„å¢å¼ºæ–¹æ³•ï¼Œå¹¶ç»“åˆå·ç§¯å’Œtransformeréª¨å¹²ç½‘ï¼Œåœ¨è„‘è‚¿ç˜¤MRIå’Œçœ¼åº•ç–¾ç—…æ•°æ®é›†ä¸Šè¿›è¡Œåº”ç”¨ã€‚æˆ‘ä»¬çš„è´¡çŒ®æœ‰ä¸‰ç‚¹ã€‚ï¼ˆ1ï¼‰æˆ‘ä»¬å¼•å…¥äº†MediAugï¼Œè¿™æ˜¯åŒ»å­¦æˆåƒä¸­é«˜çº§æ•°æ®å¢å¼ºçš„å…¨é¢å¯é‡ç°åŸºå‡†ã€‚ ï¼ˆ2ï¼‰æˆ‘ä»¬ç³»ç»Ÿåœ°è¯„ä¼°äº†MixUpã€YOCOã€CropMixã€CutMixã€AugMixå’ŒSnapMixä¸ResNet-50å’ŒViT-Béª¨å¹²ç½‘çš„æ•ˆæœã€‚ï¼ˆ3ï¼‰æˆ‘ä»¬é€šè¿‡å¤§é‡å®éªŒè¯æ˜ï¼ŒMixUpåœ¨ResNet-50çš„è„‘è‚¿ç˜¤åˆ†ç±»ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å¤§çš„æ”¹è¿›ï¼Œå‡†ç¡®ç‡ä¸º79.19%ï¼ŒSnapMixåœ¨ViT-Bä¸Šå–å¾—äº†æœ€å¤§çš„æ”¹è¿›ï¼Œå‡†ç¡®ç‡ä¸º99.44%ï¼›YOCOåœ¨ResNet-50çš„çœ¼ç—…åˆ†ç±»ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å¤§æ”¹è¿›ï¼Œå‡†ç¡®ç‡ä¸º91.60%ï¼ŒCutMixåœ¨ViT-Bä¸Šå–å¾—äº†æœ€å¤§çš„æ”¹è¿›ï¼Œå‡†ç¡®ç‡ä¸º97.94%ã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/AIGeeksGroup/MediAug">https://github.com/AIGeeksGroup/MediAug</a>ä¸Šæä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.18983v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æ•°æ®å¢å¼ºåœ¨åŒ»å­¦æˆåƒä¸­å¯¹äºæé«˜åˆ†ç±»ç²¾åº¦ã€ç—…å˜æ£€æµ‹ä»¥åŠå™¨å®˜åˆ†å‰²åœ¨æœ‰é™æ•°æ®æ¡ä»¶ä¸‹çš„æ€§èƒ½è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œä»å­˜åœ¨ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šä¸€æ˜¯è‡ªç„¶ç…§ç‰‡ä¸åŒ»å­¦å›¾åƒä¹‹é—´çš„æ˜æ˜¾åŸŸå·®è·å¯èƒ½å¯¼è‡´ç–¾ç—…ç‰¹å¾å¤±çœŸï¼›äºŒæ˜¯åŒ»å­¦æˆåƒä¸­çš„å¢å¼ºç ”ç©¶é›¶æ•£ä¸”ä»…é™äºå•ä¸€ä»»åŠ¡æˆ–æ¶æ„ï¼Œä½¿å¾—æ··åˆç­–ç•¥çš„ä¼˜åŠ¿ä¸æ˜æœ—ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„è¯„ä¼°æ¡†æ¶ï¼Œé›†æˆäº†å…­ç§æ··åˆå¢å¼ºæ–¹æ³•ï¼Œå¹¶ç»“åˆå·ç§¯å’Œtransformeréª¨å¹²ç½‘åœ¨è„‘è‚¿ç˜¤MRIå’Œçœ¼åº•ç–¾ç—…æ•°æ®é›†ä¸Šè¿›è¡Œå®éªŒã€‚æˆ‘ä»¬å¼•å…¥äº†MediAugï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºåŒ»å­¦æˆåƒçš„é«˜çº§æ•°æ®å¢å¼ºçš„ç»¼åˆå¯é‡å¤åŸºå‡†æµ‹è¯•ã€‚é€šè¿‡ç³»ç»Ÿè¯„ä¼°å…­ç§æ··åˆæ–¹æ³•ï¼ˆMixUpã€YOCOã€CropMixã€CutMixã€AugMixå’ŒSnapMixï¼‰ä¸ResNet-50å’ŒViT-Béª¨å¹²ç½‘ç»“åˆï¼Œæˆ‘ä»¬å‘ç°åœ¨è„‘è‚¿ç˜¤åˆ†ç±»ä»»åŠ¡ä¸­ï¼ŒMixUpå¯¹ResNet-50çš„å‡†ç¡®åº¦æé«˜æœ€å¤§ï¼Œè¾¾åˆ°79.19%ï¼›åœ¨çœ¼åº•ç–¾ç—…åˆ†ç±»ä»»åŠ¡ä¸­ï¼ŒYOCOå¯¹ResNet-50çš„å‡†ç¡®åº¦æé«˜æœ€å¤§ï¼Œè¾¾åˆ°91.60%ã€‚ä»£ç å°†å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/AIGeeksGroup/MediAug%E3%80%82">https://github.com/AIGeeksGroup/MediAugã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ•°æ®å¢å¼ºåœ¨åŒ»å­¦æˆåƒä¸­å¯¹äºæé«˜åˆ†ç±»ç²¾åº¦ã€ç—…å˜æ£€æµ‹å’Œå™¨å®˜åˆ†å‰²è‡³å…³é‡è¦ï¼Œå°¤å…¶åœ¨æœ‰é™æ•°æ®æ¡ä»¶ä¸‹ã€‚</li>
<li>å½“å‰é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šè‡ªç„¶ç…§ç‰‡ä¸åŒ»å­¦å›¾åƒé—´çš„åŸŸå·®è·å¯¼è‡´çš„ç–¾ç—…ç‰¹å¾å¤±çœŸï¼Œä»¥åŠç¢ç‰‡åŒ–ç ”ç©¶ä½¿å¾—æ··åˆå¢å¼ºç­–ç•¥çš„ä¼˜åŠ¿ä¸æ˜ç¡®ã€‚</li>
<li>æå‡ºä¸€ä¸ªç»Ÿä¸€çš„è¯„ä¼°æ¡†æ¶æ¥è¯„ä¼°å…­ç§æ··åˆå¢å¼ºæ–¹æ³•ï¼ˆMixUpã€YOCOç­‰ï¼‰ï¼Œç»“åˆå·ç§¯å’Œtransformeréª¨å¹²ç½‘è¿›è¡Œå®éªŒã€‚</li>
<li>å¼•å…¥MediAugä½œä¸ºåŒ»å­¦æˆåƒä¸­çš„é«˜çº§æ•°æ®å¢å¼ºç»¼åˆåŸºå‡†æµ‹è¯•ã€‚</li>
<li>MixUpåœ¨è„‘è‚¿ç˜¤åˆ†ç±»ä»»åŠ¡ä¸­å¯¹ResNet-50çš„å‡†ç¡®åº¦æé«˜æœ€å¤§ï¼Œè¾¾åˆ°79.19%ã€‚</li>
<li>YOCOåœ¨çœ¼åº•ç–¾ç—…åˆ†ç±»ä»»åŠ¡ä¸­å¯¹ResNet-50çš„å‡†ç¡®åº¦æé«˜æœ€å¤§ï¼Œè¾¾åˆ°91.60%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.18983">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a1ff6ee08ab3b5add9f47db7965ec06a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c75628b833a1a6bc4bc181cfbefbdcb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bd328dbb43cecaafe4eda12add447570.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-855ac7e35d84d38f272608bb848e4b89.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Transformer-Empowered-Actor-Critic-Reinforcement-Learning-for-Sequence-Aware-Service-Function-Chain-Partitioning"><a href="#Transformer-Empowered-Actor-Critic-Reinforcement-Learning-for-Sequence-Aware-Service-Function-Chain-Partitioning" class="headerlink" title="Transformer-Empowered Actor-Critic Reinforcement Learning for   Sequence-Aware Service Function Chain Partitioning"></a>Transformer-Empowered Actor-Critic Reinforcement Learning for   Sequence-Aware Service Function Chain Partitioning</h2><p><strong>Authors:Cyril Shih-Huan Hsu, Anestis Dalgkitsis, Chrysa Papagianni, Paola Grosso</strong></p>
<p>In the forthcoming era of 6G networks, characterized by unprecedented data rates, ultra-low latency, and extensive connectivity, effective management of Virtualized Network Functions (VNFs) is essential. VNFs are software-based counterparts of traditional hardware devices that facilitate flexible and scalable service provisioning. Service Function Chains (SFCs), structured as ordered sequences of VNFs, are pivotal in orchestrating complex network services. Nevertheless, partitioning SFCs across multi-domain network infrastructures presents substantial challenges due to stringent latency constraints and limited resource availability. Conventional optimization-based methods typically exhibit low scalability, whereas existing data-driven approaches often fail to adequately balance computational efficiency with the capability to effectively account for dependencies inherent in SFCs. To overcome these limitations, we introduce a Transformer-empowered actor-critic framework specifically designed for sequence-aware SFC partitioning. By utilizing the self-attention mechanism, our approach effectively models complex inter-dependencies among VNFs, facilitating coordinated and parallelized decision-making processes. Additionally, we enhance training stability and convergence using $\epsilon$-LoPe exploration strategy as well as Asymptotic Return Normalization. Comprehensive simulation results demonstrate that the proposed methodology outperforms existing state-of-the-art solutions in terms of long-term acceptance rates, resource utilization efficiency, and scalability, while achieving rapid inference. This study not only advances intelligent network orchestration by delivering a scalable and robust solution for SFC partitioning within emerging 6G environments, but also bridging recent advancements in Large Language Models (LLMs) with the optimization of next-generation networks. </p>
<blockquote>
<p>åœ¨å³å°†åˆ°æ¥çš„6Gç½‘ç»œæ—¶ä»£ï¼Œä»¥å…¶å‰æ‰€æœªæœ‰çš„æ•°æ®é€Ÿç‡ã€è¶…ä½å»¶è¿Ÿå’Œå¹¿æ³›è¿æ¥æ€§ä¸ºç‰¹å¾ï¼Œå¯¹è™šæ‹ŸåŒ–ç½‘ç»œåŠŸèƒ½ï¼ˆVNFsï¼‰çš„æœ‰æ•ˆç®¡ç†è‡³å…³é‡è¦ã€‚VNFsæ˜¯ä¼ ç»Ÿç¡¬ä»¶è®¾å¤‡åŸºäºè½¯ä»¶çš„å¯¹åº”ç‰©ï¼Œæœ‰åŠ©äºçµæ´»ä¸”å¯æ‰©å±•çš„æœåŠ¡æä¾›ã€‚ä½œä¸ºVNFsæœ‰åºåºåˆ—çš„æœåŠ¡åŠŸèƒ½é“¾ï¼ˆSFCsï¼‰åœ¨åè°ƒå¤æ‚ç½‘ç»œæœåŠ¡æ–¹é¢è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œåœ¨å¤šåŸŸç½‘ç»œåŸºç¡€è®¾æ–½ä¸­åˆ’åˆ†SFCsé¢ä¸´ç€å·¨å¤§çš„æŒ‘æˆ˜ï¼Œå› ä¸ºå­˜åœ¨ä¸¥æ ¼çš„å»¶è¿Ÿçº¦æŸå’Œèµ„æºå¯ç”¨æ€§é™åˆ¶ã€‚ä¼ ç»Ÿçš„ä¼˜åŒ–æ–¹æ³•é€šå¸¸å¯æ‰©å±•æ€§è¾ƒä½ï¼Œè€Œç°æœ‰çš„æ•°æ®é©±åŠ¨æ–¹æ³•å¾€å¾€ä¸èƒ½åœ¨è®¡ç®—æ•ˆç‡å’ŒSFCså›ºæœ‰çš„ä¾èµ–æ€§çš„æœ‰æ•ˆè€ƒé‡ä¹‹é—´å–å¾—å¹³è¡¡ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç”±Transformerèµ‹èƒ½çš„Actor-Criticæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä¸“é—¨ç”¨äºåºåˆ—æ„ŸçŸ¥çš„SFCåˆ’åˆ†ã€‚é€šè¿‡åˆ©ç”¨è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æœ‰æ•ˆåœ°å»ºæ¨¡äº†VNFsä¹‹é—´çš„å¤æ‚ç›¸äº’ä¾èµ–æ€§ï¼Œä¿ƒè¿›äº†åè°ƒå’Œå¹¶è¡ŒåŒ–çš„å†³ç­–è¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä½¿ç”¨Îµ-LoPeæ¢ç´¢ç­–ç•¥å’Œæ¸è¿‘å›æŠ¥å½’ä¸€åŒ–å¢å¼ºäº†è®­ç»ƒçš„ç¨³å®šæ€§å’Œæ”¶æ•›æ€§ã€‚ç»¼åˆä»¿çœŸç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨é•¿æœŸæ¥å—ç‡ã€èµ„æºåˆ©ç”¨æ•ˆç‡å’Œå¯æ‰©å±•æ€§æ–¹é¢ä¼˜äºç°æœ‰çš„æœ€æ–°è§£å†³æ–¹æ¡ˆï¼ŒåŒæ—¶å®ç°äº†å¿«é€Ÿæ¨ç†ã€‚æœ¬ç ”ç©¶ä¸ä»…é€šè¿‡ä¸ºæ–°å…´6Gç¯å¢ƒä¸­çš„SFCåˆ’åˆ†æä¾›å¯ä¼¸ç¼©å’Œç¨³å¥çš„è§£å†³æ–¹æ¡ˆæ¥æ¨åŠ¨æ™ºèƒ½ç½‘ç»œç¼–æ’çš„å‘å±•ï¼Œè€Œä¸”è¿˜å°†æœ€æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸ä¸‹ä¸€ä»£ç½‘ç»œçš„ä¼˜åŒ–ç›¸ç»“åˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.18902v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>åœ¨å³å°†åˆ°æ¥çš„6Gç½‘ç»œæ—¶ä»£ï¼Œä»¥è™šæ‹Ÿç½‘ç»œåŠŸèƒ½ï¼ˆVNFsï¼‰çš„æœ‰æ•ˆç®¡ç†ä¸ºæ ¸å¿ƒï¼Œè½¯ä»¶åŒ–çš„VNFsæ›¿ä»£äº†ä¼ ç»Ÿçš„ç¡¬ä»¶è®¾å¤‡ä»¥å®ç°çµæ´»ä¸”å¯æ‰©å±•çš„æœåŠ¡ä¾›åº”ã€‚æœåŠ¡å‡½æ•°é“¾ï¼ˆSFCsï¼‰ä½œä¸ºä¸€ç³»åˆ—æœ‰åºçš„VNFsï¼Œåœ¨ç¼–æ’å¤æ‚çš„ç½‘ç»œæœåŠ¡ä¸­æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ã€‚ç„¶è€Œï¼Œåœ¨è·¨å¤šåŸŸç½‘ç»œåŸºç¡€è®¾æ–½çš„SFCåˆ†åŒºé¢ä¸´è¯¸å¤šæŒ‘æˆ˜ï¼Œå¦‚å»¶è¿Ÿçº¦æŸä¸¥æ ¼å’Œèµ„æºå¯ç”¨æ€§æœ‰é™ç­‰ã€‚ä¼ ç»Ÿä¼˜åŒ–æ–¹æ³•ç¼ºä¹å¯æ‰©å±•æ€§ï¼Œè€Œç°æœ‰çš„æ•°æ®é©±åŠ¨æ–¹æ³•åœ¨è®¡ç®—æ•ˆç‡å’ŒSFCå›ºæœ‰ä¾èµ–æ€§çš„å¹³è¡¡æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºTransformerçš„actor-criticæ¡†æ¶ï¼Œä¸“é—¨ç”¨äºåºåˆ—æ„ŸçŸ¥çš„SFCåˆ†åŒºã€‚é€šè¿‡åˆ©ç”¨è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°æ¨¡æ‹ŸVNFsä¹‹é—´çš„å¤æ‚ç›¸äº’ä¾èµ–æ€§ï¼Œå®ç°ååŒå’Œå¹³è¡Œçš„å†³ç­–è¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä½¿ç”¨Îµ-LoPeæ¢ç´¢ç­–ç•¥å’Œæ¸è¿‘å›æŠ¥å½’ä¸€åŒ–æ¥å¢å¼ºè®­ç»ƒç¨³å®šæ€§å’Œæ”¶æ•›æ€§ã€‚ä»¿çœŸç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é•¿æœŸæ¥å—ç‡ã€èµ„æºåˆ©ç”¨æ•ˆç‡å’Œå¯æ‰©å±•æ€§æ–¹é¢ä¼˜äºç°æœ‰è§£å†³æ–¹æ¡ˆï¼ŒåŒæ—¶å®ç°äº†å¿«é€Ÿæ¨ç†ã€‚æœ¬ç ”ç©¶ä¸ä»…ä¸ºæ–°å…´6Gç¯å¢ƒä¸­çš„SFCåˆ†åŒºæä¾›äº†å¯æ‰©å±•å’Œç¨³å¥çš„è§£å†³æ–¹æ¡ˆï¼Œè€Œä¸”é€šè¿‡å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æœ€æ–°è¿›å±•ä¸ä¸‹ä¸€ä»£ç½‘ç»œçš„ä¼˜åŒ–ç›¸ç»“åˆï¼Œæ¨åŠ¨äº†æ™ºèƒ½ç½‘ç»œç¼–æ’çš„å‘å±•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>6Gç½‘ç»œæ—¶ä»£å¯¹VNFsçš„æœ‰æ•ˆç®¡ç†è‡³å…³é‡è¦ï¼Œå®ƒä»¬åœ¨ç½‘ç»œæœåŠ¡ä¾›åº”ä¸­æ›¿ä»£äº†ä¼ ç»Ÿç¡¬ä»¶ã€‚</li>
<li>SFCsä½œä¸ºæœ‰åºçš„VNFåºåˆ—ï¼Œåœ¨å¤æ‚çš„ç½‘ç»œæœåŠ¡çš„ç¼–æ’ä¸­æ‰®æ¼”ç€æ ¸å¿ƒè§’è‰²ã€‚</li>
<li>SFCåˆ†åŒºåœ¨å¤šåŸŸç½‘ç»œåŸºç¡€è®¾æ–½ä¸­å­˜åœ¨æŒ‘æˆ˜ï¼Œå¦‚å»¶è¿Ÿçº¦æŸä¸¥æ ¼å’Œèµ„æºå¯ç”¨æ€§æœ‰é™ç­‰ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨è®¡ç®—æ•ˆç‡ä¸SFCä¾èµ–æ€§å¹³è¡¡æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºTransformerçš„actor-criticæ¡†æ¶ç”¨äºåºåˆ—æ„ŸçŸ¥çš„SFCåˆ†åŒºã€‚</li>
<li>åˆ©ç”¨è‡ªæ³¨æ„åŠ›æœºåˆ¶æ¨¡æ‹ŸVNFé—´çš„å¤æ‚ä¾èµ–æ€§å¹¶å®ç°ååŒå†³ç­–è¿‡ç¨‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.18902">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f11fce0ce06b2c2b706617f1ef396ccf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-35341051e3ba9107285835ebe30b26f2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-492ea3f1650fd5dcb128c647f76099ea.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Towards-Robust-Dialogue-Breakdown-Detection-Addressing-Disruptors-in-Large-Language-Models-with-Self-Guided-Reasoning"><a href="#Towards-Robust-Dialogue-Breakdown-Detection-Addressing-Disruptors-in-Large-Language-Models-with-Self-Guided-Reasoning" class="headerlink" title="Towards Robust Dialogue Breakdown Detection: Addressing Disruptors in   Large Language Models with Self-Guided Reasoning"></a>Towards Robust Dialogue Breakdown Detection: Addressing Disruptors in   Large Language Models with Self-Guided Reasoning</h2><p><strong>Authors:Abdellah Ghassel, Xianzhi Li, Xiaodan Zhu</strong></p>
<p>Large language models (LLMs) are rapidly changing various domains. However, their capabilities in handling conversational breakdowns still require an in-depth exploration. This paper addresses the challenge of detecting and mitigating dialogue breakdowns within LLM-driven conversational systems. While powerful models from OpenAI and Anthropic excel in many dialogue tasks, they can still produce incoherent or contradictory responses, commonly referred to as breakdowns, which undermine user trust. To tackle this, we propose an approach that combines specialized fine-tuning with advanced prompting strategies, including few-shot learning, chain-of-thought reasoning, and analogical prompting. In particular, we fine-tune a small 8B model and demonstrate its robust classification and calibration capabilities in English and Japanese dialogue. We also validate its generalization on the BETOLD dataset, achieving a 7% accuracy improvement over its base model. Furthermore, we introduce a real-time deployment architecture that selectively escalates suspicious responses to more resource-intensive frontier models only when breakdowns are detected, significantly cutting operational expenses and energy consumption. Experimental results show our method surpasses prior state-of-the-art specialized classifiers while also narrowing performance gaps between smaller open-source models and large proprietary ones. Our approach offers a scalable solution for robust conversational AI in high-impact domains by combining efficiency, interpretability, and reliability. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ­£åœ¨è¿…é€Ÿæ”¹å˜å„ä¸ªé¢†åŸŸã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨å¤„ç†å¯¹è¯ä¸­æ–­æ–¹é¢çš„èƒ½åŠ›ä»ç„¶éœ€è¦æ·±å…¥æ¢ç´¢ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨LLMé©±åŠ¨çš„å¯¹è¯ç³»ç»Ÿä¸­æ£€æµ‹å’Œç¼“è§£å¯¹è¯ä¸­æ–­çš„æŒ‘æˆ˜ã€‚è™½ç„¶OpenAIå’ŒAnthropicçš„å¼ºå¤§æ¨¡å‹åœ¨è®¸å¤šå¯¹è¯ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬ä»ç„¶å¯èƒ½äº§ç”Ÿä¸è¿è´¯æˆ–çŸ›ç›¾çš„å›åº”ï¼Œè¿™äº›å›åº”é€šå¸¸è¢«ç§°ä¸ºä¸­æ–­ï¼Œä¼šç ´åç”¨æˆ·ä¿¡ä»»ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œå®ƒå°†ä¸“é—¨çš„å¾®è°ƒä¸å…ˆè¿›çš„æç¤ºç­–ç•¥ç›¸ç»“åˆï¼ŒåŒ…æ‹¬å°‘æ ·æœ¬å­¦ä¹ ã€é“¾å¼æ€ç»´æ¨ç†å’Œç±»æ¯”æç¤ºã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬å¯¹ä¸€ä¸ªå°å‹çš„8Bæ¨¡å‹è¿›è¡Œäº†å¾®è°ƒï¼Œå¹¶å±•ç¤ºäº†å®ƒåœ¨è‹±è¯­å’Œæ—¥è¯­å¯¹è¯ä¸­çš„ç¨³å¥åˆ†ç±»å’Œæ ¡å‡†èƒ½åŠ›ã€‚æˆ‘ä»¬åœ¨BETOLDæ•°æ®é›†ä¸ŠéªŒè¯äº†å…¶æ³›åŒ–èƒ½åŠ›ï¼Œç›¸è¾ƒäºåŸºç¡€æ¨¡å‹å®ç°äº†7%çš„å‡†ç¡®ç‡æå‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å®æ—¶éƒ¨ç½²æ¶æ„ï¼Œè¯¥æ¶æ„ä»…åœ¨æ£€æµ‹åˆ°ä¸­æ–­æ—¶é€‰æ‹©å°†å¯ç–‘çš„å“åº”å‡çº§åˆ°æ›´è€—è´¹èµ„æºå’Œè®¡ç®—èµ„æºçš„å°–ç«¯æ¨¡å‹ï¼Œä»è€Œå¤§å¹…å‡å°‘è¿è¥å¼€æ”¯å’Œèƒ½æºæ¶ˆè€—ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›çš„åˆ†ç±»å™¨ï¼ŒåŒæ—¶ç¼©å°äº†å¼€æºå°å‹æ¨¡å‹å’Œå¤§å‹ä¸“æœ‰æ¨¡å‹ä¹‹é—´çš„æ€§èƒ½å·®è·ã€‚é€šè¿‡ç»“åˆæ•ˆç‡ã€å¯è§£é‡Šæ€§å’Œå¯é æ€§ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ºé«˜å½±å“åŠ›é¢†åŸŸæä¾›ç¨³å¥å¯¹è¯AIçš„å¯æ‰©å±•è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.18839v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šä¸ªé¢†åŸŸå±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†åœ¨å¤„ç†å¯¹è¯ä¸­æ–­æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºä¸€ç§ç»“åˆä¸“é¡¹å¾®è°ƒä¸é«˜çº§æç¤ºç­–ç•¥çš„æ–¹æ³•ï¼Œå¦‚å°æ ·æœ¬å­¦ä¹ ã€é“¾å¼æ€ç»´æ¨ç†å’Œç±»æ¯”æç¤ºï¼Œä»¥æ£€æµ‹å¹¶ç¼“è§£LLMé©±åŠ¨å¯¹è¯ç³»ç»Ÿä¸­çš„å¯¹è¯ä¸­æ–­é—®é¢˜ã€‚é€šè¿‡å¾®è°ƒä¸€ä¸ªå°å‹8Bæ¨¡å‹ï¼Œå¹¶åœ¨è‹±è¯­å’Œæ—¥è¯­å¯¹è¯ä¸­è¿›è¡Œæ¼”ç¤ºï¼ŒéªŒè¯äº†å…¶ç¨³å¥çš„åˆ†ç±»å’Œæ ¡å‡†èƒ½åŠ›ã€‚åŒæ—¶ï¼Œå¼•å…¥å®æ—¶éƒ¨ç½²æ¶æ„ï¼Œä»…åœ¨æ£€æµ‹åˆ°å¯¹è¯ä¸­æ–­æ—¶é€‰æ‹©å°†å¯ç–‘å“åº”å‡çº§åˆ°èµ„æºå¯†é›†å‹å‰æ²¿æ¨¡å‹ï¼Œå¤§å¤§é™ä½äº†æ“ä½œæˆæœ¬å’Œèƒ½æºæ¶ˆè€—ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•è¶…è¶Šäº†ç°æœ‰çš„ä¸“ä¸šåˆ†ç±»å™¨ï¼Œç¼©å°äº†å¼€æºå°å‹æ¨¡å‹ä¸å¤§å‹ä¸“æœ‰æ¨¡å‹ä¹‹é—´çš„æ€§èƒ½å·®è·ï¼Œä¸ºé«˜æ•ˆã€å¯è§£é‡Šå’Œå¯é çš„å¯¹è¯AIæä¾›äº†å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¯¹è¯ä¸­æ–­æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œéœ€è¦æ·±å…¥ç ”ç©¶ã€‚</li>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§ç»“åˆä¸“é¡¹å¾®è°ƒä¸é«˜çº§æç¤ºç­–ç•¥çš„æ–¹æ³•æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>é€šè¿‡å¾®è°ƒä¸€ä¸ªå°å‹8Bæ¨¡å‹ï¼Œè¯¥æ–¹æ³•åœ¨è‹±æ–‡å’Œæ—¥æ–‡å¯¹è¯ä¸­è¡¨ç°å‡ºç¨³å¥çš„åˆ†ç±»å’Œæ ¡å‡†èƒ½åŠ›ã€‚</li>
<li>å¼•å…¥çš„å®æ—¶éƒ¨ç½²æ¶æ„å¯æ˜¾è‘—é™ä½æ“ä½œæˆæœ¬å’Œèƒ½æºæ¶ˆè€—ã€‚</li>
<li>è¯¥æ–¹æ³•è¶…è¶Šäº†ç°æœ‰çš„ä¸“ä¸šåˆ†ç±»å™¨ï¼Œå¹¶ç¼©å°äº†ä¸åŒæ¨¡å‹ä¹‹é—´çš„æ€§èƒ½å·®è·ã€‚</li>
<li>è®ºæ–‡æå‡ºçš„è§£å†³æ–¹æ¡ˆä¸ºé«˜æ•ˆã€å¯è§£é‡Šå’Œå¯é çš„å¯¹è¯AIæä¾›äº†å¯æ‰©å±•çš„æ¡†æ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.18839">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7e0d3e9fd71889ccaed5ef5ff11547a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d26b3c8523cdadf3583f1aa9848b48f7.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="ThinkFL-Self-Refining-Failure-Localization-for-Microservice-Systems-via-Reinforcement-Fine-Tuning"><a href="#ThinkFL-Self-Refining-Failure-Localization-for-Microservice-Systems-via-Reinforcement-Fine-Tuning" class="headerlink" title="ThinkFL: Self-Refining Failure Localization for Microservice Systems via   Reinforcement Fine-Tuning"></a>ThinkFL: Self-Refining Failure Localization for Microservice Systems via   Reinforcement Fine-Tuning</h2><p><strong>Authors:Lingzhe Zhang, Yunpeng Zhai, Tong Jia, Chiming Duan, Siyu Yu, Jinyang Gao, Bolin Ding, Zhonghai Wu, Ying Li</strong></p>
<p>As modern microservice systems grow increasingly popular and complex-often consisting of hundreds or even thousands of fine-grained, interdependent components-they are becoming more susceptible to frequent and subtle failures. Ensuring system reliability therefore hinges on accurate and efficient failure localization. Traditional failure localization approaches based on small models lack the flexibility to adapt to diverse failure scenarios, while recent LLM-based methods suffer from two major limitations: they often rely on rigid invocation workflows that constrain the modelâ€™s ability to dynamically explore optimal localization paths, and they require resource-intensive inference, making them cost-prohibitive for real-world deployment. To address these challenges, we explore the use of reinforcement fine-tuning to equip lightweight LLMs with reasoning and self-refinement capabilities, significantly improving the cost-effectiveness and adaptability of LLM-based failure localization. We begin with an empirical study to identify three key capabilities essential for accurate localization. Building on these insights, we propose a progressive multi-stage GRPO fine-tuning framework, which integrates a multi-factor failure localization grader and a recursion-of-thought actor module. The resulting model, ThinkFL, not only outperforms existing state-of-the-art LLMs and baseline methods in localization accuracy but also reduces end-to-end localization latency from minutes to seconds, demonstrating strong potential for real-world applications. </p>
<blockquote>
<p>éšç€ç°ä»£å¾®æœåŠ¡ç³»ç»Ÿè¶Šæ¥è¶Šå—æ¬¢è¿å’Œå¤æ‚åŒ–â€”â€”é€šå¸¸ç”±æ•°ç™¾ä¸ªç”šè‡³æ•°åƒä¸ªç²¾ç»†ç²’åº¦ã€ç›¸äº’ä¾èµ–çš„ç»„ä»¶æ„æˆâ€”â€”å®ƒä»¬æ›´å®¹æ˜“å—åˆ°é¢‘ç¹å’Œå¾®å¦™çš„æ•…éšœå½±å“ã€‚å› æ­¤ï¼Œç¡®ä¿ç³»ç»Ÿå¯é æ€§å…³é”®åœ¨äºå‡†ç¡®é«˜æ•ˆçš„æ•…éšœå®šä½ã€‚åŸºäºå°å‹æ¨¡å‹çš„ä¼ ç»Ÿæ•…éšœå®šä½æ–¹æ³•ç¼ºä¹é€‚åº”å¤šç§æ•…éšœåœºæ™¯çš„çµæ´»æ€§ï¼Œè€Œæœ€è¿‘çš„åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–¹æ³•åˆ™å­˜åœ¨ä¸¤å¤§å±€é™ï¼šå®ƒä»¬é€šå¸¸ä¾èµ–äºåƒµåŒ–çš„è°ƒç”¨å·¥ä½œæµç¨‹ï¼Œé™åˆ¶æ¨¡å‹åŠ¨æ€æ¢ç´¢æœ€ä½³å®šä½è·¯å¾„çš„èƒ½åŠ›ï¼›å®ƒä»¬éœ€è¦èµ„æºå¯†é›†å‹çš„æ¨ç†ï¼Œä½¿å¾—åœ¨ç°å®ä¸–ç•Œéƒ¨ç½²ä¸­æˆæœ¬é«˜æ˜‚ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¢ç´¢ä½¿ç”¨å¼ºåŒ–å¾®è°ƒæ¥ä¸ºè½»é‡çº§LLMé…å¤‡æ¨ç†å’Œè‡ªç²¾ç‚¼èƒ½åŠ›ï¼Œæ˜¾è‘—æé«˜äº†åŸºäºLLMçš„æ•…éšœå®šä½çš„æˆæœ¬æ•ˆç›Šå’Œé€‚åº”æ€§ã€‚æˆ‘ä»¬é¦–å…ˆä»å®è¯ç ”ç©¶å‡ºå‘ï¼Œç¡®å®šäº†å®ç°å‡†ç¡®å®šä½æ‰€éœ€çš„ä¸‰ä¸ªå…³é”®èƒ½åŠ›ã€‚åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåˆ†é˜¶æ®µçš„GRPOå¾®è°ƒæ¡†æ¶ï¼Œå®ƒæ•´åˆäº†å¤šå› ç´ æ•…éšœå®šä½è¯„åˆ†å™¨å’Œé€’å½’æ€ç»´è¡ŒåŠ¨æ¨¡å—ã€‚ç”±æ­¤äº§ç”Ÿçš„ThinkFLæ¨¡å‹ä¸ä»…åœ¨å½“åœ°åŒ–å‡†ç¡®æ€§æ–¹é¢è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›çš„LLMå’ŒåŸºçº¿æ–¹æ³•ï¼Œè€Œä¸”å°†ç«¯åˆ°ç«¯çš„å®šä½å»¶è¿Ÿä»åˆ†é’Ÿå‡å°‘åˆ°ç§’ï¼Œæ˜¾ç¤ºå‡ºåœ¨å®é™…åº”ç”¨ä¸­å¼ºå¤§çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.18776v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>éšç€ç°ä»£å¾®æœåŠ¡ç³»ç»Ÿè¶Šæ¥è¶Šå—æ¬¢è¿ä¸”å˜å¾—å¤æ‚ï¼Œé¢‘ç¹å’Œå¾®å¦™çš„æ•…éšœå®šä½é—®é¢˜å˜å¾—å°¤ä¸ºå…³é”®ã€‚ä¼ ç»Ÿæ–¹æ³•ç¼ºä¹çµæ´»æ€§ï¼Œè€ŒåŸºäºLLMçš„æ–¹æ³•åˆ™é¢ä¸´èµ„æºå¯†é›†æ¨ç†çš„é—®é¢˜ã€‚æœ¬ç ”ç©¶é€šè¿‡å¼ºåŒ–å¾®è°ƒèµ‹äºˆè½»é‡çº§LLMæ¨ç†å’Œè‡ªæˆ‘ä¼˜åŒ–èƒ½åŠ›ï¼Œè§£å†³äº†è¿™äº›é—®é¢˜ã€‚ç ”ç©¶å§‹äºå®è¯ç ”ç©¶ï¼Œç¡®å®šäº†ä¸‰ä¸ªå…³é”®å®šä½èƒ½åŠ›ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šæå‡ºäº†ä¸€ä¸ªè¿›æ­¥å¼çš„å¤šé˜¶æ®µGRPOå¾®è°ƒæ¡†æ¶ThinkFLã€‚å®ƒæ•´åˆäº†å¤šå› ç´ æ•…éšœå®šä½è¯„åˆ†å™¨å’Œé€’å½’æ€ç»´æ¨¡å—ï¼Œä¸ä»…æé«˜äº†å®šä½å‡†ç¡®æ€§ï¼Œè¿˜å¤§å¤§ç¼©çŸ­äº†å®šä½å»¶è¿Ÿã€‚è¿™ä¸ºå®é™…åº”ç”¨å±•ç¤ºäº†å¼ºå¤§æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯è¯¥æ–‡æœ¬çš„å…³é”®è§è§£ï¼š</p>
<ol>
<li>éšç€ç°ä»£å¾®æœåŠ¡ç³»ç»Ÿçš„å¤æ‚æ€§å¢åŠ ï¼Œæ•…éšœå®šä½å˜å¾—è‡³å…³é‡è¦ã€‚éœ€è¦ä¸€ç§å‡†ç¡®è€Œæœ‰æ•ˆçš„æ–¹æ³•æ¥å¤„ç†è¿™ä¸ªæŒ‘æˆ˜ã€‚</li>
<li>ä¼ ç»Ÿå¤±è´¥å®šä½æ–¹æ³•åŸºäºå°å‹æ¨¡å‹ï¼Œç¼ºä¹é€‚åº”å¤šæ ·å¤±è´¥åœºæ™¯çš„çµæ´»æ€§ã€‚åŸºäºLLMçš„æ–¹æ³•è™½ç„¶åœ¨ç†è®ºä¸Šå¼ºå¤§ï¼Œä½†å­˜åœ¨é™åˆ¶æ€§é—®é¢˜ã€‚ä»–ä»¬ä¾èµ–äºåƒµåŒ–çš„è°ƒç”¨æµç¨‹é™åˆ¶äº†æ¨¡å‹çš„åŠ¨æ€æ¢ç´¢èƒ½åŠ›ï¼ŒåŒæ—¶éœ€è¦èµ„æºå¯†é›†æ¨ç†ã€‚è¿™é™åˆ¶äº†å®ƒä»¬åœ¨å®é™…éƒ¨ç½²ä¸­çš„å¯è¡Œæ€§ã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜æå‡ºäº†ç»“åˆå¼ºåŒ–å¾®è°ƒç­–ç•¥çš„è§£å†³æ–¹æ¡ˆæ¥ä¼˜åŒ–LLMåŠŸèƒ½æå‡ºäº†ä»¥ç®€åŒ–å¼ºåŒ–å­¦ä¹ ä¸ºæ ¸å¿ƒçš„æ¡†æ¶ä¸ºæ”¹è¿›æä¾›äº†ä¸€ä¸ªå®ç”¨çš„æ–¹æ³•åˆ©ç”¨æ¨¡å‹çš„æ¨ç†å’Œè‡ªæˆ‘ä¼˜åŒ–èƒ½åŠ›ä»¥å®ç°é«˜æ•ˆç²¾ç¡®çš„æ•…éšœå®šä½è¿™ä¸ä»…å¯ä»¥æé«˜æ¨¡å‹æ€§èƒ½è¿˜æ˜¾è‘—æé«˜äº†ç³»ç»Ÿçš„é€‚åº”æ€§å’Œæˆæœ¬æ•ˆç›Š</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.18776">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-74bf5637605d1a6f6f8912f1fdaf1d43.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee0ea70a875db937618271d0caa1b451.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="SORT3D-Spatial-Object-centric-Reasoning-Toolbox-for-Zero-Shot-3D-Grounding-Using-Large-Language-Models"><a href="#SORT3D-Spatial-Object-centric-Reasoning-Toolbox-for-Zero-Shot-3D-Grounding-Using-Large-Language-Models" class="headerlink" title="SORT3D: Spatial Object-centric Reasoning Toolbox for Zero-Shot 3D   Grounding Using Large Language Models"></a>SORT3D: Spatial Object-centric Reasoning Toolbox for Zero-Shot 3D   Grounding Using Large Language Models</h2><p><strong>Authors:Nader Zantout, Haochen Zhang, Pujith Kachana, Jinkai Qiu, Ji Zhang, Wenshan Wang</strong></p>
<p>Interpreting object-referential language and grounding objects in 3D with spatial relations and attributes is essential for robots operating alongside humans. However, this task is often challenging due to the diversity of scenes, large number of fine-grained objects, and complex free-form nature of language references. Furthermore, in the 3D domain, obtaining large amounts of natural language training data is difficult. Thus, it is important for methods to learn from little data and zero-shot generalize to new environments. To address these challenges, we propose SORT3D, an approach that utilizes rich object attributes from 2D data and merges a heuristics-based spatial reasoning toolbox with the ability of large language models (LLMs) to perform sequential reasoning. Importantly, our method does not require text-to-3D data for training and can be applied zero-shot to unseen environments. We show that SORT3D achieves state-of-the-art performance on complex view-dependent grounding tasks on two benchmarks. We also implement the pipeline to run real-time on an autonomous vehicle and demonstrate that our approach can be used for object-goal navigation on previously unseen real-world environments. All source code for the system pipeline is publicly released at <a target="_blank" rel="noopener" href="https://github.com/nzantout/SORT3D">https://github.com/nzantout/SORT3D</a> . </p>
<blockquote>
<p>è§£é‡Šå¯¹è±¡å‚ç…§è¯­è¨€å¹¶å°†å¯¹è±¡ä¸ä¸‰ç»´ç©ºé—´ä¸­çš„å…³ç³»å’Œå±æ€§ç›¸å¯¹åº”ï¼Œå¯¹äºä¸äººç±»åä½œçš„æœºå™¨äººæ¥è¯´è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç”±äºåœºæ™¯å¤šæ ·æ€§ã€ç²¾ç»†å¯¹è±¡çš„æ•°é‡ä¼—å¤šä»¥åŠè¯­è¨€å‚ç…§çš„è‡ªç”±å½¢å¼å¤æ‚ï¼Œè¿™é¡¹ä»»åŠ¡å¾€å¾€å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æ­¤å¤–ï¼Œåœ¨ä¸‰ç»´é¢†åŸŸï¼Œè·å–å¤§é‡è‡ªç„¶è¯­è¨€è®­ç»ƒæ•°æ®æ˜¯éå¸¸å›°éš¾çš„ã€‚å› æ­¤ï¼Œæ–¹æ³•éœ€è¦ä»å°é‡æ•°æ®ä¸­å­¦ä¹ å¹¶é›¶æ ·æœ¬æ¨å¹¿åˆ°æ–°çš„ç¯å¢ƒä¸­ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†SORT3Dæ–¹æ³•ï¼Œå®ƒåˆ©ç”¨äºŒç»´æ•°æ®ä¸­çš„ä¸°å¯Œå¯¹è±¡å±æ€§ï¼Œå¹¶å°†åŸºäºå¯å‘å¼ç®—æ³•çš„ç©ºé—´æ¨ç†å·¥å…·ç®±ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œåºåˆ—æ¨ç†çš„èƒ½åŠ›ç›¸ç»“åˆã€‚é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸éœ€è¦æ–‡æœ¬åˆ°ä¸‰ç»´æ•°æ®çš„è®­ç»ƒï¼Œå¹¶ä¸”å¯ä»¥é›¶æ ·æœ¬åº”ç”¨äºæœªè§è¿‡çš„ç¯å¢ƒã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¤æ‚è§†å›¾ç›¸å…³å®šä½ä»»åŠ¡ä¸Šå±•ç¤ºäº†SORT3Dçš„å“è¶Šæ€§èƒ½ã€‚æˆ‘ä»¬è¿˜å®ç°äº†å®æ—¶è¿è¡Œåœ¨è‡ªåŠ¨é©¾é©¶æ±½è½¦ä¸Šçš„ç®¡é“ï¼Œå¹¶è¯æ˜æˆ‘ä»¬çš„æ–¹æ³•å¯ç”¨äºä¹‹å‰åœ¨æœªè§è¿‡çš„çœŸå®ç¯å¢ƒä¸­çš„ç›®æ ‡å¯¼èˆªã€‚ç³»ç»Ÿç®¡é“çš„æ‰€æœ‰æºä»£ç å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/nzantout/SORT3D%E5%85%AC%E5%BC%80%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/nzantout/SORT3Då…¬å¼€å‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.18684v1">PDF</a> 7 pages, 6 figures, submitted to IROS 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºSORT3Dçš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†äºŒç»´æ•°æ®çš„ä¸°å¯Œå¯¹è±¡å±æ€§å’ŒåŸºäºå¯å‘å¼ç©ºé—´æ¨ç†å·¥å…·çš„èƒ½åŠ›ï¼Œä¸å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œåºåˆ—åŒ–æ¨ç†ç›¸ç»“åˆã€‚è¿™ç§æ–¹æ³•å¯ä»¥åœ¨æ— è®­ç»ƒæ–‡æœ¬åˆ°ä¸‰ç»´æ•°æ®çš„æƒ…å†µä¸‹è¿›è¡Œé›¶æ—¶å·®åº”ç”¨ï¼Œå¹¶åœ¨å¤æ‚çš„ç¯å¢ƒä¸­å®Œæˆç‰©ä½“å‚ç…§è¯­è¨€è§£é‡Šå’Œä¸‰ç»´ç©ºé—´ç‰©ä½“å®šä½ä»»åŠ¡ã€‚SORT3Dåœ¨ä¸¤é¡¹åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ï¼Œå¹¶ä¸”åœ¨å®é™…è‡ªåŠ¨é©¾é©¶è½¦è¾†ä¸Šè¿›è¡Œäº†å®æ—¶è¿è¡Œæµ‹è¯•ã€‚æ‰€æœ‰ç³»ç»Ÿç®¡é“æºä»£ç å‡å·²å…¬å¼€å‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>SORT3Dç»“åˆäº†äºŒç»´æ•°æ®çš„ä¸°å¯Œå¯¹è±¡å±æ€§å’Œå¯å‘å¼ç©ºé—´æ¨ç†å·¥å…·çš„èƒ½åŠ›ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”¨äºæ‰§è¡Œåºåˆ—åŒ–æ¨ç†ã€‚</li>
<li>æ–¹æ³•æ— éœ€æ–‡æœ¬åˆ°ä¸‰ç»´æ•°æ®çš„è®­ç»ƒï¼Œå¯é›¶æ—¶å·®åº”ç”¨äºæ–°ç¯å¢ƒã€‚</li>
<li>SORT3Dåœ¨å¤æ‚çš„ç¯å¢ƒä¸­å®Œæˆäº†ç‰©ä½“å‚ç…§è¯­è¨€è§£é‡Šå’Œä¸‰ç»´ç©ºé—´ç‰©ä½“å®šä½ä»»åŠ¡ã€‚</li>
<li>åœ¨ä¸¤é¡¹åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ã€‚</li>
<li>SORT3Då·²ç»åœ¨å®é™…è‡ªåŠ¨é©¾é©¶è½¦è¾†ä¸Šè¿›è¡Œäº†å®æ—¶è¿è¡Œæµ‹è¯•ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.18684">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-420d8d52b6aa431d8b1490897bb6e2b9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-22ac52c996b31219b9e8845a11607048.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4bc8932650172c042fbf6f85b725ca3a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-aab2b25f1b84c332c9ea4bfee0b25423.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6b8eebeabd4bd9578aa15256523c0fec.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Proof-of-TBI-â€“-Fine-Tuned-Vision-Language-Model-Consortium-and-OpenAI-o3-Reasoning-LLM-Based-Medical-Diagnosis-Support-System-for-Mild-Traumatic-Brain-Injury-TBI-Prediction"><a href="#Proof-of-TBI-â€“-Fine-Tuned-Vision-Language-Model-Consortium-and-OpenAI-o3-Reasoning-LLM-Based-Medical-Diagnosis-Support-System-for-Mild-Traumatic-Brain-Injury-TBI-Prediction" class="headerlink" title="Proof-of-TBI â€“ Fine-Tuned Vision Language Model Consortium and   OpenAI-o3 Reasoning LLM-Based Medical Diagnosis Support System for Mild   Traumatic Brain Injury (TBI) Prediction"></a>Proof-of-TBI â€“ Fine-Tuned Vision Language Model Consortium and   OpenAI-o3 Reasoning LLM-Based Medical Diagnosis Support System for Mild   Traumatic Brain Injury (TBI) Prediction</h2><p><strong>Authors:Ross Gore, Eranga Bandara, Sachin Shetty, Alberto E. Musto, Pratip Rana, Ambrosio Valencia-Romero, Christopher Rhea, Lobat Tayebi, Heather Richter, Atmaram Yarlagadda, Donna Edmonds, Steven Wallace, Donna Broshek</strong></p>
<p>Mild Traumatic Brain Injury (TBI) detection presents significant challenges due to the subtle and often ambiguous presentation of symptoms in medical imaging, making accurate diagnosis a complex task. To address these challenges, we propose Proof-of-TBI, a medical diagnosis support system that integrates multiple fine-tuned vision-language models with the OpenAI-o3 reasoning large language model (LLM). Our approach fine-tunes multiple vision-language models using a labeled dataset of TBI MRI scans, training them to diagnose TBI symptoms effectively. The predictions from these models are aggregated through a consensus-based decision-making process. The system evaluates the predictions from all fine-tuned vision language models using the OpenAI-o3 reasoning LLM, a model that has demonstrated remarkable reasoning performance, to produce the most accurate final diagnosis. The LLM Agents orchestrates interactions between the vision-language models and the reasoning LLM, managing the final decision-making process with transparency, reliability, and automation. This end-to-end decision-making workflow combines the vision-language model consortium with the OpenAI-o3 reasoning LLM, enabled by custom prompt engineering by the LLM agents. The prototype for the proposed platform was developed in collaboration with the U.S. Army Medical Research team in Newport News, Virginia, incorporating five fine-tuned vision-language models. The results demonstrate the transformative potential of combining fine-tuned vision-language model inputs with the OpenAI-o3 reasoning LLM to create a robust, secure, and highly accurate diagnostic system for mild TBI prediction. To the best of our knowledge, this research represents the first application of fine-tuned vision-language models integrated with a reasoning LLM for TBI prediction tasks. </p>
<blockquote>
<p>è½»åº¦è„‘å¤–ä¼¤ï¼ˆTraumatic Brain Injury, TBIï¼‰æ£€æµ‹é¢ä¸´æ˜¾è‘—æŒ‘æˆ˜ï¼Œå› ä¸ºåœ¨åŒ»å­¦å½±åƒä¸­ï¼Œå…¶ç—‡çŠ¶è¡¨ç°å¾®å¦™ä¸”å¸¸å…·æ¨¡ç³Šæ€§ï¼Œä½¿å¾—å‡†ç¡®è¯Šæ–­æˆä¸ºä¸€é¡¹å¤æ‚ä»»åŠ¡ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºProof-of-TBIè¯Šæ–­æ”¯æŒç³»ç»Ÿï¼Œå®ƒå°†å¤šä¸ªå¾®è°ƒåçš„è§†è§‰è¯­è¨€æ¨¡å‹ä¸OpenAI-o3æ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç›¸ç»“åˆã€‚æˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨å¸¦æœ‰æ ‡ç­¾çš„TBIæ ¸ç£å…±æŒ¯æ‰«ææ•°æ®é›†å¯¹å¤šä¸ªè§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œè®­ç»ƒå®ƒä»¬æœ‰æ•ˆè¯Šæ–­TBIç—‡çŠ¶ã€‚è¿™äº›æ¨¡å‹çš„é¢„æµ‹ç»“æœé€šè¿‡åŸºäºå…±è¯†çš„å†³ç­–åˆ¶å®šè¿‡ç¨‹è¿›è¡Œæ±‡æ€»ã€‚ç³»ç»Ÿåˆ©ç”¨OpenAI-o3æ¨ç†LLMè¯„ä¼°æ‰€æœ‰å¾®è°ƒåçš„è§†è§‰è¯­è¨€æ¨¡å‹çš„é¢„æµ‹ç»“æœã€‚è¯¥æ¨¡å‹å±•ç°å‡ºå“è¶Šçš„æ¨ç†æ€§èƒ½ï¼Œç”¨ä»¥äº§ç”Ÿæœ€å‡†ç¡®çš„æœ€ç»ˆè¯Šæ–­ç»“æœã€‚LLMä»£ç†åè°ƒè§†è§‰è¯­è¨€æ¨¡å‹å’Œæ¨ç†LLMä¹‹é—´çš„äº¤äº’ï¼Œä»¥é€æ˜ã€å¯é å’Œè‡ªåŠ¨åŒ–çš„æ–¹å¼ç®¡ç†æœ€ç»ˆå†³ç­–åˆ¶å®šè¿‡ç¨‹ã€‚è¿™ä¸€ç«¯åˆ°ç«¯çš„å†³ç­–åˆ¶å®šå·¥ä½œæµç¨‹ç»“åˆäº†è§†è§‰è¯­è¨€æ¨¡å‹è”ç›Ÿå’ŒOpenAI-o3æ¨ç†LLMï¼Œé€šè¿‡LLMä»£ç†çš„è‡ªå®šä¹‰æç¤ºå·¥ç¨‹å¾—ä»¥å®ç°ã€‚æ‰€æè®®å¹³å°çš„åŸå‹æ˜¯ä¸å¼—å‰å°¼äºšå·çº½æ³¢ç‰¹çº½æ–¯ç¾å›½é™†å†›åŒ»å­¦ç ”ç©¶å°ç»„åˆä½œå¼€å‘çš„ï¼Œè¯¥å¹³å°ç»“åˆäº†äº”ä¸ªç»è¿‡å¾®è°ƒçš„è¯­è¨€è§†è§‰æ¨¡å‹ã€‚ç»“æœè¡¨æ˜ï¼Œå°†ç»è¿‡å¾®è°ƒçš„è¯­è¨€è§†è§‰æ¨¡å‹è¾“å…¥ä¸OpenAI-o3æ¨ç†LLMç›¸ç»“åˆï¼Œå…·æœ‰åˆ›å»ºç¨³å¥ã€å®‰å…¨å’Œé«˜ç²¾åº¦è¯Šæ–­ç³»ç»Ÿçš„æ½œåŠ›ï¼Œç”¨äºé¢„æµ‹è½»åº¦TBIã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™é¡¹ç ”ç©¶ä»£è¡¨äº†é¦–ä¸ªå°†ç»è¿‡å¾®è°ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ä¸æ¨ç†LLMç›¸ç»“åˆç”¨äºé¢„æµ‹TBIçš„ä»»åŠ¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.18671v1">PDF</a> </p>
<p><strong>Summary</strong><br>    åˆ©ç”¨OpenAI-o3æ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¤šæ¬¾å¾®è°ƒåçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œæ„å»ºäº†ä¸€ä¸ªåä¸ºProof-of-TBIçš„åŒ»å­¦è¯Šæ–­æ”¯æŒç³»ç»Ÿï¼Œä»¥æé«˜è½»åº¦è„‘å¤–ä¼¤ï¼ˆTBIï¼‰æ£€æµ‹çš„å‡†ç¡®æ€§ã€‚é€šè¿‡æ ‡è®°çš„TBI MRIæ‰«ææ•°æ®é›†å¯¹å¤šä¸ªè§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå†é€šè¿‡å…±è¯†å†³ç­–è¿‡ç¨‹è¿›è¡Œé¢„æµ‹ã€‚OpenAI-o3æ¨ç†LLMè´Ÿè´£è¯„ä¼°è¿™äº›æ¨¡å‹çš„é¢„æµ‹ç»“æœï¼Œä»¥äº§ç”Ÿæœ€å‡†ç¡®çš„æœ€ç»ˆè¯Šæ–­ç»“æœã€‚å®ç°äº†ç«¯åˆ°ç«¯çš„å†³ç­–æµç¨‹è‡ªåŠ¨åŒ–ã€‚è¯¥é¡¹ç›®ä¸ºé¦–åˆ›ï¼Œå…·æœ‰å˜é©æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Proof-of-TBIç³»ç»Ÿç»“åˆäº†å¤šä¸ªå¾®è°ƒåçš„è§†è§‰è¯­è¨€æ¨¡å‹å’ŒOpenAI-o3æ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œä»¥æé«˜è½»åº¦è„‘å¤–ä¼¤ï¼ˆTBIï¼‰è¯Šæ–­çš„å‡†ç¡®æ€§ã€‚</li>
<li>ç³»ç»Ÿé€šè¿‡å…±è¯†å†³ç­–è¿‡ç¨‹è¿›è¡Œé¢„æµ‹è¯Šæ–­ï¼Œè¿™ä¸€è¿‡ç¨‹åŸºäºæ‰€æœ‰ç»è¿‡å¾®è°ƒéªŒè¯çš„è§†è§‰è¯­è¨€æ¨¡å‹åšå‡ºçš„é¢„æµ‹è¯„ä¼°ã€‚</li>
<li>OpenAI-o3æ¨ç†LLMåœ¨æ•´ä¸ªå†³ç­–è¿‡ç¨‹ä¸­èµ·åˆ°æ ¸å¿ƒä½œç”¨ï¼Œä¸ä»…è´Ÿè´£è¯„ä¼°æ¨¡å‹çš„é¢„æµ‹ç»“æœï¼Œè€Œä¸”äº§å‡ºæœ€å‡†ç¡®çš„æœ€ç»ˆè¯Šæ–­æŠ¥å‘Šã€‚</li>
<li>è¯¥ç³»ç»Ÿçš„å…³é”®ç‰¹ç‚¹æ˜¯å®ç°è‡ªåŠ¨åŒ–å’Œç«¯åˆ°ç«¯çš„å†³ç­–æµç¨‹ï¼Œæå‡äº†å†³ç­–è¿‡ç¨‹çš„é€æ˜åº¦å’Œå¯é æ€§ã€‚</li>
<li>LLMä»£ç†è´Ÿè´£ç®¡ç†è§†è§‰è¯­è¨€æ¨¡å‹å’Œæ¨ç†LLMä¹‹é—´çš„äº¤äº’ï¼Œæ¨åŠ¨äº†ç³»ç»Ÿçš„è¿ä½œå’Œæœ€ç»ˆå†³ç­–æµç¨‹çš„é¡ºç•…æ‰§è¡Œã€‚</li>
<li>è¯¥ç³»ç»Ÿçš„åŸå‹ä¸ç¾å›½é™†å†›åŒ»å­¦ç ”ç©¶å›¢é˜Ÿåˆä½œå¼€å‘ï¼Œå¹¶æˆåŠŸé›†æˆäº†äº”ä¸ªç»è¿‡å¾®è°ƒéªŒè¯çš„è§†è§‰è¯­è¨€æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.18671">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3f11e0dd300bf0c4560689d1d12b1628.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f0189596864db2e3ee9e2b408f77e0f2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a2f81c7955361b94f6b8a714329a1414.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a4309ecd615cbf660a251bc4374bb136.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3deeb8cb7f1e44425fe9da053aec07f8.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Research-on-Personalized-Medical-Intervention-Strategy-Generation-System-based-on-Group-Relative-Policy-Optimization-and-Time-Series-Data-Fusion"><a href="#Research-on-Personalized-Medical-Intervention-Strategy-Generation-System-based-on-Group-Relative-Policy-Optimization-and-Time-Series-Data-Fusion" class="headerlink" title="Research on Personalized Medical Intervention Strategy Generation System   based on Group Relative Policy Optimization and Time-Series Data Fusion"></a>Research on Personalized Medical Intervention Strategy Generation System   based on Group Relative Policy Optimization and Time-Series Data Fusion</h2><p><strong>Authors:Dingxin Lu, Shurui Wu, Xinyi Huang</strong></p>
<p>With the timely formation of personalized intervention plans based on high-dimensional heterogeneous time series information becoming an important challenge in the medical field today, electronic medical records, wearables, and other multi-source medical data are increasingly generated and diversified. In this work, we develop a system to generate personalized medical intervention strategies based on Group Relative Policy Optimization (GRPO) and Time-Series Data Fusion. First, by incorporating relative policy constraints among the groups during policy gradient updates, we adaptively balance individual and group gains. To improve the robustness and interpretability of decision-making, a multi-layer neural network structure is employed to group-code patient characteristics. Second, for the rapid multi-modal fusion of multi-source heterogeneous time series, a multi-channel neural network combined with a self-attention mechanism is used for dynamic feature extraction. Key feature screening and aggregation are achieved through a differentiable gating network. Finally, a collaborative search process combining a genetic algorithm and Monte Carlo tree search is proposed to find the ideal intervention strategy, achieving global optimization. Experimental results show significant improvements in accuracy, coverage, and decision-making benefits compared with existing methods. </p>
<blockquote>
<p>éšç€åŸºäºé«˜ç»´å¼‚è´¨æ—¶é—´åºåˆ—ä¿¡æ¯åŠæ—¶å½¢æˆä¸ªæ€§åŒ–å¹²é¢„è®¡åˆ’æˆä¸ºå½“ä»ŠåŒ»ç–—é¢†åŸŸçš„é‡è¦æŒ‘æˆ˜ï¼Œç”µå­ç—…å†ã€å¯ç©¿æˆ´è®¾å¤‡ç­‰å¤šå…ƒåŒ»ç–—æ•°æ®ä¸æ–­ç”Ÿæˆå’Œå¤šæ ·åŒ–ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªç³»ç»Ÿï¼Œè¯¥ç³»ç»ŸåŸºäºç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰å’Œæ—¶åºæ•°æ®èåˆç”Ÿæˆä¸ªæ€§åŒ–åŒ»ç–—å¹²é¢„ç­–ç•¥ã€‚é¦–å…ˆï¼Œé€šè¿‡åœ¨ç­–ç•¥æ¢¯åº¦æ›´æ–°ä¸­èå…¥ç¾¤ä½“é—´çš„ç›¸å¯¹ç­–ç•¥çº¦æŸï¼Œæˆ‘ä»¬è‡ªé€‚åº”åœ°å¹³è¡¡äº†ä¸ªä½“å’Œç¾¤ä½“çš„æ”¶ç›Šã€‚ä¸ºäº†æé«˜å†³ç­–çš„ç¨³å¥æ€§å’Œå¯è§£é‡Šæ€§ï¼Œæˆ‘ä»¬é‡‡ç”¨å¤šå±‚ç¥ç»ç½‘ç»œç»“æ„å¯¹æ‚£è€…ç‰¹å¾è¿›è¡Œåˆ†ç»„ç¼–ç ã€‚å…¶æ¬¡ï¼Œä¸ºäº†è¿…é€Ÿå¤šæ¨¡æ€èåˆå¤šæºå¼‚è´¨æ—¶é—´åºåˆ—ï¼Œæˆ‘ä»¬é‡‡ç”¨ç»“åˆè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„å¤šé€šé“ç¥ç»ç½‘ç»œè¿›è¡ŒåŠ¨æ€ç‰¹å¾æå–ã€‚å…³é”®ç‰¹å¾çš„ç­›é€‰å’Œèšåˆé€šè¿‡å¯å¾®åˆ†é—¨æ§ç½‘ç»œå®ç°ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºä¸€ç§ç»“åˆé—ä¼ ç®—æ³•å’Œè’™ç‰¹å¡æ´›æ ‘æœç´¢çš„ååŒæœç´¢è¿‡ç¨‹ï¼Œä»¥æ‰¾åˆ°ç†æƒ³çš„å¹²é¢„ç­–ç•¥ï¼Œå®ç°å…¨å±€ä¼˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨å‡†ç¡®æ€§ã€è¦†ç›–ç‡å’Œå†³ç­–æ•ˆç›Šæ–¹é¢éƒ½æœ‰æ˜¾è‘—æé«˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.18631v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºé«˜ç»´å¼‚è´¨æ—¶é—´åºåˆ—ä¿¡æ¯åŠæ—¶åˆ¶å®šä¸ªæ€§åŒ–å¹²é¢„è®¡åˆ’ï¼Œå·²æˆä¸ºå½“ä»ŠåŒ»ç–—é¢†åŸŸçš„é‡è¦æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶å¼€å‘äº†ä¸€ç§åŸºäºç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰å’Œæ—¶åºæ•°æ®èåˆçš„ç³»ç»Ÿï¼Œç”Ÿæˆä¸ªæ€§åŒ–åŒ»ç–—å¹²é¢„ç­–ç•¥ã€‚é€šè¿‡èå…¥ç¾¤ç»„ç›¸å¯¹ç­–ç•¥çº¦æŸï¼Œå¹³è¡¡ä¸ªä½“ä¸ç¾¤ä½“æ”¶ç›Šï¼›é‡‡ç”¨å¤šå±‚ç¥ç»ç½‘ç»œç»“æ„è¿›è¡Œæ‚£è€…ç‰¹å¾åˆ†ç»„ç¼–ç ï¼Œæé«˜å†³ç­–ç¨³å¥æ€§å’Œå¯è§£é‡Šæ€§ã€‚å¯¹äºå¤šæºå¼‚è´¨æ—¶é—´åºåˆ—çš„å¿«é€Ÿå¤šæ¨¡å¼èåˆï¼Œç»“åˆå¤šé€šé“ç¥ç»ç½‘ç»œä¸è‡ªæ³¨æ„åŠ›æœºåˆ¶è¿›è¡ŒåŠ¨æ€ç‰¹å¾æå–ã€‚é€šè¿‡å¯å¾®åˆ†çš„é—¨æ§ç½‘ç»œå®ç°å…³é”®ç‰¹å¾ç­›é€‰å’Œèšåˆã€‚æœ€åï¼Œç»“åˆé—ä¼ ç®—æ³•å’Œè’™ç‰¹å¡æ´›æ ‘æœç´¢çš„åä½œæœç´¢è¿‡ç¨‹ï¼Œæ‰¾åˆ°ç†æƒ³çš„å¹²é¢„ç­–ç•¥ï¼Œå®ç°å…¨å±€ä¼˜åŒ–ã€‚å®éªŒç»“æœç›¸è¾ƒäºç°æœ‰æ–¹æ³•ï¼Œåœ¨å‡†ç¡®æ€§ã€è¦†ç›–ç‡å’Œå†³ç­–æ•ˆç›Šæ–¹é¢å‡æœ‰æ˜¾è‘—æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶é’ˆå¯¹åŒ»ç–—é¢†åŸŸçš„ä¸ªæ€§åŒ–å¹²é¢„è®¡åˆ’åˆ¶å®šæŒ‘æˆ˜ï¼Œæå‡ºåŸºäºç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰å’Œæ—¶åºæ•°æ®èåˆçš„ç³»ç»Ÿã€‚</li>
<li>é€šè¿‡èå…¥ç¾¤ç»„ç›¸å¯¹ç­–ç•¥çº¦æŸï¼Œå¹³è¡¡ä¸ªä½“ä¸ç¾¤ä½“æ”¶ç›Šã€‚</li>
<li>é‡‡ç”¨å¤šå±‚ç¥ç»ç½‘ç»œç»“æ„è¿›è¡Œæ‚£è€…ç‰¹å¾åˆ†ç»„ç¼–ç ï¼Œæé«˜å†³ç­–çš„ç¨³å¥æ€§å’Œå¯è§£é‡Šæ€§ã€‚</li>
<li>åˆ©ç”¨å¤šé€šé“ç¥ç»ç½‘ç»œä¸è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ç°å¤šæºå¼‚è´¨æ—¶é—´åºåˆ—çš„å¿«é€Ÿå¤šæ¨¡å¼èåˆå’ŒåŠ¨æ€ç‰¹å¾æå–ã€‚</li>
<li>é€šè¿‡å¯å¾®åˆ†çš„é—¨æ§ç½‘ç»œè¿›è¡Œå…³é”®ç‰¹å¾ç­›é€‰å’Œèšåˆã€‚</li>
<li>ç»“åˆé—ä¼ ç®—æ³•å’Œè’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼Œæ‰¾åˆ°ç†æƒ³å¹²é¢„ç­–ç•¥ï¼Œå®ç°å…¨å±€ä¼˜åŒ–ã€‚</li>
<li>ç›¸æ¯”ç°æœ‰æ–¹æ³•ï¼Œè¯¥ç³»ç»Ÿåœ¨å‡†ç¡®æ€§ã€è¦†ç›–ç‡å’Œå†³ç­–æ•ˆç›Šæ–¹é¢æœ‰æ˜æ˜¾æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.18631">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-bbeec270af359d5444b488d7b345bb0f.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Training-Large-Language-Models-to-Reason-via-EM-Policy-Gradient"><a href="#Training-Large-Language-Models-to-Reason-via-EM-Policy-Gradient" class="headerlink" title="Training Large Language Models to Reason via EM Policy Gradient"></a>Training Large Language Models to Reason via EM Policy Gradient</h2><p><strong>Authors:Tianbing Xu</strong></p>
<p>Recently, foundation models such as OpenAIâ€™s O1 and O3, along with DeepSeekâ€™s R1, have demonstrated strong reasoning capacities and problem-solving skills acquired through large-scale reinforcement learning (RL), with wide applications in mathematics, coding, science, intelligent agents, and virtual assistants. In this work, we introduce an off-policy reinforcement learning algorithm, EM Policy Gradient, aimed at enhancing LLM reasoning by optimizing expected return over reasoning trajectories. We frame the reasoning task as an Expectation-Maximization (EM) optimization problem, alternating between sampling diverse rationale trajectories and performing reward-guided fine-tuning. Unlike PPO and GRPO, which rely on complex importance weights and heuristic clipping, our method provides a simpler, more principled off-policy policy gradient approach, eliminating these complexities while maintaining strong performance. We evaluate the effectiveness of EM Policy Gradient on the GSM8K and MATH (HARD) datasets, where it achieves performance comparable to or slightly surpassing the state-of-the-art GRPO, while offering additional advantages in scalability, simplicity, and reasoning conciseness. Moreover, models fine-tuned with our method exhibit cognitive behaviors, such as sub-problem decomposition, self-verification, and backtracking, highlighting its potential to enhance both the interpretability and robustness of LLM reasoning. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œå¦‚OpenAIçš„O1å’ŒO3ä»¥åŠDeepSeekçš„R1ç­‰åŸºç¡€æ¨¡å‹ï¼Œå·²ç»å±•ç°å‡ºé€šè¿‡å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è·å¾—çš„å¼ºå¤§æ¨ç†èƒ½åŠ›å’Œé—®é¢˜è§£å†³æŠ€èƒ½ï¼Œå¹¿æ³›åº”ç”¨äºæ•°å­¦ã€ç¼–ç¨‹ã€ç§‘å­¦ã€æ™ºèƒ½ä»£ç†å’Œè™šæ‹ŸåŠ©æ‰‹ç­‰é¢†åŸŸã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç¦»ç­–ç•¥å¼ºåŒ–å­¦ä¹ ç®—æ³•â€”â€”EMç­–ç•¥æ¢¯åº¦æ³•ï¼Œæ—¨åœ¨é€šè¿‡ä¼˜åŒ–æ¨ç†è½¨è¿¹çš„é¢„æœŸå›æŠ¥æ¥æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬å°†æ¨ç†ä»»åŠ¡æ„å»ºä¸ºæœŸæœ›æœ€å¤§åŒ–ï¼ˆEMï¼‰ä¼˜åŒ–é—®é¢˜ï¼Œäº¤æ›¿è¿›è¡Œå¤šæ ·åŒ–æ¨ç†è½¨è¿¹é‡‡æ ·å’Œå¥–åŠ±å¼•å¯¼å¾®è°ƒã€‚ä¸åŒäºä¾èµ–å¤æ‚é‡è¦æ€§æƒé‡å’Œå¯å‘å¼å‰ªè£çš„PPOå’ŒGRPOï¼Œæˆ‘ä»¬çš„æ–¹æ³•æä¾›äº†ä¸€ç§æ›´ç®€å•ã€æ›´åŸºäºåŸåˆ™çš„éç­–ç•¥ç­–ç•¥æ¢¯åº¦æ–¹æ³•ï¼Œåœ¨æ¶ˆé™¤è¿™äº›å¤æ‚æ€§çš„åŒæ—¶ä¿æŒäº†è‰¯å¥½çš„æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨GSM8Kå’ŒMATHï¼ˆHARDï¼‰æ•°æ®é›†ä¸Šè¯„ä¼°äº†EMç­–ç•¥æ¢¯åº¦çš„æœ‰æ•ˆæ€§ï¼Œå…¶åœ¨æ€§èƒ½ä¸Šè¾¾åˆ°äº†æˆ–ç•¥å¾®è¶…è¿‡äº†æœ€å…ˆè¿›çš„GRPOï¼ŒåŒæ—¶åœ¨å¯æ‰©å±•æ€§ã€ç®€å•æ€§å’Œæ¨ç†ç®€æ´æ€§æ–¹é¢æä¾›äº†é¢å¤–çš„ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼Œä½¿ç”¨æˆ‘ä»¬çš„æ–¹æ³•å¾®è°ƒåçš„æ¨¡å‹å±•ç°å‡ºè®¤çŸ¥è¡Œä¸ºï¼Œå¦‚å­é—®é¢˜åˆ†è§£ã€è‡ªæˆ‘éªŒè¯å’Œå›æº¯ï¼Œè¿™çªæ˜¾äº†å…¶å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†çš„å¯è§£é‡Šæ€§å’Œç¨³å¥æ€§çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.18587v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹å¼ºåŒ–å­¦ä¹ çš„èƒ½åŠ›ï¼ŒOpenAIçš„Oç³»åˆ—æ¨¡å‹ä»¥åŠDeepSeekçš„Rç³»åˆ—æ¨¡å‹å±•ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›å’Œé—®é¢˜è§£å†³æŠ€èƒ½ï¼Œå¹¿æ³›åº”ç”¨äºæ•°å­¦ã€ç¼–ç¨‹ã€ç§‘å­¦ç­‰é¢†åŸŸã€‚æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ç§åŸºäºé¢„æœŸæ”¶ç›Šä¼˜åŒ–çš„æœŸæœ›æœ€å¤§åŒ–ç­–ç•¥æ¢¯åº¦å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œæ—¨åœ¨ä¼˜åŒ–LLMæ¨ç†èƒ½åŠ›ã€‚é€šè¿‡äº¤æ›¿è¿›è¡Œå¤šæ ·åŒ–æ¨ç†è½¨è¿¹é‡‡æ ·å’Œå¥–åŠ±å¼•å¯¼å¾®è°ƒï¼Œè¯¥æ–¹æ³•åœ¨GSM8Kå’ŒMATHï¼ˆHARDï¼‰æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜å¼‚ï¼Œç”šè‡³ä¼˜äºå½“å‰çš„é¡¶çº§GRPOæ–¹æ³•ï¼ŒåŒæ—¶åœ¨æ‰©å±•æ€§ã€ç®€æ´æ€§å’Œæ¨ç†å‡†ç¡®æ€§æ–¹é¢è¡¨ç°å‡ºé¢å¤–ä¼˜åŠ¿ã€‚æ¨¡å‹ç»è¿‡æ­¤æ–¹æ³•çš„å¾®è°ƒåå±•ç°å‡ºå­é—®é¢˜åˆ†è§£ã€è‡ªæˆ‘éªŒè¯å’Œå›æº¯ç­‰è®¤çŸ¥è¡Œä¸ºï¼Œæš—ç¤ºå…¶åœ¨å¢å¼ºLLMæ¨ç†çš„è§£é‡Šæ€§å’Œç¨³å¥æ€§æ–¹é¢çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹å¼ºåŒ–å­¦ä¹ æ¨¡å‹å¦‚OpenAIçš„Oç³»åˆ—å’ŒDeepSeekçš„Rç³»åˆ—å±•ç°å‡ºå¼ºå¤§çš„æ¨ç†å’Œé—®é¢˜è§£å†³èƒ½åŠ›ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºæœŸæœ›æœ€å¤§åŒ–çš„ç­–ç•¥æ¢¯åº¦å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œç”¨äºä¼˜åŒ–LLMæ¨ç†èƒ½åŠ›ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡äº¤æ›¿é‡‡æ ·å¤šæ ·åŒ–æ¨ç†è½¨è¿¹å’Œå¥–åŠ±å¼•å¯¼å¾®è°ƒæ¥æå‡æ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨GSM8Kå’ŒMATHï¼ˆHARDï¼‰æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜å¼‚ï¼Œæ€§èƒ½å ªæ¯”æˆ–ç•¥ä¼˜äºGRPOã€‚</li>
<li>æ–¹æ³•åœ¨æ‰©å±•æ€§ã€ç®€æ´æ€§å’Œæ¨ç†å‡†ç¡®æ€§æ–¹é¢è¡¨ç°å‡ºé¢å¤–ä¼˜åŠ¿ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.18587">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a951e0a3e4567819b1d1095bc3f773c2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-533c792dcdac7f11673fc3d5aff9e72a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-98260b541a6853a858444e4c06280e55.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3218eab28aa98efa7401e8d6af3486b5.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Parameter-Efficient-Checkpoint-Merging-via-Metrics-Weighted-Averaging"><a href="#Parameter-Efficient-Checkpoint-Merging-via-Metrics-Weighted-Averaging" class="headerlink" title="Parameter-Efficient Checkpoint Merging via Metrics-Weighted Averaging"></a>Parameter-Efficient Checkpoint Merging via Metrics-Weighted Averaging</h2><p><strong>Authors:Shi Jie Yu, Sehyun Choi</strong></p>
<p>Checkpoint merging is a technique for combining multiple model snapshots into a single superior model, potentially reducing training time for large language models. This paper explores checkpoint merging in the context of parameter-efficient fine-tuning (PEFT), where only small adapter modules (e.g. LoRA) are trained. We propose Metrics-Weighted Averaging (MWA), a simple yet effective method to merge model checkpoints by weighting their parameters according to performance metrics. In particular, we investigate weighting by training loss and by training steps, under the intuition that lower-loss or later-step checkpoints are more valuable. We introduce a formula with a penalty factor to adjust weight distribution, requiring only one hyperparameter regardless of the number of checkpoints. Experiments on three fine-tuning tasks (mathematical reasoning, preference alignment, and general instruction tuning) show that MWA consistently produces merged models that outperform the naive uniform average of checkpoints. Notably, loss-weighted merging often yields the best results, delivering up to 5% higher task accuracy than the baseline uniform merge and even surpassing the final individual checkpointâ€™s performance. These findings validate checkpoint merging for PEFT and demonstrate that a metric-driven weighting heuristic can efficiently boost model performance with minimal computational overhead. </p>
<blockquote>
<p>æ¨¡å‹æ£€æŸ¥ç‚¹åˆå¹¶æ˜¯ä¸€ç§å°†å¤šä¸ªæ¨¡å‹å¿«ç…§åˆå¹¶ä¸ºä¸€ä¸ªæ›´ä¼˜è´¨æ¨¡å‹çš„æŠ€æœ¯ï¼Œæœ‰æœ›å‡å°‘å¤§å‹è¯­è¨€æ¨¡å‹çš„è®­ç»ƒæ—¶é—´ã€‚æœ¬æ–‡é’ˆå¯¹å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰ä¸­çš„æ£€æŸ¥ç‚¹åˆå¹¶è¿›è¡Œäº†æ¢è®¨ï¼Œåœ¨è¿™é‡Œåªè®­ç»ƒè¾ƒå°çš„é€‚é…å™¨æ¨¡å—ï¼ˆä¾‹å¦‚LoRAï¼‰ã€‚æˆ‘ä»¬æå‡ºäº†Metrics-Weighted Averagingï¼ˆMWAï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•ï¼Œé€šè¿‡æ ¹æ®æ€§èƒ½æŒ‡æ ‡å¯¹æ¨¡å‹æ£€æŸ¥ç‚¹çš„å‚æ•°è¿›è¡ŒåŠ æƒæ¥åˆå¹¶æ£€æŸ¥ç‚¹ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬æ ¹æ®è®­ç»ƒæŸå¤±å’Œè®­ç»ƒæ­¥éª¤è¿›è¡ŒåŠ æƒï¼Œç›´è§‰è®¤ä¸ºæŸå¤±è¾ƒä½æˆ–æ­¥éª¤è¾ƒæ™šçš„æ£€æŸ¥ç‚¹æ›´æœ‰ä»·å€¼ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå¸¦æœ‰æƒ©ç½šå› å­çš„å…¬å¼æ¥è°ƒæ•´æƒé‡åˆ†å¸ƒï¼Œæ— è®ºæ£€æŸ¥ç‚¹çš„æ•°é‡å¦‚ä½•ï¼Œåªéœ€ä¸€ä¸ªè¶…å‚æ•°ã€‚åœ¨ä¸‰ä¸ªå¾®è°ƒä»»åŠ¡ï¼ˆæ•°å­¦æ¨ç†ã€åå¥½å¯¹é½å’Œé€šç”¨æŒ‡ä»¤è°ƒæ•´ï¼‰ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMWAæŒç»­äº§ç”Ÿåˆå¹¶æ¨¡å‹ï¼Œå…¶æ€§èƒ½ä¼˜äºæ£€æŸ¥ç‚¹çš„ç®€å•å‡åŒ€å¹³å‡å€¼ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒæŸå¤±åŠ æƒåˆå¹¶å¾€å¾€äº§ç”Ÿæœ€ä½³ç»“æœï¼Œæ¯”åŸºçº¿å‡åŒ€åˆå¹¶é«˜å‡ºé«˜è¾¾5%çš„ä»»åŠ¡å‡†ç¡®æ€§ï¼Œç”šè‡³è¶…è¿‡æœ€ç»ˆå•ä¸ªæ£€æŸ¥ç‚¹çš„æ€§èƒ½ã€‚è¿™äº›å‘ç°éªŒè¯äº†æ£€æŸ¥ç‚¹åˆå¹¶å¯¹äºPEFTçš„æœ‰æ•ˆæ€§ï¼Œå¹¶è¡¨æ˜æŒ‡æ ‡é©±åŠ¨çš„åŠ æƒå¯å‘å¼ç­–ç•¥å¯ä»¥åœ¨å‡ ä¹ä¸å¢åŠ è®¡ç®—å¼€é”€çš„æƒ…å†µä¸‹æœ‰æ•ˆæé«˜æ¨¡å‹æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.18580v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰ä¸­çš„æ£€æŸ¥ç‚¹åˆå¹¶æŠ€æœ¯ï¼Œå¹¶æå‡ºäº†Metrics-Weighted Averagingï¼ˆMWAï¼‰æ–¹æ³•ã€‚è¯¥æ–¹æ³•æ ¹æ®æ€§èƒ½åº¦é‡å¯¹æ¨¡å‹æ£€æŸ¥ç‚¹çš„å‚æ•°è¿›è¡ŒåŠ æƒåˆå¹¶ã€‚å®éªŒè¡¨æ˜ï¼ŒMWAæ–¹æ³•äº§ç”Ÿçš„åˆå¹¶æ¨¡å‹æ€§èƒ½ä¼˜äºç®€å•å¹³å‡åˆå¹¶ï¼Œå…¶ä¸­æŸå¤±åŠ æƒåˆå¹¶æ•ˆæœæœ€ä½³ï¼Œä»»åŠ¡å‡†ç¡®ç‡è¾ƒåŸºçº¿æ–¹æ³•æé«˜è¾¾5%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ£€æŸ¥ç‚¹åˆå¹¶æ˜¯ä¸€ç§ç»“åˆå¤šä¸ªæ¨¡å‹å¿«ç…§ä»¥å½¢æˆå•ä¸€ä¼˜ç§€æ¨¡å‹çš„æŠ€æœ¯ï¼Œå¯ç¼©çŸ­å¤§å‹è¯­è¨€æ¨¡å‹çš„è®­ç»ƒæ—¶é—´ã€‚</li>
<li>æ–‡ç« ä»‹ç»äº†åœ¨å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰èƒŒæ™¯ä¸‹æ£€æŸ¥ç‚¹åˆå¹¶çš„æ¢ç©¶ï¼Œå…¶ä¸­åªè®­ç»ƒå°å‹çš„é€‚é…å™¨æ¨¡å—ï¼ˆå¦‚LoRAï¼‰ã€‚</li>
<li>æå‡ºäº†Metrics-Weighted Averagingï¼ˆMWAï¼‰æ–¹æ³•ï¼Œæ ¹æ®æ€§èƒ½åº¦é‡å¯¹æ¨¡å‹æ£€æŸ¥ç‚¹çš„å‚æ•°è¿›è¡ŒåŠ æƒåˆå¹¶ã€‚</li>
<li>MWAæ–¹æ³•é€šè¿‡è€ƒè™‘æŸå¤±å€¼å’Œè®­ç»ƒæ­¥éª¤æ¥ç¡®å®šæƒé‡ã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªå…¬å¼æ¥è°ƒæ•´æƒé‡åˆ†å¸ƒï¼Œåªéœ€ä¸€ä¸ªè¶…å‚æ•°ï¼Œæ— è®ºæ£€æŸ¥ç‚¹æ•°é‡å¦‚ä½•ã€‚</li>
<li>åœ¨ä¸‰ä¸ªå¾®è°ƒä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMWAæ–¹æ³•äº§ç”Ÿçš„åˆå¹¶æ¨¡å‹æ€§èƒ½ä¼˜äºç®€å•çš„å¹³å‡åˆå¹¶æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.18580">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-68fe610040efbc9832666f27b3dbdac6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7bd187331d269208bfd60a8b77f5a085.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7a82a9c4583cd8884da45dfcaccb9a28.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="MASR-Self-Reflective-Reasoning-through-Multimodal-Hierarchical-Attention-Focusing-for-Agent-based-Video-Understanding"><a href="#MASR-Self-Reflective-Reasoning-through-Multimodal-Hierarchical-Attention-Focusing-for-Agent-based-Video-Understanding" class="headerlink" title="MASR: Self-Reflective Reasoning through Multimodal Hierarchical   Attention Focusing for Agent-based Video Understanding"></a>MASR: Self-Reflective Reasoning through Multimodal Hierarchical   Attention Focusing for Agent-based Video Understanding</h2><p><strong>Authors:Shiwen Cao, Zhaoxing Zhang, Junming Jiao, Juyi Qiao, Guowen Song, Rong Shen, Xiangbing Meng</strong></p>
<p>Even in the era of rapid advances in large models, video understanding remains a highly challenging task. Compared to texts or images, videos commonly contain more information with redundancy, requiring large models to properly allocate attention at a global level for comprehensive and accurate understanding. To address this, we propose a Multimodal hierarchical Attention focusing Self-reflective Reasoning (MASR) framework for agent-based video understanding. The key innovation lies in its ability to detect and prioritize segments of videos that are highly relevant to the query. Firstly, MASR realizes Multimodal Coarse-to-fine Relevance Sensing (MCRS) which enhances the correlation between the acquired contextual information and the query. Secondly, MASR employs Dilated Temporal Expansion (DTE) to mitigate the risk of missing crucial details when extracting semantic information from the focused frames selected through MCRS. By iteratively applying MCRS and DTE in the self-reflective reasoning process, MASR is able to adaptively adjust the attention to extract highly query-relevant context and therefore improve the response accuracy. In the EgoSchema dataset, MASR achieves a remarkable 5% performance gain over previous leading approaches. In the Next-QA and IntentQA datasets, it outperforms the state-of-the-art standards by 0.2% and 0.3% respectively. In the Video-MME dataset that contains long-term videos, MASR also performs better than other agent-based methods. </p>
<blockquote>
<p>å³ä½¿åœ¨å¤§å‹æ¨¡å‹é£é€Ÿå‘å±•çš„æ—¶ä»£ï¼Œè§†é¢‘ç†è§£ä»ç„¶æ˜¯ä¸€é¡¹æå…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚ä¸æ–‡æœ¬æˆ–å›¾åƒç›¸æ¯”ï¼Œè§†é¢‘é€šå¸¸åŒ…å«æ›´å¤šå¸¦æœ‰å†—ä½™çš„ä¿¡æ¯ï¼Œéœ€è¦å¤§å‹æ¨¡å‹åœ¨å…¨å±€å±‚é¢ä¸Šé€‚å½“åœ°åˆ†é…æ³¨æ„åŠ›ï¼Œä»¥å®ç°å…¨é¢è€Œå‡†ç¡®çš„ç†è§£ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºå¤šæ¨¡æ€åˆ†å±‚æ³¨æ„åŠ›çš„è‡ªæˆ‘åæ€æ¨ç†ï¼ˆMASRï¼‰æ¡†æ¶ï¼Œç”¨äºåŸºäºä»£ç†çš„è§†é¢‘ç†è§£ã€‚å…³é”®åˆ›æ–°ç‚¹åœ¨äºå®ƒèƒ½å¤Ÿæ£€æµ‹å’Œä¼˜å…ˆå¤„ç†ä¸æŸ¥è¯¢é«˜åº¦ç›¸å…³çš„è§†é¢‘ç‰‡æ®µã€‚é¦–å…ˆï¼ŒMASRå®ç°äº†å¤šæ¨¡æ€ç²—ç»†ç›¸å…³æ€§æ„ŸçŸ¥ï¼ˆMCRSï¼‰ï¼Œå¢å¼ºäº†è·å–ä¸Šä¸‹æ–‡ä¿¡æ¯ä¸æŸ¥è¯¢ä¹‹é—´çš„ç›¸å…³æ€§ã€‚å…¶æ¬¡ï¼ŒMASRé‡‡ç”¨è†¨èƒ€æ—¶é—´æ‰©å±•ï¼ˆDTEï¼‰æŠ€æœ¯ï¼Œä»¥ç¼“è§£åœ¨ä»é€šè¿‡MCRSé€‰æ‹©çš„èšç„¦å¸§ä¸­æå–è¯­ä¹‰ä¿¡æ¯æ—¶é—æ¼å…³é”®ç»†èŠ‚çš„é£é™©ã€‚é€šè¿‡è‡ªæˆ‘åæ€æ¨ç†è¿‡ç¨‹ä¸­è¿­ä»£åº”ç”¨MCRSå’ŒDTEï¼ŒMASRèƒ½å¤Ÿè‡ªé€‚åº”åœ°è°ƒæ•´æ³¨æ„åŠ›ä»¥æå–ä¸æŸ¥è¯¢é«˜åº¦ç›¸å…³çš„ä¸Šä¸‹æ–‡ï¼Œä»è€Œæé«˜å“åº”å‡†ç¡®æ€§ã€‚åœ¨EgoSchemaæ•°æ®é›†ä¸Šï¼ŒMASRç›¸è¾ƒäºä¹‹å‰é¢†å…ˆçš„æ–¹æ³•å–å¾—äº†5%çš„æ€§èƒ½æå‡ã€‚åœ¨Next-QAå’ŒIntentQAæ•°æ®é›†ä¸Šï¼Œå®ƒåˆ†åˆ«è¶…å‡ºäº†å½“å‰æœ€ä½³æ ‡å‡†0.2%å’Œ0.3%ã€‚å¯¹äºåŒ…å«é•¿æœŸè§†é¢‘çš„è§†é¢‘MMEæ•°æ®é›†ï¼ŒMASRä¹Ÿè¡¨ç°å‡ºæ¯”å…¶ä»–åŸºäºä»£ç†çš„æ–¹æ³•æ›´å¥½çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.17213v2">PDF</a> </p>
<p><strong>Summary</strong><br>è§†é¢‘ç†è§£ä»»åŠ¡ä»ç„¶æå…·æŒ‘æˆ˜æ€§ã€‚åœ¨å¤§æ•°æ®æ—¶ä»£ï¼Œå³ä½¿é¢ä¸´å¤æ‚æ¨¡å‹å’Œæ•°æ®çš„å¤æ‚æ€§å¢åŠ çš„æŒ‘æˆ˜ï¼Œæå‡ºçš„å¤šæ¨¡æ€å±‚æ¬¡åŒ–æ³¨æ„åŠ›è‡ªåæ€æ¨ç†æ¡†æ¶ï¼ˆMASRï¼‰ä»ç„¶å…·æœ‰æ£€æµ‹å¹¶ä¼˜å…ˆå¤„ç†ä¸æŸ¥è¯¢é«˜åº¦ç›¸å…³çš„è§†é¢‘ç‰‡æ®µçš„èƒ½åŠ›ã€‚MASRé€šè¿‡å®ç°å¤šæ¨¡æ€ç²—åˆ°ç»†ç›¸å…³æ€§æ„ŸçŸ¥ï¼ˆMCRSï¼‰å’Œè†¨èƒ€æ—¶é—´æ‰©å±•ï¼ˆDTEï¼‰æŠ€æœ¯ï¼Œå¢å¼ºäº†ä¸Šä¸‹æ–‡ä¿¡æ¯ä¸æŸ¥è¯¢ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œå¹¶å‡å°‘äº†åœ¨æå–è¯­ä¹‰ä¿¡æ¯æ—¶é—æ¼å…³é”®ç»†èŠ‚çš„é£é™©ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒMASRç›¸è¾ƒäºå…¶ä»–å‰æ²¿æ–¹æ³•å…·æœ‰æ˜¾è‘—æ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†é¢‘ç†è§£åœ¨å½“å‰æ—¶ä»£ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ï¼Œå› ä¸ºè§†é¢‘åŒ…å«æ¯”æ–‡æœ¬æˆ–å›¾åƒæ›´å¤šçš„ä¿¡æ¯å’Œå†—ä½™ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºMultimodal hierarchical Attention focusing Self-reflective Reasoning (MASR)çš„æ¡†æ¶ï¼Œç”¨äºåŸºäºä»£ç†çš„è§†é¢‘ç†è§£ã€‚</li>
<li>MASRçš„å…³é”®åˆ›æ–°åœ¨äºå…¶æ£€æµ‹å¹¶ä¼˜å…ˆå¤„ç†ä¸æŸ¥è¯¢é«˜åº¦ç›¸å…³çš„è§†é¢‘ç‰‡æ®µçš„èƒ½åŠ›ã€‚</li>
<li>MASRé€šè¿‡å®ç°å¤šæ¨¡æ€ç²—åˆ°ç»†ç›¸å…³æ€§æ„ŸçŸ¥ï¼ˆMCRSï¼‰ï¼Œå¢å¼ºäº†ä¸Šä¸‹æ–‡ä¿¡æ¯ä¸æŸ¥è¯¢ä¹‹é—´çš„ç›¸å…³æ€§ã€‚</li>
<li>é€šè¿‡é‡‡ç”¨è†¨èƒ€æ—¶é—´æ‰©å±•ï¼ˆDTEï¼‰æŠ€æœ¯ï¼ŒMASRå‡å°‘äº†åœ¨æå–è¯­ä¹‰ä¿¡æ¯æ—¶é—æ¼å…³é”®ç»†èŠ‚çš„é£é™©ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒMASRç›¸è¾ƒäºå…¶ä»–å‰æ²¿æ–¹æ³•å…·æœ‰5%çš„æ€§èƒ½æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.17213">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-19803cf55b0f2fd085a0cabdc52e0c9e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-44350aade99be5a773d79f76671a9c0f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9bedc8630850d6b654da8a09f189de34.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7b4e01fa007f35270d8edd94f8a961d2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-54157ae2989359e849efd86647602e23.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="SARI-Structured-Audio-Reasoning-via-Curriculum-Guided-Reinforcement-Learning"><a href="#SARI-Structured-Audio-Reasoning-via-Curriculum-Guided-Reinforcement-Learning" class="headerlink" title="SARI: Structured Audio Reasoning via Curriculum-Guided Reinforcement   Learning"></a>SARI: Structured Audio Reasoning via Curriculum-Guided Reinforcement   Learning</h2><p><strong>Authors:Cheng Wen, Tingwei Guo, Shuaijiang Zhao, Wei Zou, Xiangang Li</strong></p>
<p>Recent work shows that reinforcement learning(RL) can markedly sharpen the reasoning ability of large language models (LLMs) by prompting them to â€œthink before answering.â€ Yet whether and how these gains transfer to audio-language reasoning remains largely unexplored. We extend the Group-Relative Policy Optimization (GRPO) framework from DeepSeek-R1 to a Large Audio-Language Model (LALM), and construct a 32k sample multiple-choice corpus. Using a two-stage regimen supervised fine-tuning on structured and unstructured chains-of-thought, followed by curriculum-guided GRPO, we systematically compare implicit vs. explicit, and structured vs. free form reasoning under identical architectures. Our structured audio reasoning model, SARI (Structured Audio Reasoning via Curriculum-Guided Reinforcement Learning), achieves a 16.35% improvement in average accuracy over the base model Qwen2-Audio-7B-Instruct. Furthermore, the variant built upon Qwen2.5-Omni reaches state-of-the-art performance of 67.08% on the MMAU test-mini benchmark. Ablation experiments show that on the base model we use: (i) SFT warm-up is important for stable RL training, (ii) structured chains yield more robust generalization than unstructured ones, and (iii) easy-to-hard curricula accelerate convergence and improve final performance. These findings demonstrate that explicit, structured reasoning and curriculum learning substantially enhances audio-language understanding. </p>
<blockquote>
<p>æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡æç¤ºâ€œå…ˆæ€è€ƒå†å›ç­”â€ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯ä»¥æ˜¾è‘—æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æ”¶ç›Šæ˜¯å¦ä»¥åŠå¦‚ä½•è½¬ç§»åˆ°éŸ³é¢‘è¯­è¨€æ¨ç†ï¼Œä»ç„¶åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæœªè¢«æ¢ç´¢ã€‚æˆ‘ä»¬å°†DeepSeek-R1çš„Group-Relative Policy Optimizationï¼ˆGRPOï¼‰æ¡†æ¶æ‰©å±•åˆ°ä¸€ä¸ªå¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLALMï¼‰ï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªåŒ…å«3ä¸‡å¤šä¸ªæ ·æœ¬çš„å¤šé¡¹é€‰æ‹©é¢˜è¯­æ–™åº“ã€‚é€šè¿‡ä½¿ç”¨ä¸¤é˜¶æ®µç›‘ç®¡ç›‘ç£å¾®è°ƒå¯¹ç»“æ„å’Œéç»“æ„åŒ–çš„æ€ç»´é“¾è¿›è¡Œå¾®è°ƒï¼Œéšåè¿›è¡Œè¯¾ç¨‹å¼•å¯¼çš„GRPOï¼Œæˆ‘ä»¬åœ¨ç›¸åŒçš„æ¶æ„ä¸‹ç³»ç»Ÿåœ°æ¯”è¾ƒäº†éšå¼å’Œæ˜¾å¼ä»¥åŠç»“æ„åŒ–ä¸éç»“æ„åŒ–æ¨ç†ã€‚æˆ‘ä»¬çš„ç»“æ„åŒ–éŸ³é¢‘æ¨ç†æ¨¡å‹SARIï¼ˆé€šè¿‡è¯¾ç¨‹å¼•å¯¼å¼ºåŒ–å­¦ä¹ çš„ç»“æ„åŒ–éŸ³é¢‘æ¨ç†ï¼‰åœ¨åŸºç¡€æ¨¡å‹Qwen2-Audio-7B-Instructçš„åŸºç¡€ä¸Šå®ç°äº†å¹³å‡ç²¾åº¦16.35%çš„æå‡ã€‚æ­¤å¤–ï¼ŒåŸºäºQwen2.5-Omniçš„å˜ä½“åœ¨MMAUæµ‹è¯•å°å‹åŸºå‡†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå‡†ç¡®ç‡ä¸º67.08%ã€‚æ¶ˆèå®éªŒè¡¨æ˜ï¼Œåœ¨æˆ‘ä»¬ä½¿ç”¨çš„åŸºå‡†æ¨¡å‹ä¸Šï¼šï¼ˆiï¼‰SFTé¢„çƒ­å¯¹äºç¨³å®šçš„RLè®­ç»ƒå¾ˆé‡è¦ï¼Œï¼ˆiiï¼‰ç»“æ„åŒ–é“¾æ¡ç›¸å¯¹äºéç»“æ„åŒ–é“¾æ¡äº§ç”Ÿæ›´ç¨³å¥çš„æ³›åŒ–æ€§èƒ½ï¼Œï¼ˆiiiï¼‰ä»æ˜“åˆ°éš¾çš„è¯¾ç¨‹åŠ é€Ÿæ”¶æ•›å¹¶æé«˜äº†æœ€ç»ˆæ€§èƒ½ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œæ˜ç¡®çš„ã€ç»“æ„åŒ–çš„æ¨ç†å’Œè¯¾ç¨‹å­¦ä¹ æ˜¾è‘—æé«˜äº†éŸ³é¢‘è¯­è¨€çš„ç†è§£èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15900v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢ç´¢äº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æå‡å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLALMï¼‰çš„æ¨ç†èƒ½åŠ›æ–¹é¢çš„åº”ç”¨ã€‚é€šè¿‡ç»“åˆGroup-Relative Policy Optimizationï¼ˆGRPOï¼‰æ¡†æ¶å’Œç»“æ„åŒ–éŸ³é¢‘æ¨ç†æ¨¡å‹SARIï¼Œç ”ç©¶å®ç°äº†å¯¹åŸºç¡€æ¨¡å‹çš„æ˜¾è‘—æ€§èƒ½æå‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç»“æ„åŒ–æ¨ç†å’Œè¯¾ç¨‹å­¦ä¹ èƒ½å¤§å¹…æå‡éŸ³é¢‘è¯­è¨€çš„ç†è§£èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰èƒ½æ˜¾è‘—æå‡å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLALMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ç ”ç©¶äººå‘˜å°†GRPOæ¡†æ¶ä»DeepSeek-R1æ‰©å±•åˆ°LALMï¼Œå¹¶æ„å»ºäº†åŒ…å«32kæ ·æœ¬çš„å¤šé€‰é¢˜è¯­æ–™åº“ã€‚</li>
<li>é€šè¿‡ä¸¤é˜¶æ®µç›‘ç®¡å¾®è°ƒä¸ç»“æ„åŒ–åŠéç»“æ„åŒ–æ€ç»´é“¾ç›¸ç»“åˆï¼Œå†é…åˆè¯¾ç¨‹å¼•å¯¼çš„GRPOæ–¹æ³•ï¼Œç³»ç»Ÿåœ°æ¯”è¾ƒäº†éšå¼ä¸æ˜¾å¼ã€ç»“æ„åŒ–ä¸éç»“æ„åŒ–æ¨ç†åœ¨ç›¸åŒæ¶æ„ä¸‹çš„è¡¨ç°ã€‚</li>
<li>ç»“æ„åŒ–éŸ³é¢‘æ¨ç†æ¨¡å‹SARIåœ¨åŸºç¡€æ¨¡å‹Qwen2-Audio-7B-Instructä¸Šå®ç°äº†å¹³å‡å‡†ç¡®åº¦16.35%çš„æå‡ã€‚</li>
<li>åŸºäºQwen2.5-Omniçš„å˜ä½“åœ¨MMAU test-miniåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›æ€§èƒ½ï¼Œå‡†ç¡®ç‡ä¸º67.08%ã€‚</li>
<li>æ¶ˆèå®éªŒè¡¨æ˜ï¼Œå¯¹äºåŸºç¡€æ¨¡å‹æ¥è¯´ï¼Œç›‘ç£å¾®è°ƒå‰çš„é¢„çƒ­å¯¹äºç¨³å®šçš„RLè®­ç»ƒè‡³å…³é‡è¦ï¼Œç»“æ„åŒ–æ€ç»´é“¾ç›¸æ¯”éç»“æ„åŒ–æ€ç»´é“¾èƒ½äº§ç”Ÿæ›´ç¨³å¥çš„æ³›åŒ–æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15900">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-be1fa1d6dc749dcc0a5e04dfe5946be1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7238825f57f6362fd33fb753360493ac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-52a85868ad6ee7128cfd9498f7ebc10b.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-30/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-30/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-30/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-02d9999612e01b82dcf6e5fd4521e113.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-30  From Concept to Practice an Automated LLM-aided UVM Machine for RTL   Verification
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-30
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-29/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e71fcf378663ef1f9c3ec858b3f3994b.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-29  Disentangle Identity, Cooperate Emotion Correlation-Aware Emotional   Talking Portrait Generation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-29
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">29885.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
