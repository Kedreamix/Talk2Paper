<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="医学图像">
    <meta name="description" content="医学图像 方向最新论文已更新，请持续关注 Update in 2025-04-30  Breast Cancer Detection from Multi-View Screening Mammograms with Visual   Prompt Tuning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>医学图像 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-5995b096b27118f607b5d98b2c6e3c25.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">医学图像</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">医学图像</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                医学图像
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-04-30
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    19.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    79 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-04-30-更新"><a href="#2025-04-30-更新" class="headerlink" title="2025-04-30 更新"></a>2025-04-30 更新</h1><h2 id="Breast-Cancer-Detection-from-Multi-View-Screening-Mammograms-with-Visual-Prompt-Tuning"><a href="#Breast-Cancer-Detection-from-Multi-View-Screening-Mammograms-with-Visual-Prompt-Tuning" class="headerlink" title="Breast Cancer Detection from Multi-View Screening Mammograms with Visual   Prompt Tuning"></a>Breast Cancer Detection from Multi-View Screening Mammograms with Visual   Prompt Tuning</h2><p><strong>Authors:Han Chen, Anne L. Martel</strong></p>
<p>Accurate detection of breast cancer from high-resolution mammograms is crucial for early diagnosis and effective treatment planning. Previous studies have shown the potential of using single-view mammograms for breast cancer detection. However, incorporating multi-view data can provide more comprehensive insights. Multi-view classification, especially in medical imaging, presents unique challenges, particularly when dealing with large-scale, high-resolution data. In this work, we propose a novel Multi-view Visual Prompt Tuning Network (MVPT-NET) for analyzing multiple screening mammograms. We first pretrain a robust single-view classification model on high-resolution mammograms and then innovatively adapt multi-view feature learning into a task-specific prompt tuning process. This technique selectively tunes a minimal set of trainable parameters (7%) while retaining the robustness of the pre-trained single-view model, enabling efficient integration of multi-view data without the need for aggressive downsampling. Our approach offers an efficient alternative to traditional feature fusion methods, providing a more robust, scalable, and efficient solution for high-resolution mammogram analysis. Experimental results on a large multi-institution dataset demonstrate that our method outperforms conventional approaches while maintaining detection efficiency, achieving an AUROC of 0.852 for distinguishing between Benign, DCIS, and Invasive classes. This work highlights the potential of MVPT-NET for medical imaging tasks and provides a scalable solution for integrating multi-view data in breast cancer detection. </p>
<blockquote>
<p>乳腺癌的高分辨率钼靶检测对于早期发现和有效治疗计划至关重要。早期研究已经展示了使用单视图钼靶检测乳腺癌的潜力。然而，整合多视图数据可以提供更全面的洞察。多视图分类，特别是在医学影像中，呈现出独特的挑战，尤其是在处理大规模、高分辨率数据时。在这项工作中，我们提出了一种新颖的多视图视觉提示调整网络（MVPT-NET），用于分析多个筛查钼靶图像。我们首先在高分辨率钼靶图像上训练一个稳健的单视图分类模型，然后创新地将多视图特征学习适应为任务特定的提示调整过程。该技术选择性地调整一小部分可训练参数（7%），同时保留预训练单视图模型的稳健性，能够实现多视图数据的有效集成，而无需进行激进的下采样。我们的方法提供了传统特征融合方法的有效替代方案，为钼靶图像的高分辨率分析提供了更稳健、可扩展和高效的解决方案。在大型多机构数据集上的实验结果表明，我们的方法在保持检测效率的同时，优于传统方法，在区分良性、DCIS和侵袭性类别时达到了0.852的AUROC。这项工作突出了MVPT-NET在医学成像任务中的潜力，并为整合多视图数据在乳腺癌检测中提供了可扩展的解决方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.19900v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种新型的多视角视觉提示调整网络（MVPT-NET），用于分析多视角乳腺癌筛查图像。通过预训练单视角分类模型并适应多视角特征学习，该方法能在不牺牲检测效率的前提下，实现多视角数据的有效整合，提高了乳腺癌检测的准确性和效率。实验结果表明，该方法在大型多机构数据集上的表现优于传统方法，达到AUROC 0.852。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MVPT-NET用于分析乳腺癌多视角筛查图像，以提供早期诊断和治疗方案的有效指导。</li>
<li>该方法结合了单视角分类模型和多视角特征学习，提高了乳腺癌检测的准确性。</li>
<li>MVPT-NET采用预训练模型进行微调，提高了模型的鲁棒性和性能。</li>
<li>通过创新的提示调整过程实现多视角数据的有效整合，不需要剧烈的降采样操作。</li>
<li>该方法优于传统特征融合方法，为医学图像分析提供了更稳健、可扩展和高效的解决方案。</li>
<li>实验结果表明，MVPT-NET在大型多机构数据集上的性能优于其他方法，达到了AUROC为0.852的高表现。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.19900">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2253ed1a2020ac1c6ed97c9eced6ea50.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7334a8d900495cf598d58592c9144056.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Dissipative-particle-dynamics-models-of-encapsulated-microbubbles-and-gas-vesicles-for-biomedical-ultrasound-simulations"><a href="#Dissipative-particle-dynamics-models-of-encapsulated-microbubbles-and-gas-vesicles-for-biomedical-ultrasound-simulations" class="headerlink" title="Dissipative particle dynamics models of encapsulated microbubbles and   gas vesicles for biomedical ultrasound simulations"></a>Dissipative particle dynamics models of encapsulated microbubbles and   gas vesicles for biomedical ultrasound simulations</h2><p><strong>Authors:Nikolaos Ntarakas, Maša Lah, Daniel Svenšek, Tilen Potisk, Matej Praprotnik</strong></p>
<p>Ultrasound-guided drug and gene delivery (usdg) enables controlled and spatially precise delivery of drugs and macromolecules, encapsulated in microbubbles (embs) and submicron gas vesicles (gvs), to target areas such as cancer tumors. It is a non-invasive, high precision, low toxicity process with drastically reduced drug dosage. Rheological and acoustic properties of gvs and embs critically affect the outcome of usdg and imaging. Detailed understanding and modeling of their physical properties is thus essential for ultrasound-mediated therapeutic applications. State-of-the-art continuuum models of shelled bodies cannot incorporate critical details such as varying thickness of the encapsulating shell or specific interactions between its constituents and interior or exterior solvents. Such modeling approaches also do not allow for detailed modeling of chemical surface functionalizations, which are crucial for tuning the gv-blood interactions. We develop a general particle-based modeling framework for encapsulated bodies that accurately captures elastic and rheological properties of gvs and embs. We use dissipative particle dynamics to model the solvent, the gaseous phase in the capsid, and the triangulated surfaces of immersed objects. Their elastic behavior is studied and validated through stretching and buckling simulations, eigenmode analysis, shear flow simulations, and comparison of predicted gv buckling pressure with experimental data from the literature. The presented modeling approach paves the way for large-scale simulations of encapsulated bodies, capturing their dynamics, interactions, and collective behavior. </p>
<blockquote>
<p>超声引导下的药物和基因传递（USDG）技术能够实现药物和大分子在微泡（EMBs）和亚微米气体囊泡（GVs）的封装下，对癌症肿瘤等目标区域的精准空间控制传递。这是一种非侵入性、高精度、低毒性的过程，能大幅度减少药物剂量。GVs和EMBs的流变学和声学特性对USDG和成像的结果具有重要影响。因此，对其物理性质的深入理解和建模对于超声介导的治疗应用至关重要。现有的先进连续体模型无法纳入封装体的重要细节，如封装壳厚度不同或其与内外溶剂之间的特定相互作用。这种建模方法也不允许对表面功能化进行详细的建模，这对于调节GV与血液之间的相互作用至关重要。我们开发了一种用于封装体的通用粒子建模框架，能够准确捕捉GVs和EMBs的弹性和流变性质。我们使用耗散粒子动力学来模拟溶剂、衣壳中的气相和浸没物体的三角形表面。我们研究了他们的弹性行为并通过拉伸和弯曲模拟、特征模态分析、剪切流模拟以及将预测的GV弯曲压力与文献中的实验数据进行比较来验证其有效性。所呈现的建模方法为大规模封装体模拟铺平了道路，能够捕捉其动力学、相互作用和集体行为。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.19890v1">PDF</a> 55 pages, 18 figures, 4 tables</p>
<p><strong>Summary</strong></p>
<p>本文介绍了超声引导药物和基因传递（usdg）技术，该技术利用微气泡（embs）和亚微米气体囊泡（gvs）封装药物和大分子，实现针对癌症肿瘤等目标区域的精准空间传递。这是一种非侵入性、高精度、低毒性的过程，可大幅度降低药物剂量。文章强调了对gvs和embs流变学和声学特性的深入理解与建模对于超声介导治疗应用的重要性。现有的连续模型无法涵盖关键细节，如封装壳厚度变化或其与内外溶剂间的特定相互作用。因此，开发了一种针对封装体的通用粒子基础建模框架，准确捕捉gvs和embs的弹性和流变特性。该研究为封装体的大规模模拟铺平了道路，能够捕捉其动力学、相互作用和集体行为。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>超声引导药物和基因传递（usdg）能够实现精准的空间药物传递，尤其针对癌症肿瘤等目标区域。</li>
<li>该技术利用微气泡（embs）和亚微米气体囊泡（gvs）封装药物和大分子。</li>
<li>usdg是一种非侵入性、高精度、低毒性的过程，可大幅度降低药物剂量。</li>
<li>gvs和embs的流变学和声学特性对usdg和成像结果有重要影响。</li>
<li>现有连续模型无法充分描述gvs和embs的关键特性，如封装壳的厚度变化和其与溶剂的相互作用。</li>
<li>研究开发了一种基于粒子的建模框架来准确捕捉gvs和embs的弹性和流变特性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.19890">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-27252c450846465cf9c7e719cee4e5e7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6731a0772d5854cea4e3c06830794172.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="SRMF-A-Data-Augmentation-and-Multimodal-Fusion-Approach-for-Long-Tail-UHR-Satellite-Image-Segmentation"><a href="#SRMF-A-Data-Augmentation-and-Multimodal-Fusion-Approach-for-Long-Tail-UHR-Satellite-Image-Segmentation" class="headerlink" title="SRMF: A Data Augmentation and Multimodal Fusion Approach for Long-Tail   UHR Satellite Image Segmentation"></a>SRMF: A Data Augmentation and Multimodal Fusion Approach for Long-Tail   UHR Satellite Image Segmentation</h2><p><strong>Authors:Yulong Guo, Zilun Zhang, Yongheng Shang, Tiancheng Zhao, Shuiguang Deng, Yingchun Yang, Jianwei Yin</strong></p>
<p>The long-tail problem presents a significant challenge to the advancement of semantic segmentation in ultra-high-resolution (UHR) satellite imagery. While previous efforts in UHR semantic segmentation have largely focused on multi-branch network architectures that emphasize multi-scale feature extraction and fusion, they have often overlooked the importance of addressing the long-tail issue. In contrast to prior UHR methods that focused on independent feature extraction, we emphasize data augmentation and multimodal feature fusion to alleviate the long-tail problem. In this paper, we introduce SRMF, a novel framework for semantic segmentation in UHR satellite imagery. Our approach addresses the long-tail class distribution by incorporating a multi-scale cropping technique alongside a data augmentation strategy based on semantic reordering and resampling. To further enhance model performance, we propose a multimodal fusion-based general representation knowledge injection method, which, for the first time, fuses text and visual features without the need for individual region text descriptions, extracting more robust features. Extensive experiments on the URUR, GID, and FBP datasets demonstrate that our method improves mIoU by 3.33%, 0.66%, and 0.98%, respectively, achieving state-of-the-art performance. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/BinSpa/SRMF.git">https://github.com/BinSpa/SRMF.git</a>. </p>
<blockquote>
<p>超长尾问题为超高分辨率（UHR）卫星图像语义分割的进一步发展带来了巨大挑战。以往超高分辨率语义分割的主要努力大多集中在多分支网络架构上，强调多尺度特征提取和融合，往往忽略了解决长尾问题的重要性。与以往侧重于独立特征提取的UHR方法不同，我们强调数据增强和多模态特征融合来缓解长尾问题。在本文中，我们介绍了SRMF，一个用于超高分辨率卫星图像语义分割的新型框架。我们的方法通过结合多尺度裁剪技术，以及基于语义重排和重新采样的数据增强策略，解决了长尾类分布问题。为了进一步提高模型性能，我们提出了一种基于多模态融合的一般表示知识注入方法，该方法首次融合了文本和视觉特征，而无需单独的区域文本描述，从而提取出更稳健的特征。在URUR、GID和FBP数据集上的大量实验表明，我们的方法分别提高了mIoU的3.33%、0.66%和0.98%，达到了最先进的性能。代码可访问：<a target="_blank" rel="noopener" href="https://github.com/BinSpa/SRMF.git%E3%80%82">https://github.com/BinSpa/SRMF.git。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.19839v1">PDF</a> None</p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对超高分辨率卫星图像语义分割的长尾问题，提出了一种新的框架SRMF。该框架通过多尺度裁剪技术和基于语义重排和重采样的数据增强策略来解决长尾类分布问题。同时，引入了一种基于多模态融合的一般表示知识注入方法，融合文本和视觉特征，提取更鲁棒的特征，提高了模型性能。在URUR、GID和FBP数据集上的实验表明，该方法分别提高了mIoU指标3.33%、0.66%和0.98%，达到最新性能水平。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>超高分辨率卫星图像语义分割面临长尾问题挑战。</li>
<li>现有方法多关注多分支网络架构，但忽略了解决长尾问题的重要性。</li>
<li>SRMF框架通过多尺度裁剪和数据增强策略解决长尾类分布问题。</li>
<li>引入基于多模态融合的知识注入方法，融合文本和视觉特征，提高模型性能。</li>
<li>实验在URUR、GID和FBP数据集上验证了SRMF框架的有效性。</li>
<li>SRMF框架提高了mIoU指标，达到最新性能水平。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.19839">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d19d5cfe6ca309792db140592ab2fb59.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc5971eef3afab164c6987023a9165a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-692437a040501a925ec513a265adbb93.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-258f426453fa0f97683cd69be4794f9d.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="CoDEx-Combining-Domain-Expertise-for-Spatial-Generalization-in-Satellite-Image-Analysis"><a href="#CoDEx-Combining-Domain-Expertise-for-Spatial-Generalization-in-Satellite-Image-Analysis" class="headerlink" title="CoDEx: Combining Domain Expertise for Spatial Generalization in   Satellite Image Analysis"></a>CoDEx: Combining Domain Expertise for Spatial Generalization in   Satellite Image Analysis</h2><p><strong>Authors:Abhishek Kuriyal, Elliot Vincent, Mathieu Aubry, Loic Landrieu</strong></p>
<p>Global variations in terrain appearance raise a major challenge for satellite image analysis, leading to poor model performance when training on locations that differ from those encountered at test time. This remains true even with recent large global datasets. To address this challenge, we propose a novel domain-generalization framework for satellite images. Instead of trying to learn a single generalizable model, we train one expert model per training domain, while learning experts’ similarity and encouraging similar experts to be consistent. A model selection module then identifies the most suitable experts for a given test sample and aggregates their predictions. Experiments on four datasets (DynamicEarthNet, MUDS, OSCD, and FMoW) demonstrate consistent gains over existing domain generalization and adaptation methods. Our code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/Abhishek19009/CoDEx">https://github.com/Abhishek19009/CoDEx</a>. </p>
<blockquote>
<p>地形外观的全球变化给卫星图像分析带来了重大挑战，尤其是在训练地点与测试地点不同的情况下，模型性能往往较差。即使使用最新的全球大型数据集，情况依然如此。为了应对这一挑战，我们提出了一种新型的卫星图像领域通用化框架。我们并非试图学习单一的通用模型，而是针对每个训练领域训练一个专家模型，同时学习专家之间的相似性并鼓励相似专家保持一致。模型选择模块随后可以确定给定测试样本的最合适专家并对其预测进行汇总。在四个数据集（DynamicEarthNet、MUDS、OSCD和FMoW）上的实验表明，与现有的领域通用化和适应方法相比，该框架具有持续的优势。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/Abhishek19009/CoDEx%E4%B8%8A%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/Abhishek19009/CoDEx上公开访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.19737v1">PDF</a> CVPR 2025 EarthVision Workshop</p>
<p><strong>Summary</strong></p>
<p>本文主要探讨全球地形变化的卫星图像分析难题。虽然大型全球数据集可以一定程度上解决问题，但针对测试环境与训练环境不一致的问题，文章提出一种新型卫星图像领域泛化框架。不同于单一通用模型学习，该框架训练每个训练领域的专家模型，学习专家模型间的相似性并鼓励相似专家模型保持一致。模型选择模块为给定测试样本选择最合适的专家模型并聚合其预测结果。实验证明该框架在四个数据集上的表现优于现有领域泛化和适应方法。代码已公开在GitHub上。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>全球地形变化给卫星图像分析带来挑战。</li>
<li>即使使用大型全球数据集，模型在测试时仍可能表现不佳。</li>
<li>提出一种新型卫星图像领域泛化框架，训练针对每个训练领域的专家模型。</li>
<li>通过学习专家模型间的相似性并鼓励相似专家保持一致来提高模型性能。</li>
<li>模型选择模块能根据测试样本选择最合适的专家模型。</li>
<li>在四个数据集上的实验证明该框架表现优于现有方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.19737">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-8acc03bfcc80283986b9ff67351bfdf9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-51b8e29613b6c82c5805b594c8a91450.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a318a8dc15bb2d6221a5d7eab27c574.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-895866968ea02c61916a1847ac6f7c4d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21e47b9ac508f8cdbb8838ba99e7503e.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Prompt-Guiding-Multi-Scale-Adaptive-Sparse-Representation-driven-Network-for-Low-Dose-CT-MAR"><a href="#Prompt-Guiding-Multi-Scale-Adaptive-Sparse-Representation-driven-Network-for-Low-Dose-CT-MAR" class="headerlink" title="Prompt Guiding Multi-Scale Adaptive Sparse Representation-driven Network   for Low-Dose CT MAR"></a>Prompt Guiding Multi-Scale Adaptive Sparse Representation-driven Network   for Low-Dose CT MAR</h2><p><strong>Authors:Baoshun Shi, Bing Chen, Shaolei Zhang, Huazhu Fu, Zhanli Hu</strong></p>
<p>Low-dose CT (LDCT) is capable of reducing X-ray radiation exposure, but it will potentially degrade image quality, even yields metal artifacts at the case of metallic implants. For simultaneous LDCT reconstruction and metal artifact reduction (LDMAR), existing deep learning-based efforts face two main limitations: i) the network design neglects multi-scale and within-scale information; ii) training a distinct model for each dose necessitates significant storage space for multiple doses. To fill these gaps, we propose a prompt guiding multi-scale adaptive sparse representation-driven network, abbreviated as PMSRNet, for LDMAR task. Specifically, we construct PMSRNet inspired from multi-scale sparsifying frames, and it can simultaneously employ within-scale characteristics and cross-scale complementarity owing to an elaborated prompt guiding scale-adaptive threshold generator (PSATG) and a built multi-scale coefficient fusion module (MSFuM). The PSATG can adaptively capture multiple contextual information to generate more faithful thresholds, achieved by fusing features from local, regional, and global levels. Furthermore, we elaborate a model interpretable dual domain LDMAR framework called PDuMSRNet, and train single model with a prompt guiding strategy for multiple dose levels. We build a prompt guiding module, whose input contains dose level, metal mask and input instance, to provide various guiding information, allowing a single model to accommodate various CT dose settings. Extensive experiments at various dose levels demonstrate that the proposed methods outperform the state-of-the-art LDMAR methods. </p>
<blockquote>
<p>低剂量CT（LDCT）能够减少X射线辐射暴露，但可能会降低图像质量，甚至在金属植入物的情况下产生金属伪影。对于同时进行的LDCT重建和金属伪影减少（LDMAR），现有的基于深度学习的方法面临两个主要局限性：i）网络设计忽略了多尺度和尺度内的信息；ii）为每个剂量训练一个独特的模型需要大量的存储空间来应对多种剂量。为了填补这些空白，我们提出了一种基于即时引导的多尺度自适应稀疏表示驱动网络（简称PMSRNet），用于LDMAR任务。具体来说，我们受到多尺度稀疏框架的启发构建了PMSRNet，它可以同时利用尺度内的特征和跨尺度的互补性，这得益于精心设计的即时引导尺度自适应阈值生成器（PSATG）和内置的多尺度系数融合模块（MSFuM）。PSATG可以自适应地捕获多种上下文信息来生成更真实的阈值，这是通过融合本地、区域和全局级别的特征来实现的。此外，我们详细阐述了一种模型可解释的双域LDMAR框架，称为PDuMSRNet，并采用即时引导策略为多个剂量水平训练单一模型。我们建立了一个即时引导模块，其输入包括剂量水平、金属掩码和输入实例，以提供多种指导信息，使单一模型能够适应各种CT剂量设置。在不同剂量水平上的大量实验表明，所提出的方法优于现有的最先进的LDMAR方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.19687v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该文本介绍了一种针对低剂量计算机断层扫描（LDCT）的金属伪影减少技术。文章提出了一个基于深度学习的方法，即提示引导多尺度自适应稀疏表示驱动网络（PMSRNet），用于同时实现LDCT重建和金属伪影减少（LDMAR）。该网络设计灵感来源于多尺度稀疏框架，可以充分利用尺度内特性和跨尺度互补性。此外，文章还介绍了一种模型可解释的跨域LDMAR框架（PDuMSRNet），通过提示引导策略训练单一模型以适应多种剂量水平。该方法的性能在多个剂量水平上进行了广泛实验验证，证明了其优于现有LDMAR方法的效果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>低剂量计算机断层扫描（LDCT）可以减少X射线辐射暴露，但可能导致图像质量下降，甚至出现金属伪影。</li>
<li>现有的基于深度学习的LDMAR方法面临两个主要限制：忽视多尺度和尺度内信息，以及为每个剂量水平训练不同模型需要大量存储空间。</li>
<li>提出的PMSRNet网络受到多尺度稀疏框架的启发，可以充分利用尺度内特性和跨尺度互补性，通过精细的提示引导尺度自适应阈值生成器和多尺度系数融合模块实现LDMAR。</li>
<li>提示引导模块包含剂量水平、金属掩码和输入实例，为各种剂量设置提供指导信息，使单一模型适应各种CT剂量水平。</li>
<li>PDuMSRNet框架是一种模型可解释的跨域LDMAR框架，通过提示引导策略训练单一模型，以处理多种剂量水平的LDCT图像。</li>
<li>广泛实验表明，所提出的方法在多个剂量水平上优于现有的LDMAR方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.19687">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6e479c7a843c604677a86e76b98f9f81.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2ecd3ada83967f6ed1e390e8427bd52f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-34b460f8051096779b86126564393a23.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="On-Neutron-Star-Natal-Kicks-in-High-Mass-X-Ray-Binaries-Insights-from-Population-Synthesis"><a href="#On-Neutron-Star-Natal-Kicks-in-High-Mass-X-Ray-Binaries-Insights-from-Population-Synthesis" class="headerlink" title="On Neutron Star Natal Kicks in High-Mass X-Ray Binaries: Insights from   Population Synthesis"></a>On Neutron Star Natal Kicks in High-Mass X-Ray Binaries: Insights from   Population Synthesis</h2><p><strong>Authors:Xiangyu Ivy Wang, Xiang-Dong Li</strong></p>
<p>The motion of neutron stars (NSs) in the Galaxy is largely dependent on natal kicks received by the NSs during supernova explosions. Thus, the measured peculiar velocities of NS high-mass X-ray binaries (HMXBs) provide valuable clues to natal kicks, which also play an important role in the evolution of HMXBs. In this work, we collect proper motions, radial velocities and parallaxes for 36 NS HMXBs to derive their peculiar velocities at the birth of the NSs. We then use binary population synthesis to simulate the velocities of NS HMXBs with various choices of the kick velocity distribution for both core-collapse and electron-capture supernovae. Comparing the simulated and measured velocities, orbital periods, and eccentricities, we show that the natal kick distribution that can best match the observations is characterized by a bimodal Maxwellian distribution with $\sigma_1$ &#x3D; 320 km s$^{-1}$ (for core-collapse supernovae) and $\sigma_2$ &#x3D; 80 km s$^{-1}$ (for electron-capture supernovae) and the He core mass for the latter in the range of $(1.83-2.25)$ $M_{\odot}$. Our findings provide useful insights for further population synthesis and binary evolution studies of NS binaries. </p>
<blockquote>
<p>中子星（NSs）在银河系中的运动在很大程度上取决于超新星爆发期间NSs所受到的初生冲击。因此，测量中子星高质量X射线双星（HMXBs）的特定速度提供了关于初生冲击的宝贵线索，这些线索在HMXBs的演化中也起着重要作用。在这项工作中，我们收集了36个中子星HMXB的自行运动、速度和视差距离，以导出中子星诞生时的特殊速度。然后，我们使用二进制人口合成法模拟了各种中子星初生冲击速度分布下的NS HMXB速度，包括核心崩溃型和电子俘获型超新星。通过比较模拟和测量的速度、轨道周期和离心率，我们发现最能与观测相匹配的初生冲击分布特征是具有双峰麦克斯韦分布，其中σ1&#x3D; 320公里每秒（适用于核心崩溃型超新星）和σ2&#x3D; 80公里每秒（适用于电子俘获型超新星），后者的氦核质量范围在（1.83-2.25）$M_{\odot}$之间。我们的发现为进一步的人口合成学和NS双星的二元演化研究提供了有益的见解。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.19672v1">PDF</a> 16 pages, 4 figures, 3 tables, accepted by ApJ</p>
<p><strong>Summary</strong></p>
<p>中子星在银河系中的运动受到其诞生时超新星爆炸所带来的初始速度影响。通过分析中子星高质量X射线双星（HMXBs）的特殊速度，可以了解初始速度的影响，这对理解HMXBs的演化至关重要。本研究收集了36个NS HMXB的天体运动学数据，推算出在诞生时中子星NS的特殊速度。通过二元合成人口模型模拟特殊速度，与多种模型比较核心塌缩型和电子捕获型超新星的中子星速度的分布特性。比较模拟与观测得到的速度、轨道周期和偏心度等结果后显示最佳匹配的初生状态中性流体速度为包含一组峰值为核心塌缩超新星的中均方根速度为$\sigma_1$&#x3D; 320千米每秒，另一组为电子捕获型超新星中均方根速度为$\sigma_2$&#x3D; 80千米每秒，后者的He核心质量在$(1.83-2.25)$M☉范围内。我们的研究对于进一步的二元合成人口模型和二元演化研究提供有价值的信息。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>中子星在银河系中的运动受到超新星爆炸带来的初生速度影响。</li>
<li>高质量X射线双星（HMXBs）的特殊速度提供了关于初生速度的线索。</li>
<li>研究收集了NS HMXBs的天体运动学数据以推算中子星诞生时的特殊速度。</li>
<li>使用二元人口合成模型模拟特殊速度并对比多种模型下的中子星速度分布特性。</li>
<li>最佳匹配的初生状态中性流体速度分布表现为双峰Maxwellian分布，其中核心塌缩超新星的中均方根速度为320千米每秒，另一组为电子捕获型超新星的中均方根速度为80千米每秒。</li>
<li>电子捕获型超新星的He核心质量范围在$(1.83-2.25)$M☉之间。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.19672">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f57fedae1386311c4c59da01b98a1aac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-99efb1fb9e574640d6bd450b8c031a44.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9b6033a98589a0175744c342769cc7f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-267069d83647e8108dee1ed46490a082.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="A-Unified-Benchmark-of-Federated-Learning-with-Kolmogorov-Arnold-Networks-for-Medical-Imaging"><a href="#A-Unified-Benchmark-of-Federated-Learning-with-Kolmogorov-Arnold-Networks-for-Medical-Imaging" class="headerlink" title="A Unified Benchmark of Federated Learning with Kolmogorov-Arnold   Networks for Medical Imaging"></a>A Unified Benchmark of Federated Learning with Kolmogorov-Arnold   Networks for Medical Imaging</h2><p><strong>Authors:Youngjoon Lee, Jinu Gong, Joonhyuk Kang</strong></p>
<p>Federated Learning (FL) enables model training across decentralized devices without sharing raw data, thereby preserving privacy in sensitive domains like healthcare. In this paper, we evaluate Kolmogorov-Arnold Networks (KAN) architectures against traditional MLP across six state-of-the-art FL algorithms on a blood cell classification dataset. Notably, our experiments demonstrate that KAN can effectively replace MLP in federated environments, achieving superior performance with simpler architectures. Furthermore, we analyze the impact of key hyperparameters-grid size and network architecture-on KAN performance under varying degrees of Non-IID data distribution. Additionally, our ablation studies reveal that optimizing KAN width while maintaining minimal depth yields the best performance in federated settings. As a result, these findings establish KAN as a promising alternative for privacy-preserving medical imaging applications in distributed healthcare. To the best of our knowledge, this is the first comprehensive benchmark of KAN in FL settings for medical imaging task. </p>
<blockquote>
<p>联邦学习（FL）能够在分散的设备上进行模型训练，无需共享原始数据，从而在医疗等敏感领域保护隐私。在本文中，我们评估了Kolmogorov-Arnold网络（KAN）架构与传统多层感知机（MLP）在六种最先进联邦学习算法上的表现，这些数据来自于血细胞分类数据集。值得注意的是，我们的实验表明，在联邦环境中，KAN可以有效地替代MLP，以更简单的架构实现卓越的性能。此外，我们分析了关键超参数——网格大小和网络架构在不同程度的非独立同分布（Non-IID）数据分布下对KAN性能的影响。另外，我们的消融研究结果表明，在保持深度最小的情况下优化KAN的宽度，在联邦环境中可以获得最佳性能。因此，这些发现确立了KAN在分布式医疗保健中的隐私保护医学成像应用的潜力。据我们所知，这是首次在联邦学习环境中对KAN进行医学成像任务的全面基准测试。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.19639v1">PDF</a> 5 pages</p>
<p><strong>Summary</strong></p>
<p>本文探讨了Federated Learning（FL）在医疗图像分类中的表现，评估了Kolmogorov-Arnold Networks（KAN）与传统多层感知机（MLP）在六种先进的FL算法上的性能差异。实验表明，在联邦环境中，KAN能够替代MLP实现更优越的性能，特别是在处理非独立同分布（Non-IID）数据时表现突出。此外，研究还发现优化KAN的宽度并保持较小的深度有助于提高性能。因此，KAN有望成为分布式医疗成像应用中隐私保护的有力替代方案。本文是对医疗图像领域中的KAN在FL环境下的首次全面评估。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Federated Learning (FL) 能够在不共享原始数据的情况下进行模型训练，保护隐私敏感领域如医疗的数据隐私。</li>
<li>Kolmogorov-Arnold Networks (KAN) 在Federated Learning环境中相较于传统多层感知机（MLP）展现出优越性能。</li>
<li>KAN在处理非独立同分布（Non-IID）数据时表现良好。</li>
<li>KAN的性能受到网格大小和网络架构等超参数的影响。</li>
<li>在联邦环境中，优化KAN的宽度并维持较小的深度可以获得最佳性能。</li>
<li>此研究首次全面评估了KAN在医疗图像领域的Federated Learning环境中的表现。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.19639">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8b09222d324549a8b2f8151ba8ff5cc6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6da4960f95bd7e3300a745719ecd5073.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fa6aa1a138dce7d2aaee5454f70803b9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-82f17ee4668d8a114f9a3dcc6e796473.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b8eb4d02f732b2a17ad5d1226818131.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b581ed6bdcb464932fcc1385b1a14c3c.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="NSegment-Noisy-Segment-Improves-Remote-Sensing-Image-Segmentation"><a href="#NSegment-Noisy-Segment-Improves-Remote-Sensing-Image-Segmentation" class="headerlink" title="NSegment : Noisy Segment Improves Remote Sensing Image Segmentation"></a>NSegment : Noisy Segment Improves Remote Sensing Image Segmentation</h2><p><strong>Authors:Yechan Kim, DongHo Yoon, SooYeon Kim, Moongu Jeon</strong></p>
<p>Labeling errors in remote sensing (RS) image segmentation datasets often remain implicit and subtle due to ambiguous class boundaries, mixed pixels, shadows, complex terrain features, and subjective annotator bias. Furthermore, the scarcity of annotated RS data due to high image acquisition and labeling costs complicates training noise-robust models. While sophisticated mechanisms such as label selection or noise correction might address this issue, they tend to increase training time and add implementation complexity. In this letter, we propose NSegment-a simple yet effective data augmentation solution to mitigate this issue. Unlike traditional methods, it applies elastic transformations only to segmentation labels, varying deformation intensity per sample in each training epoch to address annotation inconsistencies. Experimental results demonstrate that our approach improves the performance of RS image segmentation on various state-of-the-art models. </p>
<blockquote>
<p>遥感（RS）图像分割数据集标注错误通常由于模糊的类别边界、混合像素、阴影、复杂的地形特征和主观标注者偏见而保持隐蔽和微妙。此外，由于图像采集和标注的高成本，导致标注的遥感数据稀缺，这加剧了训练噪声鲁棒模型的复杂性。虽然标签选择或噪声校正等复杂机制可能解决此问题，但它们往往会增加训练时间并增加实现复杂性。在本信中，我们提出NSegment——一种简单有效的数据增强解决方案，以缓解这个问题。与传统的数据增强方法不同，它仅对分割标签应用弹性变换，并在每个训练周期中对每个样本的变形强度进行变化，以解决标注不一致的问题。实验结果表明，我们的方法提高了各种最先进模型在遥感图像分割方面的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.19634v1">PDF</a> Preprint</p>
<p><strong>Summary</strong></p>
<p>本文主要讨论了遥感图像分割数据集标注错误的问题，这些错误由于模糊的类别边界、混合像素、阴影、复杂地形特征和主观标注者偏见而隐性存在。由于高图像采集和标注成本，导致标注的遥感数据稀缺，训练噪声鲁棒性模型变得复杂。针对这些问题，本文提出了一种名为NSegment的简单而有效的数据增强解决方案。不同于传统方法，NSegment仅对分割标签应用弹性变换，每个训练周期中对每个样本的变形强度进行变化，以解决标注不一致的问题。实验结果表明，该方法提高了遥感图像分割在各种先进模型上的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>遥感图像分割数据集的标注错误是隐性和微妙的，主要源于模糊的类别边界、混合像素、阴影等问题。</li>
<li>标注的遥感数据稀缺，因为图像采集和标注的成本很高。</li>
<li>训练噪声鲁棒性模型是必要的，因为标注错误会影响模型的性能。</li>
<li>传统数据增强方法可能不适用于遥感图像分割问题。</li>
<li>NSegment是一种简单而有效的数据增强解决方案，它通过应用弹性变换来解决标注不一致的问题。</li>
<li>NSegment只在每个训练周期中对分割标签应用变换，以提高模型的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.19634">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-16c6d0523c8226596deead44539d0fa0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36739d39a34aef98deccd3b6b1197410.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-662089891e12ecb22aad3a2c43059dc9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5995b096b27118f607b5d98b2c6e3c25.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fc9a37c36796ae214bc4ea3f0fda136c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b0c1ab6b622b3020b334a50067590b7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-66961e898e1a789581cfa9988bc0ec62.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ebe8bf68217760045ae5c75df162de4b.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="AI-Alignment-in-Medical-Imaging-Unveiling-Hidden-Biases-Through-Counterfactual-Analysis"><a href="#AI-Alignment-in-Medical-Imaging-Unveiling-Hidden-Biases-Through-Counterfactual-Analysis" class="headerlink" title="AI Alignment in Medical Imaging: Unveiling Hidden Biases Through   Counterfactual Analysis"></a>AI Alignment in Medical Imaging: Unveiling Hidden Biases Through   Counterfactual Analysis</h2><p><strong>Authors:Haroui Ma, Francesco Quinzan, Theresa Willem, Stefan Bauer</strong></p>
<p>Machine learning (ML) systems for medical imaging have demonstrated remarkable diagnostic capabilities, but their susceptibility to biases poses significant risks, since biases may negatively impact generalization performance. In this paper, we introduce a novel statistical framework to evaluate the dependency of medical imaging ML models on sensitive attributes, such as demographics. Our method leverages the concept of counterfactual invariance, measuring the extent to which a model’s predictions remain unchanged under hypothetical changes to sensitive attributes. We present a practical algorithm that combines conditional latent diffusion models with statistical hypothesis testing to identify and quantify such biases without requiring direct access to counterfactual data. Through experiments on synthetic datasets and large-scale real-world medical imaging datasets, including \textsc{cheXpert} and MIMIC-CXR, we demonstrate that our approach aligns closely with counterfactual fairness principles and outperforms standard baselines. This work provides a robust tool to ensure that ML diagnostic systems generalize well, e.g., across demographic groups, offering a critical step towards AI safety in healthcare. Code: <a target="_blank" rel="noopener" href="https://github.com/Neferpitou3871/AI-Alignment-Medical-Imaging">https://github.com/Neferpitou3871/AI-Alignment-Medical-Imaging</a>. </p>
<blockquote>
<p>医学影像机器学习系统展现出了出色的诊断能力，但它们易受偏见影响，这构成了重大风险，因为偏见可能会给泛化性能带来负面影响。在本文中，我们介绍了一种新型统计框架，用于评估医学影像机器学习模型对人口统计学等敏感属性的依赖程度。我们的方法利用反事实不变性的概念，衡量模型预测在敏感属性假设性变化下保持不变的程度。我们提出了一种实用算法，该算法结合了条件潜在扩散模型和统计假设检验，能够在无需访问反事实数据的情况下识别和量化此类偏见。通过在合成数据集和大规模现实世界医学影像数据集（包括\textsc{cheXpert}和MIMIC-CXR）上的实验，我们证明我们的方法与反事实公平原则紧密契合并优于标准基线。这项工作为确保机器学习诊断系统具有良好的泛化能力（例如，跨越不同人口群体）提供了可靠工具，是医疗保健领域人工智能安全性的关键一步。代码：<a target="_blank" rel="noopener" href="https://github.com/Neferpitou3871/AI-Alignment-Medical-Imaging%E3%80%82">https://github.com/Neferpitou3871/AI-Alignment-Medical-Imaging。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.19621v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种新型的统计框架，用于评估医疗影像机器学习模型对敏感属性（如人口统计学特征）的依赖程度。该框架利用反事实不变性的概念，测量模型预测在敏感属性假设变化下的稳定性。通过结合条件潜在扩散模型和统计假设检验的实用算法，能够在无需反事实数据的情况下识别和量化偏见。在合成数据集和大规模真实医疗影像数据集上的实验表明，该方法与反事实公平性原则相符，并优于标准基线。此工具可确保机器学习诊断系统具有良好的泛化性能，例如跨不同人群，为医疗保健中的AI安全提供了关键步骤。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>医疗影像机器学习模型存在对敏感属性（如人口统计学特征）的偏见，可能影响模型的泛化性能。</li>
<li>新型统计框架利用反事实不变性的概念来评估模型预测的稳定性。</li>
<li>提出的实用算法结合了条件潜在扩散模型和统计假设检验，以识别和量化偏见。</li>
<li>该方法无需反事实数据即可操作，可在真实世界的应用中提供便利。</li>
<li>实验结果表明，该方法符合反事实公平性原则，并优于标准基线。</li>
<li>此工具有助于提高机器学习诊断系统的泛化性能，特别是在不同人群之间。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.19621">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-697eab417cfb0a3c638b2efa916ee444.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-07d637465e9b81f9732b8a6c72aa96ee.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Magnifier-A-Multi-grained-Neural-Network-based-Architecture-for-Burned-Area-Delineation"><a href="#Magnifier-A-Multi-grained-Neural-Network-based-Architecture-for-Burned-Area-Delineation" class="headerlink" title="Magnifier: A Multi-grained Neural Network-based Architecture for Burned   Area Delineation"></a>Magnifier: A Multi-grained Neural Network-based Architecture for Burned   Area Delineation</h2><p><strong>Authors:Daniele Rege Cambrin, Luca Colomba, Paolo Garza</strong></p>
<p>In crisis management and remote sensing, image segmentation plays a crucial role, enabling tasks like disaster response and emergency planning by analyzing visual data. Neural networks are able to analyze satellite acquisitions and determine which areas were affected by a catastrophic event. The problem in their development in this context is the data scarcity and the lack of extensive benchmark datasets, limiting the capabilities of training large neural network models. In this paper, we propose a novel methodology, namely Magnifier, to improve segmentation performance with limited data availability. The Magnifier methodology is applicable to any existing encoder-decoder architecture, as it extends a model by merging information at different contextual levels through a dual-encoder approach: a local and global encoder. Magnifier analyzes the input data twice using the dual-encoder approach. In particular, the local and global encoders extract information from the same input at different granularities. This allows Magnifier to extract more information than the other approaches given the same set of input images. Magnifier improves the quality of the results of +2.65% on average IoU while leading to a restrained increase in terms of the number of trainable parameters compared to the original model. We evaluated our proposed approach with state-of-the-art burned area segmentation models, demonstrating, on average, comparable or better performances in less than half of the GFLOPs. </p>
<blockquote>
<p>在危机管理和遥感领域，图像分割发挥着至关重要的作用，通过分析视觉数据，能够完成灾难响应和应急规划等任务。神经网络能够分析卫星采集的数据，并确定哪些区域受到灾难性事件的影响。然而，在此背景之下，神经网络的发展问题在于数据稀缺，缺乏广泛的基准数据集，这限制了训练大型神经网络模型的能力。在本文中，我们提出了一种新的方法，即Magnifier，以提高在数据有限情况下的分割性能。Magnifier方法适用于任何现有的编码器-解码器架构，因为它通过双编码器方法合并不同上下文级别的信息来扩展模型：本地编码器和全局编码器。Magnifier使用双编码器方法两次分析输入数据。特别是，本地编码器和全局编码器以不同的粒度从同一输入中提取信息。这使得Magnifier能够在给定相同输入图像集的情况下，提取比其他方法更多的信息。Magnifier在提高平均IoU值的基础上提高了结果质量达+2.65%，并且在与原始模型相比的情况下，可训练参数数量方面实现了适度的增长。我们使用先进的燃烧区域分割模型评估了我们提出的方法，结果显示，在不到一半GFLOPs的情况下，我们的方法能够达到平均相当或更好的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.19589v1">PDF</a> Accepted in IEEE Journal of Selected Topics in Applied Earth   Observations and Remote Sensing</p>
<p><strong>Summary</strong></p>
<p>神经网络在分析卫星图像数据、确定受灾区域方面发挥了重要作用。然而，数据稀缺和缺乏广泛的基准数据集限制了大型神经网络模型的发展。本文提出了一种名为Magnifier的新方法，可在有限数据的情况下提高分割性能。Magnifier方法适用于任何现有的编码器-解码器架构，它通过双编码器方法合并不同上下文级别的信息来扩展模型：本地和全局编码器。Magnifier使用双编码器方法两次分析输入数据，使得在相同的输入图像集上，它能提取比其他方法更多的信息。Magnifier提高了平均IoU值达+2.65%，同时与原始模型相比，增加了较少的可训练参数数量。本研究通过先进的燃烧区域分割模型评估了该方法，在GFLOPs不到一半的情况下，表现出平均相当或更好的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>神经网络在分析卫星图像数据方面具有重要作用，可应用于灾难响应和应急规划等任务。</li>
<li>数据稀缺和缺乏基准数据集限制了大型神经网络模型在危机管理和遥感图像分割中的发展。</li>
<li>Magnifier是一种新的方法，旨在解决有限数据下的图像分割问题，适用于各种编码器-解码器架构。</li>
<li>Magnifier通过双编码器方法（本地和全局编码器）两次分析输入数据，提取不同粒度下的信息。</li>
<li>Magnifier能提高图像分割的性能，平均IoU值提高了+2.65%。</li>
<li>与其他先进的燃烧区域分割模型相比，Magnifier在较少的GFLOPs下表现出相当或更好的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.19589">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a4e2520ec59986fd02a6d4eb5e81bfaf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eda442e4901fbbf2bb6f0058288c75fa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4032239a967968f54e448324221fcf27.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2e9805ba813077d5e31940362759e61f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a4bd3700f089e71f7d09108a7c9f30b3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-817c18881b92e155ce3b3e227a46fc5b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef36a1c66b66c9014d530dd25f6cf485.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Maximizing-Infrared-Transmission-Contrast-Upon-Phase-Transition-of-Thermally-Grown-Vanadium-Dioxide-Thin-Films-by-Rapid-Thermal-Processing"><a href="#Maximizing-Infrared-Transmission-Contrast-Upon-Phase-Transition-of-Thermally-Grown-Vanadium-Dioxide-Thin-Films-by-Rapid-Thermal-Processing" class="headerlink" title="Maximizing Infrared Transmission Contrast Upon Phase Transition of   Thermally Grown Vanadium Dioxide Thin Films by Rapid Thermal Processing"></a>Maximizing Infrared Transmission Contrast Upon Phase Transition of   Thermally Grown Vanadium Dioxide Thin Films by Rapid Thermal Processing</h2><p><strong>Authors:Ken Araki, Vishwa Krishna Rajan, Liping Wang</strong></p>
<p>Pristine vanadium dioxide (VO2), an insulator-to-metal transition (IMT) material, is grown via furnace oxidation followed by rapid thermal annealing with forming gas (5%H2&#x2F;95%N2) which reduces surface over-oxides such as V2O5 formed during the oxidation. The evolutional IMT behaviors of the thermochromic film and vanadium oxide states over different reduction time are systematically studied with temperature-dependent infrared spectrometry, electrical resistivity, and X-ray diffraction measurements. After optimally reducing surface over-oxides to VO2, infrared transmission contrast upon phase transition is enhanced to 46% (at 9 um wavelength) compared to 23% from fully oxidation without any reduction. Moreover, pristine VO2 thin film obtained from thermal oxidation and optimal reduction processes exhibits sharp phase transition and narrow thermal hysteresis within 2~4{\deg}C in both infrared transmission and electrical resistivity, which are comparable to the VO2 of best quality prepared by other sophisticated fabrication techniques. The thermally grown method presented here would facilitate the scalable fabrication of high-quality VO2 thin films and tunable radiative coatings for high-performance thermal control applications. </p>
<blockquote>
<p>纯净的二氧化钒（VO2）是一种绝缘体到金属转变（IMT）材料，通过炉氧化和形成气体（5%H2&#x2F;95%N2）的快速热退火进行生长，以减少在氧化过程中产生的表面过氧化物，如五氧化二钒（V2O5）。通过温度依赖的红外光谱法、电阻率和X射线衍射测量，系统研究了热变色膜和氧化钒在不同还原时间下的演变IMT行为。通过最优方式还原表面过氧化物至VO2后，与未进行任何还原的完全氧化相比，在相变时红外透射对比度增强至46%（在9微米波长下），增强了近红外波段的光学响应。此外，通过热氧化和最佳还原过程获得的纯净VO2薄膜在红外透射率和电阻率方面表现出锐利的相变和狭窄的热滞回线（在2至4摄氏度内），这与通过其他先进制造技术制备的VO2质量相当。这里展示的热生长方法将有助于实现高质量VO2薄膜和可调辐射涂层的规模化制造，用于高性能热控制应用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.19520v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了通过炉氧化和形成气体（5%H2&#x2F;95%N2）快速热退火制备高纯度二氧化钒（VO2）的过程。系统地研究了其在绝缘体-金属转变（IMT）行为中，热变色膜在不同还原时间下的红外光谱、电阻率和X射线衍射的测量变化。通过优化表面氧化物，红外传输对比度的相变增强至46%（在9微米波长下），相较于完全氧化未经任何还原处理的样品增强了2倍。此外，通过热氧化和最佳还原过程获得的VO2薄膜展现出锐利的相变和狭窄的热滞回线，与通过其他先进制造技术制备的VO2薄膜质量相当。所提出的热生长方法将有助于实现高性能热控制应用中高质量VO2薄膜和可调辐射涂层的规模化制造。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>通过炉氧化和形成气体快速热退火制备了VO2材料。</li>
<li>系统研究了VO2材料在不同还原时间的绝缘体-金属转变行为。</li>
<li>优化表面氧化物后，红外传输对比度的相变增强至46%。</li>
<li>高质量VO2薄膜展现了锐利的相变和狭窄的热滞回线。</li>
<li>热生长方法有助于实现高质量VO2薄膜的大规模制造。</li>
<li>这种技术可应用于高性能热控制应用中。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.19520">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-404ab803dfe3600d227d884eebcdd393.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="MediAug-Exploring-Visual-Augmentation-in-Medical-Imaging"><a href="#MediAug-Exploring-Visual-Augmentation-in-Medical-Imaging" class="headerlink" title="MediAug: Exploring Visual Augmentation in Medical Imaging"></a>MediAug: Exploring Visual Augmentation in Medical Imaging</h2><p><strong>Authors:Xuyin Qi, Zeyu Zhang, Canxuan Gang, Hao Zhang, Lei Zhang, Zhiwei Zhang, Yang Zhao</strong></p>
<p>Data augmentation is essential in medical imaging for improving classification accuracy, lesion detection, and organ segmentation under limited data conditions. However, two significant challenges remain. First, a pronounced domain gap between natural photographs and medical images can distort critical disease features. Second, augmentation studies in medical imaging are fragmented and limited to single tasks or architectures, leaving the benefits of advanced mix-based strategies unclear. To address these challenges, we propose a unified evaluation framework with six mix-based augmentation methods integrated with both convolutional and transformer backbones on brain tumour MRI and eye disease fundus datasets. Our contributions are threefold. (1) We introduce MediAug, a comprehensive and reproducible benchmark for advanced data augmentation in medical imaging. (2) We systematically evaluate MixUp, YOCO, CropMix, CutMix, AugMix, and SnapMix with ResNet-50 and ViT-B backbones. (3) We demonstrate through extensive experiments that MixUp yields the greatest improvement on the brain tumor classification task for ResNet-50 with 79.19% accuracy and SnapMix yields the greatest improvement for ViT-B with 99.44% accuracy, and that YOCO yields the greatest improvement on the eye disease classification task for ResNet-50 with 91.60% accuracy and CutMix yields the greatest improvement for ViT-B with 97.94% accuracy. Code will be available at <a target="_blank" rel="noopener" href="https://github.com/AIGeeksGroup/MediAug">https://github.com/AIGeeksGroup/MediAug</a>. </p>
<blockquote>
<p>数据增强在医学成像中至关重要，对于在有限数据条件下提高分类精度、病变检测以及器官分割都至关重要。然而，仍存在两个重大挑战。首先，自然照片和医学图像之间明显的领域差距可能会扭曲关键疾病特征。其次，医学成像中的增强研究是分散的，仅限于单一任务或架构，使得先进的混合策略的优势尚不清楚。为了应对这些挑战，我们提出了一个统一的评估框架，该框架集成了六种基于混合的方法，并结合卷积和transformer骨干网在脑肿瘤MRI和眼底疾病数据集上进行数据增强。我们的贡献主要体现在三个方面。（1）我们引入了MediAug，这是医学成像中高级数据增强的综合和可重现的基准。 （2）我们系统地评估了MixUp、YOCO、CropMix、CutMix、AugMix和SnapMix使用ResNet-50和ViT-B骨干网的效果。（3）我们通过大量实验证明，对于ResNet-50，MixUp在脑肿瘤分类任务上取得了最大的改进，准确率为79.19%；对于ViT-B，SnapMix取得了最大的改进，准确率为99.44%；YOCO在ResNet-50的眼病分类任务上取得了最大改进，准确率为91.60%；而CutMix在ViT-B上取得了最大的改进，准确率为97.94%。代码将在<a target="_blank" rel="noopener" href="https://github.com/AIGeeksGroup/MediAug%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/AIGeeksGroup/MediAug上提供。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.18983v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了数据增强在医学成像中的重要性，针对医学图像分类、病灶检测和器官分割等任务，提出了一个统一评估框架。该框架集成了六种基于混合的数据增强方法，对卷积和transformer架构进行了评估。研究发现，MixUp在脑肿瘤分类任务上表现最佳，SnapMix在ViT-B上表现最佳，YOCO在眼疾分类任务上表现最佳。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>数据增强在医学成像中非常重要，尤其对于任务如分类、病灶检测和器官分割。</li>
<li>医学图像数据增强面临两大挑战：与自然照片之间的域差距以及缺乏统一的评估框架。</li>
<li>提出了一个统一的评估框架，该框架集成了六种基于混合的数据增强方法。</li>
<li>研究表明MixUp在脑肿瘤分类任务上表现最佳，SnapMix在ViT-B架构上表现最佳。</li>
<li>YOCO在眼疾分类任务上取得了最好的结果。</li>
<li>该研究提供了一个全面的医学图像数据增强基准测试平台MediAug。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.18983">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a1ff6ee08ab3b5add9f47db7965ec06a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7c75628b833a1a6bc4bc181cfbefbdcb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bd328dbb43cecaafe4eda12add447570.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-855ac7e35d84d38f272608bb848e4b89.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Theoretical-Framework-for-Tempered-Fractional-Gradient-Descent-Application-to-Breast-Cancer-Classification"><a href="#Theoretical-Framework-for-Tempered-Fractional-Gradient-Descent-Application-to-Breast-Cancer-Classification" class="headerlink" title="Theoretical Framework for Tempered Fractional Gradient Descent:   Application to Breast Cancer Classification"></a>Theoretical Framework for Tempered Fractional Gradient Descent:   Application to Breast Cancer Classification</h2><p><strong>Authors:Omar Naifar</strong></p>
<p>This paper introduces Tempered Fractional Gradient Descent (TFGD), a novel optimization framework that synergizes fractional calculus with exponential tempering to enhance gradient-based learning. Traditional gradient descent methods often suffer from oscillatory updates and slow convergence in high-dimensional, noisy landscapes. TFGD addresses these limitations by incorporating a tempered memory mechanism, where historical gradients are weighted by fractional coefficients $|w_j| &#x3D; \binom{\alpha}{j}$ and exponentially decayed via a tempering parameter $\lambda$. Theoretical analysis establishes TFGD’s convergence guarantees: in convex settings, it achieves an $\mathcal{O}(1&#x2F;K)$ rate with alignment coefficient $d_{\alpha,\lambda} &#x3D; (1 - e^{-\lambda})^{-\alpha}$, while stochastic variants attain $\mathcal{O}(1&#x2F;k^\alpha)$ error decay. The algorithm maintains $\mathcal{O}(n)$ time complexity equivalent to SGD, with memory overhead scaling as $\mathcal{O}(d&#x2F;\lambda)$ for parameter dimension $d$. Empirical validation on the Breast Cancer Wisconsin dataset demonstrates TFGD’s superiority, achieving 98.25% test accuracy (vs. 92.11% for SGD) and 2$\times$ faster convergence. The tempered memory mechanism proves particularly effective in medical classification tasks, where feature correlations benefit from stable gradient averaging. These results position TFGD as a robust alternative to conventional optimizers in both theoretical and applied machine learning. </p>
<blockquote>
<p>本文介绍了温度化分数梯度下降法（Tempered Fractional Gradient Descent，简称TFGD）这一新型优化框架。该框架将分数微积分与指数温度化相结合，以提升基于梯度的学习能力。传统的梯度下降法在高维、有噪声的地形上常遭遇振荡更新和收敛缓慢的问题。TFGD通过引入温度化记忆机制来解决这些问题，其中历史梯度通过分数系数$|w_j| &#x3D; \binom{\alpha}{j}$进行加权，并通过温度参数$\lambda$进行指数衰减。理论分析证明了TFGD的收敛性保证：在凸设置中，它实现了与对齐系数$d_{\alpha,\lambda} &#x3D; (1 - e^{-\lambda})^{-\alpha}$相关的$\mathcal{O}(1&#x2F;K)$速率；而随机变量则达到$\mathcal{O}(1&#x2F;k^\alpha)$误差衰减。该算法的时间复杂度与SGD相当，为$\mathcal{O}(n)$，而内存开销随着参数维度$d$和温度参数$\lambda$之比的增加而增加。在威斯康辛乳腺癌数据集上的实证验证表明TFGD的优越性，其测试准确率达到98.25%（相比之下，SGD为92.11%），并且收敛速度是SGD的2倍。温度化记忆机制在医疗分类任务中表现尤其出色，其中特征相关性受益于稳定的梯度平均。这些结果使TFGD成为理论和应用机器学习领域中常规优化器的稳健替代品。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.18849v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>TFGD是一种结合分数微积分和指数衰减机制的新型优化框架，旨在提高基于梯度的学习效率并解决传统梯度下降方法在解决高维噪声景观时的局限性。TFGD采用一种加权分数系数的温度记忆机制，实现理论上的收敛保证，并在实证测试中展现了出色的性能，特别是在医学分类任务中表现突出。相较于传统优化器，TFGD提供了稳健的替代方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TFGD是一种新型优化框架，融合了分数微积分与指数衰减机制，增强了梯度学习的性能。</li>
<li>传统梯度下降方法在处理高维噪声景观时面临震荡更新和缓慢收敛的问题，而TFGD则针对这些局限进行了改进。</li>
<li>TFGD采用温度记忆机制，通过加权分数系数和指数衰减参数来实现优化。</li>
<li>在凸设置下，TFGD实现了特定的收敛率，并且其随机变体还实现了误差衰减。</li>
<li>TFGD的时间复杂度与SGD相当，但其记忆开销随参数维度的增加而增加。</li>
<li>在乳腺癌威斯康星数据集上的实证验证表明，TFGD相较于SGD具有更高的测试准确性和更快的收敛速度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.18849">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-da46fc991a1275a3eae3b508284f1df9.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Physics-Driven-Neural-Compensation-For-Electrical-Impedance-Tomography"><a href="#Physics-Driven-Neural-Compensation-For-Electrical-Impedance-Tomography" class="headerlink" title="Physics-Driven Neural Compensation For Electrical Impedance Tomography"></a>Physics-Driven Neural Compensation For Electrical Impedance Tomography</h2><p><strong>Authors:Chuyu Wang, Huiting Deng, Dong Liu</strong></p>
<p>Electrical Impedance Tomography (EIT) provides a non-invasive, portable imaging modality with significant potential in medical and industrial applications. Despite its advantages, EIT encounters two primary challenges: the ill-posed nature of its inverse problem and the spatially variable, location-dependent sensitivity distribution. Traditional model-based methods mitigate ill-posedness through regularization but overlook sensitivity variability, while supervised deep learning approaches require extensive training data and lack generalization. Recent developments in neural fields have introduced implicit regularization techniques for image reconstruction, but these methods typically neglect the physical principles underlying EIT, thus limiting their effectiveness. In this study, we propose PhyNC (Physics-driven Neural Compensation), an unsupervised deep learning framework that incorporates the physical principles of EIT. PhyNC addresses both the ill-posed inverse problem and the sensitivity distribution by dynamically allocating neural representational capacity to regions with lower sensitivity, ensuring accurate and balanced conductivity reconstructions. Extensive evaluations on both simulated and experimental data demonstrate that PhyNC outperforms existing methods in terms of detail preservation and artifact resistance, particularly in low-sensitivity regions. Our approach enhances the robustness of EIT reconstructions and provides a flexible framework that can be adapted to other imaging modalities with similar challenges. </p>
<blockquote>
<p>电阻抗断层扫描（EIT）提供了一种非侵入性、便携式的成像方式，在医疗和工业应用中具有巨大的潜力。尽管EIT具有优势，但它面临两个主要挑战：其反问题的病态性质和空间变化、位置依赖的灵敏度分布。基于传统模型的方法通过正则化来缓解病态性，但忽略了灵敏度的变化，而监督深度学习的方法需要大量的训练数据且缺乏泛化能力。神经网络领域的最新发展已经引入了隐式正则化技术进行图像重建，但这些方法通常忽略了EIT的物理原理，从而限制了其有效性。本研究中，我们提出了PhyNC（物理驱动神经网络补偿），这是一个结合了EIT物理原理的无监督深度学习框架。PhyNC通过动态分配神经表征容量到灵敏度较低的区域，解决了不适定的反问题和灵敏度分布问题，确保准确且平衡的导电率重建。对模拟和实验数据的广泛评估表明，PhyNC在细节保留和抗伪影方面优于现有方法，特别是在低灵敏度区域。我们的方法提高了EIT重建的稳健性，并提供了一个灵活的框架，可以适应具有类似挑战的其他成像方式。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.18067v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了电气阻抗成像技术（EIT）的优势及面临的挑战，包括其反问题的病态性和灵敏度分布的空间变化性。提出了一种结合物理原理和深度学习的无监督学习框架PhyNC，通过动态分配神经网络表征容量来解决这些问题，确保导电率重建的准确性和平衡性。在模拟和实验数据上的评估表明，PhyNC在细节保留和抗伪影方面优于现有方法，特别是在低灵敏度区域。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>EIT是一种具有医疗和工业应用潜力的非侵入性、可携带的成像模式。</li>
<li>EIT面临的主要挑战包括其反问题的病态性和灵敏度分布的空间变化性。</li>
<li>传统方法通过正则化解决反问题的病态性，但忽略了灵敏度的变化。</li>
<li>监督深度学习方法需要大量训练数据，并缺乏泛化能力。</li>
<li>最近的神经营略引入了隐式正则化技术用于图像重建，但忽略了EIT的物理原理。</li>
<li>PhyNC框架结合了EIT的物理原理和深度学习，通过动态分配神经网络表征容量来解决反问题。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.18067">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e36aa34dde35af876b295673e2800846.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4a9f5f33612028c079ba47b4c89c8efc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f0830a33e084a8efcc7360d157f62542.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="MRI-super-resolution-reconstruction-using-efficient-diffusion-probabilistic-model-with-residual-shifting"><a href="#MRI-super-resolution-reconstruction-using-efficient-diffusion-probabilistic-model-with-residual-shifting" class="headerlink" title="MRI super-resolution reconstruction using efficient diffusion   probabilistic model with residual shifting"></a>MRI super-resolution reconstruction using efficient diffusion   probabilistic model with residual shifting</h2><p><strong>Authors:Mojtaba Safari, Shansong Wang, Zach Eidex, Qiang Li, Erik H. Middlebrooks, David S. Yu, Xiaofeng Yang</strong></p>
<p>Objective:This study introduces a residual error-shifting mechanism that drastically reduces sampling steps while preserving critical anatomical details, thus accelerating MRI reconstruction. Approach:We propose a novel diffusion-based SR framework called Res-SRDiff, which integrates residual error shifting into the forward diffusion process. This enables efficient HR image reconstruction by aligning the degraded HR and LR distributions.We evaluated Res-SRDiff on ultra-high-field brain T1 MP2RAGE maps and T2-weighted prostate images, comparing it with Bicubic, Pix2pix, CycleGAN, and a conventional denoising diffusion probabilistic model with vision transformer backbone (TM-DDPM), using quantitative metrics such as peak signal-to-noise ratio (PSNR), structural similarity index (SSIM), gradient magnitude similarity deviation (GMSD), and learned perceptual image patch similarity (LPIPS). Main results: Res-SRDiff significantly outperformed all comparative methods in terms of PSNR, SSIM, and GMSD across both datasets, with statistically significant improvements (p-values&lt;&lt;0.05). The model achieved high-fidelity image restoration with only four sampling steps, drastically reducing computational time to under one second per slice, which is substantially faster than conventional TM-DDPM with around 20 seconds per slice. Qualitative analyses further demonstrated that Res-SRDiff effectively preserved fine anatomical details and lesion morphology in both brain and pelvic MRI images. Significance: Our findings show that Res-SRDiff is an efficient and accurate MRI SR method, markedly improving computational efficiency and image quality. Integrating residual error shifting into the diffusion process allows for rapid and robust HR image reconstruction, enhancing clinical MRI workflows and advancing medical imaging research. The source at:<a target="_blank" rel="noopener" href="https://github.com/mosaf/Res-SRDiff">https://github.com/mosaf/Res-SRDiff</a> </p>
<blockquote>
<p>目标：本研究介绍了一种残差误差偏移机制，该机制能在保持关键解剖细节的同时，大大减少采样步骤，从而加速MRI重建。方法：我们提出了一种基于扩散的超分辨率重建框架Res-SRDiff，它将残差误差偏移集成到正向扩散过程中。这通过对齐退化的高分辨率和低分辨率分布，实现了高效的高分辨率图像重建。我们对超高场脑T1 MP2RAGE图和T2加权前列腺图像进行了Res-SRDiff评估，将其与Bicubic、Pix2pix、CycleGAN以及带有视觉转换器主干（TM-DDPM）的传统去噪扩散概率模型进行了比较，使用了峰值信噪比（PSNR）、结构相似性指数（SSIM）、梯度幅度相似性偏差（GMSD）和学习的感知图像块相似性（LPIPS）等定量指标。主要结果：在的两个数据集中，Res-SRDiff在PSNR、SSIM和GMSD方面显著优于所有比较方法，具有统计学上的显著差异（p值&lt;&lt;0.05）。该模型仅在四个采样步骤内就实现了高保真图像恢复，计算时间缩短至每片不到一秒，这远远快于常规TM-DDPM的约20秒每片。定性分析进一步表明，Res-SRDiff在脑部和盆腔MRI图像中有效地保留了精细的解剖细节和病灶形态。意义：我们的研究结果表明，Res-SRDiff是一种高效且准确的MRI超分辨率重建方法，显著提高了计算效率和图像质量。将残差误差偏移集成到扩散过程中，可实现快速稳健的高分辨率图像重建，增强临床MRI工作流程，推动医学成像研究的发展。更多信息请访问：<a target="_blank" rel="noopener" href="https://github.com/mosaf/Res-SRDiff%E3%80%82">https://github.com/mosaf/Res-SRDiff</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01576v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该研究引入了一种残差误差移位机制，该机制能在减少采样步骤的同时保留关键解剖细节，从而加速MRI重建。研究团队提出了一种新型的基于扩散的超分辨率（SR）框架——Res-SRDiff，它将残差误差移位整合到前向扩散过程中，使得重建高分辨率（HR）图像时效率更高。该模型在两个不同的数据集上的表现均显著优于其他对比方法，实现了高保真度的图像恢复，且采样步骤仅需四次，计算时间缩短至每片不到一秒，大大快于传统的TM-DDPM模型。该研究显著提高了MRI图像的计算效率和图像质量。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究提出了一种新型的MRI超分辨率重建方法——Res-SRDiff框架。</li>
<li>Res-SRDiff整合了残差误差移位机制到前向扩散过程中。</li>
<li>Res-SRDiff在超高温场脑T1 MP2RAGE地图和T2加权前列腺图像上的表现均显著优于其他对比方法。</li>
<li>Res-SRDiff实现了高保真度的图像恢复，并且只需四次采样步骤。</li>
<li>Res-SRDiff的计算时间大大缩短，每片图像不到一秒。</li>
<li>Res-SRDiff提高了MRI图像的计算效率和图像质量。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01576">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-55d21bc97ef25ee7547ed1faaaad1ef9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eea2a0bd727d8b6acdb4bd984532250c.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="VerteNet-–-A-Multi-Context-Hybrid-CNN-Transformer-for-Accurate-Vertebral-Landmark-Localization-in-Lateral-Spine-DXA-Images"><a href="#VerteNet-–-A-Multi-Context-Hybrid-CNN-Transformer-for-Accurate-Vertebral-Landmark-Localization-in-Lateral-Spine-DXA-Images" class="headerlink" title="VerteNet – A Multi-Context Hybrid CNN Transformer for Accurate   Vertebral Landmark Localization in Lateral Spine DXA Images"></a>VerteNet – A Multi-Context Hybrid CNN Transformer for Accurate   Vertebral Landmark Localization in Lateral Spine DXA Images</h2><p><strong>Authors:Zaid Ilyas, Arooba Maqsood, Afsah Saleem, Erchuan Zhang, David Suter, Parminder Raina, Jonathan M. Hodgson, John T. Schousboe, William D. Leslie, Joshua R. Lewis, Syed Zulqarnain Gilani</strong></p>
<p>Lateral Spine Image (LSI) analysis is important for medical diagnosis, treatment planning, and detailed spinal health assessments. Although modalities like Computed Tomography and Digital X-ray Imaging are commonly used, Dual Energy X-ray Absorptiometry (DXA) is often preferred due to lower radiation exposure, seamless capture, and cost-effectiveness. Accurate Vertebral Landmark Localization (VLL) on LSIs is important to detect spinal conditions like kyphosis and lordosis, as well as assessing Abdominal Aortic Calcification (AAC) using Inter-Vertebral Guides (IVGs). Nonetheless, few automated VLL methodologies have concentrated on DXA LSIs. We present VerteNet, a hybrid CNN-Transformer model featuring a novel dual-resolution attention mechanism in self and cross-attention domains, referred to as Dual Resolution Self-Attention (DRSA) and Dual Resolution Cross-Attention (DRCA). These mechanisms capture the diverse frequencies in DXA images by operating at two different feature map resolutions. Additionally, we design a Multi-Context Feature Fusion Block (MCFB) that efficiently integrates the features using DRSA and DRCA. We train VerteNet on 620 DXA LSIs from various machines and achieve superior results compared to existing methods. We also design an algorithm that utilizes VerteNet’s predictions in estimating the Region of Interest (ROI) to detect potential abdominal aorta cropping, where inadequate soft tissue hinders calcification assessment. Additionally, we present a small proof-of-concept study to show that IVGs generated from VLL information can improve inter-reader correlation in AAC scoring, addressing two key areas of disagreement in expert AAC-24 scoring: IVG placement and quality control for full abdominal aorta assessment. The code for this work can be found at <a target="_blank" rel="noopener" href="https://github.com/zaidilyas89/VerteNet">https://github.com/zaidilyas89/VerteNet</a>. </p>
<blockquote>
<p>侧位脊柱图像（LSI）分析在医学诊断、治疗计划和详细的脊柱健康评估中具有重要意义。尽管计算机断层扫描和数字X射线成像等模式常用，但由于较低的辐射暴露、无缝捕获和成本效益，双能X射线吸收法（DXA）往往更受欢迎。在LSIs上进行准确的椎体地标定位（VLL）对于检测脊柱疾病如驼背和腰椎前凸很重要，同时还需使用椎间指南（IVGs）评估腹部主动脉钙化（AAC）。然而，很少有自动化VLL方法专注于DXA LSIs。我们提出了VerteNet，这是一个混合CNN-Transformer模型，具有一种新型的双分辨率注意力机制，在自我和交叉注意力域中被称为双分辨率自注意力（DRSA）和双分辨率交叉注意力（DRCA）。这些机制通过在两种不同特征图分辨率上操作来捕捉DXA图像中的不同频率。此外，我们设计了一个多上下文特征融合块（MCFB），它有效地结合了使用DRSA和DRCA的特征。我们对来自各种机器的620个DXA LSI进行了VerteNet训练，并取得了比现有方法更优越的结果。我们还设计了一种算法，利用VerteNet的预测来估计感兴趣区域（ROI），以检测可能的腹部主动脉裁剪区域，其中软组织不足会阻碍钙化评估。此外，我们还进行了一项小型概念验证研究，以证明由VLL信息生成的IVG可以提高AAC评分的读者间相关性，解决专家AAC-24评分中的两个主要分歧领域：IVG放置和全长腹部主动脉评估的质量控制。该工作的代码可在<a target="_blank" rel="noopener" href="https://github.com/zaidilyas89/VerteNet%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/zaidilyas89/VerteNet找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.02097v2">PDF</a> 10 pages with 7 figures</p>
<p><strong>Summary</strong><br>     医学图像中的侧位脊柱图像（LSI）分析对于医学诊断、治疗计划和详细的脊柱健康评估至关重要。虽然计算机断层扫描和数字X射线成像等模态是常用的，但由于较低的辐射暴露、无缝捕获和成本效益，双能X射线吸收仪（DXA）往往更受欢迎。准确的椎骨地标定位（VLL）对检测脊柱状况如驼背和隆突非常重要。本文提出了一种混合CNN-Transformer模型VerteNet，具有新型的双分辨率注意力机制，用于DXA LSIs的VLL。此外，还设计了一种算法来估计感兴趣区域（ROI），以检测因软组织不足导致的腹部主动脉裁剪评估问题。同时，通过VLL信息生成的IVG能提高AAC评分的读片间相关性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Lateral Spine Image (LSI)分析在医学诊断、治疗计划和脊柱健康评估中具有重要作用。</li>
<li>虽然有多种成像技术可用，但Dual Energy X-ray Absorptiometry (DXA)因低辐射暴露、无缝捕获和成本效益而受到青睐。</li>
<li>准确的Vertebral Landmark Localization (VLL)对于检测脊柱疾病至关重要，如驼背和隆突。</li>
<li>提出的VerteNet模型具有双分辨率注意力机制，旨在捕捉DXA图像中的不同频率成分。</li>
<li>VerteNet在多种机器采集的620张DXA LSI图像上进行了训练，并取得了优于现有方法的结果。</li>
<li>开发了利用VerteNet预测结果估计感兴趣区域（ROI）的算法，以检测因软组织不足导致的腹部主动脉裁剪问题。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.02097">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-43952dfa825ae8f94e157d006f5f0e2e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3d9ffc42bad8238f73bce7880f326341.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d4fcb77a9ed4f60a88ea38402df8a330.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3b74ef7548bcd5148746673e22fca59a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0c2f8a5a420f134deca3e8f323080b71.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-926e5a1f8c7abfecf8ec183b5bb01c3d.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Self-Consistent-Nested-Diffusion-Bridge-for-Accelerated-MRI-Reconstruction"><a href="#Self-Consistent-Nested-Diffusion-Bridge-for-Accelerated-MRI-Reconstruction" class="headerlink" title="Self-Consistent Nested Diffusion Bridge for Accelerated MRI   Reconstruction"></a>Self-Consistent Nested Diffusion Bridge for Accelerated MRI   Reconstruction</h2><p><strong>Authors:Tao Song, Yicheng Wu, Minhao Hu, Xiangde Luo, Guoting Luo, Guotai Wang, Yi Guo, Feng Xu, Shaoting Zhang</strong></p>
<p>Accelerated MRI reconstruction plays a vital role in reducing scan time while preserving image quality. While most existing methods rely on complex-valued image-space or k-space data, these formats are often inaccessible in clinical practice due to proprietary reconstruction pipelines, leaving only magnitude images stored in DICOM files. To address this gap, we focus on the underexplored task of magnitude-image-based MRI reconstruction. Recent advancements in diffusion models, particularly denoising diffusion probabilistic models (DDPMs), have demonstrated strong capabilities in modeling image priors. However, their task-agnostic denoising nature limits performance in source-to-target image translation tasks, such as MRI reconstruction. In this work, we propose a novel Self-Consistent Nested Diffusion Bridge (SC-NDB) framework that models accelerated MRI reconstruction as a bi-directional image translation process between under-sampled and fully-sampled magnitude MRI images. SC-NDB introduces a nested diffusion architecture with a self-consistency constraint and reverse bridge diffusion pathways to improve intermediate prediction fidelity and better capture the explicit priors of source images. Furthermore, we incorporate a Contour Decomposition Embedding Module (CDEM) to inject structural and textural knowledge by leveraging Laplacian pyramids and directional filter banks. Extensive experiments on the fastMRI and IXI datasets demonstrate that our method achieves state-of-the-art performance compared to both magnitude-based and non-magnitude-based diffusion models, confirming the effectiveness and clinical relevance of SC-NDB. </p>
<blockquote>
<p>加速MRI重建在减少扫描时间的同时保持图像质量方面起着至关重要的作用。虽然大多数现有方法依赖于复数值图像空间或k空间数据，但由于专有重建管道，这些格式在临床实践中通常无法访问，只剩下以DICOM文件格式存储的幅度图像。为了解决这一差距，我们专注于基于幅度图像的MRI重建这一尚未得到充分研究的任务。扩散模型的最新进展，特别是去噪扩散概率模型（DDPMs），在建模图像先验方面表现出了强大的能力。然而，其任务无关的去噪性质限制了其在源到目标图像翻译任务（例如MRI重建）中的性能。在这项工作中，我们提出了一种新型的自我一致嵌套扩散桥（SC-NDB）框架，将加速MRI重建建模为欠采样和完全采样幅度MRI图像之间的双向图像翻译过程。SC-NDB引入了一个嵌套扩散架构，具有自我一致性约束和反向桥扩散路径，以提高中间预测保真度并更好地捕获源图像的显式先验。此外，我们结合了轮廓分解嵌入模块（CDEM），通过利用拉普拉斯金字塔和方向滤波银行来注入结构和纹理知识。在fastMRI和IXI数据集上的广泛实验表明，我们的方法达到了最先进的性能，与基于幅度和非基于幅度的扩散模型相比都表现出色，证实了SC-NDB的有效性和临床相关性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.09998v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探讨了加速MRI重建的重要性，指出其在减少扫描时间的同时保持图像质量的关键作用。针对现有方法主要依赖复杂值图像空间或k空间数据，而临床实践中往往无法获取这些问题，研究集中在基于幅度图像的MRI重建上。利用扩散模型，特别是去噪扩散概率模型（DDPMs）的图像先验建模能力，提出一种新型的自洽嵌套扩散桥（SC-NDB）框架，将加速MRI重建建模为欠采样和完全采样幅度MRI图像之间的双向图像翻译过程。SC-NDB引入嵌套扩散架构、自洽性约束和反向桥扩散路径，提高中间预测保真度，更好地捕捉源图像的显式先验。结合轮廓分解嵌入模块（CDEM），通过利用拉普拉斯金字塔和方向滤波器库注入结构和纹理知识。在fastMRI和IXI数据集上的广泛实验表明，该方法在幅度和非幅度扩散模型上均达到最新性能水平，验证了SC-NDB的有效性和临床相关性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>加速MRI重建对于减少扫描时间同时保持图像质量至关重要。</li>
<li>现有方法主要依赖复杂值图像空间或k空间数据，但在临床实践中往往无法获取。</li>
<li>研究集中在基于幅度图像的MRI重建上。</li>
<li>扩散模型，特别是去噪扩散概率模型（DDPMs），具有强大的图像先验建模能力。</li>
<li>提出自洽嵌套扩散桥（SC-NDB）框架，将加速MRI重建建模为欠采样和完全采样幅度MRI图像之间的双向翻译过程。</li>
<li>SC-NDB结合嵌套扩散架构、自洽性约束和反向桥扩散路径，提高预测中间结果的准确性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.09998">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-bd1f7b949aaaa5f2296d25a8a66332f9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a7e7a39234dd773301f01071b928f51a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-99ce769d63417c223290e5d676d686c2.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Learning-Modality-Aware-Representations-Adaptive-Group-wise-Interaction-Network-for-Multimodal-MRI-Synthesis"><a href="#Learning-Modality-Aware-Representations-Adaptive-Group-wise-Interaction-Network-for-Multimodal-MRI-Synthesis" class="headerlink" title="Learning Modality-Aware Representations: Adaptive Group-wise Interaction   Network for Multimodal MRI Synthesis"></a>Learning Modality-Aware Representations: Adaptive Group-wise Interaction   Network for Multimodal MRI Synthesis</h2><p><strong>Authors:Tao Song, Yicheng Wu, Minhao Hu, Xiangde Luo, Linda Wei, Guotai Wang, Yi Guo, Feng Xu, Shaoting Zhang</strong></p>
<p>Multimodal MR image synthesis aims to generate missing modality images by effectively fusing and mapping from a subset of available MRI modalities. Most existing methods adopt an image-to-image translation paradigm, treating multiple modalities as input channels. However, these approaches often yield sub-optimal results due to the inherent difficulty in achieving precise feature- or semantic-level alignment across modalities. To address these challenges, we propose an Adaptive Group-wise Interaction Network (AGI-Net) that explicitly models both inter-modality and intra-modality relationships for multimodal MR image synthesis. Specifically, feature channels are first partitioned into predefined groups, after which an adaptive rolling mechanism is applied to conventional convolutional kernels to better capture feature and semantic correspondences between different modalities. In parallel, a cross-group attention module is introduced to enable effective feature fusion across groups, thereby enhancing the network’s representational capacity. We validate the proposed AGI-Net on the publicly available IXI and BraTS2023 datasets. Experimental results demonstrate that AGI-Net achieves state-of-the-art performance in multimodal MR image synthesis tasks, confirming the effectiveness of its modality-aware interaction design. We release the relevant code at: <a target="_blank" rel="noopener" href="https://github.com/zunzhumu/Adaptive-Group-wise-Interaction-Network-for-Multimodal-MRI-Synthesis.git">https://github.com/zunzhumu/Adaptive-Group-wise-Interaction-Network-for-Multimodal-MRI-Synthesis.git</a>. </p>
<blockquote>
<p>多模态MR图像合成旨在通过有效融合和映射从可用MRI模态的子集来生成缺失的模态图像。大多数现有方法采用图像到图像的翻译模式，将多个模态视为输入通道。然而，由于在不同模态之间实现精确的特征或语义级对齐的固有困难，这些方法通常会产生次优结果。为了应对这些挑战，我们提出了一种自适应组交互网络（AGI-Net），该网络对多模态MR图像合成进行显式建模，建立模态间和模态内的关系。具体来说，特征通道首先被划分为预定义的组，然后对传统卷积核应用自适应滚动机制，以更好地捕获不同模态之间的特征和语义对应关系。同时，引入跨组注意力模块，实现跨组的有效特征融合，从而增强网络的表示能力。我们在公开可用的IXI和BraTS2023数据集上验证了所提出的AGI-Net。实验结果表明，AGI-Net在多模态MR图像合成任务上达到了最先进的性能，证明了其模态感知交互设计的有效性。我们已在<a target="_blank" rel="noopener" href="https://github.com/zunzhumu/Adaptive-Group-wise-Interaction-Network-for-Multimodal-MRI-Synthesis.git%E4%B8%8A%E5%8F%91%E5%B8%83%E4%BA%86%E7%9B%B8%E5%85%B3%E4%BB%A3%E7%A0%81%E3%80%82">https://github.com/zunzhumu/Adaptive-Group-wise-Interaction-Network-for-Multimodal-MRI-Synthesis.git上发布了相关代码。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.14684v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种自适应组交互网络（AGI-Net），用于多模态MR图像合成。该网络通过建模模态间和模态内关系，改进了图像到图像翻译范式，实现了更精确的特征或语义级对齐。采用分组卷积核自适应滚动机制，并引入跨组注意力模块，提高了网络在多模态特征融合方面的表现。在公开数据集IXI和BraTS2023上的实验结果表明，AGI-Net在多模态MR图像合成任务上达到最佳性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>本文旨在通过有效融合和映射部分可用的MRI模态，生成缺失模态的图像。</li>
<li>提出一种自适应组交互网络（AGI-Net），以改进多模态MR图像合成的效果。</li>
<li>AGI-Net通过建模模态间和模态内关系，实现更精确的特征或语义级对齐。</li>
<li>采用分组卷积核的自适应滚动机制，以更好地捕捉不同模态之间的特征和语义对应关系。</li>
<li>引入跨组注意力模块，提高网络在多模态特征融合方面的能力。</li>
<li>在IXI和BraTS2023公开数据集上的实验验证了AGI-Net的有效性。</li>
<li>相关代码已发布在<a target="_blank" rel="noopener" href="https://github.com/zunzhumu/Adaptive-Group-wise-Interaction-Network-for-Multimodal-MRI-Synthesis.git%E4%B8%8A%E3%80%82">https://github.com/zunzhumu/Adaptive-Group-wise-Interaction-Network-for-Multimodal-MRI-Synthesis.git上。</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.14684">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e70177188b5a89b9db9653bedee47f26.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6b7a55b836eef362a3ff606eb685c65d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be42448c24a43c93d42c180fdf363fcc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48e61804c046343d25fd5d7d0b47ddda.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Real-Time-Image-Segmentation-via-Hybrid-Convolutional-Transformer-Architecture-Search"><a href="#Real-Time-Image-Segmentation-via-Hybrid-Convolutional-Transformer-Architecture-Search" class="headerlink" title="Real-Time Image Segmentation via Hybrid Convolutional-Transformer   Architecture Search"></a>Real-Time Image Segmentation via Hybrid Convolutional-Transformer   Architecture Search</h2><p><strong>Authors:Hongyuan Yu, Cheng Wan, Xiyang Dai, Mengchen Liu, Dongdong Chen, Bin Xiao, Yan Huang, Yuan Lu, Liang Wang</strong></p>
<p>Image segmentation is one of the most fundamental problems in computer vision and has drawn a lot of attention due to its vast applications in image understanding and autonomous driving. However, designing effective and efficient segmentation neural architectures is a labor-intensive process that may require numerous trials by human experts. In this paper, we address the challenge of integrating multi-head self-attention into high-resolution representation CNNs efficiently by leveraging architecture search. Manually replacing convolution layers with multi-head self-attention is non-trivial due to the costly overhead in memory to maintain high resolution. By contrast, we develop a multi-target multi-branch supernet method, which not only fully utilizes the advantages of high-resolution features but also finds the proper location for placing the multi-head self-attention module. Our search algorithm is optimized towards multiple objectives (e.g., latency and mIoU) and is capable of finding architectures on the Pareto frontier with an arbitrary number of branches in a single search. We further present a series of models via the Hybrid Convolutional-Transformer Architecture Search (HyCTAS) method that searches for the best hybrid combination of light-weight convolution layers and memory-efficient self-attention layers between branches from different resolutions and fuses them to high resolution for both efficiency and effectiveness. Extensive experiments demonstrate that HyCTAS outperforms previous methods in both semantic segmentation and panoptic segmentation tasks. Code and models are available at <a target="_blank" rel="noopener" href="https://github.com/MarvinYu1995/HyCTAS">https://github.com/MarvinYu1995/HyCTAS</a>. </p>
<blockquote>
<p>图像分割是计算机视觉中最基本的问题之一，由于其在图像理解和自动驾驶等方面的广泛应用而备受关注。然而，设计和开发有效且高效的分割神经网络是一个劳动密集型过程，可能需要人类专家进行多次试验。在本文中，我们通过利用架构搜索来解决将多头自注意力高效集成到高分辨率表示CNN中的挑战。手动将卷积层替换为多头自注意力是非平凡的，因为维持高分辨率需要消耗大量内存资源。相比之下，我们开发了一种多目标多分支超网方法，它不仅充分利用了高分辨率特征的优势，还找到了放置多头自注意力模块的正确位置。我们的搜索算法针对多个目标（例如延迟时间和mIoU）进行了优化，能够在单次搜索中找到具有任意数量的分支的帕累托前沿的架构。我们进一步通过混合卷积-转换器架构搜索（HyCTAS）方法提出了一系列模型，该方法搜索最佳轻量级卷积层和内存高效自注意力层之间的最佳混合组合，这些层来自不同分辨率的分支，并以高分辨率为核心进行融合，以实现高效和有效的分割。大量实验表明，HyCTAS在语义分割和全景分割任务上的性能优于以前的方法。相关代码和模型可访问<a target="_blank" rel="noopener" href="https://github.com/MarvinYu1995/HyCTAS%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/MarvinYu1995/HyCTAS获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.10413v2">PDF</a> 29 pages, 5 figures, submitted to Knowledge-Baed Systems</p>
<p><strong>摘要</strong></p>
<p>本文将多头自注意力高效集成到高分辨率表示CNN中，解决图像分割领域中的核心问题。通过利用架构搜索，避免了手动替换卷积层与多头自注意力的繁琐过程。开发的多目标多分支超网方法既充分利用了高分辨率特征的优势，又找到了放置多头自注意力模块的最佳位置。搜索算法针对多个目标（如延迟和mIoU）进行优化，能够在单次搜索中找到任意数量的分支的帕累托前沿架构。通过Hybrid Convolutional-Transformer Architecture Search（HyCTAS）方法，搜索最佳轻量级卷积层和内存高效自注意力层的混合组合，从不同分辨率的分支融合到高分辨率，以实现效率和效果的平衡。实验证明，HyCTAS在语义分割和全景分割任务上均优于以前的方法。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>图像分割是计算机视觉中最基本的问题之一，具有广泛的应用于图像理解和自动驾驶等领域。</li>
<li>手动将卷积层替换为多头自注意力是一个繁琐且成本高昂的过程。</li>
<li>提出的多目标多分支超网方法既利用高分辨率特征，又找到放置多头自注意力模块的最佳位置。</li>
<li>搜索算法针对多个目标（如延迟和mIoU）进行优化，能在单次搜索中找到多种优秀架构。</li>
<li>HyCTAS方法能搜索最佳混合组合，结合轻量级卷积层和内存高效自注意力层。</li>
<li>实验证明HyCTAS在语义分割和全景分割任务上的性能超越之前的方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.10413">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-c8749e1391f0f2fab95fa62fe4242c28.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-239a841b86bb46a52e8ca7634d35384f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aaa4e3a340163d1ee88aad24aa6892e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1aa8855b28fb5506af77f52cecc7255.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-30/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-30/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">医学图像</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-30/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-d5de42e36b2f3bcc8ffe843a1e4ac780.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS 方向最新论文已更新，请持续关注 Update in 2025-04-30  Generative Adversarial Network based Voice Conversion Techniques,   Challenges, and Recent Advancements
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-30
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-30/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-af5411e14789d5434717749e0cb1e8ae.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-04-30  DeeCLIP A Robust and Generalizable Transformer-Based Framework for   Detecting AI-Generated Images
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-30
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">29580.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
