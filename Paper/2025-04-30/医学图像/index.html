<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-30  Breast Cancer Detection from Multi-View Screening Mammograms with Visual   Prompt Tuning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-5995b096b27118f607b5d98b2c6e3c25.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-30
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    79 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-30-æ›´æ–°"><a href="#2025-04-30-æ›´æ–°" class="headerlink" title="2025-04-30 æ›´æ–°"></a>2025-04-30 æ›´æ–°</h1><h2 id="Breast-Cancer-Detection-from-Multi-View-Screening-Mammograms-with-Visual-Prompt-Tuning"><a href="#Breast-Cancer-Detection-from-Multi-View-Screening-Mammograms-with-Visual-Prompt-Tuning" class="headerlink" title="Breast Cancer Detection from Multi-View Screening Mammograms with Visual   Prompt Tuning"></a>Breast Cancer Detection from Multi-View Screening Mammograms with Visual   Prompt Tuning</h2><p><strong>Authors:Han Chen, Anne L. Martel</strong></p>
<p>Accurate detection of breast cancer from high-resolution mammograms is crucial for early diagnosis and effective treatment planning. Previous studies have shown the potential of using single-view mammograms for breast cancer detection. However, incorporating multi-view data can provide more comprehensive insights. Multi-view classification, especially in medical imaging, presents unique challenges, particularly when dealing with large-scale, high-resolution data. In this work, we propose a novel Multi-view Visual Prompt Tuning Network (MVPT-NET) for analyzing multiple screening mammograms. We first pretrain a robust single-view classification model on high-resolution mammograms and then innovatively adapt multi-view feature learning into a task-specific prompt tuning process. This technique selectively tunes a minimal set of trainable parameters (7%) while retaining the robustness of the pre-trained single-view model, enabling efficient integration of multi-view data without the need for aggressive downsampling. Our approach offers an efficient alternative to traditional feature fusion methods, providing a more robust, scalable, and efficient solution for high-resolution mammogram analysis. Experimental results on a large multi-institution dataset demonstrate that our method outperforms conventional approaches while maintaining detection efficiency, achieving an AUROC of 0.852 for distinguishing between Benign, DCIS, and Invasive classes. This work highlights the potential of MVPT-NET for medical imaging tasks and provides a scalable solution for integrating multi-view data in breast cancer detection. </p>
<blockquote>
<p>ä¹³è…ºç™Œçš„é«˜åˆ†è¾¨ç‡é’¼é¶æ£€æµ‹å¯¹äºæ—©æœŸå‘ç°å’Œæœ‰æ•ˆæ²»ç–—è®¡åˆ’è‡³å…³é‡è¦ã€‚æ—©æœŸç ”ç©¶å·²ç»å±•ç¤ºäº†ä½¿ç”¨å•è§†å›¾é’¼é¶æ£€æµ‹ä¹³è…ºç™Œçš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œæ•´åˆå¤šè§†å›¾æ•°æ®å¯ä»¥æä¾›æ›´å…¨é¢çš„æ´å¯Ÿã€‚å¤šè§†å›¾åˆ†ç±»ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»å­¦å½±åƒä¸­ï¼Œå‘ˆç°å‡ºç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¤§è§„æ¨¡ã€é«˜åˆ†è¾¨ç‡æ•°æ®æ—¶ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¤šè§†å›¾è§†è§‰æç¤ºè°ƒæ•´ç½‘ç»œï¼ˆMVPT-NETï¼‰ï¼Œç”¨äºåˆ†æå¤šä¸ªç­›æŸ¥é’¼é¶å›¾åƒã€‚æˆ‘ä»¬é¦–å…ˆåœ¨é«˜åˆ†è¾¨ç‡é’¼é¶å›¾åƒä¸Šè®­ç»ƒä¸€ä¸ªç¨³å¥çš„å•è§†å›¾åˆ†ç±»æ¨¡å‹ï¼Œç„¶ååˆ›æ–°åœ°å°†å¤šè§†å›¾ç‰¹å¾å­¦ä¹ é€‚åº”ä¸ºä»»åŠ¡ç‰¹å®šçš„æç¤ºè°ƒæ•´è¿‡ç¨‹ã€‚è¯¥æŠ€æœ¯é€‰æ‹©æ€§åœ°è°ƒæ•´ä¸€å°éƒ¨åˆ†å¯è®­ç»ƒå‚æ•°ï¼ˆ7%ï¼‰ï¼ŒåŒæ—¶ä¿ç•™é¢„è®­ç»ƒå•è§†å›¾æ¨¡å‹çš„ç¨³å¥æ€§ï¼Œèƒ½å¤Ÿå®ç°å¤šè§†å›¾æ•°æ®çš„æœ‰æ•ˆé›†æˆï¼Œè€Œæ— éœ€è¿›è¡Œæ¿€è¿›çš„ä¸‹é‡‡æ ·ã€‚æˆ‘ä»¬çš„æ–¹æ³•æä¾›äº†ä¼ ç»Ÿç‰¹å¾èåˆæ–¹æ³•çš„æœ‰æ•ˆæ›¿ä»£æ–¹æ¡ˆï¼Œä¸ºé’¼é¶å›¾åƒçš„é«˜åˆ†è¾¨ç‡åˆ†ææä¾›äº†æ›´ç¨³å¥ã€å¯æ‰©å±•å’Œé«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚åœ¨å¤§å‹å¤šæœºæ„æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¿æŒæ£€æµ‹æ•ˆç‡çš„åŒæ—¶ï¼Œä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œåœ¨åŒºåˆ†è‰¯æ€§ã€DCISå’Œä¾µè¢­æ€§ç±»åˆ«æ—¶è¾¾åˆ°äº†0.852çš„AUROCã€‚è¿™é¡¹å·¥ä½œçªå‡ºäº†MVPT-NETåœ¨åŒ»å­¦æˆåƒä»»åŠ¡ä¸­çš„æ½œåŠ›ï¼Œå¹¶ä¸ºæ•´åˆå¤šè§†å›¾æ•°æ®åœ¨ä¹³è…ºç™Œæ£€æµ‹ä¸­æä¾›äº†å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.19900v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹çš„å¤šè§†è§’è§†è§‰æç¤ºè°ƒæ•´ç½‘ç»œï¼ˆMVPT-NETï¼‰ï¼Œç”¨äºåˆ†æå¤šè§†è§’ä¹³è…ºç™Œç­›æŸ¥å›¾åƒã€‚é€šè¿‡é¢„è®­ç»ƒå•è§†è§’åˆ†ç±»æ¨¡å‹å¹¶é€‚åº”å¤šè§†è§’ç‰¹å¾å­¦ä¹ ï¼Œè¯¥æ–¹æ³•èƒ½åœ¨ä¸ç‰ºç‰²æ£€æµ‹æ•ˆç‡çš„å‰æä¸‹ï¼Œå®ç°å¤šè§†è§’æ•°æ®çš„æœ‰æ•ˆæ•´åˆï¼Œæé«˜äº†ä¹³è…ºç™Œæ£€æµ‹çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤§å‹å¤šæœºæ„æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œè¾¾åˆ°AUROC 0.852ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MVPT-NETç”¨äºåˆ†æä¹³è…ºç™Œå¤šè§†è§’ç­›æŸ¥å›¾åƒï¼Œä»¥æä¾›æ—©æœŸè¯Šæ–­å’Œæ²»ç–—æ–¹æ¡ˆçš„æœ‰æ•ˆæŒ‡å¯¼ã€‚</li>
<li>è¯¥æ–¹æ³•ç»“åˆäº†å•è§†è§’åˆ†ç±»æ¨¡å‹å’Œå¤šè§†è§’ç‰¹å¾å­¦ä¹ ï¼Œæé«˜äº†ä¹³è…ºç™Œæ£€æµ‹çš„å‡†ç¡®æ€§ã€‚</li>
<li>MVPT-NETé‡‡ç”¨é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œæé«˜äº†æ¨¡å‹çš„é²æ£’æ€§å’Œæ€§èƒ½ã€‚</li>
<li>é€šè¿‡åˆ›æ–°çš„æç¤ºè°ƒæ•´è¿‡ç¨‹å®ç°å¤šè§†è§’æ•°æ®çš„æœ‰æ•ˆæ•´åˆï¼Œä¸éœ€è¦å‰§çƒˆçš„é™é‡‡æ ·æ“ä½œã€‚</li>
<li>è¯¥æ–¹æ³•ä¼˜äºä¼ ç»Ÿç‰¹å¾èåˆæ–¹æ³•ï¼Œä¸ºåŒ»å­¦å›¾åƒåˆ†ææä¾›äº†æ›´ç¨³å¥ã€å¯æ‰©å±•å’Œé«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒMVPT-NETåœ¨å¤§å‹å¤šæœºæ„æ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œè¾¾åˆ°äº†AUROCä¸º0.852çš„é«˜è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.19900">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2253ed1a2020ac1c6ed97c9eced6ea50.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7334a8d900495cf598d58592c9144056.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Dissipative-particle-dynamics-models-of-encapsulated-microbubbles-and-gas-vesicles-for-biomedical-ultrasound-simulations"><a href="#Dissipative-particle-dynamics-models-of-encapsulated-microbubbles-and-gas-vesicles-for-biomedical-ultrasound-simulations" class="headerlink" title="Dissipative particle dynamics models of encapsulated microbubbles and   gas vesicles for biomedical ultrasound simulations"></a>Dissipative particle dynamics models of encapsulated microbubbles and   gas vesicles for biomedical ultrasound simulations</h2><p><strong>Authors:Nikolaos Ntarakas, MaÅ¡a Lah, Daniel SvenÅ¡ek, Tilen Potisk, Matej Praprotnik</strong></p>
<p>Ultrasound-guided drug and gene delivery (usdg) enables controlled and spatially precise delivery of drugs and macromolecules, encapsulated in microbubbles (embs) and submicron gas vesicles (gvs), to target areas such as cancer tumors. It is a non-invasive, high precision, low toxicity process with drastically reduced drug dosage. Rheological and acoustic properties of gvs and embs critically affect the outcome of usdg and imaging. Detailed understanding and modeling of their physical properties is thus essential for ultrasound-mediated therapeutic applications. State-of-the-art continuuum models of shelled bodies cannot incorporate critical details such as varying thickness of the encapsulating shell or specific interactions between its constituents and interior or exterior solvents. Such modeling approaches also do not allow for detailed modeling of chemical surface functionalizations, which are crucial for tuning the gv-blood interactions. We develop a general particle-based modeling framework for encapsulated bodies that accurately captures elastic and rheological properties of gvs and embs. We use dissipative particle dynamics to model the solvent, the gaseous phase in the capsid, and the triangulated surfaces of immersed objects. Their elastic behavior is studied and validated through stretching and buckling simulations, eigenmode analysis, shear flow simulations, and comparison of predicted gv buckling pressure with experimental data from the literature. The presented modeling approach paves the way for large-scale simulations of encapsulated bodies, capturing their dynamics, interactions, and collective behavior. </p>
<blockquote>
<p>è¶…å£°å¼•å¯¼ä¸‹çš„è¯ç‰©å’ŒåŸºå› ä¼ é€’ï¼ˆUSDGï¼‰æŠ€æœ¯èƒ½å¤Ÿå®ç°è¯ç‰©å’Œå¤§åˆ†å­åœ¨å¾®æ³¡ï¼ˆEMBsï¼‰å’Œäºšå¾®ç±³æ°”ä½“å›Šæ³¡ï¼ˆGVsï¼‰çš„å°è£…ä¸‹ï¼Œå¯¹ç™Œç—‡è‚¿ç˜¤ç­‰ç›®æ ‡åŒºåŸŸçš„ç²¾å‡†ç©ºé—´æ§åˆ¶ä¼ é€’ã€‚è¿™æ˜¯ä¸€ç§éä¾µå…¥æ€§ã€é«˜ç²¾åº¦ã€ä½æ¯’æ€§çš„è¿‡ç¨‹ï¼Œèƒ½å¤§å¹…åº¦å‡å°‘è¯ç‰©å‰‚é‡ã€‚GVså’ŒEMBsçš„æµå˜å­¦å’Œå£°å­¦ç‰¹æ€§å¯¹USDGå’Œæˆåƒçš„ç»“æœå…·æœ‰é‡è¦å½±å“ã€‚å› æ­¤ï¼Œå¯¹å…¶ç‰©ç†æ€§è´¨çš„æ·±å…¥ç†è§£å’Œå»ºæ¨¡å¯¹äºè¶…å£°ä»‹å¯¼çš„æ²»ç–—åº”ç”¨è‡³å…³é‡è¦ã€‚ç°æœ‰çš„å…ˆè¿›è¿ç»­ä½“æ¨¡å‹æ— æ³•çº³å…¥å°è£…ä½“çš„é‡è¦ç»†èŠ‚ï¼Œå¦‚å°è£…å£³åšåº¦ä¸åŒæˆ–å…¶ä¸å†…å¤–æº¶å‰‚ä¹‹é—´çš„ç‰¹å®šç›¸äº’ä½œç”¨ã€‚è¿™ç§å»ºæ¨¡æ–¹æ³•ä¹Ÿä¸å…è®¸å¯¹è¡¨é¢åŠŸèƒ½åŒ–è¿›è¡Œè¯¦ç»†çš„å»ºæ¨¡ï¼Œè¿™å¯¹äºè°ƒèŠ‚GVä¸è¡€æ¶²ä¹‹é—´çš„ç›¸äº’ä½œç”¨è‡³å…³é‡è¦ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ç§ç”¨äºå°è£…ä½“çš„é€šç”¨ç²’å­å»ºæ¨¡æ¡†æ¶ï¼Œèƒ½å¤Ÿå‡†ç¡®æ•æ‰GVså’ŒEMBsçš„å¼¹æ€§å’Œæµå˜æ€§è´¨ã€‚æˆ‘ä»¬ä½¿ç”¨è€—æ•£ç²’å­åŠ¨åŠ›å­¦æ¥æ¨¡æ‹Ÿæº¶å‰‚ã€è¡£å£³ä¸­çš„æ°”ç›¸å’Œæµ¸æ²¡ç‰©ä½“çš„ä¸‰è§’å½¢è¡¨é¢ã€‚æˆ‘ä»¬ç ”ç©¶äº†ä»–ä»¬çš„å¼¹æ€§è¡Œä¸ºå¹¶é€šè¿‡æ‹‰ä¼¸å’Œå¼¯æ›²æ¨¡æ‹Ÿã€ç‰¹å¾æ¨¡æ€åˆ†æã€å‰ªåˆ‡æµæ¨¡æ‹Ÿä»¥åŠå°†é¢„æµ‹çš„GVå¼¯æ›²å‹åŠ›ä¸æ–‡çŒ®ä¸­çš„å®éªŒæ•°æ®è¿›è¡Œæ¯”è¾ƒæ¥éªŒè¯å…¶æœ‰æ•ˆæ€§ã€‚æ‰€å‘ˆç°çš„å»ºæ¨¡æ–¹æ³•ä¸ºå¤§è§„æ¨¡å°è£…ä½“æ¨¡æ‹Ÿé“ºå¹³äº†é“è·¯ï¼Œèƒ½å¤Ÿæ•æ‰å…¶åŠ¨åŠ›å­¦ã€ç›¸äº’ä½œç”¨å’Œé›†ä½“è¡Œä¸ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.19890v1">PDF</a> 55 pages, 18 figures, 4 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è¶…å£°å¼•å¯¼è¯ç‰©å’ŒåŸºå› ä¼ é€’ï¼ˆusdgï¼‰æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯åˆ©ç”¨å¾®æ°”æ³¡ï¼ˆembsï¼‰å’Œäºšå¾®ç±³æ°”ä½“å›Šæ³¡ï¼ˆgvsï¼‰å°è£…è¯ç‰©å’Œå¤§åˆ†å­ï¼Œå®ç°é’ˆå¯¹ç™Œç—‡è‚¿ç˜¤ç­‰ç›®æ ‡åŒºåŸŸçš„ç²¾å‡†ç©ºé—´ä¼ é€’ã€‚è¿™æ˜¯ä¸€ç§éä¾µå…¥æ€§ã€é«˜ç²¾åº¦ã€ä½æ¯’æ€§çš„è¿‡ç¨‹ï¼Œå¯å¤§å¹…åº¦é™ä½è¯ç‰©å‰‚é‡ã€‚æ–‡ç« å¼ºè°ƒäº†å¯¹gvså’Œembsæµå˜å­¦å’Œå£°å­¦ç‰¹æ€§çš„æ·±å…¥ç†è§£ä¸å»ºæ¨¡å¯¹äºè¶…å£°ä»‹å¯¼æ²»ç–—åº”ç”¨çš„é‡è¦æ€§ã€‚ç°æœ‰çš„è¿ç»­æ¨¡å‹æ— æ³•æ¶µç›–å…³é”®ç»†èŠ‚ï¼Œå¦‚å°è£…å£³åšåº¦å˜åŒ–æˆ–å…¶ä¸å†…å¤–æº¶å‰‚é—´çš„ç‰¹å®šç›¸äº’ä½œç”¨ã€‚å› æ­¤ï¼Œå¼€å‘äº†ä¸€ç§é’ˆå¯¹å°è£…ä½“çš„é€šç”¨ç²’å­åŸºç¡€å»ºæ¨¡æ¡†æ¶ï¼Œå‡†ç¡®æ•æ‰gvså’Œembsçš„å¼¹æ€§å’Œæµå˜ç‰¹æ€§ã€‚è¯¥ç ”ç©¶ä¸ºå°è£…ä½“çš„å¤§è§„æ¨¡æ¨¡æ‹Ÿé“ºå¹³äº†é“è·¯ï¼Œèƒ½å¤Ÿæ•æ‰å…¶åŠ¨åŠ›å­¦ã€ç›¸äº’ä½œç”¨å’Œé›†ä½“è¡Œä¸ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¶…å£°å¼•å¯¼è¯ç‰©å’ŒåŸºå› ä¼ é€’ï¼ˆusdgï¼‰èƒ½å¤Ÿå®ç°ç²¾å‡†çš„ç©ºé—´è¯ç‰©ä¼ é€’ï¼Œå°¤å…¶é’ˆå¯¹ç™Œç—‡è‚¿ç˜¤ç­‰ç›®æ ‡åŒºåŸŸã€‚</li>
<li>è¯¥æŠ€æœ¯åˆ©ç”¨å¾®æ°”æ³¡ï¼ˆembsï¼‰å’Œäºšå¾®ç±³æ°”ä½“å›Šæ³¡ï¼ˆgvsï¼‰å°è£…è¯ç‰©å’Œå¤§åˆ†å­ã€‚</li>
<li>usdgæ˜¯ä¸€ç§éä¾µå…¥æ€§ã€é«˜ç²¾åº¦ã€ä½æ¯’æ€§çš„è¿‡ç¨‹ï¼Œå¯å¤§å¹…åº¦é™ä½è¯ç‰©å‰‚é‡ã€‚</li>
<li>gvså’Œembsçš„æµå˜å­¦å’Œå£°å­¦ç‰¹æ€§å¯¹usdgå’Œæˆåƒç»“æœæœ‰é‡è¦å½±å“ã€‚</li>
<li>ç°æœ‰è¿ç»­æ¨¡å‹æ— æ³•å……åˆ†æè¿°gvså’Œembsçš„å…³é”®ç‰¹æ€§ï¼Œå¦‚å°è£…å£³çš„åšåº¦å˜åŒ–å’Œå…¶ä¸æº¶å‰‚çš„ç›¸äº’ä½œç”¨ã€‚</li>
<li>ç ”ç©¶å¼€å‘äº†ä¸€ç§åŸºäºç²’å­çš„å»ºæ¨¡æ¡†æ¶æ¥å‡†ç¡®æ•æ‰gvså’Œembsçš„å¼¹æ€§å’Œæµå˜ç‰¹æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.19890">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-27252c450846465cf9c7e719cee4e5e7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6731a0772d5854cea4e3c06830794172.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="SRMF-A-Data-Augmentation-and-Multimodal-Fusion-Approach-for-Long-Tail-UHR-Satellite-Image-Segmentation"><a href="#SRMF-A-Data-Augmentation-and-Multimodal-Fusion-Approach-for-Long-Tail-UHR-Satellite-Image-Segmentation" class="headerlink" title="SRMF: A Data Augmentation and Multimodal Fusion Approach for Long-Tail   UHR Satellite Image Segmentation"></a>SRMF: A Data Augmentation and Multimodal Fusion Approach for Long-Tail   UHR Satellite Image Segmentation</h2><p><strong>Authors:Yulong Guo, Zilun Zhang, Yongheng Shang, Tiancheng Zhao, Shuiguang Deng, Yingchun Yang, Jianwei Yin</strong></p>
<p>The long-tail problem presents a significant challenge to the advancement of semantic segmentation in ultra-high-resolution (UHR) satellite imagery. While previous efforts in UHR semantic segmentation have largely focused on multi-branch network architectures that emphasize multi-scale feature extraction and fusion, they have often overlooked the importance of addressing the long-tail issue. In contrast to prior UHR methods that focused on independent feature extraction, we emphasize data augmentation and multimodal feature fusion to alleviate the long-tail problem. In this paper, we introduce SRMF, a novel framework for semantic segmentation in UHR satellite imagery. Our approach addresses the long-tail class distribution by incorporating a multi-scale cropping technique alongside a data augmentation strategy based on semantic reordering and resampling. To further enhance model performance, we propose a multimodal fusion-based general representation knowledge injection method, which, for the first time, fuses text and visual features without the need for individual region text descriptions, extracting more robust features. Extensive experiments on the URUR, GID, and FBP datasets demonstrate that our method improves mIoU by 3.33%, 0.66%, and 0.98%, respectively, achieving state-of-the-art performance. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/BinSpa/SRMF.git">https://github.com/BinSpa/SRMF.git</a>. </p>
<blockquote>
<p>è¶…é•¿å°¾é—®é¢˜ä¸ºè¶…é«˜åˆ†è¾¨ç‡ï¼ˆUHRï¼‰å«æ˜Ÿå›¾åƒè¯­ä¹‰åˆ†å‰²çš„è¿›ä¸€æ­¥å‘å±•å¸¦æ¥äº†å·¨å¤§æŒ‘æˆ˜ã€‚ä»¥å¾€è¶…é«˜åˆ†è¾¨ç‡è¯­ä¹‰åˆ†å‰²çš„ä¸»è¦åŠªåŠ›å¤§å¤šé›†ä¸­åœ¨å¤šåˆ†æ”¯ç½‘ç»œæ¶æ„ä¸Šï¼Œå¼ºè°ƒå¤šå°ºåº¦ç‰¹å¾æå–å’Œèåˆï¼Œå¾€å¾€å¿½ç•¥äº†è§£å†³é•¿å°¾é—®é¢˜çš„é‡è¦æ€§ã€‚ä¸ä»¥å¾€ä¾§é‡äºç‹¬ç«‹ç‰¹å¾æå–çš„UHRæ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬å¼ºè°ƒæ•°æ®å¢å¼ºå’Œå¤šæ¨¡æ€ç‰¹å¾èåˆæ¥ç¼“è§£é•¿å°¾é—®é¢˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†SRMFï¼Œä¸€ä¸ªç”¨äºè¶…é«˜åˆ†è¾¨ç‡å«æ˜Ÿå›¾åƒè¯­ä¹‰åˆ†å‰²çš„æ–°å‹æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ç»“åˆå¤šå°ºåº¦è£å‰ªæŠ€æœ¯ï¼Œä»¥åŠåŸºäºè¯­ä¹‰é‡æ’å’Œé‡æ–°é‡‡æ ·çš„æ•°æ®å¢å¼ºç­–ç•¥ï¼Œè§£å†³äº†é•¿å°¾ç±»åˆ†å¸ƒé—®é¢˜ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜æ¨¡å‹æ€§èƒ½ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå¤šæ¨¡æ€èåˆçš„ä¸€èˆ¬è¡¨ç¤ºçŸ¥è¯†æ³¨å…¥æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é¦–æ¬¡èåˆäº†æ–‡æœ¬å’Œè§†è§‰ç‰¹å¾ï¼Œè€Œæ— éœ€å•ç‹¬çš„åŒºåŸŸæ–‡æœ¬æè¿°ï¼Œä»è€Œæå–å‡ºæ›´ç¨³å¥çš„ç‰¹å¾ã€‚åœ¨URURã€GIDå’ŒFBPæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åˆ†åˆ«æé«˜äº†mIoUçš„3.33%ã€0.66%å’Œ0.98%ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä»£ç å¯è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/BinSpa/SRMF.git%E3%80%82">https://github.com/BinSpa/SRMF.gitã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.19839v1">PDF</a> None</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹è¶…é«˜åˆ†è¾¨ç‡å«æ˜Ÿå›¾åƒè¯­ä¹‰åˆ†å‰²çš„é•¿å°¾é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶SRMFã€‚è¯¥æ¡†æ¶é€šè¿‡å¤šå°ºåº¦è£å‰ªæŠ€æœ¯å’ŒåŸºäºè¯­ä¹‰é‡æ’å’Œé‡é‡‡æ ·çš„æ•°æ®å¢å¼ºç­–ç•¥æ¥è§£å†³é•¿å°¾ç±»åˆ†å¸ƒé—®é¢˜ã€‚åŒæ—¶ï¼Œå¼•å…¥äº†ä¸€ç§åŸºäºå¤šæ¨¡æ€èåˆçš„ä¸€èˆ¬è¡¨ç¤ºçŸ¥è¯†æ³¨å…¥æ–¹æ³•ï¼Œèåˆæ–‡æœ¬å’Œè§†è§‰ç‰¹å¾ï¼Œæå–æ›´é²æ£’çš„ç‰¹å¾ï¼Œæé«˜äº†æ¨¡å‹æ€§èƒ½ã€‚åœ¨URURã€GIDå’ŒFBPæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åˆ†åˆ«æé«˜äº†mIoUæŒ‡æ ‡3.33%ã€0.66%å’Œ0.98%ï¼Œè¾¾åˆ°æœ€æ–°æ€§èƒ½æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¶…é«˜åˆ†è¾¨ç‡å«æ˜Ÿå›¾åƒè¯­ä¹‰åˆ†å‰²é¢ä¸´é•¿å°¾é—®é¢˜æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¤šå…³æ³¨å¤šåˆ†æ”¯ç½‘ç»œæ¶æ„ï¼Œä½†å¿½ç•¥äº†è§£å†³é•¿å°¾é—®é¢˜çš„é‡è¦æ€§ã€‚</li>
<li>SRMFæ¡†æ¶é€šè¿‡å¤šå°ºåº¦è£å‰ªå’Œæ•°æ®å¢å¼ºç­–ç•¥è§£å†³é•¿å°¾ç±»åˆ†å¸ƒé—®é¢˜ã€‚</li>
<li>å¼•å…¥åŸºäºå¤šæ¨¡æ€èåˆçš„çŸ¥è¯†æ³¨å…¥æ–¹æ³•ï¼Œèåˆæ–‡æœ¬å’Œè§†è§‰ç‰¹å¾ï¼Œæé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>å®éªŒåœ¨URURã€GIDå’ŒFBPæ•°æ®é›†ä¸ŠéªŒè¯äº†SRMFæ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>SRMFæ¡†æ¶æé«˜äº†mIoUæŒ‡æ ‡ï¼Œè¾¾åˆ°æœ€æ–°æ€§èƒ½æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.19839">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d19d5cfe6ca309792db140592ab2fb59.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc5971eef3afab164c6987023a9165a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-692437a040501a925ec513a265adbb93.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-258f426453fa0f97683cd69be4794f9d.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="CoDEx-Combining-Domain-Expertise-for-Spatial-Generalization-in-Satellite-Image-Analysis"><a href="#CoDEx-Combining-Domain-Expertise-for-Spatial-Generalization-in-Satellite-Image-Analysis" class="headerlink" title="CoDEx: Combining Domain Expertise for Spatial Generalization in   Satellite Image Analysis"></a>CoDEx: Combining Domain Expertise for Spatial Generalization in   Satellite Image Analysis</h2><p><strong>Authors:Abhishek Kuriyal, Elliot Vincent, Mathieu Aubry, Loic Landrieu</strong></p>
<p>Global variations in terrain appearance raise a major challenge for satellite image analysis, leading to poor model performance when training on locations that differ from those encountered at test time. This remains true even with recent large global datasets. To address this challenge, we propose a novel domain-generalization framework for satellite images. Instead of trying to learn a single generalizable model, we train one expert model per training domain, while learning expertsâ€™ similarity and encouraging similar experts to be consistent. A model selection module then identifies the most suitable experts for a given test sample and aggregates their predictions. Experiments on four datasets (DynamicEarthNet, MUDS, OSCD, and FMoW) demonstrate consistent gains over existing domain generalization and adaptation methods. Our code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/Abhishek19009/CoDEx">https://github.com/Abhishek19009/CoDEx</a>. </p>
<blockquote>
<p>åœ°å½¢å¤–è§‚çš„å…¨çƒå˜åŒ–ç»™å«æ˜Ÿå›¾åƒåˆ†æå¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨è®­ç»ƒåœ°ç‚¹ä¸æµ‹è¯•åœ°ç‚¹ä¸åŒçš„æƒ…å†µä¸‹ï¼Œæ¨¡å‹æ€§èƒ½å¾€å¾€è¾ƒå·®ã€‚å³ä½¿ä½¿ç”¨æœ€æ–°çš„å…¨çƒå¤§å‹æ•°æ®é›†ï¼Œæƒ…å†µä¾ç„¶å¦‚æ­¤ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„å«æ˜Ÿå›¾åƒé¢†åŸŸé€šç”¨åŒ–æ¡†æ¶ã€‚æˆ‘ä»¬å¹¶éè¯•å›¾å­¦ä¹ å•ä¸€çš„é€šç”¨æ¨¡å‹ï¼Œè€Œæ˜¯é’ˆå¯¹æ¯ä¸ªè®­ç»ƒé¢†åŸŸè®­ç»ƒä¸€ä¸ªä¸“å®¶æ¨¡å‹ï¼ŒåŒæ—¶å­¦ä¹ ä¸“å®¶ä¹‹é—´çš„ç›¸ä¼¼æ€§å¹¶é¼“åŠ±ç›¸ä¼¼ä¸“å®¶ä¿æŒä¸€è‡´ã€‚æ¨¡å‹é€‰æ‹©æ¨¡å—éšåå¯ä»¥ç¡®å®šç»™å®šæµ‹è¯•æ ·æœ¬çš„æœ€åˆé€‚ä¸“å®¶å¹¶å¯¹å…¶é¢„æµ‹è¿›è¡Œæ±‡æ€»ã€‚åœ¨å››ä¸ªæ•°æ®é›†ï¼ˆDynamicEarthNetã€MUDSã€OSCDå’ŒFMoWï¼‰ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸ç°æœ‰çš„é¢†åŸŸé€šç”¨åŒ–å’Œé€‚åº”æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ¡†æ¶å…·æœ‰æŒç»­çš„ä¼˜åŠ¿ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Abhishek19009/CoDEx%E4%B8%8A%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/Abhishek19009/CoDExä¸Šå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.19737v1">PDF</a> CVPR 2025 EarthVision Workshop</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦æ¢è®¨å…¨çƒåœ°å½¢å˜åŒ–çš„å«æ˜Ÿå›¾åƒåˆ†æéš¾é¢˜ã€‚è™½ç„¶å¤§å‹å…¨çƒæ•°æ®é›†å¯ä»¥ä¸€å®šç¨‹åº¦ä¸Šè§£å†³é—®é¢˜ï¼Œä½†é’ˆå¯¹æµ‹è¯•ç¯å¢ƒä¸è®­ç»ƒç¯å¢ƒä¸ä¸€è‡´çš„é—®é¢˜ï¼Œæ–‡ç« æå‡ºä¸€ç§æ–°å‹å«æ˜Ÿå›¾åƒé¢†åŸŸæ³›åŒ–æ¡†æ¶ã€‚ä¸åŒäºå•ä¸€é€šç”¨æ¨¡å‹å­¦ä¹ ï¼Œè¯¥æ¡†æ¶è®­ç»ƒæ¯ä¸ªè®­ç»ƒé¢†åŸŸçš„ä¸“å®¶æ¨¡å‹ï¼Œå­¦ä¹ ä¸“å®¶æ¨¡å‹é—´çš„ç›¸ä¼¼æ€§å¹¶é¼“åŠ±ç›¸ä¼¼ä¸“å®¶æ¨¡å‹ä¿æŒä¸€è‡´ã€‚æ¨¡å‹é€‰æ‹©æ¨¡å—ä¸ºç»™å®šæµ‹è¯•æ ·æœ¬é€‰æ‹©æœ€åˆé€‚çš„ä¸“å®¶æ¨¡å‹å¹¶èšåˆå…¶é¢„æµ‹ç»“æœã€‚å®éªŒè¯æ˜è¯¥æ¡†æ¶åœ¨å››ä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰é¢†åŸŸæ³›åŒ–å’Œé€‚åº”æ–¹æ³•ã€‚ä»£ç å·²å…¬å¼€åœ¨GitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å…¨çƒåœ°å½¢å˜åŒ–ç»™å«æ˜Ÿå›¾åƒåˆ†æå¸¦æ¥æŒ‘æˆ˜ã€‚</li>
<li>å³ä½¿ä½¿ç”¨å¤§å‹å…¨çƒæ•°æ®é›†ï¼Œæ¨¡å‹åœ¨æµ‹è¯•æ—¶ä»å¯èƒ½è¡¨ç°ä¸ä½³ã€‚</li>
<li>æå‡ºä¸€ç§æ–°å‹å«æ˜Ÿå›¾åƒé¢†åŸŸæ³›åŒ–æ¡†æ¶ï¼Œè®­ç»ƒé’ˆå¯¹æ¯ä¸ªè®­ç»ƒé¢†åŸŸçš„ä¸“å®¶æ¨¡å‹ã€‚</li>
<li>é€šè¿‡å­¦ä¹ ä¸“å®¶æ¨¡å‹é—´çš„ç›¸ä¼¼æ€§å¹¶é¼“åŠ±ç›¸ä¼¼ä¸“å®¶ä¿æŒä¸€è‡´æ¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>æ¨¡å‹é€‰æ‹©æ¨¡å—èƒ½æ ¹æ®æµ‹è¯•æ ·æœ¬é€‰æ‹©æœ€åˆé€‚çš„ä¸“å®¶æ¨¡å‹ã€‚</li>
<li>åœ¨å››ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜è¯¥æ¡†æ¶è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.19737">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8acc03bfcc80283986b9ff67351bfdf9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-51b8e29613b6c82c5805b594c8a91450.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a318a8dc15bb2d6221a5d7eab27c574.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-895866968ea02c61916a1847ac6f7c4d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21e47b9ac508f8cdbb8838ba99e7503e.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Prompt-Guiding-Multi-Scale-Adaptive-Sparse-Representation-driven-Network-for-Low-Dose-CT-MAR"><a href="#Prompt-Guiding-Multi-Scale-Adaptive-Sparse-Representation-driven-Network-for-Low-Dose-CT-MAR" class="headerlink" title="Prompt Guiding Multi-Scale Adaptive Sparse Representation-driven Network   for Low-Dose CT MAR"></a>Prompt Guiding Multi-Scale Adaptive Sparse Representation-driven Network   for Low-Dose CT MAR</h2><p><strong>Authors:Baoshun Shi, Bing Chen, Shaolei Zhang, Huazhu Fu, Zhanli Hu</strong></p>
<p>Low-dose CT (LDCT) is capable of reducing X-ray radiation exposure, but it will potentially degrade image quality, even yields metal artifacts at the case of metallic implants. For simultaneous LDCT reconstruction and metal artifact reduction (LDMAR), existing deep learning-based efforts face two main limitations: i) the network design neglects multi-scale and within-scale information; ii) training a distinct model for each dose necessitates significant storage space for multiple doses. To fill these gaps, we propose a prompt guiding multi-scale adaptive sparse representation-driven network, abbreviated as PMSRNet, for LDMAR task. Specifically, we construct PMSRNet inspired from multi-scale sparsifying frames, and it can simultaneously employ within-scale characteristics and cross-scale complementarity owing to an elaborated prompt guiding scale-adaptive threshold generator (PSATG) and a built multi-scale coefficient fusion module (MSFuM). The PSATG can adaptively capture multiple contextual information to generate more faithful thresholds, achieved by fusing features from local, regional, and global levels. Furthermore, we elaborate a model interpretable dual domain LDMAR framework called PDuMSRNet, and train single model with a prompt guiding strategy for multiple dose levels. We build a prompt guiding module, whose input contains dose level, metal mask and input instance, to provide various guiding information, allowing a single model to accommodate various CT dose settings. Extensive experiments at various dose levels demonstrate that the proposed methods outperform the state-of-the-art LDMAR methods. </p>
<blockquote>
<p>ä½å‰‚é‡CTï¼ˆLDCTï¼‰èƒ½å¤Ÿå‡å°‘Xå°„çº¿è¾å°„æš´éœ²ï¼Œä½†å¯èƒ½ä¼šé™ä½å›¾åƒè´¨é‡ï¼Œç”šè‡³åœ¨é‡‘å±æ¤å…¥ç‰©çš„æƒ…å†µä¸‹äº§ç”Ÿé‡‘å±ä¼ªå½±ã€‚å¯¹äºåŒæ—¶è¿›è¡Œçš„LDCTé‡å»ºå’Œé‡‘å±ä¼ªå½±å‡å°‘ï¼ˆLDMARï¼‰ï¼Œç°æœ‰çš„åŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•é¢ä¸´ä¸¤ä¸ªä¸»è¦å±€é™æ€§ï¼šiï¼‰ç½‘ç»œè®¾è®¡å¿½ç•¥äº†å¤šå°ºåº¦å’Œå°ºåº¦å†…çš„ä¿¡æ¯ï¼›iiï¼‰ä¸ºæ¯ä¸ªå‰‚é‡è®­ç»ƒä¸€ä¸ªç‹¬ç‰¹çš„æ¨¡å‹éœ€è¦å¤§é‡çš„å­˜å‚¨ç©ºé—´æ¥åº”å¯¹å¤šç§å‰‚é‡ã€‚ä¸ºäº†å¡«è¡¥è¿™äº›ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå³æ—¶å¼•å¯¼çš„å¤šå°ºåº¦è‡ªé€‚åº”ç¨€ç–è¡¨ç¤ºé©±åŠ¨ç½‘ç»œï¼ˆç®€ç§°PMSRNetï¼‰ï¼Œç”¨äºLDMARä»»åŠ¡ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å—åˆ°å¤šå°ºåº¦ç¨€ç–æ¡†æ¶çš„å¯å‘æ„å»ºäº†PMSRNetï¼Œå®ƒå¯ä»¥åŒæ—¶åˆ©ç”¨å°ºåº¦å†…çš„ç‰¹å¾å’Œè·¨å°ºåº¦çš„äº’è¡¥æ€§ï¼Œè¿™å¾—ç›Šäºç²¾å¿ƒè®¾è®¡çš„å³æ—¶å¼•å¯¼å°ºåº¦è‡ªé€‚åº”é˜ˆå€¼ç”Ÿæˆå™¨ï¼ˆPSATGï¼‰å’Œå†…ç½®çš„å¤šå°ºåº¦ç³»æ•°èåˆæ¨¡å—ï¼ˆMSFuMï¼‰ã€‚PSATGå¯ä»¥è‡ªé€‚åº”åœ°æ•è·å¤šç§ä¸Šä¸‹æ–‡ä¿¡æ¯æ¥ç”Ÿæˆæ›´çœŸå®çš„é˜ˆå€¼ï¼Œè¿™æ˜¯é€šè¿‡èåˆæœ¬åœ°ã€åŒºåŸŸå’Œå…¨å±€çº§åˆ«çš„ç‰¹å¾æ¥å®ç°çš„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯¦ç»†é˜è¿°äº†ä¸€ç§æ¨¡å‹å¯è§£é‡Šçš„åŒåŸŸLDMARæ¡†æ¶ï¼Œç§°ä¸ºPDuMSRNetï¼Œå¹¶é‡‡ç”¨å³æ—¶å¼•å¯¼ç­–ç•¥ä¸ºå¤šä¸ªå‰‚é‡æ°´å¹³è®­ç»ƒå•ä¸€æ¨¡å‹ã€‚æˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªå³æ—¶å¼•å¯¼æ¨¡å—ï¼Œå…¶è¾“å…¥åŒ…æ‹¬å‰‚é‡æ°´å¹³ã€é‡‘å±æ©ç å’Œè¾“å…¥å®ä¾‹ï¼Œä»¥æä¾›å¤šç§æŒ‡å¯¼ä¿¡æ¯ï¼Œä½¿å•ä¸€æ¨¡å‹èƒ½å¤Ÿé€‚åº”å„ç§CTå‰‚é‡è®¾ç½®ã€‚åœ¨ä¸åŒå‰‚é‡æ°´å¹³ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›çš„LDMARæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.19687v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†ä¸€ç§é’ˆå¯¹ä½å‰‚é‡è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆLDCTï¼‰çš„é‡‘å±ä¼ªå½±å‡å°‘æŠ€æœ¯ã€‚æ–‡ç« æå‡ºäº†ä¸€ä¸ªåŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•ï¼Œå³æç¤ºå¼•å¯¼å¤šå°ºåº¦è‡ªé€‚åº”ç¨€ç–è¡¨ç¤ºé©±åŠ¨ç½‘ç»œï¼ˆPMSRNetï¼‰ï¼Œç”¨äºåŒæ—¶å®ç°LDCTé‡å»ºå’Œé‡‘å±ä¼ªå½±å‡å°‘ï¼ˆLDMARï¼‰ã€‚è¯¥ç½‘ç»œè®¾è®¡çµæ„Ÿæ¥æºäºå¤šå°ºåº¦ç¨€ç–æ¡†æ¶ï¼Œå¯ä»¥å……åˆ†åˆ©ç”¨å°ºåº¦å†…ç‰¹æ€§å’Œè·¨å°ºåº¦äº’è¡¥æ€§ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜ä»‹ç»äº†ä¸€ç§æ¨¡å‹å¯è§£é‡Šçš„è·¨åŸŸLDMARæ¡†æ¶ï¼ˆPDuMSRNetï¼‰ï¼Œé€šè¿‡æç¤ºå¼•å¯¼ç­–ç•¥è®­ç»ƒå•ä¸€æ¨¡å‹ä»¥é€‚åº”å¤šç§å‰‚é‡æ°´å¹³ã€‚è¯¥æ–¹æ³•çš„æ€§èƒ½åœ¨å¤šä¸ªå‰‚é‡æ°´å¹³ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒéªŒè¯ï¼Œè¯æ˜äº†å…¶ä¼˜äºç°æœ‰LDMARæ–¹æ³•çš„æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä½å‰‚é‡è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆLDCTï¼‰å¯ä»¥å‡å°‘Xå°„çº¿è¾å°„æš´éœ²ï¼Œä½†å¯èƒ½å¯¼è‡´å›¾åƒè´¨é‡ä¸‹é™ï¼Œç”šè‡³å‡ºç°é‡‘å±ä¼ªå½±ã€‚</li>
<li>ç°æœ‰çš„åŸºäºæ·±åº¦å­¦ä¹ çš„LDMARæ–¹æ³•é¢ä¸´ä¸¤ä¸ªä¸»è¦é™åˆ¶ï¼šå¿½è§†å¤šå°ºåº¦å’Œå°ºåº¦å†…ä¿¡æ¯ï¼Œä»¥åŠä¸ºæ¯ä¸ªå‰‚é‡æ°´å¹³è®­ç»ƒä¸åŒæ¨¡å‹éœ€è¦å¤§é‡å­˜å‚¨ç©ºé—´ã€‚</li>
<li>æå‡ºçš„PMSRNetç½‘ç»œå—åˆ°å¤šå°ºåº¦ç¨€ç–æ¡†æ¶çš„å¯å‘ï¼Œå¯ä»¥å……åˆ†åˆ©ç”¨å°ºåº¦å†…ç‰¹æ€§å’Œè·¨å°ºåº¦äº’è¡¥æ€§ï¼Œé€šè¿‡ç²¾ç»†çš„æç¤ºå¼•å¯¼å°ºåº¦è‡ªé€‚åº”é˜ˆå€¼ç”Ÿæˆå™¨å’Œå¤šå°ºåº¦ç³»æ•°èåˆæ¨¡å—å®ç°LDMARã€‚</li>
<li>æç¤ºå¼•å¯¼æ¨¡å—åŒ…å«å‰‚é‡æ°´å¹³ã€é‡‘å±æ©ç å’Œè¾“å…¥å®ä¾‹ï¼Œä¸ºå„ç§å‰‚é‡è®¾ç½®æä¾›æŒ‡å¯¼ä¿¡æ¯ï¼Œä½¿å•ä¸€æ¨¡å‹é€‚åº”å„ç§CTå‰‚é‡æ°´å¹³ã€‚</li>
<li>PDuMSRNetæ¡†æ¶æ˜¯ä¸€ç§æ¨¡å‹å¯è§£é‡Šçš„è·¨åŸŸLDMARæ¡†æ¶ï¼Œé€šè¿‡æç¤ºå¼•å¯¼ç­–ç•¥è®­ç»ƒå•ä¸€æ¨¡å‹ï¼Œä»¥å¤„ç†å¤šç§å‰‚é‡æ°´å¹³çš„LDCTå›¾åƒã€‚</li>
<li>å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨å¤šä¸ªå‰‚é‡æ°´å¹³ä¸Šä¼˜äºç°æœ‰çš„LDMARæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.19687">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6e479c7a843c604677a86e76b98f9f81.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2ecd3ada83967f6ed1e390e8427bd52f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-34b460f8051096779b86126564393a23.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="On-Neutron-Star-Natal-Kicks-in-High-Mass-X-Ray-Binaries-Insights-from-Population-Synthesis"><a href="#On-Neutron-Star-Natal-Kicks-in-High-Mass-X-Ray-Binaries-Insights-from-Population-Synthesis" class="headerlink" title="On Neutron Star Natal Kicks in High-Mass X-Ray Binaries: Insights from   Population Synthesis"></a>On Neutron Star Natal Kicks in High-Mass X-Ray Binaries: Insights from   Population Synthesis</h2><p><strong>Authors:Xiangyu Ivy Wang, Xiang-Dong Li</strong></p>
<p>The motion of neutron stars (NSs) in the Galaxy is largely dependent on natal kicks received by the NSs during supernova explosions. Thus, the measured peculiar velocities of NS high-mass X-ray binaries (HMXBs) provide valuable clues to natal kicks, which also play an important role in the evolution of HMXBs. In this work, we collect proper motions, radial velocities and parallaxes for 36 NS HMXBs to derive their peculiar velocities at the birth of the NSs. We then use binary population synthesis to simulate the velocities of NS HMXBs with various choices of the kick velocity distribution for both core-collapse and electron-capture supernovae. Comparing the simulated and measured velocities, orbital periods, and eccentricities, we show that the natal kick distribution that can best match the observations is characterized by a bimodal Maxwellian distribution with $\sigma_1$ &#x3D; 320 km s$^{-1}$ (for core-collapse supernovae) and $\sigma_2$ &#x3D; 80 km s$^{-1}$ (for electron-capture supernovae) and the He core mass for the latter in the range of $(1.83-2.25)$ $M_{\odot}$. Our findings provide useful insights for further population synthesis and binary evolution studies of NS binaries. </p>
<blockquote>
<p>ä¸­å­æ˜Ÿï¼ˆNSsï¼‰åœ¨é“¶æ²³ç³»ä¸­çš„è¿åŠ¨åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºè¶…æ–°æ˜Ÿçˆ†å‘æœŸé—´NSsæ‰€å—åˆ°çš„åˆç”Ÿå†²å‡»ã€‚å› æ­¤ï¼Œæµ‹é‡ä¸­å­æ˜Ÿé«˜è´¨é‡Xå°„çº¿åŒæ˜Ÿï¼ˆHMXBsï¼‰çš„ç‰¹å®šé€Ÿåº¦æä¾›äº†å…³äºåˆç”Ÿå†²å‡»çš„å®è´µçº¿ç´¢ï¼Œè¿™äº›çº¿ç´¢åœ¨HMXBsçš„æ¼”åŒ–ä¸­ä¹Ÿèµ·ç€é‡è¦ä½œç”¨ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ”¶é›†äº†36ä¸ªä¸­å­æ˜ŸHMXBçš„è‡ªè¡Œè¿åŠ¨ã€é€Ÿåº¦å’Œè§†å·®è·ç¦»ï¼Œä»¥å¯¼å‡ºä¸­å­æ˜Ÿè¯ç”Ÿæ—¶çš„ç‰¹æ®Šé€Ÿåº¦ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨äºŒè¿›åˆ¶äººå£åˆæˆæ³•æ¨¡æ‹Ÿäº†å„ç§ä¸­å­æ˜Ÿåˆç”Ÿå†²å‡»é€Ÿåº¦åˆ†å¸ƒä¸‹çš„NS HMXBé€Ÿåº¦ï¼ŒåŒ…æ‹¬æ ¸å¿ƒå´©æºƒå‹å’Œç”µå­ä¿˜è·å‹è¶…æ–°æ˜Ÿã€‚é€šè¿‡æ¯”è¾ƒæ¨¡æ‹Ÿå’Œæµ‹é‡çš„é€Ÿåº¦ã€è½¨é“å‘¨æœŸå’Œç¦»å¿ƒç‡ï¼Œæˆ‘ä»¬å‘ç°æœ€èƒ½ä¸è§‚æµ‹ç›¸åŒ¹é…çš„åˆç”Ÿå†²å‡»åˆ†å¸ƒç‰¹å¾æ˜¯å…·æœ‰åŒå³°éº¦å…‹æ–¯éŸ¦åˆ†å¸ƒï¼Œå…¶ä¸­Ïƒ1&#x3D; 320å…¬é‡Œæ¯ç§’ï¼ˆé€‚ç”¨äºæ ¸å¿ƒå´©æºƒå‹è¶…æ–°æ˜Ÿï¼‰å’ŒÏƒ2&#x3D; 80å…¬é‡Œæ¯ç§’ï¼ˆé€‚ç”¨äºç”µå­ä¿˜è·å‹è¶…æ–°æ˜Ÿï¼‰ï¼Œåè€…çš„æ°¦æ ¸è´¨é‡èŒƒå›´åœ¨ï¼ˆ1.83-2.25ï¼‰$M_{\odot}$ä¹‹é—´ã€‚æˆ‘ä»¬çš„å‘ç°ä¸ºè¿›ä¸€æ­¥çš„äººå£åˆæˆå­¦å’ŒNSåŒæ˜Ÿçš„äºŒå…ƒæ¼”åŒ–ç ”ç©¶æä¾›äº†æœ‰ç›Šçš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.19672v1">PDF</a> 16 pages, 4 figures, 3 tables, accepted by ApJ</p>
<p><strong>Summary</strong></p>
<p>ä¸­å­æ˜Ÿåœ¨é“¶æ²³ç³»ä¸­çš„è¿åŠ¨å—åˆ°å…¶è¯ç”Ÿæ—¶è¶…æ–°æ˜Ÿçˆ†ç‚¸æ‰€å¸¦æ¥çš„åˆå§‹é€Ÿåº¦å½±å“ã€‚é€šè¿‡åˆ†æä¸­å­æ˜Ÿé«˜è´¨é‡Xå°„çº¿åŒæ˜Ÿï¼ˆHMXBsï¼‰çš„ç‰¹æ®Šé€Ÿåº¦ï¼Œå¯ä»¥äº†è§£åˆå§‹é€Ÿåº¦çš„å½±å“ï¼Œè¿™å¯¹ç†è§£HMXBsçš„æ¼”åŒ–è‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶æ”¶é›†äº†36ä¸ªNS HMXBçš„å¤©ä½“è¿åŠ¨å­¦æ•°æ®ï¼Œæ¨ç®—å‡ºåœ¨è¯ç”Ÿæ—¶ä¸­å­æ˜ŸNSçš„ç‰¹æ®Šé€Ÿåº¦ã€‚é€šè¿‡äºŒå…ƒåˆæˆäººå£æ¨¡å‹æ¨¡æ‹Ÿç‰¹æ®Šé€Ÿåº¦ï¼Œä¸å¤šç§æ¨¡å‹æ¯”è¾ƒæ ¸å¿ƒå¡Œç¼©å‹å’Œç”µå­æ•è·å‹è¶…æ–°æ˜Ÿçš„ä¸­å­æ˜Ÿé€Ÿåº¦çš„åˆ†å¸ƒç‰¹æ€§ã€‚æ¯”è¾ƒæ¨¡æ‹Ÿä¸è§‚æµ‹å¾—åˆ°çš„é€Ÿåº¦ã€è½¨é“å‘¨æœŸå’Œåå¿ƒåº¦ç­‰ç»“æœåæ˜¾ç¤ºæœ€ä½³åŒ¹é…çš„åˆç”ŸçŠ¶æ€ä¸­æ€§æµä½“é€Ÿåº¦ä¸ºåŒ…å«ä¸€ç»„å³°å€¼ä¸ºæ ¸å¿ƒå¡Œç¼©è¶…æ–°æ˜Ÿçš„ä¸­å‡æ–¹æ ¹é€Ÿåº¦ä¸º$\sigma_1$&#x3D; 320åƒç±³æ¯ç§’ï¼Œå¦ä¸€ç»„ä¸ºç”µå­æ•è·å‹è¶…æ–°æ˜Ÿä¸­å‡æ–¹æ ¹é€Ÿåº¦ä¸º$\sigma_2$&#x3D; 80åƒç±³æ¯ç§’ï¼Œåè€…çš„Heæ ¸å¿ƒè´¨é‡åœ¨$(1.83-2.25)$Mâ˜‰èŒƒå›´å†…ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¯¹äºè¿›ä¸€æ­¥çš„äºŒå…ƒåˆæˆäººå£æ¨¡å‹å’ŒäºŒå…ƒæ¼”åŒ–ç ”ç©¶æä¾›æœ‰ä»·å€¼çš„ä¿¡æ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¸­å­æ˜Ÿåœ¨é“¶æ²³ç³»ä¸­çš„è¿åŠ¨å—åˆ°è¶…æ–°æ˜Ÿçˆ†ç‚¸å¸¦æ¥çš„åˆç”Ÿé€Ÿåº¦å½±å“ã€‚</li>
<li>é«˜è´¨é‡Xå°„çº¿åŒæ˜Ÿï¼ˆHMXBsï¼‰çš„ç‰¹æ®Šé€Ÿåº¦æä¾›äº†å…³äºåˆç”Ÿé€Ÿåº¦çš„çº¿ç´¢ã€‚</li>
<li>ç ”ç©¶æ”¶é›†äº†NS HMXBsçš„å¤©ä½“è¿åŠ¨å­¦æ•°æ®ä»¥æ¨ç®—ä¸­å­æ˜Ÿè¯ç”Ÿæ—¶çš„ç‰¹æ®Šé€Ÿåº¦ã€‚</li>
<li>ä½¿ç”¨äºŒå…ƒäººå£åˆæˆæ¨¡å‹æ¨¡æ‹Ÿç‰¹æ®Šé€Ÿåº¦å¹¶å¯¹æ¯”å¤šç§æ¨¡å‹ä¸‹çš„ä¸­å­æ˜Ÿé€Ÿåº¦åˆ†å¸ƒç‰¹æ€§ã€‚</li>
<li>æœ€ä½³åŒ¹é…çš„åˆç”ŸçŠ¶æ€ä¸­æ€§æµä½“é€Ÿåº¦åˆ†å¸ƒè¡¨ç°ä¸ºåŒå³°Maxwellianåˆ†å¸ƒï¼Œå…¶ä¸­æ ¸å¿ƒå¡Œç¼©è¶…æ–°æ˜Ÿçš„ä¸­å‡æ–¹æ ¹é€Ÿåº¦ä¸º320åƒç±³æ¯ç§’ï¼Œå¦ä¸€ç»„ä¸ºç”µå­æ•è·å‹è¶…æ–°æ˜Ÿçš„ä¸­å‡æ–¹æ ¹é€Ÿåº¦ä¸º80åƒç±³æ¯ç§’ã€‚</li>
<li>ç”µå­æ•è·å‹è¶…æ–°æ˜Ÿçš„Heæ ¸å¿ƒè´¨é‡èŒƒå›´åœ¨$(1.83-2.25)$Mâ˜‰ä¹‹é—´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.19672">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f57fedae1386311c4c59da01b98a1aac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-99efb1fb9e574640d6bd450b8c031a44.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9b6033a98589a0175744c342769cc7f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-267069d83647e8108dee1ed46490a082.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="A-Unified-Benchmark-of-Federated-Learning-with-Kolmogorov-Arnold-Networks-for-Medical-Imaging"><a href="#A-Unified-Benchmark-of-Federated-Learning-with-Kolmogorov-Arnold-Networks-for-Medical-Imaging" class="headerlink" title="A Unified Benchmark of Federated Learning with Kolmogorov-Arnold   Networks for Medical Imaging"></a>A Unified Benchmark of Federated Learning with Kolmogorov-Arnold   Networks for Medical Imaging</h2><p><strong>Authors:Youngjoon Lee, Jinu Gong, Joonhyuk Kang</strong></p>
<p>Federated Learning (FL) enables model training across decentralized devices without sharing raw data, thereby preserving privacy in sensitive domains like healthcare. In this paper, we evaluate Kolmogorov-Arnold Networks (KAN) architectures against traditional MLP across six state-of-the-art FL algorithms on a blood cell classification dataset. Notably, our experiments demonstrate that KAN can effectively replace MLP in federated environments, achieving superior performance with simpler architectures. Furthermore, we analyze the impact of key hyperparameters-grid size and network architecture-on KAN performance under varying degrees of Non-IID data distribution. Additionally, our ablation studies reveal that optimizing KAN width while maintaining minimal depth yields the best performance in federated settings. As a result, these findings establish KAN as a promising alternative for privacy-preserving medical imaging applications in distributed healthcare. To the best of our knowledge, this is the first comprehensive benchmark of KAN in FL settings for medical imaging task. </p>
<blockquote>
<p>è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰èƒ½å¤Ÿåœ¨åˆ†æ•£çš„è®¾å¤‡ä¸Šè¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œæ— éœ€å…±äº«åŸå§‹æ•°æ®ï¼Œä»è€Œåœ¨åŒ»ç–—ç­‰æ•æ„Ÿé¢†åŸŸä¿æŠ¤éšç§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è¯„ä¼°äº†Kolmogorov-Arnoldç½‘ç»œï¼ˆKANï¼‰æ¶æ„ä¸ä¼ ç»Ÿå¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰åœ¨å…­ç§æœ€å…ˆè¿›è”é‚¦å­¦ä¹ ç®—æ³•ä¸Šçš„è¡¨ç°ï¼Œè¿™äº›æ•°æ®æ¥è‡ªäºè¡€ç»†èƒåˆ†ç±»æ•°æ®é›†ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œåœ¨è”é‚¦ç¯å¢ƒä¸­ï¼ŒKANå¯ä»¥æœ‰æ•ˆåœ°æ›¿ä»£MLPï¼Œä»¥æ›´ç®€å•çš„æ¶æ„å®ç°å“è¶Šçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ†æäº†å…³é”®è¶…å‚æ•°â€”â€”ç½‘æ ¼å¤§å°å’Œç½‘ç»œæ¶æ„åœ¨ä¸åŒç¨‹åº¦çš„éç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆNon-IIDï¼‰æ•°æ®åˆ†å¸ƒä¸‹å¯¹KANæ€§èƒ½çš„å½±å“ã€‚å¦å¤–ï¼Œæˆ‘ä»¬çš„æ¶ˆèç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåœ¨ä¿æŒæ·±åº¦æœ€å°çš„æƒ…å†µä¸‹ä¼˜åŒ–KANçš„å®½åº¦ï¼Œåœ¨è”é‚¦ç¯å¢ƒä¸­å¯ä»¥è·å¾—æœ€ä½³æ€§èƒ½ã€‚å› æ­¤ï¼Œè¿™äº›å‘ç°ç¡®ç«‹äº†KANåœ¨åˆ†å¸ƒå¼åŒ»ç–—ä¿å¥ä¸­çš„éšç§ä¿æŠ¤åŒ»å­¦æˆåƒåº”ç”¨çš„æ½œåŠ›ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–æ¬¡åœ¨è”é‚¦å­¦ä¹ ç¯å¢ƒä¸­å¯¹KANè¿›è¡ŒåŒ»å­¦æˆåƒä»»åŠ¡çš„å…¨é¢åŸºå‡†æµ‹è¯•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.19639v1">PDF</a> 5 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†Federated Learningï¼ˆFLï¼‰åœ¨åŒ»ç–—å›¾åƒåˆ†ç±»ä¸­çš„è¡¨ç°ï¼Œè¯„ä¼°äº†Kolmogorov-Arnold Networksï¼ˆKANï¼‰ä¸ä¼ ç»Ÿå¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰åœ¨å…­ç§å…ˆè¿›çš„FLç®—æ³•ä¸Šçš„æ€§èƒ½å·®å¼‚ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨è”é‚¦ç¯å¢ƒä¸­ï¼ŒKANèƒ½å¤Ÿæ›¿ä»£MLPå®ç°æ›´ä¼˜è¶Šçš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†éç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆNon-IIDï¼‰æ•°æ®æ—¶è¡¨ç°çªå‡ºã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å‘ç°ä¼˜åŒ–KANçš„å®½åº¦å¹¶ä¿æŒè¾ƒå°çš„æ·±åº¦æœ‰åŠ©äºæé«˜æ€§èƒ½ã€‚å› æ­¤ï¼ŒKANæœ‰æœ›æˆä¸ºåˆ†å¸ƒå¼åŒ»ç–—æˆåƒåº”ç”¨ä¸­éšç§ä¿æŠ¤çš„æœ‰åŠ›æ›¿ä»£æ–¹æ¡ˆã€‚æœ¬æ–‡æ˜¯å¯¹åŒ»ç–—å›¾åƒé¢†åŸŸä¸­çš„KANåœ¨FLç¯å¢ƒä¸‹çš„é¦–æ¬¡å…¨é¢è¯„ä¼°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Federated Learning (FL) èƒ½å¤Ÿåœ¨ä¸å…±äº«åŸå§‹æ•°æ®çš„æƒ…å†µä¸‹è¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œä¿æŠ¤éšç§æ•æ„Ÿé¢†åŸŸå¦‚åŒ»ç–—çš„æ•°æ®éšç§ã€‚</li>
<li>Kolmogorov-Arnold Networks (KAN) åœ¨Federated Learningç¯å¢ƒä¸­ç›¸è¾ƒäºä¼ ç»Ÿå¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰å±•ç°å‡ºä¼˜è¶Šæ€§èƒ½ã€‚</li>
<li>KANåœ¨å¤„ç†éç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆNon-IIDï¼‰æ•°æ®æ—¶è¡¨ç°è‰¯å¥½ã€‚</li>
<li>KANçš„æ€§èƒ½å—åˆ°ç½‘æ ¼å¤§å°å’Œç½‘ç»œæ¶æ„ç­‰è¶…å‚æ•°çš„å½±å“ã€‚</li>
<li>åœ¨è”é‚¦ç¯å¢ƒä¸­ï¼Œä¼˜åŒ–KANçš„å®½åº¦å¹¶ç»´æŒè¾ƒå°çš„æ·±åº¦å¯ä»¥è·å¾—æœ€ä½³æ€§èƒ½ã€‚</li>
<li>æ­¤ç ”ç©¶é¦–æ¬¡å…¨é¢è¯„ä¼°äº†KANåœ¨åŒ»ç–—å›¾åƒé¢†åŸŸçš„Federated Learningç¯å¢ƒä¸­çš„è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.19639">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8b09222d324549a8b2f8151ba8ff5cc6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6da4960f95bd7e3300a745719ecd5073.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fa6aa1a138dce7d2aaee5454f70803b9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-82f17ee4668d8a114f9a3dcc6e796473.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b8eb4d02f732b2a17ad5d1226818131.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b581ed6bdcb464932fcc1385b1a14c3c.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="NSegment-Noisy-Segment-Improves-Remote-Sensing-Image-Segmentation"><a href="#NSegment-Noisy-Segment-Improves-Remote-Sensing-Image-Segmentation" class="headerlink" title="NSegment : Noisy Segment Improves Remote Sensing Image Segmentation"></a>NSegment : Noisy Segment Improves Remote Sensing Image Segmentation</h2><p><strong>Authors:Yechan Kim, DongHo Yoon, SooYeon Kim, Moongu Jeon</strong></p>
<p>Labeling errors in remote sensing (RS) image segmentation datasets often remain implicit and subtle due to ambiguous class boundaries, mixed pixels, shadows, complex terrain features, and subjective annotator bias. Furthermore, the scarcity of annotated RS data due to high image acquisition and labeling costs complicates training noise-robust models. While sophisticated mechanisms such as label selection or noise correction might address this issue, they tend to increase training time and add implementation complexity. In this letter, we propose NSegment-a simple yet effective data augmentation solution to mitigate this issue. Unlike traditional methods, it applies elastic transformations only to segmentation labels, varying deformation intensity per sample in each training epoch to address annotation inconsistencies. Experimental results demonstrate that our approach improves the performance of RS image segmentation on various state-of-the-art models. </p>
<blockquote>
<p>é¥æ„Ÿï¼ˆRSï¼‰å›¾åƒåˆ†å‰²æ•°æ®é›†æ ‡æ³¨é”™è¯¯é€šå¸¸ç”±äºæ¨¡ç³Šçš„ç±»åˆ«è¾¹ç•Œã€æ··åˆåƒç´ ã€é˜´å½±ã€å¤æ‚çš„åœ°å½¢ç‰¹å¾å’Œä¸»è§‚æ ‡æ³¨è€…åè§è€Œä¿æŒéšè”½å’Œå¾®å¦™ã€‚æ­¤å¤–ï¼Œç”±äºå›¾åƒé‡‡é›†å’Œæ ‡æ³¨çš„é«˜æˆæœ¬ï¼Œå¯¼è‡´æ ‡æ³¨çš„é¥æ„Ÿæ•°æ®ç¨€ç¼ºï¼Œè¿™åŠ å‰§äº†è®­ç»ƒå™ªå£°é²æ£’æ¨¡å‹çš„å¤æ‚æ€§ã€‚è™½ç„¶æ ‡ç­¾é€‰æ‹©æˆ–å™ªå£°æ ¡æ­£ç­‰å¤æ‚æœºåˆ¶å¯èƒ½è§£å†³æ­¤é—®é¢˜ï¼Œä½†å®ƒä»¬å¾€å¾€ä¼šå¢åŠ è®­ç»ƒæ—¶é—´å¹¶å¢åŠ å®ç°å¤æ‚æ€§ã€‚åœ¨æœ¬ä¿¡ä¸­ï¼Œæˆ‘ä»¬æå‡ºNSegmentâ€”â€”ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ•°æ®å¢å¼ºè§£å†³æ–¹æ¡ˆï¼Œä»¥ç¼“è§£è¿™ä¸ªé—®é¢˜ã€‚ä¸ä¼ ç»Ÿçš„æ•°æ®å¢å¼ºæ–¹æ³•ä¸åŒï¼Œå®ƒä»…å¯¹åˆ†å‰²æ ‡ç­¾åº”ç”¨å¼¹æ€§å˜æ¢ï¼Œå¹¶åœ¨æ¯ä¸ªè®­ç»ƒå‘¨æœŸä¸­å¯¹æ¯ä¸ªæ ·æœ¬çš„å˜å½¢å¼ºåº¦è¿›è¡Œå˜åŒ–ï¼Œä»¥è§£å†³æ ‡æ³¨ä¸ä¸€è‡´çš„é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æé«˜äº†å„ç§æœ€å…ˆè¿›æ¨¡å‹åœ¨é¥æ„Ÿå›¾åƒåˆ†å‰²æ–¹é¢çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.19634v1">PDF</a> Preprint</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦è®¨è®ºäº†é¥æ„Ÿå›¾åƒåˆ†å‰²æ•°æ®é›†æ ‡æ³¨é”™è¯¯çš„é—®é¢˜ï¼Œè¿™äº›é”™è¯¯ç”±äºæ¨¡ç³Šçš„ç±»åˆ«è¾¹ç•Œã€æ··åˆåƒç´ ã€é˜´å½±ã€å¤æ‚åœ°å½¢ç‰¹å¾å’Œä¸»è§‚æ ‡æ³¨è€…åè§è€Œéšæ€§å­˜åœ¨ã€‚ç”±äºé«˜å›¾åƒé‡‡é›†å’Œæ ‡æ³¨æˆæœ¬ï¼Œå¯¼è‡´æ ‡æ³¨çš„é¥æ„Ÿæ•°æ®ç¨€ç¼ºï¼Œè®­ç»ƒå™ªå£°é²æ£’æ€§æ¨¡å‹å˜å¾—å¤æ‚ã€‚é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºNSegmentçš„ç®€å•è€Œæœ‰æ•ˆçš„æ•°æ®å¢å¼ºè§£å†³æ–¹æ¡ˆã€‚ä¸åŒäºä¼ ç»Ÿæ–¹æ³•ï¼ŒNSegmentä»…å¯¹åˆ†å‰²æ ‡ç­¾åº”ç”¨å¼¹æ€§å˜æ¢ï¼Œæ¯ä¸ªè®­ç»ƒå‘¨æœŸä¸­å¯¹æ¯ä¸ªæ ·æœ¬çš„å˜å½¢å¼ºåº¦è¿›è¡Œå˜åŒ–ï¼Œä»¥è§£å†³æ ‡æ³¨ä¸ä¸€è‡´çš„é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æé«˜äº†é¥æ„Ÿå›¾åƒåˆ†å‰²åœ¨å„ç§å…ˆè¿›æ¨¡å‹ä¸Šçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¥æ„Ÿå›¾åƒåˆ†å‰²æ•°æ®é›†çš„æ ‡æ³¨é”™è¯¯æ˜¯éšæ€§å’Œå¾®å¦™çš„ï¼Œä¸»è¦æºäºæ¨¡ç³Šçš„ç±»åˆ«è¾¹ç•Œã€æ··åˆåƒç´ ã€é˜´å½±ç­‰é—®é¢˜ã€‚</li>
<li>æ ‡æ³¨çš„é¥æ„Ÿæ•°æ®ç¨€ç¼ºï¼Œå› ä¸ºå›¾åƒé‡‡é›†å’Œæ ‡æ³¨çš„æˆæœ¬å¾ˆé«˜ã€‚</li>
<li>è®­ç»ƒå™ªå£°é²æ£’æ€§æ¨¡å‹æ˜¯å¿…è¦çš„ï¼Œå› ä¸ºæ ‡æ³¨é”™è¯¯ä¼šå½±å“æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>ä¼ ç»Ÿæ•°æ®å¢å¼ºæ–¹æ³•å¯èƒ½ä¸é€‚ç”¨äºé¥æ„Ÿå›¾åƒåˆ†å‰²é—®é¢˜ã€‚</li>
<li>NSegmentæ˜¯ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ•°æ®å¢å¼ºè§£å†³æ–¹æ¡ˆï¼Œå®ƒé€šè¿‡åº”ç”¨å¼¹æ€§å˜æ¢æ¥è§£å†³æ ‡æ³¨ä¸ä¸€è‡´çš„é—®é¢˜ã€‚</li>
<li>NSegmentåªåœ¨æ¯ä¸ªè®­ç»ƒå‘¨æœŸä¸­å¯¹åˆ†å‰²æ ‡ç­¾åº”ç”¨å˜æ¢ï¼Œä»¥æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.19634">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-16c6d0523c8226596deead44539d0fa0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36739d39a34aef98deccd3b6b1197410.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-662089891e12ecb22aad3a2c43059dc9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5995b096b27118f607b5d98b2c6e3c25.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fc9a37c36796ae214bc4ea3f0fda136c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b0c1ab6b622b3020b334a50067590b7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-66961e898e1a789581cfa9988bc0ec62.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ebe8bf68217760045ae5c75df162de4b.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="AI-Alignment-in-Medical-Imaging-Unveiling-Hidden-Biases-Through-Counterfactual-Analysis"><a href="#AI-Alignment-in-Medical-Imaging-Unveiling-Hidden-Biases-Through-Counterfactual-Analysis" class="headerlink" title="AI Alignment in Medical Imaging: Unveiling Hidden Biases Through   Counterfactual Analysis"></a>AI Alignment in Medical Imaging: Unveiling Hidden Biases Through   Counterfactual Analysis</h2><p><strong>Authors:Haroui Ma, Francesco Quinzan, Theresa Willem, Stefan Bauer</strong></p>
<p>Machine learning (ML) systems for medical imaging have demonstrated remarkable diagnostic capabilities, but their susceptibility to biases poses significant risks, since biases may negatively impact generalization performance. In this paper, we introduce a novel statistical framework to evaluate the dependency of medical imaging ML models on sensitive attributes, such as demographics. Our method leverages the concept of counterfactual invariance, measuring the extent to which a modelâ€™s predictions remain unchanged under hypothetical changes to sensitive attributes. We present a practical algorithm that combines conditional latent diffusion models with statistical hypothesis testing to identify and quantify such biases without requiring direct access to counterfactual data. Through experiments on synthetic datasets and large-scale real-world medical imaging datasets, including \textsc{cheXpert} and MIMIC-CXR, we demonstrate that our approach aligns closely with counterfactual fairness principles and outperforms standard baselines. This work provides a robust tool to ensure that ML diagnostic systems generalize well, e.g., across demographic groups, offering a critical step towards AI safety in healthcare. Code: <a target="_blank" rel="noopener" href="https://github.com/Neferpitou3871/AI-Alignment-Medical-Imaging">https://github.com/Neferpitou3871/AI-Alignment-Medical-Imaging</a>. </p>
<blockquote>
<p>åŒ»å­¦å½±åƒæœºå™¨å­¦ä¹ ç³»ç»Ÿå±•ç°å‡ºäº†å‡ºè‰²çš„è¯Šæ–­èƒ½åŠ›ï¼Œä½†å®ƒä»¬æ˜“å—åè§å½±å“ï¼Œè¿™æ„æˆäº†é‡å¤§é£é™©ï¼Œå› ä¸ºåè§å¯èƒ½ä¼šç»™æ³›åŒ–æ€§èƒ½å¸¦æ¥è´Ÿé¢å½±å“ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°å‹ç»Ÿè®¡æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°åŒ»å­¦å½±åƒæœºå™¨å­¦ä¹ æ¨¡å‹å¯¹äººå£ç»Ÿè®¡å­¦ç­‰æ•æ„Ÿå±æ€§çš„ä¾èµ–ç¨‹åº¦ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨åäº‹å®ä¸å˜æ€§çš„æ¦‚å¿µï¼Œè¡¡é‡æ¨¡å‹é¢„æµ‹åœ¨æ•æ„Ÿå±æ€§å‡è®¾æ€§å˜åŒ–ä¸‹ä¿æŒä¸å˜çš„ç¨‹åº¦ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å®ç”¨ç®—æ³•ï¼Œè¯¥ç®—æ³•ç»“åˆäº†æ¡ä»¶æ½œåœ¨æ‰©æ•£æ¨¡å‹å’Œç»Ÿè®¡å‡è®¾æ£€éªŒï¼Œèƒ½å¤Ÿåœ¨æ— éœ€è®¿é—®åäº‹å®æ•°æ®çš„æƒ…å†µä¸‹è¯†åˆ«å’Œé‡åŒ–æ­¤ç±»åè§ã€‚é€šè¿‡åœ¨åˆæˆæ•°æ®é›†å’Œå¤§è§„æ¨¡ç°å®ä¸–ç•ŒåŒ»å­¦å½±åƒæ•°æ®é›†ï¼ˆåŒ…æ‹¬\textsc{cheXpert}å’ŒMIMIC-CXRï¼‰ä¸Šçš„å®éªŒï¼Œæˆ‘ä»¬è¯æ˜æˆ‘ä»¬çš„æ–¹æ³•ä¸åäº‹å®å…¬å¹³åŸåˆ™ç´§å¯†å¥‘åˆå¹¶ä¼˜äºæ ‡å‡†åŸºçº¿ã€‚è¿™é¡¹å·¥ä½œä¸ºç¡®ä¿æœºå™¨å­¦ä¹ è¯Šæ–­ç³»ç»Ÿå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼ˆä¾‹å¦‚ï¼Œè·¨è¶Šä¸åŒäººå£ç¾¤ä½“ï¼‰æä¾›äº†å¯é å·¥å…·ï¼Œæ˜¯åŒ»ç–—ä¿å¥é¢†åŸŸäººå·¥æ™ºèƒ½å®‰å…¨æ€§çš„å…³é”®ä¸€æ­¥ã€‚ä»£ç ï¼š<a target="_blank" rel="noopener" href="https://github.com/Neferpitou3871/AI-Alignment-Medical-Imaging%E3%80%82">https://github.com/Neferpitou3871/AI-Alignment-Medical-Imagingã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.19621v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„ç»Ÿè®¡æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°åŒ»ç–—å½±åƒæœºå™¨å­¦ä¹ æ¨¡å‹å¯¹æ•æ„Ÿå±æ€§ï¼ˆå¦‚äººå£ç»Ÿè®¡å­¦ç‰¹å¾ï¼‰çš„ä¾èµ–ç¨‹åº¦ã€‚è¯¥æ¡†æ¶åˆ©ç”¨åäº‹å®ä¸å˜æ€§çš„æ¦‚å¿µï¼Œæµ‹é‡æ¨¡å‹é¢„æµ‹åœ¨æ•æ„Ÿå±æ€§å‡è®¾å˜åŒ–ä¸‹çš„ç¨³å®šæ€§ã€‚é€šè¿‡ç»“åˆæ¡ä»¶æ½œåœ¨æ‰©æ•£æ¨¡å‹å’Œç»Ÿè®¡å‡è®¾æ£€éªŒçš„å®ç”¨ç®—æ³•ï¼Œèƒ½å¤Ÿåœ¨æ— éœ€åäº‹å®æ•°æ®çš„æƒ…å†µä¸‹è¯†åˆ«å’Œé‡åŒ–åè§ã€‚åœ¨åˆæˆæ•°æ®é›†å’Œå¤§è§„æ¨¡çœŸå®åŒ»ç–—å½±åƒæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¸åäº‹å®å…¬å¹³æ€§åŸåˆ™ç›¸ç¬¦ï¼Œå¹¶ä¼˜äºæ ‡å‡†åŸºçº¿ã€‚æ­¤å·¥å…·å¯ç¡®ä¿æœºå™¨å­¦ä¹ è¯Šæ–­ç³»ç»Ÿå…·æœ‰è‰¯å¥½çš„æ³›åŒ–æ€§èƒ½ï¼Œä¾‹å¦‚è·¨ä¸åŒäººç¾¤ï¼Œä¸ºåŒ»ç–—ä¿å¥ä¸­çš„AIå®‰å…¨æä¾›äº†å…³é”®æ­¥éª¤ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»ç–—å½±åƒæœºå™¨å­¦ä¹ æ¨¡å‹å­˜åœ¨å¯¹æ•æ„Ÿå±æ€§ï¼ˆå¦‚äººå£ç»Ÿè®¡å­¦ç‰¹å¾ï¼‰çš„åè§ï¼Œå¯èƒ½å½±å“æ¨¡å‹çš„æ³›åŒ–æ€§èƒ½ã€‚</li>
<li>æ–°å‹ç»Ÿè®¡æ¡†æ¶åˆ©ç”¨åäº‹å®ä¸å˜æ€§çš„æ¦‚å¿µæ¥è¯„ä¼°æ¨¡å‹é¢„æµ‹çš„ç¨³å®šæ€§ã€‚</li>
<li>æå‡ºçš„å®ç”¨ç®—æ³•ç»“åˆäº†æ¡ä»¶æ½œåœ¨æ‰©æ•£æ¨¡å‹å’Œç»Ÿè®¡å‡è®¾æ£€éªŒï¼Œä»¥è¯†åˆ«å’Œé‡åŒ–åè§ã€‚</li>
<li>è¯¥æ–¹æ³•æ— éœ€åäº‹å®æ•°æ®å³å¯æ“ä½œï¼Œå¯åœ¨çœŸå®ä¸–ç•Œçš„åº”ç”¨ä¸­æä¾›ä¾¿åˆ©ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ç¬¦åˆåäº‹å®å…¬å¹³æ€§åŸåˆ™ï¼Œå¹¶ä¼˜äºæ ‡å‡†åŸºçº¿ã€‚</li>
<li>æ­¤å·¥å…·æœ‰åŠ©äºæé«˜æœºå™¨å­¦ä¹ è¯Šæ–­ç³»ç»Ÿçš„æ³›åŒ–æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸åŒäººç¾¤ä¹‹é—´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.19621">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-697eab417cfb0a3c638b2efa916ee444.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-07d637465e9b81f9732b8a6c72aa96ee.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Magnifier-A-Multi-grained-Neural-Network-based-Architecture-for-Burned-Area-Delineation"><a href="#Magnifier-A-Multi-grained-Neural-Network-based-Architecture-for-Burned-Area-Delineation" class="headerlink" title="Magnifier: A Multi-grained Neural Network-based Architecture for Burned   Area Delineation"></a>Magnifier: A Multi-grained Neural Network-based Architecture for Burned   Area Delineation</h2><p><strong>Authors:Daniele Rege Cambrin, Luca Colomba, Paolo Garza</strong></p>
<p>In crisis management and remote sensing, image segmentation plays a crucial role, enabling tasks like disaster response and emergency planning by analyzing visual data. Neural networks are able to analyze satellite acquisitions and determine which areas were affected by a catastrophic event. The problem in their development in this context is the data scarcity and the lack of extensive benchmark datasets, limiting the capabilities of training large neural network models. In this paper, we propose a novel methodology, namely Magnifier, to improve segmentation performance with limited data availability. The Magnifier methodology is applicable to any existing encoder-decoder architecture, as it extends a model by merging information at different contextual levels through a dual-encoder approach: a local and global encoder. Magnifier analyzes the input data twice using the dual-encoder approach. In particular, the local and global encoders extract information from the same input at different granularities. This allows Magnifier to extract more information than the other approaches given the same set of input images. Magnifier improves the quality of the results of +2.65% on average IoU while leading to a restrained increase in terms of the number of trainable parameters compared to the original model. We evaluated our proposed approach with state-of-the-art burned area segmentation models, demonstrating, on average, comparable or better performances in less than half of the GFLOPs. </p>
<blockquote>
<p>åœ¨å±æœºç®¡ç†å’Œé¥æ„Ÿé¢†åŸŸï¼Œå›¾åƒåˆ†å‰²å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œé€šè¿‡åˆ†æè§†è§‰æ•°æ®ï¼Œèƒ½å¤Ÿå®Œæˆç¾éš¾å“åº”å’Œåº”æ€¥è§„åˆ’ç­‰ä»»åŠ¡ã€‚ç¥ç»ç½‘ç»œèƒ½å¤Ÿåˆ†æå«æ˜Ÿé‡‡é›†çš„æ•°æ®ï¼Œå¹¶ç¡®å®šå“ªäº›åŒºåŸŸå—åˆ°ç¾éš¾æ€§äº‹ä»¶çš„å½±å“ã€‚ç„¶è€Œï¼Œåœ¨æ­¤èƒŒæ™¯ä¹‹ä¸‹ï¼Œç¥ç»ç½‘ç»œçš„å‘å±•é—®é¢˜åœ¨äºæ•°æ®ç¨€ç¼ºï¼Œç¼ºä¹å¹¿æ³›çš„åŸºå‡†æ•°æ®é›†ï¼Œè¿™é™åˆ¶äº†è®­ç»ƒå¤§å‹ç¥ç»ç½‘ç»œæ¨¡å‹çš„èƒ½åŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå³Magnifierï¼Œä»¥æé«˜åœ¨æ•°æ®æœ‰é™æƒ…å†µä¸‹çš„åˆ†å‰²æ€§èƒ½ã€‚Magnifieræ–¹æ³•é€‚ç”¨äºä»»ä½•ç°æœ‰çš„ç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œå› ä¸ºå®ƒé€šè¿‡åŒç¼–ç å™¨æ–¹æ³•åˆå¹¶ä¸åŒä¸Šä¸‹æ–‡çº§åˆ«çš„ä¿¡æ¯æ¥æ‰©å±•æ¨¡å‹ï¼šæœ¬åœ°ç¼–ç å™¨å’Œå…¨å±€ç¼–ç å™¨ã€‚Magnifierä½¿ç”¨åŒç¼–ç å™¨æ–¹æ³•ä¸¤æ¬¡åˆ†æè¾“å…¥æ•°æ®ã€‚ç‰¹åˆ«æ˜¯ï¼Œæœ¬åœ°ç¼–ç å™¨å’Œå…¨å±€ç¼–ç å™¨ä»¥ä¸åŒçš„ç²’åº¦ä»åŒä¸€è¾“å…¥ä¸­æå–ä¿¡æ¯ã€‚è¿™ä½¿å¾—Magnifierèƒ½å¤Ÿåœ¨ç»™å®šç›¸åŒè¾“å…¥å›¾åƒé›†çš„æƒ…å†µä¸‹ï¼Œæå–æ¯”å…¶ä»–æ–¹æ³•æ›´å¤šçš„ä¿¡æ¯ã€‚Magnifieråœ¨æé«˜å¹³å‡IoUå€¼çš„åŸºç¡€ä¸Šæé«˜äº†ç»“æœè´¨é‡è¾¾+2.65%ï¼Œå¹¶ä¸”åœ¨ä¸åŸå§‹æ¨¡å‹ç›¸æ¯”çš„æƒ…å†µä¸‹ï¼Œå¯è®­ç»ƒå‚æ•°æ•°é‡æ–¹é¢å®ç°äº†é€‚åº¦çš„å¢é•¿ã€‚æˆ‘ä»¬ä½¿ç”¨å…ˆè¿›çš„ç‡ƒçƒ§åŒºåŸŸåˆ†å‰²æ¨¡å‹è¯„ä¼°äº†æˆ‘ä»¬æå‡ºçš„æ–¹æ³•ï¼Œç»“æœæ˜¾ç¤ºï¼Œåœ¨ä¸åˆ°ä¸€åŠGFLOPsçš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿè¾¾åˆ°å¹³å‡ç›¸å½“æˆ–æ›´å¥½çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.19589v1">PDF</a> Accepted in IEEE Journal of Selected Topics in Applied Earth   Observations and Remote Sensing</p>
<p><strong>Summary</strong></p>
<p>ç¥ç»ç½‘ç»œåœ¨åˆ†æå«æ˜Ÿå›¾åƒæ•°æ®ã€ç¡®å®šå—ç¾åŒºåŸŸæ–¹é¢å‘æŒ¥äº†é‡è¦ä½œç”¨ã€‚ç„¶è€Œï¼Œæ•°æ®ç¨€ç¼ºå’Œç¼ºä¹å¹¿æ³›çš„åŸºå‡†æ•°æ®é›†é™åˆ¶äº†å¤§å‹ç¥ç»ç½‘ç»œæ¨¡å‹çš„å‘å±•ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºMagnifierçš„æ–°æ–¹æ³•ï¼Œå¯åœ¨æœ‰é™æ•°æ®çš„æƒ…å†µä¸‹æé«˜åˆ†å‰²æ€§èƒ½ã€‚Magnifieræ–¹æ³•é€‚ç”¨äºä»»ä½•ç°æœ‰çš„ç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œå®ƒé€šè¿‡åŒç¼–ç å™¨æ–¹æ³•åˆå¹¶ä¸åŒä¸Šä¸‹æ–‡çº§åˆ«çš„ä¿¡æ¯æ¥æ‰©å±•æ¨¡å‹ï¼šæœ¬åœ°å’Œå…¨å±€ç¼–ç å™¨ã€‚Magnifierä½¿ç”¨åŒç¼–ç å™¨æ–¹æ³•ä¸¤æ¬¡åˆ†æè¾“å…¥æ•°æ®ï¼Œä½¿å¾—åœ¨ç›¸åŒçš„è¾“å…¥å›¾åƒé›†ä¸Šï¼Œå®ƒèƒ½æå–æ¯”å…¶ä»–æ–¹æ³•æ›´å¤šçš„ä¿¡æ¯ã€‚Magnifieræé«˜äº†å¹³å‡IoUå€¼è¾¾+2.65%ï¼ŒåŒæ—¶ä¸åŸå§‹æ¨¡å‹ç›¸æ¯”ï¼Œå¢åŠ äº†è¾ƒå°‘çš„å¯è®­ç»ƒå‚æ•°æ•°é‡ã€‚æœ¬ç ”ç©¶é€šè¿‡å…ˆè¿›çš„ç‡ƒçƒ§åŒºåŸŸåˆ†å‰²æ¨¡å‹è¯„ä¼°äº†è¯¥æ–¹æ³•ï¼Œåœ¨GFLOPsä¸åˆ°ä¸€åŠçš„æƒ…å†µä¸‹ï¼Œè¡¨ç°å‡ºå¹³å‡ç›¸å½“æˆ–æ›´å¥½çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¥ç»ç½‘ç»œåœ¨åˆ†æå«æ˜Ÿå›¾åƒæ•°æ®æ–¹é¢å…·æœ‰é‡è¦ä½œç”¨ï¼Œå¯åº”ç”¨äºç¾éš¾å“åº”å’Œåº”æ€¥è§„åˆ’ç­‰ä»»åŠ¡ã€‚</li>
<li>æ•°æ®ç¨€ç¼ºå’Œç¼ºä¹åŸºå‡†æ•°æ®é›†é™åˆ¶äº†å¤§å‹ç¥ç»ç½‘ç»œæ¨¡å‹åœ¨å±æœºç®¡ç†å’Œé¥æ„Ÿå›¾åƒåˆ†å‰²ä¸­çš„å‘å±•ã€‚</li>
<li>Magnifieræ˜¯ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³æœ‰é™æ•°æ®ä¸‹çš„å›¾åƒåˆ†å‰²é—®é¢˜ï¼Œé€‚ç”¨äºå„ç§ç¼–ç å™¨-è§£ç å™¨æ¶æ„ã€‚</li>
<li>Magnifieré€šè¿‡åŒç¼–ç å™¨æ–¹æ³•ï¼ˆæœ¬åœ°å’Œå…¨å±€ç¼–ç å™¨ï¼‰ä¸¤æ¬¡åˆ†æè¾“å…¥æ•°æ®ï¼Œæå–ä¸åŒç²’åº¦ä¸‹çš„ä¿¡æ¯ã€‚</li>
<li>Magnifierèƒ½æé«˜å›¾åƒåˆ†å‰²çš„æ€§èƒ½ï¼Œå¹³å‡IoUå€¼æé«˜äº†+2.65%ã€‚</li>
<li>ä¸å…¶ä»–å…ˆè¿›çš„ç‡ƒçƒ§åŒºåŸŸåˆ†å‰²æ¨¡å‹ç›¸æ¯”ï¼ŒMagnifieråœ¨è¾ƒå°‘çš„GFLOPsä¸‹è¡¨ç°å‡ºç›¸å½“æˆ–æ›´å¥½çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.19589">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a4e2520ec59986fd02a6d4eb5e81bfaf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eda442e4901fbbf2bb6f0058288c75fa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4032239a967968f54e448324221fcf27.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2e9805ba813077d5e31940362759e61f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a4bd3700f089e71f7d09108a7c9f30b3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-817c18881b92e155ce3b3e227a46fc5b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef36a1c66b66c9014d530dd25f6cf485.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Maximizing-Infrared-Transmission-Contrast-Upon-Phase-Transition-of-Thermally-Grown-Vanadium-Dioxide-Thin-Films-by-Rapid-Thermal-Processing"><a href="#Maximizing-Infrared-Transmission-Contrast-Upon-Phase-Transition-of-Thermally-Grown-Vanadium-Dioxide-Thin-Films-by-Rapid-Thermal-Processing" class="headerlink" title="Maximizing Infrared Transmission Contrast Upon Phase Transition of   Thermally Grown Vanadium Dioxide Thin Films by Rapid Thermal Processing"></a>Maximizing Infrared Transmission Contrast Upon Phase Transition of   Thermally Grown Vanadium Dioxide Thin Films by Rapid Thermal Processing</h2><p><strong>Authors:Ken Araki, Vishwa Krishna Rajan, Liping Wang</strong></p>
<p>Pristine vanadium dioxide (VO2), an insulator-to-metal transition (IMT) material, is grown via furnace oxidation followed by rapid thermal annealing with forming gas (5%H2&#x2F;95%N2) which reduces surface over-oxides such as V2O5 formed during the oxidation. The evolutional IMT behaviors of the thermochromic film and vanadium oxide states over different reduction time are systematically studied with temperature-dependent infrared spectrometry, electrical resistivity, and X-ray diffraction measurements. After optimally reducing surface over-oxides to VO2, infrared transmission contrast upon phase transition is enhanced to 46% (at 9 um wavelength) compared to 23% from fully oxidation without any reduction. Moreover, pristine VO2 thin film obtained from thermal oxidation and optimal reduction processes exhibits sharp phase transition and narrow thermal hysteresis within 2~4{\deg}C in both infrared transmission and electrical resistivity, which are comparable to the VO2 of best quality prepared by other sophisticated fabrication techniques. The thermally grown method presented here would facilitate the scalable fabrication of high-quality VO2 thin films and tunable radiative coatings for high-performance thermal control applications. </p>
<blockquote>
<p>çº¯å‡€çš„äºŒæ°§åŒ–é’’ï¼ˆVO2ï¼‰æ˜¯ä¸€ç§ç»ç¼˜ä½“åˆ°é‡‘å±è½¬å˜ï¼ˆIMTï¼‰ææ–™ï¼Œé€šè¿‡ç‚‰æ°§åŒ–å’Œå½¢æˆæ°”ä½“ï¼ˆ5%H2&#x2F;95%N2ï¼‰çš„å¿«é€Ÿçƒ­é€€ç«è¿›è¡Œç”Ÿé•¿ï¼Œä»¥å‡å°‘åœ¨æ°§åŒ–è¿‡ç¨‹ä¸­äº§ç”Ÿçš„è¡¨é¢è¿‡æ°§åŒ–ç‰©ï¼Œå¦‚äº”æ°§åŒ–äºŒé’’ï¼ˆV2O5ï¼‰ã€‚é€šè¿‡æ¸©åº¦ä¾èµ–çš„çº¢å¤–å…‰è°±æ³•ã€ç”µé˜»ç‡å’ŒXå°„çº¿è¡å°„æµ‹é‡ï¼Œç³»ç»Ÿç ”ç©¶äº†çƒ­å˜è‰²è†œå’Œæ°§åŒ–é’’åœ¨ä¸åŒè¿˜åŸæ—¶é—´ä¸‹çš„æ¼”å˜IMTè¡Œä¸ºã€‚é€šè¿‡æœ€ä¼˜æ–¹å¼è¿˜åŸè¡¨é¢è¿‡æ°§åŒ–ç‰©è‡³VO2åï¼Œä¸æœªè¿›è¡Œä»»ä½•è¿˜åŸçš„å®Œå…¨æ°§åŒ–ç›¸æ¯”ï¼Œåœ¨ç›¸å˜æ—¶çº¢å¤–é€å°„å¯¹æ¯”åº¦å¢å¼ºè‡³46%ï¼ˆåœ¨9å¾®ç±³æ³¢é•¿ä¸‹ï¼‰ï¼Œå¢å¼ºäº†è¿‘çº¢å¤–æ³¢æ®µçš„å…‰å­¦å“åº”ã€‚æ­¤å¤–ï¼Œé€šè¿‡çƒ­æ°§åŒ–å’Œæœ€ä½³è¿˜åŸè¿‡ç¨‹è·å¾—çš„çº¯å‡€VO2è–„è†œåœ¨çº¢å¤–é€å°„ç‡å’Œç”µé˜»ç‡æ–¹é¢è¡¨ç°å‡ºé”åˆ©çš„ç›¸å˜å’Œç‹­çª„çš„çƒ­æ»å›çº¿ï¼ˆåœ¨2è‡³4æ‘„æ°åº¦å†…ï¼‰ï¼Œè¿™ä¸é€šè¿‡å…¶ä»–å…ˆè¿›åˆ¶é€ æŠ€æœ¯åˆ¶å¤‡çš„VO2è´¨é‡ç›¸å½“ã€‚è¿™é‡Œå±•ç¤ºçš„çƒ­ç”Ÿé•¿æ–¹æ³•å°†æœ‰åŠ©äºå®ç°é«˜è´¨é‡VO2è–„è†œå’Œå¯è°ƒè¾å°„æ¶‚å±‚çš„è§„æ¨¡åŒ–åˆ¶é€ ï¼Œç”¨äºé«˜æ€§èƒ½çƒ­æ§åˆ¶åº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.19520v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é€šè¿‡ç‚‰æ°§åŒ–å’Œå½¢æˆæ°”ä½“ï¼ˆ5%H2&#x2F;95%N2ï¼‰å¿«é€Ÿçƒ­é€€ç«åˆ¶å¤‡é«˜çº¯åº¦äºŒæ°§åŒ–é’’ï¼ˆVO2ï¼‰çš„è¿‡ç¨‹ã€‚ç³»ç»Ÿåœ°ç ”ç©¶äº†å…¶åœ¨ç»ç¼˜ä½“-é‡‘å±è½¬å˜ï¼ˆIMTï¼‰è¡Œä¸ºä¸­ï¼Œçƒ­å˜è‰²è†œåœ¨ä¸åŒè¿˜åŸæ—¶é—´ä¸‹çš„çº¢å¤–å…‰è°±ã€ç”µé˜»ç‡å’ŒXå°„çº¿è¡å°„çš„æµ‹é‡å˜åŒ–ã€‚é€šè¿‡ä¼˜åŒ–è¡¨é¢æ°§åŒ–ç‰©ï¼Œçº¢å¤–ä¼ è¾“å¯¹æ¯”åº¦çš„ç›¸å˜å¢å¼ºè‡³46%ï¼ˆåœ¨9å¾®ç±³æ³¢é•¿ä¸‹ï¼‰ï¼Œç›¸è¾ƒäºå®Œå…¨æ°§åŒ–æœªç»ä»»ä½•è¿˜åŸå¤„ç†çš„æ ·å“å¢å¼ºäº†2å€ã€‚æ­¤å¤–ï¼Œé€šè¿‡çƒ­æ°§åŒ–å’Œæœ€ä½³è¿˜åŸè¿‡ç¨‹è·å¾—çš„VO2è–„è†œå±•ç°å‡ºé”åˆ©çš„ç›¸å˜å’Œç‹­çª„çš„çƒ­æ»å›çº¿ï¼Œä¸é€šè¿‡å…¶ä»–å…ˆè¿›åˆ¶é€ æŠ€æœ¯åˆ¶å¤‡çš„VO2è–„è†œè´¨é‡ç›¸å½“ã€‚æ‰€æå‡ºçš„çƒ­ç”Ÿé•¿æ–¹æ³•å°†æœ‰åŠ©äºå®ç°é«˜æ€§èƒ½çƒ­æ§åˆ¶åº”ç”¨ä¸­é«˜è´¨é‡VO2è–„è†œå’Œå¯è°ƒè¾å°„æ¶‚å±‚çš„è§„æ¨¡åŒ–åˆ¶é€ ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é€šè¿‡ç‚‰æ°§åŒ–å’Œå½¢æˆæ°”ä½“å¿«é€Ÿçƒ­é€€ç«åˆ¶å¤‡äº†VO2ææ–™ã€‚</li>
<li>ç³»ç»Ÿç ”ç©¶äº†VO2ææ–™åœ¨ä¸åŒè¿˜åŸæ—¶é—´çš„ç»ç¼˜ä½“-é‡‘å±è½¬å˜è¡Œä¸ºã€‚</li>
<li>ä¼˜åŒ–è¡¨é¢æ°§åŒ–ç‰©åï¼Œçº¢å¤–ä¼ è¾“å¯¹æ¯”åº¦çš„ç›¸å˜å¢å¼ºè‡³46%ã€‚</li>
<li>é«˜è´¨é‡VO2è–„è†œå±•ç°äº†é”åˆ©çš„ç›¸å˜å’Œç‹­çª„çš„çƒ­æ»å›çº¿ã€‚</li>
<li>çƒ­ç”Ÿé•¿æ–¹æ³•æœ‰åŠ©äºå®ç°é«˜è´¨é‡VO2è–„è†œçš„å¤§è§„æ¨¡åˆ¶é€ ã€‚</li>
<li>è¿™ç§æŠ€æœ¯å¯åº”ç”¨äºé«˜æ€§èƒ½çƒ­æ§åˆ¶åº”ç”¨ä¸­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.19520">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-404ab803dfe3600d227d884eebcdd393.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="MediAug-Exploring-Visual-Augmentation-in-Medical-Imaging"><a href="#MediAug-Exploring-Visual-Augmentation-in-Medical-Imaging" class="headerlink" title="MediAug: Exploring Visual Augmentation in Medical Imaging"></a>MediAug: Exploring Visual Augmentation in Medical Imaging</h2><p><strong>Authors:Xuyin Qi, Zeyu Zhang, Canxuan Gang, Hao Zhang, Lei Zhang, Zhiwei Zhang, Yang Zhao</strong></p>
<p>Data augmentation is essential in medical imaging for improving classification accuracy, lesion detection, and organ segmentation under limited data conditions. However, two significant challenges remain. First, a pronounced domain gap between natural photographs and medical images can distort critical disease features. Second, augmentation studies in medical imaging are fragmented and limited to single tasks or architectures, leaving the benefits of advanced mix-based strategies unclear. To address these challenges, we propose a unified evaluation framework with six mix-based augmentation methods integrated with both convolutional and transformer backbones on brain tumour MRI and eye disease fundus datasets. Our contributions are threefold. (1) We introduce MediAug, a comprehensive and reproducible benchmark for advanced data augmentation in medical imaging. (2) We systematically evaluate MixUp, YOCO, CropMix, CutMix, AugMix, and SnapMix with ResNet-50 and ViT-B backbones. (3) We demonstrate through extensive experiments that MixUp yields the greatest improvement on the brain tumor classification task for ResNet-50 with 79.19% accuracy and SnapMix yields the greatest improvement for ViT-B with 99.44% accuracy, and that YOCO yields the greatest improvement on the eye disease classification task for ResNet-50 with 91.60% accuracy and CutMix yields the greatest improvement for ViT-B with 97.94% accuracy. Code will be available at <a target="_blank" rel="noopener" href="https://github.com/AIGeeksGroup/MediAug">https://github.com/AIGeeksGroup/MediAug</a>. </p>
<blockquote>
<p>æ•°æ®å¢å¼ºåœ¨åŒ»å­¦æˆåƒä¸­è‡³å…³é‡è¦ï¼Œå¯¹äºåœ¨æœ‰é™æ•°æ®æ¡ä»¶ä¸‹æé«˜åˆ†ç±»ç²¾åº¦ã€ç—…å˜æ£€æµ‹ä»¥åŠå™¨å®˜åˆ†å‰²éƒ½è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œä»å­˜åœ¨ä¸¤ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚é¦–å…ˆï¼Œè‡ªç„¶ç…§ç‰‡å’ŒåŒ»å­¦å›¾åƒä¹‹é—´æ˜æ˜¾çš„é¢†åŸŸå·®è·å¯èƒ½ä¼šæ‰­æ›²å…³é”®ç–¾ç—…ç‰¹å¾ã€‚å…¶æ¬¡ï¼ŒåŒ»å­¦æˆåƒä¸­çš„å¢å¼ºç ”ç©¶æ˜¯åˆ†æ•£çš„ï¼Œä»…é™äºå•ä¸€ä»»åŠ¡æˆ–æ¶æ„ï¼Œä½¿å¾—å…ˆè¿›çš„æ··åˆç­–ç•¥çš„ä¼˜åŠ¿å°šä¸æ¸…æ¥šã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„è¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é›†æˆäº†å…­ç§åŸºäºæ··åˆçš„æ–¹æ³•ï¼Œå¹¶ç»“åˆå·ç§¯å’Œtransformeréª¨å¹²ç½‘åœ¨è„‘è‚¿ç˜¤MRIå’Œçœ¼åº•ç–¾ç—…æ•°æ®é›†ä¸Šè¿›è¡Œæ•°æ®å¢å¼ºã€‚æˆ‘ä»¬çš„è´¡çŒ®ä¸»è¦ä½“ç°åœ¨ä¸‰ä¸ªæ–¹é¢ã€‚ï¼ˆ1ï¼‰æˆ‘ä»¬å¼•å…¥äº†MediAugï¼Œè¿™æ˜¯åŒ»å­¦æˆåƒä¸­é«˜çº§æ•°æ®å¢å¼ºçš„ç»¼åˆå’Œå¯é‡ç°çš„åŸºå‡†ã€‚ ï¼ˆ2ï¼‰æˆ‘ä»¬ç³»ç»Ÿåœ°è¯„ä¼°äº†MixUpã€YOCOã€CropMixã€CutMixã€AugMixå’ŒSnapMixä½¿ç”¨ResNet-50å’ŒViT-Béª¨å¹²ç½‘çš„æ•ˆæœã€‚ï¼ˆ3ï¼‰æˆ‘ä»¬é€šè¿‡å¤§é‡å®éªŒè¯æ˜ï¼Œå¯¹äºResNet-50ï¼ŒMixUpåœ¨è„‘è‚¿ç˜¤åˆ†ç±»ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å¤§çš„æ”¹è¿›ï¼Œå‡†ç¡®ç‡ä¸º79.19%ï¼›å¯¹äºViT-Bï¼ŒSnapMixå–å¾—äº†æœ€å¤§çš„æ”¹è¿›ï¼Œå‡†ç¡®ç‡ä¸º99.44%ï¼›YOCOåœ¨ResNet-50çš„çœ¼ç—…åˆ†ç±»ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å¤§æ”¹è¿›ï¼Œå‡†ç¡®ç‡ä¸º91.60%ï¼›è€ŒCutMixåœ¨ViT-Bä¸Šå–å¾—äº†æœ€å¤§çš„æ”¹è¿›ï¼Œå‡†ç¡®ç‡ä¸º97.94%ã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/AIGeeksGroup/MediAug%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/AIGeeksGroup/MediAugä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.18983v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ•°æ®å¢å¼ºåœ¨åŒ»å­¦æˆåƒä¸­çš„é‡è¦æ€§ï¼Œé’ˆå¯¹åŒ»å­¦å›¾åƒåˆ†ç±»ã€ç—…ç¶æ£€æµ‹å’Œå™¨å®˜åˆ†å‰²ç­‰ä»»åŠ¡ï¼Œæå‡ºäº†ä¸€ä¸ªç»Ÿä¸€è¯„ä¼°æ¡†æ¶ã€‚è¯¥æ¡†æ¶é›†æˆäº†å…­ç§åŸºäºæ··åˆçš„æ•°æ®å¢å¼ºæ–¹æ³•ï¼Œå¯¹å·ç§¯å’Œtransformeræ¶æ„è¿›è¡Œäº†è¯„ä¼°ã€‚ç ”ç©¶å‘ç°ï¼ŒMixUpåœ¨è„‘è‚¿ç˜¤åˆ†ç±»ä»»åŠ¡ä¸Šè¡¨ç°æœ€ä½³ï¼ŒSnapMixåœ¨ViT-Bä¸Šè¡¨ç°æœ€ä½³ï¼ŒYOCOåœ¨çœ¼ç–¾åˆ†ç±»ä»»åŠ¡ä¸Šè¡¨ç°æœ€ä½³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ•°æ®å¢å¼ºåœ¨åŒ»å­¦æˆåƒä¸­éå¸¸é‡è¦ï¼Œå°¤å…¶å¯¹äºä»»åŠ¡å¦‚åˆ†ç±»ã€ç—…ç¶æ£€æµ‹å’Œå™¨å®˜åˆ†å‰²ã€‚</li>
<li>åŒ»å­¦å›¾åƒæ•°æ®å¢å¼ºé¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šä¸è‡ªç„¶ç…§ç‰‡ä¹‹é—´çš„åŸŸå·®è·ä»¥åŠç¼ºä¹ç»Ÿä¸€çš„è¯„ä¼°æ¡†æ¶ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„è¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é›†æˆäº†å…­ç§åŸºäºæ··åˆçš„æ•°æ®å¢å¼ºæ–¹æ³•ã€‚</li>
<li>ç ”ç©¶è¡¨æ˜MixUpåœ¨è„‘è‚¿ç˜¤åˆ†ç±»ä»»åŠ¡ä¸Šè¡¨ç°æœ€ä½³ï¼ŒSnapMixåœ¨ViT-Bæ¶æ„ä¸Šè¡¨ç°æœ€ä½³ã€‚</li>
<li>YOCOåœ¨çœ¼ç–¾åˆ†ç±»ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å¥½çš„ç»“æœã€‚</li>
<li>è¯¥ç ”ç©¶æä¾›äº†ä¸€ä¸ªå…¨é¢çš„åŒ»å­¦å›¾åƒæ•°æ®å¢å¼ºåŸºå‡†æµ‹è¯•å¹³å°MediAugã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.18983">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a1ff6ee08ab3b5add9f47db7965ec06a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7c75628b833a1a6bc4bc181cfbefbdcb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bd328dbb43cecaafe4eda12add447570.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-855ac7e35d84d38f272608bb848e4b89.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Theoretical-Framework-for-Tempered-Fractional-Gradient-Descent-Application-to-Breast-Cancer-Classification"><a href="#Theoretical-Framework-for-Tempered-Fractional-Gradient-Descent-Application-to-Breast-Cancer-Classification" class="headerlink" title="Theoretical Framework for Tempered Fractional Gradient Descent:   Application to Breast Cancer Classification"></a>Theoretical Framework for Tempered Fractional Gradient Descent:   Application to Breast Cancer Classification</h2><p><strong>Authors:Omar Naifar</strong></p>
<p>This paper introduces Tempered Fractional Gradient Descent (TFGD), a novel optimization framework that synergizes fractional calculus with exponential tempering to enhance gradient-based learning. Traditional gradient descent methods often suffer from oscillatory updates and slow convergence in high-dimensional, noisy landscapes. TFGD addresses these limitations by incorporating a tempered memory mechanism, where historical gradients are weighted by fractional coefficients $|w_j| &#x3D; \binom{\alpha}{j}$ and exponentially decayed via a tempering parameter $\lambda$. Theoretical analysis establishes TFGDâ€™s convergence guarantees: in convex settings, it achieves an $\mathcal{O}(1&#x2F;K)$ rate with alignment coefficient $d_{\alpha,\lambda} &#x3D; (1 - e^{-\lambda})^{-\alpha}$, while stochastic variants attain $\mathcal{O}(1&#x2F;k^\alpha)$ error decay. The algorithm maintains $\mathcal{O}(n)$ time complexity equivalent to SGD, with memory overhead scaling as $\mathcal{O}(d&#x2F;\lambda)$ for parameter dimension $d$. Empirical validation on the Breast Cancer Wisconsin dataset demonstrates TFGDâ€™s superiority, achieving 98.25% test accuracy (vs. 92.11% for SGD) and 2$\times$ faster convergence. The tempered memory mechanism proves particularly effective in medical classification tasks, where feature correlations benefit from stable gradient averaging. These results position TFGD as a robust alternative to conventional optimizers in both theoretical and applied machine learning. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†æ¸©åº¦åŒ–åˆ†æ•°æ¢¯åº¦ä¸‹é™æ³•ï¼ˆTempered Fractional Gradient Descentï¼Œç®€ç§°TFGDï¼‰è¿™ä¸€æ–°å‹ä¼˜åŒ–æ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†åˆ†æ•°å¾®ç§¯åˆ†ä¸æŒ‡æ•°æ¸©åº¦åŒ–ç›¸ç»“åˆï¼Œä»¥æå‡åŸºäºæ¢¯åº¦çš„å­¦ä¹ èƒ½åŠ›ã€‚ä¼ ç»Ÿçš„æ¢¯åº¦ä¸‹é™æ³•åœ¨é«˜ç»´ã€æœ‰å™ªå£°çš„åœ°å½¢ä¸Šå¸¸é­é‡æŒ¯è¡æ›´æ–°å’Œæ”¶æ•›ç¼“æ…¢çš„é—®é¢˜ã€‚TFGDé€šè¿‡å¼•å…¥æ¸©åº¦åŒ–è®°å¿†æœºåˆ¶æ¥è§£å†³è¿™äº›é—®é¢˜ï¼Œå…¶ä¸­å†å²æ¢¯åº¦é€šè¿‡åˆ†æ•°ç³»æ•°$|w_j| &#x3D; \binom{\alpha}{j}$è¿›è¡ŒåŠ æƒï¼Œå¹¶é€šè¿‡æ¸©åº¦å‚æ•°$\lambda$è¿›è¡ŒæŒ‡æ•°è¡°å‡ã€‚ç†è®ºåˆ†æè¯æ˜äº†TFGDçš„æ”¶æ•›æ€§ä¿è¯ï¼šåœ¨å‡¸è®¾ç½®ä¸­ï¼Œå®ƒå®ç°äº†ä¸å¯¹é½ç³»æ•°$d_{\alpha,\lambda} &#x3D; (1 - e^{-\lambda})^{-\alpha}$ç›¸å…³çš„$\mathcal{O}(1&#x2F;K)$é€Ÿç‡ï¼›è€Œéšæœºå˜é‡åˆ™è¾¾åˆ°$\mathcal{O}(1&#x2F;k^\alpha)$è¯¯å·®è¡°å‡ã€‚è¯¥ç®—æ³•çš„æ—¶é—´å¤æ‚åº¦ä¸SGDç›¸å½“ï¼Œä¸º$\mathcal{O}(n)$ï¼Œè€Œå†…å­˜å¼€é”€éšç€å‚æ•°ç»´åº¦$d$å’Œæ¸©åº¦å‚æ•°$\lambda$ä¹‹æ¯”çš„å¢åŠ è€Œå¢åŠ ã€‚åœ¨å¨æ–¯åº·è¾›ä¹³è…ºç™Œæ•°æ®é›†ä¸Šçš„å®è¯éªŒè¯è¡¨æ˜TFGDçš„ä¼˜è¶Šæ€§ï¼Œå…¶æµ‹è¯•å‡†ç¡®ç‡è¾¾åˆ°98.25%ï¼ˆç›¸æ¯”ä¹‹ä¸‹ï¼ŒSGDä¸º92.11%ï¼‰ï¼Œå¹¶ä¸”æ”¶æ•›é€Ÿåº¦æ˜¯SGDçš„2å€ã€‚æ¸©åº¦åŒ–è®°å¿†æœºåˆ¶åœ¨åŒ»ç–—åˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°å°¤å…¶å‡ºè‰²ï¼Œå…¶ä¸­ç‰¹å¾ç›¸å…³æ€§å—ç›Šäºç¨³å®šçš„æ¢¯åº¦å¹³å‡ã€‚è¿™äº›ç»“æœä½¿TFGDæˆä¸ºç†è®ºå’Œåº”ç”¨æœºå™¨å­¦ä¹ é¢†åŸŸä¸­å¸¸è§„ä¼˜åŒ–å™¨çš„ç¨³å¥æ›¿ä»£å“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.18849v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>TFGDæ˜¯ä¸€ç§ç»“åˆåˆ†æ•°å¾®ç§¯åˆ†å’ŒæŒ‡æ•°è¡°å‡æœºåˆ¶çš„æ–°å‹ä¼˜åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜åŸºäºæ¢¯åº¦çš„å­¦ä¹ æ•ˆç‡å¹¶è§£å†³ä¼ ç»Ÿæ¢¯åº¦ä¸‹é™æ–¹æ³•åœ¨è§£å†³é«˜ç»´å™ªå£°æ™¯è§‚æ—¶çš„å±€é™æ€§ã€‚TFGDé‡‡ç”¨ä¸€ç§åŠ æƒåˆ†æ•°ç³»æ•°çš„æ¸©åº¦è®°å¿†æœºåˆ¶ï¼Œå®ç°ç†è®ºä¸Šçš„æ”¶æ•›ä¿è¯ï¼Œå¹¶åœ¨å®è¯æµ‹è¯•ä¸­å±•ç°äº†å‡ºè‰²çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»å­¦åˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°çªå‡ºã€‚ç›¸è¾ƒäºä¼ ç»Ÿä¼˜åŒ–å™¨ï¼ŒTFGDæä¾›äº†ç¨³å¥çš„æ›¿ä»£æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TFGDæ˜¯ä¸€ç§æ–°å‹ä¼˜åŒ–æ¡†æ¶ï¼Œèåˆäº†åˆ†æ•°å¾®ç§¯åˆ†ä¸æŒ‡æ•°è¡°å‡æœºåˆ¶ï¼Œå¢å¼ºäº†æ¢¯åº¦å­¦ä¹ çš„æ€§èƒ½ã€‚</li>
<li>ä¼ ç»Ÿæ¢¯åº¦ä¸‹é™æ–¹æ³•åœ¨å¤„ç†é«˜ç»´å™ªå£°æ™¯è§‚æ—¶é¢ä¸´éœ‡è¡æ›´æ–°å’Œç¼“æ…¢æ”¶æ•›çš„é—®é¢˜ï¼Œè€ŒTFGDåˆ™é’ˆå¯¹è¿™äº›å±€é™è¿›è¡Œäº†æ”¹è¿›ã€‚</li>
<li>TFGDé‡‡ç”¨æ¸©åº¦è®°å¿†æœºåˆ¶ï¼Œé€šè¿‡åŠ æƒåˆ†æ•°ç³»æ•°å’ŒæŒ‡æ•°è¡°å‡å‚æ•°æ¥å®ç°ä¼˜åŒ–ã€‚</li>
<li>åœ¨å‡¸è®¾ç½®ä¸‹ï¼ŒTFGDå®ç°äº†ç‰¹å®šçš„æ”¶æ•›ç‡ï¼Œå¹¶ä¸”å…¶éšæœºå˜ä½“è¿˜å®ç°äº†è¯¯å·®è¡°å‡ã€‚</li>
<li>TFGDçš„æ—¶é—´å¤æ‚åº¦ä¸SGDç›¸å½“ï¼Œä½†å…¶è®°å¿†å¼€é”€éšå‚æ•°ç»´åº¦çš„å¢åŠ è€Œå¢åŠ ã€‚</li>
<li>åœ¨ä¹³è…ºç™Œå¨æ–¯åº·æ˜Ÿæ•°æ®é›†ä¸Šçš„å®è¯éªŒè¯è¡¨æ˜ï¼ŒTFGDç›¸è¾ƒäºSGDå…·æœ‰æ›´é«˜çš„æµ‹è¯•å‡†ç¡®æ€§å’Œæ›´å¿«çš„æ”¶æ•›é€Ÿåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.18849">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-da46fc991a1275a3eae3b508284f1df9.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Physics-Driven-Neural-Compensation-For-Electrical-Impedance-Tomography"><a href="#Physics-Driven-Neural-Compensation-For-Electrical-Impedance-Tomography" class="headerlink" title="Physics-Driven Neural Compensation For Electrical Impedance Tomography"></a>Physics-Driven Neural Compensation For Electrical Impedance Tomography</h2><p><strong>Authors:Chuyu Wang, Huiting Deng, Dong Liu</strong></p>
<p>Electrical Impedance Tomography (EIT) provides a non-invasive, portable imaging modality with significant potential in medical and industrial applications. Despite its advantages, EIT encounters two primary challenges: the ill-posed nature of its inverse problem and the spatially variable, location-dependent sensitivity distribution. Traditional model-based methods mitigate ill-posedness through regularization but overlook sensitivity variability, while supervised deep learning approaches require extensive training data and lack generalization. Recent developments in neural fields have introduced implicit regularization techniques for image reconstruction, but these methods typically neglect the physical principles underlying EIT, thus limiting their effectiveness. In this study, we propose PhyNC (Physics-driven Neural Compensation), an unsupervised deep learning framework that incorporates the physical principles of EIT. PhyNC addresses both the ill-posed inverse problem and the sensitivity distribution by dynamically allocating neural representational capacity to regions with lower sensitivity, ensuring accurate and balanced conductivity reconstructions. Extensive evaluations on both simulated and experimental data demonstrate that PhyNC outperforms existing methods in terms of detail preservation and artifact resistance, particularly in low-sensitivity regions. Our approach enhances the robustness of EIT reconstructions and provides a flexible framework that can be adapted to other imaging modalities with similar challenges. </p>
<blockquote>
<p>ç”µé˜»æŠ—æ–­å±‚æ‰«æï¼ˆEITï¼‰æä¾›äº†ä¸€ç§éä¾µå…¥æ€§ã€ä¾¿æºå¼çš„æˆåƒæ–¹å¼ï¼Œåœ¨åŒ»ç–—å’Œå·¥ä¸šåº”ç”¨ä¸­å…·æœ‰å·¨å¤§çš„æ½œåŠ›ã€‚å°½ç®¡EITå…·æœ‰ä¼˜åŠ¿ï¼Œä½†å®ƒé¢ä¸´ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šå…¶åé—®é¢˜çš„ç—…æ€æ€§è´¨å’Œç©ºé—´å˜åŒ–ã€ä½ç½®ä¾èµ–çš„çµæ•åº¦åˆ†å¸ƒã€‚åŸºäºä¼ ç»Ÿæ¨¡å‹çš„æ–¹æ³•é€šè¿‡æ­£åˆ™åŒ–æ¥ç¼“è§£ç—…æ€æ€§ï¼Œä½†å¿½ç•¥äº†çµæ•åº¦çš„å˜åŒ–ï¼Œè€Œç›‘ç£æ·±åº¦å­¦ä¹ çš„æ–¹æ³•éœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®ä¸”ç¼ºä¹æ³›åŒ–èƒ½åŠ›ã€‚ç¥ç»ç½‘ç»œé¢†åŸŸçš„æœ€æ–°å‘å±•å·²ç»å¼•å…¥äº†éšå¼æ­£åˆ™åŒ–æŠ€æœ¯è¿›è¡Œå›¾åƒé‡å»ºï¼Œä½†è¿™äº›æ–¹æ³•é€šå¸¸å¿½ç•¥äº†EITçš„ç‰©ç†åŸç†ï¼Œä»è€Œé™åˆ¶äº†å…¶æœ‰æ•ˆæ€§ã€‚æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†PhyNCï¼ˆç‰©ç†é©±åŠ¨ç¥ç»ç½‘ç»œè¡¥å¿ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç»“åˆäº†EITç‰©ç†åŸç†çš„æ— ç›‘ç£æ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚PhyNCé€šè¿‡åŠ¨æ€åˆ†é…ç¥ç»è¡¨å¾å®¹é‡åˆ°çµæ•åº¦è¾ƒä½çš„åŒºåŸŸï¼Œè§£å†³äº†ä¸é€‚å®šçš„åé—®é¢˜å’Œçµæ•åº¦åˆ†å¸ƒé—®é¢˜ï¼Œç¡®ä¿å‡†ç¡®ä¸”å¹³è¡¡çš„å¯¼ç”µç‡é‡å»ºã€‚å¯¹æ¨¡æ‹Ÿå’Œå®éªŒæ•°æ®çš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒPhyNCåœ¨ç»†èŠ‚ä¿ç•™å’ŒæŠ—ä¼ªå½±æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½çµæ•åº¦åŒºåŸŸã€‚æˆ‘ä»¬çš„æ–¹æ³•æé«˜äº†EITé‡å»ºçš„ç¨³å¥æ€§ï¼Œå¹¶æä¾›äº†ä¸€ä¸ªçµæ´»çš„æ¡†æ¶ï¼Œå¯ä»¥é€‚åº”å…·æœ‰ç±»ä¼¼æŒ‘æˆ˜çš„å…¶ä»–æˆåƒæ–¹å¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.18067v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç”µæ°”é˜»æŠ—æˆåƒæŠ€æœ¯ï¼ˆEITï¼‰çš„ä¼˜åŠ¿åŠé¢ä¸´çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å…¶åé—®é¢˜çš„ç—…æ€æ€§å’Œçµæ•åº¦åˆ†å¸ƒçš„ç©ºé—´å˜åŒ–æ€§ã€‚æå‡ºäº†ä¸€ç§ç»“åˆç‰©ç†åŸç†å’Œæ·±åº¦å­¦ä¹ çš„æ— ç›‘ç£å­¦ä¹ æ¡†æ¶PhyNCï¼Œé€šè¿‡åŠ¨æ€åˆ†é…ç¥ç»ç½‘ç»œè¡¨å¾å®¹é‡æ¥è§£å†³è¿™äº›é—®é¢˜ï¼Œç¡®ä¿å¯¼ç”µç‡é‡å»ºçš„å‡†ç¡®æ€§å’Œå¹³è¡¡æ€§ã€‚åœ¨æ¨¡æ‹Ÿå’Œå®éªŒæ•°æ®ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒPhyNCåœ¨ç»†èŠ‚ä¿ç•™å’ŒæŠ—ä¼ªå½±æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½çµæ•åº¦åŒºåŸŸã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>EITæ˜¯ä¸€ç§å…·æœ‰åŒ»ç–—å’Œå·¥ä¸šåº”ç”¨æ½œåŠ›çš„éä¾µå…¥æ€§ã€å¯æºå¸¦çš„æˆåƒæ¨¡å¼ã€‚</li>
<li>EITé¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜åŒ…æ‹¬å…¶åé—®é¢˜çš„ç—…æ€æ€§å’Œçµæ•åº¦åˆ†å¸ƒçš„ç©ºé—´å˜åŒ–æ€§ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•é€šè¿‡æ­£åˆ™åŒ–è§£å†³åé—®é¢˜çš„ç—…æ€æ€§ï¼Œä½†å¿½ç•¥äº†çµæ•åº¦çš„å˜åŒ–ã€‚</li>
<li>ç›‘ç£æ·±åº¦å­¦ä¹ æ–¹æ³•éœ€è¦å¤§é‡è®­ç»ƒæ•°æ®ï¼Œå¹¶ç¼ºä¹æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>æœ€è¿‘çš„ç¥ç»è¥ç•¥å¼•å…¥äº†éšå¼æ­£åˆ™åŒ–æŠ€æœ¯ç”¨äºå›¾åƒé‡å»ºï¼Œä½†å¿½ç•¥äº†EITçš„ç‰©ç†åŸç†ã€‚</li>
<li>PhyNCæ¡†æ¶ç»“åˆäº†EITçš„ç‰©ç†åŸç†å’Œæ·±åº¦å­¦ä¹ ï¼Œé€šè¿‡åŠ¨æ€åˆ†é…ç¥ç»ç½‘ç»œè¡¨å¾å®¹é‡æ¥è§£å†³åé—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.18067">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e36aa34dde35af876b295673e2800846.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4a9f5f33612028c079ba47b4c89c8efc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f0830a33e084a8efcc7360d157f62542.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="MRI-super-resolution-reconstruction-using-efficient-diffusion-probabilistic-model-with-residual-shifting"><a href="#MRI-super-resolution-reconstruction-using-efficient-diffusion-probabilistic-model-with-residual-shifting" class="headerlink" title="MRI super-resolution reconstruction using efficient diffusion   probabilistic model with residual shifting"></a>MRI super-resolution reconstruction using efficient diffusion   probabilistic model with residual shifting</h2><p><strong>Authors:Mojtaba Safari, Shansong Wang, Zach Eidex, Qiang Li, Erik H. Middlebrooks, David S. Yu, Xiaofeng Yang</strong></p>
<p>Objective:This study introduces a residual error-shifting mechanism that drastically reduces sampling steps while preserving critical anatomical details, thus accelerating MRI reconstruction. Approach:We propose a novel diffusion-based SR framework called Res-SRDiff, which integrates residual error shifting into the forward diffusion process. This enables efficient HR image reconstruction by aligning the degraded HR and LR distributions.We evaluated Res-SRDiff on ultra-high-field brain T1 MP2RAGE maps and T2-weighted prostate images, comparing it with Bicubic, Pix2pix, CycleGAN, and a conventional denoising diffusion probabilistic model with vision transformer backbone (TM-DDPM), using quantitative metrics such as peak signal-to-noise ratio (PSNR), structural similarity index (SSIM), gradient magnitude similarity deviation (GMSD), and learned perceptual image patch similarity (LPIPS). Main results: Res-SRDiff significantly outperformed all comparative methods in terms of PSNR, SSIM, and GMSD across both datasets, with statistically significant improvements (p-values&lt;&lt;0.05). The model achieved high-fidelity image restoration with only four sampling steps, drastically reducing computational time to under one second per slice, which is substantially faster than conventional TM-DDPM with around 20 seconds per slice. Qualitative analyses further demonstrated that Res-SRDiff effectively preserved fine anatomical details and lesion morphology in both brain and pelvic MRI images. Significance: Our findings show that Res-SRDiff is an efficient and accurate MRI SR method, markedly improving computational efficiency and image quality. Integrating residual error shifting into the diffusion process allows for rapid and robust HR image reconstruction, enhancing clinical MRI workflows and advancing medical imaging research. The source at:<a target="_blank" rel="noopener" href="https://github.com/mosaf/Res-SRDiff">https://github.com/mosaf/Res-SRDiff</a> </p>
<blockquote>
<p>ç›®æ ‡ï¼šæœ¬ç ”ç©¶ä»‹ç»äº†ä¸€ç§æ®‹å·®è¯¯å·®åç§»æœºåˆ¶ï¼Œè¯¥æœºåˆ¶èƒ½åœ¨ä¿æŒå…³é”®è§£å‰–ç»†èŠ‚çš„åŒæ—¶ï¼Œå¤§å¤§å‡å°‘é‡‡æ ·æ­¥éª¤ï¼Œä»è€ŒåŠ é€ŸMRIé‡å»ºã€‚æ–¹æ³•ï¼šæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£çš„è¶…åˆ†è¾¨ç‡é‡å»ºæ¡†æ¶Res-SRDiffï¼Œå®ƒå°†æ®‹å·®è¯¯å·®åç§»é›†æˆåˆ°æ­£å‘æ‰©æ•£è¿‡ç¨‹ä¸­ã€‚è¿™é€šè¿‡å¯¹é½é€€åŒ–çš„é«˜åˆ†è¾¨ç‡å’Œä½åˆ†è¾¨ç‡åˆ†å¸ƒï¼Œå®ç°äº†é«˜æ•ˆçš„é«˜åˆ†è¾¨ç‡å›¾åƒé‡å»ºã€‚æˆ‘ä»¬å¯¹è¶…é«˜åœºè„‘T1 MP2RAGEå›¾å’ŒT2åŠ æƒå‰åˆ—è…ºå›¾åƒè¿›è¡Œäº†Res-SRDiffè¯„ä¼°ï¼Œå°†å…¶ä¸Bicubicã€Pix2pixã€CycleGANä»¥åŠå¸¦æœ‰è§†è§‰è½¬æ¢å™¨ä¸»å¹²ï¼ˆTM-DDPMï¼‰çš„ä¼ ç»Ÿå»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒï¼Œä½¿ç”¨äº†å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰ã€ç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°ï¼ˆSSIMï¼‰ã€æ¢¯åº¦å¹…åº¦ç›¸ä¼¼æ€§åå·®ï¼ˆGMSDï¼‰å’Œå­¦ä¹ çš„æ„ŸçŸ¥å›¾åƒå—ç›¸ä¼¼æ€§ï¼ˆLPIPSï¼‰ç­‰å®šé‡æŒ‡æ ‡ã€‚ä¸»è¦ç»“æœï¼šåœ¨çš„ä¸¤ä¸ªæ•°æ®é›†ä¸­ï¼ŒRes-SRDiffåœ¨PSNRã€SSIMå’ŒGMSDæ–¹é¢æ˜¾è‘—ä¼˜äºæ‰€æœ‰æ¯”è¾ƒæ–¹æ³•ï¼Œå…·æœ‰ç»Ÿè®¡å­¦ä¸Šçš„æ˜¾è‘—å·®å¼‚ï¼ˆpå€¼&lt;&lt;0.05ï¼‰ã€‚è¯¥æ¨¡å‹ä»…åœ¨å››ä¸ªé‡‡æ ·æ­¥éª¤å†…å°±å®ç°äº†é«˜ä¿çœŸå›¾åƒæ¢å¤ï¼Œè®¡ç®—æ—¶é—´ç¼©çŸ­è‡³æ¯ç‰‡ä¸åˆ°ä¸€ç§’ï¼Œè¿™è¿œè¿œå¿«äºå¸¸è§„TM-DDPMçš„çº¦20ç§’æ¯ç‰‡ã€‚å®šæ€§åˆ†æè¿›ä¸€æ­¥è¡¨æ˜ï¼ŒRes-SRDiffåœ¨è„‘éƒ¨å’Œç›†è…”MRIå›¾åƒä¸­æœ‰æ•ˆåœ°ä¿ç•™äº†ç²¾ç»†çš„è§£å‰–ç»†èŠ‚å’Œç—…ç¶å½¢æ€ã€‚æ„ä¹‰ï¼šæˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒRes-SRDiffæ˜¯ä¸€ç§é«˜æ•ˆä¸”å‡†ç¡®çš„MRIè¶…åˆ†è¾¨ç‡é‡å»ºæ–¹æ³•ï¼Œæ˜¾è‘—æé«˜äº†è®¡ç®—æ•ˆç‡å’Œå›¾åƒè´¨é‡ã€‚å°†æ®‹å·®è¯¯å·®åç§»é›†æˆåˆ°æ‰©æ•£è¿‡ç¨‹ä¸­ï¼Œå¯å®ç°å¿«é€Ÿç¨³å¥çš„é«˜åˆ†è¾¨ç‡å›¾åƒé‡å»ºï¼Œå¢å¼ºä¸´åºŠMRIå·¥ä½œæµç¨‹ï¼Œæ¨åŠ¨åŒ»å­¦æˆåƒç ”ç©¶çš„å‘å±•ã€‚æ›´å¤šä¿¡æ¯è¯·è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/mosaf/Res-SRDiff%E3%80%82">https://github.com/mosaf/Res-SRDiff</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01576v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥ç ”ç©¶å¼•å…¥äº†ä¸€ç§æ®‹å·®è¯¯å·®ç§»ä½æœºåˆ¶ï¼Œè¯¥æœºåˆ¶èƒ½åœ¨å‡å°‘é‡‡æ ·æ­¥éª¤çš„åŒæ—¶ä¿ç•™å…³é”®è§£å‰–ç»†èŠ‚ï¼Œä»è€ŒåŠ é€ŸMRIé‡å»ºã€‚ç ”ç©¶å›¢é˜Ÿæå‡ºäº†ä¸€ç§æ–°å‹çš„åŸºäºæ‰©æ•£çš„è¶…åˆ†è¾¨ç‡ï¼ˆSRï¼‰æ¡†æ¶â€”â€”Res-SRDiffï¼Œå®ƒå°†æ®‹å·®è¯¯å·®ç§»ä½æ•´åˆåˆ°å‰å‘æ‰©æ•£è¿‡ç¨‹ä¸­ï¼Œä½¿å¾—é‡å»ºé«˜åˆ†è¾¨ç‡ï¼ˆHRï¼‰å›¾åƒæ—¶æ•ˆç‡æ›´é«˜ã€‚è¯¥æ¨¡å‹åœ¨ä¸¤ä¸ªä¸åŒçš„æ•°æ®é›†ä¸Šçš„è¡¨ç°å‡æ˜¾è‘—ä¼˜äºå…¶ä»–å¯¹æ¯”æ–¹æ³•ï¼Œå®ç°äº†é«˜ä¿çœŸåº¦çš„å›¾åƒæ¢å¤ï¼Œä¸”é‡‡æ ·æ­¥éª¤ä»…éœ€å››æ¬¡ï¼Œè®¡ç®—æ—¶é—´ç¼©çŸ­è‡³æ¯ç‰‡ä¸åˆ°ä¸€ç§’ï¼Œå¤§å¤§å¿«äºä¼ ç»Ÿçš„TM-DDPMæ¨¡å‹ã€‚è¯¥ç ”ç©¶æ˜¾è‘—æé«˜äº†MRIå›¾åƒçš„è®¡ç®—æ•ˆç‡å’Œå›¾åƒè´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹çš„MRIè¶…åˆ†è¾¨ç‡é‡å»ºæ–¹æ³•â€”â€”Res-SRDiffæ¡†æ¶ã€‚</li>
<li>Res-SRDiffæ•´åˆäº†æ®‹å·®è¯¯å·®ç§»ä½æœºåˆ¶åˆ°å‰å‘æ‰©æ•£è¿‡ç¨‹ä¸­ã€‚</li>
<li>Res-SRDiffåœ¨è¶…é«˜æ¸©åœºè„‘T1 MP2RAGEåœ°å›¾å’ŒT2åŠ æƒå‰åˆ—è…ºå›¾åƒä¸Šçš„è¡¨ç°å‡æ˜¾è‘—ä¼˜äºå…¶ä»–å¯¹æ¯”æ–¹æ³•ã€‚</li>
<li>Res-SRDiffå®ç°äº†é«˜ä¿çœŸåº¦çš„å›¾åƒæ¢å¤ï¼Œå¹¶ä¸”åªéœ€å››æ¬¡é‡‡æ ·æ­¥éª¤ã€‚</li>
<li>Res-SRDiffçš„è®¡ç®—æ—¶é—´å¤§å¤§ç¼©çŸ­ï¼Œæ¯ç‰‡å›¾åƒä¸åˆ°ä¸€ç§’ã€‚</li>
<li>Res-SRDiffæé«˜äº†MRIå›¾åƒçš„è®¡ç®—æ•ˆç‡å’Œå›¾åƒè´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01576">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-55d21bc97ef25ee7547ed1faaaad1ef9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eea2a0bd727d8b6acdb4bd984532250c.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="VerteNet-â€“-A-Multi-Context-Hybrid-CNN-Transformer-for-Accurate-Vertebral-Landmark-Localization-in-Lateral-Spine-DXA-Images"><a href="#VerteNet-â€“-A-Multi-Context-Hybrid-CNN-Transformer-for-Accurate-Vertebral-Landmark-Localization-in-Lateral-Spine-DXA-Images" class="headerlink" title="VerteNet â€“ A Multi-Context Hybrid CNN Transformer for Accurate   Vertebral Landmark Localization in Lateral Spine DXA Images"></a>VerteNet â€“ A Multi-Context Hybrid CNN Transformer for Accurate   Vertebral Landmark Localization in Lateral Spine DXA Images</h2><p><strong>Authors:Zaid Ilyas, Arooba Maqsood, Afsah Saleem, Erchuan Zhang, David Suter, Parminder Raina, Jonathan M. Hodgson, John T. Schousboe, William D. Leslie, Joshua R. Lewis, Syed Zulqarnain Gilani</strong></p>
<p>Lateral Spine Image (LSI) analysis is important for medical diagnosis, treatment planning, and detailed spinal health assessments. Although modalities like Computed Tomography and Digital X-ray Imaging are commonly used, Dual Energy X-ray Absorptiometry (DXA) is often preferred due to lower radiation exposure, seamless capture, and cost-effectiveness. Accurate Vertebral Landmark Localization (VLL) on LSIs is important to detect spinal conditions like kyphosis and lordosis, as well as assessing Abdominal Aortic Calcification (AAC) using Inter-Vertebral Guides (IVGs). Nonetheless, few automated VLL methodologies have concentrated on DXA LSIs. We present VerteNet, a hybrid CNN-Transformer model featuring a novel dual-resolution attention mechanism in self and cross-attention domains, referred to as Dual Resolution Self-Attention (DRSA) and Dual Resolution Cross-Attention (DRCA). These mechanisms capture the diverse frequencies in DXA images by operating at two different feature map resolutions. Additionally, we design a Multi-Context Feature Fusion Block (MCFB) that efficiently integrates the features using DRSA and DRCA. We train VerteNet on 620 DXA LSIs from various machines and achieve superior results compared to existing methods. We also design an algorithm that utilizes VerteNetâ€™s predictions in estimating the Region of Interest (ROI) to detect potential abdominal aorta cropping, where inadequate soft tissue hinders calcification assessment. Additionally, we present a small proof-of-concept study to show that IVGs generated from VLL information can improve inter-reader correlation in AAC scoring, addressing two key areas of disagreement in expert AAC-24 scoring: IVG placement and quality control for full abdominal aorta assessment. The code for this work can be found at <a target="_blank" rel="noopener" href="https://github.com/zaidilyas89/VerteNet">https://github.com/zaidilyas89/VerteNet</a>. </p>
<blockquote>
<p>ä¾§ä½è„ŠæŸ±å›¾åƒï¼ˆLSIï¼‰åˆ†æåœ¨åŒ»å­¦è¯Šæ–­ã€æ²»ç–—è®¡åˆ’å’Œè¯¦ç»†çš„è„ŠæŸ±å¥åº·è¯„ä¼°ä¸­å…·æœ‰é‡è¦æ„ä¹‰ã€‚å°½ç®¡è®¡ç®—æœºæ–­å±‚æ‰«æå’Œæ•°å­—Xå°„çº¿æˆåƒç­‰æ¨¡å¼å¸¸ç”¨ï¼Œä½†ç”±äºè¾ƒä½çš„è¾å°„æš´éœ²ã€æ— ç¼æ•è·å’Œæˆæœ¬æ•ˆç›Šï¼ŒåŒèƒ½Xå°„çº¿å¸æ”¶æ³•ï¼ˆDXAï¼‰å¾€å¾€æ›´å—æ¬¢è¿ã€‚åœ¨LSIsä¸Šè¿›è¡Œå‡†ç¡®çš„æ¤ä½“åœ°æ ‡å®šä½ï¼ˆVLLï¼‰å¯¹äºæ£€æµ‹è„ŠæŸ±ç–¾ç—…å¦‚é©¼èƒŒå’Œè…°æ¤å‰å‡¸å¾ˆé‡è¦ï¼ŒåŒæ—¶è¿˜éœ€ä½¿ç”¨æ¤é—´æŒ‡å—ï¼ˆIVGsï¼‰è¯„ä¼°è…¹éƒ¨ä¸»åŠ¨è„‰é’™åŒ–ï¼ˆAACï¼‰ã€‚ç„¶è€Œï¼Œå¾ˆå°‘æœ‰è‡ªåŠ¨åŒ–VLLæ–¹æ³•ä¸“æ³¨äºDXA LSIsã€‚æˆ‘ä»¬æå‡ºäº†VerteNetï¼Œè¿™æ˜¯ä¸€ä¸ªæ··åˆCNN-Transformeræ¨¡å‹ï¼Œå…·æœ‰ä¸€ç§æ–°å‹çš„åŒåˆ†è¾¨ç‡æ³¨æ„åŠ›æœºåˆ¶ï¼Œåœ¨è‡ªæˆ‘å’Œäº¤å‰æ³¨æ„åŠ›åŸŸä¸­è¢«ç§°ä¸ºåŒåˆ†è¾¨ç‡è‡ªæ³¨æ„åŠ›ï¼ˆDRSAï¼‰å’ŒåŒåˆ†è¾¨ç‡äº¤å‰æ³¨æ„åŠ›ï¼ˆDRCAï¼‰ã€‚è¿™äº›æœºåˆ¶é€šè¿‡åœ¨ä¸¤ç§ä¸åŒç‰¹å¾å›¾åˆ†è¾¨ç‡ä¸Šæ“ä½œæ¥æ•æ‰DXAå›¾åƒä¸­çš„ä¸åŒé¢‘ç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå¤šä¸Šä¸‹æ–‡ç‰¹å¾èåˆå—ï¼ˆMCFBï¼‰ï¼Œå®ƒæœ‰æ•ˆåœ°ç»“åˆäº†ä½¿ç”¨DRSAå’ŒDRCAçš„ç‰¹å¾ã€‚æˆ‘ä»¬å¯¹æ¥è‡ªå„ç§æœºå™¨çš„620ä¸ªDXA LSIè¿›è¡Œäº†VerteNetè®­ç»ƒï¼Œå¹¶å–å¾—äº†æ¯”ç°æœ‰æ–¹æ³•æ›´ä¼˜è¶Šçš„ç»“æœã€‚æˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸€ç§ç®—æ³•ï¼Œåˆ©ç”¨VerteNetçš„é¢„æµ‹æ¥ä¼°è®¡æ„Ÿå…´è¶£åŒºåŸŸï¼ˆROIï¼‰ï¼Œä»¥æ£€æµ‹å¯èƒ½çš„è…¹éƒ¨ä¸»åŠ¨è„‰è£å‰ªåŒºåŸŸï¼Œå…¶ä¸­è½¯ç»„ç»‡ä¸è¶³ä¼šé˜»ç¢é’™åŒ–è¯„ä¼°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¿›è¡Œäº†ä¸€é¡¹å°å‹æ¦‚å¿µéªŒè¯ç ”ç©¶ï¼Œä»¥è¯æ˜ç”±VLLä¿¡æ¯ç”Ÿæˆçš„IVGå¯ä»¥æé«˜AACè¯„åˆ†çš„è¯»è€…é—´ç›¸å…³æ€§ï¼Œè§£å†³ä¸“å®¶AAC-24è¯„åˆ†ä¸­çš„ä¸¤ä¸ªä¸»è¦åˆ†æ­§é¢†åŸŸï¼šIVGæ”¾ç½®å’Œå…¨é•¿è…¹éƒ¨ä¸»åŠ¨è„‰è¯„ä¼°çš„è´¨é‡æ§åˆ¶ã€‚è¯¥å·¥ä½œçš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/zaidilyas89/VerteNet%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/zaidilyas89/VerteNetæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.02097v2">PDF</a> 10 pages with 7 figures</p>
<p><strong>Summary</strong><br>     åŒ»å­¦å›¾åƒä¸­çš„ä¾§ä½è„ŠæŸ±å›¾åƒï¼ˆLSIï¼‰åˆ†æå¯¹äºåŒ»å­¦è¯Šæ–­ã€æ²»ç–—è®¡åˆ’å’Œè¯¦ç»†çš„è„ŠæŸ±å¥åº·è¯„ä¼°è‡³å…³é‡è¦ã€‚è™½ç„¶è®¡ç®—æœºæ–­å±‚æ‰«æå’Œæ•°å­—Xå°„çº¿æˆåƒç­‰æ¨¡æ€æ˜¯å¸¸ç”¨çš„ï¼Œä½†ç”±äºè¾ƒä½çš„è¾å°„æš´éœ²ã€æ— ç¼æ•è·å’Œæˆæœ¬æ•ˆç›Šï¼ŒåŒèƒ½Xå°„çº¿å¸æ”¶ä»ªï¼ˆDXAï¼‰å¾€å¾€æ›´å—æ¬¢è¿ã€‚å‡†ç¡®çš„æ¤éª¨åœ°æ ‡å®šä½ï¼ˆVLLï¼‰å¯¹æ£€æµ‹è„ŠæŸ±çŠ¶å†µå¦‚é©¼èƒŒå’Œéš†çªéå¸¸é‡è¦ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ··åˆCNN-Transformeræ¨¡å‹VerteNetï¼Œå…·æœ‰æ–°å‹çš„åŒåˆ†è¾¨ç‡æ³¨æ„åŠ›æœºåˆ¶ï¼Œç”¨äºDXA LSIsçš„VLLã€‚æ­¤å¤–ï¼Œè¿˜è®¾è®¡äº†ä¸€ç§ç®—æ³•æ¥ä¼°è®¡æ„Ÿå…´è¶£åŒºåŸŸï¼ˆROIï¼‰ï¼Œä»¥æ£€æµ‹å› è½¯ç»„ç»‡ä¸è¶³å¯¼è‡´çš„è…¹éƒ¨ä¸»åŠ¨è„‰è£å‰ªè¯„ä¼°é—®é¢˜ã€‚åŒæ—¶ï¼Œé€šè¿‡VLLä¿¡æ¯ç”Ÿæˆçš„IVGèƒ½æé«˜AACè¯„åˆ†çš„è¯»ç‰‡é—´ç›¸å…³æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Lateral Spine Image (LSI)åˆ†æåœ¨åŒ»å­¦è¯Šæ–­ã€æ²»ç–—è®¡åˆ’å’Œè„ŠæŸ±å¥åº·è¯„ä¼°ä¸­å…·æœ‰é‡è¦ä½œç”¨ã€‚</li>
<li>è™½ç„¶æœ‰å¤šç§æˆåƒæŠ€æœ¯å¯ç”¨ï¼Œä½†Dual Energy X-ray Absorptiometry (DXA)å› ä½è¾å°„æš´éœ²ã€æ— ç¼æ•è·å’Œæˆæœ¬æ•ˆç›Šè€Œå—åˆ°é’çã€‚</li>
<li>å‡†ç¡®çš„Vertebral Landmark Localization (VLL)å¯¹äºæ£€æµ‹è„ŠæŸ±ç–¾ç—…è‡³å…³é‡è¦ï¼Œå¦‚é©¼èƒŒå’Œéš†çªã€‚</li>
<li>æå‡ºçš„VerteNetæ¨¡å‹å…·æœ‰åŒåˆ†è¾¨ç‡æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ—¨åœ¨æ•æ‰DXAå›¾åƒä¸­çš„ä¸åŒé¢‘ç‡æˆåˆ†ã€‚</li>
<li>VerteNetåœ¨å¤šç§æœºå™¨é‡‡é›†çš„620å¼ DXA LSIå›¾åƒä¸Šè¿›è¡Œäº†è®­ç»ƒï¼Œå¹¶å–å¾—äº†ä¼˜äºç°æœ‰æ–¹æ³•çš„ç»“æœã€‚</li>
<li>å¼€å‘äº†åˆ©ç”¨VerteNeté¢„æµ‹ç»“æœä¼°è®¡æ„Ÿå…´è¶£åŒºåŸŸï¼ˆROIï¼‰çš„ç®—æ³•ï¼Œä»¥æ£€æµ‹å› è½¯ç»„ç»‡ä¸è¶³å¯¼è‡´çš„è…¹éƒ¨ä¸»åŠ¨è„‰è£å‰ªé—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.02097">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-43952dfa825ae8f94e157d006f5f0e2e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3d9ffc42bad8238f73bce7880f326341.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d4fcb77a9ed4f60a88ea38402df8a330.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3b74ef7548bcd5148746673e22fca59a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0c2f8a5a420f134deca3e8f323080b71.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-926e5a1f8c7abfecf8ec183b5bb01c3d.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Self-Consistent-Nested-Diffusion-Bridge-for-Accelerated-MRI-Reconstruction"><a href="#Self-Consistent-Nested-Diffusion-Bridge-for-Accelerated-MRI-Reconstruction" class="headerlink" title="Self-Consistent Nested Diffusion Bridge for Accelerated MRI   Reconstruction"></a>Self-Consistent Nested Diffusion Bridge for Accelerated MRI   Reconstruction</h2><p><strong>Authors:Tao Song, Yicheng Wu, Minhao Hu, Xiangde Luo, Guoting Luo, Guotai Wang, Yi Guo, Feng Xu, Shaoting Zhang</strong></p>
<p>Accelerated MRI reconstruction plays a vital role in reducing scan time while preserving image quality. While most existing methods rely on complex-valued image-space or k-space data, these formats are often inaccessible in clinical practice due to proprietary reconstruction pipelines, leaving only magnitude images stored in DICOM files. To address this gap, we focus on the underexplored task of magnitude-image-based MRI reconstruction. Recent advancements in diffusion models, particularly denoising diffusion probabilistic models (DDPMs), have demonstrated strong capabilities in modeling image priors. However, their task-agnostic denoising nature limits performance in source-to-target image translation tasks, such as MRI reconstruction. In this work, we propose a novel Self-Consistent Nested Diffusion Bridge (SC-NDB) framework that models accelerated MRI reconstruction as a bi-directional image translation process between under-sampled and fully-sampled magnitude MRI images. SC-NDB introduces a nested diffusion architecture with a self-consistency constraint and reverse bridge diffusion pathways to improve intermediate prediction fidelity and better capture the explicit priors of source images. Furthermore, we incorporate a Contour Decomposition Embedding Module (CDEM) to inject structural and textural knowledge by leveraging Laplacian pyramids and directional filter banks. Extensive experiments on the fastMRI and IXI datasets demonstrate that our method achieves state-of-the-art performance compared to both magnitude-based and non-magnitude-based diffusion models, confirming the effectiveness and clinical relevance of SC-NDB. </p>
<blockquote>
<p>åŠ é€ŸMRIé‡å»ºåœ¨å‡å°‘æ‰«ææ—¶é—´çš„åŒæ—¶ä¿æŒå›¾åƒè´¨é‡æ–¹é¢èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚è™½ç„¶å¤§å¤šæ•°ç°æœ‰æ–¹æ³•ä¾èµ–äºå¤æ•°å€¼å›¾åƒç©ºé—´æˆ–kç©ºé—´æ•°æ®ï¼Œä½†ç”±äºä¸“æœ‰é‡å»ºç®¡é“ï¼Œè¿™äº›æ ¼å¼åœ¨ä¸´åºŠå®è·µä¸­é€šå¸¸æ— æ³•è®¿é—®ï¼Œåªå‰©ä¸‹ä»¥DICOMæ–‡ä»¶æ ¼å¼å­˜å‚¨çš„å¹…åº¦å›¾åƒã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬ä¸“æ³¨äºåŸºäºå¹…åº¦å›¾åƒçš„MRIé‡å»ºè¿™ä¸€å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶çš„ä»»åŠ¡ã€‚æ‰©æ•£æ¨¡å‹çš„æœ€æ–°è¿›å±•ï¼Œç‰¹åˆ«æ˜¯å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMsï¼‰ï¼Œåœ¨å»ºæ¨¡å›¾åƒå…ˆéªŒæ–¹é¢è¡¨ç°å‡ºäº†å¼ºå¤§çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå…¶ä»»åŠ¡æ— å…³çš„å»å™ªæ€§è´¨é™åˆ¶äº†å…¶åœ¨æºåˆ°ç›®æ ‡å›¾åƒç¿»è¯‘ä»»åŠ¡ï¼ˆä¾‹å¦‚MRIé‡å»ºï¼‰ä¸­çš„æ€§èƒ½ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„è‡ªæˆ‘ä¸€è‡´åµŒå¥—æ‰©æ•£æ¡¥ï¼ˆSC-NDBï¼‰æ¡†æ¶ï¼Œå°†åŠ é€ŸMRIé‡å»ºå»ºæ¨¡ä¸ºæ¬ é‡‡æ ·å’Œå®Œå…¨é‡‡æ ·å¹…åº¦MRIå›¾åƒä¹‹é—´çš„åŒå‘å›¾åƒç¿»è¯‘è¿‡ç¨‹ã€‚SC-NDBå¼•å…¥äº†ä¸€ä¸ªåµŒå¥—æ‰©æ•£æ¶æ„ï¼Œå…·æœ‰è‡ªæˆ‘ä¸€è‡´æ€§çº¦æŸå’Œåå‘æ¡¥æ‰©æ•£è·¯å¾„ï¼Œä»¥æé«˜ä¸­é—´é¢„æµ‹ä¿çœŸåº¦å¹¶æ›´å¥½åœ°æ•è·æºå›¾åƒçš„æ˜¾å¼å…ˆéªŒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç»“åˆäº†è½®å»“åˆ†è§£åµŒå…¥æ¨¡å—ï¼ˆCDEMï¼‰ï¼Œé€šè¿‡åˆ©ç”¨æ‹‰æ™®æ‹‰æ–¯é‡‘å­—å¡”å’Œæ–¹å‘æ»¤æ³¢é“¶è¡Œæ¥æ³¨å…¥ç»“æ„å’Œçº¹ç†çŸ¥è¯†ã€‚åœ¨fastMRIå’ŒIXIæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¸åŸºäºå¹…åº¦å’ŒéåŸºäºå¹…åº¦çš„æ‰©æ•£æ¨¡å‹ç›¸æ¯”éƒ½è¡¨ç°å‡ºè‰²ï¼Œè¯å®äº†SC-NDBçš„æœ‰æ•ˆæ€§å’Œä¸´åºŠç›¸å…³æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.09998v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åŠ é€ŸMRIé‡å»ºçš„é‡è¦æ€§ï¼ŒæŒ‡å‡ºå…¶åœ¨å‡å°‘æ‰«ææ—¶é—´çš„åŒæ—¶ä¿æŒå›¾åƒè´¨é‡çš„å…³é”®ä½œç”¨ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–å¤æ‚å€¼å›¾åƒç©ºé—´æˆ–kç©ºé—´æ•°æ®ï¼Œè€Œä¸´åºŠå®è·µä¸­å¾€å¾€æ— æ³•è·å–è¿™äº›é—®é¢˜ï¼Œç ”ç©¶é›†ä¸­åœ¨åŸºäºå¹…åº¦å›¾åƒçš„MRIé‡å»ºä¸Šã€‚åˆ©ç”¨æ‰©æ•£æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMsï¼‰çš„å›¾åƒå…ˆéªŒå»ºæ¨¡èƒ½åŠ›ï¼Œæå‡ºä¸€ç§æ–°å‹çš„è‡ªæ´½åµŒå¥—æ‰©æ•£æ¡¥ï¼ˆSC-NDBï¼‰æ¡†æ¶ï¼Œå°†åŠ é€ŸMRIé‡å»ºå»ºæ¨¡ä¸ºæ¬ é‡‡æ ·å’Œå®Œå…¨é‡‡æ ·å¹…åº¦MRIå›¾åƒä¹‹é—´çš„åŒå‘å›¾åƒç¿»è¯‘è¿‡ç¨‹ã€‚SC-NDBå¼•å…¥åµŒå¥—æ‰©æ•£æ¶æ„ã€è‡ªæ´½æ€§çº¦æŸå’Œåå‘æ¡¥æ‰©æ•£è·¯å¾„ï¼Œæé«˜ä¸­é—´é¢„æµ‹ä¿çœŸåº¦ï¼Œæ›´å¥½åœ°æ•æ‰æºå›¾åƒçš„æ˜¾å¼å…ˆéªŒã€‚ç»“åˆè½®å»“åˆ†è§£åµŒå…¥æ¨¡å—ï¼ˆCDEMï¼‰ï¼Œé€šè¿‡åˆ©ç”¨æ‹‰æ™®æ‹‰æ–¯é‡‘å­—å¡”å’Œæ–¹å‘æ»¤æ³¢å™¨åº“æ³¨å…¥ç»“æ„å’Œçº¹ç†çŸ¥è¯†ã€‚åœ¨fastMRIå’ŒIXIæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¹…åº¦å’Œéå¹…åº¦æ‰©æ•£æ¨¡å‹ä¸Šå‡è¾¾åˆ°æœ€æ–°æ€§èƒ½æ°´å¹³ï¼ŒéªŒè¯äº†SC-NDBçš„æœ‰æ•ˆæ€§å’Œä¸´åºŠç›¸å…³æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŠ é€ŸMRIé‡å»ºå¯¹äºå‡å°‘æ‰«ææ—¶é—´åŒæ—¶ä¿æŒå›¾åƒè´¨é‡è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–å¤æ‚å€¼å›¾åƒç©ºé—´æˆ–kç©ºé—´æ•°æ®ï¼Œä½†åœ¨ä¸´åºŠå®è·µä¸­å¾€å¾€æ— æ³•è·å–ã€‚</li>
<li>ç ”ç©¶é›†ä¸­åœ¨åŸºäºå¹…åº¦å›¾åƒçš„MRIé‡å»ºä¸Šã€‚</li>
<li>æ‰©æ•£æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMsï¼‰ï¼Œå…·æœ‰å¼ºå¤§çš„å›¾åƒå…ˆéªŒå»ºæ¨¡èƒ½åŠ›ã€‚</li>
<li>æå‡ºè‡ªæ´½åµŒå¥—æ‰©æ•£æ¡¥ï¼ˆSC-NDBï¼‰æ¡†æ¶ï¼Œå°†åŠ é€ŸMRIé‡å»ºå»ºæ¨¡ä¸ºæ¬ é‡‡æ ·å’Œå®Œå…¨é‡‡æ ·å¹…åº¦MRIå›¾åƒä¹‹é—´çš„åŒå‘ç¿»è¯‘è¿‡ç¨‹ã€‚</li>
<li>SC-NDBç»“åˆåµŒå¥—æ‰©æ•£æ¶æ„ã€è‡ªæ´½æ€§çº¦æŸå’Œåå‘æ¡¥æ‰©æ•£è·¯å¾„ï¼Œæé«˜é¢„æµ‹ä¸­é—´ç»“æœçš„å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.09998">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bd1f7b949aaaa5f2296d25a8a66332f9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a7e7a39234dd773301f01071b928f51a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-99ce769d63417c223290e5d676d686c2.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Learning-Modality-Aware-Representations-Adaptive-Group-wise-Interaction-Network-for-Multimodal-MRI-Synthesis"><a href="#Learning-Modality-Aware-Representations-Adaptive-Group-wise-Interaction-Network-for-Multimodal-MRI-Synthesis" class="headerlink" title="Learning Modality-Aware Representations: Adaptive Group-wise Interaction   Network for Multimodal MRI Synthesis"></a>Learning Modality-Aware Representations: Adaptive Group-wise Interaction   Network for Multimodal MRI Synthesis</h2><p><strong>Authors:Tao Song, Yicheng Wu, Minhao Hu, Xiangde Luo, Linda Wei, Guotai Wang, Yi Guo, Feng Xu, Shaoting Zhang</strong></p>
<p>Multimodal MR image synthesis aims to generate missing modality images by effectively fusing and mapping from a subset of available MRI modalities. Most existing methods adopt an image-to-image translation paradigm, treating multiple modalities as input channels. However, these approaches often yield sub-optimal results due to the inherent difficulty in achieving precise feature- or semantic-level alignment across modalities. To address these challenges, we propose an Adaptive Group-wise Interaction Network (AGI-Net) that explicitly models both inter-modality and intra-modality relationships for multimodal MR image synthesis. Specifically, feature channels are first partitioned into predefined groups, after which an adaptive rolling mechanism is applied to conventional convolutional kernels to better capture feature and semantic correspondences between different modalities. In parallel, a cross-group attention module is introduced to enable effective feature fusion across groups, thereby enhancing the networkâ€™s representational capacity. We validate the proposed AGI-Net on the publicly available IXI and BraTS2023 datasets. Experimental results demonstrate that AGI-Net achieves state-of-the-art performance in multimodal MR image synthesis tasks, confirming the effectiveness of its modality-aware interaction design. We release the relevant code at: <a target="_blank" rel="noopener" href="https://github.com/zunzhumu/Adaptive-Group-wise-Interaction-Network-for-Multimodal-MRI-Synthesis.git">https://github.com/zunzhumu/Adaptive-Group-wise-Interaction-Network-for-Multimodal-MRI-Synthesis.git</a>. </p>
<blockquote>
<p>å¤šæ¨¡æ€MRå›¾åƒåˆæˆæ—¨åœ¨é€šè¿‡æœ‰æ•ˆèåˆå’Œæ˜ å°„ä»å¯ç”¨MRIæ¨¡æ€çš„å­é›†æ¥ç”Ÿæˆç¼ºå¤±çš„æ¨¡æ€å›¾åƒã€‚å¤§å¤šæ•°ç°æœ‰æ–¹æ³•é‡‡ç”¨å›¾åƒåˆ°å›¾åƒçš„ç¿»è¯‘æ¨¡å¼ï¼Œå°†å¤šä¸ªæ¨¡æ€è§†ä¸ºè¾“å…¥é€šé“ã€‚ç„¶è€Œï¼Œç”±äºåœ¨ä¸åŒæ¨¡æ€ä¹‹é—´å®ç°ç²¾ç¡®çš„ç‰¹å¾æˆ–è¯­ä¹‰çº§å¯¹é½çš„å›ºæœ‰å›°éš¾ï¼Œè¿™äº›æ–¹æ³•é€šå¸¸ä¼šäº§ç”Ÿæ¬¡ä¼˜ç»“æœã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è‡ªé€‚åº”ç»„äº¤äº’ç½‘ç»œï¼ˆAGI-Netï¼‰ï¼Œè¯¥ç½‘ç»œå¯¹å¤šæ¨¡æ€MRå›¾åƒåˆæˆè¿›è¡Œæ˜¾å¼å»ºæ¨¡ï¼Œå»ºç«‹æ¨¡æ€é—´å’Œæ¨¡æ€å†…çš„å…³ç³»ã€‚å…·ä½“æ¥è¯´ï¼Œç‰¹å¾é€šé“é¦–å…ˆè¢«åˆ’åˆ†ä¸ºé¢„å®šä¹‰çš„ç»„ï¼Œç„¶åå¯¹ä¼ ç»Ÿå·ç§¯æ ¸åº”ç”¨è‡ªé€‚åº”æ»šåŠ¨æœºåˆ¶ï¼Œä»¥æ›´å¥½åœ°æ•è·ä¸åŒæ¨¡æ€ä¹‹é—´çš„ç‰¹å¾å’Œè¯­ä¹‰å¯¹åº”å…³ç³»ã€‚åŒæ—¶ï¼Œå¼•å…¥è·¨ç»„æ³¨æ„åŠ›æ¨¡å—ï¼Œå®ç°è·¨ç»„çš„æœ‰æ•ˆç‰¹å¾èåˆï¼Œä»è€Œå¢å¼ºç½‘ç»œçš„è¡¨ç¤ºèƒ½åŠ›ã€‚æˆ‘ä»¬åœ¨å…¬å¼€å¯ç”¨çš„IXIå’ŒBraTS2023æ•°æ®é›†ä¸ŠéªŒè¯äº†æ‰€æå‡ºçš„AGI-Netã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAGI-Netåœ¨å¤šæ¨¡æ€MRå›¾åƒåˆæˆä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶æ¨¡æ€æ„ŸçŸ¥äº¤äº’è®¾è®¡çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/zunzhumu/Adaptive-Group-wise-Interaction-Network-for-Multimodal-MRI-Synthesis.git%E4%B8%8A%E5%8F%91%E5%B8%83%E4%BA%86%E7%9B%B8%E5%85%B3%E4%BB%A3%E7%A0%81%E3%80%82">https://github.com/zunzhumu/Adaptive-Group-wise-Interaction-Network-for-Multimodal-MRI-Synthesis.gitä¸Šå‘å¸ƒäº†ç›¸å…³ä»£ç ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.14684v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§è‡ªé€‚åº”ç»„äº¤äº’ç½‘ç»œï¼ˆAGI-Netï¼‰ï¼Œç”¨äºå¤šæ¨¡æ€MRå›¾åƒåˆæˆã€‚è¯¥ç½‘ç»œé€šè¿‡å»ºæ¨¡æ¨¡æ€é—´å’Œæ¨¡æ€å†…å…³ç³»ï¼Œæ”¹è¿›äº†å›¾åƒåˆ°å›¾åƒç¿»è¯‘èŒƒå¼ï¼Œå®ç°äº†æ›´ç²¾ç¡®çš„ç‰¹å¾æˆ–è¯­ä¹‰çº§å¯¹é½ã€‚é‡‡ç”¨åˆ†ç»„å·ç§¯æ ¸è‡ªé€‚åº”æ»šåŠ¨æœºåˆ¶ï¼Œå¹¶å¼•å…¥è·¨ç»„æ³¨æ„åŠ›æ¨¡å—ï¼Œæé«˜äº†ç½‘ç»œåœ¨å¤šæ¨¡æ€ç‰¹å¾èåˆæ–¹é¢çš„è¡¨ç°ã€‚åœ¨å…¬å¼€æ•°æ®é›†IXIå’ŒBraTS2023ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒAGI-Netåœ¨å¤šæ¨¡æ€MRå›¾åƒåˆæˆä»»åŠ¡ä¸Šè¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡æ—¨åœ¨é€šè¿‡æœ‰æ•ˆèåˆå’Œæ˜ å°„éƒ¨åˆ†å¯ç”¨çš„MRIæ¨¡æ€ï¼Œç”Ÿæˆç¼ºå¤±æ¨¡æ€çš„å›¾åƒã€‚</li>
<li>æå‡ºä¸€ç§è‡ªé€‚åº”ç»„äº¤äº’ç½‘ç»œï¼ˆAGI-Netï¼‰ï¼Œä»¥æ”¹è¿›å¤šæ¨¡æ€MRå›¾åƒåˆæˆçš„æ•ˆæœã€‚</li>
<li>AGI-Neté€šè¿‡å»ºæ¨¡æ¨¡æ€é—´å’Œæ¨¡æ€å†…å…³ç³»ï¼Œå®ç°æ›´ç²¾ç¡®çš„ç‰¹å¾æˆ–è¯­ä¹‰çº§å¯¹é½ã€‚</li>
<li>é‡‡ç”¨åˆ†ç»„å·ç§¯æ ¸çš„è‡ªé€‚åº”æ»šåŠ¨æœºåˆ¶ï¼Œä»¥æ›´å¥½åœ°æ•æ‰ä¸åŒæ¨¡æ€ä¹‹é—´çš„ç‰¹å¾å’Œè¯­ä¹‰å¯¹åº”å…³ç³»ã€‚</li>
<li>å¼•å…¥è·¨ç»„æ³¨æ„åŠ›æ¨¡å—ï¼Œæé«˜ç½‘ç»œåœ¨å¤šæ¨¡æ€ç‰¹å¾èåˆæ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>åœ¨IXIå’ŒBraTS2023å…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†AGI-Netçš„æœ‰æ•ˆæ€§ã€‚</li>
<li>ç›¸å…³ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/zunzhumu/Adaptive-Group-wise-Interaction-Network-for-Multimodal-MRI-Synthesis.git%E4%B8%8A%E3%80%82">https://github.com/zunzhumu/Adaptive-Group-wise-Interaction-Network-for-Multimodal-MRI-Synthesis.gitä¸Šã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.14684">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e70177188b5a89b9db9653bedee47f26.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6b7a55b836eef362a3ff606eb685c65d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be42448c24a43c93d42c180fdf363fcc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48e61804c046343d25fd5d7d0b47ddda.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Real-Time-Image-Segmentation-via-Hybrid-Convolutional-Transformer-Architecture-Search"><a href="#Real-Time-Image-Segmentation-via-Hybrid-Convolutional-Transformer-Architecture-Search" class="headerlink" title="Real-Time Image Segmentation via Hybrid Convolutional-Transformer   Architecture Search"></a>Real-Time Image Segmentation via Hybrid Convolutional-Transformer   Architecture Search</h2><p><strong>Authors:Hongyuan Yu, Cheng Wan, Xiyang Dai, Mengchen Liu, Dongdong Chen, Bin Xiao, Yan Huang, Yuan Lu, Liang Wang</strong></p>
<p>Image segmentation is one of the most fundamental problems in computer vision and has drawn a lot of attention due to its vast applications in image understanding and autonomous driving. However, designing effective and efficient segmentation neural architectures is a labor-intensive process that may require numerous trials by human experts. In this paper, we address the challenge of integrating multi-head self-attention into high-resolution representation CNNs efficiently by leveraging architecture search. Manually replacing convolution layers with multi-head self-attention is non-trivial due to the costly overhead in memory to maintain high resolution. By contrast, we develop a multi-target multi-branch supernet method, which not only fully utilizes the advantages of high-resolution features but also finds the proper location for placing the multi-head self-attention module. Our search algorithm is optimized towards multiple objectives (e.g., latency and mIoU) and is capable of finding architectures on the Pareto frontier with an arbitrary number of branches in a single search. We further present a series of models via the Hybrid Convolutional-Transformer Architecture Search (HyCTAS) method that searches for the best hybrid combination of light-weight convolution layers and memory-efficient self-attention layers between branches from different resolutions and fuses them to high resolution for both efficiency and effectiveness. Extensive experiments demonstrate that HyCTAS outperforms previous methods in both semantic segmentation and panoptic segmentation tasks. Code and models are available at <a target="_blank" rel="noopener" href="https://github.com/MarvinYu1995/HyCTAS">https://github.com/MarvinYu1995/HyCTAS</a>. </p>
<blockquote>
<p>å›¾åƒåˆ†å‰²æ˜¯è®¡ç®—æœºè§†è§‰ä¸­æœ€åŸºæœ¬çš„é—®é¢˜ä¹‹ä¸€ï¼Œç”±äºå…¶åœ¨å›¾åƒç†è§£å’Œè‡ªåŠ¨é©¾é©¶ç­‰æ–¹é¢çš„å¹¿æ³›åº”ç”¨è€Œå¤‡å—å…³æ³¨ã€‚ç„¶è€Œï¼Œè®¾è®¡å’Œå¼€å‘æœ‰æ•ˆä¸”é«˜æ•ˆçš„åˆ†å‰²ç¥ç»ç½‘ç»œæ˜¯ä¸€ä¸ªåŠ³åŠ¨å¯†é›†å‹è¿‡ç¨‹ï¼Œå¯èƒ½éœ€è¦äººç±»ä¸“å®¶è¿›è¡Œå¤šæ¬¡è¯•éªŒã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡åˆ©ç”¨æ¶æ„æœç´¢æ¥è§£å†³å°†å¤šå¤´è‡ªæ³¨æ„åŠ›é«˜æ•ˆé›†æˆåˆ°é«˜åˆ†è¾¨ç‡è¡¨ç¤ºCNNä¸­çš„æŒ‘æˆ˜ã€‚æ‰‹åŠ¨å°†å·ç§¯å±‚æ›¿æ¢ä¸ºå¤šå¤´è‡ªæ³¨æ„åŠ›æ˜¯éå¹³å‡¡çš„ï¼Œå› ä¸ºç»´æŒé«˜åˆ†è¾¨ç‡éœ€è¦æ¶ˆè€—å¤§é‡å†…å­˜èµ„æºã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§å¤šç›®æ ‡å¤šåˆ†æ”¯è¶…ç½‘æ–¹æ³•ï¼Œå®ƒä¸ä»…å……åˆ†åˆ©ç”¨äº†é«˜åˆ†è¾¨ç‡ç‰¹å¾çš„ä¼˜åŠ¿ï¼Œè¿˜æ‰¾åˆ°äº†æ”¾ç½®å¤šå¤´è‡ªæ³¨æ„åŠ›æ¨¡å—çš„æ­£ç¡®ä½ç½®ã€‚æˆ‘ä»¬çš„æœç´¢ç®—æ³•é’ˆå¯¹å¤šä¸ªç›®æ ‡ï¼ˆä¾‹å¦‚å»¶è¿Ÿæ—¶é—´å’ŒmIoUï¼‰è¿›è¡Œäº†ä¼˜åŒ–ï¼Œèƒ½å¤Ÿåœ¨å•æ¬¡æœç´¢ä¸­æ‰¾åˆ°å…·æœ‰ä»»æ„æ•°é‡çš„åˆ†æ”¯çš„å¸•ç´¯æ‰˜å‰æ²¿çš„æ¶æ„ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥é€šè¿‡æ··åˆå·ç§¯-è½¬æ¢å™¨æ¶æ„æœç´¢ï¼ˆHyCTASï¼‰æ–¹æ³•æå‡ºäº†ä¸€ç³»åˆ—æ¨¡å‹ï¼Œè¯¥æ–¹æ³•æœç´¢æœ€ä½³è½»é‡çº§å·ç§¯å±‚å’Œå†…å­˜é«˜æ•ˆè‡ªæ³¨æ„åŠ›å±‚ä¹‹é—´çš„æœ€ä½³æ··åˆç»„åˆï¼Œè¿™äº›å±‚æ¥è‡ªä¸åŒåˆ†è¾¨ç‡çš„åˆ†æ”¯ï¼Œå¹¶ä»¥é«˜åˆ†è¾¨ç‡ä¸ºæ ¸å¿ƒè¿›è¡Œèåˆï¼Œä»¥å®ç°é«˜æ•ˆå’Œæœ‰æ•ˆçš„åˆ†å‰²ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒHyCTASåœ¨è¯­ä¹‰åˆ†å‰²å’Œå…¨æ™¯åˆ†å‰²ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¼˜äºä»¥å‰çš„æ–¹æ³•ã€‚ç›¸å…³ä»£ç å’Œæ¨¡å‹å¯è®¿é—®<a target="_blank" rel="noopener" href="https://github.com/MarvinYu1995/HyCTAS%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/MarvinYu1995/HyCTASè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.10413v2">PDF</a> 29 pages, 5 figures, submitted to Knowledge-Baed Systems</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡å°†å¤šå¤´è‡ªæ³¨æ„åŠ›é«˜æ•ˆé›†æˆåˆ°é«˜åˆ†è¾¨ç‡è¡¨ç¤ºCNNä¸­ï¼Œè§£å†³å›¾åƒåˆ†å‰²é¢†åŸŸä¸­çš„æ ¸å¿ƒé—®é¢˜ã€‚é€šè¿‡åˆ©ç”¨æ¶æ„æœç´¢ï¼Œé¿å…äº†æ‰‹åŠ¨æ›¿æ¢å·ç§¯å±‚ä¸å¤šå¤´è‡ªæ³¨æ„åŠ›çš„ç¹çè¿‡ç¨‹ã€‚å¼€å‘çš„å¤šç›®æ ‡å¤šåˆ†æ”¯è¶…ç½‘æ–¹æ³•æ—¢å……åˆ†åˆ©ç”¨äº†é«˜åˆ†è¾¨ç‡ç‰¹å¾çš„ä¼˜åŠ¿ï¼Œåˆæ‰¾åˆ°äº†æ”¾ç½®å¤šå¤´è‡ªæ³¨æ„åŠ›æ¨¡å—çš„æœ€ä½³ä½ç½®ã€‚æœç´¢ç®—æ³•é’ˆå¯¹å¤šä¸ªç›®æ ‡ï¼ˆå¦‚å»¶è¿Ÿå’ŒmIoUï¼‰è¿›è¡Œä¼˜åŒ–ï¼Œèƒ½å¤Ÿåœ¨å•æ¬¡æœç´¢ä¸­æ‰¾åˆ°ä»»æ„æ•°é‡çš„åˆ†æ”¯çš„å¸•ç´¯æ‰˜å‰æ²¿æ¶æ„ã€‚é€šè¿‡Hybrid Convolutional-Transformer Architecture Searchï¼ˆHyCTASï¼‰æ–¹æ³•ï¼Œæœç´¢æœ€ä½³è½»é‡çº§å·ç§¯å±‚å’Œå†…å­˜é«˜æ•ˆè‡ªæ³¨æ„åŠ›å±‚çš„æ··åˆç»„åˆï¼Œä»ä¸åŒåˆ†è¾¨ç‡çš„åˆ†æ”¯èåˆåˆ°é«˜åˆ†è¾¨ç‡ï¼Œä»¥å®ç°æ•ˆç‡å’Œæ•ˆæœçš„å¹³è¡¡ã€‚å®éªŒè¯æ˜ï¼ŒHyCTASåœ¨è¯­ä¹‰åˆ†å‰²å’Œå…¨æ™¯åˆ†å‰²ä»»åŠ¡ä¸Šå‡ä¼˜äºä»¥å‰çš„æ–¹æ³•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å›¾åƒåˆ†å‰²æ˜¯è®¡ç®—æœºè§†è§‰ä¸­æœ€åŸºæœ¬çš„é—®é¢˜ä¹‹ä¸€ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨äºå›¾åƒç†è§£å’Œè‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸã€‚</li>
<li>æ‰‹åŠ¨å°†å·ç§¯å±‚æ›¿æ¢ä¸ºå¤šå¤´è‡ªæ³¨æ„åŠ›æ˜¯ä¸€ä¸ªç¹çä¸”æˆæœ¬é«˜æ˜‚çš„è¿‡ç¨‹ã€‚</li>
<li>æå‡ºçš„å¤šç›®æ ‡å¤šåˆ†æ”¯è¶…ç½‘æ–¹æ³•æ—¢åˆ©ç”¨é«˜åˆ†è¾¨ç‡ç‰¹å¾ï¼Œåˆæ‰¾åˆ°æ”¾ç½®å¤šå¤´è‡ªæ³¨æ„åŠ›æ¨¡å—çš„æœ€ä½³ä½ç½®ã€‚</li>
<li>æœç´¢ç®—æ³•é’ˆå¯¹å¤šä¸ªç›®æ ‡ï¼ˆå¦‚å»¶è¿Ÿå’ŒmIoUï¼‰è¿›è¡Œä¼˜åŒ–ï¼Œèƒ½åœ¨å•æ¬¡æœç´¢ä¸­æ‰¾åˆ°å¤šç§ä¼˜ç§€æ¶æ„ã€‚</li>
<li>HyCTASæ–¹æ³•èƒ½æœç´¢æœ€ä½³æ··åˆç»„åˆï¼Œç»“åˆè½»é‡çº§å·ç§¯å±‚å’Œå†…å­˜é«˜æ•ˆè‡ªæ³¨æ„åŠ›å±‚ã€‚</li>
<li>å®éªŒè¯æ˜HyCTASåœ¨è¯­ä¹‰åˆ†å‰²å’Œå…¨æ™¯åˆ†å‰²ä»»åŠ¡ä¸Šçš„æ€§èƒ½è¶…è¶Šä¹‹å‰çš„æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.10413">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-c8749e1391f0f2fab95fa62fe4242c28.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-239a841b86bb46a52e8ca7634d35384f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aaa4e3a340163d1ee88aad24aa6892e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1aa8855b28fb5506af77f52cecc7255.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-30/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-30/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-30/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-d5de42e36b2f3bcc8ffe843a1e4ac780.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-30  Generative Adversarial Network based Voice Conversion Techniques,   Challenges, and Recent Advancements
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-30
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-30/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-af5411e14789d5434717749e0cb1e8ae.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-30  DeeCLIP A Robust and Generalizable Transformer-Based Framework for   Detecting AI-Generated Images
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-30
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">29580.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
