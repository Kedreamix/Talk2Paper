<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="I2I Translation">
    <meta name="description" content="I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-04-30  ClearVision Leveraging CycleGAN and SigLIP-2 for Robust All-Weather   Classification in Traffic Camera Imagery">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>I2I Translation | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-104b102456f8c3c59d56519d6c5def1e.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">I2I Translation</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/I2I-Translation/">
                                <span class="chip bg-color">I2I Translation</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                I2I Translation
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-04-30
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    6.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    25 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-04-30-更新"><a href="#2025-04-30-更新" class="headerlink" title="2025-04-30 更新"></a>2025-04-30 更新</h1><h2 id="ClearVision-Leveraging-CycleGAN-and-SigLIP-2-for-Robust-All-Weather-Classification-in-Traffic-Camera-Imagery"><a href="#ClearVision-Leveraging-CycleGAN-and-SigLIP-2-for-Robust-All-Weather-Classification-in-Traffic-Camera-Imagery" class="headerlink" title="ClearVision: Leveraging CycleGAN and SigLIP-2 for Robust All-Weather   Classification in Traffic Camera Imagery"></a>ClearVision: Leveraging CycleGAN and SigLIP-2 for Robust All-Weather   Classification in Traffic Camera Imagery</h2><p><strong>Authors:Anush Lakshman Sivaraman, Kojo Adu-Gyamfi, Ibne Farabi Shihab, Anuj Sharma</strong></p>
<p>Accurate weather classification from low-quality traffic camera imagery remains a challenging task, particularly under adverse nighttime conditions. In this study, we propose a scalable framework that combines generative domain adaptation with efficient contrastive learning to enhance classification performance. Using CycleGAN-based domain translation, we improve the quality of nighttime images, enabling better feature extraction by downstream models. While the baseline EVA-02 model employing CLIP-based contrastive loss achieves an overall accuracy of 96.55%, it exhibits a significant performance gap between daytime (97.21%) and nighttime conditions (63.40%). Replacing CLIP with the lightweight SigLIP-2 (Sigmoid contrastive loss) achieves a competitive overall accuracy of 94.00%, with substantial improvements in nighttime performance (85.90% accuracy). The combination of Vision-SigLIP-2, Text-SigLIP-2, CycleGAN, and contrastive training achieves the best nighttime accuracy (85.90%) among all models tested, while EVA-02 with CycleGAN maintains the highest overall accuracy (97.01%) and per-class accuracies. These findings demonstrate the potential of combining domain adaptation and efficient contrastive learning to build practical, resource-efficient weather classification systems for intelligent transportation infrastructure. </p>
<blockquote>
<p>从低质量的交通摄像头图像中准确进行天气分类仍然是一个具有挑战性的任务，特别是在恶劣的夜间条件下。在这项研究中，我们提出了一个可扩展的框架，该框架结合了生成域适应和高效的对比学习，以提高分类性能。我们使用基于CycleGAN的域翻译技术，提高夜间图像的质量，使下游模型能够更好地进行特征提取。虽然使用基于CLIP的对比损失的EVA-02基线模型总体准确率为96.55%，但在日间（97.21%）和夜间条件（63.40%）之间存在显著的性能差距。用轻量级的SigLIP-2（Sigmoid对比损失）替换CLIP后，总体准确率达到了具有竞争力的94.00%，夜间性能也有了显著改善（准确率为85.90%）。结合Vision-SigLIP-2、Text-SigLIP-2、CycleGAN和对比训练的方法在所有测试模型中实现了最佳的夜间准确率（85.90%），而EVA-02与CycleGAN的组合则保持了最高的总体准确率（97.01%）和每类准确率。这些发现表明，结合域适应和高效的对比学习，有潜力为智能交通基础设施构建实用的天气分类系统，同时实现资源的高效利用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.19684v1">PDF</a> </p>
<p><strong>Summary</strong>：该研究提出一种可伸缩框架，结合生成性领域自适应和高效对比学习，以提高恶劣天气条件下从低质量交通摄像头图像进行准确分类的性能。通过CycleGAN领域的图像翻译技术提升夜间图像质量，使得下游模型能够更好地提取特征。研究发现，融合多种技术和模型后，能在夜间条件下实现最佳分类精度，同时保持总体和各类别的准确率。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>结合生成性领域自适应和高效对比学习的方法能够提高低质量交通摄像头图像的分类性能。</li>
<li>CycleGAN能够有效提升夜间图像质量，增强特征提取能力。</li>
<li>EVA-02模型采用CLIP对比损失在日间条件下表现最佳，但在夜间条件下性能显著下降。</li>
<li>SigLIP-2模型在夜间条件下的性能有显著改善，但总体准确率略低于EVA-02。</li>
<li>结合Vision-SigLIP-2、Text-SigLIP-2、CycleGAN和对比训练的模型在夜间条件下实现最佳分类精度。</li>
<li>EVA-02与CycleGAN结合保持了最高的总体准确率和各类别的准确率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.19684">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9fe428b9f4173ec02fc09babbfaebd92.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-712c93d5e909496b55cd3fdec8edd3c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b02f7f651f5590c476ecd6332935d17.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5940734a4e3f3824dc9c3259c2c18a2e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dcfd51369da4e17125b8cabb1304b7be.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="EarthMapper-Visual-Autoregressive-Models-for-Controllable-Bidirectional-Satellite-Map-Translation"><a href="#EarthMapper-Visual-Autoregressive-Models-for-Controllable-Bidirectional-Satellite-Map-Translation" class="headerlink" title="EarthMapper: Visual Autoregressive Models for Controllable Bidirectional   Satellite-Map Translation"></a>EarthMapper: Visual Autoregressive Models for Controllable Bidirectional   Satellite-Map Translation</h2><p><strong>Authors:Zhe Dong, Yuzhe Sun, Tianzhu Liu, Wangmeng Zuo, Yanfeng Gu</strong></p>
<p>Satellite imagery and maps, as two fundamental data modalities in remote sensing, offer direct observations of the Earth’s surface and human-interpretable geographic abstractions, respectively. The task of bidirectional translation between satellite images and maps (BSMT) holds significant potential for applications in urban planning and disaster response. However, this task presents two major challenges: first, the absence of precise pixel-wise alignment between the two modalities substantially complicates the translation process; second, it requires achieving both high-level abstraction of geographic features and high-quality visual synthesis, which further elevates the technical complexity. To address these limitations, we introduce EarthMapper, a novel autoregressive framework for controllable bidirectional satellite-map translation. EarthMapper employs geographic coordinate embeddings to anchor generation, ensuring region-specific adaptability, and leverages multi-scale feature alignment within a geo-conditioned joint scale autoregression (GJSA) process to unify bidirectional translation in a single training cycle. A semantic infusion (SI) mechanism is introduced to enhance feature-level consistency, while a key point adaptive guidance (KPAG) mechanism is proposed to dynamically balance diversity and precision during inference. We further contribute CNSatMap, a large-scale dataset comprising 302,132 precisely aligned satellite-map pairs across 38 Chinese cities, enabling robust benchmarking. Extensive experiments on CNSatMap and the New York dataset demonstrate EarthMapper’s superior performance, achieving significant improvements in visual realism, semantic consistency, and structural fidelity over state-of-the-art methods. Additionally, EarthMapper excels in zero-shot tasks like in-painting, out-painting and coordinate-conditional generation, underscoring its versatility. </p>
<blockquote>
<p>卫星图像和地图作为遥感中的两种基本数据模式，分别提供了对地球表面的直接观察和人类可解释的地理抽象。卫星图像与地图之间的双向翻译任务（BSMT）在城市规划和灾害应对等领域具有广泛应用潜力。然而，该任务存在两大挑战：首先，两种模式之间缺乏精确的像素级对齐，使得翻译过程变得复杂；其次，它需要在地理特征的高级抽象和高质量视觉合成方面达到平衡，进一步增加了技术复杂性。为了解决这些局限性，我们引入了EarthMapper，这是一个用于可控双向卫星地图翻译的新型自回归框架。EarthMapper通过地理坐标嵌入来锚定生成，确保区域特定的适应性，并在地理条件联合尺度自回归（GJSA）过程中利用多尺度特征对齐，在一个单一的训练周期中统一双向翻译。引入语义融合（SI）机制以增强特征级别的一致性，同时提出关键点自适应引导（KPAG）机制，以在推理过程中动态平衡多样性和精度。我们还贡献了CNSatMap数据集，该数据集包含38个中国城市的302,132个精确对齐的卫星地图对，可实现稳健的基准测试。在CNSatMap和纽约数据集上的广泛实验表明，EarthMapper在视觉真实性、语义一致性和结构保真度方面均优于最新方法。此外，EarthMapper还擅长于诸如绘画、外推和坐标条件生成等零样本任务，突显了其通用性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.19432v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>卫星图像与地图作为遥感中的两种基本数据模态，分别提供了对地球表面的直接观察和人类可解释的地理抽象。卫星图像与地图之间的双向翻译任务（BSMT）在城市规划和灾害响应等领域具有广泛的应用前景。然而，该任务面临两大挑战：一是两种模态之间缺乏精确的像素级对齐，使得翻译过程复杂化；二是需要实现地理特征的高级抽象和高质量的视觉合成，进一步增加了技术复杂性。为了解决这些限制，我们引入了EarthMapper，这是一个用于可控双向卫星-地图翻译的新型自回归框架。EarthMapper通过地理坐标嵌入来锚定生成，确保区域特异性适应性，并利用地理条件联合尺度自回归过程中的多尺度特征对齐，在一个训练周期中统一了双向翻译。引入语义融合机制来提高特征级别的一致性，并提出关键点自适应引导机制，以在推理过程中动态平衡多样性和精度。我们还贡献了CNSatMap数据集，该数据集包含38个中国城市的302,132个精确对齐的卫星地图对，以实现稳健的基准测试。在CNSatMap和纽约数据集上的实验表明，EarthMapper在视觉真实性、语义一致性和结构保真度方面均优于现有方法。此外，EarthMapper在零样本任务（如填充、扩展和坐标条件生成）中的出色表现也证明了其通用性。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>卫星图像和地图是遥感中的两种基本数据模态，分别提供地球表面的直接观察和地理抽象。</li>
<li>卫星图像与地图之间的双向翻译任务（BSMT）在城市规划和灾害响应中具有广泛应用前景。</li>
<li>BSMT面临的主要挑战是缺乏像素级对齐和需要高级地理特征抽象与高质量视觉合成。</li>
<li>EarthMapper是一个新型自回归框架，用于可控的双向卫星-地图翻译，通过地理坐标嵌入和多重机制解决上述挑战。</li>
<li>EarthMapper在视觉真实性、语义一致性和结构保真度方面优于现有方法。</li>
<li>CNSatMap数据集的贡献为稳健的基准测试提供了38个中国城市的精确对齐的卫星地图对。</li>
<li>EarthMapper在零样本任务中的出色表现证明了其通用性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.19432">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-dcf87c32d83175ea590c802c4098ba47.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-004c2f2301996f5f72b00443cbde7607.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8951841014a3451e563538b6165a8e88.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Imitation-Learning-for-Autonomous-Driving-Insights-from-Real-World-Testing"><a href="#Imitation-Learning-for-Autonomous-Driving-Insights-from-Real-World-Testing" class="headerlink" title="Imitation Learning for Autonomous Driving: Insights from Real-World   Testing"></a>Imitation Learning for Autonomous Driving: Insights from Real-World   Testing</h2><p><strong>Authors:Hidayet Ersin Dursun, Yusuf Güven, Tufan Kumbasar</strong></p>
<p>This work focuses on the design of a deep learning-based autonomous driving system deployed and tested on the real-world MIT Racecar to assess its effectiveness in driving scenarios. The Deep Neural Network (DNN) translates raw image inputs into real-time steering commands in an end-to-end learning fashion, following the imitation learning framework. The key design challenge is to ensure that DNN predictions are accurate and fast enough, at a high sampling frequency, and result in smooth vehicle operation under different operating conditions. In this study, we design and compare various DNNs, to identify the most effective approach for real-time autonomous driving. In designing the DNNs, we adopted an incremental design approach that involved enhancing the model capacity and dataset to address the challenges of real-world driving scenarios. We designed a PD system, CNN, CNN-LSTM, and CNN-NODE, and evaluated their performance on the real-world MIT Racecar. While the PD system handled basic lane following, it struggled with sharp turns and lighting variations. The CNN improved steering but lacked temporal awareness, which the CNN-LSTM addressed as it resulted in smooth driving performance. The CNN-NODE performed similarly to the CNN-LSTM in handling driving dynamics, yet with slightly better driving performance. The findings of this research highlight the importance of iterative design processes in developing robust DNNs for autonomous driving applications. The experimental video is available at <a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=FNNYgU--iaY">https://www.youtube.com/watch?v=FNNYgU--iaY</a>. </p>
<blockquote>
<p>本文重点关注基于深度学习的自动驾驶系统的设计，该系统已部署在真实世界的MIT赛车上进行测试，以评估其在驾驶场景中的有效性。深度神经网络（DNN）以端到端的学习方式将原始图像输入实时转换为转向命令，遵循模仿学习框架。主要设计挑战是确保DNN的预测在高采样频率下足够准确和快速，并在不同操作条件下实现车辆的平稳运行。在这项研究中，我们设计并比较了各种DNN，以找出最适合实时自动驾驶的方法。在设计DNN时，我们采用了增量设计方法，通过增强模型能力和扩展数据集来应对真实世界驾驶场景的挑战。我们设计了PD系统、CNN、CNN-LSTM和CNN-NODE，并在真实的MIT赛车上对它们的性能进行了评估。PD系统可以完成基本的跟道行驶，但在急转弯和光照变化的情况下会遇到困难。CNN改善了转向性能，但缺乏时间意识，而CNN-LSTM解决了这一问题，实现了平稳驾驶性能。CNN-NODE在处理驾驶动态方面表现与CNN-LSTM相似，但驾驶性能略好一些。这项研究的结果强调了迭代设计过程对于开发用于自动驾驶应用的稳健DNN的重要性。实验视频可在<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=FNNYgU-iaY%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://www.youtube.com/watch?v=FNNYgU--iaY上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.18847v1">PDF</a> In International Congress on Human-Computer Interaction, Optimization   and Robotic Applications, 2025</p>
<p><strong>Summary</strong></p>
<p>本文着重研究基于深度学习的自动驾驶系统设计，并在真实世界的MIT赛车上进行部署和测试。采用深度神经网络（DNN）将原始图像输入实时转换为转向命令，遵循模仿学习框架。研究的关键挑战在于确保DNN预测的准确性、高速性和高采样频率下的平稳车辆操作。设计了多种DNN并比较，找出最适合实时自动驾驶的方法。采用增量设计法，通过增强模型容量和数据库来解决现实驾驶场景的挑战。评估了PD系统、CNN、CNN-LSTM和CNN-NODE的性能，发现CNN-LSTM和CNN-NODE在驾驶动力学方面表现较好，其中CNN-NODE性能略优。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>本文研究基于深度学习的自动驾驶系统在真实世界的MIT赛车上的部署和测试效果。</li>
<li>采用深度神经网络（DNN）进行端到端的模仿学习，将原始图像转化为实时转向指令。</li>
<li>设计挑战在于确保DNN预测的高速性、准确性和平滑操作。</li>
<li>通过多种DNN的设计和比较，找出最适合现实自动驾驶的方法。</li>
<li>采用增量设计法增强模型容量和数据库以解决现实驾驶难题。</li>
<li>PD系统适合基本驾驶场景，但在急转弯和光照变化时表现不佳。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.18847">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9758b4b4d0aca312e26291b8ace5429c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4916f294b9d1a44781acd7d66fdb8ff2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b488528fff068fd8b79b6fd195d57f7a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aae72cd1df0dddca642fd2da80a0153c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-80f2034777da1566487cea567247a9ed.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e75f97f53868350d1a17122646c29afd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a5b037fcc0d22c4bfc1abf402a97483.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="From-Brainwaves-to-Brain-Scans-A-Robust-Neural-Network-for-EEG-to-fMRI-Synthesis"><a href="#From-Brainwaves-to-Brain-Scans-A-Robust-Neural-Network-for-EEG-to-fMRI-Synthesis" class="headerlink" title="From Brainwaves to Brain Scans: A Robust Neural Network for EEG-to-fMRI   Synthesis"></a>From Brainwaves to Brain Scans: A Robust Neural Network for EEG-to-fMRI   Synthesis</h2><p><strong>Authors:Kristofer Grover Roos, Atsushi Fukuda, Quan Huu Cap</strong></p>
<p>While functional magnetic resonance imaging (fMRI) offers valuable insights into brain activity, it is limited by high operational costs and significant infrastructural demands. In contrast, electroencephalography (EEG) provides millisecond-level precision in capturing electrical activity but lacks the spatial fidelity necessary for precise neural localization. To bridge these gaps, we propose E2fNet, a simple yet effective deep learning model for synthesizing fMRI images from low-cost EEG data. E2fNet is an encoder-decoder network specifically designed to capture and translate meaningful multi-scale features from EEG across electrode channels into accurate fMRI representations. Extensive evaluations across three public datasets demonstrate that E2fNet consistently outperforms existing CNN- and transformer-based methods, achieving state-of-the-art results in terms of the structural similarity index measure (SSIM). These results demonstrate that E2fNet is a promising, cost-effective solution for enhancing neuroimaging capabilities. The code is available at <a target="_blank" rel="noopener" href="https://github.com/kgr20/E2fNet">https://github.com/kgr20/E2fNet</a>. </p>
<blockquote>
<p>功能磁共振成像（fMRI）虽然能深入了解大脑活动，但因其运营成本较高且需要大量基础设施而受到限制。相比之下，脑电图（EEG）在捕捉电活动方面具有毫秒级的精确度，但在精确神经定位方面缺乏必要的空间保真度。为了弥补这些差距，我们提出了E2fNet，这是一个简单有效的深度学习模型，可以从低成本的EEG数据中合成fMRI图像。E2fNet是一种专门设计的编码器-解码器网络，能够捕获和翻译EEG电极通道中的有意义的多尺度特征，并将其转化为准确的fMRI表示。在三个公共数据集上的广泛评估表明，E2fNet持续优于现有的基于CNN和transformer的方法，在结构相似性指数度量（SSIM）方面达到了最新结果。这些结果表明，E2fNet是一个有前景的、性价比高的解决方案，可以提高神经成像能力。代码可在<a target="_blank" rel="noopener" href="https://github.com/kgr20/E2fNet%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/kgr20/E2fNet获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.08025v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>fMRI与EEG在神经成像领域各有所长，但存在成本和精度上的限制。为弥补这些不足，本文提出了E2fNet，一种用于从低成本EEG数据中合成fMRI图像的有效深度学习模型。E2fNet采用编码器-解码器结构，旨在捕捉EEG中的多尺度特征并将其转化为准确的fMRI表示。在三个公开数据集上的评估表明，E2fNet在结构相似性指数度量（SSIM）方面取得了最先进的成果，显著优于现有的CNN和transformer方法。这表明E2fNet是一种具有成本效益的提升神经成像能力的有前途的解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>fMRI和EEG在神经成像中具有不同的优势和局限性。</li>
<li>E2fNet是一个深度学习模型，旨在从低成本EEG数据中合成fMRI图像。</li>
<li>E2fNet采用编码器-解码器结构，能够捕捉和翻译EEG中的多尺度特征。</li>
<li>E2fNet在多个公开数据集上的表现超过了现有的CNN和transformer方法。</li>
<li>E2fNet通过提高成像能力，为神经科学研究提供了成本效益更高的解决方案。</li>
<li>E2fNet的源代码已公开发布，可供研究使用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.08025">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-104b102456f8c3c59d56519d6c5def1e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-391e901dfe08a30aa61f3f7ee7de890b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f3a9a40383e3f24082e8acdab1123b2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-14bc23dc3a925b17dd5736cba553433b.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Self-Consistent-Nested-Diffusion-Bridge-for-Accelerated-MRI-Reconstruction"><a href="#Self-Consistent-Nested-Diffusion-Bridge-for-Accelerated-MRI-Reconstruction" class="headerlink" title="Self-Consistent Nested Diffusion Bridge for Accelerated MRI   Reconstruction"></a>Self-Consistent Nested Diffusion Bridge for Accelerated MRI   Reconstruction</h2><p><strong>Authors:Tao Song, Yicheng Wu, Minhao Hu, Xiangde Luo, Guoting Luo, Guotai Wang, Yi Guo, Feng Xu, Shaoting Zhang</strong></p>
<p>Accelerated MRI reconstruction plays a vital role in reducing scan time while preserving image quality. While most existing methods rely on complex-valued image-space or k-space data, these formats are often inaccessible in clinical practice due to proprietary reconstruction pipelines, leaving only magnitude images stored in DICOM files. To address this gap, we focus on the underexplored task of magnitude-image-based MRI reconstruction. Recent advancements in diffusion models, particularly denoising diffusion probabilistic models (DDPMs), have demonstrated strong capabilities in modeling image priors. However, their task-agnostic denoising nature limits performance in source-to-target image translation tasks, such as MRI reconstruction. In this work, we propose a novel Self-Consistent Nested Diffusion Bridge (SC-NDB) framework that models accelerated MRI reconstruction as a bi-directional image translation process between under-sampled and fully-sampled magnitude MRI images. SC-NDB introduces a nested diffusion architecture with a self-consistency constraint and reverse bridge diffusion pathways to improve intermediate prediction fidelity and better capture the explicit priors of source images. Furthermore, we incorporate a Contour Decomposition Embedding Module (CDEM) to inject structural and textural knowledge by leveraging Laplacian pyramids and directional filter banks. Extensive experiments on the fastMRI and IXI datasets demonstrate that our method achieves state-of-the-art performance compared to both magnitude-based and non-magnitude-based diffusion models, confirming the effectiveness and clinical relevance of SC-NDB. </p>
<blockquote>
<p>加速MRI重建在减少扫描时间的同时保持图像质量方面起着至关重要的作用。虽然大多数现有方法依赖于复数值图像空间或k空间数据，但这些格式由于专有重建管道而在临床实践中往往无法访问，只留下以DICOM文件存储的幅度图像。为了弥补这一空白，我们专注于基于幅度图像的MRI重建这一尚未充分探索的任务。扩散模型的最新进展，特别是去噪扩散概率模型（DDPMs），在建模图像先验方面表现出了强大的能力。然而，其通用的去噪性质限制了其在源到目标图像翻译任务（如MRI重建）中的性能。在这项工作中，我们提出了一种新型的自我一致嵌套扩散桥（SC-NDB）框架，它将加速MRI重建建模为欠采样MRI图像和完全采样幅度MRI图像之间的双向图像翻译过程。SC-NDB引入了一个具有自我一致性约束和反向桥扩散路径的嵌套扩散架构，以提高中间预测保真度并更好地捕获源图像的显式先验。此外，我们结合轮廓分解嵌入模块（CDEM），通过利用拉普拉斯金字塔和方向滤波器库来注入结构和纹理知识。在fastMRI和IXI数据集上的广泛实验表明，我们的方法与基于幅度和非基于幅度的扩散模型相比，达到了最先进的性能，证实了SC-NDB的有效性和临床相关性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.09998v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种新型的MRI重建方法——Self-Consistent Nested Diffusion Bridge（SC-NDB）框架，它解决了加速MRI重建中扫描时间缩短与图像质量保持的矛盾。针对现有方法依赖于复杂的图像空间或k空间数据的问题，本文专注于基于幅度图像的MRI重建。结合扩散模型的优势，特别是denoising diffusion probabilistic models（DDPMs），本文框架通过双向图像翻译过程对欠采样和完全采样的幅度MRI图像进行建模。SC-NDB引入嵌套扩散架构、自我一致性约束和反向桥梁扩散路径，以提高中间预测保真度和更好地捕捉源图像的显式先验信息。此外，还结合了Contour Decomposition Embedding Module（CDEM），通过利用Laplacian金字塔和方向滤波器库来注入结构和纹理知识。在fastMRI和IXI数据集上的实验表明，该方法在扩散模型中实现了最先进的性能，验证了SC-NDB的有效性和临床相关性。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>加速MRI重建在减少扫描时间的同时保持图像质量至关重要。</li>
<li>当前方法主要依赖复杂的图像空间或k空间数据，这在临床实践中由于专有重建管道而难以应用。</li>
<li>幅度图像在MRI重建中是一个未被充分研究的领域。</li>
<li>SC-NDB框架利用扩散模型将加速MRI重建建模为欠采样和完全采样幅度MRI图像之间的双向图像翻译过程。</li>
<li>SC-NDB通过嵌套扩散架构和自我一致性约束提高预测准确性并捕捉源图像的显式先验信息。</li>
<li>CDEM模块利用Laplacian金字塔和方向滤波器库注入结构和纹理知识。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.09998">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-bd1f7b949aaaa5f2296d25a8a66332f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a7e7a39234dd773301f01071b928f51a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-99ce769d63417c223290e5d676d686c2.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Learning-Modality-Aware-Representations-Adaptive-Group-wise-Interaction-Network-for-Multimodal-MRI-Synthesis"><a href="#Learning-Modality-Aware-Representations-Adaptive-Group-wise-Interaction-Network-for-Multimodal-MRI-Synthesis" class="headerlink" title="Learning Modality-Aware Representations: Adaptive Group-wise Interaction   Network for Multimodal MRI Synthesis"></a>Learning Modality-Aware Representations: Adaptive Group-wise Interaction   Network for Multimodal MRI Synthesis</h2><p><strong>Authors:Tao Song, Yicheng Wu, Minhao Hu, Xiangde Luo, Linda Wei, Guotai Wang, Yi Guo, Feng Xu, Shaoting Zhang</strong></p>
<p>Multimodal MR image synthesis aims to generate missing modality images by effectively fusing and mapping from a subset of available MRI modalities. Most existing methods adopt an image-to-image translation paradigm, treating multiple modalities as input channels. However, these approaches often yield sub-optimal results due to the inherent difficulty in achieving precise feature- or semantic-level alignment across modalities. To address these challenges, we propose an Adaptive Group-wise Interaction Network (AGI-Net) that explicitly models both inter-modality and intra-modality relationships for multimodal MR image synthesis. Specifically, feature channels are first partitioned into predefined groups, after which an adaptive rolling mechanism is applied to conventional convolutional kernels to better capture feature and semantic correspondences between different modalities. In parallel, a cross-group attention module is introduced to enable effective feature fusion across groups, thereby enhancing the network’s representational capacity. We validate the proposed AGI-Net on the publicly available IXI and BraTS2023 datasets. Experimental results demonstrate that AGI-Net achieves state-of-the-art performance in multimodal MR image synthesis tasks, confirming the effectiveness of its modality-aware interaction design. We release the relevant code at: <a target="_blank" rel="noopener" href="https://github.com/zunzhumu/Adaptive-Group-wise-Interaction-Network-for-Multimodal-MRI-Synthesis.git">https://github.com/zunzhumu/Adaptive-Group-wise-Interaction-Network-for-Multimodal-MRI-Synthesis.git</a>. </p>
<blockquote>
<p>多模态MR图像合成旨在通过有效地融合和映射从可用MRI模态的子集来生成缺失的模态图像。大多数现有方法采用图像到图像的翻译模式，将多种模态视为输入通道。然而，这些方法通常会产生次优结果，因为实现跨模态的特征或语义级对齐存在固有的困难。为了应对这些挑战，我们提出了一种自适应组交互网络（AGI-Net），该网络对多模态MR图像合成进行显式建模，同时建立模态间和模态内的关系。具体而言，特征通道首先被划分成预定义的组，然后对传统的卷积核应用自适应滚动机制，以更好地捕获不同模态之间的特征和语义对应关系。同时，引入跨组注意力模块，以实现各组之间的有效特征融合，从而增强网络的表示能力。我们在公开可用的IXI和BraTS2023数据集上验证了所提出的AGI-Net。实验结果表明，AGI-Net在多模态MR图像合成任务上达到了最先进的性能，证明了其模态感知交互设计的有效性。我们已在<a target="_blank" rel="noopener" href="https://github.com/zunzhumu/Adaptive-Group-wise-Interaction-Network-for-Multimodal-MRI-Synthesis.git%E5%8F%91%E5%B8%83%E7%9B%B8%E5%85%B3%E4%BB%A3%E7%A0%81%E3%80%82">https://github.com/zunzhumu/Adaptive-Group-wise-Interaction-Network-for-Multimodal-MRI-Synthesis.git发布相关代码。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.14684v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该文提出了一种自适应组交互网络（AGI-Net），用于多模态MR图像合成。通过建模模态间和模态内的关系，实现了在不同模态间特征语义层面的精确对齐。采用分组机制和跨组注意力模块，提高了网络在合成任务上的性能。在公开数据集上的实验验证了该方法的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态MR图像合成的目标是利用部分可用的MRI模态生成缺失模态的图像。</li>
<li>现有方法多采用图像到图像的翻译模式，将多个模态作为输入通道来处理。</li>
<li>现有方法面临的挑战是难以实现模态间的精确特征或语义对齐。</li>
<li>自适应组交互网络（AGI-Net）通过建模模态间和模态内的关系来解决这一问题。</li>
<li>AGI-Net采用分组机制和自适应滚动机制来更好地捕捉不同模态间的特征语义对应关系。</li>
<li>AGI-Net引入跨组注意力模块，实现组间的有效特征融合，提高了网络的表征能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.14684">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e70177188b5a89b9db9653bedee47f26.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6b7a55b836eef362a3ff606eb685c65d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-be42448c24a43c93d42c180fdf363fcc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48e61804c046343d25fd5d7d0b47ddda.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-30/I2I%20Translation/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-30/I2I%20Translation/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/I2I-Translation/">
                                    <span class="chip bg-color">I2I Translation</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-30/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-b6b524ea8e8b99e4bc41a2e554d8b478.jpg" class="responsive-img" alt="视频理解">
                        
                        <span class="card-title">视频理解</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            视频理解 方向最新论文已更新，请持续关注 Update in 2025-04-30  MASR Self-Reflective Reasoning through Multimodal Hierarchical   Attention Focusing for Agent-based Video Understanding
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-30
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    视频理解
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">视频理解</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-30/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-68629da72c57f0f6036e4e1cc6636951.jpg" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-04-30  Masked Language Prompting for Generative Data Augmentation in Few-shot   Fashion Style Recognition
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-30
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">29474.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
