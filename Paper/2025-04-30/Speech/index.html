<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech 方向最新论文已更新，请持续关注 Update in 2025-04-30  DEEMO De-identity Multimodal Emotion Recognition and Reasoning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-493698fca6a32a2cd575ee3c345040e1.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-04-30
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-12
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    6.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    24 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-04-30-更新"><a href="#2025-04-30-更新" class="headerlink" title="2025-04-30 更新"></a>2025-04-30 更新</h1><h2 id="DEEMO-De-identity-Multimodal-Emotion-Recognition-and-Reasoning"><a href="#DEEMO-De-identity-Multimodal-Emotion-Recognition-and-Reasoning" class="headerlink" title="DEEMO: De-identity Multimodal Emotion Recognition and Reasoning"></a>DEEMO: De-identity Multimodal Emotion Recognition and Reasoning</h2><p><strong>Authors:Deng Li, Bohao Xing, Xin Liu, Baiqiang Xia, Bihan Wen, Heikki Kälviäinen</strong></p>
<p>Emotion understanding is a critical yet challenging task. Most existing approaches rely heavily on identity-sensitive information, such as facial expressions and speech, which raises concerns about personal privacy. To address this, we introduce the De-identity Multimodal Emotion Recognition and Reasoning (DEEMO), a novel task designed to enable emotion understanding using de-identified video and audio inputs. The DEEMO dataset consists of two subsets: DEEMO-NFBL, which includes rich annotations of Non-Facial Body Language (NFBL), and DEEMO-MER, an instruction dataset for Multimodal Emotion Recognition and Reasoning using identity-free cues. This design supports emotion understanding without compromising identity privacy. In addition, we propose DEEMO-LLaMA, a Multimodal Large Language Model (MLLM) that integrates de-identified audio, video, and textual information to enhance both emotion recognition and reasoning. Extensive experiments show that DEEMO-LLaMA achieves state-of-the-art performance on both tasks, outperforming existing MLLMs by a significant margin, achieving 74.49% accuracy and 74.45% F1-score in de-identity emotion recognition, and 6.20 clue overlap and 7.66 label overlap in de-identity emotion reasoning. Our work contributes to ethical AI by advancing privacy-preserving emotion understanding and promoting responsible affective computing. </p>
<blockquote>
<p>情感理解是一项至关重要且具有挑战性的任务。大多数现有方法都严重依赖于身份敏感信息，如面部表情和语音，这引发了关于个人隐私的担忧。为了解决这一问题，我们引入了去身份化多模态情感识别与推理（DEEMO），这是一项旨在利用去身份化的视频和音频输入实现情感理解的新任务。DEEMO数据集包含两个子集：DEEMO-NFBL，包含丰富的非面部肢体语言（NFBL）注释；以及DEEMO-MER，这是一个使用无身份线索进行多模态情感识别和推理的指令数据集。这种设计能够在不泄露身份隐私的情况下支持情感理解。此外，我们提出了DEEMO-LLaMA，这是一种多模态大型语言模型（MLLM），它整合了去身份化的音频、视频和文本信息，以提高情感识别和推理能力。大量实验表明，DEEMO-LLaMA在这两项任务上都达到了最新技术水平，显著优于现有的MLLMs，在去身份情感识别方面达到74.49%的准确率和74.45%的F1分数，在去身份情感推理方面达到6.20的线索重叠率和7.66的标签重叠率。我们的工作通过推进保护隐私的情感理解和促进负责任的情感计算，为伦理人工智能做出贡献。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.19549v1">PDF</a> </p>
<p><strong>Summary</strong><br>     该文本介绍了情绪理解的重要性及其面临的挑战。现有方法过于依赖身份敏感信息，引发隐私担忧。为此，提出一种去身份多模态情绪识别和推理（DEEMO）的新任务，旨在使用去身份的视频和音频输入实现情绪理解。数据集包括两个子集，一个关注非面部肢体语言（NFBL）的丰富注释，另一个用于多模态情绪识别和推理的身份无关线索指令数据集。此外，还提出了一种多模态大型语言模型（MLLM），可整合去身份音频、视频和文本信息，提高情绪识别和推理能力。实验表明，DEEMO-LLaMA在两项任务上均达到最新技术水平，显著优于现有MLLMs，在去身份情绪识别方面达到74.49%的准确率和74.45%的F1得分，在去身份情绪推理方面达到6.20的线索重叠率和7.66的标签重叠率。本研究为伦理人工智能做出贡献，推动了隐私保护的情绪理解和负责任的情感计算。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>情绪理解是一项重要而具有挑战性的任务。</li>
<li>现有方法过于依赖身份敏感信息，引发隐私担忧。</li>
<li>提出了一种新型任务——去身份多模态情绪识别和推理（DEEMO）。</li>
<li>DEEMO数据集包括两个子集：关注非面部肢体语言的DEEM-NFBL和用于多模态情绪识别和推理的身份无关指令数据集DEEM-MER。</li>
<li>提出了多模态大型语言模型（MLLM）DEEMO-LLaMA，融合了去身份音频、视频和文本信息。</li>
<li>实验结果显示，DEEMO-LLaMA在去身份情绪识别和推理方面表现优异，准确率分别为74.49%和F1得分为74.45%。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.19549">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ef82a44ca1f97add7ab4c3da2a47a368.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c1fc19665aeaa5cbae8038bf9ec45c2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4b62ba7a578626ac9dcefddbe070d1ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-71bbd8b45ff99960170648dd6515acf9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8e179c322c6e7c71764844bf3cd7da20.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f434c9b3bb26f69965e33fb10b185f33.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Context-Selection-and-Rewriting-for-Video-based-EducationalQuestion-Generation"><a href="#Context-Selection-and-Rewriting-for-Video-based-EducationalQuestion-Generation" class="headerlink" title="Context Selection and Rewriting for Video-based EducationalQuestion   Generation"></a>Context Selection and Rewriting for Video-based EducationalQuestion   Generation</h2><p><strong>Authors:Mengxia Yu, Bang Nguyen, Olivia Zino, Meng Jiang</strong></p>
<p>Educational question generation (EQG) is a crucial component of intelligent educational systems, significantly aiding self-assessment, active learning, and personalized education. While EQG systems have emerged, existing datasets typically rely on predefined, carefully edited texts, failing to represent real-world classroom content, including lecture speech with a set of complementary slides. To bridge this gap, we collect a dataset of educational questions based on lectures from real-world classrooms. On this realistic dataset, we find that current methods for EQG struggle with accurately generating questions from educational videos, particularly in aligning with specific timestamps and target answers. Common challenges include selecting informative contexts from extensive transcripts and ensuring generated questions meaningfully incorporate the target answer. To address the challenges, we introduce a novel framework utilizing large language models for dynamically selecting and rewriting contexts based on target timestamps and answers. First, our framework selects contexts from both lecture transcripts and video keyframes based on answer relevance and temporal proximity. Then, we integrate the contexts selected from both modalities and rewrite them into answer-containing knowledge statements, to enhance the logical connection between the contexts and the desired answer. This approach significantly improves the quality and relevance of the generated questions. Our dataset and code are released in <a target="_blank" rel="noopener" href="https://github.com/mengxiayu/COSER">https://github.com/mengxiayu/COSER</a>. </p>
<blockquote>
<p>教育问题生成（EQG）是智能教育系统的关键组成部分，极大地有助于自我评估、主动学习和个性化教育。虽然已经出现了一些EQG系统，但现有的数据集通常依赖于预先定义、精心编辑的文本，无法代表真实课堂内容，包括配有辅助幻灯片的讲座演讲。为了弥补这一差距，我们收集了一个基于真实课堂讲座的教育问题数据集。在这个现实的数据集上，我们发现现有的EQG方法在生成教育视频的问题时面临困难，特别是在与特定时间戳和目标答案对齐方面。常见的挑战包括从大量脚本中选择有意义上下文和确保生成的问题有意义地融入目标答案。为了应对这些挑战，我们引入了一个利用大型语言模型的新框架，该框架可根据目标时间戳和答案动态选择和重写上下文。首先，我们的框架根据答案相关性和时间接近性从讲座脚本和视频关键帧中选择上下文。然后，我们整合从这两种模式中选择出的上下文，并将其改写为包含答案的知识陈述，以增强上下文与所需答案之间的逻辑联系。这种方法显著提高了生成问题的质量和相关性。我们的数据集和代码已在<a target="_blank" rel="noopener" href="https://github.com/mengxiayu/COSER%E4%B8%AD%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/mengxiayu/COSER中发布。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.19406v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>教育问题生成（EQG）是智能教育系统的重要组成部分，有助于自我评估、主动学习和个性化教育。针对现有EQG数据集主要依赖预设编辑文本，未能真实反映课堂内容的问题，我们收集了一个基于真实课堂讲座的教育问题数据集。研究发现，现有方法难以准确从教育视频中生成问题，特别是在与特定时间戳和目标答案对齐方面存在挑战。为此，我们提出了一种利用大型语言模型的新框架，该框架可根据目标时间戳和答案动态选择和改写上下文。通过选择与讲座视频内容相关的语境，并进行重写以强化上下文与目标答案之间的逻辑联系，进而提高生成问题的质量和相关性。我们的数据集和代码已发布在：<a target="_blank" rel="noopener" href="https://github.com/mengxiayu/COSER%E3%80%82">https://github.com/mengxiayu/COSER。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>教育问题生成（EQG）是智能教育系统的重要部分，有助于自我评估、主动学习和个性化教育。</li>
<li>目前的数据集主要依赖预设编辑文本，未能真实反映课堂环境，特别是讲座内容和相关幻灯片。</li>
<li>现有方法在生成与特定时间戳和目标答案对齐的问题时存在困难。</li>
<li>提出的框架利用大型语言模型动态选择和改写上下文，基于目标时间戳和答案。</li>
<li>框架从讲座视频内容和视觉关键帧中选择相关语境。</li>
<li>通过整合来自不同模态的上下文并将其改写为包含答案的知识陈述，增强了上下文与目标答案的逻辑联系。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.19406">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a2a5bbe126399f7889515690a3515ea3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-091c258e7492f9c97b2f701b41b9ae55.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8310396b0e396a5c602691c735826f8e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5fa2c9f78a654d9b27f82c3e47a4c7cc.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Muyan-TTS-A-Trainable-Text-to-Speech-Model-Optimized-for-Podcast-Scenarios-with-a-50K-Budget"><a href="#Muyan-TTS-A-Trainable-Text-to-Speech-Model-Optimized-for-Podcast-Scenarios-with-a-50K-Budget" class="headerlink" title="Muyan-TTS: A Trainable Text-to-Speech Model Optimized for Podcast   Scenarios with a $50K Budget"></a>Muyan-TTS: A Trainable Text-to-Speech Model Optimized for Podcast   Scenarios with a $50K Budget</h2><p><strong>Authors:Xin Li, Kaikai Jia, Hao Sun, Jun Dai, Ziyang Jiang</strong></p>
<p>Recent advancements in text-to-speech (TTS) models have been driven by the integration of large language models (LLMs), enhancing semantic comprehension and improving speech naturalness. However, existing LLM-based TTS models often lack open-source training code and efficient inference acceleration frameworks, limiting their accessibility and adaptability. Additionally, there is no publicly available TTS model specifically optimized for podcast scenarios, which are in high demand for voice interaction applications. To address these limitations, we introduce Muyan-TTS, an open-source trainable TTS model designed for podcast applications within a $50,000 budget. Our model is pre-trained on over 100,000 hours of podcast audio data, enabling zero-shot TTS synthesis with high-quality voice generation. Furthermore, Muyan-TTS supports speaker adaptation with dozens of minutes of target speech, making it highly customizable for individual voices. In addition to open-sourcing the model, we provide a comprehensive data collection and processing pipeline, a full training procedure, and an optimized inference framework that accelerates LLM-based TTS synthesis. Our code and models are available at <a target="_blank" rel="noopener" href="https://github.com/MYZY-AI/Muyan-TTS">https://github.com/MYZY-AI/Muyan-TTS</a>. </p>
<blockquote>
<p>近期文本转语音（TTS）模型的进步得益于大型语言模型（LLM）的集成，提高了语义理解和语音自然度。然而，现有的基于LLM的TTS模型通常缺乏开源训练代码和高效的推理加速框架，限制了其可访问性和适应性。此外，目前没有一个针对播客场景的公开可用的TTS模型，而语音交互应用程序对此类模型的需求很高。为了解决这些限制，我们推出了Muyan-TTS，这是一个开源的可训练TTS模型，专为预算为5万美元的播客应用程序设计。我们的模型在超过10万小时的播客音频数据上进行预训练，能够实现零样本TTS合成和高质量的语音生成。此外，Muyan-TTS支持使用几十分钟的目标语音进行声纹适配，使其能够轻松定制个人声音。除了开源模型外，我们还提供了全面的数据收集和处理流程、完整的训练过程以及优化的推理框架，以加速基于LLM的TTS合成。我们的代码和模型可在<a target="_blank" rel="noopener" href="https://github.com/MYZY-AI/Muyan-TTS%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/MYZY-AI/Muyan-TTS获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.19146v1">PDF</a> </p>
<p><strong>Summary</strong>：<br>近期文本转语音（TTS）模型的进步得益于大型语言模型（LLM）的集成，提高了语义理解和语音自然度。然而，现有的LLM-based TTS模型缺乏开源训练代码和高效的推理加速框架，限制了其可访问性和适应性。为解决这一问题，我们推出了Muyan-TTS，这是一款为Podcast应用场景设计的开源训练型TTS模型，预算为5万美元。该模型在超过10万小时的Podcast音频数据上进行预训练，实现了零样本TTS合成高质量语音生成。Muyan-TTS还支持利用几十分钟的目标语音进行声音适应，高度个性化定制。除了开源模型外，我们还提供了全面的数据收集和处理流程、完整的训练流程以及优化的推理框架，以加速LLM-based TTS的合成。我们的代码和模型可在<a target="_blank" rel="noopener" href="https://github.com/MYZY-AI/Muyan-TTS%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/MYZY-AI/Muyan-TTS获取。</a></p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>近期TTS模型的进步得益于大型语言模型的集成，提高了语义理解和语音自然度。</li>
<li>现有TTS模型缺乏开源训练代码和高效的推理加速框架。</li>
<li>Muyan-TTS是一款为Podcast应用场景设计的开源训练型TTS模型，预算友好。</li>
<li>Muyan-TTS在大量Podcast音频数据上预训练，可实现零样本TTS合成高质量语音。</li>
<li>Muyan-TTS支持利用几十分钟的语音数据进行声音适应，具有个性化定制的特点。</li>
<li>除了模型开源，还提供了全面的数据收集和处理流程、完整的训练流程。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.19146">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-b021c4a80f6c509fe7c63984c4c19154.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1afcfde174f9fa8c77d3bde178afe37.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d58777200f6cdb62301226283ccc27e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3d228cec6028a7052436c8ee07cc3a2b.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Speaker-Diarization-for-Low-Resource-Languages-Through-Wav2vec-Fine-Tuning"><a href="#Speaker-Diarization-for-Low-Resource-Languages-Through-Wav2vec-Fine-Tuning" class="headerlink" title="Speaker Diarization for Low-Resource Languages Through Wav2vec   Fine-Tuning"></a>Speaker Diarization for Low-Resource Languages Through Wav2vec   Fine-Tuning</h2><p><strong>Authors:Abdulhady Abas Abdullah, Sarkhel H. Taher Karim, Sara Azad Ahmed, Kanar R. Tariq, Tarik A. Rashid</strong></p>
<p>Speaker diarization is a fundamental task in speech processing that involves dividing an audio stream by speaker. Although state-of-the-art models have advanced performance in high-resource languages, low-resource languages such as Kurdish pose unique challenges due to limited annotated data, multiple dialects and frequent code-switching. In this study, we address these issues by training the Wav2Vec 2.0 self-supervised learning model on a dedicated Kurdish corpus. By leveraging transfer learning, we adapted multilingual representations learned from other languages to capture the phonetic and acoustic characteristics of Kurdish speech. Relative to a baseline method, our approach reduced the diarization error rate by seven point two percent and improved cluster purity by thirteen percent. These findings demonstrate that enhancements to existing models can significantly improve diarization performance for under-resourced languages. Our work has practical implications for developing transcription services for Kurdish-language media and for speaker segmentation in multilingual call centers, teleconferencing and video-conferencing systems. The results establish a foundation for building effective diarization systems in other understudied languages, contributing to greater equity in speech technology. </p>
<blockquote>
<p>说话人识别是语音处理中的一项基本任务，涉及按说话人划分音频流。虽然最新模型在高资源语言中的性能得到了提升，但低资源语言（如库尔德语）由于缺乏标注数据、多种方言和频繁的转码等因素而面临独特挑战。在这项研究中，我们通过在一个专门的库尔德语料库上训练Wav2Vec 2.0自监督学习模型来解决这些问题。通过利用迁移学习，我们适应了从其他语言中学到的多语言表示，以捕捉库尔德语的语音和声学特征。相对于基准方法，我们的方法将识别错误率降低了7.2%，聚类纯度提高了13%。这些发现表明对现有模型的改进可以显著提高资源匮乏语言的识别性能。我们的工作对于开发库尔德语媒体的转录服务以及多语言呼叫中心、电话会议和视频会议系统中的说话人分段具有实际意义。结果为其在其他未充分研究的语言中建立有效的识别系统奠定了基础，为语音技术的更大公平性做出了贡献。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.18582v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文研究了基于Wav2Vec 2.0自监督学习模型的库尔德语语音说话人识别技术。针对库尔德语资源有限、方言多样和频繁的代码切换等问题，该研究通过迁移学习技术，利用其他语言的多语言表示来学习库尔德语的语音和声音特征。相较于基准方法，该研究的方法将识别错误率降低了7.2%，并提高了聚类纯度达13%。该研究为发展库尔德语媒体的语音识别服务和多语种呼叫中心、远程会议和视频会议系统中的说话人分割提供了实践启示。它为在其他缺乏研究的语言中建立有效的识别系统奠定了基础，为语音技术的公平发展做出了贡献。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究使用Wav2Vec 2.0自监督学习模型进行库尔德语的说话人识别。</li>
<li>针对库尔德语资源有限、方言多样和代码切换频繁的问题，研究采用迁移学习技术。</li>
<li>与基准方法相比，该研究的方法降低了7.2%的识别错误率，提高了13%的聚类纯度。</li>
<li>研究具有发展库尔德语媒体语音识别服务和多语种呼叫中心、远程会议和视频会议系统中说话人分割的实践意义。</li>
<li>该研究为在其他缺乏研究的语言中建立有效的识别系统奠定了基础。</li>
<li>研究促进了语音技术的公平发展。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.18582">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-b3a39757e021e0104978b88057bc3ece.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-493698fca6a32a2cd575ee3c345040e1.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="A-Multi-modal-Approach-to-Dysarthria-Detection-and-Severity-Assessment-Using-Speech-and-Text-Information"><a href="#A-Multi-modal-Approach-to-Dysarthria-Detection-and-Severity-Assessment-Using-Speech-and-Text-Information" class="headerlink" title="A Multi-modal Approach to Dysarthria Detection and Severity Assessment   Using Speech and Text Information"></a>A Multi-modal Approach to Dysarthria Detection and Severity Assessment   Using Speech and Text Information</h2><p><strong>Authors:Anuprabha M, Krishna Gurugubelli, V Kesavaraj, Anil Kumar Vuppala</strong></p>
<p>Automatic detection and severity assessment of dysarthria are crucial for delivering targeted therapeutic interventions to patients. While most existing research focuses primarily on speech modality, this study introduces a novel approach that leverages both speech and text modalities. By employing cross-attention mechanism, our method learns the acoustic and linguistic similarities between speech and text representations. This approach assesses specifically the pronunciation deviations across different severity levels, thereby enhancing the accuracy of dysarthric detection and severity assessment. All the experiments have been performed using UA-Speech dysarthric database. Improved accuracies of 99.53% and 93.20% in detection, and 98.12% and 51.97% for severity assessment have been achieved when speaker-dependent and speaker-independent, unseen and seen words settings are used. These findings suggest that by integrating text information, which provides a reference linguistic knowledge, a more robust framework has been developed for dysarthric detection and assessment, thereby potentially leading to more effective diagnoses. </p>
<blockquote>
<p>自动检测和评估言语障碍的严重程度对于为患者提供有针对性的治疗干预至关重要。虽然现有的大多数研究主要集中在语音模式上，但本研究介绍了一种利用语音和文本模式的新方法。通过采用交叉注意力机制，我们的方法学习了语音和文本表示之间的声学和语言学相似性。这种方法特别评估了不同严重程度水平的发音偏差，从而提高了言语障碍检测和严重程度评估的准确性。所有实验均使用UA-Speech言语障碍数据库进行。在依赖说话者和独立说话者、未见词和可见词设置的情况下，检测准确率分别提高了99.53%和93.20%，评估准确率分别达到了98.12%和51.97%。这些研究结果表明，通过将文本信息集成提供为参考语言学知识，我们已开发出更稳健的言语障碍检测和评估框架，从而可能带来更准确的诊断结果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16874v4">PDF</a> Submitted to ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>该研究利用语音和文字两种模态，通过跨注意力机制学习语音和文本表示之间的声学和语言学相似性，从而检测发音偏差并进行不同程度评估。这一新型方法在UA-Speechdysarthria数据库上进行实验，显示对于语言病理学患者能提高检测和评估的准确性，具体在未见单词的场景中检测准确性为99.53%，评估准确性为98.12%。该方法融合了文本信息中的参考语言学知识，从而为语言障碍的检测和评估开发了一个更稳健的框架。这有助于提高诊断的精确度，实现更有针对性的治疗干预。 </p>
<p><strong>Key Takeaways</strong> </p>
<p>该文章研究了自动检测语言障碍（如发音障碍）并进行严重程度评估的重要性： </p>
<ul>
<li>研究提出了一种新方法，结合了语音和文字两种模态进行语言障碍的检测和严重程度评估。 </li>
<li>该方法采用跨注意力机制，旨在学习语音和文本之间的声学相似性以及语言学相似性。 </li>
<li>通过在UA-Speechdysarthria数据库上实验验证，此方法能提高检测准确性和评估准确性。对于未见过单词的检测准确性高达99.53%，而评估准确性为98.12%。 </li>
<li>通过结合文本信息中的参考语言学知识，研究构建了一个更为稳健的语言障碍检测和评估框架。 </li>
<li>此方法不仅有助于准确诊断语言障碍，还有助于为语言障碍患者提供针对性的治疗干预。 </li>
<li>该研究显示了利用两种模态或多模态信息的潜力和价值在检测和评估语言障碍领域中具有广泛的应用前景和临床实用价值。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16874">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5bdc7f833ee337ca197f5d84e19ded73.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f65b1001b738413c43c4f3c674f90f25.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-40a1524273b3bc6a496645e48193a05d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d5666ba7d34627fc1b71a5652ff4fc92.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Mamba-in-Speech-Towards-an-Alternative-to-Self-Attention"><a href="#Mamba-in-Speech-Towards-an-Alternative-to-Self-Attention" class="headerlink" title="Mamba in Speech: Towards an Alternative to Self-Attention"></a>Mamba in Speech: Towards an Alternative to Self-Attention</h2><p><strong>Authors:Xiangyu Zhang, Qiquan Zhang, Hexin Liu, Tianyi Xiao, Xinyuan Qian, Beena Ahmed, Eliathamby Ambikairajah, Haizhou Li, Julien Epps</strong></p>
<p>Transformer and its derivatives have achieved success in diverse tasks across computer vision, natural language processing, and speech processing. To reduce the complexity of computations within the multi-head self-attention mechanism in Transformer, Selective State Space Models (i.e., Mamba) were proposed as an alternative. Mamba exhibited its effectiveness in natural language processing and computer vision tasks, but its superiority has rarely been investigated in speech signal processing. This paper explores solutions for applying Mamba to speech processing by discussing two typical speech processing tasks: speech recognition, which requires semantic and sequential information, and speech enhancement, which focuses primarily on sequential patterns. The experimental results confirm that bidirectional Mamba (BiMamba) consistently outperforms vanilla Mamba, highlighting the advantages of a bidirectional design for speech processing. Moreover, experiments demonstrate the effectiveness of BiMamba as an alternative to the self-attention module in the Transformer model and its derivates, particularly for the semantic-aware task. The crucial technologies for transferring Mamba to speech are then summarized in ablation studies and the discussion section, offering insights for extending this research to a broader scope of tasks. </p>
<blockquote>
<p>Transformer及其衍生品已经在计算机视觉、自然语言处理和语音识别等多个领域取得了巨大的成功。为了减少Transformer多头自注意力机制中的计算复杂性，选择性状态空间模型（例如Mamba）作为一种替代方案被提出。Mamba在自然语言处理和计算机视觉任务中展现了其有效性，但在语音信号处理方面的优越性却鲜有研究。本文通过讨论两个典型的语音处理任务：需要语义和序列信息的语音识别以及主要关注序列模式的语音增强，探索了将Mamba应用于语音处理的方法。实验结果表明，双向Mamba（BiMamba）始终优于普通Mamba，凸显了双向设计在语音处理中的优势。此外，实验还证明了BiMamba作为Transformer模型及其衍生模型的自注意力模块的替代方案的有效性，特别是在语义感知任务中。随后，在剔除研究和讨论部分中对将Mamba转移到语音的关键技术进行了总结，为将这项研究扩展到更广泛的任务范围提供了见解。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.12609v6">PDF</a> </p>
<p><strong>Summary</strong><br>     本文探讨了将Selective State Space Models（Mamba）应用于语音处理的可能性，通过对比Mamba和双向Mamba（BiMamba）在语音识别和语音增强两个典型任务上的表现，发现BiMamba表现更优。实验证明BiMamba可以作为Transformer模型及其衍生版本中自注意模块的有效替代方案，特别是在语义感知任务中。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformer及其衍生物已在计算机视觉、自然语言处理和语音处理等多个任务中取得巨大成功。</li>
<li>Mamba作为一种选择性状态空间模型，旨在简化Transformer中的多头自注意机制的计算复杂性。</li>
<li>Mamba在自然语言处理和计算机视觉任务中表现出其有效性，但在语音信号处理中的优越性鲜有研究。</li>
<li>本文尝试将Mamba应用于语音处理，并探讨了语音识别和语音增强两个典型任务。</li>
<li>实验结果显示，双向Mamba（BiMamba）在语音处理任务上表现优于普通Mamba，特别是在语义感知任务中。</li>
<li>BiMamba可作为Transformer模型及其衍生版本中自注意模块的有效替代方案。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.12609">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f3f86747c9626cbebc79580c19c0eeb6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fde81d4b675707adbc5be7c0dbaa3225.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-de75439355389d2528042e20fed8555b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f9d09357f6a5acd3adf0e1e33c356c6f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f71a59af5cf4fc8bcaa911b5eb1262b.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-30/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-30/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-30/GAN/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-4bebe86866bcabccac2c738a695a9385.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN 方向最新论文已更新，请持续关注 Update in 2025-04-30  DeeCLIP A Robust and Generalizable Transformer-Based Framework for   Detecting AI-Generated Images
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-30
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-30/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-5bd3151e5ac50600dcdf2680966804d0.jpg" class="responsive-img" alt="无监督/半监督/对比学习">
                        
                        <span class="card-title">无监督/半监督/对比学习</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            无监督/半监督/对比学习 方向最新论文已更新，请持续关注 Update in 2025-04-30  Co-Training with Active Contrastive Learning and Meta-Pseudo-Labeling on   2D Projections for Deep Semi-Supervised Learning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-30
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    无监督/半监督/对比学习
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">无监督/半监督/对比学习</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">17665k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
