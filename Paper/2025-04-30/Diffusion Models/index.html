<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-30  DeeCLIP A Robust and Generalizable Transformer-Based Framework for   Detecting AI-Generated Images">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-af5411e14789d5434717749e0cb1e8ae.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-30
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    18.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    74 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-30-æ›´æ–°"><a href="#2025-04-30-æ›´æ–°" class="headerlink" title="2025-04-30 æ›´æ–°"></a>2025-04-30 æ›´æ–°</h1><h2 id="DeeCLIP-A-Robust-and-Generalizable-Transformer-Based-Framework-for-Detecting-AI-Generated-Images"><a href="#DeeCLIP-A-Robust-and-Generalizable-Transformer-Based-Framework-for-Detecting-AI-Generated-Images" class="headerlink" title="DeeCLIP: A Robust and Generalizable Transformer-Based Framework for   Detecting AI-Generated Images"></a>DeeCLIP: A Robust and Generalizable Transformer-Based Framework for   Detecting AI-Generated Images</h2><p><strong>Authors:Mamadou Keita, Wassim Hamidouche, Hessen Bougueffa Eutamene, Abdelmalik Taleb-Ahmed, Abdenour Hadid</strong></p>
<p>This paper introduces DeeCLIP, a novel framework for detecting AI-generated images using CLIP-ViT and fusion learning. Despite significant advancements in generative models capable of creating highly photorealistic images, existing detection methods often struggle to generalize across different models and are highly sensitive to minor perturbations. To address these challenges, DeeCLIP incorporates DeeFuser, a fusion module that combines high-level and low-level features, improving robustness against degradations such as compression and blurring. Additionally, we apply triplet loss to refine the embedding space, enhancing the modelâ€™s ability to distinguish between real and synthetic content. To further enable lightweight adaptation while preserving pre-trained knowledge, we adopt parameter-efficient fine-tuning using low-rank adaptation (LoRA) within the CLIP-ViT backbone. This approach supports effective zero-shot learning without sacrificing generalization. Trained exclusively on 4-class ProGAN data, DeeCLIP achieves an average accuracy of 89.00% on 19 test subsets composed of generative adversarial network (GAN) and diffusion models. Despite having fewer trainable parameters, DeeCLIP outperforms existing methods, demonstrating superior robustness against various generative models and real-world distortions. The code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/Mamadou-Keita/DeeCLIP">https://github.com/Mamadou-Keita/DeeCLIP</a> for research purposes. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†DeeCLIPï¼Œè¿™æ˜¯ä¸€ä¸ªä½¿ç”¨CLIP-ViTå’Œèåˆå­¦ä¹ æ£€æµ‹AIç”Ÿæˆå›¾åƒçš„æ–°å‹æ¡†æ¶ã€‚å°½ç®¡ç”Ÿæˆæ¨¡å‹åœ¨åˆ›é€ é«˜åº¦é€¼çœŸçš„å›¾åƒæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç°æœ‰çš„æ£€æµ‹æ–¹æ³•åœ¨è·¨ä¸åŒæ¨¡å‹æ¨å¹¿æ—¶ç»å¸¸é‡åˆ°å›°éš¾ï¼Œå¹¶ä¸”å¯¹å¾®å°çš„æ‰°åŠ¨éå¸¸æ•æ„Ÿã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼ŒDeeCLIPç»“åˆäº†DeeFuserï¼Œä¸€ä¸ªèåˆæ¨¡å—ï¼Œå®ƒç»“åˆäº†é«˜çº§å’Œä½çº§ç‰¹å¾ï¼Œæé«˜äº†å¯¹å‹ç¼©ã€æ¨¡ç³Šç­‰é™è´¨çš„ç¨³å¥æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åº”ç”¨äº†ä¸‰å…ƒç»„æŸå¤±æ¥ä¼˜åŒ–åµŒå…¥ç©ºé—´ï¼Œæé«˜æ¨¡å‹åŒºåˆ†çœŸå®å’Œåˆæˆå†…å®¹çš„èƒ½åŠ›ã€‚ä¸ºäº†è¿›ä¸€æ­¥å®ç°åœ¨ä¿æŒé¢„è®­ç»ƒçŸ¥è¯†çš„åŒæ—¶è¿›è¡Œè½»é‡çº§é€‚åº”ï¼Œæˆ‘ä»¬åœ¨CLIP-ViTä¸»å¹²ä¸­é‡‡ç”¨ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰è¿›è¡Œå‚æ•°é«˜æ•ˆå¾®è°ƒã€‚è¿™ç§æ–¹æ³•æ”¯æŒæœ‰æ•ˆçš„é›¶æ ·æœ¬å­¦ä¹ ï¼Œè€Œä¸ä¼šç‰ºç‰²æ³›åŒ–èƒ½åŠ›ã€‚DeeCLIPä»…å¯¹ProGANçš„å››å¤§ç±»æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œåœ¨å¯¹ç”±ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰å’Œæ‰©æ•£æ¨¡å‹ç»„æˆçš„19ä¸ªæµ‹è¯•å­é›†ä¸Šå–å¾—äº†å¹³å‡89.00%çš„å‡†ç¡®ç‡ã€‚å°½ç®¡æ‹¥æœ‰è¾ƒå°‘çš„å¯è®­ç»ƒå‚æ•°ï¼Œä½†DeeCLIPåœ¨é’ˆå¯¹å„ç§ç”Ÿæˆæ¨¡å‹å’Œç°å®ä¸–ç•Œç•¸å˜çš„æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šçš„ç¨³å¥æ€§ã€‚è¯¥ä»£ç å·²å…¬å¼€å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/Mamadou-Keita/DeeCLIP%E4%B8%8A%E4%BE%9B%E7%A0%94%E7%A9%B6%E4%BD%BF%E7%94%A8%E3%80%82">https://github.com/Mamadou-Keita/DeeCLIPä¸Šä¾›ç ”ç©¶ä½¿ç”¨ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.19876v1">PDF</a> </p>
<p><strong>Summary</strong><br>DeeCLIPæ¡†æ¶åˆ©ç”¨CLIP-ViTå’Œèåˆå­¦ä¹ æŠ€æœ¯æ£€æµ‹AIç”Ÿæˆçš„å›¾åƒã€‚å®ƒé€šè¿‡ç»“åˆé«˜çº§å’Œä½çº§ç‰¹å¾æé«˜æ£€æµ‹æ¨¡å‹çš„ç¨³å¥æ€§ï¼Œå¹¶é‡‡ç”¨ä¸‰é‡æŸå¤±ä¼˜åŒ–åµŒå…¥ç©ºé—´ã€‚é€šè¿‡é‡‡ç”¨ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰è¿›è¡Œå¾®è°ƒï¼Œæ”¯æŒé›¶æ ·æœ¬å­¦ä¹ ä¸”ä¸å½±å“æ³›åŒ–èƒ½åŠ›ã€‚åœ¨ç‰¹å®šæ•°æ®é›†ä¸Šè®­ç»ƒçš„DeeCLIPå¯¹å¤šç§ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰å’Œæ‰©æ•£æ¨¡å‹è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DeeCLIPæ˜¯ä¸€ä¸ªç”¨äºæ£€æµ‹AIç”Ÿæˆå›¾åƒçš„æ–°æ¡†æ¶ï¼Œç»“åˆCLIP-ViTå’Œèåˆå­¦ä¹ æŠ€æœ¯ã€‚</li>
<li>DeeCLIPè§£å†³äº†ç°æœ‰æ£€æµ‹æ–¹æ³•çš„ä¸¤å¤§æŒ‘æˆ˜ï¼šéš¾ä»¥è·¨ä¸åŒæ¨¡å‹æ³›åŒ–å’Œå¯¹å¾®å°æ‰°åŠ¨çš„æ•æ„Ÿæ€§ã€‚</li>
<li>DeeCLIPå¼•å…¥DeeFuseræ¨¡å—ï¼Œç»“åˆé«˜ä½çº§ç‰¹å¾ä»¥æé«˜ç¨³å¥æ€§ï¼Œå¯¹æŠ—å›¾åƒå‹ç¼©ã€æ¨¡ç³Šç­‰é™è§£ã€‚</li>
<li>é‡‡ç”¨ä¸‰é‡æŸå¤±æ¥ä¼˜åŒ–åµŒå…¥ç©ºé—´ï¼Œæé«˜æ¨¡å‹åŒºåˆ†çœŸå®å’Œåˆæˆå†…å®¹çš„èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡é‡‡ç”¨ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰è¿›è¡Œå¾®è°ƒï¼ŒDeeCLIPå®ç°äº†æœ‰æ•ˆçš„é›¶æ ·æœ¬å­¦ä¹ ï¼ŒåŒæ—¶ä¿æŒæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>DeeCLIPåœ¨ç‰¹å®šæ•°æ®é›†ä¸Šè®­ç»ƒï¼Œå¯¹å¤šç§ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰å’Œæ‰©æ•£æ¨¡å‹è¡¨ç°å‡ºå¼ºå¤§çš„æ£€æµ‹æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.19876">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4e6320e6fe50117872a2d951d4e968ac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce2cfa4abda0ce37191585c56cdd6e7e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4bebe86866bcabccac2c738a695a9385.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-000067ea4bdd3d6a56cd412e0960d028.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Multimodal-Conditioned-Diffusive-Time-Series-Forecasting"><a href="#Multimodal-Conditioned-Diffusive-Time-Series-Forecasting" class="headerlink" title="Multimodal Conditioned Diffusive Time Series Forecasting"></a>Multimodal Conditioned Diffusive Time Series Forecasting</h2><p><strong>Authors:Chen Su, Yuanhe Tian, Yan Song</strong></p>
<p>Diffusion models achieve remarkable success in processing images and text, and have been extended to special domains such as time series forecasting (TSF). Existing diffusion-based approaches for TSF primarily focus on modeling single-modality numerical sequences, overlooking the rich multimodal information in time series data. To effectively leverage such information for prediction, we propose a multimodal conditioned diffusion model for TSF, namely, MCD-TSF, to jointly utilize timestamps and texts as extra guidance for time series modeling, especially for forecasting. Specifically, Timestamps are combined with time series to establish temporal and semantic correlations among different data points when aggregating information along the temporal dimension. Texts serve as supplementary descriptions of time seriesâ€™ history, and adaptively aligned with data points as well as dynamically controlled in a classifier-free manner. Extensive experiments on real-world benchmark datasets across eight domains demonstrate that the proposed MCD-TSF model achieves state-of-the-art performance. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨å¤„ç†å›¾åƒå’Œæ–‡æœ¬æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œå¹¶å·²æ‰©å±•åˆ°æ—¶é—´åºåˆ—é¢„æµ‹ï¼ˆTSFï¼‰ç­‰ç‰¹æ®Šé¢†åŸŸã€‚ç°æœ‰çš„åŸºäºæ‰©æ•£çš„TSFæ–¹æ³•ä¸»è¦ä¸“æ³¨äºå¯¹å•æ¨¡æ€æ•°å€¼åºåˆ—è¿›è¡Œå»ºæ¨¡ï¼Œå¿½è§†äº†æ—¶é—´åºåˆ—æ•°æ®ä¸­çš„ä¸°å¯Œå¤šæ¨¡æ€ä¿¡æ¯ã€‚ä¸ºäº†æœ‰æ•ˆåˆ©ç”¨è¿™äº›ä¿¡æ¯æ¥è¿›è¡Œé¢„æµ‹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºTSFçš„å¤šæ¨¡æ€æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œå³MCD-TSFï¼Œä»¥è”åˆä½¿ç”¨æ—¶é—´æˆ³å’Œæ–‡æœ¬ä½œä¸ºæ—¶é—´åºåˆ—å»ºæ¨¡çš„é¢å¤–æŒ‡å¯¼ï¼Œå°¤å…¶æ˜¯è¿›è¡Œé¢„æµ‹ã€‚å…·ä½“æ¥è¯´ï¼Œæ—¶é—´æˆ³ä¸æ—¶é—´åºåˆ—ç›¸ç»“åˆï¼Œåœ¨æ²¿æ—¶é—´ç»´åº¦èšåˆä¿¡æ¯æ—¶å»ºç«‹ä¸åŒæ•°æ®ç‚¹ä¹‹é—´çš„æ—¶æ€å’Œè¯­ä¹‰å…³è”ã€‚æ–‡æœ¬ä½œä¸ºæ—¶é—´åºåˆ—å†å²çš„è¡¥å……æè¿°ï¼Œä¸æ•°æ®ç‚¹è‡ªé€‚åº”å¯¹é½ï¼Œå¹¶ä»¥æ— åˆ†ç±»å™¨çš„æ–¹å¼åŠ¨æ€æ§åˆ¶ã€‚åœ¨å…«ä¸ªé¢†åŸŸçš„çœŸå®ä¸–ç•ŒåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„MCD-TSFæ¨¡å‹è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.19669v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒå’Œæ–‡æœ¬å¤„ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œå¹¶å·²æ‰©å±•åˆ°æ—¶é—´åºåˆ—é¢„æµ‹ç­‰ç‰¹æ®Šé¢†åŸŸã€‚é’ˆå¯¹ç°æœ‰åŸºäºæ‰©æ•£çš„æ—¶é—´åºåˆ—é¢„æµ‹æ–¹æ³•ä¸»è¦å…³æ³¨å•æ¨¡æ€æ•°å€¼åºåˆ—å»ºæ¨¡ï¼Œå¿½è§†æ—¶é—´åºåˆ—æ•°æ®ä¸­ä¸°å¯Œçš„å¤šæ¨¡æ€ä¿¡æ¯çš„é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€æ¡ä»¶æ‰©æ•£æ¨¡å‹MCD-TSFï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿè”åˆåˆ©ç”¨æ—¶é—´æˆ³å’Œæ–‡æœ¬ä½œä¸ºæ—¶é—´åºåˆ—å»ºæ¨¡çš„é¢å¤–æŒ‡å¯¼ï¼Œå°¤å…¶æ˜¯è¿›è¡Œé¢„æµ‹ã€‚é€šè¿‡ç»“åˆæ—¶é—´æˆ³å’Œæ—¶é—´åºåˆ—ï¼Œå»ºç«‹ä¸åŒæ•°æ®ç‚¹ä¹‹é—´çš„æ—¶é—´å’Œè¯­ä¹‰å…³è”ï¼ŒåŒæ—¶åˆ©ç”¨æ–‡æœ¬ä½œä¸ºæ—¶é—´åºåˆ—å†å²çš„è¡¥å……æè¿°ï¼Œè‡ªé€‚åº”åœ°ä¸ç›®æ ‡æ•°æ®ç‚¹å¯¹é½ï¼Œå¹¶ä»¥æ— åˆ†ç±»å™¨çš„æ–¹å¼åŠ¨æ€æ§åˆ¶ã€‚åœ¨å…«ä¸ªé¢†åŸŸçš„çœŸå®ä¸–ç•ŒåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„MCD-TSFæ¨¡å‹è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹å·²æˆåŠŸåº”ç”¨äºå›¾åƒå’Œæ–‡æœ¬å¤„ç†ï¼Œå¹¶æ‰©å±•è‡³æ—¶é—´åºåˆ—é¢„æµ‹é¢†åŸŸã€‚</li>
<li>ç°æœ‰æ—¶åºé¢„æµ‹æ¨¡å‹ä¸»è¦å…³æ³¨å•æ¨¡æ€æ•°å€¼åºåˆ—å»ºæ¨¡ï¼Œå¿½è§†äº†å¤šæ¨¡æ€ä¿¡æ¯çš„é‡è¦æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„å¤šæ¨¡æ€æ¡ä»¶æ‰©æ•£æ¨¡å‹MCD-TSFï¼Œè”åˆåˆ©ç”¨æ—¶é—´æˆ³å’Œæ–‡æœ¬ä¿¡æ¯ã€‚</li>
<li>MCD-TSFæ¨¡å‹é€šè¿‡ç»“åˆæ—¶é—´æˆ³å’Œæ—¶é—´åºåˆ—ä¿¡æ¯ï¼Œå»ºç«‹æ•°æ®ç‚¹é—´çš„æ—¶ç©ºè¯­ä¹‰å…³è”ã€‚</li>
<li>æ–‡æœ¬ä¿¡æ¯ä½œä¸ºæ—¶é—´åºåˆ—å†å²çš„è¡¥å……æè¿°ï¼Œä¸æ•°æ®ç‚¹è‡ªé€‚åº”å¯¹é½ã€‚</li>
<li>MCD-TSFæ¨¡å‹ä»¥æ— åˆ†ç±»å™¨çš„æ–¹å¼åŠ¨æ€æ§åˆ¶æ–‡æœ¬ä¿¡æ¯çš„åˆ©ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.19669">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cc1d46bddb7f11313f4f2a0baa1a44c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9ebc3fbdb7a5b55eb373be0dba321b38.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de78149cfa5974b26e8223a6972744d4.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Robot-Motion-Planning-using-One-Step-Diffusion-with-Noise-Optimized-Approximate-Motions"><a href="#Robot-Motion-Planning-using-One-Step-Diffusion-with-Noise-Optimized-Approximate-Motions" class="headerlink" title="Robot Motion Planning using One-Step Diffusion with Noise-Optimized   Approximate Motions"></a>Robot Motion Planning using One-Step Diffusion with Noise-Optimized   Approximate Motions</h2><p><strong>Authors:Tomoharu Aizu, Takeru Oba, Yuki Kondo, Norimichi Ukita</strong></p>
<p>This paper proposes an image-based robot motion planning method using a one-step diffusion model. While the diffusion model allows for high-quality motion generation, its computational cost is too expensive to control a robot in real time. To achieve high quality and efficiency simultaneously, our one-step diffusion model takes an approximately generated motion, which is predicted directly from input images. This approximate motion is optimized by additive noise provided by our novel noise optimizer. Unlike general isotropic noise, our noise optimizer adjusts noise anisotropically depending on the uncertainty of each motion element. Our experimental results demonstrate that our method outperforms state-of-the-art methods while maintaining its efficiency by one-step diffusion. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå›¾åƒçš„æœºå™¨äººè¿åŠ¨è§„åˆ’æ–¹æ³•ï¼Œä½¿ç”¨ä¸€æ­¥æ‰©æ•£æ¨¡å‹ã€‚è™½ç„¶æ‰©æ•£æ¨¡å‹å¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„è¿åŠ¨ï¼Œä½†å…¶è®¡ç®—æˆæœ¬è¿‡é«˜ï¼Œæ— æ³•å®æ—¶æ§åˆ¶æœºå™¨äººã€‚ä¸ºäº†åŒæ—¶å®ç°é«˜è´¨é‡å’Œé«˜æ•ˆç‡ï¼Œæˆ‘ä»¬çš„ä¸€æ­¥æ‰©æ•£æ¨¡å‹ç›´æ¥ä»è¾“å…¥å›¾åƒé¢„æµ‹è¿‘ä¼¼ç”Ÿæˆçš„è¿åŠ¨ã€‚è¿™ç§è¿‘ä¼¼è¿åŠ¨é€šè¿‡æˆ‘ä»¬æ–°å‹å™ªå£°ä¼˜åŒ–å™¨æä¾›çš„é™„åŠ å™ªå£°è¿›è¡Œä¼˜åŒ–ã€‚ä¸åŒäºä¸€èˆ¬çš„åŒå‘å™ªå£°ï¼Œæˆ‘ä»¬çš„å™ªå£°ä¼˜åŒ–å™¨ä¼šæ ¹æ®æ¯ä¸ªè¿åŠ¨å…ƒç´ çš„ä¸ç¡®å®šæ€§æ¥è°ƒæ•´å¼‚å‘å™ªå£°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¿æŒä¸€æ­¥æ‰©æ•£æ•ˆç‡çš„åŒæ—¶ï¼Œä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.19652v1">PDF</a> 7 pages, 5 figures. Under peer review</p>
<p><strong>Summary</strong></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„å›¾åƒæœºå™¨äººè¿åŠ¨è§„åˆ’æ–¹æ³•ã€‚é€šè¿‡æ‰©æ•£æ¨¡å‹å®ç°é«˜è´¨é‡è¿åŠ¨ç”Ÿæˆï¼Œä½†å…¶è®¡ç®—æˆæœ¬é«˜æ˜‚æ— æ³•å®ç°å®æ—¶æ§åˆ¶æœºå™¨äººã€‚ä¸ºè§£å†³è¿™ä¸€éš¾é¢˜ï¼Œé‡‡ç”¨ä¸€æ­¥æ‰©æ•£æ¨¡å‹å¯¹åŸºäºå›¾åƒé¢„æµ‹çš„è¿‘ä¼¼è¿åŠ¨è¿›è¡Œä¼˜åŒ–ï¼Œå¹¶é€šè¿‡æ–°å‹å™ªå£°ä¼˜åŒ–å™¨æ·»åŠ å™ªå£°ã€‚è¯¥å™ªå£°ä¼˜åŒ–å™¨èƒ½å¤Ÿæ ¹æ®ä¸åŒè¿åŠ¨å…ƒç´ çš„ä¸ç¡®å®šæ€§è°ƒæ•´å™ªå£°çš„å¼‚å‘æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒæ•ˆç‡çš„åŒæ—¶ï¼Œä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„å›¾åƒæœºå™¨äººè¿åŠ¨è§„åˆ’æ–¹æ³•ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹å¯å®ç°é«˜è´¨é‡è¿åŠ¨ç”Ÿæˆï¼Œä½†è®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚</li>
<li>ä¸ºæé«˜æ•ˆç‡ï¼Œé‡‡ç”¨ä¸€æ­¥æ‰©æ•£æ¨¡å‹å¯¹åŸºäºå›¾åƒé¢„æµ‹çš„è¿‘ä¼¼è¿åŠ¨è¿›è¡Œä¼˜åŒ–ã€‚</li>
<li>æ–°å‹å™ªå£°ä¼˜åŒ–å™¨èƒ½å¤Ÿæ ¹æ®ä¸åŒè¿åŠ¨å…ƒç´ çš„ä¸ç¡®å®šæ€§è°ƒæ•´å™ªå£°çš„å¼‚å‘æ€§ã€‚</li>
<li>è®ºæ–‡é€šè¿‡å®éªŒç»“æœéªŒè¯äº†è¯¥æ–¹æ³•åœ¨ä¿æŒæ•ˆç‡çš„åŒæ—¶ï¼Œä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
<li>è¯¥æ–¹æ³•å®ç°äº†å›¾åƒä¿¡æ¯åˆ°æœºå™¨äººè¿åŠ¨çš„è½¬åŒ–ï¼Œæé«˜äº†æœºå™¨äººçš„è‡ªä¸»æ€§å’Œé€‚åº”æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.19652">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9933c8a68d1c769376477b7cb612055c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5ab5d8e6b8d99c864f37c4c5f7f5f59d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-527890657b23651684012b3f9d546231.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0607c5b57b9367a103c2e3ca00dd8b91.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b646a0cffeff07d501920ff63f331509.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f0a8d00eb6384b21e009e7ec040b4d1.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="AI-Alignment-in-Medical-Imaging-Unveiling-Hidden-Biases-Through-Counterfactual-Analysis"><a href="#AI-Alignment-in-Medical-Imaging-Unveiling-Hidden-Biases-Through-Counterfactual-Analysis" class="headerlink" title="AI Alignment in Medical Imaging: Unveiling Hidden Biases Through   Counterfactual Analysis"></a>AI Alignment in Medical Imaging: Unveiling Hidden Biases Through   Counterfactual Analysis</h2><p><strong>Authors:Haroui Ma, Francesco Quinzan, Theresa Willem, Stefan Bauer</strong></p>
<p>Machine learning (ML) systems for medical imaging have demonstrated remarkable diagnostic capabilities, but their susceptibility to biases poses significant risks, since biases may negatively impact generalization performance. In this paper, we introduce a novel statistical framework to evaluate the dependency of medical imaging ML models on sensitive attributes, such as demographics. Our method leverages the concept of counterfactual invariance, measuring the extent to which a modelâ€™s predictions remain unchanged under hypothetical changes to sensitive attributes. We present a practical algorithm that combines conditional latent diffusion models with statistical hypothesis testing to identify and quantify such biases without requiring direct access to counterfactual data. Through experiments on synthetic datasets and large-scale real-world medical imaging datasets, including \textsc{cheXpert} and MIMIC-CXR, we demonstrate that our approach aligns closely with counterfactual fairness principles and outperforms standard baselines. This work provides a robust tool to ensure that ML diagnostic systems generalize well, e.g., across demographic groups, offering a critical step towards AI safety in healthcare. Code: <a target="_blank" rel="noopener" href="https://github.com/Neferpitou3871/AI-Alignment-Medical-Imaging">https://github.com/Neferpitou3871/AI-Alignment-Medical-Imaging</a>. </p>
<blockquote>
<p>åŒ»å­¦å½±åƒæœºå™¨å­¦ä¹ ç³»ç»Ÿå±•ç°å‡ºäº†å‡ºè‰²çš„è¯Šæ–­èƒ½åŠ›ï¼Œä½†å®ƒä»¬å®¹æ˜“å—åˆ°åè§çš„å½±å“ï¼Œä»è€Œå¸¦æ¥é‡å¤§é£é™©ï¼Œå› ä¸ºåè§å¯èƒ½ä¼šæŸå®³å…¶æ³›åŒ–æ€§èƒ½ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°å‹ç»Ÿè®¡æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°åŒ»å­¦å½±åƒæœºå™¨å­¦ä¹ æ¨¡å‹å¯¹æ•æ„Ÿå±æ€§çš„ä¾èµ–ç¨‹åº¦ï¼Œä¾‹å¦‚äººå£ç»Ÿè®¡å­¦ç‰¹å¾ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨åäº‹å®ä¸å˜æ€§çš„æ¦‚å¿µï¼Œè¡¡é‡æ¨¡å‹é¢„æµ‹åœ¨æ•æ„Ÿå±æ€§å‡è®¾æ€§å˜åŒ–ä¸‹ä¿æŒä¸å˜çš„ç¨‹åº¦ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å®ç”¨ç®—æ³•ï¼Œå®ƒå°†æ¡ä»¶æ½œåœ¨æ‰©æ•£æ¨¡å‹ä¸ç»Ÿè®¡å‡è®¾æ£€éªŒç›¸ç»“åˆï¼Œæ— éœ€ç›´æ¥è®¿é—®åäº‹å®æ•°æ®å³å¯è¯†åˆ«å’Œé‡åŒ–è¿™ç§åè§ã€‚é€šè¿‡å¯¹åˆæˆæ•°æ®é›†å’Œå¤§è§„æ¨¡ç°å®ä¸–ç•ŒåŒ»å­¦å½±åƒæ•°æ®é›†ï¼ˆåŒ…æ‹¬ CheXpert å’Œ MIMIC-CXRï¼‰çš„å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•ä¸åäº‹å®å…¬å¹³åŸåˆ™é«˜åº¦å»åˆï¼Œå¹¶ä¼˜äºæ ‡å‡†åŸºçº¿ã€‚è¿™é¡¹å·¥ä½œæä¾›äº†ä¸€ä¸ªå¼ºå¤§çš„å·¥å…·ï¼Œå¯ä»¥ç¡®ä¿æœºå™¨å­¦ä¹ è¯Šæ–­ç³»ç»Ÿåœ¨è¯¸å¦‚äººå£ç»Ÿè®¡å­¦ç‰¹å¾ç­‰æ–¹é¢å…·æœ‰è‰¯å¥½çš„æ³›åŒ–æ€§èƒ½ï¼Œä¸ºåŒ»ç–—ä¿å¥é¢†åŸŸçš„äººå·¥æ™ºèƒ½å®‰å…¨è¿ˆå‡ºäº†å…³é”®ä¸€æ­¥ã€‚ä»£ç ï¼š<a target="_blank" rel="noopener" href="https://github.com/Neferpitou3871/AI-Alignment-Medical-Imaging%E3%80%82">https://github.com/Neferpitou3871/AI-Alignment-Medical-Imagingã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.19621v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>é’ˆå¯¹æœºå™¨å­¦ä¹ åœ¨åŒ»ç–—å½±åƒè¯Šæ–­ä¸­å­˜åœ¨çš„åè§é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºä¸€ç§æ–°å‹ç»Ÿè®¡æ¡†æ¶ç”¨äºè¯„ä¼°åŒ»ç–—å½±åƒæœºå™¨å­¦ä¹ æ¨¡å‹å¯¹æ•æ„Ÿå±æ€§ï¼ˆå¦‚äººå£ç»Ÿè®¡å­¦ç‰¹å¾ï¼‰çš„ä¾èµ–ç¨‹åº¦ã€‚è¯¥æ¡†æ¶åˆ©ç”¨åäº‹å®ä¸å˜æ€§æ¦‚å¿µï¼Œè¡¡é‡æ¨¡å‹é¢„æµ‹åœ¨æ•æ„Ÿå±æ€§å‡è®¾å˜åŒ–ä¸‹çš„ä¸€è‡´æ€§ã€‚ç»“åˆæ¡ä»¶æ½œåœ¨æ‰©æ•£æ¨¡å‹å’Œç»Ÿè®¡å‡è®¾æ£€éªŒçš„å®ç”¨ç®—æ³•ï¼Œå¯åœ¨æ— éœ€åäº‹å®æ•°æ®çš„æƒ…å†µä¸‹è¯†åˆ«å’Œé‡åŒ–åè§ã€‚åœ¨åˆæˆæ•°æ®é›†å’Œå¤§è§„æ¨¡çœŸå®åŒ»ç–—å½±åƒæ•°æ®é›†ï¼ˆåŒ…æ‹¬CheXpertå’ŒMIMIC-CXRï¼‰ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¸åäº‹å®å…¬å¹³åŸåˆ™ç´§å¯†å¥‘åˆå¹¶ä¼˜äºæ ‡å‡†åŸºçº¿ã€‚æœ¬ç ”ç©¶ä¸ºç¡®ä¿æœºå™¨å­¦ä¹ è¯Šæ–­ç³»ç»Ÿåœ¨è¯¸å¦‚è·¨ä¸åŒäººç¾¤ç­‰æƒ…å†µä¸‹å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›æä¾›äº†å¼ºæœ‰åŠ›çš„å·¥å…·ï¼Œæ˜¯åŒ»ç–—ä¿å¥é¢†åŸŸäººå·¥æ™ºèƒ½å®‰å…¨çš„å…³é”®ä¸€æ­¥ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æœºå™¨å­¦ä¹ åœ¨åŒ»ç–—å½±åƒè¯Šæ–­ä¸­å­˜åœ¨åè§é£é™©ï¼Œå¯èƒ½å½±å“æ¨¡å‹çš„æ³›åŒ–æ€§èƒ½ã€‚</li>
<li>æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹ç»Ÿè®¡æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°åŒ»ç–—å½±åƒMLæ¨¡å‹å¯¹æ•æ„Ÿå±æ€§çš„ä¾èµ–ç¨‹åº¦ã€‚</li>
<li>åˆ©ç”¨åäº‹å®ä¸å˜æ€§æ¦‚å¿µï¼Œè¡¡é‡æ¨¡å‹é¢„æµ‹åœ¨æ•æ„Ÿå±æ€§å‡è®¾å˜åŒ–ä¸‹çš„ä¸€è‡´æ€§ã€‚</li>
<li>æå‡ºç»“åˆæ¡ä»¶æ½œåœ¨æ‰©æ•£æ¨¡å‹å’Œç»Ÿè®¡å‡è®¾æ£€éªŒçš„å®ç”¨ç®—æ³•ï¼Œè¯†åˆ«å¹¶é‡åŒ–åè§ã€‚</li>
<li>æ— éœ€åäº‹å®æ•°æ®å³å¯è¿›è¡Œåè§è¯†åˆ«å’Œé‡åŒ–æ˜¯ä¸€ç§åˆ›æ–°æ–¹æ³•ã€‚</li>
<li>åœ¨åˆæˆå’ŒçœŸå®åŒ»ç–—å½±åƒæ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚</li>
<li>è¯¥æ–¹æ³•ä¸ºç¡®ä¿æœºå™¨å­¦ä¹ è¯Šæ–­ç³»ç»Ÿçš„æ³›åŒ–èƒ½åŠ›å’Œåœ¨åŒ»ç–—ä¿å¥é¢†åŸŸçš„äººå·¥æ™ºèƒ½å®‰å…¨æä¾›äº†é‡è¦å·¥å…·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.19621">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-697eab417cfb0a3c638b2efa916ee444.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-07d637465e9b81f9732b8a6c72aa96ee.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="GenPTW-In-Generation-Image-Watermarking-for-Provenance-Tracing-and-Tamper-Localization"><a href="#GenPTW-In-Generation-Image-Watermarking-for-Provenance-Tracing-and-Tamper-Localization" class="headerlink" title="GenPTW: In-Generation Image Watermarking for Provenance Tracing and   Tamper Localization"></a>GenPTW: In-Generation Image Watermarking for Provenance Tracing and   Tamper Localization</h2><p><strong>Authors:Zhenliang Gan, Chunya Liu, Yichao Tang, Binghao Wang, Weiqiang Wang, Xinpeng Zhang</strong></p>
<p>The rapid development of generative image models has brought tremendous opportunities to AI-generated content (AIGC) creation, while also introducing critical challenges in ensuring content authenticity and copyright ownership. Existing image watermarking methods, though partially effective, often rely on post-processing or reference images, and struggle to balance fidelity, robustness, and tamper localization. To address these limitations, we propose GenPTW, an In-Generation image watermarking framework for latent diffusion models (LDMs), which integrates Provenance Tracing and Tamper Localization into a unified Watermark-based design. It embeds structured watermark signals during the image generation phase, enabling unified provenance tracing and tamper localization. For extraction, we construct a frequency-coordinated decoder to improve robustness and localization precision in complex editing scenarios. Additionally, a distortion layer that simulates AIGC editing is introduced to enhance robustness. Extensive experiments demonstrate that GenPTW outperforms existing methods in image fidelity, watermark extraction accuracy, and tamper localization performance, offering an efficient and practical solution for trustworthy AIGC image generation. </p>
<blockquote>
<p>ç”Ÿæˆå¼å›¾åƒæ¨¡å‹çš„å¿«é€Ÿå‘å±•ä¸ºäººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹ï¼ˆAIGCï¼‰åˆ›ä½œå¸¦æ¥äº†å·¨å¤§çš„æœºé‡ï¼ŒåŒæ—¶ä¹Ÿå¸¦æ¥äº†ç¡®ä¿å†…å®¹çœŸå®æ€§å’Œç‰ˆæƒå½’å±çš„å…³é”®æŒ‘æˆ˜ã€‚ç°æœ‰çš„å›¾åƒæ°´å°æ–¹æ³•è™½ç„¶éƒ¨åˆ†æœ‰æ•ˆï¼Œä½†é€šå¸¸ä¾èµ–äºåå¤„ç†æˆ–å‚è€ƒå›¾åƒï¼Œå¹¶ä¸”åœ¨å¹³è¡¡ä¿çœŸåº¦ã€é²æ£’æ€§å’Œç¯¡æ”¹å®šä½æ–¹é¢å­˜åœ¨å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†GenPTWï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰çš„åœ¨ç”Ÿæˆå›¾åƒæ°´å°æ¡†æ¶ï¼Œå®ƒå°†æ¥æºè¿½è¸ªå’Œç¯¡æ”¹å®šä½é›†æˆåˆ°ä¸€ä¸ªç»Ÿä¸€çš„æ°´å°è®¾è®¡ä¹‹ä¸­ã€‚å®ƒåœ¨å›¾åƒç”Ÿæˆé˜¶æ®µåµŒå…¥ç»“æ„åŒ–æ°´å°ä¿¡å·ï¼Œä»¥å®ç°ç»Ÿä¸€çš„æ¥æºè¿½è¸ªå’Œç¯¡æ”¹å®šä½ã€‚å¯¹äºæå–ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªé¢‘ç‡åè°ƒè§£ç å™¨ï¼Œä»¥æé«˜å¤æ‚ç¼–è¾‘åœºæ™¯ä¸‹çš„é²æ£’æ€§å’Œå®šä½ç²¾åº¦ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†ä¸€ä¸ªæ¨¡æ‹ŸAIGCç¼–è¾‘çš„å¤±çœŸå±‚ï¼Œä»¥æé«˜é²æ£’æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒGenPTWåœ¨å›¾åƒä¿çœŸåº¦ã€æ°´å°æå–å‡†ç¡®æ€§å’Œç¯¡æ”¹å®šä½æ€§èƒ½ç­‰æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸ºå¯ä¿¡çš„AIGCå›¾åƒç”Ÿæˆæä¾›äº†é«˜æ•ˆå®ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.19567v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ç”Ÿæˆå¼å›¾åƒæ¨¡å‹çš„å¿«é€Ÿå‘å±•ä¸ºäººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹ï¼ˆAIGCï¼‰åˆ›ä½œå¸¦æ¥äº†å·¨å¤§æœºé‡ï¼ŒåŒæ—¶ä¹Ÿå¯¹ç¡®ä¿å†…å®¹çœŸå®æ€§å’Œç‰ˆæƒæ‰€æœ‰æƒæå‡ºäº†ä¸¥å³»æŒ‘æˆ˜ã€‚é’ˆå¯¹ç°æœ‰å›¾åƒæ°´å°æ–¹æ³•åœ¨ä¿çœŸåº¦ã€ç¨³å¥æ€§å’Œç¯¡æ”¹å®šä½æ–¹é¢çš„ä¸è¶³ï¼Œæˆ‘ä»¬æå‡ºäº†GenPTWï¼Œä¸€ä¸ªåŸºäºæ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰çš„ç”Ÿæˆå¼å›¾åƒæ°´å°æ¡†æ¶ã€‚å®ƒé€šè¿‡åµŒå…¥ç»“æ„åŒ–æ°´å°ä¿¡å·ï¼Œå®ç°äº†æº¯æºè¿½è¸ªå’Œç¯¡æ”¹å®šä½çš„ç»Ÿä¸€è®¾è®¡ã€‚å®éªŒè¡¨æ˜ï¼ŒGenPTWåœ¨å›¾åƒä¿çœŸåº¦ã€æ°´å°æå–å‡†ç¡®æ€§å’Œç¯¡æ”¹å®šä½æ€§èƒ½ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸ºå¯ä¿¡çš„AIGCå›¾åƒç”Ÿæˆæä¾›äº†é«˜æ•ˆå®ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆå¼å›¾åƒæ¨¡å‹çš„å‘å±•ä¿ƒè¿›äº†AIGCå†…å®¹çš„åˆ›ä½œï¼ŒåŒæ—¶ä¹Ÿå¸¦æ¥äº†å†…å®¹çœŸå®æ€§å’Œç‰ˆæƒä¿æŠ¤çš„æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰å›¾åƒæ°´å°æ–¹æ³•å­˜åœ¨å±€é™æ€§ï¼Œéš¾ä»¥å¹³è¡¡ä¿çœŸåº¦ã€ç¨³å¥æ€§å’Œç¯¡æ”¹å®šä½ã€‚</li>
<li>GenPTWæ˜¯ä¸€ä¸ªé’ˆå¯¹æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰çš„ç”Ÿæˆå¼å›¾åƒæ°´å°æ¡†æ¶ï¼Œå®ç°äº†æº¯æºè¿½è¸ªå’Œç¯¡æ”¹å®šä½çš„ç»Ÿä¸€è®¾è®¡ã€‚</li>
<li>GenPTWé€šè¿‡åµŒå…¥ç»“æ„åŒ–æ°´å°ä¿¡å·ï¼Œåœ¨å›¾åƒç”Ÿæˆé˜¶æ®µä¿æŠ¤ç‰ˆæƒã€‚</li>
<li>GenPTWä½¿ç”¨é¢‘ç‡åè°ƒè§£ç å™¨æé«˜æ°´å°æå–çš„ç¨³å¥æ€§å’Œå®šä½ç²¾åº¦ã€‚</li>
<li>å¼•å…¥æ¨¡æ‹ŸAIGCç¼–è¾‘çš„å¤±çœŸå±‚ï¼Œå¢å¼ºäº†GenPTWçš„ç¨³å¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.19567">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d6e1c4a9efd6ba45858300a92cfddb07.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-87fcd3f6223252c987fa678351e42c68.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7ba4aa0fd4821e38580e173d94a2fd1e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5ef80ee02c25237ad1f974f8721f74b8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-805b82a10918d3b2a1a95b805bf43d17.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="SynergyAmodal-Deocclude-Anything-with-Text-Control"><a href="#SynergyAmodal-Deocclude-Anything-with-Text-Control" class="headerlink" title="SynergyAmodal: Deocclude Anything with Text Control"></a>SynergyAmodal: Deocclude Anything with Text Control</h2><p><strong>Authors:Xinyang Li, Chengjie Yi, Jiawei Lai, Mingbao Lin, Yansong Qu, Shengchuan Zhang, Liujuan Cao</strong></p>
<p>Image deocclusion (or amodal completion) aims to recover the invisible regions (\ie, shape and appearance) of occluded instances in images. Despite recent advances, the scarcity of high-quality data that balances diversity, plausibility, and fidelity remains a major obstacle. To address this challenge, we identify three critical elements: leveraging in-the-wild image data for diversity, incorporating human expertise for plausibility, and utilizing generative priors for fidelity. We propose SynergyAmodal, a novel framework for co-synthesizing in-the-wild amodal datasets with comprehensive shape and appearance annotations, which integrates these elements through a tripartite data-human-model collaboration. First, we design an occlusion-grounded self-supervised learning algorithm to harness the diversity of in-the-wild image data, fine-tuning an inpainting diffusion model into a partial completion diffusion model. Second, we establish a co-synthesis pipeline to iteratively filter, refine, select, and annotate the initial deocclusion results of the partial completion diffusion model, ensuring plausibility and fidelity through human expert guidance and prior model constraints. This pipeline generates a high-quality paired amodal dataset with extensive category and scale diversity, comprising approximately 16K pairs. Finally, we train a full completion diffusion model on the synthesized dataset, incorporating text prompts as conditioning signals. Extensive experiments demonstrate the effectiveness of our framework in achieving zero-shot generalization and textual controllability. Our code, dataset, and models will be made publicly available at <a target="_blank" rel="noopener" href="https://github.com/imlixinyang/SynergyAmodal">https://github.com/imlixinyang/SynergyAmodal</a>. </p>
<blockquote>
<p>å›¾åƒå»é®æŒ¡ï¼ˆæˆ–æ¨¡æ€å®Œæˆï¼‰æ—¨åœ¨æ¢å¤å›¾åƒä¸­é®æŒ¡å®ä¾‹çš„ä¸å¯è§åŒºåŸŸï¼ˆå³å½¢çŠ¶å’Œå¤–è§‚ï¼‰ã€‚å°½ç®¡æœ€è¿‘å–å¾—äº†è¿›å±•ï¼Œä½†ç¼ºä¹é«˜è´¨é‡æ•°æ®ä»ç„¶æ˜¯ä¸€ä¸ªä¸»è¦éšœç¢ï¼Œè¿™äº›é«˜è´¨é‡æ•°æ®éœ€è¦å¹³è¡¡å¤šæ ·æ€§ã€å¯è¡Œæ€§å’Œä¿çœŸåº¦ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬ç¡®å®šäº†ä¸‰ä¸ªå…³é”®è¦ç´ ï¼šåˆ©ç”¨é‡ç”Ÿå›¾åƒæ•°æ®å¢åŠ å¤šæ ·æ€§ï¼Œç»“åˆäººç±»ä¸“å®¶çŸ¥è¯†å¢åŠ å¯è¡Œæ€§ï¼Œåˆ©ç”¨ç”Ÿæˆå…ˆéªŒå¢åŠ ä¿çœŸåº¦ã€‚æˆ‘ä»¬æå‡ºäº†SynergyAmodalï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œç”¨äºåˆæˆå¸¦æœ‰å…¨é¢å½¢çŠ¶å’Œå¤–è§‚æ³¨é‡Šçš„é‡ç”Ÿæ— æ¨¡æ€æ•°æ®é›†ï¼Œé€šè¿‡ä¸‰æ–¹æ•°æ®-äººç±»-æ¨¡å‹åä½œæ•´åˆè¿™äº›è¦ç´ ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åŸºäºé®æŒ¡çš„è‡ªç›‘ç£å­¦ä¹ ç®—æ³•ï¼Œä»¥åˆ©ç”¨é‡ç”Ÿå›¾åƒæ•°æ®çš„å¤šæ ·æ€§ï¼Œå¯¹å¡«å……æ‰©æ•£æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä½¿å…¶é€‚åº”éƒ¨åˆ†å®Œæˆæ‰©æ•£æ¨¡å‹ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªåˆæˆç®¡é“ï¼Œé€šè¿‡äººç±»ä¸“å®¶çš„æŒ‡å¯¼å’Œå…ˆéªŒæ¨¡å‹çº¦æŸæ¥è¿­ä»£è¿‡æ»¤ã€ç»†åŒ–ã€é€‰æ‹©å’Œæ³¨é‡Šéƒ¨åˆ†å®Œæˆæ‰©æ•£æ¨¡å‹çš„åˆå§‹å»é®æŒ¡ç»“æœï¼Œç¡®ä¿å¯è¡Œæ€§å’Œä¿çœŸåº¦ã€‚è¯¥ç®¡é“ç”Ÿæˆäº†ä¸€ä¸ªé«˜è´¨é‡é…å¯¹çš„æ— æ¨¡æ€æ•°æ®é›†ï¼Œå…·æœ‰å¹¿æ³›çš„ç±»åˆ«å’Œè§„æ¨¡å¤šæ ·æ€§ï¼ŒåŒ…å«å¤§çº¦16Kå¯¹æ•°æ®ã€‚æœ€åï¼Œæˆ‘ä»¬åœ¨åˆæˆæ•°æ®é›†ä¸Šè®­ç»ƒäº†ä¸€ä¸ªå®Œæ•´çš„å®Œæˆæ‰©æ•£æ¨¡å‹ï¼Œå°†æ–‡æœ¬æç¤ºä½œä¸ºæ¡ä»¶ä¿¡å·ã€‚å¤§é‡å®éªŒè¯æ˜äº†æˆ‘ä»¬æ¡†æ¶åœ¨é›¶æ ·æœ¬æ³›åŒ–å’Œæ–‡æœ¬å¯æ§æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ä»£ç ã€æ•°æ®é›†å’Œæ¨¡å‹å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/imlixinyang/SynergyAmodal%E4%B8%8A%E5%85%AC%E5%BC%80%E5%8F%AF%E7%94%A8%E3%80%82">https://github.com/imlixinyang/SynergyAmodalä¸Šå…¬å¼€å¯ç”¨ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.19506v1">PDF</a> 17 pages</p>
<p><strong>Summary</strong><br>    æœ¬æ–‡æ—¨åœ¨è§£å†³å›¾åƒå»é®æŒ¡ï¼ˆæˆ–æ¨¡æ€å®Œæˆï¼‰ä¸­é«˜è´¨é‡æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼Œé€šè¿‡ç»“åˆé‡å¤–å›¾åƒæ•°æ®ã€äººç±»ä¸“å®¶å’Œç”Ÿæˆå…ˆéªŒï¼Œæå‡ºSynergyAmodalæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡ä¸‰æ–¹æ•°æ®-äººç±»-æ¨¡å‹åˆä½œï¼Œå…±åŒåˆæˆé‡å¤–æ¨¡æ€æ•°æ®é›†ï¼Œå¹¶å¸¦æœ‰å…¨é¢çš„å½¢çŠ¶å’Œå¤–è§‚æ³¨é‡Šã€‚è¯¥æ¡†æ¶è®¾è®¡äº†ä¸€ç§åŸºäºé®æŒ¡çš„è‡ªç›‘ç£å­¦ä¹ ç®—æ³•ï¼Œåˆ©ç”¨é‡å¤–å›¾åƒæ•°æ®çš„å¤šæ ·æ€§ï¼Œå°†æ‰©æ•£æ¨¡å‹å¾®è°ƒä¸ºéƒ¨åˆ†å®Œæˆæ‰©æ•£æ¨¡å‹ã€‚å»ºç«‹åˆæˆç®¡é“ï¼Œé€šè¿‡äººç±»ä¸“å®¶æŒ‡å¯¼å’Œå…ˆéªŒæ¨¡å‹çº¦æŸï¼Œå¯¹åˆå§‹å»é®æŒ¡ç»“æœè¿›è¡Œè¿‡æ»¤ã€ç»†åŒ–ã€é€‰æ‹©å’Œæ³¨é‡Šï¼Œç”Ÿæˆé«˜è´¨é‡é…å¯¹æ¨¡æ€æ•°æ®é›†ã€‚åœ¨åˆæˆæ•°æ®é›†ä¸Šè®­ç»ƒå®Œæ•´çš„å®Œæˆæ‰©æ•£æ¨¡å‹ï¼Œç»“åˆæ–‡æœ¬æç¤ºä½œä¸ºæ¡ä»¶ä¿¡å·ï¼Œå®ç°é›¶æ ·æœ¬æ³›åŒ–å’Œæ–‡æœ¬å¯æ§æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒå»é®æŒ¡æ—¨åœ¨æ¢å¤è¢«é®æŒ¡å®ä¾‹çš„éšå½¢åŒºåŸŸï¼ˆå½¢çŠ¶å’Œå¤–è§‚ï¼‰ã€‚</li>
<li>é«˜è´¨é‡æ•°æ®çš„ç¨€ç¼ºæ€§æ˜¯ä¸»è¦æŒ‘æˆ˜ï¼Œéœ€è¦å¹³è¡¡å¤šæ ·æ€§ã€å¯è¡Œæ€§å’Œä¿çœŸåº¦ã€‚</li>
<li>æå‡ºSynergyAmodalæ¡†æ¶ï¼Œç»“åˆé‡å¤–å›¾åƒæ•°æ®ã€äººç±»ä¸“å®¶å’Œç”Ÿæˆå…ˆéªŒã€‚</li>
<li>é€šè¿‡ä¸‰æ–¹æ•°æ®-äººç±»-æ¨¡å‹åˆä½œï¼Œå…±åŒåˆæˆå¸¦æœ‰å…¨é¢æ³¨é‡Šçš„æ¨¡æ€æ•°æ®é›†ã€‚</li>
<li>åˆ©ç”¨åŸºäºé®æŒ¡çš„è‡ªç›‘ç£å­¦ä¹ ç®—æ³•ï¼Œå°†æ‰©æ•£æ¨¡å‹å¾®è°ƒä¸ºéƒ¨åˆ†å®Œæˆæ‰©æ•£æ¨¡å‹ã€‚</li>
<li>å»ºç«‹åˆæˆç®¡é“ï¼Œç¡®ä¿å»é®æŒ¡ç»“æœçš„å¯è¡Œæ€§å’Œä¿çœŸåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.19506">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a48df0491bc24050397d36ca01f62aeb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c42655d821b64d73ec811fb875668bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c5638dc70e29c781f0307d0938f2d2d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-383f5fecdf130f265fc61d7521a1a48f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8faca007efe8f236c389168b7e3524ca.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-039df3ed64c5efe5ffb9da26d48c012b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-57ce3c25f673a2630ca2ebdd47569d89.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e1d73f70de288432d16952fe30f57cb2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-902596f5c437257841f93c682efc8377.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Generative-AI-for-Character-Animation-A-Comprehensive-Survey-of-Techniques-Applications-and-Future-Directions"><a href="#Generative-AI-for-Character-Animation-A-Comprehensive-Survey-of-Techniques-Applications-and-Future-Directions" class="headerlink" title="Generative AI for Character Animation: A Comprehensive Survey of   Techniques, Applications, and Future Directions"></a>Generative AI for Character Animation: A Comprehensive Survey of   Techniques, Applications, and Future Directions</h2><p><strong>Authors:Mohammad Mahdi Abootorabi, Omid Ghahroodi, Pardis Sadat Zahraei, Hossein Behzadasl, Alireza Mirrokni, Mobina Salimipanah, Arash Rasouli, Bahar Behzadipour, Sara Azarnoush, Benyamin Maleki, Erfan Sadraiye, Kiarash Kiani Feriz, Mahdi Teymouri Nahad, Ali Moghadasi, Abolfazl Eshagh Abianeh, Nizi Nazar, Hamid R. Rabiee, Mahdieh Soleymani Baghshah, Meisam Ahmadi, Ehsaneddin Asgari</strong></p>
<p>Generative AI is reshaping art, gaming, and most notably animation. Recent breakthroughs in foundation and diffusion models have reduced the time and cost of producing animated content. Characters are central animation components, involving motion, emotions, gestures, and facial expressions. The pace and breadth of advances in recent months make it difficult to maintain a coherent view of the field, motivating the need for an integrative review. Unlike earlier overviews that treat avatars, gestures, or facial animation in isolation, this survey offers a single, comprehensive perspective on all the main generative AI applications for character animation. We begin by examining the state-of-the-art in facial animation, expression rendering, image synthesis, avatar creation, gesture modeling, motion synthesis, object generation, and texture synthesis. We highlight leading research, practical deployments, commonly used datasets, and emerging trends for each area. To support newcomers, we also provide a comprehensive background section that introduces foundational models and evaluation metrics, equipping readers with the knowledge needed to enter the field. We discuss open challenges and map future research directions, providing a roadmap to advance AI-driven character-animation technologies. This survey is intended as a resource for researchers and developers entering the field of generative AI animation or adjacent fields. Resources are available at: <a target="_blank" rel="noopener" href="https://github.com/llm-lab-org/Generative-AI-for-Character-Animation-Survey">https://github.com/llm-lab-org/Generative-AI-for-Character-Animation-Survey</a>. </p>
<blockquote>
<p>ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ­£åœ¨é‡å¡‘è‰ºæœ¯ã€æ¸¸æˆï¼Œå°¤å…¶æ˜¯åŠ¨ç”»äº§ä¸šã€‚è¿‘æœŸåœ¨åŸºç¡€å’Œæ‰©æ•£æ¨¡å‹æ–¹é¢çš„çªç ´å‡å°‘äº†åŠ¨ç”»å†…å®¹åˆ¶ä½œçš„æ—¶é—´å’Œæˆæœ¬ã€‚è§’è‰²æ˜¯åŠ¨ç”»çš„æ ¸å¿ƒç»„æˆéƒ¨åˆ†ï¼Œæ¶‰åŠåŠ¨ä½œã€æƒ…æ„Ÿã€æ‰‹åŠ¿å’Œé¢éƒ¨è¡¨æƒ…ã€‚è¿‘å‡ ä¸ªæœˆæ¥ï¼ŒæŠ€æœ¯çš„è¿›æ­¥é€Ÿåº¦ä¹‹å¿«ã€èŒƒå›´ä¹‹å¹¿ï¼Œä½¿å¾—å¾ˆéš¾å¯¹è¯¥é¢†åŸŸä¿æŒè¿è´¯çš„è§†é‡ï¼Œè¿™æ¿€å‘äº†éœ€è¦è¿›è¡Œç»¼åˆæ€§å›é¡¾çš„éœ€è¦ã€‚ä¸åŒäºä»¥å‰åªé’ˆå¯¹åŒ–èº«ã€æ‰‹åŠ¿æˆ–é¢éƒ¨åŠ¨ç”»çš„æ¦‚è¿°ï¼Œè¿™ç¯‡ç»¼è¿°æä¾›äº†å…³äºè§’è‰²åŠ¨ç”»æ‰€æœ‰ä¸»è¦ç”Ÿæˆå¼äººå·¥æ™ºèƒ½åº”ç”¨çš„å•ä¸€ã€å…¨é¢çš„è§†è§’ã€‚æˆ‘ä»¬é¦–å…ˆè€ƒå¯Ÿé¢éƒ¨åŠ¨ç”»ã€è¡¨æƒ…æ¸²æŸ“ã€å›¾åƒåˆæˆã€åŒ–èº«åˆ›å»ºã€æ‰‹åŠ¿å»ºæ¨¡ã€åŠ¨ä½œåˆæˆã€å¯¹è±¡ç”Ÿæˆå’Œçº¹ç†åˆæˆçš„æœ€æ–°çŠ¶æ€ã€‚æˆ‘ä»¬å¼ºè°ƒäº†å„é¢†åŸŸçš„ä¸»è¦ç ”ç©¶ã€å®é™…éƒ¨ç½²ã€å¸¸ç”¨æ•°æ®é›†å’Œæ–°å…´è¶‹åŠ¿ã€‚ä¸ºäº†æ”¯æŒæ–°æ‰‹ï¼Œæˆ‘ä»¬è¿˜æä¾›äº†ä¸€ä¸ªå…¨é¢çš„èƒŒæ™¯éƒ¨åˆ†ï¼Œä»‹ç»åŸºç¡€æ¨¡å‹å’Œè¯„ä¼°æŒ‡æ ‡ï¼Œä¸ºè¯»è€…æä¾›è¿›å…¥è¯¥é¢†åŸŸæ‰€éœ€çš„çŸ¥è¯†ã€‚æˆ‘ä»¬è®¨è®ºäº†å¼€æ”¾æŒ‘æˆ˜å¹¶ç»˜åˆ¶äº†æœªæ¥ç ”ç©¶çš„æ–¹å‘å›¾ï¼Œä¸ºæ¨è¿›äººå·¥æ™ºèƒ½é©±åŠ¨çš„è§’è‰²åŠ¨ç”»æŠ€æœ¯æä¾›è·¯çº¿å›¾ã€‚æœ¬ç»¼è¿°æ—¨åœ¨ä¸ºè¿›å…¥ç”Ÿæˆå¼äººå·¥æ™ºèƒ½åŠ¨ç”»æˆ–å…¶ç›¸å…³é¢†åŸŸçš„ç ”ç©¶è€…å’Œå¼€å‘è€…æä¾›å‚è€ƒã€‚ç›¸å…³èµ„æºå¯è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/llm-lab-org/Generative-AI-for-Character-Animation-Survey">é“¾æ¥åœ°å€</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.19056v1">PDF</a> 50 main pages, 30 pages appendix, 21 figures, 8 tables, GitHub   Repository:   <a target="_blank" rel="noopener" href="https://github.com/llm-lab-org/Generative-AI-for-Character-Animation-Survey">https://github.com/llm-lab-org/Generative-AI-for-Character-Animation-Survey</a></p>
<p><strong>Summary</strong><br>     ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ­£åœ¨é‡å¡‘è‰ºæœ¯ã€æ¸¸æˆå’ŒåŠ¨ç”»é¢†åŸŸã€‚è¿‘æœŸåœ¨åŸºç¡€æ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹æ–¹é¢çš„çªç ´ï¼Œå‡å°‘äº†åŠ¨ç”»å†…å®¹åˆ¶ä½œçš„æ—¶é—´å’Œæˆæœ¬ã€‚è§’è‰²åŠ¨ç”»æ¶‰åŠåŠ¨ä½œã€æƒ…æ„Ÿã€æ‰‹åŠ¿å’Œé¢éƒ¨è¡¨æƒ…ç­‰æ ¸å¿ƒè¦ç´ ã€‚æœ¬æ–‡ç»¼è¿°äº†é¢éƒ¨åŠ¨ç”»ã€è¡¨æƒ…æ¸²æŸ“ã€å›¾åƒåˆæˆã€è§’è‰²åˆ›å»ºã€æ‰‹åŠ¿å»ºæ¨¡ã€åŠ¨ä½œåˆæˆã€å¯¹è±¡ç”Ÿæˆå’Œçº¹ç†åˆæˆç­‰é¢†åŸŸçš„æœ€æ–°è¿›å±•ï¼Œå¼ºè°ƒäº†å‰æ²¿ç ”ç©¶ã€å®é™…åº”ç”¨éƒ¨ç½²ã€å¸¸ç”¨æ•°æ®é›†å’Œæœªæ¥è¶‹åŠ¿ã€‚æœ¬æ–‡æ—¨åœ¨ä¸ºè¿›å…¥ç”Ÿæˆå¼åŠ¨ç”»æˆ–ç›¸å…³é¢†åŸŸçš„ç ”ç©¶è€…å’Œå¼€å‘è€…æä¾›èµ„æºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜è‰ºæœ¯ã€æ¸¸æˆå’ŒåŠ¨ç”»é¢†åŸŸã€‚</li>
<li>çªç ´æ€§çš„åŸºç¡€æ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹å‡å°‘äº†åŠ¨ç”»åˆ¶ä½œçš„æ—¶é—´å’Œæˆæœ¬ã€‚</li>
<li>è§’è‰²åŠ¨ç”»åŒ…æ‹¬åŠ¨ä½œã€æƒ…æ„Ÿã€æ‰‹åŠ¿å’Œé¢éƒ¨è¡¨æƒ…ç­‰æ ¸å¿ƒè¦ç´ ã€‚</li>
<li>ç»¼è¿°å…¨é¢æ¶µç›–äº†é¢éƒ¨åŠ¨ç”»ã€è¡¨æƒ…æ¸²æŸ“ç­‰å¤šä¸ªä¸»è¦ç”Ÿæˆå¼AIåº”ç”¨é¢†åŸŸã€‚</li>
<li>ç»¼è¿°å¼ºè°ƒäº†å‰æ²¿ç ”ç©¶ã€å®é™…åº”ç”¨éƒ¨ç½²å’Œå¸¸ç”¨æ•°æ®é›†ã€‚</li>
<li>ç»¼è¿°æä¾›äº†å¼€æ”¾æŒ‘æˆ˜å’Œæœªæ¥ç ”ç©¶æ–¹å‘çš„æ¢è®¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.19056">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-3fdf169a6d41d71f39b9183889695bf7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7a509eac1b0e6ba63a44c3e9d08e3b4b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-72f895e27a7e9b6a2bc0e5efeda4abaa.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="REED-VAE-RE-Encode-Decode-Training-for-Iterative-Image-Editing-with-Diffusion-Models"><a href="#REED-VAE-RE-Encode-Decode-Training-for-Iterative-Image-Editing-with-Diffusion-Models" class="headerlink" title="REED-VAE: RE-Encode Decode Training for Iterative Image Editing with   Diffusion Models"></a>REED-VAE: RE-Encode Decode Training for Iterative Image Editing with   Diffusion Models</h2><p><strong>Authors:Gal Almog, Ariel Shamir, Ohad Fried</strong></p>
<p>While latent diffusion models achieve impressive image editing results, their application to iterative editing of the same image is severely restricted. When trying to apply consecutive edit operations using current models, they accumulate artifacts and noise due to repeated transitions between pixel and latent spaces. Some methods have attempted to address this limitation by performing the entire edit chain within the latent space, sacrificing flexibility by supporting only a limited, predetermined set of diffusion editing operations. We present a RE-encode decode (REED) training scheme for variational autoencoders (VAEs), which promotes image quality preservation even after many iterations. Our work enables multi-method iterative image editing: users can perform a variety of iterative edit operations, with each operation building on the output of the previous one using both diffusion-based operations and conventional editing techniques. We demonstrate the advantage of REED-VAE across a range of image editing scenarios, including text-based and mask-based editing frameworks. In addition, we show how REED-VAE enhances the overall editability of images, increasing the likelihood of successful and precise edit operations. We hope that this work will serve as a benchmark for the newly introduced task of multi-method image editing. Our code and models will be available at <a target="_blank" rel="noopener" href="https://github.com/galmog/REED-VAE">https://github.com/galmog/REED-VAE</a> </p>
<blockquote>
<p>æ½œåœ¨æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç¼–è¾‘æ–¹é¢å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„æˆæœï¼Œä½†å®ƒä»¬åœ¨ç›¸åŒå›¾åƒçš„è¿­ä»£ç¼–è¾‘åº”ç”¨ä¸Šå—åˆ°ä¸¥é‡é™åˆ¶ã€‚å½“å°è¯•ä½¿ç”¨å½“å‰æ¨¡å‹è¿›è¡Œè¿ç»­ç¼–è¾‘æ“ä½œæ—¶ï¼Œç”±äºåƒç´ å’Œæ½œåœ¨ç©ºé—´ä¹‹é—´çš„é‡å¤è½¬æ¢ï¼Œå®ƒä»¬ä¼šç§¯ç´¯ä¼ªå½±å’Œå™ªå£°ã€‚ä¸€äº›æ–¹æ³•è¯•å›¾é€šè¿‡ä»…åœ¨æ½œåœ¨ç©ºé—´å†…æ‰§è¡Œæ•´ä¸ªç¼–è¾‘é“¾æ¥è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œä½†åªæ”¯æŒæœ‰é™ã€é¢„å…ˆç¡®å®šçš„æ‰©æ•£ç¼–è¾‘æ“ä½œï¼Œç‰ºç‰²äº†çµæ´»æ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§é’ˆå¯¹å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰çš„RE-encode decodeï¼ˆREEDï¼‰è®­ç»ƒæ–¹æ¡ˆï¼Œå³ä½¿åœ¨å¤šæ¬¡è¿­ä»£åä¹Ÿèƒ½ä¿ƒè¿›å›¾åƒè´¨é‡ä¿ç•™ã€‚æˆ‘ä»¬çš„å·¥ä½œå®ç°äº†å¤šæ–¹æ³•è¿­ä»£å›¾åƒç¼–è¾‘ï¼šç”¨æˆ·å¯ä»¥è¿›è¡Œå¤šç§è¿­ä»£ç¼–è¾‘æ“ä½œï¼Œæ¯ä¸ªæ“ä½œéƒ½ä»¥å‰ä¸€ä¸ªæ“ä½œçš„è¾“å‡ºä¸ºåŸºç¡€ï¼Œä½¿ç”¨åŸºäºæ‰©æ•£çš„æ“ä½œå’Œå¸¸è§„ç¼–è¾‘æŠ€æœ¯ã€‚æˆ‘ä»¬å±•ç¤ºäº†REED-VAEåœ¨å„ç§å›¾åƒç¼–è¾‘åœºæ™¯ä¸­çš„ä¼˜åŠ¿ï¼ŒåŒ…æ‹¬åŸºäºæ–‡æœ¬å’ŒåŸºäºæ©ç çš„ç¼–è¾‘æ¡†æ¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å±•ç¤ºäº†REED-VAEå¦‚ä½•å¢å¼ºå›¾åƒçš„æ•´ä½“å¯ç¼–è¾‘æ€§ï¼Œæé«˜æˆåŠŸå’Œç²¾ç¡®ç¼–è¾‘æ“ä½œçš„å¯èƒ½æ€§ã€‚æˆ‘ä»¬å¸Œæœ›è¿™é¡¹å·¥ä½œèƒ½ä¸ºæ–°å¼•å…¥çš„å¤šæ–¹æ³•å›¾åƒç¼–è¾‘ä»»åŠ¡æä¾›åŸºå‡†ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/galmog/REED-VAE%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/galmog/REED-VAEä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.18989v1">PDF</a> Accepted to Eurographics 2025. Project page:   <a target="_blank" rel="noopener" href="https://reed-vae.github.io/">https://reed-vae.github.io/</a></p>
<p><strong>Summary</strong>ï¼š<br>æ½œåœ¨æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç¼–è¾‘ä¸­è¡¨ç°å“è¶Šï¼Œä½†åœ¨åŒä¸€å›¾åƒçš„è¿­ä»£ç¼–è¾‘æ–¹é¢å­˜åœ¨å±€é™ã€‚å½“å‰æ¨¡å‹åœ¨è¿ç»­ç¼–è¾‘æ“ä½œæ—¶ä¼šäº§ç”Ÿä¼ªå½±å’Œå™ªå£°ã€‚æˆ‘ä»¬æå‡ºä¸€ç§é’ˆå¯¹å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ï¼ˆVAEï¼‰çš„RE-encode decodeï¼ˆREEDï¼‰è®­ç»ƒæ–¹æ¡ˆï¼Œä¿ƒè¿›å›¾åƒè´¨é‡åœ¨å¤šæ¬¡è¿­ä»£åçš„ä¿ç•™ï¼Œå®ç°å¤šæ–¹æ³•è¿­ä»£å›¾åƒç¼–è¾‘ã€‚æ­¤æ–¹æ¡ˆä½¿ç”¨æˆ·å¯è¿ç”¨å¤šç§è¿­ä»£ç¼–è¾‘æ“ä½œï¼Œæ¯æ¬¡æ“ä½œåŸºäºå‰ä¸€æ¬¡è¾“å‡ºï¼Œæ”¯æŒæ‰©æ•£æ“ä½œå’Œä¼ ç»Ÿç¼–è¾‘æŠ€æœ¯ã€‚REED-VAEåœ¨å¤šç§å›¾åƒç¼–è¾‘åœºæ™¯ä¸­è¡¨ç°ä¼˜è¶Šï¼Œå¦‚æ–‡æœ¬å’Œé®ç½©ç¼–è¾‘æ¡†æ¶ã€‚å®ƒæé«˜äº†å›¾åƒçš„æ•´ä½“å¯ç¼–è¾‘æ€§ï¼Œå¢åŠ ç²¾ç¡®ç¼–è¾‘æ“ä½œçš„æˆåŠŸç‡ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ½œåœ¨æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç¼–è¾‘ä¸­è¡¨ç°ä¼˜ç§€ï¼Œä½†è¿­ä»£ç¼–è¾‘å­˜åœ¨å±€é™ã€‚</li>
<li>å½“å‰æ¨¡å‹åœ¨è¿ç»­ç¼–è¾‘æ“ä½œæ—¶ä¼šäº§ç”Ÿä¼ªå½±å’Œå™ªå£°ã€‚</li>
<li>æå‡ºRE-encode decodeï¼ˆREEDï¼‰è®­ç»ƒæ–¹æ¡ˆï¼Œé€‚ç”¨äºå˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ï¼ˆVAEï¼‰ã€‚</li>
<li>REEDæ–¹æ¡ˆä¿ƒè¿›å›¾åƒè´¨é‡åœ¨å¤šæ¬¡è¿­ä»£åçš„ä¿ç•™ã€‚</li>
<li>å®ç°å¤šæ–¹æ³•è¿­ä»£å›¾åƒç¼–è¾‘ï¼Œæ”¯æŒæ‰©æ•£æ“ä½œå’Œä¼ ç»Ÿç¼–è¾‘æŠ€æœ¯ã€‚</li>
<li>REED-VAEåœ¨å¤šç§å›¾åƒç¼–è¾‘åœºæ™¯ä¸­è¡¨ç°ä¼˜è¶Šã€‚</li>
<li>REED-VAEæé«˜äº†å›¾åƒçš„æ•´ä½“å¯ç¼–è¾‘æ€§ï¼Œå¢åŠ ç²¾ç¡®ç¼–è¾‘æ“ä½œçš„æˆåŠŸç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.18989">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-5849848f378ad9681b064d2140064878.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-56defafe7f5c8352e4df02d511653217.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8384aa374006b5b75846ad792f2a0ade.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-462be26ce3048ea34d6c8595cc9e4850.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0634cb8e92fe1b3b3dff692a5d0198a.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Dream-Box-Object-wise-Outlier-Generation-for-Out-of-Distribution-Detection"><a href="#Dream-Box-Object-wise-Outlier-Generation-for-Out-of-Distribution-Detection" class="headerlink" title="Dream-Box: Object-wise Outlier Generation for Out-of-Distribution   Detection"></a>Dream-Box: Object-wise Outlier Generation for Out-of-Distribution   Detection</h2><p><strong>Authors:Brian K. S. Isaac-Medina, Toby P. Breckon</strong></p>
<p>Deep neural networks have demonstrated great generalization capabilities for tasks whose training and test sets are drawn from the same distribution. Nevertheless, out-of-distribution (OOD) detection remains a challenging task that has received significant attention in recent years. Specifically, OOD detection refers to the detection of instances that do not belong to the training distribution, while still having good performance on the in-distribution task (e.g., classification or object detection). Recent work has focused on generating synthetic outliers and using them to train an outlier detector, generally achieving improved OOD detection than traditional OOD methods. In this regard, outliers can be generated either in feature or pixel space. Feature space driven methods have shown strong performance on both the classification and object detection tasks, at the expense that the visualization of training outliers remains unknown, making further analysis on OOD failure modes challenging. On the other hand, pixel space outlier generation techniques enabled by diffusion models have been used for image classification using, providing improved OOD detection performance and outlier visualization, although their adaption to the object detection task is as yet unexplored. We therefore introduce Dream-Box, a method that provides a link to object-wise outlier generation in the pixel space for OOD detection. Specifically, we use diffusion models to generate object-wise outliers that are used to train an object detector for an in-distribution task and OOD detection. Our method achieves comparable performance to previous traditional methods while being the first technique to provide concrete visualization of generated OOD objects. </p>
<blockquote>
<p>æ·±åº¦ç¥ç»ç½‘ç»œå¯¹äºè®­ç»ƒå’Œæµ‹è¯•é›†æ¥è‡ªåŒä¸€åˆ†å¸ƒçš„ä»»åŠ¡è¡¨ç°å‡ºäº†å¾ˆå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç¦»ç¾¤ç‚¹æ£€æµ‹ä»ç„¶æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œè¿‘å¹´æ¥å¤‡å—å…³æ³¨ã€‚å…·ä½“æ¥è¯´ï¼Œç¦»ç¾¤ç‚¹æ£€æµ‹æ˜¯æŒ‡æ£€æµ‹ä¸å±äºè®­ç»ƒåˆ†å¸ƒä½†å±äºåŒä¸€åˆ†å¸ƒä»»åŠ¡ï¼ˆä¾‹å¦‚åˆ†ç±»æˆ–å¯¹è±¡æ£€æµ‹ï¼‰çš„å®ä¾‹ï¼Œè¿‘å¹´æ¥è®¸å¤šç ”ç©¶å¼€å§‹é›†ä¸­åœ¨ç”Ÿæˆåˆæˆå¼‚å¸¸å€¼å’Œä½¿ç”¨å®ƒä»¬æ¥è®­ç»ƒå¼‚å¸¸æ£€æµ‹å™¨ï¼Œé€šå¸¸æ¯”åœ¨å¸¸è§„ç¦»ç¾¤ç‚¹æ£€æµ‹æ–¹é¢ä¼ ç»Ÿæ–¹æ³•è¾¾åˆ°æ›´å¥½çš„æ•ˆæœã€‚åœ¨è¿™æ–¹é¢ï¼Œå¼‚å¸¸å€¼å¯ä»¥åœ¨ç‰¹å¾ç©ºé—´æˆ–åƒç´ ç©ºé—´ä¸­ç”Ÿæˆã€‚ç‰¹å¾ç©ºé—´é©±åŠ¨çš„æ–¹æ³•åœ¨åˆ†ç±»å’Œå¯¹è±¡æ£€æµ‹ä»»åŠ¡ä¸­éƒ½æ˜¾ç¤ºå‡ºå¼ºå¤§æ€§èƒ½ï¼Œä½†ä»¥æ— æ³•å¯è§†åŒ–è®­ç»ƒå¼‚å¸¸å€¼ä¸ºä»£ä»·ï¼Œè¿™ä½¿å¾—å¯¹ç¦»ç¾¤ç‚¹å¤±æ•ˆæ¨¡å¼çš„è¿›ä¸€æ­¥åˆ†æå…·æœ‰æŒ‘æˆ˜æ€§ã€‚å¦ä¸€æ–¹é¢ï¼Œç”±æ‰©æ•£æ¨¡å‹æ”¯æŒçš„åƒç´ ç©ºé—´å¼‚å¸¸å€¼ç”ŸæˆæŠ€æœ¯å·²ç”¨äºå›¾åƒåˆ†ç±»ä¸­ï¼Œæä¾›æ”¹è¿›çš„å¼‚å¸¸å€¼æ£€æµ‹æ€§èƒ½å’Œå¼‚å¸¸å€¼å¯è§†åŒ–ï¼Œå°½ç®¡å®ƒä»¬åœ¨å¯¹è±¡æ£€æµ‹ä»»åŠ¡ä¸Šçš„é€‚åº”æ€§å°šæœªæ¢ç´¢ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†Dream-Boxæ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä¸ºç¦»ç¾¤ç‚¹æ£€æµ‹åœ¨åƒç´ ç©ºé—´ä¸­æä¾›äº†ç›®æ ‡ç›¸å…³çš„å¼‚å¸¸å€¼ç”Ÿæˆè”ç³»ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä½¿ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆç›®æ ‡ç›¸å…³çš„å¼‚å¸¸å€¼ï¼Œç”¨äºè®­ç»ƒç”¨äºå†…éƒ¨åˆ†å¸ƒä»»åŠ¡å’Œç¦»ç¾¤ç‚¹æ£€æµ‹çš„å¯¹è±¡æ£€æµ‹å™¨ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸å…ˆå‰çš„æ–¹æ³•ç›¸æ¯”è¾¾åˆ°äº†ç›¸å½“çš„æ€§èƒ½æ°´å¹³ï¼Œå¹¶é¦–æ¬¡æä¾›äº†ç”Ÿæˆå¯¹è±¡å¤–ç»Ÿè®¡çš„å…·ä½“å¯è§†åŒ–å·¥å…·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.18746v1">PDF</a> 9 pages, 6 figures, 2 tables, LatinX in AI CVPR 2025 Workshop</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ·±åº¦ç¥ç»ç½‘ç»œåœ¨ä»»åŠ¡ä¸­çš„ä¼˜å¼‚è¡¨ç°ï¼Œå°¤å…¶æ˜¯åœ¨è®­ç»ƒå’Œæµ‹è¯•é›†æ¥è‡ªåŒä¸€åˆ†å¸ƒæ—¶ã€‚ç„¶è€Œï¼Œå¯¹äºä¸å±äºè®­ç»ƒåˆ†å¸ƒä½†ä»åœ¨åˆ†å¸ƒå†…çš„ä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½çš„å®ä¾‹ï¼ˆå³ç¦»ç¾¤å€¼æ£€æµ‹ï¼‰ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚è¿‘æœŸç ”ç©¶é€šè¿‡ç”Ÿæˆåˆæˆå¼‚å¸¸å€¼æ¥è®­ç»ƒå¼‚å¸¸æ£€æµ‹å™¨ï¼Œå¹¶å®ç°äº†æ¯”ä¼ ç»Ÿå¼‚å¸¸æ£€æµ‹æ–¹æ³•æ›´å¥½çš„ç¦»ç¾¤å€¼æ£€æµ‹æ€§èƒ½ã€‚é’ˆå¯¹ç‰¹å¾ç©ºé—´å’Œåƒç´ ç©ºé—´ç”Ÿæˆç¦»ç¾¤å€¼çš„æ–¹æ³•ï¼Œç‰¹å¾ç©ºé—´é©±åŠ¨çš„æ–¹æ³•åœ¨åˆ†ç±»å’Œå¯¹è±¡æ£€æµ‹ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†æ— æ³•å¯è§†åŒ–è®­ç»ƒä¸­çš„å¼‚å¸¸å€¼ï¼Œä½¿å¾—å¯¹ç¦»ç¾¤å€¼æ£€æµ‹å¤±è´¥æ¨¡å¼çš„è¿›ä¸€æ­¥åˆ†æå˜å¾—å›°éš¾ã€‚è€Œæ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„åƒç´ ç©ºé—´å¼‚å¸¸æ£€æµ‹æŠ€æœ¯å·²ç»ç”¨äºå›¾åƒåˆ†ç±»ï¼Œä¸ºå¯¹è±¡æ£€æµ‹ä»»åŠ¡çš„ç¦»ç¾¤å€¼æ£€æµ‹æä¾›äº†æ–°çš„å¯èƒ½ã€‚æœ¬ç ”ç©¶ä»‹ç»äº†ä¸€ç§åä¸ºDream-Boxçš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆå¯¹è±¡çº§å¼‚å¸¸å€¼ï¼Œç”¨äºè®­ç»ƒå¯¹è±¡æ£€æµ‹å™¨è¿›è¡Œåˆ†å¸ƒå†…ä»»åŠ¡å’Œç¦»ç¾¤å€¼æ£€æµ‹ã€‚è¯¥æ–¹æ³•å®ç°äº†ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸å½“çš„æ€§èƒ½ï¼Œå¹¶ä¸”æ˜¯é¦–ä¸ªæä¾›ç”Ÿæˆç¦»ç¾¤å¯¹è±¡çš„å¯è§†åŒ–æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦ç¥ç»ç½‘ç»œåœ¨è®­ç»ƒå’Œæµ‹è¯•é›†æ¥è‡ªåŒä¸€åˆ†å¸ƒçš„ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ç¦»ç¾¤å€¼æ£€æµ‹ï¼ˆOODï¼‰æ˜¯ä¸€ä¸ªæŒ‘æˆ˜æ€§é—®é¢˜ï¼ŒæŒ‡çš„æ˜¯æ£€æµ‹ä¸å±äºè®­ç»ƒåˆ†å¸ƒä½†ä»å±äºç‰¹å®šä»»åŠ¡åˆ†å¸ƒå®ä¾‹çš„æ£€æµ‹é—®é¢˜ã€‚</li>
<li>ç”Ÿæˆåˆæˆå¼‚å¸¸å€¼çš„è¿‘æœŸæ–¹æ³•åœ¨åˆ†ç±»å’Œå¯¹è±¡æ£€æµ‹ä»»åŠ¡ä¸­è¡¨ç°å‡ºä¼˜äºä¼ ç»ŸOODæ–¹æ³•çš„æ•ˆæœã€‚è¿™äº›æ–¹æ³•åœ¨ç‰¹å¾ç©ºé—´å’Œåƒç´ ç©ºé—´ä¸­ç”Ÿæˆå¼‚å¸¸å€¼ï¼Œä¸ºè¿™äº›å¼‚å¸¸å€¼çš„å¯è§†åŒ–æä¾›äº†æ–°çš„é€”å¾„ã€‚ç„¶è€Œç‰¹å¾ç©ºé—´æ–¹æ³•çš„å¯è§†åŒ–æ•ˆæœæœ‰å¾…è¿›ä¸€æ­¥éªŒè¯ï¼Œåƒç´ ç©ºé—´æ–¹æ³•åˆ™å…·æœ‰å¯è§†åŒ–ä¼˜åŠ¿ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒåˆ†ç±»ä¸­ç”¨äºç”Ÿæˆåƒç´ ç©ºé—´çš„å¼‚å¸¸å€¼æ£€æµ‹å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å¯¹å¯¹è±¡æ£€æµ‹ä»»åŠ¡çš„é€‚åº”ä»æ˜¯ä¸€ä¸ªæ–°çš„æŒ‘æˆ˜ã€‚ä¸ºæ­¤æå‡ºäº†ä¸€ç§åä¸ºDream-Boxçš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆå¯¹è±¡çº§å¼‚å¸¸å€¼ç”¨äºè®­ç»ƒå¯¹è±¡æ£€æµ‹å™¨è¿›è¡ŒOODæ£€æµ‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.18746">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bf39517afe3b8593a8bd974c5be52a3c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d6429e7cdd663807b6630e216deaf7b7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4204bc3471779fc4be8cb1b24dce7eac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5178da29efb4cde707e5b6246edbecad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a1ab2816d72badbb022538b3de7b605.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5685a381d81943931ed94d123f2f6301.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-174d4978980e924d5f8828f8494f0158.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f19718890c3f7316811e08d496411c6f.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="VistaDepth-Frequency-Modulation-With-Bias-Reweighting-For-Enhanced-Long-Range-Depth-Estimation"><a href="#VistaDepth-Frequency-Modulation-With-Bias-Reweighting-For-Enhanced-Long-Range-Depth-Estimation" class="headerlink" title="VistaDepth: Frequency Modulation With Bias Reweighting For Enhanced   Long-Range Depth Estimation"></a>VistaDepth: Frequency Modulation With Bias Reweighting For Enhanced   Long-Range Depth Estimation</h2><p><strong>Authors:Mingxia Zhan, Li Zhang, Xiaomeng Chu, Beibei Wang</strong></p>
<p>Monocular depth estimation (MDE) aims to predict per-pixel depth values from a single RGB image. Recent advancements have positioned diffusion models as effective MDE tools by framing the challenge as a conditional image generation task. Despite their progress, these methods often struggle with accurately reconstructing distant depths, due largely to the imbalanced distribution of depth values and an over-reliance on spatial-domain features. To overcome these limitations, we introduce VistaDepth, a novel framework that integrates adaptive frequency-domain feature enhancements with an adaptive weight-balancing mechanism into the diffusion process. Central to our approach is the Latent Frequency Modulation (LFM) module, which dynamically refines spectral responses in the latent feature space, thereby improving the preservation of structural details and reducing noisy artifacts. Furthermore, we implement an adaptive weighting strategy that modulates the diffusion loss in real-time, enhancing the modelâ€™s sensitivity towards distant depth reconstruction. These innovations collectively result in superior depth perception performance across both distance and detail. Experimental evaluations confirm that VistaDepth achieves state-of-the-art performance among diffusion-based MDE techniques, particularly excelling in the accurate reconstruction of distant regions. </p>
<blockquote>
<p>å•çœ¼æ·±åº¦ä¼°è®¡ï¼ˆMDEï¼‰æ—¨åœ¨ä»å•ä¸€RGBå›¾åƒé¢„æµ‹æ¯ä¸ªåƒç´ çš„æ·±åº¦å€¼ã€‚æœ€è¿‘çš„è¿›å±•å°†æ‰©æ•£æ¨¡å‹å®šä½ä¸ºæœ‰æ•ˆçš„MDEå·¥å…·ï¼Œå°†æŒ‘æˆ˜è§†ä¸ºæ¡ä»¶å›¾åƒç”Ÿæˆä»»åŠ¡ã€‚å°½ç®¡æœ‰æ‰€è¿›å±•ï¼Œè¿™äº›æ–¹æ³•åœ¨å‡†ç¡®é‡å»ºè¿œè·ç¦»æ·±åº¦æ–¹é¢å¾€å¾€é‡åˆ°å›°éš¾ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºæ·±åº¦å€¼åˆ†å¸ƒä¸å¹³è¡¡ä»¥åŠè¿‡äºä¾èµ–ç©ºé—´åŸŸç‰¹å¾ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†VistaDepthï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œå°†è‡ªé€‚åº”é¢‘åŸŸç‰¹å¾å¢å¼ºå’Œè‡ªé€‚åº”æƒé‡å¹³è¡¡æœºåˆ¶é›†æˆåˆ°æ‰©æ•£è¿‡ç¨‹ä¸­ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„æ ¸å¿ƒæ˜¯æ½œåœ¨é¢‘ç‡è°ƒåˆ¶ï¼ˆLFMï¼‰æ¨¡å—ï¼Œå®ƒåŠ¨æ€åœ°ä¼˜åŒ–æ½œåœ¨ç‰¹å¾ç©ºé—´ä¸­çš„å…‰è°±å“åº”ï¼Œä»è€Œæé«˜ç»“æ„ç»†èŠ‚çš„ä¿ç•™ï¼Œå‡å°‘å™ªå£°ä¼ªå½±ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å®ç°äº†è‡ªé€‚åº”æƒé‡ç­–ç•¥ï¼Œä»¥å®æ—¶è°ƒæ•´æ‰©æ•£æŸå¤±ï¼Œæé«˜æ¨¡å‹å¯¹è¿œè·ç¦»æ·±åº¦é‡å»ºçš„æ•æ„Ÿæ€§ã€‚è¿™äº›åˆ›æ–°å…±åŒå¸¦æ¥äº†åœ¨è·ç¦»å’Œç»†èŠ‚æ–¹é¢çš„å“è¶Šæ·±åº¦æ„ŸçŸ¥æ€§èƒ½ã€‚å®éªŒè¯„ä¼°è¯å®ï¼ŒVistaDepthåœ¨åŸºäºæ‰©æ•£çš„MDEæŠ€æœ¯ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å‡†ç¡®é‡å»ºè¿œè·ç¦»åŒºåŸŸæ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15095v3">PDF</a> 8 pages, 6 figures, 4 tables</p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹åœ¨å•ç›®æ·±åº¦ä¼°è®¡ï¼ˆMDEï¼‰é¢†åŸŸå±•ç°å‡ºå¼ºå¤§çš„æ½œåŠ›ï¼Œé€šè¿‡å°†æŒ‘æˆ˜è½¬åŒ–ä¸ºæ¡ä»¶å›¾åƒç”Ÿæˆä»»åŠ¡ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¸¸é¢ä¸´é‡å»ºè¿œè·ç¦»æ·±åº¦ä¸å‡†ç¡®çš„é—®é¢˜ã€‚ä¸ºè§£å†³æ­¤ï¼ŒVistaDepthæ¡†æ¶å¼•å…¥è‡ªé€‚åº”é¢‘åŸŸç‰¹å¾å¢å¼ºå’Œè‡ªé€‚åº”æƒé‡å¹³è¡¡æœºåˆ¶ã€‚å…¶æ ¸å¿ƒæ¨¡å—â€”â€”æ½œåœ¨é¢‘ç‡è°ƒåˆ¶ï¼ˆLFMï¼‰èƒ½åŠ¨æ€ä¼˜åŒ–æ½œåœ¨ç‰¹å¾ç©ºé—´çš„é¢‘è°±å“åº”ï¼Œæå‡ç»“æ„ç»†èŠ‚ä¿ç•™å¹¶å‡å°‘å™ªå£°ã€‚åŒæ—¶ï¼Œå®æ–½è‡ªé€‚åº”æƒé‡ç­–ç•¥ï¼Œå®æ—¶è°ƒæ•´æ‰©æ•£æŸå¤±ï¼Œå¢å¼ºæ¨¡å‹å¯¹è¿œè·ç¦»æ·±åº¦çš„æ•æ„Ÿæ€§ã€‚è¿™äº›åˆ›æ–°å…±åŒæå‡äº†æ·±åº¦æ„ŸçŸ¥æ€§èƒ½ï¼Œåœ¨è·ç¦»å’Œç»†èŠ‚ä¸Šå‡è¡¨ç°ä¼˜è¶Šã€‚å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼ŒVistaDepthåœ¨åŸºäºæ‰©æ•£çš„MDEæŠ€æœ¯ä¸­è¾¾åˆ°é¢†å…ˆæ°´å¹³ï¼Œå°¤å…¶åœ¨é‡å»ºè¿œè·ç¦»åŒºåŸŸæ–¹é¢è¡¨ç°çªå‡ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å•ç›®æ·±åº¦ä¼°è®¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œé€šè¿‡æ¡ä»¶å›¾åƒç”Ÿæˆä»»åŠ¡æ¡†æ¶åº”å¯¹æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é¢ä¸´é‡å»ºè¿œè·ç¦»æ·±åº¦ä¸å‡†ç¡®çš„é—®é¢˜ï¼Œä¸»è¦æ˜¯ç”±äºæ·±åº¦å€¼åˆ†å¸ƒä¸å‡è¡¡ä»¥åŠè¿‡äºä¾èµ–ç©ºé—´åŸŸç‰¹å¾ã€‚</li>
<li>VistaDepthæ¡†æ¶é€šè¿‡æ•´åˆè‡ªé€‚åº”é¢‘åŸŸç‰¹å¾å¢å¼ºå’Œè‡ªé€‚åº”æƒé‡å¹³è¡¡æœºåˆ¶æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>æ½œåœ¨é¢‘ç‡è°ƒåˆ¶ï¼ˆLFMï¼‰æ¨¡å—èƒ½åŠ¨æ€ä¼˜åŒ–æ½œåœ¨ç‰¹å¾ç©ºé—´çš„é¢‘è°±å“åº”ï¼Œæ”¹å–„ç»“æ„ç»†èŠ‚ä¿ç•™ï¼Œå‡å°‘å™ªå£°ã€‚</li>
<li>è‡ªé€‚åº”æƒé‡ç­–ç•¥å®æ—¶è°ƒæ•´æ‰©æ•£æŸå¤±ï¼Œå¢å¼ºæ¨¡å‹å¯¹è¿œè·ç¦»æ·±åº¦çš„æ•æ„Ÿæ€§ã€‚</li>
<li>è¿™äº›åˆ›æ–°å…±åŒæå‡äº†æ·±åº¦æ„ŸçŸ¥æ€§èƒ½ï¼Œåœ¨è·ç¦»å’Œç»†èŠ‚ä¸Šå‡æœ‰æ‰€çªç ´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15095">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-40f687e371d13d2495496d469bba6de3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-72cda878b092ffe90a7ac0e1850b497a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2e61bd7439e03175edb293be32c720b7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aada7c6e3a3d4d7c7ab217b83811009b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d38f9f6803a7a066c6cf2b5a5a508534.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="U-Shape-Mamba-State-Space-Model-for-faster-diffusion"><a href="#U-Shape-Mamba-State-Space-Model-for-faster-diffusion" class="headerlink" title="U-Shape Mamba: State Space Model for faster diffusion"></a>U-Shape Mamba: State Space Model for faster diffusion</h2><p><strong>Authors:Alex Ergasti, Filippo Botti, Tomaso Fontanini, Claudio Ferrari, Massimo Bertozzi, Andrea Prati</strong></p>
<p>Diffusion models have become the most popular approach for high-quality image generation, but their high computational cost still remains a significant challenge. To address this problem, we propose U-Shape Mamba (USM), a novel diffusion model that leverages Mamba-based layers within a U-Net-like hierarchical structure. By progressively reducing sequence length in the encoder and restoring it in the decoder through Mamba blocks, USM significantly lowers computational overhead while maintaining strong generative capabilities. Experimental results against Zigma, which is currently the most efficient Mamba-based diffusion model, demonstrate that USM achieves one-third the GFlops, requires less memory and is faster, while outperforming Zigma in image quality. Frechet Inception Distance (FID) is improved by 15.3, 0.84 and 2.7 points on AFHQ, CelebAHQ and COCO datasets, respectively. These findings highlight USM as a highly efficient and scalable solution for diffusion-based generative models, making high-quality image synthesis more accessible to the research community while reducing computational costs. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹å·²æˆä¸ºé«˜è´¨é‡å›¾åƒç”Ÿæˆçš„æœ€æµè¡Œæ–¹æ³•ï¼Œä½†å…¶é«˜è®¡ç®—æˆæœ¬ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Uå½¢Mambaï¼ˆUSMï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„æ‰©æ•£æ¨¡å‹ï¼Œå®ƒåˆ©ç”¨åŸºäºMambaçš„å±‚æ¬¡ç»“æ„å†…çš„U-Netç»“æ„ã€‚é€šè¿‡é€æ­¥å‡å°‘ç¼–ç å™¨ä¸­çš„åºåˆ—é•¿åº¦å¹¶åœ¨è§£ç å™¨é€šè¿‡Mambaå—è¿›è¡Œæ¢å¤ï¼ŒUSMåœ¨ä¿æŒå¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›çš„åŒæ—¶æ˜¾è‘—é™ä½äº†è®¡ç®—å¼€é”€ã€‚ä¸ç›®å‰åŸºäºMambaçš„æœ€æœ‰æ•ˆçš„Zigmaæ¨¡å‹çš„å®éªŒå¯¹æ¯”è¡¨æ˜ï¼ŒUSMå®ç°äº†ä¸‰åˆ†ä¹‹ä¸€GFLOPSï¼Œå†…å­˜éœ€æ±‚æ›´å°‘ä¸”é€Ÿåº¦æ›´å¿«ï¼ŒåŒæ—¶åœ¨å›¾åƒè´¨é‡ä¸Šä¼˜äºZigmaã€‚åœ¨AFHQã€CelebAHQå’ŒCOCOæ•°æ®é›†ä¸Šï¼ŒFrechet Inception Distanceï¼ˆFIDï¼‰åˆ†åˆ«æé«˜äº†15.3ã€0.84å’Œ2.7ç‚¹ã€‚è¿™äº›å‘ç°çªå‡ºäº†USMä½œä¸ºåŸºäºæ‰©æ•£çš„ç”Ÿæˆæ¨¡å‹çš„é«˜æ•ˆä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆï¼Œä½¿é«˜è´¨é‡å›¾åƒåˆæˆæ›´å®¹æ˜“ä¸ºç ”ç©¶é¢†åŸŸæ‰€è·å–ï¼ŒåŒæ—¶é™ä½äº†è®¡ç®—æˆæœ¬ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13499v2">PDF</a> Accepted at CVPR 2025 eLVM workshop. The code is here:   <a target="_blank" rel="noopener" href="https://github.com/ErgastiAlex/U-Shape-Mamba">https://github.com/ErgastiAlex/U-Shape-Mamba</a></p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹åœ¨é«˜è´¨é‡å›¾åƒç”Ÿæˆä¸­å¤‡å—æ¬¢è¿ï¼Œä½†å…¶é«˜è®¡ç®—æˆæœ¬ä»æ˜¯é‡å¤§æŒ‘æˆ˜ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†U-Shape Mambaï¼ˆUSMï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„æ‰©æ•£æ¨¡å‹ï¼Œå®ƒåˆ©ç”¨åŸºäºMambaçš„å±‚æ¬¡ç»“æ„å†…çš„U-Netæ ·ç»“æ„ã€‚é€šè¿‡é€æ­¥å‡å°‘ç¼–ç å™¨ä¸­çš„åºåˆ—é•¿åº¦å¹¶åœ¨è§£ç å™¨ä¸­æ¢å¤ï¼ŒUSMåœ¨é™ä½è®¡ç®—å¼€é”€çš„åŒæ—¶ä¿æŒäº†å¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›ã€‚ä¸å½“å‰æœ€æœ‰æ•ˆçš„åŸºäºMambaçš„æ‰©æ•£æ¨¡å‹Zigmaç›¸æ¯”ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼ŒUSMå®ç°äº†ä¸‰åˆ†ä¹‹ä¸€GFLOPSï¼Œå†…å­˜éœ€æ±‚æ›´å°‘ï¼Œé€Ÿåº¦æ›´å¿«ï¼ŒåŒæ—¶åœ¨å›¾åƒè´¨é‡ä¸Šä¼˜äºZigmaã€‚åœ¨AFHQã€CelebAHQå’ŒCOCOæ•°æ®é›†ä¸Šï¼ŒFrechet Inception Distanceï¼ˆFIDï¼‰åˆ†åˆ«æé«˜äº†15.3ã€0.84å’Œ2.7ç‚¹ã€‚è¿™äº›å‘ç°çªæ˜¾äº†USMä½œä¸ºæ‰©æ•£ç”Ÿæˆæ¨¡å‹çš„é«˜æ•ˆå¯ä¼¸ç¼©è§£å†³æ–¹æ¡ˆï¼Œä½¿é«˜è´¨é‡å›¾åƒåˆæˆæ›´å®¹æ˜“ä¸ºç ”ç©¶é¢†åŸŸæ‰€æ¥å—ï¼ŒåŒæ—¶é™ä½äº†è®¡ç®—æˆæœ¬ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆä¸­å—åˆ°æ¬¢è¿ï¼Œä½†è®¡ç®—æˆæœ¬é«˜æ˜‚ä»æ˜¯æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºU-Shape Mambaï¼ˆUSMï¼‰æ¨¡å‹ï¼Œç»“åˆMamba-basedå±‚å’ŒU-Netç»“æ„ä»¥é™ä½æˆæœ¬ã€‚</li>
<li>USMé€šè¿‡å‡å°‘ç¼–ç å™¨ä¸­çš„åºåˆ—é•¿åº¦å¹¶å¢åŠ è§£ç å™¨ä¸­çš„æ¢å¤æ¥é™ä½è®¡ç®—å¼€é”€ã€‚</li>
<li>ä¸ç°æœ‰æ¨¡å‹Zigmaç›¸æ¯”ï¼ŒUSMåœ¨è®¡ç®—æ•ˆç‡ã€å†…å­˜ä½¿ç”¨å’Œé€Ÿåº¦æ–¹é¢è¡¨ç°æ›´ä¼˜ç§€ã€‚</li>
<li>USMåœ¨ç»´æŒä½è®¡ç®—æˆæœ¬çš„åŒæ—¶ï¼Œæé«˜äº†å›¾åƒè´¨é‡ï¼Œä¼˜äºZigmaã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„Frechet Inception Distanceï¼ˆFIDï¼‰æŒ‡æ ‡æ˜¾ç¤ºUSMæ€§èƒ½æ˜¾è‘—æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13499">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5d4c4793c606ddbab2db980510445ce9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-50aeab6709a80cc3ffd0614f29ef05e5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b13177fbcf357af3bbc8e1aa72b42c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-71fdd97d39625a338bc16ebfa60281d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5ca6ba7bb3fb91de61abcb156add967d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5fbb93af68d7181e326d9e42b4ff27d3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec0b5287f4ba9e370f0e1bb0cb7a031c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1bbbcb2654e303f36eec7bec3c535e6.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="ReStyle3D-Scene-Level-Appearance-Transfer-with-Semantic-Correspondences"><a href="#ReStyle3D-Scene-Level-Appearance-Transfer-with-Semantic-Correspondences" class="headerlink" title="ReStyle3D: Scene-Level Appearance Transfer with Semantic Correspondences"></a>ReStyle3D: Scene-Level Appearance Transfer with Semantic Correspondences</h2><p><strong>Authors:Liyuan Zhu, Shengqu Cai, Shengyu Huang, Gordon Wetzstein, Naji Khosravan, Iro Armeni</strong></p>
<p>We introduce ReStyle3D, a novel framework for scene-level appearance transfer from a single style image to a real-world scene represented by multiple views. The method combines explicit semantic correspondences with multi-view consistency to achieve precise and coherent stylization. Unlike conventional stylization methods that apply a reference style globally, ReStyle3D uses open-vocabulary segmentation to establish dense, instance-level correspondences between the style and real-world images. This ensures that each object is stylized with semantically matched textures. It first transfers the style to a single view using a training-free semantic-attention mechanism in a diffusion model. It then lifts the stylization to additional views via a learned warp-and-refine network guided by monocular depth and pixel-wise correspondences. Experiments show that ReStyle3D consistently outperforms prior methods in structure preservation, perceptual style similarity, and multi-view coherence. User studies further validate its ability to produce photo-realistic, semantically faithful results. Our code, pretrained models, and dataset will be publicly released, to support new applications in interior design, virtual staging, and 3D-consistent stylization. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†ReStyle3Dï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„åœºæ™¯çº§å¤–è§‚è½¬æ¢æ¡†æ¶ï¼Œå®ƒå¯ä»¥ä»å•ä¸ªé£æ ¼å›¾åƒå°†é£æ ¼åº”ç”¨åˆ°ç°å®ä¸–ç•Œåœºæ™¯ï¼Œè¯¥åœºæ™¯ç”±å¤šä¸ªè§†å›¾è¡¨ç¤ºã€‚è¯¥æ–¹æ³•ç»“åˆäº†æ˜ç¡®çš„è¯­ä¹‰å¯¹åº”å’Œå¤šè§†å›¾ä¸€è‡´æ€§ï¼Œä»¥å®ç°ç²¾ç¡®å’Œè¿è´¯çš„é£æ ¼åŒ–ã€‚ä¸ä¼ ç»Ÿçš„å…¨å±€åº”ç”¨å‚è€ƒé£æ ¼çš„æ–¹æ³•ä¸åŒï¼ŒReStyle3Dä½¿ç”¨å¼€æ”¾è¯æ±‡åˆ†å‰²æ¥åœ¨é£æ ¼å›¾åƒå’ŒçœŸå®ä¸–ç•Œå›¾åƒä¹‹é—´å»ºç«‹å¯†é›†ã€å®ä¾‹çº§çš„å¯¹åº”ã€‚è¿™ç¡®ä¿æ¯ä¸ªå¯¹è±¡éƒ½ç”¨è¯­ä¹‰åŒ¹é…çš„çº¹ç†è¿›è¡Œé£æ ¼åŒ–ã€‚å®ƒé¦–å…ˆä½¿ç”¨æ‰©æ•£æ¨¡å‹ä¸­çš„æ— è®­ç»ƒè¯­ä¹‰æ³¨æ„åŠ›æœºåˆ¶å°†é£æ ¼è½¬ç§»åˆ°å•ä¸ªè§†å›¾ã€‚ç„¶åï¼Œé€šè¿‡ç”±å•çœ¼æ·±åº¦å’Œåƒç´ çº§å¯¹åº”å¼•å¯¼çš„å­¦ä¹ warp-and-refineç½‘ç»œï¼Œå°†é£æ ¼åŒ–æå‡åˆ°å…¶ä»–è§†å›¾ã€‚å®éªŒè¡¨æ˜ï¼ŒReStyle3Dåœ¨ç»“æ„ä¿æŒã€æ„ŸçŸ¥é£æ ¼ç›¸ä¼¼æ€§å’Œå¤šè§†å›¾ä¸€è‡´æ€§æ–¹é¢å‡ä¼˜äºä»¥å‰çš„æ–¹æ³•ã€‚ç”¨æˆ·ç ”ç©¶è¿›ä¸€æ­¥éªŒè¯äº†å…¶ç”Ÿæˆå…·æœ‰çœŸå®æ„Ÿå’Œè¯­ä¹‰å¿ å®ç»“æœçš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ä»£ç ã€é¢„è®­ç»ƒæ¨¡å‹å’Œæ•°æ®é›†å°†å…¬å¼€å‘å¸ƒï¼Œä»¥æ”¯æŒå®¤å†…è®¾è®¡ã€è™šæ‹Ÿèˆå°å’Œ3Dä¸€è‡´é£æ ¼åŒ–çš„æ–°åº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.10377v2">PDF</a> SIGGRAPH 2025. Project page: <a target="_blank" rel="noopener" href="https://restyle3d.github.io/">https://restyle3d.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>ReStyle3Dæ˜¯ä¸€ç§æ–°é¢–çš„åœºæ™¯çº§é£æ ¼è½¬æ¢æ¡†æ¶ï¼Œå¯ä»å•ä¸ªé£æ ¼å›¾åƒå¯¹ç°å®ä¸–ç•Œåœºæ™¯è¿›è¡Œé£æ ¼è½¬æ¢ï¼Œè¯¥åœºæ™¯ç”±å¤šä¸ªè§†å›¾è¡¨ç¤ºã€‚ç»“åˆæ˜¾å¼è¯­ä¹‰å¯¹åº”å’Œå¤šè§†å›¾ä¸€è‡´æ€§ï¼Œå®ç°ç²¾ç¡®å’Œè¿è´¯çš„é£æ ¼åŒ–ã€‚ä¸åŒäºä¼ ç»Ÿå…¨å±€åº”ç”¨å‚è€ƒé£æ ¼çš„æ–¹æ³•ï¼ŒReStyle3Dä½¿ç”¨å¼€æ”¾è¯æ±‡åˆ†å‰²åœ¨é£æ ¼å›¾åƒå’ŒçœŸå®ä¸–ç•Œå›¾åƒä¹‹é—´å»ºç«‹å¯†é›†ã€å®ä¾‹çº§çš„å¯¹åº”ï¼Œç¡®ä¿æ¯ä¸ªå¯¹è±¡éƒ½ç”¨è¯­ä¹‰åŒ¹é…çš„çº¹ç†è¿›è¡Œé£æ ¼åŒ–ã€‚é¦–å…ˆï¼Œé€šè¿‡æ‰©æ•£æ¨¡å‹ä¸­çš„æ— è®­ç»ƒè¯­ä¹‰æ³¨æ„åŠ›æœºåˆ¶å°†é£æ ¼è½¬ç§»åˆ°å•ä¸ªè§†å›¾ä¸Šï¼Œç„¶åé€šè¿‡ä»¥å•ç›®æ·±åº¦å’Œåƒç´ çº§å¯¹åº”ä¸ºæŒ‡å¯¼çš„å­¦ä¹ warp-and-refineç½‘ç»œå°†é£æ ¼åŒ–æå‡åˆ°å…¶ä»–è§†å›¾ã€‚å®éªŒè¡¨æ˜ï¼ŒReStyle3Dåœ¨ç»“æ„ä¿æŒã€æ„ŸçŸ¥é£æ ¼ç›¸ä¼¼æ€§å’Œå¤šè§†å›¾ä¸€è‡´æ€§æ–¹é¢å‡ä¼˜äºå…ˆå‰çš„æ–¹æ³•ã€‚ç”¨æˆ·ç ”ç©¶è¿›ä¸€æ­¥éªŒè¯äº†å…¶åœ¨ç”Ÿæˆé€¼çœŸã€è¯­ä¹‰å¿ å®ç»“æœæ–¹é¢çš„èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ReStyle3Dæ˜¯ä¸€ä¸ªåœºæ™¯çº§é£æ ¼è½¬æ¢æ¡†æ¶ï¼Œèƒ½å¤Ÿå®ç°ä»å•ä¸ªé£æ ¼å›¾åƒåˆ°å¤šè§†å›¾ç°å®åœºæ™¯çš„è½¬æ¢ã€‚</li>
<li>è¯¥æ–¹æ³•ç»“åˆæ˜¾å¼è¯­ä¹‰å¯¹åº”å’Œå¤šè§†å›¾ä¸€è‡´æ€§ï¼Œå®ç°ç²¾ç¡®å’Œè¿è´¯çš„é£æ ¼åŒ–ã€‚</li>
<li>ReStyle3Dä½¿ç”¨å¼€æ”¾è¯æ±‡åˆ†å‰²å»ºç«‹å¯†é›†ã€å®ä¾‹çº§çš„é£æ ¼å›¾åƒå’ŒçœŸå®ä¸–ç•Œå›¾åƒä¹‹é—´çš„å¯¹åº”ã€‚</li>
<li>é€šè¿‡æ‰©æ•£æ¨¡å‹ä¸­çš„æ— è®­ç»ƒè¯­ä¹‰æ³¨æ„åŠ›æœºåˆ¶ï¼ŒReStyle3Dé¦–å…ˆå°†é£æ ¼è½¬ç§»åˆ°å•ä¸ªè§†å›¾ä¸Šã€‚</li>
<li>é€šè¿‡å­¦ä¹ warp-and-refineç½‘ç»œï¼ŒReStyle3Dèƒ½å¤Ÿå°†é£æ ¼åŒ–æ‰©å±•åˆ°å…¶ä»–è§†å›¾ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒReStyle3Dåœ¨ç»“æ„ä¿æŒã€æ„ŸçŸ¥é£æ ¼ç›¸ä¼¼æ€§å’Œå¤šè§†å›¾ä¸€è‡´æ€§æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.10377">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8728e1ea7f8f900359dc1900fd74ff2c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f389bfd42768774e1b9ee51f0ba0517.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6491a865634decf8a754cabfd78e69cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a64821a361c7dd68e5ae4bf64a8b042.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0413c8a186009ee1b90c8e74a30f8ffb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42ac12f67d1b01329ff4a9cac8f7baeb.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Wonderland-Navigating-3D-Scenes-from-a-Single-Image"><a href="#Wonderland-Navigating-3D-Scenes-from-a-Single-Image" class="headerlink" title="Wonderland: Navigating 3D Scenes from a Single Image"></a>Wonderland: Navigating 3D Scenes from a Single Image</h2><p><strong>Authors:Hanwen Liang, Junli Cao, Vidit Goel, Guocheng Qian, Sergei Korolev, Demetri Terzopoulos, Konstantinos N. Plataniotis, Sergey Tulyakov, Jian Ren</strong></p>
<p>How can one efficiently generate high-quality, wide-scope 3D scenes from arbitrary single images? Existing methods suffer several drawbacks, such as requiring multi-view data, time-consuming per-scene optimization, distorted geometry in occluded areas, and low visual quality in backgrounds. Our novel 3D scene reconstruction pipeline overcomes these limitations to tackle the aforesaid challenge. Specifically, we introduce a large-scale reconstruction model that leverages latents from a video diffusion model to predict 3D Gaussian Splattings of scenes in a feed-forward manner. The video diffusion model is designed to create videos precisely following specified camera trajectories, allowing it to generate compressed video latents that encode multi-view information while maintaining 3D consistency. We train the 3D reconstruction model to operate on the video latent space with a progressive learning strategy, enabling the efficient generation of high-quality, wide-scope, and generic 3D scenes. Extensive evaluations across various datasets affirm that our model significantly outperforms existing single-view 3D scene generation methods, especially with out-of-domain images. Thus, we demonstrate for the first time that a 3D reconstruction model can effectively be built upon the latent space of a diffusion model in order to realize efficient 3D scene generation. </p>
<blockquote>
<p>å¦‚ä½•ä»ä»»æ„å•å¼ å›¾åƒé«˜æ•ˆç”Ÿæˆé«˜è´¨é‡ã€å¤§èŒƒå›´çš„ä¸‰ç»´åœºæ™¯ï¼Ÿç°æœ‰æ–¹æ³•å­˜åœ¨è¯¸å¤šç¼ºç‚¹ï¼Œä¾‹å¦‚éœ€è¦å¤šè§†è§’æ•°æ®ã€è€—æ—¶çš„åœºæ™¯ä¼˜åŒ–ã€é®æŒ¡åŒºåŸŸçš„å¤±çœŸå‡ ä½•ä»¥åŠèƒŒæ™¯çš„ä½è§†è§‰è´¨é‡ã€‚æˆ‘ä»¬çš„æ–°å‹ä¸‰ç»´åœºæ™¯é‡å»ºæµç¨‹å…‹æœäº†è¿™äº›é™åˆ¶ï¼Œä»¥åº”å¯¹ä¸Šè¿°æŒ‘æˆ˜ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¤§è§„æ¨¡é‡å»ºæ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä»¥è§†é¢‘æ‰©æ•£æ¨¡å‹çš„æ½œåœ¨ç‰¹å¾ä¸ºåŸºç¡€ï¼Œä»¥å‰é¦ˆæ–¹å¼é¢„æµ‹åœºæ™¯çš„ä¸‰ç»´é«˜æ–¯Splattingsã€‚è§†é¢‘æ‰©æ•£æ¨¡å‹çš„è®¾è®¡æ—¨åœ¨ç²¾ç¡®éµå¾ªæŒ‡å®šçš„ç›¸æœºè½¨è¿¹åˆ›å»ºè§†é¢‘ï¼Œä½¿å…¶èƒ½å¤Ÿç”Ÿæˆå‹ç¼©çš„è§†é¢‘æ½œåœ¨ç‰¹å¾ï¼Œåœ¨ç¼–ç å¤šè§†è§’ä¿¡æ¯çš„åŒæ—¶ä¿æŒä¸‰ç»´ä¸€è‡´æ€§ã€‚æˆ‘ä»¬é‡‡ç”¨æ¸è¿›å­¦ä¹ ç­–ç•¥å¯¹ä¸‰ç»´é‡å»ºæ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨è§†é¢‘æ½œåœ¨ç©ºé—´ä¸Šè¿è¡Œï¼Œä»è€Œèƒ½å¤Ÿé«˜æ•ˆç”Ÿæˆé«˜è´¨é‡ã€å¤§èŒƒå›´ä¸”é€šç”¨çš„ä¸‰ç»´åœºæ™¯ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¯å®ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å•è§†å›¾ä¸‰ç»´åœºæ™¯ç”Ÿæˆæ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨åŸŸå¤–å›¾åƒä¸Šã€‚å› æ­¤ï¼Œæˆ‘ä»¬é¦–æ¬¡è¯æ˜ï¼Œå¯ä»¥åœ¨æ‰©æ•£æ¨¡å‹çš„æ½œåœ¨ç©ºé—´ä¸Šå»ºç«‹ä¸‰ç»´é‡å»ºæ¨¡å‹ï¼Œä»¥å®ç°é«˜æ•ˆçš„ä¸‰ç»´åœºæ™¯ç”Ÿæˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12091v2">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://snap-research.github.io/wonderland/">https://snap-research.github.io/wonderland/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºè§†é¢‘æ‰©æ•£æ¨¡å‹çš„3Dåœºæ™¯é‡å»ºæ–¹æ³•ï¼Œè§£å†³äº†ä»ä»»æ„å•å›¾åƒé«˜æ•ˆç”Ÿæˆé«˜è´¨é‡ã€å¤§èŒƒå›´3Dåœºæ™¯çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•åˆ©ç”¨è§†é¢‘æ‰©æ•£æ¨¡å‹çš„æ½œåœ¨ç‰¹å¾é¢„æµ‹åœºæ™¯çš„3Dé«˜æ–¯Splattingsï¼Œä»¥ç”Ÿæˆå…·æœ‰å¤šè§†è§’ä¿¡æ¯çš„å‹ç¼©è§†é¢‘æ½œåœ¨ç‰¹å¾å¹¶ä¿æŒ3Dä¸€è‡´æ€§ã€‚é‡‡ç”¨æ¸è¿›å­¦ä¹ ç­–ç•¥è®­ç»ƒ3Dé‡å»ºæ¨¡å‹ï¼Œä½¿å…¶èƒ½åœ¨è§†é¢‘æ½œåœ¨ç©ºé—´ä¸Šæ“ä½œï¼Œå®ç°é«˜æ•ˆã€é«˜è´¨é‡ã€é€šç”¨æ€§å¼ºçš„å¤§èŒƒå›´3Dåœºæ™¯ç”Ÿæˆã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹æ˜¾è‘—ä¼˜äºç°æœ‰å•è§†è§’3Dåœºæ™¯ç”Ÿæˆæ–¹æ³•ï¼Œå°¤å…¶æ˜¯å¤„ç†éåŸŸå›¾åƒæ—¶è¡¨ç°æ›´ä½³ã€‚é¦–æ¬¡å±•ç¤ºäº†åœ¨æ‰©æ•£æ¨¡å‹çš„æ½œåœ¨ç©ºé—´ä¸Šæ„å»ºæœ‰æ•ˆçš„3Dé‡å»ºæ¨¡å‹ï¼Œå¯å®ç°é«˜æ•ˆçš„3Dåœºæ™¯ç”Ÿæˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†ä¸€ç§åŸºäºè§†é¢‘æ‰©æ•£æ¨¡å‹çš„3Dåœºæ™¯é‡å»ºæ–¹æ³•ï¼Œæ—¨åœ¨ä»ä»»æ„å•å›¾åƒç”Ÿæˆé«˜è´¨é‡ã€å¤§èŒƒå›´3Dåœºæ™¯ã€‚</li>
<li>åˆ©ç”¨è§†é¢‘æ‰©æ•£æ¨¡å‹çš„æ½œåœ¨ç‰¹å¾é¢„æµ‹åœºæ™¯çš„3Dé«˜æ–¯Splattingsï¼Œå®ç°å¤šè§†è§’ä¿¡æ¯çš„å‹ç¼©è§†é¢‘æ½œåœ¨ç‰¹å¾å¹¶ä¿æŒ3Dä¸€è‡´æ€§ã€‚</li>
<li>é‡‡ç”¨æ¸è¿›å­¦ä¹ ç­–ç•¥è®­ç»ƒçš„3Dé‡å»ºæ¨¡å‹èƒ½å¤Ÿåœ¨è§†é¢‘æ½œåœ¨ç©ºé—´ä¸Šæ“ä½œï¼Œæé«˜äº†ç”Ÿæˆæ•ˆç‡å’Œè´¨é‡ã€‚</li>
<li>è¯¥æ¨¡å‹æ˜¾è‘—ä¼˜äºç°æœ‰å•è§†è§’3Dåœºæ™¯ç”Ÿæˆæ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†éåŸŸå›¾åƒæ—¶è¡¨ç°æ›´ä½³ã€‚</li>
<li>é¦–æ¬¡å±•ç¤ºäº†ç»“åˆæ‰©æ•£æ¨¡å‹çš„æ½œåœ¨ç©ºé—´æ„å»ºæœ‰æ•ˆçš„3Dé‡å»ºæ¨¡å‹ï¼Œä¸ºé«˜æ•ˆç”Ÿæˆé«˜è´¨é‡3Dåœºæ™¯æä¾›äº†æ–°çš„é€”å¾„ã€‚</li>
<li>è¯¥æ–¹æ³•å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯åº”ç”¨äºè™šæ‹Ÿç°å®ã€å¢å¼ºç°å®ã€æ¸¸æˆå¼€å‘ç­‰é¢†åŸŸã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12091">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e6a2d95694980601963f3bccbda83423.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb22c485b79e1e1879dd15b2d1a685e0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8497b85ef44881476146de4b318058d5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9248f46536c06ac8e68f22d2002853b0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Self-Consistent-Nested-Diffusion-Bridge-for-Accelerated-MRI-Reconstruction"><a href="#Self-Consistent-Nested-Diffusion-Bridge-for-Accelerated-MRI-Reconstruction" class="headerlink" title="Self-Consistent Nested Diffusion Bridge for Accelerated MRI   Reconstruction"></a>Self-Consistent Nested Diffusion Bridge for Accelerated MRI   Reconstruction</h2><p><strong>Authors:Tao Song, Yicheng Wu, Minhao Hu, Xiangde Luo, Guoting Luo, Guotai Wang, Yi Guo, Feng Xu, Shaoting Zhang</strong></p>
<p>Accelerated MRI reconstruction plays a vital role in reducing scan time while preserving image quality. While most existing methods rely on complex-valued image-space or k-space data, these formats are often inaccessible in clinical practice due to proprietary reconstruction pipelines, leaving only magnitude images stored in DICOM files. To address this gap, we focus on the underexplored task of magnitude-image-based MRI reconstruction. Recent advancements in diffusion models, particularly denoising diffusion probabilistic models (DDPMs), have demonstrated strong capabilities in modeling image priors. However, their task-agnostic denoising nature limits performance in source-to-target image translation tasks, such as MRI reconstruction. In this work, we propose a novel Self-Consistent Nested Diffusion Bridge (SC-NDB) framework that models accelerated MRI reconstruction as a bi-directional image translation process between under-sampled and fully-sampled magnitude MRI images. SC-NDB introduces a nested diffusion architecture with a self-consistency constraint and reverse bridge diffusion pathways to improve intermediate prediction fidelity and better capture the explicit priors of source images. Furthermore, we incorporate a Contour Decomposition Embedding Module (CDEM) to inject structural and textural knowledge by leveraging Laplacian pyramids and directional filter banks. Extensive experiments on the fastMRI and IXI datasets demonstrate that our method achieves state-of-the-art performance compared to both magnitude-based and non-magnitude-based diffusion models, confirming the effectiveness and clinical relevance of SC-NDB. </p>
<blockquote>
<p>åŠ é€ŸMRIé‡å»ºåœ¨å‡å°‘æ‰«ææ—¶é—´çš„åŒæ—¶ä¿æŒå›¾åƒè´¨é‡æ–¹é¢èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚è™½ç„¶ç°æœ‰çš„å¤§å¤šæ•°æ–¹æ³•éƒ½ä¾èµ–äºå¤æ•°å€¼å›¾åƒç©ºé—´æˆ–kç©ºé—´æ•°æ®ï¼Œä½†è¿™äº›æ ¼å¼ç”±äºä¸“æœ‰é‡å»ºç®¡é“è€Œåœ¨ä¸´åºŠå®è·µä¸­å¾€å¾€æ— æ³•è®¿é—®ï¼Œåªç•™ä¸‹ä»¥DICOMæ–‡ä»¶å­˜å‚¨çš„å¹…åº¦å›¾åƒã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬ä¸“æ³¨äºåŸºäºå¹…åº¦å›¾åƒçš„MRIé‡å»ºè¿™ä¸€å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶çš„ä»»åŠ¡ã€‚æ‰©æ•£æ¨¡å‹çš„æœ€æ–°è¿›å±•ï¼Œç‰¹åˆ«æ˜¯å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMsï¼‰ï¼Œåœ¨å»ºæ¨¡å›¾åƒå…ˆéªŒæ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå…¶ä»»åŠ¡æ— å…³çš„å»å™ªæ€§è´¨é™åˆ¶äº†å…¶åœ¨æºåˆ°ç›®æ ‡å›¾åƒç¿»è¯‘ä»»åŠ¡ï¼ˆä¾‹å¦‚MRIé‡å»ºï¼‰ä¸­çš„æ€§èƒ½ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„Self-Consistent Nested Diffusion Bridgeï¼ˆSC-NDBï¼‰æ¡†æ¶ï¼Œå°†åŠ é€ŸMRIé‡å»ºå»ºæ¨¡ä¸ºæ¬ é‡‡æ ·å¹…åº¦MRIå›¾åƒä¸å…¨é‡‡æ ·å¹…åº¦MRIå›¾åƒä¹‹é—´çš„åŒå‘å›¾åƒç¿»è¯‘è¿‡ç¨‹ã€‚SC-NDBå¼•å…¥äº†ä¸€ç§å¸¦æœ‰è‡ªä¸€è‡´æ€§çº¦æŸå’Œåå‘æ¡¥æ‰©æ•£è·¯å¾„çš„åµŒå¥—æ‰©æ•£æ¶æ„ï¼Œä»¥æé«˜ä¸­é—´é¢„æµ‹ä¿çœŸåº¦å¹¶æ›´å¥½åœ°æ•è·æºå›¾åƒçš„æ˜¾å¼å…ˆéªŒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç»“åˆäº†ä¸€ä¸ªè½®å»“åˆ†è§£åµŒå…¥æ¨¡å—ï¼ˆCDEMï¼‰ï¼Œé€šè¿‡åˆ©ç”¨æ‹‰æ™®æ‹‰æ–¯é‡‘å­—å¡”å’Œæ–¹å‘æ»¤æ³¢å™¨åº“æ¥æ³¨å…¥ç»“æ„å’Œçº¹ç†çŸ¥è¯†ã€‚åœ¨fastMRIå’ŒIXIæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç›¸è¾ƒäºåŸºäºå¹…åº¦å’ŒéåŸºäºå¹…åº¦çš„æ‰©æ•£æ¨¡å‹è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¯å®äº†SC-NDBçš„æœ‰æ•ˆæ€§å’Œä¸´åºŠç›¸å…³æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.09998v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬ç ”ç©¶é’ˆå¯¹åŠ é€ŸMRIé‡å»ºé—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–°æ–¹æ³•ã€‚è€ƒè™‘åˆ°ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–å¤æ‚å€¼å›¾åƒç©ºé—´æˆ–kç©ºé—´æ•°æ®ï¼Œè€Œä¸´åºŠå®è·µä¸­å¸¸å› ä¸“æœ‰é‡å»ºæµç¨‹åªèƒ½è·å–åˆ°DICOMæ–‡ä»¶ä¸­çš„å¹…åº¦å›¾åƒï¼Œæœ¬ç ”ç©¶ä¸“æ³¨äºåŸºäºå¹…åº¦å›¾åƒçš„MRIé‡å»ºè¿™ä¸€æœªå……åˆ†ç ”ç©¶çš„é—®é¢˜ã€‚é‡‡ç”¨è‡ªæ´½åµŒå¥—æ‰©æ•£æ¡¥ï¼ˆSC-NDBï¼‰æ¡†æ¶ï¼Œå°†åŠ é€ŸMRIé‡å»ºå»ºæ¨¡ä¸ºæ¬ é‡‡æ ·ä¸å…¨é‡‡æ ·å¹…åº¦MRIå›¾åƒä¹‹é—´çš„åŒå‘å›¾åƒç¿»è¯‘è¿‡ç¨‹ã€‚SC-NDBå¼•å…¥åµŒå¥—æ‰©æ•£æ¶æ„ï¼Œé€šè¿‡è‡ªæˆ‘ä¸€è‡´æ€§çº¦æŸå’Œåå‘æ¡¥æ¢æ‰©æ•£è·¯å¾„æé«˜ä¸­é—´é¢„æµ‹ä¿çœŸåº¦ï¼Œæ›´å¥½åœ°æ•æ‰æºå›¾åƒçš„æ˜¾å¼å…ˆéªŒä¿¡æ¯ã€‚ç»“åˆè½®å»“åˆ†è§£åµŒå…¥æ¨¡å—ï¼ˆCDEMï¼‰ï¼Œé€šè¿‡åˆ©ç”¨æ‹‰æ™®æ‹‰æ–¯é‡‘å­—å¡”å’Œæ–¹å‘æ»¤æ³¢å™¨åº“æ³¨å…¥ç»“æ„å’Œçº¹ç†çŸ¥è¯†ã€‚åœ¨fastMRIå’ŒIXIæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•è¾ƒåŸºäºå¹…åº¦å’ŒéåŸºäºå¹…åº¦çš„æ‰©æ•£æ¨¡å‹å‡å–å¾—æœ€æ–°æ€§èƒ½ï¼Œè¯æ˜äº†SC-NDBçš„æœ‰æ•ˆæ€§å’Œä¸´åºŠç›¸å…³æ€§ã€‚</p>
<p><strong>è¦ç‚¹æŒæ¡</strong></p>
<ol>
<li>åŠ é€ŸMRIé‡å»ºå¯¹äºç¼©çŸ­æ‰«ææ—¶é—´åŒæ—¶ä¿æŒå›¾åƒè´¨é‡è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¤§å¤šä¾èµ–äºå¤æ‚å€¼å›¾åƒç©ºé—´æˆ–kç©ºé—´æ•°æ®ï¼Œä½†åœ¨ä¸´åºŠå®è·µä¸­ç”±äºä¸“æœ‰é‡å»ºæµç¨‹ï¼Œåªèƒ½è·å–åˆ°DICOMæ–‡ä»¶ä¸­çš„å¹…åº¦å›¾åƒã€‚</li>
<li>æœ¬ç ”ç©¶ä¸“æ³¨äºåŸºäºå¹…åº¦å›¾åƒçš„MRIé‡å»ºæ–¹æ³•ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºè‡ªæ´½åµŒå¥—æ‰©æ•£æ¡¥ï¼ˆSC-NDBï¼‰çš„æ–°æ¡†æ¶ï¼Œå°†MRIé‡å»ºå»ºæ¨¡ä¸ºæ¬ é‡‡æ ·ä¸å…¨é‡‡æ ·å¹…åº¦å›¾åƒä¹‹é—´çš„åŒå‘ç¿»è¯‘è¿‡ç¨‹ã€‚</li>
<li>SC-NDBé€šè¿‡åµŒå¥—æ‰©æ•£æ¶æ„å’Œè‡ªæˆ‘ä¸€è‡´æ€§çº¦æŸæé«˜é¢„æµ‹ä¸­é—´ç»“æœçš„å‡†ç¡®æ€§ï¼Œæ›´å¥½åœ°æ•æ‰æºå›¾åƒçš„æ˜¾å¼å…ˆéªŒä¿¡æ¯ã€‚</li>
<li>ç»“åˆè½®å»“åˆ†è§£åµŒå…¥æ¨¡å—ï¼ˆCDEMï¼‰ä»¥å¢å¼ºå›¾åƒçš„ç»“æ„å’Œçº¹ç†ä¿¡æ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.09998">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-bd1f7b949aaaa5f2296d25a8a66332f9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a7e7a39234dd773301f01071b928f51a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-99ce769d63417c223290e5d676d686c2.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Hidden-in-the-Noise-Two-Stage-Robust-Watermarking-for-Images"><a href="#Hidden-in-the-Noise-Two-Stage-Robust-Watermarking-for-Images" class="headerlink" title="Hidden in the Noise: Two-Stage Robust Watermarking for Images"></a>Hidden in the Noise: Two-Stage Robust Watermarking for Images</h2><p><strong>Authors:Kasra Arabi, Benjamin Feuer, R. Teal Witter, Chinmay Hegde, Niv Cohen</strong></p>
<p>As the quality of image generators continues to improve, deepfakes become a topic of considerable societal debate. Image watermarking allows responsible model owners to detect and label their AI-generated content, which can mitigate the harm. Yet, current state-of-the-art methods in image watermarking remain vulnerable to forgery and removal attacks. This vulnerability occurs in part because watermarks distort the distribution of generated images, unintentionally revealing information about the watermarking techniques.   In this work, we first demonstrate a distortion-free watermarking method for images, based on a diffusion modelâ€™s initial noise. However, detecting the watermark requires comparing the initial noise reconstructed for an image to all previously used initial noises. To mitigate these issues, we propose a two-stage watermarking framework for efficient detection. During generation, we augment the initial noise with generated Fourier patterns to embed information about the group of initial noises we used. For detection, we (i) retrieve the relevant group of noises, and (ii) search within the given group for an initial noise that might match our image. This watermarking approach achieves state-of-the-art robustness to forgery and removal against a large battery of attacks. </p>
<blockquote>
<p>éšç€å›¾åƒç”Ÿæˆå™¨çš„è´¨é‡ä¸æ–­æé«˜ï¼Œæ·±åº¦ä¼ªé€ æŠ€æœ¯æˆä¸ºç¤¾ä¼šçƒ­è®®çš„è¯é¢˜ã€‚å›¾åƒæ°´å°å…è®¸æ¨¡å‹æ‰€æœ‰è€…å¯¹å…¶ç”Ÿæˆçš„AIå†…å®¹è¿›è¡Œæ£€æµ‹å’Œæ ‡è®°ï¼Œä»è€Œå‡è½»æ½œåœ¨çš„ä¼¤å®³ã€‚ç„¶è€Œï¼Œå½“å‰æœ€å…ˆè¿›çš„æ°´å°åµŒå…¥æŠ€æœ¯ä»å®¹æ˜“å—åˆ°ä¼ªé€ å’Œç§»é™¤æ”»å‡»çš„å½±å“ã€‚è¿™ç§è„†å¼±æ€§éƒ¨åˆ†æ˜¯å› ä¸ºæ°´å°ä¼šæ‰­æ›²ç”Ÿæˆçš„å›¾åƒåˆ†å¸ƒï¼Œä»è€Œæ— æ„ä¸­æ³„éœ²æœ‰å…³æ°´å°æŠ€æœ¯çš„ä¿¡æ¯ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆå±•ç¤ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„åˆå§‹å™ªå£°çš„æ— å¤±çœŸæ°´å°åµŒå…¥æ–¹æ³•ã€‚ç„¶è€Œï¼Œæ£€æµ‹æ°´å°éœ€è¦å¯¹æ¯”å›¾åƒé‡å»ºçš„åˆå§‹å™ªå£°ä¸æ‰€æœ‰ä¹‹å‰ä½¿ç”¨çš„åˆå§‹å™ªå£°ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºé«˜æ•ˆæ£€æµ‹çš„ä¸¤é˜¶æ®µæ°´å°æ¡†æ¶ã€‚åœ¨ç”Ÿæˆé˜¶æ®µï¼Œæˆ‘ä»¬é€šè¿‡å°†ç”Ÿæˆçš„å‚…é‡Œå¶æ¨¡å¼æ·»åŠ åˆ°åˆå§‹å™ªå£°ä¸­æ¥åµŒå…¥æœ‰å…³æˆ‘ä»¬ä½¿ç”¨çš„åˆå§‹å™ªå£°ç»„çš„ä¿¡æ¯ã€‚å¯¹äºæ£€æµ‹é˜¶æ®µï¼Œæˆ‘ä»¬ï¼ˆiï¼‰æ£€ç´¢ç›¸å…³çš„å™ªå£°ç»„ï¼Œï¼ˆiiï¼‰åœ¨ç»™å®šçš„å™ªå£°ç»„å†…æœç´¢å¯èƒ½ä¸æˆ‘ä»¬çš„å›¾åƒåŒ¹é…çš„åˆå§‹å™ªå£°ã€‚è¿™ç§æ°´å°æ–¹æ³•è¾¾åˆ°äº†å¯¹æŠ—ä¸€ç³»åˆ—æ”»å‡»çš„å“è¶ŠæŠ—ä¼ªé€ å’Œç§»é™¤æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04653v5">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–°ä¸€ä»£å›¾åƒç”Ÿæˆå™¨è´¨é‡ä¸æ–­æå‡ï¼Œæ·±ä¼ªæŠ€æœ¯æˆä¸ºç¤¾ä¼šçƒ­è®®çš„è¯é¢˜ã€‚å›¾åƒæ°´å°æŠ€æœ¯èƒ½è®©æ¨¡å‹æ‰€æœ‰è€…å¯¹å…¶AIç”Ÿæˆçš„å†…å®¹è¿›è¡Œæ£€æµ‹å’Œæ ‡æ³¨ï¼Œå‡è½»æ½œåœ¨å±å®³ã€‚ç„¶è€Œï¼Œå½“å‰å…ˆè¿›çš„æ°´å°æ–¹æ³•ä»ç„¶å®¹æ˜“å—åˆ°ä¼ªé€ å’Œç§»é™¤æ”»å‡»ã€‚æœ¬æ–‡é¦–æ¬¡å±•ç¤ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹åˆå§‹å™ªå£°çš„æ— å¤±çœŸå›¾åƒæ°´å°æ–¹æ³•ã€‚ä¸ºäº†è§£å†³æ£€æµ‹é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µçš„æ°´å°æ£€æµ‹æ¡†æ¶ã€‚ç”Ÿæˆé˜¶æ®µï¼Œæˆ‘ä»¬é€šè¿‡å°†ç”Ÿæˆçš„å‚…é‡Œå¶æ¨¡å¼ä¸åˆå§‹å™ªå£°ç›¸ç»“åˆï¼ŒåµŒå…¥å…³äºæ‰€ç”¨åˆå§‹å™ªå£°ç»„çš„ä¿¡æ¯ã€‚æ£€æµ‹é˜¶æ®µåˆ†ä¸ºä¸¤æ­¥ï¼šé¦–å…ˆæ‰¾å‡ºç›¸å…³çš„å™ªå£°ç»„ï¼Œç„¶ååœ¨è¯¥ç»„å†…æœç´¢å¯èƒ½ä¸å›¾åƒåŒ¹é…çš„åˆå§‹å™ªå£°ã€‚æ­¤æ°´å°æ–¹æ³•åœ¨å¯¹æŠ—ä¸€ç³»åˆ—æ”»å‡»æ—¶ï¼Œå®ç°äº†å“è¶Šçš„é˜²ä¼ªå’Œé˜²ç§»é™¤æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒç”Ÿæˆå™¨è´¨é‡çš„æå‡å¼•å‘äº†æ·±ä¼ªæŠ€æœ¯çš„ç¤¾ä¼šè®¨è®ºã€‚</li>
<li>å›¾åƒæ°´å°æŠ€æœ¯å…è®¸æ¨¡å‹æ‰€æœ‰è€…æ£€æµ‹å’Œæ ‡æ³¨å…¶AIç”Ÿæˆçš„å†…å®¹ã€‚</li>
<li>å½“å‰å…ˆè¿›çš„æ°´å°æ–¹æ³•å®¹æ˜“å—åˆ°ä¼ªé€ å’Œç§»é™¤æ”»å‡»ï¼Œå­˜åœ¨ç¼ºé™·ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹åˆå§‹å™ªå£°çš„æ— å¤±çœŸå›¾åƒæ°´å°æ–¹æ³•ã€‚</li>
<li>ä¸ºäº†è§£å†³æ£€æµ‹é—®é¢˜ï¼Œé‡‡ç”¨ä¸¤é˜¶æ®µæ°´å°æ£€æµ‹æ¡†æ¶ã€‚</li>
<li>åœ¨ç”Ÿæˆé˜¶æ®µï¼Œç»“åˆå‚…é‡Œå¶æ¨¡å¼ä¸åˆå§‹å™ªå£°åµŒå…¥ä¿¡æ¯ã€‚</li>
<li>æ­¤æ°´å°æ–¹æ³•åœ¨å¯¹æŠ—ä¸€ç³»åˆ—æ”»å‡»æ—¶è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.04653">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-22544c5167522252f4a0edfffaccce6b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c408e9179c0ad8c709dd36f5a51ffece.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9a6680abe6520758c46ecb354591555d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-122dbbeb9da300fbbfb0ba4f9ee5a142.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d1b0dd49c947ecd9f04d958b3d9342ce.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="OmniEdit-Building-Image-Editing-Generalist-Models-Through-Specialist-Supervision"><a href="#OmniEdit-Building-Image-Editing-Generalist-Models-Through-Specialist-Supervision" class="headerlink" title="OmniEdit: Building Image Editing Generalist Models Through Specialist   Supervision"></a>OmniEdit: Building Image Editing Generalist Models Through Specialist   Supervision</h2><p><strong>Authors:Cong Wei, Zheyang Xiong, Weiming Ren, Xinrun Du, Ge Zhang, Wenhu Chen</strong></p>
<p>Instruction-guided image editing methods have demonstrated significant potential by training diffusion models on automatically synthesized or manually annotated image editing pairs. However, these methods remain far from practical, real-life applications. We identify three primary challenges contributing to this gap. Firstly, existing models have limited editing skills due to the biased synthesis process. Secondly, these methods are trained with datasets with a high volume of noise and artifacts. This is due to the application of simple filtering methods like CLIP-score. Thirdly, all these datasets are restricted to a single low resolution and fixed aspect ratio, limiting the versatility to handle real-world use cases. In this paper, we present \omniedit, which is an omnipotent editor to handle seven different image editing tasks with any aspect ratio seamlessly. Our contribution is in four folds: (1) \omniedit is trained by utilizing the supervision from seven different specialist models to ensure task coverage. (2) we utilize importance sampling based on the scores provided by large multimodal models (like GPT-4o) instead of CLIP-score to improve the data quality. (3) we propose a new editing architecture called EditNet to greatly boost the editing success rate, (4) we provide images with different aspect ratios to ensure that our model can handle any image in the wild. We have curated a test set containing images of different aspect ratios, accompanied by diverse instructions to cover different tasks. Both automatic evaluation and human evaluations demonstrate that \omniedit can significantly outperform all the existing models. Our code, dataset and model will be available at <a target="_blank" rel="noopener" href="https://tiger-ai-lab.github.io/OmniEdit/">https://tiger-ai-lab.github.io/OmniEdit/</a> </p>
<blockquote>
<p>åŸºäºæŒ‡ä»¤çš„å›¾åƒç¼–è¾‘æ–¹æ³•é€šè¿‡åœ¨è‡ªåŠ¨åˆæˆæˆ–æ‰‹åŠ¨æ ‡æ³¨çš„å›¾åƒç¼–è¾‘å¯¹ä¸Šè®­ç»ƒæ‰©æ•£æ¨¡å‹ï¼Œå·²ç»å±•ç°å‡ºæ˜¾è‘—æ½œåŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•è·ç¦»å®é™…ã€ç°å®ç”Ÿæ´»åº”ç”¨ä»ç„¶ç›¸å»ç”šè¿œã€‚æˆ‘ä»¬ç¡®å®šäº†å¯¼è‡´è¿™ä¸€å·®è·çš„ä¸‰ä¸ªä¸»è¦æŒ‘æˆ˜ã€‚é¦–å…ˆï¼Œç°æœ‰æ¨¡å‹çš„ç¼–è¾‘èƒ½åŠ›æœ‰é™ï¼Œè¿™æ˜¯ç”±äºåˆæˆè¿‡ç¨‹å­˜åœ¨åè§ã€‚å…¶æ¬¡ï¼Œè¿™äº›æ–¹æ³•ä½¿ç”¨å¤§é‡å™ªå£°å’Œä¼ªå½±çš„æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œè¿™æ˜¯ç”±äºåº”ç”¨äº†ç®€å•çš„è¿‡æ»¤æ–¹æ³•ï¼Œå¦‚CLIPè¯„åˆ†ã€‚ç¬¬ä¸‰ï¼Œæ‰€æœ‰è¿™äº›æ•°æ®é›†éƒ½å±€é™äºå•ä¸€çš„ä½åˆ†è¾¨ç‡å’Œå›ºå®šçš„æ¯”ä¾‹ï¼Œé™åˆ¶äº†å…¶å¤„ç†çœŸå®ä¸–ç•Œç”¨ä¾‹çš„é€šç”¨æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†å…¨èƒ½ç¼–è¾‘å™¨\omnieditï¼Œå®ƒèƒ½å¤Ÿæ— ç¼åœ°å¤„ç†ä¸ƒç§ä¸åŒçš„å›¾åƒç¼–è¾‘ä»»åŠ¡ï¼Œä¸”é€‚ç”¨äºä»»ä½•æ¯”ä¾‹ã€‚æˆ‘ä»¬çš„è´¡çŒ®åœ¨äºå››ä¸ªæ–¹é¢ï¼šï¼ˆ1ï¼‰\omniedité€šè¿‡åˆ©ç”¨ä¸ƒç§ä¸åŒä¸“ä¸šæ¨¡å‹çš„ç›‘ç£æ¥ä¿è¯ä»»åŠ¡è¦†ç›–ã€‚æˆ‘ä»¬åˆ©ç”¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆå¦‚GPT-4oï¼‰æä¾›çš„åˆ†æ•°è¿›è¡Œé‡è¦æ€§é‡‡æ ·ï¼Œè€Œä¸æ˜¯ä½¿ç”¨CLIPè¯„åˆ†æ¥æé«˜æ•°æ®è´¨é‡ã€‚ï¼ˆ3ï¼‰æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ç¼–è¾‘æ¶æ„â€”â€”EditNetï¼Œä»¥æé«˜ç¼–è¾‘æˆåŠŸç‡ã€‚ï¼ˆ4ï¼‰æˆ‘ä»¬æä¾›äº†ä¸åŒæ¯”ä¾‹çš„å›¾ç‰‡ï¼Œä»¥ç¡®ä¿æˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿå¤„ç†ä»»ä½•é‡ç”Ÿå›¾åƒã€‚æˆ‘ä»¬ç²¾å¿ƒå‡†å¤‡äº†ä¸€ä¸ªæµ‹è¯•é›†ï¼Œå…¶ä¸­åŒ…å«ä¸åŒæ¯”ä¾‹çš„å›¾ç‰‡å’Œå¤šæ ·åŒ–çš„æŒ‡ä»¤æ¥æ¶µç›–ä¸åŒçš„ä»»åŠ¡ã€‚è‡ªåŠ¨è¯„ä¼°å’Œäººä¸ºè¯„ä¼°å‡è¡¨æ˜ï¼Œ\omnieditå¯ä»¥æ˜¾è‘—ä¼˜äºæ‰€æœ‰ç°æœ‰æ¨¡å‹ã€‚æˆ‘ä»¬çš„ä»£ç ã€æ•°æ®é›†å’Œæ¨¡å‹å°†åœ¨<a target="_blank" rel="noopener" href="https://tiger-ai-lab.github.io/OmniEdit/%E4%B8%8A%E6%8F%9B%E4%BA%8C%E6%9C%AF%E6%AF%94%E3%80%82">https://tiger-ai-lab.github.io/OmniEdit/ä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.07199v2">PDF</a> 21 pages</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æŒ‡å‡ºäº†å½“å‰æŒ‡ä»¤å¼•å¯¼çš„å›¾åƒç¼–è¾‘æ–¹æ³•åœ¨å®è·µä¸­å­˜åœ¨çš„ä¸‰å¤§æŒ‘æˆ˜ï¼Œå¹¶ä¸ºæ­¤æå‡ºäº†ä¸€ç§å…¨æ–°çš„å›¾åƒç¼–è¾‘æ–¹æ³•â€”â€”OmniEditã€‚OmniEdité€šè¿‡åˆ©ç”¨æ¥è‡ªä¸ƒä¸ªä¸åŒä¸“ä¸šæ¨¡å‹çš„ç›‘ç£ï¼Œå®ç°å¤šç§ä»»åŠ¡è¦†ç›–ï¼›åˆ©ç”¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆå¦‚GPT-4oï¼‰æä¾›çš„åˆ†æ•°è¿›è¡Œé‡è¦æ€§é‡‡æ ·ä»¥æé«˜æ•°æ®è´¨é‡ï¼›æ¨å‡ºæ–°çš„ç¼–è¾‘æ¶æ„EditNetï¼Œæå¤§åœ°æé«˜äº†ç¼–è¾‘æˆåŠŸç‡ï¼›èƒ½å¤„ç†ä¸åŒæ¯”ä¾‹å°ºçš„å›¾åƒï¼Œç¡®ä¿æ¨¡å‹é€‚åº”ä»»ä½•é‡ç”Ÿå›¾åƒã€‚å®éªŒè¯æ˜ï¼ŒOmniEditåœ¨è‡ªåŠ¨è¯„ä¼°å’Œäººå·¥è¯„ä¼°ä¸­éƒ½æ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰æŒ‡ä»¤å¼•å¯¼çš„å›¾åƒç¼–è¾‘æ–¹æ³•å­˜åœ¨ä¸‰å¤§æŒ‘æˆ˜ï¼šæ¨¡å‹ç¼–è¾‘æŠ€èƒ½æœ‰é™ã€æ•°æ®é›†å™ªå£°å’Œä¼ªå½±å¤šã€ä»¥åŠæ•°æ®é›†åˆ†è¾¨ç‡å’Œæ¯”ä¾‹å›ºå®šï¼Œéš¾ä»¥åº”å¯¹çœŸå®ä¸–ç•Œåº”ç”¨ã€‚</li>
<li>OmniEdité€šè¿‡ç»“åˆä¸ƒä¸ªä¸åŒä¸“ä¸šæ¨¡å‹çš„ç›‘ç£ï¼Œå®ç°äº†å¤šç§å›¾åƒç¼–è¾‘ä»»åŠ¡çš„è¦†ç›–ã€‚</li>
<li>OmniEditåˆ©ç”¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹æä¾›çš„åˆ†æ•°è¿›è¡Œé‡è¦æ€§é‡‡æ ·ï¼Œæé«˜äº†æ•°æ®è´¨é‡å’Œç¼–è¾‘æ•ˆæœã€‚</li>
<li>OmniEditå¼•å…¥äº†æ–°çš„ç¼–è¾‘æ¶æ„EditNetï¼Œæ˜¾è‘—æé«˜äº†ç¼–è¾‘æˆåŠŸç‡ã€‚</li>
<li>OmniEditèƒ½å¤Ÿå¤„ç†ä¸åŒæ¯”ä¾‹å°ºçš„å›¾åƒï¼Œå¢å¼ºäº†æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„é€šç”¨æ€§ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒOmniEditåœ¨è‡ªåŠ¨å’Œäººå·¥è¯„ä¼°ä¸­éƒ½ä¼˜äºç°æœ‰æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.07199">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3e8bcfd2e5073b8605d1a3e989cca6cb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e1d197b4715332bade9e676e8eacc47d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef27667995dbf472aebc7251e8a4dc73.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7f4c0e634a818d55ad7e1633626fd6a2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4551d81af5785ce2a4d4511a1c449e6b.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Free-Mask-A-Novel-Paradigm-of-Integration-Between-the-Segmentation-Diffusion-Model-and-Image-Editing"><a href="#Free-Mask-A-Novel-Paradigm-of-Integration-Between-the-Segmentation-Diffusion-Model-and-Image-Editing" class="headerlink" title="Free-Mask: A Novel Paradigm of Integration Between the Segmentation   Diffusion Model and Image Editing"></a>Free-Mask: A Novel Paradigm of Integration Between the Segmentation   Diffusion Model and Image Editing</h2><p><strong>Authors:Bo Gao, Jianhui Wang, Xinyuan Song, Yangfan He, Fangxu Xing, Tianyu Shi</strong></p>
<p>Current semantic segmentation models typically require a substantial amount of manually annotated data, a process that is both time-consuming and resource-intensive. Alternatively, leveraging advanced text-to-image models such as Midjourney and Stable Diffusion has emerged as an efficient strategy, enabling the automatic generation of synthetic data in place of manual annotations. However, previous methods have been limited to generating single-instance images, as the generation of multiple instances with Stable Diffusion has proven unstable. To address this limitation and expand the scope and diversity of synthetic datasets, we propose a framework \textbf{Free-Mask} that combines a Diffusion Model for segmentation with advanced image editing capabilities, allowing for the integration of multiple objects into images via text-to-image models. Our method facilitates the creation of highly realistic datasets that closely emulate open-world environments while generating accurate segmentation masks. It reduces the labor associated with manual annotation and also ensures precise mask generation. Experimental results demonstrate that synthetic data generated by \textbf{Free-Mask} enables segmentation models to outperform those trained on real data, especially in zero-shot settings. Notably, \textbf{Free-Mask} achieves new state-of-the-art results on previously unseen classes in the VOC 2012 benchmark. </p>
<blockquote>
<p>å½“å‰è¯­ä¹‰åˆ†å‰²æ¨¡å‹é€šå¸¸éœ€è¦å¤§é‡æ‰‹åŠ¨æ ‡æ³¨çš„æ•°æ®ï¼Œè¿™ä¸€è¿‡ç¨‹æ—¢è€—æ—¶åˆè€—èµ„æºã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œåˆ©ç”¨å…ˆè¿›çš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ï¼Œå¦‚Midjourneyå’ŒStable Diffusionï¼Œå·²æˆä¸ºä¸€ç§æœ‰æ•ˆç­–ç•¥ï¼Œèƒ½å¤Ÿåœ¨ä¸éœ€è¦æ‰‹åŠ¨æ³¨é‡Šçš„æƒ…å†µä¸‹è‡ªåŠ¨ç”Ÿæˆåˆæˆæ•°æ®ã€‚ç„¶è€Œï¼Œä»¥å‰çš„æ–¹æ³•ä»…é™äºç”Ÿæˆå•ä¸€å®ä¾‹å›¾åƒï¼Œå› ä¸ºä½¿ç”¨Stable Diffusionç”Ÿæˆå¤šä¸ªå®ä¾‹è¢«è¯æ˜æ˜¯ä¸ç¨³å®šçš„ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é™åˆ¶å¹¶æ‰©å¤§åˆæˆæ•°æ®é›†çš„èŒƒå›´å’Œå¤šæ ·æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåä¸ºFree-Maskçš„æ¡†æ¶ï¼Œå®ƒå°†æ‰©æ•£æ¨¡å‹ç”¨äºåˆ†å‰²ï¼Œå¹¶ç»“åˆå…ˆè¿›çš„å›¾åƒç¼–è¾‘åŠŸèƒ½ï¼Œå…è®¸é€šè¿‡æ–‡æœ¬åˆ°å›¾åƒçš„æ¨¡å‹å°†å¤šä¸ªå¯¹è±¡é›†æˆåˆ°å›¾åƒä¸­ã€‚æˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿåˆ›å»ºé«˜åº¦é€¼çœŸçš„æ•°æ®é›†ï¼Œç´§å¯†æ¨¡æ‹Ÿå¼€æ”¾ä¸–ç•Œç¯å¢ƒï¼ŒåŒæ—¶ç”Ÿæˆç²¾ç¡®çš„åˆ†å‰²æ©è†œã€‚å®ƒå‡å°‘äº†ä¸æ‰‹åŠ¨æ³¨é‡Šç›¸å…³çš„åŠ³åŠ¨åŠ›ï¼Œå¹¶ç¡®ä¿ç²¾ç¡®ç”Ÿæˆæ©è†œã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨Free-Maskç”Ÿæˆçš„åˆæˆæ•°æ®è®­ç»ƒçš„åˆ†å‰²æ¨¡å‹åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹è¡¨ç°ä¼˜äºåœ¨çœŸå®æ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒFree-Maskåœ¨VOC 2012åŸºå‡†æµ‹è¯•ä¸­çš„æœªè§ç±»åˆ«ä¸Šå–å¾—äº†æœ€æ–°æœ€å…ˆè¿›çš„æˆæœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.01819v3">PDF</a> 19 pages,11 figures,5 tables</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬æè¿°äº†ä¸€ç§åˆ©ç”¨å…ˆè¿›çš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ï¼ˆå¦‚Midjourneyå’ŒStable Diffusionï¼‰è‡ªåŠ¨ç”Ÿæˆåˆæˆæ•°æ®ä»¥æ›¿ä»£æ‰‹åŠ¨æ ‡æ³¨çš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä»…é™äºç”Ÿæˆå•ä¸€å®ä¾‹å›¾åƒï¼Œä¸”ä¸ç¨³å®šã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜å¹¶æ‰©å¤§åˆæˆæ•°æ®é›†çš„èŒƒå›´å’Œå¤šæ ·æ€§ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆæ‰©æ•£æ¨¡å‹åˆ†å‰²å’Œé«˜çº§å›¾åƒç¼–è¾‘åŠŸèƒ½çš„æ¡†æ¶Free-Maskã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæ•´åˆæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ï¼Œå°†å¤šä¸ªå¯¹è±¡èå…¥å›¾åƒä¸­ï¼Œåˆ›å»ºé«˜åº¦é€¼çœŸçš„æ•°æ®é›†ï¼Œæ¨¡æ‹Ÿå¼€æ”¾ä¸–ç•Œç¯å¢ƒï¼ŒåŒæ—¶ç”Ÿæˆç²¾ç¡®çš„åˆ†å‰²æ©è†œã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFree-Maskç”Ÿæˆçš„åˆæˆæ•°æ®ä½¿åˆ†å‰²æ¨¡å‹åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸­çš„æ€§èƒ½è¶…è¶Šäº†çœŸå®æ•°æ®è®­ç»ƒçš„æ¨¡å‹ï¼Œå¹¶åœ¨VOC 2012åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å‰æ‰€æœªæœ‰çš„æ–°æœ€ä½³ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰è¯­ä¹‰åˆ†å‰²æ¨¡å‹éœ€è¦å¤§é‡æ‰‹åŠ¨æ ‡æ³¨æ•°æ®ï¼Œè¿™ä¸€è¿‡ç¨‹æ—¢è€—æ—¶åˆè€—èµ„æºã€‚</li>
<li>åˆ©ç”¨å…ˆè¿›çš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ï¼ˆå¦‚Midjourneyå’ŒStable Diffusionï¼‰å¯ä»¥è‡ªåŠ¨ç”Ÿæˆåˆæˆæ•°æ®ï¼Œä½œä¸ºæ‰‹åŠ¨æ ‡æ³¨çš„æ›¿ä»£æ–¹æ¡ˆã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦é™äºç”Ÿæˆå•ä¸€å®ä¾‹å›¾åƒï¼Œä¸”ä¸ç¨³å®šã€‚</li>
<li>æå‡ºçš„Free-Maskæ¡†æ¶ç»“åˆäº†æ‰©æ•£æ¨¡å‹åˆ†å‰²å’Œé«˜çº§å›¾åƒç¼–è¾‘åŠŸèƒ½ã€‚</li>
<li>Free-Maskèƒ½å¤Ÿæ•´åˆå¤šä¸ªå¯¹è±¡åˆ°å›¾åƒä¸­ï¼Œåˆ›å»ºé«˜åº¦é€¼çœŸçš„æ•°æ®é›†ï¼Œæ¨¡æ‹Ÿå¼€æ”¾ä¸–ç•Œç¯å¢ƒã€‚</li>
<li>Free-Maskå¯ä»¥ç”Ÿæˆç²¾ç¡®çš„åˆ†å‰²æ©è†œï¼Œå‡å°‘æ‰‹åŠ¨æ ‡æ³¨çš„åŠ³åŠ¨åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.01819">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0101288b649accdb3c663eee33698618.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8eb4370a8bdabb8246ec97a5f4792dcd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c20378698cd9a8bcd331b9edda184625.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ab25a3d52ddb087574cc443353e9f049.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e68cf921b3f4c0d2a447f2fad8569d43.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Infusion-internal-diffusion-for-inpainting-of-dynamic-textures-and-complex-motion"><a href="#Infusion-internal-diffusion-for-inpainting-of-dynamic-textures-and-complex-motion" class="headerlink" title="Infusion: internal diffusion for inpainting of dynamic textures and   complex motion"></a>Infusion: internal diffusion for inpainting of dynamic textures and   complex motion</h2><p><strong>Authors:Nicolas Cherel, AndrÃ©s Almansa, Yann Gousseau, Alasdair Newson</strong></p>
<p>Video inpainting is the task of filling a region in a video in a visually convincing manner. It is very challenging due to the high dimensionality of the data and the temporal consistency required for obtaining convincing results. Recently, diffusion models have shown impressive results in modeling complex data distributions, including images and videos. Such models remain nonetheless very expensive to train and to perform inference with, which strongly reduce their applicability to videos, and yields unreasonable computational loads. We show that in the case of video inpainting, thanks to the highly auto-similar nature of videos, the training data of a diffusion model can be restricted to the input video and still produce very satisfying results. With this internal learning approach, where the training data is limited to a single video, our lightweight models perform very well with only half a million parameters, in contrast to the very large networks with billions of parameters typically found in the literature. We also introduce a new method for efficient training and inference of diffusion models in the context of internal learning, by splitting the diffusion process into different learning intervals corresponding to different noise levels of the diffusion process. We show qualitative and quantitative results, demonstrating that our method reaches or exceeds state of the art performance in the case of dynamic textures and complex dynamic backgrounds </p>
<blockquote>
<p>è§†é¢‘è¡¥å…¨çš„ä»»åŠ¡æ˜¯ä»¥è§†è§‰è¯´æœåŠ›çš„æ–¹å¼å¡«å……è§†é¢‘ä¸­çš„åŒºåŸŸã€‚ç”±äºæ•°æ®çš„é«˜ç»´åº¦å’Œè·å¾—ä»¤äººä¿¡æœç»“æœæ‰€éœ€çš„æ—¶åºä¸€è‡´æ€§ï¼Œè¿™ä½¿å¾—è¯¥ä»»åŠ¡éå¸¸å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æœ€è¿‘ï¼Œæ‰©æ•£æ¨¡å‹åœ¨å»ºæ¨¡å¤æ‚æ•°æ®åˆ†å¸ƒæ–¹é¢å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœï¼ŒåŒ…æ‹¬å›¾åƒå’Œè§†é¢‘ã€‚ç„¶è€Œï¼Œè¿™æ ·çš„æ¨¡å‹åœ¨è®­ç»ƒå’Œè¿›è¡Œæ¨ç†æ—¶ä»ç„¶éå¸¸æ˜‚è´µï¼Œå¤§å¤§é™ä½äº†å®ƒä»¬å¯¹è§†é¢‘çš„é€‚ç”¨æ€§ï¼Œå¹¶äº§ç”Ÿäº†ä¸åˆç†çš„è®¡ç®—è´Ÿè½½ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œåœ¨è§†é¢‘è¡¥å…¨çš„æƒ…å†µä¸‹ï¼Œç”±äºè§†é¢‘çš„å›ºæœ‰é«˜åº¦è‡ªç›¸ä¼¼æ€§ï¼Œæ‰©æ•£æ¨¡å‹çš„è®­ç»ƒæ•°æ®å¯ä»¥å±€é™äºè¾“å…¥è§†é¢‘ï¼Œä½†ä»èƒ½äº§ç”Ÿéå¸¸ä»¤äººæ»¡æ„çš„ç»“æœã€‚é‡‡ç”¨è¿™ç§å†…éƒ¨å­¦ä¹ æ–¹æ³•ï¼Œè®­ç»ƒæ•°æ®ä»…é™äºå•ä¸ªè§†é¢‘ï¼Œæˆ‘ä»¬çš„è½»é‡çº§æ¨¡å‹è¡¨ç°è‰¯å¥½ï¼Œä»…ä½¿ç”¨äº”ç™¾ä¸‡ä¸ªå‚æ•°ï¼Œä¸æ–‡çŒ®ä¸­é€šå¸¸å‘ç°çš„ä½¿ç”¨æ•°åäº¿å‚æ•°çš„åºå¤§ç½‘ç»œå½¢æˆé²œæ˜å¯¹æ¯”ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†ä¸€ç§æ–°çš„é«˜æ•ˆè®­ç»ƒå’Œæ¨ç†æ‰©æ•£æ¨¡å‹çš„æ–¹æ³•ï¼Œå®ƒæ˜¯åœ¨å†…éƒ¨å­¦ä¹ çš„èƒŒæ™¯ä¸‹ï¼Œé€šè¿‡å°†æ‰©æ•£è¿‡ç¨‹åˆ†å‰²æˆå¯¹åº”äºæ‰©æ•£è¿‡ç¨‹ä¸åŒå™ªå£°æ°´å¹³çš„ä¸åŒå­¦ä¹ é—´éš”æ¥è¿›è¡Œã€‚æˆ‘ä»¬å±•ç¤ºäº†å®šæ€§å’Œå®šé‡çš„ç»“æœï¼Œè¯æ˜æˆ‘ä»¬çš„æ–¹æ³•åœ¨åŠ¨æ€çº¹ç†å’Œå¤æ‚åŠ¨æ€èƒŒæ™¯çš„æ¡ˆä¾‹ä¸­è¾¾åˆ°äº†æˆ–è¶…è¿‡äº†æœ€æ–°æŠ€æœ¯çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01090v4">PDF</a> 14 pages, 11 figures. Published in Eurographics 2025</p>
<p><strong>Summary</strong><br>     è§†é¢‘è¡¥å…¨ä»»åŠ¡æ—¨åœ¨ä»¥è§†è§‰è¯´æœåŠ›çš„æ–¹å¼å¡«å……è§†é¢‘ä¸­çš„åŒºåŸŸã€‚ç”±äºæ•°æ®çš„é«˜ç»´åº¦æ€§å’Œè·å¾—ä»¤äººä¿¡æœç»“æœæ‰€éœ€çš„æ—¶åºä¸€è‡´æ€§ï¼Œè¿™ä½¿å¾—ä»»åŠ¡éå¸¸å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æœ€è¿‘ï¼Œæ‰©æ•£æ¨¡å‹åœ¨å»ºæ¨¡å¤æ‚æ•°æ®åˆ†å¸ƒï¼ˆåŒ…æ‹¬å›¾åƒå’Œè§†é¢‘ï¼‰æ–¹é¢æ˜¾ç¤ºå‡ºä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœã€‚ç„¶è€Œï¼Œæ‰©æ•£æ¨¡å‹åœ¨è®­ç»ƒå’Œè¿›è¡Œæ¨ç†æ—¶ä»ç„¶éå¸¸æ˜‚è´µï¼Œå¤§å¤§é™ä½äº†å…¶åº”ç”¨äºè§†é¢‘çš„å®ç”¨æ€§ï¼Œå¹¶äº§ç”Ÿäº†ä¸åˆç†çš„è®¡ç®—è´Ÿè½½ã€‚é’ˆå¯¹è§†é¢‘è¡¥å…¨ä»»åŠ¡ï¼Œæˆ‘ä»¬åˆ©ç”¨è§†é¢‘çš„é«˜åº¦è‡ªç›¸ä¼¼æ€§è´¨ï¼Œå±•ç¤ºäº†æ‰©æ•£æ¨¡å‹å¯ä»¥é€šè¿‡ä»…ä½¿ç”¨è¾“å…¥è§†é¢‘ä½œä¸ºè®­ç»ƒæ•°æ®æ¥äº§ç”Ÿä»¤äººæ»¡æ„çš„ç»“æœã€‚é€šè¿‡å†…éƒ¨å­¦ä¹ æ–¹æ³•ï¼Œæˆ‘ä»¬çš„è½»é‡çº§æ¨¡å‹åœ¨åªæœ‰äº”ç™¾ä¸‡å‚æ•°çš„æƒ…å†µä¸‹è¡¨ç°è‰¯å¥½ï¼Œè¿™ä¸æ–‡çŒ®ä¸­é€šå¸¸å‘ç°çš„å¤§è§„æ¨¡ç½‘ç»œå½¢æˆé²œæ˜å¯¹æ¯”ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†ä¸€ç§åœ¨å†…éƒ¨å­¦ä¹ çš„èƒŒæ™¯ä¸‹å¯¹æ‰©æ•£æ¨¡å‹è¿›è¡Œé«˜æ•ˆè®­ç»ƒå’Œæ¨ç†çš„æ–°æ–¹æ³•ï¼Œé€šè¿‡å°†æ‰©æ•£è¿‡ç¨‹åˆ†æˆå¯¹åº”äºæ‰©æ•£è¿‡ç¨‹ä¸åŒå™ªå£°æ°´å¹³çš„å­¦ä¹ é—´éš”ã€‚æˆ‘ä»¬å±•ç¤ºäº†å®šæ€§å’Œå®šé‡çš„ç»“æœï¼Œè¯æ˜æˆ‘ä»¬çš„æ–¹æ³•åœ¨åŠ¨æ€çº¹ç†å’Œå¤æ‚åŠ¨æ€èƒŒæ™¯çš„æƒ…å†µä¸‹è¾¾åˆ°äº†æˆ–è¶…è¿‡äº†æœ€æ–°æŠ€æœ¯çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†é¢‘è¡¥å…¨ä»»åŠ¡å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºéœ€è¦å¤„ç†é«˜ç»´åº¦æ•°æ®å’Œä¿è¯æ—¶åºä¸€è‡´æ€§ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åœ¨å»ºæ¨¡å¤æ‚æ•°æ®åˆ†å¸ƒï¼ˆå¦‚å›¾åƒå’Œè§†é¢‘ï¼‰æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹çš„è®­ç»ƒå’Œæ¨ç†æˆæœ¬é«˜æ˜‚ï¼Œé™åˆ¶äº†å…¶åœ¨å®é™…è§†é¢‘åº”ç”¨ä¸­çš„ä½¿ç”¨ã€‚</li>
<li>è§†é¢‘çš„è‡ªæˆ‘ç›¸ä¼¼æ€§ä½¿å¾—ä»…ä½¿ç”¨è¾“å…¥è§†é¢‘ä½œä¸ºè®­ç»ƒæ•°æ®æˆä¸ºå¯èƒ½ã€‚</li>
<li>é€šè¿‡å†…éƒ¨å­¦ä¹ æ–¹æ³•ï¼Œè½»é‡çº§æ‰©æ•£æ¨¡å‹åœ¨åªæœ‰å°‘é‡å‚æ•°çš„æƒ…å†µä¸‹è¡¨ç°è‰¯å¥½ã€‚</li>
<li>ä»‹ç»äº†ä¸€ç§é€šè¿‡åˆ†å‰²æ‰©æ•£è¿‡ç¨‹æ¥é«˜æ•ˆè®­ç»ƒå’Œæ¨ç†æ‰©æ•£æ¨¡å‹çš„æ–°æ–¹æ³•ï¼Œå¯¹åº”ä¸åŒçš„å™ªå£°æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2311.01090">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d808e055ac0dec09813c4ace5b632b00.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-20a4c25b7d497577d101ba57a62ab425.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-af5411e14789d5434717749e0cb1e8ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1750a328ea34107eb0cd0a9787fbbf8a.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-30/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-30/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-30/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-5995b096b27118f607b5d98b2c6e3c25.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-30  Breast Cancer Detection from Multi-View Screening Mammograms with Visual   Prompt Tuning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-30
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-30/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-9d1d8a2d3376eb8bcec01d470d25c2b5.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-30  Joint Optimization of Neural Radiance Fields and Continuous Camera   Motion from a Monocular Video
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-30
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">27348.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
