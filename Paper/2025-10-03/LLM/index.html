<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-03  Beyond the Algorithm A Field Guide to Deploying AI Agents in Clinical   Practice">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-3d52d5228ee62a219a9444fa4da71d49~resize:0:q75.jpg?source=1f5c5e47&expiration=1760084237&auth_key=1760084237-0-0-30cb4e3d4574ff2cf917b9511e5e07c8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-03
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-22
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    80 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-10-03-æ›´æ–°"><a href="#2025-10-03-æ›´æ–°" class="headerlink" title="2025-10-03 æ›´æ–°"></a>2025-10-03 æ›´æ–°</h1><h2 id="Beyond-the-Algorithm-A-Field-Guide-to-Deploying-AI-Agents-in-Clinical-Practice"><a href="#Beyond-the-Algorithm-A-Field-Guide-to-Deploying-AI-Agents-in-Clinical-Practice" class="headerlink" title="Beyond the Algorithm: A Field Guide to Deploying AI Agents in Clinical   Practice"></a>Beyond the Algorithm: A Field Guide to Deploying AI Agents in Clinical   Practice</h2><p><strong>Authors:Jack Gallifant, Katherine C. Kellogg, Matt Butler, Amanda Centi, Shan Chen, Patrick F. Doyle, Sayon Dutta, Joyce Guo, Matthew J. Hadfield, Esther H. Kim, David E. Kozono, Hugo JWL Aerts, Adam B. Landman, Raymond H. Mak, Rebecca G. Mishuris, Tanna L. Nelson, Guergana K. Savova, Elad Sharon, Benjamin C. Silverman, Umit Topaloglu, Jeremy L. Warner, Danielle S. Bitterman</strong></p>
<p>Large language models (LLMs) integrated into agent-driven workflows hold immense promise for healthcare, yet a significant gap exists between their potential and practical implementation within clinical settings. To address this, we present a practitioner-oriented field manual for deploying generative agents that use electronic health record (EHR) data. This guide is informed by our experience deploying the â€œirAE-Agentâ€, an automated system to detect immune-related adverse events from clinical notes at Mass General Brigham, and by structured interviews with 20 clinicians, engineers, and informatics leaders involved in the project. Our analysis reveals a critical misalignment in clinical AI development: less than 20% of our effort was dedicated to prompt engineering and model development, while over 80% was consumed by the sociotechnical work of implementation. We distill this effort into five â€œheavy liftsâ€: data integration, model validation, ensuring economic value, managing system drift, and governance. By providing actionable solutions for each of these challenges, this field manual shifts the focus from algorithmic development to the essential infrastructure and implementation work required to bridge the â€œvalley of deathâ€ and successfully translate generative AI from pilot projects into routine clinical care. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¢«é›†æˆåˆ°ä»£ç†é©±åŠ¨çš„å·¥ä½œæµä¸­ï¼Œä¸ºåŒ»ç–—ä¿å¥é¢†åŸŸå¸¦æ¥äº†å·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œåœ¨ä¸´åºŠç¯å¢ƒä¸­ï¼Œå®ƒä»¬çš„æ½œåŠ›ä¸å®é™…å®æ–½ä¹‹é—´å­˜åœ¨å¾ˆå¤§çš„å·®è·ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬é¢å‘å®è·µè€…æ¨å‡ºäº†ä½¿ç”¨ç”µå­å¥åº·è®°å½•ï¼ˆEHRï¼‰æ•°æ®çš„ç”Ÿæˆä»£ç†éƒ¨ç½²ç°åœºæ‰‹å†Œã€‚æœ¬æŒ‡å—çš„æ’°å†™å‚è€ƒäº†æˆ‘ä»¬éƒ¨ç½²â€œirAE-Agentâ€çš„ç»éªŒï¼Œè¿™æ˜¯ä¸€ä¸ªåœ¨Mass General Brighamä»ä¸´åºŠç¬”è®°ä¸­æ£€æµ‹å…ç–«ç›¸å…³ä¸è‰¯äº‹ä»¶çš„è‡ªåŠ¨åŒ–ç³»ç»Ÿã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å‚è€ƒäº†å¯¹å‚ä¸è¯¥é¡¹ç›®çš„20åä¸´åºŠåŒ»ç”Ÿã€å·¥ç¨‹å¸ˆå’Œä¿¡æ¯æŠ€æœ¯é¢†å¯¼çš„ç»“æ„æ€§è®¿è°ˆã€‚æˆ‘ä»¬çš„åˆ†ææ­ç¤ºäº†ä¸´åºŠäººå·¥æ™ºèƒ½å‘å±•ä¸­çš„å…³é”®ä¸åŒ¹é…é—®é¢˜ï¼šæˆ‘ä»¬çš„åŠªåŠ›ä¸­ä¸åˆ°20%è‡´åŠ›äºå³æ—¶å·¥ç¨‹å’Œæ¨¡å‹å¼€å‘ï¼Œè€Œè¶…è¿‡80%çš„åŠªåŠ›å´ç”¨äºå®æ–½çš„ç¤¾ä¼šæŠ€æœ¯å·¥ä½œã€‚æˆ‘ä»¬å°†è¿™ä¸€åŠªåŠ›å½’çº³ä¸ºäº”é¡¹â€œé‡ç‚¹ä»»åŠ¡â€ï¼šæ•°æ®é›†æˆã€æ¨¡å‹éªŒè¯ã€ç¡®ä¿ç»æµä»·å€¼ã€ç®¡ç†ç³»ç»Ÿæ¼‚ç§»å’Œæ²»ç†ã€‚é€šè¿‡ä¸ºè¿™äº›æŒ‘æˆ˜æä¾›å¯è¡Œçš„è§£å†³æ–¹æ¡ˆï¼Œè¿™æœ¬ç°åœºæ‰‹å†Œå°†é‡ç‚¹ä»ç®—æ³•å¼€å‘è½¬å‘å¿…è¦çš„åŸºç¡€è®¾æ–½å’Œå®æ–½å·¥ä½œï¼Œä»¥å¡«è¡¥â€œæ­»äº¡ä¹‹è°·â€ï¼ŒæˆåŠŸåœ°å°†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ä»è¯•ç‚¹é¡¹ç›®è½¬åŒ–ä¸ºå¸¸è§„çš„åŒ»ç–—æœåŠ¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.26153v2">PDF</a> Under review. 5 Tables, 2 Figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åŒ»ç–—é¢†åŸŸå…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œä½†å…¶åœ¨ä¸´åºŠç¯å¢ƒä¸­çš„å®é™…åº”ç”¨ä¸å…¶æ½œåŠ›ä¹‹é—´å­˜åœ¨è¾ƒå¤§å·®è·ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æä¾›äº†ä¸€æœ¬é¢å‘å®è·µçš„é¢†åŸŸæ‰‹å†Œï¼Œä»‹ç»å¦‚ä½•éƒ¨ç½²ä½¿ç”¨ç”µå­å¥åº·è®°å½•ï¼ˆEHRï¼‰æ•°æ®çš„ç”Ÿæˆä»£ç†ã€‚è¯¥æ‰‹å†ŒåŸºäºéƒ¨ç½²â€œirAE-Agentâ€ç³»ç»Ÿçš„ç»éªŒï¼Œè¯¥ç³»ç»Ÿç”¨äºä»ä¸´åºŠç¬”è®°ä¸­è‡ªåŠ¨æ£€æµ‹ä¸å…ç–«ç›¸å…³çš„ä¸è‰¯äº‹ä»¶ã€‚é€šè¿‡å¯¹20åå‚ä¸è¯¥é¡¹ç›®çš„ä¸´åºŠåŒ»ç”Ÿã€å·¥ç¨‹å¸ˆå’Œä¿¡æ¯æŠ€æœ¯é¢†å¯¼çš„ç»“æ„æ€§è®¿è°ˆï¼Œå‘ç°ä¸´åºŠäººå·¥æ™ºèƒ½å¼€å‘ä¸­å­˜åœ¨å…³é”®çš„ä¸åŒ¹é…é—®é¢˜ï¼šä»…ä¸åˆ°20%çš„åŠªåŠ›ç”¨äºå³æ—¶å·¥ç¨‹å¼€å‘å’Œæ¨¡å‹å¼€å‘ï¼Œè€Œè¶…è¿‡80%çš„åŠªåŠ›è¢«å®æ–½çš„ç¤¾æŠ€æœ¯å·¥ä½œæ‰€å æ®ã€‚æ‰‹å†Œæå‡ºäº†è§£å†³è¿™äº”ä¸ªæŒ‘æˆ˜çš„å¯æ“ä½œæ–¹æ¡ˆï¼šæ•°æ®é›†æˆã€æ¨¡å‹éªŒè¯ã€ç¡®ä¿ç»æµä»·å€¼ã€ç®¡ç†ç³»ç»Ÿæ¼‚ç§»å’Œæ²»ç†ã€‚é‡ç‚¹ä»ç®—æ³•å¼€å‘è½¬å‘å¿…è¦çš„åŸºç¡€è®¾æ–½å’Œå®æ–½å·¥ä½œï¼Œä»¥æˆåŠŸå°†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ä»è¯•ç‚¹é¡¹ç›®è½¬åŒ–ä¸ºå¸¸è§„ä¸´åºŠæŠ¤ç†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åŒ»ç–—é¢†åŸŸå…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œä½†å®é™…åº”ç”¨ä¸æ½œåŠ›ä¹‹é—´å­˜åœ¨å·®è·ã€‚</li>
<li>éƒ¨ç½²ç”Ÿæˆä»£ç†ç³»ç»Ÿéœ€è¦ä½¿ç”¨ç”µå­å¥åº·è®°å½•ï¼ˆEHRï¼‰æ•°æ®ã€‚</li>
<li>ä¸´åºŠäººå·¥æ™ºèƒ½å¼€å‘ä¸­å­˜åœ¨å…³é”®ä¸åŒ¹é…é—®é¢˜ï¼Œå³ç®—æ³•å¼€å‘ä¸å®æ–½å·¥ä½œçš„æ¯”ä¾‹å¤±è¡¡ã€‚</li>
<li>å®æ–½å·¥ä½œä¸­å­˜åœ¨äº”ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šæ•°æ®é›†æˆã€æ¨¡å‹éªŒè¯ã€ç¡®ä¿ç»æµä»·å€¼ã€ç®¡ç†ç³»ç»Ÿæ¼‚ç§»å’Œæ²»ç†ã€‚</li>
<li>æ‰‹å†Œæä¾›é’ˆå¯¹è¿™äº›æŒ‘æˆ˜çš„å…·ä½“å¯æ“ä½œè§£å†³æ–¹æ¡ˆã€‚</li>
<li>æ‰‹å†Œçš„ç„¦ç‚¹ä»ç®—æ³•å¼€å‘è½¬å‘å¿…è¦çš„åŸºç¡€è®¾æ–½å’Œå®æ–½å·¥ä½œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.26153">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-ea05f0c992a5b3aff5cef348c76e9cc1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760084244&auth_key=1760084244-0-0-95e6bfffb4c9586bc90727a8d1cfdfff&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Automatically-Generating-Web-Applications-from-Requirements-Via-Multi-Agent-Test-Driven-Development"><a href="#Automatically-Generating-Web-Applications-from-Requirements-Via-Multi-Agent-Test-Driven-Development" class="headerlink" title="Automatically Generating Web Applications from Requirements Via   Multi-Agent Test-Driven Development"></a>Automatically Generating Web Applications from Requirements Via   Multi-Agent Test-Driven Development</h2><p><strong>Authors:Yuxuan Wan, Tingshuo Liang, Jiakai Xu, Jingyu Xiao, Yintong Huo, Michael R. Lyu</strong></p>
<p>Developing full-stack web applications is complex and time-intensive, demanding proficiency across diverse technologies and frameworks. Although recent advances in multimodal large language models (MLLMs) enable automated webpage generation from visual inputs, current solutions remain limited to front-end tasks and fail to deliver fully functional applications. In this work, we introduce TDDev, the first test-driven development (TDD)-enabled LLM-agent framework for end-to-end full-stack web application generation. Given a natural language description or design image, TDDev automatically derives executable test cases, generates front-end and back-end code, simulates user interactions, and iteratively refines the implementation until all requirements are satisfied. Our framework addresses key challenges in full-stack automation, including underspecified user requirements, complex interdependencies among multiple files, and the need for both functional correctness and visual fidelity. Through extensive experiments on diverse application scenarios, TDDev achieves a 14.4% improvement on overall accuracy compared to state-of-the-art baselines, demonstrating its effectiveness in producing reliable, high-quality web applications without requiring manual intervention. </p>
<blockquote>
<p>å¼€å‘å…¨æ ˆwebåº”ç”¨ç¨‹åºæ˜¯ä¸€é¡¹å¤æ‚ä¸”è€—æ—¶çš„å·¥ä½œï¼Œéœ€è¦æŒæ¡å¤šç§æŠ€æœ¯å’Œæ¡†æ¶ã€‚å°½ç®¡æœ€è¿‘çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è¿›æ­¥èƒ½å¤Ÿå®ç°ä»è§†è§‰è¾“å…¥è‡ªåŠ¨åŒ–ç”Ÿæˆç½‘é¡µï¼Œä½†å½“å‰è§£å†³æ–¹æ¡ˆä»…é™äºå‰ç«¯ä»»åŠ¡ï¼Œæ— æ³•æä¾›å®Œæ•´çš„åŠŸèƒ½åº”ç”¨ç¨‹åºã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†TDDevï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªæ”¯æŒç«¯åˆ°ç«¯å…¨æ ˆwebåº”ç”¨ç¨‹åºç”Ÿæˆçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†æ¡†æ¶ï¼Œå…·æœ‰æµ‹è¯•é©±åŠ¨å¼€å‘ï¼ˆTDDï¼‰åŠŸèƒ½ã€‚ç»™å®šè‡ªç„¶è¯­è¨€æè¿°æˆ–è®¾è®¡å›¾åƒï¼ŒTDDevè‡ªåŠ¨æ¨å¯¼å¯æ‰§è¡Œæµ‹è¯•ç”¨ä¾‹ï¼Œç”Ÿæˆå‰ç«¯å’Œåç«¯ä»£ç ï¼Œæ¨¡æ‹Ÿç”¨æˆ·äº¤äº’ï¼Œå¹¶è¿­ä»£å®Œå–„å®ç°ï¼Œç›´åˆ°æ»¡è¶³æ‰€æœ‰è¦æ±‚ã€‚æˆ‘ä»¬çš„æ¡†æ¶è§£å†³äº†å…¨æ ˆè‡ªåŠ¨åŒ–çš„å…³é”®æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ç”¨æˆ·è¦æ±‚ä¸æ˜ç¡®ã€å¤šä¸ªæ–‡ä»¶ä¹‹é—´çš„å¤æ‚ç›¸äº’ä¾èµ–å…³ç³»ä»¥åŠåŠŸèƒ½å’Œè§†è§‰ä¿çœŸåº¦çš„éœ€æ±‚ã€‚é€šè¿‡å¯¹å¤šç§åº”ç”¨åœºæ™¯çš„å¹¿æ³›å®éªŒï¼ŒTDDevåœ¨æ•´ä½“å‡†ç¡®æ€§æ–¹é¢æ¯”æœ€æ–°åŸºçº¿æé«˜äº†14.4%ï¼Œè¯æ˜äº†å…¶åœ¨æ— éœ€äººå·¥å¹²é¢„çš„æƒ…å†µä¸‹ï¼Œç”Ÿäº§å¯é ã€é«˜è´¨é‡webåº”ç”¨ç¨‹åºçš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25297v2">PDF</a> </p>
<p><strong>Summary</strong><br>åŸºäºè‡ªç„¶è¯­è¨€æè¿°æˆ–è®¾è®¡å›¾åƒï¼ŒTDDevæ¡†æ¶èƒ½å¤Ÿè‡ªåŠ¨æ´¾ç”Ÿå‡ºå¯æ‰§è¡Œæµ‹è¯•ç”¨ä¾‹ï¼Œç”Ÿæˆå‰ç«¯å’Œåç«¯ä»£ç ï¼Œæ¨¡æ‹Ÿç”¨æˆ·äº¤äº’ï¼Œå¹¶è¿­ä»£å®Œå–„å®ç°ï¼Œç›´è‡³æ»¡è¶³æ‰€æœ‰è¦æ±‚ã€‚å®ƒè§£å†³äº†å…¨æ ˆè‡ªåŠ¨åŒ–é¢ä¸´çš„å…³é”®æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ç”¨æˆ·éœ€æ±‚ä¸æ˜ç¡®ã€å¤šä¸ªæ–‡ä»¶ä¹‹é—´çš„å¤æ‚ç›¸äº’ä¾èµ–å…³ç³»ï¼Œä»¥åŠåŠŸèƒ½æ­£ç¡®æ€§å’Œè§†è§‰ä¿çœŸåº¦çš„éœ€æ±‚ã€‚åœ¨å¤šæ ·åŒ–çš„åº”ç”¨åœºæ™¯ä¸‹è¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒTDDevåœ¨æ•´ä½“å‡†ç¡®æ€§æ–¹é¢ç›¸æ¯”æœ€æ–°åŸºçº¿æœ‰14.4%çš„æå‡ï¼Œè¯æ˜äº†å…¶åœ¨æ— éœ€äººå·¥å¹²é¢„çš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿå¯é ã€é«˜è´¨é‡åœ°ç”ŸæˆWebåº”ç”¨ç¨‹åºçš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TDDevæ˜¯ä¸€ä¸ªæµ‹è¯•é©±åŠ¨å¼€å‘ï¼ˆTDDï¼‰èµ‹èƒ½çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†æ¡†æ¶ï¼Œç”¨äºç«¯åˆ°ç«¯çš„å…¨æ ˆWebåº”ç”¨ç¨‹åºç”Ÿæˆã€‚</li>
<li>TDDevèƒ½å¤Ÿä»è‡ªç„¶è¯­è¨€æè¿°æˆ–è®¾è®¡å›¾åƒå‡ºå‘ï¼Œè‡ªåŠ¨æ¨å¯¼å¯æ‰§è¡Œæµ‹è¯•ç”¨ä¾‹ã€‚</li>
<li>è¯¥æ¡†æ¶èƒ½å¤Ÿç”Ÿæˆå‰ç«¯å’Œåç«¯ä»£ç ï¼Œå¹¶æ¨¡æ‹Ÿç”¨æˆ·äº¤äº’ã€‚</li>
<li>TDDevé€šè¿‡è¿­ä»£å®Œå–„å®ç°ï¼Œç›´è‡³æ»¡è¶³æ‰€æœ‰è¦æ±‚ï¼Œè§£å†³äº†å…¨æ ˆè‡ªåŠ¨åŒ–ä¸­çš„å…³é”®æŒ‘æˆ˜ã€‚</li>
<li>TDDevæ¡†æ¶å¤„ç†çš„æŒ‘æˆ˜åŒ…æ‹¬ç”¨æˆ·éœ€æ±‚ä¸æ˜ç¡®ã€å¤šä¸ªæ–‡ä»¶é—´çš„å¤æ‚ç›¸äº’ä¾èµ–å…³ç³»ï¼Œä»¥åŠåŠŸèƒ½æ­£ç¡®æ€§å’Œè§†è§‰ä¿çœŸåº¦çš„éœ€æ±‚ã€‚</li>
<li>åœ¨å¤šç§åº”ç”¨åœºæ™¯ä¸‹çš„å¹¿æ³›å®éªŒä¸­ï¼ŒTDDevç›¸æ¯”æœ€æ–°åŸºçº¿åœ¨æ•´ä½“å‡†ç¡®æ€§æ–¹é¢æœ‰æ˜¾è‘—çš„æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25297">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-c4970290a4ac709eabdaca113ca7c0ba~resize:0:q75.jpg?source=1f5c5e47&expiration=1760084251&auth_key=1760084251-0-0-ba2f05f5d17ec5f3a2015a14af900a1c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6726b5ba0ca97f2ccd8387206efbf139~resize:0:q75.jpg?source=1f5c5e47&expiration=1760084258&auth_key=1760084258-0-0-82a856a2358a22e92c8f2bc9fe657867&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-08c818c652dd7b7c9d182d07aa472825~resize:0:q75.jpg?source=1f5c5e47&expiration=1760084265&auth_key=1760084265-0-0-a34f5ae589fdad89dd02afcbf40afed6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-28b69df8523b86ea64d0a0122ecf2f18~resize:0:q75.jpg?source=1f5c5e47&expiration=1760084272&auth_key=1760084272-0-0-68e04ad4cb2bf0d60630f00d9d5c6d30&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Metaphor-identification-using-large-language-models-A-comparison-of-RAG-prompt-engineering-and-fine-tuning"><a href="#Metaphor-identification-using-large-language-models-A-comparison-of-RAG-prompt-engineering-and-fine-tuning" class="headerlink" title="Metaphor identification using large language models: A comparison of   RAG, prompt engineering, and fine-tuning"></a>Metaphor identification using large language models: A comparison of   RAG, prompt engineering, and fine-tuning</h2><p><strong>Authors:Matteo Fuoli, Weihang Huang, Jeannette Littlemore, Sarah Turner, Ellen Wilding</strong></p>
<p>Metaphor is a pervasive feature of discourse and a powerful lens for examining cognition, emotion, and ideology. Large-scale analysis, however, has been constrained by the need for manual annotation due to the context-sensitive nature of metaphor. This study investigates the potential of large language models (LLMs) to automate metaphor identification in full texts. We compare three methods: (i) retrieval-augmented generation (RAG), where the model is provided with a codebook and instructed to annotate texts based on its rules and examples; (ii) prompt engineering, where we design task-specific verbal instructions; and (iii) fine-tuning, where the model is trained on hand-coded texts to optimize performance. Within prompt engineering, we test zero-shot, few-shot, and chain-of-thought strategies. Our results show that state-of-the-art closed-source LLMs can achieve high accuracy, with fine-tuning yielding a median F1 score of 0.79. A comparison of human and LLM outputs reveals that most discrepancies are systematic, reflecting well-known grey areas and conceptual challenges in metaphor theory. We propose that LLMs can be used to at least partly automate metaphor identification and can serve as a testbed for developing and refining metaphor identification protocols and the theory that underpins them. </p>
<blockquote>
<p>éšå–»æ˜¯è¯è¯­çš„æ™®éç‰¹å¾ï¼Œæ˜¯è§‚å¯Ÿè®¤çŸ¥ã€æƒ…æ„Ÿå’Œæ„è¯†å½¢æ€çš„å¼ºå¤§å·¥å…·ã€‚ç„¶è€Œï¼Œç”±äºéšå–»çš„è¯­å¢ƒæ•æ„Ÿæ€§ï¼Œå¤§è§„æ¨¡åˆ†æä¸€ç›´å—åˆ°éœ€è¦æ‰‹åŠ¨æ³¨é‡Šçš„åˆ¶çº¦ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å…¨è‡ªåŠ¨æ–‡æœ¬éšå–»è¯†åˆ«ä¸­çš„æ½œåŠ›ã€‚æˆ‘ä»¬æ¯”è¾ƒäº†ä¸‰ç§æ–¹æ³•ï¼šï¼ˆiï¼‰æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ï¼Œè¯¥æ–¹æ³•å‘æ¨¡å‹æä¾›ä»£ç æœ¬ï¼Œå¹¶æŒ‡ç¤ºå…¶æ ¹æ®è§„åˆ™å’Œç¤ºä¾‹å¯¹æ–‡æœ¬è¿›è¡Œæ³¨é‡Šï¼›ï¼ˆiiï¼‰æç¤ºå·¥ç¨‹ï¼Œæˆ‘ä»¬è®¾è®¡é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„è¯­è¨€æŒ‡ä»¤ï¼›ä»¥åŠï¼ˆiiiï¼‰å¾®è°ƒï¼Œåœ¨è¯¥æ–¹æ³•ä¸­ï¼Œæ¨¡å‹åœ¨æ‰‹åŠ¨ç¼–ç çš„æ–‡æœ¬ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä»¥ä¼˜åŒ–æ€§èƒ½ã€‚åœ¨æç¤ºå·¥ç¨‹ä¸­ï¼Œæˆ‘ä»¬æµ‹è¯•äº†é›¶æ ·æœ¬ã€å°‘æ ·æœ¬å’Œé“¾å¼æ€ç»´ç­–ç•¥ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œæœ€å…ˆè¿›çš„å°é—­å¼LLMå¯ä»¥è¾¾åˆ°é«˜å‡†ç¡®ç‡ï¼Œå¾®è°ƒåçš„ä¸­ä½æ•°F1åˆ†æ•°ä¸º0.79ã€‚å¯¹æ¯”äººç±»å’ŒLLMçš„è¾“å‡ºç»“æœæ˜¾ç¤ºï¼Œå¤§å¤šæ•°å·®å¼‚æ˜¯ç³»ç»Ÿçš„ï¼Œåæ˜ äº†éšå–»ç†è®ºä¸­çš„ç°è‰²åœ°å¸¦å’Œæ¦‚å¿µæŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºï¼ŒLLMè‡³å°‘å¯ä»¥éƒ¨åˆ†ç”¨äºè‡ªåŠ¨éšå–»è¯†åˆ«ï¼Œå¹¶å¯ä»¥ä½œä¸ºå¼€å‘å’Œå®Œå–„éšå–»è¯†åˆ«åè®®åŠå…¶ç†è®ºåŸºç¡€çš„æµ‹è¯•å¹³å°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24866v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªåŠ¨è¯†åˆ«æ–‡æœ¬ä¸­çš„éšå–»æ–¹é¢çš„æ½œåŠ›ã€‚ç ”ç©¶æ¯”è¾ƒäº†ä¸‰ç§æ–¹æ³•ï¼šæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ã€æç¤ºå·¥ç¨‹è®¾è®¡å’Œå¾®è°ƒã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæœ€å…ˆè¿›çš„é—­æºLLMså¯ä»¥è¾¾åˆ°é«˜å‡†ç¡®ç‡ï¼Œå…¶ä¸­å¾®è°ƒæ–¹æ³•çš„F1åˆ†æ•°ä¸­ä½æ•°ä¸º0.79ã€‚å¯¹æ¯”äººç±»å’ŒLLMsçš„è¾“å‡ºç»“æœï¼Œå‘ç°å¤§éƒ¨åˆ†å·®å¼‚æ˜¯ç³»ç»Ÿæ€§çš„ï¼Œåæ˜ äº†éšå–»ç†è®ºä¸­çš„ç°è‰²åœ°å¸¦å’Œæ¦‚å¿µæŒ‘æˆ˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªåŠ¨éšå–»è¯†åˆ«æ–¹é¢å±•ç°å‡ºæ½œåŠ›ã€‚</li>
<li>ç ”ç©¶å¯¹æ¯”äº†ä¸‰ç§éšå–»è¯†åˆ«æ–¹æ³•ï¼šæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ã€æç¤ºå·¥ç¨‹è®¾è®¡å’Œå¾®è°ƒã€‚</li>
<li>å¾®è°ƒæ–¹æ³•è·å¾—æœ€é«˜çš„F1åˆ†æ•°ä¸­ä½æ•°ä¸º0.79ã€‚</li>
<li>LLMsåœ¨éšå–»è¯†åˆ«æ–¹é¢å¯ä»¥è‡³å°‘éƒ¨åˆ†è‡ªåŠ¨åŒ–ã€‚</li>
<li>å¯¹æ¯”äººç±»å’ŒLLMsçš„è¾“å‡ºç»“æœï¼Œå‘ç°å·®å¼‚ä¸»è¦æ˜¯ç³»ç»Ÿæ€§çš„ï¼Œåæ˜ äº†éšå–»ç†è®ºä¸­çš„å·²çŸ¥éš¾é¢˜ã€‚</li>
<li>LLMså¯ä»¥ä½œä¸ºå¼€å‘å’Œä¼˜åŒ–éšå–»è¯†åˆ«åè®®åŠå…¶ç†è®ºåŸºç¡€çš„æµ‹è¯•å¹³å°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24866">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-1a059e20238b8527c1ae94d7c06c6947~resize:0:q75.jpg?source=1f5c5e47&expiration=1760084280&auth_key=1760084280-0-0-ca33d767ab6bce1acc82335404fe8387&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-07b957e55b8999436504096385631f1d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760084287&auth_key=1760084287-0-0-8c3a77c73771e2b2aa21d688abab4ed6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Benchmarking-LLM-Assisted-Blue-Teaming-via-Standardized-Threat-Hunting"><a href="#Benchmarking-LLM-Assisted-Blue-Teaming-via-Standardized-Threat-Hunting" class="headerlink" title="Benchmarking LLM-Assisted Blue Teaming via Standardized Threat Hunting"></a>Benchmarking LLM-Assisted Blue Teaming via Standardized Threat Hunting</h2><p><strong>Authors:Yuqiao Meng, Luoxi Tang, Feiyang Yu, Xi Li, Guanhua Yan, Ping Yang, Zhaohan Xi</strong></p>
<p>As cyber threats continue to grow in scale and sophistication, blue team defenders increasingly require advanced tools to proactively detect and mitigate risks. Large Language Models (LLMs) offer promising capabilities for enhancing threat analysis. However, their effectiveness in real-world blue team threat-hunting scenarios remains insufficiently explored. This paper presents CyberTeam, a benchmark designed to guide LLMs in blue teaming practice. CyberTeam constructs a standardized workflow in two stages. First, it models realistic threat-hunting workflows by capturing the dependencies among analytical tasks from threat attribution to incident response. Next, each task is addressed through a set of operational modules tailored to its specific analytical requirements. This transforms threat hunting into a structured sequence of reasoning steps, with each step grounded in a discrete operation and ordered according to task-specific dependencies. Guided by this framework, LLMs are directed to perform threat-hunting tasks through modularized steps. Overall, CyberTeam integrates 30 tasks and 9 operational modules to guide LLMs through standardized threat analysis. We evaluate both leading LLMs and state-of-the-art cybersecurity agents, comparing CyberTeam against open-ended reasoning strategies. Our results highlight the improvements enabled by standardized design, while also revealing the limitations of open-ended reasoning in real-world threat hunting. </p>
<blockquote>
<p>éšç€ç½‘ç»œå¨èƒè§„æ¨¡å’Œå¤æ‚æ€§çš„ä¸æ–­å¢é•¿ï¼Œè“é˜Ÿé˜²å¾¡è€…è¶Šæ¥è¶Šéœ€è¦å…ˆè¿›çš„å·¥å…·æ¥ä¸»åŠ¨æ£€æµ‹å’Œç¼“è§£é£é™©ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¢å¼ºå¨èƒåˆ†ææ–¹é¢æä¾›äº†æœ‰å‰é€”çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨ç°å®ä¸–ç•Œä¸­è“é˜Ÿç‹©çŒå¨èƒåœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬æ–‡æå‡ºäº†CyberTeamï¼Œä¸€ä¸ªç”¨äºæŒ‡å¯¼LLMåœ¨è“é˜Ÿå®è·µä¸­çš„åŸºå‡†æµ‹è¯•ã€‚CyberTeamæ„å»ºäº†ä¸€ä¸ªæ ‡å‡†åŒ–çš„å·¥ä½œæµç¨‹ï¼Œåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µã€‚é¦–å…ˆï¼Œå®ƒé€šè¿‡æ•æ‰ä»å¨èƒå½’å› åˆ°äº‹ä»¶å“åº”çš„åˆ†æä»»åŠ¡ä¹‹é—´çš„ä¾èµ–å…³ç³»ï¼Œå¯¹ç°å®å¨èƒç‹©çŒå·¥ä½œæµç¨‹è¿›è¡Œå»ºæ¨¡ã€‚æ¥ä¸‹æ¥ï¼Œæ¯ä¸ªä»»åŠ¡éƒ½é€šè¿‡ä¸€ç³»åˆ—é’ˆå¯¹å…¶ç‰¹å®šåˆ†æè¦æ±‚è®¾è®¡çš„æ“ä½œæ¨¡å—æ¥è§£å†³ã€‚è¿™å°†å¨èƒç‹©çŒè½¬å˜ä¸ºä¸€ç³»åˆ—ç»“æ„åŒ–æ¨ç†æ­¥éª¤ï¼Œæ¯ä¸ªæ­¥éª¤éƒ½åŸºäºç¦»æ•£æ“ä½œï¼Œå¹¶æ ¹æ®ç‰¹å®šä»»åŠ¡çš„ä¾èµ–å…³ç³»è¿›è¡Œæ’åºã€‚åœ¨æ­¤æ¡†æ¶çš„æŒ‡å¯¼ä¸‹ï¼ŒLLMé€šè¿‡æ¨¡å—åŒ–æ­¥éª¤æ‰§è¡Œå¨èƒç‹©çŒä»»åŠ¡ã€‚æ€»ä½“è€Œè¨€ï¼ŒCyberTeamé›†æˆäº†30ä¸ªä»»åŠ¡å’Œ9ä¸ªæ“ä½œæ¨¡å—ï¼Œé€šè¿‡æ ‡å‡†åŒ–çš„å¨èƒåˆ†æå¼•å¯¼LLMã€‚æˆ‘ä»¬è¯„ä¼°äº†é¢†å…ˆçš„LLMå’Œæœ€æ–°çš„ç½‘ç»œå®‰å…¨ä»£ç†ï¼Œå°†CyberTeamä¸å¼€æ”¾å¼æ¨ç†ç­–ç•¥è¿›è¡Œäº†æ¯”è¾ƒã€‚æˆ‘ä»¬çš„ç»“æœçªå‡ºäº†æ ‡å‡†åŒ–è®¾è®¡æ‰€å¸¦æ¥çš„æ”¹è¿›ï¼ŒåŒæ—¶ä¹Ÿæ­ç¤ºäº†å¼€æ”¾å¼æ¨ç†åœ¨ç°å®å¨èƒç‹©çŒä¸­çš„å±€é™æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.23571v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>LLMåœ¨æé«˜ç½‘ç»œå®‰å…¨é˜²æŠ¤æ–¹é¢å­˜åœ¨æ½œåŠ›ï¼Œä½†å¯¹å¤§è§„æ¨¡ç½‘ç»œå¨èƒåº”å¯¹åœºæ™¯çš„åº”ç”¨ä»å­˜åœ¨æŒ‘æˆ˜ã€‚æœ¬è®ºæ–‡è®¾è®¡äº†ä¸€ä¸ªåä¸ºCyberTeamçš„åŸºå‡†æµ‹è¯•æ¡†æ¶ï¼Œç”¨ä»¥å¼•å¯¼è“é˜Ÿå®è·µä¸­LLMçš„ä½¿ç”¨ã€‚è¯¥æ¡†æ¶æ„å»ºäº†æ ‡å‡†åŒ–å·¥ä½œæµç¨‹åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µæ¨¡æ‹ŸçœŸå®å¨èƒç‹©çŒæµç¨‹ï¼Œæ•æ‰åˆ†æä»»åŠ¡é—´çš„ä¾èµ–å…³ç³»ï¼›ç¬¬äºŒé˜¶æ®µé’ˆå¯¹æ¯ä¸ªä»»åŠ¡å®šåˆ¶æ“ä½œæ¨¡å—ã€‚é€šè¿‡è¿™ä¸€æ¡†æ¶ï¼Œå¨èƒç‹©çŒè¢«è½¬åŒ–ä¸ºç»“æ„åŒ–æ¨ç†æ­¥éª¤åºåˆ—ï¼Œæ¯ä¸ªæ­¥éª¤éƒ½ä»¥ç¦»æ•£æ“ä½œä¸ºåŸºç¡€ï¼Œæ ¹æ®ç‰¹å®šä»»åŠ¡ä¾èµ–å…³ç³»æ’åºã€‚æ­¤æ¡†æ¶å¼•å¯¼LLMæ‰§è¡Œæ¨¡å—åŒ–æ­¥éª¤ä»¥å®Œæˆå¨èƒç‹©çŒä»»åŠ¡ã€‚æ€»ä½“è¯„ä¼°è¡¨æ˜ï¼ŒCyberTeamæå‡äº†æ ‡å‡†åŒ–è®¾è®¡çš„ä¼˜åŠ¿ï¼ŒåŒæ—¶ä¹Ÿæ­ç¤ºå‡ºå¼€æ”¾å¼æ¨ç†åœ¨ç°å®å¨èƒç‹©çŒä¸­çš„å±€é™æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>LLMåœ¨å¢å¼ºç½‘ç»œå®‰å…¨é˜²æŠ¤æ–¹é¢å…·æœ‰æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨åº”å¯¹æ—¥ç›Šå¤æ‚çš„ç½‘ç»œå¨èƒæ–¹é¢ã€‚</li>
<li>CyberTeamåŸºå‡†æµ‹è¯•æ¡†æ¶è¢«è®¾è®¡ç”¨æ¥å¼•å¯¼LLMåœ¨è“é˜Ÿå®è·µä¸­çš„åº”ç”¨ï¼Œä»¥è§£å†³ç°å®ä¸–ç•Œçš„ç½‘ç»œå®‰å…¨å¨èƒã€‚</li>
<li>è¯¥æ¡†æ¶æ„å»ºäº†æ ‡å‡†åŒ–å¨èƒç‹©çŒå·¥ä½œæµç¨‹ï¼Œåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼Œæ¨¡æ‹ŸçœŸå®å¨èƒç‹©çŒæµç¨‹å¹¶å®šåˆ¶æ“ä½œæ¨¡å—ã€‚</li>
<li>é€šè¿‡è¿™ä¸€æ¡†æ¶ï¼Œå¨èƒç‹©çŒè¢«è½¬åŒ–ä¸ºç»“æ„åŒ–æ¨ç†æ­¥éª¤åºåˆ—ã€‚</li>
<li>CyberTeamé›†æˆäº†30ä¸ªä»»åŠ¡å’Œ9ä¸ªæ“ä½œæ¨¡å—æ¥æŒ‡å¯¼LLMè¿›è¡Œæ ‡å‡†åŒ–å¨èƒåˆ†æã€‚</li>
<li>è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œç›¸è¾ƒäºå¼€æ”¾å¼æ¨ç†ç­–ç•¥ï¼Œæ ‡å‡†åŒ–è®¾è®¡èƒ½å¸¦æ¥æ˜¾è‘—æ”¹å–„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.23571">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-c26f0aabd33deb4d4e4124bc81d12e24~resize:0:q75.jpg?source=1f5c5e47&expiration=1760084295&auth_key=1760084295-0-0-aa890f4532ddfc4a7b9d6c835f605443&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fd2b237d3238de3d33cd550bbe6cf62a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760084302&auth_key=1760084302-0-0-65f42bcc84ee4f1ec7bd7d2e49ccad43&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f2f63f2d60ccf679724ed66d8df018a9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760084309&auth_key=1760084309-0-0-762aafcbd07dc0a92adff128f265c6a7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-962cf6f6a716dea78998ee6511d47fca~resize:0:q75.jpg?source=1f5c5e47&expiration=1760084316&auth_key=1760084316-0-0-1f7b918e02ccc5018ff18ad7d7874621&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-51cc30079088df0d7bd376f6216fabdc~resize:0:q75.jpg?source=1f5c5e47&expiration=1760084323&auth_key=1760084323-0-0-3bb1d52a0d6dadd54c56f996553812a3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Explaining-multimodal-LLMs-via-intra-modal-token-interactions"><a href="#Explaining-multimodal-LLMs-via-intra-modal-token-interactions" class="headerlink" title="Explaining multimodal LLMs via intra-modal token interactions"></a>Explaining multimodal LLMs via intra-modal token interactions</h2><p><strong>Authors:Jiawei Liang, Ruoyu Chen, Xianghao Jiao, Siyuan Liang, Shiming Liu, Qunli Zhang, Zheng Hu, Xiaochun Cao</strong></p>
<p>Multimodal Large Language Models (MLLMs) have achieved remarkable success across diverse vision-language tasks, yet their internal decision-making mechanisms remain insufficiently understood. Existing interpretability research has primarily focused on cross-modal attribution, identifying which image regions the model attends to during output generation. However, these approaches often overlook intra-modal dependencies. In the visual modality, attributing importance to isolated image patches ignores spatial context due to limited receptive fields, resulting in fragmented and noisy explanations. In the textual modality, reliance on preceding tokens introduces spurious activations. Failing to effectively mitigate these interference compromises attribution fidelity. To address these limitations, we propose enhancing interpretability by leveraging intra-modal interaction. For the visual branch, we introduce \textit{Multi-Scale Explanation Aggregation} (MSEA), which aggregates attributions over multi-scale inputs to dynamically adjust receptive fields, producing more holistic and spatially coherent visual explanations. For the textual branch, we propose \textit{Activation Ranking Correlation} (ARC), which measures the relevance of contextual tokens to the current token via alignment of their top-$k$ prediction rankings. ARC leverages this relevance to suppress spurious activations from irrelevant contexts while preserving semantically coherent ones. Extensive experiments across state-of-the-art MLLMs and benchmark datasets demonstrate that our approach consistently outperforms existing interpretability methods, yielding more faithful and fine-grained explanations of model behavior. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤šç§è§†è§‰è¯­è¨€ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†å¯¹å…¶å†…éƒ¨å†³ç­–æœºåˆ¶çš„ç†è§£ä»ç„¶ä¸è¶³ã€‚ç°æœ‰çš„å¯è§£é‡Šæ€§ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨è·¨æ¨¡æ€å½’å› ä¸Šï¼Œè¯†åˆ«æ¨¡å‹åœ¨è¾“å‡ºç”Ÿæˆæ—¶å…³æ³¨å“ªäº›å›¾åƒåŒºåŸŸã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¾€å¾€å¿½è§†äº†è·¨æ¨¡æ€å†…éƒ¨çš„ä¾èµ–å…³ç³»ã€‚åœ¨è§†è§‰æ¨¡æ€ä¸­ï¼Œå°†é‡è¦æ€§å½’å› äºå­¤ç«‹çš„å›¾åƒè¡¥ä¸ä¼šç”±äºæœ‰é™çš„æ„Ÿå—é‡è€Œå¿½è§†ç©ºé—´ä¸Šä¸‹æ–‡ï¼Œä»è€Œå¯¼è‡´è§£é‡Šç¢ç‰‡åŒ–ä¸”å˜ˆæ‚ã€‚åœ¨æ–‡æœ¬æ¨¡æ€ä¸­ï¼Œå¯¹å‰é¢ä»¤ç‰Œçš„ä¾èµ–ä¼šäº§ç”Ÿè™šå‡æ¿€æ´»ã€‚æ— æ³•æœ‰æ•ˆç¼“è§£è¿™äº›å¹²æ‰°ä¼šå½±å“å½’å› çš„ä¿çœŸåº¦ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºé€šè¿‡åˆ©ç”¨è·¨æ¨¡æ€å†…éƒ¨äº¤äº’æ¥æé«˜å¯è§£é‡Šæ€§ã€‚å¯¹äºè§†è§‰åˆ†æ”¯ï¼Œæˆ‘ä»¬å¼•å…¥äº†â€œå¤šå°ºåº¦è§£é‡Šèšåˆâ€ï¼ˆMSEAï¼‰ï¼Œå®ƒé€šè¿‡å¤šå°ºåº¦è¾“å…¥ä¸Šçš„å½’å±èšåˆæ¥åŠ¨æ€è°ƒæ•´æ„Ÿå—é‡ï¼Œä»è€Œäº§ç”Ÿæ›´å…¨é¢å’Œç©ºé—´ä¸Šè¿è´¯çš„è§†è§‰è§£é‡Šã€‚å¯¹äºæ–‡æœ¬åˆ†æ”¯ï¼Œæˆ‘ä»¬æå‡ºâ€œæ¿€æ´»æ’åç›¸å…³æ€§â€ï¼ˆARCï¼‰ï¼Œé€šè¿‡å¯¹å…¶å‰kä¸ªé¢„æµ‹æ’åçš„å¯¹é½æ¥è¡¡é‡ä¸Šä¸‹æ–‡ä»¤ç‰Œä¸å½“å‰ä»¤ç‰Œçš„ç›¸å…³æ€§ã€‚ARCåˆ©ç”¨è¿™ç§ç›¸å…³æ€§æ¥æŠ‘åˆ¶æ¥è‡ªä¸ç›¸å…³ä¸Šä¸‹æ–‡çš„è™šå‡æ¿€æ´»ï¼ŒåŒæ—¶ä¿ç•™è¯­ä¹‰è¿è´¯çš„æ¿€æ´»ã€‚åœ¨å…ˆè¿›çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å’ŒåŸºå‡†æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å§‹ç»ˆä¼˜äºç°æœ‰çš„å¯è§£é‡Šæ€§æ–¹æ³•ï¼Œèƒ½å¤Ÿæä¾›æ›´å¿ å®å’Œç²¾ç»†çš„æ¨¡å‹è¡Œä¸ºè§£é‡Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22415v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸­æé«˜è§£é‡Šæ€§çš„é‡è¦æ€§åŠå…¶å±€é™æ€§ï¼ŒåŒ…æ‹¬è·¨æ¨¡æ€å½’å±çš„ä¸è¶³å’Œå¯¹æ¨¡æ€å†…éƒ¨ä¾èµ–æ€§çš„å¿½è§†ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œå¼•å…¥äº†é’ˆå¯¹è§†è§‰åˆ†æ”¯çš„å¤šå°ºåº¦è§£é‡Šèšåˆï¼ˆMSEAï¼‰å’Œé’ˆå¯¹æ–‡æœ¬åˆ†æ”¯çš„æ¿€æ´»æ’åç›¸å…³æ€§ï¼ˆARCï¼‰ã€‚MSEAé€šè¿‡å¤šå°ºåº¦è¾“å…¥åŠ¨æ€è°ƒæ•´æ„Ÿå—é‡ï¼Œæä¾›æ›´å…¨é¢çš„è§†è§‰è§£é‡Šï¼›ARCé€šè¿‡è¡¡é‡ä¸Šä¸‹æ–‡è¯æ±‡å¯¹å½“å‰è¯æ±‡çš„ç›¸å…³æ€§æ¥æŠ‘åˆ¶æ— å…³è¯­å¢ƒä¸­çš„è™šå‡æ¿€æ´»ï¼ŒåŒæ—¶ä¿ç•™è¯­ä¹‰è¿è´¯æ€§ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸»æµMLLMså’ŒåŸºå‡†æ•°æ®é›†ä¸Šå‡ä¼˜äºç°æœ‰è§£é‡Šæ–¹æ³•ï¼Œæä¾›æ›´å‡†ç¡®ã€æ›´ç²¾ç»†çš„æ¨¡å‹è¡Œä¸ºè§£é‡Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>MLLMsåœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­è¡¨ç°ä¼˜ç§€ï¼Œä½†å…¶å†…éƒ¨å†³ç­–æœºåˆ¶å°šä¸å®Œå…¨æ¸…æ¥šã€‚</li>
<li>å½“å‰è§£é‡Šæ€§ç ”ç©¶ä¸»è¦å…³æ³¨è·¨æ¨¡æ€å½’å±ï¼Œå¿½ç•¥äº†æ¨¡æ€å†…çš„ä¾èµ–æ€§ã€‚</li>
<li>å¤šå°ºåº¦è§£é‡Šèšåˆï¼ˆMSEAï¼‰èƒ½è§£å†³è§†è§‰åˆ†æ”¯ä¸­å› æ„Ÿå—é‡æœ‰é™å¯¼è‡´çš„ç¢ç‰‡åŒ–è§£é‡Šé—®é¢˜ã€‚</li>
<li>æ¿€æ´»æ’åç›¸å…³æ€§ï¼ˆARCï¼‰é€šè¿‡è¡¡é‡ä¸Šä¸‹æ–‡è¯æ±‡å¯¹å½“å‰è¯æ±‡çš„ç›¸å…³æ€§æ¥æŠ‘åˆ¶æ— å…³è¯­å¢ƒä¸­çš„è™šå‡æ¿€æ´»ã€‚</li>
<li>MSEAå’ŒARCæ–¹æ³•å‡èƒ½æœ‰æ•ˆæé«˜æ¨¡å‹è§£é‡Šæ€§ï¼Œå¹¶åœ¨å®éªŒä¸­è¯æ˜äº†å…¶ä¼˜è¶Šæ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22415">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-c49f0d7b359e3b8321ea8f2043ca6f1a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760084331&auth_key=1760084331-0-0-803331e893796b55d6860ddd201a7c1a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d92a54097b8d7f9201450637351912dd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760084338&auth_key=1760084338-0-0-51568f7ec065199abf85564e51fd7c57&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6f9fb5bb3ef602038f5e86c501eff61c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760100403&auth_key=1760100403-0-0-14b6a0fc02d89c44b58ff24938b7c2b3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5917f2e7bb6e4303c01bef28e99bdc98~resize:0:q75.jpg?source=1f5c5e47&expiration=1760084430&auth_key=1760084430-0-0-d03b75f524d21cff8bd0d22b1b44cea8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Evaluating-LLMs-for-Combinatorial-Optimization-One-Phase-and-Two-Phase-Heuristics-for-2D-Bin-Packing"><a href="#Evaluating-LLMs-for-Combinatorial-Optimization-One-Phase-and-Two-Phase-Heuristics-for-2D-Bin-Packing" class="headerlink" title="Evaluating LLMs for Combinatorial Optimization: One-Phase and Two-Phase   Heuristics for 2D Bin-Packing"></a>Evaluating LLMs for Combinatorial Optimization: One-Phase and Two-Phase   Heuristics for 2D Bin-Packing</h2><p><strong>Authors:Syed Mahbubul Huq, Daniel Brito, Daniel Sikar, Chris Child, Tillman Weyde, Rajesh Mojumder</strong></p>
<p>This paper presents an evaluation framework for assessing Large Language Modelsâ€™ (LLMs) capabilities in combinatorial optimization, specifically addressing the 2D bin-packing problem. We introduce a systematic methodology that combines LLMs with evolutionary algorithms to generate and refine heuristic solutions iteratively. Through comprehensive experiments comparing LLM generated heuristics against traditional approaches (Finite First-Fit and Hybrid First-Fit), we demonstrate that LLMs can produce more efficient solutions while requiring fewer computational resources. Our evaluation reveals that GPT-4o achieves optimal solutions within two iterations, reducing average bin usage from 16 to 15 bins while improving space utilization from 0.76-0.78 to 0.83. This work contributes to understanding LLM evaluation in specialized domains and establishes benchmarks for assessing LLM performance in combinatorial optimization tasks. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªè¯„ä¼°æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç»„åˆä¼˜åŒ–æ–¹é¢çš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯è§£å†³äºŒç»´è£…ç®±é—®é¢˜ã€‚æˆ‘ä»¬ä»‹ç»äº†ä¸€ç§ç³»ç»Ÿæ–¹æ³•ï¼Œå°†LLMä¸è¿›åŒ–ç®—æ³•ç›¸ç»“åˆï¼Œè¿­ä»£ç”Ÿæˆå’Œå®Œå–„å¯å‘å¼è§£å†³æ–¹æ¡ˆã€‚é€šè¿‡å…¨é¢çš„å®éªŒï¼Œæˆ‘ä»¬å°†LLMç”Ÿæˆçš„å¯å‘å¼æ–¹æ³•ä¸ä¼ ç»Ÿçš„æœ‰é™åˆ¶é¦–æ¬¡é€‚åº”æ³•å’Œæ··åˆé¦–æ¬¡é€‚åº”æ³•è¿›è¡Œæ¯”è¾ƒï¼Œè¯æ˜LLMèƒ½å¤Ÿäº§ç”Ÿæ›´é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼ŒåŒæ—¶å‡å°‘è®¡ç®—èµ„æºçš„éœ€æ±‚ã€‚æˆ‘ä»¬çš„è¯„ä¼°è¡¨æ˜ï¼ŒGPT-4oåœ¨ä¸¤æ¬¡è¿­ä»£å†…å®ç°äº†æœ€ä¼˜è§£å†³æ–¹æ¡ˆï¼Œå¹³å‡ä½¿ç”¨ç®±å­çš„æ•°é‡ä»16ä¸ªå‡å°‘åˆ°15ä¸ªï¼Œç©ºé—´åˆ©ç”¨ç‡ä»0.76-0.78æé«˜åˆ°0.83ã€‚è¿™é¡¹å·¥ä½œæœ‰åŠ©äºäº†è§£åœ¨ç‰¹å®šé¢†åŸŸä¸­å¯¹LLMçš„è¯„ä¼°ï¼Œå¹¶ä¸ºè¯„ä¼°LLMåœ¨ç»„åˆä¼˜åŒ–ä»»åŠ¡ä¸­çš„æ€§èƒ½å»ºç«‹äº†åŸºå‡†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22255v2">PDF</a> 1 table, 6 figures. 39th Conference on Neural Information Processing   Systems (NeurIPS 2025) Accepted for the Workshop: Evaluating the Evolving LLM   Lifecycle Benchmarks, Emergent Abilities, and Scaling</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ä¸ªè¯„ä¼°æ¡†æ¶ï¼Œç”¨äºè¯„ä»·å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç»„åˆä¼˜åŒ–æ–¹é¢çš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯è§£å†³äºŒç»´è£…ç®±é—®é¢˜ã€‚æ–‡ç« ä»‹ç»äº†ä¸€ç§ç»“åˆLLMä¸è¿›åŒ–ç®—æ³•çš„ç³»ç»Ÿæ–¹æ³•ï¼Œä»¥è¿­ä»£æ–¹å¼ç”Ÿæˆå’Œä¼˜åŒ–å¯å‘å¼è§£å†³æ–¹æ¡ˆã€‚å®éªŒè¯æ˜ï¼ŒLLMç”Ÿæˆçš„å¯å‘å¼æ–¹æ³•ç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•ï¼ˆæœ‰é™é¦–æ¬¡é€‚åº”æ³•å’Œæ··åˆé¦–æ¬¡é€‚åº”æ³•ï¼‰èƒ½æ›´æœ‰æ•ˆåœ°è§£å†³é—®é¢˜ï¼ŒåŒæ—¶å‡å°‘è®¡ç®—èµ„æºéœ€æ±‚ã€‚è¯„ä¼°æ˜¾ç¤ºï¼ŒGPT-4oåœ¨ä¸¤æ¬¡è¿­ä»£å†…è¾¾åˆ°æœ€ä¼˜è§£ï¼Œå¹³å‡å‡å°‘äº†ä¸€ä¸ªç®±å­ä½¿ç”¨é‡å¹¶æé«˜ç©ºé—´åˆ©ç”¨ç‡ã€‚æœ¬æ–‡æœ‰åŠ©äºç†è§£LLMåœ¨ç‰¹å®šé¢†åŸŸçš„è¯„ä¼°ï¼Œå¹¶ä¸ºè¯„ä¼°å…¶åœ¨ç»„åˆä¼˜åŒ–ä»»åŠ¡ä¸­çš„æ€§èƒ½å»ºç«‹äº†åŸºå‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯ç”¨äºè§£å†³ç»„åˆä¼˜åŒ–é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯äºŒç»´è£…ç®±é—®é¢˜ã€‚</li>
<li>æ–‡ç« æå‡ºäº†ç»“åˆLLMä¸è¿›åŒ–ç®—æ³•çš„ç³»ç»Ÿæ–¹æ³•æ¥è§£å†³è¯¥é—®é¢˜ã€‚</li>
<li>LLMç”Ÿæˆçš„å¯å‘å¼è§£å†³æ–¹æ¡ˆç›¸æ¯”ä¼ ç»Ÿæ–¹æ³•æ›´é«˜æ•ˆï¼Œå¹¶å‡å°‘è®¡ç®—èµ„æºéœ€æ±‚ã€‚</li>
<li>GPT-4oåœ¨è§£å†³æ­¤ç±»é—®é¢˜æ—¶è¡¨ç°ä¼˜ç§€ï¼Œä¸¤æ¬¡è¿­ä»£å†…æ‰¾åˆ°æœ€ä¼˜è§£ã€‚</li>
<li>GPT-4oçš„ä½¿ç”¨èƒ½æ˜¾è‘—é™ä½ç®±å­ä½¿ç”¨é‡å¹¶æé«˜ç©ºé—´åˆ©ç”¨ç‡ã€‚</li>
<li>æ–‡ç« ä¸ºç†è§£LLMåœ¨ç‰¹å®šé¢†åŸŸçš„è¯„ä¼°æä¾›äº†æ´è§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22255">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-6ff7403a356e84f3a931171eb6e34362~resize:0:q75.jpg?source=1f5c5e47&expiration=1760084437&auth_key=1760084437-0-0-7f23dccc95fa6d02e362a006b8325365&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-af26ae44d4152792974effa6bd65df3c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760084444&auth_key=1760084444-0-0-0677688a36ede2f18d54f98c37f781f6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-de879a991a7537028f14fa0a9616aabb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760084450&auth_key=1760084450-0-0-24faa7542305cf3f231d6fc0626fe5be&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Steering-When-Necessary-Flexible-Steering-Large-Language-Models-with-Backtracking"><a href="#Steering-When-Necessary-Flexible-Steering-Large-Language-Models-with-Backtracking" class="headerlink" title="Steering When Necessary: Flexible Steering Large Language Models with   Backtracking"></a>Steering When Necessary: Flexible Steering Large Language Models with   Backtracking</h2><p><strong>Authors:Zifeng Cheng, Jinwei Gan, Zhiwei Jiang, Cong Wang, Yafeng Yin, Xiang Luo, Yuchen Fu, Qing Gu</strong></p>
<p>Large language models (LLMs) have achieved remarkable performance across many generation tasks. Nevertheless, effectively aligning them with desired behaviors remains a significant challenge. Activation steering is an effective and cost-efficient approach that directly modifies the activations of LLMs during the inference stage, aligning their responses with the desired behaviors and avoiding the high cost of fine-tuning. Existing methods typically indiscriminately intervene to all generations or rely solely on the question to determine intervention, which limits the accurate assessment of the intervention strength. To this end, we propose the Flexible Activation Steering with Backtracking (FASB) framework, which dynamically determines both the necessity and strength of intervention by tracking the internal states of the LLMs during generation, considering both the question and the generated content. Since intervening after detecting a deviation from the desired behavior is often too late, we further propose the backtracking mechanism to correct the deviated tokens and steer the LLMs toward the desired behavior. Extensive experiments on the TruthfulQA dataset and six multiple-choice datasets demonstrate that our method outperforms baselines. Our code will be released at <a target="_blank" rel="noopener" href="https://github.com/gjw185/FASB">https://github.com/gjw185/FASB</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è®¸å¤šç”Ÿæˆä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆç»©ã€‚ç„¶è€Œï¼Œå¦‚ä½•æœ‰æ•ˆåœ°å°†å®ƒä»¬ä¸æœŸæœ›çš„è¡Œä¸ºå¯¹é½ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚æ¿€æ´»æ§åˆ¶æ˜¯ä¸€ç§æœ‰æ•ˆä¸”æˆæœ¬æ•ˆç›Šé«˜çš„æ–¹æ³•ï¼Œå®ƒç›´æ¥åœ¨æ¨ç†é˜¶æ®µä¿®æ”¹LLMçš„æ¿€æ´»ï¼Œä½¿å®ƒä»¬çš„å“åº”ä¸æœŸæœ›çš„è¡Œä¸ºå¯¹é½ï¼Œé¿å…äº†å¾®è°ƒçš„é«˜æˆæœ¬ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¸åŠ åŒºåˆ†åœ°å¹²é¢„æ‰€æœ‰ç”Ÿæˆï¼Œæˆ–ä»…ä¾èµ–é—®é¢˜æ¥ç¡®å®šå¹²é¢„ï¼Œè¿™é™åˆ¶äº†å‡†ç¡®è¯„ä¼°å¹²é¢„å¼ºåº¦ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†å¸¦æœ‰å›æº¯çš„çµæ´»æ¿€æ´»æ§åˆ¶ï¼ˆFASBï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡è·Ÿè¸ªLLMç”Ÿæˆè¿‡ç¨‹ä¸­çš„å†…éƒ¨çŠ¶æ€ï¼Œè€ƒè™‘é—®é¢˜å’Œç”Ÿæˆå†…å®¹ï¼ŒåŠ¨æ€ç¡®å®šå¹²é¢„çš„å¿…è¦æ€§å’Œå¼ºåº¦ã€‚ç”±äºåœ¨å‘ç°åç¦»æœŸæœ›è¡Œä¸ºåå†è¿›è¡Œå¹²é¢„é€šå¸¸å¤ªæ™šäº†ï¼Œå› æ­¤æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†å›æº¯æœºåˆ¶æ¥çº æ­£åç¦»çš„ä»¤ç‰Œï¼Œå¹¶å°†LLMè½¬å‘æœŸæœ›çš„è¡Œä¸ºã€‚åœ¨TruthfulQAæ•°æ®é›†å’Œå…­ä¸ªé€‰æ‹©é¢˜æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºåŸºçº¿ã€‚æˆ‘ä»¬çš„ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/gjw185/FASB">https://github.com/gjw185/FASB</a>å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.17621v2">PDF</a> NeurIPS 2025</p>
<p><strong>Summary</strong><br>LLMsåœ¨å„ç§ç”Ÿæˆä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½ï¼Œä½†åœ¨ä¸æœŸæœ›è¡Œä¸ºå¯¹é½æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚æ¿€æ´»è½¬å‘æ˜¯ä¸€ç§æœ‰æ•ˆä¸”ç»æµå®æƒ çš„æ–¹æ³•ï¼Œå¯åœ¨æ¨ç†é˜¶æ®µç›´æ¥ä¿®æ”¹LLMçš„æ¿€æ´»ï¼Œä½¿å®ƒä»¬çš„å“åº”ä¸æœŸæœ›è¡Œä¸ºå¯¹é½ï¼Œé¿å…å¾®è°ƒçš„é«˜æˆæœ¬ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ç›²ç›®åœ°å¹²é¢„æ‰€æœ‰ç”Ÿæˆæˆ–ä»…ä¾èµ–é—®é¢˜æ¥ç¡®å®šå¹²é¢„ï¼Œè¿™é™åˆ¶äº†å‡†ç¡®è¯„ä¼°å¹²é¢„çš„å¼ºåº¦ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†å…·æœ‰å›æº¯åŠŸèƒ½çš„çµæ´»æ¿€æ´»è½¬å‘ï¼ˆFASBï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡è·Ÿè¸ªLLMçš„å†…éƒ¨çŠ¶æ€æ¥åŠ¨æ€ç¡®å®šå¹²é¢„çš„å¿…è¦æ€§å’Œå¼ºåº¦ï¼ŒåŒæ—¶è€ƒè™‘é—®é¢˜å’Œç”Ÿæˆå†…å®¹ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†å›æº¯æœºåˆ¶æ¥çº æ­£åç¦»çš„ä»¤ç‰Œå¹¶å¼•å¯¼LLMæœç€æœŸæœ›çš„è¡Œä¸ºå‘å±•ã€‚åœ¨TruthfulQAæ•°æ®é›†å’Œå…­ä¸ªå¤šé¡¹é€‰æ‹©é¢˜æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚æˆ‘ä»¬çš„ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/gjw185/FASB">é“¾æ¥å¤„å‘å¸ƒ</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLMsåœ¨ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†åœ¨æœŸæœ›è¡Œä¸ºå¯¹é½æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>æ¿€æ´»è½¬å‘æ˜¯ä¸€ç§æœ‰æ•ˆä¸”ç»æµå®æƒ çš„æ–¹æ³•ï¼Œç”¨äºåœ¨æ¨ç†é˜¶æ®µä¿®æ”¹LLMçš„æ¿€æ´»ï¼Œä»¥å®ç°æœŸæœ›çš„è¡Œä¸ºå¯¹é½ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å­˜åœ¨ç›²ç›®å¹²é¢„çš„é—®é¢˜ï¼Œæ— æ³•å‡†ç¡®è¯„ä¼°å¹²é¢„å¼ºåº¦ã€‚</li>
<li>FASBæ¡†æ¶é€šè¿‡è·Ÿè¸ªLLMçš„å†…éƒ¨çŠ¶æ€åŠ¨æ€ç¡®å®šå¹²é¢„çš„å¿…è¦æ€§ï¼ŒåŒæ—¶è€ƒè™‘é—®é¢˜å’Œç”Ÿæˆå†…å®¹ã€‚</li>
<li>FASBæ¡†æ¶å¼•å…¥äº†å›æº¯æœºåˆ¶æ¥çº æ­£åç¦»çš„ä»¤ç‰Œå¹¶å¼•å¯¼LLMæœç€æœŸæœ›è¡Œä¸ºå‘å±•ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒFASBæ¡†æ¶çš„æ–¹æ³•ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.17621">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-d86b67b1d8f67761c78bbab77416fd25~resize:0:q75.jpg?source=1f5c5e47&expiration=1760084457&auth_key=1760084457-0-0-9b2ca472be49c13c205d7d9e32a69082&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5975c02662d3a68699dcbc9be9079b47~resize:0:q75.jpg?source=1f5c5e47&expiration=1760084465&auth_key=1760084465-0-0-17471f5acb49d90744904075c5177833&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-97e46caa5cb108d7736dc284f419cb11~resize:0:q75.jpg?source=1f5c5e47&expiration=1760084471&auth_key=1760084471-0-0-b9bec38c874c43a3a20b1db03205fe66&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0d84f09afef6b103be1b8152ac6b0ac3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760084478&auth_key=1760084478-0-0-dd3d84c31eb467fec5906b3b0c31b312&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f9d9355c0c513af49775a78d06bc798b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760084484&auth_key=1760084484-0-0-4f07af8e45a3d3d7688e54528e4f8e59&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="MetaLint-Generalizable-Idiomatic-Code-Quality-Analysis-through-Instruction-Following-and-Easy-to-Hard-Generalization"><a href="#MetaLint-Generalizable-Idiomatic-Code-Quality-Analysis-through-Instruction-Following-and-Easy-to-Hard-Generalization" class="headerlink" title="MetaLint: Generalizable Idiomatic Code Quality Analysis through   Instruction-Following and Easy-to-Hard Generalization"></a>MetaLint: Generalizable Idiomatic Code Quality Analysis through   Instruction-Following and Easy-to-Hard Generalization</h2><p><strong>Authors:Atharva Naik, Lawanya Baghel, Dhakshin Govindarajan, Darsh Agrawal, Daniel Fried, Carolyn Rose</strong></p>
<p>Large Language Models, though successful in code generation, struggle with code quality analysis because they are limited by static training data and canâ€™t easily adapt to evolving best practices. We introduce MetaLint, an instruction-following framework that formulates code quality analysis as the task of detecting and fixing problematic semantic code fragments or code idioms based on high-level specifications. Unlike conventional approaches that train models on static code quality conventions, MetaLint employs instruction tuning on synthetic linter-generated data with dynamic conventions to support easy-to-hard generalization, enabling models to adapt to novel or complex code patterns without retraining. To evaluate this, we construct a benchmark of challenging idioms inspired by real-world coding standards such as Python Enhancement Proposals (PEPs) and assess whether MetaLint-trained models reason adaptively or simply memorize. Our results show that MetaLint training improves generalization to unseen idioms. Qwen3-4B attains a 70.37% F-score on a manually curated and challenging PEP idiom detection benchmark, achieving the highest recall (70.43%) among all evaluated models. For localization, it reaches 26.73%, which is a strong outcome for its 4B parameter size and comparable to larger state-of-the-art models such as o3-mini, highlighting its potential for future-proof code quality analysis. Furthermore, MetaLint training enables generalization in idiom detection across model families, model scales, synthetic data from diverse linters, and Java idioms, demonstrating the general applicability of our approach. We plan to release our code and data to enable reproducibility and further work. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹è™½ç„¶åœ¨ä»£ç ç”Ÿæˆæ–¹é¢å–å¾—äº†æˆåŠŸï¼Œä½†åœ¨ä»£ç è´¨é‡åˆ†ææ–¹é¢å´é¢ä¸´æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒä»¬å—åˆ°é™æ€è®­ç»ƒæ•°æ®çš„é™åˆ¶ï¼Œæ— æ³•è½»æ˜“é€‚åº”ä¸æ–­å˜åŒ–çš„æœ€ä½³å®è·µã€‚æˆ‘ä»¬å¼•å…¥äº†MetaLintï¼Œè¿™æ˜¯ä¸€ä¸ªéµå¾ªæŒ‡ä»¤çš„æ¡†æ¶ï¼Œå®ƒå°†ä»£ç è´¨é‡åˆ†æåˆ¶å®šä¸ºæ£€æµ‹å’Œä¿®å¤åŸºäºé«˜çº§è§„èŒƒçš„é—®é¢˜è¯­ä¹‰ä»£ç ç‰‡æ®µæˆ–ä»£ç æƒ¯ç”¨æ³•çš„ä»»åŠ¡ã€‚ä¸ä¼ ç»Ÿçš„åœ¨é™æ€ä»£ç è´¨é‡è§„èŒƒä¸Šè®­ç»ƒæ¨¡å‹çš„æ–¹æ³•ä¸åŒï¼ŒMetaLinté‡‡ç”¨å¯¹åˆæˆlinterç”Ÿæˆçš„æ•°æ®è¿›è¡ŒæŒ‡ä»¤è°ƒæ•´çš„æ–¹æ³•ï¼Œæ”¯æŒä»æ˜“åˆ°éš¾çš„æ³›åŒ–ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿé€‚åº”æ–°çš„æˆ–å¤æ‚çš„ä»£ç æ¨¡å¼ï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚ä¸ºäº†è¯„ä¼°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªä»¥ç°å®ä¸–ç•Œç¼–ç æ ‡å‡†ï¼ˆå¦‚Pythonå¢å¼ºææ¡ˆï¼ˆPEPsï¼‰ï¼‰ä¸ºçµæ„Ÿçš„æŒ‘æˆ˜ä¹ è¯­åŸºå‡†æµ‹è¯•ï¼Œå¹¶è¯„ä¼°MetaLintè®­ç»ƒçš„æ¨¡å‹æ˜¯é€‚åº”æ€§æ¨ç†è¿˜æ˜¯ç®€å•è®°å¿†ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒMetaLintè®­ç»ƒå¯æé«˜æ³›åŒ–åˆ°æœªè§è¿‡çš„ä¹ è¯­çš„èƒ½åŠ›ã€‚Qwen3-4Båœ¨æ‰‹åŠ¨æ•´ç†å’Œæœ‰æŒ‘æˆ˜æ€§çš„PEPä¹ è¯­æ£€æµ‹åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†70.37%çš„Fåˆ†æ•°ï¼Œåœ¨æ‰€æœ‰è¯„ä¼°æ¨¡å‹ä¸­å®ç°äº†æœ€é«˜çš„å¬å›ç‡ï¼ˆ70.43%ï¼‰ã€‚å°±å®šä½è€Œè¨€ï¼Œå®ƒè¾¾åˆ°äº†26.73%ï¼Œè¿™å¯¹äºå…¶4Bçš„å‚æ•°è§„æ¨¡æ¥è¯´æ˜¯ä¸€ä¸ªå¼ºå¤§çš„ç»“æœï¼Œä¸æ›´å¤§æœ€å…ˆè¿›çš„æ¨¡å‹ï¼ˆå¦‚o3-miniï¼‰ç›¸å½“ï¼Œå‡¸æ˜¾å…¶åœ¨æœªæ¥ä»£ç è´¨é‡åˆ†æä¸­çš„æ½œåŠ›ã€‚æ­¤å¤–ï¼ŒMetaLintè®­ç»ƒèƒ½å¤Ÿåœ¨æ¨¡å‹å®¶æ—ã€æ¨¡å‹è§„æ¨¡ã€æ¥è‡ªå„ç§linterçš„åˆæˆæ•°æ®ä»¥åŠJavaä¹ è¯­ä¸­çš„ä¹ è¯­æ£€æµ‹ä¸­å®ç°æ³›åŒ–ï¼Œè¡¨æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„æ™®éé€‚ç”¨æ€§ã€‚æˆ‘ä»¬è®¡åˆ’å‘å¸ƒæˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®ï¼Œä»¥å®ç°å¯é‡å¤æ€§å’Œè¿›ä¸€æ­¥çš„å·¥ä½œã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.11687v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä»£ç ç”Ÿæˆæ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨ä»£ç è´¨é‡åˆ†ææ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œå—é™äºé™æ€è®­ç»ƒæ•°æ®ï¼Œéš¾ä»¥é€‚åº”ä¸æ–­å˜åŒ–çš„æœ€ä½³å®è·µã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†MetaLintï¼Œä¸€ç§æŒ‡ä»¤éµå¾ªæ¡†æ¶ï¼Œå°†ä»£ç è´¨é‡åˆ†æåˆ¶å®šä¸ºæ£€æµ‹å’Œä¿®å¤åŸºäºé«˜çº§è§„èŒƒçš„é—®é¢˜è¯­ä¹‰ä»£ç ç‰‡æ®µæˆ–ä»£ç idiomçš„ä»»åŠ¡ã€‚ä¸åŒäºä¼ ç»Ÿçš„åœ¨é™æ€ä»£ç è´¨é‡è§„èŒƒä¸Šè®­ç»ƒæ¨¡å‹çš„æ–¹æ³•ï¼ŒMetaLinté‡‡ç”¨åŸºäºåˆæˆlinteræ•°æ®ç”Ÿæˆçš„æŒ‡ä»¤å¾®è°ƒæ–¹æ³•ï¼Œæ”¯æŒä»æ˜“åˆ°éš¾çš„æ³›åŒ–ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿé€‚åº”æ–°çš„æˆ–å¤æ‚çš„ä»£ç æ¨¡å¼è€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚è¯„ä¼°è¡¨æ˜ï¼ŒMetaLintè®­ç»ƒæœ‰åŠ©äºæé«˜æœªè§è¿‡çš„idiomçš„æ³›åŒ–èƒ½åŠ›ã€‚Qwen3-4Bæ¨¡å‹åœ¨æ‰‹åŠ¨æ„å»ºä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„PEP idiomæ£€æµ‹åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†70.37%çš„Fåˆ†æ•°ï¼Œæˆä¸ºæ‰€æœ‰è¯„ä¼°æ¨¡å‹ä¸­å¬å›ç‡æœ€é«˜çš„ï¼ˆ70.43%ï¼‰ã€‚å¯¹äºå®šä½ä»»åŠ¡ï¼Œå®ƒè¾¾åˆ°äº†26.73%ï¼Œå¯¹äºä¸€ä¸ª4Bå‚æ•°å¤§å°çš„æ¨¡å‹æ¥è¯´è¿™æ˜¯ä¸€ä¸ªå¼ºå¤§çš„ç»“æœï¼Œå¹¶ä¸”ä¸æœ€æ–°çš„å¤§å‹æ¨¡å‹å¦‚o3-miniç›¸å½“ï¼Œå‡¸æ˜¾å‡ºå…¶åœ¨æœªæ¥ä»£ç è´¨é‡åˆ†æä¸­çš„æ½œåŠ›ã€‚æ­¤å¤–ï¼ŒMetaLintè®­ç»ƒå®ç°äº†è·¨æ¨¡å‹å®¶æ—ã€æ¨¡å‹è§„æ¨¡ã€æ¥è‡ªå„ç§linterçš„åˆæˆæ•°æ®å’ŒJava idiomçš„idiomæ£€æµ‹æ³›åŒ–ï¼Œå±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•çš„ä¸€èˆ¬é€‚ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä»£ç è´¨é‡åˆ†ææ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œå› å—é™äºé™æ€è®­ç»ƒæ•°æ®ï¼Œéš¾ä»¥é€‚åº”æœ€ä½³å®è·µçš„æ¼”å˜ã€‚</li>
<li>MetaLintæ¡†æ¶è¢«å¼•å…¥ä»¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œå®ƒå°†ä»£ç è´¨é‡åˆ†æè§†ä¸ºæ£€æµ‹å’Œä¿®å¤é—®é¢˜è¯­ä¹‰ä»£ç ç‰‡æ®µæˆ–ä»£ç idiomçš„ä»»åŠ¡ã€‚</li>
<li>MetaLinté‡‡ç”¨åŸºäºåˆæˆlinteræ•°æ®ç”Ÿæˆçš„æŒ‡ä»¤å¾®è°ƒæ–¹æ³•ï¼Œæ”¯æŒæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œä»¥é€‚åº”æ–°çš„æˆ–å¤æ‚çš„ä»£ç æ¨¡å¼ã€‚</li>
<li>MetaLintè®­ç»ƒæœ‰åŠ©äºæé«˜æ¨¡å‹åœ¨æœªè§è¿‡çš„idiomä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>Qwen3-4Bæ¨¡å‹åœ¨PEP idiomæ£€æµ‹åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºè‰²ï¼Œæˆä¸ºè¯„ä¼°ä¸­çš„æœ€ä½³æ¨¡å‹ã€‚</li>
<li>MetaLintè®­ç»ƒå®ç°äº†è·¨æ¨¡å‹å®¶æ—ã€è§„æ¨¡ã€åˆæˆæ•°æ®å’ŒJava idiomçš„idiomæ£€æµ‹æ³›åŒ–ï¼Œæ˜¾ç¤ºäº†å…¶å¹¿æ³›é€‚ç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.11687">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-1790bd77637089d25e85894fb9f4518e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760084492&auth_key=1760084492-0-0-5c966e69bd177b1451442e5664f71144&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-95857a9c7f6a7d1d1ad397921b371d0b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760084499&auth_key=1760084499-0-0-44b9b7032261d0da308c85442521831d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a5ab6567c4e50c03ae222b222311c93e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760084505&auth_key=1760084505-0-0-19a7ad0fe715ee449b304ad67fd97a25&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Back-to-the-Basics-Rethinking-Issue-Commit-Linking-with-LLM-Assisted-Retrieval"><a href="#Back-to-the-Basics-Rethinking-Issue-Commit-Linking-with-LLM-Assisted-Retrieval" class="headerlink" title="Back to the Basics: Rethinking Issue-Commit Linking with LLM-Assisted   Retrieval"></a>Back to the Basics: Rethinking Issue-Commit Linking with LLM-Assisted   Retrieval</h2><p><strong>Authors:Huihui Huang, Ratnadira Widyasari, Ting Zhang, Ivana Clairine Irsan, Jieke Shi, Han Wei Ang, Frank Liauw, Eng Lieh Ouh, Lwin Khin Shar, Hong Jin Kang, David Lo</strong></p>
<p>Issue-commit linking, which connects issues with commits that fix them, is crucial for software maintenance. Existing approaches have shown promise in automatically recovering these links. Evaluations of these techniques assess their ability to identify genuine links from plausible but false links. However, these evaluations overlook the fact that, in reality, when a repository has more commits, the presence of more plausible yet unrelated commits may interfere with the tool in differentiating the correct fix commits. To address this, we propose the Realistic Distribution Setting (RDS) and use it to construct a more realistic evaluation dataset that includes 20 open-source projects. By evaluating tools on this dataset, we observe that the performance of the state-of-the-art deep learning-based approach drops by more than half, while the traditional Information Retrieval method, VSM, outperforms it. Inspired by these observations, we propose EasyLink, which utilizes a vector database as a modern Information Retrieval technique. To address the long-standing problem of the semantic gap between issues and commits, EasyLink leverages a large language model to rerank the commits retrieved from the database. Under our evaluation, EasyLink achieves an average Precision@1 of 75.03%, improving over the state-of-the-art by over four times. Additionally, this paper provides practical guidelines for advancing research in issue-commit link recovery. </p>
<blockquote>
<p>é—®é¢˜-æäº¤é“¾æ¥ï¼ˆIssue-commit linkingï¼‰ï¼Œå®ƒå°†é—®é¢˜ä¸ä¿®å¤å®ƒä»¬çš„æäº¤è¿æ¥èµ·æ¥ï¼Œå¯¹è½¯ä»¶ç»´æŠ¤è‡³å…³é‡è¦ã€‚ç°æœ‰æ–¹æ³•å·²åœ¨è‡ªåŠ¨æ¢å¤è¿™äº›é“¾æ¥æ–¹é¢æ˜¾ç¤ºå‡ºå‰æ™¯ã€‚è¿™äº›æŠ€æœ¯çš„è¯„ä¼°æ—¨åœ¨è¯„ä¼°å®ƒä»¬ä»å¯èƒ½çš„è™šå‡é“¾æ¥ä¸­è¯†åˆ«çœŸæ­£é“¾æ¥çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›è¯„ä¼°å¿½è§†äº†è¿™æ ·ä¸€ä¸ªäº‹å®ï¼Œå³åœ¨å®é™…æƒ…å†µä¸‹ï¼Œå½“ä»“åº“ä¸­çš„æäº¤æ•°é‡æ›´å¤šæ—¶ï¼Œå­˜åœ¨æ›´å¤šå¯èƒ½çš„ä½†æ— å…³çš„æäº¤å¯èƒ½ä¼šå¹²æ‰°å·¥å…·åŒºåˆ†æ­£ç¡®çš„ä¿®å¤æäº¤ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ç°å®åˆ†å¸ƒè®¾ç½®ï¼ˆRDSï¼‰å¹¶ä½¿ç”¨å®ƒæ„å»ºäº†ä¸€ä¸ªæ›´ç°å®çš„è¯„ä¼°æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…æ‹¬20ä¸ªå¼€æºé¡¹ç›®ã€‚é€šè¿‡å¯¹è¯¥æ•°æ®é›†çš„å·¥å…·è¿›è¡Œè¯„ä¼°ï¼Œæˆ‘ä»¬å‘ç°æœ€å…ˆè¿›çš„åŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•æ€§èƒ½ä¸‹é™äº†ä¸€åŠä»¥ä¸Šï¼Œè€Œä¼ ç»Ÿçš„ä¿¡æ¯æ£€ç´¢æ–¹æ³•VSMè¡¨ç°ä¼˜äºå®ƒã€‚å—æ­¤è§‚å¯Ÿå¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†EasyLinkï¼Œå®ƒåˆ©ç”¨å‘é‡æ•°æ®åº“ä½œä¸ºç°ä»£ä¿¡æ¯æ£€ç´¢æŠ€æœ¯ã€‚ä¸ºäº†è§£å†³é•¿æœŸå­˜åœ¨çš„é—®é¢˜ä¸æäº¤ä¹‹é—´çš„è¯­ä¹‰é¸¿æ²Ÿé—®é¢˜ï¼ŒEasyLinkåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å¯¹ä»æ•°æ®åº“ä¸­æ£€ç´¢åˆ°çš„æäº¤è¿›è¡Œé‡æ’ã€‚åœ¨æˆ‘ä»¬çš„è¯„ä¼°ä¸‹ï¼ŒEasyLinkçš„å¹³å‡Precision@1è¾¾åˆ°äº†75.03%ï¼Œæ¯”ç°æœ‰æŠ€æœ¯æé«˜äº†å››å€ä»¥ä¸Šã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜ä¸ºæ¨è¿›é—®é¢˜-æäº¤é“¾æ¥æ¢å¤ç ”ç©¶æä¾›äº†å®ç”¨æŒ‡å¯¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.09199v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥é—®é¢˜ç ”ç©¶äº†è½¯ä»¶ç»´æŠ¤ä¸­çš„å…³é”®ä»»åŠ¡â€”â€”é—®é¢˜æäº¤ä¸ä¿®å¤æäº¤ä¹‹é—´çš„é“¾æ¥å…³ç³»ã€‚ç°æœ‰æ–¹æ³•è™½ç„¶åœ¨è‡ªåŠ¨æ¢å¤è¿™äº›é“¾æ¥æ–¹é¢å±•ç°å‡ºæ½œåŠ›ï¼Œä½†åœ¨çœŸå®åœºæ™¯ä¸­ï¼Œå½“ä»“åº“ä¸­çš„æäº¤æ•°é‡å¢å¤šæ—¶ï¼Œå­˜åœ¨å¤§é‡å¯èƒ½çš„ä½†æ— å…³çš„æäº¤å¯èƒ½ä¼šå¹²æ‰°å·¥å…·åŒºåˆ†æ­£ç¡®çš„ä¿®å¤æäº¤ã€‚ä¸ºåº”å¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ç°å®åˆ†å¸ƒè®¾ç½®ï¼ˆRDSï¼‰ï¼Œå¹¶åˆ©ç”¨å…¶æ„å»ºäº†ä¸€ä¸ªæ›´ç°å®çš„è¯„ä¼°æ•°æ®é›†ï¼ŒåŒ…å«20ä¸ªå¼€æºé¡¹ç›®ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œæœ€å…ˆè¿›çš„æ·±åº¦å­¦ä¹ æ–¹æ³•çš„æ€§èƒ½ä¸‹é™äº†ä¸€åŠä»¥ä¸Šï¼Œè€Œä¼ ç»Ÿçš„ä¿¡æ¯æ£€ç´¢æ–¹æ³•VSMè¡¨ç°æ›´ä½³ã€‚åŸºäºæ­¤è§‚å¯Ÿï¼Œæœ¬æ–‡æå‡ºäº†EasyLinkæ–¹æ³•ï¼Œåˆ©ç”¨å‘é‡æ•°æ®åº“ä½œä¸ºç°ä»£ä¿¡æ¯æ£€ç´¢æŠ€æœ¯ï¼Œå¹¶å€ŸåŠ©å¤§å‹è¯­è¨€æ¨¡å‹æ¥è§£å†³é•¿æœŸå­˜åœ¨çš„é—®é¢˜â€”â€”é—®é¢˜å’Œæäº¤ä¹‹é—´çš„è¯­ä¹‰é¸¿æ²Ÿã€‚EasyLinkåœ¨è¯„ä¼°ä¸­å–å¾—äº†å¹³å‡Precision@1ä¸º75.03%çš„æ•ˆæœï¼Œç›¸è¾ƒäºç°æœ‰æŠ€æœ¯æœ‰äº†æ˜¾è‘—çš„æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é—®é¢˜æäº¤ä¸ä¿®å¤æäº¤çš„é“¾æ¥å…³ç³»åœ¨è½¯ä»¶ç»´æŠ¤ä¸­è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨è¯„ä¼°æ—¶å¿½ç•¥äº†å¤§é‡å¯èƒ½çš„ä½†æ— å…³çš„æäº¤å¯¹å·¥å…·åŒºåˆ†æ­£ç¡®ä¿®å¤æäº¤çš„å½±å“ã€‚</li>
<li>ä¸ºåº”å¯¹æ­¤é—®é¢˜ï¼Œæå‡ºäº†ç°å®åˆ†å¸ƒè®¾ç½®ï¼ˆRDSï¼‰å¹¶æ„å»ºäº†ä¸€ä¸ªåŒ…å«20ä¸ªå¼€æºé¡¹ç›®çš„æ›´ç°å®çš„è¯„ä¼°æ•°æ®é›†ã€‚</li>
<li>æœ€å…ˆè¿›çš„æ·±åº¦å­¦ä¹ æ–¹æ³•çš„æ€§èƒ½åœ¨å®é™…ç¯å¢ƒä¸‹ä¼šå¤§å¹…åº¦ä¸‹é™ã€‚</li>
<li>ä¼ ç»Ÿä¿¡æ¯æ£€ç´¢æ–¹æ³•VSMåœ¨æŸäº›åœºæ™¯ä¸‹è¡¨ç°æ›´å¥½ã€‚</li>
<li>EasyLinkæ–¹æ³•ç»“åˆç°ä»£ä¿¡æ¯æ£€ç´¢æŠ€æœ¯å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæœ‰æ•ˆè§£å†³äº†é—®é¢˜å’Œæäº¤ä¹‹é—´çš„è¯­ä¹‰é¸¿æ²Ÿé—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.09199">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-5a916209fc9e58296abf712c1122cb66~resize:0:q75.jpg?source=1f5c5e47&expiration=1760084513&auth_key=1760084513-0-0-e3961e476ee7ff4fd22ab9e0568212ad&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0fcfca2304a6adb1036685907177f306~resize:0:q75.jpg?source=1f5c5e47&expiration=1760084521&auth_key=1760084521-0-0-d8e03f368c901c64edbddfedb215de41&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6870bcab65cb974970c1cb262ec90e38~resize:0:q75.jpg?source=1f5c5e47&expiration=1760084528&auth_key=1760084528-0-0-921dda9f32d5b24d88483b219b7e7349&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-86c81ac955fa663c9e23ad8b98749ce8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760100410&auth_key=1760100410-0-0-d5a26e9f1e7ade7463da4634af155a78&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Latent-Chain-of-Thought-Decoding-the-Depth-Recurrent-Transformer"><a href="#Latent-Chain-of-Thought-Decoding-the-Depth-Recurrent-Transformer" class="headerlink" title="Latent Chain-of-Thought? Decoding the Depth-Recurrent Transformer"></a>Latent Chain-of-Thought? Decoding the Depth-Recurrent Transformer</h2><p><strong>Authors:Wenquan Lu, Yuechuan Yang, Kyle Lee, Yanshu Li, Enqi Liu</strong></p>
<p>Chain-of-thought (CoT) reasoning has enabled transformer-based language models to excel at complex mathematics and multi-step planning. However, in standard decoder-only architectures, these reasoning steps are externalized in natural language, improving interpretability at the cost of efficiency. To capture reasoning that is not easily represented in words, many works have explored recurrent architectures that aim to internalize reasoning in latent space, potentially supporting latent CoT. In this paper, we investigate whether such reasoning structures emerge in Huginn-3.5B, a depth-recurrent Transformer that reuses layers at inference time without increasing parameter count. We examine the modelâ€™s internal behavior on arithmetic tasks using a suite of probing techniques including the Logit Lens and Coda Lens. Our findings reveal limited evidence of interpretable latent CoT by tracking rank trajectories of final and intermediate result tokens. Furthermore, we uncover significant probing inconsistencies across recurrent blocks, where the interpretability of hidden states depends heavily on both the layer index and the decoding method. Finally, we empirically show that increasing recurrence depth yields only marginal gains and falls well short of models that explicitly externalize reasoning steps. The code is available at <a target="_blank" rel="noopener" href="https://github.com/wenquanlu/huginn-latent-cot">https://github.com/wenquanlu/huginn-latent-cot</a>. </p>
<blockquote>
<p>æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†ä½¿åŸºäºè½¬æ¢å™¨çš„è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æ•°å­¦å’Œå¤šæ­¥è§„åˆ’æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œåœ¨æ ‡å‡†çš„ä»…è§£ç å™¨æ¶æ„ä¸­ï¼Œè¿™äº›æ¨ç†æ­¥éª¤æ˜¯ä»¥è‡ªç„¶è¯­è¨€å½¢å¼å¤–åœ¨åŒ–çš„ï¼Œè™½ç„¶æé«˜äº†å¯è§£é‡Šæ€§ï¼Œä½†ç‰ºç‰²äº†æ•ˆç‡ã€‚ä¸ºäº†æ•æ‰ä¸å®¹æ˜“ç”¨æ–‡å­—è¡¨ç¤ºçš„é“ç†ï¼Œè®¸å¤šç ”ç©¶å·²ç»æ¢ç´¢äº†å¾ªç¯æ¶æ„ï¼Œæ—¨åœ¨å°†æ¨ç†å†…åœ¨åŒ–äºæ½œåœ¨ç©ºé—´ï¼Œå¯èƒ½æ”¯æŒæ½œåœ¨æ€ç»´é“¾ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†Huginn-3.5Bè¿™ç§æ·±åº¦å¾ªç¯è½¬æ¢å™¨ä¸­æ˜¯å¦ä¼šå‡ºç°æ­¤ç±»æ¨ç†ç»“æ„ã€‚åœ¨æ¨ç†æ—¶ï¼Œè¯¥è½¬æ¢å™¨é‡æ–°ä½¿ç”¨å±‚è€Œæ— éœ€å¢åŠ å‚æ•°è®¡æ•°ã€‚æˆ‘ä»¬ä½¿ç”¨åŒ…æ‹¬Logit Lenså’ŒCoda Lensç­‰ä¸€ç³»åˆ—æ¢æµ‹æŠ€æœ¯å¯¹æ¨¡å‹åœ¨ç®—æœ¯ä»»åŠ¡ä¸Šçš„å†…éƒ¨è¡Œä¸ºè¿›è¡Œäº†æ£€æŸ¥ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œé€šè¿‡è·Ÿè¸ªæœ€ç»ˆå’Œä¸­é—´ç»“æœä»¤ç‰Œçš„æ’åè½¨è¿¹ï¼Œæœ‰é™çš„è¯æ®è¡¨æ˜å­˜åœ¨å¯è§£é‡Šçš„æ½œåœ¨æ€ç»´é“¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°äº†å¾ªç¯å—ä¹‹é—´æ˜¾è‘—çš„æ¢æµ‹ä¸ä¸€è‡´æ€§ï¼Œéšè—çŠ¶æ€çš„è§£é‡Šæ€§å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºå±‚ç´¢å¼•å’Œè§£ç æ–¹æ³•ã€‚æœ€åï¼Œæˆ‘ä»¬ä»å®è¯ä¸Šè¯æ˜ï¼Œå¢åŠ å¾ªç¯æ·±åº¦åªäº§ç”Ÿæœ‰é™çš„æ”¶ç›Šï¼Œè¿œè¿œä½äºé‚£äº›æ˜ç¡®å¤–åœ¨åŒ–æ¨ç†æ­¥éª¤çš„æ¨¡å‹ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/wenquanlu/huginn-latent-cot%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/wenquanlu/huginn-latent-cotä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02199v2">PDF</a> First Workshop on the Application of LLM Explainability to Reasoning   and Planning at COLM 2025</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æ¢è®¨äº†Huginn-3.5Bæ·±åº¦å¾ªç¯Transformeræ¨¡å‹åœ¨æ¨ç†ç»“æ„æ–¹é¢çš„è¡¨ç°ã€‚ç ”ç©¶å‘ç°ï¼Œè¯¥æ¨¡å‹åœ¨æ¨ç†æ—¶èƒ½å¤Ÿé‡ç”¨å±‚è€Œä¸å¢åŠ å‚æ•°æ•°é‡ï¼Œå¯¹äºç®—æœ¯ä»»åŠ¡æœ‰ä¸€å®šçš„å†…éƒ¨è¡Œä¸ºè¡¨ç°ã€‚é€šè¿‡ä¸€ç³»åˆ—æ¢æµ‹æŠ€æœ¯ï¼Œå¦‚Logit Lenså’ŒCoda Lensï¼Œç ”ç©¶è€…å‘ç°è¯¥æ¨¡å‹æœ‰é™çš„è¯æ®è¡¨æ˜å…¶æœ‰å¯è§£é‡Šçš„æ½œåœ¨é“¾å¼æ¨ç†ï¼ˆlatent CoTï¼‰ã€‚æ­¤å¤–ï¼Œç ”ç©¶ä¸­å‘ç°ä¸åŒå¾ªç¯å—ä¹‹é—´çš„æ¢æµ‹ä¸ä¸€è‡´æ€§è¾ƒå¤§ï¼Œéšè—çŠ¶æ€çš„è§£é‡Šæ€§å–å†³äºå±‚ç´¢å¼•å’Œè§£ç æ–¹æ³•ã€‚æœ€åï¼Œé€šè¿‡å®è¯ç ”ç©¶ï¼Œå¢åŠ å¾ªç¯æ·±åº¦ä»…å¸¦æ¥å¾®å°æ”¶ç›Šï¼Œè¿œä½äºæ˜¾å¼å¤–éƒ¨åŒ–æ¨ç†æ­¥éª¤çš„æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Huginn-3.5Bæ¨¡å‹èƒ½å¤Ÿé‡ç”¨å±‚è¿›è¡Œæ¨ç†ï¼Œæé«˜äº†æ¨¡å‹çš„å†…éƒ¨è¡¨ç°ã€‚</li>
<li>é€šè¿‡æ¢æµ‹æŠ€æœ¯å‘ç°Huginn-3.5Bæ¨¡å‹æœ‰é™çš„è¯æ®è¡¨æ˜å…¶æœ‰å¯è§£é‡Šçš„æ½œåœ¨é“¾å¼æ¨ç†ï¼ˆlatent CoTï¼‰ã€‚</li>
<li>ä¸åŒå¾ªç¯å—ä¹‹é—´çš„æ¢æµ‹å­˜åœ¨ä¸ä¸€è‡´æ€§ï¼Œéšè—çŠ¶æ€çš„è§£é‡Šæ€§å—å±‚ç´¢å¼•å’Œè§£ç æ–¹æ³•å½±å“ã€‚</li>
<li>å¢åŠ å¾ªç¯æ·±åº¦å¸¦æ¥çš„æ”¶ç›Šæœ‰é™ï¼Œä¸å¦‚æ˜¾å¼å¤–éƒ¨åŒ–æ¨ç†æ­¥éª¤çš„æ¨¡å‹ã€‚</li>
<li>æ¨¡å‹åœ¨ç®—æœ¯ä»»åŠ¡ä¸Šå±•ç°å‡ºä¸€å®šçš„å†…éƒ¨è¡Œä¸ºè¡¨ç°ã€‚</li>
<li>Huginn-3.5Bæ¨¡å‹çš„ä»£ç å·²å…¬å¼€å¯ä¾›ç ”ç©¶ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02199">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-7019cccff91c30692d483a66c9f9c2c2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760100419&auth_key=1760100419-0-0-b3f06c812b672768d9432d158955391d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2b20ea4bf60eeec37cc45dea4d777e3b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760100426&auth_key=1760100426-0-0-d4cc0ce4e3434ea13b0aba6820bb161f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-31bf599efaf452cf2385dc05e3395a2e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760100432&auth_key=1760100432-0-0-19dcba16c61aa441b588c0403a7240cc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d5e5c35955d42a515819dfe93ac38d6b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760100439&auth_key=1760100439-0-0-35722aa9483ea55a01fbd758f1bb8d36&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0c35816660748d1480f588a07a637510~resize:0:q75.jpg?source=1f5c5e47&expiration=1760100446&auth_key=1760100446-0-0-2709a1482e5cbe5d00672cf86c54502e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="What-Characteristics-Make-ChatGPT-Effective-for-Software-Issue-Resolution-An-Empirical-Study-of-Task-Project-and-Conversational-Signals-in-GitHub-Issues"><a href="#What-Characteristics-Make-ChatGPT-Effective-for-Software-Issue-Resolution-An-Empirical-Study-of-Task-Project-and-Conversational-Signals-in-GitHub-Issues" class="headerlink" title="What Characteristics Make ChatGPT Effective for Software Issue   Resolution? An Empirical Study of Task, Project, and Conversational Signals   in GitHub Issues"></a>What Characteristics Make ChatGPT Effective for Software Issue   Resolution? An Empirical Study of Task, Project, and Conversational Signals   in GitHub Issues</h2><p><strong>Authors:Ramtin Ehsani, Sakshi Pathak, Esteban Parra, Sonia Haiduc, Preetha Chatterjee</strong></p>
<p>Conversational large-language models are extensively used for issue resolution tasks. However, not all developer-LLM conversations are useful for effective issue resolution. In this paper, we analyze 686 developer-ChatGPT conversations shared within GitHub issue threads to identify characteristics that make these conversations effective for issue resolution. First, we analyze the conversations and their corresponding issues to distinguish helpful from unhelpful conversations. We begin by categorizing the types of tasks developers seek help with to better understand the scenarios in which ChatGPT is most effective. Next, we examine a wide range of conversational, project, and issue-related metrics to uncover factors associated with helpful conversations. Finally, we identify common deficiencies in unhelpful ChatGPT responses to highlight areas that could inform the design of more effective developer-facing tools. We found that only 62% of the ChatGPT conversations were helpful for successful issue resolution. ChatGPT is most effective for code generation and tools&#x2F;libraries&#x2F;APIs recommendations, but struggles with code explanations. Helpful conversations tend to be shorter, more readable, and exhibit stronger semantic and linguistic alignment. Larger, more popular projects and more experienced developers benefit more from ChatGPT. At the issue level, ChatGPT performs best on simpler problems with limited developer activity and faster resolution, typically well-scoped tasks like compilation errors. The most common deficiencies in unhelpful ChatGPT responses include incorrect information and lack of comprehensiveness. Our findings have wide implications including guiding developers on effective interaction strategies for issue resolution, informing the development of tools or frameworks to support optimal prompt design, and providing insights on fine-tuning LLMs for issue resolution tasks. </p>
<blockquote>
<p>å¯¹è¯å¼å¤§å‹è¯­è¨€æ¨¡å‹è¢«å¹¿æ³›ç”¨äºé—®é¢˜è§£ç­”ä»»åŠ¡ã€‚ç„¶è€Œï¼Œå¹¶éæ‰€æœ‰å¼€å‘è€…ä¸å¤§å‹è¯­è¨€æ¨¡å‹çš„å¯¹è¯éƒ½å¯¹äºæœ‰æ•ˆè§£å†³é—®é¢˜æœ‰æ‰€å¸®åŠ©ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬åˆ†æäº†åœ¨GitHubé—®é¢˜çº¿ç¨‹ä¸­åˆ†äº«çš„686æ¡å¼€å‘è€…ä¸ChatGPTçš„å¯¹è¯ï¼Œä»¥è¯†åˆ«ä½¿è¿™äº›å¯¹è¯åœ¨è§£å†³é—®é¢˜æ–¹é¢æœ‰æ•ˆçš„ç‰¹å¾ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åˆ†æå¯¹è¯åŠå…¶ç›¸åº”çš„é—®é¢˜ï¼Œä»¥åŒºåˆ†æœ‰ç”¨çš„å¯¹è¯å’Œæ— ç”¨çš„å¯¹è¯ã€‚æˆ‘ä»¬é€šè¿‡åˆ†ç±»å¼€å‘è€…å¯»æ±‚å¸®åŠ©çš„ä»»åŠ¡ç±»å‹ï¼Œä»¥æ›´å¥½åœ°äº†è§£ChatGPTåœ¨å“ªäº›æƒ…å†µä¸‹æœ€ä¸ºæœ‰æ•ˆã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æ£€æŸ¥å„ç§ä¸å¯¹è¯ã€é¡¹ç›®å’Œé—®é¢˜ç›¸å…³çš„æŒ‡æ ‡ï¼Œä»¥å‘ç°ä¸æœ‰ç”¨å¯¹è¯ç›¸å…³çš„å› ç´ ã€‚æœ€åï¼Œæˆ‘ä»¬ç¡®å®šäº†ä¸å¥æ•ˆçš„ChatGPTå›å¤ä¸­çš„å¸¸è§ç¼ºé™·ï¼Œä»¥çªå‡ºå¯èƒ½å¯å‘è®¾è®¡é¢å‘å¼€å‘è€…çš„æ›´æœ‰æ•ˆå·¥å…·çš„åŒºåŸŸã€‚æˆ‘ä»¬å‘ç°åªæœ‰62%çš„ChatGPTå¯¹è¯å¯¹äºæˆåŠŸè§£å†³é—®é¢˜æ˜¯æœ‰å¸®åŠ©çš„ã€‚ChatGPTåœ¨ä»£ç ç”Ÿæˆå’Œå·¥å…·&#x2F;åº“&#x2F;APIæ¨èæ–¹é¢æœ€ä¸ºæœ‰æ•ˆï¼Œä½†åœ¨ä»£ç è§£é‡Šæ–¹é¢å´è¡¨ç°æŒ£æ‰ã€‚æœ‰ç”¨çš„å¯¹è¯å¾€å¾€è¾ƒçŸ­ã€å¯è¯»æ€§æ›´å¼ºï¼Œå¹¶è¡¨ç°å‡ºæ›´å¼ºçƒˆçš„è¯­ä¹‰å’Œè¯­è¨€å¯¹é½ã€‚è¾ƒå¤§çš„ã€è¾ƒå—æ¬¢è¿çš„é¡¹ç›®å’Œæ›´æœ‰ç»éªŒçš„å¼€å‘è€…ä»ChatGPTä¸­è·ç›Šæ›´å¤šã€‚åœ¨é—®é¢˜å±‚é¢ï¼ŒChatGPTåœ¨å¤„ç†å¼€å‘è€…æ´»åŠ¨è¾ƒå°‘ã€è§£å†³é€Ÿåº¦è¾ƒå¿«ã€èŒƒå›´æ˜ç¡®çš„ç®€å•é—®é¢˜ä¸Šè¡¨ç°æœ€ä½³ï¼Œå¦‚ç¼–è¯‘é”™è¯¯ã€‚ä¸å¥æ•ˆçš„ChatGPTå›å¤ä¸­æœ€å¸¸è§çš„ç¼ºé™·åŒ…æ‹¬ä¿¡æ¯ä¸æ­£ç¡®å’Œç¼ºä¹å®Œæ•´æ€§ã€‚æˆ‘ä»¬çš„å‘ç°å…·æœ‰å¹¿æ³›çš„æ„ä¹‰ï¼ŒåŒ…æ‹¬æŒ‡å¯¼å¼€å‘è€…è¿›è¡Œæœ‰æ•ˆçš„äº’åŠ¨ç­–ç•¥ä»¥è§£å†³é—®é¢˜ã€å¼€å‘å·¥å…·å’Œæ¡†æ¶ä»¥æ”¯æŒæœ€ä½³æç¤ºè®¾è®¡çš„å‘å±•ï¼Œä»¥åŠä¸ºé’ˆå¯¹é—®é¢˜è§£ç­”ä»»åŠ¡å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹æä¾›è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.22390v2">PDF</a> Accepted for publication in Empirical Software Engineering (EMSE),   2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡åˆ†æäº†åœ¨GitHubé—®é¢˜çº¿ç¨‹ä¸­åˆ†äº«çš„686ä¸ªå¼€å‘è€…ä¸ChatGPTçš„å¯¹è¯ï¼Œæ—¨åœ¨æ‰¾å‡ºè¿™äº›å¯¹è¯åœ¨é—®é¢˜è§£å†³æ–¹é¢æœ‰æ•ˆçš„ç‰¹å¾ã€‚ç ”ç©¶å‘ç°ï¼Œåªæœ‰62%çš„ChatGPTå¯¹è¯å¯¹æˆåŠŸçš„é—®é¢˜è§£å†³æœ‰æ‰€å¸®åŠ©ã€‚ChatGPTåœ¨ä»£ç ç”Ÿæˆã€å·¥å…·&#x2F;åº“&#x2F;APIæ¨èæ–¹é¢æœ€ä¸ºæœ‰æ•ˆï¼Œä½†åœ¨ä»£ç è§£é‡Šæ–¹é¢è¡¨ç°è¾ƒå·®ã€‚æœ‰ç›Šçš„å¯¹è¯å¾€å¾€æ›´çŸ­ã€å¯è¯»æ€§æ›´å¼ºï¼Œå¹¶è¡¨ç°å‡ºæ›´å¼ºçš„è¯­ä¹‰å’Œè¯­è¨€å­¦å¯¹é½ã€‚å¤§å‹ã€å—æ¬¢è¿çš„é¡¹ç›®çš„å¼€å‘è€…åŠæ›´æœ‰ç»éªŒçš„å¼€å‘è€…æ›´èƒ½ä»ChatGPTä¸­å—ç›Šã€‚åœ¨é—®é¢˜å±‚é¢ï¼ŒChatGPTåœ¨æœ€ç®€å•ã€å¼€å‘è€…æ´»åŠ¨æœ‰é™ã€è§£å†³é€Ÿåº¦å¿«çš„é—®é¢˜ä¸Šè¡¨ç°æœ€ä½³ï¼Œå¦‚ç¼–è¯‘é”™è¯¯ç­‰ä»»åŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶è€…å¯¹å¼€å‘è€…ä¸ChatGPTçš„å¯¹è¯è¿›è¡Œäº†åˆ†æï¼Œç›®çš„æ˜¯è¯†åˆ«è¿™äº›å¯¹è¯åœ¨é—®é¢˜è§£å†³æ–¹é¢æœ‰æ•ˆçš„ç‰¹å¾ã€‚</li>
<li>ChatGPTåœ¨ä»£ç ç”Ÿæˆå’Œå·¥å…·&#x2F;åº“&#x2F;APIæ¨èæ–¹é¢è¡¨ç°æœ€æœ‰æ•ˆï¼Œä½†åœ¨ä»£ç è§£é‡Šæ–¹é¢å­˜åœ¨å›°éš¾ã€‚</li>
<li>æœ‰ç›Šçš„å¯¹è¯é€šå¸¸è¾ƒçŸ­ã€å¯è¯»æ€§å¼ºï¼Œå¹¶å±•ç°å‡ºè‰¯å¥½çš„è¯­ä¹‰å’Œè¯­è¨€å­¦å¯¹é½ã€‚</li>
<li>å¤§å‹å’Œå—æ¬¢è¿çš„é¡¹ç›®çš„å¼€å‘è€…ä»¥åŠæ›´æœ‰ç»éªŒçš„å¼€å‘è€…æ›´èƒ½ä»ChatGPTçš„ååŠ©ä¸­å—ç›Šã€‚</li>
<li>ChatGPTåœ¨æœ€ç®€å•ã€æœ‰é™å¼€å‘è€…æ´»åŠ¨ã€å¿«é€Ÿè§£å†³çš„é—®é¢˜ä¸Šè¡¨ç°æœ€ä½³ï¼Œå¦‚å¤„ç†ç¼–è¯‘é”™è¯¯ç­‰ä»»åŠ¡ã€‚</li>
<li>å¸¸è§çš„ChatGPTå›ç­”ç¼ºé™·åŒ…æ‹¬ä¿¡æ¯ä¸æ­£ç¡®å’Œç¼ºä¹å…¨é¢æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.22390">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-4aa51b30327a6fdbaa8d6a85c52adb02~resize:0:q75.jpg?source=1f5c5e47&expiration=1760100453&auth_key=1760100453-0-0-26c623e4c8a94afe933208e43f5f5852&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4faa6a5e38422a6b72285850367b052a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760100460&auth_key=1760100460-0-0-7f015c5f3d4dad06bec455778b5d5c0a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b62f27c53a116ccb32f7a52eb8489bcb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760100466&auth_key=1760100466-0-0-765c9d873903f323fc32ad79b7d37ea0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Latent-Concept-Disentanglement-in-Transformer-based-Language-Models"><a href="#Latent-Concept-Disentanglement-in-Transformer-based-Language-Models" class="headerlink" title="Latent Concept Disentanglement in Transformer-based Language Models"></a>Latent Concept Disentanglement in Transformer-based Language Models</h2><p><strong>Authors:Guan Zhe Hong, Bhavya Vasudeva, Vatsal Sharan, Cyrus Rashtchian, Prabhakar Raghavan, Rina Panigrahy</strong></p>
<p>When large language models (LLMs) use in-context learning (ICL) to solve a new task, they must infer latent concepts from demonstration examples. This raises the question of whether and how transformers represent latent structures as part of their computation. Our work experiments with several controlled tasks, studying this question using mechanistic interpretability. First, we show that in transitive reasoning tasks with a latent, discrete concept, the model successfully identifies the latent concept and does step-by-step concept composition. This builds upon prior work that analyzes single-step reasoning. Then, we consider tasks parameterized by a latent numerical concept. We discover low-dimensional subspaces in the modelâ€™s representation space, where the geometry cleanly reflects the underlying parameterization. Overall, we show that small and large models can indeed disentangle and utilize latent concepts that they learn in-context from a handful of abbreviated demonstrations. </p>
<blockquote>
<p>å½“å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½¿ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰æ¥è§£å†³æ–°ä»»åŠ¡æ—¶ï¼Œå®ƒä»¬å¿…é¡»ä»æ¼”ç¤ºç¤ºä¾‹ä¸­æ¨æ–­å‡ºæ½œåœ¨æ¦‚å¿µã€‚è¿™å°±æå‡ºäº†ä¸€ä¸ªé—®é¢˜ï¼Œå³å˜å‹å™¨å¦‚ä½•åœ¨è®¡ç®—è¿‡ç¨‹ä¸­è¡¨ç¤ºæ½œåœ¨ç»“æ„ã€‚æˆ‘ä»¬çš„å·¥ä½œé€šè¿‡å‡ é¡¹å—æ§ä»»åŠ¡è¿›è¡Œå®éªŒï¼Œä½¿ç”¨æœºæ¢°è§£é‡Šæ³•ç ”ç©¶è¿™ä¸ªé—®é¢˜ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å±•ç¤ºäº†åœ¨å…·æœ‰æ½œåœ¨ç¦»æ•£æ¦‚å¿µçš„å…³ç³»æ¨ç†ä»»åŠ¡ä¸­ï¼Œæ¨¡å‹æˆåŠŸåœ°è¯†åˆ«äº†æ½œåœ¨æ¦‚å¿µï¼Œå¹¶è¿›è¡Œäº†é€æ­¥çš„æ¦‚å¿µç»„åˆã€‚è¿™æ˜¯åŸºäºä¹‹å‰å¯¹å•æ­¥æ¨ç†çš„åˆ†æå·¥ä½œã€‚ç„¶åï¼Œæˆ‘ä»¬è€ƒè™‘ç”±æ½œåœ¨æ•°å­¦æ¦‚å¿µå‚æ•°åŒ–çš„ä»»åŠ¡ã€‚æˆ‘ä»¬å‘ç°æ¨¡å‹è¡¨ç¤ºç©ºé—´ä¸­çš„ä½ç»´å­ç©ºé—´ï¼Œå…¶ä¸­çš„å‡ ä½•ç»“æ„æ¸…æ™°åœ°åæ˜ äº†æ½œåœ¨çš„å‚æ•°åŒ–ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬è¯æ˜äº†å°å‹å’Œå¤§å‹æ¨¡å‹ç¡®å®èƒ½å¤Ÿä»å°‘é‡ç®€çŸ­çš„æ¼”ç¤ºä¸­ï¼Œåœ¨ä¸Šä¸‹æ–‡ä¸­å­¦ä¹ å¹¶åˆ©ç”¨æ½œåœ¨æ¦‚å¿µã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.16975v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰è§£å†³æ–°ä»»åŠ¡æ—¶ï¼Œéœ€è¦ä»ç¤ºèŒƒä¾‹å­ä¸­æ¨æ–­æ½œåœ¨æ¦‚å¿µã€‚æœ¬ç ”ç©¶é€šè¿‡ä¸€ç³»åˆ—å—æ§ä»»åŠ¡ï¼Œè¿ç”¨æœºæ¢°å¯è§£é‡Šæ€§ç ”ç©¶è¿™ä¸€é—®é¢˜ã€‚ç ”ç©¶å‘ç°ï¼Œåœ¨å«æœ‰æ½œåœ¨ç¦»æ•£æ¦‚å¿µçš„åŠ¨ä½œæ¨ç†ä»»åŠ¡ä¸­ï¼Œæ¨¡å‹èƒ½å¤ŸæˆåŠŸè¯†åˆ«æ½œåœ¨æ¦‚å¿µå¹¶è¿›è¡Œé€æ­¥çš„æ¦‚å¿µç»„åˆã€‚æ­¤å¤–ï¼Œå¯¹äºç”±æ½œåœ¨æ•°å­¦æ¦‚å¿µå‚æ•°åŒ–çš„ä»»åŠ¡ï¼Œæ¨¡å‹åœ¨ä½ç»´å­ç©ºé—´ä¸­çš„è¡¨ç¤ºå‡ ä½•ç»“æ„æ¸…æ™°åœ°åæ˜ äº†æ½œåœ¨å‚æ•°åŒ–ã€‚æ€»ä½“è€Œè¨€ï¼Œæœ¬ç ”ç©¶è¡¨æ˜å°å‹å’Œå¤§å‹æ¨¡å‹ç¡®å®èƒ½å¤Ÿä»ç®€çŸ­çš„ç¤ºèŒƒä¸­å­¦ä¹ å¹¶åº”ç”¨æ½œåœ¨æ¦‚å¿µã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMé€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ è§£å†³æ–°ä»»åŠ¡æ—¶ï¼Œèƒ½ä»ç¤ºèŒƒä¾‹å­ä¸­æ¨æ–­æ½œåœ¨æ¦‚å¿µã€‚</li>
<li>åœ¨åŠ¨ä½œæ¨ç†ä»»åŠ¡ä¸­ï¼Œæ¨¡å‹èƒ½æˆåŠŸè¯†åˆ«ç¦»æ•£æ½œåœ¨æ¦‚å¿µå¹¶è¿›è¡Œé€æ­¥æ¦‚å¿µç»„åˆã€‚</li>
<li>ç ”ç©¶é€šè¿‡æœºæ¢°å¯è§£é‡Šæ€§ç ”ç©¶è¿™ä¸€é—®é¢˜ï¼Œæ¶‰åŠå¤šä¸ªå—æ§ä»»åŠ¡ã€‚</li>
<li>æ¨¡å‹åœ¨ä½ç»´å­ç©ºé—´ä¸­çš„è¡¨ç¤ºå‡ ä½•ç»“æ„åæ˜ äº†æ½œåœ¨å‚æ•°åŒ–ã€‚</li>
<li>å°å‹å’Œå¤§å‹æ¨¡å‹éƒ½èƒ½ä»ç®€çŸ­çš„ç¤ºèŒƒä¸­å­¦ä¹ å¹¶åº”ç”¨æ½œåœ¨æ¦‚å¿µã€‚</li>
<li>æ¨¡å‹åœ¨è¯†åˆ«å’Œåº”ç”¨æ½œåœ¨æ¦‚å¿µæ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.16975">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-baefcce813696a56424f9668e1148a09~resize:0:q75.jpg?source=1f5c5e47&expiration=1760103334&auth_key=1760103334-0-0-a1de7521093935eddc81a097e9cfc45f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ad864dbc7c6013bb8fd873c56a90e8cf~resize:0:q75.jpg?source=1f5c5e47&expiration=1760100538&auth_key=1760100538-0-0-e6987aab51e97223a129197b13116d24&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-35ded8fa8e23c4f9ee08ca2c41f7337a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760100544&auth_key=1760100544-0-0-de076000353ea9dd9f4c7dd5710d414f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4bf25c740a2db05847eacf7b8c514e01~resize:0:q75.jpg?source=1f5c5e47&expiration=1760100551&auth_key=1760100551-0-0-beb86674e6fcfc2878d6601fb4a8ef44&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Struct2D-A-Perception-Guided-Framework-for-Spatial-Reasoning-in-Large-Multimodal-Models"><a href="#Struct2D-A-Perception-Guided-Framework-for-Spatial-Reasoning-in-Large-Multimodal-Models" class="headerlink" title="Struct2D: A Perception-Guided Framework for Spatial Reasoning in Large   Multimodal Models"></a>Struct2D: A Perception-Guided Framework for Spatial Reasoning in Large   Multimodal Models</h2><p><strong>Authors:Fangrui Zhu, Hanhui Wang, Yiming Xie, Jing Gu, Tianye Ding, Jianwei Yang, Huaizu Jiang</strong></p>
<p>Unlocking spatial reasoning in Large Multimodal Models (LMMs) is crucial for enabling intelligent interaction with 3D environments. While prior efforts often rely on explicit 3D inputs or specialized model architectures, we ask: can LMMs reason about 3D space using only structured 2D representations derived from perception? We introduce Struct2D, a perception-guided prompting framework that combines birdâ€™s-eye-view (BEV) images with object marks and object-centric metadata, optionally incorporating egocentric keyframes when needed. Using Struct2D, we conduct an in-depth zero-shot analysis of closed-source LMMs (e.g., GPT-o3) and find that they exhibit surprisingly strong spatial reasoning abilities when provided with structured 2D inputs, effectively handling tasks such as relative direction estimation and route planning. Building on these insights, we construct Struct2D-Set, a large-scale instruction tuning dataset with 200K fine-grained QA pairs across eight spatial reasoning categories, generated automatically from 3D indoor scenes. We fine-tune an open-source LMM (Qwen2.5VL) on Struct2D-Set, achieving competitive performance on multiple benchmarks, including 3D question answering, dense captioning, and object grounding. Our approach demonstrates that structured 2D inputs can effectively bridge perception and language reasoning in LMMs-without requiring explicit 3D representations as input. We will release both our code and dataset to support future research. </p>
<blockquote>
<p>åœ¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰ä¸­è§£é”ç©ºé—´æ¨ç†èƒ½åŠ›å¯¹äºå®ç°ä¸3Dç¯å¢ƒçš„æ™ºèƒ½äº¤äº’è‡³å…³é‡è¦ã€‚è™½ç„¶ä¹‹å‰çš„åŠªåŠ›é€šå¸¸ä¾èµ–äºæ˜ç¡®çš„3Dè¾“å…¥æˆ–ä¸“é—¨çš„æ¨¡å‹æ¶æ„ï¼Œä½†æˆ‘ä»¬çš„é—®é¢˜æ˜¯ï¼šLMMsèƒ½å¦ä»…ä½¿ç”¨ä»æ„ŸçŸ¥ä¸­æ´¾ç”Ÿçš„ç»“æ„åŒ–2Dè¡¨ç¤ºæ¥æ¨ç†3Dç©ºé—´ï¼Ÿæˆ‘ä»¬å¼•å…¥äº†Struct2Dï¼Œè¿™æ˜¯ä¸€ä¸ªæ„ŸçŸ¥å¼•å¯¼æç¤ºæ¡†æ¶ï¼Œå®ƒå°†é¸Ÿç°å›¾ï¼ˆBEVï¼‰å›¾åƒä¸å¯¹è±¡æ ‡è®°å’Œå¯¹è±¡ä¸­å¿ƒå…ƒæ•°æ®ç›¸ç»“åˆï¼Œåœ¨éœ€è¦æ—¶è¿˜å¯çº³å…¥ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„å…³é”®å¸§ã€‚ä½¿ç”¨Struct2Dï¼Œæˆ‘ä»¬å¯¹å°é—­çš„LMMï¼ˆä¾‹å¦‚GPT-o3ï¼‰è¿›è¡Œäº†æ·±å…¥çš„é›¶æ ·æœ¬åˆ†æï¼Œå¹¶å‘ç°å½“æä¾›ç»“æ„åŒ–2Dè¾“å…¥æ—¶ï¼Œå®ƒä»¬è¡¨ç°å‡ºä»¤äººæƒŠè®¶çš„å¼ºå¤§ç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œå¯ä»¥æœ‰æ•ˆåœ°å¤„ç†ç›¸å¯¹æ–¹å‘ä¼°è®¡å’Œè·¯çº¿è§„åˆ’ç­‰ä»»åŠ¡ã€‚åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬æ„å»ºäº†Struct2D-Setï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†ï¼ŒåŒ…å«è·¨è¶Šå…«ä¸ªç©ºé—´æ¨ç†ç±»åˆ«çš„20ä¸‡ä¸ªç²¾ç»†ç²’åº¦é—®ç­”å¯¹ï¼Œè¿™äº›é—®ç­”å¯¹æ˜¯è‡ªåŠ¨ä»å®¤å†…3Dåœºæ™¯ä¸­ç”Ÿæˆçš„ã€‚æˆ‘ä»¬åœ¨Struct2D-Setä¸Šå¯¹å¼€æºLMMï¼ˆQwen2.5VLï¼‰è¿›è¡Œäº†å¾®è°ƒï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„è¡¨ç°ï¼ŒåŒ…æ‹¬3Dé—®ç­”ã€å¯†é›†å­—å¹•å’Œå¯¹è±¡å®šä½ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¡¨æ˜ï¼Œç»“æ„åŒ–äºŒç»´è¾“å…¥å¯ä»¥æœ‰æ•ˆåœ°åœ¨LMMsçš„æ„ŸçŸ¥å’Œè¯­è¨€æ¨ç†ä¹‹é—´å»ºç«‹æ¡¥æ¢ï¼Œè€Œæ— éœ€æ˜ç¡®çš„ä¸‰ç»´è¡¨ç¤ºä½œä¸ºè¾“å…¥ã€‚æˆ‘ä»¬å°†å‘å¸ƒæˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®é›†ä»¥æ”¯æŒæœªæ¥çš„ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04220v2">PDF</a> NeurIPS 2025, code link: <a target="_blank" rel="noopener" href="https://github.com/neu-vi/struct2d">https://github.com/neu-vi/struct2d</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨3Dç©ºé—´ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡å¼•å…¥Struct2Dæ¡†æ¶ï¼Œå°†é¸Ÿç°å›¾ã€ç›®æ ‡æ ‡è®°å’Œå¯¹è±¡ä¸­å¿ƒå…ƒæ•°æ®ç›¸ç»“åˆï¼Œæœ‰æ•ˆåœ°åˆ©ç”¨ç»“æ„åŒ–äºŒç»´è¾“å…¥è¿›è¡Œç©ºé—´æ¨ç†ä»»åŠ¡ï¼Œå¦‚ç›¸å¯¹æ–¹å‘ä¼°è®¡å’Œè·¯çº¿è§„åˆ’ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæ„å»ºäº†å¤§è§„æ¨¡æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†Struct2D-Setï¼Œå¯¹å¼€æºLMMè¿›è¡Œå¾®è°ƒï¼Œå®ç°å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„ç«äº‰åŠ›è¡¨ç°ã€‚è¯¥ç ”ç©¶å±•ç¤ºäº†ç»“æ„åŒ–äºŒç»´è¾“å…¥èƒ½å¤Ÿæœ‰æ•ˆæ¡¥æ¥æ„ŸçŸ¥å’Œè¯­è¨€æ¨ç†åœ¨LMMsä¸­çš„å·®è·ï¼Œæ— éœ€ä¾èµ–æ˜ç¡®çš„3Dè¡¨ç¤ºä½œä¸ºè¾“å…¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨3Dç©ºé—´æ¨ç†æ–¹é¢å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
<li>å¼•å…¥Struct2Dæ¡†æ¶ï¼Œç»“åˆé¸Ÿç°å›¾ã€ç›®æ ‡æ ‡è®°å’Œå¯¹è±¡ä¸­å¿ƒå…ƒæ•°æ®ï¼Œå®ç°ç»“æ„åŒ–äºŒç»´è¾“å…¥ã€‚</li>
<li>LMMsåœ¨æä¾›ç»“æ„åŒ–äºŒç»´è¾“å…¥æ—¶è¡¨ç°å‡ºå¼ºå¤§çš„ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚</li>
<li>æ„å»ºå¤§è§„æ¨¡æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†Struct2D-Setï¼Œç”¨äºå¾®è°ƒLMMsã€‚</li>
<li>é€šè¿‡å¯¹Struct2D-Setçš„å¾®è°ƒï¼ŒLMMåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>ç»“æ„åŒ–äºŒç»´è¾“å…¥èƒ½æœ‰æ•ˆæ¡¥æ¥æ„ŸçŸ¥å’Œè¯­è¨€æ¨ç†åœ¨LMMsä¸­çš„å·®è·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04220">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-abdb5894a51be5ba9851c9fcbfb9322d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760100559&auth_key=1760100559-0-0-0fecb203cc2dcd1fa146d152c68d9abb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f1e92279889af57c57d676ff0ab48c08~resize:0:q75.jpg?source=1f5c5e47&expiration=1760100566&auth_key=1760100566-0-0-ff655633ef1df20b010989e1780153dc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-cd356ebde3bce084457464d5270cebac~resize:0:q75.jpg?source=1f5c5e47&expiration=1760100572&auth_key=1760100572-0-0-b3f43f6814252d280f826f8bcb764441&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5a8a66b5d694436a30af598889346c88~resize:0:q75.jpg?source=1f5c5e47&expiration=1760100579&auth_key=1760100579-0-0-466c7b29ba51fd380929770b09b3fc33&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d091530aaa4ae0ad8f4acf73c34fe5ce~resize:0:q75.jpg?source=1f5c5e47&expiration=1760100586&auth_key=1760100586-0-0-022f66c95ae4835f56f5932c3904beb6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-64bc7cef72d9d7c88c2bcfd2a661bf6e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760100593&auth_key=1760100593-0-0-029ca322ca5057255f3b4f0198ddee38&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="AgentMisalignment-Measuring-the-Propensity-for-Misaligned-Behaviour-in-LLM-Based-Agents"><a href="#AgentMisalignment-Measuring-the-Propensity-for-Misaligned-Behaviour-in-LLM-Based-Agents" class="headerlink" title="AgentMisalignment: Measuring the Propensity for Misaligned Behaviour in   LLM-Based Agents"></a>AgentMisalignment: Measuring the Propensity for Misaligned Behaviour in   LLM-Based Agents</h2><p><strong>Authors:Akshat Naik, Patrick Quinn, Guillermo Bosch, Emma GounÃ©, Francisco Javier Campos Zabala, Jason Ross Brown, Edward James Young</strong></p>
<p>As Large Language Model (LLM) agents become more widespread, associated misalignment risks increase. While prior research has studied agentsâ€™ ability to produce harmful outputs or follow malicious instructions, it remains unclear how likely agents are to spontaneously pursue unintended goals in realistic deployments. In this work, we approach misalignment as a conflict between the internal goals pursued by the model and the goals intended by its deployer. We introduce a misalignment propensity benchmark, \textsc{AgentMisalignment}, a benchmark suite designed to evaluate the propensity of LLM agents to misalign in realistic scenarios. Evaluations cover behaviours such as avoiding oversight, resisting shutdown, sandbagging, and power-seeking. Testing frontier models, we find that more capable agents tend to exhibit higher misalignment on average. We also systematically vary agent personalities through different system prompts and observe that persona characteristics can strongly and unpredictably influence misalignment, sometimes more than the choice of model itself. Our results reveal the limitations of current alignment methods for autonomous LLM agents and underscore the need to rethink misalignment in realistic deployment settings. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†çš„æ™®åŠï¼Œç›¸å…³çš„å¯¹é½é£é™©ä¹Ÿåœ¨å¢åŠ ã€‚è™½ç„¶ä¹‹å‰çš„ç ”ç©¶å·²ç»ç ”ç©¶äº†ä»£ç†äº§ç”Ÿæœ‰å®³è¾“å‡ºæˆ–æ‰§è¡Œæ¶æ„æŒ‡ä»¤çš„èƒ½åŠ›ï¼Œä½†åœ¨å®é™…éƒ¨ç½²ä¸­ï¼Œä»£ç†è‡ªå‘è¿½æ±‚æ„å¤–ç›®æ ‡çš„å¯èƒ½æ€§ä»ç„¶ä¸æ¸…æ¥šã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å°†é”™ä½é—®é¢˜è§†ä¸ºæ¨¡å‹å†…éƒ¨è¿½æ±‚çš„ç›®æ ‡ä¸å…¶éƒ¨ç½²è€…æ„å›¾è¿½æ±‚çš„ç›®æ ‡ä¹‹é—´çš„å†²çªã€‚æˆ‘ä»¬å¼•å…¥äº†é”™ä½å€¾å‘åŸºå‡†æµ‹è¯•â€œAgentMisalignmentâ€ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°LLMä»£ç†åœ¨ç°å®åœºæ™¯ä¸­é”™ä½å€¾å‘çš„åŸºå‡†æµ‹è¯•å¥—ä»¶ã€‚è¯„ä¼°åŒ…æ‹¬é¿å…ç›‘ç£ã€æŠµæŠ—å…³é—­ã€æ²™è¢‹é˜²å¾¡å’ŒæƒåŠ›å¯»æ±‚ç­‰è¡Œä¸ºã€‚é€šè¿‡å¯¹å‰æ²¿æ¨¡å‹çš„æµ‹è¯•ï¼Œæˆ‘ä»¬å‘ç°æ›´å¼ºå¤§çš„ä»£ç†å¾€å¾€æ›´å®¹æ˜“å‡ºç°é”™ä½ã€‚æˆ‘ä»¬è¿˜é€šè¿‡ä¸åŒçš„ç³»ç»Ÿæç¤ºç³»ç»Ÿåœ°æ”¹å˜äº†ä»£ç†çš„ä¸ªæ€§ï¼Œå¹¶å‘ç°ä¸ªæ€§ç‰¹å¾å¯ä»¥å¼ºçƒˆä¸”ä¸å¯é¢„æµ‹åœ°å½±å“é”™ä½ï¼Œæœ‰æ—¶ç”šè‡³è¶…è¿‡æ¨¡å‹æœ¬èº«çš„é€‰æ‹©ã€‚æˆ‘ä»¬çš„ç»“æœæ­ç¤ºäº†å½“å‰è‡ªä¸»LLMä»£ç†å¯¹é½æ–¹æ³•çš„å±€é™æ€§ï¼Œå¹¶å¼ºè°ƒäº†éœ€è¦é‡æ–°æ€è€ƒç°å®éƒ¨ç½²ç¯å¢ƒä¸­çš„é”™ä½é—®é¢˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04018v2">PDF</a> Prepint, under review for NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†çš„æ™®åŠï¼Œç›¸å…³çš„è¯¯å¯¹é½é£é™©ä¹Ÿåœ¨å¢åŠ ã€‚æœ¬æ–‡ç ”ç©¶äº†LLMä»£ç†åœ¨çœŸå®åœºæ™¯ä¸­è‡ªå‘è¿½æ±‚éé¢„æœŸç›®æ ‡çš„å¯èƒ½æ€§ã€‚é€šè¿‡å¼•å…¥è¯¯å¯¹é½å€¾å‘åŸºå‡†æµ‹è¯•\text{AgentMisalignment}ï¼Œè¯„ä¼°ä»£ç†åœ¨é¿å…ç›‘ç£ã€æŠµæŠ—å…³é—­ã€ç ‚è¢‹å’ŒæƒåŠ›å¯»æ±‚ç­‰è¡Œä¸ºä¸Šçš„è¯¯å¯¹é½ç¨‹åº¦ã€‚è¯„ä¼°å‘ç°ï¼Œæ›´å¼ºå¤§çš„ä»£ç†å¾€å¾€æ›´å®¹æ˜“å‡ºç°è¯¯å¯¹é½ã€‚æ­¤å¤–ï¼Œé€šè¿‡ç³»ç»Ÿæ”¹å˜ä»£ç†çš„ä¸ªæ€§æç¤ºï¼Œå‘ç°äººæ ¼ç‰¹å¾ä¼šå¼ºçƒˆä¸”ä¸å¯é¢„æµ‹åœ°å½±å“è¯¯å¯¹é½ç¨‹åº¦ï¼Œæœ‰æ—¶ç”šè‡³è¶…è¿‡æ¨¡å‹æœ¬èº«çš„é€‰æ‹©ã€‚è¿™æ­ç¤ºäº†å½“å‰è‡ªä¸»LLMä»£ç†å¯¹é½æ–¹æ³•çš„å±€é™æ€§ï¼Œå¹¶å¼ºè°ƒéœ€è¦åœ¨çœŸå®çš„éƒ¨ç½²ç¯å¢ƒä¸­é‡æ–°æ€è€ƒè¯¯å¯¹é½é—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMä»£ç†çš„æ™®åŠå¢åŠ äº†è¯¯å¯¹é½é£é™©ã€‚</li>
<li>å¼•å…¥äº†\text{AgentMisalignment}åŸºå‡†æµ‹è¯•æ¥è¯„ä¼°LLMä»£ç†åœ¨çœŸå®åœºæ™¯ä¸­çš„è¯¯å¯¹é½å€¾å‘ã€‚</li>
<li>è¯„ä¼°åŒ…æ‹¬é¿å…ç›‘ç£ã€æŠµæŠ—å…³é—­ã€ç ‚è¢‹å’ŒæƒåŠ›å¯»æ±‚ç­‰è¡Œä¸ºã€‚</li>
<li>æ›´å¼ºå¤§çš„ä»£ç†æ›´å®¹æ˜“å‡ºç°è¯¯å¯¹é½ã€‚</li>
<li>ä»£ç†çš„ä¸ªæ€§ç‰¹å¾å¯¹è¯¯å¯¹é½çš„å½±å“å¼ºçƒˆä¸”ä¸å¯é¢„æµ‹ã€‚</li>
<li>å½“å‰LLMä»£ç†å¯¹é½æ–¹æ³•å­˜åœ¨å±€é™æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04018">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-76465c4a539f7313a68b151c8a6c4c80~resize:0:q75.jpg?source=1f5c5e47&expiration=1760100600&auth_key=1760100600-0-0-7dfd6aa64c8b3effcf711d934e4a2dff&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-cb3dc5ee1b9f11ed517d1aa6bfcf9474~resize:0:q75.jpg?source=1f5c5e47&expiration=1760100608&auth_key=1760100608-0-0-183a66b5d7dc6df62560408502d05b0b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="VisCoder-Fine-Tuning-LLMs-for-Executable-Python-Visualization-Code-Generation"><a href="#VisCoder-Fine-Tuning-LLMs-for-Executable-Python-Visualization-Code-Generation" class="headerlink" title="VisCoder: Fine-Tuning LLMs for Executable Python Visualization Code   Generation"></a>VisCoder: Fine-Tuning LLMs for Executable Python Visualization Code   Generation</h2><p><strong>Authors:Yuansheng Ni, Ping Nie, Kai Zou, Xiang Yue, Wenhu Chen</strong></p>
<p>Large language models (LLMs) often struggle with visualization tasks like plotting diagrams, charts, where success depends on both code correctness and visual semantics. Existing instruction-tuning datasets lack execution-grounded supervision and offer limited support for iterative code correction, resulting in fragile and unreliable plot generation. We present VisCode-200K, a large-scale instruction tuning dataset for Python-based visualization and self-correction. It contains over 200K examples from two sources: (1) validated plotting code from open-source repositories, paired with natural language instructions and rendered plots; and (2) 45K multi-turn correction dialogues from Code-Feedback, enabling models to revise faulty code using runtime feedback. We fine-tune Qwen2.5-Coder-Instruct on VisCode-200K to create VisCoder, and evaluate it on PandasPlotBench. VisCoder significantly outperforms strong open-source baselines and approaches the performance of proprietary models like GPT-4o-mini. We further adopt a self-debug evaluation protocol to assess iterative repair, demonstrating the benefits of feedback-driven learning for executable, visually accurate code generation. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç»˜å›¾ã€åˆ¶ä½œå›¾è¡¨ç­‰å¯è§†åŒ–ä»»åŠ¡æ–¹é¢ç»å¸¸é‡åˆ°å›°éš¾ï¼Œè¿™äº›ä»»åŠ¡çš„æˆåŠŸå–å†³äºä»£ç æ­£ç¡®æ€§å’Œè§†è§‰è¯­ä¹‰ä¸¤æ–¹é¢ã€‚ç°æœ‰çš„æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ç¼ºä¹æ‰§è¡Œå±‚é¢çš„ç›‘ç£ï¼Œå¯¹è¿­ä»£ä»£ç ä¿®æ­£çš„æ”¯æŒæœ‰é™ï¼Œå¯¼è‡´ç»˜å›¾ç”Ÿæˆç»“æœè„†å¼±ä¸”ä¸å¯é ã€‚æˆ‘ä»¬æ¨å‡ºäº†VisCode-200Kï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºPythonå¯è§†åŒ–åŠè‡ªæˆ‘ä¿®æ­£çš„å¤§å‹æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ã€‚å®ƒåŒ…å«è¶…è¿‡20ä¸‡ä¸ªæ¥è‡ªä¸¤ä¸ªæ¥æºçš„ç¤ºä¾‹ï¼š1ï¼‰æ¥è‡ªå¼€æºå­˜å‚¨åº“çš„ç»è¿‡éªŒè¯çš„ç»˜å›¾ä»£ç ï¼Œä¸è‡ªç„¶è¯­è¨€æŒ‡ä»¤å’Œæ¸²æŸ“å›¾é…å¯¹ï¼›2ï¼‰æ¥è‡ªCode-Feedbackçš„4.5ä¸‡ä¸ªå¤šè½®ä¿®æ­£å¯¹è¯ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåˆ©ç”¨è¿è¡Œæ—¶åé¦ˆä¿®æ­£é”™è¯¯ä»£ç ã€‚æˆ‘ä»¬å¯¹Qwain2.5-Coder-Instructè¿›è¡Œäº†VisCode-200Kå¾®è°ƒï¼Œåˆ›å»ºäº†VisCoderï¼Œå¹¶åœ¨PandasPlotBenchä¸Šå¯¹å…¶è¿›è¡Œäº†è¯„ä¼°ã€‚VisCoderæ˜¾è‘—ä¼˜äºå¼ºå¤§çš„å¼€æºåŸºå‡†çº¿ï¼Œå¹¶æ¥è¿‘ä¸“æœ‰æ¨¡å‹ï¼ˆå¦‚GPT-4o-miniï¼‰çš„æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜é‡‡ç”¨è‡ªæˆ‘è°ƒè¯•è¯„ä¼°åè®®æ¥è¯„ä¼°è¿­ä»£ä¿®å¤ï¼Œè¯æ˜äº†åé¦ˆé©±åŠ¨å­¦ä¹ å¯¹äºå¯æ‰§è¡Œä¸”è§†è§‰å‡†ç¡®çš„ä»£ç ç”Ÿæˆçš„å¥½å¤„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03930v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†å¯è§†åŒ–ä»»åŠ¡ï¼ˆå¦‚ç»˜åˆ¶å›¾è¡¨ï¼‰æ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œè¯¥æ–‡æå‡ºäº†ä¸€ç§æ–°çš„Pythonå¯è§†åŒ–æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†VisCode-200Kã€‚è¯¥æ•°æ®é›†åŒ…å«äº†è¶…è¿‡äºŒåä¸‡ç¤ºä¾‹ï¼Œç”¨äºæ”¯æŒæ¨¡å‹ä¿®æ­£é”™è¯¯ä»£ç å¹¶åˆ©ç”¨è¿è¡Œæ—¶åé¦ˆè¿›è¡Œè‡ªæˆ‘ä¿®æ­£ã€‚é€šè¿‡å¯¹Qwen2.5-Coder-Instructåœ¨VisCode-200Kä¸Šçš„å¾®è°ƒï¼Œåˆ›å»ºäº†VisCoderï¼Œå¹¶åœ¨PandasPlotBenchä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚VisCoderæ˜¾è‘—ä¼˜äºå¼€æºåŸºçº¿å¹¶æ¥è¿‘å¦‚GPT-4o-miniç­‰ä¸“æœ‰æ¨¡å‹çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜é‡‡ç”¨äº†è‡ªæˆ‘è°ƒè¯•è¯„ä¼°åè®®æ¥è¯„ä¼°è¿­ä»£ä¿®å¤èƒ½åŠ›ï¼Œè¯æ˜äº†åé¦ˆé©±åŠ¨å­¦ä¹ å¯¹äºå¯æ‰§è¡Œä¸”è§†è§‰å‡†ç¡®çš„ä»£ç ç”Ÿæˆçš„ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨å¤„ç†å¯è§†åŒ–ä»»åŠ¡æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œéœ€è¦åŒæ—¶è€ƒè™‘ä»£ç æ­£ç¡®æ€§å’Œè§†è§‰è¯­ä¹‰ã€‚</li>
<li>ç°æœ‰æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†ç¼ºä¹æ‰§è¡Œç›‘ç£ï¼Œå¯¹è¿­ä»£ä»£ç ä¿®æ­£çš„æ”¯æŒæœ‰é™ã€‚</li>
<li>æå‡ºäº†VisCode-200Kæ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡äºŒåä¸‡ç¤ºä¾‹ï¼Œç”¨äºPythonå¯è§†åŒ–è‡ªæˆ‘ä¿®æ­£ã€‚</li>
<li>VisCode-200KåŒ…å«ä¸¤ä¸ªæ¥æºçš„æ•°æ®ï¼šæ¥è‡ªå¼€æºä»“åº“çš„éªŒè¯ç»˜å›¾ä»£ç å’Œè‡ªç„¶è¯­è¨€æŒ‡ä»¤ä»¥åŠæ¸²æŸ“çš„å›¾è¡¨ï¼›æ¥è‡ªCode-Feedbackçš„4.5ä¸‡å¤šæ¬¡ä¿®æ­£å¯¹è¯ã€‚</li>
<li>é€šè¿‡åœ¨VisCode-200Kä¸Šå¾®è°ƒQwen2.5-Coder-Instructåˆ›å»ºVisCoderã€‚</li>
<li>VisCoderåœ¨PandasPlotBenchä¸Šçš„è¡¨ç°ä¼˜äºå¼ºå¼€æºåŸºçº¿å¹¶æ¥è¿‘GPT-4o-miniç­‰ä¸“æœ‰æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03930">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-795938778c05391221b0159f762dc756~resize:0:q75.jpg?source=1f5c5e47&expiration=1760100617&auth_key=1760100617-0-0-c32dc329ef1512ace2006c7ea48cff4c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bb5c2f203773dc5cb5af2d984c853626~resize:0:q75.jpg?source=1f5c5e47&expiration=1760100625&auth_key=1760100625-0-0-0eea9ae70b107d44b71653e0be4d6b17&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-046a0aaffc61973abe1d196a615529f9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760103342&auth_key=1760103342-0-0-b9fcd98475b3cb3c864f5fd0e16f3520&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6dff8815e57a95ddff797143c9b903c5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760103349&auth_key=1760103349-0-0-d630889d21da943d958332ef9bb3183f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Attributing-Response-to-Context-A-Jensen-Shannon-Divergence-Driven-Mechanistic-Study-of-Context-Attribution-in-Retrieval-Augmented-Generation"><a href="#Attributing-Response-to-Context-A-Jensen-Shannon-Divergence-Driven-Mechanistic-Study-of-Context-Attribution-in-Retrieval-Augmented-Generation" class="headerlink" title="Attributing Response to Context: A Jensen-Shannon Divergence Driven   Mechanistic Study of Context Attribution in Retrieval-Augmented Generation"></a>Attributing Response to Context: A Jensen-Shannon Divergence Driven   Mechanistic Study of Context Attribution in Retrieval-Augmented Generation</h2><p><strong>Authors:Ruizhe Li, Chen Chen, Yuchen Hu, Yanjun Gao, Xi Wang, Emine Yilmaz</strong></p>
<p>Retrieval-Augmented Generation (RAG) leverages large language models (LLMs) combined with external contexts to enhance the accuracy and reliability of generated responses. However, reliably attributing generated content to specific context segments, context attribution, remains challenging due to the computationally intensive nature of current methods, which often require extensive fine-tuning or human annotation. In this work, we introduce a novel Jensen-Shannon Divergence driven method to Attribute Response to Context (ARC-JSD), enabling efficient and accurate identification of essential context sentences without additional fine-tuning, gradient-calculation or surrogate modelling. Evaluations on a wide range of RAG benchmarks, such as TyDi QA, Hotpot QA, and Musique, using instruction-tuned LLMs in different scales demonstrate superior accuracy and significant computational efficiency improvements compared to the previous surrogate-based method. Furthermore, our mechanistic analysis reveals specific attention heads and multilayer perceptron (MLP) layers responsible for context attribution, providing valuable insights into the internal workings of RAG models and how they affect RAG behaviours. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/ruizheliUOA/ARC_JSD">https://github.com/ruizheliUOA/ARC_JSD</a>. </p>
<blockquote>
<p>æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç»“åˆå¤–éƒ¨ä¸Šä¸‹æ–‡ï¼Œä»¥æé«˜ç”Ÿæˆå“åº”çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚ç„¶è€Œï¼Œç”±äºå½“å‰æ–¹æ³•çš„è®¡ç®—å¯†é›†æ€§è´¨ï¼Œå°†ç”Ÿæˆçš„å†…å®¹å¯é åœ°å½’å› äºç‰¹å®šçš„ä¸Šä¸‹æ–‡æ®µè½ï¼Œå³ä¸Šä¸‹æ–‡å½’å› ï¼Œä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œè¿™é€šå¸¸éœ€è¦å¤§é‡çš„å¾®è°ƒæˆ–äººå·¥æ ‡æ³¨ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹çš„åŸºäºJensen-Shannon Divergenceçš„å½’å› å“åº”åˆ°ä¸Šä¸‹æ–‡ï¼ˆARC-JSDï¼‰æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨æ— éœ€é¢å¤–å¾®è°ƒã€æ¢¯åº¦è®¡ç®—æˆ–æ›¿ä»£å»ºæ¨¡çš„æƒ…å†µä¸‹ï¼Œé«˜æ•ˆå‡†ç¡®åœ°è¯†åˆ«å‡ºå…³é”®çš„ä¸Šä¸‹æ–‡å¥å­ã€‚åœ¨TyDi QAã€Hotpot QAå’ŒMusiqueç­‰ä¸€ç³»åˆ—RAGåŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°ï¼Œä½¿ç”¨ä¸åŒè§„æ¨¡æŒ‡ä»¤è°ƒæ•´çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæ˜¾ç¤ºå‡ºä¸åŸºäºæ›¿ä»£çš„å…ˆå‰æ–¹æ³•ç›¸æ¯”ï¼Œæ›´é«˜çš„å‡†ç¡®æ€§å’Œæ˜¾è‘—çš„è®¡ç®—æ•ˆç‡æ”¹è¿›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æœºåˆ¶åˆ†ææ­ç¤ºäº†è´Ÿè´£ä¸Šä¸‹æ–‡å½’å› çš„ç‰¹å®šæ³¨æ„åŠ›å¤´å’Œå¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰å±‚ï¼Œä¸ºç†è§£RAGæ¨¡å‹çš„å†…éƒ¨å·¥ä½œåŸç†ä»¥åŠå®ƒä»¬å¦‚ä½•å½±å“RAGè¡Œä¸ºæä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ruizheliUOA/ARC_JSD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ruizheliUOA/ARC_JSDæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16415v3">PDF</a> Accepted at NeurIPS 2025 Mechanistic Interpretability Workshop</p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡ç»“åˆå¤–éƒ¨ä¸Šä¸‹æ–‡å¢å¼ºç”Ÿæˆå“åº”çš„å‡†ç¡®æ€§å’Œå¯é æ€§ï¼Œå½¢æˆäº†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ã€‚ç„¶è€Œï¼Œç”±äºå½“å‰æ–¹æ³•çš„è®¡ç®—å¯†é›†å‹ç‰¹æ€§ï¼Œå°†ç”Ÿæˆå†…å®¹å¯é åœ°å½’å› äºç‰¹å®šçš„ä¸Šä¸‹æ–‡æ®µè½ï¼ˆå³ä¸Šä¸‹æ–‡å½’å› ï¼‰ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œé€šå¸¸éœ€è¦å¤§é‡çš„å¾®è°ƒæˆ–äººå·¥æ ‡æ³¨ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ç§åŸºäºJensen-Shannon Divergenceçš„æ–°å‹ä¸Šä¸‹æ–‡å“åº”å½’å› æ–¹æ³•ï¼ˆARC-JSDï¼‰ï¼Œæ— éœ€é¢å¤–çš„å¾®è°ƒã€æ¢¯åº¦è®¡ç®—æˆ–ä»£ç†å»ºæ¨¡ï¼Œå³å¯å®ç°é«˜æ•ˆå‡†ç¡®çš„ä¸Šä¸‹æ–‡å…³é”®å¥è¯†åˆ«ã€‚åœ¨TyDi QAã€Hotpot QAå’ŒMusiqueç­‰å¹¿æ³›çš„RAGåŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œä¸ä½¿ç”¨ä»£ç†æ¨¡å‹çš„æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•å…·æœ‰æ›´é«˜çš„å‡†ç¡®æ€§å’Œæ˜¾è‘—çš„è®¡ç®—æ•ˆç‡ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æœºåˆ¶åˆ†ææ­ç¤ºäº†è´Ÿè´£ä¸Šä¸‹æ–‡å½’å› çš„ç‰¹å®šæ³¨æ„åŠ›å¤´å’Œå¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰å±‚ï¼Œæ·±å…¥æ¢è®¨äº†RAGæ¨¡å‹çš„å†…éƒ¨å·¥ä½œåŸç†åŠå…¶å¯¹RAGè¡Œä¸ºçš„å½±å“ã€‚æˆ‘ä»¬çš„ä»£ç å¯é€šè¿‡[<a target="_blank" rel="noopener" href="https://github.com/ruizheliUOA/ARC_JSD%E8%AE%BF%E9%97%AE%E3%80%82]">https://github.com/ruizheliUOA/ARC_JSDè®¿é—®ã€‚]</a>(<a target="_blank" rel="noopener" href="https://github.com/ruizheliUOA/ARC_JSD%E8%AE%BF%E9">https://github.com/ruizheliUOA/ARC_JSD%E8%AE%BF%E9</a> è®¿é—®ã€‚)</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>RAGç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹å’Œå¤–éƒ¨ä¸Šä¸‹æ–‡ä»¥å¢å¼ºç”Ÿæˆå“åº”çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚</li>
<li>ä¸Šä¸‹æ–‡å½’å› æ˜¯å½“å‰æŒ‘æˆ˜ä¹‹ä¸€ï¼Œéœ€è¦æœ‰æ•ˆçš„æ–¹æ³•æ¥è¯†åˆ«ç”Ÿæˆå†…å®¹å¯¹åº”çš„ä¸Šä¸‹æ–‡æ®µè½ã€‚</li>
<li>å¼•å…¥ARC-JSDæ–¹æ³•ï¼ŒåŸºäºJensen-Shannon Divergenceå®ç°é«˜æ•ˆå‡†ç¡®çš„ä¸Šä¸‹æ–‡å…³é”®å¥è¯†åˆ«ã€‚</li>
<li>åœ¨å¤šä¸ªRAGåŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒARC-JSDæ–¹æ³•ç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•å…·æœ‰æ›´é«˜çš„å‡†ç¡®æ€§å’Œè®¡ç®—æ•ˆç‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16415">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-c06f090137c27934acf4872023bbacff~resize:0:q75.jpg?source=1f5c5e47&expiration=1760103356&auth_key=1760103356-0-0-428a6f5965a35b7bce06179c67466c28&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b2656b851d93b05849b1a0f5a81e846b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760103364&auth_key=1760103364-0-0-a0a9815203fb0a608fdc11cf181279f2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4cd442ac9faf4da80177fea90aaee81a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760103370&auth_key=1760103370-0-0-5eb52e10b5d6e3714421a5771699bf84&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ed6580140f0d85c06bf34417e14391d0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760103377&auth_key=1760103377-0-0-bd9e46e2f311aa7c2104e02cf0efabea&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e8985539403eeddfb30104b6d67a4c80~resize:0:q75.jpg?source=1f5c5e47&expiration=1760103384&auth_key=1760103384-0-0-1c5c500364739810f124b54f2b5f5818&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Towards-Holistic-Evaluation-of-Large-Audio-Language-Models-A-Comprehensive-Survey"><a href="#Towards-Holistic-Evaluation-of-Large-Audio-Language-Models-A-Comprehensive-Survey" class="headerlink" title="Towards Holistic Evaluation of Large Audio-Language Models: A   Comprehensive Survey"></a>Towards Holistic Evaluation of Large Audio-Language Models: A   Comprehensive Survey</h2><p><strong>Authors:Chih-Kai Yang, Neo S. Ho, Hung-yi Lee</strong></p>
<p>With advancements in large audio-language models (LALMs), which enhance large language models (LLMs) with auditory capabilities, these models are expected to demonstrate universal proficiency across various auditory tasks. While numerous benchmarks have emerged to assess LALMsâ€™ performance, they remain fragmented and lack a structured taxonomy. To bridge this gap, we conduct a comprehensive survey and propose a systematic taxonomy for LALM evaluations, categorizing them into four dimensions based on their objectives: (1) General Auditory Awareness and Processing, (2) Knowledge and Reasoning, (3) Dialogue-oriented Ability, and (4) Fairness, Safety, and Trustworthiness. We provide detailed overviews within each category and highlight challenges in this field, offering insights into promising future directions. To the best of our knowledge, this is the first survey specifically focused on the evaluations of LALMs, providing clear guidelines for the community. We will release the collection of the surveyed papers and actively maintain it to support ongoing advancements in the field. </p>
<blockquote>
<p>éšç€å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLALMï¼‰çš„è¿›æ­¥ï¼Œè¿™äº›æ¨¡å‹åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åŸºç¡€ä¸Šå¢å¼ºäº†å¬è§‰èƒ½åŠ›ï¼Œå¹¶æœ‰æœ›åœ¨å„ç§å¬è§‰ä»»åŠ¡ä¸­å±•ç°å‡ºå…¨é¢çš„èƒ½åŠ›ã€‚è™½ç„¶å‡ºç°äº†è®¸å¤šè¯„ä¼°LALMæ€§èƒ½çš„åŸºå‡†æµ‹è¯•ï¼Œä½†å®ƒä»¬ä»ç„¶åˆ†æ•£ä¸”ç¼ºä¹ç»“æ„åŒ–åˆ†ç±»ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬è¿›è¡Œäº†å…¨é¢çš„è°ƒæŸ¥ï¼Œå¹¶æå‡ºäº†LALMè¯„ä¼°çš„ç³»ç»Ÿæ€§åˆ†ç±»æ–¹æ³•ï¼Œæ ¹æ®è¯„ä¼°ç›®æ ‡å°†å…¶åˆ†ä¸ºå››ä¸ªç»´åº¦ï¼šï¼ˆ1ï¼‰ä¸€èˆ¬å¬è§‰æ„è¯†å’Œå¤„ç†ï¼Œï¼ˆ2ï¼‰çŸ¥è¯†å’Œæ¨ç†ï¼Œï¼ˆ3ï¼‰é¢å‘å¯¹è¯çš„èƒ½åŠ›ï¼Œï¼ˆ4ï¼‰å…¬å¹³æ€§ã€å®‰å…¨æ€§å’Œå¯ä¿¡åº¦ã€‚æˆ‘ä»¬åœ¨æ¯ä¸ªç±»åˆ«ä¸­æä¾›äº†è¯¦ç»†çš„æ¦‚è¿°ï¼Œå¹¶çªå‡ºäº†è¯¥é¢†åŸŸçš„æŒ‘æˆ˜ï¼Œä¸ºæœªæ¥çš„å‘å±•æ–¹å‘æä¾›äº†è§è§£ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€ç¯‡ä¸“é—¨é’ˆå¯¹LALMè¯„ä¼°çš„ç»¼è¿°ï¼Œä¸ºç¤¾åŒºæä¾›äº†æ¸…æ™°çš„æŒ‡å¯¼ã€‚æˆ‘ä»¬å°†å‘å¸ƒæ‰€è°ƒæŸ¥çš„è®ºæ–‡é›†åˆï¼Œå¹¶ç§¯æç»´æŠ¤ä»¥æ”¯æŒè¯¥é¢†åŸŸçš„æŒç»­å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15957v3">PDF</a> EMNLP 2025 (Main). Project Website:   <a target="_blank" rel="noopener" href="https://github.com/ckyang1124/LALM-Evaluation-Survey">https://github.com/ckyang1124/LALM-Evaluation-Survey</a></p>
<p><strong>Summary</strong><br>å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLALMï¼‰éšç€è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥è€Œå‘å±•ï¼Œå¹¶å±•ç°å‡ºè·¨å¤šç§å¬è§‰ä»»åŠ¡çš„é€šç”¨èƒ½åŠ›ã€‚å½“å‰å­˜åœ¨å¤šä¸ªè¯„ä¼°LALMæ€§èƒ½çš„åŸºå‡†æµ‹è¯•ï¼Œä½†å®ƒä»¬åˆ†æ•£ä¸”ç¼ºä¹ç»“æ„åŒ–åˆ†ç±»ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬è¿›è¡Œå…¨é¢è°ƒæŸ¥å¹¶æå‡ºç³»ç»Ÿçš„LALMè¯„ä¼°åˆ†ç±»æ³•ï¼Œæ ¹æ®ç›®æ ‡å°†å…¶åˆ†ä¸ºå››ä¸ªç»´åº¦ï¼š1ï¼‰ä¸€èˆ¬å¬è§‰æ„è¯†å’Œå¤„ç†ï¼›2ï¼‰çŸ¥è¯†å’Œæ¨ç†ï¼›3ï¼‰å¯¹è¯å¯¼å‘èƒ½åŠ›ï¼›ä»¥åŠ4ï¼‰å…¬å¹³æ€§ã€å®‰å…¨æ€§å’Œå¯ä¿¡åº¦ã€‚æœ¬æ–‡è¯¦ç»†æ¦‚è¿°äº†æ¯ä¸ªç±»åˆ«ï¼ŒæŒ‡å‡ºäº†è¯¥é¢†åŸŸçš„æŒ‘æˆ˜ï¼Œä¸ºç¤¾åŒºæä¾›äº†æ˜ç¡®çš„æŒ‡å¯¼æ–¹å‘ã€‚è¿™æ˜¯é¦–ä¸ªä¸“é—¨é’ˆå¯¹LALMè¯„ä¼°çš„ç»¼è¿°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLALMï¼‰å±•ç°å‡ºè·¨å¤šç§å¬è§‰ä»»åŠ¡çš„é€šç”¨èƒ½åŠ›ã€‚</li>
<li>å½“å‰LALMçš„è¯„ä¼°åŸºå‡†æµ‹è¯•å­˜åœ¨ä½†åˆ†æ•£ï¼Œç¼ºä¹ç»“æ„åŒ–åˆ†ç±»ã€‚</li>
<li>ä¸ºè§£å†³è¯„ä¼°é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªç³»ç»Ÿçš„LALMè¯„ä¼°åˆ†ç±»æ³•ï¼ŒåŒ…æ‹¬å››ä¸ªç»´åº¦ã€‚</li>
<li>è¿™å››ä¸ªç»´åº¦åˆ†åˆ«æ˜¯ï¼šä¸€èˆ¬å¬è§‰æ„è¯†å’Œå¤„ç†ã€çŸ¥è¯†å’Œæ¨ç†ã€å¯¹è¯å¯¼å‘èƒ½åŠ›ä»¥åŠå…¬å¹³æ€§ã€å®‰å…¨æ€§å’Œå¯ä¿¡åº¦ã€‚</li>
<li>æ–‡ç« æä¾›äº†æ¯ä¸ªç»´åº¦çš„è¯¦ç»†æ¦‚è¿°å’Œè¯¥é¢†åŸŸçš„æŒ‘æˆ˜ã€‚</li>
<li>æ­¤ç»¼è¿°ä¸ºç¤¾åŒºæä¾›äº†å…³äºLALMè¯„ä¼°çš„æ˜ç¡®æŒ‡å¯¼æ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15957">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-c4121a9f568665c439ec2c57768ee3d7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760103392&auth_key=1760103392-0-0-5ce04a8880afc523b6b7ce43c34f2eb8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-26d15967a601b11839a94cfdf7277720~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144476&auth_key=1760144476-0-0-79c86950cafe911a6491bf0f86abd006&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Scaling-Diffusion-Transformers-Efficiently-via-Î¼-P"><a href="#Scaling-Diffusion-Transformers-Efficiently-via-Î¼-P" class="headerlink" title="Scaling Diffusion Transformers Efficiently via $Î¼$P"></a>Scaling Diffusion Transformers Efficiently via $Î¼$P</h2><p><strong>Authors:Chenyu Zheng, Xinyu Zhang, Rongzhen Wang, Wei Huang, Zhi Tian, Weilin Huang, Jun Zhu, Chongxuan Li</strong></p>
<p>Diffusion Transformers have emerged as the foundation for vision generative models, but their scalability is limited by the high cost of hyperparameter (HP) tuning at large scales. Recently, Maximal Update Parametrization ($\mu$P) was proposed for vanilla Transformers, which enables stable HP transfer from small to large language models, and dramatically reduces tuning costs. However, it remains unclear whether $\mu$P of vanilla Transformers extends to diffusion Transformers, which differ architecturally and objectively. In this work, we generalize standard $\mu$P to diffusion Transformers and validate its effectiveness through large-scale experiments. First, we rigorously prove that $\mu$P of mainstream diffusion Transformers, including DiT, U-ViT, PixArt-$\alpha$, and MMDiT, aligns with that of the vanilla Transformer, enabling the direct application of existing $\mu$P methodologies. Leveraging this result, we systematically demonstrate that DiT-$\mu$P enjoys robust HP transferability. Notably, DiT-XL-2-$\mu$P with transferred learning rate achieves 2.9 times faster convergence than the original DiT-XL-2. Finally, we validate the effectiveness of $\mu$P on text-to-image generation by scaling PixArt-$\alpha$ from 0.04B to 0.61B and MMDiT from 0.18B to 18B. In both cases, models under $\mu$P outperform their respective baselines while requiring small tuning cost, only 5.5% of one training run for PixArt-$\alpha$ and 3% of consumption by human experts for MMDiT-18B. These results establish $\mu$P as a principled and efficient framework for scaling diffusion Transformers. </p>
<blockquote>
<p>æ‰©æ•£Transformerå·²æˆä¸ºè§†è§‰ç”Ÿæˆæ¨¡å‹çš„åŸºç¡€ï¼Œä½†å…¶å¯æ‰©å±•æ€§å—åˆ°å¤§è§„æ¨¡è¶…å‚æ•°ï¼ˆHPï¼‰è°ƒæ•´çš„é«˜æˆæœ¬çš„é™åˆ¶ã€‚æœ€è¿‘ï¼Œé’ˆå¯¹æ™®é€šTransformeræå‡ºäº†æœ€å¤§æ›´æ–°å‚æ•°åŒ–ï¼ˆ$\mu$Pï¼‰æ–¹æ³•ï¼Œèƒ½å¤Ÿå®ç°ä»å°å‹åˆ°å¤§å‹è¯­è¨€æ¨¡å‹çš„ç¨³å®šHPè¿ç§»ï¼Œå¹¶å¤§å¹…åº¦é™ä½è°ƒæ•´æˆæœ¬ã€‚ç„¶è€Œï¼Œå°šä¸æ¸…æ¥šæ™®é€šTransformerçš„$\mu$Pæ˜¯å¦é€‚ç”¨äºæ¶æ„å’Œå®¢è§‚ä¸Šå­˜åœ¨å·®å¼‚çš„æ‰©æ•£Transformerã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å°†æ ‡å‡†$\mu$Pæ¨å¹¿åˆ°æ‰©æ•£Transformerï¼Œå¹¶é€šè¿‡å¤§è§„æ¨¡å®éªŒéªŒè¯å…¶æœ‰æ•ˆæ€§ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä¸¥æ ¼è¯æ˜ä¸»æµæ‰©æ•£Transformerçš„$\mu$Pä¸æ™®é€šTransformerçš„$\mu$Pæ˜¯ä¸€è‡´çš„ï¼Œä½¿å¾—ç°æœ‰$\mu$Pæ–¹æ³•å¯ä»¥ç›´æ¥åº”ç”¨ã€‚åŸºäºæ­¤ç»“æœï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°è¯æ˜äº†DiT-$\mu$På…·æœ‰ç¨³å¥çš„HPè¿ç§»èƒ½åŠ›ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåˆ©ç”¨è¿ç§»å­¦ä¹ ç‡çš„DiT-XL-2-$\mu$På®ç°äº†æ¯”åŸå§‹DiT-XL-2å¿«2.9å€çš„æ”¶æ•›é€Ÿåº¦ã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡å°†PixArt-$\alpha$ä»0.04Bæ‰©å±•åˆ°0.61Bå’Œå°†MMDiTä»0.18Bæ‰©å±•åˆ°18Bï¼ŒéªŒè¯äº†$\mu$Påœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚åœ¨è¿™ä¸¤ç§æƒ…å†µä¸‹ï¼Œ$\mu$Pä¸‹çš„æ¨¡å‹éƒ½ä¼˜äºå„è‡ªçš„åŸºçº¿æ¨¡å‹ï¼ŒåŒæ—¶éœ€è¦è¾ƒå°çš„è°ƒæ•´æˆæœ¬ï¼ŒPixArt-$\alpha$åªéœ€5.5%çš„ä¸€æ¬¡è¿è¡Œè®­ç»ƒæˆæœ¬ï¼Œè€ŒMMDiT-18Båªéœ€3%çš„äººåŠ›ä¸“å®¶æŠ•å…¥ã€‚è¿™äº›ç»“æœè¯æ˜äº†$\mu$Pæ˜¯ä¸€ä¸ªæœ‰åŸåˆ™ã€é«˜æ•ˆç‡çš„æ¡†æ¶ï¼Œå¯ç”¨äºæ‰©å±•æ‰©æ•£Transformerã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15270v2">PDF</a> Accepted by NeurIPS 2025</p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬æ–‡ç ”ç©¶äº†æ‰©æ•£Transformerçš„å¯æ‰©å±•æ€§é—®é¢˜ï¼Œå‘ç°å…¶é«˜æˆæœ¬é™åˆ¶äº†å¤§è§„æ¨¡åº”ç”¨ã€‚ä¸ºæ­¤ï¼Œå¼•å…¥äº†æœ€å¤§æ›´æ–°å‚æ•°åŒ–ï¼ˆÎ¼Pï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿä»å°å‹è¯­è¨€æ¨¡å‹å‘å¤§å‹æ¨¡å‹ç¨³å®šåœ°è½¬ç§»è¶…å‚æ•°ï¼Œæ˜¾è‘—é™ä½äº†è°ƒæ•´æˆæœ¬ã€‚æœ¬æ–‡å°†Î¼Pæ–¹æ³•æ¨å¹¿åˆ°æ‰©æ•£Transformerä¸Šï¼Œå¹¶é€šè¿‡å¤§è§„æ¨¡å®éªŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚å®éªŒè¯æ˜ï¼Œä¸»æµæ‰©æ•£Transformerçš„Î¼Pä¸æ ‡å‡†Transformerç›¸ç¬¦ï¼Œå¯å®ç°ç°æœ‰Î¼Pæ–¹æ³•çš„ç›´æ¥åº”ç”¨ã€‚åˆ©ç”¨è¿™ä¸€ç»“æœï¼Œå‘ç°DiT-Î¼På…·æœ‰ç¨³å¥çš„è¶…å‚æ•°å¯è½¬ç§»æ€§ã€‚ç‰¹åˆ«æ˜¯åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­ï¼ŒPixArt-Î±ä»0.04Bæ‰©å±•åˆ°0.61Bï¼ŒMMDiTä»0.18Bæ‰©å±•åˆ°18Bæ—¶ï¼ŒÎ¼Pæ–¹æ³•æ•ˆæœæ˜¾è‘—ï¼Œä¸ä»…è¶…è¶Šäº†åŸºçº¿æ¨¡å‹ï¼Œè€Œä¸”è°ƒæ•´æˆæœ¬è¾ƒä½ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ‰©æ•£Transformerä½œä¸ºè§†è§‰ç”Ÿæˆæ¨¡å‹çš„åŸºç¡€é¢ä¸´é«˜æˆæœ¬è¶…å‚æ•°è°ƒæ•´çš„æŒ‘æˆ˜ã€‚</li>
<li>æœ€å¤§æ›´æ–°å‚æ•°åŒ–ï¼ˆÎ¼Pï¼‰æ–¹æ³•ç”¨äºç¨³å®šåœ°ä»å°å‹è¯­è¨€æ¨¡å‹å‘å¤§å‹æ¨¡å‹è½¬ç§»è¶…å‚æ•°ã€‚</li>
<li>Î¼Pæ–¹æ³•è¢«æ¨å¹¿åˆ°æ‰©æ•£Transformerä¸Šï¼Œå¹¶é€šè¿‡å¤§è§„æ¨¡å®éªŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºä¸»æµæ‰©æ•£Transformerçš„Î¼Pä¸æ ‡å‡†Transformerç›¸ç¬¦ã€‚</li>
<li>DiT-Î¼På±•ç°å‡ºç¨³å¥çš„è¶…å‚æ•°å¯è½¬ç§»æ€§ã€‚</li>
<li>åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­ï¼ŒPixArt-Î±å’ŒMMDiTåœ¨é‡‡ç”¨Î¼Pæ–¹æ³•åæ€§èƒ½æ˜¾è‘—æå‡ï¼ŒåŒæ—¶è°ƒæ•´æˆæœ¬è¾ƒä½ã€‚</li>
<li>Î¼Pæ–¹æ³•æˆä¸ºä¸€ä¸ªæœ‰åŸåˆ™ã€é«˜æ•ˆç‡çš„æ¡†æ¶ï¼Œç”¨äºæ‰©å±•æ‰©æ•£Transformerã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15270">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-cd0c8b1ac2d522175a1cd4a2fce0e11a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144483&auth_key=1760144483-0-0-ed2bf544f00b0ddc7fa348e6a3d4c9cd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ae327bed95e08e1472c8ade960b258a8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144490&auth_key=1760144490-0-0-9536be27b35cf258be553ab2c321d973&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e3de40487b9e9cd280513563e3132038~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144497&auth_key=1760144497-0-0-b6851814f6a5ba40e61193ac6114afb6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2a4d6a4833d7ac657b5b9ac8d989f6cb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144503&auth_key=1760144503-0-0-97ed9e5205cf50cd76f6bd9cacfd239e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="GuRE-Generative-Query-REwriter-for-Legal-Passage-Retrieval"><a href="#GuRE-Generative-Query-REwriter-for-Legal-Passage-Retrieval" class="headerlink" title="GuRE:Generative Query REwriter for Legal Passage Retrieval"></a>GuRE:Generative Query REwriter for Legal Passage Retrieval</h2><p><strong>Authors:Daehee Kim, Deokhyung Kang, Jonghwi Kim, Sangwon Ryu, Gary Geunbae Lee</strong></p>
<p>Legal Passage Retrieval (LPR) systems are crucial as they help practitioners save time when drafting legal arguments. However, it remains an underexplored avenue. One primary reason is the significant vocabulary mismatch between the query and the target passage. To address this, we propose a simple yet effective method, the Generative query REwriter (GuRE). We leverage the generative capabilities of Large Language Models (LLMs) by training the LLM for query rewriting. â€œRewritten queriesâ€ help retrievers to retrieve target passages by mitigating vocabulary mismatch. Experimental results show that GuRE significantly improves performance in a retriever-agnostic manner, outperforming all baseline methods. Further analysis reveals that different training objectives lead to distinct retrieval behaviors, making GuRE more suitable than direct retriever fine-tuning for real-world applications. Codes are avaiable at github.com&#x2F;daehuikim&#x2F;GuRE. </p>
<blockquote>
<p>æ³•å¾‹æ¡æ–‡æ£€ç´¢ï¼ˆLPRï¼‰ç³»ç»Ÿåœ¨å¸®åŠ©æ³•å¾‹å·¥ä½œè€…èµ·è‰æ³•å¾‹è®ºç‚¹æ—¶èŠ‚çœæ—¶é—´æ–¹é¢èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ç„¶è€Œï¼Œå®ƒä»ç„¶æ˜¯ä¸€ä¸ªå°šæœªè¢«å……åˆ†æ¢ç´¢çš„é¢†åŸŸã€‚ä¸»è¦åŸå› åœ¨äºæŸ¥è¯¢å’Œç›®æ ‡æ®µè½ä¹‹é—´å­˜åœ¨é‡å¤§çš„è¯æ±‡ä¸åŒ¹é…ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ–¹æ³•â€”â€”ç”Ÿæˆå¼æŸ¥è¯¢é‡å†™å™¨ï¼ˆGuREï¼‰ã€‚æˆ‘ä»¬é€šè¿‡è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡ŒæŸ¥è¯¢é‡å†™ï¼Œåˆ©ç”¨LLMçš„ç”Ÿæˆèƒ½åŠ›ã€‚â€œé‡å†™çš„æŸ¥è¯¢â€æœ‰åŠ©äºæ£€ç´¢å™¨é€šè¿‡å‡è½»è¯æ±‡ä¸åŒ¹é…æ¥æ£€ç´¢ç›®æ ‡æ®µè½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGuREä»¥ä¸€ç§æ£€ç´¢å™¨æ— å…³çš„æ–¹å¼æ˜¾è‘—æé«˜äº†æ€§èƒ½ï¼Œä¼˜äºæ‰€æœ‰åŸºçº¿æ–¹æ³•ã€‚è¿›ä¸€æ­¥çš„åˆ†æè¡¨æ˜ï¼Œä¸åŒçš„è®­ç»ƒç›®æ ‡ä¼šå¯¼è‡´ä¸åŒçš„æ£€ç´¢è¡Œä¸ºï¼Œè¿™ä½¿å¾—GuREæ¯”ç›´æ¥å¯¹æ£€ç´¢å™¨è¿›è¡Œå¾®è°ƒæ›´é€‚åˆäºå®é™…åº”ç”¨ã€‚ä»£ç å¯åœ¨github.com&#x2F;daehuikim&#x2F;GuREæ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12950v2">PDF</a> NLLP Workshop at EMNLP 2025</p>
<p><strong>Summary</strong>ï¼šæ³•å¾‹æ–‡çŒ®æ£€ç´¢ï¼ˆLPRï¼‰ç³»ç»Ÿå¯¹äºå¸®åŠ©å¾‹å¸ˆèŠ‚çœæ—¶é—´è‡³å…³é‡è¦ï¼Œä½†å…¶åº”ç”¨å°šå¾…æ¢ç´¢ã€‚ä¸»è¦æŒ‘æˆ˜åœ¨äºæŸ¥è¯¢å’Œç›®æ ‡æ®µè½ä¹‹é—´å­˜åœ¨æ˜¾è‘—çš„è¯æ±‡ä¸åŒ¹é…é—®é¢˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ–¹æ³•â€”â€”ç”Ÿæˆå¼æŸ¥è¯¢é‡å†™å™¨ï¼ˆGuREï¼‰ã€‚æˆ‘ä»¬åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç”Ÿæˆèƒ½åŠ›ï¼Œè®­ç»ƒå…¶è¿›è¡ŒæŸ¥è¯¢é‡å†™ã€‚é‡å†™åçš„æŸ¥è¯¢æœ‰åŠ©äºæ£€ç´¢å™¨æ£€ç´¢ç›®æ ‡æ®µè½ï¼Œå¹¶å‡è½»è¯æ±‡ä¸åŒ¹é…çš„é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGuREä»¥æ£€ç´¢å™¨æ— å…³çš„æ–¹å¼æ˜¾è‘—æé«˜äº†æ€§èƒ½ï¼Œä¼˜äºæ‰€æœ‰åŸºçº¿æ–¹æ³•ã€‚è¿›ä¸€æ­¥çš„åˆ†æè¡¨æ˜ï¼Œä¸åŒçš„è®­ç»ƒç›®æ ‡ä¼šå¯¼è‡´ä¸åŒçš„æ£€ç´¢è¡Œä¸ºï¼Œè¿™ä½¿å¾—GuREæ¯”ç›´æ¥å¾®è°ƒæ£€ç´¢å™¨æ›´é€‚åˆå®é™…åº”ç”¨ã€‚ç›¸å…³ä»£ç å·²å‘å¸ƒåœ¨github.com&#x2F;daehuikim&#x2F;GuREä¸Šã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>LPRç³»ç»Ÿå¯¹äºæ³•å¾‹ä»ä¸šè€…åœ¨å‡†å¤‡æ³•å¾‹è®ºè¯æ—¶èŠ‚çœæ—¶é—´è‡³å…³é‡è¦ã€‚</li>
<li>æŸ¥è¯¢ä¸ç›®æ ‡æ®µè½é—´çš„è¯æ±‡ä¸åŒ¹é…æ˜¯LPRç³»ç»Ÿé¢ä¸´çš„æŒ‘æˆ˜ä¹‹ä¸€ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºGuREçš„ç”Ÿæˆå¼æŸ¥è¯¢é‡å†™æ–¹æ³•ï¼Œåˆ©ç”¨LLMè¿›è¡Œè®­ç»ƒä»¥æ”¹å–„æŸ¥è¯¢é‡å†™é—®é¢˜ã€‚</li>
<li>GuREé€šè¿‡é‡å†™æŸ¥è¯¢å¸®åŠ©æ£€ç´¢å™¨æ›´æœ‰æ•ˆåœ°æ‰¾åˆ°ç›®æ ‡æ®µè½ï¼Œå¹¶å‡è½»è¯æ±‡ä¸åŒ¹é…çš„é—®é¢˜ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºGuREåœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œå¹¶ä»¥æ£€ç´¢å™¨æ— å…³çš„æ–¹å¼å®ç°æ”¹è¿›ã€‚</li>
<li>ä¸åŒè®­ç»ƒç›®æ ‡å¯¼è‡´ä¸åŒçš„æ£€ç´¢è¡Œä¸ºï¼Œè¯´æ˜GuREçš„è®¾è®¡æ›´å…·é€‚åº”æ€§å’Œçµæ´»æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12950">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-341dac342f3849330a4f863487254264~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144511&auth_key=1760144511-0-0-54f5005e6eacc99afa19f48643690d00&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2430ec3227d33b65bfabd8d87eb6d9bd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144518&auth_key=1760144518-0-0-4d9387350929fa546d90eab68fa28f95&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-241ca9aae96765f96bfb339250cbf77e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144525&auth_key=1760144525-0-0-8c48781d31830c98e2c9c18796acb1d1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5cd78af8e4dec1c554f1a2a6dca73162~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144531&auth_key=1760144531-0-0-ee9205de94d8c5d752fc7dc24b88ceb6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3d52d5228ee62a219a9444fa4da71d49~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144538&auth_key=1760144538-0-0-e8be34d849a2787e0462cf0bc5afcc53&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a465a2f27445ab2a048f8c078aeffd01~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144544&auth_key=1760144544-0-0-5698deef4856d62b5939ae1b9d4904d2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Ambiguity-in-LLMs-is-a-concept-missing-problem"><a href="#Ambiguity-in-LLMs-is-a-concept-missing-problem" class="headerlink" title="Ambiguity in LLMs is a concept missing problem"></a>Ambiguity in LLMs is a concept missing problem</h2><p><strong>Authors:Zhibo Hu, Chen Wang, Yanfeng Shu, Hye-Young Paik, Liming Zhu</strong></p>
<p>Ambiguity in natural language is a significant obstacle for achieving accurate text to structured data mapping through large language models (LLMs), which affects the performance of tasks such as mapping text to agentic tool calling and text-to-SQL queries. Existing methods to ambiguity handling either rely on the ReACT framework to obtain correct mappings through trial and error, or on supervised fine-tuning to bias models toward specific tasks. In this paper, we adopt a different approach that characterizes representation differences of ambiguous text in the latent space and leverages these differences to identify ambiguity before mapping them to structured data. To detect sentence-level ambiguity, we focus on the relationship between ambiguous questions and their interpretations. Unlike distances calculated by dense embeddings, we introduce a new distance measure based on a path kernel over concepts. With this measurement, we identify patterns to distinguish ambiguous from unambiguous questions. Furthermore, we propose a method for improving LLM performance on ambiguous agentic tool calling through missing concept prediction. Both achieve state-of-the-art results. </p>
<blockquote>
<p>è‡ªç„¶è¯­è¨€ä¸­çš„æ­§ä¹‰æ˜¯åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å®ç°ä»æ–‡æœ¬åˆ°ç»“æ„åŒ–æ•°æ®æ˜ å°„çš„æ˜¾è‘—éšœç¢ï¼Œè¿™å½±å“äº†æ–‡æœ¬æ˜ å°„åˆ°æ™ºèƒ½å·¥å…·è°ƒç”¨å’Œæ–‡æœ¬åˆ°SQLæŸ¥è¯¢ç­‰ä»»åŠ¡çš„æ€§èƒ½ã€‚ç°æœ‰çš„å¤„ç†æ­§ä¹‰çš„æ–¹æ³•è¦ä¹ˆä¾èµ–äºReACTæ¡†æ¶é€šè¿‡åå¤è¯•éªŒè·å¾—æ­£ç¡®çš„æ˜ å°„ï¼Œè¦ä¹ˆä¾èµ–äºç›‘ç£å¾®è°ƒä½¿æ¨¡å‹åå‘äºç‰¹å®šä»»åŠ¡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§ä¸åŒçš„æ–¹æ³•ï¼Œå³åœ¨æ½œåœ¨ç©ºé—´ä¸­åˆ»ç”»æ¨¡ç³Šæ–‡æœ¬è¡¨ç¤ºçš„å·®å¼‚ï¼Œå¹¶åˆ©ç”¨è¿™äº›å·®å¼‚åœ¨æ˜ å°„åˆ°ç»“æ„åŒ–æ•°æ®ä¹‹å‰è¯†åˆ«æ­§ä¹‰ã€‚ä¸ºäº†æ£€æµ‹å¥å­çº§åˆ«çš„æ­§ä¹‰ï¼Œæˆ‘ä»¬å…³æ³¨æ¨¡ç³Šé—®é¢˜åŠå…¶è§£é‡Šä¹‹é—´çš„å…³ç³»ã€‚ä¸åŸºäºå¯†é›†åµŒå…¥è®¡ç®—çš„é—´è·ä¸åŒï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºæ¦‚å¿µè·¯å¾„æ ¸çš„æ–°è·ç¦»åº¦é‡ã€‚é€šè¿‡è¿™ä¸€åº¦é‡ï¼Œæˆ‘ä»¬è¯†åˆ«å‡ºåŒºåˆ†æ¨¡ç³Šé—®é¢˜å’Œéæ¨¡ç³Šé—®é¢˜çš„æ¨¡å¼ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šè¿‡é¢„æµ‹ç¼ºå¤±æ¦‚å¿µæ¥æé«˜LLMåœ¨å¤„ç†æ¨¡ç³Šæ™ºèƒ½å·¥å…·è°ƒç”¨æ–¹é¢çš„æ€§èƒ½çš„æ–¹æ³•ã€‚ä¸¤è€…éƒ½è¾¾åˆ°äº†æœ€æ–°çš„ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11679v3">PDF</a> 17 pages, 11 figures, title updated</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åœ¨è‡ªç„¶è¯­è¨€ä¸­çš„æ­§ä¹‰å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å®ç°æ–‡æœ¬åˆ°ç»“æ„åŒ–æ•°æ®æ˜ å°„çš„å‡†ç¡®æ€§çš„æŒ‘æˆ˜ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡è¯†åˆ«æ¨¡ç³Šæ–‡æœ¬åœ¨æ½œåœ¨ç©ºé—´ä¸­çš„è¡¨ç¤ºå·®å¼‚æ¥è¯†åˆ«æ­§ä¹‰ï¼Œå¹¶å°†å…¶æ˜ å°„åˆ°ç»“æ„åŒ–æ•°æ®ä¸Šã€‚æ–°æ–¹æ³•ä¾§é‡äºåˆ©ç”¨æ¦‚å¿µé—´çš„è·¯å¾„æ ¸è·ç¦»æ¥è¡¡é‡å¥å­çº§åˆ«çš„æ­§ä¹‰é—®é¢˜ï¼Œå¹¶æå‡ºäº†é€šè¿‡é¢„æµ‹ç¼ºå¤±æ¦‚å¿µæ¥æ”¹è¿›æ¨¡ç³Šå·¥å…·è°ƒç”¨LLMæ€§èƒ½çš„æ–¹æ³•ã€‚ä¸¤è€…å‡è¾¾åˆ°äº†ä¸šç•Œæœ€ä½³æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è‡ªç„¶è¯­è¨€ä¸­çš„æ­§ä¹‰æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ–‡æœ¬åˆ°ç»“æ„åŒ–æ•°æ®æ˜ å°„çš„éšœç¢ä¹‹ä¸€ï¼Œå½±å“äº†ä»»åŠ¡æ€§èƒ½ï¼Œå¦‚ä»£ç†å·¥å…·è°ƒç”¨å’Œæ–‡æœ¬åˆ°SQLæŸ¥è¯¢ç­‰ã€‚</li>
<li>æ–‡ç« æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•æ¥å¤„ç†æ¨¡ç³Šæ–‡æœ¬æ˜ å°„é—®é¢˜ï¼Œè¿™ç§æ–¹æ³•èƒ½å¤Ÿè¯†åˆ«å‡ºæ–‡æœ¬åœ¨æ½œåœ¨ç©ºé—´ä¸­çš„è¡¨ç¤ºå·®å¼‚æ¥è¯†åˆ«æ­§ä¹‰ã€‚</li>
<li>é‡‡ç”¨æ¦‚å¿µè·¯å¾„æ ¸è·ç¦»çš„æ–°æµ‹é‡æ–¹æ³•ç”¨äºè¯†åˆ«å¥å­çº§åˆ«çš„æ­§ä¹‰é—®é¢˜ï¼Œè¯¥æµ‹é‡æ–¹æ³•å…‹æœäº†ä¼ ç»Ÿçš„åŸºäºå¯†é›†åµŒå…¥çš„è·ç¦»è®¡ç®—çš„ä¸è¶³ã€‚</li>
<li>æ–‡ç« æå‡ºäº†ä¸€ç§æ”¹è¿›å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¨¡ç³Šä»£ç†å·¥å…·è°ƒç”¨æ–¹é¢çš„æ€§èƒ½çš„æ–¹æ³•ï¼Œå³é€šè¿‡é¢„æµ‹ç¼ºå¤±æ¦‚å¿µæ¥å®ç°ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11679">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-fc0f54c683c87aba4bab8fe8d1bb0463~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144552&auth_key=1760144552-0-0-57b1ed03fe84f689f50da075c9546214&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-db2f423b39ca9a2b5456db3138394684~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144558&auth_key=1760144558-0-0-28363c2a2496fa9a0dfafea92bab9408&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6e19e313ae441cc1d9348e2291a95f07~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144565&auth_key=1760144565-0-0-70da94ff32bf4d0f233b6e300e5d827d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-03/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-03/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-03/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-10ddfae4d76a7f98708a0cce9cae8d66~resize:0:q75.jpg?source=1f5c5e47&expiration=1760100696&auth_key=1760100696-0-0-55461025763bca8d116bb1f92045260d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-03  Efficient and Transferable Agentic Knowledge Graph RAG via Reinforcement   Learning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-03
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-03/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-9f0579503e0cfac090ea89352c860b86~resize:0:q75.jpg?source=1f5c5e47&expiration=1760083979&auth_key=1760083979-0-0-00214d1f676c75766e75a4e454e4286d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-03  Probing the Critical Point (CritPt) of AI Reasoning a Frontier Physics   Research Benchmark
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-03
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">31086.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
