<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-03  A Multimodal LLM Approach for Visual Question Answering on   Multiparametric 3D Brain MRI">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2509.22444v2/page_1_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-03
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-09
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    88 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-10-03-æ›´æ–°"><a href="#2025-10-03-æ›´æ–°" class="headerlink" title="2025-10-03 æ›´æ–°"></a>2025-10-03 æ›´æ–°</h1><h2 id="A-Multimodal-LLM-Approach-for-Visual-Question-Answering-on-Multiparametric-3D-Brain-MRI"><a href="#A-Multimodal-LLM-Approach-for-Visual-Question-Answering-on-Multiparametric-3D-Brain-MRI" class="headerlink" title="A Multimodal LLM Approach for Visual Question Answering on   Multiparametric 3D Brain MRI"></a>A Multimodal LLM Approach for Visual Question Answering on   Multiparametric 3D Brain MRI</h2><p><strong>Authors:Arvind Murari Vepa, Yannan Yu, Jingru Gan, Anthony Cuturrufo, Weikai Li, Wei Wang, Fabien Scalzo, Yizhou Sun</strong></p>
<p>We introduce mpLLM, a prompt-conditioned hierarchical mixture-of-experts (MoE) architecture for visual question answering over multi-parametric 3D brain MRI (mpMRI). mpLLM routes across modality-level and token-level projection experts to fuse multiple interrelated 3D modalities, enabling efficient training without image-report pretraining. To address limited image-text paired supervision, mpLLM integrates a synthetic visual question answering (VQA) protocol that generates medically relevant VQA from segmentation annotations, and we collaborate with medical experts for clinical validation. mpLLM outperforms strong medical VLM baselines by 5.3% on average across multiple mpMRI datasets. Our study features three main contributions: (1) the first clinically validated VQA dataset for 3D brain mpMRI, (2) a novel multimodal LLM that handles multiple interrelated 3D modalities, and (3) strong empirical results that demonstrate the medical utility of our methodology. Ablations highlight the importance of modality-level and token-level experts and prompt-conditioned routing. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†mpLLMï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹å¤šå‚æ•°3Dè„‘MRIï¼ˆmpMRIï¼‰çš„è§†è§‰é—®ç­”çš„æç¤ºæ¡ä»¶åˆ†å±‚ä¸“å®¶æ··åˆï¼ˆMoEï¼‰æ¶æ„ã€‚mpLLMåœ¨æ¨¡æ€çº§åˆ«å’Œä»¤ç‰Œçº§åˆ«æŠ•å½±ä¸“å®¶ä¹‹é—´è¿›è¡Œè·¯ç”±ï¼Œä»¥èåˆå¤šä¸ªç›¸å…³çš„3Dæ¨¡æ€ï¼Œå®ç°æœ‰æ•ˆçš„è®­ç»ƒï¼Œæ— éœ€å›¾åƒæŠ¥å‘Šé¢„å¤„ç†ã€‚ä¸ºäº†è§£å†³æœ‰é™çš„å›¾åƒæ–‡æœ¬é…å¯¹ç›‘ç£é—®é¢˜ï¼ŒmpLLMé›†æˆäº†ä¸€ä¸ªåˆæˆè§†è§‰é—®ç­”ï¼ˆVQAï¼‰åè®®ï¼Œè¯¥åè®®ä»åˆ†å‰²æ³¨é‡Šç”ŸæˆåŒ»å­¦ç›¸å…³çš„VQAï¼Œå¹¶ä¸åŒ»å­¦ä¸“å®¶è¿›è¡Œä¸´åºŠéªŒè¯ã€‚mpLLMåœ¨å¤šä¸ªmpMRIæ•°æ®é›†ä¸Šçš„å¹³å‡è¡¨ç°ä¼˜äºå¼ºå¤§çš„åŒ»å­¦VLMåŸºå‡†æµ‹è¯•5.3%ã€‚æˆ‘ä»¬çš„ç ”ç©¶æœ‰ä¸‰ä¸ªä¸»è¦è´¡çŒ®ï¼šï¼ˆ1ï¼‰é¦–ä¸ªç»è¿‡ä¸´åºŠéªŒè¯çš„3Dè„‘mpMRIçš„VQAæ•°æ®é›†ï¼Œï¼ˆ2ï¼‰ä¸€ç§å¤„ç†å¤šä¸ªç›¸å…³3Dæ¨¡æ€çš„æ–°å‹å¤šæ¨¡æ€LLMï¼Œä»¥åŠï¼ˆ3ï¼‰å¼ºå¤§çš„å®è¯ç»“æœï¼Œè¯æ˜äº†æˆ‘ä»¬æ–¹æ³•è®ºçš„åŒ»å­¦å®ç”¨æ€§ã€‚æ¶ˆèç ”ç©¶çªå‡ºäº†æ¨¡æ€çº§åˆ«å’Œä»¤ç‰Œçº§åˆ«ä¸“å®¶ä»¥åŠæç¤ºæ¡ä»¶è·¯ç”±çš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25889v2">PDF</a> 23 pages, 3 figures</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä»‹ç»äº†mpLLMï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºå¤šå‚æ•°ä¸‰ç»´è„‘MRIï¼ˆmpMRIï¼‰çš„è§†è§‰é—®ç­”çš„æç¤ºæ¡ä»¶åˆ†å±‚æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¶æ„ã€‚mpLLMé€šè¿‡è·¨æ¨¡æ€å’Œä»¤ç‰Œçº§åˆ«çš„æŠ•å½±ä¸“å®¶è·¯ç”±ï¼Œèåˆå¤šä¸ªç›¸å…³çš„ä¸‰ç»´æ¨¡æ€ï¼Œèƒ½å¤Ÿåœ¨æ²¡æœ‰å›¾åƒæŠ¥å‘Šé¢„è®­ç»ƒçš„æƒ…å†µä¸‹è¿›è¡Œé«˜æ•ˆè®­ç»ƒã€‚é’ˆå¯¹æœ‰é™çš„å›¾åƒæ–‡æœ¬é…å¯¹ç›‘ç£é—®é¢˜ï¼ŒmpLLMé›†æˆäº†ä¸€ç§åˆæˆè§†è§‰é—®ç­”ï¼ˆVQAï¼‰åè®®ï¼Œè¯¥åè®®å¯ä»åˆ†å‰²æ³¨é‡Šç”ŸæˆåŒ»å­¦ç›¸å…³çš„VQAï¼Œå¹¶ä¸åŒ»å­¦ä¸“å®¶è¿›è¡Œä¸´åºŠéªŒè¯ã€‚mpLLMåœ¨å¤šä¸ªmpMRIæ•°æ®é›†ä¸Šçš„å¹³å‡è¡¨ç°ä¼˜äºå¼ºå¤§çš„åŒ»å­¦VLMåŸºçº¿æ¨¡å‹ï¼Œè¾¾åˆ°5.3%ã€‚æœ¬ç ”ç©¶çš„ä¸»è¦è´¡çŒ®åŒ…æ‹¬ï¼šï¼ˆ1ï¼‰é¦–ä¸ªç»è¿‡ä¸´åºŠéªŒè¯çš„ç”¨äºä¸‰ç»´è„‘mpMRIçš„è§†è§‰é—®ç­”æ•°æ®é›†ï¼Œï¼ˆ2ï¼‰ä¸€ç§å¤„ç†å¤šä¸ªç›¸å…³ä¸‰ç»´æ¨¡æ€çš„æ–°å‹å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œï¼ˆ3ï¼‰å¼ºæœ‰åŠ›çš„å®è¯ç»“æœè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•å…·æœ‰åŒ»å­¦åº”ç”¨ä»·å€¼ã€‚æ¶ˆèå®éªŒçªå‡ºäº†æ¨¡æ€çº§åˆ«å’Œä»¤ç‰Œçº§åˆ«ä¸“å®¶ä»¥åŠæç¤ºæ¡ä»¶è·¯ç”±çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>mpLLMæ˜¯ä¸€ä¸ªç”¨äºå¤šå‚æ•°ä¸‰ç»´è„‘MRIçš„è§†è§‰é—®ç­”çš„åˆ†å±‚æ··åˆä¸“å®¶æ¶æ„ã€‚</li>
<li>å®ƒé€šè¿‡è·¨æ¨¡æ€å’Œä»¤ç‰Œçº§åˆ«çš„æŠ•å½±ä¸“å®¶è·¯ç”±èåˆå¤šä¸ªç›¸å…³ä¸‰ç»´æ¨¡æ€ã€‚</li>
<li>mpLLMåœ¨æ²¡æœ‰å›¾åƒæŠ¥å‘Šé¢„è®­ç»ƒçš„æƒ…å†µä¸‹è¿›è¡Œé«˜æ•ˆè®­ç»ƒã€‚</li>
<li>é›†æˆåˆæˆè§†è§‰é—®ç­”ï¼ˆVQAï¼‰åè®®ä»¥ç”ŸæˆåŒ»å­¦ç›¸å…³çš„VQAå¹¶è¿›è¡Œä¸´åºŠéªŒè¯ã€‚</li>
<li>mpLLMåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰åŒ»å­¦VLMæ¨¡å‹ã€‚</li>
<li>ç ”ç©¶çš„ä¸»è¦è´¡çŒ®åŒ…æ‹¬ä¸´åºŠéªŒè¯çš„VQAæ•°æ®é›†ã€å¤„ç†å¤šæ¨¡æ€çš„å¤§å‹è¯­è¨€æ¨¡å‹å’Œå®è¯ç»“æœè¯æ˜å…¶åŒ»å­¦ä»·å€¼ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25889">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2509.25889v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2509.25889v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2509.25889v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2509.25889v2/page_4_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Dolphin-v1-0-Technical-Report"><a href="#Dolphin-v1-0-Technical-Report" class="headerlink" title="Dolphin v1.0 Technical Report"></a>Dolphin v1.0 Technical Report</h2><p><strong>Authors:Taohan Weng, Chi zhang, Chaoran Yan, Siya Liu, Xiaoyang Liu, Yalun Wu, Boyang Wang, Boyan Wang, Jiren Ren, Kaiwen Yan, Jinze Yu, Kaibing Hu, Henan Liu, Haoyun Zheng, Zhenyu Liu, Duo Zhang, Xiaoqing Guo, Anjie Le, Hongcheng Guo</strong></p>
<p>Ultrasound is crucial in modern medicine but faces challenges like operator dependence, image noise, and real-time scanning, hindering AI integration. While large multimodal models excel in other medical imaging areas, they struggle with ultrasoundâ€™s complexities. To address this, we introduce Dolphin v1.0 (V1) and its reasoning-augmented version, Dolphin R1-the first large-scale multimodal ultrasound foundation models unifying diverse clinical tasks in a single vision-language framework.To tackle ultrasound variability and noise, we curated a 2-million-scale multimodal dataset, combining textbook knowledge, public data, synthetic samples, and general corpora. This ensures robust perception, generalization, and clinical adaptability.The Dolphin series employs a three-stage training strategy: domain-specialized pretraining, instruction-driven alignment, and reinforcement-based refinement. Dolphin v1.0 delivers reliable performance in classification, detection, regression, and report generation. Dolphin R1 enhances diagnostic inference, reasoning transparency, and interpretability through reinforcement learning with ultrasound-specific rewards.Evaluated on U2-Bench across eight ultrasound tasks, Dolphin R1 achieves a U2-score of 0.5835-over twice the second-best model (0.2968) setting a new state of the art. Dolphin v1.0 also performs competitively, validating the unified framework. Comparisons show reasoning-enhanced training significantly improves diagnostic accuracy, consistency, and interpretability, highlighting its importance for high-stakes medical AI. </p>
<blockquote>
<p>è¶…å£°åœ¨ç°ä»£åŒ»å­¦ä¸­è‡³å…³é‡è¦ï¼Œä½†é¢ä¸´ç€æ“ä½œè€…ä¾èµ–ã€å›¾åƒå™ªå£°å’Œå®æ—¶æ‰«æç­‰æŒ‘æˆ˜ï¼Œé˜»ç¢äº†äººå·¥æ™ºèƒ½çš„æ•´åˆã€‚è™½ç„¶å¤§å‹å¤šæ¨¡å¼æ¨¡å‹åœ¨å…¶ä»–åŒ»å­¦æˆåƒé¢†åŸŸè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨åº”å¯¹è¶…å£°çš„å¤æ‚æ€§æ–¹é¢å´é‡åˆ°äº†å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Dolphin v1.0ï¼ˆV1ï¼‰åŠå…¶å¢å¼ºæ¨ç†ç‰ˆDolphin R1â€”â€”é¦–ä¸ªå¤§è§„æ¨¡è¶…å£°å¤šæ¨¡å¼åŸºç¡€æ¨¡å‹ï¼Œåœ¨ä¸€ä¸ªè§†è§‰è¯­è¨€æ¡†æ¶ä¸‹ç»Ÿä¸€äº†å¤šç§ä¸´åºŠä»»åŠ¡ã€‚ä¸ºäº†åº”å¯¹è¶…å£°çš„å˜æ€§å’Œå™ªå£°é—®é¢˜ï¼Œæˆ‘ä»¬ç­›é€‰äº†ä¸€ä¸ª200ä¸‡è§„æ¨¡çš„å¤šæ¨¡å¼æ•°æ®é›†ï¼Œç»“åˆäº†æ•™ç§‘ä¹¦çŸ¥è¯†ã€å…¬å¼€æ•°æ®ã€åˆæˆæ ·æœ¬å’Œä¸€èˆ¬è¯­æ–™åº“ã€‚è¿™ç¡®ä¿äº†ç¨³å¥çš„æ„ŸçŸ¥ã€é€šç”¨æ€§å’Œä¸´åºŠé€‚åº”æ€§ã€‚Dolphinç³»åˆ—é‡‡ç”¨ä¸‰é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼šé¢†åŸŸä¸“ä¸šåŒ–é¢„è®­ç»ƒã€æŒ‡ä»¤é©±åŠ¨å¯¹é½å’ŒåŸºäºå¼ºåŒ–çš„ç²¾ç‚¼ã€‚Dolphin v1.0åœ¨åˆ†ç±»ã€æ£€æµ‹ã€å›å½’å’ŒæŠ¥å‘Šç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºå¯é çš„æ€§èƒ½ã€‚Dolphin R1é€šè¿‡å¼ºåŒ–å­¦ä¹ ä¸è¶…å£°ç‰¹å®šå¥–åŠ±ç›¸ç»“åˆï¼Œå¢å¼ºäº†è¯Šæ–­æ¨ç†ã€æ¨ç†é€æ˜åº¦å’Œå¯è§£é‡Šæ€§ã€‚åœ¨U2-Benchä¸Šè¯„ä¼°çš„å…«ä¸ªè¶…å£°ä»»åŠ¡ä¸­ï¼ŒDolphin R1å–å¾—äº†U2åˆ†æ•°0.5835ï¼Œæ˜¯ç¬¬äºŒåæœ€ä½³æ¨¡å‹ï¼ˆ0.2968ï¼‰çš„ä¸¤å€å¤šï¼Œåˆ›é€ äº†æ–°çš„æŠ€æœ¯çºªå½•ã€‚Dolphin v1.0ä¹Ÿè¡¨ç°å‡ºç«äº‰åŠ›ï¼ŒéªŒè¯äº†ç»Ÿä¸€æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚å¯¹æ¯”ç»“æœæ˜¾ç¤ºï¼Œç»è¿‡å¢å¼ºæ¨ç†çš„è®­ç»ƒæ˜¾è‘—æé«˜äº†è¯Šæ–­çš„å‡†ç¡®æ€§ã€ä¸€è‡´æ€§å’Œå¯è§£é‡Šæ€§ï¼Œçªæ˜¾å…¶åœ¨é«˜é£é™©åŒ»ç–—äººå·¥æ™ºèƒ½ä¸­çš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25748v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è¶…å£°æ³¢åœ¨ç°ä»£åŒ»å­¦ä¸­çš„é‡è¦æ€§åŠå…¶æ‰€é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚æ“ä½œä¾èµ–æ€§ã€å›¾åƒå™ªå£°å’Œå®æ—¶æ‰«æç­‰ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæ–‡ç« æå‡ºäº†æµ·è±šv1.0åŠå…¶å¢å¼ºç‰ˆæµ·è±šR1ï¼Œè¿™æ˜¯é¦–ä¸ªå¤§è§„æ¨¡çš„å¤šæ¨¡å¼è¶…å£°æ³¢åŸºç¡€æ¨¡å‹ï¼Œåœ¨ä¸€ä¸ªç»Ÿä¸€çš„è§†è§‰è¯­è¨€æ¡†æ¶å†…èåˆäº†ä¸åŒçš„ä¸´åºŠä»»åŠ¡ã€‚é€šè¿‡é‡‡ç”¨ä¸‰é˜¶æ®µè®­ç»ƒç­–ç•¥å’Œå¤§è§„æ¨¡å¤šæ¨¡å¼æ•°æ®é›†ï¼Œæµ·è±šç³»åˆ—æ¨¡å‹åœ¨åˆ†ç±»ã€æ£€æµ‹ã€å›å½’å’ŒæŠ¥å‘Šç”Ÿæˆç­‰æ–¹é¢è¡¨ç°å‡ºå¯é æ€§èƒ½ï¼Œæµ·è±šR1åœ¨U2-Benchä¸Šçš„è¡¨ç°å°¤ä¸ºçªå‡ºï¼Œå®ç°äº†ä¸šç•Œé¢†å…ˆçš„ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¶…å£°æ³¢åœ¨ç°ä»£åŒ»å­¦ä¸­è‡³å…³é‡è¦ï¼Œä½†é¢ä¸´æ“ä½œä¾èµ–æ€§ã€å›¾åƒå™ªå£°å’Œå®æ—¶æ‰«æç­‰æŒ‘æˆ˜ã€‚</li>
<li>å¤šæ¨¡å¼æ¨¡å‹åœ¨è§£å†³è¶…å£°æ³¢å¤æ‚æ€§æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</li>
<li>æµ·è±šv1.0åŠå…¶å¢å¼ºç‰ˆæµ·è±šR1æ˜¯é¦–ä¸ªå¤§è§„æ¨¡çš„å¤šæ¨¡å¼è¶…å£°æ³¢åŸºç¡€æ¨¡å‹ï¼Œç»Ÿä¸€äº†ä¸åŒçš„ä¸´åºŠä»»åŠ¡ã€‚</li>
<li>é€šè¿‡é‡‡ç”¨ä¸‰é˜¶æ®µè®­ç»ƒç­–ç•¥å’Œå¤§è§„æ¨¡å¤šæ¨¡å¼æ•°æ®é›†ï¼Œæµ·è±šç³»åˆ—æ¨¡å‹å±•ç°å‡ºä¼˜ç§€æ€§èƒ½ã€‚</li>
<li>æµ·è±šR1åœ¨U2-Benchä¸Šçš„è¡¨ç°çªå‡ºï¼Œå®ç°äº†ä¸šç•Œé¢†å…ˆçš„ç»“æœã€‚</li>
<li>æ¨ç†å¢å¼ºè®­ç»ƒæ˜¾è‘—æé«˜äº†è¯Šæ–­çš„å‡†ç¡®æ€§ã€ä¸€è‡´æ€§å’Œå¯è§£é‡Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25748">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2509.25748v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2509.25748v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2509.25748v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2509.25748v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="U-MAN-U-Net-with-Multi-scale-Adaptive-KAN-Network-for-Medical-Image-Segmentation"><a href="#U-MAN-U-Net-with-Multi-scale-Adaptive-KAN-Network-for-Medical-Image-Segmentation" class="headerlink" title="U-MAN: U-Net with Multi-scale Adaptive KAN Network for Medical Image   Segmentation"></a>U-MAN: U-Net with Multi-scale Adaptive KAN Network for Medical Image   Segmentation</h2><p><strong>Authors:Bohan Huang, Qianyun Bao, Haoyuan Ma</strong></p>
<p>Medical image segmentation faces significant challenges in preserving fine-grained details and precise boundaries due to complex anatomical structures and pathological regions. These challenges primarily stem from two key limitations of conventional U-Net architectures: (1) their simple skip connections ignore the encoder-decoder semantic gap between various features, and (2) they lack the capability for multi-scale feature extraction in deep layers. To address these challenges, we propose the U-Net with Multi-scale Adaptive KAN (U-MAN), a novel architecture that enhances the emerging Kolmogorov-Arnold Network (KAN) with two specialized modules: Progressive Attention-Guided Feature Fusion (PAGF) and the Multi-scale Adaptive KAN (MAN). Our PAGF module replaces the simple skip connection, using attention to fuse features from the encoder and decoder. The MAN module enables the network to adaptively process features at multiple scales, improving its ability to segment objects of various sizes. Experiments on three public datasets (BUSI, GLAS, and CVC) show that U-MAN outperforms state-of-the-art methods, particularly in defining accurate boundaries and preserving fine details. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²åœ¨ä¿ç•™ç²¾ç»†ç²’åº¦å’Œç²¾ç¡®è¾¹ç•Œæ–¹é¢é¢ä¸´ç€é‡å¤§æŒ‘æˆ˜ï¼Œè¿™æ˜¯ç”±äºå¤æ‚çš„è§£å‰–ç»“æ„å’Œç—…ç†åŒºåŸŸæ‰€å¯¼è‡´çš„ã€‚è¿™äº›æŒ‘æˆ˜ä¸»è¦æºäºä¼ ç»ŸU-Netæ¶æ„çš„ä¸¤ä¸ªå…³é”®å±€é™ï¼šï¼ˆ1ï¼‰å…¶ç®€å•çš„è·³è·ƒè¿æ¥å¿½ç•¥äº†ç¼–ç å™¨-è§£ç å™¨ä¹‹é—´å„ç§ç‰¹å¾çš„è¯­ä¹‰å·®è·ï¼›ï¼ˆ2ï¼‰å®ƒä»¬åœ¨æ·±å±‚ä¸­ç¼ºä¹å¤šå°ºåº¦ç‰¹å¾æå–çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†å…·æœ‰å¤šå°ºåº¦è‡ªé€‚åº”KANï¼ˆU-MANï¼‰çš„U-Netï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ¶æ„ï¼Œå®ƒé€šè¿‡ä¸¤ä¸ªä¸“ç”¨æ¨¡å—å¢å¼ºäº†æ–°å…´çš„Kolmogorov-Arnoldç½‘ç»œï¼ˆKANï¼‰ï¼šæ¸è¿›æ³¨æ„åŠ›å¼•å¯¼ç‰¹å¾èåˆï¼ˆPAGFï¼‰å’Œå¤šå°ºåº¦è‡ªé€‚åº”KANï¼ˆMANï¼‰ã€‚æˆ‘ä»¬çš„PAGFæ¨¡å—ä½¿ç”¨æ³¨æ„åŠ›èåˆç¼–ç å™¨å’Œè§£ç å™¨çš„ç‰¹å¾ï¼Œæ›¿æ¢äº†ç®€å•çš„è·³è·ƒè¿æ¥ã€‚MANæ¨¡å—ä½¿ç½‘ç»œèƒ½å¤Ÿè‡ªé€‚åº”åœ°å¤„ç†å¤šå°ºåº¦ç‰¹å¾ï¼Œæé«˜äº†å…¶åˆ†å‰²å„ç§å°ºå¯¸ç‰©ä½“çš„èƒ½åŠ›ã€‚åœ¨BUSIã€GLASå’ŒCVCä¸‰ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒU-MANä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨å®šä¹‰ç²¾ç¡®è¾¹ç•Œå’Œä¿ç•™ç»†èŠ‚æ–¹é¢ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22444v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²åœ¨ä¿ç•™ç²¾ç»†ç²’åº¦å’Œç²¾ç¡®è¾¹ç•Œæ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼Œæºäºä¼ ç»ŸU-Netæ¶æ„çš„ä¸¤ä¸ªä¸»è¦å±€é™ï¼šä¸€æ˜¯ç®€å•çš„è·³è·ƒè¿æ¥å¿½ç•¥äº†ç¼–ç å™¨ä¸è§£ç å™¨ä¹‹é—´çš„è¯­ä¹‰å·®è·ï¼ŒäºŒæ˜¯ç¼ºä¹æ·±å±‚å¤šå°ºåº¦ç‰¹å¾æå–èƒ½åŠ›ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å¸¦æœ‰å¤šå°ºåº¦è‡ªé€‚åº”KANï¼ˆU-MANï¼‰çš„æ–°å‹U-Netæ¶æ„ï¼Œé€šè¿‡ä¸¤ä¸ªä¸“é—¨æ¨¡å—â€”â€”æ¸è¿›å¼æ³¨æ„åŠ›å¼•å¯¼ç‰¹å¾èåˆï¼ˆPAGFï¼‰å’Œå¤šå°ºåº¦è‡ªé€‚åº”KANï¼ˆMANï¼‰æ¥å¢å¼ºKolmogorov-Arnoldç½‘ç»œï¼ˆKANï¼‰ã€‚PAGFæ¨¡å—ç”¨æ³¨æ„åŠ›èåˆç¼–ç å™¨å’Œè§£ç å™¨çš„ç‰¹å¾ï¼Œæ›¿ä»£äº†ç®€å•çš„è·³è·ƒè¿æ¥ã€‚MANæ¨¡å—ä½¿ç½‘ç»œèƒ½å¤Ÿè‡ªé€‚åº”åœ°å¤„ç†å¤šå°ºåº¦ç‰¹å¾ï¼Œæé«˜äº†åˆ†å‰²å„ç§å°ºå¯¸ç‰©ä½“çš„èƒ½åŠ›ã€‚åœ¨ä¸‰ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒU-MANä¼˜äºæœ€æ–°æ–¹æ³•ï¼Œå°¤å…¶åœ¨å®šä¹‰ç²¾ç¡®è¾¹ç•Œå’Œä¿ç•™ç»†èŠ‚æ–¹é¢ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²é¢ä¸´ä¿ç•™ç²¾ç»†ç²’åº¦å’Œç²¾ç¡®è¾¹ç•Œçš„æŒ‘æˆ˜ã€‚</li>
<li>ä¼ ç»ŸU-Netæ¶æ„å­˜åœ¨ä¸¤ä¸ªä¸»è¦å±€é™ï¼šå¿½ç•¥ç¼–ç å™¨å’Œè§£ç å™¨ä¹‹é—´çš„è¯­ä¹‰å·®è·ï¼Œä»¥åŠç¼ºä¹æ·±å±‚å¤šå°ºåº¦ç‰¹å¾æå–èƒ½åŠ›ã€‚</li>
<li>ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæå‡ºäº†å¸¦æœ‰U-MANçš„æ–°å‹U-Netæ¶æ„ã€‚</li>
<li>U-MANåŒ…æ‹¬ä¸¤ä¸ªä¸“é—¨æ¨¡å—ï¼šPAGFå’ŒMANã€‚</li>
<li>PAGFæ¨¡å—ç”¨æ³¨æ„åŠ›èåˆç‰¹å¾ï¼Œæ›¿ä»£äº†ç®€å•çš„è·³è·ƒè¿æ¥ã€‚</li>
<li>MANæ¨¡å—ä½¿ç½‘ç»œèƒ½å¤Ÿè‡ªé€‚åº”å¤„ç†å¤šå°ºåº¦ç‰¹å¾ï¼Œæé«˜åˆ†å‰²å„ç§å°ºå¯¸ç‰©ä½“çš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22444">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2509.22444v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2509.22444v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2509.22444v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2509.22444v2/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2509.22444v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2509.22444v2/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2509.22444v2/page_3_2.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Diffusion-Bridge-Variational-Inference-for-Deep-Gaussian-Processes"><a href="#Diffusion-Bridge-Variational-Inference-for-Deep-Gaussian-Processes" class="headerlink" title="Diffusion Bridge Variational Inference for Deep Gaussian Processes"></a>Diffusion Bridge Variational Inference for Deep Gaussian Processes</h2><p><strong>Authors:Jian Xu, Qibin Zhao, John Paisley, Delu Zeng</strong></p>
<p>Deep Gaussian processes (DGPs) enable expressive hierarchical Bayesian modeling but pose substantial challenges for posterior inference, especially over inducing variables. Denoising diffusion variational inference (DDVI) addresses this by modeling the posterior as a time-reversed diffusion from a simple Gaussian prior. However, DDVIâ€™s fixed unconditional starting distribution remains far from the complex true posterior, resulting in inefficient inference trajectories and slow convergence. In this work, we propose Diffusion Bridge Variational Inference (DBVI), a principled extension of DDVI that initiates the reverse diffusion from a learnable, data-dependent initial distribution. This initialization is parameterized via an amortized neural network and progressively adapted using gradients from the ELBO objective, reducing the posterior gap and improving sample efficiency. To enable scalable amortization, we design the network to operate on the inducing inputs, which serve as structured, low-dimensional summaries of the dataset and naturally align with the inducing variablesâ€™ shape. DBVI retains the mathematical elegance of DDVI, including Girsanov-based ELBOs and reverse-time SDEs,while reinterpreting the prior via a Doob-bridged diffusion process. We derive a tractable training objective under this formulation and implement DBVI for scalable inference in large-scale DGPs. Across regression, classification, and image reconstruction tasks, DBVI consistently outperforms DDVI and other variational baselines in predictive accuracy, convergence speed, and posterior quality. </p>
<blockquote>
<p>æ·±åº¦é«˜æ–¯è¿‡ç¨‹ï¼ˆDGPsï¼‰èƒ½å¤Ÿå®ç°è¡¨è¾¾æ€§å±‚æ¬¡åŒ–çš„è´å¶æ–¯å»ºæ¨¡ï¼Œä½†ç»™åéªŒæ¨æ–­å¸¦æ¥äº†å®è´¨æ€§çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯±å¯¼å˜é‡æ–¹é¢ã€‚é™å™ªæ‰©æ•£å˜åˆ†æ¨æ–­ï¼ˆDDVIï¼‰é€šè¿‡æ¨¡æ‹ŸåéªŒä½œä¸ºä»ç®€å•é«˜æ–¯å…ˆéªŒçš„æ—¶é—´åè½¬æ‰©æ•£æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚ç„¶è€Œï¼ŒDDVIçš„å›ºå®šæ— æ¡ä»¶èµ·å§‹åˆ†å¸ƒä¸å¤æ‚çš„çœŸå®åéªŒç›¸å·®ç”šè¿œï¼Œå¯¼è‡´æ¨ç†è½¨è¿¹æ•ˆç‡ä½ä¸‹ï¼Œæ”¶æ•›ç¼“æ…¢ã€‚</p>
</blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†æ‰©æ•£æ¡¥å˜åˆ†æ¨æ–­ï¼ˆDBVIï¼‰ï¼Œè¿™æ˜¯ä¸€ç§DDVIçš„åŸåˆ™æ€§æ‰©å±•ï¼Œå®ƒä»å¯å­¦ä¹ çš„ã€æ•°æ®ä¾èµ–çš„åˆå§‹åˆ†å¸ƒå¼€å§‹åå‘æ‰©æ•£ã€‚è¿™ä¸ªåˆå§‹åŒ–æ˜¯é€šè¿‡å¹³å‡ç¥ç»ç½‘ç»œå‚æ•°åŒ–çš„ï¼Œå¹¶éšç€æ¥è‡ªELBOç›®æ ‡çš„æ¢¯åº¦é€æ­¥é€‚åº”ï¼Œä»è€Œå‡å°‘åéªŒå·®è·å¹¶æé«˜äº†æ ·æœ¬æ•ˆç‡ã€‚ä¸ºäº†å®ç°å¯æ‰©å±•çš„å¹³å‡å¤„ç†ï¼Œæˆ‘ä»¬è®¾è®¡ç½‘ç»œåœ¨è¯±å¯¼è¾“å…¥ä¸Šè¿è¡Œï¼Œè¿™äº›è¾“å…¥ä½œä¸ºç»“æ„åŒ–ã€ä½ç»´åº¦çš„æ•°æ®é›†æ‘˜è¦ï¼Œè‡ªç„¶ä¸è¯±å¯¼å˜é‡çš„å½¢çŠ¶å¯¹é½ã€‚DBVIä¿ç•™äº†DDVIçš„æ•°å­¦ä¼˜é›…æ€§ï¼ŒåŒ…æ‹¬åŸºäºGirsanovçš„ELBOså’Œåå‘æ—¶é—´SDEsï¼ŒåŒæ—¶é€šè¿‡Doobæ¡¥æ‰©æ•£è¿‡ç¨‹é‡æ–°è§£é‡Šå…ˆéªŒã€‚æˆ‘ä»¬åœ¨æ­¤å…¬å¼ä¸‹æ¨å¯¼äº†ä¸€ä¸ªå¯è¡Œçš„è®­ç»ƒç›®æ ‡ï¼Œå¹¶å®ç°äº†DBVIç”¨äºå¤§è§„æ¨¡DGPä¸­çš„å¯æ‰©å±•æ¨ç†ã€‚åœ¨å›å½’ã€åˆ†ç±»å’Œå›¾åƒé‡å»ºä»»åŠ¡ä¸­ï¼ŒDBVIåœ¨é¢„æµ‹ç²¾åº¦ã€æ”¶æ•›é€Ÿåº¦å’ŒåéªŒè´¨é‡æ–¹é¢å§‹ç»ˆä¼˜äºDDVIå’Œå…¶ä»–å˜åˆ†åŸºå‡†ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.19078v2">PDF</a> </p>
<p><strong>Summary</strong><br>     æ‰©æ•£æ¡¥å˜åˆ†æ¨ç†ï¼ˆDBVIï¼‰æ˜¯ä¸€ç§åŸºäºæ·±åº¦é«˜æ–¯è¿‡ç¨‹ï¼ˆDGPsï¼‰å’Œå»å™ªæ‰©æ•£å˜åˆ†æ¨ç†ï¼ˆDDVIï¼‰çš„æ–¹æ³•ï¼Œå®ƒé€šè¿‡ä»å¯å­¦ä¹ çš„ã€æ•°æ®ä¾èµ–çš„åˆå§‹åˆ†å¸ƒå¼€å§‹åå‘æ‰©æ•£ï¼Œæé«˜äº†æ¨ç†è½¨è¿¹çš„æ•ˆç‡å’Œæ”¶æ•›é€Ÿåº¦ã€‚DBVIé‡‡ç”¨æ‘Šé”€ç¥ç»ç½‘ç»œè¿›è¡Œå‚æ•°åŒ–ï¼Œå¹¶ä½¿ç”¨ELBOç›®æ ‡çš„æ¢¯åº¦è¿›è¡Œé€æ­¥é€‚åº”ï¼Œç¼©å°äº†åéªŒå·®è·å¹¶æé«˜äº†æ ·æœ¬æ•ˆç‡ã€‚å®ƒå¯¹è¯±å¯¼è¾“å…¥è¿›è¡Œæ“ä½œï¼Œä½œä¸ºæ•°æ®é›†çš„ç»“æ„æ€§ã€ä½ç»´æ‘˜è¦ï¼Œä¸è‡ªç„¶å¯¹é½è¯±å¯¼å˜é‡çš„å½¢çŠ¶ã€‚DBVIåœ¨å›å½’ã€åˆ†ç±»å’Œå›¾åƒé‡å»ºä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œåœ¨é¢„æµ‹å‡†ç¡®æ€§ã€æ”¶æ•›é€Ÿåº¦å’ŒåéªŒè´¨é‡æ–¹é¢å‡ä¼˜äºDDVIå’Œå…¶ä»–å˜åˆ†åŸºå‡†æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Deep Gaussian Processes (DGPs) æä¾›äº†è¡¨è¾¾ä¸°å¯Œçš„å±‚æ¬¡åŒ–è´å¶æ–¯å»ºæ¨¡ï¼Œä½†åéªŒæ¨ç†å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>Denoising Diffusion Variational Inference (DDVI) é€šè¿‡å°†åéªŒå»ºæ¨¡ä¸ºä»ç®€å•é«˜æ–¯å…ˆéªŒå¼€å§‹çš„é€†å‘æ‰©æ•£è¿‡ç¨‹æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚</li>
<li>DDVIçš„æ— æ¡ä»¶èµ·å§‹åˆ†å¸ƒå›ºå®šï¼Œä¸å¤æ‚çš„çœŸå®åéªŒç›¸å·®è¾ƒè¿œï¼Œå¯¼è‡´æ¨ç†è½¨è¿¹ä¸å¤Ÿé«˜æ•ˆä¸”æ”¶æ•›ç¼“æ…¢ã€‚</li>
<li>Diffusion Bridge Variational Inference (DBVI) æ˜¯DDVIçš„æ‰©å±•ï¼Œå®ƒé€šè¿‡ä»å¯å­¦ä¹ çš„ã€æ•°æ®ä¾èµ–çš„åˆå§‹åˆ†å¸ƒå¼€å§‹åå‘æ‰©æ•£ï¼Œç¼©å°äº†åéªŒå·®è·å¹¶æé«˜äº†æ ·æœ¬æ•ˆç‡ã€‚</li>
<li>DBVIä½¿ç”¨æ‘Šé”€ç¥ç»ç½‘ç»œè¿›è¡Œå‚æ•°åŒ–ï¼Œå¹¶é€šè¿‡ELBOç›®æ ‡çš„æ¢¯åº¦è¿›è¡Œé€æ­¥é€‚åº”ã€‚</li>
<li>DBVIå¯¹è¯±å¯¼è¾“å…¥è¿›è¡Œæ“ä½œï¼Œåˆ©ç”¨ç»“æ„åŒ–ã€ä½ç»´çš„æ•°æ®æ‘˜è¦ï¼Œæé«˜äº†æ–¹æ³•çš„å¯æ‰©å±•æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.19078">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2509.19078v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2509.19078v2/page_1_0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Physics-Guided-Null-Space-Diffusion-with-Sparse-Masking-for-Corrective-Sparse-View-CT-Reconstruction"><a href="#Physics-Guided-Null-Space-Diffusion-with-Sparse-Masking-for-Corrective-Sparse-View-CT-Reconstruction" class="headerlink" title="Physics-Guided Null-Space Diffusion with Sparse Masking for Corrective   Sparse-View CT Reconstruction"></a>Physics-Guided Null-Space Diffusion with Sparse Masking for Corrective   Sparse-View CT Reconstruction</h2><p><strong>Authors:Zekun Zhou, Yanru Gong, Liu Shi, Qiegen Liu</strong></p>
<p>Diffusion models have demonstrated remarkable generative capabilities in image processing tasks. We propose a Sparse condition Temporal Rewighted Integrated Distribution Estimation guided diffusion model (STRIDE) for sparse-view CT reconstruction. Specifically, we design a joint training mechanism guided by sparse conditional probabilities to facilitate the model effective learning of missing projection view completion and global information modeling. Based on systematic theoretical analysis, we propose a temporally varying sparse condition reweighting guidance strategy to dynamically adjusts weights during the progressive denoising process from pure noise to the real image, enabling the model to progressively perceive sparse-view information. The linear regression is employed to correct distributional shifts between known and generated data, mitigating inconsistencies arising during the guidance process. Furthermore, we construct a dual-network parallel architecture to perform global correction and optimization across multiple sub-frequency components, thereby effectively improving the model capability in both detail restoration and structural preservation, ultimately achieving high-quality image reconstruction. Experimental results on both public and real datasets demonstrate that the proposed method achieves the best improvement of 2.58 dB in PSNR, increase of 2.37% in SSIM, and reduction of 0.236 in MSE compared to the best-performing baseline methods. The reconstructed images exhibit excellent generalization and robustness in terms of structural consistency, detail restoration, and artifact suppression. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒå¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†æ˜¾è‘—çš„ç”Ÿæˆèƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºç¨€ç–è§†å›¾CTé‡å»ºçš„ç¨€ç–æ¡ä»¶æ—¶é—´åŠ æƒç§¯åˆ†åˆ†å¸ƒä¼°è®¡å¼•å¯¼æ‰©æ•£æ¨¡å‹ï¼ˆSTRIDEï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§ç”±ç¨€ç–æ¡ä»¶æ¦‚ç‡å¼•å¯¼çš„è”åˆè®­ç»ƒæœºåˆ¶ï¼Œä»¥ä¿ƒè¿›æ¨¡å‹æœ‰æ•ˆåœ°å­¦ä¹ ç¼ºå¤±æŠ•å½±è§†å›¾çš„è¡¥å…¨å’Œå…¨å±€ä¿¡æ¯å»ºæ¨¡ã€‚åŸºäºç³»ç»Ÿçš„ç†è®ºåˆ†æï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ—¶é—´å˜åŒ–çš„ç¨€ç–æ¡ä»¶é‡åŠ æƒå¼•å¯¼ç­–ç•¥ï¼Œåœ¨ä»çº¯å™ªå£°åˆ°çœŸå®å›¾åƒçš„æ¸è¿›å»å™ªè¿‡ç¨‹ä¸­åŠ¨æ€è°ƒæ•´æƒé‡ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿé€æ­¥æ„ŸçŸ¥ç¨€ç–è§†å›¾ä¿¡æ¯ã€‚é‡‡ç”¨çº¿æ€§å›å½’æ¥æ ¡æ­£å·²çŸ¥æ•°æ®å’Œç”Ÿæˆæ•°æ®ä¹‹é—´çš„åˆ†å¸ƒåç§»ï¼Œå‡è½»æŒ‡å¯¼è¿‡ç¨‹ä¸­äº§ç”Ÿçš„ä¸ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŒç½‘ç»œå¹¶è¡Œæ¶æ„ï¼Œä»¥åœ¨å¤šä¸ªå­é¢‘ç‡åˆ†é‡ä¸Šæ‰§è¡Œå…¨å±€æ ¡æ­£å’Œä¼˜åŒ–ï¼Œä»è€Œæœ‰æ•ˆåœ°æé«˜äº†æ¨¡å‹åœ¨ç»†èŠ‚æ¢å¤å’Œç»“æ„ä¿æŒæ–¹é¢çš„èƒ½åŠ›ï¼Œæœ€ç»ˆå®ç°äº†é«˜è´¨é‡å›¾åƒé‡å»ºã€‚åœ¨å…¬å…±å’ŒçœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œä¸è¡¨ç°æœ€ä½³çš„åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨PSNRä¸Šå®ç°äº†2.58 dBçš„æœ€ä½³æå‡ï¼ŒSSIMå¢åŠ äº†2.37%ï¼ŒMSEå‡å°‘äº†0.236ã€‚é‡å»ºçš„å›¾åƒåœ¨ç»“æ„ä¸€è‡´æ€§ã€ç»†èŠ‚æ¢å¤å’Œä¼ªå½±æŠ‘åˆ¶æ–¹é¢å…·æœ‰è‰¯å¥½çš„é€šç”¨æ€§å’Œé²æ£’æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.05992v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºç¨€ç–æ¡ä»¶æ—¶é—´åŠ æƒç§¯åˆ†åˆ†å¸ƒä¼°è®¡çš„æ‰©æ•£æ¨¡å‹ï¼ˆSTRIDEï¼‰ï¼Œç”¨äºç¨€ç–è§†å›¾CTé‡å»ºã€‚é€šè¿‡è”åˆè®­ç»ƒæœºåˆ¶å’Œç¨€ç–æ¡ä»¶æ¦‚ç‡æŒ‡å¯¼ï¼Œæ¨¡å‹èƒ½æœ‰æ•ˆå­¦ä¹ ç¼ºå¤±æŠ•å½±è§†å›¾çš„è¡¥å…¨å’Œå…¨å±€ä¿¡æ¯å»ºæ¨¡ã€‚é‡‡ç”¨åŠ¨æ€è°ƒæ•´æƒé‡çš„æ—¶é—´å˜åŒ–ç¨€ç–æ¡ä»¶é‡åŠ æƒæŒ‡å¯¼ç­–ç•¥ï¼Œåœ¨ä»çº¯å™ªå£°åˆ°çœŸå®å›¾åƒçš„é€æ­¥å»å™ªè¿‡ç¨‹ä¸­é€æ­¥æ„ŸçŸ¥ç¨€ç–è§†å›¾ä¿¡æ¯ã€‚é€šè¿‡çº¿æ€§å›å½’æ ¡æ­£å·²çŸ¥å’Œç”Ÿæˆæ•°æ®ä¹‹é—´çš„åˆ†å¸ƒåç§»ï¼Œå‡è½»æŒ‡å¯¼è¿‡ç¨‹ä¸­çš„ä¸ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œæ„å»ºåŒç½‘ç»œå¹¶è¡Œæ¶æ„ï¼Œå®ç°å¤šå­é¢‘ç‡ç»„ä»¶çš„å…¨å±€æ ¡æ­£å’Œä¼˜åŒ–ï¼Œæé«˜æ¨¡å‹åœ¨ç»†èŠ‚æ¢å¤å’Œç»“æ„ä¿æŒæ–¹é¢çš„èƒ½åŠ›ï¼Œæœ€ç»ˆå®ç°é«˜è´¨é‡å›¾åƒé‡å»ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºçš„STRIDEæ¨¡å‹åˆ©ç”¨æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒå¤„ç†ä»»åŠ¡ä¸­çš„ç”Ÿæˆèƒ½åŠ›ï¼Œç‰¹åˆ«é€‚ç”¨äºç¨€ç–è§†å›¾CTé‡å»ºã€‚</li>
<li>é€šè¿‡è”åˆè®­ç»ƒæœºåˆ¶å’Œç¨€ç–æ¡ä»¶æ¦‚ç‡æŒ‡å¯¼ï¼Œæ¨¡å‹èƒ½æœ‰æ•ˆå­¦ä¹ ç¼ºå¤±æŠ•å½±è§†å›¾çš„è¡¥å…¨å’Œå…¨å±€ä¿¡æ¯å»ºæ¨¡ã€‚</li>
<li>é‡‡ç”¨æ—¶é—´å˜åŒ–çš„ç¨€ç–æ¡ä»¶é‡åŠ æƒç­–ç•¥ï¼Œåœ¨å»å™ªè¿‡ç¨‹ä¸­åŠ¨æ€è°ƒæ•´æƒé‡ï¼Œé€æ­¥æ„ŸçŸ¥ç¨€ç–è§†å›¾ä¿¡æ¯ã€‚</li>
<li>é€šè¿‡çº¿æ€§å›å½’æ ¡æ­£åˆ†å¸ƒåç§»ï¼Œå‡è½»æŒ‡å¯¼è¿‡ç¨‹ä¸­çš„ä¸ä¸€è‡´æ€§ã€‚</li>
<li>æ„å»ºåŒç½‘ç»œå¹¶è¡Œæ¶æ„ï¼Œå®ç°å¤šå­é¢‘ç‡ç»„ä»¶çš„å…¨å±€æ ¡æ­£å’Œä¼˜åŒ–ã€‚</li>
<li>å®éªŒç»“æœå±•ç¤ºï¼Œä¸æœ€ä½³åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼Œæ‰€ææ–¹æ³•åœ¨PSNRä¸Šæé«˜äº†2.58dBï¼Œåœ¨SSIMä¸Šå¢åŠ äº†2.37%ï¼Œåœ¨MSEä¸Šå‡å°‘äº†0.236ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.05992">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2509.05992v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2509.05992v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2509.05992v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2509.05992v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2509.05992v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Imagining-Alternatives-Towards-High-Resolution-3D-Counterfactual-Medical-Image-Generation-via-Language-Guidance"><a href="#Imagining-Alternatives-Towards-High-Resolution-3D-Counterfactual-Medical-Image-Generation-via-Language-Guidance" class="headerlink" title="Imagining Alternatives: Towards High-Resolution 3D Counterfactual   Medical Image Generation via Language Guidance"></a>Imagining Alternatives: Towards High-Resolution 3D Counterfactual   Medical Image Generation via Language Guidance</h2><p><strong>Authors:Mohamed Mohamed, Brennan Nichyporuk, Douglas L. Arnold, Tal Arbel</strong></p>
<p>Vision-language models have demonstrated impressive capabilities in generating 2D images under various conditions; however, the success of these models is largely enabled by extensive, readily available pretrained foundation models. Critically, comparable pretrained models do not exist for 3D, significantly limiting progress. As a result, the potential of vision-language models to produce high-resolution 3D counterfactual medical images conditioned solely on natural language remains unexplored. Addressing this gap would enable powerful clinical and research applications, such as personalized counterfactual explanations, simulation of disease progression, and enhanced medical training by visualizing hypothetical conditions in realistic detail. Our work takes a step toward this challenge by introducing a framework capable of generating high-resolution 3D counterfactual medical images of synthesized patients guided by free-form language prompts. We adapt state-of-the-art 3D diffusion models with enhancements from Simple Diffusion and incorporate augmented conditioning to improve text alignment and image quality. To our knowledge, this is the first demonstration of a language-guided native-3D diffusion model applied to neurological imaging, where faithful three-dimensional modeling is essential. On two neurological MRI datasets, our framework simulates varying counterfactual lesion loads in Multiple Sclerosis and cognitive states in Alzheimerâ€™s disease, generating high-quality images while preserving subject fidelity. Our results lay the groundwork for prompt-driven disease progression analysis in 3D medical imaging. Project link - <a target="_blank" rel="noopener" href="https://lesupermomo.github.io/imagining-alternatives/">https://lesupermomo.github.io/imagining-alternatives/</a>. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å„ç§æ¡ä»¶ä¸‹ç”Ÿæˆ2Då›¾åƒçš„èƒ½åŠ›ä»¤äººå°è±¡æ·±åˆ»ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹çš„æˆåŠŸåœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå¾—ç›Šäºå¹¿æ³›ä¸”æ˜“äºè·å–çš„é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹ã€‚å…³é”®çš„æ˜¯ï¼Œ3Dé¢†åŸŸçš„é¢„è®­ç»ƒæ¨¡å‹å°šä¸å­˜åœ¨ï¼Œè¿™æå¤§åœ°é™åˆ¶äº†è¿›å±•ã€‚å› æ­¤ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹ä»…æ ¹æ®è‡ªç„¶è¯­è¨€ç”Ÿæˆé«˜åˆ†è¾¨ç‡çš„3Dåäº‹å®åŒ»ç–—å›¾åƒçš„æ½œåŠ›å°šæœªè¢«æ¢ç´¢ã€‚å¼¥è¡¥è¿™ä¸€ç©ºç™½å°†èƒ½æ¨åŠ¨å¼ºå¤§çš„ä¸´åºŠå’Œç ”ç©¶åº”ç”¨ï¼Œå¦‚ä¸ªæ€§åŒ–çš„åäº‹å®è§£é‡Šã€ç–¾ç—…è¿›å±•æ¨¡æ‹Ÿï¼Œä»¥åŠé€šè¿‡è¯¦ç»†å±•ç¤ºå‡è®¾æ¡ä»¶æ¥å¢å¼ºåŒ»ç–—åŸ¹è®­ã€‚æˆ‘ä»¬çš„å·¥ä½œæœç€è¿™ä¸€æŒ‘æˆ˜è¿ˆå‡ºäº†ä¸€æ­¥ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæ ¹æ®è‡ªç”±å½¢å¼çš„è¯­è¨€æç¤ºï¼Œç”Ÿæˆç”±åˆæˆæ‚£è€…å¼•å¯¼çš„é«˜åˆ†è¾¨ç‡çš„3Dåäº‹å®åŒ»ç–—å›¾åƒã€‚æˆ‘ä»¬æ”¹è¿›äº†æœ€å…ˆè¿›çš„3Dæ‰©æ•£æ¨¡å‹ï¼ŒåŠ å…¥äº†Simple Diffusionçš„å¢å¼ºåŠŸèƒ½ï¼Œå¹¶å¢åŠ äº†é™„åŠ æ¡ä»¶ï¼Œä»¥æé«˜æ–‡æœ¬å¯¹é½å’Œå›¾åƒè´¨é‡ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–æ¬¡å°†è¯­è¨€å¼•å¯¼çš„æœ¬åœ°3Dæ‰©æ•£æ¨¡å‹åº”ç”¨äºç¥ç»æˆåƒï¼Œå¿ å®çš„ä¸‰ç»´å»ºæ¨¡åœ¨è¿™é‡Œè‡³å…³é‡è¦ã€‚æˆ‘ä»¬çš„æ¡†æ¶åœ¨ä¸¤ç»„ç¥ç»MRIæ•°æ®é›†ä¸Šæ¨¡æ‹Ÿäº†å¤šå‘æ€§ç¡¬åŒ–ç—‡çš„å¤šç§åäº‹å®ç—…å˜è´Ÿè·å’Œé˜¿å°”èŒ¨æµ·é»˜ç—…ä¸­çš„è®¤çŸ¥çŠ¶æ€ï¼Œç”Ÿæˆé«˜è´¨é‡å›¾åƒçš„åŒæ—¶ä¿æŒäº†ä¸»ä½“ä¿çœŸåº¦ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºåŸºäºæç¤ºçš„ç–¾ç—…è¿›å±•åˆ†æåœ¨3DåŒ»ç–—æˆåƒæ–¹é¢çš„åº”ç”¨å¥ å®šäº†åŸºç¡€ã€‚é¡¹ç›®é“¾æ¥ï¼š<a target="_blank" rel="noopener" href="https://lesupermomo.github.io/imagining-alternatives/">https://lesupermomo.github.io/imagining-alternatives/</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.05978v2">PDF</a> Accepted to the 2025 MICCAI ELAMI Workshop</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç”ŸæˆäºŒç»´å›¾åƒæ–¹é¢çš„å‡ºè‰²è¡¨ç°ï¼Œä½†å…¶åœ¨ä¸‰ç»´å›¾åƒç”Ÿæˆé¢†åŸŸçš„åº”ç”¨ä»å—é™ã€‚ç”±äºç¼ºä¹ç›¸åº”çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆé«˜åˆ†è¾¨ç‡çš„ä¸‰ç»´è™šæ„åŒ»ç–—å›¾åƒæ–¹é¢çš„æ½œåŠ›å°šæœªè¢«æ¢ç´¢ã€‚æœ¬æ–‡å¼•å…¥äº†ä¸€ä¸ªæ¡†æ¶ï¼Œèƒ½å¤Ÿé€šè¿‡è‡ªç”±å½¢å¼çš„æ–‡å­—æç¤ºç”Ÿæˆé«˜åˆ†è¾¨ç‡çš„ä¸‰ç»´è™šæ„åŒ»ç–—å›¾åƒï¼Œä¸ºä¸´åºŠå’Œç ”ç©¶åº”ç”¨æä¾›äº†å¯èƒ½ï¼Œå¦‚ä¸ªæ€§åŒ–åäº‹å®è§£é‡Šã€ç–¾ç—…è¿›å±•æ¨¡æ‹Ÿå’Œå¢å¼ºåŒ»ç–—è®­ç»ƒç­‰ã€‚è¯¥æ¡†æ¶é¦–æ¬¡å°†è¯­è¨€å¼•å¯¼çš„ä¸‰ç»´æ‰©æ•£æ¨¡å‹åº”ç”¨äºç¥ç»æˆåƒï¼Œå¹¶åœ¨ä¸¤ä¸ªç¥ç»MRIæ•°æ®é›†ä¸Šè¿›è¡Œäº†æ¨¡æ‹Ÿæµ‹è¯•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç”ŸæˆäºŒç»´å›¾åƒæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ä¸‰ç»´å›¾åƒç”Ÿæˆä¸Šå—é¢„è®­ç»ƒæ¨¡å‹ç¼ºä¹çš„é™åˆ¶ã€‚</li>
<li>ç¼ºä¹ç›¸åº”çš„é¢„è®­ç»ƒæ¨¡å‹é™åˆ¶äº†è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆé«˜åˆ†è¾¨ç‡ä¸‰ç»´è™šæ„åŒ»ç–—å›¾åƒæ–¹é¢çš„åº”ç”¨ã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªæ¡†æ¶ï¼Œèƒ½å¤Ÿé€šè¿‡è‡ªç”±å½¢å¼çš„æ–‡å­—æç¤ºç”Ÿæˆé«˜åˆ†è¾¨ç‡çš„ä¸‰ç»´è™šæ„åŒ»ç–—å›¾åƒã€‚</li>
<li>è¯¥æ¡†æ¶ä¸ºä¸´åºŠå’Œç ”ç©¶åº”ç”¨æä¾›äº†å¯èƒ½ï¼Œå¦‚ä¸ªæ€§åŒ–åäº‹å®è§£é‡Šã€ç–¾ç—…è¿›å±•æ¨¡æ‹Ÿå’Œå¢å¼ºåŒ»ç–—è®­ç»ƒã€‚</li>
<li>è¯¥æ¡†æ¶é¦–æ¬¡å°†è¯­è¨€å¼•å¯¼çš„ä¸‰ç»´æ‰©æ•£æ¨¡å‹åº”ç”¨äºç¥ç»æˆåƒã€‚</li>
<li>æ¡†æ¶åœ¨ä¸¤ä¸ªç¥ç»MRIæ•°æ®é›†ä¸Šè¿›è¡Œäº†æ¨¡æ‹Ÿæµ‹è¯•ï¼Œç”Ÿæˆäº†é«˜è´¨é‡çš„å›¾ç‰‡ï¼Œå¹¶ä¿æŒäº†ä¸»é¢˜å¿ å®åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.05978">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2509.05978v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2509.05978v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2509.05978v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2509.05978v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2509.05978v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Can-General-Purpose-Omnimodels-Compete-with-Specialists-A-Case-Study-in-Medical-Image-Segmentation"><a href="#Can-General-Purpose-Omnimodels-Compete-with-Specialists-A-Case-Study-in-Medical-Image-Segmentation" class="headerlink" title="Can General-Purpose Omnimodels Compete with Specialists? A Case Study in   Medical Image Segmentation"></a>Can General-Purpose Omnimodels Compete with Specialists? A Case Study in   Medical Image Segmentation</h2><p><strong>Authors:Yizhe Zhang, Qiang Chen, Tao Zhou</strong></p>
<p>The emergence of powerful, general-purpose omnimodels capable of processing diverse data modalities has raised a critical question: can these <code>jack-of-all-trades&#39;&#39; systems perform on par with highly specialized models in knowledge-intensive domains? This work investigates this question within the high-stakes field of medical image segmentation. We conduct a comparative study analyzing the zero-shot performance of a state-of-the-art omnimodel (Gemini, the </code>Nano Bananaâ€™â€™ model) against domain-specific deep learning models on three distinct tasks: polyp (endoscopy), retinal vessel (fundus), and breast tumor segmentation (ultrasound). Our study focuses on performance at the extremes by curating subsets of the <code>easiest&#39;&#39; and </code>hardestâ€™â€™ cases based on the specialist modelsâ€™ accuracy. Our findings reveal a nuanced and task-dependent landscape. For polyp and breast tumor segmentation, specialist models excel on easy samples, but the omnimodel demonstrates greater robustness on hard samples where specialists fail catastrophically. Conversely, for the fine-grained task of retinal vessel segmentation, the specialist model maintains superior performance across both easy and hard cases. Intriguingly, qualitative analysis suggests omnimodels may possess higher sensitivity, identifying subtle anatomical features missed by human annotators. Our results indicate that while current omnimodels are not yet a universal replacement for specialists, their unique strengths suggest a potential complementary role with specialist models, particularly in enhancing robustness on challenging edge cases. </p>
<blockquote>
<p>é€šç”¨å¤šæ¨¡æ€å…¨åŠŸèƒ½æ¨¡å‹çš„å‡ºç°å¼•å‘äº†å…³é”®é—®é¢˜ï¼šè¿™äº›â€œæ— æ‰€ä¸èƒ½â€çš„ç³»ç»Ÿèƒ½å¦åœ¨çŸ¥è¯†å¯†é›†å‹é¢†åŸŸä¸é«˜åº¦ä¸“ä¸šåŒ–çš„æ¨¡å‹è¡¨ç°ç›¸å½“ï¼Ÿæœ¬ç ”ç©¶åœ¨åŒ»ç–—å›¾åƒåˆ†å‰²è¿™ä¸€é«˜é£é™©é¢†åŸŸæ¢è®¨äº†è¿™ä¸ªé—®é¢˜ã€‚æˆ‘ä»¬å¯¹æœ€å…ˆè¿›çš„å…¨åŠŸèƒ½æ¨¡å‹Geminiï¼ˆâ€œNano Bananaâ€æ¨¡å‹ï¼‰è¿›è¡Œäº†æ¯”è¾ƒç ”ç©¶ï¼Œåˆ†æäº†å…¶åœ¨ä¸‰é¡¹ä¸åŒä»»åŠ¡ä¸Šçš„é›¶æ ·æœ¬æ€§èƒ½è¡¨ç°ï¼Œå¹¶ä¸é’ˆå¯¹ç‰¹å®šé¢†åŸŸçš„æ·±åº¦å­¦ä¹ æ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒï¼šæ¯è‚‰ï¼ˆå†…çª¥é•œï¼‰ã€è§†ç½‘è†œè¡€ç®¡ï¼ˆçœ¼åº•ï¼‰å’Œè‚¿ç˜¤åˆ†å‰²ï¼ˆè¶…å£°ï¼‰ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¾§é‡äºåŸºäºä¸“ä¸šæ¨¡å‹çš„å‡†ç¡®æ€§ç­›é€‰å‡ºçš„â€œæœ€å®¹æ˜“â€å’Œâ€œæœ€å›°éš¾â€æ¡ˆä¾‹å­é›†çš„æ€§èƒ½è¡¨ç°ã€‚ç ”ç©¶å‘ç°äº†ä¸€ä¸ªå¾®å¦™ä¸”ä¾èµ–äºç‰¹å®šä»»åŠ¡çš„æ™¯è±¡ã€‚åœ¨æ¯è‚‰å’Œè‚¿ç˜¤åˆ†å‰²æ–¹é¢ï¼Œä¸“ä¸šæ¨¡å‹åœ¨ç®€å•æ ·æœ¬ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†å…¨åŠŸèƒ½æ¨¡å‹åœ¨å›°éš¾æ ·æœ¬ä¸Šè¡¨ç°å‡ºäº†æ›´å¤§çš„ç¨³å¥æ€§ï¼Œåœ¨è¿™äº›å›°éš¾æ ·æœ¬ä¸Šä¸“ä¸šæ¨¡å‹é­é‡äº†ç¾éš¾æ€§çš„å¤±è´¥ã€‚ç›¸åï¼Œå¯¹äºç²¾ç»†çš„è§†ç½‘è†œè¡€ç®¡åˆ†å‰²ä»»åŠ¡ï¼Œä¸“ä¸šæ¨¡å‹åœ¨ç®€å•å’Œå›°éš¾æ¡ˆä¾‹ä¸­éƒ½ä¿æŒäº†å“è¶Šçš„æ€§èƒ½ã€‚æœ‰è¶£çš„æ˜¯ï¼Œå®šæ€§åˆ†æè¡¨æ˜å…¨åŠŸèƒ½æ¨¡å‹å¯èƒ½å…·æœ‰æ›´é«˜çš„æ•æ„Ÿæ€§ï¼Œèƒ½å¤Ÿè¯†åˆ«å‡ºäººç±»æ³¨é‡Šå™¨é—æ¼çš„å¾®å¦™è§£å‰–ç‰¹å¾ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œè™½ç„¶å½“å‰çš„å…¨åŠŸèƒ½æ¨¡å‹è¿˜æ— æ³•æ™®éå–ä»£ä¸“å®¶ï¼Œä½†å…¶ç‹¬ç‰¹çš„ä¼˜åŠ¿è¡¨æ˜å®ƒä»¬å¯èƒ½ä¸ä¸“å®¶æ¨¡å‹äº’è¡¥ï¼Œç‰¹åˆ«æ˜¯åœ¨æé«˜å¤„ç†å…·æœ‰æŒ‘æˆ˜æ€§çš„è¾¹ç¼˜æ¡ˆä¾‹çš„ç¨³å¥æ€§æ–¹é¢ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.00866v2">PDF</a> 15 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å…¨åŠŸèƒ½omnimodelåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²é¢†åŸŸå†…çš„è¡¨ç°ã€‚ç ”ç©¶é€šè¿‡å¯¹æ¯”å…ˆè¿›çš„omnimodelï¼ˆGeminiæ¨¡å‹ï¼‰ä¸ç‰¹å®šé¢†åŸŸçš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œåœ¨ä¸‰ç§ä¸åŒä»»åŠ¡ä¸Šçš„é›¶æ ·æœ¬æ€§èƒ½è¡¨ç°ï¼Œå‘ç°å…¶åœ¨æç«¯æƒ…å†µä¸‹çš„è¡¨ç°å‘ˆç°å‡ºå¾®å¦™çš„ä»»åŠ¡ä¾èµ–æ€§ã€‚åœ¨æ¯è‚‰å’Œè‚¿ç˜¤åˆ†å‰²æ–¹é¢ï¼Œç‰¹å®šä»»åŠ¡çš„æ¨¡å‹åœ¨ç®€å•æ ·æœ¬ä¸Šè¡¨ç°ä¼˜ç§€ï¼Œè€Œomnimodelåœ¨å¤æ‚æ ·æœ¬ä¸Šè¡¨ç°å‡ºæ›´å¤§çš„ç¨³å¥æ€§ã€‚è€Œåœ¨è§†ç½‘è†œè¡€ç®¡åˆ†å‰²ç­‰ç²¾ç»†ä»»åŠ¡ä¸Šï¼Œç‰¹å®šä»»åŠ¡æ¨¡å‹åœ¨ç®€å•å’Œå¤æ‚æ¡ˆä¾‹ä¸­å‡è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚æ­¤å¤–ï¼Œå®šæ€§åˆ†æè¡¨æ˜omnimodelå¯èƒ½å…·æœ‰æ›´é«˜çš„æ•æ„Ÿæ€§ï¼Œèƒ½å¤Ÿè¯†åˆ«å‡ºäººç±»æ³¨é‡Šå™¨é—æ¼çš„ç»†å¾®è§£å‰–ç‰¹å¾ã€‚å› æ­¤ï¼Œå°½ç®¡omnimodelå°šæœªæˆä¸ºä¸“ä¸šæ¨¡å‹çš„å…¨é¢æ›¿ä»£è€…ï¼Œä½†å…¶ç‹¬ç‰¹ä¼˜åŠ¿è¡¨æ˜å®ƒä»¬å¯èƒ½ä¸ä¸“ä¸šæ¨¡å‹äº’è¡¥ï¼Œç‰¹åˆ«æ˜¯åœ¨æé«˜æŒ‘æˆ˜æ€§è¾¹ç¼˜æ¡ˆä¾‹çš„ç¨³å¥æ€§æ–¹é¢ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å…¨åŠŸèƒ½omnimodelåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²é¢†åŸŸå†…çš„è¡¨ç°è¢«ç ”ç©¶å¹¶å’Œç‰¹å®šé¢†åŸŸçš„æ·±åº¦å­¦ä¹ æ¨¡å‹è¿›è¡Œäº†å¯¹æ¯”ã€‚</li>
<li>åœ¨æ¯è‚‰å’Œè‚¿ç˜¤åˆ†å‰²æ–¹é¢ï¼Œç‰¹å®šä»»åŠ¡çš„æ¨¡å‹åœ¨ç®€å•æ ·æœ¬ä¸Šè¡¨ç°ä¼˜ç§€ï¼Œè€Œomnimodelåœ¨å¤æ‚æ ·æœ¬ä¸Šå±•ç°å‡ºæ›´å¤§çš„ç¨³å¥æ€§ã€‚</li>
<li>åœ¨è§†ç½‘è†œè¡€ç®¡åˆ†å‰²ç­‰ç²¾ç»†ä»»åŠ¡ä¸Šï¼Œç‰¹å®šä»»åŠ¡æ¨¡å‹è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œomnimodelç›¸å¯¹è¾ƒå¼±ã€‚</li>
<li>omnimodelå…·æœ‰æ›´é«˜çš„æ•æ„Ÿæ€§ï¼Œèƒ½å¤Ÿè¯†åˆ«å‡ºäººç±»æ³¨é‡Šå™¨é—æ¼çš„ç»†å¾®è§£å‰–ç‰¹å¾ã€‚</li>
<li>omnimodelå°šæœªæˆä¸ºä¸“ä¸šæ¨¡å‹çš„å…¨é¢æ›¿ä»£è€…ï¼Œä½†åœ¨æŸäº›æƒ…å†µä¸‹å¯ä»¥ä¸å…¶äº’è¡¥ã€‚</li>
<li>åœ¨å¤„ç†æŒ‘æˆ˜æ€§è¾¹ç¼˜æ¡ˆä¾‹æ—¶ï¼Œomnimodelçš„ç¨³å¥æ€§æ½œåŠ›å¾—åˆ°äº†çªæ˜¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.00866">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2509.00866v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2509.00866v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2509.00866v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="CADDesigner-Conceptual-Design-of-CAD-Models-Based-on-General-Purpose-Agent"><a href="#CADDesigner-Conceptual-Design-of-CAD-Models-Based-on-General-Purpose-Agent" class="headerlink" title="CADDesigner: Conceptual Design of CAD Models Based on General-Purpose   Agent"></a>CADDesigner: Conceptual Design of CAD Models Based on General-Purpose   Agent</h2><p><strong>Authors:Jingzhe Ni, Xiaolong Yin, Xingyu Lu, Xintong Li, Ji Wei, Ruofeng Tong, Min Tang, Peng Du</strong></p>
<p>Computer-Aided Design (CAD) plays a pivotal role in industrial manufacturing but typically requires a high level of expertise from designers. To lower the entry barrier and improve design efficiency, we present an agent for CAD conceptual design powered by large language models (LLMs). The agent accepts both abstract textual descriptions and freehand sketches as input, engaging in interactive dialogue with users to refine and clarify design requirements through comprehensive requirement analysis. Built upon a novel Context-Independent Imperative Paradigm (CIP), the agent generates high-quality CAD modeling code. During the generation process, the agent incorporates iterative visual feedback to improve model quality. Generated design cases are stored in a structured knowledge base, enabling continuous improvement of the agentâ€™s code generation capabilities. Experimental results demonstrate that our method achieves state-of-the-art performance in CAD code generation. </p>
<blockquote>
<p>è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰åœ¨å·¥ä¸šåˆ¶é€ ä¸­æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ï¼Œé€šå¸¸éœ€è¦è®¾è®¡å¸ˆå…·å¤‡é«˜æ°´å¹³çš„ä¸“ä¸šçŸ¥è¯†ã€‚ä¸ºäº†é™ä½å…¥é—¨é—¨æ§›å¹¶æé«˜è®¾è®¡æ•ˆç‡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”±å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„CADæ¦‚å¿µè®¾è®¡ä»£ç†ã€‚è¯¥ä»£ç†æ¥å—æŠ½è±¡çš„æ–‡æœ¬æè¿°å’Œæ‰‹ç»˜è‰å›¾ä½œä¸ºè¾“å…¥ï¼Œé€šè¿‡å…¨é¢çš„éœ€æ±‚åˆ†æä¸ç”¨æˆ·è¿›è¡Œäº’åŠ¨å¯¹è¯ï¼Œä»¥ç»†åŒ–å’Œæ˜ç¡®è®¾è®¡è¦æ±‚ã€‚ä»£ç†å»ºç«‹åœ¨æ–°å‹ä¸Šä¸‹æ–‡ç‹¬ç«‹æŒ‡ä»¤èŒƒå¼ï¼ˆCIPï¼‰ä¹‹ä¸Šï¼Œå¯ç”Ÿæˆé«˜è´¨é‡çš„CADå»ºæ¨¡ä»£ç ã€‚åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œä»£ç†ä¼šç»“åˆè¿­ä»£è§†è§‰åé¦ˆæ¥æé«˜æ¨¡å‹è´¨é‡ã€‚ç”Ÿæˆçš„è®¾è®¡æ¡ˆä¾‹å­˜å‚¨åœ¨ç»“æ„åŒ–çŸ¥è¯†åº“ä¸­ï¼Œèƒ½å¤ŸæŒç»­æ”¹å–„ä»£ç†çš„ä»£ç ç”Ÿæˆèƒ½åŠ›ã€‚å®éªŒç»“æœè¯æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨CADä»£ç ç”Ÿæˆæ–¹é¢è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01031v3">PDF</a> The theoretical proof of Context-Independent Imperative Paradigm is   flawed; I request withdrawal of the manuscript</p>
<p><strong>Summary</strong></p>
<p>åŸºäºè®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰åœ¨å·¥ä¸šåˆ¶é€ ä¸­çš„é‡è¦æ€§ï¼Œä»¥åŠè®¾è®¡å¸ˆæ‰€éœ€çš„é«˜æ°´å¹³ä¸“ä¸šçŸ¥è¯†ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”±å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„CADæ¦‚å¿µè®¾è®¡ä»£ç†ã€‚è¯¥ä»£ç†æ¥å—æŠ½è±¡æ–‡æœ¬æè¿°å’Œæ‰‹ç»˜è‰å›¾ä½œä¸ºè¾“å…¥ï¼Œé€šè¿‡å…¨é¢çš„éœ€æ±‚åˆ†æä¸ç”¨æˆ·è¿›è¡Œäº¤äº’å¯¹è¯ï¼Œä»¥æ¾„æ¸…å’Œç»†åŒ–è®¾è®¡è¦æ±‚ã€‚å®ƒé‡‡ç”¨æ–°é¢–çš„Context-Independent Imperative Paradigmï¼ˆCIPï¼‰ç”Ÿæˆé«˜è´¨é‡çš„CADå»ºæ¨¡ä»£ç ã€‚åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œä»£ç†ä¼šç»“åˆè¿­ä»£è§†è§‰åé¦ˆæ¥æé«˜æ¨¡å‹è´¨é‡ã€‚ç”Ÿæˆçš„æ¡ˆä¾‹è¢«å­˜å‚¨åœ¨ç»“æ„åŒ–çŸ¥è¯†åº“ä¸­ï¼Œä½¿ä»£ç†çš„ä»£ç ç”Ÿæˆèƒ½åŠ›å¾—ä»¥æŒç»­æ”¹è¿›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨CADä»£ç ç”Ÿæˆæ–¹é¢è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥ä»£ç†ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æŠ€æœ¯ä¸ºè®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰æä¾›æ”¯æŒã€‚</li>
<li>ä»£ç†æ¥å—æŠ½è±¡æ–‡æœ¬æè¿°å’Œæ‰‹ç»˜è‰å›¾ä½œä¸ºè¾“å…¥ï¼Œä¾¿äºç”¨æˆ·æ“ä½œã€‚</li>
<li>é€šè¿‡å…¨é¢çš„éœ€æ±‚åˆ†æï¼Œä»£ç†èƒ½ä¸ç”¨æˆ·è¿›è¡Œäº¤äº’å¯¹è¯ä»¥æ¾„æ¸…å’Œç»†åŒ–è®¾è®¡è¦æ±‚ã€‚</li>
<li>é‡‡ç”¨æ–°é¢–çš„Context-Independent Imperative Paradigmï¼ˆCIPï¼‰ç”Ÿæˆé«˜è´¨é‡çš„CADå»ºæ¨¡ä»£ç ã€‚</li>
<li>åœ¨ç”ŸæˆCADå»ºæ¨¡ä»£ç çš„è¿‡ç¨‹ä¸­ï¼Œä»£ç†ä¼šç»“åˆè¿­ä»£è§†è§‰åé¦ˆæ¥æé«˜æ¨¡å‹è´¨é‡ã€‚</li>
<li>ç”Ÿæˆçš„æ¡ˆä¾‹è¢«å­˜å‚¨åœ¨ç»“æ„åŒ–çŸ¥è¯†åº“ä¸­ï¼Œä½¿ä»£ç†èƒ½å¤ŸæŒç»­æ”¹è¿›å…¶ä»£ç ç”Ÿæˆèƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01031">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2508.01031v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2508.01031v3/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2508.01031v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2508.01031v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="MIRAGE-Multimodal-foundation-model-and-benchmark-for-comprehensive-retinal-OCT-image-analysis"><a href="#MIRAGE-Multimodal-foundation-model-and-benchmark-for-comprehensive-retinal-OCT-image-analysis" class="headerlink" title="MIRAGE: Multimodal foundation model and benchmark for comprehensive   retinal OCT image analysis"></a>MIRAGE: Multimodal foundation model and benchmark for comprehensive   retinal OCT image analysis</h2><p><strong>Authors:JosÃ© Morano, Botond Fazekas, Emese SÃ¼kei, Ronald Fecso, Taha Emre, Markus Gumpinger, Georg Faustmann, Marzieh Oghbaie, Ursula Schmidt-Erfurth, Hrvoje BogunoviÄ‡</strong></p>
<p>Artificial intelligence (AI) has become a fundamental tool for assisting clinicians in analyzing ophthalmic images, such as optical coherence tomography (OCT). However, developing AI models often requires extensive annotation, and existing models tend to underperform on independent, unseen data. Foundation models (FMs), large AI models trained on vast unlabeled datasets, have shown promise in overcoming these challenges. Nonetheless, available FMs for ophthalmology lack extensive validation, especially for segmentation tasks, and focus on a single imaging modality. In this context, we propose MIRAGE, a novel multimodal FM for the analysis of OCT and scanning laser ophthalmoscopy (SLO) images. Additionally, we propose a new evaluation benchmark with OCT&#x2F;SLO classification and segmentation tasks. The comparison with general and specialized FMs and segmentation methods shows the superiority of MIRAGE in both types of tasks, highlighting its suitability as a basis for the development of robust AI systems for retinal OCT image analysis. Both MIRAGE and the evaluation benchmark are publicly available: <a target="_blank" rel="noopener" href="https://github.com/j-morano/MIRAGE">https://github.com/j-morano/MIRAGE</a>. </p>
<blockquote>
<p>äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰å·²ç»æˆä¸ºååŠ©ä¸´åºŠåŒ»ç”Ÿåˆ†æçœ¼ç§‘å›¾åƒçš„é‡è¦å·¥å…·ï¼Œå¦‚å…‰å­¦ç›¸å¹²æ–­å±‚æ‰«æï¼ˆOCTï¼‰ã€‚ç„¶è€Œï¼Œå¼€å‘AIæ¨¡å‹é€šå¸¸éœ€è¦å¤§é‡çš„æ ‡æ³¨ï¼Œç°æœ‰æ¨¡å‹åœ¨ç‹¬ç«‹ã€æœªè§æ•°æ®ä¸Šçš„è¡¨ç°å¾€å¾€ä¸ä½³ã€‚åŸºç¡€æ¨¡å‹ï¼ˆFMsï¼‰æ˜¯åœ¨å¤§é‡æ— æ ‡ç­¾æ•°æ®é›†ä¸Šè®­ç»ƒçš„åºå¤§AIæ¨¡å‹ï¼Œåœ¨å…‹æœè¿™äº›æŒ‘æˆ˜æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„çœ¼ç§‘åŸºç¡€æ¨¡å‹ç¼ºä¹å¹¿æ³›çš„éªŒè¯ï¼Œå°¤å…¶æ˜¯åœ¨åˆ†å‰²ä»»åŠ¡æ–¹é¢ï¼Œè€Œä¸”ä¸“æ³¨äºå•ä¸€çš„æˆåƒæ¨¡å¼ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬æå‡ºäº†MIRAGEï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºåˆ†æOCTå’Œæ‰«ææ¿€å…‰çœ¼ç§‘æ£€æŸ¥ï¼ˆSLOï¼‰å›¾åƒçš„æ–°å‹å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ä¸ªæ–°çš„è¯„ä¼°åŸºå‡†ï¼ŒåŒ…æ‹¬OCT&#x2F;SLOåˆ†ç±»å’Œåˆ†å‰²ä»»åŠ¡ã€‚ä¸é€šç”¨å’ŒåŸºç¡€æ¨¡å‹ä»¥åŠåˆ†å‰²æ–¹æ³•çš„æ¯”è¾ƒè¡¨æ˜ï¼ŒMIRAGEåœ¨è¿™ä¸¤ç§ç±»å‹çš„ä»»åŠ¡ä¸­éƒ½è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶ä½œä¸ºå¼€å‘ç”¨äºè§†ç½‘è†œOCTå›¾åƒåˆ†æçš„ç¨³å¥AIç³»ç»Ÿçš„åŸºç¡€çš„é€‚ç”¨æ€§ã€‚MIRAGEå’Œè¯„ä¼°åŸºå‡†å‡å¯å…¬å¼€è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/j-morano/MIRAGE%E3%80%82">https://github.com/j-morano/MIRAGEã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08900v3">PDF</a> Accepted for publication in npj Digital Medicine</p>
<p><strong>Summary</strong></p>
<p>AIåœ¨çœ¼ç§‘å›¾åƒåˆ†æä¸­çš„åº”ç”¨æ—¥ç›Šæ™®åŠï¼Œå°¤å…¶åœ¨å…‰å­¦ç›¸å¹²æ–­å±‚æ‰«æï¼ˆOCTï¼‰æ–¹é¢ã€‚ç„¶è€Œï¼Œå¼€å‘AIæ¨¡å‹éœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®ï¼Œç°æœ‰æ¨¡å‹åœ¨æ–°æ•°æ®ä¸Šè¡¨ç°ä¸ä½³ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œç ”ç©¶è€…æå‡ºäº†åŸºç¡€æ¨¡å‹ï¼ˆFMsï¼‰ï¼Œè¿™äº›å¤§å‹AIæ¨¡å‹åœ¨å¤§é‡æ— æ ‡ç­¾æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒã€‚å°½ç®¡å¦‚æ­¤ï¼Œç°æœ‰çš„çœ¼ç§‘åŸºç¡€æ¨¡å‹åœ¨éªŒè¯æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯åœ¨åˆ†å‰²ä»»åŠ¡ä¸Šï¼Œä¸”ä¸»è¦å…³æ³¨å•ä¸€æˆåƒæ¨¡å¼ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…æå‡ºäº†MIRAGEè¿™ä¸€æ–°å‹çš„å¤šæ¨¡å¼åŸºç¡€æ¨¡å‹ï¼Œç”¨äºåˆ†æOCTå’Œæ‰«ææ¿€å…‰çœ¼ç§‘æ£€æŸ¥ï¼ˆSLOï¼‰å›¾åƒã€‚æ­¤å¤–ï¼Œä»–ä»¬è¿˜å»ºç«‹äº†ä¸€ä¸ªæ–°çš„è¯„ä¼°åŸºå‡†ï¼ŒåŒ…æ‹¬OCT&#x2F;SLOåˆ†ç±»å’Œåˆ†å‰²ä»»åŠ¡ã€‚ä¸é€šç”¨å’ŒåŸºç¡€æ¨¡å‹åŠåˆ†å‰²æ–¹æ³•çš„æ¯”è¾ƒæ˜¾ç¤ºï¼ŒMIRAGEåœ¨ä¸¤ç±»ä»»åŠ¡ä¸­éƒ½è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œé€‚åˆä½œä¸ºå¼€å‘ç¨³å¥çš„è§†ç½‘è†œOCTå›¾åƒåˆ†æAIç³»ç»Ÿçš„åŸºç¡€ã€‚MIRAGEå’Œè¯„ä¼°åŸºå‡†å‡å·²å…¬å¼€å¯ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AIå·²æˆä¸ºçœ¼ç§‘å›¾åƒåˆ†æçš„é‡è¦å·¥å…·ï¼Œç‰¹åˆ«æ˜¯åœ¨OCTæ–¹é¢ã€‚</li>
<li>å¼€å‘AIæ¨¡å‹éœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®ï¼Œç°æœ‰æ¨¡å‹åœ¨æ–°æ•°æ®ä¸Šè¡¨ç°ä¸ç¨³å®šã€‚</li>
<li>åŸºç¡€æ¨¡å‹ï¼ˆFMsï¼‰åœ¨è§£å†³è¿™äº›é—®é¢˜æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ã€‚</li>
<li>ç°æœ‰çš„çœ¼ç§‘åŸºç¡€æ¨¡å‹åœ¨éªŒè¯æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šæ¨¡å¼å›¾åƒåˆ†ææ–¹é¢ã€‚</li>
<li>MIRAGEæ˜¯ä¸€ç§æ–°å‹çš„å¤šæ¨¡å¼åŸºç¡€æ¨¡å‹ï¼Œå¯ç”¨äºåˆ†æOCTå’ŒSLOå›¾åƒã€‚</li>
<li>MIRAGEåœ¨åˆ†ç±»å’Œåˆ†å‰²ä»»åŠ¡ä¸­éƒ½è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08900">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2506.08900v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2506.08900v3/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2506.08900v3/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2506.08900v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Text-to-CT-Generation-via-3D-Latent-Diffusion-Model-with-Contrastive-Vision-Language-Pretraining"><a href="#Text-to-CT-Generation-via-3D-Latent-Diffusion-Model-with-Contrastive-Vision-Language-Pretraining" class="headerlink" title="Text-to-CT Generation via 3D Latent Diffusion Model with Contrastive   Vision-Language Pretraining"></a>Text-to-CT Generation via 3D Latent Diffusion Model with Contrastive   Vision-Language Pretraining</h2><p><strong>Authors:Daniele Molino, Camillo Maria Caruso, Filippo Ruffini, Paolo Soda, Valerio Guarrasi</strong></p>
<p>Objective: While recent advances in text-conditioned generative models have enabled the synthesis of realistic medical images, progress has been largely confined to 2D modalities such as chest X-rays. Extending text-to-image generation to volumetric CT remains a significant challenge, due to its high dimensionality, anatomical complexity, and the absence of robust frameworks that align vision-language data in 3D medical imaging. Methods: We introduce a novel architecture for Text-to-CT generation that combines a latent diffusion model with a 3D contrastive vision-language pretraining scheme. Our approach leverages a dual-encoder CLIP-style model trained on paired CT volumes and radiology reports to establish a shared embedding space, which serves as the conditioning input for generation. CT volumes are compressed into a low-dimensional latent space via a pretrained volumetric VAE, enabling efficient 3D denoising diffusion without requiring external super-resolution stages. Results: We evaluate our method on the CT-RATE dataset and conduct a comprehensive assessment of image fidelity, clinical relevance, and semantic alignment. Our model achieves competitive performance across all tasks, significantly outperforming prior baselines for text-to-CT generation. Moreover, we demonstrate that CT scans synthesized by our framework can effectively augment real data, improving downstream diagnostic performance. Conclusion: Our results show that modality-specific vision-language alignment is a key component for high-quality 3D medical image generation. By integrating contrastive pretraining and volumetric diffusion, our method offers a scalable and controllable solution for synthesizing clinically meaningful CT volumes from text, paving the way for new applications in data augmentation, medical education, and automated clinical simulation. Code at <a target="_blank" rel="noopener" href="https://github.com/cosbidev/Text2CT">https://github.com/cosbidev/Text2CT</a>. </p>
<blockquote>
<p>ç›®æ ‡ï¼šå°½ç®¡è¿‘æœŸæ–‡æœ¬æ¡ä»¶ç”Ÿæˆæ¨¡å‹å–å¾—äº†è¿›å±•ï¼Œä½¿å¾—åˆæˆé€¼çœŸçš„åŒ»å­¦å›¾åƒæˆä¸ºå¯èƒ½ï¼Œä½†è¿›å±•ä¸»è¦å±€é™äºå¦‚èƒ¸éƒ¨Xå°„çº¿ç­‰2Dæ¨¡å¼ã€‚å°†æ–‡æœ¬åˆ°å›¾åƒçš„ç”Ÿæˆæ‰©å±•åˆ°ä½“ç§¯CTä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼Œè¿™æ˜¯ç”±äºå…¶é«˜ç»´åº¦ã€è§£å‰–ç»“æ„å¤æ‚ï¼Œä»¥åŠç¼ºä¹èƒ½å¤Ÿåœ¨3DåŒ»å­¦å›¾åƒä¸­å¯¹é½è§†è§‰è¯­è¨€æ•°æ®çš„ç¨³å¥æ¡†æ¶ã€‚æ–¹æ³•ï¼šæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§ç”¨äºæ–‡æœ¬åˆ°CTç”Ÿæˆçš„æ–°å‹æ¶æ„ï¼Œè¯¥æ¶æ„ç»“åˆäº†æ½œåœ¨æ‰©æ•£æ¨¡å‹ä¸3Då¯¹æ¯”è§†è§‰è¯­è¨€é¢„è®­ç»ƒæ–¹æ¡ˆã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨åœ¨é…å¯¹CTä½“ç§¯å’Œæ”¾å°„å­¦æŠ¥å‘Šä¸Šè®­ç»ƒçš„åŒé‡ç¼–ç å™¨CLIPé£æ ¼æ¨¡å‹ï¼Œå»ºç«‹ä¸€ä¸ªå…±äº«åµŒå…¥ç©ºé—´ï¼Œä½œä¸ºç”Ÿæˆçš„æ¡ä»¶è¾“å…¥ã€‚CTä½“ç§¯é€šè¿‡é¢„è®­ç»ƒçš„ä½“ç§¯VAEå‹ç¼©åˆ°ä½ç»´æ½œåœ¨ç©ºé—´ï¼Œä»è€Œå®ç°é«˜æ•ˆçš„3Då»å™ªæ‰©æ•£ï¼Œè€Œæ— éœ€å¤–éƒ¨è¶…åˆ†è¾¨ç‡é˜¶æ®µã€‚ç»“æœï¼šæˆ‘ä»¬åœ¨CT-RATEæ•°æ®é›†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå¹¶å¯¹å›¾åƒä¿çœŸåº¦ã€ä¸´åºŠç›¸å…³æ€§å’Œè¯­ä¹‰å¯¹é½è¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨æ‰€æœ‰ä»»åŠ¡ä¸Šéƒ½è¡¨ç°å‡ºç«äº‰åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨æ–‡æœ¬åˆ°CTç”Ÿæˆæ–¹é¢æ˜¾è‘—è¶…è¶Šäº†å…ˆå‰çš„åŸºå‡†æµ‹è¯•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬æ¡†æ¶åˆæˆçš„CTæ‰«æå¯ä»¥æœ‰æ•ˆåœ°å¢å¼ºçœŸå®æ•°æ®ï¼Œæé«˜ä¸‹æ¸¸è¯Šæ–­æ€§èƒ½ã€‚ç»“è®ºï¼šæˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œç‰¹å®šæ¨¡æ€çš„è§†è§‰è¯­è¨€å¯¹é½æ˜¯é«˜è´¨é‡3DåŒ»å­¦å›¾åƒç”Ÿæˆçš„å…³é”®ç»„æˆéƒ¨åˆ†ã€‚é€šè¿‡é›†æˆå¯¹æ¯”é¢„è®­ç»ƒå’Œä½“ç§¯æ‰©æ•£ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æä¾›äº†ä¸€ç§å¯æ‰©å±•å’Œå¯æ§çš„è§£å†³æ–¹æ¡ˆï¼Œå¯ä»¥æ ¹æ®æ–‡æœ¬åˆæˆå…·æœ‰ä¸´åºŠæ„ä¹‰çš„CTä½“ç§¯ï¼Œä¸ºæ•°æ®å¢å¼ºã€åŒ»å­¦æ•™è‚²å’Œè‡ªåŠ¨åŒ–ä¸´åºŠæ¨¡æ‹Ÿç­‰é¢†åŸŸå¼€è¾Ÿæ–°çš„åº”ç”¨é€”å¾„ã€‚ä»£ç åœ°å€ï¼š<a target="_blank" rel="noopener" href="https://github.com/cosbidev/Text2CT%E3%80%82">https://github.com/cosbidev/Text2CTã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.00633v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§å°†æ–‡æœ¬è½¬åŒ–ä¸ºCTå›¾åƒçš„æ–°æ–¹æ³•ï¼Œç»“åˆäº†æ½œåœ¨æ‰©æ•£æ¨¡å‹å’Œä¸‰ç»´å¯¹æ¯”è§†è§‰è¯­è¨€é¢„è®­ç»ƒæ–¹æ¡ˆã€‚è¯¥æ–¹æ³•ä½¿ç”¨åœ¨CTä½“ç§¯å’Œæ”¾å°„å­¦æŠ¥å‘Šä¸Šè®­ç»ƒçš„åŒé‡ç¼–ç å™¨CLIPé£æ ¼æ¨¡å‹ï¼Œå»ºç«‹å…±äº«åµŒå…¥ç©ºé—´ï¼Œä½œä¸ºç”Ÿæˆçš„æ¡ä»¶è¾“å…¥ã€‚é€šè¿‡é¢„è®­ç»ƒçš„ä½“ç§¯VAEå°†CTä½“ç§¯å‹ç¼©åˆ°ä½ç»´æ½œåœ¨ç©ºé—´ï¼Œå®ç°äº†é«˜æ•ˆçš„3Dé™å™ªæ‰©æ•£ï¼Œæ— éœ€å¤–éƒ¨è¶…åˆ†è¾¨ç‡é˜¶æ®µã€‚è¯¥æ–¹æ³•åœ¨CT-RATEæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå›¾åƒä¿çœŸåº¦ã€ä¸´åºŠç›¸å…³æ€§å’Œè¯­ä¹‰å¯¹é½å‡ä¼˜äºå…ˆå‰åŸºçº¿ã€‚åˆæˆçš„CTæ‰«æå¯æœ‰æ•ˆåœ°æ‰©å……çœŸå®æ•°æ®ï¼Œæé«˜ä¸‹æ¸¸è¯Šæ–­æ€§èƒ½ã€‚æœ¬ç ”ç©¶ä¸ºé«˜è´¨é‡çš„ä¸‰ç»´åŒ»å­¦å›¾åƒç”Ÿæˆæä¾›äº†ä¸€ç§å¯æ‰©å±•å’Œå¯æ§çš„è§£å†³æ–¹æ¡ˆï¼Œå…·æœ‰æ•°æ®å¢å¼ºã€åŒ»å­¦æ•™è‚²å’Œè‡ªåŠ¨åŒ–ä¸´åºŠæ¨¡æ‹Ÿç­‰æ½œåœ¨åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬è½¬åŒ–ä¸ºCTå›¾åƒçš„æ–¹æ³•ç»“åˆäº†æ½œåœ¨æ‰©æ•£æ¨¡å‹å’Œä¸‰ç»´å¯¹æ¯”è§†è§‰è¯­è¨€é¢„è®­ç»ƒæ–¹æ¡ˆï¼Œåº”å¯¹é«˜ç»´åº¦å’Œå¤æ‚çš„è§£å‰–ç»“æ„æŒ‘æˆ˜ã€‚</li>
<li>ä½¿ç”¨åŒé‡ç¼–ç å™¨CLIPé£æ ¼æ¨¡å‹ï¼ŒåŸºäºé…å¯¹çš„CTä½“ç§¯å’Œæ”¾å°„å­¦æŠ¥å‘Šå»ºç«‹å…±äº«åµŒå…¥ç©ºé—´ï¼Œä¸ºç”Ÿæˆæä¾›æ¡ä»¶è¾“å…¥ã€‚</li>
<li>é€šè¿‡é¢„è®­ç»ƒçš„ä½“ç§¯VAEå‹ç¼©CTä½“ç§¯è‡³ä½ç»´æ½œåœ¨ç©ºé—´ï¼Œå®ç°é«˜æ•ˆçš„3Dé™å™ªæ‰©æ•£ã€‚</li>
<li>åœ¨CT-RATEæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå›¾åƒä¿çœŸåº¦ã€ä¸´åºŠç›¸å…³æ€§å’Œè¯­ä¹‰å¯¹é½å‡ä¼˜äºå…ˆå‰æ–¹æ³•ã€‚</li>
<li>åˆæˆçš„CTæ‰«æå¯æ‰©å……çœŸå®æ•°æ®ï¼Œæé«˜ä¸‹æ¸¸è¯Šæ–­æ€§èƒ½ã€‚</li>
<li>æ–¹æ³•å…·æœ‰å¯æ‰©å±•æ€§å’Œå¯æ§æ€§ï¼Œä¸ºé«˜è´¨é‡çš„ä¸‰ç»´åŒ»å­¦å›¾åƒç”Ÿæˆæä¾›äº†è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.00633">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2506.00633v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2506.00633v2/page_3_0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="GOUHFI-a-novel-contrast-and-resolution-agnostic-segmentation-tool-for-Ultra-High-Field-MRI"><a href="#GOUHFI-a-novel-contrast-and-resolution-agnostic-segmentation-tool-for-Ultra-High-Field-MRI" class="headerlink" title="GOUHFI: a novel contrast- and resolution-agnostic segmentation tool for   Ultra-High Field MRI"></a>GOUHFI: a novel contrast- and resolution-agnostic segmentation tool for   Ultra-High Field MRI</h2><p><strong>Authors:Marc-Antoine Fortin, Anne Louise Kristoffersen, Michael Staff Larsen, Laurent Lamalle, Ruediger Stirnberg, Paal Erik Goa</strong></p>
<p>Recently, Ultra-High Field MRI (UHF-MRI) has become more available and one of the best tools to study the brain. One common step in quantitative neuroimaging is to segment the brain into several regions, which has been done using software packages like FreeSurfer , FastSurferVINN or SynthSeg. However, the differences between UHF-MRI and 1.5T or 3T images are such that the automatic segmentation techniques optimized at these field strengths usually produce unsatisfactory segmentation results for UHF images. Thus, it has been particularly challenging to perform region-based quantitative analyses as typically done with 1.5-3T data, underscoring the crucial need for developing new automatic segmentation techniques designed to handle UHF images. Hence, we propose a novel Deep Learning (DL)-based segmentation technique called GOUHFI: Generalized and Optimized segmentation tool for Ultra-High Field Images, designed to segment UHF images of various contrasts and resolutions. For training, we used a total of 206 label maps from datasets acquired at 3T, 7T and 9.4T. In contrast to most DL strategies, we used a domain randomization approach, where synthetic images were used to train a 3D U-Net. GOUHFI was tested on seven different datasets and compared to existing techniques like FastSurferVINN,SynthSeg and CEREBRUM-7T. GOUHFI was able to segment the six contrasts and seven resolutions tested at 3T, 7T and 9.4T. Average Dice scores of 0.90, 0.90 and 0.93 were computed against the ground truth segmentations at 3T, 7T and 9.4T, respectively. Ultimately, GOUHFI is a promising new segmentation tool, being the first of its kind proposing a contrast- and resolution-agnostic alternative for UHF-MRI without requiring fine-tuning or retraining, making it the forthcoming alternative for neuroscientists working with UHF-MRI or even lower field strengths. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œè¶…é«˜åœºç£å…±æŒ¯æˆåƒï¼ˆUHF-MRIï¼‰è¶Šæ¥è¶Šæ™®åŠï¼Œæˆä¸ºç ”ç©¶å¤§è„‘çš„æœ€ä½³å·¥å…·ä¹‹ä¸€ã€‚å®šé‡ç¥ç»æˆåƒä¸­çš„ä¸€ä¸ªå¸¸è§æ­¥éª¤æ˜¯å°†å¤§è„‘åˆ†å‰²æˆå‡ ä¸ªåŒºåŸŸï¼Œè¿™å·²é€šè¿‡è½¯ä»¶åŒ…å¦‚FreeSurferã€FastSurferVINNæˆ–SynthSegå®Œæˆã€‚ç„¶è€Œï¼ŒUHF-MRIä¸1.5Tæˆ–3Tå›¾åƒä¹‹é—´çš„å·®å¼‚ä½¿å¾—åœ¨è¿™äº›åœºå¼ºä¸‹ä¼˜åŒ–çš„è‡ªåŠ¨åˆ†å‰²æŠ€æœ¯é€šå¸¸ä¼šäº§ç”Ÿä»¤äººä¸æ»¡æ„çš„UHFå›¾åƒåˆ†å‰²ç»“æœã€‚å› æ­¤ï¼Œåƒé€šå¸¸ä½¿ç”¨1.5-3Tæ•°æ®é‚£æ ·è¿›è¡ŒåŸºäºåŒºåŸŸçš„å®šé‡åˆ†æå…·æœ‰ç‰¹åˆ«å¤§çš„æŒ‘æˆ˜æ€§ï¼Œè¿™å¼ºè°ƒäº†å¯¹å¼€å‘ä¸“é—¨å¤„ç†UHFå›¾åƒçš„æ–°å‹è‡ªåŠ¨åˆ†å‰²æŠ€æœ¯çš„è¿«åˆ‡éœ€æ±‚ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰çš„æ–°çš„åˆ†å‰²æŠ€æœ¯ï¼Œç§°ä¸ºGOUHFIï¼šç”¨äºè¶…é«˜åœºå›¾åƒçš„é€šç”¨å’Œä¼˜åŒ–åˆ†å‰²å·¥å…·ï¼Œæ—¨åœ¨åˆ†å‰²å…·æœ‰å„ç§å¯¹æ¯”åº¦å’Œåˆ†è¾¨ç‡çš„UHFå›¾åƒã€‚ä¸ºäº†è®­ç»ƒï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ä»3Tã€7Tå’Œ9.4Té‡‡é›†çš„æ•°æ®é›†çš„æ€»å…±206ä¸ªæ ‡ç­¾å›¾ã€‚ä¸å¤§å¤šæ•°æ·±åº¦å­¦ä¹ ç­–ç•¥ä¸åŒï¼Œæˆ‘ä»¬é‡‡ç”¨äº†é¢†åŸŸéšæœºåŒ–æ–¹æ³•ï¼Œä½¿ç”¨åˆæˆå›¾åƒæ¥è®­ç»ƒ3D U-Netã€‚GOUHFIåœ¨ä¸ƒä¸ªä¸åŒçš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†æµ‹è¯•ï¼Œå¹¶ä¸ç°æœ‰çš„æŠ€æœ¯å¦‚FastSurferVINNã€SynthSegå’ŒCEREBRUM-7Tè¿›è¡Œäº†æ¯”è¾ƒã€‚GOUHFIèƒ½å¤Ÿåœ¨3Tã€7Tå’Œ9.4Tæµ‹è¯•çš„å…­ä¸ªå¯¹æ¯”åº¦å’Œä¸ƒä¸ªåˆ†è¾¨ç‡ä¸Šè¿›è¡Œåˆ†å‰²ã€‚ç›¸å¯¹äºåœ¨3Tã€7Tå’Œ9.4Tçš„åŸºå‡†åˆ†å‰²ï¼Œå…¶å¹³å‡Diceå¾—åˆ†åˆ†åˆ«ä¸º0.90ã€0.90å’Œ0.93ã€‚æœ€ç»ˆï¼ŒGOUHFIæ˜¯ä¸€ä¸ªå¾ˆæœ‰å‰é€”çš„æ–°åˆ†å‰²å·¥å…·ï¼Œå®ƒæ˜¯ç¬¬ä¸€ä¸ªæå‡ºä¸€ç§å¯¹æ¯”åº¦å’Œåˆ†è¾¨ç‡ä¸å˜çš„æ›¿ä»£æ–¹æ¡ˆï¼Œé€‚ç”¨äºUHF-MRIè€Œæ— éœ€å¾®è°ƒæˆ–é‡æ–°è®­ç»ƒï¼Œä½¿å…¶æˆä¸ºä»äº‹UHF-MRIæˆ–ç”šè‡³è¾ƒä½åœºå¼ºçš„ç¥ç»ç§‘å­¦å®¶ä»¬çš„æœªæ¥é¦–é€‰å·¥å…·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11445v2">PDF</a> 51 pages, 10 Figures, 7 Tables, Accepted for publication to Imaging   Neuroscience after being peer-reviewed on 29-09-25</p>
<p><strong>Summary</strong></p>
<p>ä¸€ç§å…¨æ–°çš„åŸºäºæ·±åº¦å­¦ä¹ çš„è¶…é«˜é¢‘MRIå›¾åƒåˆ†å‰²å·¥å…·â€”â€”GOUHFIï¼Œè®¾è®¡ç”¨äºåˆ†å‰²å…·æœ‰ä¸åŒå¯¹æ¯”åº¦å’Œåˆ†è¾¨ç‡çš„è¶…é«˜é¢‘å›¾åƒã€‚è¯¥æ–¹æ³•é€šè¿‡åˆæˆå›¾åƒè®­ç»ƒ3D U-Netç½‘ç»œï¼Œå…·æœ‰å¯¹å¯¹æ¯”åº¦ä¸åˆ†è¾¨ç‡çš„é€šç”¨æ€§ï¼Œæ— éœ€å¾®è°ƒæˆ–é‡æ–°è®­ç»ƒã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„æµ‹è¯•è¡¨æ˜ï¼Œå…¶åˆ†å‰²æ•ˆæœè‰¯å¥½ï¼Œå¹³å‡Diceå¾—åˆ†è¾ƒé«˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UHF-MRIæˆä¸ºç ”ç©¶å¤§è„‘çš„æœ€ä½³å·¥å…·ä¹‹ä¸€ï¼Œä½†è‡ªåŠ¨åˆ†å‰²æŠ€æœ¯åœ¨è¶…é«˜é¢‘å›¾åƒä¸Šçš„åº”ç”¨å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„åŸºäºæ·±åº¦å­¦ä¹ çš„åˆ†å‰²å·¥å…·GOUHFIï¼Œé€‚ç”¨äºè¶…é«˜é¢‘å›¾åƒï¼Œå¯å¤„ç†ä¸åŒå¯¹æ¯”åº¦å’Œåˆ†è¾¨ç‡çš„å›¾åƒã€‚</li>
<li>GOUHFIé€šè¿‡ä½¿ç”¨åˆæˆå›¾åƒè®­ç»ƒ3D U-Netç½‘ç»œï¼Œé‡‡ç”¨é¢†åŸŸéšæœºåŒ–æ–¹æ³•ã€‚</li>
<li>GOUHFIåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†æµ‹è¯•ï¼Œä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”è¡¨ç°å‡ºè‰¯å¥½çš„åˆ†å‰²æ•ˆæœã€‚</li>
<li>GOUHFIå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¸ºç¥ç»ç§‘å­¦å®¶æä¾›ä¸€ç§æ–°çš„è¶…é«˜é¢‘MRIå›¾åƒåˆ†å‰²å·¥å…·ã€‚</li>
<li>GOUHFIå…·æœ‰å¯¹æ¯”åº¦å’Œåˆ†è¾¨ç‡çš„é€šç”¨æ€§ï¼Œæ— éœ€é’ˆå¯¹ç‰¹å®šæ•°æ®é›†è¿›è¡Œå¾®è°ƒæˆ–é‡æ–°è®­ç»ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11445">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2505.11445v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2505.11445v2/page_4_0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="First-Results-on-the-Search-for-Lepton-Number-Violating-Neutrinoless-Double-Beta-Decay-with-the-LEGEND-200-Experiment"><a href="#First-Results-on-the-Search-for-Lepton-Number-Violating-Neutrinoless-Double-Beta-Decay-with-the-LEGEND-200-Experiment" class="headerlink" title="First Results on the Search for Lepton Number Violating Neutrinoless   Double Beta Decay with the LEGEND-200 Experiment"></a>First Results on the Search for Lepton Number Violating Neutrinoless   Double Beta Decay with the LEGEND-200 Experiment</h2><p><strong>Authors:H. Acharya, N. Ackermann, M. Agostini, A. Alexander, C. Andreoiu, G. R. Araujo, F. T. Avignone III, M. Babicz, W. Bae, A. Bakalyarov, M. Balata, A. S. Barabash, P. S. Barbeau, C. J. Barton, L. Baudis, C. Bauer, E. Bernieri, L. Bezrukov, K. H. Bhimani, V. Biancacci, E. Blalock, S. J. Borden, G. Borghi, F. Borra, B. Bos, A. Boston, V. Bothe, R. Bouabid, R. Brugnera, N. Burlac, M. Busch, S. Calgaro, L. Canonica, S. Capra, M. Carminati, R. M. D. Carney, C. Cattadori, R. Cesarano, Y. -D. Chan, J. R. Chapman, A. Chernogorov, P. -J. Chiu, C. D. Christofferson, M. L. Clark, A. I. Colon-Rivera, T. Comellato, V. Dâ€™Andrea, R. Deckert, J. A. Detwiler, A. Di Giacinto, N. Di Marco, T. Dixon, K. -M. Dong, A. Drobizhev, G. Duran, Yu. Efremenko, S. R. Elliott, C. H. J. Emmanuel, E. Engelhardt, E. Esch, M. T. Febbraro, F. Ferella, D. E. Fields, C. Fiorini, M. Fomina, N. Fuad, R. Gala, A. Galindo-Uribarri, A. Gangapshev, A. Garfagnini, S. Gazzana, A. Geraci, L. Gessler, C. Ghiano, A. Gieb, S. Giri, M. Gold, C. Gooch, G. GrÃ¼nauer, M. P. Green, J. Gruszko, I. Guinn, V. E. Guiseppe, V. Gurentsov, Y. Gurov, K. Gusev, B. Hackett, F. Hagemann, M. Haranczyk, F. Henkes, R. Henning, J. Herrera, D. Hervas Aguilar, J. Hinton, R. HodÃ¡k, H. F. R. Hoffmann, M. A. Howe, M. Huber, M. Hult, A. Ianni, K. JÄ™drzejczak, J. Jochum, R. W. L. Jones, D. S. Judson, M. Junker, J. Kaizer, V. Kazalov, M. F. Kidd, T. Kihm, K. Kilgus, A. Klimenko, K. T. KnÃ¶pfle, I. Kochanek, O. Kochetov, I. Kontul, L. L. Kormos, V. N. Kornoukhov, P. Krause, H. Krishnamoorthy, V. V. Kuzminov, K. Lang, M. Laubenstein, N. N. P. N. Lay, E. LeÃ³n, A. Leder, B. Lehnert, A. Leonhardt, N. Levashko, L. Y. Li, A. Li, Y. -R. Lin, M. Lindner, I. Lippi, A. Love, A. Lubashevskiy, B. Lubsandorzhiev, N. Lusardi, C. Macolino, B. Majorovits, F. Mamedov, L. Manzanillas, G. G. Marshall, R. D. Martin, E. L. Martin, R. Massarczyk, A. Mazumdar, G. McDowell, D. -M. Mei, S. P. Meireles, M. Menzel, S. Mertens, E. Miller, I. Mirza, M. Misiaszek, M. Morella, B. Morgan, T. Mroz, D. Muenstermann, C. J. Nave, I. Nemchenok, M. Neuberger, N. Oâ€™Briant, F. Paissan, L. Papp, L. S. Paudel, K. Pelczar, L. Pertoldi, W. Pettus, F. Piastra, M. Pichotta, P. Piseri, A. W. P. Poon, P. P. Povinec, M. Pruckner, A. Pullia, W. S. Quinn, D. C. Radford, Y. A. Ramachers, A. Razeto, M. Redchuk, A. L. Reine, S. Riboldi, K. Rielage, C. Romo-Luque, N. Rossi, S. Rozov, T. J. Ruland, N. Rumyantseva, J. Runge, R. Saakyan, S. Sailer, G. Salamanna, F. Salamida, G. Saleh, V. Sandukovsky, C. Savarese, S. SchÃ¶nert, A. -K. SchÃ¼tz, D. C. Schaper, L. SchlÃ¼ter, S. J. Schleich, O. Schulz, M. Schwarz, B. Schwingenheuer, C. Seibt, O. Selivanenko, G. Senatore, A. Serafini, K. Shakhov, E. Shevchik, M. Shirchenko, Y. Shitov, H. Simgen, F. Å imkovic, S. Simonaitis-Boyd, M. Skorokhvatov, M. SlavÃ­ÄkovÃ¡, A. Smolnikov, J. A. Solomon, G. Song, A. C. Sousa, A. R. Sreekala, L. Steinhart, I. Å tekl, T. Sterr, M. Stommel, S. A. Sullivan, R. R. Sumathi, K. Szczepaniec, L. Taffarello, D. Tagnani, D. J. Tedeschi, T. N. Thorpe, V. Tretyak, M. Turqueti, E. E. Van Nieuwenhuizen, L. J. Varriano, S. Vasilyev, A. Veresnikova, C. Vignoli, C. Vogl, K. von Sturm, A. Warren, D. Waters, S. L. Watkins, C. Wiesinger, J. F. Wilkerson, M. Willers, C. Wiseman, M. Wojcik, D. Xu, W. Xu, E. Yakushev, T. Ye, C. -H. Yu, V. Yumatov, D. Zinatulina, K. Zuber, G. Zuzel</strong></p>
<p>The LEGEND collaboration is searching for neutrinoless double beta ($0\nu\beta\beta$) decay by operating high-purity germanium detectors enriched in $^{76}$Ge in a low-background liquid argon environment. Building on key technological innovations from GERDA and the MAJORANA DEMONSTRATOR, LEGEND-200 has performed a first $0\nu\beta\beta$ decay search based on 61.0 kg yr of data. Over half of this exposure comes from our highest performing detectors, including newly developed inverted-coaxial detectors, and is characterized by an estimated background level of $0.5^{+0.3}<em>{-0.2}$ cts&#x2F;(keV kg yr) in the $0\nu\beta\beta$ decay signal region. A combined analysis of data from GERDA, the MAJORANA DEMONSTRATOR, and LEGEND-200, characterized by a 90% confidence level exclusion sensitivity of $2.8 \times 10^{26}$ yr on the half-life of $0\nu\beta\beta$ decay, reveals no evidence for a signal and sets a new observed lower limit at $T^{0\nu}</em>{1&#x2F;2} &gt; 1.9 \times 10^{26}$ yr (90% confidence level). Assuming the decay is mediated by Majorana neutrinos, this corresponds to an upper limit on the effective Majorana mass in the range $m_{\beta\beta} &lt; 75-200$ meV, depending on the adopted nuclear matrix element. </p>
<blockquote>
<p>ä¼ å¥‡ï¼ˆLEGENDï¼‰åˆä½œæ­£åœ¨ä½èƒŒæ™¯æ¶²æ°©ç¯å¢ƒä¸­æ“ä½œé«˜çº¯åº¦é”—æ¢æµ‹å™¨ï¼Œå¯»æ‰¾æ— ä¸­å¾®å­åŒÎ²è¡°å˜ï¼ˆ$0Î½Î²Î²$ï¼‰ã€‚ä¼ å¥‡åŸºäºGERDAå’ŒMAJORANAæ¼”ç¤ºè€…çš„å…³é”®æŠ€æœ¯åˆ›æ–°ï¼Œå·²ç»åŸºäºé•¿è¾¾61åƒå…‹å¹´çš„æ•°æ®è¿›è¡Œäº†é¦–æ¬¡æ— ä¸­å¾®å­åŒÎ²è¡°å˜æœç´¢ã€‚åŠæ•°ä»¥ä¸Šçš„æ›å…‰æ¥è‡ªæ€§èƒ½æœ€é«˜çš„æ¢æµ‹å™¨ï¼ŒåŒ…æ‹¬æœ€æ–°å¼€å‘çš„å€’ç½®åŒè½´æ¢æµ‹å™¨ï¼Œåœ¨åŒÎ²è¡°å˜ä¿¡å·åŒºåŸŸçš„èƒŒæ™¯æ°´å¹³ä¼°è®¡ä¸º $ï¼ˆä¸­å­æ•°ç›®&#x2F;åƒç“¦æ—¶ï¼‰Ã— (ä¸ŠÂ±ä¸‹)-ii \mathrm{cnts}$ ï¼ˆå•ä½ä¸ºåƒå…‹å¹´ï¼‰ã€‚å¯¹GERDAã€MAJORANAæ¼”ç¤ºå™¨å’ŒLEGENDçš„ç»¼åˆæ•°æ®åˆ†æï¼Œåœ¨æ’é™¤åŠè¡°æœŸè¾¾åˆ° $ï¼ˆåŠè¡°æœŸï¼‰å¹´ $çš„ç½®ä¿¡æ°´å¹³ä¸ºç™¾åˆ†ä¹‹ä¹åçš„æƒ…å†µä¸‹ï¼Œæ­ç¤ºå‡ºæ²¡æœ‰ä¿¡å·çš„è¯æ®ï¼Œå¹¶è®¾å®šäº†æ–°çš„è§‚æµ‹ä¸‹é™ä¸º $T^{0Î½}_{åŠè¡°æœŸ} &gt; 1.9 \times 10^{åŠè¡°æœŸ}$ å¹´ï¼ˆç™¾åˆ†ä¹‹ä¹åçš„ç½®ä¿¡æ°´å¹³ï¼‰ã€‚å‡è®¾è¡°å˜æ˜¯ç”±é©¬çº¦æ‹‰çº³ä¸­å¾®å­ä»‹å¯¼çš„ï¼Œé‚£ä¹ˆè¿™å¯¹åº”æœ‰æ•ˆé©¬çº¦æ‹‰çº³è´¨é‡çš„ä¸Šé™æ˜¯åœ¨æ•°å€¼èŒƒå›´ä¸ºæ¯«ç”µå­ä¼ç‰¹ä¸‹çš„å€¼ä»¥å†…ä¸è¶…è¿‡è§„å®šæ•°å€¼å’Œå‚æ•°åœ¨å¯¹åº”çš„ç†è®ºæ ‡å‡†å€¼çš„æ­£è´ŸèŒƒå›´èŒƒå›´å†…æµ®åŠ¨ï¼ˆæ ¹æ®æ‰€é€‰æ‹©çš„æ ¸çŸ©é˜µå…ƒç´ æœ‰æ‰€ä¸åŒï¼‰ã€‚æ¢å¥è¯è¯´ï¼Œè¿™ä¸ªå‘ç°ä¸ºæˆ‘ä»¬æä¾›äº†å…³äºæ— ä¸­å¾®å­åŒÎ²è¡°å˜çš„æ–°è§‚æµ‹ç»“æœå’Œç†è®ºé™åˆ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.10440v4">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    ä¼ å¥‡åˆä½œè¿ç”¨é«˜çº¯åº¦é”—æ¢æµ‹å™¨å¯»æ‰¾æ— ä¸­å¾®å­åŒÎ²è¡°å˜ï¼ˆ$0Î½Î²Î²$ï¼‰ï¼Œæ¢æµ‹å™¨ä»¥$^{76}$Geæµ“ç¼©ç‰©ä¸ºå·¥ä½œä»‹è´¨ï¼Œåœ¨ä½æœ¬åº•æ¶²æ°©ç¯å¢ƒä¸­æ“ä½œã€‚åŸºäºGERDAå’ŒMAJORANA DEMONSTRATORçš„å…³é”®æŠ€æœ¯åˆ›æ–°ï¼ŒLEGEND-200é¦–æ¬¡è¿›è¡Œäº†åŸºäº$0Î½Î²Î²$è¡°å˜çš„æœç´¢ï¼Œåˆ†ææ•°æ®è¾¾61.0å…¬æ–¤å¹´ã€‚æ–°å¼€å‘çš„å€’ç½®åŒè½´æ¢æµ‹å™¨ç­‰é«˜æ€§èƒ½æ¢æµ‹å™¨è´¡çŒ®äº†è¶…è¿‡ä¸€åŠçš„æ›å…‰é‡ï¼Œå…¶åœ¨$0Î½Î²Î²$è¡°å˜ä¿¡å·åŒºåŸŸçš„èƒŒæ™¯æ°´å¹³ä¼°è®¡ä¸º$0.5^{+0.3}<em>{-0.2}$ cts&#x2F;(kgå¹´)ã€‚ç»¼åˆåˆ†æGERDAã€MAJORANA DEMONSTRATORå’ŒLEGEND-200çš„æ•°æ®ï¼Œåœ¨90%çš„ç½®ä¿¡æ°´å¹³ä¸‹ï¼ŒåŠè¡°æœŸçš„æ’é™¤æ•æ„Ÿæ€§ä¸º$2.8 \times 10^{26}$å¹´ï¼Œæœªè§ä¿¡å·è¿¹è±¡ï¼Œå¹¶è®¾ç½®æ–°çš„è§‚æµ‹ä¸‹é™ä¸º$T^{0Î½}</em>{1&#x2F;2} &gt; 1.9 \times 10^{26}$å¹´ã€‚å‡è®¾è¡°å˜ç”±é©¬çº¦æ‹‰çº³ä¸­å¾®å­ä»‹å¯¼ï¼Œè¿™å¯¹åº”äºæœ‰æ•ˆé©¬çº¦æ‹‰çº³è´¨é‡ä¸Šé™åœ¨é‡‡ç”¨ä¸åŒæ ¸çŸ©é˜µå…ƒç´ æ—¶æœ‰æ‰€ä¸åŒï¼ŒèŒƒå›´ä¸º$m_{\beta\beta} &lt; 75-200$ meVã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>LEGENDåˆä½œä½¿ç”¨é«˜çº¯åº¦é”—æ¢æµ‹å™¨å¯»æ‰¾æ— ä¸­å¾®å­åŒÎ²è¡°å˜ï¼ˆ$0Î½Î²Î²$ï¼‰ã€‚</li>
<li>LEGEND-200åŸºäºGERDAå’ŒMAJORANA DEMONSTRATORçš„æŠ€æœ¯åˆ›æ–°è¿›è¡Œæœç´¢ã€‚</li>
<li>æ•°æ®åˆ†ææ­ç¤ºäº†æ— ä¿¡å·è¯æ®ï¼Œå¹¶è®¾å®šäº†æ–°çš„è§‚æµ‹ä¸‹é™ã€‚</li>
<li>åœ¨æ— ä¸­å¾®å­åŒÎ²è¡°å˜åŒºåŸŸï¼ŒèƒŒæ™¯æ°´å¹³ä¼°è®¡è¾ƒä½ã€‚</li>
<li>é«˜æ€§èƒ½æ¢æµ‹å™¨å¦‚å€’ç½®åŒè½´æ¢æµ‹å™¨å¯¹å®éªŒè´¡çŒ®æ˜¾è‘—ã€‚</li>
<li>ç»¼åˆåˆ†ææ•°æ®æ¥è‡ªGERDAã€MAJORANA DEMONSTRATORå’ŒLEGEND-200ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.10440">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2505.10440v4/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2505.10440v4/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2505.10440v4/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2505.10440v4/page_5_0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Robustness-and-sex-differences-in-skin-cancer-detection-logistic-regression-vs-CNNs"><a href="#Robustness-and-sex-differences-in-skin-cancer-detection-logistic-regression-vs-CNNs" class="headerlink" title="Robustness and sex differences in skin cancer detection: logistic   regression vs CNNs"></a>Robustness and sex differences in skin cancer detection: logistic   regression vs CNNs</h2><p><strong>Authors:Nikolette Pedersen, Regitze Sydendal, Andreas Wulff, Ralf Raumanns, Eike Petersen, Veronika Cheplygina</strong></p>
<p>Deep learning has been reported to achieve high performances in the detection of skin cancer, yet many challenges regarding the reproducibility of results and biases remain. This study is a replication (different data, same analysis) of a previous study on Alzheimerâ€™s disease detection, which studied the robustness of logistic regression (LR) and convolutional neural networks (CNN) across patient sexes. We explore sex bias in skin cancer detection, using the PAD-UFES-20 dataset with LR trained on handcrafted features reflecting dermatological guidelines (ABCDE and the 7-point checklist), and a pre-trained ResNet-50 model. We evaluate these models in alignment with the replicated study: across multiple training datasets with varied sex composition to determine their robustness. Our results show that both the LR and the CNN were robust to the sex distribution, but the results also revealed that the CNN had a significantly higher accuracy (ACC) and area under the receiver operating characteristics (AUROC) for male patients compared to female patients. The data and relevant scripts to reproduce our results are publicly available (<a target="_blank" rel="noopener" href="https://github.com/">https://github.com/</a> nikodice4&#x2F;Skin-cancer-detection-sex-bias). </p>
<blockquote>
<p>æ·±åº¦å­¦ä¹ åœ¨çš®è‚¤ç™Œæ£€æµ‹æ–¹é¢å·²è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†ä»å­˜åœ¨å…³äºç»“æœå¯é‡å¤æ€§å’Œåè§æ–¹é¢çš„è¯¸å¤šæŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æ˜¯å¯¹ä¸€é¡¹å…³äºé˜¿å°”èŒ¨æµ·é»˜ç—…æ£€æµ‹ç ”ç©¶çš„å¤ç°ï¼ˆä¸åŒæ•°æ®ï¼Œç›¸åŒåˆ†æï¼‰ï¼Œè¯¥ç ”ç©¶æ¢è®¨äº†é€»è¾‘å›å½’ï¼ˆLRï¼‰å’Œå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰åœ¨ä¸åŒæ‚£è€…æ€§åˆ«é—´çš„ç¨³å¥æ€§ã€‚æˆ‘ä»¬åˆ©ç”¨PAD-UFES-20æ•°æ®é›†ï¼Œæ¢ç´¢çš®è‚¤ç™Œæ£€æµ‹ä¸­çš„æ€§åˆ«åè§ï¼Œä½¿ç”¨é€»è¾‘å›å½’å¯¹æ‰‹å·¥ç‰¹å¾è¿›è¡Œè®­ç»ƒï¼Œè¿™äº›ç‰¹å¾åæ˜ äº†çš®è‚¤ç§‘æŒ‡å—ï¼ˆABCDEå’Œ7ç‚¹æ¸…å•ï¼‰ï¼Œä»¥åŠé¢„è®­ç»ƒçš„ResNet-50æ¨¡å‹ã€‚æˆ‘ä»¬ä¸å¤ç°çš„ç ”ç©¶ç›¸ä¸€è‡´ï¼Œå¯¹å¤šä¸ªè®­ç»ƒæ•°æ®é›†è¿›è¡Œäº†è¯„ä¼°ï¼Œè¿™äº›æ•°æ®é›†å…·æœ‰ä¸åŒçš„æ€§åˆ«ç»„æˆï¼Œä»¥ç¡®å®šæ¨¡å‹çš„ç¨³å¥æ€§ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œé€»è¾‘å›å½’å’Œå·ç§¯ç¥ç»ç½‘ç»œå¯¹æ€§åˆ«åˆ†å¸ƒå…·æœ‰ç¨³å¥æ€§ï¼Œä½†ç»“æœè¿˜æ˜¾ç¤ºï¼Œå·ç§¯ç¥ç»ç½‘ç»œå¯¹ç”·æ€§æ‚£è€…çš„å‡†ç¡®ç‡ï¼ˆACCï¼‰å’Œå—è¯•è€…æ“ä½œç‰¹æ€§æ›²çº¿ä¸‹é¢ç§¯ï¼ˆAUROCï¼‰æ˜¾è‘—é«˜äºå¥³æ€§æ‚£è€…ã€‚æœ‰å…³å¤ç°æˆ‘ä»¬ç»“æœçš„æ•°æ®å’Œç›¸å…³è„šæœ¬å¯å…¬å¼€è·å–ï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/nikodice4/Skin-cancer-detection-sex-bias%EF%BC%89%E3%80%82">https://github.com/nikodice4/Skin-cancer-detection-sex-biasï¼‰ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11415v2">PDF</a> 10 pages, 1 figure, published at FAIMI workshop at the MICCAI 2025   conference</p>
<p><strong>Summary</strong><br>     æ·±åº¦å­¦ä¹ åœ¨çš®è‚¤ç™Œæ£€æµ‹ä¸­è¡¨ç°å‡ºé«˜æ€§èƒ½ï¼Œä½†ä»å­˜åœ¨ç»“æœå¯é‡å¤æ€§å’Œåè§ç­‰å¤šæ–¹é¢çš„æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æ˜¯å¯¹ä¸€é¡¹é˜¿å°”èŒ¨æµ·é»˜ç—…æ£€æµ‹ç ”ç©¶çš„å¤åˆ¶ï¼ˆä¸åŒæ•°æ®ï¼Œç›¸åŒåˆ†æï¼‰ï¼Œæ¢è®¨äº†é€»è¾‘å›å½’ï¼ˆLRï¼‰å’Œå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰åœ¨æ‚£è€…æ€§åˆ«é—´çš„ç¨³å¥æ€§ã€‚ç ”ç©¶ä½¿ç”¨PAD-UFES-20æ•°æ®é›†ï¼Œé‡‡ç”¨åŸºäºçš®è‚¤ç§‘æŒ‡å—ï¼ˆABCDEå’Œ7ç‚¹æ¸…å•ï¼‰çš„æ‰‹å·¥ç‰¹å¾è®­ç»ƒçš„LRå’Œé¢„è®­ç»ƒçš„ResNet-50æ¨¡å‹ï¼Œè¯„ä¼°æ¨¡å‹åœ¨æ€§åˆ«åˆ†å¸ƒä¸åŒçš„å¤šä¸ªè®­ç»ƒæ•°æ®é›†ä¸Šçš„ç¨³å¥æ€§ã€‚ç»“æœè¡¨æ˜ï¼ŒLRå’ŒCNNå¯¹æ€§åˆ«åˆ†å¸ƒå…·æœ‰ç¨³å¥æ€§ï¼Œä½†CNNå¯¹ç”·æ€§æ‚£è€…çš„å‡†ç¡®åº¦ï¼ˆACCï¼‰å’Œå—è¯•è€…ç‰¹å¾æ¥æ”¶æ›²çº¿ä¸‹é¢ç§¯ï¼ˆAUROCï¼‰æ˜¾è‘—é«˜äºå¥³æ€§æ‚£è€…ã€‚ç›¸å…³æ•°æ®åŠè„šæœ¬å·²å…¬å¼€å¯ä¾›å¤åˆ¶ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ åœ¨çš®è‚¤ç™Œæ£€æµ‹ä¸­å±•ç°å‡ºé«˜æ€§èƒ½ï¼Œä½†ç»“æœçš„å¯é‡å¤æ€§å’Œåè§é—®é¢˜ä»éœ€å…³æ³¨ã€‚</li>
<li>æœ¬ç ”ç©¶æ˜¯å¯¹é˜¿å°”èŒ¨æµ·é»˜ç—…æ£€æµ‹ç ”ç©¶çš„å¤åˆ¶ï¼Œæ—¨åœ¨æ¢è®¨é€»è¾‘å›å½’å’Œå·ç§¯ç¥ç»ç½‘ç»œåœ¨ä¸åŒæ‚£è€…æ€§åˆ«é—´çš„ç¨³å¥æ€§ã€‚</li>
<li>ä½¿ç”¨PAD-UFES-20æ•°æ®é›†è¿›è¡Œçš®è‚¤ç™Œæ£€æµ‹ç ”ç©¶ï¼Œé‡‡ç”¨åŸºäºçš®è‚¤ç§‘æŒ‡å—çš„æ‰‹å·¥ç‰¹å¾è®­ç»ƒçš„LRæ¨¡å‹å’Œé¢„è®­ç»ƒçš„ResNet-50æ¨¡å‹ã€‚</li>
<li>è¯„ä¼°æ¨¡å‹åœ¨å¤šä¸ªæ€§åˆ«åˆ†å¸ƒä¸åŒçš„è®­ç»ƒæ•°æ®é›†ä¸Šçš„è¡¨ç°ï¼Œæ˜¾ç¤ºæ¨¡å‹å¯¹æ€§åˆ«åˆ†å¸ƒå…·æœ‰ç¨³å¥æ€§ã€‚</li>
<li>CNNæ¨¡å‹å¯¹ç”·æ€§æ‚£è€…çš„æ£€æµ‹å‡†ç¡®åº¦å’Œå—è¯•è€…ç‰¹å¾æ¥æ”¶æ›²çº¿ä¸‹é¢ç§¯æ˜¾è‘—é«˜äºå¥³æ€§æ‚£è€…ã€‚</li>
<li>ç ”ç©¶æ•°æ®å’Œç›¸å…³è„šæœ¬å·²å…¬å¼€ï¼Œæ–¹ä¾¿å…¶ä»–ç ”ç©¶è€…è¿›è¡Œå¤åˆ¶å’Œè¿›ä¸€æ­¥åˆ†æã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11415">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2504.11415v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2504.11415v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2504.11415v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Self-Evolving-Multi-Agent-Simulations-for-Realistic-Clinical-Interactions"><a href="#Self-Evolving-Multi-Agent-Simulations-for-Realistic-Clinical-Interactions" class="headerlink" title="Self-Evolving Multi-Agent Simulations for Realistic Clinical   Interactions"></a>Self-Evolving Multi-Agent Simulations for Realistic Clinical   Interactions</h2><p><strong>Authors:Mohammad Almansoori, Komal Kumar, Hisham Cholakkal</strong></p>
<p>In this work, we introduce MedAgentSim, an open-source simulated clinical environment with doctor, patient, and measurement agents designed to evaluate and enhance LLM performance in dynamic diagnostic settings. Unlike prior approaches, our framework requires doctor agents to actively engage with patients through multi-turn conversations, requesting relevant medical examinations (e.g., temperature, blood pressure, ECG) and imaging results (e.g., MRI, X-ray) from a measurement agent to mimic the real-world diagnostic process. Additionally, we incorporate self improvement mechanisms that allow models to iteratively refine their diagnostic strategies. We enhance LLM performance in our simulated setting by integrating multi-agent discussions, chain-of-thought reasoning, and experience-based knowledge retrieval, facilitating progressive learning as doctor agents interact with more patients. We also introduce an evaluation benchmark for assessing the LLMâ€™s ability to engage in dynamic, context-aware diagnostic interactions. While MedAgentSim is fully automated, it also supports a user-controlled mode, enabling human interaction with either the doctor or patient agent. Comprehensive evaluations in various simulated diagnostic scenarios demonstrate the effectiveness of our approach. Our code, simulation tool, and benchmark are available at \href{<a target="_blank" rel="noopener" href="https://medagentsim.netlify.app/%7D">https://medagentsim.netlify.app/}</a>. </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†MedAgentSimï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€æ”¾æºç çš„æ¨¡æ‹Ÿä¸´åºŠç¯å¢ƒï¼Œå…¶ä¸­åŒ…å«åŒ»ç”Ÿã€æ‚£è€…å’Œæµ‹é‡ä»£ç†ï¼Œæ—¨åœ¨è¯„ä¼°å’Œæé«˜å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŠ¨æ€è¯Šæ–­ç¯å¢ƒä¸­çš„æ€§èƒ½ã€‚ä¸ä¼ ç»Ÿçš„æ¡†æ¶ä¸åŒï¼Œæˆ‘ä»¬çš„æ¡†æ¶è¦æ±‚åŒ»ç”Ÿä»£ç†é€šè¿‡å¤šè½®å¯¹è¯ç§¯æåœ°ä¸æ‚£è€…äº’åŠ¨ï¼Œè¯·æ±‚ç›¸å…³çš„åŒ»å­¦æ£€æŸ¥ï¼ˆå¦‚ä½“æ¸©ã€è¡€å‹ã€å¿ƒç”µå›¾ï¼‰å’Œæ¥è‡ªæµ‹é‡ä»£ç†çš„æˆåƒç»“æœï¼ˆå¦‚ç£å…±æŒ¯æˆåƒã€Xå°„çº¿ï¼‰ï¼Œä»¥æ¨¡æ‹Ÿç°å®ä¸–ç•Œçš„è¯Šæ–­è¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†è‡ªæˆ‘æ”¹è¿›æœºåˆ¶ï¼Œå…è®¸æ¨¡å‹é€šè¿‡ä¸æ–­ä¸æ›´å¤šæ‚£è€…äº’åŠ¨æ¥é€æ­¥ä¼˜åŒ–å…¶è¯Šæ–­ç­–ç•¥ã€‚æˆ‘ä»¬é€šè¿‡æ•´åˆå¤šä»£ç†è®¨è®ºã€é“¾å¼æ€ç»´æ¨ç†å’ŒåŸºäºç»éªŒçš„çŸ¥è¯†æ£€ç´¢ï¼Œå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­çš„æ€§èƒ½ï¼Œä¿ƒè¿›åŒ»ç”Ÿä»£ç†åœ¨äº’åŠ¨ä¸­çš„æ¸è¿›å­¦ä¹ ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªè¯„ä¼°åŸºå‡†ï¼Œä»¥è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹å‚ä¸åŠ¨æ€ã€åŸºäºä¸Šä¸‹æ–‡è¯Šæ–­äº¤äº’çš„èƒ½åŠ›ã€‚è™½ç„¶MedAgentSimå®Œå…¨è‡ªåŠ¨åŒ–ï¼Œä½†å®ƒä¹Ÿæ”¯æŒç”¨æˆ·æ§åˆ¶æ¨¡å¼ï¼Œå…è®¸äººç±»ä¸åŒ»ç”Ÿæˆ–æ‚£è€…ä»£ç†è¿›è¡Œäº’åŠ¨ã€‚åœ¨å„ç§æ¨¡æ‹Ÿè¯Šæ–­åœºæ™¯ä¸­çš„ç»¼åˆè¯„ä¼°è¯æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ä»£ç ã€ä»¿çœŸå·¥å…·å’ŒåŸºå‡†æµ‹è¯•å¹³å°å¯åœ¨[<a target="_blank" rel="noopener" href="https://medagentsim.netlify.app/]%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://medagentsim.netlify.app/]ä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.22678v2">PDF</a> 14 page, 4 figures, 61 references, presented in MICCAI (Oral)</p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒé¢†åŸŸçš„ç ”ç©¶äººå‘˜æ¨å‡ºäº†ä¸€æ¬¾åä¸ºMedAgentSimçš„å¼€æºæ¨¡æ‹Ÿä¸´åºŠç¯å¢ƒï¼Œè¯¥ç¯å¢ƒé€šè¿‡åŒ»ç”Ÿã€æ‚£è€…å’Œæµ‹é‡ä»£ç†æ¨¡æ‹ŸåŠ¨æ€è¯Šæ–­åœºæ™¯ï¼Œè¯„ä¼°å¹¶æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ€§èƒ½ã€‚MedAgentSimè¦æ±‚åŒ»ç”Ÿä»£ç†é€šè¿‡å¤šè½®å¯¹è¯ç§¯æä¸æ‚£è€…äº’åŠ¨ï¼Œä»æµ‹é‡ä»£ç†è¯·æ±‚ç›¸å…³çš„åŒ»å­¦æ£€æŸ¥å’ŒåŒ»å­¦å›¾åƒç»“æœï¼Œä»¥æ¨¡æ‹ŸçœŸå®ä¸–ç•Œçš„è¯Šæ–­è¿‡ç¨‹ã€‚è¯¥æ¡†æ¶è¿˜å…è®¸æ¨¡å‹é€šè¿‡è‡ªæˆ‘æ”¹è¿›æœºåˆ¶ä¸æ–­å®Œå–„å…¶è¯Šæ–­ç­–ç•¥ã€‚è¯¥æ¨¡æ‹Ÿç¯å¢ƒæ”¯æŒå¤šä»£ç†è®¨è®ºã€é“¾å¼æ€ç»´å’ŒåŸºäºç»éªŒçš„çŸ¥ä¼æ£€ç´¢ç­‰ç‰¹è‰²åŠŸèƒ½ï¼Œä»¥ä¿ƒè¿›åŒ»ç”Ÿä»£ç†ä¸æ‚£è€…äº’åŠ¨çš„æ¸è¿›å­¦ä¹ ã€‚æ­¤å¤–ï¼Œè¿˜æ¨å‡ºäº†è¯„ä¼°åŸºå‡†ï¼Œä»¥è¯„ä¼°LLMåœ¨åŠ¨æ€ã€è¯­å¢ƒæ„ŸçŸ¥çš„è¯Šæ–­äº¤äº’ä¸­çš„èƒ½åŠ›ã€‚MedAgentSimæ—¢æ”¯æŒå…¨è‡ªåŠ¨æ¨¡å¼ï¼Œä¹Ÿæ”¯æŒç”¨æˆ·æ§åˆ¶æ¨¡å¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MedAgentSimæ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿä¸´åºŠç¯å¢ƒçš„å¼€æºæ¡†æ¶ï¼Œé’ˆå¯¹åŠ¨æ€è¯Šæ–­åœºæ™¯è¯„ä¼°å’Œæå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ€§èƒ½ã€‚</li>
<li>é€šè¿‡åŒ»ç”Ÿã€æ‚£è€…å’Œæµ‹é‡ä»£ç†çš„äº’åŠ¨æ¥æ¨¡æ‹ŸçœŸå®ä¸–ç•Œçš„è¯Šæ–­è¿‡ç¨‹ã€‚</li>
<li>æ¡†æ¶å¼•å…¥äº†å¤šè½®å¯¹è¯ï¼ŒåŒ»ç”Ÿä»£ç†éœ€ç§¯æä¸æ‚£è€…äº’åŠ¨ï¼Œå¹¶è¯·æ±‚åŒ»å­¦æ£€æŸ¥å’ŒåŒ»å­¦å›¾åƒç»“æœã€‚</li>
<li>æ¡†æ¶åŒ…å«è‡ªæˆ‘æ”¹è¿›æœºåˆ¶ï¼Œå…è®¸æ¨¡å‹ä¸æ–­å®Œå–„å…¶è¯Šæ–­ç­–ç•¥ã€‚</li>
<li>æ”¯æŒå¤šä»£ç†è®¨è®ºã€é“¾å¼æ€ç»´åŠåŸºäºç»éªŒçš„çŸ¥ä¼æ£€ç´¢ç­‰åŠŸèƒ½ï¼Œä¿ƒè¿›æ¸è¿›å­¦ä¹ ã€‚</li>
<li>æä¾›äº†ä¸€ä¸ªè¯„ä¼°åŸºå‡†ï¼Œç”¨äºè¯„ä¼°LLMåœ¨åŠ¨æ€ã€è¯­å¢ƒæ„ŸçŸ¥çš„è¯Šæ–­äº¤äº’ä¸­çš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.22678">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2503.22678v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2503.22678v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2503.22678v2/page_3_0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Efficient-Self-Supervised-Adaptation-for-Medical-Image-Analysis"><a href="#Efficient-Self-Supervised-Adaptation-for-Medical-Image-Analysis" class="headerlink" title="Efficient Self-Supervised Adaptation for Medical Image Analysis"></a>Efficient Self-Supervised Adaptation for Medical Image Analysis</h2><p><strong>Authors:Moein Sorkhei, Emir Konuk, Jingyu Guo, Chanjuan Meng, Christos Matsoukas, Kevin Smith</strong></p>
<p>Self-supervised adaptation (SSA) improves foundation model transfer to medical domains but is computationally prohibitive. Although parameter efficient fine-tuning methods such as LoRA have been explored for supervised adaptation, their effectiveness for SSA remains unknown. In this work, we introduce efficient self-supervised adaptation (ESSA), a framework that applies parameter-efficient fine-tuning techniques to SSA with the aim of reducing computational cost and improving adaptation performance. Among the methods tested, Attention Projection Layer Adaptation (APLA) sets a new state-of-the-art, consistently surpassing full-parameter SSA and supervised fine-tuning across diverse medical tasks, while reducing GPU memory by up to 40.1% and increasing training throughput by 25.2%, all while maintaining inference efficiency. </p>
<blockquote>
<p>è‡ªæˆ‘ç›‘ç£é€‚åº”ï¼ˆSSAï¼‰æ”¹å–„äº†åŸºç¡€æ¨¡å‹åœ¨åŒ»å­¦é¢†åŸŸçš„è¿ç§»ï¼Œä½†è®¡ç®—æˆæœ¬å¾ˆé«˜ã€‚è™½ç„¶å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæ–¹æ³•ï¼ˆå¦‚LoRAï¼‰å·²è¢«æ¢ç´¢ç”¨äºç›‘ç£é€‚åº”ï¼Œä½†å®ƒä»¬åœ¨SSAä¸­çš„æœ‰æ•ˆæ€§å°šä¸æ¸…æ¥šã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†é«˜æ•ˆè‡ªæˆ‘ç›‘ç£é€‚åº”ï¼ˆESSAï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæŠ€æœ¯åº”ç”¨äºSSAï¼Œæ—¨åœ¨é™ä½è®¡ç®—æˆæœ¬å¹¶æé«˜é€‚åº”æ€§èƒ½ã€‚åœ¨æµ‹è¯•çš„æ–¹æ³•ä¸­ï¼Œæ³¨æ„åŠ›æŠ•å½±å±‚é€‚åº”ï¼ˆAPLAï¼‰æ ‘ç«‹äº†æ–°çš„æŠ€æœ¯æ ‡æ†ï¼Œå®ƒä¸æ–­è¶…è¶Šå…¨å‚æ•°SSAå’Œç›‘ç£å¾®è°ƒï¼Œåœ¨å„ç§åŒ»å­¦ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚åŒæ—¶ï¼Œå®ƒå‡å°‘äº†é«˜è¾¾40.1%çš„GPUå†…å­˜ï¼Œæé«˜äº†25.2%çš„è®­ç»ƒæ•ˆç‡ï¼ŒåŒæ—¶ä¿æŒäº†æ¨ç†æ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18873v3">PDF</a> Accepted to ICCV CVAMD 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºè‡ªç›‘ç£é€‚åº”ï¼ˆSSAï¼‰çš„æ–¹æ³•æ”¹å–„äº†åŸºç¡€æ¨¡å‹åœ¨åŒ»å­¦é¢†åŸŸçš„è¿ç§»æ•ˆæœï¼Œä½†å…¶è®¡ç®—æˆæœ¬è¾ƒé«˜ã€‚æœ¬ç ”ç©¶å¼•å…¥é«˜æ•ˆè‡ªç›‘ç£é€‚åº”ï¼ˆESSAï¼‰æ¡†æ¶ï¼Œé‡‡ç”¨å‚æ•°ä¼˜åŒ–å¾®è°ƒæŠ€æœ¯æ¥ä¼˜åŒ–SSAï¼Œæ—¨åœ¨é™ä½è®¡ç®—æˆæœ¬å¹¶æé«˜é€‚åº”æ€§æ€§èƒ½ã€‚æµ‹è¯•ä¸­ï¼Œæ³¨æ„åŠ›æŠ•å½±å±‚é€‚åº”ï¼ˆAPLAï¼‰è¡¨ç°å“è¶Šï¼Œåœ¨å¤šç§åŒ»å­¦ä»»åŠ¡ä¸Šå‡è¶…è¶Šå…¨å‚æ•°SSAå’Œç›‘ç£å¾®è°ƒæ–¹æ³•ï¼ŒåŒæ—¶å‡å°‘GPUå†…å­˜ä½¿ç”¨è¾¾40.1%ï¼Œæé«˜è®­ç»ƒæ•ˆç‡è¾¾25.2%ï¼Œå¹¶ç»´æŒæ¨ç†æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªç›‘ç£é€‚åº”ï¼ˆSSAï¼‰åœ¨åŒ»å­¦é¢†åŸŸæ¨¡å‹è¿ç§»ä¸­æœ‰è‰¯å¥½è¡¨ç°ï¼Œä½†è®¡ç®—æˆæœ¬è¾ƒé«˜ã€‚</li>
<li>å‚æ•°ä¼˜åŒ–å¾®è°ƒæŠ€æœ¯å¦‚LoRAåœ¨ç›‘ç£é€‚åº”ä¸­å·²è¢«æ¢ç´¢ï¼Œä½†åœ¨SSAä¸­çš„æœ‰æ•ˆæ€§æœªçŸ¥ã€‚</li>
<li>é«˜æ•ˆè‡ªç›‘ç£é€‚åº”ï¼ˆESSAï¼‰æ¡†æ¶æ—¨åœ¨é™ä½SSAçš„è®¡ç®—æˆæœ¬å¹¶æé«˜é€‚åº”æ€§ã€‚</li>
<li>æ³¨æ„åŠ›æŠ•å½±å±‚é€‚åº”ï¼ˆAPLAï¼‰åœ¨å¤šç§åŒ»å­¦ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šå…¨å‚æ•°SSAåŠç›‘ç£å¾®è°ƒã€‚</li>
<li>APLAèƒ½æ˜¾è‘—é™ä½GPUå†…å­˜ä½¿ç”¨å¹¶æé«˜è®­ç»ƒæ•ˆç‡ã€‚</li>
<li>APLAèƒ½ç»´æŒæ¨ç†æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18873">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2503.18873v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2503.18873v3/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2503.18873v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2503.18873v3/page_4_0.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="A-Survey-on-Self-supervised-Contrastive-Learning-for-Multimodal-Text-Image-Analysis"><a href="#A-Survey-on-Self-supervised-Contrastive-Learning-for-Multimodal-Text-Image-Analysis" class="headerlink" title="A Survey on Self-supervised Contrastive Learning for Multimodal   Text-Image Analysis"></a>A Survey on Self-supervised Contrastive Learning for Multimodal   Text-Image Analysis</h2><p><strong>Authors:Asifullah Khan, Laiba Asmatullah, Anza Malik, Shahzaib Khan, Hamna Asif</strong></p>
<p>Self-supervised learning is a machine learning approach that generates implicit labels by learning underlined patterns and extracting discriminative features from unlabeled data without manual labelling. Contrastive learning introduces the concept of â€œpositiveâ€ and â€œnegativeâ€ samples, where positive pairs (e.g., variation of the same image&#x2F;object) are brought together in the embedding space, and negative pairs (e.g., views from different images&#x2F;objects) are pushed farther away. This methodology has shown significant improvements in image understanding and image text analysis without much reliance on labeled data. In this paper, we comprehensively discuss the terminologies, recent developments and applications of contrastive learning with respect to text-image models. Specifically, we provide an overview of the approaches of contrastive learning in text-image models in recent years. Secondly, we categorize the approaches based on different model structures. Thirdly, we further introduce and discuss the latest advances of the techniques used in the process such as pretext tasks for both images and text, architectural structures, and key trends. Lastly, we discuss the recent state-of-art applications of self-supervised contrastive learning Text-Image based models. </p>
<blockquote>
<p>è‡ªç›‘ç£å­¦ä¹ æ˜¯ä¸€ç§æœºå™¨å­¦ä¹ çš„æ–¹æ³•ï¼Œå®ƒé€šè¿‡å­¦ä¹ æ½œåœ¨çš„æ¨¡å¼å¹¶ä»æ— æ ‡ç­¾æ•°æ®ä¸­æå–åˆ¤åˆ«ç‰¹å¾ï¼Œä»è€Œç”Ÿæˆéšå¼æ ‡ç­¾ï¼Œè€Œæ— éœ€æ‰‹åŠ¨æ ‡æ³¨ã€‚å¯¹æ¯”å­¦ä¹ å¼•å…¥äº†â€œæ­£æ ·æœ¬â€å’Œâ€œè´Ÿæ ·æœ¬â€çš„æ¦‚å¿µï¼Œå…¶ä¸­æ­£æ ·æœ¬å¯¹ï¼ˆä¾‹å¦‚ï¼ŒåŒä¸€å›¾åƒ&#x2F;å¯¹è±¡çš„å˜ä½“ï¼‰åœ¨åµŒå…¥ç©ºé—´ä¸­èšé›†åœ¨ä¸€èµ·ï¼Œè€Œè´Ÿæ ·æœ¬å¯¹ï¼ˆä¾‹å¦‚ï¼Œæ¥è‡ªä¸åŒå›¾åƒ&#x2F;å¯¹è±¡çš„è§†å›¾ï¼‰åˆ™è¢«æ¨å¼€ã€‚è¿™ç§æ–¹æ³•åœ¨å›¾åƒç†è§£å’Œå›¾åƒæ–‡æœ¬åˆ†ææ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œè€Œä¸”ä¸éœ€è¦ä¾èµ–å¤§é‡çš„æ ‡æ³¨æ•°æ®ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å…¨é¢è®¨è®ºäº†ä¸æ–‡æœ¬-å›¾åƒæ¨¡å‹ç›¸å…³çš„å¯¹æ¯”å­¦ä¹ çš„æœ¯è¯­ã€æœ€æ–°å‘å±•ä»¥åŠåº”ç”¨ã€‚å…·ä½“åœ°ï¼Œæˆ‘ä»¬æ¦‚è¿°äº†è¿‘å¹´æ¥æ–‡æœ¬-å›¾åƒæ¨¡å‹ä¸­å¯¹æ¯”å­¦ä¹ çš„æ–¹æ³•ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æ ¹æ®ä¸åŒçš„æ¨¡å‹ç»“æ„å¯¹è¿™äº›æ–¹æ³•è¿›è¡Œäº†åˆ†ç±»ã€‚å†æ¬¡ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥ä»‹ç»äº†è¿‡ç¨‹ä¸­ä½¿ç”¨çš„æœ€æ–°æŠ€æœ¯ï¼Œå¦‚å›¾åƒå’Œæ–‡æœ¬çš„é¢„è®­ç»ƒä»»åŠ¡ã€æ¶æ„ç»“æ„å’Œå…³é”®è¶‹åŠ¿ã€‚æœ€åï¼Œæˆ‘ä»¬è®¨è®ºäº†åŸºäºæ–‡æœ¬-å›¾åƒçš„æœ€æ–°å…ˆè¿›çš„è‡ªç›‘ç£å¯¹æ¯”å­¦ä¹ çš„åº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.11101v4">PDF</a> 38 pages, 8 figures, survey paper</p>
<p><strong>Summary</strong><br>     è‡ªç›‘ç£å­¦ä¹ é€šè¿‡ä»éæ ‡è®°æ•°æ®ä¸­å­¦ä¹ æ½œåœ¨æ¨¡å¼å’Œæå–åˆ¤åˆ«ç‰¹å¾ï¼Œç”Ÿæˆéšå¼æ ‡ç­¾ï¼Œæ— éœ€äººå·¥æ ‡æ³¨ã€‚å¯¹æ¯”å­¦ä¹ å¼•å…¥äº†â€œæ­£æ ·æœ¬â€å’Œâ€œè´Ÿæ ·æœ¬â€çš„æ¦‚å¿µï¼Œå°†æ­£æ ·æœ¬å¯¹æ‹‰è¿‘åµŒå…¥ç©ºé—´ï¼Œå°†è´Ÿæ ·æœ¬å¯¹æ¨å¼€ã€‚æ­¤æ–¹æ³•åœ¨å›¾åƒç†è§£å’Œæ–‡æœ¬åˆ†ææ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—æ”¹è¿›ï¼Œå¯¹æ ‡æ³¨æ•°æ®çš„ä¾èµ–è¾ƒå°ã€‚æœ¬æ–‡å…¨é¢æ¢è®¨äº†æ–‡æœ¬å›¾åƒæ¨¡å‹ä¸­å¯¹æ¯”å­¦ä¹ çš„æœ¯è¯­ã€æœ€æ–°å‘å±•åŠåº”ç”¨ï¼Œä»‹ç»äº†æœ€æ–°çš„æŠ€æœ¯å’Œè¶‹åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªç›‘ç£å­¦ä¹ æ˜¯é€šè¿‡å­¦ä¹ æ½œåœ¨æ¨¡å¼å’Œæå–éæ ‡è®°æ•°æ®çš„åˆ¤åˆ«ç‰¹å¾æ¥ç”Ÿæˆéšå¼æ ‡ç­¾ã€‚</li>
<li>å¯¹æ¯”å­¦ä¹ é€šè¿‡å¼•å…¥æ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬çš„æ¦‚å¿µï¼Œåœ¨åµŒå…¥ç©ºé—´ä¸­åŒºåˆ†ä¸åŒçš„è¾“å…¥ã€‚</li>
<li>æ­£æ ·æœ¬å¯¹åœ¨åµŒå…¥ç©ºé—´ä¸­ç›¸äº’æ¥è¿‘ï¼Œè€Œè´Ÿæ ·æœ¬å¯¹è¢«æ¨å¼€ã€‚</li>
<li>å¯¹æ¯”å­¦ä¹ æ–¹æ³•åœ¨å›¾åƒç†è§£å’Œæ–‡æœ¬åˆ†ææ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œå‡å°‘å¯¹æ ‡æ³¨æ•°æ®çš„ä¾èµ–ã€‚</li>
<li>æ–‡æœ¬å›¾åƒæ¨¡å‹ä¸­å¯¹æ¯”å­¦ä¹ çš„æœ€æ–°å‘å±•å’Œåº”ç”¨è¢«è¯¦ç»†è®¨è®ºã€‚</li>
<li>æ–‡ç« ä»‹ç»äº†åŸºäºä¸åŒæ¨¡å‹ç»“æ„çš„å¯¹æ¯”å­¦ä¹ æ–¹æ³•åˆ†ç±»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.11101">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2503.11101v4/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2503.11101v4/page_4_0.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="High-Precision-Dichotomous-Image-Segmentation-via-Depth-Integrity-Prior-and-Fine-Grained-Patch-Strategy"><a href="#High-Precision-Dichotomous-Image-Segmentation-via-Depth-Integrity-Prior-and-Fine-Grained-Patch-Strategy" class="headerlink" title="High-Precision Dichotomous Image Segmentation via Depth Integrity-Prior   and Fine-Grained Patch Strategy"></a>High-Precision Dichotomous Image Segmentation via Depth Integrity-Prior   and Fine-Grained Patch Strategy</h2><p><strong>Authors:Xianjie Liu, Keren Fu, Qijun Zhao</strong></p>
<p>High-precision dichotomous image segmentation (DIS) is a task of extracting fine-grained objects from high-resolution images. Existing methods face a dilemma: non-diffusion methods work efficiently but suffer from false or missed detections due to weak semantics and less robust spatial priors; diffusion methods, using strong generative priors, have high accuracy but encounter high computational burdens. As a solution, we find pseudo depth information from monocular depth estimation models can provide essential semantic understanding that quickly reveals spatial differences across target objects and backgrounds. Inspired by this phenomenon, we discover a novel insight we term the depth integrity-prior: in pseudo depth maps, foreground objects consistently convey stable depth values with much lower variances than chaotic background patterns. To exploit such a prior, we propose a Prior of Depth Fusion Network (PDFNet). Specifically, our network establishes multimodal interactive modeling to achieve depth-guided structural perception by deeply fusing RGB and pseudo depth features. We further introduce a novel depth integrity-prior loss to explicitly enforce depth consistency in segmentation results. Additionally, we design a fine-grained perception enhancement module with adaptive patch selection to perform boundary-sensitive detail refinement. Notably, PDFNet achieves state-of-the-art performance with only 94M parameters (&lt;11% of those diffusion-based models), outperforming all non-diffusion methods and surpassing some diffusion methods. Code is provided in the supplementary materials. </p>
<blockquote>
<p>é«˜ç²¾åº¦äºŒå€¼å›¾åƒåˆ†å‰²ï¼ˆDISï¼‰æ˜¯ä»é«˜åˆ†è¾¨ç‡å›¾åƒä¸­æå–ç»†ç²’åº¦ç‰©ä½“çš„ä»»åŠ¡ã€‚ç°æœ‰æ–¹æ³•é¢ä¸´ä¸€ä¸ªå›°å¢ƒï¼šéæ‰©æ•£æ–¹æ³•è™½ç„¶æ•ˆç‡é«˜ï¼Œä½†ç”±äºè¯­ä¹‰è¾ƒå¼±å’Œç©ºé—´å…ˆéªŒä¸å¤Ÿç¨³å¥ï¼Œå®¹æ˜“å‡ºç°è¯¯æ£€æˆ–æ¼æ£€ï¼›è€Œä½¿ç”¨å¼ºç”Ÿæˆå…ˆéªŒçš„æ‰©æ•£æ–¹æ³•è™½ç„¶ç²¾åº¦é«˜ï¼Œä½†è®¡ç®—è´Ÿæ‹…å¤§ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬ä»å•ç›®æ·±åº¦ä¼°è®¡æ¨¡å‹ä¸­æ‰¾åˆ°ä¼ªæ·±åº¦ä¿¡æ¯ï¼Œå®ƒèƒ½æä¾›å¯¹è¯­ä¹‰çš„é‡è¦ç†è§£ï¼Œå¿«é€Ÿæ­ç¤ºç›®æ ‡ç‰©ä½“å’ŒèƒŒæ™¯ä¹‹é—´çš„ç©ºé—´å·®å¼‚ã€‚å—æ­¤ç°è±¡çš„å¯å‘ï¼Œæˆ‘ä»¬å‘ç°äº†ç§°ä¸ºæ·±åº¦å®Œæ•´æ€§å…ˆéªŒçš„æ–°è§è§£ï¼šåœ¨ä¼ªæ·±åº¦å›¾ä¸­ï¼Œå‰æ™¯ç‰©ä½“å…·æœ‰ç¨³å®šçš„æ·±åº¦å€¼ï¼Œå…¶æ–¹å·®è¿œä½äºæ··ä¹±çš„èƒŒæ™¯æ¨¡å¼ã€‚ä¸ºäº†åˆ©ç”¨è¿™ä¸€å…ˆéªŒï¼Œæˆ‘ä»¬æå‡ºäº†æ·±åº¦èåˆç½‘ç»œï¼ˆPDFNetï¼‰ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬çš„ç½‘ç»œå»ºç«‹å¤šæ¨¡æ€äº¤äº’å»ºæ¨¡ï¼Œé€šè¿‡æ·±åº¦èåˆRGBå’Œä¼ªæ·±åº¦ç‰¹å¾æ¥å®ç°æ·±åº¦å¼•å¯¼çš„ç»“æ„æ„ŸçŸ¥ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§æ–°çš„æ·±åº¦å®Œæ•´æ€§å…ˆéªŒæŸå¤±ï¼Œä»¥æ˜¾å¼åœ°å¼ºåˆ¶åˆ†å‰²ç»“æœä¸­çš„æ·±åº¦ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå…·æœ‰è‡ªé€‚åº”è¡¥ä¸é€‰æ‹©çš„ç²¾ç»†ç²’åº¦æ„ŸçŸ¥å¢å¼ºæ¨¡å—ï¼Œä»¥æ‰§è¡Œè¾¹ç•Œæ•æ„Ÿçš„ç»†èŠ‚ä¼˜åŒ–ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒPDFNetä»…ä½¿ç”¨94Må‚æ•°ï¼ˆä¸åˆ°æ‰©æ•£æ¨¡å‹çš„11%ï¼‰ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¸ä»…ä¼˜äºæ‰€æœ‰éæ‰©æ•£æ–¹æ³•ï¼Œè¿˜è¶…è¶Šäº†ä¸€äº›æ‰©æ•£æ–¹æ³•ã€‚ä»£ç å·²åŒ…å«åœ¨è¡¥å……ææ–™ä¸­ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06100v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºä¼ªæ·±åº¦ä¿¡æ¯çš„é«˜ç²¾åº¦äºŒå€¼å›¾åƒåˆ†å‰²æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡èåˆRGBå’Œä¼ªæ·±åº¦ç‰¹å¾ï¼Œå»ºç«‹å¤šæ¨¡æ€äº¤äº’æ¨¡å‹ï¼Œå®ç°æ·±åº¦å¼•å¯¼çš„ç»“æ„æ„ŸçŸ¥ï¼Œå¹¶å¼•å…¥æ·±åº¦å®Œæ•´æ€§å…ˆéªŒæŸå¤±æ¥å¢å¼ºåˆ†å‰²ç»“æœçš„æ·±åº¦ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œè¿˜è®¾è®¡äº†ä¸€ä¸ªå…·æœ‰è‡ªé€‚åº”è¡¥ä¸é€‰æ‹©çš„ç²¾ç»†æ„ŸçŸ¥å¢å¼ºæ¨¡å—ï¼Œä»¥æ‰§è¡Œè¾¹ç•Œæ•æ„Ÿçš„ç»†èŠ‚ä¼˜åŒ–ã€‚è¯¥æ–¹æ³•å‚æ•°å°‘ï¼Œæ€§èƒ½ä¼˜å¼‚ï¼Œä¼˜äºæ‰€æœ‰éæ‰©æ•£æ–¹æ³•ï¼Œå¹¶è¶…è¶Šäº†ä¸€äº›æ‰©æ•£æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é«˜ç²¾åº¦äºŒå€¼å›¾åƒåˆ†å‰²ï¼ˆDISï¼‰æ˜¯ä»é«˜åˆ†è¾¨ç‡å›¾åƒä¸­æå–ç»†ç²’åº¦å¯¹è±¡çš„ä»»åŠ¡ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é¢ä¸´éæ‰©æ•£å’Œæ‰©æ•£æ–¹æ³•çš„æƒè¡¡ï¼šéæ‰©æ•£æ–¹æ³•æ•ˆç‡é«˜ä½†å­˜åœ¨è¯¯æ£€æˆ–æ¼æ£€ï¼Œæ‰©æ•£æ–¹æ³•å‡†ç¡®ç‡é«˜ä½†è®¡ç®—è´Ÿæ‹…å¤§ã€‚</li>
<li>ä¼ªæ·±åº¦ä¿¡æ¯å¯ä»¥æä¾›å¯¹ç›®æ ‡å¯¹è±¡å’ŒèƒŒæ™¯ç©ºé—´å·®å¼‚çš„å…³é”®è¯­ä¹‰ç†è§£ã€‚</li>
<li>æå‡ºäº†æ·±åº¦å®Œæ•´æ€§å…ˆéªŒï¼ˆDIPï¼‰ï¼ŒæŒ‡å‡ºå‰æ™¯å¯¹è±¡åœ¨ä¼ªæ·±åº¦å›¾ä¸­å…·æœ‰ç¨³å®šçš„æ·±åº¦å€¼ï¼Œæ–¹å·®è¾ƒä½ã€‚</li>
<li>ä»‹ç»äº†åŸºäºæ·±åº¦å®Œæ•´æ€§å…ˆéªŒçš„PDFNetç½‘ç»œï¼Œé€šè¿‡èåˆRGBå’Œä¼ªæ·±åº¦ç‰¹å¾å®ç°æ·±åº¦å¼•å¯¼çš„ç»“æ„æ„ŸçŸ¥ã€‚</li>
<li>PDFNetå¼•å…¥äº†ä¸€ç§æ–°çš„æ·±åº¦å®Œæ•´æ€§å…ˆéªŒæŸå¤±ï¼Œä»¥æ˜ç¡®åŠ å¼ºåˆ†å‰²ç»“æœçš„æ·±åº¦ä¸€è‡´æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06100">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2503.06100v4/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2503.06100v4/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2503.06100v4/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2503.06100v4/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2503.06100v4/page_5_0.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Toward-a-Robust-R2D2-Paradigm-for-Radio-interferometric-Imaging-Revisiting-Deep-Neural-Network-Training-and-Architecture"><a href="#Toward-a-Robust-R2D2-Paradigm-for-Radio-interferometric-Imaging-Revisiting-Deep-Neural-Network-Training-and-Architecture" class="headerlink" title="Toward a Robust R2D2 Paradigm for Radio-interferometric Imaging:   Revisiting Deep Neural Network Training and Architecture"></a>Toward a Robust R2D2 Paradigm for Radio-interferometric Imaging:   Revisiting Deep Neural Network Training and Architecture</h2><p><strong>Authors:Amir Aghabiglou, Chung San Chu, Chao Tang, Arwa Dabbech, Yves Wiaux</strong></p>
<p>The R2D2 Deep Neural Network (DNN) series was recently introduced for image formation in radio interferometry. It can be understood as a learned version of CLEAN, whose minor cycles are substituted with DNNs. We revisit R2D2 on the grounds of series convergence, training methodology, and DNN architecture, improving its robustness in terms of generalizability beyond training conditions, capability to deliver high data fidelity, and epistemic uncertainty. First, while still focusing on telescope-specific training, we enhance the learning process by randomizing Fourier sampling integration times, incorporating multiscan multinoise configurations, and varying imaging settings, including pixel resolution and visibility-weighting scheme. Second, we introduce a convergence criterion whereby the reconstruction process stops when the data residual is compatible with noise, rather than simply using all available DNNs. This not only increases the reconstruction efficiency by reducing its computational cost, but also refines training by pruning out the data&#x2F;image pairs for which optimal data fidelity is reached before training the next DNN. Third, we substitute R2D2â€™s early U-Net DNN with a novel architecture (U-WDSR) combining U-Net and WDSR, which leverages wide activation, dense skip connections, weight normalization, and low-rank convolution to improve feature reuse and reconstruction precision. As previously, R2D2 was trained for monochromatic intensity imaging with the Very Large Array at fixed $512 \times 512$ image size. Simulations on a wide range of inverse problems and a case study on real data reveal that the new R2D2 model consistently outperforms its earlier version in image reconstruction quality, data fidelity, and epistemic uncertainty. </p>
<blockquote>
<p>R2D2æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰ç³»åˆ—æœ€è¿‘è¢«å¼•å…¥åˆ°å°„ç”µå¹²æ¶‰æ³•æˆåƒä¸­ã€‚å¯ä»¥å°†å…¶ç†è§£ä¸ºCLEANçš„ä¹ å¾—ç‰ˆæœ¬ï¼Œå…¶å°å‘¨æœŸè¢«DNNæ›¿ä»£ã€‚æˆ‘ä»¬ä»åºåˆ—æ”¶æ•›ã€è®­ç»ƒæ–¹æ³•å’ŒDNNæ¶æ„ç­‰æ–¹é¢é‡æ–°å®¡è§†R2D2ï¼Œæé«˜äº†å…¶åœ¨è¶…è¶Šè®­ç»ƒæ¡ä»¶çš„ä¸€èˆ¬æ€§ã€æä¾›é«˜æ•°æ®ä¿çœŸæ€§å’Œè®¤è¯†è®ºä¸ç¡®å®šæ€§æ–¹é¢çš„ç¨³å¥æ€§ã€‚é¦–å…ˆï¼Œåœ¨ä»å…³æ³¨æœ›è¿œé•œç‰¹å®šè®­ç»ƒçš„åŒæ—¶ï¼Œæˆ‘ä»¬é€šè¿‡éšæœºå‚…é‡Œå¶é‡‡æ ·ç§¯åˆ†æ—¶é—´ã€èå…¥å¤šæ‰«æå¤šå™ªå£°é…ç½®ä»¥åŠå˜åŒ–æˆåƒè®¾ç½®ï¼ˆåŒ…æ‹¬åƒç´ åˆ†è¾¨ç‡å’Œå¯è§æ€§åŠ æƒæ–¹æ¡ˆï¼‰æ¥å¢å¼ºå­¦ä¹ è¿‡ç¨‹ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ”¶æ•›æ ‡å‡†ï¼Œå³å½“æ•°æ®æ®‹å·®ä¸å™ªå£°å…¼å®¹æ—¶ï¼Œé‡å»ºè¿‡ç¨‹å°†åœæ­¢ï¼Œè€Œä¸æ˜¯ç®€å•åœ°ä½¿ç”¨æ‰€æœ‰å¯ç”¨çš„DNNã€‚è¿™ä¸ä»…é€šè¿‡å‡å°‘è®¡ç®—æˆæœ¬æé«˜äº†é‡å»ºæ•ˆç‡ï¼Œè€Œä¸”é€šè¿‡å‰”é™¤é‚£äº›å·²ç»åœ¨è®­ç»ƒä¸‹ä¸€ä¸ªDNNä¹‹å‰è¾¾åˆ°æœ€ä½³æ•°æ®ä¿çœŸåº¦çš„æ•°æ®&#x2F;å›¾åƒå¯¹æ¥ä¼˜åŒ–è®­ç»ƒã€‚ç¬¬ä¸‰ï¼Œæˆ‘ä»¬ç”¨ä¸€ç§æ–°å‹æ¶æ„ï¼ˆU-WDSRï¼‰æ›¿ä»£R2D2æ—©æœŸçš„U-Net DNNï¼Œè¯¥æ¶æ„ç»“åˆäº†U-Netå’ŒWDSRï¼Œåˆ©ç”¨å®½æ¿€æ´»ã€å¯†é›†è·³è·ƒè¿æ¥ã€æƒé‡å½’ä¸€åŒ–å’Œä½ç§©å·ç§¯æ¥æé«˜ç‰¹å¾å¤ç”¨å’Œé‡å»ºç²¾åº¦ã€‚ä¸ä¹‹å‰ä¸€æ ·ï¼ŒR2D2ä»¥å›ºå®š$512 \times 512$å›¾åƒå¤§å°ä½¿ç”¨è¶…å¤§é˜µåˆ—è¿›è¡Œå•è‰²å¼ºåº¦æˆåƒè®­ç»ƒã€‚åœ¨å¹¿æ³›èŒƒå›´çš„åé—®é¢˜æ¨¡æ‹Ÿå’ŒçœŸå®æ•°æ®æ¡ˆä¾‹ç ”ç©¶ä¸­çš„ç»“æœè¡¨æ˜ï¼Œæ–°å‹R2D2æ¨¡å‹åœ¨å›¾åƒé‡å»ºè´¨é‡ã€æ•°æ®ä¿çœŸæ€§å’Œè®¤è¯†è®ºä¸ç¡®å®šæ€§æ–¹é¢å‡ä¼˜äºå…¶æ—©æœŸç‰ˆæœ¬ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02554v2">PDF</a> 18 pages, 6 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>R2D2æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰ç³»åˆ—æœ€è¿‘è¢«å¼•å…¥å°„ç”µå¹²æ¶‰ä»ªå›¾åƒå½¢æˆä¸­ã€‚å¯ç†è§£ä¸ºå¯¹CLEANçš„æ·±åº¦å­¦ä¹ ç‰ˆæœ¬ï¼Œå…¶å°å‘¨æœŸè¢«DNNæ›¿ä»£ã€‚æœ¬æ–‡ä»åºåˆ—æ”¶æ•›ã€è®­ç»ƒæ–¹æ³•å’ŒDNNæ¶æ„ç­‰æ–¹é¢é‡æ–°å®¡è§†R2D2ï¼Œæé«˜äº†å…¶åœ¨è¶…è¶Šè®­ç»ƒæ¡ä»¶çš„ä¸€èˆ¬åŒ–èƒ½åŠ›ã€é«˜æ•°æ®ä¿çœŸåº¦å’Œè®¤çŸ¥ä¸ç¡®å®šæ€§æ–¹é¢çš„ç¨³å¥æ€§ã€‚é¦–å…ˆï¼Œåœ¨æœ›è¿œé•œç‰¹å®šè®­ç»ƒçš„åŸºç¡€ä¸Šï¼Œé€šè¿‡éšæœºå‚…é‡Œå¶é‡‡æ ·ç§¯åˆ†æ—¶é—´ã€å¼•å…¥å¤šæ‰«æå¤šå™ªå£°é…ç½®å’Œå˜åŒ–æˆåƒè®¾ç½®ï¼ˆåŒ…æ‹¬åƒç´ åˆ†è¾¨ç‡å’Œå¯è§æ€§åŠ æƒæ–¹æ¡ˆï¼‰æ¥å¢å¼ºå­¦ä¹ è¿‡ç¨‹ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ”¶æ•›å‡†åˆ™ï¼Œå³å½“æ•°æ®æ®‹å·®ä¸å™ªå£°å…¼å®¹æ—¶ï¼Œé‡å»ºè¿‡ç¨‹å°±ä¼šåœæ­¢ï¼Œè€Œä¸æ˜¯ç®€å•åœ°ä½¿ç”¨æ‰€æœ‰å¯ç”¨çš„DNNsã€‚è¿™ä¸ä»…æé«˜äº†é‡å»ºæ•ˆç‡å¹¶é™ä½äº†è®¡ç®—æˆæœ¬ï¼Œè€Œä¸”è¿˜é€šè¿‡å‰”é™¤é‚£äº›åœ¨è¾¾åˆ°æœ€ä½³æ•°æ®ä¿çœŸåº¦ä¹‹å‰å°±å·²è®­ç»ƒå¥½çš„ä¸‹ä¸€ä¸ªDNNçš„æ•°æ®&#x2F;å›¾åƒå¯¹ï¼Œä»è€Œä¼˜åŒ–äº†è®­ç»ƒã€‚ç¬¬ä¸‰ï¼Œæˆ‘ä»¬ç”¨ä¸€ç§æ–°å‹æ¶æ„U-WDSRæ›¿ä»£äº†R2D2æ—©æœŸçš„U-Net DNNï¼Œè¯¥æ¶æ„ç»“åˆäº†U-Netå’ŒWDSRï¼Œåˆ©ç”¨å®½æ¿€æ´»ã€å¯†é›†è·³è·ƒè¿æ¥ã€æƒé‡å½’ä¸€åŒ–å’Œä½ç§©å·ç§¯æ¥æé«˜ç‰¹å¾å¤ç”¨å’Œé‡å»ºç²¾åº¦ã€‚æ¨¡æ‹Ÿå¹¿æ³›åé—®é¢˜å’ŒçœŸå®æ•°æ®çš„æ¡ˆä¾‹ç ”ç©¶è¡¨æ˜ï¼Œæ–°R2D2æ¨¡å‹åœ¨å›¾åƒé‡å»ºè´¨é‡ã€æ•°æ®ä¿çœŸåº¦å’Œè®¤çŸ¥ä¸ç¡®å®šæ€§æ–¹é¢å‡ä¼˜äºæ—©æœŸç‰ˆæœ¬ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>R2D2ç³»åˆ—ç½‘ç»œåœ¨å°„ç”µå¹²æ¶‰ä»ªå›¾åƒå½¢æˆä¸­å¼•å…¥æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰ï¼Œæ˜¯å¯¹CLEANç®—æ³•çš„æ·±åº¦å­¦ä¹ æ”¹è¿›ã€‚</li>
<li>é€šè¿‡éšæœºå‚…é‡Œå¶é‡‡æ ·ç§¯åˆ†æ—¶é—´ã€å¤šæ‰«æå¤šå™ªå£°é…ç½®å’Œå˜åŒ–çš„æˆåƒè®¾ç½®å¢å¼ºäº†R2D2çš„ç¨³å¥æ€§ã€‚</li>
<li>å¼•å…¥æ–°çš„æ”¶æ•›å‡†åˆ™ï¼ŒåŸºäºæ•°æ®æ®‹å·®ä¸å™ªå£°çš„å…¼å®¹æ€§æ¥åœæ­¢é‡å»ºè¿‡ç¨‹ï¼Œæé«˜é‡å»ºæ•ˆç‡å’Œè®¡ç®—æˆæœ¬æ•ˆç›Šã€‚</li>
<li>ç”¨æ–°å‹æ¶æ„U-WDSRæ›¿ä»£R2D2æ—©æœŸçš„U-Net DNNï¼Œç»“åˆäº†U-Netå’ŒWDSRçš„ä¼˜ç‚¹ï¼Œæé«˜äº†ç‰¹å¾å¤ç”¨å’Œé‡å»ºç²¾åº¦ã€‚</li>
<li>æ–°R2D2æ¨¡å‹åœ¨å›¾åƒé‡å»ºè´¨é‡ã€æ•°æ®ä¿çœŸåº¦å’Œè®¤çŸ¥ä¸ç¡®å®šæ€§æ–¹é¢è¡¨ç°ä¼˜è¶Šã€‚</li>
<li>è®­ç»ƒè¿‡ç¨‹é’ˆå¯¹å¹¿æ³›çš„é€†é—®é¢˜å’ŒçœŸå®æ•°æ®è¿›è¡Œäº†æ¨¡æ‹Ÿå’Œå®è¯ç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02554">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2503.02554v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2503.02554v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2503.02554v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="IM360-Large-scale-Indoor-Mapping-with-360-Cameras"><a href="#IM360-Large-scale-Indoor-Mapping-with-360-Cameras" class="headerlink" title="IM360: Large-scale Indoor Mapping with 360 Cameras"></a>IM360: Large-scale Indoor Mapping with 360 Cameras</h2><p><strong>Authors:Dongki Jung, Jaehoon Choi, Yonghan Lee, Dinesh Manocha</strong></p>
<p>We present a novel 3D mapping pipeline for large-scale indoor environments. To address the significant challenges in large-scale indoor scenes, such as prevalent occlusions and textureless regions, we propose IM360, a novel approach that leverages the wide field of view of omnidirectional images and integrates the spherical camera model into the Structure-from-Motion (SfM) pipeline. Our SfM utilizes dense matching features specifically designed for 360 images, demonstrating superior capability in image registration. Furthermore, with the aid of mesh-based neural rendering techniques, we introduce a texture optimization method that refines texture maps and accurately captures view-dependent properties by combining diffuse and specular components. We evaluate our pipeline on large-scale indoor scenes, demonstrating its effectiveness in real-world scenarios. In practice, IM360 demonstrates superior performance, achieving a 3.5 PSNR increase in textured mesh reconstruction. We attain state-of-the-art performance in terms of camera localization and registration on Matterport3D and Stanford2D3D. </p>
<blockquote>
<p>æˆ‘ä»¬é’ˆå¯¹å¤§è§„æ¨¡å®¤å†…ç¯å¢ƒæå‡ºäº†ä¸€ç§æ–°å‹çš„ä¸‰ç»´æ˜ å°„æµç¨‹ã€‚ä¸ºäº†è§£å†³å¤§è§„æ¨¡å®¤å†…åœºæ™¯æ‰€é¢ä¸´çš„å·¨å¤§æŒ‘æˆ˜ï¼Œå¦‚æ™®éçš„é®æŒ¡å’Œçº¹ç†ç¼ºå¤±åŒºåŸŸï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•IM360ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å…¨æ™¯å›¾åƒçš„å®½è§†é‡ï¼Œå¹¶å°†çƒå½¢ç›¸æœºæ¨¡å‹èå…¥è¿åŠ¨æ¢å¤ç»“æ„ï¼ˆSfMï¼‰æµç¨‹ä¸­ã€‚æˆ‘ä»¬çš„SfMåˆ©ç”¨ä¸“ä¸ºå…¨æ™¯å›¾åƒè®¾è®¡çš„å¯†é›†åŒ¹é…ç‰¹å¾ï¼Œåœ¨å›¾åƒé…å‡†æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œå€ŸåŠ©åŸºäºç½‘æ ¼çš„ç¥ç»ç½‘ç»œæ¸²æŸ“æŠ€æœ¯ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§çº¹ç†ä¼˜åŒ–æ–¹æ³•ï¼Œç”¨äºæ”¹è¿›çº¹ç†æ˜ å°„ï¼Œé€šè¿‡ç»“åˆæ¼«åå°„å’Œé•œé¢æˆåˆ†å‡†ç¡®æ•æ‰è§†å›¾ç›¸å…³å±æ€§ã€‚æˆ‘ä»¬åœ¨å¤§è§„æ¨¡å®¤å†…åœºæ™¯ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æµç¨‹ï¼Œè¯æ˜äº†å…¶åœ¨ç°å®åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚åœ¨å®è·µä¸­ï¼ŒIM30å±•ç°äº†å“è¶Šçš„æ€§èƒ½ï¼Œåœ¨çº¹ç†ç½‘æ ¼é‡å»ºæ–¹é¢å®ç°äº†3.5çš„å³°å€¼ä¿¡å™ªæ¯”å¢åŠ ã€‚æˆ‘ä»¬åœ¨Matterport3Då’ŒStanford2D3Dä¸Šçš„ç›¸æœºå®šä½å’Œé…å‡†æ–¹é¢è¾¾åˆ°äº†ä¸šç•Œå…ˆè¿›æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12545v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç”¨äºå¤§è§„æ¨¡å®¤å†…ç¯å¢ƒçš„æ–°å‹ä¸‰ç»´æ˜ å°„ç®¡é“IM360ï¼Œå®ƒåˆ©ç”¨å…¨æ™¯å›¾åƒçš„å¹¿è§†è§’å¹¶å°†çƒå½¢ç›¸æœºæ¨¡å‹èå…¥SfMï¼ˆä»è¿åŠ¨ä¸­æ¢å¤ç»“æ„ï¼‰ç®¡é“ï¼Œä»¥åº”å¯¹å¤§è§„æ¨¡å®¤å†…åœºæ™¯ä¸­çš„é®æŒ¡å’Œçº¹ç†ç¼ºå¤±ç­‰æŒ‘æˆ˜ã€‚é€šè¿‡åˆ©ç”¨ä¸“ä¸ºå…¨æ™¯å›¾åƒè®¾è®¡çš„å¯†é›†åŒ¹é…ç‰¹å¾ï¼ŒIM360åœ¨å›¾åƒæ³¨å†Œæ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œå€ŸåŠ©åŸºäºç½‘æ ¼çš„ç¥ç»æ¸²æŸ“æŠ€æœ¯ï¼Œå¼•å…¥äº†ä¸€ç§çº¹ç†ä¼˜åŒ–æ–¹æ³•ï¼Œå¯ä¼˜åŒ–çº¹ç†æ˜ å°„å¹¶å‡†ç¡®æ•æ‰ä¸è§†å›¾ç›¸å…³çš„å±æ€§ï¼Œé€šè¿‡ç»“åˆæ¼«åå°„å’Œé•œé¢æˆåˆ†å®ç°é«˜è´¨é‡çš„çº¹ç†é‡å»ºã€‚åœ¨å¤§å‹å®¤å†…åœºæ™¯çš„å®è·µä¸­ï¼ŒIM360è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œå®ç°äº†çº¹ç†ç½‘æ ¼é‡å»ºçš„PSNRå€¼å¢åŠ 3.5ã€‚åœ¨Matterport3Då’ŒStanford2D3Dä¸Šçš„ç›¸æœºå®šä½å’Œæ³¨å†Œæ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†æ–°å‹ä¸‰ç»´æ˜ å°„ç®¡é“IM360ï¼Œç”¨äºå¤§è§„æ¨¡å®¤å†…ç¯å¢ƒã€‚</li>
<li>IM360åˆ©ç”¨å…¨æ™¯å›¾åƒçš„å¹¿è§†è§’å’Œçƒå½¢ç›¸æœºæ¨¡å‹åº”å¯¹å®¤å†…åœºæ™¯çš„é®æŒ¡å’Œçº¹ç†ç¼ºå¤±æŒ‘æˆ˜ã€‚</li>
<li>IM360åœ¨SfMç®¡é“ä¸­é‡‡ç”¨å¯†é›†åŒ¹é…ç‰¹å¾ï¼Œå¢å¼ºå›¾åƒæ³¨å†Œèƒ½åŠ›ã€‚</li>
<li>å¼•å…¥åŸºäºç½‘æ ¼çš„ç¥ç»æ¸²æŸ“æŠ€æœ¯çš„çº¹ç†ä¼˜åŒ–æ–¹æ³•ï¼Œä¼˜åŒ–çº¹ç†æ˜ å°„å¹¶æ•æ‰è§†å›¾ç›¸å…³å±æ€§ã€‚</li>
<li>IM360å®ç°äº†é«˜è´¨é‡çš„çº¹ç†é‡å»ºï¼ŒPSNRå€¼å¢åŠ 3.5ã€‚</li>
<li>åœ¨Matterport3Då’ŒStanford2D3Dä¸Šçš„ç›¸æœºå®šä½å’Œæ³¨å†Œæ–¹é¢è¾¾åˆ°æœ€å…ˆè¿›æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12545">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2502.12545v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2502.12545v3/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2502.12545v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2502.12545v3/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2502.12545v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="LEAD-Large-Foundation-Model-for-EEG-Based-Alzheimerâ€™s-Disease-Detection"><a href="#LEAD-Large-Foundation-Model-for-EEG-Based-Alzheimerâ€™s-Disease-Detection" class="headerlink" title="LEAD: Large Foundation Model for EEG-Based Alzheimerâ€™s Disease Detection"></a>LEAD: Large Foundation Model for EEG-Based Alzheimerâ€™s Disease Detection</h2><p><strong>Authors:Yihe Wang, Nan Huang, Nadia Mammone, Marco Cecchi, Xiang Zhang</strong></p>
<p>Electroencephalography (EEG) provides a non-invasive, highly accessible, and cost-effective approach for detecting Alzheimerâ€™s disease (AD). However, existing methods, whether based on handcrafted feature engineering or standard deep learning, face two major challenges: 1) the lack of large-scale EEG-AD datasets for robust representation learning, and 2) the absence of a dedicated deep learning pipeline for subject-level detection, which is more clinically meaningful than the commonly used sample-level detection. To address these gaps, we have curated the worldâ€™s largest EEG-AD corpus to date, comprising 2,255 subjects. Leveraging this unique data corpus, we propose LEAD, the first large-scale foundation model for EEG analysis in dementia. Our approach provides an innovative framework for subject-level AD detection, including: 1) a comprehensive preprocessing pipeline such as artifact removal, resampling, and filtering, and a newly proposed multi-scale segmentation strategy, 2) a subject-regularized spatio-temporal transformer trained with a novel subject-level cross-entropy loss and an indices group-shuffling algorithm, and 3) AD-guided contrastive pre-training. We pre-train on 12 datasets (3 AD-related and 9 non-AD) and fine-tune&#x2F;test on 4 AD datasets. Compared with 10 baselines, LEAD consistently obtains superior subject-level detection performance under the challenging subject-independent cross-validation protocol. On the benchmark ADFTD dataset, our model achieves an impressive subject-level Sensitivity of 90.91% under the leave-one-subject-out (LOSO) setting. These results strongly validate the effectiveness of our method for real-world EEG-based AD detection. Source code: <a target="_blank" rel="noopener" href="https://github.com/DL4mHealth/LEAD">https://github.com/DL4mHealth/LEAD</a> </p>
<blockquote>
<p>è„‘ç”µå›¾ï¼ˆEEGï¼‰ä¸ºæ£€æµ‹é˜¿å°”èŒ¨æµ·é»˜ç—…ï¼ˆADï¼‰æä¾›äº†ä¸€ç§éä¾µå…¥æ€§ã€æ˜“äºè·å–ä¸”æˆæœ¬æ•ˆç›Šé«˜çš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ï¼Œæ— è®ºæ˜¯åŸºäºæ‰‹å·¥ç‰¹å¾å·¥ç¨‹è¿˜æ˜¯æ ‡å‡†æ·±åº¦å­¦ä¹ ï¼Œéƒ½é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼š1ï¼‰ç¼ºä¹ç”¨äºç¨³å¥è¡¨ç¤ºå­¦ä¹ çš„å¤§è§„æ¨¡EEG-ADæ•°æ®é›†ï¼›2ï¼‰ç¼ºä¹ç”¨äºä¸»ä½“çº§åˆ«æ£€æµ‹çš„ä¸“ç”¨æ·±åº¦å­¦ä¹ ç®¡é“ï¼Œè¿™åœ¨ä¸´åºŠä¸Šæ¯”å¸¸ç”¨çš„æ ·æœ¬çº§åˆ«æ£€æµ‹æ›´æœ‰æ„ä¹‰ã€‚ä¸ºäº†è§£å†³è¿™äº›å·®è·ï¼Œæˆ‘ä»¬è¿„ä»Šä¸ºæ­¢æ•´ç†äº†ä¸–ç•Œä¸Šæœ€å¤§çš„EEG-ADè¯­æ–™åº“ï¼ŒåŒ…å«2255ä¸ªå—è¯•è€…ã€‚åˆ©ç”¨è¿™ä¸€ç‹¬ç‰¹çš„æ•°æ®é›†ï¼Œæˆ‘ä»¬æå‡ºäº†LEADï¼Œè¿™æ˜¯ç”¨äºç—´å‘†ç—‡è„‘ç”µå›¾åˆ†æçš„å¤§è§„æ¨¡åŸºç¡€æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ºä¸»ä½“çº§ADæ£€æµ‹æä¾›äº†ä¸€ä¸ªåˆ›æ–°æ¡†æ¶ï¼ŒåŒ…æ‹¬ï¼š1ï¼‰å…¨é¢çš„é¢„å¤„ç†ç®¡é“ï¼Œå¦‚ä¼ªè¿¹å»é™¤ã€é‡æ–°é‡‡æ ·å’Œè¿‡æ»¤ï¼Œä»¥åŠæ–°æå‡ºçš„å¤šå°ºåº¦åˆ†å‰²ç­–ç•¥ï¼›2ï¼‰ä½¿ç”¨æ–°å‹ä¸»ä½“çº§äº¤å‰ç†µæŸå¤±å’ŒæŒ‡æ•°ç»„æ‰“ä¹±ç®—æ³•è®­ç»ƒçš„å—è¯•è€…è§„èŒƒåŒ–æ—¶ç©ºå˜å‹å™¨ï¼›3ï¼‰ADå¼•å¯¼å¯¹æ¯”é¢„è®­ç»ƒã€‚æˆ‘ä»¬åœ¨12ä¸ªæ•°æ®é›†ï¼ˆå…¶ä¸­3ä¸ªä¸ADç›¸å…³ï¼Œ9ä¸ªä¸ADä¸ç›¸å…³ï¼‰ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶åœ¨4ä¸ªADæ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒ&#x2F;æµ‹è¯•ã€‚ä¸10ä¸ªåŸºçº¿ç›¸æ¯”ï¼ŒLEADåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å—è¯•è€…ç‹¬ç«‹äº¤å‰éªŒè¯åè®®ä¸‹å§‹ç»ˆè·å¾—æ›´ä¼˜ç§€çš„ä¸»ä½“çº§æ£€æµ‹æ€§èƒ½ã€‚åœ¨åŸºå‡†ADFTDæ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨ç•™å‡ºä¸€ä½å—è¯•è€…ï¼ˆLOSOï¼‰è®¾ç½®ä¸‹è¾¾åˆ°äº†ä»¤äººå°è±¡æ·±åˆ»çš„ä¸»ä½“çº§æ•æ„Ÿåº¦90.91%ã€‚è¿™äº›ç»“æœå¼ºçƒˆéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨ç°å®ä¸–ç•Œä¸­åŸºäºEEGçš„ADæ£€æµ‹ä¸­çš„æœ‰æ•ˆæ€§ã€‚æºä»£ç ï¼š<a target="_blank" rel="noopener" href="https://github.com/DL4mHealth/LEAD">https://github.com/DL4mHealth/LEAD</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01678v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åˆ©ç”¨è„‘ç”µå›¾ï¼ˆEEGï¼‰è¿›è¡Œé˜¿å°”èŒ¨æµ·é»˜ç—…ï¼ˆADï¼‰æ£€æµ‹çš„éä¾µå…¥æ€§ã€é«˜å¯åŠæ€§å’Œæˆæœ¬æ•ˆç›Šæ–¹æ³•ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•çš„æŒ‘æˆ˜ï¼Œç ”ç©¶å›¢é˜Ÿæ„å»ºäº†è¿„ä»Šä¸ºæ­¢ä¸–ç•Œä¸Šæœ€å¤§çš„EEG-ADè¯­æ–™åº“ï¼Œå¹¶æå‡ºäº†LEADæ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä¸ºç—´å‘†ç—‡è„‘ç”µå›¾åˆ†ææä¾›äº†å¤§è§„æ¨¡æ¡†æ¶ï¼Œç”¨äºä¸»ä½“å±‚é¢çš„ADæ£€æµ‹ã€‚è¯¥ç ”ç©¶é‡‡ç”¨äº†ä¸€ç³»åˆ—æŠ€æœ¯ï¼ŒåŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€ä¸»ä½“æ­£åˆ™åŒ–æ—¶ç©ºå˜æ¢å™¨è®­ç»ƒã€ä»¥åŠADå¼•å¯¼å¯¹æ¯”é¢„è®­ç»ƒã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒLEADæ¨¡å‹åœ¨ä¸»ä½“å±‚é¢æ£€æµ‹ADçš„æ€§èƒ½ä¼˜äºå…¶ä»–åŸºçº¿æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>EEGä¸ºé˜¿å°”èŒ¨æµ·é»˜ç—…æ£€æµ‹æä¾›äº†éä¾µå…¥æ€§ã€é«˜å¯åŠæ€§å’Œæˆæœ¬æ•ˆç›Šçš„æ–¹æ³•ã€‚</li>
<li>ç°æœ‰EEG-ADæ–¹æ³•é¢ä¸´æ•°æ®é›†è§„æ¨¡ä¸è¶³å’Œç¼ºä¹ä¸“ç”¨æ·±åº¦å­¦ä¹ ç®¡é“çš„æŒ‘æˆ˜ã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿæ„å»ºäº†ä¸–ç•Œä¸Šæœ€å¤§çš„EEG-ADè¯­æ–™åº“ï¼ŒåŒ…å«2,255ä¸ªä¸»ä½“ã€‚</li>
<li>æå‡ºäº†LEADæ¨¡å‹ï¼Œä¸€ä¸ªç”¨äºç—´å‘†ç—‡è„‘ç”µå›¾åˆ†æçš„å¤§è§„æ¨¡æ¡†æ¶ã€‚</li>
<li>LEADæ¨¡å‹é‡‡ç”¨äº†ä¸€ç³»åˆ—æŠ€æœ¯ï¼ŒåŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€ä¸»ä½“æ­£åˆ™åŒ–æ—¶ç©ºå˜æ¢å™¨è®­ç»ƒã€ä»¥åŠADå¼•å¯¼å¯¹æ¯”é¢„è®­ç»ƒã€‚</li>
<li>LEADæ¨¡å‹åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨ç°ä¼˜äºå…¶ä»–åŸºçº¿æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01678">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2502.01678v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2502.01678v3/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2502.01678v3/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_åŒ»å­¦å›¾åƒ/2502.01678v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-03/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-03/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-03/TTS/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_TTS/2401.09512v8/page_0_0.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-03  MLAAD The Multi-Language Audio Anti-Spoofing Dataset
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-03
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-03/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_Diffusion Models/2509.05978v2/page_3_0.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-03  DC-Gen Post-Training Diffusion Acceleration with Deeply Compressed   Latent Space
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-03
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">29997.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
