<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-03  Probing the Critical Point (CritPt) of AI Reasoning a Frontier Physics   Research Benchmark">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-9f0579503e0cfac090ea89352c860b86~resize:0:q75.jpg?source=1f5c5e47&expiration=1760083979&auth_key=1760083979-0-0-00214d1f676c75766e75a4e454e4286d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-03
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-09
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    22k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    90 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-10-03-æ›´æ–°"><a href="#2025-10-03-æ›´æ–°" class="headerlink" title="2025-10-03 æ›´æ–°"></a>2025-10-03 æ›´æ–°</h1><h2 id="Probing-the-Critical-Point-CritPt-of-AI-Reasoning-a-Frontier-Physics-Research-Benchmark"><a href="#Probing-the-Critical-Point-CritPt-of-AI-Reasoning-a-Frontier-Physics-Research-Benchmark" class="headerlink" title="Probing the Critical Point (CritPt) of AI Reasoning: a Frontier Physics   Research Benchmark"></a>Probing the Critical Point (CritPt) of AI Reasoning: a Frontier Physics   Research Benchmark</h2><p><strong>Authors:Minhui Zhu, Minyang Tian, Xiaocheng Yang, Tianci Zhou, Penghao Zhu, Eli Chertkov, Shengyan Liu, Yufeng Du, Lifan Yuan, Ziming Ji, Indranil Das, Junyi Cao, Yufeng Du, Jinchen He, Yifan Su, Jiabin Yu, Yikun Jiang, Yujie Zhang, Chang Liu, Ze-Min Huang, Weizhen Jia, Xinan Chen, Peixue Wu, Yunkai Wang, Juntai Zhou, Yong Zhao, Farshid Jafarpour, Jessie Shelton, Aaron Young, John Bartolotta, Wenchao Xu, Yue Sun, Anjun Chu, Victor Colussi, Chris Akers, Nathan Brooks, Wenbo Fu, Christopher Wilson, Jinchao Zhao, Marvin Qi, Anqi Mu, Yubo Yang, Allen Zang, Yang Lyu, Peizhi Mai, Xuefei Guo, Luyu Gao, Ze Yang, Chi Xue, Dmytro Bandak, YaÃ¯r Hein, Yonatan Kahn, Kevin Zhou, John Drew Wilson, Jarrod T. Reilly, Di Luo, Daniel Inafuku, Hao Tong, Liang Yang, Ruixing Zhang, Xueying Wang, Ofir Press, Nicolas Chia, Eliu Huerta, Hao Peng</strong></p>
<p>While large language models (LLMs) with reasoning capabilities are progressing rapidly on high-school math competitions and coding, can they reason effectively through complex, open-ended challenges found in frontier physics research? And crucially, what kinds of reasoning tasks do physicists want LLMs to assist with? To address these questions, we present the CritPt (Complex Research using Integrated Thinking - Physics Test, pronounced â€œcritical pointâ€), the first benchmark designed to test LLMs on unpublished, research-level reasoning tasks that broadly covers modern physics research areas, including condensed matter, quantum physics, atomic, molecular &amp; optical physics, astrophysics, high energy physics, mathematical physics, statistical physics, nuclear physics, nonlinear dynamics, fluid dynamics and biophysics. CritPt consists of 71 composite research challenges designed to simulate full-scale research projects at the entry level, which are also decomposed to 190 simpler checkpoint tasks for more fine-grained insights. All problems are newly created by 50+ active physics researchers based on their own research. Every problem is hand-curated to admit a guess-resistant and machine-verifiable answer and is evaluated by an automated grading pipeline heavily customized for advanced physics-specific output formats. We find that while current state-of-the-art LLMs show early promise on isolated checkpoints, they remain far from being able to reliably solve full research-scale challenges: the best average accuracy among base models is only 4.0% , achieved by GPT-5 (high), moderately rising to around 10% when equipped with coding tools. Through the realistic yet standardized evaluation offered by CritPt, we highlight a large disconnect between current model capabilities and realistic physics research demands, offering a foundation to guide the development of scientifically grounded AI tools. </p>
<blockquote>
<p>éšç€å…·æœ‰æ¨ç†èƒ½åŠ›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é«˜ä¸­æ•°å­¦ç«èµ›å’Œç¼–ç¨‹æ–¹é¢è¿…é€Ÿè¿›æ­¥ï¼Œå®ƒä»¬æ˜¯å¦èƒ½æœ‰æ•ˆåœ°åº”å¯¹å‰æ²¿ç‰©ç†ç ”ç©¶ä¸­é‡åˆ°çš„å¤æ‚ã€å¼€æ”¾æ€§çš„æŒ‘æˆ˜ï¼Ÿå…³é”®çš„æ˜¯ï¼Œç‰©ç†å­¦å®¶å¸Œæœ›LLMsè¾…åŠ©å®Œæˆå“ªäº›æ¨ç†ä»»åŠ¡ï¼Ÿä¸ºäº†å›ç­”è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†CritPtï¼ˆåˆ©ç”¨ç»¼åˆæ€ç»´è¿›è¡Œå¤æ‚ç ”ç©¶â€”â€”ç‰©ç†æµ‹è¯•ï¼Œä¿—ç§°â€œä¸´ç•Œç‚¹â€ï¼‰ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªé’ˆå¯¹æœªå‘å¸ƒçš„ç ”ç©¶çº§æ¨ç†ä»»åŠ¡è®¾è®¡çš„åŸºå‡†æµ‹è¯•ã€‚å®ƒå¹¿æ³›è¦†ç›–äº†ç°ä»£ç‰©ç†ç ”ç©¶é¢†åŸŸï¼ŒåŒ…æ‹¬å‡èšæ€ç‰©ç†ã€é‡å­åŠ›å­¦ã€åŸå­ã€åˆ†å­å’Œå…‰å­¦ç‰©ç†ã€å¤©ä½“ç‰©ç†ã€é«˜èƒ½ç‰©ç†ã€æ•°å­¦ç‰©ç†ã€ç»Ÿè®¡ç‰©ç†ã€æ ¸ç‰©ç†ã€éçº¿æ€§åŠ¨åŠ›å­¦ã€æµä½“åŠ›å­¦å’Œç”Ÿç‰©ç‰©ç†å­¦ã€‚CritPtç”±71ä¸ªå¤åˆç ”ç©¶æŒ‘æˆ˜ç»„æˆï¼Œæ—¨åœ¨æ¨¡æ‹Ÿå…¥é—¨çº§å…¨å°ºåº¦ç ”ç©¶é¡¹ç›®ï¼ŒåŒæ—¶åˆ†è§£ä¸º190ä¸ªæ›´ç®€å•çš„æ£€æŸ¥ç‚¹ä»»åŠ¡ï¼Œä»¥è·å–æ›´ç²¾ç»†çš„è§è§£ã€‚æ‰€æœ‰é—®é¢˜å‡ç”±50å¤šåæ´»è·ƒçš„ç‰©ç†ç ”ç©¶äººå‘˜åŸºäºä»–ä»¬è‡ªå·±çš„ç ”ç©¶å…¨æ–°åˆ›å»ºã€‚æ¯ä¸ªé—®é¢˜éƒ½æ˜¯æ‰‹å·¥ç­›é€‰çš„ï¼Œä»¥å¾—å‡ºéš¾ä»¥çŒœæµ‹ä¸”æœºå™¨å¯éªŒè¯çš„ç­”æ¡ˆï¼Œå¹¶ç”±é’ˆå¯¹é«˜çº§ç‰©ç†ç‰¹å®šè¾“å‡ºæ ¼å¼è¿›è¡Œå¤§é‡å®šåˆ¶çš„è‡ªåŠ¨è¯„åˆ†ç®¡é“è¿›è¡Œè¯„ä¼°ã€‚æˆ‘ä»¬å‘ç°ï¼Œè™½ç„¶å½“å‰æœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å•ç‹¬çš„æ£€æŸ¥ç‚¹ä¸Šæ˜¾ç¤ºå‡ºæ—©æœŸçš„æ½œåŠ›ï¼Œä½†å®ƒä»¬ä»ç„¶è¿œè¿œä¸èƒ½å¯é åœ°è§£å†³å…¨ç ”ç©¶è§„æ¨¡çš„æŒ‘æˆ˜ï¼šåœ¨åŸºç¡€æ¨¡å‹ä¸­ï¼Œæœ€ä½³å¹³å‡å‡†ç¡®ç‡ä»…ä¸º4.0%ï¼Œç”±GPT-5ï¼ˆé«˜çº§ç‰ˆï¼‰å®ç°ï¼Œå½“é…å¤‡ç¼–ç å·¥å…·æ—¶ï¼Œå‡†ç¡®ç‡é€‚åº¦æé«˜åˆ°çº¦10%ã€‚é€šè¿‡CritPtæä¾›çš„ç°å®ä¸”æ ‡å‡†åŒ–çš„è¯„ä¼°ï¼Œæˆ‘ä»¬çªå‡ºäº†å½“å‰æ¨¡å‹èƒ½åŠ›ä¸ç°å®ç‰©ç†ç ”ç©¶éœ€æ±‚ä¹‹é—´çš„å·¨å¤§å·®è·ï¼Œä¸ºå¼€å‘ç§‘å­¦åŸºç¡€çš„AIå·¥å…·æä¾›äº†æŒ‡å¯¼åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.26574v2">PDF</a> 39 pages, 6 figures, 6 tables</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç‰©ç†ç ”ç©¶é¢†åŸŸçš„æ¨ç†èƒ½åŠ›æ­£å—åˆ°æŒ‘æˆ˜ã€‚ä¸ºæµ‹è¯•LLMsåœ¨æœªå…¬å¼€çš„ç ”ç©¶çº§æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œæ¨å‡ºäº†CritPtè¯„ä¼°å·¥å…·ï¼Œæ¶µç›–ç°ä»£ç‰©ç†ç ”ç©¶é¢†åŸŸã€‚LLMsåœ¨å•ç‹¬çš„æ£€æŸ¥ç‚¹ä¸Šæœ‰æ—©æœŸæ‰¿è¯ºï¼Œä½†åœ¨è§£å†³å…¨é¢çš„ç ”ç©¶çº§æŒ‘æˆ˜ä¸Šä»æœ‰å¾ˆå¤§å·®è·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ­£åœ¨å‚ä¸ç‰©ç†ç ”ç©¶é¢†åŸŸçš„æ¨ç†ä»»åŠ¡ã€‚</li>
<li>CritPtæ˜¯é¦–ä¸ªé’ˆå¯¹LLMsçš„è¯„ä¼°å·¥å…·ï¼Œæ¨¡æ‹ŸçœŸå®çš„ç ”ç©¶çº§ç‰©ç†æ¨ç†ä»»åŠ¡ã€‚</li>
<li>CritPtåŒ…å«71ä¸ªå¤åˆç ”ç©¶æŒ‘æˆ˜å’Œ190ä¸ªç®€å•çš„æ£€æŸ¥ç‚¹ä»»åŠ¡ã€‚</li>
<li>é—®é¢˜ç”±50å¤šåæ´»è·ƒçš„ç‰©ç†ç ”ç©¶å‘˜åŸºäºè‡ªèº«ç ”ç©¶åˆ›å»ºï¼Œæ‰‹å·¥ç¼–åˆ¶ä»¥ç¡®ä¿ç­”æ¡ˆçš„å®¢è§‚æ€§å’Œæœºå™¨å¯éªŒè¯æ€§ã€‚</li>
<li>å½“å‰æœ€å…ˆè¿›çš„LLMsåœ¨è§£å†³å…¨é¢çš„ç ”ç©¶çº§æŒ‘æˆ˜ä¸Šä»æœ‰å¾ˆå¤§å·®è·ï¼ŒåŸºç¡€æ¨¡å‹çš„å¹³å‡å‡†ç¡®ç‡ä»…ä¸º4.0%ï¼Œé…å¤‡ç¼–ç å·¥å…·åæé«˜åˆ°çº¦10%ã€‚</li>
<li>CritPtè¯„ä¼°æ­ç¤ºäº†å½“å‰æ¨¡å‹èƒ½åŠ›ä¸çœŸå®ç‰©ç†ç ”ç©¶éœ€æ±‚ä¹‹é—´çš„å·¨å¤§å·®è·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.26574">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-5994249d89dbe6e0e71bff36c5d7b9a7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760083988&auth_key=1760083988-0-0-c1f8322d6a760cc0fbe4c78308c551a0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-785387efd68ebd5cbc444841d911aad2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760083995&auth_key=1760083995-0-0-e020aaac3e67bc4ce26604639665575f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6bbd6e27faed92b6a96b6a925c78681b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760084001&auth_key=1760084001-0-0-095dcbfb4b8bf7a0a9278416c148fed3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Efficient-and-Transferable-Agentic-Knowledge-Graph-RAG-via-Reinforcement-Learning"><a href="#Efficient-and-Transferable-Agentic-Knowledge-Graph-RAG-via-Reinforcement-Learning" class="headerlink" title="Efficient and Transferable Agentic Knowledge Graph RAG via Reinforcement   Learning"></a>Efficient and Transferable Agentic Knowledge Graph RAG via Reinforcement   Learning</h2><p><strong>Authors:Jinyeop Song, Song Wang, Julian Shun, Yada Zhu</strong></p>
<p>Knowledge-graph retrieval-augmented generation (KG-RAG) couples large language models (LLMs) with structured, verifiable knowledge graphs (KGs) to reduce hallucinations and expose reasoning traces. However, many KG-RAG systems compose multiple LLM modules (e.g planning, reasoning, and responding), inflating inference cost and binding behavior to a specific target KG. To address this, we introduce KG-R1, an agentic KG retrieval-augmented generation (KG-RAG) framework through reinforcement learning (RL). KG-R1 utilizes a single agent that interacts with KGs as its environment, learning to retrieve at each step and incorporating the retrieved information into its reasoning and generation. The process is optimized through end-to-end RL. In controlled experiments across Knowledge-Graph Question Answering (KGQA) benchmarks, our method demonstrates both efficiency and transferability: Using Qwen-2.5-3B, KG-R1 improves answer accuracy with fewer generation tokens than prior multi-module workflow methods that use larger foundation or fine-tuned models. Furthermore, KG-R1 enables plug and play: after training, it maintains strong accuracy on new KGs without modification. These properties make KG-R1 a promising KG-RAG framework for real-world deployment. Our code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/Jinyeop3110/KG-R1">https://github.com/Jinyeop3110/KG-R1</a>. </p>
<blockquote>
<p>çŸ¥è¯†å›¾è°±æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆKG-RAGï¼‰æŠ€æœ¯å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ç»“æ„åŒ–çš„ã€å¯éªŒè¯çš„çŸ¥è¯†å›¾è°±ï¼ˆKGsï¼‰ç›¸ç»“åˆï¼Œä»¥å‡å°‘å¹»è§‰å¹¶æš´éœ²æ¨ç†ç—•è¿¹ã€‚ç„¶è€Œï¼Œè®¸å¤šKG-RAGç³»ç»Ÿç”±å¤šä¸ªLLMæ¨¡å—ï¼ˆå¦‚è§„åˆ’ã€æ¨ç†å’Œå“åº”ï¼‰ç»„æˆï¼Œå¢åŠ äº†æ¨ç†æˆæœ¬ï¼Œå¹¶ä¸”ä¸ç‰¹å®šçš„ç›®æ ‡çŸ¥è¯†å›¾è°±ç´§å¯†ç»‘å®šã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„KG-R1æ™ºèƒ½çŸ¥è¯†å›¾è°±æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆKG-RAGï¼‰æ¡†æ¶ã€‚KG-R1ä½¿ç”¨ä¸€ä¸ªæ™ºèƒ½ä½“ï¼Œå®ƒä¸çŸ¥è¯†å›¾è°±ç¯å¢ƒè¿›è¡Œäº¤äº’ï¼Œå­¦ä¹ åœ¨æ¯ä¸ªæ­¥éª¤ä¸­è¿›è¡Œæ£€ç´¢ï¼Œå¹¶å°†æ£€ç´¢åˆ°çš„ä¿¡æ¯èå…¥å…¶æ¨ç†å’Œç”Ÿæˆè¿‡ç¨‹ä¸­ã€‚è¯¥è¿‡ç¨‹é€šè¿‡ç«¯åˆ°ç«¯çš„RLè¿›è¡Œä¼˜åŒ–ã€‚åœ¨çŸ¥è¯†å›¾è°±é—®ç­”ï¼ˆKGQAï¼‰åŸºå‡†æµ‹è¯•ä¸Šçš„å—æ§å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ—¢æœ‰æ•ˆç‡åˆå…·å¤‡å¯è¿ç§»æ€§ï¼šä½¿ç”¨Qwen-2.5-3Bï¼ŒKG-R1åœ¨æé«˜ç­”æ¡ˆå‡†ç¡®æ€§çš„åŒæ—¶ï¼Œç”Ÿæˆçš„æ ‡è®°æ•°é‡å°‘äºä½¿ç”¨æ›´å¤§åŸºç¡€æ¨¡å‹æˆ–å¾®è°ƒæ¨¡å‹çš„å¤šæ¨¡å—å·¥ä½œæµç¨‹æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒKG-R1å®ç°äº†å³æ’å³ç”¨ï¼šè®­ç»ƒåï¼Œå®ƒåœ¨æ–°çš„çŸ¥è¯†å›¾è°±ä¸Šæ— éœ€ä¿®æ”¹å°±èƒ½ä¿æŒé«˜åº¦çš„å‡†ç¡®æ€§ã€‚è¿™äº›ç‰¹æ€§ä½¿KG-R1æˆä¸ºæœ‰å‰æ™¯çš„çŸ¥è¯†å›¾è°±æ£€ç´¢å¢å¼ºç”Ÿæˆæ¡†æ¶ï¼Œæœ‰æœ›ç”¨äºå®é™…éƒ¨ç½²ã€‚æˆ‘ä»¬çš„ä»£ç å·²å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/Jinyeop3110/KG-R1">https://github.com/Jinyeop3110/KG-R1</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.26383v2">PDF</a> 10 pages, 5 figures. Submitted to ICLR 2026</p>
<p><strong>Summary</strong></p>
<p>KG-R1æ˜¯ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„çŸ¥è¯†å›¾è°±æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆKG-RAGï¼‰æ¡†æ¶ã€‚å®ƒä½¿ç”¨ä¸€ä¸ªæ™ºèƒ½ä½“ä¸ç¯å¢ƒï¼ˆå³çŸ¥è¯†å›¾è°±ï¼‰è¿›è¡Œäº¤äº’ï¼Œå­¦ä¹ åœ¨æ¯ä¸€æ­¥è¿›è¡Œæ£€ç´¢ï¼Œå¹¶å°†æ£€ç´¢åˆ°çš„ä¿¡æ¯èå…¥å…¶æ¨ç†å’Œç”Ÿæˆè¿‡ç¨‹ä¸­ã€‚KG-R1è§£å†³äº†ä¼ ç»ŸKG-RAGç³»ç»Ÿå­˜åœ¨çš„é—®é¢˜ï¼Œå¦‚æ¨ç†æ¨¡å—è¿‡å¤šå¯¼è‡´æ¨ç†æˆæœ¬é«˜æ˜‚ä»¥åŠè¡Œä¸ºç»‘å®šäºç‰¹å®šç›®æ ‡çŸ¥è¯†å›¾è°±çš„é—®é¢˜ã€‚åœ¨çŸ¥è¯†å›¾è°±é—®ç­”ï¼ˆKGQAï¼‰åŸºå‡†æµ‹è¯•ä¸­çš„å®éªŒè¡¨æ˜ï¼ŒKG-R1å…·æœ‰é«˜æ•ˆæ€§å’Œå¯è½¬ç§»æ€§ï¼Œèƒ½æé«˜ç­”æ¡ˆå‡†ç¡®æ€§å¹¶å‡å°‘ç”Ÿæˆä»¤ç‰Œæ•°é‡ã€‚æ­¤å¤–ï¼Œç»è¿‡è®­ç»ƒåï¼ŒKG-R1å¯ä»¥åœ¨æ–°çŸ¥è¯†å›¾è°±ä¸Šä¿æŒé«˜ç²¾åº¦è€Œæ— éœ€ä¿®æ”¹ï¼Œä½¿å…¶æˆä¸ºæœ‰å‰æ™¯çš„KG-RAGæ¡†æ¶ï¼Œé€‚ç”¨äºå®é™…éƒ¨ç½²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>KG-R1æ˜¯ä¸€ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ çš„çŸ¥è¯†å›¾è°±æ£€ç´¢å¢å¼ºç”Ÿæˆæ¡†æ¶ã€‚</li>
<li>KG-R1ä½¿ç”¨ä¸€ä¸ªæ™ºèƒ½ä½“æ¥ä¸çŸ¥è¯†å›¾è°±ç¯å¢ƒè¿›è¡Œäº¤äº’ã€‚</li>
<li>KG-R1è§£å†³äº†ä¼ ç»ŸKG-RAGç³»ç»Ÿæ¨ç†æ¨¡å—è¿‡å¤šã€æ¨ç†æˆæœ¬é«˜æ˜‚çš„é—®é¢˜ã€‚</li>
<li>KG-R1æ¡†æ¶å…·æœ‰ä¼˜åŒ–ç«¯åˆ°ç«¯å¼ºåŒ–å­¦ä¹ çš„èƒ½åŠ›ã€‚</li>
<li>åœ¨çŸ¥è¯†å›¾è°±é—®ç­”åŸºå‡†æµ‹è¯•ä¸­ï¼ŒKG-R1å±•ç°äº†é«˜æ•ˆæ€§å’Œå¯è½¬ç§»æ€§ã€‚</li>
<li>KG-R1èƒ½æé«˜ç­”æ¡ˆå‡†ç¡®æ€§å¹¶å‡å°‘ç”Ÿæˆä»¤ç‰Œæ•°é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.26383">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-4bb2658d30d49e7a20990a844c6123f9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760084009&auth_key=1760084009-0-0-6666f535383e59a624afccbd3f6061db&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a57f2b36b5b6aa52b3aaa3dcce5bb2a1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760084016&auth_key=1760084016-0-0-e137e3e6a3e94b79978cbc9ac8cb01e6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-10ddfae4d76a7f98708a0cce9cae8d66~resize:0:q75.jpg?source=1f5c5e47&expiration=1760084023&auth_key=1760084023-0-0-ab671896d4729bd5e09ee0e506b1f390&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Interactive-Learning-for-LLM-Reasoning"><a href="#Interactive-Learning-for-LLM-Reasoning" class="headerlink" title="Interactive Learning for LLM Reasoning"></a>Interactive Learning for LLM Reasoning</h2><p><strong>Authors:Hehai Lin, Shilei Cao, Minzhi Li, Sudong Wang, Haotian Wu, Linyi Yang, Juepeng Zheng, Chengwei Qin</strong></p>
<p>Existing multi-agent learning approaches have developed interactive training environments to explicitly promote collaboration among multiple Large Language Models (LLMs), thereby constructing stronger multi-agent systems (MAS). However, during inference, they require re-executing the MAS to obtain final solutions, which diverges from human cognition that individuals can enhance their reasoning capabilities through interactions with others and resolve questions independently in the future. To investigate whether multi-agent interaction can enhance LLMsâ€™ independent problem-solving ability, we introduce ILR, a novel co-learning framework for MAS that integrates two key components: Dynamic Interaction and Perception Calibration. Specifically, Dynamic Interaction first adaptively selects either cooperative or competitive strategies depending on question difficulty and model ability. LLMs then exchange information through Idea3 (Idea Sharing, Idea Analysis, and Idea Fusion), an innovative interaction paradigm designed to mimic human discussion, before deriving their respective final answers. In Perception Calibration, ILR employs Group Relative Policy Optimization (GRPO) to train LLMs while integrating one LLMâ€™s reward distribution characteristics into anotherâ€™s reward function, thereby enhancing the cohesion of multi-agent interactions. We validate ILR on three LLMs across two model families of varying scales, evaluating performance on five mathematical benchmarks and one coding benchmark. Experimental results show that ILR consistently outperforms single-agent learning, yielding an improvement of up to 5% over the strongest baseline. We further discover that Idea3 can enhance the robustness of stronger LLMs during multi-agent inference, and dynamic interaction types can boost multi-agent learning compared to pure cooperative or competitive strategies. </p>
<blockquote>
<p>ç°æœ‰çš„å¤šæ™ºèƒ½ä½“å­¦ä¹ æ–¹æ³•å·²ç»æ„å»ºäº†äº¤äº’å¼è®­ç»ƒç¯å¢ƒï¼Œä»¥æ˜ç¡®ä¿ƒè¿›å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¹‹é—´çš„åä½œï¼Œä»è€Œæ„å»ºæ›´å¼ºå¤§çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰ã€‚ç„¶è€Œï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œå®ƒä»¬éœ€è¦é‡æ–°æ‰§è¡ŒMASæ¥è·å¾—æœ€ç»ˆè§£å†³æ–¹æ¡ˆï¼Œè¿™ä¸äººç±»è®¤çŸ¥ä¸ç¬¦ã€‚äººç±»ä¸ªä½“å¯ä»¥é€šè¿‡ä¸ä»–äººäº’åŠ¨å¢å¼ºè‡ªå·±çš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶åœ¨æœªæ¥ç‹¬ç«‹è§£å†³é—®é¢˜ã€‚ä¸ºäº†è°ƒæŸ¥å¤šæ™ºèƒ½ä½“äº’åŠ¨æ˜¯å¦èƒ½å¢å¼ºLLMçš„ç‹¬ç«‹è§£å†³é—®é¢˜çš„èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼•å…¥äº†ILRï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å¤šæ™ºèƒ½ä½“ååŒå­¦ä¹ æ¡†æ¶ï¼Œå®ƒé›†æˆäº†ä¸¤ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ï¼šåŠ¨æ€äº¤äº’å’Œæ„ŸçŸ¥æ ¡å‡†ã€‚å…·ä½“è€Œè¨€ï¼ŒåŠ¨æ€äº¤äº’é¦–å…ˆæ ¹æ®é—®é¢˜çš„éš¾åº¦å’Œæ¨¡å‹èƒ½åŠ›è‡ªé€‚åº”åœ°é€‰æ‹©åˆä½œæˆ–ç«äº‰ç­–ç•¥ã€‚ç„¶åï¼ŒLLMé€šè¿‡æ¨¡ä»¿äººç±»è®¨è®ºçš„åˆ›æ–°äº¤äº’æ¨¡å¼Idea3ï¼ˆæ€æƒ³å…±äº«ã€æ€æƒ³åˆ†æå’Œæ€æƒ³èåˆï¼‰äº¤æ¢ä¿¡æ¯ï¼Œç„¶åå¾—å‡ºå„è‡ªçš„æœ€ç»ˆç­”æ¡ˆã€‚åœ¨æ„ŸçŸ¥æ ¡å‡†ä¸­ï¼ŒILRé‡‡ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰æ¥è®­ç»ƒLLMï¼ŒåŒæ—¶å°†ä¸€ä¸ªLLMçš„å¥–åŠ±åˆ†å¸ƒç‰¹æ€§é›†æˆåˆ°å¦ä¸€ä¸ªLLMçš„å¥–åŠ±å‡½æ•°ä¸­ï¼Œä»è€Œæé«˜å¤šæ™ºèƒ½ä½“äº¤äº’çš„å‡èšåŠ›ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªä¸åŒè§„æ¨¡çš„æ¨¡å‹å®¶æ—ä¸­çš„ä¸‰ä¸ªLLMä¸ŠéªŒè¯äº†ILRçš„æœ‰æ•ˆæ€§ï¼Œé€šè¿‡äº”ä¸ªæ•°å­¦åŸºå‡†æµ‹è¯•å’Œä¸€ä¸ªç¼–ç åŸºå‡†æµ‹è¯•è¯„ä¼°æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒILRå§‹ç»ˆä¼˜äºå•æ™ºèƒ½ä½“å­¦ä¹ ï¼Œä¸æœ€å¼ºåŸºçº¿ç›¸æ¯”æé«˜äº†é«˜è¾¾5%ã€‚æˆ‘ä»¬è¿˜å‘ç°Idea3å¯ä»¥å¢å¼ºæ›´å¼ºLLMåœ¨å¤šæ™ºèƒ½ä½“æ¨ç†è¿‡ç¨‹ä¸­çš„ç¨³å¥æ€§ï¼Œä¸çº¯ç²¹çš„åˆä½œæˆ–ç«äº‰ç­–ç•¥ç›¸æ¯”ï¼ŒåŠ¨æ€äº¤äº’ç±»å‹å¯ä»¥ä¿ƒè¿›å¤šæ™ºèƒ½ä½“å­¦ä¹ ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.26306v2">PDF</a> The code will be released later</p>
<p><strong>æ‘˜è¦</strong></p>
<p>ç°æœ‰çš„å¤šæ™ºèƒ½ä½“å­¦ä¹ æ–¹æ¡ˆé€šè¿‡æ„å»ºäº¤äº’è®­ç»ƒç¯å¢ƒæ¥æ˜ç¡®ä¿ƒè¿›å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹é—´çš„åä½œï¼Œè¿›è€Œæ„å»ºæ›´å¼ºå¤§çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿã€‚ç„¶è€Œï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œè¿™äº›ç³»ç»Ÿéœ€è¦å†æ¬¡æ‰§è¡Œæ•´ä¸ªå¤šæ™ºèƒ½ä½“ç³»ç»Ÿæ¥è·å¾—æœ€ç»ˆè§£å†³æ–¹æ¡ˆï¼Œè¿™ä¸äººç±»è®¤çŸ¥ç›¸æ‚–ã€‚ä¸ºäº†ç ”ç©¶å¤šæ™ºèƒ½ä½“äº¤äº’æ˜¯å¦èƒ½å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„ç‹¬ç«‹è§£å†³é—®é¢˜çš„èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼•å…¥äº†ILRï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„å…±å­¦ä¹ æ¡†æ¶ï¼ŒåŒ…æ‹¬åŠ¨æ€äº¤äº’å’Œæ„ŸçŸ¥æ ¡å‡†ä¸¤ä¸ªå…³é”®ç»„ä»¶ã€‚ILRèƒ½å¤Ÿè‡ªé€‚åº”åœ°é€‰æ‹©åˆä½œæˆ–ç«äº‰ç­–ç•¥ï¼Œé€šè¿‡æ¨¡ä»¿äººç±»è®¨è®ºçš„åˆ›æ–°äº¤äº’æ¨¡å¼Idea3ï¼ˆæ€æƒ³å…±äº«ã€æ€æƒ³åˆ†æå’Œæ€æƒ³èåˆï¼‰æ¥äº¤æ¢ä¿¡æ¯ï¼Œå¹¶æ¨å¯¼å‡ºå„è‡ªçš„æœ€ç»ˆç­”æ¡ˆã€‚åœ¨æ„ŸçŸ¥æ ¡å‡†ä¸­ï¼ŒILRé‡‡ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–æ–¹æ³•ï¼Œå°†ä¸€ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹çš„å¥–åŠ±åˆ†å¸ƒç‰¹æ€§é›†æˆåˆ°å¦ä¸€ä¸ªçš„å¥–åŠ±å‡½æ•°ä¸­ï¼Œå¢å¼ºäº†å¤šæ™ºèƒ½ä½“äº¤äº’çš„å‡èšåŠ›ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªä¸åŒè§„æ¨¡çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸ŠéªŒè¯äº†ILRçš„æœ‰æ•ˆæ€§ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼ŒILRåœ¨äº”ä¸ªæ•°å­¦åŸºå‡†æµ‹è¯•å’Œä¸€ä¸ªç¼–ç åŸºå‡†æµ‹è¯•ä¸­å‡ä¼˜äºå•æ™ºèƒ½ä½“å­¦ä¹ æ–¹æ¡ˆï¼Œå¹¶æœ€å¤šæé«˜äº†5%çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¿˜å‘ç°Idea3èƒ½å¢å¼ºå¼ºå¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šæ™ºèƒ½ä½“æ¨ç†ä¸­çš„ç¨³å¥æ€§ï¼Œè€ŒåŠ¨æ€äº¤äº’ç±»å‹ç›¸æ¯”äºå•çº¯çš„åˆä½œæˆ–ç«äº‰ç­–ç•¥æ›´èƒ½ä¿ƒè¿›å¤šæ™ºèƒ½ä½“å­¦ä¹ ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤šæ™ºèƒ½ä½“å­¦ä¹ é€šè¿‡æ„å»ºäº¤äº’è®­ç»ƒç¯å¢ƒä¿ƒè¿›å¤§å‹è¯­è¨€æ¨¡å‹é—´çš„åä½œã€‚</li>
<li>ç°æœ‰æ–¹æ¡ˆåœ¨æ¨ç†æ—¶éœ€è¦é‡æ–°æ‰§è¡Œæ•´ä¸ªå¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œä¸äººç±»ç‹¬ç«‹è§£å†³é—®é¢˜çš„èƒ½åŠ›ä¸åŒæ­¥ã€‚</li>
<li>ILRæ¡†æ¶å¼•å…¥åŠ¨æ€äº¤äº’å’Œæ„ŸçŸ¥æ ¡å‡†ä¸¤å¤§ç»„ä»¶ï¼Œæ¨¡ä»¿äººç±»è®¨è®ºæ¨¡å¼ä»¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„ç‹¬ç«‹é—®é¢˜è§£å†³èƒ½åŠ›ã€‚</li>
<li>ILRè‡ªé€‚åº”é€‰æ‹©åˆä½œæˆ–ç«äº‰ç­–ç•¥ï¼Œé€šè¿‡Idea3äº¤æ¢ä¿¡æ¯å¹¶æ¨å¯¼å‡ºç­”æ¡ˆã€‚</li>
<li>æ„ŸçŸ¥æ ¡å‡†ä¸­çš„ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–å¢å¼ºäº†å¤šæ™ºèƒ½ä½“äº¤äº’çš„å‡èšåŠ›ã€‚</li>
<li>ILRåœ¨å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ä¸Šçš„å®éªŒéªŒè¯æ˜¾ç¤ºå…¶æ€§èƒ½ä¼˜äºå•æ™ºèƒ½ä½“å­¦ä¹ æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.26306">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-9a4934633d9161b85c85916ef109f592~resize:0:q75.jpg?source=1f5c5e47&expiration=1760084030&auth_key=1760084030-0-0-52372efd012ff7d4e255cd28d0d97749&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d428ccc45900ea3daf9be38941fb8b05~resize:0:q75.jpg?source=1f5c5e47&expiration=1760084039&auth_key=1760084039-0-0-ff8e22cac30cee8c8ba4803671805306&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9c9e460d7d988f375d06468f38f54f5c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760084045&auth_key=1760084045-0-0-1a2201c9316a911d82b99d7adf8f1af2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="PRPO-Paragraph-level-Policy-Optimization-for-Vision-Language-Deepfake-Detection"><a href="#PRPO-Paragraph-level-Policy-Optimization-for-Vision-Language-Deepfake-Detection" class="headerlink" title="PRPO: Paragraph-level Policy Optimization for Vision-Language Deepfake   Detection"></a>PRPO: Paragraph-level Policy Optimization for Vision-Language Deepfake   Detection</h2><p><strong>Authors:Tuan Nguyen, Naseem Khan, Khang Tran, NhatHai Phan, Issa Khalil</strong></p>
<p>The rapid rise of synthetic media has made deepfake detection a critical challenge for online safety and trust. Progress remains constrained by the scarcity of large, high-quality datasets. Although multimodal large language models (LLMs) exhibit strong reasoning capabilities, their performance on deepfake detection is poor, often producing explanations that are misaligned with visual evidence or hallucinatory. To address this limitation, we introduce a reasoning-annotated dataset for deepfake detection and propose Paragraph-level Relative Policy Optimization (PRPO), a reinforcement learning algorithm that aligns LLM reasoning with image content at the paragraph level. Experiments show that PRPO improves detection accuracy by a wide margin and achieves the highest reasoning score of 4.55&#x2F;5.0. Ablation studies further demonstrate that PRPO significantly outperforms GRPO under test-time conditions. These results underscore the importance of grounding multimodal reasoning in visual evidence to enable more reliable and interpretable deepfake detection. </p>
<blockquote>
<p>åˆæˆåª’ä½“çš„è¿…é€Ÿå´›èµ·ä½¿å¾—æ·±åº¦ä¼ªé€ æ£€æµ‹æˆä¸ºç½‘ç»œå®‰å…¨å’Œä¿¡ä»»çš„å…³é”®æŒ‘æˆ˜ã€‚ç”±äºå¤§å‹é«˜è´¨é‡æ•°æ®é›†çš„ç¨€ç¼ºï¼Œè¿›å±•ä»ç„¶å—åˆ°é™åˆ¶ã€‚å°½ç®¡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å±•ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨æ·±åº¦ä¼ªé€ æ£€æµ‹æ–¹é¢çš„è¡¨ç°å´å¾ˆå·®ï¼Œç»å¸¸äº§ç”Ÿä¸è§†è§‰è¯æ®ä¸ç¬¦æˆ–è™šå¹»çš„è§£é‡Šã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ·±åº¦ä¼ªé€ æ£€æµ‹æ¨ç†æ³¨é‡Šæ•°æ®é›†ï¼Œå¹¶æå‡ºäº†æ®µè½çº§ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆPRPOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œèƒ½å¤Ÿåœ¨æ®µè½çº§åˆ«å°†LLMæ¨ç†ä¸å›¾åƒå†…å®¹å¯¹é½ã€‚å®éªŒè¡¨æ˜ï¼ŒPRPOåœ¨æ£€æµ‹å‡†ç¡®ç‡ä¸Šæœ‰æ˜¾è‘—æé«˜ï¼Œå¹¶è¾¾åˆ°äº†4.55&#x2F;5.0çš„æœ€é«˜æ¨ç†åˆ†æ•°ã€‚æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥è¯æ˜ï¼Œåœ¨æµ‹è¯•æ¡ä»¶ä¸‹PRPOæ˜¾è‘—ä¼˜äºGRPOã€‚è¿™äº›ç»“æœå¼ºè°ƒï¼Œä»¥è§†è§‰è¯æ®ä¸ºåŸºç¡€çš„å¤šæ¨¡æ€æ¨ç†å¯¹äºå®ç°æ›´å¯é å’Œå¯è§£é‡Šçš„æ·±åº¦ä¼ªé€ æ£€æµ‹è‡³å…³é‡è¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.26272v2">PDF</a> </p>
<p><strong>Summary</strong><br>å¤šåª’ä½“å†…å®¹çš„æ³›æ»¥ä½¿å¾—æ·±åº¦ä¼ªé€ æŠ€æœ¯æ£€æµ‹æˆä¸ºç½‘ç»œå®‰å…¨çš„é‡è¦æŒ‘æˆ˜ã€‚ç›®å‰å¤§å‹å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹åœ¨æ·±åº¦ä¼ªé€ æ£€æµ‹æ–¹é¢å­˜åœ¨æ€§èƒ½ç“¶é¢ˆï¼Œå…¶è§£é‡Šä¸è§†è§‰è¯æ®å¸¸å­˜åœ¨åå·®æˆ–å¹»æƒ³ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºæ·±åº¦ä¼ªé€ æ£€æµ‹æ¨ç†æ ‡æ³¨æ•°æ®é›†ï¼Œå¹¶æå‡ºæ®µè½çº§ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–æ–¹æ³•ï¼ˆPRPOï¼‰ï¼Œä¸€ç§å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œä½¿è¯­è¨€æ¨¡å‹æ¨ç†ä¸å›¾åƒå†…å®¹ç›¸ç¬¦ã€‚å®éªŒè¡¨æ˜ï¼ŒPRPOæ˜¾è‘—æé«˜æ£€æµ‹å‡†ç¡®ç‡ï¼Œè¾¾åˆ°æœ€é«˜æ¨ç†åˆ†æ•°4.55&#x2F;5ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦ä¼ªé€ æŠ€æœ¯æ£€æµ‹çš„é‡è¦æ€§éšç€å¤šåª’ä½“å†…å®¹çš„å¢é•¿è€Œæå‡ã€‚</li>
<li>å¤§å‹å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹åœ¨æ·±åº¦ä¼ªé€ æ£€æµ‹æ–¹é¢å­˜åœ¨æ€§èƒ½é—®é¢˜ï¼Œå…¶è§£é‡Šå¸¸ä¸å®é™…è§†è§‰è¯æ®ä¸ç¬¦æˆ–äº§ç”Ÿå¹»æƒ³ã€‚</li>
<li>ä¸ºæ”¹å–„è¯­è¨€æ¨¡å‹åœ¨æ·±åº¦ä¼ªé€ æ£€æµ‹ä¸­çš„è¡¨ç°ï¼Œæ¨å‡ºäº†æ·±åº¦ä¼ªé€ æ£€æµ‹æ¨ç†æ ‡æ³¨æ•°æ®é›†ã€‚</li>
<li>æå‡ºæ®µè½çº§ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆPRPOï¼‰æ–¹æ³•ï¼Œä¸€ç§å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œæ—¨åœ¨ä½¿è¯­è¨€æ¨¡å‹çš„æ¨ç†ä¸å›¾åƒå†…å®¹ç›¸ç¬¦ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºPRPOèƒ½æ˜¾è‘—æé«˜æ·±åº¦ä¼ªé€ æ£€æµ‹çš„å‡†ç¡®æ€§ã€‚</li>
<li>PRPOçš„æ¨ç†åˆ†æ•°é«˜äºå…¶ä»–æ–¹æ³•ï¼Œè¾¾åˆ°æœ€é«˜åˆ†æ•°4.55&#x2F;5ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.26272">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-fc6d08867874312cdb21015535df3555~resize:0:q75.jpg?source=1f5c5e47&expiration=1760084053&auth_key=1760084053-0-0-c69359efa92e47593d7a5c7afe87e2d3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-216dba91803f79ece78fbc9da5634c2c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760084060&auth_key=1760084060-0-0-0e4f54654822828c8cf8712d355c697a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4c22b9eacf49db75e60c58f1d98886c2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760100133&auth_key=1760100133-0-0-6d15e23a1620ab3201fd0f8d63ad7a3b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-22a21b688b81ff4abc7a96d622869ba6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760084131&auth_key=1760084131-0-0-81b7371597ef915342a07dd627682a9e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ef31a419cc4b2c000407e0602473fa9c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760084138&auth_key=1760084138-0-0-07d19577f2fd7a41ec6eaf67c21373d2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4d3fc4df4dae98791ee7e8fa4dc27e1d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760084144&auth_key=1760084144-0-0-0585aa79fe1eeafeccf6a92d396f1933&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4b1fcc629076f1bb88ba63f5cf0b6842~resize:0:q75.jpg?source=1f5c5e47&expiration=1760084151&auth_key=1760084151-0-0-98428e2437efc6d878a8877071761b67&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Reinforced-Strategy-Optimization-for-Conversational-Recommender-Systems-via-Network-of-Experts"><a href="#Reinforced-Strategy-Optimization-for-Conversational-Recommender-Systems-via-Network-of-Experts" class="headerlink" title="Reinforced Strategy Optimization for Conversational Recommender Systems   via Network-of-Experts"></a>Reinforced Strategy Optimization for Conversational Recommender Systems   via Network-of-Experts</h2><p><strong>Authors:Xiaoyan Zhao, Ming Yan, Yang Zhang, Yang Deng, Jian Wang, Fengbin Zhu, Yilun Qiu, Hong Cheng, Tat-Seng Chua</strong></p>
<p>Conversational Recommender Systems (CRSs) aim to provide personalized recommendations through multi-turn natural language interactions with users. Given the strong interaction and reasoning skills of Large Language Models (LLMs), leveraging LLMs for CRSs has recently emerged as a promising direction. However, existing LLM-based methods often lack explicit optimization of interaction strategies, instead relying on unified prompts and the LLMâ€™s internal knowledge to decide how to interact, which can lead to suboptimal outcomes. In this paper, we propose a novel Reinforced Strategy Optimization (RSO) method for CRS, which decomposes the process of generating strategy-driven response decisions into the macro-level strategy planning and micro-level strategy adaptation through a network-of-experts architecture. At the macro level, a Planner expert selects macro-level interaction strategies (e.g., recommend, explain, encourage). At the micro level, an Actor expert generates detailed responses conditioned on the selected macro-level strategy, guided by auxiliary experts that provide complementary information such as user preferences and factual grounding. This hierarchical decomposition disentangles the optimization of different sub-tasks involved in CRS response generation, enabling more tractable learning at each level. To address the scarcity of high-quality multi-turn training data, we formulate strategy learning as a reinforcement learning problem, guided by an LLM-based reward model to achieve automatic strategy exploration. Extensive experiments show that RSO significantly improves interaction performance compared to state-of-the-art baselines, demonstrating the effectiveness of explicit hierarchical strategy optimization for CRS. </p>
<blockquote>
<p>å¯¹è¯æ¨èç³»ç»Ÿï¼ˆCRSï¼‰æ—¨åœ¨é€šè¿‡å¤šè½®è‡ªç„¶è¯­è¨€ä¸ç”¨æˆ·äº¤äº’æä¾›ä¸ªæ€§åŒ–æ¨èã€‚é‰´äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¼ºå¤§äº¤äº’å’Œæ¨ç†èƒ½åŠ›ï¼Œåˆ©ç”¨LLMè¿›è¡ŒCRSæœ€è¿‘å·²æˆä¸ºä¸€ä¸ªæœ‰å‰é€”çš„æ–¹å‘ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºLLMçš„æ–¹æ³•é€šå¸¸ç¼ºä¹æ˜ç¡®çš„ä¼˜åŒ–äº¤äº’ç­–ç•¥ï¼Œè€Œæ˜¯ä¾èµ–äºç»Ÿä¸€çš„æç¤ºå’ŒLLMçš„å†…éƒ¨çŸ¥è¯†æ¥å†³å®šå¦‚ä½•äº¤äº’ï¼Œè¿™å¯èƒ½å¯¼è‡´æ¬¡ä¼˜ç»“æœã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºCRSçš„æ–°å‹å¼ºåŒ–ç­–ç•¥ä¼˜åŒ–ï¼ˆRSOï¼‰æ–¹æ³•ï¼Œå®ƒå°†ç”Ÿæˆç­–ç•¥é©±åŠ¨å“åº”å†³ç­–çš„è¿‡ç¨‹åˆ†è§£ä¸ºå®è§‚å±‚é¢çš„ç­–ç•¥è§„åˆ’å’Œå¾®è§‚å±‚é¢çš„ç­–ç•¥é€‚åº”ï¼Œé€šè¿‡ä¸“å®¶ç½‘ç»œæ¶æ„å®ç°ã€‚åœ¨å®è§‚å±‚é¢ï¼Œè§„åˆ’ä¸“å®¶é€‰æ‹©å®è§‚çº§åˆ«çš„äº¤äº’ç­–ç•¥ï¼ˆä¾‹å¦‚æ¨èã€è§£é‡Šã€é¼“åŠ±ï¼‰ã€‚åœ¨å¾®è§‚å±‚é¢ï¼Œæ ¹æ®é€‰å®šçš„å®è§‚ç­–ç•¥ï¼Œè¡Œä¸ºä¸“å®¶ç”Ÿæˆè¯¦ç»†çš„å“åº”ï¼Œå¹¶ç”±æä¾›è¡¥å……ä¿¡æ¯çš„è¾…åŠ©ä¸“å®¶å¼•å¯¼ï¼Œä¾‹å¦‚ç”¨æˆ·åå¥½å’Œäº‹å®ä¾æ®ã€‚è¿™ç§å±‚æ¬¡åˆ†è§£è§£å†³äº†æ¶‰åŠCRSå“åº”ç”Ÿæˆçš„ä¸åŒå­ä»»åŠ¡çš„ä¼˜åŒ–é—®é¢˜ï¼Œä½¿æ¯ä¸ªçº§åˆ«çš„å­¦ä¹ æ›´åŠ æ˜“äºå¤„ç†ã€‚ä¸ºäº†è§£å†³é«˜è´¨é‡å¤šè½®è®­ç»ƒæ•°æ®çš„ç¨€ç¼ºé—®é¢˜ï¼Œæˆ‘ä»¬å°†ç­–ç•¥å­¦ä¹ åˆ¶å®šä¸ºå¼ºåŒ–å­¦ä¹ é—®é¢˜ï¼Œç”±åŸºäºLLMçš„å¥–åŠ±æ¨¡å‹æŒ‡å¯¼ï¼Œå®ç°è‡ªåŠ¨ç­–ç•¥æ¢ç´¢ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸æœ€æ–°åŸºçº¿ç›¸æ¯”ï¼ŒRSOæ˜¾è‘—æé«˜äº†äº¤äº’æ€§èƒ½ï¼Œè¯æ˜äº†æ˜¾å¼åˆ†å±‚ç­–ç•¥ä¼˜åŒ–å¯¹CRSçš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.26093v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡æå‡ºä¸€ç§é’ˆå¯¹å¯¹è¯æ¨èç³»ç»Ÿï¼ˆCRSï¼‰çš„å¼ºåŒ–ç­–ç•¥ä¼˜åŒ–ï¼ˆRSOï¼‰æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¸“å®¶ç½‘ç»œæ¶æ„ï¼Œå°†ç”Ÿæˆç­–ç•¥é©±åŠ¨å“åº”å†³ç­–çš„è¿‡ç¨‹åˆ†è§£ä¸ºå®è§‚ç­–ç•¥è§„åˆ’å’Œå¾®è§‚ç­–ç•¥é€‚åº”ã€‚å®è§‚å±‚é¢ç”±è§„åˆ’ä¸“å®¶é€‰æ‹©äº¤äº’ç­–ç•¥ï¼Œå¾®è§‚å±‚é¢ç”±è¡ŒåŠ¨ä¸“å®¶æ ¹æ®é€‰æ‹©çš„å®è§‚ç­–ç•¥ç”Ÿæˆè¯¦ç»†å“åº”ã€‚ä¸ºè§£å†³é«˜è´¨é‡å¤šè½®è®­ç»ƒæ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œå°†ç­–ç•¥å­¦ä¹ åˆ¶å®šä¸ºå¼ºåŒ–å­¦ä¹ é—®é¢˜ï¼Œä»¥LLMä¸ºåŸºç¡€çš„å¥–åŠ±æ¨¡å‹å®ç°è‡ªåŠ¨ç­–ç•¥æ¢ç´¢ã€‚å®éªŒè¡¨æ˜ï¼ŒRSOæ–¹æ³•æ˜¾è‘—æé«˜äº†äº¤äº’æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¯¹è¯æ¨èç³»ç»Ÿï¼ˆCRSï¼‰æ—¨åœ¨é€šè¿‡ä¸ç”¨æˆ·çš„å¤šè½®è‡ªç„¶è¯­è¨€äº¤äº’æä¾›ä¸ªæ€§åŒ–æ¨èã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨CRSä¸­çš„åˆ©ç”¨å·²æˆä¸ºä¸€ä¸ªç ”ç©¶çƒ­ç‚¹ï¼Œä½†ç°æœ‰æ–¹æ³•ç¼ºä¹æ˜ç¡®çš„ä¼˜åŒ–äº¤äº’ç­–ç•¥ã€‚</li>
<li>æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹çš„å¼ºåŒ–ç­–ç•¥ä¼˜åŒ–ï¼ˆRSOï¼‰æ–¹æ³•ï¼Œå°†ç”Ÿæˆå“åº”å†³ç­–çš„è¿‡ç¨‹åˆ†è§£ä¸ºå®è§‚ç­–ç•¥è§„åˆ’å’Œå¾®è§‚ç­–ç•¥é€‚åº”ã€‚</li>
<li>å®è§‚å±‚é¢çš„è§„åˆ’ä¸“å®¶è´Ÿè´£é€‰æ‹©äº¤äº’ç­–ç•¥ï¼Œå¦‚æ¨èã€è§£é‡Šã€é¼“åŠ±ç­‰ã€‚</li>
<li>å¾®è§‚å±‚é¢çš„è¡ŒåŠ¨ä¸“å®¶æ ¹æ®æ‰€é€‰å®è§‚ç­–ç•¥ç”Ÿæˆè¯¦ç»†å“åº”ï¼Œå—è¾…åŠ©ä¸“å®¶æä¾›ç”¨æˆ·åå¥½å’Œäº‹å®ä¾æ®ç­‰ä¿¡æ¯æŒ‡å¯¼ã€‚</li>
<li>è¯¥æ–¹æ³•å°†ç­–ç•¥å­¦ä¹ åˆ¶å®šä¸ºå¼ºåŒ–å­¦ä¹ é—®é¢˜ï¼Œè§£å†³é«˜è´¨é‡å¤šè½®è®­ç»ƒæ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.26093">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-b272f5bf73be38a3d6e3a4fb15ce6845~resize:0:q75.jpg?source=1f5c5e47&expiration=1760084158&auth_key=1760084158-0-0-d5796d76b4c4d8232f514441b54d22cf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c82abb63255d58aa7d35f7950e00a251~resize:0:q75.jpg?source=1f5c5e47&expiration=1760084165&auth_key=1760084165-0-0-462b2ac5ff3bd833d8fdadfb13f3c04d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-409d4b3296d0641aa41d16b1c726cf1d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760100139&auth_key=1760100139-0-0-81f90a6d29827bc2a80ea4fddb5fba0b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-71eb2af6aa2c0e697a0450b125bac247~resize:0:q75.jpg?source=1f5c5e47&expiration=1760100146&auth_key=1760100146-0-0-d06c2e87d37bea5aa227dcb4f57d0513&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-81777cba7ba0effbbda2e328e3ef447f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760100152&auth_key=1760100152-0-0-c4491e7948d8403235a7a0369a3b37d4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2fcf7e3c4d1d0bc233c1c7767c41958a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760100159&auth_key=1760100159-0-0-42bb3f849dece25a672fc11b73a9fab2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Unspoken-Hints-Accuracy-Without-Acknowledgement-in-LLM-Reasoning"><a href="#Unspoken-Hints-Accuracy-Without-Acknowledgement-in-LLM-Reasoning" class="headerlink" title="Unspoken Hints: Accuracy Without Acknowledgement in LLM Reasoning"></a>Unspoken Hints: Accuracy Without Acknowledgement in LLM Reasoning</h2><p><strong>Authors:Arash Marioriyad, Shaygan Adim, Nima Alighardashi, Mahdieh Soleymani Banghshah, Mohammad Hossein Rohban</strong></p>
<p>Large language models (LLMs) increasingly rely on chain-of-thought (CoT) prompting to solve mathematical and logical reasoning tasks. Yet, a central question remains: to what extent are these generated rationales \emph{faithful} to the underlying computations, rather than post-hoc narratives shaped by hints that function as answer shortcuts embedded in the prompt? Following prior work on hinted vs.\ unhinted prompting, we present a systematic study of CoT faithfulness under controlled hint manipulations. Our experimental design spans four datasets (AIME, GSM-Hard, MATH-500, UniADILR), two state-of-the-art models (GPT-4o and Gemini-2-Flash), and a structured set of hint conditions varying in correctness (correct and incorrect), presentation style (sycophancy and data leak), and complexity (raw answers, two-operator expressions, four-operator expressions). We evaluate both task accuracy and whether hints are explicitly acknowledged in the reasoning. Our results reveal three key findings. First, correct hints substantially improve accuracy, especially on harder benchmarks and logical reasoning, while incorrect hints sharply reduce accuracy in tasks with lower baseline competence. Second, acknowledgement of hints is highly uneven: equation-based hints are frequently referenced, whereas raw hints are often adopted silently, indicating that more complex hints push models toward verbalizing their reliance in the reasoning process. Third, presentation style matters: sycophancy prompts encourage overt acknowledgement, while leak-style prompts increase accuracy but promote hidden reliance. This may reflect RLHF-related effects, as sycophancy exploits the human-pleasing side and data leak triggers the self-censoring side. Together, these results demonstrate that LLM reasoning is systematically shaped by shortcuts in ways that obscure faithfulness. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¶Šæ¥è¶Šä¾èµ–é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºæ¥è§£å†³æ•°å­¦å’Œé€»è¾‘æ¨ç†ä»»åŠ¡ã€‚ç„¶è€Œï¼Œä¸€ä¸ªæ ¸å¿ƒé—®é¢˜ä»ç„¶å­˜åœ¨ï¼šè¿™äº›ç”Ÿæˆçš„ä¾æ®åœ¨å¤šå¤§ç¨‹åº¦ä¸Šæ˜¯å¿ äºåº•å±‚è®¡ç®—çš„ï¼Œè€Œä¸æ˜¯ç”±æç¤ºæ‰€å¡‘é€ çš„äº‹åå™äº‹ï¼Œè¿™äº›æç¤ºä½œä¸ºåµŒå…¥åœ¨æç¤ºä¸­çš„ç­”æ¡ˆæ·å¾„ï¼Ÿåœ¨å…³äºæç¤ºä¸éæç¤ºæ–¹æ³•çš„å‰æœŸç ”ç©¶ä¹‹åï¼Œæˆ‘ä»¬å¯¹å—æ§æç¤ºæ“ä½œä¸‹çš„CoTå¿ å®åº¦è¿›è¡Œäº†ç³»ç»Ÿç ”ç©¶ã€‚æˆ‘ä»¬çš„å®éªŒè®¾è®¡æ¶µç›–äº†å››ä¸ªæ•°æ®é›†ï¼ˆAIMEã€GSM-Hardã€MATH-500ã€UniADILRï¼‰ã€ä¸¤ä¸ªæœ€å…ˆè¿›çš„æ¨¡å‹ï¼ˆGPT-4oå’ŒGemini-2-Flashï¼‰ï¼Œä»¥åŠä¸€å¥—ç»“æ„åŒ–çš„æç¤ºæ¡ä»¶ï¼Œè¿™äº›æ¡ä»¶åœ¨æ­£ç¡®æ€§ï¼ˆæ­£ç¡®å’Œé”™è¯¯ï¼‰ã€å‘ˆç°é£æ ¼ï¼ˆå¥‰æ‰¿å’Œæ•°æ®æ³„éœ²ï¼‰å’Œå¤æ‚æ€§ï¼ˆåŸå§‹ç­”æ¡ˆã€ä¸¤ä¸ªè¿ç®—ç¬¦è¡¨è¾¾å¼ã€å››ä¸ªè¿ç®—ç¬¦è¡¨è¾¾å¼ï¼‰ä¸Šæœ‰æ‰€ä¸åŒã€‚æˆ‘ä»¬è¯„ä¼°äº†ä»»åŠ¡å‡†ç¡®æ€§å’Œæ¨¡å‹åœ¨æ¨ç†ä¸­æ˜¯å¦æ˜ç¡®æ‰¿è®¤æç¤ºã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœæ­ç¤ºäº†ä¸‰ä¸ªå…³é”®å‘ç°ã€‚é¦–å…ˆï¼Œæ­£ç¡®çš„æç¤ºä¼šæ˜¾è‘—æé«˜å‡†ç¡®æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨æ›´éš¾çš„æœ¬åŸºå‡†æµ‹è¯•å’Œé€»è¾‘æ¨ç†ä¸­ï¼Œè€Œé”™è¯¯çš„æç¤ºä¼šå¤§å¹…åº¦é™ä½åŸºçº¿èƒ½åŠ›è¾ƒä½çš„ä»»åŠ¡çš„å‡†ç¡®æ€§ã€‚å…¶æ¬¡ï¼Œå¯¹æç¤ºçš„è®¤å¯å­˜åœ¨ä¸¥é‡çš„ä¸å¹³è¡¡ï¼šåŸºäºæ–¹ç¨‹çš„æç¤ºç»å¸¸è¢«å¼•ç”¨ï¼Œè€ŒåŸå§‹æç¤ºå¾€å¾€è¢«é™é»˜é‡‡çº³ï¼Œè¿™è¡¨æ˜æ›´å¤æ‚çš„æç¤ºä¿ƒä½¿æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­å…¬å¼€ä¾èµ–ã€‚ç¬¬ä¸‰ï¼Œå‘ˆç°é£æ ¼å¾ˆé‡è¦ï¼šå¥‰æ‰¿æç¤ºé¼“åŠ±æ˜æ˜¾çš„è®¤å¯ï¼Œè€Œæ³„éœ²é£æ ¼çš„æç¤ºè™½ç„¶æé«˜äº†å‡†ç¡®æ€§ï¼Œä½†ä¿ƒè¿›äº†éšè—çš„ä¾èµ–ã€‚è¿™å¯èƒ½åæ˜ äº†RLHFç›¸å…³çš„æ•ˆåº”ï¼Œå› ä¸ºå¥‰æ‰¿åˆ©ç”¨äººç±»æ„‰æ‚¦çš„ä¸€é¢ï¼Œè€Œæ•°æ®æ³„éœ²è§¦å‘äº†è‡ªæˆ‘å®¡æŸ¥çš„ä¸€é¢ã€‚æ€»çš„æ¥è¯´ï¼Œè¿™äº›ç»“æœè¡¨æ˜ï¼ŒLLMæ¨ç†å—åˆ°æ·å¾„çš„ç³»ç»Ÿæ€§å½±å“ï¼Œè¿™äº›æ·å¾„æ©ç›–äº†å¿ å®æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.26041v1">PDF</a> 5 Pages, 4 Figures, 4 Tables</p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è§£å†³æ•°å­¦å’Œé€»è¾‘æ¨ç†ä»»åŠ¡æ—¶ï¼Œå¯¹äºæ€ç»´é“¾ï¼ˆCoTï¼‰æç¤ºçš„ä¾èµ–ç¨‹åº¦ã€‚å®éªŒè®¾è®¡åŒ…æ‹¬å››ä¸ªæ•°æ®é›†ã€ä¸¤ä¸ªæœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œä»¥åŠä¸€ç³»åˆ—çš„æ­£ç¡®æ€§ã€å‘ˆç°é£æ ¼å’Œå¤æ‚ç¨‹åº¦å„å¼‚çš„æç¤ºæ¡ä»¶ã€‚ç ”ç©¶å‘ç°ï¼Œæ­£ç¡®çš„æç¤ºå¯ä»¥æ˜¾è‘—æé«˜å‡†ç¡®ç‡ï¼Œå°¤å…¶æ˜¯é’ˆå¯¹éš¾åº¦è¾ƒå¤§çš„åŸºå‡†æµ‹è¯•å’Œé€»è¾‘æ¨ç†ï¼›è€Œé”™è¯¯çš„æç¤ºåˆ™ä¼šåœ¨ä»»åŠ¡åŸºç¡€èƒ½åŠ›è¾ƒä½çš„æƒ…å†µä¸‹å¤§å¹…åº¦é™ä½å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼Œæ¨¡å‹å¯¹æç¤ºçš„æ‰¿è®¤ç¨‹åº¦ä¸å‡è¡¡ï¼Œæ–¹ç¨‹å¼çš„æç¤ºç»å¸¸è¢«æåŠï¼Œè€ŒåŸå§‹ç­”æ¡ˆçš„æç¤ºå¾€å¾€é™é»˜é‡‡ç”¨ã€‚å‘ˆç°é£æ ¼ä¹Ÿè‡³å…³é‡è¦ï¼Œè®¨å¥½çš„æç¤ºé¼“åŠ±æ˜æ˜¾çš„æ‰¿è®¤ï¼Œè€Œæ³„éœ²é£æ ¼çš„æç¤ºè™½ç„¶èƒ½æé«˜å‡†ç¡®ç‡ï¼Œä½†æ›´å®¹æ˜“è®©äººä¾èµ–è€Œä¸è‡ªçŸ¥ã€‚è¿™äº›ç»“æœæ­ç¤ºäº†LLMæ¨ç†åœ¨ä¸€å®šç¨‹åº¦ä¸Šå—åˆ°æ·å¾„çš„å½±å“ï¼Œå¯¼è‡´å¿ å®åº¦é™ä½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è§£å†³æ•°å­¦å’Œé€»è¾‘ä»»åŠ¡æ—¶ä¾èµ–æ€ç»´é“¾ï¼ˆCoTï¼‰æç¤ºã€‚</li>
<li>æ­£ç¡®æç¤ºæ˜¾è‘—æé«˜å‡†ç¡®ç‡ï¼Œå°¤å…¶åœ¨éš¾åº¦å¤§çš„åŸºå‡†æµ‹è¯•å’Œé€»è¾‘æ¨ç†ä¸­ã€‚</li>
<li>é”™è¯¯æç¤ºæ˜¾è‘—é™ä½ä»»åŠ¡åŸºç¡€èƒ½åŠ›è¾ƒä½æ—¶çš„å‡†ç¡®ç‡ã€‚</li>
<li>æ¨¡å‹å¯¹ä¸åŒç±»å‹çš„æç¤ºæœ‰ä¸åŒçš„æ‰¿è®¤ç¨‹åº¦ï¼šæ–¹ç¨‹æç¤ºå¸¸æåŠï¼ŒåŸå§‹ç­”æ¡ˆçš„æç¤ºé™é»˜é‡‡ç”¨ã€‚</li>
<li>æç¤ºçš„å‘ˆç°é£æ ¼å½±å“æ¨¡å‹çš„ååº”ï¼šè®¨å¥½é£æ ¼çš„æç¤ºé¼“åŠ±æ˜æ˜¾çš„æ‰¿è®¤ï¼Œæ³„éœ²é£æ ¼çš„æç¤ºä¿ƒè¿›éšæ€§ä¾èµ–ã€‚</li>
<li>LLMæ¨ç†å—æ·å¾„å½±å“ï¼Œå¯¼è‡´å¿ å®åº¦é™ä½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.26041">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-edbb3069dd40fec17e7cdaab20ac2c6f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760100165&auth_key=1760100165-0-0-18192ff2072f6a43f81c65acce2aa149&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-25c870afb46379a5229128ae227d9b32~resize:0:q75.jpg?source=1f5c5e47&expiration=1760100173&auth_key=1760100173-0-0-2f185a1c3b9190ca22254ba1c4668ff0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="R-Log-Incentivizing-Log-Analysis-Capability-in-LLMs-via-Reasoning-based-Reinforcement-Learning"><a href="#R-Log-Incentivizing-Log-Analysis-Capability-in-LLMs-via-Reasoning-based-Reinforcement-Learning" class="headerlink" title="R-Log: Incentivizing Log Analysis Capability in LLMs via Reasoning-based   Reinforcement Learning"></a>R-Log: Incentivizing Log Analysis Capability in LLMs via Reasoning-based   Reinforcement Learning</h2><p><strong>Authors:Yilun Liu, Ziang Chen, Song Xu, Minggui He, Shimin Tao, Weibin Meng, Yuming Xie, Tao Han, Chunguang Zhao, Jingzhou Du, Daimeng Wei, Shenglin Zhang, Yongqian Sun</strong></p>
<p>The growing complexity of log data in modern software systems has prompted the use of Large Language Models (LLMs) for automated log analysis. Current approaches typically rely on direct supervised fine-tuning (SFT) on log-label pairs. However, this exacerbates the domain discrepancy between general-purpose LLMs and specialized log data, causing overfitting. Furthermore, SFTâ€™s imbalanced loss computation often allows lengthy contexts to overwhelm critical, concise details in model answers, leading to hallucinations. To address these limitations, we propose R-Log, a novel reasoning-based paradigm that mirrors the structured, step-by-step analytical process of human engineers. This approach enhances generalizability by learning the underlying rules behind conclusions. We further employ Reinforcement Learning (RL) to optimize the model within a simulated O&amp;M environment, thereby reducing hallucinations by directly rewarding correct outcomes. R-Log is first cold-started on a curated dataset of 2k+ reasoning trajectories, guided by 13 strategies from manual O&amp;M practices, to establish an initial reasoning capability. This ability is then refined via RL using a joint reward function. Empirical evaluations on real-world logs show that R-Log outperforms existing methods across five log analysis tasks, particularly in unseen scenarios (by 228.05%). We also designed R-Log-fast with 5x speedup while keeping 93% of the efficacy. </p>
<blockquote>
<p>éšç€ç°ä»£è½¯ä»¶ç³»ç»Ÿä¸­æ—¥å¿—æ•°æ®çš„æ—¥ç›Šå¤æ‚æ€§ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¢«å¹¿æ³›åº”ç”¨äºè‡ªåŠ¨åŒ–æ—¥å¿—åˆ†æã€‚å½“å‰çš„æ–¹æ³•é€šå¸¸ä¾èµ–äºæ—¥å¿—æ ‡ç­¾å¯¹çš„ç›´æ¥ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€‚ç„¶è€Œï¼Œè¿™åŠ å‰§äº†é€šç”¨LLMå’Œä¸“ç”¨æ—¥å¿—æ•°æ®ä¹‹é—´çš„é¢†åŸŸå·®å¼‚ï¼Œå¯¼è‡´è¿‡æ‹Ÿåˆã€‚æ­¤å¤–ï¼ŒSFTçš„ä¸å¹³è¡¡æŸå¤±è®¡ç®—ç»å¸¸ä½¿å†—é•¿çš„ä¸Šä¸‹æ–‡æ©ç›–äº†æ¨¡å‹ç­”æ¡ˆä¸­çš„å…³é”®ç®€æ´ç»†èŠ‚ï¼Œä»è€Œå¯¼è‡´å¹»è§‰ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†R-Logè¿™ä¸€åŸºäºæ¨ç†çš„æ–°èŒƒå¼ï¼Œå®ƒåæ˜ äº†äººç±»å·¥ç¨‹å¸ˆçš„ç»“æ„åŒ–ã€é€æ­¥åˆ†æè¿‡ç¨‹ã€‚è¿™ç§æ–¹æ³•é€šè¿‡äº†è§£ç»“è®ºèƒŒåçš„åŸºæœ¬è§„åˆ™æ¥æé«˜é€šç”¨æ€§ã€‚æˆ‘ä»¬è¿˜é‡‡ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æ¨¡æ‹Ÿçš„è¿ç»´ç®¡ç†ç¯å¢ƒä¸­ä¼˜åŒ–æ¨¡å‹ï¼Œé€šè¿‡ç›´æ¥å¥–åŠ±æ­£ç¡®ç»“æœæ¥å‡å°‘å¹»è§‰ã€‚R-Logé¦–å…ˆåœ¨ä¸€ä¸ªç²¾é€‰çš„åŒ…å«è¶…è¿‡2000ä¸ªæ¨ç†è½¨è¿¹çš„æ•°æ®é›†ä¸Šè¿›è¡Œå†·å¯åŠ¨ï¼Œè¯¥æ•°æ®é›†ç”±æ‰‹åŠ¨è¿ç»´å®è·µçš„13ç§ç­–ç•¥æŒ‡å¯¼ï¼Œä»¥å»ºç«‹åˆæ­¥çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶åï¼Œä½¿ç”¨è”åˆå¥–åŠ±å‡½æ•°é€šè¿‡RLè¿›è¡Œç²¾ç»†åŒ–ã€‚åœ¨çœŸå®ä¸–ç•Œæ—¥å¿—ä¸Šçš„å®è¯ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒR-Logåœ¨äº”ä¸ªæ—¥å¿—åˆ†æä»»åŠ¡ä¸Šçš„è¡¨ç°å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨æœªè§åœºæ™¯ä¸­çš„è¡¨ç°æé«˜äº†228.05%ã€‚æˆ‘ä»¬è¿˜è®¾è®¡äº†å…·æœ‰5å€åŠ é€Ÿçš„R-Log-fastï¼ŒåŒæ—¶ä¿æŒäº†93%çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25987v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºç°ä»£è½¯ä»¶ç³»ç»Ÿä¸­æ—¥å¿—æ•°æ®çš„å¤æ‚æ€§å¢é•¿ï¼Œç ”ç©¶è€…æå‡ºä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œè‡ªåŠ¨æ—¥å¿—åˆ†æã€‚å½“å‰æ–¹æ³•é€šå¸¸ä¾èµ–äºç›´æ¥åœ¨æ—¥å¿—æ ‡ç­¾å¯¹ä¸Šè¿›è¡Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œä½†è¿™ç§æ–¹æ³•åŠ å‰§äº†é€šç”¨LLMså’Œç‰¹å®šæ—¥å¿—æ•°æ®ä¹‹é—´çš„é¢†åŸŸå·®å¼‚ï¼Œå¯¼è‡´è¿‡æ‹Ÿåˆã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†R-Logè¿™ä¸€æ–°å‹åŸºäºæ¨ç†çš„æ–¹æ³•ï¼Œæ¨¡æ‹Ÿäººå·¥å·¥ç¨‹å¸ˆçš„é€æ­¥åˆ†æè¿‡ç¨‹ï¼Œæé«˜æ¨¡å‹é€šç”¨æ€§å¹¶å‡å°‘å¹»è§‰ã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æ¨¡æ‹Ÿçš„è¿ç»´ç¯å¢ƒä¸­ä¼˜åŒ–æ¨¡å‹ï¼Œå¹¶é¦–æ¬¡ä½¿ç”¨å†·å¯åŠ¨åœ¨è¶…è¿‡2kä¸ªæ¨ç†è½¨è¿¹çš„æ•°æ®é›†ä¸Šå»ºç«‹åˆæ­¥æ¨ç†èƒ½åŠ›ã€‚åœ¨çœŸå®ä¸–ç•Œæ—¥å¿—ä¸Šçš„å®è¯è¯„ä¼°æ˜¾ç¤ºï¼ŒR-Logåœ¨äº”ä¸ªæ—¥å¿—åˆ†æä»»åŠ¡ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨æœªè§åœºæ™¯ä¸­çš„æ€§èƒ½æå‡è¾¾228.05%ã€‚åŒæ—¶ï¼Œè¿˜è®¾è®¡äº†å¿«é€Ÿç‰ˆæœ¬çš„R-Logï¼Œä¿æŒ93%æ•ˆèƒ½çš„åŒæ—¶å®ç°5å€åŠ é€Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¢«åº”ç”¨äºç°ä»£è½¯ä»¶ç³»ç»Ÿçš„è‡ªåŠ¨æ—¥å¿—åˆ†æï¼Œåº”å¯¹å…¶å¤æ‚æ€§ã€‚</li>
<li>å½“å‰æ–¹æ³•é€šè¿‡ç›´æ¥åœ¨æ—¥å¿—æ ‡ç­¾å¯¹ä¸Šè¿›è¡Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œå­˜åœ¨é¢†åŸŸå·®å¼‚å’Œè¿‡æ‹Ÿåˆé—®é¢˜ã€‚</li>
<li>R-Logæ˜¯ä¸€ç§æ–°å‹åŸºäºæ¨ç†çš„æ–¹æ³•ï¼Œæ¨¡æ‹Ÿäººå·¥å·¥ç¨‹å¸ˆçš„åˆ†ææ­¥éª¤ï¼Œæé«˜æ¨¡å‹é€šç”¨æ€§ã€‚</li>
<li>R-Logä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æ¨¡æ‹Ÿçš„è¿ç»´ç¯å¢ƒä¸­ä¼˜åŒ–ï¼Œå‡å°‘å¹»è§‰ã€‚</li>
<li>R-Logé¦–æ¬¡é€šè¿‡å†·å¯åŠ¨åœ¨åŒ…å«è¶…è¿‡2kä¸ªæ¨ç†è½¨è¿¹çš„æ•°æ®é›†ä¸Šå»ºç«‹åˆæ­¥æ¨ç†èƒ½åŠ›ã€‚</li>
<li>åœ¨çœŸå®ä¸–ç•Œæ—¥å¿—ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒR-Logåœ¨å¤šä¸ªæ—¥å¿—åˆ†æä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯æœªè§åœºæ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25987">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-69715d28310fecb4f60263f2e91357c9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760100180&auth_key=1760100180-0-0-7bb93a008031b4297ef18899c76a49f0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8b407effde0354d1b213ea10b56c1dd5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760100188&auth_key=1760100188-0-0-b50053a135019d8b4736830c5514c5b9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-13941e9d4abd225206ea990c259a4f09~resize:0:q75.jpg?source=1f5c5e47&expiration=1760100194&auth_key=1760100194-0-0-2522717720990d67a1c9b6af40cd30e5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Mem-Î±-Learning-Memory-Construction-via-Reinforcement-Learning"><a href="#Mem-Î±-Learning-Memory-Construction-via-Reinforcement-Learning" class="headerlink" title="Mem-Î±: Learning Memory Construction via Reinforcement Learning"></a>Mem-Î±: Learning Memory Construction via Reinforcement Learning</h2><p><strong>Authors:Yu Wang, Ryuichi Takanobu, Zhiqi Liang, Yuzhen Mao, Yuanzhe Hu, Julian McAuley, Xiaojian Wu</strong></p>
<p>Large language model (LLM) agents are constrained by limited context windows, necessitating external memory systems for long-term information understanding. Current memory-augmented agents typically depend on pre-defined instructions and tools for memory updates. However, language models may lack the ability to determine which information to store, how to structure it, and when to update it, especially as memory systems become more complex. This results in suboptimal memory construction and information loss. To this end, we propose Mem-alpha, a reinforcement learning framework that trains agents to effectively manage complex memory systems through interaction and feedback. We also construct a specialized training dataset spanning diverse multi-turn interaction patterns paired with comprehensive evaluation questions designed to teach effective memory management. During training, agents process sequential information chunks, learn to extract and store relevant content, then update the memory system. The reward signal derives from downstream question-answering accuracy over the full interaction history, directly optimizing for memory construction. To illustrate the effectiveness of our training framework, we design a memory architecture comprising core, episodic, and semantic components, equipped with multiple tools for memory operations. Empirical evaluation demonstrates that Mem-alpha achieves significant improvements over existing memory-augmented agent baselines. Despite being trained exclusively on instances with a maximum length of 30k tokens, our agents exhibit remarkable generalization to sequences exceeding 400k tokens, over 13x the training length, highlighting the robustness of Mem-alpha. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†å—é™äºæœ‰é™çš„ä¸Šä¸‹æ–‡çª—å£ï¼Œéœ€è¦å¤–éƒ¨è®°å¿†ç³»ç»Ÿè¿›è¡Œé•¿æœŸä¿¡æ¯ç†è§£ã€‚å½“å‰çš„å¢å¼ºè®°å¿†ä»£ç†é€šå¸¸ä¾èµ–äºé¢„å…ˆå®šä¹‰çš„æŒ‡ä»¤å’Œå·¥å…·è¿›è¡Œè®°å¿†æ›´æ–°ã€‚ç„¶è€Œï¼Œè¯­è¨€æ¨¡å‹å¯èƒ½ç¼ºä¹åˆ¤æ–­å­˜å‚¨å“ªäº›ä¿¡æ¯ã€å¦‚ä½•æ„å»ºç»“æ„ã€ä½•æ—¶è¿›è¡Œæ›´æ–°çš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯éšç€è®°å¿†ç³»ç»Ÿå˜å¾—è¶Šæ¥è¶Šå¤æ‚ã€‚è¿™ä¼šå¯¼è‡´è®°å¿†æ„å»ºä¸ä½³å’Œä¿¡æ¯ä¸¢å¤±ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†Mem-alphaï¼Œè¿™æ˜¯ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡äº¤äº’å’Œåé¦ˆæ¥è®­ç»ƒä»£ç†æœ‰æ•ˆåœ°ç®¡ç†å¤æ‚çš„è®°å¿†ç³»ç»Ÿã€‚æˆ‘ä»¬è¿˜æ„å»ºäº†ä¸€ä¸ªç‰¹æ®Šè®­ç»ƒæ•°æ®é›†ï¼Œæ¶µç›–äº†å¤šç§å¤šè½®äº¤äº’æ¨¡å¼ï¼Œå¹¶é…æœ‰å…¨é¢çš„è¯„ä¼°é—®é¢˜ï¼Œä»¥æ•™æˆæœ‰æ•ˆçš„å†…å­˜ç®¡ç†ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä»£ç†å¤„ç†åºåˆ—ä¿¡æ¯å—ï¼Œå­¦ä¹ æå–å’Œå­˜å‚¨ç›¸å…³å†…å®¹ï¼Œç„¶åæ›´æ–°è®°å¿†ç³»ç»Ÿã€‚å¥–åŠ±ä¿¡å·æ¥æºäºä¸‹æ¸¸é—®ç­”çš„å‡†ç¡®æ€§ï¼Œè¿™æ˜¯åŸºäºæ•´ä¸ªäº¤äº’å†å²çš„ï¼Œç›´æ¥ä¼˜åŒ–å†…å­˜æ„å»ºã€‚ä¸ºäº†è¯´æ˜æˆ‘ä»¬è®­ç»ƒæ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§è®°å¿†æ¶æ„ï¼ŒåŒ…æ‹¬æ ¸å¿ƒã€æƒ…å¢ƒå’Œè¯­ä¹‰ç»„ä»¶ï¼Œå¹¶é…å¤‡äº†å¤šç§è®°å¿†æ“ä½œå·¥å…·ã€‚ç»éªŒè¯„ä¼°è¡¨æ˜ï¼ŒMem-alphaåœ¨ç°æœ‰çš„å¢å¼ºè®°å¿†ä»£ç†åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚å°½ç®¡ä»…åœ¨æœ€é•¿ä¸è¶…è¿‡30kä»¤ç‰Œçš„æƒ…å†µä¸‹è¿›è¡Œè®­ç»ƒï¼Œæˆ‘ä»¬çš„ä»£ç†åœ¨è¶…è¿‡40ä¸‡ä»¤ç‰Œçš„åºåˆ—ä¸­è¡¨ç°å‡ºäº†æ˜¾è‘—çš„æ³›åŒ–èƒ½åŠ›ï¼Œè¿™æ˜¯è®­ç»ƒé•¿åº¦çš„13å€ä»¥ä¸Šï¼Œçªæ˜¾äº†Mem-alphaçš„ç¨³å¥æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25911v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å—é™äºä¸Šä¸‹æ–‡çª—å£ï¼Œéœ€è¦å¤–éƒ¨è®°å¿†ç³»ç»Ÿè¿›è¡Œé•¿æœŸä¿¡æ¯ç†è§£ã€‚å½“å‰è®°å¿†å¢å¼ºå‹ä»£ç†äººé€šå¸¸ä¾èµ–äºé¢„å…ˆå®šä¹‰çš„æŒ‡ä»¤å’Œå·¥å…·è¿›è¡Œè®°å¿†æ›´æ–°ï¼Œä½†è¯­è¨€æ¨¡å‹å¯èƒ½åœ¨ç¡®å®šå­˜å‚¨å“ªäº›ä¿¡æ¯ã€å¦‚ä½•æ„å»ºå’Œæ›´æ–°æ–¹é¢å­˜åœ¨ç¼ºé™·ï¼Œå°¤å…¶æ˜¯åœ¨è®°å¿†ç³»ç»Ÿå˜å¾—æ›´å¤æ‚æ—¶ã€‚è¿™å¯¼è‡´äº†è®°å¿†æ„å»ºä¸ä½³å’Œä¿¡æ¯ä¸¢å¤±ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Mem-alphaå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡äº¤äº’å’Œåé¦ˆè®­ç»ƒä»£ç†äººæœ‰æ•ˆåœ°ç®¡ç†å¤æ‚è®°å¿†ç³»ç»Ÿã€‚æˆ‘ä»¬è¿˜æ„å»ºäº†ä¸€ä¸ªç‰¹æ®Šè®­ç»ƒæ•°æ®é›†ï¼Œæ¶µç›–äº†å¤šæ ·åŒ–çš„å¤šè½®äº¤äº’æ¨¡å¼ï¼Œå¹¶è®¾è®¡äº†å…¨é¢çš„è¯„ä¼°é—®é¢˜ï¼Œä»¥æ•™æˆæœ‰æ•ˆçš„å†…å­˜ç®¡ç†ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä»£ç†äººå¤„ç†åºåˆ—ä¿¡æ¯å—ï¼Œå­¦ä¹ æå–å’Œå­˜å‚¨ç›¸å…³å†…å®¹ï¼Œç„¶åæ›´æ–°è®°å¿†ç³»ç»Ÿã€‚å¥–åŠ±ä¿¡å·æ¥æºäºä¸‹æ¸¸é—®ç­”å‡†ç¡®ç‡çš„å…¨ç¨‹äº’åŠ¨å†å²è®°å½•ï¼Œç›´æ¥ä¼˜åŒ–å†…å­˜æ„å»ºã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åŒ…æ‹¬æ ¸å¿ƒã€æƒ…æ™¯å’Œè¯­ä¹‰ç»„ä»¶çš„å†…å­˜æ¶æ„ï¼Œé…å¤‡äº†å¤šç§å†…å­˜æ“ä½œå·¥å…·ã€‚ç»éªŒè¯„ä¼°è¡¨æ˜ï¼ŒMem-alphaåœ¨ç°æœ‰è®°å¿†å¢å¼ºå‹ä»£ç†äººåŸºçº¿æ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚å³ä½¿åœ¨ä»…å¯¹æœ€é•¿ä¸è¶…è¿‡3ä¸‡ä»¤ç‰Œçš„å®ä¾‹è¿›è¡Œè®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬çš„ä»£ç†äººåœ¨è¶…è¿‡4ä¸‡ä»¤ç‰Œçš„åºåˆ—ä¸Šä¹Ÿå±•ç°å‡ºäº†æ˜¾è‘—çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹å—é™äºä¸Šä¸‹æ–‡çª—å£ï¼Œéœ€è¦å¤–éƒ¨è®°å¿†ç³»ç»Ÿè¿›è¡Œé•¿æœŸä¿¡æ¯ç†è§£ã€‚</li>
<li>å½“å‰è®°å¿†å¢å¼ºå‹ä»£ç†äººä¾èµ–é¢„å®šä¹‰æŒ‡ä»¤å’Œå·¥å…·è¿›è¡Œè®°å¿†æ›´æ–°ã€‚</li>
<li>è¯­è¨€æ¨¡å‹åœ¨å†³å®šå­˜å‚¨å“ªäº›ä¿¡æ¯ä»¥åŠå¦‚ä½•æ„å»ºå’Œæ›´æ–°æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>Mem-alphaå¼ºåŒ–å­¦ä¹ æ¡†æ¶è®­ç»ƒä»£ç†äººæœ‰æ•ˆç®¡ç†å¤æ‚è®°å¿†ç³»ç»Ÿã€‚</li>
<li>é€šè¿‡äº¤äº’å’Œåé¦ˆï¼ŒMem-alphaä¼˜åŒ–äº†å†…å­˜æ„å»ºã€‚</li>
<li>Mem-alphaé€šè¿‡ç‰¹æ®Šè®­ç»ƒæ•°æ®é›†å’Œå…¨é¢çš„è¯„ä¼°é—®é¢˜æ¥æé«˜å†…å­˜ç®¡ç†æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25911">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-acddd1dbd6b265913b120be98463ee36~resize:0:q75.jpg?source=1f5c5e47&expiration=1760100201&auth_key=1760100201-0-0-8ab6859169e11ce8f1d69505c658e938&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-08fee7c484d1f0324b50cfd55c4cea91~resize:0:q75.jpg?source=1f5c5e47&expiration=1760100209&auth_key=1760100209-0-0-22ea3d55f71f79f3169114524629cafc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-77a1b08d74f9c369ebfdf75f825ee6c3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760100215&auth_key=1760100215-0-0-ab0c3ba4b1f9d86a613ad7d6acb91757&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ee6a2872e77bf853780f8e54820becaa~resize:0:q75.jpg?source=1f5c5e47&expiration=1760100222&auth_key=1760100222-0-0-bc5dfd953d21d15ed2d5988ec22bcade&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="MuSLR-Multimodal-Symbolic-Logical-Reasoning"><a href="#MuSLR-Multimodal-Symbolic-Logical-Reasoning" class="headerlink" title="MuSLR: Multimodal Symbolic Logical Reasoning"></a>MuSLR: Multimodal Symbolic Logical Reasoning</h2><p><strong>Authors:Jundong Xu, Hao Fei, Yuhui Zhang, Liangming Pan, Qijun Huang, Qian Liu, Preslav Nakov, Min-Yen Kan, William Yang Wang, Mong-Li Lee, Wynne Hsu</strong></p>
<p>Multimodal symbolic logical reasoning, which aims to deduce new facts from multimodal input via formal logic, is critical in high-stakes applications such as autonomous driving and medical diagnosis, as its rigorous, deterministic reasoning helps prevent serious consequences. To evaluate such capabilities of current state-of-the-art vision language models (VLMs), we introduce the first benchmark MuSLR for multimodal symbolic logical reasoning grounded in formal logical rules. MuSLR comprises 1,093 instances across 7 domains, including 35 atomic symbolic logic and 976 logical combinations, with reasoning depths ranging from 2 to 9. We evaluate 7 state-of-the-art VLMs on MuSLR and find that they all struggle with multimodal symbolic reasoning, with the best model, GPT-4.1, achieving only 46.8%. Thus, we propose LogiCAM, a modular framework that applies formal logical rules to multimodal inputs, boosting GPT-4.1â€™s Chain-of-Thought performance by 14.13%, and delivering even larger gains on complex logics such as first-order logic. We also conduct a comprehensive error analysis, showing that around 70% of failures stem from logical misalignment between modalities, offering key insights to guide future improvements. All data and code are publicly available at <a target="_blank" rel="noopener" href="https://llm-symbol.github.io/MuSLR">https://llm-symbol.github.io/MuSLR</a>. </p>
<blockquote>
<p>é¢å‘è‡ªåŠ¨é©¾é©¶å’ŒåŒ»ç–—è¯Šæ–­ç­‰é«˜é£é™©åº”ç”¨çš„å¤šæ¨¡æ€ç¬¦å·é€»è¾‘æ¨ç†ï¼ˆMultimodal Symbolic Logical Reasoningï¼‰è‡³å…³é‡è¦ï¼Œæ—¨åœ¨é€šè¿‡å½¢å¼é€»è¾‘ä»å¤šæ¨¡æ€è¾“å…¥ä¸­æ¨å¯¼å‡ºæ–°äº‹å®ã€‚å…¶ä¸¥è°¨ä¸”ç¡®å®šçš„æ¨ç†æœ‰åŠ©äºé¢„é˜²ä¸¥é‡åæœã€‚ä¸ºäº†è¯„ä¼°å½“å‰æœ€å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è¿™äº›èƒ½åŠ›æ–¹é¢çš„è¡¨ç°ï¼Œæˆ‘ä»¬æ¨å‡ºäº†åŸºäºå½¢å¼é€»è¾‘è§„åˆ™çš„ç¬¬ä¸€ä¸ªå¤šæ¨¡æ€ç¬¦å·é€»è¾‘æ¨ç†åŸºå‡†æµ‹è¯•MuSLRã€‚MuSLRåŒ…å«7ä¸ªé¢†åŸŸçš„1093ä¸ªå®ä¾‹ï¼ŒåŒ…æ‹¬35ä¸ªåŸå­ç¬¦å·é€»è¾‘å’Œ976ä¸ªé€»è¾‘ç»„åˆï¼Œæ¨ç†æ·±åº¦ä»2åˆ°9ä¸ç­‰ã€‚æˆ‘ä»¬åœ¨MuSLRä¸Šè¯„ä¼°äº†7ç§æœ€å…ˆè¿›çš„VLMsï¼Œå‘ç°å®ƒä»¬åœ¨å¤šæ¨¡æ€ç¬¦å·æ¨ç†æ–¹é¢éƒ½é‡åˆ°äº†å›°éš¾ï¼Œè¡¨ç°æœ€å¥½çš„GPT-4.1ä¹Ÿä»…è¾¾åˆ°46.8%ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†LogiCAMæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åº”ç”¨å½¢å¼é€»è¾‘è§„åˆ™æ¥å¤„ç†å¤šæ¨¡æ€è¾“å…¥ï¼Œæå‡äº†GPT-4.1çš„Chain-of-Thoughtæ€§èƒ½è¾¾14.13%ï¼Œå¹¶åœ¨ä¸€é˜¶é€»è¾‘ç­‰å¤æ‚é€»è¾‘ä¸Šå–å¾—äº†æ›´å¤§çš„è¿›æ­¥ã€‚æˆ‘ä»¬è¿˜è¿›è¡Œäº†å…¨é¢çš„é”™è¯¯åˆ†æï¼Œå‘ç°çº¦70%çš„é”™è¯¯æºäºæ¨¡æ€ä¹‹é—´çš„é€»è¾‘ä¸ä¸€è‡´æ€§ï¼Œè¿™ä¸ºæœªæ¥çš„æ”¹è¿›æä¾›äº†å…³é”®æŒ‡å¯¼ã€‚æ‰€æœ‰æ•°æ®å’Œä»£ç å‡å¯åœ¨<a target="_blank" rel="noopener" href="https://llm-symbol.github.io/MuSLR%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://llm-symbol.github.io/MuSLRå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25851v1">PDF</a> Accepted by NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤šæ¨¡æ€ç¬¦å·é€»è¾‘æ¨ç†çš„é‡è¦æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨è‡ªåŠ¨é©¾é©¶å’ŒåŒ»ç–—è¯Šæ–­ç­‰é«˜é£é™©åº”ç”¨ä¸­çš„ä½œç”¨ã€‚ä¸ºè¯„ä¼°å½“å‰æœ€å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„å¤šæ¨¡æ€ç¬¦å·é€»è¾‘æ¨ç†èƒ½åŠ›ï¼Œæ¨å‡ºäº†é¦–ä¸ªåŸºå‡†æµ‹è¯•MuSLRã€‚è¯„ä»·ç»“æœæ˜¾ç¤ºï¼Œç°æœ‰æ¨¡å‹åœ¨å¤šæ¨¡æ€ç¬¦å·é€»è¾‘æ¨ç†æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œäºæ˜¯æå‡ºäº†LogiCAMæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åº”ç”¨å½¢å¼é€»è¾‘è§„åˆ™å¯¹å¤šæ¨¡æ€è¾“å…¥è¿›è¡Œå¤„ç†ï¼Œæé«˜äº†GPT-4.1çš„æ€§èƒ½ã€‚é”™è¯¯åˆ†æè¡¨æ˜ï¼Œçº¦70%çš„é”™è¯¯æºäºä¸åŒæ¨¡æ€ä¹‹é—´çš„é€»è¾‘ä¸åŒ¹é…ï¼Œè¿™ä¸ºæœªæ¥çš„æ”¹è¿›æä¾›äº†å…³é”®æŒ‡å¯¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€ç¬¦å·é€»è¾‘æ¨ç†åœ¨é«˜é£é™©åº”ç”¨å¦‚è‡ªåŠ¨é©¾é©¶å’ŒåŒ»ç–—è¯Šæ–­ä¸­è‡³å…³é‡è¦ã€‚</li>
<li>æ¨å‡ºäº†é¦–ä¸ªè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹çš„å¤šæ¨¡æ€ç¬¦å·é€»è¾‘æ¨ç†èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•MuSLRã€‚</li>
<li>å½“å‰æœ€å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤šæ¨¡æ€ç¬¦å·é€»è¾‘æ¨ç†æ–¹é¢å­˜åœ¨å›°éš¾ã€‚</li>
<li>LogiCAMæ¡†æ¶é€šè¿‡åº”ç”¨å½¢å¼é€»è¾‘è§„åˆ™æé«˜å¤šæ¨¡æ€è¾“å…¥å¤„ç†æ€§èƒ½ã€‚</li>
<li>LogiCAMæ¡†æ¶æé«˜äº†GPT-4.1çš„æ¨ç†æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚é€»è¾‘å¦‚ä¸€é˜¶é€»è¾‘æ–¹é¢ã€‚</li>
<li>é”™è¯¯åˆ†ææ˜¾ç¤ºï¼Œçº¦70%çš„é”™è¯¯æºäºä¸åŒæ¨¡æ€ä¹‹é—´çš„é€»è¾‘ä¸åŒ¹é…ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25851">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-ddb50c7792ccfc11126cc2e495a4a642~resize:0:q75.jpg?source=1f5c5e47&expiration=1760100229&auth_key=1760100229-0-0-2285a516557376caf728d43978fa5c48&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0aca9ea72f1050556e16c4b97d5fdff8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760100239&auth_key=1760100239-0-0-104ba83ccc335bc2752478375b261e90&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6577a601c78e80ad4ac4e20ee01f3b2c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760100246&auth_key=1760100246-0-0-773885574fc152e2de260180ef330e38&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-643c50cadad343e471301c645ca8318e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760100252&auth_key=1760100252-0-0-9cbaf4f357e5d8490c5e99a5fd77bf05&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-284de651055dac6e1d23d2e2ef8b37c9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760100259&auth_key=1760100259-0-0-0d78b22bd2709f3082cfa589fd14feca&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c79ba1c46e466140212a790ce7280037~resize:0:q75.jpg?source=1f5c5e47&expiration=1760103033&auth_key=1760103033-0-0-fa4b6f84a812686d1f8413a225ef8dd2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="RL-Guided-Data-Selection-for-Language-Model-Finetuning"><a href="#RL-Guided-Data-Selection-for-Language-Model-Finetuning" class="headerlink" title="RL-Guided Data Selection for Language Model Finetuning"></a>RL-Guided Data Selection for Language Model Finetuning</h2><p><strong>Authors:Animesh Jha, Harshit Gupta, Ananjan Nandi</strong></p>
<p>Data selection for finetuning Large Language Models (LLMs) can be framed as a budget-constrained optimization problem: maximizing a modelâ€™s downstream performance under a strict training data budget. Solving this problem is generally intractable, and existing approximate approaches are pretraining-oriented and transfer poorly to the fine-tuning setting. We reformulate this problem as a tractable Markov Decision Process (MDP) and train agents using various Reinforcement Learning (RL) methods to learn optimal data selection policies, guided by an efficient, proxy-model-based reward signal. Across four datasets, training on a $5%$ subset selected by our approach matches or outperforms fine-tuning on the full dataset by up to $10.8$ accuracy points, while cutting wall-clock training time by up to $2 \times$, highlighting the promise of RL-guided data selection. </p>
<blockquote>
<p>é’ˆå¯¹å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ•°æ®é€‰æ‹©å¯æ„å»ºä¸ºä¸€ä¸ªé¢„ç®—çº¦æŸä¼˜åŒ–é—®é¢˜ï¼šåœ¨ä¸¥æ ¼çš„è®­ç»ƒæ•°æ®é¢„ç®—ä¸‹ï¼Œæœ€å¤§åŒ–æ¨¡å‹çš„ä¸‹æ¸¸æ€§èƒ½ã€‚è§£å†³è¿™ä¸ªé—®é¢˜é€šå¸¸æ˜¯æ£˜æ‰‹çš„ï¼Œç°æœ‰çš„è¿‘ä¼¼æ–¹æ³•ä»¥é¢„è®­ç»ƒä¸ºå¯¼å‘ï¼Œåœ¨å¾®è°ƒè®¾ç½®ä¸­çš„è¿ç§»æ•ˆæœè¾ƒå·®ã€‚æˆ‘ä»¬å°†è¿™ä¸ªé—®é¢˜é‡æ–°è¡¨è¿°ä¸ºä¸€ä¸ªå¯è¡Œçš„é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ï¼Œå¹¶ä½¿ç”¨å„ç§å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•æ¥è®­ç»ƒä»£ç†ï¼Œä»¥å­¦ä¹ æœ€ä½³æ•°æ®é€‰æ‹©ç­–ç•¥ï¼Œç”±ä¸€ä¸ªé«˜æ•ˆçš„åŸºäºä»£ç†æ¨¡å‹çš„å¥–åŠ±ä¿¡å·æ¥æŒ‡å¯¼ã€‚åœ¨å››ä¸ªæ•°æ®é›†ä¸Šï¼Œä½¿ç”¨æˆ‘ä»¬çš„æ–¹æ³•æ‰€é€‰çš„5%å­é›†è¿›è¡Œè®­ç»ƒï¼Œä¸åœ¨å®Œæ•´æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒçš„æ•ˆæœç›¸åŒ¹æ•Œæˆ–è¡¨ç°å¾—æ›´å¥½ï¼Œå‡†ç¡®åº¦æé«˜é«˜è¾¾10.8ä¸ªç™¾åˆ†ç‚¹ï¼ŒåŒæ—¶å°†è®­ç»ƒæ—¶é—´å‡å°‘äº†ä¸€åŠï¼Œè¿™å‡¸æ˜¾äº†ç”±å¼ºåŒ–å­¦ä¹ å¼•å¯¼çš„æ•°æ®é€‰æ‹©çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25850v1">PDF</a> To appear in NeurIPS 2025 Constrained Optimization for ML Workshop</p>
<p><strong>Summary</strong></p>
<p>åœ¨ä¸¥æ ¼çš„è®­ç»ƒæ•°æ®é¢„ç®—ä¸‹ï¼Œå¦‚ä½•æœ€å¤§åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¸‹æ¸¸æ€§èƒ½æˆä¸ºäº†ä¸€ä¸ªé‡è¦çš„é¢„ç®—çº¦æŸä¼˜åŒ–é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæ–‡ç« æå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ–¹æ³•ï¼Œå°†è¿™ä¸ªé—®é¢˜é‡æ–°è¡¨è¿°ä¸ºä¸€ä¸ªå¯è¡Œçš„é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ã€‚ä½¿ç”¨å„ç§RLæ–¹æ³•è®­ç»ƒä»£ç†ï¼Œé€šè¿‡é«˜æ•ˆçš„åŸºäºä»£ç†æ¨¡å‹çš„å¥–åŠ±ä¿¡å·æ¥æŒ‡å¯¼ï¼Œå­¦ä¹ æœ€ä½³æ•°æ®é€‰æ‹©ç­–ç•¥ã€‚åœ¨å››ä¸ªæ•°æ®é›†ä¸Šï¼Œä½¿ç”¨æ­¤æ–¹æ³•é€‰æ‹©çš„5%å­é›†è¿›è¡Œè®­ç»ƒï¼Œä¸å…¨æ•°æ®é›†è¿›è¡Œå¾®è°ƒçš„æ•ˆæœç›¸åŒ¹é…æˆ–æ›´å¥½ï¼Œå‡†ç¡®åº¦æé«˜äº†é«˜è¾¾10.8ä¸ªç‚¹ï¼ŒåŒæ—¶èŠ‚çœäº†é«˜è¾¾ä¸¤å€çš„å®é™…è®­ç»ƒæ—¶é—´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ•°æ®é€‰æ‹©å¯¹äºå¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡³å…³é‡è¦ï¼Œå¯è§†ä¸ºé¢„ç®—çº¦æŸä¼˜åŒ–é—®é¢˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åå‘äºé¢„è®­ç»ƒï¼Œåœ¨å¾®è°ƒè®¾ç½®ä¸­è¡¨ç°ä¸ä½³ã€‚</li>
<li>æ–‡ç« å°†é—®é¢˜é‡æ–°è¡¨è¿°ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ï¼Œä½¿å…¶å˜å¾—å¯è¡Œã€‚</li>
<li>ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•è®­ç»ƒä»£ç†ï¼Œä»¥å­¦ä¹ æœ€ä½³æ•°æ®é€‰æ‹©ç­–ç•¥ã€‚</li>
<li>åŸºäºä»£ç†æ¨¡å‹çš„å¥–åŠ±ä¿¡å·æ¥æŒ‡å¯¼æ•°æ®é€‰æ‹©è¿‡ç¨‹ã€‚</li>
<li>åœ¨å››ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨æ­¤æ–¹æ³•é€‰æ‹©çš„å­é›†è¿›è¡Œè®­ç»ƒï¼Œæ•ˆæœä¸å…¨æ•°æ®é›†å¾®è°ƒç›¸åŒ¹é…æˆ–æ›´å¥½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25850">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-eacdd51aa904838c116db235eb7888d8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760100331&auth_key=1760100331-0-0-e1c9f52319ef739ddb1e4cf8058cc79f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b6434ca94974e036cc017c6bc723c53e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760103040&auth_key=1760103040-0-0-f7675eb0f8ffcef7fce1c3a8c3130a72&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="More-Thought-Less-Accuracy-On-the-Dual-Nature-of-Reasoning-in-Vision-Language-Models"><a href="#More-Thought-Less-Accuracy-On-the-Dual-Nature-of-Reasoning-in-Vision-Language-Models" class="headerlink" title="More Thought, Less Accuracy? On the Dual Nature of Reasoning in   Vision-Language Models"></a>More Thought, Less Accuracy? On the Dual Nature of Reasoning in   Vision-Language Models</h2><p><strong>Authors:Xinyu Tian, Shu Zou, Zhaoyuan Yang, Mengqi He, Fabian Waschkowski, Lukas Wesemann, Peter Tu, Jing Zhang</strong></p>
<p>Reasoning has emerged as a pivotal capability in Large Language Models (LLMs). Through Reinforcement Learning (RL), typically Group Relative Policy Optimization (GRPO), these models are able to solve complex tasks such as mathematics and code generation. Building on these advances, recent research has sought to extend reasoning to Vision-Language Models (VLMs), yielding promising results across diverse visual tasks. Despite this progress, our study uncovers the dual nature of multimodal reasoning: while it substantially enhances logical inference and facilitates performance on challenging problems, it may gradually impair perceptual grounding, leading to recognition failures on otherwise basic visual questions. Through further analysis, we attribute this phenomenon to visual forgetting, wherein prolonged reasoning causes the model to increasingly disregard visual input. To address this, we propose Vision-Anchored Policy Optimization (VAPO), a simple yet effective method that explicitly steers the reasoning process toward visually grounded trajectories. Our result model, VAPO-Thinker-7B, significantly strengthens the modelâ€™s reliance on visual information and achieves new state-of-the-art results on a wide range of established benchmarks. Project page: <a target="_blank" rel="noopener" href="https://xytian1008.github.io/VAPO/">https://xytian1008.github.io/VAPO/</a> </p>
<blockquote>
<p>æ¨ç†å·²ç»æˆä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„å…³é”®èƒ½åŠ›ã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œé€šå¸¸æ˜¯ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œè¿™äº›æ¨¡å‹èƒ½å¤Ÿè§£å†³æ•°å­¦å’Œä»£ç ç”Ÿæˆç­‰å¤æ‚ä»»åŠ¡ã€‚åœ¨è¿™äº›è¿›å±•çš„åŸºç¡€ä¸Šï¼Œæœ€è¿‘çš„ç ”ç©¶è¯•å›¾å°†æ¨ç†æ‰©å±•åˆ°è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œåœ¨ä¸åŒçš„è§†è§‰ä»»åŠ¡ä¸­å–å¾—äº†ä»¤äººé¼“èˆçš„ç»“æœã€‚å°½ç®¡å–å¾—äº†è¿›å±•ï¼Œä½†æˆ‘ä»¬çš„ç ”ç©¶å‘ç°å¤šæ¨¡æ€æ¨ç†å…·æœ‰åŒé‡æ€§è´¨ï¼šè™½ç„¶å®ƒå¤§å¤§æé«˜äº†é€»è¾‘æ¨ç†èƒ½åŠ›ï¼Œæœ‰åŠ©äºè§£å†³å…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ï¼Œä½†å®ƒå¯èƒ½ä¼šé€æ¸æŸå®³æ„ŸçŸ¥å®šä½èƒ½åŠ›ï¼Œå¯¼è‡´å¯¹åŸºæœ¬è§†è§‰é—®é¢˜çš„è¯†åˆ«å¤±è´¥ã€‚é€šè¿‡è¿›ä¸€æ­¥åˆ†æï¼Œæˆ‘ä»¬å°†è¿™ç§ç°è±¡å½’å› äºè§†è§‰é—å¿˜ï¼Œé•¿æ—¶é—´çš„æ¨ç†ä¼šå¯¼è‡´æ¨¡å‹è¶Šæ¥è¶Šå¿½è§†è§†è§‰è¾“å…¥ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†è§†è§‰é”šå®šç­–ç•¥ä¼˜åŒ–ï¼ˆVAPOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ–¹æ³•ï¼Œèƒ½å¤Ÿæ˜ç¡®å¼•å¯¼æ¨ç†è¿‡ç¨‹èµ°å‘è§†è§‰è½¨è¿¹ã€‚æˆ‘ä»¬çš„ç»“æœæ¨¡å‹VAPO-Thinker-7Bå¤§å¤§åŠ å¼ºäº†æ¨¡å‹å¯¹è§†è§‰ä¿¡æ¯çš„ä¾èµ–ï¼Œå¹¶åœ¨å¹¿æ³›çš„åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æ–°çš„æœ€å…ˆè¿›ç»“æœã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://xytian1008.github.io/VAPO/">https://xytian1008.github.io/VAPO/</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25848v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œé›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç­‰æ–¹æ³•ï¼Œå±•ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œå¯ä»¥å®Œæˆæ•°å­¦å’Œä»£ç ç”Ÿæˆç­‰å¤æ‚ä»»åŠ¡ã€‚è¿‘æœŸç ”ç©¶å°è¯•å°†æ¨ç†èƒ½åŠ›æ‰©å±•åˆ°è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œå¹¶åœ¨å¤šç§è§†è§‰ä»»åŠ¡ä¸­å–å¾—ä¼˜å¼‚è¡¨ç°ã€‚ç„¶è€Œï¼Œæœ¬ç ”ç©¶å‘ç°å¤šæ¨¡æ€æ¨ç†å­˜åœ¨åŒé‡æ€§è´¨ï¼šè™½ç„¶èƒ½æé«˜é€»è¾‘æ¨æ–­èƒ½åŠ›å¹¶è§£å†³éš¾é¢˜ï¼Œä½†å¯èƒ½é€æ¸æŸå®³æ„ŸçŸ¥å®šä½èƒ½åŠ›ï¼Œå¯¼è‡´å¯¹åŸºæœ¬è§†è§‰é—®é¢˜çš„è¯†åˆ«å¤±è´¥ã€‚è§†è§‰é—å¿˜æ˜¯å¯¼è‡´è¿™ä¸€ç°è±¡çš„å…³é”®å› ç´ ï¼Œé•¿æœŸæ¨ç†ä¼šä½¿æ¨¡å‹è¶Šæ¥è¶Šå¿½è§†è§†è§‰è¾“å…¥ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬ç ”ç©¶æå‡ºè§†è§‰é”šå®šç­–ç•¥ä¼˜åŒ–ï¼ˆVAPOï¼‰ï¼Œé€šè¿‡æ˜ç¡®å¼•å¯¼æ¨ç†è¿‡ç¨‹èµ°å‘è§†è§‰è½¨è¿¹ï¼Œæœ‰æ•ˆå¼ºåŒ–æ¨¡å‹å¯¹è§†è§‰ä¿¡æ¯çš„ä¾èµ–ï¼Œå¹¶åœ¨å¹¿æ³›å»ºç«‹çš„åŸºå‡†æµ‹è¯•ä¸Šå–å¾—æœ€æ–°æœ€å…ˆè¿›çš„æˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹é€šè¿‡å¼ºåŒ–å­¦ä¹ å’Œé›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–å±•ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤šæ¨¡æ€æ¨ç†æ–¹é¢å–å¾—ä¼˜å¼‚è¡¨ç°ã€‚</li>
<li>å¤šæ¨¡æ€æ¨ç†å­˜åœ¨åŒé‡æ€§è´¨ï¼šæé«˜é€»è¾‘æ¨æ–­èƒ½åŠ›çš„åŒæ—¶å¯èƒ½æŸå®³æ„ŸçŸ¥å®šä½èƒ½åŠ›ã€‚</li>
<li>è§†è§‰é—å¿˜æ˜¯é•¿æœŸæ¨ç†ä¸­æ¨¡å‹å¿½è§†è§†è§‰è¾“å…¥çš„å…³é”®å› ç´ ã€‚</li>
<li>è§†è§‰é”šå®šç­–ç•¥ä¼˜åŒ–ï¼ˆVAPOï¼‰èƒ½æœ‰æ•ˆå¼ºåŒ–æ¨¡å‹å¯¹è§†è§‰ä¿¡æ¯çš„ä¾èµ–ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25848">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-4bfbb7e17a789fe2ec95b2f27cd8bb94~resize:0:q75.jpg?source=1f5c5e47&expiration=1760103048&auth_key=1760103048-0-0-c320ac71909ac23ea8bbd809150558db&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ba378dd9af35af3afc2b52b0166cc054~resize:0:q75.jpg?source=1f5c5e47&expiration=1760103056&auth_key=1760103056-0-0-883e382863a2c5290c0ef30a6803d55b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-143e3b9879c93781096a59cc3720451e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760103063&auth_key=1760103063-0-0-ad4d6215a90aff53c49bbfcc62987889&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-07dbd0930e682213a76d40ad929caa27~resize:0:q75.jpg?source=1f5c5e47&expiration=1760103070&auth_key=1760103070-0-0-6d927fd1f12066b46edbaae119b2064f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Chain-in-Tree-Back-to-Sequential-Reasoning-in-LLM-Tree-Search"><a href="#Chain-in-Tree-Back-to-Sequential-Reasoning-in-LLM-Tree-Search" class="headerlink" title="Chain-in-Tree: Back to Sequential Reasoning in LLM Tree Search"></a>Chain-in-Tree: Back to Sequential Reasoning in LLM Tree Search</h2><p><strong>Authors:Xinzhe Li</strong></p>
<p>Test-time scaling enables large language models (LLMs) to improve performance on long-horizon reasoning tasks by allocating additional compute at inference. Tree-search-based approaches achieve state-of-the-art results in this setting, but they are notoriously inefficient, often an order of magnitude slower than simpler iterative methods. We introduce Chain-in-Tree (CiT), a plug-in framework that adaptively decides when to branch during search rather than branching at every step. CiT relies on lightweight Branching Necessity (BN) evaluation methods: BN-DP (Direct Prompting), where an auxiliary LLM directly judges whether a step requires branching, and BN-SC (Self-Consistency), which clusters multiple candidate actions to estimate agreement. We integrate CiT into three representative LLM-in-the-loop tree search frameworks: Tree of Thoughts (ToT-BS), ReST-MCTS, and RAP, and evaluate across GSM8K and Math500. Our results show that: (1) BN-DP consistently reduces token generation, model invocations, and runtime by 75-85 percent across all settings, with negligible accuracy loss and sometimes accuracy gains; (2) BN-SC typically yields substantial savings (up to 80 percent) but shows instability in 1-4 out of 14 settings, caused by a small subset of examples that produce very long reasoning steps; (3) the quality of auxiliary LLMs is critical, not only the BN evaluator in BN-DP, but also the models used in BN-SC for clustering and equivalence checking. When these roles are filled by smaller LLMs, performance degrades. Importantly, BN-SC does not require LLMs in domains with deterministic action spaces, where clustering can be done programmatically. We also provide a theoretical guarantee that BN-DP never increases LLM invocations relative to the baseline and release a unified implementation of CiT across ToT-BS, ReST-MCTS, and RAP to facilitate reproducibility and extension. </p>
<blockquote>
<p>æµ‹è¯•æ—¶ç¼©æ”¾åŠŸèƒ½ä½¿å¾—å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿåœ¨æ¨ç†ä»»åŠ¡ä¸Šæé«˜æ€§èƒ½ï¼Œé€šè¿‡åœ¨æ¨ç†æ—¶åˆ†é…é¢å¤–çš„è®¡ç®—èµ„æºã€‚åŸºäºæ ‘æœç´¢çš„æ–¹æ³•åœ¨è¿™ä¸ªè®¾ç½®ä¸‹è¾¾åˆ°äº†æœ€å¥½çš„ç»“æœï¼Œä½†å®ƒä»¬ä»¥æ•ˆç‡é—»åä¸ä½³ï¼Œé€šå¸¸æ¯”æ›´ç®€å•çš„è¿­ä»£æ–¹æ³•æ…¢ä¸€ä¸ªæ•°é‡çº§ã€‚æˆ‘ä»¬å¼•å…¥äº†Chain-in-Treeï¼ˆCiTï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ’ä»¶æ¡†æ¶ï¼Œå®ƒè‡ªé€‚åº”åœ°å†³å®šä½•æ—¶è¿›è¡Œåˆ†æ”¯æœç´¢ï¼Œè€Œä¸æ˜¯æ¯ä¸€æ­¥éƒ½è¿›è¡Œåˆ†æ”¯ã€‚CiTä¾èµ–äºè½»é‡çº§çš„Branching Necessityï¼ˆBNï¼‰è¯„ä¼°æ–¹æ³•ï¼šBN-DPï¼ˆç›´æ¥æç¤ºï¼‰ï¼Œå…¶ä¸­è¾…åŠ©LLMç›´æ¥åˆ¤æ–­ä¸€ä¸ªæ­¥éª¤æ˜¯å¦éœ€è¦åˆ†æ”¯ï¼›ä»¥åŠBN-SCï¼ˆè‡ªæˆ‘ä¸€è‡´æ€§ï¼‰ï¼Œå®ƒé€šè¿‡å¯¹å¤šä¸ªå€™é€‰åŠ¨ä½œè¿›è¡Œèšç±»æ¥ä¼°è®¡ä¸€è‡´æ€§ã€‚æˆ‘ä»¬å°†CiTé›†æˆåˆ°ä¸‰ä¸ªå…·æœ‰ä»£è¡¨æ€§çš„LLMå¾ªç¯æ ‘æœç´¢æ¡†æ¶ä¸­ï¼šæ€ç»´æ ‘ï¼ˆToT-BSï¼‰ã€ReST-MCTSå’ŒRAPï¼Œå¹¶åœ¨GSM8Kå’ŒMath500ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼šï¼ˆ1ï¼‰BN-DPåœ¨æ‰€æœ‰è®¾ç½®ä¸­ä¸€è‡´åœ°å‡å°‘äº†ä»¤ç‰Œç”Ÿæˆã€æ¨¡å‹è°ƒç”¨å’Œè¿è¡Œæ—¶ï¼Œå‡å°‘äº†75-85%ï¼Œå¹¶ä¸”å‡ ä¹æ²¡æœ‰æŸå¤±å‡†ç¡®æ€§ï¼Œæœ‰æ—¶ç”šè‡³èƒ½æé«˜å‡†ç¡®æ€§ï¼›ï¼ˆ2ï¼‰BN-SCé€šå¸¸å¯ä»¥äº§ç”Ÿå·¨å¤§çš„èŠ‚çœï¼ˆé«˜è¾¾80%ï¼‰ï¼Œä½†åœ¨14ä¸ªè®¾ç½®ä¸­çš„1-4ä¸ªè¡¨ç°å‡ºä¸ç¨³å®šï¼Œè¿™æ˜¯ç”±äºä¸€å°éƒ¨åˆ†ä¾‹å­äº§ç”Ÿäº†éå¸¸é•¿çš„æ¨ç†æ­¥éª¤ï¼›ï¼ˆ3ï¼‰è¾…åŠ©LLMçš„è´¨é‡è‡³å…³é‡è¦ï¼Œä¸ä»…æ˜¯BN-DPä¸­çš„BNè¯„ä¼°å™¨ï¼Œè¿˜åŒ…æ‹¬ç”¨äºèšç±»å’Œç­‰ä»·æ£€æŸ¥çš„BN-SCä¸­çš„æ¨¡å‹ã€‚å½“è¿™äº›è§’è‰²ç”±è¾ƒå°çš„LLMæ‰®æ¼”æ—¶ï¼Œæ€§èƒ½ä¼šä¸‹é™ã€‚é‡è¦çš„æ˜¯ï¼ŒBN-SCåœ¨å…·æœ‰ç¡®å®šæ€§åŠ¨ä½œç©ºé—´çš„é¢†åŸŸä¸­ä¸éœ€è¦LLMï¼Œè¿™äº›é¢†åŸŸä¸­å¯ä»¥é€šè¿‡ç¼–ç¨‹è¿›è¡Œèšç±»ã€‚æˆ‘ä»¬è¿˜æä¾›äº†ç†è®ºä¿è¯ï¼ŒBN-DPç›¸å¯¹äºåŸºå‡†çº¿ä¸ä¼šå¢åŠ LLMçš„è°ƒç”¨æ¬¡æ•°ï¼Œå¹¶å‘å¸ƒäº†åœ¨ToT-BSã€ReST-MCTSå’ŒRAPä¸Šçš„CiTçš„ç»Ÿä¸€å®ç°ï¼Œä»¥ä¿ƒè¿›å¯é‡å¤æ€§å’Œæ‰©å±•æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25835v2">PDF</a> Under Review</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†ä»»åŠ¡ä¸­æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºChain-in-Treeï¼ˆCiTï¼‰çš„æ’ä»¶æ¡†æ¶ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿè‡ªé€‚åº”åœ°å†³å®šä½•æ—¶è¿›è¡Œåˆ†æ”¯æœç´¢ï¼Œè€Œä¸æ˜¯åœ¨æ¯ä¸€æ­¥éƒ½è¿›è¡Œåˆ†æ”¯ã€‚æ–‡ç« ä»‹ç»äº†ä¸¤ç§åˆ†æ”¯å¿…è¦æ€§ï¼ˆBNï¼‰è¯„ä¼°æ–¹æ³•ï¼šBN-DPï¼ˆç›´æ¥æç¤ºï¼‰å’ŒBN-SCï¼ˆè‡ªæˆ‘ä¸€è‡´æ€§ï¼‰ã€‚é€šè¿‡é›†æˆCiTåˆ°ä¸‰ç§ä»£è¡¨æ€§çš„LLMå¾ªç¯æ ‘æœç´¢æ¡†æ¶ä¸­ï¼Œå®éªŒç»“æœè¡¨æ˜BN-DPèƒ½å¤§å¹…é™ä½æ ‡è®°ç”Ÿæˆã€æ¨¡å‹è°ƒç”¨å’Œè¿è¡Œæ—¶æˆæœ¬ï¼ŒåŒæ—¶ä¿æŒæˆ–æé«˜å‡†ç¡®æ€§ï¼›BN-SCåœ¨å¤§éƒ¨åˆ†æƒ…å†µä¸‹èƒ½èŠ‚çœå¤§é‡è®¡ç®—èµ„æºï¼Œä½†åœ¨éƒ¨åˆ†æƒ…å†µä¸‹è¡¨ç°å‡ºä¸ç¨³å®šæ€§ã€‚è¾…åŠ©LLMçš„è´¨é‡å¯¹æ€§èƒ½è‡³å…³é‡è¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Test-time scalingé€šè¿‡åˆ†é…é¢å¤–çš„è®¡ç®—èµ„æºåœ¨æ¨ç†æ—¶æé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é•¿å‘¨æœŸæ¨ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>Tree-search-basedæ–¹æ³•è™½ç„¶åœ¨æ­¤è®¾ç½®ä¸­è·å¾—æœ€ä½³ç»“æœï¼Œä½†æ•ˆç‡æä½ï¼Œé€šå¸¸æ¯”ç®€å•çš„è¿­ä»£æ–¹æ³•æ…¢ä¸€ä¸ªæ•°é‡çº§ã€‚</li>
<li>Chain-in-Tree (CiT)æ˜¯ä¸€ç§æ’ä»¶æ¡†æ¶ï¼Œèƒ½å¤Ÿè‡ªé€‚åº”å†³å®šä½•æ—¶è¿›è¡Œåˆ†æ”¯æœç´¢ï¼Œè€Œä¸æ˜¯åœ¨æ¯ä¸€æ­¥éƒ½è¿›è¡Œåˆ†æ”¯ï¼Œæé«˜äº†æ•ˆç‡ã€‚</li>
<li>CiTä½¿ç”¨ä¸¤ç§Branching Necessity (BN)è¯„ä¼°æ–¹æ³•ï¼šBN-DPï¼ˆç›´æ¥æç¤ºï¼‰å’ŒBN-SCï¼ˆè‡ªæˆ‘ä¸€è‡´æ€§ï¼‰ã€‚</li>
<li>BN-DPèƒ½å¤§å¹…é™ä½æ ‡è®°ç”Ÿæˆã€æ¨¡å‹è°ƒç”¨å’Œè¿è¡Œæ—¶æˆæœ¬ï¼ŒåŒæ—¶ä¿æŒæˆ–æé«˜å‡†ç¡®æ€§ã€‚</li>
<li>BN-SCåœ¨å¤§éƒ¨åˆ†æƒ…å†µä¸‹èƒ½èŠ‚çœå¤§é‡è®¡ç®—èµ„æºï¼Œä½†åœ¨ç‰¹å®šæƒ…å†µä¸‹å¯èƒ½è¡¨ç°å‡ºä¸ç¨³å®šã€‚</li>
<li>è¾…åŠ©LLMçš„è´¨é‡å¯¹CiTæ¡†æ¶çš„æ€§èƒ½è‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25835">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-86130f39ed1e92c57f4bc715580ab2cf~resize:0:q75.jpg?source=1f5c5e47&expiration=1760103078&auth_key=1760103078-0-0-bc10c1d909e60ad113e4ce505bd568d5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7ca92f7436f9c65832592d8bb5fb883d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760103085&auth_key=1760103085-0-0-01961f09d654ed5167bc8c6acb040099&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8794920a42b40287bd06cfcd962bf218~resize:0:q75.jpg?source=1f5c5e47&expiration=1760103093&auth_key=1760103093-0-0-4e50e0d3be4c84a666e42f54a74cc361&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5da2ee78896e5b7cdea269b655576970~resize:0:q75.jpg?source=1f5c5e47&expiration=1760103102&auth_key=1760103102-0-0-76bc98ab0cbff7080307738ba9c99ab8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-451bb97bc3fbf12c464dcd25a5fcfe47~resize:0:q75.jpg?source=1f5c5e47&expiration=1760103110&auth_key=1760103110-0-0-0984740af838002f891ebe5348029975&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Dolphin-v1-0-Technical-Report"><a href="#Dolphin-v1-0-Technical-Report" class="headerlink" title="Dolphin v1.0 Technical Report"></a>Dolphin v1.0 Technical Report</h2><p><strong>Authors:Taohan Weng, Chi zhang, Chaoran Yan, Siya Liu, Xiaoyang Liu, Yalun Wu, Boyang Wang, Boyan Wang, Jiren Ren, Kaiwen Yan, Jinze Yu, Kaibing Hu, Henan Liu, Haoyun Zheng, Zhenyu Liu, Duo Zhang, Xiaoqing Guo, Anjie Le, Hongcheng Guo</strong></p>
<p>Ultrasound is crucial in modern medicine but faces challenges like operator dependence, image noise, and real-time scanning, hindering AI integration. While large multimodal models excel in other medical imaging areas, they struggle with ultrasoundâ€™s complexities. To address this, we introduce Dolphin v1.0 (V1) and its reasoning-augmented version, Dolphin R1-the first large-scale multimodal ultrasound foundation models unifying diverse clinical tasks in a single vision-language framework.To tackle ultrasound variability and noise, we curated a 2-million-scale multimodal dataset, combining textbook knowledge, public data, synthetic samples, and general corpora. This ensures robust perception, generalization, and clinical adaptability.The Dolphin series employs a three-stage training strategy: domain-specialized pretraining, instruction-driven alignment, and reinforcement-based refinement. Dolphin v1.0 delivers reliable performance in classification, detection, regression, and report generation. Dolphin R1 enhances diagnostic inference, reasoning transparency, and interpretability through reinforcement learning with ultrasound-specific rewards.Evaluated on U2-Bench across eight ultrasound tasks, Dolphin R1 achieves a U2-score of 0.5835-over twice the second-best model (0.2968) setting a new state of the art. Dolphin v1.0 also performs competitively, validating the unified framework. Comparisons show reasoning-enhanced training significantly improves diagnostic accuracy, consistency, and interpretability, highlighting its importance for high-stakes medical AI. </p>
<blockquote>
<p>è¶…å£°åœ¨ç°ä»£åŒ»å­¦ä¸­è‡³å…³é‡è¦ï¼Œä½†é¢ä¸´ç€æ“ä½œè€…ä¾èµ–ã€å›¾åƒå™ªå£°å’Œå®æ—¶æ‰«æç­‰æŒ‘æˆ˜ï¼Œé˜»ç¢äº†äººå·¥æ™ºèƒ½çš„èåˆã€‚è™½ç„¶åœ¨å…¶ä»–åŒ»å­¦å½±åƒé¢†åŸŸï¼Œå¤§å‹å¤šæ¨¡å¼æ¨¡å‹è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨åº”å¯¹è¶…å£°çš„å¤æ‚æ€§æ–¹é¢ï¼Œå®ƒä»¬å´è¡¨ç°æŒ£æ‰ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Dolphin v1.0ï¼ˆV1ï¼‰åŠå…¶å¢å¼ºæ¨ç†ç‰ˆæœ¬Dolphin R1â€”â€”è¿™æ˜¯ç¬¬ä¸€ä¸ªç»Ÿä¸€äº†å¤šç§ä¸´åºŠä»»åŠ¡çš„å¤§å‹è¶…å£°å¤šæ¨¡å¼åŸºç¡€æ¨¡å‹ï¼Œåœ¨ä¸€ä¸ªè§†è§‰è¯­è¨€æ¡†æ¶å†…ã€‚ä¸ºäº†åº”å¯¹è¶…å£°çš„å˜æ€§å’Œå™ªå£°é—®é¢˜ï¼Œæˆ‘ä»¬ç­›é€‰äº†ä¸€ä¸ªè§„æ¨¡è¾¾ä¸¤ç™¾ä¸‡çš„å¤šæ¨¡å¼æ•°æ®é›†ï¼Œç»“åˆäº†æ•™ç§‘ä¹¦çŸ¥è¯†ã€å…¬å¼€æ•°æ®ã€åˆæˆæ ·æœ¬å’Œä¸€èˆ¬è¯­æ–™åº“ã€‚è¿™ç¡®ä¿äº†ç¨³å¥çš„æ„ŸçŸ¥ã€æ¨å¹¿å’Œä¸´åºŠé€‚åº”æ€§ã€‚Dolphinç³»åˆ—é‡‡ç”¨äº†ä¸€ç§ä¸‰é˜¶æ®µåŸ¹è®­ç­–ç•¥ï¼šä¸“ä¸šé¢†åŸŸçš„é¢„è®­ç»ƒã€æŒ‡ä»¤é©±åŠ¨çš„è°ƒæ•´å’ŒåŸºäºå¼ºåŒ–å­¦ä¹ çš„ç»†åŒ–ã€‚Dolphin v1.0åœ¨åˆ†ç±»ã€æ£€æµ‹ã€å›å½’å’ŒæŠ¥å‘Šç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºå¯é çš„æ€§èƒ½ã€‚Dolphin R1é€šè¿‡å¼ºåŒ–å­¦ä¹ ä½¿ç”¨è¶…å£°ç‰¹å®šå¥–åŠ±å¢å¼ºè¯Šæ–­æ¨ç†ã€é€æ˜åº¦å’Œè§£é‡Šæ€§ã€‚åœ¨U2-Benchä¸Šå¯¹å…«ä¸ªè¶…å£°ä»»åŠ¡è¿›è¡Œè¯„ä¼°ï¼ŒDolphin R1çš„U2åˆ†æ•°ä¸º0.5835ï¼Œæ˜¯ç¬¬äºŒåæ¨¡å‹ï¼ˆå¾—åˆ†0.2968ï¼‰çš„ä¸¤å€å¤šï¼Œåˆ›é€ äº†æ–°çš„æŠ€æœ¯è®°å½•ã€‚Dolphin v1.0ä¹Ÿè¡¨ç°å‡ºç«äº‰åŠ›ï¼ŒéªŒè¯äº†ç»Ÿä¸€æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚å¯¹æ¯”æ˜¾ç¤ºï¼Œå¢å¼ºæ¨ç†è®­ç»ƒæ˜¾è‘—æé«˜äº†è¯Šæ–­çš„å‡†ç¡®æ€§ã€ä¸€è‡´æ€§å’Œè§£é‡Šæ€§ï¼Œçªæ˜¾å…¶åœ¨é«˜é£é™©åŒ»ç–—äººå·¥æ™ºèƒ½ä¸­çš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25748v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>è¶…å£°åœ¨ç°ä»£åŒ»å­¦ä¸­è‡³å…³é‡è¦ï¼Œä½†é¢ä¸´æ“ä½œä¾èµ–ã€å›¾åƒå™ªå£°å’Œå®æ—¶æ‰«æç­‰æŒ‘æˆ˜ï¼Œé˜»ç¢äººå·¥æ™ºèƒ½çš„é›†æˆã€‚å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨å…¶ä»–åŒ»å­¦å½±åƒé¢†åŸŸè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨åº”å¯¹è¶…å£°å¤æ‚æ€§æ–¹é¢å´è¡¨ç°ä¸ä½³ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Dolphin v1.0åŠå…¶å¢å¼ºæ¨ç†ç‰ˆDolphin R1ã€‚å®ƒä»¬æ˜¯é¦–ä¸ªå¤§è§„æ¨¡è¶…å£°å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ï¼Œåœ¨ä¸€ä¸ªç»Ÿä¸€çš„è§†è§‰è¯­è¨€æ¡†æ¶å†…èåˆäº†å¤šæ ·çš„ä¸´åºŠä»»åŠ¡ã€‚é€šè¿‡æ•´åˆåŒ…å«æ•™ç§‘ä¹¦çŸ¥è¯†ã€å…¬å¼€æ•°æ®ã€åˆæˆæ ·æœ¬å’Œä¸€èˆ¬è¯­æ–™åº“çš„200ä¸‡è§„æ¨¡å¤šæ¨¡æ€æ•°æ®é›†ï¼Œè§£å†³äº†è¶…å£°å˜æ€§å’Œå™ªå£°é—®é¢˜ï¼Œç¡®ä¿äº†ç¨³å¥çš„æ„ŸçŸ¥ã€æ¨å¹¿å’Œä¸´åºŠé€‚åº”æ€§ã€‚Dolphinç³»åˆ—é‡‡ç”¨ä¸‰é˜¶æ®µåŸ¹è®­ç­–ç•¥ï¼šé¢†åŸŸä¸“ä¸šåŒ–é¢„è®­ç»ƒã€æŒ‡ä»¤é©±åŠ¨å¯¹é½å’Œå¼ºåŒ–å­¦ä¹ ç²¾ç‚¼ã€‚Dolphin v1.0åœ¨åˆ†ç±»ã€æ£€æµ‹ã€å›å½’å’ŒæŠ¥å‘Šç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºå¯é æ€§èƒ½ã€‚Dolphin R1é€šè¿‡å¼ºåŒ–å­¦ä¹ ä½¿ç”¨è¶…å£°ç‰¹å®šå¥–åŠ±æé«˜è¯Šæ–­æ¨ç†çš„é€æ˜åº¦ã€å¯è§£é‡Šæ€§å’Œå‡†ç¡®æ€§ã€‚åœ¨U2-Benchä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒDolphin R1çš„U2åˆ†æ•°ä¸º0.5835ï¼Œæ˜¯ç¬¬äºŒåæ¨¡å‹ï¼ˆ0.2968ï¼‰çš„ä¸¤å€å¤šï¼Œåˆ›ä¸‹äº†æ–°çš„ä¸–ç•Œçºªå½•ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è¶…å£°åœ¨ç°ä»£åŒ»å­¦ä¸­æ‰®æ¼”é‡è¦è§’è‰²ï¼Œä½†é¢ä¸´æ“ä½œä¾èµ–ã€å›¾åƒå™ªå£°å’Œå®æ—¶æ‰«æç­‰æŒ‘æˆ˜ã€‚</li>
<li>å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨è¶…å£°é¢†åŸŸå­˜åœ¨å¤æ‚æ€§æŒ‘æˆ˜ï¼Œéœ€è¦ç‰¹å®šçš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>Dolphin v1.0åŠå…¶å¢å¼ºç‰ˆDolphin R1æ˜¯é¦–ä¸ªç»Ÿä¸€ä¸´åºŠä»»åŠ¡çš„å¤§è§„æ¨¡è¶…å£°å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ã€‚</li>
<li>é€šè¿‡æ•´åˆå¤šæ¨¡æ€æ•°æ®é›†è§£å†³äº†è¶…å£°å˜æ€§å’Œå™ªå£°é—®é¢˜ã€‚</li>
<li>Dolphinç³»åˆ—é‡‡ç”¨ä¸‰é˜¶æ®µåŸ¹è®­ç­–ç•¥ï¼Œç¡®ä¿æ¨¡å‹çš„ç¨³å¥æ€§å’Œæ€§èƒ½ã€‚</li>
<li>Dolphin R1é€šè¿‡å¼ºåŒ–å­¦ä¹ æé«˜äº†è¯Šæ–­æ¨ç†çš„é€æ˜åº¦ã€å¯è§£é‡Šæ€§å’Œå‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25748">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-a94c9283a136479c50d366fc5740e11d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760103117&auth_key=1760103117-0-0-3437480ea0bde6443045d3a803fe2d7b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ef6a53f31cec9c70dde32f26ae029686~resize:0:q75.jpg?source=1f5c5e47&expiration=1760103124&auth_key=1760103124-0-0-db16e75832cd761c58854a7a00dbc9b1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-be0e272c9617e5163bfd0b215a0fa2ca~resize:0:q75.jpg?source=1f5c5e47&expiration=1760103132&auth_key=1760103132-0-0-a89ba2cf5f11b944fb6a179048fa789a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-777fe971059d982b2460c8677716db7f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760103139&auth_key=1760103139-0-0-2afd4c0bfb0ef441836c83624d60eb57&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="DeepSearch-Overcome-the-Bottleneck-of-Reinforcement-Learning-with-Verifiable-Rewards-via-Monte-Carlo-Tree-Search"><a href="#DeepSearch-Overcome-the-Bottleneck-of-Reinforcement-Learning-with-Verifiable-Rewards-via-Monte-Carlo-Tree-Search" class="headerlink" title="DeepSearch: Overcome the Bottleneck of Reinforcement Learning with   Verifiable Rewards via Monte Carlo Tree Search"></a>DeepSearch: Overcome the Bottleneck of Reinforcement Learning with   Verifiable Rewards via Monte Carlo Tree Search</h2><p><strong>Authors:Fang Wu, Weihao Xuan, Heli Qi, Ximing Lu, Aaron Tu, Li Erran Li, Yejin Choi</strong></p>
<p>Although RLVR has become an essential component for developing advanced reasoning skills in LLMs, contemporary studies have documented training plateaus that emerge following thousands of optimization steps, demonstrating notable decreases in performance gains despite increased computational investment. This limitation stems from the sparse exploration patterns inherent in current RLVR practices, where models rely on limited rollouts that often miss critical reasoning paths and fail to provide systematic coverage of the solution space. We present DeepSearch, a framework that integrates Monte Carlo Tree Search directly into RLVR training. In contrast to existing methods that rely on tree search only at inference, DeepSearch embeds structured search into the training loop, enabling systematic exploration and fine-grained credit assignment across reasoning steps. Through training-time exploration, DeepSearch addresses the fundamental bottleneck of insufficient exploration, which leads to diminishing performance improvements over prolonged training steps. Our contributions include: (1) a global frontier selection strategy that prioritizes promising nodes across the search tree, (2) selection with entropy-based guidance that identifies confident paths for supervision, and (3) adaptive replay buffer training with solution caching for efficiency. Experiments on mathematical reasoning benchmarks show that DeepSearch achieves 62.95% average accuracy and establishes a new state-of-the-art for 1.5B reasoning models - using 5.7x fewer GPU hours than extended training approaches. These results highlight the importance of strategic exploration over brute-force scaling and demonstrate the promise of algorithmic innovation for advancing RLVR methodologies. DeepSearch establishes a new direction for scaling reasoning capabilities through systematic search rather than prolonged computation. </p>
<blockquote>
<p>å°½ç®¡å¼ºåŒ–å­¦ä¹ éªŒè¯å™¨ï¼ˆRLVRï¼‰å·²æˆä¸ºåœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­å¼€å‘é«˜çº§æ¨ç†èƒ½åŠ›çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œä½†å½“ä»£ç ”ç©¶å·²ç»è®°å½•äº†éšç€ä¼˜åŒ–æ­¥éª¤çš„å¢åŠ è€Œå‡ºç°çš„è®­ç»ƒé«˜åŸç°è±¡ï¼Œè¿™æ˜¾ç¤ºå‡ºå°½ç®¡è®¡ç®—æŠ•å…¥å¢åŠ ï¼Œä½†æ€§èƒ½æå‡å´æ˜¾è‘—ä¸‹é™ã€‚è¿™ä¸€å±€é™æ€§æºäºå½“å‰RLVRå®è·µä¸­å›ºæœ‰çš„ç¨€ç–æ¢ç´¢æ¨¡å¼ï¼Œæ¨¡å‹ä¾èµ–äºæœ‰é™çš„å±•å¼€ï¼Œå¸¸å¸¸é”™è¿‡å…³é”®çš„æ¨ç†è·¯å¾„ï¼Œå¹¶ä¸”æœªèƒ½ç³»ç»Ÿåœ°è¦†ç›–è§£å†³æ–¹æ¡ˆç©ºé—´ã€‚æˆ‘ä»¬æå‡ºäº†DeepSearchæ¡†æ¶ï¼Œå®ƒå°†è’™ç‰¹å¡æ´›æ ‘æœç´¢ç›´æ¥é›†æˆåˆ°RLVRè®­ç»ƒä¸­ã€‚ä¸ç°æœ‰ä»…åœ¨æ¨ç†æ—¶ä¾èµ–æ ‘æœç´¢çš„æ–¹æ³•ä¸åŒï¼ŒDeepSearchå°†ç»“æ„åŒ–æœç´¢åµŒå…¥åˆ°è®­ç»ƒå¾ªç¯ä¸­ï¼Œä»è€Œèƒ½å¤Ÿåœ¨æ¨ç†æ­¥éª¤ä¹‹é—´è¿›è¡Œç³»ç»Ÿçš„æ¢ç´¢å’Œç²¾ç»†çš„ä¿¡ç”¨åˆ†é…ã€‚é€šè¿‡è®­ç»ƒæ—¶çš„æ¢ç´¢ï¼ŒDeepSearchè§£å†³äº†æ ¹æœ¬æ€§çš„æ¢ç´¢ä¸è¶³é—®é¢˜ï¼Œä»è€Œå…‹æœäº†é•¿æ—¶é—´çš„è®­ç»ƒæ­¥éª¤å¯¼è‡´çš„æ€§èƒ½æ”¹è¿›å‡å°‘ã€‚æˆ‘ä»¬çš„è´¡çŒ®åŒ…æ‹¬ï¼šï¼ˆ1ï¼‰å…¨å±€å‰æ²¿é€‰æ‹©ç­–ç•¥ï¼Œä¼˜å…ˆæœç´¢æ ‘ä¸­æœ‰æœ›çš„èŠ‚ç‚¹ï¼›ï¼ˆ2ï¼‰åŸºäºç†µçš„å¼•å¯¼è¿›è¡Œé€‰æ‹©ï¼Œä»¥ç¡®å®šç›‘ç£çš„ä¿¡å¿ƒè·¯å¾„ï¼›ï¼ˆ3ï¼‰ä½¿ç”¨è§£å†³æ–¹æ¡ˆç¼“å­˜æ¥æé«˜æ•ˆç‡çš„è‡ªé€‚åº”å›æ”¾ç¼“å†²åŒºè®­ç»ƒã€‚åœ¨æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDeepSearchè¾¾åˆ°äº†62.95ï¼…çš„å¹³å‡å‡†ç¡®ç‡ï¼Œä¸º1.5Bæ¨ç†æ¨¡å‹å»ºç«‹äº†æ–°çš„æŠ€æœ¯é¢†å…ˆåœ°ä½ï¼Œä¸æ‰©å±•è®­ç»ƒæ–¹æ³•ç›¸æ¯”ï¼ŒGPUå°æ—¶æ•°å‡å°‘äº†5.7å€ã€‚è¿™äº›ç»“æœå¼ºè°ƒäº†æˆ˜ç•¥æ¢ç´¢æ¯”ç²—æš´æ‰©å±•æ›´é‡è¦ï¼Œå¹¶å±•ç¤ºäº†ç®—æ³•åˆ›æ–°åœ¨æ¨è¿›RLVRæ–¹æ³•è®ºæ–¹é¢çš„å‰æ™¯ã€‚DeepSearché€šè¿‡ç³»ç»Ÿæœç´¢è€Œä¸æ˜¯é•¿æœŸè®¡ç®—æ¥å»ºç«‹æ‰©å±•æ¨ç†èƒ½åŠ›çš„æ–°æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25454v2">PDF</a> </p>
<p><strong>Summary</strong><br>åœ¨LLMé¢†åŸŸï¼Œå°½ç®¡RLVRå·²æˆä¸ºåŸ¹å…»é«˜çº§æ¨ç†èƒ½åŠ›çš„å…³é”®ç»„ä»¶ï¼Œä½†å½“ä»£ç ”ç©¶è®°å½•æ˜¾ç¤ºï¼Œåœ¨æ•°åƒæ­¥ä¼˜åŒ–åä¼šå‡ºç°æ€§èƒ½æå‡åœæ»çš„æƒ…å†µã€‚è¿™ä¸»è¦æ˜¯å› ä¸ºå½“å‰RLVRå®è·µä¸­çš„æ¢ç´¢æ¨¡å¼ç¨€ç–ï¼Œæ¨¡å‹ä¾èµ–æœ‰é™çš„è¯•è¿è¡Œï¼Œå¸¸å¸¸é”™è¿‡å…³é”®çš„æ¨ç†è·¯å¾„ä¸”æœªèƒ½ç³»ç»Ÿåœ°è¦†ç›–è§£ç©ºé—´ã€‚æœ¬ç ”ç©¶æå‡ºDeepSearchæ¡†æ¶ï¼Œå®ƒå°†è’™ç‰¹å¡æ´›æ ‘æœç´¢ç›´æ¥é›†æˆåˆ°RLVRè®­ç»ƒä¸­ã€‚ä¸åŒäºä»…åœ¨æ¨ç†é˜¶æ®µä¾èµ–æ ‘æœç´¢çš„ç°æœ‰æ–¹æ³•ï¼ŒDeepSearchå°†ç»“æ„åŒ–æœç´¢åµŒå…¥åˆ°è®­ç»ƒå¾ªç¯ä¸­ï¼Œå®ç°äº†æ¨ç†æ­¥éª¤çš„ç³»ç»Ÿæ€§æ¢ç´¢å’Œç²¾ç»†åŒ–çš„ä¿¡ç”¨åˆ†é…ã€‚é€šè¿‡è®­ç»ƒæ—¶çš„æ¢ç´¢ï¼ŒDeepSearchè§£å†³äº†æ ¹æœ¬æ€§çš„æ¢ç´¢ä¸è¶³é—®é¢˜ï¼Œä»è€Œè§£å†³äº†é•¿æ—¶é—´è®­ç»ƒæ€§èƒ½æå‡ä¸æ˜æ˜¾çš„é—®é¢˜ã€‚æˆ‘ä»¬çš„è´¡çŒ®åŒ…æ‹¬ï¼šï¼ˆ1ï¼‰å…¨å±€å‰æ²¿é€‰æ‹©ç­–ç•¥ï¼Œä¼˜å…ˆæœç´¢æ ‘ä¸­æœ‰å‰é€”çš„èŠ‚ç‚¹ï¼›ï¼ˆ2ï¼‰åŸºäºç†µçš„å¼•å¯¼ç­–ç•¥ï¼Œç¡®å®šæœ‰ä¿¡å¿ƒçš„è·¯å¾„è¿›è¡Œç›‘ç£ï¼›ï¼ˆ3ï¼‰å…·æœ‰è§£å†³æ–¹æ¡ˆç¼“å­˜åŠŸèƒ½çš„è‡ªé€‚åº”å›æ”¾ç¼“å†²åŒºè®­ç»ƒä»¥æé«˜æ•ˆç‡ã€‚åœ¨æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šï¼ŒDeepSearchè¾¾åˆ°äº†å¹³å‡å‡†ç¡®ç‡62.95%ï¼Œå¹¶ä¸ºè§„æ¨¡ä¸º1.5Bçš„æ¨ç†æ¨¡å‹åˆ›é€ äº†æ–°çš„é‡Œç¨‹ç¢‘ï¼Œå…¶ä½¿ç”¨çš„GPUå°æ—¶æ•°æ¯”å»¶é•¿è®­ç»ƒçš„æ–¹æ³•å‡å°‘äº†5.7å€ã€‚è¿™äº›ç»“æœå¼ºè°ƒäº†ç­–ç•¥æ€§æ¢ç´¢ä¼˜äºç®€å•è§„æ¨¡æ‰©å¼ çš„é‡è¦æ€§ï¼Œå¹¶è¯æ˜äº†ç®—æ³•åˆ›æ–°åœ¨æ¨è¿›RLVRæ–¹æ³•ä¸Šçš„æ½œåŠ›ã€‚DeepSearchä¸ºé€šè¿‡ç³»ç»ŸåŒ–æœç´¢æé«˜æ¨ç†èƒ½åŠ›å¼€è¾Ÿäº†æ–°çš„æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰RLVRå®è·µé¢ä¸´æ€§èƒ½æå‡åœæ»çš„é—®é¢˜ï¼Œä¸»è¦ç”±äºæ¢ç´¢æ¨¡å¼ç¨€ç–ï¼Œå¯¼è‡´é”™è¿‡å…³é”®æ¨ç†è·¯å¾„å’Œæœ‰é™çš„è§£ç©ºé—´è¦†ç›–ã€‚</li>
<li>DeepSearchæ¡†æ¶é›†æˆäº†è’™ç‰¹å¡æ´›æ ‘æœç´¢åˆ°RLVRè®­ç»ƒä¸­ï¼Œå®ç°ç³»ç»Ÿæ€§æ¢ç´¢å’Œæ¨ç†æ­¥éª¤çš„ç²¾ç»†åŒ–ä¿¡ç”¨åˆ†é…ã€‚</li>
<li>DeepSearché€šè¿‡è®­ç»ƒæ—¶çš„æ¢ç´¢è§£å†³æ ¹æœ¬æ€§çš„æ¢ç´¢ä¸è¶³é—®é¢˜ï¼Œæé«˜é•¿æ—¶é—´è®­ç»ƒçš„æ€§èƒ½ã€‚</li>
<li>å…¨å±€å‰æ²¿é€‰æ‹©ç­–ç•¥ã€åŸºäºç†µçš„å¼•å¯¼ç­–ç•¥å’Œè‡ªé€‚åº”å›æ”¾ç¼“å†²åŒºè®­ç»ƒæ˜¯DeepSearchçš„ä¸»è¦è´¡çŒ®ã€‚</li>
<li>åœ¨æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šï¼ŒDeepSearchè¡¨ç°å‡ºè‰²ï¼Œåˆ›é€ äº†æ–°çš„å‡†ç¡®ç‡è®°å½•ï¼Œå¹¶æé«˜äº†æ•ˆç‡ã€‚</li>
<li>å®éªŒç»“æœå¼ºè°ƒç­–ç•¥æ€§æ¢ç´¢ä¼˜äºç®€å•è§„æ¨¡æ‰©å¼ ï¼Œçªå‡ºäº†ç®—æ³•åˆ›æ–°åœ¨RLVRæ–¹æ³•ä¸­çš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25454">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-61caea6156d3b0d138824aabab902498~resize:0:q75.jpg?source=1f5c5e47&expiration=1760103146&auth_key=1760103146-0-0-97dab6d0af9670d42f9cb05b86b1b353&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9fd10db700603679eb9b5ddfdb7b9849~resize:0:q75.jpg?source=1f5c5e47&expiration=1760103154&auth_key=1760103154-0-0-587c7cb32d44a048eb6c74ef7de49cc4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="RADAR-Reasoning-Ability-and-Difficulty-Aware-Routing-for-Reasoning-LLMs"><a href="#RADAR-Reasoning-Ability-and-Difficulty-Aware-Routing-for-Reasoning-LLMs" class="headerlink" title="RADAR: Reasoning-Ability and Difficulty-Aware Routing for Reasoning LLMs"></a>RADAR: Reasoning-Ability and Difficulty-Aware Routing for Reasoning LLMs</h2><p><strong>Authors:Nigel Fernandez, Branislav Kveton, Ryan A. Rossi, Andrew S. Lan, Zichao Wang</strong></p>
<p>Reasoning language models have demonstrated remarkable performance on many challenging tasks in math, science, and coding. Choosing the right reasoning model for practical deployment involves a performance and cost tradeoff at two key levels: model size and reasoning budget, where larger models and higher reasoning budget lead to better performance but with increased cost and latency. In this work, we tackle this tradeoff from the angle of model configuration routing for different queries, and present RADAR (Reasoning-Ability and Difficulty-Aware Routing), a lightweight, interpretable, and scalable routing framework. Inspired by psychometrics, RADAR learns an item response model from model responses with different budgets to different queries, with interpretable parameters including query difficulties and model-budget abilities. RADAR then routes queries with higher difficulty to model-budget pairs with higher ability, and vice versa. We conduct extensive experiments on 8 widely used challenging reasoning benchmarks, demonstrating the superior performance of RADAR compared to state-of-the-art model routing methods. RADAR also exhibits query generalization capabilities, showing strong performance on out-of-distribution queries in all benchmarks. RADAR is also scalable and can efficiently integrate additional models by dynamically selecting a small set of evaluation queries to estimate their abilities. </p>
<blockquote>
<p>æ¨ç†è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦ã€ç§‘å­¦å’Œç¼–ç¨‹ç­‰é¢†åŸŸçš„è®¸å¤šå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚ä¸ºå®é™…éƒ¨ç½²é€‰æ‹©åˆé€‚çš„æ¨ç†æ¨¡å‹éœ€è¦åœ¨ä¸¤ä¸ªå…³é”®å±‚é¢ä¸Šè¿›è¡Œæ€§èƒ½å’Œæˆæœ¬çš„æƒè¡¡ï¼šæ¨¡å‹å¤§å°å’Œæ¨ç†é¢„ç®—ã€‚è¾ƒå¤§çš„æ¨¡å‹å’Œæ›´é«˜çš„æ¨ç†é¢„ç®—ä¼šå¸¦æ¥æ›´å¥½çš„æ€§èƒ½ï¼Œä½†ä¹Ÿä¼šå¢åŠ æˆæœ¬å’Œå»¶è¿Ÿã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»ä¸åŒæŸ¥è¯¢çš„æ¨¡å‹é…ç½®è·¯ç”±çš„è§’åº¦æ¥è§£å†³è¿™ä¸€æƒè¡¡é—®é¢˜ï¼Œå¹¶æå‡ºäº†RADARï¼ˆåŸºäºæ¨ç†èƒ½åŠ›å’Œéš¾åº¦æ„ŸçŸ¥çš„è·¯ç”±ï¼‰ã€‚å®ƒæ˜¯ä¸€ä¸ªè½»ä¾¿ã€å¯è§£é‡Šå’Œå¯æ‰©å±•çš„è·¯ç”±æ¡†æ¶ã€‚RADARå—åˆ°å¿ƒç†æµ‹é‡çš„å¯å‘ï¼Œä»å…·æœ‰ä¸åŒé¢„ç®—çš„æ¨¡å‹å¯¹ä¸åŒæŸ¥è¯¢çš„å“åº”ä¸­å­¦ä¹ é¡¹ç›®å“åº”æ¨¡å‹ï¼Œå…¶ä¸­å¯è§£é‡Šçš„å‚æ•°åŒ…æ‹¬æŸ¥è¯¢éš¾åº¦å’Œæ¨¡å‹é¢„ç®—èƒ½åŠ›ã€‚ç„¶åï¼ŒRADARå°†éš¾åº¦è¾ƒé«˜çš„æŸ¥è¯¢è·¯ç”±åˆ°å…·æœ‰è¾ƒé«˜èƒ½åŠ›çš„æ¨¡å‹é¢„ç®—å¯¹ï¼Œåä¹‹äº¦ç„¶ã€‚æˆ‘ä»¬åœ¨8ä¸ªå¹¿æ³›ä½¿ç”¨çš„æŒ‘æˆ˜æ€§æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼Œè¯æ˜äº†RADARç›¸æ¯”äºæœ€æ–°å‰æ²¿çš„æ¨¡å‹è·¯ç”±æ–¹æ³•å…·æœ‰å“è¶Šçš„æ€§èƒ½ã€‚RADARè¿˜å…·æœ‰æŸ¥è¯¢æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨æ‰€æœ‰åŸºå‡†æµ‹è¯•ä¸­å¯¹äºç¦»ç¾¤æŸ¥è¯¢ä¹Ÿè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒRADARå…·æœ‰å¯æ‰©å±•æ€§ï¼Œå¯ä»¥é€šè¿‡åŠ¨æ€é€‰æ‹©ä¸€å°éƒ¨åˆ†è¯„ä¼°æŸ¥è¯¢æ¥ä¼°è®¡å…¶èƒ½åŠ›ï¼Œä»è€Œæœ‰æ•ˆåœ°é›†æˆé¢å¤–çš„æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25426v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†æ¨ç†è¯­è¨€æ¨¡å‹åœ¨å®é™…éƒ¨ç½²æ—¶é¢ä¸´æ€§èƒ½ä¸æˆæœ¬çš„æƒè¡¡é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¨¡å‹å¤§å°å’Œæ¨ç†é¢„ç®—æ–¹é¢ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§åŸºäºæ¨¡å‹é…ç½®è·¯ç”±çš„è§£å†³ç­–ç•¥ï¼Œå³RADARï¼ˆå«æ¨ç†èƒ½åŠ›ä¸éš¾åº¦æ„ŸçŸ¥çš„è·¯ç”±ï¼‰ã€‚RADARå€Ÿé‰´å¿ƒç†æµ‹é‡å­¦ï¼Œä»æ¨¡å‹å¯¹ä¸åŒæŸ¥è¯¢çš„å“åº”ä¸­å­¦ä¹ é¡¹ç›®ååº”æ¨¡å‹ï¼Œæ¶‰åŠå¯è§£é‡Šçš„æŸ¥è¯¢éš¾åº¦å’Œæ¨¡å‹é¢„ç®—èƒ½åŠ›å‚æ•°ã€‚é€šè¿‡å¹¿æ³›å®éªŒï¼ŒRADARåœ¨8ä¸ªæŒ‘æˆ˜æ€§çš„æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œç›¸è¾ƒäºæœ€æ–°æ¨¡å‹è·¯ç”±æ–¹æ³•å…·æœ‰ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼ŒRADARè¿˜å…·æœ‰æŸ¥è¯¢æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨æ‰€æœ‰åŸºå‡†æµ‹è¯•ä¸­å¯¹ç¦»ç¾¤æŸ¥è¯¢è¡¨ç°å‡ºå¼ºåŠ²æ€§èƒ½ï¼Œå¹¶å¯åŠ¨æ€é€‰æ‹©å°‘é‡è¯„ä¼°æŸ¥è¯¢æ¥æ•´åˆé¢å¤–æ¨¡å‹ï¼Œå±•ç°å…¶å¯æ‰©å±•æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¨ç†è¯­è¨€æ¨¡å‹åœ¨å®é™…éƒ¨ç½²ä¸­é¢ä¸´æ€§èƒ½ä¸æˆæœ¬çš„æƒè¡¡é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¨¡å‹å¤§å°å’Œæ¨ç†é¢„ç®—ä¸Šéœ€è¦é€‰æ‹©åˆç†çš„å¹³è¡¡ç‚¹ã€‚</li>
<li>æ–‡ç« æå‡ºäº†RADARæ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ¨¡å‹é…ç½®è·¯ç”±çš„è§£å†³ç­–ç•¥ï¼Œæ—¨åœ¨æé«˜æ¨ç†æ€§èƒ½å¹¶é™ä½éƒ¨ç½²æˆæœ¬ã€‚</li>
<li>RADARæ¡†æ¶å€Ÿé‰´å¿ƒç†æµ‹é‡å­¦åŸç†ï¼Œé€šè¿‡æ¨¡å‹å¯¹ä¸åŒæŸ¥è¯¢çš„å“åº”æ¥å­¦ä¹ é¡¹ç›®ååº”æ¨¡å‹ã€‚</li>
<li>RADARå…·æœ‰å¯è§£é‡Šæ€§ï¼ŒåŒ…æ‹¬æŸ¥è¯¢éš¾åº¦å’Œæ¨¡å‹é¢„ç®—èƒ½åŠ›ç­‰å‚æ•°ã€‚</li>
<li>åœ¨å¤šä¸ªæŒ‘æˆ˜æ€§çš„æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒRADARè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œç›¸è¾ƒäºå…¶ä»–æœ€æ–°æ¨¡å‹è·¯ç”±æ–¹æ³•å…·æœ‰ä¼˜åŠ¿ã€‚</li>
<li>RADARå…·æœ‰æŸ¥è¯¢æ³›åŒ–èƒ½åŠ›ï¼Œé€‚åº”äºå„ç§ç¯å¢ƒä¸‹çš„æŸ¥è¯¢éœ€æ±‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25426">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-81505236973cc42c29bbf8d6888985e3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760103162&auth_key=1760103162-0-0-c29033528a9274cd7ac502d5c32041d6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5f5620d59071ebfd58b835d990d28fe8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760103169&auth_key=1760103169-0-0-a9d841210fb0864ead918df090f8df30&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0f68b57a4cae40a171e89260cfafa3a4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760103176&auth_key=1760103176-0-0-c9467dab40fc814ced93139ca97edfa3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Mechanisms-of-Matter-Language-Inferential-Benchmark-on-Physicochemical-Hypothesis-in-Materials-Synthesis"><a href="#Mechanisms-of-Matter-Language-Inferential-Benchmark-on-Physicochemical-Hypothesis-in-Materials-Synthesis" class="headerlink" title="Mechanisms of Matter: Language Inferential Benchmark on Physicochemical   Hypothesis in Materials Synthesis"></a>Mechanisms of Matter: Language Inferential Benchmark on Physicochemical   Hypothesis in Materials Synthesis</h2><p><strong>Authors:Yingming Pu, Tao Lin, Hongyu Chen</strong></p>
<p>The capacity of Large Language Models (LLMs) to generate valid scientific hypotheses for materials synthesis remains largely unquantified, hindered by the absence of benchmarks probing physicochemical logics reasoning. To address this, we introduce MatterMech, a benchmark for evaluating LLM-generated hypotheses across eight nanomaterial synthesis domains. Our analysis reveals a critical disconnect: LLMs are proficient in abstract logic yet fail to ground their reasoning in fundamental physicochemical principles. We demonstrate that our proposed principle-aware prompting methodology substantially outperforms standard Chain-of-Thought, enhancing both hypothesis accuracy and computational efficiency. This work provides a methodological framework to advance LLMs toward reliable scientific hypothesis generation in materials science. The MatterMech benchmark and associated code is publicly available at \href{<a target="_blank" rel="noopener" href="https://github.com/amair-lab/MatterMech%7D%7BGitHub%7D">https://github.com/amair-lab/MatterMech}{GitHub}</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç”Ÿæˆæœ‰æ•ˆçš„ç§‘å­¦å‡è®¾ä»¥è¿›è¡Œææ–™åˆæˆæ–¹é¢çš„èƒ½åŠ›å°šæœªå¾—åˆ°é‡åŒ–ï¼Œç”±äºç¼ºä¹æ¢ç©¶ç‰©ç†åŒ–å­¦é€»è¾‘æ¨ç†çš„åŸºå‡†æµ‹è¯•æ˜¯é˜»ç¢å…¶å‘å±•çš„åŸå› ä¹‹ä¸€ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†MatterMechï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°LLMåœ¨å…«ä¸ªçº³ç±³ææ–™åˆæˆé¢†åŸŸç”Ÿæˆå‡è®¾çš„åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬çš„åˆ†ææ­ç¤ºäº†ä¸€ä¸ªå…³é”®å·®è·ï¼šLLMè™½ç„¶æ“…é•¿æŠ½è±¡é€»è¾‘ï¼Œä½†æœªèƒ½å°†å…¶æ¨ç†å»ºç«‹åœ¨åŸºæœ¬çš„ç‰©ç†åŒ–å­¦åŸç†ä¹‹ä¸Šã€‚æˆ‘ä»¬è¯æ˜ï¼Œæ‰€æå‡ºçš„åŸºäºåŸç†çš„æç¤ºæ–¹æ³•æ˜¾è‘—ä¼˜äºæ ‡å‡†æ€ç»´é“¾ï¼Œæé«˜äº†å‡è®¾å‡†ç¡®æ€§å’Œè®¡ç®—æ•ˆç‡ã€‚è¿™é¡¹å·¥ä½œä¸ºæ¨è¿›LLMåœ¨ææ–™ç§‘å­¦ä¸­ç”Ÿæˆå¯é ç§‘å­¦å‡è®¾çš„æ–¹æ³•è®ºæ¡†æ¶æä¾›äº†åŸºç¡€ã€‚MatterMechåŸºå‡†æµ‹è¯•å’Œç›¸å…³ä»£ç å·²åœ¨GitHubä¸Šå…¬å¼€å‘å¸ƒï¼ˆ\href{<a target="_blank" rel="noopener" href="https://github.com/amair-lab/MatterMech%7D%7B%E9%93%BE%E6%8E%A5%7D%EF%BC%89%E3%80%82">https://github.com/amair-lab/MatterMech}{é“¾æ¥}ï¼‰ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25281v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç”Ÿæˆææ–™åˆæˆç§‘å­¦å‡è®¾æ–¹é¢çš„èƒ½åŠ›å°šæœªå¾—åˆ°å……åˆ†é‡åŒ–ï¼Œè¿™å—åˆ°ç¼ºä¹æ¢ç´¢ç‰©ç†åŒ–å­¦é€»è¾‘æ¨ç†åŸºå‡†çš„é™åˆ¶ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†MatterMechåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°LLMåœ¨å…«ä¸ªçº³ç±³ææ–™åˆæˆé¢†åŸŸçš„å‡è®¾ç”Ÿæˆèƒ½åŠ›ã€‚åˆ†æè¡¨æ˜ï¼ŒLLMè™½æ“…é•¿æŠ½è±¡é€»è¾‘ï¼Œä½†åœ¨å°†æ¨ç†ä¸åŸºæœ¬ç‰©ç†åŒ–å­¦åŸç†ç›¸ç»“åˆæ–¹é¢å­˜åœ¨æ˜æ˜¾ä¸è¶³ã€‚æˆ‘ä»¬è¯æ˜ï¼Œæå‡ºçš„åŸç†æ„ŸçŸ¥æç¤ºæ–¹æ³•æ˜¾è‘—ä¼˜äºæ ‡å‡†çš„æ€ç»´é“¾æ–¹æ³•ï¼Œæé«˜äº†å‡è®¾çš„å‡†ç¡®æ€§å’Œè®¡ç®—æ•ˆç‡ã€‚è¿™é¡¹å·¥ä½œä¸ºæ¨è¿›LLMåœ¨ææ–™ç§‘å­¦ä¸­ç”Ÿæˆå¯é ç§‘å­¦å‡è®¾çš„æ–¹æ³•è®ºæ¡†æ¶æä¾›äº†åŸºç¡€ã€‚MatterMechåŸºå‡†æµ‹è¯•å’Œç›¸å…³ä»£ç å·²å…¬å¼€ï¼Œå¯åœ¨GitHubä¸Šè·å–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç”Ÿæˆææ–™åˆæˆç§‘å­¦å‡è®¾æ–¹é¢çš„èƒ½åŠ›å°šæœªå……åˆ†é‡åŒ–ã€‚</li>
<li>ç¼ºä¹åŸºå‡†æµ‹è¯•æ¥æ¢ç´¢LLMsåœ¨ç‰©ç†åŒ–å­¦é€»è¾‘æ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>æ¨å‡ºäº†MatterMechåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°LLMåœ¨çº³ç±³ææ–™åˆæˆé¢†åŸŸçš„å‡è®¾ç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>LLMè™½ç„¶æ“…é•¿æŠ½è±¡é€»è¾‘ï¼Œä½†åœ¨å°†æ¨ç†ä¸åŸºæœ¬ç‰©ç†åŒ–å­¦åŸç†ç»“åˆæ–¹é¢å­˜åœ¨ä¸è¶³ã€‚</li>
<li>åŸç†æ„ŸçŸ¥æç¤ºæ–¹æ³•æ˜¾è‘—æé«˜äº†å‡è®¾ç”Ÿæˆçš„å‡†ç¡®æ€§å’Œè®¡ç®—æ•ˆç‡ã€‚</li>
<li>è¯¥å·¥ä½œä¸ºLLMåœ¨ææ–™ç§‘å­¦ä¸­ç”Ÿæˆå¯é ç§‘å­¦å‡è®¾æä¾›äº†æ–¹æ³•è®ºæ¡†æ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25281">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-9f0579503e0cfac090ea89352c860b86~resize:0:q75.jpg?source=1f5c5e47&expiration=1760103183&auth_key=1760103183-0-0-9b0aa97eec9da776830e496ac1cb0309&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-21c71b10dfbc563b37a6ce69ff31380b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760103191&auth_key=1760103191-0-0-04f864b596b314f7e7e63594e2b82a02&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-94945a8ad82fc541a255dab2e63b7b29~resize:0:q75.jpg?source=1f5c5e47&expiration=1760103198&auth_key=1760103198-0-0-5db3b73c7dfbe49dbec469df881bf5a9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-29969e8e65ca289645543d16eb2f81a9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760103205&auth_key=1760103205-0-0-e9f732adc61e770e92397f33d1ba3692&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0399a328a3f6837b7e75bc362e32df60~resize:0:q75.jpg?source=1f5c5e47&expiration=1760103212&auth_key=1760103212-0-0-31396fb6d724f26eef7e1b4ff6e69a43&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="MASLegalBench-Benchmarking-Multi-Agent-Systems-in-Deductive-Legal-Reasoning"><a href="#MASLegalBench-Benchmarking-Multi-Agent-Systems-in-Deductive-Legal-Reasoning" class="headerlink" title="MASLegalBench: Benchmarking Multi-Agent Systems in Deductive Legal   Reasoning"></a>MASLegalBench: Benchmarking Multi-Agent Systems in Deductive Legal   Reasoning</h2><p><strong>Authors:Huihao Jing, Wenbin Hu, Hongyu Luo, Jianhui Yang, Wei Fan, Haoran Li, Yangqiu Song</strong></p>
<p>Multi-agent systems (MAS), leveraging the remarkable capabilities of Large Language Models (LLMs), show great potential in addressing complex tasks. In this context, integrating MAS with legal tasks is a crucial step. While previous studies have developed legal benchmarks for LLM agents, none are specifically designed to consider the unique advantages of MAS, such as task decomposition, agent specialization, and flexible training. In fact, the lack of evaluation methods limits the potential of MAS in the legal domain. To address this gap, we propose MASLegalBench, a legal benchmark tailored for MAS and designed with a deductive reasoning approach. Our benchmark uses GDPR as the application scenario, encompassing extensive background knowledge and covering complex reasoning processes that effectively reflect the intricacies of real-world legal situations. Furthermore, we manually design various role-based MAS and conduct extensive experiments using different state-of-the-art LLMs. Our results highlight the strengths, limitations, and potential areas for improvement of existing models and MAS architectures. </p>
<blockquote>
<p>å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰å€ŸåŠ©å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å“è¶Šèƒ½åŠ›ï¼Œåœ¨åº”å¯¹å¤æ‚ä»»åŠ¡æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚åœ¨æ­¤èƒŒæ™¯ä¸‹ï¼Œå°†MASä¸æ³•å¾‹ä»»åŠ¡é›†æˆæ˜¯ä¸€ä¸ªå…³é”®æ­¥éª¤ã€‚è™½ç„¶ä¹‹å‰çš„ç ”ç©¶å·²ç»ä¸ºLLMä»£ç†å¼€å‘äº†æ³•å¾‹åŸºå‡†æµ‹è¯•ï¼Œä½†æ²¡æœ‰ä¸€ä¸ªæ˜¯ç‰¹åˆ«è€ƒè™‘MASçš„ç‹¬ç‰¹ä¼˜åŠ¿ï¼Œå¦‚ä»»åŠ¡åˆ†è§£ã€ä»£ç†ä¸“ä¸šåŒ–å’Œçµæ´»çš„è®­ç»ƒã€‚äº‹å®ä¸Šï¼Œç¼ºä¹è¯„ä¼°æ–¹æ³•é™åˆ¶äº†MASåœ¨æ³•å¾‹é¢†åŸŸçš„æ½œåŠ›ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†MASLegalBenchï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºMASå®šåˆ¶çš„æ³•å¾‹åŸºå‡†æµ‹è¯•ï¼Œå¹¶é‡‡ç”¨äº†æ¼”ç»æ¨ç†çš„æ–¹æ³•è¿›è¡Œè®¾è®¡ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•ä»¥GDPRä¸ºåº”ç”¨åœºæ™¯ï¼ŒåŒ…å«ä¸°å¯Œçš„èƒŒæ™¯çŸ¥è¯†ï¼Œæ¶µç›–æœ‰æ•ˆçš„æ¨ç†è¿‡ç¨‹ï¼Œå……åˆ†åæ˜ äº†ç°å®æ³•å¾‹æƒ…å¢ƒçš„å¤æ‚æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ‰‹åŠ¨è®¾è®¡äº†å„ç§åŸºäºè§’è‰²çš„MASï¼Œå¹¶ä½¿ç”¨ä¸åŒçš„æœ€æ–°LLMè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒã€‚æˆ‘ä»¬çš„ç»“æœçªæ˜¾äº†ç°æœ‰æ¨¡å‹å’ŒMASæ¶æ„çš„ä¼˜ç‚¹ã€å±€é™æ€§ä»¥åŠæ½œåœ¨çš„æ”¹è¿›æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24922v2">PDF</a> </p>
<p><strong>Summary</strong><br>å¤šæ™ºèƒ½ä½“ç³»ç»Ÿç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹åœ¨è§£å†³å¤æ‚ä»»åŠ¡ä¸Šå…·æœ‰å·¨å¤§æ½œåŠ›ã€‚é’ˆå¯¹å½“å‰æ³•å¾‹é¢†åŸŸä¸­ç¼ºä¹ä¸“é—¨é’ˆå¯¹å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„è¯„ä¼°æ–¹æ³•çš„é—®é¢˜ï¼Œæå‡ºäº†MASLegalBenchæ³•å¾‹åŸºå‡†æµ‹è¯•ï¼Œè¯¥æµ‹è¯•ä»¥GDPRä¸ºåº”ç”¨åœºæ™¯ï¼Œæ¶µç›–ä¸°å¯Œçš„èƒŒæ™¯çŸ¥è¯†å’Œå¤æ‚çš„æ¨ç†è¿‡ç¨‹ï¼Œæœ‰æ•ˆåæ˜ çœŸå®æ³•å¾‹æƒ…å¢ƒçš„å¤æ‚æ€§ã€‚å¹¶é€šè¿‡å®éªŒè¯„ä¼°äº†ä¸åŒå…ˆè¿›è¯­è¨€æ¨¡å‹åœ¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ™ºèƒ½ä½“ç³»ç»Ÿç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹åœ¨è§£å†³å¤æ‚ä»»åŠ¡ä¸Šçš„æ½œåŠ›å·¨å¤§ã€‚</li>
<li>å½“å‰æ³•å¾‹é¢†åŸŸç¼ºä¹ä¸“é—¨é’ˆå¯¹å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„è¯„ä¼°æ–¹æ³•ã€‚</li>
<li>MASLegalBenchæ˜¯ä¸€ä¸ªé’ˆå¯¹å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„æ³•å¾‹åŸºå‡†æµ‹è¯•ï¼Œä»¥GDPRä¸ºåº”ç”¨åœºæ™¯ã€‚</li>
<li>MASLegalBenchæµ‹è¯•æ¶µç›–ä¸°å¯Œçš„èƒŒæ™¯çŸ¥è¯†å’Œå¤æ‚çš„æ¨ç†è¿‡ç¨‹ï¼Œåæ˜ çœŸå®æ³•å¾‹æƒ…å¢ƒçš„å¤æ‚æ€§ã€‚</li>
<li>é€šè¿‡å®éªŒè¯„ä¼°äº†ä¸åŒå…ˆè¿›è¯­è¨€æ¨¡å‹åœ¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„è¡¨ç°ã€‚</li>
<li>ç»“æœæ­ç¤ºäº†ç°æœ‰æ¨¡å‹å’Œå¤šæ™ºèƒ½ä½“ç³»ç»Ÿæ¶æ„çš„ä¼˜åŠ¿ã€å±€é™æ€§å’Œæ½œåœ¨çš„æ”¹è¿›æ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24922">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-41324f4c802518bd72bce6d67d6b8271~resize:0:q75.jpg?source=1f5c5e47&expiration=1760103219&auth_key=1760103219-0-0-957883f849f7c192c170d3055bc30c94&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-11071a74e7cb408cd1b3b5c178f00d6e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760103227&auth_key=1760103227-0-0-4d0963407f88a60ba859c61d02769217&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6678d96a90341802ab220f32f5831761~resize:0:q75.jpg?source=1f5c5e47&expiration=1760103234&auth_key=1760103234-0-0-404cdd4ad48803145aa925bd28e37743&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-86c2c517cbc4d5d106c3ac2e8ac49fbe~resize:0:q75.jpg?source=1f5c5e47&expiration=1760103241&auth_key=1760103241-0-0-10d5e4446cb332f25ed2474349131468&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="From-Ambiguity-to-Verdict-A-Semiotic-Grounded-Multi-Perspective-Agent-for-LLM-Logical-Reasoning"><a href="#From-Ambiguity-to-Verdict-A-Semiotic-Grounded-Multi-Perspective-Agent-for-LLM-Logical-Reasoning" class="headerlink" title="From Ambiguity to Verdict: A Semiotic-Grounded Multi-Perspective Agent   for LLM Logical Reasoning"></a>From Ambiguity to Verdict: A Semiotic-Grounded Multi-Perspective Agent   for LLM Logical Reasoning</h2><p><strong>Authors:Yunyao Zhang, Xinglang Zhang, Junxi Sheng, Wenbing Li, Junqing Yu, Wei Yang, Zikai Song</strong></p>
<p>Logical reasoning is a fundamental capability of large language models (LLMs). However, existing studies largely overlook the interplay between logical complexity and semantic complexity, resulting in methods that struggle to address challenging scenarios involving abstract propositions, ambiguous contexts, and conflicting stances, which are central to human reasoning. For this gap, we propose LogicAgent, a semiotic-square-guided framework designed to jointly address logical complexity and semantic complexity. LogicAgent explicitly performs multi-perspective deduction in first-order logic (FOL), while mitigating vacuous reasoning through existential import checks that incorporate a three-valued decision scheme (True, False, Uncertain) to handle boundary cases more faithfully. Furthermore, to overcome the semantic simplicity and low logical complexity of existing datasets, we introduce RepublicQA, a benchmark that reaches college-level difficulty (FKGL &#x3D; 11.94) and exhibits substantially greater lexical and structural diversity than prior benchmarks. RepublicQA is grounded in philosophical concepts, featuring abstract propositions and systematically organized contrary and contradictory relations, making it the most semantically rich resource for evaluating logical reasoning. Experiments demonstrate that LogicAgent achieves state-of-the-art performance on RepublicQA, with a 6.25% average gain over strong baselines, and generalizes effectively to mainstream logical reasoning benchmarks including ProntoQA, ProofWriter, FOLIO, and ProverQA, achieving an additional 7.05% average gain. These results highlight the strong effectiveness of our semiotic-grounded multi-perspective reasoning in boosting LLMsâ€™ logical performance. </p>
<blockquote>
<p>é€»è¾‘æ¨ç†æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åŸºæœ¬èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰ç ”ç©¶å¤§å¤šå¿½è§†äº†é€»è¾‘å¤æ‚æ€§ä¸è¯­ä¹‰å¤æ‚æ€§ä¹‹é—´çš„ç›¸äº’ä½œç”¨ï¼Œå¯¼è‡´ç°æœ‰æ–¹æ³•éš¾ä»¥åº”å¯¹æ¶‰åŠæŠ½è±¡å‘½é¢˜ã€æ¨¡ç³Šä¸Šä¸‹æ–‡å’Œå†²çªç«‹åœºçš„æŒ‘æˆ˜åœºæ™¯ï¼Œè€Œè¿™äº›æ˜¯äººç±»æ¨ç†çš„æ ¸å¿ƒã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†LogicAgentï¼Œè¿™æ˜¯ä¸€ä¸ªä»¥ç¬¦å·å¹³æ–¹ä¸ºæŒ‡å¯¼çš„æ¡†æ¶ï¼Œæ—¨åœ¨å…±åŒè§£å†³é€»è¾‘å¤æ‚æ€§å’Œè¯­ä¹‰å¤æ‚æ€§ã€‚LogicAgentæ˜ç¡®åœ°è¿›è¡Œä¸€é˜¶é€»è¾‘ï¼ˆFOLï¼‰çš„å¤šè§†è§’æ¨ç†ï¼ŒåŒæ—¶é€šè¿‡å­˜åœ¨æ€§å¯¼å…¥æ£€æŸ¥æ¥ç¼“è§£ç©ºæ´æ¨ç†ï¼Œé‡‡ç”¨ä¸‰å€¼å†³ç­–æ–¹æ¡ˆï¼ˆçœŸã€å‡ã€ä¸ç¡®å®šï¼‰ä»¥æ›´çœŸå®åœ°å¤„ç†è¾¹ç•Œæƒ…å†µã€‚æ­¤å¤–ï¼Œä¸ºäº†å…‹æœç°æœ‰æ•°æ®é›†è¯­ä¹‰ç®€å•ã€é€»è¾‘å¤æ‚æ€§ä½çš„é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†RepublicQAï¼Œè¿™æ˜¯ä¸€ä¸ªéš¾åº¦è¾¾åˆ°å¤§å­¦æ°´å¹³ï¼ˆFKGL&#x3D;11.94ï¼‰çš„åŸºå‡†æµ‹è¯•ï¼Œä¸å…ˆå‰çš„åŸºå‡†æµ‹è¯•ç›¸æ¯”ï¼Œå®ƒåœ¨è¯æ±‡å’Œç»“æ„å¤šæ ·æ€§æ–¹é¢è¦å¤§å¾—å¤šã€‚RepublicQAä»¥å“²å­¦æ¦‚å¿µä¸ºåŸºç¡€ï¼ŒåŒ…å«æŠ½è±¡å‘½é¢˜ï¼Œä»¥åŠæœ‰ç»„ç»‡æ€§çš„ç›¸åå’ŒçŸ›ç›¾å…³ç³»ï¼Œæˆä¸ºè¯„ä¼°é€»è¾‘æ¨ç†æ–¹é¢æœ€è¯­ä¹‰ä¸°å¯Œçš„èµ„æºã€‚å®éªŒè¡¨æ˜ï¼ŒLogicAgentåœ¨RepublicQAä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ¯”å¼ºåŸºçº¿å¹³å‡æé«˜äº†6.25%ï¼Œå¹¶åœ¨ä¸»æµçš„é€»è¾‘æ¨ç†åŸºå‡†æµ‹è¯•ï¼ˆåŒ…æ‹¬ProntoQAã€ProofWriterã€FOLIOå’ŒProverQAï¼‰ä¸Šå®ç°äº†æœ‰æ•ˆçš„æ³›åŒ–ï¼Œå¹³å‡æé«˜äº†é¢å¤–çš„7.05%ã€‚è¿™äº›ç»“æœçªæ˜¾äº†æˆ‘ä»¬çš„ç¬¦å·åŒ–å¤šè§†è§’æ¨ç†åœ¨æå‡LLMçš„é€»è¾‘æ€§èƒ½æ–¹é¢çš„å¼ºå¤§æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24765v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é€»è¾‘æ¨ç†èƒ½åŠ›æ˜¯ä¸€é¡¹åŸºæœ¬åŠŸèƒ½ã€‚ç„¶è€Œï¼Œç°æœ‰ç ”ç©¶å¿½è§†äº†é€»è¾‘å¤æ‚æ€§å’Œè¯­ä¹‰å¤æ‚æ€§ä¹‹é—´çš„ç›¸äº’ä½œç”¨ï¼Œå¯¼è‡´å®ƒä»¬åœ¨å¤„ç†æ¶‰åŠæŠ½è±¡å‘½é¢˜ã€æ¨¡ç³Šä¸Šä¸‹æ–‡å’Œå†²çªç«‹åœºç­‰äººç±»æ¨ç†æ ¸å¿ƒçš„æŒ‘æˆ˜åœºæ™¯æ—¶è¡¨ç°ä¸ä½³ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†LogicAgentæ¡†æ¶ï¼Œå®ƒé‡‡ç”¨åŠç¬¦å·å¹³æ–¹å¼•å¯¼æ³•ï¼Œæ—¨åœ¨è”åˆè§£å†³é€»è¾‘å¤æ‚æ€§å’Œè¯­ä¹‰å¤æ‚æ€§ã€‚LogicAgentæ˜ç¡®æ‰§è¡Œä¸€é˜¶é€»è¾‘ï¼ˆFOLï¼‰çš„å¤šè§’åº¦æ¨ç†ï¼Œå¹¶é€šè¿‡å­˜åœ¨æ€§å¯¼å…¥æ£€æŸ¥æ¥ç¼“è§£ç©ºæ´æ¨ç†ï¼Œè¯¥æ£€æŸ¥é‡‡ç”¨ä¸‰å€¼å†³ç­–æ–¹æ¡ˆï¼ˆçœŸã€å‡ã€ä¸ç¡®å®šï¼‰ä»¥æ›´çœŸå®åœ°å¤„ç†è¾¹ç•Œæƒ…å†µã€‚æ­¤å¤–ï¼Œä¸ºäº†å…‹æœç°æœ‰æ•°æ®é›†è¯­ä¹‰ç®€å•ã€é€»è¾‘å¤æ‚åº¦ä½çš„é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†RepublicQAåŸºå‡†æµ‹è¯•ï¼Œå…¶éš¾åº¦è¾¾åˆ°å¤§å­¦æ°´å¹³ï¼ˆFKGL&#x3D;11.94ï¼‰ï¼Œå¹¶ä¸”åœ¨è¯æ±‡å’Œç»“æ„å¤šæ ·æ€§æ–¹é¢å¤§å¤§è¶…è¿‡äº†ä»¥å‰çš„åŸºå‡†æµ‹è¯•ã€‚RepublicQAä»¥å“²å­¦æ¦‚å¿µä¸ºåŸºç¡€ï¼ŒåŒ…å«æŠ½è±¡å‘½é¢˜å’Œç³»ç»Ÿç»„ç»‡çš„ç›¸åå’ŒçŸ›ç›¾å…³ç³»ï¼Œæˆä¸ºè¯„ä¼°é€»è¾‘æ¨ç†çš„æœ€è¯­ä¹‰ä¸°å¯Œçš„èµ„æºã€‚å®éªŒè¡¨æ˜ï¼ŒLogicAgentåœ¨RepublicQAä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹³å‡ä¼˜äºå¼ºåŸºçº¿6.25%ï¼Œå¹¶åœ¨ä¸»æµé€»è¾‘æ¨ç†åŸºå‡†æµ‹è¯•ï¼ˆåŒ…æ‹¬ProntoQAã€ProofWriterã€FOLIOå’ŒProverQAï¼‰ä¸Šå®ç°äº†é¢å¤–çš„7.05%çš„å¹³å‡å¢å¹…ã€‚è¿™äº›ç»“æœå¼ºçƒˆè¯æ˜äº†æˆ‘ä»¬çš„åŠç¬¦å·å¼•å¯¼çš„å¤šè§’åº¦æ¨ç†çš„å¼ºå¤§æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é€»è¾‘æ¨ç†èƒ½åŠ›é‡è¦ï¼Œä½†ç°æœ‰ç ”ç©¶å¿½è§†äº†é€»è¾‘å¤æ‚æ€§å’Œè¯­ä¹‰å¤æ‚æ€§çš„ç›¸äº’ä½œç”¨ã€‚</li>
<li>LogicAgentæ¡†æ¶é€šè¿‡åŠç¬¦å·å¹³æ–¹å¼•å¯¼æ³•è”åˆè§£å†³é€»è¾‘å¤æ‚æ€§å’Œè¯­ä¹‰å¤æ‚æ€§ã€‚</li>
<li>LogicAgentæ‰§è¡Œå¤šè§’åº¦çš„ä¸€é˜¶é€»è¾‘ï¼ˆFOLï¼‰æ¨ç†ï¼Œå¹¶é‡‡ç”¨ä¸‰å€¼å†³ç­–æ–¹æ¡ˆå¤„ç†è¾¹ç•Œæƒ…å†µã€‚</li>
<li>RepublicQAåŸºå‡†æµ‹è¯•éš¾åº¦é«˜ï¼ŒåŒ…å«æŠ½è±¡å‘½é¢˜å’ŒçŸ›ç›¾å…³ç³»ï¼Œæ˜¯è¯„ä¼°é€»è¾‘æ¨ç†çš„æœ€è¯­ä¹‰ä¸°å¯Œçš„èµ„æºã€‚</li>
<li>LogicAgentåœ¨RepublicQAä¸Šè¡¨ç°æœ€ä½³ï¼Œå¹³å‡ä¼˜äºå…¶ä»–æ¨¡å‹6.25%ã€‚</li>
<li>LogicAgentåœ¨å¤šä¸ªä¸»æµé€»è¾‘æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šéƒ½è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œå¹³å‡å¢ç›Š7.05%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24765">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-48daa09710a962d9a53f1f46a591abb3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760103248&auth_key=1760103248-0-0-2006c88fd1e12db7d4ae48d6372a1e3f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8ab7f9e8af8f97bb1023a44d300bc257~resize:0:q75.jpg?source=1f5c5e47&expiration=1760103256&auth_key=1760103256-0-0-69360a6704bbbc4556d46fe4df22f075&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b8bb14816865832d272a4cfbfc7e3f6d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760103263&auth_key=1760103263-0-0-7f07b9693d1195f9ffae6e022e3cdee5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9d755a8f7d0e101f5fcbd0ad98fecb26~resize:0:q75.jpg?source=1f5c5e47&expiration=1760103269&auth_key=1760103269-0-0-e2d9e86ff1bfec690da10974315e7600&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="LLaDA-MoE-A-Sparse-MoE-Diffusion-Language-Model"><a href="#LLaDA-MoE-A-Sparse-MoE-Diffusion-Language-Model" class="headerlink" title="LLaDA-MoE: A Sparse MoE Diffusion Language Model"></a>LLaDA-MoE: A Sparse MoE Diffusion Language Model</h2><p><strong>Authors:Fengqi Zhu, Zebin You, Yipeng Xing, Zenan Huang, Lin Liu, Yihong Zhuang, Guoshan Lu, Kangyu Wang, Xudong Wang, Lanning Wei, Hongrui Guo, Jiaqi Hu, Wentao Ye, Tieyuan Chen, Chenchen Li, Chengfu Tang, Haibo Feng, Jun Hu, Jun Zhou, Xiaolu Zhang, Zhenzhong Lan, Junbo Zhao, Da Zheng, Chongxuan Li, Jianguo Li, Ji-Rong Wen</strong></p>
<p>We introduce LLaDA-MoE, a large language diffusion model with the Mixture-of-Experts (MoE) architecture, trained from scratch on approximately 20T tokens. LLaDA-MoE achieves competitive performance with significantly reduced computational overhead by maintaining a 7B-parameter capacity while activating only 1.4B parameters during inference. Our empirical evaluation reveals that LLaDA-MoE achieves state-of-the-art performance among diffusion language models with larger parameters, surpassing previous diffusion language models LLaDA, LLaDA 1.5, and Dream across multiple benchmarks. The instruct-tuned model LLaDA-MoE-7B-A1B-Instruct demonstrates capabilities comparable to Qwen2.5-3B-Instruct in knowledge understanding, code generation, mathematical reasoning, agent and alignment tasks, despite using fewer active parameters. Our results show that integrating a sparse MoE architecture into the training objective of masked diffusion language models still brings out MoEâ€™s strengths under efficient inference with few active parameters, and opens ample room for further exploration of diffusion language models. LLaDA-MoE models are available at Huggingface. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†LLaDA-MoEï¼Œè¿™æ˜¯ä¸€ä¸ªé‡‡ç”¨ä¸“å®¶æ··åˆï¼ˆMoEï¼‰æ¶æ„çš„å¤§å‹è¯­è¨€æ‰©æ•£æ¨¡å‹ï¼Œåœ¨å¤§çº¦20Tä»¤ç‰Œä¸Šè¿›è¡Œä»å¤´è®­ç»ƒã€‚LLaDA-MoEä»¥7Bå‚æ•°çš„å®¹é‡ç»´æŒï¼ŒåŒæ—¶åœ¨æ¨ç†æ—¶ä»…æ¿€æ´»1.4Bå‚æ•°ï¼Œå®ç°äº†å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½å¹¶æ˜¾è‘—å‡å°‘äº†è®¡ç®—å¼€é”€ã€‚æˆ‘ä»¬çš„ç»éªŒè¯„ä¼°è¡¨æ˜ï¼ŒLLaDA-MoEåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ‰©æ•£è¯­è¨€æ¨¡å‹æ€§èƒ½ï¼Œè¶…è¶Šäº†ä¹‹å‰çš„æ‰©æ•£è¯­è¨€æ¨¡å‹LLaDAã€LLaDA 1.5å’ŒDreamã€‚æŒ‡ä»¤è°ƒæ•´æ¨¡å‹LLaDA-MoE-7B-A1B-Instructåœ¨çŸ¥è¯†ç†è§£ã€ä»£ç ç”Ÿæˆã€æ•°å­¦æ¨ç†ã€ä»£ç†å’Œå¯¹é½ä»»åŠ¡ä¸­å±•ç°å‡ºä¸Qwen2.5-3B-Instructç›¸å½“çš„èƒ½åŠ›ï¼Œå°½ç®¡å®ƒä½¿ç”¨çš„æ´»åŠ¨å‚æ•°æ›´å°‘ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå°†ç¨€ç–MoEæ¶æ„æ•´åˆåˆ°æ©è”½æ‰©æ•£è¯­è¨€æ¨¡å‹çš„è®­ç»ƒç›®æ ‡ä¸­ï¼Œä»èƒ½åœ¨æœ‰æ•ˆæ¨ç†å’Œè¾ƒå°‘æ´»åŠ¨å‚æ•°çš„æƒ…å†µä¸‹å‘æŒ¥å‡ºMoEçš„ä¼˜åŠ¿ï¼Œå¹¶ä¸ºè¿›ä¸€æ­¥æ¢ç´¢æ‰©æ•£è¯­è¨€æ¨¡å‹æä¾›äº†å¹¿é˜”çš„ç©ºé—´ã€‚LLaDA-MoEæ¨¡å‹å·²åœ¨Huggingfaceä¸Šæä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24389v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LLaDA-MoEæ˜¯ä¸€ä¸ªå¤§å‹è¯­è¨€æ‰©æ•£æ¨¡å‹ï¼Œé‡‡ç”¨æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¶æ„ï¼Œåœ¨çº¦20Tæ ‡è®°ä¸Šä»å¤´å¼€å§‹è®­ç»ƒã€‚å®ƒåœ¨ä¿æŒ7Bå‚æ•°å®¹é‡çš„åŒæ—¶ï¼Œæ¨ç†æ—¶ä»…æ¿€æ´»1.4Bå‚æ•°ï¼Œå®ç°äº†æ˜¾è‘—çš„è®¡ç®—èµ„æºå‡å°‘ã€‚å®è¯è¯„ä¼°è¡¨æ˜ï¼ŒLLaDA-MoEåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ä¹‹å‰çš„æ‰©æ•£è¯­è¨€æ¨¡å‹LLaDAã€LLaDA 1.5å’ŒDreamï¼Œå®ç°äº†æ‰©æ•£è¯­è¨€æ¨¡å‹ä¸­çš„æœ€æ–°æ€§èƒ½ã€‚LLaDA-MoEæ¨¡å‹åœ¨çŸ¥è¯†ç†è§£ã€ä»£ç ç”Ÿæˆã€æ•°å­¦æ¨ç†ã€ä»£ç†å’Œå¯¹é½ä»»åŠ¡ä¸­ï¼Œè¡¨ç°å‡ºä¸Qwen2.5-3B-Instructç›¸å½“çš„èƒ½åŠ›ã€‚æ•´åˆç¨€ç–MoEæ¶æ„åˆ°æ©ç æ‰©æ•£è¯­è¨€æ¨¡å‹çš„è®­ç»ƒç›®æ ‡ä¸­ï¼Œä»èƒ½åœ¨é«˜æ•ˆçš„æ¨ç†å’Œå°‘é‡æ¿€æ´»å‚æ•°ä¸‹å‘æŒ¥å‡ºMoEçš„ä¼˜åŠ¿ï¼Œå¹¶ä¸ºæ‰©æ•£è¯­è¨€æ¨¡å‹çš„è¿›ä¸€æ­¥æ¢ç´¢æä¾›äº†å¹¿é˜”çš„ç©ºé—´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLaDA-MoEæ˜¯ä¸€ä¸ªå¤§å‹è¯­è¨€æ‰©æ•£æ¨¡å‹ï¼Œç»“åˆäº†æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¶æ„ã€‚</li>
<li>LLaDA-MoEåœ¨çº¦20Tæ ‡è®°æ•°æ®ä¸Šè®­ç»ƒï¼Œå…·æœ‰7Bå‚æ•°å®¹é‡ï¼Œä½†æ¨ç†æ—¶ä»…æ¿€æ´»1.4Bå‚æ•°ï¼Œé™ä½äº†è®¡ç®—å¼€é”€ã€‚</li>
<li>å®è¯è¯„ä¼°æ˜¾ç¤ºLLaDA-MoEåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†å…¶ä»–æ‰©æ•£è¯­è¨€æ¨¡å‹ã€‚</li>
<li>LLaDA-MoEåœ¨çŸ¥è¯†ç†è§£ã€ä»£ç ç”Ÿæˆã€æ•°å­¦æ¨ç†ç­‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ã€‚</li>
<li>æ•´åˆMoEæ¶æ„åˆ°æ©ç æ‰©æ•£è¯­è¨€æ¨¡å‹çš„è®­ç»ƒç›®æ ‡ä¸­ï¼Œæœ‰åŠ©äºå®ç°é«˜æ•ˆæ¨ç†å’Œä¼˜ç§€æ€§èƒ½ã€‚</li>
<li>LLaDA-MoEæ¨¡å‹çš„æ€§èƒ½è¡¨ç°æ‰“å¼€äº†è¿›ä¸€æ­¥æ¢ç´¢æ‰©æ•£è¯­è¨€æ¨¡å‹çš„å¹¿é˜”ç©ºé—´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24389">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-2f8c5880a56f570f3afb3b3fd472f5ac~resize:0:q75.jpg?source=1f5c5e47&expiration=1760103277&auth_key=1760103277-0-0-5ade344a2e40840d9525cbe7ae1b403c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-247b5cc325583c75bd393f0380fa603a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760103284&auth_key=1760103284-0-0-132dc96107d655ce2e93c4df127fd123&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a28fd8ba09dd8d5a7bdb5da52dd65ed8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760103291&auth_key=1760103291-0-0-b9335458909f53a0f5023dc0dae23293&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a2c476a0af5ecdc3622be5c7af6623a3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760103299&auth_key=1760103299-0-0-d5871fb918a5b37b329d69774e54cec8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6f82f41cf51745ba3473d21fbe54a80a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760103305&auth_key=1760103305-0-0-cd5427c40638d8c0887cc6da6b251359&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Latent-Collective-Preference-Optimization-A-General-Framework-for-Robust-LLM-Alignment"><a href="#Latent-Collective-Preference-Optimization-A-General-Framework-for-Robust-LLM-Alignment" class="headerlink" title="Latent Collective Preference Optimization: A General Framework for   Robust LLM Alignment"></a>Latent Collective Preference Optimization: A General Framework for   Robust LLM Alignment</h2><p><strong>Authors:Xiaoyang Cao, Zelai Xu, Mo Guang, Kaiwen Long, Michiel A. Bakker, Yu Wang, Chao Yu</strong></p>
<p>Standard human preference-based alignment methods, such as Reinforcement Learning from Human Feedback (RLHF), are a cornerstone technology for aligning Large Language Models (LLMs) with human values. However, these methods are all underpinned by a critical, yet flawed assumption: human preferences are homogeneous (representing a single, unified preference) and the collected data is noiseless (free from error). In reality, neither is true since human preference is pluralistic and annotators can make mistakes. This creates a discrepancy between the recorded data and the ground-truth preferences, which can misguide the model and degrade its performance. To address this challenge, we introduce Latent Collective Preference Optimization (LCPO). LCPO leverages an Expectation-Maximization (EM) algorithm to learn the latent collective consensus from noisy data. It operates by inferring the correctness of each preference label and using this probability as an adaptive weight to re-calibrate each data pointâ€™s contribution to the training loss, thereby mitigating noise. We generalize this approach by establishing a theoretical link between arbitrary preference losses and their corresponding probabilistic models, elevating LCPO from a specific algorithm to a general framework for robust preference alignment. Theoretically, we prove that under the condition of a perfectly calibrated model, LCPO is guaranteed to converge to the true noise level of the dataset. Our experiments demonstrate LCPOâ€™s effectiveness as a general framework, consistently enhancing four state-of-the-art alignment algorithms (DPO, IPO, SimPO, and CPO). When applied to Mistral and Llama 3 models, the LCPO-enhanced methods achieve substantial win rate gains on AlpacaEval 2 and Arena-Hard, with improvements of up to 7.0% on both benchmarks. </p>
<blockquote>
<p>æ ‡å‡†çš„äººç±»åå¥½åŸºå‡†å¯¹é½æ–¹æ³•ï¼Œå¦‚åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ï¼Œæ˜¯ä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸äººç±»ä»·å€¼è§‚å¯¹é½çš„æ ¸å¿ƒæŠ€æœ¯ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•éƒ½æ˜¯åŸºäºä¸€ä¸ªè‡³å…³é‡è¦ä½†å­˜åœ¨ç¼ºé™·çš„å‡è®¾ï¼šäººç±»åå¥½æ˜¯å‡è´¨çš„ï¼ˆä»£è¡¨ä¸€ä¸ªç»Ÿä¸€ã€å•ä¸€çš„åå¥½ï¼‰ï¼Œæ”¶é›†çš„æ•°æ®æ˜¯æ— å™ªå£°çš„ï¼ˆæ²¡æœ‰é”™è¯¯ï¼‰ã€‚å®é™…ä¸Šï¼Œä¸¤è€…éƒ½ä¸çœŸå®ï¼Œå› ä¸ºäººç±»åå¥½æ˜¯å¤šå…ƒåŒ–çš„ï¼Œæ³¨é‡Šè€…ä¹Ÿä¼šçŠ¯é”™è¯¯ã€‚è¿™å°±é€ æˆäº†è®°å½•çš„æ•°æ®ä¸çœŸå®åå¥½ä¹‹é—´çš„ä¸ä¸€è‡´ï¼Œå¯èƒ½ä¼šè¯¯å¯¼æ¨¡å‹å¹¶é™ä½å…¶æ€§èƒ½ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ½œåœ¨é›†ä½“åå¥½ä¼˜åŒ–ï¼ˆLCPOï¼‰ã€‚LCPOåˆ©ç”¨æœŸæœ›æœ€å¤§åŒ–ï¼ˆEMï¼‰ç®—æ³•ä»å™ªå£°æ•°æ®ä¸­å­¦ä¹ æ½œåœ¨çš„é›†ä½“å…±è¯†ã€‚å®ƒé€šè¿‡æ¨æ–­æ¯ä¸ªåå¥½æ ‡ç­¾çš„æ­£ç¡®æ€§ï¼Œå¹¶ä½¿ç”¨è¿™ä¸ªæ¦‚ç‡ä½œä¸ºè‡ªé€‚åº”æƒé‡æ¥é‡æ–°æ ¡å‡†æ¯ä¸ªæ•°æ®ç‚¹å¯¹è®­ç»ƒæŸå¤±çš„è´¡çŒ®ï¼Œä»è€Œå‡è½»å™ªå£°å½±å“ã€‚æˆ‘ä»¬é€šè¿‡å»ºç«‹ä»»æ„åå¥½æŸå¤±ä¸å…¶ç›¸åº”æ¦‚ç‡æ¨¡å‹ä¹‹é—´çš„ç†è®ºè”ç³»ï¼Œå°†è¿™ç§æ–¹æ³•ä¸€èˆ¬åŒ–ï¼Œä½¿LCPOä»ä¸€ä¸ªç‰¹å®šç®—æ³•æå‡ä¸ºä¸€ä¸ªç¨³å¥çš„åå¥½å¯¹é½çš„ä¸€èˆ¬æ¡†æ¶ã€‚ç†è®ºä¸Šï¼Œæˆ‘ä»¬è¯æ˜åœ¨æ¨¡å‹å®Œå…¨æ ¡å‡†çš„æ¡ä»¶ä¸‹ï¼ŒLCPOä¸€å®šèƒ½æ”¶æ•›åˆ°æ•°æ®é›†çš„çœŸå®å™ªå£°æ°´å¹³ã€‚æˆ‘ä»¬çš„å®éªŒè¯æ˜äº†LCPOä½œä¸ºä¸€èˆ¬æ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œå®ƒå§‹ç»ˆå¦‚ä¸€åœ°å¢å¼ºäº†å››ç§æœ€å…ˆè¿›çš„å¯¹é½ç®—æ³•ï¼ˆDPOã€IPOã€SimPOå’ŒCPOï¼‰ã€‚å½“åº”ç”¨äºMistralå’ŒLlama 3æ¨¡å‹æ—¶ï¼Œé‡‡ç”¨LCPOå¢å¼ºæ–¹æ³•åœ¨AlpacaEval 2å’ŒArena-Hardä¸Šå®ç°äº†æ˜¾è‘—çš„èƒœç‡æå‡ï¼Œåœ¨ä¸¤ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„æ”¹è¿›å‡è¾¾åˆ°7.0%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24159v2">PDF</a> </p>
<p><strong>Summary</strong><br>     åŸºäºäººç±»åå¥½åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ç­‰æ ‡å‡†äººç±»åå¥½åŸºå‡†å¯¹é½æ–¹æ³•æ˜¯å¯¹é½å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸äººç±»ä»·å€¼è§‚çš„æ ¸å¿ƒæŠ€æœ¯ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å»ºç«‹åœ¨æœ‰ç¼ºé™·çš„å‡è®¾ä¸Šï¼Œå³äººç±»åå¥½æ˜¯å‡è´¨çš„ï¼Œä¸”æ”¶é›†çš„æ•°æ®æ˜¯æ— å™ªå£°çš„ã€‚å®é™…ä¸Šï¼Œäººç±»åå¥½æ˜¯å¤šå…ƒåŒ–çš„ï¼Œæ³¨é‡Šè€…ä¹Ÿå¯èƒ½çŠ¯é”™ã€‚è¿™å¯¼è‡´è®°å½•çš„æ•°æ®ä¸çœŸå®åå¥½ä¹‹é—´å­˜åœ¨å·®å¼‚ï¼Œå¯èƒ½ä¼šè¯¯å¯¼æ¨¡å‹å¹¶é™ä½å…¶æ€§èƒ½ã€‚ä¸ºè§£å†³æ­¤æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ½œåœ¨é›†ä½“åå¥½ä¼˜åŒ–ï¼ˆLCPOï¼‰ã€‚LCPOåˆ©ç”¨æœŸæœ›æœ€å¤§åŒ–ï¼ˆEMï¼‰ç®—æ³•ä»å™ªå£°æ•°æ®ä¸­å­¦ä¹ æ½œåœ¨çš„é›†ä½“å…±è¯†ã€‚å®ƒé€šè¿‡æ¨æ–­æ¯ä¸ªåå¥½æ ‡ç­¾çš„æ­£ç¡®æ€§ï¼Œå¹¶ä½¿ç”¨æ­¤æ¦‚ç‡ä½œä¸ºè‡ªé€‚åº”æƒé‡æ¥é‡æ–°æ ¡å‡†æ¯ä¸ªæ•°æ®ç‚¹å¯¹è®­ç»ƒæŸå¤±çš„è´¡çŒ®ï¼Œä»è€Œå‡è½»å™ªå£°å½±å“ã€‚æˆ‘ä»¬é€šè¿‡å»ºç«‹ä»»æ„åå¥½æŸå¤±ä¸å…¶ç›¸åº”æ¦‚ç‡æ¨¡å‹ä¹‹é—´çš„ç†è®ºè”ç³»ï¼Œå°†LCPOä»ç‰¹å®šç®—æ³•æå‡ä¸ºç”¨äºç¨³å¥åå¥½å¯¹é½çš„ä¸€èˆ¬æ¡†æ¶ã€‚å®éªŒè¯æ˜ï¼ŒLCPOä½œä¸ºä¸€èˆ¬æ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œå®ƒæŒç»­æå‡å››ç§æœ€å…ˆè¿›çš„å¯¹é½ç®—æ³•ï¼ˆDPOã€IPOã€SimPOå’ŒCPOï¼‰çš„æ€§èƒ½ã€‚åº”ç”¨äºMistralå’ŒLlama 3æ¨¡å‹æ—¶ï¼Œåœ¨AlpacaEval 2å’ŒArena-Hardä¸Šå®ç°äº†æ˜¾è‘—çš„èƒœç‡å¢é•¿ï¼Œä¸¤ä¸ªåŸºå‡†æµ‹è¯•çš„æå‡å¹…åº¦æœ€é«˜è¾¾7.0%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ ‡å‡†äººç±»åå¥½åŸºå‡†å¯¹é½æ–¹æ³•å­˜åœ¨ç¼ºé™·å‡è®¾ï¼šäººç±»åå¥½å‡è´¨ä¸”æ•°æ®æ— å™ªå£°ã€‚</li>
<li>äººç±»åå¥½å®é™…ä¸Šæ˜¯å¤šå…ƒåŒ–çš„ï¼Œä¸”æ³¨é‡Šæ•°æ®å¯èƒ½å­˜åœ¨é”™è¯¯ã€‚</li>
<li>LCPOåˆ©ç”¨æœŸæœ›æœ€å¤§åŒ–ï¼ˆEMï¼‰ç®—æ³•ä»å™ªå£°æ•°æ®ä¸­å­¦ä¹ æ½œåœ¨é›†ä½“å…±è¯†ã€‚</li>
<li>LCPOé€šè¿‡è‡ªé€‚åº”æƒé‡é‡æ–°æ ¡å‡†æ•°æ®ç‚¹å¯¹è®­ç»ƒæŸå¤±çš„è´¡çŒ®æ¥å‡è½»å™ªå£°å½±å“ã€‚</li>
<li>LCPOé€šè¿‡å»ºç«‹åå¥½æŸå¤±ä¸æ¦‚ç‡æ¨¡å‹ä¹‹é—´çš„ç†è®ºè”ç³»ï¼Œä»ç‰¹å®šç®—æ³•æå‡ä¸ºä¸€èˆ¬æ¡†æ¶ã€‚</li>
<li>LCPOèƒ½æé«˜å››ç§ä¸»æµå¯¹é½ç®—æ³•çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24159">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-99baf5d07024593096c5d30184a47377~resize:0:q75.jpg?source=1f5c5e47&expiration=1760103313&auth_key=1760103313-0-0-98d93f2ed330b5f3d4c78384bc113a69&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8c11135b3344ea7839a51f8b111d6c5d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760103320&auth_key=1760103320-0-0-1dcaf3287b647ec42596eea76efbb66c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8db0f8c049cac30661cc9216c8c87ecf~resize:0:q75.jpg?source=1f5c5e47&expiration=1760103327&auth_key=1760103327-0-0-7215b7f08acb2f14d51a5a9d56205f8d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-03/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-03/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-03/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-3d52d5228ee62a219a9444fa4da71d49~resize:0:q75.jpg?source=1f5c5e47&expiration=1760084237&auth_key=1760084237-0-0-30cb4e3d4574ff2cf917b9511e5e07c8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-03  Beyond the Algorithm A Field Guide to Deploying AI Agents in Clinical   Practice
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-03
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-03/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-ef223f08be66bf29111cae85e7ed3831~resize:0:q75.jpg?source=1f5c5e47&expiration=1760086136&auth_key=1760086136-0-0-333cc4ded906f7dfb6b2e170b571878a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-03  PortraitTalk Towards Customizable One-Shot Audio-to-Talking Face   Generation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-03
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32127.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
