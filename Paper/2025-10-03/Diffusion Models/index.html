<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-10-03  DC-Gen Post-Training Diffusion Acceleration with Deeply Compressed   Latent Space">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_Diffusion Models/2509.05978v2/page_3_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-10-03
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-10
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    8.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    34 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-10-03-更新"><a href="#2025-10-03-更新" class="headerlink" title="2025-10-03 更新"></a>2025-10-03 更新</h1><h2 id="DC-Gen-Post-Training-Diffusion-Acceleration-with-Deeply-Compressed-Latent-Space"><a href="#DC-Gen-Post-Training-Diffusion-Acceleration-with-Deeply-Compressed-Latent-Space" class="headerlink" title="DC-Gen: Post-Training Diffusion Acceleration with Deeply Compressed   Latent Space"></a>DC-Gen: Post-Training Diffusion Acceleration with Deeply Compressed   Latent Space</h2><p><strong>Authors:Wenkun He, Yuchao Gu, Junyu Chen, Dongyun Zou, Yujun Lin, Zhekai Zhang, Haocheng Xi, Muyang Li, Ligeng Zhu, Jincheng Yu, Junsong Chen, Enze Xie, Song Han, Han Cai</strong></p>
<p>Existing text-to-image diffusion models excel at generating high-quality images, but face significant efficiency challenges when scaled to high resolutions, like 4K image generation. While previous research accelerates diffusion models in various aspects, it seldom handles the inherent redundancy within the latent space. To bridge this gap, this paper introduces DC-Gen, a general framework that accelerates text-to-image diffusion models by leveraging a deeply compressed latent space. Rather than a costly training-from-scratch approach, DC-Gen uses an efficient post-training pipeline to preserve the quality of the base model. A key challenge in this paradigm is the representation gap between the base model’s latent space and a deeply compressed latent space, which can lead to instability during direct fine-tuning. To overcome this, DC-Gen first bridges the representation gap with a lightweight embedding alignment training. Once the latent embeddings are aligned, only a small amount of LoRA fine-tuning is needed to unlock the base model’s inherent generation quality. We verify DC-Gen’s effectiveness on SANA and FLUX.1-Krea. The resulting DC-Gen-SANA and DC-Gen-FLUX models achieve quality comparable to their base models but with a significant speedup. Specifically, DC-Gen-FLUX reduces the latency of 4K image generation by 53x on the NVIDIA H100 GPU. When combined with NVFP4 SVDQuant, DC-Gen-FLUX generates a 4K image in just 3.5 seconds on a single NVIDIA 5090 GPU, achieving a total latency reduction of 138x compared to the base FLUX.1-Krea model. Code: <a target="_blank" rel="noopener" href="https://github.com/dc-ai-projects/DC-Gen">https://github.com/dc-ai-projects/DC-Gen</a>. </p>
<blockquote>
<p>现有的文本到图像扩散模型在生成高质量图像方面表现出色，但在扩展到高分辨率（如4K图像生成）时面临重大的效率挑战。尽管之前的研究从各个方面加速了扩散模型，但很少处理潜在空间中的固有冗余。为了填补这一空白，本文介绍了DC-Gen，这是一个通过利用深度压缩潜在空间来加速文本到图像扩散模型的通用框架。DC-Gen并非采用昂贵的从头开始训练的方法，而是采用高效的后训练管道来保留基础模型的质量。这一范式中的关键挑战是基础模型的潜在空间与深度压缩的潜在空间之间的表示差距，这可能导致直接微调时的不稳定性。为了克服这一点，DC-Gen首先通过轻量级的嵌入对齐训练来弥合表示差距。一旦潜在嵌入对齐，只需少量的LoRA微调即可释放基础模型的固有生成质量。我们在SANA和FLUX.1-Krea上验证了DC-Gen的有效性。DC-Gen-SANA和DC-Gen-FLUX模型在质量上与基础模型相当，但速度显著提高。具体来说，DC-Gen-FLUX在NVIDIA H100 GPU上将4K图像的生成延迟减少了53倍。当与NVFP4 SVDQuant结合时，DC-Gen-FLUX在单个NVIDIA 5090 GPU上仅用3.5秒即可生成4K图像，与基础FLUX.1-Krea模型相比，总延迟减少了138倍。代码：<a target="_blank" rel="noopener" href="https://github.com/dc-ai-projects/DC-Gen%E3%80%82">https://github.com/dc-ai-projects/DC-Gen。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25180v2">PDF</a> Tech Report. The first three authors contributed equally to this work</p>
<p><strong>Summary</strong></p>
<p>本文介绍了DC-Gen框架，它通过利用深度压缩的潜在空间来加速文本到图像的扩散模型。DC-Gen采用高效的训练后流程，而非从头开始训练的方法，保留了基础模型的质量。解决了基础模型的潜在空间与深度压缩的潜在空间之间的表示鸿沟问题，采用轻量级嵌入对齐训练进行弥补，然后通过少量的LoRA微调来解锁基础模型的生成质量。在SANA和FLUX.1-Krea上验证了DC-Gen的有效性，生成的模型在质量上可与基础模型相当，但速度更快。特别是DC-Gen-FLUX在NVIDIA H100 GPU上将4K图像生成的延迟减少了53倍。结合NVFP4 SVDQuant，DC-Gen-FLUX在单个NVIDIA 5090 GPU上生成4K图像只需3.5秒，与基础FLUX.1-Krea模型相比，总延迟降低了138倍。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DC-Gen是一个用于加速文本到图像扩散模型的框架，它通过深度压缩潜在空间来提高效率。</li>
<li>DC-Gen采用训练后流程，而不是从头开始训练，以保留基础模型的质量。</li>
<li>解决了基础模型与深度压缩潜在空间之间的表示鸿沟问题。</li>
<li>采用轻量级嵌入对齐训练来弥补表示鸿沟。</li>
<li>仅需少量LoRA微调即可解锁基础模型的生成质量。</li>
<li>DC-Gen在SANA和FLUX.1-Krea模型上的实施效果显著，生成图像质量相当，但速度更快。</li>
<li>DC-Gen与FLUX模型的结合在4K图像生成方面实现了显著的延迟降低，为实际应用带来了更高的效率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25180">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_Diffusion Models/2509.25180v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_Diffusion Models/2509.25180v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_Diffusion Models/2509.25180v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_Diffusion Models/2509.25180v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_Diffusion Models/2509.25180v2/page_5_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Imagining-Alternatives-Towards-High-Resolution-3D-Counterfactual-Medical-Image-Generation-via-Language-Guidance"><a href="#Imagining-Alternatives-Towards-High-Resolution-3D-Counterfactual-Medical-Image-Generation-via-Language-Guidance" class="headerlink" title="Imagining Alternatives: Towards High-Resolution 3D Counterfactual   Medical Image Generation via Language Guidance"></a>Imagining Alternatives: Towards High-Resolution 3D Counterfactual   Medical Image Generation via Language Guidance</h2><p><strong>Authors:Mohamed Mohamed, Brennan Nichyporuk, Douglas L. Arnold, Tal Arbel</strong></p>
<p>Vision-language models have demonstrated impressive capabilities in generating 2D images under various conditions; however, the success of these models is largely enabled by extensive, readily available pretrained foundation models. Critically, comparable pretrained models do not exist for 3D, significantly limiting progress. As a result, the potential of vision-language models to produce high-resolution 3D counterfactual medical images conditioned solely on natural language remains unexplored. Addressing this gap would enable powerful clinical and research applications, such as personalized counterfactual explanations, simulation of disease progression, and enhanced medical training by visualizing hypothetical conditions in realistic detail. Our work takes a step toward this challenge by introducing a framework capable of generating high-resolution 3D counterfactual medical images of synthesized patients guided by free-form language prompts. We adapt state-of-the-art 3D diffusion models with enhancements from Simple Diffusion and incorporate augmented conditioning to improve text alignment and image quality. To our knowledge, this is the first demonstration of a language-guided native-3D diffusion model applied to neurological imaging, where faithful three-dimensional modeling is essential. On two neurological MRI datasets, our framework simulates varying counterfactual lesion loads in Multiple Sclerosis and cognitive states in Alzheimer’s disease, generating high-quality images while preserving subject fidelity. Our results lay the groundwork for prompt-driven disease progression analysis in 3D medical imaging. Project link - <a target="_blank" rel="noopener" href="https://lesupermomo.github.io/imagining-alternatives/">https://lesupermomo.github.io/imagining-alternatives/</a>. </p>
<blockquote>
<p>视觉语言模型在各种条件下生成2D图像的能力令人印象深刻。然而，这些模型的成功在很大程度上得益于广泛且易于获取的预训练基础模型。关键的是，对于3D领域，并没有与之相当的预训练模型，这严重限制了进展。因此，视觉语言模型在仅根据自然语言生成高分辨率的3D反事实医学影像方面的潜力尚未被探索。填补这一空白将能推动临床和研究应用的发展，如个性化的反事实解释、疾病进展模拟以及通过详细展示假设条件来增强医学培训。我们的工作朝着这一挑战迈出了一步，我们引入了一个框架，该框架能够通过自由形式的语言提示来生成由合成患者的高分辨率3D反事实医学影像。我们改进了最先进的3D扩散模型并融入了简单扩散，同时增加了附加条件来提高文本对齐和图像质量。据我们所知，这是首次将语言引导的原生3D扩散模型应用于神经成像，忠实的三维建模在这里至关重要。在两个神经MRI数据集上，我们的框架模拟了多发性硬化症的多种反事实病灶负荷以及阿尔茨海默病的认知状态，生成了高质量图像，同时保持了主体忠实度。我们的研究为3D医学影像中的提示驱动疾病进展分析奠定了基础。项目链接：<a target="_blank" rel="noopener" href="https://lesupermomo.github.io/imagining-alternatives/">https://lesupermomo.github.io/imagining-alternatives/</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.05978v2">PDF</a> Accepted to the 2025 MICCAI ELAMI Workshop</p>
<p><strong>Summary</strong><br>文本主要介绍了在生成高质量三维图像方面的技术挑战与现状。目前大多数成功的视觉语言模型都依赖于已经存在的预训练模型作为基础，但这样的模型在三维图像生成领域仍属稀缺。本文介绍了一个能够基于自然语言提示生成高质量三维医疗图像的系统框架，这可以应用在医疗的临床和研究上，比如生成个性化反向解释、模拟疾病进展等。这为后续的进一步研究，如基于提示的疾病进展分析奠定了基础。该项目已在两个神经MRI数据集上进行了验证。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>当前视觉语言模型在生成高质量三维图像方面面临挑战，原因在于缺乏相应的预训练模型基础。</li>
<li>本文介绍的系统框架首次展示了语言引导的原生三维扩散模型在神经成像中的应用。</li>
<li>该框架能够基于自然语言提示生成高质量的三维医疗图像，并应用于个性化反向解释、模拟疾病进展等医疗临床和研究领域。</li>
<li>该系统在两个神经MRI数据集上进行了验证，能够模拟多发性硬化症的病变负荷和阿尔茨海默病中的认知状态变化。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.05978">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_Diffusion Models/2509.05978v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_Diffusion Models/2509.05978v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_Diffusion Models/2509.05978v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_Diffusion Models/2509.05978v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_Diffusion Models/2509.05978v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Text-to-CT-Generation-via-3D-Latent-Diffusion-Model-with-Contrastive-Vision-Language-Pretraining"><a href="#Text-to-CT-Generation-via-3D-Latent-Diffusion-Model-with-Contrastive-Vision-Language-Pretraining" class="headerlink" title="Text-to-CT Generation via 3D Latent Diffusion Model with Contrastive   Vision-Language Pretraining"></a>Text-to-CT Generation via 3D Latent Diffusion Model with Contrastive   Vision-Language Pretraining</h2><p><strong>Authors:Daniele Molino, Camillo Maria Caruso, Filippo Ruffini, Paolo Soda, Valerio Guarrasi</strong></p>
<p>Objective: While recent advances in text-conditioned generative models have enabled the synthesis of realistic medical images, progress has been largely confined to 2D modalities such as chest X-rays. Extending text-to-image generation to volumetric CT remains a significant challenge, due to its high dimensionality, anatomical complexity, and the absence of robust frameworks that align vision-language data in 3D medical imaging. Methods: We introduce a novel architecture for Text-to-CT generation that combines a latent diffusion model with a 3D contrastive vision-language pretraining scheme. Our approach leverages a dual-encoder CLIP-style model trained on paired CT volumes and radiology reports to establish a shared embedding space, which serves as the conditioning input for generation. CT volumes are compressed into a low-dimensional latent space via a pretrained volumetric VAE, enabling efficient 3D denoising diffusion without requiring external super-resolution stages. Results: We evaluate our method on the CT-RATE dataset and conduct a comprehensive assessment of image fidelity, clinical relevance, and semantic alignment. Our model achieves competitive performance across all tasks, significantly outperforming prior baselines for text-to-CT generation. Moreover, we demonstrate that CT scans synthesized by our framework can effectively augment real data, improving downstream diagnostic performance. Conclusion: Our results show that modality-specific vision-language alignment is a key component for high-quality 3D medical image generation. By integrating contrastive pretraining and volumetric diffusion, our method offers a scalable and controllable solution for synthesizing clinically meaningful CT volumes from text, paving the way for new applications in data augmentation, medical education, and automated clinical simulation. Code at <a target="_blank" rel="noopener" href="https://github.com/cosbidev/Text2CT">https://github.com/cosbidev/Text2CT</a>. </p>
<blockquote>
<p>目标：尽管近期文本条件生成模型取得了进展，已经能够合成逼真的医学图像，但进展主要局限于如胸部X光等2D模式。将文本到图像的生成扩展到体积CT仍然是一个重大挑战，这主要是由于其高维度、解剖结构复杂，以及缺乏能够在3D医学成像中对视觉语言数据进行对齐的稳健框架。方法：我们引入了一种用于文本到CT生成的新型架构，它结合了潜在扩散模型与3D对比视觉语言预训练方案。我们的方法利用在配对CT体积和放射学报告上训练的双重编码器CLIP风格模型，建立共享嵌入空间，作为生成的条件输入。CT体积通过预训练的体积VAE压缩成低维潜在空间，实现高效的3D去噪扩散，而无需外部超分辨率阶段。结果：我们在CT-RATE数据集上评估了我们的方法，并全面评估了图像保真度、临床相关性和语义对齐。我们的模型在所有任务上均表现出竞争力，在文本到CT生成方面显著超越了先前的基线。此外，我们证明了我们框架合成的CT扫描可以有效地增强真实数据，提高下游诊断性能。结论：我们的结果表明，模态特定的视觉语言对齐是高质量3D医学图像生成的关键组成部分。通过结合对比预训练和体积扩散，我们的方法提供了一种可扩展和可控的解决方案，可以根据文本合成具有临床意义的CT体积，为数据增强、医学教育和自动化临床模拟等领域开辟了新应用途径。相关代码可在[<a target="_blank" rel="noopener" href="https://github.com/cosbidev/Text2CT%E6%89%BE%E5%88%B0%E3%80%82]">https://github.com/cosbidev/Text2CT找到。]</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.00633v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种将文本转化为CT图像的新方法，通过结合潜在扩散模型和三维对比视觉语言预训练方案，实现了文本到CT图像的高效生成。该方法使用双编码器CLIP风格的模型，在配对CT体积和放射学报告上训练，建立共享嵌入空间作为生成的条件输入。将CT体积压缩到低维潜在空间，通过预训练的体积VAE实现高效的三维去噪扩散。评估结果显示，该方法在图像保真度、临床相关性和语义对齐方面表现优异，显著优于先前的基线文本到CT生成模型。此外，通过合成CT扫描数据对真实数据进行增强，可提高下游诊断性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>介绍了一种新的文本转CT图像生成方法，能够合成逼真的三维医学图像。</li>
<li>采用了潜在扩散模型和三维对比视觉语言预训练，提高了生成模型的效率和性能。</li>
<li>使用双编码器CLIP风格的模型，建立共享嵌入空间作为生成条件，增强了图像与文本的对应关系。</li>
<li>通过将CT体积压缩到低维潜在空间，实现了高效的三维去噪扩散，无需外部超分辨率阶段。</li>
<li>评估结果表明，该方法在图像保真度、临床相关性和语义对齐方面表现优异。</li>
<li>合成CT扫描数据可有效增强真实数据，提高下游诊断性能。</li>
<li>模态特定的视觉语言对齐是高质量三维医学图像生成的关键组件。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.00633">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_Diffusion Models/2506.00633v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_Diffusion Models/2506.00633v2/page_3_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="STORK-Faster-Diffusion-And-Flow-Matching-Sampling-By-Resolving-Both-Stiffness-And-Structure-Dependence"><a href="#STORK-Faster-Diffusion-And-Flow-Matching-Sampling-By-Resolving-Both-Stiffness-And-Structure-Dependence" class="headerlink" title="STORK: Faster Diffusion And Flow Matching Sampling By Resolving Both   Stiffness And Structure-Dependence"></a>STORK: Faster Diffusion And Flow Matching Sampling By Resolving Both   Stiffness And Structure-Dependence</h2><p><strong>Authors:Zheng Tan, Weizhen Wang, Andrea L. Bertozzi, Ernest K. Ryu</strong></p>
<p>Diffusion models (DMs) and flow-matching models have demonstrated remarkable performance in image and video generation. However, such models require a significant number of function evaluations (NFEs) during sampling, leading to costly inference. Consequently, quality-preserving fast sampling methods that require fewer NFEs have been an active area of research. However, prior training-free sampling methods fail to simultaneously address two key challenges: the stiffness of the ODE (i.e., the non-straightness of the velocity field) and dependence on the semi-linear structure of the DM ODE (which limits their direct applicability to flow-matching models). In this work, we introduce the Stabilized Taylor Orthogonal Runge–Kutta (STORK) method, addressing both design concerns. We demonstrate that STORK consistently improves the quality of diffusion and flow-matching sampling for image and video generation. Code is available at <a target="_blank" rel="noopener" href="https://github.com/ZT220501/STORK">https://github.com/ZT220501/STORK</a>. </p>
<blockquote>
<p>扩散模型（DMs）和流量匹配模型在图像和视频生成方面表现出了显著的性能。然而，这些模型在采样过程中需要大量的函数评估（NFEs），导致推理成本高昂。因此，需要较少NFEs的保质保速采样方法已成为研究热点。然而，之前的无训练采样方法未能同时解决两个关键挑战：常微分方程的刚度（即速度场的非直线性）和对DM常微分方程的半线性结构的依赖（这限制了它们直接应用于流量匹配模型）。在这项工作中，我们引入了稳定的泰勒正交龙格-库塔（STORK）方法，解决了这两个设计问题。我们证明STORK方法能持续提高图像和视频生成的扩散和流量匹配采样的质量。代码可在 <a target="_blank" rel="noopener" href="https://github.com/ZT220501/STORK">https://github.com/ZT220501/STORK</a> 获得。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24210v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了Diffusion models（DMs）在图像和视频生成中的出色表现，但采样过程中需要大量功能评估（NFEs），导致推理成本高昂。为减少NFEs，研究者们致力于开发保持质量的同时更快的采样方法。然而，先前无训练采样方法未能同时解决两个关键问题：常微分方程的刚度（即速度场的不直线性）和对DM ODE半线性结构的依赖（限制了它们对流动匹配模型的直接应用）。本研究引入稳定化的泰勒正交龙格库塔（STORK）方法，解决了这两个设计难题。实验证明，STORK在图像和视频生成的扩散和流动匹配采样中持续提高了质量。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Diffusion models (DMs) 和流动匹配模型在图像和视频生成中表现出卓越性能。</li>
<li>采样过程中需要大量功能评估（NFEs），导致推理成本高昂。</li>
<li>此前无训练采样方法无法同时解决ODE的刚度问题和半线性结构依赖问题。</li>
<li>引入的STORK方法解决了这两个设计难题。</li>
<li>STORK方法提高了图像和视频生成的扩散和流动匹配采样的质量。</li>
<li>STORK方法的代码已公开在GitHub上。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24210">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_Diffusion Models/2505.24210v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_Diffusion Models/2505.24210v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_Diffusion Models/2505.24210v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_Diffusion Models/2505.24210v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_Diffusion Models/2505.24210v2/page_4_0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="H3AE-High-Compression-High-Speed-and-High-Quality-AutoEncoder-for-Video-Diffusion-Models"><a href="#H3AE-High-Compression-High-Speed-and-High-Quality-AutoEncoder-for-Video-Diffusion-Models" class="headerlink" title="H3AE: High Compression, High Speed, and High Quality AutoEncoder for   Video Diffusion Models"></a>H3AE: High Compression, High Speed, and High Quality AutoEncoder for   Video Diffusion Models</h2><p><strong>Authors:Yushu Wu, Yanyu Li, Ivan Skorokhodov, Anil Kag, Willi Menapace, Sharath Girish, Aliaksandr Siarohin, Yanzhi Wang, Sergey Tulyakov</strong></p>
<p>Autoencoder (AE) is the key to the success of latent diffusion models for image and video generation, reducing the denoising resolution and improving efficiency. However, the power of AE has long been underexplored in terms of network design, compression ratio, and training strategy. In this work, we systematically examine the architecture design choices and optimize the computation distribution to obtain a series of efficient and high-compression video AEs that can decode in real time even on mobile devices. We also propose an omni-training objective to unify the design of plain Autoencoder and image-conditioned I2V VAE, achieving multifunctionality in a single VAE network but with enhanced quality. In addition, we propose a novel latent consistency loss that provides stable improvements in reconstruction quality. Latent consistency loss outperforms prior auxiliary losses including LPIPS, GAN and DWT in terms of both quality improvements and simplicity. H3AE achieves ultra-high compression ratios and real-time decoding speed on GPU and mobile, and outperforms prior arts in terms of reconstruction metrics by a large margin. We finally validate our AE by training a DiT on its latent space and demonstrate fast, high-quality text-to-video generation capability. </p>
<blockquote>
<p>自编码器（AE）是潜在扩散模型在图像和视频生成方面取得成功的关键，它可以降低降噪分辨率并提高效率。然而，关于自编码器的网络设计、压缩比和训练策略等方面的潜力长期以来一直未被充分探索。在这项工作中，我们系统地研究了架构设计选择，优化了计算分配，获得了一系列高效的高压缩视频自编码器，即使在移动设备上也能实时解码。我们还提出了一个全面的训练目标，以统一普通自编码器和图像条件I2V VAE的设计，在一个单一的VAE网络中实现多功能性，同时提高了质量。此外，我们提出了一种新型潜在一致性损失，在重建质量方面提供了稳定的改进。潜在一致性损失在质量和简洁性方面优于先前的辅助损失，包括LPIPS、GAN和DWT。H3AE在GPU和移动设备上实现了超高压缩比和实时解码速度，在重建指标上大大优于先前技术。最后，我们通过在其潜在空间上训练DiT来验证我们的自编码器，并展示了快速、高质量的文字到视频生成能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10567v2">PDF</a> 17 pages, 6 figures, 9 tables</p>
<p><strong>Summary</strong></p>
<p>自动编码器（AE）是潜在扩散模型在图像和视频生成中成功的关键，能够降低去噪分辨率并提高效率。本研究系统地探讨了架构设计的选择，优化了计算分布，获得了一系列高效、高压缩的视频自动编码器，即使在移动设备上也能实时解码。此外，还提出了一种全面的训练目标，实现了普通自动编码器和图像条件I2V VAE的统一设计，在单个VAE网络中实现多功能并增强了质量。同时，我们提出了一种新型潜在一致性损失，在重建质量方面提供了稳定的改进。潜在一致性损失在质量和简洁性方面优于先前的辅助损失，包括LPIPS、GAN和DWT。H3AE在GPU和移动设备上实现了超高压缩比和实时解码速度，在重建指标上大幅优于先前技术。最后，我们通过在其潜在空间上训练DiT来验证我们的AE，并展示了快速、高质量的文字到视频生成能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自动编码器（AE）是潜在扩散模型成功的关键，能降低去噪分辨率并提高图像和视频生成效率。</li>
<li>研究优化了视频自动编码器的架构设计，实现高效、高压缩的视频处理，支持实时解码，包括在移动设备上。</li>
<li>提出全面的训练目标，统一普通自动编码器和图像条件I2V VAE的设计，增强网络多功能性和质量。</li>
<li>引入新型潜在一致性损失，有效提高重建质量，优于其他辅助损失方法。</li>
<li>H3AE在GPU和移动设备上实现超高压缩比和快速解码，并在重建指标上显著优于现有技术。</li>
<li>实验验证了AE的文本到视频生成能力，表现出高质量和快速生成的特点。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10567">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_Diffusion Models/2504.10567v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_Diffusion Models/2504.10567v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_Diffusion Models/2504.10567v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_Diffusion Models/2504.10567v2/page_4_1.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="BlobCtrl-Taming-Controllable-Blob-for-Element-level-Image-Editing"><a href="#BlobCtrl-Taming-Controllable-Blob-for-Element-level-Image-Editing" class="headerlink" title="BlobCtrl: Taming Controllable Blob for Element-level Image Editing"></a>BlobCtrl: Taming Controllable Blob for Element-level Image Editing</h2><p><strong>Authors:Yaowei Li, Lingen Li, Zhaoyang Zhang, Xiaoyu Li, Guangzhi Wang, Hongxiang Li, Xiaodong Cun, Ying Shan, Yuexian Zou</strong></p>
<p>As user expectations for image editing continue to rise, the demand for flexible, fine-grained manipulation of specific visual elements presents a challenge for current diffusion-based methods. In this work, we present BlobCtrl, a framework for element-level image editing based on a probabilistic blob-based representation. Treating blobs as visual primitives, BlobCtrl disentangles layout from appearance, affording fine-grained, controllable object-level manipulation. Our key contributions are twofold: (1) an in-context dual-branch diffusion model that separates foreground and background processing, incorporating blob representations to explicitly decouple layout and appearance, and (2) a self-supervised disentangle-then-reconstruct training paradigm with an identity-preserving loss function, along with tailored strategies to efficiently leverage blob-image pairs. To foster further research, we introduce BlobData for large-scale training and BlobBench, a benchmark for systematic evaluation. Experimental results demonstrate that BlobCtrl achieves state-of-the-art performance in a variety of element-level editing tasks, such as object addition, removal, scaling, and replacement, while maintaining computational efficiency. Project Webpage: <a target="_blank" rel="noopener" href="https://liyaowei-stu.github.io/project/BlobCtrl/">https://liyaowei-stu.github.io/project/BlobCtrl/</a> </p>
<blockquote>
<p>随着用户对图像编辑的期望不断提高，对特定视觉元素进行灵活、精细操控的需求为当前基于扩散的方法带来了挑战。在这项工作中，我们提出了BlobCtrl，这是一个基于概率性blob表示的元素级图像编辑框架。将blob视为视觉基本元素，BlobCtrl将布局和外观分开处理，实现了精细可控的对象级操作。我们的主要贡献有两点：（1）上下文中的双分支扩散模型，该模型将前景和背景处理分开，通过融入blob表示来显式地解耦布局和外观；（2）采用自监督的解耦重建训练范式，并配有身份保持损失函数，以及针对blob图像对进行有效利用的定制策略。为了促进进一步研究，我们引入了用于大规模训练的BlobData和用于系统评估的BlobBench基准测试。实验结果表明，BlobCtrl在各种元素级编辑任务中实现了最先进的性能，如对象添加、删除、缩放和替换，同时保持了计算效率。项目网页：<a target="_blank" rel="noopener" href="https://liyaowei-stu.github.io/project/BlobCtrl/">https://liyaowei-stu.github.io/project/BlobCtrl/</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13434v2">PDF</a> Project Webpage: <a target="_blank" rel="noopener" href="https://liyaowei-stu.github.io/project/BlobCtrl/">https://liyaowei-stu.github.io/project/BlobCtrl/</a>   This version presents a major update with rephrased writing. Accepted to   SIGGRAPH Asia 2025</p>
<p><strong>Summary</strong><br>     随着用户对图像编辑的期望不断提高，对特定视觉元素进行灵活、精细的操控对当前的扩散方法提出了挑战。本研究提出了BlobCtrl框架，基于概率性斑块表示进行元素级图像编辑。BlobCtrl将斑块视为视觉基本元素，使布局和外观分离，从而实现精细、可控的对象级操作。主要贡献包括：一、上下文双分支扩散模型，将前景和背景处理分离，结合斑块表示显式地解耦布局和外观；二、自我监督的解耦重建训练模式，具有身份保留的损失函数，以及有效利用斑块图像对的策略。为支持进一步研究，引入了BlobData用于大规模训练和BlobBench系统评估基准。实验结果表明，BlobCtrl在元素级编辑任务上达到了最新技术水平，如对象添加、删除、缩放和替换，同时保持了计算效率。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>用户对图像编辑的期望不断提高，对特定视觉元素的精细操控成为挑战。</li>
<li>BlobCtrl框架基于概率性斑块表示进行元素级图像编辑。</li>
<li>BlobCtrl将布局和外观解耦，实现精细、可控的对象级操作。</li>
<li>主要贡献包括上下文双分支扩散模型和自我监督的解耦重建训练模式。</li>
<li>引入了BlobData用于大规模训练和BlobBench作为系统评估基准。</li>
<li>BlobCtrl在元素级编辑任务上达到了最新技术水平。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13434">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_Diffusion Models/2503.13434v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_Diffusion Models/2503.13434v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_Diffusion Models/2503.13434v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_Diffusion Models/2503.13434v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Diffusion-Model-as-a-Noise-Aware-Latent-Reward-Model-for-Step-Level-Preference-Optimization"><a href="#Diffusion-Model-as-a-Noise-Aware-Latent-Reward-Model-for-Step-Level-Preference-Optimization" class="headerlink" title="Diffusion Model as a Noise-Aware Latent Reward Model for Step-Level   Preference Optimization"></a>Diffusion Model as a Noise-Aware Latent Reward Model for Step-Level   Preference Optimization</h2><p><strong>Authors:Tao Zhang, Cheng Da, Kun Ding, Huan Yang, Kun Jin, Yan Li, Tingting Gao, Di Zhang, Shiming Xiang, Chunhong Pan</strong></p>
<p>Preference optimization for diffusion models aims to align them with human preferences for images. Previous methods typically use Vision-Language Models (VLMs) as pixel-level reward models to approximate human preferences. However, when used for step-level preference optimization, these models face challenges in handling noisy images of different timesteps and require complex transformations into pixel space. In this work, we show that pre-trained diffusion models are naturally suited for step-level reward modeling in the noisy latent space, as they are explicitly designed to process latent images at various noise levels. Accordingly, we propose the Latent Reward Model (LRM), which repurposes components of the diffusion model to predict preferences of latent images at arbitrary timesteps. Building on LRM, we introduce Latent Preference Optimization (LPO), a step-level preference optimization method conducted directly in the noisy latent space. Experimental results indicate that LPO significantly improves the model’s alignment with general, aesthetic, and text-image alignment preferences, while achieving a 2.5-28x training speedup over existing preference optimization methods. Our code and models are available at <a target="_blank" rel="noopener" href="https://github.com/Kwai-Kolors/LPO">https://github.com/Kwai-Kolors/LPO</a>. </p>
<blockquote>
<p>扩散模型的偏好优化旨在使图像与人类偏好对齐。之前的方法通常使用视觉语言模型（VLMs）作为像素级奖励模型来近似人类偏好。然而，当用于步骤级偏好优化时，这些模型在处理不同时间步的噪声图像时面临挑战，需要将复杂的转换进行到像素空间。在这项工作中，我们展示了预训练的扩散模型自然适用于噪声潜在空间的步骤级奖励建模，因为它们被明确设计为处理各种噪声水平的潜在图像。因此，我们提出了潜在奖励模型（LRM），该模型重新利用扩散模型的组件来预测任意时间步的潜在图像的偏好。基于LRM，我们引入了潜在偏好优化（LPO），这是一种直接在噪声潜在空间中进行的步骤级偏好优化方法。实验结果表明，LPO显著提高了模型与一般偏好、审美偏好和文本图像对齐偏好的对齐程度，同时实现了相对于现有偏好优化方法的2.5-28倍训练速度提升。我们的代码和模型可在<a target="_blank" rel="noopener" href="https://github.com/Kwai-Kolors/LPO%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Kwai-Kolors/LPO找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01051v4">PDF</a> NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对扩散模型的偏好优化，旨在使图像与人类偏好对齐。传统方法使用视觉语言模型（VLMs）作为像素级奖励模型来模拟人类偏好，但在处理不同时间步的噪声图像时面临挑战。本文提出利用预训练的扩散模型进行天然适合噪声潜在空间的步级奖励建模，并介绍了潜在奖励模型（LRM）。基于LRM，本文提出了潜在偏好优化（LPO），直接在噪声潜在空间进行步级偏好优化。实验结果表明，LPO在通用、美学和文本图像对齐偏好方面显著提高模型对齐度，同时实现现有偏好优化方法的2.5-28倍训练速度提升。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型的偏好优化旨在使图像与人类偏好对齐。</li>
<li>传统方法使用视觉语言模型（VLMs）作为像素级奖励模型，处理不同时间步的噪声图像时存在挑战。</li>
<li>预训练的扩散模型适合进行噪声潜在空间的步级奖励建模。</li>
<li>提出了潜在奖励模型（LRM）以预测不同时间步的潜在图像的偏好。</li>
<li>基于LRM，引入了潜在偏好优化（LPO），直接在噪声潜在空间进行步级偏好优化。</li>
<li>实验结果表明，LPO在多种偏好方面显著提高模型性能，同时实现训练速度提升。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01051">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_Diffusion Models/2502.01051v4/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_Diffusion Models/2502.01051v4/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_Diffusion Models/2502.01051v4/page_1_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_Diffusion Models/2502.01051v4/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_Diffusion Models/2502.01051v4/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_Diffusion Models/2502.01051v4/page_5_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Towards-More-Accurate-Diffusion-Model-Acceleration-with-A-Timestep-Tuner"><a href="#Towards-More-Accurate-Diffusion-Model-Acceleration-with-A-Timestep-Tuner" class="headerlink" title="Towards More Accurate Diffusion Model Acceleration with A Timestep Tuner"></a>Towards More Accurate Diffusion Model Acceleration with A Timestep Tuner</h2><p><strong>Authors:Mengfei Xia, Yujun Shen, Changsong Lei, Yu Zhou, Ran Yi, Deli Zhao, Wenping Wang, Yong-Jin Liu</strong></p>
<p>A diffusion model, which is formulated to produce an image using thousands of denoising steps, usually suffers from a slow inference speed. Existing acceleration algorithms simplify the sampling by skipping most steps yet exhibit considerable performance degradation. By viewing the generation of diffusion models as a discretized integral process, we argue that the quality drop is partly caused by applying an inaccurate integral direction to a timestep interval. To rectify this issue, we propose a \textbf{timestep tuner} that helps find a more accurate integral direction for a particular interval at the minimum cost. Specifically, at each denoising step, we replace the original parameterization by conditioning the network on a new timestep, enforcing the sampling distribution towards the real one. Extensive experiments show that our plug-in design can be trained efficiently and boost the inference performance of various state-of-the-art acceleration methods, especially when there are few denoising steps. For example, when using 10 denoising steps on LSUN Bedroom dataset, we improve the FID of DDIM from 9.65 to 6.07, simply by adopting our method for a more appropriate set of timesteps. Code is available at \href{<a target="_blank" rel="noopener" href="https://github.com/THU-LYJ-Lab/time-tuner%7D%7Bhttps://github.com/THU-LYJ-Lab/time-tuner%7D">https://github.com/THU-LYJ-Lab/time-tuner}{https://github.com/THU-LYJ-Lab/time-tuner}</a>. </p>
<blockquote>
<p>扩散模型通过数千步去噪步骤生成图像，通常面临推理速度较慢的问题。现有的加速算法通过跳过大部分步骤来简化采样，但会导致性能显著下降。我们将扩散模型的生成视为离散积分过程，认为质量下降部分是由于对时间步长应用了不准确的方向积分。为了解决这个问题，我们提出了一种<strong>时间步长调整器</strong>，它可以帮助以最低成本为特定间隔找到更精确的方向积分。具体来说，在每个去噪步骤中，我们通过以新的时间步长对网络进行条件约束，替换原始参数化，强制采样分布接近真实分布。大量实验表明，我们的插件设计可以高效地进行训练，并提升各种最先进的加速方法的推理性能，特别是在去噪步骤较少时。例如，在LSUN卧室数据集上使用10个去噪步骤时，我们仅仅通过采用我们的方法为适当的时间步长集，就可以将DDIM的FID从9.65提高到6.07。代码可在<a target="_blank" rel="noopener" href="https://github.com/THU-LYJ-Lab/time-tuner%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/THU-LYJ-Lab/time-tuner获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09469v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种针对扩散模型的时序调整器（timestep tuner），解决了现有加速算法在简化采样过程中导致的性能下降问题。通过将扩散模型的生成过程视为离散积分过程，该调整器能够在每个去噪步骤中，通过条件网络对新的时序进行参数化，使采样分布更加接近真实分布。实验结果证明了该方法的有效性，提高了加速方法的推理性能，尤其是在使用较少去噪步骤的情况下。代码已公开。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型通常通过数千步去噪生成图像，导致推理速度慢。</li>
<li>现有加速算法通过跳过大部分步骤来简化采样，但会导致性能显著下降。</li>
<li>本文将扩散模型的生成过程视为离散积分过程，并提出时序调整器（timestep tuner）来解决性能下降问题。</li>
<li>时序调整器通过在每个去噪步骤中条件网络对新的时序进行参数化，使采样分布更接近真实分布。</li>
<li>实验证明，该方法可以高效训练，并提升各种先进加速方法的推理性能。</li>
<li>在使用较少去噪步骤的情况下，该方法表现尤其出色。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2310.09469">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_Diffusion Models/2310.09469v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_Diffusion Models/2310.09469v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_Diffusion Models/2310.09469v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_Diffusion Models/2310.09469v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-03/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-03/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-03/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_医学图像/2509.22444v2/page_1_0.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-10-03  A Multimodal LLM Approach for Visual Question Answering on   Multiparametric 3D Brain MRI
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-03
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-03/NeRF/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-10-03\./crop_NeRF/2311.18208v3/page_0_0.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2025-10-03  SMaRt Improving GANs with Score Matching Regularity
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-03
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30055.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
