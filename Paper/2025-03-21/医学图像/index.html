<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-21  FedSCA Federated Tuning with Similarity-guided Collaborative   Aggregation for Heterogeneous Medical Image Segmentation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-430b460135811c7ede99b5a295a3a404.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-21
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    18.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    75 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-21-æ›´æ–°"><a href="#2025-03-21-æ›´æ–°" class="headerlink" title="2025-03-21 æ›´æ–°"></a>2025-03-21 æ›´æ–°</h1><h2 id="FedSCA-Federated-Tuning-with-Similarity-guided-Collaborative-Aggregation-for-Heterogeneous-Medical-Image-Segmentation"><a href="#FedSCA-Federated-Tuning-with-Similarity-guided-Collaborative-Aggregation-for-Heterogeneous-Medical-Image-Segmentation" class="headerlink" title="FedSCA: Federated Tuning with Similarity-guided Collaborative   Aggregation for Heterogeneous Medical Image Segmentation"></a>FedSCA: Federated Tuning with Similarity-guided Collaborative   Aggregation for Heterogeneous Medical Image Segmentation</h2><p><strong>Authors:Yumin Zhang, Yan Gao, Haoran Duan, Hanqing Guo, Tejal Shah, Rajiv Ranjan, Bo Wei</strong></p>
<p>Transformer-based foundation models (FMs) have recently demonstrated remarkable performance in medical image segmentation. However, scaling these models is challenging due to the limited size of medical image datasets within isolated hospitals, where data centralization is restricted due to privacy concerns. These constraints, combined with the data-intensive nature of FMs, hinder their broader application. Integrating federated learning (FL) with foundation models (FLFM) fine-tuning offers a potential solution to these challenges by enabling collaborative model training without data sharing, thus allowing FMs to take advantage of a diverse pool of sensitive medical image data across hospitals&#x2F;clients. However, non-independent and identically distributed (non-IID) data among clients, paired with computational and communication constraints in federated environments, presents an additional challenge that limits further performance improvements and remains inadequately addressed in existing studies. In this work, we propose a novel FLFM fine-tuning framework, \underline{\textbf{Fed}}erated tuning with \underline{\textbf{S}}imilarity-guided \underline{\textbf{C}}ollaborative \underline{\textbf{A}}ggregation (FedSCA), encompassing all phases of the FL process. This includes (1) specially designed parameter-efficient fine-tuning (PEFT) for local client training to enhance computational efficiency; (2) partial low-level adapter transmission for communication efficiency; and (3) similarity-guided collaborative aggregation (SGCA) on the server side to address non-IID issues. Extensive experiments on three FL benchmarks for medical image segmentation demonstrate the effectiveness of our proposed FedSCA, establishing new SOTA performance. </p>
<blockquote>
<p>åŸºäºTransformerçš„åŸºç¡€æ¨¡å‹ï¼ˆFMsï¼‰åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹é¢æœ€è¿‘è¡¨ç°å‡ºäº†æ˜¾è‘—çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç”±äºå­¤ç«‹åŒ»é™¢ä¸­åŒ»ç–—å›¾åƒæ•°æ®é›†è§„æ¨¡çš„é™åˆ¶ï¼Œä»¥åŠéšç§æ‹…å¿§å¯¼è‡´çš„æ•°æ®é›†ä¸­åŒ–å—é™ï¼Œè¿™äº›æ¨¡å‹çš„è§„æ¨¡æ‰©å¤§å…·æœ‰æŒ‘æˆ˜æ€§ã€‚è¿™äº›çº¦æŸï¼Œç»“åˆFMsæ•°æ®å¯†é›†å‹çš„ç‰¹æ€§ï¼Œé˜»ç¢äº†å…¶æ›´å¹¿æ³›çš„åº”ç”¨ã€‚å°†è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰ä¸åŸºç¡€æ¨¡å‹ï¼ˆFLFMï¼‰å¾®è°ƒç›¸ç»“åˆï¼Œæä¾›äº†ä¸€ç§é€šè¿‡ååŒæ¨¡å‹è®­ç»ƒå®ç°æ— éœ€æ•°æ®å…±äº«çš„è§£å†³æ–¹æ¡ˆï¼Œä»è€Œå…è®¸FMsåˆ©ç”¨è·¨åŒ»é™¢&#x2F;å®¢æˆ·ç«¯çš„å¤šæ ·åŒ–æ•æ„ŸåŒ»ç–—å›¾åƒæ•°æ®ã€‚ç„¶è€Œï¼Œå®¢æˆ·ç«¯ä¹‹é—´çš„éç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆnon-IIDï¼‰æ•°æ®ï¼Œä»¥åŠè”é‚¦ç¯å¢ƒä¸­çš„è®¡ç®—å’Œé€šä¿¡çº¦æŸï¼Œå‘ˆç°äº†ä¸€ä¸ªé¢å¤–çš„æŒ‘æˆ˜ï¼Œé™åˆ¶äº†æ€§èƒ½çš„è¿›ä¸€æ­¥æé«˜ï¼Œå¹¶ä¸”åœ¨ç°æœ‰ç ”ç©¶ä¸­ä»æœªå¾—åˆ°å……åˆ†çš„è§£å†³ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„FLFMå¾®è°ƒæ¡†æ¶â€”â€”åŸºäºç›¸ä¼¼æ€§å¼•å¯¼çš„ååŒèšåˆçš„è”é‚¦è°ƒä¼˜ï¼ˆFedSCAï¼‰ï¼Œæ¶µç›–äº†FLè¿‡ç¨‹çš„å„ä¸ªé˜¶æ®µã€‚è¿™åŒ…æ‹¬ï¼ˆ1ï¼‰ä¸ºæœ¬åœ°å®¢æˆ·ç«¯è®­ç»ƒè®¾è®¡çš„å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰ï¼Œä»¥æé«˜è®¡ç®—æ•ˆç‡ï¼›ï¼ˆ2ï¼‰éƒ¨åˆ†ä½çº§é€‚é…å™¨ä¼ è¾“ä»¥æé«˜é€šä¿¡æ•ˆç‡ï¼›ï¼ˆ3ï¼‰æœåŠ¡å™¨ç«¯ç›¸ä¼¼æ€§å¼•å¯¼çš„ååŒèšåˆï¼ˆSGCAï¼‰ä»¥è§£å†³éIIDé—®é¢˜ã€‚åœ¨ä¸‰ä¸ªåŒ»å­¦å›¾åƒåˆ†å‰²çš„è”é‚¦å­¦ä¹ åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜äº†æˆ‘ä»¬æå‡ºçš„FedSCAçš„æœ‰æ•ˆæ€§ï¼Œå¹¶å»ºç«‹äº†æ–°çš„æ€§èƒ½æœ€ä½³è®°å½•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15390v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºè”é‚¦å­¦ä¹ ï¼ˆFLï¼‰å’Œæ·±åº¦å­¦ä¹ åŸºç¡€æ¨¡å‹ï¼ˆFMsï¼‰çš„æ–°å‹ç²¾ç»†è°ƒèŠ‚æ¡†æ¶FedSCAï¼Œé’ˆå¯¹åŒ»é™¢é—´ç‹¬ç«‹åˆ†å¸ƒçš„æ•°æ®é›†å°ä¸”éç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆnon-IIDï¼‰çš„é—®é¢˜ï¼Œå®ç°å‚æ•°é«˜æ•ˆçš„æœ¬åœ°è®­ç»ƒï¼ŒåŒæ—¶åˆ©ç”¨ç›¸ä¼¼åº¦å¼•å¯¼çš„ååŒèšåˆè§£å†³non-IIDé—®é¢˜ï¼Œç¡®ä¿åŒ»å­¦å›¾åƒåˆ†å‰²åœ¨è”é‚¦ç¯å¢ƒä¸‹çš„é«˜æ€§èƒ½è¿è¡Œã€‚æ­¤æ¡†æ¶å·²åœ¨ä¸‰ä¸ªè”é‚¦å­¦ä¹ åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒéªŒè¯ï¼Œå¹¶è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰ä¸æ·±åº¦å­¦ä¹ åŸºç¡€æ¨¡å‹ï¼ˆFMsï¼‰ç»“åˆè§£å†³äº†åŒ»é™¢æ•°æ®è§„æ¨¡å°å’Œæ•°æ®éšç§çš„æŒ‘æˆ˜ã€‚</li>
<li>éç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆnon-IIDï¼‰æ•°æ®æ˜¯è”é‚¦å­¦ä¹ ä¸­çš„ä¸€å¤§éš¾é¢˜ï¼Œå½±å“äº†æ¨¡å‹æ€§èƒ½çš„æå‡ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹è”é‚¦å­¦ä¹ ç²¾ç»†è°ƒèŠ‚æ¡†æ¶FedSCAï¼ŒåŒ…å«å‚æ•°é«˜æ•ˆçš„æœ¬åœ°è®­ç»ƒã€éƒ¨åˆ†ä½çº§é€‚é…å™¨ä¼ è¾“å’Œç›¸ä¼¼åº¦å¼•å¯¼çš„ååŒèšåˆã€‚</li>
<li>FedSCAæ¡†æ¶é€šè¿‡ä¸“é—¨è®¾è®¡çš„å‚æ•°é«˜æ•ˆç²¾ç»†è°ƒèŠ‚ï¼ˆPEFTï¼‰æå‡è®¡ç®—æ•ˆç‡ã€‚</li>
<li>é€šè¿‡éƒ¨åˆ†ä½çº§é€‚é…å™¨ä¼ è¾“æé«˜é€šä¿¡æ•ˆç‡ã€‚</li>
<li>æœåŠ¡å™¨ç«¯çš„ç›¸ä¼¼åº¦å¼•å¯¼ååŒèšåˆï¼ˆSGCAï¼‰è§£å†³äº†non-IIDé—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15390">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1b64108bc9c44c83421cec197c43b35a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-64e67f2509a30a723a93fb17c9e463e1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c0bd207fda033accbe66b9e0edf13a3e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6a24aa0ad1e9b67b2b6c02b1edf729c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a9a483f52eac6b9e1611e505cb7c508.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-afbf8c52241a8e257f161893fda280d0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="DEPT-Deep-Extreme-Point-Tracing-for-Ultrasound-Image-Segmentation"><a href="#DEPT-Deep-Extreme-Point-Tracing-for-Ultrasound-Image-Segmentation" class="headerlink" title="DEPT: Deep Extreme Point Tracing for Ultrasound Image Segmentation"></a>DEPT: Deep Extreme Point Tracing for Ultrasound Image Segmentation</h2><p><strong>Authors:Lei Shi, Xi Fang, Naiyu Wang, Junxing Zhang</strong></p>
<p>Automatic medical image segmentation plays a crucial role in computer aided diagnosis. However, fully supervised learning approaches often require extensive and labor-intensive annotation efforts. To address this challenge, weakly supervised learning methods, particularly those using extreme points as supervisory signals, have the potential to offer an effective solution. In this paper, we introduce Deep Extreme Point Tracing (DEPT) integrated with Feature-Guided Extreme Point Masking (FGEPM) algorithm for ultrasound image segmentation. Notably, our method generates pseudo labels by identifying the lowest-cost path that connects all extreme points on the feature map-based cost matrix. Additionally, an iterative training strategy is proposed to refine pseudo labels progressively, enabling continuous network improvement. Experimental results on two public datasets demonstrate the effectiveness of our proposed method. The performance of our method approaches that of the fully supervised method and outperforms several existing weakly supervised methods. </p>
<blockquote>
<p>åœ¨åŒ»å­¦å›¾åƒè¯Šæ–­è¾…åŠ©ä¸­ï¼Œè‡ªåŠ¨åŒ»å­¦å›¾åƒåˆ†å‰²å‘æŒ¥ç€å…³é”®ä½œç”¨ã€‚ç„¶è€Œï¼Œå®Œå…¨ç›‘ç£å­¦ä¹ æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡ä¸”åŠ³åŠ¨å¼ºåº¦å¤§çš„æ ‡æ³¨å·¥ä½œã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œå¼±ç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯ä½¿ç”¨æç«¯ç‚¹ä½œä¸ºç›‘ç£ä¿¡å·çš„å¼±ç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œå…·æœ‰æä¾›æœ‰æ•ˆè§£å†³æ–¹æ¡ˆçš„æ½œåŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºç‰¹å¾å¼•å¯¼æç«¯ç‚¹é®è”½ç®—æ³•ï¼ˆFGEPMï¼‰çš„æ·±åº¦æå€¼ç‚¹è¿½è¸ªï¼ˆDEPTï¼‰æ–¹æ³•ï¼Œç”¨äºè¶…å£°å›¾åƒåˆ†å‰²ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡è¯†åˆ«åŸºäºç‰¹å¾æ˜ å°„çš„æˆæœ¬çŸ©é˜µä¸Šæ‰€æœ‰æç«¯ç‚¹ä¹‹é—´çš„æœ€ä½æˆæœ¬è·¯å¾„æ¥ç”Ÿæˆä¼ªæ ‡ç­¾ã€‚æ­¤å¤–ï¼Œæå‡ºäº†ä¸€ç§è¿­ä»£è®­ç»ƒç­–ç•¥æ¥é€æ­¥ä¼˜åŒ–ä¼ªæ ‡ç­¾ï¼Œä»è€Œå®ç°ç½‘ç»œçš„æŒç»­æ”¹è¿›ã€‚åœ¨ä¸¤ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¯æ˜äº†æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚è¯¥æ–¹æ³•çš„æ€§èƒ½æ¥è¿‘å…¨ç›‘ç£æ–¹æ³•ï¼Œä¼˜äºå…¶ä»–ä¸€äº›ç°æœ‰çš„å¼±ç›‘ç£æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15260v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç»“åˆæ·±åº¦æç«¯ç‚¹è¿½è¸ªï¼ˆDEPTï¼‰å’Œç‰¹å¾å¼•å¯¼æç«¯ç‚¹æ©æ¨¡ï¼ˆFGEPMï¼‰ç®—æ³•çš„è¶…å£°å›¾åƒåˆ†å‰²æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡è¯†åˆ«åŸºäºç‰¹å¾æ˜ å°„æˆæœ¬çŸ©é˜µä¸Šæ‰€æœ‰æç«¯ç‚¹çš„æœ€ä½æˆæœ¬è·¯å¾„æ¥ç”Ÿæˆä¼ªæ ‡ç­¾ï¼Œå¹¶é‡‡ç”¨è¿­ä»£è®­ç»ƒç­–ç•¥é€æ­¥ä¼˜åŒ–ä¼ªæ ‡ç­¾ï¼Œä»è€Œæé«˜ç½‘ç»œæ€§èƒ½ã€‚åœ¨å…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§æ¥è¿‘å…¨ç›‘ç£æ–¹æ³•ï¼Œå¹¶ä¼˜äºä¸€äº›ç°æœ‰çš„å¼±ç›‘ç£æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨åŒ»å­¦å›¾åƒåˆ†å‰²åœ¨è®¡ç®—æœºè¾…åŠ©è¯Šæ–­ä¸­èµ·å…³é”®ä½œç”¨ï¼Œä½†å…¨ç›‘ç£å­¦ä¹ æ–¹æ³•éœ€è¦å¤§é‡äººå·¥æ ‡æ³¨ï¼Œå­˜åœ¨åŠ³åŠ¨å¯†é›†å‹é—®é¢˜ã€‚</li>
<li>å¼±ç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯ä½¿ç”¨æç«¯ç‚¹ä½œä¸ºç›‘ç£ä¿¡å·çš„æ–¹æ³•ï¼Œä¸ºè§£å†³è¿™ä¸€é—®é¢˜æä¾›äº†æœ‰æ•ˆæ½œåŠ›ã€‚</li>
<li>æœ¬æ–‡æå‡ºç»“åˆæ·±åº¦æç«¯ç‚¹è¿½è¸ªï¼ˆDEPTï¼‰å’Œç‰¹å¾å¼•å¯¼æç«¯ç‚¹æ©æ¨¡ï¼ˆFGEPMï¼‰ç®—æ³•çš„è¶…å£°å›¾åƒåˆ†å‰²æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡è¯†åˆ«åŸºäºç‰¹å¾æ˜ å°„æˆæœ¬çŸ©é˜µä¸Šæ‰€æœ‰æç«¯ç‚¹çš„æœ€ä½æˆæœ¬è·¯å¾„æ¥ç”Ÿæˆä¼ªæ ‡ç­¾ã€‚</li>
<li>é‡‡ç”¨è¿­ä»£è®­ç»ƒç­–ç•¥é€æ­¥ä¼˜åŒ–ä¼ªæ ‡ç­¾ï¼Œæœ‰åŠ©äºæé«˜ç½‘ç»œæ€§èƒ½ã€‚</li>
<li>åœ¨å…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§æ¥è¿‘å…¨ç›‘ç£æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15260">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dc3b2d9754cc535c039b8bb8558db330.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b34342f1592710054ed7f5c79a39ba6d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-89a4fea9f53a434652ad550a1ea32768.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3520a9c92094fa0c08a265027f641331.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-509c13d2541a2fbf376fea0541f654c4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d0ddf98a8439d4da479a27db8d8b5044.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-028196cfa132832ce1152de8f167c933.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Exploring-Large-Language-Models-for-Word-Games-Who-is-the-Spy"><a href="#Exploring-Large-Language-Models-for-Word-Games-Who-is-the-Spy" class="headerlink" title="Exploring Large Language Models for Word Games:Who is the Spy?"></a>Exploring Large Language Models for Word Games:Who is the Spy?</h2><p><strong>Authors:Chentian Wei, Jiewei Chen, Jinzhu Xu</strong></p>
<p>Word games hold significant research value for natural language processing (NLP), game theory, and related fields due to their rule-based and situational nature. This study explores how large language models (LLMs) can be effectively involved in word games and proposes a training-free framework. â€œShei Shi Wo Diâ€ or â€œWho is the Spyâ€ in English, is a classic word game. Using this game as an example, we introduce a Chain-of-Thought (CoT)-based scheduling framework to enable LLMs to achieve excellent performance in tasks such as inferring role words and disguising their identities. We evaluate the frameworkâ€™s performance based on game success rates and the accuracy of the LLM agentsâ€™ analytical results. Experimental results affirm the frameworkâ€™s effectiveness, demonstrating notable improvements in LLM performance across multiple datasets. This work highlights the potential of LLMs in mastering situational reasoning and social interactions within structured game environments. Our code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/ct-wei/Who-is-The-Spy">https://github.com/ct-wei/Who-is-The-Spy</a>. </p>
<blockquote>
<p>æ–‡å­—æ¸¸æˆå› å…¶åŸºäºè§„åˆ™å’Œæƒ…å¢ƒçš„ç‰¹æ€§ï¼Œå¯¹è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ã€åšå¼ˆè®ºå’Œç›¸å…³é¢†åŸŸå…·æœ‰é‡è¦çš„ç ”ç©¶ä»·å€¼ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¦‚ä½•æœ‰æ•ˆåœ°å‚ä¸æ–‡å­—æ¸¸æˆï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªæ— éœ€è®­ç»ƒæ¡†æ¶ã€‚ â€œè°æ˜¯é—´è°â€ï¼ˆShei Shi Wo Diï¼‰æ˜¯è‹±æ–‡æ–‡å­—æ¸¸æˆâ€œè°æ˜¯å§åº•â€çš„ä¸€ç§ç»å…¸ç©æ³•ã€‚ä»¥è¿™æ¬¾æ¸¸æˆä¸ºä¾‹ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºæ€ç»´é“¾ï¼ˆChain-of-Thoughtï¼ŒCoTï¼‰çš„è°ƒåº¦æ¡†æ¶ï¼Œä½¿LLMèƒ½å¤Ÿåœ¨æ¨æ–­è§’è‰²è¯å’Œä¼ªè£…èº«ä»½ç­‰ä»»åŠ¡ä¸­å–å¾—å“è¶Šè¡¨ç°ã€‚æˆ‘ä»¬æ ¹æ®æ¸¸æˆæˆåŠŸç‡å’ŒLLMä»£ç†äººçš„åˆ†æç»“æœå‡†ç¡®æ€§æ¥è¯„ä¼°æ¡†æ¶çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¯å®äº†æ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œå¹¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šæ˜¾ç¤ºå‡ºLLMæ€§èƒ½çš„æ˜¾è‘—æ”¹è¿›ã€‚è¿™é¡¹å·¥ä½œçªå‡ºäº†LLMåœ¨æŒæ¡ç»“æ„åŒ–æ¸¸æˆç¯å¢ƒä¸­çš„æƒ…å¢ƒæ¨ç†å’Œç¤¾ä¼šäº¤äº’æ–¹é¢çš„æ½œåŠ›ã€‚æˆ‘ä»¬çš„ä»£ç å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/ct-wei/Who-is-The-Spy%E3%80%82">https://github.com/ct-wei/Who-is-The-Spyã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15235v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å­—è°œæ¸¸æˆâ€œè°æ˜¯æˆ‘é—´è°â€ï¼ˆShei Shi Wo Diï¼‰ä¸­çš„åº”ç”¨ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ€ç»´é“¾ï¼ˆChain-of-Thoughtï¼ŒCoTï¼‰çš„è°ƒåº¦æ¡†æ¶ï¼Œä»¥æé«˜LLMsåœ¨è§’è‰²è¯æ¨æ–­å’Œèº«ä»½ä¼ªè£…ç­‰ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶èƒ½æœ‰æ•ˆæé«˜LLMsåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°ã€‚æœ¬æ–‡å¼ºè°ƒäº†LLMsåœ¨æŒæ¡ç»“æ„åŒ–æ¸¸æˆç¯å¢ƒä¸­çš„æƒ…å¢ƒæ¨ç†å’Œç¤¾ä¼šäº¤äº’æ–¹é¢çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å­—è°œæ¸¸æˆä¸­çš„ç ”ç©¶å…·æœ‰é‡è¦ä»·å€¼ã€‚</li>
<li>ä»¥â€œè°æ˜¯æˆ‘é—´è°â€ä¸ºä¾‹ï¼Œå±•ç¤ºäº†LLMsåœ¨è§’è‰²è¯æ¨æ–­å’Œèº«ä»½ä¼ªè£…ä»»åŠ¡ä¸­çš„æ€§èƒ½æå‡ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºæ€ç»´é“¾ï¼ˆChain-of-Thoughtï¼ŒCoTï¼‰çš„è°ƒåº¦æ¡†æ¶ï¼Œä»¥æé«˜LLMsåœ¨æ¸¸æˆä¸­çš„è¡¨ç°ã€‚</li>
<li>é€šè¿‡å®éªŒéªŒè¯äº†æ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œå¹¶å±•ç¤ºäº†å…¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„æ˜¾è‘—æ”¹è¿›ã€‚</li>
<li>è¯¥æ¡†æ¶å¯¹æå‡LLMsçš„æƒ…å¢ƒæ¨ç†å’Œç¤¾ä¼šäº¤äº’èƒ½åŠ›æœ‰æ½œåŠ›ã€‚</li>
<li>è¯¥ç ”ç©¶çš„ä»£ç å·²å…¬å¼€ï¼Œä¾›å…¬ä¼—å‚è€ƒä¸å­¦ä¹ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15235">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-568844cd95ab840b742f689409202abc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8bd9ba651de03eb8ffac3d9d5e80b649.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a30d32091f1b8e54625f2d59d773a7e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de845e2089067ad19bb6271f6b7ba752.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="3D-Occupancy-Prediction-with-Low-Resolution-Queries-via-Prototype-aware-View-Transformation"><a href="#3D-Occupancy-Prediction-with-Low-Resolution-Queries-via-Prototype-aware-View-Transformation" class="headerlink" title="3D Occupancy Prediction with Low-Resolution Queries via Prototype-aware   View Transformation"></a>3D Occupancy Prediction with Low-Resolution Queries via Prototype-aware   View Transformation</h2><p><strong>Authors:Gyeongrok Oh, Sungjune Kim, Heeju Ko, Hyung-gun Chi, Jinkyu Kim, Dongwook Lee, Daehyun Ji, Sungjoon Choi, Sujin Jang, Sangpil Kim</strong></p>
<p>The resolution of voxel queries significantly influences the quality of view transformation in camera-based 3D occupancy prediction. However, computational constraints and the practical necessity for real-time deployment require smaller query resolutions, which inevitably leads to an information loss. Therefore, it is essential to encode and preserve rich visual details within limited query sizes while ensuring a comprehensive representation of 3D occupancy. To this end, we introduce ProtoOcc, a novel occupancy network that leverages prototypes of clustered image segments in view transformation to enhance low-resolution context. In particular, the mapping of 2D prototypes onto 3D voxel queries encodes high-level visual geometries and complements the loss of spatial information from reduced query resolutions. Additionally, we design a multi-perspective decoding strategy to efficiently disentangle the densely compressed visual cues into a high-dimensional 3D occupancy scene. Experimental results on both Occ3D and SemanticKITTI benchmarks demonstrate the effectiveness of the proposed method, showing clear improvements over the baselines. More importantly, ProtoOcc achieves competitive performance against the baselines even with 75% reduced voxel resolution. </p>
<blockquote>
<p>åŸºäºç›¸æœºçš„ä¸‰ç»´å ç”¨é¢„æµ‹ä¸­ï¼Œä½“ç´ æŸ¥è¯¢çš„åˆ†è¾¨ç‡å¯¹è§†å›¾è½¬æ¢çš„è´¨é‡æœ‰é‡è¦å½±å“ã€‚ç„¶è€Œï¼Œè®¡ç®—çº¦æŸå’Œå®æ—¶éƒ¨ç½²çš„å®é™…éœ€æ±‚è¦æ±‚è¾ƒå°çš„æŸ¥è¯¢åˆ†è¾¨ç‡ï¼Œè¿™ä¸å¯é¿å…åœ°å¯¼è‡´ä¿¡æ¯ä¸¢å¤±ã€‚å› æ­¤ï¼Œåœ¨æœ‰é™çš„æŸ¥è¯¢å¤§å°å†…ç¼–ç å’Œä¿ç•™ä¸°å¯Œçš„è§†è§‰ç»†èŠ‚è‡³å…³é‡è¦ï¼ŒåŒæ—¶è¿˜è¦ç¡®ä¿ä¸‰ç»´å ç”¨çš„å…¨é¢è¡¨ç¤ºã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ProtoOccï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹å ç”¨ç½‘ç»œï¼Œå®ƒåˆ©ç”¨èšç±»å›¾åƒæ®µçš„åŸå‹è¿›è¡Œè§†å›¾è½¬æ¢ï¼Œä»¥å¢å¼ºä½åˆ†è¾¨ç‡ä¸Šä¸‹æ–‡ã€‚ç‰¹åˆ«æ˜¯ï¼Œå°†äºŒç»´åŸå‹æ˜ å°„åˆ°ä¸‰ç»´ä½“ç´ æŸ¥è¯¢ä¸Šï¼Œå¯ä»¥ç¼–ç é«˜çº§è§†è§‰å‡ ä½•ç‰¹å¾ï¼Œå¹¶è¡¥å……å› é™ä½æŸ¥è¯¢åˆ†è¾¨ç‡è€ŒæŸå¤±çš„ç©ºé—´ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸€ç§å¤šè§†è§’è§£ç ç­–ç•¥ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°å°†å¯†é›†å‹ç¼©çš„è§†è§‰çº¿ç´¢è§£çº ç¼ ä¸ºé«˜ç»´çš„ä¸‰ç»´å ç”¨åœºæ™¯ã€‚åœ¨Occ3Då’ŒSemanticKITTIåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œä¸åŸºçº¿ç›¸æ¯”æœ‰æ˜æ˜¾çš„æ”¹è¿›ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œå³ä½¿åœ¨ä½“ç´ åˆ†è¾¨ç‡é™ä½75%çš„æƒ…å†µä¸‹ï¼ŒProtoOccä»èƒ½è¾¾åˆ°ä¸åŸºçº¿ç«äº‰çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15185v1">PDF</a> Accepted to CVPR2025</p>
<p><strong>Summary</strong><br>     é«˜åˆ†è¾¨ç‡çš„ä½“ç´ æŸ¥è¯¢å¯¹åŸºäºç›¸æœºçš„ä¸‰ç»´å ç”¨é¢„æµ‹ä¸­çš„è§†å›¾è½¬æ¢è´¨é‡æœ‰é‡è¦å½±å“ã€‚ç”±äºè®¡ç®—çº¦æŸå’Œå®æ—¶éƒ¨ç½²çš„å®é™…éœ€æ±‚ï¼Œé€šå¸¸éœ€è¦è¾ƒå°çš„æŸ¥è¯¢åˆ†è¾¨ç‡ï¼Œè¿™ä¼šå¯¼è‡´ä¿¡æ¯æŸå¤±ã€‚å› æ­¤ï¼ŒProtoOccä½œä¸ºä¸€ç§æ–°å‹å ç”¨ç½‘ç»œï¼Œé€šè¿‡åˆ©ç”¨èšç±»å›¾åƒæ®µçš„åŸå‹è¿›è¡Œè§†å›¾è½¬æ¢ï¼Œä»¥åœ¨ä½åˆ†è¾¨ç‡ä¸Šä¸‹æ–‡ä¸­å¢å¼ºä¿¡æ¯ã€‚å…·ä½“è€Œè¨€ï¼Œå°†äºŒç»´åŸå‹æ˜ å°„åˆ°ä¸‰ç»´ä½“ç´ æŸ¥è¯¢ä¸­ï¼Œç¼–ç é«˜çº§è§†è§‰å‡ ä½•ä¿¡æ¯ï¼Œå¹¶è¡¥å……å› æŸ¥è¯¢åˆ†è¾¨ç‡é™ä½è€ŒæŸå¤±çš„ç©ºé—´ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œè®¾è®¡äº†ä¸€ç§å¤šè§†è§’è§£ç ç­–ç•¥ï¼Œä»¥æœ‰æ•ˆåœ°å°†å¯†é›†å‹ç¼©çš„è§†è§‰çº¿ç´¢è§£çº ç¼ ä¸ºé«˜ç»´åº¦çš„ä¸‰ç»´å ç”¨åœºæ™¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨Occ3Då’ŒSemanticKITTIåŸºå‡†æµ‹è¯•ä¸Šå‡è¡¨ç°å‡ºæœ‰æ•ˆæ€§ï¼Œå³ä½¿åœ¨ä½“ç´ åˆ†è¾¨ç‡é™ä½75%çš„æƒ…å†µä¸‹ï¼ŒProtoOccä»å®ç°ä¸åŸºçº¿æ–¹æ³•ç›¸å½“çš„ç«äº‰åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>åˆ†è¾¨ç‡å¯¹è§†å›¾è½¬æ¢è´¨é‡æœ‰å½±å“ã€‚</li>
<li>å°æŸ¥è¯¢åˆ†è¾¨ç‡å¯¼è‡´ä¿¡æ¯æŸå¤±ã€‚</li>
<li>ProtoOccåˆ©ç”¨å›¾åƒæ®µåŸå‹ä»¥å¢å¼ºä½åˆ†è¾¨ç‡ä¸Šä¸‹æ–‡ä¸­çš„ä¿¡æ¯ã€‚</li>
<li>äºŒç»´åŸå‹æ˜ å°„åˆ°ä¸‰ç»´ä½“ç´ æŸ¥è¯¢ä»¥ç¼–ç é«˜çº§è§†è§‰å‡ ä½•ä¿¡æ¯ã€‚</li>
<li>å¤šè§†è§’è§£ç ç­–ç•¥å¯æœ‰æ•ˆè§£çº ç¼ è§†è§‰çº¿ç´¢ä»¥å‘ˆç°ä¸‰ç»´å ç”¨åœºæ™¯ã€‚</li>
<li>ProtoOccåœ¨åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¹¶åœ¨ä½åˆ†è¾¨ç‡ä¸‹ä»å…·æœ‰ç«äº‰åŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15185">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e1c407cc3e69f35fc0c0d75907527377.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bc702290723867505a028c53ffe88668.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d90d4259be9b100c5995510f6fa07edc.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Texture-Aware-StarGAN-for-CT-data-harmonisation"><a href="#Texture-Aware-StarGAN-for-CT-data-harmonisation" class="headerlink" title="Texture-Aware StarGAN for CT data harmonisation"></a>Texture-Aware StarGAN for CT data harmonisation</h2><p><strong>Authors:Francesco Di Feola, Ludovica Pompilio, Cecilia Assolito, Valerio Guarrasi, Paolo Soda</strong></p>
<p>Computed Tomography (CT) plays a pivotal role in medical diagnosis; however, variability across reconstruction kernels hinders data-driven approaches, such as deep learning models, from achieving reliable and generalized performance. To this end, CT data harmonization has emerged as a promising solution to minimize such non-biological variances by standardizing data across different sources or conditions. In this context, Generative Adversarial Networks (GANs) have proved to be a powerful framework for harmonization, framing it as a style-transfer problem. However, GAN-based approaches still face limitations in capturing complex relationships within the images, which are essential for effective harmonization. In this work, we propose a novel texture-aware StarGAN for CT data harmonization, enabling one-to-many translations across different reconstruction kernels. Although the StarGAN model has been successfully applied in other domains, its potential for CT data harmonization remains unexplored. Furthermore, our approach introduces a multi-scale texture loss function that embeds texture information across different spatial and angular scales into the harmonization process, effectively addressing kernel-induced texture variations. We conducted extensive experimentation on a publicly available dataset, utilizing a total of 48667 chest CT slices from 197 patients distributed over three different reconstruction kernels, demonstrating the superiority of our method over the baseline StarGAN. </p>
<blockquote>
<p>è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰åœ¨åŒ»å­¦è¯Šæ–­ä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼›ç„¶è€Œï¼Œé‡å»ºæ ¸çš„å·®å¼‚æ€§é˜»ç¢äº†æ•°æ®é©±åŠ¨çš„æ–¹æ³•ï¼ˆå¦‚æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼‰å®ç°å¯é å’Œé€šç”¨çš„æ€§èƒ½ã€‚ä¸ºæ­¤ï¼ŒCTæ•°æ®è°ƒå’Œå·²æˆä¸ºä¸€ç§æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡æ ‡å‡†åŒ–ä¸åŒæºæˆ–æ¡ä»¶ä¸‹çš„æ•°æ®ï¼Œä»¥å°½é‡å‡å°‘è¿™ç§éç”Ÿç‰©æ€§å·®å¼‚ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å·²è¢«è¯æ˜æ˜¯è°ƒå’Œçš„æœ‰åŠ›æ¡†æ¶ï¼Œå°†å…¶æ„é€ æˆé£æ ¼è½¬æ¢é—®é¢˜ã€‚ç„¶è€Œï¼ŒåŸºäºGANçš„æ–¹æ³•åœ¨æ•è·å›¾åƒå†…çš„å¤æ‚å…³ç³»æ–¹é¢ä»ç„¶å­˜åœ¨å±€é™æ€§ï¼Œè¿™å¯¹äºæœ‰æ•ˆçš„è°ƒå’Œè‡³å…³é‡è¦ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºCTæ•°æ®è°ƒå’Œçš„æ–°å‹çº¹ç†æ„ŸçŸ¥StarGANï¼Œèƒ½å¤Ÿå®ç°ä¸åŒé‡å»ºæ ¸ä¹‹é—´çš„ä¸€å¯¹å¤šç¿»è¯‘ã€‚è™½ç„¶StarGANæ¨¡å‹å·²åœ¨å…¶ä»–é¢†åŸŸæˆåŠŸåº”ç”¨ï¼Œä½†å…¶å¯¹CTæ•°æ®è°ƒå’Œçš„æ½œåŠ›ä»æœªè¢«æ¢ç´¢ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸€ç§å¤šå°ºåº¦çº¹ç†æŸå¤±å‡½æ•°ï¼Œè¯¥å‡½æ•°å°†ä¸åŒç©ºé—´å’Œè§’åº¦å°ºåº¦ä¸Šçš„çº¹ç†ä¿¡æ¯åµŒå…¥åˆ°è°ƒå’Œè¿‡ç¨‹ä¸­ï¼Œæœ‰æ•ˆåœ°è§£å†³äº†æ ¸å¼•èµ·çš„çº¹ç†å˜åŒ–ã€‚æˆ‘ä»¬åœ¨å…¬å¼€æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒï¼Œä½¿ç”¨äº†æ¥è‡ª197åæ‚£è€…çš„æ€»å…±48667å¼ èƒ¸éƒ¨CTåˆ‡ç‰‡ï¼Œè¿™äº›åˆ‡ç‰‡åˆ†å¸ƒåœ¨ä¸‰ç§ä¸åŒçš„é‡å»ºæ ¸ä¸Šï¼Œè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºåŸºçº¿StarGANã€‚</p>
</blockquote>
<p><strong>ç®€åŒ–è§£é‡Š</strong></p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15058v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æ¢è®¨äº†è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰åœ¨åŒ»å­¦è¯Šæ–­ä¸­çš„é‡è¦ä½œç”¨ä»¥åŠé‡å»ºæ ¸å·®å¼‚å¯¹æ•°æ®é©±åŠ¨æ–¹æ³•ï¼ˆå¦‚æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼‰çš„å½±å“ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰çš„çº¹ç†æ„ŸçŸ¥StarGANæ¨¡å‹ï¼Œç”¨äºCTæ•°æ®å’Œè°åŒ–ï¼Œèƒ½å¤Ÿè·¨ä¸åŒé‡å»ºæ ¸è¿›è¡Œä¸€å¯¹ä¸€è‡³å¤šç¿»è¯‘ã€‚é€šè¿‡å¤šå°ºåº¦çº¹ç†æŸå¤±å‡½æ•°åµŒå…¥ä¸åŒç©ºé—´å’Œè§’åº¦å°ºåº¦çš„çº¹ç†ä¿¡æ¯åˆ°å’Œè°åŒ–è¿‡ç¨‹ä¸­ï¼Œè§£å†³äº†æ ¸å¼•èµ·çš„çº¹ç†å˜åŒ–é—®é¢˜ã€‚å®éªŒåœ¨å…¬å¼€æ•°æ®é›†ä¸Šè¿›è¡Œï¼Œåˆ©ç”¨æ¥è‡ª197åæ‚£è€…çš„48667å¼ èƒ¸éƒ¨CTåˆ‡ç‰‡ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•ä¼˜äºåŸºçº¿StarGANã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CTåœ¨åŒ»å­¦è¯Šæ–­ä¸­å…·æœ‰é‡è¦ä½œç”¨ï¼Œä½†ä¸åŒé‡å»ºæ ¸ä¹‹é—´çš„å·®å¼‚å½±å“äº†æ•°æ®é©±åŠ¨æ–¹æ³•çš„æ€§èƒ½ã€‚</li>
<li>æ•°æ®å’Œè°åŒ–æ˜¯å‡å°‘éç”Ÿç‰©å˜å¼‚çš„ä¸€ç§æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>GANså·²è¢«è¯æ˜æ˜¯ç”¨äºæ•°æ®å’Œè°åŒ–çš„å¼ºå¤§æ¡†æ¶ï¼Œä½†å­˜åœ¨æ•æ‰å›¾åƒå¤æ‚å…³ç³»çš„å±€é™æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„çº¹ç†æ„ŸçŸ¥StarGANæ¨¡å‹ç”¨äºCTæ•°æ®å’Œè°åŒ–ï¼Œèƒ½å¤Ÿè¿›è¡Œä¸€å¯¹ä¸€è‡³å¤šçš„ç¿»è¯‘ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥å¤šå°ºåº¦çº¹ç†æŸå¤±å‡½æ•°è§£å†³äº†æ ¸å¼•èµ·çš„çº¹ç†å˜åŒ–é—®é¢˜ã€‚</li>
<li>å®éªŒåœ¨å…¬å¼€æ•°æ®é›†ä¸Šè¿›è¡Œï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ä¼˜äºåŸºçº¿StarGANã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15058">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6ed0807139549de51e49442834f97092.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a47fa1ae297c34f5f42a6cd9d49a7a3f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8687e9abb286ce80866f401743308a8a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-897badc3e9c144ff95b246205d49fbb8.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Ultrasound-Image-to-Video-Synthesis-via-Latent-Dynamic-Diffusion-Models"><a href="#Ultrasound-Image-to-Video-Synthesis-via-Latent-Dynamic-Diffusion-Models" class="headerlink" title="Ultrasound Image-to-Video Synthesis via Latent Dynamic Diffusion Models"></a>Ultrasound Image-to-Video Synthesis via Latent Dynamic Diffusion Models</h2><p><strong>Authors:Tingxiu Chen, Yilei Shi, Zixuan Zheng, Bingcong Yan, Jingliang Hu, Xiao Xiang Zhu, Lichao Mou</strong></p>
<p>Ultrasound video classification enables automated diagnosis and has emerged as an important research area. However, publicly available ultrasound video datasets remain scarce, hindering progress in developing effective video classification models. We propose addressing this shortage by synthesizing plausible ultrasound videos from readily available, abundant ultrasound images. To this end, we introduce a latent dynamic diffusion model (LDDM) to efficiently translate static images to dynamic sequences with realistic video characteristics. We demonstrate strong quantitative results and visually appealing synthesized videos on the BUSV benchmark. Notably, training video classification models on combinations of real and LDDM-synthesized videos substantially improves performance over using real data alone, indicating our method successfully emulates dynamics critical for discrimination. Our image-to-video approach provides an effective data augmentation solution to advance ultrasound video analysis. Code is available at <a target="_blank" rel="noopener" href="https://github.com/MedAITech/U_I2V">https://github.com/MedAITech/U_I2V</a>. </p>
<blockquote>
<p>è¶…å£°è§†é¢‘åˆ†ç±»å¯å®ç°è‡ªåŠ¨è¯Šæ–­ï¼Œå·²æˆä¸ºé‡è¦ç ”ç©¶é¢†åŸŸã€‚ç„¶è€Œï¼Œå…¬å…±è¶…å£°è§†é¢‘æ•°æ®é›†ä»ç„¶åŒ®ä¹ï¼Œé˜»ç¢äº†æœ‰æ•ˆè§†é¢‘åˆ†ç±»æ¨¡å‹çš„å¼€å‘è¿›å±•ã€‚æˆ‘ä»¬æå‡ºé€šè¿‡åˆæˆåˆç†çš„è¶…å£°è§†é¢‘æ¥è§£å†³è¿™ä¸€çŸ­ç¼ºé—®é¢˜ï¼Œè¿™äº›è§†é¢‘å¯ä»ä¸°å¯Œä¸”æ˜“è·å¾—çš„è¶…å£°å›¾åƒä¸­ç”Ÿæˆã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥æ½œåœ¨åŠ¨æ€æ‰©æ•£æ¨¡å‹ï¼ˆLDDMï¼‰ï¼Œä»¥æœ‰æ•ˆå°†é™æ€å›¾åƒè½¬æ¢ä¸ºå…·æœ‰çœŸå®è§†é¢‘ç‰¹æ€§çš„åŠ¨æ€åºåˆ—ã€‚æˆ‘ä»¬åœ¨BUSVåŸºå‡†æµ‹è¯•ä¸Šå±•ç¤ºäº†å¼ºå¤§çš„å®šé‡ç»“æœå’Œè§†è§‰ä¸Šå¸å¼•äººçš„åˆæˆè§†é¢‘ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä½¿ç”¨çœŸå®å’ŒLDDMåˆæˆè§†é¢‘ç›¸ç»“åˆçš„æ•°æ®é›†è®­ç»ƒè§†é¢‘åˆ†ç±»æ¨¡å‹ï¼Œå…¶æ€§èƒ½å¤§å¤§è¶…è¿‡äº†ä»…ä½¿ç”¨çœŸå®æ•°æ®çš„æ•ˆæœï¼Œè¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•æˆåŠŸæ¨¡æ‹Ÿäº†å…³é”®é‰´åˆ«åŠ¨æ€ã€‚æˆ‘ä»¬çš„å›¾åƒåˆ°è§†é¢‘çš„æ–¹æ³•ä¸ºæ¨è¿›è¶…å£°è§†é¢‘åˆ†ææä¾›äº†æœ‰æ•ˆçš„æ•°æ®å¢å¼ºè§£å†³æ–¹æ¡ˆã€‚ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/MedAITech/U_I2V%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/MedAITech/U_I2Vè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14966v1">PDF</a> MICCAI 2024</p>
<p><strong>Summary</strong><br>è¶…å£°è§†é¢‘åˆ†ç±»å¯¹äºè‡ªåŠ¨åŒ–è¯Šæ–­è‡³å…³é‡è¦ï¼Œä½†å¯ç”¨çš„è¶…å£°è§†é¢‘æ•°æ®é›†ç¨€ç¼ºï¼Œåˆ¶çº¦äº†è§†é¢‘åˆ†ç±»æ¨¡å‹çš„å‘å±•ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºé€šè¿‡ä¸°å¯Œçš„è¶…å£°å›¾åƒåˆæˆè¶…å£°è§†é¢‘æ¥å¼¥è¡¥è¿™ä¸€ä¸è¶³ã€‚æˆ‘ä»¬å¼•å…¥æ½œåœ¨åŠ¨æ€æ‰©æ•£æ¨¡å‹ï¼ˆLDDMï¼‰ï¼ŒæˆåŠŸå°†é™æ€å›¾åƒè½¬åŒ–ä¸ºå…·æœ‰çœŸå®è§†é¢‘ç‰¹æ€§çš„åŠ¨æ€åºåˆ—ã€‚åœ¨BUSVåŸºå‡†æµ‹è¯•ä¸Šï¼Œåˆæˆçš„è§†é¢‘æ—¢åœ¨æ•°é‡ä¸Šä¸°å¯Œï¼Œè§†è§‰ä¸Šä¹Ÿå¾ˆé€¼çœŸã€‚ä½¿ç”¨çœŸå®å’Œåˆæˆè§†é¢‘ç»„åˆè®­ç»ƒçš„è§†é¢‘åˆ†ç±»æ¨¡å‹æ€§èƒ½ä¼˜äºä»…ä½¿ç”¨çœŸå®æ•°æ®ï¼Œè¯æ˜æˆ‘ä»¬çš„æ–¹æ³•æˆåŠŸæ¨¡æ‹Ÿäº†å…³é”®åŠ¨æ€é‰´åˆ«ç‰¹å¾ã€‚æˆ‘ä»¬çš„å›¾åƒåˆ°è§†é¢‘çš„è½¬æ¢æ–¹æ³•ä¸ºè¶…å£°è§†é¢‘åˆ†ææä¾›äº†æœ‰æ•ˆçš„æ•°æ®å¢å¼ºè§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¶…å£°è§†é¢‘åˆ†ç±»åœ¨è‡ªåŠ¨åŒ–è¯Šæ–­ä¸­å…·æœ‰é‡è¦æ„ä¹‰ï¼Œä½†ç¼ºä¹å¯ç”¨çš„è¶…å£°è§†é¢‘æ•°æ®é›†é™åˆ¶äº†è¿›å±•ã€‚</li>
<li>æå‡ºé€šè¿‡æ½œåœ¨åŠ¨æ€æ‰©æ•£æ¨¡å‹ï¼ˆLDDMï¼‰åˆæˆè¶…å£°è§†é¢‘ä»¥å¼¥è¡¥æ•°æ®é›†ä¸è¶³ã€‚</li>
<li>LDDMæ¨¡å‹æˆåŠŸå°†é™æ€è¶…å£°å›¾åƒè½¬åŒ–ä¸ºå…·æœ‰çœŸå®è§†é¢‘ç‰¹æ€§çš„åŠ¨æ€åºåˆ—ã€‚</li>
<li>åœ¨BUSVåŸºå‡†æµ‹è¯•ä¸Šï¼Œåˆæˆçš„è§†é¢‘æ—¢ä¸°å¯Œä¸”è§†è§‰æ•ˆæœå¥½ã€‚</li>
<li>ä½¿ç”¨çœŸå®å’Œåˆæˆè§†é¢‘ç»„åˆè®­ç»ƒçš„è§†é¢‘åˆ†ç±»æ¨¡å‹æ€§èƒ½æ›´ä½³ã€‚</li>
<li>è¯¥æ–¹æ³•æˆåŠŸæ¨¡æ‹Ÿäº†å…³é”®åŠ¨æ€é‰´åˆ«ç‰¹å¾ï¼Œè¯æ˜å…¶æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14966">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8e49b6aef94727d2e5c3afa2b31df8fa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c896922487736d835240b97ed3bc02fa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a1cc7baf7a6c1fe7bd6d904284b499aa.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Effect-of-substitution-La-by-Mg-on-electrochemical-and-electronic-properties-in-La-2-x-Mg-x-Ni-7-alloys-a-combined-experimental-and-ab-initio-studies"><a href="#Effect-of-substitution-La-by-Mg-on-electrochemical-and-electronic-properties-in-La-2-x-Mg-x-Ni-7-alloys-a-combined-experimental-and-ab-initio-studies" class="headerlink" title="Effect of substitution La by Mg on electrochemical and electronic   properties in La$_{2-x}$Mg$_x$Ni$_7$ alloys: a combined experimental and ab   initio studies"></a>Effect of substitution La by Mg on electrochemical and electronic   properties in La$_{2-x}$Mg$_x$Ni$_7$ alloys: a combined experimental and ab   initio studies</h2><p><strong>Authors:MirosÅ‚aw WerwiÅ„ski, Andrzej Szajek, Agnieszka MarczyÅ„ska, LesÅ‚aw Smardz, Marek Nowak, MieczysÅ‚aw Jurczyk</strong></p>
<p>La-Mg-Ni-based alloys are promising negative electrode materials for 3rd generation of Ni-MH$<em>x$ batteries. In this work, we investigate the effect of Mg substitution on the electrochemical and electronic properties of La$</em>{2-x}$Mg$_x$Ni$<em>7$ materials. The mechanical alloying technique is used to produce a series of La$</em>{2-x}$Mg$_x$Ni$_7$ alloys ($x$ &#x3D; 0.00, 0.25, 0.50 and 0.75). The X-ray diffraction measurements indicate multiphase character of the samples with majority (La,Mg)$_2$Ni$_7$ phases of hexagonal Ce$<em>2$Ni$<em>7$-type and rhombohedral Gd$<em>2$Co$<em>7$-type. Electrochemical measurements show how the maximum discharge capacity ($C</em>{max}$) increases with Mg concentration and that reach the highest value of 304 mAh&#x2F;g for La$</em>{1.5}$Mg$</em>{0.5}$Ni$<em>7$ ($x$ &#x3D; 0.5). The experimental efforts are followed by the density functional theory (DFT) calculations performed with the full-potential local-orbital minimum-basis scheme (FPLO). To simulate chemical disorder, we use the coherent potential approximation (CPA). The calculations are focused on the La$</em>{1.5}$Mg$</em>{0.5}$Ni$<em>7$ composition with the highest measured value of $C</em>{max}$. Additionally, several other structures are considered as reference points. We find that hexagonal and rhombohedral structures of La$_2$Ni$_7$ have almost identical total energies, which is in a good agreement with a coexistence of both phases in the samples. The calculated site preferences of Mg in both Ce$_2$Ni$<em>7$-type and Gd$<em>2$Co$<em>7$-type La$</em>{1.5}$Mg$</em>{0.5}$Ni$<em>7$ phases are consistent with the previous experimental data. Furthermore, the valence band of the nanocrystalline La$</em>{1.5}$Mg$</em>{0.5}$Ni$_7$ sample is investigated by X-ray photoelectron spectroscopy (XPS). The experimental XPS are interpreted based on the corresponding spectra calculated with DFT. </p>
<blockquote>
<p>La-Mg-NiåŸºåˆé‡‘ä½œä¸ºç¬¬ä¸‰ä»£Ni-MHç”µæ± çš„è´Ÿæææ–™å…·æœ‰å¹¿é˜”å‰æ™¯ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†Mgæ›¿ä»£å¯¹La_{2-x}Mg_xNi_7ææ–™ç”µåŒ–å­¦å’Œç”µå­æ€§èƒ½çš„å½±å“ã€‚é‡‡ç”¨æœºæ¢°åˆé‡‘åŒ–æŠ€æœ¯åˆ¶å¤‡äº†ä¸€ç³»åˆ—La_{2-x}Mg_xNi_7åˆé‡‘ï¼ˆx&#x3D;0.00ã€0.25ã€0.50å’Œ0.75ï¼‰ã€‚Xå°„çº¿è¡å°„æµ‹é‡è¡¨æ˜æ ·å“å…·æœ‰å¤šç›¸ç‰¹å¾ï¼Œä¸»è¦ä»¥Laã€Mgï¼‰<em>2Ni_7ç›¸ä¸ºä¸»ï¼Œå‘ˆå…­æ–¹Ce_2Ni_7å‹å’Œæ–œæ–¹Gd_2Co_7å‹ã€‚ç”µåŒ–å­¦æµ‹é‡è¡¨æ˜æœ€å¤§æ”¾ç”µå®¹é‡ï¼ˆC</em>{max}ï¼‰éšMgæµ“åº¦çš„å¢åŠ è€Œå¢åŠ ï¼Œå¹¶åœ¨La_{1.5}Mg_{0.5}Ni_7ï¼ˆx&#x3D;0.5ï¼‰æ—¶è¾¾åˆ°æœ€é«˜å€¼304mAh&#x2F;gã€‚æˆ‘ä»¬åœ¨å¯†åº¦æ³›å‡½ç†è®ºï¼ˆDFTï¼‰è®¡ç®—ä¸­è¿›è¡Œäº†å®éªŒéªŒè¯ï¼Œé‡‡ç”¨å…¨åŠ¿å±€åŸŸè½¨é“æœ€å°åŸºæ–¹æ¡ˆï¼ˆFPLOï¼‰ã€‚ä¸ºäº†æ¨¡æ‹ŸåŒ–å­¦æ— åºæ€§ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ç›¸å¹²åŠ¿è¿‘ä¼¼ï¼ˆCPAï¼‰ã€‚è®¡ç®—çš„é‡ç‚¹æ˜¯æœ€å¤§æ”¾ç”µå®¹é‡æœ€é«˜çš„La_{1.5}Mg_{0.5}Ni_7æˆåˆ†ã€‚æ­¤å¤–ï¼Œè¿˜è€ƒè™‘äº†å…¶ä»–ä¸€äº›ç»“æ„ä½œä¸ºå‚è€ƒç‚¹ã€‚æˆ‘ä»¬å‘ç°La_2Ni_7çš„å…­è¾¹å½¢å’Œæ–œæ–¹ç»“æ„å…·æœ‰å‡ ä¹ç›¸åŒçš„æ€»èƒ½é‡ï¼Œè¿™ä¸æ ·å“ä¸­ä¸¤ç§ç»“æ„å…±å­˜çš„æƒ…å†µç›¸ç¬¦ã€‚è®¡ç®—å¾—å‡ºçš„Ce_2Ni_7å‹å’ŒGd_2Co_7å‹La_{1.5}Mg_{0.5}Ni_7ç›¸ä¸­Mgçš„ä½ç‚¹åå¥½ä¸ä¹‹å‰çš„å®éªŒæ•°æ®ä¸€è‡´ã€‚æ­¤å¤–ï¼Œé€šè¿‡Xå°„çº¿å…‰ç”µå­å…‰è°±ä»ªï¼ˆXPSï¼‰ç ”ç©¶äº†çº³ç±³æ™¶La_{1.5}Mg_{0.5}Ni_7æ ·å“çš„ä»·å¸¦ã€‚æ ¹æ®DFTè®¡ç®—å¾—åˆ°çš„ç›¸åº”å…‰è°±æ¥è§£é‡Šå®éªŒXPSã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14952v1">PDF</a> </p>
<p><strong>Summary</strong><br>     è¯¥ç ”ç©¶æ¢è®¨äº†Mgæ›¿ä»£å¯¹La-Mg-NiåŸºåˆé‡‘çš„ç”µåŒ–å­¦å’Œç”µå­æ€§èƒ½çš„å½±å“ï¼Œå‘ç°éšç€Mgæµ“åº¦çš„å¢åŠ ï¼Œæœ€å¤§æ”¾ç”µå®¹é‡å¢å¤§ï¼Œå¹¶åœ¨La_{1.5}Mg_{0.5}Ni_7æ—¶è¾¾åˆ°æœ€å¤§å€¼ã€‚é€šè¿‡å¯†åº¦æ³›å‡½ç†è®ºè®¡ç®—å’ŒXå°„çº¿å…‰ç”µå­è°±ç ”ç©¶å…¶ç»“æ„ç‰¹æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>La-Mg-NiåŸºåˆé‡‘æ˜¯ç¬¬ä¸‰ä»£Ni-MHç”µæ± çš„æ½œåœ¨è´Ÿæææ–™ã€‚</li>
<li>Mgæ›¿ä»£å¯¹La_{2-x}Mg_{x}Ni_{7}ææ–™å…·æœ‰æ˜¾è‘—å½±å“ï¼Œæœ€å¤§æ”¾ç”µå®¹é‡éšMgæµ“åº¦å¢åŠ è€Œå¢å¤§ã€‚</li>
<li>La_{1.5}Mg_{0.5}Ni_{7}çš„æ”¾ç”µå®¹é‡è¾¾åˆ°æœ€é«˜å€¼304mAh&#x2F;gã€‚</li>
<li>Xå°„çº¿è¡å°„ç»“æœè¡¨æ˜æ ·å“å‘ˆç°å¤šç›¸ç‰¹å¾ï¼Œä»¥Ce_{2}Ni_{7}-å‹ä¸ºä¸»ç›¸ã€‚</li>
<li>å¯†åº¦æ³›å‡½ç†è®ºè®¡ç®—ç”¨äºæ¨¡æ‹ŸåŒ–å­¦æ— åºçŠ¶æ€ï¼Œå¹¶ç ”ç©¶La_{1.5}Mg_{0.5}Ni_{7}ç»“æ„ç‰¹æ€§ã€‚</li>
<li>XPSå®éªŒéªŒè¯äº†ææ–™çš„ç”µå­ç»“æ„ç‰¹å¾ï¼Œä¸DFTè®¡ç®—ç»“æœä¸€è‡´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14952">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-15bb0e8e98969d8587eed7bb04709a56.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0ecd0ddb42ec9d31990df0980e9dd9ab.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4826f389305e10e2a4c24cfd50651edc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5781acfe6dda54b8af7d47449ba02770.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b8fdda327ebaf67e8d249e85b9fdcf62.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b36c139610fcd4cb5d32957d0ab030e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f174e23a1219f280dd7e78400996b63b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-61f9e2a4373a9df3b4f0a853c4c4eaf9.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="A-Simple-Combination-of-Diffusion-Models-for-Better-Quality-Trade-Offs-in-Image-Denoising"><a href="#A-Simple-Combination-of-Diffusion-Models-for-Better-Quality-Trade-Offs-in-Image-Denoising" class="headerlink" title="A Simple Combination of Diffusion Models for Better Quality Trade-Offs   in Image Denoising"></a>A Simple Combination of Diffusion Models for Better Quality Trade-Offs   in Image Denoising</h2><p><strong>Authors:Jonas Dornbusch, Emanuel Pfarr, Florin-Alexandru Vasluianu, Frank Werner, Radu Timofte</strong></p>
<p>Diffusion models have garnered considerable interest in computer vision, owing both to their capacity to synthesize photorealistic images and to their proven effectiveness in image reconstruction tasks. However, existing approaches fail to efficiently balance the high visual quality of diffusion models with the low distortion achieved by previous image reconstruction methods. Specifically, for the fundamental task of additive Gaussian noise removal, we first illustrate an intuitive method for leveraging pretrained diffusion models. Further, we introduce our proposed Linear Combination Diffusion Denoiser (LCDD), which unifies two complementary inference procedures - one that leverages the modelâ€™s generative potential and another that ensures faithful signal recovery. By exploiting the inherent structure of the denoising samples, LCDD achieves state-of-the-art performance and offers controlled, well-behaved trade-offs through a simple scalar hyperparameter adjustment. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸå¼•èµ·äº†å¹¿æ³›å…³æ³¨ï¼Œå› ä¸ºå®ƒä»¬æ—¢èƒ½åˆæˆé€¼çœŸçš„å›¾åƒï¼Œåœ¨å›¾åƒé‡å»ºä»»åŠ¡ä¸­ä¹Ÿè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•æ— æ³•åœ¨é«˜è§†è§‰è´¨é‡çš„æ‰©æ•£æ¨¡å‹ä¸å…ˆå‰å›¾åƒé‡å»ºæ–¹æ³•å®ç°çš„ä½å¤±çœŸä¹‹é—´è¿›è¡Œæœ‰æ•ˆå¹³è¡¡ã€‚é’ˆå¯¹å»é™¤æ·»åŠ æ€§é«˜æ–¯å™ªå£°çš„åŸºæœ¬ä»»åŠ¡ï¼Œæˆ‘ä»¬é¦–å…ˆæå‡ºäº†ä¸€ç§åˆ©ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„ç›´è§‚æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ‰€æå‡ºçš„çº¿æ€§ç»„åˆæ‰©æ•£å»å™ªå™¨ï¼ˆLCDDï¼‰ï¼Œå®ƒç»Ÿä¸€äº†ä¸¤ç§äº’è¡¥çš„æ¨ç†ç¨‹åºâ€”â€”ä¸€ç§åˆ©ç”¨æ¨¡å‹çš„ç”Ÿæˆæ½œåŠ›ï¼Œå¦ä¸€ç§ç¡®ä¿å¿ å®ä¿¡å·æ¢å¤ã€‚é€šè¿‡åˆ©ç”¨å»å™ªæ ·æœ¬çš„å›ºæœ‰ç»“æ„ï¼ŒLCDDå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶é€šè¿‡ç®€å•çš„æ ‡é‡è¶…å‚æ•°è°ƒæ•´æä¾›äº†å¯æ§ã€è¡¨ç°è‰¯å¥½çš„æƒè¡¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14654v1">PDF</a> 10 pages, 7 figures, 2 tables</p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸå› å…¶èƒ½åˆæˆé€¼çœŸçš„å›¾åƒå¹¶åœ¨å›¾åƒé‡å»ºä»»åŠ¡ä¸­è¯æ˜å…¶æœ‰æ•ˆæ€§è€Œå—åˆ°å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•æ— æ³•åœ¨æ‰©æ•£æ¨¡å‹çš„é«˜è§†è§‰è´¨é‡ä¸å…ˆå‰å›¾åƒé‡å»ºæ–¹æ³•å®ç°çš„ä½å¤±çœŸä¹‹é—´è¿›è¡Œæœ‰æ•ˆå¹³è¡¡ã€‚é’ˆå¯¹å»é™¤åŠ æ€§é«˜æ–¯å™ªå£°è¿™ä¸€åŸºæœ¬ä»»åŠ¡ï¼Œæœ¬æ–‡é¦–å…ˆå±•ç¤ºäº†ä¸€ç§åˆ©ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„ç›´è§‚æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºäº†çº¿æ€§ç»„åˆæ‰©æ•£å»å™ªå™¨ï¼ˆLCDDï¼‰ï¼Œå®ƒç»Ÿä¸€äº†ä¸¤ç§äº’è¡¥çš„æ¨ç†ç¨‹åºâ€”â€”ä¸€ç§åˆ©ç”¨æ¨¡å‹çš„ç”Ÿæˆæ½œåŠ›ï¼Œå¦ä¸€ç§ç¡®ä¿å¿ å®ä¿¡å·æ¢å¤ã€‚LCDDé€šè¿‡åˆ©ç”¨å»å™ªæ ·æœ¬çš„å›ºæœ‰ç»“æ„å®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼Œå¹¶é€šè¿‡ç®€å•çš„æ ‡é‡è¶…å‚æ•°è°ƒæ•´æä¾›äº†å¯æ§çš„ã€è¡¨ç°è‰¯å¥½çš„æƒè¡¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸå—åˆ°å…³æ³¨ï¼Œå› å…¶èƒ½åˆæˆé«˜è´¨é‡å›¾åƒå¹¶åœ¨å›¾åƒé‡å»ºä»»åŠ¡ä¸­è¡¨ç°å‡ºæœ‰æ•ˆæ€§ã€‚</li>
<li>ç°æœ‰æ–¹æ³•éš¾ä»¥å¹³è¡¡æ‰©æ•£æ¨¡å‹çš„é«˜è§†è§‰è´¨é‡ä¸ä½å¤±çœŸã€‚</li>
<li>é’ˆå¯¹å»é™¤åŠ æ€§é«˜æ–¯å™ªå£°ä»»åŠ¡ï¼Œæå‡ºäº†ä¸€ç§åˆ©ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„ç›´è§‚æ–¹æ³•ã€‚</li>
<li>å¼•å…¥çº¿æ€§ç»„åˆæ‰©æ•£å»å™ªå™¨ï¼ˆLCDDï¼‰ï¼Œç»Ÿä¸€ä¸¤ç§äº’è¡¥æ¨ç†ç¨‹åºï¼Œåˆ©ç”¨ç”Ÿæˆæ½œåŠ›å’Œç¡®ä¿å¿ å®ä¿¡å·æ¢å¤ã€‚</li>
<li>LCDDåˆ©ç”¨å»å™ªæ ·æœ¬çš„å›ºæœ‰ç»“æ„å®ç°å“è¶Šæ€§èƒ½ã€‚</li>
<li>LCDDé€šè¿‡ç®€å•çš„æ ‡é‡è¶…å‚æ•°è°ƒæ•´æä¾›å¯æ§çš„ã€è¡¨ç°è‰¯å¥½çš„æƒè¡¡ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡ç»Ÿä¸€æ¡†æ¶å®ç°äº†æ‰©æ•£æ¨¡å‹ä¸å›¾åƒé‡å»ºæ–¹æ³•çš„ä¼˜åŠ¿ç»“åˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14654">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4c2ea409c2b04e690604d25421327e3b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7188d13e2cc7b21da6e64e6d7bb71719.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93b44faf967acf903f4b924bd1f27a4d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b5143422e6155faefdf1ac8d888f8ee4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3b5d4c8620f17420552564c5994a2a2d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-280f0608a09b9cd1db2be5b3efc5ff40.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Interpretable-High-order-Knowledge-Graph-Neural-Network-for-Predicting-Synthetic-Lethality-in-Human-Cancers"><a href="#Interpretable-High-order-Knowledge-Graph-Neural-Network-for-Predicting-Synthetic-Lethality-in-Human-Cancers" class="headerlink" title="Interpretable High-order Knowledge Graph Neural Network for Predicting   Synthetic Lethality in Human Cancers"></a>Interpretable High-order Knowledge Graph Neural Network for Predicting   Synthetic Lethality in Human Cancers</h2><p><strong>Authors:Xuexin Chen, Ruichu Cai, Zhengting Huang, Zijian Li, Jie Zheng, Min Wu</strong></p>
<p>Synthetic lethality (SL) is a promising gene interaction for cancer therapy. Recent SL prediction methods integrate knowledge graphs (KGs) into graph neural networks (GNNs) and employ attention mechanisms to extract local subgraphs as explanations for target gene pairs. However, attention mechanisms often lack fidelity, typically generate a single explanation per gene pair, and fail to ensure trustworthy high-order structures in their explanations. To overcome these limitations, we propose Diverse Graph Information Bottleneck for Synthetic Lethality (DGIB4SL), a KG-based GNN that generates multiple faithful explanations for the same gene pair and effectively encodes high-order structures. Specifically, we introduce a novel DGIB objective, integrating a Determinant Point Process (DPP) constraint into the standard IB objective, and employ 13 motif-based adjacency matrices to capture high-order structures in gene representations. Experimental results show that DGIB4SL outperforms state-of-the-art baselines and provides multiple explanations for SL prediction, revealing diverse biological mechanisms underlying SL inference. </p>
<blockquote>
<p>åˆæˆè‡´æ­»æ€§ï¼ˆSynthetic Lethality, SLï¼‰æ˜¯ä¸€ç§å…·æœ‰æ½œåŠ›çš„ç™Œç—‡æ²»ç–—åŸºå› äº¤äº’ä½œç”¨ã€‚æœ€è¿‘çš„SLé¢„æµ‹æ–¹æ³•å°†çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰èå…¥å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰ï¼Œå¹¶åº”ç”¨æ³¨æ„åŠ›æœºåˆ¶æå–å±€éƒ¨å­å›¾ä½œä¸ºç›®æ ‡åŸºå› å¯¹çš„è§£é‡Šã€‚ç„¶è€Œï¼Œæ³¨æ„åŠ›æœºåˆ¶é€šå¸¸ç¼ºä¹å‡†ç¡®æ€§ï¼Œé€šå¸¸åªä¸ºæ¯ä¸ªåŸºå› å¯¹ç”Ÿæˆä¸€ä¸ªè§£é‡Šï¼Œå¹¶ä¸”æ— æ³•åœ¨è§£é‡Šä¸­ç¡®ä¿å¯é çš„é«˜é˜¶ç»“æ„ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºçŸ¥è¯†å›¾è°±çš„åˆæˆè‡´æ­»æ€§å¤šæ ·å›¾ä¿¡æ¯ç“¶é¢ˆï¼ˆDGIB4SLï¼‰æ–¹æ³•ã€‚è¯¥æ–¹æ³•ä¸ºåŒä¸€åŸºå› å¯¹ç”Ÿæˆå¤šä¸ªå¿ å®è§£é‡Šï¼Œå¹¶æœ‰æ•ˆåœ°ç¼–ç é«˜é˜¶ç»“æ„ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„DGIBç›®æ ‡ï¼Œå°†è¡Œåˆ—å¼ç‚¹è¿‡ç¨‹ï¼ˆDPPï¼‰çº¦æŸé›†æˆåˆ°æ ‡å‡†IBç›®æ ‡ä¸­ï¼Œå¹¶ä½¿ç”¨13ä¸ªåŸºäºåŸºåºçš„é‚»æ¥çŸ©é˜µæ¥æ•è·åŸºå› è¡¨ç¤ºä¸­çš„é«˜é˜¶ç»“æ„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDGIB4SLä¼˜äºæœ€æ–°åŸºçº¿æ–¹æ³•ï¼Œä¸ºSLé¢„æµ‹æä¾›äº†å¤šä¸ªè§£é‡Šï¼Œæ­ç¤ºäº†SLæ¨æ–­èƒŒåå¤šæ ·åŒ–çš„ç”Ÿç‰©å­¦æœºåˆ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06052v2">PDF</a> 15 pages. Accepted by Briefings in Bioinformatics</p>
<p><strong>Summary</strong><br>    åˆæˆè‡´æ­»æ€§ï¼ˆSLï¼‰æ˜¯ç™Œç—‡æ²»ç–—ä¸­æœ‰å‰æ™¯çš„åŸºå› ç›¸äº’ä½œç”¨ã€‚é’ˆå¯¹ç°æœ‰SLé¢„æµ‹æ–¹æ³•ä¸­çš„æ³¨æ„åŠ›æœºåˆ¶ç¼ºä¹ä¿çœŸåº¦ã€é€šå¸¸åªä¸ºåŸºå› å¯¹ç”Ÿæˆå•ä¸€è§£é‡Šä»¥åŠæ— æ³•ç¡®ä¿è§£é‡Šä¸­é«˜é˜¶ç»“æ„å¯ä¿¡åº¦çš„é—®é¢˜ï¼Œæå‡ºäº†åŸºäºçŸ¥è¯†å›¾è°±çš„å›¾ç¥ç»ç½‘ç»œï¼ˆDGIB4SLï¼‰ã€‚è¯¥æ–¹æ³•ä¸ºåŒä¸€åŸºå› å¯¹ç”Ÿæˆå¤šä¸ªå¿ å®è§£é‡Šï¼Œå¹¶æœ‰æ•ˆç¼–ç é«˜é˜¶ç»“æ„ã€‚é€šè¿‡å¼•å…¥æ–°çš„DGIBç›®æ ‡ï¼Œå°†è¡Œåˆ—å¼ç‚¹è¿‡ç¨‹ï¼ˆDPPï¼‰çº¦æŸé›†æˆåˆ°æ ‡å‡†IBç›®æ ‡ä¸­ï¼Œå¹¶ä½¿ç”¨13ä¸ªåŸºäºmotifçš„é‚»æ¥çŸ©é˜µæ•è·åŸºå› è¡¨ç¤ºä¸­çš„é«˜é˜¶ç»“æ„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDGIB4SLä¼˜äºæœ€æ–°åŸºçº¿ï¼Œä¸ºSLé¢„æµ‹æä¾›å¤šé‡è§£é‡Šï¼Œæ­ç¤ºSLæ¨æ–­èƒŒåçš„ä¸åŒç”Ÿç‰©å­¦æœºåˆ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆæˆè‡´æ­»æ€§ï¼ˆSLï¼‰æ˜¯ç™Œç—‡æ²»ç–—é¢†åŸŸçš„ä¸€ä¸ªé‡è¦åŸºå› äº¤äº’ç°è±¡ã€‚</li>
<li>ç°æœ‰SLé¢„æµ‹æ–¹æ³•é€šè¿‡æ•´åˆçŸ¥è¯†å›¾è°±å’Œå›¾ç¥ç»ç½‘ç»œè¿›è¡Œã€‚</li>
<li>æ³¨æ„åŠ›æœºåˆ¶åœ¨SLé¢„æµ‹ä¸­å¸¸ç¼ºä¹ä¿çœŸåº¦ï¼Œé€šå¸¸åªä¸ºåŸºå› å¯¹ç”Ÿæˆå•ä¸€è§£é‡Šã€‚</li>
<li>æå‡ºäº†DGIB4SLæ–¹æ³•ï¼Œæ—¨åœ¨ç”Ÿæˆå¤šä¸ªå¿ å®çš„è§£é‡Šå¹¶ä¸ºåŒä¸€åŸºå› å¯¹ç¼–ç é«˜é˜¶ç»“æ„ã€‚</li>
<li>DGIB4SLé€šè¿‡å¼•å…¥DGIBç›®æ ‡å’Œè¡Œåˆ—å¼ç‚¹è¿‡ç¨‹ï¼ˆDPPï¼‰çº¦æŸæ¥å®ç°è¿™ä¸€ç›®æ ‡ã€‚</li>
<li>DGIB4SLä½¿ç”¨åŸºäºmotifçš„é‚»æ¥çŸ©é˜µæ¥æ•è·åŸºå› è¡¨ç¤ºä¸­çš„é«˜é˜¶ç»“æ„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06052">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-430b460135811c7ede99b5a295a3a404.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7e09a962d6351d1d76d2df13687af0eb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-88c1a2d35388d5c366bd493c38f13a4f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-364c878f003c413ffc8e1e25fa3c35e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed57fd8ee6ba2c3dd428384b6f28081b.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Gaussian-Random-Fields-as-an-Abstract-Representation-of-Patient-Metadata-for-Multimodal-Medical-Image-Segmentation"><a href="#Gaussian-Random-Fields-as-an-Abstract-Representation-of-Patient-Metadata-for-Multimodal-Medical-Image-Segmentation" class="headerlink" title="Gaussian Random Fields as an Abstract Representation of Patient Metadata   for Multimodal Medical Image Segmentation"></a>Gaussian Random Fields as an Abstract Representation of Patient Metadata   for Multimodal Medical Image Segmentation</h2><p><strong>Authors:Bill Cassidy, Christian McBride, Connah Kendrick, Neil D. Reeves, Joseph M. Pappachan, Shaghayegh Raad, Moi Hoon Yap</strong></p>
<p>The growing rate of chronic wound occurrence, especially in patients with diabetes, has become a concerning trend in recent years. Chronic wounds are difficult and costly to treat, and have become a serious burden on health care systems worldwide. Chronic wounds can have devastating consequences for the patient, with infection often leading to reduced quality of life and increased mortality risk. Innovative deep learning methods for the detection and monitoring of such wounds have the potential to reduce the impact to both patient and clinician. We present a novel multimodal segmentation method which allows for the introduction of patient metadata into the training workflow whereby the patient data are expressed as Gaussian random fields. Our results indicate that the proposed method improved performance when utilising multiple models, each trained on different metadata categories. Using the Diabetic Foot Ulcer Challenge 2022 test set, when compared to the baseline results (intersection over union &#x3D; 0.4670, Dice similarity coefficient &#x3D; 0.5908) we demonstrate improvements of +0.0220 and +0.0229 for intersection over union and Dice similarity coefficient respectively. This paper presents the first study to focus on integrating patient data into a chronic wound segmentation workflow. Our results show significant performance gains when training individual models using specific metadata categories, followed by average merging of prediction masks using distance transforms. All source code for this study is available at: <a target="_blank" rel="noopener" href="https://github.com/mmu-dermatology-research/multimodal-grf">https://github.com/mmu-dermatology-research/multimodal-grf</a> </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œç‰¹åˆ«æ˜¯åœ¨ç³–å°¿ç—…æ‚£è€…ä¸­ï¼Œæ…¢æ€§ä¼¤å£çš„å‘ç”Ÿç‡å‘ˆä¸Šå‡è¶‹åŠ¿ï¼Œè¿™å·²æˆä¸ºä»¤äººæ‹…å¿§çš„è¶‹åŠ¿ã€‚æ…¢æ€§ä¼¤å£çš„æ²»ç–—éš¾åº¦å¤§ã€è´¹ç”¨é«˜ï¼Œå·²æˆä¸ºå…¨çƒå«ç”Ÿä¿å¥ç³»ç»Ÿçš„ä¸€é¡¹ä¸¥é‡è´Ÿæ‹…ã€‚æ…¢æ€§ä¼¤å£ä¼šä¸ºæ‚£è€…å¸¦æ¥ç¾éš¾æ€§åæœï¼Œæ„ŸæŸ“å¾€å¾€ä¼šå¯¼è‡´ç”Ÿæ´»è´¨é‡ä¸‹é™å’Œæ­»äº¡é£é™©å¢åŠ ã€‚é’ˆå¯¹æ­¤ç±»ä¼¤å£çš„æ£€æµ‹å’Œç›‘æµ‹ï¼Œåˆ›æ–°çš„æ·±åº¦å­¦ä¹ æ–¹æ³•æœ‰æœ›å‡è½»å¯¹æ‚£è€…å’Œä¸´åºŠåŒ»ç”Ÿçš„å½±å“ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„å¤šæ¨¡æ€åˆ†å‰²æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å…è®¸å°†æ‚£è€…å…ƒæ•°æ®å¼•å…¥åˆ°è®­ç»ƒæµç¨‹ä¸­ï¼Œå…¶ä¸­æ‚£è€…æ•°æ®è¡¨ç¤ºä¸ºé«˜æ–¯éšæœºåœºã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œåœ¨åˆ©ç”¨å¤šä¸ªæ¨¡å‹æ—¶ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨è®­ç»ƒä¸åŒå…ƒæ•°æ®ç±»åˆ«ä¸Šè¡¨ç°å‡ºæ€§èƒ½æå‡ã€‚ä¸åŸºå‡†ç»“æœç›¸æ¯”ï¼ˆäº¤å¹¶æ¯”&#x3D;0.4670ï¼Œè¿ªæ°æ–¯ç‰¹æ‹‰ç›¸ä¼¼ç³»æ•°&#x3D;0.5908ï¼‰ï¼Œåœ¨Diabetic Foot Ulcer Challenge 2022æµ‹è¯•é›†ä¸Šï¼Œæˆ‘ä»¬å±•ç¤ºå‡ºåœ¨äº¤å¹¶æ¯”å’Œè¿ªæ°æ–¯ç‰¹æ‹‰ç›¸ä¼¼ç³»æ•°æ–¹é¢åˆ†åˆ«æé«˜äº†+0.0220å’Œ+0.0229ã€‚æœ¬æ–‡æ˜¯é¦–æ¬¡ä¸“æ³¨äºå°†æ‚£è€…æ•°æ®æ•´åˆåˆ°æ…¢æ€§ä¼¤å£åˆ†å‰²æµç¨‹ä¸­çš„ç ”ç©¶ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œåœ¨é’ˆå¯¹ç‰¹å®šå…ƒæ•°æ®ç±»åˆ«è®­ç»ƒä¸ªåˆ«æ¨¡å‹åï¼Œé€šè¿‡è·ç¦»å˜æ¢å¹³å‡åˆå¹¶é¢„æµ‹æ©æ¨¡å¯ä»¥å–å¾—æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚è¯¥ç ”ç©¶çš„æ‰€æœ‰æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/mmu-dermatology-research/multimodal-grf%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/mmu-dermatology-research/multimodal-grfæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05214v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å…³æ³¨æ…¢æ€§ä¼¤å£ï¼Œç‰¹åˆ«æ˜¯ç³–å°¿ç—…æ‚£è€…æ…¢æ€§ä¼¤å£çš„æ—¥ç›Šå¢é•¿è¶‹åŠ¿ï¼Œæå‡ºä¸€ç§æ–°é¢–çš„å¤šæ¨¡æ€åˆ†å‰²æ–¹æ³•ï¼Œå°†æ‚£è€…å…ƒæ•°æ®å¼•å…¥è®­ç»ƒæµç¨‹ã€‚é€šè¿‡é‡‡ç”¨é«˜æ–¯éšæœºåœºè¡¨è¾¾æ‚£è€…æ•°æ®ï¼Œè¯¥æ–¹æ³•åœ¨åˆ©ç”¨ä¸åŒå…ƒæ•°æ®ç±»åˆ«è®­ç»ƒçš„å¤šä¸ªæ¨¡å‹ä¸Šè¡¨ç°å‡ºæ”¹è¿›æ•ˆæœã€‚ç›¸è¾ƒäºåŸºçº¿ç»“æœï¼Œè¯¥æ–¹æ³•åœ¨äº¤é›†æ¯”å’ŒDiceç›¸ä¼¼ç³»æ•°ä¸Šåˆ†åˆ«æé«˜äº†0.0220å’Œ0.0229ã€‚æœ¬æ–‡ä¸ºé¦–æ¬¡å°è¯•å°†æ‚£è€…æ•°æ®æ•´åˆåˆ°æ…¢æ€§ä¼¤å£åˆ†å‰²æµç¨‹ä¸­çš„ç ”ç©¶ï¼Œé€šè¿‡è®­ç»ƒç‰¹å®šå…ƒæ•°æ®ç±»åˆ«çš„ç‹¬ç«‹æ¨¡å‹ï¼Œç„¶åè¿›è¡Œé¢„æµ‹æ©è†œçš„å¹³å‡åˆå¹¶ï¼Œå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ…¢æ€§ä¼¤å£ï¼Œç‰¹åˆ«æ˜¯åœ¨ç³–å°¿ç—…æ‚£è€…ä¸­çš„å‘ç”Ÿç‡çš„å¢é•¿ï¼Œå·²æˆä¸ºå…¨çƒå«ç”Ÿç³»ç»Ÿçš„ä¸¥é‡è´Ÿæ‹…ã€‚</li>
<li>æ–°å‹æ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨æ…¢æ€§ä¼¤å£æ£€æµ‹å’Œç›‘æµ‹ä¸­å…·æœ‰å‡å°‘æ‚£è€…å’Œä¸´åºŠåŒ»ç”Ÿè´Ÿæ‹…çš„æ½œåŠ›ã€‚</li>
<li>æå‡ºä¸€ç§å¤šæ¨¡æ€åˆ†å‰²æ–¹æ³•ï¼Œæ•´åˆæ‚£è€…å…ƒæ•°æ®åˆ°è®­ç»ƒæµç¨‹ä¸­ï¼Œä»¥é«˜æ–¯éšæœºåœºè¡¨è¾¾æ‚£è€…æ•°æ®ã€‚</li>
<li>æ–¹æ³•åœ¨å¤šä¸ªæ¨¡å‹ä¸Šè¡¨ç°æ”¹è¿›æ•ˆæœï¼Œè¿™äº›æ¨¡å‹åˆ†åˆ«é’ˆå¯¹ä¸åŒçš„å…ƒæ•°æ®ç±»åˆ«è¿›è¡Œè®­ç»ƒã€‚</li>
<li>ä¸åŸºçº¿ç»“æœç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨äº¤é›†æ¯”å’ŒDiceç›¸ä¼¼ç³»æ•°ä¸Šæœ‰æ‰€æå‡ã€‚</li>
<li>æ­¤ç ”ç©¶ä¸ºé¦–æ¬¡å°è¯•å°†æ‚£è€…æ•°æ®æ•´åˆåˆ°æ…¢æ€§ä¼¤å£åˆ†å‰²æµç¨‹ä¸­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05214">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-fa7c65544ade7a8e45ba37ee307fbcb3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e3c4d985735e39937496b9d9e98715f1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5a57ce4b0117c9747fdec5fe16843aec.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d113fb0447467825c49f9b473574964c.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="MedVLM-R1-Incentivizing-Medical-Reasoning-Capability-of-Vision-Language-Models-VLMs-via-Reinforcement-Learning"><a href="#MedVLM-R1-Incentivizing-Medical-Reasoning-Capability-of-Vision-Language-Models-VLMs-via-Reinforcement-Learning" class="headerlink" title="MedVLM-R1: Incentivizing Medical Reasoning Capability of Vision-Language   Models (VLMs) via Reinforcement Learning"></a>MedVLM-R1: Incentivizing Medical Reasoning Capability of Vision-Language   Models (VLMs) via Reinforcement Learning</h2><p><strong>Authors:Jiazhen Pan, Che Liu, Junde Wu, Fenglin Liu, Jiayuan Zhu, Hongwei Bran Li, Chen Chen, Cheng Ouyang, Daniel Rueckert</strong></p>
<p>Reasoning is a critical frontier for advancing medical image analysis, where transparency and trustworthiness play a central role in both clinician trust and regulatory approval. Although Medical Visual Language Models (VLMs) show promise for radiological tasks, most existing VLMs merely produce final answers without revealing the underlying reasoning. To address this gap, we introduce MedVLM-R1, a medical VLM that explicitly generates natural language reasoning to enhance transparency and trustworthiness. Instead of relying on supervised fine-tuning (SFT), which often suffers from overfitting to training distributions and fails to foster genuine reasoning, MedVLM-R1 employs a reinforcement learning framework that incentivizes the model to discover human-interpretable reasoning paths without using any reasoning references. Despite limited training data (600 visual question answering samples) and model parameters (2B), MedVLM-R1 boosts accuracy from 55.11% to 78.22% across MRI, CT, and X-ray benchmarks, outperforming larger models trained on over a million samples. It also demonstrates robust domain generalization under out-of-distribution tasks. By unifying medical image analysis with explicit reasoning, MedVLM-R1 marks a pivotal step toward trustworthy and interpretable AI in clinical practice. Inference model is available at: <a target="_blank" rel="noopener" href="https://huggingface.co/JZPeterPan/MedVLM-R1">https://huggingface.co/JZPeterPan/MedVLM-R1</a>. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†ææ˜¯å½“å‰åŒ»ç–—é¢†åŸŸå‘å±•çš„å…³é”®å‰æ²¿ï¼Œå…¶ä¸­é€æ˜åº¦å’Œå¯ä¿¡åº¦å¯¹äºåŒ»ç”Ÿä¿¡ä»»åŠç›‘ç®¡æœºæ„çš„æ‰¹å‡†è‡³å…³é‡è¦ã€‚å°½ç®¡åŒ»å­¦è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨æ”¾å°„å­¦ä»»åŠ¡ä¸Šå±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†ç°æœ‰çš„å¤§å¤šæ•°VLMsä»…æä¾›æœ€ç»ˆç­”æ¡ˆï¼Œè€Œæœªèƒ½æ­ç¤ºèƒŒåçš„æ¨ç†è¿‡ç¨‹ã€‚ä¸ºäº†è§£å†³è¿™ä¸€ä¸è¶³ï¼Œæˆ‘ä»¬æ¨å‡ºäº†MedVLM-R1ï¼Œè¿™æ˜¯ä¸€æ¬¾æ˜ç¡®çš„åŒ»å­¦è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿç”Ÿæˆè‡ªç„¶è¯­è¨€æ¨ç†ä»¥å¢å¼ºé€æ˜åº¦å’Œå¯ä¿¡åº¦ã€‚ä¸å…¶ä»–æ¨¡å‹ä¸åŒï¼ŒMedVLM-R1æ²¡æœ‰ä¾èµ–ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œå› ä¸ºç›‘ç£å¾®è°ƒå¸¸å¸¸ä¼šå¯¼è‡´è¿‡åº¦æ‹Ÿåˆè®­ç»ƒåˆ†å¸ƒå¹¶å¿½ç•¥çœŸæ­£çš„æ¨ç†è¿‡ç¨‹ã€‚ç›¸åï¼ŒMedVLM-R1é‡‡ç”¨å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ¿€åŠ±æ¨¡å‹å‘ç°äººç±»å¯è§£é‡Šçš„æ¨ç†è·¯å¾„ï¼Œæ— éœ€ä½¿ç”¨ä»»ä½•æ¨ç†å‚è€ƒã€‚å°½ç®¡è®­ç»ƒæ•°æ®é‡æœ‰é™ï¼ˆä»…600ä¸ªè§†è§‰é—®ç­”æ ·æœ¬ï¼‰ä¸”æ¨¡å‹å‚æ•°è¾ƒå°‘ï¼ˆ2Bï¼‰ï¼Œä½†MedVLM-R1åœ¨MRIã€CTå’ŒXå…‰åŸºå‡†æµ‹è¯•ä¸Šçš„å‡†ç¡®ç‡ä»55.11%æå‡è‡³78.22%ï¼Œè¶…è¿‡äº†åœ¨è¶…è¿‡ç™¾ä¸‡æ ·æœ¬ä¸Šè®­ç»ƒçš„æ›´å¤§æ¨¡å‹ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜åœ¨éå†…éƒ¨åˆ†å¸ƒçš„ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„é¢†åŸŸæ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡å°†åŒ»å­¦å›¾åƒåˆ†æä¸æ˜ç¡®æ¨ç†ç›¸ç»“åˆï¼ŒMedVLM-R1æ ‡å¿—ç€ä¸´åºŠå®è·µä¸­å¯ä¿¡å’Œå¯è§£é‡Šçš„AIçš„é‡è¦ä¸€æ­¥ã€‚æ¨ç†æ¨¡å‹åœ°å€ä¸ºï¼š<a target="_blank" rel="noopener" href="https://huggingface.co/JZPeterPan/MedVLM-R1">https://huggingface.co/JZPeterPan/MedVLM-R1</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.19634v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨åŒ»å­¦å›¾åƒåˆ†æé¢†åŸŸï¼Œæ¨è¿›è‡ªç„¶è¯­è¨€æ¨ç†çš„é‡è¦æ€§ã€‚ä¸ºè§£å†³ç°æœ‰åŒ»ç–—è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ç¼ºä¹é€æ˜åº¦ä¸ä¿¡ä»»åº¦çš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åŒ»ç–—VLMâ€”â€”MedVLM-R1ã€‚MedVLM-R1ä½¿ç”¨å¼ºåŒ–å­¦ä¹ æ¡†æ¶è¿›è¡Œè®­ç»ƒï¼Œå¯æ¿€åŠ±æ¨¡å‹å‘ç°äººç±»å¯è§£é‡Šæ€§æ¨ç†è·¯å¾„ï¼Œä¸”æ— éœ€ä»»ä½•å‚è€ƒã€‚åœ¨æœ‰é™çš„è®­ç»ƒæ•°æ®å’Œæ¨¡å‹å‚æ•°æ¡ä»¶ä¸‹ï¼ŒMedVLM-R1å®ç°äº†MRIã€CTå’ŒXå…‰ç­‰å¤šåŸºå‡†æµ‹è¯•çš„æ€§èƒ½æå‡ï¼Œå¹¶è¡¨ç°å‡ºå‡ºè‰²çš„è·¨åŸŸæ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ¨¡å‹æ ‡å¿—ç€ä¸´åºŠåŒ»å­¦å®è·µæœç€å¯ä¿¡å¯è§£é‡Šçš„AIæ–¹å‘è¿ˆå‡ºé‡è¦ä¸€æ­¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MedVLM-R1æ˜¯ä¸€ä¸ªåŒ»ç–—è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³åŒ»å­¦å›¾åƒåˆ†æä¸­çš„é€æ˜åº¦å’Œä¿¡ä»»åº¦é—®é¢˜ã€‚</li>
<li>MedVLM-R1é‡‡ç”¨å¼ºåŒ–å­¦ä¹ æ¡†æ¶è®­ç»ƒæ¨¡å‹ï¼Œæ¿€åŠ±å…¶å‘ç°äººç±»å¯è§£é‡Šæ€§æ¨ç†è·¯å¾„ï¼Œæ— éœ€å‚è€ƒä»»ä½•æ¨ç†ä¾æ®ã€‚</li>
<li>åœ¨æœ‰é™çš„è®­ç»ƒæ•°æ®å’Œæ¨¡å‹å‚æ•°æ¡ä»¶ä¸‹ï¼ŒMedVLM-R1æå‡äº†åœ¨MRIã€CTå’ŒXå…‰ç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•çš„æ€§èƒ½ã€‚</li>
<li>MedVLM-R1å®ç°çš„å‡†ç¡®ç‡ä»55.11%æå‡åˆ°78.22%ï¼Œè¡¨ç°å‡ºè‰¯å¥½çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚</li>
<li>MedVLM-R1æ¨¡å‹å…·å¤‡è‰¯å¥½çš„è·¨åŸŸæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>MedVLM-R1æ¨¡å‹æ¨åŠ¨äº†åŒ»å­¦å›¾åƒåˆ†æé¢†åŸŸçš„è¿›æ­¥ï¼Œæ ‡å¿—ç€å‘ä¸´åºŠå®è·µä¸­å¯ä¿¡å¯è§£é‡Šçš„AIè¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.19634">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4866253832a007295535196f5d583a46.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd5227bee00a8424e43c4608d4f863f0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="A-study-of-why-we-need-to-reassess-full-reference-image-quality-assessment-with-medical-images"><a href="#A-study-of-why-we-need-to-reassess-full-reference-image-quality-assessment-with-medical-images" class="headerlink" title="A study of why we need to reassess full reference image quality   assessment with medical images"></a>A study of why we need to reassess full reference image quality   assessment with medical images</h2><p><strong>Authors:Anna Breger, Ander Biguri, Malena SabatÃ© Landman, Ian Selby, Nicole Amberg, Elisabeth Brunner, Janek GrÃ¶hl, Sepideh Hatamikia, Clemens Karner, Lipeng Ning, SÃ¶ren Dittmer, Michael Roberts, AIX-COVNET Collaboration, Carola-Bibiane SchÃ¶nlieb</strong></p>
<p>Image quality assessment (IQA) is indispensable in clinical practice to ensure high standards, as well as in the development stage of machine learning algorithms that operate on medical images. The popular full reference (FR) IQA measures PSNR and SSIM are known and tested for working successfully in many natural imaging tasks, but discrepancies in medical scenarios have been reported in the literature, highlighting the gap between development and actual clinical application. Such inconsistencies are not surprising, as medical images have very different properties than natural images, and PSNR and SSIM have neither been targeted nor properly tested for medical images. This may cause unforeseen problems in clinical applications due to wrong judgment of novel methods. This paper provides a structured and comprehensive overview of examples where PSNR and SSIM prove to be unsuitable for the assessment of novel algorithms using different kinds of medical images, including real-world MRI, CT, OCT, X-Ray, digital pathology and photoacoustic imaging data. Therefore, improvement is urgently needed in particular in this era of AI to increase reliability and explainability in machine learning for medical imaging and beyond. Lastly, we will provide ideas for future research as well as suggesting guidelines for the usage of FR-IQA measures applied to medical images. </p>
<blockquote>
<p>å›¾åƒè´¨é‡è¯„ä¼°ï¼ˆIQAï¼‰åœ¨ä¸´åºŠå®è·µä¸­æ˜¯ä¸å¯æˆ–ç¼ºçš„ï¼Œä»¥ç¡®ä¿é«˜æ ‡å‡†ï¼Œä»¥åŠåœ¨åŒ»å­¦å½±åƒæ“ä½œçš„æœºå™¨å­¦ä¹ ç®—æ³•çš„å¼€å‘é˜¶æ®µä¹Ÿæ˜¯å¦‚æ­¤ã€‚æµè¡Œçš„å…¨å‚è€ƒï¼ˆFRï¼‰IQAæªæ–½PSNRå’ŒSSIMåœ¨ä¼—å¤šçš„è‡ªç„¶æˆåƒä»»åŠ¡ä¸­å·²è¢«è¯æ˜å’Œæµ‹è¯•è¿‡å…¶æˆåŠŸæ€§ï¼Œä½†åœ¨åŒ»ç–—åœºæ™¯ä¸­ï¼Œæ–‡çŒ®ä¸­å·²æŠ¥é“å­˜åœ¨åˆ†æ­§ï¼Œçªå‡ºäº†å¼€å‘ä¸å®é™…åº”ç”¨ä¹‹é—´çš„å·®è·ã€‚è¿™ç§ä¸ä¸€è‡´æ€§å¹¶ä¸å¥‡æ€ªï¼Œå› ä¸ºåŒ»ç–—å›¾åƒå…·æœ‰ä¸è‡ªç„¶å›¾åƒéå¸¸ä¸åŒçš„å±æ€§ï¼Œè€ŒPSNRå’ŒSSIMå¹¶æœªé’ˆå¯¹åŒ»ç–—å›¾åƒè¿›è¡Œç›®æ ‡å®šä½æˆ–é€‚å½“çš„æµ‹è¯•ã€‚è¿™å¯èƒ½å¯¼è‡´ç”±äºæ–°æ–¹æ³•çš„è¯¯åˆ¤è€Œåœ¨ä¸´åºŠåº”ç”¨ä¸­å‡ºç°æ„æƒ³ä¸åˆ°çš„é—®é¢˜ã€‚æœ¬æ–‡æä¾›äº†ç»“æ„åŒ–å’Œå…¨é¢çš„æ¦‚è¿°ï¼Œä¸¾ä¾‹è¯´æ˜äº†PSNRå’ŒSSIMåœ¨åˆ©ç”¨ä¸åŒç§ç±»çš„åŒ»å­¦å›¾åƒï¼ˆåŒ…æ‹¬ç°å®ä¸–ç•Œä¸­çš„MRIã€CTã€OCTã€Xå°„çº¿ã€æ•°å­—ç—…ç†å’Œå…‰å£°æˆåƒæ•°æ®ï¼‰è¯„ä¼°æ–°ç®—æ³•æ—¶çš„ä¸é€‚ç”¨æ€§ã€‚å› æ­¤ï¼Œç‰¹åˆ«æ˜¯åœ¨äººå·¥æ™ºèƒ½æ—¶ä»£ï¼Œè¿«åˆ‡éœ€è¦æ”¹è¿›è¿™ä¸€é¢†åŸŸï¼Œä»¥æé«˜åŒ»å­¦å½±åƒå’Œå…¶ä»–é¢†åŸŸçš„æœºå™¨å­¦ä¹ çš„å¯é æ€§å’Œå¯è§£é‡Šæ€§ã€‚æœ€åï¼Œæˆ‘ä»¬å°†ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›æ€è·¯ï¼Œå¹¶æå‡ºåœ¨å…¨å‚è€ƒIQAæªæ–½åº”ç”¨äºåŒ»å­¦å›¾åƒæ—¶çš„ä½¿ç”¨æŒ‡å—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.19097v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŒ»å­¦å›¾åƒè´¨é‡è¯„ä¼°ï¼ˆIQAï¼‰åœ¨ä¸´åºŠå®è·µå’Œå¯¹åŒ»ç–—å›¾åƒæ“ä½œçš„æœºå™¨å­¦ä¹ ç®—æ³•çš„å¼€å‘é˜¶æ®µéƒ½ä¸å¯æˆ–ç¼ºã€‚è™½ç„¶æµè¡Œçš„å…¨å‚è€ƒï¼ˆFRï¼‰IQAæªæ–½PSNRå’ŒSSIMå·²åœ¨è®¸å¤šè‡ªç„¶æˆåƒä»»åŠ¡ä¸­æˆåŠŸåº”ç”¨ï¼Œä½†åœ¨åŒ»å­¦åœºæ™¯ä¸­å´å­˜åœ¨ä¸ä¸€è‡´æ€§ï¼Œè¿™çªæ˜¾äº†å¼€å‘ä¸å®é™…åº”ç”¨ä¹‹é—´çš„å·®è·ã€‚ç”±äºåŒ»å­¦å›¾åƒä¸è‡ªç„¶å›¾åƒå…·æœ‰ä¸åŒçš„ç‰¹æ€§ï¼ŒPSNRå’ŒSSIMå¹¶æœªé’ˆå¯¹åŒ»å­¦å›¾åƒè¿›è¡Œä¸“é¡¹æµ‹è¯•æˆ–é€‚å½“æµ‹è¯•ï¼Œå¯èƒ½å¯¼è‡´æ–°å‹æ–¹æ³•çš„è¯¯åˆ¤ã€‚æœ¬æ–‡æä¾›äº†åœ¨ä¸åŒåŒ»å­¦å›¾åƒä¸­ä½¿ç”¨PSNRå’ŒSSIMè¯„ä¼°æ–°å‹ç®—æ³•çš„ä¸é€‚ç”¨æ¡ˆä¾‹çš„ç»¼åˆæ¦‚è¿°ï¼ŒåŒ…æ‹¬ç°å®ä¸–ç•Œçš„MRIã€CTã€OCTã€Xå°„çº¿ã€æ•°å­—ç—…ç†å’Œå…‰å£°æˆåƒæ•°æ®ã€‚å› æ­¤ï¼Œç‰¹åˆ«æ˜¯åœ¨äººå·¥æ™ºèƒ½æ—¶ä»£ï¼ŒäºŸéœ€æé«˜å¯é æ€§å¹¶æå‡æœºå™¨å­¦ä¹ åœ¨åŒ»å­¦æˆåƒç­‰é¢†åŸŸçš„å¯è§£é‡Šæ€§ã€‚æœ€åï¼Œæœ¬æ–‡å°†æä¾›æœªæ¥ç ”ç©¶çš„æ–¹å‘å’Œåœ¨ä½¿ç”¨FR-IQAæªæ–½è¿›è¡ŒåŒ»å­¦å›¾åƒåº”ç”¨çš„æŒ‡å¯¼å»ºè®®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>IQAåœ¨åŒ»å­¦é¢†åŸŸä¸å¯æˆ–ç¼ºï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸´åºŠå®è·µåŠåŒ»ç–—å›¾åƒæœºå™¨å­¦ä¹ ç®—æ³•å¼€å‘é˜¶æ®µã€‚</li>
<li>PSNRå’ŒSSIMç­‰ä¼ ç»Ÿå…¨å‚è€ƒIQAæ–¹æ³•åœ¨è‡ªç„¶æˆåƒä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨åŒ»å­¦åœºæ™¯ä¸­å¯èƒ½å­˜åœ¨ä¸ä¸€è‡´æ€§ã€‚</li>
<li>åŒ»å­¦å›¾åƒä¸è‡ªç„¶å›¾åƒç‰¹æ€§ä¸åŒï¼Œä¼ ç»ŸIQAæ–¹æ³•å¹¶æœªé’ˆå¯¹åŒ»å­¦å›¾åƒè¿›è¡Œä¸“é¡¹æµ‹è¯•æˆ–é€‚å½“æµ‹è¯•ã€‚</li>
<li>PSNRå’ŒSSIMåœ¨è¯„ä¼°æ–°å‹ç®—æ³•æ—¶å¯èƒ½ä¸é€‚ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½¿ç”¨ä¸åŒç±»å‹çš„åŒ»å­¦å›¾åƒæ—¶ã€‚</li>
<li>åœ¨äººå·¥æ™ºèƒ½æ—¶ä»£ï¼Œæé«˜åŒ»å­¦æˆåƒä¸­æœºå™¨å­¦ä¹ ç®—æ³•çš„å¯é æ€§å’Œå¯è§£é‡Šæ€§è‡³å…³é‡è¦ã€‚</li>
<li>éœ€è¦æ”¹è¿›å’Œå‘å±•æ–°çš„IQAæ–¹æ³•ä»¥é€‚åº”åŒ»å­¦å›¾åƒçš„ç‰¹æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.19097">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3e4072c33b378070c93032a158d70fe7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-31f10cd04ebef96792b3425f4920dba8.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="ModeTv2-GPU-accelerated-Motion-Decomposition-Transformer-for-Pairwise-Optimization-in-Medical-Image-Registration"><a href="#ModeTv2-GPU-accelerated-Motion-Decomposition-Transformer-for-Pairwise-Optimization-in-Medical-Image-Registration" class="headerlink" title="ModeTv2: GPU-accelerated Motion Decomposition Transformer for Pairwise   Optimization in Medical Image Registration"></a>ModeTv2: GPU-accelerated Motion Decomposition Transformer for Pairwise   Optimization in Medical Image Registration</h2><p><strong>Authors:Haiqiao Wang, Zhuoyuan Wang, Dong Ni, Yi Wang</strong></p>
<p>Deformable image registration plays a crucial role in medical imaging, aiding in disease diagnosis and image-guided interventions. Traditional iterative methods are slow, while deep learning (DL) accelerates solutions but faces usability and precision challenges. This study introduces a pyramid network with the enhanced motion decomposition Transformer (ModeTv2) operator, showcasing superior pairwise optimization (PO) akin to traditional methods. We re-implement ModeT operator with CUDA extensions to enhance its computational efficiency. We further propose RegHead module which refines deformation fields, improves the realism of deformation and reduces parameters. By adopting the PO, the proposed network balances accuracy, efficiency, and generalizability. Extensive experiments on three public brain MRI datasets and one abdominal CT dataset demonstrate the networkâ€™s suitability for PO, providing a DL model with enhanced usability and interpretability. The code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/ZAX130/ModeTv2">https://github.com/ZAX130/ModeTv2</a>. </p>
<blockquote>
<p>å¯å˜å½¢å›¾åƒé…å‡†åœ¨åŒ»å­¦æˆåƒä¸­æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ï¼Œæœ‰åŠ©äºç–¾ç—…è¯Šæ–­å’Œå›¾åƒå¼•å¯¼å¹²é¢„ã€‚ä¼ ç»Ÿè¿­ä»£æ–¹æ³•é€Ÿåº¦æ…¢ï¼Œè€Œæ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰è™½ç„¶å¯ä»¥åŠ é€Ÿè§£å†³æ–¹æ¡ˆçš„è®¡ç®—ï¼Œä½†å´é¢ä¸´å¯ç”¨æ€§å’Œç²¾ç¡®åº¦æ–¹é¢çš„æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ç§é‡‘å­—å¡”ç½‘ç»œï¼Œé…å¤‡äº†å¢å¼ºçš„è¿åŠ¨åˆ†è§£Transformerï¼ˆModeTv2ï¼‰ç®—å­ï¼Œå±•ç°äº†ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸åª²ç¾çš„ä¼˜è¶Šç‚¹å¯¹ç‚¹ä¼˜åŒ–ï¼ˆPOï¼‰ã€‚æˆ‘ä»¬ä½¿ç”¨CUDAæ‰©å±•é‡æ–°å®ç°äº†ModeTç®—å­ï¼Œä»¥æé«˜å…¶è®¡ç®—æ•ˆç‡ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†RegHeadæ¨¡å—ï¼Œç”¨äºç»†åŒ–å˜å½¢åœºï¼Œæé«˜å˜å½¢çš„çœŸå®æ€§å¹¶å‡å°‘å‚æ•°ã€‚é€šè¿‡é‡‡ç”¨ç‚¹å¯¹ç‚¹ä¼˜åŒ–ï¼Œæ‰€æå‡ºçš„ç½‘ç»œåœ¨å‡†ç¡®æ€§ã€æ•ˆç‡å’Œé€šç”¨æ€§ä¹‹é—´è¾¾åˆ°äº†å¹³è¡¡ã€‚åœ¨ä¸‰ä¸ªå…¬å…±è„‘MRIæ•°æ®é›†å’Œä¸€ä¸ªè…¹éƒ¨CTæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥ç½‘ç»œéå¸¸é€‚åˆè¿›è¡Œç‚¹å¯¹ç‚¹ä¼˜åŒ–ï¼Œä¸ºæ·±åº¦å­¦ä¹ æ¨¡å‹æä¾›äº†å¢å¼ºå¯ç”¨æ€§å’Œå¯è§£é‡Šæ€§çš„æ¨¡å‹ã€‚ç›¸å…³ä»£ç å·²å…¬å¼€å‘å¸ƒåœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/ZAX130/ModeTv2%E3%80%82">https://github.com/ZAX130/ModeTv2ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.16526v2">PDF</a> </p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒé…å‡†ä¸­ï¼Œå½¢å˜å›¾åƒé…å‡†ååˆ†é‡è¦ï¼Œæœ‰åŠ©äºç–¾ç—…è¯Šæ–­å’Œå›¾åƒå¼•å¯¼å¹²é¢„ã€‚ä¼ ç»Ÿè¿­ä»£æ–¹æ³•é€Ÿåº¦æ…¢ï¼Œæ·±åº¦å­¦ä¹ è™½èƒ½åŠ é€Ÿä½†é¢ä¸´å¯ç”¨æ€§å’Œç²¾åº¦æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶å¼•å…¥å¸¦æœ‰å¢å¼ºè¿åŠ¨åˆ†è§£Transformerï¼ˆModeTv2ï¼‰ç®—å­çš„é‡‘å­—å¡”ç½‘ç»œï¼Œå±•ç¤ºä¸ä¼ ç»Ÿæ–¹æ³•ç›¸ä¼¼çš„ä¼˜è¶Šæˆå¯¹ä¼˜åŒ–ï¼ˆPOï¼‰ã€‚é€šè¿‡CUDAæ‰©å±•é‡æ–°å®ç°ModeTç®—å­ä»¥æé«˜è®¡ç®—æ•ˆç‡ã€‚è¿›ä¸€æ­¥æå‡ºRegHeadæ¨¡å—ï¼Œç”¨äºç»†åŒ–å˜å½¢åœºï¼Œæé«˜å˜å½¢çœŸå®æ€§å’Œå‡å°‘å‚æ•°ã€‚é‡‡ç”¨POï¼Œè¯¥ç½‘ç»œåœ¨å‡†ç¡®æ€§ã€æ•ˆç‡å’Œé€šç”¨æ€§ä¹‹é—´å–å¾—å¹³è¡¡ã€‚åœ¨ä¸‰ä¸ªå…¬å…±è„‘MRIæ•°æ®é›†å’Œä¸€ä¸ªè…¹éƒ¨CTæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†è¯¥ç½‘ç»œé€‚ç”¨äºPOï¼Œæä¾›å…·æœ‰å¢å¼ºå¯ç”¨æ€§å’Œå¯è§£é‡Šæ€§çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½¢å˜å›¾åƒé…å‡†åœ¨åŒ»å­¦æˆåƒä¸­èµ·å…³é”®ä½œç”¨ï¼Œæœ‰åŠ©äºç–¾ç—…è¯Šæ–­å’Œå›¾åƒå¼•å¯¼å¹²é¢„ã€‚</li>
<li>ä¼ ç»Ÿè¿­ä»£æ–¹æ³•é€Ÿåº¦æ…¢ï¼Œæ·±åº¦å­¦ä¹ æä¾›åŠ é€Ÿä½†å­˜åœ¨å¯ç”¨æ€§å’Œç²¾åº¦æŒ‘æˆ˜ã€‚</li>
<li>ç ”ç©¶å¼•å…¥é‡‘å­—å¡”ç½‘ç»œå¹¶ç»“åˆModeTv2ç®—å­ï¼Œå®ç°ä¼˜è¶Šæˆå¯¹ä¼˜åŒ–ï¼ˆPOï¼‰ã€‚</li>
<li>ModeTç®—å­é€šè¿‡CUDAæ‰©å±•é‡æ–°å®ç°ï¼Œæé«˜è®¡ç®—æ•ˆç‡ã€‚</li>
<li>æå‡ºRegHeadæ¨¡å—ï¼Œç”¨äºç»†åŒ–å˜å½¢åœºï¼Œæé«˜å˜å½¢çš„çœŸå®æ€§å’Œå‡å°‘å‚æ•°ã€‚</li>
<li>é‡‡ç”¨POå¹³è¡¡å‡†ç¡®æ€§ã€æ•ˆç‡å’Œé€šç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.16526">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-dc431982f411a46a0e40cb72efcf5eea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0df64c2ee24142ec2cf96c21a97dd416.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ee9840133c9ee59996a30c4d2ef9aaf8.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Semi-Supervised-Semantic-Segmentation-Based-on-Pseudo-Labels-A-Survey"><a href="#Semi-Supervised-Semantic-Segmentation-Based-on-Pseudo-Labels-A-Survey" class="headerlink" title="Semi-Supervised Semantic Segmentation Based on Pseudo-Labels: A Survey"></a>Semi-Supervised Semantic Segmentation Based on Pseudo-Labels: A Survey</h2><p><strong>Authors:Lingyan Ran, Yali Li, Guoqiang Liang, Yanning Zhang</strong></p>
<p>Semantic segmentation is an important and popular research area in computer vision that focuses on classifying pixels in an image based on their semantics. However, supervised deep learning requires large amounts of data to train models and the process of labeling images pixel by pixel is time-consuming and laborious. This review aims to provide a first comprehensive and organized overview of the state-of-the-art research results on pseudo-label methods in the field of semi-supervised semantic segmentation, which we categorize from different perspectives and present specific methods for specific application areas. In addition, we explore the application of pseudo-label technology in medical and remote-sensing image segmentation. Finally, we also propose some feasible future research directions to address the existing challenges. </p>
<blockquote>
<p>è¯­ä¹‰åˆ†å‰²æ˜¯è®¡ç®—æœºè§†è§‰ä¸­ä¸€ä¸ªé‡è¦ä¸”çƒ­é—¨çš„ç ”ç©¶é¢†åŸŸï¼Œä¸»è¦å…³æ³¨åŸºäºè¯­ä¹‰å¯¹å›¾åƒä¸­çš„åƒç´ è¿›è¡Œåˆ†ç±»ã€‚ç„¶è€Œï¼Œç›‘ç£æ·±åº¦å­¦ä¹ éœ€è¦å¤§é‡çš„æ•°æ®æ¥è®­ç»ƒæ¨¡å‹ï¼Œé€åƒç´ æ ‡æ³¨å›¾åƒçš„è¿‡ç¨‹æ—¢è€—æ—¶åˆç¹çã€‚æœ¬æ–‡æ—¨åœ¨æä¾›ä¼ªæ ‡ç­¾æ–¹æ³•åœ¨åŠç›‘ç£è¯­ä¹‰åˆ†å‰²é¢†åŸŸæœ€æ–°ç ”ç©¶æˆæœçš„é¦–æ¬¡å…¨é¢ã€ç³»ç»Ÿçš„æ¦‚è¿°ï¼Œä»ä¸åŒè§’åº¦å¯¹æ–¹æ³•è¿›è¡Œåˆ†ç±»ï¼Œå¹¶é’ˆå¯¹ç‰¹å®šåº”ç”¨é¢†åŸŸä»‹ç»å…·ä½“æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¢è®¨äº†ä¼ªæ ‡ç­¾æŠ€æœ¯åœ¨åŒ»å­¦å’Œé¥æ„Ÿå›¾åƒåˆ†å‰²ä¸­çš„åº”ç”¨ã€‚æœ€åï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€äº›å¯è¡Œçš„æœªæ¥ç ”ç©¶æ–¹å‘ï¼Œä»¥è§£å†³ç°æœ‰æŒ‘æˆ˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.01909v3">PDF</a> Accepted by IEEE Transactions on Circuits and Systems for Video   Technology(TCSVT)</p>
<p><strong>Summary</strong><br>     è¯­ä¹‰åˆ†å‰²æ˜¯è®¡ç®—æœºè§†è§‰ä¸­ä¸€ä¸ªé‡è¦ä¸”çƒ­é—¨çš„ç ”ç©¶é¢†åŸŸï¼Œä¸»è¦å¯¹å›¾åƒä¸­çš„åƒç´ è¿›è¡Œè¯­ä¹‰åˆ†ç±»ã€‚ç„¶è€Œï¼Œç›‘ç£æ·±åº¦å­¦ä¹ éœ€è¦å¤§é‡çš„æ•°æ®è¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œé€åƒç´ æ ‡æ³¨å›¾åƒçš„è¿‡ç¨‹è€—æ—¶ä¸”ç¹çã€‚æœ¬æ–‡æ—¨åœ¨æä¾›ä¼ªæ ‡ç­¾æ–¹æ³•åœ¨åŠç›‘ç£è¯­ä¹‰åˆ†å‰²é¢†åŸŸæœ€æ–°ç ”ç©¶æˆæœçš„é¦–æ¬¡å…¨é¢ç»¼è¿°ï¼Œä»ä¸åŒè§’åº¦åˆ†ç±»å¹¶ä»‹ç»ç‰¹å®šåº”ç”¨é¢†åŸŸçš„æ–¹æ³•ã€‚æ­¤å¤–ï¼Œè¿˜æ¢è®¨äº†ä¼ªæ ‡ç­¾æŠ€æœ¯åœ¨åŒ»å­¦å’Œé¥æ„Ÿå›¾åƒåˆ†å‰²ä¸­çš„åº”ç”¨ã€‚æœ€åï¼Œæœ¬æ–‡è¿˜æå‡ºäº†ä¸€äº›å¯è¡Œçš„æœªæ¥ç ”ç©¶æ–¹å‘ï¼Œä»¥åº”å¯¹ç°æœ‰æŒ‘æˆ˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­ä¹‰åˆ†å‰²æ˜¯è®¡ç®—æœºè§†è§‰ä¸­ç ”ç©¶çƒ­ç‚¹ï¼Œä¸»è¦å¯¹å›¾åƒä¸­çš„åƒç´ è¿›è¡Œè¯­ä¹‰åˆ†ç±»ã€‚</li>
<li>ç›‘ç£æ·±åº¦å­¦ä¹ éœ€è¦å¤§é‡çš„æ•°æ®è¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œé€åƒç´ æ ‡æ³¨å›¾åƒæ˜¯ç¹çè€—æ—¶çš„å·¥ä½œã€‚</li>
<li>ä¼ªæ ‡ç­¾æ–¹æ³•åœ¨åŠç›‘ç£è¯­ä¹‰åˆ†å‰²é¢†åŸŸå¾—åˆ°å¹¿æ³›åº”ç”¨ï¼Œæœ¬æ–‡æä¾›äº†å…¨é¢çš„ç»¼è¿°ã€‚</li>
<li>ä¼ªæ ‡ç­¾æŠ€æœ¯ä¸ä»…é€‚ç”¨äºæ™®é€šå›¾åƒåˆ†å‰²ï¼Œè¿˜åº”ç”¨äºåŒ»å­¦å’Œé¥æ„Ÿå›¾åƒåˆ†å‰²ã€‚</li>
<li>æ–‡ç« ä»ä¸åŒè§’åº¦å¯¹ä¼ªæ ‡ç­¾æ–¹æ³•è¿›è¡Œäº†åˆ†ç±»ï¼Œå¹¶ä»‹ç»äº†ç‰¹å®šåº”ç”¨é¢†åŸŸçš„æ–¹æ³•ã€‚</li>
<li>å½“å‰è¯¥é¢†åŸŸå­˜åœ¨æŒ‘æˆ˜ï¼Œæ–‡ç« æå‡ºäº†å¯è¡Œçš„æœªæ¥ç ”ç©¶æ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.01909">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e61ae1cc5d887d26086ba20ed50b1d36.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aad6bbbd8d18cc7d69a2aff258df9a2a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50c235727093679ffd70b362dce66667.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c80a069b21c7cb3e3e897e7994498a1d.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Masked-LoGoNet-Fast-and-Accurate-3D-Image-Analysis-for-Medical-Domain"><a href="#Masked-LoGoNet-Fast-and-Accurate-3D-Image-Analysis-for-Medical-Domain" class="headerlink" title="Masked LoGoNet: Fast and Accurate 3D Image Analysis for Medical Domain"></a>Masked LoGoNet: Fast and Accurate 3D Image Analysis for Medical Domain</h2><p><strong>Authors:Amin Karimi Monsefi, Payam Karisani, Mengxi Zhou, Stacey Choi, Nathan Doble, Heng Ji, Srinivasan Parthasarathy, Rajiv Ramnath</strong></p>
<p>Standard modern machine-learning-based imaging methods have faced challenges in medical applications due to the high cost of dataset construction and, thereby, the limited labeled training data available. Additionally, upon deployment, these methods are usually used to process a large volume of data on a daily basis, imposing a high maintenance cost on medical facilities. In this paper, we introduce a new neural network architecture, termed LoGoNet, with a tailored self-supervised learning (SSL) method to mitigate such challenges. LoGoNet integrates a novel feature extractor within a U-shaped architecture, leveraging Large Kernel Attention (LKA) and a dual encoding strategy to capture both long-range and short-range feature dependencies adeptly. This is in contrast to existing methods that rely on increasing network capacity to enhance feature extraction. This combination of novel techniques in our model is especially beneficial in medical image segmentation, given the difficulty of learning intricate and often irregular body organ shapes, such as the spleen. Complementary, we propose a novel SSL method tailored for 3D images to compensate for the lack of large labeled datasets. The method combines masking and contrastive learning techniques within a multi-task learning framework and is compatible with both Vision Transformer (ViT) and CNN-based models. We demonstrate the efficacy of our methods in numerous tasks across two standard datasets (i.e., BTCV and MSD). Benchmark comparisons with eight state-of-the-art models highlight LoGoNetâ€™s superior performance in both inference time and accuracy. </p>
<blockquote>
<p>æ ‡å‡†ç°ä»£æœºå™¨å­¦ä¹ æˆåƒæ–¹æ³•åœ¨é¢å¯¹åŒ»ç–—åº”ç”¨æ—¶é¢ä¸´äº†æ•°æ®é›†æ„å»ºæˆæœ¬é«˜æ˜‚å’Œå¯ç”¨æ ‡è®°è®­ç»ƒæ•°æ®æœ‰é™çš„æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•åœ¨éƒ¨ç½²åé€šå¸¸ç”¨äºå¤„ç†æ¯å¤©çš„å¤§é‡æ•°æ®ï¼Œç»™åŒ»ç–—æœºæ„å¸¦æ¥äº†é«˜æ˜‚çš„ç»´æŠ¤æˆæœ¬ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°å‹ç¥ç»ç½‘ç»œæ¶æ„ï¼Œç§°ä¸ºLoGoNetï¼Œå®ƒé‡‡ç”¨é‡èº«å®šåˆ¶çš„è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•æ¥ç¼“è§£è¿™äº›æŒ‘æˆ˜ã€‚LoGoNetåœ¨Uå½¢æ¶æ„å†…é›†æˆäº†ä¸€ç§æ–°å‹ç‰¹å¾æå–å™¨ï¼Œåˆ©ç”¨å¤§å†…æ ¸æ³¨æ„åŠ›ï¼ˆLKAï¼‰å’ŒåŒç¼–ç ç­–ç•¥æ¥å·§å¦™åœ°æ•æ‰é•¿çŸ­èŒƒå›´çš„ç‰¹å¾ä¾èµ–æ€§ã€‚è¿™ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼Œåè€…ä¾èµ–äºå¢åŠ ç½‘ç»œå®¹é‡æ¥æé«˜ç‰¹å¾æå–èƒ½åŠ›ã€‚æ¨¡å‹ä¸­è¿™ç§æ–°æŠ€æœ¯çš„ç»„åˆåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­å°¤å…¶æœ‰ç›Šï¼Œè€ƒè™‘åˆ°å­¦ä¹ å¤æ‚ä¸”é€šå¸¸ä¸è§„åˆ™çš„å™¨å®˜å½¢çŠ¶ï¼ˆå¦‚è„¾è„ï¼‰çš„å›°éš¾æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é’ˆå¯¹3Då›¾åƒçš„æ–°å‹è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œä»¥å¼¥è¡¥å¤§å‹æ ‡è®°æ•°æ®é›†çš„ç¼ºä¹ã€‚è¯¥æ–¹æ³•åœ¨å¤šä»»åŠ¡å­¦ä¹ æ¡†æ¶å†…ç»“åˆäº†æ©è”½å’Œå¯¹æ¯”å­¦ä¹ æŠ€æœ¯ï¼Œå¯ä¸Vision Transformerï¼ˆViTï¼‰å’ŒCNNæ¨¡å‹å…¼å®¹ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªæ ‡å‡†æ•°æ®é›†ï¼ˆå³BTCVå’ŒMSDï¼‰çš„å¤šä¸ªä»»åŠ¡ä¸­å±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ä¸å…«ç§æœ€æ–°æ¨¡å‹çš„åŸºå‡†å¯¹æ¯”è¡¨æ˜ï¼ŒLoGoNetåœ¨æ¨ç†æ—¶é—´å’Œå‡†ç¡®æ€§æ–¹é¢å‡è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.06190v2">PDF</a> </p>
<p><strong>Summary</strong><br>    æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„ç¥ç»ç½‘ç»œæ¶æ„LoGoNetï¼Œç»“åˆè‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºè§£å†³åŒ»å­¦å›¾åƒåˆ†æä¸­æ•°æ®é›†æ„å»ºæˆæœ¬é«˜ã€è®­ç»ƒæ•°æ®æœ‰é™çš„é—®é¢˜ã€‚LoGoNeté‡‡ç”¨Uå‹æ¶æ„ã€å¤§å†…æ ¸æ³¨æ„åŠ›æœºåˆ¶å’ŒåŒç¼–ç ç­–ç•¥ï¼Œèƒ½æœ‰æ•ˆæ•æ‰é•¿ç¨‹å’ŒçŸ­ç¨‹ç‰¹å¾ä¾èµ–å…³ç³»ï¼Œç‰¹åˆ«é€‚ç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²ä¸­å¤æ‚ä¸”ä¸è§„åˆ™å™¨å®˜å½¢çŠ¶çš„å­¦ä¹ ã€‚åŒæ—¶ï¼Œæå‡ºäº†ä¸€ç§é’ˆå¯¹3Då›¾åƒçš„æ–°å‹è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œä»¥å¼¥è¡¥ç¼ºä¹å¤§é‡æ ‡è®°æ•°æ®é›†çš„é—®é¢˜ã€‚åœ¨å¤šä¸ªä»»åŠ¡ã€ä¸¤ä¸ªæ ‡å‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒLoGoNetä¸å…«ç§æœ€æ–°æ¨¡å‹ç›¸æ¯”ï¼Œåœ¨æ¨ç†æ—¶é—´å’Œå‡†ç¡®æ€§ä¸Šå‡è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LoGoNetæ˜¯ä¸€ç§æ–°çš„ç¥ç»ç½‘ç»œæ¶æ„ï¼Œç”¨äºè§£å†³åŒ»å­¦å›¾åƒåˆ†æä¸­çš„æŒ‘æˆ˜ï¼Œå¦‚é«˜æˆæœ¬çš„æ•°æ®é›†æ„å»ºå’Œæœ‰é™çš„æ ‡è®°è®­ç»ƒæ•°æ®ã€‚</li>
<li>LoGoNetç»“åˆè‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œèƒ½å¤Ÿå¤„ç†å¤§è§„æ¨¡æ•°æ®å¹¶é™ä½ç»´æŠ¤æˆæœ¬ã€‚</li>
<li>LoGoNeté‡‡ç”¨Uå‹æ¶æ„ã€å¤§å†…æ ¸æ³¨æ„åŠ›æœºåˆ¶å’ŒåŒç¼–ç ç­–ç•¥ï¼Œèƒ½æ›´æœ‰æ•ˆåœ°æ•æ‰ç‰¹å¾ä¾èµ–å…³ç³»ã€‚</li>
<li>æå‡ºäº†ä¸€ç§é’ˆå¯¹3Då›¾åƒçš„æ–°å‹è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œä»¥åº”å¯¹ç¼ºä¹å¤§é‡æ ‡è®°æ•°æ®é›†çš„é—®é¢˜ã€‚</li>
<li>è¯¥æ–¹æ³•ç»“åˆäº†é®è”½å’Œå¯¹æ¯”å­¦ä¹ æŠ€æœ¯ï¼Œåœ¨ä¸€ä¸ªå¤šä»»åŠ¡å­¦ä¹ æ¡†æ¶å†…å®ç°ï¼Œå…¼å®¹Vision Transformerï¼ˆViTï¼‰å’ŒCNNæ¨¡å‹ã€‚</li>
<li>åœ¨ä¸¤ä¸ªæ ‡å‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒLoGoNetåœ¨æ¨ç†æ—¶é—´å’Œå‡†ç¡®æ€§æ–¹é¢å‡ä¼˜äºå…¶ä»–å…«ç§æœ€æ–°æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2402.06190">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-798ba352c3a78b35a46c24d8da54158a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-633d9ccad9baba3c7cf2f763e4d1b953.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea006b54cbff3ce97de70c07152c8b38.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b6e3efb149ba8b1340f34bcf77e17796.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-819a5dbc858d19aa0001260dd1ad4b34.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-69a0363843da6aa4f73c13d70aafd3cf.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="PULASki-Learning-inter-rater-variability-using-statistical-distances-to-improve-probabilistic-segmentation"><a href="#PULASki-Learning-inter-rater-variability-using-statistical-distances-to-improve-probabilistic-segmentation" class="headerlink" title="PULASki: Learning inter-rater variability using statistical distances to   improve probabilistic segmentation"></a>PULASki: Learning inter-rater variability using statistical distances to   improve probabilistic segmentation</h2><p><strong>Authors:Soumick Chatterjee, Franziska Gaidzik, Alessandro Sciarra, Hendrik Mattern, GÃ¡bor Janiga, Oliver Speck, Andreas NÃ¼rnberger, Sahani Pathiraja</strong></p>
<p>In the domain of medical imaging, many supervised learning based methods for segmentation face several challenges such as high variability in annotations from multiple experts, paucity of labelled data and class imbalanced datasets. These issues may result in segmentations that lack the requisite precision for clinical analysis and can be misleadingly overconfident without associated uncertainty quantification. This work proposes the PULASki method as a computationally efficient generative tool for biomedical image segmentation that accurately captures variability in expert annotations, even in small datasets. This approach makes use of an improved loss function based on statistical distances in a conditional variational autoencoder structure (Probabilistic UNet), which improves learning of the conditional decoder compared to the standard cross-entropy particularly in class imbalanced problems. The proposed method was analysed for two structurally different segmentation tasks (intracranial vessel and multiple sclerosis (MS) lesion) and compare our results to four well-established baselines in terms of quantitative metrics and qualitative output. These experiments involve class-imbalanced datasets characterised by challenging features, including suboptimal signal-to-noise ratios and high ambiguity. Empirical results demonstrate the PULASKi method outperforms all baselines at the 5% significance level. Our experiments are also of the first to present a comparative study of the computationally feasible segmentation of complex geometries using 3D patches and the traditional use of 2D slices. The generated segmentations are shown to be much more anatomically plausible than in the 2D case, particularly for the vessel task. </p>
<blockquote>
<p>åœ¨åŒ»å­¦æˆåƒé¢†åŸŸï¼Œè®¸å¤šåŸºäºç›‘ç£å­¦ä¹ çš„åˆ†å‰²æ–¹æ³•é¢ä¸´å¤šé‡æŒ‘æˆ˜ï¼Œå¦‚å¤šä¸ªä¸“å®¶æ ‡æ³¨çš„é«˜å˜å¼‚æ€§ã€æ ‡è®°æ•°æ®ç¼ºä¹å’Œç±»åˆ«ä¸å¹³è¡¡æ•°æ®é›†ã€‚è¿™äº›é—®é¢˜å¯èƒ½å¯¼è‡´åˆ†å‰²ç»“æœç¼ºä¹ä¸´åºŠåˆ†ææ‰€éœ€çš„ç²¾åº¦ï¼Œå¹¶ä¸”åœ¨æ²¡æœ‰ç›¸åº”çš„ä¸ç¡®å®šæ€§é‡åŒ–çš„æƒ…å†µä¸‹ï¼Œå¯èƒ½ä¼šäº§ç”Ÿè¯¯å¯¼æ€§çš„è¿‡åº¦è‡ªä¿¡ã€‚è¿™é¡¹å·¥ä½œæå‡ºäº†PULASkiæ–¹æ³•ï¼Œä½œä¸ºä¸€ç§è®¡ç®—é«˜æ•ˆçš„ç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†å‰²ç”Ÿæˆå·¥å…·ï¼Œèƒ½å¤Ÿå‡†ç¡®æ•æ‰ä¸“å®¶æ ‡æ³¨çš„å˜å¼‚æ€§ï¼Œå³ä½¿åœ¨å°å‹æ•°æ®é›†ä¸­ä¹Ÿæ˜¯å¦‚æ­¤ã€‚è¯¥æ–¹æ³•åˆ©ç”¨åŸºäºæ¡ä»¶å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ç»“æ„ä¸­çš„ç»Ÿè®¡è·ç¦»çš„æ”¹è¿›æŸå¤±å‡½æ•°ï¼ˆæ¦‚ç‡Uç½‘ï¼‰ï¼Œæ”¹è¿›äº†æ¡ä»¶è§£ç å™¨çš„å­¦ä¹ ï¼Œç‰¹åˆ«æ˜¯åœ¨ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ä¸­ä¸æ ‡å‡†äº¤å‰ç†µç›¸æ¯”ã€‚å¯¹æ‰€æå‡ºçš„æ–¹æ³•è¿›è¡Œäº†ä¸¤ä¸ªç»“æ„ä¸åŒçš„åˆ†å‰²ä»»åŠ¡ï¼ˆé¢…å†…è¡€ç®¡å’Œå¤šå‘æ€§ç¡¬åŒ–ç—‡ï¼ˆMSï¼‰ç—…å˜ï¼‰çš„åˆ†æï¼Œå¹¶æ ¹æ®å®šé‡æŒ‡æ ‡å’Œå®šæ€§è¾“å‡ºä¸å››ä¸ªæˆç†Ÿçš„åŸºçº¿æ–¹æ³•è¿›è¡Œæ¯”è¾ƒã€‚è¿™äº›å®éªŒæ¶‰åŠå…·æœ‰æŒ‘æˆ˜æ€§çš„ç‰¹å¾ï¼ŒåŒ…æ‹¬æ¬¡ä¼˜çš„ä¿¡å™ªæ¯”å’Œé«˜æ¨¡ç³Šæ€§ã€‚ç»éªŒç»“æœè¡¨æ˜ï¼ŒPULASKiæ–¹æ³•åœ¨5%çš„æ˜¾è‘—æ€§æ°´å¹³ä¸Šä¼˜äºæ‰€æœ‰åŸºçº¿æ–¹æ³•ã€‚æˆ‘ä»¬çš„å®éªŒä¹Ÿæ˜¯é¦–æ¬¡å¯¹ä½¿ç”¨3Dè¡¥ä¸è¿›è¡Œå¤æ‚å‡ ä½•çš„å¯è¡Œåˆ†å‰²ä¸ä¼ ç»Ÿçš„ä½¿ç”¨2Dåˆ‡ç‰‡è¿›è¡Œæ¯”è¾ƒç ”ç©¶ã€‚ç”Ÿæˆçš„åˆ†å‰²ç»“æœæ¯”2Dæƒ…å†µä¸‹æ›´åŠ è§£å‰–ä¸Šåˆç†ï¼Œç‰¹åˆ«æ˜¯åœ¨è¡€ç®¡ä»»åŠ¡ä¸­ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.15686v2">PDF</a> </p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­ï¼ŒåŸºäºç›‘ç£å­¦ä¹ çš„æ–¹æ³•é¢ä¸´å¤šæ–¹æŒ‘æˆ˜ï¼Œå¦‚ä¸“å®¶æ ‡æ³¨å·®å¼‚å¤§ã€ç¼ºä¹æ ‡æ³¨æ•°æ®å’Œç±»åˆ«ä¸å‡è¡¡ç­‰é—®é¢˜ã€‚é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæœ¬ç ”ç©¶æå‡ºäº†PULASkiæ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ˜¯ä¸€ç§è®¡ç®—é«˜æ•ˆçš„ç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†å‰²å·¥å…·ï¼Œèƒ½å‡†ç¡®æ•æ‰ä¸“å®¶æ ‡æ³¨çš„å˜å¼‚æ€§ï¼Œå³ä½¿åœ¨å°æ•°æ®é›†ä¸‹ä¹Ÿèƒ½è¡¨ç°è‰¯å¥½ã€‚è¯¥æ–¹æ³•åŸºäºæ¡ä»¶å˜åˆ†è‡ªç¼–ç å™¨çš„ç»“æ„ï¼Œé‡‡ç”¨æ”¹è¿›çš„æŸå¤±å‡½æ•°ï¼Œåœ¨ç±»åˆ«ä¸å‡è¡¡é—®é¢˜ä¸­è¡¨ç°ä¼˜å¼‚ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é¢…å†…è¡€ç®¡å’Œå¤šå‘æ€§ç¡¬åŒ–ç—…å˜åˆ†å‰²ä»»åŠ¡ä¸Šä¼˜äºå…¶ä»–å››ç§åŸºçº¿æ–¹æ³•ï¼Œå¹¶ä¸”åœ¨å¤æ‚å‡ ä½•ç»“æ„çš„3Dåˆ‡ç‰‡åˆ†å‰²æ–¹é¢ä¹Ÿæœ‰è¾ƒå¥½è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­é¢ä¸´ä¸“å®¶æ ‡æ³¨å·®å¼‚å¤§ã€ç¼ºä¹æ ‡æ³¨æ•°æ®å’Œç±»åˆ«ä¸å‡è¡¡ç­‰æŒ‘æˆ˜ã€‚</li>
<li>PULASkiæ–¹æ³•æ˜¯ä¸€ç§é«˜æ•ˆçš„ç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†å‰²å·¥å…·ï¼Œå¯è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>PULASkiæ–¹æ³•é‡‡ç”¨åŸºäºæ¡ä»¶å˜åˆ†è‡ªç¼–ç å™¨çš„ç»“æ„ï¼Œç»“åˆæ”¹è¿›çš„æŸå¤±å‡½æ•°ï¼Œåœ¨ç±»åˆ«ä¸å‡è¡¡é—®é¢˜ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>PULASkiæ–¹æ³•åœ¨é¢…å†…è¡€ç®¡å’Œå¤šå‘æ€§ç¡¬åŒ–ç—…å˜åˆ†å‰²ä»»åŠ¡ä¸Šä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
<li>è¯¥ç ”ç©¶é¦–æ¬¡æ¯”è¾ƒäº†å¤æ‚å‡ ä½•ç»“æ„åˆ†å‰²çš„3Dåˆ‡ç‰‡å’Œä¼ ç»Ÿ2Dåˆ‡ç‰‡æ–¹æ³•ã€‚</li>
<li>PULASkiæ–¹æ³•ç”Ÿæˆçš„åˆ†å‰²ç»“æœæ›´åŠ ç¬¦åˆè§£å‰–å­¦ç»“æ„ï¼Œç‰¹åˆ«æ˜¯åœ¨è¡€ç®¡åˆ†å‰²ä»»åŠ¡ä¸Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2312.15686">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3534a3368642db9a2d60654a191b6cb2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c21ce96fade93bb6af9edc4e4f2dc19.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Uncertainty-Quantification-in-Machine-Learning-Based-Segmentation-A-Post-Hoc-Approach-for-Left-Ventricle-Volume-Estimation-in-MRI"><a href="#Uncertainty-Quantification-in-Machine-Learning-Based-Segmentation-A-Post-Hoc-Approach-for-Left-Ventricle-Volume-Estimation-in-MRI" class="headerlink" title="Uncertainty Quantification in Machine Learning Based Segmentation: A   Post-Hoc Approach for Left Ventricle Volume Estimation in MRI"></a>Uncertainty Quantification in Machine Learning Based Segmentation: A   Post-Hoc Approach for Left Ventricle Volume Estimation in MRI</h2><p><strong>Authors:F. Terhag, P. Knechtges, A. Basermann, R. Tempone</strong></p>
<p>Recent studies have confirmed cardiovascular diseases remain responsible for highest death toll amongst non-communicable diseases. Accurate left ventricular (LV) volume estimation is critical for valid diagnosis and management of various cardiovascular conditions, but poses significant challenge due to inherent uncertainties associated with segmentation algorithms in magnetic resonance imaging (MRI). Recent machine learning advancements, particularly U-Net-like convolutional networks, have facilitated automated segmentation for medical images, but struggles under certain pathologies and&#x2F;or different scanner vendors and imaging protocols. This study proposes a novel methodology for post-hoc uncertainty estimation in LV volume prediction using It^{o} stochastic differential equations (SDEs) to model path-wise behavior for the prediction error. The model describes the area of the left ventricle along the heartâ€™s long axis. The method is agnostic to the underlying segmentation algorithm, facilitating its use with various existing and future segmentation technologies. The proposed approach provides a mechanism for quantifying uncertainty, enabling medical professionals to intervene for unreliable predictions. This is of utmost importance in critical applications such as medical diagnosis, where prediction accuracy and reliability can directly impact patient outcomes. The method is also robust to dataset changes, enabling application for medical centers with limited access to labeled data. Our findings highlight the proposed uncertainty estimation methodologyâ€™s potential to enhance automated segmentation robustness and generalizability, paving the way for more reliable and accurate LV volume estimation in clinical settings as well as opening new avenues for uncertainty quantification in biomedical image segmentation, providing promising directions for future research. </p>
<blockquote>
<p>æœ€è¿‘çš„ç ”ç©¶å·²ç»ç¡®è®¤ï¼Œå¿ƒè¡€ç®¡ç–¾ç—…ä»æ˜¯å¯¼è‡´éä¼ æŸ“æ€§ç–¾ç—…æ­»äº¡äººæ•°æœ€å¤šçš„åŸå› ã€‚å·¦å¿ƒå®¤ï¼ˆLVï¼‰ä½“ç§¯çš„å‡†ç¡®ä¼°è®¡æ˜¯å„ç§å¿ƒè¡€ç®¡ç–¾ç—…çš„æœ‰æ•ˆè¯Šæ–­å’Œæ²»ç–—çš„å…³é”®ï¼Œä½†ç”±äºç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ä¸­åˆ†å‰²ç®—æ³•å›ºæœ‰çš„ä¸ç¡®å®šæ€§ï¼Œè¿™æ„æˆäº†ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚æœ€è¿‘çš„æœºå™¨å­¦ä¹ è¿›å±•ï¼Œå°¤å…¶æ˜¯U-Netç±»ä¼¼çš„å·ç§¯ç½‘ç»œï¼Œå·²ç»ä¿ƒè¿›äº†åŒ»ç–—å›¾åƒçš„è‡ªåŠ¨åˆ†å‰²ï¼Œä½†åœ¨æŸäº›ç—…ç†æƒ…å†µä¸‹å’Œ&#x2F;æˆ–ä¸åŒçš„æ‰«æä»ªä¾›åº”å•†å’Œæˆåƒåè®®ä¸‹ä»å­˜åœ¨å›°éš¾ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ä½¿ç”¨It^{o}éšæœºå¾®åˆ†æ–¹ç¨‹ï¼ˆSDEsï¼‰å¯¹å·¦å¿ƒå®¤ä½“ç§¯é¢„æµ‹è¿›è¡Œäº‹åä¸ç¡®å®šæ€§ä¼°è®¡çš„æ–°æ–¹æ³•ï¼Œä»¥æ¨¡æ‹Ÿé¢„æµ‹è¯¯å·®çš„è·¯å¾„è¡Œä¸ºã€‚è¯¥æ¨¡å‹æè¿°äº†å·¦å¿ƒå®¤æ²¿å¿ƒè„é•¿è½´çš„åŒºåŸŸã€‚è¯¥æ–¹æ³•å¯¹æ½œåœ¨çš„åˆ†å‰²ç®—æ³•æŒä¸­ç«‹æ€åº¦ï¼Œå¯ä»¥ä¸å„ç§ç°æœ‰å’Œæœªæ¥åˆ†å‰²æŠ€æœ¯ä¸€èµ·ä½¿ç”¨ã€‚æ‰€æå‡ºçš„æ–¹æ³•æä¾›äº†é‡åŒ–ä¸ç¡®å®šæ€§çš„æœºåˆ¶ï¼Œä½¿åŒ»ç–—ä¸“ä¸šäººå‘˜å¯ä»¥å¯¹ä¸å¯é çš„é¢„æµ‹è¿›è¡Œå¹²é¢„ã€‚åœ¨åŒ»ç–—è¯Šæ–­ç­‰å…³é”®åº”ç”¨ä¸­ï¼Œè¿™è‡³å…³é‡è¦ï¼Œé¢„æµ‹å‡†ç¡®æ€§å’Œå¯é æ€§ä¼šç›´æ¥å½±å“æ‚£è€…ç»“æœã€‚è¯¥æ–¹æ³•å¯¹æ•°æ®é›†çš„å˜åŒ–ä¹Ÿå¾ˆç¨³å¥ï¼Œé€‚ç”¨äºæœ‰é™è®¿é—®æ ‡è®°æ•°æ®çš„åŒ»å­¦ä¸­å¿ƒã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœçªå‡ºäº†æ‰€æå‡ºçš„ä¸ç¡®å®šæ€§ä¼°è®¡æ–¹æ³•åœ¨å¢å¼ºè‡ªåŠ¨åˆ†å‰²çš„ç¨³å¥æ€§å’Œé€šç”¨æ€§æ–¹é¢çš„æ½œåŠ›ï¼Œä¸ºä¸´åºŠç¯å¢ƒä¸­æ›´å¯é å’Œå‡†ç¡®çš„å·¦å¿ƒå®¤ä½“ç§¯ä¼°è®¡é“ºå¹³äº†é“è·¯ï¼ŒåŒæ—¶ä¹Ÿä¸ºç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„ä¸ç¡®å®šæ€§é‡åŒ–æ‰“å¼€äº†æ–°çš„é€”å¾„ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æœ‰å¸Œæœ›çš„æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02167v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºIt^{o}éšæœºå¾®åˆ†æ–¹ç¨‹ï¼ˆSDEsï¼‰çš„å·¦å¿ƒå®¤ä½“ç§¯é¢„æµ‹äº‹åä¸ç¡®å®šæ€§ä¼°è®¡æ–°æ–¹æ³•ã€‚æ­¤æ–¹æ³•æè¿°å·¦å¿ƒå®¤åœ¨å¿ƒè„é•¿è½´ä¸Šçš„åŒºåŸŸï¼Œå¯¹åŸºç¡€åˆ†å‰²ç®—æ³•æ— ç‰¹å®šè¦æ±‚ï¼Œå¯åº”ç”¨äºå„ç§ç°æœ‰å’Œæœªæ¥åˆ†å‰²æŠ€æœ¯ã€‚æ­¤æ–¹æ³•èƒ½å®šé‡è¯„ä¼°ä¸ç¡®å®šæ€§ï¼Œä½¿åŒ»ç–—ä¸“ä¸šäººå‘˜èƒ½å¯¹ä¸å¯é çš„é¢„æµ‹è¿›è¡Œå¹²é¢„ï¼Œå¯¹åŒ»ç–—è¯Šæ–­ç­‰å…³é”®åº”ç”¨è‡³å…³é‡è¦ã€‚è¯¥æ–¹æ³•å¯¹æ•°æ®é›†å˜åŒ–å…·æœ‰ç¨³å¥æ€§ï¼Œé€‚ç”¨äºåŒ»ç–—èµ„æºæœ‰é™çš„ä¸­å¿ƒã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¯¥ä¸ç¡®å®šæ€§ä¼°è®¡æ–¹æ³•æœ‰æœ›å¢å¼ºè‡ªåŠ¨åˆ†å‰²çš„ç¨³å¥æ€§å’Œé€šç”¨æ€§ï¼Œä¸ºä¸´åºŠç¯å¢ƒä¸­æ›´å¯é ã€å‡†ç¡®çš„å·¦å¿ƒå®¤ä½“ç§¯ä¼°è®¡é“ºå¹³é“è·¯ï¼ŒåŒæ—¶ä¸ºç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„ä¸ç¡®å®šæ€§é‡åŒ–æä¾›æ–°çš„ç ”ç©¶æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¿ƒè¡€ç®¡ç–¾ç—…åœ¨éä¼ æŸ“æ€§ç–¾ç—…ä¸­çš„æ­»äº¡äººæ•°å æ¯”æœ€é«˜ã€‚</li>
<li>å‡†ç¡®çš„å·¦å¿ƒå®¤ä½“ç§¯ä¼°è®¡æ˜¯å¿ƒè¡€ç®¡ç–¾ç—…è¯Šæ–­å’Œç®¡ç†çš„å…³é”®ã€‚</li>
<li>ç£å…±æŒ¯æˆåƒä¸­çš„åˆ†å‰²ç®—æ³•å­˜åœ¨ä¸ç¡®å®šæ€§ï¼Œç»™å·¦å¿ƒå®¤ä½“ç§¯ä¼°è®¡å¸¦æ¥æŒ‘æˆ˜ã€‚</li>
<li>æœ€è¿‘çš„æœºå™¨å­¦ä¹ è¿›å±•ï¼Œå°¤å…¶æ˜¯U-Netç±»ä¼¼çš„å·ç§¯ç½‘ç»œï¼Œä¿ƒè¿›äº†åŒ»å­¦å›¾åƒè‡ªåŠ¨åŒ–åˆ†å‰²çš„åº”ç”¨ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºIt^{o}éšæœºå¾®åˆ†æ–¹ç¨‹çš„æ–°å‹æ–¹æ³•ï¼Œç”¨äºäº‹åä¸ç¡®å®šæ€§ä¼°è®¡ã€‚</li>
<li>è¯¥æ–¹æ³•ç‹¬ç«‹äºåŸºç¡€åˆ†å‰²ç®—æ³•ï¼Œå¯ç”¨äºå„ç§ç°æœ‰å’Œæœªæ¥çš„åˆ†å‰²æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2312.02167">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f980ae7f8f3c4bdbc9803be2fab0a007.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-af5fe8cc1706d391f0aa471d95869361.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-65246be7ad1f3aca82bb51455a743e00.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-40a9b92c9139a5ba9b17a7a41f8794bf.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Spectral-wise-Implicit-Neural-Representation-for-Hyperspectral-Image-Reconstruction"><a href="#Spectral-wise-Implicit-Neural-Representation-for-Hyperspectral-Image-Reconstruction" class="headerlink" title="Spectral-wise Implicit Neural Representation for Hyperspectral Image   Reconstruction"></a>Spectral-wise Implicit Neural Representation for Hyperspectral Image   Reconstruction</h2><p><strong>Authors:Huan Chen, Wangcai Zhao, Tingfa Xu, Shiyun Zhou, Peifu Liu, Jianan Li</strong></p>
<p>Coded Aperture Snapshot Spectral Imaging (CASSI) reconstruction aims to recover the 3D spatial-spectral signal from 2D measurement. Existing methods for reconstructing Hyperspectral Image (HSI) typically involve learning mappings from a 2D compressed image to a predetermined set of discrete spectral bands. However, this approach overlooks the inherent continuity of the spectral information. In this study, we propose an innovative method called Spectral-wise Implicit Neural Representation (SINR) as a pioneering step toward addressing this limitation. SINR introduces a continuous spectral amplification process for HSI reconstruction, enabling spectral super-resolution with customizable magnification factors. To achieve this, we leverage the concept of implicit neural representation. Specifically, our approach introduces a spectral-wise attention mechanism that treats individual channels as distinct tokens, thereby capturing global spectral dependencies. Additionally, our approach incorporates two components, namely a Fourier coordinate encoder and a spectral scale factor module. The Fourier coordinate encoder enhances the SINRâ€™s ability to emphasize high-frequency components, while the spectral scale factor module guides the SINR to adapt to the variable number of spectral channels. Notably, the SINR framework enhances the flexibility of CASSI reconstruction by accommodating an unlimited number of spectral bands in the desired output. Extensive experiments demonstrate that our SINR outperforms baseline methods. By enabling continuous reconstruction within the CASSI framework, we take the initial stride toward integrating implicit neural representation into the field. </p>
<blockquote>
<p>ç¼–ç å­”å¾„å¿«ç…§å…‰è°±æˆåƒï¼ˆCASSIï¼‰é‡å»ºæ—¨åœ¨ä»äºŒç»´æµ‹é‡ä¸­æ¢å¤ä¸‰ç»´ç©ºé—´å…‰è°±ä¿¡å·ã€‚ç°æœ‰çš„è¶…å…‰è°±å›¾åƒï¼ˆHSIï¼‰é‡å»ºæ–¹æ³•é€šå¸¸æ¶‰åŠä»äºŒç»´å‹ç¼©å›¾åƒå­¦ä¹ æ˜ å°„åˆ°é¢„å…ˆç¡®å®šçš„ä¸€ç»„ç¦»æ•£å…‰è°±å¸¦ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•å¿½ç•¥äº†å…‰è°±ä¿¡æ¯çš„å†…åœ¨è¿ç»­æ€§ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ›æ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºå…‰è°±éšç¥ç»è¡¨ç¤ºï¼ˆSINRï¼‰ï¼Œä½œä¸ºè§£å†³è¿™ä¸€é™åˆ¶çš„å¼€åˆ›æ€§æ­¥éª¤ã€‚SINRå¼•å…¥äº†ä¸€ä¸ªè¿ç»­å…‰è°±æ”¾å¤§è¿‡ç¨‹ï¼Œç”¨äºHSIé‡å»ºï¼Œå®ç°äº†å…·æœ‰å¯å®šåˆ¶æ”¾å¤§å€æ•°çš„å…‰è°±è¶…åˆ†è¾¨ç‡ã€‚ä¸ºå®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬åˆ©ç”¨äº†éšç¥ç»è¡¨ç¤ºçš„æ¦‚å¿µã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸€ç§å…‰è°±æ³¨æ„æœºåˆ¶ï¼Œå°†å•ä¸ªé€šé“è§†ä¸ºä¸åŒçš„æ ‡è®°ï¼Œä»è€Œæ•è·å…¨å±€å…‰è°±ä¾èµ–æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬ä¸¤ä¸ªç»„ä»¶ï¼Œå³å‚…ç«‹å¶åæ ‡ç¼–ç å™¨å’Œå…‰è°±å°ºåº¦å› å­æ¨¡å—ã€‚å‚…ç«‹å¶åæ ‡ç¼–ç å™¨å¢å¼ºäº†SINRå¯¹é«˜é¢‘åˆ†é‡çš„å¼ºè°ƒèƒ½åŠ›ï¼Œè€Œå…‰è°±å°ºåº¦å› å­æ¨¡å—æŒ‡å¯¼SINRé€‚åº”å¯å˜æ•°é‡çš„å…‰è°±é€šé“ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒSINRæ¡†æ¶é€šè¿‡é€‚åº”è¾“å‡ºæ‰€éœ€çš„æ— é™æ•°é‡çš„å…‰è°±å¸¦ï¼Œå¢å¼ºäº†CASSIé‡å»ºçš„çµæ´»æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„SINRä¼˜äºåŸºçº¿æ–¹æ³•ã€‚é€šè¿‡åœ¨CASSIæ¡†æ¶å†…å®ç°è¿ç»­é‡å»ºï¼Œæˆ‘ä»¬è¿ˆå‡ºäº†å°†éšç¥ç»è¡¨ç¤ºæ•´åˆåˆ°è¯¥é¢†åŸŸçš„åˆæ­¥å°è¯•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.01061v2">PDF</a> Accepted by IEEE Transactions on Circuits and Systems for Video   Technology, has been published</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºéšå¼ç¥ç»ç½‘ç»œè¡¨ç¤ºçš„Spectral-wise Implicit Neural Representationï¼ˆSINRï¼‰æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³Coded Aperture Snapshot Spectral Imagingï¼ˆCASSIï¼‰é‡å»ºä¸­çš„è¶…å…‰è°±å›¾åƒé‡å»ºé—®é¢˜ã€‚è¯¥æ–¹æ³•å¼•å…¥è¿ç»­å…‰è°±æ”¾å¤§è¿‡ç¨‹ï¼Œå®ç°å…‰è°±è¶…åˆ†è¾¨ç‡å¹¶å¯è‡ªå®šä¹‰æ”¾å¤§å€æ•°ã€‚é€šè¿‡å¼•å…¥å…‰è°±æ³¨æ„æœºåˆ¶ï¼ŒSINRèƒ½å¤Ÿæ•æ‰å…¨å±€å…‰è°±ä¾èµ–æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†å‚…é‡Œå¶åæ ‡ç¼–ç å™¨å’Œå…‰è°±å°ºåº¦å› å­æ¨¡å—ï¼Œæé«˜äº†å¯¹é«˜é¢‘æˆåˆ†çš„å¼ºè°ƒå’Œé€‚åº”æ€§ã€‚æ€»ä½“è€Œè¨€ï¼ŒSINRå¢å¼ºäº†CASSIé‡å»ºçš„çµæ´»æ€§ï¼Œèƒ½å¤Ÿåº”å¯¹æ— é™æ•°é‡çš„å…‰è°±é€šé“è¾“å‡ºï¼Œä¸”åœ¨å®éªŒä¸­è¡¨ç°ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SINRæ˜¯ä¸€ç§åˆ›æ–°çš„è¶…å…‰è°±å›¾åƒé‡å»ºæ–¹æ³•ï¼Œç”¨äºè§£å†³CASSIé‡å»ºä¸­çš„è¿ç»­å…‰è°±æ”¾å¤§é—®é¢˜ã€‚</li>
<li>é€šè¿‡å¼•å…¥å…‰è°±æ³¨æ„æœºåˆ¶ï¼ŒSINRèƒ½å¤Ÿæ•æ‰å…¨å±€å…‰è°±ä¾èµ–æ€§ã€‚</li>
<li>å‚…é‡Œå¶åæ ‡ç¼–ç å™¨å¢å¼ºäº†SINRå¯¹é«˜é¢‘æˆåˆ†çš„å¼ºè°ƒèƒ½åŠ›ã€‚</li>
<li>å…‰è°±å°ºåº¦å› å­æ¨¡å—ä½¿SINRèƒ½å¤Ÿé€‚åº”å¯å˜æ•°é‡çš„å…‰è°±é€šé“ã€‚</li>
<li>SINRæ¡†æ¶å¢å¼ºäº†CASSIé‡å»ºçš„çµæ´»æ€§ï¼Œå¯ä»¥å¤„ç†æ— é™æ•°é‡çš„å…‰è°±é€šé“è¾“å‡ºã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒSINRåœ¨æ€§èƒ½ä¸Šä¼˜äºä¼ ç»Ÿçš„HSIé‡å»ºæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2312.01061">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cb3aec09084ceabc52ccae0d75493a7a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c83c2a782705abe00bc290b6b9c8cd4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-54220232b45b5bf2e409f108f7a72050.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5ad676dd48397375461118ca03343720.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Co-Learning-Semantic-aware-Unsupervised-Segmentation-for-Pathological-Image-Registration"><a href="#Co-Learning-Semantic-aware-Unsupervised-Segmentation-for-Pathological-Image-Registration" class="headerlink" title="Co-Learning Semantic-aware Unsupervised Segmentation for Pathological   Image Registration"></a>Co-Learning Semantic-aware Unsupervised Segmentation for Pathological   Image Registration</h2><p><strong>Authors:Yang Liu, Shi Gu</strong></p>
<p>The registration of pathological images plays an important role in medical applications. Despite its significance, most researchers in this field primarily focus on the registration of normal tissue into normal tissue. The negative impact of focal tissue, such as the loss of spatial correspondence information and the abnormal distortion of tissue, are rarely considered. In this paper, we propose GIRNet, a novel unsupervised approach for pathological image registration by incorporating segmentation and inpainting through the principles of Generation, Inpainting, and Registration (GIR). The registration, segmentation, and inpainting modules are trained simultaneously in a co-learning manner so that the segmentation of the focal area and the registration of inpainted pairs can improve collaboratively. Overall, the registration of pathological images is achieved in a completely unsupervised learning framework. Experimental results on multiple datasets, including Magnetic Resonance Imaging (MRI) of T1 sequences, demonstrate the efficacy of our proposed method. Our results show that our method can accurately achieve the registration of pathological images and identify lesions even in challenging imaging modalities. Our unsupervised approach offers a promising solution for the efficient and cost-effective registration of pathological images. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/brain-intelligence-lab/GIRNet">https://github.com/brain-intelligence-lab/GIRNet</a>. </p>
<blockquote>
<p>ç—…ç†å›¾åƒçš„é…å‡†åœ¨åŒ»ç–—åº”ç”¨ä¸­æ‰®æ¼”ç€é‡è¦è§’è‰²ã€‚å°½ç®¡å…¶æ„ä¹‰é‡å¤§ï¼Œä½†è¯¥é¢†åŸŸçš„å¤§å¤šæ•°ç ”ç©¶è€…ä¸»è¦å…³æ³¨æ­£å¸¸ç»„ç»‡åˆ°æ­£å¸¸ç»„ç»‡çš„é…å‡†ã€‚å¾ˆå°‘è€ƒè™‘ç„¦ç‚¹ç»„ç»‡çš„è´Ÿé¢å½±å“ï¼Œå¦‚ç©ºé—´å¯¹åº”å…³ç³»ä¿¡æ¯çš„ä¸¢å¤±å’Œç»„ç»‡å¼‚å¸¸æ‰­æ›²ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†GIRNetï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æ— ç›‘ç£ç—…ç†å›¾åƒé…å‡†æ–¹æ³•ï¼Œé€šè¿‡ç»“åˆåˆ†å‰²å’Œè¡¥å…¨æŠ€æœ¯ï¼Œéµå¾ªç”Ÿæˆã€è¡¥å…¨å’Œé…å‡†ï¼ˆGIRï¼‰çš„åŸåˆ™ã€‚é…å‡†ã€åˆ†å‰²å’Œè¡¥å…¨æ¨¡å—ä»¥ååŒå­¦ä¹ çš„æ–¹å¼è¿›è¡Œè®­ç»ƒï¼Œä»¥ä¾¿ç„¦ç‚¹åŒºåŸŸçš„åˆ†å‰²å’Œè¡¥å…¨å¯¹çš„é…å‡†èƒ½å¤ŸååŒæ”¹è¿›ã€‚æ€»çš„æ¥è¯´ï¼Œç—…ç†å›¾åƒçš„é…å‡†æ˜¯åœ¨ä¸€ä¸ªå®Œå…¨æ— ç›‘ç£çš„å­¦ä¹ æ¡†æ¶ä¸­å®ç°çš„ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœï¼ŒåŒ…æ‹¬T1åºåˆ—çš„ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ï¼Œè¯æ˜äº†æˆ‘ä»¬æå‡ºçš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥å‡†ç¡®åœ°å®ç°ç—…ç†å›¾åƒçš„é…å‡†ï¼Œç”šè‡³åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æˆåƒæ¨¡å¼ä¸Šä¹Ÿèƒ½è¯†åˆ«ç—…å˜ã€‚æˆ‘ä»¬çš„æ— ç›‘ç£æ–¹æ³•ä¸ºå®ç°é«˜æ•ˆä¸”ç»æµçš„ç—…ç†å›¾åƒé…å‡†æä¾›äº†æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨ <a target="_blank" rel="noopener" href="https://github.com/brain-intelligence-lab/GIRNet">https://github.com/brain-intelligence-lab/GIRNet</a> è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11040v3">PDF</a> 13 pages, 7 figures, published in Medical Image Computing and   Computer Assisted Intervention (MICCAI) 2023</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç”Ÿæˆã€ä¿®å¤å’Œæ³¨å†Œï¼ˆGIRï¼‰åŸåˆ™çš„æ–°å‹æ— ç›‘ç£ç—…ç†å›¾åƒæ³¨å†Œæ–¹æ³•GIRNetã€‚è¯¥æ–¹æ³•åŒæ—¶è®­ç»ƒæ³¨å†Œã€åˆ†å‰²å’Œä¿®å¤æ¨¡å—ï¼Œå®ç°å¯¹ç„¦ç‚¹åŒºåŸŸçš„åˆ†å‰²å’Œå¯¹ä¿®å¤å¯¹å›¾åƒçš„æ³¨å†Œï¼Œè¿›è€Œæé«˜ååŒæ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯æœ‰æ•ˆå®ç°ç—…ç†å›¾åƒçš„æ³¨å†Œï¼Œç”šè‡³åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æˆåƒæ¨¡å¼ä¸‹ä¹Ÿèƒ½è¯†åˆ«ç—…å˜ã€‚è¯¥æ— ç›‘ç£æ–¹æ³•ä¸ºç—…ç†å›¾åƒçš„æ•ˆç‡å’Œæˆæœ¬æ•ˆç›Šæä¾›äº†æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç—…ç†å›¾åƒæ³¨å†Œåœ¨åŒ»å­¦åº”ç”¨ä¸­çš„é‡è¦æ€§è¢«å¼ºè°ƒï¼Œä½†ç°æœ‰ç ”ç©¶ä¸»è¦å…³æ³¨æ­£å¸¸ç»„ç»‡çš„æ³¨å†Œï¼Œå¿½ç•¥äº†ç„¦ç‚¹ç»„ç»‡çš„è´Ÿé¢å½±å“ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„æ— ç›‘ç£ç—…ç†å›¾åƒæ³¨å†Œæ–¹æ³•GIRNetï¼Œè¯¥æ–¹æ³•ç»“åˆäº†åˆ†å‰²å’Œä¿®å¤æŠ€æœ¯ã€‚</li>
<li>GIRNeté€šè¿‡åŒæ—¶è®­ç»ƒæ³¨å†Œã€åˆ†å‰²å’Œä¿®å¤æ¨¡å—ï¼Œå®ç°äº†å¯¹ç„¦ç‚¹åŒºåŸŸçš„ååŒæ”¹è¿›ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒGIRNetåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬T1åºåˆ—çš„ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ã€‚</li>
<li>GIRNetèƒ½å‡†ç¡®å®ç°ç—…ç†å›¾åƒçš„æ³¨å†Œï¼Œå¹¶åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æˆåƒæ¨¡å¼ä¸‹è¯†åˆ«ç—…å˜ã€‚</li>
<li>æ— ç›‘ç£çš„GIRNetæ–¹æ³•ä¸ºé«˜æ•ˆä¸”ç»æµçš„ç—…ç†å›¾åƒæ³¨å†Œæä¾›äº†è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2310.11040">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d0b752b2690a964f4ff37d51c64f21fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19c4512bf5070012829b402135127eb8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7a2becdd260ae923525ea4e9ed4ba656.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-21/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-21/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-21/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-52d94899ad04465abb9b14edad72337d.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-21  MoonCast High-Quality Zero-Shot Podcast Generation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-21
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-21/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-762573e30cb534041dcb372a9e017808.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-21  FP4DiT Towards Effective Floating Point Quantization for Diffusion   Transformers
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-21
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">25156.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
