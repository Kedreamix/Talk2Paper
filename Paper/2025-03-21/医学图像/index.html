<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="医学图像">
    <meta name="description" content="医学图像 方向最新论文已更新，请持续关注 Update in 2025-03-21  FedSCA Federated Tuning with Similarity-guided Collaborative   Aggregation for Heterogeneous Medical Image Segmentation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>医学图像 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-430b460135811c7ede99b5a295a3a404.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">医学图像</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">医学图像</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                医学图像
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-03-21
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    18.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    75 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-03-21-更新"><a href="#2025-03-21-更新" class="headerlink" title="2025-03-21 更新"></a>2025-03-21 更新</h1><h2 id="FedSCA-Federated-Tuning-with-Similarity-guided-Collaborative-Aggregation-for-Heterogeneous-Medical-Image-Segmentation"><a href="#FedSCA-Federated-Tuning-with-Similarity-guided-Collaborative-Aggregation-for-Heterogeneous-Medical-Image-Segmentation" class="headerlink" title="FedSCA: Federated Tuning with Similarity-guided Collaborative   Aggregation for Heterogeneous Medical Image Segmentation"></a>FedSCA: Federated Tuning with Similarity-guided Collaborative   Aggregation for Heterogeneous Medical Image Segmentation</h2><p><strong>Authors:Yumin Zhang, Yan Gao, Haoran Duan, Hanqing Guo, Tejal Shah, Rajiv Ranjan, Bo Wei</strong></p>
<p>Transformer-based foundation models (FMs) have recently demonstrated remarkable performance in medical image segmentation. However, scaling these models is challenging due to the limited size of medical image datasets within isolated hospitals, where data centralization is restricted due to privacy concerns. These constraints, combined with the data-intensive nature of FMs, hinder their broader application. Integrating federated learning (FL) with foundation models (FLFM) fine-tuning offers a potential solution to these challenges by enabling collaborative model training without data sharing, thus allowing FMs to take advantage of a diverse pool of sensitive medical image data across hospitals&#x2F;clients. However, non-independent and identically distributed (non-IID) data among clients, paired with computational and communication constraints in federated environments, presents an additional challenge that limits further performance improvements and remains inadequately addressed in existing studies. In this work, we propose a novel FLFM fine-tuning framework, \underline{\textbf{Fed}}erated tuning with \underline{\textbf{S}}imilarity-guided \underline{\textbf{C}}ollaborative \underline{\textbf{A}}ggregation (FedSCA), encompassing all phases of the FL process. This includes (1) specially designed parameter-efficient fine-tuning (PEFT) for local client training to enhance computational efficiency; (2) partial low-level adapter transmission for communication efficiency; and (3) similarity-guided collaborative aggregation (SGCA) on the server side to address non-IID issues. Extensive experiments on three FL benchmarks for medical image segmentation demonstrate the effectiveness of our proposed FedSCA, establishing new SOTA performance. </p>
<blockquote>
<p>基于Transformer的基础模型（FMs）在医学图像分割方面最近表现出了显著的性能。然而，由于孤立医院中医疗图像数据集规模的限制，以及隐私担忧导致的数据集中化受限，这些模型的规模扩大具有挑战性。这些约束，结合FMs数据密集型的特性，阻碍了其更广泛的应用。将联邦学习（FL）与基础模型（FLFM）微调相结合，提供了一种通过协同模型训练实现无需数据共享的解决方案，从而允许FMs利用跨医院&#x2F;客户端的多样化敏感医疗图像数据。然而，客户端之间的非独立同分布（non-IID）数据，以及联邦环境中的计算和通信约束，呈现了一个额外的挑战，限制了性能的进一步提高，并且在现有研究中仍未得到充分的解决。在这项工作中，我们提出了一种新型的FLFM微调框架——基于相似性引导的协同聚合的联邦调优（FedSCA），涵盖了FL过程的各个阶段。这包括（1）为本地客户端训练设计的参数高效微调（PEFT），以提高计算效率；（2）部分低级适配器传输以提高通信效率；（3）服务器端相似性引导的协同聚合（SGCA）以解决非IID问题。在三个医学图像分割的联邦学习基准测试上的广泛实验证明了我们提出的FedSCA的有效性，并建立了新的性能最佳记录。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15390v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种基于联邦学习（FL）和深度学习基础模型（FMs）的新型精细调节框架FedSCA，针对医院间独立分布的数据集小且非独立同分布（non-IID）的问题，实现参数高效的本地训练，同时利用相似度引导的协同聚合解决non-IID问题，确保医学图像分割在联邦环境下的高性能运行。此框架已在三个联邦学习基准测试上进行了广泛实验验证，并达到了最先进的性能表现。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>联邦学习（FL）与深度学习基础模型（FMs）结合解决了医院数据规模小和数据隐私的挑战。</li>
<li>非独立同分布（non-IID）数据是联邦学习中的一大难题，影响了模型性能的提升。</li>
<li>提出了一种新型联邦学习精细调节框架FedSCA，包含参数高效的本地训练、部分低级适配器传输和相似度引导的协同聚合。</li>
<li>FedSCA框架通过专门设计的参数高效精细调节（PEFT）提升计算效率。</li>
<li>通过部分低级适配器传输提高通信效率。</li>
<li>服务器端的相似度引导协同聚合（SGCA）解决了non-IID问题。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15390">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-1b64108bc9c44c83421cec197c43b35a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-64e67f2509a30a723a93fb17c9e463e1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c0bd207fda033accbe66b9e0edf13a3e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6a24aa0ad1e9b67b2b6c02b1edf729c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a9a483f52eac6b9e1611e505cb7c508.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-afbf8c52241a8e257f161893fda280d0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="DEPT-Deep-Extreme-Point-Tracing-for-Ultrasound-Image-Segmentation"><a href="#DEPT-Deep-Extreme-Point-Tracing-for-Ultrasound-Image-Segmentation" class="headerlink" title="DEPT: Deep Extreme Point Tracing for Ultrasound Image Segmentation"></a>DEPT: Deep Extreme Point Tracing for Ultrasound Image Segmentation</h2><p><strong>Authors:Lei Shi, Xi Fang, Naiyu Wang, Junxing Zhang</strong></p>
<p>Automatic medical image segmentation plays a crucial role in computer aided diagnosis. However, fully supervised learning approaches often require extensive and labor-intensive annotation efforts. To address this challenge, weakly supervised learning methods, particularly those using extreme points as supervisory signals, have the potential to offer an effective solution. In this paper, we introduce Deep Extreme Point Tracing (DEPT) integrated with Feature-Guided Extreme Point Masking (FGEPM) algorithm for ultrasound image segmentation. Notably, our method generates pseudo labels by identifying the lowest-cost path that connects all extreme points on the feature map-based cost matrix. Additionally, an iterative training strategy is proposed to refine pseudo labels progressively, enabling continuous network improvement. Experimental results on two public datasets demonstrate the effectiveness of our proposed method. The performance of our method approaches that of the fully supervised method and outperforms several existing weakly supervised methods. </p>
<blockquote>
<p>在医学图像诊断辅助中，自动医学图像分割发挥着关键作用。然而，完全监督学习方法通常需要大量且劳动强度大的标注工作。为了应对这一挑战，弱监督学习方法，特别是使用极端点作为监督信号的弱监督学习方法，具有提供有效解决方案的潜力。在本文中，我们引入了基于特征引导极端点遮蔽算法（FGEPM）的深度极值点追踪（DEPT）方法，用于超声图像分割。值得注意的是，我们的方法通过识别基于特征映射的成本矩阵上所有极端点之间的最低成本路径来生成伪标签。此外，提出了一种迭代训练策略来逐步优化伪标签，从而实现网络的持续改进。在两个公共数据集上的实验结果证明了所提出方法的有效性。该方法的性能接近全监督方法，优于其他一些现有的弱监督方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15260v1">PDF</a> </p>
<p><strong>Summary</strong><br>     本文介绍了一种结合深度极端点追踪（DEPT）和特征引导极端点掩模（FGEPM）算法的超声图像分割方法。该方法通过识别基于特征映射成本矩阵上所有极端点的最低成本路径来生成伪标签，并采用迭代训练策略逐步优化伪标签，从而提高网络性能。在公共数据集上的实验结果表明，该方法的有效性接近全监督方法，并优于一些现有的弱监督方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自动医学图像分割在计算机辅助诊断中起关键作用，但全监督学习方法需要大量人工标注，存在劳动密集型问题。</li>
<li>弱监督学习方法，特别是使用极端点作为监督信号的方法，为解决这一问题提供了有效潜力。</li>
<li>本文提出结合深度极端点追踪（DEPT）和特征引导极端点掩模（FGEPM）算法的超声图像分割方法。</li>
<li>该方法通过识别基于特征映射成本矩阵上所有极端点的最低成本路径来生成伪标签。</li>
<li>采用迭代训练策略逐步优化伪标签，有助于提高网络性能。</li>
<li>在公共数据集上的实验结果表明，该方法的有效性接近全监督方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15260">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-dc3b2d9754cc535c039b8bb8558db330.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b34342f1592710054ed7f5c79a39ba6d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-89a4fea9f53a434652ad550a1ea32768.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3520a9c92094fa0c08a265027f641331.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-509c13d2541a2fbf376fea0541f654c4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d0ddf98a8439d4da479a27db8d8b5044.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-028196cfa132832ce1152de8f167c933.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Exploring-Large-Language-Models-for-Word-Games-Who-is-the-Spy"><a href="#Exploring-Large-Language-Models-for-Word-Games-Who-is-the-Spy" class="headerlink" title="Exploring Large Language Models for Word Games:Who is the Spy?"></a>Exploring Large Language Models for Word Games:Who is the Spy?</h2><p><strong>Authors:Chentian Wei, Jiewei Chen, Jinzhu Xu</strong></p>
<p>Word games hold significant research value for natural language processing (NLP), game theory, and related fields due to their rule-based and situational nature. This study explores how large language models (LLMs) can be effectively involved in word games and proposes a training-free framework. “Shei Shi Wo Di” or “Who is the Spy” in English, is a classic word game. Using this game as an example, we introduce a Chain-of-Thought (CoT)-based scheduling framework to enable LLMs to achieve excellent performance in tasks such as inferring role words and disguising their identities. We evaluate the framework’s performance based on game success rates and the accuracy of the LLM agents’ analytical results. Experimental results affirm the framework’s effectiveness, demonstrating notable improvements in LLM performance across multiple datasets. This work highlights the potential of LLMs in mastering situational reasoning and social interactions within structured game environments. Our code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/ct-wei/Who-is-The-Spy">https://github.com/ct-wei/Who-is-The-Spy</a>. </p>
<blockquote>
<p>文字游戏因其基于规则和情境的特性，对自然语言处理（NLP）、博弈论和相关领域具有重要的研究价值。本研究探讨了大型语言模型（LLM）如何有效地参与文字游戏，并提出了一个无需训练框架。 “谁是间谍”（Shei Shi Wo Di）是英文文字游戏“谁是卧底”的一种经典玩法。以这款游戏为例，我们引入了一种基于思维链（Chain-of-Thought，CoT）的调度框架，使LLM能够在推断角色词和伪装身份等任务中取得卓越表现。我们根据游戏成功率和LLM代理人的分析结果准确性来评估框架的性能。实验结果证实了框架的有效性，并在多个数据集上显示出LLM性能的显著改进。这项工作突出了LLM在掌握结构化游戏环境中的情境推理和社会交互方面的潜力。我们的代码公开在<a target="_blank" rel="noopener" href="https://github.com/ct-wei/Who-is-The-Spy%E3%80%82">https://github.com/ct-wei/Who-is-The-Spy。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15235v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文研究了大型语言模型（LLMs）在字谜游戏“谁是我间谍”（Shei Shi Wo Di）中的应用，提出了一种基于思维链（Chain-of-Thought，CoT）的调度框架，以提高LLMs在角色词推断和身份伪装等任务中的性能。实验结果表明，该框架能有效提高LLMs在多个数据集上的表现。本文强调了LLMs在掌握结构化游戏环境中的情境推理和社会交互方面的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）在字谜游戏中的研究具有重要价值。</li>
<li>以“谁是我间谍”为例，展示了LLMs在角色词推断和身份伪装任务中的性能提升。</li>
<li>提出了一种基于思维链（Chain-of-Thought，CoT）的调度框架，以提高LLMs在游戏中的表现。</li>
<li>通过实验验证了框架的有效性，并展示了其在多个数据集上的显著改进。</li>
<li>该框架对提升LLMs的情境推理和社会交互能力有潜力。</li>
<li>该研究的代码已公开，供公众参考与学习。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15235">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-568844cd95ab840b742f689409202abc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8bd9ba651de03eb8ffac3d9d5e80b649.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a30d32091f1b8e54625f2d59d773a7e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de845e2089067ad19bb6271f6b7ba752.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="3D-Occupancy-Prediction-with-Low-Resolution-Queries-via-Prototype-aware-View-Transformation"><a href="#3D-Occupancy-Prediction-with-Low-Resolution-Queries-via-Prototype-aware-View-Transformation" class="headerlink" title="3D Occupancy Prediction with Low-Resolution Queries via Prototype-aware   View Transformation"></a>3D Occupancy Prediction with Low-Resolution Queries via Prototype-aware   View Transformation</h2><p><strong>Authors:Gyeongrok Oh, Sungjune Kim, Heeju Ko, Hyung-gun Chi, Jinkyu Kim, Dongwook Lee, Daehyun Ji, Sungjoon Choi, Sujin Jang, Sangpil Kim</strong></p>
<p>The resolution of voxel queries significantly influences the quality of view transformation in camera-based 3D occupancy prediction. However, computational constraints and the practical necessity for real-time deployment require smaller query resolutions, which inevitably leads to an information loss. Therefore, it is essential to encode and preserve rich visual details within limited query sizes while ensuring a comprehensive representation of 3D occupancy. To this end, we introduce ProtoOcc, a novel occupancy network that leverages prototypes of clustered image segments in view transformation to enhance low-resolution context. In particular, the mapping of 2D prototypes onto 3D voxel queries encodes high-level visual geometries and complements the loss of spatial information from reduced query resolutions. Additionally, we design a multi-perspective decoding strategy to efficiently disentangle the densely compressed visual cues into a high-dimensional 3D occupancy scene. Experimental results on both Occ3D and SemanticKITTI benchmarks demonstrate the effectiveness of the proposed method, showing clear improvements over the baselines. More importantly, ProtoOcc achieves competitive performance against the baselines even with 75% reduced voxel resolution. </p>
<blockquote>
<p>基于相机的三维占用预测中，体素查询的分辨率对视图转换的质量有重要影响。然而，计算约束和实时部署的实际需求要求较小的查询分辨率，这不可避免地导致信息丢失。因此，在有限的查询大小内编码和保留丰富的视觉细节至关重要，同时还要确保三维占用的全面表示。为此，我们引入了ProtoOcc，这是一种新型占用网络，它利用聚类图像段的原型进行视图转换，以增强低分辨率上下文。特别是，将二维原型映射到三维体素查询上，可以编码高级视觉几何特征，并补充因降低查询分辨率而损失的空间信息。此外，我们还设计了一种多视角解码策略，能够高效地将密集压缩的视觉线索解纠缠为高维的三维占用场景。在Occ3D和SemanticKITTI基准测试上的实验结果表明了该方法的有效性，与基线相比有明显的改进。更重要的是，即使在体素分辨率降低75%的情况下，ProtoOcc仍能达到与基线竞争的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15185v1">PDF</a> Accepted to CVPR2025</p>
<p><strong>Summary</strong><br>     高分辨率的体素查询对基于相机的三维占用预测中的视图转换质量有重要影响。由于计算约束和实时部署的实际需求，通常需要较小的查询分辨率，这会导致信息损失。因此，ProtoOcc作为一种新型占用网络，通过利用聚类图像段的原型进行视图转换，以在低分辨率上下文中增强信息。具体而言，将二维原型映射到三维体素查询中，编码高级视觉几何信息，并补充因查询分辨率降低而损失的空间信息。此外，设计了一种多视角解码策略，以有效地将密集压缩的视觉线索解纠缠为高维度的三维占用场景。实验结果表明，该方法在Occ3D和SemanticKITTI基准测试上均表现出有效性，即使在体素分辨率降低75%的情况下，ProtoOcc仍实现与基线方法相当的竞争力。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>分辨率对视图转换质量有影响。</li>
<li>小查询分辨率导致信息损失。</li>
<li>ProtoOcc利用图像段原型以增强低分辨率上下文中的信息。</li>
<li>二维原型映射到三维体素查询以编码高级视觉几何信息。</li>
<li>多视角解码策略可有效解纠缠视觉线索以呈现三维占用场景。</li>
<li>ProtoOcc在基准测试中表现优异，并在低分辨率下仍具有竞争力。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15185">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e1c407cc3e69f35fc0c0d75907527377.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bc702290723867505a028c53ffe88668.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d90d4259be9b100c5995510f6fa07edc.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Texture-Aware-StarGAN-for-CT-data-harmonisation"><a href="#Texture-Aware-StarGAN-for-CT-data-harmonisation" class="headerlink" title="Texture-Aware StarGAN for CT data harmonisation"></a>Texture-Aware StarGAN for CT data harmonisation</h2><p><strong>Authors:Francesco Di Feola, Ludovica Pompilio, Cecilia Assolito, Valerio Guarrasi, Paolo Soda</strong></p>
<p>Computed Tomography (CT) plays a pivotal role in medical diagnosis; however, variability across reconstruction kernels hinders data-driven approaches, such as deep learning models, from achieving reliable and generalized performance. To this end, CT data harmonization has emerged as a promising solution to minimize such non-biological variances by standardizing data across different sources or conditions. In this context, Generative Adversarial Networks (GANs) have proved to be a powerful framework for harmonization, framing it as a style-transfer problem. However, GAN-based approaches still face limitations in capturing complex relationships within the images, which are essential for effective harmonization. In this work, we propose a novel texture-aware StarGAN for CT data harmonization, enabling one-to-many translations across different reconstruction kernels. Although the StarGAN model has been successfully applied in other domains, its potential for CT data harmonization remains unexplored. Furthermore, our approach introduces a multi-scale texture loss function that embeds texture information across different spatial and angular scales into the harmonization process, effectively addressing kernel-induced texture variations. We conducted extensive experimentation on a publicly available dataset, utilizing a total of 48667 chest CT slices from 197 patients distributed over three different reconstruction kernels, demonstrating the superiority of our method over the baseline StarGAN. </p>
<blockquote>
<p>计算机断层扫描（CT）在医学诊断中起着至关重要的作用；然而，重建核的差异性阻碍了数据驱动的方法（如深度学习模型）实现可靠和通用的性能。为此，CT数据调和已成为一种有前途的解决方案，通过标准化不同源或条件下的数据，以尽量减少这种非生物性差异。在这种情况下，生成对抗网络（GANs）已被证明是调和的有力框架，将其构造成风格转换问题。然而，基于GAN的方法在捕获图像内的复杂关系方面仍然存在局限性，这对于有效的调和至关重要。在这项工作中，我们提出了一种用于CT数据调和的新型纹理感知StarGAN，能够实现不同重建核之间的一对多翻译。虽然StarGAN模型已在其他领域成功应用，但其对CT数据调和的潜力仍未被探索。此外，我们的方法引入了一种多尺度纹理损失函数，该函数将不同空间和角度尺度上的纹理信息嵌入到调和过程中，有效地解决了核引起的纹理变化。我们在公开数据集上进行了广泛实验，使用了来自197名患者的总共48667张胸部CT切片，这些切片分布在三种不同的重建核上，证明了我们的方法优于基线StarGAN。</p>
</blockquote>
<p><strong>简化解释</strong></p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15058v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该文探讨了计算机断层扫描（CT）在医学诊断中的重要作用以及重建核差异对数据驱动方法（如深度学习模型）的影响。为解决这一问题，提出了一种基于生成对抗网络（GANs）的纹理感知StarGAN模型，用于CT数据和谐化，能够跨不同重建核进行一对一至多翻译。通过多尺度纹理损失函数嵌入不同空间和角度尺度的纹理信息到和谐化过程中，解决了核引起的纹理变化问题。实验在公开数据集上进行，利用来自197名患者的48667张胸部CT切片，验证了该方法优于基线StarGAN。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CT在医学诊断中具有重要作用，但不同重建核之间的差异影响了数据驱动方法的性能。</li>
<li>数据和谐化是减少非生物变异的一种有前途的解决方案。</li>
<li>GANs已被证明是用于数据和谐化的强大框架，但存在捕捉图像复杂关系的局限性。</li>
<li>提出了一种新的纹理感知StarGAN模型用于CT数据和谐化，能够进行一对一至多的翻译。</li>
<li>该方法通过引入多尺度纹理损失函数解决了核引起的纹理变化问题。</li>
<li>实验在公开数据集上进行，验证了该方法的有效性优于基线StarGAN。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15058">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6ed0807139549de51e49442834f97092.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a47fa1ae297c34f5f42a6cd9d49a7a3f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8687e9abb286ce80866f401743308a8a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-897badc3e9c144ff95b246205d49fbb8.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Ultrasound-Image-to-Video-Synthesis-via-Latent-Dynamic-Diffusion-Models"><a href="#Ultrasound-Image-to-Video-Synthesis-via-Latent-Dynamic-Diffusion-Models" class="headerlink" title="Ultrasound Image-to-Video Synthesis via Latent Dynamic Diffusion Models"></a>Ultrasound Image-to-Video Synthesis via Latent Dynamic Diffusion Models</h2><p><strong>Authors:Tingxiu Chen, Yilei Shi, Zixuan Zheng, Bingcong Yan, Jingliang Hu, Xiao Xiang Zhu, Lichao Mou</strong></p>
<p>Ultrasound video classification enables automated diagnosis and has emerged as an important research area. However, publicly available ultrasound video datasets remain scarce, hindering progress in developing effective video classification models. We propose addressing this shortage by synthesizing plausible ultrasound videos from readily available, abundant ultrasound images. To this end, we introduce a latent dynamic diffusion model (LDDM) to efficiently translate static images to dynamic sequences with realistic video characteristics. We demonstrate strong quantitative results and visually appealing synthesized videos on the BUSV benchmark. Notably, training video classification models on combinations of real and LDDM-synthesized videos substantially improves performance over using real data alone, indicating our method successfully emulates dynamics critical for discrimination. Our image-to-video approach provides an effective data augmentation solution to advance ultrasound video analysis. Code is available at <a target="_blank" rel="noopener" href="https://github.com/MedAITech/U_I2V">https://github.com/MedAITech/U_I2V</a>. </p>
<blockquote>
<p>超声视频分类可实现自动诊断，已成为重要研究领域。然而，公共超声视频数据集仍然匮乏，阻碍了有效视频分类模型的开发进展。我们提出通过合成合理的超声视频来解决这一短缺问题，这些视频可从丰富且易获得的超声图像中生成。为此，我们引入潜在动态扩散模型（LDDM），以有效将静态图像转换为具有真实视频特性的动态序列。我们在BUSV基准测试上展示了强大的定量结果和视觉上吸引人的合成视频。值得注意的是，使用真实和LDDM合成视频相结合的数据集训练视频分类模型，其性能大大超过了仅使用真实数据的效果，表明我们的方法成功模拟了关键鉴别动态。我们的图像到视频的方法为推进超声视频分析提供了有效的数据增强解决方案。代码可通过<a target="_blank" rel="noopener" href="https://github.com/MedAITech/U_I2V%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/MedAITech/U_I2V获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14966v1">PDF</a> MICCAI 2024</p>
<p><strong>Summary</strong><br>超声视频分类对于自动化诊断至关重要，但可用的超声视频数据集稀缺，制约了视频分类模型的发展。为此，我们提出通过丰富的超声图像合成超声视频来弥补这一不足。我们引入潜在动态扩散模型（LDDM），成功将静态图像转化为具有真实视频特性的动态序列。在BUSV基准测试上，合成的视频既在数量上丰富，视觉上也很逼真。使用真实和合成视频组合训练的视频分类模型性能优于仅使用真实数据，证明我们的方法成功模拟了关键动态鉴别特征。我们的图像到视频的转换方法为超声视频分析提供了有效的数据增强解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>超声视频分类在自动化诊断中具有重要意义，但缺乏可用的超声视频数据集限制了进展。</li>
<li>提出通过潜在动态扩散模型（LDDM）合成超声视频以弥补数据集不足。</li>
<li>LDDM模型成功将静态超声图像转化为具有真实视频特性的动态序列。</li>
<li>在BUSV基准测试上，合成的视频既丰富且视觉效果好。</li>
<li>使用真实和合成视频组合训练的视频分类模型性能更佳。</li>
<li>该方法成功模拟了关键动态鉴别特征，证明其有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14966">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-8e49b6aef94727d2e5c3afa2b31df8fa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c896922487736d835240b97ed3bc02fa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a1cc7baf7a6c1fe7bd6d904284b499aa.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Effect-of-substitution-La-by-Mg-on-electrochemical-and-electronic-properties-in-La-2-x-Mg-x-Ni-7-alloys-a-combined-experimental-and-ab-initio-studies"><a href="#Effect-of-substitution-La-by-Mg-on-electrochemical-and-electronic-properties-in-La-2-x-Mg-x-Ni-7-alloys-a-combined-experimental-and-ab-initio-studies" class="headerlink" title="Effect of substitution La by Mg on electrochemical and electronic   properties in La$_{2-x}$Mg$_x$Ni$_7$ alloys: a combined experimental and ab   initio studies"></a>Effect of substitution La by Mg on electrochemical and electronic   properties in La$_{2-x}$Mg$_x$Ni$_7$ alloys: a combined experimental and ab   initio studies</h2><p><strong>Authors:Mirosław Werwiński, Andrzej Szajek, Agnieszka Marczyńska, Lesław Smardz, Marek Nowak, Mieczysław Jurczyk</strong></p>
<p>La-Mg-Ni-based alloys are promising negative electrode materials for 3rd generation of Ni-MH$<em>x$ batteries. In this work, we investigate the effect of Mg substitution on the electrochemical and electronic properties of La$</em>{2-x}$Mg$_x$Ni$<em>7$ materials. The mechanical alloying technique is used to produce a series of La$</em>{2-x}$Mg$_x$Ni$_7$ alloys ($x$ &#x3D; 0.00, 0.25, 0.50 and 0.75). The X-ray diffraction measurements indicate multiphase character of the samples with majority (La,Mg)$_2$Ni$_7$ phases of hexagonal Ce$<em>2$Ni$<em>7$-type and rhombohedral Gd$<em>2$Co$<em>7$-type. Electrochemical measurements show how the maximum discharge capacity ($C</em>{max}$) increases with Mg concentration and that reach the highest value of 304 mAh&#x2F;g for La$</em>{1.5}$Mg$</em>{0.5}$Ni$<em>7$ ($x$ &#x3D; 0.5). The experimental efforts are followed by the density functional theory (DFT) calculations performed with the full-potential local-orbital minimum-basis scheme (FPLO). To simulate chemical disorder, we use the coherent potential approximation (CPA). The calculations are focused on the La$</em>{1.5}$Mg$</em>{0.5}$Ni$<em>7$ composition with the highest measured value of $C</em>{max}$. Additionally, several other structures are considered as reference points. We find that hexagonal and rhombohedral structures of La$_2$Ni$_7$ have almost identical total energies, which is in a good agreement with a coexistence of both phases in the samples. The calculated site preferences of Mg in both Ce$_2$Ni$<em>7$-type and Gd$<em>2$Co$<em>7$-type La$</em>{1.5}$Mg$</em>{0.5}$Ni$<em>7$ phases are consistent with the previous experimental data. Furthermore, the valence band of the nanocrystalline La$</em>{1.5}$Mg$</em>{0.5}$Ni$_7$ sample is investigated by X-ray photoelectron spectroscopy (XPS). The experimental XPS are interpreted based on the corresponding spectra calculated with DFT. </p>
<blockquote>
<p>La-Mg-Ni基合金作为第三代Ni-MH电池的负极材料具有广阔前景。在这项工作中，我们研究了Mg替代对La_{2-x}Mg_xNi_7材料电化学和电子性能的影响。采用机械合金化技术制备了一系列La_{2-x}Mg_xNi_7合金（x&#x3D;0.00、0.25、0.50和0.75）。X射线衍射测量表明样品具有多相特征，主要以La、Mg）<em>2Ni_7相为主，呈六方Ce_2Ni_7型和斜方Gd_2Co_7型。电化学测量表明最大放电容量（C</em>{max}）随Mg浓度的增加而增加，并在La_{1.5}Mg_{0.5}Ni_7（x&#x3D;0.5）时达到最高值304mAh&#x2F;g。我们在密度泛函理论（DFT）计算中进行了实验验证，采用全势局域轨道最小基方案（FPLO）。为了模拟化学无序性，我们使用了相干势近似（CPA）。计算的重点是最大放电容量最高的La_{1.5}Mg_{0.5}Ni_7成分。此外，还考虑了其他一些结构作为参考点。我们发现La_2Ni_7的六边形和斜方结构具有几乎相同的总能量，这与样品中两种结构共存的情况相符。计算得出的Ce_2Ni_7型和Gd_2Co_7型La_{1.5}Mg_{0.5}Ni_7相中Mg的位点偏好与之前的实验数据一致。此外，通过X射线光电子光谱仪（XPS）研究了纳米晶La_{1.5}Mg_{0.5}Ni_7样品的价带。根据DFT计算得到的相应光谱来解释实验XPS。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14952v1">PDF</a> </p>
<p><strong>Summary</strong><br>     该研究探讨了Mg替代对La-Mg-Ni基合金的电化学和电子性能的影响，发现随着Mg浓度的增加，最大放电容量增大，并在La_{1.5}Mg_{0.5}Ni_7时达到最大值。通过密度泛函理论计算和X射线光电子谱研究其结构特性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>La-Mg-Ni基合金是第三代Ni-MH电池的潜在负极材料。</li>
<li>Mg替代对La_{2-x}Mg_{x}Ni_{7}材料具有显著影响，最大放电容量随Mg浓度增加而增大。</li>
<li>La_{1.5}Mg_{0.5}Ni_{7}的放电容量达到最高值304mAh&#x2F;g。</li>
<li>X射线衍射结果表明样品呈现多相特征，以Ce_{2}Ni_{7}-型为主相。</li>
<li>密度泛函理论计算用于模拟化学无序状态，并研究La_{1.5}Mg_{0.5}Ni_{7}结构特性。</li>
<li>XPS实验验证了材料的电子结构特征，与DFT计算结果一致。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14952">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-15bb0e8e98969d8587eed7bb04709a56.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0ecd0ddb42ec9d31990df0980e9dd9ab.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4826f389305e10e2a4c24cfd50651edc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5781acfe6dda54b8af7d47449ba02770.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b8fdda327ebaf67e8d249e85b9fdcf62.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b36c139610fcd4cb5d32957d0ab030e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f174e23a1219f280dd7e78400996b63b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-61f9e2a4373a9df3b4f0a853c4c4eaf9.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="A-Simple-Combination-of-Diffusion-Models-for-Better-Quality-Trade-Offs-in-Image-Denoising"><a href="#A-Simple-Combination-of-Diffusion-Models-for-Better-Quality-Trade-Offs-in-Image-Denoising" class="headerlink" title="A Simple Combination of Diffusion Models for Better Quality Trade-Offs   in Image Denoising"></a>A Simple Combination of Diffusion Models for Better Quality Trade-Offs   in Image Denoising</h2><p><strong>Authors:Jonas Dornbusch, Emanuel Pfarr, Florin-Alexandru Vasluianu, Frank Werner, Radu Timofte</strong></p>
<p>Diffusion models have garnered considerable interest in computer vision, owing both to their capacity to synthesize photorealistic images and to their proven effectiveness in image reconstruction tasks. However, existing approaches fail to efficiently balance the high visual quality of diffusion models with the low distortion achieved by previous image reconstruction methods. Specifically, for the fundamental task of additive Gaussian noise removal, we first illustrate an intuitive method for leveraging pretrained diffusion models. Further, we introduce our proposed Linear Combination Diffusion Denoiser (LCDD), which unifies two complementary inference procedures - one that leverages the model’s generative potential and another that ensures faithful signal recovery. By exploiting the inherent structure of the denoising samples, LCDD achieves state-of-the-art performance and offers controlled, well-behaved trade-offs through a simple scalar hyperparameter adjustment. </p>
<blockquote>
<p>扩散模型在计算机视觉领域引起了广泛关注，因为它们既能合成逼真的图像，在图像重建任务中也证明了其有效性。然而，现有方法无法在高视觉质量的扩散模型与先前图像重建方法实现的低失真之间进行有效平衡。针对去除添加性高斯噪声的基本任务，我们首先提出了一种利用预训练扩散模型的直观方法。此外，我们引入了所提出的线性组合扩散去噪器（LCDD），它统一了两种互补的推理程序——一种利用模型的生成潜力，另一种确保忠实信号恢复。通过利用去噪样本的固有结构，LCDD实现了最先进的性能，并通过简单的标量超参数调整提供了可控、表现良好的权衡。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14654v1">PDF</a> 10 pages, 7 figures, 2 tables</p>
<p><strong>Summary</strong></p>
<p>扩散模型在计算机视觉领域因其能合成逼真的图像并在图像重建任务中证明其有效性而受到广泛关注。然而，现有方法无法在扩散模型的高视觉质量与先前图像重建方法实现的低失真之间进行有效平衡。针对去除加性高斯噪声这一基本任务，本文首先展示了一种利用预训练扩散模型的直观方法。此外，本文提出了线性组合扩散去噪器（LCDD），它统一了两种互补的推理程序——一种利用模型的生成潜力，另一种确保忠实信号恢复。LCDD通过利用去噪样本的固有结构实现了卓越的性能，并通过简单的标量超参数调整提供了可控的、表现良好的权衡。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型在计算机视觉领域受到关注，因其能合成高质量图像并在图像重建任务中表现出有效性。</li>
<li>现有方法难以平衡扩散模型的高视觉质量与低失真。</li>
<li>针对去除加性高斯噪声任务，提出了一种利用预训练扩散模型的直观方法。</li>
<li>引入线性组合扩散去噪器（LCDD），统一两种互补推理程序，利用生成潜力和确保忠实信号恢复。</li>
<li>LCDD利用去噪样本的固有结构实现卓越性能。</li>
<li>LCDD通过简单的标量超参数调整提供可控的、表现良好的权衡。</li>
<li>该方法通过统一框架实现了扩散模型与图像重建方法的优势结合。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14654">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4c2ea409c2b04e690604d25421327e3b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7188d13e2cc7b21da6e64e6d7bb71719.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93b44faf967acf903f4b924bd1f27a4d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b5143422e6155faefdf1ac8d888f8ee4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3b5d4c8620f17420552564c5994a2a2d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-280f0608a09b9cd1db2be5b3efc5ff40.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Interpretable-High-order-Knowledge-Graph-Neural-Network-for-Predicting-Synthetic-Lethality-in-Human-Cancers"><a href="#Interpretable-High-order-Knowledge-Graph-Neural-Network-for-Predicting-Synthetic-Lethality-in-Human-Cancers" class="headerlink" title="Interpretable High-order Knowledge Graph Neural Network for Predicting   Synthetic Lethality in Human Cancers"></a>Interpretable High-order Knowledge Graph Neural Network for Predicting   Synthetic Lethality in Human Cancers</h2><p><strong>Authors:Xuexin Chen, Ruichu Cai, Zhengting Huang, Zijian Li, Jie Zheng, Min Wu</strong></p>
<p>Synthetic lethality (SL) is a promising gene interaction for cancer therapy. Recent SL prediction methods integrate knowledge graphs (KGs) into graph neural networks (GNNs) and employ attention mechanisms to extract local subgraphs as explanations for target gene pairs. However, attention mechanisms often lack fidelity, typically generate a single explanation per gene pair, and fail to ensure trustworthy high-order structures in their explanations. To overcome these limitations, we propose Diverse Graph Information Bottleneck for Synthetic Lethality (DGIB4SL), a KG-based GNN that generates multiple faithful explanations for the same gene pair and effectively encodes high-order structures. Specifically, we introduce a novel DGIB objective, integrating a Determinant Point Process (DPP) constraint into the standard IB objective, and employ 13 motif-based adjacency matrices to capture high-order structures in gene representations. Experimental results show that DGIB4SL outperforms state-of-the-art baselines and provides multiple explanations for SL prediction, revealing diverse biological mechanisms underlying SL inference. </p>
<blockquote>
<p>合成致死性（Synthetic Lethality, SL）是一种具有潜力的癌症治疗基因交互作用。最近的SL预测方法将知识图谱（KG）融入图神经网络（GNN），并应用注意力机制提取局部子图作为目标基因对的解释。然而，注意力机制通常缺乏准确性，通常只为每个基因对生成一个解释，并且无法在解释中确保可靠的高阶结构。为了克服这些局限性，我们提出了基于知识图谱的合成致死性多样图信息瓶颈（DGIB4SL）方法。该方法为同一基因对生成多个忠实解释，并有效地编码高阶结构。具体来说，我们引入了一种新颖的DGIB目标，将行列式点过程（DPP）约束集成到标准IB目标中，并使用13个基于基序的邻接矩阵来捕获基因表示中的高阶结构。实验结果表明，DGIB4SL优于最新基线方法，为SL预测提供了多个解释，揭示了SL推断背后多样化的生物学机制。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06052v2">PDF</a> 15 pages. Accepted by Briefings in Bioinformatics</p>
<p><strong>Summary</strong><br>    合成致死性（SL）是癌症治疗中有前景的基因相互作用。针对现有SL预测方法中的注意力机制缺乏保真度、通常只为基因对生成单一解释以及无法确保解释中高阶结构可信度的问题，提出了基于知识图谱的图神经网络（DGIB4SL）。该方法为同一基因对生成多个忠实解释，并有效编码高阶结构。通过引入新的DGIB目标，将行列式点过程（DPP）约束集成到标准IB目标中，并使用13个基于motif的邻接矩阵捕获基因表示中的高阶结构。实验结果表明，DGIB4SL优于最新基线，为SL预测提供多重解释，揭示SL推断背后的不同生物学机制。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>合成致死性（SL）是癌症治疗领域的一个重要基因交互现象。</li>
<li>现有SL预测方法通过整合知识图谱和图神经网络进行。</li>
<li>注意力机制在SL预测中常缺乏保真度，通常只为基因对生成单一解释。</li>
<li>提出了DGIB4SL方法，旨在生成多个忠实的解释并为同一基因对编码高阶结构。</li>
<li>DGIB4SL通过引入DGIB目标和行列式点过程（DPP）约束来实现这一目标。</li>
<li>DGIB4SL使用基于motif的邻接矩阵来捕获基因表示中的高阶结构。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06052">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-430b460135811c7ede99b5a295a3a404.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7e09a962d6351d1d76d2df13687af0eb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-88c1a2d35388d5c366bd493c38f13a4f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-364c878f003c413ffc8e1e25fa3c35e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed57fd8ee6ba2c3dd428384b6f28081b.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Gaussian-Random-Fields-as-an-Abstract-Representation-of-Patient-Metadata-for-Multimodal-Medical-Image-Segmentation"><a href="#Gaussian-Random-Fields-as-an-Abstract-Representation-of-Patient-Metadata-for-Multimodal-Medical-Image-Segmentation" class="headerlink" title="Gaussian Random Fields as an Abstract Representation of Patient Metadata   for Multimodal Medical Image Segmentation"></a>Gaussian Random Fields as an Abstract Representation of Patient Metadata   for Multimodal Medical Image Segmentation</h2><p><strong>Authors:Bill Cassidy, Christian McBride, Connah Kendrick, Neil D. Reeves, Joseph M. Pappachan, Shaghayegh Raad, Moi Hoon Yap</strong></p>
<p>The growing rate of chronic wound occurrence, especially in patients with diabetes, has become a concerning trend in recent years. Chronic wounds are difficult and costly to treat, and have become a serious burden on health care systems worldwide. Chronic wounds can have devastating consequences for the patient, with infection often leading to reduced quality of life and increased mortality risk. Innovative deep learning methods for the detection and monitoring of such wounds have the potential to reduce the impact to both patient and clinician. We present a novel multimodal segmentation method which allows for the introduction of patient metadata into the training workflow whereby the patient data are expressed as Gaussian random fields. Our results indicate that the proposed method improved performance when utilising multiple models, each trained on different metadata categories. Using the Diabetic Foot Ulcer Challenge 2022 test set, when compared to the baseline results (intersection over union &#x3D; 0.4670, Dice similarity coefficient &#x3D; 0.5908) we demonstrate improvements of +0.0220 and +0.0229 for intersection over union and Dice similarity coefficient respectively. This paper presents the first study to focus on integrating patient data into a chronic wound segmentation workflow. Our results show significant performance gains when training individual models using specific metadata categories, followed by average merging of prediction masks using distance transforms. All source code for this study is available at: <a target="_blank" rel="noopener" href="https://github.com/mmu-dermatology-research/multimodal-grf">https://github.com/mmu-dermatology-research/multimodal-grf</a> </p>
<blockquote>
<p>近年来，特别是在糖尿病患者中，慢性伤口的发生率呈上升趋势，这已成为令人担忧的趋势。慢性伤口的治疗难度大、费用高，已成为全球卫生保健系统的一项严重负担。慢性伤口会为患者带来灾难性后果，感染往往会导致生活质量下降和死亡风险增加。针对此类伤口的检测和监测，创新的深度学习方法有望减轻对患者和临床医生的影响。我们提出了一种新型的多模态分割方法，该方法允许将患者元数据引入到训练流程中，其中患者数据表示为高斯随机场。我们的结果表明，在利用多个模型时，所提出的方法在训练不同元数据类别上表现出性能提升。与基准结果相比（交并比&#x3D;0.4670，迪杰斯特拉相似系数&#x3D;0.5908），在Diabetic Foot Ulcer Challenge 2022测试集上，我们展示出在交并比和迪杰斯特拉相似系数方面分别提高了+0.0220和+0.0229。本文是首次专注于将患者数据整合到慢性伤口分割流程中的研究。我们的结果表明，在针对特定元数据类别训练个别模型后，通过距离变换平均合并预测掩模可以取得显著的性能提升。该研究的所有源代码可在<a target="_blank" rel="noopener" href="https://github.com/mmu-dermatology-research/multimodal-grf%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/mmu-dermatology-research/multimodal-grf找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05214v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文关注慢性伤口，特别是糖尿病患者慢性伤口的日益增长趋势，提出一种新颖的多模态分割方法，将患者元数据引入训练流程。通过采用高斯随机场表达患者数据，该方法在利用不同元数据类别训练的多个模型上表现出改进效果。相较于基线结果，该方法在交集比和Dice相似系数上分别提高了0.0220和0.0229。本文为首次尝试将患者数据整合到慢性伤口分割流程中的研究，通过训练特定元数据类别的独立模型，然后进行预测掩膜的平均合并，实现了显著的性能提升。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>慢性伤口，特别是在糖尿病患者中的发生率的增长，已成为全球卫生系统的严重负担。</li>
<li>新型深度学习方法在慢性伤口检测和监测中具有减少患者和临床医生负担的潜力。</li>
<li>提出一种多模态分割方法，整合患者元数据到训练流程中，以高斯随机场表达患者数据。</li>
<li>方法在多个模型上表现改进效果，这些模型分别针对不同的元数据类别进行训练。</li>
<li>与基线结果相比，该方法在交集比和Dice相似系数上有所提升。</li>
<li>此研究为首次尝试将患者数据整合到慢性伤口分割流程中。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05214">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-fa7c65544ade7a8e45ba37ee307fbcb3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e3c4d985735e39937496b9d9e98715f1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5a57ce4b0117c9747fdec5fe16843aec.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d113fb0447467825c49f9b473574964c.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="MedVLM-R1-Incentivizing-Medical-Reasoning-Capability-of-Vision-Language-Models-VLMs-via-Reinforcement-Learning"><a href="#MedVLM-R1-Incentivizing-Medical-Reasoning-Capability-of-Vision-Language-Models-VLMs-via-Reinforcement-Learning" class="headerlink" title="MedVLM-R1: Incentivizing Medical Reasoning Capability of Vision-Language   Models (VLMs) via Reinforcement Learning"></a>MedVLM-R1: Incentivizing Medical Reasoning Capability of Vision-Language   Models (VLMs) via Reinforcement Learning</h2><p><strong>Authors:Jiazhen Pan, Che Liu, Junde Wu, Fenglin Liu, Jiayuan Zhu, Hongwei Bran Li, Chen Chen, Cheng Ouyang, Daniel Rueckert</strong></p>
<p>Reasoning is a critical frontier for advancing medical image analysis, where transparency and trustworthiness play a central role in both clinician trust and regulatory approval. Although Medical Visual Language Models (VLMs) show promise for radiological tasks, most existing VLMs merely produce final answers without revealing the underlying reasoning. To address this gap, we introduce MedVLM-R1, a medical VLM that explicitly generates natural language reasoning to enhance transparency and trustworthiness. Instead of relying on supervised fine-tuning (SFT), which often suffers from overfitting to training distributions and fails to foster genuine reasoning, MedVLM-R1 employs a reinforcement learning framework that incentivizes the model to discover human-interpretable reasoning paths without using any reasoning references. Despite limited training data (600 visual question answering samples) and model parameters (2B), MedVLM-R1 boosts accuracy from 55.11% to 78.22% across MRI, CT, and X-ray benchmarks, outperforming larger models trained on over a million samples. It also demonstrates robust domain generalization under out-of-distribution tasks. By unifying medical image analysis with explicit reasoning, MedVLM-R1 marks a pivotal step toward trustworthy and interpretable AI in clinical practice. Inference model is available at: <a target="_blank" rel="noopener" href="https://huggingface.co/JZPeterPan/MedVLM-R1">https://huggingface.co/JZPeterPan/MedVLM-R1</a>. </p>
<blockquote>
<p>医学图像分析是当前医疗领域发展的关键前沿，其中透明度和可信度对于医生信任及监管机构的批准至关重要。尽管医学视觉语言模型（VLMs）在放射学任务上展现出巨大潜力，但现有的大多数VLMs仅提供最终答案，而未能揭示背后的推理过程。为了解决这一不足，我们推出了MedVLM-R1，这是一款明确的医学视觉语言模型，能够生成自然语言推理以增强透明度和可信度。与其他模型不同，MedVLM-R1没有依赖监督微调（SFT），因为监督微调常常会导致过度拟合训练分布并忽略真正的推理过程。相反，MedVLM-R1采用强化学习框架，激励模型发现人类可解释的推理路径，无需使用任何推理参考。尽管训练数据量有限（仅600个视觉问答样本）且模型参数较少（2B），但MedVLM-R1在MRI、CT和X光基准测试上的准确率从55.11%提升至78.22%，超过了在超过百万样本上训练的更大模型。此外，它还在非内部分布的任务中表现出强大的领域泛化能力。通过将医学图像分析与明确推理相结合，MedVLM-R1标志着临床实践中可信和可解释的AI的重要一步。推理模型地址为：<a target="_blank" rel="noopener" href="https://huggingface.co/JZPeterPan/MedVLM-R1">https://huggingface.co/JZPeterPan/MedVLM-R1</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.19634v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了在医学图像分析领域，推进自然语言推理的重要性。为解决现有医疗视觉语言模型（VLMs）缺乏透明度与信任度的问题，本文提出了一种新的医疗VLM——MedVLM-R1。MedVLM-R1使用强化学习框架进行训练，可激励模型发现人类可解释性推理路径，且无需任何参考。在有限的训练数据和模型参数条件下，MedVLM-R1实现了MRI、CT和X光等多基准测试的性能提升，并表现出出色的跨域泛化能力。该模型标志着临床医学实践朝着可信可解释的AI方向迈出重要一步。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MedVLM-R1是一个医疗视觉语言模型，旨在解决医学图像分析中的透明度和信任度问题。</li>
<li>MedVLM-R1采用强化学习框架训练模型，激励其发现人类可解释性推理路径，无需参考任何推理依据。</li>
<li>在有限的训练数据和模型参数条件下，MedVLM-R1提升了在MRI、CT和X光等多个基准测试的性能。</li>
<li>MedVLM-R1实现的准确率从55.11%提升到78.22%，表现出良好的准确性和鲁棒性。</li>
<li>MedVLM-R1模型具备良好的跨域泛化能力。</li>
<li>MedVLM-R1模型推动了医学图像分析领域的进步，标志着向临床实践中可信可解释的AI迈出了重要一步。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.19634">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4866253832a007295535196f5d583a46.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd5227bee00a8424e43c4608d4f863f0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="A-study-of-why-we-need-to-reassess-full-reference-image-quality-assessment-with-medical-images"><a href="#A-study-of-why-we-need-to-reassess-full-reference-image-quality-assessment-with-medical-images" class="headerlink" title="A study of why we need to reassess full reference image quality   assessment with medical images"></a>A study of why we need to reassess full reference image quality   assessment with medical images</h2><p><strong>Authors:Anna Breger, Ander Biguri, Malena Sabaté Landman, Ian Selby, Nicole Amberg, Elisabeth Brunner, Janek Gröhl, Sepideh Hatamikia, Clemens Karner, Lipeng Ning, Sören Dittmer, Michael Roberts, AIX-COVNET Collaboration, Carola-Bibiane Schönlieb</strong></p>
<p>Image quality assessment (IQA) is indispensable in clinical practice to ensure high standards, as well as in the development stage of machine learning algorithms that operate on medical images. The popular full reference (FR) IQA measures PSNR and SSIM are known and tested for working successfully in many natural imaging tasks, but discrepancies in medical scenarios have been reported in the literature, highlighting the gap between development and actual clinical application. Such inconsistencies are not surprising, as medical images have very different properties than natural images, and PSNR and SSIM have neither been targeted nor properly tested for medical images. This may cause unforeseen problems in clinical applications due to wrong judgment of novel methods. This paper provides a structured and comprehensive overview of examples where PSNR and SSIM prove to be unsuitable for the assessment of novel algorithms using different kinds of medical images, including real-world MRI, CT, OCT, X-Ray, digital pathology and photoacoustic imaging data. Therefore, improvement is urgently needed in particular in this era of AI to increase reliability and explainability in machine learning for medical imaging and beyond. Lastly, we will provide ideas for future research as well as suggesting guidelines for the usage of FR-IQA measures applied to medical images. </p>
<blockquote>
<p>图像质量评估（IQA）在临床实践中是不可或缺的，以确保高标准，以及在医学影像操作的机器学习算法的开发阶段也是如此。流行的全参考（FR）IQA措施PSNR和SSIM在众多的自然成像任务中已被证明和测试过其成功性，但在医疗场景中，文献中已报道存在分歧，突出了开发与实际应用之间的差距。这种不一致性并不奇怪，因为医疗图像具有与自然图像非常不同的属性，而PSNR和SSIM并未针对医疗图像进行目标定位或适当的测试。这可能导致由于新方法的误判而在临床应用中出现意想不到的问题。本文提供了结构化和全面的概述，举例说明了PSNR和SSIM在利用不同种类的医学图像（包括现实世界中的MRI、CT、OCT、X射线、数字病理和光声成像数据）评估新算法时的不适用性。因此，特别是在人工智能时代，迫切需要改进这一领域，以提高医学影像和其他领域的机器学习的可靠性和可解释性。最后，我们将为未来的研究提供思路，并提出在全参考IQA措施应用于医学图像时的使用指南。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.19097v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>医学图像质量评估（IQA）在临床实践和对医疗图像操作的机器学习算法的开发阶段都不可或缺。虽然流行的全参考（FR）IQA措施PSNR和SSIM已在许多自然成像任务中成功应用，但在医学场景中却存在不一致性，这突显了开发与实际应用之间的差距。由于医学图像与自然图像具有不同的特性，PSNR和SSIM并未针对医学图像进行专项测试或适当测试，可能导致新型方法的误判。本文提供了在不同医学图像中使用PSNR和SSIM评估新型算法的不适用案例的综合概述，包括现实世界的MRI、CT、OCT、X射线、数字病理和光声成像数据。因此，特别是在人工智能时代，亟需提高可靠性并提升机器学习在医学成像等领域的可解释性。最后，本文将提供未来研究的方向和在使用FR-IQA措施进行医学图像应用的指导建议。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>IQA在医学领域不可或缺，特别是在临床实践及医疗图像机器学习算法开发阶段。</li>
<li>PSNR和SSIM等传统全参考IQA方法在自然成像中表现良好，但在医学场景中可能存在不一致性。</li>
<li>医学图像与自然图像特性不同，传统IQA方法并未针对医学图像进行专项测试或适当测试。</li>
<li>PSNR和SSIM在评估新型算法时可能不适用，特别是在使用不同类型的医学图像时。</li>
<li>在人工智能时代，提高医学成像中机器学习算法的可靠性和可解释性至关重要。</li>
<li>需要改进和发展新的IQA方法以适应医学图像的特性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.19097">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3e4072c33b378070c93032a158d70fe7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-31f10cd04ebef96792b3425f4920dba8.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="ModeTv2-GPU-accelerated-Motion-Decomposition-Transformer-for-Pairwise-Optimization-in-Medical-Image-Registration"><a href="#ModeTv2-GPU-accelerated-Motion-Decomposition-Transformer-for-Pairwise-Optimization-in-Medical-Image-Registration" class="headerlink" title="ModeTv2: GPU-accelerated Motion Decomposition Transformer for Pairwise   Optimization in Medical Image Registration"></a>ModeTv2: GPU-accelerated Motion Decomposition Transformer for Pairwise   Optimization in Medical Image Registration</h2><p><strong>Authors:Haiqiao Wang, Zhuoyuan Wang, Dong Ni, Yi Wang</strong></p>
<p>Deformable image registration plays a crucial role in medical imaging, aiding in disease diagnosis and image-guided interventions. Traditional iterative methods are slow, while deep learning (DL) accelerates solutions but faces usability and precision challenges. This study introduces a pyramid network with the enhanced motion decomposition Transformer (ModeTv2) operator, showcasing superior pairwise optimization (PO) akin to traditional methods. We re-implement ModeT operator with CUDA extensions to enhance its computational efficiency. We further propose RegHead module which refines deformation fields, improves the realism of deformation and reduces parameters. By adopting the PO, the proposed network balances accuracy, efficiency, and generalizability. Extensive experiments on three public brain MRI datasets and one abdominal CT dataset demonstrate the network’s suitability for PO, providing a DL model with enhanced usability and interpretability. The code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/ZAX130/ModeTv2">https://github.com/ZAX130/ModeTv2</a>. </p>
<blockquote>
<p>可变形图像配准在医学成像中扮演着至关重要的角色，有助于疾病诊断和图像引导干预。传统迭代方法速度慢，而深度学习（DL）虽然可以加速解决方案的计算，但却面临可用性和精确度方面的挑战。本研究引入了一种金字塔网络，配备了增强的运动分解Transformer（ModeTv2）算子，展现了与传统方法相媲美的优越点对点优化（PO）。我们使用CUDA扩展重新实现了ModeT算子，以提高其计算效率。我们进一步提出了RegHead模块，用于细化变形场，提高变形的真实性并减少参数。通过采用点对点优化，所提出的网络在准确性、效率和通用性之间达到了平衡。在三个公共脑MRI数据集和一个腹部CT数据集上的大量实验表明，该网络非常适合进行点对点优化，为深度学习模型提供了增强可用性和可解释性的模型。相关代码已公开发布在：<a target="_blank" rel="noopener" href="https://github.com/ZAX130/ModeTv2%E3%80%82">https://github.com/ZAX130/ModeTv2。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.16526v2">PDF</a> </p>
<p><strong>Summary</strong><br>医学图像配准中，形变图像配准十分重要，有助于疾病诊断和图像引导干预。传统迭代方法速度慢，深度学习虽能加速但面临可用性和精度挑战。本研究引入带有增强运动分解Transformer（ModeTv2）算子的金字塔网络，展示与传统方法相似的优越成对优化（PO）。通过CUDA扩展重新实现ModeT算子以提高计算效率。进一步提出RegHead模块，用于细化变形场，提高变形真实性和减少参数。采用PO，该网络在准确性、效率和通用性之间取得平衡。在三个公共脑MRI数据集和一个腹部CT数据集上的实验证明了该网络适用于PO，提供具有增强可用性和可解释性的深度学习模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>形变图像配准在医学成像中起关键作用，有助于疾病诊断和图像引导干预。</li>
<li>传统迭代方法速度慢，深度学习提供加速但存在可用性和精度挑战。</li>
<li>研究引入金字塔网络并结合ModeTv2算子，实现优越成对优化（PO）。</li>
<li>ModeT算子通过CUDA扩展重新实现，提高计算效率。</li>
<li>提出RegHead模块，用于细化变形场，提高变形的真实性和减少参数。</li>
<li>采用PO平衡准确性、效率和通用性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.16526">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-dc431982f411a46a0e40cb72efcf5eea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0df64c2ee24142ec2cf96c21a97dd416.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ee9840133c9ee59996a30c4d2ef9aaf8.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Semi-Supervised-Semantic-Segmentation-Based-on-Pseudo-Labels-A-Survey"><a href="#Semi-Supervised-Semantic-Segmentation-Based-on-Pseudo-Labels-A-Survey" class="headerlink" title="Semi-Supervised Semantic Segmentation Based on Pseudo-Labels: A Survey"></a>Semi-Supervised Semantic Segmentation Based on Pseudo-Labels: A Survey</h2><p><strong>Authors:Lingyan Ran, Yali Li, Guoqiang Liang, Yanning Zhang</strong></p>
<p>Semantic segmentation is an important and popular research area in computer vision that focuses on classifying pixels in an image based on their semantics. However, supervised deep learning requires large amounts of data to train models and the process of labeling images pixel by pixel is time-consuming and laborious. This review aims to provide a first comprehensive and organized overview of the state-of-the-art research results on pseudo-label methods in the field of semi-supervised semantic segmentation, which we categorize from different perspectives and present specific methods for specific application areas. In addition, we explore the application of pseudo-label technology in medical and remote-sensing image segmentation. Finally, we also propose some feasible future research directions to address the existing challenges. </p>
<blockquote>
<p>语义分割是计算机视觉中一个重要且热门的研究领域，主要关注基于语义对图像中的像素进行分类。然而，监督深度学习需要大量的数据来训练模型，逐像素标注图像的过程既耗时又繁琐。本文旨在提供伪标签方法在半监督语义分割领域最新研究成果的首次全面、系统的概述，从不同角度对方法进行分类，并针对特定应用领域介绍具体方法。此外，我们还探讨了伪标签技术在医学和遥感图像分割中的应用。最后，我们还提出了一些可行的未来研究方向，以解决现有挑战。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.01909v3">PDF</a> Accepted by IEEE Transactions on Circuits and Systems for Video   Technology(TCSVT)</p>
<p><strong>Summary</strong><br>     语义分割是计算机视觉中一个重要且热门的研究领域，主要对图像中的像素进行语义分类。然而，监督深度学习需要大量的数据进行模型训练，逐像素标注图像的过程耗时且繁琐。本文旨在提供伪标签方法在半监督语义分割领域最新研究成果的首次全面综述，从不同角度分类并介绍特定应用领域的方法。此外，还探讨了伪标签技术在医学和遥感图像分割中的应用。最后，本文还提出了一些可行的未来研究方向，以应对现有挑战。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语义分割是计算机视觉中研究热点，主要对图像中的像素进行语义分类。</li>
<li>监督深度学习需要大量的数据进行模型训练，逐像素标注图像是繁琐耗时的工作。</li>
<li>伪标签方法在半监督语义分割领域得到广泛应用，本文提供了全面的综述。</li>
<li>伪标签技术不仅适用于普通图像分割，还应用于医学和遥感图像分割。</li>
<li>文章从不同角度对伪标签方法进行了分类，并介绍了特定应用领域的方法。</li>
<li>当前该领域存在挑战，文章提出了可行的未来研究方向。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.01909">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-e61ae1cc5d887d26086ba20ed50b1d36.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aad6bbbd8d18cc7d69a2aff258df9a2a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50c235727093679ffd70b362dce66667.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c80a069b21c7cb3e3e897e7994498a1d.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Masked-LoGoNet-Fast-and-Accurate-3D-Image-Analysis-for-Medical-Domain"><a href="#Masked-LoGoNet-Fast-and-Accurate-3D-Image-Analysis-for-Medical-Domain" class="headerlink" title="Masked LoGoNet: Fast and Accurate 3D Image Analysis for Medical Domain"></a>Masked LoGoNet: Fast and Accurate 3D Image Analysis for Medical Domain</h2><p><strong>Authors:Amin Karimi Monsefi, Payam Karisani, Mengxi Zhou, Stacey Choi, Nathan Doble, Heng Ji, Srinivasan Parthasarathy, Rajiv Ramnath</strong></p>
<p>Standard modern machine-learning-based imaging methods have faced challenges in medical applications due to the high cost of dataset construction and, thereby, the limited labeled training data available. Additionally, upon deployment, these methods are usually used to process a large volume of data on a daily basis, imposing a high maintenance cost on medical facilities. In this paper, we introduce a new neural network architecture, termed LoGoNet, with a tailored self-supervised learning (SSL) method to mitigate such challenges. LoGoNet integrates a novel feature extractor within a U-shaped architecture, leveraging Large Kernel Attention (LKA) and a dual encoding strategy to capture both long-range and short-range feature dependencies adeptly. This is in contrast to existing methods that rely on increasing network capacity to enhance feature extraction. This combination of novel techniques in our model is especially beneficial in medical image segmentation, given the difficulty of learning intricate and often irregular body organ shapes, such as the spleen. Complementary, we propose a novel SSL method tailored for 3D images to compensate for the lack of large labeled datasets. The method combines masking and contrastive learning techniques within a multi-task learning framework and is compatible with both Vision Transformer (ViT) and CNN-based models. We demonstrate the efficacy of our methods in numerous tasks across two standard datasets (i.e., BTCV and MSD). Benchmark comparisons with eight state-of-the-art models highlight LoGoNet’s superior performance in both inference time and accuracy. </p>
<blockquote>
<p>标准现代机器学习成像方法在面对医疗应用时面临了数据集构建成本高昂和可用标记训练数据有限的挑战。此外，这些方法在部署后通常用于处理每天的大量数据，给医疗机构带来了高昂的维护成本。在本文中，我们介绍了一种新型神经网络架构，称为LoGoNet，它采用量身定制的自监督学习方法来缓解这些挑战。LoGoNet在U形架构内集成了一种新型特征提取器，利用大内核注意力（LKA）和双编码策略来巧妙地捕捉长短范围的特征依赖性。这与现有方法不同，后者依赖于增加网络容量来提高特征提取能力。模型中这种新技术的组合在医学图像分割中尤其有益，考虑到学习复杂且通常不规则的器官形状（如脾脏）的困难性。此外，我们提出了一种针对3D图像的新型自监督学习方法，以弥补大型标记数据集的缺乏。该方法在多任务学习框架内结合了掩蔽和对比学习技术，可与Vision Transformer（ViT）和CNN模型兼容。我们在两个标准数据集（即BTCV和MSD）的多个任务中展示了我们的方法的有效性。与八种最新模型的基准对比表明，LoGoNet在推理时间和准确性方面均表现出卓越的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.06190v2">PDF</a> </p>
<p><strong>Summary</strong><br>    本文介绍了一种新的神经网络架构LoGoNet，结合自监督学习方法，用于解决医学图像分析中数据集构建成本高、训练数据有限的问题。LoGoNet采用U型架构、大内核注意力机制和双编码策略，能有效捕捉长程和短程特征依赖关系，特别适用于医学图像分割中复杂且不规则器官形状的学习。同时，提出了一种针对3D图像的新型自监督学习方法，以弥补缺乏大量标记数据集的问题。在多个任务、两个标准数据集上的实验表明，LoGoNet与八种最新模型相比，在推理时间和准确性上均表现出卓越性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LoGoNet是一种新的神经网络架构，用于解决医学图像分析中的挑战，如高成本的数据集构建和有限的标记训练数据。</li>
<li>LoGoNet结合自监督学习方法，能够处理大规模数据并降低维护成本。</li>
<li>LoGoNet采用U型架构、大内核注意力机制和双编码策略，能更有效地捕捉特征依赖关系。</li>
<li>提出了一种针对3D图像的新型自监督学习方法，以应对缺乏大量标记数据集的问题。</li>
<li>该方法结合了遮蔽和对比学习技术，在一个多任务学习框架内实现，兼容Vision Transformer（ViT）和CNN模型。</li>
<li>在两个标准数据集上的实验表明，LoGoNet在推理时间和准确性方面均优于其他八种最新模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2402.06190">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-798ba352c3a78b35a46c24d8da54158a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-633d9ccad9baba3c7cf2f763e4d1b953.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea006b54cbff3ce97de70c07152c8b38.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b6e3efb149ba8b1340f34bcf77e17796.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-819a5dbc858d19aa0001260dd1ad4b34.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-69a0363843da6aa4f73c13d70aafd3cf.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="PULASki-Learning-inter-rater-variability-using-statistical-distances-to-improve-probabilistic-segmentation"><a href="#PULASki-Learning-inter-rater-variability-using-statistical-distances-to-improve-probabilistic-segmentation" class="headerlink" title="PULASki: Learning inter-rater variability using statistical distances to   improve probabilistic segmentation"></a>PULASki: Learning inter-rater variability using statistical distances to   improve probabilistic segmentation</h2><p><strong>Authors:Soumick Chatterjee, Franziska Gaidzik, Alessandro Sciarra, Hendrik Mattern, Gábor Janiga, Oliver Speck, Andreas Nürnberger, Sahani Pathiraja</strong></p>
<p>In the domain of medical imaging, many supervised learning based methods for segmentation face several challenges such as high variability in annotations from multiple experts, paucity of labelled data and class imbalanced datasets. These issues may result in segmentations that lack the requisite precision for clinical analysis and can be misleadingly overconfident without associated uncertainty quantification. This work proposes the PULASki method as a computationally efficient generative tool for biomedical image segmentation that accurately captures variability in expert annotations, even in small datasets. This approach makes use of an improved loss function based on statistical distances in a conditional variational autoencoder structure (Probabilistic UNet), which improves learning of the conditional decoder compared to the standard cross-entropy particularly in class imbalanced problems. The proposed method was analysed for two structurally different segmentation tasks (intracranial vessel and multiple sclerosis (MS) lesion) and compare our results to four well-established baselines in terms of quantitative metrics and qualitative output. These experiments involve class-imbalanced datasets characterised by challenging features, including suboptimal signal-to-noise ratios and high ambiguity. Empirical results demonstrate the PULASKi method outperforms all baselines at the 5% significance level. Our experiments are also of the first to present a comparative study of the computationally feasible segmentation of complex geometries using 3D patches and the traditional use of 2D slices. The generated segmentations are shown to be much more anatomically plausible than in the 2D case, particularly for the vessel task. </p>
<blockquote>
<p>在医学成像领域，许多基于监督学习的分割方法面临多重挑战，如多个专家标注的高变异性、标记数据缺乏和类别不平衡数据集。这些问题可能导致分割结果缺乏临床分析所需的精度，并且在没有相应的不确定性量化的情况下，可能会产生误导性的过度自信。这项工作提出了PULASki方法，作为一种计算高效的生物医学图像分割生成工具，能够准确捕捉专家标注的变异性，即使在小型数据集中也是如此。该方法利用基于条件变分自动编码器结构中的统计距离的改进损失函数（概率U网），改进了条件解码器的学习，特别是在类别不平衡问题中与标准交叉熵相比。对所提出的方法进行了两个结构不同的分割任务（颅内血管和多发性硬化症（MS）病变）的分析，并根据定量指标和定性输出与四个成熟的基线方法进行比较。这些实验涉及具有挑战性的特征，包括次优的信噪比和高模糊性。经验结果表明，PULASKi方法在5%的显著性水平上优于所有基线方法。我们的实验也是首次对使用3D补丁进行复杂几何的可行分割与传统的使用2D切片进行比较研究。生成的分割结果比2D情况下更加解剖上合理，特别是在血管任务中。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.15686v2">PDF</a> </p>
<p><strong>Summary</strong><br>医学图像分割中，基于监督学习的方法面临多方挑战，如专家标注差异大、缺乏标注数据和类别不均衡等问题。针对这些问题，本研究提出了PULASki方法，该方法是一种计算高效的生物医学图像分割工具，能准确捕捉专家标注的变异性，即使在小数据集下也能表现良好。该方法基于条件变分自编码器的结构，采用改进的损失函数，在类别不均衡问题中表现优异。实验证明，该方法在颅内血管和多发性硬化病变分割任务上优于其他四种基线方法，并且在复杂几何结构的3D切片分割方面也有较好表现。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>医学图像分割中面临专家标注差异大、缺乏标注数据和类别不均衡等挑战。</li>
<li>PULASki方法是一种高效的生物医学图像分割工具，可解决上述问题。</li>
<li>PULASki方法采用基于条件变分自编码器的结构，结合改进的损失函数，在类别不均衡问题中表现优异。</li>
<li>PULASki方法在颅内血管和多发性硬化病变分割任务上优于其他方法。</li>
<li>该研究首次比较了复杂几何结构分割的3D切片和传统2D切片方法。</li>
<li>PULASki方法生成的分割结果更加符合解剖学结构，特别是在血管分割任务上。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2312.15686">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3534a3368642db9a2d60654a191b6cb2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c21ce96fade93bb6af9edc4e4f2dc19.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Uncertainty-Quantification-in-Machine-Learning-Based-Segmentation-A-Post-Hoc-Approach-for-Left-Ventricle-Volume-Estimation-in-MRI"><a href="#Uncertainty-Quantification-in-Machine-Learning-Based-Segmentation-A-Post-Hoc-Approach-for-Left-Ventricle-Volume-Estimation-in-MRI" class="headerlink" title="Uncertainty Quantification in Machine Learning Based Segmentation: A   Post-Hoc Approach for Left Ventricle Volume Estimation in MRI"></a>Uncertainty Quantification in Machine Learning Based Segmentation: A   Post-Hoc Approach for Left Ventricle Volume Estimation in MRI</h2><p><strong>Authors:F. Terhag, P. Knechtges, A. Basermann, R. Tempone</strong></p>
<p>Recent studies have confirmed cardiovascular diseases remain responsible for highest death toll amongst non-communicable diseases. Accurate left ventricular (LV) volume estimation is critical for valid diagnosis and management of various cardiovascular conditions, but poses significant challenge due to inherent uncertainties associated with segmentation algorithms in magnetic resonance imaging (MRI). Recent machine learning advancements, particularly U-Net-like convolutional networks, have facilitated automated segmentation for medical images, but struggles under certain pathologies and&#x2F;or different scanner vendors and imaging protocols. This study proposes a novel methodology for post-hoc uncertainty estimation in LV volume prediction using It^{o} stochastic differential equations (SDEs) to model path-wise behavior for the prediction error. The model describes the area of the left ventricle along the heart’s long axis. The method is agnostic to the underlying segmentation algorithm, facilitating its use with various existing and future segmentation technologies. The proposed approach provides a mechanism for quantifying uncertainty, enabling medical professionals to intervene for unreliable predictions. This is of utmost importance in critical applications such as medical diagnosis, where prediction accuracy and reliability can directly impact patient outcomes. The method is also robust to dataset changes, enabling application for medical centers with limited access to labeled data. Our findings highlight the proposed uncertainty estimation methodology’s potential to enhance automated segmentation robustness and generalizability, paving the way for more reliable and accurate LV volume estimation in clinical settings as well as opening new avenues for uncertainty quantification in biomedical image segmentation, providing promising directions for future research. </p>
<blockquote>
<p>最近的研究已经确认，心血管疾病仍是导致非传染性疾病死亡人数最多的原因。左心室（LV）体积的准确估计是各种心血管疾病的有效诊断和治疗的关键，但由于磁共振成像（MRI）中分割算法固有的不确定性，这构成了一个巨大的挑战。最近的机器学习进展，尤其是U-Net类似的卷积网络，已经促进了医疗图像的自动分割，但在某些病理情况下和&#x2F;或不同的扫描仪供应商和成像协议下仍存在困难。本研究提出了一种使用It^{o}随机微分方程（SDEs）对左心室体积预测进行事后不确定性估计的新方法，以模拟预测误差的路径行为。该模型描述了左心室沿心脏长轴的区域。该方法对潜在的分割算法持中立态度，可以与各种现有和未来分割技术一起使用。所提出的方法提供了量化不确定性的机制，使医疗专业人员可以对不可靠的预测进行干预。在医疗诊断等关键应用中，这至关重要，预测准确性和可靠性会直接影响患者结果。该方法对数据集的变化也很稳健，适用于有限访问标记数据的医学中心。我们的研究结果突出了所提出的不确定性估计方法在增强自动分割的稳健性和通用性方面的潜力，为临床环境中更可靠和准确的左心室体积估计铺平了道路，同时也为生物医学图像分割中的不确定性量化打开了新的途径，为未来的研究提供了有希望的方向。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02167v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种基于It^{o}随机微分方程（SDEs）的左心室体积预测事后不确定性估计新方法。此方法描述左心室在心脏长轴上的区域，对基础分割算法无特定要求，可应用于各种现有和未来分割技术。此方法能定量评估不确定性，使医疗专业人员能对不可靠的预测进行干预，对医疗诊断等关键应用至关重要。该方法对数据集变化具有稳健性，适用于医疗资源有限的中心。研究结果表明，该不确定性估计方法有望增强自动分割的稳健性和通用性，为临床环境中更可靠、准确的左心室体积估计铺平道路，同时为生物医学图像分割中的不确定性量化提供新的研究方向。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>心血管疾病在非传染性疾病中的死亡人数占比最高。</li>
<li>准确的左心室体积估计是心血管疾病诊断和管理的关键。</li>
<li>磁共振成像中的分割算法存在不确定性，给左心室体积估计带来挑战。</li>
<li>最近的机器学习进展，尤其是U-Net类似的卷积网络，促进了医学图像自动化分割的应用。</li>
<li>研究提出了一种基于It^{o}随机微分方程的新型方法，用于事后不确定性估计。</li>
<li>该方法独立于基础分割算法，可用于各种现有和未来的分割技术。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2312.02167">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f980ae7f8f3c4bdbc9803be2fab0a007.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-af5fe8cc1706d391f0aa471d95869361.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-65246be7ad1f3aca82bb51455a743e00.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-40a9b92c9139a5ba9b17a7a41f8794bf.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Spectral-wise-Implicit-Neural-Representation-for-Hyperspectral-Image-Reconstruction"><a href="#Spectral-wise-Implicit-Neural-Representation-for-Hyperspectral-Image-Reconstruction" class="headerlink" title="Spectral-wise Implicit Neural Representation for Hyperspectral Image   Reconstruction"></a>Spectral-wise Implicit Neural Representation for Hyperspectral Image   Reconstruction</h2><p><strong>Authors:Huan Chen, Wangcai Zhao, Tingfa Xu, Shiyun Zhou, Peifu Liu, Jianan Li</strong></p>
<p>Coded Aperture Snapshot Spectral Imaging (CASSI) reconstruction aims to recover the 3D spatial-spectral signal from 2D measurement. Existing methods for reconstructing Hyperspectral Image (HSI) typically involve learning mappings from a 2D compressed image to a predetermined set of discrete spectral bands. However, this approach overlooks the inherent continuity of the spectral information. In this study, we propose an innovative method called Spectral-wise Implicit Neural Representation (SINR) as a pioneering step toward addressing this limitation. SINR introduces a continuous spectral amplification process for HSI reconstruction, enabling spectral super-resolution with customizable magnification factors. To achieve this, we leverage the concept of implicit neural representation. Specifically, our approach introduces a spectral-wise attention mechanism that treats individual channels as distinct tokens, thereby capturing global spectral dependencies. Additionally, our approach incorporates two components, namely a Fourier coordinate encoder and a spectral scale factor module. The Fourier coordinate encoder enhances the SINR’s ability to emphasize high-frequency components, while the spectral scale factor module guides the SINR to adapt to the variable number of spectral channels. Notably, the SINR framework enhances the flexibility of CASSI reconstruction by accommodating an unlimited number of spectral bands in the desired output. Extensive experiments demonstrate that our SINR outperforms baseline methods. By enabling continuous reconstruction within the CASSI framework, we take the initial stride toward integrating implicit neural representation into the field. </p>
<blockquote>
<p>编码孔径快照光谱成像（CASSI）重建旨在从二维测量中恢复三维空间光谱信号。现有的超光谱图像（HSI）重建方法通常涉及从二维压缩图像学习映射到预先确定的一组离散光谱带。然而，这种方法忽略了光谱信息的内在连续性。在这项研究中，我们提出了一种创新的方法，称为光谱隐神经表示（SINR），作为解决这一限制的开创性步骤。SINR引入了一个连续光谱放大过程，用于HSI重建，实现了具有可定制放大倍数的光谱超分辨率。为实现这一点，我们利用了隐神经表示的概念。具体来说，我们的方法引入了一种光谱注意机制，将单个通道视为不同的标记，从而捕获全局光谱依赖性。此外，我们的方法包括两个组件，即傅立叶坐标编码器和光谱尺度因子模块。傅立叶坐标编码器增强了SINR对高频分量的强调能力，而光谱尺度因子模块指导SINR适应可变数量的光谱通道。值得注意的是，SINR框架通过适应输出所需的无限数量的光谱带，增强了CASSI重建的灵活性。大量实验表明，我们的SINR优于基线方法。通过在CASSI框架内实现连续重建，我们迈出了将隐神经表示整合到该领域的初步尝试。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.01061v2">PDF</a> Accepted by IEEE Transactions on Circuits and Systems for Video   Technology, has been published</p>
<p><strong>Summary</strong></p>
<p>本文介绍了基于隐式神经网络表示的Spectral-wise Implicit Neural Representation（SINR）方法，旨在解决Coded Aperture Snapshot Spectral Imaging（CASSI）重建中的超光谱图像重建问题。该方法引入连续光谱放大过程，实现光谱超分辨率并可自定义放大倍数。通过引入光谱注意机制，SINR能够捕捉全局光谱依赖性。此外，该方法结合了傅里叶坐标编码器和光谱尺度因子模块，提高了对高频成分的强调和适应性。总体而言，SINR增强了CASSI重建的灵活性，能够应对无限数量的光谱通道输出，且在实验中表现优于基线方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SINR是一种创新的超光谱图像重建方法，用于解决CASSI重建中的连续光谱放大问题。</li>
<li>通过引入光谱注意机制，SINR能够捕捉全局光谱依赖性。</li>
<li>傅里叶坐标编码器增强了SINR对高频成分的强调能力。</li>
<li>光谱尺度因子模块使SINR能够适应可变数量的光谱通道。</li>
<li>SINR框架增强了CASSI重建的灵活性，可以处理无限数量的光谱通道输出。</li>
<li>实验表明，SINR在性能上优于传统的HSI重建方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2312.01061">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-cb3aec09084ceabc52ccae0d75493a7a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c83c2a782705abe00bc290b6b9c8cd4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-54220232b45b5bf2e409f108f7a72050.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5ad676dd48397375461118ca03343720.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Co-Learning-Semantic-aware-Unsupervised-Segmentation-for-Pathological-Image-Registration"><a href="#Co-Learning-Semantic-aware-Unsupervised-Segmentation-for-Pathological-Image-Registration" class="headerlink" title="Co-Learning Semantic-aware Unsupervised Segmentation for Pathological   Image Registration"></a>Co-Learning Semantic-aware Unsupervised Segmentation for Pathological   Image Registration</h2><p><strong>Authors:Yang Liu, Shi Gu</strong></p>
<p>The registration of pathological images plays an important role in medical applications. Despite its significance, most researchers in this field primarily focus on the registration of normal tissue into normal tissue. The negative impact of focal tissue, such as the loss of spatial correspondence information and the abnormal distortion of tissue, are rarely considered. In this paper, we propose GIRNet, a novel unsupervised approach for pathological image registration by incorporating segmentation and inpainting through the principles of Generation, Inpainting, and Registration (GIR). The registration, segmentation, and inpainting modules are trained simultaneously in a co-learning manner so that the segmentation of the focal area and the registration of inpainted pairs can improve collaboratively. Overall, the registration of pathological images is achieved in a completely unsupervised learning framework. Experimental results on multiple datasets, including Magnetic Resonance Imaging (MRI) of T1 sequences, demonstrate the efficacy of our proposed method. Our results show that our method can accurately achieve the registration of pathological images and identify lesions even in challenging imaging modalities. Our unsupervised approach offers a promising solution for the efficient and cost-effective registration of pathological images. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/brain-intelligence-lab/GIRNet">https://github.com/brain-intelligence-lab/GIRNet</a>. </p>
<blockquote>
<p>病理图像的配准在医疗应用中扮演着重要角色。尽管其意义重大，但该领域的大多数研究者主要关注正常组织到正常组织的配准。很少考虑焦点组织的负面影响，如空间对应关系信息的丢失和组织异常扭曲。在本文中，我们提出了GIRNet，这是一种新的无监督病理图像配准方法，通过结合分割和补全技术，遵循生成、补全和配准（GIR）的原则。配准、分割和补全模块以协同学习的方式进行训练，以便焦点区域的分割和补全对的配准能够协同改进。总的来说，病理图像的配准是在一个完全无监督的学习框架中实现的。在多个数据集上的实验结果，包括T1序列的磁共振成像（MRI），证明了我们提出的方法的有效性。我们的结果表明，我们的方法可以准确地实现病理图像的配准，甚至在具有挑战性的成像模式上也能识别病变。我们的无监督方法为实现高效且经济的病理图像配准提供了有前景的解决方案。我们的代码可在 <a target="_blank" rel="noopener" href="https://github.com/brain-intelligence-lab/GIRNet">https://github.com/brain-intelligence-lab/GIRNet</a> 获得。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11040v3">PDF</a> 13 pages, 7 figures, published in Medical Image Computing and   Computer Assisted Intervention (MICCAI) 2023</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种基于生成、修复和注册（GIR）原则的新型无监督病理图像注册方法GIRNet。该方法同时训练注册、分割和修复模块，实现对焦点区域的分割和对修复对图像的注册，进而提高协同性能。实验结果表明，该方法可有效实现病理图像的注册，甚至在具有挑战性的成像模式下也能识别病变。该无监督方法为病理图像的效率和成本效益提供了有前景的解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>病理图像注册在医学应用中的重要性被强调，但现有研究主要关注正常组织的注册，忽略了焦点组织的负面影响。</li>
<li>提出了一种新型的无监督病理图像注册方法GIRNet，该方法结合了分割和修复技术。</li>
<li>GIRNet通过同时训练注册、分割和修复模块，实现了对焦点区域的协同改进。</li>
<li>实验结果表明，GIRNet在多个数据集上表现出优异的性能，包括T1序列的磁共振成像（MRI）。</li>
<li>GIRNet能准确实现病理图像的注册，并在具有挑战性的成像模式下识别病变。</li>
<li>无监督的GIRNet方法为高效且经济的病理图像注册提供了解决方案。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2310.11040">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-d0b752b2690a964f4ff37d51c64f21fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19c4512bf5070012829b402135127eb8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7a2becdd260ae923525ea4e9ed4ba656.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-21/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-21/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">医学图像</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-21/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-52d94899ad04465abb9b14edad72337d.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS 方向最新论文已更新，请持续关注 Update in 2025-03-21  MoonCast High-Quality Zero-Shot Podcast Generation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-21
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-21/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-762573e30cb534041dcb372a9e017808.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-03-21  FP4DiT Towards Effective Floating Point Quantization for Diffusion   Transformers
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-21
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">25156.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
