<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-03-21  TULIP Towards Unified Language-Image Pretraining">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-ff3b2227764a96f8af1a4f8aac0fe6c0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-03-21
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    6.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    27 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-03-21-更新"><a href="#2025-03-21-更新" class="headerlink" title="2025-03-21 更新"></a>2025-03-21 更新</h1><h2 id="TULIP-Towards-Unified-Language-Image-Pretraining"><a href="#TULIP-Towards-Unified-Language-Image-Pretraining" class="headerlink" title="TULIP: Towards Unified Language-Image Pretraining"></a>TULIP: Towards Unified Language-Image Pretraining</h2><p><strong>Authors:Zineng Tang, Long Lian, Seun Eisape, XuDong Wang, Roei Herzig, Adam Yala, Alane Suhr, Trevor Darrell, David M. Chan</strong></p>
<p>Despite the recent success of image-text contrastive models like CLIP and SigLIP, these models often struggle with vision-centric tasks that demand high-fidelity image understanding, such as counting, depth estimation, and fine-grained object recognition. These models, by performing language alignment, tend to prioritize high-level semantics over visual understanding, weakening their image understanding. On the other hand, vision-focused models are great at processing visual information but struggle to understand language, limiting their flexibility for language-driven tasks. In this work, we introduce TULIP, an open-source, drop-in replacement for existing CLIP-like models. Our method leverages generative data augmentation, enhanced image-image and text-text contrastive learning, and image&#x2F;text reconstruction regularization to learn fine-grained visual features while preserving global semantic alignment. Our approach, scaling to over 1B parameters, outperforms existing state-of-the-art (SOTA) models across multiple benchmarks, establishing a new SOTA zero-shot performance on ImageNet-1K, delivering up to a $2\times$ enhancement over SigLIP on RxRx1 in linear probing for few-shot classification, and improving vision-language models, achieving over $3\times$ higher scores than SigLIP on MMVP. Our code&#x2F;checkpoints are available at <a target="_blank" rel="noopener" href="https://tulip-berkeley.github.io/">https://tulip-berkeley.github.io</a> </p>
<blockquote>
<p>尽管CLIP和SigLIP等图文对比模型近期取得了成功，但这些模型在进行需要高保真图像理解的任务时，如计数、深度估计和精细粒度对象识别等视觉为中心的任务上经常遇到困难。这些模型通过执行语言对齐，倾向于优先处理高级语义而非视觉理解，从而削弱了它们的图像理解能力。另一方面，以视觉为中心的模型在处理视觉信息方面表现出色，但在理解语言方面却遇到困难，这限制了它们在语言驱动任务中的灵活性。在这项工作中，我们介绍了TULIP，这是一个开源的、可以替代现有CLIP等模型的工具。我们的方法利用生成数据增强、增强的图像与图像以及文本与文本对比学习以及图像&#x2F;文本重建正则化，以学习精细的视听觉特征同时保留全局语义对齐。我们的方法规模扩大到超过1亿个参数，在多个基准测试中超越了现有的先进技术（SOTA）模型，在ImageNet-1K上建立了新的零样本性能标准，在RxRx1的线性探测中相对于SigLIP提高了高达两倍的性能用于小样本分类任务，并在MMVP上实现了超过SigLIP三倍的高分。我们的代码和检查点可在<a target="_blank" rel="noopener" href="https://tulip-berkeley.github.io找到./">https://tulip-berkeley.github.io找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15485v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该文本介绍了尽管CLIP和SigLIP等图文对比模型取得了成功，但在需要高保真图像理解的任务上仍有不足。为此，本文提出了TULIP模型，通过生成数据增强、图像和文本对比学习以及图像&#x2F;文本重建正则化方法，在保留全局语义对齐的同时学习精细的视觉特征。该方法在多项基准测试中表现优于现有最先进的模型，并在ImageNet-1K上实现了零样本性能的新的SOTA水平。此外，TULIP在少数样本分类任务的线性探测性能比SigLIP提高了两倍以上，并在MMVP上获得了超过SigLIP三倍的高得分。模型和检查点已经开源可用。该模型的改进为复杂任务提供了一种可行解决方案。在保持全局语义对齐的同时，增强了图像理解的能力。同时，该模型在解决语言驱动的任务时表现出灵活性。这一模型旨在改进现有模型的不足之处，提升模型的性能和效率。这些优点使TULIP模型在图像分类等任务中展现出卓越的表现力。它的应用前景广阔，具有重要的应用价值和发展潜力。</p>
<p><strong>Key Takeaways</strong></p>
<p>以下是七个关键见解：</p>
<ul>
<li>当前流行的图像文本对比模型（如CLIP和SigLIP）在精细视觉任务上存在缺陷，但具有高层次的语义理解能力。他们面临着需要提升高保真图像理解的挑战。而某些纯视觉模型虽然在处理视觉信息方面表现出色，但在语言理解方面却显得捉襟见肘。这些模型的缺陷限制了它们在复杂任务中的灵活性。</li>
<li>TULIP作为一种新型的开源模型，旨在解决上述问题。它通过结合生成数据增强、图像和文本对比学习以及图像&#x2F;文本重建正则化方法等手段提高性能。该方法不仅能保留全局语义信息，而且能够更好地处理精细的视觉特征。这有助于增强模型在多种任务上的表现能力。同时提高图像理解和语言理解的平衡性。</li>
<li>TULIP模型的性能表现超过了现有最先进的模型，如SigLIP等模型在某些特定任务上的表现有了显著的提升。它在多个基准测试中表现优异，并在ImageNet-1K上实现了零样本性能的新的记录，达到SOTA水平，也显著提高零样本能力以解决下游任务的鲁棒性和适应性问题。。其在少量样本下的线性探测性能和全景能力得到了提升也极大地推动了视觉语言任务的改进发展态势顺利确立正确；相对于sigLIP模型和感知机预测等模型在MMVP上的得分有了显著的提升。。这表明TULIP在处理复杂任务时具有更高的效率和准确性。。该模型的引入有望为计算机视觉和自然语言处理领域带来新的突破。。因此可以预期在未来中将会获得更广泛的应用场景。。因此它对于解决复杂任务提供了一种可行解决方案。。其改进了现有模型的不足之处并提升了模型的性能和效率。。同时其推广可能会启发后续的创新探索算法解决任务层面逻辑的结构特征化处理与创新研究和基础方案结合创新性引用实用性确保有助于在该领域创造出更具创新和适应性的新技术进一步赋能自然语言理解和认知处理推理技术的进步的发展出新的技术发展驱动力空间扩大了我们对自然界的认识让我们获得了更快解决此类难题的新途径并带来了广泛的实际应用前景及市场潜力</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15485">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c2c979b1d4f62a1afd8e9f26b6760fc0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-37139187b967100da8d29ea01e0626a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d83684fd9be1c9949ddf15c214b946a3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-90f0f71e7d247b0655a7e48243c9f8e3.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Conjuring-Positive-Pairs-for-Efficient-Unification-of-Representation-Learning-and-Image-Synthesis"><a href="#Conjuring-Positive-Pairs-for-Efficient-Unification-of-Representation-Learning-and-Image-Synthesis" class="headerlink" title="Conjuring Positive Pairs for Efficient Unification of Representation   Learning and Image Synthesis"></a>Conjuring Positive Pairs for Efficient Unification of Representation   Learning and Image Synthesis</h2><p><strong>Authors:Imanol G. Estepa, Jesús M. Rodríguez-de-Vera, Ignacio Sarasúa, Bhalaji Nagarajan, Petia Radeva</strong></p>
<p>While representation learning and generative modeling seek to understand visual data, unifying both domains remains unexplored. Recent Unified Self-Supervised Learning (SSL) methods have started to bridge the gap between both paradigms. However, they rely solely on semantic token reconstruction, which requires an external tokenizer during training – introducing a significant overhead. In this work, we introduce Sorcen, a novel unified SSL framework, incorporating a synergic Contrastive-Reconstruction objective. Our Contrastive objective, “Echo Contrast”, leverages the generative capabilities of Sorcen, eliminating the need for additional image crops or augmentations during training. Sorcen “generates” an echo sample in the semantic token space, forming the contrastive positive pair. Sorcen operates exclusively on precomputed tokens, eliminating the need for an online token transformation during training, thereby significantly reducing computational overhead. Extensive experiments on ImageNet-1k demonstrate that Sorcen outperforms the previous Unified SSL SoTA by 0.4%, 1.48 FID, 1.76%, and 1.53% on linear probing, unconditional image generation, few-shot learning, and transfer learning, respectively, while being 60.8% more efficient. Additionally, Sorcen surpasses previous single-crop MIM SoTA in linear probing and achieves SoTA performance in unconditional image generation, highlighting significant improvements and breakthroughs in Unified SSL models. </p>
<blockquote>
<p>表示学习和生成建模都在努力理解视觉数据，但融合这两个领域仍然未被探索。最近的统一自监督学习（SSL）方法已经开始弥合两种范式之间的鸿沟。然而，它们仅依赖于语义令牌重建，这需要训练过程中的外部令牌器——这引入了相当大的开销。在这项工作中，我们介绍了Sorcen，这是一个新的统一SSL框架，它结合了协同对比重建目标。我们的对比目标“回声对比”利用Sorcen的生成能力，在训练过程中无需额外的图像裁剪或增强。Sorcen在语义令牌空间中“生成”一个回声样本，形成对比正对。Sorcen只处理预先计算好的令牌，无需在训练过程中进行在线令牌转换，从而大大降低了计算开销。在ImageNet-1k上的大量实验表明，在线性探测、无条件图像生成、小样本学习和迁移学习方面，索森分别比之前的统一SSL现状高出0.4%、1.48 FID、1.76%和1.53%，同时效率提高60.8%。此外，索森还在线性探测中超越了之前的单裁剪MIM现状，并在无条件图像生成方面达到了最新性能水平，这突显了统一SSL模型的显著改进和突破。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15060v1">PDF</a> The source code is available in <a target="_blank" rel="noopener" href="https://github.com/ImaGonEs/Sorcen">https://github.com/ImaGonEs/Sorcen</a></p>
<p><strong>Summary</strong><br>     本文介绍了一种新型的统一自监督学习（SSL）框架Sorcen，它结合了对比和重建目标，无需额外的图像裁剪或增强即可进行训练。Sorcen在预计算令牌上运行，减少了计算开销，并在ImageNet-1k数据集上实现了显著的性能改进。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Sorcen是一个新型的统一SSL框架，结合了对比和重建目标。</li>
<li>Sorcen引入了”Echo Contrast”对比目标，利用生成能力，无需额外的图像裁剪或增强即可进行训练。</li>
<li>Sorcen在预计算令牌上运行，消除了在线令牌转换的需要，从而显著减少了计算开销。</li>
<li>Sorcen在ImageNet-1k数据集上的性能超越了之前的统一SSL技术，特别是在线性探测、无条件图像生成、少镜学习和迁移学习方面。</li>
<li>Sorcen在无条件图像生成方面达到了最新技术水平。</li>
<li>Sorcen框架通过整合对比和重建目标，在视觉数据理解方面取得了突破。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15060">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-21e1e3fc5d62f9c04d37ddca5fbea98f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5c0439c7a81af2941727063a1f161805.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7752ace7b23f3430fc3279a1c0e402f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a0eeba2eca04982030c2592cc91ec63.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c34fc7cdff00c2f9b0dedb17067c890f.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Reducing-Annotation-Burden-Exploiting-Image-Knowledge-for-Few-Shot-Medical-Video-Object-Segmentation-via-Spatiotemporal-Consistency-Relearning"><a href="#Reducing-Annotation-Burden-Exploiting-Image-Knowledge-for-Few-Shot-Medical-Video-Object-Segmentation-via-Spatiotemporal-Consistency-Relearning" class="headerlink" title="Reducing Annotation Burden: Exploiting Image Knowledge for Few-Shot   Medical Video Object Segmentation via Spatiotemporal Consistency Relearning"></a>Reducing Annotation Burden: Exploiting Image Knowledge for Few-Shot   Medical Video Object Segmentation via Spatiotemporal Consistency Relearning</h2><p><strong>Authors:Zixuan Zheng, Yilei Shi, Chunlei Li, Jingliang Hu, Xiao Xiang Zhu, Lichao Mou</strong></p>
<p>Few-shot video object segmentation aims to reduce annotation costs; however, existing methods still require abundant dense frame annotations for training, which are scarce in the medical domain. We investigate an extremely low-data regime that utilizes annotations from only a few video frames and leverages existing labeled images to minimize costly video annotations. Specifically, we propose a two-phase framework. First, we learn a few-shot segmentation model using labeled images. Subsequently, to improve performance without full supervision, we introduce a spatiotemporal consistency relearning approach on medical videos that enforces consistency between consecutive frames. Constraints are also enforced between the image model and relearning model at both feature and prediction levels. Experiments demonstrate the superiority of our approach over state-of-the-art few-shot segmentation methods. Our model bridges the gap between abundant annotated medical images and scarce, sparsely labeled medical videos to achieve strong video segmentation performance in this low data regime. Code is available at <a target="_blank" rel="noopener" href="https://github.com/MedAITech/RAB">https://github.com/MedAITech/RAB</a>. </p>
<blockquote>
<p>少样本视频目标分割旨在降低标注成本。然而，现有方法仍然需要大量密集帧标注进行训练，这在医学领域是非常罕见的。我们研究了一种极少量数据的模式，该模式仅利用来自少数视频帧的标注，并利用现有的标记图像来最大限度地减少昂贵的视频标注。具体来说，我们提出了一个两阶段框架。首先，我们使用标记图像学习小样本分割模型。随后，为了在无完全监督的情况下提高性能，我们引入了医学视频上的时空一致性再学习的方法，强制连续帧之间的一致性。在特征和预测层面，图像模型和再学习模型之间也强制实施约束。实验表明，我们的方法优于最先进的少样本分割方法。我们的模型缩小了大量标注医学图像和稀少、稀疏标注医学视频之间的差距，在这个低数据模式下实现了强大的视频分割性能。代码可在<a target="_blank" rel="noopener" href="https://github.com/MedAITech/RAB%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/MedAITech/RAB找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14958v1">PDF</a> MICCAI 2024</p>
<p><strong>Summary</strong></p>
<p>本文研究了基于少量视频帧标注信息的视频目标分割技术，并引入了一种两阶段框架。首先利用图像标注数据学习一个分割模型，然后通过时空一致性重学习的方法提高模型在医疗视频上的性能。该方法实现了在数据稀缺情况下的视频分割任务，并展示了其在低数据环境下的优越性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究目标为解决医疗领域标注数据稀缺的问题，通过减少视频标注成本实现视频目标分割。</li>
<li>提出一种两阶段框架，首先利用少量视频帧标注信息和现有图像标注数据进行模型学习。</li>
<li>采用时空一致性重学习的方法提高模型性能，增强模型在连续视频帧之间的预测一致性。</li>
<li>模型在特征预测层面与图像模型保持一致性，增强了模型的泛化能力。</li>
<li>实验结果表明，该方法在少量标注数据的情况下优于现有技术。</li>
<li>该方法成功解决了从丰富的医学图像标注到稀疏医学视频标注的过渡问题。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14958">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-05ab530b4b8a174e11040385876a7927.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff3b2227764a96f8af1a4f8aac0fe6c0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Optimal-Transport-Adapter-Tuning-for-Bridging-Modality-Gaps-in-Few-Shot-Remote-Sensing-Scene-Classification"><a href="#Optimal-Transport-Adapter-Tuning-for-Bridging-Modality-Gaps-in-Few-Shot-Remote-Sensing-Scene-Classification" class="headerlink" title="Optimal Transport Adapter Tuning for Bridging Modality Gaps in Few-Shot   Remote Sensing Scene Classification"></a>Optimal Transport Adapter Tuning for Bridging Modality Gaps in Few-Shot   Remote Sensing Scene Classification</h2><p><strong>Authors:Zhong Ji, Ci Liu, Jingren Liu, Chen Tang, Yanwei Pang, Xuelong Li</strong></p>
<p>Few-Shot Remote Sensing Scene Classification (FS-RSSC) presents the challenge of classifying remote sensing images with limited labeled samples. Existing methods typically emphasize single-modal feature learning, neglecting the potential benefits of optimizing multi-modal representations. To address this limitation, we propose a novel Optimal Transport Adapter Tuning (OTAT) framework aimed at constructing an ideal Platonic representational space through optimal transport (OT) theory. This framework seeks to harmonize rich visual information with less dense textual cues, enabling effective cross-modal information transfer and complementarity. Central to this approach is the Optimal Transport Adapter (OTA), which employs a cross-modal attention mechanism to enrich textual representations and facilitate subsequent better information interaction. By transforming the network optimization into an OT optimization problem, OTA establishes efficient pathways for balanced information exchange between modalities. Moreover, we introduce a sample-level Entropy-Aware Weighted (EAW) loss, which combines difficulty-weighted similarity scores with entropy-based regularization. This loss function provides finer control over the OT optimization process, enhancing its solvability and stability. Our framework offers a scalable and efficient solution for advancing multimodal learning in remote sensing applications. Extensive experiments on benchmark datasets demonstrate that OTAT achieves state-of-the-art performance in FS-RSSC, significantly improving the model performance and generalization. </p>
<blockquote>
<p>少量遥感场景分类（FS-RSSC）面临着对有限标记样本的遥感图像进行分类的挑战。现有方法通常侧重于单模态特征学习，忽略了优化多模态表示可能带来的潜在好处。为了解决这个问题，我们提出了一种新型的基于最优传输适配器调整（OTAT）的框架，旨在通过最优传输（OT）理论构建一个理想的柏拉图表示空间。该框架旨在协调丰富的视觉信息与较少的文本线索，实现有效的跨模态信息转移和互补。该方法的核心是最佳传输适配器（OTA），它采用跨模态注意力机制来丰富文本表示，并促进后续更好的信息交互。通过将网络优化转化为OT优化问题，OTA为跨模态之间的平衡信息交换建立了有效的路径。此外，我们引入了样本级熵感知加权（EAW）损失，将难度加权相似度得分与基于熵的正则化相结合。此损失函数可以更精细地控制OT优化过程，提高其求解性和稳定性。我们的框架为遥感应用中的多模态学习提供了可扩展和高效的解决方案。在基准数据集上的广泛实验表明，OTAT在FS-RSSC中达到了最先进的性能，显著提高了模型性能和泛化能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14938v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了小样本遥感场景分类（FS-RSSC）的挑战，并提出了一个新的Optimal Transport Adapter Tuning（OTAT）框架来解决现有方法的局限性。该框架利用最优传输（OT）理论构建一个理想的柏拉图表示空间，以融合视觉和文本信息。中心是Optimal Transport Adapter（OTA），采用跨模态注意力机制来丰富文本表示，促进更有效的信息交互。此外，引入了样本级的Entropy-Aware Weighted（EAW）损失函数，为OT优化过程提供更精细的控制，增强其可解性和稳定性。OTAT框架为遥感应用中的多模态学习提供了可扩展和高效的解决方案，并在基准数据集上实现了最先进的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Few-Shot Remote Sensing Scene Classification (FS-RSSC)存在挑战，需要解决在有限标记样本下对遥感图像进行分类的问题。</li>
<li>现有方法主要关注单模态特征学习，忽略了优化多模态表示的巨大潜力。</li>
<li>提出了Optimal Transport Adapter Tuning (OTAT)框架，旨在通过最优传输（OT）理论构建理想的柏拉图表示空间。</li>
<li>OTAT框架融合了视觉和文本信息，实现了跨模态信息转移和互补。</li>
<li>Optimal Transport Adapter (OTA)是框架的核心，采用跨模态注意力机制来丰富文本表示并促进更有效的信息交互。</li>
<li>引入了样本级的Entropy-Aware Weighted (EAW)损失函数，为OT优化过程提供更精细的控制，增强其稳定性和可解决性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14938">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4162fa7353a0110851ce9005f36756a7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ab4c06702356b54b37e0e6775ed5e8ed.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7f82600b9ac651faad4f9081ed98cef4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2d439e957ad01c78ff3ed1bb92b91cfc.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Derm1M-A-Million-scale-Vision-Language-Dataset-Aligned-with-Clinical-Ontology-Knowledge-for-Dermatology"><a href="#Derm1M-A-Million-scale-Vision-Language-Dataset-Aligned-with-Clinical-Ontology-Knowledge-for-Dermatology" class="headerlink" title="Derm1M: A Million-scale Vision-Language Dataset Aligned with Clinical   Ontology Knowledge for Dermatology"></a>Derm1M: A Million-scale Vision-Language Dataset Aligned with Clinical   Ontology Knowledge for Dermatology</h2><p><strong>Authors:Siyuan Yan, Ming Hu, Yiwen Jiang, Xieji Li, Hao Fei, Philipp Tschandl, Harald Kittler, Zongyuan Ge</strong></p>
<p>The emergence of vision-language models has transformed medical AI, enabling unprecedented advances in diagnostic capability and clinical applications. However, progress in dermatology has lagged behind other medical domains due to the lack of standard image-text pairs. Existing dermatological datasets are limited in both scale and depth, offering only single-label annotations across a narrow range of diseases instead of rich textual descriptions, and lacking the crucial clinical context needed for real-world applications. To address these limitations, we present Derm1M, the first large-scale vision-language dataset for dermatology, comprising 1,029,761 image-text pairs. Built from diverse educational resources and structured around a standard ontology collaboratively developed by experts, Derm1M provides comprehensive coverage for over 390 skin conditions across four hierarchical levels and 130 clinical concepts with rich contextual information such as medical history, symptoms, and skin tone. To demonstrate Derm1M potential in advancing both AI research and clinical application, we pretrained a series of CLIP-like models, collectively called DermLIP, on this dataset. The DermLIP family significantly outperforms state-of-the-art foundation models on eight diverse datasets across multiple tasks, including zero-shot skin disease classification, clinical and artifacts concept identification, few-shot&#x2F;full-shot learning, and cross-modal retrieval. Our dataset and code will be public. </p>
<blockquote>
<p>视觉语言模型的兴起已经推动了医疗人工智能的发展，为诊断能力和临床应用带来了前所未有的进步。然而，皮肤病学的进展由于缺少标准图像文本配对而落后于其他医学领域。现有的皮肤病数据集在规模和深度上均有限，只提供狭窄疾病范围内的单一标签注释，而非丰富的文本描述，并缺乏真实世界应用所需的关键临床背景。为了解决这些局限性，我们推出了Derm1M，这是皮肤病领域首个大规模视觉语言数据集，包含1,029,761个图像文本对。Derm1M由多样化的教育资源构建，围绕专家共同开发的标准本体结构，为超过390种皮肤状况提供了四级层次结构和130种临床概念的全面覆盖，以及丰富的上下文信息，如病史、症状和肤色等。为了证明Derm1M在推动人工智能研究和临床应用方面的潜力，我们在该数据集上预训练了一系列类似于CLIP的模型，统称为DermLIP。DermLIP家族在多个任务的八个不同数据集上显著优于最先进的基础模型，包括零样本皮肤病分类、临床和文物概念识别、少样本&#x2F;全样本学习和跨模态检索。我们的数据集和代码将公开。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14911v1">PDF</a> 23 pages</p>
<p><strong>Summary</strong></p>
<p>本文介绍了医学人工智能领域中的视觉语言模型，特别是在皮肤科的应用。由于缺少标准图像文本配对，皮肤科的进步滞后于其他医学领域。为解决此问题，研究团队构建了首个大规模皮肤科视觉语言数据集Derm1M，包含1,029,761个图像文本对，涵盖超过390种皮肤状况、130种临床概念和丰富的上下文信息。同时，基于该数据集预训练了一系列CLIP类模型（统称为DermLIP），在多个任务上显著超越了当前最先进的模型，包括零样本皮肤疾病分类、临床和伪概念识别、少样本&#x2F;全样本学习和跨模态检索。数据集和代码将公开。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视觉语言模型在医学AI领域有重要应用，特别是在皮肤科。</li>
<li>皮肤科进步滞后于其他医学领域，主要因为缺少标准图像文本配对。</li>
<li>研究团队构建了首个大规模皮肤科视觉语言数据集Derm1M，包含丰富的图像文本对和临床概念信息。</li>
<li>DermLIP模型在多个任务上表现优异，包括皮肤疾病分类、临床和伪概念识别等。</li>
<li>Derm1M数据集将促进AI研究和临床应用的发展。</li>
<li>该数据集和代码将公开供公众使用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14911">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-9d626408036c3f4de8d88a062c5f0882.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7fb0523ca20153cf2f299e70a722d893.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3d282352ea9b853fe9fc1712f959adb4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8846420170919213121157aed47e6edb.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Matching-Skeleton-based-Activity-Representations-with-Heterogeneous-Signals-for-HAR"><a href="#Matching-Skeleton-based-Activity-Representations-with-Heterogeneous-Signals-for-HAR" class="headerlink" title="Matching Skeleton-based Activity Representations with Heterogeneous   Signals for HAR"></a>Matching Skeleton-based Activity Representations with Heterogeneous   Signals for HAR</h2><p><strong>Authors:Shuheng Li, Jiayun Zhang, Xiaohan Fu, Xiyuan Zhang, Jingbo Shang, Rajesh K. Gupta</strong></p>
<p>In human activity recognition (HAR), activity labels have typically been encoded in one-hot format, which has a recent shift towards using textual representations to provide contextual knowledge. Here, we argue that HAR should be anchored to physical motion data, as motion forms the basis of activity and applies effectively across sensing systems, whereas text is inherently limited. We propose SKELAR, a novel HAR framework that pretrains activity representations from skeleton data and matches them with heterogeneous HAR signals. Our method addresses two major challenges: (1) capturing core motion knowledge without context-specific details. We achieve this through a self-supervised coarse angle reconstruction task that recovers joint rotation angles, invariant to both users and deployments; (2) adapting the representations to downstream tasks with varying modalities and focuses. To address this, we introduce a self-attention matching module that dynamically prioritizes relevant body parts in a data-driven manner. Given the lack of corresponding labels in existing skeleton data, we establish MASD, a new HAR dataset with IMU, WiFi, and skeleton, collected from 20 subjects performing 27 activities. This is the first broadly applicable HAR dataset with time-synchronized data across three modalities. Experiments show that SKELAR achieves the state-of-the-art performance in both full-shot and few-shot settings. We also demonstrate that SKELAR can effectively leverage synthetic skeleton data to extend its use in scenarios without skeleton collections. </p>
<blockquote>
<p>在人类活动识别（HAR）中，活动标签通常被编码为一热格式，最近有转向使用文本表示以提供上下文知识的趋势。在这里，我们主张HAR应该以物理运动数据为基础，因为运动是活动的基础，并在不同的感应系统中有效应用，而文本则具有内在的局限性。我们提出了SKELAR，这是一种新型HAR框架，可以从骨架数据中预训练活动表示，并与异构HAR信号进行匹配。我们的方法解决了两个主要挑战：（1）捕获核心运动知识而不涉及特定上下文细节。我们通过自我监督的粗略角度重建任务实现这一点，该任务能够恢复关节旋转角度，对用户和部署具有不变性；（2）将表示形式适应具有不同模态和重点的下游任务。为解决此问题，我们引入了一个自我注意力匹配模块，该模块能够以数据驱动的方式动态地优先处理相关部位。考虑到现有骨架数据中缺少相应的标签，我们建立了MASD，这是一个新的HAR数据集，包含IMU、WiFi和骨架数据，从执行27项活动的20名受试者中收集。这是第一个在三模态中时间同步数据的广泛应用HAR数据集。实验表明，无论是在全镜头和少镜头设置中，SKELAR都达到了最先进的表现。我们还证明了SKELAR可以有效地利用合成骨架数据来扩展其在没有骨架收集的场景中的应用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14547v1">PDF</a> This paper is accepted by SenSys 2025</p>
<p><strong>Summary</strong></p>
<p>在人体活动识别（HAR）领域，以往多采用独热编码的活动标签方式，但近期逐渐向使用文本表示提供上下文知识转变。然而，我们主张HAR应以物理运动数据为基础，因为运动是活动的基础并有效适用于各种传感系统，而文本则存在固有局限性。为此，我们提出了SKELAR这一新型HAR框架，它能从骨架数据中预训练活动表示并与异质HAR信号匹配。此框架解决了两大挑战：一、捕捉核心运动知识而不涉及特定上下文细节，我们通过自监督的粗略角度重建任务实现这一点，能够恢复关节旋转角度，对用户部署均保持不变；二、针对具有不同模态和重点的下游任务进行自适应表示，为此我们引入了自注意力匹配模块，以数据驱动的方式动态优先处理重要部位。由于缺乏相应的骨架数据标签，我们建立了MASD这一新HAR数据集，包含IMU、WiFi和骨架数据，由20名受试者进行27项活动采集而成。这是首个适用于多种模态的通用HAR数据集。实验表明，SKELAR在全镜头和少镜头设置中都达到了最先进的性能。我们还证明了SKELAR在没有骨架收集的场景中能有效利用合成骨架数据。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>活动识别应基于物理运动数据，强调运动在活动中的基础地位及跨传感系统的适用性。</li>
<li>SKELAR框架解决了从骨架数据中预训练活动表示的挑战。</li>
<li>SKELAR通过自监督的粗略角度重建任务捕捉核心运动知识，对用户和部署均保持不变。</li>
<li>SKELAR通过自注意力匹配模块实现对不同下游任务的自适应表示。</li>
<li>建立了新的HAR数据集MASD，包含IMU、WiFi和骨架数据，适用于多种模态。</li>
<li>SKELAR在全镜头和少镜头设置中表现优越。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14547">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-4507ee7b32240c91f2f66bc5819a5170.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-841920e3c08e80e5f40542baedb5ad3f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b886daa18ff3459c28b3f909ea71e0b5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0c29179ae2da0ad6dbc04a77d768f60d.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-21/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-21/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-21/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-952bd772b48e9d7c2e223217ec9b0b76.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-03-21  ARC Anchored Representation Clouds for High-Resolution INR   Classification
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-21
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-21/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-e6116c0b8dbcca8d73715e625323a88b.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-03-21  SWEET-RL Training Multi-Turn LLM Agents on Collaborative Reasoning   Tasks
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-21
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">28791.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
