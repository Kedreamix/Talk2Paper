<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-03-21  FP4DiT Towards Effective Floating Point Quantization for Diffusion   Transformers">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-762573e30cb534041dcb372a9e017808.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-03-21
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    19.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    77 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-03-21-更新"><a href="#2025-03-21-更新" class="headerlink" title="2025-03-21 更新"></a>2025-03-21 更新</h1><h2 id="FP4DiT-Towards-Effective-Floating-Point-Quantization-for-Diffusion-Transformers"><a href="#FP4DiT-Towards-Effective-Floating-Point-Quantization-for-Diffusion-Transformers" class="headerlink" title="FP4DiT: Towards Effective Floating Point Quantization for Diffusion   Transformers"></a>FP4DiT: Towards Effective Floating Point Quantization for Diffusion   Transformers</h2><p><strong>Authors:Ruichen Chen, Keith G. Mills, Di Niu</strong></p>
<p>Diffusion Models (DM) have revolutionized the text-to-image visual generation process. However, the large computational cost and model footprint of DMs hinders practical deployment, especially on edge devices. Post-training quantization (PTQ) is a lightweight method to alleviate these burdens without the need for training or fine-tuning. While recent DM PTQ methods achieve W4A8 on integer-based PTQ, two key limitations remain: First, while most existing DM PTQ methods evaluate on classical DMs like Stable Diffusion XL, 1.5 or earlier, which use convolutional U-Nets, newer Diffusion Transformer (DiT) models like the PixArt series, Hunyuan and others adopt fundamentally different transformer backbones to achieve superior image synthesis. Second, integer (INT) quantization is prevailing in DM PTQ but doesn’t align well with the network weight and activation distribution, while Floating-Point Quantization (FPQ) is still under-investigated, yet it holds the potential to better align the weight and activation distributions in low-bit settings for DiT. In response, we introduce FP4DiT, a PTQ method that leverages FPQ to achieve W4A6 quantization. Specifically, we extend and generalize the Adaptive Rounding PTQ technique to adequately calibrate weight quantization for FPQ and demonstrate that DiT activations depend on input patch data, necessitating robust online activation quantization techniques. Experimental results demonstrate that FP4DiT outperforms integer-based PTQ at W4A6 and W4A8 precision and generates convincing visual content on PixArt-$\alpha$, PixArt-$\Sigma$ and Hunyuan in terms of several T2I metrics such as HPSv2 and CLIP. </p>
<blockquote>
<p>扩散模型（DM）已经彻底改变了文本到图像的视觉生成过程。然而，扩散模型的大量计算成本和模型占用空间阻碍了其实践部署，尤其是在边缘设备上。后训练量化（PTQ）是一种轻量级方法，可以在无需训练和微调的情况下缓解这些负担。尽管最近的扩散模型PTQ方法实现了基于整数的W4A8量化，但仍存在两个主要局限性：首先，尽管现有的大多数扩散模型PTQ方法都是在诸如Stable Diffusion XL 1.5或更早的经典扩散模型上进行评估的，这些模型使用卷积U-Nets，但更新的扩散转换器（DiT）模型，如PixArt系列、Hunyuan和其他模型采用根本不同的转换器骨干网来实现优越的图像合成。其次，整数（INT）量化在DM PTQ中很普遍，但并不能很好地与网络权重和激活分布相匹配。而浮点量化（FPQ）的研究仍然不足，但它具有在低位设置中更好地对齐DiT权重和激活分布的潜力。作为回应，我们介绍了FP4DiT，这是一种利用FPQ实现W4A6量化的PTQ方法。具体来说，我们对自适应舍入PTQ技术进行了扩展和概括，以充分校准FPQ的权重量化，并证明DiT的激活取决于输入补丁数据，这需要稳健的在线激活量化技术。实验结果表明，FP4DiT在W4A6和W4A8精度上优于基于整数的PTQ，并在PixArt-α、PixArt-Σ和Hunyuan等多个T2I指标（如HPSv2和CLIP）上生成了令人信服的视觉内容。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15465v1">PDF</a> The code is available at <a target="_blank" rel="noopener" href="https://github.com/cccrrrccc/FP4DiT">https://github.com/cccrrrccc/FP4DiT</a></p>
<p><strong>摘要</strong></p>
<p>扩散模型（DM）在文本到图像的视觉生成过程中起到了革命性的作用，但其庞大的计算成本和模型占用空间阻碍了实际应用，特别是在边缘设备上。针对这一问题，无需训练或微调的后训练量化（PTQ）是一种轻量级方法。虽然最近的扩散模型PTQ方法实现了整数量化的W4A8精度，但仍存在两个关键局限：首先，大多数现有的扩散模型PTQ方法是在使用卷积U-Net的经典扩散模型（如Stable Diffusion XL 1.5等）上评估的，而新一代如PixArt系列和Hunyuan等扩散变换器（DiT）模型采用了根本不同的转换器主干，以实现更优越的图像合成。其次，整数量化（INT）在扩散模型PTQ中占据主导地位，但与网络权重和激活分布不太匹配。虽然浮点量化（FPQ）仍处于研究初期阶段，但其在对齐低比特设置中的权重和激活分布方面具有潜力。为了解决这个问题，我们提出了一种基于FPQ的后训练量化方法FP4DiT，并实现了W4A6量化。具体来说，我们扩展并推广了自适应四舍五入PTQ技术来校准权重量化以适应FPQ，并证明了DiT激活依赖于输入补丁数据，需要稳健的在线激活量化技术。实验结果表明，FP4DiT在PixArt-α、PixArt-Σ和Hunyuan等多种型号的T2I度量标准下表现优异，例如在HPSv2和CLIP中表现出生成可信的视觉内容能力，并且在W4A6和W4A8精度下超过了基于整数的PTQ。 </p>
<p><strong>要点分析</strong> </p>
<p>扩散模型已经引发了文本到图像生成过程的革命性变革。然而由于其计算成本较高且占用空间较大，限制了其在边缘设备上的实际应用部署。对此问题提出了后训练量化（PTQ）的轻量级解决方案。现有方法主要集中在整数量化上，但这种方法对网络权重和激活分布不太友好，需要进一步探讨优化方法。此外大部分PTQ方法的测试是基于经典的扩散模型架构进行的评估测试与新型扩散变换器模型存在较大差异且并未深入探究。在此背景下本文提出了FP4DiT这一创新型的PTQ方案以满足轻量级低能耗场景的应用需求重点涉及两大方向对扩收缩改进提升了对不同数据集的整体效果并展现出强大的性能优势。具体来说本文的创新点包括： </p>
<ol>
<li>针对新型扩散变换器模型的特性进行了深入研究实现了该类模型的精细化量化同时保持了较好的性能表现这极大推动了其在实际应用场景下的应用可能性与效果提升； </li>
<li>基于浮点数量化理论进行优化克服了传统的基于整数的量化的缺点网络权重的匹配和激活分布的精确度得到进一步提升使得模型在低比特设置下表现更为出色； </li>
<li>提出一种全新的在线激活量化技术该方法能够有效处理激活数据的输入依赖性保证模型在不同数据集下的鲁棒性提升了模型的通用性和实用性；</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15465">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-69529399f19b98f91d5f448e8968ea0b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-98853f1f3e4df7d6aab34597c6d7fed0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c11b54c8c4cd4955ada30be0b644cfdf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1837dcbb4e919a8b0949481e6861d57d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-19a32d91539562c4529312133bfec74f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2bc95b9183933de04293a6489f3628e0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Visual-Persona-Foundation-Model-for-Full-Body-Human-Customization"><a href="#Visual-Persona-Foundation-Model-for-Full-Body-Human-Customization" class="headerlink" title="Visual Persona: Foundation Model for Full-Body Human Customization"></a>Visual Persona: Foundation Model for Full-Body Human Customization</h2><p><strong>Authors:Jisu Nam, Soowon Son, Zhan Xu, Jing Shi, Difan Liu, Feng Liu, Aashish Misraa, Seungryong Kim, Yang Zhou</strong></p>
<p>We introduce Visual Persona, a foundation model for text-to-image full-body human customization that, given a single in-the-wild human image, generates diverse images of the individual guided by text descriptions. Unlike prior methods that focus solely on preserving facial identity, our approach captures detailed full-body appearance, aligning with text descriptions for body structure and scene variations. Training this model requires large-scale paired human data, consisting of multiple images per individual with consistent full-body identities, which is notoriously difficult to obtain. To address this, we propose a data curation pipeline leveraging vision-language models to evaluate full-body appearance consistency, resulting in Visual Persona-500K, a dataset of 580k paired human images across 100k unique identities. For precise appearance transfer, we introduce a transformer encoder-decoder architecture adapted to a pre-trained text-to-image diffusion model, which augments the input image into distinct body regions, encodes these regions as local appearance features, and projects them into dense identity embeddings independently to condition the diffusion model for synthesizing customized images. Visual Persona consistently surpasses existing approaches, generating high-quality, customized images from in-the-wild inputs. Extensive ablation studies validate design choices, and we demonstrate the versatility of Visual Persona across various downstream tasks. </p>
<blockquote>
<p>我们介绍了Visual Persona，这是一种面向文本到图像全身人体定制的基础模型。给定一张野生环境下的单人图像，该模型会根据文本描述生成多样化的个人图像。与以往仅专注于保留面部身份的方法不同，我们的方法捕捉了详细的全身外观，与文本描述的身体结构和场景变化保持一致。训练这个模型需要大规模配对的人体数据，对于每个个体，都需要有多张具有一致全身身份的图像，这些数据是众所周知的难以获得的。为了解决这个问题，我们提出了一种利用视觉语言模型评估全身外观一致性的数据整理流程，从而形成了Visual Persona-500K数据集，包含580k张配对的人体图像，涉及10万张独特的身份。为了实现精确的外观转移，我们引入了一种适应于预训练文本到图像扩散模型的transformer编码器-解码器架构。它将输入图像分割为不同的身体区域，将这些区域编码为局部外观特征，并将它们投影到密集的身份嵌入中，以独立地控制扩散模型来合成定制的图像。Visual Persona始终超越现有方法，从野生环境中生成高质量、定制的图像。广泛的消融研究验证了设计选择，我们展示了Visual Persona在各种下游任务中的通用性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15406v1">PDF</a> CVPR 2025, Project page is available at   <a target="_blank" rel="noopener" href="https://cvlab-kaist.github.io/Visual-Persona">https://cvlab-kaist.github.io/Visual-Persona</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了Visual Persona模型，这是一种面向文本到图像的全身人体定制模型。给定一张野生环境下的单人图像，该模型能够根据文本描述生成多样化的个体图像。与仅关注面部身份保留的现有方法不同，Visual Persona模型能够捕捉全身的详细外观，并根据文本描述进行身体结构和场景变化的对齐。为训练此模型，需要大规模配对的人类数据，包含每张个体图像都有一致的全身身份。为解决数据获取难题，研究团队提出了一个数据整理管道，利用视觉语言模型评估全身外观的一致性，并创建了Visual Persona-500K数据集，包含58万张配对的人类图像和10万个唯一身份。为实现精确的外观转移，研究团队引入了变压器编码器-解码器架构，该架构适应于预训练的文本到图像扩散模型，可将输入图像分为不同的身体区域，编码为局部外观特征，并投影到密集的身份嵌入中，以条件扩散模型合成定制图像。Visual Persona在生成高质量、定制的图像方面超越了现有方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Visual Persona是一个面向文本到图像的全身人体定制模型。</li>
<li>该模型通过文本描述生成多样化的个体图像。</li>
<li>Visual Persona模型不仅关注面部身份，还捕捉全身的详细外观。</li>
<li>为训练此模型，需要大规模配对的人类数据，包含每张个体图像都有一致的全身身份。</li>
<li>研究团队利用视觉语言模型创建了一个数据整理管道，并创建了Visual Persona-500K数据集。</li>
<li>Visual Persona采用了变压器编码器-解码器架构，并结合预训练的文本到图像扩散模型实现精确外观转移。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15406">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d834701677500103c02e9ef72774420b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e25a9f84fd426fdd8d5a460a3d38f3a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1c92e379e533959e5ec2fab7a446485a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-77048ff89ff89af9f35a29e75239d916.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Material-Decomposition-in-Photon-Counting-Computed-Tomography-with-Diffusion-Models-Comparative-Study-and-Hybridization-with-Variational-Regularizers"><a href="#Material-Decomposition-in-Photon-Counting-Computed-Tomography-with-Diffusion-Models-Comparative-Study-and-Hybridization-with-Variational-Regularizers" class="headerlink" title="Material Decomposition in Photon-Counting Computed Tomography with   Diffusion Models: Comparative Study and Hybridization with Variational   Regularizers"></a>Material Decomposition in Photon-Counting Computed Tomography with   Diffusion Models: Comparative Study and Hybridization with Variational   Regularizers</h2><p><strong>Authors:Corentin Vazia, Thore Dassow, Alexandre Bousse, Jacques Froment, Béatrice Vedel, Franck Vermet, Alessandro Perelli, Jean-Pierre Tasu, Dimitris Visvikis</strong></p>
<p>Photon-counting computed tomography (PCCT) has emerged as a promising imaging technique, enabling spectral imaging and material decomposition (MD). However, images typically suffer from a low signal-to-noise ratio due to constraints such as low photon counts and sparse-view settings. Variational methods minimize a data-fit function coupled with handcrafted regularizers but are highly dependent on the choice of the regularizers. Artificial intelligence (AI)-based approaches and more particularly convolutional neural networks (CNNs) are now considered the state-of-the-art approach and can be used as an end-to-end method for MD or to implicitly learn an a priori. In the last few years, diffusion models (DMs) became predominant in the field of generative models where a distribution function is learned. This distribution function can be used as a prior for solving inverse problems. This work investigates the use of DMs as regularizers for MD tasks in PCCT. MD by diffusion posterior sampling (DPS) can be achieved. Three DPS-based approaches – image-domain two-step DPS (im-TDPS), projection-domain two-step DPS (proj-TDPS), and one-step DPS (ODPS) – are evaluated. The first two methods perform MD in two steps: im-TDPS samples spectral images by DPS then performs image-based MD, while proj-TDPS performs projection-based MD then samples material images by DPS. The last method, ODPS, samples the material images directly from the measurement data. The results indicate that ODPS achieves superior performance compared to im-TDPS and proj-TDPS, providing sharper, noise-free and crosstalk-free images. Furthermore, we introduce a novel hybrid ODPS method combining DM priors with standard variational regularizers for scenarios involving materials absent from the training dataset. This hybrid method demonstrates improved material reconstruction quality compared to a standard variational method. </p>
<blockquote>
<p>光子计数计算机断层扫描（PCCT）作为一种有前景的成像技术已经出现，能够实现光谱成像和材料分解（MD）。然而，由于光子计数低和稀疏视图设置等限制，图像通常信噪比低。变分方法通过最小化数据拟合函数与手工正则化相结合，但高度依赖于正则化的选择。基于人工智能（AI）的方法，特别是卷积神经网络（CNN）现在被认为是最新技术，可以用作端到端的MD方法或隐式学习先验知识。在过去的几年中，扩散模型（DMs）在生成模型中占据了主导地位，其中分布函数得以学习。该分布函数可以用作解决逆问题的先验。这项工作探讨了将DMs用作PCCT中MD任务的正规化方法。通过扩散后采样（DPS）可以实现MD。评估了三种基于DPS的方法——图像域两步DPS（im-TDPS）、投影域两步DPS（proj-TDPS）和一步DPS（ODPS）。前两种方法分两步进行MD：im-TDPS通过DPS采样光谱图像，然后进行基于图像的MD，而proj-TDPS进行基于投影的MD，然后通过DPS采样材料图像。最后一种方法ODPS直接从测量数据中采样材料图像。结果表明，与im-TDPS和proj-TDPS相比，ODPS实现了卓越的性能，提供了更清晰、无噪声且无串扰的图像。此外，我们引入了一种新型混合ODPS方法，将DM先验与标准变分正则化器相结合，用于涉及训练数据集中缺少的材料的情况。该混合方法提高了材料重建的质量，与标准变分方法相比表现出优势。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15383v1">PDF</a> 12 pages, 10 figures, 4 tables</p>
<p><strong>Summary</strong></p>
<p>光子计数计算机断层扫描（PCCT）中的物质分解（MD）技术因图像的低信噪比而受到挑战，如光子计数低和视图稀疏等问题。卷积神经网络（CNN）已成为目前最前沿的技术，可作为端到端的MD方法或隐式学习先验。扩散模型（DM）可用于生成模型中的先验分布函数来解决反问题。本研究探讨了将DM作为PCCT中MD任务的正规化器。通过扩散后采样（DPS）实现MD。评估了三种基于DPS的方法，包括图像域两步DPS、投影域两步DPS和一步DPS。结果显示，一步DPS性能最优，能提供清晰、无噪声且无串扰的图像。此外，还介绍了一种结合DM先验与传统变分正则化的混合一步DPS方法，用于处理训练数据集中缺少的材料场景，该方法提高了材料重建质量。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>光子计数计算机断层扫描（PCCT）中的物质分解（MD）技术面临低信噪比问题。</li>
<li>卷积神经网络（CNN）已成为解决MD问题的最新技术。</li>
<li>扩散模型（DM）可用作生成模型中的先验分布函数，以解决反问题。</li>
<li>本研究探讨了将DM用作PCCT中MD任务的正规化器，采用扩散后采样（DPS）方法实现。</li>
<li>三种基于DPS的方法中，一步DPS性能最优，能提供清晰、无噪声且无串扰的图像。</li>
<li>引入了一种结合DM先验与传统变分正则化的混合方法，用于处理训练数据集中缺少的材料场景。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15383">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-eb9be5375c5148986f6dc31e734f2045.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2ca4bbff21bfcec40966236130414fd3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1c0b7087ab4a8396851be33d920432c4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b4040de9acababdd43fba2fc46efab54.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1f95eb72229ea129a163c45678d6816f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-359b63a1d388d81161cf36a17488221e.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Detect-and-Guide-Self-regulation-of-Diffusion-Models-for-Safe-Text-to-Image-Generation-via-Guideline-Token-Optimization"><a href="#Detect-and-Guide-Self-regulation-of-Diffusion-Models-for-Safe-Text-to-Image-Generation-via-Guideline-Token-Optimization" class="headerlink" title="Detect-and-Guide: Self-regulation of Diffusion Models for Safe   Text-to-Image Generation via Guideline Token Optimization"></a>Detect-and-Guide: Self-regulation of Diffusion Models for Safe   Text-to-Image Generation via Guideline Token Optimization</h2><p><strong>Authors:Feifei Li, Mi Zhang, Yiming Sun, Min Yang</strong></p>
<p>Text-to-image diffusion models have achieved state-of-the-art results in synthesis tasks; however, there is a growing concern about their potential misuse in creating harmful content. To mitigate these risks, post-hoc model intervention techniques, such as concept unlearning and safety guidance, have been developed. However, fine-tuning model weights or adapting the hidden states of the diffusion model operates in an uninterpretable way, making it unclear which part of the intermediate variables is responsible for unsafe generation. These interventions severely affect the sampling trajectory when erasing harmful concepts from complex, multi-concept prompts, thus hindering their practical use in real-world settings. In this work, we propose the safe generation framework Detect-and-Guide (DAG), leveraging the internal knowledge of diffusion models to perform self-diagnosis and fine-grained self-regulation during the sampling process. DAG first detects harmful concepts from noisy latents using refined cross-attention maps of optimized tokens, then applies safety guidance with adaptive strength and editing regions to negate unsafe generation. The optimization only requires a small annotated dataset and can provide precise detection maps with generalizability and concept specificity. Moreover, DAG does not require fine-tuning of diffusion models, and therefore introduces no loss to their generation diversity. Experiments on erasing sexual content show that DAG achieves state-of-the-art safe generation performance, balancing harmfulness mitigation and text-following performance on multi-concept real-world prompts. </p>
<blockquote>
<p>文本到图像的扩散模型在合成任务中取得了最先进的成果；然而，关于其可能用于创建有害内容的潜在滥用问题日益受到关注。为了减轻这些风险，开发了事后模型干预技术，如概念遗忘和安全指导。然而，微调扩散模型的权重或调整隐藏状态的方式是不可解释的，因此不清楚中间变量中的哪一部分负责产生不安全的内容。这些干预在从复杂的多概念提示中删除有害概念时，严重影响采样轨迹，从而阻碍了它们在现实世界设置中的实际应用。在这项工作中，我们提出了安全生成框架Detect-and-Guide（DAG），利用扩散模型的内部知识在采样过程中进行自诊断和精细的自我保护。DAG首先使用优化令牌的精细交叉注意力图从嘈杂的潜在空间中检测有害概念，然后应用自适应强度和编辑区域的安全指导来抵消不安全的生成。优化只需要一个小型注释数据集，就可以提供具有通用性和概念特异性的精确检测图。此外，DAG不需要对扩散模型进行微调，因此不会对其生成多样性造成损失。在消除性内容的实验方面，DAG实现了最先进的安全生成性能，在具有多概念的真实世界提示中平衡了有害性的减轻和遵循文本的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15197v1">PDF</a> CVPR25</p>
<p><strong>摘要</strong><br>扩散模型在合成任务中取得了最先进的成果，但其潜在滥用风险引发关注。为缓解风险，采取了事后模型干预技术，如概念遗忘和安全指导等。然而，对扩散模型的权重微调或隐藏状态的调整方式缺乏可解释性，使得中间变量在产生不安全内容时难以明确其责任归属。本工作提出安全生成框架Detect-and-Guide（DAG），利用扩散模型的内部知识在采样过程中进行自诊断和精细化的自我调控。DAG首先使用优化后的标记的精细交叉注意力图检测潜在噪声中的有害概念，然后应用自适应强度和编辑区域的安全指导来避免不安全内容的生成。优化仅需少量标注数据集即可提供精确的检测图，具有通用性和概念特异性。此外，DAG不需要对扩散模型进行微调，因此不会损失其生成多样性。在消除色情内容实验中，DAG在安全生成方面取得最先进的成果，平衡了危害减轻和遵循文本的多个实际概念提示性能。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>文本至图像扩散模型在合成任务上取得顶尖成果，但存在潜在滥用风险。</li>
<li>为应对风险，采取了事后模型干预技术，但现有技术影响采样轨迹并减少生成多样性。</li>
<li>提出安全生成框架Detect-and-Guide（DAG），利用扩散模型的内部知识进行自我诊断和调控。</li>
<li>DAG通过检测有害概念并提供安全指导来实现精确控制和优化生成内容。</li>
<li>DAG采用小型标注数据集即可实现精确检测图，具有通用性和概念特异性。</li>
<li>DAG无需微调扩散模型权重，维持模型的生成多样性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15197">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-c234b50800fb22675eb6ea53e7617cbf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6817ee9514bbb293a4de62a56f5969a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6f759d9a8ef4351fa320bb974177c93b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9c9124092bb6af40675d83a1e7d09801.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1d5c2d3f4403b88b43dd2a5de0623b88.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-35c2248fbb5598b6a4efb0b51453011f.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Single-Step-Bidirectional-Unpaired-Image-Translation-Using-Implicit-Bridge-Consistency-Distillation"><a href="#Single-Step-Bidirectional-Unpaired-Image-Translation-Using-Implicit-Bridge-Consistency-Distillation" class="headerlink" title="Single-Step Bidirectional Unpaired Image Translation Using Implicit   Bridge Consistency Distillation"></a>Single-Step Bidirectional Unpaired Image Translation Using Implicit   Bridge Consistency Distillation</h2><p><strong>Authors:Suhyeon Lee, Kwanyoung Kim, Jong Chul Ye</strong></p>
<p>Unpaired image-to-image translation has seen significant progress since the introduction of CycleGAN. However, methods based on diffusion models or Schr&quot;odinger bridges have yet to be widely adopted in real-world applications due to their iterative sampling nature. To address this challenge, we propose a novel framework, Implicit Bridge Consistency Distillation (IBCD), which enables single-step bidirectional unpaired translation without using adversarial loss. IBCD extends consistency distillation by using a diffusion implicit bridge model that connects PF-ODE trajectories between distributions. Additionally, we introduce two key improvements: 1) distribution matching for consistency distillation and 2) adaptive weighting method based on distillation difficulty. Experimental results demonstrate that IBCD achieves state-of-the-art performance on benchmark datasets in a single generation step. Project page available at <a target="_blank" rel="noopener" href="https://hyn2028.github.io/project_page/IBCD/index.html">https://hyn2028.github.io/project_page/IBCD/index.html</a> </p>
<blockquote>
<p>非配对图像到图像的翻译自CycleGAN提出以来取得了重大进展。然而，基于扩散模型或Schrödinger桥梁的方法由于其迭代采样的特性，尚未在真实世界应用中广泛采用。为了应对这一挑战，我们提出了一种新型框架——隐桥一致性蒸馏（IBCD），它能够在不使用对抗性损失的情况下实现单步双向非配对翻译。IBCD通过采用连接分布间PF-ODE轨迹的扩散隐桥模型，扩展了一致性蒸馏。此外，我们还引入了两种关键改进：1）用于一致性蒸馏的分布匹配；2）基于蒸馏难度的自适应加权方法。实验结果表明，IBCD在基准数据集上实现了单步生成的最佳性能。项目页面可通过<a target="_blank" rel="noopener" href="https://hyn2028.github.io/project_page/IBCD/index.html%E8%AE%BF%E9%97%AE%E3%80%82">https://hyn2028.github.io/project_page/IBCD/index.html访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15056v1">PDF</a> 25 pages, 16 figures</p>
<p><strong>Summary</strong><br>     循环生成对抗网络（CycleGAN）推出后，非配对图像到图像转换取得显著进展。然而，基于扩散模型或薛定谔桥的方法由于迭代采样性质尚未在现实世界应用中广泛采用。为解决这一挑战，我们提出一种新型框架——隐桥一致性蒸馏（IBCD），实现无需对抗性损失的单步双向非配对翻译。IBCD通过扩散隐桥模型连接概率流常微分方程轨迹，扩展一致性蒸馏。此外，我们引入两个关键改进：1）用于一致性蒸馏的分布匹配；2）基于蒸馏难度的自适应加权方法。实验结果表明，IBCD在基准数据集上实现单次生成步骤的最佳性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>循环生成对抗网络（CycleGAN）推动了非配对图像到图像转换的进展。</li>
<li>扩散模型和薛定谔桥方法由于迭代采样的性质，在现实世界应用中的普及受到限制。</li>
<li>提出新型框架——隐桥一致性蒸馏（IBCD），实现无需对抗性损失的单步双向非配对翻译。</li>
<li>IBCD通过扩散隐桥模型连接概率流常微分方程轨迹，扩展了一致性蒸馏的概念。</li>
<li>IBCD引入分布匹配用于一致性蒸馏。</li>
<li>IBCD采用基于蒸馏难度的自适应加权方法。</li>
<li>实验结果显示，IBCD在基准数据集上实现了单次生成步骤的最佳性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15056">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-90433ecf976f002d86b148258f2efa41.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cceec70611475697114a4a2d624eed1a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-63361ae4d82df26c967ea382caa9918d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5b8c90d55407b05a5b4e0ef9e64013e4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0eb598331204a4ea7f0237b8ad099bc8.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Exploiting-Diffusion-Prior-for-Real-World-Image-Dehazing-with-Unpaired-Training"><a href="#Exploiting-Diffusion-Prior-for-Real-World-Image-Dehazing-with-Unpaired-Training" class="headerlink" title="Exploiting Diffusion Prior for Real-World Image Dehazing with Unpaired   Training"></a>Exploiting Diffusion Prior for Real-World Image Dehazing with Unpaired   Training</h2><p><strong>Authors:Yunwei Lan, Zhigao Cui, Chang Liu, Jialun Peng, Nian Wang, Xin Luo, Dong Liu</strong></p>
<p>Unpaired training has been verified as one of the most effective paradigms for real scene dehazing by learning from unpaired real-world hazy and clear images. Although numerous studies have been proposed, current methods demonstrate limited generalization for various real scenes due to limited feature representation and insufficient use of real-world prior. Inspired by the strong generative capabilities of diffusion models in producing both hazy and clear images, we exploit diffusion prior for real-world image dehazing, and propose an unpaired framework named Diff-Dehazer. Specifically, we leverage diffusion prior as bijective mapping learners within the CycleGAN, a classic unpaired learning framework. Considering that physical priors contain pivotal statistics information of real-world data, we further excavate real-world knowledge by integrating physical priors into our framework. Furthermore, we introduce a new perspective for adequately leveraging the representation ability of diffusion models by removing degradation in image and text modalities, so as to improve the dehazing effect. Extensive experiments on multiple real-world datasets demonstrate the superior performance of our method. Our code <a target="_blank" rel="noopener" href="https://github.com/ywxjm/Diff-Dehazer">https://github.com/ywxjm/Diff-Dehazer</a>. </p>
<blockquote>
<p>无配对训练已通过从不成对的真实世界雾霾和清晰图像中学习验证为最有效的实景去雾范式之一。尽管已经提出了许多研究，但当前的方法由于特征表示有限和真实世界先验的利用不足，对各种真实场景的泛化能力有限。受扩散模型在生成雾霾和清晰图像方面的强大生成能力的启发，我们利用扩散先验进行真实图像去雾，并提出了一种无配对的框架Diff-Dehazer。具体来说，我们在CycleGAN（一种经典的无配对学习框架）中利用扩散先验作为双向映射学习者。考虑到物理先验包含真实世界数据的关键统计信息，我们进一步通过整合物理先验到我们的框架中来挖掘真实世界的知识。此外，我们通过去除图像和文本模态的退化，为充分利用扩散模型的表示能力引入了新的视角，以提高去雾效果。在多个真实世界数据集上的广泛实验证明了我们方法的优越性。我们的代码为 <a target="_blank" rel="noopener" href="https://github.com/ywxjm/Diff-Dehazer%E3%80%82">https://github.com/ywxjm/Diff-Dehazer。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15017v1">PDF</a> Accepted by AAAI2025</p>
<p><strong>摘要</strong></p>
<p>利用无配对训练方式，通过从现实世界的雾霾和清晰图像中学习，实现真实场景去雾的最有效方法之一。虽然已有许多相关研究，但受限于特征表达和未能充分利用现实世界的先验知识，现有方法的泛化能力受限。受到扩散模型在生成雾霾和清晰图像方面的强大能力的启发，本文利用扩散先验进行真实图像去雾，提出了一种名为Diff-Dehazer的无配对框架。具体来说，我们在经典的CycleGAN无配对学习框架中引入扩散先验作为双向映射学习者。考虑到物理先验包含现实世界数据的关键统计信息，我们进一步通过整合物理先验来挖掘现实世界知识。此外，通过去除图像和文本模态的退化，引入了充分利用扩散模型表达能力的新视角，以提高去雾效果。在多个真实世界数据集上的实验证明了我们的方法具有卓越性能。代码链接：<a target="_blank" rel="noopener" href="https://github.com/ywxjm/Diff-Dehazer">https://github.com/ywxjm/Diff-Dehazer</a>。</p>
<p><strong>关键见解</strong></p>
<ul>
<li>无配对训练已被验证为从现实世界的雾霾和清晰图像中学习去雾最有效的范式之一。</li>
<li>扩散模型因其强大的生成能力被用于去雾任务中，提出了利用扩散先验的Diff-Dehazer框架。</li>
<li>在经典的无配对学习框架CycleGAN中引入扩散先验作为双向映射学习者。</li>
<li>通过整合物理先验挖掘现实世界知识，提高去雾效果。</li>
<li>引入新的视角，通过去除图像和文本模态的退化来充分利用扩散模型的表达能力。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15017">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9f7a11e82c401a085378b40145b8cf9e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c93a92167929754bfbbe6ea9c69b07b4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-95988624282b4f6c1afffcdce49097b9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-da33ea6241328a4c20bb60df485462d0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1d117ad5760bc24b2381e14633f6b11b.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Language-based-Image-Colorization-A-Benchmark-and-Beyond"><a href="#Language-based-Image-Colorization-A-Benchmark-and-Beyond" class="headerlink" title="Language-based Image Colorization: A Benchmark and Beyond"></a>Language-based Image Colorization: A Benchmark and Beyond</h2><p><strong>Authors:Yifan Li, Shuai Yang, Jiaying Liu</strong></p>
<p>Image colorization aims to bring colors back to grayscale images. Automatic image colorization methods, which requires no additional guidance, struggle to generate high-quality images due to color ambiguity, and provides limited user controllability. Thanks to the emergency of cross-modality datasets and models, language-based colorization methods are proposed to fully utilize the efficiency and flexibly of text descriptions to guide colorization. In view of the lack of a comprehensive review of language-based colorization literature, we conduct a thorough analysis and benchmarking. We first briefly summarize existing automatic colorization methods. Then, we focus on language-based methods and point out their core challenge on cross-modal alignment. We further divide these methods into two categories: one attempts to train a cross-modality network from scratch, while the other utilizes the pre-trained cross-modality model to establish the textual-visual correspondence. Based on the analyzed limitations of existing language-based methods, we propose a simple yet effective method based on distilled diffusion model. Extensive experiments demonstrate that our simple baseline can produces better results than previous complex methods with 14 times speed up. To the best of our knowledge, this is the first comprehensive review and benchmark on language-based image colorization field, providing meaningful insights for the community. The code is available at <a target="_blank" rel="noopener" href="https://github.com/lyf1212/Color-Turbo">https://github.com/lyf1212/Color-Turbo</a>. </p>
<blockquote>
<p>图像彩色化的目标是给黑白图像恢复色彩。自动图像彩色化方法无需额外指导，但由于色彩模糊度，其在生成高质量图像方面存在困难，且用户可控性有限。得益于跨模态数据集和模型的兴起，基于语言的彩色化方法被提出，以充分利用文本描述的效率和灵活性来指导彩色化。鉴于缺乏基于语言的彩色化文献的全面综述，我们进行了深入分析和基准测试。首先，我们简要总结了现有的自动彩色化方法。然后，我们重点关注基于语言的方法，并指出了它们在跨模态对齐方面的核心挑战。我们进一步将这些方法分为两类：一类试图从头开始训练一个跨模态网络，另一类则利用预训练的跨模态模型来建立文本-视觉对应关系。基于对现有基于语言的方法的分析和局限性，我们提出了一种简单而有效的方法，该方法基于蒸馏扩散模型。大量实验表明，我们简单的基线方法能够在比以前的复杂方法快14倍的情况下产生更好的结果。据我们所知，这是语言驱动图像彩色化领域的首次全面综述和基准测试，为社区提供了有意义的见解。代码可通过<a target="_blank" rel="noopener" href="https://github.com/lyf1212/Color-Turbo%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/lyf1212/Color-Turbo获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14974v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了图像彩色化的目标和方法。自动图像彩色化方法因色彩模糊和用户控制有限而难以生成高质量图像。近年来，基于语言描述的彩色化方法因其效率和灵活性而受到关注。本文对基于语言的彩色化文献进行了全面综述和基准测试，总结了现有自动彩色化方法的不足，指出了基于语言的方法在跨模态对齐方面的核心挑战，并提出了基于蒸馏扩散模型的简单有效方法。实验表明，该简单基线方法比以前的复杂方法更好，速度提高了14倍。本文是对基于语言的图像彩色化领域的首次全面审查和基准测试，为社区提供了有意义的见解。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>图像彩色化的目标是恢复灰度图像的色彩。</li>
<li>自动图像彩色化方法因色彩模糊和用户控制有限而面临挑战。</li>
<li>基于语言描述的彩色化方法利用文本描述来提高效率和灵活性。</li>
<li>本文对基于语言的图像彩色化方法进行了全面综述和基准测试。</li>
<li>现有自动彩色化方法存在局限性，需要新的方法改进。</li>
<li>本文提出了一种基于蒸馏扩散模型的简单有效图像彩色化方法，实验表现优异。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14974">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-c52aae32e5528349fdf76495c7da5d43.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2a68120e45d82b9881ac5a6fa9f8cd0b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-72e6a6b7fe48603fdfae212f8795f802.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fc2f31dc2288185d1583c8880ea43b65.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d947b614d6647c1d23b47d0b9a59654.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Ultrasound-Image-to-Video-Synthesis-via-Latent-Dynamic-Diffusion-Models"><a href="#Ultrasound-Image-to-Video-Synthesis-via-Latent-Dynamic-Diffusion-Models" class="headerlink" title="Ultrasound Image-to-Video Synthesis via Latent Dynamic Diffusion Models"></a>Ultrasound Image-to-Video Synthesis via Latent Dynamic Diffusion Models</h2><p><strong>Authors:Tingxiu Chen, Yilei Shi, Zixuan Zheng, Bingcong Yan, Jingliang Hu, Xiao Xiang Zhu, Lichao Mou</strong></p>
<p>Ultrasound video classification enables automated diagnosis and has emerged as an important research area. However, publicly available ultrasound video datasets remain scarce, hindering progress in developing effective video classification models. We propose addressing this shortage by synthesizing plausible ultrasound videos from readily available, abundant ultrasound images. To this end, we introduce a latent dynamic diffusion model (LDDM) to efficiently translate static images to dynamic sequences with realistic video characteristics. We demonstrate strong quantitative results and visually appealing synthesized videos on the BUSV benchmark. Notably, training video classification models on combinations of real and LDDM-synthesized videos substantially improves performance over using real data alone, indicating our method successfully emulates dynamics critical for discrimination. Our image-to-video approach provides an effective data augmentation solution to advance ultrasound video analysis. Code is available at <a target="_blank" rel="noopener" href="https://github.com/MedAITech/U_I2V">https://github.com/MedAITech/U_I2V</a>. </p>
<blockquote>
<p>超声波视频分类可实现自动化诊断，已成为重要的研究领域。然而，可用的超声波视频数据集仍然稀缺，阻碍了有效视频分类模型的开发进展。我们提出通过合成合理的超声波视频来解决这一短缺问题，这些视频由易于获取且丰富的超声波图像合成而来。为此，我们引入了一种潜在动态扩散模型（Latent Dynamic Diffusion Model, LDDM），以高效地将静态图像转换为具有真实视频特性的动态序列。我们在BUSV基准测试上展示了强大的定量结果和视觉上令人满意的合成视频。值得注意的是，使用真实和LDDM合成视频组合训练的视频分类模型，其性能远超仅使用真实数据的情况，这表明我们的方法成功模拟了用于鉴别的关键动态。我们的图像到视频的方法为推进超声波视频分析提供了有效的数据增强解决方案。代码可在<a target="_blank" rel="noopener" href="https://github.com/MedAITech/U_I2V">https://github.com/MedAITech/U_I2V</a>获取。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14966v1">PDF</a> MICCAI 2024</p>
<p><strong>Summary</strong></p>
<p>本摘要提出通过利用静态图像生成动态序列来解决超声视频数据集短缺的问题。研究团队引入了一种潜在动态扩散模型（LDDM），该模型能够将现有的大量超声图像转化为逼真的视频特性序列。在BUSV基准测试上，合成的视频既具有强大的定量结果又具有良好的视觉效果。此外，在真实视频与合成的视频结合训练分类模型的情况下，性能得到了显著提升，证明合成视频对于判别能力具有关键作用。该研究提供了一种有效的数据增强解决方案，推动了超声视频分析的发展。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>超声视频分类对于自动化诊断至关重要，但可用的超声视频数据集有限。</li>
<li>提出通过潜在动态扩散模型（LDDM）将静态超声图像转化为动态视频序列的方法。</li>
<li>LDDM合成的视频在BUSV基准测试中表现出强大的定量结果和良好的视觉效果。</li>
<li>结合真实和合成视频训练分类模型能显著提高性能。</li>
<li>合成视频具有关键的判别能力，可模拟重要的动态特性。</li>
<li>该方法提供了一种有效的数据增强解决方案，有助于推动超声视频分析的发展。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14966">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8e49b6aef94727d2e5c3afa2b31df8fa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c896922487736d835240b97ed3bc02fa.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a1cc7baf7a6c1fe7bd6d904284b499aa.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Efficient-Personalization-of-Quantized-Diffusion-Model-without-Backpropagation"><a href="#Efficient-Personalization-of-Quantized-Diffusion-Model-without-Backpropagation" class="headerlink" title="Efficient Personalization of Quantized Diffusion Model without   Backpropagation"></a>Efficient Personalization of Quantized Diffusion Model without   Backpropagation</h2><p><strong>Authors:Hoigi Seo, Wongi Jeong, Kyungryeol Lee, Se Young Chun</strong></p>
<p>Diffusion models have shown remarkable performance in image synthesis, but they demand extensive computational and memory resources for training, fine-tuning and inference. Although advanced quantization techniques have successfully minimized memory usage for inference, training and fine-tuning these quantized models still require large memory possibly due to dequantization for accurate computation of gradients and&#x2F;or backpropagation for gradient-based algorithms. However, memory-efficient fine-tuning is particularly desirable for applications such as personalization that often must be run on edge devices like mobile phones with private data. In this work, we address this challenge by quantizing a diffusion model with personalization via Textual Inversion and by leveraging a zeroth-order optimization on personalization tokens without dequantization so that it does not require gradient and activation storage for backpropagation that consumes considerable memory. Since a gradient estimation using zeroth-order optimization is quite noisy for a single or a few images in personalization, we propose to denoise the estimated gradient by projecting it onto a subspace that is constructed with the past history of the tokens, dubbed Subspace Gradient. In addition, we investigated the influence of text embedding in image generation, leading to our proposed time steps sampling, dubbed Partial Uniform Timestep Sampling for sampling with effective diffusion timesteps. Our method achieves comparable performance to prior methods in image and text alignment scores for personalizing Stable Diffusion with only forward passes while reducing training memory demand up to $8.2\times$. </p>
<blockquote>
<p>扩散模型在图像合成中表现出卓越的性能，但它们对于训练、微调以及推理需要大量的计算和内存资源。虽然先进的量化技术已成功最小化了推理过程中的内存使用，但对这些量化模型进行训练和微调仍然需要大内存，这可能是由于为了准确计算梯度和基于梯度的算法进行反向传播而需要进行反量化。然而，对于必须在边缘设备（如带有私人数据的移动电话）上运行的应用程序而言，对内存效率高的微调有特别需求。在这项工作中，我们通过使用文本反转对扩散模型进行量化，并利用零阶优化对个性化令牌进行优化，解决了这一挑战，无需反量化即可在不要求梯度和反向传播激活存储（这会消耗大量内存）的情况下进行。由于使用零阶优化对梯度估计在个性化中的单个或少数图像是相当嘈杂的，我们提出通过投影到由过去令牌历史构建的子空间来对估计的梯度进行去噪，被称为子空间梯度。此外，我们研究了文本嵌入在图像生成中的影响，导致我们提出时间步长采样方法，被称为部分均匀时间步长采样，用于有效的扩散时间步长采样。我们的方法在图像和文本对齐得分方面实现了与先前方法相当的性能，可以在个性化Stable Diffusion的同时仅通过前向传递减少训练内存需求高达8.2倍。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14868v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>扩散模型在图像合成方面表现出卓越的性能，但其在训练、微调及推理过程中需要大量的计算和内存资源。尽管先进的量化技术已成功减少了推理过程中的内存使用，但量化模型的训练和微调仍然需要较大的内存，这可能是由于为了准确计算梯度和进行反向传播而需要进行反量化所致。针对个性化应用等需要在边缘设备（如手机）上运行并使用私有数据的应用程序，内存高效的微调尤为关键。本研究通过结合文本反转对扩散模型进行量化，并利用零阶优化对个人化令牌进行优化，无需反量化即可满足这一需求，从而避免了梯度计算和反向传播过程中大量内存的消耗。针对个性化中单一或少量图像的梯度估计噪声较大，本研究提出了基于过去令牌历史的子空间梯度去噪方法。此外，本研究还探讨了文本嵌入在图像生成中的影响，并据此提出了部分均匀时间步长采样方法。该方法在个性化Stable Diffusion的图像和文本对齐得分方面取得了与以往方法相当的效果，同时仅通过前向传递就降低了高达8.2倍的训练内存需求。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型在图像合成方面表现出强大的性能，但需要大量的计算和内存资源。</li>
<li>先进量化技术虽能减少推理过程中的内存使用，但训练和微调时仍需大量内存。</li>
<li>研究提出了一种结合文本反转和零阶优化的方法，进行扩散模型的量化及个性化，降低内存消耗。</li>
<li>针对个性化应用中梯度估计的噪声问题，提出了子空间梯度去噪方法。</li>
<li>研究发现文本嵌入在图像生成中具有重要影响，并据此提出了部分均匀时间步长采样方法。</li>
<li>方法在图像和文本对齐得分方面与以往方法相当，同时显著降低了训练内存需求。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14868">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-4f4c9903ad7038d53b8e91e4a140a4d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb9191eb59727da71f7bd967b3b21515.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae03172599c25ec2bba305b20f8bcf15.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cbce0cbede4c8d6e3f49bc87c0acd7c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-91f21ced883ed4956a14b173b548e09a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a08ba91527fc01535a1086e2cd5a2175.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-32d03845a2716518246e87546c2fe2a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-60087318efb90dd2aaed282e1bf759ef.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="A-Simple-Combination-of-Diffusion-Models-for-Better-Quality-Trade-Offs-in-Image-Denoising"><a href="#A-Simple-Combination-of-Diffusion-Models-for-Better-Quality-Trade-Offs-in-Image-Denoising" class="headerlink" title="A Simple Combination of Diffusion Models for Better Quality Trade-Offs   in Image Denoising"></a>A Simple Combination of Diffusion Models for Better Quality Trade-Offs   in Image Denoising</h2><p><strong>Authors:Jonas Dornbusch, Emanuel Pfarr, Florin-Alexandru Vasluianu, Frank Werner, Radu Timofte</strong></p>
<p>Diffusion models have garnered considerable interest in computer vision, owing both to their capacity to synthesize photorealistic images and to their proven effectiveness in image reconstruction tasks. However, existing approaches fail to efficiently balance the high visual quality of diffusion models with the low distortion achieved by previous image reconstruction methods. Specifically, for the fundamental task of additive Gaussian noise removal, we first illustrate an intuitive method for leveraging pretrained diffusion models. Further, we introduce our proposed Linear Combination Diffusion Denoiser (LCDD), which unifies two complementary inference procedures - one that leverages the model’s generative potential and another that ensures faithful signal recovery. By exploiting the inherent structure of the denoising samples, LCDD achieves state-of-the-art performance and offers controlled, well-behaved trade-offs through a simple scalar hyperparameter adjustment. </p>
<blockquote>
<p>扩散模型在计算机视觉领域引起了极大的关注，这既是因为它们能够合成逼真的图像，也是因为它们在图像重建任务中的有效性得到了验证。然而，现有的方法未能有效地平衡扩散模型的高视觉质量与先前图像重建方法实现的低失真。具体来说，对于基本的添加高斯噪声去除任务，我们首先提出了一种利用预训练扩散模型的直观方法。此外，我们引入了所提出的线性组合扩散去噪器（LCDD），它统一了两种互补的推理程序——一种利用模型的生成潜力，另一种确保忠实信号恢复。通过利用去噪样本的固有结构，LCDD实现了最先进的性能，并通过简单的标量超参数调整提供了可控且表现良好的权衡。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14654v1">PDF</a> 10 pages, 7 figures, 2 tables</p>
<p><strong>Summary</strong></p>
<p>扩散模型在计算机视觉领域受到广泛关注，既能够合成逼真的图像，也在图像重建任务中展现出效果。然而，现有方法难以在扩散模型的高视觉质量与先前图像重建方法实现的低失真之间取得有效平衡。针对加性高斯噪声去除这一基本任务，本文首先提出了一种利用预训练扩散模型的直观方法。此外，还介绍了提出的线性组合扩散去噪器（LCDD），它统一了两种互补的推理程序——一种利用模型的生成潜力，另一种确保信号的真实恢复。LCDD通过利用去噪样本的固有结构，实现了卓越的性能，并通过简单的标量超参数调整，提供了可控的优化权衡。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型在计算机视觉领域备受关注，尤其在图像合成和图像重建任务中表现优异。</li>
<li>现有方法难以平衡扩散模型的高视觉质量与低失真。</li>
<li>针对加性高斯噪声去除任务，提出了一种利用预训练扩散模型的直观方法。</li>
<li>引入了线性组合扩散去噪器（LCDD），融合了两种互补的推理程序。</li>
<li>LCDD利用模型的生成潜力，同时确保信号的真实恢复。</li>
<li>LCDD通过利用去噪样本的固有结构实现了卓越性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14654">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-4c2ea409c2b04e690604d25421327e3b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7188d13e2cc7b21da6e64e6d7bb71719.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93b44faf967acf903f4b924bd1f27a4d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b5143422e6155faefdf1ac8d888f8ee4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3b5d4c8620f17420552564c5994a2a2d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-280f0608a09b9cd1db2be5b3efc5ff40.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="CTSR-Controllable-Fidelity-Realness-Trade-off-Distillation-for-Real-World-Image-Super-Resolution"><a href="#CTSR-Controllable-Fidelity-Realness-Trade-off-Distillation-for-Real-World-Image-Super-Resolution" class="headerlink" title="CTSR: Controllable Fidelity-Realness Trade-off Distillation for   Real-World Image Super Resolution"></a>CTSR: Controllable Fidelity-Realness Trade-off Distillation for   Real-World Image Super Resolution</h2><p><strong>Authors:Runyi Li, Bin Chen, Jian Zhang, Radu Timofte</strong></p>
<p>Real-world image super-resolution is a critical image processing task, where two key evaluation criteria are the fidelity to the original image and the visual realness of the generated results. Although existing methods based on diffusion models excel in visual realness by leveraging strong priors, they often struggle to achieve an effective balance between fidelity and realness. In our preliminary experiments, we observe that a linear combination of multiple models outperforms individual models, motivating us to harness the strengths of different models for a more effective trade-off. Based on this insight, we propose a distillation-based approach that leverages the geometric decomposition of both fidelity and realness, alongside the performance advantages of multiple teacher models, to strike a more balanced trade-off. Furthermore, we explore the controllability of this trade-off, enabling a flexible and adjustable super-resolution process, which we call CTSR (Controllable Trade-off Super-Resolution). Experiments conducted on several real-world image super-resolution benchmarks demonstrate that our method surpasses existing state-of-the-art approaches, achieving superior performance across both fidelity and realness metrics. </p>
<blockquote>
<p>现实世界图像超分辨率处理是一项关键的图像处理任务，其两个主要的评估标准是对于原始图像的保真度和生成结果的视觉逼真度。尽管现有的基于扩散模型的方法通过利用强大的先验知识在视觉逼真度方面表现出色，但它们往往难以在保真度和逼真度之间取得有效平衡。在我们的初步实验中，我们观察到多种模型的线性组合表现优于单个模型，这激励我们利用不同模型的优势以实现更有效的权衡。基于此见解，我们提出了一种基于蒸馏的方法，该方法利用保真度和逼真度的几何分解，以及多个教师模型的性能优势，以实现更平衡的权衡。此外，我们探索了这种权衡的可控性，以实现一个灵活可调的超级分辨率过程，我们称之为可控权衡超级分辨率（CTSR）。在几个现实世界图像超分辨率处理基准测试上的实验表明，我们的方法超越了现有的最新方法，在保真度和逼真度指标上都实现了卓越的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14272v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于扩散模型的现实图像超分辨率处理是一项关键任务，面临保真度和视觉真实度两个核心评价指标的挑战。现有方法虽在视觉真实度上表现优异，但在平衡保真度和真实度方面存在困难。本文提出一种基于蒸馏的方法，通过几何分解实现保真度和真实度的平衡，并结合多个教师模型的性能优势，提出可控的权衡超分辨率处理（CTSR）。实验证明，该方法优于现有技术前沿，在保真度和真实度指标上均表现出卓越性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>现实图像超分辨率处理是图像处理的重点任务。</li>
<li>现有扩散模型方法在平衡图像保真度和视觉真实度方面存在挑战。</li>
<li>本文通过几何分解实现保真度和真实度的平衡。</li>
<li>提出一种基于蒸馏的方法，结合多个教师模型的性能优势。</li>
<li>提出了可控的权衡超分辨率处理（CTSR）。</li>
<li>实验证明，该方法在多个真实世界图像超分辨率基准测试中表现优异。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14272">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ccca9a821f250eea5dd83a0f5a69217e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-568f43e0303e91a36949122b03bea56e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-87253920683e3ec60bb543b8063d9079.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-908f143b20660e06d1f8dcdb0279b4cc.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="FlexWorld-Progressively-Expanding-3D-Scenes-for-Flexiable-View-Synthesis"><a href="#FlexWorld-Progressively-Expanding-3D-Scenes-for-Flexiable-View-Synthesis" class="headerlink" title="FlexWorld: Progressively Expanding 3D Scenes for Flexiable-View   Synthesis"></a>FlexWorld: Progressively Expanding 3D Scenes for Flexiable-View   Synthesis</h2><p><strong>Authors:Luxi Chen, Zihan Zhou, Min Zhao, Yikai Wang, Ge Zhang, Wenhao Huang, Hao Sun, Ji-Rong Wen, Chongxuan Li</strong></p>
<p>Generating flexible-view 3D scenes, including 360{\deg} rotation and zooming, from single images is challenging due to a lack of 3D data. To this end, we introduce FlexWorld, a novel framework consisting of two key components: (1) a strong video-to-video (V2V) diffusion model to generate high-quality novel view images from incomplete input rendered from a coarse scene, and (2) a progressive expansion process to construct a complete 3D scene. In particular, leveraging an advanced pre-trained video model and accurate depth-estimated training pairs, our V2V model can generate novel views under large camera pose variations. Building upon it, FlexWorld progressively generates new 3D content and integrates it into the global scene through geometry-aware scene fusion. Extensive experiments demonstrate the effectiveness of FlexWorld in generating high-quality novel view videos and flexible-view 3D scenes from single images, achieving superior visual quality under multiple popular metrics and datasets compared to existing state-of-the-art methods. Qualitatively, we highlight that FlexWorld can generate high-fidelity scenes with flexible views like 360{\deg} rotations and zooming. Project page: <a target="_blank" rel="noopener" href="https://ml-gsai.github.io/FlexWorld">https://ml-gsai.github.io/FlexWorld</a>. </p>
<blockquote>
<p>生成包含360°旋转和缩放功能的灵活视角3D场景，从单张图像开始是一项挑战，因为缺乏3D数据。为此，我们引入了FlexWorld，这是一个由两个关键组件构成的新型框架：（1）强大的视频到视频（V2V）扩散模型，用于从由粗糙场景渲染的不完整输入生成高质量的新视角图像；（2）渐进式扩展过程，用于构建完整的3D场景。特别是，借助先进的预训练视频模型和精确的深度估计训练对，我们的V2V模型可以在大相机姿态变化下生成新视角。在此基础上，FlexWorld逐步生成新的3D内容，并通过几何感知场景融合将其集成到全局场景中。大量实验表明，FlexWorld在生成高质量的新视角视频和灵活视角的3D场景方面非常有效，从单张图像开始，在多个流行指标和数据集上与现有最先进的方法相比，实现了优越的视觉质量。从定性角度看，我们强调FlexWorld可以生成具有灵活视角的高保真场景，如360°旋转和缩放。项目页面：<a target="_blank" rel="noopener" href="https://ml-gsai.github.io/FlexWorld%E3%80%82">https://ml-gsai.github.io/FlexWorld。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13265v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于单图生成具有360°旋转和缩放功能的灵活视图3D场景是一项挑战，因为缺乏3D数据。为此，我们引入了FlexWorld框架，该框架包含两个关键组件：（1）强大的视频到视频（V2V）扩散模型，用于从由粗糙场景渲染的不完整输入生成高质量的新视图图像；（2）渐进扩展过程，用于构建完整的3D场景。我们的V2V模型利用先进的预训练视频模型和精确的深度估计训练对，可以在大相机姿态变化下生成新视图。FlexWorld在此基础上逐步生成新的3D内容，并通过几何感知场景融合将其集成到全局场景中。实验证明，FlexWorld在生成高质量新视图视频和灵活视图3D场景方面效果显著，在多个流行指标和数据集上与现有最先进的方法相比具有优势。FlexWorld能够生成具有灵活视图的高保真场景，如360°旋转和缩放。更多信息请访问项目页面。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FlexWorld是一个用于从单图像生成灵活视图3D场景的新框架。</li>
<li>框架包含两个关键组件：视频到视频（V2V）扩散模型和渐进扩展过程。</li>
<li>V2V扩散模型基于预训练的视频模型和深度估计训练对，可生成高质量的新视图图像。</li>
<li>渐进扩展过程用于构建完整的3D场景，通过几何感知场景融合集成新生成的3D内容。</li>
<li>FlexWorld能够在多种数据集上实现高质量的灵活视图生成，包括360°旋转和缩放。</li>
<li>FlexWorld在多个评估指标上优于现有的最先进方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13265">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-dd4764ca7c7736aae75729cf12488f65.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5d17af8e7c95e02f57f511c5eab56f44.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f0b9de323eebe60f047419695d601d3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fceedf4e8d2678f9bae0e00404790899.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9ba1da5f214a038cdd2e2942c7032834.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Low-Biased-General-Annotated-Dataset-Generation"><a href="#Low-Biased-General-Annotated-Dataset-Generation" class="headerlink" title="Low-Biased General Annotated Dataset Generation"></a>Low-Biased General Annotated Dataset Generation</h2><p><strong>Authors:Dengyang Jiang, Haoyu Wang, Lei Zhang, Wei Wei, Guang Dai, Mengmeng Wang, Jingdong Wang, Yanning Zhang</strong></p>
<p>Pre-training backbone networks on a general annotated dataset (e.g., ImageNet) that comprises numerous manually collected images with category annotations has proven to be indispensable for enhancing the generalization capacity of downstream visual tasks. However, those manually collected images often exhibit bias, which is non-transferable across either categories or domains, thus causing the model’s generalization capacity degeneration. To mitigate this problem, we present a low-biased general annotated dataset generation framework (lbGen). Instead of expensive manual collection, we aim at directly generating low-biased images with category annotations. To achieve this goal, we propose to leverage the advantage of a multimodal foundation model (e.g., CLIP), in terms of aligning images in a low-biased semantic space defined by language. Specifically, we develop a bi-level semantic alignment loss, which not only forces all generated images to be consistent with the semantic distribution of all categories belonging to the target dataset in an adversarial learning manner, but also requires each generated image to match the semantic description of its category name. In addition, we further cast an existing image quality scoring model into a quality assurance loss to preserve the quality of the generated image. By leveraging these two loss functions, we can obtain a low-biased image generation model by simply fine-tuning a pre-trained diffusion model using only all category names in the target dataset as input. Experimental results confirm that, compared with the manually labeled dataset or other synthetic datasets, the utilization of our generated low-biased dataset leads to stable generalization capacity enhancement of different backbone networks across various tasks, especially in tasks where the manually labeled samples are scarce. </p>
<blockquote>
<p>在包含大量手动收集并带有类别注释的图像的一般注释数据集上，对骨干网络进行预训练，对于提高下游视觉任务的泛化能力来说是不可或缺的。然而，这些手动收集的图像往往存在偏见，这些偏见在类别或域之间是不可转移的，从而导致模型的泛化能力下降。为了缓解这个问题，我们提出了一个低偏置的一般注释数据集生成框架（lbGen）。我们旨在通过直接生成带有类别注释的低偏置图像，而非昂贵的手动收集。为了实现这一目标，我们提议利用多模态基础模型的优点（例如CLIP），以语言定义的低偏置语义空间来对齐图像。具体来说，我们开发了一种两级语义对齐损失，这不仅以对抗性学习的方式强制所有生成的图像与目标数据集中所有类别的语义分布保持一致，还要求每个生成的图像与其类别名称的语义描述相匹配。此外，我们将现有的图像质量评分模型转化为质量保证损失，以保持生成图像的质量。通过利用这两种损失函数，我们只需使用目标数据集中的所有类别名称对预训练的扩散模型进行微调，即可获得低偏置的图像生成模型。实验结果表明，与使用手动标记的数据集或其他合成数据集相比，使用我们生成的低偏置数据集可以提高不同骨干网络在不同任务上的泛化能力，特别是在手动标记样本稀缺的任务中。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.10831v3">PDF</a> CVPR2025 Accepted Paper</p>
<p><strong>Summary</strong></p>
<p>基于预训练模型在通用标注数据集上的训练，如ImageNet，对于下游视觉任务的泛化能力提升具有重要意义。然而，手动收集的图像常常存在偏见，影响模型的泛化能力。为解决这一问题，我们提出了一个低偏见的通用标注数据集生成框架（lbGen）。它旨在通过直接生成低偏见的图像及其类别标注来替代昂贵的手动收集过程。我们利用多模态基础模型（如CLIP）的优势，在一个由语言定义的低偏见语义空间中对齐图像。具体来说，我们开发了一种两级语义对齐损失，不仅以对抗性学习的方式迫使所有生成的图像与目标数据集中所有类别的语义分布保持一致，还要求每个生成的图像与其类别名称的语义描述相匹配。此外，我们还利用现有的图像质量评分模型构建了一个质量保障损失，以保持生成图像的质量。通过利用这两种损失函数，我们只需通过微调预训练的扩散模型，以目标数据集的所有类别名称作为输入，即可获得低偏见的图像生成模型。实验结果表明，与使用手动标注数据集或其他合成数据集相比，利用我们生成的低偏见数据集可以稳定提升不同骨干网络在各种任务上的泛化能力，特别是在手动标注样本稀缺的任务中表现尤为突出。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>预训练模型在通用标注数据集上的训练对于下游视觉任务的泛化能力提升至关重要。</li>
<li>手动收集的图像存在偏见，影响模型的泛化能力。</li>
<li>提出了一个低偏见的通用标注数据集生成框架（lbGen）来生成低偏见的图像及其类别标注。</li>
<li>利用多模态基础模型（如CLIP）在一个低偏见语义空间中对齐图像。</li>
<li>开发了一种两级语义对齐损失，确保生成的图像与类别语义分布一致。</li>
<li>利用图像质量评分模型构建质量保障损失，保持生成图像的质量。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.10831">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-33ab8b01b5fc33803ab613a659146196.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e16fcaf8b93f38fee986aa074dcd9355.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7fd9e366ddeb04638181b6cf907e7554.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-32a5886bea118e3a9dbda0a77fee15ec.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="VideoDirector-Precise-Video-Editing-via-Text-to-Video-Models"><a href="#VideoDirector-Precise-Video-Editing-via-Text-to-Video-Models" class="headerlink" title="VideoDirector: Precise Video Editing via Text-to-Video Models"></a>VideoDirector: Precise Video Editing via Text-to-Video Models</h2><p><strong>Authors:Yukun Wang, Longguang Wang, Zhiyuan Ma, Qibin Hu, Kai Xu, Yulan Guo</strong></p>
<p>Despite the typical inversion-then-editing paradigm using text-to-image (T2I) models has demonstrated promising results, directly extending it to text-to-video (T2V) models still suffers severe artifacts such as color flickering and content distortion. Consequently, current video editing methods primarily rely on T2I models, which inherently lack temporal-coherence generative ability, often resulting in inferior editing results. In this paper, we attribute the failure of the typical editing paradigm to: 1) Tightly Spatial-temporal Coupling. The vanilla pivotal-based inversion strategy struggles to disentangle spatial-temporal information in the video diffusion model; 2) Complicated Spatial-temporal Layout. The vanilla cross-attention control is deficient in preserving the unedited content. To address these limitations, we propose a spatial-temporal decoupled guidance (STDG) and multi-frame null-text optimization strategy to provide pivotal temporal cues for more precise pivotal inversion. Furthermore, we introduce a self-attention control strategy to maintain higher fidelity for precise partial content editing. Experimental results demonstrate that our method (termed VideoDirector) effectively harnesses the powerful temporal generation capabilities of T2V models, producing edited videos with state-of-the-art performance in accuracy, motion smoothness, realism, and fidelity to unedited content. </p>
<blockquote>
<p>尽管使用文本到图像（T2I）模型的典型反转编辑范式已经取得了有前景的结果，但直接将其扩展到文本到视频（T2V）模型仍然会出现严重的伪影问题，如色彩闪烁和内容失真。因此，目前的视频编辑方法主要依赖于T2I模型，这些模型本质上缺乏时间连贯性生成能力，往往导致编辑结果较差。在本文中，我们将典型的编辑范式的失败归结为以下两点：1）紧密的空间时间耦合。基于原始关键点的反转策略在视频扩散模型中很难分离空间时间信息；2）复杂的空间时间布局。原始的交叉注意力控制在于保持未编辑的内容。为了解决这些局限性，我们提出了时空解耦指导（STDG）和多帧无文本优化策略，为更精确的关键点反转提供关键的时间线索。此外，我们还引入了一种自注意力控制策略，以在精确部分内容编辑中保持更高的保真度。实验结果表明，我们的方法（称为VideoDirector）有效地利用了T2V模型的强大时间生成能力，在准确性、运动平滑度、逼真度和对未编辑内容的保真度方面达到了最先进的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.17592v3">PDF</a> 15 figures</p>
<p><strong>Summary</strong><br>     针对文本转视频（T2V）模型中的典型编辑范式存在的问题，如颜色闪烁和内容失真等严重伪影，本文提出了一种新的方法。通过解决空间时间耦合复杂和空间时间布局的问题，引入时空分离指导（STDG）和多帧无文本优化策略，为精确的关键点反转提供关键的时序线索。同时采用自我关注控制策略，保持对精确部分内容编辑的高保真度。实验结果表明，该方法（称为VideoDirector）有效利用了T2V模型的强大时序生成能力，在准确性、运动平滑度、真实性和对未编辑内容的保真度方面达到了业界领先水平。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文本转视频（T2V）模型中的典型编辑范式存在颜色闪烁和内容失真等问题。</li>
<li>这些问题主要源于紧密的空间时间耦合和复杂的空间时间布局。</li>
<li>引入的时空分离指导（STDG）有助于解决空间时间耦合问题。</li>
<li>多帧无文本优化策略为精确的关键点反转提供了关键的时序线索。</li>
<li>自我关注控制策略有助于保持对精确部分内容编辑的高保真度。</li>
<li>VideoDirector方法有效利用了T2V模型的强大时序生成能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.17592">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-14bf48a277b8d0bb2613a41c54153e43.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f9c10b9f2e40b493a4d4b51708ed1111.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-20db587638bd4b9efafd0e945a5435e6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-96f532fc8eb4b1834bc83294e1a20507.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-762573e30cb534041dcb372a9e017808.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a3ddc851d3027763743214974263bda2.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Pathways-on-the-Image-Manifold-Image-Editing-via-Video-Generation"><a href="#Pathways-on-the-Image-Manifold-Image-Editing-via-Video-Generation" class="headerlink" title="Pathways on the Image Manifold: Image Editing via Video Generation"></a>Pathways on the Image Manifold: Image Editing via Video Generation</h2><p><strong>Authors:Noam Rotstein, Gal Yona, Daniel Silver, Roy Velich, David Bensaïd, Ron Kimmel</strong></p>
<p>Recent advances in image editing, driven by image diffusion models, have shown remarkable progress. However, significant challenges remain, as these models often struggle to follow complex edit instructions accurately and frequently compromise fidelity by altering key elements of the original image. Simultaneously, video generation has made remarkable strides, with models that effectively function as consistent and continuous world simulators. In this paper, we propose merging these two fields by utilizing image-to-video models for image editing. We reformulate image editing as a temporal process, using pretrained video models to create smooth transitions from the original image to the desired edit. This approach traverses the image manifold continuously, ensuring consistent edits while preserving the original image’s key aspects. Our approach achieves state-of-the-art results on text-based image editing, demonstrating significant improvements in both edit accuracy and image preservation. Visit our project page: <a target="_blank" rel="noopener" href="https://rotsteinnoam.github.io/Frame2Frame">https://rotsteinnoam.github.io/Frame2Frame</a>. </p>
<blockquote>
<p>图像编辑领域的最新进展，得益于图像扩散模型的推动，已经取得了显著的进步。然而，仍然存在重大挑战，因为这些模型往往难以准确遵循复杂的编辑指令，并且经常改变原始图像的关键元素，从而影响保真度。同时，视频生成方面已经取得了显著的进步，模型有效地充当了连贯和连续的世界模拟器。在本文中，我们提出通过利用图像到视频模型进行图像编辑来融合这两个领域。我们将图像编辑重新制定为一个时间过程，使用预训练的视频模型来创建从原始图像到所需编辑的平滑过渡。这种方法在图像流形上连续遍历，确保编辑的一致性，同时保留原始图像的关键方面。我们的方法在基于文本的图像编辑方面达到了最新水平，在编辑准确性和图像保留方面都取得了显著改进。欢迎访问我们的项目页面：<a target="_blank" rel="noopener" href="https://rotsteinnoam.github.io/Frame2Frame">Frame2Frame</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.16819v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>图像扩散模型在图像编辑方面的最新进展已经显示出显著的成果，但仍存在挑战，如难以准确遵循复杂的编辑指令，以及在修改过程中频繁改变原始图像的关键元素。本文提出将图像编辑和视频生成两个领域相结合，利用图像到视频的模型进行图像编辑。我们重新将图像编辑表述为一个时间过程，并使用预训练的视频模型来创建从原始图像到所需编辑的平滑过渡。此方法在文本驱动的图像编辑方面取得了最新成果，在编辑准确性和图像保留方面都显示出重大改进。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>图像扩散模型在图像编辑领域取得显著进展，但仍面临准确遵循复杂编辑指令和保持原始图像关键元素的挑战。</li>
<li>提出将图像编辑与视频生成相结合的新思路。</li>
<li>利用预训练的视频模型，将图像编辑重新表述为一个时间过程。</li>
<li>通过创建从原始图像到所需编辑的平滑过渡，实现了图像编辑的新方法。</li>
<li>该方法在实现文本驱动的图像编辑方面达到最新成果。</li>
<li>在编辑准确性和图像保留方面都有显著改进。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.16819">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2a0613d27fdcbd73dfbc500d8cd885f7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-af85b05da4766f4665f7bb10765f0765.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bc043f4cab323197ccf1cade71a10936.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0612d2fad51192fc2eb43f6776fc60fc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-23fbcdc43e3b62f8e50a91b8e48f4d12.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Leveraging-BEV-Paradigm-for-Ground-to-Aerial-Image-Synthesis"><a href="#Leveraging-BEV-Paradigm-for-Ground-to-Aerial-Image-Synthesis" class="headerlink" title="Leveraging BEV Paradigm for Ground-to-Aerial Image Synthesis"></a>Leveraging BEV Paradigm for Ground-to-Aerial Image Synthesis</h2><p><strong>Authors:Junyan Ye, Jun He, Weijia Li, Zhutao Lv, Yi Lin, Jinhua Yu, Haote Yang, Conghui He</strong></p>
<p>Ground-to-aerial image synthesis focuses on generating realistic aerial images from corresponding ground street view images while maintaining consistent content layout, simulating a top-down view. The significant viewpoint difference leads to domain gaps between views, and dense urban scenes limit the visible range of street views, making this cross-view generation task particularly challenging. In this paper, we introduce SkyDiffusion, a novel cross-view generation method for synthesizing aerial images from street view images, utilizing a diffusion model and the Bird’s-Eye View (BEV) paradigm. The Curved-BEV method in SkyDiffusion converts street-view images into a BEV perspective, effectively bridging the domain gap, and employs a “multi-to-one” mapping strategy to address occlusion issues in dense urban scenes. Next, SkyDiffusion designed a BEV-guided diffusion model to generate content-consistent and realistic aerial images. Additionally, we introduce a novel dataset, Ground2Aerial-3, designed for diverse ground-to-aerial image synthesis applications, including disaster scene aerial synthesis, low-altitude UAV image synthesis, and historical high-resolution satellite image synthesis tasks. Experimental results demonstrate that SkyDiffusion outperforms state-of-the-art methods on cross-view datasets across natural (CVUSA), suburban (CVACT), urban (VIGOR-Chicago), and various application scenarios (G2A-3), achieving realistic and content-consistent aerial image generation. The code, datasets and more information of this work can be found at <a target="_blank" rel="noopener" href="https://opendatalab.github.io/skydiffusion/">https://opendatalab.github.io/skydiffusion/</a> . </p>
<blockquote>
<p>地面到空中的图像合成专注于从相应的地面街景图像生成逼真的空中图像，同时保持内容布局的一致性，模拟从上到下的视角。显著的视点差异导致了不同视角之间的领域差距，而密集的城区场景限制了街景的可见范围，这使得跨视图生成任务特别具有挑战性。在本文中，我们介绍了SkyDiffusion，这是一种新的跨视图生成方法，用于从街景图像合成空中图像，它利用扩散模型和鸟瞰图（BEV）范式。SkyDiffusion中的Curved-BEV方法将街景图像转换为BEV视角，有效地弥合了领域差距，并采用了“多到一”的映射策略来解决密集城区场景中的遮挡问题。接下来，SkyDiffusion设计了一个受BEV引导的扩散模型来生成内容一致且逼真的空中图像。此外，我们还介绍了一个新的数据集Ground2Aerial-3，该数据集专为多样化的地面到空中图像合成应用而设计，包括灾害场景空中合成、低空无人机图像合成以及历史高分辨率卫星图像合成任务。实验结果表明，SkyDiffusion在跨视图数据集（CVUSA自然场景、CVACT郊区、VIGOR-Chicago城区以及G2A-3各种应用场景）上的表现优于最新方法，实现了逼真和内容一致性的空中图像生成。有关这项工作的代码、数据集和更多信息，请访问<a target="_blank" rel="noopener" href="https://opendatalab.github.io/skydiffusion/%E3%80%82">https://opendatalab.github.io/skydiffusion/。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.01812v4">PDF</a> 10 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>基于地面到空中的图像合成技术，研究团队引入了SkyDiffusion模型。此模型通过扩散模型结合鸟瞰图（BEV）范式，实现了从街道视角图像生成空中图像的任务。SkyDiffusion采用Curved-BEV方法转换街道视角图像到BEV视角，解决了视角差异带来的领域差距问题，并采用“多对一”映射策略解决密集城市场景中的遮挡问题。同时，该模型展示了强大的生成性能，能生成一致且逼真的空中图像。此外，研究团队还引入了专为地面到空中图像合成应用设计的Ground2Aerial-3数据集。实验结果显示，SkyDiffusion在自然、郊区、城市等不同场景以及多种应用情境下的跨视角数据集上均表现出超越现有方法的效果。详细信息请访问其官网获取。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SkyDiffusion是一个实现从街道视角图像生成空中图像的跨视角生成方法。</li>
<li>利用扩散模型和鸟瞰图（BEV）范式进行图像合成。</li>
<li>Curved-BEV方法有效解决了视角差异带来的领域差距问题。</li>
<li>“多对一”映射策略用于处理密集城市场景中的遮挡问题。</li>
<li>引入Ground2Aerial-3数据集，适用于多种地面到空中图像合成应用。</li>
<li>实验结果显示SkyDiffusion在多种场景和应用情境上表现优异。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.01812">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-c254de20c6ac2391b00a3df58425e64b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0de97b051203f4e3c4ad002a56f19b7d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a195c69ae29391e688852407db881373.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c2442e406dc9c5e1e6a6cfd8f3680465.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36a2ba407f1991907046a66a7020eed2.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Paint-by-Inpaint-Learning-to-Add-Image-Objects-by-Removing-Them-First"><a href="#Paint-by-Inpaint-Learning-to-Add-Image-Objects-by-Removing-Them-First" class="headerlink" title="Paint by Inpaint: Learning to Add Image Objects by Removing Them First"></a>Paint by Inpaint: Learning to Add Image Objects by Removing Them First</h2><p><strong>Authors:Navve Wasserman, Noam Rotstein, Roy Ganz, Ron Kimmel</strong></p>
<p>Image editing has advanced significantly with the introduction of text-conditioned diffusion models. Despite this progress, seamlessly adding objects to images based on textual instructions without requiring user-provided input masks remains a challenge. We address this by leveraging the insight that removing objects (Inpaint) is significantly simpler than its inverse process of adding them (Paint), attributed to inpainting models that benefit from segmentation mask guidance. Capitalizing on this realization, by implementing an automated and extensive pipeline, we curate a filtered large-scale image dataset containing pairs of images and their corresponding object-removed versions. Using these pairs, we train a diffusion model to inverse the inpainting process, effectively adding objects into images. Unlike other editing datasets, ours features natural target images instead of synthetic ones while ensuring source-target consistency by construction. Additionally, we utilize a large Vision-Language Model to provide detailed descriptions of the removed objects and a Large Language Model to convert these descriptions into diverse, natural-language instructions. Our quantitative and qualitative results show that the trained model surpasses existing models in both object addition and general editing tasks. Visit our project page for the released dataset and trained models: <a target="_blank" rel="noopener" href="https://rotsteinnoam.github.io/Paint-by-Inpaint">https://rotsteinnoam.github.io/Paint-by-Inpaint</a>. </p>
<blockquote>
<p>图像编辑随着文本条件扩散模型的引入而取得了显著进展。尽管如此，根据文本指令无缝地向图像添加对象，而无需用户提供输入掩码，仍然是一个挑战。我们通过利用这样一个见解来解决这个问题：去除物体（补画）比反向添加物体的过程（绘画）要简单得多，这归功于受益于分割掩码指导的补画模型。利用这一认识，我们通过实现自动化和广泛的管道，整理了一个包含图像及其相应去对象版本的大型图像数据集。使用这些数据对，我们训练了一个扩散模型来反转补画过程，有效地向图像中添加对象。与其他编辑数据集不同的是，我们的数据集以自然目标图像为特征，同时通过构建确保源-目标的一致性。此外，我们利用大型视觉语言模型为移除的对象提供详细描述，并利用大型语言模型将这些描述转换为多样化、自然语言指令。我们的定量和定性结果表明，训练后的模型在对象添加和一般编辑任务上超过了现有模型。有关发布的数据集和训练模型，请访问我们的项目页面：<a target="_blank" rel="noopener" href="https://rotsteinnoam.github.io/Paint-by-Inpaint%E3%80%82">https://rotsteinnoam.github.io/Paint-by-Inpaint。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.18212v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种基于文本条件扩散模型的新型图像编辑技术。针对在图像中根据文本指令无缝添加对象这一挑战，研究团队提出了一种利用去遮挡（Inpaint）技术来简化对象添加的新方法。研究团队创建了一个大型图像数据集，包含图像及其对应的去遮挡版本，并利用这些数据训练扩散模型，逆转去遮挡过程实现对象添加。相较于其他编辑数据集，该研究的特点在于采用自然目标图像并确保源目标一致性。此外，研究团队还使用了大型视觉语言模型和大型语言模型来提供详细的对象描述和指令转换。训练出的模型在物体添加和一般编辑任务上均超越了现有模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文本条件扩散模型在图像编辑领域取得显著进展。</li>
<li>在图像中根据文本指令无缝添加对象仍然是一个挑战。</li>
<li>研究团队利用去遮挡技术简化对象添加过程。</li>
<li>创建了一个大型图像数据集，包含图像及其对应的去遮挡版本。</li>
<li>利用这些数据集训练扩散模型实现对象添加。</li>
<li>该研究采用自然目标图像，确保源目标一致性。</li>
<li>研究团队使用大型视觉语言模型和大型语言模型提供详细的对象描述和指令转换。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.18212">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-5953998cb5bc062ff14d91e45d03e4e8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-512de7ea95b15c5219a8b179a51422bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-523779e544c79d00d1b090b64c82f6ce.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ce78a46abbcfda33a093a80484ec2708.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-00c373105a235fe44c7a219664c68dd6.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Learning-from-Mistakes-Iterative-Prompt-Relabeling-for-Text-to-Image-Diffusion-Model-Training"><a href="#Learning-from-Mistakes-Iterative-Prompt-Relabeling-for-Text-to-Image-Diffusion-Model-Training" class="headerlink" title="Learning from Mistakes: Iterative Prompt Relabeling for Text-to-Image   Diffusion Model Training"></a>Learning from Mistakes: Iterative Prompt Relabeling for Text-to-Image   Diffusion Model Training</h2><p><strong>Authors:Xinyan Chen, Jiaxin Ge, Tianjun Zhang, Jiaming Liu, Shanghang Zhang</strong></p>
<p>Diffusion models have shown impressive performance in many domains. However, the model’s capability to follow natural language instructions (e.g., spatial relationships between objects, generating complex scenes) is still unsatisfactory. In this work, we propose Iterative Prompt Relabeling (IPR), a novel algorithm that aligns images to text through iterative image sampling and prompt relabeling with feedback. IPR first samples a batch of images conditioned on the text, then relabels the text prompts of unmatched text-image pairs with classifier feedback. We conduct thorough experiments on SDv2 and SDXL, testing their capability to follow instructions on spatial relations. With IPR, we improved up to 15.22% (absolute improvement) on the challenging spatial relation VISOR benchmark, demonstrating superior performance compared to previous RL methods. Our code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/xinyan-cxy/IPR-RLDF">https://github.com/xinyan-cxy/IPR-RLDF</a>. </p>
<blockquote>
<p>扩散模型在很多领域都表现出了令人印象深刻的性能。然而，模型遵循自然语言指令的能力（例如，物体之间的空间关系、生成复杂场景等）仍然令人不满意。在这项工作中，我们提出了迭代提示重标记（IPR）算法，这是一种通过迭代图像采样和带有反馈的提示重标记来对齐图像和文本的新型算法。首先，我们根据文本对一批图像进行采样，然后利用分类器反馈对未匹配的文本图像对的文本提示进行重新标记。我们在SDv2和SDXL上进行了全面的实验，测试了它们在空间关系指令方面的能力。借助迭代提示重标记（IPR），我们在具有挑战性的空间关系VISOR基准测试中提高了高达15.22%（绝对提升），相较于之前的强化学习方法表现出卓越的性能。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/xinyan-cxy/IPR-RLDF%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/xinyan-cxy/IPR-RLDF公开访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.16204v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了迭代提示重标记（Iterative Prompt Relabeling，简称IPR）算法，该算法通过迭代图像采样和提示重标记的方式对齐图像和文本，提高扩散模型在自然语言指令遵循方面的性能。在SDv2和SDXL的实验中，通过IPR算法对空间关系指令的遵循能力有所提升，尤其在具有挑战性的VISOR基准测试中，相较于先前的强化学习方法，提升了高达15.22%（绝对提升）。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型在众多领域表现优异，但在遵循自然语言指令（如空间关系和场景生成）方面仍有不足。</li>
<li>提出了一种新的算法——迭代提示重标记（IPR），该算法通过迭代图像采样和提示重标记，实现对图像和文本的对齐。</li>
<li>IPR算法能有效提升扩散模型在空间关系指令遵循方面的性能。</li>
<li>在SDv2和SDXL的实验中验证了IPR算法的有效性。</li>
<li>在具有挑战性的VISOR基准测试中，相较于先前的强化学习方法，使用IPR算法的模型性能有显著提升，提升了高达15.22%（绝对提升）。</li>
<li>该研究的代码已公开可访问。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2312.16204">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-b2e2a1a50792ae10569d3db7b533d456.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3d57435fa3d2c6144bca20961be5aab7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-522fd13276f29c0f8c9bd0f5c1e3b27a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0dc5b2cbb1137d7d31b7bf20cac8eedd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bc54f988acd73ac22f8be0da7f003b8b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b61fa2382402e03e3bf1838fd609bea2.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-21/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-21/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-21/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-430b460135811c7ede99b5a295a3a404.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-03-21  FedSCA Federated Tuning with Similarity-guided Collaborative   Aggregation for Heterogeneous Medical Image Segmentation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-21
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-21/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-711b7c9275f5abdb371425b303b6ee25.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2025-03-21  GO-N3RDet Geometry Optimized NeRF-enhanced 3D Object Detector
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-21
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">24474.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
