<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-21  FP4DiT Towards Effective Floating Point Quantization for Diffusion   Transformers">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-762573e30cb534041dcb372a9e017808.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-21
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    77 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-21-æ›´æ–°"><a href="#2025-03-21-æ›´æ–°" class="headerlink" title="2025-03-21 æ›´æ–°"></a>2025-03-21 æ›´æ–°</h1><h2 id="FP4DiT-Towards-Effective-Floating-Point-Quantization-for-Diffusion-Transformers"><a href="#FP4DiT-Towards-Effective-Floating-Point-Quantization-for-Diffusion-Transformers" class="headerlink" title="FP4DiT: Towards Effective Floating Point Quantization for Diffusion   Transformers"></a>FP4DiT: Towards Effective Floating Point Quantization for Diffusion   Transformers</h2><p><strong>Authors:Ruichen Chen, Keith G. Mills, Di Niu</strong></p>
<p>Diffusion Models (DM) have revolutionized the text-to-image visual generation process. However, the large computational cost and model footprint of DMs hinders practical deployment, especially on edge devices. Post-training quantization (PTQ) is a lightweight method to alleviate these burdens without the need for training or fine-tuning. While recent DM PTQ methods achieve W4A8 on integer-based PTQ, two key limitations remain: First, while most existing DM PTQ methods evaluate on classical DMs like Stable Diffusion XL, 1.5 or earlier, which use convolutional U-Nets, newer Diffusion Transformer (DiT) models like the PixArt series, Hunyuan and others adopt fundamentally different transformer backbones to achieve superior image synthesis. Second, integer (INT) quantization is prevailing in DM PTQ but doesnâ€™t align well with the network weight and activation distribution, while Floating-Point Quantization (FPQ) is still under-investigated, yet it holds the potential to better align the weight and activation distributions in low-bit settings for DiT. In response, we introduce FP4DiT, a PTQ method that leverages FPQ to achieve W4A6 quantization. Specifically, we extend and generalize the Adaptive Rounding PTQ technique to adequately calibrate weight quantization for FPQ and demonstrate that DiT activations depend on input patch data, necessitating robust online activation quantization techniques. Experimental results demonstrate that FP4DiT outperforms integer-based PTQ at W4A6 and W4A8 precision and generates convincing visual content on PixArt-$\alpha$, PixArt-$\Sigma$ and Hunyuan in terms of several T2I metrics such as HPSv2 and CLIP. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹ï¼ˆDMï¼‰å·²ç»å½»åº•æ”¹å˜äº†æ–‡æœ¬åˆ°å›¾åƒçš„è§†è§‰ç”Ÿæˆè¿‡ç¨‹ã€‚ç„¶è€Œï¼Œæ‰©æ•£æ¨¡å‹çš„å¤§é‡è®¡ç®—æˆæœ¬å’Œæ¨¡å‹å ç”¨ç©ºé—´é˜»ç¢äº†å…¶å®è·µéƒ¨ç½²ï¼Œå°¤å…¶æ˜¯åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šã€‚åè®­ç»ƒé‡åŒ–ï¼ˆPTQï¼‰æ˜¯ä¸€ç§è½»é‡çº§æ–¹æ³•ï¼Œå¯ä»¥åœ¨æ— éœ€è®­ç»ƒå’Œå¾®è°ƒçš„æƒ…å†µä¸‹ç¼“è§£è¿™äº›è´Ÿæ‹…ã€‚å°½ç®¡æœ€è¿‘çš„æ‰©æ•£æ¨¡å‹PTQæ–¹æ³•å®ç°äº†åŸºäºæ•´æ•°çš„W4A8é‡åŒ–ï¼Œä½†ä»å­˜åœ¨ä¸¤ä¸ªä¸»è¦å±€é™æ€§ï¼šé¦–å…ˆï¼Œå°½ç®¡ç°æœ‰çš„å¤§å¤šæ•°æ‰©æ•£æ¨¡å‹PTQæ–¹æ³•éƒ½æ˜¯åœ¨è¯¸å¦‚Stable Diffusion XL 1.5æˆ–æ›´æ—©çš„ç»å…¸æ‰©æ•£æ¨¡å‹ä¸Šè¿›è¡Œè¯„ä¼°çš„ï¼Œè¿™äº›æ¨¡å‹ä½¿ç”¨å·ç§¯U-Netsï¼Œä½†æ›´æ–°çš„æ‰©æ•£è½¬æ¢å™¨ï¼ˆDiTï¼‰æ¨¡å‹ï¼Œå¦‚PixArtç³»åˆ—ã€Hunyuanå’Œå…¶ä»–æ¨¡å‹é‡‡ç”¨æ ¹æœ¬ä¸åŒçš„è½¬æ¢å™¨éª¨å¹²ç½‘æ¥å®ç°ä¼˜è¶Šçš„å›¾åƒåˆæˆã€‚å…¶æ¬¡ï¼Œæ•´æ•°ï¼ˆINTï¼‰é‡åŒ–åœ¨DM PTQä¸­å¾ˆæ™®éï¼Œä½†å¹¶ä¸èƒ½å¾ˆå¥½åœ°ä¸ç½‘ç»œæƒé‡å’Œæ¿€æ´»åˆ†å¸ƒç›¸åŒ¹é…ã€‚è€Œæµ®ç‚¹é‡åŒ–ï¼ˆFPQï¼‰çš„ç ”ç©¶ä»ç„¶ä¸è¶³ï¼Œä½†å®ƒå…·æœ‰åœ¨ä½ä½è®¾ç½®ä¸­æ›´å¥½åœ°å¯¹é½DiTæƒé‡å’Œæ¿€æ´»åˆ†å¸ƒçš„æ½œåŠ›ã€‚ä½œä¸ºå›åº”ï¼Œæˆ‘ä»¬ä»‹ç»äº†FP4DiTï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨FPQå®ç°W4A6é‡åŒ–çš„PTQæ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¯¹è‡ªé€‚åº”èˆå…¥PTQæŠ€æœ¯è¿›è¡Œäº†æ‰©å±•å’Œæ¦‚æ‹¬ï¼Œä»¥å……åˆ†æ ¡å‡†FPQçš„æƒé‡é‡åŒ–ï¼Œå¹¶è¯æ˜DiTçš„æ¿€æ´»å–å†³äºè¾“å…¥è¡¥ä¸æ•°æ®ï¼Œè¿™éœ€è¦ç¨³å¥çš„åœ¨çº¿æ¿€æ´»é‡åŒ–æŠ€æœ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFP4DiTåœ¨W4A6å’ŒW4A8ç²¾åº¦ä¸Šä¼˜äºåŸºäºæ•´æ•°çš„PTQï¼Œå¹¶åœ¨PixArt-Î±ã€PixArt-Î£å’ŒHunyuanç­‰å¤šä¸ªT2IæŒ‡æ ‡ï¼ˆå¦‚HPSv2å’ŒCLIPï¼‰ä¸Šç”Ÿæˆäº†ä»¤äººä¿¡æœçš„è§†è§‰å†…å®¹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15465v1">PDF</a> The code is available at <a target="_blank" rel="noopener" href="https://github.com/cccrrrccc/FP4DiT">https://github.com/cccrrrccc/FP4DiT</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ‰©æ•£æ¨¡å‹ï¼ˆDMï¼‰åœ¨æ–‡æœ¬åˆ°å›¾åƒçš„è§†è§‰ç”Ÿæˆè¿‡ç¨‹ä¸­èµ·åˆ°äº†é©å‘½æ€§çš„ä½œç”¨ï¼Œä½†å…¶åºå¤§çš„è®¡ç®—æˆæœ¬å’Œæ¨¡å‹å ç”¨ç©ºé—´é˜»ç¢äº†å®é™…åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæ— éœ€è®­ç»ƒæˆ–å¾®è°ƒçš„åè®­ç»ƒé‡åŒ–ï¼ˆPTQï¼‰æ˜¯ä¸€ç§è½»é‡çº§æ–¹æ³•ã€‚è™½ç„¶æœ€è¿‘çš„æ‰©æ•£æ¨¡å‹PTQæ–¹æ³•å®ç°äº†æ•´æ•°é‡åŒ–çš„W4A8ç²¾åº¦ï¼Œä½†ä»å­˜åœ¨ä¸¤ä¸ªå…³é”®å±€é™ï¼šé¦–å…ˆï¼Œå¤§å¤šæ•°ç°æœ‰çš„æ‰©æ•£æ¨¡å‹PTQæ–¹æ³•æ˜¯åœ¨ä½¿ç”¨å·ç§¯U-Netçš„ç»å…¸æ‰©æ•£æ¨¡å‹ï¼ˆå¦‚Stable Diffusion XL 1.5ç­‰ï¼‰ä¸Šè¯„ä¼°çš„ï¼Œè€Œæ–°ä¸€ä»£å¦‚PixArtç³»åˆ—å’ŒHunyuanç­‰æ‰©æ•£å˜æ¢å™¨ï¼ˆDiTï¼‰æ¨¡å‹é‡‡ç”¨äº†æ ¹æœ¬ä¸åŒçš„è½¬æ¢å™¨ä¸»å¹²ï¼Œä»¥å®ç°æ›´ä¼˜è¶Šçš„å›¾åƒåˆæˆã€‚å…¶æ¬¡ï¼Œæ•´æ•°é‡åŒ–ï¼ˆINTï¼‰åœ¨æ‰©æ•£æ¨¡å‹PTQä¸­å æ®ä¸»å¯¼åœ°ä½ï¼Œä½†ä¸ç½‘ç»œæƒé‡å’Œæ¿€æ´»åˆ†å¸ƒä¸å¤ªåŒ¹é…ã€‚è™½ç„¶æµ®ç‚¹é‡åŒ–ï¼ˆFPQï¼‰ä»å¤„äºç ”ç©¶åˆæœŸé˜¶æ®µï¼Œä½†å…¶åœ¨å¯¹é½ä½æ¯”ç‰¹è®¾ç½®ä¸­çš„æƒé‡å’Œæ¿€æ´»åˆ†å¸ƒæ–¹é¢å…·æœ‰æ½œåŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºFPQçš„åè®­ç»ƒé‡åŒ–æ–¹æ³•FP4DiTï¼Œå¹¶å®ç°äº†W4A6é‡åŒ–ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æ‰©å±•å¹¶æ¨å¹¿äº†è‡ªé€‚åº”å››èˆäº”å…¥PTQæŠ€æœ¯æ¥æ ¡å‡†æƒé‡é‡åŒ–ä»¥é€‚åº”FPQï¼Œå¹¶è¯æ˜äº†DiTæ¿€æ´»ä¾èµ–äºè¾“å…¥è¡¥ä¸æ•°æ®ï¼Œéœ€è¦ç¨³å¥çš„åœ¨çº¿æ¿€æ´»é‡åŒ–æŠ€æœ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFP4DiTåœ¨PixArt-Î±ã€PixArt-Î£å’ŒHunyuanç­‰å¤šç§å‹å·çš„T2Iåº¦é‡æ ‡å‡†ä¸‹è¡¨ç°ä¼˜å¼‚ï¼Œä¾‹å¦‚åœ¨HPSv2å’ŒCLIPä¸­è¡¨ç°å‡ºç”Ÿæˆå¯ä¿¡çš„è§†è§‰å†…å®¹èƒ½åŠ›ï¼Œå¹¶ä¸”åœ¨W4A6å’ŒW4A8ç²¾åº¦ä¸‹è¶…è¿‡äº†åŸºäºæ•´æ•°çš„PTQã€‚ </p>
<p><strong>è¦ç‚¹åˆ†æ</strong> </p>
<p>æ‰©æ•£æ¨¡å‹å·²ç»å¼•å‘äº†æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆè¿‡ç¨‹çš„é©å‘½æ€§å˜é©ã€‚ç„¶è€Œç”±äºå…¶è®¡ç®—æˆæœ¬è¾ƒé«˜ä¸”å ç”¨ç©ºé—´è¾ƒå¤§ï¼Œé™åˆ¶äº†å…¶åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šçš„å®é™…åº”ç”¨éƒ¨ç½²ã€‚å¯¹æ­¤é—®é¢˜æå‡ºäº†åè®­ç»ƒé‡åŒ–ï¼ˆPTQï¼‰çš„è½»é‡çº§è§£å†³æ–¹æ¡ˆã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨æ•´æ•°é‡åŒ–ä¸Šï¼Œä½†è¿™ç§æ–¹æ³•å¯¹ç½‘ç»œæƒé‡å’Œæ¿€æ´»åˆ†å¸ƒä¸å¤ªå‹å¥½ï¼Œéœ€è¦è¿›ä¸€æ­¥æ¢è®¨ä¼˜åŒ–æ–¹æ³•ã€‚æ­¤å¤–å¤§éƒ¨åˆ†PTQæ–¹æ³•çš„æµ‹è¯•æ˜¯åŸºäºç»å…¸çš„æ‰©æ•£æ¨¡å‹æ¶æ„è¿›è¡Œçš„è¯„ä¼°æµ‹è¯•ä¸æ–°å‹æ‰©æ•£å˜æ¢å™¨æ¨¡å‹å­˜åœ¨è¾ƒå¤§å·®å¼‚ä¸”å¹¶æœªæ·±å…¥æ¢ç©¶ã€‚åœ¨æ­¤èƒŒæ™¯ä¸‹æœ¬æ–‡æå‡ºäº†FP4DiTè¿™ä¸€åˆ›æ–°å‹çš„PTQæ–¹æ¡ˆä»¥æ»¡è¶³è½»é‡çº§ä½èƒ½è€—åœºæ™¯çš„åº”ç”¨éœ€æ±‚é‡ç‚¹æ¶‰åŠä¸¤å¤§æ–¹å‘å¯¹æ‰©æ”¶ç¼©æ”¹è¿›æå‡äº†å¯¹ä¸åŒæ•°æ®é›†çš„æ•´ä½“æ•ˆæœå¹¶å±•ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ä¼˜åŠ¿ã€‚å…·ä½“æ¥è¯´æœ¬æ–‡çš„åˆ›æ–°ç‚¹åŒ…æ‹¬ï¼š </p>
<ol>
<li>é’ˆå¯¹æ–°å‹æ‰©æ•£å˜æ¢å™¨æ¨¡å‹çš„ç‰¹æ€§è¿›è¡Œäº†æ·±å…¥ç ”ç©¶å®ç°äº†è¯¥ç±»æ¨¡å‹çš„ç²¾ç»†åŒ–é‡åŒ–åŒæ—¶ä¿æŒäº†è¾ƒå¥½çš„æ€§èƒ½è¡¨ç°è¿™æå¤§æ¨åŠ¨äº†å…¶åœ¨å®é™…åº”ç”¨åœºæ™¯ä¸‹çš„åº”ç”¨å¯èƒ½æ€§ä¸æ•ˆæœæå‡ï¼› </li>
<li>åŸºäºæµ®ç‚¹æ•°é‡åŒ–ç†è®ºè¿›è¡Œä¼˜åŒ–å…‹æœäº†ä¼ ç»Ÿçš„åŸºäºæ•´æ•°çš„é‡åŒ–çš„ç¼ºç‚¹ç½‘ç»œæƒé‡çš„åŒ¹é…å’Œæ¿€æ´»åˆ†å¸ƒçš„ç²¾ç¡®åº¦å¾—åˆ°è¿›ä¸€æ­¥æå‡ä½¿å¾—æ¨¡å‹åœ¨ä½æ¯”ç‰¹è®¾ç½®ä¸‹è¡¨ç°æ›´ä¸ºå‡ºè‰²ï¼› </li>
<li>æå‡ºä¸€ç§å…¨æ–°çš„åœ¨çº¿æ¿€æ´»é‡åŒ–æŠ€æœ¯è¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆå¤„ç†æ¿€æ´»æ•°æ®çš„è¾“å…¥ä¾èµ–æ€§ä¿è¯æ¨¡å‹åœ¨ä¸åŒæ•°æ®é›†ä¸‹çš„é²æ£’æ€§æå‡äº†æ¨¡å‹çš„é€šç”¨æ€§å’Œå®ç”¨æ€§ï¼›</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15465">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-69529399f19b98f91d5f448e8968ea0b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-98853f1f3e4df7d6aab34597c6d7fed0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c11b54c8c4cd4955ada30be0b644cfdf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1837dcbb4e919a8b0949481e6861d57d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-19a32d91539562c4529312133bfec74f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2bc95b9183933de04293a6489f3628e0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Visual-Persona-Foundation-Model-for-Full-Body-Human-Customization"><a href="#Visual-Persona-Foundation-Model-for-Full-Body-Human-Customization" class="headerlink" title="Visual Persona: Foundation Model for Full-Body Human Customization"></a>Visual Persona: Foundation Model for Full-Body Human Customization</h2><p><strong>Authors:Jisu Nam, Soowon Son, Zhan Xu, Jing Shi, Difan Liu, Feng Liu, Aashish Misraa, Seungryong Kim, Yang Zhou</strong></p>
<p>We introduce Visual Persona, a foundation model for text-to-image full-body human customization that, given a single in-the-wild human image, generates diverse images of the individual guided by text descriptions. Unlike prior methods that focus solely on preserving facial identity, our approach captures detailed full-body appearance, aligning with text descriptions for body structure and scene variations. Training this model requires large-scale paired human data, consisting of multiple images per individual with consistent full-body identities, which is notoriously difficult to obtain. To address this, we propose a data curation pipeline leveraging vision-language models to evaluate full-body appearance consistency, resulting in Visual Persona-500K, a dataset of 580k paired human images across 100k unique identities. For precise appearance transfer, we introduce a transformer encoder-decoder architecture adapted to a pre-trained text-to-image diffusion model, which augments the input image into distinct body regions, encodes these regions as local appearance features, and projects them into dense identity embeddings independently to condition the diffusion model for synthesizing customized images. Visual Persona consistently surpasses existing approaches, generating high-quality, customized images from in-the-wild inputs. Extensive ablation studies validate design choices, and we demonstrate the versatility of Visual Persona across various downstream tasks. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†Visual Personaï¼Œè¿™æ˜¯ä¸€ç§é¢å‘æ–‡æœ¬åˆ°å›¾åƒå…¨èº«äººä½“å®šåˆ¶çš„åŸºç¡€æ¨¡å‹ã€‚ç»™å®šä¸€å¼ é‡ç”Ÿç¯å¢ƒä¸‹çš„å•äººå›¾åƒï¼Œè¯¥æ¨¡å‹ä¼šæ ¹æ®æ–‡æœ¬æè¿°ç”Ÿæˆå¤šæ ·åŒ–çš„ä¸ªäººå›¾åƒã€‚ä¸ä»¥å¾€ä»…ä¸“æ³¨äºä¿ç•™é¢éƒ¨èº«ä»½çš„æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ•æ‰äº†è¯¦ç»†çš„å…¨èº«å¤–è§‚ï¼Œä¸æ–‡æœ¬æè¿°çš„èº«ä½“ç»“æ„å’Œåœºæ™¯å˜åŒ–ä¿æŒä¸€è‡´ã€‚è®­ç»ƒè¿™ä¸ªæ¨¡å‹éœ€è¦å¤§è§„æ¨¡é…å¯¹çš„äººä½“æ•°æ®ï¼Œå¯¹äºæ¯ä¸ªä¸ªä½“ï¼Œéƒ½éœ€è¦æœ‰å¤šå¼ å…·æœ‰ä¸€è‡´å…¨èº«èº«ä»½çš„å›¾åƒï¼Œè¿™äº›æ•°æ®æ˜¯ä¼—æ‰€å‘¨çŸ¥çš„éš¾ä»¥è·å¾—çš„ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹è¯„ä¼°å…¨èº«å¤–è§‚ä¸€è‡´æ€§çš„æ•°æ®æ•´ç†æµç¨‹ï¼Œä»è€Œå½¢æˆäº†Visual Persona-500Kæ•°æ®é›†ï¼ŒåŒ…å«580kå¼ é…å¯¹çš„äººä½“å›¾åƒï¼Œæ¶‰åŠ10ä¸‡å¼ ç‹¬ç‰¹çš„èº«ä»½ã€‚ä¸ºäº†å®ç°ç²¾ç¡®çš„å¤–è§‚è½¬ç§»ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§é€‚åº”äºé¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„transformerç¼–ç å™¨-è§£ç å™¨æ¶æ„ã€‚å®ƒå°†è¾“å…¥å›¾åƒåˆ†å‰²ä¸ºä¸åŒçš„èº«ä½“åŒºåŸŸï¼Œå°†è¿™äº›åŒºåŸŸç¼–ç ä¸ºå±€éƒ¨å¤–è§‚ç‰¹å¾ï¼Œå¹¶å°†å®ƒä»¬æŠ•å½±åˆ°å¯†é›†çš„èº«ä»½åµŒå…¥ä¸­ï¼Œä»¥ç‹¬ç«‹åœ°æ§åˆ¶æ‰©æ•£æ¨¡å‹æ¥åˆæˆå®šåˆ¶çš„å›¾åƒã€‚Visual Personaå§‹ç»ˆè¶…è¶Šç°æœ‰æ–¹æ³•ï¼Œä»é‡ç”Ÿç¯å¢ƒä¸­ç”Ÿæˆé«˜è´¨é‡ã€å®šåˆ¶çš„å›¾åƒã€‚å¹¿æ³›çš„æ¶ˆèç ”ç©¶éªŒè¯äº†è®¾è®¡é€‰æ‹©ï¼Œæˆ‘ä»¬å±•ç¤ºäº†Visual Personaåœ¨å„ç§ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„é€šç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15406v1">PDF</a> CVPR 2025, Project page is available at   <a target="_blank" rel="noopener" href="https://cvlab-kaist.github.io/Visual-Persona">https://cvlab-kaist.github.io/Visual-Persona</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Visual Personaæ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ç§é¢å‘æ–‡æœ¬åˆ°å›¾åƒçš„å…¨èº«äººä½“å®šåˆ¶æ¨¡å‹ã€‚ç»™å®šä¸€å¼ é‡ç”Ÿç¯å¢ƒä¸‹çš„å•äººå›¾åƒï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿæ ¹æ®æ–‡æœ¬æè¿°ç”Ÿæˆå¤šæ ·åŒ–çš„ä¸ªä½“å›¾åƒã€‚ä¸ä»…å…³æ³¨é¢éƒ¨èº«ä»½ä¿ç•™çš„ç°æœ‰æ–¹æ³•ä¸åŒï¼ŒVisual Personaæ¨¡å‹èƒ½å¤Ÿæ•æ‰å…¨èº«çš„è¯¦ç»†å¤–è§‚ï¼Œå¹¶æ ¹æ®æ–‡æœ¬æè¿°è¿›è¡Œèº«ä½“ç»“æ„å’Œåœºæ™¯å˜åŒ–çš„å¯¹é½ã€‚ä¸ºè®­ç»ƒæ­¤æ¨¡å‹ï¼Œéœ€è¦å¤§è§„æ¨¡é…å¯¹çš„äººç±»æ•°æ®ï¼ŒåŒ…å«æ¯å¼ ä¸ªä½“å›¾åƒéƒ½æœ‰ä¸€è‡´çš„å…¨èº«èº«ä»½ã€‚ä¸ºè§£å†³æ•°æ®è·å–éš¾é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºäº†ä¸€ä¸ªæ•°æ®æ•´ç†ç®¡é“ï¼Œåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹è¯„ä¼°å…¨èº«å¤–è§‚çš„ä¸€è‡´æ€§ï¼Œå¹¶åˆ›å»ºäº†Visual Persona-500Kæ•°æ®é›†ï¼ŒåŒ…å«58ä¸‡å¼ é…å¯¹çš„äººç±»å›¾åƒå’Œ10ä¸‡ä¸ªå”¯ä¸€èº«ä»½ã€‚ä¸ºå®ç°ç²¾ç¡®çš„å¤–è§‚è½¬ç§»ï¼Œç ”ç©¶å›¢é˜Ÿå¼•å…¥äº†å˜å‹å™¨ç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œè¯¥æ¶æ„é€‚åº”äºé¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œå¯å°†è¾“å…¥å›¾åƒåˆ†ä¸ºä¸åŒçš„èº«ä½“åŒºåŸŸï¼Œç¼–ç ä¸ºå±€éƒ¨å¤–è§‚ç‰¹å¾ï¼Œå¹¶æŠ•å½±åˆ°å¯†é›†çš„èº«ä»½åµŒå…¥ä¸­ï¼Œä»¥æ¡ä»¶æ‰©æ•£æ¨¡å‹åˆæˆå®šåˆ¶å›¾åƒã€‚Visual Personaåœ¨ç”Ÿæˆé«˜è´¨é‡ã€å®šåˆ¶çš„å›¾åƒæ–¹é¢è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Visual Personaæ˜¯ä¸€ä¸ªé¢å‘æ–‡æœ¬åˆ°å›¾åƒçš„å…¨èº«äººä½“å®šåˆ¶æ¨¡å‹ã€‚</li>
<li>è¯¥æ¨¡å‹é€šè¿‡æ–‡æœ¬æè¿°ç”Ÿæˆå¤šæ ·åŒ–çš„ä¸ªä½“å›¾åƒã€‚</li>
<li>Visual Personaæ¨¡å‹ä¸ä»…å…³æ³¨é¢éƒ¨èº«ä»½ï¼Œè¿˜æ•æ‰å…¨èº«çš„è¯¦ç»†å¤–è§‚ã€‚</li>
<li>ä¸ºè®­ç»ƒæ­¤æ¨¡å‹ï¼Œéœ€è¦å¤§è§„æ¨¡é…å¯¹çš„äººç±»æ•°æ®ï¼ŒåŒ…å«æ¯å¼ ä¸ªä½“å›¾åƒéƒ½æœ‰ä¸€è‡´çš„å…¨èº«èº«ä»½ã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹åˆ›å»ºäº†ä¸€ä¸ªæ•°æ®æ•´ç†ç®¡é“ï¼Œå¹¶åˆ›å»ºäº†Visual Persona-500Kæ•°æ®é›†ã€‚</li>
<li>Visual Personaé‡‡ç”¨äº†å˜å‹å™¨ç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œå¹¶ç»“åˆé¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å®ç°ç²¾ç¡®å¤–è§‚è½¬ç§»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15406">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d834701677500103c02e9ef72774420b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e25a9f84fd426fdd8d5a460a3d38f3a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1c92e379e533959e5ec2fab7a446485a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-77048ff89ff89af9f35a29e75239d916.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Material-Decomposition-in-Photon-Counting-Computed-Tomography-with-Diffusion-Models-Comparative-Study-and-Hybridization-with-Variational-Regularizers"><a href="#Material-Decomposition-in-Photon-Counting-Computed-Tomography-with-Diffusion-Models-Comparative-Study-and-Hybridization-with-Variational-Regularizers" class="headerlink" title="Material Decomposition in Photon-Counting Computed Tomography with   Diffusion Models: Comparative Study and Hybridization with Variational   Regularizers"></a>Material Decomposition in Photon-Counting Computed Tomography with   Diffusion Models: Comparative Study and Hybridization with Variational   Regularizers</h2><p><strong>Authors:Corentin Vazia, Thore Dassow, Alexandre Bousse, Jacques Froment, BÃ©atrice Vedel, Franck Vermet, Alessandro Perelli, Jean-Pierre Tasu, Dimitris Visvikis</strong></p>
<p>Photon-counting computed tomography (PCCT) has emerged as a promising imaging technique, enabling spectral imaging and material decomposition (MD). However, images typically suffer from a low signal-to-noise ratio due to constraints such as low photon counts and sparse-view settings. Variational methods minimize a data-fit function coupled with handcrafted regularizers but are highly dependent on the choice of the regularizers. Artificial intelligence (AI)-based approaches and more particularly convolutional neural networks (CNNs) are now considered the state-of-the-art approach and can be used as an end-to-end method for MD or to implicitly learn an a priori. In the last few years, diffusion models (DMs) became predominant in the field of generative models where a distribution function is learned. This distribution function can be used as a prior for solving inverse problems. This work investigates the use of DMs as regularizers for MD tasks in PCCT. MD by diffusion posterior sampling (DPS) can be achieved. Three DPS-based approaches â€“ image-domain two-step DPS (im-TDPS), projection-domain two-step DPS (proj-TDPS), and one-step DPS (ODPS) â€“ are evaluated. The first two methods perform MD in two steps: im-TDPS samples spectral images by DPS then performs image-based MD, while proj-TDPS performs projection-based MD then samples material images by DPS. The last method, ODPS, samples the material images directly from the measurement data. The results indicate that ODPS achieves superior performance compared to im-TDPS and proj-TDPS, providing sharper, noise-free and crosstalk-free images. Furthermore, we introduce a novel hybrid ODPS method combining DM priors with standard variational regularizers for scenarios involving materials absent from the training dataset. This hybrid method demonstrates improved material reconstruction quality compared to a standard variational method. </p>
<blockquote>
<p>å…‰å­è®¡æ•°è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆPCCTï¼‰ä½œä¸ºä¸€ç§æœ‰å‰æ™¯çš„æˆåƒæŠ€æœ¯å·²ç»å‡ºç°ï¼Œèƒ½å¤Ÿå®ç°å…‰è°±æˆåƒå’Œææ–™åˆ†è§£ï¼ˆMDï¼‰ã€‚ç„¶è€Œï¼Œç”±äºå…‰å­è®¡æ•°ä½å’Œç¨€ç–è§†å›¾è®¾ç½®ç­‰é™åˆ¶ï¼Œå›¾åƒé€šå¸¸ä¿¡å™ªæ¯”ä½ã€‚å˜åˆ†æ–¹æ³•é€šè¿‡æœ€å°åŒ–æ•°æ®æ‹Ÿåˆå‡½æ•°ä¸æ‰‹å·¥æ­£åˆ™åŒ–ç›¸ç»“åˆï¼Œä½†é«˜åº¦ä¾èµ–äºæ­£åˆ™åŒ–çš„é€‰æ‹©ã€‚åŸºäºäººå·¥æ™ºèƒ½ï¼ˆAIï¼‰çš„æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ç°åœ¨è¢«è®¤ä¸ºæ˜¯æœ€æ–°æŠ€æœ¯ï¼Œå¯ä»¥ç”¨ä½œç«¯åˆ°ç«¯çš„MDæ–¹æ³•æˆ–éšå¼å­¦ä¹ å…ˆéªŒçŸ¥è¯†ã€‚åœ¨è¿‡å»çš„å‡ å¹´ä¸­ï¼Œæ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰åœ¨ç”Ÿæˆæ¨¡å‹ä¸­å æ®äº†ä¸»å¯¼åœ°ä½ï¼Œå…¶ä¸­åˆ†å¸ƒå‡½æ•°å¾—ä»¥å­¦ä¹ ã€‚è¯¥åˆ†å¸ƒå‡½æ•°å¯ä»¥ç”¨ä½œè§£å†³é€†é—®é¢˜çš„å…ˆéªŒã€‚è¿™é¡¹å·¥ä½œæ¢è®¨äº†å°†DMsç”¨ä½œPCCTä¸­MDä»»åŠ¡çš„æ­£è§„åŒ–æ–¹æ³•ã€‚é€šè¿‡æ‰©æ•£åé‡‡æ ·ï¼ˆDPSï¼‰å¯ä»¥å®ç°MDã€‚è¯„ä¼°äº†ä¸‰ç§åŸºäºDPSçš„æ–¹æ³•â€”â€”å›¾åƒåŸŸä¸¤æ­¥DPSï¼ˆim-TDPSï¼‰ã€æŠ•å½±åŸŸä¸¤æ­¥DPSï¼ˆproj-TDPSï¼‰å’Œä¸€æ­¥DPSï¼ˆODPSï¼‰ã€‚å‰ä¸¤ç§æ–¹æ³•åˆ†ä¸¤æ­¥è¿›è¡ŒMDï¼šim-TDPSé€šè¿‡DPSé‡‡æ ·å…‰è°±å›¾åƒï¼Œç„¶åè¿›è¡ŒåŸºäºå›¾åƒçš„MDï¼Œè€Œproj-TDPSè¿›è¡ŒåŸºäºæŠ•å½±çš„MDï¼Œç„¶åé€šè¿‡DPSé‡‡æ ·ææ–™å›¾åƒã€‚æœ€åä¸€ç§æ–¹æ³•ODPSç›´æ¥ä»æµ‹é‡æ•°æ®ä¸­é‡‡æ ·ææ–™å›¾åƒã€‚ç»“æœè¡¨æ˜ï¼Œä¸im-TDPSå’Œproj-TDPSç›¸æ¯”ï¼ŒODPSå®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼Œæä¾›äº†æ›´æ¸…æ™°ã€æ— å™ªå£°ä¸”æ— ä¸²æ‰°çš„å›¾åƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹æ··åˆODPSæ–¹æ³•ï¼Œå°†DMå…ˆéªŒä¸æ ‡å‡†å˜åˆ†æ­£åˆ™åŒ–å™¨ç›¸ç»“åˆï¼Œç”¨äºæ¶‰åŠè®­ç»ƒæ•°æ®é›†ä¸­ç¼ºå°‘çš„ææ–™çš„æƒ…å†µã€‚è¯¥æ··åˆæ–¹æ³•æé«˜äº†ææ–™é‡å»ºçš„è´¨é‡ï¼Œä¸æ ‡å‡†å˜åˆ†æ–¹æ³•ç›¸æ¯”è¡¨ç°å‡ºä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15383v1">PDF</a> 12 pages, 10 figures, 4 tables</p>
<p><strong>Summary</strong></p>
<p>å…‰å­è®¡æ•°è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆPCCTï¼‰ä¸­çš„ç‰©è´¨åˆ†è§£ï¼ˆMDï¼‰æŠ€æœ¯å› å›¾åƒçš„ä½ä¿¡å™ªæ¯”è€Œå—åˆ°æŒ‘æˆ˜ï¼Œå¦‚å…‰å­è®¡æ•°ä½å’Œè§†å›¾ç¨€ç–ç­‰é—®é¢˜ã€‚å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å·²æˆä¸ºç›®å‰æœ€å‰æ²¿çš„æŠ€æœ¯ï¼Œå¯ä½œä¸ºç«¯åˆ°ç«¯çš„MDæ–¹æ³•æˆ–éšå¼å­¦ä¹ å…ˆéªŒã€‚æ‰©æ•£æ¨¡å‹ï¼ˆDMï¼‰å¯ç”¨äºç”Ÿæˆæ¨¡å‹ä¸­çš„å…ˆéªŒåˆ†å¸ƒå‡½æ•°æ¥è§£å†³åé—®é¢˜ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†å°†DMä½œä¸ºPCCTä¸­MDä»»åŠ¡çš„æ­£è§„åŒ–å™¨ã€‚é€šè¿‡æ‰©æ•£åé‡‡æ ·ï¼ˆDPSï¼‰å®ç°MDã€‚è¯„ä¼°äº†ä¸‰ç§åŸºäºDPSçš„æ–¹æ³•ï¼ŒåŒ…æ‹¬å›¾åƒåŸŸä¸¤æ­¥DPSã€æŠ•å½±åŸŸä¸¤æ­¥DPSå’Œä¸€æ­¥DPSã€‚ç»“æœæ˜¾ç¤ºï¼Œä¸€æ­¥DPSæ€§èƒ½æœ€ä¼˜ï¼Œèƒ½æä¾›æ¸…æ™°ã€æ— å™ªå£°ä¸”æ— ä¸²æ‰°çš„å›¾åƒã€‚æ­¤å¤–ï¼Œè¿˜ä»‹ç»äº†ä¸€ç§ç»“åˆDMå…ˆéªŒä¸ä¼ ç»Ÿå˜åˆ†æ­£åˆ™åŒ–çš„æ··åˆä¸€æ­¥DPSæ–¹æ³•ï¼Œç”¨äºå¤„ç†è®­ç»ƒæ•°æ®é›†ä¸­ç¼ºå°‘çš„ææ–™åœºæ™¯ï¼Œè¯¥æ–¹æ³•æé«˜äº†ææ–™é‡å»ºè´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å…‰å­è®¡æ•°è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆPCCTï¼‰ä¸­çš„ç‰©è´¨åˆ†è§£ï¼ˆMDï¼‰æŠ€æœ¯é¢ä¸´ä½ä¿¡å™ªæ¯”é—®é¢˜ã€‚</li>
<li>å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å·²æˆä¸ºè§£å†³MDé—®é¢˜çš„æœ€æ–°æŠ€æœ¯ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹ï¼ˆDMï¼‰å¯ç”¨ä½œç”Ÿæˆæ¨¡å‹ä¸­çš„å…ˆéªŒåˆ†å¸ƒå‡½æ•°ï¼Œä»¥è§£å†³åé—®é¢˜ã€‚</li>
<li>æœ¬ç ”ç©¶æ¢è®¨äº†å°†DMç”¨ä½œPCCTä¸­MDä»»åŠ¡çš„æ­£è§„åŒ–å™¨ï¼Œé‡‡ç”¨æ‰©æ•£åé‡‡æ ·ï¼ˆDPSï¼‰æ–¹æ³•å®ç°ã€‚</li>
<li>ä¸‰ç§åŸºäºDPSçš„æ–¹æ³•ä¸­ï¼Œä¸€æ­¥DPSæ€§èƒ½æœ€ä¼˜ï¼Œèƒ½æä¾›æ¸…æ™°ã€æ— å™ªå£°ä¸”æ— ä¸²æ‰°çš„å›¾åƒã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§ç»“åˆDMå…ˆéªŒä¸ä¼ ç»Ÿå˜åˆ†æ­£åˆ™åŒ–çš„æ··åˆæ–¹æ³•ï¼Œç”¨äºå¤„ç†è®­ç»ƒæ•°æ®é›†ä¸­ç¼ºå°‘çš„ææ–™åœºæ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15383">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-eb9be5375c5148986f6dc31e734f2045.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2ca4bbff21bfcec40966236130414fd3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1c0b7087ab4a8396851be33d920432c4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b4040de9acababdd43fba2fc46efab54.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1f95eb72229ea129a163c45678d6816f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-359b63a1d388d81161cf36a17488221e.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Detect-and-Guide-Self-regulation-of-Diffusion-Models-for-Safe-Text-to-Image-Generation-via-Guideline-Token-Optimization"><a href="#Detect-and-Guide-Self-regulation-of-Diffusion-Models-for-Safe-Text-to-Image-Generation-via-Guideline-Token-Optimization" class="headerlink" title="Detect-and-Guide: Self-regulation of Diffusion Models for Safe   Text-to-Image Generation via Guideline Token Optimization"></a>Detect-and-Guide: Self-regulation of Diffusion Models for Safe   Text-to-Image Generation via Guideline Token Optimization</h2><p><strong>Authors:Feifei Li, Mi Zhang, Yiming Sun, Min Yang</strong></p>
<p>Text-to-image diffusion models have achieved state-of-the-art results in synthesis tasks; however, there is a growing concern about their potential misuse in creating harmful content. To mitigate these risks, post-hoc model intervention techniques, such as concept unlearning and safety guidance, have been developed. However, fine-tuning model weights or adapting the hidden states of the diffusion model operates in an uninterpretable way, making it unclear which part of the intermediate variables is responsible for unsafe generation. These interventions severely affect the sampling trajectory when erasing harmful concepts from complex, multi-concept prompts, thus hindering their practical use in real-world settings. In this work, we propose the safe generation framework Detect-and-Guide (DAG), leveraging the internal knowledge of diffusion models to perform self-diagnosis and fine-grained self-regulation during the sampling process. DAG first detects harmful concepts from noisy latents using refined cross-attention maps of optimized tokens, then applies safety guidance with adaptive strength and editing regions to negate unsafe generation. The optimization only requires a small annotated dataset and can provide precise detection maps with generalizability and concept specificity. Moreover, DAG does not require fine-tuning of diffusion models, and therefore introduces no loss to their generation diversity. Experiments on erasing sexual content show that DAG achieves state-of-the-art safe generation performance, balancing harmfulness mitigation and text-following performance on multi-concept real-world prompts. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹åœ¨åˆæˆä»»åŠ¡ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æˆæœï¼›ç„¶è€Œï¼Œå…³äºå…¶å¯èƒ½ç”¨äºåˆ›å»ºæœ‰å®³å†…å®¹çš„æ½œåœ¨æ»¥ç”¨é—®é¢˜æ—¥ç›Šå—åˆ°å…³æ³¨ã€‚ä¸ºäº†å‡è½»è¿™äº›é£é™©ï¼Œå¼€å‘äº†äº‹åæ¨¡å‹å¹²é¢„æŠ€æœ¯ï¼Œå¦‚æ¦‚å¿µé—å¿˜å’Œå®‰å…¨æŒ‡å¯¼ã€‚ç„¶è€Œï¼Œå¾®è°ƒæ‰©æ•£æ¨¡å‹çš„æƒé‡æˆ–è°ƒæ•´éšè—çŠ¶æ€çš„æ–¹å¼æ˜¯ä¸å¯è§£é‡Šçš„ï¼Œå› æ­¤ä¸æ¸…æ¥šä¸­é—´å˜é‡ä¸­çš„å“ªä¸€éƒ¨åˆ†è´Ÿè´£äº§ç”Ÿä¸å®‰å…¨çš„å†…å®¹ã€‚è¿™äº›å¹²é¢„åœ¨ä»å¤æ‚çš„å¤šæ¦‚å¿µæç¤ºä¸­åˆ é™¤æœ‰å®³æ¦‚å¿µæ—¶ï¼Œä¸¥é‡å½±å“é‡‡æ ·è½¨è¿¹ï¼Œä»è€Œé˜»ç¢äº†å®ƒä»¬åœ¨ç°å®ä¸–ç•Œè®¾ç½®ä¸­çš„å®é™…åº”ç”¨ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†å®‰å…¨ç”Ÿæˆæ¡†æ¶Detect-and-Guideï¼ˆDAGï¼‰ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„å†…éƒ¨çŸ¥è¯†åœ¨é‡‡æ ·è¿‡ç¨‹ä¸­è¿›è¡Œè‡ªè¯Šæ–­å’Œç²¾ç»†çš„è‡ªæˆ‘ä¿æŠ¤ã€‚DAGé¦–å…ˆä½¿ç”¨ä¼˜åŒ–ä»¤ç‰Œçš„ç²¾ç»†äº¤å‰æ³¨æ„åŠ›å›¾ä»å˜ˆæ‚çš„æ½œåœ¨ç©ºé—´ä¸­æ£€æµ‹æœ‰å®³æ¦‚å¿µï¼Œç„¶ååº”ç”¨è‡ªé€‚åº”å¼ºåº¦å’Œç¼–è¾‘åŒºåŸŸçš„å®‰å…¨æŒ‡å¯¼æ¥æŠµæ¶ˆä¸å®‰å…¨çš„ç”Ÿæˆã€‚ä¼˜åŒ–åªéœ€è¦ä¸€ä¸ªå°å‹æ³¨é‡Šæ•°æ®é›†ï¼Œå°±å¯ä»¥æä¾›å…·æœ‰é€šç”¨æ€§å’Œæ¦‚å¿µç‰¹å¼‚æ€§çš„ç²¾ç¡®æ£€æµ‹å›¾ã€‚æ­¤å¤–ï¼ŒDAGä¸éœ€è¦å¯¹æ‰©æ•£æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå› æ­¤ä¸ä¼šå¯¹å…¶ç”Ÿæˆå¤šæ ·æ€§é€ æˆæŸå¤±ã€‚åœ¨æ¶ˆé™¤æ€§å†…å®¹çš„å®éªŒæ–¹é¢ï¼ŒDAGå®ç°äº†æœ€å…ˆè¿›çš„å®‰å…¨ç”Ÿæˆæ€§èƒ½ï¼Œåœ¨å…·æœ‰å¤šæ¦‚å¿µçš„çœŸå®ä¸–ç•Œæç¤ºä¸­å¹³è¡¡äº†æœ‰å®³æ€§çš„å‡è½»å’Œéµå¾ªæ–‡æœ¬çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15197v1">PDF</a> CVPR25</p>
<p><strong>æ‘˜è¦</strong><br>æ‰©æ•£æ¨¡å‹åœ¨åˆæˆä»»åŠ¡ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æˆæœï¼Œä½†å…¶æ½œåœ¨æ»¥ç”¨é£é™©å¼•å‘å…³æ³¨ã€‚ä¸ºç¼“è§£é£é™©ï¼Œé‡‡å–äº†äº‹åæ¨¡å‹å¹²é¢„æŠ€æœ¯ï¼Œå¦‚æ¦‚å¿µé—å¿˜å’Œå®‰å…¨æŒ‡å¯¼ç­‰ã€‚ç„¶è€Œï¼Œå¯¹æ‰©æ•£æ¨¡å‹çš„æƒé‡å¾®è°ƒæˆ–éšè—çŠ¶æ€çš„è°ƒæ•´æ–¹å¼ç¼ºä¹å¯è§£é‡Šæ€§ï¼Œä½¿å¾—ä¸­é—´å˜é‡åœ¨äº§ç”Ÿä¸å®‰å…¨å†…å®¹æ—¶éš¾ä»¥æ˜ç¡®å…¶è´£ä»»å½’å±ã€‚æœ¬å·¥ä½œæå‡ºå®‰å…¨ç”Ÿæˆæ¡†æ¶Detect-and-Guideï¼ˆDAGï¼‰ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„å†…éƒ¨çŸ¥è¯†åœ¨é‡‡æ ·è¿‡ç¨‹ä¸­è¿›è¡Œè‡ªè¯Šæ–­å’Œç²¾ç»†åŒ–çš„è‡ªæˆ‘è°ƒæ§ã€‚DAGé¦–å…ˆä½¿ç”¨ä¼˜åŒ–åçš„æ ‡è®°çš„ç²¾ç»†äº¤å‰æ³¨æ„åŠ›å›¾æ£€æµ‹æ½œåœ¨å™ªå£°ä¸­çš„æœ‰å®³æ¦‚å¿µï¼Œç„¶ååº”ç”¨è‡ªé€‚åº”å¼ºåº¦å’Œç¼–è¾‘åŒºåŸŸçš„å®‰å…¨æŒ‡å¯¼æ¥é¿å…ä¸å®‰å…¨å†…å®¹çš„ç”Ÿæˆã€‚ä¼˜åŒ–ä»…éœ€å°‘é‡æ ‡æ³¨æ•°æ®é›†å³å¯æä¾›ç²¾ç¡®çš„æ£€æµ‹å›¾ï¼Œå…·æœ‰é€šç”¨æ€§å’Œæ¦‚å¿µç‰¹å¼‚æ€§ã€‚æ­¤å¤–ï¼ŒDAGä¸éœ€è¦å¯¹æ‰©æ•£æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå› æ­¤ä¸ä¼šæŸå¤±å…¶ç”Ÿæˆå¤šæ ·æ€§ã€‚åœ¨æ¶ˆé™¤è‰²æƒ…å†…å®¹å®éªŒä¸­ï¼ŒDAGåœ¨å®‰å…¨ç”Ÿæˆæ–¹é¢å–å¾—æœ€å…ˆè¿›çš„æˆæœï¼Œå¹³è¡¡äº†å±å®³å‡è½»å’Œéµå¾ªæ–‡æœ¬çš„å¤šä¸ªå®é™…æ¦‚å¿µæç¤ºæ€§èƒ½ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ–‡æœ¬è‡³å›¾åƒæ‰©æ•£æ¨¡å‹åœ¨åˆæˆä»»åŠ¡ä¸Šå–å¾—é¡¶å°–æˆæœï¼Œä½†å­˜åœ¨æ½œåœ¨æ»¥ç”¨é£é™©ã€‚</li>
<li>ä¸ºåº”å¯¹é£é™©ï¼Œé‡‡å–äº†äº‹åæ¨¡å‹å¹²é¢„æŠ€æœ¯ï¼Œä½†ç°æœ‰æŠ€æœ¯å½±å“é‡‡æ ·è½¨è¿¹å¹¶å‡å°‘ç”Ÿæˆå¤šæ ·æ€§ã€‚</li>
<li>æå‡ºå®‰å…¨ç”Ÿæˆæ¡†æ¶Detect-and-Guideï¼ˆDAGï¼‰ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„å†…éƒ¨çŸ¥è¯†è¿›è¡Œè‡ªæˆ‘è¯Šæ–­å’Œè°ƒæ§ã€‚</li>
<li>DAGé€šè¿‡æ£€æµ‹æœ‰å®³æ¦‚å¿µå¹¶æä¾›å®‰å…¨æŒ‡å¯¼æ¥å®ç°ç²¾ç¡®æ§åˆ¶å’Œä¼˜åŒ–ç”Ÿæˆå†…å®¹ã€‚</li>
<li>DAGé‡‡ç”¨å°å‹æ ‡æ³¨æ•°æ®é›†å³å¯å®ç°ç²¾ç¡®æ£€æµ‹å›¾ï¼Œå…·æœ‰é€šç”¨æ€§å’Œæ¦‚å¿µç‰¹å¼‚æ€§ã€‚</li>
<li>DAGæ— éœ€å¾®è°ƒæ‰©æ•£æ¨¡å‹æƒé‡ï¼Œç»´æŒæ¨¡å‹çš„ç”Ÿæˆå¤šæ ·æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15197">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c234b50800fb22675eb6ea53e7617cbf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6817ee9514bbb293a4de62a56f5969a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6f759d9a8ef4351fa320bb974177c93b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9c9124092bb6af40675d83a1e7d09801.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1d5c2d3f4403b88b43dd2a5de0623b88.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-35c2248fbb5598b6a4efb0b51453011f.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Single-Step-Bidirectional-Unpaired-Image-Translation-Using-Implicit-Bridge-Consistency-Distillation"><a href="#Single-Step-Bidirectional-Unpaired-Image-Translation-Using-Implicit-Bridge-Consistency-Distillation" class="headerlink" title="Single-Step Bidirectional Unpaired Image Translation Using Implicit   Bridge Consistency Distillation"></a>Single-Step Bidirectional Unpaired Image Translation Using Implicit   Bridge Consistency Distillation</h2><p><strong>Authors:Suhyeon Lee, Kwanyoung Kim, Jong Chul Ye</strong></p>
<p>Unpaired image-to-image translation has seen significant progress since the introduction of CycleGAN. However, methods based on diffusion models or Schr&quot;odinger bridges have yet to be widely adopted in real-world applications due to their iterative sampling nature. To address this challenge, we propose a novel framework, Implicit Bridge Consistency Distillation (IBCD), which enables single-step bidirectional unpaired translation without using adversarial loss. IBCD extends consistency distillation by using a diffusion implicit bridge model that connects PF-ODE trajectories between distributions. Additionally, we introduce two key improvements: 1) distribution matching for consistency distillation and 2) adaptive weighting method based on distillation difficulty. Experimental results demonstrate that IBCD achieves state-of-the-art performance on benchmark datasets in a single generation step. Project page available at <a target="_blank" rel="noopener" href="https://hyn2028.github.io/project_page/IBCD/index.html">https://hyn2028.github.io/project_page/IBCD/index.html</a> </p>
<blockquote>
<p>éé…å¯¹å›¾åƒåˆ°å›¾åƒçš„ç¿»è¯‘è‡ªCycleGANæå‡ºä»¥æ¥å–å¾—äº†é‡å¤§è¿›å±•ã€‚ç„¶è€Œï¼ŒåŸºäºæ‰©æ•£æ¨¡å‹æˆ–SchrÃ¶dingeræ¡¥æ¢çš„æ–¹æ³•ç”±äºå…¶è¿­ä»£é‡‡æ ·çš„ç‰¹æ€§ï¼Œå°šæœªåœ¨çœŸå®ä¸–ç•Œåº”ç”¨ä¸­å¹¿æ³›é‡‡ç”¨ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶â€”â€”éšæ¡¥ä¸€è‡´æ€§è’¸é¦ï¼ˆIBCDï¼‰ï¼Œå®ƒèƒ½å¤Ÿåœ¨ä¸ä½¿ç”¨å¯¹æŠ—æ€§æŸå¤±çš„æƒ…å†µä¸‹å®ç°å•æ­¥åŒå‘éé…å¯¹ç¿»è¯‘ã€‚IBCDé€šè¿‡é‡‡ç”¨è¿æ¥åˆ†å¸ƒé—´PF-ODEè½¨è¿¹çš„æ‰©æ•£éšæ¡¥æ¨¡å‹ï¼Œæ‰©å±•äº†ä¸€è‡´æ€§è’¸é¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸¤ç§å…³é”®æ”¹è¿›ï¼š1ï¼‰ç”¨äºä¸€è‡´æ€§è’¸é¦çš„åˆ†å¸ƒåŒ¹é…ï¼›2ï¼‰åŸºäºè’¸é¦éš¾åº¦çš„è‡ªé€‚åº”åŠ æƒæ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒIBCDåœ¨åŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†å•æ­¥ç”Ÿæˆçš„æœ€ä½³æ€§èƒ½ã€‚é¡¹ç›®é¡µé¢å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://hyn2028.github.io/project_page/IBCD/index.html%E8%AE%BF%E9%97%AE%E3%80%82">https://hyn2028.github.io/project_page/IBCD/index.htmlè®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15056v1">PDF</a> 25 pages, 16 figures</p>
<p><strong>Summary</strong><br>     å¾ªç¯ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆCycleGANï¼‰æ¨å‡ºåï¼Œéé…å¯¹å›¾åƒåˆ°å›¾åƒè½¬æ¢å–å¾—æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼ŒåŸºäºæ‰©æ•£æ¨¡å‹æˆ–è–›å®šè°”æ¡¥çš„æ–¹æ³•ç”±äºè¿­ä»£é‡‡æ ·æ€§è´¨å°šæœªåœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­å¹¿æ³›é‡‡ç”¨ã€‚ä¸ºè§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºä¸€ç§æ–°å‹æ¡†æ¶â€”â€”éšæ¡¥ä¸€è‡´æ€§è’¸é¦ï¼ˆIBCDï¼‰ï¼Œå®ç°æ— éœ€å¯¹æŠ—æ€§æŸå¤±çš„å•æ­¥åŒå‘éé…å¯¹ç¿»è¯‘ã€‚IBCDé€šè¿‡æ‰©æ•£éšæ¡¥æ¨¡å‹è¿æ¥æ¦‚ç‡æµå¸¸å¾®åˆ†æ–¹ç¨‹è½¨è¿¹ï¼Œæ‰©å±•ä¸€è‡´æ€§è’¸é¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥ä¸¤ä¸ªå…³é”®æ”¹è¿›ï¼š1ï¼‰ç”¨äºä¸€è‡´æ€§è’¸é¦çš„åˆ†å¸ƒåŒ¹é…ï¼›2ï¼‰åŸºäºè’¸é¦éš¾åº¦çš„è‡ªé€‚åº”åŠ æƒæ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒIBCDåœ¨åŸºå‡†æ•°æ®é›†ä¸Šå®ç°å•æ¬¡ç”Ÿæˆæ­¥éª¤çš„æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¾ªç¯ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆCycleGANï¼‰æ¨åŠ¨äº†éé…å¯¹å›¾åƒåˆ°å›¾åƒè½¬æ¢çš„è¿›å±•ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹å’Œè–›å®šè°”æ¡¥æ–¹æ³•ç”±äºè¿­ä»£é‡‡æ ·çš„æ€§è´¨ï¼Œåœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„æ™®åŠå—åˆ°é™åˆ¶ã€‚</li>
<li>æå‡ºæ–°å‹æ¡†æ¶â€”â€”éšæ¡¥ä¸€è‡´æ€§è’¸é¦ï¼ˆIBCDï¼‰ï¼Œå®ç°æ— éœ€å¯¹æŠ—æ€§æŸå¤±çš„å•æ­¥åŒå‘éé…å¯¹ç¿»è¯‘ã€‚</li>
<li>IBCDé€šè¿‡æ‰©æ•£éšæ¡¥æ¨¡å‹è¿æ¥æ¦‚ç‡æµå¸¸å¾®åˆ†æ–¹ç¨‹è½¨è¿¹ï¼Œæ‰©å±•äº†ä¸€è‡´æ€§è’¸é¦çš„æ¦‚å¿µã€‚</li>
<li>IBCDå¼•å…¥åˆ†å¸ƒåŒ¹é…ç”¨äºä¸€è‡´æ€§è’¸é¦ã€‚</li>
<li>IBCDé‡‡ç”¨åŸºäºè’¸é¦éš¾åº¦çš„è‡ªé€‚åº”åŠ æƒæ–¹æ³•ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒIBCDåœ¨åŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†å•æ¬¡ç”Ÿæˆæ­¥éª¤çš„æœ€ä½³æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15056">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-90433ecf976f002d86b148258f2efa41.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cceec70611475697114a4a2d624eed1a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-63361ae4d82df26c967ea382caa9918d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5b8c90d55407b05a5b4e0ef9e64013e4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0eb598331204a4ea7f0237b8ad099bc8.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Exploiting-Diffusion-Prior-for-Real-World-Image-Dehazing-with-Unpaired-Training"><a href="#Exploiting-Diffusion-Prior-for-Real-World-Image-Dehazing-with-Unpaired-Training" class="headerlink" title="Exploiting Diffusion Prior for Real-World Image Dehazing with Unpaired   Training"></a>Exploiting Diffusion Prior for Real-World Image Dehazing with Unpaired   Training</h2><p><strong>Authors:Yunwei Lan, Zhigao Cui, Chang Liu, Jialun Peng, Nian Wang, Xin Luo, Dong Liu</strong></p>
<p>Unpaired training has been verified as one of the most effective paradigms for real scene dehazing by learning from unpaired real-world hazy and clear images. Although numerous studies have been proposed, current methods demonstrate limited generalization for various real scenes due to limited feature representation and insufficient use of real-world prior. Inspired by the strong generative capabilities of diffusion models in producing both hazy and clear images, we exploit diffusion prior for real-world image dehazing, and propose an unpaired framework named Diff-Dehazer. Specifically, we leverage diffusion prior as bijective mapping learners within the CycleGAN, a classic unpaired learning framework. Considering that physical priors contain pivotal statistics information of real-world data, we further excavate real-world knowledge by integrating physical priors into our framework. Furthermore, we introduce a new perspective for adequately leveraging the representation ability of diffusion models by removing degradation in image and text modalities, so as to improve the dehazing effect. Extensive experiments on multiple real-world datasets demonstrate the superior performance of our method. Our code <a target="_blank" rel="noopener" href="https://github.com/ywxjm/Diff-Dehazer">https://github.com/ywxjm/Diff-Dehazer</a>. </p>
<blockquote>
<p>æ— é…å¯¹è®­ç»ƒå·²é€šè¿‡ä»ä¸æˆå¯¹çš„çœŸå®ä¸–ç•Œé›¾éœ¾å’Œæ¸…æ™°å›¾åƒä¸­å­¦ä¹ éªŒè¯ä¸ºæœ€æœ‰æ•ˆçš„å®æ™¯å»é›¾èŒƒå¼ä¹‹ä¸€ã€‚å°½ç®¡å·²ç»æå‡ºäº†è®¸å¤šç ”ç©¶ï¼Œä½†å½“å‰çš„æ–¹æ³•ç”±äºç‰¹å¾è¡¨ç¤ºæœ‰é™å’ŒçœŸå®ä¸–ç•Œå…ˆéªŒçš„åˆ©ç”¨ä¸è¶³ï¼Œå¯¹å„ç§çœŸå®åœºæ™¯çš„æ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚å—æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé›¾éœ¾å’Œæ¸…æ™°å›¾åƒæ–¹é¢çš„å¼ºå¤§ç”Ÿæˆèƒ½åŠ›çš„å¯å‘ï¼Œæˆ‘ä»¬åˆ©ç”¨æ‰©æ•£å…ˆéªŒè¿›è¡ŒçœŸå®å›¾åƒå»é›¾ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ— é…å¯¹çš„æ¡†æ¶Diff-Dehazerã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åœ¨CycleGANï¼ˆä¸€ç§ç»å…¸çš„æ— é…å¯¹å­¦ä¹ æ¡†æ¶ï¼‰ä¸­åˆ©ç”¨æ‰©æ•£å…ˆéªŒä½œä¸ºåŒå‘æ˜ å°„å­¦ä¹ è€…ã€‚è€ƒè™‘åˆ°ç‰©ç†å…ˆéªŒåŒ…å«çœŸå®ä¸–ç•Œæ•°æ®çš„å…³é”®ç»Ÿè®¡ä¿¡æ¯ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥é€šè¿‡æ•´åˆç‰©ç†å…ˆéªŒåˆ°æˆ‘ä»¬çš„æ¡†æ¶ä¸­æ¥æŒ–æ˜çœŸå®ä¸–ç•Œçš„çŸ¥è¯†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡å»é™¤å›¾åƒå’Œæ–‡æœ¬æ¨¡æ€çš„é€€åŒ–ï¼Œä¸ºå……åˆ†åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„è¡¨ç¤ºèƒ½åŠ›å¼•å…¥äº†æ–°çš„è§†è§’ï¼Œä»¥æé«˜å»é›¾æ•ˆæœã€‚åœ¨å¤šä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚æˆ‘ä»¬çš„ä»£ç ä¸º <a target="_blank" rel="noopener" href="https://github.com/ywxjm/Diff-Dehazer%E3%80%82">https://github.com/ywxjm/Diff-Dehazerã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15017v1">PDF</a> Accepted by AAAI2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>åˆ©ç”¨æ— é…å¯¹è®­ç»ƒæ–¹å¼ï¼Œé€šè¿‡ä»ç°å®ä¸–ç•Œçš„é›¾éœ¾å’Œæ¸…æ™°å›¾åƒä¸­å­¦ä¹ ï¼Œå®ç°çœŸå®åœºæ™¯å»é›¾çš„æœ€æœ‰æ•ˆæ–¹æ³•ä¹‹ä¸€ã€‚è™½ç„¶å·²æœ‰è®¸å¤šç›¸å…³ç ”ç©¶ï¼Œä½†å—é™äºç‰¹å¾è¡¨è¾¾å’Œæœªèƒ½å……åˆ†åˆ©ç”¨ç°å®ä¸–ç•Œçš„å…ˆéªŒçŸ¥è¯†ï¼Œç°æœ‰æ–¹æ³•çš„æ³›åŒ–èƒ½åŠ›å—é™ã€‚å—åˆ°æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé›¾éœ¾å’Œæ¸…æ™°å›¾åƒæ–¹é¢çš„å¼ºå¤§èƒ½åŠ›çš„å¯å‘ï¼Œæœ¬æ–‡åˆ©ç”¨æ‰©æ•£å…ˆéªŒè¿›è¡ŒçœŸå®å›¾åƒå»é›¾ï¼Œæå‡ºäº†ä¸€ç§åä¸ºDiff-Dehazerçš„æ— é…å¯¹æ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åœ¨ç»å…¸çš„CycleGANæ— é…å¯¹å­¦ä¹ æ¡†æ¶ä¸­å¼•å…¥æ‰©æ•£å…ˆéªŒä½œä¸ºåŒå‘æ˜ å°„å­¦ä¹ è€…ã€‚è€ƒè™‘åˆ°ç‰©ç†å…ˆéªŒåŒ…å«ç°å®ä¸–ç•Œæ•°æ®çš„å…³é”®ç»Ÿè®¡ä¿¡æ¯ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥é€šè¿‡æ•´åˆç‰©ç†å…ˆéªŒæ¥æŒ–æ˜ç°å®ä¸–ç•ŒçŸ¥è¯†ã€‚æ­¤å¤–ï¼Œé€šè¿‡å»é™¤å›¾åƒå’Œæ–‡æœ¬æ¨¡æ€çš„é€€åŒ–ï¼Œå¼•å…¥äº†å……åˆ†åˆ©ç”¨æ‰©æ•£æ¨¡å‹è¡¨è¾¾èƒ½åŠ›çš„æ–°è§†è§’ï¼Œä»¥æé«˜å»é›¾æ•ˆæœã€‚åœ¨å¤šä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•å…·æœ‰å“è¶Šæ€§èƒ½ã€‚ä»£ç é“¾æ¥ï¼š<a target="_blank" rel="noopener" href="https://github.com/ywxjm/Diff-Dehazer">https://github.com/ywxjm/Diff-Dehazer</a>ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ul>
<li>æ— é…å¯¹è®­ç»ƒå·²è¢«éªŒè¯ä¸ºä»ç°å®ä¸–ç•Œçš„é›¾éœ¾å’Œæ¸…æ™°å›¾åƒä¸­å­¦ä¹ å»é›¾æœ€æœ‰æ•ˆçš„èŒƒå¼ä¹‹ä¸€ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹å› å…¶å¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›è¢«ç”¨äºå»é›¾ä»»åŠ¡ä¸­ï¼Œæå‡ºäº†åˆ©ç”¨æ‰©æ•£å…ˆéªŒçš„Diff-Dehazeræ¡†æ¶ã€‚</li>
<li>åœ¨ç»å…¸çš„æ— é…å¯¹å­¦ä¹ æ¡†æ¶CycleGANä¸­å¼•å…¥æ‰©æ•£å…ˆéªŒä½œä¸ºåŒå‘æ˜ å°„å­¦ä¹ è€…ã€‚</li>
<li>é€šè¿‡æ•´åˆç‰©ç†å…ˆéªŒæŒ–æ˜ç°å®ä¸–ç•ŒçŸ¥è¯†ï¼Œæé«˜å»é›¾æ•ˆæœã€‚</li>
<li>å¼•å…¥æ–°çš„è§†è§’ï¼Œé€šè¿‡å»é™¤å›¾åƒå’Œæ–‡æœ¬æ¨¡æ€çš„é€€åŒ–æ¥å……åˆ†åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15017">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9f7a11e82c401a085378b40145b8cf9e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c93a92167929754bfbbe6ea9c69b07b4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-95988624282b4f6c1afffcdce49097b9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-da33ea6241328a4c20bb60df485462d0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1d117ad5760bc24b2381e14633f6b11b.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Language-based-Image-Colorization-A-Benchmark-and-Beyond"><a href="#Language-based-Image-Colorization-A-Benchmark-and-Beyond" class="headerlink" title="Language-based Image Colorization: A Benchmark and Beyond"></a>Language-based Image Colorization: A Benchmark and Beyond</h2><p><strong>Authors:Yifan Li, Shuai Yang, Jiaying Liu</strong></p>
<p>Image colorization aims to bring colors back to grayscale images. Automatic image colorization methods, which requires no additional guidance, struggle to generate high-quality images due to color ambiguity, and provides limited user controllability. Thanks to the emergency of cross-modality datasets and models, language-based colorization methods are proposed to fully utilize the efficiency and flexibly of text descriptions to guide colorization. In view of the lack of a comprehensive review of language-based colorization literature, we conduct a thorough analysis and benchmarking. We first briefly summarize existing automatic colorization methods. Then, we focus on language-based methods and point out their core challenge on cross-modal alignment. We further divide these methods into two categories: one attempts to train a cross-modality network from scratch, while the other utilizes the pre-trained cross-modality model to establish the textual-visual correspondence. Based on the analyzed limitations of existing language-based methods, we propose a simple yet effective method based on distilled diffusion model. Extensive experiments demonstrate that our simple baseline can produces better results than previous complex methods with 14 times speed up. To the best of our knowledge, this is the first comprehensive review and benchmark on language-based image colorization field, providing meaningful insights for the community. The code is available at <a target="_blank" rel="noopener" href="https://github.com/lyf1212/Color-Turbo">https://github.com/lyf1212/Color-Turbo</a>. </p>
<blockquote>
<p>å›¾åƒå½©è‰²åŒ–çš„ç›®æ ‡æ˜¯ç»™é»‘ç™½å›¾åƒæ¢å¤è‰²å½©ã€‚è‡ªåŠ¨å›¾åƒå½©è‰²åŒ–æ–¹æ³•æ— éœ€é¢å¤–æŒ‡å¯¼ï¼Œä½†ç”±äºè‰²å½©æ¨¡ç³Šåº¦ï¼Œå…¶åœ¨ç”Ÿæˆé«˜è´¨é‡å›¾åƒæ–¹é¢å­˜åœ¨å›°éš¾ï¼Œä¸”ç”¨æˆ·å¯æ§æ€§æœ‰é™ã€‚å¾—ç›Šäºè·¨æ¨¡æ€æ•°æ®é›†å’Œæ¨¡å‹çš„å…´èµ·ï¼ŒåŸºäºè¯­è¨€çš„å½©è‰²åŒ–æ–¹æ³•è¢«æå‡ºï¼Œä»¥å……åˆ†åˆ©ç”¨æ–‡æœ¬æè¿°çš„æ•ˆç‡å’Œçµæ´»æ€§æ¥æŒ‡å¯¼å½©è‰²åŒ–ã€‚é‰´äºç¼ºä¹åŸºäºè¯­è¨€çš„å½©è‰²åŒ–æ–‡çŒ®çš„å…¨é¢ç»¼è¿°ï¼Œæˆ‘ä»¬è¿›è¡Œäº†æ·±å…¥åˆ†æå’ŒåŸºå‡†æµ‹è¯•ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ç®€è¦æ€»ç»“äº†ç°æœ‰çš„è‡ªåŠ¨å½©è‰²åŒ–æ–¹æ³•ã€‚ç„¶åï¼Œæˆ‘ä»¬é‡ç‚¹å…³æ³¨åŸºäºè¯­è¨€çš„æ–¹æ³•ï¼Œå¹¶æŒ‡å‡ºäº†å®ƒä»¬åœ¨è·¨æ¨¡æ€å¯¹é½æ–¹é¢çš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å°†è¿™äº›æ–¹æ³•åˆ†ä¸ºä¸¤ç±»ï¼šä¸€ç±»è¯•å›¾ä»å¤´å¼€å§‹è®­ç»ƒä¸€ä¸ªè·¨æ¨¡æ€ç½‘ç»œï¼Œå¦ä¸€ç±»åˆ™åˆ©ç”¨é¢„è®­ç»ƒçš„è·¨æ¨¡æ€æ¨¡å‹æ¥å»ºç«‹æ–‡æœ¬-è§†è§‰å¯¹åº”å…³ç³»ã€‚åŸºäºå¯¹ç°æœ‰åŸºäºè¯­è¨€çš„æ–¹æ³•çš„åˆ†æå’Œå±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åŸºäºè’¸é¦æ‰©æ•£æ¨¡å‹ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬ç®€å•çš„åŸºçº¿æ–¹æ³•èƒ½å¤Ÿåœ¨æ¯”ä»¥å‰çš„å¤æ‚æ–¹æ³•å¿«14å€çš„æƒ…å†µä¸‹äº§ç”Ÿæ›´å¥½çš„ç»“æœã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯è¯­è¨€é©±åŠ¨å›¾åƒå½©è‰²åŒ–é¢†åŸŸçš„é¦–æ¬¡å…¨é¢ç»¼è¿°å’ŒåŸºå‡†æµ‹è¯•ï¼Œä¸ºç¤¾åŒºæä¾›äº†æœ‰æ„ä¹‰çš„è§è§£ã€‚ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/lyf1212/Color-Turbo%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/lyf1212/Color-Turboè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14974v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å›¾åƒå½©è‰²åŒ–çš„ç›®æ ‡å’Œæ–¹æ³•ã€‚è‡ªåŠ¨å›¾åƒå½©è‰²åŒ–æ–¹æ³•å› è‰²å½©æ¨¡ç³Šå’Œç”¨æˆ·æ§åˆ¶æœ‰é™è€Œéš¾ä»¥ç”Ÿæˆé«˜è´¨é‡å›¾åƒã€‚è¿‘å¹´æ¥ï¼ŒåŸºäºè¯­è¨€æè¿°çš„å½©è‰²åŒ–æ–¹æ³•å› å…¶æ•ˆç‡å’Œçµæ´»æ€§è€Œå—åˆ°å…³æ³¨ã€‚æœ¬æ–‡å¯¹åŸºäºè¯­è¨€çš„å½©è‰²åŒ–æ–‡çŒ®è¿›è¡Œäº†å…¨é¢ç»¼è¿°å’ŒåŸºå‡†æµ‹è¯•ï¼Œæ€»ç»“äº†ç°æœ‰è‡ªåŠ¨å½©è‰²åŒ–æ–¹æ³•çš„ä¸è¶³ï¼ŒæŒ‡å‡ºäº†åŸºäºè¯­è¨€çš„æ–¹æ³•åœ¨è·¨æ¨¡æ€å¯¹é½æ–¹é¢çš„æ ¸å¿ƒæŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†åŸºäºè’¸é¦æ‰©æ•£æ¨¡å‹çš„ç®€å•æœ‰æ•ˆæ–¹æ³•ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥ç®€å•åŸºçº¿æ–¹æ³•æ¯”ä»¥å‰çš„å¤æ‚æ–¹æ³•æ›´å¥½ï¼Œé€Ÿåº¦æé«˜äº†14å€ã€‚æœ¬æ–‡æ˜¯å¯¹åŸºäºè¯­è¨€çš„å›¾åƒå½©è‰²åŒ–é¢†åŸŸçš„é¦–æ¬¡å…¨é¢å®¡æŸ¥å’ŒåŸºå‡†æµ‹è¯•ï¼Œä¸ºç¤¾åŒºæä¾›äº†æœ‰æ„ä¹‰çš„è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒå½©è‰²åŒ–çš„ç›®æ ‡æ˜¯æ¢å¤ç°åº¦å›¾åƒçš„è‰²å½©ã€‚</li>
<li>è‡ªåŠ¨å›¾åƒå½©è‰²åŒ–æ–¹æ³•å› è‰²å½©æ¨¡ç³Šå’Œç”¨æˆ·æ§åˆ¶æœ‰é™è€Œé¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>åŸºäºè¯­è¨€æè¿°çš„å½©è‰²åŒ–æ–¹æ³•åˆ©ç”¨æ–‡æœ¬æè¿°æ¥æé«˜æ•ˆç‡å’Œçµæ´»æ€§ã€‚</li>
<li>æœ¬æ–‡å¯¹åŸºäºè¯­è¨€çš„å›¾åƒå½©è‰²åŒ–æ–¹æ³•è¿›è¡Œäº†å…¨é¢ç»¼è¿°å’ŒåŸºå‡†æµ‹è¯•ã€‚</li>
<li>ç°æœ‰è‡ªåŠ¨å½©è‰²åŒ–æ–¹æ³•å­˜åœ¨å±€é™æ€§ï¼Œéœ€è¦æ–°çš„æ–¹æ³•æ”¹è¿›ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè’¸é¦æ‰©æ•£æ¨¡å‹çš„ç®€å•æœ‰æ•ˆå›¾åƒå½©è‰²åŒ–æ–¹æ³•ï¼Œå®éªŒè¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14974">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c52aae32e5528349fdf76495c7da5d43.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2a68120e45d82b9881ac5a6fa9f8cd0b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-72e6a6b7fe48603fdfae212f8795f802.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fc2f31dc2288185d1583c8880ea43b65.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d947b614d6647c1d23b47d0b9a59654.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Ultrasound-Image-to-Video-Synthesis-via-Latent-Dynamic-Diffusion-Models"><a href="#Ultrasound-Image-to-Video-Synthesis-via-Latent-Dynamic-Diffusion-Models" class="headerlink" title="Ultrasound Image-to-Video Synthesis via Latent Dynamic Diffusion Models"></a>Ultrasound Image-to-Video Synthesis via Latent Dynamic Diffusion Models</h2><p><strong>Authors:Tingxiu Chen, Yilei Shi, Zixuan Zheng, Bingcong Yan, Jingliang Hu, Xiao Xiang Zhu, Lichao Mou</strong></p>
<p>Ultrasound video classification enables automated diagnosis and has emerged as an important research area. However, publicly available ultrasound video datasets remain scarce, hindering progress in developing effective video classification models. We propose addressing this shortage by synthesizing plausible ultrasound videos from readily available, abundant ultrasound images. To this end, we introduce a latent dynamic diffusion model (LDDM) to efficiently translate static images to dynamic sequences with realistic video characteristics. We demonstrate strong quantitative results and visually appealing synthesized videos on the BUSV benchmark. Notably, training video classification models on combinations of real and LDDM-synthesized videos substantially improves performance over using real data alone, indicating our method successfully emulates dynamics critical for discrimination. Our image-to-video approach provides an effective data augmentation solution to advance ultrasound video analysis. Code is available at <a target="_blank" rel="noopener" href="https://github.com/MedAITech/U_I2V">https://github.com/MedAITech/U_I2V</a>. </p>
<blockquote>
<p>è¶…å£°æ³¢è§†é¢‘åˆ†ç±»å¯å®ç°è‡ªåŠ¨åŒ–è¯Šæ–­ï¼Œå·²æˆä¸ºé‡è¦çš„ç ”ç©¶é¢†åŸŸã€‚ç„¶è€Œï¼Œå¯ç”¨çš„è¶…å£°æ³¢è§†é¢‘æ•°æ®é›†ä»ç„¶ç¨€ç¼ºï¼Œé˜»ç¢äº†æœ‰æ•ˆè§†é¢‘åˆ†ç±»æ¨¡å‹çš„å¼€å‘è¿›å±•ã€‚æˆ‘ä»¬æå‡ºé€šè¿‡åˆæˆåˆç†çš„è¶…å£°æ³¢è§†é¢‘æ¥è§£å†³è¿™ä¸€çŸ­ç¼ºé—®é¢˜ï¼Œè¿™äº›è§†é¢‘ç”±æ˜“äºè·å–ä¸”ä¸°å¯Œçš„è¶…å£°æ³¢å›¾åƒåˆæˆè€Œæ¥ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ½œåœ¨åŠ¨æ€æ‰©æ•£æ¨¡å‹ï¼ˆLatent Dynamic Diffusion Model, LDDMï¼‰ï¼Œä»¥é«˜æ•ˆåœ°å°†é™æ€å›¾åƒè½¬æ¢ä¸ºå…·æœ‰çœŸå®è§†é¢‘ç‰¹æ€§çš„åŠ¨æ€åºåˆ—ã€‚æˆ‘ä»¬åœ¨BUSVåŸºå‡†æµ‹è¯•ä¸Šå±•ç¤ºäº†å¼ºå¤§çš„å®šé‡ç»“æœå’Œè§†è§‰ä¸Šä»¤äººæ»¡æ„çš„åˆæˆè§†é¢‘ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä½¿ç”¨çœŸå®å’ŒLDDMåˆæˆè§†é¢‘ç»„åˆè®­ç»ƒçš„è§†é¢‘åˆ†ç±»æ¨¡å‹ï¼Œå…¶æ€§èƒ½è¿œè¶…ä»…ä½¿ç”¨çœŸå®æ•°æ®çš„æƒ…å†µï¼Œè¿™è¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•æˆåŠŸæ¨¡æ‹Ÿäº†ç”¨äºé‰´åˆ«çš„å…³é”®åŠ¨æ€ã€‚æˆ‘ä»¬çš„å›¾åƒåˆ°è§†é¢‘çš„æ–¹æ³•ä¸ºæ¨è¿›è¶…å£°æ³¢è§†é¢‘åˆ†ææä¾›äº†æœ‰æ•ˆçš„æ•°æ®å¢å¼ºè§£å†³æ–¹æ¡ˆã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/MedAITech/U_I2V">https://github.com/MedAITech/U_I2V</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14966v1">PDF</a> MICCAI 2024</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ‘˜è¦æå‡ºé€šè¿‡åˆ©ç”¨é™æ€å›¾åƒç”ŸæˆåŠ¨æ€åºåˆ—æ¥è§£å†³è¶…å£°è§†é¢‘æ•°æ®é›†çŸ­ç¼ºçš„é—®é¢˜ã€‚ç ”ç©¶å›¢é˜Ÿå¼•å…¥äº†ä¸€ç§æ½œåœ¨åŠ¨æ€æ‰©æ•£æ¨¡å‹ï¼ˆLDDMï¼‰ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿå°†ç°æœ‰çš„å¤§é‡è¶…å£°å›¾åƒè½¬åŒ–ä¸ºé€¼çœŸçš„è§†é¢‘ç‰¹æ€§åºåˆ—ã€‚åœ¨BUSVåŸºå‡†æµ‹è¯•ä¸Šï¼Œåˆæˆçš„è§†é¢‘æ—¢å…·æœ‰å¼ºå¤§çš„å®šé‡ç»“æœåˆå…·æœ‰è‰¯å¥½çš„è§†è§‰æ•ˆæœã€‚æ­¤å¤–ï¼Œåœ¨çœŸå®è§†é¢‘ä¸åˆæˆçš„è§†é¢‘ç»“åˆè®­ç»ƒåˆ†ç±»æ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œæ€§èƒ½å¾—åˆ°äº†æ˜¾è‘—æå‡ï¼Œè¯æ˜åˆæˆè§†é¢‘å¯¹äºåˆ¤åˆ«èƒ½åŠ›å…·æœ‰å…³é”®ä½œç”¨ã€‚è¯¥ç ”ç©¶æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æ•°æ®å¢å¼ºè§£å†³æ–¹æ¡ˆï¼Œæ¨åŠ¨äº†è¶…å£°è§†é¢‘åˆ†æçš„å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¶…å£°è§†é¢‘åˆ†ç±»å¯¹äºè‡ªåŠ¨åŒ–è¯Šæ–­è‡³å…³é‡è¦ï¼Œä½†å¯ç”¨çš„è¶…å£°è§†é¢‘æ•°æ®é›†æœ‰é™ã€‚</li>
<li>æå‡ºé€šè¿‡æ½œåœ¨åŠ¨æ€æ‰©æ•£æ¨¡å‹ï¼ˆLDDMï¼‰å°†é™æ€è¶…å£°å›¾åƒè½¬åŒ–ä¸ºåŠ¨æ€è§†é¢‘åºåˆ—çš„æ–¹æ³•ã€‚</li>
<li>LDDMåˆæˆçš„è§†é¢‘åœ¨BUSVåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„å®šé‡ç»“æœå’Œè‰¯å¥½çš„è§†è§‰æ•ˆæœã€‚</li>
<li>ç»“åˆçœŸå®å’Œåˆæˆè§†é¢‘è®­ç»ƒåˆ†ç±»æ¨¡å‹èƒ½æ˜¾è‘—æé«˜æ€§èƒ½ã€‚</li>
<li>åˆæˆè§†é¢‘å…·æœ‰å…³é”®çš„åˆ¤åˆ«èƒ½åŠ›ï¼Œå¯æ¨¡æ‹Ÿé‡è¦çš„åŠ¨æ€ç‰¹æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æ•°æ®å¢å¼ºè§£å†³æ–¹æ¡ˆï¼Œæœ‰åŠ©äºæ¨åŠ¨è¶…å£°è§†é¢‘åˆ†æçš„å‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14966">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8e49b6aef94727d2e5c3afa2b31df8fa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c896922487736d835240b97ed3bc02fa.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a1cc7baf7a6c1fe7bd6d904284b499aa.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Efficient-Personalization-of-Quantized-Diffusion-Model-without-Backpropagation"><a href="#Efficient-Personalization-of-Quantized-Diffusion-Model-without-Backpropagation" class="headerlink" title="Efficient Personalization of Quantized Diffusion Model without   Backpropagation"></a>Efficient Personalization of Quantized Diffusion Model without   Backpropagation</h2><p><strong>Authors:Hoigi Seo, Wongi Jeong, Kyungryeol Lee, Se Young Chun</strong></p>
<p>Diffusion models have shown remarkable performance in image synthesis, but they demand extensive computational and memory resources for training, fine-tuning and inference. Although advanced quantization techniques have successfully minimized memory usage for inference, training and fine-tuning these quantized models still require large memory possibly due to dequantization for accurate computation of gradients and&#x2F;or backpropagation for gradient-based algorithms. However, memory-efficient fine-tuning is particularly desirable for applications such as personalization that often must be run on edge devices like mobile phones with private data. In this work, we address this challenge by quantizing a diffusion model with personalization via Textual Inversion and by leveraging a zeroth-order optimization on personalization tokens without dequantization so that it does not require gradient and activation storage for backpropagation that consumes considerable memory. Since a gradient estimation using zeroth-order optimization is quite noisy for a single or a few images in personalization, we propose to denoise the estimated gradient by projecting it onto a subspace that is constructed with the past history of the tokens, dubbed Subspace Gradient. In addition, we investigated the influence of text embedding in image generation, leading to our proposed time steps sampling, dubbed Partial Uniform Timestep Sampling for sampling with effective diffusion timesteps. Our method achieves comparable performance to prior methods in image and text alignment scores for personalizing Stable Diffusion with only forward passes while reducing training memory demand up to $8.2\times$. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒåˆæˆä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä½†å®ƒä»¬å¯¹äºè®­ç»ƒã€å¾®è°ƒä»¥åŠæ¨ç†éœ€è¦å¤§é‡çš„è®¡ç®—å’Œå†…å­˜èµ„æºã€‚è™½ç„¶å…ˆè¿›çš„é‡åŒ–æŠ€æœ¯å·²æˆåŠŸæœ€å°åŒ–äº†æ¨ç†è¿‡ç¨‹ä¸­çš„å†…å­˜ä½¿ç”¨ï¼Œä½†å¯¹è¿™äº›é‡åŒ–æ¨¡å‹è¿›è¡Œè®­ç»ƒå’Œå¾®è°ƒä»ç„¶éœ€è¦å¤§å†…å­˜ï¼Œè¿™å¯èƒ½æ˜¯ç”±äºä¸ºäº†å‡†ç¡®è®¡ç®—æ¢¯åº¦å’ŒåŸºäºæ¢¯åº¦çš„ç®—æ³•è¿›è¡Œåå‘ä¼ æ’­è€Œéœ€è¦è¿›è¡Œåé‡åŒ–ã€‚ç„¶è€Œï¼Œå¯¹äºå¿…é¡»åœ¨è¾¹ç¼˜è®¾å¤‡ï¼ˆå¦‚å¸¦æœ‰ç§äººæ•°æ®çš„ç§»åŠ¨ç”µè¯ï¼‰ä¸Šè¿è¡Œçš„åº”ç”¨ç¨‹åºè€Œè¨€ï¼Œå¯¹å†…å­˜æ•ˆç‡é«˜çš„å¾®è°ƒæœ‰ç‰¹åˆ«éœ€æ±‚ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡ä½¿ç”¨æ–‡æœ¬åè½¬å¯¹æ‰©æ•£æ¨¡å‹è¿›è¡Œé‡åŒ–ï¼Œå¹¶åˆ©ç”¨é›¶é˜¶ä¼˜åŒ–å¯¹ä¸ªæ€§åŒ–ä»¤ç‰Œè¿›è¡Œä¼˜åŒ–ï¼Œè§£å†³äº†è¿™ä¸€æŒ‘æˆ˜ï¼Œæ— éœ€åé‡åŒ–å³å¯åœ¨ä¸è¦æ±‚æ¢¯åº¦å’Œåå‘ä¼ æ’­æ¿€æ´»å­˜å‚¨ï¼ˆè¿™ä¼šæ¶ˆè€—å¤§é‡å†…å­˜ï¼‰çš„æƒ…å†µä¸‹è¿›è¡Œã€‚ç”±äºä½¿ç”¨é›¶é˜¶ä¼˜åŒ–å¯¹æ¢¯åº¦ä¼°è®¡åœ¨ä¸ªæ€§åŒ–ä¸­çš„å•ä¸ªæˆ–å°‘æ•°å›¾åƒæ˜¯ç›¸å½“å˜ˆæ‚çš„ï¼Œæˆ‘ä»¬æå‡ºé€šè¿‡æŠ•å½±åˆ°ç”±è¿‡å»ä»¤ç‰Œå†å²æ„å»ºçš„å­ç©ºé—´æ¥å¯¹ä¼°è®¡çš„æ¢¯åº¦è¿›è¡Œå»å™ªï¼Œè¢«ç§°ä¸ºå­ç©ºé—´æ¢¯åº¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç ”ç©¶äº†æ–‡æœ¬åµŒå…¥åœ¨å›¾åƒç”Ÿæˆä¸­çš„å½±å“ï¼Œå¯¼è‡´æˆ‘ä»¬æå‡ºæ—¶é—´æ­¥é•¿é‡‡æ ·æ–¹æ³•ï¼Œè¢«ç§°ä¸ºéƒ¨åˆ†å‡åŒ€æ—¶é—´æ­¥é•¿é‡‡æ ·ï¼Œç”¨äºæœ‰æ•ˆçš„æ‰©æ•£æ—¶é—´æ­¥é•¿é‡‡æ ·ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å›¾åƒå’Œæ–‡æœ¬å¯¹é½å¾—åˆ†æ–¹é¢å®ç°äº†ä¸å…ˆå‰æ–¹æ³•ç›¸å½“çš„æ€§èƒ½ï¼Œå¯ä»¥åœ¨ä¸ªæ€§åŒ–Stable Diffusionçš„åŒæ—¶ä»…é€šè¿‡å‰å‘ä¼ é€’å‡å°‘è®­ç»ƒå†…å­˜éœ€æ±‚é«˜è¾¾8.2å€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14868v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒåˆæˆæ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä½†å…¶åœ¨è®­ç»ƒã€å¾®è°ƒåŠæ¨ç†è¿‡ç¨‹ä¸­éœ€è¦å¤§é‡çš„è®¡ç®—å’Œå†…å­˜èµ„æºã€‚å°½ç®¡å…ˆè¿›çš„é‡åŒ–æŠ€æœ¯å·²æˆåŠŸå‡å°‘äº†æ¨ç†è¿‡ç¨‹ä¸­çš„å†…å­˜ä½¿ç”¨ï¼Œä½†é‡åŒ–æ¨¡å‹çš„è®­ç»ƒå’Œå¾®è°ƒä»ç„¶éœ€è¦è¾ƒå¤§çš„å†…å­˜ï¼Œè¿™å¯èƒ½æ˜¯ç”±äºä¸ºäº†å‡†ç¡®è®¡ç®—æ¢¯åº¦å’Œè¿›è¡Œåå‘ä¼ æ’­è€Œéœ€è¦è¿›è¡Œåé‡åŒ–æ‰€è‡´ã€‚é’ˆå¯¹ä¸ªæ€§åŒ–åº”ç”¨ç­‰éœ€è¦åœ¨è¾¹ç¼˜è®¾å¤‡ï¼ˆå¦‚æ‰‹æœºï¼‰ä¸Šè¿è¡Œå¹¶ä½¿ç”¨ç§æœ‰æ•°æ®çš„åº”ç”¨ç¨‹åºï¼Œå†…å­˜é«˜æ•ˆçš„å¾®è°ƒå°¤ä¸ºå…³é”®ã€‚æœ¬ç ”ç©¶é€šè¿‡ç»“åˆæ–‡æœ¬åè½¬å¯¹æ‰©æ•£æ¨¡å‹è¿›è¡Œé‡åŒ–ï¼Œå¹¶åˆ©ç”¨é›¶é˜¶ä¼˜åŒ–å¯¹ä¸ªäººåŒ–ä»¤ç‰Œè¿›è¡Œä¼˜åŒ–ï¼Œæ— éœ€åé‡åŒ–å³å¯æ»¡è¶³è¿™ä¸€éœ€æ±‚ï¼Œä»è€Œé¿å…äº†æ¢¯åº¦è®¡ç®—å’Œåå‘ä¼ æ’­è¿‡ç¨‹ä¸­å¤§é‡å†…å­˜çš„æ¶ˆè€—ã€‚é’ˆå¯¹ä¸ªæ€§åŒ–ä¸­å•ä¸€æˆ–å°‘é‡å›¾åƒçš„æ¢¯åº¦ä¼°è®¡å™ªå£°è¾ƒå¤§ï¼Œæœ¬ç ”ç©¶æå‡ºäº†åŸºäºè¿‡å»ä»¤ç‰Œå†å²çš„å­ç©ºé—´æ¢¯åº¦å»å™ªæ–¹æ³•ã€‚æ­¤å¤–ï¼Œæœ¬ç ”ç©¶è¿˜æ¢è®¨äº†æ–‡æœ¬åµŒå…¥åœ¨å›¾åƒç”Ÿæˆä¸­çš„å½±å“ï¼Œå¹¶æ®æ­¤æå‡ºäº†éƒ¨åˆ†å‡åŒ€æ—¶é—´æ­¥é•¿é‡‡æ ·æ–¹æ³•ã€‚è¯¥æ–¹æ³•åœ¨ä¸ªæ€§åŒ–Stable Diffusionçš„å›¾åƒå’Œæ–‡æœ¬å¯¹é½å¾—åˆ†æ–¹é¢å–å¾—äº†ä¸ä»¥å¾€æ–¹æ³•ç›¸å½“çš„æ•ˆæœï¼ŒåŒæ—¶ä»…é€šè¿‡å‰å‘ä¼ é€’å°±é™ä½äº†é«˜è¾¾8.2å€çš„è®­ç»ƒå†…å­˜éœ€æ±‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒåˆæˆæ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†éœ€è¦å¤§é‡çš„è®¡ç®—å’Œå†…å­˜èµ„æºã€‚</li>
<li>å…ˆè¿›é‡åŒ–æŠ€æœ¯è™½èƒ½å‡å°‘æ¨ç†è¿‡ç¨‹ä¸­çš„å†…å­˜ä½¿ç”¨ï¼Œä½†è®­ç»ƒå’Œå¾®è°ƒæ—¶ä»éœ€å¤§é‡å†…å­˜ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§ç»“åˆæ–‡æœ¬åè½¬å’Œé›¶é˜¶ä¼˜åŒ–çš„æ–¹æ³•ï¼Œè¿›è¡Œæ‰©æ•£æ¨¡å‹çš„é‡åŒ–åŠä¸ªæ€§åŒ–ï¼Œé™ä½å†…å­˜æ¶ˆè€—ã€‚</li>
<li>é’ˆå¯¹ä¸ªæ€§åŒ–åº”ç”¨ä¸­æ¢¯åº¦ä¼°è®¡çš„å™ªå£°é—®é¢˜ï¼Œæå‡ºäº†å­ç©ºé—´æ¢¯åº¦å»å™ªæ–¹æ³•ã€‚</li>
<li>ç ”ç©¶å‘ç°æ–‡æœ¬åµŒå…¥åœ¨å›¾åƒç”Ÿæˆä¸­å…·æœ‰é‡è¦å½±å“ï¼Œå¹¶æ®æ­¤æå‡ºäº†éƒ¨åˆ†å‡åŒ€æ—¶é—´æ­¥é•¿é‡‡æ ·æ–¹æ³•ã€‚</li>
<li>æ–¹æ³•åœ¨å›¾åƒå’Œæ–‡æœ¬å¯¹é½å¾—åˆ†æ–¹é¢ä¸ä»¥å¾€æ–¹æ³•ç›¸å½“ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½äº†è®­ç»ƒå†…å­˜éœ€æ±‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14868">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-4f4c9903ad7038d53b8e91e4a140a4d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb9191eb59727da71f7bd967b3b21515.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae03172599c25ec2bba305b20f8bcf15.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cbce0cbede4c8d6e3f49bc87c0acd7c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-91f21ced883ed4956a14b173b548e09a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a08ba91527fc01535a1086e2cd5a2175.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-32d03845a2716518246e87546c2fe2a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-60087318efb90dd2aaed282e1bf759ef.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="A-Simple-Combination-of-Diffusion-Models-for-Better-Quality-Trade-Offs-in-Image-Denoising"><a href="#A-Simple-Combination-of-Diffusion-Models-for-Better-Quality-Trade-Offs-in-Image-Denoising" class="headerlink" title="A Simple Combination of Diffusion Models for Better Quality Trade-Offs   in Image Denoising"></a>A Simple Combination of Diffusion Models for Better Quality Trade-Offs   in Image Denoising</h2><p><strong>Authors:Jonas Dornbusch, Emanuel Pfarr, Florin-Alexandru Vasluianu, Frank Werner, Radu Timofte</strong></p>
<p>Diffusion models have garnered considerable interest in computer vision, owing both to their capacity to synthesize photorealistic images and to their proven effectiveness in image reconstruction tasks. However, existing approaches fail to efficiently balance the high visual quality of diffusion models with the low distortion achieved by previous image reconstruction methods. Specifically, for the fundamental task of additive Gaussian noise removal, we first illustrate an intuitive method for leveraging pretrained diffusion models. Further, we introduce our proposed Linear Combination Diffusion Denoiser (LCDD), which unifies two complementary inference procedures - one that leverages the modelâ€™s generative potential and another that ensures faithful signal recovery. By exploiting the inherent structure of the denoising samples, LCDD achieves state-of-the-art performance and offers controlled, well-behaved trade-offs through a simple scalar hyperparameter adjustment. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸå¼•èµ·äº†æå¤§çš„å…³æ³¨ï¼Œè¿™æ—¢æ˜¯å› ä¸ºå®ƒä»¬èƒ½å¤Ÿåˆæˆé€¼çœŸçš„å›¾åƒï¼Œä¹Ÿæ˜¯å› ä¸ºå®ƒä»¬åœ¨å›¾åƒé‡å»ºä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§å¾—åˆ°äº†éªŒè¯ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ–¹æ³•æœªèƒ½æœ‰æ•ˆåœ°å¹³è¡¡æ‰©æ•£æ¨¡å‹çš„é«˜è§†è§‰è´¨é‡ä¸å…ˆå‰å›¾åƒé‡å»ºæ–¹æ³•å®ç°çš„ä½å¤±çœŸã€‚å…·ä½“æ¥è¯´ï¼Œå¯¹äºåŸºæœ¬çš„æ·»åŠ é«˜æ–¯å™ªå£°å»é™¤ä»»åŠ¡ï¼Œæˆ‘ä»¬é¦–å…ˆæå‡ºäº†ä¸€ç§åˆ©ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„ç›´è§‚æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ‰€æå‡ºçš„çº¿æ€§ç»„åˆæ‰©æ•£å»å™ªå™¨ï¼ˆLCDDï¼‰ï¼Œå®ƒç»Ÿä¸€äº†ä¸¤ç§äº’è¡¥çš„æ¨ç†ç¨‹åºâ€”â€”ä¸€ç§åˆ©ç”¨æ¨¡å‹çš„ç”Ÿæˆæ½œåŠ›ï¼Œå¦ä¸€ç§ç¡®ä¿å¿ å®ä¿¡å·æ¢å¤ã€‚é€šè¿‡åˆ©ç”¨å»å™ªæ ·æœ¬çš„å›ºæœ‰ç»“æ„ï¼ŒLCDDå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶é€šè¿‡ç®€å•çš„æ ‡é‡è¶…å‚æ•°è°ƒæ•´æä¾›äº†å¯æ§ä¸”è¡¨ç°è‰¯å¥½çš„æƒè¡¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14654v1">PDF</a> 10 pages, 7 figures, 2 tables</p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸå—åˆ°å¹¿æ³›å…³æ³¨ï¼Œæ—¢èƒ½å¤Ÿåˆæˆé€¼çœŸçš„å›¾åƒï¼Œä¹Ÿåœ¨å›¾åƒé‡å»ºä»»åŠ¡ä¸­å±•ç°å‡ºæ•ˆæœã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥åœ¨æ‰©æ•£æ¨¡å‹çš„é«˜è§†è§‰è´¨é‡ä¸å…ˆå‰å›¾åƒé‡å»ºæ–¹æ³•å®ç°çš„ä½å¤±çœŸä¹‹é—´å–å¾—æœ‰æ•ˆå¹³è¡¡ã€‚é’ˆå¯¹åŠ æ€§é«˜æ–¯å™ªå£°å»é™¤è¿™ä¸€åŸºæœ¬ä»»åŠ¡ï¼Œæœ¬æ–‡é¦–å…ˆæå‡ºäº†ä¸€ç§åˆ©ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„ç›´è§‚æ–¹æ³•ã€‚æ­¤å¤–ï¼Œè¿˜ä»‹ç»äº†æå‡ºçš„çº¿æ€§ç»„åˆæ‰©æ•£å»å™ªå™¨ï¼ˆLCDDï¼‰ï¼Œå®ƒç»Ÿä¸€äº†ä¸¤ç§äº’è¡¥çš„æ¨ç†ç¨‹åºâ€”â€”ä¸€ç§åˆ©ç”¨æ¨¡å‹çš„ç”Ÿæˆæ½œåŠ›ï¼Œå¦ä¸€ç§ç¡®ä¿ä¿¡å·çš„çœŸå®æ¢å¤ã€‚LCDDé€šè¿‡åˆ©ç”¨å»å™ªæ ·æœ¬çš„å›ºæœ‰ç»“æ„ï¼Œå®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼Œå¹¶é€šè¿‡ç®€å•çš„æ ‡é‡è¶…å‚æ•°è°ƒæ•´ï¼Œæä¾›äº†å¯æ§çš„ä¼˜åŒ–æƒè¡¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸå¤‡å—å…³æ³¨ï¼Œå°¤å…¶åœ¨å›¾åƒåˆæˆå’Œå›¾åƒé‡å»ºä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>ç°æœ‰æ–¹æ³•éš¾ä»¥å¹³è¡¡æ‰©æ•£æ¨¡å‹çš„é«˜è§†è§‰è´¨é‡ä¸ä½å¤±çœŸã€‚</li>
<li>é’ˆå¯¹åŠ æ€§é«˜æ–¯å™ªå£°å»é™¤ä»»åŠ¡ï¼Œæå‡ºäº†ä¸€ç§åˆ©ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„ç›´è§‚æ–¹æ³•ã€‚</li>
<li>å¼•å…¥äº†çº¿æ€§ç»„åˆæ‰©æ•£å»å™ªå™¨ï¼ˆLCDDï¼‰ï¼Œèåˆäº†ä¸¤ç§äº’è¡¥çš„æ¨ç†ç¨‹åºã€‚</li>
<li>LCDDåˆ©ç”¨æ¨¡å‹çš„ç”Ÿæˆæ½œåŠ›ï¼ŒåŒæ—¶ç¡®ä¿ä¿¡å·çš„çœŸå®æ¢å¤ã€‚</li>
<li>LCDDé€šè¿‡åˆ©ç”¨å»å™ªæ ·æœ¬çš„å›ºæœ‰ç»“æ„å®ç°äº†å“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14654">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4c2ea409c2b04e690604d25421327e3b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7188d13e2cc7b21da6e64e6d7bb71719.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93b44faf967acf903f4b924bd1f27a4d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b5143422e6155faefdf1ac8d888f8ee4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3b5d4c8620f17420552564c5994a2a2d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-280f0608a09b9cd1db2be5b3efc5ff40.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="CTSR-Controllable-Fidelity-Realness-Trade-off-Distillation-for-Real-World-Image-Super-Resolution"><a href="#CTSR-Controllable-Fidelity-Realness-Trade-off-Distillation-for-Real-World-Image-Super-Resolution" class="headerlink" title="CTSR: Controllable Fidelity-Realness Trade-off Distillation for   Real-World Image Super Resolution"></a>CTSR: Controllable Fidelity-Realness Trade-off Distillation for   Real-World Image Super Resolution</h2><p><strong>Authors:Runyi Li, Bin Chen, Jian Zhang, Radu Timofte</strong></p>
<p>Real-world image super-resolution is a critical image processing task, where two key evaluation criteria are the fidelity to the original image and the visual realness of the generated results. Although existing methods based on diffusion models excel in visual realness by leveraging strong priors, they often struggle to achieve an effective balance between fidelity and realness. In our preliminary experiments, we observe that a linear combination of multiple models outperforms individual models, motivating us to harness the strengths of different models for a more effective trade-off. Based on this insight, we propose a distillation-based approach that leverages the geometric decomposition of both fidelity and realness, alongside the performance advantages of multiple teacher models, to strike a more balanced trade-off. Furthermore, we explore the controllability of this trade-off, enabling a flexible and adjustable super-resolution process, which we call CTSR (Controllable Trade-off Super-Resolution). Experiments conducted on several real-world image super-resolution benchmarks demonstrate that our method surpasses existing state-of-the-art approaches, achieving superior performance across both fidelity and realness metrics. </p>
<blockquote>
<p>ç°å®ä¸–ç•Œå›¾åƒè¶…åˆ†è¾¨ç‡å¤„ç†æ˜¯ä¸€é¡¹å…³é”®çš„å›¾åƒå¤„ç†ä»»åŠ¡ï¼Œå…¶ä¸¤ä¸ªä¸»è¦çš„è¯„ä¼°æ ‡å‡†æ˜¯å¯¹äºåŸå§‹å›¾åƒçš„ä¿çœŸåº¦å’Œç”Ÿæˆç»“æœçš„è§†è§‰é€¼çœŸåº¦ã€‚å°½ç®¡ç°æœ‰çš„åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–¹æ³•é€šè¿‡åˆ©ç”¨å¼ºå¤§çš„å…ˆéªŒçŸ¥è¯†åœ¨è§†è§‰é€¼çœŸåº¦æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬å¾€å¾€éš¾ä»¥åœ¨ä¿çœŸåº¦å’Œé€¼çœŸåº¦ä¹‹é—´å–å¾—æœ‰æ•ˆå¹³è¡¡ã€‚åœ¨æˆ‘ä»¬çš„åˆæ­¥å®éªŒä¸­ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°å¤šç§æ¨¡å‹çš„çº¿æ€§ç»„åˆè¡¨ç°ä¼˜äºå•ä¸ªæ¨¡å‹ï¼Œè¿™æ¿€åŠ±æˆ‘ä»¬åˆ©ç”¨ä¸åŒæ¨¡å‹çš„ä¼˜åŠ¿ä»¥å®ç°æ›´æœ‰æ•ˆçš„æƒè¡¡ã€‚åŸºäºæ­¤è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè’¸é¦çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨ä¿çœŸåº¦å’Œé€¼çœŸåº¦çš„å‡ ä½•åˆ†è§£ï¼Œä»¥åŠå¤šä¸ªæ•™å¸ˆæ¨¡å‹çš„æ€§èƒ½ä¼˜åŠ¿ï¼Œä»¥å®ç°æ›´å¹³è¡¡çš„æƒè¡¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ¢ç´¢äº†è¿™ç§æƒè¡¡çš„å¯æ§æ€§ï¼Œä»¥å®ç°ä¸€ä¸ªçµæ´»å¯è°ƒçš„è¶…çº§åˆ†è¾¨ç‡è¿‡ç¨‹ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºå¯æ§æƒè¡¡è¶…çº§åˆ†è¾¨ç‡ï¼ˆCTSRï¼‰ã€‚åœ¨å‡ ä¸ªç°å®ä¸–ç•Œå›¾åƒè¶…åˆ†è¾¨ç‡å¤„ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¶…è¶Šäº†ç°æœ‰çš„æœ€æ–°æ–¹æ³•ï¼Œåœ¨ä¿çœŸåº¦å’Œé€¼çœŸåº¦æŒ‡æ ‡ä¸Šéƒ½å®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14272v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ‰©æ•£æ¨¡å‹çš„ç°å®å›¾åƒè¶…åˆ†è¾¨ç‡å¤„ç†æ˜¯ä¸€é¡¹å…³é”®ä»»åŠ¡ï¼Œé¢ä¸´ä¿çœŸåº¦å’Œè§†è§‰çœŸå®åº¦ä¸¤ä¸ªæ ¸å¿ƒè¯„ä»·æŒ‡æ ‡çš„æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•è™½åœ¨è§†è§‰çœŸå®åº¦ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨å¹³è¡¡ä¿çœŸåº¦å’ŒçœŸå®åº¦æ–¹é¢å­˜åœ¨å›°éš¾ã€‚æœ¬æ–‡æå‡ºä¸€ç§åŸºäºè’¸é¦çš„æ–¹æ³•ï¼Œé€šè¿‡å‡ ä½•åˆ†è§£å®ç°ä¿çœŸåº¦å’ŒçœŸå®åº¦çš„å¹³è¡¡ï¼Œå¹¶ç»“åˆå¤šä¸ªæ•™å¸ˆæ¨¡å‹çš„æ€§èƒ½ä¼˜åŠ¿ï¼Œæå‡ºå¯æ§çš„æƒè¡¡è¶…åˆ†è¾¨ç‡å¤„ç†ï¼ˆCTSRï¼‰ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰æŠ€æœ¯å‰æ²¿ï¼Œåœ¨ä¿çœŸåº¦å’ŒçœŸå®åº¦æŒ‡æ ‡ä¸Šå‡è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°å®å›¾åƒè¶…åˆ†è¾¨ç‡å¤„ç†æ˜¯å›¾åƒå¤„ç†çš„é‡ç‚¹ä»»åŠ¡ã€‚</li>
<li>ç°æœ‰æ‰©æ•£æ¨¡å‹æ–¹æ³•åœ¨å¹³è¡¡å›¾åƒä¿çœŸåº¦å’Œè§†è§‰çœŸå®åº¦æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>æœ¬æ–‡é€šè¿‡å‡ ä½•åˆ†è§£å®ç°ä¿çœŸåº¦å’ŒçœŸå®åº¦çš„å¹³è¡¡ã€‚</li>
<li>æå‡ºä¸€ç§åŸºäºè’¸é¦çš„æ–¹æ³•ï¼Œç»“åˆå¤šä¸ªæ•™å¸ˆæ¨¡å‹çš„æ€§èƒ½ä¼˜åŠ¿ã€‚</li>
<li>æå‡ºäº†å¯æ§çš„æƒè¡¡è¶…åˆ†è¾¨ç‡å¤„ç†ï¼ˆCTSRï¼‰ã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªçœŸå®ä¸–ç•Œå›¾åƒè¶…åˆ†è¾¨ç‡åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14272">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ccca9a821f250eea5dd83a0f5a69217e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-568f43e0303e91a36949122b03bea56e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-87253920683e3ec60bb543b8063d9079.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-908f143b20660e06d1f8dcdb0279b4cc.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="FlexWorld-Progressively-Expanding-3D-Scenes-for-Flexiable-View-Synthesis"><a href="#FlexWorld-Progressively-Expanding-3D-Scenes-for-Flexiable-View-Synthesis" class="headerlink" title="FlexWorld: Progressively Expanding 3D Scenes for Flexiable-View   Synthesis"></a>FlexWorld: Progressively Expanding 3D Scenes for Flexiable-View   Synthesis</h2><p><strong>Authors:Luxi Chen, Zihan Zhou, Min Zhao, Yikai Wang, Ge Zhang, Wenhao Huang, Hao Sun, Ji-Rong Wen, Chongxuan Li</strong></p>
<p>Generating flexible-view 3D scenes, including 360{\deg} rotation and zooming, from single images is challenging due to a lack of 3D data. To this end, we introduce FlexWorld, a novel framework consisting of two key components: (1) a strong video-to-video (V2V) diffusion model to generate high-quality novel view images from incomplete input rendered from a coarse scene, and (2) a progressive expansion process to construct a complete 3D scene. In particular, leveraging an advanced pre-trained video model and accurate depth-estimated training pairs, our V2V model can generate novel views under large camera pose variations. Building upon it, FlexWorld progressively generates new 3D content and integrates it into the global scene through geometry-aware scene fusion. Extensive experiments demonstrate the effectiveness of FlexWorld in generating high-quality novel view videos and flexible-view 3D scenes from single images, achieving superior visual quality under multiple popular metrics and datasets compared to existing state-of-the-art methods. Qualitatively, we highlight that FlexWorld can generate high-fidelity scenes with flexible views like 360{\deg} rotations and zooming. Project page: <a target="_blank" rel="noopener" href="https://ml-gsai.github.io/FlexWorld">https://ml-gsai.github.io/FlexWorld</a>. </p>
<blockquote>
<p>ç”ŸæˆåŒ…å«360Â°æ—‹è½¬å’Œç¼©æ”¾åŠŸèƒ½çš„çµæ´»è§†è§’3Dåœºæ™¯ï¼Œä»å•å¼ å›¾åƒå¼€å§‹æ˜¯ä¸€é¡¹æŒ‘æˆ˜ï¼Œå› ä¸ºç¼ºä¹3Dæ•°æ®ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†FlexWorldï¼Œè¿™æ˜¯ä¸€ä¸ªç”±ä¸¤ä¸ªå…³é”®ç»„ä»¶æ„æˆçš„æ–°å‹æ¡†æ¶ï¼šï¼ˆ1ï¼‰å¼ºå¤§çš„è§†é¢‘åˆ°è§†é¢‘ï¼ˆV2Vï¼‰æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºä»ç”±ç²—ç³™åœºæ™¯æ¸²æŸ“çš„ä¸å®Œæ•´è¾“å…¥ç”Ÿæˆé«˜è´¨é‡çš„æ–°è§†è§’å›¾åƒï¼›ï¼ˆ2ï¼‰æ¸è¿›å¼æ‰©å±•è¿‡ç¨‹ï¼Œç”¨äºæ„å»ºå®Œæ•´çš„3Dåœºæ™¯ã€‚ç‰¹åˆ«æ˜¯ï¼Œå€ŸåŠ©å…ˆè¿›çš„é¢„è®­ç»ƒè§†é¢‘æ¨¡å‹å’Œç²¾ç¡®çš„æ·±åº¦ä¼°è®¡è®­ç»ƒå¯¹ï¼Œæˆ‘ä»¬çš„V2Væ¨¡å‹å¯ä»¥åœ¨å¤§ç›¸æœºå§¿æ€å˜åŒ–ä¸‹ç”Ÿæˆæ–°è§†è§’ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼ŒFlexWorldé€æ­¥ç”Ÿæˆæ–°çš„3Då†…å®¹ï¼Œå¹¶é€šè¿‡å‡ ä½•æ„ŸçŸ¥åœºæ™¯èåˆå°†å…¶é›†æˆåˆ°å…¨å±€åœºæ™¯ä¸­ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒFlexWorldåœ¨ç”Ÿæˆé«˜è´¨é‡çš„æ–°è§†è§’è§†é¢‘å’Œçµæ´»è§†è§’çš„3Dåœºæ™¯æ–¹é¢éå¸¸æœ‰æ•ˆï¼Œä»å•å¼ å›¾åƒå¼€å§‹ï¼Œåœ¨å¤šä¸ªæµè¡ŒæŒ‡æ ‡å’Œæ•°æ®é›†ä¸Šä¸ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œå®ç°äº†ä¼˜è¶Šçš„è§†è§‰è´¨é‡ã€‚ä»å®šæ€§è§’åº¦çœ‹ï¼Œæˆ‘ä»¬å¼ºè°ƒFlexWorldå¯ä»¥ç”Ÿæˆå…·æœ‰çµæ´»è§†è§’çš„é«˜ä¿çœŸåœºæ™¯ï¼Œå¦‚360Â°æ—‹è½¬å’Œç¼©æ”¾ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://ml-gsai.github.io/FlexWorld%E3%80%82">https://ml-gsai.github.io/FlexWorldã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13265v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå•å›¾ç”Ÿæˆå…·æœ‰360Â°æ—‹è½¬å’Œç¼©æ”¾åŠŸèƒ½çš„çµæ´»è§†å›¾3Dåœºæ™¯æ˜¯ä¸€é¡¹æŒ‘æˆ˜ï¼Œå› ä¸ºç¼ºä¹3Dæ•°æ®ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†FlexWorldæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šï¼ˆ1ï¼‰å¼ºå¤§çš„è§†é¢‘åˆ°è§†é¢‘ï¼ˆV2Vï¼‰æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºä»ç”±ç²—ç³™åœºæ™¯æ¸²æŸ“çš„ä¸å®Œæ•´è¾“å…¥ç”Ÿæˆé«˜è´¨é‡çš„æ–°è§†å›¾å›¾åƒï¼›ï¼ˆ2ï¼‰æ¸è¿›æ‰©å±•è¿‡ç¨‹ï¼Œç”¨äºæ„å»ºå®Œæ•´çš„3Dåœºæ™¯ã€‚æˆ‘ä»¬çš„V2Væ¨¡å‹åˆ©ç”¨å…ˆè¿›çš„é¢„è®­ç»ƒè§†é¢‘æ¨¡å‹å’Œç²¾ç¡®çš„æ·±åº¦ä¼°è®¡è®­ç»ƒå¯¹ï¼Œå¯ä»¥åœ¨å¤§ç›¸æœºå§¿æ€å˜åŒ–ä¸‹ç”Ÿæˆæ–°è§†å›¾ã€‚FlexWorldåœ¨æ­¤åŸºç¡€ä¸Šé€æ­¥ç”Ÿæˆæ–°çš„3Då†…å®¹ï¼Œå¹¶é€šè¿‡å‡ ä½•æ„ŸçŸ¥åœºæ™¯èåˆå°†å…¶é›†æˆåˆ°å…¨å±€åœºæ™¯ä¸­ã€‚å®éªŒè¯æ˜ï¼ŒFlexWorldåœ¨ç”Ÿæˆé«˜è´¨é‡æ–°è§†å›¾è§†é¢‘å’Œçµæ´»è§†å›¾3Dåœºæ™¯æ–¹é¢æ•ˆæœæ˜¾è‘—ï¼Œåœ¨å¤šä¸ªæµè¡ŒæŒ‡æ ‡å’Œæ•°æ®é›†ä¸Šä¸ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”å…·æœ‰ä¼˜åŠ¿ã€‚FlexWorldèƒ½å¤Ÿç”Ÿæˆå…·æœ‰çµæ´»è§†å›¾çš„é«˜ä¿çœŸåœºæ™¯ï¼Œå¦‚360Â°æ—‹è½¬å’Œç¼©æ”¾ã€‚æ›´å¤šä¿¡æ¯è¯·è®¿é—®é¡¹ç›®é¡µé¢ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FlexWorldæ˜¯ä¸€ä¸ªç”¨äºä»å•å›¾åƒç”Ÿæˆçµæ´»è§†å›¾3Dåœºæ™¯çš„æ–°æ¡†æ¶ã€‚</li>
<li>æ¡†æ¶åŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šè§†é¢‘åˆ°è§†é¢‘ï¼ˆV2Vï¼‰æ‰©æ•£æ¨¡å‹å’Œæ¸è¿›æ‰©å±•è¿‡ç¨‹ã€‚</li>
<li>V2Væ‰©æ•£æ¨¡å‹åŸºäºé¢„è®­ç»ƒçš„è§†é¢‘æ¨¡å‹å’Œæ·±åº¦ä¼°è®¡è®­ç»ƒå¯¹ï¼Œå¯ç”Ÿæˆé«˜è´¨é‡çš„æ–°è§†å›¾å›¾åƒã€‚</li>
<li>æ¸è¿›æ‰©å±•è¿‡ç¨‹ç”¨äºæ„å»ºå®Œæ•´çš„3Dåœºæ™¯ï¼Œé€šè¿‡å‡ ä½•æ„ŸçŸ¥åœºæ™¯èåˆé›†æˆæ–°ç”Ÿæˆçš„3Då†…å®¹ã€‚</li>
<li>FlexWorldèƒ½å¤Ÿåœ¨å¤šç§æ•°æ®é›†ä¸Šå®ç°é«˜è´¨é‡çš„çµæ´»è§†å›¾ç”Ÿæˆï¼ŒåŒ…æ‹¬360Â°æ—‹è½¬å’Œç¼©æ”¾ã€‚</li>
<li>FlexWorldåœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13265">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dd4764ca7c7736aae75729cf12488f65.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5d17af8e7c95e02f57f511c5eab56f44.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f0b9de323eebe60f047419695d601d3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fceedf4e8d2678f9bae0e00404790899.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9ba1da5f214a038cdd2e2942c7032834.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Low-Biased-General-Annotated-Dataset-Generation"><a href="#Low-Biased-General-Annotated-Dataset-Generation" class="headerlink" title="Low-Biased General Annotated Dataset Generation"></a>Low-Biased General Annotated Dataset Generation</h2><p><strong>Authors:Dengyang Jiang, Haoyu Wang, Lei Zhang, Wei Wei, Guang Dai, Mengmeng Wang, Jingdong Wang, Yanning Zhang</strong></p>
<p>Pre-training backbone networks on a general annotated dataset (e.g., ImageNet) that comprises numerous manually collected images with category annotations has proven to be indispensable for enhancing the generalization capacity of downstream visual tasks. However, those manually collected images often exhibit bias, which is non-transferable across either categories or domains, thus causing the modelâ€™s generalization capacity degeneration. To mitigate this problem, we present a low-biased general annotated dataset generation framework (lbGen). Instead of expensive manual collection, we aim at directly generating low-biased images with category annotations. To achieve this goal, we propose to leverage the advantage of a multimodal foundation model (e.g., CLIP), in terms of aligning images in a low-biased semantic space defined by language. Specifically, we develop a bi-level semantic alignment loss, which not only forces all generated images to be consistent with the semantic distribution of all categories belonging to the target dataset in an adversarial learning manner, but also requires each generated image to match the semantic description of its category name. In addition, we further cast an existing image quality scoring model into a quality assurance loss to preserve the quality of the generated image. By leveraging these two loss functions, we can obtain a low-biased image generation model by simply fine-tuning a pre-trained diffusion model using only all category names in the target dataset as input. Experimental results confirm that, compared with the manually labeled dataset or other synthetic datasets, the utilization of our generated low-biased dataset leads to stable generalization capacity enhancement of different backbone networks across various tasks, especially in tasks where the manually labeled samples are scarce. </p>
<blockquote>
<p>åœ¨åŒ…å«å¤§é‡æ‰‹åŠ¨æ”¶é›†å¹¶å¸¦æœ‰ç±»åˆ«æ³¨é‡Šçš„å›¾åƒçš„ä¸€èˆ¬æ³¨é‡Šæ•°æ®é›†ä¸Šï¼Œå¯¹éª¨å¹²ç½‘ç»œè¿›è¡Œé¢„è®­ç»ƒï¼Œå¯¹äºæé«˜ä¸‹æ¸¸è§†è§‰ä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›æ¥è¯´æ˜¯ä¸å¯æˆ–ç¼ºçš„ã€‚ç„¶è€Œï¼Œè¿™äº›æ‰‹åŠ¨æ”¶é›†çš„å›¾åƒå¾€å¾€å­˜åœ¨åè§ï¼Œè¿™äº›åè§åœ¨ç±»åˆ«æˆ–åŸŸä¹‹é—´æ˜¯ä¸å¯è½¬ç§»çš„ï¼Œä»è€Œå¯¼è‡´æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ä¸‹é™ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä½åç½®çš„ä¸€èˆ¬æ³¨é‡Šæ•°æ®é›†ç”Ÿæˆæ¡†æ¶ï¼ˆlbGenï¼‰ã€‚æˆ‘ä»¬æ—¨åœ¨é€šè¿‡ç›´æ¥ç”Ÿæˆå¸¦æœ‰ç±»åˆ«æ³¨é‡Šçš„ä½åç½®å›¾åƒï¼Œè€Œéæ˜‚è´µçš„æ‰‹åŠ¨æ”¶é›†ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬æè®®åˆ©ç”¨å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹çš„ä¼˜ç‚¹ï¼ˆä¾‹å¦‚CLIPï¼‰ï¼Œä»¥è¯­è¨€å®šä¹‰çš„ä½åç½®è¯­ä¹‰ç©ºé—´æ¥å¯¹é½å›¾åƒã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§ä¸¤çº§è¯­ä¹‰å¯¹é½æŸå¤±ï¼Œè¿™ä¸ä»…ä»¥å¯¹æŠ—æ€§å­¦ä¹ çš„æ–¹å¼å¼ºåˆ¶æ‰€æœ‰ç”Ÿæˆçš„å›¾åƒä¸ç›®æ ‡æ•°æ®é›†ä¸­æ‰€æœ‰ç±»åˆ«çš„è¯­ä¹‰åˆ†å¸ƒä¿æŒä¸€è‡´ï¼Œè¿˜è¦æ±‚æ¯ä¸ªç”Ÿæˆçš„å›¾åƒä¸å…¶ç±»åˆ«åç§°çš„è¯­ä¹‰æè¿°ç›¸åŒ¹é…ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†ç°æœ‰çš„å›¾åƒè´¨é‡è¯„åˆ†æ¨¡å‹è½¬åŒ–ä¸ºè´¨é‡ä¿è¯æŸå¤±ï¼Œä»¥ä¿æŒç”Ÿæˆå›¾åƒçš„è´¨é‡ã€‚é€šè¿‡åˆ©ç”¨è¿™ä¸¤ç§æŸå¤±å‡½æ•°ï¼Œæˆ‘ä»¬åªéœ€ä½¿ç”¨ç›®æ ‡æ•°æ®é›†ä¸­çš„æ‰€æœ‰ç±»åˆ«åç§°å¯¹é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå³å¯è·å¾—ä½åç½®çš„å›¾åƒç”Ÿæˆæ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ä½¿ç”¨æ‰‹åŠ¨æ ‡è®°çš„æ•°æ®é›†æˆ–å…¶ä»–åˆæˆæ•°æ®é›†ç›¸æ¯”ï¼Œä½¿ç”¨æˆ‘ä»¬ç”Ÿæˆçš„ä½åç½®æ•°æ®é›†å¯ä»¥æé«˜ä¸åŒéª¨å¹²ç½‘ç»œåœ¨ä¸åŒä»»åŠ¡ä¸Šçš„æ³›åŒ–èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨æ‰‹åŠ¨æ ‡è®°æ ·æœ¬ç¨€ç¼ºçš„ä»»åŠ¡ä¸­ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.10831v3">PDF</a> CVPR2025 Accepted Paper</p>
<p><strong>Summary</strong></p>
<p>åŸºäºé¢„è®­ç»ƒæ¨¡å‹åœ¨é€šç”¨æ ‡æ³¨æ•°æ®é›†ä¸Šçš„è®­ç»ƒï¼Œå¦‚ImageNetï¼Œå¯¹äºä¸‹æ¸¸è§†è§‰ä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›æå‡å…·æœ‰é‡è¦æ„ä¹‰ã€‚ç„¶è€Œï¼Œæ‰‹åŠ¨æ”¶é›†çš„å›¾åƒå¸¸å¸¸å­˜åœ¨åè§ï¼Œå½±å“æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä½åè§çš„é€šç”¨æ ‡æ³¨æ•°æ®é›†ç”Ÿæˆæ¡†æ¶ï¼ˆlbGenï¼‰ã€‚å®ƒæ—¨åœ¨é€šè¿‡ç›´æ¥ç”Ÿæˆä½åè§çš„å›¾åƒåŠå…¶ç±»åˆ«æ ‡æ³¨æ¥æ›¿ä»£æ˜‚è´µçš„æ‰‹åŠ¨æ”¶é›†è¿‡ç¨‹ã€‚æˆ‘ä»¬åˆ©ç”¨å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰çš„ä¼˜åŠ¿ï¼Œåœ¨ä¸€ä¸ªç”±è¯­è¨€å®šä¹‰çš„ä½åè§è¯­ä¹‰ç©ºé—´ä¸­å¯¹é½å›¾åƒã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§ä¸¤çº§è¯­ä¹‰å¯¹é½æŸå¤±ï¼Œä¸ä»…ä»¥å¯¹æŠ—æ€§å­¦ä¹ çš„æ–¹å¼è¿«ä½¿æ‰€æœ‰ç”Ÿæˆçš„å›¾åƒä¸ç›®æ ‡æ•°æ®é›†ä¸­æ‰€æœ‰ç±»åˆ«çš„è¯­ä¹‰åˆ†å¸ƒä¿æŒä¸€è‡´ï¼Œè¿˜è¦æ±‚æ¯ä¸ªç”Ÿæˆçš„å›¾åƒä¸å…¶ç±»åˆ«åç§°çš„è¯­ä¹‰æè¿°ç›¸åŒ¹é…ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜åˆ©ç”¨ç°æœ‰çš„å›¾åƒè´¨é‡è¯„åˆ†æ¨¡å‹æ„å»ºäº†ä¸€ä¸ªè´¨é‡ä¿éšœæŸå¤±ï¼Œä»¥ä¿æŒç”Ÿæˆå›¾åƒçš„è´¨é‡ã€‚é€šè¿‡åˆ©ç”¨è¿™ä¸¤ç§æŸå¤±å‡½æ•°ï¼Œæˆ‘ä»¬åªéœ€é€šè¿‡å¾®è°ƒé¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ï¼Œä»¥ç›®æ ‡æ•°æ®é›†çš„æ‰€æœ‰ç±»åˆ«åç§°ä½œä¸ºè¾“å…¥ï¼Œå³å¯è·å¾—ä½åè§çš„å›¾åƒç”Ÿæˆæ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ä½¿ç”¨æ‰‹åŠ¨æ ‡æ³¨æ•°æ®é›†æˆ–å…¶ä»–åˆæˆæ•°æ®é›†ç›¸æ¯”ï¼Œåˆ©ç”¨æˆ‘ä»¬ç”Ÿæˆçš„ä½åè§æ•°æ®é›†å¯ä»¥ç¨³å®šæå‡ä¸åŒéª¨å¹²ç½‘ç»œåœ¨å„ç§ä»»åŠ¡ä¸Šçš„æ³›åŒ–èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨æ‰‹åŠ¨æ ‡æ³¨æ ·æœ¬ç¨€ç¼ºçš„ä»»åŠ¡ä¸­è¡¨ç°å°¤ä¸ºçªå‡ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢„è®­ç»ƒæ¨¡å‹åœ¨é€šç”¨æ ‡æ³¨æ•°æ®é›†ä¸Šçš„è®­ç»ƒå¯¹äºä¸‹æ¸¸è§†è§‰ä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›æå‡è‡³å…³é‡è¦ã€‚</li>
<li>æ‰‹åŠ¨æ”¶é›†çš„å›¾åƒå­˜åœ¨åè§ï¼Œå½±å“æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªä½åè§çš„é€šç”¨æ ‡æ³¨æ•°æ®é›†ç”Ÿæˆæ¡†æ¶ï¼ˆlbGenï¼‰æ¥ç”Ÿæˆä½åè§çš„å›¾åƒåŠå…¶ç±»åˆ«æ ‡æ³¨ã€‚</li>
<li>åˆ©ç”¨å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰åœ¨ä¸€ä¸ªä½åè§è¯­ä¹‰ç©ºé—´ä¸­å¯¹é½å›¾åƒã€‚</li>
<li>å¼€å‘äº†ä¸€ç§ä¸¤çº§è¯­ä¹‰å¯¹é½æŸå¤±ï¼Œç¡®ä¿ç”Ÿæˆçš„å›¾åƒä¸ç±»åˆ«è¯­ä¹‰åˆ†å¸ƒä¸€è‡´ã€‚</li>
<li>åˆ©ç”¨å›¾åƒè´¨é‡è¯„åˆ†æ¨¡å‹æ„å»ºè´¨é‡ä¿éšœæŸå¤±ï¼Œä¿æŒç”Ÿæˆå›¾åƒçš„è´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.10831">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-33ab8b01b5fc33803ab613a659146196.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e16fcaf8b93f38fee986aa074dcd9355.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7fd9e366ddeb04638181b6cf907e7554.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-32a5886bea118e3a9dbda0a77fee15ec.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="VideoDirector-Precise-Video-Editing-via-Text-to-Video-Models"><a href="#VideoDirector-Precise-Video-Editing-via-Text-to-Video-Models" class="headerlink" title="VideoDirector: Precise Video Editing via Text-to-Video Models"></a>VideoDirector: Precise Video Editing via Text-to-Video Models</h2><p><strong>Authors:Yukun Wang, Longguang Wang, Zhiyuan Ma, Qibin Hu, Kai Xu, Yulan Guo</strong></p>
<p>Despite the typical inversion-then-editing paradigm using text-to-image (T2I) models has demonstrated promising results, directly extending it to text-to-video (T2V) models still suffers severe artifacts such as color flickering and content distortion. Consequently, current video editing methods primarily rely on T2I models, which inherently lack temporal-coherence generative ability, often resulting in inferior editing results. In this paper, we attribute the failure of the typical editing paradigm to: 1) Tightly Spatial-temporal Coupling. The vanilla pivotal-based inversion strategy struggles to disentangle spatial-temporal information in the video diffusion model; 2) Complicated Spatial-temporal Layout. The vanilla cross-attention control is deficient in preserving the unedited content. To address these limitations, we propose a spatial-temporal decoupled guidance (STDG) and multi-frame null-text optimization strategy to provide pivotal temporal cues for more precise pivotal inversion. Furthermore, we introduce a self-attention control strategy to maintain higher fidelity for precise partial content editing. Experimental results demonstrate that our method (termed VideoDirector) effectively harnesses the powerful temporal generation capabilities of T2V models, producing edited videos with state-of-the-art performance in accuracy, motion smoothness, realism, and fidelity to unedited content. </p>
<blockquote>
<p>å°½ç®¡ä½¿ç”¨æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ¨¡å‹çš„å…¸å‹åè½¬ç¼–è¾‘èŒƒå¼å·²ç»å–å¾—äº†æœ‰å‰æ™¯çš„ç»“æœï¼Œä½†ç›´æ¥å°†å…¶æ‰©å±•åˆ°æ–‡æœ¬åˆ°è§†é¢‘ï¼ˆT2Vï¼‰æ¨¡å‹ä»ç„¶ä¼šå‡ºç°ä¸¥é‡çš„ä¼ªå½±é—®é¢˜ï¼Œå¦‚è‰²å½©é—ªçƒå’Œå†…å®¹å¤±çœŸã€‚å› æ­¤ï¼Œç›®å‰çš„è§†é¢‘ç¼–è¾‘æ–¹æ³•ä¸»è¦ä¾èµ–äºT2Iæ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹æœ¬è´¨ä¸Šç¼ºä¹æ—¶é—´è¿è´¯æ€§ç”Ÿæˆèƒ½åŠ›ï¼Œå¾€å¾€å¯¼è‡´ç¼–è¾‘ç»“æœè¾ƒå·®ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†å…¸å‹çš„ç¼–è¾‘èŒƒå¼çš„å¤±è´¥å½’ç»“ä¸ºä»¥ä¸‹ä¸¤ç‚¹ï¼š1ï¼‰ç´§å¯†çš„ç©ºé—´æ—¶é—´è€¦åˆã€‚åŸºäºåŸå§‹å…³é”®ç‚¹çš„åè½¬ç­–ç•¥åœ¨è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­å¾ˆéš¾åˆ†ç¦»ç©ºé—´æ—¶é—´ä¿¡æ¯ï¼›2ï¼‰å¤æ‚çš„ç©ºé—´æ—¶é—´å¸ƒå±€ã€‚åŸå§‹çš„äº¤å‰æ³¨æ„åŠ›æ§åˆ¶åœ¨äºä¿æŒæœªç¼–è¾‘çš„å†…å®¹ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†æ—¶ç©ºè§£è€¦æŒ‡å¯¼ï¼ˆSTDGï¼‰å’Œå¤šå¸§æ— æ–‡æœ¬ä¼˜åŒ–ç­–ç•¥ï¼Œä¸ºæ›´ç²¾ç¡®çš„å…³é”®ç‚¹åè½¬æä¾›å…³é”®çš„æ—¶é—´çº¿ç´¢ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§è‡ªæ³¨æ„åŠ›æ§åˆ¶ç­–ç•¥ï¼Œä»¥åœ¨ç²¾ç¡®éƒ¨åˆ†å†…å®¹ç¼–è¾‘ä¸­ä¿æŒæ›´é«˜çš„ä¿çœŸåº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ï¼ˆç§°ä¸ºVideoDirectorï¼‰æœ‰æ•ˆåœ°åˆ©ç”¨äº†T2Væ¨¡å‹çš„å¼ºå¤§æ—¶é—´ç”Ÿæˆèƒ½åŠ›ï¼Œåœ¨å‡†ç¡®æ€§ã€è¿åŠ¨å¹³æ»‘åº¦ã€é€¼çœŸåº¦å’Œå¯¹æœªç¼–è¾‘å†…å®¹çš„ä¿çœŸåº¦æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.17592v3">PDF</a> 15 figures</p>
<p><strong>Summary</strong><br>     é’ˆå¯¹æ–‡æœ¬è½¬è§†é¢‘ï¼ˆT2Vï¼‰æ¨¡å‹ä¸­çš„å…¸å‹ç¼–è¾‘èŒƒå¼å­˜åœ¨çš„é—®é¢˜ï¼Œå¦‚é¢œè‰²é—ªçƒå’Œå†…å®¹å¤±çœŸç­‰ä¸¥é‡ä¼ªå½±ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ã€‚é€šè¿‡è§£å†³ç©ºé—´æ—¶é—´è€¦åˆå¤æ‚å’Œç©ºé—´æ—¶é—´å¸ƒå±€çš„é—®é¢˜ï¼Œå¼•å…¥æ—¶ç©ºåˆ†ç¦»æŒ‡å¯¼ï¼ˆSTDGï¼‰å’Œå¤šå¸§æ— æ–‡æœ¬ä¼˜åŒ–ç­–ç•¥ï¼Œä¸ºç²¾ç¡®çš„å…³é”®ç‚¹åè½¬æä¾›å…³é”®çš„æ—¶åºçº¿ç´¢ã€‚åŒæ—¶é‡‡ç”¨è‡ªæˆ‘å…³æ³¨æ§åˆ¶ç­–ç•¥ï¼Œä¿æŒå¯¹ç²¾ç¡®éƒ¨åˆ†å†…å®¹ç¼–è¾‘çš„é«˜ä¿çœŸåº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ï¼ˆç§°ä¸ºVideoDirectorï¼‰æœ‰æ•ˆåˆ©ç”¨äº†T2Væ¨¡å‹çš„å¼ºå¤§æ—¶åºç”Ÿæˆèƒ½åŠ›ï¼Œåœ¨å‡†ç¡®æ€§ã€è¿åŠ¨å¹³æ»‘åº¦ã€çœŸå®æ€§å’Œå¯¹æœªç¼–è¾‘å†…å®¹çš„ä¿çœŸåº¦æ–¹é¢è¾¾åˆ°äº†ä¸šç•Œé¢†å…ˆæ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬è½¬è§†é¢‘ï¼ˆT2Vï¼‰æ¨¡å‹ä¸­çš„å…¸å‹ç¼–è¾‘èŒƒå¼å­˜åœ¨é¢œè‰²é—ªçƒå’Œå†…å®¹å¤±çœŸç­‰é—®é¢˜ã€‚</li>
<li>è¿™äº›é—®é¢˜ä¸»è¦æºäºç´§å¯†çš„ç©ºé—´æ—¶é—´è€¦åˆå’Œå¤æ‚çš„ç©ºé—´æ—¶é—´å¸ƒå±€ã€‚</li>
<li>å¼•å…¥çš„æ—¶ç©ºåˆ†ç¦»æŒ‡å¯¼ï¼ˆSTDGï¼‰æœ‰åŠ©äºè§£å†³ç©ºé—´æ—¶é—´è€¦åˆé—®é¢˜ã€‚</li>
<li>å¤šå¸§æ— æ–‡æœ¬ä¼˜åŒ–ç­–ç•¥ä¸ºç²¾ç¡®çš„å…³é”®ç‚¹åè½¬æä¾›äº†å…³é”®çš„æ—¶åºçº¿ç´¢ã€‚</li>
<li>è‡ªæˆ‘å…³æ³¨æ§åˆ¶ç­–ç•¥æœ‰åŠ©äºä¿æŒå¯¹ç²¾ç¡®éƒ¨åˆ†å†…å®¹ç¼–è¾‘çš„é«˜ä¿çœŸåº¦ã€‚</li>
<li>VideoDirectoræ–¹æ³•æœ‰æ•ˆåˆ©ç”¨äº†T2Væ¨¡å‹çš„å¼ºå¤§æ—¶åºç”Ÿæˆèƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.17592">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-14bf48a277b8d0bb2613a41c54153e43.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f9c10b9f2e40b493a4d4b51708ed1111.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-20db587638bd4b9efafd0e945a5435e6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-96f532fc8eb4b1834bc83294e1a20507.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-762573e30cb534041dcb372a9e017808.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a3ddc851d3027763743214974263bda2.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Pathways-on-the-Image-Manifold-Image-Editing-via-Video-Generation"><a href="#Pathways-on-the-Image-Manifold-Image-Editing-via-Video-Generation" class="headerlink" title="Pathways on the Image Manifold: Image Editing via Video Generation"></a>Pathways on the Image Manifold: Image Editing via Video Generation</h2><p><strong>Authors:Noam Rotstein, Gal Yona, Daniel Silver, Roy Velich, David BensaÃ¯d, Ron Kimmel</strong></p>
<p>Recent advances in image editing, driven by image diffusion models, have shown remarkable progress. However, significant challenges remain, as these models often struggle to follow complex edit instructions accurately and frequently compromise fidelity by altering key elements of the original image. Simultaneously, video generation has made remarkable strides, with models that effectively function as consistent and continuous world simulators. In this paper, we propose merging these two fields by utilizing image-to-video models for image editing. We reformulate image editing as a temporal process, using pretrained video models to create smooth transitions from the original image to the desired edit. This approach traverses the image manifold continuously, ensuring consistent edits while preserving the original imageâ€™s key aspects. Our approach achieves state-of-the-art results on text-based image editing, demonstrating significant improvements in both edit accuracy and image preservation. Visit our project page: <a target="_blank" rel="noopener" href="https://rotsteinnoam.github.io/Frame2Frame">https://rotsteinnoam.github.io/Frame2Frame</a>. </p>
<blockquote>
<p>å›¾åƒç¼–è¾‘é¢†åŸŸçš„æœ€æ–°è¿›å±•ï¼Œå¾—ç›Šäºå›¾åƒæ‰©æ•£æ¨¡å‹çš„æ¨åŠ¨ï¼Œå·²ç»å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œä»ç„¶å­˜åœ¨é‡å¤§æŒ‘æˆ˜ï¼Œå› ä¸ºè¿™äº›æ¨¡å‹å¾€å¾€éš¾ä»¥å‡†ç¡®éµå¾ªå¤æ‚çš„ç¼–è¾‘æŒ‡ä»¤ï¼Œå¹¶ä¸”ç»å¸¸æ”¹å˜åŸå§‹å›¾åƒçš„å…³é”®å…ƒç´ ï¼Œä»è€Œå½±å“ä¿çœŸåº¦ã€‚åŒæ—¶ï¼Œè§†é¢‘ç”Ÿæˆæ–¹é¢å·²ç»å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œæ¨¡å‹æœ‰æ•ˆåœ°å……å½“äº†è¿è´¯å’Œè¿ç»­çš„ä¸–ç•Œæ¨¡æ‹Ÿå™¨ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºé€šè¿‡åˆ©ç”¨å›¾åƒåˆ°è§†é¢‘æ¨¡å‹è¿›è¡Œå›¾åƒç¼–è¾‘æ¥èåˆè¿™ä¸¤ä¸ªé¢†åŸŸã€‚æˆ‘ä»¬å°†å›¾åƒç¼–è¾‘é‡æ–°åˆ¶å®šä¸ºä¸€ä¸ªæ—¶é—´è¿‡ç¨‹ï¼Œä½¿ç”¨é¢„è®­ç»ƒçš„è§†é¢‘æ¨¡å‹æ¥åˆ›å»ºä»åŸå§‹å›¾åƒåˆ°æ‰€éœ€ç¼–è¾‘çš„å¹³æ»‘è¿‡æ¸¡ã€‚è¿™ç§æ–¹æ³•åœ¨å›¾åƒæµå½¢ä¸Šè¿ç»­éå†ï¼Œç¡®ä¿ç¼–è¾‘çš„ä¸€è‡´æ€§ï¼ŒåŒæ—¶ä¿ç•™åŸå§‹å›¾åƒçš„å…³é”®æ–¹é¢ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨åŸºäºæ–‡æœ¬çš„å›¾åƒç¼–è¾‘æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼Œåœ¨ç¼–è¾‘å‡†ç¡®æ€§å’Œå›¾åƒä¿ç•™æ–¹é¢éƒ½å–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚æ¬¢è¿è®¿é—®æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://rotsteinnoam.github.io/Frame2Frame">Frame2Frame</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.16819v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å›¾åƒæ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç¼–è¾‘æ–¹é¢çš„æœ€æ–°è¿›å±•å·²ç»æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æˆæœï¼Œä½†ä»å­˜åœ¨æŒ‘æˆ˜ï¼Œå¦‚éš¾ä»¥å‡†ç¡®éµå¾ªå¤æ‚çš„ç¼–è¾‘æŒ‡ä»¤ï¼Œä»¥åŠåœ¨ä¿®æ”¹è¿‡ç¨‹ä¸­é¢‘ç¹æ”¹å˜åŸå§‹å›¾åƒçš„å…³é”®å…ƒç´ ã€‚æœ¬æ–‡æå‡ºå°†å›¾åƒç¼–è¾‘å’Œè§†é¢‘ç”Ÿæˆä¸¤ä¸ªé¢†åŸŸç›¸ç»“åˆï¼Œåˆ©ç”¨å›¾åƒåˆ°è§†é¢‘çš„æ¨¡å‹è¿›è¡Œå›¾åƒç¼–è¾‘ã€‚æˆ‘ä»¬é‡æ–°å°†å›¾åƒç¼–è¾‘è¡¨è¿°ä¸ºä¸€ä¸ªæ—¶é—´è¿‡ç¨‹ï¼Œå¹¶ä½¿ç”¨é¢„è®­ç»ƒçš„è§†é¢‘æ¨¡å‹æ¥åˆ›å»ºä»åŸå§‹å›¾åƒåˆ°æ‰€éœ€ç¼–è¾‘çš„å¹³æ»‘è¿‡æ¸¡ã€‚æ­¤æ–¹æ³•åœ¨æ–‡æœ¬é©±åŠ¨çš„å›¾åƒç¼–è¾‘æ–¹é¢å–å¾—äº†æœ€æ–°æˆæœï¼Œåœ¨ç¼–è¾‘å‡†ç¡®æ€§å’Œå›¾åƒä¿ç•™æ–¹é¢éƒ½æ˜¾ç¤ºå‡ºé‡å¤§æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒæ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç¼–è¾‘é¢†åŸŸå–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†ä»é¢ä¸´å‡†ç¡®éµå¾ªå¤æ‚ç¼–è¾‘æŒ‡ä»¤å’Œä¿æŒåŸå§‹å›¾åƒå…³é”®å…ƒç´ çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºå°†å›¾åƒç¼–è¾‘ä¸è§†é¢‘ç”Ÿæˆç›¸ç»“åˆçš„æ–°æ€è·¯ã€‚</li>
<li>åˆ©ç”¨é¢„è®­ç»ƒçš„è§†é¢‘æ¨¡å‹ï¼Œå°†å›¾åƒç¼–è¾‘é‡æ–°è¡¨è¿°ä¸ºä¸€ä¸ªæ—¶é—´è¿‡ç¨‹ã€‚</li>
<li>é€šè¿‡åˆ›å»ºä»åŸå§‹å›¾åƒåˆ°æ‰€éœ€ç¼–è¾‘çš„å¹³æ»‘è¿‡æ¸¡ï¼Œå®ç°äº†å›¾åƒç¼–è¾‘çš„æ–°æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å®ç°æ–‡æœ¬é©±åŠ¨çš„å›¾åƒç¼–è¾‘æ–¹é¢è¾¾åˆ°æœ€æ–°æˆæœã€‚</li>
<li>åœ¨ç¼–è¾‘å‡†ç¡®æ€§å’Œå›¾åƒä¿ç•™æ–¹é¢éƒ½æœ‰æ˜¾è‘—æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.16819">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2a0613d27fdcbd73dfbc500d8cd885f7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-af85b05da4766f4665f7bb10765f0765.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bc043f4cab323197ccf1cade71a10936.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0612d2fad51192fc2eb43f6776fc60fc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-23fbcdc43e3b62f8e50a91b8e48f4d12.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Leveraging-BEV-Paradigm-for-Ground-to-Aerial-Image-Synthesis"><a href="#Leveraging-BEV-Paradigm-for-Ground-to-Aerial-Image-Synthesis" class="headerlink" title="Leveraging BEV Paradigm for Ground-to-Aerial Image Synthesis"></a>Leveraging BEV Paradigm for Ground-to-Aerial Image Synthesis</h2><p><strong>Authors:Junyan Ye, Jun He, Weijia Li, Zhutao Lv, Yi Lin, Jinhua Yu, Haote Yang, Conghui He</strong></p>
<p>Ground-to-aerial image synthesis focuses on generating realistic aerial images from corresponding ground street view images while maintaining consistent content layout, simulating a top-down view. The significant viewpoint difference leads to domain gaps between views, and dense urban scenes limit the visible range of street views, making this cross-view generation task particularly challenging. In this paper, we introduce SkyDiffusion, a novel cross-view generation method for synthesizing aerial images from street view images, utilizing a diffusion model and the Birdâ€™s-Eye View (BEV) paradigm. The Curved-BEV method in SkyDiffusion converts street-view images into a BEV perspective, effectively bridging the domain gap, and employs a â€œmulti-to-oneâ€ mapping strategy to address occlusion issues in dense urban scenes. Next, SkyDiffusion designed a BEV-guided diffusion model to generate content-consistent and realistic aerial images. Additionally, we introduce a novel dataset, Ground2Aerial-3, designed for diverse ground-to-aerial image synthesis applications, including disaster scene aerial synthesis, low-altitude UAV image synthesis, and historical high-resolution satellite image synthesis tasks. Experimental results demonstrate that SkyDiffusion outperforms state-of-the-art methods on cross-view datasets across natural (CVUSA), suburban (CVACT), urban (VIGOR-Chicago), and various application scenarios (G2A-3), achieving realistic and content-consistent aerial image generation. The code, datasets and more information of this work can be found at <a target="_blank" rel="noopener" href="https://opendatalab.github.io/skydiffusion/">https://opendatalab.github.io/skydiffusion/</a> . </p>
<blockquote>
<p>åœ°é¢åˆ°ç©ºä¸­çš„å›¾åƒåˆæˆä¸“æ³¨äºä»ç›¸åº”çš„åœ°é¢è¡—æ™¯å›¾åƒç”Ÿæˆé€¼çœŸçš„ç©ºä¸­å›¾åƒï¼ŒåŒæ—¶ä¿æŒå†…å®¹å¸ƒå±€çš„ä¸€è‡´æ€§ï¼Œæ¨¡æ‹Ÿä»ä¸Šåˆ°ä¸‹çš„è§†è§’ã€‚æ˜¾è‘—çš„è§†ç‚¹å·®å¼‚å¯¼è‡´äº†ä¸åŒè§†è§’ä¹‹é—´çš„é¢†åŸŸå·®è·ï¼Œè€Œå¯†é›†çš„åŸåŒºåœºæ™¯é™åˆ¶äº†è¡—æ™¯çš„å¯è§èŒƒå›´ï¼Œè¿™ä½¿å¾—è·¨è§†å›¾ç”Ÿæˆä»»åŠ¡ç‰¹åˆ«å…·æœ‰æŒ‘æˆ˜æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†SkyDiffusionï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„è·¨è§†å›¾ç”Ÿæˆæ–¹æ³•ï¼Œç”¨äºä»è¡—æ™¯å›¾åƒåˆæˆç©ºä¸­å›¾åƒï¼Œå®ƒåˆ©ç”¨æ‰©æ•£æ¨¡å‹å’Œé¸Ÿç°å›¾ï¼ˆBEVï¼‰èŒƒå¼ã€‚SkyDiffusionä¸­çš„Curved-BEVæ–¹æ³•å°†è¡—æ™¯å›¾åƒè½¬æ¢ä¸ºBEVè§†è§’ï¼Œæœ‰æ•ˆåœ°å¼¥åˆäº†é¢†åŸŸå·®è·ï¼Œå¹¶é‡‡ç”¨äº†â€œå¤šåˆ°ä¸€â€çš„æ˜ å°„ç­–ç•¥æ¥è§£å†³å¯†é›†åŸåŒºåœºæ™¯ä¸­çš„é®æŒ¡é—®é¢˜ã€‚æ¥ä¸‹æ¥ï¼ŒSkyDiffusionè®¾è®¡äº†ä¸€ä¸ªå—BEVå¼•å¯¼çš„æ‰©æ•£æ¨¡å‹æ¥ç”Ÿæˆå†…å®¹ä¸€è‡´ä¸”é€¼çœŸçš„ç©ºä¸­å›¾åƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä»‹ç»äº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†Ground2Aerial-3ï¼Œè¯¥æ•°æ®é›†ä¸“ä¸ºå¤šæ ·åŒ–çš„åœ°é¢åˆ°ç©ºä¸­å›¾åƒåˆæˆåº”ç”¨è€Œè®¾è®¡ï¼ŒåŒ…æ‹¬ç¾å®³åœºæ™¯ç©ºä¸­åˆæˆã€ä½ç©ºæ— äººæœºå›¾åƒåˆæˆä»¥åŠå†å²é«˜åˆ†è¾¨ç‡å«æ˜Ÿå›¾åƒåˆæˆä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSkyDiffusionåœ¨è·¨è§†å›¾æ•°æ®é›†ï¼ˆCVUSAè‡ªç„¶åœºæ™¯ã€CVACTéƒŠåŒºã€VIGOR-ChicagoåŸåŒºä»¥åŠG2A-3å„ç§åº”ç”¨åœºæ™¯ï¼‰ä¸Šçš„è¡¨ç°ä¼˜äºæœ€æ–°æ–¹æ³•ï¼Œå®ç°äº†é€¼çœŸå’Œå†…å®¹ä¸€è‡´æ€§çš„ç©ºä¸­å›¾åƒç”Ÿæˆã€‚æœ‰å…³è¿™é¡¹å·¥ä½œçš„ä»£ç ã€æ•°æ®é›†å’Œæ›´å¤šä¿¡æ¯ï¼Œè¯·è®¿é—®<a target="_blank" rel="noopener" href="https://opendatalab.github.io/skydiffusion/%E3%80%82">https://opendatalab.github.io/skydiffusion/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.01812v4">PDF</a> 10 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºåœ°é¢åˆ°ç©ºä¸­çš„å›¾åƒåˆæˆæŠ€æœ¯ï¼Œç ”ç©¶å›¢é˜Ÿå¼•å…¥äº†SkyDiffusionæ¨¡å‹ã€‚æ­¤æ¨¡å‹é€šè¿‡æ‰©æ•£æ¨¡å‹ç»“åˆé¸Ÿç°å›¾ï¼ˆBEVï¼‰èŒƒå¼ï¼Œå®ç°äº†ä»è¡—é“è§†è§’å›¾åƒç”Ÿæˆç©ºä¸­å›¾åƒçš„ä»»åŠ¡ã€‚SkyDiffusioné‡‡ç”¨Curved-BEVæ–¹æ³•è½¬æ¢è¡—é“è§†è§’å›¾åƒåˆ°BEVè§†è§’ï¼Œè§£å†³äº†è§†è§’å·®å¼‚å¸¦æ¥çš„é¢†åŸŸå·®è·é—®é¢˜ï¼Œå¹¶é‡‡ç”¨â€œå¤šå¯¹ä¸€â€æ˜ å°„ç­–ç•¥è§£å†³å¯†é›†åŸå¸‚åœºæ™¯ä¸­çš„é®æŒ¡é—®é¢˜ã€‚åŒæ—¶ï¼Œè¯¥æ¨¡å‹å±•ç¤ºäº†å¼ºå¤§çš„ç”Ÿæˆæ€§èƒ½ï¼Œèƒ½ç”Ÿæˆä¸€è‡´ä¸”é€¼çœŸçš„ç©ºä¸­å›¾åƒã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜å¼•å…¥äº†ä¸“ä¸ºåœ°é¢åˆ°ç©ºä¸­å›¾åƒåˆæˆåº”ç”¨è®¾è®¡çš„Ground2Aerial-3æ•°æ®é›†ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSkyDiffusionåœ¨è‡ªç„¶ã€éƒŠåŒºã€åŸå¸‚ç­‰ä¸åŒåœºæ™¯ä»¥åŠå¤šç§åº”ç”¨æƒ…å¢ƒä¸‹çš„è·¨è§†è§’æ•°æ®é›†ä¸Šå‡è¡¨ç°å‡ºè¶…è¶Šç°æœ‰æ–¹æ³•çš„æ•ˆæœã€‚è¯¦ç»†ä¿¡æ¯è¯·è®¿é—®å…¶å®˜ç½‘è·å–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SkyDiffusionæ˜¯ä¸€ä¸ªå®ç°ä»è¡—é“è§†è§’å›¾åƒç”Ÿæˆç©ºä¸­å›¾åƒçš„è·¨è§†è§’ç”Ÿæˆæ–¹æ³•ã€‚</li>
<li>åˆ©ç”¨æ‰©æ•£æ¨¡å‹å’Œé¸Ÿç°å›¾ï¼ˆBEVï¼‰èŒƒå¼è¿›è¡Œå›¾åƒåˆæˆã€‚</li>
<li>Curved-BEVæ–¹æ³•æœ‰æ•ˆè§£å†³äº†è§†è§’å·®å¼‚å¸¦æ¥çš„é¢†åŸŸå·®è·é—®é¢˜ã€‚</li>
<li>â€œå¤šå¯¹ä¸€â€æ˜ å°„ç­–ç•¥ç”¨äºå¤„ç†å¯†é›†åŸå¸‚åœºæ™¯ä¸­çš„é®æŒ¡é—®é¢˜ã€‚</li>
<li>å¼•å…¥Ground2Aerial-3æ•°æ®é›†ï¼Œé€‚ç”¨äºå¤šç§åœ°é¢åˆ°ç©ºä¸­å›¾åƒåˆæˆåº”ç”¨ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºSkyDiffusionåœ¨å¤šç§åœºæ™¯å’Œåº”ç”¨æƒ…å¢ƒä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.01812">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c254de20c6ac2391b00a3df58425e64b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0de97b051203f4e3c4ad002a56f19b7d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a195c69ae29391e688852407db881373.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c2442e406dc9c5e1e6a6cfd8f3680465.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36a2ba407f1991907046a66a7020eed2.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Paint-by-Inpaint-Learning-to-Add-Image-Objects-by-Removing-Them-First"><a href="#Paint-by-Inpaint-Learning-to-Add-Image-Objects-by-Removing-Them-First" class="headerlink" title="Paint by Inpaint: Learning to Add Image Objects by Removing Them First"></a>Paint by Inpaint: Learning to Add Image Objects by Removing Them First</h2><p><strong>Authors:Navve Wasserman, Noam Rotstein, Roy Ganz, Ron Kimmel</strong></p>
<p>Image editing has advanced significantly with the introduction of text-conditioned diffusion models. Despite this progress, seamlessly adding objects to images based on textual instructions without requiring user-provided input masks remains a challenge. We address this by leveraging the insight that removing objects (Inpaint) is significantly simpler than its inverse process of adding them (Paint), attributed to inpainting models that benefit from segmentation mask guidance. Capitalizing on this realization, by implementing an automated and extensive pipeline, we curate a filtered large-scale image dataset containing pairs of images and their corresponding object-removed versions. Using these pairs, we train a diffusion model to inverse the inpainting process, effectively adding objects into images. Unlike other editing datasets, ours features natural target images instead of synthetic ones while ensuring source-target consistency by construction. Additionally, we utilize a large Vision-Language Model to provide detailed descriptions of the removed objects and a Large Language Model to convert these descriptions into diverse, natural-language instructions. Our quantitative and qualitative results show that the trained model surpasses existing models in both object addition and general editing tasks. Visit our project page for the released dataset and trained models: <a target="_blank" rel="noopener" href="https://rotsteinnoam.github.io/Paint-by-Inpaint">https://rotsteinnoam.github.io/Paint-by-Inpaint</a>. </p>
<blockquote>
<p>å›¾åƒç¼–è¾‘éšç€æ–‡æœ¬æ¡ä»¶æ‰©æ•£æ¨¡å‹çš„å¼•å…¥è€Œå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚å°½ç®¡å¦‚æ­¤ï¼Œæ ¹æ®æ–‡æœ¬æŒ‡ä»¤æ— ç¼åœ°å‘å›¾åƒæ·»åŠ å¯¹è±¡ï¼Œè€Œæ— éœ€ç”¨æˆ·æä¾›è¾“å…¥æ©ç ï¼Œä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æˆ‘ä»¬é€šè¿‡åˆ©ç”¨è¿™æ ·ä¸€ä¸ªè§è§£æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼šå»é™¤ç‰©ä½“ï¼ˆè¡¥ç”»ï¼‰æ¯”åå‘æ·»åŠ ç‰©ä½“çš„è¿‡ç¨‹ï¼ˆç»˜ç”»ï¼‰è¦ç®€å•å¾—å¤šï¼Œè¿™å½’åŠŸäºå—ç›Šäºåˆ†å‰²æ©ç æŒ‡å¯¼çš„è¡¥ç”»æ¨¡å‹ã€‚åˆ©ç”¨è¿™ä¸€è®¤è¯†ï¼Œæˆ‘ä»¬é€šè¿‡å®ç°è‡ªåŠ¨åŒ–å’Œå¹¿æ³›çš„ç®¡é“ï¼Œæ•´ç†äº†ä¸€ä¸ªåŒ…å«å›¾åƒåŠå…¶ç›¸åº”å»å¯¹è±¡ç‰ˆæœ¬çš„å¤§å‹å›¾åƒæ•°æ®é›†ã€‚ä½¿ç”¨è¿™äº›æ•°æ®å¯¹ï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªæ‰©æ•£æ¨¡å‹æ¥åè½¬è¡¥ç”»è¿‡ç¨‹ï¼Œæœ‰æ•ˆåœ°å‘å›¾åƒä¸­æ·»åŠ å¯¹è±¡ã€‚ä¸å…¶ä»–ç¼–è¾‘æ•°æ®é›†ä¸åŒçš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ•°æ®é›†ä»¥è‡ªç„¶ç›®æ ‡å›¾åƒä¸ºç‰¹å¾ï¼ŒåŒæ—¶é€šè¿‡æ„å»ºç¡®ä¿æº-ç›®æ ‡çš„ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ©ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ä¸ºç§»é™¤çš„å¯¹è±¡æä¾›è¯¦ç»†æè¿°ï¼Œå¹¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å°†è¿™äº›æè¿°è½¬æ¢ä¸ºå¤šæ ·åŒ–ã€è‡ªç„¶è¯­è¨€æŒ‡ä»¤ã€‚æˆ‘ä»¬çš„å®šé‡å’Œå®šæ€§ç»“æœè¡¨æ˜ï¼Œè®­ç»ƒåçš„æ¨¡å‹åœ¨å¯¹è±¡æ·»åŠ å’Œä¸€èˆ¬ç¼–è¾‘ä»»åŠ¡ä¸Šè¶…è¿‡äº†ç°æœ‰æ¨¡å‹ã€‚æœ‰å…³å‘å¸ƒçš„æ•°æ®é›†å’Œè®­ç»ƒæ¨¡å‹ï¼Œè¯·è®¿é—®æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://rotsteinnoam.github.io/Paint-by-Inpaint%E3%80%82">https://rotsteinnoam.github.io/Paint-by-Inpaintã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.18212v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºæ–‡æœ¬æ¡ä»¶æ‰©æ•£æ¨¡å‹çš„æ–°å‹å›¾åƒç¼–è¾‘æŠ€æœ¯ã€‚é’ˆå¯¹åœ¨å›¾åƒä¸­æ ¹æ®æ–‡æœ¬æŒ‡ä»¤æ— ç¼æ·»åŠ å¯¹è±¡è¿™ä¸€æŒ‘æˆ˜ï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºäº†ä¸€ç§åˆ©ç”¨å»é®æŒ¡ï¼ˆInpaintï¼‰æŠ€æœ¯æ¥ç®€åŒ–å¯¹è±¡æ·»åŠ çš„æ–°æ–¹æ³•ã€‚ç ”ç©¶å›¢é˜Ÿåˆ›å»ºäº†ä¸€ä¸ªå¤§å‹å›¾åƒæ•°æ®é›†ï¼ŒåŒ…å«å›¾åƒåŠå…¶å¯¹åº”çš„å»é®æŒ¡ç‰ˆæœ¬ï¼Œå¹¶åˆ©ç”¨è¿™äº›æ•°æ®è®­ç»ƒæ‰©æ•£æ¨¡å‹ï¼Œé€†è½¬å»é®æŒ¡è¿‡ç¨‹å®ç°å¯¹è±¡æ·»åŠ ã€‚ç›¸è¾ƒäºå…¶ä»–ç¼–è¾‘æ•°æ®é›†ï¼Œè¯¥ç ”ç©¶çš„ç‰¹ç‚¹åœ¨äºé‡‡ç”¨è‡ªç„¶ç›®æ ‡å›¾åƒå¹¶ç¡®ä¿æºç›®æ ‡ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜ä½¿ç”¨äº†å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹å’Œå¤§å‹è¯­è¨€æ¨¡å‹æ¥æä¾›è¯¦ç»†çš„å¯¹è±¡æè¿°å’ŒæŒ‡ä»¤è½¬æ¢ã€‚è®­ç»ƒå‡ºçš„æ¨¡å‹åœ¨ç‰©ä½“æ·»åŠ å’Œä¸€èˆ¬ç¼–è¾‘ä»»åŠ¡ä¸Šå‡è¶…è¶Šäº†ç°æœ‰æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬æ¡ä»¶æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç¼–è¾‘é¢†åŸŸå–å¾—æ˜¾è‘—è¿›å±•ã€‚</li>
<li>åœ¨å›¾åƒä¸­æ ¹æ®æ–‡æœ¬æŒ‡ä»¤æ— ç¼æ·»åŠ å¯¹è±¡ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨å»é®æŒ¡æŠ€æœ¯ç®€åŒ–å¯¹è±¡æ·»åŠ è¿‡ç¨‹ã€‚</li>
<li>åˆ›å»ºäº†ä¸€ä¸ªå¤§å‹å›¾åƒæ•°æ®é›†ï¼ŒåŒ…å«å›¾åƒåŠå…¶å¯¹åº”çš„å»é®æŒ¡ç‰ˆæœ¬ã€‚</li>
<li>åˆ©ç”¨è¿™äº›æ•°æ®é›†è®­ç»ƒæ‰©æ•£æ¨¡å‹å®ç°å¯¹è±¡æ·»åŠ ã€‚</li>
<li>è¯¥ç ”ç©¶é‡‡ç”¨è‡ªç„¶ç›®æ ‡å›¾åƒï¼Œç¡®ä¿æºç›®æ ‡ä¸€è‡´æ€§ã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿä½¿ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹å’Œå¤§å‹è¯­è¨€æ¨¡å‹æä¾›è¯¦ç»†çš„å¯¹è±¡æè¿°å’ŒæŒ‡ä»¤è½¬æ¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.18212">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5953998cb5bc062ff14d91e45d03e4e8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-512de7ea95b15c5219a8b179a51422bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-523779e544c79d00d1b090b64c82f6ce.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ce78a46abbcfda33a093a80484ec2708.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-00c373105a235fe44c7a219664c68dd6.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Learning-from-Mistakes-Iterative-Prompt-Relabeling-for-Text-to-Image-Diffusion-Model-Training"><a href="#Learning-from-Mistakes-Iterative-Prompt-Relabeling-for-Text-to-Image-Diffusion-Model-Training" class="headerlink" title="Learning from Mistakes: Iterative Prompt Relabeling for Text-to-Image   Diffusion Model Training"></a>Learning from Mistakes: Iterative Prompt Relabeling for Text-to-Image   Diffusion Model Training</h2><p><strong>Authors:Xinyan Chen, Jiaxin Ge, Tianjun Zhang, Jiaming Liu, Shanghang Zhang</strong></p>
<p>Diffusion models have shown impressive performance in many domains. However, the modelâ€™s capability to follow natural language instructions (e.g., spatial relationships between objects, generating complex scenes) is still unsatisfactory. In this work, we propose Iterative Prompt Relabeling (IPR), a novel algorithm that aligns images to text through iterative image sampling and prompt relabeling with feedback. IPR first samples a batch of images conditioned on the text, then relabels the text prompts of unmatched text-image pairs with classifier feedback. We conduct thorough experiments on SDv2 and SDXL, testing their capability to follow instructions on spatial relations. With IPR, we improved up to 15.22% (absolute improvement) on the challenging spatial relation VISOR benchmark, demonstrating superior performance compared to previous RL methods. Our code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/xinyan-cxy/IPR-RLDF">https://github.com/xinyan-cxy/IPR-RLDF</a>. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨å¾ˆå¤šé¢†åŸŸéƒ½è¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œæ¨¡å‹éµå¾ªè‡ªç„¶è¯­è¨€æŒ‡ä»¤çš„èƒ½åŠ›ï¼ˆä¾‹å¦‚ï¼Œç‰©ä½“ä¹‹é—´çš„ç©ºé—´å…³ç³»ã€ç”Ÿæˆå¤æ‚åœºæ™¯ç­‰ï¼‰ä»ç„¶ä»¤äººä¸æ»¡æ„ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†è¿­ä»£æç¤ºé‡æ ‡è®°ï¼ˆIPRï¼‰ç®—æ³•ï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡è¿­ä»£å›¾åƒé‡‡æ ·å’Œå¸¦æœ‰åé¦ˆçš„æç¤ºé‡æ ‡è®°æ¥å¯¹é½å›¾åƒå’Œæ–‡æœ¬çš„æ–°å‹ç®—æ³•ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æ ¹æ®æ–‡æœ¬å¯¹ä¸€æ‰¹å›¾åƒè¿›è¡Œé‡‡æ ·ï¼Œç„¶ååˆ©ç”¨åˆ†ç±»å™¨åé¦ˆå¯¹æœªåŒ¹é…çš„æ–‡æœ¬å›¾åƒå¯¹çš„æ–‡æœ¬æç¤ºè¿›è¡Œé‡æ–°æ ‡è®°ã€‚æˆ‘ä»¬åœ¨SDv2å’ŒSDXLä¸Šè¿›è¡Œäº†å…¨é¢çš„å®éªŒï¼Œæµ‹è¯•äº†å®ƒä»¬åœ¨ç©ºé—´å…³ç³»æŒ‡ä»¤æ–¹é¢çš„èƒ½åŠ›ã€‚å€ŸåŠ©è¿­ä»£æç¤ºé‡æ ‡è®°ï¼ˆIPRï¼‰ï¼Œæˆ‘ä»¬åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ç©ºé—´å…³ç³»VISORåŸºå‡†æµ‹è¯•ä¸­æé«˜äº†é«˜è¾¾15.22%ï¼ˆç»å¯¹æå‡ï¼‰ï¼Œç›¸è¾ƒäºä¹‹å‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/xinyan-cxy/IPR-RLDF%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/xinyan-cxy/IPR-RLDFå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.16204v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†è¿­ä»£æç¤ºé‡æ ‡è®°ï¼ˆIterative Prompt Relabelingï¼Œç®€ç§°IPRï¼‰ç®—æ³•ï¼Œè¯¥ç®—æ³•é€šè¿‡è¿­ä»£å›¾åƒé‡‡æ ·å’Œæç¤ºé‡æ ‡è®°çš„æ–¹å¼å¯¹é½å›¾åƒå’Œæ–‡æœ¬ï¼Œæé«˜æ‰©æ•£æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€æŒ‡ä»¤éµå¾ªæ–¹é¢çš„æ€§èƒ½ã€‚åœ¨SDv2å’ŒSDXLçš„å®éªŒä¸­ï¼Œé€šè¿‡IPRç®—æ³•å¯¹ç©ºé—´å…³ç³»æŒ‡ä»¤çš„éµå¾ªèƒ½åŠ›æœ‰æ‰€æå‡ï¼Œå°¤å…¶åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„VISORåŸºå‡†æµ‹è¯•ä¸­ï¼Œç›¸è¾ƒäºå…ˆå‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œæå‡äº†é«˜è¾¾15.22%ï¼ˆç»å¯¹æå‡ï¼‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨ä¼—å¤šé¢†åŸŸè¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨éµå¾ªè‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼ˆå¦‚ç©ºé—´å…³ç³»å’Œåœºæ™¯ç”Ÿæˆï¼‰æ–¹é¢ä»æœ‰ä¸è¶³ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„ç®—æ³•â€”â€”è¿­ä»£æç¤ºé‡æ ‡è®°ï¼ˆIPRï¼‰ï¼Œè¯¥ç®—æ³•é€šè¿‡è¿­ä»£å›¾åƒé‡‡æ ·å’Œæç¤ºé‡æ ‡è®°ï¼Œå®ç°å¯¹å›¾åƒå’Œæ–‡æœ¬çš„å¯¹é½ã€‚</li>
<li>IPRç®—æ³•èƒ½æœ‰æ•ˆæå‡æ‰©æ•£æ¨¡å‹åœ¨ç©ºé—´å…³ç³»æŒ‡ä»¤éµå¾ªæ–¹é¢çš„æ€§èƒ½ã€‚</li>
<li>åœ¨SDv2å’ŒSDXLçš„å®éªŒä¸­éªŒè¯äº†IPRç®—æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„VISORåŸºå‡†æµ‹è¯•ä¸­ï¼Œç›¸è¾ƒäºå…ˆå‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œä½¿ç”¨IPRç®—æ³•çš„æ¨¡å‹æ€§èƒ½æœ‰æ˜¾è‘—æå‡ï¼Œæå‡äº†é«˜è¾¾15.22%ï¼ˆç»å¯¹æå‡ï¼‰ã€‚</li>
<li>è¯¥ç ”ç©¶çš„ä»£ç å·²å…¬å¼€å¯è®¿é—®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2312.16204">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-b2e2a1a50792ae10569d3db7b533d456.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3d57435fa3d2c6144bca20961be5aab7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-522fd13276f29c0f8c9bd0f5c1e3b27a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0dc5b2cbb1137d7d31b7bf20cac8eedd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bc54f988acd73ac22f8be0da7f003b8b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b61fa2382402e03e3bf1838fd609bea2.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-21/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-21/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-21/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-430b460135811c7ede99b5a295a3a404.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-21  FedSCA Federated Tuning with Similarity-guided Collaborative   Aggregation for Heterogeneous Medical Image Segmentation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-21
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-21/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-711b7c9275f5abdb371425b303b6ee25.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-21  GO-N3RDet Geometry Optimized NeRF-enhanced 3D Object Detector
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-21
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32714.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
