<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-21  SWEET-RL Training Multi-Turn LLM Agents on Collaborative Reasoning   Tasks">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-bf70a7dfc1ecb33cb7416364e83d1f6c.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-21
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    22k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    90 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-21-æ›´æ–°"><a href="#2025-03-21-æ›´æ–°" class="headerlink" title="2025-03-21 æ›´æ–°"></a>2025-03-21 æ›´æ–°</h1><h2 id="SWEET-RL-Training-Multi-Turn-LLM-Agents-on-Collaborative-Reasoning-Tasks"><a href="#SWEET-RL-Training-Multi-Turn-LLM-Agents-on-Collaborative-Reasoning-Tasks" class="headerlink" title="SWEET-RL: Training Multi-Turn LLM Agents on Collaborative Reasoning   Tasks"></a>SWEET-RL: Training Multi-Turn LLM Agents on Collaborative Reasoning   Tasks</h2><p><strong>Authors:Yifei Zhou, Song Jiang, Yuandong Tian, Jason Weston, Sergey Levine, Sainbayar Sukhbaatar, Xian Li</strong></p>
<p>Large language model (LLM) agents need to perform multi-turn interactions in real-world tasks. However, existing multi-turn RL algorithms for optimizing LLM agents fail to perform effective credit assignment over multiple turns while leveraging the generalization capabilities of LLMs and it remains unclear how to develop such algorithms. To study this, we first introduce a new benchmark, ColBench, where an LLM agent interacts with a human collaborator over multiple turns to solve realistic tasks in backend programming and frontend design. Building on this benchmark, we propose a novel RL algorithm, SWEET-RL (RL with Step-WisE Evaluation from Training-time information), that uses a carefully designed optimization objective to train a critic model with access to additional training-time information. The critic provides step-level rewards for improving the policy model. Our experiments demonstrate that SWEET-RL achieves a 6% absolute improvement in success and win rates on ColBench compared to other state-of-the-art multi-turn RL algorithms, enabling Llama-3.1-8B to match or exceed the performance of GPT4-o in realistic collaborative content creation. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†éœ€è¦åœ¨ç°å®ä¸–ç•Œçš„ä»»åŠ¡ä¸­è¿›è¡Œå¤šè½®äº¤äº’ã€‚ç„¶è€Œï¼Œç°æœ‰çš„é’ˆå¯¹LLMä»£ç†ä¼˜åŒ–çš„å¤šè½®å¼ºåŒ–å­¦ä¹ ç®—æ³•æœªèƒ½å……åˆ†åˆ©ç”¨LLMçš„é€šç”¨èƒ½åŠ›ï¼Œå¹¶åœ¨å¤šè½®äº¤äº’ä¸­æœ‰æ•ˆåœ°è¿›è¡Œä¿¡ç”¨åˆ†é…ï¼Œå¦‚ä½•å¼€å‘æ­¤ç±»ç®—æ³•ä»ä¸æ˜ç¡®ã€‚ä¸ºäº†ç ”ç©¶è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬é¦–å…ˆå¼•å…¥äº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ColBenchï¼Œåœ¨è¿™ä¸ªæµ‹è¯•ä¸­ï¼ŒLLMä»£ç†ä¸äººç±»çš„åˆä½œè€…è¿›è¡Œå¤šè½®äº¤äº’ï¼Œä»¥è§£å†³åç«¯ç¼–ç¨‹å’Œå‰ç«¯è®¾è®¡ç­‰ç°å®ä»»åŠ¡ã€‚åŸºäºè¿™ä¸ªåŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•SWEET-RLï¼ˆå¸¦æœ‰è®­ç»ƒæ—¶ä¿¡æ¯çš„ä¸€æ­¥æ˜æ™ºè¯„ä»·å¼ºåŒ–å­¦ä¹ ï¼‰ï¼Œå®ƒä½¿ç”¨ä¸€ä¸ªç²¾å¿ƒè®¾è®¡çš„ä¼˜åŒ–ç›®æ ‡æ¥è®­ç»ƒä¸€ä¸ªå…·æœ‰é¢å¤–è®­ç»ƒæ—¶ä¿¡æ¯è®¿é—®æƒé™çš„è¯„è®ºå®¶æ¨¡å‹ã€‚è¯„è®ºå®¶ä¸ºæ”¹å–„ç­–ç•¥æ¨¡å‹æä¾›æ­¥éª¤çº§åˆ«çš„å¥–åŠ±ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œä¸å…¶ä»–æœ€å…ˆè¿›çš„å¤šè½®å¼ºåŒ–å­¦ä¹ ç®—æ³•ç›¸æ¯”ï¼ŒSWEET-RLåœ¨ColBenchä¸Šçš„æˆåŠŸç‡å’Œèƒœç‡æé«˜äº†6%çš„ç»å¯¹å€¼ï¼Œè¿™ä½¿å¾—Llama 3.1-8Bèƒ½å¤Ÿåœ¨ç°å®åä½œå†…å®¹åˆ›å»ºä¸­åŒ¹é…æˆ–è¶…è¶ŠGPT4-oçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15478v1">PDF</a> 29 pages, 16 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šå›åˆäº¤äº’ä¸­çš„æ€§èƒ½æå‡æ˜¯ä¸€ä¸ªé‡è¦è¯¾é¢˜ã€‚é’ˆå¯¹ç°æœ‰ç®—æ³•åœ¨å¤šå›åˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸­å¯¹LLMçš„ä¿¡ç”¨åˆ†é…ä¸å¤Ÿæœ‰æ•ˆçš„é—®é¢˜ï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ColBenchã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæå‡ºäº†ä¸€ç§æ–°çš„RLç®—æ³•SWEET-RLï¼Œè¯¥ç®—æ³•åˆ©ç”¨ç²¾å¿ƒè®¾è®¡çš„ä¼˜åŒ–ç›®æ ‡è®­ç»ƒäº†ä¸€ä¸ªè¯„è®ºå®¶æ¨¡å‹ï¼Œå¯ä»¥åœ¨è®­ç»ƒé˜¶æ®µè·å¾—é¢å¤–ä¿¡æ¯ã€‚è¯„è®ºå®¶æ¨¡å‹æä¾›æ­¥éª¤çº§çš„å¥–åŠ±æ¥æ”¹è¿›ç­–ç•¥æ¨¡å‹ã€‚å®éªŒè¡¨æ˜ï¼Œä¸å…¶ä»–å…ˆè¿›çš„å¤šå›åˆRLç®—æ³•ç›¸æ¯”ï¼ŒSWEET-RLåœ¨ColBenchä¸Šçš„æˆåŠŸç‡æé«˜äº†6%ï¼Œå®ç°äº†é«˜æ•ˆçš„ç°å®ä»»åŠ¡åä½œè§£å†³ã€‚è¿™å¯¹äºå¦‚Llama 3.1-8Bè¿™æ ·çš„è¯­è¨€æ¨¡å‹åœ¨ååŒåˆ›ä½œä»»åŠ¡ä¸­è¶…è¶ŠGPT4-oçš„æ€§èƒ½å…·æœ‰é‡è¦æ„ä¹‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨å¤šå›åˆäº¤äº’ä¸­çš„æ€§èƒ½æå‡æ˜¯ç ”ç©¶çš„é‡ç‚¹ã€‚</li>
<li>å½“å‰çš„å¤šå›åˆRLç®—æ³•åœ¨ä¼˜åŒ–LLMæ—¶å­˜åœ¨ä¿¡ç”¨åˆ†é…é—®é¢˜ã€‚</li>
<li>å¼•å…¥æ–°çš„åŸºå‡†æµ‹è¯•ColBenchï¼Œç”¨äºæ¨¡æ‹ŸLLMä¸äººç±»çš„åä½œä»»åŠ¡ã€‚</li>
<li>æå‡ºæ–°çš„RLç®—æ³•SWEET-RLï¼Œé€šè¿‡è®­ç»ƒè¯„è®ºå®¶æ¨¡å‹å®ç°æ›´æœ‰æ•ˆçš„ä¿¡ç”¨åˆ†é…ã€‚</li>
<li>è¯„è®ºè€…æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­èƒ½å¤Ÿè·å–é¢å¤–çš„ä¿¡æ¯ï¼Œä¸ºç­–ç•¥æ¨¡å‹æä¾›æ­¥éª¤çº§åˆ«çš„å¥–åŠ±åé¦ˆã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼ŒSWEET-RLç›¸è¾ƒäºå…¶ä»–å¤šå›åˆRLç®—æ³•åœ¨ColBenchä¸Šæœ‰æ˜¾è‘—æå‡ã€‚</li>
<li>SWEET-RLæˆåŠŸè®©Llam3.aè¾¾åˆ°ç”šè‡³è¶…è¶ŠGPT 4åœ¨å¤šå›åˆååŒä»»åŠ¡ä¸­çš„æ€§èƒ½è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15478">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-103b1a90029ee8e84117c61fbdc67652.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e6116c0b8dbcca8d73715e625323a88b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-af458b77ecb3fb3477fb230aba167a6d.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Visual-Position-Prompt-for-MLLM-based-Visual-Grounding"><a href="#Visual-Position-Prompt-for-MLLM-based-Visual-Grounding" class="headerlink" title="Visual Position Prompt for MLLM based Visual Grounding"></a>Visual Position Prompt for MLLM based Visual Grounding</h2><p><strong>Authors:Wei Tang, Yanpeng Sun, Qinying Gu, Zechao Li</strong></p>
<p>Although Multimodal Large Language Models (MLLMs) excel at various image-related tasks, they encounter challenges in precisely aligning coordinates with spatial information within images, particularly in position-aware tasks such as visual grounding. This limitation arises from two key factors. First, MLLMs lack explicit spatial references, making it difficult to associate textual descriptions with precise image locations. Second, their feature extraction processes prioritize global context over fine-grained spatial details, leading to weak localization capability. To address this issue, we introduce VPP-LLaVA, an MLLM equipped with Visual Position Prompt (VPP) to improve its grounding capability. VPP-LLaVA integrates two complementary mechanisms. The global VPP overlays learnable, axis-like embeddings onto the input image to provide structured spatial cues. The local VPP focuses on fine-grained localization by incorporating position-aware queries, which suggests probable object locations. We also introduce a VPP-SFT dataset with 0.6M samples, consolidating high-quality visual grounding data into a compact format for efficient model training. Training on this dataset with VPP enhances the modelâ€™s performance, achieving state-of-the-art results on standard grounding benchmarks despite using fewer training samples compared to other MLLMs like MiniGPT-v2, which rely on much larger datasets ($\sim$21M samples). The code and VPP-SFT dataset will be available at <a target="_blank" rel="noopener" href="https://github.com/WayneTomas/VPP-LLaVA">https://github.com/WayneTomas/VPP-LLaVA</a> upon acceptance. </p>
<blockquote>
<p>å°½ç®¡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å„ç§å›¾åƒç›¸å…³ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ä¸å›¾åƒå†…ç©ºé—´ä¿¡æ¯çš„åæ ‡ç²¾ç¡®å¯¹é½æ–¹é¢å´é¢ä¸´æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½ç½®æ„ŸçŸ¥ä»»åŠ¡ï¼ˆå¦‚è§†è§‰å®šä½ï¼‰ä¸­ã€‚è¿™ä¸€é™åˆ¶æºäºä¸¤ä¸ªå…³é”®å› ç´ ã€‚é¦–å…ˆï¼ŒMLLMsç¼ºä¹æ˜ç¡®çš„ç©ºé—´å‚è€ƒï¼Œè¿™ä½¿å¾—å°†æ–‡æœ¬æè¿°ä¸ç²¾ç¡®å›¾åƒä½ç½®ç›¸å…³è”å˜å¾—å›°éš¾ã€‚å…¶æ¬¡ï¼Œå®ƒä»¬çš„ç‰¹å¾æå–è¿‡ç¨‹æ›´ä¾§é‡äºå…¨å±€ä¸Šä¸‹æ–‡è€Œéç²¾ç»†çš„ç©ºé—´ç»†èŠ‚ï¼Œå¯¼è‡´å®šä½èƒ½åŠ›è¾ƒå¼±ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†VPP-LLaVAï¼Œè¿™æ˜¯ä¸€ç§é…å¤‡è§†è§‰ä½ç½®æç¤ºï¼ˆVPPï¼‰çš„MLLMï¼Œä»¥æé«˜å…¶å®šä½èƒ½åŠ›ã€‚VPP-LLaVAé›†æˆäº†ä¸¤ç§äº’è¡¥æœºåˆ¶ã€‚å…¨å±€VPPå°†å¯å­¦ä¹ çš„ã€ç±»ä¼¼è½´çŠ¶çš„åµŒå…¥å åŠ åœ¨è¾“å…¥å›¾åƒä¸Šï¼Œä»¥æä¾›ç»“æ„åŒ–çš„ç©ºé—´çº¿ç´¢ã€‚å±€éƒ¨VPPåˆ™é€šè¿‡å¼•å…¥ä½ç½®æ„ŸçŸ¥æŸ¥è¯¢æ¥å…³æ³¨ç²¾ç»†å®šä½ï¼Œè¿™å¯ä»¥æŒ‡å‡ºå¯èƒ½çš„å¯¹è±¡ä½ç½®ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†åŒ…å«60ä¸‡æ ·æœ¬çš„VPP-SFTæ•°æ®é›†ï¼Œå°†é«˜è´¨é‡è§†è§‰å®šä½æ•°æ®æ•´åˆä¸ºç´§å‡‘æ ¼å¼ï¼Œä»¥ä¾¿è¿›è¡Œé«˜æ•ˆçš„æ¨¡å‹è®­ç»ƒã€‚ä½¿ç”¨è¯¥æ•°æ®é›†å’ŒVPPè¿›è¡Œè®­ç»ƒæé«˜äº†æ¨¡å‹çš„æ€§èƒ½ï¼Œåœ¨æ ‡å‡†å®šä½åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æœ€æ–°ç»“æœï¼Œå°½ç®¡ä¸å…¶ä»–MLLMï¼ˆå¦‚ä¾èµ–å¤§é‡æ•°æ®é›†ï¼ˆçº¦2100ä¸‡æ ·æœ¬ï¼‰çš„MiniGPT-v2ï¼‰ç›¸æ¯”ï¼Œä½¿ç”¨çš„è®­ç»ƒæ ·æœ¬æ•°é‡è¾ƒå°‘ã€‚ä»£ç å’ŒVPP-SFTæ•°æ®é›†ç»å®¡æ ¸é€šè¿‡åï¼Œå°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/WayneTomas/VPP-LLaVA">https://github.com/WayneTomas/VPP-LLaVA</a>ä¸Šæä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15426v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤„ç†å›¾åƒç›¸å…³ä»»åŠ¡æ—¶çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½ç½®æ„ŸçŸ¥ä»»åŠ¡ï¼ˆå¦‚è§†è§‰å®šä½ï¼‰ä¸­ç²¾ç¡®å¯¹é½åæ ‡ä¸ç©ºé—´ä¿¡æ¯çš„é—®é¢˜ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œå¼•å…¥äº†VPP-LLaVAæ¨¡å‹ï¼Œè¯¥æ¨¡å‹é…å¤‡äº†è§†è§‰ä½ç½®æç¤ºï¼ˆVPPï¼‰ä»¥æé«˜å…¶å®šä½èƒ½åŠ›ã€‚VPP-LLaVAé€šè¿‡å…¨å±€VPPå’Œå±€éƒ¨VPPä¸¤ç§äº’è¡¥æœºåˆ¶æ¥æ”¹è¿›æ¨¡å‹çš„ç©ºé—´æ„ŸçŸ¥èƒ½åŠ›ã€‚åŒæ—¶ï¼Œè¿˜å¼•å…¥äº†VPP-SFTæ•°æ®é›†è¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œå¹¶åœ¨æ ‡å‡†å®šä½åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLMsåœ¨å¤„ç†å›¾åƒç›¸å…³ä»»åŠ¡æ—¶é¢ä¸´ç²¾ç¡®å¯¹é½åæ ‡ä¸ç©ºé—´ä¿¡æ¯çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½ç½®æ„ŸçŸ¥ä»»åŠ¡ä¸­ã€‚</li>
<li>MLLMsç¼ºä¹æ˜ç¡®çš„ç©ºé—´å‚è€ƒï¼Œéš¾ä»¥å°†æ–‡æœ¬æè¿°ä¸ç²¾ç¡®å›¾åƒä½ç½®ç›¸å…³è”ã€‚</li>
<li>MLLMsçš„ç‰¹å¾æå–è¿‡ç¨‹æ³¨é‡å…¨å±€ä¸Šä¸‹æ–‡è€Œå¿½ç•¥ç»†å¾®çš„ç©ºé—´ç»†èŠ‚ï¼Œå¯¼è‡´å®šä½èƒ½åŠ›è¾ƒå¼±ã€‚</li>
<li>VPP-LLaVAæ¨¡å‹é€šè¿‡é›†æˆVisual Position Promptï¼ˆVPPï¼‰æ¥æé«˜MLLMsçš„å®šä½èƒ½åŠ›ã€‚</li>
<li>VPP-LLaVAåŒ…å«å…¨å±€VPPå’Œå±€éƒ¨VPPä¸¤ç§äº’è¡¥æœºåˆ¶ï¼Œåˆ†åˆ«æä¾›ç»“æ„åŒ–ç©ºé—´çº¿ç´¢å’Œç²¾ç»†å®šä½ã€‚</li>
<li>å¼•å…¥çš„VPP-SFTæ•°æ®é›†ç”¨äºé«˜æ•ˆæ¨¡å‹è®­ç»ƒï¼Œæé«˜äº†æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>VPP-LLaVAåœ¨æ ‡å‡†å®šä½åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æˆæœï¼Œä¸”ä½¿ç”¨è¾ƒå°‘çš„è®­ç»ƒæ ·æœ¬ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15426">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f660b9b04719a8ad39fdc163e3230f15.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ada5c00481fe63934a62e0a52be0969f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1c4d2ad5796fae7d9d6f6abcc8d40622.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3ac53aa31081a30f98892c391db65bd5.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Improving-Adversarial-Transferability-on-Vision-Transformers-via-Forward-Propagation-Refinement"><a href="#Improving-Adversarial-Transferability-on-Vision-Transformers-via-Forward-Propagation-Refinement" class="headerlink" title="Improving Adversarial Transferability on Vision Transformers via Forward   Propagation Refinement"></a>Improving Adversarial Transferability on Vision Transformers via Forward   Propagation Refinement</h2><p><strong>Authors:Yuchen Ren, Zhengyu Zhao, Chenhao Lin, Bo Yang, Lu Zhou, Zhe Liu, Chao Shen</strong></p>
<p>Vision Transformers (ViTs) have been widely applied in various computer vision and vision-language tasks. To gain insights into their robustness in practical scenarios, transferable adversarial examples on ViTs have been extensively studied. A typical approach to improving adversarial transferability is by refining the surrogate model. However, existing work on ViTs has restricted their surrogate refinement to backward propagation. In this work, we instead focus on Forward Propagation Refinement (FPR) and specifically refine two key modules of ViTs: attention maps and token embeddings. For attention maps, we propose Attention Map Diversification (AMD), which diversifies certain attention maps and also implicitly imposes beneficial gradient vanishing during backward propagation. For token embeddings, we propose Momentum Token Embedding (MTE), which accumulates historical token embeddings to stabilize the forward updates in both the Attention and MLP blocks. We conduct extensive experiments with adversarial examples transferred from ViTs to various CNNs and ViTs, demonstrating that our FPR outperforms the current best (backward) surrogate refinement by up to 7.0% on average. We also validate its superiority against popular defenses and its compatibility with other transfer methods. Codes and appendix are available at <a target="_blank" rel="noopener" href="https://github.com/RYC-98/FPR">https://github.com/RYC-98/FPR</a>. </p>
<blockquote>
<p>è§†è§‰Transformerï¼ˆViTsï¼‰å·²å¹¿æ³›åº”ç”¨äºå„ç§è®¡ç®—æœºè§†è§‰å’Œè§†è§‰è¯­è¨€ä»»åŠ¡ã€‚ä¸ºäº†æ·±å…¥äº†è§£å…¶åœ¨å®é™…åœºæ™¯ä¸­çš„ç¨³å¥æ€§ï¼ŒViTsä¸Šçš„å¯è½¬ç§»å¯¹æŠ—æ ·æœ¬å·²è¢«å¹¿æ³›ç ”ç©¶ã€‚æé«˜å¯¹æŠ—å¯è½¬ç§»æ€§çš„å…¸å‹æ–¹æ³•æ˜¯é€šè¿‡æ”¹è¿›æ›¿ä»£æ¨¡å‹ã€‚ç„¶è€Œï¼Œå…³äºViTsçš„ç°æœ‰å·¥ä½œå°†å…¶æ›¿ä»£ä¼˜åŒ–é™åˆ¶åœ¨åå‘ä¼ æ’­ä¸­ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä¸“æ³¨äºæ­£å‘ä¼ æ’­ç»†åŒ–ï¼ˆFPRï¼‰ï¼Œå¹¶ç‰¹åˆ«æ”¹è¿›ViTsçš„ä¸¤ä¸ªå…³é”®æ¨¡å—ï¼šæ³¨æ„åŠ›å›¾å’Œä»¤ç‰ŒåµŒå…¥ã€‚å¯¹äºæ³¨æ„åŠ›å›¾ï¼Œæˆ‘ä»¬æå‡ºäº†æ³¨æ„åŠ›å›¾å¤šæ ·åŒ–ï¼ˆAMDï¼‰ï¼Œå®ƒä½¿æŸäº›æ³¨æ„åŠ›å›¾å¤šæ ·åŒ–ï¼Œå¹¶åœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­éšå¼åœ°æ–½åŠ æœ‰ç›Šçš„æ¢¯åº¦æ¶ˆå¤±ã€‚å¯¹äºä»¤ç‰ŒåµŒå…¥ï¼Œæˆ‘ä»¬æå‡ºäº†åŠ¨é‡ä»¤ç‰ŒåµŒå…¥ï¼ˆMTEï¼‰ï¼Œå®ƒç´¯ç§¯å†å²ä»¤ç‰ŒåµŒå…¥ä»¥ç¨³å®šæ­£å‘æ›´æ–°ä¸­çš„æ³¨æ„åŠ›å’ŒMLPå—ã€‚æˆ‘ä»¬é€šè¿‡ä»ViTsè½¬ç§»åˆ°å„ç§CNNå’ŒViTsçš„å¯¹æŠ—æ ·æœ¬æ¥è¿›è¡Œå¤§é‡å®éªŒï¼Œç»“æœè¡¨æ˜æˆ‘ä»¬çš„FPRå¹³å‡æ¯”å½“å‰æœ€ä½³ï¼ˆåå‘ï¼‰æ›¿ä»£ä¼˜åŒ–é«˜å‡ºé«˜è¾¾7.0%ã€‚æˆ‘ä»¬è¿˜éªŒè¯äº†å…¶ä¼˜äºæµè¡Œçš„é˜²å¾¡æªæ–½ä»¥åŠä¸å…¶å®ƒè½¬ç§»æ–¹æ³•çš„å…¼å®¹æ€§ã€‚ä»£ç å’Œé™„å½•å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/RYC-98/FPR%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/RYC-98/FPRæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15404v1">PDF</a> CVPR2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†Vision Transformersï¼ˆViTsï¼‰åœ¨å®é™…åœºæ™¯ä¸­çš„é²æ£’æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¯¹æŠ—æ€§ä¾‹å­è½¬ç§»æ–¹é¢çš„æ€§èƒ½ã€‚ç°æœ‰å·¥ä½œä¸»è¦å…³æ³¨é€šè¿‡åå‘ä¼ æ’­å¯¹ViTsè¿›è¡Œæ¨¡å‹ç»†åŒ–æ¥æé«˜å¯¹æŠ—æ€§ä¾‹å­è½¬ç§»çš„èƒ½åŠ›ã€‚æœ¬æ–‡åˆ™ä¸“æ³¨äºæ­£å‘ä¼ æ’­ç»†åŒ–ï¼ˆFPRï¼‰ï¼Œå¹¶å…·ä½“é’ˆå¯¹ViTsçš„ä¸¤ä¸ªå…³é”®æ¨¡å—â€”â€”æ³¨æ„åŠ›å›¾å’Œä»¤ç‰ŒåµŒå…¥è¿›è¡Œæ”¹è¿›ã€‚é€šè¿‡æå‡ºæ³¨æ„åŠ›å›¾å¤šæ ·åŒ–å’ŒåŠ¨é‡ä»¤ç‰ŒåµŒå…¥æ–¹æ³•ï¼Œå®éªŒè¯æ˜FPRåœ¨ViTså¯¹CNNå’ŒViTsçš„å¯¹æŠ—æ€§ä¾‹å­è½¬ç§»ä¸Šçš„æ€§èƒ½ä¼˜äºå½“å‰æœ€ä½³çš„åå‘ä¼ æ’­ç»†åŒ–æ–¹æ³•ï¼Œå¹³å‡æé«˜çº¦7.0%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision Transformersï¼ˆViTsï¼‰åœ¨å®é™…åœºæ™¯ä¸­çš„é²æ£’æ€§å—åˆ°å¹¿æ³›å…³æ³¨ï¼Œç‰¹åˆ«æ˜¯åœ¨å¯¹æŠ—æ€§ä¾‹å­è½¬ç§»æ–¹é¢ã€‚</li>
<li>ç°æœ‰å·¥ä½œä¸»è¦é€šè¿‡åå‘ä¼ æ’­å¯¹ViTsè¿›è¡Œæ¨¡å‹ç»†åŒ–ä»¥æé«˜å¯¹æŠ—æ€§ä¾‹å­è½¬ç§»èƒ½åŠ›ï¼Œä½†æœ¬æ–‡æå‡ºæ­£å‘ä¼ æ’­ç»†åŒ–ï¼ˆFPRï¼‰æ–¹æ³•ã€‚</li>
<li>FPRé’ˆå¯¹ViTsçš„ä¸¤ä¸ªå…³é”®æ¨¡å—ï¼šæ³¨æ„åŠ›å›¾å’Œä»¤ç‰ŒåµŒå…¥è¿›è¡Œæ”¹è¿›ã€‚</li>
<li>æå‡ºæ³¨æ„åŠ›å›¾å¤šæ ·åŒ–ï¼ˆAMDï¼‰æ–¹æ³•ï¼Œä¸ä»…å¤šæ ·åŒ–æ³¨æ„åŠ›å›¾ï¼Œè¿˜éšå¼åœ°æ–½åŠ æœ‰ç›Šçš„æ¢¯åº¦æ¶ˆå¤±åœ¨åå‘ä¼ æ’­ä¸­ã€‚</li>
<li>æå‡ºåŠ¨é‡ä»¤ç‰ŒåµŒå…¥ï¼ˆMTEï¼‰æ–¹æ³•ï¼Œç´¯ç§¯å†å²ä»¤ç‰ŒåµŒå…¥ä»¥ç¨³å®šæ­£å‘æ›´æ–°ä¸­çš„æ³¨æ„åŠ›å’ŒMLPå—ã€‚</li>
<li>å®éªŒè¯æ˜FPRåœ¨ViTså¯¹æŠ—å¤šç§CNNå’ŒViTsçš„å¯¹æŠ—æ€§ä¾‹å­è½¬ç§»ä¸Šçš„æ€§èƒ½ä¼˜äºå½“å‰æœ€ä½³æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15404">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-514fff358c70b28ed5da1774f2c61db4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f1fdb81c7064e8da08a796bb3f73123.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-30b2919417e33a6ce218aec0b5122768.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fee57a21cd378d5eb2abbf3b7c6a1291.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Real-world-validation-of-a-multimodal-LLM-powered-pipeline-for-High-Accuracy-Clinical-Trial-Patient-Matching-leveraging-EHR-data"><a href="#Real-world-validation-of-a-multimodal-LLM-powered-pipeline-for-High-Accuracy-Clinical-Trial-Patient-Matching-leveraging-EHR-data" class="headerlink" title="Real-world validation of a multimodal LLM-powered pipeline for   High-Accuracy Clinical Trial Patient Matching leveraging EHR data"></a>Real-world validation of a multimodal LLM-powered pipeline for   High-Accuracy Clinical Trial Patient Matching leveraging EHR data</h2><p><strong>Authors:Anatole Callies, Quentin Bodinier, Philippe Ravaud, Kourosh Davarpanah</strong></p>
<p>Background: Patient recruitment in clinical trials is hindered by complex eligibility criteria and labor-intensive chart reviews. Prior research using text-only models have struggled to address this problem in a reliable and scalable way due to (1) limited reasoning capabilities, (2) information loss from converting visual records to text, and (3) lack of a generic EHR integration to extract patient data.   Methods: We introduce a broadly applicable, integration-free, LLM-powered pipeline that automates patient-trial matching using unprocessed documents extracted from EHRs. Our approach leverages (1) the new reasoning-LLM paradigm, enabling the assessment of even the most complex criteria, (2) visual capabilities of latest LLMs to interpret medical records without lossy image-to-text conversions, and (3) multimodal embeddings for efficient medical record search. The pipeline was validated on the n2c2 2018 cohort selection dataset (288 diabetic patients) and a real-world dataset composed of 485 patients from 30 different sites matched against 36 diverse trials.   Results: On the n2c2 dataset, our method achieved a new state-of-the-art criterion-level accuracy of 93%. In real-world trials, the pipeline yielded an accuracy of 87%, undermined by the difficulty to replicate human decision-making when medical records lack sufficient information. Nevertheless, users were able to review overall eligibility in under 9 minutes per patient on average, representing an 80% improvement over traditional manual chart reviews.   Conclusion: This pipeline demonstrates robust performance in clinical trial patient matching without requiring custom integration with site systems or trial-specific tailoring, thereby enabling scalable deployment across sites seeking to leverage AI for patient matching. </p>
<blockquote>
<p>èƒŒæ™¯ï¼šä¸´åºŠè¯•éªŒä¸­çš„æ‚£è€…æ‹›å‹Ÿå—åˆ°å¤æ‚çš„å…¥é€‰æ ‡å‡†å’Œç¹ççš„ç—…å†å®¡æŸ¥çš„é˜»ç¢ã€‚å…ˆå‰ä»…ä½¿ç”¨æ–‡æœ¬æ¨¡å‹çš„ç ”ç©¶ç”±äºï¼ˆ1ï¼‰æœ‰é™çš„æ¨ç†èƒ½åŠ›ï¼Œï¼ˆ2ï¼‰å°†è§†è§‰è®°å½•è½¬æ¢ä¸ºæ–‡æœ¬æ—¶çš„ä¿¡æ¯ä¸¢å¤±ï¼Œä»¥åŠï¼ˆ3ï¼‰ç¼ºä¹é€šç”¨çš„ç”µå­å¥åº·è®°å½•ï¼ˆEHRï¼‰é›†æˆä»¥æå–æ‚£è€…æ•°æ®ï¼Œå› æ­¤éš¾ä»¥ä»¥ä¸€ç§å¯é å’Œå¯æ‰©å±•çš„æ–¹å¼è§£å†³è¿™ä¸ªé—®é¢˜ã€‚æ–¹æ³•ï¼šæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§é€šç”¨ã€æ— éœ€é›†æˆã€ç”±å¤§å‹è¯­è¨€æ¨¡å‹é©±åŠ¨çš„ç®¡é“ï¼Œè¯¥ç®¡é“ä½¿ç”¨ä»EHRä¸­æå–çš„æœªå¤„ç†æ–‡æ¡£è‡ªåŠ¨è¿›è¡Œæ‚£è€…ä¸è¯•éªŒçš„åŒ¹é…ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨ï¼ˆ1ï¼‰æ–°çš„åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èŒƒå¼ï¼Œèƒ½å¤Ÿè¯„ä¼°ç”šè‡³æ˜¯æœ€å¤æ‚çš„æ ‡å‡†ï¼Œï¼ˆ2ï¼‰æœ€æ–°å¤§å‹è¯­è¨€æ¨¡å‹çš„è§†è§‰åŠŸèƒ½ï¼Œèƒ½å¤Ÿè§£é‡ŠåŒ»ç–—è®°å½•è€Œæ— éœ€æœ‰æŸçš„å›¾åƒåˆ°æ–‡æœ¬çš„è½¬æ¢ï¼Œä»¥åŠï¼ˆ3ï¼‰ç”¨äºé«˜æ•ˆåŒ»ç–—è®°å½•æœç´¢çš„å¤šæ¨¡å¼åµŒå…¥ã€‚è¯¥ç®¡é“åœ¨n2c2 2018å¹´é˜Ÿåˆ—é€‰æ‹©æ•°æ®é›†ï¼ˆ288åç³–å°¿ç—…æ‚£è€…ï¼‰å’Œç”±æ¥è‡ª30ä¸ªä¸åŒç«™ç‚¹çš„485åæ‚£è€…ä¸36ç§ä¸åŒè¯•éªŒç›¸åŒ¹é…çš„å®é™…æ•°æ®é›†ä¸Šè¿›è¡Œäº†éªŒè¯ã€‚ç»“æœï¼šåœ¨n2c2æ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†æ ‡å‡†çº§åˆ«çš„å‡†ç¡®ç‡ä¸º93%ï¼Œè¾¾åˆ°äº†æ–°çš„ä¸šç•Œæœ€ä½³æ°´å¹³ã€‚åœ¨ç°å®ä¸–ç•Œçš„è¯•éªŒä¸­ï¼Œç®¡é“çš„å‡†ç¡®æ€§ä¸º87%ï¼Œå—åˆ°åŒ»ç–—è®°å½•ä¿¡æ¯ä¸è¶³å¯¼è‡´éš¾ä»¥å¤åˆ¶äººç±»å†³ç­–çš„å½±å“ã€‚å°½ç®¡å¦‚æ­¤ï¼Œç”¨æˆ·å¹³å‡èƒ½åœ¨ä¸åˆ°9åˆ†é’Ÿå†…å®¡æŸ¥æ¯ä½æ‚£è€…çš„æ•´ä½“èµ„æ ¼ï¼Œç›¸å¯¹äºä¼ ç»Ÿçš„æ‰‹åŠ¨ç—…å†å®¡æŸ¥æé«˜äº†80%ã€‚ç»“è®ºï¼šè¯¥ç®¡é“åœ¨ä¸´åºŠè¯•éªŒæ‚£è€…åŒ¹é…æ–¹é¢è¡¨ç°å‡ºç¨³å¥çš„æ€§èƒ½ï¼Œæ— éœ€ä¸ç«™ç‚¹ç³»ç»Ÿå®šåˆ¶é›†æˆæˆ–é’ˆå¯¹ç‰¹å®šè¯•éªŒå®šåˆ¶è°ƒæ•´ï¼Œä»è€Œèƒ½å¤Ÿåœ¨å¯»æ±‚åˆ©ç”¨äººå·¥æ™ºèƒ½è¿›è¡Œæ‚£è€…åŒ¹é…çš„ç«™ç‚¹ä¸­å®ç°è·¨ç«™ç‚¹å¯æ‰©å±•éƒ¨ç½²ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15374v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºLLMçš„æ— éœ€æ•´åˆçš„è‡ªåŠ¨åŒ–æ‚£è€…è¯•éªŒåŒ¹é…ç®¡é“ï¼Œå¯ç›´æ¥å¤„ç†æ¥è‡ªç”µå­å¥åº·è®°å½•ï¼ˆEHRï¼‰çš„æœªå¤„ç†æ–‡æ¡£ã€‚è¯¥ç®¡é“ä½¿ç”¨æ–°çš„æ¨ç†æ¨¡å‹ï¼Œå…·æœ‰è¯„ä¼°å¤æ‚æ ‡å‡†çš„èƒ½åŠ›ï¼Œå¹¶å€ŸåŠ©æœ€æ–°LLMçš„è§†è§‰åŠŸèƒ½ï¼Œæ— éœ€è¿›è¡Œå›¾åƒåˆ°æ–‡æœ¬çš„è½¬æ¢å³å¯è§£é‡ŠåŒ»ç–—è®°å½•ã€‚è¯¥ç®¡é“åœ¨n2c2 2018é˜Ÿåˆ—é€‰æ‹©æ•°æ®é›†å’Œç”±æ¥è‡ªä¸åŒè¯•éªŒçš„485åæ‚£è€…ç»„æˆçš„çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œæ˜¾ç¤ºå‡ºè‰¯å¥½çš„å‡†ç¡®æ€§ã€‚æ€»ä½“è€Œè¨€ï¼Œè¯¥ç®¡é“ä¸ºæ‚£è€…åŒ¹é…æä¾›äº†ä¸€ä¸ªæœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œæ— éœ€ä¸ç‰¹å®šç«™ç‚¹ç³»ç»Ÿè¿›è¡Œå®šåˆ¶é›†æˆæˆ–é’ˆå¯¹ç‰¹å®šè¯•éªŒè¿›è¡Œè°ƒæ•´ï¼Œä»è€Œèƒ½å¤Ÿåœ¨å¯»æ±‚åˆ©ç”¨äººå·¥æ™ºèƒ½è¿›è¡Œæ‚£è€…åŒ¹é…çš„ç«™ç‚¹ä¹‹é—´å®ç°è§„æ¨¡åŒ–éƒ¨ç½²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†ä¸€ç§åŸºäºLLMçš„è‡ªåŠ¨åŒ–æ‚£è€…è¯•éªŒåŒ¹é…ç®¡é“ï¼Œè§£å†³äº†ä¸´åºŠè¯•éªŒä¸­æ‚£è€…æ‹›å‹Ÿçš„éš¾é¢˜ã€‚</li>
<li>è¯¥ç®¡é“å¯ä»¥ç›´æ¥å¤„ç†æ¥è‡ªç”µå­å¥åº·è®°å½•ï¼ˆEHRï¼‰çš„æœªå¤„ç†æ–‡æ¡£ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ€§ã€‚</li>
<li>åˆ©ç”¨æ–°çš„æ¨ç†æ¨¡å‹ã€LLMçš„è§†è§‰åŠŸèƒ½ä»¥åŠå¤šæ¨¡æ€åµŒå…¥æŠ€æœ¯ï¼Œä½¿å¾—ç®¡é“èƒ½å¤Ÿè¯„ä¼°å¤æ‚æ ‡å‡†ã€è§£é‡ŠåŒ»ç–—è®°å½•å¹¶é«˜æ•ˆæœç´¢åŒ»ç–—è®°å½•ã€‚</li>
<li>åœ¨n2c2æ•°æ®é›†å’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„éªŒè¯ç»“æœè¡¨æ˜ï¼Œè¯¥ç®¡é“å…·æœ‰è‰¯å¥½çš„å‡†ç¡®æ€§ã€‚</li>
<li>ä¸ä¼ ç»Ÿçš„æ‰‹åŠ¨å®¡æŸ¥ç›¸æ¯”ï¼Œè¯¥ç®¡é“æ˜¾è‘—æé«˜äº†æ‚£è€…èµ„æ ¼å®¡æŸ¥çš„æ•ˆç‡ã€‚</li>
<li>è¯¥ç®¡é“æ— éœ€ç‰¹å®šé›†æˆæˆ–é’ˆå¯¹ç‰¹å®šè¯•éªŒè¿›è¡Œå®šåˆ¶è°ƒæ•´ï¼Œä¾¿äºåœ¨å¤šä¸ªç«™ç‚¹ä¹‹é—´å®ç°è§„æ¨¡åŒ–éƒ¨ç½²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15374">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a552452299ae77997ec9177c9441ab3a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1a760eb97bb85aeeffa5e45c30ab901f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-29fff8a8e843f08ad215771e45986a0a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bf70a7dfc1ecb33cb7416364e83d1f6c.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="EfficientLLaVA-Generalizable-Auto-Pruning-for-Large-Vision-language-Models"><a href="#EfficientLLaVA-Generalizable-Auto-Pruning-for-Large-Vision-language-Models" class="headerlink" title="EfficientLLaVA:Generalizable Auto-Pruning for Large Vision-language   Models"></a>EfficientLLaVA:Generalizable Auto-Pruning for Large Vision-language   Models</h2><p><strong>Authors:Yinan Liang, Ziwei Wang, Xiuwei Xu, Jie Zhou, Jiwen Lu</strong></p>
<p>While multimodal large language models demonstrate strong performance in complex reasoning tasks, they pose significant challenges related to model complexity during deployment, especially for resource-limited devices. In this paper, we propose an automatic pruning method for large vision-language models to enhance the efficiency of multimodal reasoning. Conventional methods rely on the training data of the original model to select the proper pruning ratio for different network components. However, these methods are impractical for large vision-language models due to the unaffordable search costs caused by web-scale training corpus. In contrast, our approach only leverages a small number of samples to search for the desired pruning policy by maximizing its generalization ability on unknown training data while maintaining the model accuracy, which enables the achievement of an optimal trade-off between accuracy and efficiency for large visual language models. Specifically, we formulate the generalization gap of the pruning strategy using the structural risk minimization principle. Based on both task performance and generalization capability, we iteratively search for the optimal pruning policy within a given search space and optimize the vision projector to evolve the search space with higher upper bound of performance. We conduct extensive experiments on the ScienceQA, Vizwiz, MM-vet, and LLaVA-Bench datasets for the task of visual question answering. Using only 64 samples for pruning policy search, EfficientLLaVA achieves an accuracy of 83.05% on ScienceQA, along with a $\times$ 1.8 speedup compared to the dense LLaVA-v1.5-7B model. </p>
<blockquote>
<p>å°½ç®¡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚çš„æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†åœ¨éƒ¨ç½²æœŸé—´ï¼Œå°¤å…¶æ˜¯èµ„æºæœ‰é™çš„è®¾å¤‡ä¸Šï¼Œå®ƒä»¬ä¸æ¨¡å‹å¤æ‚æ€§ç›¸å…³çš„æŒ‘æˆ˜å°¤ä¸ºæ˜¾è‘—ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é’ˆå¯¹å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„è‡ªåŠ¨å‰ªææ–¹æ³•ï¼Œä»¥æé«˜å¤šæ¨¡æ€æ¨ç†çš„æ•ˆç‡ã€‚ä¼ ç»Ÿæ–¹æ³•ä¾èµ–äºåŸå§‹æ¨¡å‹çš„è®­ç»ƒæ•°æ®æ¥é€‰æ‹©ä¸åŒç½‘ç»œç»„ä»¶çš„é€‚å½“å‰ªæç‡ã€‚ç„¶è€Œï¼Œç”±äºç½‘ç»œè§„æ¨¡è®­ç»ƒè¯­æ–™åº“å¯¼è‡´çš„æœç´¢æˆæœ¬è¿‡é«˜ï¼Œè¿™äº›æ–¹æ³•å¯¹äºå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹æ¥è¯´å¹¶ä¸å®ç”¨ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä»…åˆ©ç”¨å°‘é‡æ ·æœ¬ï¼Œé€šè¿‡æœ€å¤§åŒ–å…¶åœ¨æœªçŸ¥è®­ç»ƒæ•°æ®ä¸Šçš„æ³›åŒ–èƒ½åŠ›æ¥æœç´¢æ‰€éœ€çš„å‰ªæç­–ç•¥ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹ç²¾åº¦ï¼Œè¿™ä½¿å¾—å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç²¾åº¦å’Œæ•ˆç‡ä¹‹é—´å®ç°æœ€ä½³æƒè¡¡ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä½¿ç”¨ç»“æ„é£é™©æœ€å°åŒ–åŸåˆ™æ¥åˆ¶å®šå‰ªæç­–ç•¥çš„æ³›åŒ–å·®è·ã€‚åŸºäºä»»åŠ¡æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ï¼Œæˆ‘ä»¬åœ¨ç»™å®šçš„æœç´¢ç©ºé—´å†…è¿­ä»£æœç´¢æœ€ä½³å‰ªæç­–ç•¥ï¼Œå¹¶ä¼˜åŒ–è§†è§‰æŠ•å½±ä»ªä»¥åœ¨æ›´é«˜çš„æ€§èƒ½ä¸Šé™å†…è¿›åŒ–æœç´¢ç©ºé—´ã€‚æˆ‘ä»¬åœ¨ScienceQAã€Vizwizã€MM-vetå’ŒLLaVA-Benchæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§é‡å…³äºè§†è§‰é—®ç­”ä»»åŠ¡çš„å®éªŒã€‚ä»…ä½¿ç”¨64ä¸ªæ ·æœ¬è¿›è¡Œå‰ªæç­–ç•¥æœç´¢ï¼ŒEfficientLLaVAåœ¨ScienceQAä¸Šçš„å‡†ç¡®ç‡ä¸º83.05%ï¼Œå¹¶ä¸”ç›¸å¯¹äºå¯†é›†LLaVA-v1.5-7Bæ¨¡å‹å®ç°äº†Ã—1.8çš„åŠ é€Ÿã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15369v1">PDF</a> Accepted by CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§é’ˆå¯¹å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„è‡ªåŠ¨å‰ªææ–¹æ³•ï¼Œä»¥æé«˜å¤šæ¨¡æ€æ¨ç†çš„æ•ˆç‡ã€‚ä¸åŒäºä¾èµ–åŸå§‹æ¨¡å‹è®­ç»ƒæ•°æ®è¿›è¡Œå‰ªææ¯”ä¾‹é€‰æ‹©çš„æ–¹æ³•ï¼Œæœ¬æ–‡æ–¹æ³•ä»…ä½¿ç”¨å°‘é‡æ ·æœ¬æœç´¢ç†æƒ³çš„å‰ªæç­–ç•¥ï¼Œé€šè¿‡æœ€å¤§åŒ–æœªçŸ¥è®­ç»ƒæ•°æ®ä¸Šçš„æ³›åŒ–èƒ½åŠ›åŒæ—¶ä¿æŒæ¨¡å‹ç²¾åº¦ï¼Œå®ç°å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç²¾åº¦å’Œæ•ˆç‡ä¹‹é—´çš„æœ€ä¼˜æƒè¡¡ã€‚å®éªŒåœ¨å¤šä¸ªæ•°æ®é›†ä¸ŠéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†åœ¨éƒ¨ç½²æ—¶é¢ä¸´æ¨¡å‹å¤æ‚æ€§å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨èµ„æºæœ‰é™çš„è®¾å¤‡ä¸Šã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„è‡ªåŠ¨å‰ªææ–¹æ³•ï¼Œæ—¨åœ¨æé«˜æ¨¡å‹æ•ˆç‡ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•ä¾èµ–åŸå§‹æ¨¡å‹çš„è®­ç»ƒæ•°æ®è¿›è¡Œå‰ªææ¯”ä¾‹é€‰æ‹©ï¼Œä½†è¿™ç§æ–¹æ³•å¯¹äºå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹æ¥è¯´ä¸å®ç”¨ï¼Œå› ä¸ºæœç´¢æˆæœ¬é«˜æ˜‚ã€‚</li>
<li>æœ¬æ–‡æ–¹æ³•ä»…ä½¿ç”¨å°‘é‡æ ·æœ¬æœç´¢ç†æƒ³çš„å‰ªæç­–ç•¥ï¼Œé€šè¿‡æœ€å¤§åŒ–æœªçŸ¥è®­ç»ƒæ•°æ®ä¸Šçš„æ³›åŒ–èƒ½åŠ›æ¥ä¿æŒæ¨¡å‹ç²¾åº¦ã€‚</li>
<li>é€šè¿‡åˆ¶å®šå‰ªæç­–ç•¥æ³›åŒ–å·®è·çš„ç»“æ„é£é™©æœ€å°åŒ–åŸåˆ™ï¼Œå®ç°äº†æ¨¡å‹ç²¾åº¦å’Œæ•ˆç‡ä¹‹é—´çš„æœ€ä¼˜æƒè¡¡ã€‚</li>
<li>å®éªŒåœ¨å¤šä¸ªæ•°æ®é›†ä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå…¶ä¸­EfficientLLaVAåœ¨ScienceQAæ•°æ®é›†ä¸Šè¾¾åˆ°83.05%çš„å‡†ç¡®ç‡ï¼Œå¹¶å®ç°ä¸å¯†é›†æ¨¡å‹ç›¸æ¯”çš„1.8å€é€Ÿåº¦æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15369">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0ce48a02681eb587dad0022748e838f8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d5d3a4621f66448b533726c14b947ec9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8612c0afd8825a112e1ddc990343f5e6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8d75c3e669e48656d4d8ed00182dec9f.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="SPILL-Domain-Adaptive-Intent-Clustering-based-on-Selection-and-Pooling-with-Large-Language-Models"><a href="#SPILL-Domain-Adaptive-Intent-Clustering-based-on-Selection-and-Pooling-with-Large-Language-Models" class="headerlink" title="SPILL: Domain-Adaptive Intent Clustering based on Selection and Pooling   with Large Language Models"></a>SPILL: Domain-Adaptive Intent Clustering based on Selection and Pooling   with Large Language Models</h2><p><strong>Authors:I-Fan Lin, Faegheh Hasibi, Suzan Verberne</strong></p>
<p>In this paper, we propose Selection and Pooling with Large Language Models (SPILL), an intuitive and domain-adaptive method for intent clustering without fine-tuning. Existing embeddings-based clustering methods rely on a few labeled examples or unsupervised fine-tuning to optimize results for each new dataset, which makes them less generalizable to multiple datasets. Our goal is to make these existing embedders more generalizable to new domain datasets without further fine-tuning. Inspired by our theoretical derivation and simulation results on the effectiveness of sampling and pooling techniques, we view the clustering task as a small-scale selection problem. A good solution to this problem is associated with better clustering performance. Accordingly, we propose a two-stage approach: First, for each utterance (referred to as the seed), we derive its embedding using an existing embedder. Then, we apply a distance metric to select a pool of candidates close to the seed. Because the embedder is not optimized for new datasets, in the second stage, we use an LLM to further select utterances from these candidates that share the same intent as the seed. Finally, we pool these selected candidates with the seed to derive a refined embedding for the seed. We found that our method generally outperforms directly using an embedder, and it achieves comparable results to other state-of-the-art studies, even those that use much larger models and require fine-tuning, showing its strength and efficiency. Our results indicate that our method enables existing embedders to be further improved without additional fine-tuning, making them more adaptable to new domain datasets. Additionally, viewing the clustering task as a small-scale selection problem gives the potential of using LLMs to customize clustering tasks according to the userâ€™s goals. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„é€‰æ‹©æ± åŒ–ï¼ˆSPILLï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§ç›´è§‚ä¸”é€‚ç”¨äºç‰¹å®šé¢†åŸŸçš„æ„å›¾èšç±»æ–¹æ³•ï¼Œæ— éœ€å¾®è°ƒã€‚ç°æœ‰çš„åŸºäºåµŒå…¥çš„èšç±»æ–¹æ³•ä¾èµ–äºå°‘é‡å¸¦æ ‡ç­¾çš„æ ·æœ¬æˆ–æ— ç›‘ç£å¾®è°ƒæ¥ä¼˜åŒ–æ¯ä¸ªæ–°æ•°æ®é›†çš„ç»“æœï¼Œè¿™é™ä½äº†å®ƒä»¬åœ¨æ–°æ•°æ®é›†ä¸Šçš„é€šç”¨æ€§ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯åœ¨æ— éœ€è¿›ä¸€æ­¥å¾®è°ƒçš„æƒ…å†µä¸‹ï¼Œä½¿è¿™äº›ç°æœ‰åµŒå…¥å™¨å¯¹æ–°é¢†åŸŸæ•°æ®é›†æ›´å…·é€šç”¨æ€§ã€‚å—é‡‡æ ·å’Œæ± åŒ–æŠ€æœ¯æœ‰æ•ˆæ€§ç†è®ºæ¨å¯¼å’Œä»¿çœŸç»“æœçš„å¯å‘ï¼Œæˆ‘ä»¬å°†èšç±»ä»»åŠ¡è§†ä¸ºä¸€ä¸ªå°è§„æ¨¡çš„é€‰æ‹©é—®é¢˜ã€‚å¥½çš„è§£å†³æ–¹æ¡ˆä¸æ›´å¥½çš„èšç±»æ€§èƒ½ç›¸å…³ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„æ–¹æ³•ï¼šé¦–å…ˆï¼Œé’ˆå¯¹æ¯æ¡è¯è¯­ï¼ˆç§°ä¸ºç§å­ï¼‰ï¼Œæˆ‘ä»¬ä½¿ç”¨ç°æœ‰çš„åµŒå…¥å™¨è·å–å…¶åµŒå…¥è¡¨ç¤ºã€‚ç„¶åï¼Œæˆ‘ä»¬åº”ç”¨è·ç¦»åº¦é‡æ¥é€‰æ‹©ä¸€ç»„æ¥è¿‘ç§å­çš„å€™é€‰é›†ã€‚ç”±äºåµŒå…¥å™¨å¹¶æœªé’ˆå¯¹æ–°æ•°æ®é›†è¿›è¡Œä¼˜åŒ–ï¼Œåœ¨ç¬¬äºŒé˜¶æ®µï¼Œæˆ‘ä»¬ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ä»è¿™äº›å€™é€‰é›†ä¸­è¿›ä¸€æ­¥é€‰æ‹©æ„å›¾ä¸ç§å­ç›¸åŒçš„è¯è¯­ã€‚æœ€åï¼Œæˆ‘ä»¬å°†è¿™äº›é€‰ä¸­çš„å€™é€‰é›†ä¸ç§å­åˆå¹¶ï¼Œä»¥è·å–ç§å­çš„ç²¾ç»†åµŒå…¥è¡¨ç¤ºã€‚æˆ‘ä»¬å‘ç°ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é€šå¸¸ä¼˜äºç›´æ¥ä½¿ç”¨åµŒå…¥å™¨çš„æ–¹æ³•ï¼Œå¹¶ä¸”å³ä½¿ä¸å…¶ä»–æœ€æ–°ç ”ç©¶ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¹Ÿå–å¾—äº†ç›¸å½“çš„ç»“æœï¼Œå³ä½¿é‚£äº›ä½¿ç”¨æ›´å¤§æ¨¡å‹ä¸”éœ€è¦å¾®è°ƒçš„ç ”ç©¶ä¹Ÿæ˜¯å¦‚æ­¤ï¼Œè¿™æ˜¾ç¤ºäº†å…¶å¼ºå¤§å’Œé«˜æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿåœ¨æ— éœ€é¢å¤–å¾®è°ƒçš„æƒ…å†µä¸‹è¿›ä¸€æ­¥æ”¹è¿›ç°æœ‰åµŒå…¥å™¨ï¼Œä½¿å…¶æ›´é€‚åº”æ–°é¢†åŸŸæ•°æ®é›†ã€‚æ­¤å¤–ï¼Œå°†èšç±»ä»»åŠ¡è§†ä¸ºå°è§„æ¨¡çš„é€‰æ‹©é—®é¢˜ï¼Œä¸ºä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ ¹æ®ç”¨æˆ·ç›®æ ‡å®šåˆ¶èšç±»ä»»åŠ¡æä¾›äº†æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15351v1">PDF</a> </p>
<p><strong>Summary</strong><br>æœ¬è®ºæ–‡æå‡ºä¸€ç§åä¸ºSelection and Pooling with Large Language Modelsï¼ˆSPILLï¼‰çš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨å®ç°æ— éœ€å¾®è°ƒå³å¯è¿›è¡Œæ„å›¾èšç±»ã€‚ç°æœ‰åŸºäºåµŒå…¥çš„èšç±»æ–¹æ³•éœ€è¦é’ˆå¯¹æ¯ä¸ªæ–°æ•°æ®é›†è¿›è¡Œå¾®è°ƒæˆ–ä½¿ç”¨å°‘é‡æ ‡è®°ç¤ºä¾‹è¿›è¡Œä¼˜åŒ–ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚æœ¬ç ”ç©¶æ—¨åœ¨ä½¿è¿™äº›åµŒå…¥æ–¹æ³•åœ¨æ–°é¢†åŸŸæ•°æ®é›†ä¸Šæ›´åŠ é€šç”¨ï¼Œæ— éœ€è¿›ä¸€æ­¥å¾®è°ƒã€‚æœ¬ç ”ç©¶å°†èšç±»ä»»åŠ¡è§†ä¸ºå°è§„æ¨¡é€‰æ‹©é—®é¢˜ï¼Œå¹¶æå‡ºä¸¤é˜¶æ®µè§£å†³æ–¹æ¡ˆã€‚é¦–å…ˆï¼Œä½¿ç”¨ç°æœ‰åµŒå…¥å™¨ä¸ºæ¯ä¸ªè¯è¯­ï¼ˆç§°ä¸ºç§å­ï¼‰ç”ŸæˆåµŒå…¥ï¼Œç„¶ååº”ç”¨è·ç¦»åº¦é‡é€‰æ‹©æ¥è¿‘ç§å­çš„å€™é€‰æ± ã€‚å…¶æ¬¡ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»å€™é€‰æ± ä¸­è¿›ä¸€æ­¥é€‰æ‹©ä¸ç§å­å…·æœ‰ç›¸åŒæ„å›¾çš„è¯è¯­ã€‚æœ€åï¼Œå°†è¿™äº›é€‰ä¸­çš„å€™é€‰è€…ä¸ç§å­åˆå¹¶ï¼Œä»¥å¾—å‡ºæ›´ç²¾ç¡®çš„ç§å­åµŒå…¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºç›´æ¥ä½¿ç”¨åµŒå…¥å™¨çš„æ–¹æ³•ï¼Œå¹¶ä¸å…¶ä»–æœ€æ–°ç ”ç©¶è¾¾åˆ°ç›¸å½“çš„æ•ˆæœï¼Œå³ä½¿è¿™äº›ç ”ç©¶ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹å¹¶éœ€è¦å¾®è°ƒã€‚è¿™è¡¨æ˜è¯¥æ–¹æ³•çš„å¼ºå¤§å’Œé«˜æ•ˆæ€§ï¼Œä½¿ç”¨æˆ·å¯ä»¥æ ¹æ®ç›®æ ‡å®šåˆ¶èšç±»ä»»åŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„æ„å›¾èšç±»æ–¹æ³•â€”â€”Selection and Pooling with Large Language Modelsï¼ˆSPILLï¼‰ï¼Œæ— éœ€å¾®è°ƒå³å¯é€‚åº”æ–°é¢†åŸŸæ•°æ®é›†ã€‚</li>
<li>ç°æœ‰åŸºäºåµŒå…¥çš„èšç±»æ–¹æ³•å¯¹æ–°æ•°æ®é›†æ³›åŒ–èƒ½åŠ›æœ‰é™ï¼Œéœ€è¦å¾®è°ƒæˆ–ä½¿ç”¨æ ‡è®°ç¤ºä¾‹è¿›è¡Œä¼˜åŒ–ã€‚</li>
<li>SPILLæ–¹æ³•é€šè¿‡ä¸¤é˜¶æ®µç­–ç•¥è§£å†³è¿™ä¸€é—®é¢˜ï¼šé¦–å…ˆä½¿ç”¨ç°æœ‰åµŒå…¥å™¨ç”Ÿæˆè¯è¯­åµŒå…¥ï¼Œç„¶åé€šè¿‡è·ç¦»åº¦é‡å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œé€‰æ‹©å’Œæ± åŒ–ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒSPILLæ–¹æ³•ä¼˜äºç›´æ¥ä½¿ç”¨åµŒå…¥å™¨çš„æ–¹æ³•ï¼Œå¹¶ä¸å…¶ä»–æœ€æ–°ç ”ç©¶å…·æœ‰ç›¸å½“çš„æ•ˆæœã€‚</li>
<li>SPILLæ–¹æ³•ä½¿ç°æœ‰åµŒå…¥å™¨å¾—ä»¥è¿›ä¸€æ­¥æ”¹è¿›ï¼Œæ— éœ€é¢å¤–å¾®è°ƒï¼Œå¢å¼ºäº†å…¶å¯¹æ–°é¢†åŸŸæ•°æ®é›†çš„é€‚åº”æ€§ã€‚</li>
<li>å°†èšç±»ä»»åŠ¡è§†ä¸ºå°è§„æ¨¡é€‰æ‹©é—®é¢˜ï¼Œä¸ºä½¿ç”¨LLMæ ¹æ®ç”¨æˆ·ç›®æ ‡å®šåˆ¶èšç±»ä»»åŠ¡æä¾›äº†æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15351">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-081c703dcbf06dd4c35234d3119fc3d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8b4847a1505b4683c0a977c912f459da.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c6647b44dedc055726dd65e247acc0c4.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="TruthLens-A-Training-Free-Paradigm-for-DeepFake-Detection"><a href="#TruthLens-A-Training-Free-Paradigm-for-DeepFake-Detection" class="headerlink" title="TruthLens:A Training-Free Paradigm for DeepFake Detection"></a>TruthLens:A Training-Free Paradigm for DeepFake Detection</h2><p><strong>Authors:Ritabrata Chakraborty, Rajatsubhra Chakraborty, Ali Khaleghi Rahimian, Thomas MacDougall</strong></p>
<p>The proliferation of synthetic images generated by advanced AI models poses significant challenges in identifying and understanding manipulated visual content. Current fake image detection methods predominantly rely on binary classification models that focus on accuracy while often neglecting interpretability, leaving users without clear insights into why an image is deemed real or fake. To bridge this gap, we introduce TruthLens, a novel training-free framework that reimagines deepfake detection as a visual question-answering (VQA) task. TruthLens utilizes state-of-the-art large vision-language models (LVLMs) to observe and describe visual artifacts and combines this with the reasoning capabilities of large language models (LLMs) like GPT-4 to analyze and aggregate evidence into informed decisions. By adopting a multimodal approach, TruthLens seamlessly integrates visual and semantic reasoning to not only classify images as real or fake but also provide interpretable explanations for its decisions. This transparency enhances trust and provides valuable insights into the artifacts that signal synthetic content. Extensive evaluations demonstrate that TruthLens outperforms conventional methods, achieving high accuracy on challenging datasets while maintaining a strong emphasis on explainability. By reframing deepfake detection as a reasoning-driven process, TruthLens establishes a new paradigm in combating synthetic media, combining cutting-edge performance with interpretability to address the growing threats of visual disinformation. </p>
<blockquote>
<p>ç”±å…ˆè¿›çš„äººå·¥æ™ºèƒ½æ¨¡å‹ç”Ÿæˆçš„åˆæˆå›¾åƒå¤§é‡æ¶Œç°ï¼Œç»™è¯†åˆ«å’Œäº†è§£è¢«æ“ä½œè¿‡çš„è§†è§‰å†…å®¹å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚å½“å‰çš„è™šå‡å›¾åƒæ£€æµ‹æ–¹æ³•ä¸»è¦ä¾èµ–äºäºŒè¿›åˆ¶åˆ†ç±»æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹æ³¨é‡å‡†ç¡®æ€§ï¼Œä½†å¾€å¾€å¿½è§†äº†å¯è§£é‡Šæ€§ï¼Œä½¿ç”¨æˆ·æ— æ³•æ¸…æ¥šåœ°äº†è§£å›¾åƒè¢«è§†ä¸ºçœŸå®æˆ–è™šå‡çš„åŸå› ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†TruthLensï¼Œè¿™æ˜¯ä¸€ç§å…¨æ–°çš„æ— éœ€è®­ç»ƒå³å¯çš„æ¡†æ¶ï¼Œå®ƒå°†æ·±åº¦ä¼ªé€ æ£€æµ‹é‡æ–°æ„æƒ³ä¸ºä¸€ç§è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ä»»åŠ¡ã€‚TruthLensåˆ©ç”¨æœ€å…ˆè¿›çš„è§†è§‰è¯­è¨€å¤§æ¨¡å‹ï¼ˆLVLMsï¼‰æ¥è§‚å¯Ÿå’Œæè¿°è§†è§‰ä¼ªå½±ï¼Œå¹¶å°†å…¶ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚GPT-4ï¼‰çš„æ¨ç†èƒ½åŠ›ç›¸ç»“åˆï¼Œä»¥åˆ†æå’Œæ•´åˆè¯æ®ä»¥åšå‡ºæ˜æ™ºçš„å†³ç­–ã€‚é€šè¿‡é‡‡ç”¨å¤šæ¨¡å¼æ–¹æ³•ï¼ŒTruthLensæ— ç¼é›†æˆäº†è§†è§‰å’Œè¯­ä¹‰æ¨ç†ï¼Œä¸ä»…å°†å›¾åƒåˆ†ç±»ä¸ºçœŸå®æˆ–è™šå‡ï¼Œè€Œä¸”è¿˜ä¸ºå…¶å†³ç­–æä¾›äº†å¯è§£é‡Šçš„è§£é‡Šã€‚è¿™ç§é€æ˜åº¦å¢å¼ºäº†ä¿¡ä»»ï¼Œå¹¶æä¾›äº†å…³äºè¡¨ç¤ºåˆæˆå†…å®¹çš„ä¼ªå½±çš„å®è´µè§è§£ã€‚å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒTruthLensä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ä¸Šå®ç°é«˜å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œè¿˜åšæŒå¼ºè°ƒå¯è§£é‡Šæ€§ã€‚é€šè¿‡å°†æ·±åº¦ä¼ªé€ æ£€æµ‹é‡å¡‘ä¸ºä»¥æ¨ç†ä¸ºæ ¸å¿ƒçš„è¿‡ç¨‹ï¼ŒTruthLenså»ºç«‹äº†åº”å¯¹åˆæˆåª’ä½“çš„æ–°èŒƒå¼ï¼Œå°†å°–ç«¯æ€§èƒ½å’Œå¯è§£é‡Šæ€§ç›¸ç»“åˆï¼Œåº”å¯¹è§†è§‰è™šå‡ä¿¡æ¯ä¸æ–­å¢é•¿çš„å¨èƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15342v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å…ˆè¿›AIæ¨¡å‹ç”Ÿæˆçš„å¤§é‡åˆæˆå›¾åƒï¼Œä¸ºè¯†åˆ«å’Œç†è§£æ“çºµè¿‡çš„è§†è§‰å†…å®¹å¸¦æ¥äº†å·¨å¤§æŒ‘æˆ˜ã€‚å½“å‰ä¸»æµçš„å‡å›¾åƒæ£€æµ‹æ–¹æ³•ä¸»è¦ä¾èµ–äºŒè¿›åˆ¶åˆ†ç±»æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹è™½ç„¶å‡†ç¡®ç‡é«˜ï¼Œä½†å¿½è§†äº†å¯è§£é‡Šæ€§ï¼Œä½¿ç”¨æˆ·æ— æ³•æ¸…æ¥šäº†è§£å›¾åƒè¢«åˆ¤å®šä¸ºçœŸå®æˆ–è™šå‡çš„åŸå› ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†TruthLensï¼Œè¿™æ˜¯ä¸€ç§å…¨æ–°çš„æ— è®­ç»ƒæ¡†æ¶ï¼Œå°†æ·±åº¦ä¼ªé€ æ£€æµ‹é‡æ–°æ„æƒ³ä¸ºè§†è§‰é—®ç­”ï¼ˆVQAï¼‰ä»»åŠ¡ã€‚TruthLensåˆ©ç”¨æœ€å…ˆè¿›çš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰æ¥è§‚å¯Ÿå’Œæè¿°è§†è§‰ä¼ªå½±ï¼Œå¹¶ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚GPT-4ï¼‰çš„æ¨ç†èƒ½åŠ›æ¥åˆ†æå¹¶æ±‡æ€»è¯æ®ä»¥åšå‡ºå†³ç­–ã€‚é€šè¿‡é‡‡ç”¨å¤šæ¨¡å¼æ–¹æ³•ï¼ŒTruthLensæ— ç¼é›†æˆäº†è§†è§‰å’Œè¯­ä¹‰æ¨ç†ï¼Œä¸ä»…èƒ½å¤ŸçœŸå®æˆ–è™šå‡åœ°åˆ†ç±»å›¾åƒï¼Œè€Œä¸”èƒ½å¤Ÿå¯¹å…¶å†³ç­–è¿›è¡Œå¯è§£é‡Šçš„è§£é‡Šã€‚è¿™ç§é€æ˜åº¦å¢å¼ºäº†ä¿¡ä»»ï¼Œå¹¶æä¾›äº†å…³äºåˆæˆå†…å®¹ä¿¡å·çš„å®è´µä¿¡æ¯ã€‚å…¨é¢çš„è¯„ä¼°è¡¨æ˜ï¼ŒTruthLensä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œåœ¨é«˜éš¾åº¦æ•°æ®é›†ä¸Šå®ç°äº†é«˜å‡†ç¡®æ€§ï¼ŒåŒæ—¶å¼ºçƒˆå…³æ³¨è§£é‡Šæ€§ã€‚é€šè¿‡å°†æ·±åº¦ä¼ªé€ æ£€æµ‹é‡å¡‘ä¸ºä»¥æ¨ç†ä¸ºä¸­å¿ƒçš„è¿‡ç¨‹ï¼ŒTruthLenså»ºç«‹äº†åº”å¯¹åˆæˆåª’ä½“çš„æ–°èŒƒå¼ï¼Œç»“åˆå‰æ²¿æ€§èƒ½å’Œå¯è§£é‡Šæ€§æ¥åº”å¯¹æ—¥ç›Šå¢é•¿çš„è§†è§‰å‡ä¿¡æ¯å¨èƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AIæ¨¡å‹ç”Ÿæˆçš„åˆæˆå›¾åƒåœ¨è¯†åˆ«å’Œç†è§£æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>å½“å‰å‡å›¾åƒæ£€æµ‹æ–¹æ³•ä¸»è¦ä¾èµ–äºŒè¿›åˆ¶åˆ†ç±»æ¨¡å‹ï¼Œä½†ç¼ºä¹è§£é‡Šæ€§ã€‚</li>
<li>TruthLensæ˜¯ä¸€ä¸ªå…¨æ–°çš„æ— è®­ç»ƒæ¡†æ¶ï¼Œå°†æ·±åº¦ä¼ªé€ æ£€æµ‹è§†ä¸ºè§†è§‰é—®ç­”ä»»åŠ¡ã€‚</li>
<li>TruthLensåˆ©ç”¨å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹è§‚å¯Ÿå¹¶æè¿°è§†è§‰ä¼ªå½±ã€‚</li>
<li>ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œæä¾›å¯è§£é‡Šæ€§çš„å†³ç­–è§£é‡Šã€‚</li>
<li>TruthLensé‡‡ç”¨å¤šæ¨¡å¼æ–¹æ³•æ— ç¼é›†æˆè§†è§‰å’Œè¯­ä¹‰æ¨ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15342">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2a12136cdc740f626a95d1cc2b6dad39.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-171822b9d5cd5ce351c6611568470b2e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5605367e7ff28089ce08fee632ca8275.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ddaedeafaf45f27f2683a44c75f2b14d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ad8a6014f906db552cfdeab2ae9fb57b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3d2a23abc22042df7d104cb0a660584a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-71e59bbed2cd5f74768a79e5247ebbfa.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Uncertainty-Guided-Chain-of-Thought-for-Code-Generation-with-LLMs"><a href="#Uncertainty-Guided-Chain-of-Thought-for-Code-Generation-with-LLMs" class="headerlink" title="Uncertainty-Guided Chain-of-Thought for Code Generation with LLMs"></a>Uncertainty-Guided Chain-of-Thought for Code Generation with LLMs</h2><p><strong>Authors:Yuqi Zhu, Ge Li, Xue Jiang, Jia Li, Hong Mei, Zhi Jin, Yihong Dong</strong></p>
<p>Chain-of-Thought (CoT) reasoning has been demonstrated as an effective technique for improving the problem-solving capabilities of large language models (LLMs) in the context of code generation. However, existing CoT methods often exhibit a tendency toward â€œoverthinkingâ€, where the LLM consistently applies reasoning strategies without adequately considering the taskâ€™s underlying complexity. This results in the LLMs allocating excessive computational resources, in terms of tokens, to relatively simple tasks or problems where the correct answer is already evident. Additionally, this overthinking may lead LLMs down incorrect reasoning paths, resulting in incorrect code generation. In this paper, we introduce UnCertainty-Aware Chain-of-Thought (UnCert-CoT), an LLM-based approach designed to enhance code generation by incorporating an uncertainty-aware CoT reasoning mechanism, which focuses computational resources on targeting points where LLMs are more prone to error. We propose two confidence-based uncertainty measures: Entropy-based and Probability Differential-based methods. When uncertainty is high, UnCert-CoT activates CoT-decoding to generate multiple reasoning paths and selects the final code that exhibits the highest likelihood of correctness. In contrast, LLM directly generates the code when uncertainty is low. This uncertainty judgment mechanism allows LLMs to prioritize complex tasks and avoid unnecessary steps in simpler cases, thereby improving overall efficiency and accuracy in code generation. Our experimental results demonstrate that UnCert-CoT significantly enhances code generation accuracy on challenging benchmark MHPP(Mostly Hard Python Problems), it achieves improvements up to 6.1% on PassRate accuracy, particularly in situations where traditional LLMs are prone to errors. </p>
<blockquote>
<p>é“¾å¼æ€ç»´ï¼ˆChain-of-Thoughtï¼Œç®€ç§°CoTï¼‰æ¨ç†è¢«è¯æ˜æ˜¯ä¸€ç§æœ‰æ•ˆçš„æŠ€æœ¯ï¼Œç”¨äºæé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä»£ç ç”Ÿæˆæ–¹é¢çš„è§£å†³é—®é¢˜çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„CoTæ–¹æ³•å¾€å¾€å€¾å‘äºâ€œè¿‡åº¦æ€è€ƒâ€ï¼Œå³LLMæŒç»­åº”ç”¨æ¨ç†ç­–ç•¥ï¼Œè€Œæ²¡æœ‰å……åˆ†è€ƒè™‘åˆ°ä»»åŠ¡çš„åº•å±‚å¤æ‚æ€§ã€‚è¿™å¯¼è‡´LLMå¯¹ç›¸å¯¹ç®€å•çš„ä»»åŠ¡æˆ–é—®é¢˜åˆ†é…è¿‡å¤šçš„è®¡ç®—èµ„æºï¼ˆä»¥ä»¤ç‰Œçš„å½¢å¼ï¼‰ï¼Œè€Œæ­£ç¡®çš„ç­”æ¡ˆå·²ç»å¾ˆæ˜æ˜¾ã€‚æ­¤å¤–ï¼Œè¿™ç§è¿‡åº¦æ€è€ƒå¯èƒ½å¯¼è‡´LLMé™·å…¥é”™è¯¯çš„æ¨ç†è·¯å¾„ï¼Œä»è€Œå¯¼è‡´ä»£ç ç”Ÿæˆé”™è¯¯ã€‚</p>
</blockquote>
<p>åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†åŸºäºä¸ç¡®å®šæ€§çš„é“¾å¼æ€ç»´ï¼ˆUnCert-CoTï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºLLMçš„æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡èå…¥ä¸ç¡®å®šæ€§æ„ŸçŸ¥çš„CoTæ¨ç†æœºåˆ¶ï¼Œå¢å¼ºä»£ç ç”Ÿæˆèƒ½åŠ›ã€‚è¯¥æœºåˆ¶å°†è®¡ç®—èµ„æºé›†ä¸­åœ¨LLMæ›´å®¹æ˜“å‡ºé”™çš„ç›®æ ‡ç‚¹ä¸Šã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤ç§åŸºäºç½®ä¿¡åº¦çš„ä¸ç¡®å®šæ€§åº¦é‡æ–¹æ³•ï¼šåŸºäºç†µçš„æ–¹æ³•å’ŒåŸºäºæ¦‚ç‡å·®å¼‚çš„æ–¹æ³•ã€‚å½“ä¸ç¡®å®šæ€§è¾ƒé«˜æ—¶ï¼ŒUnCert-CoTä¼šæ¿€æ´»CoTè§£ç ä»¥ç”Ÿæˆå¤šä¸ªæ¨ç†è·¯å¾„ï¼Œå¹¶é€‰æ‹©æœ€æœ‰å¯èƒ½æ­£ç¡®çš„æœ€ç»ˆä»£ç ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå½“ä¸ç¡®å®šæ€§è¾ƒä½æ—¶ï¼ŒLLMä¼šç›´æ¥ç”Ÿæˆä»£ç ã€‚è¿™ç§ä¸ç¡®å®šæ€§åˆ¤æ–­æœºåˆ¶ä½¿LLMèƒ½å¤Ÿä¼˜å…ˆå¤„ç†å¤æ‚ä»»åŠ¡ï¼Œå¹¶åœ¨æ›´ç®€å•çš„æƒ…å†µä¸‹é¿å…ä¸å¿…è¦çš„æ­¥éª¤ï¼Œä»è€Œæé«˜ä»£ç ç”Ÿæˆçš„æ€»ä½“æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15341v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä»£ç ç”Ÿæˆæ–¹é¢åº”ç”¨Chain-of-Thoughtï¼ˆCoTï¼‰æ¨ç†æŠ€æœ¯å¯ä»¥æå‡é—®é¢˜è§£å†³èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰CoTæ–¹æ³•å¾€å¾€å€¾å‘äºâ€œè¿‡åº¦æ€è€ƒâ€ï¼Œå³åœ¨ä»»åŠ¡å¤æ‚åº¦ä¸é«˜æˆ–ç­”æ¡ˆæ˜ç¡®çš„æƒ…å†µä¸‹ï¼ŒLLMä»ç„¶æŒç»­ä½¿ç”¨å¤æ‚çš„æ¨ç†ç­–ç•¥ï¼Œæ¶ˆè€—è¿‡å¤šè®¡ç®—èµ„æºã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºUnCertainty-Aware Chain-of-Thoughtï¼ˆUnCert-CoTï¼‰ï¼Œé€šè¿‡ä¸ç¡®å®šæ€§æ„ŸçŸ¥çš„CoTæ¨ç†æœºåˆ¶ï¼Œå°†è®¡ç®—èµ„æºé›†ä¸­åœ¨LLMæ˜“å‡ºé”™çš„éƒ¨åˆ†ã€‚å½“ä¸ç¡®å®šæ€§è¾ƒé«˜æ—¶ï¼ŒUnCert-CoTå¯åŠ¨CoTè§£ç ä»¥ç”Ÿæˆå¤šä¸ªæ¨ç†è·¯å¾„å¹¶é€‰æ‹©æœ€å¯èƒ½çš„æ­£ç¡®ç­”æ¡ˆï¼›å½“ä¸ç¡®å®šæ€§è¾ƒä½æ—¶ï¼ŒLLMç›´æ¥ç”Ÿæˆä»£ç ã€‚è¿™ç§ä¸ç¡®å®šæ€§åˆ¤æ–­æœºåˆ¶ä½¿LLMèƒ½ä¼˜å…ˆå¤„ç†å¤æ‚ä»»åŠ¡å¹¶é¿å…ç®€å•æ¡ˆä¾‹ä¸­çš„ä¸å¿…è¦æ­¥éª¤ï¼Œä»è€Œæé«˜ä»£ç ç”Ÿæˆçš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒUnCert-CoTåœ¨æŒ‘æˆ˜æ€§åŸºå‡†æµ‹è¯•MHPPä¸Šçš„ä»£ç ç”Ÿæˆå‡†ç¡®ç‡å¾—åˆ°æ˜¾è‘—æé«˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¼ ç»ŸLLMæ˜“å‡ºé”™çš„æƒ…å†µä¸‹ï¼ŒPassRateå‡†ç¡®ç‡æé«˜äº†6.1%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰Chain-of-Thoughtï¼ˆCoTï¼‰æ–¹æ³•åœ¨å¤„ç†ä»£ç ç”Ÿæˆä»»åŠ¡æ—¶å­˜åœ¨è¿‡åº¦æ€è€ƒçš„é—®é¢˜ã€‚</li>
<li>UnCertainty-Aware Chain-of-Thoughtï¼ˆUnCert-CoTï¼‰æ—¨åœ¨é€šè¿‡ä¸ç¡®å®šæ€§æ„ŸçŸ¥æœºåˆ¶æ”¹è¿›LLMåœ¨ä»£ç ç”Ÿæˆä¸­çš„è¡¨ç°ã€‚</li>
<li>UnCert-CoTé‡‡ç”¨ä¸¤ç§åŸºäºç½®ä¿¡åº¦çš„ä¸ç¡®å®šæ€§åº¦é‡æ–¹æ³•ï¼šåŸºäºç†µå’ŒåŸºäºæ¦‚ç‡å·®å¼‚çš„æ–¹æ³•ã€‚</li>
<li>å½“ä¸ç¡®å®šæ€§é«˜æ—¶ï¼ŒUnCert-CoTå¯åŠ¨CoTè§£ç ä»¥ç”Ÿæˆå¤šä¸ªæ¨ç†è·¯å¾„ï¼›å½“ä¸ç¡®å®šæ€§ä½æ—¶ï¼ŒLLMç›´æ¥ç”Ÿæˆä»£ç ã€‚</li>
<li>UnCert-CoTèƒ½ä¼˜å…ˆå¤„ç†å¤æ‚ä»»åŠ¡å¹¶é¿å…ç®€å•æ­¥éª¤ï¼Œæé«˜ä»£ç ç”Ÿæˆçš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºUnCert-CoTåœ¨æŒ‘æˆ˜æ€§åŸºå‡†æµ‹è¯•MHPPä¸Šçš„ä»£ç ç”Ÿæˆå‡†ç¡®ç‡æ˜¾è‘—æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15341">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a7d9bb510c7e05fa12111acc6f77190f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e82ac1a4f5732703fce5027e814c8e8e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b581a27204d9e5c635c329825f7470ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e672cab6c60d17d7c952b6f08970f8d8.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Solla-Towards-a-Speech-Oriented-LLM-That-Hears-Acoustic-Context"><a href="#Solla-Towards-a-Speech-Oriented-LLM-That-Hears-Acoustic-Context" class="headerlink" title="Solla: Towards a Speech-Oriented LLM That Hears Acoustic Context"></a>Solla: Towards a Speech-Oriented LLM That Hears Acoustic Context</h2><p><strong>Authors:Junyi Ao, Dekun Chen, Xiaohai Tian, Wenjie Feng, Jun Zhang, Lu Lu, Yuxuan Wang, Haizhou Li, Zhizheng Wu</strong></p>
<p>Large Language Models (LLMs) have recently shown remarkable ability to process not only text but also multimodal inputs such as speech and audio. However, most existing models primarily focus on analyzing input signals using text instructions, overlooking scenarios in which speech instructions and audio are mixed and serve as inputs to the model. To address these challenges, we introduce Solla, a novel framework designed to understand speech-based questions and hear the acoustic context concurrently. Solla incorporates an audio tagging module to effectively identify and represent audio events, as well as an ASR-assisted prediction method to improve comprehension of spoken content. To rigorously evaluate Solla and other publicly available models, we propose a new benchmark dataset called SA-Eval, which includes three tasks: audio event classification, audio captioning, and audio question answering. SA-Eval has diverse speech instruction with various speaking styles, encompassing two difficulty levels, easy and hard, to capture the range of real-world acoustic conditions. Experimental results show that Solla performs on par with or outperforms baseline models on both the easy and hard test sets, underscoring its effectiveness in jointly understanding speech and audio. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æœ€è¿‘æ˜¾ç¤ºå‡ºå¤„ç†æ–‡æœ¬ä»¥åŠè¯­éŸ³å’ŒéŸ³é¢‘ç­‰å¤šæ¨¡æ€è¾“å…¥çš„æ˜¾è‘—èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ¨¡å‹ä¸»è¦ä¾§é‡äºä½¿ç”¨æ–‡æœ¬æŒ‡ä»¤åˆ†æè¾“å…¥ä¿¡å·ï¼Œå¿½è§†äº†è¯­éŸ³æŒ‡ä»¤å’ŒéŸ³é¢‘æ··åˆå¹¶å­˜å¹¶ä½œä¸ºæ¨¡å‹è¾“å…¥çš„åœºæ™¯ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Sollaï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨ç†è§£åŸºäºè¯­éŸ³çš„é—®é¢˜å¹¶åŒæ—¶è†å¬å£°å­¦ä¸Šä¸‹æ–‡çš„æ–°å‹æ¡†æ¶ã€‚Sollaèå…¥äº†ä¸€ä¸ªéŸ³é¢‘æ ‡è®°æ¨¡å—ï¼Œå¯ä»¥æœ‰æ•ˆåœ°è¯†åˆ«å’Œè¡¨ç¤ºéŸ³é¢‘äº‹ä»¶ï¼Œä»¥åŠä¸€ç§è¾…åŠ©çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰é¢„æµ‹æ–¹æ³•ï¼Œä»¥æé«˜å¯¹å£è¯­å†…å®¹çš„ç†è§£ã€‚ä¸ºäº†ä¸¥æ ¼è¯„ä¼°Sollaå’Œå…¶ä»–å…¬å¼€æ¨¡å‹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„åŸºå‡†æ•°æ®é›†SA-Evalï¼Œå®ƒåŒ…æ‹¬ä¸‰ä¸ªä»»åŠ¡ï¼šéŸ³é¢‘äº‹ä»¶åˆ†ç±»ã€éŸ³é¢‘æè¿°å’ŒéŸ³é¢‘é—®ç­”ã€‚SA-EvalåŒ…å«å„ç§é£æ ¼çš„è¨€è¯­æŒ‡ä»¤ï¼Œæ¶µç›–ç®€å•å’Œå›°éš¾ä¸¤ä¸ªéš¾åº¦çº§åˆ«ï¼Œä»¥æ•æ‰çœŸå®ä¸–ç•Œçš„å£°å­¦æ¡ä»¶èŒƒå›´ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSollaåœ¨ç®€å•å’Œå›°éš¾æµ‹è¯•é›†ä¸Šçš„è¡¨ç°ä¸åŸºçº¿æ¨¡å‹æŒå¹³æˆ–è¡¨ç°æ›´å¥½ï¼Œè¿™çªæ˜¾äº†å®ƒåœ¨è”åˆç†è§£è¯­éŸ³å’ŒéŸ³é¢‘æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15338v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>Sollaæ¡†æ¶èƒ½å¤ŸåŒæ—¶ç†è§£è¯­éŸ³æŒ‡ä»¤å¹¶å¬å–å£°å­¦ä¸Šä¸‹æ–‡ï¼Œé€šè¿‡éŸ³é¢‘æ ‡æ³¨æ¨¡å—æœ‰æ•ˆè¯†åˆ«å¹¶è¡¨è¾¾éŸ³é¢‘äº‹ä»¶ï¼Œå¹¶å€ŸåŠ©ASRé¢„æµ‹æ–¹æ³•æé«˜è¯­éŸ³å†…å®¹çš„ç†è§£ã€‚ä¸ºè¯„ä¼°Sollaå’Œå…¶ä»–å…¬å¼€æ¨¡å‹ï¼Œæå‡ºäº†åŒ…å«éŸ³é¢‘äº‹ä»¶åˆ†ç±»ã€éŸ³é¢‘æè¿°å’ŒéŸ³é¢‘é—®ç­”ä»»åŠ¡çš„æ–°åŸºå‡†æ•°æ®é›†SA-Evalï¼Œèƒ½æ¨¡æ‹ŸçœŸå®ä¸–ç•Œçš„å£°å­¦æ¡ä»¶ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSollaåœ¨ç®€å•å’Œå›°éš¾æµ‹è¯•é›†ä¸Šçš„è¡¨ç°ä¸åŸºçº¿æ¨¡å‹ç›¸å½“æˆ–æœ‰æ‰€è¶…è¶Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsç°ä¸ä»…èƒ½å¤„ç†æ–‡æœ¬ï¼Œè¿˜èƒ½å¤„ç†å¤šæ¨¡æ€è¾“å…¥ï¼Œå¦‚è¯­éŸ³å’ŒéŸ³é¢‘ã€‚</li>
<li>Sollaæ¡†æ¶èƒ½å¤ŸåŒæ—¶ç†è§£è¯­éŸ³æŒ‡ä»¤å’Œå¬å–å£°å­¦ä¸Šä¸‹æ–‡ã€‚</li>
<li>Sollaä½¿ç”¨éŸ³é¢‘æ ‡æ³¨æ¨¡å—è¯†åˆ«å¹¶è¡¨è¾¾éŸ³é¢‘äº‹ä»¶ã€‚</li>
<li>Sollaå€ŸåŠ©ASRé¢„æµ‹æ–¹æ³•æé«˜è¯­éŸ³å†…å®¹çš„ç†è§£ã€‚</li>
<li>ä¸ºè¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œæå‡ºäº†æ–°åŸºå‡†æ•°æ®é›†SA-Evalï¼ŒåŒ…å«éŸ³é¢‘äº‹ä»¶åˆ†ç±»ã€éŸ³é¢‘æè¿°å’ŒéŸ³é¢‘é—®ç­”ä»»åŠ¡ã€‚</li>
<li>SA-Evalæ¨¡æ‹ŸçœŸå®ä¸–ç•Œçš„å£°å­¦æ¡ä»¶ï¼Œå…·æœ‰ä¸åŒçš„è¯­éŸ³æŒ‡ä»¤å’Œä¸¤ç§éš¾åº¦çº§åˆ«ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15338">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0ee9a8696a35e3cd72d60add3e01d42b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e7b87045cdfa6a35b7d4c1c82d6278ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab32ea3d225a238928d8408bcdab0df5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d08f0dc9de6b10cbddb4fafd1a059ba7.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="aiXcoder-7B-v2-Training-LLMs-to-Fully-Utilize-the-Long-Context-in-Repository-level-Code-Completion"><a href="#aiXcoder-7B-v2-Training-LLMs-to-Fully-Utilize-the-Long-Context-in-Repository-level-Code-Completion" class="headerlink" title="aiXcoder-7B-v2: Training LLMs to Fully Utilize the Long Context in   Repository-level Code Completion"></a>aiXcoder-7B-v2: Training LLMs to Fully Utilize the Long Context in   Repository-level Code Completion</h2><p><strong>Authors:Jia Li, Hao Zhu, Huanyu Liu, Xianjie Shi, He Zong, Yihong Dong, Kechi Zhang, Siyuan Jiang, Zhi Jin, Ge Li</strong></p>
<p>Repository-level code completion aims to complete code based on the long contexts of the repository. Existing studies extract long contexts from the repository as inputs and leverage Large Language Models (LLMs) to generate code. However, we reveal a severe limitation of LLMs, i.e., LLMs may ignore the information within long contexts in code completion. In other words, even the contexts contain useful information (e.g., relevant APIs or similar code), LLMs may fail to utilize this information. We think this limitation is caused by an inherent bias in LLMs, i.e., relying on nearby contexts and ignoring long-range contexts. To address this, we propose a novel fine-tuning approach named CoLT. The core idea of CoLT is to provide explicit supervision signals, which emphasize that long-range contexts may hold relevant information. Specifically, CoLT proposes a reinforcement learning-based training, which explicitly encourages models to utilize the information within long contexts and punishes models for ignoring long contexts. To support CoLT, we release CoLT-132K, a large-scale dataset with 132k samples across four languages, each containing long-context inputs. We apply CoLT to a popular LLM - aiXcoder-7B and release aiXcoder-7B-v2. We conduct extensive experiments on CoLT-132K and a public benchmark - CrossCodeEval. Our experiments yield the results: 1. Effectiveness. CoLT substantially improves aiXcoder-7B. aiXcoder-7B-v2 outperforms aiXcoder-7B by up to 44% in exact match. aiXcoder-7B-v2 becomes the state-of-the-art 7B model in code completion and even surpasses larger models. 2. Generalizability. The capability learned by CoLT can generalize to new languages. Besides, CoLT is model-agnostic and effectively improves multiple LLMs. 3. Enhanced Context Utilization Capability. CoLT significantly improves the capability of LLMs in utilizing the relevant information within long contexts. </p>
<blockquote>
<p>ä»£ç åº“çº§åˆ«çš„ä»£ç è¡¥å…¨æ—¨åœ¨åŸºäºä»£ç åº“çš„é•¿æœŸä¸Šä¸‹æ–‡å®Œæˆä»£ç ã€‚ç°æœ‰ç ”ç©¶ä»ä»£ç åº“ä¸­æå–é•¿æœŸä¸Šä¸‹æ–‡ä½œä¸ºè¾“å…¥ï¼Œå¹¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆä»£ç ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬æ­ç¤ºäº†LLMçš„ä¸€ä¸ªä¸¥é‡å±€é™æ€§ï¼Œå³LLMå¯èƒ½ä¼šå¿½ç•¥é•¿æœŸä¸Šä¸‹æ–‡ä¸­çš„ä¿¡æ¯ã€‚æ¢å¥è¯è¯´ï¼Œå³ä½¿ä¸Šä¸‹æ–‡åŒ…å«æœ‰ç”¨ä¿¡æ¯ï¼ˆä¾‹å¦‚ç›¸å…³APIæˆ–ç±»ä¼¼ä»£ç ï¼‰ï¼ŒLLMä¹Ÿå¯èƒ½æ— æ³•åˆ©ç”¨è¿™äº›ä¿¡æ¯ã€‚æˆ‘ä»¬è®¤ä¸ºè¿™ä¸€å±€é™æ€§æ˜¯ç”±LLMçš„å†…åœ¨åè§é€ æˆçš„ï¼Œå³ä¾èµ–é™„è¿‘çš„ä¸Šä¸‹æ–‡å¹¶å¿½ç•¥é•¿æœŸä¸Šä¸‹æ–‡ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºCoLTçš„æ–°å‹å¾®è°ƒæ–¹æ³•ã€‚CoLTçš„æ ¸å¿ƒæ€æƒ³æ˜¯æä¾›æ˜ç¡®çš„ç›‘ç£ä¿¡å·ï¼Œå¼ºè°ƒé•¿æœŸä¸Šä¸‹æ–‡å¯èƒ½åŒ…å«ç›¸å…³ä¿¡æ¯ã€‚å…·ä½“æ¥è¯´ï¼ŒCoLTæå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„è®­ç»ƒï¼Œå®ƒæ˜ç¡®é¼“åŠ±æ¨¡å‹åˆ©ç”¨é•¿æœŸä¸Šä¸‹æ–‡ä¸­çš„ä¿¡æ¯ï¼Œå¹¶æƒ©ç½šæ¨¡å‹å¿½ç•¥é•¿æœŸä¸Šä¸‹æ–‡çš„è¡Œä¸ºã€‚ä¸ºäº†æ”¯æŒCoLTï¼Œæˆ‘ä»¬å‘å¸ƒäº†CoLT-132Kæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«å››ç§è¯­è¨€çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œæ ·æœ¬é‡ä¸º13.2ä¸‡ï¼Œæ¯ä¸ªæ ·æœ¬éƒ½åŒ…å«é•¿æœŸä¸Šä¸‹æ–‡è¾“å…¥ã€‚æˆ‘ä»¬å°†CoLTåº”ç”¨äºæµè¡Œçš„LLM-aiXcoder-7Bå¹¶å‘å¸ƒäº†aiXcoder-7B-v2ã€‚æˆ‘ä»¬åœ¨CoLT-132Kå’Œå…¬å…±åŸºå‡†æµ‹è¯•CrossCodeEvalä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒã€‚æˆ‘ä»¬çš„å®éªŒç»“æœå¦‚ä¸‹ï¼š1. æœ‰æ•ˆæ€§ã€‚CoLTå¤§å¤§æé«˜äº†aiXcoder-7Bçš„æ€§èƒ½ã€‚aiXcoder-7B-v2ä¸aiXcoder-7Bç›¸æ¯”ï¼Œç²¾ç¡®åŒ¹é…åº¦æé«˜äº†é«˜è¾¾44%ã€‚aiXcoder-7B-v2åœ¨ä»£ç è¡¥å…¨æ–¹é¢æˆä¸ºæœ€å…ˆè¿›çš„7Bæ¨¡å‹ï¼Œç”šè‡³è¶…è¶Šäº†æ›´å¤§çš„æ¨¡å‹ã€‚2. æ³›åŒ–èƒ½åŠ›ã€‚CoLTæ‰€å­¦ä¹ çš„èƒ½åŠ›å¯ä»¥æ¨å¹¿åˆ°æ–°çš„è¯­è¨€ã€‚æ­¤å¤–ï¼ŒCoLTæ˜¯æ¨¡å‹æ— å…³çš„ï¼Œå¹¶èƒ½æœ‰æ•ˆæé«˜å¤šä¸ªLLMçš„æ€§èƒ½ã€‚3. å¢å¼ºä¸Šä¸‹æ–‡åˆ©ç”¨èƒ½åŠ›ã€‚CoLTæ˜¾è‘—æé«˜äº†LLMåˆ©ç”¨é•¿æœŸä¸Šä¸‹æ–‡ä¸­ç›¸å…³ä¿¡æ¯çš„èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15301v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºä»“åº“çš„é•¿æœŸä¸Šä¸‹æ–‡è¿›è¡Œä»£ç è¡¥å…¨æ—¨åœ¨åˆ©ç”¨ä»“åº“ä¸­çš„é•¿ä¸Šä¸‹æ–‡ä¿¡æ¯æ¥å®Œæˆä»£ç ã€‚ç„¶è€Œï¼Œç°æœ‰ç ”ç©¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»ä»“åº“ä¸­æå–é•¿ä¸Šä¸‹æ–‡ä½œä¸ºè¾“å…¥æ¥ç”Ÿæˆä»£ç ï¼Œå´å­˜åœ¨LLMå¯èƒ½å¿½ç•¥é•¿ä¸Šä¸‹æ–‡ä¿¡æ¯çš„ä¸¥é‡å±€é™æ€§ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºCoLTçš„æ–°å‹å¾®è°ƒæ–¹æ³•ã€‚CoLTé€šè¿‡æä¾›æ˜ç¡®çš„ç›‘ç£ä¿¡å·ï¼Œå¼ºè°ƒé•¿ä¸Šä¸‹æ–‡å¯èƒ½åŒ…å«ç›¸å…³ä¿¡æ¯ï¼Œå¹¶é€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒé¼“åŠ±æ¨¡å‹åˆ©ç”¨é•¿ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚æˆ‘ä»¬å‘å¸ƒäº†æ”¯æŒCoLTçš„CoLT-132Kæ•°æ®é›†ï¼ŒåŒ…å«13.2ä¸‡ä»½è·¨å››ç§è¯­è¨€çš„æ ·æœ¬ã€‚å°†CoLTåº”ç”¨äºæµè¡Œçš„LLMâ€”â€”aiXcoder-7Bï¼Œå¹¶å‘å¸ƒäº†aiXcoder-7B-v2ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCoLTèƒ½æœ‰æ•ˆæé«˜aiXcoder-7Bçš„æ€§èƒ½ï¼ŒaiXcoder-7B-v2åœ¨ç²¾ç¡®åŒ¹é…ä¸Šè¾ƒaiXcoder-7Bæé«˜äº†é«˜è¾¾44%ï¼Œå¹¶æˆä¸ºä»£ç è¡¥å…¨é¢†åŸŸçš„æœ€å…ˆè¿›çš„7Bæ¨¡å‹ï¼Œç”šè‡³è¶…è¶Šäº†æ›´å¤§çš„æ¨¡å‹ã€‚æ­¤å¤–ï¼ŒCoLTè¿˜å…·æœ‰è‰¯å¥½çš„é€šç”¨æ€§ï¼Œå¯ä»¥åº”ç”¨äºæ–°è¯­è¨€ï¼Œå¹¶ä¸”å¯ä»¥æœ‰æ•ˆåœ°æé«˜å¤šä¸ªLLMçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»“åº“çº§ä»£ç è¡¥å…¨ä¾èµ–äºLLMåˆ©ç”¨ä»“åº“ä¸­çš„é•¿ä¸Šä¸‹æ–‡ä¿¡æ¯æ¥å®Œæˆä»£ç ã€‚</li>
<li>LLMåœ¨ä»£ç è¡¥å…¨ä¸­å¯èƒ½å­˜åœ¨å¿½ç•¥é•¿ä¸Šä¸‹æ–‡ä¿¡æ¯çš„å±€é™æ€§ã€‚</li>
<li>CoLTæ˜¯ä¸€ç§æ–°å‹çš„å¾®è°ƒæ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³LLMå¿½ç•¥é•¿ä¸Šä¸‹æ–‡çš„é—®é¢˜ã€‚</li>
<li>CoLTé€šè¿‡æä¾›æ˜ç¡®çš„ç›‘ç£ä¿¡å·ï¼Œé¼“åŠ±æ¨¡å‹åˆ©ç”¨é•¿ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚</li>
<li>CoLT-132Kæ˜¯ä¸€ä¸ªæ”¯æŒCoLTçš„å¤§å‹æ•°æ®é›†ï¼ŒåŒ…å«è·¨å››ç§è¯­è¨€çš„æ ·æœ¬ã€‚</li>
<li>CoLTèƒ½æœ‰æ•ˆæé«˜aiXcoder-7Bçš„æ€§èƒ½ï¼Œå¹¶åœ¨ç²¾ç¡®åŒ¹é…ä¸Šå–å¾—æ˜¾è‘—æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15301">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7f76ba82c728bbecbd1548f484f1b6f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-137a60323b46f262c2c9a580d47d0af4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9917283599c3d80e23f14902670ded28.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fff029befb8f3d622061b2f451500982.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5fbaf0a3eb7c66852152931e0165a14f.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="SENAI-Towards-Software-Engineering-Native-Generative-Artificial-Intelligence"><a href="#SENAI-Towards-Software-Engineering-Native-Generative-Artificial-Intelligence" class="headerlink" title="SENAI: Towards Software Engineering Native Generative Artificial   Intelligence"></a>SENAI: Towards Software Engineering Native Generative Artificial   Intelligence</h2><p><strong>Authors:Mootez Saad, JosÃ© Antonio HernÃ¡ndez LÃ³pez, Boqi Chen, Neil Ernst, DÃ¡niel VarrÃ³, Tushar Sharma</strong></p>
<p>Large Language Models have significantly advanced the field of code generation, demonstrating the ability to produce functionally correct code snippets. However, advancements in generative AI for code overlook foundational Software Engineering (SE) principles such as modularity, and single responsibility, and concepts such as cohesion and coupling which are critical for creating maintainable, scalable, and robust software systems. These concepts are missing in pipelines that start with pre-training and end with the evaluation using benchmarks.   This vision paper argues for the integration of SE knowledge into LLMs to enhance their capability to understand, analyze, and generate code and other SE artifacts following established SE knowledge. The aim is to propose a new direction where LLMs can move beyond mere functional accuracy to perform generative tasks that require adherence to SE principles and best practices. In addition, given the interactive nature of these conversational models, we propose using Bloomâ€™s Taxonomy as a framework to assess the extent to which they internalize SE knowledge. The proposed evaluation framework offers a sound and more comprehensive evaluation technique compared to existing approaches such as linear probing. Software engineering native generative models will not only overcome the shortcomings present in current models but also pave the way for the next generation of generative models capable of handling real-world software engineering. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä»£ç ç”Ÿæˆé¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œå±•ç°å‡ºç”ŸæˆåŠŸèƒ½æ­£ç¡®ä»£ç ç‰‡æ®µçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œä»£ç ç”Ÿæˆé¢†åŸŸçš„ç”Ÿæˆäººå·¥æ™ºèƒ½å‘å±•å¿½è§†äº†è½¯ä»¶å·¥ç¨‹ä¸­è¯¸å¦‚æ¨¡å—åŒ–ã€å•ä¸€èŒè´£ç­‰åŸºæœ¬åŸåˆ™ï¼Œä»¥åŠå¯¹äºåˆ›å»ºå¯æŒç»­çš„ã€å¯æ‰©å±•çš„å’Œç¨³å¥çš„è½¯ä»¶ç³»ç»Ÿè‡³å…³é‡è¦çš„å‡èšå’Œè€¦åˆç­‰æ¦‚å¿µã€‚è¿™äº›æ¦‚å¿µåœ¨ä»å¼€å§‹è¿›è¡Œé¢„è®­ç»ƒåˆ°æœ€åä½¿ç”¨åŸºå‡†æµ‹è¯•è¿›è¡Œè¯„ä¼°çš„ç®¡é“ä¸­ç¼ºå¤±ã€‚æœ¬æ–‡ä¸»å¼ å°†è½¯ä»¶å·¥ç¨‹çŸ¥è¯†èå…¥å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä»¥æé«˜å…¶ç†è§£ã€åˆ†æå’Œç”Ÿæˆä»£ç ä»¥åŠå…¶ä»–è½¯ä»¶å·¥ç¨‹åˆ¶å“çš„èƒ½åŠ›ï¼Œéµå¾ªæ—¢å®šçš„è½¯ä»¶å·¥ç¨‹çŸ¥è¯†ã€‚ç›®æ ‡æ˜¯æå‡ºä¸€ä¸ªæ–°çš„æ–¹å‘ï¼Œä½¿å¤§å‹è¯­è¨€æ¨¡å‹ä¸ä»…èƒ½å®ç°åŠŸèƒ½å‡†ç¡®æ€§ï¼Œè¿˜èƒ½æ‰§è¡Œéœ€è¦éµå¾ªè½¯ä»¶å·¥ç¨‹åŸåˆ™å’Œä½¿ç”¨æœ€ä½³å®è·µçš„ç”Ÿæˆä»»åŠ¡ã€‚æ­¤å¤–ï¼Œè€ƒè™‘åˆ°è¿™äº›å¯¹è¯æ¨¡å‹çš„äº¤äº’æ€§è´¨ï¼Œæˆ‘ä»¬æè®®ä½¿ç”¨å¸ƒé²å§†çš„åˆ†ç±»æ³•ä½œä¸ºæ¡†æ¶æ¥è¯„ä¼°ä»–ä»¬å†…åŒ–è½¯ä»¶å·¥ç¨‹çŸ¥è¯†çš„ç¨‹åº¦ã€‚ç›¸æ¯”ç°æœ‰çš„æ–¹æ³•å¦‚çº¿æ€§æ¢æµ‹ï¼Œæ‰€æå‡ºçš„è¯„ä¼°æ¡†æ¶æä¾›äº†å¥å…¨å’Œæ›´å…¨é¢çš„è¯„ä¼°æŠ€æœ¯ã€‚è½¯ä»¶å·¥ç¨‹åŸç”Ÿç”Ÿæˆæ¨¡å‹ä¸ä»…å°†å…‹æœå½“å‰æ¨¡å‹çš„ç¼ºç‚¹ï¼Œè¿˜å°†ä¸ºä¸‹ä¸€ä»£èƒ½å¤Ÿå¤„ç†ç°å®ä¸–ç•Œè½¯ä»¶å·¥ç¨‹çš„ç”Ÿæˆæ¨¡å‹é“ºå¹³é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15282v1">PDF</a> 5 pages, 1 figure</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä»£ç ç”Ÿæˆé¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œèƒ½å¤Ÿç”ŸæˆåŠŸèƒ½æ­£ç¡®çš„ä»£ç ç‰‡æ®µã€‚ç„¶è€Œï¼Œè½¯ä»¶å·¥ç¨‹ï¼ˆSEï¼‰åŸç†ï¼Œå¦‚æ¨¡å—åŒ–ã€å•ä¸€èŒè´£åŸåˆ™ä»¥åŠå‡èšåŠ›ä¸è€¦åˆç­‰å…³é”®æ¦‚å¿µï¼Œåœ¨ç”Ÿæˆå¼AIçš„å‘å±•ä¸­è¢«å¿½è§†ã€‚è¿™äº›æ¦‚å¿µå¯¹äºåˆ›å»ºå¯æŒç»­ã€å¯æ‰©å±•å’Œç¨³å¥çš„è½¯ä»¶ç³»ç»Ÿè‡³å…³é‡è¦ã€‚æœ¬æ–‡ä¸»å¼ å°†SEçŸ¥è¯†æ•´åˆåˆ°å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ï¼Œä»¥æé«˜å…¶ç†è§£ã€åˆ†æå’Œç”Ÿæˆä»£ç åŠå…¶ä»–SEå·¥ä»¶çš„èƒ½åŠ›ã€‚ç›®æ ‡æ˜¯æå‡ºä¸€ä¸ªæ–°çš„æ–¹å‘ï¼Œä½¿å¤§å‹è¯­è¨€æ¨¡å‹ä¸ä»…èƒ½å¤Ÿå®ç°åŠŸèƒ½å‡†ç¡®æ€§ï¼Œè¿˜èƒ½æ‰§è¡Œéœ€è¦éµå¾ªSEåŸç†å’Œæœ€ä½³å®è·µçš„ç”Ÿæˆä»»åŠ¡ã€‚æ­¤å¤–ï¼Œé‰´äºè¿™äº›å¯¹è¯æ¨¡å‹çš„äº¤äº’æ€§ï¼Œæœ¬æ–‡æè®®ä½¿ç”¨å¸ƒé²å§†çš„åˆ†ç±»å­¦ä½œä¸ºè¯„ä¼°æ¡†æ¶ï¼Œä»¥è¯„ä¼°ä»–ä»¬å¯¹SEçŸ¥è¯†çš„å†…åŒ–ç¨‹åº¦ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œå¦‚çº¿æ€§æ¢æµ‹ï¼Œæ‰€æå‡ºçš„è¯„ä¼°æ¡†æ¶æä¾›äº†æ›´å…¨é¢å’Œç¨³å¥çš„è¯„ä¼°æŠ€æœ¯ã€‚è½¯ä»¶å·¥ç¨‹åŸç”Ÿç”Ÿæˆæ¨¡å‹å°†ä¸ä»…å…‹æœå½“å‰æ¨¡å‹çš„å±€é™æ€§ï¼Œè¿˜ä¸ºä¸‹ä¸€ä»£èƒ½å¤Ÿå¤„ç†ç°å®ä¸–ç•Œä¸­è½¯ä»¶å·¥ç¨‹çš„ç”Ÿæˆæ¨¡å‹é“ºå¹³é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä»£ç ç”Ÿæˆæ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œèƒ½ç”ŸæˆåŠŸèƒ½æ­£ç¡®çš„ä»£ç ç‰‡æ®µã€‚</li>
<li>è½¯ä»¶å·¥ç¨‹ï¼ˆSEï¼‰åŸç†ï¼Œå¦‚æ¨¡å—åŒ–ã€å•ä¸€èŒè´£ç­‰ï¼Œåœ¨ç°æœ‰ç”Ÿæˆå¼AIä¸­æœªå¾—åˆ°å……åˆ†é‡è§†ã€‚</li>
<li>å°†SEçŸ¥è¯†æ•´åˆåˆ°å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ï¼Œèƒ½æé«˜å…¶åœ¨ç†è§£ã€åˆ†æå’Œç”Ÿæˆä»£ç æ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>æè®®ä½¿ç”¨å¸ƒé²å§†çš„åˆ†ç±»å­¦ä½œä¸ºè¯„ä¼°æ¡†æ¶ï¼Œä»¥è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹å¯¹SEçŸ¥è¯†çš„å†…åŒ–ç¨‹åº¦ã€‚</li>
<li>ä¸ç°æœ‰è¯„ä¼°æ–¹æ³•ç›¸æ¯”ï¼Œæ–°çš„è¯„ä¼°æ¡†æ¶æ›´å…¨é¢å’Œç¨³å¥ã€‚</li>
<li>è½¯ä»¶å·¥ç¨‹åŸç”Ÿç”Ÿæˆæ¨¡å‹å°†å…‹æœå½“å‰æ¨¡å‹çš„å±€é™æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15282">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dabe3517907f61c947ab6e1cc339591c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-27fdbd8865ffe9cb6965b408dab40bc0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="LogLLaMA-Transformer-based-log-anomaly-detection-with-LLaMA"><a href="#LogLLaMA-Transformer-based-log-anomaly-detection-with-LLaMA" class="headerlink" title="LogLLaMA: Transformer-based log anomaly detection with LLaMA"></a>LogLLaMA: Transformer-based log anomaly detection with LLaMA</h2><p><strong>Authors:Zhuoyi Yang, Ian G. Harris</strong></p>
<p>Log anomaly detection refers to the task that distinguishes the anomalous log messages from normal log messages. Transformer-based large language models (LLMs) are becoming popular for log anomaly detection because of their superb ability to understand complex and long language patterns. In this paper, we propose LogLLaMA, a novel framework that leverages LLaMA2. LogLLaMA is first finetuned on normal log messages from three large-scale datasets to learn their patterns. After finetuning, the model is capable of generating successive log messages given previous log messages. Our generative model is further trained to identify anomalous log messages using reinforcement learning (RL). The experimental results show that LogLLaMA outperforms the state-of-the-art approaches for anomaly detection on BGL, Thunderbird, and HDFS datasets. </p>
<blockquote>
<p>æ—¥å¿—å¼‚å¸¸æ£€æµ‹æ˜¯æŒ‡ä»æ­£å¸¸æ—¥å¿—æ¶ˆæ¯ä¸­åŒºåˆ†å‡ºå¼‚å¸¸æ—¥å¿—æ¶ˆæ¯çš„ä»»åŠ¡ã€‚åŸºäºTransformerçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”±äºç†è§£å¤æ‚å’Œé•¿æœŸè¯­è¨€æ¨¡å¼çš„èƒ½åŠ›å‡ºè‰²ï¼Œå› æ­¤åœ¨æ—¥å¿—å¼‚å¸¸æ£€æµ‹ä¸­è¶Šæ¥è¶Šå—æ¬¢è¿ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†LogLLaMAï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨LLaMA2çš„æ–°å‹æ¡†æ¶ã€‚LogLLaMAé¦–å…ˆé€šè¿‡å¯¹ä¸‰ä¸ªå¤§å‹æ•°æ®é›†ä¸­çš„æ­£å¸¸æ—¥å¿—æ¶ˆæ¯è¿›è¡Œå¾®è°ƒæ¥å­¦ä¹ å…¶æ¨¡å¼ã€‚å¾®è°ƒåï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨ç»™å®šå…ˆå‰çš„æ—¥å¿—æ¶ˆæ¯çš„æƒ…å†µä¸‹ç”Ÿæˆè¿ç»­çš„æ—¥å¿—æ¶ˆæ¯ã€‚æˆ‘ä»¬çš„ç”Ÿæˆæ¨¡å‹è¿›ä¸€æ­¥ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›è¡Œè®­ç»ƒï¼Œä»¥è¯†åˆ«å¼‚å¸¸çš„æ—¥å¿—æ¶ˆæ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLogLLaMAåœ¨BGLã€Thunderbirdå’ŒHDFSæ•°æ®é›†ä¸Šçš„å¼‚å¸¸æ£€æµ‹ä¼˜äºæœ€æ–°æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14849v1">PDF</a> 8 pages, 5 figures</p>
<p><strong>Summary</strong>ï¼š<br>åŸºäºTransformerçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å› å…¶å¯¹å¤æ‚å’Œé•¿æœŸè¯­è¨€æ¨¡å¼çš„å‡ºè‰²ç†è§£èƒ½åŠ›ï¼Œåœ¨æ—¥å¿—å¼‚å¸¸æ£€æµ‹ä¸­å—åˆ°å¹¿æ³›å…³æ³¨ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶LogLLaMAï¼Œå®ƒåˆ©ç”¨LLaMA2æ¨¡å‹è¿›è¡Œå¾®è°ƒä»¥å­¦ä¹ æ­£å¸¸æ—¥å¿—æ¶ˆæ¯çš„æ¨¡å¼ï¼Œå¹¶é€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›ä¸€æ­¥è®­ç»ƒæ¨¡å‹ä»¥è¯†åˆ«å¼‚å¸¸æ—¥å¿—æ¶ˆæ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLogLLaMAåœ¨BGLã€Thunderbirdå’ŒHDFSæ•°æ®é›†ä¸Šçš„å¼‚å¸¸æ£€æµ‹æ€§èƒ½ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ul>
<li>LogLLaMAæ˜¯ä¸€ä¸ªåŸºäºLLaMAçš„å¤§å‹è¯­è¨€æ¨¡å‹æ¡†æ¶ï¼Œç”¨äºæ—¥å¿—å¼‚å¸¸æ£€æµ‹ã€‚</li>
<li>æ¨¡å‹é€šè¿‡å¾®è°ƒå­¦ä¹ æ­£å¸¸æ—¥å¿—æ¶ˆæ¯çš„æ¨¡å¼ï¼Œèƒ½å¤Ÿç”Ÿæˆè¿ç»­çš„æ—¥å¿—æ¶ˆæ¯ã€‚</li>
<li>LogLLaMAåˆ©ç”¨å¼ºåŒ–å­¦ä¹ è¿›ä¸€æ­¥è®­ç»ƒæ¨¡å‹ä»¥è¯†åˆ«å¼‚å¸¸æ—¥å¿—æ¶ˆæ¯ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜LogLLaMAåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å¼‚å¸¸æ£€æµ‹æ€§èƒ½ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
<li>LogLLaMAçš„ä¸»è¦ä¼˜åŠ¿åœ¨äºå…¶å¯¹å¤æ‚å’Œé•¿æœŸè¯­è¨€æ¨¡å¼çš„å‡ºè‰²ç†è§£èƒ½åŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14849">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-07a8af6d3e8342ef7ba09bb09db47902.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-316764f7815ae873d1d5a60b86f7a240.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0a29bb67a8c4ab4594ee6d9e6bb20de5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-04be2f08a9f4431640e440253e2788e0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-70b9a97bffdabf88fe57edfa3ca8ca3f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ff8b1911d630e0cfef1cbd9c7076fa86.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7bb9f170f721dff7bd3aec1c9f704e30.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Creation-MMBench-Assessing-Context-Aware-Creative-Intelligence-in-MLLM"><a href="#Creation-MMBench-Assessing-Context-Aware-Creative-Intelligence-in-MLLM" class="headerlink" title="Creation-MMBench: Assessing Context-Aware Creative Intelligence in MLLM"></a>Creation-MMBench: Assessing Context-Aware Creative Intelligence in MLLM</h2><p><strong>Authors:Xinyu Fang, Zhijian Chen, Kai Lan, Lixin Ma, Shengyuan Ding, Yingji Liang, Xiangyu Zhao, Farong Wen, Zicheng Zhang, Guofeng Zhang, Haodong Duan, Kai Chen, Dahua Lin</strong></p>
<p>Creativity is a fundamental aspect of intelligence, involving the ability to generate novel and appropriate solutions across diverse contexts. While Large Language Models (LLMs) have been extensively evaluated for their creative capabilities, the assessment of Multimodal Large Language Models (MLLMs) in this domain remains largely unexplored. To address this gap, we introduce Creation-MMBench, a multimodal benchmark specifically designed to evaluate the creative capabilities of MLLMs in real-world, image-based tasks. The benchmark comprises 765 test cases spanning 51 fine-grained tasks. To ensure rigorous evaluation, we define instance-specific evaluation criteria for each test case, guiding the assessment of both general response quality and factual consistency with visual inputs. Experimental results reveal that current open-source MLLMs significantly underperform compared to proprietary models in creative tasks. Furthermore, our analysis demonstrates that visual fine-tuning can negatively impact the base LLMâ€™s creative abilities. Creation-MMBench provides valuable insights for advancing MLLM creativity and establishes a foundation for future improvements in multimodal generative intelligence. Full data and evaluation code is released on <a target="_blank" rel="noopener" href="https://github.com/open-compass/Creation-MMBench">https://github.com/open-compass/Creation-MMBench</a>. </p>
<blockquote>
<p>åˆ›é€ åŠ›æ˜¯æ™ºèƒ½çš„ä¸€ä¸ªåŸºæœ¬æ–¹é¢ï¼Œæ¶‰åŠåœ¨ä¸åŒä¸Šä¸‹æ–‡ä¸­ç”Ÿæˆæ–°é¢–ä¸”é€‚å½“è§£å†³æ–¹æ¡ˆçš„èƒ½åŠ›ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»å¹¿æ³›è¯„ä¼°äº†å…¶åˆ›é€ åŠ›ï¼Œä½†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰åœ¨è¯¥é¢†åŸŸçš„è¯„ä¼°ä»ç„¶å¾ˆå¤§ç¨‹åº¦ä¸Šæœªè¢«æ¢ç´¢ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†Creation-MMBenchï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹åœ¨ç°å®ä¸–ç•Œä¸­åŸºäºå›¾åƒçš„ä»»åŠ¡çš„åˆ›é€ æ€§èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…å«765ä¸ªæµ‹è¯•ç”¨ä¾‹ï¼Œæ¶µç›–51ä¸ªç»†ç²’åº¦ä»»åŠ¡ã€‚ä¸ºäº†ç¡®ä¿ä¸¥æ ¼çš„è¯„ä¼°ï¼Œæˆ‘ä»¬ä¸ºæ¯ä¸ªæµ‹è¯•ç”¨ä¾‹å®šä¹‰äº†ç‰¹å®šçš„è¯„ä¼°æ ‡å‡†ï¼Œä»¥æŒ‡å¯¼å¯¹é€šç”¨å“åº”è´¨é‡å’Œä¸è§†è§‰è¾“å…¥çš„äº‹å®ä¸€è‡´æ€§çš„è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ä¸“æœ‰æ¨¡å‹ç›¸æ¯”ï¼Œå½“å‰å¼€æºçš„MLLMåœ¨åˆ›é€ æ€§ä»»åŠ¡ä¸­çš„è¡¨ç°æ˜¾è‘—è¾ƒå·®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œè§†è§‰å¾®è°ƒå¯èƒ½ä¼šè´Ÿé¢å½±å“åŸºç¡€LLMçš„åˆ›é€ åŠ›ã€‚Creation-MMBenchä¸ºæ¨è¿›MLLMçš„åˆ›é€ åŠ›æä¾›äº†å®è´µçš„è§è§£ï¼Œå¹¶ä¸ºæœªæ¥å¤šæ¨¡æ€ç”Ÿæˆæ™ºèƒ½çš„æ”¹è¿›å¥ å®šäº†åŸºç¡€ã€‚å®Œæ•´çš„æ•°æ®å’Œè¯„ä¼°ä»£ç å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/open-compass/Creation-MMBench%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/open-compass/Creation-MMBenchä¸Šå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14478v2">PDF</a> Evaluation Code and dataset see   <a target="_blank" rel="noopener" href="https://github.com/open-compass/Creation-MMBench">https://github.com/open-compass/Creation-MMBench</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨çœŸå®ä¸–ç•Œå›¾åƒä»»åŠ¡ä¸­çš„åˆ›é€ åŠ›è¯„ä¼°çš„æ–°åŸºå‡†æµ‹è¯•ï¼Œå³Creation-MMBenchã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…æ‹¬é’ˆå¯¹å„ä¸ªä¸åŒä»»åŠ¡è®¾ç½®çš„å…±765ä¸ªæµ‹è¯•æ¡ˆä¾‹ï¼Œå¹¶å®šä¹‰äº†é’ˆå¯¹æ¯ä¸ªæµ‹è¯•æ¡ˆä¾‹çš„å…·ä½“è¯„ä¼°æ ‡å‡†ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨åˆ›é€ æ€§ä»»åŠ¡æ–¹é¢ï¼Œå¼€æºçš„MLLMsè¡¨ç°æ˜¾è‘—ä¸å¦‚ä¸“æœ‰æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å‘ç°è§†è§‰å¾®è°ƒå¯èƒ½å¯¹åŸºç¡€LLMçš„åˆ›é€ åŠ›äº§ç”Ÿè´Ÿé¢å½±å“ã€‚Creation-MMBenchä¸ºæ¨è¿›MLLMçš„åˆ›é€ åŠ›ç ”ç©¶æä¾›äº†å®è´µçš„è§è§£ï¼Œå¹¶ä¸ºæœªæ¥å¤šæ¨¡æ€ç”Ÿæˆæ™ºèƒ½çš„å‘å±•å¥ å®šäº†åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨åˆ›é€ åŠ›è¯„ä¼°æ–¹é¢ä»å­˜åœ¨è¾ƒå¤§ç©ºç™½ï¼Œéœ€è¦æ–°çš„åŸºå‡†æµ‹è¯•æ¥è¯„ä¼°å…¶åœ¨çœŸå®ä¸–ç•Œå›¾åƒä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚</li>
<li>Creation-MMBenchæ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°MLLMsåœ¨å¤šç§ç²¾ç»†åŒ–ä»»åŠ¡ä¸­çš„åˆ›é€ åŠ›ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¼€æºçš„MLLMsåœ¨åˆ›é€ æ€§ä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ï¼Œä¸ä¸“æœ‰æ¨¡å‹ç›¸æ¯”å­˜åœ¨æ˜¾è‘—å·®è·ã€‚</li>
<li>è§†è§‰å¾®è°ƒå¯èƒ½å¯¹åŸºç¡€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åˆ›é€ åŠ›äº§ç”Ÿè´Ÿé¢å½±å“ã€‚</li>
<li>Creation-MMBenchä¸ºè¯„ä¼°å’Œæå‡MLLMçš„åˆ›é€ åŠ›æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚</li>
<li>è¯¥åŸºå‡†æµ‹è¯•çš„å»ºç«‹ä¸ºæœªæ¥çš„å¤šæ¨¡æ€ç”Ÿæˆæ™ºèƒ½çš„å‘å±•å’Œæ”¹è¿›å¥ å®šäº†åŸºç¡€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14478">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bb5cbd0eaa63667bd14028d16373f5d7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-09efbe47a8f3ceea96b742e454f6c65c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e67453f000ddf69c1ca3786e6c2eb249.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2dc92eb5e8bc35ceb2ca6a9e4ebd994f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92cd3363a2eb929bbdac909e11ee8524.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5d2dcea6246fd2c8eba09b110c3016c5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4f68c504d10bf168f36fe462f35e6d09.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Towards-Hierarchical-Multi-Step-Reward-Models-for-Enhanced-Reasoning-in-Large-Language-Models"><a href="#Towards-Hierarchical-Multi-Step-Reward-Models-for-Enhanced-Reasoning-in-Large-Language-Models" class="headerlink" title="Towards Hierarchical Multi-Step Reward Models for Enhanced Reasoning in   Large Language Models"></a>Towards Hierarchical Multi-Step Reward Models for Enhanced Reasoning in   Large Language Models</h2><p><strong>Authors:Teng Wang, Zhangyi Jiang, Zhenqi He, Wenhan Yang, Yanan Zheng, Zeyu Li, Zifan He, Shenyang Tong, Hailei Gong</strong></p>
<p>Recent studies show that Large Language Models (LLMs) achieve strong reasoning capabilities through supervised fine-tuning or reinforcement learning. However, a key approach, the Process Reward Model (PRM), suffers from reward hacking, making it unreliable in identifying the best intermediate steps. In this paper, we propose a novel reward model approach, Hierarchical Reward Model (HRM), which evaluates both individual and consecutive reasoning steps from fine-grained and coarse-grained level. HRM performs better in assessing reasoning coherence and self-reflection, particularly when the previous reasoning step is incorrect. Furthermore, to address the inefficiency of autonomous generating PRM training data via Monte Carlo Tree Search (MCTS), we introduce a lightweight and effective data augmentation strategy called Hierarchical Node Compression (HNC) based on node merging (combining two consecutive reasoning steps into one step) in the tree structure. This approach diversifies MCTS results for HRM with negligible computational overhead, enhancing label robustness by introducing noise. Empirical results on the PRM800K dataset demonstrate that HRM, in conjunction with HNC, achieves superior stability and reliability in evaluation compared to PRM. Furthermore, cross-domain evaluations on MATH500 and GSM8K confirm HRMâ€™s superior generalization and robustness across diverse reasoning tasks. The code for all experiments will be released at https: &#x2F;&#x2F;github.com&#x2F;tengwang0318&#x2F;hierarchial_reward_model. </p>
<blockquote>
<p>æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡ç›‘ç£å¾®è°ƒæˆ–å¼ºåŒ–å­¦ä¹ å®ç°äº†å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œä¸€ç§å…³é”®æ–¹æ³•â€”â€”è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰å­˜åœ¨å¥–åŠ±ç ´è§£çš„é—®é¢˜ï¼Œä½¿å…¶æ— æ³•å¯é åœ°è¯†åˆ«æœ€ä½³çš„ä¸­é—´æ­¥éª¤ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å¥–åŠ±æ¨¡å‹æ–¹æ³•ï¼Œå³åˆ†å±‚å¥–åŠ±æ¨¡å‹ï¼ˆHRMï¼‰ï¼Œå®ƒå¯ä»¥è¯„ä¼°ç²¾ç»†ç²’åº¦å’Œç²—ç²’åº¦çš„å•ä¸ªå’Œè¿ç»­æ¨ç†æ­¥éª¤ã€‚HRMåœ¨è¯„ä¼°æ¨ç†è¿è´¯æ€§å’Œè‡ªæˆ‘åæ€æ–¹é¢è¡¨ç°æ›´å¥½ï¼Œå°¤å…¶æ˜¯åœ¨å‰ä¸€ä¸ªæ¨ç†æ­¥éª¤é”™è¯¯çš„æƒ…å†µä¸‹ã€‚æ­¤å¤–ï¼Œä¸ºäº†è§£å†³é€šè¿‡è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰è‡ªä¸»ç”ŸæˆPRMè®­ç»ƒæ•°æ®æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºèŠ‚ç‚¹åˆå¹¶ï¼ˆå°†ä¸¤ä¸ªè¿ç»­çš„æ¨ç†æ­¥éª¤åˆå¹¶ä¸ºä¸€ä¸ªæ­¥éª¤ï¼‰çš„æ ‘ç»“æ„ä¸­çš„åˆ†å±‚èŠ‚ç‚¹å‹ç¼©ï¼ˆHNCï¼‰çš„è½»é‡çº§æœ‰æ•ˆæ•°æ®å¢å¼ºç­–ç•¥ã€‚è¯¥æ–¹æ³•åœ¨HRMä¸­å®ç°äº†å¤šæ ·åŒ–çš„MCTSç»“æœï¼ŒåŒæ—¶è®¡ç®—å¼€é”€å¾ˆå°ï¼Œé€šè¿‡å¼•å…¥å™ªå£°å¢å¼ºäº†æ ‡ç­¾çš„é²æ£’æ€§ã€‚åœ¨PRM800Kæ•°æ®é›†ä¸Šçš„ç»éªŒç»“æœè¡¨æ˜ï¼Œä¸HNCç»“åˆçš„HRMåœ¨è¯„ä¼°ä¸­å®ç°äº†æ¯”PRMæ›´ä¼˜è¶Šçš„ç¨³å®šæ€§å’Œå¯é æ€§ã€‚æ­¤å¤–ï¼ŒMATH500å’ŒGSM8Kçš„è·¨åŸŸè¯„ä¼°è¯å®äº†HRMåœ¨å¤šç§æ¨ç†ä»»åŠ¡ä¸­çš„ä¼˜å¼‚æ³›åŒ–å’Œé²æ£’æ€§ã€‚æ‰€æœ‰å®éªŒçš„ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/tengwang0318/hierarchial_reward_model%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/tengwang0318/hierarchial_reward_modelå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13551v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡ç›‘ç£å¾®è°ƒæˆ–å¼ºåŒ–å­¦ä¹ å±•ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå½“å‰çš„å…³é”®æ–¹æ³•â€”â€”è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰å­˜åœ¨å¥–åŠ±é»‘å®¢é—®é¢˜ï¼Œæ— æ³•å‡†ç¡®è¯†åˆ«æœ€ä½³çš„ä¸­é—´æ­¥éª¤ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¥–åŠ±æ¨¡å‹æ–¹æ³•ï¼Œå³åˆ†å±‚å¥–åŠ±æ¨¡å‹ï¼ˆHRMï¼‰ï¼Œå¯ä»¥ä»ç²¾ç»†ç²’åº¦å’Œç²—ç•¥ç²’åº¦ä¸¤ä¸ªå±‚é¢è¯„ä¼°å•ä¸ªå’Œè¿ç»­çš„æ¨ç†æ­¥éª¤ã€‚HRMåœ¨è¯„ä¼°æ¨ç†è¿è´¯æ€§å’Œè‡ªæˆ‘åæ€æ–¹é¢è¡¨ç°æ›´ä½³ï¼Œå°¤å…¶æ˜¯å½“å…ˆå‰çš„æ¨ç†æ­¥éª¤å‡ºé”™æ—¶ã€‚æ­¤å¤–ï¼Œä¸ºè§£å†³é€šè¿‡è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰è‡ªä¸»ç”ŸæˆPRMè®­ç»ƒæ•°æ®çš„ä¸æ•ˆç‡é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºèŠ‚ç‚¹åˆå¹¶ï¼ˆå°†ä¸¤ä¸ªè¿ç»­çš„æ¨ç†æ­¥éª¤åˆå¹¶ä¸ºä¸€ä¸ªæ­¥éª¤ï¼‰çš„åˆ†å±‚èŠ‚ç‚¹å‹ç¼©ï¼ˆHNCï¼‰è½»é‡çº§æœ‰æ•ˆæ•°æ®å¢å¼ºç­–ç•¥ã€‚è¯¥æ–¹æ³•åœ¨PRM800Kæ•°æ®é›†ä¸Šçš„å®è¯ç»“æœè¡¨æ˜ï¼Œä¸PRMç›¸æ¯”ï¼ŒHRMç»“åˆHNCå®ç°äº†æ›´ç¨³å®šå¯é çš„è¯„ä¼°ã€‚è·¨åŸŸè¯„ä¼°MATH500å’ŒGSM8Kçš„ç»“æœéªŒè¯äº†HRMåœ¨ä¸åŒæ¨ç†ä»»åŠ¡ä¸­çš„ä¼˜å¼‚æ³›åŒ–èƒ½åŠ›å’Œç¨³å¥æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså±•ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œé€šè¿‡ç›‘ç£å¾®è°ƒæˆ–å¼ºåŒ–å­¦ä¹ å®ç°ã€‚</li>
<li>å½“å‰çš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰å­˜åœ¨å¥–åŠ±é»‘å®¢é—®é¢˜ï¼Œæ— æ³•å‡†ç¡®è¯†åˆ«æœ€ä½³ä¸­é—´æ­¥éª¤ã€‚</li>
<li>åˆ†å±‚å¥–åŠ±æ¨¡å‹ï¼ˆHRMï¼‰å¯ä»¥è¯„ä¼°å•ä¸ªå’Œè¿ç»­çš„æ¨ç†æ­¥éª¤ï¼Œåœ¨è¯„ä¼°æ¨ç†è¿è´¯æ€§å’Œè‡ªæˆ‘åæ€æ–¹é¢è¡¨ç°æ›´ä½³ã€‚</li>
<li>å¼•å…¥åˆ†å±‚èŠ‚ç‚¹å‹ç¼©ï¼ˆHNCï¼‰ç­–ç•¥ï¼ŒåŸºäºèŠ‚ç‚¹åˆå¹¶ï¼Œæé«˜è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰ç»“æœçš„å¤šæ ·æ€§ï¼Œå¢å¼ºæ ‡ç­¾ç¨³å¥æ€§ã€‚</li>
<li>åœ¨PRM800Kæ•°æ®é›†ä¸Šï¼ŒHRMç»“åˆHNCè¡¨ç°æ›´ç¨³å®šå¯é ã€‚</li>
<li>è·¨åŸŸè¯„ä¼°MATH500å’ŒGSM8KéªŒè¯äº†HRMçš„ä¼˜å¼‚æ³›åŒ–èƒ½åŠ›å’Œç¨³å¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13551">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4d85bcbdd467aaf3fe9c325a6c42271b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b5f22a5e742f32be421290498f419e3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-537623a767707642cd27dfd8fa03abb2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0999880d92a1a774a606f2f405e64e85.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-29cdb0559b93fc2ef326dac3429acc69.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3bc3660fa1902121dc2185dc75be109.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="PLM-Efficient-Peripheral-Language-Models-Hardware-Co-Designed-for-Ubiquitous-Computing"><a href="#PLM-Efficient-Peripheral-Language-Models-Hardware-Co-Designed-for-Ubiquitous-Computing" class="headerlink" title="PLM: Efficient Peripheral Language Models Hardware-Co-Designed for   Ubiquitous Computing"></a>PLM: Efficient Peripheral Language Models Hardware-Co-Designed for   Ubiquitous Computing</h2><p><strong>Authors:Cheng Deng, Luoyang Sun, Jiwen Jiang, Yongcheng Zeng, Xinjian Wu, Wenxin Zhao, Qingfa Xiao, Jiachuan Wang, Haoyang Li, Lei Chen, Lionel M. Ni, Haifeng Zhang, Jun Wang</strong></p>
<p>While scaling laws have been continuously validated in large language models (LLMs) with increasing model parameters, the inherent tension between the inference demands of LLMs and the limited resources of edge devices poses a critical challenge to the development of edge intelligence. Recently, numerous small language models have emerged, aiming to distill the capabilities of LLMs into smaller footprints. However, these models often retain the fundamental architectural principles of their larger counterparts, still imposing considerable strain on the storage and bandwidth capacities of edge devices. In this paper, we introduce the PLM, a Peripheral Language Model, developed through a co-design process that jointly optimizes model architecture and edge system constraints. The PLM utilizes a Multi-head Latent Attention mechanism and employs the squared ReLU activation function to encourage sparsity, thereby reducing peak memory footprint during inference. During training, we collect and reorganize open-source datasets, implement a multi-phase training strategy, and empirically investigate the Warmup-Stable-Decay-Constant (WSDC) learning rate scheduler. Additionally, we incorporate Reinforcement Learning from Human Feedback (RLHF) by adopting the ARIES preference learning approach. Following a two-phase SFT process, this method yields performance gains of 2% in general tasks, 9% in the GSM8K task, and 11% in coding tasks. In addition to its novel architecture, evaluation results demonstrate that PLM outperforms existing small language models trained on publicly available data while maintaining the lowest number of activated parameters. Furthermore, deployment across various edge devices, including consumer-grade GPUs, mobile phones, and Raspberry Pis, validates PLMâ€™s suitability for peripheral applications. The PLM series models are publicly available at <a target="_blank" rel="noopener" href="https://github.com/plm-team/PLM">https://github.com/plm-team/PLM</a>. </p>
<blockquote>
<p>åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­ï¼Œéšç€æ¨¡å‹å‚æ•°çš„å¢åŠ ï¼Œæ¯”ä¾‹å®šå¾‹ä¸æ–­å¾—åˆ°éªŒè¯ã€‚ç„¶è€Œï¼Œå¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†éœ€æ±‚ä¸è¾¹ç¼˜è®¾å¤‡çš„æœ‰é™èµ„æºä¹‹é—´å­˜åœ¨çš„å›ºæœ‰çŸ›ç›¾ï¼Œç»™è¾¹ç¼˜æ™ºèƒ½çš„å‘å±•å¸¦æ¥äº†å·¨å¤§æŒ‘æˆ˜ã€‚è¿‘æœŸï¼Œå‡ºç°äº†è®¸å¤šå°å‹è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨å°†å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›è½¬åŒ–ä¸ºæ›´å°çš„æ¨¡å‹è§„æ¨¡ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹é€šå¸¸ä¿ç•™å…¶å¤§å‹æ¨¡å‹çš„åŸºæœ¬æ¶æ„åŸåˆ™ï¼Œä»ç„¶å¯¹è¾¹ç¼˜è®¾å¤‡çš„å­˜å‚¨å’Œå¸¦å®½å®¹é‡é€ æˆç›¸å½“å¤§çš„å‹åŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§å‘¨è¾¹è¯­è¨€æ¨¡å‹ï¼ˆPLMï¼‰ï¼Œé€šè¿‡è”åˆä¼˜åŒ–æ¨¡å‹æ¶æ„å’Œè¾¹ç¼˜ç³»ç»Ÿçº¦æŸçš„ååŒè®¾è®¡è¿‡ç¨‹è€Œå¼€å‘ã€‚PLMåˆ©ç”¨å¤šå¤´æ½œåœ¨æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¹¶é‡‡ç”¨å¹³æ–¹ReLUæ¿€æ´»å‡½æ•°æ¥ä¿ƒè¿›ç¨€ç–æ€§ï¼Œä»è€Œå‡å°‘æ¨ç†è¿‡ç¨‹ä¸­çš„å³°å€¼å†…å­˜å ç”¨ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬æ”¶é›†å’Œé‡æ–°ç»„ç»‡äº†å¼€æºæ•°æ®é›†ï¼Œå®æ–½äº†å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œå¹¶å¯¹é¢„çƒ­ç¨³å®šè¡°å‡æ’å®šï¼ˆWSDCï¼‰å­¦ä¹ ç‡è°ƒåº¦å™¨è¿›è¡Œäº†å®è¯ç ”ç©¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡é‡‡ç”¨ARIESåå¥½å­¦ä¹ æ–¹æ³•ï¼Œèå…¥äº†äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ã€‚ç»è¿‡ä¸¤é˜¶æ®µçš„SFTè¿‡ç¨‹åï¼Œè¯¥æ–¹æ³•åœ¨ä¸€èˆ¬ä»»åŠ¡ä¸Šæé«˜äº†2%çš„æ€§èƒ½ï¼Œåœ¨GSM8Kä»»åŠ¡ä¸Šæé«˜äº†9%ï¼Œåœ¨ç¼–ç ä»»åŠ¡ä¸Šæé«˜äº†11%ã€‚é™¤äº†å…¶æ–°é¢–çš„æ¶æ„å¤–ï¼Œè¯„ä¼°ç»“æœè¡¨æ˜PLMåœ¨å…¬å¼€æ•°æ®ä¸Šè®­ç»ƒçš„ç°æœ‰å°å‹è¯­è¨€æ¨¡å‹ä¹‹ä¸Šè¡¨ç°å‡ºè‰²ï¼ŒåŒæ—¶ä¿æŒäº†æœ€ä½çš„æ´»åŠ¨å‚æ•°æ•°é‡ã€‚æ­¤å¤–ï¼Œåœ¨å„ç§è¾¹ç¼˜è®¾å¤‡ä¸Šçš„éƒ¨ç½²ï¼ŒåŒ…æ‹¬æ¶ˆè´¹çº§GPUã€æ‰‹æœºå’ŒRaspberry Pisï¼ŒéªŒè¯äº†PLMåœ¨å‘¨è¾¹åº”ç”¨çš„é€‚ç”¨æ€§ã€‚PLMç³»åˆ—æ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/plm-team/PLM%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/plm-team/PLMå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12167v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„å‘¨è¾¹è¯­è¨€æ¨¡å‹ï¼ˆPLMï¼‰ï¼Œé€šè¿‡è”åˆä¼˜åŒ–æ¨¡å‹æ¶æ„å’Œè¾¹ç¼˜ç³»ç»Ÿçº¦æŸè¿›è¡Œè®¾è®¡ã€‚PLMé‡‡ç”¨å¤šå¤´æ½œåœ¨æ³¨æ„åŠ›æœºåˆ¶å’Œå¹³æ–¹ReLUæ¿€æ´»å‡½æ•°ï¼Œä»¥å‡å°‘æ¨ç†è¿‡ç¨‹ä¸­çš„å†…å­˜å³°å€¼æ¶ˆè€—ã€‚é€šè¿‡æ”¶é›†å’Œç»„ç»‡å¼€æºæ•°æ®é›†ï¼Œå®æ–½å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œå¹¶å¼•å…¥WSDCå­¦ä¹ ç‡è°ƒåº¦å™¨å’Œé‡‡ç”¨ARIESåå¥½å­¦ä¹ æ–¹æ³•çš„å¼ºåŒ–å­¦ä¹ äººç±»åé¦ˆï¼ˆRLHFï¼‰ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼ŒPLMåœ¨ä¸€èˆ¬æ€§ä»»åŠ¡ã€GSM8Kä»»åŠ¡å’Œç¼–ç ä»»åŠ¡ä¸Šçš„æ€§èƒ½åˆ†åˆ«æé«˜äº†2%ã€9%å’Œ11%ã€‚ä¸å…¶ä»–å°å‹è¯­è¨€æ¨¡å‹ç›¸æ¯”ï¼ŒPLMåœ¨å…¬å¼€æ•°æ®ä¸Šçš„è¡¨ç°æ›´ä¼˜ç§€ï¼ŒåŒæ—¶æ‹¥æœ‰æ¿€æ´»å‚æ•°æ•°é‡æœ€å°‘çš„ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼ŒPLMç³»åˆ—æ¨¡å‹å·²åœ¨å„ç§è¾¹ç¼˜è®¾å¤‡ä¸Šè¿›è¡Œäº†éƒ¨ç½²éªŒè¯ï¼ŒåŒ…æ‹¬æ¶ˆè´¹çº§GPUã€æ‰‹æœºå’ŒRaspberry Piç­‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PLMæ˜¯ä¸€ç§æ–°å‹çš„å‘¨è¾¹è¯­è¨€æ¨¡å‹ï¼Œé’ˆå¯¹è¾¹ç¼˜è®¾å¤‡çš„èµ„æºé™åˆ¶è¿›è¡Œä¼˜åŒ–è®¾è®¡ã€‚</li>
<li>PLMé‡‡ç”¨å¤šå¤´æ½œåœ¨æ³¨æ„åŠ›æœºåˆ¶å’Œå¹³æ–¹ReLUæ¿€æ´»å‡½æ•°æ¥å‡å°‘æ¨ç†æ—¶çš„å†…å­˜æ¶ˆè€—ã€‚</li>
<li>PLMé€šè¿‡æ”¶é›†å’Œç»„ç»‡å¼€æºæ•°æ®é›†ï¼Œå®æ–½å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥æ¥æé«˜æ€§èƒ½ã€‚</li>
<li>PLMå¼•å…¥WSDCå­¦ä¹ ç‡è°ƒåº¦å™¨å’Œå¼ºåŒ–å­¦ä¹ äººç±»åé¦ˆï¼ˆRLHFï¼‰æ–¹æ³•ï¼Œæå‡æ¨¡å‹è¡¨ç°ã€‚</li>
<li>PLMåœ¨ä¸€èˆ¬æ€§ä»»åŠ¡ã€GSM8Kä»»åŠ¡å’Œç¼–ç ä»»åŠ¡ä¸Šçš„æ€§èƒ½æå‡æ˜¾è‘—ã€‚</li>
<li>PLMåœ¨å…¬å¼€æ•°æ®ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–å°å‹è¯­è¨€æ¨¡å‹ï¼ŒåŒæ—¶å…·æœ‰è¾ƒå°‘çš„æ¿€æ´»å‚æ•°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12167">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-3a5f7e629de109742bc712639db34e41.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b62586a274c11e3234ea155ce95fcc13.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-297c05f6376ec4ad4815db42b33ccff5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d0ca0bab6691aa042149c85b6c9d8c75.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-13c6d12b861dd39abed42fa8ae9baf0e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b4a5e9a73631efc6ab474d2cf9a3a6c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3f4ebb7fcdb92336312466784ada57e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f0401b5fbcdd6d35fd9814e92318f1a5.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Reinforcement-Learning-Outperforms-Supervised-Fine-Tuning-A-Case-Study-on-Audio-Question-Answering"><a href="#Reinforcement-Learning-Outperforms-Supervised-Fine-Tuning-A-Case-Study-on-Audio-Question-Answering" class="headerlink" title="Reinforcement Learning Outperforms Supervised Fine-Tuning: A Case Study   on Audio Question Answering"></a>Reinforcement Learning Outperforms Supervised Fine-Tuning: A Case Study   on Audio Question Answering</h2><p><strong>Authors:Gang Li, Jizhong Liu, Heinrich Dinkel, Yadong Niu, Junbo Zhang, Jian Luan</strong></p>
<p>Recently, reinforcement learning (RL) has been shown to greatly enhance the reasoning capabilities of large language models (LLMs), and RL-based approaches have been progressively applied to visual multimodal tasks. However, the audio modality has largely been overlooked in these developments. Thus, we conduct a series of RL explorations in audio understanding and reasoning, specifically focusing on the audio question answering (AQA) task. We leverage the group relative policy optimization (GRPO) algorithm to Qwen2-Audio-7B-Instruct, and our experiments demonstrated state-of-the-art performance on the MMAU Test-mini benchmark, achieving an accuracy rate of 64.5%. The main findings in this technical report are as follows: 1) The GRPO algorithm can be effectively applied to large audio language models (LALMs), even when the model has only 8.2B parameters; 2) With only 38k post-training samples, RL significantly outperforms supervised fine-tuning (SFT), indicating that RL-based approaches can be effective without large datasets; 3) The explicit reasoning process has not shown significant benefits for AQA tasks, and how to efficiently utilize deep thinking remains an open question for further research; 4) LALMs still lag far behind humans auditory-language reasoning, suggesting that the RL-based approaches warrant further exploration. Our project is available at <a target="_blank" rel="noopener" href="https://github.com/xiaomi-research/r1-aqa">https://github.com/xiaomi-research/r1-aqa</a> and <a target="_blank" rel="noopener" href="https://huggingface.co/mispeech/r1-aqa">https://huggingface.co/mispeech/r1-aqa</a>. </p>
<blockquote>
<p>è¿‘æœŸï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¢«è¯æ˜å¯ä»¥æå¤§åœ°æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶ä¸”åŸºäºRLçš„æ–¹æ³•å·²é€æ¸åº”ç”¨äºè§†è§‰å¤šæ¨¡æ€ä»»åŠ¡ã€‚ç„¶è€Œï¼ŒéŸ³é¢‘æ¨¡æ€åœ¨è¿™äº›å‘å±•ä¸­å´è¢«å¾ˆå¤§ç¨‹åº¦ä¸Šå¿½è§†äº†ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åœ¨éŸ³é¢‘ç†è§£å’Œæ¨ç†ä¸­è¿›è¡Œäº†ä¸€ç³»åˆ—RLæ¢ç´¢ï¼Œç‰¹åˆ«å…³æ³¨éŸ³é¢‘é—®ç­”ï¼ˆAQAï¼‰ä»»åŠ¡ã€‚æˆ‘ä»¬åˆ©ç”¨é›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç®—æ³•å¯¹Qwen2-Audio-7B-Instructè¿›è¡Œäº†ä¼˜åŒ–ï¼Œå®éªŒè¡¨æ˜åœ¨MMAU Test-miniåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå‡†ç¡®ç‡ä¸º64.5%ã€‚æœ¬æŠ€æœ¯æŠ¥å‘Šçš„ä¸»è¦å‘ç°å¦‚ä¸‹ï¼š1ï¼‰GRPOç®—æ³•å¯æœ‰æ•ˆåº”ç”¨äºå¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLALMï¼‰ï¼Œå³ä½¿æ¨¡å‹åªæœ‰8.2Bå‚æ•°ï¼›2ï¼‰ä»…ä½¿ç”¨38kä¸ªåè®­ç»ƒæ ·æœ¬ï¼ŒRLå°±æ˜¾è‘—ä¼˜äºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œè¡¨æ˜åŸºäºRLçš„æ–¹æ³•åœ¨ä¸éœ€è¦å¤§è§„æ¨¡æ•°æ®é›†çš„æƒ…å†µä¸‹ä¹Ÿèƒ½æœ‰æ•ˆï¼›3ï¼‰æ˜ç¡®çš„æ¨ç†è¿‡ç¨‹å¹¶æœªæ˜¾ç¤ºå‡ºå¯¹AQAä»»åŠ¡çš„é‡å¤§ç›Šå¤„ï¼Œå¦‚ä½•æœ‰æ•ˆåˆ©ç”¨æ·±åº¦æ€è€ƒä»æ˜¯æœªæ¥ç ”ç©¶çš„ä¸€ä¸ªå¼€æ”¾é—®é¢˜ï¼›4ï¼‰LALMä»ç„¶è¿œè¿œè½åäºäººç±»çš„å¬è§‰è¯­è¨€æ¨ç†ï¼Œè¿™è¡¨æ˜åŸºäºRLçš„æ–¹æ³•éœ€è¦è¿›ä¸€æ­¥æ¢ç´¢ã€‚æˆ‘ä»¬çš„é¡¹ç›®å¯åœ¨[<a target="_blank" rel="noopener" href="https://github.com/xiaomi-research/r1-aqa%E5%92%8Chttps://huggingface.co/mispeech/r1-aqa%E8%AE%BF%E9%97%AE%E3%80%82]">https://github.com/xiaomi-research/r1-aqaå’Œhttps://huggingface.co/mispeech/r1-aqaè®¿é—®ã€‚]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.11197v3">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œä¸”RLæ–¹æ³•å·²é€æ¸åº”ç”¨äºè§†è§‰å¤šæ¨¡æ€ä»»åŠ¡ã€‚ç„¶è€Œï¼ŒéŸ³é¢‘æ¨¡å¼åœ¨è¿™äº›å‘å±•ä¸­å´è¢«å¿½è§†äº†ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åœ¨éŸ³é¢‘ç†è§£å’Œæ¨ç†é¢†åŸŸè¿›è¡Œäº†ä¸€ç³»åˆ—RLæ¢ç´¢ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹éŸ³é¢‘é—®ç­”ï¼ˆAQAï¼‰ä»»åŠ¡ã€‚æˆ‘ä»¬åˆ©ç”¨ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç®—æ³•å¯¹Qwen2-Audio-7B-Instructè¿›è¡Œäº†ä¼˜åŒ–ï¼Œå®éªŒè¡¨æ˜åœ¨MMAU Test-miniåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†é¢†å…ˆæ°´å¹³ï¼Œå‡†ç¡®ç‡ä¸º64.5%ã€‚æœ¬æŠ€æœ¯æŠ¥å‘Šçš„ä¸»è¦å‘ç°åŒ…æ‹¬ï¼šGRPOç®—æ³•å¯æœ‰æ•ˆåœ°åº”ç”¨äºå¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLALMï¼‰ï¼›ä»…éœ€3.8ä¸‡ä»½è®­ç»ƒåæ ·æœ¬ï¼ŒRLä¾¿æ˜¾è‘—ä¼˜äºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œè¡¨æ˜RLæ–¹æ³•å¯åœ¨æ— éœ€å¤§å‹æ•°æ®é›†çš„æƒ…å†µä¸‹æœ‰æ•ˆï¼›åœ¨AQAä»»åŠ¡ä¸­ï¼Œæ˜ç¡®çš„æ¨ç†è¿‡ç¨‹å¹¶æœªæ˜¾ç¤ºå‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œå¦‚ä½•æœ‰æ•ˆåˆ©ç”¨æ·±åº¦æ€è€ƒä»æ˜¯æœªæ¥ç ”ç©¶çš„å¼€æ”¾é—®é¢˜ï¼›LALMåœ¨å¬è§‰è¯­è¨€æ¨ç†æ–¹é¢ä»è¿œè¿œè½åäºäººç±»ï¼Œè¿™è¡¨æ˜RLæ–¹æ³•éœ€è¦è¿›ä¸€æ­¥æ¢ç´¢ã€‚æˆ‘ä»¬çš„é¡¹ç›®å¯åœ¨<a href="#">é“¾æ¥1</a>å’Œ<a href="#">é“¾æ¥2</a>æ‰¾åˆ°ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>GRPOç®—æ³•å¯æœ‰æ•ˆåº”ç”¨äºå¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLALMï¼‰ã€‚</li>
<li>åœ¨ä»…æœ‰å°‘é‡è®­ç»ƒåæ ·æœ¬çš„æƒ…å†µä¸‹ï¼Œå¼ºåŒ–å­¦ä¹ æ˜¾è‘—ä¼˜äºç›‘ç£å¾®è°ƒã€‚</li>
<li>åœ¨AQAä»»åŠ¡ä¸­ï¼Œæ˜ç¡®çš„æ¨ç†è¿‡ç¨‹å¹¶æœªæ˜¾ç¤ºå‡ºæ˜æ˜¾ä¼˜åŠ¿ã€‚</li>
<li>å¦‚ä½•æœ‰æ•ˆåˆ©ç”¨æ·±åº¦æ€è€ƒåœ¨AQAä»»åŠ¡ä¸­æ˜¯æœªæ¥ç ”ç©¶çš„é‡ç‚¹ã€‚</li>
<li>LALMåœ¨å¬è§‰è¯­è¨€æ¨ç†æ–¹é¢ä»è¿œè¿œè½åäºäººç±»ã€‚</li>
<li>RLæ–¹æ³•åœ¨éŸ³é¢‘é—®ç­”é¢†åŸŸçš„æ½œåŠ›å·¨å¤§ï¼Œå€¼å¾—è¿›ä¸€æ­¥æ¢ç´¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.11197">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-388314d48f17825aaef7a5b1dc0830d2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4c97be7ba52440a6ae4a1f243f393377.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-07ee4c5a9ee74401f65fa7d338581db2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-73d766d91c02ddb2edf1681e5ba53502.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-abeef9e0258bc8346c48c955699c805d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e0154e2c76754beb485b201ea8b43e3f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-80290bf5c983f03ca3460976ec31d55a.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Predictable-Scale-Part-I-â€“-Optimal-Hyperparameter-Scaling-Law-in-Large-Language-Model-Pretraining"><a href="#Predictable-Scale-Part-I-â€“-Optimal-Hyperparameter-Scaling-Law-in-Large-Language-Model-Pretraining" class="headerlink" title="Predictable Scale: Part I â€“ Optimal Hyperparameter Scaling Law in Large   Language Model Pretraining"></a>Predictable Scale: Part I â€“ Optimal Hyperparameter Scaling Law in Large   Language Model Pretraining</h2><p><strong>Authors:Houyi Li, Wenzhen Zheng, Jingcheng Hu, Qiufeng Wang, Hanshan Zhang, Zili Wang, Shijie Xuyang, Yuantao Fan, Shuigeng Zhou, Xiangyu Zhang, Daxin Jiang</strong></p>
<p>The impressive capabilities of Large Language Models (LLMs) across diverse tasks are now well-established, yet their effective deployment necessitates careful hyperparameter optimization. Through extensive empirical studies involving grid searches across diverse configurations, we discover universal scaling laws governing these hyperparameters: optimal learning rate follows a power-law relationship with both model parameters and data sizes, while optimal batch size scales primarily with data sizes. Our analysis reveals a convex optimization landscape for hyperparameters under fixed models and data size conditions. This convexity implies an optimal hyperparameter plateau. We contribute a universal, plug-and-play optimal hyperparameter tool for the community. Its estimated values on the test set are merely 0.09% away from the globally optimal LLM performance found via an exhaustive search. These laws demonstrate remarkable robustness across variations in model sparsity, training data distribution, and model shape. To our best known, this is the first work that unifies different model shapes and structures, such as Mixture-of-Experts models and dense transformers, as well as establishes optimal hyperparameter scaling laws across diverse data distributions. This exhaustive optimization process demands substantial computational resources, utilizing nearly one million NVIDIA H800 GPU hours to train 3,700 LLMs of varying sizes and hyperparameters from scratch and consuming approximately 100 trillion tokens in total. To facilitate reproducibility and further research, we will progressively release all loss measurements and model checkpoints through our designated repository <a target="_blank" rel="noopener" href="https://step-law.github.io/">https://step-law.github.io/</a> </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šç§ä»»åŠ¡ä¸Šçš„å‡ºè‰²èƒ½åŠ›ç°å·²å¾—åˆ°å¹¿æ³›è®¤å¯ï¼Œä½†å…¶æœ‰æ•ˆéƒ¨ç½²ä»éœ€è¿›è¡Œç»†è‡´çš„è¶…å‚æ•°ä¼˜åŒ–ã€‚é€šè¿‡æ¶‰åŠä¸åŒé…ç½®çš„ç½‘æ ¼æœç´¢çš„å¹¿æ³›å®è¯ç ”ç©¶ï¼Œæˆ‘ä»¬å‘ç°äº†è¿™äº›è¶…å‚æ•°çš„é€šç”¨ç¼©æ”¾å®šå¾‹ï¼šæœ€ä½³å­¦ä¹ ç‡ä¸æ¨¡å‹å‚æ•°å’Œæ•°æ®å¤§å°å‘ˆå¹‚å¾‹å…³ç³»ï¼Œè€Œæœ€ä½³æ‰¹æ¬¡å¤§å°ä¸»è¦éšæ•°æ®å¤§å°è¿›è¡Œç¼©æ”¾ã€‚æˆ‘ä»¬çš„åˆ†ææ˜¾ç¤ºï¼Œåœ¨å›ºå®šæ¨¡å‹å’Œæ•°æ®å¤§å°æ¡ä»¶ä¸‹ï¼Œè¶…å‚æ•°çš„ä¼˜åŒ–æ™¯è§‚å‘ˆç°å‡¸æ€§ã€‚è¿™ç§å‡¸æ€§æ„å‘³ç€å­˜åœ¨ä¸€ä¸ªæœ€ä½³è¶…å‚æ•°å¹³å°ã€‚æˆ‘ä»¬ä¸ºç¤¾åŒºè´¡çŒ®äº†ä¸€ä¸ªé€šç”¨çš„ã€å³æ’å³ç”¨çš„æœ€ä½³è¶…å‚æ•°å·¥å…·ã€‚å…¶åœ¨æµ‹è¯•é›†ä¸Šçš„ä¼°è®¡å€¼ä¸é€šè¿‡ç©·å°½æœç´¢æ‰¾åˆ°çš„å…¨å±€æœ€ä½³LLMæ€§èƒ½ä»…ç›¸å·®0.09%ã€‚è¿™äº›å®šå¾‹åœ¨æ¨¡å‹ç¨€ç–æ€§ã€è®­ç»ƒæ•°æ®åˆ†å¸ƒå’Œæ¨¡å‹å½¢çŠ¶çš„å˜åŒ–ä¸­è¡¨ç°å‡ºæƒŠäººçš„ç¨³å¥æ€§ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€é¡¹ç»Ÿä¸€ä¸åŒæ¨¡å‹å½¢çŠ¶å’Œç»“æ„çš„å·¥ä½œï¼Œå¦‚ä¸“å®¶æ··åˆæ¨¡å‹å’Œå¯†é›†è½¬æ¢å™¨ï¼Œå¹¶å»ºç«‹äº†ä¸åŒæ•°æ®åˆ†å¸ƒä¸‹çš„æœ€ä½³è¶…å‚æ•°ç¼©æ”¾å®šå¾‹ã€‚è¿™ä¸€è¯¦å°½çš„ä¼˜åŒ–è¿‡ç¨‹éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºï¼Œåˆ©ç”¨è¿‘ç™¾ä¸‡çš„NVIDIA H800 GPUå°æ—¶ä»å¤´å¼€å§‹è®­ç»ƒ3700ä¸ªä¸åŒå¤§å°å’Œè¶…å‚æ•°çš„LLMï¼Œæ€»å…±æ¶ˆè€—çº¦10ä¸‡äº¿ä¸ªä»¤ç‰Œã€‚ä¸ºäº†æ–¹ä¾¿å¤ç°å’Œè¿›ä¸€æ­¥ç ”ç©¶ï¼Œæˆ‘ä»¬å°†é€æ­¥å‘å¸ƒæ‰€æœ‰æŸå¤±æµ‹é‡å’Œæ¨¡å‹æ£€æŸ¥ç‚¹ï¼Œå¯é€šè¿‡æˆ‘ä»¬çš„æŒ‡å®šä»“åº“<a target="_blank" rel="noopener" href="https://step-law.github.io/%E8%BF%BD%E9%9C%80%E3%80%82">https://step-law.github.io/è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04715v4">PDF</a> 22 pages</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„é¡¹ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†å…¶æœ‰æ•ˆéƒ¨ç½²éœ€è¦è¿›è¡Œç²¾ç»†çš„è¶…å‚æ•°ä¼˜åŒ–ã€‚é€šè¿‡å¹¿æ³›çš„å®è¯ç ”ç©¶ï¼Œæˆ‘ä»¬å‘ç°äº†æ™®éçš„è¶…å‚æ•°ç¼©æ”¾å®šå¾‹ï¼šæœ€ä½³å­¦ä¹ ç‡ä¸æ¨¡å‹å‚æ•°å’Œæ•°æ®é›†å¤§å°å‘ˆå¹‚å¾‹å…³ç³»ï¼Œè€Œæœ€ä½³æ‰¹æ¬¡å¤§å°ä¸»è¦éšæ•°æ®é›†å¤§å°è€Œç¼©æ”¾ã€‚åˆ†ææ˜¾ç¤ºï¼Œåœ¨å›ºå®šæ¨¡å‹å’Œæ•°æ®é›†æ¡ä»¶ä¸‹ï¼Œè¶…å‚æ•°å…·æœ‰å‡¸ä¼˜åŒ–æ™¯è§‚ï¼Œæ„å‘³ç€å­˜åœ¨æœ€ä½³è¶…å‚æ•°å¹³å°ã€‚æˆ‘ä»¬ä¸ºç¤¾åŒºè´¡çŒ®äº†ä¸€ä¸ªé€šç”¨ã€å³æ’å³ç”¨çš„æœ€ä½³è¶…å‚æ•°å·¥å…·ï¼Œå…¶ä¼°è®¡å€¼åœ¨æµ‹è¯•é›†ä¸Šè·ç¦»é€šè¿‡å…¨é¢æœç´¢æ‰¾åˆ°çš„å…¨å±€æœ€ä½³LLMæ€§èƒ½ä»…æœ‰0.09%çš„å·®è·ã€‚è¿™äº›å®šå¾‹åœ¨æ¨¡å‹ç¨€ç–æ€§ã€è®­ç»ƒæ•°æ®åˆ†å¸ƒå’Œæ¨¡å‹ç»“æ„å˜åŒ–æ–¹é¢å±•ç°å‡ºæƒŠäººçš„ç¨³å¥æ€§ã€‚æˆ‘ä»¬çš„å·¥ä½œç»Ÿä¸€äº†ä¸åŒçš„æ¨¡å‹ç»“æ„å’Œå½¢çŠ¶ï¼Œå¦‚Mixture-of-Expertsæ¨¡å‹å’Œå¯†é›†å˜å‹å™¨ï¼Œå¹¶å»ºç«‹äº†è·¨ä¸åŒæ•°æ®åˆ†å¸ƒçš„æœ€ä½³è¶…å‚æ•°ç¼©æ”¾å®šå¾‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨å¤šç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œè¶…å‚æ•°ä¼˜åŒ–å¯¹å…¶æœ‰æ•ˆéƒ¨ç½²è‡³å…³é‡è¦ã€‚</li>
<li>é€šè¿‡å®è¯ç ”ç©¶ï¼Œå‘ç°äº†è¶…å‚æ•°çš„æ™®éç¼©æ”¾å®šå¾‹ï¼ŒåŒ…æ‹¬æœ€ä½³å­¦ä¹ ç‡ä¸æ¨¡å‹å‚æ•°å’Œæ•°æ®é›†å¤§å°çš„å¹‚å¾‹å…³ç³»ä»¥åŠæœ€ä½³æ‰¹æ¬¡å¤§å°ä¸æ•°æ®é›†å¤§å°çš„å…³è”ã€‚</li>
<li>å›ºå®šæ¨¡å‹å’Œæ•°æ®é›†æ¡ä»¶ä¸‹ï¼Œè¶…å‚æ•°å…·æœ‰å‡¸ä¼˜åŒ–æ™¯è§‚ï¼Œå­˜åœ¨æœ€ä½³è¶…å‚æ•°å¹³å°ã€‚</li>
<li>æå‡ºä¸€ä¸ªé€šç”¨ã€å³æ’å³ç”¨çš„æœ€ä½³è¶…å‚æ•°å·¥å…·ï¼Œå…¶æ€§èƒ½æ¥è¿‘å…¨å±€æœ€ä¼˜ã€‚</li>
<li>è¿™äº›å®šå¾‹åœ¨æ¨¡å‹ç¨€ç–æ€§ã€è®­ç»ƒæ•°æ®åˆ†å¸ƒå’Œæ¨¡å‹ç»“æ„å˜åŒ–æ–¹é¢è¡¨ç°å‡ºç¨³å¥æ€§ã€‚</li>
<li>ç ”ç©¶æˆæœç»Ÿä¸€äº†ä¸åŒçš„æ¨¡å‹ç»“æ„å’Œå½¢çŠ¶ï¼Œå¦‚Mixture-of-Expertsæ¨¡å‹å’Œå¯†é›†å˜å‹å™¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04715">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c1e632d604ddb46f68b1c54626dabea3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e40bdc66bab28e6cd68862b2d2a8a0aa.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-88e087ed23230eec2409f067201dbbe8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-010f385fdbd828a4ed164cc05ac2bdef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73d2e510691da61526bc11fa0295f9cd.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Bootstrap-Your-Own-Context-Length"><a href="#Bootstrap-Your-Own-Context-Length" class="headerlink" title="Bootstrap Your Own Context Length"></a>Bootstrap Your Own Context Length</h2><p><strong>Authors:Liang Wang, Nan Yang, Xingxing Zhang, Xiaolong Huang, Furu Wei</strong></p>
<p>We introduce a bootstrapping approach to train long-context language models by exploiting their short-context capabilities only. Our method utilizes a simple agent workflow to synthesize diverse long-context instruction tuning data, thereby eliminating the necessity for manual data collection and annotation. The proposed data synthesis workflow requires only a short-context language model, a text retriever, and a document collection, all of which are readily accessible within the open-source ecosystem. Subsequently, language models are fine-tuned using the synthesized data to extend their context lengths. In this manner, we effectively transfer the short-context capabilities of language models to long-context scenarios through a bootstrapping process. We conduct experiments with the open-source Llama-3 family of models and demonstrate that our method can successfully extend the context length to up to 1M tokens, achieving superior performance across various benchmarks. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨é•¿æ–‡æœ¬ç”Ÿæˆæ¨¡å‹çŸ­è¯­å¢ƒèƒ½åŠ›çš„å¼•å¯¼æ–¹æ³•æ¥è¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ä¸€ä¸ªç®€å•çš„ä»£ç†å·¥ä½œæµç¨‹æ¥åˆæˆå¤šç§é•¿è¯­å¢ƒæŒ‡ä»¤è°ƒæ•´æ•°æ®ï¼Œä»è€Œæ¶ˆé™¤äº†æ‰‹åŠ¨æ”¶é›†å’Œæ ‡æ³¨æ•°æ®çš„å¿…è¦æ€§ã€‚æ‰€æå‡ºçš„æ•°æ®åˆæˆå·¥ä½œæµç¨‹ä»…éœ€è¦çŸ­è¯­å¢ƒè¯­è¨€æ¨¡å‹ã€æ–‡æœ¬æ£€ç´¢å™¨å’Œæ–‡æ¡£é›†åˆï¼Œæ‰€æœ‰è¿™äº›éƒ½å¯ä»¥åœ¨å¼€æºç”Ÿæ€ç³»ç»Ÿä¸­è½»æ¾è·å–ã€‚éšåï¼Œä½¿ç”¨åˆæˆæ•°æ®å¯¹è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥æ‰©å±•å…¶ä¸Šä¸‹æ–‡é•¿åº¦ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬é€šè¿‡ä¸€ä¸ªå¼•å¯¼è¿‡ç¨‹æœ‰æ•ˆåœ°å°†è¯­è¨€æ¨¡å‹çš„çŸ­è¯­å¢ƒèƒ½åŠ›è½¬ç§»åˆ°é•¿è¯­å¢ƒåœºæ™¯ä¸­ã€‚æˆ‘ä»¬ä½¿ç”¨å¼€æºçš„Llama-3ç³»åˆ—æ¨¡å‹è¿›è¡Œå®éªŒï¼Œå¹¶è¯æ˜æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥å°†ä¸Šä¸‹æ–‡é•¿åº¦æˆåŠŸæ‰©å±•åˆ°æœ€å¤š1Mä¸ªä»¤ç‰Œï¼Œåœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18860v2">PDF</a> 19 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åˆ©ç”¨çŸ­è¯­å¢ƒèƒ½åŠ›æ¥è®­ç»ƒé•¿è¯­å¢ƒè¯­è¨€æ¨¡å‹çš„å¼•å¯¼æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡ç®€å•çš„ä»£ç†å·¥ä½œæµç¨‹åˆæˆå¤šæ ·çš„é•¿è¯­å¢ƒæŒ‡ä»¤è°ƒæ•´æ•°æ®ï¼Œæ— éœ€æ‰‹åŠ¨æ”¶é›†å’Œæ ‡æ³¨æ•°æ®ã€‚ä½¿ç”¨çŸ­è¯­å¢ƒè¯­è¨€æ¨¡å‹ã€æ–‡æœ¬æ£€ç´¢å™¨å’Œæ–‡æ¡£é›†åˆç­‰å¼€æºç”Ÿæ€ç³»ç»Ÿä¸­çš„èµ„æºå³å¯å®ç°æ•°æ®åˆæˆã€‚éšåï¼Œä½¿ç”¨åˆæˆæ•°æ®å¯¹è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒä»¥æ‰©å±•å…¶ä¸Šä¸‹æ–‡é•¿åº¦ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬é€šè¿‡å¼•å¯¼è¿‡ç¨‹æœ‰æ•ˆåœ°å°†è¯­è¨€æ¨¡å‹çš„çŸ­è¯­å¢ƒèƒ½åŠ›è½¬ç§»åˆ°é•¿è¯­å¢ƒåœºæ™¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥äº†ä¸€ç§åˆ©ç”¨çŸ­è¯­å¢ƒèƒ½åŠ›è®­ç»ƒé•¿è¯­å¢ƒè¯­è¨€æ¨¡å‹çš„å¼•å¯¼æ–¹æ³•ã€‚</li>
<li>é€šè¿‡ç®€å•çš„ä»£ç†å·¥ä½œæµç¨‹åˆæˆé•¿è¯­å¢ƒæŒ‡ä»¤è°ƒæ•´æ•°æ®ï¼Œé¿å…æ‰‹åŠ¨æ•°æ®æ”¶é›†å’Œæ ‡æ³¨ã€‚</li>
<li>ä½¿ç”¨çŸ­è¯­å¢ƒè¯­è¨€æ¨¡å‹ã€æ–‡æœ¬æ£€ç´¢å™¨å’Œæ–‡æ¡£é›†åˆç­‰å¼€æºèµ„æºå®ç°æ•°æ®åˆæˆã€‚</li>
<li>é€šè¿‡å¾®è°ƒè¯­è¨€æ¨¡å‹ä½¿ç”¨åˆæˆæ•°æ®æ¥æ‰©å±•å…¶ä¸Šä¸‹æ–‡é•¿åº¦ã€‚</li>
<li>æˆåŠŸå°†è¯­è¨€æ¨¡å‹çš„çŸ­è¯­å¢ƒèƒ½åŠ›è½¬ç§»åˆ°é•¿è¯­å¢ƒåœºæ™¯ã€‚</li>
<li>åœ¨å¼€æºLlama-3å®¶æ—æ¨¡å‹ä¸Šè¿›è¡Œäº†å®éªŒï¼ŒæˆåŠŸå°†ä¸Šä¸‹æ–‡é•¿åº¦æ‰©å±•åˆ°1Mä»¤ç‰Œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18860">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2c33a8d601bbff255b890e529ac3edfc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c6fe93fec98c2b03721402ee347f7b4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6887075798d5261f614a01d352c78fee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b25ef66750857a4887482e874cbdf2f0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dbdf7f19c3b8680ade5d3cd2145ac0a9.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="LLaVA-UHD-v2-an-MLLM-Integrating-High-Resolution-Semantic-Pyramid-via-Hierarchical-Window-Transformer"><a href="#LLaVA-UHD-v2-an-MLLM-Integrating-High-Resolution-Semantic-Pyramid-via-Hierarchical-Window-Transformer" class="headerlink" title="LLaVA-UHD v2: an MLLM Integrating High-Resolution Semantic Pyramid via   Hierarchical Window Transformer"></a>LLaVA-UHD v2: an MLLM Integrating High-Resolution Semantic Pyramid via   Hierarchical Window Transformer</h2><p><strong>Authors:Yipeng Zhang, Yifan Liu, Zonghao Guo, Yidan Zhang, Xuesong Yang, Xiaoying Zhang, Chi Chen, Jun Song, Bo Zheng, Yuan Yao, Zhiyuan Liu, Tat-Seng Chua, Maosong Sun</strong></p>
<p>Vision transformers (ViTs) are widely employed in multimodal large language models (MLLMs) for visual encoding. However, they exhibit inferior performance on tasks regarding fine-grained visual perception. We attribute this to the limitations of ViTs in capturing diverse multi-modal visual levels, such as low-level details. To address this issue, we present LLaVA-UHD v2, an MLLM with advanced perception abilities by introducing a well-designed vision-language projector, the Hierarchical window (Hiwin) transformer. Hiwin transformer enhances MLLMâ€™s ability to capture diverse multi-modal visual granularities, by incorporating our constructed high-resolution semantic pyramid. Specifically, Hiwin transformer comprises two key modules: (i) a visual detail injection module, which progressively injects low-level visual details into high-level language-aligned semantics features, thereby forming an inverse semantic pyramid (ISP), and (ii) a hierarchical window attention module, which leverages cross-scale windows to condense multi-level semantics from the ISP. Extensive experiments show that LLaVA-UHD v2 outperforms compared MLLMs on a wide range of benchmarks. Notably, our design achieves an average boost of 3.7% across 14 benchmarks compared with the baseline method, 9.3% on DocVQA for instance. All the data and code will be publicly available to facilitate future research. </p>
<blockquote>
<p>è§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸­å¹¿æ³›åº”ç”¨äºè§†è§‰ç¼–ç ã€‚ç„¶è€Œï¼Œåœ¨ç²¾ç»†è§†è§‰æ„ŸçŸ¥ä»»åŠ¡æ–¹é¢ï¼Œå®ƒä»¬çš„è¡¨ç°è¾ƒå·®ã€‚æˆ‘ä»¬å°†è¿™å½’å› äºViTsåœ¨æ•æ‰å¤šæ ·å¤šæ¨¡æ€è§†è§‰å±‚æ¬¡æ–¹é¢çš„å±€é™æ€§ï¼Œä¾‹å¦‚ä½çº§ç»†èŠ‚ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†LLaVA-UHD v2ï¼Œè¿™æ˜¯ä¸€ä¸ªå…·æœ‰å…ˆè¿›æ„ŸçŸ¥èƒ½åŠ›çš„å¤§å‹å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼Œå®ƒé€šè¿‡å¼•å…¥ç²¾å¿ƒè®¾è®¡å¥½çš„è§†è§‰è¯­è¨€æŠ•å½±ä»ªâ€”â€”åˆ†å±‚çª—å£ï¼ˆHiwinï¼‰è½¬æ¢å™¨æ¥å®ç°ã€‚Hiwinè½¬æ¢å™¨é€šè¿‡èå…¥æˆ‘ä»¬æ„å»ºçš„é«˜åˆ†è¾¨ç‡è¯­ä¹‰é‡‘å­—å¡”ï¼Œå¢å¼ºäº†å¤§å‹å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹æ•æ‰å¤šæ ·å¤šæ¨¡æ€è§†è§‰ç»†ç²’åº¦ä¿¡æ¯çš„èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼ŒHiwinè½¬æ¢å™¨åŒ…å«ä¸¤ä¸ªå…³é”®æ¨¡å—ï¼šï¼ˆiï¼‰è§†è§‰ç»†èŠ‚æ³¨å…¥æ¨¡å—ï¼Œå®ƒé€æ­¥å°†ä½çº§åˆ«çš„è§†è§‰ç»†èŠ‚æ³¨å…¥åˆ°é«˜çº§çš„è¯­è¨€å¯¹é½è¯­ä¹‰ç‰¹å¾ä¸­ï¼Œä»è€Œå½¢æˆé€†è¯­ä¹‰é‡‘å­—å¡”ï¼ˆISPï¼‰ï¼Œï¼ˆiiï¼‰åˆ†å±‚çª—å£æ³¨æ„åŠ›æ¨¡å—ï¼Œå®ƒåˆ©ç”¨è·¨å°ºåº¦çª—å£æ¥æµ“ç¼©ISPä¸­çš„å¤šçº§è¯­ä¹‰ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒLLaVA-UHD v2åœ¨å¹¿æ³›çš„åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºä¼˜äºå…¶ä»–å¤§å‹å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹çš„æ•ˆæœã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä¸åŸºå‡†æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„è®¾è®¡åœ¨14ä¸ªåŸºå‡†æµ‹è¯•ä¸­å¹³å‡æå‡äº†3.7%çš„æ€§èƒ½ï¼Œå¦‚åœ¨DocVQAä¸Šæå‡äº†9.3%ã€‚æ‰€æœ‰æ•°æ®å’Œä»£ç éƒ½å°†å…¬å¼€å‘å¸ƒï¼Œä»¥æ–¹ä¾¿æœªæ¥çš„ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13871v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†åœ¨é¢å‘è§†è§‰ç¼–ç çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸­ï¼Œå¼•å…¥äº†ä¸€ç§åä¸ºLLaVA-UHD v2çš„æ–°å‹è¯­è¨€æ¨¡å‹æ¶æ„æ¥è§£å†³æŸäº›ç°æœ‰æ¨¡å‹çš„æ€§èƒ½ç“¶é¢ˆé—®é¢˜ã€‚å…¶é‡ç‚¹åœ¨ç»†èŠ‚å±‚é¢å¤„ç†æ–¹é¢çš„ä¸è¶³é—®é¢˜ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªä¸“é—¨çš„è§†è§‰è¯­è¨€æŠ•å½±å™¨â€”â€”åˆ†å±‚çª—å£ï¼ˆHiwinï¼‰è½¬æ¢å™¨æ¥å¢å¼ºæ¨¡å‹æ•æ‰å¤šæ¨¡æ€è§†è§‰ç»†èŠ‚çš„èƒ½åŠ›ã€‚Hiwinè½¬æ¢å™¨é€šè¿‡æ„å»ºé«˜åˆ†è¾¨ç‡è¯­ä¹‰é‡‘å­—å¡”ï¼Œå®ç°äº†å¯¹å¤šæ¨¡æ€è§†è§‰é¢—ç²’çš„ç²¾ç»†æ•æ‰ã€‚å…¶æ ¸å¿ƒç»„ä»¶åŒ…æ‹¬è§†è§‰ç»†èŠ‚æ³¨å…¥æ¨¡å—å’Œå±‚æ¬¡çª—å£æ³¨æ„åŠ›æ¨¡å—ï¼Œåˆ†åˆ«åœ¨ä¿¡æ¯æŠ½å–ä¸æ•´åˆæ–¹é¢å‘æŒ¥äº†å…³é”®ä½œç”¨ã€‚å®éªŒç»“æœè¯æ˜LLaVA-UHD v2åœ¨å¤šåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ï¼Œç›¸æ¯”åŸºå‡†æ–¹æ³•å¹³å‡æå‡3.7%ï¼Œåœ¨æ–‡æ¡£è§†è§‰é—®ç­”ï¼ˆDocVQAï¼‰ç­‰ç‰¹å®šä»»åŠ¡ä¸Šæå‡å¹…åº¦é«˜è¾¾9.3%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLaVA-UHD v2æ˜¯ä¸€ç§é’ˆå¯¹å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ–°å‹æ¶æ„ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ¨¡å‹åœ¨ç²¾ç»†è§†è§‰æ„ŸçŸ¥ä»»åŠ¡ä¸Šçš„æ€§èƒ½ç“¶é¢ˆé—®é¢˜ã€‚</li>
<li>LLaVA-UHD v2å¼•å…¥äº†ä¸€ç§ç§°ä¸ºHiwinè½¬æ¢å™¨çš„è§†è§‰è¯­è¨€æŠ•å½±å™¨æ¨¡å—ï¼Œå®ƒèƒ½å¤Ÿæœ‰æ•ˆæ•æ‰å¤šæ¨¡æ€è§†è§‰é¢—ç²’ï¼Œé€šè¿‡æ„å»ºé«˜åˆ†è¾¨ç‡è¯­ä¹‰é‡‘å­—å¡”æé«˜æ¨¡å‹çš„æ„ŸçŸ¥èƒ½åŠ›ã€‚</li>
<li>Hiwinè½¬æ¢å™¨åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒæ¨¡å—ï¼šè§†è§‰ç»†èŠ‚æ³¨å…¥æ¨¡å—å’Œå±‚æ¬¡çª—å£æ³¨æ„åŠ›æ¨¡å—ï¼Œåˆ†åˆ«è´Ÿè´£å‘è¯­ä¹‰ç‰¹å¾æ³¨å…¥ä½çº§åˆ«è§†è§‰ç»†èŠ‚å’Œåˆ©ç”¨è·¨å°ºåº¦çª—å£æµ“ç¼©å¤šå±‚æ¬¡è¯­ä¹‰ä¿¡æ¯ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜LLaVA-UHD v2åœ¨å¤šåŸºå‡†æµ‹è¯•ä¸­æ€§èƒ½æ˜¾è‘—ä¼˜äºå…¶ä»–MLLMsæ¨¡å‹ï¼Œç›¸æ¯”åŸºå‡†æ–¹æ³•å¹³å‡æå‡3.7%ã€‚åœ¨æŸäº›ç‰¹å®šä»»åŠ¡ä¸Šå¦‚æ–‡æ¡£è§†è§‰é—®ç­”ï¼ˆDocVQAï¼‰ï¼Œæå‡å¹…åº¦æ›´é«˜è¾¾9.3%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13871">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b6ded07bfd638dc2e9f51ef472d04866.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-62cc7f5a9b42bb7b7345e0f3f12f9d37.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5a252c024ba2b06abf1f8f4797728f4e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b4b72bb04599f2945cae3e840ad8e987.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3edd06cb2b3a2d06f7607983833a856.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Can-LLMs-be-Good-Graph-Judger-for-Knowledge-Graph-Construction"><a href="#Can-LLMs-be-Good-Graph-Judger-for-Knowledge-Graph-Construction" class="headerlink" title="Can LLMs be Good Graph Judger for Knowledge Graph Construction?"></a>Can LLMs be Good Graph Judger for Knowledge Graph Construction?</h2><p><strong>Authors:Haoyu Huang, Chong Chen, Conghui He, Yang Li, Jiawei Jiang, Wentao Zhang</strong></p>
<p>In real-world scenarios, most of the data obtained from information retrieval (IR) system is unstructured. Converting natural language sentences into structured Knowledge Graphs (KGs) remains a critical challenge. The quality of constructed KGs may also impact the performance of some KG-dependent domains like GraphRAG systems and recommendation systems. Recently, Large Language Models (LLMs) have demonstrated impressive capabilities in addressing a wide range of natural language processing tasks. However, there are still challenges when utilizing LLMs to address the task of generating structured KGs. And we have identified three limitations with respect to existing KG construction methods. (1)There is a large amount of information and excessive noise in real-world documents, which could result in extracting messy information. (2)Native LLMs struggle to effectively extract accuracy knowledge from some domain-specific documents. (3)Hallucinations phenomenon cannot be overlooked when utilizing LLMs directly as an unsupervised method for constructing KGs.   In this paper, we propose GraphJudger, a knowledge graph construction framework to address the aforementioned challenges. We introduce three innovative modules in our method, which are entity-centric iterative text denoising, knowledge aware instruction tuning and graph judgement, respectively. We seek to utilize the capacity of LLMs to function as a graph judger, a capability superior to their role only as a predictor for KG construction problems. Experiments conducted on two general text-graph pair datasets and one domain-specific text-graph pair dataset show superior performances compared to baseline methods. The code of our proposed method is available at <a target="_blank" rel="noopener" href="https://github.com/hhy-huang/GraphJudger">https://github.com/hhy-huang/GraphJudger</a>. </p>
<blockquote>
<p>åœ¨çœŸå®åœºæ™¯ä¸­ï¼Œä»ä¿¡æ¯æ£€ç´¢ï¼ˆIRï¼‰ç³»ç»Ÿè·å¾—çš„æ•°æ®å¤§éƒ¨åˆ†æ˜¯éç»“æ„åŒ–çš„ã€‚å°†è‡ªç„¶è¯­è¨€å¥å­è½¬æ¢ä¸ºç»“æ„åŒ–çŸ¥è¯†å›¾è°±ï¼ˆKGsï¼‰ä»ç„¶æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚æ„å»ºçš„çŸ¥è¯†å›¾è°±çš„è´¨é‡ä¹Ÿå¯èƒ½å½±å“ä¸€äº›ä¾èµ–çŸ¥è¯†å›¾è°±çš„é¢†åŸŸï¼ˆå¦‚GraphRAGç³»ç»Ÿå’Œæ¨èç³»ç»Ÿï¼‰çš„æ€§èƒ½ã€‚æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†å¹¿æ³›çš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œåœ¨åˆ©ç”¨LLMè§£å†³ç”Ÿæˆç»“æ„åŒ–çŸ¥è¯†å›¾è°±çš„ä»»åŠ¡æ—¶ï¼Œä»ç„¶å­˜åœ¨æŒ‘æˆ˜ã€‚æˆ‘ä»¬å·²ç»ç¡®å®šäº†ç°æœ‰çŸ¥è¯†å›¾è°±æ„å»ºæ–¹æ³•çš„ä¸‰ä¸ªå±€é™æ€§ã€‚ï¼ˆ1ï¼‰çœŸå®ä¸–ç•Œæ–‡æ¡£ä¸­å­˜åœ¨å¤§é‡ä¿¡æ¯å’Œè¿‡å¤šå™ªéŸ³ï¼Œå¯èƒ½å¯¼è‡´æå–çš„ä¿¡æ¯æ‚ä¹±æ— ç« ã€‚ï¼ˆ2ï¼‰åŸç”ŸLLMå¾ˆéš¾ä»æŸäº›ç‰¹å®šé¢†åŸŸçš„æ–‡æ¡£ä¸­æœ‰æ•ˆåœ°æå–å‡†ç¡®çŸ¥è¯†ã€‚ï¼ˆ3ï¼‰å½“å°†LLMç›´æ¥ç”¨ä½œæ„å»ºçŸ¥è¯†å›¾è°±çš„æ— ç›‘ç£æ–¹æ³•æ—¶ï¼Œä¸å¯å¿½è§†è™šæ„ç°è±¡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†GraphJudgerï¼Œä¸€ä¸ªçŸ¥è¯†å›¾è°±æ„å»ºæ¡†æ¶ï¼Œä»¥è§£å†³ä¸Šè¿°æŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸‰ä¸ªåˆ›æ–°æ¨¡å—ï¼Œåˆ†åˆ«æ˜¯å®ä½“ä¸ºä¸­å¿ƒè¿­ä»£æ–‡æœ¬å»å™ªã€çŸ¥è¯†æ„ŸçŸ¥æŒ‡ä»¤è°ƒæ•´å’Œå›¾åˆ¤æ–­ã€‚æˆ‘ä»¬è¯•å›¾åˆ©ç”¨LLMä½œä¸ºå›¾åˆ¤æ–­è€…çš„èƒ½åŠ›ï¼Œå‘æŒ¥å…¶åœ¨è§£å†³çŸ¥è¯†å›¾è°±æ„å»ºé—®é¢˜ä¸­çš„ä¼˜åŠ¿ï¼Œè€Œä¸ä»…ä»…æ˜¯ä½œä¸ºé¢„æµ‹å™¨ã€‚åœ¨ä¸¤ç§é€šç”¨æ–‡æœ¬-å›¾å¯¹æ•°æ®é›†å’Œä¸€ç§ç‰¹å®šé¢†åŸŸçš„æ–‡æœ¬-å›¾å¯¹æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒæ˜¾ç¤ºï¼Œä¸åŸºå‡†æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•å…·æœ‰å“è¶Šçš„æ€§èƒ½ã€‚æˆ‘ä»¬æå‡ºçš„æ–¹æ³•çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/hhy-huang/GraphJudger%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/hhy-huang/GraphJudgerè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.17388v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>åœ¨ä¿¡æ¯æ£€ç´¢ç³»ç»Ÿä¸­è·å–çš„å¤§éƒ¨åˆ†æ•°æ®é€šå¸¸æ˜¯ç»“æ„åŒ–çš„ï¼Œè¿™å¯¹äºæ„å»ºçŸ¥è¯†å›¾è°±æ˜¯ä¸€å¤§æŒ‘æˆ˜ã€‚è¯¥æ–‡é’ˆå¯¹æ­¤é—®é¢˜æå‡ºGraphJudgeræ¡†æ¶ï¼Œé‡‡ç”¨ä¸‰ä¸ªåˆ›æ–°æ¨¡å—ï¼ŒåŒ…æ‹¬å®ä½“ä¸ºä¸­å¿ƒçš„è¿­ä»£æ–‡æœ¬å»å™ªã€çŸ¥è¯†æ„ŸçŸ¥æŒ‡ä»¤è°ƒæ•´å’Œå›¾è°±åˆ¤æ–­ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰çŸ¥è¯†å›¾è°±æ„å»ºæ–¹æ³•çš„å±€é™æ€§ã€‚å®éªŒè¯æ˜ï¼ŒGraphJudgeråœ¨ä¸€èˆ¬æ–‡æœ¬-å›¾è°±å¯¹å’Œç‰¹å®šé¢†åŸŸæ–‡æœ¬-å›¾è°±å¯¹æ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚è¯¥æ–¹æ³•çš„ä»£ç å·²åœ¨GitHubä¸Šå…¬å¼€ã€‚æ„å»ºé«˜è´¨é‡çŸ¥è¯†å›¾è°±å¯¹äºå›¾RAç³»ç»Ÿå’Œæ¨èç³»ç»Ÿç­‰çŸ¥è¯†å›¾è°±ä¾èµ–é¢†åŸŸå…·æœ‰é‡è¦å½±å“ã€‚åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºå›¾åˆ¤å®˜çš„æ½œåŠ›æ¥æ„å»ºçŸ¥è¯†å›¾è°±ã€‚æœ¬ç ”ç©¶å¯¹è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„å®é™…åº”ç”¨å…·æœ‰é‡è¦çš„ç†è®ºå’Œå®è·µæ„ä¹‰ã€‚åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ„å»ºçŸ¥è¯†å›¾è°±çš„æŒ‘æˆ˜ä»ç„¶å­˜åœ¨ä¸€äº›å±€é™å’ŒæŒ‘æˆ˜ã€‚æ— æ³•å¿½è§†å‡ºç°è¯¯è¯»ç°è±¡é—®é¢˜ï¼Œæˆ‘ä»¬ä»åœ¨ç§¯ææ¢ç´¢å¯»æ‰¾æ›´æœ‰æ•ˆçš„è§£å†³é€”å¾„å’Œçªç ´æ–¹å¼ã€‚æˆ‘ä»¬å‘ç°å…¬å¼€æ•°æ®æºä¸å…¬å…±ç”¨æˆ·æä¾›çš„ç½‘é¡µæˆ–è€…å…·ä½“æŸ¥è¯¢çš„ä¸åŒã€ç”¨æˆ·å’Œåˆ©ç›Šç›¸å…³è€…åœ¨æœåŠ¡é¢†åŸŸä¸­åµŒå…¥ç¤¾ä¼šå…³ç³»çš„æ–¹æ³•å’Œç”¨æˆ·éœ€æ±‚è¿™äº›éš¾ä»¥å®Œå…¨ä¾èµ–æ¨¡å‹å’ŒæŠ€æœ¯çš„é¢„æµ‹è§£å†³çš„åœºæ™¯é—®é¢˜æ—¶é¢å¯¹æ–°çš„æŒ‘æˆ˜åŠç‰¹å®šæŠ€æœ¯åº”ç”¨çš„ç‰¹å®šæ–¹æ³•åœ¨ä¸åŒåœºæ™¯çš„è½åœ°å·®è·å¯¹äºæ•´ä½“å†³ç­–ç­‰æ–¹é¢æœ‰ä¸€å®šçš„å½±å“ä¸æŒ‡å¯¼æ•ˆæœå¹¶å…¬å¼€æ•°æ®åŠç›¸å…³å·¥å…·ä½œä¸ºæœªæ¥çš„ç ”ç©¶æ–¹å‘ä¹‹ä¸€ã€‚é’ˆå¯¹æœªæ¥ç ”ç©¶æ–¹å‘æå‡ºäº†å…¬å¼€æ•°æ®åŠç›¸å…³å·¥å…·ç­‰æ–¹å‘ä½œä¸ºç ”ç©¶é‡ç‚¹ä¹‹ä¸€ï¼Œé€šè¿‡æ¢ç´¢ç›¸å…³é¢†åŸŸå’Œåº”ç”¨çš„æ·±åº¦ç ”ç©¶å’Œå‘å±•æ½œåŠ›æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚é€šè¿‡å¯¹æ•°æ®çš„æŒ–æ˜å’Œè¿›ä¸€æ­¥åº”ç”¨ä¼˜åŒ–ç­‰æ”¹è¿›å’Œåˆ›æ–°æŠ€æœ¯æå‡çŸ¥è¯†å›¾è°±æ„å»ºçš„è´¨é‡å’Œæ•ˆç‡ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ul>
<li>ä¿¡æ¯æ£€ç´¢ç³»ç»Ÿä¸­è·å–çš„æ•°æ®å¤šä¸ºéç»“æ„åŒ–ï¼Œè½¬åŒ–ä¸ºç»“æ„åŒ–çŸ¥è¯†å›¾è°±æ˜¯ä¸€å¤§æŒ‘æˆ˜ã€‚</li>
<li>GraphJudgeræ¡†æ¶é€šè¿‡ä¸‰ä¸ªåˆ›æ–°æ¨¡å—è§£å†³ç°æœ‰çŸ¥è¯†å›¾è°±æ„å»ºæ–¹æ³•çš„å±€é™æ€§ã€‚</li>
<li>GraphJudgeråœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œå…·æœ‰å…¬å¼€çš„ä»£ç åº“ã€‚</li>
<li>æ„å»ºé«˜è´¨é‡çŸ¥è¯†å›¾è°±å¯¹å›¾RAç³»ç»Ÿå’Œæ¨èç³»ç»Ÿç­‰é¢†åŸŸæœ‰é‡è¦å½±å“ã€‚</li>
<li>LLMsåœ¨çŸ¥è¯†å›¾è°±æ„å»ºä¸­çš„è§’è‰²ä¸ä»…ä»…æ˜¯é¢„æµ‹å™¨ï¼Œè¿˜å¯ä»¥ä½œä¸ºå›¾åˆ¤å®˜æ¥æé«˜æ„å»ºè´¨é‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.17388">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-445521dbbf0e83cb3cd7107b3c5a74c3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7ca7f38159f59c8b2e12ab45ed0ebb36.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1500a7244d3a0e3e39d015ecef63ab9f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b46ec1157229f2bed0ab8a97aa88f780.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89ca56b8dac752f329cdf9eb597fc3a5.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-21/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-21/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-21/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-e6116c0b8dbcca8d73715e625323a88b.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-21  SWEET-RL Training Multi-Turn LLM Agents on Collaborative Reasoning   Tasks
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-21
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-21/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-0999880d92a1a774a606f2f405e64e85.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-21  SWEET-RL Training Multi-Turn LLM Agents on Collaborative Reasoning   Tasks
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-21
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">23394.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
