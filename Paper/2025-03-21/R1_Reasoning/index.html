<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-21  SWEET-RL Training Multi-Turn LLM Agents on Collaborative Reasoning   Tasks">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-0999880d92a1a774a606f2f405e64e85.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-21
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-21
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    78 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-21-æ›´æ–°"><a href="#2025-03-21-æ›´æ–°" class="headerlink" title="2025-03-21 æ›´æ–°"></a>2025-03-21 æ›´æ–°</h1><h2 id="SWEET-RL-Training-Multi-Turn-LLM-Agents-on-Collaborative-Reasoning-Tasks"><a href="#SWEET-RL-Training-Multi-Turn-LLM-Agents-on-Collaborative-Reasoning-Tasks" class="headerlink" title="SWEET-RL: Training Multi-Turn LLM Agents on Collaborative Reasoning   Tasks"></a>SWEET-RL: Training Multi-Turn LLM Agents on Collaborative Reasoning   Tasks</h2><p><strong>Authors:Yifei Zhou, Song Jiang, Yuandong Tian, Jason Weston, Sergey Levine, Sainbayar Sukhbaatar, Xian Li</strong></p>
<p>Large language model (LLM) agents need to perform multi-turn interactions in real-world tasks. However, existing multi-turn RL algorithms for optimizing LLM agents fail to perform effective credit assignment over multiple turns while leveraging the generalization capabilities of LLMs and it remains unclear how to develop such algorithms. To study this, we first introduce a new benchmark, ColBench, where an LLM agent interacts with a human collaborator over multiple turns to solve realistic tasks in backend programming and frontend design. Building on this benchmark, we propose a novel RL algorithm, SWEET-RL (RL with Step-WisE Evaluation from Training-time information), that uses a carefully designed optimization objective to train a critic model with access to additional training-time information. The critic provides step-level rewards for improving the policy model. Our experiments demonstrate that SWEET-RL achieves a 6% absolute improvement in success and win rates on ColBench compared to other state-of-the-art multi-turn RL algorithms, enabling Llama-3.1-8B to match or exceed the performance of GPT4-o in realistic collaborative content creation. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†éœ€è¦åœ¨ç°å®ä¸–ç•Œçš„ä»»åŠ¡ä¸­è¿›è¡Œå¤šè½®äº¤äº’ã€‚ç„¶è€Œï¼Œç°æœ‰çš„é’ˆå¯¹LLMä»£ç†ä¼˜åŒ–çš„å¤šè½®å¼ºåŒ–å­¦ä¹ ç®—æ³•æœªèƒ½å……åˆ†åˆ©ç”¨LLMçš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶åœ¨å¤šè½®äº¤äº’ä¸­è¿›è¡Œæœ‰æ•ˆçš„ä¿¡ç”¨åˆ†é…ï¼Œå¦‚ä½•å¼€å‘æ­¤ç±»ç®—æ³•ä»ä¸æ˜ç¡®ã€‚ä¸ºäº†ç ”ç©¶è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬é¦–å…ˆå¼•å…¥äº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ColBenchï¼Œåœ¨è¿™ä¸ªæµ‹è¯•ä¸­ï¼ŒLLMä»£ç†ä¸äººç±»çš„åˆä½œè€…è¿›è¡Œå¤šè½®äº¤äº’ï¼Œä»¥è§£å†³åç«¯ç¼–ç¨‹å’Œå‰ç«¯è®¾è®¡ä¸­çš„å®é™…ä»»åŠ¡ã€‚åŸºäºè¿™ä¸ªåŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•SWEET-RLï¼ˆå…·æœ‰è®­ç»ƒæ—¶ä¿¡æ¯æ­¥è¿›æ˜æ™ºè¯„ä»·çš„å¼ºåŒ–å­¦ä¹ ï¼‰ï¼Œè¯¥ç®—æ³•ä½¿ç”¨ç²¾å¿ƒè®¾è®¡çš„ä¼˜åŒ–ç›®æ ‡æ¥è®­ç»ƒä¸€ä¸ªè¯„è®ºå®¶æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯ä»¥è®¿é—®é¢å¤–çš„è®­ç»ƒæ—¶ä¿¡æ¯ã€‚è¯„è®ºå®¶ä¸ºæ”¹å–„ç­–ç•¥æ¨¡å‹æä¾›æ­¥éª¤çº§åˆ«çš„å¥–åŠ±ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œåœ¨ColBenchä¸Šï¼Œä¸å…¶ä»–æœ€æ–°çš„å¤šè½®å¼ºåŒ–å­¦ä¹ ç®—æ³•ç›¸æ¯”ï¼ŒSWEET-RLåœ¨æˆåŠŸç‡å’Œèƒœç‡æ–¹é¢å–å¾—äº†6%çš„ç»å¯¹æå‡ï¼Œä½¿å¾—Llama-3.1-8Bèƒ½å¤Ÿåœ¨ç°å®åä½œå†…å®¹åˆ›å»ºä¸­åŒ¹é…æˆ–è¶…è¶ŠGPT4-oçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15478v1">PDF</a> 29 pages, 16 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†åœ¨çœŸå®ä¸–ç•Œä»»åŠ¡ä¸­éœ€è¦è¿›è¡Œå¤šè½®äº¤äº’ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤šè½®å¼ºåŒ–å­¦ä¹ ç®—æ³•åœ¨ä¼˜åŒ–LLMä»£ç†æ—¶ï¼Œæœªèƒ½æœ‰æ•ˆåœ°è¿›è¡Œå¤šè½®ä¿¡ç”¨åˆ†é…ï¼ŒåŒæ—¶åˆ©ç”¨LLMçš„æ³›åŒ–èƒ½åŠ›å°šä¸æ¸…æ¥šå¦‚ä½•å¼€å‘æ­¤ç±»ç®—æ³•ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ–°çš„åŸºå‡†æµ‹è¯•ColBenchï¼ŒLLMä»£ç†å¯ä¸äººç±»åˆä½œè€…è¿›è¡Œå¤šè½®äº¤äº’ä»¥è§£å†³åç«¯ç¼–ç¨‹å’Œå‰ç«¯è®¾è®¡ä»»åŠ¡ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†æ–°å‹å¼ºåŒ–å­¦ä¹ ç®—æ³•SWEET-RLï¼Œè¯¥ç®—æ³•åˆ©ç”¨ç²¾å¿ƒè®¾è®¡ä¼˜åŒ–ç›®æ ‡è®­ç»ƒæ‰¹åˆ¤æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨é¢å¤–çš„è®­ç»ƒæ—¶ä¿¡æ¯ã€‚æ‰¹åˆ¤æ¨¡å‹ä¸ºç­–ç•¥æ¨¡å‹æä¾›æ­¥éª¤çº§å¥–åŠ±ã€‚å®éªŒè¡¨æ˜ï¼Œä¸å…¶ä»–æœ€å…ˆè¿›çš„å¤šè½®å¼ºåŒ–å­¦ä¹ ç®—æ³•ç›¸æ¯”ï¼ŒSWEET-RLåœ¨ColBenchä¸Šçš„æˆåŠŸç‡å’Œèƒœç‡æé«˜äº†6%ï¼Œä½¿Llama-3.1-8Båœ¨çœŸå®åä½œå†…å®¹åˆ›å»ºæ–¹é¢çš„æ€§èƒ½ä¸GPT4-oç›¸åŒ¹é…æˆ–è¶…è¶Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†åœ¨å¤šè½®äº¤äº’ä¸­é¢ä¸´æŒ‘æˆ˜ï¼Œéœ€è¦æ›´æœ‰æ•ˆçš„ä¿¡ç”¨åˆ†é…ç®—æ³•ã€‚</li>
<li>å¼•å…¥æ–°çš„åŸºå‡†æµ‹è¯•ColBenchï¼Œç”¨äºè¯„ä¼°LLMä»£ç†åœ¨å¤šè½®äº¤äº’ä¸­çš„æ€§èƒ½ã€‚</li>
<li>æå‡ºäº†æ–°å‹å¼ºåŒ–å­¦ä¹ ç®—æ³•SWEET-RLï¼Œç»“åˆè®­ç»ƒæ—¶çš„é¢å¤–ä¿¡æ¯æ¥ä¼˜åŒ–æ‰¹åˆ¤æ¨¡å‹ã€‚</li>
<li>SWEET-RLæä¾›æ­¥éª¤çº§çš„å¥–åŠ±åé¦ˆï¼Œæ”¹è¿›ç­–ç•¥æ¨¡å‹ã€‚</li>
<li>SWEET-RLåœ¨ColBenchä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–å¤šè½®å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ŒæˆåŠŸç‡å’Œèƒœç‡æœ‰æ˜¾è‘—æé«˜ã€‚</li>
<li>LLMä»£ç†é€šè¿‡SWEET-RLç®—æ³•åœ¨åä½œå†…å®¹åˆ›å»ºæ–¹é¢è¾¾åˆ°æˆ–è¶…è¶Šäº†GPT4-oçš„æ€§èƒ½æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15478">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-103b1a90029ee8e84117c61fbdc67652.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e6116c0b8dbcca8d73715e625323a88b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-af458b77ecb3fb3477fb230aba167a6d.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="SENAI-Towards-Software-Engineering-Native-Generative-Artificial-Intelligence"><a href="#SENAI-Towards-Software-Engineering-Native-Generative-Artificial-Intelligence" class="headerlink" title="SENAI: Towards Software Engineering Native Generative Artificial   Intelligence"></a>SENAI: Towards Software Engineering Native Generative Artificial   Intelligence</h2><p><strong>Authors:Mootez Saad, JosÃ© Antonio HernÃ¡ndez LÃ³pez, Boqi Chen, Neil Ernst, DÃ¡niel VarrÃ³, Tushar Sharma</strong></p>
<p>Large Language Models have significantly advanced the field of code generation, demonstrating the ability to produce functionally correct code snippets. However, advancements in generative AI for code overlook foundational Software Engineering (SE) principles such as modularity, and single responsibility, and concepts such as cohesion and coupling which are critical for creating maintainable, scalable, and robust software systems. These concepts are missing in pipelines that start with pre-training and end with the evaluation using benchmarks.   This vision paper argues for the integration of SE knowledge into LLMs to enhance their capability to understand, analyze, and generate code and other SE artifacts following established SE knowledge. The aim is to propose a new direction where LLMs can move beyond mere functional accuracy to perform generative tasks that require adherence to SE principles and best practices. In addition, given the interactive nature of these conversational models, we propose using Bloomâ€™s Taxonomy as a framework to assess the extent to which they internalize SE knowledge. The proposed evaluation framework offers a sound and more comprehensive evaluation technique compared to existing approaches such as linear probing. Software engineering native generative models will not only overcome the shortcomings present in current models but also pave the way for the next generation of generative models capable of handling real-world software engineering. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä»£ç ç”Ÿæˆé¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œå±•ç°å‡ºç”ŸæˆåŠŸèƒ½æ­£ç¡®ä»£ç ç‰‡æ®µçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œä»£ç ç”Ÿæˆé¢†åŸŸçš„ç”Ÿæˆäººå·¥æ™ºèƒ½å‘å±•è¿›æ­¥å¿½ç•¥äº†åŸºç¡€è½¯ä»¶å·¥ç¨‹ï¼ˆSEï¼‰åŸåˆ™ï¼Œå¦‚æ¨¡å—åŒ–ã€å•ä¸€èŒè´£åŸåˆ™ï¼Œä»¥åŠå¯¹äºåˆ›å»ºå¯æŒç»­ã€å¯æ‰©å±•å’Œç¨³å¥çš„è½¯ä»¶ç³»ç»Ÿè‡³å…³é‡è¦çš„å†…èšå’Œè€¦åˆç­‰æ¦‚å¿µã€‚è¿™äº›æ¦‚å¿µåœ¨å§‹äºé¢„è®­ç»ƒã€ç»ˆäºä½¿ç”¨åŸºå‡†æµ‹è¯•è¯„ä¼°çš„ç®¡é“ä¸­ç¼ºå¤±ã€‚æœ¬æ–‡ä¸»å¼ å°†è½¯ä»¶å·¥ç¨‹çŸ¥è¯†æ•´åˆåˆ°å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ï¼Œä»¥æé«˜å…¶ç†è§£ã€åˆ†æå’Œç”Ÿæˆä»£ç ä»¥åŠå…¶ä»–è½¯ä»¶å·¥ç¨‹å·¥ä»¶çš„èƒ½åŠ›ï¼Œéµå¾ªå·²å»ºç«‹çš„è½¯ä»¶å·¥ç¨‹çŸ¥è¯†ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯æå‡ºä¸€ä¸ªæ–°çš„æ–¹å‘ï¼Œä½¿å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿè¶…è¶Šå•çº¯çš„åŠŸèƒ½å‡†ç¡®æ€§ï¼Œæ‰§è¡Œéœ€è¦éµå¾ªè½¯ä»¶å·¥ç¨‹åŸåˆ™å’Œè§„èŒƒçš„æœ€ä½³å®è·µæ¥ç”Ÿæˆçš„ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œè€ƒè™‘åˆ°è¿™äº›å¯¹è¯æ¨¡å‹çš„äº¤äº’æ€§è´¨ï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨å¸ƒé²å§†çš„åˆ†ç±»æ³•ä½œä¸ºä¸€ä¸ªæ¡†æ¶æ¥è¯„ä¼°ä»–ä»¬å¯¹è½¯ä»¶å·¥ç¨‹çŸ¥è¯†çš„å†…åŒ–ç¨‹åº¦ã€‚ä¸ç°æœ‰çš„çº¿æ€§æ¢æŸ¥ç­‰è¯„ä¼°æ–¹æ³•ç›¸æ¯”ï¼Œæå‡ºçš„è¯„ä¼°æ¡†æ¶æä¾›äº†ä¸€ä¸ªæ›´ä¸ºç¨³å¥å’Œç»¼åˆçš„è¯„ä¼°æŠ€æœ¯ã€‚è½¯ä»¶å·¥ç¨‹æœ¬åœ°ç”Ÿæˆæ¨¡å‹ä¸ä»…å¯ä»¥å…‹æœå½“å‰æ¨¡å‹çš„å±€é™æ€§ï¼Œè¿˜å¯ä»¥ä¸ºä¸‹ä¸€ä»£èƒ½å¤Ÿå¤„ç†ç°å®ä¸–ç•Œè½¯ä»¶å·¥ç¨‹çš„ç”Ÿæˆæ¨¡å‹é“ºå¹³é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15282v1">PDF</a> 5 pages, 1 figure</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä»£ç ç”Ÿæˆé¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œèƒ½å¤Ÿç”ŸæˆåŠŸèƒ½æ­£ç¡®çš„ä»£ç ç‰‡æ®µã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹å¿½ç•¥äº†è½¯ä»¶å·¥ç¨‹çš„åŸºæœ¬åŸåˆ™ï¼Œå¦‚æ¨¡å—åŒ–ã€å•ä¸€èŒè´£ã€å‡èšåŠ›å’Œè€¦åˆç­‰å…³é”®æ¦‚å¿µï¼Œè¿™äº›æ¦‚å¿µå¯¹äºåˆ›å»ºå¯æŒç»­ã€å¯æ‰©å±•å’Œç¨³å¥çš„è½¯ä»¶ç³»ç»Ÿè‡³å…³é‡è¦ã€‚æœ¬æ–‡ä¸»å¼ å°†è½¯ä»¶å·¥ç¨‹çŸ¥è¯†èå…¥å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä»¥æé«˜å…¶ç†è§£ã€åˆ†æå’Œç”Ÿæˆä»£ç çš„èƒ½åŠ›ã€‚æ–‡ç« çš„ç›®æ ‡æ˜¯æå‡ºä¸€ä¸ªæ–°æ–¹å‘ï¼Œä½¿å¤§å‹è¯­è¨€æ¨¡å‹ä¸ä»…åœ¨åŠŸèƒ½å‡†ç¡®æ€§ä¸Šæœ‰æ‰€æå‡ï¼Œè¿˜èƒ½åœ¨éœ€è¦éµå¾ªè½¯ä»¶å·¥ç¨‹åŸåˆ™å’Œæœ€ä½³å®è·µçš„ç”Ÿæˆä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚æ­¤å¤–ï¼Œé‰´äºè¿™äº›å¯¹è¯æ¨¡å‹çš„äº¤äº’æ€§ï¼Œæ–‡ç« è¿˜æå‡ºäº†ä½¿ç”¨å¸ƒé²å§†åˆ†ç±»æ³•ä½œä¸ºè¯„ä¼°æ¡†æ¶ï¼Œä»¥è¯„ä¼°å®ƒä»¬å¯¹è½¯ä»¶å·¥ç¨‹çŸ¥è¯†çš„å†…åŒ–ç¨‹åº¦ã€‚è¯¥è¯„ä¼°æ¡†æ¶ç›¸æ¯”ç°æœ‰çš„æ–¹æ³•å¦‚çº¿æ€§æ¢æµ‹æ³•æ›´ä¸ºå…¨é¢å’Œç¨³å¥ã€‚è½¯ä»¶å·¥ç¨‹åŸç”Ÿç”Ÿæˆæ¨¡å‹ä¸ä»…èƒ½å…‹æœå½“å‰æ¨¡å‹çš„ä¸è¶³ï¼Œè¿˜èƒ½ä¸ºä¸‹ä¸€ä»£ç”Ÿæˆæ¨¡å‹é“ºå¹³é“è·¯ï¼Œä½¿å…¶èƒ½å¤Ÿå¤„ç†ç°å®ä¸–ç•Œä¸­çš„è½¯ä»¶å·¥ç¨‹é—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä»£ç ç”Ÿæˆæ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œèƒ½ç”ŸæˆåŠŸèƒ½æ­£ç¡®çš„ä»£ç ç‰‡æ®µã€‚</li>
<li>è¿™äº›æ¨¡å‹å¿½ç•¥äº†è½¯ä»¶å·¥ç¨‹çš„åŸºæœ¬åŸåˆ™å’Œå…³é”®æ¦‚å¿µï¼Œå¦‚æ¨¡å—åŒ–ã€å•ä¸€èŒè´£ã€å‡èšåŠ›å’Œè€¦åˆã€‚</li>
<li>æ–‡ç« çš„æ„¿æ™¯æ˜¯å°†è½¯ä»¶å·¥ç¨‹çŸ¥è¯†èå…¥å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæé«˜å…¶ç†è§£å’Œç”Ÿæˆä»£ç çš„èƒ½åŠ›ã€‚</li>
<li>æ–‡ç« æå€¡åœ¨æ¨¡å‹è¯„ä¼°ä¸­ä½¿ç”¨å¸ƒé²å§†åˆ†ç±»æ³•ï¼Œä»¥è¯„ä¼°æ¨¡å‹å¯¹è½¯ä»¶å·¥ç¨‹çŸ¥è¯†çš„å†…åŒ–ç¨‹åº¦ã€‚</li>
<li>ç°æœ‰è¯„ä¼°æ–¹æ³•å¦‚çº¿æ€§æ¢æµ‹æ³•å­˜åœ¨å±€é™æ€§ï¼Œæ–°çš„è¯„ä¼°æ¡†æ¶æ›´ä¸ºå…¨é¢å’Œç¨³å¥ã€‚</li>
<li>è½¯ä»¶å·¥ç¨‹åŸç”Ÿç”Ÿæˆæ¨¡å‹èƒ½å…‹æœå½“å‰æ¨¡å‹çš„ä¸è¶³ï¼Œå¹¶å¼€å¯ä¸‹ä¸€ä»£æ¨¡å‹å¤„ç†ç°å®è½¯ä»¶å·¥ç¨‹é—®é¢˜çš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15282">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-dabe3517907f61c947ab6e1cc339591c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-27fdbd8865ffe9cb6965b408dab40bc0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Reasoning-Effort-and-Problem-Complexity-A-Scaling-Analysis-in-LLMs"><a href="#Reasoning-Effort-and-Problem-Complexity-A-Scaling-Analysis-in-LLMs" class="headerlink" title="Reasoning Effort and Problem Complexity: A Scaling Analysis in LLMs"></a>Reasoning Effort and Problem Complexity: A Scaling Analysis in LLMs</h2><p><strong>Authors:Benjamin Estermann, Roger Wattenhofer</strong></p>
<p>Large Language Models (LLMs) have demonstrated remarkable text generation capabilities, and recent advances in training paradigms have led to breakthroughs in their reasoning performance. In this work, we investigate how the reasoning effort of such models scales with problem complexity. We use the infinitely scalable Tents puzzle, which has a known linear-time solution, to analyze this scaling behavior. Our results show that reasoning effort scales with problem size, but only up to a critical problem complexity. Beyond this threshold, the reasoning effort does not continue to increase, and may even decrease. This observation highlights a critical limitation in the logical coherence of current LLMs as problem complexity increases, and underscores the need for strategies to improve reasoning scalability. Furthermore, our results reveal significant performance differences between current state-of-the-art reasoning models when faced with increasingly complex logical puzzles. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»å±•ç°å‡ºæ˜¾è‘—çš„æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›ï¼Œè€Œè®­ç»ƒæ¨¡å¼çš„æœ€æ–°è¿›å±•ä¹Ÿæå‡äº†å…¶æ¨ç†æ€§èƒ½ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†è¿™ç§æ¨¡å‹çš„æ¨ç†åŠªåŠ›æ˜¯å¦‚ä½•éšç€é—®é¢˜å¤æ‚æ€§çš„å¢åŠ è€Œå˜åŒ–çš„ã€‚æˆ‘ä»¬ä½¿ç”¨å¯æ— é™æ‰©å±•çš„å¸ç¯·è°œé¢˜è¿›è¡Œåˆ†æï¼Œè¯¥è°œé¢˜æœ‰ä¸€ä¸ªå·²çŸ¥çš„çº¿æ€§æ—¶é—´è§£å†³æ–¹æ¡ˆï¼Œä»¥ç ”ç©¶è¿™ç§æ‰©å±•è¡Œä¸ºã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œæ¨ç†åŠªåŠ›éšç€é—®é¢˜è§„æ¨¡è€Œå¢åŠ ï¼Œä½†ä»…é™äºä¸€ä¸ªå…³é”®çš„é—®é¢˜å¤æ‚æ€§ã€‚è¶…è¿‡è¿™ä¸ªé˜ˆå€¼ï¼Œæ¨ç†åŠªåŠ›å¹¶ä¸ä¼šç»§ç»­å¢åŠ ï¼Œç”šè‡³å¯èƒ½ä¼šå‡å°‘ã€‚è¿™ä¸€è§‚å¯Ÿç»“æœçªå‡ºäº†å½“å‰LLMåœ¨é—®é¢˜å¤æ‚æ€§å¢åŠ æ—¶åœ¨é€»è¾‘è¿è´¯æ€§æ–¹é¢çš„ä¸€ä¸ªå…³é”®å±€é™æ€§ï¼Œå¹¶å¼ºè°ƒäº†æé«˜æ¨ç†å¯æ‰©å±•æ€§çš„ç­–ç•¥éœ€æ±‚ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¿˜æ­ç¤ºäº†é¢å¯¹æ—¥ç›Šå¤æ‚çš„é€»è¾‘è°œé¢˜æ—¶ï¼Œå½“å‰æœ€å…ˆè¿›çš„æ¨ç†æ¨¡å‹ä¹‹é—´å­˜åœ¨çš„æ˜¾è‘—æ€§èƒ½å·®å¼‚ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15113v1">PDF</a> Published at ICLR 2025 Workshop on Reasoning and Planning for LLMs</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ–‡æœ¬ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œè€Œè®­ç»ƒæ¨¡å¼çš„æœ€æ–°è¿›å±•ä½¿å…¶åœ¨æ¨ç†æ€§èƒ½æ–¹é¢ä¹Ÿå–å¾—äº†çªç ´ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†è¿™ç§æ¨¡å‹çš„æ¨ç†åŠªåŠ›å¦‚ä½•éšé—®é¢˜å¤æ‚æ€§è€Œæ‰©å±•ã€‚æˆ‘ä»¬ä½¿ç”¨å…·æœ‰å·²çŸ¥çº¿æ€§æ—¶é—´è§£çš„æ— é™å¯æ‰©å±•å¸ç¯·è°œé¢˜è¿›è¡Œåˆ†æã€‚ç»“æœè¡¨æ˜ï¼Œæ¨ç†åŠªåŠ›éšé—®é¢˜è§„æ¨¡è€Œå¢åŠ ï¼Œä½†ä»…é™äºä¸€ä¸ªå…³é”®çš„é—®é¢˜å¤æ‚æ€§é˜ˆå€¼ã€‚è¶…è¿‡æ­¤é˜ˆå€¼åï¼Œæ¨ç†åŠªåŠ›ä¸ä¼šç»§ç»­å¢åŠ ï¼Œç”šè‡³å¯èƒ½å‡å°‘ã€‚è¿™çªæ˜¾äº†å½“å‰LLMsåœ¨é—®é¢˜å¤æ‚æ€§å¢åŠ æ—¶åœ¨é€»è¾‘è¿è´¯æ€§æ–¹é¢çš„å…³é”®å±€é™æ€§ï¼Œå¹¶å¼ºè°ƒäº†æé«˜æ¨ç†å¯æ‰©å±•æ€§çš„ç­–ç•¥çš„å¿…è¦æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç ”ç©¶ç»“æœæ­ç¤ºäº†é¢å¯¹æ—¥ç›Šå¤æ‚çš„é€»è¾‘è°œé¢˜æ—¶ï¼Œå½“å‰æœ€å…ˆè¿›çš„æ¨ç†æ¨¡å‹ä¹‹é—´çš„æ€§èƒ½å·®å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ–‡æœ¬ç”Ÿæˆå’Œæ¨ç†æ–¹é¢éƒ½è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ã€‚</li>
<li>LLMsçš„æ¨ç†åŠªåŠ›éšé—®é¢˜å¤æ‚æ€§è€Œå¢åŠ ï¼Œä½†ä»…é™äºä¸€ä¸ªé˜ˆå€¼ã€‚</li>
<li>è¶…è¿‡é˜ˆå€¼åï¼ŒLLMsçš„æ¨ç†åŠªåŠ›ä¸å†å¢åŠ ï¼Œç”šè‡³å¯èƒ½å‡å°‘ã€‚</li>
<li>è¿™è¡¨æ˜LLMsåœ¨é¢ä¸´æ›´å¤æ‚é—®é¢˜æ—¶å­˜åœ¨é€»è¾‘è¿è´¯æ€§çš„å±€é™æ€§ã€‚</li>
<li>éœ€è¦ç­–ç•¥æ¥æé«˜LLMsçš„æ¨ç†å¯æ‰©å±•æ€§ã€‚</li>
<li>å½“å‰æœ€å…ˆè¿›çš„æ¨ç†æ¨¡å‹åœ¨é¢å¯¹å¤æ‚çš„é€»è¾‘è°œé¢˜æ—¶è¡¨ç°å‡ºæ€§èƒ½å·®å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15113">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-45a25c8d2e707021934cc9fb44b1a67a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b95a5bc3f3e62d6fc52fa1c5cbd93c7e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-393ae4a6f142166434446f95b40047af.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="VIPER-Visual-Perception-and-Explainable-Reasoning-for-Sequential-Decision-Making"><a href="#VIPER-Visual-Perception-and-Explainable-Reasoning-for-Sequential-Decision-Making" class="headerlink" title="VIPER: Visual Perception and Explainable Reasoning for Sequential   Decision-Making"></a>VIPER: Visual Perception and Explainable Reasoning for Sequential   Decision-Making</h2><p><strong>Authors:Mohamed Salim Aissi, Clemence Grislain, Mohamed Chetouani, Olivier Sigaud, Laure Soulier, Nicolas Thome</strong></p>
<p>While Large Language Models (LLMs) excel at reasoning on text and Vision-Language Models (VLMs) are highly effective for visual perception, applying those models for visual instruction-based planning remains a widely open problem. In this paper, we introduce VIPER, a novel framework for multimodal instruction-based planning that integrates VLM-based perception with LLM-based reasoning. Our approach uses a modular pipeline where a frozen VLM generates textual descriptions of image observations, which are then processed by an LLM policy to predict actions based on the task goal. We fine-tune the reasoning module using behavioral cloning and reinforcement learning, improving our agentâ€™s decision-making capabilities. Experiments on the ALFWorld benchmark show that VIPER significantly outperforms state-of-the-art visual instruction-based planners while narrowing the gap with purely text-based oracles. By leveraging text as an intermediate representation, VIPER also enhances explainability, paving the way for a fine-grained analysis of perception and reasoning components. </p>
<blockquote>
<p>è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ–‡æœ¬æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œè€Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨è§†è§‰æ„ŸçŸ¥æ–¹é¢éå¸¸æœ‰æ•ˆï¼Œä½†å°†è¿™äº›æ¨¡å‹åº”ç”¨äºåŸºäºè§†è§‰æŒ‡ä»¤çš„è§„åˆ’ä»ç„¶æ˜¯ä¸€ä¸ªå¹¿æ³›å­˜åœ¨çš„é—®é¢˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†VIPERï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹çš„å¤šæ¨¡æ€åŸºäºæŒ‡ä»¤çš„è§„åˆ’æ¡†æ¶ï¼Œå®ƒå°†åŸºäºVLMçš„æ„ŸçŸ¥ä¸åŸºäºLLMçš„æ¨ç†ç›¸ç»“åˆã€‚æˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨æ¨¡å—åŒ–ç®¡é“ï¼Œå…¶ä¸­å†»ç»“çš„VLMç”Ÿæˆå›¾åƒè§‚å¯Ÿçš„æ–‡å­—æè¿°ï¼Œç„¶åç”±LLMç­–ç•¥å¤„ç†ï¼ŒåŸºäºä»»åŠ¡ç›®æ ‡é¢„æµ‹åŠ¨ä½œã€‚æˆ‘ä»¬ä½¿ç”¨è¡Œä¸ºå…‹éš†å’Œå¼ºåŒ–å­¦ä¹ å¯¹æ¨ç†æ¨¡å—è¿›è¡Œå¾®è°ƒï¼Œæé«˜æˆ‘ä»¬ä»£ç†çš„å†³ç­–èƒ½åŠ›ã€‚åœ¨ALFWorldåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒVIPERåœ¨åŸºäºè§†è§‰æŒ‡ä»¤çš„è§„åˆ’æ–¹é¢æ˜¾è‘—ä¼˜äºæœ€æ–°æŠ€æœ¯ï¼Œå¹¶ç¼©å°äº†ä¸çº¯æ–‡æœ¬åŸºå‡†æµ‹è¯•æˆ–aclesçš„å·®è·ã€‚é€šè¿‡åˆ©ç”¨æ–‡æœ¬ä½œä¸ºä¸­é—´è¡¨ç¤ºï¼ŒVIPERè¿˜æé«˜äº†å¯è§£é‡Šæ€§ï¼Œä¸ºæ„ŸçŸ¥å’Œæ¨ç†ç»„ä»¶çš„ç²¾ç»†ç²’åº¦åˆ†æé“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15108v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŸºäºè§†è§‰æŒ‡ä»¤çš„è§„åˆ’ä»ç„¶æ˜¯ä¸€ä¸ªå¹¿æ³›å­˜åœ¨çš„é—®é¢˜ï¼Œå°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ–‡æœ¬æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œè€Œè§†è§‰è¯­è¨€æ¨¡å‹åœ¨è§†è§‰æ„ŸçŸ¥æ–¹é¢éå¸¸æœ‰æ•ˆã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„å¤šæ¨¡æ€æŒ‡ä»¤è§„åˆ’æ¡†æ¶VIPERï¼Œå®ƒå°†åŸºäºVLMçš„æ„ŸçŸ¥ä¸åŸºäºLLMçš„æ¨ç†ç›¸ç»“åˆã€‚VIPERä½¿ç”¨æ¨¡å—åŒ–ç®¡é“ï¼Œå…¶ä¸­å†»ç»“çš„VLMç”Ÿæˆå›¾åƒè§‚å¯Ÿçš„æ–‡å­—æè¿°ï¼Œç„¶åç”±LLMç­–ç•¥å¤„ç†å¹¶æ ¹æ®ä»»åŠ¡ç›®æ ‡é¢„æµ‹åŠ¨ä½œã€‚é€šè¿‡è¡Œä¸ºå…‹éš†å’Œå¼ºåŒ–å­¦ä¹ å¯¹æ¨ç†æ¨¡å—è¿›è¡Œå¾®è°ƒï¼Œæé«˜äº†ä»£ç†çš„å†³ç­–èƒ½åŠ›ã€‚åœ¨ALFWorldåŸºå‡†æµ‹è¯•ä¸Šï¼ŒVIPERæ˜¾è‘—ä¼˜äºæœ€æ–°çš„è§†è§‰æŒ‡ä»¤è§„åˆ’å™¨ï¼Œå¹¶ç¼©å°äº†ä¸çº¯æ–‡æœ¬åŸºå‡†æµ‹è¯•çš„å·®è·ã€‚åˆ©ç”¨æ–‡æœ¬ä½œä¸ºä¸­é—´è¡¨ç¤ºå½¢å¼ï¼ŒVIPERè¿˜æé«˜äº†å¯è§£é‡Šæ€§ï¼Œä¸ºæ„ŸçŸ¥å’Œæ¨ç†ç»„ä»¶çš„ç²¾ç»†åˆ†æé“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VIPERæ˜¯ä¸€ä¸ªæ–°çš„å¤šæ¨¡æ€æŒ‡ä»¤è§„åˆ’æ¡†æ¶ï¼Œç»“åˆäº†VLMå’ŒLLMçš„ä¼˜åŠ¿ã€‚</li>
<li>VLMç”Ÿæˆå›¾åƒè§‚å¯Ÿçš„æ–‡å­—æè¿°ï¼ŒLLMç­–ç•¥å¤„ç†å¹¶æ ¹æ®ä»»åŠ¡ç›®æ ‡é¢„æµ‹åŠ¨ä½œã€‚</li>
<li>VIPERé€šè¿‡è¡Œä¸ºå…‹éš†å’Œå¼ºåŒ–å­¦ä¹ å¾®è°ƒæ¨ç†æ¨¡å—ï¼Œæé«˜å†³ç­–èƒ½åŠ›ã€‚</li>
<li>VIPERåœ¨ALFWorldåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—ä¼˜äºå…¶ä»–è§†è§‰æŒ‡ä»¤è§„åˆ’å™¨ã€‚</li>
<li>VIPERç¼©å°äº†ä¸çº¯æ–‡æœ¬åŸºå‡†æµ‹è¯•çš„å·®è·ã€‚</li>
<li>åˆ©ç”¨æ–‡æœ¬ä½œä¸ºä¸­é—´è¡¨ç¤ºå½¢å¼ï¼ŒVIPERæé«˜äº†æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15108">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4ef9bca04f76d5e49ffad476f8b84bdf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4eda153711c810f679e72bfd120cb840.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-36a8171cb98df683b0fe8d5630be4672.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Towards-Understanding-the-Safety-Boundaries-of-DeepSeek-Models-Evaluation-and-Findings"><a href="#Towards-Understanding-the-Safety-Boundaries-of-DeepSeek-Models-Evaluation-and-Findings" class="headerlink" title="Towards Understanding the Safety Boundaries of DeepSeek Models:   Evaluation and Findings"></a>Towards Understanding the Safety Boundaries of DeepSeek Models:   Evaluation and Findings</h2><p><strong>Authors:Zonghao Ying, Guangyi Zheng, Yongxin Huang, Deyue Zhang, Wenxin Zhang, Quanchen Zou, Aishan Liu, Xianglong Liu, Dacheng Tao</strong></p>
<p>This study presents the first comprehensive safety evaluation of the DeepSeek models, focusing on evaluating the safety risks associated with their generated content. Our evaluation encompasses DeepSeekâ€™s latest generation of large language models, multimodal large language models, and text-to-image models, systematically examining their performance regarding unsafe content generation. Notably, we developed a bilingual (Chinese-English) safety evaluation dataset tailored to Chinese sociocultural contexts, enabling a more thorough evaluation of the safety capabilities of Chinese-developed models. Experimental results indicate that despite their strong general capabilities, DeepSeek models exhibit significant safety vulnerabilities across multiple risk dimensions, including algorithmic discrimination and sexual content. These findings provide crucial insights for understanding and improving the safety of large foundation models. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/NY1024/DeepSeek-Safety-Eval">https://github.com/NY1024/DeepSeek-Safety-Eval</a>. </p>
<blockquote>
<p>æœ¬ç ”ç©¶é¦–æ¬¡å¯¹DeepSeekæ¨¡å‹è¿›è¡Œäº†å…¨é¢çš„å®‰å…¨è¯„ä¼°ï¼Œé‡ç‚¹è¯„ä¼°ä¸ç”Ÿæˆå†…å®¹ç›¸å…³çš„å®‰å…¨é£é™©ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ¶µç›–äº†DeepSeekçš„æœ€æ–°ä¸€ä»£å¤§å‹è¯­è¨€æ¨¡å‹ã€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ–‡æœ¬å›¾åƒæ¨¡å‹ï¼Œç³»ç»Ÿåœ°æ£€æŸ¥äº†å®ƒä»¬åœ¨ç”Ÿæˆä¸å®‰å…¨å†…å®¹æ–¹é¢çš„æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬é’ˆå¯¹ä¸­æ–‡ç¤¾ä¼šæ–‡åŒ–ç¯å¢ƒå¼€å‘äº†ä¸€ä¸ªåŒè¯­ï¼ˆä¸­æ–‡è‹±æ–‡ï¼‰å®‰å…¨è¯„ä¼°æ•°æ®é›†ï¼Œèƒ½å¤Ÿå¯¹ä¸­æ–‡å¼€å‘æ¨¡å‹çš„å®‰å…¨èƒ½åŠ›è¿›è¡Œæ›´å…¨é¢çš„è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå°½ç®¡DeepSeekæ¨¡å‹å…·æœ‰å¾ˆå¼ºçš„é€šç”¨èƒ½åŠ›ï¼Œä½†åœ¨å¤šä¸ªé£é™©ç»´åº¦ä¸Šä»å­˜åœ¨æ˜¾è‘—çš„å®‰å…¨æ¼æ´ï¼ŒåŒ…æ‹¬ç®—æ³•æ­§è§†å’Œæ€§å†…å®¹ã€‚è¿™äº›å‘ç°å¯¹äºç†è§£å’Œæ”¹è¿›å¤§å‹åŸºç¡€æ¨¡å‹çš„å®‰å…¨è‡³å…³é‡è¦ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨ <a target="_blank" rel="noopener" href="https://github.com/NY1024/DeepSeek-Safety-Eval">https://github.com/NY1024/DeepSeek-Safety-Eval</a> è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15092v1">PDF</a> </p>
<p><strong>Summary</strong><br>    æœ¬ç ”ç©¶é¦–æ¬¡å…¨é¢è¯„ä¼°äº†DeepSeekæ¨¡å‹çš„å®‰å…¨æ€§èƒ½ï¼Œé‡ç‚¹è¯„ä¼°å…¶ç”Ÿæˆå†…å®¹çš„å®‰å…¨é£é™©ã€‚ç ”ç©¶æ¶µç›–DeepSeekæœ€æ–°ä¸€ä»£çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ–‡æœ¬å›¾åƒæ¨¡å‹ï¼Œç³»ç»Ÿåœ°ç ”ç©¶äº†å®ƒä»¬åœ¨ç”Ÿæˆä¸å®‰å…¨å†…å®¹æ–¹é¢çš„è¡¨ç°ã€‚ç ”ç©¶è¿˜é’ˆå¯¹ä¸­æ–‡ç¤¾ä¼šæ–‡åŒ–ç¯å¢ƒå¼€å‘äº†ä¸€ä¸ªåŒè¯­ï¼ˆä¸­æ–‡-è‹±æ–‡ï¼‰å®‰å…¨è¯„ä¼°æ•°æ®é›†ï¼Œå¯ä»¥æ›´å…¨é¢åœ°è¯„ä¼°ä¸­æ–‡å¼€å‘æ¨¡å‹çš„å®‰å…¨æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDeepSeekæ¨¡å‹åœ¨å¤šä¸ªé£é™©ç»´åº¦ä¸Šå­˜åœ¨æ˜¾è‘—çš„å®‰å…¨æ¼æ´ï¼ŒåŒ…æ‹¬ç®—æ³•æ­§è§†å’Œè‰²æƒ…å†…å®¹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬ç ”ç©¶é¦–æ¬¡å…¨é¢è¯„ä¼°äº†DeepSeekæ¨¡å‹çš„å®‰å…¨æ€§èƒ½ã€‚</li>
<li>ç ”ç©¶é‡ç‚¹è¯„ä¼°äº†DeepSeekæ¨¡å‹ç”Ÿæˆå†…å®¹çš„å®‰å…¨é£é™©ã€‚</li>
<li>ç ”ç©¶æ¶µç›–äº†DeepSeekçš„ä¸åŒç±»å‹æ¨¡å‹ï¼ŒåŒ…æ‹¬å¤§å‹è¯­è¨€æ¨¡å‹ã€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ–‡æœ¬å›¾åƒæ¨¡å‹ã€‚</li>
<li>ç ”ç©¶é’ˆå¯¹ä¸­æ–‡ç¤¾ä¼šæ–‡åŒ–ç¯å¢ƒå¼€å‘äº†ä¸€ä¸ªåŒè¯­å®‰å…¨è¯„ä¼°æ•°æ®é›†ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºDeepSeekæ¨¡å‹åœ¨å¤šä¸ªé£é™©ç»´åº¦ä¸Šå­˜åœ¨å®‰å…¨æ¼æ´ã€‚</li>
<li>è¿™äº›å®‰å…¨é£é™©åŒ…æ‹¬ç®—æ³•æ­§è§†å’Œç”Ÿæˆè‰²æƒ…å†…å®¹ç­‰é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15092">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-af64168fac03fd4ff6f19e388d6a544f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-651cd5790a1ff8baa738c85704ad50b4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3fc5bafdd34afa3392d84c37eea22810.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2589540ff9d6061d30289c2e860890c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c30802c1936d160cd1234368da0ee22.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Neuro-Symbolic-Knowledge-Reasoning-for-Procedural-Video-Question-Answering"><a href="#Neuro-Symbolic-Knowledge-Reasoning-for-Procedural-Video-Question-Answering" class="headerlink" title="Neuro Symbolic Knowledge Reasoning for Procedural Video Question   Answering"></a>Neuro Symbolic Knowledge Reasoning for Procedural Video Question   Answering</h2><p><strong>Authors:Thanh-Son Nguyen, Hong Yang, Tzeh Yuan Neoh, Hao Zhang, Ee Yeo Keat, Basura Fernando</strong></p>
<p>This paper introduces a new video question-answering (VQA) dataset that challenges models to leverage procedural knowledge for complex reasoning. It requires recognizing visual entities, generating hypotheses, and performing contextual, causal, and counterfactual reasoning. To address this, we propose neuro symbolic reasoning module that integrates neural networks and LLM-driven constrained reasoning over variables for interpretable answer generation. Results show that combining LLMs with structured knowledge reasoning with logic enhances procedural reasoning on the STAR benchmark and our dataset. Code and dataset at <a target="_blank" rel="noopener" href="https://github.com/LUNAProject22/KML">https://github.com/LUNAProject22/KML</a> soon. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªæ–°çš„è§†é¢‘é—®ç­”ï¼ˆVQAï¼‰æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†æŒ‘æˆ˜æ¨¡å‹åˆ©ç”¨ç¨‹åºçŸ¥è¯†æ¥è¿›è¡Œå¤æ‚æ¨ç†ã€‚è¿™éœ€è¦è¯†åˆ«è§†è§‰å®ä½“ã€ç”Ÿæˆå‡è®¾ï¼Œå¹¶è¿›è¡Œä¸Šä¸‹æ–‡ã€å› æœå’Œåäº‹å®æ¨ç†ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ç¥ç»ç¬¦å·æ¨ç†æ¨¡å—ï¼Œè¯¥æ¨¡å—ç»“åˆäº†ç¥ç»ç½‘ç»œå’ŒåŸºäºé€»è¾‘ç¼–ç¨‹è¯­è¨€çš„å¤§æ¨¡å‹é©±åŠ¨çš„æœ‰çº¦æŸæ¨ç†ï¼Œä»¥ç”Ÿæˆå¯è§£é‡Šçš„ç­”æ¡ˆã€‚ç»“æœè¡¨æ˜ï¼Œå°†é€»è¾‘ç¼–ç¨‹ä¸å¤§å‹æ¨¡å‹çš„ç»“æ„åŒ–çŸ¥è¯†æ¨ç†ç›¸ç»“åˆï¼Œå¯ä»¥åœ¨STARåŸºå‡†æµ‹è¯•å’Œæˆ‘ä»¬è‡ªå·±çš„æ•°æ®é›†ä¸Šå¢å¼ºç¨‹åºæ¨ç†èƒ½åŠ›ã€‚ä»£ç å’Œæ•°æ®é›†å¾ˆå¿«å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/LUNAProject22/KML%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/LUNAProject22/KMLæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14957v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šæœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªæ–°çš„è§†é¢‘é—®ç­”ï¼ˆVQAï¼‰æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†æŒ‘æˆ˜æ¨¡å‹åˆ©ç”¨è¿‡ç¨‹æ€§çŸ¥è¯†è¿›è¡Œå¤æ‚æ¨ç†ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†ç¥ç»ç¬¦å·æ¨ç†æ¨¡å—ï¼Œç»“åˆäº†ç¥ç»ç½‘ç»œå’ŒåŸºäºé€»è¾‘ç¼–ç¨‹çš„å¤§å‹è¯­è¨€æ¨¡å‹é©±åŠ¨çš„çº¦æŸæ¨ç†ï¼Œä»¥ç”Ÿæˆå¯è§£é‡Šçš„ç­”æ¡ˆã€‚åœ¨STARåŸºå‡†æµ‹è¯•é›†å’Œè‡ªæœ‰æ•°æ®é›†ä¸Šçš„ç»“æœè¡¨æ˜ï¼Œç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹å’Œç»“æ„åŒ–çŸ¥è¯†æ¨ç†å¯æé«˜ç¨‹åºæ€§æ¨ç†èƒ½åŠ›ã€‚ç›¸å…³ä»£ç å’Œæ•°æ®é›†å°†åœ¨ä¸ä¹…åé€šè¿‡LUNAProjectç½‘ç«™å…¬å¼€å‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>è¯¥è®ºæ–‡ä»‹ç»äº†ä¸€ä¸ªæ–°çš„è§†é¢‘é—®ç­”æ•°æ®é›†ï¼Œæ—¨åœ¨æŒ‘æˆ˜æ¨¡å‹åˆ©ç”¨è¿‡ç¨‹æ€§çŸ¥è¯†è¿›è¡Œå¤æ‚æ¨ç†çš„èƒ½åŠ›ã€‚</li>
<li>æ•°æ®é›†è¦æ±‚æ¨¡å‹èƒ½å¤Ÿè¯†åˆ«è§†è§‰å®ä½“ã€ç”Ÿæˆå‡è®¾ï¼Œå¹¶è¿›è¡Œä¸Šä¸‹æ–‡ã€å› æœå’Œåå‘äº‹å®æ¨ç†ã€‚</li>
<li>ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„ç¥ç»ç¬¦å·æ¨ç†æ¨¡å—ã€‚</li>
<li>æ­¤æ¨¡å—ç»“åˆäº†ç¥ç»ç½‘ç»œå’Œå¤§å‹è¯­è¨€æ¨¡å‹é©±åŠ¨çš„çº¦æŸæ¨ç†ï¼Œä»¥ç”Ÿæˆå¯è§£é‡Šçš„ç­”æ¡ˆã€‚</li>
<li>åœ¨STARåŸºå‡†æµ‹è¯•é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹å’Œç»“æ„åŒ–çŸ¥è¯†æ¨ç†æœ‰åŠ©äºæé«˜ç¨‹åºæ€§æ¨ç†èƒ½åŠ›ã€‚</li>
<li>è®ºæ–‡ä¸­æåˆ°çš„æ•°æ®é›†ä¸­çš„å¤æ‚æ¨ç†åŒ…æ‹¬è¯†åˆ«å’Œè§£é‡Šè§†è§‰å®ä½“ä¹‹é—´çš„å…³ç³»ä»¥åŠç”ŸæˆåŸºäºè¿™äº›å…³ç³»çš„å‡è®¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14957">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-0c03c8ce5820bca96c89327c2fedce71.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d3c351111fee16f740c5e2e176d8ed5c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c264616bbd196a20f4296dfc83b9b5ec.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="UPME-An-Unsupervised-Peer-Review-Framework-for-Multimodal-Large-Language-Model-Evaluation"><a href="#UPME-An-Unsupervised-Peer-Review-Framework-for-Multimodal-Large-Language-Model-Evaluation" class="headerlink" title="UPME: An Unsupervised Peer Review Framework for Multimodal Large   Language Model Evaluation"></a>UPME: An Unsupervised Peer Review Framework for Multimodal Large   Language Model Evaluation</h2><p><strong>Authors:Qihui Zhang, Munan Ning, Zheyuan Liu, Yanbo Wang, Jiayi Ye, Yue Huang, Shuo Yang, Xiao Chen, Yibing Song, Li Yuan</strong></p>
<p>Multimodal Large Language Models (MLLMs) have emerged to tackle the challenges of Visual Question Answering (VQA), sparking a new research focus on conducting objective evaluations of these models. Existing evaluation methods face limitations due to the significant human workload required to design Q&amp;A pairs for visual images, which inherently restricts the scale and scope of evaluations. Although automated MLLM-as-judge approaches attempt to reduce the human workload through automatic evaluations, they often introduce biases. To address these problems, we propose an Unsupervised Peer review MLLM Evaluation framework. It utilizes only image data, allowing models to automatically generate questions and conduct peer review assessments of answers from other models, effectively alleviating the reliance on human workload. Additionally, we introduce the vision-language scoring system to mitigate the bias issues, which focuses on three aspects: (i) response correctness; (ii) visual understanding and reasoning; and (iii) image-text correlation. Experimental results demonstrate that UPME achieves a Pearson correlation of 0.944 with human evaluations on the MMstar dataset and 0.814 on the ScienceQA dataset, indicating that our framework closely aligns with human-designed benchmarks and inherent human preferences. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å‡ºç°ï¼Œè§£å†³äº†è§†è§‰é—®ç­”ï¼ˆVQAï¼‰çš„æŒ‘æˆ˜ï¼Œå¼•å‘äº†å…³äºè¿™äº›æ¨¡å‹è¿›è¡Œå®¢è§‚è¯„ä¼°çš„æ–°ç ”ç©¶ç„¦ç‚¹ã€‚ç”±äºä¸ºè§†è§‰å›¾åƒè®¾è®¡é—®ç­”å¯¹éœ€è¦å¤§é‡çš„äººå·¥å·¥ä½œï¼Œç°æœ‰çš„è¯„ä¼°æ–¹æ³•é¢ä¸´å±€é™æ€§ï¼Œè¿™æœ¬è´¨ä¸Šé™åˆ¶äº†è¯„ä¼°çš„è§„æ¨¡å’ŒèŒƒå›´ã€‚å°½ç®¡è‡ªåŠ¨çš„MLLMè¯„åˆ¤æ–¹æ³•è¯•å›¾é€šè¿‡è‡ªåŠ¨è¯„ä¼°æ¥å‡å°‘äººå·¥å·¥ä½œé‡ï¼Œä½†å®ƒä»¬å¾€å¾€å¼•å…¥åè§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— ç›‘ç£çš„åŒè¡Œè¯„å®¡MLLMè¯„ä¼°æ¡†æ¶ã€‚å®ƒä»…ä½¿ç”¨å›¾åƒæ•°æ®ï¼Œå…è®¸æ¨¡å‹è‡ªåŠ¨ç”Ÿæˆé—®é¢˜å¹¶å¯¹å…¶ä»–æ¨¡å‹çš„ç­”æ¡ˆè¿›è¡ŒåŒè¡Œè¯„å®¡è¯„ä¼°ï¼Œæœ‰æ•ˆåœ°å‡è½»äº†äººå·¥å·¥ä½œé‡çš„ä¾èµ–ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†è§†è§‰è¯­è¨€è¯„åˆ†ç³»ç»Ÿæ¥ç¼“è§£åè§é—®é¢˜ï¼Œè¯¥ç³»ç»Ÿä¾§é‡äºä¸‰ä¸ªæ–¹é¢ï¼šï¼ˆiï¼‰å“åº”çš„æ­£ç¡®æ€§ï¼›ï¼ˆiiï¼‰è§†è§‰ç†è§£å’Œæ¨ç†ï¼›ï¼ˆiiiï¼‰å›¾åƒæ–‡æœ¬ç›¸å…³æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUPMEåœ¨MMstaræ•°æ®é›†ä¸Šä¸äººå·¥è¯„ä¼°çš„Pearsonç›¸å…³æ€§è¾¾åˆ°0.944ï¼Œåœ¨ScienceQAæ•°æ®é›†ä¸Šè¾¾åˆ°0.814ï¼Œè¿™è¡¨æ˜æˆ‘ä»¬çš„æ¡†æ¶ä¸äººå·¥è®¾è®¡çš„åŸºå‡†æµ‹è¯•å’Œå†…åœ¨çš„äººç±»åå¥½é«˜åº¦ä¸€è‡´ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14941v1">PDF</a> Accepted by CVPR 2025</p>
<p><strong>Summary</strong>ï¼šæ–°å…´çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ­£åŠªåŠ›åº”å¯¹è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ä¸­çš„æŒ‘æˆ˜ï¼Œå› æ­¤ç ”ç©¶é‡ç‚¹å·²ç»è½¬å‘å¯¹è¿™äº›æ¨¡å‹è¿›è¡Œå®¢è§‚è¯„ä¼°çš„æ–¹æ³•ä¸Šã€‚ç°æœ‰è¯„ä¼°æ–¹æ³•è®¾è®¡é—®ç­”å¯¹éœ€è¦å¤§é‡äººå·¥æ“ä½œï¼Œé™åˆ¶äº†è¯„ä¼°è§„æ¨¡å’ŒèŒƒå›´ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†æ— ç›‘ç£åŒè¡Œè¯„å®¡çš„MLLMè¯„ä¼°æ¡†æ¶ï¼Œä»…ä½¿ç”¨å›¾åƒæ•°æ®ï¼Œè®©æ¨¡å‹è‡ªåŠ¨ç”Ÿæˆé—®é¢˜å¹¶å¯¹å…¶ä»–æ¨¡å‹çš„ç­”æ¡ˆè¿›è¡ŒåŒè¡Œè¯„å®¡è¯„ä¼°ï¼Œå¤§å¤§å‡å°‘äº†äººå·¥å·¥ä½œé‡ã€‚åŒæ—¶ï¼Œå¼•å…¥è§†è§‰è¯­è¨€è¯„åˆ†ç³»ç»Ÿï¼Œä»å›ç­”æ­£ç¡®æ€§ã€è§†è§‰ç†è§£å’Œæ¨ç†ä»¥åŠå›¾åƒæ–‡æœ¬ç›¸å…³æ€§ä¸‰ä¸ªæ–¹é¢å‡è½»åè§é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUPMEåœ¨MMstaræ•°æ®é›†ä¸Šä¸äººç±»è¯„ä¼°çš„çš®å°”é€Šç›¸å…³æ€§è¾¾åˆ°0.944ï¼Œåœ¨ScienceQAæ•°æ®é›†ä¸Šè¾¾åˆ°0.814ï¼Œè¡¨æ˜æˆ‘ä»¬çš„æ¡†æ¶ä¸äººç±»è®¾è®¡çš„åŸºå‡†æµ‹è¯•å’Œå†…åœ¨äººç±»åå¥½é«˜åº¦ä¸€è‡´ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ä¸­é¢ä¸´æŒ‘æˆ˜ï¼Œéœ€è¦æ–°çš„è¯„ä¼°æ–¹æ³•ã€‚</li>
<li>ç°æœ‰è¯„ä¼°æ–¹æ³•éœ€è¦å¤§é‡äººå·¥è®¾è®¡é—®ç­”å¯¹ï¼Œé™åˆ¶äº†è¯„ä¼°è§„æ¨¡å’ŒèŒƒå›´ã€‚</li>
<li>æå‡ºæ— ç›‘ç£åŒè¡Œè¯„å®¡çš„MLLMè¯„ä¼°æ¡†æ¶ï¼Œå‡å°‘äººå·¥å·¥ä½œé‡ï¼Œåˆ©ç”¨å›¾åƒæ•°æ®è‡ªåŠ¨ç”Ÿæˆé—®é¢˜å’Œç­”æ¡ˆè¯„å®¡ã€‚</li>
<li>å¼•å…¥è§†è§‰è¯­è¨€è¯„åˆ†ç³»ç»Ÿï¼ŒåŒ…æ‹¬ä¸‰ä¸ªå…³é”®æ–¹é¢ï¼šå›ç­”æ­£ç¡®æ€§ã€è§†è§‰ç†è§£å’Œæ¨ç†ã€å›¾åƒæ–‡æœ¬ç›¸å…³æ€§ã€‚</li>
<li>UPMEåœ¨MMstarå’ŒScienceQAæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ä¸äººç±»è¯„ä¼°é«˜åº¦ä¸€è‡´ã€‚</li>
<li>UPMEæ¡†æ¶æœ‰åŠ©äºæ›´å®¢è§‚ã€å¤§è§„æ¨¡åœ°è¯„ä¼°MLLMsçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14941">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6b30f62b6bf80612aaa68104c3178961.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-900eff205ae6daa8a7c5e8e1e41cd279.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-15744e21fd421f3edb4a4c0f973a51b2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a00608a133403d0c71b7a3cf40260bac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e2985c1fc31e721d08f9233adc4c3e03.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="MetaLadder-Ascending-Mathematical-Solution-Quality-via-Analogical-Problem-Reasoning-Transfer"><a href="#MetaLadder-Ascending-Mathematical-Solution-Quality-via-Analogical-Problem-Reasoning-Transfer" class="headerlink" title="MetaLadder: Ascending Mathematical Solution Quality via   Analogical-Problem Reasoning Transfer"></a>MetaLadder: Ascending Mathematical Solution Quality via   Analogical-Problem Reasoning Transfer</h2><p><strong>Authors:Honglin Lin, Zhuoshi Pan, Yu Li, Qizhi Pei, Xin Gao, Mengzhang Cai, Conghui He, Lijun Wu</strong></p>
<p>Large Language Models (LLMs) have demonstrated promising capabilities in solving mathematical reasoning tasks, leveraging Chain-of-Thought (CoT) data as a vital component in guiding answer generation. Current paradigms typically generate CoT and answers directly for a given problem, diverging from human problem-solving strategies to some extent. Humans often solve problems by recalling analogous cases and leveraging their solutions to reason about the current task. Inspired by this cognitive process, we propose \textbf{MetaLadder}, a novel framework that explicitly prompts LLMs to recall and reflect on meta-problems, those structurally or semantically analogous problems, alongside their CoT solutions before addressing the target problem. Additionally, we introduce a problem-restating mechanism to enhance the modelâ€™s comprehension of the target problem by regenerating the original question, which further improves reasoning accuracy. Therefore, the model can achieve reasoning transfer from analogical problems, mimicking human-like â€œlearning from examplesâ€ and generalization abilities. Extensive experiments on mathematical benchmarks demonstrate that our MetaLadder significantly boosts LLMsâ€™ problem-solving accuracy, largely outperforming standard CoT-based methods (\textbf{10.3%} accuracy gain) and other methods. Our code and data has been released at <a target="_blank" rel="noopener" href="https://github.com/LHL3341/MetaLadder">https://github.com/LHL3341/MetaLadder</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è§£å†³æ•°å­¦æ¨ç†ä»»åŠ¡æ–¹é¢å±•ç°å‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œåˆ©ç”¨â€œæ€ç»´é“¾â€ï¼ˆCoTï¼‰æ•°æ®ä½œä¸ºå¼•å¯¼ç­”æ¡ˆç”Ÿæˆçš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚å½“å‰çš„æ–¹æ³•é€šå¸¸ç›´æ¥ä¸ºç»™å®šçš„é—®é¢˜ç”ŸæˆCoTå’Œç­”æ¡ˆï¼Œè¿™åœ¨æŸç§ç¨‹åº¦ä¸Šä¸äººç±»çš„é—®é¢˜è§£å†³ç­–ç•¥æœ‰æ‰€ä¸åŒã€‚äººç±»å¾€å¾€é€šè¿‡å›å¿†ç±»ä¼¼æ¡ˆä¾‹å¹¶å€ŸåŠ©å…¶è§£å†³æ–¹æ¡ˆæ¥æ¨ç†å½“å‰ä»»åŠ¡ã€‚å—æ­¤è®¤çŸ¥è¿‡ç¨‹çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†<strong>MetaLadder</strong>è¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œå®ƒæ˜ç¡®æç¤ºLLMå›å¿†å¹¶åæ€å…ƒé—®é¢˜ï¼ˆé‚£äº›åœ¨ç»“æ„æˆ–è¯­ä¹‰ä¸Šç±»ä¼¼çš„é—®é¢˜ï¼‰åŠå…¶CoTè§£å†³æ–¹æ¡ˆï¼Œå†è§£å†³ç›®æ ‡é—®é¢˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§é—®é¢˜é‡è¿°æœºåˆ¶ï¼Œé€šè¿‡é‡æ–°ç”ŸæˆåŸå§‹é—®é¢˜æ¥æé«˜æ¨¡å‹å¯¹ç›®æ ‡é—®é¢˜çš„ç†è§£ï¼Œè¿™è¿›ä¸€æ­¥æé«˜äº†æ¨ç†å‡†ç¡®æ€§ã€‚å› æ­¤ï¼Œè¯¥æ¨¡å‹å¯ä»¥ä»ç±»ä¼¼é—®é¢˜ä¸­å®ç°æ¨ç†è¿ç§»ï¼Œæ¨¡ä»¿äººç±»â€œä»ä¾‹å­ä¸­å­¦ä¹ â€å’Œæ³›åŒ–èƒ½åŠ›ã€‚åœ¨æ•°å­¦åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„MetaLadderæ˜¾è‘—æé«˜äº†LLMçš„é—®é¢˜è§£å†³å‡†ç¡®æ€§ï¼Œå¤§å¤§ä¼˜äºæ ‡å‡†CoTæ–¹æ³•ï¼ˆ**æé«˜äº†10.3%**çš„å‡†ç¡®æ€§ï¼‰å’Œå…¶ä»–æ–¹æ³•ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/LHL3341/MetaLadder%E3%80%82">https://github.com/LHL3341/MetaLadderã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14891v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è§£å†³æ•°å­¦æ¨ç†ä»»åŠ¡æ—¶è¡¨ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œå…¶ä¸­æ€ç»´é“¾ï¼ˆCoTï¼‰æ•°æ®ä½œä¸ºå¼•å¯¼ç­”æ¡ˆç”Ÿæˆçš„å…³é”®ç»„æˆéƒ¨åˆ†ã€‚å½“å‰çš„æ–¹æ³•é€šå¸¸ç›´æ¥ä¸ºç»™å®šé—®é¢˜ç”ŸæˆCoTå’Œç­”æ¡ˆï¼Œè¿™ä¸äººç±»çš„é—®é¢˜è§£å†³ç­–ç•¥æœ‰æ‰€ä¸åŒã€‚äººç±»å¸¸å¸¸é€šè¿‡å›å¿†ç±»ä¼¼æ¡ˆä¾‹å¹¶å€Ÿé‰´å…¶è§£å†³æ–¹æ¡ˆæ¥æ¨ç†å½“å‰ä»»åŠ¡ã€‚å—è¿™ä¸€è®¤çŸ¥è¿‡ç¨‹çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†MetaLadderè¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œå®ƒæ˜ç¡®æç¤ºLLMsåœ¨è§£å†³ç›®æ ‡é—®é¢˜ä¹‹å‰å›å¿†å’Œåæ€ä¸å½“å‰é—®é¢˜ç»“æ„æˆ–è¯­ä¹‰ä¸Šç±»ä¼¼çš„é—®é¢˜åŠå…¶CoTè§£å†³æ–¹æ¡ˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§é—®é¢˜é‡è¿°æœºåˆ¶ï¼Œä»¥æé«˜æ¨¡å‹å¯¹ç›®æ ‡é—®é¢˜çš„ç†è§£ï¼Œé€šè¿‡é‡æ–°ç”ŸæˆåŸå§‹é—®é¢˜æ¥æé«˜æ¨ç†å‡†ç¡®æ€§ã€‚å› æ­¤ï¼Œè¯¥æ¨¡å‹èƒ½ä»ç±»ä¼¼é—®é¢˜ä¸­å®ç°æ¨ç†è¿ç§»ï¼Œæ¨¡ä»¿äººç±»â€œä»ä¾‹å­ä¸­å­¦ä¹ â€çš„èƒ½åŠ›å¹¶å®ç°æ³›åŒ–ã€‚åœ¨æ•°å­¦åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„MetaLadderæ˜¾è‘—æé«˜äº†LLMsçš„é—®é¢˜è§£å†³å‡†ç¡®æ€§ï¼Œå¤§å¹…è¶…è¶Šäº†æ ‡å‡†CoTæ–¹æ³•ï¼ˆå‡†ç¡®ç‡æå‡10.3%ï¼‰ï¼Œä»¥åŠå…¶ä»–æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså·²å±•ç°å‡ºè§£å†³æ•°å­¦æ¨ç†ä»»åŠ¡çš„æ½œåŠ›ï¼Œå…¶ä¸­æ€ç»´é“¾ï¼ˆCoTï¼‰æ•°æ®åœ¨å¼•å¯¼ç­”æ¡ˆç”Ÿæˆæ–¹é¢èµ·å…³é”®ä½œç”¨ã€‚</li>
<li>å½“å‰LLMæ–¹æ³•ç›´æ¥ç”Ÿæˆç­”æ¡ˆï¼Œä¸äººç±»çš„é—®é¢˜è§£å†³ç­–ç•¥æœ‰æ‰€ä¸åŒï¼Œäººç±»æ›´å–„äºåˆ©ç”¨ç±»ä¼¼æ¡ˆä¾‹è¿›è¡Œæ¨ç†ã€‚</li>
<li>MetaLadderæ¡†æ¶é¼“åŠ±LLMsåœ¨è§£å†³ç›®æ ‡é—®é¢˜å‰å›å¿†å’Œåæ€ç±»ä¼¼é—®é¢˜çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>MetaLadderå¼•å…¥é—®é¢˜é‡è¿°æœºåˆ¶ï¼Œæé«˜æ¨¡å‹å¯¹ç›®æ ‡é—®é¢˜çš„ç†è§£ã€‚</li>
<li>MetaLadderæé«˜äº†LLMsçš„æ¨ç†å‡†ç¡®æ€§ï¼Œé€šè¿‡ä»ç±»ä¼¼é—®é¢˜ä¸­å­¦ä¹ å¹¶æ³›åŒ–ã€‚</li>
<li>ä¸æ ‡å‡†æ€ç»´é“¾æ–¹æ³•å’Œå…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼ŒMetaLadderè¡¨ç°å‡ºæ˜¾è‘—æå‡ï¼Œå‡†ç¡®ç‡æé«˜10.3%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14891">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-626cc8425528154b2a05f685ddf961a6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a95163efa64b2a88d239ec58199e50ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0dae500757460ce4638e15ede45379b.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="LogLLaMA-Transformer-based-log-anomaly-detection-with-LLaMA"><a href="#LogLLaMA-Transformer-based-log-anomaly-detection-with-LLaMA" class="headerlink" title="LogLLaMA: Transformer-based log anomaly detection with LLaMA"></a>LogLLaMA: Transformer-based log anomaly detection with LLaMA</h2><p><strong>Authors:Zhuoyi Yang, Ian G. Harris</strong></p>
<p>Log anomaly detection refers to the task that distinguishes the anomalous log messages from normal log messages. Transformer-based large language models (LLMs) are becoming popular for log anomaly detection because of their superb ability to understand complex and long language patterns. In this paper, we propose LogLLaMA, a novel framework that leverages LLaMA2. LogLLaMA is first finetuned on normal log messages from three large-scale datasets to learn their patterns. After finetuning, the model is capable of generating successive log messages given previous log messages. Our generative model is further trained to identify anomalous log messages using reinforcement learning (RL). The experimental results show that LogLLaMA outperforms the state-of-the-art approaches for anomaly detection on BGL, Thunderbird, and HDFS datasets. </p>
<blockquote>
<p>æ—¥å¿—å¼‚å¸¸æ£€æµ‹æ˜¯æŒ‡ä»æ­£å¸¸æ—¥å¿—æ¶ˆæ¯ä¸­åŒºåˆ†å‡ºå¼‚å¸¸æ—¥å¿—æ¶ˆæ¯çš„ä»»åŠ¡ã€‚åŸºäºTransformerçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”±äºå…¶ç†è§£å¤æ‚å’Œé•¿æœŸè¯­è¨€æ¨¡å¼çš„èƒ½åŠ›ï¼Œåœ¨æ—¥å¿—å¼‚å¸¸æ£€æµ‹ä¸­è¶Šæ¥è¶Šå—æ¬¢è¿ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†LogLLaMAï¼Œä¸€ä¸ªåˆ©ç”¨LLaMA2çš„æ–°å‹æ¡†æ¶ã€‚LogLLaMAé¦–å…ˆé€šè¿‡å¯¹ä¸‰ä¸ªå¤§å‹æ•°æ®é›†ä¸­çš„æ­£å¸¸æ—¥å¿—æ¶ˆæ¯è¿›è¡Œå¾®è°ƒï¼Œå­¦ä¹ å…¶æ¨¡å¼ã€‚å¾®è°ƒåï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨ç»™å®šä¹‹å‰çš„æ—¥å¿—æ¶ˆæ¯çš„æƒ…å†µä¸‹ç”Ÿæˆè¿ç»­çš„æ—¥å¿—æ¶ˆæ¯ã€‚æˆ‘ä»¬çš„ç”Ÿæˆæ¨¡å‹è¿›ä¸€æ­¥ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›è¡Œè®­ç»ƒï¼Œä»¥è¯†åˆ«å¼‚å¸¸çš„æ—¥å¿—æ¶ˆæ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLogLLaMAåœ¨BGLã€Thunderbirdå’ŒHDFSæ•°æ®é›†ä¸Šçš„å¼‚å¸¸æ£€æµ‹ä¼˜äºæœ€æ–°æŠ€æœ¯æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14849v1">PDF</a> 8 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºTransformerçš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ—¥å¿—å¼‚å¸¸æ£€æµ‹ä¸­å±•ç°å‡ºå“è¶Šçš„ç†è§£å¤æ‚å’Œé•¿æœŸè¯­è¨€æ¨¡å¼çš„èƒ½åŠ›ï¼Œå› æ­¤å¤‡å—å…³æ³¨ã€‚æœ¬æ–‡æå‡ºäº†LogLLaMAæ¡†æ¶ï¼Œå®ƒåˆ©ç”¨LLaMA2æ¨¡å‹è¿›è¡Œå¾®è°ƒå­¦ä¹ æ­£å¸¸æ—¥å¿—æ¶ˆæ¯çš„æ¨¡å¼ï¼Œç„¶åé€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›ä¸€æ­¥è®­ç»ƒç”Ÿæˆæ¨¡å‹ä»¥è¯†åˆ«å¼‚å¸¸æ—¥å¿—æ¶ˆæ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLogLLaMAåœ¨BGLã€Thunderbirdå’ŒHDFSæ•°æ®é›†ä¸Šçš„å¼‚å¸¸æ£€æµ‹æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LogLLaMAæ¡†æ¶åˆ©ç”¨äº†å¤§å‹è¯­è¨€æ¨¡å‹LLaMA2è¿›è¡Œæ—¥å¿—å¼‚å¸¸æ£€æµ‹ã€‚</li>
<li>è¯¥æ¡†æ¶é¦–å…ˆé€šè¿‡å¾®è°ƒå­¦ä¹ æ­£å¸¸æ—¥å¿—æ¶ˆæ¯çš„æ¨¡å¼ã€‚</li>
<li>LogLLaMAèƒ½å¤ŸåŸºäºä¹‹å‰æ—¥å¿—ç”Ÿæˆåç»­æ—¥å¿—æ¶ˆæ¯ã€‚</li>
<li>ä½¿ç”¨å¼ºåŒ–å­¦ä¹ è¿›ä¸€æ­¥è®­ç»ƒç”Ÿæˆæ¨¡å‹ä»¥è¯†åˆ«å¼‚å¸¸æ—¥å¿—æ¶ˆæ¯ã€‚</li>
<li>LogLLaMAåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„æ€§èƒ½è¶…è¿‡äº†ç°æœ‰çš„å¼‚å¸¸æ£€æµ‹æ–¹æ³•ã€‚</li>
<li>Transformeræ¨¡å‹ç†è§£å¤æ‚å’Œé•¿æœŸè¯­è¨€æ¨¡å¼çš„èƒ½åŠ›ä½¿å…¶åœ¨æ—¥å¿—å¼‚å¸¸æ£€æµ‹ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14849">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-07a8af6d3e8342ef7ba09bb09db47902.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-316764f7815ae873d1d5a60b86f7a240.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a29bb67a8c4ab4594ee6d9e6bb20de5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-04be2f08a9f4431640e440253e2788e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70b9a97bffdabf88fe57edfa3ca8ca3f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ff8b1911d630e0cfef1cbd9c7076fa86.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7bb9f170f721dff7bd3aec1c9f704e30.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Think-Like-Human-Developers-Harnessing-Community-Knowledge-for-Structured-Code-Reasoning"><a href="#Think-Like-Human-Developers-Harnessing-Community-Knowledge-for-Structured-Code-Reasoning" class="headerlink" title="Think Like Human Developers: Harnessing Community Knowledge for   Structured Code Reasoning"></a>Think Like Human Developers: Harnessing Community Knowledge for   Structured Code Reasoning</h2><p><strong>Authors:Chengran Yang, Zhensu Sun, Hong Jin Kang, Jieke Shi, David Lo</strong></p>
<p>Large Language Models (LLMs) have significantly advanced automated code generation, yet they struggle with complex coding tasks requiring multi-step logical reasoning. High-quality reasoning data is crucial for improving LLMsâ€™ reasoning capabilities, but such datasets remain scarce. Existing approaches either rely on computationally expensive reinforcement learning (RL) or error-prone reasoning chains synthesized by LLMs, posing challenges in scalability and accuracy.   To address this challenge, we propose SVRC (Structured and Validated Reasoning Chains for Code Generation), a novel framework that mines, restructures, and enriches reasoning chains from community-driven discussions on software engineering platforms. SVRC refines unstructured and incomplete discussions of coding problems by aligning them with Software Development Life Cycle (SDLC) principles, ensuring that reasoning chains capture real-world problem-solving strategies and support iterative refinement.   To evaluate the effectiveness of SVRC, we introduce CodeThinker, an LLM fine-tuned on 12,444 reasoning-augmented samples generated by SVRC. Experiments on LiveCodeBench show that CodeThinker surpasses its base model by 42.86% on medium-level code problems in terms of pass@1 and outperforms GPT-4o-mini and GPT-4o by 73.14% and 115.86%, respectively. Our ablation study further highlights that each component of SVRC contributes to the reasoning capabilities of CodeThinker. </p>
<blockquote>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªåŠ¨ä»£ç ç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨éœ€è¦å¤šæ­¥é€»è¾‘æ¨ç†çš„å¤æ‚ç¼–ç ä»»åŠ¡æ–¹é¢ä»é¢ä¸´å›°éš¾ã€‚é«˜è´¨é‡çš„æ¨ç†æ•°æ®å¯¹äºæé«˜LLMçš„æ¨ç†èƒ½åŠ›è‡³å…³é‡è¦ï¼Œä½†è¿™æ ·çš„æ•°æ®é›†ä»ç„¶ç¨€ç¼ºã€‚ç°æœ‰æ–¹æ³•è¦ä¹ˆä¾èµ–äºè®¡ç®—æ˜‚è´µçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œè¦ä¹ˆä¾èµ–äºLLMåˆæˆçš„å®¹æ˜“å‡ºé”™çš„æ¨ç†é“¾ï¼Œè¿™å¸¦æ¥äº†å¯æ‰©å±•æ€§å’Œå‡†ç¡®æ€§æ–¹é¢çš„æŒ‘æˆ˜ã€‚</p>
</blockquote>
<p>ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†SVRCï¼ˆç”¨äºä»£ç ç”Ÿæˆçš„ç»“æ„åŒ–å’ŒéªŒè¯æ¨ç†é“¾ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªä»è½¯ä»¶å·¥ç¨‹å¹³å°ä¸Šçš„ç¤¾åŒºé©±åŠ¨è®¨è®ºä¸­æŒ–æ˜ã€é‡æ„å’Œä¸°å¯Œæ¨ç†é“¾çš„æ–°å‹æ¡†æ¶ã€‚SVRCé€šè¿‡éµå¾ªè½¯ä»¶å¼€å‘ç”Ÿå‘½å‘¨æœŸï¼ˆSDLCï¼‰åŸåˆ™ï¼Œå¯¹ç¼–ç é—®é¢˜çš„éç»“æ„åŒ–å’Œä¸å®Œæ•´è®¨è®ºè¿›è¡Œç²¾ç‚¼ï¼Œç¡®ä¿æ¨ç†é“¾æ•æ‰ç°å®ä¸–ç•Œçš„é—®é¢˜è§£å†³ç­–ç•¥å¹¶æ”¯æŒè¿­ä»£ä¼˜åŒ–ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14838v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªåŠ¨åŒ–ä»£ç ç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨éœ€è¦å¤šæ­¥é€»è¾‘æ¨ç†çš„å¤æ‚ç¼–ç ä»»åŠ¡ä¸Šè¡¨ç°æŒ£æ‰ã€‚é«˜è´¨é‡æ¨ç†æ•°æ®å¯¹äºæå‡LLMçš„æ¨ç†èƒ½åŠ›è‡³å…³é‡è¦ï¼Œä½†è¿™æ ·çš„æ•°æ®é›†ä»ç„¶ç¨€ç¼ºã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äºè®¡ç®—æˆæœ¬é«˜æ˜‚çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æˆ–LLMåˆæˆçš„æ˜“å‡ºé”™æ¨ç†é“¾ï¼Œå­˜åœ¨å¯æ‰©å±•æ€§å’Œå‡†ç¡®æ€§æ–¹é¢çš„æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºSVRCï¼ˆç”¨äºä»£ç ç”Ÿæˆçš„ç»“æ„åŒ–éªŒè¯æ¨ç†é“¾ï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä»è½¯ä»¶å·¥ç¨‹å¹³å°ä¸Šçš„ç¤¾åŒºé©±åŠ¨è®¨è®ºä¸­æŒ–æ˜ã€é‡æ„å’Œä¸°å¯Œæ¨ç†é“¾ã€‚SVRCé€šè¿‡éµå¾ªè½¯ä»¶å¼€å‘ç”Ÿå‘½å‘¨æœŸï¼ˆSDLCï¼‰åŸåˆ™ï¼Œå¯¹ç¼–ç é—®é¢˜çš„éç»“æ„åŒ–å’Œä¸å®Œæ•´è®¨è®ºè¿›è¡Œç²¾ç‚¼ï¼Œç¡®ä¿æ¨ç†é“¾æ•æ‰ç°å®ä¸–ç•Œçš„é—®é¢˜è§£å†³ç­–ç•¥å¹¶æ”¯æŒè¿­ä»£æ”¹è¿›ã€‚é€šè¿‡å¼•å…¥CodeThinkerï¼Œä¸€ä¸ªåœ¨SVRCç”Ÿæˆçš„12,444ä¸ªæ¨ç†å¢å¼ºæ ·æœ¬ä¸Šè¿›è¡Œå¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå®éªŒè¡¨æ˜CodeThinkeråœ¨ä¸­ç­‰éš¾åº¦çš„ä»£ç é—®é¢˜ä¸Šè¶…è¶Šäº†å…¶åŸºç¡€æ¨¡å‹42.86%ï¼Œå¹¶ä¸”åœ¨LiveCodeBenchä¸Šçš„pass@1æŒ‡æ ‡ä¸Šä¼˜äºGPT-4o-miniå’ŒGPT-4oåˆ†åˆ«è¾¾73.14%å’Œ115.86%ã€‚æˆ‘ä»¬çš„æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥å¼ºè°ƒäº†SVRCçš„æ¯ä¸ªç»„ä»¶å¯¹CodeThinkeræ¨ç†èƒ½åŠ›çš„è´¡çŒ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªåŠ¨åŒ–ä»£ç ç”Ÿæˆæ–¹é¢æœ‰æ‰€æˆå°±ï¼Œä½†åœ¨å¤æ‚ç¼–ç ä»»åŠ¡çš„é€»è¾‘æ¨ç†ä¸Šä»æœ‰ä¸è¶³ã€‚</li>
<li>é«˜è´¨é‡æ¨ç†æ•°æ®å¯¹æå‡LLMsçš„æ¨ç†èƒ½åŠ›è‡³å…³é‡è¦ï¼Œä½†è¿™æ ·çš„æ•°æ®é›†è¾ƒä¸ºç¨€ç¼ºã€‚</li>
<li>ç°æœ‰æ–¹æ³•é¢ä¸´çš„æŒ‘æˆ˜åŒ…æ‹¬å¼ºåŒ–å­¦ä¹ çš„è®¡ç®—æˆæœ¬é«˜æ˜‚å’Œæ¨ç†é“¾çš„å‡†ç¡®æ€§é—®é¢˜ã€‚</li>
<li>SVRCæ¡†æ¶é€šè¿‡æŒ–æ˜å’Œé‡æ„ç¤¾åŒºé©±åŠ¨çš„è½¯ä»¶å·¥ç¨‹è®¨è®ºä¸­çš„æ¨ç†é“¾ï¼Œä»¥æ”¹å–„ä»£ç ç”Ÿæˆçš„é€»è¾‘æ¨ç†èƒ½åŠ›ã€‚</li>
<li>SVRCéµå¾ªè½¯ä»¶å¼€å‘ç”Ÿå‘½å‘¨æœŸåŸåˆ™ï¼Œç¡®ä¿æ¨ç†é“¾æ•æ‰ç°å®ä¸–ç•Œçš„è§£å†³é—®é¢˜ç­–ç•¥å¹¶æ”¯æŒè¿­ä»£æ”¹è¿›ã€‚</li>
<li>CodeThinkeræ¨¡å‹åœ¨ä¸­ç­‰éš¾åº¦ä»£ç é—®é¢˜ä¸Šçš„è¡¨ç°è¶…è¶Šäº†åŸºç¡€æ¨¡å‹å’Œå…¶ä»–æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14838">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b4eb590f185931a5bfeb4ada751cc2a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d7f2c8b3827b780a4237ea48dd4096ab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce4c69e53259f8ccbae3382978daab2f.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Elevating-Visual-Question-Answering-through-Implicitly-Learned-Reasoning-Pathways-in-LVLMs"><a href="#Elevating-Visual-Question-Answering-through-Implicitly-Learned-Reasoning-Pathways-in-LVLMs" class="headerlink" title="Elevating Visual Question Answering through Implicitly Learned Reasoning   Pathways in LVLMs"></a>Elevating Visual Question Answering through Implicitly Learned Reasoning   Pathways in LVLMs</h2><p><strong>Authors:Liu Jing, Amirul Rahman</strong></p>
<p>Large Vision-Language Models (LVLMs) have shown remarkable progress in various multimodal tasks, yet they often struggle with complex visual reasoning that requires multi-step inference. To address this limitation, we propose MF-SQ-LLaVA, a novel approach that enhances LVLMs by enabling implicit self-questioning through end-to-end training. Our method involves augmenting visual question answering datasets with reasoning chains consisting of sub-question and answer pairs, and training the LVLM with a multi-task loss that encourages the generation and answering of these intermediate steps, as well as the prediction of the final answer. We conduct extensive experiments on the ScienceQA and VQAv2 datasets, demonstrating that MF-SQ-LLaVA significantly outperforms existing state-of-the-art models, including the base LLaVA and the original SQ-LLaVA. Ablation studies further validate the contribution of each component of our approach, and human evaluation confirms the improved accuracy and coherence of the reasoning process enabled by our method. </p>
<blockquote>
<p>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨å„ç§å¤šæ¨¡æ€ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä½†åœ¨éœ€è¦å¤šæ­¥éª¤æ¨ç†çš„å¤æ‚è§†è§‰æ¨ç†æ–¹é¢å¸¸å¸¸é‡åˆ°å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†MF-SQ-LLaVAè¿™ä¸€æ–°æ–¹æ³•ï¼Œå®ƒé€šè¿‡ç«¯åˆ°ç«¯çš„è®­ç»ƒæ¥å¢å¼ºLVLMsçš„èƒ½åŠ›ï¼Œå®ç°éšå¼è‡ªé—®ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡å¢åŠ è§†è§‰é—®ç­”æ•°æ®é›†æ¥åŒ…å«ç”±å­é—®é¢˜å’Œç­”æ¡ˆå¯¹ç»„æˆçš„æ¨ç†é“¾ï¼Œå¹¶ç”¨å¤šä»»åŠ¡æŸå¤±æ¥è®­ç»ƒLVLMï¼Œé¼“åŠ±ç”Ÿæˆå’Œå›ç­”è¿™äº›ä¸­é—´æ­¥éª¤ï¼Œä»¥åŠé¢„æµ‹æœ€ç»ˆç­”æ¡ˆã€‚æˆ‘ä»¬åœ¨ScienceQAå’ŒVQAv2æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼Œç»“æœè¡¨æ˜MF-SQ-LLaVAæ˜¾è‘—ä¼˜äºåŒ…æ‹¬åŸºç¡€LLaVAå’ŒåŸå§‹SQ-LLaVAåœ¨å†…çš„ç°æœ‰æœ€å…ˆè¿›çš„æ¨¡å‹ã€‚æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥éªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ä¸­æ¯ä¸ªç»„ä»¶çš„è´¡çŒ®ï¼Œäººç±»è¯„ä¼°ä¹Ÿè¯å®äº†æˆ‘ä»¬æ–¹æ³•å¯ç”¨çš„æ¨ç†è¿‡ç¨‹çš„å‡†ç¡®æ€§å’Œè¿è´¯æ€§æœ‰æ‰€æé«˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14674v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨éœ€è¦å¤šæ­¥éª¤æ¨ç†çš„å¤æ‚è§†è§‰æ¨ç†æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚ä¸ºè§£å†³æ­¤å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†MF-SQ-LLaVAè¿™ä¸€æ–°æ–¹æ³•ï¼Œå®ƒé€šè¿‡ç«¯åˆ°ç«¯çš„è®­ç»ƒæ¥å¢å¼ºLVLMsï¼Œå®ç°éšå¼è‡ªæˆ‘æé—®ã€‚è¯¥æ–¹æ³•é€šè¿‡æ‰©å……è§†è§‰é—®ç­”æ•°æ®é›†åŒ…å«æ¨ç†é“¾ï¼ˆç”±å­é—®é¢˜å¯¹å’Œç­”æ¡ˆå¯¹ç»„æˆï¼‰ï¼Œå¹¶ç”¨å¤šä»»åŠ¡æŸå¤±è®­ç»ƒLVLMï¼Œé¼“åŠ±ç”Ÿæˆå’Œå›ç­”è¿™äº›ä¸­é—´æ­¥éª¤ï¼Œä»¥åŠé¢„æµ‹æœ€ç»ˆç­”æ¡ˆã€‚åœ¨ScienceQAå’ŒVQAv2æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMF-SQ-LLaVAæ˜¾è‘—ä¼˜äºåŒ…æ‹¬åŸºç¡€LLaVAå’ŒåŸå§‹SQ-LLaVAåœ¨å†…çš„ç°æœ‰æœ€å…ˆè¿›çš„æ¨¡å‹ã€‚æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥éªŒè¯äº†è¯¥æ–¹æ³•æ¯ä¸ªç»„ä»¶çš„è´¡çŒ®ï¼Œäººç±»è¯„ä¼°ä¹Ÿè¯å®äº†æˆ‘ä»¬çš„æ–¹æ³•æ‰€å¯ç”¨çš„æ¨ç†è¿‡ç¨‹çš„å‡†ç¡®æ€§å’Œè¿è´¯æ€§çš„æé«˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤æ‚è§†è§‰æ¨ç†æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>MF-SQ-LLaVAæ˜¯ä¸€ç§å¢å¼ºLVLMsçš„æ–¹æ³•ï¼Œé€šè¿‡éšå¼è‡ªæˆ‘æé—®å’Œç«¯åˆ°ç«¯çš„è®­ç»ƒæ¥å®ç°ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡æ‰©å……è§†è§‰é—®ç­”æ•°æ®é›†åŒ…å«æ¨ç†é“¾ï¼Œå¹¶è®­ç»ƒæ¨¡å‹ä»¥ç”Ÿæˆå’Œå›ç­”ä¸­é—´æ­¥éª¤åŠé¢„æµ‹æœ€ç»ˆç­”æ¡ˆã€‚</li>
<li>MF-SQ-LLaVAåœ¨ScienceQAå’ŒVQAv2æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨ç°ä¼˜äºå…¶ä»–å…ˆè¿›æ¨¡å‹ã€‚</li>
<li>æ¶ˆèç ”ç©¶è¯å®äº†è¯¥æ–¹æ³•æ¯ä¸ªç»„ä»¶çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>äººç±»è¯„ä¼°æ˜¾ç¤ºï¼ŒMF-SQ-LLaVAæé«˜äº†æ¨ç†è¿‡ç¨‹çš„å‡†ç¡®æ€§å’Œè¿è´¯æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14674">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-db66f56caba2ac7341bff484f87b4eda.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="A-Generalist-Hanabi-Agent"><a href="#A-Generalist-Hanabi-Agent" class="headerlink" title="A Generalist Hanabi Agent"></a>A Generalist Hanabi Agent</h2><p><strong>Authors:Arjun V Sudhakar, Hadi Nekoei, Mathieu Reymond, Miao Liu, Janarthanan Rajendran, Sarath Chandar</strong></p>
<p>Traditional multi-agent reinforcement learning (MARL) systems can develop cooperative strategies through repeated interactions. However, these systems are unable to perform well on any other setting than the one they have been trained on, and struggle to successfully cooperate with unfamiliar collaborators. This is particularly visible in the Hanabi benchmark, a popular 2-to-5 player cooperative card-game which requires complex reasoning and precise assistance to other agents. Current MARL agents for Hanabi can only learn one specific game-setting (e.g., 2-player games), and play with the same algorithmic agents. This is in stark contrast to humans, who can quickly adjust their strategies to work with unfamiliar partners or situations. In this paper, we introduce Recurrent Replay Relevance Distributed DQN (R3D2), a generalist agent for Hanabi, designed to overcome these limitations. We reformulate the task using text, as language has been shown to improve transfer. We then propose a distributed MARL algorithm that copes with the resulting dynamic observation- and action-space. In doing so, our agent is the first that can play all game settings concurrently, and extend strategies learned from one setting to other ones. As a consequence, our agent also demonstrates the ability to collaborate with different algorithmic agents â€“ agents that are themselves unable to do so. The implementation code is available at: $\href{<a target="_blank" rel="noopener" href="https://github.com/chandar-lab/R3D2-A-Generalist-Hanabi-Agent%7D%7BR3D2-A-Generalist-Hanabi-Agent%7D$">https://github.com/chandar-lab/R3D2-A-Generalist-Hanabi-Agent}{R3D2-A-Generalist-Hanabi-Agent}$</a> </p>
<blockquote>
<p>ä¼ ç»Ÿå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰ç³»ç»Ÿå¯ä»¥é€šè¿‡åå¤äº¤äº’æ¥å¼€å‘åˆä½œç­–ç•¥ã€‚ç„¶è€Œï¼Œè¿™äº›ç³»ç»Ÿåªèƒ½åœ¨è®­ç»ƒè¿‡çš„åœºæ™¯ä¸Šè¡¨ç°è‰¯å¥½ï¼Œåœ¨ä¸ä¸ç†Ÿæ‚‰çš„åˆä½œä¼™ä¼´è¿›è¡Œåä½œæ—¶åˆ™è¡¨ç°ä¸ä½³ã€‚è¿™åœ¨æµè¡Œçš„å°å‹å¤šäººåœ¨çº¿å¡ç‰Œæ¸¸æˆHanabiçš„åŸºå‡†æµ‹è¯•ä¸­å°¤ä¸ºæ˜æ˜¾ã€‚Hanabiéœ€è¦å¤æ‚çš„æ¨ç†å’Œç²¾ç¡®çš„è¾…åŠ©å…¶ä»–æ™ºèƒ½ä½“æ¥å®Œæˆæ¸¸æˆã€‚å½“å‰é’ˆå¯¹Hanabiçš„MARLæ™ºèƒ½ä½“åªèƒ½å­¦ä¹ ä¸€ç§ç‰¹å®šçš„æ¸¸æˆè®¾ç½®ï¼ˆä¾‹å¦‚ï¼ŒåŒç©å®¶æ¸¸æˆï¼‰ï¼Œå¹¶ä¸”åªèƒ½ä¸ç›¸åŒçš„ç®—æ³•æ™ºèƒ½ä½“è¿›è¡Œæ¸¸æˆã€‚ä¸æ­¤å½¢æˆé²œæ˜å¯¹æ¯”çš„æ˜¯äººç±»ï¼Œä»–ä»¬å¯ä»¥è¿…é€Ÿè°ƒæ•´ç­–ç•¥ä»¥é€‚åº”ä¸é™Œç”Ÿä¼™ä¼´æˆ–ä¸åŒæƒ…å¢ƒçš„åˆä½œã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†é’ˆå¯¹Hanabiçš„é€šç”¨æ™ºèƒ½ä½“Recurrent Replay Relevance Distributed DQNï¼ˆR3D2ï¼‰ï¼Œæ—¨åœ¨å…‹æœè¿™äº›å±€é™æ€§ã€‚æˆ‘ä»¬ä½¿ç”¨æ–‡æœ¬é‡æ–°æ„å»ºä»»åŠ¡ï¼Œå› ä¸ºè¯­è¨€å·²è¢«è¯æ˜å¯ä»¥æé«˜è¿ç§»èƒ½åŠ›ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ†å¸ƒå¼MARLç®—æ³•ï¼Œä»¥åº”å¯¹ç”±æ­¤äº§ç”Ÿçš„åŠ¨æ€è§‚æµ‹ç©ºé—´å’ŒåŠ¨ä½œç©ºé—´ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„æ™ºèƒ½ä½“èƒ½å¤ŸåŒæ—¶åº”å¯¹æ‰€æœ‰æ¸¸æˆè®¾ç½®ï¼Œå¹¶å°†ä»ä¸€ä¸ªè®¾ç½®ä¸­å­¦ä¹ çš„ç­–ç•¥æ‰©å±•åˆ°å…¶ä»–è®¾ç½®ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„æ™ºèƒ½ä½“è¿˜æ˜¾ç¤ºå‡ºä¸ä¸åŒç®—æ³•æ™ºèƒ½ä½“åä½œçš„èƒ½åŠ›â€”â€”è¿™äº›æ™ºèƒ½ä½“æœ¬èº«å´æ— æ³•åšåˆ°è¿™ä¸€ç‚¹ã€‚å®ç°ä»£ç å¯è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/chandar-lab/R3D2-A-Generalist-Hanabi-Agent">https://github.com/chandar-lab/R3D2-A-Generalist-Hanabi-Agent</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14555v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>ä¼ ç»Ÿå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰ç³»ç»Ÿèƒ½å¤Ÿé€šè¿‡åå¤äº¤äº’å‘å±•åˆä½œç­–ç•¥ï¼Œä½†åœ¨é¢å¯¹éè®­ç»ƒç¯å¢ƒæˆ–ä¸é™Œç”Ÿæ™ºèƒ½ä½“åˆä½œæ—¶è¡¨ç°ä¸ä½³ã€‚åœ¨éœ€è¦å¤æ‚æ¨ç†å’Œç²¾ç¡®ååŠ©çš„2è‡³5äººåˆä½œå¡ç‰Œæ¸¸æˆHanabiä¸­å°¤ä¸ºæ˜æ˜¾ã€‚å½“å‰é’ˆå¯¹Hanabiçš„MARLæ™ºèƒ½ä½“åªèƒ½å­¦ä¹ ä¸€ç§ç‰¹å®šæ¸¸æˆè®¾ç½®ï¼ˆå¦‚2äººæ¸¸æˆï¼‰ï¼Œå¹¶ä¸ç›¸åŒç®—æ³•çš„æ™ºèƒ½ä½“è¿›è¡Œæ¸¸æˆã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é’ˆå¯¹Hanabiçš„é€šç”¨æ™ºèƒ½ä½“R3D2ï¼Œæ—¨åœ¨å…‹æœè¿™äº›é™åˆ¶ã€‚é€šè¿‡æ”¹é©ä»»åŠ¡å¹¶ä½¿ç”¨æ–‡æœ¬ï¼Œæˆ‘ä»¬çš„æ™ºèƒ½ä½“æˆä¸ºé¦–ä¸ªå¯ä»¥åŒæ—¶è¿›è¡Œæ‰€æœ‰æ¸¸æˆè®¾ç½®å¹¶æ‰©å±•ä»ä¸€ä¸ªè®¾ç½®ä¸­å­¦åˆ°çš„ç­–ç•¥åˆ°å…¶ä»–è®¾ç½®çš„æ™ºèƒ½ä½“ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„æ™ºèƒ½ä½“è¿˜æ˜¾ç¤ºå‡ºä¸ä¸åŒç®—æ³•çš„æ™ºèƒ½ä½“åä½œçš„èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ä¼ ç»ŸMARLç³»ç»Ÿåœ¨éè®­ç»ƒç¯å¢ƒæˆ–é¢å¯¹é™Œç”Ÿæ™ºèƒ½ä½“æ—¶åˆä½œèƒ½åŠ›å—é™ã€‚</li>
<li>Hanabiæ¸¸æˆä¸­çš„å½“å‰MARLæ™ºèƒ½ä½“åªèƒ½é€‚åº”ç‰¹å®šæ¸¸æˆè®¾ç½®ï¼Œæ— æ³•ä¸é™Œç”Ÿæ™ºèƒ½ä½“åˆä½œã€‚</li>
<li>æ–°æå‡ºçš„R3D2æ™ºèƒ½ä½“æ—¨åœ¨å…‹æœè¿™äº›é™åˆ¶ï¼Œæˆä¸ºé¦–ä¸ªèƒ½åŒæ—¶è¿›è¡Œå¤šç§æ¸¸æˆè®¾ç½®çš„æ™ºèƒ½ä½“ã€‚</li>
<li>R3D2é€šè¿‡æ”¹é©ä»»åŠ¡å¹¶ä½¿ç”¨æ–‡æœ¬å®ç°è¿™ä¸€ç‚¹ï¼Œè¿™æé«˜äº†å…¶è·¨æƒ…å¢ƒçš„è½¬ç§»èƒ½åŠ›ã€‚</li>
<li>R3D2èƒ½å¤Ÿä»ä¸€ç§æ¸¸æˆè®¾ç½®ä¸­å­¦ä¹ ç­–ç•¥å¹¶å°†å…¶åº”ç”¨äºå…¶ä»–è®¾ç½®ã€‚</li>
<li>R3D2å±•ç¤ºäº†ä¸ä¸åŒç®—æ³•çš„æ™ºèƒ½ä½“åä½œçš„èƒ½åŠ›ï¼Œè¿™åœ¨ä»¥å‰æ˜¯æ— æ³•å®ç°çš„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14555">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2bb12b801b74ebb5e67958ef1361ce7a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a916103dd74f67cdedb473d1782be01a.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Synchronous-vs-Asynchronous-Reinforcement-Learning-in-a-Real-World-Robot"><a href="#Synchronous-vs-Asynchronous-Reinforcement-Learning-in-a-Real-World-Robot" class="headerlink" title="Synchronous vs Asynchronous Reinforcement Learning in a Real World Robot"></a>Synchronous vs Asynchronous Reinforcement Learning in a Real World Robot</h2><p><strong>Authors:Ali Parsaee, Fahim Shahriar, Chuxin He, Ruiqing Tan</strong></p>
<p>In recent times, reinforcement learning (RL) with physical robots has attracted the attention of a wide range of researchers. However, state-of-the-art RL algorithms do not consider that physical environments do not wait for the RL agent to make decisions or updates. RL agents learn by periodically conducting computationally expensive gradient updates. When decision-making and gradient update tasks are carried out sequentially by the RL agent in a physical robot, it significantly increases the agentâ€™s response time. In a rapidly changing environment, this increased response time may be detrimental to the performance of the learning agent. Asynchronous RL methods, which separate the computation of decision-making and gradient updates, are a potential solution to this problem. However, only a few comparisons between asynchronous and synchronous RL have been made with physical robots. For this reason, the exact performance benefits of using asynchronous RL methods over synchronous RL methods are still unclear. In this study, we provide a performance comparison between asynchronous and synchronous RL using a physical robotic arm called Franka Emika Panda. Our experiments show that the agents learn faster and attain significantly more returns using asynchronous RL. Our experiments also demonstrate that the learning agent with a faster response time performs better than the agent with a slower response time, even if the agent with a slower response time performs a higher number of gradient updates. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œä½¿ç”¨å®ä½“æœºå™¨äººè¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·²ç»å¸å¼•äº†å¹¿å¤§ç ”ç©¶äººå‘˜çš„å…³æ³¨ã€‚ç„¶è€Œï¼Œæœ€å…ˆè¿›çš„RLç®—æ³•å¹¶æ²¡æœ‰è€ƒè™‘åˆ°ç‰©ç†ç¯å¢ƒä¸ä¼šç­‰å¾…RLä»£ç†è¿›è¡Œå†³ç­–æˆ–æ›´æ–°ã€‚RLä»£ç†é€šè¿‡å®šæœŸæ‰§è¡Œè®¡ç®—é‡å¤§çš„æ¢¯åº¦æ›´æ–°æ¥å­¦ä¹ ã€‚å½“RLä»£ç†åœ¨ç‰©ç†æœºå™¨äººä¸Šé¡ºåºæ‰§è¡Œå†³ç­–å’Œæ¢¯åº¦æ›´æ–°ä»»åŠ¡æ—¶ï¼Œä¼šæ˜¾è‘—å¢åŠ ä»£ç†çš„ååº”æ—¶é—´ã€‚åœ¨å¿«é€Ÿå˜åŒ–çš„ç¯å¢ƒä¸­ï¼Œè¿™ç§å¢åŠ çš„ååº”æ—¶é—´å¯èƒ½ä¼šå¯¹å­¦ä¹ ä»£ç†çš„æ€§èƒ½äº§ç”Ÿä¸åˆ©å½±å“ã€‚å¼‚æ­¥RLæ–¹æ³•å°†å†³ç­–å’Œæ¢¯åº¦æ›´æ–°çš„è®¡ç®—åˆ†å¼€ï¼Œå¯èƒ½æ˜¯è§£å†³è¿™ä¸ªé—®é¢˜çš„æ½œåœ¨æ–¹æ³•ã€‚ç„¶è€Œï¼Œåªæœ‰å°‘æ•°ç ”ç©¶å¯¹å¼‚æ­¥å’ŒåŒæ­¥RLåœ¨å®ä½“æœºå™¨äººä¸Šè¿›è¡Œäº†æ¯”è¾ƒã€‚å› æ­¤ï¼Œä½¿ç”¨å¼‚æ­¥RLæ–¹æ³•ç›¸å¯¹äºåŒæ­¥RLæ–¹æ³•çš„æ€§èƒ½ä¼˜åŠ¿å°šä¸æ¸…æ¥šã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨åä¸ºFanka Emika Pandaçš„å®ä½“æœºæ¢°è‡‚å¯¹å¼‚æ­¥å’ŒåŒæ­¥RLè¿›è¡Œäº†æ€§èƒ½æ¯”è¾ƒã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œä»£ç†ä½¿ç”¨å¼‚æ­¥RLå­¦ä¹ å¾—æ›´å¿«ï¼Œå¹¶ä¸”è·å¾—æ›´é«˜çš„å›æŠ¥ã€‚æˆ‘ä»¬çš„å®éªŒè¿˜è¡¨æ˜ï¼Œååº”æ—¶é—´è¾ƒå¿«çš„ä»£ç†æ¯”ååº”æ—¶é—´è¾ƒæ…¢çš„ä»£ç†è¡¨ç°æ›´å¥½ï¼Œå³ä½¿ååº”æ—¶é—´è¾ƒæ…¢çš„ä»£ç†æ‰§è¡Œäº†æ›´å¤šçš„æ¢¯åº¦æ›´æ–°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14554v1">PDF</a> Presented at Alberta Robotics &amp; Intelligent Systems Expo (RISE)   Conference</p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸å®ä½“æœºå™¨äººçš„ç»“åˆå¸å¼•äº†ä¼—å¤šç ”ç©¶è€…çš„å…³æ³¨ã€‚ç„¶è€Œï¼Œå½“å‰ä¸»æµçš„RLç®—æ³•æœªè€ƒè™‘åˆ°ç‰©ç†ç¯å¢ƒä¸ä¼šç­‰å¾…RLä»£ç†è¿›è¡Œå†³ç­–æˆ–æ›´æ–°ã€‚RLä»£ç†é€šè¿‡å‘¨æœŸæ€§çš„è®¡ç®—æ˜‚è´µçš„æ¢¯åº¦æ›´æ–°è¿›è¡Œå­¦ä¹ ã€‚å½“å†³ç­–åˆ¶å®šå’Œæ¢¯åº¦æ›´æ–°ä»»åŠ¡ç”±å®ä½“æœºå™¨äººä¸­çš„RLä»£ç†é¡ºåºæ‰§è¡Œæ—¶ï¼Œä¼šæ˜¾è‘—å¢åŠ ä»£ç†çš„å“åº”æ—¶é—´ã€‚åœ¨å¿«é€Ÿå˜åŒ–çš„ç¯å¢ƒä¸­ï¼Œè¿™ç§å¢åŠ çš„å“åº”æ—¶é—´å¯èƒ½ä¼šå¯¹å­¦ä¹ ä»£ç†çš„æ€§èƒ½äº§ç”Ÿä¸åˆ©å½±å“ã€‚å¼‚æ­¥RLæ–¹æ³•å°†å†³ç­–åˆ¶å®šå’Œæ¢¯åº¦æ›´æ–°çš„è®¡ç®—åˆ†å¼€ï¼Œæ˜¯è§£å†³è¿™ä¸ªé—®é¢˜çš„ä¸€ç§æ½œåœ¨æ–¹æ³•ã€‚ç„¶è€Œï¼Œå…³äºå¼‚æ­¥RLä¸åŒæ­¥RLåœ¨å®ä½“æœºå™¨äººä¸Šçš„æ¯”è¾ƒä»è¾ƒå°‘ã€‚æœ¬ç ”ç©¶é€šè¿‡å®ä½“æœºå™¨äººæ‰‹è‡‚Franka Emika Pandaå¯¹æ¯”äº†å¼‚æ­¥RLå’ŒåŒæ­¥RLçš„æ€§èƒ½è¡¨ç°ã€‚å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨å¼‚æ­¥RLçš„ä»£ç†å­¦ä¹ é€Ÿåº¦æ›´å¿«ï¼Œæ”¶ç›Šæ›´é«˜ã€‚æ­¤å¤–ï¼Œå³ä½¿å“åº”ç¼“æ…¢çš„ä»£ç†æ‰§è¡Œäº†æ›´å¤šçš„æ¢¯åº¦æ›´æ–°ï¼Œå…·æœ‰å¿«é€Ÿå“åº”æ—¶é—´çš„ä»£ç†è¡¨ç°ä¹Ÿæ›´å¥½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸ç‰©ç†æœºå™¨äººç»“åˆå—åˆ°å¹¿æ³›å…³æ³¨ã€‚</li>
<li>å½“å‰RLç®—æ³•æœªå……åˆ†è€ƒè™‘åˆ°ç‰©ç†ç¯å¢ƒçš„å®æ—¶æ€§ã€‚</li>
<li>å®ä½“æœºå™¨äººåœ¨æ‰§è¡Œå†³ç­–å’Œæ¢¯åº¦æ›´æ–°ä»»åŠ¡æ—¶ï¼Œé¡ºåºæ‰§è¡Œä¼šæ˜¾è‘—å¢åŠ å“åº”æ—¶é—´ã€‚</li>
<li>å¼‚æ­¥RLæ–¹æ³•æ˜¯ä¸€ç§è§£å†³æ­¤é—®é¢˜çš„æ½œåœ¨æ–¹æ¡ˆã€‚</li>
<li>å…³äºå¼‚æ­¥ä¸åŒæ­¥RLåœ¨å®ä½“æœºå™¨äººä¸Šçš„å¯¹æ¯”ç ”ç©¶ä»ä¸è¶³ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºå¼‚æ­¥RLçš„ä»£ç†å­¦ä¹ é€Ÿåº¦æ›´å¿«ï¼Œæ”¶ç›Šæ›´é«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14554">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fdb6de8d04d286cacc8c735ced447197.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4dcfaad12a89fa4811d6da9f3b29b726.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d95ba9300013eba86c6f1d1f027264ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d1e4d32d2cfc14c08b064f1045a21e1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d90b1eb2777d2842ff77f72415e5147b.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Towards-Hierarchical-Multi-Step-Reward-Models-for-Enhanced-Reasoning-in-Large-Language-Models"><a href="#Towards-Hierarchical-Multi-Step-Reward-Models-for-Enhanced-Reasoning-in-Large-Language-Models" class="headerlink" title="Towards Hierarchical Multi-Step Reward Models for Enhanced Reasoning in   Large Language Models"></a>Towards Hierarchical Multi-Step Reward Models for Enhanced Reasoning in   Large Language Models</h2><p><strong>Authors:Teng Wang, Zhangyi Jiang, Zhenqi He, Wenhan Yang, Yanan Zheng, Zeyu Li, Zifan He, Shenyang Tong, Hailei Gong</strong></p>
<p>Recent studies show that Large Language Models (LLMs) achieve strong reasoning capabilities through supervised fine-tuning or reinforcement learning. However, a key approach, the Process Reward Model (PRM), suffers from reward hacking, making it unreliable in identifying the best intermediate steps. In this paper, we propose a novel reward model approach, Hierarchical Reward Model (HRM), which evaluates both individual and consecutive reasoning steps from fine-grained and coarse-grained level. HRM performs better in assessing reasoning coherence and self-reflection, particularly when the previous reasoning step is incorrect. Furthermore, to address the inefficiency of autonomous generating PRM training data via Monte Carlo Tree Search (MCTS), we introduce a lightweight and effective data augmentation strategy called Hierarchical Node Compression (HNC) based on node merging (combining two consecutive reasoning steps into one step) in the tree structure. This approach diversifies MCTS results for HRM with negligible computational overhead, enhancing label robustness by introducing noise. Empirical results on the PRM800K dataset demonstrate that HRM, in conjunction with HNC, achieves superior stability and reliability in evaluation compared to PRM. Furthermore, cross-domain evaluations on MATH500 and GSM8K confirm HRMâ€™s superior generalization and robustness across diverse reasoning tasks. The code for all experiments will be released at https: &#x2F;&#x2F;github.com&#x2F;tengwang0318&#x2F;hierarchial_reward_model. </p>
<blockquote>
<p>æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡ç›‘ç£å¾®è°ƒæˆ–å¼ºåŒ–å­¦ä¹ è·å¾—äº†å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œä¸€ç§ä¸»è¦æ–¹æ³•â€”â€”è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰å­˜åœ¨å¥–åŠ±é»‘å®¢æ”»å‡»çš„é—®é¢˜ï¼Œä½¿å…¶åœ¨è¯†åˆ«æœ€ä½³ä¸­é—´æ­¥éª¤æ—¶å˜å¾—ä¸å¯é ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å¥–åŠ±æ¨¡å‹æ–¹æ³•ï¼Œå³åˆ†å±‚å¥–åŠ±æ¨¡å‹ï¼ˆHRMï¼‰ï¼Œå®ƒå¯ä»¥ä»ç²¾ç»†ç²’åº¦å’Œç²—ç•¥ç²’åº¦ä¸¤ä¸ªå±‚é¢è¯„ä¼°å•ä¸ªå’Œè¿ç»­çš„æ¨ç†æ­¥éª¤ã€‚HRMåœ¨è¯„ä¼°æ¨ç†è¿è´¯æ€§å’Œè‡ªæˆ‘åæ€æ–¹é¢è¡¨ç°æ›´å¥½ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¹‹å‰çš„æ¨ç†æ­¥éª¤å‡ºé”™æ—¶ã€‚æ­¤å¤–ï¼Œä¸ºäº†è§£å†³é€šè¿‡è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰è‡ªä¸»ç”ŸæˆPRMè®­ç»ƒæ•°æ®çš„ä¸æ•ˆç‡é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºèŠ‚ç‚¹åˆå¹¶ï¼ˆå°†ä¸¤ä¸ªè¿ç»­çš„æ¨ç†æ­¥éª¤åˆå¹¶ä¸ºä¸€ä¸ªæ­¥éª¤ï¼‰çš„æ ‘ç»“æ„ä¸­çš„åˆ†å±‚èŠ‚ç‚¹å‹ç¼©ï¼ˆHNCï¼‰çš„è½»é‡çº§æœ‰æ•ˆæ•°æ®å¢å¼ºç­–ç•¥ã€‚è¿™ç§æ–¹æ³•é€šè¿‡å¼•å…¥å™ªå£°å¢å¼ºäº†æ ‡ç­¾çš„é²æ£’æ€§ï¼Œä»¥å¾®å°çš„è®¡ç®—å¼€é”€å®ç°äº†å¯¹HRMçš„MCTSç»“æœçš„å¤šæ ·åŒ–ã€‚åœ¨PRM800Kæ•°æ®é›†ä¸Šçš„å®è¯ç»“æœè¡¨æ˜ï¼Œä¸HNCç»“åˆçš„HRMåœ¨è¯„ä¼°ä¸­å®ç°äº†æ¯”PRMæ›´ä¼˜è¶Šçš„ç¨³å®šæ€§å’Œå¯é æ€§ã€‚æ­¤å¤–ï¼ŒMATH500å’ŒGSM8Kçš„è·¨åŸŸè¯„ä¼°è¯å®äº†HRMåœ¨å¤šç§æ¨ç†ä»»åŠ¡ä¸­çš„ä¼˜å¼‚æ³›åŒ–å’Œé²æ£’æ€§ã€‚æ‰€æœ‰å®éªŒçš„ä»£ç å°†å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/tengwang0318/hierarchial_reward_model%E3%80%82">https://github.com/tengwang0318/hierarchial_reward_modelã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13551v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç›‘ç£å¾®è°ƒæˆ–å¼ºåŒ–å­¦ä¹ ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰å­˜åœ¨å¥–åŠ±é»‘å®¢é—®é¢˜ï¼Œæ— æ³•å¯é åœ°è¯†åˆ«æœ€ä½³ä¸­é—´æ­¥éª¤ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¥–åŠ±æ¨¡å‹æ–¹æ³•â€”â€”åˆ†å±‚å¥–åŠ±æ¨¡å‹ï¼ˆHRMï¼‰ï¼Œå¯ä»¥ä»ç²¾ç»†ç²’åº¦å’Œç²—ç•¥ç²’åº¦çº§åˆ«è¯„ä¼°å•ä¸ªå’Œè¿ç»­çš„æ¨ç†æ­¥éª¤ã€‚HRMåœ¨è¯„ä¼°æ¨ç†è¿è´¯æ€§å’Œè‡ªæˆ‘åæ€æ–¹é¢è¡¨ç°æ›´ä½³ï¼Œå°¤å…¶åœ¨ä¹‹å‰çš„æ¨ç†æ­¥éª¤å‡ºé”™æ—¶ã€‚æ­¤å¤–ï¼Œä¸ºè§£å†³é€šè¿‡è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰è‡ªä¸»ç”ŸæˆPRMè®­ç»ƒæ•°æ®çš„ä¸æ•ˆç‡é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºèŠ‚ç‚¹åˆå¹¶ï¼ˆå°†ä¸¤ä¸ªè¿ç»­çš„æ¨ç†æ­¥éª¤åˆå¹¶ä¸ºä¸€ä¸ªæ­¥éª¤ï¼‰çš„åˆ†å±‚èŠ‚ç‚¹å‹ç¼©ï¼ˆHNCï¼‰çš„è½»é‡çº§æœ‰æ•ˆæ•°æ®å¢å¼ºç­–ç•¥ã€‚è¯¥ç­–ç•¥ä½¿MCTSç»“æœå¤šæ ·åŒ–ï¼Œå¼•å…¥å™ªå£°ä»¥å¢å¼ºæ ‡ç­¾ç¨³å¥æ€§ï¼Œä¸”è®¡ç®—å¼€é”€è¾ƒå°ã€‚åœ¨PRM800Kæ•°æ®é›†ä¸Šçš„å®è¯ç»“æœè¡¨æ˜ï¼Œä¸PRMç›¸æ¯”ï¼ŒHRMç»“åˆHNCåœ¨è¯„ä¼°ä¸­å®ç°äº†æ›´ä¼˜è¶Šçš„ç¨³å®šæ€§ä¸å¯é æ€§ã€‚è·¨åŸŸè¯„ä¼°MATH500å’ŒGSM8Kè¿›ä¸€æ­¥è¯å®äº†HRMåœ¨å¤šç§æ¨ç†ä»»åŠ¡ä¸­çš„ä¼˜å¼‚æ³›åŒ–å’Œç¨³å¥æ€§ã€‚æ‰€æœ‰å®éªŒçš„ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/tengwang0318/hierarchial_reward_model%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/tengwang0318/hierarchial_reward_modelå‘å¸ƒã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šè¿‡ç›‘ç£å¾®è°ƒæˆ–å¼ºåŒ–å­¦ä¹ å±•ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰å­˜åœ¨å¥–åŠ±é»‘å®¢é—®é¢˜ï¼Œéš¾ä»¥è¯†åˆ«æœ€ä½³ä¸­é—´æ­¥éª¤ã€‚</li>
<li>åˆ†å±‚å¥–åŠ±æ¨¡å‹ï¼ˆHRMï¼‰èƒ½å¤Ÿè¯„ä¼°å•ä¸ªå’Œè¿ç»­çš„æ¨ç†æ­¥éª¤ï¼Œæé«˜è¯„ä¼°æ¨ç†è¿è´¯æ€§å’Œè‡ªæˆ‘åæ€çš„èƒ½åŠ›ã€‚</li>
<li>HRMåœ¨ä¹‹å‰çš„æ¨ç†æ­¥éª¤å‡ºé”™æ—¶è¡¨ç°æ›´ä¼˜ã€‚</li>
<li>å¼•å…¥åˆ†å±‚èŠ‚ç‚¹å‹ç¼©ï¼ˆHNCï¼‰ç­–ç•¥ï¼ŒåŸºäºèŠ‚ç‚¹åˆå¹¶ï¼Œä»¥æé«˜è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰ç”ŸæˆPRMè®­ç»ƒæ•°æ®æ•ˆç‡ã€‚</li>
<li>HRMç»“åˆHNCåœ¨PRM800Kæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºPRMï¼Œå®ç°æ›´ç¨³å®šçš„è¯„ä¼°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13551">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4d85bcbdd467aaf3fe9c325a6c42271b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b5f22a5e742f32be421290498f419e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-537623a767707642cd27dfd8fa03abb2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0999880d92a1a774a606f2f405e64e85.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-29cdb0559b93fc2ef326dac3429acc69.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3bc3660fa1902121dc2185dc75be109.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="EXAONE-Deep-Reasoning-Enhanced-Language-Models"><a href="#EXAONE-Deep-Reasoning-Enhanced-Language-Models" class="headerlink" title="EXAONE Deep: Reasoning Enhanced Language Models"></a>EXAONE Deep: Reasoning Enhanced Language Models</h2><p><strong>Authors:LG AI Research, Kyunghoon Bae, Eunbi Choi, Kibong Choi, Stanley Jungkyu Choi, Yemuk Choi, Seokhee Hong, Junwon Hwang, Hyojin Jeon, Kijeong Jeon, Gerrard Jeongwon Jo, Hyunjik Jo, Jiyeon Jung, Hyosang Kim, Joonkee Kim, Seonghwan Kim, Soyeon Kim, Sunkyoung Kim, Yireun Kim, Yongil Kim, Youchul Kim, Edward Hwayoung Lee, Haeju Lee, Honglak Lee, Jinsik Lee, Kyungmin Lee, Sangha Park, Yongmin Park, Sihoon Yang, Heuiyeen Yeen, Sihyuk Yi, Hyeongu Yun</strong></p>
<p>We present EXAONE Deep series, which exhibits superior capabilities in various reasoning tasks, including math and coding benchmarks. We train our models mainly on the reasoning-specialized dataset that incorporates long streams of thought processes. Evaluation results show that our smaller models, EXAONE Deep 2.4B and 7.8B, outperform other models of comparable size, while the largest model, EXAONE Deep 32B, demonstrates competitive performance against leading open-weight models. All EXAONE Deep models are openly available for research purposes and can be downloaded from <a target="_blank" rel="noopener" href="https://huggingface.co/LGAI-EXAONE">https://huggingface.co/LGAI-EXAONE</a> </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºEXAONE Deepç³»åˆ—ï¼Œè¯¥ç³»åˆ—åœ¨å„ç§æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬æ•°å­¦å’Œç¼–ç¨‹åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬çš„æ¨¡å‹ä¸»è¦åœ¨æœ‰å¤§é‡æ€è€ƒè¿‡ç¨‹çš„æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä»¥å¼ºåŒ–æ¨ç†èƒ½åŠ›ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„å°å‹æ¨¡å‹EXAONE Deep 2.4Bå’Œ7.8Båœ¨åŒç±»æ¨¡å‹ä¸­è¡¨ç°çªå‡ºï¼Œè€Œæœ€å¤§çš„æ¨¡å‹EXAONE Deep 32Båˆ™ä¸é¢†å…ˆçš„å…¬å¼€æƒé‡æ¨¡å‹è¡¨ç°å‡ºç«äº‰åŠ›ã€‚æ‰€æœ‰EXAONE Deepæ¨¡å‹å‡å…¬å¼€ç”¨äºç ”ç©¶ç›®çš„ï¼Œå¯ä»<a target="_blank" rel="noopener" href="https://huggingface.co/LGAI-EXAONE%E4%B8%8B%E8%BD%BD%E3%80%82">https://huggingface.co/LGAI-EXAONEä¸‹è½½ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12524v2">PDF</a> arXiv admin note: substantial text overlap with arXiv:2412.04862,   arXiv:2408.03541</p>
<p><strong>Summary</strong></p>
<p>EXAONE Deepç³»åˆ—æ¨¡å‹åœ¨å„ç§æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬æ•°å­¦å’Œç¼–ç åŸºå‡†æµ‹è¯•ã€‚æ¨¡å‹ä¸»è¦åœ¨åŒ…å«é•¿æµæ€ç»´è¿‡ç¨‹çš„æ¨ç†ä¸“ç”¨æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œè¾ƒå°çš„EXAONE Deep 2.4Bå’Œ7.8Bæ¨¡å‹åœ¨åŒç±»æ¨¡å‹ä¸­è¡¨ç°çªå‡ºï¼Œè€Œæœ€å¤§çš„32Bæ¨¡å‹åˆ™åœ¨é¢†å…ˆçš„å¼€æ”¾æƒé‡æ¨¡å‹ä¸­è¡¨ç°å‡ºç«äº‰åŠ›ã€‚æ‰€æœ‰EXAONE Deepæ¨¡å‹å‡å…¬å¼€ç”¨äºç ”ç©¶ç›®çš„ï¼Œå¯ä»<a target="_blank" rel="noopener" href="https://huggingface.co/LGAI-EXAONE%E4%B8%8B%E8%BD%BD%E3%80%82">https://huggingface.co/LGAI-EXAONEä¸‹è½½ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>EXAONE Deepç³»åˆ—æ¨¡å‹åœ¨å¤šç§æ¨ç†ä»»åŠ¡ä¸­å±•ç°å“è¶Šæ€§èƒ½ã€‚</li>
<li>æ¨¡å‹è®­ç»ƒä¸»è¦åŸºäºåŒ…å«é•¿æµæ€ç»´è¿‡ç¨‹çš„æ¨ç†ä¸“ç”¨æ•°æ®é›†ã€‚</li>
<li>è¾ƒå°è§„æ¨¡çš„EXAONE Deep 2.4Bå’Œ7.8Bæ¨¡å‹åœ¨åŒç±»ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>æœ€å¤§çš„EXAONE Deep 32Bæ¨¡å‹åœ¨é¢†å…ˆå¼€æ”¾æƒé‡æ¨¡å‹ä¸­å…·æœ‰ç«äº‰åŠ›ã€‚</li>
<li>EXAONE Deepç³»åˆ—æ¨¡å‹é€‚ç”¨äºç ”ç©¶ç›®çš„ã€‚</li>
<li>å…¬å¼€å¯ä¸‹è½½ï¼Œä¸‹è½½åœ°å€ï¼š<a target="_blank" rel="noopener" href="https://huggingface.co/LGAI-EXAONE%E3%80%82">https://huggingface.co/LGAI-EXAONEã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12524">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-984d6c366a3fd54557bbbbd7fbe64bb5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-68ac0b4aecb87f968d212657bd37834c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ba61c2df212b2a4f839e6bf393bc6c23.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e2510c59c42699cefc885105c2c62077.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-05576d05fb2a62c30759d7765b500706.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f63ab924f7c13208d5004c1e9e830093.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-90ba29c0672769adee08afda80c2aaa7.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="PLM-Efficient-Peripheral-Language-Models-Hardware-Co-Designed-for-Ubiquitous-Computing"><a href="#PLM-Efficient-Peripheral-Language-Models-Hardware-Co-Designed-for-Ubiquitous-Computing" class="headerlink" title="PLM: Efficient Peripheral Language Models Hardware-Co-Designed for   Ubiquitous Computing"></a>PLM: Efficient Peripheral Language Models Hardware-Co-Designed for   Ubiquitous Computing</h2><p><strong>Authors:Cheng Deng, Luoyang Sun, Jiwen Jiang, Yongcheng Zeng, Xinjian Wu, Wenxin Zhao, Qingfa Xiao, Jiachuan Wang, Haoyang Li, Lei Chen, Lionel M. Ni, Haifeng Zhang, Jun Wang</strong></p>
<p>While scaling laws have been continuously validated in large language models (LLMs) with increasing model parameters, the inherent tension between the inference demands of LLMs and the limited resources of edge devices poses a critical challenge to the development of edge intelligence. Recently, numerous small language models have emerged, aiming to distill the capabilities of LLMs into smaller footprints. However, these models often retain the fundamental architectural principles of their larger counterparts, still imposing considerable strain on the storage and bandwidth capacities of edge devices. In this paper, we introduce the PLM, a Peripheral Language Model, developed through a co-design process that jointly optimizes model architecture and edge system constraints. The PLM utilizes a Multi-head Latent Attention mechanism and employs the squared ReLU activation function to encourage sparsity, thereby reducing peak memory footprint during inference. During training, we collect and reorganize open-source datasets, implement a multi-phase training strategy, and empirically investigate the Warmup-Stable-Decay-Constant (WSDC) learning rate scheduler. Additionally, we incorporate Reinforcement Learning from Human Feedback (RLHF) by adopting the ARIES preference learning approach. Following a two-phase SFT process, this method yields performance gains of 2% in general tasks, 9% in the GSM8K task, and 11% in coding tasks. In addition to its novel architecture, evaluation results demonstrate that PLM outperforms existing small language models trained on publicly available data while maintaining the lowest number of activated parameters. Furthermore, deployment across various edge devices, including consumer-grade GPUs, mobile phones, and Raspberry Pis, validates PLMâ€™s suitability for peripheral applications. The PLM series models are publicly available at <a target="_blank" rel="noopener" href="https://github.com/plm-team/PLM">https://github.com/plm-team/PLM</a>. </p>
<blockquote>
<p>éšç€æ¨¡å‹å‚æ•°çš„å¢åŠ ï¼Œæ¯”ä¾‹å®šå¾‹åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­å¾—åˆ°äº†æŒç»­éªŒè¯ã€‚ç„¶è€Œï¼Œå¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†éœ€æ±‚ä¸è¾¹ç¼˜è®¾å¤‡çš„æœ‰é™èµ„æºä¹‹é—´å­˜åœ¨çš„å›ºæœ‰çŸ›ç›¾ï¼Œç»™è¾¹ç¼˜æ™ºèƒ½çš„å‘å±•å¸¦æ¥äº†å…³é”®æŒ‘æˆ˜ã€‚è¿‘æœŸï¼Œå‡ºç°äº†è®¸å¤šå°è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨å°†å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›è½¬åŒ–ä¸ºæ›´å°çš„å ç”¨ç©ºé—´ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹å¾€å¾€ä¿ç•™å…¶å¤§å‹å¯¹åº”æ¨¡å‹çš„åŸºæœ¬æ¶æ„åŸåˆ™ï¼Œä»ç„¶ç»™è¾¹ç¼˜è®¾å¤‡çš„å­˜å‚¨å’Œå¸¦å®½å®¹é‡å¸¦æ¥ç›¸å½“å¤§çš„å‹åŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§å‘¨è¾¹è¯­è¨€æ¨¡å‹ï¼ˆPLMï¼‰ï¼Œè¯¥æ¨¡å‹é€šè¿‡è”åˆä¼˜åŒ–æ¨¡å‹æ¶æ„å’Œè¾¹ç¼˜ç³»ç»Ÿçº¦æŸçš„ååŒè®¾è®¡è¿‡ç¨‹å¼€å‘ã€‚PLMåˆ©ç”¨å¤šå¤´æ½œåœ¨æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¹¶é‡‡ç”¨å¹³æ–¹ReLUæ¿€æ´»å‡½æ•°æ¥ä¿ƒè¿›ç¨€ç–æ€§ï¼Œä»è€Œå‡å°‘æ¨ç†è¿‡ç¨‹ä¸­çš„å³°å€¼å†…å­˜å ç”¨ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬æ”¶é›†å¹¶é‡æ–°ç»„ç»‡å¼€æºæ•°æ®é›†ï¼Œå®æ–½å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œå¹¶å¯¹é¢„çƒ­ç¨³å®šè¡°å‡æ’å®šï¼ˆWSDCï¼‰å­¦ä¹ ç‡è°ƒåº¦å™¨è¿›è¡Œå®è¯ç ”ç©¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡é‡‡ç”¨ARIEåå¥½å­¦ä¹ æ–¹æ³•ï¼Œå°†äººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ çº³å…¥å…¶ä¸­ã€‚ç»è¿‡ä¸¤é˜¶æ®µçš„SFTè¿‡ç¨‹åï¼Œè¯¥æ–¹æ³•åœ¨ä¸€èˆ¬ä»»åŠ¡ä¸Šå–å¾—äº†2%çš„æ€§èƒ½æå‡ï¼Œåœ¨GSM8Kä»»åŠ¡ä¸Šå–å¾—äº†9%çš„æå‡ï¼Œåœ¨ç¼–ç ä»»åŠ¡ä¸Šå–å¾—äº†11%çš„æå‡ã€‚é™¤äº†å…¶æ–°é¢–çš„æ¶æ„å¤–ï¼Œè¯„ä¼°ç»“æœè¡¨æ˜PLMåœ¨å…¬å¼€æ•°æ®ä¸Šè®­ç»ƒçš„ç°æœ‰å°è¯­è¨€æ¨¡å‹çš„è¡¨ç°ä¼˜äºå…¶ä»–æ¨¡å‹ï¼ŒåŒæ—¶ä¿æŒæœ€ä½çš„æ´»åŠ¨å‚æ•°æ•°é‡ã€‚æ­¤å¤–ï¼Œåœ¨å„ç§è¾¹ç¼˜è®¾å¤‡ä¸Šçš„éƒ¨ç½²ï¼ŒåŒ…æ‹¬æ¶ˆè´¹çº§GPUã€æ‰‹æœºå’ŒRaspberry Pisï¼ŒéªŒè¯äº†PLMåœ¨å‘¨è¾¹åº”ç”¨çš„é€‚ç”¨æ€§ã€‚PLMç³»åˆ—æ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/plm-team/PLM">https://github.com/plm-team/PLM</a>å…¬å¼€è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12167v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ä¸€ç§æ–°å‹çš„è¾¹ç¼˜æ™ºèƒ½è¯­è¨€æ¨¡å‹â€”â€”å‘¨è¾¹è¯­è¨€æ¨¡å‹ï¼ˆPLMï¼‰è¢«æå‡ºæ¥åº”å¯¹å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šåº”ç”¨çš„æŒ‘æˆ˜ã€‚PLMé€šè¿‡è”åˆä¼˜åŒ–æ¨¡å‹æ¶æ„å’Œè¾¹ç¼˜ç³»ç»Ÿçº¦æŸè¿›è¡Œè®¾è®¡ï¼Œé‡‡ç”¨å¤šå¤´æ½œåœ¨æ³¨æ„åŠ›æœºåˆ¶å’ŒReLUæ¿€æ´»å‡½æ•°æ¥é¼“åŠ±ç¨€ç–æ€§ï¼Œå‡å°‘æ¨ç†æ—¶çš„å†…å­˜å ç”¨ã€‚æ­¤å¤–ï¼ŒPLMè¿˜é‡‡ç”¨äº†åˆ†é˜¶æ®µè®­ç»ƒç­–ç•¥å’ŒWSDCå­¦ä¹ ç‡è°ƒåº¦å™¨ï¼Œå¹¶ç»“åˆäº†åŸºäºäººç±»åé¦ˆçš„å¢å¼ºå­¦ä¹ ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPLMåœ¨ä¸€èˆ¬ä»»åŠ¡ã€GSM8Kä»»åŠ¡å’Œç¼–ç ä»»åŠ¡ä¸Šçš„æ€§èƒ½åˆ†åˆ«æé«˜äº†2%ã€9%å’Œ11%ã€‚PLMç³»åˆ—æ¨¡å‹é€‚åˆåœ¨å„ç±»è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²ï¼ŒåŒ…æ‹¬æ¶ˆè´¹çº§GPUã€æ‰‹æœºå’ŒRaspberry Piã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¾¹ç¼˜æ™ºèƒ½è¯­è¨€æ¨¡å‹çš„å‘å±•é¢ä¸´æ¥è‡ªå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†éœ€æ±‚å’Œè¾¹ç¼˜è®¾å¤‡æœ‰é™èµ„æºçš„å†…åœ¨çŸ›ç›¾ã€‚</li>
<li>PLMä½œä¸ºä¸€ç§æ–°å‹è¯­è¨€æ¨¡å‹è¢«æå‡ºï¼Œæ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ï¼Œå®ƒé€šè¿‡è”åˆä¼˜åŒ–æ¨¡å‹æ¶æ„å’Œè¾¹ç¼˜ç³»ç»Ÿçº¦æŸè¿›è¡Œè®¾è®¡ã€‚</li>
<li>PLMé‡‡ç”¨å¤šå¤´æ½œåœ¨æ³¨æ„åŠ›æœºåˆ¶å’ŒReLUæ¿€æ´»å‡½æ•°é¼“åŠ±ç¨€ç–æ€§ï¼Œå‡å°‘æ¨ç†æ—¶çš„å†…å­˜å ç”¨ã€‚</li>
<li>PLMé‡‡ç”¨åˆ†é˜¶æ®µè®­ç»ƒç­–ç•¥å’ŒWSDCå­¦ä¹ ç‡è°ƒåº¦å™¨ï¼Œç»“åˆåŸºäºäººç±»åé¦ˆçš„å¢å¼ºå­¦ä¹ æ¥æå‡æ€§èƒ½ã€‚</li>
<li>PLMåœ¨ä¸€èˆ¬ä»»åŠ¡ã€GSM8Kä»»åŠ¡å’Œç¼–ç ä»»åŠ¡ä¸Šçš„æ€§èƒ½æœ‰æ‰€æå‡ã€‚</li>
<li>PLMç³»åˆ—æ¨¡å‹é€‚åˆåœ¨å„ç±»è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²ï¼ŒåŒ…æ‹¬æ¶ˆè´¹çº§GPUã€æ‰‹æœºå’ŒRaspberry Piç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12167">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3a5f7e629de109742bc712639db34e41.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b62586a274c11e3234ea155ce95fcc13.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-297c05f6376ec4ad4815db42b33ccff5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d0ca0bab6691aa042149c85b6c9d8c75.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-13c6d12b861dd39abed42fa8ae9baf0e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b4a5e9a73631efc6ab474d2cf9a3a6c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3f4ebb7fcdb92336312466784ada57e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f0401b5fbcdd6d35fd9814e92318f1a5.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="A-Review-of-DeepSeek-Modelsâ€™-Key-Innovative-Techniques"><a href="#A-Review-of-DeepSeek-Modelsâ€™-Key-Innovative-Techniques" class="headerlink" title="A Review of DeepSeek Modelsâ€™ Key Innovative Techniques"></a>A Review of DeepSeek Modelsâ€™ Key Innovative Techniques</h2><p><strong>Authors:Chengen Wang, Murat Kantarcioglu</strong></p>
<p>DeepSeek-V3 and DeepSeek-R1 are leading open-source Large Language Models (LLMs) for general-purpose tasks and reasoning, achieving performance comparable to state-of-the-art closed-source models from companies like OpenAI and Anthropic â€“ while requiring only a fraction of their training costs. Understanding the key innovative techniques behind DeepSeekâ€™s success is crucial for advancing LLM research. In this paper, we review the core techniques driving the remarkable effectiveness and efficiency of these models, including refinements to the transformer architecture, innovations such as Multi-Head Latent Attention and Mixture of Experts, Multi-Token Prediction, the co-design of algorithms, frameworks, and hardware, the Group Relative Policy Optimization algorithm, post-training with pure reinforcement learning and iterative training alternating between supervised fine-tuning and reinforcement learning. Additionally, we identify several open questions and highlight potential research opportunities in this rapidly advancing field. </p>
<blockquote>
<p>DeepSeek-V3å’ŒDeepSeek-R1æ˜¯é’ˆå¯¹é€šç”¨ä»»åŠ¡å’Œæ¨ç†çš„é¢†å…ˆå¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå…¶æ€§èƒ½ä¸OpenAIå’ŒAnthropicç­‰å…¬å¸æœ€å…ˆè¿›çš„é—­æºæ¨¡å‹ç›¸å½“ï¼ŒåŒæ—¶åªéœ€ä¸€å°éƒ¨åˆ†è®­ç»ƒæˆæœ¬ã€‚äº†è§£DeepSeekæˆåŠŸçš„å…³é”®åˆ›æ–°æŠ€æœ¯å¯¹äºæ¨è¿›LLMç ”ç©¶è‡³å…³é‡è¦ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å›é¡¾äº†é©±åŠ¨è¿™äº›æ¨¡å‹æ˜¾è‘—æœ‰æ•ˆæ€§å’Œæ•ˆç‡çš„æ ¸å¿ƒæŠ€æœ¯ï¼ŒåŒ…æ‹¬æ”¹è¿›Transformeræ¶æ„çš„æŠ€æœ¯ï¼Œåˆ›æ–°æŠ€æœ¯å¦‚å¤šå¤´æ½œåœ¨æ³¨æ„åŠ›ã€æ··åˆä¸“å®¶ã€å¤šä»¤ç‰Œé¢„æµ‹ç­‰ï¼Œç®—æ³•ã€æ¡†æ¶å’Œç¡¬ä»¶çš„ååŒè®¾è®¡ï¼Œç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ç®—æ³•ï¼Œä»¥åŠä½¿ç”¨çº¯å¼ºåŒ–å­¦ä¹ è¿›è¡Œè®­ç»ƒåï¼Œä»¥åŠåœ¨ç›‘ç£å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ ä¹‹é—´äº¤æ›¿è¿›è¡Œè¿­ä»£è®­ç»ƒç­‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ç¡®å®šäº†å‡ ä¸ªæ‚¬è€Œæœªå†³çš„é—®é¢˜ï¼Œå¹¶å¼ºè°ƒäº†è¿™ä¸€è¿…é€Ÿå‘å±•çš„é¢†åŸŸä¸­çš„æ½œåœ¨ç ”ç©¶æœºä¼šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.11486v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šDeepSeek-V3å’ŒDeepSeek-R1æ˜¯é€šç”¨çš„å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œæ€§èƒ½ä¸OpenAIå’ŒAnthropicç­‰å…¬å¸å¼€å‘çš„å…ˆè¿›ä¸“æœ‰æ¨¡å‹ç›¸å½“ï¼Œä¸”è®­ç»ƒæˆæœ¬è¾ƒä½ã€‚æœ¬æ–‡å›é¡¾äº†é©±åŠ¨è¿™äº›æ¨¡å‹å“è¶Šæ•ˆæœå’Œæ•ˆç‡çš„æ ¸å¿ƒæŠ€æœ¯ï¼ŒåŒ…æ‹¬æ”¹è¿›åçš„è½¬æ¢å™¨æ¶æ„ã€å¦‚å¤šå¤´æ½œåœ¨æ³¨æ„åŠ›ç­‰åˆ›æ–°æŠ€æœ¯ï¼Œä»¥åŠç®—æ³•ã€æ¡†æ¶å’Œç¡¬ä»¶çš„ååŒè®¾è®¡ã€ç¾¤ä½“ç›¸å¯¹æ”¿ç­–ä¼˜åŒ–ç®—æ³•ç­‰ã€‚åŒæ—¶ï¼Œæ–‡ç« è¿˜æ¢è®¨äº†çº¯å¼ºåŒ–å­¦ä¹ è¿›è¡Œè®­ç»ƒåçš„è°ƒæ•´æ–¹æ³•ä»¥åŠäº¤æ›¿ä½¿ç”¨ç›‘ç£å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ çš„è¿­ä»£è®­ç»ƒæ–¹æ³•ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡ä¹Ÿæå‡ºäº†è¯¥é¢†åŸŸå‡ ä¸ªå¾…è§£å†³çš„é—®é¢˜å’Œæ½œåœ¨çš„ç ”ç©¶æœºä¼šã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>DeepSeek-V3å’ŒDeepSeek-R1æ˜¯æ€§èƒ½ä¼˜è¶Šçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¼€æºä¸”è®­ç»ƒæˆæœ¬ä½ã€‚</li>
<li>æ¨¡å‹æˆåŠŸçš„å…³é”®åœ¨äºæ ¸å¿ƒæŠ€æœ¯çš„åˆ›æ–°ï¼ŒåŒ…æ‹¬æ”¹è¿›è½¬æ¢å™¨æ¶æ„å’Œå¤šå¤´æ½œåœ¨æ³¨æ„åŠ›ç­‰ã€‚</li>
<li>æ¨¡å‹é€šè¿‡ååŒè®¾è®¡ç®—æ³•ã€æ¡†æ¶å’Œç¡¬ä»¶æå‡æ€§èƒ½ã€‚</li>
<li>æ¨¡å‹é‡‡ç”¨ç¾¤ä½“ç›¸å¯¹æ”¿ç­–ä¼˜åŒ–ç®—æ³•è¿›è¡Œä¼˜åŒ–ã€‚</li>
<li>æ¨¡å‹ç»“åˆäº†ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ è¿›è¡Œè¿­ä»£è®­ç»ƒã€‚</li>
<li>æ–‡ç« æŒ‡å‡ºäº†è¯¥é¢†åŸŸçš„å¾…è§£å†³é—®é¢˜å’Œæ½œåœ¨ç ”ç©¶æœºä¼šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.11486">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f1eddb50b26e721a747112dec2e06729.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-74e48d55208488322ec9a63f42971b4d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-81c948f838df69828ff53c6d2deb32b4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6a5587c30dbf1fa05425d4a4a9594d3e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-69a6571ed4354fbe66561f92f5760a0e.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Reinforcement-Learning-Outperforms-Supervised-Fine-Tuning-A-Case-Study-on-Audio-Question-Answering"><a href="#Reinforcement-Learning-Outperforms-Supervised-Fine-Tuning-A-Case-Study-on-Audio-Question-Answering" class="headerlink" title="Reinforcement Learning Outperforms Supervised Fine-Tuning: A Case Study   on Audio Question Answering"></a>Reinforcement Learning Outperforms Supervised Fine-Tuning: A Case Study   on Audio Question Answering</h2><p><strong>Authors:Gang Li, Jizhong Liu, Heinrich Dinkel, Yadong Niu, Junbo Zhang, Jian Luan</strong></p>
<p>Recently, reinforcement learning (RL) has been shown to greatly enhance the reasoning capabilities of large language models (LLMs), and RL-based approaches have been progressively applied to visual multimodal tasks. However, the audio modality has largely been overlooked in these developments. Thus, we conduct a series of RL explorations in audio understanding and reasoning, specifically focusing on the audio question answering (AQA) task. We leverage the group relative policy optimization (GRPO) algorithm to Qwen2-Audio-7B-Instruct, and our experiments demonstrated state-of-the-art performance on the MMAU Test-mini benchmark, achieving an accuracy rate of 64.5%. The main findings in this technical report are as follows: 1) The GRPO algorithm can be effectively applied to large audio language models (LALMs), even when the model has only 8.2B parameters; 2) With only 38k post-training samples, RL significantly outperforms supervised fine-tuning (SFT), indicating that RL-based approaches can be effective without large datasets; 3) The explicit reasoning process has not shown significant benefits for AQA tasks, and how to efficiently utilize deep thinking remains an open question for further research; 4) LALMs still lag far behind humans auditory-language reasoning, suggesting that the RL-based approaches warrant further exploration. Our project is available at <a target="_blank" rel="noopener" href="https://github.com/xiaomi-research/r1-aqa">https://github.com/xiaomi-research/r1-aqa</a> and <a target="_blank" rel="noopener" href="https://huggingface.co/mispeech/r1-aqa">https://huggingface.co/mispeech/r1-aqa</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¢«è¯æ˜å¯ä»¥æå¤§åœ°æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶ä¸”åŸºäºRLçš„æ–¹æ³•å·²ç»é€æ­¥åº”ç”¨äºè§†è§‰å¤šæ¨¡æ€ä»»åŠ¡ã€‚ç„¶è€Œï¼ŒéŸ³é¢‘æ¨¡æ€åœ¨è¿™äº›å‘å±•ä¸­å´è¢«å¤§å¤§å¿½è§†äº†ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åœ¨éŸ³é¢‘ç†è§£å’Œæ¨ç†æ–¹é¢è¿›è¡Œäº†ä¸€ç³»åˆ—RLæ¢ç´¢ï¼Œç‰¹åˆ«å…³æ³¨éŸ³é¢‘é—®ç­”ï¼ˆAQAï¼‰ä»»åŠ¡ã€‚æˆ‘ä»¬åˆ©ç”¨ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç®—æ³•å¯¹Qwen2-Audio-7B-Instructè¿›è¡Œä¼˜åŒ–ï¼Œå®éªŒè¡¨æ˜åœ¨MMAU Test-miniåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†å…ˆè¿›æ€§èƒ½ï¼Œå‡†ç¡®ç‡ä¸º64.5%ã€‚æœ¬æŠ€æœ¯æŠ¥å‘Šçš„ä¸»è¦å‘ç°å¦‚ä¸‹ï¼š1ï¼‰GRPOç®—æ³•å¯æœ‰æ•ˆåœ°åº”ç”¨äºå¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLALMï¼‰ï¼Œå³ä½¿æ¨¡å‹å‚æ•°ä»…æœ‰8.2Bï¼›2ï¼‰ä»…ä½¿ç”¨38kä¸ªåè®­ç»ƒæ ·æœ¬ï¼ŒRLä¾¿æ˜¾è‘—ä¼˜äºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œè¡¨æ˜åŸºäºRLçš„æ–¹æ³•å¯åœ¨æ— éœ€å¤§è§„æ¨¡æ•°æ®é›†çš„æƒ…å†µä¸‹æœ‰æ•ˆï¼›3ï¼‰æ˜ç¡®çš„æ¨ç†è¿‡ç¨‹åœ¨AQAä»»åŠ¡ä¸­å¹¶æœªæ˜¾ç¤ºå‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œå¦‚ä½•æœ‰æ•ˆåˆ©ç”¨æ·±åº¦æ€è€ƒä»æ˜¯è¿›ä¸€æ­¥ç ”ç©¶çš„å¼€æ”¾é—®é¢˜ï¼›4ï¼‰LALMä»ç„¶è¿œè¿œè½åäºäººç±»çš„å¬è§‰è¯­è¨€æ¨ç†èƒ½åŠ›ï¼Œè¿™è¡¨æ˜åŸºäºRLçš„æ–¹æ³•éœ€è¦è¿›ä¸€æ­¥æ¢ç´¢ã€‚æˆ‘ä»¬çš„é¡¹ç›®å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/xiaomi-research/r1-aqa%E5%92%8Chttps://huggingface.co/mispeech/r1-aqa%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/xiaomi-research/r1-aqaå’Œhttps://huggingface.co/mispeech/r1-aqaè®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.11197v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢ç´¢äº†å¼ºåŒ–å­¦ä¹ åœ¨éŸ³é¢‘ç†è§£å’Œæ¨ç†é¢†åŸŸçš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨éŸ³é¢‘é—®ç­”ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚ç ”ç©¶å›¢é˜Ÿå°†ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ç®—æ³•åº”ç”¨äºå¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼Œå¹¶åœ¨å°å‹æµ‹è¯•é›†ä¸Šå–å¾—äº†çªç ´æ€§æˆæœï¼Œå‡†ç¡®ç‡è¾¾åˆ°64.5%ã€‚ä¸»è¦å‘ç°åŒ…æ‹¬ï¼šGRPOç®—æ³•å¯æœ‰æ•ˆåº”ç”¨äºå¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼›å¼ºåŒ–å­¦ä¹ åœ¨ä»…ä½¿ç”¨å°‘é‡è®­ç»ƒæ ·æœ¬æ—¶è¡¨ç°å‡ºä¼˜åŠ¿ï¼›æ˜¾å¼æ¨ç†è¿‡ç¨‹å¯¹éŸ³é¢‘é—®ç­”ä»»åŠ¡å¹¶æ— æ˜¾è‘—å¸®åŠ©ï¼›ä¸”å½“å‰æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ç›¸è¾ƒäºäººç±»ä»å­˜å·¨å¤§å·®è·ã€‚æ›´å¤šè¯¦æƒ…å‚è§ç ”ç©¶é¡¹ç›®åœ¨GitHubå’ŒHuggingfaceå¹³å°çš„ä»“åº“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GRPOç®—æ³•æˆåŠŸåº”ç”¨äºå¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLALMsï¼‰ï¼Œæ˜¾ç¤ºå…¶åœ¨éŸ³é¢‘é—®ç­”ä»»åŠ¡ä¸Šçš„æœ‰æ•ˆæ€§ã€‚</li>
<li>ä»…ä½¿ç”¨å°‘é‡è®­ç»ƒæ ·æœ¬ï¼ˆ38kï¼‰ï¼Œå¼ºåŒ–å­¦ä¹ æ€§èƒ½æ˜¾è‘—è¶…è¶Šç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€‚</li>
<li>åœ¨éŸ³é¢‘é—®ç­”ä»»åŠ¡ä¸­ï¼Œæ˜ç¡®çš„æ¨ç†è¿‡ç¨‹å¹¶æ²¡æœ‰æ˜¾è‘—çš„ä¼˜åŠ¿ï¼Œå¦‚ä½•æœ‰æ•ˆåˆ©ç”¨æ·±åº¦æ€è€ƒä»å¾…ç ”ç©¶ã€‚</li>
<li>å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹åœ¨å¬è§‰è¯­è¨€æ¨ç†ä¸Šä»ç„¶è¿œè¿œè½åäºäººç±»ï¼Œè¡¨æ˜éœ€è¦è¿›ä¸€æ­¥æ¢ç´¢åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•ã€‚</li>
<li>ç ”ç©¶é¡¹ç›®æä¾›äº†GitHubå’ŒHuggingfaceå¹³å°ä¸Šçš„èµ„æºé“¾æ¥ï¼Œä¾¿äºè¿›ä¸€æ­¥äº†è§£å’Œè®¿é—®é¡¹ç›®ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ åœ¨å¢å¼ºè¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨è§†è§‰å¤šåª’ä½“ä»»åŠ¡ä¸­çš„åº”ç”¨å·²è·å¾—è¿›å±•ã€‚ä½†åœ¨éŸ³é¢‘æ¨¡æ€é¢†åŸŸçš„ç ”ç©¶ä»å¤„äºèµ·æ­¥é˜¶æ®µã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.11197">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-388314d48f17825aaef7a5b1dc0830d2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4c97be7ba52440a6ae4a1f243f393377.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07ee4c5a9ee74401f65fa7d338581db2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-73d766d91c02ddb2edf1681e5ba53502.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-abeef9e0258bc8346c48c955699c805d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0154e2c76754beb485b201ea8b43e3f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-80290bf5c983f03ca3460976ec31d55a.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Falcon-A-Remote-Sensing-Vision-Language-Foundation-Model"><a href="#Falcon-A-Remote-Sensing-Vision-Language-Foundation-Model" class="headerlink" title="Falcon: A Remote Sensing Vision-Language Foundation Model"></a>Falcon: A Remote Sensing Vision-Language Foundation Model</h2><p><strong>Authors:Kelu Yao, Nuo Xu, Rong Yang, Yingying Xu, Zhuoyan Gao, Titinunt Kitrungrotsakul, Yi Ren, Pu Zhang, Jin Wang, Ning Wei, Chao Li</strong></p>
<p>This paper introduces a holistic vision-language foundation model tailored for remote sensing, named Falcon. Falcon offers a unified, prompt-based paradigm that effectively executes comprehensive and complex remote sensing tasks. Falcon demonstrates powerful understanding and reasoning abilities at the image, region, and pixel levels. Specifically, given simple natural language instructions and remote sensing images, Falcon can produce impressive results in text form across 14 distinct tasks, i.e., image classification, object detection, segmentation, image captioning, and etc. To facilitate Falconâ€™s training and empower its representation capacity to encode rich spatial and semantic information, we developed Falcon_SFT, a large-scale, multi-task, instruction-tuning dataset in the field of remote sensing. The Falcon_SFT dataset consists of approximately 78 million high-quality data samples, covering 5.6 million multi-spatial resolution and multi-view remote sensing images with diverse instructions. It features hierarchical annotations and undergoes manual sampling verification to ensure high data quality and reliability. Extensive comparative experiments are conducted, which verify that Falcon achieves remarkable performance over 67 datasets and 14 tasks, despite having only 0.7B parameters. We release the complete dataset, code, and model weights at <a target="_blank" rel="noopener" href="https://github.com/TianHuiLab/Falcon">https://github.com/TianHuiLab/Falcon</a>, hoping to help further develop the open-source community. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªä¸“ä¸ºé¥æ„Ÿé¢†åŸŸå®šåˆ¶çš„ä¸€ä½“åŒ–è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹ï¼Œåä¸ºFalconã€‚Falconæä¾›äº†ä¸€ç§åŸºäºæç¤ºçš„ç»Ÿä¸€èŒƒå¼ï¼Œå¯ä»¥æœ‰æ•ˆåœ°æ‰§è¡Œå…¨é¢è€Œå¤æ‚çš„é¥æ„Ÿä»»åŠ¡ã€‚Falconåœ¨å›¾åƒã€åŒºåŸŸå’Œåƒç´ çº§åˆ«è¡¨ç°å‡ºå¼ºå¤§çš„ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œç»™å®šç®€å•çš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤å’Œé¥æ„Ÿå›¾åƒï¼ŒFalconå¯ä»¥åœ¨14ä¸ªä¸åŒçš„ä»»åŠ¡ä¸­ä»¥æ–‡æœ¬å½¢å¼äº§ç”Ÿä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœï¼Œä¾‹å¦‚å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ã€åˆ†å‰²ã€å›¾åƒæè¿°ç­‰ã€‚ä¸ºäº†ä¿ƒè¿›Falconçš„è®­ç»ƒå¹¶å¢å¼ºå…¶è¡¨ç¤ºèƒ½åŠ›ä»¥ç¼–ç ä¸°å¯Œçš„ç©ºé—´è¯­ä¹‰ä¿¡æ¯ï¼Œæˆ‘ä»¬å¼€å‘äº†Falcon_SFTæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªé¥æ„Ÿé¢†åŸŸçš„å¤§è§„æ¨¡å¤šä»»åŠ¡æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ã€‚Falcon_SFTæ•°æ®é›†åŒ…å«çº¦7800ä¸‡é«˜è´¨é‡æ•°æ®æ ·æœ¬ï¼Œæ¶µç›–560ä¸‡å¤šä¸ªå¤šç©ºé—´åˆ†è¾¨ç‡å’Œå¤šè§†è§’é¥æ„Ÿå›¾åƒï¼Œä»¥åŠå¤šç§æŒ‡ä»¤ã€‚å®ƒé‡‡ç”¨åˆ†å±‚æ³¨é‡Šï¼Œå¹¶ç»è¿‡æ‰‹åŠ¨æŠ½æ ·éªŒè¯ï¼Œä»¥ç¡®ä¿æ•°æ®çš„é«˜è´¨é‡å’Œå¯é æ€§ã€‚è¿›è¡Œäº†å¹¿æ³›çš„å¯¹æ¯”å®éªŒï¼ŒéªŒè¯äº†Falconåœ¨67ä¸ªæ•°æ®é›†å’Œ14ä¸ªä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½ï¼Œå°½ç®¡å…¶å‚æ•°åªæœ‰0.7Bã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/TianHuiLab/Falcon%E4%B8%8A%E5%8F%91%E5%B8%83%E4%BA%86%E5%AE%8C%E6%95%B4%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86%E3%80%81%E4%BB%A3%E7%A0%81%E5%92%8C%E6%A8%A1%E5%9E%8B%E6%9D%83%E9%87%8D%EF%BC%8C%E5%B8%8C%E6%9C%9B%E8%83%BD%E5%B8%AE%E5%8A%A9%E8%BF%9B%E4%B8%80%E6%AD%A5%E6%8E%A8%E5%8A%A8%E5%BC%80%E6%BA%90%E7%A4%BE%E5%8C%BA%E7%9A%84%E5%8F%91%E5%B1%95%E3%80%82">https://github.com/TianHuiLab/Falconä¸Šå‘å¸ƒäº†å®Œæ•´çš„æ•°æ®é›†ã€ä»£ç å’Œæ¨¡å‹æƒé‡ï¼Œå¸Œæœ›èƒ½å¸®åŠ©è¿›ä¸€æ­¥æ¨åŠ¨å¼€æºç¤¾åŒºçš„å‘å±•ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.11070v1">PDF</a> Under Review</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é’ˆå¯¹é¥æ„Ÿé¢†åŸŸçš„å…¨æ–°è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹â€”â€”Falconã€‚è¯¥æ¨¡å‹é‡‡ç”¨åŸºäºæç¤ºçš„ç»Ÿä¸€èŒƒå¼ï¼Œèƒ½æœ‰æ•ˆæ‰§è¡Œå¤æ‚é¥æ„Ÿä»»åŠ¡ã€‚é€šè¿‡ç®€å•è‡ªç„¶è¯­è¨€æŒ‡ä»¤å’Œé¥æ„Ÿå›¾åƒï¼ŒFalconèƒ½åœ¨æ–‡æœ¬å½¢å¼ä¸‹å®Œæˆå¤šç§ä»»åŠ¡ï¼Œå±•ç°å‡ºå¼ºå¤§çš„ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚ä¸ºäº†è®­ç»ƒå’Œå¼ºåŒ–Falconçš„ç©ºé—´å’Œè¯­ä¹‰ä¿¡æ¯ç¼–ç èƒ½åŠ›ï¼Œå¼€å‘äº†å¤§å‹å¤šä»»åŠ¡æŒ‡ä»¤è°ƒä¼˜æ•°æ®é›†Falcon_SFTã€‚è¯¥æ•°æ®é›†åŒ…å«çº¦78ä¸‡é«˜è´¨é‡æ ·æœ¬ï¼Œè¦†ç›–å¤šç§ç©ºé—´åˆ†è¾¨ç‡å’Œè§†è§’çš„é¥æ„Ÿå›¾åƒï¼Œå…·å¤‡å±‚æ¬¡åŒ–æ³¨é‡Šï¼Œå¹¶ç»è¿‡äººå·¥é‡‡æ ·éªŒè¯ä»¥ç¡®ä¿æ•°æ®è´¨é‡å’Œå¯é æ€§ã€‚å®éªŒè¯æ˜ï¼Œå°½ç®¡åªæœ‰0.7Bå‚æ•°ï¼ŒFalconåœ¨å¤šä¸ªæ•°æ®é›†å’Œä»»åŠ¡ä¸Šçš„è¡¨ç°ä»ç„¶å“è¶Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Falconæ˜¯ä¸€ä¸ªé’ˆå¯¹é¥æ„Ÿé¢†åŸŸçš„å…¨æ–°è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹ã€‚</li>
<li>é‡‡ç”¨åŸºäºæç¤ºçš„ç»Ÿä¸€èŒƒå¼ï¼Œæ‰§è¡Œå¤æ‚é¥æ„Ÿä»»åŠ¡ã€‚</li>
<li>é€šè¿‡ç®€å•è‡ªç„¶è¯­è¨€æŒ‡ä»¤å’Œé¥æ„Ÿå›¾åƒï¼Œèƒ½åœ¨æ–‡æœ¬å½¢å¼ä¸‹å®Œæˆå¤šç§ä»»åŠ¡ã€‚</li>
<li>Falconå±•ç°äº†å¼ºå¤§çš„ç†è§£å’Œæ¨ç†èƒ½åŠ›ï¼Œåœ¨å›¾åƒã€åŒºåŸŸå’Œåƒç´ çº§åˆ«éƒ½æœ‰å‡ºè‰²è¡¨ç°ã€‚</li>
<li>Falcon_SFTæ•°æ®é›†ç”¨äºè®­ç»ƒå’Œå¼ºåŒ–Falconçš„ç©ºé—´å’Œè¯­ä¹‰ä¿¡æ¯ç¼–ç èƒ½åŠ›ã€‚</li>
<li>Falcon_SFTæ•°æ®é›†åŒ…å«çº¦78ä¸‡é«˜è´¨é‡æ ·æœ¬ï¼Œè¦†ç›–å¤šç§ç©ºé—´åˆ†è¾¨ç‡å’Œè§†è§’çš„é¥æ„Ÿå›¾åƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.11070">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-33504beacbb170eda88331525efcb1da.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0e0b96d8e08091fcd3732edca7d18dcb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-34c6faffe783053ca95b7c40a4853c74.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-98d73e3842ecc217667f058374b8ad99.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3362166d5b933dcc990b7626d73a8265.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6476b8bf9370aebbe30cc8efb899b905.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Chat-TS-Enhancing-Multi-Modal-Reasoning-Over-Time-Series-and-Natural-Language-Data"><a href="#Chat-TS-Enhancing-Multi-Modal-Reasoning-Over-Time-Series-and-Natural-Language-Data" class="headerlink" title="Chat-TS: Enhancing Multi-Modal Reasoning Over Time-Series and Natural   Language Data"></a>Chat-TS: Enhancing Multi-Modal Reasoning Over Time-Series and Natural   Language Data</h2><p><strong>Authors:Paul Quinlan, Qingguo Li, Xiaodan Zhu</strong></p>
<p>Time-series analysis is critical for a wide range of fields such as healthcare, finance, transportation, and energy, among many others. The practical applications often involve analyzing time-series data alongside contextual information in the form of natural language to support informed decisions. However, current time-series models are limited in their ability to perform reasoning that involves both time-series and their textual content. In this work, we address this gap by introducing \textit{Chat-TS}, a large language model (LLM) based framework, designed to support reasoning over time series and textual data. Unlike traditional models, Chat-TS integrates time-series tokens into LLMsâ€™ vocabulary, enhancing its reasoning ability over both modalities without compromising the core natural language capabilities, enabling practical analysis and reasoning across modalities. To support learning and evaluation in this setup, we contribute new datasets: the \textit{TS Instruct Training Dataset} which pairs diverse time-series data with relevant text instructions and responses for instruction tuning, the \textit{TS Instruct Question and Answer (QA) Gold Dataset} which provides multiple-choice questions designed to evaluate multimodal reasoning, and a \textit{TS Instruct Quantitative Probing Set} which contains a small subset of the TS Instruct QA tasks alongside math and decision-making questions for LLM evaluation. We designed a training strategy to preserve the inherent reasoning capabilities of LLMs while augmenting them for time-series reasoning. Experiments show that Chat-TS achieves state-of-the-art performance in multi-modal reasoning tasks by maintaining strong natural language proficiency while improving time-series reasoning. ~\footnote{To ensure replicability and facilitate future research, all models, datasets, and code will be available at [\texttt{Github-URL}].} </p>
<blockquote>
<p>æ—¶é—´åºåˆ—åˆ†æåœ¨åŒ»ç–—ã€é‡‘èã€äº¤é€šã€èƒ½æºç­‰è¯¸å¤šé¢†åŸŸå…·æœ‰å¹¿æ³›åº”ç”¨ã€‚å®é™…åº”ç”¨ä¸­ç»å¸¸éœ€è¦åˆ†ææ—¶é—´åºåˆ—æ•°æ®ä»¥åŠä¸è‡ªç„¶è¯­è¨€å½¢å¼çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä»¥æ”¯æŒåšå‡ºæ˜æ™ºçš„å†³ç­–ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ—¶é—´åºåˆ—æ¨¡å‹åœ¨æ¶‰åŠæ—¶é—´åºåˆ—å’Œæ–‡æœ¬å†…å®¹çš„æ¨ç†æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å¼•å…¥Chat-TSæ¡†æ¶æ¥è§£å†³è¿™ä¸€å·®è·ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¡†æ¶ï¼Œæ—¨åœ¨æ”¯æŒæ—¶é—´åºåˆ—å’Œæ–‡æœ¬æ•°æ®çš„æ¨ç†ã€‚ä¸ä¼ ç»Ÿçš„æ¨¡å‹ä¸åŒï¼ŒChat-TSå°†æ—¶é—´åºåˆ—æ ‡è®°é›†æˆåˆ°LLMçš„è¯æ±‡è¡¨ä¸­ï¼Œæé«˜äº†å…¶åœ¨ä¸¤ç§æ¨¡æ€ä¸Šçš„æ¨ç†èƒ½åŠ›ï¼ŒåŒæ—¶ä¸æŸå®³å…¶æ ¸å¿ƒçš„è‡ªç„¶è¯­è¨€åŠŸèƒ½ï¼Œä»è€Œå®ç°äº†è·¨æ¨¡æ€çš„å®é™…åˆ†æå’Œæ¨ç†ã€‚ä¸ºäº†æ”¯æŒåœ¨æ­¤è®¾ç½®ä¸­çš„å­¦ä¹ å’Œè¯„ä¼°ï¼Œæˆ‘ä»¬æä¾›äº†æ–°çš„æ•°æ®é›†ï¼šTS Instructè®­ç»ƒæ•°æ®é›†ï¼Œå®ƒå°†å„ç§æ—¶é—´åºåˆ—æ•°æ®ä¸ç›¸å…³çš„æ–‡æœ¬æŒ‡ä»¤å’Œå“åº”é…å¯¹ï¼Œç”¨äºæŒ‡ä»¤è°ƒæ•´ï¼›TS Instructé—®ç­”ï¼ˆQAï¼‰é»„é‡‘æ•°æ®é›†ï¼Œæä¾›æ—¨åœ¨è¯„ä¼°å¤šæ¨¡æ€æ¨ç†çš„é€‰æ‹©é¢˜ï¼›ä»¥åŠTS Instructå®šé‡æ¢æµ‹é›†ï¼Œå…¶ä¸­åŒ…å«TS Instruct QAä»»åŠ¡çš„ä¸€ä¸ªå°å­é›†ä»¥åŠç”¨äºLLMè¯„ä¼°çš„æ•°å­¦å’Œå†³ç­–é—®é¢˜ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ç§è®­ç»ƒç­–ç•¥ï¼Œæ—¨åœ¨ä¿ç•™LLMçš„å›ºæœ‰æ¨ç†èƒ½åŠ›ï¼ŒåŒæ—¶å¯¹å…¶è¿›è¡Œæ—¶é—´åºåˆ—æ¨ç†çš„å¢å¼ºã€‚å®éªŒè¡¨æ˜ï¼ŒChat-TSåœ¨å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ—¢ä¿æŒäº†å¼ºå¤§çš„è‡ªç„¶è¯­è¨€æŠ€èƒ½ï¼Œåˆæé«˜äº†æ—¶é—´åºåˆ—æ¨ç†èƒ½åŠ›ã€‚~\footnote{ä¸ºç¡®ä¿å¯å¤åˆ¶æ€§å’Œä¿ƒè¿›æœªæ¥ç ”ç©¶ï¼Œæ‰€æœ‰æ¨¡å‹ã€æ•°æ®é›†å’Œä»£ç éƒ½å°†åœ¨[\texttt{Github-URL}]ä¸Šæä¾›ã€‚}</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10883v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†æ—¶é—´åºåˆ†æåœ¨å¤šä¸ªé¢†åŸŸå¦‚åŒ»ç–—ã€é‡‘èç­‰çš„é‡è¦æ€§ï¼Œå¹¶æŒ‡å‡ºäº†å½“å‰æ—¶é—´åºæ¨¡å‹åœ¨å¤„ç†æ¶‰åŠæ—¶é—´åºåˆ—å’Œæ–‡æœ¬å†…å®¹æ¨ç†æ—¶çš„å±€é™æ€§ã€‚ä¸ºæ­¤ï¼Œè¯¥æ–‡æå‡ºäº†ä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¡†æ¶Chat-TSï¼Œæ—¨åœ¨æ”¯æŒæ—¶é—´åºåˆ—å’Œæ–‡æœ¬æ•°æ®çš„æ¨ç†ã€‚Chat-TSé€šè¿‡æ•´åˆæ—¶é—´åºåˆ—æ ‡è®°åˆ°LLMçš„è¯æ±‡è¡¨ä¸­ï¼Œæå‡äº†è·¨æ¨¡æ€æ¨ç†èƒ½åŠ›ï¼ŒåŒæ—¶ä¸æŸå¤±æ ¸å¿ƒçš„è‡ªç„¶è¯­è¨€åŠŸèƒ½ã€‚ä¸ºæ”¯æŒåœ¨æ­¤è®¾ç½®ä¸‹çš„å­¦ä¹ å’Œè¯„ä¼°ï¼Œä½œè€…è´¡çŒ®äº†æ–°çš„æ•°æ®é›†å’Œè®­ç»ƒç­–ç•¥ã€‚å®éªŒè¡¨æ˜ï¼ŒChat-TSåœ¨å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ—¶é—´åºåˆ—åˆ†æåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰å¹¿æ³›åº”ç”¨ï¼Œå¦‚åŒ»ç–—ã€é‡‘èç­‰ã€‚</li>
<li>å½“å‰æ—¶é—´åºåˆ—æ¨¡å‹åœ¨å¤„ç†æ¶‰åŠæ—¶é—´åºåˆ—å’Œæ–‡æœ¬å†…å®¹çš„æ¨ç†æ—¶å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>Chat-TSæ¡†æ¶æ—¨åœ¨æ”¯æŒæ—¶é—´åºåˆ—å’Œæ–‡æœ¬æ•°æ®çš„æ¨ç†ï¼Œé€šè¿‡æ•´åˆæ—¶é—´åºåˆ—æ ‡è®°åˆ°LLMçš„è¯æ±‡è¡¨ä¸­ï¼Œå¢å¼ºäº†è·¨æ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚</li>
<li>Chat-TSè´¡çŒ®äº†æ–°çš„æ•°æ®é›†ä»¥æ”¯æŒå­¦ä¹ å’Œè¯„ä¼°ï¼ŒåŒ…æ‹¬TS Instruct Training Datasetã€TS Instruct Question and Answer (QA) Gold Datasetä»¥åŠTS Instruct Quantitative Probing Setã€‚</li>
<li>Chat-TSè®¾è®¡äº†ä¸€ç§è®­ç»ƒç­–ç•¥ï¼Œæ—¨åœ¨ä¿ç•™LLMçš„å›ºæœ‰æ¨ç†èƒ½åŠ›ï¼ŒåŒæ—¶å¢å¼ºå…¶æ—¶é—´åºåˆ—æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒChat-TSåœ¨å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒå¼ºå¤§çš„è‡ªç„¶è¯­è¨€å¤„ç†èƒ½åŠ›çš„åŒæ—¶ï¼Œæé«˜æ—¶é—´åºåˆ—æ¨ç†èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10883">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-04bd58c51633b226dfcc75ebf6f2bb16.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dc6c05ba14a7f603a032caf69ef98f85.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-281d0a97a4734e18fc83efcc46a60a01.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8165d394feadcee13c682e3d8cb560b1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-491ec84055dc938c15282347e4c5bc6f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c866f5701a23d5ba62c9a701382983a7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ed26452a47bc1b4b743a68209301eeed.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-21/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-21/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-21/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-bf70a7dfc1ecb33cb7416364e83d1f6c.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-21  SWEET-RL Training Multi-Turn LLM Agents on Collaborative Reasoning   Tasks
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-21
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-20/Text-to-Motion/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-82b742bc479adb9e7b33d83aad6fef07.jpg" class="responsive-img" alt="Text-to-Motion">
                        
                        <span class="card-title">Text-to-Motion</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Text-to-Motion æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-20  Less is More Improving Motion Diffusion Models with Sparse Keyframes
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-20
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Text-to-Motion/" class="post-category">
                                    Text-to-Motion
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Text-to-Motion/">
                        <span class="chip bg-color">Text-to-Motion</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">17196.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
