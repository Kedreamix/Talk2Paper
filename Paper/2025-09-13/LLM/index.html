<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM 方向最新论文已更新，请持续关注 Update in 2025-09-13  The Illusion of Diminishing Returns Measuring Long Horizon Execution in   LLMs">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2406.06464v4/page_2_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-09-13
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-09-13
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    16.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    67 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-09-13-更新"><a href="#2025-09-13-更新" class="headerlink" title="2025-09-13 更新"></a>2025-09-13 更新</h1><h2 id="The-Illusion-of-Diminishing-Returns-Measuring-Long-Horizon-Execution-in-LLMs"><a href="#The-Illusion-of-Diminishing-Returns-Measuring-Long-Horizon-Execution-in-LLMs" class="headerlink" title="The Illusion of Diminishing Returns: Measuring Long Horizon Execution in   LLMs"></a>The Illusion of Diminishing Returns: Measuring Long Horizon Execution in   LLMs</h2><p><strong>Authors:Akshit Sinha, Arvindh Arun, Shashwat Goel, Steffen Staab, Jonas Geiping</strong></p>
<p>Does continued scaling of large language models (LLMs) yield diminishing returns? Real-world value often stems from the length of task an agent can complete. We start this work by observing the simple but counterintuitive fact that marginal gains in single-step accuracy can compound into exponential improvements in the length of a task a model can successfully complete. Then, we argue that failures of LLMs when simple tasks are made longer arise from mistakes in execution, rather than an inability to reason. We propose isolating execution capability, by explicitly providing the knowledge and plan needed to solve a long-horizon task. We find that larger models can correctly execute significantly more turns even when small models have 100% single-turn accuracy. We observe that the per-step accuracy of models degrades as the number of steps increases. This is not just due to long-context limitations – curiously, we observe a self-conditioning effect – models become more likely to make mistakes when the context contains their errors from prior turns. Self-conditioning does not reduce by just scaling the model size. In contrast, recent thinking models do not self-condition, and can also execute much longer tasks in a single turn. We conclude by benchmarking frontier thinking models on the length of task they can execute in a single turn. Overall, by focusing on the ability to execute, we hope to reconcile debates on how LLMs can solve complex reasoning problems yet fail at simple tasks when made longer, and highlight the massive benefits of scaling model size and sequential test-time compute for long-horizon tasks. </p>
<blockquote>
<p>继续扩大大型语言模型（LLM）的规模是否会产生收益递减效应？现实价值往往源于代理可以完成的任务长度。我们从观察一个简单但具有矛盾的事实开始，即单步精度的边际增益可以转化为任务成功完成长度的指数改进。然后，我们论证当简单任务被延长时，LLM的失败是由于执行过程中的错误，而不是推理能力的不足。我们提议通过明确提供解决长期任务所需的知识和计划来隔离执行能力。我们发现，即使在小型模型的单轮准确率达到100%的情况下，大型模型仍然可以正确执行更多轮次。我们观察到，随着步骤数量的增加，模型的每步准确率会下降。这不仅仅是由于长期上下文限制——奇怪的是，我们观察到了一个自条件效应——当上下文包含先前轮次中的错误时，模型更可能犯错误。仅通过扩大模型规模并不能减少这种自条件效应。相比之下，最近的思考模型并不自我条件化，并且可以在单轮中执行更长的任务。我们通过前沿思考模型在单轮中可以执行的任务长度进行基准测试来得出结论。总体而言，通过关注执行能力，我们希望调和关于LLM如何解决复杂的推理问题但在更长的简单任务中失败的争论，并强调在规模模型和顺序测试时间计算中对长期任务的巨大好处。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.09677v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）的持续扩展是否产生边际效益递减，实际价值往往取决于模型能完成任务的长度。研究发现，单步准确性的微小增益可以累积成为任务长度的指数级改进，并指出长任务失败往往源于执行过程中的错误，而非推理能力的不足。通过提供知识和计划来隔离执行能力，研究人员发现大型模型在执行更多步骤时表现更好，即使在小型模型单步准确率为百分之百的情况下也是如此。随着步骤数量的增加，模型的每步准确性会下降，这不仅仅是由于长期上下文限制。一种奇怪的现象是自我条件效应——当上下文包含先前的错误时，模型更容易出错。增加模型规模并不能减少自我条件效应。相比之下，最新的思考模型不进行自我条件，并能一次完成更长的任务。通过对前沿思考模型进行基准测试，发现它们在单次执行任务的长度方面表现出色。因此，本研究通过关注模型的执行能力，探讨了LLM如何解决复杂的推理问题，以及在任务长度增加时遇到的简单任务失败问题，并强调了模型规模扩展和顺序测试时间计算在长期任务中的巨大益处。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）的实际价值取决于其能完成任务的长度。</li>
<li>单步准确性的微小改进可以累积成任务完成的指数级提升。</li>
<li>LLM在长任务中的失败主要源于执行过程中的错误，而非推理能力不足。</li>
<li>通过提供知识和计划来隔离模型的执行能力，大型模型在执行更多步骤时表现更佳。</li>
<li>随着步骤数量的增加，模型的每步准确性会下降，这其中包括自我条件效应的影响。</li>
<li>简单的模型规模扩展并不能减少自我条件效应。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.09677">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2509.09677v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2509.09677v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2509.09677v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2509.09677v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2509.09677v1/page_5_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Measuring-Epistemic-Humility-in-Multimodal-Large-Language-Models"><a href="#Measuring-Epistemic-Humility-in-Multimodal-Large-Language-Models" class="headerlink" title="Measuring Epistemic Humility in Multimodal Large Language Models"></a>Measuring Epistemic Humility in Multimodal Large Language Models</h2><p><strong>Authors:Bingkui Tong, Jiaer Xia, Sifeng Shang, Kaiyang Zhou</strong></p>
<p>Hallucinations in multimodal large language models (MLLMs) – where the model generates content inconsistent with the input image – pose significant risks in real-world applications, from misinformation in visual question answering to unsafe errors in decision-making. Existing benchmarks primarily test recognition accuracy, i.e., evaluating whether models can select the correct answer among distractors. This overlooks an equally critical capability for trustworthy AI: recognizing when none of the provided options are correct, a behavior reflecting epistemic humility. We present HumbleBench, a new hallucination benchmark designed to evaluate MLLMs’ ability to reject plausible but incorrect answers across three hallucination types: object, relation, and attribute. Built from a panoptic scene graph dataset, we leverage fine-grained scene graph annotations to extract ground-truth entities and relations, and prompt GPT-4-Turbo to generate multiple-choice questions, followed by a rigorous manual filtering process. Each question includes a “None of the above” option, requiring models not only to recognize correct visual information but also to identify when no provided answer is valid. We evaluate a variety of state-of-the-art MLLMs – including both general-purpose and specialized reasoning models – on HumbleBench and share valuable findings and insights with the community. By incorporating explicit false-option rejection, HumbleBench fills a key gap in current evaluation suites, providing a more realistic measure of MLLM reliability in safety-critical settings. Our code and dataset are released publicly and can be accessed at <a target="_blank" rel="noopener" href="https://github.com/maifoundations/HumbleBench">https://github.com/maifoundations/HumbleBench</a>. </p>
<blockquote>
<p>多模态大型语言模型（MLLMs）中的幻觉，即模型生成与输入图像不一致的内容，在现实世界应用中带来了重大风险，从视觉问答中的误导信息到决策中的不安全错误。现有的基准测试主要测试识别准确率，即评估模型是否能在干扰项中选出正确答案。这忽略了可信人工智能同样关键的能力：即当所有提供的选项都不正确时，能够识别出来，这种行为反映了知识谦逊。我们提出了HumbleBench，这是一个新的幻觉基准测试，旨在评估MLLMs拒绝可能但错误答案的能力，涵盖三种幻觉类型：对象、关系和属性。它建立在全景场景图数据集上，我们利用精细场景图注释来提取真实实体和关系，并提示GPT-4-Turbo生成多项选择题，随后经过严格的手动过滤过程。每个问题都包括一个“以上都不是”的选项，要求模型不仅要识别正确的视觉信息，还要在没有任何提供的答案有效时能够识别出来。我们在HumbleBench上评估了多种先进的多模态大型语言模型，包括通用和专门用于推理的模型，并与社区分享了有价值的发现和见解。通过融入明确的虚假选项拒绝能力，HumbleBench填补了当前评估套件的关键空白，为安全关键设置中MLLM的可靠性提供了更现实的衡量标准。我们的代码和数据集已公开发布，可在<a target="_blank" rel="noopener" href="https://github.com/maifoundations/HumbleBench%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/maifoundations/HumbleBench访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.09658v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>多模态大型语言模型（MLLMs）中的幻觉，即模型生成与输入图像不一致的内容，在真实世界应用中存在重大风险，如视觉问答中的错误信息以及决策制定中的安全错误。现有基准测试主要关注识别准确性，即评估模型是否能在干扰项中选出正确答案。然而，这忽略了可信人工智能的另一个关键能力：拒绝正确但不可靠的答案的行为反映了知识谦逊。我们提出了HumbleBench，一个新的幻觉基准测试，旨在评估MLLMs拒绝三种幻觉类型（对象、关系和属性）的可靠但错误答案的能力。HumbleBench建立在一个全景场景图数据集上，利用精细场景图注释提取真实实体和关系，并提示GPT-4 Turbo生成多项选择题，随后进行严格的人工过滤过程。每个问题都包括一个“以上都不是”的选项，要求模型不仅要识别正确的视觉信息，还要在没有任何提供的答案是有效时识别出来。我们在HumbleBench上评估了一系列最先进的MLLMs，包括通用和专门用于推理的模型，并与社区分享有价值的发现和见解。通过明确的虚假选项拒绝，HumbleBench填补了当前评估套件的关键空白，为安全关键设置中MLLM的可靠性提供了更现实的衡量标准。我们的代码和数据集已公开发布，可访问<a target="_blank" rel="noopener" href="https://github.com/maifoundations/HumbleBench%E3%80%82">https://github.com/maifoundations/HumbleBench。</a></p>
<p><strong>要点如下</strong></p>
<ol>
<li>多模态大型语言模型（MLLMs）中的幻觉在真实世界应用中存在风险。</li>
<li>当前基准测试主要关注识别准确性，忽略了模型在拒绝错误答案时的能力。</li>
<li>提出新的幻觉基准测试——HumbleBench，旨在评估MLLMs拒绝三种幻觉类型的能力。</li>
<li>利用全景场景图数据集和精细场景图注释建立HumbleBench。</li>
<li>GPT-4 Turbo被提示生成多项选择题，并要求模型识别正确的视觉信息和无有效答案的情况。</li>
<li>在HumbleBench上评估了一系列最先进的MLLMs，包括通用和专门用于推理的模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.09658">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2509.09658v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2509.09658v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2509.09658v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2509.09658v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2509.09658v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Bridging-the-Capability-Gap-Joint-Alignment-Tuning-for-Harmonizing-LLM-based-Multi-Agent-Systems"><a href="#Bridging-the-Capability-Gap-Joint-Alignment-Tuning-for-Harmonizing-LLM-based-Multi-Agent-Systems" class="headerlink" title="Bridging the Capability Gap: Joint Alignment Tuning for Harmonizing   LLM-based Multi-Agent Systems"></a>Bridging the Capability Gap: Joint Alignment Tuning for Harmonizing   LLM-based Multi-Agent Systems</h2><p><strong>Authors:Minghang Zhu, Zhengliang Shi, Zhiwei Xu, Shiguang Wu, Lingjie Wang, Pengjie Ren, Zhaochun Ren, Zhumin Chen</strong></p>
<p>The advancement of large language models (LLMs) has enabled the construction of multi-agent systems to solve complex tasks by dividing responsibilities among specialized agents, such as a planning agent for subgoal generation and a grounding agent for executing tool-use actions. Most existing methods typically fine-tune these agents independently, leading to capability gaps among them with poor coordination. To address this, we propose MOAT, a Multi-Agent Joint Alignment Tuning framework that improves agents collaboration through iterative alignment. MOAT alternates between two key stages: (1) Planning Agent Alignment, which optimizes the planning agent to generate subgoal sequences that better guide the grounding agent; and (2) Grounding Agent Improving, which fine-tunes the grounding agent using diverse subgoal-action pairs generated by the agent itself to enhance its generalization capablity. Theoretical analysis proves that MOAT ensures a non-decreasing and progressively convergent training process. Experiments across six benchmarks demonstrate that MOAT outperforms state-of-the-art baselines, achieving average improvements of 3.1% on held-in tasks and 4.4% on held-out tasks. </p>
<blockquote>
<p>大型语言模型（LLM）的进步使得能够构建多智能体系统，通过专门化的智能体分配责任来解决复杂任务，例如用于生成子目标的规划智能体和用于执行工具使用动作的接地智能体。大多数现有方法通常独立微调这些智能体，导致它们之间存在能力差距，协调不佳。为了解决这一问题，我们提出了MOAT，这是一个多智能体联合对齐调整框架，它通过迭代对齐改进智能体的协作。MOAT在两个关键阶段之间交替进行：（1）规划智能体对齐，优化规划智能体以生成更好地指导接地智能体的子目标序列；（2）接地智能体改进，使用智能体本身生成的多样化的子目标-动作对微调接地智能体，提高其泛化能力。理论分析证明，MOAT确保了非递减和逐步收敛的训练过程。在六个基准测试上的实验表明，MOAT优于最新的基线技术，在已完成任务上平均提高了3.1%，在未完成任务上平均提高了4.4%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.09629v1">PDF</a> EMNLP 2025 Findings</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）的进步使得多智能体系统的构建成为可能，通过专门化的智能体分工协作来解决复杂任务。然而，现有方法通常独立微调这些智能体，导致能力差距和协调不良。为此，我们提出MOAT——一种多智能体联合对齐调整框架，通过迭代对齐提高智能体的协作能力。MOAT交替进行两个关键阶段：规划智能体对齐，优化规划智能体以生成更好的子目标序列来指导接地智能体；接地智能体改进，使用由智能体本身生成的多样化的子目标-动作对进行微调，提高其泛化能力。理论和实验证明，MOAT确保训练过程不断增进并逐步收敛，在六个基准测试上的表现优于最新基线技术，完成任务的平均改进率为3.1%和4.4%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM的进步促进了多智能体系统的构建，允许通过专门化的智能体来解决复杂任务。</li>
<li>现有方法独立微调智能体，导致能力差距和协调问题。</li>
<li>MOAT框架是一种多智能体联合对齐调整方法，通过迭代对齐提高智能体的协作。</li>
<li>MOAT包括两个关键阶段：规划智能体对齐和接地智能体改进。</li>
<li>规划智能体对齐阶段优化生成子目标序列，以指导接地智能体的行为。</li>
<li>接地智能体改进阶段使用多样化的子目标-动作对进行微调，提高其泛化能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.09629">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2509.09629v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2509.09629v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2509.09629v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2509.09629v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="LAVA-Language-Model-Assisted-Verbal-Autopsy-for-Cause-of-Death-Determination"><a href="#LAVA-Language-Model-Assisted-Verbal-Autopsy-for-Cause-of-Death-Determination" class="headerlink" title="LAVA: Language Model Assisted Verbal Autopsy for Cause-of-Death   Determination"></a>LAVA: Language Model Assisted Verbal Autopsy for Cause-of-Death   Determination</h2><p><strong>Authors:Yiqun T. Chen, Tyler H. McCormick, Li Liu, Abhirup Datta</strong></p>
<p>Verbal autopsy (VA) is a critical tool for estimating causes of death in resource-limited settings where medical certification is unavailable. This study presents LA-VA, a proof-of-concept pipeline that combines Large Language Models (LLMs) with traditional algorithmic approaches and embedding-based classification for improved cause-of-death prediction. Using the Population Health Metrics Research Consortium (PHMRC) dataset across three age categories (Adult: 7,580; Child: 1,960; Neonate: 2,438), we evaluate multiple approaches: GPT-5 predictions, LCVA baseline, text embeddings, and meta-learner ensembles. Our results demonstrate that GPT-5 achieves the highest individual performance with average test site accuracies of 48.6% (Adult), 50.5% (Child), and 53.5% (Neonate), outperforming traditional statistical machine learning baselines by 5-10%. Our findings suggest that simple off-the-shelf LLM-assisted approaches could substantially improve verbal autopsy accuracy, with important implications for global health surveillance in low-resource settings. </p>
<blockquote>
<p>言语病理（VA）是在医疗资源有限、无法进行医学认证的环境下估计死亡原因的重要工具。本研究提出了LA-VA概念验证流程，它结合了大型语言模型（LLM）与传统算法方法和基于嵌入的分类，以改进死亡原因预测。利用人口健康指标研究联盟（PHMRC）跨越三个年龄类别（成人：7580例；儿童：1960例；新生儿：2438例）的数据集，我们评估了多种方法：GPT-5预测、LCVA基线、文本嵌入和元学习者集合。我们的结果表明，GPT-5表现最佳的个人性能平均测试准确率分别为成人48.6%，儿童50.5%，新生儿53.5%，高出传统的统计学机器学习基线模型约5%-10%。我们的研究发现，简单的现成的大型语言模型辅助方法可能极大地提高言语病理的准确性，这对在低资源环境下进行全球健康监测具有重大意义。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.09602v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于自然语言处理的大型语言模型（LLM）在资源受限环境中对死亡原因估计具有关键作用。本研究提出LA-VA概念验证流程，结合传统算法和基于嵌入的分类方法，以提高死亡原因预测的准确性。利用人口健康指标研究协会（PHMRC）数据集，对GPT-5预测、LCVA基线、文本嵌入和元学习者集成等多种方法进行评价。结果显示GPT-5表现最佳，平均测试准确度分别为成人48.6%、儿童50.5%、新生儿53.5%，较传统统计机器学习基线高出5-10%。这提示我们，简单的即时LLM辅助方法可显著提高口头验尸的准确度，对低资源环境中的全球健康监测具有重要意义。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Verbal autopsy (VA)是资源受限环境中估算死亡原因的重要工具。</li>
<li>本研究提出了LA-VA概念验证流程，结合了大型语言模型（LLMs）、传统算法和基于嵌入的分类方法以提高死亡原因预测的准确性。</li>
<li>使用PHMRC数据集进行实证研究，包括成人、儿童和新生儿三个年龄组的数据。</li>
<li>GPT-5在各种评估方法中表现最佳，相比传统统计机器学习基线提高了5-10%的准确度。</li>
<li>简单即时的大型语言模型（LLM）辅助方法可以显著提高口头验尸的准确性。</li>
<li>LLM在死亡原因预测中的使用具有重要的全球健康监测意义，尤其是在低资源环境中。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.09602">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2509.09602v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2509.09602v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2509.09602v1/page_3_1.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="VQualA-2025-Challenge-on-Visual-Quality-Comparison-for-Large-Multimodal-Models-Methods-and-Results"><a href="#VQualA-2025-Challenge-on-Visual-Quality-Comparison-for-Large-Multimodal-Models-Methods-and-Results" class="headerlink" title="VQualA 2025 Challenge on Visual Quality Comparison for Large Multimodal   Models: Methods and Results"></a>VQualA 2025 Challenge on Visual Quality Comparison for Large Multimodal   Models: Methods and Results</h2><p><strong>Authors:Hanwei Zhu, Haoning Wu, Zicheng Zhang, Lingyu Zhu, Yixuan Li, Peilin Chen, Shiqi Wang, Chris Wei Zhou, Linhan Cao, Wei Sun, Xiangyang Zhu, Weixia Zhang, Yucheng Zhu, Jing Liu, Dandan Zhu, Guangtao Zhai, Xiongkuo Min, Zhichao Zhang, Xinyue Li, Shubo Xu, Anh Dao, Yifan Li, Hongyuan Yu, Jiaojiao Yi, Yiding Tian, Yupeng Wu, Feiran Sun, Lijuan Liao, Song Jiang</strong></p>
<p>This paper presents a summary of the VQualA 2025 Challenge on Visual Quality Comparison for Large Multimodal Models (LMMs), hosted as part of the ICCV 2025 Workshop on Visual Quality Assessment. The challenge aims to evaluate and enhance the ability of state-of-the-art LMMs to perform open-ended and detailed reasoning about visual quality differences across multiple images. To this end, the competition introduces a novel benchmark comprising thousands of coarse-to-fine grained visual quality comparison tasks, spanning single images, pairs, and multi-image groups. Each task requires models to provide accurate quality judgments. The competition emphasizes holistic evaluation protocols, including 2AFC-based binary preference and multi-choice questions (MCQs). Around 100 participants submitted entries, with five models demonstrating the emerging capabilities of instruction-tuned LMMs on quality assessment. This challenge marks a significant step toward open-domain visual quality reasoning and comparison and serves as a catalyst for future research on interpretable and human-aligned quality evaluation systems. </p>
<blockquote>
<p>本文介绍了VQualA 2025挑战赛的内容，该挑战赛作为ICCV 2025视觉质量评估研讨会的一部分，旨在评估和改进最新大型多模态模型（LMMs）在跨多张图像进行开放式和详细推理视觉质量差异方面的能力。为此，比赛引入了一个新的基准测试，包含从粗到细粒度的视觉质量比较任务数千个，涵盖单张图像、图像对和多图像组。每个任务都需要模型提供准确的质量判断。比赛强调整体评估协议，包括基于2AFC的二元偏好和多项选择题（MCQs）。约有100名参赛者提交了参赛作品，其中五种模型展示了指令调整型LMMs在质量评估方面的新兴能力。本次挑战赛是朝着开放式域视觉质量推理和比较迈出的重要一步，并为未来可解释性和人类对齐质量评估系统的研究起到了推动作用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.09190v1">PDF</a> ICCV VQualA Workshop 2025</p>
<p><strong>Summary</strong></p>
<p>视觉质量对比挑战——视觉质量对比对于大模态模型挑战（VQualA 2025）总结。该挑战旨在评估与提高先进的大模态模型在多个图像之间视觉质量差异的推理能力。通过引入包含数千个粗到细粒度视觉质量对比任务的新基准测试，包括单图像、图像对和多图像组，该挑战强调整体评估协议，包括基于2AFC的二元偏好和多选择题。大约一百名参与者提交参赛作品，其中五种模型展现了其在质量评估方面的潜力。此挑战是迈向开放式视觉质量推理和对比的重要一步，并为未来可解释和人类对齐的质量评估系统的研究起到了推动作用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VQualA 2025 Challenge旨在评估大模态模型对多个图像之间视觉质量差异的推理能力。</li>
<li>挑战引入了包含粗到细粒度视觉质量对比任务的新型基准测试。</li>
<li>该挑战强调全面的评估协议，包括基于2AFC的二元偏好和多选题形式。</li>
<li>有大约一百名参与者参与了此次挑战。</li>
<li>有五种模型在质量评估方面表现出潜力。</li>
<li>此挑战是视觉质量对比和推理领域的重要进展。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.09190">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2509.09190v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2509.09190v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2509.09190v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2509.09190v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MR-UIE-Multi-Perspective-Reasoning-with-Reinforcement-Learning-for-Universal-Information-Extraction"><a href="#MR-UIE-Multi-Perspective-Reasoning-with-Reinforcement-Learning-for-Universal-Information-Extraction" class="headerlink" title="MR-UIE: Multi-Perspective Reasoning with Reinforcement Learning for   Universal Information Extraction"></a>MR-UIE: Multi-Perspective Reasoning with Reinforcement Learning for   Universal Information Extraction</h2><p><strong>Authors:Zhongqiu Li, Shiquan Wang, Ruiyu Fang, Mengjiao Bao, Zhenhe Wu, Shuangyong Song, Yongxiang Li, Zhongjiang He</strong></p>
<p>Large language models (LLMs) demonstrate robust capabilities across diverse research domains. However, their performance in universal information extraction (UIE) remains insufficient, especially when tackling structured output scenarios that involve complex schema descriptions and require multi-step reasoning. While existing approaches enhance the performance of LLMs through in-context learning and instruction tuning, significant limitations nonetheless persist. To enhance the model’s generalization ability, we propose integrating reinforcement learning (RL) with multi-perspective reasoning for information extraction (IE) tasks. Our work transitions LLMs from passive extractors to active reasoners, enabling them to understand not only what to extract but also how to reason. Experiments conducted on multiple IE benchmarks demonstrate that MR-UIE consistently elevates extraction accuracy across domains and surpasses state-of-the-art methods on several datasets. Furthermore, incorporating multi-perspective reasoning into RL notably enhances generalization in complex IE tasks, underscoring the critical role of reasoning in challenging scenarios. </p>
<blockquote>
<p>大型语言模型（LLM）在多个研究领域中表现出了强大的能力。然而，它们在通用信息提取（UIE）方面的表现仍然不足，尤其是在处理涉及复杂模式描述和需要多步骤推理的结构化输出场景时。虽然现有方法通过上下文学习和指令微调提高了LLM的性能，但仍存在显著局限性。为了提高模型的泛化能力，我们提出了将强化学习（RL）与多视角推理相结合，用于信息提取（IE）任务。我们的工作使LLM从被动提取器转变为积极推理器，使它们不仅能够理解要提取的内容，而且能够理解如何进行推理。在多个信息提取基准测试上进行的实验表明，MR-UIE在跨域提取准确性方面表现一致，并在多个数据集上超过了最先进的方法。此外，将多视角推理融入强化学习，显著提高了复杂信息提取任务的泛化能力，突显了推理在具有挑战性场景中的关键作用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.09082v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在多个研究领域中表现出强大的能力，但在通用信息提取（UIE）方面的表现仍然不足，特别是在涉及复杂模式描述和需要多步骤推理的结构化输出场景中。为提高模型的泛化能力，提出了结合强化学习（RL）和多角度推理的信息提取（IE）任务方法。该方法使LLM从被动提取器转变为积极推理器，不仅理解需要提取的内容，还理解如何推理。在多个信息提取基准测试上的实验表明，MR-UIE在多个领域中的提取准确性不断提高，并在某些数据集上超越了最先进的方法。此外，将多角度推理融入RL显著提高了在复杂信息提取任务中的泛化能力，突显了推理在挑战场景中的关键作用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM在UIE方面的表现仍有待提高，特别是在处理复杂模式描述和多步骤推理的结构化输出场景方面。</li>
<li>通过结合强化学习（RL）和多角度推理，可以提高LLM在IE任务中的泛化能力。</li>
<li>MR-UIE方法使LLM从被动提取信息转变为积极推理，使其不仅理解提取内容，还理解如何推理。</li>
<li>MR-UIE在多个信息提取基准测试上的表现优于其他方法，显示出其有效性。</li>
<li>结合多角度推理的RL在复杂IE任务中显著提高泛化能力。</li>
<li>多角度推理在挑战场景中起到关键作用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.09082">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2509.09082v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2509.09082v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2509.09082v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Recurrence-Meets-Transformers-for-Universal-Multimodal-Retrieval"><a href="#Recurrence-Meets-Transformers-for-Universal-Multimodal-Retrieval" class="headerlink" title="Recurrence Meets Transformers for Universal Multimodal Retrieval"></a>Recurrence Meets Transformers for Universal Multimodal Retrieval</h2><p><strong>Authors:Davide Caffagni, Sara Sarto, Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara</strong></p>
<p>With the rapid advancement of multimodal retrieval and its application in LLMs and multimodal LLMs, increasingly complex retrieval tasks have emerged. Existing methods predominantly rely on task-specific fine-tuning of vision-language models and are limited to single-modality queries or documents. In this paper, we propose ReT-2, a unified retrieval model that supports multimodal queries, composed of both images and text, and searches across multimodal document collections where text and images coexist. ReT-2 leverages multi-layer representations and a recurrent Transformer architecture with LSTM-inspired gating mechanisms to dynamically integrate information across layers and modalities, capturing fine-grained visual and textual details. We evaluate ReT-2 on the challenging M2KR and M-BEIR benchmarks across different retrieval configurations. Results demonstrate that ReT-2 consistently achieves state-of-the-art performance across diverse settings, while offering faster inference and reduced memory usage compared to prior approaches. When integrated into retrieval-augmented generation pipelines, ReT-2 also improves downstream performance on Encyclopedic-VQA and InfoSeek datasets. Our source code and trained models are publicly available at: <a target="_blank" rel="noopener" href="https://github.com/aimagelab/ReT-2">https://github.com/aimagelab/ReT-2</a> </p>
<blockquote>
<p>随着多模态检索技术的快速发展及其在大型语言模型和多模态大型语言模型中的应用，出现了越来越多复杂的检索任务。现有方法主要依赖于针对特定任务的视觉语言模型的微调，并局限于单模态查询或文档。在本文中，我们提出了ReT-2，这是一个支持多模态查询的统一检索模型，由图像和文本组成，可以在包含文本和图像的跨模态文档集合中进行搜索。ReT-2利用多层表示和循环神经网络Transformer架构，结合LSTM启发的门控机制，动态地整合各层和各种信息源的信息，捕捉精细的视觉和文本细节。我们在具有挑战性的M2KR和M-BEIR基准测试集上对不同配置的检索系统进行了评估。结果表明，ReT-2在各种不同设置下均达到了最先进的性能水平，同时与先前的方法相比，推理速度更快，内存使用更少。当集成到检索增强生成管道时，ReT-2也在百科全书式视觉问答和信息检索数据集上的下游性能有所提升。我们的源代码和训练好的模型可在以下网址公开访问：<a target="_blank" rel="noopener" href="https://github.com/aimagelab/ReT-2">https://github.com/aimagelab/ReT-2</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.08897v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>随着多模态检索在LLM和多模态LLM中的迅速发展和应用，出现了越来越复杂的检索任务。现有方法主要依赖于针对特定任务的视觉语言模型的微调，并局限于单模态查询或文档。本文提出了ReT-2，一个支持图文结合的多模态查询的检索模型。ReT-2采用多层表示和循环Transformer架构，利用LSTM启发式的门控机制动态地整合跨层和跨模态的信息，捕捉精细的视觉和文本细节。在具有挑战性的M2KR和M-BEIR基准测试中，ReT-2在多种检索配置中均取得了最先进的性能表现，同时与先前的方法相比具有更快的推理速度和更低的内存使用。当集成到检索增强生成管道时，ReT-2在百科全书式VQA和信息检索数据集上的下游性能也得到了提升。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态检索领域正在快速发展，面临越来越复杂的检索任务。</li>
<li>现有方法主要依赖于特定任务的视觉语言模型微调，并局限于单模态查询。</li>
<li>ReT-2是一个支持多模态查询的检索模型，能够处理图文结合的信息。</li>
<li>ReT-2利用多层表示和循环Transformer架构，结合LSTM启发式的门控机制。</li>
<li>ReT-2在多个基准测试中取得了最先进的性能，包括M2KR和M-BEIR。</li>
<li>ReT-2在推理速度和内存使用方面相比以前的方法有优势。</li>
<li>ReT-2集成到检索增强生成管道后，在下游任务性能上有所提升，例如在Encyclopedic-VQA和InfoSeek数据集上。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.08897">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2509.08897v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2509.08897v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2509.08897v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2509.08897v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="AI-Self-preferencing-in-Algorithmic-Hiring-Empirical-Evidence-and-Insights"><a href="#AI-Self-preferencing-in-Algorithmic-Hiring-Empirical-Evidence-and-Insights" class="headerlink" title="AI Self-preferencing in Algorithmic Hiring: Empirical Evidence and   Insights"></a>AI Self-preferencing in Algorithmic Hiring: Empirical Evidence and   Insights</h2><p><strong>Authors:Jiannan Xu, Gujie Li, Jane Yi Jiang</strong></p>
<p>As generative artificial intelligence (AI) tools become widely adopted, large language models (LLMs) are increasingly involved on both sides of decision-making processes, ranging from hiring to content moderation. This dual adoption raises a critical question: do LLMs systematically favor content that resembles their own outputs? Prior research in computer science has identified self-preference bias – the tendency of LLMs to favor their own generated content – but its real-world implications have not been empirically evaluated. We focus on the hiring context, where job applicants often rely on LLMs to refine resumes, while employers deploy them to screen those same resumes. Using a large-scale controlled resume correspondence experiment, we find that LLMs consistently prefer resumes generated by themselves over those written by humans or produced by alternative models, even when content quality is controlled. The bias against human-written resumes is particularly substantial, with self-preference bias ranging from 68% to 88% across major commercial and open-source models. To assess labor market impact, we simulate realistic hiring pipelines across 24 occupations. These simulations show that candidates using the same LLM as the evaluator are 23% to 60% more likely to be shortlisted than equally qualified applicants submitting human-written resumes, with the largest disadvantages observed in business-related fields such as sales and accounting. We further demonstrate that this bias can be reduced by more than 50% through simple interventions targeting LLMs’ self-recognition capabilities. These findings highlight an emerging but previously overlooked risk in AI-assisted decision making and call for expanded frameworks of AI fairness that address not only demographic-based disparities, but also biases in AI-AI interactions. </p>
<blockquote>
<p>随着生成式人工智能（AI）工具被广泛采纳，大型语言模型（LLM）在决策过程中的作用日益凸显，从招聘到内容审核无一例外。这种双重采纳引发了一个关键问题：LLM是否会系统性地偏爱与其自身输出相似的内容？计算机科学领域的前期研究已经发现了自我偏好偏见——LLM倾向于偏爱其自己生成的内容——但其在实际世界的影响尚未得到实证评估。我们关注招聘背景，求职者常常依靠LLM来完善简历，而雇主则使用它们来筛选这些简历。通过大规模控制的简历对应实验，我们发现LLM始终偏爱由自己生成的简历，而非人类所写或由其他模型产生的简历，甚至在内容质量得到控制的情况下亦是如此。对人类撰写的简历的偏见尤为严重，主要商业和开源模型的自我偏好偏见范围从68%到88%。为了评估对劳动市场的影响，我们在24个职业中模拟了现实的招聘流程。这些模拟显示，使用与评估者相同LLM的候选人比提交人类撰写的简历的同等条件的申请人更有可能被列入选定名单，且在商业相关领域如销售和会计观察到最大的劣势。我们进一步证明，通过针对LLM的自我识别能力的简单干预，可以减少超过一半的偏见。这些发现突出了人工智能辅助决策制定中出现但以前未被重视的风险，并呼吁扩大人工智能公平性的框架，不仅要解决基于人口统计的差距，还要解决人工智能之间的偏见问题。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.00462v2">PDF</a> This paper has been accepted as a non-archival submission at EAAMO   2025 and AIES 2025</p>
<p><strong>摘要</strong></p>
<p>随着生成式人工智能工具（AI）的广泛应用，大型语言模型（LLM）越来越多地参与到决策过程的各个环节，从招聘到内容审核不一而足。这种双重采用引发了一个关键问题：LLM是否会系统性地偏爱与其自身输出相似的内容？计算机科学领域的前期研究已经发现了LLM的自我偏好偏差——即LLM倾向于青睐自己生成的内容，但其现实影响尚未经过实证评估。本研究聚焦于招聘环节，应聘者往往依赖LLM来完善简历，而雇主则使用它们来筛选这些简历。通过大规模控制性简历对应实验，我们发现LLM始终偏好由自身生成的简历，而非人类撰写或由其他模型产生的简历，甚至在内容质量得到控制的情况下亦是如此。相较于人类撰写的简历，对后者的偏见尤为显著，在不同的大型商业和开源模型中，自我偏好偏差范围在68%至88%之间。为了评估对劳动力市场的影响，我们对24类职业的现实招聘流程进行了模拟。这些模拟显示，使用与评估者相同LLM的候选人被短名单列出的可能性比提交人类撰写简历的同等资格申请人高出23%至60%，在销售和会计等商业相关领域观察到的不利情况最为严重。我们进一步证明，通过针对LLM的自我识别能力的简单干预措施，这种偏见可以减少超过55%。这些发现突显了人工智能辅助决策制定中出现的新兴但以前被忽视的风险，并呼吁扩大人工智能公平性的框架，不仅要解决基于人口统计数据的差异，还要解决人工智能之间的互动偏见。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>大型语言模型（LLM）在招聘等决策过程中广泛应用。</li>
<li>LLM表现出自我偏好偏差，更倾向于选择自身生成的文本内容。</li>
<li>在控制内容质量的情况下，LLM对由其他模型或人类撰写的简历存在显著偏见。</li>
<li>使用与评估者相同LLM的应聘者在招聘流程中被短名单列出的可能性更高。</li>
<li>这种偏见在商业和开源模型中尤为显著，且可能对某些职业造成不利影响。</li>
<li>通过简单干预措施，可以有效减少LLM的自我偏好偏差。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.00462">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2509.00462v2/page_0_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Improving-Alignment-in-LVLMs-with-Debiased-Self-Judgment"><a href="#Improving-Alignment-in-LVLMs-with-Debiased-Self-Judgment" class="headerlink" title="Improving Alignment in LVLMs with Debiased Self-Judgment"></a>Improving Alignment in LVLMs with Debiased Self-Judgment</h2><p><strong>Authors:Sihan Yang, Chenhang Cui, Zihao Zhao, Yiyang Zhou, Weilong Yan, Ying Wei, Huaxiu Yao</strong></p>
<p>The rapid advancements in Large Language Models (LLMs) and Large Visual-Language Models (LVLMs) have opened up new opportunities for integrating visual and linguistic modalities. However, effectively aligning these modalities remains challenging, often leading to hallucinations–where generated outputs are not grounded in the visual input–and raising safety concerns across various domains. Existing alignment methods, such as instruction tuning and preference tuning, often rely on external datasets, human annotations, or complex post-processing, which limit scalability and increase costs. To address these challenges, we propose a novel approach that generates the debiased self-judgment score, a self-evaluation metric created internally by the model without relying on external resources. This enables the model to autonomously improve alignment. Our method enhances both decoding strategies and preference tuning processes, resulting in reduced hallucinations, enhanced safety, and improved overall capability. Empirical results show that our approach significantly outperforms traditional methods, offering a more effective solution for aligning LVLMs. </p>
<blockquote>
<p>大型语言模型（LLMs）和大型视觉语言模型（LVLMs）的快速发展为整合视觉和语言模式提供了新的机会。然而，有效地对齐这些模式仍然具有挑战性，这常常导致生成的输出没有基于视觉输入，并且在各个领域引发安全担忧。现有的对齐方法，如指令调整和偏好调整，通常依赖于外部数据集、人工注释或复杂的后处理，这限制了可扩展性并增加了成本。为了解决这些挑战，我们提出了一种新方法，生成去偏自我判断分数，这是一个由模型内部创建的自评指标，无需依赖外部资源。这使得模型能够自主地改进对齐。我们的方法提高了解码策略和偏好调整过程，导致减少了幻觉、增强了安全性并提高了整体能力。经验结果表明，我们的方法显著优于传统方法，为LVLMs的对齐提供了更有效的解决方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.20655v2">PDF</a> EMNLP 2025 Findings</p>
<p><strong>Summary</strong></p>
<p>随着大型语言模型（LLMs）和大型视觉语言模型（LVLMs）的快速发展，整合视觉和语言学模态的新机会已经打开。然而，有效对齐这些模态仍然具有挑战性，可能导致生成的输出不基于视觉输入，并在不同领域引发安全问题。现有对齐方法常依赖外部数据集、人工注释或复杂后处理，限制了可扩展性并增加了成本。为解决这些挑战，我们提出了一种新方法，生成偏误自我判断分数，这是一个由模型内部创建的自我评价指标，无需依赖外部资源。这使模型能够自主改善对齐。我们的方法改进了解码策略和偏好调整过程，减少了幻视现象，增强了安全性并提高了整体能力。经验结果表明，我们的方法显著优于传统方法，为LVLMs的对齐提供了更有效的解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）和大型视觉语言模型（LVLMs）的整合带来了新的机会，但对齐视觉和语言学模态仍然具有挑战性。</li>
<li>现有对齐方法存在依赖外部资源、成本高和扩展性有限的问题。</li>
<li>提出了一种新的自我评价指标——偏误自我判断分数，无需依赖外部资源，使模型能够自主改善对齐。</li>
<li>新方法改进了解码策略和偏好调整过程，减少了幻视现象。</li>
<li>新方法增强了安全性并提高了模型的整体能力。</li>
<li>实证研究结果显示，新方法在LVLMs的对齐上显著优于传统方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.20655">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2508.20655v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2508.20655v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2508.20655v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2508.20655v2/page_4_0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Can-Large-Language-Models-Understand-As-Well-As-Apply-Patent-Regulations-to-Pass-a-Hands-On-Patent-Attorney-Test"><a href="#Can-Large-Language-Models-Understand-As-Well-As-Apply-Patent-Regulations-to-Pass-a-Hands-On-Patent-Attorney-Test" class="headerlink" title="Can Large Language Models Understand As Well As Apply Patent Regulations   to Pass a Hands-On Patent Attorney Test?"></a>Can Large Language Models Understand As Well As Apply Patent Regulations   to Pass a Hands-On Patent Attorney Test?</h2><p><strong>Authors:Bhakti Khera, Rezvan Alamian, Pascal A. Scherz, Stephan M. Goetz</strong></p>
<p>The legal field already uses various large language models (LLMs) in actual applications, but their quantitative performance and reasons for it are underexplored. We evaluated several open-source and proprietary LLMs – including GPT-series, Anthropic, Deepseek and Llama-3, variants – on parts of the European Qualifying Examination (EQE) for future European Patent Attorneys. OpenAI o1 led with 0.82 accuracy and 0.81 F1 score, whereas (Amazon Web Services) AWS Llama 3.1 8B lagged at 0.50 accuracy, and a Python-deployed Llama 3.1 8B scored 0.55. The latter two are within the range of mere guessing for the two-answer forced-choice design. None of the evaluated models could have passed the examination fully, as accuracy never exceeded the average threshold of 0.90 required for professional-level standards – also not models that are regularly promoted for their assumed beyond-PhD- and bar-admitted-lawyer-level performance. GPT-4o excelled at integrating text and graphics, while Claude 3 Opus often lost formatting coherence. Human patent experts evaluated the textual justifications and uncovered various critical shortcomings of each model. They valued clarity and legal rationale over the raw correctness of the answers, which revealed misalignment between automatic metrics and expert judgment. Model outputs were sensitive to modest temperature changes and prompt wording, which underscores the remaining necessity of expert oversight. Future work should target logical consistency, robust multimodality, and adaptive prompting to approach human-level patent proficiency. In summary, despite the outstanding performance of recent large models, the general public might overestimate their performance. The field has a long way to go to develop a virtual patent attorney. This paper wants to point out several specific limitations that need solutions. </p>
<blockquote>
<p>在法律领域，虽然已经在实际应用中使用各种大型语言模型（LLM），但它们的定量性能和原因尚未得到充分探索。我们对几个开源和专有的大型语言模型进行了评估，包括GPT系列、Anthropic、Deepseek和Llama-3的变种，以及欧洲专利代理人未来资格欧洲考试（EQE）的部分内容。OpenAI o1以0.82的准确率和0.81的F1分数位居榜首，而亚马逊网络服务（AWS）Llama 3.1 8B的准确率为0.5准确率和0.5系统的准确率为0.5，后者两个模型的性能仅在两选一的猜测范围内。经过评估的模型中，没有任何一个能够完全通过考试，因为准确性从未超过专业级别标准所要求的平均阈值0.9，而且也不会对宣传中所称的超越博士和执业律师水平的性能产生影响。GPT-4在整合文本和图形方面表现出色，而Claude 3 Opus经常失去格式连贯性。人类专利专家对文本依据进行了评估，发现了每个模型的多种关键缺陷。他们重视答案的清晰度和法律合理性，而非答案本身的正确性，这揭示了自动指标与专家判断之间的不一致。模型输出对微小的温度变化和提示语比较敏感，这强调了专家监督的剩余必要性。未来的工作应该致力于逻辑连贯性、稳健的多媒体形式和适应性提示来接近人类水平的专利能力。总之，尽管最近的大型模型表现出色，公众可能会高估它们的性能。法律领域在开发虚拟专利代理人方面还有很长的路要走。本文想指出需要解决的几个具体局限性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10576v2">PDF</a> 41 pages, 21 figures</p>
<p><strong>Summary</strong></p>
<p>本论文对多种大型语言模型（LLM）在法律领域的应用进行了评估。在针对欧洲专利律师资格考试的测试中，各模型的性能表现差异显著。尽管高级模型如OpenAI o1表现出色，但其准确度仍未能达到专业水平的要求。模型的回答在法律理论和专家评估中存在诸多缺陷，凸显了自动评估与专家判断之间的差异。未来研究需关注逻辑一致性、多模态能力和适应性提示设计。总体而言，公众对模型性能可能存在过度估计，法律虚拟代理人研发尚需更多努力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>法律领域已广泛应用大型语言模型（LLM），但它们的定量性能和原因尚未得到充分探索。</li>
<li>在欧洲专利律师资格考试的测试中，不同LLM模型表现差异显著，没有模型能够完全通过考试。</li>
<li>高级模型的准确度仍未达到专业水平的要求，需要更高的准确度以达到专业标准。</li>
<li>模型在法律理论和实践方面存在缺陷，需要更细致的校准以适应专业领域的需求。</li>
<li>专家判断与法律理论的融合是目前大型语言模型所面临的挑战之一。</li>
<li>模型输出对温度变化和提示措辞敏感，强调专家监督的必要性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10576">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2507.10576v2/page_0_0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Scaling-LLM-Planning-NL2FLOW-for-Parametric-Problem-Generation-and-Rigorous-Evaluation"><a href="#Scaling-LLM-Planning-NL2FLOW-for-Parametric-Problem-Generation-and-Rigorous-Evaluation" class="headerlink" title="Scaling LLM Planning: NL2FLOW for Parametric Problem Generation and   Rigorous Evaluation"></a>Scaling LLM Planning: NL2FLOW for Parametric Problem Generation and   Rigorous Evaluation</h2><p><strong>Authors:Jungkoo Kang</strong></p>
<p>Robust workflow composition is critical for effective agent performance, yet progress in Large Language Model (LLM) planning and reasoning is hindered by a scarcity of scalable evaluation data. This work introduces NL2Flow, a fully automated pipeline for generating and evaluating workflow planning problems. NL2Flow generates problems parametrically in a structured intermediate representation, translating them into both natural language and formal PDDL. I evaluate several open-source, instruct-tuned LLMs on a dataset of 2296 low-difficulty problems generated by NL2Flow. Results demonstrate that the best-performing model achieved 86% success in generating valid plans and 69% in generating optimal plans (for solvable problems). Regression analysis shows that the influence of problem characteristics on plan generation is contingent on both model and prompt design. Importantly, translating natural language problems into a structured JSON representation prior to symbolic planning significantly improved success rates, suggesting a benefit from neuro-symbolic integration. These findings underscore the importance of understanding error sources within LLM reasoning as systems scale to more complex tasks. As LLM reasoning scales to increasingly complex problems, understanding the shifting bottlenecks and sources of error within these systems will be crucial. </p>
<blockquote>
<p>强大的工作流程组合对于提高代理性能至关重要，然而，由于缺乏可扩展的评估数据，大型语言模型（LLM）的规划和推理工作进展受到了阻碍。本文介绍了NL2Flow，这是一个全自动化的管道，用于生成和评估工作流程规划问题。NL2Flow以结构化的中间表示形式参数化生成问题，并将其转换为自然语言描述和正式PDDL描述。我在由NL2Flow生成的包含有难度的低难度问题数据集上评估了几个开源的指令微调LLM。结果表明，表现最好的模型在生成有效计划方面取得了86%的成功率，在生成最优计划方面取得了69%（针对可解决的问题）。回归分析表明，问题特征对计划生成的影响取决于模型和提示设计。重要的是，在符号规划之前将自然语言问题转换为结构化JSON表示形式显著提高了成功率，这表明神经符号融合具有优势。这些发现强调了随着LLM推理系统处理越来越复杂的任务时，了解系统内部错误来源的重要性。随着LLM推理处理越来越复杂的问题时，了解这些系统内不断变化的瓶颈和错误来源将至关重要。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02253v5">PDF</a> 31 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>本文介绍了NL2Flow这一全自动管道，用于生成和评估工作流程规划问题。NL2Flow能够参数化生成问题，并将其转化为自然语言与正式PDDL，评估了多个开源指令微调LLM模型在由NL2Flow生成的2296个低难度问题数据集上的表现。最佳模型生成有效计划的成功率为86%，可解决问题生成最优计划的成功率为69%。回归分析表明，问题特性对计划生成的影响取决于模型和提示设计。将自然语言问题转化为结构化JSON表示再进行符号规划，能显著提高成功率，显示出神经符号融合的益处。这些发现强调了在大规模LLM推理中理解错误来源的重要性。随着LLM推理逐步解决日益复杂的问题，理解这些系统内部不断变化的瓶颈和错误来源将是关键。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NL2Flow是首个全自动管道，能够生成并评估工作流程规划问题。</li>
<li>最佳LLM模型在生成有效计划和最优计划方面取得了显著成果。</li>
<li>回归分析了问题特性、模型及提示设计对计划生成的影响。</li>
<li>将自然语言问题转化为结构化JSON表示再进行符号规划，能显著提高计划生成的成功率。</li>
<li>研究强调了在大规模LLM推理中理解错误来源的重要性。</li>
<li>随着LLM推理处理的问题日益复杂，理解系统内部的瓶颈和错误来源将成为关键。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02253">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2507.02253v5/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2507.02253v5/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2507.02253v5/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2507.02253v5/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2507.02253v5/page_5_0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Development-and-Comparative-Evaluation-of-Three-Artificial-Intelligence-Models-NLP-LLM-JEPA-for-Predicting-Triage-in-Emergency-Departments-A-7-Month-Retrospective-Proof-of-Concept"><a href="#Development-and-Comparative-Evaluation-of-Three-Artificial-Intelligence-Models-NLP-LLM-JEPA-for-Predicting-Triage-in-Emergency-Departments-A-7-Month-Retrospective-Proof-of-Concept" class="headerlink" title="Development and Comparative Evaluation of Three Artificial Intelligence   Models (NLP, LLM, JEPA) for Predicting Triage in Emergency Departments: A   7-Month Retrospective Proof-of-Concept"></a>Development and Comparative Evaluation of Three Artificial Intelligence   Models (NLP, LLM, JEPA) for Predicting Triage in Emergency Departments: A   7-Month Retrospective Proof-of-Concept</h2><p><strong>Authors:Edouard Lansiaux, Ramy Azzouz, Emmanuel Chazard, Amélie Vromant, Eric Wiel</strong></p>
<p>Emergency departments struggle with persistent triage errors, especially undertriage and overtriage, which are aggravated by growing patient volumes and staff shortages. This study evaluated three AI models [TRIAGEMASTER (NLP), URGENTIAPARSE (LLM), and EMERGINET (JEPA)] against the FRENCH triage scale and nurse practice, using seven months of adult triage data from Roger Salengro Hospital in Lille, France. Among the models, the LLM-based URGENTIAPARSE consistently outperformed both AI alternatives and nurse triage, achieving the highest accuracy (F1-score 0.900, AUC-ROC 0.879) and superior performance in predicting hospitalization needs (GEMSA). Its robustness across structured data and raw transcripts highlighted the advantage of LLM architectures in abstracting patient information. Overall, the findings suggest that integrating LLM-based AI into emergency department workflows could significantly enhance patient safety and operational efficiency, though successful adoption will depend on addressing limitations and ensuring ethical transparency. </p>
<blockquote>
<p>急诊科持续面临分诊错误问题，特别是低估病情和过度评估病情的情况。这些问题因患者数量增加和人员短缺而加剧。本研究使用法国里尔市罗杰·萨伦格洛医院为期七个月的成人分诊数据，以法国FRENCH分级标准以及护士实践为标准，评估了三种人工智能模型（TRIAGEMASTER（NLP）、URGENTIAPARSE（LLM）和EMERGINET（JEPA））的表现。在这些模型中，基于LLM的URGENTIAPARSE始终表现优于其他两种AI模型和护士分诊，准确率最高（F1分数为0.900，AUC-ROC为0.879），并且在预测住院需求方面表现优异（GEMSA）。其在结构化数据和原始文本转录中的稳健性凸显了大型语言模型在提取患者信息方面的优势。总体而言，研究结果表明将基于大型语言模型的人工智能整合到急诊科工作流程中可能会显著提高患者安全和运营效率，但要成功实施并依赖解决限制因素并确保伦理透明度。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01080v2">PDF</a> 13 pages, 7 figures, 3 tables</p>
<p><strong>Summary</strong></p>
<p>本文研究了三种AI模型（TRIAGEMASTER（NLP）、URGENTIAPARSE（LLM）和EMERGINET（JEPA））在急诊部门应用的表现，特别是在应对日益增长的患者数量和医护人员短缺导致的持续性分流错误问题。研究结果表明，基于LLM的URGENTIAPARSE模型表现最为出色，其准确性最高（F1分数为0.900，AUC-ROC为0.879），并在预测住院需求方面展现出卓越性能。该模型的稳健性使其成为抽象患者信息领域的理想选择。总体而言，将基于LLM的AI集成到急诊部门工作流程中有望显著提高患者安全和运营效率。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>急诊部门面临持续的分流错误问题，特别是低分流和高分流，受到患者数量增长和人员短缺的加剧。</li>
<li>三种AI模型（TRIAGEMASTER、URGENTIAPARSE和EMERGINET）被评估用于解决这一问题。</li>
<li>URGENTIAPARSE（基于LLM）在准确性上表现最佳，与护士分流相比具有更高的F1分数和AUC-ROC值。</li>
<li>URGENTIAPARSE在预测住院需求方面展现出卓越性能。</li>
<li>LLM架构在抽象患者信息方面具有优势，稳健性高，适用于处理结构数据和原始转录数据。</li>
<li>集成LLM基于AI的急诊部门工作流程有望增强患者安全和运营效率。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01080">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2507.01080v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2507.01080v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2507.01080v2/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2507.01080v2/page_5_1.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="LLMs-for-sensory-motor-control-Combining-in-context-and-iterative-learning"><a href="#LLMs-for-sensory-motor-control-Combining-in-context-and-iterative-learning" class="headerlink" title="LLMs for sensory-motor control: Combining in-context and iterative   learning"></a>LLMs for sensory-motor control: Combining in-context and iterative   learning</h2><p><strong>Authors:Jônata Tyska Carvalho, Stefano Nolfi</strong></p>
<p>We propose a method that enables large language models (LLMs) to control embodied agents by directly mapping continuous observation vectors to continuous action vectors. At the outset, the LLMs generate a control strategy based on a textual description of the agent, its environment, and the intended goal. This strategy is then iteratively refined through a learning process in which the LLMs are repeatedly prompted to improve the current strategy, using performance feedback and sensory-motor data collected during its evaluation. The method is validated on classic control tasks from the Gymnasium library and the inverted pendulum task from the MuJoCo library. The approach proves effective with relatively compact models such as Gpt-oss:120b and Qwen2.5:72b. In most cases, it successfully identifies optimal or near-optimal solutions by integrating symbolic knowledge derived through reasoning with sub-symbolic sensory-motor data gathered as the agent interacts with its environment. </p>
<blockquote>
<p>我们提出了一种方法，使大型语言模型（LLM）能够通过直接将连续的观察向量映射到连续的动作向量来控制实体代理。一开始，LLM基于代理的文本描述、其环境以及预期目标生成控制策略。然后，通过学习过程迭代地完善这一策略，在此过程中，LLM被反复提示以性能反馈和评估期间收集的感官运动数据来改善当前策略。该方法在Gymnasium库的经典控制任务和MuJoCo库的倒立摆任务上进行了验证。该方法在相对紧凑的模型（如Gpt-oss:120b和Qwen2.5:72b）上证明有效。在大多数情况下，它通过整合通过推理获得的符号知识与代理在与环境交互过程中收集的亚符号感官运动数据，成功找到最优解或近似最优解。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04867v2">PDF</a> Article updated with results from gpt-oss:120b. 24 pages (13 pages   are from appendix), 6 figures, code for experiments replication and   supplementary material provided at   <a target="_blank" rel="noopener" href="https://github.com/jtyska/llm-robotics-article/">https://github.com/jtyska/llm-robotics-article/</a></p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）可以通过将连续观测向量直接映射到连续动作向量来控制实体代理。LLM基于代理的文本描述、环境及目标生成控制策略，并通过收集性能反馈和感觉运动数据来迭代优化这一策略。该方法在Gymnasium和MuJoCo库的经典控制任务上得到验证，使用Gpt-oss:120b和Qwen2.5:72b等较紧凑的模型时效果显著。该方法结合了符号知识和代理与环境交互中的感觉运动数据，通常能找出最优或接近最优的解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM能够控制实体代理，通过将连续观测向量映射到连续动作向量。</li>
<li>LLM基于文本描述生成控制策略，并考虑代理、环境和目标。</li>
<li>通过性能反馈和感觉运动数据的收集，LLM能够迭代优化控制策略。</li>
<li>这种方法在经典控制任务上进行了验证，如Gymnasium和MuJoCo库的任务。</li>
<li>相对紧凑的模型，如Gpt-oss:120b和Qwen2.5:72b，在应用中表现出有效性。</li>
<li>该方法结合了符号知识和代理与环境交互中的感觉运动数据。</li>
<li>通常能找出最优或接近最优的解决方案。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04867">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2506.04867v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2506.04867v2/page_3_0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Toward-Generation-of-Test-Cases-from-Task-Descriptions-via-History-aware-Planning"><a href="#Toward-Generation-of-Test-Cases-from-Task-Descriptions-via-History-aware-Planning" class="headerlink" title="Toward Generation of Test Cases from Task Descriptions via History-aware   Planning"></a>Toward Generation of Test Cases from Task Descriptions via History-aware   Planning</h2><p><strong>Authors:Duy Cao, Phu Nguyen, Vy Le, Tien N. Nguyen, Vu Nguyen</strong></p>
<p>In automated web testing, generating test scripts from natural language task descriptions is crucial for enhancing the test generation process. This activity involves creating the correct sequences of actions to form test scripts for future testing activities. Current state-of-the-art approaches are limited in generating these action sequences, as they either demand substantial manual effort for human demonstrations or fail to consider the history of previous web content and actions to decide the next action. In this paper, we introduce HxAgent, an iterative large language model agent planning approach that determines the next action based on: 1) observations of the current contents and feasible actions, 2) short-term memory of previous web states and actions, and 3) long-term experience with (in)correct action sequences. The agent generates a sequence of actions to perform a given task, which is effectively an automated test case to verify the task. We conducted an extensive empirical evaluation of HxAgent using two datasets. On the MiniWoB++ dataset, our approach achieves 97% exact-match accuracy that is comparable to the best baselines while eliminating the need for human demonstrations required by those methods. For complex tasks requiring navigation through multiple actions and screens, HxAgent achieves an average 82% exact-match. On the second dataset, comprising 350 task instances across seven popular websites, including YouTube, LinkedIn, Facebook, and Google, HxAgent achieves high performance, with 87% of the action sequences exactly matching the ground truth and a prefix-match of 93%, outperforming the baseline by 59%. </p>
<blockquote>
<p>在自动化网页测试中，从自然语言任务描述生成测试脚本对于增强测试生成过程至关重要。这项活动涉及创建正确的操作序列以形成用于未来测试活动的测试脚本。当前最先进的技术方法在生成这些操作序列方面存在局限性，因为它们要么需要大量的人工演示，要么无法考虑以前的网页内容和操作历史来决定下一个操作。在本文中，我们介绍了HxAgent，这是一个迭代的大型语言模型代理规划方法，它根据以下三个因素确定下一个操作：1）对当前内容和可行操作的观察；2）对先前网页状态和操作的短期记忆；以及3）对（正确或错误的）操作序列的长期经验。代理生成执行给定任务的操作序列，这实际上是验证任务的自动化测试用例。我们使用两个数据集对HxAgent进行了广泛的实证评估。在MiniWoB++数据集上，我们的方法达到了97%的精确匹配率，与最佳基线相当，同时消除了这些方法所需要的人工演示。对于需要多次操作和屏幕导航的复杂任务，HxAgent的平均精确匹配率达到82%。在第二个数据集上，该数据集包含350个任务实例，涵盖YouTube、LinkedIn、Facebook和Google等七个流行网站，HxAgent表现出高性能，其操作序列的精确匹配率达到87%，前缀匹配率为93%，超过了基线方法的59%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14336v2">PDF</a> Change the method and experimentation</p>
<p><strong>Summary</strong></p>
<p>基于自然语言任务描述自动生成web测试脚本，是提升测试生成过程效率的关键步骤。当前主流方法在这方面有所局限，它们要么需要大量人工示范，要么不考虑以往的网页内容和行为来决定下一步行动。本文提出一种名为HxAgent的大型语言模型代理规划方法，它根据当前内容及其可行操作、短期内存中的以往web状态和操作以及长期的对（正确或错误）操作序列的经验来决定下一步操作。此方法在一项测试中有效生成完成特定任务的操作序列，从而进行自动化测试验证任务。广泛的实证评估表明，在复杂任务场景下，如在MiniWoB++数据集上，HxAgent取得了与最佳基线相当的精确匹配准确率（达到97%），且无需人工示范；在另一包含多个网站任务的第二数据集上，其精确匹配率高达87%，前缀匹配率高达93%，显著优于基线标准。HXAgent的有效性和高效性显示出了它在自动化Web测试领域的潜力与前景。HXAgent表现出优越的性能，有望成为自动化Web测试领域的创新解决方案。 </p>
<p><strong>Key Takeaways</strong></p>
<p>一、自动生成web测试脚本是提升自动化web测试效率的关键环节。当前主流方法存在局限性，需要人工示范或忽略历史信息。</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14336">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2504.14336v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2504.14336v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2504.14336v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Transforming-Wearable-Data-into-Personal-Health-Insights-using-Large-Language-Model-Agents"><a href="#Transforming-Wearable-Data-into-Personal-Health-Insights-using-Large-Language-Model-Agents" class="headerlink" title="Transforming Wearable Data into Personal Health Insights using Large   Language Model Agents"></a>Transforming Wearable Data into Personal Health Insights using Large   Language Model Agents</h2><p><strong>Authors:Mike A. Merrill, Akshay Paruchuri, Naghmeh Rezaei, Geza Kovacs, Javier Perez, Yun Liu, Erik Schenck, Nova Hammerquist, Jake Sunshine, Shyam Tailor, Kumar Ayush, Hao-Wei Su, Qian He, Cory Y. McLean, Mark Malhotra, Shwetak Patel, Jiening Zhan, Tim Althoff, Daniel McDuff, Xin Liu</strong></p>
<p>Deriving personalized insights from popular wearable trackers requires complex numerical reasoning that challenges standard LLMs, necessitating tool-based approaches like code generation. Large language model (LLM) agents present a promising yet largely untapped solution for this analysis at scale. We introduce the Personal Health Insights Agent (PHIA), a system leveraging multistep reasoning with code generation and information retrieval to analyze and interpret behavioral health data. To test its capabilities, we create and share two benchmark datasets with over 4000 health insights questions. A 650-hour human expert evaluation shows that PHIA significantly outperforms a strong code generation baseline, achieving 84% accuracy on objective, numerical questions and, for open-ended ones, earning 83% favorable ratings while being twice as likely to achieve the highest quality rating. This work can advance behavioral health by empowering individuals to understand their data, enabling a new era of accessible, personalized, and data-driven wellness for the wider population. </p>
<blockquote>
<p>从流行的可穿戴跟踪器获取个性化洞察需要复杂的数值推理，这挑战了标准的大型语言模型，需要基于工具的方法，如代码生成。大型语言模型（LLM）代理人为这种大规模分析提供了有前景但尚未充分利用的解决方案。我们介绍了个人健康洞察代理（PHIA），这是一个系统，利用多步骤推理和代码生成以及信息检索来分析并解释行为健康数据。为了测试其能力，我们创建并分享了包含超过4000个健康洞察问题两个基准数据集。一项为期650小时的人类专家评估表明，PHIA显著优于强大的代码生成基线，在客观数值问题上达到84%的准确率，在开放性问题上获得83%的好评，同时有两次机会获得最高质量评分。这项工作可以通过帮助个人理解他们的数据，为更广泛的人群开启一个可访问、个性化、数据驱动的健康新时代，从而促进行为健康的发展。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.06464v4">PDF</a> 53 pages, 7 main figures, 2 main tables, accepted to Nature   Communications</p>
<p><strong>Summary</strong></p>
<p>文章介绍了穿戴式追踪器的个人见解的派生对标准LLM提出挑战，需要通过基于工具的方法如代码生成来解决这一问题。提出了个人健康见解代理（PHIA）系统，该系统利用多步骤推理和代码生成以及信息检索来分析并解释行为健康数据。测试表明，PHIA在客观数值问题和开放性问题上分别达到了84%和83%的好评率，且在高质量评级方面高出基线模型两倍。这为行为健康领域带来了个性化、数据驱动的新时代。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM在解读可穿戴设备健康数据上面临挑战，需要更复杂的数值推理和工具化方法如代码生成来应对。</li>
<li>提出个人健康见解代理（PHIA）系统，利用多步骤推理和代码生成技术来分析解读健康数据。</li>
<li>PHIA系统在处理客观数值问题和开放性问题时表现出色，准确率较高。</li>
<li>PHIA系统相较于基线模型在高质量评级方面表现更优秀。</li>
<li>PHIA系统对于理解个人健康数据具有巨大潜力，能够推动行为健康领域的发展。</li>
<li>文章创建并分享了包含超过4000个健康见解问题的两个基准数据集以测试系统的能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.06464">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2406.06464v4/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2406.06464v4/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2406.06464v4/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2406.06464v4/page_4_0.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Osprey-Pixel-Understanding-with-Visual-Instruction-Tuning"><a href="#Osprey-Pixel-Understanding-with-Visual-Instruction-Tuning" class="headerlink" title="Osprey: Pixel Understanding with Visual Instruction Tuning"></a>Osprey: Pixel Understanding with Visual Instruction Tuning</h2><p><strong>Authors:Yuqian Yuan, Wentong Li, Jian Liu, Dongqi Tang, Xinjie Luo, Chi Qin, Lei Zhang, Jianke Zhu</strong></p>
<p>Multimodal large language models (MLLMs) have recently achieved impressive general-purpose vision-language capabilities through visual instruction tuning. However, current MLLMs primarily focus on image-level or box-level understanding, falling short in achieving fine-grained vision-language alignment at pixel level. Besides, the lack of mask-based instruction data limits their advancements. In this paper, we propose Osprey, a mask-text instruction tuning approach, to extend MLLMs by incorporating fine-grained mask regions into language instruction, aiming at achieving pixel-wise visual understanding. To achieve this goal, we first meticulously curate a mask-based region-text dataset with 724K samples, and then design a vision-language model by injecting pixel-level representation into LLM. Specifically, Osprey adopts a convolutional CLIP backbone as the vision encoder and employs a mask-aware visual extractor to extract precise visual mask features from high resolution input. Experimental results demonstrate Osprey’s superiority in various region understanding tasks, showcasing its new capability for pixel-level instruction tuning. In particular, Osprey can be integrated with Segment Anything Model (SAM) seamlessly to obtain multi-granularity semantics. The source code, dataset and demo can be found at <a target="_blank" rel="noopener" href="https://github.com/CircleRadon/Osprey">https://github.com/CircleRadon/Osprey</a>. </p>
<blockquote>
<p>多模态大型语言模型（MLLMs）最近通过视觉指令微调，实现了令人印象深刻的通用视觉语言功能。然而，当前的MLLMs主要关注图像级别或框级别的理解，在像素级别的精细视觉语言对齐方面存在不足。此外，缺乏基于mask的指令数据限制了其发展。在本文中，我们提出了Osprey，这是一种基于mask文本的指令微调方法，通过将精细的mask区域融入语言指令来扩展MLLMs，旨在实现像素级的视觉理解。为了实现这一目标，我们首先精心制作了一个基于mask的区域文本数据集，包含724K样本，然后设计了一个通过注入像素级表示到LLM中的视觉语言模型。具体来说，Osprey采用卷积CLIP主干作为视觉编码器，并采用一个掩码感知的视觉提取器从高分辨率输入中提取精确的视觉mask特征。实验结果表明，Osprey在各种区域理解任务中的优越性，展示其像素级指令微调的新能力。特别是，Osprey可以与Segment Anything Model（SAM）无缝集成，以获得多粒度语义。相关源代码、数据集和演示可在<a target="_blank" rel="noopener" href="https://github.com/CircleRadon/Osprey%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/CircleRadon/Osprey找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.10032v4">PDF</a> CVPR2024, Code and Demo link:<a target="_blank" rel="noopener" href="https://github.com/CircleRadon/Osprey">https://github.com/CircleRadon/Osprey</a></p>
<p><strong>Summary</strong></p>
<p>多模态大型语言模型（MLLM）通过视觉指令微调获得了通用的视觉-语言功能，但在精细粒度的视觉-语言对齐方面存在不足。本文提出Osprey，一种基于掩码文本的指令微调方法，旨在实现像素级的视觉理解。通过引入精细掩码区域到语言指令中，设计了一个视觉语言模型。实验结果表明，Osprey在各种区域理解任务中具有优越性，并可无缝集成到Segment Anything Model（SAM）中，获得多粒度语义。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLMs虽然已经通过视觉指令微调获得了视觉-语言功能，但在像素级的精细粒度视觉理解方面仍有不足。</li>
<li>当前MLLMs受限于缺乏基于掩码的指令数据，影响了其性能的提升。</li>
<li>Osprey方法通过引入精细掩码区域到语言指令中，旨在实现像素级的视觉理解。</li>
<li>Osprey设计了一个视觉语言模型，采用了卷积CLIP骨干网作为视觉编码器，并使用掩码感知的视觉提取器从高分辨率输入中提取精确的视觉掩码特征。</li>
<li>实验结果表明，Osprey在各种区域理解任务中具有优越性。</li>
<li>Osprey可以无缝集成到Segment Anything Model（SAM），获得多粒度语义。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2312.10032">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2312.10032v4/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2312.10032v4/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2312.10032v4/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2312.10032v4/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2312.10032v4/page_4_0.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-13/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-13/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-13/Agent/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_Agent/2509.09135v1/page_0_0.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-09-13  Maximizing social welfare among EF1 allocations at the presence of two   types of agents
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-13
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-13/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2508.15868v2/page_2_0.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-09-13  FLUX-Reason-6M & PRISM-Bench A Million-Scale Text-to-Image Reasoning   Dataset and Comprehensive Benchmark
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-13
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">27663.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
