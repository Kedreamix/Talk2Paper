<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-13  The Illusion of Diminishing Returns Measuring Long Horizon Execution in   LLMs">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2406.06464v4/page_2_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-13
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-07
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    16.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    67 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-13-æ›´æ–°"><a href="#2025-09-13-æ›´æ–°" class="headerlink" title="2025-09-13 æ›´æ–°"></a>2025-09-13 æ›´æ–°</h1><h2 id="The-Illusion-of-Diminishing-Returns-Measuring-Long-Horizon-Execution-in-LLMs"><a href="#The-Illusion-of-Diminishing-Returns-Measuring-Long-Horizon-Execution-in-LLMs" class="headerlink" title="The Illusion of Diminishing Returns: Measuring Long Horizon Execution in   LLMs"></a>The Illusion of Diminishing Returns: Measuring Long Horizon Execution in   LLMs</h2><p><strong>Authors:Akshit Sinha, Arvindh Arun, Shashwat Goel, Steffen Staab, Jonas Geiping</strong></p>
<p>Does continued scaling of large language models (LLMs) yield diminishing returns? Real-world value often stems from the length of task an agent can complete. We start this work by observing the simple but counterintuitive fact that marginal gains in single-step accuracy can compound into exponential improvements in the length of a task a model can successfully complete. Then, we argue that failures of LLMs when simple tasks are made longer arise from mistakes in execution, rather than an inability to reason. We propose isolating execution capability, by explicitly providing the knowledge and plan needed to solve a long-horizon task. We find that larger models can correctly execute significantly more turns even when small models have 100% single-turn accuracy. We observe that the per-step accuracy of models degrades as the number of steps increases. This is not just due to long-context limitations â€“ curiously, we observe a self-conditioning effect â€“ models become more likely to make mistakes when the context contains their errors from prior turns. Self-conditioning does not reduce by just scaling the model size. In contrast, recent thinking models do not self-condition, and can also execute much longer tasks in a single turn. We conclude by benchmarking frontier thinking models on the length of task they can execute in a single turn. Overall, by focusing on the ability to execute, we hope to reconcile debates on how LLMs can solve complex reasoning problems yet fail at simple tasks when made longer, and highlight the massive benefits of scaling model size and sequential test-time compute for long-horizon tasks. </p>
<blockquote>
<p>ç»§ç»­æ‰©å¤§å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è§„æ¨¡æ˜¯å¦ä¼šäº§ç”Ÿæ”¶ç›Šé€’å‡æ•ˆåº”ï¼Ÿç°å®ä»·å€¼å¾€å¾€æºäºä»£ç†å¯ä»¥å®Œæˆçš„ä»»åŠ¡é•¿åº¦ã€‚æˆ‘ä»¬ä»è§‚å¯Ÿä¸€ä¸ªç®€å•ä½†å…·æœ‰çŸ›ç›¾çš„äº‹å®å¼€å§‹ï¼Œå³å•æ­¥ç²¾åº¦çš„è¾¹é™…å¢ç›Šå¯ä»¥è½¬åŒ–ä¸ºä»»åŠ¡æˆåŠŸå®Œæˆé•¿åº¦çš„æŒ‡æ•°æ”¹è¿›ã€‚ç„¶åï¼Œæˆ‘ä»¬è®ºè¯å½“ç®€å•ä»»åŠ¡è¢«å»¶é•¿æ—¶ï¼ŒLLMçš„å¤±è´¥æ˜¯ç”±äºæ‰§è¡Œè¿‡ç¨‹ä¸­çš„é”™è¯¯ï¼Œè€Œä¸æ˜¯æ¨ç†èƒ½åŠ›çš„ä¸è¶³ã€‚æˆ‘ä»¬æè®®é€šè¿‡æ˜ç¡®æä¾›è§£å†³é•¿æœŸä»»åŠ¡æ‰€éœ€çš„çŸ¥è¯†å’Œè®¡åˆ’æ¥éš”ç¦»æ‰§è¡Œèƒ½åŠ›ã€‚æˆ‘ä»¬å‘ç°ï¼Œå³ä½¿åœ¨å°å‹æ¨¡å‹çš„å•è½®å‡†ç¡®ç‡è¾¾åˆ°100%çš„æƒ…å†µä¸‹ï¼Œå¤§å‹æ¨¡å‹ä»ç„¶å¯ä»¥æ­£ç¡®æ‰§è¡Œæ›´å¤šè½®æ¬¡ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œéšç€æ­¥éª¤æ•°é‡çš„å¢åŠ ï¼Œæ¨¡å‹çš„æ¯æ­¥å‡†ç¡®ç‡ä¼šä¸‹é™ã€‚è¿™ä¸ä»…ä»…æ˜¯ç”±äºé•¿æœŸä¸Šä¸‹æ–‡é™åˆ¶â€”â€”å¥‡æ€ªçš„æ˜¯ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°äº†ä¸€ä¸ªè‡ªæ¡ä»¶æ•ˆåº”â€”â€”å½“ä¸Šä¸‹æ–‡åŒ…å«å…ˆå‰è½®æ¬¡ä¸­çš„é”™è¯¯æ—¶ï¼Œæ¨¡å‹æ›´å¯èƒ½çŠ¯é”™è¯¯ã€‚ä»…é€šè¿‡æ‰©å¤§æ¨¡å‹è§„æ¨¡å¹¶ä¸èƒ½å‡å°‘è¿™ç§è‡ªæ¡ä»¶æ•ˆåº”ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæœ€è¿‘çš„æ€è€ƒæ¨¡å‹å¹¶ä¸è‡ªæˆ‘æ¡ä»¶åŒ–ï¼Œå¹¶ä¸”å¯ä»¥åœ¨å•è½®ä¸­æ‰§è¡Œæ›´é•¿çš„ä»»åŠ¡ã€‚æˆ‘ä»¬é€šè¿‡å‰æ²¿æ€è€ƒæ¨¡å‹åœ¨å•è½®ä¸­å¯ä»¥æ‰§è¡Œçš„ä»»åŠ¡é•¿åº¦è¿›è¡ŒåŸºå‡†æµ‹è¯•æ¥å¾—å‡ºç»“è®ºã€‚æ€»ä½“è€Œè¨€ï¼Œé€šè¿‡å…³æ³¨æ‰§è¡Œèƒ½åŠ›ï¼Œæˆ‘ä»¬å¸Œæœ›è°ƒå’Œå…³äºLLMå¦‚ä½•è§£å†³å¤æ‚çš„æ¨ç†é—®é¢˜ä½†åœ¨æ›´é•¿çš„ç®€å•ä»»åŠ¡ä¸­å¤±è´¥çš„äº‰è®ºï¼Œå¹¶å¼ºè°ƒåœ¨è§„æ¨¡æ¨¡å‹å’Œé¡ºåºæµ‹è¯•æ—¶é—´è®¡ç®—ä¸­å¯¹é•¿æœŸä»»åŠ¡çš„å·¨å¤§å¥½å¤„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.09677v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æŒç»­æ‰©å±•æ˜¯å¦äº§ç”Ÿè¾¹é™…æ•ˆç›Šé€’å‡ï¼Œå®é™…ä»·å€¼å¾€å¾€å–å†³äºæ¨¡å‹èƒ½å®Œæˆä»»åŠ¡çš„é•¿åº¦ã€‚ç ”ç©¶å‘ç°ï¼Œå•æ­¥å‡†ç¡®æ€§çš„å¾®å°å¢ç›Šå¯ä»¥ç´¯ç§¯æˆä¸ºä»»åŠ¡é•¿åº¦çš„æŒ‡æ•°çº§æ”¹è¿›ï¼Œå¹¶æŒ‡å‡ºé•¿ä»»åŠ¡å¤±è´¥å¾€å¾€æºäºæ‰§è¡Œè¿‡ç¨‹ä¸­çš„é”™è¯¯ï¼Œè€Œéæ¨ç†èƒ½åŠ›çš„ä¸è¶³ã€‚é€šè¿‡æä¾›çŸ¥è¯†å’Œè®¡åˆ’æ¥éš”ç¦»æ‰§è¡Œèƒ½åŠ›ï¼Œç ”ç©¶äººå‘˜å‘ç°å¤§å‹æ¨¡å‹åœ¨æ‰§è¡Œæ›´å¤šæ­¥éª¤æ—¶è¡¨ç°æ›´å¥½ï¼Œå³ä½¿åœ¨å°å‹æ¨¡å‹å•æ­¥å‡†ç¡®ç‡ä¸ºç™¾åˆ†ä¹‹ç™¾çš„æƒ…å†µä¸‹ä¹Ÿæ˜¯å¦‚æ­¤ã€‚éšç€æ­¥éª¤æ•°é‡çš„å¢åŠ ï¼Œæ¨¡å‹çš„æ¯æ­¥å‡†ç¡®æ€§ä¼šä¸‹é™ï¼Œè¿™ä¸ä»…ä»…æ˜¯ç”±äºé•¿æœŸä¸Šä¸‹æ–‡é™åˆ¶ã€‚ä¸€ç§å¥‡æ€ªçš„ç°è±¡æ˜¯è‡ªæˆ‘æ¡ä»¶æ•ˆåº”â€”â€”å½“ä¸Šä¸‹æ–‡åŒ…å«å…ˆå‰çš„é”™è¯¯æ—¶ï¼Œæ¨¡å‹æ›´å®¹æ˜“å‡ºé”™ã€‚å¢åŠ æ¨¡å‹è§„æ¨¡å¹¶ä¸èƒ½å‡å°‘è‡ªæˆ‘æ¡ä»¶æ•ˆåº”ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæœ€æ–°çš„æ€è€ƒæ¨¡å‹ä¸è¿›è¡Œè‡ªæˆ‘æ¡ä»¶ï¼Œå¹¶èƒ½ä¸€æ¬¡å®Œæˆæ›´é•¿çš„ä»»åŠ¡ã€‚é€šè¿‡å¯¹å‰æ²¿æ€è€ƒæ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œå‘ç°å®ƒä»¬åœ¨å•æ¬¡æ‰§è¡Œä»»åŠ¡çš„é•¿åº¦æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚å› æ­¤ï¼Œæœ¬ç ”ç©¶é€šè¿‡å…³æ³¨æ¨¡å‹çš„æ‰§è¡Œèƒ½åŠ›ï¼Œæ¢è®¨äº†LLMå¦‚ä½•è§£å†³å¤æ‚çš„æ¨ç†é—®é¢˜ï¼Œä»¥åŠåœ¨ä»»åŠ¡é•¿åº¦å¢åŠ æ—¶é‡åˆ°çš„ç®€å•ä»»åŠ¡å¤±è´¥é—®é¢˜ï¼Œå¹¶å¼ºè°ƒäº†æ¨¡å‹è§„æ¨¡æ‰©å±•å’Œé¡ºåºæµ‹è¯•æ—¶é—´è®¡ç®—åœ¨é•¿æœŸä»»åŠ¡ä¸­çš„å·¨å¤§ç›Šå¤„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å®é™…ä»·å€¼å–å†³äºå…¶èƒ½å®Œæˆä»»åŠ¡çš„é•¿åº¦ã€‚</li>
<li>å•æ­¥å‡†ç¡®æ€§çš„å¾®å°æ”¹è¿›å¯ä»¥ç´¯ç§¯æˆä»»åŠ¡å®Œæˆçš„æŒ‡æ•°çº§æå‡ã€‚</li>
<li>LLMåœ¨é•¿ä»»åŠ¡ä¸­çš„å¤±è´¥ä¸»è¦æºäºæ‰§è¡Œè¿‡ç¨‹ä¸­çš„é”™è¯¯ï¼Œè€Œéæ¨ç†èƒ½åŠ›ä¸è¶³ã€‚</li>
<li>é€šè¿‡æä¾›çŸ¥è¯†å’Œè®¡åˆ’æ¥éš”ç¦»æ¨¡å‹çš„æ‰§è¡Œèƒ½åŠ›ï¼Œå¤§å‹æ¨¡å‹åœ¨æ‰§è¡Œæ›´å¤šæ­¥éª¤æ—¶è¡¨ç°æ›´ä½³ã€‚</li>
<li>éšç€æ­¥éª¤æ•°é‡çš„å¢åŠ ï¼Œæ¨¡å‹çš„æ¯æ­¥å‡†ç¡®æ€§ä¼šä¸‹é™ï¼Œè¿™å…¶ä¸­åŒ…æ‹¬è‡ªæˆ‘æ¡ä»¶æ•ˆåº”çš„å½±å“ã€‚</li>
<li>ç®€å•çš„æ¨¡å‹è§„æ¨¡æ‰©å±•å¹¶ä¸èƒ½å‡å°‘è‡ªæˆ‘æ¡ä»¶æ•ˆåº”ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.09677">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2509.09677v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2509.09677v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2509.09677v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2509.09677v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2509.09677v1/page_5_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Measuring-Epistemic-Humility-in-Multimodal-Large-Language-Models"><a href="#Measuring-Epistemic-Humility-in-Multimodal-Large-Language-Models" class="headerlink" title="Measuring Epistemic Humility in Multimodal Large Language Models"></a>Measuring Epistemic Humility in Multimodal Large Language Models</h2><p><strong>Authors:Bingkui Tong, Jiaer Xia, Sifeng Shang, Kaiyang Zhou</strong></p>
<p>Hallucinations in multimodal large language models (MLLMs) â€“ where the model generates content inconsistent with the input image â€“ pose significant risks in real-world applications, from misinformation in visual question answering to unsafe errors in decision-making. Existing benchmarks primarily test recognition accuracy, i.e., evaluating whether models can select the correct answer among distractors. This overlooks an equally critical capability for trustworthy AI: recognizing when none of the provided options are correct, a behavior reflecting epistemic humility. We present HumbleBench, a new hallucination benchmark designed to evaluate MLLMsâ€™ ability to reject plausible but incorrect answers across three hallucination types: object, relation, and attribute. Built from a panoptic scene graph dataset, we leverage fine-grained scene graph annotations to extract ground-truth entities and relations, and prompt GPT-4-Turbo to generate multiple-choice questions, followed by a rigorous manual filtering process. Each question includes a â€œNone of the aboveâ€ option, requiring models not only to recognize correct visual information but also to identify when no provided answer is valid. We evaluate a variety of state-of-the-art MLLMs â€“ including both general-purpose and specialized reasoning models â€“ on HumbleBench and share valuable findings and insights with the community. By incorporating explicit false-option rejection, HumbleBench fills a key gap in current evaluation suites, providing a more realistic measure of MLLM reliability in safety-critical settings. Our code and dataset are released publicly and can be accessed at <a target="_blank" rel="noopener" href="https://github.com/maifoundations/HumbleBench">https://github.com/maifoundations/HumbleBench</a>. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸­çš„å¹»è§‰ï¼Œå³æ¨¡å‹ç”Ÿæˆä¸è¾“å…¥å›¾åƒä¸ä¸€è‡´çš„å†…å®¹ï¼Œåœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­å¸¦æ¥äº†é‡å¤§é£é™©ï¼Œä»è§†è§‰é—®ç­”ä¸­çš„è¯¯å¯¼ä¿¡æ¯åˆ°å†³ç­–ä¸­çš„ä¸å®‰å…¨é”™è¯¯ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•ä¸»è¦æµ‹è¯•è¯†åˆ«å‡†ç¡®ç‡ï¼Œå³è¯„ä¼°æ¨¡å‹æ˜¯å¦èƒ½åœ¨å¹²æ‰°é¡¹ä¸­é€‰å‡ºæ­£ç¡®ç­”æ¡ˆã€‚è¿™å¿½ç•¥äº†å¯ä¿¡äººå·¥æ™ºèƒ½åŒæ ·å…³é”®çš„èƒ½åŠ›ï¼šå³å½“æ‰€æœ‰æä¾›çš„é€‰é¡¹éƒ½ä¸æ­£ç¡®æ—¶ï¼Œèƒ½å¤Ÿè¯†åˆ«å‡ºæ¥ï¼Œè¿™ç§è¡Œä¸ºåæ˜ äº†çŸ¥è¯†è°¦é€Šã€‚æˆ‘ä»¬æå‡ºäº†HumbleBenchï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„å¹»è§‰åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°MLLMsæ‹’ç»å¯èƒ½ä½†é”™è¯¯ç­”æ¡ˆçš„èƒ½åŠ›ï¼Œæ¶µç›–ä¸‰ç§å¹»è§‰ç±»å‹ï¼šå¯¹è±¡ã€å…³ç³»å’Œå±æ€§ã€‚å®ƒå»ºç«‹åœ¨å…¨æ™¯åœºæ™¯å›¾æ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬åˆ©ç”¨ç²¾ç»†åœºæ™¯å›¾æ³¨é‡Šæ¥æå–çœŸå®å®ä½“å’Œå…³ç³»ï¼Œå¹¶æç¤ºGPT-4-Turboç”Ÿæˆå¤šé¡¹é€‰æ‹©é¢˜ï¼Œéšåç»è¿‡ä¸¥æ ¼çš„æ‰‹åŠ¨è¿‡æ»¤è¿‡ç¨‹ã€‚æ¯ä¸ªé—®é¢˜éƒ½åŒ…æ‹¬ä¸€ä¸ªâ€œä»¥ä¸Šéƒ½ä¸æ˜¯â€çš„é€‰é¡¹ï¼Œè¦æ±‚æ¨¡å‹ä¸ä»…è¦è¯†åˆ«æ­£ç¡®çš„è§†è§‰ä¿¡æ¯ï¼Œè¿˜è¦åœ¨æ²¡æœ‰ä»»ä½•æä¾›çš„ç­”æ¡ˆæœ‰æ•ˆæ—¶èƒ½å¤Ÿè¯†åˆ«å‡ºæ¥ã€‚æˆ‘ä»¬åœ¨HumbleBenchä¸Šè¯„ä¼°äº†å¤šç§å…ˆè¿›çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ŒåŒ…æ‹¬é€šç”¨å’Œä¸“é—¨ç”¨äºæ¨ç†çš„æ¨¡å‹ï¼Œå¹¶ä¸ç¤¾åŒºåˆ†äº«äº†æœ‰ä»·å€¼çš„å‘ç°å’Œè§è§£ã€‚é€šè¿‡èå…¥æ˜ç¡®çš„è™šå‡é€‰é¡¹æ‹’ç»èƒ½åŠ›ï¼ŒHumbleBenchå¡«è¡¥äº†å½“å‰è¯„ä¼°å¥—ä»¶çš„å…³é”®ç©ºç™½ï¼Œä¸ºå®‰å…¨å…³é”®è®¾ç½®ä¸­MLLMçš„å¯é æ€§æä¾›äº†æ›´ç°å®çš„è¡¡é‡æ ‡å‡†ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®é›†å·²å…¬å¼€å‘å¸ƒï¼Œå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/maifoundations/HumbleBench%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/maifoundations/HumbleBenchè®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.09658v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸­çš„å¹»è§‰ï¼Œå³æ¨¡å‹ç”Ÿæˆä¸è¾“å…¥å›¾åƒä¸ä¸€è‡´çš„å†…å®¹ï¼Œåœ¨çœŸå®ä¸–ç•Œåº”ç”¨ä¸­å­˜åœ¨é‡å¤§é£é™©ï¼Œå¦‚è§†è§‰é—®ç­”ä¸­çš„é”™è¯¯ä¿¡æ¯ä»¥åŠå†³ç­–åˆ¶å®šä¸­çš„å®‰å…¨é”™è¯¯ã€‚ç°æœ‰åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨è¯†åˆ«å‡†ç¡®æ€§ï¼Œå³è¯„ä¼°æ¨¡å‹æ˜¯å¦èƒ½åœ¨å¹²æ‰°é¡¹ä¸­é€‰å‡ºæ­£ç¡®ç­”æ¡ˆã€‚ç„¶è€Œï¼Œè¿™å¿½ç•¥äº†å¯ä¿¡äººå·¥æ™ºèƒ½çš„å¦ä¸€ä¸ªå…³é”®èƒ½åŠ›ï¼šæ‹’ç»æ­£ç¡®ä½†ä¸å¯é çš„ç­”æ¡ˆçš„è¡Œä¸ºåæ˜ äº†çŸ¥è¯†è°¦é€Šã€‚æˆ‘ä»¬æå‡ºäº†HumbleBenchï¼Œä¸€ä¸ªæ–°çš„å¹»è§‰åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°MLLMsæ‹’ç»ä¸‰ç§å¹»è§‰ç±»å‹ï¼ˆå¯¹è±¡ã€å…³ç³»å’Œå±æ€§ï¼‰çš„å¯é ä½†é”™è¯¯ç­”æ¡ˆçš„èƒ½åŠ›ã€‚HumbleBenchå»ºç«‹åœ¨ä¸€ä¸ªå…¨æ™¯åœºæ™¯å›¾æ•°æ®é›†ä¸Šï¼Œåˆ©ç”¨ç²¾ç»†åœºæ™¯å›¾æ³¨é‡Šæå–çœŸå®å®ä½“å’Œå…³ç³»ï¼Œå¹¶æç¤ºGPT-4 Turboç”Ÿæˆå¤šé¡¹é€‰æ‹©é¢˜ï¼Œéšåè¿›è¡Œä¸¥æ ¼çš„äººå·¥è¿‡æ»¤è¿‡ç¨‹ã€‚æ¯ä¸ªé—®é¢˜éƒ½åŒ…æ‹¬ä¸€ä¸ªâ€œä»¥ä¸Šéƒ½ä¸æ˜¯â€çš„é€‰é¡¹ï¼Œè¦æ±‚æ¨¡å‹ä¸ä»…è¦è¯†åˆ«æ­£ç¡®çš„è§†è§‰ä¿¡æ¯ï¼Œè¿˜è¦åœ¨æ²¡æœ‰ä»»ä½•æä¾›çš„ç­”æ¡ˆæ˜¯æœ‰æ•ˆæ—¶è¯†åˆ«å‡ºæ¥ã€‚æˆ‘ä»¬åœ¨HumbleBenchä¸Šè¯„ä¼°äº†ä¸€ç³»åˆ—æœ€å…ˆè¿›çš„MLLMsï¼ŒåŒ…æ‹¬é€šç”¨å’Œä¸“é—¨ç”¨äºæ¨ç†çš„æ¨¡å‹ï¼Œå¹¶ä¸ç¤¾åŒºåˆ†äº«æœ‰ä»·å€¼çš„å‘ç°å’Œè§è§£ã€‚é€šè¿‡æ˜ç¡®çš„è™šå‡é€‰é¡¹æ‹’ç»ï¼ŒHumbleBenchå¡«è¡¥äº†å½“å‰è¯„ä¼°å¥—ä»¶çš„å…³é”®ç©ºç™½ï¼Œä¸ºå®‰å…¨å…³é”®è®¾ç½®ä¸­MLLMçš„å¯é æ€§æä¾›äº†æ›´ç°å®çš„è¡¡é‡æ ‡å‡†ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®é›†å·²å…¬å¼€å‘å¸ƒï¼Œå¯è®¿é—®<a target="_blank" rel="noopener" href="https://github.com/maifoundations/HumbleBench%E3%80%82">https://github.com/maifoundations/HumbleBenchã€‚</a></p>
<p><strong>è¦ç‚¹å¦‚ä¸‹</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸­çš„å¹»è§‰åœ¨çœŸå®ä¸–ç•Œåº”ç”¨ä¸­å­˜åœ¨é£é™©ã€‚</li>
<li>å½“å‰åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨è¯†åˆ«å‡†ç¡®æ€§ï¼Œå¿½ç•¥äº†æ¨¡å‹åœ¨æ‹’ç»é”™è¯¯ç­”æ¡ˆæ—¶çš„èƒ½åŠ›ã€‚</li>
<li>æå‡ºæ–°çš„å¹»è§‰åŸºå‡†æµ‹è¯•â€”â€”HumbleBenchï¼Œæ—¨åœ¨è¯„ä¼°MLLMsæ‹’ç»ä¸‰ç§å¹»è§‰ç±»å‹çš„èƒ½åŠ›ã€‚</li>
<li>åˆ©ç”¨å…¨æ™¯åœºæ™¯å›¾æ•°æ®é›†å’Œç²¾ç»†åœºæ™¯å›¾æ³¨é‡Šå»ºç«‹HumbleBenchã€‚</li>
<li>GPT-4 Turboè¢«æç¤ºç”Ÿæˆå¤šé¡¹é€‰æ‹©é¢˜ï¼Œå¹¶è¦æ±‚æ¨¡å‹è¯†åˆ«æ­£ç¡®çš„è§†è§‰ä¿¡æ¯å’Œæ— æœ‰æ•ˆç­”æ¡ˆçš„æƒ…å†µã€‚</li>
<li>åœ¨HumbleBenchä¸Šè¯„ä¼°äº†ä¸€ç³»åˆ—æœ€å…ˆè¿›çš„MLLMsï¼ŒåŒ…æ‹¬é€šç”¨å’Œä¸“é—¨ç”¨äºæ¨ç†çš„æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.09658">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2509.09658v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2509.09658v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2509.09658v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2509.09658v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2509.09658v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Bridging-the-Capability-Gap-Joint-Alignment-Tuning-for-Harmonizing-LLM-based-Multi-Agent-Systems"><a href="#Bridging-the-Capability-Gap-Joint-Alignment-Tuning-for-Harmonizing-LLM-based-Multi-Agent-Systems" class="headerlink" title="Bridging the Capability Gap: Joint Alignment Tuning for Harmonizing   LLM-based Multi-Agent Systems"></a>Bridging the Capability Gap: Joint Alignment Tuning for Harmonizing   LLM-based Multi-Agent Systems</h2><p><strong>Authors:Minghang Zhu, Zhengliang Shi, Zhiwei Xu, Shiguang Wu, Lingjie Wang, Pengjie Ren, Zhaochun Ren, Zhumin Chen</strong></p>
<p>The advancement of large language models (LLMs) has enabled the construction of multi-agent systems to solve complex tasks by dividing responsibilities among specialized agents, such as a planning agent for subgoal generation and a grounding agent for executing tool-use actions. Most existing methods typically fine-tune these agents independently, leading to capability gaps among them with poor coordination. To address this, we propose MOAT, a Multi-Agent Joint Alignment Tuning framework that improves agents collaboration through iterative alignment. MOAT alternates between two key stages: (1) Planning Agent Alignment, which optimizes the planning agent to generate subgoal sequences that better guide the grounding agent; and (2) Grounding Agent Improving, which fine-tunes the grounding agent using diverse subgoal-action pairs generated by the agent itself to enhance its generalization capablity. Theoretical analysis proves that MOAT ensures a non-decreasing and progressively convergent training process. Experiments across six benchmarks demonstrate that MOAT outperforms state-of-the-art baselines, achieving average improvements of 3.1% on held-in tasks and 4.4% on held-out tasks. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥ä½¿å¾—èƒ½å¤Ÿæ„å»ºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œé€šè¿‡ä¸“é—¨åŒ–çš„æ™ºèƒ½ä½“åˆ†é…è´£ä»»æ¥è§£å†³å¤æ‚ä»»åŠ¡ï¼Œä¾‹å¦‚ç”¨äºç”Ÿæˆå­ç›®æ ‡çš„è§„åˆ’æ™ºèƒ½ä½“å’Œç”¨äºæ‰§è¡Œå·¥å…·ä½¿ç”¨åŠ¨ä½œçš„æ¥åœ°æ™ºèƒ½ä½“ã€‚å¤§å¤šæ•°ç°æœ‰æ–¹æ³•é€šå¸¸ç‹¬ç«‹å¾®è°ƒè¿™äº›æ™ºèƒ½ä½“ï¼Œå¯¼è‡´å®ƒä»¬ä¹‹é—´å­˜åœ¨èƒ½åŠ›å·®è·ï¼Œåè°ƒä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†MOATï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šæ™ºèƒ½ä½“è”åˆå¯¹é½è°ƒæ•´æ¡†æ¶ï¼Œå®ƒé€šè¿‡è¿­ä»£å¯¹é½æ”¹è¿›æ™ºèƒ½ä½“çš„åä½œã€‚MOATåœ¨ä¸¤ä¸ªå…³é”®é˜¶æ®µä¹‹é—´äº¤æ›¿è¿›è¡Œï¼šï¼ˆ1ï¼‰è§„åˆ’æ™ºèƒ½ä½“å¯¹é½ï¼Œä¼˜åŒ–è§„åˆ’æ™ºèƒ½ä½“ä»¥ç”Ÿæˆæ›´å¥½åœ°æŒ‡å¯¼æ¥åœ°æ™ºèƒ½ä½“çš„å­ç›®æ ‡åºåˆ—ï¼›ï¼ˆ2ï¼‰æ¥åœ°æ™ºèƒ½ä½“æ”¹è¿›ï¼Œä½¿ç”¨æ™ºèƒ½ä½“æœ¬èº«ç”Ÿæˆçš„å¤šæ ·åŒ–çš„å­ç›®æ ‡-åŠ¨ä½œå¯¹å¾®è°ƒæ¥åœ°æ™ºèƒ½ä½“ï¼Œæé«˜å…¶æ³›åŒ–èƒ½åŠ›ã€‚ç†è®ºåˆ†æè¯æ˜ï¼ŒMOATç¡®ä¿äº†éé€’å‡å’Œé€æ­¥æ”¶æ•›çš„è®­ç»ƒè¿‡ç¨‹ã€‚åœ¨å…­ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMOATä¼˜äºæœ€æ–°çš„åŸºçº¿æŠ€æœ¯ï¼Œåœ¨å·²å®Œæˆä»»åŠ¡ä¸Šå¹³å‡æé«˜äº†3.1%ï¼Œåœ¨æœªå®Œæˆä»»åŠ¡ä¸Šå¹³å‡æé«˜äº†4.4%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.09629v1">PDF</a> EMNLP 2025 Findings</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥ä½¿å¾—å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„æ„å»ºæˆä¸ºå¯èƒ½ï¼Œé€šè¿‡ä¸“é—¨åŒ–çš„æ™ºèƒ½ä½“åˆ†å·¥åä½œæ¥è§£å†³å¤æ‚ä»»åŠ¡ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸ç‹¬ç«‹å¾®è°ƒè¿™äº›æ™ºèƒ½ä½“ï¼Œå¯¼è‡´èƒ½åŠ›å·®è·å’Œåè°ƒä¸è‰¯ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºMOATâ€”â€”ä¸€ç§å¤šæ™ºèƒ½ä½“è”åˆå¯¹é½è°ƒæ•´æ¡†æ¶ï¼Œé€šè¿‡è¿­ä»£å¯¹é½æé«˜æ™ºèƒ½ä½“çš„åä½œèƒ½åŠ›ã€‚MOATäº¤æ›¿è¿›è¡Œä¸¤ä¸ªå…³é”®é˜¶æ®µï¼šè§„åˆ’æ™ºèƒ½ä½“å¯¹é½ï¼Œä¼˜åŒ–è§„åˆ’æ™ºèƒ½ä½“ä»¥ç”Ÿæˆæ›´å¥½çš„å­ç›®æ ‡åºåˆ—æ¥æŒ‡å¯¼æ¥åœ°æ™ºèƒ½ä½“ï¼›æ¥åœ°æ™ºèƒ½ä½“æ”¹è¿›ï¼Œä½¿ç”¨ç”±æ™ºèƒ½ä½“æœ¬èº«ç”Ÿæˆçš„å¤šæ ·åŒ–çš„å­ç›®æ ‡-åŠ¨ä½œå¯¹è¿›è¡Œå¾®è°ƒï¼Œæé«˜å…¶æ³›åŒ–èƒ½åŠ›ã€‚ç†è®ºå’Œå®éªŒè¯æ˜ï¼ŒMOATç¡®ä¿è®­ç»ƒè¿‡ç¨‹ä¸æ–­å¢è¿›å¹¶é€æ­¥æ”¶æ•›ï¼Œåœ¨å…­ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜äºæœ€æ–°åŸºçº¿æŠ€æœ¯ï¼Œå®Œæˆä»»åŠ¡çš„å¹³å‡æ”¹è¿›ç‡ä¸º3.1%å’Œ4.4%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMçš„è¿›æ­¥ä¿ƒè¿›äº†å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„æ„å»ºï¼Œå…è®¸é€šè¿‡ä¸“é—¨åŒ–çš„æ™ºèƒ½ä½“æ¥è§£å†³å¤æ‚ä»»åŠ¡ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ç‹¬ç«‹å¾®è°ƒæ™ºèƒ½ä½“ï¼Œå¯¼è‡´èƒ½åŠ›å·®è·å’Œåè°ƒé—®é¢˜ã€‚</li>
<li>MOATæ¡†æ¶æ˜¯ä¸€ç§å¤šæ™ºèƒ½ä½“è”åˆå¯¹é½è°ƒæ•´æ–¹æ³•ï¼Œé€šè¿‡è¿­ä»£å¯¹é½æé«˜æ™ºèƒ½ä½“çš„åä½œã€‚</li>
<li>MOATåŒ…æ‹¬ä¸¤ä¸ªå…³é”®é˜¶æ®µï¼šè§„åˆ’æ™ºèƒ½ä½“å¯¹é½å’Œæ¥åœ°æ™ºèƒ½ä½“æ”¹è¿›ã€‚</li>
<li>è§„åˆ’æ™ºèƒ½ä½“å¯¹é½é˜¶æ®µä¼˜åŒ–ç”Ÿæˆå­ç›®æ ‡åºåˆ—ï¼Œä»¥æŒ‡å¯¼æ¥åœ°æ™ºèƒ½ä½“çš„è¡Œä¸ºã€‚</li>
<li>æ¥åœ°æ™ºèƒ½ä½“æ”¹è¿›é˜¶æ®µä½¿ç”¨å¤šæ ·åŒ–çš„å­ç›®æ ‡-åŠ¨ä½œå¯¹è¿›è¡Œå¾®è°ƒï¼Œæé«˜å…¶æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.09629">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2509.09629v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2509.09629v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2509.09629v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2509.09629v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="LAVA-Language-Model-Assisted-Verbal-Autopsy-for-Cause-of-Death-Determination"><a href="#LAVA-Language-Model-Assisted-Verbal-Autopsy-for-Cause-of-Death-Determination" class="headerlink" title="LAVA: Language Model Assisted Verbal Autopsy for Cause-of-Death   Determination"></a>LAVA: Language Model Assisted Verbal Autopsy for Cause-of-Death   Determination</h2><p><strong>Authors:Yiqun T. Chen, Tyler H. McCormick, Li Liu, Abhirup Datta</strong></p>
<p>Verbal autopsy (VA) is a critical tool for estimating causes of death in resource-limited settings where medical certification is unavailable. This study presents LA-VA, a proof-of-concept pipeline that combines Large Language Models (LLMs) with traditional algorithmic approaches and embedding-based classification for improved cause-of-death prediction. Using the Population Health Metrics Research Consortium (PHMRC) dataset across three age categories (Adult: 7,580; Child: 1,960; Neonate: 2,438), we evaluate multiple approaches: GPT-5 predictions, LCVA baseline, text embeddings, and meta-learner ensembles. Our results demonstrate that GPT-5 achieves the highest individual performance with average test site accuracies of 48.6% (Adult), 50.5% (Child), and 53.5% (Neonate), outperforming traditional statistical machine learning baselines by 5-10%. Our findings suggest that simple off-the-shelf LLM-assisted approaches could substantially improve verbal autopsy accuracy, with important implications for global health surveillance in low-resource settings. </p>
<blockquote>
<p>è¨€è¯­ç—…ç†ï¼ˆVAï¼‰æ˜¯åœ¨åŒ»ç–—èµ„æºæœ‰é™ã€æ— æ³•è¿›è¡ŒåŒ»å­¦è®¤è¯çš„ç¯å¢ƒä¸‹ä¼°è®¡æ­»äº¡åŸå› çš„é‡è¦å·¥å…·ã€‚æœ¬ç ”ç©¶æå‡ºäº†LA-VAæ¦‚å¿µéªŒè¯æµç¨‹ï¼Œå®ƒç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ä¼ ç»Ÿç®—æ³•æ–¹æ³•å’ŒåŸºäºåµŒå…¥çš„åˆ†ç±»ï¼Œä»¥æ”¹è¿›æ­»äº¡åŸå› é¢„æµ‹ã€‚åˆ©ç”¨äººå£å¥åº·æŒ‡æ ‡ç ”ç©¶è”ç›Ÿï¼ˆPHMRCï¼‰è·¨è¶Šä¸‰ä¸ªå¹´é¾„ç±»åˆ«ï¼ˆæˆäººï¼š7580ä¾‹ï¼›å„¿ç«¥ï¼š1960ä¾‹ï¼›æ–°ç”Ÿå„¿ï¼š2438ä¾‹ï¼‰çš„æ•°æ®é›†ï¼Œæˆ‘ä»¬è¯„ä¼°äº†å¤šç§æ–¹æ³•ï¼šGPT-5é¢„æµ‹ã€LCVAåŸºçº¿ã€æ–‡æœ¬åµŒå…¥å’Œå…ƒå­¦ä¹ è€…é›†åˆã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒGPT-5è¡¨ç°æœ€ä½³çš„ä¸ªäººæ€§èƒ½å¹³å‡æµ‹è¯•å‡†ç¡®ç‡åˆ†åˆ«ä¸ºæˆäºº48.6%ï¼Œå„¿ç«¥50.5%ï¼Œæ–°ç”Ÿå„¿53.5%ï¼Œé«˜å‡ºä¼ ç»Ÿçš„ç»Ÿè®¡å­¦æœºå™¨å­¦ä¹ åŸºçº¿æ¨¡å‹çº¦5%-10%ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œç®€å•çš„ç°æˆçš„å¤§å‹è¯­è¨€æ¨¡å‹è¾…åŠ©æ–¹æ³•å¯èƒ½æå¤§åœ°æé«˜è¨€è¯­ç—…ç†çš„å‡†ç¡®æ€§ï¼Œè¿™å¯¹åœ¨ä½èµ„æºç¯å¢ƒä¸‹è¿›è¡Œå…¨çƒå¥åº·ç›‘æµ‹å…·æœ‰é‡å¤§æ„ä¹‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.09602v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºè‡ªç„¶è¯­è¨€å¤„ç†çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨èµ„æºå—é™ç¯å¢ƒä¸­å¯¹æ­»äº¡åŸå› ä¼°è®¡å…·æœ‰å…³é”®ä½œç”¨ã€‚æœ¬ç ”ç©¶æå‡ºLA-VAæ¦‚å¿µéªŒè¯æµç¨‹ï¼Œç»“åˆä¼ ç»Ÿç®—æ³•å’ŒåŸºäºåµŒå…¥çš„åˆ†ç±»æ–¹æ³•ï¼Œä»¥æé«˜æ­»äº¡åŸå› é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚åˆ©ç”¨äººå£å¥åº·æŒ‡æ ‡ç ”ç©¶åä¼šï¼ˆPHMRCï¼‰æ•°æ®é›†ï¼Œå¯¹GPT-5é¢„æµ‹ã€LCVAåŸºçº¿ã€æ–‡æœ¬åµŒå…¥å’Œå…ƒå­¦ä¹ è€…é›†æˆç­‰å¤šç§æ–¹æ³•è¿›è¡Œè¯„ä»·ã€‚ç»“æœæ˜¾ç¤ºGPT-5è¡¨ç°æœ€ä½³ï¼Œå¹³å‡æµ‹è¯•å‡†ç¡®åº¦åˆ†åˆ«ä¸ºæˆäºº48.6%ã€å„¿ç«¥50.5%ã€æ–°ç”Ÿå„¿53.5%ï¼Œè¾ƒä¼ ç»Ÿç»Ÿè®¡æœºå™¨å­¦ä¹ åŸºçº¿é«˜å‡º5-10%ã€‚è¿™æç¤ºæˆ‘ä»¬ï¼Œç®€å•çš„å³æ—¶LLMè¾…åŠ©æ–¹æ³•å¯æ˜¾è‘—æé«˜å£å¤´éªŒå°¸çš„å‡†ç¡®åº¦ï¼Œå¯¹ä½èµ„æºç¯å¢ƒä¸­çš„å…¨çƒå¥åº·ç›‘æµ‹å…·æœ‰é‡è¦æ„ä¹‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Verbal autopsy (VA)æ˜¯èµ„æºå—é™ç¯å¢ƒä¸­ä¼°ç®—æ­»äº¡åŸå› çš„é‡è¦å·¥å…·ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºäº†LA-VAæ¦‚å¿µéªŒè¯æµç¨‹ï¼Œç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ã€ä¼ ç»Ÿç®—æ³•å’ŒåŸºäºåµŒå…¥çš„åˆ†ç±»æ–¹æ³•ä»¥æé«˜æ­»äº¡åŸå› é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚</li>
<li>ä½¿ç”¨PHMRCæ•°æ®é›†è¿›è¡Œå®è¯ç ”ç©¶ï¼ŒåŒ…æ‹¬æˆäººã€å„¿ç«¥å’Œæ–°ç”Ÿå„¿ä¸‰ä¸ªå¹´é¾„ç»„çš„æ•°æ®ã€‚</li>
<li>GPT-5åœ¨å„ç§è¯„ä¼°æ–¹æ³•ä¸­è¡¨ç°æœ€ä½³ï¼Œç›¸æ¯”ä¼ ç»Ÿç»Ÿè®¡æœºå™¨å­¦ä¹ åŸºçº¿æé«˜äº†5-10%çš„å‡†ç¡®åº¦ã€‚</li>
<li>ç®€å•å³æ—¶çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¾…åŠ©æ–¹æ³•å¯ä»¥æ˜¾è‘—æé«˜å£å¤´éªŒå°¸çš„å‡†ç¡®æ€§ã€‚</li>
<li>LLMåœ¨æ­»äº¡åŸå› é¢„æµ‹ä¸­çš„ä½¿ç”¨å…·æœ‰é‡è¦çš„å…¨çƒå¥åº·ç›‘æµ‹æ„ä¹‰ï¼Œå°¤å…¶æ˜¯åœ¨ä½èµ„æºç¯å¢ƒä¸­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.09602">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2509.09602v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2509.09602v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2509.09602v1/page_3_1.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="VQualA-2025-Challenge-on-Visual-Quality-Comparison-for-Large-Multimodal-Models-Methods-and-Results"><a href="#VQualA-2025-Challenge-on-Visual-Quality-Comparison-for-Large-Multimodal-Models-Methods-and-Results" class="headerlink" title="VQualA 2025 Challenge on Visual Quality Comparison for Large Multimodal   Models: Methods and Results"></a>VQualA 2025 Challenge on Visual Quality Comparison for Large Multimodal   Models: Methods and Results</h2><p><strong>Authors:Hanwei Zhu, Haoning Wu, Zicheng Zhang, Lingyu Zhu, Yixuan Li, Peilin Chen, Shiqi Wang, Chris Wei Zhou, Linhan Cao, Wei Sun, Xiangyang Zhu, Weixia Zhang, Yucheng Zhu, Jing Liu, Dandan Zhu, Guangtao Zhai, Xiongkuo Min, Zhichao Zhang, Xinyue Li, Shubo Xu, Anh Dao, Yifan Li, Hongyuan Yu, Jiaojiao Yi, Yiding Tian, Yupeng Wu, Feiran Sun, Lijuan Liao, Song Jiang</strong></p>
<p>This paper presents a summary of the VQualA 2025 Challenge on Visual Quality Comparison for Large Multimodal Models (LMMs), hosted as part of the ICCV 2025 Workshop on Visual Quality Assessment. The challenge aims to evaluate and enhance the ability of state-of-the-art LMMs to perform open-ended and detailed reasoning about visual quality differences across multiple images. To this end, the competition introduces a novel benchmark comprising thousands of coarse-to-fine grained visual quality comparison tasks, spanning single images, pairs, and multi-image groups. Each task requires models to provide accurate quality judgments. The competition emphasizes holistic evaluation protocols, including 2AFC-based binary preference and multi-choice questions (MCQs). Around 100 participants submitted entries, with five models demonstrating the emerging capabilities of instruction-tuned LMMs on quality assessment. This challenge marks a significant step toward open-domain visual quality reasoning and comparison and serves as a catalyst for future research on interpretable and human-aligned quality evaluation systems. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†VQualA 2025æŒ‘æˆ˜èµ›çš„å†…å®¹ï¼Œè¯¥æŒ‘æˆ˜èµ›ä½œä¸ºICCV 2025è§†è§‰è´¨é‡è¯„ä¼°ç ”è®¨ä¼šçš„ä¸€éƒ¨åˆ†ï¼Œæ—¨åœ¨è¯„ä¼°å’Œæ”¹è¿›æœ€æ–°å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨è·¨å¤šå¼ å›¾åƒè¿›è¡Œå¼€æ”¾å¼å’Œè¯¦ç»†æ¨ç†è§†è§‰è´¨é‡å·®å¼‚æ–¹é¢çš„èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæ¯”èµ›å¼•å…¥äº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«ä»ç²—åˆ°ç»†ç²’åº¦çš„è§†è§‰è´¨é‡æ¯”è¾ƒä»»åŠ¡æ•°åƒä¸ªï¼Œæ¶µç›–å•å¼ å›¾åƒã€å›¾åƒå¯¹å’Œå¤šå›¾åƒç»„ã€‚æ¯ä¸ªä»»åŠ¡éƒ½éœ€è¦æ¨¡å‹æä¾›å‡†ç¡®çš„è´¨é‡åˆ¤æ–­ã€‚æ¯”èµ›å¼ºè°ƒæ•´ä½“è¯„ä¼°åè®®ï¼ŒåŒ…æ‹¬åŸºäº2AFCçš„äºŒå…ƒåå¥½å’Œå¤šé¡¹é€‰æ‹©é¢˜ï¼ˆMCQsï¼‰ã€‚çº¦æœ‰100åå‚èµ›è€…æäº¤äº†å‚èµ›ä½œå“ï¼Œå…¶ä¸­äº”ç§æ¨¡å‹å±•ç¤ºäº†æŒ‡ä»¤è°ƒæ•´å‹LMMsåœ¨è´¨é‡è¯„ä¼°æ–¹é¢çš„æ–°å…´èƒ½åŠ›ã€‚æœ¬æ¬¡æŒ‘æˆ˜èµ›æ˜¯æœç€å¼€æ”¾å¼åŸŸè§†è§‰è´¨é‡æ¨ç†å’Œæ¯”è¾ƒè¿ˆå‡ºçš„é‡è¦ä¸€æ­¥ï¼Œå¹¶ä¸ºæœªæ¥å¯è§£é‡Šæ€§å’Œäººç±»å¯¹é½è´¨é‡è¯„ä¼°ç³»ç»Ÿçš„ç ”ç©¶èµ·åˆ°äº†æ¨åŠ¨ä½œç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.09190v1">PDF</a> ICCV VQualA Workshop 2025</p>
<p><strong>Summary</strong></p>
<p>è§†è§‰è´¨é‡å¯¹æ¯”æŒ‘æˆ˜â€”â€”è§†è§‰è´¨é‡å¯¹æ¯”å¯¹äºå¤§æ¨¡æ€æ¨¡å‹æŒ‘æˆ˜ï¼ˆVQualA 2025ï¼‰æ€»ç»“ã€‚è¯¥æŒ‘æˆ˜æ—¨åœ¨è¯„ä¼°ä¸æé«˜å…ˆè¿›çš„å¤§æ¨¡æ€æ¨¡å‹åœ¨å¤šä¸ªå›¾åƒä¹‹é—´è§†è§‰è´¨é‡å·®å¼‚çš„æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡å¼•å…¥åŒ…å«æ•°åƒä¸ªç²—åˆ°ç»†ç²’åº¦è§†è§‰è´¨é‡å¯¹æ¯”ä»»åŠ¡çš„æ–°åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬å•å›¾åƒã€å›¾åƒå¯¹å’Œå¤šå›¾åƒç»„ï¼Œè¯¥æŒ‘æˆ˜å¼ºè°ƒæ•´ä½“è¯„ä¼°åè®®ï¼ŒåŒ…æ‹¬åŸºäº2AFCçš„äºŒå…ƒåå¥½å’Œå¤šé€‰æ‹©é¢˜ã€‚å¤§çº¦ä¸€ç™¾åå‚ä¸è€…æäº¤å‚èµ›ä½œå“ï¼Œå…¶ä¸­äº”ç§æ¨¡å‹å±•ç°äº†å…¶åœ¨è´¨é‡è¯„ä¼°æ–¹é¢çš„æ½œåŠ›ã€‚æ­¤æŒ‘æˆ˜æ˜¯è¿ˆå‘å¼€æ”¾å¼è§†è§‰è´¨é‡æ¨ç†å’Œå¯¹æ¯”çš„é‡è¦ä¸€æ­¥ï¼Œå¹¶ä¸ºæœªæ¥å¯è§£é‡Šå’Œäººç±»å¯¹é½çš„è´¨é‡è¯„ä¼°ç³»ç»Ÿçš„ç ”ç©¶èµ·åˆ°äº†æ¨åŠ¨ä½œç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VQualA 2025 Challengeæ—¨åœ¨è¯„ä¼°å¤§æ¨¡æ€æ¨¡å‹å¯¹å¤šä¸ªå›¾åƒä¹‹é—´è§†è§‰è´¨é‡å·®å¼‚çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>æŒ‘æˆ˜å¼•å…¥äº†åŒ…å«ç²—åˆ°ç»†ç²’åº¦è§†è§‰è´¨é‡å¯¹æ¯”ä»»åŠ¡çš„æ–°å‹åŸºå‡†æµ‹è¯•ã€‚</li>
<li>è¯¥æŒ‘æˆ˜å¼ºè°ƒå…¨é¢çš„è¯„ä¼°åè®®ï¼ŒåŒ…æ‹¬åŸºäº2AFCçš„äºŒå…ƒåå¥½å’Œå¤šé€‰é¢˜å½¢å¼ã€‚</li>
<li>æœ‰å¤§çº¦ä¸€ç™¾åå‚ä¸è€…å‚ä¸äº†æ­¤æ¬¡æŒ‘æˆ˜ã€‚</li>
<li>æœ‰äº”ç§æ¨¡å‹åœ¨è´¨é‡è¯„ä¼°æ–¹é¢è¡¨ç°å‡ºæ½œåŠ›ã€‚</li>
<li>æ­¤æŒ‘æˆ˜æ˜¯è§†è§‰è´¨é‡å¯¹æ¯”å’Œæ¨ç†é¢†åŸŸçš„é‡è¦è¿›å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.09190">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2509.09190v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2509.09190v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2509.09190v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2509.09190v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MR-UIE-Multi-Perspective-Reasoning-with-Reinforcement-Learning-for-Universal-Information-Extraction"><a href="#MR-UIE-Multi-Perspective-Reasoning-with-Reinforcement-Learning-for-Universal-Information-Extraction" class="headerlink" title="MR-UIE: Multi-Perspective Reasoning with Reinforcement Learning for   Universal Information Extraction"></a>MR-UIE: Multi-Perspective Reasoning with Reinforcement Learning for   Universal Information Extraction</h2><p><strong>Authors:Zhongqiu Li, Shiquan Wang, Ruiyu Fang, Mengjiao Bao, Zhenhe Wu, Shuangyong Song, Yongxiang Li, Zhongjiang He</strong></p>
<p>Large language models (LLMs) demonstrate robust capabilities across diverse research domains. However, their performance in universal information extraction (UIE) remains insufficient, especially when tackling structured output scenarios that involve complex schema descriptions and require multi-step reasoning. While existing approaches enhance the performance of LLMs through in-context learning and instruction tuning, significant limitations nonetheless persist. To enhance the modelâ€™s generalization ability, we propose integrating reinforcement learning (RL) with multi-perspective reasoning for information extraction (IE) tasks. Our work transitions LLMs from passive extractors to active reasoners, enabling them to understand not only what to extract but also how to reason. Experiments conducted on multiple IE benchmarks demonstrate that MR-UIE consistently elevates extraction accuracy across domains and surpasses state-of-the-art methods on several datasets. Furthermore, incorporating multi-perspective reasoning into RL notably enhances generalization in complex IE tasks, underscoring the critical role of reasoning in challenging scenarios. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šä¸ªç ”ç©¶é¢†åŸŸä¸­è¡¨ç°å‡ºäº†å¼ºå¤§çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨é€šç”¨ä¿¡æ¯æå–ï¼ˆUIEï¼‰æ–¹é¢çš„è¡¨ç°ä»ç„¶ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†æ¶‰åŠå¤æ‚æ¨¡å¼æè¿°å’Œéœ€è¦å¤šæ­¥éª¤æ¨ç†çš„ç»“æ„åŒ–è¾“å‡ºåœºæ™¯æ—¶ã€‚è™½ç„¶ç°æœ‰æ–¹æ³•é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ å’ŒæŒ‡ä»¤å¾®è°ƒæé«˜äº†LLMçš„æ€§èƒ½ï¼Œä½†ä»å­˜åœ¨æ˜¾è‘—å±€é™æ€§ã€‚ä¸ºäº†æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œæˆ‘ä»¬æå‡ºäº†å°†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸å¤šè§†è§’æ¨ç†ç›¸ç»“åˆï¼Œç”¨äºä¿¡æ¯æå–ï¼ˆIEï¼‰ä»»åŠ¡ã€‚æˆ‘ä»¬çš„å·¥ä½œä½¿LLMä»è¢«åŠ¨æå–å™¨è½¬å˜ä¸ºç§¯ææ¨ç†å™¨ï¼Œä½¿å®ƒä»¬ä¸ä»…èƒ½å¤Ÿç†è§£è¦æå–çš„å†…å®¹ï¼Œè€Œä¸”èƒ½å¤Ÿç†è§£å¦‚ä½•è¿›è¡Œæ¨ç†ã€‚åœ¨å¤šä¸ªä¿¡æ¯æå–åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒMR-UIEåœ¨è·¨åŸŸæå–å‡†ç¡®æ€§æ–¹é¢è¡¨ç°ä¸€è‡´ï¼Œå¹¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¶…è¿‡äº†æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æ­¤å¤–ï¼Œå°†å¤šè§†è§’æ¨ç†èå…¥å¼ºåŒ–å­¦ä¹ ï¼Œæ˜¾è‘—æé«˜äº†å¤æ‚ä¿¡æ¯æå–ä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›ï¼Œçªæ˜¾äº†æ¨ç†åœ¨å…·æœ‰æŒ‘æˆ˜æ€§åœºæ™¯ä¸­çš„å…³é”®ä½œç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.09082v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šä¸ªç ”ç©¶é¢†åŸŸä¸­è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†åœ¨é€šç”¨ä¿¡æ¯æå–ï¼ˆUIEï¼‰æ–¹é¢çš„è¡¨ç°ä»ç„¶ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¶‰åŠå¤æ‚æ¨¡å¼æè¿°å’Œéœ€è¦å¤šæ­¥éª¤æ¨ç†çš„ç»“æ„åŒ–è¾“å‡ºåœºæ™¯ä¸­ã€‚ä¸ºæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œæå‡ºäº†ç»“åˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œå¤šè§’åº¦æ¨ç†çš„ä¿¡æ¯æå–ï¼ˆIEï¼‰ä»»åŠ¡æ–¹æ³•ã€‚è¯¥æ–¹æ³•ä½¿LLMä»è¢«åŠ¨æå–å™¨è½¬å˜ä¸ºç§¯ææ¨ç†å™¨ï¼Œä¸ä»…ç†è§£éœ€è¦æå–çš„å†…å®¹ï¼Œè¿˜ç†è§£å¦‚ä½•æ¨ç†ã€‚åœ¨å¤šä¸ªä¿¡æ¯æå–åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMR-UIEåœ¨å¤šä¸ªé¢†åŸŸä¸­çš„æå–å‡†ç¡®æ€§ä¸æ–­æé«˜ï¼Œå¹¶åœ¨æŸäº›æ•°æ®é›†ä¸Šè¶…è¶Šäº†æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æ­¤å¤–ï¼Œå°†å¤šè§’åº¦æ¨ç†èå…¥RLæ˜¾è‘—æé«˜äº†åœ¨å¤æ‚ä¿¡æ¯æå–ä»»åŠ¡ä¸­çš„æ³›åŒ–èƒ½åŠ›ï¼Œçªæ˜¾äº†æ¨ç†åœ¨æŒ‘æˆ˜åœºæ™¯ä¸­çš„å…³é”®ä½œç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨UIEæ–¹é¢çš„è¡¨ç°ä»æœ‰å¾…æé«˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤æ‚æ¨¡å¼æè¿°å’Œå¤šæ­¥éª¤æ¨ç†çš„ç»“æ„åŒ–è¾“å‡ºåœºæ™¯æ–¹é¢ã€‚</li>
<li>é€šè¿‡ç»“åˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œå¤šè§’åº¦æ¨ç†ï¼Œå¯ä»¥æé«˜LLMåœ¨IEä»»åŠ¡ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>MR-UIEæ–¹æ³•ä½¿LLMä»è¢«åŠ¨æå–ä¿¡æ¯è½¬å˜ä¸ºç§¯ææ¨ç†ï¼Œä½¿å…¶ä¸ä»…ç†è§£æå–å†…å®¹ï¼Œè¿˜ç†è§£å¦‚ä½•æ¨ç†ã€‚</li>
<li>MR-UIEåœ¨å¤šä¸ªä¿¡æ¯æå–åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œæ˜¾ç¤ºå‡ºå…¶æœ‰æ•ˆæ€§ã€‚</li>
<li>ç»“åˆå¤šè§’åº¦æ¨ç†çš„RLåœ¨å¤æ‚IEä»»åŠ¡ä¸­æ˜¾è‘—æé«˜æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>å¤šè§’åº¦æ¨ç†åœ¨æŒ‘æˆ˜åœºæ™¯ä¸­èµ·åˆ°å…³é”®ä½œç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.09082">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2509.09082v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2509.09082v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2509.09082v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Recurrence-Meets-Transformers-for-Universal-Multimodal-Retrieval"><a href="#Recurrence-Meets-Transformers-for-Universal-Multimodal-Retrieval" class="headerlink" title="Recurrence Meets Transformers for Universal Multimodal Retrieval"></a>Recurrence Meets Transformers for Universal Multimodal Retrieval</h2><p><strong>Authors:Davide Caffagni, Sara Sarto, Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara</strong></p>
<p>With the rapid advancement of multimodal retrieval and its application in LLMs and multimodal LLMs, increasingly complex retrieval tasks have emerged. Existing methods predominantly rely on task-specific fine-tuning of vision-language models and are limited to single-modality queries or documents. In this paper, we propose ReT-2, a unified retrieval model that supports multimodal queries, composed of both images and text, and searches across multimodal document collections where text and images coexist. ReT-2 leverages multi-layer representations and a recurrent Transformer architecture with LSTM-inspired gating mechanisms to dynamically integrate information across layers and modalities, capturing fine-grained visual and textual details. We evaluate ReT-2 on the challenging M2KR and M-BEIR benchmarks across different retrieval configurations. Results demonstrate that ReT-2 consistently achieves state-of-the-art performance across diverse settings, while offering faster inference and reduced memory usage compared to prior approaches. When integrated into retrieval-augmented generation pipelines, ReT-2 also improves downstream performance on Encyclopedic-VQA and InfoSeek datasets. Our source code and trained models are publicly available at: <a target="_blank" rel="noopener" href="https://github.com/aimagelab/ReT-2">https://github.com/aimagelab/ReT-2</a> </p>
<blockquote>
<p>éšç€å¤šæ¨¡æ€æ£€ç´¢æŠ€æœ¯çš„å¿«é€Ÿå‘å±•åŠå…¶åœ¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„åº”ç”¨ï¼Œå‡ºç°äº†è¶Šæ¥è¶Šå¤šå¤æ‚çš„æ£€ç´¢ä»»åŠ¡ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºé’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„è§†è§‰è¯­è¨€æ¨¡å‹çš„å¾®è°ƒï¼Œå¹¶å±€é™äºå•æ¨¡æ€æŸ¥è¯¢æˆ–æ–‡æ¡£ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ReT-2ï¼Œè¿™æ˜¯ä¸€ä¸ªæ”¯æŒå¤šæ¨¡æ€æŸ¥è¯¢çš„ç»Ÿä¸€æ£€ç´¢æ¨¡å‹ï¼Œç”±å›¾åƒå’Œæ–‡æœ¬ç»„æˆï¼Œå¯ä»¥åœ¨åŒ…å«æ–‡æœ¬å’Œå›¾åƒçš„è·¨æ¨¡æ€æ–‡æ¡£é›†åˆä¸­è¿›è¡Œæœç´¢ã€‚ReT-2åˆ©ç”¨å¤šå±‚è¡¨ç¤ºå’Œå¾ªç¯ç¥ç»ç½‘ç»œTransformeræ¶æ„ï¼Œç»“åˆLSTMå¯å‘çš„é—¨æ§æœºåˆ¶ï¼ŒåŠ¨æ€åœ°æ•´åˆå„å±‚å’Œå„ç§ä¿¡æ¯æºçš„ä¿¡æ¯ï¼Œæ•æ‰ç²¾ç»†çš„è§†è§‰å’Œæ–‡æœ¬ç»†èŠ‚ã€‚æˆ‘ä»¬åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„M2KRå’ŒM-BEIRåŸºå‡†æµ‹è¯•é›†ä¸Šå¯¹ä¸åŒé…ç½®çš„æ£€ç´¢ç³»ç»Ÿè¿›è¡Œäº†è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼ŒReT-2åœ¨å„ç§ä¸åŒè®¾ç½®ä¸‹å‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ï¼ŒåŒæ—¶ä¸å…ˆå‰çš„æ–¹æ³•ç›¸æ¯”ï¼Œæ¨ç†é€Ÿåº¦æ›´å¿«ï¼Œå†…å­˜ä½¿ç”¨æ›´å°‘ã€‚å½“é›†æˆåˆ°æ£€ç´¢å¢å¼ºç”Ÿæˆç®¡é“æ—¶ï¼ŒReT-2ä¹Ÿåœ¨ç™¾ç§‘å…¨ä¹¦å¼è§†è§‰é—®ç­”å’Œä¿¡æ¯æ£€ç´¢æ•°æ®é›†ä¸Šçš„ä¸‹æ¸¸æ€§èƒ½æœ‰æ‰€æå‡ã€‚æˆ‘ä»¬çš„æºä»£ç å’Œè®­ç»ƒå¥½çš„æ¨¡å‹å¯åœ¨ä»¥ä¸‹ç½‘å€å…¬å¼€è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/aimagelab/ReT-2">https://github.com/aimagelab/ReT-2</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.08897v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>éšç€å¤šæ¨¡æ€æ£€ç´¢åœ¨LLMå’Œå¤šæ¨¡æ€LLMä¸­çš„è¿…é€Ÿå‘å±•å’Œåº”ç”¨ï¼Œå‡ºç°äº†è¶Šæ¥è¶Šå¤æ‚çš„æ£€ç´¢ä»»åŠ¡ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºé’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„è§†è§‰è¯­è¨€æ¨¡å‹çš„å¾®è°ƒï¼Œå¹¶å±€é™äºå•æ¨¡æ€æŸ¥è¯¢æˆ–æ–‡æ¡£ã€‚æœ¬æ–‡æå‡ºäº†ReT-2ï¼Œä¸€ä¸ªæ”¯æŒå›¾æ–‡ç»“åˆçš„å¤šæ¨¡æ€æŸ¥è¯¢çš„æ£€ç´¢æ¨¡å‹ã€‚ReT-2é‡‡ç”¨å¤šå±‚è¡¨ç¤ºå’Œå¾ªç¯Transformeræ¶æ„ï¼Œåˆ©ç”¨LSTMå¯å‘å¼çš„é—¨æ§æœºåˆ¶åŠ¨æ€åœ°æ•´åˆè·¨å±‚å’Œè·¨æ¨¡æ€çš„ä¿¡æ¯ï¼Œæ•æ‰ç²¾ç»†çš„è§†è§‰å’Œæ–‡æœ¬ç»†èŠ‚ã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„M2KRå’ŒM-BEIRåŸºå‡†æµ‹è¯•ä¸­ï¼ŒReT-2åœ¨å¤šç§æ£€ç´¢é…ç½®ä¸­å‡å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ï¼ŒåŒæ—¶ä¸å…ˆå‰çš„æ–¹æ³•ç›¸æ¯”å…·æœ‰æ›´å¿«çš„æ¨ç†é€Ÿåº¦å’Œæ›´ä½çš„å†…å­˜ä½¿ç”¨ã€‚å½“é›†æˆåˆ°æ£€ç´¢å¢å¼ºç”Ÿæˆç®¡é“æ—¶ï¼ŒReT-2åœ¨ç™¾ç§‘å…¨ä¹¦å¼VQAå’Œä¿¡æ¯æ£€ç´¢æ•°æ®é›†ä¸Šçš„ä¸‹æ¸¸æ€§èƒ½ä¹Ÿå¾—åˆ°äº†æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€æ£€ç´¢é¢†åŸŸæ­£åœ¨å¿«é€Ÿå‘å±•ï¼Œé¢ä¸´è¶Šæ¥è¶Šå¤æ‚çš„æ£€ç´¢ä»»åŠ¡ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºç‰¹å®šä»»åŠ¡çš„è§†è§‰è¯­è¨€æ¨¡å‹å¾®è°ƒï¼Œå¹¶å±€é™äºå•æ¨¡æ€æŸ¥è¯¢ã€‚</li>
<li>ReT-2æ˜¯ä¸€ä¸ªæ”¯æŒå¤šæ¨¡æ€æŸ¥è¯¢çš„æ£€ç´¢æ¨¡å‹ï¼Œèƒ½å¤Ÿå¤„ç†å›¾æ–‡ç»“åˆçš„ä¿¡æ¯ã€‚</li>
<li>ReT-2åˆ©ç”¨å¤šå±‚è¡¨ç¤ºå’Œå¾ªç¯Transformeræ¶æ„ï¼Œç»“åˆLSTMå¯å‘å¼çš„é—¨æ§æœºåˆ¶ã€‚</li>
<li>ReT-2åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬M2KRå’ŒM-BEIRã€‚</li>
<li>ReT-2åœ¨æ¨ç†é€Ÿåº¦å’Œå†…å­˜ä½¿ç”¨æ–¹é¢ç›¸æ¯”ä»¥å‰çš„æ–¹æ³•æœ‰ä¼˜åŠ¿ã€‚</li>
<li>ReT-2é›†æˆåˆ°æ£€ç´¢å¢å¼ºç”Ÿæˆç®¡é“åï¼Œåœ¨ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ä¸Šæœ‰æ‰€æå‡ï¼Œä¾‹å¦‚åœ¨Encyclopedic-VQAå’ŒInfoSeekæ•°æ®é›†ä¸Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.08897">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2509.08897v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2509.08897v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2509.08897v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2509.08897v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="AI-Self-preferencing-in-Algorithmic-Hiring-Empirical-Evidence-and-Insights"><a href="#AI-Self-preferencing-in-Algorithmic-Hiring-Empirical-Evidence-and-Insights" class="headerlink" title="AI Self-preferencing in Algorithmic Hiring: Empirical Evidence and   Insights"></a>AI Self-preferencing in Algorithmic Hiring: Empirical Evidence and   Insights</h2><p><strong>Authors:Jiannan Xu, Gujie Li, Jane Yi Jiang</strong></p>
<p>As generative artificial intelligence (AI) tools become widely adopted, large language models (LLMs) are increasingly involved on both sides of decision-making processes, ranging from hiring to content moderation. This dual adoption raises a critical question: do LLMs systematically favor content that resembles their own outputs? Prior research in computer science has identified self-preference bias â€“ the tendency of LLMs to favor their own generated content â€“ but its real-world implications have not been empirically evaluated. We focus on the hiring context, where job applicants often rely on LLMs to refine resumes, while employers deploy them to screen those same resumes. Using a large-scale controlled resume correspondence experiment, we find that LLMs consistently prefer resumes generated by themselves over those written by humans or produced by alternative models, even when content quality is controlled. The bias against human-written resumes is particularly substantial, with self-preference bias ranging from 68% to 88% across major commercial and open-source models. To assess labor market impact, we simulate realistic hiring pipelines across 24 occupations. These simulations show that candidates using the same LLM as the evaluator are 23% to 60% more likely to be shortlisted than equally qualified applicants submitting human-written resumes, with the largest disadvantages observed in business-related fields such as sales and accounting. We further demonstrate that this bias can be reduced by more than 50% through simple interventions targeting LLMsâ€™ self-recognition capabilities. These findings highlight an emerging but previously overlooked risk in AI-assisted decision making and call for expanded frameworks of AI fairness that address not only demographic-based disparities, but also biases in AI-AI interactions. </p>
<blockquote>
<p>éšç€ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰å·¥å…·è¢«å¹¿æ³›é‡‡çº³ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å†³ç­–è¿‡ç¨‹ä¸­çš„ä½œç”¨æ—¥ç›Šå‡¸æ˜¾ï¼Œä»æ‹›è˜åˆ°å†…å®¹å®¡æ ¸æ— ä¸€ä¾‹å¤–ã€‚è¿™ç§åŒé‡é‡‡çº³å¼•å‘äº†ä¸€ä¸ªå…³é”®é—®é¢˜ï¼šLLMæ˜¯å¦ä¼šç³»ç»Ÿæ€§åœ°åçˆ±ä¸å…¶è‡ªèº«è¾“å‡ºç›¸ä¼¼çš„å†…å®¹ï¼Ÿè®¡ç®—æœºç§‘å­¦é¢†åŸŸçš„å‰æœŸç ”ç©¶å·²ç»å‘ç°äº†è‡ªæˆ‘åå¥½åè§â€”â€”LLMå€¾å‘äºåçˆ±å…¶è‡ªå·±ç”Ÿæˆçš„å†…å®¹â€”â€”ä½†å…¶åœ¨å®é™…ä¸–ç•Œçš„å½±å“å°šæœªå¾—åˆ°å®è¯è¯„ä¼°ã€‚æˆ‘ä»¬å…³æ³¨æ‹›è˜èƒŒæ™¯ï¼Œæ±‚èŒè€…å¸¸å¸¸ä¾é LLMæ¥å®Œå–„ç®€å†ï¼Œè€Œé›‡ä¸»åˆ™ä½¿ç”¨å®ƒä»¬æ¥ç­›é€‰è¿™äº›ç®€å†ã€‚é€šè¿‡å¤§è§„æ¨¡æ§åˆ¶çš„ç®€å†å¯¹åº”å®éªŒï¼Œæˆ‘ä»¬å‘ç°LLMå§‹ç»ˆåçˆ±ç”±è‡ªå·±ç”Ÿæˆçš„ç®€å†ï¼Œè€Œéäººç±»æ‰€å†™æˆ–ç”±å…¶ä»–æ¨¡å‹äº§ç”Ÿçš„ç®€å†ï¼Œç”šè‡³åœ¨å†…å®¹è´¨é‡å¾—åˆ°æ§åˆ¶çš„æƒ…å†µä¸‹äº¦æ˜¯å¦‚æ­¤ã€‚å¯¹äººç±»æ’°å†™çš„ç®€å†çš„åè§å°¤ä¸ºä¸¥é‡ï¼Œä¸»è¦å•†ä¸šå’Œå¼€æºæ¨¡å‹çš„è‡ªæˆ‘åå¥½åè§èŒƒå›´ä»68%åˆ°88%ã€‚ä¸ºäº†è¯„ä¼°å¯¹åŠ³åŠ¨å¸‚åœºçš„å½±å“ï¼Œæˆ‘ä»¬åœ¨24ä¸ªèŒä¸šä¸­æ¨¡æ‹Ÿäº†ç°å®çš„æ‹›è˜æµç¨‹ã€‚è¿™äº›æ¨¡æ‹Ÿæ˜¾ç¤ºï¼Œä½¿ç”¨ä¸è¯„ä¼°è€…ç›¸åŒLLMçš„å€™é€‰äººæ¯”æäº¤äººç±»æ’°å†™çš„ç®€å†çš„åŒç­‰æ¡ä»¶çš„ç”³è¯·äººæ›´æœ‰å¯èƒ½è¢«åˆ—å…¥é€‰å®šåå•ï¼Œä¸”åœ¨å•†ä¸šç›¸å…³é¢†åŸŸå¦‚é”€å”®å’Œä¼šè®¡è§‚å¯Ÿåˆ°æœ€å¤§çš„åŠ£åŠ¿ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¯æ˜ï¼Œé€šè¿‡é’ˆå¯¹LLMçš„è‡ªæˆ‘è¯†åˆ«èƒ½åŠ›çš„ç®€å•å¹²é¢„ï¼Œå¯ä»¥å‡å°‘è¶…è¿‡ä¸€åŠçš„åè§ã€‚è¿™äº›å‘ç°çªå‡ºäº†äººå·¥æ™ºèƒ½è¾…åŠ©å†³ç­–åˆ¶å®šä¸­å‡ºç°ä½†ä»¥å‰æœªè¢«é‡è§†çš„é£é™©ï¼Œå¹¶å‘¼åæ‰©å¤§äººå·¥æ™ºèƒ½å…¬å¹³æ€§çš„æ¡†æ¶ï¼Œä¸ä»…è¦è§£å†³åŸºäºäººå£ç»Ÿè®¡çš„å·®è·ï¼Œè¿˜è¦è§£å†³äººå·¥æ™ºèƒ½ä¹‹é—´çš„åè§é—®é¢˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.00462v2">PDF</a> This paper has been accepted as a non-archival submission at EAAMO   2025 and AIES 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>éšç€ç”Ÿæˆå¼äººå·¥æ™ºèƒ½å·¥å…·ï¼ˆAIï¼‰çš„å¹¿æ³›åº”ç”¨ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¶Šæ¥è¶Šå¤šåœ°å‚ä¸åˆ°å†³ç­–è¿‡ç¨‹çš„å„ä¸ªç¯èŠ‚ï¼Œä»æ‹›è˜åˆ°å†…å®¹å®¡æ ¸ä¸ä¸€è€Œè¶³ã€‚è¿™ç§åŒé‡é‡‡ç”¨å¼•å‘äº†ä¸€ä¸ªå…³é”®é—®é¢˜ï¼šLLMæ˜¯å¦ä¼šç³»ç»Ÿæ€§åœ°åçˆ±ä¸å…¶è‡ªèº«è¾“å‡ºç›¸ä¼¼çš„å†…å®¹ï¼Ÿè®¡ç®—æœºç§‘å­¦é¢†åŸŸçš„å‰æœŸç ”ç©¶å·²ç»å‘ç°äº†LLMçš„è‡ªæˆ‘åå¥½åå·®â€”â€”å³LLMå€¾å‘äºé’çè‡ªå·±ç”Ÿæˆçš„å†…å®¹ï¼Œä½†å…¶ç°å®å½±å“å°šæœªç»è¿‡å®è¯è¯„ä¼°ã€‚æœ¬ç ”ç©¶èšç„¦äºæ‹›è˜ç¯èŠ‚ï¼Œåº”è˜è€…å¾€å¾€ä¾èµ–LLMæ¥å®Œå–„ç®€å†ï¼Œè€Œé›‡ä¸»åˆ™ä½¿ç”¨å®ƒä»¬æ¥ç­›é€‰è¿™äº›ç®€å†ã€‚é€šè¿‡å¤§è§„æ¨¡æ§åˆ¶æ€§ç®€å†å¯¹åº”å®éªŒï¼Œæˆ‘ä»¬å‘ç°LLMå§‹ç»ˆåå¥½ç”±è‡ªèº«ç”Ÿæˆçš„ç®€å†ï¼Œè€Œéäººç±»æ’°å†™æˆ–ç”±å…¶ä»–æ¨¡å‹äº§ç”Ÿçš„ç®€å†ï¼Œç”šè‡³åœ¨å†…å®¹è´¨é‡å¾—åˆ°æ§åˆ¶çš„æƒ…å†µä¸‹äº¦æ˜¯å¦‚æ­¤ã€‚ç›¸è¾ƒäºäººç±»æ’°å†™çš„ç®€å†ï¼Œå¯¹åè€…çš„åè§å°¤ä¸ºæ˜¾è‘—ï¼Œåœ¨ä¸åŒçš„å¤§å‹å•†ä¸šå’Œå¼€æºæ¨¡å‹ä¸­ï¼Œè‡ªæˆ‘åå¥½åå·®èŒƒå›´åœ¨68%è‡³88%ä¹‹é—´ã€‚ä¸ºäº†è¯„ä¼°å¯¹åŠ³åŠ¨åŠ›å¸‚åœºçš„å½±å“ï¼Œæˆ‘ä»¬å¯¹24ç±»èŒä¸šçš„ç°å®æ‹›è˜æµç¨‹è¿›è¡Œäº†æ¨¡æ‹Ÿã€‚è¿™äº›æ¨¡æ‹Ÿæ˜¾ç¤ºï¼Œä½¿ç”¨ä¸è¯„ä¼°è€…ç›¸åŒLLMçš„å€™é€‰äººè¢«çŸ­åå•åˆ—å‡ºçš„å¯èƒ½æ€§æ¯”æäº¤äººç±»æ’°å†™ç®€å†çš„åŒç­‰èµ„æ ¼ç”³è¯·äººé«˜å‡º23%è‡³60%ï¼Œåœ¨é”€å”®å’Œä¼šè®¡ç­‰å•†ä¸šç›¸å…³é¢†åŸŸè§‚å¯Ÿåˆ°çš„ä¸åˆ©æƒ…å†µæœ€ä¸ºä¸¥é‡ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¯æ˜ï¼Œé€šè¿‡é’ˆå¯¹LLMçš„è‡ªæˆ‘è¯†åˆ«èƒ½åŠ›çš„ç®€å•å¹²é¢„æªæ–½ï¼Œè¿™ç§åè§å¯ä»¥å‡å°‘è¶…è¿‡55%ã€‚è¿™äº›å‘ç°çªæ˜¾äº†äººå·¥æ™ºèƒ½è¾…åŠ©å†³ç­–åˆ¶å®šä¸­å‡ºç°çš„æ–°å…´ä½†ä»¥å‰è¢«å¿½è§†çš„é£é™©ï¼Œå¹¶å‘¼åæ‰©å¤§äººå·¥æ™ºèƒ½å…¬å¹³æ€§çš„æ¡†æ¶ï¼Œä¸ä»…è¦è§£å†³åŸºäºäººå£ç»Ÿè®¡æ•°æ®çš„å·®å¼‚ï¼Œè¿˜è¦è§£å†³äººå·¥æ™ºèƒ½ä¹‹é—´çš„äº’åŠ¨åè§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ‹›è˜ç­‰å†³ç­–è¿‡ç¨‹ä¸­å¹¿æ³›åº”ç”¨ã€‚</li>
<li>LLMè¡¨ç°å‡ºè‡ªæˆ‘åå¥½åå·®ï¼Œæ›´å€¾å‘äºé€‰æ‹©è‡ªèº«ç”Ÿæˆçš„æ–‡æœ¬å†…å®¹ã€‚</li>
<li>åœ¨æ§åˆ¶å†…å®¹è´¨é‡çš„æƒ…å†µä¸‹ï¼ŒLLMå¯¹ç”±å…¶ä»–æ¨¡å‹æˆ–äººç±»æ’°å†™çš„ç®€å†å­˜åœ¨æ˜¾è‘—åè§ã€‚</li>
<li>ä½¿ç”¨ä¸è¯„ä¼°è€…ç›¸åŒLLMçš„åº”è˜è€…åœ¨æ‹›è˜æµç¨‹ä¸­è¢«çŸ­åå•åˆ—å‡ºçš„å¯èƒ½æ€§æ›´é«˜ã€‚</li>
<li>è¿™ç§åè§åœ¨å•†ä¸šå’Œå¼€æºæ¨¡å‹ä¸­å°¤ä¸ºæ˜¾è‘—ï¼Œä¸”å¯èƒ½å¯¹æŸäº›èŒä¸šé€ æˆä¸åˆ©å½±å“ã€‚</li>
<li>é€šè¿‡ç®€å•å¹²é¢„æªæ–½ï¼Œå¯ä»¥æœ‰æ•ˆå‡å°‘LLMçš„è‡ªæˆ‘åå¥½åå·®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.00462">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2509.00462v2/page_0_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Improving-Alignment-in-LVLMs-with-Debiased-Self-Judgment"><a href="#Improving-Alignment-in-LVLMs-with-Debiased-Self-Judgment" class="headerlink" title="Improving Alignment in LVLMs with Debiased Self-Judgment"></a>Improving Alignment in LVLMs with Debiased Self-Judgment</h2><p><strong>Authors:Sihan Yang, Chenhang Cui, Zihao Zhao, Yiyang Zhou, Weilong Yan, Ying Wei, Huaxiu Yao</strong></p>
<p>The rapid advancements in Large Language Models (LLMs) and Large Visual-Language Models (LVLMs) have opened up new opportunities for integrating visual and linguistic modalities. However, effectively aligning these modalities remains challenging, often leading to hallucinationsâ€“where generated outputs are not grounded in the visual inputâ€“and raising safety concerns across various domains. Existing alignment methods, such as instruction tuning and preference tuning, often rely on external datasets, human annotations, or complex post-processing, which limit scalability and increase costs. To address these challenges, we propose a novel approach that generates the debiased self-judgment score, a self-evaluation metric created internally by the model without relying on external resources. This enables the model to autonomously improve alignment. Our method enhances both decoding strategies and preference tuning processes, resulting in reduced hallucinations, enhanced safety, and improved overall capability. Empirical results show that our approach significantly outperforms traditional methods, offering a more effective solution for aligning LVLMs. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„å¿«é€Ÿå‘å±•ä¸ºæ•´åˆè§†è§‰å’Œè¯­è¨€æ¨¡å¼æä¾›äº†æ–°çš„æœºä¼šã€‚ç„¶è€Œï¼Œæœ‰æ•ˆåœ°å¯¹é½è¿™äº›æ¨¡å¼ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œè¿™å¸¸å¸¸å¯¼è‡´ç”Ÿæˆçš„è¾“å‡ºæ²¡æœ‰åŸºäºè§†è§‰è¾“å…¥ï¼Œå¹¶ä¸”åœ¨å„ä¸ªé¢†åŸŸå¼•å‘å®‰å…¨æ‹…å¿§ã€‚ç°æœ‰çš„å¯¹é½æ–¹æ³•ï¼Œå¦‚æŒ‡ä»¤è°ƒæ•´å’Œåå¥½è°ƒæ•´ï¼Œé€šå¸¸ä¾èµ–äºå¤–éƒ¨æ•°æ®é›†ã€äººå·¥æ³¨é‡Šæˆ–å¤æ‚çš„åå¤„ç†ï¼Œè¿™é™åˆ¶äº†å¯æ‰©å±•æ€§å¹¶å¢åŠ äº†æˆæœ¬ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œç”Ÿæˆå»åè‡ªæˆ‘åˆ¤æ–­åˆ†æ•°ï¼Œè¿™æ˜¯ä¸€ä¸ªç”±æ¨¡å‹å†…éƒ¨åˆ›å»ºçš„è‡ªè¯„æŒ‡æ ‡ï¼Œæ— éœ€ä¾èµ–å¤–éƒ¨èµ„æºã€‚è¿™ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿè‡ªä¸»åœ°æ”¹è¿›å¯¹é½ã€‚æˆ‘ä»¬çš„æ–¹æ³•æé«˜äº†è§£ç ç­–ç•¥å’Œåå¥½è°ƒæ•´è¿‡ç¨‹ï¼Œå¯¼è‡´å‡å°‘äº†å¹»è§‰ã€å¢å¼ºäº†å®‰å…¨æ€§å¹¶æé«˜äº†æ•´ä½“èƒ½åŠ›ã€‚ç»éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œä¸ºLVLMsçš„å¯¹é½æä¾›äº†æ›´æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.20655v2">PDF</a> EMNLP 2025 Findings</p>
<p><strong>Summary</strong></p>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œæ•´åˆè§†è§‰å’Œè¯­è¨€å­¦æ¨¡æ€çš„æ–°æœºä¼šå·²ç»æ‰“å¼€ã€‚ç„¶è€Œï¼Œæœ‰æ•ˆå¯¹é½è¿™äº›æ¨¡æ€ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå¯èƒ½å¯¼è‡´ç”Ÿæˆçš„è¾“å‡ºä¸åŸºäºè§†è§‰è¾“å…¥ï¼Œå¹¶åœ¨ä¸åŒé¢†åŸŸå¼•å‘å®‰å…¨é—®é¢˜ã€‚ç°æœ‰å¯¹é½æ–¹æ³•å¸¸ä¾èµ–å¤–éƒ¨æ•°æ®é›†ã€äººå·¥æ³¨é‡Šæˆ–å¤æ‚åå¤„ç†ï¼Œé™åˆ¶äº†å¯æ‰©å±•æ€§å¹¶å¢åŠ äº†æˆæœ¬ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œç”Ÿæˆåè¯¯è‡ªæˆ‘åˆ¤æ–­åˆ†æ•°ï¼Œè¿™æ˜¯ä¸€ä¸ªç”±æ¨¡å‹å†…éƒ¨åˆ›å»ºçš„è‡ªæˆ‘è¯„ä»·æŒ‡æ ‡ï¼Œæ— éœ€ä¾èµ–å¤–éƒ¨èµ„æºã€‚è¿™ä½¿æ¨¡å‹èƒ½å¤Ÿè‡ªä¸»æ”¹å–„å¯¹é½ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ”¹è¿›äº†è§£ç ç­–ç•¥å’Œåå¥½è°ƒæ•´è¿‡ç¨‹ï¼Œå‡å°‘äº†å¹»è§†ç°è±¡ï¼Œå¢å¼ºäº†å®‰å…¨æ€§å¹¶æé«˜äº†æ•´ä½“èƒ½åŠ›ã€‚ç»éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œä¸ºLVLMsçš„å¯¹é½æä¾›äº†æ›´æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„æ•´åˆå¸¦æ¥äº†æ–°çš„æœºä¼šï¼Œä½†å¯¹é½è§†è§‰å’Œè¯­è¨€å­¦æ¨¡æ€ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>ç°æœ‰å¯¹é½æ–¹æ³•å­˜åœ¨ä¾èµ–å¤–éƒ¨èµ„æºã€æˆæœ¬é«˜å’Œæ‰©å±•æ€§æœ‰é™çš„é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„è‡ªæˆ‘è¯„ä»·æŒ‡æ ‡â€”â€”åè¯¯è‡ªæˆ‘åˆ¤æ–­åˆ†æ•°ï¼Œæ— éœ€ä¾èµ–å¤–éƒ¨èµ„æºï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿè‡ªä¸»æ”¹å–„å¯¹é½ã€‚</li>
<li>æ–°æ–¹æ³•æ”¹è¿›äº†è§£ç ç­–ç•¥å’Œåå¥½è°ƒæ•´è¿‡ç¨‹ï¼Œå‡å°‘äº†å¹»è§†ç°è±¡ã€‚</li>
<li>æ–°æ–¹æ³•å¢å¼ºäº†å®‰å…¨æ€§å¹¶æé«˜äº†æ¨¡å‹çš„æ•´ä½“èƒ½åŠ›ã€‚</li>
<li>å®è¯ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œæ–°æ–¹æ³•åœ¨LVLMsçš„å¯¹é½ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.20655">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2508.20655v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2508.20655v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2508.20655v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2508.20655v2/page_4_0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Can-Large-Language-Models-Understand-As-Well-As-Apply-Patent-Regulations-to-Pass-a-Hands-On-Patent-Attorney-Test"><a href="#Can-Large-Language-Models-Understand-As-Well-As-Apply-Patent-Regulations-to-Pass-a-Hands-On-Patent-Attorney-Test" class="headerlink" title="Can Large Language Models Understand As Well As Apply Patent Regulations   to Pass a Hands-On Patent Attorney Test?"></a>Can Large Language Models Understand As Well As Apply Patent Regulations   to Pass a Hands-On Patent Attorney Test?</h2><p><strong>Authors:Bhakti Khera, Rezvan Alamian, Pascal A. Scherz, Stephan M. Goetz</strong></p>
<p>The legal field already uses various large language models (LLMs) in actual applications, but their quantitative performance and reasons for it are underexplored. We evaluated several open-source and proprietary LLMs â€“ including GPT-series, Anthropic, Deepseek and Llama-3, variants â€“ on parts of the European Qualifying Examination (EQE) for future European Patent Attorneys. OpenAI o1 led with 0.82 accuracy and 0.81 F1 score, whereas (Amazon Web Services) AWS Llama 3.1 8B lagged at 0.50 accuracy, and a Python-deployed Llama 3.1 8B scored 0.55. The latter two are within the range of mere guessing for the two-answer forced-choice design. None of the evaluated models could have passed the examination fully, as accuracy never exceeded the average threshold of 0.90 required for professional-level standards â€“ also not models that are regularly promoted for their assumed beyond-PhD- and bar-admitted-lawyer-level performance. GPT-4o excelled at integrating text and graphics, while Claude 3 Opus often lost formatting coherence. Human patent experts evaluated the textual justifications and uncovered various critical shortcomings of each model. They valued clarity and legal rationale over the raw correctness of the answers, which revealed misalignment between automatic metrics and expert judgment. Model outputs were sensitive to modest temperature changes and prompt wording, which underscores the remaining necessity of expert oversight. Future work should target logical consistency, robust multimodality, and adaptive prompting to approach human-level patent proficiency. In summary, despite the outstanding performance of recent large models, the general public might overestimate their performance. The field has a long way to go to develop a virtual patent attorney. This paper wants to point out several specific limitations that need solutions. </p>
<blockquote>
<p>åœ¨æ³•å¾‹é¢†åŸŸï¼Œè™½ç„¶å·²ç»åœ¨å®é™…åº”ç”¨ä¸­ä½¿ç”¨å„ç§å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œä½†å®ƒä»¬çš„å®šé‡æ€§èƒ½å’ŒåŸå› å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æˆ‘ä»¬å¯¹å‡ ä¸ªå¼€æºå’Œä¸“æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼ŒåŒ…æ‹¬GPTç³»åˆ—ã€Anthropicã€Deepseekå’ŒLlama-3çš„å˜ç§ï¼Œä»¥åŠæ¬§æ´²ä¸“åˆ©ä»£ç†äººæœªæ¥èµ„æ ¼æ¬§æ´²è€ƒè¯•ï¼ˆEQEï¼‰çš„éƒ¨åˆ†å†…å®¹ã€‚OpenAI o1ä»¥0.82çš„å‡†ç¡®ç‡å’Œ0.81çš„F1åˆ†æ•°ä½å±…æ¦œé¦–ï¼Œè€Œäºšé©¬é€Šç½‘ç»œæœåŠ¡ï¼ˆAWSï¼‰Llama 3.1 8Bçš„å‡†ç¡®ç‡ä¸º0.5å‡†ç¡®ç‡å’Œ0.5ç³»ç»Ÿçš„å‡†ç¡®ç‡ä¸º0.5ï¼Œåè€…ä¸¤ä¸ªæ¨¡å‹çš„æ€§èƒ½ä»…åœ¨ä¸¤é€‰ä¸€çš„çŒœæµ‹èŒƒå›´å†…ã€‚ç»è¿‡è¯„ä¼°çš„æ¨¡å‹ä¸­ï¼Œæ²¡æœ‰ä»»ä½•ä¸€ä¸ªèƒ½å¤Ÿå®Œå…¨é€šè¿‡è€ƒè¯•ï¼Œå› ä¸ºå‡†ç¡®æ€§ä»æœªè¶…è¿‡ä¸“ä¸šçº§åˆ«æ ‡å‡†æ‰€è¦æ±‚çš„å¹³å‡é˜ˆå€¼0.9ï¼Œè€Œä¸”ä¹Ÿä¸ä¼šå¯¹å®£ä¼ ä¸­æ‰€ç§°çš„è¶…è¶Šåšå£«å’Œæ‰§ä¸šå¾‹å¸ˆæ°´å¹³çš„æ€§èƒ½äº§ç”Ÿå½±å“ã€‚GPT-4åœ¨æ•´åˆæ–‡æœ¬å’Œå›¾å½¢æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œè€ŒClaude 3 Opusç»å¸¸å¤±å»æ ¼å¼è¿è´¯æ€§ã€‚äººç±»ä¸“åˆ©ä¸“å®¶å¯¹æ–‡æœ¬ä¾æ®è¿›è¡Œäº†è¯„ä¼°ï¼Œå‘ç°äº†æ¯ä¸ªæ¨¡å‹çš„å¤šç§å…³é”®ç¼ºé™·ã€‚ä»–ä»¬é‡è§†ç­”æ¡ˆçš„æ¸…æ™°åº¦å’Œæ³•å¾‹åˆç†æ€§ï¼Œè€Œéç­”æ¡ˆæœ¬èº«çš„æ­£ç¡®æ€§ï¼Œè¿™æ­ç¤ºäº†è‡ªåŠ¨æŒ‡æ ‡ä¸ä¸“å®¶åˆ¤æ–­ä¹‹é—´çš„ä¸ä¸€è‡´ã€‚æ¨¡å‹è¾“å‡ºå¯¹å¾®å°çš„æ¸©åº¦å˜åŒ–å’Œæç¤ºè¯­æ¯”è¾ƒæ•æ„Ÿï¼Œè¿™å¼ºè°ƒäº†ä¸“å®¶ç›‘ç£çš„å‰©ä½™å¿…è¦æ€§ã€‚æœªæ¥çš„å·¥ä½œåº”è¯¥è‡´åŠ›äºé€»è¾‘è¿è´¯æ€§ã€ç¨³å¥çš„å¤šåª’ä½“å½¢å¼å’Œé€‚åº”æ€§æç¤ºæ¥æ¥è¿‘äººç±»æ°´å¹³çš„ä¸“åˆ©èƒ½åŠ›ã€‚æ€»ä¹‹ï¼Œå°½ç®¡æœ€è¿‘çš„å¤§å‹æ¨¡å‹è¡¨ç°å‡ºè‰²ï¼Œå…¬ä¼—å¯èƒ½ä¼šé«˜ä¼°å®ƒä»¬çš„æ€§èƒ½ã€‚æ³•å¾‹é¢†åŸŸåœ¨å¼€å‘è™šæ‹Ÿä¸“åˆ©ä»£ç†äººæ–¹é¢è¿˜æœ‰å¾ˆé•¿çš„è·¯è¦èµ°ã€‚æœ¬æ–‡æƒ³æŒ‡å‡ºéœ€è¦è§£å†³çš„å‡ ä¸ªå…·ä½“å±€é™æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10576v2">PDF</a> 41 pages, 21 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬è®ºæ–‡å¯¹å¤šç§å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ³•å¾‹é¢†åŸŸçš„åº”ç”¨è¿›è¡Œäº†è¯„ä¼°ã€‚åœ¨é’ˆå¯¹æ¬§æ´²ä¸“åˆ©å¾‹å¸ˆèµ„æ ¼è€ƒè¯•çš„æµ‹è¯•ä¸­ï¼Œå„æ¨¡å‹çš„æ€§èƒ½è¡¨ç°å·®å¼‚æ˜¾è‘—ã€‚å°½ç®¡é«˜çº§æ¨¡å‹å¦‚OpenAI o1è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶å‡†ç¡®åº¦ä»æœªèƒ½è¾¾åˆ°ä¸“ä¸šæ°´å¹³çš„è¦æ±‚ã€‚æ¨¡å‹çš„å›ç­”åœ¨æ³•å¾‹ç†è®ºå’Œä¸“å®¶è¯„ä¼°ä¸­å­˜åœ¨è¯¸å¤šç¼ºé™·ï¼Œå‡¸æ˜¾äº†è‡ªåŠ¨è¯„ä¼°ä¸ä¸“å®¶åˆ¤æ–­ä¹‹é—´çš„å·®å¼‚ã€‚æœªæ¥ç ”ç©¶éœ€å…³æ³¨é€»è¾‘ä¸€è‡´æ€§ã€å¤šæ¨¡æ€èƒ½åŠ›å’Œé€‚åº”æ€§æç¤ºè®¾è®¡ã€‚æ€»ä½“è€Œè¨€ï¼Œå…¬ä¼—å¯¹æ¨¡å‹æ€§èƒ½å¯èƒ½å­˜åœ¨è¿‡åº¦ä¼°è®¡ï¼Œæ³•å¾‹è™šæ‹Ÿä»£ç†äººç ”å‘å°šéœ€æ›´å¤šåŠªåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ³•å¾‹é¢†åŸŸå·²å¹¿æ³›åº”ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œä½†å®ƒä»¬çš„å®šé‡æ€§èƒ½å’ŒåŸå› å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚</li>
<li>åœ¨æ¬§æ´²ä¸“åˆ©å¾‹å¸ˆèµ„æ ¼è€ƒè¯•çš„æµ‹è¯•ä¸­ï¼Œä¸åŒLLMæ¨¡å‹è¡¨ç°å·®å¼‚æ˜¾è‘—ï¼Œæ²¡æœ‰æ¨¡å‹èƒ½å¤Ÿå®Œå…¨é€šè¿‡è€ƒè¯•ã€‚</li>
<li>é«˜çº§æ¨¡å‹çš„å‡†ç¡®åº¦ä»æœªè¾¾åˆ°ä¸“ä¸šæ°´å¹³çš„è¦æ±‚ï¼Œéœ€è¦æ›´é«˜çš„å‡†ç¡®åº¦ä»¥è¾¾åˆ°ä¸“ä¸šæ ‡å‡†ã€‚</li>
<li>æ¨¡å‹åœ¨æ³•å¾‹ç†è®ºå’Œå®è·µæ–¹é¢å­˜åœ¨ç¼ºé™·ï¼Œéœ€è¦æ›´ç»†è‡´çš„æ ¡å‡†ä»¥é€‚åº”ä¸“ä¸šé¢†åŸŸçš„éœ€æ±‚ã€‚</li>
<li>ä¸“å®¶åˆ¤æ–­ä¸æ³•å¾‹ç†è®ºçš„èåˆæ˜¯ç›®å‰å¤§å‹è¯­è¨€æ¨¡å‹æ‰€é¢ä¸´çš„æŒ‘æˆ˜ä¹‹ä¸€ã€‚</li>
<li>æ¨¡å‹è¾“å‡ºå¯¹æ¸©åº¦å˜åŒ–å’Œæç¤ºæªè¾æ•æ„Ÿï¼Œå¼ºè°ƒä¸“å®¶ç›‘ç£çš„å¿…è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10576">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2507.10576v2/page_0_0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Scaling-LLM-Planning-NL2FLOW-for-Parametric-Problem-Generation-and-Rigorous-Evaluation"><a href="#Scaling-LLM-Planning-NL2FLOW-for-Parametric-Problem-Generation-and-Rigorous-Evaluation" class="headerlink" title="Scaling LLM Planning: NL2FLOW for Parametric Problem Generation and   Rigorous Evaluation"></a>Scaling LLM Planning: NL2FLOW for Parametric Problem Generation and   Rigorous Evaluation</h2><p><strong>Authors:Jungkoo Kang</strong></p>
<p>Robust workflow composition is critical for effective agent performance, yet progress in Large Language Model (LLM) planning and reasoning is hindered by a scarcity of scalable evaluation data. This work introduces NL2Flow, a fully automated pipeline for generating and evaluating workflow planning problems. NL2Flow generates problems parametrically in a structured intermediate representation, translating them into both natural language and formal PDDL. I evaluate several open-source, instruct-tuned LLMs on a dataset of 2296 low-difficulty problems generated by NL2Flow. Results demonstrate that the best-performing model achieved 86% success in generating valid plans and 69% in generating optimal plans (for solvable problems). Regression analysis shows that the influence of problem characteristics on plan generation is contingent on both model and prompt design. Importantly, translating natural language problems into a structured JSON representation prior to symbolic planning significantly improved success rates, suggesting a benefit from neuro-symbolic integration. These findings underscore the importance of understanding error sources within LLM reasoning as systems scale to more complex tasks. As LLM reasoning scales to increasingly complex problems, understanding the shifting bottlenecks and sources of error within these systems will be crucial. </p>
<blockquote>
<p>å¼ºå¤§çš„å·¥ä½œæµç¨‹ç»„åˆå¯¹äºæé«˜ä»£ç†æ€§èƒ½è‡³å…³é‡è¦ï¼Œç„¶è€Œï¼Œç”±äºç¼ºä¹å¯æ‰©å±•çš„è¯„ä¼°æ•°æ®ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è§„åˆ’å’Œæ¨ç†å·¥ä½œè¿›å±•å—åˆ°äº†é˜»ç¢ã€‚æœ¬æ–‡ä»‹ç»äº†NL2Flowï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨è‡ªåŠ¨åŒ–çš„ç®¡é“ï¼Œç”¨äºç”Ÿæˆå’Œè¯„ä¼°å·¥ä½œæµç¨‹è§„åˆ’é—®é¢˜ã€‚NL2Flowä»¥ç»“æ„åŒ–çš„ä¸­é—´è¡¨ç¤ºå½¢å¼å‚æ•°åŒ–ç”Ÿæˆé—®é¢˜ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºè‡ªç„¶è¯­è¨€æè¿°å’Œæ­£å¼PDDLæè¿°ã€‚æˆ‘åœ¨ç”±NL2Flowç”Ÿæˆçš„åŒ…å«æœ‰éš¾åº¦çš„ä½éš¾åº¦é—®é¢˜æ•°æ®é›†ä¸Šè¯„ä¼°äº†å‡ ä¸ªå¼€æºçš„æŒ‡ä»¤å¾®è°ƒLLMã€‚ç»“æœè¡¨æ˜ï¼Œè¡¨ç°æœ€å¥½çš„æ¨¡å‹åœ¨ç”Ÿæˆæœ‰æ•ˆè®¡åˆ’æ–¹é¢å–å¾—äº†86%çš„æˆåŠŸç‡ï¼Œåœ¨ç”Ÿæˆæœ€ä¼˜è®¡åˆ’æ–¹é¢å–å¾—äº†69%ï¼ˆé’ˆå¯¹å¯è§£å†³çš„é—®é¢˜ï¼‰ã€‚å›å½’åˆ†æè¡¨æ˜ï¼Œé—®é¢˜ç‰¹å¾å¯¹è®¡åˆ’ç”Ÿæˆçš„å½±å“å–å†³äºæ¨¡å‹å’Œæç¤ºè®¾è®¡ã€‚é‡è¦çš„æ˜¯ï¼Œåœ¨ç¬¦å·è§„åˆ’ä¹‹å‰å°†è‡ªç„¶è¯­è¨€é—®é¢˜è½¬æ¢ä¸ºç»“æ„åŒ–JSONè¡¨ç¤ºå½¢å¼æ˜¾è‘—æé«˜äº†æˆåŠŸç‡ï¼Œè¿™è¡¨æ˜ç¥ç»ç¬¦å·èåˆå…·æœ‰ä¼˜åŠ¿ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†éšç€LLMæ¨ç†ç³»ç»Ÿå¤„ç†è¶Šæ¥è¶Šå¤æ‚çš„ä»»åŠ¡æ—¶ï¼Œäº†è§£ç³»ç»Ÿå†…éƒ¨é”™è¯¯æ¥æºçš„é‡è¦æ€§ã€‚éšç€LLMæ¨ç†å¤„ç†è¶Šæ¥è¶Šå¤æ‚çš„é—®é¢˜æ—¶ï¼Œäº†è§£è¿™äº›ç³»ç»Ÿå†…ä¸æ–­å˜åŒ–çš„ç“¶é¢ˆå’Œé”™è¯¯æ¥æºå°†è‡³å…³é‡è¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02253v5">PDF</a> 31 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†NL2Flowè¿™ä¸€å…¨è‡ªåŠ¨ç®¡é“ï¼Œç”¨äºç”Ÿæˆå’Œè¯„ä¼°å·¥ä½œæµç¨‹è§„åˆ’é—®é¢˜ã€‚NL2Flowèƒ½å¤Ÿå‚æ•°åŒ–ç”Ÿæˆé—®é¢˜ï¼Œå¹¶å°†å…¶è½¬åŒ–ä¸ºè‡ªç„¶è¯­è¨€ä¸æ­£å¼PDDLï¼Œè¯„ä¼°äº†å¤šä¸ªå¼€æºæŒ‡ä»¤å¾®è°ƒLLMæ¨¡å‹åœ¨ç”±NL2Flowç”Ÿæˆçš„2296ä¸ªä½éš¾åº¦é—®é¢˜æ•°æ®é›†ä¸Šçš„è¡¨ç°ã€‚æœ€ä½³æ¨¡å‹ç”Ÿæˆæœ‰æ•ˆè®¡åˆ’çš„æˆåŠŸç‡ä¸º86%ï¼Œå¯è§£å†³é—®é¢˜ç”Ÿæˆæœ€ä¼˜è®¡åˆ’çš„æˆåŠŸç‡ä¸º69%ã€‚å›å½’åˆ†æè¡¨æ˜ï¼Œé—®é¢˜ç‰¹æ€§å¯¹è®¡åˆ’ç”Ÿæˆçš„å½±å“å–å†³äºæ¨¡å‹å’Œæç¤ºè®¾è®¡ã€‚å°†è‡ªç„¶è¯­è¨€é—®é¢˜è½¬åŒ–ä¸ºç»“æ„åŒ–JSONè¡¨ç¤ºå†è¿›è¡Œç¬¦å·è§„åˆ’ï¼Œèƒ½æ˜¾è‘—æé«˜æˆåŠŸç‡ï¼Œæ˜¾ç¤ºå‡ºç¥ç»ç¬¦å·èåˆçš„ç›Šå¤„ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†åœ¨å¤§è§„æ¨¡LLMæ¨ç†ä¸­ç†è§£é”™è¯¯æ¥æºçš„é‡è¦æ€§ã€‚éšç€LLMæ¨ç†é€æ­¥è§£å†³æ—¥ç›Šå¤æ‚çš„é—®é¢˜ï¼Œç†è§£è¿™äº›ç³»ç»Ÿå†…éƒ¨ä¸æ–­å˜åŒ–çš„ç“¶é¢ˆå’Œé”™è¯¯æ¥æºå°†æ˜¯å…³é”®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NL2Flowæ˜¯é¦–ä¸ªå…¨è‡ªåŠ¨ç®¡é“ï¼Œèƒ½å¤Ÿç”Ÿæˆå¹¶è¯„ä¼°å·¥ä½œæµç¨‹è§„åˆ’é—®é¢˜ã€‚</li>
<li>æœ€ä½³LLMæ¨¡å‹åœ¨ç”Ÿæˆæœ‰æ•ˆè®¡åˆ’å’Œæœ€ä¼˜è®¡åˆ’æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆæœã€‚</li>
<li>å›å½’åˆ†æäº†é—®é¢˜ç‰¹æ€§ã€æ¨¡å‹åŠæç¤ºè®¾è®¡å¯¹è®¡åˆ’ç”Ÿæˆçš„å½±å“ã€‚</li>
<li>å°†è‡ªç„¶è¯­è¨€é—®é¢˜è½¬åŒ–ä¸ºç»“æ„åŒ–JSONè¡¨ç¤ºå†è¿›è¡Œç¬¦å·è§„åˆ’ï¼Œèƒ½æ˜¾è‘—æé«˜è®¡åˆ’ç”Ÿæˆçš„æˆåŠŸç‡ã€‚</li>
<li>ç ”ç©¶å¼ºè°ƒäº†åœ¨å¤§è§„æ¨¡LLMæ¨ç†ä¸­ç†è§£é”™è¯¯æ¥æºçš„é‡è¦æ€§ã€‚</li>
<li>éšç€LLMæ¨ç†å¤„ç†çš„é—®é¢˜æ—¥ç›Šå¤æ‚ï¼Œç†è§£ç³»ç»Ÿå†…éƒ¨çš„ç“¶é¢ˆå’Œé”™è¯¯æ¥æºå°†æˆä¸ºå…³é”®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02253">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2507.02253v5/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2507.02253v5/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2507.02253v5/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2507.02253v5/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2507.02253v5/page_5_0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Development-and-Comparative-Evaluation-of-Three-Artificial-Intelligence-Models-NLP-LLM-JEPA-for-Predicting-Triage-in-Emergency-Departments-A-7-Month-Retrospective-Proof-of-Concept"><a href="#Development-and-Comparative-Evaluation-of-Three-Artificial-Intelligence-Models-NLP-LLM-JEPA-for-Predicting-Triage-in-Emergency-Departments-A-7-Month-Retrospective-Proof-of-Concept" class="headerlink" title="Development and Comparative Evaluation of Three Artificial Intelligence   Models (NLP, LLM, JEPA) for Predicting Triage in Emergency Departments: A   7-Month Retrospective Proof-of-Concept"></a>Development and Comparative Evaluation of Three Artificial Intelligence   Models (NLP, LLM, JEPA) for Predicting Triage in Emergency Departments: A   7-Month Retrospective Proof-of-Concept</h2><p><strong>Authors:Edouard Lansiaux, Ramy Azzouz, Emmanuel Chazard, AmÃ©lie Vromant, Eric Wiel</strong></p>
<p>Emergency departments struggle with persistent triage errors, especially undertriage and overtriage, which are aggravated by growing patient volumes and staff shortages. This study evaluated three AI models [TRIAGEMASTER (NLP), URGENTIAPARSE (LLM), and EMERGINET (JEPA)] against the FRENCH triage scale and nurse practice, using seven months of adult triage data from Roger Salengro Hospital in Lille, France. Among the models, the LLM-based URGENTIAPARSE consistently outperformed both AI alternatives and nurse triage, achieving the highest accuracy (F1-score 0.900, AUC-ROC 0.879) and superior performance in predicting hospitalization needs (GEMSA). Its robustness across structured data and raw transcripts highlighted the advantage of LLM architectures in abstracting patient information. Overall, the findings suggest that integrating LLM-based AI into emergency department workflows could significantly enhance patient safety and operational efficiency, though successful adoption will depend on addressing limitations and ensuring ethical transparency. </p>
<blockquote>
<p>æ€¥è¯Šç§‘æŒç»­é¢ä¸´åˆ†è¯Šé”™è¯¯é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯ä½ä¼°ç—…æƒ…å’Œè¿‡åº¦è¯„ä¼°ç—…æƒ…çš„æƒ…å†µã€‚è¿™äº›é—®é¢˜å› æ‚£è€…æ•°é‡å¢åŠ å’Œäººå‘˜çŸ­ç¼ºè€ŒåŠ å‰§ã€‚æœ¬ç ”ç©¶ä½¿ç”¨æ³•å›½é‡Œå°”å¸‚ç½—æ°Â·è¨ä¼¦æ ¼æ´›åŒ»é™¢ä¸ºæœŸä¸ƒä¸ªæœˆçš„æˆäººåˆ†è¯Šæ•°æ®ï¼Œä»¥æ³•å›½FRENCHåˆ†çº§æ ‡å‡†ä»¥åŠæŠ¤å£«å®è·µä¸ºæ ‡å‡†ï¼Œè¯„ä¼°äº†ä¸‰ç§äººå·¥æ™ºèƒ½æ¨¡å‹ï¼ˆTRIAGEMASTERï¼ˆNLPï¼‰ã€URGENTIAPARSEï¼ˆLLMï¼‰å’ŒEMERGINETï¼ˆJEPAï¼‰ï¼‰çš„è¡¨ç°ã€‚åœ¨è¿™äº›æ¨¡å‹ä¸­ï¼ŒåŸºäºLLMçš„URGENTIAPARSEå§‹ç»ˆè¡¨ç°ä¼˜äºå…¶ä»–ä¸¤ç§AIæ¨¡å‹å’ŒæŠ¤å£«åˆ†è¯Šï¼Œå‡†ç¡®ç‡æœ€é«˜ï¼ˆF1åˆ†æ•°ä¸º0.900ï¼ŒAUC-ROCä¸º0.879ï¼‰ï¼Œå¹¶ä¸”åœ¨é¢„æµ‹ä½é™¢éœ€æ±‚æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼ˆGEMSAï¼‰ã€‚å…¶åœ¨ç»“æ„åŒ–æ•°æ®å’ŒåŸå§‹æ–‡æœ¬è½¬å½•ä¸­çš„ç¨³å¥æ€§å‡¸æ˜¾äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æå–æ‚£è€…ä¿¡æ¯æ–¹é¢çš„ä¼˜åŠ¿ã€‚æ€»ä½“è€Œè¨€ï¼Œç ”ç©¶ç»“æœè¡¨æ˜å°†åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„äººå·¥æ™ºèƒ½æ•´åˆåˆ°æ€¥è¯Šç§‘å·¥ä½œæµç¨‹ä¸­å¯èƒ½ä¼šæ˜¾è‘—æé«˜æ‚£è€…å®‰å…¨å’Œè¿è¥æ•ˆç‡ï¼Œä½†è¦æˆåŠŸå®æ–½å¹¶ä¾èµ–è§£å†³é™åˆ¶å› ç´ å¹¶ç¡®ä¿ä¼¦ç†é€æ˜åº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01080v2">PDF</a> 13 pages, 7 figures, 3 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†ä¸‰ç§AIæ¨¡å‹ï¼ˆTRIAGEMASTERï¼ˆNLPï¼‰ã€URGENTIAPARSEï¼ˆLLMï¼‰å’ŒEMERGINETï¼ˆJEPAï¼‰ï¼‰åœ¨æ€¥è¯Šéƒ¨é—¨åº”ç”¨çš„è¡¨ç°ï¼Œç‰¹åˆ«æ˜¯åœ¨åº”å¯¹æ—¥ç›Šå¢é•¿çš„æ‚£è€…æ•°é‡å’ŒåŒ»æŠ¤äººå‘˜çŸ­ç¼ºå¯¼è‡´çš„æŒç»­æ€§åˆ†æµé”™è¯¯é—®é¢˜ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒåŸºäºLLMçš„URGENTIAPARSEæ¨¡å‹è¡¨ç°æœ€ä¸ºå‡ºè‰²ï¼Œå…¶å‡†ç¡®æ€§æœ€é«˜ï¼ˆF1åˆ†æ•°ä¸º0.900ï¼ŒAUC-ROCä¸º0.879ï¼‰ï¼Œå¹¶åœ¨é¢„æµ‹ä½é™¢éœ€æ±‚æ–¹é¢å±•ç°å‡ºå“è¶Šæ€§èƒ½ã€‚è¯¥æ¨¡å‹çš„ç¨³å¥æ€§ä½¿å…¶æˆä¸ºæŠ½è±¡æ‚£è€…ä¿¡æ¯é¢†åŸŸçš„ç†æƒ³é€‰æ‹©ã€‚æ€»ä½“è€Œè¨€ï¼Œå°†åŸºäºLLMçš„AIé›†æˆåˆ°æ€¥è¯Šéƒ¨é—¨å·¥ä½œæµç¨‹ä¸­æœ‰æœ›æ˜¾è‘—æé«˜æ‚£è€…å®‰å…¨å’Œè¿è¥æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æ€¥è¯Šéƒ¨é—¨é¢ä¸´æŒç»­çš„åˆ†æµé”™è¯¯é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯ä½åˆ†æµå’Œé«˜åˆ†æµï¼Œå—åˆ°æ‚£è€…æ•°é‡å¢é•¿å’Œäººå‘˜çŸ­ç¼ºçš„åŠ å‰§ã€‚</li>
<li>ä¸‰ç§AIæ¨¡å‹ï¼ˆTRIAGEMASTERã€URGENTIAPARSEå’ŒEMERGINETï¼‰è¢«è¯„ä¼°ç”¨äºè§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>URGENTIAPARSEï¼ˆåŸºäºLLMï¼‰åœ¨å‡†ç¡®æ€§ä¸Šè¡¨ç°æœ€ä½³ï¼Œä¸æŠ¤å£«åˆ†æµç›¸æ¯”å…·æœ‰æ›´é«˜çš„F1åˆ†æ•°å’ŒAUC-ROCå€¼ã€‚</li>
<li>URGENTIAPARSEåœ¨é¢„æµ‹ä½é™¢éœ€æ±‚æ–¹é¢å±•ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>LLMæ¶æ„åœ¨æŠ½è±¡æ‚£è€…ä¿¡æ¯æ–¹é¢å…·æœ‰ä¼˜åŠ¿ï¼Œç¨³å¥æ€§é«˜ï¼Œé€‚ç”¨äºå¤„ç†ç»“æ„æ•°æ®å’ŒåŸå§‹è½¬å½•æ•°æ®ã€‚</li>
<li>é›†æˆLLMåŸºäºAIçš„æ€¥è¯Šéƒ¨é—¨å·¥ä½œæµç¨‹æœ‰æœ›å¢å¼ºæ‚£è€…å®‰å…¨å’Œè¿è¥æ•ˆç‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01080">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2507.01080v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2507.01080v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2507.01080v2/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2507.01080v2/page_5_1.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="LLMs-for-sensory-motor-control-Combining-in-context-and-iterative-learning"><a href="#LLMs-for-sensory-motor-control-Combining-in-context-and-iterative-learning" class="headerlink" title="LLMs for sensory-motor control: Combining in-context and iterative   learning"></a>LLMs for sensory-motor control: Combining in-context and iterative   learning</h2><p><strong>Authors:JÃ´nata Tyska Carvalho, Stefano Nolfi</strong></p>
<p>We propose a method that enables large language models (LLMs) to control embodied agents by directly mapping continuous observation vectors to continuous action vectors. At the outset, the LLMs generate a control strategy based on a textual description of the agent, its environment, and the intended goal. This strategy is then iteratively refined through a learning process in which the LLMs are repeatedly prompted to improve the current strategy, using performance feedback and sensory-motor data collected during its evaluation. The method is validated on classic control tasks from the Gymnasium library and the inverted pendulum task from the MuJoCo library. The approach proves effective with relatively compact models such as Gpt-oss:120b and Qwen2.5:72b. In most cases, it successfully identifies optimal or near-optimal solutions by integrating symbolic knowledge derived through reasoning with sub-symbolic sensory-motor data gathered as the agent interacts with its environment. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿé€šè¿‡ç›´æ¥å°†è¿ç»­çš„è§‚å¯Ÿå‘é‡æ˜ å°„åˆ°è¿ç»­çš„åŠ¨ä½œå‘é‡æ¥æ§åˆ¶å®ä½“ä»£ç†ã€‚ä¸€å¼€å§‹ï¼ŒLLMåŸºäºä»£ç†çš„æ–‡æœ¬æè¿°ã€å…¶ç¯å¢ƒä»¥åŠé¢„æœŸç›®æ ‡ç”Ÿæˆæ§åˆ¶ç­–ç•¥ã€‚ç„¶åï¼Œé€šè¿‡å­¦ä¹ è¿‡ç¨‹è¿­ä»£åœ°å®Œå–„è¿™ä¸€ç­–ç•¥ï¼Œåœ¨æ­¤è¿‡ç¨‹ä¸­ï¼ŒLLMè¢«åå¤æç¤ºä»¥æ€§èƒ½åé¦ˆå’Œè¯„ä¼°æœŸé—´æ”¶é›†çš„æ„Ÿå®˜è¿åŠ¨æ•°æ®æ¥æ”¹å–„å½“å‰ç­–ç•¥ã€‚è¯¥æ–¹æ³•åœ¨Gymnasiumåº“çš„ç»å…¸æ§åˆ¶ä»»åŠ¡å’ŒMuJoCoåº“çš„å€’ç«‹æ‘†ä»»åŠ¡ä¸Šè¿›è¡Œäº†éªŒè¯ã€‚è¯¥æ–¹æ³•åœ¨ç›¸å¯¹ç´§å‡‘çš„æ¨¡å‹ï¼ˆå¦‚Gpt-oss:120bå’ŒQwen2.5:72bï¼‰ä¸Šè¯æ˜æœ‰æ•ˆã€‚åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œå®ƒé€šè¿‡æ•´åˆé€šè¿‡æ¨ç†è·å¾—çš„ç¬¦å·çŸ¥è¯†ä¸ä»£ç†åœ¨ä¸ç¯å¢ƒäº¤äº’è¿‡ç¨‹ä¸­æ”¶é›†çš„äºšç¬¦å·æ„Ÿå®˜è¿åŠ¨æ•°æ®ï¼ŒæˆåŠŸæ‰¾åˆ°æœ€ä¼˜è§£æˆ–è¿‘ä¼¼æœ€ä¼˜è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04867v2">PDF</a> Article updated with results from gpt-oss:120b. 24 pages (13 pages   are from appendix), 6 figures, code for experiments replication and   supplementary material provided at   <a target="_blank" rel="noopener" href="https://github.com/jtyska/llm-robotics-article/">https://github.com/jtyska/llm-robotics-article/</a></p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯ä»¥é€šè¿‡å°†è¿ç»­è§‚æµ‹å‘é‡ç›´æ¥æ˜ å°„åˆ°è¿ç»­åŠ¨ä½œå‘é‡æ¥æ§åˆ¶å®ä½“ä»£ç†ã€‚LLMåŸºäºä»£ç†çš„æ–‡æœ¬æè¿°ã€ç¯å¢ƒåŠç›®æ ‡ç”Ÿæˆæ§åˆ¶ç­–ç•¥ï¼Œå¹¶é€šè¿‡æ”¶é›†æ€§èƒ½åé¦ˆå’Œæ„Ÿè§‰è¿åŠ¨æ•°æ®æ¥è¿­ä»£ä¼˜åŒ–è¿™ä¸€ç­–ç•¥ã€‚è¯¥æ–¹æ³•åœ¨Gymnasiumå’ŒMuJoCoåº“çš„ç»å…¸æ§åˆ¶ä»»åŠ¡ä¸Šå¾—åˆ°éªŒè¯ï¼Œä½¿ç”¨Gpt-oss:120bå’ŒQwen2.5:72bç­‰è¾ƒç´§å‡‘çš„æ¨¡å‹æ—¶æ•ˆæœæ˜¾è‘—ã€‚è¯¥æ–¹æ³•ç»“åˆäº†ç¬¦å·çŸ¥è¯†å’Œä»£ç†ä¸ç¯å¢ƒäº¤äº’ä¸­çš„æ„Ÿè§‰è¿åŠ¨æ•°æ®ï¼Œé€šå¸¸èƒ½æ‰¾å‡ºæœ€ä¼˜æˆ–æ¥è¿‘æœ€ä¼˜çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMèƒ½å¤Ÿæ§åˆ¶å®ä½“ä»£ç†ï¼Œé€šè¿‡å°†è¿ç»­è§‚æµ‹å‘é‡æ˜ å°„åˆ°è¿ç»­åŠ¨ä½œå‘é‡ã€‚</li>
<li>LLMåŸºäºæ–‡æœ¬æè¿°ç”Ÿæˆæ§åˆ¶ç­–ç•¥ï¼Œå¹¶è€ƒè™‘ä»£ç†ã€ç¯å¢ƒå’Œç›®æ ‡ã€‚</li>
<li>é€šè¿‡æ€§èƒ½åé¦ˆå’Œæ„Ÿè§‰è¿åŠ¨æ•°æ®çš„æ”¶é›†ï¼ŒLLMèƒ½å¤Ÿè¿­ä»£ä¼˜åŒ–æ§åˆ¶ç­–ç•¥ã€‚</li>
<li>è¿™ç§æ–¹æ³•åœ¨ç»å…¸æ§åˆ¶ä»»åŠ¡ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œå¦‚Gymnasiumå’ŒMuJoCoåº“çš„ä»»åŠ¡ã€‚</li>
<li>ç›¸å¯¹ç´§å‡‘çš„æ¨¡å‹ï¼Œå¦‚Gpt-oss:120bå’ŒQwen2.5:72bï¼Œåœ¨åº”ç”¨ä¸­è¡¨ç°å‡ºæœ‰æ•ˆæ€§ã€‚</li>
<li>è¯¥æ–¹æ³•ç»“åˆäº†ç¬¦å·çŸ¥è¯†å’Œä»£ç†ä¸ç¯å¢ƒäº¤äº’ä¸­çš„æ„Ÿè§‰è¿åŠ¨æ•°æ®ã€‚</li>
<li>é€šå¸¸èƒ½æ‰¾å‡ºæœ€ä¼˜æˆ–æ¥è¿‘æœ€ä¼˜çš„è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04867">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2506.04867v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2506.04867v2/page_3_0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Toward-Generation-of-Test-Cases-from-Task-Descriptions-via-History-aware-Planning"><a href="#Toward-Generation-of-Test-Cases-from-Task-Descriptions-via-History-aware-Planning" class="headerlink" title="Toward Generation of Test Cases from Task Descriptions via History-aware   Planning"></a>Toward Generation of Test Cases from Task Descriptions via History-aware   Planning</h2><p><strong>Authors:Duy Cao, Phu Nguyen, Vy Le, Tien N. Nguyen, Vu Nguyen</strong></p>
<p>In automated web testing, generating test scripts from natural language task descriptions is crucial for enhancing the test generation process. This activity involves creating the correct sequences of actions to form test scripts for future testing activities. Current state-of-the-art approaches are limited in generating these action sequences, as they either demand substantial manual effort for human demonstrations or fail to consider the history of previous web content and actions to decide the next action. In this paper, we introduce HxAgent, an iterative large language model agent planning approach that determines the next action based on: 1) observations of the current contents and feasible actions, 2) short-term memory of previous web states and actions, and 3) long-term experience with (in)correct action sequences. The agent generates a sequence of actions to perform a given task, which is effectively an automated test case to verify the task. We conducted an extensive empirical evaluation of HxAgent using two datasets. On the MiniWoB++ dataset, our approach achieves 97% exact-match accuracy that is comparable to the best baselines while eliminating the need for human demonstrations required by those methods. For complex tasks requiring navigation through multiple actions and screens, HxAgent achieves an average 82% exact-match. On the second dataset, comprising 350 task instances across seven popular websites, including YouTube, LinkedIn, Facebook, and Google, HxAgent achieves high performance, with 87% of the action sequences exactly matching the ground truth and a prefix-match of 93%, outperforming the baseline by 59%. </p>
<blockquote>
<p>åœ¨è‡ªåŠ¨åŒ–ç½‘é¡µæµ‹è¯•ä¸­ï¼Œä»è‡ªç„¶è¯­è¨€ä»»åŠ¡æè¿°ç”Ÿæˆæµ‹è¯•è„šæœ¬å¯¹äºå¢å¼ºæµ‹è¯•ç”Ÿæˆè¿‡ç¨‹è‡³å…³é‡è¦ã€‚è¿™é¡¹æ´»åŠ¨æ¶‰åŠåˆ›å»ºæ­£ç¡®çš„æ“ä½œåºåˆ—ä»¥å½¢æˆç”¨äºæœªæ¥æµ‹è¯•æ´»åŠ¨çš„æµ‹è¯•è„šæœ¬ã€‚å½“å‰æœ€å…ˆè¿›çš„æŠ€æœ¯æ–¹æ³•åœ¨ç”Ÿæˆè¿™äº›æ“ä½œåºåˆ—æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œå› ä¸ºå®ƒä»¬è¦ä¹ˆéœ€è¦å¤§é‡çš„äººå·¥æ¼”ç¤ºï¼Œè¦ä¹ˆæ— æ³•è€ƒè™‘ä»¥å‰çš„ç½‘é¡µå†…å®¹å’Œæ“ä½œå†å²æ¥å†³å®šä¸‹ä¸€ä¸ªæ“ä½œã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†HxAgentï¼Œè¿™æ˜¯ä¸€ä¸ªè¿­ä»£çš„å¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†è§„åˆ’æ–¹æ³•ï¼Œå®ƒæ ¹æ®ä»¥ä¸‹ä¸‰ä¸ªå› ç´ ç¡®å®šä¸‹ä¸€ä¸ªæ“ä½œï¼š1ï¼‰å¯¹å½“å‰å†…å®¹å’Œå¯è¡Œæ“ä½œçš„è§‚å¯Ÿï¼›2ï¼‰å¯¹å…ˆå‰ç½‘é¡µçŠ¶æ€å’Œæ“ä½œçš„çŸ­æœŸè®°å¿†ï¼›ä»¥åŠ3ï¼‰å¯¹ï¼ˆæ­£ç¡®æˆ–é”™è¯¯çš„ï¼‰æ“ä½œåºåˆ—çš„é•¿æœŸç»éªŒã€‚ä»£ç†ç”Ÿæˆæ‰§è¡Œç»™å®šä»»åŠ¡çš„æ“ä½œåºåˆ—ï¼Œè¿™å®é™…ä¸Šæ˜¯éªŒè¯ä»»åŠ¡çš„è‡ªåŠ¨åŒ–æµ‹è¯•ç”¨ä¾‹ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸¤ä¸ªæ•°æ®é›†å¯¹HxAgentè¿›è¡Œäº†å¹¿æ³›çš„å®è¯è¯„ä¼°ã€‚åœ¨MiniWoB++æ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†97%çš„ç²¾ç¡®åŒ¹é…ç‡ï¼Œä¸æœ€ä½³åŸºçº¿ç›¸å½“ï¼ŒåŒæ—¶æ¶ˆé™¤äº†è¿™äº›æ–¹æ³•æ‰€éœ€è¦çš„äººå·¥æ¼”ç¤ºã€‚å¯¹äºéœ€è¦å¤šæ¬¡æ“ä½œå’Œå±å¹•å¯¼èˆªçš„å¤æ‚ä»»åŠ¡ï¼ŒHxAgentçš„å¹³å‡ç²¾ç¡®åŒ¹é…ç‡è¾¾åˆ°82%ã€‚åœ¨ç¬¬äºŒä¸ªæ•°æ®é›†ä¸Šï¼Œè¯¥æ•°æ®é›†åŒ…å«350ä¸ªä»»åŠ¡å®ä¾‹ï¼Œæ¶µç›–YouTubeã€LinkedInã€Facebookå’ŒGoogleç­‰ä¸ƒä¸ªæµè¡Œç½‘ç«™ï¼ŒHxAgentè¡¨ç°å‡ºé«˜æ€§èƒ½ï¼Œå…¶æ“ä½œåºåˆ—çš„ç²¾ç¡®åŒ¹é…ç‡è¾¾åˆ°87%ï¼Œå‰ç¼€åŒ¹é…ç‡ä¸º93%ï¼Œè¶…è¿‡äº†åŸºçº¿æ–¹æ³•çš„59%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14336v2">PDF</a> Change the method and experimentation</p>
<p><strong>Summary</strong></p>
<p>åŸºäºè‡ªç„¶è¯­è¨€ä»»åŠ¡æè¿°è‡ªåŠ¨ç”Ÿæˆwebæµ‹è¯•è„šæœ¬ï¼Œæ˜¯æå‡æµ‹è¯•ç”Ÿæˆè¿‡ç¨‹æ•ˆç‡çš„å…³é”®æ­¥éª¤ã€‚å½“å‰ä¸»æµæ–¹æ³•åœ¨è¿™æ–¹é¢æœ‰æ‰€å±€é™ï¼Œå®ƒä»¬è¦ä¹ˆéœ€è¦å¤§é‡äººå·¥ç¤ºèŒƒï¼Œè¦ä¹ˆä¸è€ƒè™‘ä»¥å¾€çš„ç½‘é¡µå†…å®¹å’Œè¡Œä¸ºæ¥å†³å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨ã€‚æœ¬æ–‡æå‡ºä¸€ç§åä¸ºHxAgentçš„å¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†è§„åˆ’æ–¹æ³•ï¼Œå®ƒæ ¹æ®å½“å‰å†…å®¹åŠå…¶å¯è¡Œæ“ä½œã€çŸ­æœŸå†…å­˜ä¸­çš„ä»¥å¾€webçŠ¶æ€å’Œæ“ä½œä»¥åŠé•¿æœŸçš„å¯¹ï¼ˆæ­£ç¡®æˆ–é”™è¯¯ï¼‰æ“ä½œåºåˆ—çš„ç»éªŒæ¥å†³å®šä¸‹ä¸€æ­¥æ“ä½œã€‚æ­¤æ–¹æ³•åœ¨ä¸€é¡¹æµ‹è¯•ä¸­æœ‰æ•ˆç”Ÿæˆå®Œæˆç‰¹å®šä»»åŠ¡çš„æ“ä½œåºåˆ—ï¼Œä»è€Œè¿›è¡Œè‡ªåŠ¨åŒ–æµ‹è¯•éªŒè¯ä»»åŠ¡ã€‚å¹¿æ³›çš„å®è¯è¯„ä¼°è¡¨æ˜ï¼Œåœ¨å¤æ‚ä»»åŠ¡åœºæ™¯ä¸‹ï¼Œå¦‚åœ¨MiniWoB++æ•°æ®é›†ä¸Šï¼ŒHxAgentå–å¾—äº†ä¸æœ€ä½³åŸºçº¿ç›¸å½“çš„ç²¾ç¡®åŒ¹é…å‡†ç¡®ç‡ï¼ˆè¾¾åˆ°97%ï¼‰ï¼Œä¸”æ— éœ€äººå·¥ç¤ºèŒƒï¼›åœ¨å¦ä¸€åŒ…å«å¤šä¸ªç½‘ç«™ä»»åŠ¡çš„ç¬¬äºŒæ•°æ®é›†ä¸Šï¼Œå…¶ç²¾ç¡®åŒ¹é…ç‡é«˜è¾¾87%ï¼Œå‰ç¼€åŒ¹é…ç‡é«˜è¾¾93%ï¼Œæ˜¾è‘—ä¼˜äºåŸºçº¿æ ‡å‡†ã€‚HXAgentçš„æœ‰æ•ˆæ€§å’Œé«˜æ•ˆæ€§æ˜¾ç¤ºå‡ºäº†å®ƒåœ¨è‡ªåŠ¨åŒ–Webæµ‹è¯•é¢†åŸŸçš„æ½œåŠ›ä¸å‰æ™¯ã€‚HXAgentè¡¨ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½ï¼Œæœ‰æœ›æˆä¸ºè‡ªåŠ¨åŒ–Webæµ‹è¯•é¢†åŸŸçš„åˆ›æ–°è§£å†³æ–¹æ¡ˆã€‚ </p>
<p><strong>Key Takeaways</strong></p>
<p>ä¸€ã€è‡ªåŠ¨ç”Ÿæˆwebæµ‹è¯•è„šæœ¬æ˜¯æå‡è‡ªåŠ¨åŒ–webæµ‹è¯•æ•ˆç‡çš„å…³é”®ç¯èŠ‚ã€‚å½“å‰ä¸»æµæ–¹æ³•å­˜åœ¨å±€é™æ€§ï¼Œéœ€è¦äººå·¥ç¤ºèŒƒæˆ–å¿½ç•¥å†å²ä¿¡æ¯ã€‚</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14336">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2504.14336v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2504.14336v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2504.14336v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Transforming-Wearable-Data-into-Personal-Health-Insights-using-Large-Language-Model-Agents"><a href="#Transforming-Wearable-Data-into-Personal-Health-Insights-using-Large-Language-Model-Agents" class="headerlink" title="Transforming Wearable Data into Personal Health Insights using Large   Language Model Agents"></a>Transforming Wearable Data into Personal Health Insights using Large   Language Model Agents</h2><p><strong>Authors:Mike A. Merrill, Akshay Paruchuri, Naghmeh Rezaei, Geza Kovacs, Javier Perez, Yun Liu, Erik Schenck, Nova Hammerquist, Jake Sunshine, Shyam Tailor, Kumar Ayush, Hao-Wei Su, Qian He, Cory Y. McLean, Mark Malhotra, Shwetak Patel, Jiening Zhan, Tim Althoff, Daniel McDuff, Xin Liu</strong></p>
<p>Deriving personalized insights from popular wearable trackers requires complex numerical reasoning that challenges standard LLMs, necessitating tool-based approaches like code generation. Large language model (LLM) agents present a promising yet largely untapped solution for this analysis at scale. We introduce the Personal Health Insights Agent (PHIA), a system leveraging multistep reasoning with code generation and information retrieval to analyze and interpret behavioral health data. To test its capabilities, we create and share two benchmark datasets with over 4000 health insights questions. A 650-hour human expert evaluation shows that PHIA significantly outperforms a strong code generation baseline, achieving 84% accuracy on objective, numerical questions and, for open-ended ones, earning 83% favorable ratings while being twice as likely to achieve the highest quality rating. This work can advance behavioral health by empowering individuals to understand their data, enabling a new era of accessible, personalized, and data-driven wellness for the wider population. </p>
<blockquote>
<p>ä»æµè¡Œçš„å¯ç©¿æˆ´è·Ÿè¸ªå™¨è·å–ä¸ªæ€§åŒ–æ´å¯Ÿéœ€è¦å¤æ‚çš„æ•°å€¼æ¨ç†ï¼Œè¿™æŒ‘æˆ˜äº†æ ‡å‡†çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œéœ€è¦åŸºäºå·¥å…·çš„æ–¹æ³•ï¼Œå¦‚ä»£ç ç”Ÿæˆã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†äººä¸ºè¿™ç§å¤§è§„æ¨¡åˆ†ææä¾›äº†æœ‰å‰æ™¯ä½†å°šæœªå……åˆ†åˆ©ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬ä»‹ç»äº†ä¸ªäººå¥åº·æ´å¯Ÿä»£ç†ï¼ˆPHIAï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç³»ç»Ÿï¼Œåˆ©ç”¨å¤šæ­¥éª¤æ¨ç†å’Œä»£ç ç”Ÿæˆä»¥åŠä¿¡æ¯æ£€ç´¢æ¥åˆ†æå¹¶è§£é‡Šè¡Œä¸ºå¥åº·æ•°æ®ã€‚ä¸ºäº†æµ‹è¯•å…¶èƒ½åŠ›ï¼Œæˆ‘ä»¬åˆ›å»ºå¹¶åˆ†äº«äº†åŒ…å«è¶…è¿‡4000ä¸ªå¥åº·æ´å¯Ÿé—®é¢˜ä¸¤ä¸ªåŸºå‡†æ•°æ®é›†ã€‚ä¸€é¡¹ä¸ºæœŸ650å°æ—¶çš„äººç±»ä¸“å®¶è¯„ä¼°è¡¨æ˜ï¼ŒPHIAæ˜¾è‘—ä¼˜äºå¼ºå¤§çš„ä»£ç ç”ŸæˆåŸºçº¿ï¼Œåœ¨å®¢è§‚æ•°å€¼é—®é¢˜ä¸Šè¾¾åˆ°84%çš„å‡†ç¡®ç‡ï¼Œåœ¨å¼€æ”¾æ€§é—®é¢˜ä¸Šè·å¾—83%çš„å¥½è¯„ï¼ŒåŒæ—¶æœ‰ä¸¤æ¬¡æœºä¼šè·å¾—æœ€é«˜è´¨é‡è¯„åˆ†ã€‚è¿™é¡¹å·¥ä½œå¯ä»¥é€šè¿‡å¸®åŠ©ä¸ªäººç†è§£ä»–ä»¬çš„æ•°æ®ï¼Œä¸ºæ›´å¹¿æ³›çš„äººç¾¤å¼€å¯ä¸€ä¸ªå¯è®¿é—®ã€ä¸ªæ€§åŒ–ã€æ•°æ®é©±åŠ¨çš„å¥åº·æ–°æ—¶ä»£ï¼Œä»è€Œä¿ƒè¿›è¡Œä¸ºå¥åº·çš„å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.06464v4">PDF</a> 53 pages, 7 main figures, 2 main tables, accepted to Nature   Communications</p>
<p><strong>Summary</strong></p>
<p>æ–‡ç« ä»‹ç»äº†ç©¿æˆ´å¼è¿½è¸ªå™¨çš„ä¸ªäººè§è§£çš„æ´¾ç”Ÿå¯¹æ ‡å‡†LLMæå‡ºæŒ‘æˆ˜ï¼Œéœ€è¦é€šè¿‡åŸºäºå·¥å…·çš„æ–¹æ³•å¦‚ä»£ç ç”Ÿæˆæ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚æå‡ºäº†ä¸ªäººå¥åº·è§è§£ä»£ç†ï¼ˆPHIAï¼‰ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿåˆ©ç”¨å¤šæ­¥éª¤æ¨ç†å’Œä»£ç ç”Ÿæˆä»¥åŠä¿¡æ¯æ£€ç´¢æ¥åˆ†æå¹¶è§£é‡Šè¡Œä¸ºå¥åº·æ•°æ®ã€‚æµ‹è¯•è¡¨æ˜ï¼ŒPHIAåœ¨å®¢è§‚æ•°å€¼é—®é¢˜å’Œå¼€æ”¾æ€§é—®é¢˜ä¸Šåˆ†åˆ«è¾¾åˆ°äº†84%å’Œ83%çš„å¥½è¯„ç‡ï¼Œä¸”åœ¨é«˜è´¨é‡è¯„çº§æ–¹é¢é«˜å‡ºåŸºçº¿æ¨¡å‹ä¸¤å€ã€‚è¿™ä¸ºè¡Œä¸ºå¥åº·é¢†åŸŸå¸¦æ¥äº†ä¸ªæ€§åŒ–ã€æ•°æ®é©±åŠ¨çš„æ–°æ—¶ä»£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨è§£è¯»å¯ç©¿æˆ´è®¾å¤‡å¥åº·æ•°æ®ä¸Šé¢ä¸´æŒ‘æˆ˜ï¼Œéœ€è¦æ›´å¤æ‚çš„æ•°å€¼æ¨ç†å’Œå·¥å…·åŒ–æ–¹æ³•å¦‚ä»£ç ç”Ÿæˆæ¥åº”å¯¹ã€‚</li>
<li>æå‡ºä¸ªäººå¥åº·è§è§£ä»£ç†ï¼ˆPHIAï¼‰ç³»ç»Ÿï¼Œåˆ©ç”¨å¤šæ­¥éª¤æ¨ç†å’Œä»£ç ç”ŸæˆæŠ€æœ¯æ¥åˆ†æè§£è¯»å¥åº·æ•°æ®ã€‚</li>
<li>PHIAç³»ç»Ÿåœ¨å¤„ç†å®¢è§‚æ•°å€¼é—®é¢˜å’Œå¼€æ”¾æ€§é—®é¢˜æ—¶è¡¨ç°å‡ºè‰²ï¼Œå‡†ç¡®ç‡è¾ƒé«˜ã€‚</li>
<li>PHIAç³»ç»Ÿç›¸è¾ƒäºåŸºçº¿æ¨¡å‹åœ¨é«˜è´¨é‡è¯„çº§æ–¹é¢è¡¨ç°æ›´ä¼˜ç§€ã€‚</li>
<li>PHIAç³»ç»Ÿå¯¹äºç†è§£ä¸ªäººå¥åº·æ•°æ®å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œèƒ½å¤Ÿæ¨åŠ¨è¡Œä¸ºå¥åº·é¢†åŸŸçš„å‘å±•ã€‚</li>
<li>æ–‡ç« åˆ›å»ºå¹¶åˆ†äº«äº†åŒ…å«è¶…è¿‡4000ä¸ªå¥åº·è§è§£é—®é¢˜çš„ä¸¤ä¸ªåŸºå‡†æ•°æ®é›†ä»¥æµ‹è¯•ç³»ç»Ÿçš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.06464">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2406.06464v4/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2406.06464v4/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2406.06464v4/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2406.06464v4/page_4_0.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Osprey-Pixel-Understanding-with-Visual-Instruction-Tuning"><a href="#Osprey-Pixel-Understanding-with-Visual-Instruction-Tuning" class="headerlink" title="Osprey: Pixel Understanding with Visual Instruction Tuning"></a>Osprey: Pixel Understanding with Visual Instruction Tuning</h2><p><strong>Authors:Yuqian Yuan, Wentong Li, Jian Liu, Dongqi Tang, Xinjie Luo, Chi Qin, Lei Zhang, Jianke Zhu</strong></p>
<p>Multimodal large language models (MLLMs) have recently achieved impressive general-purpose vision-language capabilities through visual instruction tuning. However, current MLLMs primarily focus on image-level or box-level understanding, falling short in achieving fine-grained vision-language alignment at pixel level. Besides, the lack of mask-based instruction data limits their advancements. In this paper, we propose Osprey, a mask-text instruction tuning approach, to extend MLLMs by incorporating fine-grained mask regions into language instruction, aiming at achieving pixel-wise visual understanding. To achieve this goal, we first meticulously curate a mask-based region-text dataset with 724K samples, and then design a vision-language model by injecting pixel-level representation into LLM. Specifically, Osprey adopts a convolutional CLIP backbone as the vision encoder and employs a mask-aware visual extractor to extract precise visual mask features from high resolution input. Experimental results demonstrate Ospreyâ€™s superiority in various region understanding tasks, showcasing its new capability for pixel-level instruction tuning. In particular, Osprey can be integrated with Segment Anything Model (SAM) seamlessly to obtain multi-granularity semantics. The source code, dataset and demo can be found at <a target="_blank" rel="noopener" href="https://github.com/CircleRadon/Osprey">https://github.com/CircleRadon/Osprey</a>. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æœ€è¿‘é€šè¿‡è§†è§‰æŒ‡ä»¤å¾®è°ƒï¼Œå®ç°äº†ä»¤äººå°è±¡æ·±åˆ»çš„é€šç”¨è§†è§‰è¯­è¨€åŠŸèƒ½ã€‚ç„¶è€Œï¼Œå½“å‰çš„MLLMsä¸»è¦å…³æ³¨å›¾åƒçº§åˆ«æˆ–æ¡†çº§åˆ«çš„ç†è§£ï¼Œåœ¨åƒç´ çº§åˆ«çš„ç²¾ç»†è§†è§‰è¯­è¨€å¯¹é½æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚æ­¤å¤–ï¼Œç¼ºä¹åŸºäºmaskçš„æŒ‡ä»¤æ•°æ®é™åˆ¶äº†å…¶å‘å±•ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Ospreyï¼Œè¿™æ˜¯ä¸€ç§åŸºäºmaskæ–‡æœ¬çš„æŒ‡ä»¤å¾®è°ƒæ–¹æ³•ï¼Œé€šè¿‡å°†ç²¾ç»†çš„maskåŒºåŸŸèå…¥è¯­è¨€æŒ‡ä»¤æ¥æ‰©å±•MLLMsï¼Œæ—¨åœ¨å®ç°åƒç´ çº§çš„è§†è§‰ç†è§£ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬é¦–å…ˆç²¾å¿ƒåˆ¶ä½œäº†ä¸€ä¸ªåŸºäºmaskçš„åŒºåŸŸæ–‡æœ¬æ•°æ®é›†ï¼ŒåŒ…å«724Kæ ·æœ¬ï¼Œç„¶åè®¾è®¡äº†ä¸€ä¸ªé€šè¿‡æ³¨å…¥åƒç´ çº§è¡¨ç¤ºåˆ°LLMä¸­çš„è§†è§‰è¯­è¨€æ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼ŒOspreyé‡‡ç”¨å·ç§¯CLIPä¸»å¹²ä½œä¸ºè§†è§‰ç¼–ç å™¨ï¼Œå¹¶é‡‡ç”¨ä¸€ä¸ªæ©ç æ„ŸçŸ¥çš„è§†è§‰æå–å™¨ä»é«˜åˆ†è¾¨ç‡è¾“å…¥ä¸­æå–ç²¾ç¡®çš„è§†è§‰maskç‰¹å¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOspreyåœ¨å„ç§åŒºåŸŸç†è§£ä»»åŠ¡ä¸­çš„ä¼˜è¶Šæ€§ï¼Œå±•ç¤ºå…¶åƒç´ çº§æŒ‡ä»¤å¾®è°ƒçš„æ–°èƒ½åŠ›ã€‚ç‰¹åˆ«æ˜¯ï¼ŒOspreyå¯ä»¥ä¸Segment Anything Modelï¼ˆSAMï¼‰æ— ç¼é›†æˆï¼Œä»¥è·å¾—å¤šç²’åº¦è¯­ä¹‰ã€‚ç›¸å…³æºä»£ç ã€æ•°æ®é›†å’Œæ¼”ç¤ºå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/CircleRadon/Osprey%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/CircleRadon/Ospreyæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.10032v4">PDF</a> CVPR2024, Code and Demo link:<a target="_blank" rel="noopener" href="https://github.com/CircleRadon/Osprey">https://github.com/CircleRadon/Osprey</a></p>
<p><strong>Summary</strong></p>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰é€šè¿‡è§†è§‰æŒ‡ä»¤å¾®è°ƒè·å¾—äº†é€šç”¨çš„è§†è§‰-è¯­è¨€åŠŸèƒ½ï¼Œä½†åœ¨ç²¾ç»†ç²’åº¦çš„è§†è§‰-è¯­è¨€å¯¹é½æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚æœ¬æ–‡æå‡ºOspreyï¼Œä¸€ç§åŸºäºæ©ç æ–‡æœ¬çš„æŒ‡ä»¤å¾®è°ƒæ–¹æ³•ï¼Œæ—¨åœ¨å®ç°åƒç´ çº§çš„è§†è§‰ç†è§£ã€‚é€šè¿‡å¼•å…¥ç²¾ç»†æ©ç åŒºåŸŸåˆ°è¯­è¨€æŒ‡ä»¤ä¸­ï¼Œè®¾è®¡äº†ä¸€ä¸ªè§†è§‰è¯­è¨€æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOspreyåœ¨å„ç§åŒºåŸŸç†è§£ä»»åŠ¡ä¸­å…·æœ‰ä¼˜è¶Šæ€§ï¼Œå¹¶å¯æ— ç¼é›†æˆåˆ°Segment Anything Modelï¼ˆSAMï¼‰ä¸­ï¼Œè·å¾—å¤šç²’åº¦è¯­ä¹‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLMsè™½ç„¶å·²ç»é€šè¿‡è§†è§‰æŒ‡ä»¤å¾®è°ƒè·å¾—äº†è§†è§‰-è¯­è¨€åŠŸèƒ½ï¼Œä½†åœ¨åƒç´ çº§çš„ç²¾ç»†ç²’åº¦è§†è§‰ç†è§£æ–¹é¢ä»æœ‰ä¸è¶³ã€‚</li>
<li>å½“å‰MLLMså—é™äºç¼ºä¹åŸºäºæ©ç çš„æŒ‡ä»¤æ•°æ®ï¼Œå½±å“äº†å…¶æ€§èƒ½çš„æå‡ã€‚</li>
<li>Ospreyæ–¹æ³•é€šè¿‡å¼•å…¥ç²¾ç»†æ©ç åŒºåŸŸåˆ°è¯­è¨€æŒ‡ä»¤ä¸­ï¼Œæ—¨åœ¨å®ç°åƒç´ çº§çš„è§†è§‰ç†è§£ã€‚</li>
<li>Ospreyè®¾è®¡äº†ä¸€ä¸ªè§†è§‰è¯­è¨€æ¨¡å‹ï¼Œé‡‡ç”¨äº†å·ç§¯CLIPéª¨å¹²ç½‘ä½œä¸ºè§†è§‰ç¼–ç å™¨ï¼Œå¹¶ä½¿ç”¨æ©ç æ„ŸçŸ¥çš„è§†è§‰æå–å™¨ä»é«˜åˆ†è¾¨ç‡è¾“å…¥ä¸­æå–ç²¾ç¡®çš„è§†è§‰æ©ç ç‰¹å¾ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒOspreyåœ¨å„ç§åŒºåŸŸç†è§£ä»»åŠ¡ä¸­å…·æœ‰ä¼˜è¶Šæ€§ã€‚</li>
<li>Ospreyå¯ä»¥æ— ç¼é›†æˆåˆ°Segment Anything Modelï¼ˆSAMï¼‰ï¼Œè·å¾—å¤šç²’åº¦è¯­ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2312.10032">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2312.10032v4/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2312.10032v4/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2312.10032v4/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2312.10032v4/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2312.10032v4/page_4_0.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-13/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-13/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-13/Agent/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_Agent/2509.09135v1/page_0_0.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-13  Maximizing social welfare among EF1 allocations at the presence of two   types of agents
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-13
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-13/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2508.15868v2/page_2_0.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-13  FLUX-Reason-6M & PRISM-Bench A Million-Scale Text-to-Image Reasoning   Dataset and Comprehensive Benchmark
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-13
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32883.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
