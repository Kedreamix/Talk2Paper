<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-13  Multiwavelength observations of a new black-widow millisecond pulsar PSR   J1544-2555">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2509.09558v1/page_3_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-13
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-07
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    87 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-13-æ›´æ–°"><a href="#2025-09-13-æ›´æ–°" class="headerlink" title="2025-09-13 æ›´æ–°"></a>2025-09-13 æ›´æ–°</h1><h2 id="Multiwavelength-observations-of-a-new-black-widow-millisecond-pulsar-PSR-J1544-2555"><a href="#Multiwavelength-observations-of-a-new-black-widow-millisecond-pulsar-PSR-J1544-2555" class="headerlink" title="Multiwavelength observations of a new black-widow millisecond pulsar PSR   J1544-2555"></a>Multiwavelength observations of a new black-widow millisecond pulsar PSR   J1544-2555</h2><p><strong>Authors:Sergio Belmonte Diaz, Tinn Thingmeearkom, Adipol Phosrisom, Rene Breton, Marta Burgay, Colin Clark, Lars Nieder, Martin Mayer, Werner Becker, Ewann Barr, Sarah Buchner, Kaustav Kashyap Das, Vik Dhillon, Oliver Dodge, Elizabeth Ferrara, Jean-Mathias Griessmeier, Ramesh Karuppusamy, Mark Kennedy, Michael Kramer, Prajwal Padmanabh, John Paice, Antonio Rodriguez, Ben Stappers</strong></p>
<p>We report the discovery of a new black-widow millisecond pulsar, PSR J1544-2555, associated with the Fermi-LAT source 4FGL J1544.2-2554. Optical, radio, and gamma-ray observations confirmed its nature as a compact spider binary system. Optical photometry from ULTRACAM revealed a (\sim)2.7-hour orbital period, guiding MeerKAT observations that detected (\sim)2.4-ms radio pulsations. Subsequent timing campaigns using the Murriyang Parkes Telescope, the Effelsberg 100-m Radio Telescope, and the Nan\c{c}ay Radio Telescope allowed us to obtain a preliminary timing solution, which enabled us to find gamma-ray pulsations. The final timing solution, spanning 16 years of Fermi-LAT gamma-ray data, also displays orbital period variations typical of spider pulsars. X-ray observations from eROSITA indicate non-thermal emission, but the relatively low count rate prohibits the search for X-ray pulsations. Optical light curve modelling using Icarus suggests the asymmetry is best explained by a spot model, where uneven heating creates localised temperature variations on the companion. While the optical spectra we obtained are compatible with the physical properties we infer for the companion star, they were not of sufficient signal-to-noise to allow for radial velocity measurements, thus limiting constraints on the neutron starâ€™s mass. The observed bluer colour near the light curve minimum suggests possible non-thermal emission from intra-binary shocks, supported by the presence of an X-ray source. This discovery exemplifies the proven capability of the Fermi-LAT catalogue in identifying millisecond pulsar candidates and highlights the role of optical surveys in detecting variable sources suitable for radio follow-up. </p>
<blockquote>
<p>æˆ‘ä»¬æŠ¥å‘Šäº†ä¸€é¡¹æ–°å‘ç°ï¼šä¸€é¢—ä¸è´¹ç±³å¤§è§†åœºæœ›è¿œé•œæºï¼ˆFermi-LATæºï¼‰ç›¸å…³çš„é»‘å¯¡å¦‡æ¯«ç§’è„‰å†²æ˜ŸPSR J1544-2555ã€‚é€šè¿‡å…‰å­¦ã€å°„ç”µå’Œä¼½é©¬å°„çº¿çš„è§‚æµ‹ï¼Œæˆ‘ä»¬ç¡®è®¤å…¶ä¸ºä¸€ä¸ªç´§å‡‘çš„èœ˜è››åŒæ˜Ÿç³»ç»Ÿã€‚ULTRACAMçš„å…‰å­¦æ‘„å½±æœ¯æ­ç¤ºäº†ä¸€ä¸ªçº¦2.7å°æ—¶çš„è½¨é“å‘¨æœŸï¼Œä¸ºMeerKATè§‚æµ‹æ£€æµ‹åˆ°çš„çº¦2.4æ¯«ç§’çš„å°„ç”µè„‰å†²æä¾›äº†æŒ‡å¯¼ã€‚éšåä½¿ç”¨Murriyang Parkesæœ›è¿œé•œã€åŸƒè´¹å°”æ–¯è´æ ¼100ç±³å°„ç”µæœ›è¿œé•œå’Œå—èµ›å°„ç”µæœ›è¿œé•œçš„æ—¶é—´æµ‹é‡æ´»åŠ¨è®©æˆ‘ä»¬è·å¾—äº†åˆæ­¥çš„æ—¶é—´è§£ç®—æ–¹æ¡ˆï¼Œè¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿå‘ç°ä¼½é©¬å°„çº¿è„‰å†²ã€‚è·¨è¶Š16å¹´è´¹ç±³å¤§è§†åœºæœ›è¿œé•œä¼½é©¬å°„çº¿æ•°æ®çš„æœ€ç»ˆæ—¶é—´è§£ç®—æ–¹æ¡ˆè¿˜æ˜¾ç¤ºå‡ºèœ˜è››è„‰å†²æ˜Ÿå…¸å‹çš„è½¨é“å‘¨æœŸå˜åŒ–ã€‚æ¥è‡ªeROSITAçš„Xå°„çº¿è§‚æµ‹è¡¨æ˜å­˜åœ¨éçƒ­å‘å°„ï¼Œä½†ç”±äºè®¡æ•°ç‡ç›¸å¯¹è¾ƒä½ï¼Œæ— æ³•è¿›è¡ŒXå°„çº¿è„‰å†²æœç´¢ã€‚ä½¿ç”¨Icarusè¿›è¡Œçš„å…‰å­¦å…‰æ›²çº¿å»ºæ¨¡è¡¨æ˜ï¼Œä¸å¯¹ç§°æ€§æœ€å¥½ç”¨æ–‘ç‚¹æ¨¡å‹è§£é‡Šï¼Œå…¶ä¸­ä¸å‡åŒ€çš„åŠ çƒ­ä¼šåœ¨ä¼´ä¾£æ˜Ÿä¸Šäº§ç”Ÿå±€éƒ¨æ¸©åº¦å·®å¼‚ã€‚è™½ç„¶æˆ‘ä»¬è·å¾—çš„å…‰å­¦å…‰è°±ä¸æˆ‘ä»¬å¯¹ä¼´ä¾£æ˜Ÿæ¨æ–­çš„ç‰©ç†æ€§è´¨ç›¸ç¬¦ï¼Œä½†ç”±äºä¿¡å™ªæ¯”ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œå¾„å‘é€Ÿåº¦æµ‹é‡ï¼Œä»è€Œå¯¹ä¸­å­æ˜Ÿçš„è´¨é‡çº¦æŸé€ æˆäº†é™åˆ¶ã€‚å…‰æ›²çº¿æœ€å°å€¼é™„è¿‘è§‚å¯Ÿåˆ°çš„åè“é¢œè‰²è¡¨æ˜å¯èƒ½å­˜åœ¨æ¥è‡ªåŒæ˜Ÿå†…éƒ¨å†²å‡»çš„éçƒ­å‘å°„ï¼ŒXå°„çº¿æºçš„å­˜åœ¨ä¹Ÿæ”¯æŒäº†è¿™ä¸€è§‚ç‚¹ã€‚è¿™ä¸€å‘ç°å……åˆ†è¯æ˜äº†è´¹ç±³å¤§è§†åœºæœ›è¿œé•œç›®å½•åœ¨è¯†åˆ«æ¯«ç§’è„‰å†²æ˜Ÿå€™é€‰å¯¹è±¡æ–¹é¢çš„èƒ½åŠ›ï¼Œå¹¶å¼ºè°ƒäº†å…‰å­¦è§‚æµ‹åœ¨æ£€æµ‹é€‚åˆå°„ç”µåç»­è§‚æµ‹çš„å¯å˜æºæ–¹é¢æ‰€èµ·çš„ä½œç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.09605v1">PDF</a> Accepted for publication in Monthly Notices of the Royal Astronomical   Society. 16 pages. 11 figures</p>
<p><strong>æ‘˜è¦</strong><br>    å‘ç°æ–°çš„é»‘å¯¡å¦‡æ¯«ç§’è„‰å†²æ˜ŸPSR J1544-2555ï¼Œä¸è´¹ç±³-LATæº4FGL J1544.2-2554ç›¸å…³è”ã€‚é€šè¿‡å…‰å­¦ã€å°„ç”µå’Œä¼½é©¬å°„çº¿è§‚æµ‹ç¡®è®¤å…¶ä¸ºç´§å‡‘èœ˜è››åŒæ˜Ÿç³»ç»Ÿã€‚ULTRACAMå…‰å­¦æ‘„å½±æœ¯æ­ç¤ºå…¶çº¦2.7å°æ—¶è½¨é“å‘¨æœŸï¼ŒMeerKATè§‚æµ‹æ£€æµ‹åˆ°çº¦2.4æ¯«ç§’å°„ç”µè„‰å†²ã€‚ä½¿ç”¨Murriyang Parkesæœ›è¿œé•œã€åŸƒè´¹å°”æ–¯è´æ ¼100ç±³å°„ç”µæœ›è¿œé•œå’Œçº³èµ›å°„ç”µæœ›è¿œé•œçš„å®šæ—¶è§‚æµ‹æ´»åŠ¨å¾—åˆ°åˆæ­¥å®šæ—¶è§£å†³æ–¹æ¡ˆï¼Œå¹¶å‘ç°ä¼½é©¬å°„çº¿è„‰å†²ã€‚è·¨è¶Š16å¹´çš„è´¹ç±³-LATä¼½é©¬å°„çº¿æ•°æ®çš„æœ€ç»ˆå®šæ—¶è§£å†³æ–¹æ¡ˆä¹Ÿæ˜¾ç¤ºå‡ºèœ˜è››è„‰å†²æ˜Ÿå…¸å‹çš„è½¨é“å‘¨æœŸå˜åŒ–ã€‚æ¥è‡ªeROSITAçš„Xå°„çº¿è§‚æµ‹è¡¨æ˜å­˜åœ¨éçƒ­å‘å°„ï¼Œä½†ç”±äºè®¡æ•°ç‡ç›¸å¯¹è¾ƒä½ï¼Œæ— æ³•æœç´¢Xå°„çº¿è„‰å†²ã€‚ä½¿ç”¨Icaruså¯¹å…‰å­¦å…‰æ›²çº¿è¿›è¡Œå»ºæ¨¡ï¼Œä¸å‡åŒ€åŠ çƒ­é€ æˆçš„å±€éƒ¨æ¸©åº¦å˜åŒ–çš„æ–‘ç‚¹æ¨¡å‹æœ€ä½³è§£é‡Šä¸å¯¹ç§°æ€§ã€‚è™½ç„¶è·å¾—çš„å…‰å­¦å…‰è°±ä¸æˆ‘ä»¬æ¨æ–­çš„ä¼´æ˜Ÿç‰©ç†æ€§è´¨ç›¸ç¬¦ï¼Œä½†ç”±äºä¿¡å™ªæ¯”ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œå¾„å‘é€Ÿåº¦æµ‹é‡ï¼Œå› æ­¤å¯¹ä¸­å­æ˜Ÿè´¨é‡çº¦æŸæœ‰é™ã€‚å…‰æ›²çº¿æœ€å°å€¼é™„è¿‘çš„è¾ƒè“é¢œè‰²å¯èƒ½è¡¨æ˜å­˜åœ¨æ¥è‡ªäºŒå…ƒå†…å†²å‡»çš„éçƒ­å‘å°„ï¼ŒXå°„çº¿æºçš„å­˜åœ¨ä¹Ÿæ”¯æŒè¿™ä¸€è§‚ç‚¹ã€‚è¿™ä¸€å‘ç°è¯æ˜äº†è´¹ç±³-LATç›®å½•åœ¨è¯†åˆ«æ¯«ç§’è„‰å†²æ˜Ÿå€™é€‰å¯¹è±¡æ–¹é¢çš„èƒ½åŠ›ï¼Œå¹¶å¼ºè°ƒäº†å…‰å­¦å‹˜æµ‹åœ¨æ£€æµ‹é€‚åˆå°„ç”µåç»­è§‚æµ‹çš„å¯å˜æºä¸­çš„ä½œç”¨ã€‚</p>
<p><strong>è¦ç‚¹æ‘˜è¦</strong></p>
<ol>
<li>å‘ç°äº†ä¸€ä¸ªæ–°çš„é»‘å¯¡å¦‡æ¯«ç§’è„‰å†²æ˜ŸPSR J1544-2555ï¼Œä¸è´¹ç±³-LATæºç›¸å…³è”ã€‚</li>
<li>é€šè¿‡å¤šç§æ³¢æ®µè§‚æµ‹ç¡®è®¤äº†å…¶ä¸ºç´§å‡‘èœ˜è››åŒæ˜Ÿç³»ç»Ÿã€‚</li>
<li>ULTRACAMæ­ç¤ºäº†çº¦2.7å°æ—¶çš„è½¨é“å‘¨æœŸã€‚</li>
<li>é€šè¿‡ä¸€ç³»åˆ—å°„ç”µæœ›è¿œé•œçš„å®šæ—¶è§‚æµ‹ï¼Œæ£€æµ‹åˆ°äº†æ¯«ç§’çº§çš„å°„ç”µè„‰å†²ã€‚</li>
<li>åˆæ­¥å’Œæœ€ç»ˆçš„å®šæ—¶è§£å†³æ–¹æ¡ˆæ­ç¤ºäº†èœ˜è››è„‰å†²æ˜Ÿçš„å…¸å‹è½¨é“å‘¨æœŸå˜åŒ–ã€‚</li>
<li>Xå°„çº¿è§‚æµ‹è¡¨æ˜å­˜åœ¨éçƒ­å‘å°„ï¼Œä½†å—é™äºè¾ƒä½çš„è®¡æ•°ç‡ï¼Œæ— æ³•ç¡®å®šXå°„çº¿è„‰å†²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.09605">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2509.09605v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2509.09605v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2509.09605v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2509.09605v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2509.09605v1/page_5_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Invisible-Attributes-Visible-Biases-Exploring-Demographic-Shortcuts-in-MRI-based-Alzheimerâ€™s-Disease-Classification"><a href="#Invisible-Attributes-Visible-Biases-Exploring-Demographic-Shortcuts-in-MRI-based-Alzheimerâ€™s-Disease-Classification" class="headerlink" title="Invisible Attributes, Visible Biases: Exploring Demographic Shortcuts in   MRI-based Alzheimerâ€™s Disease Classification"></a>Invisible Attributes, Visible Biases: Exploring Demographic Shortcuts in   MRI-based Alzheimerâ€™s Disease Classification</h2><p><strong>Authors:Akshit Achara, Esther Puyol Anton, Alexander Hammers, Andrew P. King</strong></p>
<p>Magnetic resonance imaging (MRI) is the gold standard for brain imaging. Deep learning (DL) algorithms have been proposed to aid in the diagnosis of diseases such as Alzheimerâ€™s disease (AD) from MRI scans. However, DL algorithms can suffer from shortcut learning, in which spurious features, not directly related to the output label, are used for prediction. When these features are related to protected attributes, they can lead to performance bias against underrepresented protected groups, such as those defined by race and sex. In this work, we explore the potential for shortcut learning and demographic bias in DL based AD diagnosis from MRI. We first investigate if DL algorithms can identify race or sex from 3D brain MRI scans to establish the presence or otherwise of race and sex based distributional shifts. Next, we investigate whether training set imbalance by race or sex can cause a drop in model performance, indicating shortcut learning and bias. Finally, we conduct a quantitative and qualitative analysis of feature attributions in different brain regions for both the protected attribute and AD classification tasks. Through these experiments, and using multiple datasets and DL models (ResNet and SwinTransformer), we demonstrate the existence of both race and sex based shortcut learning and bias in DL based AD classification. Our work lays the foundation for fairer DL diagnostic tools in brain MRI. The code is provided at <a target="_blank" rel="noopener" href="https://github.com/acharaakshit/ShortMR">https://github.com/acharaakshit/ShortMR</a> </p>
<blockquote>
<p>ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰æ˜¯å¤§è„‘æˆåƒçš„é‡‘æ ‡å‡†ã€‚æ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰ç®—æ³•å·²è¢«æå‡ºç”¨äºè¾…åŠ©ä»MRIæ‰«æä¸­è¯Šæ–­é˜¿å°”èŒ¨æµ·é»˜ç—…ï¼ˆADï¼‰ç­‰ç–¾ç—…ã€‚ç„¶è€Œï¼ŒDLç®—æ³•å¯èƒ½ä¼šé™·å…¥æ·å¾„å­¦ä¹ ï¼ˆshortcut learningï¼‰ï¼Œåœ¨è¿™ç§å­¦ä¹ æ–¹å¼ä¸­ï¼Œä¸è¾“å‡ºæ ‡ç­¾æ— ç›´æ¥å…³è”çš„è™šå‡ç‰¹å¾è¢«ç”¨äºé¢„æµ‹ã€‚å½“è¿™äº›ç‰¹å¾ä¸å—ä¿æŠ¤çš„å±æ€§ç›¸å…³æ—¶ï¼Œå®ƒä»¬å¯èƒ½å¯¼è‡´å¯¹ä»£è¡¨æ€§ä¸è¶³çš„å—ä¿æŠ¤ç¾¤ä½“ï¼ˆå¦‚æŒ‰ç§æ—å’Œæ€§åˆ«å®šä¹‰çš„ç¾¤ä½“ï¼‰çš„æ€§èƒ½åè§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†åŸºäºMRIçš„ADè¯Šæ–­ä¸­æ·å¾„å­¦ä¹ å’Œäººå£ç»Ÿè®¡åè§çš„æ½œåŠ›ã€‚æˆ‘ä»¬é¦–å…ˆè°ƒæŸ¥DLç®—æ³•æ˜¯å¦èƒ½ä»3Då¤§è„‘MRIæ‰«æä¸­è¯†åˆ«ç§æ—æˆ–æ€§åˆ«ï¼Œä»¥ç¡®å®šæ˜¯å¦å­˜åœ¨åŸºäºç§æ—å’Œæ€§åˆ«çš„åˆ†å¸ƒè½¬ç§»ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬è°ƒæŸ¥æŒ‰ç§æ—æˆ–æ€§åˆ«è®­ç»ƒé›†çš„ä¸å¹³è¡¡æ˜¯å¦ä¼šå¯¼è‡´æ¨¡å‹æ€§èƒ½ä¸‹é™ï¼Œè¿™è¡¨æ˜å­˜åœ¨æ·å¾„å­¦ä¹ å’Œåè§ã€‚æœ€åï¼Œæˆ‘ä»¬å¯¹ä¸åŒå¤§è„‘åŒºåŸŸçš„ç‰¹å¾å½’å±è¿›è¡Œå®šé‡å’Œå®šæ€§åˆ†æï¼ŒåŒ…æ‹¬å—ä¿æŠ¤å±æ€§å’ŒADåˆ†ç±»ä»»åŠ¡ã€‚é€šè¿‡è¿™äº›å®éªŒä»¥åŠä½¿ç”¨å¤šä¸ªæ•°æ®é›†å’ŒDLæ¨¡å‹ï¼ˆResNetå’ŒSwinTransformerï¼‰ï¼Œæˆ‘ä»¬è¯æ˜äº†åŸºäºç§æ—å’Œæ€§åˆ«çš„æ·å¾„å­¦ä¹ å’Œåè§å­˜åœ¨äºåŸºäºDLçš„ADåˆ†ç±»ä¸­ã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ºå»ºç«‹æ›´å…¬å¹³çš„è„‘MRIè¯Šæ–­å·¥å…·å¥ å®šäº†åŸºç¡€ã€‚ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/acharaakshit/ShortMR%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/acharaakshit/ShortMRè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.09558v1">PDF</a> FAIMI @ MICCAI 2025</p>
<p><strong>Summary</strong></p>
<p>MRIåœ¨è„‘æˆåƒä¸­æ˜¯é‡‘æ ‡å‡†ï¼Œæ·±åº¦å­¦ä¹ ç®—æ³•å·²è¢«åº”ç”¨äºè¾…åŠ©è¯Šæ–­é˜¿å°”èŒ¨æµ·é»˜ç—…ç­‰ç–¾ç—…ã€‚ä½†æ·±åº¦å­¦ä¹ ç®—æ³•å¯èƒ½é­å—æ·å¾„å­¦ä¹ çš„å½±å“ï¼Œåˆ©ç”¨ä¸è¾“å‡ºæ ‡ç­¾æ— ç›´æ¥å…³ç³»çš„ç‰¹å¾è¿›è¡Œé¢„æµ‹ï¼Œå¯èƒ½å¯¼è‡´å¯¹ä»£è¡¨æ€§ä¸è¶³çš„ç¾¤ä½“çš„æ€§èƒ½åè§ã€‚æœ¬æ–‡æ¢ç´¢äº†åŸºäºæ·±åº¦å­¦ä¹ çš„é˜¿å°”èŒ¨æµ·é»˜ç—…MRIè¯Šæ–­ä¸­çš„æ·å¾„å­¦ä¹ å’Œäººå£ç»Ÿè®¡åè§ã€‚é€šè¿‡å®éªŒå’Œå¤šä¸ªæ•°æ®é›†åŠæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ˆResNetå’ŒSwinTransformerï¼‰ï¼Œè¯æ˜å­˜åœ¨åŸºäºç§æ—å’Œæ€§åˆ«çš„æ·å¾„å­¦ä¹ å’Œåè§ã€‚æœ¬æ–‡å·¥ä½œä¸ºåŸºç¡€å¼€å‘æ›´å…¬å¹³çš„æ·±åº¦å­¦ä¹ è¯Šæ–­å·¥å…·æä¾›äº†åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ ç®—æ³•åœ¨MRIæ‰«æè¯Šæ–­é˜¿å°”èŒ¨æµ·é»˜ç—…ä¸­æœ‰åº”ç”¨æ½œåŠ›ã€‚</li>
<li>æ·±åº¦å­¦ä¹ ç®—æ³•å¯èƒ½é­å—æ·å¾„å­¦ä¹ çš„å½±å“ï¼Œåˆ©ç”¨ä¸è¾“å‡ºæ ‡ç­¾æ— å…³çš„ç‰¹å¾è¿›è¡Œé¢„æµ‹ã€‚</li>
<li>æ·å¾„å­¦ä¹ å¯èƒ½å¯¼è‡´å¯¹ä»£è¡¨æ€§ä¸è¶³çš„ç¾¤ä½“çš„æ€§èƒ½åè§ï¼Œå¦‚ç§æ—å’Œæ€§åˆ«ã€‚</li>
<li>æœ¬æ–‡æ¢ç´¢äº†åŸºäºç§æ—å’Œæ€§åˆ«çš„æ·å¾„å­¦ä¹ å’Œåè§åœ¨é˜¿å°”èŒ¨æµ·é»˜ç—…MRIè¯Šæ–­ä¸­çš„å­˜åœ¨æ€§ã€‚</li>
<li>é€šè¿‡å®éªŒå’Œå¤šä¸ªæ•°æ®é›†åŠæ·±åº¦å­¦ä¹ æ¨¡å‹çš„éªŒè¯ï¼Œè¯æ˜äº†åŸºäºç§æ—å’Œæ€§åˆ«çš„æ·å¾„å­¦ä¹ å’Œåè§çš„å­˜åœ¨ã€‚</li>
<li>æœ¬æ–‡å·¥ä½œä¸ºå¼€å‘æ›´å…¬å¹³çš„æ·±åº¦å­¦ä¹ è¯Šæ–­å·¥å…·å¥ å®šäº†åŸºç¡€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.09558">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2509.09558v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2509.09558v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2509.09558v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="DualTrack-Sensorless-3D-Ultrasound-needs-Local-and-Global-Context"><a href="#DualTrack-Sensorless-3D-Ultrasound-needs-Local-and-Global-Context" class="headerlink" title="DualTrack: Sensorless 3D Ultrasound needs Local and Global Context"></a>DualTrack: Sensorless 3D Ultrasound needs Local and Global Context</h2><p><strong>Authors:Paul F. R. Wilson, Matteo Ronchetti, RÃ¼diger GÃ¶bl, Viktoria Markova, Sebastian Rosenzweig, Raphael Prevost, Parvin Mousavi, Oliver Zettinig</strong></p>
<p>Three-dimensional ultrasound (US) offers many clinical advantages over conventional 2D imaging, yet its widespread adoption is limited by the cost and complexity of traditional 3D systems. Sensorless 3D US, which uses deep learning to estimate a 3D probe trajectory from a sequence of 2D US images, is a promising alternative. Local features, such as speckle patterns, can help predict frame-to-frame motion, while global features, such as coarse shapes and anatomical structures, can situate the scan relative to anatomy and help predict its general shape. In prior approaches, global features are either ignored or tightly coupled with local feature extraction, restricting the ability to robustly model these two complementary aspects. We propose DualTrack, a novel dual-encoder architecture that leverages decoupled local and global encoders specialized for their respective scales of feature extraction. The local encoder uses dense spatiotemporal convolutions to capture fine-grained features, while the global encoder utilizes an image backbone (e.g., a 2D CNN or foundation model) and temporal attention layers to embed high-level anatomical features and long-range dependencies. A lightweight fusion module then combines these features to estimate the trajectory. Experimental results on a large public benchmark show that DualTrack achieves state-of-the-art accuracy and globally consistent 3D reconstructions, outperforming previous methods and yielding an average reconstruction error below 5 mm. </p>
<blockquote>
<p>ä¸‰ç»´è¶…å£°ï¼ˆUSï¼‰ç›¸è¾ƒäºä¼ ç»Ÿçš„äºŒç»´æˆåƒåœ¨ä¸´åºŠåº”ç”¨ä¸Šæä¾›äº†è®¸å¤šä¼˜åŠ¿ï¼Œç„¶è€Œå…¶å¹¿æ³›åº”ç”¨å—é™äºä¼ ç»Ÿä¸‰ç»´ç³»ç»Ÿçš„æˆæœ¬å’Œå¤æ‚æ€§ã€‚æ— ä¼ æ„Ÿå™¨ä¸‰ç»´è¶…å£°åˆ©ç”¨æ·±åº¦å­¦ä¹ ä»ä¸€ç³»åˆ—äºŒç»´è¶…å£°å›¾åƒä¸­ä¼°è®¡ä¸‰ç»´æ¢å¤´è½¨è¿¹ï¼Œæ˜¯ä¸€ç§å¾ˆæœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ¡ˆã€‚å±€éƒ¨ç‰¹å¾ï¼ˆå¦‚æ–‘ç‚¹æ¨¡å¼ï¼‰æœ‰åŠ©äºé¢„æµ‹å¸§é—´è¿åŠ¨ï¼Œè€Œå…¨å±€ç‰¹å¾ï¼ˆå¦‚ç²—ç•¥å½¢çŠ¶å’Œè§£å‰–ç»“æ„ï¼‰å¯ä»¥å°†æ‰«æå®šä½åœ¨è§£å‰–ç»“æ„çš„ç›¸å…³ä½ç½®å¹¶å¸®åŠ©é¢„æµ‹å…¶æ•´ä½“å½¢çŠ¶ã€‚åœ¨ä»¥å‰çš„æ–¹æ³•ä¸­ï¼Œå…¨å±€ç‰¹å¾è¦ä¹ˆè¢«å¿½ç•¥ï¼Œè¦ä¹ˆä¸å±€éƒ¨ç‰¹å¾æå–ç´§å¯†è€¦åˆï¼Œé™åˆ¶äº†ç¨³å¥åœ°å»ºæ¨¡è¿™ä¸¤ä¸ªäº’è¡¥æ–¹é¢çš„èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†DualTrackï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„åŒç¼–ç å™¨æ¶æ„ï¼Œå®ƒåˆ©ç”¨è§£è€¦çš„å±€éƒ¨å’Œå…¨å±€ç¼–ç å™¨ï¼Œä¸“é—¨é’ˆå¯¹å…¶å„è‡ªå°ºåº¦çš„ç‰¹å¾æå–è¿›è¡Œä¸“ä¸šåŒ–å¤„ç†ã€‚å±€éƒ¨ç¼–ç å™¨ä½¿ç”¨å¯†é›†çš„æ—¶ç©ºå·ç§¯æ¥æ•æ‰ç²¾ç»†ç‰¹å¾ï¼Œè€Œå…¨å±€ç¼–ç å™¨åˆ™åˆ©ç”¨å›¾åƒä¸»å¹²ï¼ˆä¾‹å¦‚äºŒç»´å·ç§¯ç¥ç»ç½‘ç»œæˆ–åŸºç¡€æ¨¡å‹ï¼‰å’Œä¸´æ—¶æ³¨æ„åŠ›å±‚æ¥åµŒå…¥é«˜çº§è§£å‰–ç‰¹å¾å’Œé•¿æœŸä¾èµ–å…³ç³»ã€‚ç„¶åï¼Œä¸€ä¸ªè½»é‡çº§çš„èåˆæ¨¡å—ç»“åˆè¿™äº›ç‰¹å¾æ¥ä¼°è®¡è½¨è¿¹ã€‚åœ¨å¤§å‹å…¬å…±åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒDualTrackè¾¾åˆ°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ï¼Œå®ç°äº†å…¨å±€ä¸€è‡´çš„ä¸‰ç»´é‡å»ºï¼Œä¼˜äºä»¥å‰çš„æ–¹æ³•ï¼Œå¹³å‡é‡å»ºè¯¯å·®ä½äº5æ¯«ç±³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.09530v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸‰ç»´è¶…å£°ï¼ˆUSï¼‰ç›¸è¾ƒäºä¼ ç»ŸäºŒç»´æˆåƒçš„ä¸´åºŠä¼˜åŠ¿ï¼Œä½†å…¶å¹¿æ³›åº”ç”¨å—é™äºä¼ ç»Ÿä¸‰ç»´ç³»ç»Ÿçš„æˆæœ¬å’Œå¤æ‚æ€§ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„æ— ä¼ æ„Ÿå™¨ä¸‰ç»´è¶…å£°æŠ€æœ¯â€”â€”DualTrackï¼Œè¯¥æŠ€æœ¯åˆ©ç”¨è§£è€¦çš„å±€éƒ¨å’Œå…¨å±€ç¼–ç å™¨ï¼Œåˆ†åˆ«æå–ä¸åŒå°ºåº¦çš„ç‰¹å¾ï¼Œå®ç°äº†ç²¾ç»†ç‰¹å¾ä¸é«˜çº§è§£å‰–ç‰¹å¾çš„èåˆï¼Œä»è€Œåœ¨å…¬å…±æ•°æ®é›†ä¸Šå–å¾—äº†æœ€æ–°çš„è½¨è¿¹ä¼°è®¡å’Œä¸‰ç»´é‡å»ºæ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¸‰ç»´è¶…å£°ç›¸æ¯”ä¼ ç»ŸäºŒç»´æˆåƒå…·æœ‰å¤šç§ä¸´åºŠä¼˜åŠ¿ï¼Œä½†å—é™äºæˆæœ¬å’Œç³»ç»Ÿå¤æ‚æ€§ã€‚</li>
<li>æ— ä¼ æ„Ÿå™¨ä¸‰ç»´è¶…å£°åˆ©ç”¨æ·±åº¦å­¦ä¹ ä»ä¸€ç³»åˆ—äºŒç»´è¶…å£°å›¾åƒä¼°è®¡ä¸‰ç»´æ¢é’ˆè½¨è¿¹ã€‚</li>
<li>å±€éƒ¨ç‰¹å¾ï¼ˆå¦‚æ–‘ç‚¹æ¨¡å¼ï¼‰æœ‰åŠ©äºé¢„æµ‹å¸§é—´è¿åŠ¨ï¼Œè€Œå…¨å±€ç‰¹å¾ï¼ˆå¦‚ç²—ç•¥å½¢çŠ¶å’Œè§£å‰–ç»“æ„ï¼‰æœ‰åŠ©äºå°†æ‰«æå®šä½åœ¨è§£å‰–ç»“æ„å¹¶é¢„æµ‹å…¶æ€»ä½“å½¢çŠ¶ã€‚</li>
<li>DualTrackæ˜¯ä¸€ç§æ–°å‹çš„åŒç¼–ç å™¨æ¶æ„ï¼Œæ—¨åœ¨æå–å±€éƒ¨å’Œå…¨å±€ç‰¹å¾ï¼Œå®ç°ç²¾ç»†ç‰¹å¾ä¸é«˜çº§è§£å‰–ç‰¹å¾çš„åˆ†ç¦»æå–ä¸èåˆã€‚</li>
<li>å±€éƒ¨ç¼–ç å™¨é€šè¿‡å¯†é›†æ—¶ç©ºå·ç§¯æ•è·ç²¾ç»†ç‰¹å¾ã€‚</li>
<li>å…¨å±€ç¼–ç å™¨åˆ©ç”¨å›¾åƒä¸»å¹²ï¼ˆå¦‚äºŒç»´å·ç§¯ç¥ç»ç½‘ç»œæˆ–åŸºç¡€æ¨¡å‹ï¼‰å’Œæ—¶é—´æ³¨æ„åŠ›å±‚æ¥åµŒå…¥é«˜çº§è§£å‰–ç‰¹å¾å’Œé•¿æœŸä¾èµ–æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.09530">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2509.09530v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2509.09530v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2509.09530v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="FlexiD-Fuse-Flexible-number-of-inputs-multi-modal-medical-image-fusion-based-on-diffusion-model"><a href="#FlexiD-Fuse-Flexible-number-of-inputs-multi-modal-medical-image-fusion-based-on-diffusion-model" class="headerlink" title="FlexiD-Fuse: Flexible number of inputs multi-modal medical image fusion   based on diffusion model"></a>FlexiD-Fuse: Flexible number of inputs multi-modal medical image fusion   based on diffusion model</h2><p><strong>Authors:Yushen Xu, Xiaosong Li, Yuchun Wang, Xiaoqi Cheng, Huafeng Li, Haishu Tan</strong></p>
<p>Different modalities of medical images provide unique physiological and anatomical information for diseases. Multi-modal medical image fusion integrates useful information from different complementary medical images with different modalities, producing a fused image that comprehensively and objectively reflects lesion characteristics to assist doctors in clinical diagnosis. However, existing fusion methods can only handle a fixed number of modality inputs, such as accepting only two-modal or tri-modal inputs, and cannot directly process varying input quantities, which hinders their application in clinical settings. To tackle this issue, we introduce FlexiD-Fuse, a diffusion-based image fusion network designed to accommodate flexible quantities of input modalities. It can end-to-end process two-modal and tri-modal medical image fusion under the same weight. FlexiD-Fuse transforms the diffusion fusion problem, which supports only fixed-condition inputs, into a maximum likelihood estimation problem based on the diffusion process and hierarchical Bayesian modeling. By incorporating the Expectation-Maximization algorithm into the diffusion sampling iteration process, FlexiD-Fuse can generate high-quality fused images with cross-modal information from source images, independently of the number of input images. We compared the latest two and tri-modal medical image fusion methods, tested them on Harvard datasets, and evaluated them using nine popular metrics. The experimental results show that our method achieves the best performance in medical image fusion with varying inputs. Meanwhile, we conducted extensive extension experiments on infrared-visible, multi-exposure, and multi-focus image fusion tasks with arbitrary numbers, and compared them with the perspective SOTA methods. The results of the extension experiments consistently demonstrate the effectiveness and superiority of our method. </p>
<blockquote>
<p>ä¸åŒæ¨¡æ€çš„åŒ»å­¦å›¾åƒä¸ºç–¾ç—…æä¾›äº†ç‹¬ç‰¹çš„ç”Ÿç†å’Œè§£å‰–ä¿¡æ¯ã€‚å¤šæ¨¡æ€åŒ»å­¦å›¾åƒèåˆä»ä¸åŒæ¨¡æ€çš„äº’è¡¥åŒ»å­¦å›¾åƒä¸­é›†æˆæœ‰ç”¨ä¿¡æ¯ï¼Œç”Ÿæˆä¸€ä¸ªèåˆå›¾åƒï¼Œè¯¥å›¾åƒå…¨é¢å®¢è§‚åœ°åæ˜ äº†ç—…å˜ç‰¹å¾ï¼Œä»¥è¾…åŠ©åŒ»ç”Ÿè¿›è¡Œä¸´åºŠè¯Šæ–­ã€‚ç„¶è€Œï¼Œç°æœ‰çš„èåˆæ–¹æ³•åªèƒ½å¤„ç†å›ºå®šæ•°é‡çš„æ¨¡æ€è¾“å…¥ï¼Œä¾‹å¦‚ä»…æ¥å—äºŒæ¨¡æ€æˆ–ä¸‰æ¨¡æ€è¾“å…¥ï¼Œä¸èƒ½ç›´æ¥å¤„ç†ä¸åŒæ•°é‡çš„è¾“å…¥ï¼Œè¿™é˜»ç¢äº†å®ƒä»¬åœ¨ä¸´åºŠç¯å¢ƒä¸­çš„åº”ç”¨ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†FlexiD-Fuseï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºæ‰©æ•£çš„å›¾åƒèåˆç½‘ç»œï¼Œæ—¨åœ¨é€‚åº”çµæ´»æ•°é‡çš„è¾“å…¥æ¨¡æ€ã€‚å®ƒå¯ä»¥åœ¨ç›¸åŒçš„æƒé‡ä¸‹ï¼Œç«¯åˆ°ç«¯åœ°å¤„ç†äºŒæ¨¡æ€å’Œä¸‰æ¨¡æ€åŒ»å­¦å›¾åƒèåˆã€‚FlexiD-Fuseå°†ä»…æ”¯æŒå›ºå®šæ¡ä»¶è¾“å…¥çš„æ‰©æ•£èåˆé—®é¢˜è½¬åŒ–ä¸ºåŸºäºæ‰©æ•£è¿‡ç¨‹å’Œåˆ†å±‚è´å¶æ–¯å»ºæ¨¡çš„æœ€å¤§ä¼¼ç„¶ä¼°è®¡é—®é¢˜ã€‚é€šè¿‡å°†æœŸæœ›æœ€å¤§åŒ–ç®—æ³•èå…¥æ‰©æ•£é‡‡æ ·è¿­ä»£è¿‡ç¨‹ï¼ŒFlexiD-Fuseå¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„èåˆå›¾åƒï¼ŒåŒ…å«æ¥è‡ªæºå›¾åƒçš„è·¨æ¨¡æ€ä¿¡æ¯ï¼Œä¸”ä¸å—è¾“å…¥å›¾åƒæ•°é‡çš„é™åˆ¶ã€‚æˆ‘ä»¬æ¯”è¾ƒäº†æœ€æ–°çš„äºŒæ¨¡æ€å’Œä¸‰æ¨¡æ€åŒ»å­¦å›¾åƒèåˆæ–¹æ³•ï¼Œåœ¨å“ˆä½›æ•°æ®é›†ä¸Šè¿›è¡Œäº†æµ‹è¯•ï¼Œä½¿ç”¨ä¹ä¸ªæµè¡ŒæŒ‡æ ‡è¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨åŒ»å­¦å›¾åƒèåˆä¸­å®ç°äº†æœ€ä½³æ€§èƒ½ï¼Œä¸”è¾“å…¥å¯å˜ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬åœ¨çº¢å¤–å¯è§ã€å¤šæ›å…‰å’Œå¤šç„¦ç‚¹å›¾åƒèåˆä»»åŠ¡ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„æ‰©å±•å®éªŒï¼Œå¹¶ä¸å½“å‰æœ€ä½³æ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒã€‚æ‰©å±•å®éªŒçš„ç»“æœä¸€è‡´è¡¨æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.09456v1">PDF</a> </p>
<p><strong>Summary</strong><br>    å¼•å…¥FlexiD-Fuseç½‘ç»œï¼Œæ”¯æŒçµæ´»å¤šå˜æ¨¡æ€è¾“å…¥æ•°é‡çš„åŒ»å­¦å›¾åƒèåˆã€‚è¯¥æ–¹æ³•åŸºäºæ‰©æ•£è¿‡ç¨‹å’Œåˆ†å±‚è´å¶æ–¯å»ºæ¨¡ï¼Œå°†å›ºå®šæ¡ä»¶çš„èåˆé—®é¢˜è½¬åŒ–ä¸ºæœ€å¤§ä¼¼ç„¶ä¼°è®¡é—®é¢˜ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡èåˆå›¾åƒï¼Œä¸”èƒ½ç‹¬ç«‹å¤„ç†ä¸åŒæ•°é‡çš„è¾“å…¥å›¾åƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åŒ»å­¦å›¾åƒèåˆä»»åŠ¡ä¸Šå–å¾—äº†æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¸åŒæ¨¡æ€çš„åŒ»å­¦å›¾åƒä¸ºç–¾ç—…æä¾›ç‹¬ç‰¹çš„ç”Ÿç†å’Œè§£å‰–ä¿¡æ¯ã€‚</li>
<li>å¤šæ¨¡æ€åŒ»å­¦å›¾åƒèåˆèƒ½å¤Ÿé›†æˆä¸åŒæ¨¡æ€çš„åŒ»å­¦å›¾åƒä¸­çš„æœ‰ç”¨ä¿¡æ¯ï¼Œç”Ÿæˆä¸€ä¸ªå…¨é¢å®¢è§‚åœ°åæ˜ ç—…ç¶ç‰¹å¾çš„èåˆå›¾åƒï¼Œè¾…åŠ©åŒ»ç”Ÿè¿›è¡Œä¸´åºŠè¯Šæ–­ã€‚</li>
<li>ç°æœ‰èåˆæ–¹æ³•åªèƒ½å¤„ç†å›ºå®šæ•°é‡çš„æ¨¡æ€è¾“å…¥ï¼Œæ— æ³•ç›´æ¥å¤„ç†å˜åŒ–çš„è¾“å…¥é‡ï¼Œé™åˆ¶äº†å…¶åœ¨ä¸´åºŠç¯å¢ƒä¸­çš„åº”ç”¨ã€‚</li>
<li>FlexiD-Fuseç½‘ç»œè¢«è®¾è®¡ç”¨æ¥è§£å†³è¿™ä¸€éš¾é¢˜ï¼Œå®ƒæ”¯æŒçµæ´»å¤šå˜çš„è¾“å…¥æ¨¡æ€æ•°é‡ï¼Œå¹¶èƒ½åœ¨åŒä¸€æƒé‡ä¸‹è¿›è¡Œä¸¤æ¨¡æ€å’Œä¸‰æ¨¡æ€åŒ»å­¦å›¾åƒèåˆã€‚</li>
<li>FlexiD-Fuseå°†æ‰©æ•£èåˆé—®é¢˜è½¬åŒ–ä¸ºåŸºäºæ‰©æ•£è¿‡ç¨‹å’Œåˆ†å±‚è´å¶æ–¯å»ºæ¨¡çš„æœ€å¤§ä¼¼ç„¶ä¼°è®¡é—®é¢˜ã€‚</li>
<li>é€šè¿‡åœ¨æ‰©æ•£é‡‡æ ·è¿­ä»£è¿‡ç¨‹ä¸­èå…¥æœŸæœ›æœ€å¤§åŒ–ç®—æ³•ï¼ŒFlexiD-Fuseå¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„èåˆå›¾åƒï¼ŒåŒ…å«è·¨æ¨¡æ€çš„æºå›¾åƒä¿¡æ¯ï¼Œä¸”ç‹¬ç«‹äºè¾“å…¥å›¾åƒçš„æ•°é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.09456">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2509.09456v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2509.09456v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2509.09456v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2509.09456v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Modality-Agnostic-Input-Channels-Enable-Segmentation-of-Brain-lesions-in-Multimodal-MRI-with-Sequences-Unavailable-During-Training"><a href="#Modality-Agnostic-Input-Channels-Enable-Segmentation-of-Brain-lesions-in-Multimodal-MRI-with-Sequences-Unavailable-During-Training" class="headerlink" title="Modality-Agnostic Input Channels Enable Segmentation of Brain lesions in   Multimodal MRI with Sequences Unavailable During Training"></a>Modality-Agnostic Input Channels Enable Segmentation of Brain lesions in   Multimodal MRI with Sequences Unavailable During Training</h2><p><strong>Authors:Anthony P. Addison, Felix Wagner, Wentian Xu, Natalie Voets, Konstantinos Kamnitsas</strong></p>
<p>Segmentation models are important tools for the detection and analysis of lesions in brain MRI. Depending on the type of brain pathology that is imaged, MRI scanners can acquire multiple, different image modalities (contrasts). Most segmentation models for multimodal brain MRI are restricted to fixed modalities and cannot effectively process new ones at inference. Some models generalize to unseen modalities but may lose discriminative modality-specific information. This work aims to develop a model that can perform inference on data that contain image modalities unseen during training, previously seen modalities, and heterogeneous combinations of both, thus allowing a user to utilize any available imaging modalities. We demonstrate this is possible with a simple, thus practical alteration to the U-net architecture, by integrating a modality-agnostic input channel or pathway, alongside modality-specific input channels. To train this modality-agnostic component, we develop an image augmentation scheme that synthesizes artificial MRI modalities. Augmentations differentially alter the appearance of pathological and healthy brain tissue to create artificial contrasts between them while maintaining realistic anatomical integrity. We evaluate the method using 8 MRI databases that include 5 types of pathologies (stroke, tumours, traumatic brain injury, multiple sclerosis and white matter hyperintensities) and 8 modalities (T1, T1+contrast, T2, PD, SWI, DWI, ADC and FLAIR). The results demonstrate that the approach preserves the ability to effectively process MRI modalities encountered during training, while being able to process new, unseen modalities to improve its segmentation. Project code: <a target="_blank" rel="noopener" href="https://github.com/Anthony-P-Addison/AGN-MOD-SEG">https://github.com/Anthony-P-Addison/AGN-MOD-SEG</a> </p>
<blockquote>
<p>åœ¨å¤§è„‘MRIä¸­ï¼Œåˆ†å‰²æ¨¡å‹æ˜¯æ£€æµ‹å’Œåˆ†æç—…å˜çš„é‡è¦å·¥å…·ã€‚æ ¹æ®ä¸åŒçš„è„‘ç—…ç†æˆåƒç±»å‹ï¼ŒMRIæ‰«æä»ªå¯ä»¥è·å–å¤šç§ä¸åŒçš„å›¾åƒæ¨¡å¼ï¼ˆå¯¹æ¯”åº¦ï¼‰ã€‚é’ˆå¯¹å¤šæ¨¡æ€å¤§è„‘MRIçš„å¤šæ•°åˆ†å‰²æ¨¡å‹ä»…é™äºå›ºå®šæ¨¡å¼ï¼Œæ— æ³•åœ¨æ¨ç†è¿‡ç¨‹ä¸­æœ‰æ•ˆåœ°å¤„ç†æ–°æ¨¡å¼ã€‚ä¸€äº›æ¨¡å‹å¯ä»¥æ¨å¹¿åˆ°æœªè§è¿‡çš„æ¨¡å¼ï¼Œä½†å¯èƒ½ä¼šå¤±å»åˆ¤åˆ«æ€§çš„æ¨¡å¼ç‰¹å®šä¿¡æ¯ã€‚è¿™é¡¹å·¥ä½œæ—¨åœ¨å¼€å‘ä¸€ç§æ¨¡å‹ï¼Œèƒ½å¤Ÿå¯¹åŒ…å«è®­ç»ƒè¿‡ç¨‹ä¸­æœªè§è¿‡çš„å›¾åƒæ¨¡å¼ã€ä»¥å‰è§è¿‡çš„æ¨¡å¼ä»¥åŠä¸¤è€…çš„å¼‚è´¨ç»„åˆçš„æ•°æ®è¿›è¡Œæ¨ç†ï¼Œä»è€Œå…è®¸ç”¨æˆ·åˆ©ç”¨ä»»ä½•å¯ç”¨çš„æˆåƒæ¨¡å¼ã€‚æˆ‘ä»¬é€šè¿‡é›†æˆæ¨¡æ€æ— å…³è¾“å…¥é€šé“æˆ–è·¯å¾„ï¼Œå¯¹U-netæ¶æ„è¿›è¡Œäº†ç®€å•å®ç”¨çš„æ”¹è¿›ï¼Œè¯æ˜äº†è¿™æ˜¯å¯èƒ½çš„ï¼ŒåŒæ—¶è¿˜ä¿ç•™äº†æ¨¡æ€ç‰¹å®šè¾“å…¥é€šé“ã€‚ä¸ºäº†è®­ç»ƒè¿™ä¸ªæ¨¡æ€æ— å…³ç»„ä»¶ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§å›¾åƒå¢å¼ºæ–¹æ¡ˆï¼Œå¯ä»¥åˆæˆäººå·¥MRIæ¨¡å¼ã€‚å¢å¼ºé€šè¿‡æœ‰é€‰æ‹©åœ°æ”¹å˜ç—…ç†å’Œæ­£å¸¸è„‘ç»„ç»‡çš„å¤–è²Œï¼Œåœ¨å®ƒä»¬ä¹‹é—´åˆ›é€ äººä¸ºå¯¹æ¯”ï¼ŒåŒæ—¶ä¿æŒçœŸå®çš„è§£å‰–å®Œæ•´æ€§ã€‚æˆ‘ä»¬ä½¿ç”¨åŒ…å«5ç§ç—…ç†ç±»å‹ï¼ˆä¸­é£ã€è‚¿ç˜¤ã€è„‘å¤–ä¼¤ã€å¤šå‘æ€§ç¡¬åŒ–ç—‡å’Œç™½è´¨é«˜ä¿¡å·ï¼‰å’Œ8ç§æ¨¡å¼ï¼ˆT1ã€T1+å¯¹æ¯”åº¦ã€T2ã€PDã€SWIã€DWIã€ADCå’ŒFLAIRï¼‰çš„8ä¸ªMRIæ•°æ®åº“æ¥è¯„ä¼°è¯¥æ–¹æ³•ã€‚ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿ç•™å¤„ç†è®­ç»ƒè¿‡ç¨‹ä¸­é‡åˆ°çš„MRIæ¨¡å¼çš„èƒ½åŠ›çš„åŒæ—¶ï¼Œè¿˜èƒ½å¤„ç†æ–°çš„æœªè§è¿‡çš„æ¨¡å¼ä»¥æé«˜å…¶åˆ†å‰²èƒ½åŠ›ã€‚é¡¹ç›®ä»£ç ï¼š<a target="_blank" rel="noopener" href="https://github.com/Anthony-P-Addison/AGN-MOD-SEG">https://github.com/Anthony-P-Addison/AGN-MOD-SEG</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.09290v1">PDF</a> Accepted to MICCAI 2025, for the following workshop: ML-CDS 2025:   Multimodal Learning and Fusion Across Scales for Clinical Decision Support</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é’ˆå¯¹è„‘MRIå›¾åƒä¸­ç—…ç¶æ£€æµ‹å’Œåˆ†å‰²çš„æ¨¡å‹ã€‚è¯¥æ¨¡å‹å…·æœ‰å¤„ç†è®­ç»ƒæœŸé—´æœªè§è¿‡çš„æ¨¡æ€æ•°æ®çš„èƒ½åŠ›ï¼Œé€šè¿‡æ·»åŠ ä¸€ç§é€šç”¨çš„è¾“å…¥é€šé“æˆ–è·¯å¾„æ¥å®ç°è¿™ä¸€ç‚¹ã€‚åŒæ—¶ï¼Œé€šè¿‡å›¾åƒå¢å¼ºæŠ€æœ¯è®­ç»ƒè¯¥é€šç”¨ç»„ä»¶ï¼Œåˆæˆäººå·¥MRIæ¨¡æ€ï¼Œä»¥æ¨¡æ‹Ÿä¸åŒç—…ç†å’Œæ­£å¸¸è„‘ç»„ç»‡ä¹‹é—´çš„å¯¹æ¯”ã€‚ç»è¿‡åœ¨åŒ…å«å¤šç§ç—…ç†ç±»å‹å’Œæ¨¡æ€çš„MRIæ•°æ®åº“ä¸Šçš„è¯„ä¼°ï¼Œè¯æ˜è¯¥æ–¹æ³•åœ¨è®­ç»ƒæœŸé—´é‡åˆ°çš„æ¨¡æ€å’Œæ–°æœªè§è¿‡çš„æ¨¡æ€ä¸Šéƒ½èƒ½å®ç°æœ‰æ•ˆå¤„ç†ï¼Œæé«˜äº†åˆ†å‰²æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†é’ˆå¯¹è„‘MRIå›¾åƒä¸­ç—…ç¶æ£€æµ‹å’Œåˆ†å‰²çš„æ¨¡å‹çš„é‡è¦æ€§ã€‚</li>
<li>å¤§å¤šæ•°å¤šæ¨¡æ€è„‘MRIåˆ†å‰²æ¨¡å‹å—é™äºå›ºå®šçš„æ¨¡æ€ï¼Œæ— æ³•æœ‰æ•ˆå¤„ç†æ–°æ¨¡æ€æ•°æ®ã€‚</li>
<li>è¯¥æ¨¡å‹æ—¨åœ¨å¤„ç†è®­ç»ƒæœŸé—´æœªè§è¿‡çš„æ¨¡æ€æ•°æ®ï¼Œå¹¶å…è®¸ç”¨æˆ·åˆ©ç”¨ä»»ä½•å¯ç”¨çš„æˆåƒæ¨¡æ€ã€‚</li>
<li>é€šè¿‡å‘U-netæ¶æ„ä¸­æ·»åŠ é€šç”¨çš„è¾“å…¥é€šé“æˆ–è·¯å¾„ï¼Œå®ç°äº†æ¨¡å‹çš„çµæ´»æ€§ã€‚</li>
<li>é€šè¿‡å›¾åƒå¢å¼ºæŠ€æœ¯è®­ç»ƒé€šç”¨ç»„ä»¶ï¼Œåˆæˆäººå·¥MRIæ¨¡æ€ä»¥å¢å¼ºæ¨¡å‹çš„é€‚åº”èƒ½åŠ›ã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨å¤šä¸ªMRIæ•°æ®åº“ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼ŒåŒ…æ‹¬å¤šç§ç—…ç†ç±»å‹å’Œæ¨¡æ€ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.09290">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2509.09290v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2509.09290v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2509.09290v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2509.09290v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Unified-Start-Personalized-End-Progressive-Pruning-for-Efficient-3D-Medical-Image-Segmentation"><a href="#Unified-Start-Personalized-End-Progressive-Pruning-for-Efficient-3D-Medical-Image-Segmentation" class="headerlink" title="Unified Start, Personalized End: Progressive Pruning for Efficient 3D   Medical Image Segmentation"></a>Unified Start, Personalized End: Progressive Pruning for Efficient 3D   Medical Image Segmentation</h2><p><strong>Authors:Linhao Li, Yiwen Ye, Ziyang Chen, Yong Xia</strong></p>
<p>3D medical image segmentation often faces heavy resource and time consumption, limiting its scalability and rapid deployment in clinical environments. Existing efficient segmentation models are typically static and manually designed prior to training, which restricts their adaptability across diverse tasks and makes it difficult to balance performance with resource efficiency. In this paper, we propose PSP-Seg, a progressive pruning framework that enables dynamic and efficient 3D segmentation. PSP-Seg begins with a redundant model and iteratively prunes redundant modules through a combination of block-wise pruning and a functional decoupling loss. We evaluate PSP-Seg on five public datasets, benchmarking it against seven state-of-the-art models and six efficient segmentation models. Results demonstrate that the lightweight variant, PSP-Seg-S, achieves performance on par with nnU-Net while reducing GPU memory usage by 42-45%, training time by 29-48%, and parameter number by 83-87% across all datasets. These findings underscore PSP-Segâ€™s potential as a cost-effective yet high-performing alternative for widespread clinical application. </p>
<blockquote>
<p>ä¸‰ç»´åŒ»å­¦å›¾åƒåˆ†å‰²å¸¸å¸¸é¢ä¸´èµ„æºå’Œæ—¶é—´æ¶ˆè€—å¤§çš„é—®é¢˜ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨ä¸´åºŠç¯å¢ƒä¸­çš„å¯æ‰©å±•æ€§å’Œå¿«é€Ÿéƒ¨ç½²èƒ½åŠ›ã€‚ç°æœ‰çš„é«˜æ•ˆåˆ†å‰²æ¨¡å‹é€šå¸¸åœ¨è®­ç»ƒå‰é™æ€ä¸”æ‰‹åŠ¨è®¾è®¡ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨ä¸åŒä»»åŠ¡ä¸­çš„é€‚åº”æ€§ï¼Œå¹¶ä¸”éš¾ä»¥åœ¨æ€§èƒ½å’Œèµ„æºæ•ˆç‡ä¹‹é—´å–å¾—å¹³è¡¡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†PSP-Segï¼Œè¿™æ˜¯ä¸€ç§åŠ¨æ€ä¸”é«˜æ•ˆçš„ä¸‰ç»´åˆ†å‰²æ¸è¿›å¼ä¿®å‰ªæ¡†æ¶ã€‚PSP-Segä»å†—ä½™æ¨¡å‹å¼€å§‹ï¼Œé€šè¿‡å—çº§ä¿®å‰ªå’ŒåŠŸèƒ½è§£è€¦æŸå¤±çš„ç»“åˆï¼Œè¿­ä»£åœ°ä¿®å‰ªå†—ä½™æ¨¡å—ã€‚æˆ‘ä»¬åœ¨äº”ä¸ªå…¬å…±æ•°æ®é›†ä¸Šå¯¹PSP-Segè¿›è¡Œäº†è¯„ä¼°ï¼Œå°†å…¶ä¸ä¸ƒä¸ªæœ€æ–°æ¨¡å‹å’Œå…­ä¸ªé«˜æ•ˆåˆ†å‰²æ¨¡å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚ç»“æœè¡¨æ˜ï¼Œè½»é‡çº§å˜ä½“PSP-Seg-Sçš„æ€§èƒ½ä¸nnU-Netç›¸å½“ï¼ŒåŒæ—¶é™ä½äº†GPUå†…å­˜ä½¿ç”¨ç‡çš„42-45%ï¼Œè®­ç»ƒæ—¶é—´ç¼©çŸ­äº†29-48%ï¼Œåœ¨æ‰€æœ‰æ•°æ®é›†ä¸Šçš„å‚æ•°æ•°é‡å‡å°‘äº†83-87%ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†PSP-Segä½œä¸ºç»æµé«˜æ•ˆä¸”æ€§èƒ½ä¼˜å¼‚çš„é€‰æ‹©ï¼Œåœ¨å¹¿æ³›çš„ä¸´åºŠåº”ç”¨ä¸­çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.09267v1">PDF</a> 15 pages, 8 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºPSP-Segçš„æ¸è¿›å¼è£å‰ªæ¡†æ¶ï¼Œç”¨äºå®ç°åŠ¨æ€å’Œé«˜æ•ˆçš„3DåŒ»å­¦å›¾åƒåˆ†å‰²ã€‚è¯¥æ¡†æ¶ä»å†—ä½™æ¨¡å‹å¼€å§‹ï¼Œé€šè¿‡å—çº§è£å‰ªå’ŒåŠŸèƒ½è§£è€¦æŸå¤±çš„ç»“åˆï¼Œè¿­ä»£åœ°è£å‰ªå†—ä½™æ¨¡å—ã€‚åœ¨äº”ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒPSP-Segçš„è½»é‡çº§å˜ä½“PSP-Seg-Såœ¨æ€§èƒ½ä¸Šä¸nnU-Netç›¸å½“ï¼ŒåŒæ—¶é™ä½äº†GPUå†…å­˜ä½¿ç”¨ç‡42-45%ï¼Œè®­ç»ƒæ—¶é—´ç¼©çŸ­29-48%ï¼Œå‚æ•°æ•°é‡å‡å°‘83-87%ã€‚è¿™çªæ˜¾äº†PSP-Segä½œä¸ºç»æµå®æƒ ä¸”é«˜æ€§èƒ½çš„æ›¿ä»£æ–¹æ¡ˆåœ¨ä¸´åºŠåº”ç”¨ä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>3DåŒ»å­¦å›¾åƒåˆ†å‰²é¢ä¸´èµ„æºå’Œæ—¶é—´æ¶ˆè€—å¤§çš„é—®é¢˜ï¼Œé™åˆ¶äº†å…¶åœ¨ä¸´åºŠç¯å¢ƒä¸­çš„å¯æ‰©å±•æ€§å’Œå¿«é€Ÿéƒ¨ç½²ã€‚</li>
<li>ç°æœ‰é«˜æ•ˆåˆ†å‰²æ¨¡å‹é€šå¸¸æ˜¯é™æ€çš„ï¼Œåœ¨è®­ç»ƒå‰æ‰‹åŠ¨è®¾è®¡ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨ä¸åŒä»»åŠ¡ä¸­çš„é€‚åº”æ€§ã€‚</li>
<li>PSP-Segæ˜¯ä¸€ç§æ¸è¿›å¼è£å‰ªæ¡†æ¶ï¼Œèƒ½å¤Ÿå®ç°åŠ¨æ€å’Œé«˜æ•ˆçš„3Dåˆ†å‰²ã€‚</li>
<li>PSP-Segé€šè¿‡å—çº§è£å‰ªå’ŒåŠŸèƒ½è§£è€¦æŸå¤±çš„ç»“åˆï¼Œè¿­ä»£åœ°è£å‰ªå†—ä½™æ¨¡å—ã€‚</li>
<li>åœ¨å¤šä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒPSP-Segçš„è½»é‡çº§å˜ä½“PSP-Seg-Sæ€§èƒ½ä¼˜å¼‚ï¼Œä¸nnU-Netç›¸å½“ã€‚</li>
<li>PSP-Seg-Såœ¨GPUå†…å­˜ä½¿ç”¨ã€è®­ç»ƒæ—¶é—´å’Œå‚æ•°æ•°é‡æ–¹é¢æœ‰æ˜æ˜¾ä¼˜åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.09267">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2509.09267v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2509.09267v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2509.09267v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2509.09267v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2509.09267v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2509.09267v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Towards-Better-Dental-AI-A-Multimodal-Benchmark-and-Instruction-Dataset-for-Panoramic-X-ray-Analysis"><a href="#Towards-Better-Dental-AI-A-Multimodal-Benchmark-and-Instruction-Dataset-for-Panoramic-X-ray-Analysis" class="headerlink" title="Towards Better Dental AI: A Multimodal Benchmark and Instruction Dataset   for Panoramic X-ray Analysis"></a>Towards Better Dental AI: A Multimodal Benchmark and Instruction Dataset   for Panoramic X-ray Analysis</h2><p><strong>Authors:Jing Hao, Yuxuan Fan, Yanpeng Sun, Kaixin Guo, Lizhuo Lin, Jinrong Yang, Qi Yong H. Ai, Lun M. Wong, Hao Tang, Kuo Feng Hung</strong></p>
<p>Recent advances in large vision-language models (LVLMs) have demonstrated strong performance on general-purpose medical tasks. However, their effectiveness in specialized domains such as dentistry remains underexplored. In particular, panoramic X-rays, a widely used imaging modality in oral radiology, pose interpretative challenges due to dense anatomical structures and subtle pathological cues, which are not captured by existing medical benchmarks or instruction datasets. To this end, we introduce MMOral, the first large-scale multimodal instruction dataset and benchmark tailored for panoramic X-ray interpretation. MMOral consists of 20,563 annotated images paired with 1.3 million instruction-following instances across diverse task types, including attribute extraction, report generation, visual question answering, and image-grounded dialogue. In addition, we present MMOral-Bench, a comprehensive evaluation suite covering five key diagnostic dimensions in dentistry. We evaluate 64 LVLMs on MMOral-Bench and find that even the best-performing model, i.e., GPT-4o, only achieves 41.45% accuracy, revealing significant limitations of current models in this domain. To promote the progress of this specific domain, we also propose OralGPT, which conducts supervised fine-tuning (SFT) upon Qwen2.5-VL-7B with our meticulously curated MMOral instruction dataset. Remarkably, a single epoch of SFT yields substantial performance enhancements for LVLMs, e.g., OralGPT demonstrates a 24.73% improvement. Both MMOral and OralGPT hold significant potential as a critical foundation for intelligent dentistry and enable more clinically impactful multimodal AI systems in the dental field. The dataset, model, benchmark, and evaluation suite are available at <a target="_blank" rel="noopener" href="https://github.com/isbrycee/OralGPT">https://github.com/isbrycee/OralGPT</a>. </p>
<blockquote>
<p>è¿‘æœŸå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„è¿›å±•åœ¨é€šç”¨åŒ»ç–—ä»»åŠ¡ä¸Šè¡¨ç°å‡ºäº†å¼ºå¤§çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨ç‰¹å®šé¢†åŸŸï¼Œå¦‚ç‰™ç§‘é¢†åŸŸçš„æ•ˆæœå°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ç‰¹åˆ«æ˜¯å…¨æ™¯Xå°„çº¿ï¼Œä½œä¸ºå£è…”æ”¾å°„å­¦ä¸­å¹¿æ³›ä½¿ç”¨çš„æˆåƒæ–¹å¼ï¼Œç”±äºå…¶å¯†é›†çš„è§£å‰–ç»“æ„å’Œå¾®å¦™çš„ç—…ç†çº¿ç´¢ï¼Œç»™è§£è¯»å¸¦æ¥äº†æŒ‘æˆ˜ï¼Œè¿™äº›æŒ‘æˆ˜å¹¶æœªè¢«ç°æœ‰çš„åŒ»ç–—åŸºå‡†æµ‹è¯•æˆ–æŒ‡ä»¤æ•°æ®é›†æ‰€æ¶µç›–ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†MMOralï¼Œè¿™æ˜¯é¦–ä¸ªä¸“é—¨é’ˆå¯¹å…¨æ™¯Xå°„çº¿è§£è¯»çš„å¤§å‹å¤šæ¨¡å¼æŒ‡ä»¤æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•ã€‚MMOralåŒ…å«20,563å¼ å¸¦æ³¨é‡Šçš„å›¾åƒï¼Œä»¥åŠä¸å¤šç§ä»»åŠ¡ç±»å‹ç›¸åŒ¹é…çš„130ä¸‡ä¸ªæŒ‡ä»¤å®ä¾‹ï¼ŒåŒ…æ‹¬å±æ€§æå–ã€æŠ¥å‘Šç”Ÿæˆã€è§†è§‰é—®ç­”å’ŒåŸºäºå›¾åƒçš„å¯¹è¯ç­‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¨å‡ºäº†MMOral-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°å¥—ä»¶ï¼Œæ¶µç›–äº†ç‰™ç§‘çš„äº”ä¸ªå…³é”®è¯Šæ–­ç»´åº¦ã€‚æˆ‘ä»¬è¯„ä¼°äº†64ä¸ªLVLMåœ¨MMOral-Benchä¸Šçš„è¡¨ç°ï¼Œå¹¶å‘ç°å³ä½¿è¡¨ç°æœ€ä½³çš„æ¨¡å‹â€”â€”GPT-4oï¼Œå…¶å‡†ç¡®ç‡ä¹Ÿåªæœ‰41.45%ï¼Œè¿™æ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨è¿™ä¸ªé¢†åŸŸçš„æ˜¾è‘—å±€é™æ€§ã€‚ä¸ºäº†æ¨åŠ¨è¿™ä¸ªç‰¹å®šé¢†åŸŸçš„è¿›æ­¥ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†OralGPTï¼Œå®ƒæ˜¯åŸºäºQwen2.5-VL-7Bè¿›è¡Œçš„æœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰çš„äº§ç‰©ï¼Œä½¿ç”¨äº†æˆ‘ä»¬ç²¾å¿ƒç­–åˆ’çš„MMOralæŒ‡ä»¤æ•°æ®é›†ã€‚ä»¤äººç©ç›®çš„æ˜¯ï¼Œä»…éœ€ä¸€ä¸ªå‘¨æœŸçš„SFTå°±èƒ½ä¸ºLVLMå¸¦æ¥æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œä¾‹å¦‚OralGPTæ˜¾ç¤ºå‡º24.73%çš„æ”¹è¿›ã€‚MMOralå’ŒOralGPTåœ¨æ™ºèƒ½ç‰™ç§‘é¢†åŸŸå…·æœ‰å·¨å¤§çš„æ½œåŠ›ï¼Œä¸ºç‰™ç§‘é¢†åŸŸçš„å¤šæ¨¡å¼äººå·¥æ™ºèƒ½ç³»ç»Ÿå¸¦æ¥äº†æ›´å…·ä¸´åºŠå½±å“åŠ›çš„å¯èƒ½æ€§ã€‚æ•°æ®é›†ã€æ¨¡å‹ã€åŸºå‡†æµ‹è¯•å’Œè¯„ä¼°å¥—ä»¶å‡å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/isbrycee/OralGPT%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/isbrycee/OralGPTæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.09254v1">PDF</a> 40 pages, 26 figures, 9 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹ç‰™ç§‘å…¨æ™¯Xå°„çº¿è§£è¯»çš„å¤§è§„æ¨¡å¤šæ¨¡æ€æŒ‡ä»¤æ•°æ®é›†MMOralåŠå…¶è¯„ä¼°åŸºå‡†MMOral-Benchã€‚ç°æœ‰å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨è¯¥é¢†åŸŸè¡¨ç°æœ‰é™ï¼Œéœ€è¦MMOralæ•°æ®é›†è¿›è¡Œè®­ç»ƒå’Œæå‡ã€‚ç ”ç©¶æå‡ºOralGPTæ¨¡å‹ï¼Œé€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰åœ¨MMOralæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæ˜¾è‘—æé«˜LVLMsçš„æ€§èƒ½ã€‚MMOralå’ŒOralGPTä¸ºæ™ºèƒ½ç‰™ç§‘æä¾›äº†å…³é”®åŸºç¡€ï¼Œä¿ƒè¿›äº†ç‰™ç§‘é¢†åŸŸçš„å¤šæ¨¡æ€äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LVLMsåœ¨ä¸€èˆ¬åŒ»ç–—ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†åœ¨ç‰™ç§‘ç­‰ä¸“ä¸šé¢†åŸŸçš„åº”ç”¨ä»éœ€æ¢ç´¢ã€‚</li>
<li>ç‰™ç§‘å…¨æ™¯Xå°„çº¿è§£è¯»å­˜åœ¨æŒ‘æˆ˜ï¼Œéœ€è¦ä¸“é—¨çš„æ•°æ®é›†å’Œè¯„ä¼°åŸºå‡†ã€‚</li>
<li>å¼•å…¥MMOralæ•°æ®é›†ï¼ŒåŒ…å«20,563å¼ æ³¨é‡Šå›¾åƒå’Œ130ä¸‡æ¡æŒ‡ä»¤å®ä¾‹ã€‚</li>
<li>æå‡ºMMOral-Benchè¯„ä¼°åŸºå‡†ï¼Œæ¶µç›–ç‰™ç§‘çš„äº”ä¸ªå…³é”®è¯Šæ–­ç»´åº¦ã€‚</li>
<li>å½“å‰æœ€ä½³æ¨¡å‹GPT-4oåœ¨MMOral-Benchä¸Šçš„å‡†ç¡®ç‡ä»…ä¸º41.45%ï¼Œæ˜¾ç¤ºç°æœ‰æ¨¡å‹çš„å±€é™æ€§ã€‚</li>
<li>æå‡ºOralGPTæ¨¡å‹ï¼Œé€šè¿‡ç›‘ç£å¾®è°ƒåœ¨MMOralæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæ˜¾è‘—æé«˜æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.09254">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2509.09254v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2509.09254v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2509.09254v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2509.09254v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2509.09254v1/page_4_1.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Medverse-A-Universal-Model-for-Full-Resolution-3D-Medical-Image-Segmentation-Transformation-and-Enhancement"><a href="#Medverse-A-Universal-Model-for-Full-Resolution-3D-Medical-Image-Segmentation-Transformation-and-Enhancement" class="headerlink" title="Medverse: A Universal Model for Full-Resolution 3D Medical Image   Segmentation, Transformation and Enhancement"></a>Medverse: A Universal Model for Full-Resolution 3D Medical Image   Segmentation, Transformation and Enhancement</h2><p><strong>Authors:Jiesi Hu, Jianfeng Cao, Yanwu Yang, Chenfei Ye, Yixuan Zhang, Hanyang Peng, Ting Ma</strong></p>
<p>In-context learning (ICL) offers a promising paradigm for universal medical image analysis, enabling models to perform diverse image processing tasks without retraining. However, current ICL models for medical imaging remain limited in two critical aspects: they cannot simultaneously achieve high-fidelity predictions and global anatomical understanding, and there is no unified model trained across diverse medical imaging tasks (e.g., segmentation and enhancement) and anatomical regions. As a result, the full potential of ICL in medical imaging remains underexplored. Thus, we present \textbf{Medverse}, a universal ICL model for 3D medical imaging, trained on 22 datasets covering diverse tasks in universal image segmentation, transformation, and enhancement across multiple organs, imaging modalities, and clinical centers. Medverse employs a next-scale autoregressive in-context learning framework that progressively refines predictions from coarse to fine, generating consistent, full-resolution volumetric outputs and enabling multi-scale anatomical awareness. We further propose a blockwise cross-attention module that facilitates long-range interactions between context and target inputs while preserving computational efficiency through spatial sparsity. Medverse is extensively evaluated on a broad collection of held-out datasets covering previously unseen clinical centers, organs, species, and imaging modalities. Results demonstrate that Medverse substantially outperforms existing ICL baselines and establishes a novel paradigm for in-context learning. Code and model weights will be made publicly available. Our model are publicly available at <a target="_blank" rel="noopener" href="https://github.com/jiesihu/Medverse">https://github.com/jiesihu/Medverse</a>. </p>
<blockquote>
<p>ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ä¸ºé€šç”¨åŒ»å­¦å›¾åƒåˆ†ææä¾›äº†ä¸€ä¸ªæœ‰å‰é€”çš„èŒƒå¼ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸è¿›è¡Œå†è®­ç»ƒçš„æƒ…å†µä¸‹æ‰§è¡Œå¤šç§å›¾åƒå¤„ç†ä»»åŠ¡ã€‚ç„¶è€Œï¼Œå½“å‰çš„åŒ»å­¦æˆåƒICLæ¨¡å‹åœ¨ä¸¤ä¸ªæ–¹é¢ä»å­˜åœ¨å±€é™æ€§ï¼šå®ƒä»¬ä¸èƒ½åŒæ—¶å®ç°é«˜ä¿çœŸé¢„æµ‹å’Œå…¨å±€è§£å‰–ç»“æ„ç†è§£ï¼Œå¹¶ä¸”æ²¡æœ‰ç»Ÿä¸€çš„æ¨¡å‹å¯ä»¥åœ¨å„ç§åŒ»å­¦æˆåƒä»»åŠ¡ï¼ˆå¦‚åˆ†å‰²å’Œå¢å¼ºï¼‰å’Œè§£å‰–åŒºåŸŸä¸Šè¿›è¡Œè®­ç»ƒã€‚å› æ­¤ï¼ŒåŒ»å­¦æˆåƒä¸­ICLçš„æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†<strong>Medverse</strong>ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äº3DåŒ»å­¦æˆåƒçš„é€šç”¨ICLæ¨¡å‹ï¼Œåœ¨22ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæ¶µç›–äº†å¤šä¸ªå™¨å®˜ã€æˆåƒæ¨¡å¼ã€ä¸´åºŠä¸­å¿ƒçš„é€šç”¨å›¾åƒåˆ†å‰²ã€è½¬æ¢å’Œå¢å¼ºä»»åŠ¡ã€‚Medverseé‡‡ç”¨äº†ä¸€ç§å¤§è§„æ¨¡ä¸Šä¸‹æ–‡å­¦ä¹ çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä»ç²—ç•¥åˆ°ç²¾ç»†é€æ­¥ä¼˜åŒ–é¢„æµ‹ï¼Œç”Ÿæˆä¸€è‡´çš„å…¨åˆ†è¾¨ç‡ä½“ç§¯è¾“å‡ºï¼Œå¹¶å®ç°å¤šå°ºåº¦è§£å‰–ç»“æ„æ„ŸçŸ¥ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§å—çº§äº¤å‰æ³¨æ„æ¨¡å—ï¼Œè¯¥æ¨¡å—ä¿ƒè¿›äº†ä¸Šä¸‹æ–‡å’Œç›®æ ‡è¾“å…¥ä¹‹é—´çš„é•¿è·ç¦»äº¤äº’ï¼ŒåŒæ—¶é€šè¿‡ç©ºé—´ç¨€ç–æ€§ä¿æŒè®¡ç®—æ•ˆç‡ã€‚Medverseåœ¨æ¶µç›–ä»¥å‰æœªè§è¿‡çš„ä¸´åºŠä¸­å¿ƒã€å™¨å®˜ã€ç‰©ç§å’Œæˆåƒæ¨¡å¼çš„å¤§é‡ç‹¬ç«‹æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼ŒMedverseæ˜¾è‘—ä¼˜äºç°æœ‰çš„ICLåŸºçº¿ï¼Œå¹¶ä¸ºä¸Šä¸‹æ–‡å­¦ä¹ å»ºç«‹äº†æ–°çš„èŒƒå¼ã€‚ä»£ç å’Œæ¨¡å‹æƒé‡å°†å…¬å¼€å‘å¸ƒã€‚æˆ‘ä»¬çš„æ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/jiesihu/Medverse">https://github.com/jiesihu/Medverse</a>è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.09232v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹åŒ»å­¦å›¾åƒåˆ†æçš„ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰çš„æ–°æ¨¡å‹â€”â€”Medverseã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨å¤šä¸ªå™¨å®˜ã€æˆåƒæ–¹å¼ã€ä¸´åºŠä¸­å¿ƒçš„å¤šç§ä»»åŠ¡ä¸Šè¿›è¡Œè®­ç»ƒï¼ŒåŒ…æ‹¬å›¾åƒåˆ†å‰²ã€è½¬æ¢å’Œå¢å¼ºç­‰ã€‚Medverseé‡‡ç”¨æ¸è¿›å¼ç²¾ç»†åŒ–é¢„æµ‹çš„æ–¹å¼ï¼Œç”Ÿæˆé«˜åˆ†è¾¨ç‡çš„ä¸‰ç»´è¾“å‡ºï¼Œå¹¶å…·å¤‡å¤šå°ºåº¦è§£å‰–æ„è¯†ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜å…·æœ‰å—äº¤å‰æ³¨æ„åŠ›æ¨¡å—ï¼Œä¿ƒè¿›ä¸Šä¸‹æ–‡ä¸ç›®æ ‡è¾“å…¥ä¹‹é—´çš„é•¿æœŸäº¤äº’ï¼ŒåŒæ—¶é€šè¿‡ç©ºé—´ç¨€ç–æ€§ä¿æŒè®¡ç®—æ•ˆç‡ã€‚ç»è¿‡å¹¿æ³›çš„è¯„ä¼°ï¼ŒMedverseæ˜¾è‘—ä¼˜äºç°æœ‰çš„ICLåŸºçº¿ï¼Œä¸ºä¸Šä¸‹æ–‡å­¦ä¹ æä¾›äº†æ–°çš„èŒƒä¾‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Medverseæ˜¯ä¸€ä¸ªé’ˆå¯¹åŒ»å­¦å›¾åƒåˆ†æçš„é€šç”¨ICLæ¨¡å‹ï¼Œèƒ½åœ¨å¤šç§ä»»åŠ¡ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¦‚å›¾åƒåˆ†å‰²ã€è½¬æ¢å’Œå¢å¼ºç­‰ã€‚</li>
<li>Medverseé‡‡ç”¨æ¸è¿›å¼ç²¾ç»†åŒ–é¢„æµ‹ï¼Œç”Ÿæˆé«˜åˆ†è¾¨ç‡çš„ä¸‰ç»´è¾“å‡ºï¼Œå®ç°å¤šå°ºåº¦è§£å‰–æ„è¯†ã€‚</li>
<li>è¯¥æ¨¡å‹é€šè¿‡å—äº¤å‰æ³¨æ„åŠ›æ¨¡å—ä¿ƒè¿›ä¸Šä¸‹æ–‡ä¸ç›®æ ‡è¾“å…¥ä¹‹é—´çš„é•¿æœŸäº¤äº’ã€‚</li>
<li>Medverseèƒ½å¤Ÿåœ¨å¤šç§ä¸åŒçš„æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜è¶Šï¼ŒåŒ…æ‹¬æ¥è‡ªæœªè§è¿‡çš„ä¸´åºŠä¸­å¿ƒã€å™¨å®˜ã€ç‰©ç§å’Œæˆåƒæ–¹å¼çš„æ•°æ®ã€‚</li>
<li>Medverseæ˜¾è‘—ä¼˜äºç°æœ‰çš„ICLåŸºçº¿ï¼Œä¸ºä¸Šä¸‹æ–‡å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒåˆ†æé¢†åŸŸæä¾›äº†æ–°çš„èŒƒä¾‹ã€‚</li>
<li>Medverseæ¨¡å‹å…¬å¼€å¯ç”¨ï¼Œå¹¶æä¾›äº†ä»£ç å’Œæ¨¡å‹æƒé‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.09232">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2509.09232v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2509.09232v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2509.09232v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2509.09232v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2509.09232v1/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2509.09232v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2509.09232v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Continuous-Time-Value-Iteration-for-Multi-Agent-Reinforcement-Learning"><a href="#Continuous-Time-Value-Iteration-for-Multi-Agent-Reinforcement-Learning" class="headerlink" title="Continuous-Time Value Iteration for Multi-Agent Reinforcement Learning"></a>Continuous-Time Value Iteration for Multi-Agent Reinforcement Learning</h2><p><strong>Authors:Xuefeng Wang, Lei Zhang, Henglin Pu, Ahmed H. Qureshi, Husheng Li</strong></p>
<p>Existing reinforcement learning (RL) methods struggle with complex dynamical systems that demand interactions at high frequencies or irregular time intervals. Continuous-time RL (CTRL) has emerged as a promising alternative by replacing discrete-time Bellman recursion with differential value functions defined as viscosity solutions of the Hamiltonâ€“Jacobiâ€“Bellman (HJB) equation. While CTRL has shown promise, its applications have been largely limited to the single-agent domain. This limitation stems from two key challenges: (i) conventional solution methods for HJB equations suffer from the curse of dimensionality (CoD), making them intractable in high-dimensional systems; and (ii) even with HJB-based learning approaches, accurately approximating centralized value functions in multi-agent settings remains difficult, which in turn destabilizes policy training. In this paper, we propose a CT-MARL framework that uses physics-informed neural networks (PINNs) to approximate HJB-based value functions at scale. To ensure the value is consistent with its differential structure, we align value learning with value-gradient learning by introducing a Value Gradient Iteration (VGI) module that iteratively refines value gradients along trajectories. This improves gradient fidelity, in turn yielding more accurate values and stronger policy learning. We evaluate our method using continuous-time variants of standard benchmarks, including multi-agent particle environment (MPE) and multi-agent MuJoCo. Our results demonstrate that our approach consistently outperforms existing continuous-time RL baselines and scales to complex multi-agent dynamics. </p>
<blockquote>
<p>ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•åœ¨åº”å¯¹éœ€è¦é«˜é¢‘æˆ–ä¸è§„åˆ™æ—¶é—´é—´éš”äº¤äº’çš„å¤æ‚åŠ¨æ€ç³»ç»Ÿæ—¶é¢ä¸´æŒ‘æˆ˜ã€‚è¿ç»­æ—¶é—´å¼ºåŒ–å­¦ä¹ ï¼ˆCTRLï¼‰ä½œä¸ºä¸€ç§æœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ³•åº”è¿è€Œç”Ÿï¼Œå®ƒé€šè¿‡ç”¨å“ˆå¯†é¡¿-é›…å¯æ¯”-è´å°”æ›¼ï¼ˆHJBï¼‰æ–¹ç¨‹çš„ç²˜æ€§è§£æ‰€å®šä¹‰çš„å¾®åˆ†å€¼å‡½æ•°æ›¿æ¢ç¦»æ•£æ—¶é—´è´å°”æ›¼é€’å½’ã€‚è™½ç„¶CTRLå·²ç»æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å…¶åº”ç”¨å¤§å¤šä»…é™äºå•æ™ºèƒ½ä½“é¢†åŸŸã€‚è¿™ä¸€é™åˆ¶æºäºä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼š(i)è§£å†³HJBæ–¹ç¨‹çš„ä¼ ç»Ÿæ–¹æ³•å—åˆ°ç»´æ•°è¯…å’’ï¼ˆCoDï¼‰çš„å½±å“ï¼Œä½¿å…¶åœ¨é«˜ç»´ç³»ç»Ÿä¸­éš¾ä»¥å¤„ç†ï¼›(ii)å³ä½¿åœ¨åŸºäºHJBçš„å­¦ä¹ æ–¹æ³•ä¸­ï¼Œåœ¨å¤šäººç¯å¢ƒä¸­å‡†ç¡®åœ°è¿‘ä¼¼é›†ä¸­å€¼å‡½æ•°ä»ç„¶å¾ˆå›°éš¾ï¼Œè¿™åè¿‡æ¥åˆä¼šä½¿ç­–ç•¥è®­ç»ƒä¸ç¨³å®šã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä½¿ç”¨ç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œï¼ˆPINNsï¼‰æ¥å¤§è§„æ¨¡è¿‘ä¼¼åŸºäºHJBçš„å€¼å‡½æ•°çš„CT-MARLæ¡†æ¶ã€‚ä¸ºäº†ç¡®ä¿å€¼ä¸å®ƒçš„å¾®åˆ†ç»“æ„ä¸€è‡´ï¼Œæˆ‘ä»¬é€šè¿‡å¼•å…¥å€¼æ¢¯åº¦è¿­ä»£ï¼ˆVGIï¼‰æ¨¡å—å°†å€¼å­¦ä¹ ä¸å€¼æ¢¯åº¦å­¦ä¹ å¯¹é½ï¼Œè¯¥æ¨¡å—æ²¿è½¨è¿¹è¿­ä»£åœ°ä¼˜åŒ–å€¼æ¢¯åº¦ã€‚è¿™æé«˜äº†æ¢¯åº¦çš„ä¿çœŸåº¦ï¼Œè¿›è€Œå¾—åˆ°æ›´å‡†ç¡®çš„å€¼å’Œæ›´å¼ºçš„ç­–ç•¥å­¦ä¹ ã€‚æˆ‘ä»¬ä½¿ç”¨åŒ…æ‹¬å¤šæ™ºèƒ½ä½“ç²’å­ç¯å¢ƒï¼ˆMPEï¼‰å’Œå¤šæ™ºèƒ½ä½“MuJoCoåœ¨å†…çš„æ ‡å‡†è¿ç»­æ—¶é—´å˜ä½“æ¥è¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•ã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å§‹ç»ˆä¼˜äºç°æœ‰çš„è¿ç»­æ—¶é—´RLåŸºå‡†æµ‹è¯•ï¼Œå¹¶èƒ½å¤Ÿæ‰©å±•åˆ°å¤æ‚çš„å¤šæ™ºèƒ½ä½“åŠ¨æ€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.09135v1">PDF</a> 19 pages, 10 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œï¼ˆPINNsï¼‰çš„è¿ç»­æ—¶é—´å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼ˆCT-MARLï¼‰ã€‚è¯¥æ¡†æ¶åˆ©ç”¨PINNsè¿‘ä¼¼æ±‰å¯†å°”é¡¿-é›…å¯æ¯”-è´å°”æ›¼ï¼ˆHJBï¼‰æ–¹ç¨‹çš„ä»·å€¼å‡½æ•°ï¼Œè§£å†³äº†ä¼ ç»Ÿè§£å†³HJBæ–¹ç¨‹æ–¹æ³•é¢ä¸´çš„ç»´åº¦è¯…å’’ï¼ˆCoDï¼‰é—®é¢˜ã€‚åŒæ—¶ï¼Œå¼•å…¥ä»·å€¼æ¢¯åº¦è¿­ä»£ï¼ˆVGIï¼‰æ¨¡å—ï¼Œç¡®ä¿ä»·å€¼ä¸å¾®åˆ†ç»“æ„çš„ä¸€è‡´æ€§ï¼Œæé«˜æ¢¯åº¦å‡†ç¡®æ€§ï¼Œè¿›è€Œå®ç°æ›´ç²¾ç¡®çš„ä»·å€¼ä¼°è®¡å’Œæ›´å¼ºçš„ç­–ç•¥å­¦ä¹ ã€‚åœ¨æ ‡å‡†è¿ç»­æ—¶é—´åŸºå‡†æµ‹è¯•ç¯å¢ƒä¸­ï¼ŒåŒ…æ‹¬å¤šæ™ºèƒ½ä½“ç²’å­ç¯å¢ƒï¼ˆMPEï¼‰å’Œå¤šæ™ºèƒ½ä½“MuJoCoï¼Œè¯¥æ–¹æ³•è¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿ç»­æ—¶é—´å¼ºåŒ–å­¦ä¹ ï¼ˆCTRLï¼‰é€šè¿‡è§£å†³æ±‰å¯†å°”é¡¿-é›…å¯æ¯”-è´å°”æ›¼ï¼ˆHJBï¼‰æ–¹ç¨‹å¤„ç†å¤æ‚åŠ¨æ€ç³»ç»Ÿã€‚</li>
<li>ä¼ ç»Ÿè§£å†³HJBæ–¹ç¨‹æ–¹æ³•é¢ä¸´ç»´åº¦è¯…å’’ï¼ˆCoDï¼‰é—®é¢˜ï¼Œéš¾ä»¥åº”ç”¨äºé«˜ç»´ç³»ç»Ÿã€‚</li>
<li>ç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œï¼ˆPINNsï¼‰è¢«ç”¨äºè¿‘ä¼¼HJBæ–¹ç¨‹çš„ä»·å€¼å‡½æ•°ï¼Œè§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>å¼•å…¥ä»·å€¼æ¢¯åº¦è¿­ä»£ï¼ˆVGIï¼‰æ¨¡å—ï¼Œç¡®ä¿ä»·å€¼ä¸å¾®åˆ†ç»“æ„çš„ä¸€è‡´æ€§ï¼Œæé«˜æ¢¯åº¦å‡†ç¡®æ€§ã€‚</li>
<li>CT-MARLæ¡†æ¶èƒ½åœ¨å¤æ‚å¤šæ™ºèƒ½ä½“åŠ¨æ€ç¯å¢ƒä¸­å®ç°æ›´ç²¾ç¡®çš„ä»·å€¼ä¼°è®¡å’Œæ›´å¼ºçš„ç­–ç•¥å­¦ä¹ ã€‚</li>
<li>åœ¨è¿ç»­æ—¶é—´åŸºå‡†æµ‹è¯•ç¯å¢ƒä¸­ï¼ŒCT-MARLæ¡†æ¶è¡¨ç°ä¼˜äºç°æœ‰è¿ç»­æ—¶é—´RLåŸºçº¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.09135">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2509.09135v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2509.09135v1/page_1_0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Enhancing-3D-Medical-Image-Understanding-with-Pretraining-Aided-by-2D-Multimodal-Large-Language-Models"><a href="#Enhancing-3D-Medical-Image-Understanding-with-Pretraining-Aided-by-2D-Multimodal-Large-Language-Models" class="headerlink" title="Enhancing 3D Medical Image Understanding with Pretraining Aided by 2D   Multimodal Large Language Models"></a>Enhancing 3D Medical Image Understanding with Pretraining Aided by 2D   Multimodal Large Language Models</h2><p><strong>Authors:Qiuhui Chen, Xuancheng Yao, Huping Ye, Yi Hong</strong></p>
<p>Understanding 3D medical image volumes is critical in the medical field, yet existing 3D medical convolution and transformer-based self-supervised learning (SSL) methods often lack deep semantic comprehension. Recent advancements in multimodal large language models (MLLMs) provide a promising approach to enhance image understanding through text descriptions. To leverage these 2D MLLMs for improved 3D medical image understanding, we propose Med3DInsight, a novel pretraining framework that integrates 3D image encoders with 2D MLLMs via a specially designed plane-slice-aware transformer module. Additionally, our model employs a partial optimal transport based alignment, demonstrating greater tolerance to noise introduced by potential noises in LLM-generated content. Med3DInsight introduces a new paradigm for scalable multimodal 3D medical representation learning without requiring human annotations. Extensive experiments demonstrate our state-of-the-art performance on two downstream tasks, i.e., segmentation and classification, across various public datasets with CT and MRI modalities, outperforming current SSL methods. Med3DInsight can be seamlessly integrated into existing 3D medical image understanding networks, potentially enhancing their performance. Our source code, generated datasets, and pre-trained models will be available at <a target="_blank" rel="noopener" href="https://github.com/Qybc/Med3DInsight">https://github.com/Qybc/Med3DInsight</a>. </p>
<blockquote>
<p>ç†è§£ä¸‰ç»´åŒ»å­¦å›¾åƒä½“ç§¯åœ¨åŒ»å­¦é¢†åŸŸè‡³å…³é‡è¦ï¼Œç„¶è€Œç°æœ‰çš„åŸºäºä¸‰ç»´åŒ»å­¦å·ç§¯å’Œè½¬æ¢å™¨è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰çš„æ–¹æ³•é€šå¸¸ç¼ºä¹æ·±åº¦è¯­ä¹‰ç†è§£ã€‚æœ€è¿‘çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„è¿›æ­¥æä¾›äº†ä¸€ç§æœ‰å‰é€”çš„æ–¹æ³•ï¼Œå¯ä»¥é€šè¿‡æ–‡æœ¬æè¿°å¢å¼ºå›¾åƒç†è§£ã€‚ä¸ºäº†åˆ©ç”¨è¿™äº›äºŒç»´MLLMæ¥æ”¹å–„ä¸‰ç»´åŒ»å­¦å›¾åƒçš„ç†è§£ï¼Œæˆ‘ä»¬æå‡ºäº†Med3DInsightï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹é¢„è®­ç»ƒæ¡†æ¶ï¼Œé€šè¿‡ä¸“é—¨è®¾è®¡çš„å¹³é¢åˆ‡ç‰‡æ„ŸçŸ¥è½¬æ¢å™¨æ¨¡å—ï¼Œå°†ä¸‰ç»´å›¾åƒç¼–ç å™¨ä¸äºŒç»´MLLMé›†æˆåœ¨ä¸€èµ·ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ¨¡å‹é‡‡ç”¨åŸºäºéƒ¨åˆ†æœ€ä¼˜ä¼ è¾“çš„å¯¹é½æ–¹å¼ï¼Œè¡¨ç°å‡ºå¯¹è¯­è¨€æ¨¡å‹ç”Ÿæˆå†…å®¹ä¸­æ½œåœ¨å™ªå£°å¼•å…¥çš„å™ªå£°çš„æ›´å¤§å®¹å¿åº¦ã€‚Med3DInsightä¸ºå¯æ‰©å±•çš„å¤šæ¨¡æ€ä¸‰ç»´åŒ»å­¦è¡¨ç¤ºå­¦ä¹ æä¾›äº†æ–°çš„èŒƒå¼ï¼Œæ— éœ€äººä¸ºæ³¨é‡Šã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬åœ¨ä¸¤ä¸ªä¸‹æ¸¸ä»»åŠ¡ï¼ˆå³åˆ†å‰²å’Œåˆ†ç±»ï¼‰ä¸Šçš„è¡¨ç°è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼Œåœ¨å„ç§å…¬å…±æ•°æ®é›†ä¸Šï¼Œä½¿ç”¨CTå’ŒMRIæ¨¡æ€å‡è¶…è¶Šäº†å½“å‰çš„SSLæ–¹æ³•ã€‚Med3DInsightå¯ä»¥æ— ç¼é›†æˆåˆ°ç°æœ‰çš„ä¸‰ç»´åŒ»å­¦å›¾åƒç†è§£ç½‘ç»œä¸­ï¼Œæœ‰å¯èƒ½æé«˜å®ƒä»¬çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„æºä»£ç ã€ç”Ÿæˆçš„æ•°æ®é›†å’Œé¢„è®­ç»ƒæ¨¡å‹å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/Qybc/Med3DInsight%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/Qybc/Med3DInsightä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.09064v1">PDF</a> Accepted by IEEE Journal of Biomedical and Health Informatics (JBHI)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„é¢„è®­ç»ƒæ¡†æ¶Med3DInsightï¼Œç”¨äºå¢å¼ºå¯¹ä¸‰ç»´åŒ»å­¦å›¾åƒçš„ç†è§£ã€‚è¯¥æ¡†æ¶ç»“åˆäº†ä¸‰ç»´å›¾åƒç¼–ç å™¨å’ŒäºŒç»´å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡ä¸“é—¨çš„å¹³é¢åˆ‡ç‰‡æ„ŸçŸ¥è½¬æ¢å™¨æ¨¡å—å®ç°ã€‚Med3DInsighté‡‡ç”¨éƒ¨åˆ†æœ€ä¼˜ä¼ è¾“å¯¹é½ï¼Œå¯¹è¯­è¨€æ¨¡å‹ç”Ÿæˆå†…å®¹ä¸­çš„æ½œåœ¨å™ªå£°è¡¨ç°å‡ºè¾ƒå¼ºçš„è€å—æ€§ã€‚è¯¥æ¡†æ¶åœ¨å¤šä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„åˆ†å‰²å’Œåˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä¸”å¯æ— ç¼é›†æˆåˆ°ç°æœ‰çš„ä¸‰ç»´åŒ»å­¦å›¾åƒç†è§£ç½‘ç»œä¸­ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Med3DInsightæ˜¯ä¸€ä¸ªæ–°çš„é¢„è®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºå¯¹ä¸‰ç»´åŒ»å­¦å›¾åƒçš„ç†è§£ã€‚</li>
<li>æ¡†æ¶ç»“åˆäº†ä¸‰ç»´å›¾åƒç¼–ç å™¨å’ŒäºŒç»´å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>é€šè¿‡ä¸“é—¨çš„å¹³é¢åˆ‡ç‰‡æ„ŸçŸ¥è½¬æ¢å™¨æ¨¡å—å®ç°å›¾åƒä¸æ–‡æœ¬çš„èåˆã€‚</li>
<li>é‡‡ç”¨éƒ¨åˆ†æœ€ä¼˜ä¼ è¾“å¯¹é½ï¼Œæé«˜äº†å¯¹è¯­è¨€æ¨¡å‹ç”Ÿæˆå†…å®¹ä¸­å™ªå£°çš„è€å—æ€§ã€‚</li>
<li>åœ¨å¤šä¸ªå…¬å…±æ•°æ®é›†çš„åˆ†å‰²å’Œåˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä¼˜äºç°æœ‰çš„è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ã€‚</li>
<li>Med3DInsightå¯æ— ç¼é›†æˆåˆ°ç°æœ‰çš„ä¸‰ç»´åŒ»å­¦å›¾åƒç†è§£ç½‘ç»œä¸­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.09064">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2509.09064v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2509.09064v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2509.09064v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2509.09064v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="USEANet-Ultrasound-Specific-Edge-Aware-Multi-Branch-Network-for-Lightweight-Medical-Image-Segmentation"><a href="#USEANet-Ultrasound-Specific-Edge-Aware-Multi-Branch-Network-for-Lightweight-Medical-Image-Segmentation" class="headerlink" title="USEANet: Ultrasound-Specific Edge-Aware Multi-Branch Network for   Lightweight Medical Image Segmentation"></a>USEANet: Ultrasound-Specific Edge-Aware Multi-Branch Network for   Lightweight Medical Image Segmentation</h2><p><strong>Authors:Jingyi Gao, Di Wu, Baha lhnaini</strong></p>
<p>Ultrasound image segmentation faces unique challenges including speckle noise, low contrast, and ambiguous boundaries, while clinical deployment demands computationally efficient models. We propose USEANet, an ultrasound-specific edge-aware multi-branch network that achieves optimal performance-efficiency balance through four key innovations: (1) ultrasound-specific multi-branch processing with specialized modules for noise reduction, edge enhancement, and contrast improvement; (2) edge-aware attention mechanisms that focus on boundary information with minimal computational overhead; (3) hierarchical feature aggregation with adaptive weight learning; and (4) ultrasound-aware decoder enhancement for optimal segmentation refinement. Built on an ultra-lightweight PVT-B0 backbone, USEANet significantly outperforms existing methods across five ultrasound datasets while using only 3.64M parameters and 0.79G FLOPs. Experimental results demonstrate superior segmentation accuracy with 67.01 IoU on BUSI dataset, representing substantial improvements over traditional approaches while maintaining exceptional computational efficiency suitable for real-time clinical applications. Code is available at <a target="_blank" rel="noopener" href="https://github.com/chouheiwa/USEANet">https://github.com/chouheiwa/USEANet</a>. </p>
<blockquote>
<p>è¶…å£°å›¾åƒåˆ†å‰²é¢ä¸´ç€æ–‘ç‚¹å™ªå£°ã€ä½å¯¹æ¯”åº¦å’Œæ¨¡ç³Šè¾¹ç•Œç­‰ç‹¬ç‰¹æŒ‘æˆ˜ï¼Œè€Œä¸´åºŠéƒ¨ç½²åˆ™è¦æ±‚è®¡ç®—æ•ˆç‡é«˜çš„æ¨¡å‹ã€‚æˆ‘ä»¬æå‡ºäº†USEANetï¼Œè¿™æ˜¯ä¸€ä¸ªè¶…å£°ä¸“ç”¨çš„è¾¹ç¼˜æ„ŸçŸ¥å¤šåˆ†æ”¯ç½‘ç»œï¼Œé€šè¿‡å››ä¸ªå…³é”®åˆ›æ–°å®ç°äº†æ€§èƒ½ä¸æ•ˆç‡çš„å¹³è¡¡ï¼šï¼ˆ1ï¼‰è¶…å£°ä¸“ç”¨å¤šåˆ†æ”¯å¤„ç†ï¼Œå…·æœ‰ç”¨äºé™å™ªã€è¾¹ç¼˜å¢å¼ºå’Œå¯¹æ¯”åº¦æ”¹è¿›çš„ä¸“ç”¨æ¨¡å—ï¼›ï¼ˆ2ï¼‰è¾¹ç¼˜æ„ŸçŸ¥æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥æœ€å°çš„è®¡ç®—å¼€é”€å…³æ³¨è¾¹ç•Œä¿¡æ¯ï¼›ï¼ˆ3ï¼‰å…·æœ‰è‡ªé€‚åº”æƒé‡å­¦ä¹ çš„åˆ†å±‚ç‰¹å¾èšåˆï¼›ï¼ˆ4ï¼‰ç”¨äºæœ€ä½³åˆ†å‰²ç»†åŒ–çš„è¶…å£°æ„ŸçŸ¥è§£ç å™¨å¢å¼ºã€‚USEANetå»ºç«‹åœ¨è¶…è½»é‡çº§PVT-B0ä¸»å¹²ç½‘ç»œä¸Šï¼Œåœ¨äº”ä¸ªè¶…å£°æ•°æ®é›†ä¸Šçš„æ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒæ—¶ä»…ä½¿ç”¨364ä¸‡ä¸ªå‚æ•°å’Œ0.79GFLOPsã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨BUSIæ•°æ®é›†ä¸Šè¾¾åˆ°äº†67.01çš„IoUåˆ†å‰²ç²¾åº¦ï¼Œç›¸å¯¹äºä¼ ç»Ÿæ–¹æ³•æœ‰äº†å¾ˆå¤§çš„æ”¹è¿›ï¼ŒåŒæ—¶ä¿æŒäº†é€‚åˆå®æ—¶ä¸´åºŠåº”ç”¨çš„å‡ºè‰²è®¡ç®—æ•ˆç‡ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/chouheiwa/USEANet%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/chouheiwa/USEANetæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.08860v1">PDF</a> This work has been submitted to the IEEE for possible publication</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºUSEANetçš„è¶…å£°ä¸“ç”¨è¾¹ç¼˜æ„ŸçŸ¥å¤šåˆ†æ”¯ç½‘ç»œï¼Œç”¨äºè§£å†³è¶…å£°å›¾åƒåˆ†å‰²é¢ä¸´çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ–‘ç‚¹å™ªå£°ã€ä½å¯¹æ¯”åº¦å’Œæ¨¡ç³Šè¾¹ç•Œã€‚è¯¥ç½‘ç»œé€šè¿‡å››ä¸ªå…³é”®åˆ›æ–°å®ç°äº†æ€§èƒ½ä¸æ•ˆç‡çš„å¹³è¡¡ï¼ŒåŒ…æ‹¬è¶…å£°ä¸“ç”¨å¤šåˆ†æ”¯å¤„ç†ã€è¾¹ç¼˜æ„ŸçŸ¥æ³¨æ„åŠ›æœºåˆ¶ã€åˆ†å±‚ç‰¹å¾èšåˆå’Œè¶…å£°æ„ŸçŸ¥è§£ç å™¨å¢å¼ºã€‚USEANetåœ¨äº”ä¸ªè¶…å£°æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå‚æ•°ä»…ä¸º3.64Mï¼Œæµ®ç‚¹è¿ç®—é‡ä¸º0.79Gï¼ŒåŒæ—¶ä¿æŒäº†å‡ºè‰²çš„è®¡ç®—æ•ˆç‡ï¼Œé€‚åˆå®æ—¶ä¸´åºŠåº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>USEANetæ˜¯ä¸€ç§ä¸“é—¨é’ˆå¯¹è¶…å£°å›¾åƒåˆ†å‰²è®¾è®¡çš„ç½‘ç»œã€‚</li>
<li>è¯¥ç½‘ç»œé€šè¿‡å››ä¸ªå…³é”®åˆ›æ–°å®ç°æ€§èƒ½ä¸æ•ˆç‡çš„å¹³è¡¡ã€‚</li>
<li>USEANetåŒ…æ‹¬è¶…å£°ä¸“ç”¨å¤šåˆ†æ”¯å¤„ç†ï¼Œç”¨äºå‡å°‘å™ªå£°ã€å¢å¼ºè¾¹ç¼˜å’Œæé«˜å¯¹æ¯”åº¦ã€‚</li>
<li>ç½‘ç»œé‡‡ç”¨è¾¹ç¼˜æ„ŸçŸ¥æ³¨æ„åŠ›æœºåˆ¶ï¼Œé‡ç‚¹å…³æ³¨è¾¹ç•Œä¿¡æ¯ï¼ŒåŒæ—¶å‡å°‘è®¡ç®—å¼€é”€ã€‚</li>
<li>USEANeté€šè¿‡åˆ†å±‚ç‰¹å¾èšåˆå’Œè‡ªé€‚åº”æƒé‡å­¦ä¹ æ¥æé«˜æ€§èƒ½ã€‚</li>
<li>è¶…å£°æ„ŸçŸ¥è§£ç å™¨å¢å¼ºå¯å®ç°æ›´ç²¾ç¡®çš„åˆ†å‰²ç»†åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.08860">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2509.08860v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2509.08860v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2509.08860v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2509.08860v1/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2509.08860v1/page_3_2.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Expert-Guided-Explainable-Few-Shot-Learning-for-Medical-Image-Diagnosis"><a href="#Expert-Guided-Explainable-Few-Shot-Learning-for-Medical-Image-Diagnosis" class="headerlink" title="Expert-Guided Explainable Few-Shot Learning for Medical Image Diagnosis"></a>Expert-Guided Explainable Few-Shot Learning for Medical Image Diagnosis</h2><p><strong>Authors:Ifrat Ikhtear Uddin, Longwei Wang, KC Santosh</strong></p>
<p>Medical image analysis often faces significant challenges due to limited expert-annotated data, hindering both model generalization and clinical adoption. We propose an expert-guided explainable few-shot learning framework that integrates radiologist-provided regions of interest (ROIs) into model training to simultaneously enhance classification performance and interpretability. Leveraging Grad-CAM for spatial attention supervision, we introduce an explanation loss based on Dice similarity to align model attention with diagnostically relevant regions during training. This explanation loss is jointly optimized with a standard prototypical network objective, encouraging the model to focus on clinically meaningful features even under limited data conditions. We evaluate our framework on two distinct datasets: BraTS (MRI) and VinDr-CXR (Chest X-ray), achieving significant accuracy improvements from 77.09% to 83.61% on BraTS and from 54.33% to 73.29% on VinDr-CXR compared to non-guided models. Grad-CAM visualizations further confirm that expert-guided training consistently aligns attention with diagnostic regions, improving both predictive reliability and clinical trustworthiness. Our findings demonstrate the effectiveness of incorporating expert-guided attention supervision to bridge the gap between performance and interpretability in few-shot medical image diagnosis. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†æå¸¸å¸¸é¢ä¸´ç”±äºä¸“å®¶æ ‡æ³¨æ•°æ®æœ‰é™è€Œå¯¼è‡´çš„é‡å¤§æŒ‘æˆ˜ï¼Œè¿™ä¸ä»…é˜»ç¢äº†æ¨¡å‹çš„æ³›åŒ–ï¼Œä¹Ÿé˜»ç¢äº†ä¸´åºŠé‡‡ç”¨ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸“å®¶å¼•å¯¼çš„å¯è§£é‡Šçš„å°‘é‡å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†æ”¾å°„ç§‘åŒ»ç”Ÿæä¾›çš„æ„Ÿå…´è¶£åŒºåŸŸï¼ˆROIï¼‰é›†æˆåˆ°æ¨¡å‹è®­ç»ƒä¸­ï¼Œä»¥åŒæ—¶æé«˜åˆ†ç±»æ€§èƒ½å’Œè§£é‡Šæ€§ã€‚æˆ‘ä»¬åˆ©ç”¨Grad-CAMè¿›è¡Œç©ºé—´æ³¨æ„åŠ›ç›‘ç£ï¼Œå¹¶å¼•å…¥åŸºäºDiceç›¸ä¼¼åº¦çš„è§£é‡ŠæŸå¤±ï¼Œä»¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿æ¨¡å‹æ³¨æ„åŠ›ä¸è¯Šæ–­ç›¸å…³çš„åŒºåŸŸå¯¹é½ã€‚è¿™ç§è§£é‡ŠæŸå¤±ä¸æ ‡å‡†åŸå‹ç½‘ç»œç›®æ ‡è”åˆä¼˜åŒ–ï¼Œé¼“åŠ±æ¨¡å‹å³ä½¿åœ¨æ•°æ®æœ‰é™çš„æƒ…å†µä¸‹ä¹Ÿå…³æ³¨ä¸´åºŠä¸Šé‡è¦çš„ç‰¹å¾ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªä¸åŒçš„æ•°æ®é›†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ¡†æ¶ï¼šBraTSï¼ˆMRIï¼‰å’ŒVinDr-CXRï¼ˆèƒ¸éƒ¨Xå°„çº¿ï¼‰ã€‚ä¸æ— å¼•å¯¼æ¨¡å‹ç›¸æ¯”ï¼Œæˆ‘ä»¬åœ¨BraTSä¸Šçš„å‡†ç¡®ç‡ä»77.09%æé«˜åˆ°83.61%ï¼Œåœ¨VinDr-CXRä¸Šçš„å‡†ç¡®ç‡ä»54.33%æé«˜åˆ°73.29%ã€‚Grad-CAMå¯è§†åŒ–è¿›ä¸€æ­¥è¯å®ï¼Œä¸“å®¶å¼•å¯¼çš„è®­ç»ƒèƒ½ä½¿æ³¨æ„åŠ›å§‹ç»ˆä¸è¯Šæ–­åŒºåŸŸå¯¹é½ï¼Œæé«˜äº†é¢„æµ‹å¯é æ€§å’Œä¸´åºŠå¯ä¿¡åº¦ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåœ¨å°‘é‡åŒ»å­¦å›¾åƒè¯Šæ–­ä¸­èå…¥ä¸“å®¶å¼•å¯¼çš„æ³¨æ„åŠ›ç›‘ç£ï¼Œèƒ½æœ‰æ•ˆç¼©å°æ€§èƒ½ä¸è§£é‡Šæ€§ä¹‹é—´çš„å·®è·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.08007v2">PDF</a> Accepted for publication in the proceedings of MICCAI Workshop on   Data Engineering in Medical Imaging 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§ä¸“å®¶å¼•å¯¼çš„å¯è§£é‡Šçš„å°‘æ ·æœ¬å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†æ”¾å°„ç§‘åŒ»ç”Ÿæä¾›çš„æ„Ÿå…´è¶£åŒºåŸŸï¼ˆROIsï¼‰é›†æˆåˆ°æ¨¡å‹è®­ç»ƒä¸­ï¼ŒåŒæ—¶æé«˜åˆ†ç±»æ€§èƒ½å’Œè§£é‡Šæ€§ã€‚é€šè¿‡åˆ©ç”¨Grad-CAMè¿›è¡Œç©ºé—´æ³¨æ„åŠ›ç›‘ç£ï¼Œå¼•å…¥åŸºäºDiceç›¸ä¼¼åº¦çš„è§£é‡ŠæŸå¤±ï¼Œä½¿æ¨¡å‹æ³¨æ„åŠ›ä¸è¯Šæ–­ç›¸å…³åŒºåŸŸå¯¹é½ã€‚è¯¥è§£é‡ŠæŸå¤±ä¸æ ‡å‡†åŸå‹ç½‘ç»œç›®æ ‡è”åˆä¼˜åŒ–ï¼Œé¼“åŠ±æ¨¡å‹åœ¨æœ‰é™æ•°æ®æ¡ä»¶ä¸‹å…³æ³¨ä¸´åºŠæœ‰æ„ä¹‰çš„ç‰¹å¾ã€‚åœ¨BraTSå’ŒVinDr-CXRä¸¤ä¸ªæ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œä¸æ— å¼•å¯¼æ¨¡å‹ç›¸æ¯”ï¼Œè¯¥æ¡†æ¶çš„å‡†ç¡®ç‡åˆ†åˆ«ä»77.09%æé«˜åˆ°83.61%å’Œä»54.33%æé«˜åˆ°73.29%ã€‚Grad-CAMå¯è§†åŒ–è¿›ä¸€æ­¥è¯å®ï¼Œä¸“å®¶å¼•å¯¼çš„è®­ç»ƒèƒ½ä½¿æ¨¡å‹æ³¨æ„åŠ›å§‹ç»ˆä¸è¯Šæ–­åŒºåŸŸå¯¹é½ï¼Œæé«˜é¢„æµ‹å¯é æ€§å’Œä¸´åºŠå¯ä¿¡åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†æé¢ä¸´ä¸“å®¶æ ‡æ³¨æ•°æ®æœ‰é™çš„æŒ‘æˆ˜ï¼Œå½±å“æ¨¡å‹é€šç”¨åŒ–å’Œä¸´åºŠé‡‡ç”¨ã€‚</li>
<li>æå‡ºä¸€ç§ä¸“å®¶å¼•å¯¼çš„å¯è§£é‡Šçš„å°‘æ ·æœ¬å­¦ä¹ æ¡†æ¶ï¼Œç»“åˆæ”¾å°„ç§‘åŒ»ç”Ÿæä¾›çš„æ„Ÿå…´è¶£åŒºåŸŸï¼ˆROIsï¼‰è¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚</li>
<li>åˆ©ç”¨Grad-CAMè¿›è¡Œç©ºé—´æ³¨æ„åŠ›ç›‘ç£ï¼Œå¼•å…¥åŸºäºDiceç›¸ä¼¼åº¦çš„è§£é‡ŠæŸå¤±ï¼Œæé«˜æ¨¡å‹è¯Šæ–­ç›¸å…³åŒºåŸŸçš„æ³¨æ„åŠ›ã€‚</li>
<li>è§£é‡ŠæŸå¤±ä¸æ ‡å‡†åŸå‹ç½‘ç»œç›®æ ‡è”åˆä¼˜åŒ–ï¼Œæé«˜æ¨¡å‹åœ¨æœ‰é™æ•°æ®ä¸‹çš„ä¸´åºŠæœ‰æ„ä¹‰ç‰¹å¾å…³æ³¨èƒ½åŠ›ã€‚</li>
<li>åœ¨BraTSå’ŒVinDr-CXRæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ¡†æ¶æ˜¾è‘—æé«˜åˆ†ç±»å‡†ç¡®ç‡ã€‚</li>
<li>Grad-CAMå¯è§†åŒ–è¯å®ä¸“å®¶å¼•å¯¼çš„è®­ç»ƒèƒ½æé«˜æ¨¡å‹é¢„æµ‹å¯é æ€§å’Œä¸´åºŠå¯ä¿¡åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.08007">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2509.08007v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2509.08007v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Developing-a-Framework-to-Simulate-Quantitative-Ultrasound-Flow-and-Tissue-Motion-for-Ultrafast-Doppler-Ultrasound"><a href="#Developing-a-Framework-to-Simulate-Quantitative-Ultrasound-Flow-and-Tissue-Motion-for-Ultrafast-Doppler-Ultrasound" class="headerlink" title="Developing a Framework to Simulate Quantitative Ultrasound Flow and   Tissue Motion for Ultrafast Doppler Ultrasound"></a>Developing a Framework to Simulate Quantitative Ultrasound Flow and   Tissue Motion for Ultrafast Doppler Ultrasound</h2><p><strong>Authors:Qiang Fu, Changhui Li</strong></p>
<p>Ultrafast power Doppler imaging (uPDI) has achieved substantial progress and emerged as a key modality for both research and clinical applications. However, existing simulation tools are insufficient for generating three-dimensional (3D), quantitatively accurate flow fields with tissue motion that closely approximate in vivo conditions. In this study, we present an open-source framework, termed \emph{3D-Fully Quantitative Flow} (3D-FQFlow), designed to provide quantitative modeling of 3D vascular hemodynamics with physiologically realistic tissue motion for uPDI.The framework integrates a L-system-based vascular generator with hemodynamics modeling, a tissue motion simulator supporting user-defined or clinical-data-driven condition, an optimized ultrasound simulator, a GPU-accelerated image reconstruction module, and a quantitative analyzer (MSE&#x2F;PSNR&#x2F;SSIM).   We demonstrate the workflow and performance of 3D-FQFlow using both synthetic vascular structures and clinical datasets. This framework provides ground-truth simulation models to support the development, validation, and benchmarking of uPDI techniques. The complete source code is freely available online at <a target="_blank" rel="noopener" href="https://github.com/FortuneOU/3D-FQFlow">https://github.com/FortuneOU/3D-FQFlow</a>. </p>
<blockquote>
<p>æé€ŸåŠŸç‡å¤šæ™®å‹’æˆåƒï¼ˆuPDIï¼‰å·²ç»å–å¾—äº†é‡è¦è¿›å±•ï¼Œå¹¶å·²æˆä¸ºç ”ç©¶å’Œä¸´åºŠåº”ç”¨çš„å…³é”®æŠ€æœ¯ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ä»¿çœŸå·¥å…·åœ¨ç”Ÿæˆå…·æœ‰ç»„ç»‡è¿åŠ¨çš„å®šé‡ä¸‰ç»´ï¼ˆ3Dï¼‰è¡€æµåœºæ–¹é¢ä»å­˜åœ¨ä¸è¶³ï¼Œæ— æ³•ç´§å¯†è¿‘ä¼¼ä½“å†…æ¡ä»¶ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¼€æºæ¡†æ¶ï¼Œåä¸ºâ€œä¸‰ç»´å…¨å®šé‡è¡€æµâ€ï¼ˆ3D-FQFlowï¼‰ï¼Œæ—¨åœ¨ä¸ºuPDIæä¾›å…·æœ‰ç”Ÿç†ç°å®æ€§ç»„ç»‡è¿åŠ¨çš„å®šé‡ä¸‰ç»´è¡€æµåŠ¨åŠ›å­¦å»ºæ¨¡ã€‚è¯¥æ¡†æ¶é›†æˆäº†åŸºäºLç³»ç»Ÿçš„è¡€ç®¡ç”Ÿæˆå™¨ä¸è¡€æµåŠ¨åŠ›å­¦å»ºæ¨¡ã€æ”¯æŒç”¨æˆ·å®šä¹‰æˆ–ä¸´åºŠæ•°æ®é©±åŠ¨æ¡ä»¶çš„ç»„ç»‡è¿åŠ¨æ¨¡æ‹Ÿå™¨ã€ä¼˜åŒ–åçš„è¶…å£°æ¨¡æ‹Ÿå™¨ã€GPUåŠ é€Ÿçš„å›¾åƒé‡å»ºæ¨¡å—ä»¥åŠå®šé‡åˆ†æä»ªï¼ˆMSE&#x2F;PSNR&#x2F;SSIMï¼‰ã€‚æˆ‘ä»¬å±•ç¤ºäº†ä½¿ç”¨åˆæˆè¡€ç®¡ç»“æ„å’Œä¸´åºŠæ•°æ®é›†çš„å·¥ä½œæµç¨‹å’Œæ€§èƒ½ã€‚è¯¥æ¡†æ¶æä¾›äº†åŸºå‡†ä»¿çœŸæ¨¡å‹ï¼Œä»¥æ”¯æŒuPDIæŠ€æœ¯çš„å¼€å‘ã€éªŒè¯å’ŒåŸºå‡†æµ‹è¯•ã€‚å®Œæ•´çš„æºä»£ç å¯å…è´¹åœ¨çº¿è·å–ï¼Œåœ°å€æ˜¯ï¼š<a target="_blank" rel="noopener" href="https://github.com/FortuneOU/3D-FQFlow%E3%80%82">https://github.com/FortuneOU/3D-FQFlowã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.05464v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€é¡¹åä¸ºâ€œä¸‰ç»´å…¨å®šé‡æµâ€ï¼ˆ3D-FQFlowï¼‰çš„å¼€æºæ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ—¨åœ¨æä¾›å®šé‡å»ºæ¨¡ä¸‰ç»´è¡€ç®¡è¡€æµåŠ¨åŠ›å­¦çš„æ–¹æ³•ï¼Œå¹¶æ¨¡æ‹Ÿç”Ÿç†ç°å®æ¡ä»¶ä¸‹çš„ç»„ç»‡è¿åŠ¨ï¼Œä»¥æ”¯æŒè¶…å¿«é€ŸåŠŸç‡å¤šæ™®å‹’æˆåƒï¼ˆuPDIï¼‰ã€‚è¯¥æ¡†æ¶é›†æˆäº†åŸºäºLç³»ç»Ÿçš„è¡€ç®¡ç”Ÿæˆå™¨ã€è¡€æµåŠ¨åŠ›å­¦å»ºæ¨¡ã€æ”¯æŒç”¨æˆ·å®šä¹‰æˆ–ä¸´åºŠæ•°æ®é©±åŠ¨æ¡ä»¶çš„ç»„ç»‡è¿åŠ¨æ¨¡æ‹Ÿå™¨ã€ä¼˜åŒ–çš„è¶…å£°æ¨¡æ‹Ÿå™¨ã€GPUåŠ é€Ÿçš„å›¾åƒé‡å»ºæ¨¡å—å’Œå®šé‡åˆ†æä»ªã€‚é€šè¿‡åˆæˆè¡€ç®¡ç»“æ„å’Œä¸´åºŠæ•°æ®é›†ï¼Œå±•ç¤ºäº†è¯¥æ¡†æ¶çš„å·¥ä½œæµç¨‹å’Œæ€§èƒ½ã€‚å®ƒä¸ºuPDIæŠ€æœ¯çš„å‘å±•ã€éªŒè¯å’ŒåŸºå‡†æµ‹è¯•æä¾›äº†åœ°é¢çœŸå®æ¨¡æ‹Ÿæ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†åä¸ºâ€œä¸‰ç»´å…¨å®šé‡æµâ€ï¼ˆ3D-FQFlowï¼‰çš„å¼€æºæ¡†æ¶ã€‚</li>
<li>è¯¥æ¡†æ¶æ—¨åœ¨æä¾›å®šé‡å»ºæ¨¡ä¸‰ç»´è¡€ç®¡è¡€æµåŠ¨åŠ›å­¦çš„æ–¹æ³•ã€‚</li>
<li>è¯¥æ¡†æ¶å¯ä»¥æ¨¡æ‹Ÿç”Ÿç†ç°å®æ¡ä»¶ä¸‹çš„ç»„ç»‡è¿åŠ¨ã€‚</li>
<li>è¯¥æ¡†æ¶é›†æˆäº†å¤šä¸ªæ¨¡å—ï¼ŒåŒ…æ‹¬è¡€ç®¡ç”Ÿæˆå™¨ã€è¡€æµåŠ¨åŠ›å­¦å»ºæ¨¡ç­‰ã€‚</li>
<li>è¯¥æ¡†æ¶æ”¯æŒè¶…å¿«é€ŸåŠŸç‡å¤šæ™®å‹’æˆåƒï¼ˆuPDIï¼‰ã€‚</li>
<li>é€šè¿‡åˆæˆè¡€ç®¡ç»“æ„å’Œä¸´åºŠæ•°æ®é›†ï¼Œå±•ç¤ºäº†è¯¥æ¡†æ¶çš„å·¥ä½œæµç¨‹å’Œæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.05464">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2509.05464v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2509.05464v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2509.05464v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2509.05464v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2509.05464v2/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2509.05464v2/page_5_1.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Learning-functions-through-Diffusion-Maps"><a href="#Learning-functions-through-Diffusion-Maps" class="headerlink" title="Learning functions through Diffusion Maps"></a>Learning functions through Diffusion Maps</h2><p><strong>Authors:Alvaro Almeida Gomez</strong></p>
<p>We propose a data-driven method for approximating real-valued functions on smooth manifolds, building on the Diffusion Maps framework under the manifold hypothesis. Given pointwise evaluations of a function, the method constructs a smooth extension to the ambient space by exploiting diffusion geometry and its connection to the heat equation and the Laplace-Beltrami operator.   To address the computational challenges of high-dimensional data, we introduce a dimensionality reduction strategy based on the low-rank structure of the distance matrix, revealed via singular value decomposition (SVD). In addition, we develop an online updating mechanism that enables efficient incorporation of new data, thereby improving scalability and reducing computational cost.   Numerical experiments, including applications to sparse CT reconstruction, demonstrate that the proposed methodology outperforms classical feedforward neural networks and interpolation methods in terms of both accuracy and efficiency. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ•°æ®é©±åŠ¨çš„æ–¹æ³•ï¼Œç”¨äºåœ¨å¹³æ»‘æµå½¢ä¸Šé€¼è¿‘å®å€¼å‡½æ•°ã€‚è¯¥æ–¹æ³•å»ºç«‹åœ¨æµå½¢å‡è®¾ä¸‹çš„æ‰©æ•£æ˜ å°„æ¡†æ¶ä¹‹ä¸Šã€‚ç»™å®šå‡½æ•°çš„ç‚¹æ€è¯„ä¼°ï¼Œè¯¥æ–¹æ³•é€šè¿‡åˆ©ç”¨æ‰©æ•£å‡ ä½•åŠå…¶ä¸çƒ­æ–¹ç¨‹å’ŒLaplace-Beltramiç®—å­çš„è”ç³»ï¼Œåœ¨ç¯å¢ƒç©ºé—´ä¸­æ„å»ºå¹³æ»‘æ‰©å±•ã€‚ä¸ºäº†è§£å†³é«˜ç»´æ•°æ®çš„è®¡ç®—æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè·ç¦»çŸ©é˜µä½ç§©ç»“æ„çš„é™ç»´ç­–ç•¥ï¼Œè¯¥ç­–ç•¥é€šè¿‡å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰æ­ç¤ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§åœ¨çº¿æ›´æ–°æœºåˆ¶ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°èå…¥æ–°æ•°æ®ï¼Œä»è€Œæé«˜å¯æ‰©å±•æ€§å¹¶é™ä½è®¡ç®—æˆæœ¬ã€‚æ•°å€¼å®éªŒè¡¨æ˜ï¼ŒåŒ…æ‹¬åœ¨ç¨€ç–CTé‡å»ºä¸­çš„åº”ç”¨åœ¨å†…ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡æ–¹é¢éƒ½ä¼˜äºç»å…¸çš„å‰é¦ˆç¥ç»ç½‘ç»œå’Œæ’å€¼æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.03758v2">PDF</a> Comments are welcome</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºæ•°æ®é©±åŠ¨çš„æ–¹æ³•ï¼Œç”¨äºåœ¨å¹³æ»‘æµå½¢ä¸Šè¿‘ä¼¼å®å€¼å‡½æ•°ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ‰©æ•£æ˜ å°„æ¡†æ¶å’Œæµå½¢å‡è®¾ï¼Œé€šè¿‡æ‰©æ•£å‡ ä½•åŠå…¶ä¸çƒ­æ–¹ç¨‹å’ŒLaplace-Beltramiç®—å­çš„è”ç³»ï¼Œåœ¨ç¯å¢ƒç©ºé—´ä¸­æ„å»ºå¹³æ»‘æ‰©å±•ã€‚é’ˆå¯¹é«˜ç»´æ•°æ®çš„è®¡ç®—æŒ‘æˆ˜ï¼Œæˆ‘ä»¬é‡‡ç”¨åŸºäºè·ç¦»çŸ©é˜µä½ç§©ç»“æ„çš„é™ç»´ç­–ç•¥ï¼Œé€šè¿‡å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰æ­ç¤ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§åœ¨çº¿æ›´æ–°æœºåˆ¶ï¼Œèƒ½å¤Ÿé«˜æ•ˆæ•´åˆæ–°æ•°æ®ï¼Œä»è€Œæé«˜å¯æ‰©å±•æ€§å¹¶é™ä½è®¡ç®—æˆæœ¬ã€‚æ•°å€¼å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç²¾åº¦å’Œæ•ˆç‡æ–¹é¢éƒ½ä¼˜äºä¼ ç»Ÿçš„å‰é¦ˆç¥ç»ç½‘ç»œå’Œæ’å€¼æ–¹æ³•ï¼ŒåŒ…æ‹¬åœ¨ç¨€ç–CTé‡å»ºä¸­çš„åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºæ•°æ®é©±åŠ¨çš„æ–¹æ³•ï¼Œç”¨äºåœ¨å¹³æ»‘æµå½¢ä¸Šè¿‘ä¼¼å®å€¼å‡½æ•°ã€‚</li>
<li>æ–¹æ³•åˆ©ç”¨æ‰©æ•£æ˜ å°„æ¡†æ¶å’Œæµå½¢å‡è®¾ï¼Œæ„å»ºå¹³æ»‘æ‰©å±•è‡³ç¯å¢ƒç©ºé—´ã€‚</li>
<li>é€šè¿‡æ‰©æ•£å‡ ä½•å’Œä¸çƒ­æ–¹ç¨‹ã€Laplace-Beltramiç®—å­çš„è”ç³»å®ç°æ–¹æ³•ã€‚</li>
<li>é‡‡ç”¨åŸºäºè·ç¦»çŸ©é˜µä½ç§©ç»“æ„çš„é™ç»´ç­–ç•¥ï¼Œé€šè¿‡å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰å®ç°é«˜ç»´æ•°æ®çš„å¤„ç†ã€‚</li>
<li>å¼•å…¥åœ¨çº¿æ›´æ–°æœºåˆ¶ï¼Œé«˜æ•ˆæ•´åˆæ–°æ•°æ®ï¼Œæé«˜æ–¹æ³•å¯æ‰©å±•æ€§å¹¶é™ä½è®¡ç®—æˆæœ¬ã€‚</li>
<li>æ•°å€¼å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç²¾åº¦å’Œæ•ˆç‡æ–¹é¢ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.03758">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2509.03758v2/page_0_0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Drawing2CAD-Sequence-to-Sequence-Learning-for-CAD-Generation-from-Vector-Drawings"><a href="#Drawing2CAD-Sequence-to-Sequence-Learning-for-CAD-Generation-from-Vector-Drawings" class="headerlink" title="Drawing2CAD: Sequence-to-Sequence Learning for CAD Generation from   Vector Drawings"></a>Drawing2CAD: Sequence-to-Sequence Learning for CAD Generation from   Vector Drawings</h2><p><strong>Authors:Feiwei Qin, Shichao Lu, Junhao Hou, Changmiao Wang, Meie Fang, Ligang Liu</strong></p>
<p>Computer-Aided Design (CAD) generative modeling is driving significant innovations across industrial applications. Recent works have shown remarkable progress in creating solid models from various inputs such as point clouds, meshes, and text descriptions. However, these methods fundamentally diverge from traditional industrial workflows that begin with 2D engineering drawings. The automatic generation of parametric CAD models from these 2D vector drawings remains underexplored despite being a critical step in engineering design. To address this gap, our key insight is to reframe CAD generation as a sequence-to-sequence learning problem where vector drawing primitives directly inform the generation of parametric CAD operations, preserving geometric precision and design intent throughout the transformation process. We propose Drawing2CAD, a framework with three key technical components: a network-friendly vector primitive representation that preserves precise geometric information, a dual-decoder transformer architecture that decouples command type and parameter generation while maintaining precise correspondence, and a soft target distribution loss function accommodating inherent flexibility in CAD parameters. To train and evaluate Drawing2CAD, we create CAD-VGDrawing, a dataset of paired engineering drawings and parametric CAD models, and conduct thorough experiments to demonstrate the effectiveness of our method. Code and dataset are available at <a target="_blank" rel="noopener" href="https://github.com/lllssc/Drawing2CAD">https://github.com/lllssc/Drawing2CAD</a>. </p>
<blockquote>
<p>è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰ç”Ÿæˆå»ºæ¨¡æ­£åœ¨æ¨åŠ¨å·¥ä¸šåº”ç”¨çš„é‡å¤§åˆ›æ–°ã€‚è¿‘æœŸçš„ç ”ç©¶å·¥ä½œæ˜¾ç¤ºï¼Œä»ç‚¹äº‘ã€ç½‘æ ¼å’Œæ–‡æœ¬æè¿°ç­‰å„ç§è¾“å…¥åˆ›å»ºå®ä½“æ¨¡å‹å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ä»æ ¹æœ¬ä¸Šåç¦»äº†å§‹äºäºŒç»´å·¥ç¨‹å›¾çº¸çš„ä¼ ç»Ÿå·¥ä¸šå·¥ä½œæµç¨‹ã€‚å°½ç®¡ä»äºŒç»´çŸ¢é‡å›¾å½¢è‡ªåŠ¨ç”Ÿæˆå‚æ•°åŒ–CADæ¨¡å‹æ˜¯å·¥ç¨‹è®¾è®¡ä¸­çš„å…³é”®æ­¥éª¤ï¼Œä½†è¿™ä¸€é¢†åŸŸçš„æ¢ç´¢ä»ç„¶ä¸è¶³ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬çš„ä¸»è¦è§è§£æ˜¯å°†CADç”Ÿæˆé‡æ–°æ„å»ºä¸ºä¸€ä¸ªåºåˆ—åˆ°åºåˆ—çš„å­¦ä¹ é—®é¢˜ï¼Œå…¶ä¸­çŸ¢é‡ç»˜å›¾åŸè¯­ç›´æ¥ä¸ºå‚æ•°åŒ–CADæ“ä½œç”Ÿæˆæä¾›ä¿¡æ¯ï¼Œå¹¶åœ¨æ•´ä¸ªè½¬æ¢è¿‡ç¨‹ä¸­ä¿ç•™å‡ ä½•ç²¾åº¦å’Œè®¾è®¡æ„å›¾ã€‚æˆ‘ä»¬æå‡ºäº†Drawing2CADæ¡†æ¶ï¼ŒåŒ…å«ä¸‰ä¸ªå…³é”®æŠ€æœ¯ç»„ä»¶ï¼šå‹å¥½çš„ç½‘ç»œçŸ¢é‡åŸå§‹è¡¨ç¤ºï¼Œä¿ç•™ç²¾ç¡®çš„å‡ ä½•ä¿¡æ¯ï¼›åŒè§£ç å™¨è½¬æ¢å™¨æ¶æ„ï¼Œåœ¨ä¿æŒç²¾ç¡®å¯¹åº”çš„åŒæ—¶è§£è€¦å‘½ä»¤ç±»å‹å’Œå‚æ•°ç”Ÿæˆï¼›ä»¥åŠé€‚åº”CADå‚æ•°å›ºæœ‰çµæ´»æ€§çš„è½¯ç›®æ ‡åˆ†å¸ƒæŸå¤±å‡½æ•°ã€‚ä¸ºäº†è®­ç»ƒå’Œè¯„ä¼°Drawing2CADï¼Œæˆ‘ä»¬åˆ›å»ºäº†CAD-VGDrawingæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«é…å¯¹å·¥ç¨‹å›¾çº¸å’Œå‚æ•°åŒ–CADæ¨¡å‹ï¼Œå¹¶è¿›è¡Œäº†ä¸€ç³»åˆ—å®éªŒæ¥è¯æ˜æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/lllssc/Drawing2CAD">https://github.com/lllssc/Drawing2CAD</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.18733v5">PDF</a> Accepted to ACM MM 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºè®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰çš„ç”Ÿæˆå»ºæ¨¡æ­£åœ¨æ¨åŠ¨å·¥ä¸šåº”ç”¨çš„é‡å¤§åˆ›æ–°ã€‚æœ€æ–°ç ”ç©¶å·²ä»ç‚¹äº‘ã€ç½‘æ ¼å’Œæ–‡æœ¬æè¿°ç­‰è¾“å…¥åˆ›å»ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„å®ä½“æ¨¡å‹ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ä¸ä¼ ç»Ÿå·¥ä¸šå·¥ä½œæµç¨‹ç›¸æ‚–ï¼Œä¼ ç»Ÿæµç¨‹å§‹äºäºŒç»´å·¥ç¨‹å›¾çº¸ã€‚å°½ç®¡ä»äºŒç»´çŸ¢é‡å›¾çº¸è‡ªåŠ¨ç”Ÿæˆå‚æ•°åŒ–CADæ¨¡å‹æ˜¯å·¥ç¨‹è®¾è®¡ä¸­çš„å…³é”®æ­¥éª¤ï¼Œä½†ç›®å‰è¿™ä¸€é¢†åŸŸçš„è‡ªåŠ¨æ¢ç´¢ä»è¾ƒå°‘ã€‚ä¸ºè§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬çš„è§è§£æ˜¯å°†CADç”Ÿæˆé‡æ–°æ„å»ºä¸ºåºåˆ—åˆ°åºåˆ—çš„å­¦ä¹ é—®é¢˜ï¼Œå…¶ä¸­çŸ¢é‡ç»˜å›¾åŸè¯­ç›´æ¥ä¸ºå‚æ•°åŒ–CADæ“ä½œç”Ÿæˆæä¾›ä¿¡æ¯ï¼Œåœ¨è½¬æ¢è¿‡ç¨‹ä¸­ä¿ç•™å‡ ä½•ç²¾åº¦å’Œè®¾è®¡æ„å›¾ã€‚æˆ‘ä»¬æå‡ºDrawing2CADæ¡†æ¶ï¼ŒåŒ…å«ä¸‰ä¸ªå…³é”®æŠ€æœ¯ç»„ä»¶ï¼šå‹å¥½çš„ç½‘ç»œçŸ¢é‡åŸå§‹è¡¨ç¤ºï¼Œä¿ç•™ç²¾ç¡®çš„å‡ ä½•ä¿¡æ¯ï¼›åŒè§£ç å™¨è½¬æ¢å™¨æ¶æ„ï¼Œåœ¨ä¿æŒç²¾ç¡®å¯¹åº”çš„åŒæ—¶è§£è€¦å‘½ä»¤ç±»å‹å’Œå‚æ•°ç”Ÿæˆï¼›ä»¥åŠå®¹çº³CADå‚æ•°å›ºæœ‰çµæ´»æ€§çš„è½¯ç›®æ ‡åˆ†å¸ƒæŸå¤±å‡½æ•°ã€‚ä¸ºè®­ç»ƒå’Œè¯„ä¼°Drawing2CADï¼Œæˆ‘ä»¬åˆ›å»ºäº†CAD-VGDrawingæ•°æ®é›†ï¼ŒåŒ…å«é…å¯¹çš„å·¥ç¨‹å›¾çº¸å’Œå‚æ•°åŒ–CADæ¨¡å‹ï¼Œå¹¶é€šè¿‡å®éªŒè¯æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰ç”Ÿæˆå»ºæ¨¡æ­£åœ¨æ¨åŠ¨å·¥ä¸šåº”ç”¨çš„åˆ›æ–°ã€‚</li>
<li>æœ€æ–°æ–¹æ³•ä¸»è¦ä»ç‚¹äº‘ã€ç½‘æ ¼å’Œæ–‡æœ¬æè¿°åˆ›å»ºå®ä½“æ¨¡å‹ï¼Œä½†ä¼ ç»Ÿå·¥ä¸šæµç¨‹å§‹äºäºŒç»´å·¥ç¨‹å›¾çº¸ã€‚</li>
<li>ä»äºŒç»´çŸ¢é‡å›¾çº¸è‡ªåŠ¨ç”Ÿæˆå‚æ•°åŒ–CADæ¨¡å‹æ˜¯å·¥ç¨‹è®¾è®¡ä¸­çš„å…³é”®æ­¥éª¤ï¼Œä½†æ­¤é¢†åŸŸçš„è‡ªåŠ¨æ¢ç´¢è¾ƒå°‘ã€‚</li>
<li>Drawing2CADæ¡†æ¶å°†CADç”Ÿæˆé‡æ–°æ„å»ºä¸ºåºåˆ—åˆ°åºåˆ—çš„å­¦ä¹ é—®é¢˜ï¼Œåˆ©ç”¨çŸ¢é‡ç»˜å›¾åŸè¯­ç›´æ¥ç”Ÿæˆå‚æ•°åŒ–CADæ“ä½œã€‚</li>
<li>Drawing2CADæ¡†æ¶åŒ…å«ä¸‰ä¸ªå…³é”®æŠ€æœ¯ç»„ä»¶ï¼šç½‘ç»œçŸ¢é‡åŸå§‹è¡¨ç¤ºã€åŒè§£ç å™¨è½¬æ¢å™¨æ¶æ„å’Œè½¯ç›®æ ‡åˆ†å¸ƒæŸå¤±å‡½æ•°ã€‚</li>
<li>ä¸ºè®­ç»ƒå’Œè¯„ä¼°Drawing2CADï¼Œåˆ›å»ºäº†CAD-VGDrawingæ•°æ®é›†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.18733">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2508.18733v5/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2508.18733v5/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2508.18733v5/page_5_0.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Scaling-Artificial-Intelligence-for-Prostate-Cancer-Detection-on-MRI-towards-Organized-Screening-and-Primary-Diagnosis-in-a-Global-Multiethnic-Population-Study-Protocol"><a href="#Scaling-Artificial-Intelligence-for-Prostate-Cancer-Detection-on-MRI-towards-Organized-Screening-and-Primary-Diagnosis-in-a-Global-Multiethnic-Population-Study-Protocol" class="headerlink" title="Scaling Artificial Intelligence for Prostate Cancer Detection on MRI   towards Organized Screening and Primary Diagnosis in a Global, Multiethnic   Population (Study Protocol)"></a>Scaling Artificial Intelligence for Prostate Cancer Detection on MRI   towards Organized Screening and Primary Diagnosis in a Global, Multiethnic   Population (Study Protocol)</h2><p><strong>Authors:Anindo Saha, Joeran S. Bosma, Jasper J. Twilt, Alexander B. C. D. Ng, Aqua Asif, Kirti Magudia, Peder Larson, Qinglin Xie, Xiaodong Zhang, Chi Pham Minh, Samuel N. Gitau, Ivo G. Schoots, Martijn F. Boomsma, Renato Cuocolo, Nikolaos Papanikolaou, Daniele Regge, Derya Yakar, Mattijs Elschot, Jeroen Veltman, Baris Turkbey, Nancy A. Obuchowski, Jurgen J. FÃ¼tterer, Anwar R. Padhani, Hashim U. Ahmed, Tobias NordstrÃ¶m, Martin Eklund, Veeru Kasivisvanathan, Maarten de Rooij, Henkjan Huisman</strong></p>
<p>In this intercontinental, confirmatory study, we include a retrospective cohort of 22,481 MRI examinations (21,288 patients; 46 cities in 22 countries) to train and externally validate the PI-CAI-2B model, i.e., an efficient, next-generation iteration of the state-of-the-art AI system that was developed for detecting Gleason grade group $\geq$2 prostate cancer on MRI during the PI-CAI study. Of these examinations, 20,471 cases (19,278 patients; 26 cities in 14 countries) from two EU Horizon projects (ProCAncer-I, COMFORT) and 12 independent centers based in Europe, North America, Asia and Africa, are used for training and internal testing. Additionally, 2010 cases (2010 patients; 20 external cities in 12 countries) from population-based screening (STHLM3-MRI, IP1-PROSTAGRAM trials) and primary diagnostic settings (PRIME trial) based in Europe, North and South Americas, Asia and Australia, are used for external testing. Primary endpoint is the proportion of AI-based assessments in agreement with the standard of care diagnoses (i.e., clinical assessments made by expert uropathologists on histopathology, if available, or at least two expert urogenital radiologists in consensus; with access to patient history and peer consultation) in the detection of Gleason grade group $\geq$2 prostate cancer within the external testing cohorts. Our statistical analysis plan is prespecified with a hypothesis of diagnostic interchangeability to the standard of care at the PI-RADS $\geq$3 (primary diagnosis) or $\geq$4 (screening) cut-off, considering an absolute margin of 0.05 and reader estimates derived from the PI-CAI observer study (62 radiologists reading 400 cases). Secondary measures comprise the area under the receiver operating characteristic curve (AUROC) of the AI system stratified by imaging quality, patient age and patient ethnicity to identify underlying biases (if any). </p>
<blockquote>
<p>åœ¨è¿™é¡¹æ´²é™…ç¡®è®¤æ€§ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬çº³å…¥äº†22,481ä¾‹MRIæ£€æŸ¥ï¼ˆæ¶‰åŠæ¥è‡ª22ä¸ªå›½å®¶ä¸­çš„46åº§åŸå¸‚ã€æ€»è®¡æ‚£è€…æ•°é‡21,288åæ‚£è€…ï¼‰æ¥å¯¹PI-CAI-2Bæ¨¡å‹è¿›è¡Œè®­ç»ƒå’Œå¤–éƒ¨éªŒè¯ã€‚è¯¥æ¨¡å‹æ˜¯PI-CAIç ”ç©¶æœŸé—´é’ˆå¯¹æ£€æµ‹ Gleason â‰¥ åˆ†ç»„ç»„çš„å››çº§æˆ–ä»¥ä¸Šå‰åˆ—è…ºè‚¿ç˜¤å¼€åˆ›ä¸‹ä¸€ä»£æœ€æ–° AI ç³»ç»Ÿæ‰€æ„å»ºçš„å‡çº§ç‰ˆäº§å“ã€‚æ¥è‡ªä¸¤é¡¹æ¬§ç›Ÿåœ°å¹³çº¿è®¡åˆ’ï¼ˆProCAncer-I å’Œ COMFORT è®¡åˆ’ï¼‰å’Œå¦å¤–æ¥è‡ªæ¬§æ´²ã€åŒ—ç¾ã€äºšæ´²å’Œéæ´²çš„åäºŒå®¶ç‹¬ç«‹ä¸­å¿ƒçš„å…±è®¡ 20,471 ä¾‹ï¼ˆæ¶‰åŠæ¥è‡ªåå››ä¸ªå›½å®¶ä¸­çš„äºŒåå…­åº§åŸå¸‚ã€æ€»è®¡æ‚£è€…æ•°é‡ 19,278 åæ‚£è€…ï¼‰çš„æ¡ˆä¾‹ç”¨äºå†…éƒ¨æµ‹è¯•å’Œè®­ç»ƒã€‚å¦å¤–ï¼Œåœ¨æ¥è‡ªåŸºäºæ¬§æ´²çš„åŸºäºäººå£çš„ç­›æŸ¥ç ”ç©¶ï¼ˆSTHLM3-MRI ä¸ IP1-PROSTAGRAM è¯•éªŒï¼‰ã€æ¶‰åŠæ¬§æ´²å’ŒåŒ—å—ç¾æ´²ã€äºšæ´²å’Œæ¾³å¤§åˆ©äºšçš„åŸºäºåˆçº§è¯Šæ–­è®¾ç½®ï¼ˆPRIME è¯•éªŒï¼‰çš„å…±è®¡ 2010 ä¾‹æ‚£è€…çš„æ”¯æŒä¸‹æ¥ï¼Œå¯¹æ­¤ AI æ¨¡å‹è¿›è¡Œäº†å¤–éƒ¨æµ‹è¯•ï¼Œæ•°æ®æä¾›è€…æœ‰ç»è¿‡é›†ä½“åˆ†æè®°å½•çš„ç´¯ç§¯ç»„ç»‡èµ·æ¥çš„è‡ªç„¶ç”Ÿæˆçš„å›¢é˜Ÿå‚è€ƒæµç¨‹ã€‚ï¼ˆä¸€èˆ¬è€Œè¨€æ‚£è€…å¹³å‡å¹´é¾„ä¸å›½é™…æµè¡Œè¶‹åŠ¿è¶‹äºä¸€è‡´çš„æ‚£è€…è§‚å¯Ÿæ‰€å¾—çš„ç‹¬æœ‰é€»è¾‘è¡¨å¾ä¼šåœ¨ä»‹å…¥å‡†ç¡®åº¦ä¸´åºŠè¯•éªŒä½œä¸ºå¸¸è§„æ£€æŸ¥è¯æ˜æœ‰æ„ä¹‰ç­‰ä¸åˆ©è§‚å¯Ÿä»¥åæ¨å¹¿åˆ°è¯„ä»·é¡¹ç›®ä¸Šä½¿ç”¨ï¼‰ï¼ˆThe independent supportin main question corresponds to proportion of the intelligent decisions being aligned with those by conventional diagnostic procedures on detection Gleason grade group $\geq$2 prostate cancer.ï¼‰ä¸»è¦è§‚å¯Ÿç»ˆç‚¹æ˜¯ä¸å¸¸è§„è¯Šæ–­ç»“æœä¸€è‡´çš„æ¯”ä¾‹è¿›è¡ŒæŒ‡æ ‡ç¡®è®¤æ€§ï¼ˆå½“ç—…ç†è¯Šæ–­ç”±ç»éªŒä¸°å¯Œçš„æ³Œå°¿ç—…ç†å­¦å®¶è¿›è¡Œç»„ç»‡ç—…ç†å­¦è¯„ä¼°æ—¶ï¼Œå¦‚æœæ— æ³•è·å–åˆ™è‡³å°‘ç”±ä¸¤åæ³Œå°¿ç”Ÿæ®–æ”¾å°„ä¸“å®¶å…±è¯†è¿›è¡Œï¼‰åœ¨è¯¥ç ”ç©¶çš„å®éªŒæ ·æœ¬é˜Ÿåˆ—ä¸­å¯¹ Gleason â‰¥ åˆ†ç»„ç»„çš„å››çº§æˆ–ä»¥ä¸Šå‰åˆ—è…ºè‚¿ç˜¤çš„æ£€æµ‹ç‡ä¸Šåº”ç”¨æ™ºèƒ½ç³»ç»Ÿåˆ¤æ–­ç»“æœçš„å‡†ç¡®æ€§å’Œç²¾ç¡®æ€§ä¸Šèµ·ç€æ— å¯æˆ–ç¼ºçš„ä½œç”¨ã€‚è¯¥è¯•éªŒç»è¿‡å‰æœŸåˆ¶å®šçš„é¢„è®¾è®¡åˆ’åŒæ—¶å®æ–½æ ·æœ¬ç—…ç†ç­‰çº§çš„ç›¸å¯¹å¯¹ç…§æ¨¡å‹ç•Œå®šä¸ºæœ‰èƒ½è§£é‡ŠæœªçŸ¥é—®é¢˜çš„æ‰€æœ‰å®éªŒè®¾è®¡ä¸æ“ä½œæµç¨‹é£é™©ç—…ç†æ£€æŸ¥ç»“æœç­‰æŒ‡æ ‡ç»æ¥è‡ªæ—©æœŸé¢å‘ç³»ç»Ÿè¦†ç›–èŒƒå›´æ¨å¯¼æ¶‰åŠçš„åˆ†ç±»å¹¶ä¸”ç»„æˆæ•ˆæœå‡½æ•°ä¸­æœ‰æœºåˆ¤æ–­äº’ä¸ºå…³é”®çš„å—ä¼—å•å…ƒç»§è€Œå‚è€ƒç”¨äºç—…ç†ç”Ÿç†è¯„åˆ†å‚è€ƒä¸Šé™å¹¶è¦†ç›–æœ€ç»ˆäº¤å‰æ¯”è¾ƒè¾…åŠ©é˜…è¯»è¯„ä»·å‘˜æ±‡æ€»å’Œå…±äº«çš„è§‚å¯Ÿå­¦ä¹ è¿›æ­¥æˆ–å¾…ä¼˜åŒ–çš„é—ç•™æ€§ç—‡çŠ¶æ¥æŒ‡å¯¼å…¶å·¥ä½œè§„ç¨‹çš„æ‰§è¡Œç»“æœåŒæ—¶æ¥å—å¤–éƒ¨ä¸“ä¸šåŒ»ç–—è´¨é‡ç›‘æ§ç»“æœç¡®è®¤æ˜¯å¦è¾¾åˆ°äº†æ£€æµ‹å‰åˆ—è…ºç™Œçš„ä¸´åºŠè¯Šæ–­é‡‘æ ‡å‡†å¯¹äºæé«˜æ‚£è€…æ²»ç–—æ•ˆæœæœ‰ç€é‡å¤§æ„ä¹‰ã€‚æˆ‘ä»¬çš„ç»Ÿè®¡è®¡åˆ’å‡è®¾æ˜¯åœ¨ä¸ PI-RADS â‰¥ åˆ†ç»„ç­‰çº§ä¸‰ï¼ˆåˆçº§è¯Šæ–­ï¼‰æˆ–å››ï¼ˆç­›æŸ¥ï¼‰çš„å¸¸è§„è¯Šæ–­æ ‡å‡†äº’æ¢æ€§ä¸Šï¼Œè€ƒè™‘äº† PI-CAI è§‚å¯Ÿè€…ç ”ç©¶ä¸­ç”±å…­åäºŒä½æ”¾å°„ç§‘åŒ»ç”Ÿå¯¹å››ç™¾ä¾‹æ¡ˆä¾‹å¾—å‡ºçš„è¯„ä¼°å€¼åŠä¸å·®å¼‚ 0.05 ç›¸å…³æ€§çš„èŒƒå›´è¯„ä¼°æˆ‘ä»¬å°†ä¼šåˆ†ææµ‹é‡å—è¯•ç³»ç»Ÿçš„è¡¨ç°å¹¶ä¸å…³æ³¨è´¨é‡æ§åˆ¶çš„å‘å±•ä¹‹å†åšä¸€ä¸ªæ˜ç¡®æ¢ç©¶è€…æ„è¯†çš„çµæ•åº¦èƒ½å¤Ÿè®¾å®šåº•çº¿æ¥è¾¾åˆ°è¿™æ¬¡æ“ä½œçš„ç­‰çº§å¦‚ä½•ç­‰ç»¼åˆè¯„ä¼°ã€‚æ¬¡è¦è¯„ä¼°æŒ‡æ ‡åŒ…æ‹¬é€šè¿‡åˆ†å±‚æˆåƒè´¨é‡ã€æ‚£è€…å¹´é¾„å’Œç§æ—è¯†åˆ«æ½œåœ¨åè§ï¼ˆå¦‚æœå­˜åœ¨ï¼‰æ¥æµ‹é‡ AI ç³»ç»Ÿçš„æ›²çº¿ä¸‹é¢ç§¯ï¼ˆAUROCï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03762v2">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶é‡‡ç”¨å¤šå›½é™…è·¨åœ°åŸŸçš„å¤§å‹é˜Ÿåˆ—ç ”ç©¶ï¼Œåˆ©ç”¨å…¨çƒä¸åŒåœ°åŒºçš„MRIæ£€æŸ¥æ•°æ®è®­ç»ƒå’ŒéªŒè¯æ–°ä¸€ä»£å‰åˆ—è…ºç™ŒAIè¯Šæ–­æ¨¡å‹PI-CAI-2Bã€‚è¯¥æ¨¡å‹æ—¨åœ¨æé«˜å‰åˆ—è…ºç™Œåˆ†çº§è¯Šæ–­çš„å‡†ç¡®åº¦ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹Glasonè¯„åˆ†â‰¥2çš„å‰åˆ—è…ºç™Œæ£€æµ‹ã€‚ç ”ç©¶ä¸­åˆ©ç”¨å¤šæ¥æºæ•°æ®åŒ…æ‹¬æ¬§ç›ŸHorizoné¡¹ç›®çš„æ¡ˆä¾‹ï¼Œç”¨äºå†…éƒ¨è®­ç»ƒæµ‹è¯•å’Œå¤–éƒ¨æµ‹è¯•ã€‚ä¸»è¦ç ”ç©¶ç›®æ ‡æ˜¯è¯„ä¼°AIè¯Šæ–­ç»“æœä¸æ ‡å‡†æŠ¤ç†è¯Šæ–­ç»“æœçš„ç¬¦åˆç¨‹åº¦ï¼ŒåŒæ—¶æ¢è®¨æ¨¡å‹åœ¨ä¸åŒæˆåƒè´¨é‡ã€æ‚£è€…å¹´é¾„å’Œç§æ—æ–¹é¢çš„è¡¨ç°å·®å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬ç ”ç©¶é‡‡ç”¨å…¨çƒå¤šåœ°åŸŸçš„MRIæ£€æŸ¥æ•°æ®ï¼Œæ—¨åœ¨è®­ç»ƒå’ŒéªŒè¯æ–°ä¸€ä»£å‰åˆ—è…ºç™ŒAIè¯Šæ–­æ¨¡å‹PI-CAI-2Bã€‚</li>
<li>è¯¥æ¨¡å‹æ˜¯åŸºäºå·²æœ‰çš„å…ˆè¿›AIç³»ç»ŸPI-CAIå¼€å‘ï¼Œç”¨äºæ£€æµ‹Glasonè¯„åˆ†â‰¥2çš„å‰åˆ—è…ºç™Œã€‚</li>
<li>æ•°æ®æ¥æºäºä¸¤å¤§æ¬§ç›ŸHorizoné¡¹ç›®åŠå¤šä¸ªç‹¬ç«‹ç ”ç©¶ä¸­å¿ƒï¼Œæ¶µç›–æ¬§æ´²ã€åŒ—ç¾ã€äºšæ´²å’Œéæ´²ç­‰ä¸åŒåœ°åŒºã€‚</li>
<li>ä¸»è¦ç ”ç©¶ç›®æ ‡æ˜¯è¯„ä¼°AIè¯Šæ–­ç»“æœä¸æ ‡å‡†æŠ¤ç†è¯Šæ–­ç»“æœçš„ç¬¦åˆç¨‹åº¦ã€‚</li>
<li>ç ”ç©¶å°†è€ƒè™‘AIè¯Šæ–­åœ¨æˆåƒè´¨é‡ã€æ‚£è€…å¹´é¾„å’Œç§æ—ç­‰æ–¹é¢çš„å·®å¼‚è¡¨ç°ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨é¢„è®¾çš„ç»Ÿè®¡åˆ†æè®¡åˆ’ï¼ŒåŒ…æ‹¬å‡è®¾æ£€éªŒå’Œè¯»è€…è¯„ä¼°ï¼Œä»¥éªŒè¯æ¨¡å‹çš„è¯Šæ–­äº’æ¢æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03762">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2508.03762v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2508.03762v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2508.03762v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2508.03762v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2508.03762v2/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2508.03762v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="SV-DRR-High-Fidelity-Novel-View-X-Ray-Synthesis-Using-Diffusion-Model"><a href="#SV-DRR-High-Fidelity-Novel-View-X-Ray-Synthesis-Using-Diffusion-Model" class="headerlink" title="SV-DRR: High-Fidelity Novel View X-Ray Synthesis Using Diffusion Model"></a>SV-DRR: High-Fidelity Novel View X-Ray Synthesis Using Diffusion Model</h2><p><strong>Authors:Chun Xie, Yuichi Yoshii, Itaru Kitahara</strong></p>
<p>X-ray imaging is a rapid and cost-effective tool for visualizing internal human anatomy. While multi-view X-ray imaging provides complementary information that enhances diagnosis, intervention, and education, acquiring images from multiple angles increases radiation exposure and complicates clinical workflows. To address these challenges, we propose a novel view-conditioned diffusion model for synthesizing multi-view X-ray images from a single view. Unlike prior methods, which are limited in angular range, resolution, and image quality, our approach leverages the Diffusion Transformer to preserve fine details and employs a weak-to-strong training strategy for stable high-resolution image generation. Experimental results demonstrate that our method generates higher-resolution outputs with improved control over viewing angles. This capability has significant implications not only for clinical applications but also for medical education and data extension, enabling the creation of diverse, high-quality datasets for training and analysis. Our code is available at GitHub. </p>
<blockquote>
<p>Xå°„çº¿æˆåƒæ˜¯ä¸€ç§å¿«é€Ÿä¸”æˆæœ¬æ•ˆç›Šé«˜çš„å·¥å…·ï¼Œç”¨äºå¯è§†åŒ–äººä½“å†…éƒ¨ç»“æ„ã€‚è™½ç„¶å¤šè§†è§’Xå°„çº¿æˆåƒæä¾›äº†å¢å¼ºè¯Šæ–­ã€å¹²é¢„å’Œæ•™è‚²çš„è¡¥å……ä¿¡æ¯ï¼Œä½†ä»å¤šä¸ªè§’åº¦è·å–å›¾åƒä¼šå¢åŠ è¾å°„æš´éœ²å¹¶ä½¿ä¸´åºŠå·¥ä½œæµç¨‹å¤æ‚åŒ–ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–°å‹è§†è§’åˆæˆæ–¹æ³•ï¼Œå¯ä»¥ä»å•ä¸ªè§†è§’åˆæˆå¤šè§†è§’Xå°„çº¿å›¾åƒã€‚ä¸åŒäºä»¥å‰çš„æ–¹æ³•ï¼Œåœ¨è§’åº¦èŒƒå›´ã€åˆ†è¾¨ç‡å’Œå›¾åƒè´¨é‡ä¸Šæœ‰å±€é™æ€§ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨æ‰©æ•£å˜å‹å™¨ä¿ç•™ç»†èŠ‚ï¼Œå¹¶é‡‡ç”¨ç”±å¼±åˆ°å¼ºçš„è®­ç»ƒç­–ç•¥è¿›è¡Œç¨³å®šçš„é«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç”Ÿæˆäº†æ›´é«˜åˆ†è¾¨ç‡çš„è¾“å‡ºï¼Œå¯¹è§‚å¯Ÿè§’åº¦çš„æ§åˆ¶ä¹Ÿæœ‰æ‰€æ”¹å–„ã€‚è¿™é¡¹èƒ½åŠ›ä¸ä»…å¯¹äºä¸´åºŠåº”ç”¨å…·æœ‰é‡è¦æ„ä¹‰ï¼Œè€Œä¸”å¯¹äºåŒ»å­¦æ•™è‚²å’Œæ•°æ®æ‰©å±•ä¹Ÿæœ‰é‡å¤§å½±å“ï¼Œèƒ½å¤Ÿåˆ›å»ºå¤šæ ·ã€é«˜è´¨é‡çš„æ•°æ®é›†ï¼Œç”¨äºåŸ¹è®­å’Œæ•°æ®åˆ†æã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨GitHubä¸Šè·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05148v2">PDF</a> Accepted by MICCAI2025</p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶æå‡ºä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–°å‹è§†å›¾æ¡ä»¶åŒ–æ–¹æ³•ï¼Œå¯ä»å•ä¸€è§†è§’åˆæˆå¤šè§†è§’Xå°„çº¿å›¾åƒã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ‰©æ•£å˜å‹å™¨ä¿å­˜ç»†èŠ‚ï¼Œé‡‡ç”¨å¼±åˆ°å¼ºçš„è®­ç»ƒç­–ç•¥å®ç°ç¨³å®šçš„é«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ç”Ÿæˆçš„å›¾åƒåˆ†è¾¨ç‡é«˜ï¼Œå¯¹è§†è§’æ§åˆ¶æ›´åŠ çµæ´»ã€‚è¯¥ç ”ç©¶ä¸ä»…åœ¨ä¸´åºŠåº”ç”¨ä¸­å…·æœ‰æ˜¾è‘—æ„ä¹‰ï¼Œè¿˜å¯¹åŒ»å­¦æ•™è‚²å’Œæ•°æ®æ‰©å±•äº§ç”Ÿå½±å“ï¼Œèƒ½ä¸ºè®­ç»ƒå’Œåˆ†æåˆ›å»ºå¤šæ ·ã€é«˜è´¨é‡çš„æ•°æ®é›†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Xå°„çº¿æˆåƒæ˜¯ä¸€ç§å¿«é€Ÿä¸”ç»æµå®æƒ çš„å†…éƒ¨äººä½“ç»“æ„å¯è§†åŒ–å·¥å…·ã€‚</li>
<li>å¤šè§†è§’Xå°„çº¿æˆåƒèƒ½å¤Ÿæä¾›äº’è¡¥ä¿¡æ¯ï¼Œå¢å¼ºè¯Šæ–­ã€æ²»ç–—å’Œæ•™è‚²çš„æ•ˆæœã€‚</li>
<li>å½“å‰å¤šè§†è§’æˆåƒé¢ä¸´çš„æŒ‘æˆ˜åŒ…æ‹¬è¾å°„æš´éœ²å¢åŠ å’Œä¸´åºŠå·¥ä½œæµç¨‹å¤æ‚åŒ–ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºè§†å›¾æ¡ä»¶çš„æ‰©æ•£æ¨¡å‹ï¼Œå¯ä»å•ä¸€è§†è§’åˆæˆå¤šè§†è§’Xå°„çº¿å›¾åƒã€‚</li>
<li>è¯¥æ–¹æ³•åˆ©ç”¨æ‰©æ•£å˜å‹å™¨ä¿å­˜ç»†èŠ‚ï¼Œå¹¶é‡‡ç”¨å¼±åˆ°å¼ºçš„è®­ç»ƒç­–ç•¥è¿›è¡Œå›¾åƒç”Ÿæˆã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•ç”Ÿæˆçš„é«˜åˆ†è¾¨ç‡å›¾åƒå…·æœ‰æ›´å¥½çš„è§†è§’æ§åˆ¶èƒ½åŠ›ã€‚</li>
<li>è¯¥ç ”ç©¶å¯¹ä¸´åºŠåº”ç”¨ã€åŒ»å­¦æ•™è‚²å’Œæ•°æ®æ‰©å±•å…·æœ‰é‡è¦å½±å“ï¼Œèƒ½åˆ›å»ºé«˜è´¨é‡ã€å¤šæ ·åŒ–çš„æ•°æ®é›†ï¼Œç”¨äºè®­ç»ƒå’Œæ·±å…¥åˆ†æã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05148">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2507.05148v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2507.05148v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2507.05148v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Generation-of-Indoor-Open-Street-Maps-for-Robot-Navigation-from-CAD-Files"><a href="#Generation-of-Indoor-Open-Street-Maps-for-Robot-Navigation-from-CAD-Files" class="headerlink" title="Generation of Indoor Open Street Maps for Robot Navigation from CAD   Files"></a>Generation of Indoor Open Street Maps for Robot Navigation from CAD   Files</h2><p><strong>Authors:Jiajie Zhang, Shenrui Wu, Xu Ma, SÃ¶ren Schwertfeger</strong></p>
<p>The deployment of autonomous mobile robots is predicated on the availability of environmental maps, yet conventional generation via SLAM (Simultaneous Localization and Mapping) suffers from significant limitations in time, labor, and robustness, particularly in dynamic, large-scale indoor environments where map obsolescence can lead to critical localization failures. To address these challenges, this paper presents a complete and automated system for converting architectural Computer-Aided Design (CAD) files into a hierarchical topometric OpenStreetMap (OSM) representation, tailored for robust life-long robot navigation. Our core methodology involves a multi-stage pipeline that first isolates key structural layers from the raw CAD data and then employs an AreaGraph-based topological segmentation to partition the building layout into a hierarchical graph of navigable spaces. This process yields a comprehensive and semantically rich map, further enhanced by automatically associating textual labels from the CAD source and cohesively merging multiple building floors into a unified, topologically-correct model. By leveraging the permanent structural information inherent in CAD files, our system circumvents the inefficiencies and fragility of SLAM, offering a practical and scalable solution for deploying robots in complex indoor spaces. The software is encapsulated within an intuitive Graphical User Interface (GUI) to facilitate practical use. The code and dataset are available at <a target="_blank" rel="noopener" href="https://github.com/jiajiezhang7/osmAG-from-cad">https://github.com/jiajiezhang7/osmAG-from-cad</a>. </p>
<blockquote>
<p>è‡ªä¸»ç§»åŠ¨æœºå™¨äººçš„éƒ¨ç½²ä¾èµ–äºç¯å¢ƒåœ°å›¾çš„å¯ç”¨æ€§ï¼Œç„¶è€Œï¼Œä¼ ç»Ÿçš„é€šè¿‡SLAMï¼ˆåŒæ—¶å®šä½ä¸åœ°å›¾æ„å»ºï¼‰ç”Ÿæˆåœ°å›¾çš„æ–¹æ³•åœ¨æ—¶é—´ã€åŠ³åŠ¨åŠ›å’Œç¨³å¥æ€§æ–¹é¢å­˜åœ¨é‡å¤§å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨åŠ¨æ€ã€å¤§è§„æ¨¡å®¤å†…ç¯å¢ƒä¸­ï¼Œåœ°å›¾å¤±æ•ˆå¯èƒ½å¯¼è‡´å…³é”®å®šä½å¤±è´¥ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§å®Œæ•´ã€è‡ªåŠ¨åŒ–çš„ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿå¯å°†å»ºç­‘è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰æ–‡ä»¶è½¬æ¢ä¸ºåˆ†å±‚çš„æ‹“æ‰‘OpenStreetMapï¼ˆOSMï¼‰è¡¨ç¤ºå½¢å¼ï¼Œç‰¹åˆ«é€‚åˆç”¨äºç¨³å¥çš„ç»ˆèº«æœºå™¨äººå¯¼èˆªã€‚æˆ‘ä»¬çš„æ ¸å¿ƒæ–¹æ³•æ¶‰åŠä¸€ä¸ªå¤šé˜¶æ®µç®¡é“ï¼Œé¦–å…ˆä»åŸå§‹CADæ•°æ®ä¸­éš”ç¦»å‡ºå…³é”®ç»“æ„å±‚ï¼Œç„¶åé‡‡ç”¨åŸºäºAreaGraphçš„æ‹“æ‰‘åˆ†å‰²å°†å»ºç­‘å¸ƒå±€åˆ’åˆ†ä¸ºå¯å¯¼èˆªç©ºé—´çš„å±‚æ¬¡å›¾ã€‚è¿™ä¸€è¿‡ç¨‹ç”Ÿæˆäº†å…¨é¢ä¸”è¯­ä¹‰ä¸°å¯Œçš„åœ°å›¾ï¼Œé€šè¿‡è‡ªåŠ¨å…³è”æ¥è‡ªCADæºçš„æ–‡æœ¬æ ‡ç­¾ï¼Œå¹¶å°†å¤šä¸ªå»ºç­‘æ¥¼å±‚åˆå¹¶ä¸ºä¸€ä¸ªç»Ÿä¸€ã€æ‹“æ‰‘æ­£ç¡®çš„æ¨¡å‹ï¼Œè¿›ä¸€æ­¥å¢å¼ºäº†åœ°å›¾çš„åŠŸèƒ½ã€‚é€šè¿‡åˆ©ç”¨CADæ–‡ä»¶ä¸­å›ºæœ‰çš„æ°¸ä¹…ç»“æ„ä¿¡æ¯ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿé¿å…äº†SLAMçš„ä¸æ•ˆç‡å’Œè„†å¼±æ€§ï¼Œä¸ºåœ¨å¤æ‚å®¤å†…ç©ºé—´éƒ¨ç½²æœºå™¨äººæä¾›äº†å®ç”¨ä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚è½¯ä»¶è¢«å°è£…åœ¨ä¸€ä¸ªç›´è§‚çš„å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä¸­ï¼Œä»¥æ–¹ä¾¿å®é™…åº”ç”¨ã€‚ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/jiajiezhang7/osmAG-from-cad%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/jiajiezhang7/osmAG-from-cadä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.00552v2">PDF</a> 8 pages, 8 figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºç¯å¢ƒåœ°å›¾çš„è‡ªä¸»ç§»åŠ¨æœºå™¨äººéƒ¨ç½²é¢ä¸´æ—¶é—´ã€åŠ³åŠ¨åŠ›å’Œç¨³å¥æ€§æ–¹é¢çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨åŠ¨æ€ã€å¤§è§„æ¨¡å®¤å†…ç¯å¢ƒä¸­ï¼Œåœ°å›¾å¤±æ•ˆå¯èƒ½å¯¼è‡´å…³é”®å®šä½å¤±è´¥ã€‚é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºä¸€ç§å°†å»ºç­‘è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰æ–‡ä»¶è½¬æ¢ä¸ºåˆ†å±‚æ‹“æ‰‘OpenStreetMapï¼ˆOSMï¼‰è¡¨ç¤ºå½¢å¼çš„å®Œæ•´è‡ªåŠ¨åŒ–ç³»ç»Ÿï¼Œç”¨äºå®ç°ç¨³å¥çš„ç»ˆèº«æœºå™¨äººå¯¼èˆªã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥æ–¹æ³•ä½¿ç”¨å¤šé˜¶æ®µç®¡é“å¤„ç†ï¼Œä»åŸå§‹CADæ•°æ®ä¸­éš”ç¦»å…³é”®ç»“æ„å±‚ã€‚</li>
<li>é‡‡ç”¨AreaGraphåŸºäºæ‹“æ‰‘çš„åˆ†å‰²å°†å»ºç­‘å¸ƒå±€åˆ’åˆ†ä¸ºå¯å¯¼èˆªç©ºé—´çš„å±‚æ¬¡å›¾ã€‚</li>
<li>ç³»ç»Ÿèƒ½å¤Ÿç”Ÿæˆç»¼åˆä¸”è¯­ä¹‰ä¸°å¯Œçš„åœ°å›¾ï¼Œé€šè¿‡è‡ªåŠ¨å…³è”CADæºä¸­çš„æ–‡æœ¬æ ‡ç­¾å¹¶æ•´åˆå¤šä¸ªæ¥¼å±‚ï¼Œå½¢æˆç»Ÿä¸€ã€æ‹“æ‰‘æ­£ç¡®çš„æ¨¡å‹ã€‚</li>
<li>åˆ©ç”¨CADæ–‡ä»¶ä¸­å›ºæœ‰çš„æ°¸ä¹…ç»“æ„ä¿¡æ¯ï¼Œé¿å…äº†SLAMæ–¹æ³•çš„ä¸æ•ˆç‡å’Œè„†å¼±æ€§ã€‚</li>
<li>ç³»ç»Ÿæä¾›äº†ä¸€ä¸ªå®ç”¨çš„å®¤å†…ç©ºé—´æœºå™¨äººéƒ¨ç½²è§£å†³æ–¹æ¡ˆã€‚</li>
<li>è½¯ä»¶å°è£…åœ¨ç›´è§‚çš„ç”¨æˆ·å›¾å½¢ç•Œé¢ï¼ˆGUIï¼‰å†…ï¼Œä¾¿äºå®é™…åº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.00552">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2507.00552v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2507.00552v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2507.00552v2/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2507.00552v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2507.00552v2/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2507.00552v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2507.00552v2/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2507.00552v2/page_5_1.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Uncertainty-aware-Diffusion-and-Reinforcement-Learning-for-Joint-Plane-Localization-and-Anomaly-Diagnosis-in-3D-Ultrasound"><a href="#Uncertainty-aware-Diffusion-and-Reinforcement-Learning-for-Joint-Plane-Localization-and-Anomaly-Diagnosis-in-3D-Ultrasound" class="headerlink" title="Uncertainty-aware Diffusion and Reinforcement Learning for Joint Plane   Localization and Anomaly Diagnosis in 3D Ultrasound"></a>Uncertainty-aware Diffusion and Reinforcement Learning for Joint Plane   Localization and Anomaly Diagnosis in 3D Ultrasound</h2><p><strong>Authors:Yuhao Huang, Yueyue Xu, Haoran Dou, Jiaxiao Deng, Xin Yang, Hongyu Zheng, Dong Ni</strong></p>
<p>Congenital uterine anomalies (CUAs) can lead to infertility, miscarriage, preterm birth, and an increased risk of pregnancy complications. Compared to traditional 2D ultrasound (US), 3D US can reconstruct the coronal plane, providing a clear visualization of the uterine morphology for assessing CUAs accurately. In this paper, we propose an intelligent system for simultaneous automated plane localization and CUA diagnosis. Our highlights are: 1) we develop a denoising diffusion model with local (plane) and global (volume&#x2F;text) guidance, using an adaptive weighting strategy to optimize attention allocation to different conditions; 2) we introduce a reinforcement learning-based framework with unsupervised rewards to extract the key slice summary from redundant sequences, fully integrating information across multiple planes to reduce learning difficulty; 3) we provide text-driven uncertainty modeling for coarse prediction, and leverage it to adjust the classification probability for overall performance improvement. Extensive experiments on a large 3D uterine US dataset show the efficacy of our method, in terms of plane localization and CUA diagnosis. Code is available at <a target="_blank" rel="noopener" href="https://github.com/yuhoo0302/CUA-US">https://github.com/yuhoo0302/CUA-US</a>. </p>
<blockquote>
<p>å…ˆå¤©æ€§å­å®«å¼‚å¸¸ï¼ˆCUAsï¼‰å¯èƒ½å¯¼è‡´ä¸å­•ã€æµäº§ã€æ—©äº§å’Œå¦Šå¨ å¹¶å‘ç—‡é£é™©å¢åŠ ã€‚ä¸ä¼ ç»Ÿçš„äºŒç»´è¶…å£°ï¼ˆUSï¼‰ç›¸æ¯”ï¼Œä¸‰ç»´è¶…å£°èƒ½å¤Ÿé‡å»ºå† çŠ¶é¢ï¼Œä¸ºè¯„ä¼°CUAsæä¾›æ¸…æ™°çš„å­å®«å½¢æ€å¯è§†åŒ–ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ™ºèƒ½ç³»ç»Ÿï¼Œç”¨äºåŒæ—¶è¿›è¡Œå¹³é¢å®šä½è‡ªåŠ¨åŒ–å’ŒCUAè¯Šæ–­ã€‚æˆ‘ä»¬çš„è¦ç‚¹å¦‚ä¸‹ï¼š1ï¼‰æˆ‘ä»¬å¼€å‘äº†ä¸€ç§å¸¦æœ‰å±€éƒ¨ï¼ˆå¹³é¢ï¼‰å’Œå…¨å±€ï¼ˆä½“ç§¯&#x2F;æ–‡æœ¬ï¼‰æŒ‡å¯¼çš„å»å™ªæ‰©æ•£æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨è‡ªé€‚åº”åŠ æƒç­–ç•¥ä¼˜åŒ–å¯¹ä¸åŒæ¡ä»¶çš„æ³¨æ„åŠ›åˆ†é…ï¼›2ï¼‰æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ¡†æ¶ï¼Œé€šè¿‡æ— ç›‘ç£å¥–åŠ±ä»å†—ä½™åºåˆ—ä¸­æå–å…³é”®åˆ‡ç‰‡æ‘˜è¦ï¼Œå……åˆ†æ•´åˆå¤šä¸ªå¹³é¢çš„ä¿¡æ¯ï¼Œé™ä½å­¦ä¹ éš¾åº¦ï¼›3ï¼‰æˆ‘ä»¬ä¸ºç²—ç•¥é¢„æµ‹æä¾›äº†æ–‡æœ¬é©±åŠ¨çš„ä¸ç¡®å®šæ€§å»ºæ¨¡ï¼Œå¹¶åˆ©ç”¨å®ƒè°ƒæ•´åˆ†ç±»æ¦‚ç‡ä»¥æé«˜æ•´ä½“æ€§èƒ½ã€‚åœ¨å¤§å‹ä¸‰ç»´å­å®«è¶…å£°æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¹³é¢å®šä½å’ŒCUAè¯Šæ–­æ–¹é¢éƒ½å¾ˆæœ‰æ•ˆã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/yuhoo0302/CUA-US%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/yuhoo0302/CUA-USæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23538v2">PDF</a> Accepted by MICCAI 2025;10 pages, 3 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å…ˆå¤©æ€§å­å®«å¼‚å¸¸ï¼ˆCUAsï¼‰å¯èƒ½å¯¼è‡´ä¸å­•ã€æµäº§ã€æ—©äº§å’Œå¦Šå¨ å¹¶å‘ç—‡é£é™©å¢åŠ ã€‚ä¸ä¼ ç»ŸäºŒç»´è¶…å£°ï¼ˆUSï¼‰ç›¸æ¯”ï¼Œä¸‰ç»´è¶…å£°ï¼ˆUSï¼‰èƒ½é‡å»ºå† çŠ¶é¢ï¼Œæ›´å‡†ç¡®åœ°è¯„ä¼°å­å®«å½¢æ€ï¼Œä»è€Œè¯Šæ–­CUAsã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ™ºèƒ½ç³»ç»Ÿï¼Œç”¨äºåŒæ—¶è¿›è¡Œå¹³é¢å®šä½å’ŒCUAè¯Šæ–­ã€‚ä¸»è¦äº®ç‚¹åŒ…æ‹¬ï¼š1ï¼‰å¼€å‘äº†ä¸€ç§å¸¦æœ‰å±€éƒ¨ï¼ˆå¹³é¢ï¼‰å’Œå…¨å±€ï¼ˆä½“ç§¯&#x2F;æ–‡æœ¬ï¼‰å¼•å¯¼çš„é™å™ªæ‰©æ•£æ¨¡å‹ï¼Œé‡‡ç”¨è‡ªé€‚åº”åŠ æƒç­–ç•¥ä¼˜åŒ–ä¸åŒæ¡ä»¶ä¸‹çš„æ³¨æ„åŠ›åˆ†é…ï¼›2ï¼‰å¼•å…¥åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ¡†æ¶å’Œæ— äººç›‘ç£å¥–åŠ±æœºåˆ¶ï¼Œä»å†—ä½™åºåˆ—ä¸­æå–å…³é”®åˆ‡ç‰‡æ‘˜è¦ï¼Œå¹¶æ•´åˆå¤šå¹³é¢ä¿¡æ¯é™ä½å­¦ä¹ éš¾åº¦ï¼›3ï¼‰å»ºç«‹æ–‡æœ¬é©±åŠ¨çš„ä¸ç¡®å®šæ€§æ¨¡å‹è¿›è¡Œç²—ç•¥é¢„æµ‹ï¼Œå¹¶åˆ©ç”¨å…¶è°ƒæ•´åˆ†ç±»æ¦‚ç‡ä»¥æé«˜æ•´ä½“æ€§èƒ½ã€‚åœ¨å¤§å‹ä¸‰ç»´å­å®«è¶…å£°æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜äº†è¯¥æ–¹æ³•åœ¨å¹³é¢å®šä½å’ŒCUAè¯Šæ–­æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚ç›¸å…³ä»£ç å¯é€šè¿‡é“¾æ¥è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/yuhoo0302/CUA-US%E3%80%82">https://github.com/yuhoo0302/CUA-USã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å…ˆå¤©æ€§å­å®«å¼‚å¸¸ï¼ˆCUAsï¼‰æ˜¯å¤šç§å¦Šå¨ é—®é¢˜çš„æ½œåœ¨åŸå› ï¼ŒåŒ…æ‹¬ä¸å­•ã€æµäº§å’Œå¦Šå¨ å¹¶å‘ç—‡ã€‚</li>
<li>ä¸‰ç»´è¶…å£°ï¼ˆUSï¼‰æŠ€æœ¯æ¯”ä¼ ç»ŸäºŒç»´USæŠ€æœ¯æ›´èƒ½å‡†ç¡®è¯„ä¼°å­å®«å½¢æ€ï¼Œæœ‰åŠ©äºCUAsçš„è¯Šæ–­ã€‚</li>
<li>æå‡ºçš„æ™ºèƒ½ç³»ç»Ÿç»“åˆäº†é™å™ªæ‰©æ•£æ¨¡å‹å’Œå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå®ç°å¹³é¢è‡ªåŠ¨å®šä½å’ŒCUAsè¯Šæ–­ã€‚</li>
<li>è¯¥ç³»ç»Ÿé€šè¿‡å±€éƒ¨å’Œå…¨å±€å¼•å¯¼ä¼˜åŒ–æ³¨æ„åŠ›åˆ†é…ï¼Œå¹¶ä»å†—ä½™åºåˆ—ä¸­æå–å…³é”®ä¿¡æ¯ä»¥æé«˜æ•ˆç‡ã€‚</li>
<li>é€šè¿‡æ–‡æœ¬é©±åŠ¨çš„ä¸ç¡®å®šæ€§å»ºæ¨¡è¿›è¡Œç²—ç•¥é¢„æµ‹ï¼Œæé«˜è¯Šæ–­åˆ†ç±»çš„å‡†ç¡®æ€§ã€‚</li>
<li>åœ¨å¤§å‹ä¸‰ç»´å­å®«è¶…å£°æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†è¯¥ç³»ç»Ÿçš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23538">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2506.23538v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2506.23538v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2506.23538v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="ABS-Mamba-SAM2-Driven-Bidirectional-Spiral-Mamba-Network-for-Medical-Image-Translation"><a href="#ABS-Mamba-SAM2-Driven-Bidirectional-Spiral-Mamba-Network-for-Medical-Image-Translation" class="headerlink" title="ABS-Mamba: SAM2-Driven Bidirectional Spiral Mamba Network for Medical   Image Translation"></a>ABS-Mamba: SAM2-Driven Bidirectional Spiral Mamba Network for Medical   Image Translation</h2><p><strong>Authors:Feng Yuan, Yifan Gao, Wenbin Wu, Keqing Wu, Xiaotong Guo, Jie Jiang, Xin Gao</strong></p>
<p>Accurate multi-modal medical image translation requires ha-rmonizing global anatomical semantics and local structural fidelity, a challenge complicated by intermodality information loss and structural distortion. We propose ABS-Mamba, a novel architecture integrating the Segment Anything Model 2 (SAM2) for organ-aware semantic representation, specialized convolutional neural networks (CNNs) for preserving modality-specific edge and texture details, and Mambaâ€™s selective state-space modeling for efficient long- and short-range feature dependencies. Structurally, our dual-resolution framework leverages SAM2â€™s image encoder to capture organ-scale semantics from high-resolution inputs, while a parallel CNNs branch extracts fine-grained local features. The Robust Feature Fusion Network (RFFN) integrates these epresentations, and the Bidirectional Mamba Residual Network (BMRN) models spatial dependencies using spiral scanning and bidirectional state-space dynamics. A three-stage skip fusion decoder enhances edge and texture fidelity. We employ Efficient Low-Rank Adaptation (LoRA+) fine-tuning to enable precise domain specialization while maintaining the foundational capabilities of the pre-trained components. Extensive experimental validation on the SynthRAD2023 and BraTS2019 datasets demonstrates that ABS-Mamba outperforms state-of-the-art methods, delivering high-fidelity cross-modal synthesis that preserves anatomical semantics and structural details to enhance diagnostic accuracy in clinical applications. The code is available at <a target="_blank" rel="noopener" href="https://github.com/gatina-yone/ABS-Mamba">https://github.com/gatina-yone/ABS-Mamba</a> </p>
<blockquote>
<p>ç²¾ç¡®çš„å¤šæ¨¡æ€åŒ»å­¦å›¾åƒç¿»è¯‘éœ€è¦åè°ƒå…¨å±€è§£å‰–è¯­ä¹‰å’Œå±€éƒ¨ç»“æ„å¿ å®æ€§ï¼Œè¿™ä¸€æŒ‘æˆ˜å› æ¨¡æ€é—´ä¿¡æ¯ä¸¢å¤±å’Œç»“æ„å¤±çœŸè€Œå¤æ‚åŒ–ã€‚æˆ‘ä»¬æå‡ºäº†ABS-Mambaï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ¶æ„ï¼Œé›†æˆäº†Segment Anything Model 2ï¼ˆSAM2ï¼‰ç”¨äºå™¨å®˜æ„ŸçŸ¥è¯­ä¹‰è¡¨ç¤ºï¼Œä¸“é—¨è®¾è®¡çš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ç”¨äºä¿ç•™æ¨¡æ€ç‰¹å®šçš„è¾¹ç¼˜å’Œçº¹ç†ç»†èŠ‚ï¼Œä»¥åŠMambaçš„é€‰æ‹©æ€§çŠ¶æ€ç©ºé—´å»ºæ¨¡ï¼Œä»¥å®ç°é«˜æ•ˆçš„é•¿çŸ­ç¨‹ç‰¹å¾ä¾èµ–æ€§ã€‚ç»“æ„ä¸Šï¼Œæˆ‘ä»¬çš„åŒåˆ†è¾¨ç‡æ¡†æ¶åˆ©ç”¨SAM2çš„å›¾åƒç¼–ç å™¨æ•è·é«˜åˆ†è¾¨ç‡è¾“å…¥çš„ç»„ç»‡è§„æ¨¡è¯­ä¹‰ï¼Œè€Œå¹¶è¡Œçš„CNNåˆ†æ”¯åˆ™æå–ç²¾ç»†çš„å±€éƒ¨ç‰¹å¾ã€‚é²æ£’ç‰¹å¾èåˆç½‘ç»œï¼ˆRFFNï¼‰é›†æˆäº†è¿™äº›è¡¨ç¤ºï¼ŒåŒå‘Mambaæ®‹å·®ç½‘ç»œï¼ˆBMRNï¼‰ä½¿ç”¨èºæ—‹æ‰«æå’ŒåŒå‘çŠ¶æ€ç©ºé—´åŠ¨åŠ›å­¦è¿›è¡Œç©ºé—´ä¾èµ–æ€§å»ºæ¨¡ã€‚ä¸‰é˜¶æ®µè·³è·ƒèåˆè§£ç å™¨å¢å¼ºäº†è¾¹ç¼˜å’Œçº¹ç†çš„å¿ å®æ€§ã€‚æˆ‘ä»¬é‡‡ç”¨æœ‰æ•ˆçš„ä½ç§©é€‚åº”ï¼ˆLoRA+ï¼‰å¾®è°ƒæ–¹æ³•ï¼Œä»¥å®ç°åœ¨ä¿æŒé¢„è®­ç»ƒç»„ä»¶åŸºç¡€èƒ½åŠ›çš„åŒæ—¶ï¼Œè¿›è¡Œç²¾ç¡®çš„é¢†åŸŸä¸“ä¸šåŒ–ã€‚åœ¨SynthRAD2023å’ŒBraTS2019æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒéªŒè¯è¡¨æ˜ï¼ŒABS-Mambaä¼˜äºæœ€æ–°æ–¹æ³•ï¼Œå®ç°äº†é«˜ä¿çœŸè·¨æ¨¡æ€åˆæˆï¼Œä¿ç•™äº†è§£å‰–è¯­ä¹‰å’Œç»“æ„ç»†èŠ‚ï¼Œæé«˜äº†ä¸´åºŠåº”ç”¨ä¸­è¯Šæ–­çš„å‡†ç¡®æ€§ã€‚ä»£ç å¯ç”¨åœ¨<a target="_blank" rel="noopener" href="https://github.com/gatina-yone/ABS-Mamba%E3%80%82">https://github.com/gatina-yone/ABS-Mambaã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07687v2">PDF</a> MICCAI 2025(under view)</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤šæ¨¡æ€åŒ»å­¦å›¾åƒç¿»è¯‘çš„å‡†ç¡®æ€§éœ€è¦è°ƒå’Œå…¨å±€è§£å‰–è¯­ä¹‰å’Œå±€éƒ¨ç»“æ„å¿ å®åº¦ï¼Œè¿™ä¸€æŒ‘æˆ˜å› æ¨¡æ€é—´ä¿¡æ¯ä¸¢å¤±å’Œç»“æ„å¤±çœŸè€Œå¤æ‚åŒ–ã€‚æˆ‘ä»¬æå‡ºABS-Mambaï¼Œä¸€ç§ç»“åˆSAM2è¿›è¡Œå™¨å®˜æ„ŸçŸ¥è¯­ä¹‰è¡¨ç¤ºã€ä¸“ä¸šå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ä¿ç•™æ¨¡æ€ç‰¹å®šçš„è¾¹ç¼˜å’Œçº¹ç†ç»†èŠ‚ï¼Œä»¥åŠMambaçš„é€‰æ‹©çŠ¶æ€ç©ºé—´å»ºæ¨¡è¿›è¡Œé•¿çŸ­è·ç¦»ç‰¹å¾ä¾èµ–çš„æ–°æ¶æ„ã€‚æˆ‘ä»¬çš„åŒåˆ†è¾¨ç‡æ¡†æ¶åˆ©ç”¨SAM2çš„å›¾åƒç¼–ç å™¨æ•æ‰é«˜åˆ†è¾¨ç‡è¾“å…¥çš„å™¨å®˜è§„æ¨¡è¯­ä¹‰ï¼Œè€Œå¹¶è¡Œçš„CNNåˆ†æ”¯æå–ç²¾ç»†çš„å±€éƒ¨ç‰¹å¾ã€‚èåˆç½‘ç»œï¼ˆRFFNï¼‰å°†è¿™äº›è¡¨ç¤ºé›†æˆåœ¨ä¸€èµ·ï¼ŒåŒå‘Mambaæ®‹å·®ç½‘ç»œï¼ˆBMRNï¼‰ä½¿ç”¨èºæ—‹æ‰«æå’ŒåŒå‘çŠ¶æ€ç©ºé—´åŠ¨åŠ›å­¦å»ºæ¨¡ç©ºé—´ä¾èµ–å…³ç³»ã€‚ä¸‰é˜¶æ®µè·³è¿‡èåˆè§£ç å™¨å¢å¼ºäº†è¾¹ç¼˜å’Œçº¹ç†å¿ å®åº¦ã€‚æˆ‘ä»¬åœ¨SynthRAD2023å’ŒBraTS2019æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒéªŒè¯è¡¨æ˜ï¼ŒABS-Mambaä¼˜äºæœ€æ–°æ–¹æ³•ï¼Œå¯å®ç°é«˜ä¿çœŸè·¨æ¨¡æ€åˆæˆï¼Œä¿ç•™è§£å‰–è¯­ä¹‰å’Œç»“æ„ç»†èŠ‚ï¼Œæé«˜ä¸´åºŠåº”ç”¨çš„è¯Šæ–­å‡†ç¡®æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ABS-Mambaæ¶æ„é›†æˆäº†å¤šç§æŠ€æœ¯ï¼ŒåŒ…æ‹¬SAM2ç”¨äºå™¨å®˜æ„ŸçŸ¥è¯­ä¹‰è¡¨ç¤ºã€ä¸“ä¸šCNNä¿ç•™æ¨¡æ€ç‰¹å®šçš„ç»†èŠ‚ï¼Œä»¥åŠMambaçš„çŠ¶æ€ç©ºé—´å»ºæ¨¡ã€‚</li>
<li>åŒåˆ†è¾¨ç‡æ¡†æ¶èƒ½å¤Ÿæ•æ‰å™¨å®˜è§„æ¨¡è¯­ä¹‰å’Œç²¾ç»†çš„å±€éƒ¨ç‰¹å¾ã€‚</li>
<li>RFFNç½‘ç»œèåˆäº†ä¸åŒè¡¨ç¤ºï¼Œè€ŒBMRNåˆ™é€šè¿‡èºæ—‹æ‰«æå’ŒåŒå‘çŠ¶æ€ç©ºé—´åŠ¨åŠ›å­¦å»ºæ¨¡ç©ºé—´ä¾èµ–å…³ç³»ã€‚</li>
<li>ä¸‰é˜¶æ®µè·³è¿‡èåˆè§£ç å™¨å¢å¼ºäº†è¾¹ç¼˜å’Œçº¹ç†çš„å¿ å®åº¦ã€‚</li>
<li>ABS-Mambaä½¿ç”¨LoRA+å¾®è°ƒæ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨ç²¾ç¡®åŸŸä¸“ä¸šåŒ–çš„åŒæ—¶ä¿æŒé¢„è®­ç»ƒç»„ä»¶çš„åŸºç¡€èƒ½åŠ›ã€‚</li>
<li>åœ¨SynthRAD2023å’ŒBraTS2019æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒABS-Mambaåœ¨è·¨æ¨¡æ€åˆæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿä¿ç•™è§£å‰–è¯­ä¹‰å’Œç»“æ„ç»†èŠ‚ã€‚</li>
<li>ABS-Mambaæ¶æ„æœ‰æœ›æé«˜åŒ»å­¦å›¾åƒè¯Šæ–­çš„å‡†ç¡®æ€§ï¼Œå¹¶åœ¨ä¸´åºŠåº”ç”¨ä¸­æœ‰å¹¿é˜”å‰æ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07687">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2505.07687v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_åŒ»å­¦å›¾åƒ/2505.07687v2/page_2_0.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-13/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-13/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-13/TTS/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_TTS/2509.09631v1/page_5_1.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-13  DiFlow-TTS Discrete Flow Matching with Factorized Speech Tokens for   Low-Latency Zero-Shot Text-To-Speech
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-13
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-13/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_ç‰™é½¿ä¿®å¤/2509.09254v1/page_2_0.jpg" class="responsive-img" alt="ç‰™é½¿ä¿®å¤">
                        
                        <span class="card-title">ç‰™é½¿ä¿®å¤</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            ç‰™é½¿ä¿®å¤ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-13  Towards Better Dental AI A Multimodal Benchmark and Instruction Dataset   for Panoramic X-ray Analysis
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-13
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/" class="post-category">
                                    ç‰™é½¿ä¿®å¤
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                        <span class="chip bg-color">ç‰™é½¿ä¿®å¤</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">31686.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
