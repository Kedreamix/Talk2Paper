<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-09-13  Decoupling Clinical and Class-Agnostic Features for Reliable Few-Shot   Adaptation under Shift">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_Few-Shot/2509.09310v1/page_3_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-09-13
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-07
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    8.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    35 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-09-13-更新"><a href="#2025-09-13-更新" class="headerlink" title="2025-09-13 更新"></a>2025-09-13 更新</h1><h2 id="Decoupling-Clinical-and-Class-Agnostic-Features-for-Reliable-Few-Shot-Adaptation-under-Shift"><a href="#Decoupling-Clinical-and-Class-Agnostic-Features-for-Reliable-Few-Shot-Adaptation-under-Shift" class="headerlink" title="Decoupling Clinical and Class-Agnostic Features for Reliable Few-Shot   Adaptation under Shift"></a>Decoupling Clinical and Class-Agnostic Features for Reliable Few-Shot   Adaptation under Shift</h2><p><strong>Authors:Umaima Rahman, Raza Imam, Mohammad Yaqub, Dwarikanath Mahapatra</strong></p>
<p>Medical vision-language models (VLMs) offer promise for clinical decision support, yet their reliability under distribution shifts remains a major concern for safe deployment. These models often learn task-agnostic correlations due to variability in imaging protocols and free-text reports, limiting their generalizability and increasing the risk of failure in real-world settings. We propose DRiFt, a structured feature decoupling framework that explicitly separates clinically relevant signals from task-agnostic noise using parameter-efficient tuning (LoRA) and learnable prompt tokens. To enhance cross-modal alignment and reduce uncertainty, we curate high-quality, clinically grounded image-text pairs by generating captions for a diverse medical dataset. Our approach improves in-distribution performance by +11.4% Top-1 accuracy and +3.3% Macro-F1 over prior prompt-based methods, while maintaining strong robustness across unseen datasets. Ablation studies reveal that disentangling task-relevant features and careful alignment significantly enhance model generalization and reduce unpredictable behavior under domain shift. These insights contribute toward building safer, more trustworthy VLMs for clinical use. The code is available at <a target="_blank" rel="noopener" href="https://github.com/rumaima/DRiFt">https://github.com/rumaima/DRiFt</a>. </p>
<blockquote>
<p>医学视觉-语言模型（VLMs）在临床决策支持方面显示出巨大潜力，但在分布变化下的可靠性仍是其安全部署的主要关注点。这些模型通常会由于成像协议和文本报告的差异性而学习一些与任务无关的相关性，从而限制了其泛化能力并增加了在现实世界环境中的失败风险。我们提出了DRiFt这一结构化特征解耦框架，它利用参数效率高的调整（LoRA）和学习提示令牌，明确地将临床相关的信号从任务无关的噪声中分离出来。为了提高跨模态对齐和减少不确定性，我们通过为多样化的医学数据集生成标题来整理高质量的临床基础图像文本对。我们的方法较之前的提示方法提高了+11.4%的Top-1准确率和+3.3%的宏观F1分数，同时在未见数据集上保持了强大的稳健性。消融研究表明，解开任务相关特征并进行仔细对齐显著提高了模型的泛化能力并减少了领域迁移下的不可预测行为。这些见解为构建用于临床使用的更安全、更可信赖的VLMs做出了贡献。代码可在<a target="_blank" rel="noopener" href="https://github.com/rumaima/DRiFt%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/rumaima/DRiFt获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.09397v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>医疗视觉-语言模型（VLM）在临床决策支持中具有广阔的应用前景，但其分布转移下的可靠性仍是安全部署的主要关注点。本文提出DRiFt框架，通过参数高效调整（LoRA）和可学习提示令牌，明确分离临床相关信号和任务无关噪声。为提高跨模态对齐和减少不确定性，本文还为多样化的医疗数据集生成了高质量、基于临床的图像文本对。该框架在现有提示方法的基础上提高了+11.4%的Top-1准确率和+3.3%的宏观F1得分，同时在未见数据集上保持了强大的稳健性。消融研究证明，解耦任务相关特征并进行仔细对齐能显著增强模型的泛化能力并减少域转移下的不可预测行为。这些见解有助于构建更安全、更可靠的用于临床使用的VLM。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>医疗视觉-语言模型（VLM）在临床决策支持中有广泛应用，但分布转移下的可靠性是部署的主要挑战。</li>
<li>DRiFt框架旨在通过参数高效调整和可学习提示令牌，分离临床相关信号和任务无关噪声。</li>
<li>DRiFt框架提高了模型的性能，同时在未见数据集上保持了强大的稳健性。</li>
<li>通过解耦任务相关特征并进行仔细对齐，DRiFt框架能显著增强模型的泛化能力并减少不可预测行为。</li>
<li>DRiFt框架利用高质量、基于临床的图像文本对，提高跨模态对齐和减少不确定性。</li>
<li>DRiFt框架对构建更安全、更可靠的用于临床使用的VLM具有重要意义。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.09397">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_Few-Shot/2509.09397v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_Few-Shot/2509.09397v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_Few-Shot/2509.09397v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_Few-Shot/2509.09397v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_Few-Shot/2509.09397v1/page_5_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Exploring-Pre-training-Across-Domains-for-Few-Shot-Surgical-Skill-Assessment"><a href="#Exploring-Pre-training-Across-Domains-for-Few-Shot-Surgical-Skill-Assessment" class="headerlink" title="Exploring Pre-training Across Domains for Few-Shot Surgical Skill   Assessment"></a>Exploring Pre-training Across Domains for Few-Shot Surgical Skill   Assessment</h2><p><strong>Authors:Dimitrios Anastasiou, Razvan Caramalau, Nazir Sirajudeen, Matthew Boal, Philip Edwards, Justin Collins, John Kelly, Ashwin Sridhar, Maxine Tran, Faiz Mumtaz, Nevil Pavithran, Nader Francis, Danail Stoyanov, Evangelos B. Mazomenos</strong></p>
<p>Automated surgical skill assessment (SSA) is a central task in surgical computer vision. Developing robust SSA models is challenging due to the scarcity of skill annotations, which are time-consuming to produce and require expert consensus. Few-shot learning (FSL) offers a scalable alternative enabling model development with minimal supervision, though its success critically depends on effective pre-training. While widely studied for several surgical downstream tasks, pre-training has remained largely unexplored in SSA. In this work, we formulate SSA as a few-shot task and investigate how self-supervised pre-training strategies affect downstream few-shot SSA performance. We annotate a publicly available robotic surgery dataset with Objective Structured Assessment of Technical Skill (OSATS) scores, and evaluate various pre-training sources across three few-shot settings. We quantify domain similarity and analyze how domain gap and the inclusion of procedure-specific data into pre-training influence transferability. Our results show that small but domain-relevant datasets can outperform large scale, less aligned ones, achieving accuracies of 60.16%, 66.03%, and 73.65% in the 1-, 2-, and 5-shot settings, respectively. Moreover, incorporating procedure-specific data into pre-training with a domain-relevant external dataset significantly boosts downstream performance, with an average gain of +1.22% in accuracy and +2.28% in F1-score; however, applying the same strategy with less similar but large-scale sources can instead lead to performance degradation. Code and models are available at <a target="_blank" rel="noopener" href="https://github.com/anastadimi/ssa-fsl">https://github.com/anastadimi/ssa-fsl</a>. </p>
<blockquote>
<p>自动化手术技能评估（SSA）是计算机手术视觉中的核心任务。由于技能注释的稀缺性，开发稳健的SSA模型是一个挑战，这些注释耗费时间且需要专家共识。小样本学习（FSL）提供了一种可扩展的替代方案，能够在最小监督下实现模型开发，尽管其成功在很大程度上取决于有效的预训练。虽然预训练对于几个手术下游任务已经得到了广泛的研究，但在SSA中，它仍然基本上未被探索。在这项工作中，我们将SSA制定为小样本任务，并研究自监督预训练策略如何影响下游小样本SSA性能。我们使用目标结构化技术技能评估（OSATS）分数对公众可用的机器人手术数据集进行注释，并在三种小样本设置下评估各种预训练源的效果。我们量化领域相似性并分析领域差距以及特定程序数据纳入预训练对可转移性的影响。我们的结果表明，规模小但领域相关的数据集可以超越大规模、较少对齐的数据集，在1、2和5个样本的场景下分别达到60.16%、66.03%和73.65%的准确率。此外，将特定程序数据与领域相关的外部数据集结合进行预训练可显着提高下游性能，平均准确率提高+1.22%，F1分数提高+2.28%；然而，将相同的策略应用于不那么相似但规模较大的源可能会导致性能下降。代码和模型可在<a target="_blank" rel="noopener" href="https://github.com/anastadimi/ssa-fsl%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/anastadimi/ssa-fsl中找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.09327v1">PDF</a> Accepted at MICCAI 2025 DEMI Workshop</p>
<p><strong>Summary</strong></p>
<p>本文研究了基于少样本学习的自动化手术技能评估（SSA）的预训练策略。文章指出，在数据稀缺的情况下，利用自我监督的预训练策略能够提高少样本手术技能评估模型的性能。实验结果显示，与大规模、少对齐的数据集相比，规模小但与任务相关的数据集能够取得更高的准确率。此外，结合任务特定数据与相关外部数据集进行预训练可以进一步提高下游性能。相关代码和模型可在<a target="_blank" rel="noopener" href="https://github.com/anastadimi/ssa-fsl%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/anastadimi/ssa-fsl找到。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究采用少样本学习（FSL）方法应用于自动化手术技能评估（SSA）。</li>
<li>预训练策略在少样本SSA中至关重要，能提高模型的性能。</li>
<li>与大规模、少对齐的数据集相比，规模小但任务相关的数据集表现更好。</li>
<li>在预训练中结合任务特定数据可提高下游性能。</li>
<li>使用领域相关的外部数据集与特定程序的数据进行预训练效果最佳。</li>
<li>与不相似的大规模数据源结合进行预训练可能导致性能下降。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.09327">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_Few-Shot/2509.09327v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_Few-Shot/2509.09327v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_Few-Shot/2509.09327v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="You-Share-Beliefs-I-Adapt-Progressive-Heterogeneous-Collaborative-Perception"><a href="#You-Share-Beliefs-I-Adapt-Progressive-Heterogeneous-Collaborative-Perception" class="headerlink" title="You Share Beliefs, I Adapt: Progressive Heterogeneous Collaborative   Perception"></a>You Share Beliefs, I Adapt: Progressive Heterogeneous Collaborative   Perception</h2><p><strong>Authors:Hao Si, Ehsan Javanmardi, Manabu Tsukada</strong></p>
<p>Collaborative perception enables vehicles to overcome individual perception limitations by sharing information, allowing them to see further and through occlusions. In real-world scenarios, models on different vehicles are often heterogeneous due to manufacturer variations. Existing methods for heterogeneous collaborative perception address this challenge by fine-tuning adapters or the entire network to bridge the domain gap. However, these methods are impractical in real-world applications, as each new collaborator must undergo joint training with the ego vehicle on a dataset before inference, or the ego vehicle stores models for all potential collaborators in advance. Therefore, we pose a new question: Can we tackle this challenge directly during inference, eliminating the need for joint training? To answer this, we introduce Progressive Heterogeneous Collaborative Perception (PHCP), a novel framework that formulates the problem as few-shot unsupervised domain adaptation. Unlike previous work, PHCP dynamically aligns features by self-training an adapter during inference, eliminating the need for labeled data and joint training. Extensive experiments on the OPV2V dataset demonstrate that PHCP achieves strong performance across diverse heterogeneous scenarios. Notably, PHCP achieves performance comparable to SOTA methods trained on the entire dataset while using only a small amount of unlabeled data. </p>
<blockquote>
<p>协同感知使车辆通过共享信息来克服个体感知的局限性，从而使它们能够看得更远并透过遮挡物。在现实世界的场景中，由于制造商的差异，不同车辆上的模型通常是异构的。现有的异构协同感知方法通过微调适配器或整个网络来解决这一挑战，以弥合领域差距。然而，这些方法在真实世界的应用中并不实用，因为每个新合作伙伴必须在推理之前在数据集上与自主车辆进行联合训练，或者自主车辆预先存储所有潜在合作伙伴的模型。因此，我们提出了一个新问题：我们能否在推理过程中直接解决这一挑战，从而不需要联合训练？为了回答这个问题，我们引入了渐进式异构协同感知（PHCP），这是一种新的框架，将问题表述为少量无监督域适应问题。与以前的工作不同，PHCP通过自我训练适配器在推理过程中动态对齐特征，从而消除了对标记数据和联合训练的需要。在OPV2V数据集上的广泛实验表明，PHCP在不同种类的异构场景中表现出强大的性能。值得注意的是，PHCP在仅使用少量未标记数据的情况下，实现了与在整个数据集上训练的最新方法相当的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.09310v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>协作感知技术通过车辆间信息共享，克服了个体感知的局限性，使车辆能够看得更远并透过遮挡物。针对不同车辆模型间的异质性问题，现有方法通过微调适配器或整个网络来弥合领域差距。然而，这些方法在现实世界应用中并不实用，因为每个新协作方在推理前都需要与自主车辆进行联合训练，或者自主车辆需要提前存储所有潜在协作方的模型。为此，我们提出新问题：能否在推理过程中直接解决这一挑战，而无需进行联合训练？我们引入了渐进式异构协作感知（PHCP）这一新型框架，将问题表述为小样本无监督域适应问题。PHCP通过自训练适配器在推理过程中动态对齐特征，无需标记数据和联合训练。在OPV2V数据集上的广泛实验表明，PHCP在多种异构场景下表现出强大的性能，其性能可与使用整个数据集训练的最新技术相媲美，同时仅使用少量未标记数据。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>协作感知技术通过信息共享克服个体感知限制，增强车辆视野。</li>
<li>不同车辆模型间的协作感知面临异质性问题。</li>
<li>现有方法通过微调适配器或整个网络进行领域适应，但实际应用中不实用。</li>
<li>无需联合训练的新方法——渐进式异构协作感知（PHCP）被提出。</li>
<li>PHCP将问题表述为小样本案无监督域适应问题。</li>
<li>PHCP通过自训练适配器在推理过程中动态对齐特征。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.09310">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_Few-Shot/2509.09310v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_Few-Shot/2509.09310v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_Few-Shot/2509.09310v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_Few-Shot/2509.09310v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_Few-Shot/2509.09310v1/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_Few-Shot/2509.09310v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_Few-Shot/2509.09310v1/page_5_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_Few-Shot/2509.09310v1/page_5_2.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Can-Multimodal-LLMs-See-Materials-Clearly-A-Multimodal-Benchmark-on-Materials-Characterization"><a href="#Can-Multimodal-LLMs-See-Materials-Clearly-A-Multimodal-Benchmark-on-Materials-Characterization" class="headerlink" title="Can Multimodal LLMs See Materials Clearly? A Multimodal Benchmark on   Materials Characterization"></a>Can Multimodal LLMs See Materials Clearly? A Multimodal Benchmark on   Materials Characterization</h2><p><strong>Authors:Zhengzhao Lai, Youbin Zheng, Zhenyang Cai, Haonan Lyu, Jinpu Yang, Hongqing Liang, Yan Hu, Benyou Wang</strong></p>
<p>Materials characterization is fundamental to acquiring materials information, revealing the processing-microstructure-property relationships that guide material design and optimization. While multimodal large language models (MLLMs) have recently shown promise in generative and predictive tasks within materials science, their capacity to understand real-world characterization imaging data remains underexplored. To bridge this gap, we present MatCha, the first benchmark for materials characterization image understanding, comprising 1,500 questions that demand expert-level domain expertise. MatCha encompasses four key stages of materials research comprising 21 distinct tasks, each designed to reflect authentic challenges faced by materials scientists. Our evaluation of state-of-the-art MLLMs on MatCha reveals a significant performance gap compared to human experts. These models exhibit degradation when addressing questions requiring higher-level expertise and sophisticated visual perception. Simple few-shot and chain-of-thought prompting struggle to alleviate these limitations. These findings highlight that existing MLLMs still exhibit limited adaptability to real-world materials characterization scenarios. We hope MatCha will facilitate future research in areas such as new material discovery and autonomous scientific agents. MatCha is available at <a target="_blank" rel="noopener" href="https://github.com/FreedomIntelligence/MatCha">https://github.com/FreedomIntelligence/MatCha</a>. </p>
<blockquote>
<p>材料表征是获取材料信息的基础，揭示了加工-微观结构-性能关系，为材料设计和优化提供指导。虽然最近多模态大型语言模型（MLLM）在材料科学的生成和预测任务中显示出潜力，但它们理解真实世界的表征成像数据的能力仍然被低估。为了弥补这一差距，我们推出了MatCha，这是首个材料表征图像理解基准测试，包含1500个需要专家级领域知识的问题。MatCha涵盖材料研究的四个阶段，包括21个不同的任务，每个任务都反映材料科学家面临的真实挑战。我们对最新MLLM在MatCha上的评估显示，与人类专家相比，存在显著的性能差距。这些模型在处理需要高级专业知识和复杂视觉感知的问题时表现出退化。简单的少镜头和思维链提示难以克服这些局限性。这些发现表明，现有的MLLM对真实世界的材料表征场景适应性仍然有限。我们希望MatCha能促进未来在新材料发现和自主科学代理等领域的研究。MatCha可在<a target="_blank" rel="noopener" href="https://github.com/FreedomIntelligence/MatCha%E4%B8%8A%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/FreedomIntelligence/MatCha上获得。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.09307v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了材料表征在材料信息获取、揭示加工-微观结构-性能关系以及材料设计与优化中的重要性。针对当前多模态大型语言模型在材料科学领域的应用，本文提出了MatCha基准测试集，包含1500个需要专家级领域知识的问答题目。MatCha涵盖了材料研究的四个阶段，共包含21个不同任务，每个任务旨在反映材料科学家面临的实际挑战。评估显示，现有模型与专家相比存在显著性能差距，尤其是在处理高水平专业知识需求和复杂视觉感知的问题时表现不佳。简单的few-shot和chain-of-thought提示难以解决这些问题。因此，现有的多模态大型语言模型在真实世界材料表征场景中仍存在适应性不足的局限性。本文期望MatCha能够推动未来新材料发现和自主科学代理等领域的研究进步。MatCha可在特定网址访问。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>材料表征在材料设计和优化中扮演着揭示加工-微观结构-性能关系的重要角色。</li>
<li>多模态大型语言模型在材料科学领域具有潜力，但在理解真实世界的表征成像数据方面仍存在不足。</li>
<li>MatCha是首个针对材料表征图像理解提出的基准测试集，包含反映真实挑战的任务和专家级领域知识需求的题目。</li>
<li>评估显示现有模型在解决高水平专业知识和复杂视觉感知需求方面存在局限性，简单的提示方式无法有效解决这些问题。</li>
<li>MatCha基准测试集期望促进新材料发现和自主科学代理等领域的研究进步。</li>
<li>MatCha的访问渠道已经公开。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.09307">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_Few-Shot/2509.09307v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_Few-Shot/2509.09307v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_Few-Shot/2509.09307v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Bridging-the-Gap-Between-Ideal-and-Real-world-Evaluation-Benchmarking-AI-Generated-Image-Detection-in-Challenging-Scenarios"><a href="#Bridging-the-Gap-Between-Ideal-and-Real-world-Evaluation-Benchmarking-AI-Generated-Image-Detection-in-Challenging-Scenarios" class="headerlink" title="Bridging the Gap Between Ideal and Real-world Evaluation: Benchmarking   AI-Generated Image Detection in Challenging Scenarios"></a>Bridging the Gap Between Ideal and Real-world Evaluation: Benchmarking   AI-Generated Image Detection in Challenging Scenarios</h2><p><strong>Authors:Chunxiao Li, Xiaoxiao Wang, Meiling Li, Boming Miao, Peng Sun, Yunjian Zhang, Xiangyang Ji, Yao Zhu</strong></p>
<p>With the rapid advancement of generative models, highly realistic image synthesis has posed new challenges to digital security and media credibility. Although AI-generated image detection methods have partially addressed these concerns, a substantial research gap remains in evaluating their performance under complex real-world conditions. This paper introduces the Real-World Robustness Dataset (RRDataset) for comprehensive evaluation of detection models across three dimensions: 1) Scenario Generalization: RRDataset encompasses high-quality images from seven major scenarios (War and Conflict, Disasters and Accidents, Political and Social Events, Medical and Public Health, Culture and Religion, Labor and Production, and everyday life), addressing existing dataset gaps from a content perspective. 2) Internet Transmission Robustness: examining detector performance on images that have undergone multiple rounds of sharing across various social media platforms. 3) Re-digitization Robustness: assessing model effectiveness on images altered through four distinct re-digitization methods. We benchmarked 17 detectors and 10 vision-language models (VLMs) on RRDataset and conducted a large-scale human study involving 192 participants to investigate human few-shot learning capabilities in detecting AI-generated images. The benchmarking results reveal the limitations of current AI detection methods under real-world conditions and underscore the importance of drawing on human adaptability to develop more robust detection algorithms. </p>
<blockquote>
<p>随着生成模型的快速发展，高度逼真的图像合成对数字安全和媒体可信度提出了新的挑战。虽然人工智能生成的图像检测方法已经部分解决了这些担忧，但在复杂现实条件下评估它们的性能仍存在巨大研究空白。本文介绍了用于全面评估检测模型的三维度现实鲁棒性数据集（RRDataset）：1）场景泛化：RRDataset包含来自七大场景的高质量图像（战争与冲突、灾害与事故、政治与社会事件、医疗与公共卫生、文化与宗教、劳动与生产以及日常生活），从内容角度解决了现有数据集的空白。2）互联网传播鲁棒性：考察图像在各大社交媒体平台经过多轮共享后的检测器性能。3）再数字化鲁棒性：评估四种不同再数字化方法改变图像的模型有效性。我们在RRDataset上对17种检测器和10种视觉语言模型进行了基准测试，并进行了一项涉及192名参与者的大规模人类研究，以调查人类在小样本学习中检测人工智能生成图像的能力。基准测试结果揭示了当前人工智能检测方法在现实世界条件下的局限性，并强调了借鉴人类适应性来开发更稳健的检测算法的重要性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.09172v1">PDF</a> ICCV2025</p>
<p><strong>Summary</strong><br>在生成模型迅速发展的背景下，高度逼真的图像合成对数字安全和媒体可信度提出了新的挑战。尽管已有部分AI生成的图像检测方法，但在复杂现实条件下评估其性能仍存在较大研究空白。本文介绍了一个用于全面评估检测模型的真实世界鲁棒性数据集（RRDataset），涵盖场景泛化、互联网传输鲁棒性和再数字化鲁棒性三个维度。对17种检测器和10种视觉语言模型进行了基准测试，并进行了一项涉及192名参与者的大规模人类研究，以探索检测AI生成图像的人类小样本学习能力。结果表明当前AI检测方法的局限性，并强调需要借鉴人类适应性来开发更稳健的检测算法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>生成模型的快速发展导致高度逼真的图像合成对数字安全和媒体可信度提出挑战。</li>
<li>AI生成的图像检测方法已部分出现以应对这些挑战，但在复杂现实条件下评估其性能仍存在研究空白。</li>
<li>介绍了真实世界鲁棒性数据集（RRDataset），用于全面评估检测模型的性能。</li>
<li>RRDataset涵盖场景泛化、互联网传输鲁棒性和再数字化鲁棒性三个维度。</li>
<li>对多种检测器和视觉语言模型进行了基准测试。</li>
<li>通过大规模人类研究探索了检测AI生成图像的人类小样本学习能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.09172">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_Few-Shot/2509.09172v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_Few-Shot/2509.09172v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_Few-Shot/2509.09172v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_Few-Shot/2509.09172v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_Few-Shot/2509.09172v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Automated-Evidence-Extraction-and-Scoring-for-Corporate-Climate-Policy-Engagement-A-Multilingual-RAG-Approach"><a href="#Automated-Evidence-Extraction-and-Scoring-for-Corporate-Climate-Policy-Engagement-A-Multilingual-RAG-Approach" class="headerlink" title="Automated Evidence Extraction and Scoring for Corporate Climate Policy   Engagement: A Multilingual RAG Approach"></a>Automated Evidence Extraction and Scoring for Corporate Climate Policy   Engagement: A Multilingual RAG Approach</h2><p><strong>Authors:Imene Kolli, Ario Saeid Vaghefi, Chiara Colesanti Senni, Shantam Raj, Markus Leippold</strong></p>
<p>InfluenceMap’s LobbyMap Platform monitors the climate policy engagement of over 500 companies and 250 industry associations, assessing each entity’s support or opposition to science-based policy pathways for achieving the Paris Agreement’s goal of limiting global warming to 1.5{\deg}C. Although InfluenceMap has made progress with automating key elements of the analytical workflow, a significant portion of the assessment remains manual, making it time- and labor-intensive and susceptible to human error. We propose an AI-assisted framework to accelerate the monitoring of corporate climate policy engagement by leveraging Retrieval-Augmented Generation to automate the most time-intensive extraction of relevant evidence from large-scale textual data. Our evaluation shows that a combination of layout-aware parsing, the Nomic embedding model, and few-shot prompting strategies yields the best performance in extracting and classifying evidence from multilingual corporate documents. We conclude that while the automated RAG system effectively accelerates evidence extraction, the nuanced nature of the analysis necessitates a human-in-the-loop approach where the technology augments, rather than replaces, expert judgment to ensure accuracy. </p>
<blockquote>
<p>InfluenceMap的LobbyMap平台正在监测超过500家公司和250个行业协会的气候政策参与情况。它评估每个实体对基于科学的政策路径的支持或反对态度，以实现《巴黎协定》将全球变暖限制在1.5摄氏度的目标。尽管InfluenceMap在自动化分析工作流的关键方面取得了进展，但评估的大部分内容仍然需要人工操作，这使得它耗时耗力，并容易出错。我们提出了一种AI辅助框架，旨在利用检索增强生成技术来加速对企业气候政策参与的监测，通过自动化从大规模文本数据中提取最耗时且相关的证据。我们的评估显示，结合布局感知解析、Nomic嵌入模型和少样本提示策略，在从多语言企业文档中提取和分类证据方面表现最佳。我们得出结论，尽管自动化的RAG系统有效地加速了证据提取，但分析的微妙性质需要一种人工参与的方法，在这种方法中，技术增强而不是替代专家判断，以确保准确性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.08907v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于InfluenceMap的LobbyMap平台，该文提出了一个AI辅助框架，用于加速对企业气候政策参与情况的监测，通过利用增强检索生成技术，自动化提取大规模文本数据中的相关证据。文中评价了几种技术方案的性能，认为结合布局感知解析、Nomic嵌入模型和少样本提示策略的方法在提取和分类证据方面的表现最佳。最终得出结论，虽然自动化加速证据提取，但分析仍需要人的参与，技术与专家判断相结合是确保准确性的关键。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>InfluenceMap监测企业和行业协会的气候政策参与情况，评估其对巴黎协议目标的支持或反对立场。</li>
<li>当前评估存在手动操作多、耗时耗力、易出错的问题。</li>
<li>提出AI辅助框架，利用增强检索生成技术自动化提取相关证据。</li>
<li>技术方案结合布局感知解析、Nomic嵌入模型和少样本提示策略表现最佳。</li>
<li>自动化技术在加速证据提取方面有效。</li>
<li>分析具有微妙性，需要人的参与和判断，技术与专家结合是确保准确性的关键。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.08907">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_Few-Shot/2509.08907v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_Few-Shot/2509.08907v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_Few-Shot/2509.08907v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_Few-Shot/2509.08907v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_Few-Shot/2509.08907v1/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_Few-Shot/2509.08907v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_Few-Shot/2509.08907v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Two-Sides-of-the-Same-Optimization-Coin-Model-Degradation-and-Representation-Collapse-in-Graph-Foundation-Models"><a href="#Two-Sides-of-the-Same-Optimization-Coin-Model-Degradation-and-Representation-Collapse-in-Graph-Foundation-Models" class="headerlink" title="Two Sides of the Same Optimization Coin: Model Degradation and   Representation Collapse in Graph Foundation Models"></a>Two Sides of the Same Optimization Coin: Model Degradation and   Representation Collapse in Graph Foundation Models</h2><p><strong>Authors:Xunkai Li, Daohan Su, Sicheng Liu, Ru Zhang, Zhenjun Li, Bing Zhou, Rong-Hua Li, Guoren Wang</strong></p>
<p>Graph foundation models, inspired by the success of LLMs, are designed to learn the optimal embedding from multi-domain TAGs for the downstream cross-task generalization capability. During our investigation, graph VQ-MAE stands out among the increasingly diverse landscape of GFM architectures. This is attributed to its ability to jointly encode topology and textual attributes from multiple domains into discrete embedding spaces with clear semantic boundaries. Despite its potential, domain generalization conflicts cause imperceptible pitfalls. In this paper, we instantiate two of them, and they are just like two sides of the same GFM optimization coin - Side 1 Model Degradation: The encoder and codebook fail to capture the diversity of inputs; Side 2 Representation Collapse: The hidden embedding and codebook vector fail to preserve semantic separability due to constraints from narrow representation subspaces. These two pitfalls (sides) collectively impair the decoder and generate the low-quality reconstructed supervision, causing the GFM optimization dilemma during pre-training (coin). Through empirical investigation, we attribute the above challenges to Information Bottleneck and Regularization Deficit. To address them, we propose MoT (Mixture-of-Tinkers) - (1) Information Tinker for Two Pitfalls, which utilizes an edge-wise semantic fusion strategy and a mixture-of-codebooks with domain-aware routing to improve information capacity. (2) Regularization Tinker for Optimization Coin, which utilizes two additional regularizations to further improve gradient supervision in our proposed Information Tinker. Notably, as a flexible architecture, MoT adheres to the scaling laws of GFM, offering a controllable model scale. Compared to SOTA baselines, experiments on 22 datasets across 6 domains demonstrate that MoT achieves significant improvements in supervised, few-shot, and zero-shot scenarios. </p>
<blockquote>
<p>图模型基础，受到大型语言模型（LLMs）成功的启发，旨在从多域标签（TAGs）中学习最佳嵌入，以用于下游跨任务的泛化能力。在我们的研究中，图VQ-MAE在日益多样化的GFM架构中脱颖而出。这归功于其能够联合编码多个领域的拓扑和文本属性到具有清晰语义边界的离散嵌入空间的能力。尽管其具有潜力，但领域泛化冲突会导致难以察觉的陷阱。在本文中，我们实例化了两个这样的陷阱，它们就像是GFM优化同一枚硬币的两面——第一面是模型退化：编码器和代码本无法捕捉输入的多样性；第二面是表示崩溃：隐藏嵌入和代码本向量由于来自狭窄表示子空间的约束而无法保持语义可分性。这两个陷阱（面）共同损害了解码器并产生了低质量的重构监督，从而在预训练期间造成GFM优化困境（硬币）。通过实证研究，我们将上述挑战归因于信息瓶颈和正则化缺陷。为了解决这个问题，我们提出了MoT（混合微调器）——（1）针对两个陷阱的信息微调器，它利用边缘语义融合策略和带有领域感知路由的混合代码本来提高信息容量。（2）针对优化硬币的正则化微调器，它利用两种额外的正则化方法进一步改进我们提出的信息微调器中的梯度监督。值得注意的是，作为一个灵活的架构，MoT遵循GFM的扩展定律，提供了一个可控的模型规模。在6个领域的22个数据集上的实验表明，与最先进的基线相比，MoT在监督、小样本和零样本场景中实现了显著改进。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.08401v2">PDF</a> </p>
<p><strong>Summary</strong><br>     图基础模型通过借鉴大型语言模型的成功经验，旨在学习多域标签的最优嵌入，以提升下游跨任务的泛化能力。图VQ-MAE在众多图基础模型架构中脱颖而出，它能将拓扑和文本属性编码进离散嵌入空间，并具有清晰的语义边界。然而，也存在领域泛化冲突引起的隐患，如模型退化与表示崩溃。本文实证调查发现这些问题源于信息瓶颈和正则化缺陷，并提出了MoT（混合微调）方法来解决这些问题。MoT方法通过信息微调与针对优化币的正则化微调，改善了信息容量和优化梯度监督。作为灵活的图基础模型架构，MoT遵循规模定律，并在多个数据集上的实验表明，它在监督学习、小样本学习和零样本学习场景下均取得了显著的提升。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>图基础模型借鉴大型语言模型的成功经验，旨在通过多域标签的最优嵌入提升下游任务的泛化能力。</li>
<li>图VQ-MAE具有联合编码拓扑和文本属性的能力，能够生成具有清晰语义边界的离散嵌入空间。</li>
<li>领域泛化冲突可能导致模型退化与表示崩溃的问题。</li>
<li>通过实证调查，这些问题被归因于信息瓶颈和正则化缺陷。</li>
<li>针对这些问题，提出了MoT（混合微调）方法，包括信息微调与针对优化币的正则化微调。</li>
<li>MoT方法通过改善信息容量和优化梯度监督来解决领域泛化冲突问题。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.08401">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_Few-Shot/2509.08401v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_Few-Shot/2509.08401v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_Few-Shot/2509.08401v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_Few-Shot/2509.08401v2/page_4_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Expert-Guided-Explainable-Few-Shot-Learning-for-Medical-Image-Diagnosis"><a href="#Expert-Guided-Explainable-Few-Shot-Learning-for-Medical-Image-Diagnosis" class="headerlink" title="Expert-Guided Explainable Few-Shot Learning for Medical Image Diagnosis"></a>Expert-Guided Explainable Few-Shot Learning for Medical Image Diagnosis</h2><p><strong>Authors:Ifrat Ikhtear Uddin, Longwei Wang, KC Santosh</strong></p>
<p>Medical image analysis often faces significant challenges due to limited expert-annotated data, hindering both model generalization and clinical adoption. We propose an expert-guided explainable few-shot learning framework that integrates radiologist-provided regions of interest (ROIs) into model training to simultaneously enhance classification performance and interpretability. Leveraging Grad-CAM for spatial attention supervision, we introduce an explanation loss based on Dice similarity to align model attention with diagnostically relevant regions during training. This explanation loss is jointly optimized with a standard prototypical network objective, encouraging the model to focus on clinically meaningful features even under limited data conditions. We evaluate our framework on two distinct datasets: BraTS (MRI) and VinDr-CXR (Chest X-ray), achieving significant accuracy improvements from 77.09% to 83.61% on BraTS and from 54.33% to 73.29% on VinDr-CXR compared to non-guided models. Grad-CAM visualizations further confirm that expert-guided training consistently aligns attention with diagnostic regions, improving both predictive reliability and clinical trustworthiness. Our findings demonstrate the effectiveness of incorporating expert-guided attention supervision to bridge the gap between performance and interpretability in few-shot medical image diagnosis. </p>
<blockquote>
<p>医学图像分析常常因为专家标注数据有限而面临重大挑战，这不仅影响了模型的泛化能力，也阻碍了其在临床的采纳。我们提出了一种专家引导的可解释少量学习框架，该框架将放射科医生提供的感兴趣区域（ROI）整合到模型训练中，以同时提高分类性能和解释性。我们利用Grad-CAM进行空间注意力监督，并基于Dice相似性引入一种解释损失，以在训练过程中使模型注意力与诊断相关的区域对齐。这种解释损失与标准原型网络目标联合优化，鼓励模型即使在数据有限的情况下也关注临床上具有意义的特征。我们在两个不同的数据集BraTS（MRI）和VinDr-CXR（胸部X光）上评估了我们的框架，与无引导模型相比，BraTS的准确性从77.09%提高到83.61%，VinDr-CXR从54.33%提高到73.29%，实现了显著的准确性提高。Grad-CAM可视化进一步证实，专家引导的训练能使注意力始终与诊断区域对齐，提高了预测可靠性和临床可信度。我们的研究结果表明，在少量医学图像诊断中融入专家引导的注意力监督，能有效弥合性能和可解释性之间的差距。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.08007v2">PDF</a> Accepted for publication in the proceedings of MICCAI Workshop on   Data Engineering in Medical Imaging 2025</p>
<p><strong>Summary</strong></p>
<p>本摘要介绍了医学图像分析面临的挑战，特别是由于缺少专家标注数据所带来的问题。为解决这些问题，提出了一种专家引导的可解释的少样本学习框架。该框架将放射科医生提供的感兴趣区域（ROI）融入模型训练，同时提高分类性能和解释性。通过利用Grad-CAM进行空间注意力监督，引入基于Dice相似度的解释损失，使模型注意力与诊断相关区域对齐。该解释损失与标准原型网络目标联合优化，鼓励模型在有限数据条件下关注临床有意义的特征。在BraTS和VinDr-CXR两个数据集上的评估显示，与无引导模型相比，该框架的准确率分别从77.09%提高到83.61%和从54.33%提高到73.29%。Grad-CAM可视化进一步证实，专家引导的训练能使模型注意力始终与诊断区域对齐，提高了预测可靠性和临床可信度。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>医学图像分析面临专家标注数据有限的挑战。</li>
<li>提出了一种专家引导的可解释的少样本学习框架，集成放射科医生提供的感兴趣区域（ROI）。</li>
<li>通过Grad-CAM进行空间注意力监督，并引入基于Dice相似度的解释损失。</li>
<li>解释损失与标准原型网络目标联合优化，提高模型在有限数据下的临床特征关注能力。</li>
<li>在BraTS和VinDr-CXR数据集上的评估显示，该框架显著提高了分类准确率。</li>
<li>专家引导的训练能提高模型的预测可靠性和临床可信度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.08007">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_Few-Shot/2509.08007v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_Few-Shot/2509.08007v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-13/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-13/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-13/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_I2I Translation/2509.08959v1/page_5_0.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-09-13  COCO-Urdu A Large-Scale Urdu Image-Caption Dataset with Multimodal   Quality Estimation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-13
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-13/Agent/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_Agent/2509.09135v1/page_0_0.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-09-13  Maximizing social welfare among EF1 allocations at the presence of two   types of agents
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-13
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30055.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
