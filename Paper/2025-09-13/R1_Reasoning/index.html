<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-13  FLUX-Reason-6M &amp; PRISM-Bench A Million-Scale Text-to-Image Reasoning   Dataset and Comprehensive Benchmark">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2508.15868v2/page_2_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-13
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-07
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    86 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-13-æ›´æ–°"><a href="#2025-09-13-æ›´æ–°" class="headerlink" title="2025-09-13 æ›´æ–°"></a>2025-09-13 æ›´æ–°</h1><h2 id="FLUX-Reason-6M-PRISM-Bench-A-Million-Scale-Text-to-Image-Reasoning-Dataset-and-Comprehensive-Benchmark"><a href="#FLUX-Reason-6M-PRISM-Bench-A-Million-Scale-Text-to-Image-Reasoning-Dataset-and-Comprehensive-Benchmark" class="headerlink" title="FLUX-Reason-6M &amp; PRISM-Bench: A Million-Scale Text-to-Image Reasoning   Dataset and Comprehensive Benchmark"></a>FLUX-Reason-6M &amp; PRISM-Bench: A Million-Scale Text-to-Image Reasoning   Dataset and Comprehensive Benchmark</h2><p><strong>Authors:Rongyao Fang, Aldrich Yu, Chengqi Duan, Linjiang Huang, Shuai Bai, Yuxuan Cai, Kun Wang, Si Liu, Xihui Liu, Hongsheng Li</strong></p>
<p>The advancement of open-source text-to-image (T2I) models has been hindered by the absence of large-scale, reasoning-focused datasets and comprehensive evaluation benchmarks, resulting in a performance gap compared to leading closed-source systems. To address this challenge, We introduce FLUX-Reason-6M and PRISM-Bench (Precise and Robust Image Synthesis Measurement Benchmark). FLUX-Reason-6M is a massive dataset consisting of 6 million high-quality FLUX-generated images and 20 million bilingual (English and Chinese) descriptions specifically designed to teach complex reasoning. The image are organized according to six key characteristics: Imagination, Entity, Text rendering, Style, Affection, and Composition, and design explicit Generation Chain-of-Thought (GCoT) to provide detailed breakdowns of image generation steps. The whole data curation takes 15,000 A100 GPU days, providing the community with a resource previously unattainable outside of large industrial labs. PRISM-Bench offers a novel evaluation standard with seven distinct tracks, including a formidable Long Text challenge using GCoT. Through carefully designed prompts, it utilizes advanced vision-language models for nuanced human-aligned assessment of prompt-image alignment and image aesthetics. Our extensive evaluation of 19 leading models on PRISM-Bench reveals critical performance gaps and highlights specific areas requiring improvement. Our dataset, benchmark, and evaluation code are released to catalyze the next wave of reasoning-oriented T2I generation. Project page: <a target="_blank" rel="noopener" href="https://flux-reason-6m.github.io/">https://flux-reason-6m.github.io/</a> . </p>
<blockquote>
<p>å¼€æºæ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ¨¡å‹çš„è¿›æ­¥å—åˆ°äº†ç¼ºä¹å¤§è§„æ¨¡ä»¥æ¨ç†ä¸ºä¸­å¿ƒçš„æ•°æ®é›†å’Œå…¨é¢è¯„ä¼°åŸºå‡†çš„é˜»ç¢ï¼Œä¸é¢†å…ˆçš„é—­æºç³»ç»Ÿç›¸æ¯”ï¼Œè¿™å¯¼è‡´äº†æ€§èƒ½ä¸Šçš„å·®è·ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†FLUX-Reason-6Må’ŒPRISM-Benchï¼ˆç²¾ç¡®å¯é çš„å›¾åƒåˆæˆæµ‹é‡åŸºå‡†ï¼‰ã€‚FLUX-Reason-6Mæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡æ•°æ®é›†ï¼ŒåŒ…å«600ä¸‡å¼ é«˜è´¨é‡çš„FLUXç”Ÿæˆå›¾åƒå’Œ2000ä¸‡æ¡ä¸“é—¨ç”¨äºæ•™æˆå¤æ‚æ¨ç†çš„åŒè¯­ï¼ˆè‹±è¯­å’Œä¸­æ–‡ï¼‰æè¿°ã€‚å›¾åƒæ ¹æ®å…­ä¸ªå…³é”®ç‰¹å¾è¿›è¡Œç»„ç»‡ï¼šæƒ³è±¡åŠ›ã€å®ä½“ã€æ–‡æœ¬æ¸²æŸ“ã€é£æ ¼ã€æƒ…æ„Ÿå’Œæ„å›¾ï¼Œå¹¶è®¾è®¡æ˜ç¡®çš„ç”Ÿæˆæ€ç»´é“¾ï¼ˆGCoTï¼‰ï¼Œä»¥æä¾›å›¾åƒç”Ÿæˆæ­¥éª¤çš„è¯¦ç»†åˆ†è§£ã€‚æ•´ä¸ªæ•°æ®é›†çš„æ•´ç†éœ€è¦15,000ä¸ªA100 GPUå¤©ï¼Œä¸ºç¤¾åŒºæä¾›äº†ä¸€ä¸ªä¹‹å‰åªæœ‰åœ¨å¤§å‹å·¥ä¸šå®éªŒå®¤æ‰èƒ½è·å¾—çš„èµ„æºã€‚PRISM-Benchæä¾›äº†ä¸€ä¸ªæ–°çš„è¯„ä¼°æ ‡å‡†ï¼ŒåŒ…æ‹¬ä¸ƒä¸ªä¸åŒçš„èµ›é“ï¼Œå…¶ä¸­æœ‰ä¸€ä¸ªå¼ºå¤§çš„é•¿æ–‡æœ¬æŒ‘æˆ˜ä½¿ç”¨GCoTã€‚å®ƒé€šè¿‡ç²¾å¿ƒè®¾è®¡çš„æç¤ºï¼Œåˆ©ç”¨å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®å¦™çš„ã€ç¬¦åˆäººç±»æ ‡å‡†çš„æç¤ºå›¾åƒå¯¹é½å’Œå›¾åƒç¾å­¦è¯„ä¼°ã€‚æˆ‘ä»¬åœ¨PRISM-Benchä¸Šå¯¹19ä¸ªé¢†å…ˆæ¨¡å‹è¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ï¼Œæ­ç¤ºäº†å…³é”®çš„æ€§èƒ½å·®è·å¹¶å¼ºè°ƒäº†éœ€è¦æ”¹è¿›çš„å…·ä½“é¢†åŸŸã€‚æˆ‘ä»¬çš„æ•°æ®é›†ã€åŸºå‡†å’Œè¯„ä¼°ä»£ç å·²å‘å¸ƒï¼Œä»¥å‚¬åŒ–ä¸‹ä¸€æ³¢ä»¥æ¨ç†ä¸ºå¯¼å‘çš„T2Iç”Ÿæˆã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://flux-reason-6m.github.io/%E3%80%82">https://flux-reason-6m.github.io/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.09680v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://flux-reason-6m.github.io/">https://flux-reason-6m.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¼€æ”¾æºä»£ç æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ¨¡å‹çš„è¿›æ­¥å—åˆ°é˜»ç¢çš„é—®é¢˜ï¼Œç¼ºä¹å¤§è§„æ¨¡ã€ä»¥æ¨ç†ä¸ºé‡ç‚¹çš„æ•°æ®é›†å’Œå…¨é¢çš„è¯„ä¼°åŸºå‡†ã€‚ä¸ºè§£å†³æ­¤æŒ‘æˆ˜ï¼Œå¼•å…¥äº†FLUX-Reason-6Mæ•°æ®é›†å’ŒPRISM-Benchè¯„ä¼°åŸºå‡†ã€‚FLUX-Reason-6MåŒ…å«600ä¸‡å¼ é«˜è´¨é‡å›¾åƒå’Œç›¸åº”çš„2äº¿åŒè¯­æè¿°ï¼Œç‰¹åˆ«è®¾è®¡ç”¨äºæ•™æˆå¤æ‚æ¨ç†ã€‚å›¾åƒæŒ‰å…­å¤§ç‰¹ç‚¹ç»„ç»‡ï¼Œå¹¶è®¾è®¡äº†è¯¦ç»†çš„å›¾åƒç”Ÿæˆæ­¥éª¤åˆ†è§£ã€‚PRISM-Benchåˆ™æä¾›äº†ä¸€ä¸ªæ–°é¢–çš„è¯„ä»·æ ‡å‡†ï¼Œå¯¹é«˜çº§è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®å¦™çš„ä»¥äººä¸ºæœ¬çš„è¯„ä¼°ã€‚å¯¹é¢†å…ˆæ¨¡å‹çš„å¹¿æ³›è¯„ä¼°æ­ç¤ºäº†å…³é”®çš„æ€§èƒ½å·®è·å¹¶å¼ºè°ƒäº†éœ€è¦æ”¹è¿›çš„é¢†åŸŸã€‚æ•°æ®é›†ã€åŸºå‡†å’Œè¯„ä¼°ä»£ç å·²å‘å¸ƒï¼Œä»¥æ¨åŠ¨ä¸‹ä¸€æ³¢ä»¥æ¨ç†ä¸ºå¯¼å‘çš„T2Iç”Ÿæˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼€æ”¾æºä»£ç æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ¨¡å‹çš„è¿›æ­¥å—åˆ°ç¼ºä¹å¤§è§„æ¨¡ã€ä»¥æ¨ç†ä¸ºé‡ç‚¹çš„æ•°æ®é›†å’Œå…¨é¢çš„è¯„ä¼°åŸºå‡†çš„é™åˆ¶ã€‚</li>
<li>FLUX-Reason-6Mæ˜¯ä¸€ä¸ªåŒ…å«6ç™¾ä¸‡é«˜è´¨é‡å›¾åƒå’Œç›¸åº”çš„åŒè¯­æè¿°çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œç”¨äºæ•™æˆå¤æ‚æ¨ç†ã€‚</li>
<li>å›¾åƒæŒ‰å…­å¤§ç‰¹ç‚¹ç»„ç»‡ï¼ŒåŒ…æ‹¬æƒ³è±¡åŠ›ã€å®ä½“ã€æ–‡æœ¬æ¸²æŸ“ç­‰ï¼Œå¹¶è®¾è®¡äº†è¯¦ç»†çš„å›¾åƒç”Ÿæˆæ­¥éª¤åˆ†è§£ã€‚</li>
<li>PRISM-Benchæä¾›äº†ä¸€ä¸ªæ–°é¢–çš„è¯„ä»·æ ‡å‡†ï¼ŒåŒ…å«ä¸ƒä¸ªä¸åŒçš„è½¨é“ï¼Œä¸ºå…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹æä¾›äº†å¾®å¦™çš„ä»¥äººä¸ºæœ¬çš„è¯„ä¼°æ–¹æ³•ã€‚</li>
<li>é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„æç¤ºï¼ŒPRISM-Benchåˆ©ç”¨å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œå›¾åƒæç¤ºå¯¹é½å’Œå›¾åƒç¾å­¦çš„è¯„ä¼°ã€‚</li>
<li>å¯¹é¢†å…ˆæ¨¡å‹çš„å¹¿æ³›è¯„ä¼°æ­ç¤ºäº†å…³é”®çš„æ€§èƒ½å·®è·ï¼Œç‰¹åˆ«æ˜¯åœ¨æŸäº›ç‰¹å®šé¢†åŸŸéœ€è¦æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.09680">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2509.09680v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2509.09680v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2509.09680v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2509.09680v1/page_5_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="SimpleVLA-RL-Scaling-VLA-Training-via-Reinforcement-Learning"><a href="#SimpleVLA-RL-Scaling-VLA-Training-via-Reinforcement-Learning" class="headerlink" title="SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning"></a>SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning</h2><p><strong>Authors:Haozhan Li, Yuxin Zuo, Jiale Yu, Yuhao Zhang, Zhaohui Yang, Kaiyan Zhang, Xuekai Zhu, Yuchen Zhang, Tianxing Chen, Ganqu Cui, Dehui Wang, Dingxiang Luo, Yuchen Fan, Youbang Sun, Jia Zeng, Jiangmiao Pang, Shanghang Zhang, Yu Wang, Yao Mu, Bowen Zhou, Ning Ding</strong></p>
<p>Vision-Language-Action (VLA) models have recently emerged as a powerful paradigm for robotic manipulation. Despite substantial progress enabled by large-scale pretraining and supervised fine-tuning (SFT), these models face two fundamental challenges: (i) the scarcity and high cost of large-scale human-operated robotic trajectories required for SFT scaling, and (ii) limited generalization to tasks involving distribution shift. Recent breakthroughs in Large Reasoning Models (LRMs) demonstrate that reinforcement learning (RL) can dramatically enhance step-by-step reasoning capabilities, raising a natural question: Can RL similarly improve the long-horizon step-by-step action planning of VLA? In this work, we introduce SimpleVLA-RL, an efficient RL framework tailored for VLA models. Building upon veRL, we introduce VLA-specific trajectory sampling, scalable parallelization, multi-environment rendering, and optimized loss computation. When applied to OpenVLA-OFT, SimpleVLA-RL achieves SoTA performance on LIBERO and even outperforms $\pi_0$ on RoboTwin 1.0&amp;2.0 with the exploration-enhancing strategies we introduce. SimpleVLA-RL not only reduces dependence on large-scale data and enables robust generalization, but also remarkably surpasses SFT in real-world tasks. Moreover, we identify a novel phenomenon &#96;&#96;pushcutâ€™â€™ during RL training, wherein the policy discovers previously unseen patterns beyond those seen in the previous training process. Github: <a target="_blank" rel="noopener" href="https://github.com/PRIME-RL/SimpleVLA-RL">https://github.com/PRIME-RL/SimpleVLA-RL</a> </p>
<blockquote>
<p>è§†è§‰è¯­è¨€åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹æœ€è¿‘ä½œä¸ºæœºå™¨äººæ“ä½œçš„ä¸€ç§å¼ºå¤§èŒƒå¼è€Œå‡ºç°ã€‚å°½ç®¡å¤§è§„æ¨¡é¢„è®­ç»ƒå’Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†è¿™äº›æ¨¡å‹ä»é¢ä¸´ä¸¤å¤§åŸºæœ¬æŒ‘æˆ˜ï¼šï¼ˆiï¼‰ç”¨äºSFTæ‰©å±•æ‰€éœ€çš„å¤§è§„æ¨¡äººç±»æ“ä½œæœºå™¨äººè½¨è¿¹çš„ç¨€ç¼ºæ€§å’Œé«˜æˆæœ¬ï¼›ï¼ˆiiï¼‰å¯¹æ¶‰åŠåˆ†å¸ƒè½¬ç§»çš„ä»»åŠ¡çš„æœ‰é™æ³›åŒ–èƒ½åŠ›ã€‚æœ€è¿‘åœ¨å¤§è§„æ¨¡æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰æ–¹é¢çš„çªç ´è¡¨æ˜ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯ä»¥å¤§å¹…æé«˜é€æ­¥æ¨ç†èƒ½åŠ›ï¼Œè¿™å¼•å‘äº†ä¸€ä¸ªè‡ªç„¶çš„é—®é¢˜ï¼šRLèƒ½å¦ä»¥ç±»ä¼¼æ–¹å¼æé«˜VLAçš„é•¿è¿œé€æ­¥è¡ŒåŠ¨è®¡åˆ’ï¼Ÿåœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸“ä¸ºVLAæ¨¡å‹å®šåˆ¶çš„ç®€å•VLA-RLé«˜æ•ˆå¼ºåŒ–å­¦ä¹ æ¡†æ¶ã€‚åŸºäºveRLï¼Œæˆ‘ä»¬å¼•å…¥äº†é’ˆå¯¹VLAçš„è½¨è¿¹é‡‡æ ·ã€å¯æ‰©å±•çš„å¹¶è¡ŒåŒ–ã€å¤šç¯å¢ƒæ¸²æŸ“å’Œä¼˜åŒ–æŸå¤±è®¡ç®—ã€‚å½“åº”ç”¨äºOpenVLA-OFTæ—¶ï¼ŒSimpleVLA-RLåœ¨LIBEROä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨æˆ‘ä»¬å¼•å…¥çš„æ¢ç´¢å¢å¼ºç­–ç•¥ä¸‹ï¼Œç”šè‡³åœ¨RoboTwin 1.0å’Œ2.0ä¸Šè¶…è¿‡äº†Ï€0ã€‚SimpleVLA-RLä¸ä»…å‡å°‘äº†å¤§è§„æ¨¡æ•°æ®çš„ä¾èµ–ï¼Œå®ç°äº†ç¨³å¥çš„æ³›åŒ–èƒ½åŠ›ï¼Œè€Œä¸”åœ¨ç°å®ä»»åŠ¡ä¸­æ˜¾è‘—è¶…è¶Šäº†SFTã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç°äº†ä¸€ä¸ªæ–°ç°è±¡â€œpushcutâ€ï¼Œåœ¨æ­¤ç°è±¡ä¸­ï¼Œç­–ç•¥å‘ç°äº†å…ˆå‰æœªè§çš„æ¨¡å¼ï¼Œè¿™äº›æ¨¡å¼è¶…å‡ºäº†ä¹‹å‰è®­ç»ƒè¿‡ç¨‹ä¸­çš„èŒƒå›´ã€‚Githubåœ°å€ï¼š<a target="_blank" rel="noopener" href="https://github.com/PRIME-RL/SimpleVLA-RL">https://github.com/PRIME-RL/SimpleVLA-RL</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.09674v1">PDF</a> </p>
<p><strong>Summary</strong><br>     è¿‘æœŸï¼Œè§†è§‰è¯­è¨€åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹åœ¨æœºå™¨äººæ“ä½œé¢†åŸŸå±•ç°å‡ºå¼ºå¤§çš„æ½œåŠ›ã€‚å°½ç®¡é€šè¿‡å¤§è§„æ¨¡é¢„è®­ç»ƒå’Œç²¾ç»†ç›‘ç£è®­ç»ƒï¼ˆSFTï¼‰å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä»é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šä¸€æ˜¯éœ€è¦å¤§è§„æ¨¡äººç±»æ“ä½œçš„æœºå™¨äººè½¨è¿¹æ•°æ®ï¼Œæˆæœ¬é«˜æ˜‚ä¸”ç¨€ç¼ºï¼›äºŒæ˜¯æ¨¡å‹åœ¨ä»»åŠ¡åˆ†å¸ƒå˜åŒ–æ—¶çš„æ³›åŒ–èƒ½åŠ›å—é™ã€‚æœ¬ç ”ç©¶å¼•å…¥SimpleVLA-RLï¼Œä¸€ä¸ªé’ˆå¯¹VLAæ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥VLAç‰¹å®šçš„è½¨è¿¹é‡‡æ ·ã€å¯æ‰©å±•çš„å¹¶è¡ŒåŒ–ã€å¤šç¯å¢ƒæ¸²æŸ“å’Œä¼˜åŒ–æŸå¤±è®¡ç®—ç­‰æŠ€æœ¯ï¼Œå®ç°äº†åœ¨OpenVLA-OFTä¸Šçš„å“è¶Šæ€§èƒ½ã€‚SimpleVLA-RLä¸ä»…é™ä½äº†å¯¹å¤§è§„æ¨¡æ•°æ®çš„ä¾èµ–ï¼Œæé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œè¿˜åœ¨çœŸå®ä»»åŠ¡ä¸­è¶…è¶Šäº†SFTã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å‘ç°äº†RLè®­ç»ƒä¸­çš„æ–°ç°è±¡â€”â€”â€œpushcutâ€ï¼Œå³ç­–ç•¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç°äº†ä¹‹å‰æœªè§è¿‡çš„æ¨¡å¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLAæ¨¡å‹åœ¨æœºå™¨äººæ“ä½œé¢†åŸŸå…·æœ‰å¼ºå¤§æ½œåŠ›ï¼Œä½†ä»é¢ä¸´æ•°æ®è·å–å’Œæ³›åŒ–èƒ½åŠ›çš„æŒ‘æˆ˜ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯ä»¥æé«˜VLAæ¨¡å‹çš„é•¿æœŸè§„åˆ’èƒ½åŠ›ã€‚</li>
<li>SimpleVLA-RLæ¡†æ¶é’ˆå¯¹VLAæ¨¡å‹è¿›è¡Œäº†ä¼˜åŒ–ï¼ŒåŒ…æ‹¬è½¨è¿¹é‡‡æ ·ã€å¹¶è¡ŒåŒ–ã€å¤šç¯å¢ƒæ¸²æŸ“å’ŒæŸå¤±è®¡ç®—ã€‚</li>
<li>SimpleVLA-RLåœ¨å¤šä¸ªä»»åŠ¡ä¸Šå®ç°äº†å“è¶Šæ€§èƒ½ï¼Œå¹¶é™ä½äº†å¯¹å¤§è§„æ¨¡æ•°æ®çš„ä¾èµ–ã€‚</li>
<li>SimpleVLA-RLæé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨çœŸå®ä»»åŠ¡ä¸­è¶…è¶Šäº†SFTã€‚</li>
<li>RLè®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°æ–°ç°è±¡â€”â€”â€œpushcutâ€ï¼Œç­–ç•¥èƒ½å‘ç°ä¹‹å‰æœªè§è¿‡çš„æ¨¡å¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.09674">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2509.09674v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2509.09674v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="LoCoBench-A-Benchmark-for-Long-Context-Large-Language-Models-in-Complex-Software-Engineering"><a href="#LoCoBench-A-Benchmark-for-Long-Context-Large-Language-Models-in-Complex-Software-Engineering" class="headerlink" title="LoCoBench: A Benchmark for Long-Context Large Language Models in Complex   Software Engineering"></a>LoCoBench: A Benchmark for Long-Context Large Language Models in Complex   Software Engineering</h2><p><strong>Authors:Jielin Qiu, Zuxin Liu, Zhiwei Liu, Rithesh Murthy, Jianguo Zhang, Haolin Chen, Shiyu Wang, Ming Zhu, Liangwei Yang, Juntao Tan, Zhepeng Cen, Cheng Qian, Shelby Heinecke, Weiran Yao, Silvio Savarese, Caiming Xiong, Huan Wang</strong></p>
<p>The emergence of long-context language models with context windows extending to millions of tokens has created new opportunities for sophisticated code understanding and software development evaluation. We propose LoCoBench, a comprehensive benchmark specifically designed to evaluate long-context LLMs in realistic, complex software development scenarios. Unlike existing code evaluation benchmarks that focus on single-function completion or short-context tasks, LoCoBench addresses the critical evaluation gap for long-context capabilities that require understanding entire codebases, reasoning across multiple files, and maintaining architectural consistency across large-scale software systems. Our benchmark provides 8,000 evaluation scenarios systematically generated across 10 programming languages, with context lengths spanning 10K to 1M tokens, a 100x variation that enables precise assessment of long-context performance degradation in realistic software development settings. LoCoBench introduces 8 task categories that capture essential long-context capabilities: architectural understanding, cross-file refactoring, multi-session development, bug investigation, feature implementation, code comprehension, integration testing, and security analysis. Through a 5-phase pipeline, we create diverse, high-quality scenarios that challenge LLMs to reason about complex codebases at unprecedented scale. We introduce a comprehensive evaluation framework with 17 metrics across 4 dimensions, including 8 new evaluation metrics, combined in a LoCoBench Score (LCBS). Our evaluation of state-of-the-art long-context models reveals substantial performance gaps, demonstrating that long-context understanding in complex software development represents a significant unsolved challenge that demands more attention. LoCoBench is released at: <a target="_blank" rel="noopener" href="https://github.com/SalesforceAIResearch/LoCoBench">https://github.com/SalesforceAIResearch/LoCoBench</a>. </p>
<blockquote>
<p>é•¿ä¸Šä¸‹æ–‡è¯­è¨€æ¨¡å‹çš„å…´èµ·ï¼Œå…¶ä¸Šä¸‹æ–‡çª—å£æ‰©å±•è‡³æ•°ç™¾ä¸‡ä¸ªä»¤ç‰Œï¼Œä¸ºå¤æ‚çš„ä»£ç ç†è§£å’Œè½¯ä»¶å¼€å‘è¯„ä¼°æä¾›äº†æ–°çš„æœºä¼šã€‚æˆ‘ä»¬æå‡ºäº†LoCoBenchï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºè¯„ä¼°é•¿ä¸Šä¸‹æ–‡LLMè€Œåœ¨ç°å®å¤æ‚è½¯ä»¶å¼€å‘åœºæ™¯ä¸­è®¾è®¡çš„å…¨é¢åŸºå‡†æµ‹è¯•ã€‚ä¸ç°æœ‰çš„ä»£ç è¯„ä¼°åŸºå‡†æµ‹è¯•ä¸åŒï¼Œè¿™äº›æµ‹è¯•ä¸“æ³¨äºå•åŠŸèƒ½å®Œæˆæˆ–çŸ­ä¸Šä¸‹æ–‡ä»»åŠ¡ï¼Œè€ŒLoCoBenchè§£å†³äº†å¯¹é•¿ä¸Šä¸‹æ–‡èƒ½åŠ›çš„å…³é”®è¯„ä¼°ç©ºç™½ï¼Œè¿™éœ€è¦ç†è§£æ•´ä¸ªä»£ç åº“ã€è·¨å¤šä¸ªæ–‡ä»¶è¿›è¡Œæ¨ç†ä»¥åŠåœ¨å¤§å‹è½¯ä»¶ç³»ç»Ÿä¸­ä¿æŒæ¶æ„ä¸€è‡´æ€§ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•æä¾›äº†8000ä¸ªè¯„ä¼°åœºæ™¯ï¼Œè¿™äº›åœºæ™¯ç³»ç»Ÿåœ°åœ¨10ç§ç¼–ç¨‹è¯­è¨€ä¸­ç”Ÿæˆï¼Œä¸Šä¸‹æ–‡é•¿åº¦ä»1ä¸‡åˆ°ç™¾ä¸‡ä»¤ç‰Œä¸ç­‰ï¼Œè¿™ç§ç™¾å€çš„å˜åŒ–ä½¿å¾—èƒ½å¤Ÿåœ¨ç°å®è½¯ä»¶å¼€å‘ç¯å¢ƒä¸­ç²¾ç¡®è¯„ä¼°é•¿ä¸Šä¸‹æ–‡æ€§èƒ½ä¸‹é™çš„æƒ…å†µã€‚LoCoBenchå¼•å…¥äº†8ä¸ªä»»åŠ¡ç±»åˆ«ï¼Œæ¶µç›–äº†å…³é”®çš„é•¿ä¸Šä¸‹æ–‡èƒ½åŠ›ï¼šæ¶æ„ç†è§£ã€è·¨æ–‡ä»¶é‡æ„ã€å¤šä¼šè¯å¼€å‘ã€æ•…éšœè°ƒæŸ¥ã€åŠŸèƒ½å®ç°ã€ä»£ç ç†è§£ã€é›†æˆæµ‹è¯•å’Œå®‰å…¨åˆ†æã€‚é€šè¿‡5é˜¶æ®µçš„ç®¡é“ï¼Œæˆ‘ä»¬åˆ›å»ºäº†å¤šæ ·åŒ–ä¸”é«˜è´¨é‡çš„åœºæ™¯ï¼ŒæŒ‘æˆ˜LLMä»¥å‰æ‰€æœªæœ‰çš„è§„æ¨¡å¯¹å¤æ‚ä»£ç åº“è¿›è¡Œæ¨ç†ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼ŒåŒ…æ‹¬4ä¸ªç»´åº¦å…±çš„17é¡¹æŒ‡æ ‡ï¼Œå…¶ä¸­è¿˜åŒ…æ‹¬8é¡¹æ–°çš„è¯„ä¼°æŒ‡æ ‡ï¼Œè¿™äº›æŒ‡æ ‡ç»„åˆæˆLoCoBench Scoreï¼ˆLCBSï¼‰ã€‚æˆ‘ä»¬å¯¹æœ€å…ˆè¿›çš„é•¿ä¸Šä¸‹æ–‡æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œæ­ç¤ºäº†æ˜¾è‘—çš„æ€§èƒ½å·®è·ï¼Œè¡¨æ˜åœ¨å¤æ‚è½¯ä»¶å¼€å‘ä¸­çš„é•¿ä¸Šä¸‹æ–‡ç†è§£æ˜¯ä¸€ä¸ªé‡è¦çš„æœªè§£å†³é—®é¢˜ï¼Œéœ€è¦æ›´å¤šå…³æ³¨ã€‚LoCoBenchå‘å¸ƒåœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/SalesforceAIResearch/LoCoBench%E3%80%82">https://github.com/SalesforceAIResearch/LoCoBenchã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.09614v1">PDF</a> 53 pages</p>
<p><strong>Summary</strong></p>
<p>é•¿è¯­å¢ƒè¯­è¨€æ¨¡å‹çš„æ¶Œç°ï¼Œä¸ºå¤æ‚çš„ä»£ç ç†è§£å’Œè½¯ä»¶å¼€å‘è¯„ä¼°æä¾›äº†æ–°çš„æœºä¼šã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†LoCoBenchï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºè¯„ä¼°é•¿è¯­å¢ƒLLMåœ¨ç°å®å¤æ‚è½¯ä»¶å¼€å‘åœºæ™¯ä¸­çš„è¡¨ç°è€Œè®¾è®¡çš„ç»¼åˆåŸºå‡†æµ‹è¯•ã€‚ä¸ç°æœ‰çš„ä»£ç è¯„ä¼°åŸºå‡†ä¸åŒï¼ŒLoCoBenchç€é‡äºéœ€è¦ç†è§£æ•´ä¸ªä»£ç åº“ã€è·¨å¤šä¸ªæ–‡ä»¶æ¨ç†ä»¥åŠåœ¨å¤§å‹è½¯ä»¶ç³»ç»Ÿä¸­ä¿æŒæ¶æ„ä¸€è‡´æ€§çš„é•¿è¯­å¢ƒèƒ½åŠ›è¯„ä¼°ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•æä¾›äº†8000ä¸ªè¯„ä¼°åœºæ™¯ï¼Œè¿™äº›åœºæ™¯ç³»ç»Ÿåœ°æ¶µç›–äº†10ç§ç¼–ç¨‹è¯­è¨€ï¼Œè¯­å¢ƒé•¿åº¦ä»10Kåˆ°1Mä»¤ç‰Œï¼Œå˜åŒ–èŒƒå›´è¾¾åˆ°100å€ï¼Œèƒ½å¤Ÿåœ¨ç°å®è½¯ä»¶å¼€å‘ç¯å¢ƒä¸­ç²¾ç¡®è¯„ä¼°é•¿è¯­å¢ƒæ€§èƒ½ä¸‹é™æƒ…å†µã€‚LoCoBenchå¼•å…¥äº†8ä¸ªä»»åŠ¡ç±»åˆ«ï¼Œæ¶µç›–äº†é‡è¦çš„é•¿è¯­å¢ƒèƒ½åŠ›ï¼ŒåŒ…æ‹¬æ¶æ„ç†è§£ã€è·¨æ–‡ä»¶é‡æ„ã€å¤šä¼šè¯å¼€å‘ã€æ•…éšœè°ƒæŸ¥ã€åŠŸèƒ½å®ç°ã€ä»£ç ç†è§£ã€é›†æˆæµ‹è¯•å’Œå®‰å…¨åˆ†æã€‚é€šè¿‡5é˜¶æ®µç®¡é“ï¼Œæˆ‘ä»¬åˆ›å»ºäº†å¤šæ ·åŒ–ã€é«˜è´¨é‡çš„åœºæ™¯ï¼ŒæŒ‘æˆ˜LLMsä»¥å‰æ‰€æœªæœ‰çš„è§„æ¨¡å¯¹å¤æ‚ä»£ç åº“è¿›è¡Œæ¨ç†ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ¡†æ¶åŒ…å«4ä¸ªç»´åº¦çš„17ä¸ªæŒ‡æ ‡ï¼ŒåŒ…æ‹¬8ä¸ªæ–°è¯„ä¼°æŒ‡æ ‡ï¼Œå¹¶ç»¼åˆå½¢æˆLoCoBenchè¯„åˆ†ï¼ˆLCBSï¼‰ã€‚æˆ‘ä»¬å¯¹å…ˆè¿›çš„é•¿è¯­å¢ƒæ¨¡å‹çš„è¯„ä»·æ˜¾ç¤ºï¼Œå­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®è·ï¼Œè¿™è¡¨æ˜å¤æ‚è½¯ä»¶å¼€å‘ä¸­çš„é•¿è¯­å¢ƒç†è§£æ˜¯ä¸€ä¸ªé‡å¤§æœªè§£å†³çš„é—®é¢˜ï¼Œéœ€è¦æ›´å¤šå…³æ³¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LoCoBenchæ˜¯ä¸“é—¨ä¸ºè¯„ä¼°é•¿è¯­å¢ƒLLMè€Œè®¾è®¡çš„ç»¼åˆåŸºå‡†æµ‹è¯•ã€‚</li>
<li>ä¸å…¶ä»–ä»£ç è¯„ä¼°åŸºå‡†ä¸åŒï¼ŒLoCoBenchå¼ºè°ƒé•¿è¯­å¢ƒèƒ½åŠ›ï¼Œå¦‚æ•´ä¸ªä»£ç åº“çš„ç†è§£ã€è·¨å¤šä¸ªæ–‡ä»¶çš„æ¨ç†ä»¥åŠå¤§å‹è½¯ä»¶ç³»ç»Ÿä¸­çš„æ¶æ„ä¸€è‡´æ€§ã€‚</li>
<li>LoCoBenchæä¾›äº†8000ä¸ªç³»ç»Ÿç”Ÿæˆçš„è¯„ä¼°åœºæ™¯ï¼Œè¯­å¢ƒé•¿åº¦å˜åŒ–èŒƒå›´å¤§ï¼Œæ¶µç›–10ç§ç¼–ç¨‹è¯­è¨€ã€‚</li>
<li>åŸºå‡†æµ‹è¯•åŒ…æ‹¬8ä¸ªä»»åŠ¡ç±»åˆ«ï¼šæ¶æ„ç†è§£ã€è·¨æ–‡ä»¶é‡æ„ã€å¤šä¼šè¯å¼€å‘ç­‰ã€‚</li>
<li>é€šè¿‡5é˜¶æ®µç®¡é“åˆ›å»ºå¤šæ ·åŒ–ã€é«˜è´¨é‡çš„åœºæ™¯ï¼Œä»¥æŒ‘æˆ˜LLMså¯¹å¤æ‚ä»£ç åº“çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªåŒ…å«17ä¸ªæŒ‡æ ‡çš„å…¨é¢è¯„ä¼°æ¡†æ¶ï¼ŒåŒ…æ‹¬8ä¸ªæ–°è¯„ä¼°æŒ‡æ ‡ï¼Œå½¢æˆLoCoBenchè¯„åˆ†ï¼ˆLCBSï¼‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.09614">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2509.09614v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2509.09614v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2509.09614v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="OmniEVA-Embodied-Versatile-Planner-via-Task-Adaptive-3D-Grounded-and-Embodiment-aware-Reasoning"><a href="#OmniEVA-Embodied-Versatile-Planner-via-Task-Adaptive-3D-Grounded-and-Embodiment-aware-Reasoning" class="headerlink" title="OmniEVA: Embodied Versatile Planner via Task-Adaptive 3D-Grounded and   Embodiment-aware Reasoning"></a>OmniEVA: Embodied Versatile Planner via Task-Adaptive 3D-Grounded and   Embodiment-aware Reasoning</h2><p><strong>Authors:Yuecheng Liu, Dafeng Chi, Shiguang Wu, Zhanguang Zhang, Yuzheng Zhuang, Bowen Yang, He Zhu, Lingfeng Zhang, Pengwei Xie, David Gamaliel Arcos Bravo, Yingxue Zhang, Jianye Hao, Xingyue Quan</strong></p>
<p>Recent advances in multimodal large language models (MLLMs) have opened new opportunities for embodied intelligence, enabling multimodal understanding, reasoning, and interaction, as well as continuous spatial decision-making. Nevertheless, current MLLM-based embodied systems face two critical limitations. First, Geometric Adaptability Gap: models trained solely on 2D inputs or with hard-coded 3D geometry injection suffer from either insufficient spatial information or restricted 2D generalization, leading to poor adaptability across tasks with diverse spatial demands. Second, Embodiment Constraint Gap: prior work often neglects the physical constraints and capacities of real robots, resulting in task plans that are theoretically valid but practically infeasible.To address these gaps, we introduce OmniEVA â€“ an embodied versatile planner that enables advanced embodied reasoning and task planning through two pivotal innovations: (1) a Task-Adaptive 3D Grounding mechanism, which introduces a gated router to perform explicit selective regulation of 3D fusion based on contextual requirements, enabling context-aware 3D grounding for diverse embodied tasks. (2) an Embodiment-Aware Reasoning framework that jointly incorporates task goals and embodiment constraints into the reasoning loop, resulting in planning decisions that are both goal-directed and executable. Extensive experimental results demonstrate that OmniEVA not only achieves state-of-the-art general embodied reasoning performance, but also exhibits a strong ability across a wide range of downstream scenarios. Evaluations of a suite of proposed embodied benchmarks, including both primitive and composite tasks, confirm its robust and versatile planning capabilities. Project page: <a target="_blank" rel="noopener" href="https://omnieva.github.io/">https://omnieva.github.io</a> </p>
<blockquote>
<p>æœ€è¿‘çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è¿›æ­¥ä¸ºåµŒå…¥å¼æ™ºèƒ½å¸¦æ¥äº†æ–°çš„æœºä¼šï¼Œå®ç°äº†å¤šæ¨¡æ€ç†è§£ã€æ¨ç†å’Œäº¤äº’ï¼Œä»¥åŠè¿ç»­çš„ç©ºé—´å†³ç­–ã€‚ç„¶è€Œï¼Œå½“å‰åŸºäºMLLMçš„åµŒå…¥å¼ç³»ç»Ÿé¢ä¸´ä¸¤ä¸ªå…³é”®å±€é™ã€‚é¦–å…ˆï¼Œå‡ ä½•é€‚åº”æ€§å·®è·ï¼šä»…é€šè¿‡2Dè¾“å…¥æˆ–ç¡¬ç¼–ç çš„3Då‡ ä½•æ³¨å…¥è®­ç»ƒçš„æ¨¡å‹ï¼Œç¼ºä¹ç©ºé—´ä¿¡æ¯æˆ–2Dæ³›åŒ–èƒ½åŠ›å—é™ï¼Œå¯¼è‡´åœ¨å…·æœ‰å„ç§ç©ºé—´éœ€æ±‚çš„ä»»åŠ¡ä¹‹é—´é€‚åº”æ€§è¾ƒå·®ã€‚å…¶æ¬¡ï¼Œä½“ç°çº¦æŸå·®è·ï¼šå…ˆå‰çš„å·¥ä½œå¾€å¾€å¿½è§†äº†çœŸå®æœºå™¨äººçš„ç‰©ç†çº¦æŸå’Œèƒ½åŠ›ï¼Œå¯¼è‡´ä»»åŠ¡è®¡åˆ’åœ¨ç†è®ºä¸Šæœ‰æ•ˆï¼Œä½†åœ¨å®è·µä¸­å´ä¸å¯è¡Œã€‚ä¸ºäº†è§£å†³è¿™äº›å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†OmniEVAâ€”â€”ä¸€ä¸ªåµŒå…¥å¼é€šç”¨è§„åˆ’å™¨ï¼Œå®ƒé€šè¿‡ä¸¤ä¸ªå…³é”®åˆ›æ–°ç‚¹å®ç°äº†å…ˆè¿›çš„åµŒå…¥å¼æ¨ç†å’Œä»»åŠ¡è§„åˆ’ï¼šï¼ˆ1ï¼‰ä»»åŠ¡è‡ªé€‚åº”çš„3Dæ¥åœ°æœºåˆ¶ï¼Œå®ƒå¼•å…¥äº†ä¸€ä¸ªé—¨æ§è·¯ç”±å™¨ï¼Œæ ¹æ®ä¸Šä¸‹æ–‡è¦æ±‚è¿›è¡Œæ˜¾å¼çš„é€‰æ‹©æ€§è°ƒèŠ‚3Dèåˆï¼Œä¸ºå®ç°å¤šæ ·åŒ–çš„åµŒå…¥å¼ä»»åŠ¡çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥3Dæ¥åœ°ã€‚ ï¼ˆ2ï¼‰æ„ŸçŸ¥ä½“ç°çš„æ¨ç†æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†ä»»åŠ¡ç›®æ ‡å’Œä½“ç°çº¦æŸçº³å…¥æ¨ç†å¾ªç¯ä¸­ï¼Œä»è€Œåšå‡ºæ—¢ä»¥ç›®æ ‡ä¸ºå¯¼å‘åˆå¯è¡Œçš„è§„åˆ’å†³ç­–ã€‚å¤§é‡çš„å®éªŒç»“æœè¯æ˜ï¼ŒOmniEVAä¸ä»…è¾¾åˆ°äº†æœ€å…ˆè¿›çš„é€šç”¨åµŒå…¥å¼æ¨ç†æ€§èƒ½ï¼Œè€Œä¸”åœ¨å¹¿æ³›çš„ä¸‹æ¸¸åœºæ™¯ä¸­è¡¨ç°å‡ºäº†å¼ºå¤§çš„èƒ½åŠ›ã€‚å¯¹ä¸€ç³»åˆ—æå‡ºçš„åµŒå…¥å¼åŸºå‡†æµ‹è¯•çš„è¯„ä»·ï¼ŒåŒ…æ‹¬åŸå§‹ä»»åŠ¡å’Œå¤åˆä»»åŠ¡ï¼Œè¯å®äº†å…¶ç¨³å¥å’Œé€šç”¨çš„è§„åˆ’èƒ½åŠ›ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://omnieva.github.io/">https://omnieva.github.io</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.09332v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è¿‘æœŸå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è¿›æ­¥ä¸ºå®ä½“æ™ºèƒ½å¸¦æ¥äº†æ–°çš„æœºé‡ï¼Œå¼€å¯äº†å¤šæ¨¡æ€ç†è§£ã€æ¨ç†å’Œäº¤äº’ä»¥åŠè¿ç»­ç©ºé—´å†³ç­–åˆ¶å®šçš„å¯èƒ½æ€§ã€‚ç„¶è€Œï¼Œå½“å‰åŸºäºMLLMçš„å®ä½“ç³»ç»Ÿé¢ä¸´ä¸¤å¤§å±€é™ï¼šå‡ ä½•é€‚åº”å·®è·å’Œå®ä½“çº¦æŸå·®è·ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†OmniEVAâ€”â€”ä¸€ç§å®ä½“é€šç”¨è§„åˆ’å™¨ï¼Œé€šè¿‡ä¸¤é¡¹åˆ›æ–°æŠ€æœ¯è§£å†³è¿™äº›é—®é¢˜ï¼šä»»åŠ¡è‡ªé€‚åº”çš„3Dæ¥åœ°æœºåˆ¶å’Œå®ä½“æ„ŸçŸ¥æ¨ç†æ¡†æ¶ã€‚è¯¥è§„åˆ’å™¨å®ç°äº†å…ˆè¿›çš„å®ä½“æ¨ç†å’Œä»»åŠ¡è§„åˆ’ï¼Œå®ç°äº†ä¸Šä¸‹æ–‡ä¸­ç›®æ ‡å¯¼å‘ä¸”å¯æ‰§è¡Œçš„è§„åˆ’å†³ç­–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOmniEVAä¸ä»…å®ç°äº†æœ€å…ˆè¿›çš„å®ä½“æ¨ç†æ€§èƒ½ï¼Œè€Œä¸”åœ¨å¤šç§ä¸‹æ¸¸åœºæ™¯ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„è¿›æ­¥ä¸ºå®ä½“æ™ºèƒ½å¸¦æ¥äº†æ–°æœºé‡ï¼ŒåŒ…æ‹¬å¤šæ¨¡æ€ç†è§£ã€æ¨ç†å’Œäº¤äº’ä»¥åŠè¿ç»­ç©ºé—´å†³ç­–åˆ¶å®šã€‚</li>
<li>å½“å‰åŸºäºMLLMçš„å®ä½“ç³»ç»Ÿé¢ä¸´ä¸¤å¤§å±€é™ï¼šå‡ ä½•é€‚åº”å·®è·å’Œå®ä½“çº¦æŸå·®è·ã€‚</li>
<li>OmniEVAæ˜¯ä¸€ç§å®ä½“é€šç”¨è§„åˆ’å™¨ï¼Œé€šè¿‡ä»»åŠ¡è‡ªé€‚åº”çš„3Dæ¥åœ°æœºåˆ¶å’Œå®ä½“æ„ŸçŸ¥æ¨ç†æ¡†æ¶è§£å†³ä¸Šè¿°å±€é™ã€‚</li>
<li>OmniEVAå®ç°äº†å…ˆè¿›çš„å®ä½“æ¨ç†å’Œä»»åŠ¡è§„åˆ’ï¼Œå¯ä»¥åœ¨å¤šç§ä¸‹æ¸¸åœºæ™¯ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ã€‚</li>
<li>OmniEVAçš„è§„åˆ’å†³ç­–æ˜¯ç›®æ ‡å¯¼å‘ä¸”å¯æ‰§è¡Œçš„ã€‚</li>
<li>é€šè¿‡å®éªŒéªŒè¯ï¼ŒOmniEVAä¸ä»…è¾¾åˆ°äº†æœ€å…ˆè¿›çš„å®ä½“æ¨ç†æ€§èƒ½ï¼Œè€Œä¸”åœ¨å¤šç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºç¨³å¥å’Œé€šç”¨çš„è§„åˆ’èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.09332">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2509.09332v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2509.09332v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2509.09332v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Bona-fide-Cross-Testing-Reveals-Weak-Spot-in-Audio-Deepfake-Detection-Systems"><a href="#Bona-fide-Cross-Testing-Reveals-Weak-Spot-in-Audio-Deepfake-Detection-Systems" class="headerlink" title="Bona fide Cross Testing Reveals Weak Spot in Audio Deepfake Detection   Systems"></a>Bona fide Cross Testing Reveals Weak Spot in Audio Deepfake Detection   Systems</h2><p><strong>Authors:Chin Yuen Kwok, Jia Qi Yip, Zhen Qiu, Chi Hung Chi, Kwok Yan Lam</strong></p>
<p>Audio deepfake detection (ADD) models are commonly evaluated using datasets that combine multiple synthesizers, with performance reported as a single Equal Error Rate (EER). However, this approach disproportionately weights synthesizers with more samples, underrepresenting others and reducing the overall reliability of EER. Additionally, most ADD datasets lack diversity in bona fide speech, often featuring a single environment and speech style (e.g., clean read speech), limiting their ability to simulate real-world conditions. To address these challenges, we propose bona fide cross-testing, a novel evaluation framework that incorporates diverse bona fide datasets and aggregates EERs for more balanced assessments. Our approach improves robustness and interpretability compared to traditional evaluation methods. We benchmark over 150 synthesizers across nine bona fide speech types and release a new dataset to facilitate further research at <a target="_blank" rel="noopener" href="https://github.com/cyaaronk/audio_deepfake_eval">https://github.com/cyaaronk/audio_deepfake_eval</a>. </p>
<blockquote>
<p>éŸ³é¢‘æ·±åº¦ä¼ªé€ æ£€æµ‹ï¼ˆADDï¼‰æ¨¡å‹é€šå¸¸ä½¿ç”¨ç»“åˆå¤šç§åˆæˆå™¨çš„æ•°æ®é›†è¿›è¡Œè¯„ä¼°ï¼Œæ€§èƒ½ä»¥å•ä¸€çš„ç­‰é”™è¯¯ç‡ï¼ˆEERï¼‰æŠ¥å‘Šã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•ä¼šä¸æˆæ¯”ä¾‹åœ°åŠ é‡æ ·æœ¬æ›´å¤šçš„åˆæˆå™¨çš„æƒé‡ï¼Œä½ä¼°å…¶ä»–åˆæˆå™¨ï¼Œé™ä½æ•´ä½“EERçš„å¯é æ€§ã€‚æ­¤å¤–ï¼Œå¤§å¤šæ•°ADDæ•°æ®é›†åœ¨çœŸå®è¯­éŸ³æ–¹é¢ç¼ºä¹å¤šæ ·æ€§ï¼Œé€šå¸¸åªåŒ…å«ä¸€ç§ç¯å¢ƒå’Œè¯­éŸ³é£æ ¼ï¼ˆä¾‹å¦‚ï¼Œå¹²å‡€çš„é˜…è¯»è¯­éŸ³ï¼‰ï¼Œé™åˆ¶äº†å®ƒä»¬æ¨¡æ‹ŸçœŸå®ä¸–ç•Œæ¡ä»¶çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†çœŸå®è·¨æµ‹è¯•è¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†å¤šç§çœŸå®æ•°æ®é›†å¹¶æ±‡æ€»EERï¼Œä»¥è¿›è¡Œæ›´å¹³è¡¡çš„è¯„ä¼°ã€‚ä¸ä¼ ç»Ÿè¯„ä¼°æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æé«˜äº†ç¨³å¥æ€§å’Œå¯è§£é‡Šæ€§ã€‚æˆ‘ä»¬åœ¨ä¹ç§çœŸå®è¯­éŸ³ç±»å‹ä¸Šå¯¹è¶…è¿‡150ç§åˆæˆå™¨è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œå¹¶åœ¨<a target="_blank" rel="noopener" href="https://github.com/cyaaronk/audio_deepfake_eval%E4%B8%8A%E5%8F%91%E5%B8%83%E4%BA%86%E4%B8%80%E4%B8%AA%E6%96%B0%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86%EF%BC%8C%E4%BB%A5%E4%BF%83%E8%BF%9B%E8%BF%9B%E4%B8%80%E6%AD%A5%E7%9A%84%E7%A0%94%E7%A9%B6%E3%80%82">https://github.com/cyaaronk/audio_deepfake_evalä¸Šå‘å¸ƒäº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†ï¼Œä»¥ä¿ƒè¿›è¿›ä¸€æ­¥çš„ç ”ç©¶ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.09204v1">PDF</a> Published in Interspeech 2025</p>
<p><strong>Summary</strong>ï¼šéŸ³é¢‘æ·±åº¦ä¼ªé€ æ£€æµ‹æ¨¡å‹å¸¸ç”¨ç»“åˆå¤šç§åˆæˆå™¨çš„æ•°æ®é›†è¿›è¡Œè¯„ä¼°ï¼Œä»¥å•ä¸€é”™è¯¯ç­‰ä»·ç‡æŠ¥å‘Šæ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•ä¼šè¿‡å¤šåœ°é‡è§†æ ·æœ¬è¾ƒå¤šçš„åˆæˆå™¨ï¼Œå¿½è§†å…¶ä»–åˆæˆå™¨ï¼Œé™ä½é”™è¯¯ç­‰ä»·ç‡çš„æ€»ä½“å¯é æ€§ã€‚æ­¤å¤–ï¼Œå¤§å¤šæ•°éŸ³é¢‘æ·±åº¦ä¼ªé€ æ£€æµ‹æ•°æ®é›†ç¼ºä¹çœŸå®è¯­éŸ³çš„å¤šæ ·æ€§ï¼Œé€šå¸¸åªæ¶‰åŠå•ä¸€ç¯å¢ƒå’Œè¯­éŸ³é£æ ¼ï¼ˆå¦‚æ¸…æ™°é˜…è¯»è¯­éŸ³ï¼‰ï¼Œæ— æ³•æ¨¡æ‹ŸçœŸå®ä¸–ç•Œæ¡ä»¶ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†çœŸå®è¯­éŸ³äº¤å‰æµ‹è¯•è¿™ä¸€æ–°é¢–è¯„ä¼°æ¡†æ¶ï¼Œå®ƒèå…¥å¤šæ ·çš„çœŸå®è¯­éŸ³æ•°æ®é›†å¹¶æ±‡æ€»é”™è¯¯ç­‰ä»·ç‡ä»¥è¿›è¡Œæ›´å¹³è¡¡çš„è¯„ä¼°ã€‚ç›¸è¾ƒäºä¼ ç»Ÿè¯„ä¼°æ–¹æ³•ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æé«˜äº†ç¨³å¥æ€§å’Œè§£é‡Šæ€§ã€‚æˆ‘ä»¬å¯¹è¶…è¿‡12ç§åˆæˆå™¨å’Œä¹ç§çœŸå®è¯­éŸ³ç±»å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œå¹¶å‘å¸ƒæ–°çš„æ•°æ®é›†ä»¥ä¿ƒè¿›è¿›ä¸€æ­¥ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>éŸ³é¢‘æ·±åº¦ä¼ªé€ æ£€æµ‹æ¨¡å‹çš„è¯„ä¼°å­˜åœ¨æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬åˆæˆå™¨æƒé‡åˆ†é…ä¸å‡å’ŒçœŸå®è¯­éŸ³å¤šæ ·æ€§ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>ç°æœ‰è¯„ä¼°æ–¹æ³•å€¾å‘äºè¿‡å¤šé‡è§†æ ·æœ¬è¾ƒå¤šçš„åˆæˆå™¨ï¼Œå¯¼è‡´è¯„ä¼°ç»“æœä¸å¤Ÿå…¨é¢å¯é ã€‚</li>
<li>æå‡ºçš„çœŸå®è¯­éŸ³äº¤å‰æµ‹è¯•è¯„ä¼°æ¡†æ¶æ—¨åœ¨é€šè¿‡èå…¥å¤šæ ·çš„çœŸå®è¯­éŸ³æ•°æ®é›†æ¥æé«˜è¯„ä¼°çš„ç¨³å¥æ€§å’Œè§£é‡Šæ€§ã€‚</li>
<li>è¯¥æ¡†æ¶å¯¹è¶…è¿‡15ç§åˆæˆå™¨å’Œä¹ç§çœŸå®è¯­éŸ³ç±»å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚</li>
<li>çœŸå®è¯­éŸ³äº¤å‰æµ‹è¯•é€šè¿‡æ±‡æ€»é”™è¯¯ç­‰ä»·ç‡è¿›è¡Œæ›´å¹³è¡¡çš„è¯„ä¼°ï¼Œè§£å†³äº†ä¼ ç»Ÿè¯„ä¼°æ–¹æ³•ä¸­å­˜åœ¨çš„é—®é¢˜ã€‚</li>
<li>ç ”ç©¶è€…å‘å¸ƒäº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†ä»¥ä¿ƒè¿›éŸ³é¢‘æ·±åº¦ä¼ªé€ æ£€æµ‹ç ”ç©¶çš„è¿›ä¸€æ­¥å‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.09204">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2509.09204v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2509.09204v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2509.09204v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="VQualA-2025-Challenge-on-Visual-Quality-Comparison-for-Large-Multimodal-Models-Methods-and-Results"><a href="#VQualA-2025-Challenge-on-Visual-Quality-Comparison-for-Large-Multimodal-Models-Methods-and-Results" class="headerlink" title="VQualA 2025 Challenge on Visual Quality Comparison for Large Multimodal   Models: Methods and Results"></a>VQualA 2025 Challenge on Visual Quality Comparison for Large Multimodal   Models: Methods and Results</h2><p><strong>Authors:Hanwei Zhu, Haoning Wu, Zicheng Zhang, Lingyu Zhu, Yixuan Li, Peilin Chen, Shiqi Wang, Chris Wei Zhou, Linhan Cao, Wei Sun, Xiangyang Zhu, Weixia Zhang, Yucheng Zhu, Jing Liu, Dandan Zhu, Guangtao Zhai, Xiongkuo Min, Zhichao Zhang, Xinyue Li, Shubo Xu, Anh Dao, Yifan Li, Hongyuan Yu, Jiaojiao Yi, Yiding Tian, Yupeng Wu, Feiran Sun, Lijuan Liao, Song Jiang</strong></p>
<p>This paper presents a summary of the VQualA 2025 Challenge on Visual Quality Comparison for Large Multimodal Models (LMMs), hosted as part of the ICCV 2025 Workshop on Visual Quality Assessment. The challenge aims to evaluate and enhance the ability of state-of-the-art LMMs to perform open-ended and detailed reasoning about visual quality differences across multiple images. To this end, the competition introduces a novel benchmark comprising thousands of coarse-to-fine grained visual quality comparison tasks, spanning single images, pairs, and multi-image groups. Each task requires models to provide accurate quality judgments. The competition emphasizes holistic evaluation protocols, including 2AFC-based binary preference and multi-choice questions (MCQs). Around 100 participants submitted entries, with five models demonstrating the emerging capabilities of instruction-tuned LMMs on quality assessment. This challenge marks a significant step toward open-domain visual quality reasoning and comparison and serves as a catalyst for future research on interpretable and human-aligned quality evaluation systems. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ä½œä¸ºICCV 2025è§†è§‰è´¨é‡è¯„ä¼°ç ”è®¨ä¼šä¸€éƒ¨åˆ†ä¸¾åŠçš„VQualA 2025å¤§å‹å¤šæ¨¡æ€æ¨¡å‹è§†è§‰è´¨é‡æ¯”è¾ƒæŒ‘æˆ˜èµ›ã€‚è¯¥æŒ‘æˆ˜æ—¨åœ¨è¯„ä¼°å’Œæå‡æœ€å‰æ²¿å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨è·¨å¤šå¼ å›¾åƒè¿›è¡Œå¼€æ”¾å¼å’Œè¯¦ç»†æ¨ç†è§†è§‰è´¨é‡å·®å¼‚æ–¹é¢çš„èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæ¯”èµ›å¼•å…¥äº†ä¸€ä¸ªæ–°å‹åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«ä»ç²—åˆ°ç»†ç²’åº¦çš„æ•°åƒä¸ªè§†è§‰è´¨é‡æ¯”è¾ƒä»»åŠ¡ï¼Œæ¶µç›–å•å¼ å›¾åƒã€å›¾åƒå¯¹å’Œå¤šå›¾åƒç»„ã€‚æ¯ä¸ªä»»åŠ¡éƒ½è¦æ±‚æ¨¡å‹æä¾›å‡†ç¡®çš„è´¨é‡åˆ¤æ–­ã€‚æ¯”èµ›å¼ºè°ƒæ•´ä½“è¯„ä¼°åè®®ï¼ŒåŒ…æ‹¬åŸºäº2AFCçš„äºŒå…ƒåå¥½å’Œå¤šé€‰é¢˜ã€‚çº¦æœ‰100åå‚èµ›è€…æäº¤äº†å‚èµ›ä½œå“ï¼Œå…¶ä¸­äº”ä¸ªæ¨¡å‹å±•ç¤ºäº†æŒ‡ä»¤è°ƒæ•´å‹LMMåœ¨è´¨é‡è¯„ä¼°æ–¹é¢çš„æ–°å…´èƒ½åŠ›ã€‚æœ¬æ¬¡æŒ‘æˆ˜æ ‡å¿—ç€å¼€æ”¾å¼é¢†åŸŸè§†è§‰è´¨é‡æ¨ç†å’Œæ¯”è¾ƒçš„é‡è¦ä¸€æ­¥ï¼Œä¹Ÿæ˜¯æœªæ¥å¯è§£é‡Šæ€§å’Œäººç±»å¯¹é½è´¨é‡è¯„ä¼°ç³»ç»Ÿç ”ç©¶çš„å‚¬åŒ–å‰‚ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.09190v1">PDF</a> ICCV VQualA Workshop 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¦‚è¿°äº†ä½œä¸ºICCV 2025è§†è§‰è´¨é‡è¯„ä¼°ç ”è®¨ä¼šä¸€éƒ¨åˆ†çš„VQualA 2025æŒ‘æˆ˜ã€‚è¯¥æŒ‘æˆ˜æ—¨åœ¨è¯„ä¼°å¹¶æå‡æœ€æ–°å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨è·¨å¤šå¼ å›¾åƒè¿›è¡Œå¼€æ”¾å’Œè¯¦ç»†è´¨é‡æ¯”è¾ƒæ–¹é¢çš„èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œç«èµ›å¼•å…¥äº†ä¸€ä¸ªæ–°å‹åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«ä»ç²—åˆ°ç»†ç²’åº¦çš„è§†è§‰è´¨é‡æ¯”è¾ƒä»»åŠ¡ï¼Œæ¶µç›–å•å¼ å›¾åƒã€å›¾åƒå¯¹å’Œå¤šå›¾åƒç»„ã€‚æ¯ä¸ªä»»åŠ¡éƒ½è¦æ±‚æ¨¡å‹æä¾›å‡†ç¡®çš„è´¨é‡åˆ¤æ–­ã€‚ç«èµ›å¼ºè°ƒæ•´ä½“è¯„ä¼°åè®®ï¼ŒåŒ…æ‹¬åŸºäº2AFCçš„äºŒå…ƒåå¥½å’Œå¤šé€‰é¢˜ï¼ˆMCQsï¼‰ã€‚çº¦æœ‰100åå‚ä¸è€…æäº¤äº†å‚èµ›ä½œå“ï¼Œå…¶ä¸­äº”ç§æ¨¡å‹å±•ç¤ºäº†æŒ‡ä»¤ä¼˜åŒ–åçš„è´¨é‡è¯„ä¼°èƒ½åŠ›ã€‚è¯¥æŒ‘æˆ˜æ˜¯æœç€å¼€æ”¾é¢†åŸŸè§†è§‰è´¨é‡æ¨ç†å’Œæ¯”è¾ƒçš„ä¸€å¤§æ­¥ï¼Œä¹Ÿæ˜¯æœªæ¥ç ”ç©¶å¯è§£é‡Šå’Œäººç±»å¯¹é½çš„è´¨é‡è¯„ä¼°ç³»ç»Ÿçš„å‚¬åŒ–å‰‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VQualA 2025æŒ‘æˆ˜æ—¨åœ¨è¯„ä¼°å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨è§†è§‰è´¨é‡æ¯”è¾ƒæ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>ç«èµ›å¼•å…¥äº†ä¸€ä¸ªæ–°å‹åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«å¤šç§ç²’åº¦çš„è§†è§‰è´¨é‡æ¯”è¾ƒä»»åŠ¡ã€‚</li>
<li>æ¨¡å‹éœ€è¦åœ¨è·¨å¤šå¼ å›¾åƒçš„ä»»åŠ¡ä¸­æä¾›å‡†ç¡®çš„è´¨é‡åˆ¤æ–­ã€‚</li>
<li>ç«èµ›å¼ºè°ƒæ•´ä½“è¯„ä¼°åè®®ï¼ŒåŒ…æ‹¬äºŒå…ƒåå¥½å’Œå¤šé€‰é¢˜ã€‚</li>
<li>æœ‰çº¦100åå‚ä¸è€…æäº¤äº†å‚èµ›ä½œå“ã€‚</li>
<li>äº”ç§æ¨¡å‹å±•ç¤ºäº†æŒ‡ä»¤ä¼˜åŒ–åçš„è´¨é‡è¯„ä¼°èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.09190">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2509.09190v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2509.09190v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2509.09190v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2509.09190v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Clip-Your-Sequences-Fairly-Enforcing-Length-Fairness-for-Sequence-Level-RL"><a href="#Clip-Your-Sequences-Fairly-Enforcing-Length-Fairness-for-Sequence-Level-RL" class="headerlink" title="Clip Your Sequences Fairly: Enforcing Length Fairness for Sequence-Level   RL"></a>Clip Your Sequences Fairly: Enforcing Length Fairness for Sequence-Level   RL</h2><p><strong>Authors:Hanyi Mao, Quanjia Xiao, Lei Pang, Haixiao Liu</strong></p>
<p>We propose FSPO (Fair Sequence Policy Optimization), a sequence-level reinforcement learning method for LLMs that enforces length-fair clipping directly in the importance-sampling (IS) weight space. We revisit sequence-level RL methods and identify a mismatch when PPO&#x2F;GRPO-style clipping is transplanted to sequences: a fixed clip range systematically reweights short vs. long responses, distorting the effective objective. Theoretically, we formalize length fairness via a Length Reweighting Error (LRE) and prove that small LRE yields a directional cosine guarantee between the clipped and true updates. FSPO introduces a simple, Gaussian-motivated remedy: we clip the sequence log-IS ratio with a band that applies a KL-corrected drift term and scales as $\sqrt{L}$. Empirically, FSPO flattens clip rates across length bins, stabilizes training, and outperforms all baselines across multiple evaluation datasets. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†FSPOï¼ˆå…¬å¹³åºåˆ—ç­–ç•¥ä¼˜åŒ–ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„åºåˆ—çº§å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œç›´æ¥åœ¨é‡è¦æ€§é‡‡æ ·ï¼ˆISï¼‰æƒé‡ç©ºé—´ä¸­å®æ–½é•¿åº¦å…¬å¹³è£å‰ªã€‚æˆ‘ä»¬é‡æ–°å®¡è§†äº†åºåˆ—çº§å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œå¹¶å‘ç°äº†å°†PPO&#x2F;GRPOé£æ ¼çš„è£å‰ªæ–¹æ³•åº”ç”¨äºåºåˆ—æ—¶çš„ä¸åŒ¹é…ä¹‹å¤„ï¼šå›ºå®šè£å‰ªèŒƒå›´ä¼šç³»ç»Ÿåœ°é‡æ–°è°ƒæ•´çŸ­åºåˆ—å’Œé•¿åºåˆ—çš„å“åº”æƒé‡ï¼Œæ‰­æ›²æœ‰æ•ˆç›®æ ‡ã€‚åœ¨ç†è®ºä¸Šï¼Œæˆ‘ä»¬é€šè¿‡é•¿åº¦åŠ æƒè¯¯å·®ï¼ˆLREï¼‰æ­£å¼ç¡®ç«‹é•¿åº¦å…¬å¹³æ€§ï¼Œå¹¶è¯æ˜è¾ƒå°çš„LREå¯ä»¥åœ¨å‰ªè¾‘æ›´æ–°å’ŒçœŸå®æ›´æ–°ä¹‹é—´æä¾›æ–¹å‘ä½™å¼¦ä¿è¯ã€‚FSPOå¼•å…¥äº†ä¸€ç§ç®€å•è€Œå—é«˜æ–¯å¯å‘çš„è¡¥æ•‘æªæ–½ï¼šæˆ‘ä»¬ç”¨ä¸€æ¡å¸¦å‰ªè¾‘åºåˆ—å¯¹æ•°ISæ¯”ç‡ï¼Œè¯¥å¸¦åº”ç”¨KLæ ¡æ­£çš„æ¼‚ç§»æœ¯è¯­å¹¶æŒ‰$\sqrt{L}$è¿›è¡Œç¼©æ”¾ã€‚ç»éªŒä¸Šï¼ŒFSPOä½¿ä¸åŒé•¿åº¦åˆ†æ®µçš„å‰ªè¾‘ç‡å˜å¾—å¹³å¦ï¼Œç¨³å®šäº†è®­ç»ƒï¼Œå¹¶åœ¨å¤šä¸ªè¯„ä¼°æ•°æ®é›†ä¸Šçš„è¡¨ç°è¶…è¿‡äº†æ‰€æœ‰åŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.09177v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•FSPOï¼ˆå…¬å¹³åºåˆ—ç­–ç•¥ä¼˜åŒ–ï¼‰ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é‡è¦æ€§é‡‡æ ·ï¼ˆISï¼‰æƒé‡ç©ºé—´ä¸­çš„åºåˆ—çº§åˆ«å¼ºåŒ–å­¦ä¹ é—®é¢˜ï¼Œå®ç°ç›´æ¥é•¿åº¦å…¬å¹³è£å‰ªã€‚ç ”ç©¶å‘ç°åºåˆ—çº§RLæ–¹æ³•ä¸­çš„ä¸åŒ¹é…é—®é¢˜ï¼Œå¹¶æå‡ºé€šè¿‡é•¿åº¦é‡æ–°åŠ æƒè¯¯å·®ï¼ˆLREï¼‰å®ç°é•¿åº¦å…¬å¹³æ€§çš„ç†è®ºå½¢å¼åŒ–ã€‚FSPOå¼•å…¥äº†ä¸€ç§ç®€å•ä¸”åŸºäºé«˜æ–¯åŠ¨æœºçš„è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡KLæ ¡æ­£æ¼‚ç§»é¡¹å’Œæ ¹å·Lçš„ç¼©æ”¾æ¥è£å‰ªåºåˆ—å¯¹æ•°ISæ¯”ç‡ã€‚ç»éªŒä¸Šï¼ŒFSPOèƒ½åœ¨ä¸åŒé•¿åº¦åŒºé—´å†…å¹³è¡¡å‰ªè¾‘ç‡ï¼Œç¨³å®šè®­ç»ƒï¼Œå¹¶åœ¨å¤šä¸ªè¯„ä¼°æ•°æ®é›†ä¸Šä¼˜äºæ‰€æœ‰åŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>FSPOæ˜¯ä¸€ç§é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„åºåˆ—çº§å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œè§£å†³äº†é•¿åº¦ä¸å…¬å¹³å‰ªè¾‘çš„é—®é¢˜ã€‚</li>
<li>é€šè¿‡å¯¹ç°æœ‰åºåˆ—çº§RLæ–¹æ³•çš„ç ”ç©¶ï¼Œå‘ç°å›ºå®šå‰ªè¾‘èŒƒå›´ä¼šå¯¼è‡´çŸ­é•¿å“åº”çš„æƒé‡é‡æ–°åˆ†é…ï¼Œæ‰­æ›²æœ‰æ•ˆç›®æ ‡ã€‚</li>
<li>é€šè¿‡ç†è®ºå½¢å¼åŒ–é•¿åº¦å…¬å¹³æ€§ï¼Œæå‡ºäº†é•¿åº¦é‡æ–°åŠ æƒè¯¯å·®ï¼ˆLREï¼‰ã€‚</li>
<li>å°LREå¯ä»¥åœ¨å‰ªè¾‘æ›´æ–°å’ŒçœŸå®æ›´æ–°ä¹‹é—´æä¾›æ–¹å‘ä½™å¼¦ä¿è¯ã€‚</li>
<li>FSPOå¼•å…¥äº†ä¸€ç§åŸºäºé«˜æ–¯åŠ¨æœºçš„è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡è£å‰ªåºåˆ—å¯¹æ•°ISæ¯”ç‡æ¥å®ç°å…¬å¹³å‰ªè¾‘ã€‚</li>
<li>ç»éªŒè¡¨æ˜ï¼ŒFSPOèƒ½å¹³è¡¡ä¸åŒé•¿åº¦åŒºé—´çš„å‰ªè¾‘ç‡ï¼Œç¨³å®šè®­ç»ƒï¼Œå¹¶åœ¨å¤šä¸ªè¯„ä¼°æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.09177">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2509.09177v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2509.09177v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2509.09177v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="EchoX-Towards-Mitigating-Acoustic-Semantic-Gap-via-Echo-Training-for-Speech-to-Speech-LLMs"><a href="#EchoX-Towards-Mitigating-Acoustic-Semantic-Gap-via-Echo-Training-for-Speech-to-Speech-LLMs" class="headerlink" title="EchoX: Towards Mitigating Acoustic-Semantic Gap via Echo Training for   Speech-to-Speech LLMs"></a>EchoX: Towards Mitigating Acoustic-Semantic Gap via Echo Training for   Speech-to-Speech LLMs</h2><p><strong>Authors:Yuhao Zhang, Yuhao Du, Zhanchen Dai, Xiangnan Ma, Kaiqi Kou, Benyou Wang, Haizhou Li</strong></p>
<p>Speech-to-speech large language models (SLLMs) are attracting increasing attention. Derived from text-based large language models (LLMs), SLLMs often exhibit degradation in knowledge and reasoning capabilities. We hypothesize that this limitation arises because current training paradigms for SLLMs fail to bridge the acoustic-semantic gap in the feature representation space. To address this issue, we propose EchoX, which leverages semantic representations and dynamically generates speech training targets. This approach integrates both acoustic and semantic learning, enabling EchoX to preserve strong reasoning abilities as a speech LLM. Experimental results demonstrate that EchoX, with about six thousand hours of training data, achieves advanced performance on multiple knowledge-based question-answering benchmarks. The project is available at <a target="_blank" rel="noopener" href="https://github.com/FreedomIntelligence/EchoX">https://github.com/FreedomIntelligence/EchoX</a>. </p>
<blockquote>
<p>è¯­éŸ³åˆ°è¯­éŸ³çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆSLLMsï¼‰æ­£è¶Šæ¥è¶Šå—åˆ°å…³æ³¨ã€‚SLLMæºäºåŸºäºæ–‡æœ¬çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œä½†å¾€å¾€è¡¨ç°å‡ºçŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›çš„ä¸‹é™ã€‚æˆ‘ä»¬å‡è®¾è¿™ä¸€å±€é™æ€§äº§ç”Ÿçš„åŸå› æ˜¯å½“å‰SLLMçš„è®­ç»ƒæ¨¡å¼æ— æ³•å¼¥åˆç‰¹å¾è¡¨ç¤ºç©ºé—´ä¸­çš„å£°å­¦è¯­ä¹‰é¸¿æ²Ÿã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†EchoXï¼Œå®ƒåˆ©ç”¨è¯­ä¹‰è¡¨ç¤ºå¹¶åŠ¨æ€ç”Ÿæˆè¯­éŸ³è®­ç»ƒç›®æ ‡ã€‚è¿™ç§æ–¹æ³•ç»“åˆäº†å£°å­¦å­¦ä¹ å’Œè¯­ä¹‰å­¦ä¹ ï¼Œä½¿å¾—EchoXä½œä¸ºè¯­éŸ³LLMèƒ½å¤Ÿä¿ç•™å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨çº¦å…­åƒå°æ—¶çš„è®­ç»ƒæ•°æ®ï¼ŒEchoXåœ¨å¤šä¸ªåŸºäºçŸ¥è¯†çš„é—®é¢˜å›ç­”åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†å…ˆè¿›æ€§èƒ½ã€‚é¡¹ç›®å¯åœ¨ <a target="_blank" rel="noopener" href="https://github.com/FreedomIntelligence/EchoX">https://github.com/FreedomIntelligence/EchoX</a> è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.09174v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯­éŸ³åˆ°è¯­éŸ³çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆSLLMï¼‰æ­£æ—¥ç›Šå—åˆ°å…³æ³¨ã€‚ç„¶è€Œï¼Œç›¸è¾ƒäºæ–‡æœ¬å‹çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼ŒSLLMåœ¨çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ä¸Šå¸¸æœ‰æ‰€ä¸è¶³ã€‚æœ¬æ–‡å‡è®¾è¿™ç§å±€é™æ€§æºäºå½“å‰SLLMçš„è®­ç»ƒæ¨¡å¼æ— æ³•å¡«è¡¥å£°å­¦è¯­ä¹‰ç‰¹å¾è¡¨ç¤ºç©ºé—´ä¸­çš„é¸¿æ²Ÿã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†EchoXæ–¹æ¡ˆï¼Œå®ƒé€šè¿‡åˆ©ç”¨è¯­ä¹‰è¡¨ç¤ºå¹¶åŠ¨æ€ç”Ÿæˆè¯­éŸ³è®­ç»ƒç›®æ ‡ï¼Œç»“åˆå£°å­¦å’Œè¯­ä¹‰å­¦ä¹ ï¼Œä½¿å¾—EchoXåœ¨ä½œä¸ºè¯­éŸ³LLMæ—¶èƒ½å¤Ÿä¿æŒå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å¤šä¸ªåŸºäºçŸ¥è¯†çš„é—®é¢˜å›ç­”åŸºå‡†æµ‹è¯•ä¸­ï¼ŒEchoXä½¿ç”¨çº¦å…­åƒå°æ—¶çš„è®­ç»ƒæ•°æ®å–å¾—äº†å“è¶Šè¡¨ç°ã€‚é¡¹ç›®å¯é€šè¿‡ <a target="_blank" rel="noopener" href="https://github.com/FreedomIntelligence/EchoX">https://github.com/FreedomIntelligence/EchoX</a> è·å–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SLLMæ­£åœ¨å—åˆ°å…³æ³¨ï¼Œä½†åœ¨çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ä¸Šç›¸è¾ƒLLMæœ‰æ‰€ä¸è¶³ã€‚</li>
<li>å½“å‰SLLMçš„è®­ç»ƒæ¨¡å¼æ— æ³•å¡«è¡¥å£°å­¦è¯­ä¹‰ç‰¹å¾è¡¨ç¤ºç©ºé—´ä¸­çš„é¸¿æ²Ÿæ˜¯å…¶èƒ½åŠ›å±€é™çš„ä¸»è¦åŸå› ã€‚</li>
<li>EchoXé€šè¿‡ç»“åˆå£°å­¦å’Œè¯­ä¹‰å­¦ä¹ æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>EchoXåˆ©ç”¨è¯­ä¹‰è¡¨ç¤ºå¹¶åŠ¨æ€ç”Ÿæˆè¯­éŸ³è®­ç»ƒç›®æ ‡ã€‚</li>
<li>EchoXåœ¨ä½œä¸ºè¯­éŸ³LLMæ—¶èƒ½å¤Ÿä¿æŒå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å®éªŒè¯æ˜EchoXåœ¨å¤šä¸ªçŸ¥è¯†é—®ç­”åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å“è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.09174">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2509.09174v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2509.09174v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2509.09174v1/page_1_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2509.09174v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2509.09174v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2509.09174v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="MR-UIE-Multi-Perspective-Reasoning-with-Reinforcement-Learning-for-Universal-Information-Extraction"><a href="#MR-UIE-Multi-Perspective-Reasoning-with-Reinforcement-Learning-for-Universal-Information-Extraction" class="headerlink" title="MR-UIE: Multi-Perspective Reasoning with Reinforcement Learning for   Universal Information Extraction"></a>MR-UIE: Multi-Perspective Reasoning with Reinforcement Learning for   Universal Information Extraction</h2><p><strong>Authors:Zhongqiu Li, Shiquan Wang, Ruiyu Fang, Mengjiao Bao, Zhenhe Wu, Shuangyong Song, Yongxiang Li, Zhongjiang He</strong></p>
<p>Large language models (LLMs) demonstrate robust capabilities across diverse research domains. However, their performance in universal information extraction (UIE) remains insufficient, especially when tackling structured output scenarios that involve complex schema descriptions and require multi-step reasoning. While existing approaches enhance the performance of LLMs through in-context learning and instruction tuning, significant limitations nonetheless persist. To enhance the modelâ€™s generalization ability, we propose integrating reinforcement learning (RL) with multi-perspective reasoning for information extraction (IE) tasks. Our work transitions LLMs from passive extractors to active reasoners, enabling them to understand not only what to extract but also how to reason. Experiments conducted on multiple IE benchmarks demonstrate that MR-UIE consistently elevates extraction accuracy across domains and surpasses state-of-the-art methods on several datasets. Furthermore, incorporating multi-perspective reasoning into RL notably enhances generalization in complex IE tasks, underscoring the critical role of reasoning in challenging scenarios. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šä¸ªç ”ç©¶é¢†åŸŸä¸­å±•ç¤ºäº†å¼ºå¤§çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨é€šç”¨ä¿¡æ¯æå–ï¼ˆUIEï¼‰ä¸­çš„è¡¨ç°ä»ç„¶ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†æ¶‰åŠå¤æ‚æ¨¡å¼æè¿°å’Œå¤šæ­¥æ¨ç†çš„ç»“æ„åŒ–è¾“å‡ºåœºæ™¯æ—¶ã€‚å°½ç®¡ç°æœ‰æ–¹æ³•é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ å’ŒæŒ‡ä»¤å¾®è°ƒå¢å¼ºäº†LLMçš„æ€§èƒ½ï¼Œä½†ä»å­˜åœ¨é‡å¤§å±€é™æ€§ã€‚ä¸ºäº†æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œæˆ‘ä»¬æå‡ºå°†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸å¤šè§†è§’æ¨ç†ç›¸ç»“åˆï¼Œç”¨äºä¿¡æ¯æå–ï¼ˆIEï¼‰ä»»åŠ¡ã€‚æˆ‘ä»¬çš„å·¥ä½œä½¿LLMä»è¢«åŠ¨æå–å™¨è½¬å˜ä¸ºç§¯ææ¨ç†å™¨ï¼Œä½¿å®ƒä»¬ä¸ä»…èƒ½å¤Ÿç†è§£è¦æå–çš„å†…å®¹ï¼Œè¿˜èƒ½ç†è§£å¦‚ä½•è¿›è¡Œæ¨ç†ã€‚åœ¨å¤šä¸ªä¿¡æ¯æå–åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒMR-UIEåœ¨ä¸åŒé¢†åŸŸå§‹ç»ˆæé«˜äº†æå–ç²¾åº¦ï¼Œå¹¶åœ¨å‡ ä¸ªæ•°æ®é›†ä¸Šè¶…è¿‡äº†æœ€æ–°æŠ€æœ¯çš„æ–¹æ³•ã€‚æ­¤å¤–ï¼Œå°†å¤šè§†è§’æ¨ç†èå…¥å¼ºåŒ–å­¦ä¹ æ˜¾è‘—æé«˜äº†åœ¨å¤æ‚ä¿¡æ¯æå–ä»»åŠ¡ä¸­çš„æ³›åŒ–èƒ½åŠ›ï¼Œå‡¸æ˜¾äº†æ¨ç†åœ¨æŒ‘æˆ˜åœºæ™¯ä¸­çš„å…³é”®ä½œç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.09082v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šä¸ªç ”ç©¶é¢†åŸŸä¸­è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†åœ¨é€šç”¨ä¿¡æ¯æå–ï¼ˆUIEï¼‰æ–¹é¢çš„è¡¨ç°ä»ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨æ¶‰åŠå¤æ‚æ¨¡å¼æè¿°å’Œéœ€è¦å¤šæ­¥éª¤æ¨ç†çš„ç»“æ„åŒ–è¾“å‡ºåœºæ™¯ä¸­ã€‚ä¸ºæå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œç»“åˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œå¤šè§’åº¦æ¨ç†çš„ä¿¡æ¯æå–ï¼ˆIEï¼‰ä»»åŠ¡ã€‚å®éªŒè¯æ˜ï¼ŒMR-UIEåœ¨å¤šä¸ªIEåŸºå‡†æµ‹è¯•ä¸Šçš„æå–ç²¾åº¦æŒç»­æé«˜ï¼Œå¹¶åœ¨æŸäº›æ•°æ®é›†ä¸Šè¶…è¶Šäº†æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚ç‰¹åˆ«æ˜¯å°†å¤šè§†è§’æ¨ç†èå…¥RLï¼Œæ˜¾è‘—æé«˜äº†åœ¨å¤æ‚IEä»»åŠ¡ä¸­çš„æ³›åŒ–èƒ½åŠ›ï¼Œå‡¸æ˜¾äº†æ¨ç†åœ¨æŒ‘æˆ˜åœºæ™¯ä¸­çš„å…³é”®ä½œç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é€šç”¨ä¿¡æ¯æå–ï¼ˆUIEï¼‰æ–¹é¢çš„æ€§èƒ½ä»æœ‰å¾…æå‡ã€‚</li>
<li>åœ¨å¤„ç†æ¶‰åŠå¤æ‚æ¨¡å¼æè¿°å’Œå¤šæ­¥éª¤æ¨ç†çš„ç»“æ„åŒ–è¾“å‡ºåœºæ™¯æ—¶ï¼ŒLLMsé¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>é€šè¿‡ç»“åˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œå¤šè§’åº¦æ¨ç†ï¼Œå¯ä»¥æå‡LLMsåœ¨IEä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚</li>
<li>MR-UIEæ–¹æ³•åœ¨å¤šä¸ªä¿¡æ¯æå–åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºè¾ƒé«˜çš„æå–ç²¾åº¦ã€‚</li>
<li>MR-UIEåœ¨æŸäº›æ•°æ®é›†ä¸Šçš„è¡¨ç°è¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</li>
<li>å°†å¤šè§†è§’æ¨ç†èå…¥RLæ˜¾è‘—æé«˜äº†åœ¨å¤æ‚IEä»»åŠ¡ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.09082">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2509.09082v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2509.09082v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2509.09082v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="BRoverbs-â€“-Measuring-how-much-LLMs-understand-Portuguese-proverbs"><a href="#BRoverbs-â€“-Measuring-how-much-LLMs-understand-Portuguese-proverbs" class="headerlink" title="BRoverbs â€“ Measuring how much LLMs understand Portuguese proverbs"></a>BRoverbs â€“ Measuring how much LLMs understand Portuguese proverbs</h2><p><strong>Authors:Thales Sales Almeida, Giovana Kerche BonÃ¡s, JoÃ£o Guilherme Alves Santos</strong></p>
<p>Large Language Models (LLMs) exhibit significant performance variations depending on the linguistic and cultural context in which they are applied. This disparity signals the necessity of mature evaluation frameworks that can assess their capabilities in specific regional settings. In the case of Portuguese, existing evaluations remain limited, often relying on translated datasets that may not fully capture linguistic nuances or cultural references. Meanwhile, native Portuguese-language datasets predominantly focus on structured national exams or sentiment analysis of social media interactions, leaving gaps in evaluating broader linguistic understanding. To address this limitation, we introduce BRoverbs, a dataset specifically designed to assess LLM performance through Brazilian proverbs. Proverbs serve as a rich linguistic resource, encapsulating cultural wisdom, figurative expressions, and complex syntactic structures that challenge the model comprehension of regional expressions. BRoverbs aims to provide a new evaluation tool for Portuguese-language LLMs, contributing to advancing regionally informed benchmarking. The benchmark is available at <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/Tropic-AI/BRoverbs">https://huggingface.co/datasets/Tropic-AI/BRoverbs</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åº”ç”¨æ—¶ï¼Œå…¶æ€§èƒ½ä¼šå› è¯­è¨€å’Œæ–‡åŒ–çš„ä¸Šä¸‹æ–‡ç¯å¢ƒè€Œè¡¨ç°å‡ºæ˜¾è‘—çš„å·®å¼‚ã€‚è¿™ç§å·®å¼‚è¡¨æ˜éœ€è¦æˆç†Ÿçš„è¯„ä¼°æ¡†æ¶æ¥è¯„ä¼°å…¶åœ¨ç‰¹å®šåŒºåŸŸè®¾ç½®ä¸­çš„èƒ½åŠ›ã€‚åœ¨è‘¡è„ç‰™è¯­çš„æƒ…å†µä¸‹ï¼Œç°æœ‰çš„è¯„ä¼°ä»ç„¶æœ‰é™ï¼Œé€šå¸¸ä¾èµ–äºå¯èƒ½æ— æ³•å®Œå…¨æ•æ‰è¯­è¨€ç»†å¾®å·®åˆ«æˆ–æ–‡åŒ–å‚è€ƒçš„ç¿»è¯‘æ•°æ®é›†ã€‚åŒæ—¶ï¼Œè‘¡è„ç‰™è¯­åŸç”Ÿè¯­è¨€æ•°æ®é›†ä¸»è¦é›†ä¸­åœ¨ç»“æ„åŒ–çš„å›½å®¶è€ƒè¯•æˆ–ç¤¾äº¤åª’ä½“äº’åŠ¨çš„æƒ…æ„Ÿåˆ†æä¸Šï¼Œåœ¨è¯„ä¼°æ›´å¹¿æ³›çš„è¯­è¨€ç†è§£æ–¹é¢å­˜åœ¨ç©ºç™½ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æ¨å‡ºäº†BRoverbsæ•°æ®é›†ï¼Œä¸“é—¨ç”¨äºé€šè¿‡å·´è¥¿çš„è°šè¯­æ¥è¯„ä¼°LLMçš„æ€§èƒ½ã€‚è°šè¯­æ˜¯ä¸€ç§ä¸°å¯Œçš„è¯­è¨€èµ„æºï¼ŒåŒ…å«äº†æ–‡åŒ–æ™ºæ…§ã€æ¯”å–»è¡¨è¾¾å’Œå¤æ‚çš„å¥æ³•ç»“æ„ï¼ŒæŒ‘æˆ˜äº†æ¨¡å‹å¯¹åŒºåŸŸè¡¨è¾¾çš„ç†è§£ã€‚BRoverbsæ—¨åœ¨ä¸ºè‘¡è„ç‰™è¯­LLMæä¾›æ–°çš„è¯„ä¼°å·¥å…·ï¼Œä¸ºåŒºåŸŸæ€§çš„åŸºå‡†æµ‹è¯•åšå‡ºè´¡çŒ®ã€‚è¯¥åŸºå‡†æµ‹è¯•å¯åœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/Tropic-AI/BRoverbs%E8%8E%B7%E5%8F%96%E3%80%82">https://huggingface.co/datasets/Tropic-AI/BRoverbsè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.08960v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¸åŒè¯­è¨€å’Œæ–‡åŒ–èƒŒæ™¯ä¸‹çš„åº”ç”¨æ€§èƒ½å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚è¿™ä¸€å·®å¼‚çªæ˜¾äº†éœ€è¦æˆç†Ÿçš„è¯„ä¼°æ¡†æ¶æ¥è¯„ä¼°å…¶åœ¨ç‰¹å®šåŒºåŸŸè®¾ç½®ä¸­çš„èƒ½åŠ›ã€‚å¯¹äºè‘¡è„ç‰™è¯­è€Œè¨€ï¼Œç°æœ‰è¯„ä¼°æ–¹æ³•ä»ç„¶æœ‰é™ï¼Œå¸¸å¸¸ä¾èµ–äºå¯èƒ½æ— æ³•å®Œå…¨æ•æ‰è¯­è¨€ç»†å¾®å·®åˆ«æˆ–æ–‡åŒ–å‚è€ƒçš„ç¿»è¯‘æ•°æ®é›†ã€‚åŒæ—¶ï¼Œè‘¡è„ç‰™è¯­åŸç”Ÿè¯­è¨€æ•°æ®é›†ä¸»è¦é›†ä¸­åœ¨å›½å®¶ç»“æ„è€ƒè¯•æˆ–ç¤¾äº¤åª’ä½“äº’åŠ¨çš„æƒ…æ„Ÿåˆ†æä¸Šï¼Œåœ¨è¯„ä¼°æ›´å¹¿æ³›çš„è¯­è¨€ç†è§£æ–¹é¢å­˜åœ¨ç©ºç™½ã€‚ä¸ºè§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æ¨å‡ºäº†BRoverbsæ•°æ®é›†ï¼Œä¸“é—¨ç”¨äºè¯„ä¼°è‘¡è„ç‰™è¯­LLMçš„æ€§èƒ½ã€‚è°šè¯­æ˜¯ä¸€ç§ä¸°å¯Œçš„è¯­è¨€èµ„æºï¼Œè•´å«æ–‡åŒ–æ™ºæ…§ã€æ¯”å–»è¡¨è¾¾å’Œå¤æ‚çš„å¥æ³•ç»“æ„ï¼ŒæŒ‘æˆ˜äº†æ¨¡å‹å¯¹åŒºåŸŸè¡¨è¾¾çš„ç†è§£ã€‚BRoverbsæ—¨åœ¨ä¸ºè‘¡è„ç‰™è¯­LLMæä¾›æ–°çš„è¯„ä¼°å·¥å…·ï¼Œä¸ºæ¨åŠ¨åŒºåŸŸåŸºå‡†æµ‹è¯•çš„å‘å±•åšå‡ºè´¡çŒ®ã€‚è¯¥åŸºå‡†æµ‹è¯•å¯åœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/Tropic-AI/BRoverbs%E8%BF%BD%E9%9A%8F%E3%80%82">https://huggingface.co/datasets/Tropic-AI/BRoverbsè®¿é—®ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¸åŒè¯­è¨€å’Œæ–‡åŒ–èƒŒæ™¯ä¸‹çš„æ€§èƒ½å­˜åœ¨å·®å¼‚ï¼Œéœ€è¦æˆç†Ÿçš„è¯„ä¼°æ¡†æ¶ã€‚</li>
<li>ç°æœ‰è‘¡è„ç‰™è¯­LLMè¯„ä¼°æ–¹æ³•æœ‰é™ï¼Œå¸¸ä¾èµ–äºç¿»è¯‘æ•°æ®é›†ï¼Œå¯èƒ½æ— æ³•å…¨é¢æ•æ‰è¯­è¨€ç»†å¾®å·®åˆ«å’Œæ–‡åŒ–å‚è€ƒã€‚</li>
<li>è‘¡è„ç‰™è¯­åŸç”Ÿè¯­è¨€æ•°æ®é›†ä¸»è¦é›†ä¸­åœ¨ç»“æ„è€ƒè¯•å’Œç¤¾äº¤åª’ä½“æƒ…æ„Ÿåˆ†æï¼Œç¼ºä¹æ›´å¹¿æ³›çš„è¯­è¨€ç†è§£è¯„ä¼°ã€‚</li>
<li>BRoverbsæ•°æ®é›†æ—¨åœ¨é€šè¿‡å·´è¥¿è°šè¯­è¯„ä¼°è‘¡è„ç‰™è¯­LLMæ€§èƒ½ï¼Œä¸°å¯Œè¯­è¨€èµ„æºè•´å«æ–‡åŒ–æ™ºæ…§å’Œå¤æ‚å¥æ³•ç»“æ„ã€‚</li>
<li>BRoverbsä¸ºè‘¡è„ç‰™è¯­LLMæä¾›æ–°çš„è¯„ä¼°å·¥å…·ï¼Œæ¨åŠ¨åŒºåŸŸåŸºå‡†æµ‹è¯•çš„å‘å±•ã€‚</li>
<li>BRoverbsæ•°æ®é›†å¯è®¿é—®äº<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/Tropic-AI/BRoverbs%E3%80%82">https://huggingface.co/datasets/Tropic-AI/BRoverbsã€‚</a></li>
<li>è¯¥åŸºå‡†æµ‹è¯•ä¸ºè¯„ä¼°è‘¡è„ç‰™è¯­å¤§å‹è¯­è¨€æ¨¡å‹æä¾›äº†ä¸€ä¸ªé‡è¦èµ„æºï¼Œæœ‰åŠ©äºä¿ƒè¿›å¯¹è‘¡è„ç‰™è¯­è¯­è¨€ç»†å¾®å·®åˆ«å’Œæ–‡åŒ–èƒŒæ™¯çš„ç†è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.08960">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2509.08960v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2509.08960v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2509.08960v1/page_3_1.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Merge-of-Thought-Distillation"><a href="#Merge-of-Thought-Distillation" class="headerlink" title="Merge-of-Thought Distillation"></a>Merge-of-Thought Distillation</h2><p><strong>Authors:Zhanming Shen, Zeyu Qin, Zenan Huang, Hao Chen, Jiaqi Hu, Yihong Zhuang, Guoshan Lu, Gang Chen, Junbo Zhao</strong></p>
<p>Efficient reasoning distillation for long chain-of-thought (CoT) models is increasingly constrained by the assumption of a single oracle teacher, despite practical availability of multiple candidate teachers and growing CoT corpora. We revisit teacher selection and observe that different students have different â€œbest teachers,â€ and even for the same student the best teacher can vary across datasets. Therefore, to unify multiple teachersâ€™ reasoning abilities into student with overcoming conflicts among various teachersâ€™ supervision, we propose Merge-of-Thought Distillation (MoT), a lightweight framework that alternates between teacher-specific supervised fine-tuning branches and weight-space merging of the resulting student variants. On competition math benchmarks, using only about 200 high-quality CoT samples, applying MoT to a Qwen3-14B student surpasses strong models including DEEPSEEK-R1, QWEN3-30B-A3B, QWEN3-32B, and OPENAI-O1, demonstrating substantial gains. Besides, MoT consistently outperforms the best single-teacher distillation and the naive multi-teacher union, raises the performance ceiling while mitigating overfitting, and shows robustness to distribution-shifted and peer-level teachers. Moreover, MoT reduces catastrophic forgetting, improves general reasoning beyond mathematics and even cultivates a better teacher, indicating that consensus-filtered reasoning features transfer broadly. These results position MoT as a simple, scalable route to efficiently distilling long CoT capabilities from diverse teachers into compact students. </p>
<blockquote>
<p>å¯¹äºé•¿é“¾æ¡æ€ç»´ï¼ˆCoTï¼‰æ¨¡å‹çš„æ¨ç†è’¸é¦ï¼Œå°½ç®¡å­˜åœ¨å¤šä¸ªå€™é€‰æ•™å¸ˆä¸”CoTè¯­æ–™åº“ä¸æ–­å¢é•¿ï¼Œä½†å…¶è¶Šæ¥è¶Šå—å•ä¸€æƒå¨æ•™å¸ˆçš„å‡è®¾çš„åˆ¶çº¦ã€‚æˆ‘ä»¬é‡æ–°è€ƒå¯Ÿæ•™å¸ˆé€‰æ‹©å¹¶å‘ç°ï¼Œä¸åŒçš„å­¦ç”Ÿæœ‰ä¸åŒçš„â€œæœ€ä½³æ•™å¸ˆâ€ï¼Œå³ä½¿å¯¹äºåŒä¸€å­¦ç”Ÿï¼Œæœ€ä½³æ•™å¸ˆä¹Ÿä¼šå› æ•°æ®é›†è€Œå¼‚ã€‚å› æ­¤ï¼Œä¸ºäº†å°†å¤šä½æ•™å¸ˆçš„æ¨ç†èƒ½åŠ›èå…¥å­¦ç”Ÿæ¨¡å‹ä¸­ï¼ŒåŒæ—¶è§£å†³å„ä½æ•™å¸ˆçš„ç›‘ç£å†²çªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†â€œæ€ç»´èåˆè’¸é¦â€ï¼ˆMoTï¼‰è¿™ä¸€è½»é‡çº§æ¡†æ¶ã€‚è¯¥æ¡†æ¶äº¤æ›¿è¿›è¡Œé’ˆå¯¹ç‰¹å®šæ•™å¸ˆçš„ç›‘ç£å¾®è°ƒåˆ†æ”¯å’Œç»“æœå­¦ç”Ÿå˜ä½“æƒé‡ç©ºé—´çš„åˆå¹¶ã€‚åœ¨ç«èµ›æ•°å­¦åŸºå‡†æµ‹è¯•ä¸­ï¼Œä»…ä½¿ç”¨çº¦200ä¸ªé«˜è´¨é‡CoTæ ·æœ¬ï¼Œå¯¹Qwen3-14Bå­¦ç”Ÿåº”ç”¨MoTè¶…è¶Šäº†åŒ…æ‹¬DEEPSEEK-R1ã€QWEN3-30B-A3Bã€QWEN3-32Bå’ŒOPENAI-O1åœ¨å†…çš„å¼ºå¤§æ¨¡å‹ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æå‡ã€‚æ­¤å¤–ï¼ŒMoTå§‹ç»ˆä¼˜äºæœ€ä½³å•æ•™å¸ˆè’¸é¦å’Œç®€å•çš„å¤šæ•™å¸ˆè”åˆï¼Œæé«˜äº†æ€§èƒ½ä¸Šé™ï¼ŒåŒæ—¶å‡è½»äº†è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œå¹¶å¯¹åˆ†å¸ƒåç§»å’Œæ•™å¸ˆåŒçº§æ°´å¹³è¡¨ç°å‡ºç¨³å¥æ€§ã€‚è€Œä¸”ï¼ŒMoTå‡å°‘äº†ç¾éš¾æ€§é—å¿˜ï¼Œæé«˜äº†é™¤æ•°å­¦ä¹‹å¤–çš„ä¸€èˆ¬æ¨ç†èƒ½åŠ›ï¼Œç”šè‡³åŸ¹å…»äº†æ›´å¥½çš„æ•™å¸ˆï¼Œè¡¨æ˜ç»è¿‡å…±è¯†è¿‡æ»¤çš„æ¨ç†ç‰¹å¾å…·æœ‰å¹¿æ³›çš„å¯è¿ç§»æ€§ã€‚è¿™äº›ç»“æœå°†MoTå®šä½ä¸ºä»å„ç§æ•™å¸ˆä¸­æœ‰æ•ˆè’¸é¦é•¿CoTèƒ½åŠ›åˆ°ç´§å‡‘å­¦ç”Ÿæ¨¡å‹çš„ç®€å•ä¸”å¯æ‰©å±•çš„é€”å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.08814v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¯¹äºé•¿é“¾æ€ç»´ï¼ˆCoTï¼‰æ¨¡å‹çš„æ¨ç†è’¸é¦ï¼Œå°½ç®¡å­˜åœ¨å¤šä¸ªå€™é€‰æ•™å¸ˆä¸”CoTè¯­æ–™åº“ä¸æ–­å¢é•¿ï¼Œä½†å—é™äºå•ä¸€oracleæ•™å¸ˆçš„å‡è®¾ã€‚æˆ‘ä»¬é‡æ–°è€ƒå¯Ÿæ•™å¸ˆé€‰æ‹©ï¼Œå¹¶å‘ç°ä¸åŒå­¦ç”Ÿæœ‰ä¸åŒçš„â€œæœ€ä½³æ•™å¸ˆâ€ï¼Œç”šè‡³å¯¹äºåŒä¸€å­¦ç”Ÿï¼Œæœ€ä½³æ•™å¸ˆä¹Ÿä¼šå› æ•°æ®é›†è€Œå¼‚ã€‚å› æ­¤ï¼Œä¸ºäº†å°†å­¦ç”Ÿç»Ÿä¸€èµ·æ¥å¹¶å…‹æœä¸åŒæ•™å¸ˆç›‘ç£ä¹‹é—´çš„å†²çªï¼Œæˆ‘ä»¬æå‡ºäº†Merge-of-Thought Distillationï¼ˆMoTï¼‰è¿™ä¸€è½»é‡çº§æ¡†æ¶ï¼Œå®ƒäº¤æ›¿è¿›è¡Œæ•™å¸ˆç‰¹å®šçš„ç›‘ç£å¾®è°ƒåˆ†æ”¯å’Œæƒé‡ç©ºé—´çš„åˆå¹¶å­¦ç”Ÿå˜ä½“ã€‚åœ¨ç«èµ›æ•°å­¦åŸºå‡†æµ‹è¯•ä¸­ï¼Œä»…ä½¿ç”¨å¤§çº¦200ä¸ªé«˜è´¨é‡çš„CoTæ ·æœ¬ï¼Œå°†MoTåº”ç”¨äºQwen3-14Bå­¦ç”Ÿå°±èƒ½è¶…è¶Šå¼ºå¤§çš„æ¨¡å‹ï¼Œå¦‚DEEPSEEK-R1ã€QWEN3-30B-A3Bã€QWEN3-32Bå’ŒOPENAI-O1ç­‰ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼ŒMoTæŒç»­è¶…è¶Šæœ€ä½³å•æ•™å¸ˆè’¸é¦å’Œç®€å•çš„å¤šæ•™å¸ˆè”åˆæ–¹æ³•ï¼Œæé«˜äº†æ€§èƒ½ä¸Šé™å¹¶å‡è½»äº†è¿‡æ‹Ÿåˆç°è±¡ï¼Œæ˜¾ç¤ºå‡ºå¯¹åˆ†å¸ƒåç§»å’Œæ•™å¸ˆæ°´å¹³çš„ç¨³å¥æ€§ã€‚è€Œä¸”ï¼ŒMoTå‡å°‘äº†ç¾éš¾æ€§é—å¿˜ï¼Œæé«˜äº†é€šç”¨æ¨ç†èƒ½åŠ›å¹¶åŸ¹è‚²äº†æ›´å¥½çš„æ•™å¸ˆï¼Œè¡¨æ˜ç»è¿‡å…±è¯†è¿‡æ»¤çš„æ¨ç†ç‰¹å¾å…·æœ‰å¹¿æ³›çš„è¿ç§»æ€§ã€‚è¿™äº›ç»“æœå°†MoTå®šä½ä¸ºä»å¤šç§æ•™å¸ˆé«˜æ•ˆè’¸é¦é•¿CoTèƒ½åŠ›åˆ°ç²¾ç®€å­¦ç”Ÿçš„ç®€å•ã€å¯æ‰©å±•çš„é€”å¾„ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æå‡ºMerge-of-Thought Distillationï¼ˆMoTï¼‰æ¡†æ¶ï¼Œç»Ÿä¸€å¤šä¸ªæ•™å¸ˆçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>è§‚å¯Ÿä¸åŒå­¦ç”Ÿæœ‰ä¸åŒçš„æœ€ä½³æ•™å¸ˆï¼Œä¸”æœ€ä½³æ•™å¸ˆå¯ä»¥éšæ•°æ®é›†å˜åŒ–ã€‚</li>
<li>MoTæ¡†æ¶é€šè¿‡äº¤æ›¿æ•™å¸ˆç‰¹å®šçš„ç›‘ç£å¾®è°ƒåˆ†æ”¯å’Œæƒé‡ç©ºé—´åˆå¹¶å­¦ç”Ÿå˜ä½“æ¥å·¥ä½œã€‚</li>
<li>åœ¨ç«èµ›æ•°å­¦åŸºå‡†æµ‹è¯•ä¸­ï¼ŒMoTæ˜¾è‘—è¶…è¶Šäº†å…¶ä»–å¼ºå¤§çš„æ¨¡å‹ã€‚</li>
<li>MoTæé«˜äº†æ€§èƒ½ä¸Šé™ï¼Œå‡è½»äº†è¿‡æ‹Ÿåˆç°è±¡ï¼Œå¹¶æ˜¾ç¤ºå‡ºå¯¹åˆ†å¸ƒåç§»å’Œæ•™å¸ˆæ°´å¹³çš„ç¨³å¥æ€§ã€‚</li>
<li>MoTå‡å°‘äº†ç¾éš¾æ€§é—å¿˜ï¼Œæé«˜äº†é€šç”¨æ¨ç†èƒ½åŠ›ï¼Œå¹¶åŸ¹è‚²äº†æ›´å¥½çš„æ•™å¸ˆã€‚</li>
<li>MoTçš„å…±è¯†è¿‡æ»¤æ¨ç†ç‰¹å¾å…·æœ‰å¹¿æ³›çš„è¿ç§»æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.08814">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2509.08814v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2509.08814v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2509.08814v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2509.08814v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="PersonaFuse-A-Personality-Activation-Driven-Framework-for-Enhancing-Human-LLM-Interactions"><a href="#PersonaFuse-A-Personality-Activation-Driven-Framework-for-Enhancing-Human-LLM-Interactions" class="headerlink" title="PersonaFuse: A Personality Activation-Driven Framework for Enhancing   Human-LLM Interactions"></a>PersonaFuse: A Personality Activation-Driven Framework for Enhancing   Human-LLM Interactions</h2><p><strong>Authors:Yixuan Tang, Yi Yang, Ahmed Abbasi</strong></p>
<p>Recent advancements in Large Language Models (LLMs) demonstrate remarkable capabilities across various fields. These developments have led to more direct communication between humans and LLMs in various situations, such as social companionship and psychological support. However, LLMs often exhibit limitations in emotional perception and social competence during real-world conversations. These limitations partly originate from their inability to adapt their communication style and emotional expression to different social and task contexts. In this work, we introduce PersonaFuse, a novel LLM post-training framework that enables LLMs to adapt and express different personalities for varying situations. Inspired by Trait Activation Theory and the Big Five personality model, PersonaFuse employs a Mixture-of-Expert architecture that combines persona adapters with a dynamic routing network, enabling contextual trait expression. Experimental results show that PersonaFuse substantially outperforms baseline models across multiple dimensions of social-emotional intelligence. Importantly, these gains are achieved without sacrificing general reasoning ability or model safety, which remain common limitations of direct prompting and supervised fine-tuning approaches. PersonaFuse also delivers consistent improvements in downstream human-centered applications, such as mental health counseling and review-based customer service. Finally, human preference evaluations against leading LLMs, including GPT-4o and DeepSeek, demonstrate that PersonaFuse achieves competitive response quality despite its comparatively smaller model size. These findings demonstrate that PersonaFuse offers a theoretically grounded and practical approach for developing social-emotional enhanced LLMs, marking a significant advancement toward more human-centric AI systems. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›å±•åœ¨å„ä¸ªé¢†åŸŸè¡¨ç°å‡ºæ˜¾è‘—çš„èƒ½åŠ›ã€‚è¿™äº›å‘å±•ä½¿å¾—äººç±»åœ¨å„ç§æƒ…å¢ƒä¸‹ä¸LLMä¹‹é—´çš„ç›´æ¥æ²Ÿé€šå˜å¾—æ›´åŠ é¢‘ç¹ï¼Œå¦‚åœ¨ç¤¾äº¤é™ªä¼´å’Œå¿ƒç†æ”¯æŒæ–¹é¢ã€‚ç„¶è€Œï¼Œåœ¨çœŸå®ä¸–ç•Œçš„å¯¹è¯ä¸­ï¼ŒLLMåœ¨æƒ…æ„Ÿæ„ŸçŸ¥å’Œç¤¾ä¼šèƒ½åŠ›æ–¹é¢å¸¸å¸¸è¡¨ç°å‡ºå±€é™æ€§ã€‚è¿™äº›å±€é™æ€§éƒ¨åˆ†æºäºå®ƒä»¬æ— æ³•é€‚åº”ä¸åŒçš„ç¤¾ä¼šå’Œä»»åŠ¡ä¸Šä¸‹æ–‡æ¥è°ƒæ•´å…¶äº¤æµé£æ ¼å’Œæƒ…æ„Ÿè¡¨è¾¾ã€‚</p>
</blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†PersonaFuseï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹çš„å¤§å‹è¯­è¨€æ¨¡å‹åè®­ç»ƒæ¡†æ¶ï¼Œèƒ½å¤Ÿä½¿LLMé€‚åº”å¹¶è¡¨è¾¾ä¸åŒæƒ…å¢ƒä¸‹çš„ä¸åŒä¸ªæ€§ã€‚å—ç‰¹è´¨æ¿€æ´»ç†è®ºå’Œäº”å¤§äººæ ¼æ¨¡å‹çš„å¯å‘ï¼ŒPersonaFuseé‡‡ç”¨äº†ä¸€ç§ä¸“å®¶æ··åˆæ¶æ„ï¼Œè¯¥æ¶æ„ç»“åˆäº†äººæ ¼é€‚é…å™¨å’ŒåŠ¨æ€è·¯ç”±ç½‘ç»œï¼Œä»¥å®ç°ä¸Šä¸‹æ–‡ç‰¹è´¨è¡¨è¾¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPersonaFuseåœ¨ç¤¾ä¼šæƒ…æ„Ÿæ™ºåŠ›çš„å¤šä¸ªç»´åº¦ä¸Šæ˜¾è‘—ä¼˜äºåŸºå‡†æ¨¡å‹ã€‚é‡è¦çš„æ˜¯ï¼Œè¿™äº›æ”¶ç›Šçš„å®ç°å¹¶ä¸ä¼šç‰ºç‰²ä¸€èˆ¬çš„æ¨ç†èƒ½åŠ›æˆ–æ¨¡å‹å®‰å…¨æ€§ï¼Œè¿™ä»ç„¶æ˜¯ç›´æ¥æç¤ºå’Œç›‘ç£å¾®è°ƒæ–¹æ³•çš„å¸¸è§å±€é™æ€§ã€‚PersonaFuseè¿˜åœ¨ä¸‹æ¸¸ä»¥äººä¸ºä¸­å¿ƒçš„åº”ç”¨ç¨‹åºï¼ˆå¦‚å¿ƒç†å¥åº·å’¨è¯¢å’ŒåŸºäºè¯„è®ºçš„å®¢æˆ·æœåŠ¡ï¼‰ä¸­å®ç°äº†æŒç»­çš„æ”¹è¿›ã€‚æœ€åï¼Œä¸é¢†å…ˆçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆåŒ…æ‹¬GPT-4oå’ŒDeepSeekï¼‰è¿›è¡Œçš„äººç±»åå¥½è¯„ä¼°è¡¨æ˜ï¼Œå°½ç®¡å…¶æ¨¡å‹è§„æ¨¡ç›¸å¯¹è¾ƒå°ï¼Œä½†PersonaFuseä»å®ç°äº†å…·æœ‰ç«äº‰åŠ›çš„å“åº”è´¨é‡ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼ŒPersonaFuseä¸ºå¼€å‘ç¤¾ä¼šæƒ…æ„Ÿå¢å¼ºå‹LLMæä¾›äº†ç†è®ºæ‰å®ä¸”å®ç”¨çš„æ–¹æ³•ï¼Œæ ‡å¿—ç€å‘æ›´ä»¥äººä¸ºä¸­å¿ƒçš„AIç³»ç»Ÿè¿ˆå‡ºäº†é‡å¤§æ­¥ä¼ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.07370v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥ä¸ºè·¨é¢†åŸŸäº¤æµå¸¦æ¥äº†æ˜¾è‘—çš„èƒ½åŠ›æå‡ã€‚è¿™äº›è¿›æ­¥ä½¿å¾—äººç±»ä¸LLMåœ¨å„ç§æƒ…å¢ƒä¸‹çš„ç›´æ¥äº¤æµæˆä¸ºå¯èƒ½ï¼Œå¦‚ç¤¾äº¤é™ªä¼´å’Œå¿ƒç†æ”¯æŒã€‚ç„¶è€Œï¼ŒLLMåœ¨å®é™…å¯¹è¯ä¸­çš„æƒ…æ„Ÿæ„ŸçŸ¥å’Œç¤¾ä¼šäº¤å¾€èƒ½åŠ›å­˜åœ¨å±€é™ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºPersonaFuseçš„æ–°å‹LLMåè®­ç»ƒæ¡†æ¶ï¼Œå®ƒèƒ½å¤Ÿè®©LLMé€‚åº”å¹¶è¡¨è¾¾ä¸åŒçš„ä¸ªæ€§ä»¥é€‚åº”ä¸åŒæƒ…å¢ƒã€‚è¯¥æ¡†æ¶å—åˆ°ç‰¹è´¨æ¿€æ´»ç†è®ºå’Œäº”å¤§äººæ ¼æ¨¡å‹çš„å¯å‘ï¼Œé‡‡ç”¨æ··åˆä¸“å®¶æ¶æ„ï¼Œç»“åˆäººæ ¼é€‚é…å™¨å’ŒåŠ¨æ€è·¯ç”±ç½‘ç»œï¼Œå®ç°ä¸Šä¸‹æ–‡ç‰¹è´¨è¡¨è¾¾ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPersonaFuseåœ¨ç¤¾ä¼šæƒ…æ„Ÿæ™ºèƒ½çš„å¤šä¸ªç»´åº¦ä¸Šå¤§å¹…è¶…è¶ŠåŸºå‡†æ¨¡å‹ï¼Œä¸”åœ¨ä¸ç‰ºç‰²é€šç”¨æ¨ç†èƒ½åŠ›æˆ–æ¨¡å‹å®‰å…¨æ€§çš„å‰æä¸‹å–å¾—äº†è¿™äº›è¿›æ­¥ã€‚æ­¤å¤–ï¼ŒPersonaFuseåœ¨å¿ƒç†å¥åº·å’¨è¯¢å’ŒåŸºäºè¯„è®ºçš„å®¢æœç­‰é¢å‘äººç±»çš„åº”ç”¨ä¸­ä¹Ÿè¡¨ç°å‡ºæŒç»­çš„ä¼˜åŠ¿ã€‚ä¸äººç±»åå¥½è¯„ä¼°é¢†å…ˆLLMç›¸æ¯”ï¼Œå¦‚GPT-4oå’ŒDeepSeekï¼Œå°½ç®¡æ¨¡å‹è§„æ¨¡è¾ƒå°ï¼Œä½†PersonaFuseåœ¨å“åº”è´¨é‡ä¸Šè¡¨ç°å‡ºç«äº‰åŠ›ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼ŒPersonaFuseä¸ºå¼€å‘ç¤¾ä¼šæƒ…æ„Ÿå¢å¼ºå‹LLMæä¾›äº†ç†è®ºä¸Šçš„æ”¯æŒå’Œå®ç”¨çš„æ–¹æ³•ï¼Œæ ‡å¿—ç€å‘æ›´äººæ€§åŒ–çš„AIç³»ç»Ÿè¿ˆå‡ºäº†é‡è¦çš„ä¸€æ­¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsçš„è¿›æ­¥ä¿ƒè¿›äº†è·¨é¢†åŸŸäº¤æµï¼Œå®ç°äº†å¤šç§æƒ…å¢ƒä¸‹çš„ä¸äººç±»äº¤æµèƒ½åŠ›ã€‚</li>
<li>LLMåœ¨æƒ…æ„Ÿæ„ŸçŸ¥å’Œç¤¾ä¼šäº¤å¾€æ–¹é¢å­˜åœ¨å±€é™ã€‚</li>
<li>PersonaFuseæ¡†æ¶è¢«æå‡ºï¼Œè§£å†³äº†LLMçš„å±€é™æ€§é—®é¢˜ï¼Œæå‡å…¶åœ¨ä¸åŒæƒ…å¢ƒä¸‹çš„æƒ…æ„Ÿè¡¨è¾¾å’Œä¸ªæ€§é€‚åº”èƒ½åŠ›ã€‚</li>
<li>PersonaFuseç»“åˆç‰¹è´¨æ¿€æ´»ç†è®ºå’Œäº”å¤§äººæ ¼æ¨¡å‹ã€‚</li>
<li>PersonaFuseé‡‡ç”¨æ··åˆä¸“å®¶æ¶æ„å’ŒåŠ¨æ€è·¯ç”±ç½‘ç»œæ¥å®ç°ä¸Šä¸‹æ–‡ç‰¹è´¨è¡¨è¾¾ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼ŒPersonaFuseåœ¨å¤šä¸ªç»´åº¦ä¸Šè¶…è¶Šäº†åŸºå‡†æ¨¡å‹ï¼Œä¸”ä¸å½±å“é€šç”¨æ¨ç†èƒ½åŠ›å’Œæ¨¡å‹å®‰å…¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.07370">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2509.07370v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2509.07370v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2509.07370v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="MachineLearningLM-Scaling-Many-shot-In-context-Learning-via-Continued-Pretraining"><a href="#MachineLearningLM-Scaling-Many-shot-In-context-Learning-via-Continued-Pretraining" class="headerlink" title="MachineLearningLM: Scaling Many-shot In-context Learning via Continued   Pretraining"></a>MachineLearningLM: Scaling Many-shot In-context Learning via Continued   Pretraining</h2><p><strong>Authors:Haoyu Dong, Pengkun Zhang, Mingzhe Lu, Yanzhen Shen, Guolin Ke</strong></p>
<p>Large language models (LLMs) possess broad world knowledge and strong general-purpose reasoning ability, yet they struggle to learn from many in-context examples on standard machine learning (ML) tasks, that is, to leverage many-shot demonstrations purely via in-context learning (ICL) without gradient descent. We introduce MachineLearningLM, a portable continued-pretraining framework that equips a general-purpose LLM with robust in-context ML capability while preserving its general knowledge and reasoning for broader chat workflows.   Our pretraining procedure synthesizes ML tasks from millions of structural causal models (SCMs), spanning shot counts up to 1,024. We begin with a random-forest teacher, distilling tree-based decision strategies into the LLM to strengthen robustness in numerical modeling. All tasks are serialized with a token-efficient prompt, enabling 3x to 6x more examples per context window and delivering up to 50x amortized throughput via batch inference.   Despite a modest setup (Qwen-2.5-7B-Instruct with LoRA rank 8), MachineLearningLM outperforms strong LLM baselines (e.g., GPT-5-mini) by an average of about 15% on out-of-distribution tabular classification across finance, physics, biology, and healthcare domains. It exhibits a striking many-shot scaling law: accuracy increases monotonically as in-context demonstrations grow from 8 to 1,024. Without any task-specific training, it attains random-forest-level accuracy across hundreds of shots. General chat capabilities, including knowledge and reasoning, are preserved: it achieves 75.4% on MMLU. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ‹¥æœ‰å¹¿æ³›çš„ä¸–ç•ŒçŸ¥è¯†å’Œå¼ºå¤§çš„é€šç”¨æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨æ ‡å‡†æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰ä»»åŠ¡ä¸Šï¼Œå®ƒä»¬å¾ˆéš¾ä»å¤šä¸ªä¸Šä¸‹æ–‡ç¤ºä¾‹ä¸­å­¦ä¹ ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œå®ƒä»¬æ— æ³•ä»…é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰åˆ©ç”¨å¤šä¸ªç¤ºä¾‹æ¼”ç¤ºï¼Œè€Œæ— éœ€è¿›è¡Œæ¢¯åº¦ä¸‹é™ã€‚æˆ‘ä»¬å¼•å…¥äº†MachineLearningLMï¼Œè¿™æ˜¯ä¸€ä¸ªä¾¿æºå¼æŒç»­é¢„è®­ç»ƒæ¡†æ¶ï¼Œå®ƒä½¿é€šç”¨LLMå…·å¤‡å¼ºå¤§çš„ä¸Šä¸‹æ–‡MLèƒ½åŠ›ï¼ŒåŒæ—¶ä¿ç•™å…¶ä¸€èˆ¬çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ï¼Œä»¥æ”¯æŒæ›´å¹¿æ³›çš„èŠå¤©å·¥ä½œæµç¨‹ã€‚æˆ‘ä»¬çš„é¢„è®­ç»ƒç¨‹åºä»æ•°ç™¾ä¸‡ä¸ªç»“æ„å› æœæ¨¡å‹ï¼ˆSCMï¼‰ä¸­ç»¼åˆMLä»»åŠ¡ï¼Œæ¶µç›–ç¤ºä¾‹æ•°é‡é«˜è¾¾1024ä¸ªã€‚æˆ‘ä»¬ä»¥éšæœºæ£®æ—æ•™å¸ˆå¼€å§‹ï¼Œå°†åŸºäºæ ‘çš„å†³ç­–ç­–ç•¥è’¸é¦åˆ°LLMä¸­ï¼Œä»¥åŠ å¼ºæ•°å€¼å»ºæ¨¡ä¸­çš„ç¨³å¥æ€§ã€‚æ‰€æœ‰ä»»åŠ¡éƒ½é€šè¿‡é«˜æ•ˆçš„ä»¤ç‰Œæç¤ºè¿›è¡Œåºåˆ—åŒ–ï¼Œå¯ä»¥åœ¨æ¯ä¸ªä¸Šä¸‹æ–‡çª—å£ä¸­å®ç°3åˆ°6å€çš„æ›´å¤šç¤ºä¾‹ï¼Œå¹¶é€šè¿‡æ‰¹é‡æ¨ç†å®ç°é«˜è¾¾50å€çš„æ‘Šé”€ååé‡ã€‚å°½ç®¡é…ç½®ç›¸å¯¹ç®€å•ï¼ˆä½¿ç”¨LoRAç­‰çº§8çš„Qwen-2.5-7B-Instructï¼‰ï¼Œä½†MachineLearningLMåœ¨é‡‘èã€ç‰©ç†ã€ç”Ÿç‰©å’ŒåŒ»ç–—ä¿å¥é¢†åŸŸçš„ç¦»ç¾¤åˆ†å¸ƒè¡¨æ ¼åˆ†ç±»æ–¹é¢ï¼Œå¹³å‡æ¯”å¼ºå¤§çš„LLMåŸºå‡†æµ‹è¯•ï¼ˆä¾‹å¦‚GPT-5-miniï¼‰é«˜å‡ºçº¦15%ã€‚å®ƒè¡¨ç°å‡ºæƒŠäººçš„å¤šç¤ºä¾‹æ‰©å±•å®šå¾‹ï¼šéšç€ä¸Šä¸‹æ–‡æ¼”ç¤ºä»8ä¸ªå¢åŠ åˆ°1024ä¸ªï¼Œå‡†ç¡®æ€§ä¼šå•è°ƒå¢åŠ ã€‚æ— éœ€ä»»ä½•ç‰¹å®šä»»åŠ¡è®­ç»ƒï¼Œå®ƒå°±èƒ½è·å¾—éšæœºæ£®æ—çº§åˆ«çš„å‡†ç¡®åº¦ï¼ˆæ•°ç™¾ä¸ªç¤ºä¾‹ï¼‰ã€‚åŒæ—¶ä¿ç•™äº†ä¸€èˆ¬èŠå¤©åŠŸèƒ½ï¼ŒåŒ…æ‹¬çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ï¼šå®ƒåœ¨MMLUä¸Šè¾¾åˆ°äº†75.4%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.06806v3">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹å…·å¤‡å¹¿æ³›çš„ä¸–ç•ŒçŸ¥è¯†å’Œå¼ºå¤§çš„é€šç”¨æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨æ ‡å‡†æœºå™¨å­¦ä¹ ä»»åŠ¡ä¸Šéš¾ä»¥ä»å¤šä¸ªä¸Šä¸‹æ–‡ç¤ºä¾‹ä¸­å­¦ä¹ ã€‚ä¸ºåº”å¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†MachineLearningLMï¼Œè¿™æ˜¯ä¸€ä¸ªä¾¿æºå¼æŒç»­é¢„è®­ç»ƒæ¡†æ¶ï¼Œèµ‹äºˆé€šç”¨è¯­è¨€æ¨¡å‹å¼ºå¤§çš„ä¸Šä¸‹æ–‡æœºå™¨å­¦ä¹ èƒ½åŠ›ï¼ŒåŒæ—¶ä¿ç•™å…¶ä¸€èˆ¬çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ä»¥æ”¯æŒæ›´å¹¿æ³›çš„èŠå¤©å·¥ä½œæµç¨‹ã€‚æˆ‘ä»¬çš„é¢„è®­ç»ƒç¨‹åºé€šè¿‡æ•°ç™¾ä¸‡çš„ç»“æ„å› æœæ¨¡å‹åˆæˆæœºå™¨å­¦ä¹ ä»»åŠ¡ï¼Œæ¶µç›–å¤šè¾¾1024ä¸ªæ ·æœ¬æ•°ã€‚æˆ‘ä»¬ä»¥éšæœºæ£®æ—æ•™å¸ˆä¸ºåŸºç¡€ï¼Œå°†æ ‘å½¢å†³ç­–ç­–ç•¥çŒè¾“åˆ°è¯­è¨€æ¨¡å‹ä¸­ï¼ŒåŠ å¼ºå…¶åœ¨æ•°å€¼å»ºæ¨¡ä¸­çš„ç¨³å¥æ€§ã€‚æ‰€æœ‰ä»»åŠ¡éƒ½é€šè¿‡é«˜æ•ˆçš„æ ‡è®°æç¤ºè¿›è¡Œåºåˆ—åŒ–ï¼Œèƒ½å¤Ÿåœ¨æ¯ä¸ªä¸Šä¸‹æ–‡çª—å£ä¸­å¢åŠ 3åˆ°6å€çš„ç¤ºä¾‹ï¼Œå¹¶é€šè¿‡æ‰¹é‡æ¨ç†å®ç°é«˜è¾¾50å€çš„æ‘Šé”€ååé‡ã€‚å°½ç®¡ä½¿ç”¨çš„è®¾ç½®è¾ƒä¸ºç®€å•ï¼ˆå¸¦æœ‰LoRAæ’åçš„Qwen-2.5-7B-Instructï¼‰ï¼Œä½†MachineLearningLMåœ¨é‡‘èã€ç‰©ç†ã€ç”Ÿç‰©å’ŒåŒ»ç–—ä¿å¥é¢†åŸŸçš„åˆ†ç±»è¡¨ä¸Šä»ä¼˜äºå¼ºå¤§çš„è¯­è¨€æ¨¡å‹åŸºå‡†æµ‹è¯•ï¼ˆå¦‚GPT-5-miniï¼‰ï¼Œå¹³å‡æé«˜äº†çº¦15%çš„å‡†ç¡®åº¦ã€‚å®ƒå±•ç°äº†ä¸€ä¸ªå¼•äººæ³¨ç›®çš„å¤šç¤ºä¾‹è§„æ¨¡æ•ˆåº”ï¼šéšç€ä¸Šä¸‹æ–‡æ¼”ç¤ºä»8ä¸ªå¢åŠ åˆ°1024ä¸ªï¼Œå‡†ç¡®åº¦å•è°ƒå¢åŠ ã€‚æ— éœ€ä»»ä½•ç‰¹å®šä»»åŠ¡è®­ç»ƒï¼Œå®ƒå³å¯è¾¾åˆ°éšæœºæ£®æ—çº§åˆ«çš„ç²¾åº¦ï¼ŒåŒæ—¶ä¿ç•™äº†ä¸€èˆ¬èŠå¤©èƒ½åŠ›ï¼ŒåŒ…æ‹¬çŸ¥è¯†å’Œæ¨ç†ï¼Œåœ¨MMLUä¸Šè¾¾åˆ°75.4%ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹å…·å¤‡å¹¿æ³›çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨æ ‡å‡†æœºå™¨å­¦ä¹ ä»»åŠ¡ä¸Šçš„å­¦ä¹ èƒ½åŠ›å—é™ã€‚</li>
<li>æå‡ºäº†MachineLearningLMæ¡†æ¶ï¼Œç»“åˆæŒç»­é¢„è®­ç»ƒï¼Œå¢å¼ºè¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡æœºå™¨å­¦ä¹ èƒ½åŠ›ã€‚</li>
<li>é¢„è®­ç»ƒç¨‹åºé€šè¿‡ç»“æ„å› æœæ¨¡å‹åˆæˆä»»åŠ¡ï¼Œæ¶µç›–å¤šç§æ ·æœ¬æ•°ï¼Œæé«˜æ¨¡å‹çš„ç¨³å¥æ€§ã€‚</li>
<li>ä½¿ç”¨éšæœºæ£®æ—æ•™å¸ˆæ¥æŒ‡å¯¼æ¨¡å‹å­¦ä¹ ï¼Œæå‡æ•°å€¼å»ºæ¨¡çš„ç¨³å¥æ€§ã€‚</li>
<li>é€šè¿‡é«˜æ•ˆçš„æç¤ºåºåˆ—åŒ–ä»»åŠ¡ï¼Œå¢åŠ ä¸Šä¸‹æ–‡çª—å£ä¸­çš„ç¤ºä¾‹æ•°é‡ï¼Œæé«˜æ¨ç†æ•ˆç‡ã€‚</li>
<li>MachineLearningLMåœ¨å¤šä¸ªé¢†åŸŸåˆ†ç±»ä»»åŠ¡ä¸Šä¼˜äºåŸºå‡†æµ‹è¯•ï¼Œå±•ç°å¼ºå¤§çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.06806">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2509.06806v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2509.06806v3/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2509.06806v3/page_4_0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Robix-A-Unified-Model-for-Robot-Interaction-Reasoning-and-Planning"><a href="#Robix-A-Unified-Model-for-Robot-Interaction-Reasoning-and-Planning" class="headerlink" title="Robix: A Unified Model for Robot Interaction, Reasoning and Planning"></a>Robix: A Unified Model for Robot Interaction, Reasoning and Planning</h2><p><strong>Authors:Huang Fang, Mengxi Zhang, Heng Dong, Wei Li, Zixuan Wang, Qifeng Zhang, Xueyun Tian, Yucheng Hu, Hang Li</strong></p>
<p>We introduce Robix, a unified model that integrates robot reasoning, task planning, and natural language interaction within a single vision-language architecture. Acting as the high-level cognitive layer in a hierarchical robot system, Robix dynamically generates atomic commands for the low-level controller and verbal responses for human interaction, enabling robots to follow complex instructions, plan long-horizon tasks, and interact naturally with human within an end-to-end framework. Robix further introduces novel capabilities such as proactive dialogue, real-time interruption handling, and context-aware commonsense reasoning during task execution. At its core, Robix leverages chain-of-thought reasoning and adopts a three-stage training strategy: (1) continued pretraining to enhance foundational embodied reasoning abilities including 3D spatial understanding, visual grounding, and task-centric reasoning; (2) supervised finetuning to model human-robot interaction and task planning as a unified reasoning-action sequence; and (3) reinforcement learning to improve reasoning-action consistency and long-horizon task coherence. Extensive experiments demonstrate that Robix outperforms both open-source and commercial baselines (e.g., GPT-4o and Gemini 2.5 Pro) in interactive task execution, demonstrating strong generalization across diverse instruction types (e.g., open-ended, multi-stage, constrained, invalid, and interrupted) and various user-involved tasks such as table bussing, grocery shopping, and dietary filtering. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†Robixï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€æ¨¡å‹ï¼Œå®ƒåœ¨ä¸€ä¸ªè§†è§‰è¯­è¨€æ¶æ„ä¸­é›†æˆäº†æœºå™¨äººæ¨ç†ã€ä»»åŠ¡è§„åˆ’çš„è‡ªç„¶è¯­è¨€äº¤äº’ã€‚åœ¨åˆ†å±‚æœºå™¨äººç³»ç»Ÿä¸­ï¼ŒRobixå……å½“é«˜çº§è®¤çŸ¥å±‚ï¼Œä¸ºä½çº§æ§åˆ¶å™¨åŠ¨æ€ç”ŸæˆåŸå­å‘½ä»¤å’Œäººç±»äº¤äº’çš„è¨€è¯­å“åº”ï¼Œä½¿æœºå™¨äººèƒ½å¤Ÿéµå¾ªå¤æ‚æŒ‡ä»¤ã€è§„åˆ’é•¿æœŸä»»åŠ¡ï¼Œå¹¶åœ¨ç«¯åˆ°ç«¯æ¡†æ¶å†…ä¸äººç±»è‡ªç„¶äº¤äº’ã€‚Robixè¿˜å¼•å…¥äº†æ–°é¢–çš„åŠŸèƒ½ï¼Œå¦‚ä¸»åŠ¨å¯¹è¯ã€å®æ—¶ä¸­æ–­å¤„ç†å’Œä»»åŠ¡æ‰§è¡Œè¿‡ç¨‹ä¸­çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥å¸¸è¯†æ¨ç†ã€‚å…¶æ ¸å¿ƒæ˜¯åˆ©ç”¨é“¾å¼æ€ç»´æ¨ç†å¹¶é‡‡ç”¨ä¸‰é˜¶æ®µåŸ¹è®­ç­–ç•¥ï¼š1ï¼‰æŒç»­é¢„è®­ç»ƒï¼Œä»¥å¢å¼ºåŸºæœ¬çš„èº«ä½“åŒ–æ¨ç†èƒ½åŠ›ï¼ŒåŒ…æ‹¬3Dç©ºé—´ç†è§£ã€è§†è§‰å®šä½å’Œä»»åŠ¡ä¸­å¿ƒæ¨ç†ï¼›2ï¼‰ç›‘ç£å¾®è°ƒï¼Œä»¥å°†äººæœºäº’åŠ¨å’Œä»»åŠ¡è§„åˆ’å»ºæ¨¡ä¸ºç»Ÿä¸€çš„æ¨ç†è¡ŒåŠ¨åºåˆ—ï¼›3ï¼‰å¼ºåŒ–å­¦ä¹ ï¼Œä»¥æé«˜æ¨ç†è¡ŒåŠ¨çš„åè°ƒæ€§å’Œé•¿æœŸä»»åŠ¡çš„è¿è´¯æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œåœ¨äº¤äº’å¼ä»»åŠ¡æ‰§è¡Œæ–¹é¢ï¼ŒRobixçš„è¡¨ç°ä¼˜äºå¼€æºå’Œå•†ä¸šåŸºå‡†ï¼ˆä¾‹å¦‚GPT-4oå’ŒGemini 2.5 Proï¼‰ï¼Œåœ¨å¤šç§æŒ‡ä»¤ç±»å‹ï¼ˆå¦‚å¼€æ”¾å¼ã€å¤šé˜¶æ®µã€çº¦æŸã€æ— æ•ˆå’Œä¸­æ–­ï¼‰å’Œç”¨æˆ·å‚ä¸çš„å„ç§ä»»åŠ¡ï¼ˆå¦‚é¤æ¡ŒæœåŠ¡ã€è´­ç‰©å’Œé¥®é£Ÿè¿‡æ»¤ï¼‰ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.01106v2">PDF</a> Tech report. Project page: <a target="_blank" rel="noopener" href="https://robix-seed.github.io/robix/">https://robix-seed.github.io/robix/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Robixï¼Œä¸€ä¸ªç»Ÿä¸€æ¨¡å‹ï¼Œå®ƒå°†æœºå™¨äººæ¨ç†ã€ä»»åŠ¡è§„åˆ’å’Œè‡ªç„¶è¯­è¨€äº¤äº’é›†æˆåœ¨ä¸€ä¸ªè§†è§‰è¯­è¨€æ¶æ„ä¸­ã€‚Robixä½œä¸ºåˆ†å±‚æœºå™¨äººç³»ç»Ÿçš„é«˜çº§è®¤çŸ¥å±‚ï¼Œèƒ½å¤ŸåŠ¨æ€ç”Ÿæˆä½çº§æ§åˆ¶å™¨çš„åŸå­å‘½ä»¤å’Œäººç±»äº¤äº’çš„è¨€è¯­å›åº”ï¼Œä½¿æœºå™¨äººèƒ½å¤Ÿæ‰§è¡Œå¤æ‚çš„æŒ‡ä»¤ã€è§„åˆ’é•¿æœŸä»»åŠ¡ï¼Œå¹¶åœ¨ç«¯åˆ°ç«¯æ¡†æ¶å†…ä¸äººç±»è‡ªç„¶äº¤äº’ã€‚å…¶æ ¸å¿ƒé‡‡ç”¨é“¾å¼æ€ç»´æ¨ç†ï¼Œå¹¶é‡‡ç”¨ä¸‰é˜¶æ®µåŸ¹è®­ç­–ç•¥ï¼ŒåŒ…æ‹¬æŒç»­é¢„è®­ç»ƒã€ç›‘ç£å¾®è°ƒä»¥åŠå¼ºåŒ–å­¦ä¹ ã€‚å®éªŒè¡¨æ˜ï¼ŒRobixåœ¨äº¤äº’å¼ä»»åŠ¡æ‰§è¡Œæ–¹é¢ä¼˜äºå¼€æºå’Œå•†ä¸šåŸºå‡†ï¼Œå±•ç°å‡ºå¼ºå¤§çš„æŒ‡ä»¤ç±»å‹æ³›åŒ–èƒ½åŠ›ï¼ŒåŒ…æ‹¬å¼€æ”¾å¼ã€å¤šé˜¶æ®µã€çº¦æŸã€æ— æ•ˆå’Œä¸­æ–­ç­‰ï¼Œä»¥åŠæ¶‰åŠç”¨æˆ·çš„å„ç§ä»»åŠ¡ï¼Œå¦‚é¤æ¡ŒæœåŠ¡ã€è´­ç‰©å’Œé¥®é£Ÿè¿‡æ»¤ç­‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Robixæ˜¯ä¸€ä¸ªé›†æˆæœºå™¨äººæ¨ç†ã€ä»»åŠ¡è§„åˆ’å’Œè‡ªç„¶è¯­è¨€äº¤äº’çš„ç»Ÿä¸€æ¨¡å‹ã€‚</li>
<li>Robixåœ¨åˆ†å±‚æœºå™¨äººç³»ç»Ÿä¸­ä½œä¸ºé«˜çº§è®¤çŸ¥å±‚ï¼Œèƒ½ç”ŸæˆåŸå­å‘½ä»¤å’Œè¨€è¯­å›åº”ã€‚</li>
<li>Robixå…·å¤‡å¤æ‚æŒ‡ä»¤æ‰§è¡Œã€é•¿æœŸä»»åŠ¡è§„åˆ’å’Œè‡ªç„¶äººæœºäº¤äº’èƒ½åŠ›ã€‚</li>
<li>Robixé‡‡ç”¨é“¾å¼æ€ç»´æ¨ç†å’Œä¸‰é˜¶æ®µåŸ¹è®­ç­–ç•¥ï¼ŒåŒ…æ‹¬æŒç»­é¢„è®­ç»ƒã€ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ ã€‚</li>
<li>Robixå…·å¤‡ä¸»åŠ¨å¯¹è¯ã€å®æ—¶ä¸­æ–­å¤„ç†å’Œè¯­å¢ƒå¸¸è¯†æ¨ç†ç­‰æ–°åŠŸèƒ½ã€‚</li>
<li>Robixåœ¨äº¤äº’å¼ä»»åŠ¡æ‰§è¡Œæ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä¼˜äºå¼€æºå’Œå•†ä¸šåŸºå‡†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.01106">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2509.01106v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2509.01106v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2509.01106v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2509.01106v2/page_4_0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="CARFT-Boosting-LLM-Reasoning-via-Contrastive-Learning-with-Annotated-Chain-of-Thought-based-Reinforced-Fine-Tuning"><a href="#CARFT-Boosting-LLM-Reasoning-via-Contrastive-Learning-with-Annotated-Chain-of-Thought-based-Reinforced-Fine-Tuning" class="headerlink" title="CARFT: Boosting LLM Reasoning via Contrastive Learning with Annotated   Chain-of-Thought-based Reinforced Fine-Tuning"></a>CARFT: Boosting LLM Reasoning via Contrastive Learning with Annotated   Chain-of-Thought-based Reinforced Fine-Tuning</h2><p><strong>Authors:Wenqiao Zhu, Ji Liu, Rongjuncheng Zhang, Haipang Wu, Yulun Zhang</strong></p>
<p>Reasoning capability plays a significantly critical role in the the broad applications of Large Language Models (LLMs). To enhance the reasoning performance of LLMs, diverse Reinforcement Learning (RL)-based fine-tuning approaches have been proposed to address the limited generalization capability of LLMs trained solely via Supervised Fine-Tuning (SFT). Despite their effectiveness, two major limitations hinder the advancement of LLMs. First, vanilla RL-based approaches ignore annotated Chain-of-Thought (CoT) and incorporate unstable reasoning path sampling, which typically results in model collapse, unstable training process, and suboptimal performance. Second, existing SFT approaches generally overemphasize the annotated CoT, potentially leading to performance degradation due to insufficient exploitation of potential CoT. In this paper, we propose a Contrastive learning with annotated CoT-based Reinforced Fine-Tuning approach, i.e., \TheName{}, to enhance the reasoning performance of LLMs while addressing the aforementioned limitations. Specifically, we propose learning a representation for each CoT. Based on this representation, we design novel contrastive signals to guide the fine-tuning process. Our approach not only fully exploits the available annotated CoT but also stabilizes the fine-tuning procedure by incorporating an additional unsupervised learning signal. We conduct comprehensive experiments and in-depth analysis with three baseline approaches, two foundation models, and two datasets to demonstrate significant advantages of \TheName{} in terms of robustness, performance (up to 10.15%), and efficiency (up to 30.62%). Code is available at <a target="_blank" rel="noopener" href="https://github.com/WNQzhu/CARFT">https://github.com/WNQzhu/CARFT</a>. </p>
<blockquote>
<p>æ¨ç†èƒ½åŠ›åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¹¿æ³›åº”ç”¨ä¸­å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ä¸ºäº†æé«˜LLMçš„æ¨ç†æ€§èƒ½ï¼Œå·²ç»æå‡ºäº†å¤šç§åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„å¾®è°ƒæ–¹æ³•æ¥è§£å†³ä»…é€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è®­ç»ƒçš„LLMçš„æ³›åŒ–èƒ½åŠ›æœ‰é™çš„é—®é¢˜ã€‚å°½ç®¡è¿™äº›æ–¹æ³•æœ‰æ•ˆï¼Œä½†ä¸¤ä¸ªä¸»è¦å±€é™æ€§é˜»ç¢äº†LLMçš„è¿›æ­¥ã€‚é¦–å…ˆï¼Œæ™®é€šçš„åŸºäºRLçš„æ–¹æ³•å¿½ç•¥äº†æ³¨é‡Šçš„æ€ç»´é“¾ï¼ˆCoTï¼‰å¹¶èå…¥äº†ä¸ç¨³å®šçš„æ¨ç†è·¯å¾„é‡‡æ ·ï¼Œè¿™é€šå¸¸ä¼šå¯¼è‡´æ¨¡å‹å´©æºƒã€è®­ç»ƒè¿‡ç¨‹ä¸ç¨³å®šå’Œæ€§èƒ½ä¸ä½³ã€‚å…¶æ¬¡ï¼Œç°æœ‰çš„SFTæ–¹æ³•é€šå¸¸è¿‡åˆ†å¼ºè°ƒæ³¨é‡Šçš„CoTï¼Œå¯èƒ½å¯¼è‡´æ€§èƒ½ä¸‹é™ï¼Œå› ä¸ºæœªèƒ½å……åˆ†åˆ©ç”¨æ½œåœ¨çš„CoTã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»“åˆæ³¨é‡Šçš„CoTçš„å¼ºåŒ–å¾®è°ƒæ–¹æ³•ï¼Œå³â€œå¯¹æ¯”å­¦ä¹ ä¸å¼ºåŒ–æ¨ç†è®­ç»ƒï¼ˆContrastive and Reinforced Learning with Annotated CoTï¼‰â€ï¼Œä»¥å¢å¼ºLLMçš„æ¨ç†æ€§èƒ½å¹¶è§£å†³ä¸Šè¿°å±€é™æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä¸ºæ¯ä¸ªCoTå­¦ä¹ ä¸€ç§è¡¨ç¤ºã€‚åŸºäºæ­¤è¡¨ç¤ºï¼Œæˆ‘ä»¬è®¾è®¡æ–°å‹å¯¹æ¯”ä¿¡å·æ¥æŒ‡å¯¼å¾®è°ƒè¿‡ç¨‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…å……åˆ†åˆ©ç”¨äº†å¯ç”¨çš„æ³¨é‡ŠCoTï¼Œè¿˜é€šè¿‡å¼•å…¥é¢å¤–çš„æ— ç›‘ç£å­¦ä¹ ä¿¡å·æ¥ç¨³å®šå¾®è°ƒè¿‡ç¨‹ã€‚æˆ‘ä»¬é€šè¿‡ä¸‰ç§åŸºç¡€æ–¹æ³•ã€ä¸¤ç§åŸºç¡€æ¨¡å‹å’Œä¸¤ç§æ•°æ®é›†çš„ç»¼åˆå®éªŒå’Œæ·±å…¥åˆ†æï¼Œè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨ç¨³å¥æ€§ã€æ€§èƒ½ï¼ˆæœ€é«˜æå‡10.15%ï¼‰å’Œæ•ˆç‡ï¼ˆæœ€é«˜æå‡30.62%ï¼‰æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/WNQzhu/CARFT%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/WNQzhu/CARFTæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15868v2">PDF</a> 14 pages, to appear in EMNLP25</p>
<p><strong>Summary</strong></p>
<p>å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›åœ¨å„ç§åº”ç”¨åœºæ™¯ä¸­å‘æŒ¥ç€é‡è¦ä½œç”¨ã€‚ä¸ºæé«˜å…¶æ¨ç†æ€§èƒ½ï¼Œç ”ç©¶è€…æå‡ºåŸºäºå¼ºåŒ–å­¦ä¹ çš„å¾®è°ƒæ–¹æ³•æ¥è§£å†³å•çº¯ç›‘ç£å¾®è°ƒå¯¼è‡´çš„æ¨¡å‹æ³›åŒ–èƒ½åŠ›æœ‰é™çš„é—®é¢˜ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å­˜åœ¨ä¸¤å¤§å±€é™ï¼šä¸€æ˜¯æ™®é€šå¼ºåŒ–å­¦ä¹ å¿½ç•¥æ ‡æ³¨çš„æ€ç»´é“¾ï¼Œå¯¼è‡´æ¨¡å‹å´©æºƒå’Œä¸ç¨³å®šè®­ç»ƒè¿‡ç¨‹ï¼›äºŒæ˜¯ç°æœ‰ç›‘ç£å¾®è°ƒæ–¹æ³•è¿‡åº¦ä¾èµ–æ ‡æ³¨çš„æ€ç»´é“¾ï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨æ½œåœ¨æ€ç»´é“¾ã€‚æœ¬æ–‡æå‡ºä¸€ç§ç»“åˆæ ‡æ³¨æ€ç»´é“¾çš„å¯¹æ¯”å¼ºåŒ–å­¦ä¹ å¾®è°ƒæ–¹æ³•ï¼ˆ\TheName{}ï¼‰ï¼Œæ—¨åœ¨æé«˜å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†æ€§èƒ½å¹¶è§£å†³ä¸Šè¿°é—®é¢˜ã€‚è¯¥æ–¹æ³•ä¸ä»…å……åˆ†åˆ©ç”¨æ ‡æ³¨çš„æ€ç»´é“¾ï¼Œè¿˜é€šè¿‡å¼•å…¥é¢å¤–çš„æ— ç›‘ç£å­¦ä¹ ä¿¡å·æ¥ç¨³å®šå¾®è°ƒè¿‡ç¨‹ã€‚å®éªŒå’Œæ·±åº¦åˆ†æè¡¨æ˜ï¼Œ\TheName{}åœ¨é²æ£’æ€§ã€æ€§èƒ½ï¼ˆæœ€é«˜æå‡10.15%ï¼‰å’Œæ•ˆç‡ï¼ˆæœ€é«˜æå‡30.62%ï¼‰æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¨ç†èƒ½åŠ›åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¹¿æ³›åº”ç”¨ä¸­èµ·åˆ°å…³é”®ä½œç”¨ã€‚</li>
<li>ä¸ºæé«˜LLMsçš„æ¨ç†æ€§èƒ½ï¼Œç ”ç©¶è€…å¼•å…¥äº†åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„å¾®è°ƒæ–¹æ³•ã€‚</li>
<li>æ™®é€šRLæ–¹æ³•å­˜åœ¨å¿½ç•¥æ ‡æ³¨çš„æ€ç»´é“¾ï¼ˆCoTï¼‰å’Œä¸ç¨³å®šæ¨ç†è·¯å¾„é‡‡æ ·çš„é—®é¢˜ï¼Œå¯¼è‡´æ¨¡å‹å´©æºƒå’Œè®­ç»ƒä¸ç¨³å®šã€‚</li>
<li>ç°æœ‰ç›‘ç£å¾®è°ƒæ–¹æ³•è¿‡åº¦ä¾èµ–æ ‡æ³¨çš„CoTï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨æ½œåœ¨CoTã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆæ ‡æ³¨CoTçš„å¯¹æ¯”å¼ºåŒ–å­¦ä¹ å¾®è°ƒæ–¹æ³•ï¼ˆ\TheName{}ï¼‰ï¼Œæ—¨åœ¨è§£å†³ä¸Šè¿°é—®é¢˜å¹¶æå‡LLMsçš„æ¨ç†æ€§èƒ½ã€‚</li>
<li>\TheName{}é€šè¿‡å¼•å…¥å¯¹æ¯”å­¦ä¹ å’Œæ ‡æ³¨CoTçš„ç»“åˆï¼Œä¸ä»…å……åˆ†åˆ©ç”¨äº†æ ‡æ³¨æ•°æ®ï¼Œè¿˜é€šè¿‡å¼•å…¥æ— ç›‘ç£å­¦ä¹ ä¿¡å·æ¥ç¨³å®šè®­ç»ƒè¿‡ç¨‹ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œ\TheName{}åœ¨æ€§èƒ½ã€é²æ£’æ€§å’Œæ•ˆç‡æ–¹é¢å‡è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œæœ€é«˜æå‡åˆ†åˆ«è¾¾10.15%ã€30.62%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15868">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2508.15868v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2508.15868v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2508.15868v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2508.15868v2/page_3_0.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Klear-CodeTest-Scalable-Test-Case-Generation-for-Code-Reinforcement-Learning"><a href="#Klear-CodeTest-Scalable-Test-Case-Generation-for-Code-Reinforcement-Learning" class="headerlink" title="Klear-CodeTest: Scalable Test Case Generation for Code Reinforcement   Learning"></a>Klear-CodeTest: Scalable Test Case Generation for Code Reinforcement   Learning</h2><p><strong>Authors:Jia Fu, Xinyu Yang, Hongzhi Zhang, Yahui Liu, Jingyuan Zhang, Qi Wang, Fuzheng Zhang, Guorui Zhou</strong></p>
<p>Precise, correct feedback is crucial for effectively training large language models (LLMs) in code reinforcement learning. However, synthesizing high-quality test cases remains a profoundly challenging and unsolved problem. In this work, we present Klear-CodeTest, a comprehensive test case synthesis framework featuring rigorous verification to ensure quality and reliability of test cases. Our approach achieves broad coverage of programming problems via a novel Generator-Validation (G-V) framework, ensuring correctness through a consistency validation mechanism that verifies outputs against gold solutions. The proposed G-V framework generates comprehensive test cases including both regular and corner cases, enhancing test coverage and discriminative power for solution correctness assessment in code reinforcement learning. In addition, we design a multi-layered security sandbox system optimized for online verification platforms, guaranteeing safe and reliable code execution. Through comprehensive experiments, we demonstrate the effectiveness of our curated dataset, showing significant improvements in model performance and training stability. The source codes, curated dataset and sandbox system are available at: <a target="_blank" rel="noopener" href="https://github.com/Kwai-Klear/CodeTest">https://github.com/Kwai-Klear/CodeTest</a>. </p>
<blockquote>
<p>ç²¾ç¡®ã€æ­£ç¡®çš„åé¦ˆå¯¹äºåœ¨ä»£ç å¼ºåŒ–å­¦ä¹ ä¸­æœ‰æ•ˆåœ°è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œåˆæˆé«˜è´¨é‡æµ‹è¯•ç”¨ä¾‹ä»ç„¶æ˜¯ä¸€ä¸ªæå…·æŒ‘æˆ˜ä¸”å°šæœªè§£å†³çš„é—®é¢˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Klear-CodeTestï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„æµ‹è¯•ç”¨ä¾‹åˆæˆæ¡†æ¶ï¼Œå…·æœ‰ä¸¥æ ¼éªŒè¯çš„åŠŸèƒ½ï¼Œä»¥ç¡®ä¿æµ‹è¯•ç”¨ä¾‹çš„è´¨é‡å’Œå¯é æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡æ–°é¢–çš„Generator-Validationï¼ˆG-Vï¼‰æ¡†æ¶å®ç°äº†å¯¹ç¼–ç¨‹é—®é¢˜çš„å¹¿æ³›è¦†ç›–ï¼Œè¯¥æ¡†æ¶é€šè¿‡ä¸€è‡´æ€§éªŒè¯æœºåˆ¶ç¡®ä¿æ­£ç¡®æ€§ï¼Œé€šè¿‡å¯¹ç…§é»„é‡‘è§£å†³æ–¹æ¡ˆéªŒè¯è¾“å‡ºã€‚æå‡ºçš„G-Væ¡†æ¶ç”Ÿæˆå…¨é¢çš„æµ‹è¯•ç”¨ä¾‹ï¼ŒåŒ…æ‹¬å¸¸è§„å’Œè§’è½æ¡ˆä¾‹ï¼Œæé«˜æµ‹è¯•è¦†ç›–ç‡å’Œå¯¹ä»£ç å¼ºåŒ–å­¦ä¹ è§£å†³æ–¹æ¡ˆæ­£ç¡®æ€§çš„è¾¨åˆ«åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä¸ºåœ¨çº¿éªŒè¯å¹³å°è®¾è®¡äº†ä¸€ä¸ªä¼˜åŒ–çš„å¤šå±‚å®‰å…¨æ²™ç®±ç³»ç»Ÿï¼Œä¿è¯ä»£ç çš„å®‰å…¨å¯é æ‰§è¡Œã€‚é€šè¿‡å…¨é¢çš„å®éªŒï¼Œæˆ‘ä»¬éªŒè¯äº†è‡ªå®šä¹‰æ•°æ®é›†çš„æœ‰æ•ˆæ€§ï¼Œæ˜¾ç¤ºå‡ºæ¨¡å‹æ€§èƒ½å’Œè®­ç»ƒç¨³å®šæ€§çš„æ˜¾è‘—æé«˜ã€‚æºä»£ç ã€ç²¾é€‰æ•°æ®é›†å’Œæ²™ç®±ç³»ç»Ÿå¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/Kwai-Klear/CodeTest%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/Kwai-Klear/CodeTestè®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05710v2">PDF</a> 21 pages, 11 figures</p>
<p><strong>Summary</strong></p>
<p>æ–‡ç« ä»‹ç»äº†ä¸€ç§ç”¨äºä»£ç å¼ºåŒ–å­¦ä¹ ä¸­çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æµ‹è¯•æ¡ˆä¾‹ç»¼åˆæ¡†æ¶Klear-CodeTestã€‚è¯¥æ¡†æ¶å…·æœ‰ä¸¥æ ¼çš„éªŒè¯æœºåˆ¶ï¼Œç¡®ä¿æµ‹è¯•æ¡ˆä¾‹çš„è´¨é‡å’Œå¯é æ€§ã€‚å®ƒé‡‡ç”¨æ–°é¢–çš„Generator-Validationï¼ˆG-Vï¼‰æ¡†æ¶ï¼Œé€šè¿‡ä¸€è‡´æ€§éªŒè¯æœºåˆ¶ç¡®ä¿æ­£ç¡®æ€§ï¼Œå¹¶å¯¹æ¯”é»„é‡‘è§£å†³æ–¹æ¡ˆè¿›è¡ŒéªŒè¯ã€‚G-Væ¡†æ¶èƒ½å¤Ÿç”Ÿæˆå…¨é¢çš„æµ‹è¯•æ¡ˆä¾‹ï¼ŒåŒ…æ‹¬å¸¸è§„å’Œæç«¯æƒ…å†µï¼Œæé«˜ä»£ç å¼ºåŒ–å­¦ä¹ ä¸­è§£å†³æ–¹æ¡ˆæ­£ç¡®æ€§çš„è¯„ä¼°çš„è¦†ç›–é¢å’Œé‰´åˆ«åŠ›ã€‚æ­¤å¤–ï¼Œè®¾è®¡äº†ä¸€ä¸ªé’ˆå¯¹åœ¨çº¿éªŒè¯å¹³å°çš„ä¼˜åŒ–å¤šå±‚å®‰å…¨æ²™ç®±ç³»ç»Ÿï¼Œä¿è¯ä»£ç æ‰§è¡Œçš„å®‰å…¨æ€§å’Œå¯é æ€§ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶èƒ½æœ‰æ•ˆæé«˜æ¨¡å‹æ€§èƒ½å’Œè®­ç»ƒç¨³å®šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Klear-CodeTestæ˜¯ä¸€ä¸ªç”¨äºä»£ç å¼ºåŒ–å­¦ä¹ çš„æµ‹è¯•æ¡ˆä¾‹ç»¼åˆæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³é«˜è´¨é‡æµ‹è¯•ç”¨ä¾‹çš„åˆæˆæŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥æ–°é¢–çš„Generator-Validationï¼ˆG-Vï¼‰æ¡†æ¶ï¼Œé€šè¿‡ä¸€è‡´æ€§éªŒè¯ç¡®ä¿æ­£ç¡®æ€§ã€‚</li>
<li>G-Væ¡†æ¶ç”Ÿæˆå…¨é¢çš„æµ‹è¯•æ¡ˆä¾‹ï¼ŒåŒ…æ‹¬å¸¸è§„å’Œæç«¯æƒ…å†µï¼Œæé«˜è§£å†³æ–¹æ¡ˆæ­£ç¡®æ€§çš„è¯„ä¼°èƒ½åŠ›ã€‚</li>
<li>è®¾è®¡äº†é’ˆå¯¹åœ¨çº¿éªŒè¯å¹³å°çš„ä¼˜åŒ–å¤šå±‚å®‰å…¨æ²™ç®±ç³»ç»Ÿï¼Œç¡®ä¿ä»£ç æ‰§è¡Œçš„å®‰å…¨æ€§å’Œå¯é æ€§ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡ä¸¥æ ¼çš„éªŒè¯æœºåˆ¶ç¡®ä¿æµ‹è¯•æ¡ˆä¾‹çš„è´¨é‡å’Œå¯é æ€§ã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œä½¿ç”¨Klear-CodeTestæ¡†æ¶èƒ½æ˜¾è‘—æé«˜æ¨¡å‹æ€§èƒ½å’Œè®­ç»ƒç¨³å®šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05710">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2508.05710v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2508.05710v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2508.05710v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2508.05710v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Thinking-With-Videos-Multimodal-Tool-Augmented-Reinforcement-Learning-for-Long-Video-Reasoning"><a href="#Thinking-With-Videos-Multimodal-Tool-Augmented-Reinforcement-Learning-for-Long-Video-Reasoning" class="headerlink" title="Thinking With Videos: Multimodal Tool-Augmented Reinforcement Learning   for Long Video Reasoning"></a>Thinking With Videos: Multimodal Tool-Augmented Reinforcement Learning   for Long Video Reasoning</h2><p><strong>Authors:Haoji Zhang, Xin Gu, Jiawen Li, Chixiang Ma, Sule Bai, Chubin Zhang, Bowen Zhang, Zhichao Zhou, Dongliang He, Yansong Tang</strong></p>
<p>The video reasoning ability of multimodal large language models (MLLMs) is crucial for downstream tasks like video question answering and temporal grounding. While recent approaches have explored text-based chain-of-thought (CoT) reasoning for MLLMs, these methods often suffer from limited cross-modal interaction and increased hallucination, especially with longer videos or reasoning chains. To address these challenges, we propose Video Intelligence via Tool-Augmented Learning (VITAL), a novel end-to-end agentic video reasoning framework. With a visual toolbox, the model can densely sample new video frames on demand and generate multimodal CoT for precise long video reasoning. We observe that temporal grounding and question answering are mutually beneficial for video understanding tasks. Therefore, we construct two high-quality multi-task video reasoning datasets MTVR-CoT-72k for supervised fine-tuning and MTVR-RL-110k for reinforcement learning. Moreover, we propose a Difficulty-aware Group Relative Policy Optimization algorithm (DGRPO) to mitigate difficulty imbalance in multi-task reinforcement learning. Extensive experiments on 11 challenging video understanding benchmarks demonstrate the advanced reasoning ability of VITAL, outperforming existing methods in video question answering and temporal grounding tasks, especially in long video scenarios. Code is available at <a target="_blank" rel="noopener" href="https://zhang9302002.github.io/thinkingwithvideos-page/">https://zhang9302002.github.io/thinkingwithvideos-page/</a>. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è§†é¢‘æ¨ç†èƒ½åŠ›å¯¹äºè§†é¢‘é—®ç­”å’Œæ—¶é—´å®šä½ç­‰ä¸‹æ¸¸ä»»åŠ¡è‡³å…³é‡è¦ã€‚è™½ç„¶æœ€è¿‘çš„æ–¹æ³•å·²ç»æ¢ç´¢äº†åŸºäºæ–‡æœ¬çš„æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†ç”¨äºMLLMsï¼Œä½†è¿™äº›æ–¹æ³•é€šå¸¸å—åˆ°æœ‰é™çš„è·¨æ¨¡æ€äº¤äº’å’Œå¢åŠ çš„å¹»è§‰å½±å“ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†é•¿è§†é¢‘æˆ–å¤æ‚æ¨ç†é“¾æ—¶ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†é€šè¿‡å·¥å…·å¢å¼ºå­¦ä¹ å®ç°è§†é¢‘æ™ºèƒ½ï¼ˆVITALï¼‰è¿™ä¸€æ–°å‹ç«¯åˆ°ç«¯æ™ºèƒ½è§†é¢‘æ¨ç†æ¡†æ¶ã€‚å€ŸåŠ©è§†è§‰å·¥å…·ç®±ï¼Œè¯¥æ¨¡å‹å¯ä»¥æŒ‰éœ€æ±‚å¯†é›†é‡‡æ ·æ–°çš„è§†é¢‘å¸§ï¼Œå¹¶ç”Ÿæˆå¤šæ¨¡æ€æ€ç»´é“¾ï¼Œä»¥å®ç°ç²¾ç¡®çš„é•¿è§†é¢‘æ¨ç†ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°æ—¶é—´å®šä½å’Œé—®é¢˜å›ç­”å¯¹è§†é¢‘ç†è§£ä»»åŠ¡æ˜¯äº’åˆ©çš„ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸¤ä¸ªé«˜è´¨é‡çš„å¤šä»»åŠ¡è§†é¢‘æ¨ç†æ•°æ®é›†MTVR-CoT-72kï¼Œç”¨äºç›‘ç£å¾®è°ƒï¼Œä»¥åŠMTVR-RL-110kï¼Œç”¨äºå¼ºåŒ–å­¦ä¹ ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§éš¾åº¦æ„ŸçŸ¥ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ç®—æ³•ï¼ˆDGRPOï¼‰ï¼Œä»¥ç¼“è§£å¤šä»»åŠ¡å¼ºåŒ–å­¦ä¹ ä¸­çš„éš¾åº¦ä¸å¹³è¡¡é—®é¢˜ã€‚åœ¨11ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒVITALå…·æœ‰å…ˆè¿›çš„æ¨ç†èƒ½åŠ›ï¼Œåœ¨è§†é¢‘é—®ç­”å’Œæ—¶é—´å®šä½ä»»åŠ¡ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨é•¿è§†é¢‘åœºæ™¯ä¸­ã€‚ç›¸å…³ä»£ç å¯è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://zhang9302002.github.io/thinkingwithvideos-page/%E3%80%82">https://zhang9302002.github.io/thinkingwithvideos-page/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04416v2">PDF</a> </p>
<p><strong>Summary</strong><br>å¤šåª’ä½“å¤§è¯­è¨€æ¨¡å‹çš„è§†é¢‘æ¨ç†èƒ½åŠ›å¯¹äºè§†é¢‘é—®ç­”å’Œæ—¶é—´å®šä½ç­‰ä¸‹æ¸¸ä»»åŠ¡è‡³å…³é‡è¦ã€‚ä¸ºåº”å¯¹ç°æœ‰æ–¹æ³•è·¨æ¨¡æ€äº¤äº’æœ‰é™ã€æ¨ç†é“¾æ¡å¢é•¿æ—¶å‡ºç°å¹»æƒ³çš„æŒ‘æˆ˜ï¼Œæå‡ºä¸€ç§æ–°å‹ç«¯åˆ°ç«¯æ™ºèƒ½è§†é¢‘æ¨ç†æ¡†æ¶â€”â€”å·¥å…·å¢å¼ºå­¦ä¹ è§†é¢‘æ™ºèƒ½ï¼ˆVITALï¼‰ã€‚å€ŸåŠ©è§†è§‰å·¥å…·ç®±ï¼Œæ¨¡å‹å¯æŒ‰éœ€å¯†é›†é‡‡æ ·æ–°è§†é¢‘å¸§ï¼Œç”Ÿæˆå¤šæ¨¡æ€æ€ç»´é“¾ï¼Œå®ç°ç²¾ç¡®é•¿è§†é¢‘æ¨ç†ã€‚æ„å»ºä¸¤ä¸ªé«˜è´¨é‡å¤šä»»åŠ¡è§†é¢‘æ¨ç†æ•°æ®é›†ï¼Œç”¨äºç›‘ç£å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ ã€‚æ­¤å¤–ï¼Œæå‡ºéš¾åº¦æ„ŸçŸ¥åˆ†ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ç®—æ³•ï¼ˆDGRPOï¼‰ï¼Œç¼“è§£å¤šä»»åŠ¡å¼ºåŒ–å­¦ä¹ ä¸­éš¾åº¦ä¸å‡è¡¡é—®é¢˜ã€‚åœ¨å¤šä¸ªè§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨é•¿è§†é¢‘åœºæ™¯ä¸­ï¼ŒVITALåœ¨è§†é¢‘é—®ç­”å’Œæ—¶é—´å®šä½ä»»åŠ¡ä¸Šè¶…è¶Šç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è§†é¢‘æ¨ç†èƒ½åŠ›å¯¹äºå®Œæˆä¸‹æ¸¸ä»»åŠ¡å¦‚è§†é¢‘é—®ç­”å’Œæ—¶é—´å®šä½éå¸¸é‡è¦ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨å¤„ç†é•¿è§†é¢‘æˆ–å¤æ‚æ¨ç†é“¾æ—¶å­˜åœ¨è·¨æ¨¡æ€äº¤äº’ä¸è¶³å’Œå¹»æƒ³é—®é¢˜ã€‚</li>
<li>æå‡ºçš„VITALæ¡†æ¶é€šè¿‡ä½¿ç”¨è§†è§‰å·¥å…·ç®±å¯†é›†é‡‡æ ·è§†é¢‘å¸§ï¼Œç”Ÿæˆå¤šæ¨¡æ€æ€ç»´é“¾ï¼Œä»¥å¤„ç†é•¿è§†é¢‘æ¨ç†ã€‚</li>
<li>ä¸ºé€‚åº”ç›‘ç£å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ ï¼Œæ„å»ºä¸¤ä¸ªå¤šä»»åŠ¡è§†é¢‘æ¨ç†æ•°æ®é›†MTVR-CoT-72kå’ŒMTVR-RL-110kã€‚</li>
<li>æå‡ºDGRPOç®—æ³•ä»¥å¤„ç†å¤šä»»åŠ¡å¼ºåŒ–å­¦ä¹ ä¸­çš„éš¾åº¦ä¸å‡è¡¡é—®é¢˜ã€‚</li>
<li>VITALåœ¨å¤šä¸ªåŸºå‡†è§†é¢‘ç†è§£æµ‹è¯•ä¸­è¡¨ç°å“è¶Šï¼Œç‰¹åˆ«æ˜¯åœ¨é•¿è§†é¢‘åœºæ™¯çš„è§†é¢‘é—®ç­”å’Œæ—¶é—´å®šä½ä»»åŠ¡ä¸Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04416">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2508.04416v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2508.04416v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2508.04416v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2508.04416v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2508.04416v2/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2508.04416v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2508.04416v2/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2508.04416v2/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2508.04416v2/page_5_1.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="LoRA-PAR-A-Flexible-Dual-System-LoRA-Partitioning-Approach-to-Efficient-LLM-Fine-Tuning"><a href="#LoRA-PAR-A-Flexible-Dual-System-LoRA-Partitioning-Approach-to-Efficient-LLM-Fine-Tuning" class="headerlink" title="LoRA-PAR: A Flexible Dual-System LoRA Partitioning Approach to Efficient   LLM Fine-Tuning"></a>LoRA-PAR: A Flexible Dual-System LoRA Partitioning Approach to Efficient   LLM Fine-Tuning</h2><p><strong>Authors:Yining Huang, Bin Li, Keke Tang, Meilian Chen</strong></p>
<p>Large-scale generative models like DeepSeek-R1 and OpenAI-O1 benefit substantially from chain-of-thought (CoT) reasoning, yet pushing their performance typically requires vast data, large model sizes, and full-parameter fine-tuning. While parameter-efficient fine-tuning (PEFT) helps reduce cost, most existing approaches primarily address domain adaptation or layer-wise allocation rather than explicitly tailoring data and parameters to different response demands. Inspired by â€œThinking, Fast and Slow,â€ which characterizes two distinct modes of thought-System 1 (fast, intuitive, often automatic) and System 2 (slower, more deliberative and analytic)-we draw an analogy that different â€œsubregionsâ€ of an LLMâ€™s parameters might similarly specialize for tasks that demand quick, intuitive responses versus those requiring multi-step logical reasoning. Therefore, we propose LoRA-PAR, a dual-system LoRA framework that partitions both data and parameters by System 1 or System 2 demands, using fewer yet more focused parameters for each task. Specifically, we classify task data via multi-model role-playing and voting, and partition parameters based on importance scoring, then adopt a two-stage fine-tuning strategy of training System 1 tasks with supervised fine-tuning (SFT) to enhance knowledge and intuition and refine System 2 tasks with reinforcement learning (RL) to reinforce deeper logical deliberation next. Extensive experiments show that the two-stage fine-tuning strategy, SFT and RL, lowers active parameter usage while matching or surpassing SOTA PEFT baselines. </p>
<blockquote>
<p>å¤§è§„æ¨¡ç”Ÿæˆæ¨¡å‹å¦‚DeepSeek-R1å’ŒOpenAI-O1åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå—ç›Šäºæ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†ï¼Œç„¶è€Œï¼Œè¦æå‡å®ƒä»¬çš„æ€§èƒ½é€šå¸¸éœ€è¦å¤§é‡çš„æ•°æ®ã€åºå¤§çš„æ¨¡å‹è§„æ¨¡å’Œå…¨é¢çš„å‚æ•°å¾®è°ƒã€‚è™½ç„¶å‚æ•°æ•ˆç‡å¾®è°ƒï¼ˆPEFTï¼‰æœ‰åŠ©äºé™ä½æˆæœ¬ï¼Œä½†å¤§å¤šæ•°ç°æœ‰æ–¹æ³•ä¸»è¦è§£å†³é¢†åŸŸé€‚åº”æˆ–åˆ†å±‚åˆ†é…é—®é¢˜ï¼Œè€Œä¸æ˜¯æ˜ç¡®åœ°é’ˆå¯¹æ•°æ®å’Œå‚æ•°è¿›è¡Œä¸åŒçš„å“åº”éœ€æ±‚è°ƒæ•´ã€‚å—ã€Šæ€è€ƒï¼Œå¿«ä¸æ…¢ã€‹çš„å¯å‘ï¼Œè¯¥ä¹¦æè¿°äº†ä¸¤ç§æˆªç„¶ä¸åŒçš„æ€ç»´æ¨¡å¼â€”â€”ç³»ç»Ÿ1ï¼ˆå¿«é€Ÿã€ç›´è§‰æ€§ã€é€šå¸¸è‡ªåŠ¨ï¼‰å’Œç³»ç»Ÿ2ï¼ˆè¾ƒæ…¢ã€æ›´å®¡æ…å’Œåˆ†ææ€§ï¼‰â€”â€”æˆ‘ä»¬ç±»æ¯”å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‚æ•°çš„ä¸åŒâ€œå­åŒºåŸŸâ€å¯èƒ½åŒæ ·ä¸“é•¿äºå¿«é€Ÿç›´è§‰ååº”çš„ä»»åŠ¡ä¸éœ€è¦å¤šæ­¥éª¤é€»è¾‘æ¨ç†çš„ä»»åŠ¡ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†LoRA-PARåŒç³»ç»ŸLoRAæ¡†æ¶ï¼Œå®ƒæ ¹æ®ç³»ç»Ÿ1æˆ–ç³»ç»Ÿ2çš„éœ€æ±‚å¯¹æ•°æ®å’Œå‚æ•°è¿›è¡Œåˆ†åŒºï¼Œä¸ºæ¯ä¸ªä»»åŠ¡ä½¿ç”¨æ›´å°‘ä½†æ›´é›†ä¸­çš„å‚æ•°ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é€šè¿‡å¤šæ¨¡å‹è§’è‰²æ‰®æ¼”å’ŒæŠ•ç¥¨å¯¹ä»»åŠ¡æ•°æ®è¿›è¡Œåˆ†ç±»ï¼Œæ ¹æ®é‡è¦æ€§è¯„åˆ†å¯¹å‚æ•°è¿›è¡Œåˆ†åŒºï¼Œç„¶åé‡‡ç”¨ä¸¤é˜¶æ®µå¾®è°ƒç­–ç•¥ï¼Œç”¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è®­ç»ƒç³»ç»Ÿ1ä»»åŠ¡ä»¥å¢å¼ºçŸ¥è¯†å’Œç›´è§‰ï¼Œç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç»†åŒ–ç³»ç»Ÿ2ä»»åŠ¡ä»¥åŠ å¼ºæ›´æ·±å±‚æ¬¡çš„é€»è¾‘æ€è€ƒã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸¤é˜¶æ®µå¾®è°ƒç­–ç•¥â€”â€”SFTå’ŒRLï¼Œé™ä½äº†æ´»åŠ¨å‚æ•°çš„ä½¿ç”¨é‡ï¼ŒåŒæ—¶è¾¾åˆ°æˆ–è¶…è¶Šäº†æœ€æ–°PEFTåŸºå‡†æµ‹è¯•æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.20999v2">PDF</a> 12 pages</p>
<p><strong>Summary</strong><br>    å¤§å‹ç”Ÿæˆæ¨¡å‹å¦‚DeepSeek-R1å’ŒOpenAI-O1å¯é€šè¿‡å¼•å…¥é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†å¤§å¹…æå‡æ€§èƒ½ï¼Œä½†æå‡æ€§èƒ½é€šå¸¸éœ€è¦å¤§é‡æ•°æ®ã€å¤§å‹æ¨¡å‹åŠå…¨å‚æ•°å¾®è°ƒã€‚å‚æ•°æ•ˆç‡å¾®è°ƒï¼ˆPEFTï¼‰æœ‰åŠ©äºé™ä½æˆæœ¬ï¼Œä½†ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨é¢†åŸŸé€‚åº”æˆ–é€å±‚åˆ†é…ï¼Œå¹¶æœªæ˜ç¡®é’ˆå¯¹æ•°æ®å’Œå‚æ•°çš„è°ƒæ•´æ¥é€‚åº”ä¸åŒçš„å“åº”éœ€æ±‚ã€‚å—â€œå¿«é€Ÿä¸æ…¢é€Ÿæ€è€ƒâ€å¯å‘ï¼Œæ–‡ä¸­æå‡ºä¸€ç§åŒç³»ç»ŸLoRAæ¡†æ¶LoRA-PARï¼Œè¯¥æ¡†æ¶æ ¹æ®ç³»ç»Ÿ1ï¼ˆå¿«é€Ÿã€ç›´è§‰æ€§ï¼‰å’Œç³»ç»Ÿ2ï¼ˆè¾ƒæ…¢ã€åˆ†ææ€§ï¼‰çš„éœ€æ±‚åˆ’åˆ†æ•°æ®å’Œå‚æ•°ã€‚é€šè¿‡å¤šæ¨¡å‹è§’è‰²æ‰®æ¼”å’ŒæŠ•ç¥¨å¯¹ä»»åŠ¡æ•°æ®è¿›è¡Œåˆ†ç±»ï¼ŒåŸºäºé‡è¦æ€§è¯„åˆ†å¯¹å‚æ•°è¿›è¡Œåˆ†åŒºï¼Œå¹¶é‡‡ç”¨ä¸¤é˜¶æ®µå¾®è°ƒç­–ç•¥ï¼šå…ˆç”¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è®­ç»ƒç³»ç»Ÿ1ä»»åŠ¡ä»¥å¢å¼ºçŸ¥è¯†å’Œç›´è§‰ï¼Œå†ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç»†åŒ–ç³»ç»Ÿ2ä»»åŠ¡ä»¥åŠ å¼ºæ·±åº¦é€»è¾‘æ€è€ƒã€‚å®éªŒè¡¨æ˜ï¼Œè¿™ç§ä¸¤é˜¶æ®µå¾®è°ƒç­–ç•¥åœ¨é™ä½æ´»åŠ¨å‚æ•°ä½¿ç”¨çš„åŒæ—¶ï¼Œè¾¾åˆ°æˆ–è¶…è¶Šäº†æœ€ä½³PEFTåŸºå‡†çº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹ç”Ÿæˆæ¨¡å‹é€šè¿‡å¼•å…¥é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†æé«˜æ€§èƒ½ã€‚</li>
<li>ç°æœ‰å‚æ•°æ•ˆç‡å¾®è°ƒæ–¹æ³•ä¸»è¦å…³æ³¨é¢†åŸŸé€‚åº”å’Œé€å±‚åˆ†é…ã€‚</li>
<li>æ–‡ä¸­å€Ÿé‰´â€œå¿«é€Ÿä¸æ…¢é€Ÿæ€è€ƒâ€ç†è®ºï¼Œæå‡ºåŒç³»ç»ŸLoRAæ¡†æ¶LoRA-PARã€‚</li>
<li>LoRA-PARæ¡†æ¶æ ¹æ®ç³»ç»Ÿ1å’Œç³»ç»Ÿ2çš„éœ€æ±‚è¿›è¡Œæ•°æ®å’Œå‚æ•°çš„åˆ†åŒºã€‚</li>
<li>é‡‡ç”¨å¤šæ¨¡å‹è§’è‰²æ‰®æ¼”å’ŒæŠ•ç¥¨å¯¹ä»»åŠ¡æ•°æ®è¿›è¡Œåˆ†ç±»ã€‚</li>
<li>é‡‡ç”¨åŸºäºé‡è¦æ€§è¯„åˆ†çš„å‚æ•°åˆ†åŒºæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.20999">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2507.20999v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2507.20999v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2507.20999v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2507.20999v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2507.20999v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="DistFlow-A-Fully-Distributed-RL-Framework-for-Scalable-and-Efficient-LLM-Post-Training"><a href="#DistFlow-A-Fully-Distributed-RL-Framework-for-Scalable-and-Efficient-LLM-Post-Training" class="headerlink" title="DistFlow: A Fully Distributed RL Framework for Scalable and Efficient   LLM Post-Training"></a>DistFlow: A Fully Distributed RL Framework for Scalable and Efficient   LLM Post-Training</h2><p><strong>Authors:Zhixin Wang, Tianyi Zhou, Liming Liu, Ao Li, Jiarui Hu, Dian Yang, Yinhui Lu, Jinlong Hou, Siyuan Feng, Yuan Cheng, Yuan Qi</strong></p>
<p>Reinforcement learning (RL) has become the pivotal post-training technique for large language model (LLM). Effectively scaling reinforcement learning is now the key to unlocking advanced reasoning capabilities and ensuring safe, goal-aligned behavior in the most powerful LLMs. Mainstream frameworks usually employ a hybrid-controller architecture where a single-controller dispatches the overall execution logic and manages overall data transfer and the multi-controller executes distributed computation. For large-scale reinforcement learning, minor load imbalances can introduce significant bottlenecks, ultimately constraining the scalability of the system. To address this limitation, we introduce DistFlow, a novel, fully distributed RL framework designed to break scaling barrier. We adopt a multi-controller paradigm that dispatches data transfer and execution tasks to all workers, which eliminates the centralized node. This allows each worker to operate independently, leading to near-linear scalability up to 1024 GPUs and dramatic efficiency gains. Furthermore, our architecture decouples resource configuration from execution logic, allowing each worker to have a unique execution flow, offering significant flexibility for rapid and cost-effective algorithmic experimentation. Extensive experiments show that DistFlow achieves excellent linear scalability and up to a 7x end-to-end throughput improvement in specific scenarios over state-of-the-art (SOTA) frameworks. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·²æˆä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å…³é”®åè®­ç»ƒæŠ€æœ¯ã€‚æœ‰æ•ˆåœ°æ‰©å±•å¼ºåŒ–å­¦ä¹ ç°åœ¨æ˜¯è§£é”å…ˆè¿›æ¨ç†èƒ½åŠ›å¹¶ç¡®ä¿æœ€å¼ºå¤§LLMå®‰å…¨ã€ç›®æ ‡å¯¹é½è¡Œä¸ºçš„å…³é”®ã€‚ä¸»æµæ¡†æ¶é€šå¸¸é‡‡ç”¨æ··åˆæ§åˆ¶å™¨æ¶æ„ï¼Œå…¶ä¸­å•ä¸ªæ§åˆ¶å™¨è°ƒåº¦æ•´ä½“æ‰§è¡Œé€»è¾‘å¹¶ç®¡ç†æ•´ä½“æ•°æ®ä¼ è¾“ï¼Œè€Œå¤šä¸ªæ§åˆ¶å™¨æ‰§è¡Œåˆ†å¸ƒå¼è®¡ç®—ã€‚å¯¹äºå¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ ï¼Œè½»å¾®çš„è´Ÿè½½ä¸å¹³è¡¡å¯èƒ½ä¼šå¼•å…¥é‡å¤§ç“¶é¢ˆï¼Œæœ€ç»ˆé™åˆ¶ç³»ç»Ÿçš„å¯æ‰©å±•æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†DistFlowï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹çš„å…¨åˆ†å¸ƒå¼RLæ¡†æ¶ï¼Œæ—¨åœ¨æ‰“ç ´æ‰©å±•éšœç¢ã€‚æˆ‘ä»¬é‡‡ç”¨å¤šæ§åˆ¶å™¨èŒƒå¼ï¼Œå°†æ•°æ®ä¼ è¾“å’Œæ‰§è¡Œä»»åŠ¡è°ƒåº¦åˆ°æ‰€æœ‰å·¥ä½œè€…ï¼Œæ¶ˆé™¤äº†ä¸­å¿ƒèŠ‚ç‚¹ã€‚è¿™å…è®¸æ¯ä¸ªå·¥ä½œè€…ç‹¬ç«‹æ“ä½œï¼Œä»è€Œå®ç°æ¥è¿‘çº¿æ€§çš„å¯æ‰©å±•æ€§ï¼Œæœ€é«˜å¯è¾¾1024ä¸ªGPUï¼Œå¹¶æ˜¾è‘—æé«˜æ•ˆç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ¶æ„å°†èµ„æºé…ç½®ä¸æ‰§è¡Œé€»è¾‘è§£è€¦ï¼Œä½¿æ¯ä¸ªå·¥ä½œè€…éƒ½èƒ½æ‹¥æœ‰ç‹¬ç‰¹çš„æ‰§è¡Œæµç¨‹ï¼Œä¸ºå¿«é€Ÿå’Œç»æµçš„ç®—æ³•å®éªŒæä¾›äº†é‡å¤§çµæ´»æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDistFlowå®ç°äº†å‡ºè‰²çš„çº¿æ€§å¯æ‰©å±•æ€§ï¼Œå¹¶åœ¨ç‰¹å®šåœºæ™¯ä¸‹å®ç°äº†ç›¸è¾ƒäºæœ€æ–°æ¡†æ¶é«˜è¾¾7å€ç«¯åˆ°ç«¯ååé‡æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.13833v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·²æˆä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å…³é”®åè®­ç»ƒæŠ€æœ¯ã€‚ä¸»æµæ¡†æ¶é€šå¸¸é‡‡ç”¨æ··åˆæ§åˆ¶å™¨æ¶æ„ï¼Œé€šè¿‡å•ä¸€æ§åˆ¶å™¨è°ƒåº¦æ•´ä½“æ‰§è¡Œé€»è¾‘å¹¶ç®¡ç†æ•°æ®è½¬ç§»ï¼Œå¤šæ§åˆ¶å™¨æ‰§è¡Œåˆ†å¸ƒå¼è®¡ç®—ã€‚ä¸ºè§£å†³å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ ä¸­çš„è´Ÿè½½ä¸å‡è¡¡é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†DistFlowï¼Œä¸€ç§å…¨æ–°çš„å…¨åˆ†å¸ƒå¼RLæ¡†æ¶ï¼Œæ—¨åœ¨æ‰“ç ´è§„æ¨¡æ‰©å±•çš„éšœç¢ã€‚é€šè¿‡é‡‡ç”¨å¤šæ§åˆ¶å™¨èŒƒå¼ï¼Œæˆ‘ä»¬æ¶ˆé™¤äº†é›†ä¸­èŠ‚ç‚¹ï¼Œå…è®¸æ¯ä¸ªå·¥ä½œè€…ç‹¬ç«‹æ“ä½œï¼Œä»è€Œå®ç°è¿‘çº¿æ€§æ‰©å±•åˆ°1024ä¸ªGPUï¼Œå¹¶æ˜¾è‘—æé«˜æ•ˆç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ¶æ„å°†èµ„æºé…ç½®ä¸æ‰§è¡Œé€»è¾‘è§£è€¦ï¼Œä½¿æ¯ä¸ªå·¥ä½œè€…æ‹¥æœ‰ç‹¬ç‰¹çš„æ‰§è¡Œæµç¨‹ï¼Œä¸ºå¿«é€Ÿç»æµçš„ç®—æ³•å®éªŒæä¾›äº†é‡å¤§çµæ´»æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒDistFlowå®ç°äº†å‡ºè‰²çš„çº¿æ€§å¯æ‰©å±•æ€§ï¼Œå¹¶åœ¨ç‰¹å®šåœºæ™¯ä¸‹å®ç°äº†ç›¸å¯¹äºæœ€æ–°æŠ€æœ¯é«˜è¾¾7å€ç«¯åˆ°ç«¯ååé‡æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹çš„å…³é”®åè®­ç»ƒæŠ€æœ¯ï¼Œå¯¹äºè§£é”é«˜çº§æ¨ç†èƒ½åŠ›å’Œç¡®ä¿å®‰å…¨ã€ç›®æ ‡å¯¹é½çš„è¡Œä¸ºè‡³å…³é‡è¦ã€‚</li>
<li>ä¸»æµæ¡†æ¶åœ¨å¼ºåŒ–å­¦ä¹ æ–¹é¢å­˜åœ¨è´Ÿè½½ä¸å‡è¡¡é—®é¢˜ï¼Œé™åˆ¶äº†ç³»ç»Ÿçš„å¯æ‰©å±•æ€§ã€‚</li>
<li>DistFlowæ˜¯ä¸€ä¸ªå…¨æ–°çš„å…¨åˆ†å¸ƒå¼RLæ¡†æ¶ï¼Œæ—¨åœ¨æ¶ˆé™¤é›†ä¸­èŠ‚ç‚¹ï¼Œå…è®¸æ¯ä¸ªå·¥ä½œè€…ç‹¬ç«‹æ“ä½œï¼Œå®ç°è¿‘çº¿æ€§æ‰©å±•åˆ°1024ä¸ªGPUã€‚</li>
<li>DistFlowé‡‡ç”¨å¤šæ§åˆ¶å™¨èŒƒå¼ï¼Œæ¶ˆé™¤é›†ä¸­ç“¶é¢ˆï¼Œæ˜¾è‘—æé«˜æ•ˆç‡ã€‚</li>
<li>DistFlowæ¶æ„å°†èµ„æºé…ç½®ä¸æ‰§è¡Œé€»è¾‘è§£è€¦ï¼Œä¸ºå¿«é€Ÿç»æµçš„ç®—æ³•å®éªŒæä¾›äº†é‡å¤§çµæ´»æ€§ã€‚</li>
<li>å®éªŒè¡¨æ˜DistFlowå®ç°äº†å‡ºè‰²çš„çº¿æ€§å¯æ‰©å±•æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.13833">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2507.13833v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2507.13833v3/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2507.13833v3/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2507.13833v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2507.13833v3/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2507.13833v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Scaling-LLM-Planning-NL2FLOW-for-Parametric-Problem-Generation-and-Rigorous-Evaluation"><a href="#Scaling-LLM-Planning-NL2FLOW-for-Parametric-Problem-Generation-and-Rigorous-Evaluation" class="headerlink" title="Scaling LLM Planning: NL2FLOW for Parametric Problem Generation and   Rigorous Evaluation"></a>Scaling LLM Planning: NL2FLOW for Parametric Problem Generation and   Rigorous Evaluation</h2><p><strong>Authors:Jungkoo Kang</strong></p>
<p>Robust workflow composition is critical for effective agent performance, yet progress in Large Language Model (LLM) planning and reasoning is hindered by a scarcity of scalable evaluation data. This work introduces NL2Flow, a fully automated pipeline for generating and evaluating workflow planning problems. NL2Flow generates problems parametrically in a structured intermediate representation, translating them into both natural language and formal PDDL. I evaluate several open-source, instruct-tuned LLMs on a dataset of 2296 low-difficulty problems generated by NL2Flow. Results demonstrate that the best-performing model achieved 86% success in generating valid plans and 69% in generating optimal plans (for solvable problems). Regression analysis shows that the influence of problem characteristics on plan generation is contingent on both model and prompt design. Importantly, translating natural language problems into a structured JSON representation prior to symbolic planning significantly improved success rates, suggesting a benefit from neuro-symbolic integration. These findings underscore the importance of understanding error sources within LLM reasoning as systems scale to more complex tasks. As LLM reasoning scales to increasingly complex problems, understanding the shifting bottlenecks and sources of error within these systems will be crucial. </p>
<blockquote>
<p>å¥å£®çš„å·¥ä½œæµç»„åˆå¯¹äºæœ‰æ•ˆçš„ä»£ç†æ€§èƒ½è‡³å…³é‡è¦ï¼Œç„¶è€Œï¼Œç”±äºç¼ºå°‘å¯æ‰©å±•çš„è¯„ä¼°æ•°æ®ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è§„åˆ’å’Œæ¨ç†æ–¹é¢çš„è¿›å±•å—åˆ°äº†é˜»ç¢ã€‚è¿™é¡¹å·¥ä½œæ¨å‡ºäº†NL2Flowï¼Œä¸€ä¸ªç”¨äºç”Ÿæˆå’Œè¯„ä¼°å·¥ä½œæµè§„åˆ’é—®é¢˜çš„å…¨è‡ªåŠ¨ç®¡é“ã€‚NL2Flowä»¥ç»“æ„åŒ–çš„ä¸­é—´è¡¨ç¤ºå½¢å¼å‚æ•°åŒ–ç”Ÿæˆé—®é¢˜ï¼Œå°†å®ƒä»¬ç¿»è¯‘æˆè‡ªç„¶è¯­è¨€ä»¥åŠæ­£å¼çš„PDDLã€‚æˆ‘åœ¨ç”±NL2Flowç”Ÿæˆçš„åŒ…å«2296ä¸ªä½éš¾åº¦é—®é¢˜çš„æ•°æ®é›†ä¸Šè¯„ä¼°äº†å¤šä¸ªå¼€æºçš„ã€ç»è¿‡æŒ‡ä»¤è°ƒæ•´çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚ç»“æœè¡¨æ˜ï¼Œè¡¨ç°æœ€å¥½çš„æ¨¡å‹åœ¨ç”Ÿæˆæœ‰æ•ˆè®¡åˆ’æ–¹é¢å–å¾—äº†86%çš„æˆåŠŸç‡ï¼Œåœ¨ç”Ÿæˆæœ€ä¼˜è®¡åˆ’æ–¹é¢å–å¾—äº†69%ï¼ˆé’ˆå¯¹å¯è§£å†³çš„é—®é¢˜ï¼‰ã€‚å›å½’åˆ†æè¡¨æ˜ï¼Œé—®é¢˜ç‰¹å¾å¯¹è®¡åˆ’ç”Ÿæˆçš„å½±å“å–å†³äºæ¨¡å‹å’Œæç¤ºè®¾è®¡ã€‚é‡è¦çš„æ˜¯ï¼Œåœ¨ç¬¦å·è§„åˆ’ä¹‹å‰å°†è‡ªç„¶è¯­è¨€é—®é¢˜ç¿»è¯‘æˆç»“æ„åŒ–çš„JSONè¡¨ç¤ºå½¢å¼æ˜¾è‘—æé«˜äº†æˆåŠŸç‡ï¼Œè¿™æ˜¾ç¤ºäº†ç¥ç»ç¬¦å·èåˆçš„ä¼˜åŠ¿ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†äº†è§£å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†ä¸­çš„é”™è¯¯æ¥æºéšç€ç³»ç»Ÿå¤„ç†æ›´å¤æ‚ä»»åŠ¡è€Œæ‰©å¤§çš„é‡è¦æ€§ã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†å¤„ç†è¶Šæ¥è¶Šå¤æ‚çš„é—®é¢˜ï¼Œäº†è§£è¿™äº›ç³»ç»Ÿä¸­ç“¶é¢ˆå’Œé”™è¯¯æ¥æºçš„å˜åŒ–å°†è‡³å…³é‡è¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02253v5">PDF</a> 31 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>è¿™æ˜¯ä¸€ç¯‡å…³äºLLMå·¥ä½œæµè§„åˆ’æ€§èƒ½çš„ç ”ç©¶ã€‚è¯¥ç ”ç©¶ä»‹ç»äº†ä¸€ç§åä¸ºNL2Flowçš„å…¨è‡ªåŠ¨åŒ–ç®¡é“ï¼Œç”¨äºç”Ÿæˆå’Œè¯„ä¼°å·¥ä½œæµè§„åˆ’é—®é¢˜ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œä½¿ç”¨NL2Flowç”Ÿæˆçš„å‚æ•°åŒ–é—®é¢˜å¯ä»¥æˆåŠŸè¯„ä¼°LLMçš„æ¨ç†æ€§èƒ½ï¼Œè€Œç»“æ„åŒ–çš„JSONè¡¨ç¤ºæ–¹å¼å¯¹äºè®¡åˆ’ç”ŸæˆæˆåŠŸç‡çš„æé«˜æœ‰é‡è¦ä½œç”¨ã€‚æ­¤ç ”ç©¶æœ‰åŠ©äºäº†è§£è¯¯å·®çš„æ¥æºåŠå…¶åœ¨å¤æ‚çš„ä»»åŠ¡ä¸­å¯¹LLMçš„å½±å“ã€‚éšç€LLMçš„å‘å±•ï¼Œå¯¹é”™è¯¯ç“¶é¢ˆçš„è¯†åˆ«å¯¹äºè§£å†³æ—¥ç›Šå¤æ‚çš„é—®é¢˜å°†å˜å¾—æ›´åŠ é‡è¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯å…³äºè¯¥æ–‡æœ¬çš„å…³é”®è§è§£ï¼š</p>
<ul>
<li>LLMçš„å·¥ä½œæµè§„åˆ’æ€§èƒ½è‡³å…³é‡è¦ï¼Œä½†ç¼ºä¹å¯æ‰©å±•çš„è¯„ä¼°æ•°æ®é™åˆ¶äº†å…¶è¿›å±•ã€‚</li>
<li>NL2Flowæ˜¯ä¸€ä¸ªè‡ªåŠ¨åŒ–ç®¡é“ï¼Œç”¨äºç”Ÿæˆå’Œè¯„ä¼°å·¥ä½œæµè§„åˆ’é—®é¢˜ï¼Œå¯ä»¥ç”Ÿæˆå‚æ•°åŒ–é—®é¢˜å¹¶è½¬åŒ–ä¸ºè‡ªç„¶è¯­è¨€æˆ–æ­£å¼PDDLã€‚</li>
<li>æœ€ä½³æ€§èƒ½çš„æ¨¡å‹åœ¨ç”Ÿæˆæœ‰æ•ˆè®¡åˆ’å’Œæœ€ä¼˜è®¡åˆ’æ–¹é¢è¾¾åˆ°äº†86%å’Œ69%çš„æˆåŠŸç‡ã€‚</li>
<li>ç»“æ„åŒ–çš„JSONè¡¨ç¤ºæ–¹å¼åœ¨è®¡åˆ’ç”Ÿæˆä¸­èµ·åˆ°äº†å…³é”®ä½œç”¨ï¼Œæ˜¾è‘—æé«˜äº†æˆåŠŸç‡ã€‚è¿™æ˜¾ç¤ºäº†ç¥ç»ç¬¦å·æ•´åˆçš„ç›Šå¤„ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02253">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2507.02253v5/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2507.02253v5/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2507.02253v5/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2507.02253v5/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_R1_Reasoning/2507.02253v5/page_5_0.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-13/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-13/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-13/LLM/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_LLM/2406.06464v4/page_2_0.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-13  The Illusion of Diminishing Returns Measuring Long Horizon Execution in   LLMs
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-13
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-13/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-13\./crop_Talking Head Generation/2509.08854v1/page_2_0.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-13  A vibe coding learning design to enhance EFL students' talking to,   through, and about AI
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-13
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30055.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
