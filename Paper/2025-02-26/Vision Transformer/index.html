<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Vision Transformer">
    <meta name="description" content="Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-26  MaxGlaViT A novel lightweight vision transformer-based approach for   early diagnosis of glaucoma stages from fundus images">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Vision Transformer | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-3edc94fc927424abc49ce1b01740d708.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Vision Transformer</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Vision-Transformer/">
                                <span class="chip bg-color">Vision Transformer</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                Vision Transformer
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-26
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    9.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    37 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-02-26-æ›´æ–°"><a href="#2025-02-26-æ›´æ–°" class="headerlink" title="2025-02-26 æ›´æ–°"></a>2025-02-26 æ›´æ–°</h1><h2 id="MaxGlaViT-A-novel-lightweight-vision-transformer-based-approach-for-early-diagnosis-of-glaucoma-stages-from-fundus-images"><a href="#MaxGlaViT-A-novel-lightweight-vision-transformer-based-approach-for-early-diagnosis-of-glaucoma-stages-from-fundus-images" class="headerlink" title="MaxGlaViT: A novel lightweight vision transformer-based approach for   early diagnosis of glaucoma stages from fundus images"></a>MaxGlaViT: A novel lightweight vision transformer-based approach for   early diagnosis of glaucoma stages from fundus images</h2><p><strong>Authors:Mustafa Yurdakul, Kubra Uyar, Sakir Tasdemir</strong></p>
<p>Glaucoma is a prevalent eye disease that progresses silently without symptoms. If not detected and treated early, it can cause permanent vision loss. Computer-assisted diagnosis systems play a crucial role in timely and efficient identification. This study introduces MaxGlaViT, a lightweight model based on the restructured Multi-Axis Vision Transformer (MaxViT) for early glaucoma detection. First, MaxViT was scaled to optimize block and channel numbers, resulting in a lighter architecture. Second, the stem was enhanced by adding attention mechanisms (CBAM, ECA, SE) after convolution layers to improve feature learning. Third, MBConv structures in MaxViT blocks were replaced by advanced DL blocks (ConvNeXt, ConvNeXtV2, InceptionNeXt). The model was evaluated using the HDV1 dataset, containing fundus images of different glaucoma stages. Additionally, 40 CNN and 40 ViT models were tested on HDV1 to validate MaxGlaViTâ€™s efficiency. Among CNN models, EfficientB6 achieved the highest accuracy (84.91%), while among ViT models, MaxViT-Tiny performed best (86.42%). The scaled MaxViT reached 87.93% accuracy. Adding ECA to the stem block increased accuracy to 89.01%. Replacing MBConv with ConvNeXtV2 further improved it to 89.87%. Finally, integrating ECA in the stem and ConvNeXtV2 in MaxViT blocks resulted in 92.03% accuracy. Testing 80 DL models for glaucoma stage classification, this study presents a comprehensive and comparative analysis. MaxGlaViT outperforms experimental and state-of-the-art models, achieving 92.03% accuracy, 92.33% precision, 92.03% recall, 92.13% f1-score, and 87.12% Cohenâ€™s kappa score. </p>
<blockquote>
<p>é’å…‰çœ¼æ˜¯ä¸€ç§æ™®éä¸”æ— å£°è¿›å±•çš„çœ¼ç—…ã€‚å¦‚æœæ—©æœŸæœªè¢«å‘ç°å’Œæ²»ç–—ï¼Œå®ƒå¯èƒ½å¯¼è‡´æ°¸ä¹…æ€§è§†åŠ›ä¸§å¤±ã€‚è®¡ç®—æœºè¾…åŠ©è¯Šæ–­ç³»ç»Ÿåœ¨åŠæ—¶æœ‰æ•ˆçš„è¯†åˆ«ä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚æœ¬ç ”ç©¶ä»‹ç»äº†ä¸€ç§åŸºäºé‡æ„çš„å¤šè½´è§†è§‰è½¬æ¢å™¨ï¼ˆMaxViTï¼‰çš„è½»é‡çº§æ¨¡å‹MaxGlaViTï¼Œç”¨äºæ—©æœŸé’å…‰çœ¼çš„æ£€æµ‹ã€‚é¦–å…ˆï¼Œå¯¹MaxViTè¿›è¡Œç¼©æ”¾ä»¥ä¼˜åŒ–å—å’Œé€šé“æ•°é‡ï¼Œä»è€Œå¾—åˆ°æ›´è½»çš„ç»“æ„ã€‚å…¶æ¬¡ï¼Œé€šè¿‡åœ¨å·ç§¯å±‚ä¹‹åæ·»åŠ æ³¨æ„åŠ›æœºåˆ¶ï¼ˆCBAMã€ECAã€SEï¼‰æ¥å¢å¼ºä¸»å¹²ï¼Œä»¥æé«˜ç‰¹å¾å­¦ä¹ èƒ½åŠ›ã€‚ç¬¬ä¸‰ï¼Œå°†MaxViTå—ä¸­çš„MBConvç»“æ„æ›¿æ¢ä¸ºå…ˆè¿›çš„DLå—ï¼ˆConvNeXtã€ConvNeXtV2ã€InceptionNeXtï¼‰ã€‚è¯¥æ¨¡å‹ä½¿ç”¨HDV1æ•°æ®é›†è¿›è¡Œè¯„ä¼°ï¼Œè¯¥æ•°æ®é›†åŒ…å«ä¸åŒé’å…‰çœ¼é˜¶æ®µçš„çœ¼åº•å›¾åƒã€‚å¦å¤–ï¼Œè¿˜åœ¨HDV1ä¸Šæµ‹è¯•äº†40ä¸ªCNNå’Œ40ä¸ªViTæ¨¡å‹ï¼Œä»¥éªŒè¯MaxGlaViTçš„æ•ˆç‡ã€‚åœ¨CNNæ¨¡å‹ä¸­ï¼ŒEfficientB6çš„å‡†ç¡®ç‡æœ€é«˜ï¼ˆ84.91%ï¼‰ï¼Œè€Œåœ¨ViTæ¨¡å‹ä¸­ï¼ŒMaxViT-Tinyçš„è¡¨ç°æœ€ä½³ï¼ˆ86.42%ï¼‰ã€‚ç¼©æ”¾çš„MaxViTè¾¾åˆ°äº†87.93%çš„å‡†ç¡®ç‡ã€‚åœ¨ä¸»å¹²å—ä¸­æ·»åŠ ECAå°†å‡†ç¡®ç‡æé«˜åˆ°89.01%ã€‚å°†MBConvæ›¿æ¢ä¸ºConvNeXtV2è¿›ä¸€æ­¥æé«˜äº†å‡†ç¡®ç‡è‡³89.87%ã€‚æœ€åï¼Œåœ¨ä¸»å¹²ä¸­é›†æˆECAï¼Œå¹¶åœ¨MaxViTå—ä¸­ä½¿ç”¨ConvNeXtV2ï¼Œå‡†ç¡®ç‡è¾¾åˆ°äº†92.03%ã€‚æœ¬ç ”ç©¶å¯¹80ä¸ªç”¨äºé’å…‰çœ¼åˆ†æœŸåˆ†ç±»çš„DLæ¨¡å‹è¿›è¡Œäº†ç»¼åˆæ¯”è¾ƒå’Œåˆ†æã€‚MaxGlaViTè¡¨ç°ä¼˜äºå®éªŒå’Œæœ€æ–°æ¨¡å‹ï¼Œè¾¾åˆ°äº†92.03%çš„å‡†ç¡®ç‡ã€92.33%çš„ç²¾ç¡®åº¦ã€92.03%çš„å¬å›ç‡ã€92.13%çš„f1å¾—åˆ†å’Œ87.12%çš„Cohen Kappaå¾—åˆ†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17154v1">PDF</a> </p>
<p><strong>Summary</strong><br>     è¯¥ç ”ç©¶æå‡ºä¸€ç§åŸºäºé‡æ„çš„å¤šè½´è§†è§‰è½¬æ¢å™¨ï¼ˆMaxViTï¼‰çš„è½»é‡çº§æ¨¡å‹MaxGlaViTï¼Œç”¨äºæ—©æœŸé’å…‰çœ¼æ£€æµ‹ã€‚é€šè¿‡ä¼˜åŒ–å—å’Œé€šé“æ•°é‡ï¼Œå¢å¼ºä¸»å¹²å¹¶æ›¿æ¢MBConvç»“æ„ï¼ŒMaxGlaViTåœ¨HDV1æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–80ç§ç”¨äºé’å…‰çœ¼åˆ†æœŸåˆ†ç±»çš„æ·±åº¦æ¨¡å‹ï¼Œå…·æœ‰92.03%çš„å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é’å…‰çœ¼æ˜¯ä¸€ç§æ™®éä¸”æ— å£°è¿›å±•çš„çœ¼ç–¾ï¼Œæ—©æœŸæ£€æµ‹å’Œæ²»ç–—è‡³å…³é‡è¦ã€‚</li>
<li>è®¡ç®—æœºè¾…åŠ©è¯Šæ–­ç³»ç»Ÿåœ¨åŠæ—¶æœ‰æ•ˆè¯†åˆ«é’å…‰çœ¼ä¸­èµ·å…³é”®ä½œç”¨ã€‚</li>
<li>MaxGlaViTæ˜¯ä¸€ä¸ªåŸºäºé‡ç»“æ„çš„å¤šè½´è§†è§‰è½¬æ¢å™¨ï¼ˆMaxViTï¼‰çš„è½»é‡çº§æ¨¡å‹ï¼Œç”¨äºæ—©æœŸé’å…‰çœ¼æ£€æµ‹ã€‚</li>
<li>MaxGlaViTé€šè¿‡ä¼˜åŒ–å—å’Œé€šé“æ•°é‡ã€å¢å¼ºä¸»å¹²å¹¶æ›¿æ¢MBConvç»“æ„æ¥æé«˜æ€§èƒ½ã€‚</li>
<li>åœ¨HDV1æ•°æ®é›†ä¸Šï¼ŒMaxGlaViTè¡¨ç°å‡ºè¾ƒé«˜çš„å‡†ç¡®æ€§ï¼ˆ92.03%ï¼‰ï¼Œä¼˜äºå…¶ä»–40ç§CNNå’Œ40ç§ViTæ¨¡å‹ã€‚</li>
<li>é›†æˆECAå’ŒConvNeXtV2è¿›ä¸€æ­¥æé«˜äº†æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17154">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-03cd7abc6342931b4ae3547904cb8628.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-de329420844cabb1741a06b9927fc71a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d82bc91564198980a4fe0e266ea433dd.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Cross-Model-Transferability-of-Adversarial-Patches-in-Real-time-Segmentation-for-Autonomous-Driving"><a href="#Cross-Model-Transferability-of-Adversarial-Patches-in-Real-time-Segmentation-for-Autonomous-Driving" class="headerlink" title="Cross-Model Transferability of Adversarial Patches in Real-time   Segmentation for Autonomous Driving"></a>Cross-Model Transferability of Adversarial Patches in Real-time   Segmentation for Autonomous Driving</h2><p><strong>Authors:Prashant Shekhar, Bidur Devkota, Dumindu Samaraweera, Laxima Niure Kandel, Manoj Babu</strong></p>
<p>Adversarial attacks pose a significant threat to deep learning models, particularly in safety-critical applications like healthcare and autonomous driving. Recently, patch based attacks have demonstrated effectiveness in real-time inference scenarios owing to their â€˜drag and dropâ€™ nature. Following this idea for Semantic Segmentation (SS), here we propose a novel Expectation Over Transformation (EOT) based adversarial patch attack that is more realistic for autonomous vehicles. To effectively train this attack we also propose a â€˜simplifiedâ€™ loss function that is easy to analyze and implement. Using this attack as our basis, we investigate whether adversarial patches once optimized on a specific SS model, can fool other models or architectures. We conduct a comprehensive cross-model transferability analysis of adversarial patches trained on SOTA Convolutional Neural Network (CNN) models such PIDNet-S, PIDNet-M and PIDNet-L, among others. Additionally, we also include the Segformer model to study transferability to Vision Transformers (ViTs). All of our analysis is conducted on the widely used Cityscapes dataset. Our study reveals key insights into how model architectures (CNN vs CNN or CNN vs. Transformer-based) influence attack susceptibility. In particular, we conclude that although the transferability (effectiveness) of attacks on unseen images of any dimension is really high, the attacks trained against one particular model are minimally effective on other models. And this was found to be true for both ViT and CNN based models. Additionally our results also indicate that for CNN-based models, the repercussions of patch attacks are local, unlike ViTs. Per-class analysis reveals that simple-classes like â€˜skyâ€™ suffer less misclassification than others. The code for the project is available at: <a target="_blank" rel="noopener" href="https://github.com/p-shekhar/adversarial-patch-transferability">https://github.com/p-shekhar/adversarial-patch-transferability</a> </p>
<blockquote>
<p>å¯¹æŠ—æ€§æ”»å‡»å¯¹æ·±åº¦å­¦ä¹ æ¨¡å‹æ„æˆé‡å¤§å¨èƒï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»ç–—å’Œè‡ªåŠ¨é©¾é©¶ç­‰å®‰å…¨å…³é”®åº”ç”¨ä¸­ã€‚æœ€è¿‘ï¼ŒåŸºäºâ€œæ‹–æ”¾â€ç‰¹æ€§çš„è¡¥ä¸æ”»å‡»åœ¨å®æ—¶æ¨ç†åœºæ™¯ä¸­è¡¨ç°å‡ºäº†æœ‰æ•ˆæ€§ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œé’ˆå¯¹è¯­ä¹‰åˆ†å‰²ï¼ˆSSï¼‰ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæœŸæœ›è½¬æ¢ï¼ˆEOTï¼‰çš„å¯¹æŠ—æ€§è¡¥ä¸æ”»å‡»ï¼Œå¯¹äºè‡ªåŠ¨é©¾é©¶è½¦è¾†è€Œè¨€æ›´ä¸ºç°å®ã€‚ä¸ºäº†æœ‰æ•ˆåœ°è®­ç»ƒè¿™ç§æ”»å‡»ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ä¸ªæ˜“äºåˆ†æå’Œå®ç°çš„â€œç®€åŒ–â€æŸå¤±å‡½æ•°ã€‚ä»¥è¿™ç§æ”»å‡»ä¸ºåŸºç¡€ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ç»è¿‡ä¼˜åŒ–çš„å¯¹æŠ—è¡¥ä¸æ˜¯å¦å¯ä»¥åœ¨ç‰¹å®šçš„SSæ¨¡å‹ä¸Šæ¬ºéª—å…¶ä»–æ¨¡å‹æˆ–æ¶æ„ã€‚æˆ‘ä»¬å¯¹åœ¨è¯¸å¦‚PIDNet-Sã€PIDNet-Må’ŒPIDNet-Lç­‰æœ€å…ˆè¿›çš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ¨¡å‹ä¸Šè®­ç»ƒçš„å¯¹æŠ—è¡¥ä¸è¿›è¡Œäº†å…¨é¢çš„è·¨æ¨¡å‹å¯ä¼ è¾“æ€§åˆ†æï¼Œå¦å¤–è¿˜åŒ…å«äº†é’ˆå¯¹è§†è§‰å˜å‹å™¨ï¼ˆViTï¼‰çš„å¯è½¬ç§»æ€§ç ”ç©¶ã€‚æˆ‘ä»¬çš„æ‰€æœ‰åˆ†æéƒ½æ˜¯åœ¨å¹¿æ³›ä½¿ç”¨çš„Cityscapesæ•°æ®é›†ä¸Šè¿›è¡Œçš„ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ­ç¤ºäº†æ¨¡å‹æ¶æ„ï¼ˆCNNä¸CNNæˆ–CNNä¸åŸºäºTransformerçš„æ¶æ„ï¼‰å¦‚ä½•å½±å“æ”»å‡»çš„æ˜“æ„Ÿæ€§ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬å¾—å‡ºç»“è®ºï¼Œå°½ç®¡é’ˆå¯¹ä»»ä½•å°ºå¯¸çš„æœªè§å›¾åƒçš„æ”»å‡»çš„ä¼ è¾“æ€§ï¼ˆæœ‰æ•ˆæ€§ï¼‰éå¸¸é«˜ï¼Œä½†é’ˆå¯¹ç‰¹å®šæ¨¡å‹è®­ç»ƒçš„æ”»å‡»å¯¹å…¶ä»–æ¨¡å‹çš„æ•ˆåŠ›å¾®ä¹å…¶å¾®ã€‚è¿™ä¸€å‘ç°åœ¨ViTå’ŒCNNæ¨¡å‹ä¸­å‡æˆç«‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç»“æœè¿˜è¡¨æ˜ï¼Œå¯¹äºåŸºäºCNNçš„æ¨¡å‹è€Œè¨€ï¼Œè¡¥ä¸æ”»å‡»çš„åæœæ˜¯å±€éƒ¨çš„ï¼Œä¸ViTä¸åŒã€‚æŒ‰ç±»åˆ«åˆ†æè¡¨æ˜ï¼Œâ€œå¤©ç©ºâ€ç­‰ç®€å•ç±»åˆ«é­å—çš„è¯¯åˆ†ç±»è¾ƒå°‘ã€‚è¯¥é¡¹ç›®çš„ä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/p-shekhar/adversarial-patch-transferability%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/p-shekhar/adversarial-patch-transferabilityæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.16012v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºæœŸæœ›è½¬æ¢ï¼ˆEOTï¼‰çš„å¯¹æŠ—è¡¥ä¸æ”»å‡»ï¼Œé’ˆå¯¹è‡ªåŠ¨é©¾é©¶çš„è¯­ä¹‰åˆ†å‰²æ¨¡å‹ï¼Œæ›´åŠ çœŸå®æœ‰æ•ˆã€‚åŒæ—¶ï¼Œä¸ºäº†æœ‰æ•ˆè®­ç»ƒè¯¥æ”»å‡»ï¼Œè¿˜æå‡ºäº†ä¸€ç§æ˜“äºåˆ†æå’Œå®ç°çš„ç®€åŒ–æŸå¤±å‡½æ•°ã€‚ç ”ç©¶è¿˜è¿›è¡Œäº†è·¨æ¨¡å‹çš„å¯¹æŠ—è¡¥ä¸è½¬ç§»æ€§åˆ†æï¼Œæ¶‰åŠå…ˆè¿›çš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ¨¡å‹å’Œè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰ã€‚åˆ†ææ­ç¤ºæ¨¡å‹æ¶æ„å¯¹æ”»å‡»æ˜“æ„Ÿæ€§çš„å½±å“ï¼Œå¹¶å‘ç°è™½ç„¶æ”»å‡»åœ¨ä¸åŒç»´åº¦å›¾åƒä¸Šçš„è¿ç§»æ€§å¾ˆé«˜ï¼Œä½†å¯¹ä¸åŒæ¨¡å‹çš„æ”»å‡»æ•ˆæœæœ‰é™ã€‚å¦å¤–ï¼Œå¯¹äºCNNæ¨¡å‹ï¼Œè¡¥ä¸æ”»å‡»çš„å½±å“è¾ƒä¸ºå±€éƒ¨ï¼Œä¸ViTä¸åŒã€‚ç®€å•ç±»åˆ«çš„è¯¯åˆ†ç±»ç¨‹åº¦è¾ƒä½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†åŸºäºæœŸæœ›è½¬æ¢ï¼ˆEOTï¼‰çš„å¯¹æŠ—è¡¥ä¸æ”»å‡»ï¼Œé’ˆå¯¹è‡ªåŠ¨é©¾é©¶è¯­ä¹‰åˆ†å‰²æ¨¡å‹æ›´ä¸ºçœŸå®æœ‰æ•ˆã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§ç®€åŒ–æŸå¤±å‡½æ•°ï¼Œä¾¿äºæ”»å‡»çš„è®­ç»ƒã€åˆ†æå’Œå®æ–½ã€‚</li>
<li>è¿›è¡Œäº†å¹¿æ³›çš„è·¨æ¨¡å‹å¯¹æŠ—è¡¥ä¸è¿ç§»æ€§åˆ†æï¼ŒåŒ…æ‹¬å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’Œè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰ã€‚</li>
<li>å‘ç°è™½ç„¶æ”»å‡»è¿ç§»æ€§é«˜ï¼Œä½†å¯¹ä¸åŒæ¨¡å‹çš„æ”»å‡»æ•ˆæœæœ‰é™ã€‚</li>
<li>å¯¹æŠ—è¡¥ä¸æ”»å‡»åœ¨CNNæ¨¡å‹ä¸Šçš„å½±å“è¾ƒä¸ºå±€éƒ¨ï¼Œä¸ViTæœ‰æ‰€ä¸åŒã€‚</li>
<li>ç®€å•ç±»åˆ«çš„è¯¯åˆ†ç±»ç¨‹åº¦è¾ƒä½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.16012">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6b63dde81d9277d304c73a9d7c13b38a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b5b113d8ec384655538b84e5e840240.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8dece0b8d943e3af70be7eeece7109d1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3edc94fc927424abc49ce1b01740d708.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4fd02b8b43be872f116976c54da19d47.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="ELIP-Enhanced-Visual-Language-Foundation-Models-for-Image-Retrieval"><a href="#ELIP-Enhanced-Visual-Language-Foundation-Models-for-Image-Retrieval" class="headerlink" title="ELIP: Enhanced Visual-Language Foundation Models for Image Retrieval"></a>ELIP: Enhanced Visual-Language Foundation Models for Image Retrieval</h2><p><strong>Authors:Guanqi Zhan, Yuanpei Liu, Kai Han, Weidi Xie, Andrew Zisserman</strong></p>
<p>The objective in this paper is to improve the performance of text-to-image retrieval. To this end, we introduce a new framework that can boost the performance of large-scale pre-trained vision-language models, so that they can be used for text-to-image re-ranking. The approach, Enhanced Language-Image Pre-training (ELIP), uses the text query to predict a set of visual prompts to condition the ViT image encoding. ELIP can easily be applied to the commonly used CLIP&#x2F;SigLIP and the state-of-the-art BLIP-2 architectures. To train the architecture with limited computing resources, we develop a â€˜student friendlyâ€™ best practice involving global hard sample mining, and selection and curation of a large-scale dataset. On the evaluation side, we set up two new out-of-distribution benchmarks, Occluded COCO and ImageNet-R, to assess the zero-shot generalisation of the models to different domains. Benefiting from the novel architecture and data curation, experiments show our enhanced network significantly boosts CLIP&#x2F;SigLIP performance and outperforms the state-of-the-art BLIP-2 model on text-to-image retrieval. </p>
<blockquote>
<p>æœ¬æ–‡çš„ç›®æ ‡æ˜¯æå‡æ–‡æœ¬åˆ°å›¾åƒæ£€ç´¢çš„æ€§èƒ½ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§èƒ½å¤Ÿæå‡å¤§è§„æ¨¡é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹æ€§èƒ½çš„æ–°æ¡†æ¶ï¼Œä½¿å…¶å¯ç”¨äºæ–‡æœ¬åˆ°å›¾åƒçš„é‡æ–°æ’åºã€‚è¯¥æ–¹æ³•ç§°ä¸ºå¢å¼ºè¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆELIPï¼‰ï¼Œå®ƒä½¿ç”¨æ–‡æœ¬æŸ¥è¯¢æ¥é¢„æµ‹ä¸€ç³»åˆ—è§†è§‰æç¤ºï¼Œä»¥è°ƒèŠ‚ViTå›¾åƒç¼–ç ã€‚ELIPå¯è½»æ¾åº”ç”¨äºå¸¸ç”¨çš„CLIP&#x2F;SigLIPå’Œå…ˆè¿›çš„BLIP-2æ¶æ„ã€‚ä¸ºäº†ä½¿ç”¨æœ‰é™çš„è®¡ç®—èµ„æºè¿›è¡Œè®­ç»ƒï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§â€œå­¦ç”Ÿå‹å¥½å‹â€çš„æœ€ä½³å®è·µæ–¹æ³•ï¼ŒåŒ…æ‹¬å…¨å±€ç¡¬æ ·æœ¬æŒ–æ˜ä»¥åŠå¤§è§„æ¨¡æ•°æ®é›†çš„é€‰æ‹©å’Œç­›é€‰ã€‚åœ¨è¯„ä¼°æ–¹é¢ï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸¤ä¸ªæ–°çš„ç¦»ç¾¤åˆ†å¸ƒåŸºå‡†æµ‹è¯•ï¼Œå³é®æŒ¡COCOå’ŒImageNet-Rï¼Œä»¥è¯„ä¼°æ¨¡å‹åœ¨ä¸åŒé¢†åŸŸçš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚å—ç›Šäºæ–°å‹æ¶æ„å’Œæ•°æ®ç­›é€‰ï¼Œå®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„å¢å¼ºç½‘ç»œæ˜¾è‘—æå‡äº†CLIP&#x2F;SigLIPçš„æ€§èƒ½ï¼Œå¹¶åœ¨æ–‡æœ¬åˆ°å›¾åƒæ£€ç´¢æ–¹é¢è¶…è¶Šäº†æœ€å…ˆè¿›çš„BLIP-2æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.15682v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ–‡æœ¬ä»‹ç»äº†é€šè¿‡å¼•å…¥æ–°çš„æ¡†æ¶Enhanced Language-Image Pre-training (ELIP)æ¥æé«˜å¤§è§„æ¨¡é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒæ£€ç´¢ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚è¯¥æ¡†æ¶åˆ©ç”¨æ–‡æœ¬æŸ¥è¯¢é¢„æµ‹ä¸€ç»„è§†è§‰æç¤ºæ¥å½±å“å›¾åƒç¼–ç ï¼Œå¹¶å¯ä»¥è½»æ¾åœ°åº”ç”¨äºå¸¸è§çš„CLIP&#x2F;SigLIPå’Œå…ˆè¿›çš„BLIP-2æ¶æ„ã€‚æ­¤å¤–ï¼Œé€šè¿‡å…¨çƒç¡¬æ ·æœ¬æŒ–æ˜ä»¥åŠå¤§è§„æ¨¡æ•°æ®é›†çš„é€‰æ‹©å’Œç­›é€‰æ¥ä»¥æœ‰é™çš„è®¡ç®—èµ„æºè®­ç»ƒæ¶æ„ã€‚åœ¨è¯„ä¼°æ–¹é¢ï¼Œä¸ºäº†è¯„ä¼°æ¨¡å‹åœ¨ä¸åŒé¢†åŸŸçš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œè®¾ç½®äº†ä¸¤ä¸ªæ–°çš„è¶…å‡ºåˆ†å¸ƒèŒƒå›´çš„åŸºå‡†æµ‹è¯•ï¼Œå³é®æŒ¡COCOå’ŒImageNet-Rã€‚å®éªŒè¡¨æ˜ï¼Œå¾—ç›Šäºæ–°çš„æ¶æ„å’Œæ•°æ®ç­›é€‰ï¼Œå¢å¼ºåçš„ç½‘ç»œæ˜¾è‘—æé«˜äº†CLIP&#x2F;SigLIPçš„æ€§èƒ½ï¼Œå¹¶åœ¨æ–‡æœ¬åˆ°å›¾åƒæ£€ç´¢ä»»åŠ¡ä¸Šè¶…è¶Šäº†æœ€å…ˆè¿›çš„BLIP-2æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡ç›®æ ‡æ˜¯æé«˜æ–‡æœ¬åˆ°å›¾åƒæ£€ç´¢çš„æ€§èƒ½ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„æ¡†æ¶Enhanced Language-Image Pre-training (ELIP)ï¼Œç”¨äºå¢å¼ºå¤§è§„æ¨¡é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ã€‚</li>
<li>ELIPæ¡†æ¶åˆ©ç”¨æ–‡æœ¬æŸ¥è¯¢é¢„æµ‹è§†è§‰æç¤ºæ¥å½±å“å›¾åƒç¼–ç ã€‚</li>
<li>ELIPå¯åº”ç”¨äºCLIP&#x2F;SigLIPå’ŒBLIP-2ç­‰æ¶æ„ã€‚</li>
<li>é€šè¿‡å…¨çƒç¡¬æ ·æœ¬æŒ–æ˜å’Œå¤§è§„æ¨¡æ•°æ®é›†çš„é€‰æ‹©ä¸ç­›é€‰ï¼Œä½¿ç”¨æœ‰é™çš„è®¡ç®—èµ„æºè¿›è¡Œè®­ç»ƒã€‚</li>
<li>è®ºæ–‡è®¾ç½®äº†ä¸¤ä¸ªæ–°çš„è¯„ä¼°åŸºå‡†æµ‹è¯•ï¼šOccluded COCOå’ŒImageNet-Rï¼Œç”¨äºè¯„ä¼°æ¨¡å‹åœ¨ä¸åŒé¢†åŸŸçš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.15682">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3f327fd642d7fe6ef587739ff2d291c0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c97f30791cd7904ab463f89ee0cc0602.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8012401578b5d4fcd94c79e02f28094e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-39280eeb3ce109ef5ed2c133721ae7c8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5362d3c751bea354bc286dac3fe3f0e.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="CopyJudge-Automated-Copyright-Infringement-Identification-and-Mitigation-in-Text-to-Image-Diffusion-Models"><a href="#CopyJudge-Automated-Copyright-Infringement-Identification-and-Mitigation-in-Text-to-Image-Diffusion-Models" class="headerlink" title="CopyJudge: Automated Copyright Infringement Identification and   Mitigation in Text-to-Image Diffusion Models"></a>CopyJudge: Automated Copyright Infringement Identification and   Mitigation in Text-to-Image Diffusion Models</h2><p><strong>Authors:Shunchang Liu, Zhuan Shi, Lingjuan Lyu, Yaochu Jin, Boi Faltings</strong></p>
<p>Assessing whether AI-generated images are substantially similar to copyrighted works is a crucial step in resolving copyright disputes. In this paper, we propose CopyJudge, an automated copyright infringement identification framework that leverages large vision-language models (LVLMs) to simulate practical court processes for determining substantial similarity between copyrighted images and those generated by text-to-image diffusion models. Specifically, we employ an abstraction-filtration-comparison test framework with multi-LVLM debate to assess the likelihood of infringement and provide detailed judgment rationales. Based on the judgments, we further introduce a general LVLM-based mitigation strategy that automatically optimizes infringing prompts by avoiding sensitive expressions while preserving the non-infringing content. Besides, our approach can be enhanced by exploring non-infringing noise vectors within the diffusion latent space via reinforcement learning, even without modifying the original prompts. Experimental results show that our identification method achieves comparable state-of-the-art performance, while offering superior generalization and interpretability across various forms of infringement, and that our mitigation method could more effectively mitigate memorization and IP infringement without losing non-infringing expressions. </p>
<blockquote>
<p>è¯„ä¼°äººå·¥æ™ºèƒ½ç”Ÿæˆçš„å›¾åƒæ˜¯å¦ä¸å—ç‰ˆæƒä¿æŠ¤çš„ä½œå“å­˜åœ¨å®è´¨æ€§ç›¸ä¼¼ï¼Œæ˜¯è§£å†³ç‰ˆæƒçº çº·çš„é‡è¦æ­¥éª¤ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†CopyJudgeï¼Œè¿™æ˜¯ä¸€ä¸ªè‡ªåŠ¨ç‰ˆæƒä¾µæƒè¯†åˆ«æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰æ¥æ¨¡æ‹Ÿå®é™…çš„æ³•åº­æµç¨‹ï¼Œä»¥ç¡®å®šå—ç‰ˆæƒä¿æŠ¤çš„å›¾åƒä¸æ–‡æœ¬ç”Ÿæˆçš„æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„å›¾åƒä¹‹é—´çš„å®è´¨æ€§ç›¸ä¼¼æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é‡‡ç”¨æŠ½è±¡è¿‡æ»¤æ¯”è¾ƒæµ‹è¯•æ¡†æ¶å’Œå¤šLVLMè¾©è®ºæ¥è¯„ä¼°ä¾µæƒçš„å¯èƒ½æ€§ï¼Œå¹¶æä¾›è¯¦ç»†çš„åˆ¤æ–­ä¾æ®ã€‚åŸºäºè¿™äº›åˆ¤æ–­ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥äº†ä¸€ç§é€šç”¨çš„åŸºäºLVLMçš„ç¼“è§£ç­–ç•¥ï¼Œè¯¥ç­–ç•¥é€šè¿‡é¿å…æ•æ„Ÿè¡¨è¾¾æ¥è‡ªåŠ¨ä¼˜åŒ–ä¾µæƒæç¤ºï¼ŒåŒæ—¶ä¿ç•™éä¾µæƒå†…å®¹ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ æ¢ç´¢æ‰©æ•£æ½œåœ¨ç©ºé—´ä¸­çš„éä¾µæƒå™ªå£°å‘é‡ï¼Œç”šè‡³åœ¨ä¸ä¿®æ”¹åŸå§‹æç¤ºçš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥å¾—åˆ°å¢å¼ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„è¯†åˆ«æ–¹æ³•è¾¾åˆ°äº†ä¸æœ€æ–°æŠ€æœ¯ç›¸å½“çš„æ€§èƒ½ï¼ŒåŒæ—¶åœ¨å„ç§å½¢å¼çš„ä¾µæƒé—®é¢˜ä¸Šæä¾›äº†å“è¶Šçš„ä¸€èˆ¬åŒ–å’Œå¯è§£é‡Šæ€§ï¼›æˆ‘ä»¬çš„ç¼“è§£æ–¹æ³•èƒ½æ›´æœ‰æ•ˆåœ°å‡è½»è®°å¿†å’ŒçŸ¥è¯†äº§æƒä¾µæƒé—®é¢˜ï¼ŒåŒæ—¶ä¸æŸå¤±éä¾µæƒè¡¨è¾¾ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.15278v1">PDF</a> 17pages, 8 figures</p>
<p><strong>Summary</strong><br>ç‰ˆæƒåˆ¤æ–­AIç”Ÿæˆçš„å›¾åƒæ˜¯å¦å®è´¨æ€§ç›¸ä¼¼äºç‰ˆæƒä½œå“æ˜¯è§£å†³ç‰ˆæƒçº çº·çš„å…³é”®æ­¥éª¤ã€‚æœ¬æ–‡æå‡ºCopyJudgeæ¡†æ¶ï¼Œåˆ©ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹æ¨¡æ‹Ÿå®é™…æ³•åº­æµç¨‹ï¼Œè¯„ä¼°ä¾µæƒå¯èƒ½æ€§å¹¶æä¾›è¯¦ç»†çš„åˆ¤æ–­ä¾æ®ã€‚æ­¤å¤–ï¼Œå¼•å…¥åŸºäºLVLMçš„ç¼“è§£ç­–ç•¥ï¼Œä¼˜åŒ–ä¾µæƒæç¤ºå¹¶æ¢ç´¢éä¾µæƒå™ªå£°å‘é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ€§èƒ½ä¼˜è¶Šï¼Œå…·æœ‰æ›´å¥½çš„æ³›åŒ–å’Œè§£é‡Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CopyJudgeæ¡†æ¶ç”¨äºè¯„ä¼°AIç”Ÿæˆçš„å›¾åƒæ˜¯å¦ä¾µçŠ¯ç‰ˆæƒã€‚</li>
<li>åˆ©ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹æ¨¡æ‹Ÿå®é™…æ³•åº­æµç¨‹è¿›è¡Œä¾µæƒåˆ¤æ–­ã€‚</li>
<li>é€šè¿‡æŠ½è±¡è¿‡æ»¤æ¯”è¾ƒæµ‹è¯•æ¡†æ¶å’Œå¤šLVLMè¾©è®ºè¿›è¡Œå®è´¨æ€§ç›¸ä¼¼æ€§è¯„ä¼°ã€‚</li>
<li>æä¾›è¯¦ç»†çš„åˆ¤æ–­ä¾æ®ã€‚</li>
<li>å¼•å…¥åŸºäºLVLMçš„ç¼“è§£ç­–ç•¥ï¼Œä¼˜åŒ–ä¾µæƒæç¤ºå¹¶é¿å…æ•æ„Ÿè¡¨è¾¾ã€‚</li>
<li>åœ¨æ‰©æ•£æ½œåœ¨ç©ºé—´å†…æ¢ç´¢éä¾µæƒå™ªå£°å‘é‡ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ å¢å¼ºæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.15278">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-5a67d3a08bf2247210433f135ed7eb57.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-34c0f6b5914a80ca3f3f4273007c4862.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-79710878b8ac6997fa427fe7ac19fd39.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f898e74e14bfa1576149bbb1c6075c9.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="LAVID-An-Agentic-LVLM-Framework-for-Diffusion-Generated-Video-Detection"><a href="#LAVID-An-Agentic-LVLM-Framework-for-Diffusion-Generated-Video-Detection" class="headerlink" title="LAVID: An Agentic LVLM Framework for Diffusion-Generated Video Detection"></a>LAVID: An Agentic LVLM Framework for Diffusion-Generated Video Detection</h2><p><strong>Authors:Qingyuan Liu, Yun-Yun Tsai, Ruijian Zha, Victoria Li, Pengyuan Shi, Chengzhi Mao, Junfeng Yang</strong></p>
<p>The impressive achievements of generative models in creating high-quality videos have raised concerns about digital integrity and privacy vulnerabilities. Recent works of AI-generated content detection have been widely studied in the image field (e.g., deepfake), yet the video field has been unexplored. Large Vision Language Model (LVLM) has become an emerging tool for AI-generated content detection for its strong reasoning and multimodal capabilities. It breaks the limitations of traditional deep learning based methods faced with like lack of transparency and inability to recognize new artifacts. Motivated by this, we propose LAVID, a novel LVLMs-based ai-generated video detection with explicit knowledge enhancement. Our insight list as follows: (1) The leading LVLMs can call external tools to extract useful information to facilitate its own video detection task; (2) Structuring the prompt can affect LVLMâ€™s reasoning ability to interpret information in video content. Our proposed pipeline automatically selects a set of explicit knowledge tools for detection, and then adaptively adjusts the structure prompt by self-rewriting. Different from prior SOTA that trains additional detectors, our method is fully training-free and only requires inference of the LVLM for detection. To facilitate our research, we also create a new benchmark \vidfor with high-quality videos generated from multiple sources of video generation tools. Evaluation results show that LAVID improves F1 scores by 6.2 to 30.2% over the top baselines on our datasets across four SOTA LVLMs. </p>
<blockquote>
<p>ç”Ÿæˆæ¨¡å‹åœ¨åˆ›å»ºé«˜è´¨é‡è§†é¢‘æ–¹é¢å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„æˆå°±ï¼Œè¿™å¼•å‘äº†äººä»¬å¯¹æ•°å­—å®Œæ•´æ€§å’Œéšç§æ¼æ´çš„æ‹…å¿§ã€‚å°½ç®¡äººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹çš„æ£€æµ‹åœ¨å›¾åƒé¢†åŸŸï¼ˆä¾‹å¦‚æ·±åº¦ä¼ªé€ ï¼‰å·²ç»å¾—åˆ°äº†å¹¿æ³›çš„ç ”ç©¶ï¼Œä½†è§†é¢‘é¢†åŸŸå°šæœªè¢«æ¢ç´¢ã€‚å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰å·²æˆä¸ºäººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹æ£€æµ‹çš„æ–°å…´å·¥å…·ï¼Œå› å…¶å¼ºå¤§çš„æ¨ç†å’Œå¤šæ¨¡æ€èƒ½åŠ›ã€‚å®ƒçªç ´äº†ä¼ ç»ŸåŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•æ‰€é¢ä¸´çš„å±€é™æ€§ï¼Œå¦‚ç¼ºä¹é€æ˜åº¦å’Œæ— æ³•è¯†åˆ«æ–°ä¼ªé€ çš„ç‰©å“ã€‚å—æ­¤å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†LAVIDï¼Œè¿™æ˜¯ä¸€ç§åŸºäºLVLMçš„æ–°å‹äººå·¥æ™ºèƒ½ç”Ÿæˆè§†é¢‘æ£€æµ‹ï¼Œå…·æœ‰æ˜ç¡®çš„çŸ¥è¯†å¢å¼ºåŠŸèƒ½ã€‚æˆ‘ä»¬çš„è§è§£å¦‚ä¸‹ï¼šï¼ˆ1ï¼‰é¢†å…ˆçš„LVLMå¯ä»¥è°ƒç”¨å¤–éƒ¨å·¥å…·æ¥æå–æœ‰ç”¨ä¿¡æ¯ï¼Œä»¥è¾…åŠ©å…¶è‡ªèº«çš„è§†é¢‘æ£€æµ‹ä»»åŠ¡ï¼›ï¼ˆ2ï¼‰ç»“æ„åŒ–æç¤ºä¼šå½±å“LVLMè§£é‡Šè§†é¢‘å†…å®¹ä¸­çš„ä¿¡æ¯çš„èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºçš„ç®¡é“ä¼šè‡ªåŠ¨é€‰æ‹©ä¸€ç»„æ˜ç¡®çš„çŸ¥è¯†å·¥å…·è¿›è¡Œæ£€æµ‹ï¼Œç„¶åé€šè¿‡è‡ªæˆ‘é‡å†™è‡ªé€‚åº”åœ°è°ƒæ•´ç»“æ„æç¤ºã€‚ä¸å…ˆå‰éœ€è¦è®­ç»ƒé™„åŠ æ£€æµ‹å™¨çš„æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®Œå…¨æ— éœ€è®­ç»ƒï¼Œåªéœ€è¦åˆ©ç”¨LVLMè¿›è¡Œæ¨ç†æ£€æµ‹ã€‚ä¸ºäº†æ¨åŠ¨æˆ‘ä»¬çš„ç ”ç©¶ï¼Œæˆ‘ä»¬è¿˜åˆ›å»ºäº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•\vidforï¼Œè¯¥æµ‹è¯•ä½¿ç”¨æ¥è‡ªå¤šä¸ªè§†é¢‘ç”Ÿæˆå·¥å…·çš„é«˜è´¨é‡è§†é¢‘ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œåœ¨æˆ‘ä»¬çš„æ•°æ®é›†ä¸Šï¼ŒLAVIDåœ¨å››ä¸ªæœ€å…ˆè¿›çš„LVLMä¸Šçš„F1åˆ†æ•°æé«˜äº†6.2%è‡³30.2%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14994v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å…³æ³¨AIç”Ÿæˆè§†é¢‘å†…å®¹çš„æ£€æµ‹é—®é¢˜ï¼Œæå‡ºä¸€ç§åŸºäºå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰çš„AIç”Ÿæˆè§†é¢‘æ£€æµ‹æ–°æ–¹æ³•LAVIDï¼Œå…·æœ‰æ˜ç¡®çš„çŸ¥è¯†å¢å¼ºåŠŸèƒ½ã€‚è¯¥æ–¹æ³•çªç ´äº†ä¼ ç»Ÿæ·±åº¦å­¦ä¹ æ–¹æ³•çš„å±€é™ï¼Œæ— éœ€é¢å¤–è®­ç»ƒæ£€æµ‹å™¨ï¼Œä»…é€šè¿‡LVLMçš„æ¨ç†å³å¯å®Œæˆæ£€æµ‹ä»»åŠ¡ã€‚ä¸ºæ¨è¿›ç ”ç©¶ï¼Œè¿˜å»ºç«‹äº†æ–°çš„è§†é¢‘ç”Ÿæˆå·¥å…·ç”Ÿæˆçš„é«˜è´¨é‡è§†é¢‘æ•°æ®é›†\vidforã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AIç”Ÿæˆè§†é¢‘å†…å®¹çš„æ£€æµ‹æˆä¸ºç ”ç©¶çƒ­ç‚¹ï¼Œå› æ‹…å¿§æ•°å­—å®Œæ•´æ€§å’Œéšç§æ¼æ´ã€‚</li>
<li>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰åœ¨AIç”Ÿæˆå†…å®¹æ£€æµ‹æ–¹é¢å±•ç°å‡ºå¼ºå¤§çš„æ¨ç†å’Œå¤šæ¨¡æ€èƒ½åŠ›ã€‚</li>
<li>LAVIDæ–¹æ³•åŸºäºLVLMï¼Œå¯è°ƒç”¨å¤–éƒ¨å·¥å…·æå–æœ‰ç”¨ä¿¡æ¯ï¼Œä¿ƒè¿›è§†é¢‘æ£€æµ‹ä»»åŠ¡ã€‚</li>
<li>ç»“æ„åŒ–çš„æç¤ºä¼šå½±å“LVLMå¯¹è§†é¢‘å†…å®¹ä¿¡æ¯çš„ç†è§£èƒ½åŠ›ã€‚</li>
<li>LAVIDæ–¹æ³•è‡ªé€‚åº”è°ƒæ•´ç»“æ„æç¤ºå¹¶é€šè¿‡è‡ªæˆ‘é‡å†™å®ç°ã€‚</li>
<li>ä¸ç°æœ‰å…ˆè¿›æŠ€æœ¯ç›¸æ¯”ï¼ŒLAVIDæ— éœ€é¢å¤–è®­ç»ƒæ£€æµ‹å™¨ï¼Œä»…é€šè¿‡LVLMæ¨ç†è¿›è¡Œæ£€æµ‹ã€‚</li>
<li>ä¸ºæ¨è¿›ç ”ç©¶ï¼Œå»ºç«‹äº†æ–°çš„é«˜è´¨é‡è§†é¢‘æ•°æ®é›†\vidforã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14994">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e27e3d0c13046590e56b1e316a0cf7ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f177f5b66f97aaa08bdc32155d82a917.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0867ae5a46a425511b016c546128b805.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8151f03c83e749d8435a4d911726b410.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Vision-Foundation-Models-in-Medical-Image-Analysis-Advances-and-Challenges"><a href="#Vision-Foundation-Models-in-Medical-Image-Analysis-Advances-and-Challenges" class="headerlink" title="Vision Foundation Models in Medical Image Analysis: Advances and   Challenges"></a>Vision Foundation Models in Medical Image Analysis: Advances and   Challenges</h2><p><strong>Authors:Pengchen Liang, Bin Pu, Haishan Huang, Yiwei Li, Hualiang Wang, Weibo Ma, Qing Chang</strong></p>
<p>The rapid development of Vision Foundation Models (VFMs), particularly Vision Transformers (ViT) and Segment Anything Model (SAM), has sparked significant advances in the field of medical image analysis. These models have demonstrated exceptional capabilities in capturing long-range dependencies and achieving high generalization in segmentation tasks. However, adapting these large models to medical image analysis presents several challenges, including domain differences between medical and natural images, the need for efficient model adaptation strategies, and the limitations of small-scale medical datasets. This paper reviews the state-of-the-art research on the adaptation of VFMs to medical image segmentation, focusing on the challenges of domain adaptation, model compression, and federated learning. We discuss the latest developments in adapter-based improvements, knowledge distillation techniques, and multi-scale contextual feature modeling, and propose future directions to overcome these bottlenecks. Our analysis highlights the potential of VFMs, along with emerging methodologies such as federated learning and model compression, to revolutionize medical image analysis and enhance clinical applications. The goal of this work is to provide a comprehensive overview of current approaches and suggest key areas for future research that can drive the next wave of innovation in medical image segmentation. </p>
<blockquote>
<p>è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œç‰¹åˆ«æ˜¯è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰å’Œä»»ä½•åˆ†å‰²æ¨¡å‹ï¼ˆSAMï¼‰ï¼Œå·²ç»æ¨åŠ¨äº†åŒ»å­¦å›¾åƒåˆ†æé¢†åŸŸçš„é‡å¤§è¿›æ­¥ã€‚è¿™äº›æ¨¡å‹åœ¨æ•æ‰é•¿è·ç¦»ä¾èµ–å…³ç³»å’Œå®ç°åˆ†å‰²ä»»åŠ¡çš„é«˜æ³›åŒ–æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå°†è¿™äº›å¤§å‹æ¨¡å‹é€‚åº”äºåŒ»å­¦å›¾åƒåˆ†æé¢ä¸´ç€ä¸€äº›æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬åŒ»å­¦å›¾åƒå’Œè‡ªç„¶å›¾åƒé¢†åŸŸä¹‹é—´çš„å·®å¼‚ã€å¯¹æœ‰æ•ˆçš„æ¨¡å‹é€‚åº”ç­–ç•¥çš„éœ€æ±‚ä»¥åŠå°å‹åŒ»å­¦æ•°æ®é›†çš„å±€é™æ€§ã€‚æœ¬æ–‡ç»¼è¿°äº†å°†VFMsé€‚åº”äºåŒ»å­¦å›¾åƒåˆ†å‰²çš„æœ€æ–°ç ”ç©¶ï¼Œé‡ç‚¹å…³æ³¨é¢†åŸŸé€‚åº”ã€æ¨¡å‹å‹ç¼©å’Œè”é‚¦å­¦ä¹ çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬è®¨è®ºäº†åŸºäºé€‚é…å™¨çš„æ”¹è¿›ã€çŸ¥è¯†è’¸é¦æŠ€æœ¯å’Œå¤šå°ºåº¦ä¸Šä¸‹æ–‡ç‰¹å¾å»ºæ¨¡çš„æœ€æ–°å‘å±•ï¼Œå¹¶æå‡ºäº†å…‹æœè¿™äº›ç“¶é¢ˆçš„æœªæ¥æ–¹å‘ã€‚æˆ‘ä»¬çš„åˆ†æå¼ºè°ƒäº†VFMsçš„æ½œåŠ›ï¼Œä»¥åŠä¸è”é‚¦å­¦ä¹ å’Œæ¨¡å‹å‹ç¼©ç­‰æ–°å…´æ–¹æ³•ç›¸ç»“åˆï¼Œæœ‰æœ›é©æ–°åŒ»å­¦å›¾åƒåˆ†æå¹¶å¢å¼ºä¸´åºŠåº”ç”¨ã€‚è¿™é¡¹å·¥ä½œçš„ç›®æ ‡æ˜¯æä¾›å½“å‰æ–¹æ³•çš„å…¨é¢æ¦‚è¿°ï¼Œå¹¶å»ºè®®æœªæ¥ç ”ç©¶çš„å…³é”®é¢†åŸŸï¼Œä»¥æ¨åŠ¨åŒ»å­¦å›¾åƒåˆ†å‰²é¢†åŸŸçš„åˆ›æ–°æµªæ½®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14584v2">PDF</a> 17 pages, 1 figure</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†Vision Foundation Modelsï¼ˆVFMsï¼‰åœ¨åŒ»å­¦å›¾åƒåˆ†æé¢†åŸŸçš„æœ€æ–°è¿›å±•ä¸æŒ‘æˆ˜ã€‚æ–‡ç« é‡ç‚¹ä»‹ç»äº†Vision Transformersï¼ˆViTï¼‰å’ŒSegment Anything Modelï¼ˆSAMï¼‰åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­çš„ä¼˜å¼‚è¡¨ç°ï¼Œå¹¶æŒ‡å‡ºäº†å°†è¿™äº›å¤§å‹æ¨¡å‹åº”ç”¨äºåŒ»å­¦å›¾åƒåˆ†ææ‰€é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚é¢†åŸŸå·®å¼‚ã€æ¨¡å‹é€‚é…ç­–ç•¥å’Œæœ‰é™çš„å°è§„æ¨¡åŒ»å­¦æ•°æ®é›†ç­‰ã€‚æœ¬æ–‡ç»¼è¿°äº†å½“å‰å°†VFMsé€‚é…äºåŒ»å­¦å›¾åƒåˆ†å‰²çš„æœ€æ–°ç ”ç©¶ï¼Œè®¨è®ºäº†é¢†åŸŸé€‚é…ã€æ¨¡å‹å‹ç¼©å’Œè”é‚¦å­¦ä¹ ç­‰æŒ‘æˆ˜ï¼Œå¹¶æ¢è®¨äº†æœªæ¥çš„ç ”ç©¶æ–¹å‘ã€‚æœ¬æ–‡å¼ºè°ƒäº†VFMsçš„æ½œåŠ›ï¼Œä»¥åŠè”é‚¦å­¦ä¹ å’Œæ¨¡å‹å‹ç¼©ç­‰æ–°å…´æ–¹æ³•ï¼Œè¿™äº›æœ‰æœ›æ¨åŠ¨åŒ»å­¦å›¾åƒåˆ†æçš„é©æ–°å¹¶æå‡ä¸´åºŠåº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision Foundation Models (VFMs) åœ¨åŒ»å­¦å›¾åƒåˆ†æé¢†åŸŸå–å¾—æ˜¾è‘—è¿›å±•ã€‚</li>
<li>Vision Transformers (ViT) å’Œ Segment Anything Model (SAM) åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>å°†å¤§å‹æ¨¡å‹åº”ç”¨äºåŒ»å­¦å›¾åƒåˆ†æé¢ä¸´é¢†åŸŸå·®å¼‚ã€æ¨¡å‹é€‚é…ç­–ç•¥å’Œæœ‰é™æ•°æ®é›†ç­‰æŒ‘æˆ˜ã€‚</li>
<li>é€‚é…å™¨æ”¹è¿›ã€çŸ¥è¯†è’¸é¦æŠ€æœ¯å’Œå¤šå°ºåº¦ä¸Šä¸‹æ–‡ç‰¹å¾å»ºæ¨¡æ˜¯å½“å‰çš„æœ€æ–°å‘å±•ã€‚</li>
<li>è”é‚¦å­¦ä¹ å’Œæ¨¡å‹å‹ç¼©ç­‰æ–°å…´æ–¹æ³•æœ‰æœ›æ¨åŠ¨åŒ»å­¦å›¾åƒåˆ†æçš„é©æ–°ã€‚</li>
<li>è¯¥ç»¼è¿°æä¾›äº†å¯¹å½“å‰ç ”ç©¶æ–¹æ³•çš„å…¨é¢æ¦‚è¿°ï¼Œä¸ºåŒ»å­¦å›¾åƒåˆ†å‰²çš„æœªæ¥ç ”ç©¶æä¾›äº†å…³é”®æ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14584">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f91b0fb8967365f4f3498eef6082c847.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-85fec34710fb4dc1733507b14f18ee30.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Fully-Automatic-Content-Aware-Tiling-Pipeline-for-Pathology-Whole-Slide-Images"><a href="#Fully-Automatic-Content-Aware-Tiling-Pipeline-for-Pathology-Whole-Slide-Images" class="headerlink" title="Fully Automatic Content-Aware Tiling Pipeline for Pathology Whole Slide   Images"></a>Fully Automatic Content-Aware Tiling Pipeline for Pathology Whole Slide   Images</h2><p><strong>Authors:Falah Jabar, Lill-Tove Rasmussen Busund, Biagio Ricciuti, Masoud Tafavvoghi, Mette PÃ¸hl, Sigve Andersen, Tom Donnem, David J. Kwiatkowski, Mehrdad Rakaee</strong></p>
<p>In recent years, the use of deep learning (DL) methods, including convolutional neural networks (CNNs) and vision transformers (ViTs), has significantly advanced computational pathology, enhancing both diagnostic accuracy and efficiency. Hematoxylin and Eosin (H&amp;E) Whole Slide Images (WSI) plays a crucial role by providing detailed tissue samples for the analysis and training of DL models. However, WSIs often contain regions with artifacts such as tissue folds, blurring, as well as non-tissue regions (background), which can negatively impact DL model performance. These artifacts are diagnostically irrelevant and can lead to inaccurate results. This paper proposes a fully automatic supervised DL pipeline for WSI Quality Assessment (WSI-QA) that uses a fused model combining CNNs and ViTs to detect and exclude WSI regions with artifacts, ensuring that only qualified WSI regions are used to build DL-based computational pathology applications. The proposed pipeline employs a pixel-based segmentation model to classify WSI regions as either qualified or non-qualified based on the presence of artifacts. The proposed model was trained on a large and diverse dataset and validated with internal and external data from various human organs, scanners, and H&amp;E staining procedures. Quantitative and qualitative evaluations demonstrate the superiority of the proposed model, which outperforms state-of-the-art methods in WSI artifact detection. The proposed model consistently achieved over 95% accuracy, precision, recall, and F1 score across all artifact types. Furthermore, the WSI-QA pipeline shows strong generalization across different tissue types and scanning conditions. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œæ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰æ–¹æ³•ï¼ŒåŒ…æ‹¬å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’Œè§†è§‰å˜å‹å™¨ï¼ˆViTï¼‰çš„åº”ç”¨ï¼Œåœ¨è®¡ç®—ç—…ç†å­¦é¢†åŸŸå–å¾—äº†é‡å¤§è¿›å±•ï¼Œæé«˜äº†è¯Šæ–­å’Œæ•ˆç‡ã€‚è‹æœ¨ç²¾å’Œä¼Šçº¢ï¼ˆH&amp;Eï¼‰å…¨å¹»ç¯ç‰‡å›¾åƒï¼ˆWSIï¼‰åœ¨æä¾›è¯¦ç»†çš„ç»„ç»‡æ ·æœ¬ä»¥ä¾›æ·±åº¦å­¦ä¹ æ¨¡å‹åˆ†æå’Œè®­ç»ƒæ–¹é¢å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ç„¶è€Œï¼ŒWSIç»å¸¸åŒ…å«å…·æœ‰ç»„ç»‡è¤¶çš±ã€æ¨¡ç³Šä»¥åŠéç»„ç»‡åŒºåŸŸï¼ˆèƒŒæ™¯ï¼‰ç­‰ä¼ªå½±åŒºåŸŸï¼Œè¿™äº›ä¼ªå½±ä¼šå¯¹æ·±åº¦å­¦ä¹ æ¨¡å‹çš„æ€§èƒ½äº§ç”Ÿè´Ÿé¢å½±å“ã€‚è¿™äº›ä¼ªå½±åœ¨è¯Šæ–­ä¸­æ— å…³ç´§è¦æ˜¯å¯¼è‡´ç»“æœä¸å‡†ç¡®çš„åŸå› ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å…¨è‡ªåŠ¨ç›‘ç£æ·±åº¦å­¦ä¹ ç®¡é“ï¼Œç”¨äºWSIè´¨é‡è¯„ä¼°ï¼ˆWSI-QAï¼‰ï¼Œè¯¥ç®¡é“ä½¿ç”¨èåˆæ¨¡å‹ç»“åˆCNNå’ŒViTæ¥æ£€æµ‹å’Œæ’é™¤å…·æœ‰ä¼ªå½±çš„WSIåŒºåŸŸï¼Œç¡®ä¿ä»…ä½¿ç”¨åˆæ ¼çš„WSIåŒºåŸŸæ¥æ„å»ºåŸºäºæ·±åº¦å­¦ä¹ çš„è®¡ç®—ç—…ç†å­¦åº”ç”¨ç¨‹åºã€‚è¯¥ç®¡é“é‡‡ç”¨åŸºäºåƒç´ çš„åˆ†å‰²æ¨¡å‹ï¼Œæ ¹æ®æ˜¯å¦å­˜åœ¨ä¼ªå½±å°†WSIåŒºåŸŸåˆ†ç±»ä¸ºåˆæ ¼æˆ–éåˆæ ¼ã€‚è¯¥æ¨¡å‹ç»è¿‡å¤§é‡ä¸”å¤šæ ·åŒ–çš„æ•°æ®é›†è®­ç»ƒï¼Œå¹¶ä½¿ç”¨æ¥è‡ªä¸åŒå™¨å®˜ã€æ‰«æä»ªå’ŒH&amp;EæŸ“è‰²ç¨‹åºçš„å†…éƒ¨å’Œå¤–éƒ¨æ•°æ®è¿›è¡ŒéªŒè¯ã€‚å®šé‡å’Œå®šæ€§è¯„ä¼°è¯æ˜äº†è¯¥æ¨¡å‹çš„ä¼˜è¶Šæ€§ï¼Œè¯¥æ¨¡å‹åœ¨WSIä¼ªå½±æ£€æµ‹æ–¹é¢ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚è¯¥æ¨¡å‹åœ¨æ‰€æœ‰ä¼ªå½±ç±»å‹ä¸Šçš„å‡†ç¡®ç‡ã€ç²¾ç¡®åº¦ã€å¬å›ç‡å’ŒF1åˆ†æ•°å‡è¾¾åˆ°è¶…è¿‡9Tçš„æ°´å¹³ã€‚æ­¤å¤–ï¼ŒWSI-QAç®¡é“åœ¨ä¸åŒç»„ç»‡ç±»å‹å’Œæ‰«ææ¡ä»¶ä¸‹è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.16885v2">PDF</a> Submitted to Medical Image Analysis journal, February 2025</p>
<p><strong>Summary</strong></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§å…¨è‡ªåŠ¨çš„åŸºäºæ·±åº¦å­¦ä¹ çš„ç—…ç†åˆ‡ç‰‡å›¾åƒè´¨é‡è¯„ä¼°ï¼ˆWSI-QAï¼‰ç®¡é“ï¼Œè¯¥ç®¡é“ç»“åˆäº†å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰å’Œè§†è§‰å˜å‹å™¨ï¼ˆViTsï¼‰çš„æ··åˆæ¨¡å‹ï¼Œç”¨äºæ£€æµ‹å’Œæ’é™¤å«æœ‰ä¼ªå½±çš„ç—…ç†åˆ‡ç‰‡å›¾åƒåŒºåŸŸã€‚è¯¥æ¨¡å‹ç¡®ä¿äº†åªæœ‰åˆæ ¼çš„åŒºåŸŸè¢«ç”¨äºå»ºç«‹åŸºäºæ·±åº¦å­¦ä¹ çš„è®¡ç®—ç—…ç†å­¦åº”ç”¨ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¨¡å‹åœ¨WSIä¼ªå½±æ£€æµ‹æ–¹é¢ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œå¹¶åœ¨å„ç§ä¼ªå½±ç±»å‹ä¸Šå®ç°äº†è¶…è¿‡95%çš„å‡†ç¡®æ€§ã€ç²¾ç¡®åº¦ã€å¬å›ç‡å’ŒF1åˆ†æ•°ã€‚è¯¥ç®¡é“åœ¨ä¸åŒç»„ç»‡ç±»å‹å’Œæ‰«ææ¡ä»¶ä¸‹è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨è®¡ç®—ç—…ç†å­¦é¢†åŸŸçš„åº”ç”¨å·²æ˜¾è‘—æé«˜è¯Šæ–­å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚</li>
<li>è®ºæ–‡æå‡ºäº†ä¸€ä¸ªå…¨è‡ªåŠ¨çš„WSIè´¨é‡è¯„ä¼°ï¼ˆWSI-QAï¼‰ç®¡é“ï¼Œç”¨äºæ£€æµ‹å’Œæ’é™¤å«æœ‰ä¼ªå½±çš„ç—…ç†åˆ‡ç‰‡å›¾åƒåŒºåŸŸã€‚</li>
<li>è¯¥ç®¡é“ç»“åˆäº†å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰å’Œè§†è§‰å˜å‹å™¨ï¼ˆViTsï¼‰çš„æ··åˆæ¨¡å‹ä»¥ç¡®ä¿ä»…ä½¿ç”¨åˆæ ¼çš„WSIåŒºåŸŸè¿›è¡Œè¯Šæ–­ã€‚</li>
<li>è¯¥æ¨¡å‹ç»è¿‡å¤§è§„æ¨¡å¤šæ ·åŒ–æ•°æ®é›†çš„è®­ç»ƒï¼Œå¹¶é€šè¿‡å†…å¤–éƒ¨æ•°æ®éªŒè¯ï¼Œå¯¹å„ç§å™¨å®˜ã€æ‰«æä»ªå’ŒæŸ“è‰²ç¨‹åºå…·æœ‰è‰¯å¥½çš„é€‚ç”¨æ€§ã€‚</li>
<li>å®éªŒè¯æ˜è¯¥æ¨¡å‹åœ¨WSIä¼ªå½±æ£€æµ‹æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œå‡†ç¡®ç‡é«˜ï¼Œè¾¾åˆ°æˆ–è¶…è¿‡95%ã€‚</li>
<li>æ¨¡å‹å¯¹å„ç§ç»„ç»‡ç±»å‹å’Œæ‰«ææ¡ä»¶å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.16885">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d3aa43c9af7ab22c9dc495d7d0fccec4.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="WebCode2M-A-Real-World-Dataset-for-Code-Generation-from-Webpage-Designs"><a href="#WebCode2M-A-Real-World-Dataset-for-Code-Generation-from-Webpage-Designs" class="headerlink" title="WebCode2M: A Real-World Dataset for Code Generation from Webpage Designs"></a>WebCode2M: A Real-World Dataset for Code Generation from Webpage Designs</h2><p><strong>Authors:Yi Gui, Zhen Li, Yao Wan, Yemin Shi, Hongyu Zhang, Yi Su, Bohua Chen, Dongping Chen, Siyuan Wu, Xing Zhou, Wenbin Jiang, Hai Jin, Xiangliang Zhang</strong></p>
<p>Automatically generating webpage code from webpage designs can significantly reduce the workload of front-end developers, and recent Multimodal Large Language Models (MLLMs) have shown promising potential in this area. However, our investigation reveals that most existing MLLMs are constrained by the absence of high-quality, large-scale, real-world datasets, resulting in inadequate performance in automated webpage code generation. To fill this gap, this paper introduces WebCode2M, a new dataset comprising 2.56 million instances, each containing a design image along with the corresponding webpage code and layout details. Sourced from real-world web resources, WebCode2M offers a rich and valuable dataset for webpage code generation across a variety of applications. The dataset quality is ensured by a scoring model that filters out instances with aesthetic deficiencies or other incomplete elements. To validate the effectiveness of WebCode2M, we introduce a baseline model based on the Vision Transformer (ViT), named WebCoder, and establish a benchmark for fair comparison. Additionally, we introduce a new metric, TreeBLEU, to measure the structural hierarchy recall. The benchmarking results demonstrate that our dataset significantly improves the ability of MLLMs to generate code from webpage designs, confirming its effectiveness and usability for future applications in front-end design tools. Finally, we highlight several practical challenges introduced by our dataset, calling for further research. The code and dataset are publicly available at our project homepage: <a target="_blank" rel="noopener" href="https://webcode2m.github.io/">https://webcode2m.github.io</a>. </p>
<blockquote>
<p>è‡ªåŠ¨æ ¹æ®ç½‘é¡µè®¾è®¡ç”Ÿæˆç½‘é¡µä»£ç èƒ½å¤Ÿæå¤§åœ°å‡è½»å‰ç«¯å¼€å‘äººå‘˜çš„å·¥ä½œé‡ï¼Œè€Œæœ€è¿‘çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è¿™ä¸€é¢†åŸŸå±•ç°å‡ºäº†å·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬çš„è°ƒæŸ¥å‘ç°ï¼Œå¤§å¤šæ•°ç°æœ‰çš„MLLMså—é™äºç¼ºä¹é«˜è´¨é‡ã€å¤§è§„æ¨¡ã€çœŸå®ä¸–ç•Œçš„æ•°æ®é›†ï¼Œå¯¼è‡´åœ¨è‡ªåŠ¨ç½‘é¡µä»£ç ç”Ÿæˆæ–¹é¢çš„æ€§èƒ½ä¸è¶³ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæœ¬æ–‡ä»‹ç»äº†WebCode2Mæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«256ä¸‡ä¸ªå®ä¾‹çš„æ–°æ•°æ®é›†ï¼Œæ¯ä¸ªå®ä¾‹éƒ½åŒ…å«è®¾è®¡å›¾åƒä»¥åŠç›¸åº”çš„ç½‘é¡µä»£ç å’Œå¸ƒå±€ç»†èŠ‚ã€‚æºäºçœŸå®ä¸–ç•Œç½‘ç»œèµ„æºï¼ŒWebCode2Mä¸ºå„ç§åº”ç”¨çš„ç½‘é¡µä»£ç ç”Ÿæˆæä¾›äº†ä¸€ä¸ªä¸°å¯Œè€Œå®è´µçš„æ•°æ®é›†ã€‚æ•°æ®é›†çš„è´¨é‡é€šè¿‡è¯„åˆ†æ¨¡å‹æ¥ä¿è¯ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿç­›é€‰å‡ºå…·æœ‰ç¾å­¦ç¼ºé™·æˆ–å…¶ä»–ä¸å®Œæ•´å…ƒç´ çš„å®ä¾‹ã€‚ä¸ºäº†éªŒè¯WebCode2Mçš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬åŸºäºè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰å¼•å…¥äº†ä¸€ä¸ªåä¸ºWebCoderçš„åŸºçº¿æ¨¡å‹ï¼Œå¹¶å»ºç«‹äº†ä¸€ä¸ªå…¬å¹³çš„åŸºå‡†æµ‹è¯•å¹³å°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªæ–°çš„æŒ‡æ ‡â€”â€”TreeBLEUï¼Œæ¥è¡¡é‡ç»“æ„å±‚æ¬¡å¬å›ç‡ã€‚åŸºå‡†æµ‹è¯•ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ•°æ®é›†æ˜¾è‘—æé«˜äº†MLLMsä»ç½‘é¡µè®¾è®¡ç”Ÿæˆä»£ç çš„èƒ½åŠ›ï¼Œè¯å®äº†å…¶åœ¨å‰ç«¯è®¾è®¡å·¥å…·æœªæ¥åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬å¼ºè°ƒäº†æˆ‘ä»¬æ•°æ®é›†æ‰€å¸¦æ¥çš„å‡ ä¸ªå®é™…æŒ‘æˆ˜ï¼Œå‘¼åè¿›ä¸€æ­¥çš„ç ”ç©¶ã€‚ä»£ç å’Œæ•°æ®é›†å¯åœ¨æˆ‘ä»¬çš„é¡¹ç›®ä¸»é¡µå…¬å¼€è·å–ï¼š<a target="_blank" rel="noopener" href="https://webcode2m.github.io./">https://webcode2m.github.ioã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.06369v2">PDF</a> WWWâ€™25</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€é¡¹æ–°æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯é€šè¿‡å¼•å…¥WebCode2Mæ•°æ®é›†ï¼Œæ˜¾è‘—æé«˜äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªåŠ¨ç½‘é¡µä»£ç ç”Ÿæˆæ–¹é¢çš„æ€§èƒ½ã€‚è¯¥æ•°æ®é›†åŒ…å«çœŸå®çš„ç½‘é¡µè®¾è®¡å’Œä»£ç ä¿¡æ¯ï¼Œæœ‰æ•ˆåœ°è§£å†³äº†ä¹‹å‰æ¨¡å‹çš„å±€é™æ€§é—®é¢˜ã€‚ä¸ºéªŒè¯æ•°æ®é›†çš„å®ç”¨æ€§ï¼Œæœ¬ç ”ç©¶åŸºäºè§†è§‰è½¬æ¢å™¨æå‡ºäº†ä¸€ä¸ªåä¸ºWebCoderçš„åŸºçº¿æ¨¡å‹ï¼Œå¹¶å¼•å…¥äº†æ–°çš„åº¦é‡æ ‡å‡†TreeBLEUæ¥è¡¡é‡ç”Ÿæˆä»£ç çš„ç»“æ„å±‚æ¬¡å¬å›ç‡ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒWebCode2Mæ•°æ®é›†æ˜¾è‘—æé«˜äº†MLLMsä»ç½‘é¡µè®¾è®¡ä¸­ç”Ÿæˆä»£ç çš„èƒ½åŠ›ï¼Œå¹¶å¼ºè°ƒäº†è¯¥æ•°æ®é›†åœ¨å®é™…åº”ç”¨ä¸­é¢ä¸´çš„æŒ‘æˆ˜ã€‚æ•°æ®é›†å’Œä»£ç å·²å…¬å¼€å‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨ç”Ÿæˆç½‘é¡µä»£ç å¯æ˜¾è‘—å‡å°‘å‰ç«¯å¼€å‘äººå‘˜çš„å·¥ä½œé‡ã€‚</li>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªåŠ¨åŒ–ç½‘é¡µä»£ç ç”Ÿæˆæ–¹é¢å±•ç°æ½œåŠ›ã€‚</li>
<li>ç¼ºä¹é«˜è´¨é‡çš„å¤§è§„æ¨¡æ•°æ®é›†æ˜¯ç°æœ‰æ¨¡å‹é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥WebCode2Mæ•°æ®é›†ï¼ŒåŒ…å«çœŸå®ä¸–ç•Œç½‘é¡µè®¾è®¡ä¸å…¶å¯¹åº”çš„ä»£ç å’Œå¸ƒå±€ç»†èŠ‚ã€‚</li>
<li>é‡‡ç”¨è¯„åˆ†æ¨¡å‹ç¡®ä¿æ•°æ®é›†è´¨é‡ã€‚</li>
<li>åŸºäºè§†è§‰è½¬æ¢å™¨æå‡ºåŸºçº¿æ¨¡å‹WebCoderå¹¶å»ºç«‹åŸºå‡†æµ‹è¯•æ ‡å‡†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.06369">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9942a0b5227a6885023587fbc180aa1d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ed22c5ac115a2c45408767fd02cf0b97.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-505fa8ed1208594a60340b4d4b472c06.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-053321ad0d292075ed1df094e73143dd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0ef869b2f86a0fd19ec61e6a20c3eda9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-af681aa134a102871d882de72770c91c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-74465b65dd6b6a9a21784e88aa178630.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2eadf814d4781f9a168214f093fe7787.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Which-Transformer-to-Favor-A-Comparative-Analysis-of-Efficiency-in-Vision-Transformers"><a href="#Which-Transformer-to-Favor-A-Comparative-Analysis-of-Efficiency-in-Vision-Transformers" class="headerlink" title="Which Transformer to Favor: A Comparative Analysis of Efficiency in   Vision Transformers"></a>Which Transformer to Favor: A Comparative Analysis of Efficiency in   Vision Transformers</h2><p><strong>Authors:Tobias Christian Nauen, Sebastian Palacio, Federico Raue, Andreas Dengel</strong></p>
<p>Self-attention in Transformers comes with a high computational cost because of their quadratic computational complexity, but their effectiveness in addressing problems in language and vision has sparked extensive research aimed at enhancing their efficiency. However, diverse experimental conditions, spanning multiple input domains, prevent a fair comparison based solely on reported results, posing challenges for model selection. To address this gap in comparability, we perform a large-scale benchmark of more than 45 models for image classification, evaluating key efficiency aspects, including accuracy, speed, and memory usage. Our benchmark provides a standardized baseline for efficiency-oriented transformers. We analyze the results based on the Pareto front â€“ the boundary of optimal models. Surprisingly, despite claims of other models being more efficient, ViT remains Pareto optimal across multiple metrics. We observe that hybrid attention-CNN models exhibit remarkable inference memory- and parameter-efficiency. Moreover, our benchmark shows that using a larger model in general is more efficient than using higher resolution images. Thanks to our holistic evaluation, we provide a centralized resource for practitioners and researchers, facilitating informed decisions when selecting or developing efficient transformers. </p>
<blockquote>
<p>Transformerä¸­çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ç”±äºå…¶äºŒæ¬¡æ–¹çš„è®¡ç®—å¤æ‚åº¦è€Œä¼´éšç€è¾ƒé«˜çš„è®¡ç®—æˆæœ¬ï¼Œä½†å…¶åœ¨è¯­è¨€å’Œè§†è§‰é—®é¢˜å¤„ç†ä¸­çš„æœ‰æ•ˆæ€§å¼•å‘äº†å¤§é‡æ—¨åœ¨æé«˜å…¶æ•ˆç‡çš„ç ”ç©¶ã€‚ç„¶è€Œï¼Œè·¨è¶Šå¤šä¸ªè¾“å…¥é¢†åŸŸçš„ä¸åŒå®éªŒæ¡ä»¶ä»…åŸºäºæŠ¥å‘Šç»“æœè€Œæ— æ³•è¿›è¡Œå…¬å¹³æ¯”è¾ƒï¼Œä¸ºæ¨¡å‹é€‰æ‹©å¸¦æ¥äº†æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³å¯æ¯”æ€§æ–¹é¢çš„è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¯¹è¶…è¿‡42ä¸ªæ¨¡å‹è¿›è¡Œäº†å¤§è§„æ¨¡å›¾åƒåˆ†ç±»åŸºå‡†æµ‹è¯•ï¼Œè¯„ä¼°äº†å‡†ç¡®æ€§ã€é€Ÿåº¦å’Œå†…å­˜ä½¿ç”¨ç­‰å…³é”®æ•ˆç‡æ–¹é¢ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•æä¾›äº†ä¸€ä¸ªé¢å‘æ•ˆç‡çš„æ ‡å‡†åŒ–åŸºçº¿ã€‚æˆ‘ä»¬æ ¹æ®å¸•ç´¯æ‰˜å‰æ²¿ï¼ˆæœ€ä¼˜æ¨¡å‹çš„è¾¹ç•Œï¼‰åˆ†æç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡å…¶ä»–æ¨¡å‹å£°ç§°æ•ˆç‡æ›´é«˜ï¼Œä½†ViTåœ¨å¤šæŒ‡æ ‡æ–¹é¢ä»æ˜¯å¸•ç´¯æ‰˜æœ€ä¼˜ã€‚æˆ‘ä»¬å‘ç°æ··åˆæ³¨æ„åŠ›CNNæ¨¡å‹åœ¨æ¨ç†å†…å­˜å’Œå‚æ•°æ•ˆç‡æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•æ˜¾ç¤ºï¼Œåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹æ¯”ä½¿ç”¨æ›´é«˜åˆ†è¾¨ç‡çš„å›¾åƒæ›´ä¸ºé«˜æ•ˆã€‚å¾—ç›Šäºæˆ‘ä»¬çš„å…¨é¢è¯„ä¼°ï¼Œæˆ‘ä»¬ä¸ºä»ä¸šè€…å’Œç ”ç©¶äººå‘˜æä¾›äº†ä¸€ä¸ªé›†ä¸­èµ„æºï¼Œåœ¨é€‰æ‹©æˆ–å¼€å‘é«˜æ•ˆå˜å‹å™¨æ—¶åšå‡ºæ˜æ™ºå†³ç­–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09372v4">PDF</a> v3: new models, analysis of scaling behaviors; v4: WACV 2025 camera   ready version, appendix added</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†Transformerè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„è®¡ç®—æˆæœ¬é—®é¢˜ï¼ŒåŠå…¶åœ¨ä¸åŒè¾“å…¥åŸŸä¸‹çš„æ•ˆç‡é—®é¢˜ã€‚ä¸ºå¼¥è¡¥æ¨¡å‹é€‰æ‹©çš„æ¯”è¾ƒç¼ºå¤±ï¼Œå¯¹è¶…è¿‡45ç§æ¨¡å‹è¿›è¡Œäº†å¤§è§„æ¨¡å›¾åƒåˆ†ç±»åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–äº†å‡†ç¡®åº¦ã€é€Ÿåº¦å’Œå†…å­˜ä½¿ç”¨ç­‰æ–¹é¢çš„è¯„ä¼°ã€‚é€šè¿‡å¸•ç´¯æ‰˜æœ€ä¼˜æ¨¡å‹åˆ†æï¼Œå‘ç°å°½ç®¡æœ‰å…¶ä»–æ¨¡å‹å£°ç§°æ›´é«˜æ•ˆï¼Œä½†ViTåœ¨å¤šæŒ‡æ ‡ä¸Šä»æ˜¯å¸•ç´¯æ‰˜æœ€ä¼˜ã€‚æ­¤å¤–ï¼Œè¿˜å‘ç°æ··åˆæ³¨æ„åŠ›ä¸CNNæ¨¡å‹å±•ç°å‡ºå“è¶Šçš„æ¨ç†å†…å­˜å’Œå‚æ•°æ•ˆç‡ï¼Œä½¿ç”¨æ›´å¤§çš„æ¨¡å‹é€šå¸¸æ¯”æé«˜å›¾åƒåˆ†è¾¨ç‡æ›´æœ‰æ•ˆç‡ã€‚æœ¬æ–‡çš„ç»¼åˆè¯„ä¼°ä¸ºä»ä¸šè€…å’Œç ”ç©¶äººå‘˜æä¾›äº†é›†ä¸­èµ„æºï¼Œä¾¿äºé€‰æ‹©æˆ–å¼€å‘é«˜æ•ˆTransformeræ—¶çš„å†³ç­–å‚è€ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformerçš„è‡ªæ³¨æ„åŠ›æœºåˆ¶å­˜åœ¨é«˜è®¡ç®—æˆæœ¬é—®é¢˜ï¼Œç ”ç©¶æ—¨åœ¨æé«˜å…¶æ•ˆç‡ã€‚</li>
<li>å¯¹è¶…è¿‡45ç§æ¨¡å‹è¿›è¡Œå¤§è§„æ¨¡åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–å›¾åƒåˆ†ç±»ï¼Œè¯„ä¼°å…³é”®æ•ˆç‡æ–¹é¢å¦‚å‡†ç¡®åº¦ã€é€Ÿåº¦å’Œå†…å­˜ä½¿ç”¨ã€‚</li>
<li>é€šè¿‡å¸•ç´¯æ‰˜æœ€ä¼˜åˆ†æï¼Œå‘ç°ViTæ¨¡å‹åœ¨å¤šæŒ‡æ ‡ä¸Šè¡¨ç°æœ€ä½³ã€‚</li>
<li>æ··åˆæ³¨æ„åŠ›ä¸CNNæ¨¡å‹å±•ç°å‡ºå“è¶Šçš„æ¨ç†å†…å­˜å’Œå‚æ•°æ•ˆç‡ã€‚</li>
<li>ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹é€šå¸¸æ¯”æé«˜å›¾åƒåˆ†è¾¨ç‡æ›´æœ‰æ•ˆç‡ã€‚</li>
<li>åŸºå‡†æµ‹è¯•ä¸ºä»ä¸šè€…å’Œç ”ç©¶äººå‘˜æä¾›äº†å†³ç­–å‚è€ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2308.09372">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9b9b06c8598e3ec737fe90506bfb5a5b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-91b390aa9d7b33e0e879e8f413e94881.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ce0aaf8ac953cc4684d7f2415da248bb.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-26/Vision%20Transformer/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-26/Vision%20Transformer/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Vision-Transformer/">
                                    <span class="chip bg-color">Vision Transformer</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-26/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-0207a0d308dfcb55e89c9b20177bfb54.jpg" class="responsive-img" alt="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
                        
                        <span class="card-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-26  CRTrack Low-Light Semi-Supervised Multi-object Tracking Based on   Consistency Regularization
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-26
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-26/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-65f8a60d6909538050257ca6d5ff7700.jpg" class="responsive-img" alt="è§†é¢‘ç†è§£">
                        
                        <span class="card-title">è§†é¢‘ç†è§£</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-26  Understanding Long Videos with Multimodal Language Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-26
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    è§†é¢‘ç†è§£
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">27083.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
