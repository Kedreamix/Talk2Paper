<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-26  MDN Mamba-Driven Dualstream Network For Medical Hyperspectral Image   Segmentation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-864857c43fe0a6a419d2932424a24711.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-26
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    22k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    92 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-02-26-æ›´æ–°"><a href="#2025-02-26-æ›´æ–°" class="headerlink" title="2025-02-26 æ›´æ–°"></a>2025-02-26 æ›´æ–°</h1><h2 id="MDN-Mamba-Driven-Dualstream-Network-For-Medical-Hyperspectral-Image-Segmentation"><a href="#MDN-Mamba-Driven-Dualstream-Network-For-Medical-Hyperspectral-Image-Segmentation" class="headerlink" title="MDN: Mamba-Driven Dualstream Network For Medical Hyperspectral Image   Segmentation"></a>MDN: Mamba-Driven Dualstream Network For Medical Hyperspectral Image   Segmentation</h2><p><strong>Authors:Shijie Lin, Boxiang Yun, Wei Shen, Qingli Li, Anqiang Yang, Yan Wang</strong></p>
<p>Medical Hyperspectral Imaging (MHSI) offers potential for computational pathology and precision medicine. However, existing CNN and Transformer struggle to balance segmentation accuracy and speed due to high spatial-spectral dimensionality. In this study, we leverage Mambaâ€™s global context modeling to propose a dual-stream architecture for joint spatial-spectral feature extraction. To address the limitation of Mambaâ€™s unidirectional aggregation, we introduce a recurrent spectral sequence representation to capture low-redundancy global spectral features. Experiments on a public Multi-Dimensional Choledoch dataset and a private Cervical Cancer dataset show that our method outperforms state-of-the-art approaches in segmentation accuracy while minimizing resource usage and achieving the fastest inference speed. Our code will be available at <a target="_blank" rel="noopener" href="https://github.com/DeepMed-Lab-ECNU/MDN">https://github.com/DeepMed-Lab-ECNU/MDN</a>. </p>
<blockquote>
<p>åŒ»å­¦é«˜å…‰è°±æˆåƒï¼ˆMHSIï¼‰ä¸ºè®¡ç®—ç—…ç†å­¦å’Œç²¾å‡†åŒ»å­¦æä¾›äº†æ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„CNNå’ŒTransformerç”±äºé«˜ç©ºé—´å…‰è°±ç»´åº¦ï¼Œåœ¨å¹³è¡¡åˆ†å‰²ç²¾åº¦å’Œé€Ÿåº¦æ–¹é¢å­˜åœ¨å›°éš¾ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨Mambaçš„å…¨å±€ä¸Šä¸‹æ–‡å»ºæ¨¡ï¼Œæå‡ºäº†ä¸€ç§ç”¨äºè”åˆç©ºé—´å…‰è°±ç‰¹å¾æå–çš„åŒæµæ¶æ„ã€‚ä¸ºäº†è§£å†³Mambaå•å‘èšåˆçš„å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥é€’å½’å…‰è°±åºåˆ—è¡¨ç¤ºï¼Œä»¥æ•è·ä½å†—ä½™å…¨å±€å…‰è°±ç‰¹å¾ã€‚åœ¨å…¬å¼€çš„Multi-Dimensional Choledochæ•°æ®é›†å’Œç§æœ‰å®«é¢ˆç™Œæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨åˆ†å‰²ç²¾åº¦ä¸Šä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼ŒåŒæ—¶æœ€å°åŒ–èµ„æºä½¿ç”¨ï¼Œå®ç°æœ€å¿«çš„æ¨ç†é€Ÿåº¦ã€‚æˆ‘ä»¬çš„ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/DeepMed-Lab-ECNU/MDN%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/DeepMed-Lab-ECNU/MDNä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17255v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŒ»å­¦è¶…å…‰è°±æˆåƒï¼ˆMHSIï¼‰åœ¨è®¡ç®—ç—…ç†å­¦å’Œç²¾å‡†åŒ»å­¦ä¸­å…·æœ‰æ½œåŠ›ã€‚ç„¶è€Œï¼Œç”±äºé«˜ç©ºé—´å…‰è°±ç»´æ•°ï¼Œç°æœ‰CNNå’ŒTransformeråœ¨åˆ†å‰²ç²¾åº¦å’Œé€Ÿåº¦ä¹‹é—´éš¾ä»¥å¹³è¡¡ã€‚æœ¬ç ”ç©¶åˆ©ç”¨Mambaçš„å…¨å±€ä¸Šä¸‹æ–‡å»ºæ¨¡ï¼Œæå‡ºä¸€ç§ç”¨äºè”åˆç©ºé—´å…‰è°±ç‰¹å¾æå–çš„åŒæµæ¶æ„ã€‚ä¸ºè§£å†³Mambaå•å‘èšåˆçš„å±€é™æ€§ï¼Œå¼•å…¥å¾ªç¯å…‰è°±åºåˆ—è¡¨ç¤ºæ³•ï¼Œä»¥æ•è·ä½å†—ä½™å…¨å±€å…‰è°±ç‰¹å¾ã€‚åœ¨å…¬å¼€çš„å¤šç»´èƒ†ç®¡æ•°æ®é›†å’Œç§æœ‰å®«é¢ˆç™Œæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨åˆ†å‰²ç²¾åº¦ä¸Šä¼˜äºæœ€æ–°æ–¹æ³•ï¼ŒåŒæ—¶æœ€å°åŒ–èµ„æºä½¿ç”¨ï¼Œå®ç°æœ€å¿«çš„æ¨ç†é€Ÿåº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦è¶…å…‰è°±æˆåƒï¼ˆMHSIï¼‰åœ¨è®¡ç®—ç—…ç†å­¦å’Œç²¾å‡†åŒ»å­¦ä¸­æœ‰å¹¿æ³›åº”ç”¨æ½œåŠ›ã€‚</li>
<li>ç°æœ‰æŠ€æœ¯å¦‚CNNå’ŒTransformeråœ¨å¹³è¡¡åˆ†å‰²ç²¾åº¦å’Œé€Ÿåº¦æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>ç ”ç©¶åˆ©ç”¨Mambaçš„å…¨å±€ä¸Šä¸‹æ–‡å»ºæ¨¡ï¼Œæå‡ºåŒæµæ¶æ„è¿›è¡Œç©ºé—´å…‰è°±ç‰¹å¾æå–ã€‚</li>
<li>å¼•å…¥å¾ªç¯å…‰è°±åºåˆ—è¡¨ç¤ºæ³•ï¼Œè§£å†³Mambaå•å‘èšåˆçš„å±€é™æ€§ã€‚</li>
<li>æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨ç°ä¼˜è¶Šï¼ŒåŒ…æ‹¬å…¬å¼€çš„å¤šç»´èƒ†ç®¡æ•°æ®é›†å’Œç§æœ‰å®«é¢ˆç™Œæ•°æ®é›†ã€‚</li>
<li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨åˆ†å‰²ç²¾åº¦ä¸Šæœ‰æ‰€æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17255">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-c1633270afaa8fe9bb773228663ad6df.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-54802d2b2c1a875fd565eeb9baae5f69.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-720130cc239837af8fb5b7a85b32c117.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-281f9fb3bed22406bb53f20afcc1a67f.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Motion-Robust-T2-Quantification-from-Gradient-Echo-MRI-with-Physics-Informed-Deep-Learning"><a href="#Motion-Robust-T2-Quantification-from-Gradient-Echo-MRI-with-Physics-Informed-Deep-Learning" class="headerlink" title="Motion-Robust T2* Quantification from Gradient Echo MRI with   Physics-Informed Deep Learning"></a>Motion-Robust T2* Quantification from Gradient Echo MRI with   Physics-Informed Deep Learning</h2><p><strong>Authors:Hannah Eichhorn, Veronika Spieker, Kerstin Hammernik, Elisa Saks, Lina Felsner, Kilian Weiss, Christine Preibisch, Julia A. Schnabel</strong></p>
<p>Purpose: T2* quantification from gradient echo magnetic resonance imaging is particularly affected by subject motion due to the high sensitivity to magnetic field inhomogeneities, which are influenced by motion and might cause signal loss. Thus, motion correction is crucial to obtain high-quality T2* maps.   Methods: We extend our previously introduced learning-based physics-informed motion correction method, PHIMO, by utilizing acquisition knowledge to enhance the reconstruction performance for challenging motion patterns and increase PHIMOâ€™s robustness to varying strengths of magnetic field inhomogeneities across the brain. We perform comprehensive evaluations regarding motion detection accuracy and image quality for data with simulated and real motion.   Results: Our extended version of PHIMO outperforms the learning-based baseline methods both qualitatively and quantitatively with respect to line detection and image quality. Moreover, PHIMO performs on-par with a conventional state-of-the-art motion correction method for T2* quantification from gradient echo MRI, which relies on redundant data acquisition.   Conclusion: PHIMOâ€™s competitive motion correction performance, combined with a reduction in acquisition time by over 40% compared to the state-of-the-art method, make it a promising solution for motion-robust T2* quantification in research settings and clinical routine. </p>
<blockquote>
<p>ç›®çš„ï¼šä»æ¢¯åº¦å›æ³¢ç£å…±æŒ¯æˆåƒè·å–çš„T2<em>å®šé‡å€¼ç‰¹åˆ«å—åˆ°å—è¯•è€…è¿åŠ¨çš„å½±å“ï¼Œå› ä¸ºå…¶å¯¹ç£åœºä¸å‡åŒ€æ€§å…·æœ‰è¾ƒé«˜çš„æ•æ„Ÿæ€§ï¼Œè¿åŠ¨ä¼šå½±å“ç£åœºä¸å‡åŒ€æ€§ï¼Œå¹¶å¯èƒ½å¯¼è‡´ä¿¡å·ä¸¢å¤±ã€‚å› æ­¤ï¼Œè¿åŠ¨æ ¡æ­£å¯¹äºè·å¾—é«˜è´¨é‡çš„T2</em>å›¾æ˜¯è‡³å…³é‡è¦çš„ã€‚æ–¹æ³•ï¼šæˆ‘ä»¬æ‰©å±•äº†æˆ‘ä»¬ä¹‹å‰å¼•å…¥çš„å­¦ä¹ å‹ç‰©ç†ä¿¡æ¯è¿åŠ¨æ ¡æ­£æ–¹æ³•PHIMOï¼Œåˆ©ç”¨é‡‡é›†çŸ¥è¯†æ¥æé«˜å¯¹å¤æ‚è¿åŠ¨æ¨¡å¼çš„é‡å»ºæ€§èƒ½ï¼Œå¹¶å¢å¼ºPHIMOå¯¹ä¸åŒå¼ºåº¦ç£åœºä¸å‡åŒ€æ€§çš„ç¨³å¥æ€§ã€‚æˆ‘ä»¬å¯¹æ¨¡æ‹Ÿå’Œå®é™…è¿åŠ¨çš„æ•°æ®è¿›è¡Œäº†å…¨é¢çš„è¿åŠ¨æ£€æµ‹å‡†ç¡®æ€§å’Œå›¾åƒè´¨é‡è¯„ä¼°ã€‚ç»“æœï¼šæˆ‘ä»¬çš„PHIMOæ‰©å±•ç‰ˆæœ¬åœ¨å®šæ€§å’Œå®šé‡ä¸Šå‡ä¼˜äºåŸºäºå­¦ä¹ çš„åŸºç¡€æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨çº¿æ¡æ£€æµ‹å’Œå›¾åƒè´¨é‡æ–¹é¢ã€‚æ­¤å¤–ï¼ŒPHIMOä¸æ¢¯åº¦å›æ³¢MRIçš„T2<em>å®šé‡è¯„ä¼°ä¸­çš„å¸¸è§„å…ˆè¿›è¿åŠ¨æ ¡æ­£æ–¹æ³•çš„æ€§èƒ½ç›¸å½“ï¼Œè¯¥æ–¹æ³•çš„è¿åŠ¨æ ¡æ­£ä¾èµ–äºå†—ä½™æ•°æ®é‡‡é›†ã€‚ç»“è®ºï¼šPHIMOçš„ç«äº‰æ€§è¿åŠ¨æ ¡æ­£æ€§èƒ½ä¸ç›¸æ¯”ç°æœ‰æŠ€æœ¯ç¼©çŸ­è¶…è¿‡40%çš„é‡‡é›†æ—¶é—´ç›¸ç»“åˆï¼Œä½¿å…¶æˆä¸ºç ”ç©¶å’Œä¸´åºŠå®è·µä¸­ç¨³å¥çš„T2</em>å®šé‡è¯„ä¼°çš„æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17209v1">PDF</a> Under Review</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ—¨åœ¨è§£å†³æ¢¯åº¦å›æ³¢ç£å…±æŒ¯æˆåƒä¸­T2<em>é‡åŒ–å—ä¸»ä½“è¿åŠ¨å½±å“çš„é—®é¢˜ã€‚ä¸ºæé«˜å¯¹è¿åŠ¨æ¨¡å¼çš„é‡å»ºæ€§èƒ½å¹¶å¢å¼ºPHIMOå¯¹ç£åœºä¸å‡åŒ€æ€§çš„ç¨³å¥æ€§ï¼Œä½œè€…å¯¹åŸºäºå­¦ä¹ çš„ç‰©ç†ä¿¡æ¯è¿åŠ¨æ ¡æ­£æ–¹æ³•PHIMOè¿›è¡Œäº†æ‰©å±•ã€‚ç»è¿‡å¯¹æ¨¡æ‹Ÿå’Œå®é™…è¿åŠ¨æ•°æ®çš„å…¨é¢è¯„ä¼°ï¼Œå‘ç°æ‰©å±•åçš„PHIMOåœ¨ç›´çº¿æ£€æµ‹å’Œå›¾åƒè´¨é‡æ–¹é¢å‡ä¼˜äºåŸºäºå­¦ä¹ çš„åŸºç¡€æ–¹æ³•ï¼Œå¹¶ä¸ä¼ ç»Ÿçš„T2</em>æ¢¯åº¦å›æ³¢MRIè¿åŠ¨æ ¡æ­£æ–¹æ³•ç›¸å½“ã€‚æ­¤å¤–ï¼ŒPHIMOå°†é‡‡é›†æ—¶é—´å‡å°‘äº†40%ä»¥ä¸Šï¼Œæˆä¸ºç ”ç©¶å’Œä¸´åºŠå¸¸è§„ä¸­è¿åŠ¨ç¨³å¥çš„T2*é‡åŒ–çš„æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>T2*é‡åŒ–åœ¨æ¢¯åº¦å›æ³¢ç£å…±æŒ¯æˆåƒä¸­æ˜“å—ä¸»ä½“è¿åŠ¨å½±å“ï¼Œå¯¼è‡´ç£åœºä¸å‡åŒ€æ€§å’Œä¿¡å·æŸå¤±ã€‚</li>
<li>PHIMOæ˜¯ä¸€ç§åŸºäºå­¦ä¹ å’Œç‰©ç†ä¿¡æ¯çš„è¿åŠ¨æ ¡æ­£æ–¹æ³•ï¼Œé’ˆå¯¹ç£åœºä¸å‡åŒ€æ€§çš„ä¸åŒå¼ºåº¦è¿›è¡Œäº†ä¼˜åŒ–ã€‚</li>
<li>æ‰©å±•åçš„PHIMOåœ¨æ¨¡æ‹Ÿå’Œå®é™…è¿åŠ¨æ•°æ®çš„æµ‹è¯•ä¸­è¡¨ç°å‡ºæ›´é«˜çš„æ€§èƒ½ï¼Œæé«˜äº†è¿åŠ¨æ£€æµ‹å‡†ç¡®æ€§å’Œå›¾åƒè´¨é‡ã€‚</li>
<li>PHIMOä¸ç°æœ‰çš„å…ˆè¿›è¿åŠ¨æ ¡æ­£æ–¹æ³•ç›¸æ¯”ï¼Œåœ¨T2*é‡åŒ–æ–¹é¢å…·æœ‰ç«äº‰åŠ›ã€‚</li>
<li>PHIMOèƒ½å¤Ÿå‡å°‘è¶…è¿‡40%çš„é‡‡é›†æ—¶é—´ï¼Œä½¿å…¶æˆä¸ºç ”ç©¶å’Œä¸´åºŠå®è·µä¸­å…·æœ‰å¸å¼•åŠ›çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>PHIMOçš„åº”ç”¨æœ‰åŠ©äºæ”¹å–„è¿åŠ¨ç¨³å¥çš„T2*é‡åŒ–åœ¨ç ”ç©¶å’Œä¸´åºŠå¸¸è§„ä¸­çš„è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17209">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c3df8d1777d76ad2f4ce11205fc617c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fefe3f895cc806cc0069a8da61fabbb3.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Unsupervised-Accelerated-MRI-Reconstruction-via-Ground-Truth-Free-Flow-Matching"><a href="#Unsupervised-Accelerated-MRI-Reconstruction-via-Ground-Truth-Free-Flow-Matching" class="headerlink" title="Unsupervised Accelerated MRI Reconstruction via Ground-Truth-Free Flow   Matching"></a>Unsupervised Accelerated MRI Reconstruction via Ground-Truth-Free Flow   Matching</h2><p><strong>Authors:Xinzhe Luo, Yingzhen Li, Chen Qin</strong></p>
<p>Accelerated magnetic resonance imaging involves reconstructing fully sampled images from undersampled k-space measurements. Current state-of-the-art approaches have mainly focused on either end-to-end supervised training inspired by compressed sensing formulations, or posterior sampling methods built on modern generative models. However, their efficacy heavily relies on large datasets of fully sampled images, which may not always be available in practice. To address this issue, we propose an unsupervised MRI reconstruction method based on ground-truth-free flow matching (GTF$^2$M). Particularly, the GTF$^2$M learns a prior denoising process of fully sampled ground-truth images using only undersampled data. Based on that, an efficient cyclic reconstruction algorithm is further proposed to perform forward and backward integration in the dual space of image-space signal and k-space measurement. We compared our method with state-of-the-art learning-based baselines on the fastMRI database of both single-coil knee and multi-coil brain MRIs. The results show that our proposed unsupervised method can significantly outperform existing unsupervised approaches, and achieve performance comparable to most supervised end-to-end and prior learning baselines trained on fully sampled MRI, while offering greater efficiency than the compared generative model-based approaches. </p>
<blockquote>
<p>åŠ é€Ÿç£å…±æŒ¯æˆåƒæ¶‰åŠä»æ¬ é‡‡æ ·kç©ºé—´æµ‹é‡ä¸­é‡å»ºå®Œå…¨é‡‡æ ·çš„å›¾åƒã€‚ç›®å‰æœ€å‰æ²¿çš„æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨å—å‹ç¼©æ„ŸçŸ¥å…¬å¼å¯å‘çš„ç«¯åˆ°ç«¯ç›‘ç£è®­ç»ƒï¼Œæˆ–åŸºäºç°ä»£ç”Ÿæˆæ¨¡å‹çš„åç»­é‡‡æ ·æ–¹æ³•ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„æ•ˆåŠ›ä¸¥é‡ä¾èµ–äºå®Œå…¨é‡‡æ ·çš„å›¾åƒçš„å¤§æ•°æ®é›†åˆï¼Œè¿™åœ¨å®è·µä¸­å¯èƒ½å¹¶ä¸æ€»èƒ½å¾—åˆ°ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ— çœŸå®å€¼æµåŒ¹é…ï¼ˆGTFÂ²Mï¼‰çš„æ— ç›‘ç£MRIé‡å»ºæ–¹æ³•ã€‚ç‰¹åˆ«æ˜¯ï¼ŒGTFÂ²Mä»…ä½¿ç”¨æ¬ é‡‡æ ·æ•°æ®å­¦ä¹ å®Œå…¨é‡‡æ ·çš„çœŸå®å€¼å›¾åƒçš„å‰å‘å»å™ªè¿‡ç¨‹ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œè¿›ä¸€æ­¥æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„å¾ªç¯é‡å»ºç®—æ³•ï¼Œåœ¨å›¾åƒç©ºé—´ä¿¡å·å’Œkç©ºé—´æµ‹é‡çš„åŒé‡ç©ºé—´ä¸­æ‰§è¡Œæ­£å‘å’Œåå‘é›†æˆã€‚æˆ‘ä»¬åœ¨fastMRIæ•°æ®åº“çš„å•çº¿åœˆè†å…³èŠ‚å’Œå¤šçº¿åœˆè„‘éƒ¨MRIä¸Šï¼Œå°†æˆ‘ä»¬çš„æ–¹æ³•ä¸åŸºäºå­¦ä¹ çš„æœ€æ–°åŸºçº¿è¿›è¡Œäº†æ¯”è¾ƒã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ— ç›‘ç£æ–¹æ³•å¯ä»¥æ˜¾è‘—ä¼˜äºç°æœ‰çš„æ— ç›‘ç£æ–¹æ³•ï¼Œå¹¶å®ç°ä¸å¤§å¤šæ•°åœ¨å®Œå…¨é‡‡æ ·çš„MRIä¸Šè®­ç»ƒçš„ç«¯åˆ°ç«¯ç›‘ç£æ–¹æ³•å’Œå…ˆéªŒå­¦ä¹ åŸºçº¿ç›¸å½“çš„æ€§èƒ½ï¼ŒåŒæ—¶æä¾›äº†æ¯”åŸºäºç”Ÿæˆæ¨¡å‹çš„å¯¹æ¯”æ–¹æ³•æ›´é«˜çš„æ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17174v1">PDF</a> Accepted by IPMI 2025: 29th International Conference on Information   Processing in Medical Imaging</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ— çœŸå®å€¼æµåŒ¹é…ï¼ˆGTFÂ²Mï¼‰çš„æ— ç›‘ç£MRIé‡å»ºæ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨ä»…äº†è§£æ¬ é‡‡æ ·æ•°æ®æ¥æ¨¡æ‹ŸçœŸå®å€¼çš„å…ˆéªŒå»å™ªè¿‡ç¨‹ã€‚é€šè¿‡åœ¨è¯¥å…ˆéªŒåŸºç¡€ä¸Šè¿›è¡Œæ­£å‘å’Œåå‘æ•´åˆï¼Œå®ç°äº†é«˜æ•ˆçš„å¾ªç¯é‡å»ºç®—æ³•ã€‚åœ¨fastMRIæ•°æ®åº“çš„è†å…³èŠ‚å•çº¿åœˆå’Œå¤§è„‘å¤šçº¿åœˆMRIæµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰æ— ç›‘ç£æ–¹æ³•ï¼Œæ€§èƒ½ä¸å¤§å¤šæ•°åŸºäºå®Œå…¨é‡‡æ ·MRIè®­ç»ƒçš„ç«¯åˆ°ç«¯å’Œå…ˆéªŒå­¦ä¹ åŸºçº¿ç›¸å½“ï¼ŒåŒæ—¶æ¯”åŸºäºç”Ÿæˆæ¨¡å‹çš„å¯¹æ¯”æ–¹æ³•æ›´é«˜æ•ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºæ— çœŸå®å€¼æµåŒ¹é…ï¼ˆGTFÂ²Mï¼‰çš„æ— ç›‘ç£MRIé‡å»ºæ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡æ¨¡æ‹ŸçœŸå®å€¼çš„å…ˆéªŒå»å™ªè¿‡ç¨‹æ¥å­¦ä¹ å…ˆéªŒçŸ¥è¯†ï¼Œä»…ä½¿ç”¨æ¬ é‡‡æ ·æ•°æ®ã€‚</li>
<li>é€šè¿‡ç»“åˆå¾ªç¯é‡å»ºç®—æ³•å®ç°äº†é«˜æ•ˆçš„å‰å‘å’Œåå‘æ•´åˆåœ¨å›¾åƒç©ºé—´ä¿¡å·å’Œkç©ºé—´æµ‹é‡ä¹‹é—´çš„åŒé‡ç©ºé—´ã€‚</li>
<li>åœ¨fastMRIæ•°æ®åº“æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰æ— ç›‘ç£æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•çš„æ€§èƒ½ä¸å¤§å¤šæ•°åŸºäºå®Œå…¨é‡‡æ ·MRIè®­ç»ƒçš„ç«¯åˆ°ç«¯å’Œå…ˆéªŒå­¦ä¹ åŸºçº¿ç›¸å½“ã€‚</li>
<li>ä¸å¯¹æ¯”çš„ç”Ÿæˆæ¨¡å‹æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•æ›´é«˜æ•ˆã€‚</li>
<li>æ­¤æ–¹æ³•å¯¹äºæ•°æ®é›†çš„ä¾èµ–ç¨‹åº¦è¾ƒä½ï¼Œå°¤å…¶åœ¨æ²¡æœ‰å¤§é‡å®Œå…¨é‡‡æ ·å›¾åƒçš„æƒ…å†µä¸‹å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17174">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-30fa508d92718c5eda437d72427329b7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c25b6d051cacaade7369e7a36feeef83.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="M3DA-Benchmark-for-Unsupervised-Domain-Adaptation-in-3D-Medical-Image-Segmentation"><a href="#M3DA-Benchmark-for-Unsupervised-Domain-Adaptation-in-3D-Medical-Image-Segmentation" class="headerlink" title="M3DA: Benchmark for Unsupervised Domain Adaptation in 3D Medical Image   Segmentation"></a>M3DA: Benchmark for Unsupervised Domain Adaptation in 3D Medical Image   Segmentation</h2><p><strong>Authors:Boris Shirokikh, Anvar Kurmukov, Mariia Donskova, Valentin Samokhin, Mikhail Belyaev, Ivan Oseledets</strong></p>
<p>Domain shift presents a significant challenge in applying Deep Learning to the segmentation of 3D medical images from sources like Magnetic Resonance Imaging (MRI) and Computed Tomography (CT). Although numerous Domain Adaptation methods have been developed to address this issue, they are often evaluated under impractical data shift scenarios. Specifically, the medical imaging datasets used are often either private, too small for robust training and evaluation, or limited to single or synthetic tasks. To overcome these limitations, we introduce a M3DA &#x2F;â€œmEd@&#x2F; benchmark comprising four publicly available, multiclass segmentation datasets. We have designed eight domain pairs featuring diverse and practically relevant distribution shifts. These include inter-modality shifts between MRI and CT and intra-modality shifts among various MRI acquisition parameters, different CT radiation doses, and presence&#x2F;absence of contrast enhancement in images. Within the proposed benchmark, we evaluate more than ten existing domain adaptation methods. Our results show that none of them can consistently close the performance gap between the domains. For instance, the most effective method reduces the performance gap by about 62% across the tasks. This highlights the need for developing novel domain adaptation algorithms to enhance the robustness and scalability of deep learning models in medical imaging. We made our M3DA benchmark publicly available: <a target="_blank" rel="noopener" href="https://github.com/BorisShirokikh/M3DA">https://github.com/BorisShirokikh/M3DA</a>. </p>
<blockquote>
<p>é¢†åŸŸåç§»ï¼ˆDomain Shiftï¼‰åœ¨å°†æ·±åº¦å­¦ä¹ åº”ç”¨äºä»ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰å’Œè®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰ç­‰æ¥æºçš„3DåŒ»å­¦å›¾åƒåˆ†å‰²ä¸­å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚å°½ç®¡å·²ç»å¼€å‘äº†è®¸å¤šåŸŸè‡ªé€‚åº”ï¼ˆDomain Adaptationï¼‰æ–¹æ³•æ¥åº”å¯¹è¿™ä¸ªé—®é¢˜ï¼Œä½†å®ƒä»¬é€šå¸¸æ˜¯åœ¨ä¸åˆ‡å®é™…çš„æ•°æ®åç§»åœºæ™¯ä¸‹è¿›è¡Œè¯„ä¼°çš„ã€‚å…·ä½“æ¥è¯´ï¼ŒåŒ»å­¦æˆåƒæ•°æ®é›†é€šå¸¸æ˜¯ç§æœ‰çš„ï¼Œå¯¹äºç¨³å¥è®­ç»ƒå’Œè¯„ä¼°è€Œè¨€è§„æ¨¡å¤ªå°ï¼Œæˆ–è€…ä»…é™äºå•ä¸€æˆ–åˆæˆä»»åŠ¡ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†M3DAåŸºå‡†æµ‹è¯•é›†ï¼Œå…¶ä¸­åŒ…å«å››ä¸ªå…¬å¼€å¯ç”¨çš„å¤šç±»åˆ«åˆ†å‰²æ•°æ®é›†ã€‚æˆ‘ä»¬è®¾è®¡äº†å…«ä¸ªå…·æœ‰å¤šæ ·æ€§å’Œå®é™…ç›¸å…³åˆ†å¸ƒåç§»çš„åŸŸå¯¹ã€‚è¿™äº›åŒ…æ‹¬MRIå’ŒCTä¹‹é—´çš„æ¨¡æ€é—´åç§»ä»¥åŠä¸åŒMRIé‡‡é›†å‚æ•°ã€ä¸åŒCTè¾å°„å‰‚é‡å’Œå›¾åƒä¸­æ˜¯å¦å­˜åœ¨å¯¹æ¯”å¢å¼ºä¹‹é—´çš„æ¨¡æ€å†…åç§»ã€‚åœ¨æå‡ºçš„åŸºå‡†æµ‹è¯•é›†ä¸­ï¼Œæˆ‘ä»¬å¯¹è¶…è¿‡åç§ç°æœ‰çš„åŸŸè‡ªé€‚åº”æ–¹æ³•è¿›è¡Œäº†è¯„ä¼°ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œæ²¡æœ‰ä»»ä½•æ–¹æ³•å¯ä»¥å§‹ç»ˆå¦‚ä¸€åœ°ç¼©å°åŸŸä¹‹é—´çš„æ€§èƒ½å·®è·ã€‚ä¾‹å¦‚ï¼Œæœ€æœ‰æ•ˆçš„æ–¹æ³•åœ¨å„é¡¹ä»»åŠ¡ä¸­å°†æ€§èƒ½å·®è·å‡å°‘äº†çº¦62%ã€‚è¿™å¼ºè°ƒäº†å¼€å‘æ–°å‹åŸŸè‡ªé€‚åº”ç®—æ³•çš„éœ€è¦ï¼Œä»¥æé«˜åŒ»å­¦æˆåƒä¸­æ·±åº¦å­¦ä¹ æ¨¡å‹çš„ç¨³å¥æ€§å’Œå¯æ‰©å±•æ€§ã€‚æˆ‘ä»¬çš„M3DAåŸºå‡†æµ‹è¯•é›†å·²å…¬å¼€å¯ç”¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/BorisShirokikh/M3DA">https://github.com/BorisShirokikh/M3DA</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17029v1">PDF</a> 17 pages,7 figures,11 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨å°†æ·±åº¦å­¦ä¹ åº”ç”¨äºä¸‰ç»´åŒ»å­¦å›¾åƒåˆ†å‰²ï¼ˆå¦‚ç£å…±æŒ¯æˆåƒå’Œè®¡ç®—æœºæ–­å±‚æ‰«æï¼‰æ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå³é¢†åŸŸåç§»é—®é¢˜ã€‚å°½ç®¡å·²ç»å¼€å‘äº†è®¸å¤šé¢†åŸŸé€‚åº”æ–¹æ³•æ¥åº”å¯¹è¿™ä¸€é—®é¢˜ï¼Œä½†å®ƒä»¬åœ¨ç°å®çš„æ•°æ®åç§»åœºæ™¯ä¸­å¹¶ä¸å®ç”¨ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåŒ…å«å››ä¸ªå…¬å¼€å¯ç”¨ã€å¤šç±»åˆ«åˆ†å‰²æ•°æ®é›†çš„M3DAåŸºå‡†æµ‹è¯•å¹³å°ï¼Œè®¾è®¡äº†æ¶µç›–å¤šç§å®é™…ç›¸å…³çš„åˆ†å¸ƒåç§»çš„å…«ä¸ªé¢†åŸŸå¯¹ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œç°æœ‰çš„é¢†åŸŸé€‚åº”æ–¹æ³•æ— æ³•ä¸€è‡´ç¼©å°é¢†åŸŸé—´çš„æ€§èƒ½å·®è·ï¼Œå› æ­¤éœ€è¦å¼€å‘æ–°çš„é¢†åŸŸé€‚åº”ç®—æ³•æ¥æé«˜æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨åŒ»å­¦æˆåƒä¸­çš„é²æ£’æ€§å’Œå¯æ‰©å±•æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢†åŸŸåç§»æ˜¯æ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­é¢ä¸´çš„æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰é¢†åŸŸé€‚åº”æ–¹æ³•åœ¨å®è·µä¸­å¹¶ä¸å®ç”¨ï¼Œå› ä¸ºå®ƒä»¬é€šå¸¸åœ¨ç§æœ‰æ•°æ®é›†ä¸Šè¯„ä¼°ï¼Œæ•°æ®é‡å°ä¸”é™äºå•ä¸€ä»»åŠ¡ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„M3DAåŸºå‡†æµ‹è¯•å¹³å°ï¼ŒåŒ…å«å››ä¸ªå…¬å¼€çš„å¤šç±»åˆ«åˆ†å‰²æ•°æ®é›†ã€‚</li>
<li>è®¾è®¡äº†å…«ä¸ªå…·æœ‰å®é™…æ„ä¹‰çš„é¢†åŸŸå¯¹ï¼Œæ¶µç›–å¤šç§åˆ†å¸ƒåç§»ï¼ŒåŒ…æ‹¬MRIå’ŒCTä¹‹é—´çš„æ¨¡æ€åç§»ä»¥åŠä¸åŒMRIé‡‡é›†å‚æ•°ã€CTè¾å°„å‰‚é‡å’Œæœ‰&#x2F;æ— å›¾åƒå¯¹æ¯”å¢å¼ºçš„æ¨¡æ€å†…åç§»ã€‚</li>
<li>å¯¹è¶…è¿‡åç§ç°æœ‰çš„é¢†åŸŸé€‚åº”æ–¹æ³•è¿›è¡Œäº†è¯„ä¼°ï¼Œä½†ç»“æœå¹¶ä¸ç†æƒ³ï¼Œæ²¡æœ‰ä¸€ç§æ–¹æ³•å¯ä»¥å§‹ç»ˆå¦‚ä¸€åœ°ç¼©å°é¢†åŸŸé—´çš„æ€§èƒ½å·®è·ã€‚</li>
<li>æœ€æœ‰æ•ˆçš„æ–¹æ³•å°†æ€§èƒ½å·®è·å‡å°‘äº†çº¦62%ï¼Œä½†ä»éœ€å¼€å‘æ–°çš„é¢†åŸŸé€‚åº”ç®—æ³•æ¥æé«˜æ¨¡å‹çš„é²æ£’æ€§å’Œå¯æ‰©å±•æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17029">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-62f732a42ab5cbcc24158a2f7f0bf0f5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4e472577b58e52c303874134684e1053.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ab3b7fc879a6165a9cbbf53a1523fdab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-57a9530a91a1aa889266df36c68ff012.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-58d7a530344851f01d59d4834b8c3e6f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9866a737afcdccf3d5891d1f9cbe3639.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="MAD-AD-Masked-Diffusion-for-Unsupervised-Brain-Anomaly-Detection"><a href="#MAD-AD-Masked-Diffusion-for-Unsupervised-Brain-Anomaly-Detection" class="headerlink" title="MAD-AD: Masked Diffusion for Unsupervised Brain Anomaly Detection"></a>MAD-AD: Masked Diffusion for Unsupervised Brain Anomaly Detection</h2><p><strong>Authors:Farzad Beizaee, Gregory Lodygensky, Christian Desrosiers, Jose Dolz</strong></p>
<p>Unsupervised anomaly detection in brain images is crucial for identifying injuries and pathologies without access to labels. However, the accurate localization of anomalies in medical images remains challenging due to the inherent complexity and variability of brain structures and the scarcity of annotated abnormal data. To address this challenge, we propose a novel approach that incorporates masking within diffusion models, leveraging their generative capabilities to learn robust representations of normal brain anatomy. During training, our model processes only normal brain MRI scans and performs a forward diffusion process in the latent space that adds noise to the features of randomly-selected patches. Following a dual objective, the model learns to identify which patches are noisy and recover their original features. This strategy ensures that the model captures intricate patterns of normal brain structures while isolating potential anomalies as noise in the latent space. At inference, the model identifies noisy patches corresponding to anomalies and generates a normal counterpart for these patches by applying a reverse diffusion process. Our method surpasses existing unsupervised anomaly detection techniques, demonstrating superior performance in generating accurate normal counterparts and localizing anomalies. The code is available at h<a target="_blank" rel="noopener" href="https://github.com/farzad-bz/MAD-AD">https://github.com/farzad-bz/MAD-AD</a>. </p>
<blockquote>
<p>åœ¨æ— éœ€æ ‡ç­¾çš„æƒ…å†µä¸‹ï¼Œå¯¹è„‘å›¾åƒè¿›è¡Œæ— ç›‘ç£çš„å¼‚å¸¸æ£€æµ‹å¯¹äºè¯†åˆ«æŸä¼¤å’Œç—…ç†è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç”±äºè„‘ç»“æ„å›ºæœ‰çš„å¤æ‚æ€§å’Œå¯å˜æ€§ä»¥åŠæ ‡æ³¨å¼‚å¸¸æ•°æ®çš„ç¨€ç¼ºæ€§ï¼Œå‡†ç¡®åœ°åœ¨åŒ»å­¦å›¾åƒä¸­å®šä½å¼‚å¸¸ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»“åˆæ‰©æ•£æ¨¡å‹ä¸­é®ç½©æŠ€æœ¯çš„æ–°æ–¹æ³•ï¼Œåˆ©ç”¨å…¶ç”Ÿæˆèƒ½åŠ›å­¦ä¹ æ­£å¸¸è„‘è§£å‰–ç»“æ„çš„ç¨³å¥è¡¨ç¤ºã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ä»…å¤„ç†æ­£å¸¸çš„è„‘éƒ¨MRIæ‰«æï¼Œå¹¶åœ¨æ½œåœ¨ç©ºé—´æ‰§è¡Œæ­£å‘æ‰©æ•£è¿‡ç¨‹ï¼Œå‘éšæœºé€‰æ‹©çš„è¡¥ä¸çš„ç‰¹å¾æ·»åŠ å™ªå£°ã€‚éµå¾ªåŒé‡ç›®æ ‡ï¼Œæ¨¡å‹å­¦ä¹ è¯†åˆ«å“ªäº›è¡¥ä¸æ˜¯å˜ˆæ‚çš„å¹¶æ¢å¤å…¶åŸå§‹ç‰¹å¾ã€‚æ­¤ç­–ç•¥ç¡®ä¿æ¨¡å‹æ•æ‰æ­£å¸¸è„‘ç»“æ„çš„ç²¾ç»†æ¨¡å¼ï¼ŒåŒæ—¶å°†æ½œåœ¨çš„å¼‚å¸¸å­¤ç«‹ä¸ºæ½œåœ¨ç©ºé—´ä¸­çš„å™ªå£°ã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œæ¨¡å‹ä¼šè¯†åˆ«å¯¹åº”äºå¼‚å¸¸çš„å˜ˆæ‚è¡¥ä¸ï¼Œå¹¶é€šè¿‡åå‘æ‰©æ•£è¿‡ç¨‹ç”Ÿæˆè¿™äº›è¡¥ä¸çš„æ­£å¸¸å¯¹åº”ç‰©ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¶…è¶Šäº†ç°æœ‰çš„æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹æŠ€æœ¯ï¼Œåœ¨ç”Ÿæˆå‡†ç¡®çš„æ­£å¸¸å¯¹åº”ç‰©å’Œå®šä½å¼‚å¸¸æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚ä»£ç å¯é€šè¿‡ä»¥ä¸‹é“¾æ¥è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/farzad-bz/MAD-AD%E3%80%82">https://github.com/farzad-bz/MAD-ADã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.16943v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒä¸­çš„æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹å¯¹äºåœ¨æ²¡æœ‰æ ‡ç­¾çš„æƒ…å†µä¸‹è¯†åˆ«æŸä¼¤å’Œç—…ç†è‡³å…³é‡è¦ã€‚é’ˆå¯¹åŒ»å­¦å›¾åƒä¸­å¼‚å¸¸å‡†ç¡®å®šä½çš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»“åˆæ‰©æ•£æ¨¡å‹å†…æ©è”½çš„æ–°æ–¹æ³•ï¼Œåˆ©ç”¨ç”Ÿæˆèƒ½åŠ›å­¦ä¹ æ­£å¸¸è„‘ç»“æ„çš„æœ‰åŠ›è¡¨å¾ã€‚æˆ‘ä»¬çš„æ¨¡å‹ä»…å¤„ç†æ­£å¸¸çš„è„‘éƒ¨MRIæ‰«æï¼Œåœ¨æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œæ­£å‘æ‰©æ•£è¿‡ç¨‹ï¼Œå¯¹éšæœºé€‰æ‹©çš„æ–‘å—æ·»åŠ å™ªå£°ã€‚æ¨¡å‹å­¦ä¹ è¯†åˆ«å“ªäº›æ–‘å—æ˜¯å˜ˆæ‚çš„å¹¶æ¢å¤å…¶åŸå§‹ç‰¹å¾ï¼Œç¡®ä¿æ¨¡å‹æ•æ‰æ­£å¸¸è„‘ç»“æ„çš„å¤æ‚æ¨¡å¼ï¼ŒåŒæ—¶å°†æ½œåœ¨çš„å¼‚å¸¸å­¤ç«‹ä¸ºæ½œåœ¨ç©ºé—´ä¸­çš„å™ªå£°ã€‚åœ¨æ¨ç†æ—¶ï¼Œæ¨¡å‹è¯†åˆ«ä¸å¼‚å¸¸ç›¸å¯¹åº”çš„å˜ˆæ‚æ–‘å—ï¼Œå¹¶é€šè¿‡åå‘æ‰©æ•£è¿‡ç¨‹ä¸ºè¿™äº›æ–‘å—ç”Ÿæˆæ­£å¸¸å¯¹åº”ç‰©ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ç”Ÿæˆå‡†ç¡®çš„æ­£å¸¸å¯¹åº”ç‰©å’Œå®šä½å¼‚å¸¸æ–¹é¢è¶…è¶Šäº†ç°æœ‰çš„æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹åœ¨åŒ»å­¦å›¾åƒä¸­è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¼ºä¹æ ‡ç­¾çš„æƒ…å†µä¸‹ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ç»“åˆæ‰©æ•£æ¨¡å‹å†…æ©è”½çš„æ–°æ–¹æ³•ï¼Œç”¨äºå¼‚å¸¸æ£€æµ‹ã€‚</li>
<li>æ¨¡å‹é€šè¿‡æ­£å‘æ‰©æ•£è¿‡ç¨‹å­¦ä¹ æ­£å¸¸è„‘ç»“æ„çš„è¡¨å¾ã€‚</li>
<li>æ¨¡å‹èƒ½å¤Ÿè¯†åˆ«å˜ˆæ‚çš„æ–‘å—å¹¶æ¢å¤å…¶åŸå§‹ç‰¹å¾ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨æ½œåœ¨ç©ºé—´ä¸­åŒºåˆ†æ­£å¸¸ç»“æ„ä¸å¼‚å¸¸ã€‚</li>
<li>é€šè¿‡åå‘æ‰©æ•£è¿‡ç¨‹ï¼Œæ¨¡å‹èƒ½ç”Ÿæˆä¸å¼‚å¸¸æ–‘å—å¯¹åº”çš„æ­£å¸¸å¯¹åº”ç‰©ã€‚</li>
<li>è¯¥æ–¹æ³•è¶…è¶Šäº†ç°æœ‰çš„æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹æŠ€æœ¯ï¼Œè¡¨ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.16943">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-416145dc3489b27cc482d3b51e1bf256.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6ef5318d210bef55c905e8cf4d2694cb.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="DiffKAN-Inpainting-KAN-based-Diffusion-model-for-brain-tumor-inpainting"><a href="#DiffKAN-Inpainting-KAN-based-Diffusion-model-for-brain-tumor-inpainting" class="headerlink" title="DiffKAN-Inpainting: KAN-based Diffusion model for brain tumor inpainting"></a>DiffKAN-Inpainting: KAN-based Diffusion model for brain tumor inpainting</h2><p><strong>Authors:Tianli Tao, Ziyang Wang, Han Zhang, Theodoros N. Arvanitis, Le Zhang</strong></p>
<p>Brain tumors delay the standard preprocessing workflow for further examination. Brain inpainting offers a viable, although difficult, solution for tumor tissue processing, which is necessary to improve the precision of the diagnosis and treatment. Most conventional U-Net-based generative models, however, often face challenges in capturing the complex, nonlinear latent representations inherent in brain imaging. In order to accomplish high-quality healthy brain tissue reconstruction, this work proposes DiffKAN-Inpainting, an innovative method that blends diffusion models with the Kolmogorov-Arnold Networks architecture. During the denoising process, we introduce the RePaint method and tumor information to generate images with a higher fidelity and smoother margin. Both qualitative and quantitative results demonstrate that as compared to the state-of-the-art methods, our proposed DiffKAN-Inpainting inpaints more detailed and realistic reconstructions on the BraTS dataset. The knowledge gained from ablation study provide insights for future research to balance performance with computing cost. </p>
<blockquote>
<p>è„‘è‚¿ç˜¤ä¼šå»¶è¿Ÿè¿›ä¸€æ­¥çš„æ£€æŸ¥æ ‡å‡†é¢„å¤„ç†å·¥ä½œæµç¨‹ã€‚è„‘å¡«å……ï¼ˆBrain inpaintingï¼‰ä¸ºè‚¿ç˜¤ç»„ç»‡å¤„ç†æä¾›äº†ä¸€ç§å¯è¡Œè™½ç„¶å›°éš¾çš„è§£å†³æ–¹æ¡ˆï¼Œè¿™å¯¹äºæé«˜è¯Šæ–­å’Œæ²»ç–—ç²¾åº¦æ˜¯å¿…è¦çš„ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°åŸºäºU-Netçš„ä¼ ç»Ÿç”Ÿæˆæ¨¡å‹åœ¨æ•è·è„‘æˆåƒä¸­å›ºæœ‰çš„å¤æ‚éçº¿æ€§æ½œåœ¨è¡¨ç¤ºæ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºäº†å®Œæˆé«˜è´¨é‡çš„å¥åº·è„‘ç»„ç»‡é‡å»ºï¼Œè¿™é¡¹å·¥ä½œæå‡ºäº†DiffKAN-Inpaintingè¿™ä¸€åˆ›æ–°æ–¹æ³•ï¼Œå®ƒå°†æ‰©æ•£æ¨¡å‹ä¸Kolmogorov-Arnoldç½‘ç»œæ¶æ„ç›¸ç»“åˆã€‚åœ¨é™å™ªè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†RePaintæ–¹æ³•å’Œè‚¿ç˜¤ä¿¡æ¯ï¼Œä»¥ç”Ÿæˆå…·æœ‰æ›´é«˜ä¿çœŸåº¦å’Œæ›´å¹³æ»‘è¾¹ç•Œçš„å›¾åƒã€‚å®šæ€§å’Œå®šé‡ç»“æœå‡è¡¨æ˜ï¼Œä¸æœ€æ–°æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬æå‡ºçš„DiffKAN-Inpaintingåœ¨BraTSæ•°æ®é›†ä¸Šè¿›è¡Œäº†æ›´è¯¦ç»†å’Œæ›´ç°å®çš„é‡å»ºã€‚ä»æ¶ˆèç ”ç©¶ä¸­è·å¾—çš„çŸ¥è¯†ä¸ºæœªæ¥ç ”ç©¶æä¾›äº†åœ¨æ€§èƒ½ä¸è®¡ç®—æˆæœ¬ä¹‹é—´å–å¾—å¹³è¡¡çš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.16771v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºä¸€ç§ç»“åˆæ‰©æ•£æ¨¡å‹å’ŒKolmogorov-Arnoldç½‘ç»œæ¶æ„çš„DiffKAN-Inpaintingæ–¹æ³•ï¼Œç”¨äºé«˜è´¨é‡é‡å»ºå¥åº·è„‘ç»„ç»‡ã€‚è¯¥æ–¹æ³•åœ¨å»é™¤å™ªå£°è¿‡ç¨‹ä¸­å¼•å…¥RePaintæ–¹æ³•å’Œè‚¿ç˜¤ä¿¡æ¯ï¼Œç”Ÿæˆæ›´é€¼çœŸã€ç»†èŠ‚æ›´ä¸°å¯Œçš„å›¾åƒã€‚ç›¸è¾ƒäºç°æœ‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨BraTSæ•°æ®é›†ä¸Šæœ‰æ›´å¥½çš„è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Brain tumorså½±å“é¢„å¤„ç†å·¥ä½œæµç¨‹ï¼Œéœ€è¦è¿›ä¸€æ­¥å¤„ç†ä»¥æ”¹å–„è¯Šæ–­å’Œæ²»ç–—çš„å‡†ç¡®æ€§ã€‚</li>
<li>Brain inpaintingä¸ºè§£å†³è¿™ä¸ªé—®é¢˜æä¾›äº†ä¸€ç§å¯è¡Œçš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>ä¼ ç»Ÿçš„U-Net-based generativeæ¨¡å‹åœ¨æ•è·è„‘æˆåƒä¸­çš„å¤æ‚éçº¿æ€§æ½œåœ¨è¡¨ç¤ºæ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>DiffKAN-Inpaintingæ–¹æ³•ç»“åˆäº†æ‰©æ•£æ¨¡å‹å’ŒKolmogorov-Arnoldç½‘ç»œæ¶æ„ï¼Œæ—¨åœ¨å®ç°é«˜è´¨é‡çš„å¥åº·è„‘ç»„ç»‡é‡å»ºã€‚</li>
<li>RePaintæ–¹æ³•å’Œè‚¿ç˜¤ä¿¡æ¯è¢«å¼•å…¥åˆ°å»å™ªè¿‡ç¨‹ä¸­ï¼Œç”Ÿæˆäº†æ›´é«˜ä¿çœŸåº¦å’Œæ›´å¹³æ»‘è¾¹ç¼˜çš„å›¾åƒã€‚</li>
<li>ä¸æœ€æ–°æ–¹æ³•ç›¸æ¯”ï¼ŒDiffKAN-Inpaintingåœ¨BraTSæ•°æ®é›†ä¸Šè¡¨ç°å‡ºäº†æ›´å¥½çš„é‡å»ºæ•ˆæœå’Œç»†èŠ‚è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.16771">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4b3e3c3ec1ae4b6ece50a639567de01b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ecad256aa331c5321d9abc10725ebe4d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-14ddbcf6d0076cdc7a8ef10c3295d56e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-23b320d150f888b0ac664958285dda6b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-864857c43fe0a6a419d2932424a24711.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="AeroReformer-Aerial-Referring-Transformer-for-UAV-based-Referring-Image-Segmentation"><a href="#AeroReformer-Aerial-Referring-Transformer-for-UAV-based-Referring-Image-Segmentation" class="headerlink" title="AeroReformer: Aerial Referring Transformer for UAV-based Referring Image   Segmentation"></a>AeroReformer: Aerial Referring Transformer for UAV-based Referring Image   Segmentation</h2><p><strong>Authors:Rui Li</strong></p>
<p>As a novel and challenging task, referring segmentation combines computer vision and natural language processing to localize and segment objects based on textual descriptions. While referring image segmentation (RIS) has been extensively studied in natural images, little attention has been given to aerial imagery, particularly from unmanned aerial vehicles (UAVs). The unique challenges of UAV imagery, including complex spatial scales, occlusions, and varying object orientations, render existing RIS approaches ineffective. A key limitation has been the lack of UAV-specific datasets, as manually annotating pixel-level masks and generating textual descriptions is labour-intensive and time-consuming. To address this gap, we design an automatic labelling pipeline that leverages pre-existing UAV segmentation datasets and Multimodal Large Language Models (MLLM) for generating textual descriptions. Furthermore, we propose Aerial Referring Transformer (AeroReformer), a novel framework for UAV referring image segmentation (UAV-RIS), featuring a Vision-Language Cross-Attention Module (VLCAM) for effective cross-modal understanding and a Rotation-Aware Multi-Scale Fusion (RAMSF) decoder to enhance segmentation accuracy in aerial scenes. Extensive experiments on two newly developed datasets demonstrate the superiority of AeroReformer over existing methods, establishing a new benchmark for UAV-RIS. The datasets and code will be publicly available at: <a target="_blank" rel="noopener" href="https://github.com/lironui/AeroReformer">https://github.com/lironui/AeroReformer</a>. </p>
<blockquote>
<p>ä½œä¸ºä¸€é¡¹æ–°é¢–ä¸”å¯Œæœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå¼•ç”¨åˆ†å‰²ç»“åˆäº†è®¡ç®—æœºè§†è§‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†ï¼Œæ ¹æ®æ–‡æœ¬æè¿°æ¥å®šä½å’Œåˆ†å‰²å¯¹è±¡ã€‚è™½ç„¶å¼•ç”¨å›¾åƒåˆ†å‰²ï¼ˆRISï¼‰åœ¨è‡ªç„¶å›¾åƒä¸­å¾—åˆ°äº†å¹¿æ³›çš„ç ”ç©¶ï¼Œä½†å¯¹èˆªç©ºå›¾åƒï¼Œç‰¹åˆ«æ˜¯æ¥è‡ªæ— äººæœºçš„èˆªç©ºå›¾åƒçš„å…³æ³¨å´å¾ˆå°‘ã€‚æ— äººæœºå›¾åƒå…·æœ‰ç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å¤æ‚çš„ç©ºé—´å°ºåº¦ã€é®æŒ¡å’Œå˜åŒ–çš„å¯¹è±¡æ–¹å‘ï¼Œè¿™ä½¿å¾—ç°æœ‰çš„RISæ–¹æ³•æ— æ•ˆã€‚ä¸€ä¸ªå…³é”®çš„é™åˆ¶æ˜¯ç¼ºä¹é’ˆå¯¹æ— äººæœºçš„ç‰¹å®šæ•°æ®é›†ï¼Œå› ä¸ºæ‰‹åŠ¨æ³¨é‡Šåƒç´ çº§æ©è†œå’Œç”Ÿæˆæ–‡æœ¬æè¿°æ˜¯åŠ³åŠ¨å¯†é›†å‹çš„ï¼Œè€—æ—¶è´¹åŠ›ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªè‡ªåŠ¨æ ‡æ³¨ç®¡é“ï¼Œè¯¥ç®¡é“åˆ©ç”¨é¢„å…ˆå­˜åœ¨çš„æ— äººæœºåˆ†å‰²æ•°æ®é›†å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æ¥ç”Ÿæˆæ–‡æœ¬æè¿°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ç”¨äºæ— äººæœºå¼•ç”¨å›¾åƒåˆ†å‰²ï¼ˆUAV-RISï¼‰çš„ç©ºä¸­å‚è€ƒè½¬æ¢å™¨ï¼ˆAeroReformerï¼‰è¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œå®ƒåŒ…å«ä¸€ä¸ªç”¨äºæœ‰æ•ˆè·¨æ¨¡æ€ç†è§£çš„è§†è§‰è¯­è¨€äº¤å‰æ³¨æ„åŠ›æ¨¡å—ï¼ˆVLCAMï¼‰å’Œä¸€ä¸ªæ—‹è½¬æ„ŸçŸ¥å¤šå°ºåº¦èåˆï¼ˆRAMSFï¼‰è§£ç å™¨ï¼Œä»¥æé«˜èˆªç©ºåœºæ™¯ä¸­çš„åˆ†å‰²ç²¾åº¦ã€‚åœ¨ä¸¤ä¸ªæ–°å¼€å‘çš„æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒAeroReformerä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸ºUAV-RISå»ºç«‹äº†æ–°çš„åŸºå‡†ã€‚æ•°æ®é›†å’Œä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/lironui/AeroReformer%E5%85%AC%E5%BC%80%E5%8F%AF%E7%94%A8%E3%80%82">https://github.com/lironui/AeroReformerå…¬å¼€å¯ç”¨ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.16680v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä»‹ç»äº†ä¸€é¡¹æ–°çš„æŒ‘æˆ˜ä»»åŠ¡â€”â€”åŸºäºæ–‡æœ¬æè¿°çš„æŒ‡ä»£å›¾åƒåˆ†å‰²æŠ€æœ¯ã€‚é’ˆå¯¹æ— äººæœºï¼ˆUAVï¼‰èˆªæ‹å›¾åƒçš„ç‰¹å®šæƒ…å¢ƒï¼Œæ–‡ç« æŒ‡å‡ºäº†ç°æœ‰æŠ€æœ¯çš„å±€é™æ€§å¹¶æå‡ºäº†å…¨æ–°çš„æ¡†æ¶â€”â€”Aerial Referrign Transformerï¼ˆAeroReformerï¼‰ã€‚é€šè¿‡ç»“åˆè·¨æ¨¡æ€çš„æ³¨æ„åŠ›æ¨¡å—å’Œå¤šå°ºåº¦èåˆæŠ€æœ¯ï¼Œè¿™ä¸€æ–°æ¡†æ¶è§£å†³äº†æ— äººèˆªæ‹å›¾åƒçš„æŒ‡ä»£å›¾åƒåˆ†å‰²éš¾é¢˜ã€‚å…¶æ•°æ®å¤„ç†ç®¡é“å·²åœ¨ä¸¤ä¸ªæ–°å¼€å‘çš„æ•°æ®é›†ä¸Šè¿›è¡Œæµ‹è¯•ï¼Œè¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½ï¼Œæœªæ¥å¯å¹¿æ³›ç”¨äºæ— äººæœºçš„èˆªæ‹åˆ†æä»»åŠ¡ã€‚æ›´å¤šç›¸å…³ä¿¡æ¯å°†åœ¨å¼€æºç½‘ç«™å‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æ–‡æœ¬ä»‹ç»äº†æŒ‡ä»£å›¾åƒåˆ†å‰²æŠ€æœ¯çš„æ–°æŒ‘æˆ˜ä»»åŠ¡ï¼Œå³åŸºäºæ–‡æœ¬æè¿°è¿›è¡Œç‰©ä½“å®šä½å’Œåˆ†å‰²ã€‚</li>
<li>é’ˆå¯¹æ— äººæœºèˆªæ‹å›¾åƒçš„ç‰¹æ®ŠæŒ‘æˆ˜ï¼Œå¦‚å¤æ‚ç©ºé—´å°ºåº¦ã€é®æŒ¡å’Œç‰©ä½“æ–¹å‘å˜åŒ–ç­‰ï¼Œç°æœ‰æŠ€æœ¯æ•ˆæœä¸ä½³ã€‚</li>
<li>ç¼ºä¹é’ˆå¯¹æ— äººæœºçš„ç‰¹å®šæ•°æ®é›†æ˜¯é™åˆ¶ä¹‹ä¸€ï¼Œæ‰‹åŠ¨æ ‡æ³¨åƒç´ çº§æ©è†œå’Œç”Ÿæˆæ–‡æœ¬æè¿°æ—¢è€—æ—¶åˆåŠ³åŠ›ã€‚</li>
<li>ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œè®¾è®¡äº†ä¸€ç§è‡ªåŠ¨æ ‡æ³¨ç®¡é“ï¼Œåˆ©ç”¨ç°æœ‰çš„æ— äººæœºåˆ†å‰²æ•°æ®é›†å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆæ–‡æœ¬æè¿°ã€‚</li>
<li>æå‡ºäº†Aerial Referrign Transformerï¼ˆAeroReformerï¼‰æ¡†æ¶ï¼ŒåŒ…å«è·¨æ¨¡æ€è§†è§‰è¯­è¨€æ³¨æ„åŠ›æ¨¡å—å’Œæ—‹è½¬æ„ŸçŸ¥å¤šå°ºåº¦èåˆè§£ç å™¨ã€‚</li>
<li>åœ¨ä¸¤ä¸ªæ–°å¼€å‘çš„æ•°æ®é›†ä¸Šè¿›è¡Œå®éªŒï¼Œè¯æ˜AeroReformerç›¸è¾ƒäºç°æœ‰æ–¹æ³•çš„ä¼˜è¶Šæ€§ï¼Œä¸ºæ— äººæœºæŒ‡ä»£å›¾åƒåˆ†å‰²æ ‘ç«‹äº†æ–°æ ‡æ†ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.16680">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f21107119346ab7ad6909dc69e31e370.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-94563a681bd029f13ab4bd71b9df0f30.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1dad80a4c1a092eeeaa1fdaccf907497.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dcd320e41e8da6e9320fd0657b38e59b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d2e30d0732afe262c3d4c30b7eddd609.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f27c20a18690299605e3d93bbd872a17.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Tool-or-Tutor-Experimental-evidence-from-AI-deployment-in-cancer-diagnosis"><a href="#Tool-or-Tutor-Experimental-evidence-from-AI-deployment-in-cancer-diagnosis" class="headerlink" title="Tool or Tutor? Experimental evidence from AI deployment in cancer   diagnosis"></a>Tool or Tutor? Experimental evidence from AI deployment in cancer   diagnosis</h2><p><strong>Authors:Vivianna Fang He, Sihan Li, Phanish Puranam</strong></p>
<p>Professionals increasingly use Artificial Intelligence (AI) to enhance their capabilities and assist with task execution. While prior research has examined these uses separately, their potential interaction remains underexplored. We propose that AI-driven training (tutor effect) and AI-assisted task completion (tool effect) can be complementary and test this hypothesis in the context of lung cancer diagnosis. In a field experiment with 334 medical students, we manipulated AI deployment in training, in practice, and in both. Our findings reveal that while AI-integrated training and AI assistance independently improved diagnostic performance, their combination yielded the highest accuracy. These results underscore AIâ€™s dual role in enhancing human performance through both learning and real-time support, offering insights into AI deployment in professional settings where human expertise remains essential. </p>
<blockquote>
<p>ä¸“ä¸šäººå£«è¶Šæ¥è¶Šå¤šåœ°ä½¿ç”¨äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ¥å¢å¼ºè‡ªèº«èƒ½åŠ›å¹¶è¾…åŠ©æ‰§è¡Œä»»åŠ¡ã€‚è™½ç„¶å…ˆå‰çš„ç ”ç©¶å·²ç»åˆ†åˆ«ç ”ç©¶äº†è¿™äº›ç”¨é€”ï¼Œä½†å®ƒä»¬ä¹‹é—´çš„æ½œåœ¨ç›¸äº’ä½œç”¨ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æˆ‘ä»¬æå‡ºAIé©±åŠ¨çš„åŸ¹è®­ï¼ˆå¯¼å¸ˆæ•ˆåº”ï¼‰å’ŒAIè¾…åŠ©ä»»åŠ¡å®Œæˆï¼ˆå·¥å…·æ•ˆåº”ï¼‰å¯ä»¥ç›¸äº’è¡¥å……ï¼Œå¹¶åœ¨è‚ºç™Œè¯Šæ–­çš„èƒŒæ™¯ä¸‹æµ‹è¯•è¿™ä¸€å‡è®¾ã€‚åœ¨ä¸€é¡¹æœ‰334ååŒ»å­¦ç”Ÿå‚ä¸çš„ç°åœºå®éªŒä¸­ï¼Œæˆ‘ä»¬æ“ä½œäº†AIåœ¨åŸ¹è®­ã€å®è·µå’Œä¸¤è€…ä¸­çš„éƒ¨ç½²ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œè™½ç„¶AIé›†æˆåŸ¹è®­å’ŒAIè¾…åŠ©ç‹¬ç«‹æé«˜äº†è¯Šæ–­æ€§èƒ½ï¼Œä½†å®ƒä»¬çš„ç»“åˆäº§ç”Ÿäº†æœ€é«˜çš„å‡†ç¡®æ€§ã€‚è¿™äº›ç»“æœå¼ºè°ƒäº†äººå·¥æ™ºèƒ½é€šè¿‡å­¦ä¹ å’Œå®æ—¶æ”¯æŒå¢å¼ºäººç±»æ€§èƒ½çš„åŒé‡ä½œç”¨ï¼Œä¸ºåœ¨ä¸“ä¸šç¯å¢ƒä¸­éƒ¨ç½²äººå·¥æ™ºèƒ½æä¾›äº†è§è§£ï¼Œåœ¨è¿™äº›ç¯å¢ƒä¸­ï¼Œäººç±»ä¸“ä¸šçŸ¥è¯†ä»ç„¶è‡³å…³é‡è¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.16411v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ‘˜è¦æŒ‡å‡ºï¼Œéšç€äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰åœ¨ä¸“ä¸šé¢†åŸŸçš„åº”ç”¨æ—¥ç›Šå¹¿æ³›ï¼ŒAIé©±åŠ¨çš„è®­ç»ƒï¼ˆå¯¼å¸ˆæ•ˆåº”ï¼‰å’ŒAIè¾…åŠ©ä»»åŠ¡å®Œæˆï¼ˆå·¥å…·æ•ˆåº”ï¼‰èƒ½å¤Ÿäº’è¡¥ã€‚ä¸€é¡¹é’ˆå¯¹è‚ºç™Œè¯Šæ–­çš„ç°åœºå®éªŒå‘ç°ï¼Œå•ç‹¬çš„AIé›†æˆè®­ç»ƒå’ŒAIè¾…åŠ©èƒ½å¤Ÿæé«˜è¯Šæ–­æ€§èƒ½ï¼Œä½†äºŒè€…çš„ç»“åˆèƒ½è¾¾åˆ°æœ€é«˜å‡†ç¡®åº¦ã€‚è¿™çªæ˜¾äº†AIåœ¨é€šè¿‡å­¦ä¹ å’Œå®æ—¶æ”¯æŒå¢å¼ºäººç±»è¡¨ç°æ–¹é¢çš„åŒé‡ä½œç”¨ï¼Œä¸ºåœ¨ä¸“ä¸šç¯å¢ƒä¸­éƒ¨ç½²AIæä¾›äº†æ·±åˆ»çš„è§è§£ã€‚äººç±»ä¸“ä¸šçŸ¥è¯†ä»æ˜¯ä¸å¯æˆ–ç¼ºçš„ã€‚äººå·¥æ™ºèƒ½æ˜¯ç ”ç©¶å’Œä¸´åºŠåŒ»å­¦çš„ç»“åˆä¸­çš„å…³é”®è§’è‰²ã€‚å°½ç®¡å…¶æ½œåŠ›å·¨å¤§ï¼Œä½†åœ¨åŒ»ç–—ä¿å¥é¢†åŸŸçš„å®æ–½ä¸­ä»å­˜åœ¨è®¸å¤šæŒ‘æˆ˜å’Œéšœç¢ã€‚è¿™äº›å‘ç°å¯¹ä¸“ä¸šç¯å¢ƒä¸­çš„AIéƒ¨ç½²æœ‰å¯ç¤ºæ„ä¹‰ã€‚åŸºäºæˆ‘ä»¬çš„ç ”ç©¶ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æœªæ¥è¿›ä¸€æ­¥çš„äº¤äº’æ¢ç´¢éå¸¸é‡è¦ï¼Œä¾‹å¦‚åœ¨æµ‹è¯•å’Œå‡†ç¡®æ€§ä»¥åŠä¾¿æºæ€§æ–¹é¢è¿›è¡Œã€‚å¹¶ä¸”éœ€è¦è€ƒè™‘åœ¨å®æ–½åŒ»ç–—ä¿å¥åº”ç”¨æ–¹é¢çš„æ›´å¤šç»†èŠ‚ã€‚æ€»ä¹‹ï¼ŒAIä¸äººç±»ä¸“å®¶ä¹‹é—´çš„åä½œè‡³å…³é‡è¦ã€‚æˆ‘ä»¬åº”é€šè¿‡è”åˆå­¦ä¹ å’ŒååŒå·¥ä½œï¼Œå°†ä¸¤è€…å®Œç¾ç»“åˆä»¥è¾¾æˆæ›´é«˜æˆæ•ˆçš„ä¸šç»©ç›®æ ‡ã€‚å¦å¤–æˆ‘ä»¬éœ€è¦æ—¶åˆ»ç•™æ„å‰æ²¿æŠ€æœ¯å‘å±•å¹¶ä¸”ä¸ºæ›´å¤šçš„åœºæ™¯å¯»æ±‚åˆ›æ–°å’Œæ”¹è¿›è·¯å¾„ç­‰è‡³å…³é‡è¦ï¼›å› æ­¤æœ‰å¿…è¦æ·±å…¥æ¢è®¨å’Œæ¢è®¨æœ€ä½³å®è·µæ¨¡å¼å¹¶è‡´åŠ›äºæ›´å¹¿æ³›çš„æ¨å¹¿ä¸åº”ç”¨ä¸­æ¢ç´¢æœªæ¥çš„è¶‹åŠ¿ä¸å¯èƒ½æ€§ï¼Œæœ€ç»ˆå®ç°åŒ»å­¦è¯Šæ–­æŠ€æœ¯çš„é«˜æ•ˆå’Œç²¾å‡†åŒ–ã€‚åŒæ—¶ï¼Œäººå·¥æ™ºèƒ½åœ¨åŒ»å­¦å›¾åƒé¢†åŸŸçš„åº”ç”¨å‰æ™¯å¹¿é˜”ï¼Œå€¼å¾—è¿›ä¸€æ­¥ç ”ç©¶å’Œæ¢ç´¢ã€‚è¿™å°†æœ‰åŠ©äºæ¨åŠ¨åŒ»å­¦é¢†åŸŸçš„è¿›æ­¥å’Œå‘å±•ï¼Œé€ ç¦æ›´å¤šçš„æ‚£è€…å’Œç¤¾ä¼šã€‚æ€»ä½“è€Œè¨€äººå·¥æ™ºèƒ½åœ¨ä¸“ä¸šé¢†åŸŸçš„ä»·å€¼ä¸å¯å¿½è§†å…·æœ‰å·¨å¤§æ½œåŠ›æœªæ¥å¿…å°†ç»§ç»­å¾—åˆ°å¹¿æ³›çš„åº”ç”¨å’Œæ¨å¹¿èµ·åˆ°é‡è¦çš„ä½œç”¨è§£å†³ç°å®é—®é¢˜å®ç°æ›´å¥½çš„æ•ˆæœå’Œç›®æ ‡ç­‰ç­‰æ„ä¹‰æ·±è¿œé‡å¤§å€¼å¾—æœŸå¾…è¿›ä¸€æ­¥æ·±å…¥ç ”ç©¶æ¢è®¨ç­‰ã€‚<strong>Key Takeaways</strong>ï¼š</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.16411">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8dd1ac8c9a45fb7fe95b015c72e66996.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Optimizing-normal-tissue-sparing-via-spatiotemporal-optimization-under-equivalent-tumor-radical-efficacy"><a href="#Optimizing-normal-tissue-sparing-via-spatiotemporal-optimization-under-equivalent-tumor-radical-efficacy" class="headerlink" title="Optimizing normal tissue sparing via spatiotemporal optimization under   equivalent tumor-radical efficacy"></a>Optimizing normal tissue sparing via spatiotemporal optimization under   equivalent tumor-radical efficacy</h2><p><strong>Authors:Nimita Shinde, Wangyao Li, Ronald C Chen, Hao Gao</strong></p>
<p>Objective: Spatiotemporal optimization in radiation therapy involves determining the optimal number of dose delivery fractions (temporal) and the optimal dose per fraction (spatial). Traditional approaches focus on maximizing the biologically effective dose (BED) to the target while constraining BED to organs-at-risk (OAR), which may lead to insufficient BED for complete tumor cell kill. This work proposes a formulation that ensures adequate BED delivery to the target while minimizing BED to the OAR. Approach: A spatiotemporal optimization model is developed that incorporates an inequality constraint to guarantee sufficient BED for tumor cell kill while minimizing BED to the OAR. The model accounts for tumor proliferation dynamics, including lag time (delay before proliferation begins) and doubling time (time for tumor volume to double), to optimize dose fractionation. Results: The performance of our formulation is evaluated for varying lag and doubling times. The results show that mean BED to the target consistently meets the minimum requirement for tumor cell kill. Additionally, the mean BED to OAR varies based on tumor proliferation dynamics. In the prostate case with lag time of 7 days and doubling time of 2 days, it is observed that mean BED delivered to femoral head is lowest at around 20 fractions, making this an optimal choice. While in the head-and-neck case, mean BED to OAR decreases as the number of fractions increases, suggesting that a higher number of fractions is optimal. Significance: A spatiotemporal optimization model is presented that minimizes BED to the OAR while ensuring sufficient BED for tumor cell kill. By incorporating tumor lag and doubling time, the approach identifies optimal number of fractions. This model can be extended to support hyperfractionation or accelerated fractionation strategies, offering a versatile tool for clinical treatment planning. </p>
<blockquote>
<p>ç›®æ ‡ï¼šæ”¾å°„æ²»ç–—ä¸­çš„æ—¶ç©ºä¼˜åŒ–æ¶‰åŠç¡®å®šæœ€ä½³å‰‚é‡äº¤ä»˜åˆ†æ•°ï¼ˆæ—¶é—´ï¼‰å’Œæ¯åˆ†æ•°çš„æœ€ä½³å‰‚é‡ï¼ˆç©ºé—´ï¼‰ã€‚ä¼ ç»Ÿæ–¹æ³•ä¾§é‡äºæœ€å¤§åŒ–ç›®æ ‡éƒ¨ä½çš„ç”Ÿç‰©æœ‰æ•ˆå‰‚é‡ï¼ˆBEDï¼‰ï¼ŒåŒæ—¶å¯¹é£é™©å™¨å®˜çš„BEDè¿›è¡Œçº¦æŸï¼Œè¿™å¯èƒ½å¯¼è‡´å¯¹é£é™©å™¨å®˜çš„BEDä¸è¶³ï¼Œæ— æ³•å®Œå…¨æ€æ­»è‚¿ç˜¤ç»†èƒã€‚è¿™é¡¹å·¥ä½œæå‡ºäº†ä¸€ç§å…¬å¼ï¼Œåœ¨ç¡®ä¿ç›®æ ‡éƒ¨ä½è·å¾—è¶³å¤Ÿçš„BEDçš„åŒæ—¶ï¼Œæœ€å°åŒ–é£é™©å™¨å®˜çš„BEDã€‚æ–¹æ³•ï¼šå¼€å‘äº†ä¸€ä¸ªæ—¶ç©ºä¼˜åŒ–æ¨¡å‹ï¼Œé€šè¿‡ä¸ç­‰å¼çº¦æŸä¿è¯è¶³å¤Ÿçš„BEDä»¥æ€æ­»è‚¿ç˜¤ç»†èƒï¼ŒåŒæ—¶æœ€å°åŒ–é£é™©å™¨å®˜çš„BEDã€‚è¯¥æ¨¡å‹è€ƒè™‘äº†è‚¿ç˜¤å¢æ®–åŠ¨åŠ›å­¦ï¼ŒåŒ…æ‹¬æ½œä¼æœŸï¼ˆå¢æ®–å¼€å§‹å‰çš„å»¶è¿Ÿæ—¶é—´ï¼‰å’Œå€å¢æ—¶é—´ï¼ˆè‚¿ç˜¤ä½“ç§¯ç¿»å€æ‰€éœ€çš„æ—¶é—´ï¼‰ï¼Œä»¥ä¼˜åŒ–å‰‚é‡åˆ†å‰²ã€‚ç»“æœï¼šæˆ‘ä»¬çš„å…¬å¼åœ¨ä¸åŒæ½œä¼æœŸå’Œå€å¢æ—¶é—´ä¸‹çš„æ€§èƒ½å¾—åˆ°äº†è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼Œç›®æ ‡éƒ¨ä½çš„å¹³å‡BEDå§‹ç»ˆè¾¾åˆ°è‚¿ç˜¤ç»†èƒæ€æ­»çš„æœ€ä½è¦æ±‚ã€‚æ­¤å¤–ï¼Œé£é™©å™¨å®˜çš„å¹³å‡BEDä¼šæ ¹æ®è‚¿ç˜¤å¢æ®–åŠ¨åŠ›å­¦è€Œå˜åŒ–ã€‚åœ¨å‰åˆ—è…ºæ¡ˆä¾‹ä¸­ï¼Œæ½œä¼æœŸä¸º7å¤©ï¼Œå€å¢æ—¶é—´ä¸º2å¤©çš„æƒ…å†µä¸‹ï¼Œè§‚å¯Ÿåˆ°è‚¡éª¨å¤´æ¥æ”¶çš„å¹³å‡BEDåœ¨å¤§çº¦20ä¸ªåˆ†æ•°æ—¶æœ€ä½ï¼Œä½¿å…¶æˆä¸ºæœ€ä½³é€‰æ‹©ã€‚è€Œåœ¨å¤´é¢ˆæ¡ˆä¾‹ä¸­ï¼Œéšç€åˆ†æ•°çš„å¢åŠ ï¼Œé£é™©å™¨å®˜çš„å¹³å‡BEDé™ä½ï¼Œè¡¨æ˜åˆ†æ•°è¶Šå¤šè¶Šç†æƒ³ã€‚æ„ä¹‰ï¼šæå‡ºäº†ä¸€ç§æ—¶ç©ºä¼˜åŒ–æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨ç¡®ä¿è¶³å¤ŸBEDæ€æ­»è‚¿ç˜¤ç»†èƒçš„åŒæ—¶ï¼Œæœ€å°åŒ–äº†é£é™©å™¨å®˜çš„BEDã€‚é€šè¿‡ç»“åˆè‚¿ç˜¤çš„æ½œä¼æœŸå’Œå€å¢æ—¶é—´ï¼Œè¯¥æ–¹æ³•ç¡®å®šäº†æœ€ä½³åˆ†æ•°æ•°é‡ã€‚è¯¥æ¨¡å‹å¯æ‰©å±•åˆ°æ”¯æŒè¶…åˆ†å‰²æˆ–åŠ é€Ÿåˆ†å‰²ç­–ç•¥ï¼Œæˆä¸ºä¸´åºŠæ²»ç–—è®¡åˆ’çš„çµæ´»å·¥å…·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.16333v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æ—¨åœ¨è§£å†³æ”¾å°„æ²»ç–—ä¸­çš„æ—¶ç©ºä¼˜åŒ–é—®é¢˜ï¼ŒåŒ…æ‹¬ç¡®å®šæœ€ä½³å‰‚é‡äº¤ä»˜åˆ†æ•°ï¼ˆæ—¶é—´ï¼‰å’Œæ¯ä¸ªåˆ†æ•°çš„æœ€ä½³å‰‚é‡ï¼ˆç©ºé—´ï¼‰ã€‚ä¼ ç»Ÿæ–¹æ³•ä¾§é‡äºæœ€å¤§åŒ–ç›®æ ‡åŒºåŸŸçš„ç”Ÿç‰©æœ‰æ•ˆå‰‚é‡ï¼ˆBEDï¼‰ï¼ŒåŒæ—¶é™åˆ¶å±é™©å™¨å®˜ï¼ˆOARï¼‰çš„BEDï¼Œå¯èƒ½å¯¼è‡´ä¸è¶³ä»¥å®Œå…¨æ€æ­»è‚¿ç˜¤ç»†èƒã€‚æœ¬æ–‡æå‡ºä¸€ç§ç¡®ä¿ç›®æ ‡åŒºåŸŸè·å¾—è¶³å¤Ÿçš„BEDï¼ŒåŒæ—¶æœ€å°åŒ–OARçš„BEDçš„æ—¶ç©ºä¼˜åŒ–æ¨¡å‹ã€‚è¯¥æ¨¡å‹è€ƒè™‘äº†è‚¿ç˜¤å¢æ®–åŠ¨åŠ›å­¦ï¼ŒåŒ…æ‹¬æ½œä¼æœŸï¼ˆå¢æ®–å¼€å§‹å‰çš„å»¶è¿Ÿæ—¶é—´ï¼‰å’Œå€å¢æ—¶é—´ï¼ˆè‚¿ç˜¤ä½“ç§¯ç¿»å€æ‰€éœ€çš„æ—¶é—´ï¼‰ï¼Œä»¥ä¼˜åŒ–å‰‚é‡åˆ†å‰²ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œåœ¨å„ç§æ½œä¼æœŸå’Œå€å¢æ—¶é—´ä¸‹ï¼Œç›®æ ‡åŒºåŸŸçš„å¹³å‡BEDå§‹ç»ˆè¾¾åˆ°è‚¿ç˜¤ç»†èƒæ€æ­»çš„æœ€ä½è¦æ±‚ã€‚æ­¤å¤–ï¼ŒOARçš„å¹³å‡BEDä¼šæ ¹æ®è‚¿ç˜¤å¢æ®–åŠ¨åŠ›å­¦è€Œå˜åŒ–ã€‚åœ¨å‰åˆ—è…ºæ¡ˆä¾‹ä¸­ï¼Œå½“æ½œä¼æœŸä¸º7å¤©ï¼Œå€å¢æ—¶é—´ä¸º2å¤©æ—¶ï¼Œè‚¡éª¨å¤´çš„å¹³å‡BEDåœ¨å¤§çº¦20ä¸ªåˆ†æ•°æ—¶æœ€ä½ï¼Œè¿™æ˜¯æœ€ä½³é€‰æ‹©ã€‚è€Œåœ¨å¤´é¢ˆæ¡ˆä¾‹ä¸­ï¼Œéšç€åˆ†æ•°çš„å¢åŠ ï¼ŒOARçš„å¹³å‡BEDé™ä½ï¼Œè¡¨æ˜åˆ†æ•°è¶Šå¤šè¶Šç†æƒ³ã€‚æœ¬æ–‡æå‡ºçš„æ—¶ç©ºä¼˜åŒ–æ¨¡å‹å¯åœ¨ç¡®ä¿è‚¿ç˜¤ç»†èƒè·å¾—è¶³å¤ŸBEDçš„åŒæ—¶ï¼Œæœ€å°åŒ–OARçš„BEDã€‚é€šè¿‡è€ƒè™‘è‚¿ç˜¤çš„æ½œä¼æœŸå’Œå€å¢æ—¶é—´ï¼Œè¯¥æ–¹æ³•å¯ä»¥ç¡®å®šæœ€ä½³åˆ†æ•°ã€‚è¯¥æ¨¡å‹å¯æ‰©å±•è‡³æ”¯æŒè¶…åˆ†å‰²æˆ–åŠ é€Ÿåˆ†å‰²ç­–ç•¥ï¼Œä¸ºä¸´åºŠæ²»ç–—æ–¹æ¡ˆè®¾è®¡æä¾›äº†çµæ´»å·¥å…·ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ”¾å°„æ²»ç–—ä¸­çš„æ—¶ç©ºä¼˜åŒ–æ¶‰åŠç¡®å®šæœ€ä½³å‰‚é‡äº¤ä»˜åˆ†æ•°å’Œæ¯ä¸ªåˆ†æ•°çš„æœ€ä½³å‰‚é‡ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•å¯èƒ½ä¸è¶³ä»¥å®Œå…¨æ€æ­»è‚¿ç˜¤ç»†èƒï¼Œè€Œæ–°æ–¹æ³•ç¡®ä¿ç›®æ ‡åŒºåŸŸè·å¾—è¶³å¤Ÿçš„ç”Ÿç‰©æœ‰æ•ˆå‰‚é‡ï¼ˆBEDï¼‰ã€‚</li>
<li>æå‡ºçš„æ¨¡å‹è€ƒè™‘è‚¿ç˜¤å¢æ®–åŠ¨åŠ›å­¦ï¼ŒåŒ…æ‹¬æ½œä¼æœŸå’Œå€å¢æ—¶é—´ï¼Œä»¥ä¼˜åŒ–å‰‚é‡åˆ†å‰²ã€‚</li>
<li>æ¨¡å‹è¯„ä¼°æ˜¾ç¤ºï¼Œåœ¨ä¸åŒæ½œä¼æœŸå’Œå€å¢æ—¶é—´ä¸‹ï¼Œç›®æ ‡åŒºåŸŸçš„å¹³å‡BEDå§‹ç»ˆæ»¡è¶³è‚¿ç˜¤ç»†èƒæ€æ­»çš„æœ€ä½è¦æ±‚ã€‚</li>
<li>OARçš„å¹³å‡BEDä¼šæ ¹æ®è‚¿ç˜¤ç±»å‹å’Œå…¶å¢æ®–åŠ¨åŠ›å­¦è€Œå˜åŒ–ã€‚</li>
<li>åœ¨æŸäº›æ¡ˆä¾‹ä¸­ï¼Œå¦‚å‰åˆ—è…ºæ¡ˆä¾‹å’Œå¤´é¢ˆæ¡ˆä¾‹ï¼Œæ¨¡å‹å¸®åŠ©ç¡®å®šæœ€ä½³çš„åˆ†æ•°æ•°é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.16333">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cc179ae8298bd63ea9be3c9b6de6833f.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Predicting-gene-essentiality-and-drug-response-from-perturbation-screens-in-preclinical-cancer-models-with-LEAP-Layered-Ensemble-of-Autoencoders-and-Predictors"><a href="#Predicting-gene-essentiality-and-drug-response-from-perturbation-screens-in-preclinical-cancer-models-with-LEAP-Layered-Ensemble-of-Autoencoders-and-Predictors" class="headerlink" title="Predicting gene essentiality and drug response from perturbation screens   in preclinical cancer models with LEAP: Layered Ensemble of Autoencoders and   Predictors"></a>Predicting gene essentiality and drug response from perturbation screens   in preclinical cancer models with LEAP: Layered Ensemble of Autoencoders and   Predictors</h2><p><strong>Authors:Barbara Bodinier, Gaetan Dissez, Linus Bleistein, Antonin Dauvin</strong></p>
<p>Preclinical perturbation screens, where the effects of genetic, chemical, or environmental perturbations are systematically tested on disease models, hold significant promise for machine learning-enhanced drug discovery due to their scale and causal nature. Predictive models can infer perturbation responses for previously untested disease models based on molecular profiles. These in silico labels can expand databases and guide experimental prioritization.   However, modelling perturbation-specific effects and generating robust prediction performances across diverse biological contexts remain elusive. We introduce LEAP (Layered Ensemble of Autoencoders and Predictors), a novel ensemble framework to improve robustness and generalization. LEAP leverages multiple DAMAE (Data Augmented Masked Autoencoder) representations and LASSO regressors. By combining diverse gene expression representation models learned from different random initializations, LEAP consistently outperforms state-of-the-art approaches in predicting gene essentiality or drug responses in unseen cell lines, tissues and disease models. Notably, our results show that ensembling representation models, rather than prediction models alone, yields superior predictive performance.   Beyond its performance gains, LEAP is computationally efficient, requires minimal hyperparameter tuning and can therefore be readily incorporated into drug discovery pipelines to prioritize promising targets and support biomarker-driven stratification. The code and datasets used in this work are made publicly available. </p>
<blockquote>
<p>ä¸´åºŠå‰æ‰°åŠ¨ç­›é€‰ï¼ˆåœ¨ç–¾ç—…æ¨¡å‹ä¸Šç³»ç»Ÿåœ°æµ‹è¯•é—ä¼ ã€åŒ–å­¦æˆ–ç¯å¢ƒæ‰°åŠ¨çš„æ•ˆåº”ï¼‰ç”±äºå…¶è§„æ¨¡å’Œå› æœæ€§è´¨ï¼Œåœ¨æœºå™¨å­¦ä¹ è¾…åŠ©è¯ç‰©å‘ç°æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚é¢„æµ‹æ¨¡å‹å¯ä»¥æ ¹æ®åˆ†å­ç‰¹å¾æ¨æ–­ä»¥å‰æœªç»æµ‹è¯•çš„ç–¾ç—…æ¨¡å‹çš„æ‰°åŠ¨ååº”ã€‚è¿™äº›è®¡ç®—æœºç”Ÿæˆçš„æ ‡ç­¾å¯ä»¥æ‰©å¤§æ•°æ®åº“å¹¶æŒ‡å¯¼å®éªŒä¼˜å…ˆæ¬¡åºã€‚ç„¶è€Œï¼Œåœ¨å¤šç§ç”Ÿç‰©èƒŒæ™¯ä¸‹å»ºç«‹ç‰¹å®šæ‰°åŠ¨æ•ˆåº”æ¨¡å‹å’Œç”Ÿæˆç¨³å¥çš„é¢„æµ‹æ€§èƒ½ä»ç„¶æ˜¯ä¸€ä¸ªéš¾é¢˜ã€‚æˆ‘ä»¬å¼•å…¥äº†LEAPï¼ˆAutoencoderså’Œé¢„æµ‹å™¨çš„åˆ†å±‚é›†æˆï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹é›†æˆæ¡†æ¶ï¼Œæ—¨åœ¨æé«˜ç¨³å¥æ€§å’Œé€šç”¨æ€§ã€‚LEAPåˆ©ç”¨å¤šä¸ªDAMAEï¼ˆæ•°æ®å¢å¼ºæ©ç è‡ªåŠ¨ç¼–ç å™¨ï¼‰è¡¨ç¤ºå’ŒLASSOå›å½’å™¨ã€‚é€šè¿‡ç»“åˆä»ä¸åŒéšæœºåˆå§‹åŒ–ä¸­å­¦åˆ°çš„å„ç§åŸºå› è¡¨è¾¾è¡¨ç¤ºæ¨¡å‹ï¼ŒLEAPåœ¨é¢„æµ‹æœªè§è¿‡çš„ç»†èƒç³»ã€ç»„ç»‡å’Œç–¾ç—…æ¨¡å‹ä¸­çš„åŸºå› å¿…éœ€æ€§æˆ–è¯ç‰©ååº”æ–¹é¢å§‹ç»ˆä¼˜äºæœ€æ–°æ–¹æ³•ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œé›†æˆè¡¨ç¤ºæ¨¡å‹ï¼ˆè€Œéä»…é¢„æµ‹æ¨¡å‹ï¼‰å…·æœ‰æ›´å¥½çš„é¢„æµ‹æ€§èƒ½ã€‚é™¤äº†æ€§èƒ½æå‡å¤–ï¼ŒLEAPè®¡ç®—æ•ˆç‡é«˜ï¼Œæ‰€éœ€è¶…å‚æ•°è°ƒæ•´é‡æœ€å°ï¼Œå› æ­¤å¯è½»æ¾èå…¥è¯ç‰©å‘ç°ç®¡é“ï¼Œä»¥ä¼˜å…ˆç­›é€‰æœ‰å‰é€”çš„ç›®æ ‡å¹¶æ”¯æŒç”Ÿç‰©æ ‡å¿—ç‰©é©±åŠ¨çš„åˆ†å±‚ã€‚è¯¥å·¥ä½œä¸­ä½¿ç”¨çš„ä»£ç å’Œæ•°æ®é›†å·²å…¬å¼€å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.15646v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†LEAPï¼ˆåˆ†å±‚è‡ªåŠ¨ç¼–ç å™¨ä¸é¢„æµ‹å™¨é›†åˆï¼‰è¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œç”¨äºæé«˜æ‰°åŠ¨ç­›é€‰é¢„æµ‹æ¨¡å‹çš„ç¨³å¥æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ¡†æ¶ç»“åˆäº†æ•°æ®å¢å¼ºæ©ç è‡ªåŠ¨ç¼–ç å™¨ï¼ˆDAMAEï¼‰å’ŒLASSOå›å½’å™¨çš„å¤šé‡è¡¨ç¤ºï¼Œé€šè¿‡ä»ä¸åŒéšæœºåˆå§‹åŒ–ä¸­å­¦ä¹ å¤šç§åŸºå› è¡¨è¾¾è¡¨ç¤ºæ¨¡å‹ï¼ŒLEAPåœ¨é¢„æµ‹æœªçŸ¥ç»†èƒç³»ã€ç»„ç»‡å’Œç–¾ç—…æ¨¡å‹çš„åŸºå› å¿…éœ€æ€§æˆ–è¯ç‰©ååº”æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚è¯¥ç ”ç©¶è¿˜è¡¨æ˜ï¼Œé›†åˆè¡¨ç¤ºæ¨¡å‹è€Œéå•ä¸€çš„é¢„æµ‹æ¨¡å‹èƒ½å¸¦æ¥æ›´ä¼˜è¶Šçš„é¢„æµ‹æ€§èƒ½ã€‚LEAPé™¤äº†æ€§èƒ½æå‡å¤–ï¼Œè¿˜å…·æœ‰è®¡ç®—æ•ˆç‡é«˜ã€è¶…å‚æ•°è°ƒæ•´éœ€æ±‚å°çš„ç‰¹ç‚¹ï¼Œå¯è½»æ¾èå…¥è¯ç‰©å‘ç°æµç¨‹ï¼Œä¸ºä¼˜å…ˆç­›é€‰æ½œåœ¨ç›®æ ‡å’Œæ”¯æŒç”Ÿç‰©æ ‡å¿—ç‰©é©±åŠ¨åˆ†å±‚æä¾›æ”¯æŒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LEAPæ˜¯ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œç”¨äºæé«˜æœºå™¨å­¦ä¹ èƒ½åŠ›åœ¨è¯ç‰©å‘ç°ä¸­çš„åº”ç”¨ã€‚</li>
<li>LEAPé€šè¿‡ç»“åˆå¤šç§åŸºå› è¡¨è¾¾è¡¨ç¤ºæ¨¡å‹æ¥æé«˜é¢„æµ‹æ€§èƒ½ã€‚</li>
<li>LEAPåœ¨é¢„æµ‹æœªçŸ¥ç»†èƒç³»ã€ç»„ç»‡å’Œç–¾ç—…æ¨¡å‹çš„åŸºå› å¿…éœ€æ€§æˆ–è¯ç‰©ååº”æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>LEAPä½¿ç”¨æ•°æ®å¢å¼ºæ©ç è‡ªåŠ¨ç¼–ç å™¨ï¼ˆDAMAEï¼‰å’ŒLASSOå›å½’å™¨è¿›è¡Œå»ºæ¨¡ã€‚</li>
<li>LEAPçš„ä¼˜åŠ¿åœ¨äºå…¶è®¡ç®—æ•ˆç‡é«˜ã€è¶…å‚æ•°è°ƒæ•´éœ€æ±‚å°ã€‚</li>
<li>é›†åˆè¡¨ç¤ºæ¨¡å‹èƒ½æé«˜é¢„æµ‹æ€§èƒ½ï¼Œè€Œä¸ä»…ä»…æ˜¯é¢„æµ‹æ¨¡å‹æœ¬èº«ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.15646">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-154b04d2ad635cbbcd1edf0a95a26061.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-064bdf0f317640fde0fa33a198a5791e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1f76c118187c7f4dbe993a4efad31492.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Ultra-high-energy-Î³-ray-emission-associated-with-the-tail-of-a-bow-shock-pulsar-wind-nebula"><a href="#Ultra-high-energy-Î³-ray-emission-associated-with-the-tail-of-a-bow-shock-pulsar-wind-nebula" class="headerlink" title="Ultra-high-energy $Î³$-ray emission associated with the tail of a   bow-shock pulsar wind nebula"></a>Ultra-high-energy $Î³$-ray emission associated with the tail of a   bow-shock pulsar wind nebula</h2><p><strong>Authors:Zhen Cao, F. Aharonian, Y. X. Bai, Y. W. Bao, D. Bastieri, X. J. Bi, Y. J. Bi, W. Bian, A. V. Bukevich, C. M. Cai, W. Y. Cao, Zhe Cao, J. Chang, J. F. Chang, A. M. Chen, E. S. Chen, H. X. Chen, Liang Chen, Long Chen, M. J. Chen, M. L. Chen, Q. H. Chen, S. Chen, S. H. Chen, S. Z. Chen, T. L. Chen, X. B. Chen, X. J. Chen, Y. Chen, N. Cheng, Y. D. Cheng, M. C. Chu, M. Y. Cui, S. W. Cui, X. H. Cui, Y. D. Cui B. Z. Dai, H. L. Dai, Z. G. Dai,  Danzengluobu, Y. X. Diao, X. Q. Dong, K. K. Duan, J. H. Fan, Y. Z. Fan, J. Fang, J. H. Fang, K. Fang, C. F. Feng, H. Feng L. Feng, S. H. Feng, X. T. Feng, Y. Feng, Y. L. Feng, S. Gabici, B. Gao, C. D. Gao, Q. Gao, W. Gao, W. K. Gao, M. M. Ge, T. T. Ge L. S. Geng, G. Giacinti, G. H. Gong, Q. B. Gou, M. H. Gu, F. L. Guo, J. Guo, X. L. Guo, Y. Q. Guo, Y. Y. Guo, Y. A. Han, O. A. Hannuksela, M. Hasan, H. H. He, H. N. He, J. Y. He, X. Y. He, Y. He, S. HernÃ¡ndez-Cadena, Y. K. Hor B. W. Hou, C. Hou, X. Hou, H. B. Hu, S. C. Hu, C. Huang, D. H. Huang, J. J. Huang, T. Q. Huang, W. J. Huang X. T. Huang, X. Y. Huang, Y. Huang, Y. Y. Huang, X. L. Ji, H. Y. Jia, K. Jia, H. B. Jiang, K. Jiang, X. W. Jiang, Z. J. Jiang, M. Jin, S. Kaci, M. M. Kang, I. Karpikov, D. Khangulyan, D. Kuleshov, K. Kurinov, B. B. Li, Cheng Li, Cong Li, D. Li, F. Li, H. B. Li, H. C. Li, Jian Li, Jie Li, K. Li, L. Li, R. L. Li, S. D. Li, T. Y. Li, W. L. Li, X. R. Li, Xin Li, Y. Z. Li, Zhe Li, Zhuo Li, E. W. Liang Y. F. Liang S. J. Lin B. Liu, C. Liu, D. Liu, D. B. Liu, H. Liu, H. D. Liu, J. Liu, J. L. Liu, J. R. Liu, M. Y. Liu, R. Y. Liu, S. M. Liu, W. Liu, X. Liu, Y. Liu, Y. Liu, Y. N. Liu, Y. Q. Lou, Q. Luo Y. Luo, H. K. Lv, B. Q. Ma, L. L. Ma, X. H. Ma, J. R. Mao, Z. Min, W. Mitthumsiri, G. B. Mou, H. J. Mu, Y. C. Nan, A. Neronov, K. C. Y. Ng, M. Y. Ni, L. Nie, L. J. Ou, P. Pattarakijwanich, Z. Y. Pei, J. C. Qi, M. Y. Qi, J. J. Qin, A. Raza, C. Y. Ren, D. Ruffolo, A. SÃ¡iz, M. Saeed, D. Semikoz, L. Shao, O. Shchegolev, Y. Z. Shen, X. D. Sheng, Z. D. Shi, F. W. Shu, H. C. Song, Yu. V. Stenkin, V. Stepanov, Y. Su, D. X. Sun, Q. N. Sun, X. N. Sun Z. B. Sun, J. Takata, P. H. T. Tam H. B. Tan, Q. W. Tang, R. Tang, Z. B. Tang, W. W. Tian, C. N. Tong, L. H. Wan C. Wang, G. W. Wang, H. G. Wang, H. H. Wang J. C. Wang, K. Wang, Kai Wang, Kai Wang, L. P. Wang, L. Y. Wang, L. Y. Wang, R. Wang, W. Wang X. G. Wang X. J. Wang, X. Y. Wang, Y. Wang, Y. D. Wang, Z. H. Wang, Z. X. Wang, Zheng Wang, D. M. Wei, J. J. Wei, Y. J. Wei, T. Wen, S. S. Weng, C. Y. Wu, H. R. Wu, Q. W. Wu, S. Wu, X. F. Wu, Y. S. Wu, S. Q. Xi, J. Xia, J. J. Xia, G. M. Xiang, D. X. Xiao, G. Xiao, Y. L. Xin, Y. Xing, D. R. Xiong, Z. Xiong, D. L. Xu, R. F. Xu, R. X. Xu, W. L. Xu, L. Xue, D. H. Yan, J. Z. Yan, T. Yan, C. W. Yang, C. Y. Yang, F. F. Yang, L. L. Yang M. J. Yang, R. Z. Yang, W. X. Yang, Y. H. Yao, Z. G. Yao, X. A. Ye, L. Q. Yin, N. Yin, X. H. You, Z. Y. You, Y. H. Yu, Q. Yuan, H. Yue, H. D. Zeng, T. X. Zeng, W. Zeng, M. Zha, B. B. Zhang, B. T. Zhang, F. Zhang, H. Zhang, H. M. Zhang H. Y. Zhang, J. L. Zhang, Li Zhang, P. F. Zhang, P. P. Zhang, R. Zhang, S. R. Zhang, S. S. Zhang, W. Y. Zhang, X. Zhang, X. P. Zhang, Yi Zhang, Yong Zhang, Z. P. Zhang, J. Zhao, L. Zhao, L. Z. Zhao, S. P. Zhao, X. H. Zhao, Z. H. Zhao, F. Zheng, W. J. Zhong, B. Zhou, H. Zhou, J. N. Zhou, M. Zhou, P. Zhou, R. Zhou, X. X. Zhou, X. X. Zhou, B. Y. Zhu, C. G. Zhu, F. R. Zhu, H. Zhu, K. J. Zhu, Y. C. Zou, X. Zuo</strong></p>
<p>In this study, we present a comprehensive analysis of an unidentified point-like ultra-high-energy (UHE) $\gamma$-ray source, designated as 1LHAASO J1740+0948u, situated in the vicinity of the middle-aged pulsar PSR J1740+1000. The detection significance reached 17.1$\sigma$ (9.4$\sigma$) above 25$,$TeV (100$,$TeV). The source energy spectrum extended up to 300$,$TeV, which was well fitted by a log-parabola function with $N0 &#x3D; (1.93\pm0.23) \times 10^{-16} \rm{TeV^{-1},cm^{-2},s^{-2}}$, $\alpha &#x3D; 2.14\pm0.27$, and $\beta &#x3D; 1.20\pm0.41$ at E0 &#x3D; 30$,$TeV. The associated pulsar, PSR J1740+1000, resides at a high galactic latitude and powers a bow-shock pulsar wind nebula (BSPWN) with an extended X-ray tail. The best-fit position of the gamma-ray source appeared to be shifted by $0.2^{\circ}$ with respect to the pulsar position. As the (i) currently identified pulsar halos do not demonstrate such offsets, and (ii) centroid of the gamma-ray emission is approximately located at the extension of the X-ray tail, we speculate that the UHE $\gamma$-ray emission may originate from re-accelerated electron&#x2F;positron pairs that are advected away in the bow-shock tail. </p>
<blockquote>
<p>åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å¯¹ä¸€ä¸ªæœªç¡®å®šçš„ç‚¹çŠ¶è¶…é«˜èƒ½ï¼ˆUHEï¼‰Î³å°„çº¿æºè¿›è¡Œäº†ç»¼åˆåˆ†æï¼Œè¯¥æºè¢«æŒ‡å®šä¸º1LHAASO J1740+0948uï¼Œä½äºä¸­å¹´è„‰å†²æ˜ŸPSR J1740+1000é™„è¿‘ã€‚æ£€æµ‹åˆ°çš„ä¿¡å·é‡è¦æ€§è¾¾åˆ°äº†è¶…è¿‡25TeVï¼ˆå¦‚æœä¸ºæœªæ ‡è¯†ç‰ˆæœ¬åˆ™ä¸ºæ›´ç½•è§çš„å¤§äºæˆ–ç­‰äºæ•°ç™¾TeVçš„æç«¯å€¼ï¼‰ã€‚èƒ½é‡è°±å»¶ä¼¸è‡³é«˜è¾¾æ•°ç™¾TeVï¼Œç”¨å¯¹æ•°æŠ›ç‰©çº¿å‡½æ•°æ‹Ÿåˆå¾—å¾ˆå¥½ï¼Œå‚æ•°ä¸º$N0 &#x3D; (1.93Â±0.23) Ã— 10^{-16} \rm{TeV^{-1},cm^{-2},s^{-2}}$ï¼ŒÎ±å’ŒÎ²çš„å€¼åˆ†åˆ«å¤§äºåˆå§‹çŠ¶æ€æ•°ååˆ°æ•°ç™¾å€çš„Î±å€¼ä¸ºè¶…å‡ºèƒ½é‡è°±æŒ‡æ•°åˆ†å¸ƒçš„å€¼ï¼Œä¸”æ¥è¿‘æ‹Ÿåˆå€¼çš„ä¼°è®¡å€¼åˆ†åˆ«ä¸º$Î± &#x3D; 2.14Â±0.27å’ŒÎ² &#x3D; 1.20Â±0.41ã€‚åœ¨æ¥è¿‘E0 &#x3D; 30TeVæ—¶ï¼Œå…³è”çš„è„‰å†²æ˜ŸPSR J1740+1000ä½äºé«˜é“¶æ²³çº¬åº¦å¤„ï¼Œå¹¶ä¸”æ¨åŠ¨äº†ä¸€ä¸ªå¸¦æœ‰å»¶ä¼¸Xå°„çº¿å°¾éƒ¨çš„å¼“å½¢å†²å‡»è„‰å†²æ˜Ÿé£æ˜Ÿäº‘ï¼ˆBSPWNï¼‰ã€‚Î³å°„çº¿æºçš„æœ€ä½³æ‹Ÿåˆä½ç½®ä¼¼ä¹ç›¸å¯¹äºè„‰å†²æ˜Ÿä½ç½®åç§»äº†$0.2^{\circ}$ã€‚ç”±äºç›®å‰å·²çŸ¥çš„è„‰å†²æ˜Ÿæ™•æ²¡æœ‰æ˜¾ç¤ºå‡ºè¿™ç§åç§»ï¼Œå¹¶ä¸”Î³å°„çº¿å‘å°„çš„ä¸­å¿ƒç‚¹å¤§è‡´ä½äºXå°„çº¿å°¾éƒ¨çš„å»¶ä¼¸å¤„ï¼Œå› æ­¤æˆ‘ä»¬æ¨æµ‹è¶…é«˜èƒ½Î³å°„çº¿å‘å°„å¯èƒ½æ¥æºäºåœ¨å¼“å½¢å†²å‡»å°¾éƒ¨ä¸­é‡æ–°åŠ é€Ÿçš„ç”µå­&#x2F;æ­£ç”µå­å¯¹è¢«æ’é™¤çš„è¿‡ç¨‹ã€‚æ ¹æ®è§‚æµ‹å’Œç†è®ºçš„åˆ†æå’Œæ¯”å¯¹ç»“åˆç ”ç©¶æˆæœæ„å»ºè¿›ä¸€æ­¥çš„åŸºç¡€ä¿¡æ¯çš„ç ”ç©¶è¿™ä¸€è¡¨æ˜ç¤ºå‡ºäº†ä¸€ä¸ªæ´»è·ƒçš„ä»‹è´¨æ•£å‘å·²ç»å‡ç»“åŠåµŒå…¥å…¶å†…çš„ä¸€ä¸ªå¼ºå¤§çš„å®‡å®™å¤©ä½“ç°è±¡ï¼Œå¹¶ä¸”æ­ç¤ºå‡ºä¸€ç§æ–°çš„å¯èƒ½æœºåˆ¶æ¥æ­ç¤ºÎ³å°„çº¿æºçš„ç‰¹æ€§ä»¥åŠå®ƒçš„æºå¤´å¯èƒ½æºè‡ªä½•å¤„ã€‚è¿™ä¸ªÎ³å°„çº¿æºå¯èƒ½æ˜¯å®‡å®™æ¢ç´¢çš„æ–°é‡Œç¨‹ç¢‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.15447v2">PDF</a> Corrected spelling errors in several author names</p>
<p><strong>Summary</strong><br>    æœ¬ç ”ç©¶å…¨é¢åˆ†æäº†ä¸€ä¸ªæœªè¯†åˆ«çš„è¶…é«˜èƒ½ç‚¹æºï¼ˆUHEï¼‰ï¼Œå‘½åä¸º1LHAASO J1740+0948uï¼Œä½äºä¸­å¹´è„‰å†²æ˜ŸPSR J1740+1000é™„è¿‘ã€‚è¯¥æºèƒ½é‡è°±å»¶ä¼¸è‡³300TeVï¼Œä¸å¯¹æ•°æŠ›ç‰©çº¿å‡½æ•°å»åˆè‰¯å¥½ã€‚æ¨æµ‹è¶…é«˜èƒ½Î³å°„çº¿æºè‡ªå†åŠ é€Ÿçš„ç”µå­&#x2F;æ­£ç”µå­å¯¹åœ¨å¼“å½¢å†²å‡»å°¾éƒ¨çš„å¯¹æµã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶æŠ¥å‘Šäº†ä¸€ä¸ªè¶…é«˜èƒ½ç‚¹æºï¼ˆUHEï¼‰Î³å°„çº¿æº1LHAASO J1740+0948uçš„åˆ†æç»“æœã€‚</li>
<li>è¯¥æºä¸ä¸­å¹´è„‰å†²æ˜ŸPSR J1740+1000ä½äºåŒä¸€åŒºåŸŸã€‚</li>
<li>æºèƒ½é‡è°±å»¶ä¼¸è‡³é«˜è¾¾300TeVï¼Œå¯ç”¨å¯¹æ•°æŠ›ç‰©çº¿å‡½æ•°æ‹Ÿåˆã€‚</li>
<li>Î³å°„çº¿æºçš„æœ€ä½³æ‹Ÿåˆä½ç½®ä¸è„‰å†²æ˜Ÿä½ç½®æœ‰åç§»ã€‚</li>
<li>ç›®å‰å·²çŸ¥çš„è„‰å†²æ˜Ÿæ™•å¹¶ä¸æ˜¾ç¤ºæ­¤ç±»åç§»ã€‚</li>
<li>Î³å°„çº¿å‘å°„çš„ä¸­å¿ƒç‚¹ä½äºXå°„çº¿å°¾å·´çš„å»¶ä¼¸å¤„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.15447">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-4ee59e5806c5124bd0cf7fd3c0245159.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Pseudoinverse-Diffusion-Models-for-Generative-CT-Image-Reconstruction-from-Low-Dose-Data"><a href="#Pseudoinverse-Diffusion-Models-for-Generative-CT-Image-Reconstruction-from-Low-Dose-Data" class="headerlink" title="Pseudoinverse Diffusion Models for Generative CT Image Reconstruction   from Low Dose Data"></a>Pseudoinverse Diffusion Models for Generative CT Image Reconstruction   from Low Dose Data</h2><p><strong>Authors:Matthew Tivnan, Dufan Wu, Quanzheng Li</strong></p>
<p>Score-based diffusion models have significantly advanced generative deep learning for image processing. Measurement conditioned models have also been applied to inverse problems such as CT reconstruction. However, the conventional approach, culminating in white noise, often requires a high number of reverse process update steps and score function evaluations. To address this limitation, we propose an alternative forward process in score-based diffusion models that aligns with the noise characteristics of low-dose CT reconstructions, rather than converging to white noise. This method significantly reduces the number of required score function evaluations, enhancing efficiency and maintaining familiar noise textures for radiologists, Our approach not only accelerates the generative process but also retains CT noise correlations, a key aspect often criticized by clinicians for deep learning reconstructions. In this work, we rigorously define a matrix-controlled stochastic process for this purpose and validate it through computational experiments. Using a dataset from The Cancer Genome Atlas Liver Hepatocellular Carcinoma (TCGA-LIHC), we simulate low-dose CT measurements and train our model, comparing it with a baseline scalar diffusion process and conditional diffusion model. Our results demonstrate the superiority of our pseudoinverse diffusion model in terms of efficiency and the ability to produce high-quality reconstructions that are familiar in texture to medical professionals in a low number of score function evaluations. This advancement paves the way for more efficient and clinically practical diffusion models in medical imaging, particularly beneficial in scenarios demanding rapid reconstructions or lower radiation exposure. </p>
<blockquote>
<p>åŸºäºå¾—åˆ†çš„æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒå¤„ç†ç”Ÿæˆæ·±åº¦å­¦ä¹ æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚æµ‹é‡æ¡ä»¶æ¨¡å‹ä¹Ÿè¢«åº”ç”¨äºåå‘é—®é¢˜ï¼Œå¦‚CTé‡å»ºã€‚ç„¶è€Œï¼Œä¼ ç»Ÿæ–¹æ³•ä»¥ç™½å™ªå£°ä¸ºç»ˆç‚¹ï¼Œé€šå¸¸éœ€è¦å¤§é‡çš„åå‘è¿‡ç¨‹æ›´æ–°æ­¥éª¤å’Œå¾—åˆ†å‡½æ•°è¯„ä¼°ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå¾—åˆ†çš„æ‰©æ•£æ¨¡å‹ä¸­çš„æ›¿ä»£å‰å‘è¿‡ç¨‹ï¼Œè¯¥æ–¹æ³•ä¸ä½å‰‚é‡CTé‡å»ºçš„å™ªå£°ç‰¹æ€§ç›¸ç¬¦ï¼Œè€Œä¸æ˜¯æ”¶æ•›äºç™½å™ªå£°ã€‚è¿™ç§æ–¹æ³•æ˜¾è‘—å‡å°‘äº†æ‰€éœ€çš„å¾—åˆ†å‡½æ•°è¯„ä¼°æ¬¡æ•°ï¼Œæé«˜äº†æ•ˆç‡ï¼ŒåŒæ—¶ä¿æŒäº†æ”¾å°„ç§‘åŒ»ç”Ÿç†Ÿæ‚‰çš„å™ªå£°çº¹ç†ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…åŠ é€Ÿäº†ç”Ÿæˆè¿‡ç¨‹ï¼Œè¿˜ä¿ç•™äº†CTå™ªå£°ç›¸å…³æ€§ï¼Œè¿™æ˜¯æ·±åº¦å­¦ä¹ é‡å»ºç»å¸¸å—åˆ°ä¸´åºŠåŒ»ç”Ÿæ‰¹è¯„çš„ä¸€ä¸ªå…³é”®æ–¹é¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä¸¥æ ¼å®šä¹‰äº†ä¸€ä¸ªç”¨äºæ­¤ç›®çš„çš„çŸ©é˜µæ§åˆ¶éšæœºè¿‡ç¨‹ï¼Œå¹¶é€šè¿‡è®¡ç®—å®éªŒå¯¹å…¶è¿›è¡Œäº†éªŒè¯ã€‚æˆ‘ä»¬ä½¿ç”¨ç™Œç—‡åŸºå› ç»„å›¾è°±è‚ç»†èƒè‚ç™Œï¼ˆTCGA-LIHCï¼‰æ•°æ®é›†æ¨¡æ‹Ÿä½å‰‚é‡CTæµ‹é‡å€¼å¹¶è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ï¼Œå°†å…¶ä¸åŸºçº¿æ ‡é‡æ‰©æ•£è¿‡ç¨‹å’Œæ¡ä»¶æ‰©æ•£æ¨¡å‹è¿›è¡Œæ¯”è¾ƒã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ä¼ªé€†æ‰©æ•£æ¨¡å‹åœ¨æ•ˆç‡å’Œäº§ç”Ÿé«˜è´¨é‡é‡å»ºæ–¹é¢å…·æœ‰ä¼˜åŠ¿ï¼Œè¿™äº›é‡å»ºåœ¨ä¸“ä¸šåŒ»ç–—äººå‘˜çš„çº¹ç†ä¸Šéå¸¸ç†Ÿæ‚‰ï¼Œå¹¶ä¸”åœ¨å¾—åˆ†å‡½æ•°è¯„ä¼°æ¬¡æ•°è¾ƒå°‘çš„æƒ…å†µä¸‹å³å¯å®ç°ã€‚è¿™ä¸€è¿›å±•ä¸ºåŒ»å­¦æˆåƒä¸­æ›´é«˜æ•ˆã€æ›´å®ç”¨çš„æ‰©æ•£æ¨¡å‹é“ºå¹³äº†é“è·¯ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦å¿«é€Ÿé‡å»ºæˆ–ä½è¾å°„æš´éœ²çš„åœºæ™¯ä¸­ç‰¹åˆ«æœ‰ç›Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.15064v1">PDF</a> </p>
<p><strong>Summary</strong><br>     åŸºäºåˆ†æ•°çš„æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒå¤„ç†çš„ç”Ÿæˆæ·±åº¦å­¦ä¹ é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚é’ˆå¯¹ä¼ ç»Ÿæ–¹æ³•åœ¨é«˜å™ªå£°ç¯å¢ƒä¸‹çš„å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸ä½å‰‚é‡CTé‡å»ºå™ªå£°ç‰¹æ€§ç›¸åŒ¹é…çš„æ‰©æ•£æ¨¡å‹æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•ä¸ä»…æé«˜äº†ç”Ÿæˆè¿‡ç¨‹çš„æ•ˆç‡ï¼Œä¿ç•™äº†CTå™ªå£°ç›¸å…³æ€§ï¼Œè¿˜å‡å°‘äº†æ‰€éœ€çš„åˆ†æ•°å‡½æ•°è¯„ä¼°æ¬¡æ•°ã€‚é€šè¿‡è®¡ç®—å®éªŒéªŒè¯ï¼Œè¯¥æ–¹æ³•åœ¨æ•ˆç‡å’Œè´¨é‡ä¸Šå‡è¡¨ç°å‡ºä¼˜è¶Šæ€§ï¼Œæœ‰æœ›ä¸ºåŒ»å­¦æˆåƒä¸­çš„æ‰©æ•£æ¨¡å‹å¸¦æ¥æ›´é«˜æ•ˆä¸”æ›´å®ç”¨çš„ä¸´åºŠåº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ†æ•°æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆæ·±åº¦å­¦ä¹ ä¸­æœ‰æ˜¾è‘—è¿›å±•ã€‚</li>
<li>ä¼ ç»Ÿæ‰©æ•£æ¨¡å‹åœ¨é«˜å™ªå£°ç¯å¢ƒä¸‹å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ä¸ä½å‰‚é‡CTé‡å»ºå™ªå£°ç‰¹æ€§ç›¸åŒ¹é…çš„æ‰©æ•£æ¨¡å‹æ–°æ–¹æ³•ã€‚</li>
<li>æ–°æ–¹æ³•æé«˜äº†ç”Ÿæˆè¿‡ç¨‹çš„æ•ˆç‡ï¼Œå‡å°‘äº†åˆ†æ•°å‡½æ•°è¯„ä¼°æ¬¡æ•°ã€‚</li>
<li>æ–¹æ³•ä¿ç•™äº†CTå™ªå£°ç›¸å…³æ€§ï¼Œæ›´ç¬¦åˆåŒ»å­¦ä¸“ä¸šäººå£«çš„ç†Ÿæ‚‰çº¹ç†ã€‚</li>
<li>é€šè¿‡è®¡ç®—å®éªŒéªŒè¯ï¼Œæ–°æ–¹æ³•åœ¨æ•ˆç‡å’Œè´¨é‡ä¸Šè¡¨ç°å‡ºä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.15064">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f22f47c99be662814be3d01d7124a780.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-257fdda0cea318a5f1d044bf7fe4fc01.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca07b3be2139e69c156aaa1147a0c4ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea49b7bdb6688dead135fd151c140707.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="UNGT-Ultrasound-Nasogastric-Tube-Dataset-for-Medical-Image-Analysis"><a href="#UNGT-Ultrasound-Nasogastric-Tube-Dataset-for-Medical-Image-Analysis" class="headerlink" title="UNGT: Ultrasound Nasogastric Tube Dataset for Medical Image Analysis"></a>UNGT: Ultrasound Nasogastric Tube Dataset for Medical Image Analysis</h2><p><strong>Authors:Zhaoshan Liu, Chau Hung Lee, Qiujie Lv, Nicole Kessa Wee, Lei Shen</strong></p>
<p>We develop a novel ultrasound nasogastric tube (UNGT) dataset to address the lack of public nasogastric tube datasets. The UNGT dataset includes 493 images gathered from 110 patients with an average image resolution of approximately 879 $\times$ 583. Four structures, encompassing the liver, stomach, tube, and pancreas are precisely annotated. Besides, we propose a semi-supervised adaptive-weighting aggregation medical segmenter to address data limitation and imbalance concurrently. The introduced adaptive weighting approach tackles the severe unbalanced challenge by regulating the loss across varying categories as training proceeds. The presented multiscale attention aggregation block bolsters the feature representation by integrating local and global contextual information. With these, the proposed AAMS can emphasize sparse or small structures and feature enhanced representation ability. We perform extensive segmentation experiments on our UNGT dataset, and the results show that AAMS outperforms existing state-of-the-art approaches to varying extents. In addition, we conduct comprehensive classification experiments across varying state-of-the-art methods and compare their performance. The dataset and code will be available upon publication at <a target="_blank" rel="noopener" href="https://github.com/NUS-Tim/UNGT">https://github.com/NUS-Tim/UNGT</a>. </p>
<blockquote>
<p>ä¸ºäº†è§£å†³å…¬å¼€é¼»èƒƒç®¡æ•°æ®é›†ç¼ºä¹çš„é—®é¢˜ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªæ–°å‹è¶…å£°é¼»èƒƒç®¡ï¼ˆUNGTï¼‰æ•°æ®é›†ã€‚UNGTæ•°æ®é›†åŒ…å«ä»110ä½æ‚£è€…æ”¶é›†çš„493å¼ å›¾åƒï¼Œå¹³å‡å›¾åƒåˆ†è¾¨ç‡çº¦ä¸º879Ã—583ã€‚è‚è„ã€èƒƒã€ç®¡å’Œèƒ°è…ºå››ä¸ªç»“æ„è¢«ç²¾ç¡®æ ‡æ³¨ã€‚æ­¤å¤–ï¼Œä¸ºäº†è§£å†³æ•°æ®é™åˆ¶å’Œä¸å¹³è¡¡é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŠç›‘ç£è‡ªé€‚åº”æƒé‡èšåˆåŒ»å­¦åˆ†å‰²å™¨ã€‚æ‰€å¼•å…¥çš„è‡ªé€‚åº”æƒé‡æ–¹æ³•é€šè¿‡è°ƒèŠ‚ä¸åŒç±»åˆ«çš„æŸå¤±æ¥è§£å†³ä¸¥é‡ä¸å¹³è¡¡çš„æŒ‘æˆ˜ï¼Œéšç€è®­ç»ƒçš„è¿›è¡Œï¼ŒæŸå¤±ä¼šé€æ¸è°ƒæ•´ã€‚æå‡ºçš„å¤šå°ºåº¦æ³¨æ„åŠ›èšåˆå—é€šè¿‡æ•´åˆå±€éƒ¨å’Œå…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯æ¥åŠ å¼ºç‰¹å¾è¡¨ç¤ºã€‚é€šè¿‡è¿™äº›ï¼Œæ‰€æå‡ºçš„å¤šå°ºåº¦æ³¨æ„åŠ›èšåˆæ–¹æ³•èƒ½å¤Ÿçªå‡ºç¨€ç–æˆ–å°ç»“æ„ï¼Œå¹¶å¢å¼ºç‰¹å¾è¡¨ç¤ºèƒ½åŠ›ã€‚æˆ‘ä»¬åœ¨UNGTæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§é‡çš„åˆ†å‰²å®éªŒï¼Œç»“æœè¡¨æ˜ï¼Œåœ¨å„ç§ç¨‹åº¦ä¸Šï¼ŒAAMSè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¿›è¡Œäº†å…¨é¢çš„åˆ†ç±»å®éªŒï¼Œæ¯”è¾ƒäº†ä¸åŒæœ€å…ˆè¿›æ–¹æ³•çš„æ€§èƒ½ã€‚æ•°æ®é›†å’Œä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/NUS-Tim/UNGT%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/NUS-Tim/UNGTä¸Šå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14915v1">PDF</a> 31 pages, 6 figures</p>
<p><strong>Summary</strong><br>è¶…å£°èƒƒç®¡æ•°æ®é›†å¼€å‘åŠå…¶ç›¸å…³æŠ€æœ¯ç ”ç©¶ã€‚ä¸ºè§£å†³å…¬å…±èƒƒç®¡æ•°æ®é›†ç¼ºä¹çš„é—®é¢˜ï¼Œåˆ›å»ºäº†æ–°å‹è¶…å£°èƒƒç®¡ï¼ˆUNGTï¼‰æ•°æ®é›†ï¼ŒåŒ…å«æ¥è‡ª110ä½æ‚£è€…çš„493å¼ å›¾åƒã€‚å›¾åƒå¹³å‡åˆ†è¾¨ç‡çº¦ä¸º879Ã—583ï¼Œç²¾ç¡®æ ‡æ³¨äº†è‚ã€èƒƒã€èƒƒç®¡å’Œèƒ°è…ºå››ä¸ªç»“æ„ã€‚æå‡ºåŠç›‘ç£è‡ªé€‚åº”æƒé‡èšåˆåŒ»å­¦åˆ†å‰²å™¨æ¥è§£å†³æ•°æ®é™åˆ¶å’Œä¸å¹³è¡¡é—®é¢˜ã€‚è‡ªé€‚åº”æƒé‡æ–¹æ³•é€šè¿‡è®­ç»ƒè¿‡ç¨‹ä¸­çš„æŸå¤±è°ƒèŠ‚æ¥è§£å†³ä¸¥é‡ä¸å¹³è¡¡æŒ‘æˆ˜ã€‚å¼•å…¥çš„å¤šå°ºåº¦æ³¨æ„åŠ›èšåˆå—é€šè¿‡æ•´åˆå±€éƒ¨å’Œå…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯åŠ å¼ºç‰¹å¾è¡¨ç¤ºã€‚å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„AAMSæ–¹æ³•åœ¨UNGTæ•°æ®é›†ä¸Šçš„åˆ†å‰²å’Œåˆ†ç±»æ€§èƒ½å‡ä¼˜äºç°æœ‰å…ˆè¿›æ–¹æ³•ã€‚æ•°æ®é›†å’Œä»£ç å°†åœ¨å‘å¸ƒæ—¶äº<a target="_blank" rel="noopener" href="https://github.com/NUS-Tim/UNGT%E4%B8%8A%E6%8F%9B%E4%BA%8B%E5%AE%9E%E6%9C%AF%E5%92%8C%E7%BB%BC%E5%90%88%E5%BC%BA%E5%A4%AF%E7%9A%84%E5%AF%BC%E5%AF%BC">https://github.com/NUS-Tim/UNGTä¸Šæä¾›ã€‚</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¼€å‘æ–°å‹è¶…å£°èƒƒç®¡ï¼ˆUNGTï¼‰æ•°æ®é›†ï¼ŒåŒ…å«æ¥è‡ª110ä½æ‚£è€…çš„493å¼ å›¾åƒã€‚</li>
<li>å›¾åƒç²¾ç¡®æ ‡æ³¨äº†è‚ã€èƒƒã€èƒƒç®¡å’Œèƒ°è…ºå››ä¸ªç»“æ„ã€‚</li>
<li>æå‡ºåŠç›‘ç£è‡ªé€‚åº”æƒé‡èšåˆåŒ»å­¦åˆ†å‰²å™¨è§£å†³æ•°æ®é™åˆ¶å’Œä¸å¹³è¡¡é—®é¢˜ã€‚</li>
<li>è‡ªé€‚åº”æƒé‡æ–¹æ³•é€šè¿‡è®­ç»ƒè¿‡ç¨‹ä¸­çš„æŸå¤±è°ƒèŠ‚è§£å†³ä¸¥é‡ä¸å¹³è¡¡æŒ‘æˆ˜ã€‚</li>
<li>å¤šå°ºåº¦æ³¨æ„åŠ›èšåˆå—å¼ºåŒ–ç‰¹å¾è¡¨ç¤ºï¼Œæé«˜åˆ†å‰²å’Œåˆ†ç±»æ€§èƒ½ã€‚</li>
<li>AAMSæ–¹æ³•åœ¨UNGTæ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºç°æœ‰å…ˆè¿›æ–¹æ³•ã€‚</li>
<li>æ•°æ®é›†å’Œä»£ç å°†åœ¨å‘å¸ƒæ—¶å…¬å¼€ï¼Œå¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/NUS-Tim/UNGT">https://github.com/NUS-Tim/UNGTè®¿é—®ã€‚</a></li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14915">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-48d9b895a5336715a8739b8c180311d1.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Vision-Foundation-Models-in-Medical-Image-Analysis-Advances-and-Challenges"><a href="#Vision-Foundation-Models-in-Medical-Image-Analysis-Advances-and-Challenges" class="headerlink" title="Vision Foundation Models in Medical Image Analysis: Advances and   Challenges"></a>Vision Foundation Models in Medical Image Analysis: Advances and   Challenges</h2><p><strong>Authors:Pengchen Liang, Bin Pu, Haishan Huang, Yiwei Li, Hualiang Wang, Weibo Ma, Qing Chang</strong></p>
<p>The rapid development of Vision Foundation Models (VFMs), particularly Vision Transformers (ViT) and Segment Anything Model (SAM), has sparked significant advances in the field of medical image analysis. These models have demonstrated exceptional capabilities in capturing long-range dependencies and achieving high generalization in segmentation tasks. However, adapting these large models to medical image analysis presents several challenges, including domain differences between medical and natural images, the need for efficient model adaptation strategies, and the limitations of small-scale medical datasets. This paper reviews the state-of-the-art research on the adaptation of VFMs to medical image segmentation, focusing on the challenges of domain adaptation, model compression, and federated learning. We discuss the latest developments in adapter-based improvements, knowledge distillation techniques, and multi-scale contextual feature modeling, and propose future directions to overcome these bottlenecks. Our analysis highlights the potential of VFMs, along with emerging methodologies such as federated learning and model compression, to revolutionize medical image analysis and enhance clinical applications. The goal of this work is to provide a comprehensive overview of current approaches and suggest key areas for future research that can drive the next wave of innovation in medical image segmentation. </p>
<blockquote>
<p>éšç€è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œç‰¹åˆ«æ˜¯è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰å’Œä»»ä½•åˆ†æ®µæ¨¡å‹ï¼ˆSAMï¼‰ï¼ŒåŒ»å­¦å›¾åƒåˆ†æé¢†åŸŸå–å¾—äº†é‡å¤§è¿›å±•ã€‚è¿™äº›æ¨¡å‹åœ¨æ•æ‰é•¿è·ç¦»ä¾èµ–å…³ç³»å’Œå®ç°åˆ†å‰²ä»»åŠ¡çš„é«˜æ³›åŒ–æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå°†è¿™äº›å¤§å‹æ¨¡å‹é€‚åº”åŒ»å­¦å›¾åƒåˆ†æé¢ä¸´å‡ ä¸ªæŒ‘æˆ˜ï¼ŒåŒ…æ‹¬åŒ»å­¦å›¾åƒå’Œè‡ªç„¶å›¾åƒé¢†åŸŸä¹‹é—´çš„å·®å¼‚ã€å¯¹æœ‰æ•ˆçš„æ¨¡å‹é€‚åº”ç­–ç•¥çš„éœ€æ±‚ä»¥åŠå°è§„æ¨¡åŒ»å­¦æ•°æ®é›†çš„é™åˆ¶ã€‚æœ¬æ–‡ç»¼è¿°äº†å°†VFMsé€‚åº”åŒ»å­¦å›¾åƒåˆ†å‰²çš„æœ€æ–°ç ”ç©¶ï¼Œé‡ç‚¹å…³æ³¨é¢†åŸŸé€‚åº”ã€æ¨¡å‹å‹ç¼©å’Œè”é‚¦å­¦ä¹ çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬è®¨è®ºäº†åŸºäºé€‚é…å™¨çš„æ”¹è¿›ã€çŸ¥è¯†è’¸é¦æŠ€æœ¯å’Œå¤šå°ºåº¦ä¸Šä¸‹æ–‡ç‰¹å¾å»ºæ¨¡çš„æœ€æ–°å‘å±•ï¼Œå¹¶æå‡ºäº†å…‹æœè¿™äº›ç“¶é¢ˆçš„æœªæ¥æ–¹å‘ã€‚æˆ‘ä»¬çš„åˆ†æçªå‡ºäº†VFMsçš„æ½œåŠ›ï¼Œä»¥åŠè”é‚¦å­¦ä¹ å’Œæ¨¡å‹å‹ç¼©ç­‰æ–°å…´æ–¹æ³•ï¼Œå°†å½»åº•æ”¹å˜åŒ»å­¦å›¾åƒåˆ†æï¼Œæé«˜ä¸´åºŠåº”ç”¨ã€‚è¿™é¡¹å·¥ä½œçš„ç›®æ ‡æ˜¯æä¾›å½“å‰æ–¹æ³•è®ºçš„å…¨é¢æ¦‚è¿°ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æå‡ºå…³é”®é¢†åŸŸï¼Œä»¥æ¨åŠ¨åŒ»å­¦å›¾åƒåˆ†å‰²é¢†åŸŸçš„ä¸‹ä¸€æ³¢åˆ›æ–°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14584v2">PDF</a> 17 pages, 1 figure</p>
<p><strong>Summary</strong><br>     åŒ»å­¦å›¾åƒåˆ†æé¢†åŸŸå› Vision Foundation Modelsï¼ˆVFMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œå°¤å…¶æ˜¯Vision Transformersï¼ˆViTï¼‰å’ŒSegment Anything Modelï¼ˆSAMï¼‰çš„å´›èµ·è€Œå–å¾—æ˜¾è‘—è¿›æ­¥ã€‚è¿™äº›æ¨¡å‹åœ¨æ•æ‰é•¿ç¨‹ä¾èµ–æ€§å’Œå®ç°é«˜æ³›åŒ–åˆ†å‰²ä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå°†è¿™äº›å¤§å‹æ¨¡å‹åº”ç”¨äºåŒ»å­¦å›¾åƒåˆ†æé¢ä¸´è¯¸å¤šæŒ‘æˆ˜ï¼ŒåŒ…æ‹¬åŒ»å­¦ä¸è‡ªç„¶å›¾åƒä¹‹é—´çš„é¢†åŸŸå·®å¼‚ã€å¯¹é«˜æ•ˆæ¨¡å‹é€‚é…ç­–ç•¥çš„éœ€æ±‚ä»¥åŠå°è§„æ¨¡åŒ»å­¦æ•°æ®é›†çš„å±€é™æ€§ã€‚æœ¬æ–‡ç»¼è¿°äº†VFMsåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„æœ€æ–°ç ”ç©¶ï¼Œé‡ç‚¹è®¨è®ºé¢†åŸŸé€‚é…ã€æ¨¡å‹å‹ç¼©å’Œè”é‚¦å­¦ä¹ çš„æŒ‘æˆ˜ã€‚é€šè¿‡è®¨è®ºåŸºäºé€‚é…å™¨çš„æ”¹è¿›ã€çŸ¥è¯†è’¸é¦æŠ€æœ¯å’Œå¤šå°ºåº¦ä¸Šä¸‹æ–‡ç‰¹å¾å»ºæ¨¡çš„æœ€æ–°å‘å±•ï¼Œæœ¬æ–‡æå‡ºäº†å…‹æœè¿™äº›ç“¶é¢ˆçš„æœªæ¥æ–¹å‘ã€‚æœ¬æ–‡åˆ†æäº†VFMsçš„æ½œåŠ›ï¼Œä»¥åŠè”é‚¦å­¦ä¹ å’Œæ¨¡å‹å‹ç¼©ç­‰æ–°å…´æ–¹æ³•ï¼Œæœ‰æœ›é©æ–°åŒ»å­¦å›¾åƒåˆ†æï¼Œæé«˜ä¸´åºŠåº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision Foundation Models (VFMs) åœ¨åŒ»å­¦å›¾åƒåˆ†æé¢†åŸŸå–å¾—æ˜¾è‘—è¿›å±•ï¼Œå°¤å…¶æ˜¯Vision Transformers (ViT) å’Œ Segment Anything Model (SAM)ã€‚</li>
<li>VFMs åœ¨æ•æ‰é•¿ç¨‹ä¾èµ–æ€§å’Œå®ç°é«˜æ³›åŒ–åˆ†å‰²ä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºå“è¶Šèƒ½åŠ›ã€‚</li>
<li>å°†VFMsåº”ç”¨äºåŒ»å­¦å›¾åƒåˆ†æé¢ä¸´é¢†åŸŸå·®å¼‚ã€æ¨¡å‹é€‚é…å’ŒåŒ»ç–—æ•°æ®é›†å±€é™æ€§ç­‰æŒ‘æˆ˜ã€‚</li>
<li>è®ºæ–‡ç»¼è¿°äº†VFMsåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„æœ€æ–°ç ”ç©¶ï¼ŒåŒ…æ‹¬é€‚é…å™¨æ”¹è¿›ã€çŸ¥è¯†è’¸é¦å’Œå¤šå°ºåº¦ä¸Šä¸‹æ–‡ç‰¹å¾å»ºæ¨¡ã€‚</li>
<li>è”é‚¦å­¦ä¹ å’Œæ¨¡å‹å‹ç¼©ç­‰æ–°å…´æ–¹æ³•æœ‰æœ›é©æ–°åŒ»å­¦å›¾åƒåˆ†æï¼Œæé«˜ä¸´åºŠåº”ç”¨ã€‚</li>
<li>è®ºæ–‡æä¾›äº†å½“å‰æ–¹æ³•çš„å…¨è²Œï¼Œå¹¶ä¸ºæœªæ¥ç ”ç©¶æå‡ºäº†å…³é”®æ–¹å‘ï¼Œä»¥æ¨åŠ¨åŒ»å­¦å›¾åƒåˆ†å‰²é¢†åŸŸçš„åˆ›æ–°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14584">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f91b0fb8967365f4f3498eef6082c847.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-85fec34710fb4dc1733507b14f18ee30.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Ambient-Denoising-Diffusion-Generative-Adversarial-Networks-for-Establishing-Stochastic-Object-Models-from-Noisy-Image-Data"><a href="#Ambient-Denoising-Diffusion-Generative-Adversarial-Networks-for-Establishing-Stochastic-Object-Models-from-Noisy-Image-Data" class="headerlink" title="Ambient Denoising Diffusion Generative Adversarial Networks for   Establishing Stochastic Object Models from Noisy Image Data"></a>Ambient Denoising Diffusion Generative Adversarial Networks for   Establishing Stochastic Object Models from Noisy Image Data</h2><p><strong>Authors:Xichen Xu, Wentao Chen, Weimin Zhou</strong></p>
<p>It is widely accepted that medical imaging systems should be objectively assessed via task-based image quality (IQ) measures that ideally account for all sources of randomness in the measured image data, including the variation in the ensemble of objects to be imaged. Stochastic object models (SOMs) that can randomly draw samples from the object distribution can be employed to characterize object variability. To establish realistic SOMs for task-based IQ analysis, it is desirable to employ experimental image data. However, experimental image data acquired from medical imaging systems are subject to measurement noise. Previous work investigated the ability of deep generative models (DGMs) that employ an augmented generative adversarial network (GAN), AmbientGAN, for establishing SOMs from noisy measured image data. Recently, denoising diffusion models (DDMs) have emerged as a leading DGM for image synthesis and can produce superior image quality than GANs. However, original DDMs possess a slow image-generation process because of the Gaussian assumption in the denoising steps. More recently, denoising diffusion GAN (DDGAN) was proposed to permit fast image generation while maintain high generated image quality that is comparable to the original DDMs. In this work, we propose an augmented DDGAN architecture, Ambient DDGAN (ADDGAN), for learning SOMs from noisy image data. Numerical studies that consider clinical computed tomography (CT) images and digital breast tomosynthesis (DBT) images are conducted. The ability of the proposed ADDGAN to learn realistic SOMs from noisy image data is demonstrated. It has been shown that the ADDGAN significantly outperforms the advanced AmbientGAN models for synthesizing high resolution medical images with complex textures. </p>
<blockquote>
<p>æ™®éè®¤ä¸ºï¼ŒåŒ»ç–—æˆåƒç³»ç»Ÿåº”é€šè¿‡åŸºäºä»»åŠ¡çš„å›¾åƒè´¨é‡ï¼ˆIQï¼‰åº¦é‡è¿›è¡Œå®¢è§‚è¯„ä¼°ï¼Œè¿™äº›åº¦é‡ç†æƒ³åœ°åº”è€ƒè™‘åˆ°æµ‹é‡å›¾åƒæ•°æ®ä¸­æ‰€æœ‰éšæœºæ€§çš„æ¥æºï¼ŒåŒ…æ‹¬è¦æˆåƒçš„å¯¹è±¡é›†åˆçš„å˜åŒ–ã€‚å¯ä»¥ä»å¯¹è±¡åˆ†å¸ƒä¸­éšæœºæŠ½å–æ ·æœ¬çš„éšæœºå¯¹è±¡æ¨¡å‹ï¼ˆSOMsï¼‰å¯ç”¨äºè¡¨å¾å¯¹è±¡çš„å˜åŒ–æ€§ã€‚ä¸ºäº†ä¸ºåŸºäºä»»åŠ¡çš„IQåˆ†æå»ºç«‹ç°å®çš„SOMsï¼Œä½¿ç”¨å®éªŒå›¾åƒæ•°æ®æ˜¯ç†æƒ³çš„ã€‚ç„¶è€Œï¼Œä»åŒ»ç–—æˆåƒç³»ç»Ÿè·å¾—çš„å®éªŒå›¾åƒæ•°æ®å—åˆ°æµ‹é‡å™ªå£°çš„å½±å“ã€‚å…ˆå‰çš„å·¥ä½œç ”ç©¶äº†ä½¿ç”¨å¢å¼ºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰çš„æ·±ç”Ÿæˆæ¨¡å‹ï¼ˆDGMï¼‰çš„èƒ½åŠ›ï¼Œå³AmbientGANï¼Œç”¨äºä»å¸¦å™ªå£°çš„æµ‹é‡å›¾åƒæ•°æ®ä¸­å»ºç«‹SOMsã€‚æœ€è¿‘ï¼Œé™å™ªæ‰©æ•£æ¨¡å‹ï¼ˆDDMï¼‰å·²æˆä¸ºé¢†å…ˆçš„å›¾åƒåˆæˆDGMï¼Œå¹¶ä¸”èƒ½äº§ç”Ÿæ¯”GANæ›´é«˜çš„å›¾åƒè´¨é‡ã€‚ç„¶è€Œï¼ŒåŸå§‹DDMå…·æœ‰ç¼“æ…¢çš„å›¾åƒå¤„ç†è¿‡ç¨‹ï¼Œè¿™æ˜¯ç”±äºå»å™ªæ­¥éª¤ä¸­çš„é«˜æ–¯å‡è®¾ã€‚æœ€è¿‘ï¼Œæå‡ºäº†é™å™ªæ‰©æ•£GANï¼ˆDDGANï¼‰ï¼Œä»¥å®ç°å¿«é€Ÿçš„å›¾åƒç”Ÿæˆå¹¶ä¿æŒä¸åŸå§‹DDMç›¸å½“çš„é«˜ç”Ÿæˆå›¾åƒè´¨é‡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¢å¼ºçš„DDGANæ¶æ„ï¼Œå³Ambient DDGANï¼ˆADDGANï¼‰ï¼Œç”¨äºä»å¸¦å™ªå£°çš„å›¾åƒæ•°æ®ä¸­å­¦ä¹ SOMsã€‚æˆ‘ä»¬è¿›è¡Œäº†æ¶‰åŠä¸´åºŠè®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰å›¾åƒå’Œæ•°å­—ä¹³è…ºæ–­å±‚åˆæˆï¼ˆDBTï¼‰å›¾åƒçš„æ•°å€¼ç ”ç©¶ã€‚æ‰€å±•ç¤ºçš„ADDGANä»å¸¦å™ªå£°çš„å›¾åƒæ•°æ®ä¸­å­¦ä¹ ç°å®çš„SOMsçš„èƒ½åŠ›å¾—åˆ°äº†éªŒè¯ã€‚ç»“æœè¡¨æ˜ï¼ŒADDGANåœ¨åˆæˆå…·æœ‰å¤æ‚çº¹ç†çš„é«˜åˆ†è¾¨ç‡åŒ»å­¦å›¾åƒæ–¹é¢æ˜¾è‘—ä¼˜äºå…ˆè¿›çš„AmbientGANæ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.19094v2">PDF</a> SPIE Medical Imaging 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†ä½¿ç”¨æ·±åº¦ç”Ÿæˆæ¨¡å‹ï¼ˆDGMï¼‰å»ºç«‹åŸºäºä»»åŠ¡å›¾åƒè´¨é‡ï¼ˆIQï¼‰åˆ†æçš„éšæœºå¯¹è±¡æ¨¡å‹ï¼ˆSOMï¼‰çš„æ–¹æ³•ã€‚ç‰¹åˆ«å…³æ³¨äº†æ–°å…´çš„å»å™ªæ‰©æ•£æ¨¡å‹ï¼ˆDDMsï¼‰åŠå…¶å˜ä½“DDGANå’Œå¢å¼ºçš„DDGANæ¶æ„ï¼ˆADDGANï¼‰ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒADDGANèƒ½å¤Ÿä»å«å™ªå£°çš„å›¾åƒæ•°æ®ä¸­å­¦ä¹ ç°å®çš„SOMï¼Œå¹¶åœ¨åˆæˆå…·æœ‰å¤æ‚çº¹ç†çš„é«˜åˆ†è¾¨ç‡åŒ»å­¦å›¾åƒæ–¹é¢æ˜¾è‘—ä¼˜äºå…ˆè¿›çš„AmbientGANæ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>åŒ»å­¦æˆåƒç³»ç»Ÿçš„å®¢è§‚è¯„ä¼°åº”åŸºäºä»»åŠ¡å›¾åƒè´¨é‡ï¼ˆIQï¼‰æªæ–½ï¼Œè€ƒè™‘å›¾åƒæ•°æ®ä¸­çš„æ‰€æœ‰éšæœºæ€§æ¥æºã€‚</li>
<li>éšæœºå¯¹è±¡æ¨¡å‹ï¼ˆSOMsï¼‰å¯ç”¨äºè¡¨å¾å¯¹è±¡å˜å¼‚æ€§ã€‚</li>
<li>å®éªŒå›¾åƒæ•°æ®å¯ç”¨äºå»ºç«‹ç°å®çš„SOMsè¿›è¡Œä»»åŠ¡IQåˆ†æï¼Œä½†è¿™äº›æ•°æ®é€šå¸¸å—åˆ°æµ‹é‡å™ªå£°çš„å½±å“ã€‚</li>
<li>æ·±åº¦ç”Ÿæˆæ¨¡å‹ï¼ˆDGMï¼‰å¦‚å»å™ªæ‰©æ•£æ¨¡å‹ï¼ˆDDMsï¼‰å’ŒDDGANå¯ç”¨äºä»å«å™ªå£°çš„å›¾åƒæ•°æ®ä¸­å»ºç«‹SOMã€‚</li>
<li>æœ€æ–°æå‡ºçš„å¢å¼ºçš„DDGANæ¶æ„ï¼ˆADDGANï¼‰å¯å®ç°å¿«é€Ÿå›¾åƒç”Ÿæˆï¼ŒåŒæ—¶ä¿æŒä¸åŸå§‹DDMsç›¸å½“çš„é«˜è´¨é‡å›¾åƒç”Ÿæˆèƒ½åŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.19094">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-6ab7ec549d85f3ec83866946fa4175ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e286f6622218051d9f34b42aa6c0cfff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9bbec2dc4d2b19b867c16214a9579421.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4f141bc5650c59804d3f9a2085de1663.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-770238047ef76dab46461ea22f983730.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3cedd86632b8bfe03933c6c1429d22e6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f606185b13845e181b63e9500963a7c1.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="RTFAST-Spectra-Emulation-of-X-ray-reverberation-mapping-for-active-galactic-nuclei"><a href="#RTFAST-Spectra-Emulation-of-X-ray-reverberation-mapping-for-active-galactic-nuclei" class="headerlink" title="RTFAST-Spectra: Emulation of X-ray reverberation mapping for active   galactic nuclei"></a>RTFAST-Spectra: Emulation of X-ray reverberation mapping for active   galactic nuclei</h2><p><strong>Authors:Benjamin Ricketts, Daniela Huppenkothen, Matteo Lucchini, Adam Ingram, Guglielmo Mastroserio, Matthew Ho, Benjamin Wandelt</strong></p>
<p>Bayesian analysis has begun to be more widely adopted in X-ray spectroscopy, but it has largely been constrained to relatively simple physical models due to limitations in X-ray modelling software and computation time. As a result, Bayesian analysis of numerical models with high physics complexity have remained out of reach. This is a challenge, for example when modelling the X-ray emission of accreting black hole X-ray binaries, where the slow model computations severely limit explorations of parameter space and may bias the inference of astrophysical parameters. Here, we present RTFAST-Spectra: a neural network emulator that acts as a drop in replacement for the spectral portion of the black hole X-ray reverberation model RTDIST. This is the first emulator for the reltrans model suite and the first emulator for a state-of-the-art x-ray reflection model incorporating relativistic effects with 17 physically meaningful model parameters. We use Principal Component Analysis to create a light-weight neural network that is able to preserve correlations between complex atomic lines and simple continuum, enabling consistent modelling of key parameters of scientific interest. We achieve a $\mathcal{O}(10^2)$ times speed up over the original model in the most conservative conditions with $\mathcal{O}(1%)$ precision over all 17 free parameters in the original numerical model, taking full posterior fits from months to hours. We employ Markov Chain Monte Carlo sampling to show how we can better explore the posteriors of model parameters in simulated data and discuss the complexities in interpreting the model when fitting real data. </p>
<blockquote>
<p>è´å¶æ–¯åˆ†æåœ¨Xå°„çº¿å…‰è°±å­¦ä¸­å·²ç»å¼€å§‹å¾—åˆ°æ›´å¹¿æ³›çš„åº”ç”¨ï¼Œä½†ç”±äºXå°„çº¿å»ºæ¨¡è½¯ä»¶å’Œè®¡ç®—æ—¶é—´çš„é™åˆ¶ï¼Œå®ƒä¸»è¦å±€é™äºç›¸å¯¹ç®€å•çš„ç‰©ç†æ¨¡å‹ã€‚å› æ­¤ï¼Œå…·æœ‰å¤æ‚ç‰©ç†æ•°å€¼æ¨¡å‹çš„è´å¶æ–¯åˆ†æä»ç„¶æ— æ³•è¾¾åˆ°ã€‚å½“å¯¹å¸ç§¯é»‘æ´Xå°„çº¿åŒæ˜Ÿçš„Xå°„çº¿å‘å°„è¿›è¡Œå»ºæ¨¡æ—¶ï¼Œè¿™æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æ…¢é€Ÿæ¨¡å‹è®¡ç®—ä¸¥é‡é™åˆ¶äº†å‚æ•°ç©ºé—´çš„æ¢ç´¢ï¼Œå¹¶å¯èƒ½å¯¼è‡´å¤©ä½“ç‰©ç†å‚æ•°çš„æ¨æ–­å‡ºç°åå·®ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä»‹ç»äº†RTFAST-Spectraï¼šä¸€ä¸ªç¥ç»ç½‘ç»œæ¨¡æ‹Ÿå™¨ï¼Œå®ƒå¯ä»¥ä½œä¸ºé»‘æ´Xå°„çº¿å›æ³¢æ¨¡å‹RTDISTå…‰è°±éƒ¨åˆ†çš„æ›¿ä»£ã€‚è¿™æ˜¯ç¬¬ä¸€ä¸ªä¸ºreltransæ¨¡å‹å¥—ä»¶å’Œç¬¬ä¸€ä¸ªä¸ºæœ€æ–°xå°„çº¿åå°„æ¨¡å‹è®¾è®¡çš„æ¨¡æ‹Ÿå™¨ï¼Œè¯¥æ¨¡å‹ç»“åˆäº†ç›¸å¯¹è®ºæ•ˆåº”ï¼Œå…·æœ‰17ä¸ªç‰©ç†æ„ä¹‰æ˜ç¡®çš„æ¨¡å‹å‚æ•°ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸»æˆåˆ†åˆ†æåˆ›å»ºäº†ä¸€ä¸ªè½»é‡çº§çš„ç¥ç»ç½‘ç»œï¼Œèƒ½å¤Ÿä¿æŒå¤æ‚åŸå­çº¿å’Œç®€å•è¿ç»­ä½“ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œä»è€Œèƒ½å¤Ÿå¯¹ç§‘å­¦å…´è¶£çš„å…³é”®å‚æ•°è¿›è¡Œä¸€è‡´å»ºæ¨¡ã€‚æˆ‘ä»¬åœ¨æœ€ä¿å®ˆçš„æ¡ä»¶ä¸‹å®ç°äº†ç›¸å¯¹äºåŸå§‹æ¨¡å‹çš„$\mathcal{O}(10^2)$å€åŠ é€Ÿï¼Œåœ¨åŸå§‹æ¨¡å‹çš„å…¨éƒ¨17ä¸ªè‡ªç”±å‚æ•°ä¸Šå®ç°äº†$\mathcal{O}(1%)$çš„ç²¾ç¡®åº¦ï¼Œå°†å…¨åéªŒæ‹Ÿåˆä»æ•°æœˆç¼©çŸ­è‡³æ•°å°æ—¶ã€‚æˆ‘ä»¬é‡‡ç”¨é©¬å°”å¯å¤«é“¾è’™ç‰¹å¡ç½—é‡‡æ ·æ³•ï¼Œå±•ç¤ºäº†å¦‚ä½•æ›´å¥½åœ°æ¢ç´¢æ¨¡æ‹Ÿæ•°æ®çš„æ¨¡å‹å‚æ•°åéªŒåˆ†å¸ƒï¼Œå¹¶è®¨è®ºäº†åœ¨å®é™…æ•°æ®æ‹Ÿåˆä¸­è§£é‡Šæ¨¡å‹çš„å¤æ‚æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.10131v2">PDF</a> 22 pages, 35 figures. Accepted in MNRAS</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†RTFAST-Spectraç¥ç»ç½‘ç»œæ¨¡æ‹Ÿå™¨åœ¨Xå°„çº¿å…‰è°±åˆ†æä¸­çš„åº”ç”¨ã€‚è¯¥æ¨¡æ‹Ÿå™¨å¯ä½œä¸ºé»‘æ´Xå°„çº¿å›æ³¢æ¨¡å‹RTDISTå…‰è°±éƒ¨åˆ†çš„æ›¿ä»£æ–¹æ¡ˆã€‚å®ƒæ˜¯é¦–ä¸ªä¸ºreltransæ¨¡å‹å¥—ä»¶å’Œé¦–ä¸ªä¸ºä¸€æµçš„è€ƒè™‘ç›¸å¯¹è®ºæ•ˆåº”çš„Xå°„çº¿åå°„æ¨¡å‹åˆ›å»ºçš„æ¨¡æ‹Ÿå™¨ï¼Œè¯¥æ¨¡å‹åŒ…å«17ä¸ªå…·æœ‰ç‰©ç†æ„ä¹‰çš„æ¨¡å‹å‚æ•°ã€‚ä½¿ç”¨ä¸»æˆåˆ†åˆ†æåˆ›å»ºè½»é‡çº§ç¥ç»ç½‘ç»œï¼Œèƒ½å¤Ÿä¿ç•™å¤æ‚åŸå­çº¿å’Œç®€å•è¿ç»­è°±ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œå®ç°å¯¹å…³é”®ç§‘å­¦å‚æ•°çš„ä¸€è‡´å»ºæ¨¡ã€‚åœ¨æœ€ä¿å®ˆçš„æ¡ä»¶ä¸‹ï¼Œç›¸å¯¹äºåŸå§‹æ¨¡å‹å®ç°äº†$\mathcal{O}(10^2)$å€çš„é€Ÿåº¦æå‡ï¼Œåœ¨æ‰€æœ‰çš„17ä¸ªè‡ªç”±å‚æ•°ä¸Šä¿æŒ$\mathcal{O}(1%)$çš„ç²¾åº¦ã€‚é‡‡ç”¨é©¬å°”å¯å¤«é“¾è’™ç‰¹å¡ç½—é‡‡æ ·ï¼Œå±•ç¤ºäº†å¦‚ä½•æ›´å¥½åœ°æ¢ç´¢æ¨¡æ‹Ÿæ•°æ®çš„æ¨¡å‹å‚æ•°åéªŒåˆ†å¸ƒï¼Œå¹¶è®¨è®ºäº†æ‹ŸåˆçœŸå®æ•°æ®æ—¶æ¨¡å‹è§£é‡Šçš„å¤æ‚æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>Bayesianåˆ†æåœ¨Xå°„çº¿å…‰è°±ä¸­çš„åº”ç”¨å—åˆ°è½¯ä»¶å’Œè®¡ç®—æ—¶é—´çš„é™åˆ¶ï¼Œéš¾ä»¥å¤„ç†é«˜ç‰©ç†å¤æ‚åº¦çš„æ•°å€¼æ¨¡å‹ã€‚</li>
<li>RTFAST-Spectraæ˜¯é¦–ä¸ªä¸ºreltransæ¨¡å‹å¥—ä»¶å’Œè€ƒè™‘ç›¸å¯¹è®ºæ•ˆåº”çš„å…ˆè¿›Xå°„çº¿åå°„æ¨¡å‹åˆ›å»ºçš„æ¨¡æ‹Ÿå™¨ã€‚</li>
<li>ä½¿ç”¨ä¸»æˆåˆ†åˆ†æåˆ›å»ºè½»é‡çº§ç¥ç»ç½‘ç»œï¼Œèƒ½å¤Ÿä¿ç•™å¤æ‚åŸå­çº¿å’Œç®€å•è¿ç»­è°±ä¹‹é—´çš„ç›¸å…³æ€§ã€‚</li>
<li>å®ç°äº†å¯¹åŸå§‹æ¨¡å‹$\mathcal{O}(10^2)$å€çš„é€Ÿåº¦æå‡ï¼ŒåŒæ—¶åœ¨æ‰€æœ‰17ä¸ªè‡ªç”±å‚æ•°ä¸Šä¿æŒ$\mathcal{O}(1%)$çš„ç²¾åº¦ã€‚</li>
<li>é€šè¿‡Markov Chain Monte Carloé‡‡æ ·ï¼Œæ›´å¥½åœ°æ¢ç´¢äº†æ¨¡æ‹Ÿæ•°æ®ä¸­çš„æ¨¡å‹å‚æ•°åéªŒåˆ†å¸ƒã€‚</li>
<li>åœ¨æ‹ŸåˆçœŸå®æ•°æ®æ—¶ï¼Œæ¨¡å‹è§£é‡Šçš„å¤æ‚æ€§è¢«è®¨è®ºã€‚</li>
<li>è¯¥æ¨¡æ‹Ÿå™¨æœ‰æœ›æ”¹å–„Xå°„çº¿å…‰è°±åˆ†æä¸­çš„å‚æ•°ç©ºé—´æ¢ç´¢ï¼Œå‡å°‘è®¡ç®—æ—¶é—´ï¼Œä¿ƒè¿›Bayesianåˆ†æåœ¨å¤æ‚ç‰©ç†æ¨¡å‹ä¸­çš„åº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.10131">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-69f1779a3644dd1aed82a71a110a42c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a05a5e53e99b72bc4593e550d6e3b2f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7ecc3c48819c27ac2a3551f4ae6ae9fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e84c5858404d36c67c5e58a5440fecfa.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="CT-Mamba-A-Hybrid-Convolutional-State-Space-Model-for-Low-Dose-CT-Denoising"><a href="#CT-Mamba-A-Hybrid-Convolutional-State-Space-Model-for-Low-Dose-CT-Denoising" class="headerlink" title="CT-Mamba: A Hybrid Convolutional State Space Model for Low-Dose CT   Denoising"></a>CT-Mamba: A Hybrid Convolutional State Space Model for Low-Dose CT   Denoising</h2><p><strong>Authors:Linxuan Li, Wenjia Wei, Luyao Yang, Wenwen Zhang, Jiashu Dong, Yahua Liu, Wei Zhao</strong></p>
<p>Low-dose CT (LDCT) significantly reduces the radiation dose received by patients, however, dose reduction introduces additional noise and artifacts. Currently, denoising methods based on convolutional neural networks (CNNs) face limitations in long-range modeling capabilities, while Transformer-based denoising methods, although capable of powerful long-range modeling, suffer from high computational complexity. Furthermore, the denoised images predicted by deep learning-based techniques inevitably exhibit differences in noise distribution compared to normal-dose CT (NDCT) images, which can also impact the final image quality and diagnostic outcomes. This paper proposes CT-Mamba, a hybrid convolutional State Space Model for LDCT image denoising. The model combines the local feature extraction advantages of CNNs with Mambaâ€™s strength in capturing long-range dependencies, enabling it to capture both local details and global context. Additionally, we introduce an innovative spatially coherent â€˜Zâ€™-shaped scanning scheme to ensure spatial continuity between adjacent pixels in the image. We design a Mamba-driven deep noise power spectrum (NPS) loss function to guide model training, ensuring that the noise texture of the denoised LDCT images closely resembles that of NDCT images, thereby enhancing overall image quality and diagnostic value. Experimental results have demonstrated that CT-Mamba performs excellently in reducing noise in LDCT images, enhancing detail preservation, and optimizing noise texture distribution, and exhibits higher statistical similarity with the radiomics features of NDCT images. The proposed CT-Mamba demonstrates outstanding performance in LDCT denoising and holds promise as a representative approach for applying the Mamba framework to LDCT denoising tasks. Our code will be made available after the paper is officially published: <a target="_blank" rel="noopener" href="https://github.com/zy2219105/CT-Mamba/">https://github.com/zy2219105/CT-Mamba/</a>. </p>
<blockquote>
<p>ä½å‰‚é‡CTï¼ˆLDCTï¼‰æ˜¾è‘—å‡å°‘äº†æ‚£è€…æ¥å—çš„è¾å°„å‰‚é‡ï¼Œç„¶è€Œï¼Œå‰‚é‡çš„å‡å°‘ä¼šå¼•å…¥é¢å¤–çš„å™ªå£°å’Œä¼ªå½±ã€‚ç›®å‰ï¼ŒåŸºäºå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰çš„é™å™ªæ–¹æ³•åœ¨é•¿æœŸå»ºæ¨¡èƒ½åŠ›æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œè€ŒåŸºäºTransformerçš„é™å™ªæ–¹æ³•è™½ç„¶å…·æœ‰å¼ºå¤§çš„é•¿æœŸå»ºæ¨¡èƒ½åŠ›ï¼Œä½†è®¡ç®—å¤æ‚åº¦è¾ƒé«˜ã€‚æ­¤å¤–ï¼ŒåŸºäºæ·±åº¦å­¦ä¹ æŠ€æœ¯é¢„æµ‹çš„é™å™ªå›¾åƒä¸æ­£å¸¸å‰‚é‡CTï¼ˆNDCTï¼‰å›¾åƒç›¸æ¯”ï¼Œå™ªå£°åˆ†å¸ƒä¸å¯é¿å…åœ°å­˜åœ¨å·®å¼‚ï¼Œè¿™ä¹Ÿå¯èƒ½å½±å“æœ€ç»ˆçš„å›¾åƒè´¨é‡å’Œè¯Šæ–­ç»“æœã€‚æœ¬æ–‡æå‡ºäº†CT-Mambaï¼Œä¸€ç§ç”¨äºLDCTå›¾åƒé™å™ªçš„æ··åˆå·ç§¯çŠ¶æ€ç©ºé—´æ¨¡å‹ã€‚è¯¥æ¨¡å‹ç»“åˆäº†CNNsæå–å±€éƒ¨ç‰¹å¾çš„ä¼˜åŠ¿å’ŒMambaåœ¨æ•æ‰é•¿æœŸä¾èµ–å…³ç³»æ–¹é¢çš„å®åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿæ•æ‰å±€éƒ¨ç»†èŠ‚å’Œå…¨å±€ä¸Šä¸‹æ–‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åˆ›æ–°çš„ç©ºé—´è¿è´¯æ€§â€œZâ€å½¢æ‰«ææ–¹æ¡ˆï¼Œä»¥ç¡®ä¿å›¾åƒä¸­ç›¸é‚»åƒç´ ä¹‹é—´çš„ç©ºé—´è¿ç»­æ€§ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªç”±Mambaé©±åŠ¨çš„æ·±åº¦å™ªå£°åŠŸç‡è°±ï¼ˆNPSï¼‰æŸå¤±å‡½æ•°æ¥å¼•å¯¼æ¨¡å‹è®­ç»ƒï¼Œç¡®ä¿é™å™ªLDCTå›¾åƒçš„å™ªå£°çº¹ç†ä¸NDCTå›¾åƒç›¸ä¼¼ï¼Œä»è€Œæé«˜æ•´ä½“å›¾åƒè´¨é‡å’Œè¯Šæ–­ä»·å€¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCT-Mambaåœ¨é™ä½LDCTå›¾åƒå™ªå£°ã€æé«˜ç»†èŠ‚ä¿ç•™å’Œä¼˜åŒ–å™ªå£°çº¹ç†åˆ†å¸ƒæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä¸NDCTå›¾åƒçš„æ”¾å°„å­¦ç‰¹å¾å…·æœ‰æ›´é«˜çš„ç»Ÿè®¡ç›¸ä¼¼æ€§ã€‚æ‰€æå‡ºçš„CT-Mambaåœ¨LDCTé™å™ªæ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œå¹¶æœ‰æœ›ä½œä¸ºå°†Mambaæ¡†æ¶åº”ç”¨äºLDCTé™å™ªä»»åŠ¡çš„ä¸€ç§ä»£è¡¨æ€§æ–¹æ³•ã€‚æˆ‘ä»¬çš„ä»£ç å°†åœ¨è®ºæ–‡æ­£å¼å‘è¡¨åæä¾›ï¼š<a target="_blank" rel="noopener" href="https://github.com/zy2219105/CT-Mamba/%E3%80%82">https://github.com/zy2219105/CT-Mamba/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.07930v3">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>è¿™ç¯‡è®ºæ–‡æå‡ºä¸€ç§ç”¨äºä½å‰‚é‡CTï¼ˆLDCTï¼‰å›¾åƒå»å™ªçš„æ··åˆå·ç§¯çŠ¶æ€ç©ºé—´æ¨¡å‹â€”â€”CT-Mambaã€‚è¯¥æ¨¡å‹ç»“åˆäº†å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰çš„å±€éƒ¨ç‰¹å¾æå–ä¼˜åŠ¿å’ŒMambaåœ¨æ•æ‰é•¿è·ç¦»ä¾èµ–æ–¹é¢çš„ä¼˜åŠ¿ï¼Œæ—¢èƒ½æ•æ‰å±€éƒ¨ç»†èŠ‚åˆèƒ½æ•æ‰å…¨å±€ä¸Šä¸‹æ–‡ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†ä¸€ç§åˆ›æ–°çš„Zå½¢æ‰«ææ–¹æ¡ˆï¼Œç¡®ä¿å›¾åƒä¸­ç›¸é‚»åƒç´ ä¹‹é—´çš„ç©ºé—´è¿ç»­æ€§ã€‚è®¾è®¡äº†ä¸€ç§åŸºäºMambaçš„æ·±åº¦å™ªå£°åŠŸç‡è°±ï¼ˆNPSï¼‰æŸå¤±å‡½æ•°ï¼Œä»¥æŒ‡å¯¼æ¨¡å‹è®­ç»ƒï¼Œç¡®ä¿å»å™ªåçš„LDCTå›¾åƒçš„å™ªå£°çº¹ç†ä¸å¸¸è§„å‰‚é‡CTï¼ˆNDCTï¼‰å›¾åƒç›¸ä¼¼ï¼Œä»è€Œæé«˜æ•´ä½“å›¾åƒè´¨é‡å’Œè¯Šæ–­ä»·å€¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCT-Mambaåœ¨é™ä½LDCTå›¾åƒå™ªå£°ã€å¢å¼ºç»†èŠ‚ä¿ç•™å’Œä¼˜åŒ–å™ªå£°çº¹ç†åˆ†å¸ƒæ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä¸NDCTå›¾åƒçš„æ”¾å°„å­¦ç‰¹å¾è¡¨ç°å‡ºè¾ƒé«˜çš„ç»Ÿè®¡ç›¸ä¼¼æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ä½å‰‚é‡CTï¼ˆLDCTï¼‰èƒ½å¤Ÿæ˜¾è‘—å‡å°‘æ‚£è€…æ¥å—çš„è¾å°„å‰‚é‡ï¼Œä½†å‰‚é‡å‡å°‘ä¼šå¯¼è‡´é¢å¤–çš„å™ªå£°å’Œä¼ªå½±ã€‚</li>
<li>å½“å‰åŸºäºå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰çš„å»å™ªæ–¹æ³•åœ¨é•¿æœŸå»ºæ¨¡èƒ½åŠ›æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œè€ŒåŸºäºTransformerçš„å»å™ªæ–¹æ³•è™½ç„¶å…·æœ‰å¼ºå¤§çš„é•¿æœŸå»ºæ¨¡èƒ½åŠ›ï¼Œä½†è®¡ç®—å¤æ‚åº¦è¾ƒé«˜ã€‚</li>
<li>æ·±åº¦å­¦ä¹ æŠ€æœ¯é¢„æµ‹çš„é™å™ªå›¾åƒä¸å¸¸è§„å‰‚é‡CTï¼ˆNDCTï¼‰å›¾åƒçš„å™ªå£°åˆ†å¸ƒå­˜åœ¨å·®å¼‚ï¼Œå¯èƒ½å½±å“æœ€ç»ˆå›¾åƒè´¨é‡å’Œè¯Šæ–­ç»“æœã€‚</li>
<li>CT-Mambaæ¨¡å‹ç»“åˆäº†CNNå’ŒMambaçš„ä¼˜åŠ¿ï¼Œèƒ½å¤Ÿæ•æ‰å±€éƒ¨ç»†èŠ‚å’Œå…¨å±€ä¸Šä¸‹æ–‡ã€‚</li>
<li>åˆ›æ–°çš„Zå½¢æ‰«ææ–¹æ¡ˆç¡®ä¿å›¾åƒä¸­ç›¸é‚»åƒç´ ä¹‹é—´çš„ç©ºé—´è¿ç»­æ€§ã€‚</li>
<li>å¼•å…¥æ·±åº¦å™ªå£°åŠŸç‡è°±ï¼ˆNPSï¼‰æŸå¤±å‡½æ•°ï¼Œä»¥æŒ‡å¯¼æ¨¡å‹è®­ç»ƒï¼Œä½¿å»å™ªåçš„LDCTå›¾åƒçš„å™ªå£°çº¹ç†æ›´æ¥è¿‘NDCTå›¾åƒã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒCT-Mambaåœ¨é™ä½LDCTå›¾åƒå™ªå£°ã€æé«˜ç»†èŠ‚ä¿ç•™å’Œå™ªå£°çº¹ç†åˆ†å¸ƒä¼˜åŒ–æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.07930">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8ce00c5f5eeb6d00862f231230bb2e37.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2406debcb5c6d85209de49e042ac0001.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f4d0ec11efe87f547b911c25456a2392.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5b2791429bc4b6d73e19e903fc46fb8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-124ccc867c2d7bf6de971c649d3bc020.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="New-JWST-redshifts-for-the-host-galaxies-of-CDF-S-XT1-and-XT2-understanding-their-nature"><a href="#New-JWST-redshifts-for-the-host-galaxies-of-CDF-S-XT1-and-XT2-understanding-their-nature" class="headerlink" title="New JWST redshifts for the host galaxies of CDF-S XT1 and XT2:   understanding their nature"></a>New JWST redshifts for the host galaxies of CDF-S XT1 and XT2:   understanding their nature</h2><p><strong>Authors:J. Quirola-VÃ¡squez, F. E. Bauer, P. G. Jonker, A. Levan, W. N. Brandt, M. Ravasio, D. Eappachen, Y. Q. Xue, X. C. Zheng</strong></p>
<p>CDF-S XT1 and XT2 are considered two canonical extragalactic fast X-ray transients (FXTs). In this work, we report new constraints on both FXTs, based on recent JWST NIRCam and MIRI photometry, as well as NIRspec spectroscopy for CDF-S XT2 that allow us to improve our understanding of their distances, energetics, and host galaxy properties compared to the pre-JWST era. We use the available HST and JWST archival data to determine the host properties and constrain the energetics of each FXT based on spectral energy distribution (SED) photometric fitting. The host of CDF-S XT1 is now constrained to lie at $z$&#x3D;2.76, implying a host absolute magnitude $M_R&#x3D;-19.14$<del>mag, stellar mass $M_{*}&#x3D;$1.8e8</del>$M_\odot$, and star formation rate SFR$&#x3D;0.62 M_\odot$&#x2F;yr. These properties lie at the upper end of previous estimates, leaving CDF-S XT1 with a peak X-ray luminosity of 2.8e47 erg&#x2F;s. We argue that the best progenitor scenario for XT1 is a low-luminosity gamma-ray burst (GRB), although we do not fully rule out a proto-magnetar association or a jetted tidal disruption event involving a white dwarf and an intermediate-massive black hole. In the case of CDF-S XT2, JWST imaging reveals a new highly obscured component of the host galaxy, previously missed in HST images, while NIRspec spectroscopy securely places the host at $z$&#x3D;3.4598. The new redshift implies a host with $M_R&#x3D;-21.76$<del>mag, $M_*&#x3D;5.5e10 M_\odot$, SFR&#x3D;160</del>$M_\odot$&#x2F;yr, and FXT $L_{X,peak}&#x3D;1.4e47$~erg&#x2F;s. The revised energetics, similarity to X-ray flash event light curves, small host offset, and high host SFR favor a low-luminosity collapsar progenitor for CDF-S XT2. Although a magnetar model is not ruled out, it appears improbable. While these HST and JWST observations shed light on the host galaxies of XT1 and XT2, and by extension, on the nature of FXTs, a unique explanation for both sources remains elusive. </p>
<blockquote>
<p>CDF-S XT1å’ŒXT2è¢«è®¤ä¸ºæ˜¯ä¸¤ç§å…¸å‹çš„ç³»å¤–å¿«é€ŸXå°„çº¿ç¬å˜ï¼ˆFXTsï¼‰ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬åŸºäºæœ€æ–°çš„JWST NIRCamå’ŒMIRIå…‰åº¦æ³•ä»¥åŠCDF-S XT2çš„NIRspecå…‰è°±æ³•ï¼ŒæŠ¥å‘Šäº†è¿™ä¸¤ç§FXTsçš„æ–°çº¦æŸã€‚ä¸ä¹‹å‰çš„æ—¶ä»£ç›¸æ¯”ï¼Œè¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿæ”¹è¿›å¯¹å®ƒä»¬çš„è·ç¦»ã€èƒ½é‡å’Œå®¿ä¸»æ˜Ÿç³»æ€§è´¨çš„ç†è§£ã€‚æˆ‘ä»¬ä½¿ç”¨å¯ç”¨çš„HSTå’ŒJWSTå­˜æ¡£æ•°æ®æ¥ç¡®å®šå®¿ä¸»å±æ€§å’ŒåŸºäºå…‰è°±èƒ½é‡åˆ†å¸ƒï¼ˆSEDï¼‰å…‰åº¦æ‹Ÿåˆçš„æ¯ä¸ªFXTçš„èƒ½é‡çº¦æŸã€‚CDF-S XT1çš„å®¿ä¸»ç°åœ¨è¢«é™åˆ¶åœ¨$z$&#x3D;2.76çš„ä½ç½®ï¼Œè¿™æ„å‘³ç€å®¿ä¸»ç»å¯¹æ˜Ÿç­‰$M_R&#x3D;-19.14$ magï¼Œæ’æ˜Ÿè´¨é‡$M_{<em>}$&#x3D;1.8e8 $M_\odot$ï¼Œæ˜Ÿå½¢æˆç‡SFR&#x3D;0.62 $M_\odot$&#x2F;yrã€‚è¿™äº›å±æ€§å¤„äºå…ˆå‰ä¼°è®¡å€¼çš„è¾ƒé«˜ç«¯ï¼Œä½¿CDF-S XT1çš„å³°å€¼Xå°„çº¿å…‰åº¦ä¸º2.8e47 erg&#x2F;sã€‚æˆ‘ä»¬è®¤ä¸ºXT1çš„æœ€ä½³å‰èº«æ˜Ÿæƒ…æ™¯æ˜¯ä½å…‰åº¦çš„ä¼½é©¬å°„çº¿çˆ†å‘ï¼ˆGRBï¼‰ï¼Œå°½ç®¡æˆ‘ä»¬å¹¶ä¸å®Œå…¨æ’é™¤åŸç£æ˜Ÿå…³è”æˆ–æ¶‰åŠç™½çŸ®æ˜Ÿå’Œä¸­ç­‰è´¨é‡é»‘æ´çš„å–·å°„æ½®æ±æ’•è£‚äº‹ä»¶ã€‚å¯¹äºCDF-S XT2æ¥è¯´ï¼ŒJWSTæˆåƒæ­ç¤ºäº†ä¸€ä¸ªå®¿ä¸»æ˜Ÿç³»ä¸­æ–°çš„é«˜åº¦é®è”½çš„ç»„ä»¶ï¼Œè¿™åœ¨ä¹‹å‰çš„HSTå›¾åƒä¸­æœªè¢«å‘ç°ï¼Œè€ŒNIRspecå…‰è°±æ³•å°†å…¶ç¨³å®šåœ°å®šä½åœ¨$z$&#x3D;3.4598çš„ä½ç½®ã€‚æ–°çš„çº¢ç§»æ„å‘³ç€å®¿ä¸»å…·æœ‰$M_R&#x3D;-21.76$ magï¼Œ$M_</em>&#x3D;5.5e10 M_\odot$ï¼ŒSFR&#x3D;160 $M_\odot$&#x2F;yrå’ŒFXT $L_{X,peak}&#x3D;1.4e47$ erg&#x2F;sã€‚ä¿®è®¢åçš„èƒ½é‡å­¦ã€ä¸Xå°„çº¿é—ªå…‰äº‹ä»¶å…‰æ›²çº¿çš„ç›¸ä¼¼æ€§ã€è¾ƒå°çš„å®¿ä¸»åç§»ä»¥åŠè¾ƒé«˜çš„å®¿ä¸»SFRæ”¯æŒCDF-S XT2ä¸ºä½å…‰åº¦å¡Œæ˜Ÿèµ·æºçš„è¯´æ³•ã€‚è™½ç„¶ç£æ˜Ÿæ¨¡å‹å¹¶æ²¡æœ‰è¢«æ’é™¤ï¼Œä½†ä¼¼ä¹ä¸å¤ªå¯èƒ½ã€‚è™½ç„¶è¿™äº›HSTå’ŒJWSTè§‚æµ‹æ­ç¤ºäº†XT1å’ŒXT2çš„å®¿ä¸»æ˜Ÿç³»ï¼Œå¹¶ç”±æ­¤æ‰©å±•åˆ°äº†FXTsçš„æ€§è´¨ï¼Œä½†å¯¹äºä¸¤ä¸ªæ¥æºçš„ç‹¬ç‰¹è§£é‡Šä»ç„¶éš¾ä»¥æ‰æ‘¸ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.10015v2">PDF</a> The manuscript was accepted by Astronomy &amp; Astrophysics in January   2025</p>
<p><strong>Summary</strong></p>
<p>CDF-S XT1å’ŒXT2æ˜¯ä¸¤ç§å…¸å‹çš„å¿«é€Ÿå°„ç”µæ˜Ÿç³»ç¬æ€äº‹ä»¶ï¼ˆFXTsï¼‰ã€‚åˆ©ç”¨æœ€æ–°çš„JWSTçº¢å¤–ç…§ç›¸æœºå’Œå¤šæ¨¡æ€æˆåƒå…‰åº¦çš„MIRIæ‘„å½±æœ¯ä»¥åŠCDF-S XT2çš„NIRspecå…‰è°±å­¦æ•°æ®ï¼Œæˆ‘ä»¬å¯¹å…¶è·ç¦»ã€èƒ½é‡å’Œå®¿ä¸»æ˜Ÿç³»ç‰¹æ€§æœ‰äº†æ›´æ·±å…¥çš„äº†è§£ã€‚åŸºäºå…‰è°±èƒ½é‡åˆ†å¸ƒï¼ˆSEDï¼‰çš„å…‰åº¦æ‹Ÿåˆæ–¹æ³•ï¼ŒCDF-S XT1çš„å®¿ä¸»æ˜Ÿç³»è¢«é™åˆ¶åœ¨z&#x3D;2.76å¤„ï¼Œå…¶ç»å¯¹æ˜Ÿç­‰ã€æ’æ˜Ÿè´¨é‡å’Œæ’æ˜Ÿå½¢æˆç‡å‡å¤„äºè¾ƒé«˜çš„æ°´å¹³ã€‚CDF-S XT2çš„JWSTæˆåƒæ­ç¤ºäº†å®¿ä¸»æ˜Ÿç³»ä¸­å­˜åœ¨çš„ä¸€ä¸ªæ–°çš„é«˜åº¦é®è”½ç»„åˆ†ï¼ŒNIRspecå…‰è°±è¿›ä¸€æ­¥ç¡®è®¤å…¶ä½äºz&#x3D;3.4598å¤„ã€‚å°½ç®¡å¯¹CDF-S XT1çš„æœ€ä½³èµ·æºå‡è®¾æ˜¯ä½äº®åº¦ä¼½é©¬å°„çº¿çˆ†å‘ï¼ˆGRBï¼‰ï¼Œä½†æˆ‘ä»¬å¹¶ä¸å®Œå…¨æ’é™¤ç£æ˜Ÿç›¸å…³äº‹ä»¶æˆ–æ¶‰åŠçŸ®æ˜Ÿå’Œè¶…è´¨é‡é»‘æ´çš„æ½®æ±æ’•è£‚äº‹ä»¶çš„å¯èƒ½æ€§ã€‚CDF-S XT2æ›´åƒæ˜¯ä¸€ç§ä½äº®åº¦è¶…æ–°æ˜Ÿäº‹ä»¶ï¼Œä¸å°„ç”µæ˜Ÿç³»ç°è±¡çš„å…‰æ›²çº¿ç›¸ä¼¼ï¼Œä½†å…¶ç¡®åˆ‡èµ·æºä»ä¸æ˜ç¡®ã€‚è¿™äº›è§‚æµ‹ç»“æœä¸ºæˆ‘ä»¬ç†è§£è¿™ä¸¤ç§FXTsçš„å®¿ä¸»æ˜Ÿç³»æä¾›äº†å…³é”®ä¾æ®ï¼Œæœ‰åŠ©äºæˆ‘ä»¬å¯¹ç¬æ€å¤©æ–‡å­¦æœ‰æ›´æ·±å…¥çš„è®¤è¯†ã€‚å°½ç®¡å¦‚æ­¤ï¼Œä»ç„¶ç¼ºå°‘èƒ½å¤ŸåŒæ—¶è§£é‡Šä¸¤ä¸ªæ¥æºçš„å”¯ä¸€è§£é‡Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>CDF-S XT1å’ŒXT2æ˜¯ä¸¤ç§é‡è¦çš„å¿«é€Ÿå°„ç”µæ˜Ÿç³»ç¬æ€äº‹ä»¶ï¼ˆFXTsï¼‰ã€‚</li>
<li>åˆ©ç”¨JWSTçš„æœ€æ–°æ•°æ®ï¼Œå¯¹CDF-S XT1å’ŒXT2çš„è·ç¦»ã€èƒ½é‡å’Œå®¿ä¸»æ˜Ÿç³»ç‰¹æ€§æœ‰äº†æ›´ç²¾ç¡®çš„äº†è§£ã€‚</li>
<li>CDF-S XT1å¯èƒ½æºè‡ªä½äº®åº¦ä¼½é©¬å°„çº¿çˆ†å‘ï¼ˆGRBï¼‰ï¼Œä½†ä¹Ÿè€ƒè™‘å…¶ä»–å¯èƒ½æ€§å¦‚ç£æ˜Ÿç›¸å…³äº‹ä»¶æˆ–æ½®æ±æ’•è£‚äº‹ä»¶ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.10015">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-636dd5d6f79f85f97ba1180a2ce19a90.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-95bb6a589529e9f25f9295ea1f0efbb5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ffe0135e902395478e57715f5ef6bde.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-544285440f40b16e3a5ea60d9a961be4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-84f9db6746bc71e3339bfc30e181f685.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d48a2ac3627e61a497413bef8d17bc9c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b9db5935a806e63849b7fa748ffc2e46.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="World-of-Forms-Deformable-Geometric-Templates-for-One-Shot-Surface-Meshing-in-Coronary-CT-Angiography"><a href="#World-of-Forms-Deformable-Geometric-Templates-for-One-Shot-Surface-Meshing-in-Coronary-CT-Angiography" class="headerlink" title="World of Forms: Deformable Geometric Templates for One-Shot Surface   Meshing in Coronary CT Angiography"></a>World of Forms: Deformable Geometric Templates for One-Shot Surface   Meshing in Coronary CT Angiography</h2><p><strong>Authors:Rudolf L. M. van Herten, Ioannis Lagogiannis, Jelmer M. Wolterink, Steffen Bruns, Eva R. Meulendijks, Damini Dey, Joris R. de Groot, JosÃ© P. Henriques, R. Nils Planken, Simone Saitta, Ivana IÅ¡gum</strong></p>
<p>Deep learning-based medical image segmentation and surface mesh generation typically involve a sequential pipeline from image to segmentation to meshes, often requiring large training datasets while making limited use of prior geometric knowledge. This may lead to topological inconsistencies and suboptimal performance in low-data regimes. To address these challenges, we propose a data-efficient deep learning method for direct 3D anatomical object surface meshing using geometric priors. Our approach employs a multi-resolution graph neural network that operates on a prior geometric template which is deformed to fit object boundaries of interest. We show how different templates may be used for the different surface meshing targets, and introduce a novel masked autoencoder pretraining strategy for 3D spherical data. The proposed method outperforms nnUNet in a one-shot setting for segmentation of the pericardium, left ventricle (LV) cavity and the LV myocardium. Similarly, the method outperforms other lumen segmentation operating on multi-planar reformatted images. Results further indicate that mesh quality is on par with or improves upon marching cubes post-processing of voxel mask predictions, while remaining flexible in the choice of mesh triangulation prior, thus paving the way for more accurate and topologically consistent 3D medical object surface meshing. </p>
<blockquote>
<p>åŸºäºæ·±åº¦å­¦ä¹ çš„åŒ»å­¦å›¾åƒåˆ†å‰²å’Œè¡¨é¢ç½‘æ ¼ç”Ÿæˆé€šå¸¸æ¶‰åŠä»å›¾åƒåˆ°åˆ†å‰²å†åˆ°ç½‘æ ¼çš„åºåˆ—æµç¨‹ï¼Œè¿™éœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®é›†ï¼Œè€Œå…ˆéªŒå‡ ä½•çŸ¥è¯†çš„åˆ©ç”¨å´å¾ˆæœ‰é™ã€‚è¿™å¯èƒ½å¯¼è‡´æ‹“æ‰‘ä¸ä¸€è‡´å’Œåœ¨ä½æ•°æ®ç¯å¢ƒä¸‹çš„æ€§èƒ½ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨å‡ ä½•å…ˆéªŒè¿›è¡Œç›´æ¥3Dè§£å‰–å¯¹è±¡è¡¨é¢ç½‘æ ¼åŒ–çš„é«˜æ•ˆæ•°æ®æ·±åº¦å­¦ä¹ æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨å¤šåˆ†è¾¨ç‡å›¾ç¥ç»ç½‘ç»œï¼Œåœ¨å…ˆéªŒå‡ ä½•æ¨¡æ¿ä¸Šè¿è¡Œï¼Œè¯¥æ¨¡æ¿ä¼šå˜å½¢ä»¥é€‚åº”ç›®æ ‡å¯¹è±¡çš„è¾¹ç•Œã€‚æˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•ä¸ºä¸åŒçš„è¡¨é¢ç½‘æ ¼ç›®æ ‡ä½¿ç”¨ä¸åŒçš„æ¨¡æ¿ï¼Œå¹¶å¼•å…¥äº†ç”¨äº3Dçƒå½¢æ•°æ®çš„å…¨æ–°æ©ç è‡ªç¼–ç å™¨é¢„è®­ç»ƒç­–ç•¥ã€‚æ‰€æå‡ºçš„æ–¹æ³•åœ¨ä¸€æ¬¡æˆåƒè®¾ç½®ä¸­å¯¹å¿ƒåŒ…ã€å·¦å¿ƒå®¤è…”å’Œå·¦å¿ƒå®¤å¿ƒè‚Œçš„åˆ†å‰²è¡¨ç°ä¼˜äºnnUNetã€‚åŒæ ·ï¼Œè¯¥æ–¹æ³•åœ¨é’ˆå¯¹å¤šå¹³é¢é‡å»ºå›¾åƒçš„æ“ä½œä¸­çš„è…”å®¤åˆ†å‰²è¡¨ç°ä¹Ÿä¼˜äºå…¶ä»–æ–¹æ³•ã€‚ç»“æœè¿˜è¡¨æ˜ï¼Œç½‘æ ¼è´¨é‡å¯ä¸åŸºäºä½“ç´ æ©è†œé¢„æµ‹çš„å…«å‰æ ‘è¡Œè¿›ç®—æ³•çš„åå¤„ç†ç›¸å½“æˆ–æ›´å¥½ï¼ŒåŒæ—¶åœ¨ç½‘æ ¼ä¸‰è§’å‰–åˆ†å…ˆéªŒçš„é€‰æ‹©ä¸Šå…·æœ‰çµæ´»æ€§ï¼Œä»è€Œä¸ºæ›´å‡†ç¡®å’Œæ‹“æ‰‘ä¸€è‡´çš„3DåŒ»å­¦å¯¹è±¡è¡¨é¢ç½‘æ ¼åŒ–é“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.11837v2">PDF</a> Submitted to Medical Image Analysis</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ•°æ®é«˜æ•ˆçš„æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºåŸºäºå‡ ä½•å…ˆéªŒçš„ç›´æ¥ä¸‰ç»´è§£å‰–å¯¹è±¡è¡¨é¢ç½‘æ ¼ç”Ÿæˆã€‚è¯¥æ–¹æ³•é‡‡ç”¨å¤šåˆ†è¾¨ç‡å›¾ç¥ç»ç½‘ç»œï¼Œåœ¨å…ˆéªŒå‡ ä½•æ¨¡æ¿ä¸Šè¿›è¡Œæ“ä½œï¼Œé€šè¿‡å˜å½¢ä»¥æ‹Ÿåˆæ„Ÿå…´è¶£å¯¹è±¡çš„è¾¹ç•Œã€‚è¯¥æ–¹æ³•è§£å†³äº†åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²å’Œè¡¨é¢ç½‘æ ¼ç”Ÿæˆä¸­é¢ä¸´çš„æ‹“æ‰‘ä¸ä¸€è‡´å’Œä½æ•°æ®ç¯å¢ƒä¸‹çš„æ€§èƒ½ä¸ä½³ç­‰é—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²å’Œè¡¨é¢ç½‘æ ¼ç”Ÿæˆä¸­é¢ä¸´æŒ‘æˆ˜ï¼Œå¦‚æ‹“æ‰‘ä¸ä¸€è‡´æ€§å’Œä½æ•°æ®ç¯å¢ƒä¸‹çš„æ€§èƒ½é—®é¢˜ã€‚</li>
<li>æå‡ºä¸€ç§æ•°æ®é«˜æ•ˆçš„æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼Œç›´æ¥è¿›è¡Œä¸‰ç»´è§£å‰–å¯¹è±¡è¡¨é¢ç½‘æ ¼ç”Ÿæˆï¼Œåˆ©ç”¨å‡ ä½•å…ˆéªŒä¿¡æ¯ã€‚</li>
<li>é‡‡ç”¨å¤šåˆ†è¾¨ç‡å›¾ç¥ç»ç½‘ç»œï¼Œæ“ä½œå…ˆéªŒå‡ ä½•æ¨¡æ¿ï¼Œé€šè¿‡å˜å½¢ä»¥æ‹Ÿåˆå¯¹è±¡è¾¹ç•Œã€‚</li>
<li>æ–¹æ³•å¯ç”¨äºä¸åŒçš„è¡¨é¢ç½‘æ ¼ç”Ÿæˆç›®æ ‡ï¼Œä½¿ç”¨ä¸åŒçš„æ¨¡æ¿ã€‚</li>
<li>å¼•å…¥ä¸€ç§æ–°çš„æ©ç è‡ªåŠ¨ç¼–ç å™¨é¢„è®­ç»ƒç­–ç•¥ï¼Œé€‚ç”¨äºä¸‰ç»´çƒå½¢æ•°æ®ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å•é•œå¤´è®¾ç½®ä¸‹å¯¹å¿ƒåŒ…ã€å·¦å¿ƒå®¤è…”å’Œå·¦å¿ƒå®¤å¿ƒè‚Œçš„åˆ†å‰²è¡¨ç°å‡ºä¼˜äºnnUNetçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.11837">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-86aff16868288e83f4a03f94ba613e59.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-319dcf10365aaf56159107a8df812c9c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-205d7929cb21dd001759cae401365b7e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-92e6c61cd4226e2d2f2bd9727acc9be1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-da6a6bb4eb992b452a3b31b3377b4a03.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-683561e2ca7bee81871ddb74a808ea23.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-199fc2935a5146004699b294a32a32d6.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="X-ray-Made-Simple-Radiology-Report-Generation-and-Evaluation-with-Laymanâ€™s-Terms"><a href="#X-ray-Made-Simple-Radiology-Report-Generation-and-Evaluation-with-Laymanâ€™s-Terms" class="headerlink" title="X-ray Made Simple: Radiology Report Generation and Evaluation with   Laymanâ€™s Terms"></a>X-ray Made Simple: Radiology Report Generation and Evaluation with   Laymanâ€™s Terms</h2><p><strong>Authors:Kun Zhao, Chenghao Xiao, Chen Tang, Bohao Yang, Kai Ye, Noura Al Moubayed, Liang Zhan, Chenghua Lin</strong></p>
<p>Radiology Report Generation (RRG) has achieved significant progress with the advancements of multimodal generative models. However, the evaluation in the domain suffers from a lack of fair and robust metrics. We reveal that, high performance on RRG with existing lexical-based metrics (e.g. BLEU) might be more of a mirage - a model can get a high BLEU only by learning the template of reports. This has become an urgent problem for RRG due to the highly patternized nature of these reports. In this work, we un-intuitively approach this problem by proposing the Laymanâ€™s RRG framework, a laymanâ€™s terms-based dataset, evaluation and training framework that systematically improves RRG with day-to-day language. We first contribute the translated Laymanâ€™s terms dataset. Building upon the dataset, we then propose a semantics-based evaluation method, which is proved to mitigate the inflated numbers of BLEU and provides fairer evaluation. Last, we show that training on the laymanâ€™s terms dataset encourages models to focus on the semantics of the reports, as opposed to overfitting to learning the report templates. We reveal a promising scaling law between the number of training examples and semantics gain provided by our dataset, compared to the inverse pattern brought by the original formats. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/hegehongcha/LaymanRRG">https://github.com/hegehongcha/LaymanRRG</a>. </p>
<blockquote>
<p>éšç€å¤šæ¨¡æ€ç”Ÿæˆæ¨¡å‹çš„è¿›æ­¥ï¼Œæ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆï¼ˆRRGï¼‰å·²ç»å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œè¯¥é¢†åŸŸçš„è¯„ä¼°ç¼ºä¹å…¬å¹³å’Œç¨³å¥çš„æŒ‡æ ‡ã€‚æˆ‘ä»¬æ­ç¤ºï¼Œåœ¨ç°æœ‰çš„åŸºäºè¯æ±‡çš„è¯„ä¼°æŒ‡æ ‡ï¼ˆä¾‹å¦‚BLEUï¼‰ä¸Šè¡¨ç°ä¼˜å¼‚çš„RRGå¯èƒ½æ›´åƒæ˜¯ä¸€ä¸ªå¹»è±¡â€”â€”æ¨¡å‹åªéœ€å­¦ä¹ æŠ¥å‘Šçš„æ¨¡æ¿å³å¯è·å¾—é«˜BLEUåˆ†æ•°ã€‚ç”±äºæŠ¥å‘Šçš„é«˜åº¦æ¨¡å¼åŒ–ç‰¹æ€§ï¼Œè¿™å¯¹RRGæ¥è¯´å·²ç»æˆä¸ºäº†ä¸€ä¸ªç´§è¿«çš„é—®é¢˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡æå‡ºLaymanâ€™s RRGæ¡†æ¶æ¥éä¼ ç»Ÿåœ°è§£å†³æ­¤é—®é¢˜ã€‚è¿™æ˜¯ä¸€ä¸ªåŸºäºå¤–è¡Œæœ¯è¯­çš„æ•°æ®é›†ã€è¯„ä¼°å’ŒåŸ¹è®­æ¡†æ¶ï¼Œå®ƒç³»ç»Ÿåœ°ä½¿ç”¨æ—¥å¸¸è¯­è¨€æ”¹è¿›äº†RRGã€‚æˆ‘ä»¬é¦–å…ˆè´¡çŒ®ç¿»è¯‘åçš„å¤–è¡Œæœ¯è¯­æ•°æ®é›†ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè¯­ä¹‰çš„è¯„ä¼°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•è¢«è¯æ˜å¯ä»¥ç¼“è§£BLEUçš„è†¨èƒ€æ•°å­—ï¼Œå¹¶æä¾›æ›´å…¬å¹³çš„è¯„ä¼°ã€‚æœ€åï¼Œæˆ‘ä»¬å±•ç¤ºåœ¨æ™®é€šæœ¯è¯­æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒä¼šé¼“åŠ±æ¨¡å‹å…³æ³¨æŠ¥å‘Šçš„è¯­ä¹‰ï¼Œè€Œä¸æ˜¯è¿‡åº¦æ‹Ÿåˆå­¦ä¹ æŠ¥å‘Šæ¨¡æ¿ã€‚æˆ‘ä»¬æ­ç¤ºäº†ä¸åŸå§‹æ ¼å¼å¸¦æ¥çš„é€†å‘æ¨¡å¼ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ•°æ®é›†æä¾›çš„è®­ç»ƒç¤ºä¾‹æ•°é‡å’Œè¯­ä¹‰å¢ç›Šä¹‹é—´çš„ä¸€ä¸ªä»¤äººé¼“èˆçš„è§„æ¨¡åŒ–è§„å¾‹ã€‚æˆ‘ä»¬çš„ä»£ç ä½äºï¼š<a target="_blank" rel="noopener" href="https://github.com/hegehongcha/LaymanRRG%E3%80%82">https://github.com/hegehongcha/LaymanRRGã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.17911v4">PDF</a> This paper has substantial data and conceptual changes since release   that go beyond simple updating the existing one. As a result, the authors   have changed and we need to re-coordinate and reach consensus. So we decide   to withdraw it</p>
<p><strong>Summary</strong></p>
<p>éšç€å¤šæ¨¡æ€ç”Ÿæˆæ¨¡å‹çš„å‘å±•ï¼Œæ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆï¼ˆRRGï¼‰å·²ç»å–å¾—äº†æ˜¾è‘—è¿›æ­¥ã€‚ç„¶è€Œï¼Œè¯¥é¢†åŸŸçš„è¯„ä¼°ç¼ºä¹å…¬å¹³å’Œç¨³å¥çš„æŒ‡æ ‡ã€‚ç°æœ‰åŸºäºè¯æ±‡çš„è¯„ä¼°æŒ‡æ ‡ï¼ˆå¦‚BLEUï¼‰å¯èƒ½å­˜åœ¨è¯¯å¯¼æ€§ï¼Œæ¨¡å‹åªéœ€å­¦ä¹ æŠ¥å‘Šæ¨¡æ¿å³å¯è·å¾—é«˜åˆ†ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†å¹³æ°‘åŒ–RRGæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åŸºäºå¹³æ°‘æœ¯è¯­æ•°æ®é›†ã€è¯„ä¼°å’Œè®­ç»ƒæ¡†æ¶ï¼Œç³»ç»Ÿåœ°æ”¹è¿›äº†RRGçš„æ—¥å¸¸è¯­è¨€ä½¿ç”¨ã€‚æœ¬æ–‡é¦–å…ˆè´¡çŒ®ç¿»è¯‘åçš„å¹³æ°‘æœ¯è¯­æ•°æ®é›†ï¼Œç„¶ååœ¨æ­¤åŸºç¡€ä¸Šæå‡ºäº†åŸºäºè¯­ä¹‰çš„è¯„ä¼°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç¼“è§£äº†BLEUæŒ‡æ ‡çš„è†¨èƒ€æ•°å­—é—®é¢˜ï¼Œæä¾›äº†æ›´å…¬å¹³çš„è¯„ä¼°ã€‚æœ€åï¼Œæœ¬æ–‡å±•ç¤ºäº†åœ¨å¹³æ°‘æœ¯è¯­æ•°æ®é›†ä¸Šè®­ç»ƒæ¨¡å‹èƒ½å¤Ÿé¼“åŠ±æ¨¡å‹å…³æ³¨æŠ¥å‘Šçš„è¯­ä¹‰å†…å®¹ï¼Œè€Œä¸æ˜¯è¿‡åº¦æ‹ŸåˆæŠ¥å‘Šæ¨¡æ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€ç”Ÿæˆæ¨¡å‹åœ¨æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆï¼ˆRRGï¼‰æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†è¯„ä¼°æŒ‡æ ‡ç¼ºä¹å…¬å¹³æ€§å’Œç¨³å¥æ€§ã€‚</li>
<li>ç°æœ‰åŸºäºè¯æ±‡çš„è¯„ä¼°æŒ‡æ ‡ï¼ˆå¦‚BLEUï¼‰å¯èƒ½è¯¯å¯¼ï¼Œæ¨¡å‹åªéœ€å­¦ä¹ æŠ¥å‘Šæ¨¡æ¿å³å¯è·å¾—é«˜åˆ†ã€‚</li>
<li>æå‡ºå¹³æ°‘åŒ–RRGæ¡†æ¶ï¼Œä½¿ç”¨æ—¥å¸¸è¯­è¨€ï¼Œæ”¹è¿›RRGçš„ç³»ç»Ÿæ€§ã€‚</li>
<li>è´¡çŒ®ç¿»è¯‘åçš„å¹³æ°‘æœ¯è¯­æ•°æ®é›†ï¼Œä¸ºRRGæä¾›æ–°çš„èµ„æºã€‚</li>
<li>æå‡ºåŸºäºè¯­ä¹‰çš„è¯„ä¼°æ–¹æ³•ï¼Œç¼“è§£BLEUæŒ‡æ ‡çš„è†¨èƒ€æ•°å­—é—®é¢˜ï¼Œæä¾›æ›´å…¬å¹³è¯„ä¼°ã€‚</li>
<li>åœ¨å¹³æ°‘æœ¯è¯­æ•°æ®é›†ä¸Šè®­ç»ƒæ¨¡å‹ï¼Œé¼“åŠ±æ¨¡å‹å…³æ³¨æŠ¥å‘Šçš„è¯­ä¹‰å†…å®¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.17911">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-552ab3303885eb6360ae9d5aee4540d4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-536c29e406e6c54389a4db0dd6eb1d76.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-13c0fefe723737aac386eb7ae5ccc0a9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-94455d6af20718fc233e54a33313c221.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-26/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-26/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-26/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-5b0038d0597045f5df63286c9131c0a2.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-26  Llasa Scaling Train-Time and Inference-Time Compute for Llama-based   Speech Synthesis
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-26
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-26/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-728e0cefbefe5b2e1745f9050740922c.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-26  GCC Generative Color Constancy via Diffusing a Color Checker
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-26
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32883.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
