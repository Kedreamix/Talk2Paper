<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-26  Low-Rank and Sparse Model Merging for Multi-Lingual Speech Recognition   and Translation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-07220ade929d30e1efc8ef08be33fa8e.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-26
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    78 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-02-26-æ›´æ–°"><a href="#2025-02-26-æ›´æ–°" class="headerlink" title="2025-02-26 æ›´æ–°"></a>2025-02-26 æ›´æ–°</h1><h2 id="Low-Rank-and-Sparse-Model-Merging-for-Multi-Lingual-Speech-Recognition-and-Translation"><a href="#Low-Rank-and-Sparse-Model-Merging-for-Multi-Lingual-Speech-Recognition-and-Translation" class="headerlink" title="Low-Rank and Sparse Model Merging for Multi-Lingual Speech Recognition   and Translation"></a>Low-Rank and Sparse Model Merging for Multi-Lingual Speech Recognition   and Translation</h2><p><strong>Authors:Qiuming Zhao, Guangzhi Sun, Chao Zhang, Mingxing Xu, Thomas Fang Zheng</strong></p>
<p>Language diversity presents a significant challenge in speech-to-text (S2T) tasks, such as automatic speech recognition and translation. Traditional multi-task training approaches aim to address this by jointly optimizing multiple speech recognition and translation tasks across various languages. While models like Whisper, built on these strategies, demonstrate strong performance, they still face issues of high computational cost, language interference, suboptimal training configurations, and limited extensibility. To overcome these challenges, we introduce LoRS-Merging (low-rank and sparse model merging), a novel technique designed to efficiently integrate models trained on different languages or tasks while preserving performance and reducing computational overhead. LoRS-Merging combines low-rank and sparse pruning to retain essential structures while eliminating redundant parameters, mitigating language and task interference, and enhancing extensibility. Experimental results across a range of languages demonstrate that LoRS-Merging significantly outperforms conventional multi-lingual multi-task training baselines. Our findings suggest that model merging, particularly LoRS-Merging, is a scalable and effective complement to traditional multi-lingual training strategies for S2T applications. </p>
<blockquote>
<p>è¯­è¨€å¤šæ ·æ€§åœ¨è¯­éŸ³è¯†åˆ«åˆ°æ–‡æœ¬ï¼ˆS2Tï¼‰çš„ä»»åŠ¡ä¸­ï¼Œå¦‚è‡ªåŠ¨è¯­éŸ³è¯†åˆ«å’Œç¿»è¯‘ï¼Œæ„æˆäº†ä¸€å¤§æŒ‘æˆ˜ã€‚ä¼ ç»Ÿå¤šä»»åŠ¡è®­ç»ƒçš„æ–¹æ³•æ—¨åœ¨é€šè¿‡è”åˆä¼˜åŒ–å¤šç§è¯­è¨€çš„è¯­éŸ³è¯†åˆ«å’Œç¿»è¯‘ä»»åŠ¡æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚è™½ç„¶åŸºäºè¿™äº›ç­–ç•¥çš„æ¨¡å‹ï¼Œå¦‚whisperï¼Œè¡¨ç°å‡ºäº†å¼ºå¤§çš„æ€§èƒ½ï¼Œä½†å®ƒä»¬ä»ç„¶é¢ä¸´è®¡ç®—æˆæœ¬é«˜ã€è¯­è¨€å¹²æ‰°ã€è®­ç»ƒé…ç½®ä¸ä½³å’Œæ‰©å±•æ€§æœ‰é™çš„è¯¸å¤šé—®é¢˜ã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†LoRS-Mergingï¼ˆä½ç§©å’Œç¨€ç–æ¨¡å‹åˆå¹¶ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨é«˜æ•ˆé›†æˆåœ¨ä¸åŒè¯­è¨€æˆ–ä»»åŠ¡ä¸Šè®­ç»ƒçš„æ¨¡å‹çš„æ–°æŠ€æœ¯ï¼ŒåŒæ—¶ä¿ç•™æ€§èƒ½å¹¶é™ä½è®¡ç®—å¼€é”€ã€‚LoRS-Mergingç»“åˆäº†ä½ç§©å’Œç¨€ç–å‰ªæï¼Œä»¥ä¿ç•™å…³é”®ç»“æ„å¹¶æ¶ˆé™¤å†—ä½™å‚æ•°ï¼Œå‡è½»è¯­è¨€å’Œä»»åŠ¡å¹²æ‰°ï¼Œæé«˜æ‰©å±•æ€§ã€‚è·¨å¤šç§è¯­è¨€çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒLoRS-Mergingæ˜¾è‘—ä¼˜äºä¼ ç»Ÿå¤šè¯­ç§å¤šä»»åŠ¡è®­ç»ƒåŸºçº¿ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæ¨¡å‹åˆå¹¶ï¼Œå°¤å…¶æ˜¯LoRS-Mergingï¼Œæ˜¯S2Tåº”ç”¨ä¸­ä¼ ç»Ÿå¤šè¯­ç§è®­ç»ƒç­–ç•¥çš„å¯æ‰©å±•å’Œæœ‰æ•ˆçš„è¡¥å……ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17380v1">PDF</a> 13 pages, submitted to ACL 2025</p>
<p><strong>Summary</strong></p>
<p>è¯­è¨€å¤šæ ·æ€§åœ¨è¯­éŸ³è½¬æ–‡æœ¬ï¼ˆS2Tï¼‰ä»»åŠ¡ä¸­æ˜¯ä¸€å¤§æŒ‘æˆ˜ï¼Œå¦‚è‡ªåŠ¨è¯­éŸ³è¯†åˆ«å’Œç¿»è¯‘ã€‚ä¼ ç»Ÿå¤šä»»åŠ¡è®­ç»ƒæ—¨åœ¨é€šè¿‡è”åˆä¼˜åŒ–å¤šç§è¯­è¨€å’Œç¿»è¯‘ä»»åŠ¡æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚è™½ç„¶åŸºäºè¿™äº›ç­–ç•¥æ„å»ºçš„æ¨¡å‹å¦‚Whisperè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†å®ƒä»¬ä»é¢ä¸´è®¡ç®—æˆæœ¬é«˜ã€è¯­è¨€å¹²æ‰°ã€è®­ç»ƒé…ç½®ä¸ä½³å’Œæ‰©å±•æ€§æœ‰é™ç­‰é—®é¢˜ã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†LoRS-Mergingï¼ˆä½ç§©å’Œç¨€ç–æ¨¡å‹åˆå¹¶ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§è®¾è®¡ç”¨äºé«˜æ•ˆé›†æˆä¸åŒè¯­è¨€æˆ–ä»»åŠ¡è®­ç»ƒæ¨¡å‹çš„æ–°æŠ€æœ¯ï¼ŒåŒæ—¶ä¿æŒæ€§èƒ½å¹¶é™ä½è®¡ç®—å¼€é”€ã€‚LoRS-Mergingç»“åˆä½ç§©å’Œç¨€ç–å‰ªæï¼Œä¿ç•™å…³é”®ç»“æ„ï¼Œæ¶ˆé™¤å†—ä½™å‚æ•°ï¼Œå‡è½»è¯­è¨€å’Œä»»åŠ¡å¹²æ‰°ï¼Œæé«˜æ‰©å±•æ€§ã€‚è·¨å¤šç§è¯­è¨€çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒLoRS-Mergingæ˜¾è‘—ä¼˜äºä¼ ç»Ÿå¤šè¯­è¨€å¤šä»»åŠ¡è®­ç»ƒåŸºçº¿ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæ¨¡å‹åˆå¹¶ï¼Œå°¤å…¶æ˜¯LoRS-Mergingï¼Œæ˜¯S2Tåº”ç”¨ä¸­ä¼ ç»Ÿå¤šè¯­è¨€è®­ç»ƒç­–ç•¥çš„å¯æ‰©å±•å’Œæœ‰æ•ˆçš„è¡¥å……ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­è¨€å¤šæ ·æ€§åœ¨è¯­éŸ³è½¬æ–‡æœ¬ä»»åŠ¡ä¸­æ„æˆé‡å¤§æŒ‘æˆ˜ã€‚</li>
<li>ä¼ ç»Ÿå¤šä»»åŠ¡è®­ç»ƒç­–ç•¥æ—¨åœ¨é€šè¿‡è”åˆä¼˜åŒ–å¤šç§è¯­è¨€å’Œç¿»è¯‘ä»»åŠ¡æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚</li>
<li>è™½ç„¶ç°æœ‰æ¨¡å‹å¦‚Whisperæ€§èƒ½å¼ºå¤§ï¼Œä½†ä»å­˜åœ¨è®¡ç®—æˆæœ¬é«˜ã€è¯­è¨€å¹²æ‰°å’Œæ‰©å±•æ€§æœ‰é™ç­‰é—®é¢˜ã€‚</li>
<li>LoRS-MergingæŠ€æœ¯æ—¨åœ¨é€šè¿‡é«˜æ•ˆé›†æˆä¸åŒè¯­è¨€æˆ–ä»»åŠ¡çš„è®­ç»ƒæ¨¡å‹æ¥å…‹æœè¿™äº›æŒ‘æˆ˜ã€‚</li>
<li>LoRS-Mergingç»“åˆä½ç§©å’Œç¨€ç–å‰ªæï¼Œä»¥ä¿ç•™å…³é”®ç»“æ„å¹¶æ¶ˆé™¤å†—ä½™å‚æ•°ã€‚</li>
<li>LoRS-Mergingèƒ½å‡è½»è¯­è¨€å’Œä»»åŠ¡é—´çš„å¹²æ‰°ï¼Œæé«˜æ¨¡å‹çš„æ‰©å±•æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17380">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-a951e7f6c65b72a31f20eaaec8ae202f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-14c7cb6d6488c9352a7f842e385f3f21.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3ac6825b1d9c0026c1f7b41a51bbe40.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Baichuan-Audio-A-Unified-Framework-for-End-to-End-Speech-Interaction"><a href="#Baichuan-Audio-A-Unified-Framework-for-End-to-End-Speech-Interaction" class="headerlink" title="Baichuan-Audio: A Unified Framework for End-to-End Speech Interaction"></a>Baichuan-Audio: A Unified Framework for End-to-End Speech Interaction</h2><p><strong>Authors:Tianpeng Li, Jun Liu, Tao Zhang, Yuanbo Fang, Da Pan, Mingrui Wang, Zheng Liang, Zehuan Li, Mingan Lin, Guosheng Dong, Jianhua Xu, Haoze Sun, Zenan Zhou, Weipeng Chen</strong></p>
<p>We introduce Baichuan-Audio, an end-to-end audio large language model that seamlessly integrates audio understanding and generation. It features a text-guided aligned speech generation mechanism, enabling real-time speech interaction with both comprehension and generation capabilities. Baichuan-Audio leverages a pre-trained ASR model, followed by multi-codebook discretization of speech at a frame rate of 12.5 Hz. This multi-codebook setup ensures that speech tokens retain both semantic and acoustic information. To further enhance modeling, an independent audio head is employed to process audio tokens, effectively capturing their unique characteristics. To mitigate the loss of intelligence during pre-training and preserve the original capabilities of the LLM, we propose a two-stage pre-training strategy that maintains language understanding while enhancing audio modeling. Following alignment, the model excels in real-time speech-based conversation and exhibits outstanding question-answering capabilities, demonstrating its versatility and efficiency. The proposed model demonstrates superior performance in real-time spoken dialogue and exhibits strong question-answering abilities. Our code, model and training data are available at <a target="_blank" rel="noopener" href="https://github.com/baichuan-inc/Baichuan-Audio">https://github.com/baichuan-inc/Baichuan-Audio</a> </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºç™¾å·éŸ³é¢‘ï¼ˆBaichuan-Audioï¼‰â€”â€”ç«¯åˆ°ç«¯çš„éŸ³é¢‘å¤§è¯­è¨€æ¨¡å‹ï¼Œå®ƒæ— ç¼é›†æˆäº†éŸ³é¢‘ç†è§£å’Œç”Ÿæˆã€‚è¯¥æ¨¡å‹å…·å¤‡æ–‡æœ¬å¼•å¯¼çš„å¯¹é½è¯­éŸ³ç”Ÿæˆæœºåˆ¶ï¼Œèƒ½å¤Ÿå®ç°å…·æœ‰ç†è§£å’Œç”Ÿæˆèƒ½åŠ›çš„å®æ—¶è¯­éŸ³äº¤äº’ã€‚ç™¾å·éŸ³é¢‘åˆ©ç”¨é¢„è®­ç»ƒçš„è¯­éŸ³è¯†åˆ«æ¨¡å‹ï¼Œéšåä»¥12.5Hzçš„å¸§ç‡è¿›è¡Œå¤šä»£ç ç°¿ç¦»æ•£åŒ–è¯­éŸ³ã€‚è¿™ç§å¤šä»£ç ç°¿è®¾ç½®ç¡®ä¿è¯­éŸ³æ ‡è®°ä¿ç•™è¯­ä¹‰å’Œå£°éŸ³ä¿¡æ¯ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜å»ºæ¨¡æ•ˆæœï¼Œé‡‡ç”¨ç‹¬ç«‹çš„éŸ³é¢‘å¤´æ¥å¤„ç†éŸ³é¢‘æ ‡è®°ï¼Œæœ‰æ•ˆåœ°æ•æ‰å…¶ç‹¬ç‰¹ç‰¹å¾ã€‚ä¸ºäº†å‡è½»é¢„è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ™ºåŠ›æŸå¤±å¹¶ä¿ç•™å¤§è¯­è¨€æ¨¡å‹çš„åŸå§‹èƒ½åŠ›ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤é˜¶æ®µé¢„è®­ç»ƒç­–ç•¥ï¼Œæ—¢ä¿æŒè¯­è¨€ç†è§£åˆå¢å¼ºéŸ³é¢‘å»ºæ¨¡ã€‚å¯¹é½åï¼Œè¯¥æ¨¡å‹åœ¨å®æ—¶è¯­éŸ³å¯¹è¯ä¸­è¡¨ç°å‡ºè‰²ï¼Œå…·æœ‰å‡ºè‰²çš„é—®ç­”èƒ½åŠ›ï¼Œä½“ç°äº†å…¶é€šç”¨æ€§å’Œæ•ˆç‡ã€‚æ‰€æå‡ºçš„æ¨¡å‹åœ¨å®æ—¶å£è¯­å¯¹è¯ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå±•ç°å‡ºå¼ºå¤§çš„é—®ç­”èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ä»£ç ã€æ¨¡å‹å’Œè®­ç»ƒæ•°æ®å¯åœ¨ <a target="_blank" rel="noopener" href="https://github.com/baichuan-inc/Baichuan-Audio">https://github.com/baichuan-inc/Baichuan-Audio</a> æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17239v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å·´å·éŸ³é¢‘æ¨¡å‹æ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„éŸ³é¢‘å¤§è¯­è¨€æ¨¡å‹ï¼Œèåˆäº†éŸ³é¢‘ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ï¼Œå®ç°å®æ—¶è¯­éŸ³äº¤äº’ã€‚æ¨¡å‹å…·å¤‡æ–‡æœ¬æŒ‡å¯¼å¯¹é½çš„è¯­éŸ³ç”Ÿæˆæœºåˆ¶å’Œå¤šç æœ¬ç¦»æ•£åŒ–æŠ€æœ¯ï¼Œèƒ½æœ‰æ•ˆæ•æ‰è¯­éŸ³çš„è¯­ä¹‰å’Œå£°éŸ³ä¿¡æ¯ã€‚é‡‡ç”¨ç‹¬ç«‹éŸ³é¢‘å¤´å¤„ç†éŸ³é¢‘æ ‡è®°ï¼Œå¹¶é‡‡ç”¨ä¸¤é˜¶æ®µé¢„è®­ç»ƒç­–ç•¥å¢å¼ºå»ºæ¨¡æ•ˆæœå¹¶ç»´æŠ¤è¯­è¨€ç†è§£åŠŸèƒ½ã€‚æ¨¡å‹å…·æœ‰ä¼˜ç§€çš„å®æ—¶å¯¹è¯æ€§èƒ½ï¼Œå±•ç°äº†å¼ºå¤§çš„é—®ç­”èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å·´å·éŸ³é¢‘æ¨¡å‹æ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„éŸ³é¢‘å¤§è¯­è¨€æ¨¡å‹ï¼Œèåˆäº†éŸ³é¢‘ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>æ¨¡å‹å…·å¤‡æ–‡æœ¬æŒ‡å¯¼å¯¹é½çš„è¯­éŸ³ç”Ÿæˆæœºåˆ¶ï¼Œå®ç°å®æ—¶è¯­éŸ³äº¤äº’ã€‚</li>
<li>å¤šç æœ¬ç¦»æ•£åŒ–æŠ€æœ¯ç¡®ä¿è¯­éŸ³æ ‡è®°æ•æ‰è¯­ä¹‰å’Œå£°éŸ³ä¿¡æ¯ã€‚</li>
<li>ç‹¬ç«‹éŸ³é¢‘å¤´å¤„ç†éŸ³é¢‘æ ‡è®°ï¼Œæå‡æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>é‡‡ç”¨ä¸¤é˜¶æ®µé¢„è®­ç»ƒç­–ç•¥å¢å¼ºå»ºæ¨¡æ•ˆæœï¼ŒåŒæ—¶ç»´æŠ¤è¯­è¨€ç†è§£åŠŸèƒ½ã€‚</li>
<li>æ¨¡å‹åœ¨å®æ—¶å¯¹è¯ä¸­è¡¨ç°å‡ºä¼˜ç§€çš„æ€§èƒ½ï¼Œå…·å¤‡å¼ºå¤§çš„é—®ç­”èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17239">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4e65f1fb528ef368b90aa438759522d3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fb90520650bcb0fa719fc644ddec4d2e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-094f0a0bedcf5742f279254e52431328.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8658fe40d30f7b1f7ff1b7d64af37eea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07220ade929d30e1efc8ef08be33fa8e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ad397b9225b4b44275d63854a6c0184a.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="NatSGLD-A-Dataset-with-Speech-Gesture-Logic-and-Demonstration-for-Robot-Learning-in-Natural-Human-Robot-Interaction"><a href="#NatSGLD-A-Dataset-with-Speech-Gesture-Logic-and-Demonstration-for-Robot-Learning-in-Natural-Human-Robot-Interaction" class="headerlink" title="NatSGLD: A Dataset with Speech, Gesture, Logic, and Demonstration for   Robot Learning in Natural Human-Robot Interaction"></a>NatSGLD: A Dataset with Speech, Gesture, Logic, and Demonstration for   Robot Learning in Natural Human-Robot Interaction</h2><p><strong>Authors:Snehesh Shrestha, Yantian Zha, Saketh Banagiri, Ge Gao, Yiannis Aloimonos, Cornelia FermÃ¼ller</strong></p>
<p>Recent advances in multimodal Human-Robot Interaction (HRI) datasets emphasize the integration of speech and gestures, allowing robots to absorb explicit knowledge and tacit understanding. However, existing datasets primarily focus on elementary tasks like object pointing and pushing, limiting their applicability to complex domains. They prioritize simpler human command data but place less emphasis on training robots to correctly interpret tasks and respond appropriately. To address these gaps, we present the NatSGLD dataset, which was collected using a Wizard of Oz (WoZ) method, where participants interacted with a robot they believed to be autonomous. NatSGLD records humansâ€™ multimodal commands (speech and gestures), each paired with a demonstration trajectory and a Linear Temporal Logic (LTL) formula that provides a ground-truth interpretation of the commanded tasks. This dataset serves as a foundational resource for research at the intersection of HRI and machine learning. By providing multimodal inputs and detailed annotations, NatSGLD enables exploration in areas such as multimodal instruction following, plan recognition, and human-advisable reinforcement learning from demonstrations. We release the dataset and code under the MIT License at <a target="_blank" rel="noopener" href="https://www.snehesh.com/natsgld/">https://www.snehesh.com/natsgld/</a> to support future HRI research. </p>
<blockquote>
<p>è¿‘æœŸå¤šæ¨¡æ€äººæœºäº’åŠ¨ï¼ˆHRIï¼‰æ•°æ®é›†çš„è¿›æ­¥å¼ºè°ƒè¯­éŸ³å’Œæ‰‹åŠ¿çš„æ•´åˆï¼Œè®©æœºå™¨äººèƒ½å¤Ÿå¸æ”¶æ˜ç¡®çš„çŸ¥è¯†å’Œéšå«çš„ç†è§£ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ•°æ®é›†ä¸»è¦ä¸“æ³¨äºåŸºæœ¬çš„ä»»åŠ¡ï¼Œå¦‚æŒ‡å‘ç‰©ä½“å’Œæ¨åŠ¨ç‰©ä½“ï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨å¤æ‚é¢†åŸŸçš„åº”ç”¨ã€‚å®ƒä»¬ä¼˜å…ˆå¤„ç†æ›´ç®€å•çš„äººç±»å‘½ä»¤æ•°æ®ï¼Œä½†è¾ƒå°‘å¼ºè°ƒè®­ç»ƒæœºå™¨äººæ­£ç¡®è§£é‡Šä»»åŠ¡å¹¶é€‚å½“å›åº”ã€‚ä¸ºäº†è§£å†³è¿™äº›ç©ºç™½ï¼Œæˆ‘ä»¬æ¨å‡ºäº†NatSGLDæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†æ˜¯é‡‡ç”¨â€œå¥¥å…¹å·«å¸ˆâ€ï¼ˆWoZï¼‰æ–¹æ³•æ”¶é›†çš„ï¼Œå‚ä¸è€…ä¸ä»–ä»¬è®¤ä¸ºå…·æœ‰è‡ªä¸»æ€§çš„æœºå™¨äººè¿›è¡Œäº†äº’åŠ¨ã€‚NatSGLDè®°å½•äº†äººç±»çš„å¤šæ¨¡æ€å‘½ä»¤ï¼ˆè¯­éŸ³å’Œæ‰‹åŠ¿ï¼‰ï¼Œæ¯ä¸ªå‘½ä»¤éƒ½ä¸æ¼”ç¤ºè½¨è¿¹å’Œçº¿æ€§æ—¶åºé€»è¾‘ï¼ˆLTLï¼‰å…¬å¼é…å¯¹ï¼Œä¸ºå‘½ä»¤ä»»åŠ¡æä¾›äº†çœŸå®åœ°é¢çš„è§£é‡Šã€‚è¯¥æ•°æ®é›†æ˜¯HRIå’Œæœºå™¨å­¦ä¹ äº¤å‰ç ”ç©¶çš„åŸºç¡€èµ„æºã€‚é€šè¿‡æä¾›å¤šæ¨¡æ€è¾“å…¥å’Œè¯¦ç»†æ³¨é‡Šï¼ŒNatSGLDå¯ä»¥åœ¨å¤šæ¨¡æ€æŒ‡ä»¤è·Ÿè¸ªã€è®¡åˆ’è¯†åˆ«å’ŒåŸºäºæ¼”ç¤ºçš„äººç±»å¯å»ºè®®å¼ºåŒ–å­¦ä¹ ç­‰é¢†åŸŸè¿›è¡Œæ¢ç´¢ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://www.snehesh.com/natsgld/%E4%B8%8B%E4%BB%A5MIT%E8%AE%B8%E5%8F%AF%E8%AF%81%E7%9A%84%E5%BD%A2%E5%BC%8F%E5%8F%91%E5%B8%83%E6%95%B0%E6%8D%AE%E9%9B%86%E5%92%8C%E4%BB%A3%E7%A0%81%EF%BC%8C%E4%BB%A5%E6%94%AF%E6%8C%81%E6%9C%AA%E6%9D%A5%E7%9A%84%E4%BA%BA%E6%9C%BA%E4%BA%92%E5%8A%A8%E7%A0%94%E7%A9%B6%E3%80%82">https://www.snehesh.com/natsgld/ä¸‹ä»¥MITè®¸å¯è¯çš„å½¢å¼å‘å¸ƒæ•°æ®é›†å’Œä»£ç ï¼Œä»¥æ”¯æŒæœªæ¥çš„äººæœºäº’åŠ¨ç ”ç©¶ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.16718v1">PDF</a> arXiv admin note: substantial text overlap with arXiv:2403.02274</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸå¤šæ¨¡æ€äººæœºäº¤äº’ï¼ˆHRIï¼‰æ•°æ®é›†çš„å‘å±•å¼ºè°ƒäº†è¯­éŸ³å’Œæ‰‹åŠ¿çš„é›†æˆï¼Œä½¿æœºå™¨äººèƒ½å¤Ÿå¸æ”¶æ˜¾æ€§çŸ¥è¯†å’Œéšæ€§ç†è§£ã€‚ç„¶è€Œï¼Œç°æœ‰æ•°æ®é›†ä¸»è¦å…³æ³¨åŸºç¡€ä»»åŠ¡ï¼Œå¦‚æŒ‡å‘å’Œæ¨åŠ¨ç‰©ä½“ï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨å¤æ‚é¢†åŸŸçš„åº”ç”¨ã€‚NatSGLDæ•°æ®é›†çš„æ¨å‡ºè§£å†³äº†è¿™ä¸€é—®é¢˜ï¼Œå®ƒé‡‡ç”¨å‚ä¸è€…è¯¯ä»¥ä¸ºæœºå™¨äººæ˜¯è‡ªä¸»çš„â€œWizard of Ozâ€ï¼ˆWoZï¼‰æ–¹æ³•æ”¶é›†è€Œæˆã€‚NatSGLDè®°å½•äº†äººç±»çš„å¤šæ¨¡æ€å‘½ä»¤ï¼ˆåŒ…æ‹¬è¯­éŸ³å’Œæ‰‹åŠ¿ï¼‰ï¼Œæ¯ä¸ªå‘½ä»¤éƒ½é…æœ‰æ¼”ç¤ºè½¨è¿¹å’Œçº¿æ€§æ—¶åºé€»è¾‘ï¼ˆLTLï¼‰å…¬å¼ï¼Œä¸ºå‘½ä»¤ä»»åŠ¡æä¾›äº†çœŸå®è§£è¯»ã€‚è¯¥æ•°æ®é›†ä¸ºHRIå’Œæœºå™¨å­¦ä¹ äº¤å‰é¢†åŸŸçš„ç ”ç©¶æä¾›äº†åŸºç¡€èµ„æºï¼Œé€šè¿‡æä¾›å¤šæ¨¡æ€è¾“å…¥å’Œè¯¦ç»†æ³¨é‡Šï¼Œæ”¯æŒå¯¹å¤šæ¨¡æ€æŒ‡ä»¤è·Ÿè¸ªã€è®¡åˆ’è¯†åˆ«å’Œäººç±»å¯æŒ‡å¯¼çš„å¼ºåŒ–å­¦ä¹ ç­‰é¢†åŸŸçš„æ¢ç´¢ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€HRIæ•°æ®é›†èåˆè¯­éŸ³å’Œæ‰‹åŠ¿ï¼Œå¢å¼ºæœºå™¨äººå¯¹çŸ¥è¯†çš„å¸æ”¶å’Œç†è§£ã€‚</li>
<li>ç°æœ‰æ•°æ®é›†å¤šå…³æ³¨åŸºç¡€ä»»åŠ¡ï¼Œé™åˆ¶äº†å…¶åœ¨å¤æ‚åœºæ™¯çš„åº”ç”¨ã€‚</li>
<li>NatSGLDæ•°æ®é›†é‡‡ç”¨WoZæ–¹æ³•æ”¶é›†ï¼ŒåŒ…å«äººç±»å¤šæ¨¡æ€å‘½ä»¤ã€æ¼”ç¤ºè½¨è¿¹å’ŒLTLå…¬å¼ã€‚</li>
<li>NatSGLDä¸ºHRIå’Œæœºå™¨å­¦ä¹ ç ”ç©¶æä¾›åŸºç¡€èµ„æºã€‚</li>
<li>æ•°æ®é›†æ”¯æŒå¤šæ¨¡æ€æŒ‡ä»¤è·Ÿè¸ªã€è®¡åˆ’è¯†åˆ«å’Œå¼ºåŒ–å­¦ä¹ ç­‰é¢†åŸŸçš„æ¢ç´¢ã€‚</li>
<li>NatSGLDæ•°æ®é›†å’Œä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://www.snehesh.com/natsgld/%EF%BC%8C%E4%BE%9B%E6%9C%AA%E6%9D%A5HRI%E7%A0%94%E7%A9%B6%E4%BD%BF%E7%94%A8%E3%80%82">https://www.snehesh.com/natsgld/ï¼Œä¾›æœªæ¥HRIç ”ç©¶ä½¿ç”¨ã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.16718">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-01e6731a6d950c83a6cfd0e8603270d2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2fbe2585eaec0a28a544f09950b10230.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a69ba83e8fdf84be4dc5fcaf5f45b0f.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="MemeIntel-Explainable-Detection-of-Propagandistic-and-Hateful-Memes"><a href="#MemeIntel-Explainable-Detection-of-Propagandistic-and-Hateful-Memes" class="headerlink" title="MemeIntel: Explainable Detection of Propagandistic and Hateful Memes"></a>MemeIntel: Explainable Detection of Propagandistic and Hateful Memes</h2><p><strong>Authors:Mohamed Bayan Kmainasi, Abul Hasnat, Md Arid Hasan, Ali Ezzat Shahroor, Firoj Alam</strong></p>
<p>The proliferation of multimodal content on social media presents significant challenges in understanding and moderating complex, context-dependent issues such as misinformation, hate speech, and propaganda. While efforts have been made to develop resources and propose new methods for automatic detection, limited attention has been given to label detection and the generation of explanation-based rationales for predicted labels. To address this challenge, we introduce MemeIntel, an explanation-enhanced dataset for propaganda memes in Arabic and hateful memes in English, making it the first large-scale resource for these tasks. To solve these tasks, we propose a multi-stage optimization approach and train Vision-Language Models (VLMs). Our results demonstrate that this approach significantly improves performance over the base model for both \textbf{label detection} and explanation generation, outperforming the current state-of-the-art with an absolute improvement of ~3% on ArMeme and ~7% on Hateful Memes. For reproducibility and future research, we aim to make the MemeIntel dataset and experimental resources publicly available. </p>
<blockquote>
<p>ç¤¾äº¤åª’ä½“ä¸Šå¤šæ¨¡æ€å†…å®¹çš„æ¿€å¢ï¼Œä¸ºç†è§£å’Œè°ƒèŠ‚å¤æ‚ã€ä¾èµ–äºæƒ…å¢ƒçš„è®®é¢˜ï¼ˆå¦‚è™šå‡ä¿¡æ¯ã€ä»‡æ¨è¨€è®ºå’Œå®£ä¼ ç­‰ï¼‰å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚è™½ç„¶å·²æœ‰äººåŠªåŠ›å¼€å‘èµ„æºå’Œæå‡ºè‡ªåŠ¨æ£€æµ‹çš„æ–°æ–¹æ³•ï¼Œä½†å¯¹æ ‡ç­¾æ£€æµ‹ä»¥åŠåŸºäºé¢„æµ‹çš„æ ‡ç­¾ç”Ÿæˆè§£é‡Šç†ç”±çš„å…³æ³¨æœ‰é™ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†MemeIntelï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹é˜¿æ‹‰ä¼¯è¯­çš„å®£ä¼ æ ‡è¯­å’Œè‹±æ–‡çš„ä»‡æ¨è¨€è®ºæ ‡ç­¾å¢å¼ºçš„æ•°æ®é›†ï¼Œæˆä¸ºè¿™äº›ä»»åŠ¡çš„é¦–ä¸ªå¤§è§„æ¨¡èµ„æºã€‚ä¸ºäº†è§£å†³è¿™äº›ä»»åŠ¡ï¼Œæˆ‘ä»¬æå‡ºäº†å¤šé˜¶æ®µä¼˜åŒ–æ–¹æ³•å¹¶è®­ç»ƒäº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•åœ¨æ ‡ç­¾æ£€æµ‹å’Œè§£é‡Šç”Ÿæˆæ–¹é¢éƒ½æ˜¾è‘—æé«˜äº†åŸºç¡€æ¨¡å‹çš„æ€§èƒ½ï¼Œç›¸è¾ƒäºå½“å‰æœ€å…ˆè¿›çš„æŠ€æœ¯ï¼Œåœ¨ArMemeä¸Šç»å¯¹æé«˜äº†çº¦3%ï¼Œåœ¨ä»‡æ¨è¨€è®ºæ ‡ç­¾ä¸Šæé«˜äº†çº¦7%ã€‚ä¸ºäº†å¯å¤åˆ¶æ€§å’Œæœªæ¥ç ”ç©¶ï¼Œæˆ‘ä»¬è®¡åˆ’å°†MemeIntelæ•°æ®é›†å’Œå®éªŒèµ„æºå…¬å¼€æä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.16612v1">PDF</a> disinformation, misinformation, factuality, harmfulness, fake news,   propaganda, hateful meme, multimodality, text, images</p>
<p><strong>Summary</strong></p>
<p>ç¤¾äº¤åª’ä½“ä¸Šå¤šæ¨¡æ€å†…å®¹çš„æ™®åŠï¼Œç»™ç†è§£å’Œè°ƒæ§è¯¸å¦‚è™šå‡ä¿¡æ¯ã€ä»‡æ¨è¨€è®ºå’Œå®£ä¼ ç­‰å¤æ‚ã€ä¾èµ–æƒ…å¢ƒçš„è®®é¢˜å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚å°½ç®¡å·²æœ‰èµ„æºå’Œæ–°æ–¹æ³•çš„å¼€å‘ç”¨äºè‡ªåŠ¨æ£€æµ‹ï¼Œä½†åœ¨æ ‡ç­¾æ£€æµ‹å’Œé¢„æµ‹æ ‡ç­¾çš„è§£é‡Šæ€§ä¾æ®ç”Ÿæˆæ–¹é¢å…³æ³¨æœ‰é™ã€‚ä¸ºåº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†MemeIntelæ•°æ®é›†ï¼ŒåŒ…å«é˜¿æ‹‰ä¼¯è¯­çš„å®£ä¼ æ ‡è¯­å’Œè‹±æ–‡çš„ä»‡æ¨æ ‡è¯­è§£é‡Šå¢å¼ºåŠŸèƒ½ï¼Œæˆä¸ºé¦–ä¸ªå¤§è§„æ¨¡èµ„æºç”¨äºè¿™äº›ä»»åŠ¡ã€‚æˆ‘ä»¬é‡‡ç”¨å¤šé˜¶æ®µä¼˜åŒ–æ–¹æ³•å’Œè®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰æ¥è§£å†³è¿™äº›ä»»åŠ¡ã€‚ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ ‡ç­¾æ£€æµ‹å’Œè§£é‡Šç”Ÿæˆæ–¹é¢è¾ƒåŸºç¡€æ¨¡å‹æœ‰æ˜¾è‘—æå‡ï¼Œç›¸è¾ƒäºå½“å‰æœ€å‰æ²¿æŠ€æœ¯ï¼Œåœ¨ArMemeä¸Šç»å¯¹æå‡çº¦3%ï¼Œåœ¨ä»‡æ¨æ ‡è¯­ä¸Šæå‡çº¦7%ã€‚æˆ‘ä»¬æ—¨åœ¨å°†MemeIntelæ•°æ®é›†å’Œå®éªŒèµ„æºå…¬å¼€ï¼Œä»¥ä¾›å¤åˆ¶å’Œæœªæ¥ç ”ç©¶ä½¿ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¤¾äº¤åª’ä½“å¤šæ¨¡æ€å†…å®¹å¸¦æ¥ç†è§£å’Œè°ƒæ§å¤æ‚ã€ä¾èµ–æƒ…å¢ƒçš„è®®é¢˜çš„æŒ‘æˆ˜ã€‚</li>
<li>å½“å‰è‡ªåŠ¨æ£€æµ‹æ–¹é¢å­˜åœ¨æ ‡ç­¾æ£€æµ‹å’Œè§£é‡Šæ€§ä¾æ®ç”Ÿæˆçš„å…³æ³¨ä¸è¶³ã€‚</li>
<li>å¼•å…¥MemeIntelæ•°æ®é›†ï¼ŒåŒ…å«é˜¿æ‹‰ä¼¯è¯­å’Œè‹±è¯­çš„å®£ä¼ æ ‡è¯­å’Œä»‡æ¨æ ‡è¯­è§£é‡Šå¢å¼ºåŠŸèƒ½ã€‚</li>
<li>é‡‡ç”¨å¤šé˜¶æ®µä¼˜åŒ–æ–¹æ³•å’Œè®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è§£å†³ç›¸å…³ä»»åŠ¡ã€‚</li>
<li>æ–¹æ³•åœ¨æ ‡ç­¾æ£€æµ‹å’Œè§£é‡Šç”Ÿæˆæ–¹é¢è¾ƒåŸºç¡€æ¨¡å‹æœ‰æ˜¾è‘—æå‡ã€‚</li>
<li>ä¸å½“å‰æœ€å‰æ²¿æŠ€æœ¯ç›¸æ¯”ï¼Œåœ¨ArMemeå’Œä»‡æ¨æ ‡è¯­æ–¹é¢å–å¾—æ˜¾è‘—è¿›æ­¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.16612">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3aaf80d9fe0b7b9e1e1eef13747c19bf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-24b89536fb3dd77fa966c87dda7982ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-90b796bd395a410564ec373d19f53326.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f7d7c964fff0a512abc3b4c226179603.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7e1a0640d8bd71d13052324ac60c1109.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Speech-Enhancement-Using-Continuous-Embeddings-of-Neural-Audio-Codec"><a href="#Speech-Enhancement-Using-Continuous-Embeddings-of-Neural-Audio-Codec" class="headerlink" title="Speech Enhancement Using Continuous Embeddings of Neural Audio Codec"></a>Speech Enhancement Using Continuous Embeddings of Neural Audio Codec</h2><p><strong>Authors:Haoyang Li, Jia Qi Yip, Tianyu Fan, Eng Siong Chng</strong></p>
<p>Recent advancements in Neural Audio Codec (NAC) models have inspired their use in various speech processing tasks, including speech enhancement (SE). In this work, we propose a novel, efficient SE approach by leveraging the pre-quantization output of a pretrained NAC encoder. Unlike prior NAC-based SE methods, which process discrete speech tokens using Language Models (LMs), we perform SE within the continuous embedding space of the pretrained NAC, which is highly compressed along the time dimension for efficient representation. Our lightweight SE model, optimized through an embedding-level loss, delivers results comparable to SE baselines trained on larger datasets, with a significantly lower real-time factor of 0.005. Additionally, our method achieves a low GMAC of 3.94, reducing complexity 18-fold compared to Sepformer in a simulated cloud-based audio transmission environment. This work highlights a new, efficient NAC-based SE solution, particularly suitable for cloud applications where NAC is used to compress audio before transmission.   Copyright 20XX IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting&#x2F;republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. </p>
<blockquote>
<p>è¿‘æœŸç¥ç»ç½‘ç»œéŸ³é¢‘ç¼–è§£ç å™¨ï¼ˆNACï¼‰æ¨¡å‹çš„è¿›å±•æ¿€å‘äº†å…¶åœ¨å„ç§è¯­éŸ³å¤„ç†ä»»åŠ¡ä¸­çš„åº”ç”¨ï¼ŒåŒ…æ‹¬è¯­éŸ³å¢å¼ºï¼ˆSEï¼‰ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨é¢„è®­ç»ƒNACç¼–ç å™¨çš„é¢„é‡åŒ–è¾“å‡ºè¿›è¡Œé«˜æ•ˆSEçš„æ–°æ–¹æ³•ã€‚ä¸ä»¥å‰åŸºäºNACçš„SEæ–¹æ³•ä¸åŒï¼Œåè€…ä½¿ç”¨è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰å¤„ç†ç¦»æ•£çš„è¯­éŸ³æ ‡è®°ï¼Œæˆ‘ä»¬åœ¨é¢„è®­ç»ƒçš„NACçš„è¿ç»­åµŒå…¥ç©ºé—´å†…æ‰§è¡ŒSEï¼Œè¯¥åµŒå…¥ç©ºé—´åœ¨æ—¶é—´ç»´åº¦ä¸Šè¿›è¡Œäº†é«˜åº¦å‹ç¼©ä»¥è¿›è¡Œæœ‰æ•ˆè¡¨ç¤ºã€‚æˆ‘ä»¬çš„è½»é‡çº§SEæ¨¡å‹é€šè¿‡åµŒå…¥å±‚æŸå¤±è¿›è¡Œä¼˜åŒ–ï¼Œå…¶ç»“æœè¡¨æ˜å³ä½¿ä¸åœ¨æ›´å¤§æ•°æ®é›†ä¸Šè®­ç»ƒçš„SEåŸºçº¿ç›¸æ¯”ï¼Œä¹Ÿèƒ½å–å¾—ç›¸å½“çš„ç»“æœï¼Œå®æ—¶å› å­ä¸º0.005ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ¨¡æ‹Ÿçš„äº‘éŸ³é¢‘ä¼ è¾“ç¯å¢ƒä¸­å®ç°äº†è¾ƒä½çš„GMACå€¼3.94ï¼Œä¸Sepformerç›¸æ¯”å¤æ‚åº¦é™ä½äº†18å€ã€‚è¿™é¡¹å·¥ä½œæ˜¯é¦–æ¬¡å±•ç¤ºäº†é€‚ç”¨äºä½¿ç”¨NACè¿›è¡ŒéŸ³é¢‘å‹ç¼©å’Œä¼ è¾“çš„äº‘åº”ç”¨ç¨‹åºçš„é«˜æ•ˆè§£å†³æ–¹æ¡ˆã€‚ç‰ˆæƒå£°æ˜ï¼šæœ¬è®ºæ–‡ç‰ˆæƒå½’IEEEæ‰€æœ‰ã€‚å…è®¸ä¸ªäººä½¿ç”¨æœ¬ææ–™ã€‚å¦‚éœ€ç”¨äºå…¶ä»–ç”¨é€”ï¼ˆæ— è®ºå½“å‰è¿˜æ˜¯æœªæ¥åª’ä½“ï¼‰ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºä¸ºå¹¿å‘Šæˆ–å®£ä¼ ç›®çš„é‡å°æˆ–é‡æ–°å‘å¸ƒæœ¬ææ–™ã€åˆ›å»ºæ–°çš„é›†ä½“ä½œå“ã€è½¬å”®æˆ–é‡æ–°åˆ†å‘åˆ°æœåŠ¡å™¨æˆ–åˆ—è¡¨ã€æˆ–åœ¨å…¶ä»–ä½œå“ä¸­ä½¿ç”¨æœ¬å—ç‰ˆæƒä¿æŠ¤ä½œå“çš„ä»»ä½•éƒ¨åˆ†ï¼Œéƒ½å¿…é¡»è·å¾—IEEEçš„è®¸å¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.16240v1">PDF</a> Accepted to ICASSP 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>è¿‘æœŸç¥ç»ç½‘ç»œéŸ³é¢‘ç¼–ç ï¼ˆNACï¼‰æ¨¡å‹çš„è¿›å±•ä¸ºå…¶åœ¨è¯­éŸ³å¤„ç†ä»»åŠ¡ä¸­çš„åº”ç”¨æä¾›äº†å¯èƒ½ï¼ŒåŒ…æ‹¬è¯­éŸ³å¢å¼ºï¼ˆSEï¼‰ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„é«˜æ•ˆSEæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒNACçš„é¢„é‡åŒ–è¾“å‡ºã€‚ä¸å…ˆå‰åŸºäºNACçš„SEæ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬ä¸åœ¨ç¦»æ•£è¯­éŸ³æ ‡è®°ä¸Šä½¿ç”¨è¯­è¨€æ¨¡å‹ï¼ˆLMsï¼‰è¿›è¡Œå¤„ç†ï¼Œè€Œæ˜¯åœ¨é¢„è®­ç»ƒNACçš„è¿ç»­åµŒå…¥ç©ºé—´å†…æ‰§è¡ŒSEï¼Œè¯¥ç©ºé—´åœ¨æ—¶é—´ç»´åº¦ä¸Šé«˜åº¦å‹ç¼©ä»¥å®ç°é«˜æ•ˆè¡¨ç¤ºã€‚æˆ‘ä»¬çš„è½»é‡åŒ–SEæ¨¡å‹é€šè¿‡åµŒå…¥çº§æŸå¤±è¿›è¡Œä¼˜åŒ–ï¼Œå¯æä¾›ä¸åœ¨æ›´å¤§æ•°æ®é›†ä¸Šè®­ç»ƒçš„SEåŸºçº¿ç›¸å½“çš„ç»“æœï¼Œå¹¶ä¸”å®æ—¶å› å­ä½å¾—å¤šï¼Œä¸º0.005ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ¨¡æ‹Ÿçš„äº‘åŸºäºéŸ³é¢‘ä¼ è¾“ç¯å¢ƒä¸­å®ç°äº†3.94çš„GMACï¼Œä¸Sepformerç›¸æ¯”å‡å°‘äº†18å€å¤æ‚æ€§ã€‚æœ¬ç ”ç©¶å¼ºè°ƒäº†æ–°çš„é«˜æ•ˆNAC-åŸºäºSEè§£å†³æ–¹æ¡ˆï¼Œå°¤å…¶é€‚åˆåœ¨NACç”¨äºéŸ³é¢‘ä¼ è¾“å‰å‹ç¼©çš„äº‘åº”ç”¨ç¨‹åºä¸­ä½¿ç”¨ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>NACæ¨¡å‹æœ€æ–°è¿›å±•ä¸ºè¯­éŸ³å¢å¼ºç­‰è¯­éŸ³å¤„ç†ä»»åŠ¡æä¾›äº†æ–°çš„åº”ç”¨å¯èƒ½æ€§ã€‚</li>
<li>æå‡ºä¸€ç§é«˜æ•ˆSEæ–¹æ³•ï¼Œåˆ©ç”¨é¢„è®­ç»ƒNACçš„é¢„é‡åŒ–è¾“å‡ºè¿›è¡Œè¯­éŸ³å¢å¼ºã€‚</li>
<li>ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼Œæœ¬æ–¹æ³•ä¸åœ¨ç¦»æ•£è¯­éŸ³æ ‡è®°ä¸Šä½¿ç”¨è¯­è¨€æ¨¡å‹å¤„ç†ï¼Œè€Œæ˜¯åœ¨è¿ç»­åµŒå…¥ç©ºé—´å†…æ‰§è¡ŒSEã€‚</li>
<li>æ¨¡å‹ä¼˜åŒ–é€šè¿‡åµŒå…¥çº§æŸå¤±è¿›è¡Œï¼Œå®æ—¶å› å­ä½ï¼ˆ0.005ï¼‰ï¼Œæ€§èƒ½ä¸å¤§å‹æ•°æ®é›†è®­ç»ƒçš„SEåŸºçº¿ç›¸å½“ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨æ¨¡æ‹Ÿçš„äº‘ç¯å¢ƒä¸­å®ç°ä½GMACï¼ˆ3.94ï¼‰ï¼Œç›¸è¾ƒäºSepformerå‡å°‘äº†å¤æ‚æ€§ã€‚</li>
<li>æœ¬ç ”ç©¶é€‚ç”¨äºäº‘åº”ç”¨ç¨‹åºä¸­çš„éŸ³é¢‘å‹ç¼©å’Œä¼ è¾“åœºæ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.16240">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-9ac0f07d6f35eb26cff4c11368381ae7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-707c2405a61498ed0371a12450458091.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c95baecf7f190cc481490b58b6430c1d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-78752534f590bb45f7114c56292efae2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4639273e345c3805d1c7d0b0f0fa8343.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-034588c798c07612262656838e28004b.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Improving-Speech-Enhancement-by-Cross-and-Sub-band-Processing-with-State-Space-Model"><a href="#Improving-Speech-Enhancement-by-Cross-and-Sub-band-Processing-with-State-Space-Model" class="headerlink" title="Improving Speech Enhancement by Cross- and Sub-band Processing with   State Space Model"></a>Improving Speech Enhancement by Cross- and Sub-band Processing with   State Space Model</h2><p><strong>Authors:Jizhen Li, Weiping Tu, Yuhong Yang, Xinmeng Xu, Yiqun Zhang, Yanzhen Ren</strong></p>
<p>Recently, the state space model (SSM) represented by Mamba has shown remarkable performance in long-term sequence modeling tasks, including speech enhancement. However, due to substantial differences in sub-band features, applying the same SSM to all sub-bands limits its inference capability. Additionally, when processing each time frame of the time-frequency representation, the SSM may forget certain high-frequency information of low energy, making the restoration of structure in the high-frequency bands challenging. For this reason, we propose Cross- and Sub-band Mamba (CSMamba). To assist the SSM in handling different sub-band features flexibly, we propose a band split block that splits the full-band into four sub-bands with different widths based on their information similarity. We then allocate independent weights to each sub-band, thereby reducing the inference burden on the SSM. Furthermore, to mitigate the forgetting of low-energy information in the high-frequency bands by the SSM, we introduce a spectrum restoration block that enhances the representation of the cross-band features from multiple perspectives. Experimental results on the DNS Challenge 2021 dataset demonstrate that CSMamba outperforms several state-of-the-art (SOTA) speech enhancement methods in three objective evaluation metrics with fewer parameters. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œä»¥Mambaä¸ºä»£è¡¨çš„çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰åœ¨é•¿æœŸåºåˆ—å»ºæ¨¡ä»»åŠ¡ä¸­ï¼ŒåŒ…æ‹¬è¯­éŸ³å¢å¼ºï¼Œéƒ½è¡¨ç°å‡ºäº†æ˜¾è‘—çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç”±äºå­å¸¦ç‰¹å¾ä¹‹é—´å­˜åœ¨å·¨å¤§å·®å¼‚ï¼Œå°†ç›¸åŒçš„SSMåº”ç”¨äºæ‰€æœ‰å­å¸¦ä¼šé™åˆ¶å…¶æ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œåœ¨å¤„ç†æ—¶é¢‘è¡¨ç¤ºçš„æ¯ä¸ªæ—¶é—´å¸§æ—¶ï¼ŒSSMå¯èƒ½ä¼šå¿˜è®°æŸäº›ä½èƒ½é‡çš„é«˜é¢‘ä¿¡æ¯ï¼Œä½¿å¾—æ¢å¤é«˜é¢‘å¸¦ä¸­çš„ç»“æ„å…·æœ‰æŒ‘æˆ˜æ€§ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†è·¨å¸¦å’Œå­å¸¦Mambaï¼ˆCSMambaï¼‰ã€‚ä¸ºäº†å¸®åŠ©SSMçµæ´»å¤„ç†ä¸åŒçš„å­å¸¦ç‰¹å¾ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå¸¦åˆ†å‰²å—ï¼Œè¯¥å—æ ¹æ®ä¿¡æ¯ç›¸ä¼¼æ€§å°†å…¨é¢‘å¸¦åˆ†å‰²æˆå››ä¸ªä¸åŒå®½åº¦çš„å­å¸¦ã€‚ç„¶åæˆ‘ä»¬å¯¹æ¯ä¸ªå­å¸¦åˆ†é…ç‹¬ç«‹æƒé‡ï¼Œä»è€Œå‡è½»SSMçš„æ¨ç†è´Ÿæ‹…ã€‚æ­¤å¤–ï¼Œä¸ºäº†å‡è½»SSMåœ¨é«˜é¢‘å¸¦ä¸­å¿˜è®°ä½èƒ½é‡ä¿¡æ¯çš„é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªé¢‘è°±æ¢å¤å—ï¼Œè¯¥å—ä»å¤šä¸ªè§’åº¦å¢å¼ºäº†è·¨å¸¦ç‰¹å¾çš„è¡¨ç°ã€‚åœ¨DNS Challenge 2021æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒCSMambaåœ¨ä¸‰ä¸ªå®¢è§‚è¯„ä»·æŒ‡æ ‡ä¸Šä¼˜äºå‡ ç§æœ€å…ˆè¿›çš„è¯­éŸ³å¢å¼ºæ–¹æ³•ï¼Œä¸”å‚æ•°æ›´å°‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.16207v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>Mambaä»£è¡¨çš„å·ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰åœ¨é•¿æœŸåºåˆ—å»ºæ¨¡ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒåŒ…æ‹¬è¯­éŸ³å¢å¼ºã€‚ä½†åº”ç”¨åŒä¸€SSMå¤„ç†æ‰€æœ‰å­å¸¦å­˜åœ¨å±€é™æ€§ã€‚ä¸ºæ­¤ï¼Œæå‡ºè·¨å¸¦å’Œå­å¸¦Mambaï¼ˆCSMambaï¼‰ï¼Œé€šè¿‡å¸¦åˆ†å‰²å—å°†å…¨é¢‘å¸¦åˆ†ä¸ºä¸åŒå®½åº¦çš„å­å¸¦ï¼Œå¹¶ä¸ºæ¯ä¸ªå­å¸¦åˆ†é…ç‹¬ç«‹æƒé‡ï¼Œå‡è½»SSMçš„æ¨ç†è´Ÿæ‹…ã€‚åŒæ—¶ï¼Œé€šè¿‡é¢‘è°±æ¢å¤å—å¢å¼ºè·¨å¸¦ç‰¹å¾è¡¨ç¤ºï¼Œå‡å°‘SSMåœ¨é«˜é¢‘å¸¦ä¸­é—å¿˜ä½èƒ½é‡ä¿¡æ¯çš„é—®é¢˜ã€‚åœ¨DNS Challenge 2021æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒCSMambaåœ¨ä¸‰ä¸ªå®¢è§‚è¯„ä»·æŒ‡æ ‡ä¸Šä¼˜äºå¤šç§æœ€æ–°è¯­éŸ³å¢å¼ºæ–¹æ³•ï¼Œä¸”å‚æ•°æ›´å°‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Mambaä»£è¡¨çš„å·ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰åœ¨è¯­éŸ³å¢å¼ºç­‰é•¿æœŸåºåˆ—å»ºæ¨¡ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚</li>
<li>åº”ç”¨åŒä¸€SSMå¤„ç†æ‰€æœ‰å­å¸¦å­˜åœ¨å±€é™æ€§ï¼Œå› ä¸ºå­å¸¦ç‰¹å¾å·®å¼‚æ˜¾è‘—ã€‚</li>
<li>æå‡ºCSMambaæ¨¡å‹ï¼Œé€šè¿‡å¸¦åˆ†å‰²å—å°†å…¨é¢‘å¸¦åˆ†ä¸ºä¸åŒå®½åº¦çš„å­å¸¦ã€‚</li>
<li>ä¸ºæ¯ä¸ªå­å¸¦åˆ†é…ç‹¬ç«‹æƒé‡ï¼Œä»¥å‡è½»SSMçš„æ¨ç†è´Ÿæ‹…ã€‚</li>
<li>é¢‘è°±æ¢å¤å—ç”¨äºå¢å¼ºè·¨å¸¦ç‰¹å¾è¡¨ç¤ºï¼Œå‡å°‘SSMåœ¨é«˜é¢‘å¸¦ä¸­é—å¿˜ä½èƒ½é‡ä¿¡æ¯çš„é—®é¢˜ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒCSMambaåœ¨å®¢è§‚è¯„ä»·æŒ‡æ ‡ä¸Šä¼˜äºå¤šç§æœ€æ–°è¯­éŸ³å¢å¼ºæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.16207">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d68abef25be5cb001f648baef8aae410.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f0d474369c3358d2158373c681816a71.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-38071304e3dbc2712b05ce565c8809a0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3eb4e86504d5cac66b1f4b82042b172e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-51f84ed0f125288ab87d1ea90a871cd6.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Understanding-Zero-shot-Rare-Word-Recognition-Improvements-Through-LLM-Integration"><a href="#Understanding-Zero-shot-Rare-Word-Recognition-Improvements-Through-LLM-Integration" class="headerlink" title="Understanding Zero-shot Rare Word Recognition Improvements Through LLM   Integration"></a>Understanding Zero-shot Rare Word Recognition Improvements Through LLM   Integration</h2><p><strong>Authors:Haoxuan Wang</strong></p>
<p>In this study, we investigate the integration of a large language model (LLM) with an automatic speech recognition (ASR) system, specifically focusing on enhancing rare word recognition performance. Using a 190,000-hour dataset primarily sourced from YouTube, pre-processed with Whisper V3 pseudo-labeling, we demonstrate that the LLM-ASR architecture outperforms traditional Zipformer-Transducer models in the zero-shot rare word recognition task, after training on a large dataset. Our analysis reveals that the LLM contributes significantly to improvements in rare word error rate (R-WER), while the speech encoder primarily determines overall transcription performance (Orthographic Word Error Rate, O-WER, and Normalized Word Error Rate, N-WER). Through extensive ablation studies, we highlight the importance of adapter integration in aligning speech encoder outputs with the LLMâ€™s linguistic capabilities. Furthermore, we emphasize the critical role of high-quality labeled data in achieving optimal performance. These findings provide valuable insights into the synergy between LLM-based ASR architectures, paving the way for future advancements in large-scale LLM-based speech recognition systems. </p>
<blockquote>
<p>åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æ¢ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿçš„èåˆï¼Œç‰¹åˆ«å…³æ³¨æé«˜ç½•è§è¯è¯†åˆ«æ€§èƒ½ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸»è¦æ¥æºäºYouTubeçš„19ä¸‡å°æ—¶æ•°æ®é›†ï¼Œç»è¿‡Whisper V3ä¼ªæ ‡ç­¾è¿›è¡Œé¢„å¤„ç†ï¼Œè¯æ˜åœ¨å¤§å‹æ•°æ®é›†è®­ç»ƒåï¼ŒLLM-ASRæ¶æ„åœ¨é›¶æ ·æœ¬ç½•è§è¯è¯†åˆ«ä»»åŠ¡ä¸­ä¼˜äºä¼ ç»Ÿçš„Zipformer-Transduceræ¨¡å‹ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼ŒLLMå¯¹é™ä½ç½•è§è¯é”™è¯¯ç‡ï¼ˆR-WERï¼‰åšå‡ºäº†æ˜¾è‘—è´¡çŒ®ï¼Œè€Œè¯­éŸ³ç¼–ç å™¨ä¸»è¦å†³å®šæ•´ä½“è½¬å½•æ€§èƒ½ï¼ˆæ­£å­—æ³•è¯é”™è¯¯ç‡ã€O-WERå’Œå½’ä¸€åŒ–è¯é”™è¯¯ç‡ã€N-WERï¼‰ã€‚é€šè¿‡å¹¿æ³›çš„æ¶ˆèç ”ç©¶ï¼Œæˆ‘ä»¬å¼ºè°ƒäº†é€‚é…å™¨é›†æˆåœ¨å°†è¯­éŸ³ç¼–ç å™¨è¾“å‡ºä¸LLMçš„è¯­è¨€èƒ½åŠ›å¯¹é½ä¸­çš„é‡è¦æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼ºè°ƒäº†é«˜è´¨é‡æ ‡è®°æ•°æ®åœ¨å®ç°æœ€ä½³æ€§èƒ½ä¸­çš„å…³é”®ä½œç”¨ã€‚è¿™äº›å‘ç°æ­ç¤ºäº†LLMä¸ºåŸºç¡€çš„ASRæ¶æ„ä¹‹é—´çš„ååŒä»·å€¼ï¼Œä¸ºæœªæ¥å¤§è§„æ¨¡LLMä¸ºåŸºç¡€çš„è¯­éŸ³è¯†åˆ«ç³»ç»Ÿçš„è¿›æ­¥é“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.16142v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿçš„èåˆï¼Œç‰¹åˆ«æ˜¯åœ¨æé«˜ç½•è§è¯è¯†åˆ«æ€§èƒ½æ–¹é¢çš„åº”ç”¨ã€‚é€šè¿‡ä½¿ç”¨ä¸»è¦æ¥è‡ªYouTubeçš„190,000å°æ—¶æ•°æ®é›†ï¼Œç»“åˆWhisper V3ä¼ªæ ‡ç­¾è¿›è¡Œé¢„å¤„ç†ï¼Œç ”ç©¶è¯æ˜LLM-ASRæ¶æ„åœ¨é›¶æ ·æœ¬ç½•è§è¯è¯†åˆ«ä»»åŠ¡ä¸­ä¼˜äºä¼ ç»Ÿçš„Zipformer-Transduceræ¨¡å‹ã€‚åˆ†ææ˜¾ç¤ºLLMå¯¹é™ä½ç½•è§è¯é”™è¯¯ç‡ï¼ˆR-WERï¼‰æœ‰æ˜¾è‘—è´¡çŒ®ï¼Œè€Œè¯­éŸ³ç¼–ç å™¨ä¸»è¦å†³å®šæ•´ä½“è½¬å½•æ€§èƒ½ï¼ˆåŒ…æ‹¬æ­£å­—æ³•è¯é”™è¯¯ç‡ï¼ˆO-WERï¼‰å’Œå½’ä¸€åŒ–è¯é”™è¯¯ç‡ï¼ˆN-WERï¼‰ã€‚é€šè¿‡å¹¿æ³›çš„æ¶ˆèç ”ç©¶ï¼Œå¼ºè°ƒäº†é€‚é…å™¨é›†æˆåœ¨å°†è¯­éŸ³ç¼–ç å™¨è¾“å‡ºä¸LLMçš„è¯­è¨€èƒ½åŠ›å¯¹é½ä¸­çš„é‡è¦æ€§ã€‚åŒæ—¶ï¼Œç ”ç©¶å¼ºè°ƒäº†é«˜è´¨é‡æ ‡è®°æ•°æ®åœ¨å®ç°æœ€ä½³æ€§èƒ½æ–¹é¢çš„å…³é”®ä½œç”¨ã€‚è¿™äº›å‘ç°å¯¹äºç†è§£LLMåœ¨ASRæ¶æ„ä¸­çš„ååŒä½œç”¨å…·æœ‰é‡è¦æ„ä¹‰ï¼Œä¸ºå¤§è§„æ¨¡LLMè¯­éŸ³è¯†åˆ«ç³»ç»Ÿçš„æœªæ¥å‘å±•é“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMä¸ASRç³»ç»Ÿçš„ç»“åˆèƒ½æ˜¾è‘—æé«˜ç½•è§è¯çš„è¯†åˆ«æ€§èƒ½ã€‚</li>
<li>ä½¿ç”¨YouTuBeæ•°æ®çš„190,000å°æ—¶å¤§å‹æ•°æ®é›†è¿›è¡Œè®­ç»ƒæœ‰åŠ©äºLLM-ASRæ¶æ„çš„è¡¨ç°ã€‚</li>
<li>LLMå¯¹é™ä½ç½•è§è¯é”™è¯¯ç‡æœ‰é‡è¦è´¡çŒ®ã€‚</li>
<li>è¯­éŸ³ç¼–ç å™¨åœ¨å†³å®šæ•´ä½“è½¬å½•æ€§èƒ½ä¸­èµ·å…³é”®ä½œç”¨ã€‚</li>
<li>é€‚é…å™¨é›†æˆåœ¨LLMå’Œè¯­éŸ³ç¼–ç å™¨ä¹‹é—´å‘æŒ¥é‡è¦ä½œç”¨ã€‚</li>
<li>é«˜è´¨é‡çš„æ ‡è®°æ•°æ®å¯¹äºå®ç°æœ€ä½³æ€§èƒ½è‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.16142">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-394dd0b00ad4fd1b9e25f8bba28d905e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c9f9332e2cc4af9e04fd75a0c19d4e9.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Generative-AI-Framework-for-3D-Object-Generation-in-Augmented-Reality"><a href="#Generative-AI-Framework-for-3D-Object-Generation-in-Augmented-Reality" class="headerlink" title="Generative AI Framework for 3D Object Generation in Augmented Reality"></a>Generative AI Framework for 3D Object Generation in Augmented Reality</h2><p><strong>Authors:Majid Behravan</strong></p>
<p>This thesis presents a framework that integrates state-of-the-art generative AI models for real-time creation of three-dimensional (3D) objects in augmented reality (AR) environments. The primary goal is to convert diverse inputs, such as images and speech, into accurate 3D models, enhancing user interaction and immersion. Key components include advanced object detection algorithms, user-friendly interaction techniques, and robust AI models like Shap-E for 3D generation. Leveraging Vision Language Models (VLMs) and Large Language Models (LLMs), the system captures spatial details from images and processes textual information to generate comprehensive 3D objects, seamlessly integrating virtual objects into real-world environments. The framework demonstrates applications across industries such as gaming, education, retail, and interior design. It allows players to create personalized in-game assets, customers to see products in their environments before purchase, and designers to convert real-world objects into 3D models for real-time visualization. A significant contribution is democratizing 3D model creation, making advanced AI tools accessible to a broader audience, fostering creativity and innovation. The framework addresses challenges like handling multilingual inputs, diverse visual data, and complex environments, improving object detection and model generation accuracy, as well as loading 3D models in AR space in real-time. In conclusion, this thesis integrates generative AI and AR for efficient 3D model generation, enhancing accessibility and paving the way for innovative applications and improved user interactions in AR environments. </p>
<blockquote>
<p>æœ¬è®ºæ–‡æå‡ºäº†ä¸€ä¸ªé›†æˆæœ€æ–°ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ¨¡å‹çš„æ¡†æ¶ï¼Œç”¨äºåœ¨å¢å¼ºç°å®ï¼ˆARï¼‰ç¯å¢ƒä¸­å®æ—¶åˆ›å»ºä¸‰ç»´ï¼ˆ3Dï¼‰å¯¹è±¡ã€‚ä¸»è¦ç›®æ ‡æ˜¯å°†å›¾åƒå’Œè¯­éŸ³ç­‰å¤šæ ·åŒ–çš„è¾“å…¥è½¬åŒ–ä¸ºå‡†ç¡®çš„ä¸‰ç»´æ¨¡å‹ï¼Œå¢å¼ºç”¨æˆ·äº¤äº’å’Œæ²‰æµ¸æ„Ÿã€‚å…³é”®ç»„ä»¶åŒ…æ‹¬å…ˆè¿›çš„ç‰©ä½“æ£€æµ‹ç®—æ³•ã€ç”¨æˆ·å‹å¥½çš„äº¤äº’æŠ€æœ¯ä»¥åŠç”¨äºä¸‰ç»´ç”Ÿæˆçš„ä¸‰ç»´æ¨¡å‹ï¼ˆå¦‚Shap-Eï¼‰ã€‚é€šè¿‡åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿæ•æ‰å›¾åƒä¸­çš„ç©ºé—´ç»†èŠ‚ï¼Œå¤„ç†æ–‡æœ¬ä¿¡æ¯ä»¥ç”Ÿæˆå…¨é¢çš„ä¸‰ç»´ç‰©ä½“ï¼Œæ— ç¼åœ°å°†è™šæ‹Ÿç‰©ä½“é›†æˆåˆ°çœŸå®ä¸–ç•Œç¯å¢ƒä¸­ã€‚è¯¥æ¡†æ¶å±•ç¤ºäº†åœ¨æ¸¸æˆã€æ•™è‚²ã€é›¶å”®å’Œå®¤å†…è®¾è®¡ç­‰è¡Œä¸šçš„åº”ç”¨ã€‚å®ƒå…è®¸ç©å®¶åˆ›å»ºä¸ªæ€§åŒ–çš„æ¸¸æˆèµ„äº§ï¼Œè®©å®¢æˆ·åœ¨è´­ä¹°å‰çœ‹åˆ°äº§å“åœ¨ç¯å¢ƒä¸­çš„æ•ˆæœï¼Œè®¾è®¡å¸ˆå°†ç°å®ä¸–ç•Œç‰©ä½“è½¬åŒ–ä¸ºä¸‰ç»´æ¨¡å‹è¿›è¡Œå®æ—¶å¯è§†åŒ–ã€‚ä¸€ä¸ªé‡è¦çš„è´¡çŒ®æ˜¯ä½¿ä¸‰ç»´æ¨¡å‹åˆ›å»ºæ°‘ä¸»åŒ–ï¼Œä½¿æ›´å¹¿æ³›çš„å—ä¼—èƒ½å¤Ÿä½¿ç”¨å…ˆè¿›çš„AIå·¥å…·ï¼Œä¿ƒè¿›åˆ›é€ åŠ›å’Œåˆ›æ–°ã€‚è¯¥æ¡†æ¶è§£å†³äº†å¤„ç†å¤šè¯­è¨€è¾“å…¥ã€å¤šæ ·åŒ–çš„è§†è§‰æ•°æ®å’Œå¤æ‚ç¯å¢ƒç­‰æŒ‘æˆ˜ï¼Œæé«˜äº†ç‰©ä½“æ£€æµ‹å’Œæ¨¡å‹ç”Ÿæˆçš„å‡†ç¡®æ€§ï¼Œä»¥åŠå®æ—¶åœ¨ARç©ºé—´ä¸­åŠ è½½ä¸‰ç»´æ¨¡å‹çš„èƒ½åŠ›ã€‚æ€»ä¹‹ï¼Œæœ¬è®ºæ–‡å°†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½å’ŒARæŠ€æœ¯ç›¸ç»“åˆï¼Œå®ç°é«˜æ•ˆçš„ä¸‰ç»´æ¨¡å‹ç”Ÿæˆï¼Œæé«˜å¯è®¿é—®æ€§ï¼Œä¸ºARç¯å¢ƒä¸­çš„åˆ›æ–°åº”ç”¨å’Œå¢å¼ºç”¨æˆ·äº¤äº’é“ºå¹³é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.15869v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    è¯¥è®ºæ–‡æå‡ºä¸€ä¸ªèåˆæœ€å‰æ²¿ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ¨¡å‹çš„æ¡†æ¶ï¼Œç”¨äºåœ¨å¢å¼ºç°å®ç¯å¢ƒä¸­å®æ—¶åˆ›å»ºä¸‰ç»´ç‰©ä½“ã€‚ä¸»è¦ç›®æ ‡æ˜¯å°†å›¾åƒå’Œè¯­éŸ³ç­‰å¤šæ ·åŒ–è¾“å…¥è½¬åŒ–ä¸ºç²¾ç¡®çš„ä¸‰ç»´æ¨¡å‹ï¼Œå¢å¼ºç”¨æˆ·äº’åŠ¨ä¸æ²‰æµ¸æ„Ÿã€‚å…³é”®ç»„ä»¶åŒ…æ‹¬é«˜çº§ç‰©ä½“æ£€æµ‹ç®—æ³•ã€ç”¨æˆ·å‹å¥½äº’åŠ¨æŠ€æœ¯å’Œå¼ºå¤§çš„ä¸‰ç»´ç”Ÿæˆäººå·¥æ™ºèƒ½æ¨¡å‹ï¼Œå¦‚Shap-Eã€‚å€ŸåŠ©è§†è§‰è¯­è¨€æ¨¡å‹å’Œå¤§è¯­è¨€æ¨¡å‹ï¼Œç³»ç»Ÿæ•æ‰å›¾åƒçš„ç©ºé—´ç»†èŠ‚å¹¶å¤„ç†æ–‡æœ¬ä¿¡æ¯ï¼Œä»¥ç”Ÿæˆç»¼åˆä¸‰ç»´ç‰©ä½“ï¼Œæ— ç¼é›†æˆè™šæ‹Ÿç‰©ä½“åˆ°çœŸå®ç¯å¢ƒã€‚è¯¥æ¡†æ¶å±•ç¤ºäº†åœ¨æ¸¸æˆã€æ•™è‚²ã€é›¶å”®å’Œå®¤å†…è®¾è®¡ç­‰è¡Œä¸šçš„åº”ç”¨ã€‚å®ƒå…è®¸ç©å®¶åˆ›å»ºä¸ªæ€§åŒ–æ¸¸æˆèµ„äº§ï¼Œå®¢æˆ·åœ¨è´­ä¹°å‰çœ‹åˆ°äº§å“åœ¨ç¯å¢ƒä¸­çš„æ ·å­ï¼Œä»¥åŠè®¾è®¡å¸ˆå°†çœŸå®ä¸–ç•Œç‰©ä½“è½¬åŒ–ä¸ºä¸‰ç»´æ¨¡å‹è¿›è¡Œå®æ—¶å¯è§†åŒ–ã€‚ä¸€ä¸ªé‡å¤§è´¡çŒ®æ˜¯ä½¿ä¸‰ç»´æ¨¡å‹åˆ›å»ºæ°‘ä¸»åŒ–ï¼Œä½¿å…ˆè¿›çš„AIå·¥å…·å¯¹æ›´å¹¿æ³›çš„å—ä¼—ç¾¤ä½“å¯ç”¨ï¼Œä¿ƒè¿›åˆ›é€ åŠ›å’Œåˆ›æ–°ã€‚è¯¥æ¡†æ¶è§£å†³äº†å¤„ç†å¤šè¯­è¨€è¾“å…¥ã€å¤šæ ·åŒ–çš„è§†è§‰æ•°æ®å’Œå¤æ‚ç¯å¢ƒç­‰æŒ‘æˆ˜ï¼Œæé«˜äº†ç‰©ä½“æ£€æµ‹å’Œæ¨¡å‹ç”Ÿæˆçš„å‡†ç¡®æ€§ï¼Œä»¥åŠå®æ—¶åœ¨ARç©ºé—´ä¸­åŠ è½½ä¸‰ç»´æ¨¡å‹çš„èƒ½åŠ›ã€‚æ€»ä¹‹ï¼Œè¯¥è®ºæ–‡æˆåŠŸæ•´åˆç”Ÿæˆå¼äººå·¥æ™ºèƒ½å’Œå¢å¼ºç°å®æŠ€æœ¯ï¼Œå®ç°é«˜æ•ˆçš„ä¸‰ç»´æ¨¡å‹ç”Ÿæˆï¼Œæé«˜å¯åŠæ€§å¹¶ä¸ºåˆ›æ–°åº”ç”¨å’Œå¢å¼ºç°å®ç¯å¢ƒä¸­çš„ç”¨æˆ·äº’åŠ¨é“ºå¹³é“è·¯ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è®ºæ–‡æå‡ºäº†ä¸€ä¸ªæ•´åˆæœ€å‰æ²¿ç”Ÿæˆå¼AIæ¨¡å‹çš„æ¡†æ¶ï¼Œç”¨äºåœ¨ARç¯å¢ƒä¸­å®æ—¶åˆ›å»ºä¸‰ç»´ç‰©ä½“ã€‚</li>
<li>æ¡†æ¶çš„ä¸»è¦ç›®æ ‡æ˜¯å°†å¤šæ ·åŒ–çš„è¾“å…¥ï¼ˆå¦‚å›¾åƒå’Œè¯­éŸ³ï¼‰è½¬åŒ–ä¸ºç²¾ç¡®çš„ä¸‰ç»´æ¨¡å‹ã€‚</li>
<li>æ¡†æ¶åŒ…æ‹¬é«˜çº§ç‰©ä½“æ£€æµ‹ç®—æ³•ã€ç”¨æˆ·å‹å¥½äº’åŠ¨æŠ€æœ¯å’Œå¼ºå¤§çš„ä¸‰ç»´ç”ŸæˆAIæ¨¡å‹ã€‚</li>
<li>åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹å’Œå¤§è¯­è¨€æ¨¡å‹ï¼Œå®ç°è™šæ‹Ÿç‰©ä½“ä¸çœŸå®ç¯å¢ƒçš„æ— ç¼é›†æˆã€‚</li>
<li>æ¡†æ¶å±•ç¤ºäº†åœ¨æ¸¸æˆã€æ•™è‚²ã€é›¶å”®å’Œå®¤å†…è®¾è®¡ç­‰å¤šä¸ªè¡Œä¸šçš„åº”ç”¨æ½œåŠ›ã€‚</li>
<li>è¯¥æ¡†æ¶æœ‰åŠ©äºæ°‘ä¸»åŒ–ä¸‰ç»´æ¨¡å‹åˆ›å»ºï¼Œä½¿å…ˆè¿›çš„AIå·¥å…·å¯¹æ›´å¹¿æ³›çš„ç”¨æˆ·ç¾¤ä½“å¯ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.15869">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ab0bc3bbbfd2ed16a0e96af25887beae.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Pragmatic-Reasoning-improves-LLM-Code-Generation"><a href="#Pragmatic-Reasoning-improves-LLM-Code-Generation" class="headerlink" title="Pragmatic Reasoning improves LLM Code Generation"></a>Pragmatic Reasoning improves LLM Code Generation</h2><p><strong>Authors:Zhuchen Cao, Sven Apel, Adish Singla, Vera Demberg</strong></p>
<p>Large Language Models (LLMs) have demonstrated impressive potential in translating natural language (NL) instructions into program code. However, user instructions often contain inherent ambiguities, making it challenging for LLMs to generate code that accurately reflects the userâ€™s true intent. To address this challenge, researchers have proposed to produce multiple candidates of the program code and then rerank them to identify the best solution. In this paper, we propose CodeRSA, a novel code candidate reranking mechanism built upon the Rational Speech Act (RSA) framework, designed to guide LLMs toward more comprehensive pragmatic reasoning about user intent. We evaluate CodeRSA using one of the latest LLMs on a popular code generation dataset. Our experiment results show that CodeRSA consistently outperforms common baselines, surpasses the state-of-the-art approach in most cases, and demonstrates robust overall performance. These findings underscore the effectiveness of integrating pragmatic reasoning into code candidate reranking, offering a promising direction for enhancing code generation quality in LLMs. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å°†è‡ªç„¶è¯­è¨€ï¼ˆNLï¼‰æŒ‡ä»¤ç¿»è¯‘æˆç¨‹åºä»£ç æ–¹é¢è¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œç”¨æˆ·æŒ‡ä»¤é€šå¸¸åŒ…å«å›ºæœ‰çš„æ­§ä¹‰æ€§ï¼Œè¿™ä½¿å¾—å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹æ¥è¯´ï¼Œç”Ÿæˆå‡†ç¡®åæ˜ ç”¨æˆ·çœŸå®æ„å›¾çš„ä»£ç å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œç ”ç©¶äººå‘˜å»ºè®®ç”Ÿæˆå¤šä¸ªç¨‹åºä»£ç å€™é€‰ï¼Œç„¶åå¯¹å®ƒä»¬è¿›è¡Œé‡æ–°æ’åºä»¥æ‰¾å‡ºæœ€ä½³è§£å†³æ–¹æ¡ˆã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†CodeRSAï¼Œè¿™æ˜¯ä¸€ç§åŸºäºç†æ€§è¨€è¯­è¡Œä¸ºï¼ˆRSAï¼‰æ¡†æ¶çš„æ–°å‹ä»£ç å€™é€‰é‡æ–°æ’åºæœºåˆ¶ï¼Œæ—¨åœ¨å¼•å¯¼å¤§å‹è¯­è¨€æ¨¡å‹å¯¹ç”¨æˆ·æ„å›¾è¿›è¡Œæ›´å…¨é¢ã€å®ç”¨ä¸»ä¹‰çš„æ¨ç†ã€‚æˆ‘ä»¬ä½¿ç”¨æœ€æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¹‹ä¸€åœ¨ä¸€ä¸ªæµè¡Œçš„ä»£ç ç”Ÿæˆæ•°æ®é›†ä¸Šè¯„ä¼°äº†CodeRSAã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCodeRSAåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹å‡è¡¨ç°ä¼˜å¼‚ï¼Œå¹¶ä¸”åœ¨å¤šæ•°æƒ…å†µä¸‹ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œæ˜¾ç¤ºå‡ºç¨³å¥çš„æ•´ä½“æ€§èƒ½ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†å°†å®ç”¨ä¸»ä¹‰æ¨ç†æ•´åˆåˆ°ä»£ç å€™é€‰é‡æ–°æ’åºä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ºæå‡å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„ä»£ç ç”Ÿæˆè´¨é‡æä¾›äº†æœ‰å‰æ™¯çš„æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.15835v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å°†è‡ªç„¶è¯­è¨€æŒ‡ä»¤ç¿»è¯‘æˆç¨‹åºä»£ç æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†ç”¨æˆ·æŒ‡ä»¤ä¸­å¸¸å­˜åœ¨å›ºæœ‰æ­§ä¹‰ï¼Œä½¿LLMéš¾ä»¥å‡†ç¡®åæ˜ ç”¨æˆ·çœŸå®æ„å›¾åœ°ç”Ÿæˆä»£ç ã€‚ä¸ºè§£å†³æ­¤æŒ‘æˆ˜ï¼Œç ”ç©¶è€…æè®®ç”Ÿæˆç¨‹åºä»£ç çš„å¤šé‡å€™é€‰ï¼Œå¹¶è¿ç”¨CodeRSAè¿™ä¸€åŸºäºç†æ€§è¨€è¯­è¡Œä¸ºï¼ˆRSAï¼‰æ¡†æ¶çš„æ–°å‹ä»£ç å€™é€‰é‡æ’æœºåˆ¶å¯¹å€™é€‰è¿›è¡Œæ’åºä»¥æ‰¾åˆ°æœ€ä½³è§£å†³æ–¹æ¡ˆã€‚è¯„ä¼°å®éªŒç»“æœè¡¨æ˜ï¼ŒCodeRSAåœ¨å¤šæ•°æƒ…å†µä¸‹è¡¨ç°ä¼˜äºå¸¸è§„åŸºå‡†å’Œæœ€æ–°å‰æ²¿æ–¹æ³•ï¼Œå±•ç°å‡ºç¨³å¥çš„æ•´ä½“æ€§èƒ½ã€‚è¿™è¡¨æ˜å°†è¯­ç”¨æ¨ç†èå…¥ä»£ç å€™é€‰é‡æ’å¯æœ‰æ•ˆæå‡LLMçš„ä»£ç ç”Ÿæˆè´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªç„¶è¯­è¨€ç¿»è¯‘ä»£ç æ–¹é¢å±•ç°å‡ºæ½œåŠ›ã€‚</li>
<li>ç”¨æˆ·æŒ‡ä»¤ä¸­çš„æ­§ä¹‰ç»™LLMå‡†ç¡®ç”Ÿæˆä»£ç å¸¦æ¥æŒ‘æˆ˜ã€‚</li>
<li>ä¸ºè§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œç ”ç©¶è€…æè®®ç”Ÿæˆä»£ç çš„å¤šé‡å€™é€‰å¹¶è¿›è¡Œæ’åºã€‚</li>
<li>CodeRSAæ˜¯ä¸€ç§åŸºäºç†æ€§è¨€è¯­è¡Œä¸ºï¼ˆRSAï¼‰æ¡†æ¶çš„æ–°å‹ä»£ç å€™é€‰é‡æ’æœºåˆ¶ã€‚</li>
<li>CodeRSAåœ¨è¯„ä¼°å®éªŒä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šå¸¸è§„åŸºå‡†å’Œå‰æ²¿æ–¹æ³•ã€‚</li>
<li>å®éªŒç»“æœè¯æ˜äº†å°†è¯­ç”¨æ¨ç†èå…¥ä»£ç å€™é€‰é‡æ’çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.15835">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a0438d654c48f4f2fc41fafe0c96c556.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f0d4655c541712b64ae74b815232dd95.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca3bdf58fd5ef203c69dc97548c34a72.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Retrieval-Augmented-Speech-Recognition-Approach-for-Domain-Challenges"><a href="#Retrieval-Augmented-Speech-Recognition-Approach-for-Domain-Challenges" class="headerlink" title="Retrieval-Augmented Speech Recognition Approach for Domain Challenges"></a>Retrieval-Augmented Speech Recognition Approach for Domain Challenges</h2><p><strong>Authors:Peng Shen, Xugang Lu, Hisashi Kawai</strong></p>
<p>Speech recognition systems often face challenges due to domain mismatch, particularly in real-world applications where domain-specific data is unavailable because of data accessibility and confidentiality constraints. Inspired by Retrieval-Augmented Generation (RAG) techniques for large language models (LLMs), this paper introduces a LLM-based retrieval-augmented speech recognition method that incorporates domain-specific textual data at the inference stage to enhance recognition performance. Rather than relying on domain-specific textual data during the training phase, our model is trained to learn how to utilize textual information provided in prompts for LLM decoder to improve speech recognition performance. Benefiting from the advantages of the RAG retrieval mechanism, our approach efficiently accesses locally available domain-specific documents, ensuring a convenient and effective process for solving domain mismatch problems. Experiments conducted on the CSJ database demonstrate that the proposed method significantly improves speech recognition accuracy and achieves state-of-the-art results on the CSJ dataset, even without relying on the full training data. </p>
<blockquote>
<p>è¯­éŸ³è¯†åˆ«ç³»ç»Ÿç»å¸¸é¢ä¸´å› é¢†åŸŸä¸åŒ¹é…è€Œå¸¦æ¥çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ç°å®åº”ç”¨ä¸­ï¼Œç”±äºæ•°æ®å¯è®¿é—®æ€§å’Œä¿å¯†æ€§çº¦æŸï¼Œå¾€å¾€æ— æ³•è·å¾—ç‰¹å®šé¢†åŸŸçš„è¯­éŸ³æ•°æ®ã€‚å—å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯çš„å¯å‘ï¼Œæœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºLLMçš„æ£€ç´¢å¢å¼ºè¯­éŸ³è¯†åˆ«æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨æ¨ç†é˜¶æ®µç»“åˆç‰¹å®šé¢†åŸŸçš„æ–‡æœ¬æ•°æ®ï¼Œä»¥æé«˜è¯†åˆ«æ€§èƒ½ã€‚ä¸å…¶ä»–æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ¨¡å‹ä¸æ˜¯åœ¨è®­ç»ƒé˜¶æ®µä¾èµ–ç‰¹å®šé¢†åŸŸçš„æ–‡æœ¬æ•°æ®ï¼Œè€Œæ˜¯å­¦ä¹ å¦‚ä½•åˆ©ç”¨æç¤ºä¸­çš„æ–‡æœ¬ä¿¡æ¯ï¼Œä»¥ä¿ƒä½¿LLMè§£ç å™¨æé«˜è¯­éŸ³è¯†åˆ«æ€§èƒ½ã€‚å¾—ç›ŠäºRAGæ£€ç´¢æœºåˆ¶çš„ä¼˜åŠ¿ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°è®¿é—®æœ¬åœ°å¯ç”¨çš„ç‰¹å®šé¢†åŸŸæ–‡æ¡£ï¼Œç¡®ä¿è§£å†³é¢†åŸŸä¸åŒ¹é…é—®é¢˜çš„è¿‡ç¨‹æ—¢æ–¹ä¾¿åˆé«˜æ•ˆã€‚åœ¨CSJæ•°æ®åº“ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†è¯­éŸ³è¯†åˆ«ç²¾åº¦ï¼Œåœ¨CSJæ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„è¯†åˆ«æ•ˆæœï¼Œç”šè‡³åœ¨ä¸ä¾èµ–å…¨éƒ¨è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹ä¹Ÿæ˜¯å¦‚æ­¤ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.15264v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ£€ç´¢å¢å¼ºè¯­éŸ³è¯†åˆ«æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨æ¨ç†é˜¶æ®µèå…¥é¢†åŸŸç‰¹å®šæ–‡æœ¬æ•°æ®ï¼Œä»¥æé«˜è¯­éŸ³è¯†åˆ«æ€§èƒ½ã€‚è¯¥æ–¹æ³•æ— éœ€åœ¨è®­ç»ƒé˜¶æ®µä¾èµ–é¢†åŸŸç‰¹å®šæ–‡æœ¬æ•°æ®ï¼Œè€Œæ˜¯å­¦ä¹ å¦‚ä½•åˆ©ç”¨æç¤ºä¸­çš„æ–‡æœ¬ä¿¡æ¯ï¼Œæ”¹å–„è¯­éŸ³è¯†åˆ«æ•ˆæœã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æ˜¾è‘—æé«˜è¯­éŸ³è¯†åˆ«å‡†ç¡®ç‡ï¼Œè¾¾åˆ°CSJæ•°æ®é›†ä¸Šçš„æœ€æ–°æ°´å¹³ï¼Œä¸”æ— éœ€ä¾èµ–å…¨éƒ¨è®­ç»ƒæ•°æ®ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è¯¥è®ºæ–‡è§£å†³äº†è¯­éŸ³è¯†åˆ«ç³»ç»Ÿä¸­çš„é¢†åŸŸä¸åŒ¹é…é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®è·å–å’Œä¿å¯†æ€§å—é™çš„ç°å®ä¸­ã€‚</li>
<li>é€šè¿‡ç»“åˆå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„è¯­éŸ³è¯†åˆ«æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨æ¨ç†é˜¶æ®µèå…¥é¢†åŸŸç‰¹å®šæ–‡æœ¬æ•°æ®ï¼Œè€Œä¸æ˜¯åœ¨è®­ç»ƒé˜¶æ®µã€‚</li>
<li>æ–¹æ³•çš„æ ¸å¿ƒåœ¨äºå­¦ä¹ å¦‚ä½•åˆ©ç”¨æç¤ºä¸­çš„æ–‡æœ¬ä¿¡æ¯ï¼Œæ”¹å–„è¯­éŸ³è¯†åˆ«æ•ˆæœã€‚</li>
<li>è®ºæ–‡é‡‡ç”¨RAGæ£€ç´¢æœºåˆ¶ï¼Œèƒ½é«˜æ•ˆè®¿é—®æœ¬åœ°å¯ç”¨çš„é¢†åŸŸç‰¹å®šæ–‡æ¡£ã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æ˜¾è‘—æé«˜è¯­éŸ³è¯†åˆ«å‡†ç¡®ç‡ï¼Œè¾¾åˆ°æœ€æ–°æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.15264">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-87968c188ddea404610974386096b3e4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-81867d91435f9cee822b0bb90f72c461.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1aaab7cc3f980fcace86bbf43d3f233a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d1a21a8672b96cc51c761100cb525d56.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-65de1087f77945b5d5a0dc791dcc6466.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-478aaf7d881446049035f88eb2850545.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Enhancing-Speech-Large-Language-Models-with-Prompt-Aware-Mixture-of-Audio-Encoders"><a href="#Enhancing-Speech-Large-Language-Models-with-Prompt-Aware-Mixture-of-Audio-Encoders" class="headerlink" title="Enhancing Speech Large Language Models with Prompt-Aware Mixture of   Audio Encoders"></a>Enhancing Speech Large Language Models with Prompt-Aware Mixture of   Audio Encoders</h2><p><strong>Authors:Weiqiao Shan, Yuang Li, Yuhao Zhang, Yingfeng Luo, Chen Xu, Xiaofeng Zhao, Long Meng, Yunfei Lu, Min Zhang, Hao Yang, Tong Xiao, Jingbo Zhu</strong></p>
<p>Connecting audio encoders with large language models (LLMs) allows the LLM to perform various audio understanding tasks, such as automatic speech recognition (ASR) and audio captioning (AC). Most research focuses on training an adapter layer to generate a unified audio feature for the LLM. However, different tasks may require distinct features that emphasize either semantic or acoustic aspects, making task-specific audio features more desirable. In this paper, we propose Prompt-aware Mixture (PaM) to enhance the Speech LLM that uses multiple audio encoders. Our approach involves using different experts to extract different features based on the prompt that indicates different tasks. Experiments demonstrate that with PaM, only one Speech LLM surpasses the best performances achieved by all single-encoder Speech LLMs on ASR, Speaker Number Verification, and AC tasks. PaM also outperforms other feature fusion baselines, such as concatenation and averaging. </p>
<blockquote>
<p>å°†éŸ³é¢‘ç¼–ç å™¨ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç›¸è¿æ¥ï¼Œå¯ä»¥ä½¿LLMæ‰§è¡Œå„ç§éŸ³é¢‘ç†è§£ä»»åŠ¡ï¼Œä¾‹å¦‚è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å’ŒéŸ³é¢‘æè¿°ï¼ˆACï¼‰ã€‚å¤§å¤šæ•°ç ”ç©¶é›†ä¸­äºè®­ç»ƒé€‚é…å™¨å±‚ä»¥ç”Ÿæˆç”¨äºLLMçš„ç»Ÿä¸€éŸ³é¢‘ç‰¹å¾ã€‚ç„¶è€Œï¼Œä¸åŒçš„ä»»åŠ¡å¯èƒ½éœ€è¦å¼ºè°ƒè¯­ä¹‰æˆ–å£°å­¦æ–¹é¢çš„ä¸åŒç‰¹å¾ï¼Œè¿™ä½¿å¾—ä»»åŠ¡ç‰¹å®šçš„éŸ³é¢‘ç‰¹å¾æ›´ä¸ºç†æƒ³ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºä½¿ç”¨æç¤ºæ„ŸçŸ¥æ··åˆï¼ˆPaMï¼‰æŠ€æœ¯å¢å¼ºä½¿ç”¨å¤šä¸ªéŸ³é¢‘ç¼–ç å™¨çš„è¯­éŸ³LLMã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬æ ¹æ®æç¤ºä½¿ç”¨ä¸åŒçš„ä¸“å®¶æ¥æå–ä¸åŒçš„ç‰¹å¾ï¼Œè¿™äº›æç¤ºè¡¨æ˜äº†ä¸åŒçš„ä»»åŠ¡ã€‚å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨PaMçš„å•ä¸€è¯­éŸ³LLMåœ¨ASRã€è¯´è¯äººæ•°é‡éªŒè¯å’ŒACä»»åŠ¡ä¸Šçš„è¡¨ç°è¶…è¿‡äº†æ‰€æœ‰å•ç¼–ç å™¨è¯­éŸ³LLMçš„æœ€ä½³æ€§èƒ½ã€‚PaMè¿˜ä¼˜äºå…¶ä»–ç‰¹å¾èåˆåŸºçº¿æ–¹æ³•ï¼Œå¦‚æ‹¼æ¥å’Œå¹³å‡åŒ–æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.15178v1">PDF</a> 12 pages,4 figures, 7 tables</p>
<p><strong>Summary</strong><br>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„éŸ³é¢‘ç¼–ç å™¨è¿æ¥ä½¿å¾—LLMèƒ½å¤Ÿè¿›è¡ŒéŸ³é¢‘ç†è§£ä»»åŠ¡ï¼Œå¦‚è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å’ŒéŸ³é¢‘æè¿°ï¼ˆACï¼‰ã€‚å°½ç®¡å¤§å¤šæ•°ç ”ç©¶é›†ä¸­åœ¨è®­ç»ƒä¸€ä¸ªç»Ÿä¸€çš„éŸ³é¢‘ç‰¹å¾ç”Ÿæˆé€‚é…å™¨å±‚ï¼Œä½†ä¸åŒçš„ä»»åŠ¡å¯èƒ½éœ€è¦å¼ºè°ƒè¯­ä¹‰æˆ–å£°å­¦æ–¹é¢çš„ä¸åŒç‰¹å¾ï¼Œå› æ­¤ä»»åŠ¡ç‰¹å®šçš„éŸ³é¢‘ç‰¹å¾æ›´ä¸ºç†æƒ³ã€‚æœ¬æ–‡æå‡ºäº†åŸºäºæç¤ºæ„ŸçŸ¥æ··åˆï¼ˆPaMï¼‰çš„è¯­éŸ³LLMå¢å¼ºæ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨å¤šä¸ªéŸ³é¢‘ç¼–ç å™¨æå–ä¸åŒçš„ç‰¹å¾ï¼Œæ ¹æ®æç¤ºé’ˆå¯¹ä¸åŒçš„ä»»åŠ¡ä½¿ç”¨ä¸åŒçš„ä¸“å®¶ã€‚å®éªŒè¡¨æ˜ï¼Œé‡‡ç”¨PaMçš„å•ä¸€è¯­éŸ³LLMåœ¨ASRã€è¯´è¯äººæ•°éªŒè¯å’ŒACä»»åŠ¡ä¸Šçš„æ€§èƒ½è¶…è¿‡äº†æ‰€æœ‰å•ä¸€ç¼–ç å™¨è¯­éŸ³LLMçš„æœ€ä½³æ€§èƒ½ï¼Œå¹¶ä¼˜äºå…¶ä»–ç‰¹å¾èåˆåŸºçº¿æ–¹æ³•ï¼Œå¦‚æ‹¼æ¥å’Œå¹³å‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éŸ³é¢‘ç¼–ç å™¨ä¸å¤§å‹è¯­è¨€æ¨¡å‹çš„ç»“åˆä½¿å¾—è¯­è¨€æ¨¡å‹èƒ½å¤Ÿæ‰§è¡Œå¤šç§éŸ³é¢‘ç†è§£ä»»åŠ¡ï¼Œå¦‚è¯­éŸ³è¯†åˆ«å’ŒéŸ³é¢‘æè¿°ã€‚</li>
<li>å¤§å¤šæ•°ç ”ç©¶é›†ä¸­åœ¨è®­ç»ƒé€‚é…å™¨å±‚ä»¥ç”Ÿæˆç»Ÿä¸€çš„éŸ³é¢‘ç‰¹å¾ï¼Œä½†ä»»åŠ¡ç‰¹å®šéŸ³é¢‘ç‰¹å¾çš„é‡è¦æ€§è¢«å¿½è§†ã€‚</li>
<li>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºæç¤ºæ„ŸçŸ¥æ··åˆï¼ˆPaMï¼‰çš„æ–¹æ³•ï¼Œä»¥å¢å¼ºè¯­éŸ³LLMçš„æ€§èƒ½ã€‚</li>
<li>PaMä½¿ç”¨å¤šä¸ªéŸ³é¢‘ç¼–ç å™¨ï¼Œæ ¹æ®ä»»åŠ¡æç¤ºæå–ä¸åŒçš„ç‰¹å¾ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨PaMçš„è¯­éŸ³LLMåœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¶…è¶Šäº†æœ€ä½³çš„å•ç¼–ç å™¨è¯­éŸ³LLMæ€§èƒ½ã€‚</li>
<li>PaMæ–¹æ³•ä¼˜äºå…¶ä»–ç‰¹å¾èåˆæ–¹æ³•ï¼Œå¦‚ç®€å•çš„æ‹¼æ¥å’Œå¹³å‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.15178">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4d38eb8b7624848e484d1b66d0995748.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0dfea113910f27f093691d5fbe9379f0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d33e239884d6de4fc6d45ee3864b060b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-84556732b02df940e8b1057f48d3b858.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a96d285529d5e9c988e3610f5fc1e221.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-17b506b20775114af40948f3e329094b.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Unveiling-Reasoning-Thresholds-in-Language-Models-Scaling-Fine-Tuning-and-Interpretability-through-Attention-Maps"><a href="#Unveiling-Reasoning-Thresholds-in-Language-Models-Scaling-Fine-Tuning-and-Interpretability-through-Attention-Maps" class="headerlink" title="Unveiling Reasoning Thresholds in Language Models: Scaling, Fine-Tuning,   and Interpretability through Attention Maps"></a>Unveiling Reasoning Thresholds in Language Models: Scaling, Fine-Tuning,   and Interpretability through Attention Maps</h2><p><strong>Authors:Yen-Che Hsiao, Abhishek Dutta</strong></p>
<p>This study investigates the in-context learning capabilities of various decoder-only transformer-based language models with different model sizes and training data, including GPT2, SmolLM2, OpenELM, TinyLlama, Stable LM, and Gemma 2. We identify a critical parameter threshold (~1.6 billion), beyond which reasoning performance improves significantly in tasks such as commonsense reasoning in multiple-choice question answering and deductive reasoning. Specifically, models above this threshold achieve better success rates in chain-of-thought (CoT) prompting for deductive reasoning tasks, especially those requiring longer reasoning chains, such as proof by contradiction and disjunction elimination. To address limitations in sub-threshold models, we demonstrate that fine-tuning with task-specific exemplars substantially enhances reasoning performance, enabling accurate CoT generation even without additional exemplars in the prompt for tasks with shorter reasoning chains. Finally, our analysis of attention maps reveals that models capable of generating correct CoTs exhibit higher token-level attention scores on subsequent correct tokens and the correct parts of speech, providing interpretability insights into reasoning processes. These findings collectively advance understanding of reasoning capabilities in decoder-only transformer-based models. The code is available at: <a target="_blank" rel="noopener" href="https://github.com/AnnonymousForPapers/CoT_Reasoning_Test">https://github.com/AnnonymousForPapers/CoT_Reasoning_Test</a>. </p>
<blockquote>
<p>æœ¬ç ”ç©¶è°ƒæŸ¥äº†ä¸åŒè§„æ¨¡çš„è§£ç å™¨ä¸“ç”¨åŸºäºè½¬æ¢å™¨çš„è¯­è¨€æ¨¡å‹ä»¥åŠä½¿ç”¨ä¸åŒè®­ç»ƒæ•°æ®çš„æ¨¡å‹åœ¨å„ç§ä¸Šä¸‹æ–‡å­¦ä¹ åŠŸèƒ½æ–¹é¢çš„è¡¨ç°ï¼ŒåŒ…æ‹¬GPT2ã€SmolLM2ã€OpenELMã€TinyLlamaã€Stable LMå’ŒGemma 2ã€‚æˆ‘ä»¬ç¡®å®šäº†ä¸´ç•Œå‚æ•°é˜ˆå€¼ï¼ˆçº¦1.6äº¿ï¼‰ï¼Œè¶…å‡ºè¯¥é˜ˆå€¼åï¼Œåœ¨å¤šé€‰é—®ç­”ä¸­çš„å¸¸è¯†æ¨ç†å’Œæ¼”ç»æ¨ç†ç­‰ä»»åŠ¡ä¸­çš„æ¨ç†æ€§èƒ½ä¼šæ˜¾è‘—æé«˜ã€‚å…·ä½“æ¥è¯´ï¼Œè¶…è¿‡æ­¤é˜ˆå€¼çš„æ¨¡å‹åœ¨æ¼”ç»æ¨ç†ä»»åŠ¡çš„æ€ç»´é“¾æç¤ºä¸­å–å¾—äº†æ›´é«˜çš„æˆåŠŸç‡ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦è¾ƒé•¿æ¨ç†é“¾çš„ä»»åŠ¡ä¸­ï¼Œå¦‚åè¯æ³•å’Œæå–æ¶ˆé™¤æ³•ã€‚ä¸ºäº†è§£å†³ä½äºé˜ˆå€¼æ¨¡å‹çš„å±€é™æ€§ï¼Œæˆ‘ä»¬è¯æ˜ä½¿ç”¨ç‰¹å®šä»»åŠ¡çš„ç¤ºä¾‹è¿›è¡Œå¾®è°ƒå¯ä»¥å¤§å¤§æé«˜æ¨ç†æ€§èƒ½ï¼Œå³ä½¿åœ¨æ²¡æœ‰é¢å¤–çš„ç®€çŸ­æ¨ç†é“¾æç¤ºä¸­ä¹Ÿèƒ½å‡†ç¡®ç”Ÿæˆæ€ç»´é“¾ã€‚æœ€åï¼Œé€šè¿‡å¯¹æ³¨æ„åŠ›åœ°å›¾çš„åˆ†æï¼Œæˆ‘ä»¬å‘ç°èƒ½å¤Ÿç”Ÿæˆæ­£ç¡®æ€ç»´é“¾çš„æ¨¡å‹åœ¨éšåçš„æ­£ç¡®ä»¤ç‰Œå’Œæ­£ç¡®çš„è¯æ€§æ–¹é¢è¡¨ç°å‡ºæ›´é«˜çš„ä»¤ç‰Œçº§æ³¨æ„åŠ›å¾—åˆ†ï¼Œè¿™ä¸ºç†è§£æ¨ç†è¿‡ç¨‹æä¾›äº†å¯è§£é‡Šæ€§æ´å¯Ÿã€‚è¿™äº›å‘ç°å…±åŒæ¨åŠ¨äº†æˆ‘ä»¬å¯¹åŸºäºè§£ç å™¨çš„è½¬æ¢å™¨æ¨¡å‹æ¨ç†èƒ½åŠ›çš„ç†è§£ã€‚ä»£ç å¯ç”¨åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/AnnonymousForPapers/CoT_Reasoning_Test%E3%80%82">https://github.com/AnnonymousForPapers/CoT_Reasoning_Testã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.15120v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶æ¢è®¨äº†ä¸åŒè§„æ¨¡ä¸è®­ç»ƒæ•°æ®çš„è§£ç å™¨åªå˜å‹å™¨è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ï¼ŒåŒ…æ‹¬GPT2ã€SmolLM2ã€OpenELMç­‰æ¨¡å‹ã€‚ç ”ç©¶å‘ç°ä¸€ä¸ªå…³é”®å‚æ•°é˜ˆå€¼ï¼ˆçº¦1.6äº¿ï¼‰ï¼Œè¶…è¿‡æ­¤é˜ˆå€¼çš„æ¨¡å‹åœ¨å¤šé€‰é—®ç­”ä¸æ¼”ç»æ¨ç†ç­‰ä»»åŠ¡ä¸­çš„å¸¸è¯†æ¨ç†æ€§èƒ½æ˜¾è‘—æå‡ã€‚ç‰¹åˆ«æ˜¯é€šè¿‡æ€ç»´é“¾æç¤ºæ³•ï¼Œè¶…è¿‡é˜ˆå€¼çš„æ¨¡å‹åœ¨éœ€è¦è¾ƒé•¿æ¨ç†é“¾çš„ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚å¯¹äºæœªè¾¾åˆ°é˜ˆå€¼çš„æ¨¡å‹ï¼Œé€šè¿‡ç‰¹å®šä»»åŠ¡çš„å¾®è°ƒå¯ä»¥æå¤§åœ°å¢å¼ºæ¨ç†æ€§èƒ½ã€‚æ­¤å¤–ï¼Œé€šè¿‡åˆ†ææ³¨æ„åŠ›åœ°å›¾å‘ç°ï¼Œèƒ½æ­£ç¡®ç”Ÿæˆæ€ç»´é“¾çš„æ¨¡å‹åœ¨åç»­æ­£ç¡®ä»¤ç‰Œå’Œæ­£ç¡®è¯ç±»ä¸Šçš„ä»¤ç‰Œçº§åˆ«æ³¨æ„åŠ›å¾—åˆ†æ›´é«˜ï¼Œä¸ºç†è§£æ¨ç†è¿‡ç¨‹æä¾›äº†å¯è§£é‡Šæ€§æ´å¯Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶æ¶‰åŠå¤šç§è§£ç å™¨åªå˜å‹å™¨è¯­è¨€æ¨¡å‹ï¼ŒåŒ…æ‹¬GPT2ç­‰ï¼Œæ¢è®¨äº†å®ƒä»¬çš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ã€‚</li>
<li>å‘ç°æ¨¡å‹æ€§èƒ½çš„å…³é”®å‚æ•°é˜ˆå€¼ï¼ˆçº¦1.6äº¿ï¼‰ï¼Œè¶…è¿‡æ­¤é˜ˆå€¼çš„æ¨¡å‹åœ¨å¸¸è¯†æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°æ›´ä¼˜ç§€ã€‚</li>
<li>æ€ç»´é“¾æç¤ºæ³•åœ¨éœ€è¦è¾ƒé•¿æ¨ç†é“¾çš„ä»»åŠ¡ä¸­ç‰¹åˆ«æœ‰æ•ˆã€‚</li>
<li>å¯¹æœªè¾¾åˆ°æ€§èƒ½é˜ˆå€¼çš„æ¨¡å‹è¿›è¡Œç‰¹å®šä»»åŠ¡å¾®è°ƒå¯ä»¥æ˜¾è‘—æé«˜æ¨ç†æ€§èƒ½ã€‚</li>
<li>åˆ†ææ³¨æ„åŠ›åœ°å›¾å‘ç°æ­£ç¡®ç”Ÿæˆæ€ç»´é“¾çš„æ¨¡å‹å±•ç°å‡ºæ›´é«˜çš„ä»¤ç‰Œçº§åˆ«æ³¨æ„åŠ›å¾—åˆ†ã€‚</li>
<li>ç ”ç©¶æä¾›äº†å¯¹è§£ç å™¨åªå˜å‹å™¨æ¨¡å‹çš„æ¨ç†èƒ½åŠ›çš„æ·±å…¥ç†è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.15120">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-bafc0515edfd14d0d7b69a70d5ef7530.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6615e3058ab322bfaa9d641b4a5e752a.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="VLAS-Vision-Language-Action-Model-With-Speech-Instructions-For-Customized-Robot-Manipulation"><a href="#VLAS-Vision-Language-Action-Model-With-Speech-Instructions-For-Customized-Robot-Manipulation" class="headerlink" title="VLAS: Vision-Language-Action Model With Speech Instructions For   Customized Robot Manipulation"></a>VLAS: Vision-Language-Action Model With Speech Instructions For   Customized Robot Manipulation</h2><p><strong>Authors:Wei Zhao, Pengxiang Ding, Min Zhang, Zhefei Gong, Shuanghao Bai, Han Zhao, Donglin Wang</strong></p>
<p>Vision-language-action models (VLAs) have become increasingly popular in robot manipulation for their end-to-end design and remarkable performance. However, existing VLAs rely heavily on vision-language models (VLMs) that only support text-based instructions, neglecting the more natural speech modality for human-robot interaction. Traditional speech integration methods usually involves a separate speech recognition system, which complicates the model and introduces error propagation. Moreover, the transcription procedure would lose non-semantic information in the raw speech, such as voiceprint, which may be crucial for robots to successfully complete customized tasks. To overcome above challenges, we propose VLAS, a novel end-to-end VLA that integrates speech recognition directly into the robot policy model. VLAS allows the robot to understand spoken commands through inner speech-text alignment and produces corresponding actions to fulfill the task. We also present two new datasets, SQA and CSI, to support a three-stage tuning process for speech instructions, which empowers VLAS with the ability of multimodal interaction across text, image, speech, and robot actions. Taking a step further, a voice retrieval-augmented generation (RAG) paradigm is designed to enable our model to effectively handle tasks that require individual-specific knowledge. Our extensive experiments show that VLAS can effectively accomplish robot manipulation tasks with diverse speech commands, offering a seamless and customized interaction experience. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€è¡ŒåŠ¨æ¨¡å‹ï¼ˆVLAsï¼‰å› å…¶ç«¯åˆ°ç«¯çš„è®¾è®¡å’Œå“è¶Šæ€§èƒ½è€Œåœ¨æœºå™¨äººæ“ä½œé¢†åŸŸè¶Šæ¥è¶Šå—æ¬¢è¿ã€‚ç„¶è€Œï¼Œç°æœ‰çš„VLAsä¸¥é‡ä¾èµ–äºä»…æ”¯æŒæ–‡æœ¬æŒ‡ä»¤çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ï¼Œå¿½è§†äº†äººç±»ä¸æœºå™¨äººäº¤äº’ä¸­æ›´è‡ªç„¶çš„è¯­éŸ³æ¨¡å¼ã€‚ä¼ ç»Ÿçš„è¯­éŸ³é›†æˆæ–¹æ³•é€šå¸¸éœ€è¦ä¸€ä¸ªç‹¬ç«‹çš„è¯­éŸ³è¯†åˆ«ç³»ç»Ÿï¼Œè¿™å¢åŠ äº†æ¨¡å‹çš„å¤æ‚æ€§å¹¶å¼•å…¥äº†è¯¯å·®ä¼ æ’­ã€‚æ­¤å¤–ï¼Œè½¬å½•è¿‡ç¨‹ä¼šä¸¢å¤±åŸå§‹è¯­éŸ³ä¸­çš„éè¯­ä¹‰ä¿¡æ¯ï¼Œå¦‚å£°çº¹ï¼Œè¿™å¯¹äºæœºå™¨äººæˆåŠŸå®Œæˆå®šåˆ¶ä»»åŠ¡å¯èƒ½æ˜¯è‡³å…³é‡è¦çš„ã€‚ä¸ºäº†å…‹æœä¸Šè¿°æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†VLASï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„ç«¯åˆ°ç«¯VLAï¼Œå®ƒç›´æ¥å°†è¯­éŸ³è¯†åˆ«é›†æˆåˆ°æœºå™¨äººç­–ç•¥æ¨¡å‹ä¸­ã€‚VLASå…è®¸æœºå™¨äººé€šè¿‡å†…éƒ¨è¯­éŸ³æ–‡æœ¬å¯¹é½ç†è§£å£å¤´å‘½ä»¤ï¼Œå¹¶äº§ç”Ÿç›¸åº”çš„è¡ŒåŠ¨æ¥å®Œæˆä»»åŠ¡ã€‚æˆ‘ä»¬è¿˜æ¨å‡ºäº†ä¸¤ä¸ªæ–°æ•°æ®é›†SQAå’ŒCSIï¼Œä»¥æ”¯æŒè¯­éŸ³æŒ‡ä»¤çš„ä¸‰é˜¶æ®µè°ƒæ•´è¿‡ç¨‹ï¼Œè¿™ä½¿VLASå…·å¤‡æ–‡æœ¬ã€å›¾åƒã€è¯­éŸ³å’Œæœºå™¨äººè¡ŒåŠ¨ä¹‹é—´çš„è·¨æ¨¡æ€äº¤äº’èƒ½åŠ›ã€‚æ›´è¿›ä¸€æ­¥çš„æ˜¯ï¼Œè®¾è®¡äº†ä¸€ç§å£°éŸ³æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰èŒƒå¼ï¼Œä½¿æˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆå¤„ç†éœ€è¦ä¸ªäººç‰¹å®šçŸ¥è¯†çš„ä»»åŠ¡ã€‚æˆ‘ä»¬çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒVLASå¯ä»¥æœ‰æ•ˆåœ°å®Œæˆå…·æœ‰å¤šç§è¯­éŸ³å‘½ä»¤çš„æœºå™¨äººæ“ä½œä»»åŠ¡ï¼Œæä¾›æ— ç¼ä¸”å®šåˆ¶åŒ–çš„äº¤äº’ä½“éªŒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13508v2">PDF</a> Accepted as a conference paper at ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è§†è§‰è¯­è¨€è¡ŒåŠ¨æ¨¡å‹ï¼ˆVLAsï¼‰åœ¨æœºå™¨äººæ“ä½œä¸­çš„æœ€æ–°åº”ç”¨ã€‚é’ˆå¯¹ç°æœ‰VLAsä¸»è¦ä¾èµ–æ–‡æœ¬æŒ‡ä»¤è€Œå¿½è§†è¯­éŸ³æ¨¡å¼çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹ç«¯åˆ°ç«¯çš„VLAæ¨¡å‹VLASï¼Œå°†è¯­éŸ³è¯†åˆ«ç›´æ¥é›†æˆåˆ°æœºå™¨äººç­–ç•¥æ¨¡å‹ä¸­ã€‚VLASèƒ½é€šè¿‡å†…éƒ¨è¯­éŸ³æ–‡æœ¬å¯¹é½ç†è§£å£è¯­æŒ‡ä»¤ï¼Œå¹¶äº§ç”Ÿç›¸åº”çš„è¡ŒåŠ¨æ¥å®Œæˆä»»åŠ¡ã€‚æ­¤å¤–ï¼Œè¿˜æ¨å‡ºäº†ä¸¤ä¸ªæ–°æ•°æ®é›†SQAå’ŒCSIï¼Œæ”¯æŒè¯­éŸ³æŒ‡ä»¤çš„ä¸‰é˜¶æ®µè°ƒæ•´è¿‡ç¨‹ï¼Œä½¿VLASå…·å¤‡è·¨æ–‡æœ¬ã€å›¾åƒã€è¯­éŸ³å’Œæœºå™¨äººè¡ŒåŠ¨çš„å¤šåª’ä½“äº¤äº’èƒ½åŠ›ã€‚æœ€åï¼Œé€šè¿‡å¼•å…¥å£°éŸ³æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰èŒƒå¼ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå¤„ç†éœ€è¦ä¸ªæ€§åŒ–çŸ¥è¯†çš„ä»»åŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰è¯­è¨€è¡ŒåŠ¨æ¨¡å‹ï¼ˆVLAsï¼‰åœ¨æœºå™¨äººæ“ä½œä¸­çš„åº”ç”¨æ—¥ç›Šæ™®åŠï¼Œå…·æœ‰ç«¯åˆ°ç«¯çš„è®¾è®¡å’Œå“è¶Šæ€§èƒ½ã€‚</li>
<li>ç°æœ‰VLAsä¸»è¦ä¾èµ–æ–‡æœ¬æŒ‡ä»¤ï¼Œå¿½è§†äº†æ›´è‡ªç„¶çš„è¯­éŸ³æ¨¡å¼çš„äººæœºäº¤äº’æ–¹å¼ã€‚</li>
<li>æ–°å‹VLASæ¨¡å‹å°†è¯­éŸ³è¯†åˆ«ç›´æ¥é›†æˆåˆ°æœºå™¨äººç­–ç•¥æ¨¡å‹ä¸­ï¼Œç†è§£å£è¯­æŒ‡ä»¤å¹¶äº§ç”Ÿç›¸åº”è¡ŒåŠ¨ã€‚</li>
<li>VLASé€šè¿‡å†…éƒ¨è¯­éŸ³æ–‡æœ¬å¯¹é½å®ç°å£è¯­æŒ‡ä»¤ç†è§£ã€‚</li>
<li>æ¨å‡ºäº†SQAå’ŒCSIä¸¤ä¸ªæ–°æ•°æ®é›†ï¼Œæ”¯æŒè¯­éŸ³æŒ‡ä»¤çš„ä¸‰é˜¶æ®µè°ƒæ•´è¿‡ç¨‹ï¼Œå¢å¼ºVLASçš„å¤šåª’ä½“äº¤äº’èƒ½åŠ›ã€‚</li>
<li>VLASèƒ½å¤Ÿå¤„ç†éœ€è¦ä¸ªæ€§åŒ–çŸ¥è¯†çš„ä»»åŠ¡ï¼Œå¼•å…¥å£°éŸ³æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰èŒƒå¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13508">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-495948d6a743c703d1c1bb0a7781aa28.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-09406fcc63a46cd60c63870aa3fead1f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ba13a25fefefda2b4a1af18ab7b3ce3c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad6aff11d02c3d0d6335b7cb9e5a1cb8.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="VINP-Variational-Bayesian-Inference-with-Neural-Speech-Prior-for-Joint-ASR-Effective-Speech-Dereverberation-and-Blind-RIR-Identification"><a href="#VINP-Variational-Bayesian-Inference-with-Neural-Speech-Prior-for-Joint-ASR-Effective-Speech-Dereverberation-and-Blind-RIR-Identification" class="headerlink" title="VINP: Variational Bayesian Inference with Neural Speech Prior for Joint   ASR-Effective Speech Dereverberation and Blind RIR Identification"></a>VINP: Variational Bayesian Inference with Neural Speech Prior for Joint   ASR-Effective Speech Dereverberation and Blind RIR Identification</h2><p><strong>Authors:Pengyu Wang, Ying Fang, Xiaofei Li</strong></p>
<p>Reverberant speech, denoting the speech signal degraded by the process of reverberation, contains crucial knowledge of both anechoic source speech and room impulse response (RIR). This work proposes a variational Bayesian inference (VBI) framework with neural speech prior (VINP) for joint speech dereverberation and blind RIR identification. In VINP, a probabilistic signal model is constructed in the time-frequency (T-F) domain based on convolution transfer function (CTF) approximation. For the first time, we propose using an arbitrary discriminative dereverberation deep neural network (DNN) to predict the prior distribution of anechoic speech within a probabilistic model. By integrating both reverberant speech and the anechoic speech prior, VINP yields the maximum a posteriori (MAP) and maximum likelihood (ML) estimations of the anechoic speech spectrum and CTF filter, respectively. After simple transformations, the waveforms of anechoic speech and RIR are estimated. Moreover, VINP is effective for automatic speech recognition (ASR) systems, which sets it apart from most deep learning (DL)-based single-channel dereverberation approaches. Experiments on single-channel speech dereverberation demonstrate that VINP reaches an advanced level in most metrics related to human perception and displays unquestionable state-of-the-art (SOTA) performance in ASR-related metrics. For blind RIR identification, experiments indicate that VINP attains the SOTA level in blind estimation of reverberation time at 60 dB (RT60) and direct-to-reverberation ratio (DRR). Codes and audio samples are available online. </p>
<blockquote>
<p>å¸¦æœ‰æ··å“çš„è¯­éŸ³ä¿¡å·åŒ…å«äº†å…³äºæ— æ··å“æºè¯­éŸ³å’Œæˆ¿é—´å†²å‡»å“åº”ï¼ˆRIRï¼‰çš„å…³é”®çŸ¥è¯†ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºç¥ç»è¯­éŸ³å…ˆéªŒï¼ˆVINPï¼‰çš„å˜è´å¶æ–¯æ¨æ–­ï¼ˆVBIï¼‰æ¡†æ¶ï¼Œç”¨äºè”åˆè¯­éŸ³å»æ··å“å’Œç›²RIRè¯†åˆ«ã€‚åœ¨VINPä¸­ï¼ŒåŸºäºå·ç§¯ä¼ é€’å‡½æ•°ï¼ˆCTFï¼‰è¿‘ä¼¼å€¼åœ¨æ—¶é¢‘ï¼ˆT-Fï¼‰åŸŸä¸­æ„å»ºæ¦‚ç‡ä¿¡å·æ¨¡å‹ã€‚æˆ‘ä»¬é¦–æ¬¡æå‡ºä½¿ç”¨ä»»æ„åˆ¤åˆ«å»æ··å“æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰åœ¨æ¦‚ç‡æ¨¡å‹ä¸­é¢„æµ‹æ— æ··å“è¯­éŸ³çš„å…ˆéªŒåˆ†å¸ƒã€‚é€šè¿‡æ•´åˆæ··å“è¯­éŸ³å’Œæ— æ··å“è¯­éŸ³å…ˆéªŒï¼ŒVINPå¾—åˆ°æ— æ··å“è¯­éŸ³è°±å’ŒCTFæ»¤æ³¢å™¨çš„æœ€å¤§åéªŒï¼ˆMAPï¼‰å’Œæœ€å¤§ä¼¼ç„¶ï¼ˆMLï¼‰ä¼°è®¡ã€‚ç»è¿‡ç®€å•å˜æ¢ï¼Œå¯ä»¥ä¼°ç®—å‡ºæ— æ··å“è¯­éŸ³å’ŒRIRçš„æ³¢å½¢ã€‚æ­¤å¤–ï¼ŒVINPå¯¹äºè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿä¹Ÿæœ‰æ•ˆï¼Œè¿™ä¸å¤§å¤šæ•°åŸºäºæ·±åº¦å­¦ä¹ çš„å•é€šé“å»æ··å“æ–¹æ³•ç›¸åŒºåˆ«ã€‚åœ¨å•é€šé“è¯­éŸ³å»æ··å“æ–¹é¢çš„å®éªŒè¡¨æ˜ï¼ŒVINPåœ¨å¤§å¤šæ•°ä¸äººç±»æ„ŸçŸ¥ç›¸å…³çš„æŒ‡æ ‡ä¸­è¾¾åˆ°äº†å…ˆè¿›æ°´å¹³ï¼Œå¹¶ä¸”åœ¨ä¸ASRç›¸å…³çš„æŒ‡æ ‡ä¸­è¡¨ç°å‡ºäº†æ— å¯äº‰è®®çš„æœ€å…ˆè¿›æ€§èƒ½ã€‚å¯¹äºç›²RIRè¯†åˆ«ï¼Œå®éªŒè¡¨æ˜ï¼ŒVINPåœ¨60åˆ†è´ï¼ˆRT60ï¼‰çš„æ··å“æ—¶é—´ç›²ä¼°è®¡å’Œç›´æ¥æ··å“æ¯”ï¼ˆDRRï¼‰æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›æ°´å¹³ã€‚ä»£ç å’ŒéŸ³é¢‘æ ·æœ¬å¯åœ¨ç½‘ä¸Šè·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.07205v2">PDF</a> Submitted to IEEE&#x2F;ACM Trans. on TASLP</p>
<p><strong>æ‘˜è¦</strong></p>
<p>åŸºäºç¥ç»è¯­éŸ³å…ˆéªŒçš„å˜åˆ†è´å¶æ–¯æ¨ç†æ¡†æ¶è¢«æå‡ºï¼Œç”¨äºè”åˆè¯­éŸ³å»æ··å“å’Œç›²æˆ¿é—´è„‰å†²å“åº”è¯†åˆ«ã€‚è¯¥æ¡†æ¶åœ¨æ—¶é¢‘åŸŸæ„å»ºäº†åŸºäºå·ç§¯ä¼ é€’å‡½æ•°è¿‘ä¼¼çš„æ¦‚ç‡ä¿¡å·æ¨¡å‹ã€‚æœ¬ç ”ç©¶é¦–æ¬¡æå‡ºä½¿ç”¨ä»»æ„åˆ¤åˆ«å»æ··å“æ·±åº¦ç¥ç»ç½‘ç»œæ¥é¢„æµ‹æ¦‚ç‡æ¨¡å‹ä¸­çš„æ— æ··å“è¯­éŸ³å…ˆéªŒåˆ†å¸ƒã€‚é€šè¿‡ç»“åˆæ··å“è¯­éŸ³å’Œæ— æ··å“è¯­éŸ³å…ˆéªŒï¼Œè¯¥æ¡†æ¶æä¾›äº†æ— æ··å“è¯­éŸ³è°±å’Œå·ç§¯ä¼ é€’å‡½æ•°æ»¤æ³¢å™¨çš„æœ€å¤§åéªŒå’Œæœ€å¤§ä¼¼ç„¶ä¼°è®¡ã€‚ç»è¿‡ç®€å•å˜æ¢ï¼Œå¯ä¼°ç®—å‡ºæ— æ··å“è¯­éŸ³å’Œæˆ¿é—´è„‰å†²å“åº”çš„æ³¢å½¢ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶å¯¹äºè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç³»ç»Ÿä¹Ÿæœ‰æ•ˆï¼Œä½¿å…¶æœ‰åˆ«äºå¤§å¤šæ•°åŸºäºæ·±åº¦å­¦ä¹ çš„å•é€šé“å»æ··å“æ–¹æ³•ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨äººç±»æ„ŸçŸ¥ç›¸å…³çš„å¤§å¤šæ•°æŒ‡æ ‡ä¸Šè¾¾åˆ°äº†å…ˆè¿›æ°´å¹³å’Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç›¸å…³æŒ‡æ ‡çš„æœ€æ–°æ€§èƒ½ã€‚å¯¹äºç›²æˆ¿é—´è„‰å†²å“åº”è¯†åˆ«ï¼Œå®éªŒè¡¨æ˜è¯¥æ¡†æ¶åœ¨ç›²ä¼°è®¡æ··å“æ—¶é—´ç›´æ¥æ¯”åœ¨ï¼ˆDRRï¼‰å’Œæˆ¿é—´è„‰å†²å“åº”çš„æ··å“æ—¶é—´ï¼ˆRT60ï¼‰æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚ç›¸å…³ä»£ç å’ŒéŸ³é¢‘æ ·æœ¬å¯åœ¨ç½‘ä¸Šè·å–ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æå‡ºçš„å˜åˆ†è´å¶æ–¯æ¨ç†æ¡†æ¶ç»“åˆäº†ç¥ç»è¯­éŸ³å…ˆéªŒï¼Œç”¨äºè”åˆè¯­éŸ³å»æ··å“å’Œç›²æˆ¿é—´è„‰å†²å“åº”è¯†åˆ«ã€‚</li>
<li>æ„å»ºäº†åŸºäºå·ç§¯ä¼ é€’å‡½æ•°è¿‘ä¼¼çš„æ¦‚ç‡ä¿¡å·æ¨¡å‹ï¼Œåœ¨æ—¶é¢‘åŸŸè¿›è¡Œåˆ†æã€‚</li>
<li>ä½¿ç”¨ä»»æ„åˆ¤åˆ«å»æ··å“æ·±åº¦ç¥ç»ç½‘ç»œé¢„æµ‹æ— æ··å“è¯­éŸ³çš„å…ˆéªŒåˆ†å¸ƒã€‚</li>
<li>è¯¥æ–¹æ³•æä¾›äº†æ— æ··å“è¯­éŸ³è°±å’Œå·ç§¯ä¼ é€’å‡½æ•°æ»¤æ³¢å™¨çš„æœ€å¤§åéªŒå’Œæœ€å¤§ä¼¼ç„¶ä¼°è®¡ã€‚</li>
<li>æ¡†æ¶å¯¹è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç³»ç»Ÿæœ‰æ•ˆï¼ŒåŒºåˆ«äºå¤§å¤šæ•°å•é€šé“å»æ··å“çš„æ·±åº¦å­¦ä¹ æ–¹æ³•ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨äººç±»æ„ŸçŸ¥ç›¸å…³æŒ‡æ ‡ä¸Šè¾¾åˆ°å…ˆè¿›æ°´å¹³ï¼Œä¸”åœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç›¸å…³æŒ‡æ ‡ä¸Šè¡¨ç°æœ€ä½³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.07205">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-6ffca12842c8f4a63c6f576fda4bc6f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea812fe21f4940fde73a93158c1d8760.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ab8d8083f73aecbf707ea2724ad84601.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-97d39fb86c8b2300c823bbe41181f3fc.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Llasa-Scaling-Train-Time-and-Inference-Time-Compute-for-Llama-based-Speech-Synthesis"><a href="#Llasa-Scaling-Train-Time-and-Inference-Time-Compute-for-Llama-based-Speech-Synthesis" class="headerlink" title="Llasa: Scaling Train-Time and Inference-Time Compute for Llama-based   Speech Synthesis"></a>Llasa: Scaling Train-Time and Inference-Time Compute for Llama-based   Speech Synthesis</h2><p><strong>Authors:Zhen Ye, Xinfa Zhu, Chi-Min Chan, Xinsheng Wang, Xu Tan, Jiahe Lei, Yi Peng, Haohe Liu, Yizhu Jin, Zheqi Dai, Hongzhan Lin, Jianyi Chen, Xingjian Du, Liumeng Xue, Yunlin Chen, Zhifei Li, Lei Xie, Qiuqiang Kong, Yike Guo, Wei Xue</strong></p>
<p>Recent advances in text-based large language models (LLMs), particularly in the GPT series and the o1 model, have demonstrated the effectiveness of scaling both training-time and inference-time compute. However, current state-of-the-art TTS systems leveraging LLMs are often multi-stage, requiring separate models (e.g., diffusion models after LLM), complicating the decision of whether to scale a particular model during training or testing. This work makes the following contributions: First, we explore the scaling of train-time and inference-time compute for speech synthesis. Second, we propose a simple framework Llasa for speech synthesis that employs a single-layer vector quantizer (VQ) codec and a single Transformer architecture to fully align with standard LLMs such as Llama. Our experiments reveal that scaling train-time compute for Llasa consistently improves the naturalness of synthesized speech and enables the generation of more complex and accurate prosody patterns. Furthermore, from the perspective of scaling inference-time compute, we employ speech understanding models as verifiers during the search, finding that scaling inference-time compute shifts the sampling modes toward the preferences of specific verifiers, thereby improving emotional expressiveness, timbre consistency, and content accuracy. In addition, we released the checkpoint and training code for our TTS model (1B, 3B, 8B) and codec model publicly available. </p>
<blockquote>
<p>æœ€è¿‘ï¼ŒåŸºäºæ–‡æœ¬çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•ï¼Œç‰¹åˆ«æ˜¯GPTç³»åˆ—å’ŒO1æ¨¡å‹ï¼Œè¯æ˜äº†åœ¨è®­ç»ƒæ—¶é—´å’Œæ¨ç†æ—¶é—´è®¡ç®—æ–¹é¢çš„å¯æ‰©å±•æ€§éå¸¸æœ‰æ•ˆã€‚ç„¶è€Œï¼Œå½“å‰åˆ©ç”¨LLMçš„æœ€å…ˆè¿›TTSç³»ç»Ÿé€šå¸¸æ˜¯å¤šé˜¶æ®µçš„ï¼Œéœ€è¦å•ç‹¬æ¨¡å‹ï¼ˆä¾‹å¦‚LLMä¹‹åçš„æ‰©æ•£æ¨¡å‹ï¼‰ï¼Œè¿™ä½¿å¾—åœ¨è®­ç»ƒæˆ–æµ‹è¯•æœŸé—´æ‰©å±•ç‰¹å®šæ¨¡å‹å˜å¾—æ›´åŠ å¤æ‚å†³ç­–ã€‚è¿™é¡¹å·¥ä½œåšå‡ºäº†ä»¥ä¸‹è´¡çŒ®ï¼šé¦–å…ˆï¼Œæˆ‘ä»¬æ¢ç´¢äº†è¯­éŸ³åˆæˆçš„è®­ç»ƒæ—¶é—´å’Œæ¨ç†æ—¶é—´è®¡ç®—çš„å¯æ‰©å±•æ€§ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºè¯­éŸ³åˆæˆçš„ç®€å•æ¡†æ¶Llasaï¼Œå®ƒé‡‡ç”¨å•å±‚å‘é‡é‡åŒ–ï¼ˆVQï¼‰ç¼–è§£ç å™¨å’Œå•ä¸€Transformeræ¶æ„ï¼Œä»¥å®Œå…¨ç¬¦åˆLlamaç­‰æ ‡å‡†LLMã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œæ‰©å¤§Llasaçš„è®­ç»ƒæ—¶é—´è®¡ç®—å§‹ç»ˆæé«˜äº†åˆæˆè¯­éŸ³çš„è‡ªç„¶åº¦ï¼Œå¹¶å¯ä»¥ç”Ÿæˆæ›´å¤æ‚å’Œå‡†ç¡®çš„å£°éŸ³æ¨¡å¼ã€‚æ­¤å¤–ï¼Œä»æ‰©å±•æ¨ç†æ—¶é—´è®¡ç®—çš„è§’åº¦æ¥çœ‹ï¼Œæˆ‘ä»¬åœ¨æœç´¢è¿‡ç¨‹ä¸­é‡‡ç”¨äº†è¯­éŸ³ç†è§£æ¨¡å‹ä½œä¸ºéªŒè¯å™¨ï¼Œå‘ç°æ‰©å¤§æ¨ç†æ—¶é—´è®¡ç®—ä½¿é‡‡æ ·æ¨¡å¼è½¬å‘ç‰¹å®šéªŒè¯å™¨çš„åå¥½ï¼Œä»è€Œæé«˜æƒ…æ„Ÿè¡¨ç°åŠ›ã€éŸ³è‰²ä¸€è‡´æ€§å’Œå†…å®¹å‡†ç¡®æ€§ã€‚å¦å¤–ï¼Œæˆ‘ä»¬å…¬å¼€å‘å¸ƒäº†TTSæ¨¡å‹ï¼ˆ1Bã€3Bã€8Bï¼‰å’Œç¼–è§£ç å™¨æ¨¡å‹çš„æ£€æŸ¥ç‚¹å’Œè®­ç»ƒä»£ç ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.04128v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åŸºäºæ–‡æœ¬çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è¯­éŸ³åˆæˆä¸­çš„è®­ç»ƒå’Œæ¨ç†æ—¶é—´è®¡ç®—è§„æ¨¡åŒ–çš„ç ”ç©¶ã€‚æå‡ºäº†ä¸€ç§åä¸ºLlasaçš„è¯­éŸ³åˆæˆæ¡†æ¶ï¼Œé‡‡ç”¨å•å±‚å‘é‡é‡åŒ–ç¼–ç å™¨å’Œå•ä¸€çš„Transformeræ¶æ„ï¼Œä¸æ ‡å‡†LLMså¦‚Llamaå®Œå…¨å¯¹é½ã€‚å®éªŒè¡¨æ˜ï¼Œå¢åŠ è®­ç»ƒæ—¶é—´è®¡ç®—è§„æ¨¡å¯ä»¥æé«˜åˆæˆè¯­éŸ³çš„è‡ªç„¶åº¦ï¼Œå¹¶ç”Ÿæˆæ›´å¤æ‚ã€æ›´å‡†ç¡®çš„è¯­è°ƒæ¨¡å¼ã€‚ä»æ¨ç†æ—¶é—´è®¡ç®—è§„æ¨¡çš„è§’åº¦æ¥çœ‹ï¼Œé‡‡ç”¨è¯­éŸ³ç†è§£æ¨¡å‹ä½œä¸ºéªŒè¯å™¨è¿›è¡Œæœç´¢ï¼Œå‘ç°å¢åŠ æ¨ç†æ—¶é—´è®¡ç®—è§„æ¨¡ä¼šä½¿é‡‡æ ·æ¨¡å¼è½¬å‘ç‰¹å®šéªŒè¯å™¨çš„åå¥½ï¼Œä»è€Œæé«˜æƒ…æ„Ÿè¡¨è¾¾ã€éŸ³è‰²ä¸€è‡´æ€§å’Œå†…å®¹å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬ä»‹ç»äº†LLMsåœ¨è¯­éŸ³åˆæˆä¸­çš„æœ€æ–°è¿›å±•ï¼Œç‰¹åˆ«æ˜¯GPTç³»åˆ—å’Œo1æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>å½“å‰å…ˆè¿›çš„TTSç³»ç»Ÿåˆ©ç”¨LLMsé€šå¸¸æ˜¯å¤šé˜¶æ®µçš„ï¼Œéœ€è¦å•ç‹¬æ¨¡å‹ï¼ˆå¦‚æ‰©æ•£æ¨¡å‹åçš„LLMï¼‰ï¼Œè¿™ä½¿å¾—åœ¨è®­ç»ƒå’Œæµ‹è¯•æœŸé—´é€‰æ‹©æ¨¡å‹è§„æ¨¡å˜å¾—å¤æ‚ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªåä¸ºLlasaçš„è¯­éŸ³åˆæˆæ¡†æ¶ï¼Œé‡‡ç”¨å•ä¸€å±‚å‘é‡é‡åŒ–ç¼–ç å™¨å’Œå•ä¸€çš„Transformeræ¶æ„ä¸æ ‡å‡†LLMså¯¹é½ã€‚</li>
<li>å¢åŠ è®­ç»ƒæ—¶é—´è®¡ç®—è§„æ¨¡å¯ä»¥æé«˜åˆæˆè¯­éŸ³çš„è‡ªç„¶åº¦å’Œç”Ÿæˆæ›´å¤æ‚ã€æ›´å‡†ç¡®çš„è¯­è°ƒæ¨¡å¼ã€‚</li>
<li>ä»æ¨ç†æ—¶é—´è®¡ç®—è§„æ¨¡çš„è§’åº¦æ¥çœ‹ï¼Œå¢åŠ è®¡ç®—è§„æ¨¡ä¼šä½¿é‡‡æ ·æ¨¡å¼æ›´ç¬¦åˆç‰¹å®šè¯­éŸ³ç†è§£æ¨¡å‹çš„åå¥½ï¼Œæé«˜æƒ…æ„Ÿè¡¨è¾¾ã€éŸ³è‰²ä¸€è‡´æ€§å’Œå†…å®¹å‡†ç¡®æ€§ã€‚</li>
<li>å…¬å¼€æä¾›äº†TTSæ¨¡å‹ï¼ˆ1Bã€3Bã€8Bï¼‰å’Œç¼–ç å™¨æ¨¡å‹çš„æ£€æŸ¥ç‚¹å’Œè®­ç»ƒä»£ç ã€‚</li>
<li>æ•´ä½“è€Œè¨€ï¼Œè¯¥ç ”ç©¶ä¸ºè¯­éŸ³åˆæˆçš„è®¡ç®—è§„æ¨¡åŒ–å’Œæ€§èƒ½ä¼˜åŒ–æä¾›äº†æ–°çš„è§è§£å’Œå·¥å…·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.04128">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-73e7aba508d71adf2faf33d7f539032e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8dade7875c113493333280e518621dc5.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="ToxiLab-How-Well-Do-Open-Source-LLMs-Generate-Synthetic-Toxicity-Data"><a href="#ToxiLab-How-Well-Do-Open-Source-LLMs-Generate-Synthetic-Toxicity-Data" class="headerlink" title="ToxiLab: How Well Do Open-Source LLMs Generate Synthetic Toxicity Data?"></a>ToxiLab: How Well Do Open-Source LLMs Generate Synthetic Toxicity Data?</h2><p><strong>Authors:Zheng Hui, Zhaoxiao Guo, Hang Zhao, Juanyong Duan, Lin Ai, Yinheng Li, Julia Hirschberg, Congrui Huang</strong></p>
<p>Effective toxic content detection relies heavily on high-quality and diverse data, which serve as the foundation for robust content moderation models. Synthetic data has become a common approach for training models across various NLP tasks. However, its effectiveness remains uncertain for highly subjective tasks like hate speech detection, with previous research yielding mixed results. This study explores the potential of open-source LLMs for harmful data synthesis, utilizing controlled prompting and supervised fine-tuning techniques to enhance data quality and diversity. We systematically evaluated 6 open source LLMs on 5 datasets, assessing their ability to generate diverse, high-quality harmful data while minimizing hallucination and duplication. Our results show that Mistral consistently outperforms other open models, and supervised fine-tuning significantly enhances data reliability and diversity. We further analyze the trade-offs between prompt-based vs. fine-tuned toxic data synthesis, discuss real-world deployment challenges, and highlight ethical considerations. Our findings demonstrate that fine-tuned open source LLMs provide scalable and cost-effective solutions to augment toxic content detection datasets, paving the way for more accessible and transparent content moderation tools. </p>
<blockquote>
<p>æœ‰æ•ˆçš„æœ‰æ¯’å†…å®¹æ£€æµ‹åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºé«˜è´¨é‡å’Œå¤šæ ·åŒ–çš„æ•°æ®ï¼Œè¿™äº›æ•°æ®ä¸ºæ„å»ºç¨³å¥çš„å†…å®¹ç®¡ç†æ¨¡å‹æä¾›äº†åŸºç¡€ã€‚åˆæˆæ•°æ®å·²æˆä¸ºå„ç§NLPä»»åŠ¡ä¸­è®­ç»ƒæ¨¡å‹çš„å¸¸è§æ–¹æ³•ã€‚ç„¶è€Œï¼Œå¯¹äºåƒä»‡æ¨è¨€è®ºæ£€æµ‹è¿™æ ·çš„é«˜åº¦ä¸»è§‚ä»»åŠ¡ï¼Œå…¶æœ‰æ•ˆæ€§å°šä¸ç¡®å®šï¼Œä¹‹å‰çš„ç ”ç©¶ç»“æœå–œå¿§å‚åŠã€‚æœ¬ç ”ç©¶æ¢è®¨äº†å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹åœ¨æœ‰å®³æ•°æ®åˆæˆæ–¹é¢çš„æ½œåŠ›ï¼Œåˆ©ç”¨å—æ§æç¤ºå’Œç›‘ç£å¾®è°ƒæŠ€æœ¯æ¥æé«˜æ•°æ®çš„è´¨é‡å’Œå¤šæ ·æ€§ã€‚æˆ‘ä»¬åœ¨äº”ä¸ªæ•°æ®é›†ä¸Šç³»ç»Ÿåœ°è¯„ä¼°äº†å…­ä¸ªå¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œè¯„ä¼°å®ƒä»¬ç”Ÿæˆå¤šæ ·åŒ–ã€é«˜è´¨é‡çš„æœ‰å®³æ•°æ®çš„èƒ½åŠ›ï¼ŒåŒæ—¶å°½é‡å‡å°‘å¹»è§‰å’Œé‡å¤ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒMistralå§‹ç»ˆä¼˜äºå…¶ä»–å¼€æ”¾æ¨¡å‹ï¼Œç›‘ç£å¾®è°ƒæ˜¾è‘—æé«˜äº†æ•°æ®çš„å¯é æ€§å’Œå¤šæ ·æ€§ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥åˆ†æäº†åŸºäºæç¤ºä¸å¾®è°ƒæœ‰æ¯’æ•°æ®åˆæˆçš„æƒè¡¡ï¼Œè®¨è®ºäº†ç°å®éƒ¨ç½²æŒ‘æˆ˜ï¼Œå¹¶å¼ºè°ƒäº†ä¼¦ç†è€ƒé‡ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç»è¿‡å¾®è°ƒåçš„å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ä¸ºå¢å¼ºæœ‰æ¯’å†…å®¹æ£€æµ‹æ•°æ®é›†æä¾›äº†å¯æ‰©å±•ä¸”ç»æµå®æƒ çš„è§£å†³æ–¹æ¡ˆï¼Œä¸ºæ›´å¯è®¿é—®å’Œé€æ˜çš„å†…å®¹ç®¡ç†å·¥å…·çš„é“ºè®¾é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.15175v4">PDF</a> 14 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åˆæˆæœ‰å®³æ•°æ®æ–¹é¢çš„æ½œåŠ›ï¼Œç ”ç©¶é€šè¿‡å—æ§æç¤ºå’Œç›‘ç£å¾®è°ƒæŠ€æœ¯æé«˜æ•°æ®è´¨é‡å’Œå¤šæ ·æ€§ã€‚å®éªŒè¯„ä¼°äº†6ä¸ªå¼€æºLLMsåœ¨5ä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°ï¼Œç»“æœæ˜¾ç¤ºMistralè¡¨ç°æœ€ä½³ï¼Œç›‘ç£å¾®è°ƒå¯æ˜¾è‘—æé«˜æ•°æ®å¯é æ€§å’Œå¤šæ ·æ€§ã€‚ç ”ç©¶è¿˜åˆ†æäº†æç¤ºä¸å¾®è°ƒä¹‹é—´çš„æƒè¡¡ï¼Œå¹¶è®¨è®ºäº†å®é™…éƒ¨ç½²çš„æŒ‘æˆ˜å’Œä¼¦ç†è€ƒé‡ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¾®è°ƒçš„å¼€æºLLMsä¸ºå¢å¼ºæœ‰æ¯’å†…å®¹æ£€æµ‹æ•°æ®é›†æä¾›äº†å¯æ‰©å±•ä¸”ç»æµå®æƒ çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é«˜è´¨é‡å’Œå¤šæ ·åŒ–çš„æ•°æ®æ˜¯å»ºç«‹ç¨³å¥å†…å®¹å®¡æ ¸æ¨¡å‹çš„åŸºç¡€ã€‚</li>
<li>åˆæˆæ•°æ®åœ¨NLPä»»åŠ¡ä¸­å¹¿æ³›ä½¿ç”¨ï¼Œä½†åœ¨ä¸»è§‚ä»»åŠ¡å¦‚ä»‡æ¨è¨€è®ºæ£€æµ‹ä¸­çš„æœ‰æ•ˆæ€§å°šä¸ç¡®å®šã€‚</li>
<li>å¼€æºLLMsåœ¨æœ‰å®³æ•°æ®åˆæˆæ–¹é¢å…·æœ‰æ½œåŠ›ã€‚</li>
<li>é€šè¿‡å—æ§æç¤ºå’Œç›‘ç£å¾®è°ƒæŠ€æœ¯å¯ä»¥æé«˜æ•°æ®è´¨é‡å’Œå¤šæ ·æ€§ã€‚</li>
<li>Mistralåœ¨è¯„ä¼°ä¸­è¡¨ç°æœ€ä½³ã€‚</li>
<li>ç›‘ç£å¾®è°ƒå¯æ˜¾è‘—æé«˜æ•°æ®å¯é æ€§å’Œå¤šæ ·æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.15175">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-886de0fd6fe0397114d365d0e7247f23.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-29f3956b38f04e633b8ff6646584aa6b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5d54a78ad900e6702b1afa6b8f97dce9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5a9013b41d9626c28ec9853f0c9552e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c09b3750e6e9e8f3c889381487706b1d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f7dc5efa7d9df0d3ca2232a7f0afa7eb.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Just-KIDDIN-Knowledge-Infusion-and-Distillation-for-Detection-of-INdecent-Memes"><a href="#Just-KIDDIN-Knowledge-Infusion-and-Distillation-for-Detection-of-INdecent-Memes" class="headerlink" title="Just KIDDIN: Knowledge Infusion and Distillation for Detection of   INdecent Memes"></a>Just KIDDIN: Knowledge Infusion and Distillation for Detection of   INdecent Memes</h2><p><strong>Authors:Rahul Garg, Trilok Padhi, Hemang Jain, Ugur Kursuncu, Ponnurangam Kumaraguru</strong></p>
<p>Toxicity identification in online multimodal environments remains a challenging task due to the complexity of contextual connections across modalities (e.g., textual and visual). In this paper, we propose a novel framework that integrates Knowledge Distillation (KD) from Large Visual Language Models (LVLMs) and knowledge infusion to enhance the performance of toxicity detection in hateful memes. Our approach extracts sub-knowledge graphs from ConceptNet, a large-scale commonsense Knowledge Graph (KG) to be infused within a compact VLM framework. The relational context between toxic phrases in captions and memes, as well as visual concepts in memes enhance the modelâ€™s reasoning capabilities. Experimental results from our study on two hate speech benchmark datasets demonstrate superior performance over the state-of-the-art baselines across AU-ROC, F1, and Recall with improvements of 1.1%, 7%, and 35%, respectively. Given the contextual complexity of the toxicity detection task, our approach showcases the significance of learning from both explicit (i.e. KG) as well as implicit (i.e. LVLMs) contextual cues incorporated through a hybrid neurosymbolic approach. This is crucial for real-world applications where accurate and scalable recognition of toxic content is critical for creating safer online environments. </p>
<blockquote>
<p>åœ¨å¤šæ¨¡æ€åœ¨çº¿ç¯å¢ƒä¸­è¿›è¡Œæ¯’æ€§è¯†åˆ«ä»ç„¶æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå› ä¸ºè·¨ä¸åŒæ¨¡æ€ï¼ˆä¾‹å¦‚æ–‡æœ¬å’Œè§†è§‰ï¼‰çš„ä¸Šä¸‹æ–‡è¿æ¥å¤æ‚æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†æ¥è‡ªå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„çŸ¥è¯†è’¸é¦ï¼ˆKDï¼‰å’ŒçŸ¥è¯†æ³¨å…¥ï¼Œä»¥æé«˜ä»‡æ¨æ€§memeä¸­çš„æ¯’æ€§æ£€æµ‹æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä»ConceptNetï¼ˆä¸€ä¸ªå¤§è§„æ¨¡å¸¸è¯†çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰ï¼‰ä¸­æå–å­çŸ¥è¯†å›¾è°±ï¼Œå¹¶å°†å…¶æ³¨å…¥ç´§å‡‘çš„VLMæ¡†æ¶ä¸­ã€‚æ ‡é¢˜å’Œmemeä¸­æ¯’æ€§çŸ­è¯­ä¹‹é—´çš„å…³ç³»ä¸Šä¸‹æ–‡ä»¥åŠmemeä¸­çš„è§†è§‰æ¦‚å¿µå¢å¼ºäº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªä»‡æ¨è¨€è®ºåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨AU-ROCã€F1å’Œå¬å›ç‡æ–¹é¢çš„æ€§èƒ½å‡ä¼˜äºæœ€æ–°åŸºçº¿ï¼Œåˆ†åˆ«æé«˜äº†1.1%ã€7%å’Œ35%ã€‚è€ƒè™‘åˆ°æ¯’æ€§æ£€æµ‹ä»»åŠ¡çš„ä¸Šä¸‹æ–‡å¤æ‚æ€§ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å±•ç¤ºäº†ä»æ˜¾å¼ï¼ˆå³KGï¼‰å’Œéšå¼ï¼ˆå³LVLMsï¼‰çš„ä¸Šä¸‹æ–‡çº¿ç´¢ä¸­å­¦ä¹ çš„é‡è¦æ€§ï¼Œè¿™äº›ä¸Šä¸‹æ–‡çº¿ç´¢é€šè¿‡æ··åˆç¥ç»ç¬¦å·æ–¹æ³•è¿›è¡Œé›†æˆã€‚è¿™å¯¹äºç°å®ä¸–ç•Œçš„åº”ç”¨è‡³å…³é‡è¦ï¼Œå› ä¸ºåœ¨å…¶ä¸­å‡†ç¡®ä¸”å¯æ‰©å±•åœ°è¯†åˆ«æœ‰æ¯’å†…å®¹å¯¹äºåˆ›å»ºæ›´å®‰å…¨çš„åœ¨çº¿ç¯å¢ƒè‡³å…³é‡è¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.12174v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§ç»“åˆçŸ¥è¯†è’¸é¦ï¼ˆKDï¼‰å’Œå¤§è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çŸ¥è¯†æ³¨å…¥çš„æ–°æ¡†æ¶ï¼Œç”¨äºå¢å¼ºåœ¨ä»‡æ¨è¨€è®ºæ£€æµ‹ä¸­çš„æœ‰æ¯’å†…å®¹è¯†åˆ«æ€§èƒ½ã€‚è¯¥æ¡†æ¶ä»ConceptNetä¸­æå–å­çŸ¥è¯†å›¾è°±ï¼Œæ³¨å…¥ç´§å‡‘çš„VLMæ¡†æ¶å†…ã€‚æœ‰æ¯’çŸ­è¯­ä¸æ ‡è¯­å’Œæ ‡è¯­ä¸­çš„è§†è§‰æ¦‚å¿µçš„å…³è”ä¸Šä¸‹æ–‡å¢å¼ºäº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„åŸºçº¿ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨AU-ROCã€F1å’Œå¬å›ç‡æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œåˆ†åˆ«æé«˜äº†1.1%ã€7%å’Œ35%ã€‚è¿™è¡¨æ˜ç»“åˆæ˜¾æ€§å’Œéšæ€§ä¸Šä¸‹æ–‡çº¿ç´¢ï¼Œé€šè¿‡æ··åˆç¥ç»ç¬¦å·æ–¹æ³•å­¦ä¹ çš„é‡è¦æ€§ã€‚å¯¹äºéœ€è¦å‡†ç¡®å’Œå¯æ‰©å±•åœ°è¯†åˆ«æœ‰æ¯’å†…å®¹çš„ç°å®ä¸–ç•Œåº”ç”¨è‡³å…³é‡è¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§ç»“åˆçŸ¥è¯†è’¸é¦å’ŒçŸ¥è¯†æ³¨å…¥çš„æ–°æ¡†æ¶ï¼Œç”¨äºå¢å¼ºæœ‰æ¯’å†…å®¹è¯†åˆ«çš„æ€§èƒ½ã€‚</li>
<li>åˆ©ç”¨ConceptNetæå–å­çŸ¥è¯†å›¾è°±ï¼Œå¹¶å°†å…¶æ³¨å…¥ç´§å‡‘çš„VLMæ¡†æ¶å†…ã€‚</li>
<li>æœ‰æ¯’çŸ­è¯­ä¸æ ‡è¯­å’Œè§†è§‰æ¦‚å¿µçš„å…³è”ä¸Šä¸‹æ–‡å¢å¼ºäº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨AU-ROCã€F1å’Œå¬å›ç‡æ–¹é¢ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
<li>ä¸æœ€å…ˆè¿›çš„åŸºçº¿ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•çš„æ€§èƒ½æå‡æ˜¾è‘—ï¼Œç‰¹åˆ«æ˜¯åœ¨å¬å›ç‡æ–¹é¢ã€‚</li>
<li>è¯¥æ–¹æ³•å¯¹äºç°å®ä¸–ç•Œåº”ç”¨è‡³å…³é‡è¦ï¼Œéœ€è¦å‡†ç¡®å’Œå¯æ‰©å±•åœ°è¯†åˆ«æœ‰æ¯’å†…å®¹ä»¥åˆ›å»ºæ›´å®‰å…¨çš„åœ¨çº¿ç¯å¢ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.12174">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e846b665a42f37f53f9f1fdac2438ee6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b9f4946f51a6b3d70efc809dc73a0b0e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-29a5c0cfe23d8163d02b71f6b5f665de.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Simultaneous-Diarization-and-Separation-of-Meetings-through-the-Integration-of-Statistical-Mixture-Models"><a href="#Simultaneous-Diarization-and-Separation-of-Meetings-through-the-Integration-of-Statistical-Mixture-Models" class="headerlink" title="Simultaneous Diarization and Separation of Meetings through the   Integration of Statistical Mixture Models"></a>Simultaneous Diarization and Separation of Meetings through the   Integration of Statistical Mixture Models</h2><p><strong>Authors:Tobias Cord-Landwehr, Christoph Boeddeker, Reinhold Haeb-Umbach</strong></p>
<p>We propose an approach for simultaneous diarization and separation of meeting data. It consists of a complex Angular Central Gaussian Mixture Model (cACGMM) for speech source separation, and a von-Mises-Fisher Mixture Model (VMFMM) for diarization in a joint statistical framework. Through the integration, both spatial and spectral information are exploited for diarization and separation. We also develop a method for counting the number of active speakers in a segment of a meeting to support block-wise processing. While the total number of speakers in a meeting may be known, it is usually not known on a per-segment level. With the proposed speaker counting, joint diarization and source separation can be done segment-by-segment, and the permutation problem across segments is solved, thus allowing for block-online processing in the future. Experimental results on the LibriCSS meeting corpus show that the integrated approach outperforms a cascaded approach of diarization and speech enhancement in terms of WER, both on a per-segment and on a per-meeting level. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŒæ—¶è¿›è¡Œä¼šè®®æ•°æ®åˆ†åŒ–å’Œåˆ†ç¦»çš„æ–¹æ³•ã€‚å®ƒåŒ…å«ä¸€ä¸ªå¤æ‚çš„è§’ä¸­å¿ƒé«˜æ–¯æ··åˆæ¨¡å‹ï¼ˆcACGMMï¼‰ç”¨äºè¯­éŸ³æºåˆ†ç¦»ï¼Œä»¥åŠä¸€ä¸ªå†¯Â·ç±³å¡æ–¯Â·è´¹èˆå°”æ··åˆæ¨¡å‹ï¼ˆVMFMMï¼‰ç”¨äºè”åˆç»Ÿè®¡æ¡†æ¶ä¸­çš„åˆ†åŒ–ã€‚é€šè¿‡æ•´åˆï¼Œåˆ©ç”¨ç©ºé—´å’Œé¢‘è°±ä¿¡æ¯è¿›è¡Œåˆ†åŒ–å’Œåˆ†ç¦»ã€‚æˆ‘ä»¬è¿˜å¼€å‘äº†ä¸€ç§æ–¹æ³•ï¼Œç”¨äºè®¡ç®—ä¼šè®®ç‰‡æ®µä¸­çš„æ´»åŠ¨å‘è¨€äººæ•°ï¼Œä»¥æ”¯æŒåˆ†å—å¤„ç†ã€‚è™½ç„¶ä¼šè®®çš„å‘è¨€äººæ•°æ€»æ•°å¯èƒ½æ˜¯å·²çŸ¥çš„ï¼Œä½†åœ¨æ¯ä¸€æ®µä¸­é€šå¸¸å¹¶ä¸çŸ¥é“ã€‚é€šè¿‡æå‡ºçš„å‘è¨€äººè®¡æ•°æ–¹æ³•ï¼Œå¯ä»¥åˆ†æ®µè¿›è¡Œè”åˆåˆ†åŒ–å’Œæºåˆ†ç¦»ï¼Œå¹¶è§£å†³è·¨æ®µçš„æ’åˆ—é—®é¢˜ï¼Œä»è€Œå®ç°åœ¨æœªæ¥çš„åˆ†å—åœ¨çº¿å¤„ç†ã€‚åœ¨LibriCSSä¼šè®®è¯­æ–™åº“ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥é›†æˆæ–¹æ³•ç›¸è¾ƒäºåœ¨åˆ†æ®µå’Œä¼šè®®çº§åˆ«ä¸Šçš„çº§è”åˆ†åŒ–ä¸è¯­éŸ³å¢å¼ºæ–¹æ³•åœ¨è¯é”™è¯¯ç‡ï¼ˆWERï¼‰æ–¹é¢è¡¨ç°æ›´ä¼˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.21455v2">PDF</a> Accepted at ICASSP2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŒæ—¶è¿›è¡Œä¼šè®®æ•°æ®åˆ†ç¦»ä¸æ—¥ç¨‹è®°å½•çš„æ–¹æ³•ã€‚å®ƒé‡‡ç”¨å¤æ‚çš„è§’ä¸­å¿ƒé«˜æ–¯æ··åˆæ¨¡å‹ï¼ˆcACGMMï¼‰è¿›è¡Œè¯­éŸ³æºåˆ†ç¦»ï¼Œå¹¶é‡‡ç”¨å†¯ç±³å¡æ–¯è´¹å¸Œå°”æ··åˆæ¨¡å‹ï¼ˆVMFMMï¼‰è¿›è¡Œè”åˆç»Ÿè®¡æ¡†æ¶ä¸­çš„æ—¥ç¨‹å®‰æ’ã€‚é€šè¿‡æ•´åˆï¼Œåˆ©ç”¨ç©ºé—´å’Œé¢‘è°±ä¿¡æ¯è¿›è¡Œæ—¥ç¨‹å®‰æ’å’Œåˆ†ç¦»ã€‚æ­¤å¤–ï¼Œè¿˜å¼€å‘äº†ä¸€ç§æ–¹æ³•ç”¨äºè®¡ç®—ä¼šè®®ç‰‡æ®µä¸­çš„æ´»åŠ¨è¯´è¯äººæ•°ï¼Œä»¥æ”¯æŒåˆ†å—å¤„ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨LibriCSSä¼šè®®è¯­æ–™åº“ä¸Šçš„è¡¨ç°ä¼˜äºçº§è”çš„æ—¥ç¨‹å®‰æ’å’Œè¯­éŸ³å¢å¼ºæ–¹æ³•ï¼Œæ— è®ºæ˜¯åœ¨æ¯æ®µè¿˜æ˜¯æ•´ä¸ªä¼šè®®å±‚é¢ï¼Œè¯é”™è¯¯ç‡ï¼ˆWERï¼‰éƒ½æœ‰æ‰€é™ä½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†åŒæ—¶å®ç°ä¼šè®®æ•°æ®åˆ†ç¦»ä¸æ—¥ç¨‹è®°å½•çš„æ–¹æ³•ã€‚</li>
<li>é‡‡ç”¨äº†å¤æ‚çš„è§’ä¸­å¿ƒé«˜æ–¯æ··åˆæ¨¡å‹ï¼ˆcACGMMï¼‰è¿›è¡Œè¯­éŸ³æºåˆ†ç¦»ã€‚</li>
<li>é‡‡ç”¨äº†å†¯ç±³å¡æ–¯è´¹å¸Œå°”æ··åˆæ¨¡å‹ï¼ˆVMFMMï¼‰è¿›è¡Œæ—¥ç¨‹å®‰æ’ã€‚</li>
<li>é€šè¿‡æ•´åˆï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåˆ©ç”¨ç©ºé—´å’Œé¢‘è°±ä¿¡æ¯ã€‚</li>
<li>å¼€å‘äº†ä¸€ç§è®¡ç®—ä¼šè®®ç‰‡æ®µä¸­æ´»åŠ¨è¯´è¯äººæ•°çš„æ–¹æ³•ï¼Œæ”¯æŒåˆ†å—å¤„ç†ã€‚</li>
<li>è¯¥æ–¹æ³•è§£å†³äº†è·¨ç‰‡æ®µçš„æ’åˆ—é—®é¢˜ï¼Œä¸ºæœªæ¥åˆ†å—åœ¨çº¿å¤„ç†æä¾›äº†å¯èƒ½ã€‚</li>
<li>åœ¨LibriCSSä¼šè®®è¯­æ–™åº“ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºçº§è”çš„æ—¥ç¨‹å®‰æ’å’Œè¯­éŸ³å¢å¼ºæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.21455">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6cb0b9cfbb25d02178586ca568214589.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f8e0317e0b2eb29c04adceb6088ded5f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7fedddccf8f8fc66cf55e9197a875a41.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Three-in-One-Fast-and-Accurate-Transducer-for-Hybrid-Autoregressive-ASR"><a href="#Three-in-One-Fast-and-Accurate-Transducer-for-Hybrid-Autoregressive-ASR" class="headerlink" title="Three-in-One: Fast and Accurate Transducer for Hybrid-Autoregressive ASR"></a>Three-in-One: Fast and Accurate Transducer for Hybrid-Autoregressive ASR</h2><p><strong>Authors:Hainan Xu, Travis M. Bartley, Vladimir Bataev, Boris Ginsburg</strong></p>
<p>We present Hybrid-Autoregressive INference TrANsducers (HAINAN), a novel architecture for speech recognition that extends the Token-and-Duration Transducer (TDT) model. Trained with randomly masked predictor network outputs, HAINAN supports both autoregressive inference with all network components and non-autoregressive inference without the predictor. Additionally, we propose a novel semi-autoregressive inference paradigm that first generates an initial hypothesis using non-autoregressive inference, followed by refinement steps where each token prediction is regenerated using parallelized autoregression on the initial hypothesis. Experiments on multiple datasets across different languages demonstrate that HAINAN achieves efficiency parity with CTC in non-autoregressive mode and with TDT in autoregressive mode. In terms of accuracy, autoregressive HAINAN outperforms TDT and RNN-T, while non-autoregressive HAINAN significantly outperforms CTC. Semi-autoregressive inference further enhances the modelâ€™s accuracy with minimal computational overhead, and even outperforms TDT results in some cases. These results highlight HAINANâ€™s flexibility in balancing accuracy and speed, positioning it as a strong candidate for real-world speech recognition applications. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†æ··åˆè‡ªå›å½’æ¨ç†è½¬æ¢å™¨ï¼ˆHybrid-Autoregressive INference TrANsducersï¼Œç®€ç§°HAINANï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºè¯­éŸ³è¯†åˆ«çš„æ–°å‹æ¶æ„ï¼Œå®ƒæ‰©å±•äº†Token-and-Duration Transducerï¼ˆTDTï¼‰æ¨¡å‹ã€‚é€šè¿‡è®­ç»ƒå¸¦æœ‰éšæœºæ©ç çš„é¢„æµ‹å™¨ç½‘ç»œè¾“å‡ºï¼ŒHAINANæ”¯æŒåŒ…å«æ‰€æœ‰ç½‘ç»œç»„ä»¶çš„è‡ªå›å½’æ¨ç†å’Œéè‡ªå›å½’æ¨ç†ï¼ˆæ— éœ€é¢„æµ‹å™¨ï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ–°å‹åŠè‡ªå›å½’æ¨ç†èŒƒå¼ï¼Œé¦–å…ˆä½¿ç”¨éè‡ªå›å½’æ¨ç†ç”Ÿæˆåˆå§‹å‡è®¾ï¼Œç„¶åé€šè¿‡å¹¶è¡Œè‡ªå›å½’å¯¹åˆå§‹å‡è®¾è¿›è¡Œç²¾ç»†åŒ–å¤„ç†ï¼Œé‡æ–°ç”Ÿæˆæ¯ä¸ªç¬¦å·çš„é¢„æµ‹ã€‚åœ¨å¤šè¯­è¨€æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒHAINANåœ¨éè‡ªå›å½’æ¨¡å¼ä¸‹ä¸CTCçš„æ•ˆç‡ç›¸å½“ï¼Œåœ¨è‡ªå›å½’æ¨¡å¼ä¸‹ä¸TDTçš„æ•ˆç‡ç›¸å½“ã€‚åœ¨å‡†ç¡®æ€§æ–¹é¢ï¼Œè‡ªå›å½’çš„HAINANä¼˜äºTDTå’ŒRNN-Tï¼Œè€Œéè‡ªå›å½’çš„HAINANåˆ™æ˜¾è‘—ä¼˜äºCTCã€‚åŠè‡ªå›å½’æ¨ç†è¿›ä¸€æ­¥æé«˜äº†æ¨¡å‹çš„å‡†ç¡®æ€§ï¼Œä¸”è®¡ç®—å¼€é”€æœ€å°ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ç”šè‡³è¶…è¿‡äº†TDTçš„ç»“æœã€‚è¿™äº›ç»“æœå‡¸æ˜¾äº†HAINANåœ¨å¹³è¡¡å‡†ç¡®æ€§å’Œé€Ÿåº¦æ–¹é¢çš„çµæ´»æ€§ï¼Œä½¿å…¶æˆä¸ºç°å®ä¸–ç•Œä¸­è¯­éŸ³è¯†åˆ«åº”ç”¨çš„æœ‰åŠ›å€™é€‰è€…ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.02597v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Hybrid-Autoregressive INference TrANsducersï¼ˆHAINANï¼‰è¿™ä¸€æ–°å‹è¯­éŸ³è¯†åˆ«æ¶æ„ã€‚HAINANæ‰©å±•äº†Token-and-Duration Transducerï¼ˆTDTï¼‰æ¨¡å‹ï¼Œé€šè¿‡éšæœºå±è”½é¢„æµ‹ç½‘ç»œè¾“å‡ºæ¥è¿›è¡Œè®­ç»ƒã€‚å®ƒæ”¯æŒè‡ªå›å½’æ¨ç†å’Œéè‡ªå›å½’æ¨ç†ï¼Œå¹¶æå‡ºäº†åŠè‡ªå›å½’æ¨ç†æ–°èŒƒå¼ã€‚å®éªŒè¡¨æ˜ï¼ŒHAINANåœ¨æ•ˆç‡å’Œå‡†ç¡®æ€§æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå…·æœ‰åœ¨å¹³è¡¡å‡†ç¡®ç‡å’Œé€Ÿåº¦æ–¹é¢çš„çµæ´»æ€§ï¼Œé€‚ç”¨äºçœŸå®ä¸–ç•Œçš„è¯­éŸ³è¯†åˆ«åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>HAINANæ˜¯ä¸€ç§æ–°å‹çš„è¯­éŸ³è¯†åˆ«æ¶æ„ï¼ŒåŸºäºToken-and-Duration Transducerï¼ˆTDTï¼‰æ¨¡å‹è¿›è¡Œæ‰©å±•ã€‚</li>
<li>HAINANé€šè¿‡éšæœºå±è”½é¢„æµ‹ç½‘ç»œè¾“å‡ºæ¥è¿›è¡Œè®­ç»ƒã€‚</li>
<li>HAINANæ”¯æŒè‡ªå›å½’å’Œéè‡ªå›å½’ä¸¤ç§æ¨ç†æ¨¡å¼ã€‚</li>
<li>æå‡ºäº†åŠè‡ªå›å½’æ¨ç†æ–°èŒƒå¼ï¼Œé¦–å…ˆä½¿ç”¨éè‡ªå›å½’æ¨ç†ç”Ÿæˆåˆæ­¥å‡è®¾ï¼Œç„¶åè¿›è¡ŒåŸºäºå¹¶è¡Œè‡ªå›å½’çš„ç²¾ç»†åŒ–é¢„æµ‹ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒHAINANåœ¨æ•ˆç‡å’Œå‡†ç¡®æ€§æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</li>
<li>è‡ªå›å½’HAINANåœ¨å‡†ç¡®æ€§æ–¹é¢ä¼˜äºTDTå’ŒRNN-Tï¼Œéè‡ªå›å½’HAINANåˆ™æ˜¾è‘—ä¼˜äºCTCã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.02597">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-49766e8bf898c7aae614fc914b6de07e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-577fbd2ac9729abdec8417d49706cd43.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0eba685dcfd49c0ff99d4c900c71cefa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3bbcf0b3470dc249090ed21b92d400f1.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Vibravox-A-Dataset-of-French-Speech-Captured-with-Body-conduction-Audio-Sensors"><a href="#Vibravox-A-Dataset-of-French-Speech-Captured-with-Body-conduction-Audio-Sensors" class="headerlink" title="Vibravox: A Dataset of French Speech Captured with Body-conduction Audio   Sensors"></a>Vibravox: A Dataset of French Speech Captured with Body-conduction Audio   Sensors</h2><p><strong>Authors:Julien Hauret, Malo Olivier, Thomas Joubaud, Christophe Langrenne, Sarah PoirÃ©e, VÃ©ronique Zimpfer, Ã‰ric Bavu</strong></p>
<p>Vibravox is a dataset compliant with the General Data Protection Regulation (GDPR) containing audio recordings using five different body-conduction audio sensors : two in-ear microphones, two bone conduction vibration pickups and a laryngophone. The dataset also includes audio data from an airborne microphone used as a reference. The Vibravox corpus contains 45 hours of speech samples and physiological sounds recorded by 188 participants under different acoustic conditions imposed by an high order ambisonics 3D spatializer. Annotations about the recording conditions and linguistic transcriptions are also included in the corpus. We conducted a series of experiments on various speech-related tasks, including speech recognition, speech enhancement and speaker verification. These experiments were carried out using state-of-the-art models to evaluate and compare their performances on signals captured by the different audio sensors offered by the Vibravox dataset, with the aim of gaining a better grasp of their individual characteristics. </p>
<blockquote>
<p>Vibravoxæ˜¯ä¸€ä¸ªç¬¦åˆé€šç”¨æ•°æ®ä¿æŠ¤æ¡ä¾‹ï¼ˆGDPRï¼‰è¦æ±‚çš„æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«ä½¿ç”¨äº”ç§ä¸åŒçš„èº«ä½“ä¼ å¯¼éŸ³é¢‘ä¼ æ„Ÿå™¨å½•åˆ¶çš„éŸ³é¢‘è®°å½•ï¼šä¸¤ä¸ªå…¥è€³å¼éº¦å…‹é£ï¼Œä¸¤ä¸ªéª¨ä¼ å¯¼æŒ¯åŠ¨æ‹¾éŸ³å™¨å’Œä¸€ä¸ªå–‰å¤´éº¦å…‹é£ã€‚è¯¥æ•°æ®é›†è¿˜åŒ…æ‹¬ä½œä¸ºå‚è€ƒçš„ç©ºæ°”ä¼ æ’­éº¦å…‹é£çš„éŸ³é¢‘æ•°æ®ã€‚Vibravoxè¯­æ–™åº“åŒ…å«ç”±188åå‚ä¸è€…åœ¨ç”±é«˜çº§ä¸‰ç»´ç©ºé—´å®šä½å™¨æ–½åŠ çš„ä¸åŒå£°å­¦æ¡ä»¶ä¸‹å½•åˆ¶çš„45å°æ—¶è¯­éŸ³æ ·æœ¬å’Œç”Ÿç†å£°éŸ³ã€‚è¯­æ–™åº“è¿˜åŒ…æ‹¬å…³äºå½•éŸ³æ¡ä»¶çš„æ³¨é‡Šå’Œè¯­è¨€è½¬å½•ã€‚æˆ‘ä»¬åœ¨å„ç§è¯­éŸ³ç›¸å…³ä»»åŠ¡ä¸Šè¿›è¡Œäº†ä¸€ç³»åˆ—å®éªŒï¼ŒåŒ…æ‹¬è¯­éŸ³è¯†åˆ«ã€è¯­éŸ³å¢å¼ºå’Œè¯´è¯äººéªŒè¯ã€‚è¿™äº›å®éªŒä½¿ç”¨æœ€å‰æ²¿çš„æ¨¡å‹è¿›è¡Œï¼Œæ—¨åœ¨è¯„ä¼°å¹¶æ¯”è¾ƒè¿™äº›æ¨¡å‹åœ¨Vibravoxæ•°æ®é›†æä¾›çš„ä¸åŒéŸ³é¢‘ä¼ æ„Ÿå™¨æ•è·çš„ä¿¡å·ä¸Šçš„æ€§èƒ½ï¼Œä»è€Œæ›´å¥½åœ°äº†è§£å®ƒä»¬å„è‡ªçš„ç‰¹æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.11828v3">PDF</a> 23 pages, 42 figures</p>
<p><strong>æ€»ç»“</strong></p>
<p>Vibravoxæ˜¯ä¸€ä¸ªç¬¦åˆé€šç”¨æ•°æ®ä¿æŠ¤æ¡ä¾‹ï¼ˆGDPRï¼‰è§„å®šçš„éŸ³é¢‘æ•°æ®é›†ï¼ŒåŒ…å«ä½¿ç”¨äº”ç§ä¸åŒçš„èº«ä½“ä¼ å¯¼éŸ³é¢‘ä¼ æ„Ÿå™¨å½•åˆ¶çš„éŸ³é¢‘è®°å½•ï¼Œå¦‚ä¸¤ä¸ªå…¥è€³å¼éº¦å…‹é£ã€ä¸¤ä¸ªéª¨ä¼ å¯¼æŒ¯åŠ¨æ‹¾éŸ³å™¨å’Œä¸€ä¸ªå–‰å¤´ç”µè¯ã€‚è¯¥æ•°æ®é›†è¿˜åŒ…æ‹¬ä½œä¸ºå‚è€ƒçš„ç©ºæ°”ä¼ æ’­éº¦å…‹é£çš„éŸ³é¢‘æ•°æ®ã€‚Vibravoxè¯­æ–™åº“åŒ…å«ç”±188åå‚ä¸è€…åœ¨ç”±é«˜çº§ä¸‰ç»´ç©ºé—´åŒ–æŠ€æœ¯è®¾å®šçš„é«˜é˜¶ç¯å¢ƒå™ªå£°æ¡ä»¶ä¸‹å½•åˆ¶çš„45å°æ—¶è¯­éŸ³æ ·æœ¬å’Œç”Ÿç†å£°éŸ³ã€‚è¯­æ–™åº“è¿˜åŒ…æ‹¬å…³äºå½•éŸ³æ¡ä»¶å’Œè¯­è¨€è½¬å½•çš„æ³¨é‡Šã€‚æˆ‘ä»¬è¿›è¡Œäº†ä¸€ç³»åˆ—å…³äºå„ç§è¯­éŸ³ç›¸å…³ä»»åŠ¡çš„å®éªŒï¼ŒåŒ…æ‹¬è¯­éŸ³è¯†åˆ«ã€è¯­éŸ³å¢å¼ºå’Œè¯´è¯äººéªŒè¯ï¼Œä»¥è¯„ä¼°ä¸åŒéŸ³é¢‘ä¼ æ„Ÿå™¨åœ¨Vibravoxæ•°æ®é›†ä¸Šçš„æ€§èƒ½å¹¶æ¯”è¾ƒå…¶æ€§èƒ½ï¼Œç›®çš„æ˜¯ä¸ºäº†æ›´å¥½åœ°äº†è§£å®ƒä»¬å„è‡ªçš„ç‰¹æ€§ã€‚</p>
<p><strong>è¦ç‚¹åˆ†æ</strong></p>
<ol>
<li>Vibravoxæ˜¯ä¸€ä¸ªç¬¦åˆGDPRè§„å®šçš„éŸ³é¢‘æ•°æ®é›†ã€‚</li>
<li>æ•°æ®é›†ä½¿ç”¨äº†äº”ç§ä¸åŒçš„èº«ä½“ä¼ å¯¼éŸ³é¢‘ä¼ æ„Ÿå™¨è¿›è¡ŒéŸ³é¢‘å½•åˆ¶ã€‚</li>
<li>æ•°æ®é›†åŒ…å«45å°æ—¶çš„è¯­éŸ³æ ·æœ¬å’Œç”Ÿç†å£°éŸ³è®°å½•ã€‚</li>
<li>å½•éŸ³æ¥è‡ª188åå‚ä¸è€…ï¼Œåœ¨ä¸åŒçš„å£°å­¦æ¡ä»¶ä¸‹è¿›è¡Œã€‚</li>
<li>è¯­æ–™åº“åŒ…å«å…³äºå½•éŸ³æ¡ä»¶å’Œè¯­è¨€è½¬å½•çš„æ³¨é‡Šã€‚</li>
<li>è¿›è¡Œäº†ä¸€ç³»åˆ—è¯­éŸ³ç›¸å…³ä»»åŠ¡çš„å®éªŒï¼ŒåŒ…æ‹¬è¯­éŸ³è¯†åˆ«ã€è¯­éŸ³å¢å¼ºå’Œè¯´è¯äººéªŒè¯ã€‚</li>
<li>å®éªŒçš„ç›®çš„æ˜¯è¯„ä¼°å¹¶æ¯”è¾ƒä¸åŒéŸ³é¢‘ä¼ æ„Ÿå™¨çš„æ€§èƒ½ï¼Œä»¥äº†è§£å®ƒä»¬å„è‡ªçš„ç‰¹æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.11828">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3467608a99567db4b0a76b150150dd36.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f2a5adc05deb023863067537bbfb508d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e2c6e59cffb097de38fe4caa257ffe50.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7e7c5f6042c48e2396be947bd23eb48c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7585a935a3876282d7624769dbb8865c.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-26/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-26/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-26/GAN/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e286f6622218051d9f34b42aa6c0cfff.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-26  Diffusion Models for Tabular Data Challenges, Current Progress, and   Future Directions
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-26
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-26/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-69a3d0e8c8b00a7adcab939af91e6966.jpg" class="responsive-img" alt="æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ ">
                        
                        <span class="card-title">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹  æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-26  Multi-view Hypergraph-based Contrastive Learning Model for Cold-Start   Micro-video Recommendation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-26
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ 
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">18799.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
