<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech 方向最新论文已更新，请持续关注 Update in 2025-02-26  Low-Rank and Sparse Model Merging for Multi-Lingual Speech Recognition   and Translation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-07220ade929d30e1efc8ef08be33fa8e.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-02-26
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    19.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    78 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-02-26-更新"><a href="#2025-02-26-更新" class="headerlink" title="2025-02-26 更新"></a>2025-02-26 更新</h1><h2 id="Low-Rank-and-Sparse-Model-Merging-for-Multi-Lingual-Speech-Recognition-and-Translation"><a href="#Low-Rank-and-Sparse-Model-Merging-for-Multi-Lingual-Speech-Recognition-and-Translation" class="headerlink" title="Low-Rank and Sparse Model Merging for Multi-Lingual Speech Recognition   and Translation"></a>Low-Rank and Sparse Model Merging for Multi-Lingual Speech Recognition   and Translation</h2><p><strong>Authors:Qiuming Zhao, Guangzhi Sun, Chao Zhang, Mingxing Xu, Thomas Fang Zheng</strong></p>
<p>Language diversity presents a significant challenge in speech-to-text (S2T) tasks, such as automatic speech recognition and translation. Traditional multi-task training approaches aim to address this by jointly optimizing multiple speech recognition and translation tasks across various languages. While models like Whisper, built on these strategies, demonstrate strong performance, they still face issues of high computational cost, language interference, suboptimal training configurations, and limited extensibility. To overcome these challenges, we introduce LoRS-Merging (low-rank and sparse model merging), a novel technique designed to efficiently integrate models trained on different languages or tasks while preserving performance and reducing computational overhead. LoRS-Merging combines low-rank and sparse pruning to retain essential structures while eliminating redundant parameters, mitigating language and task interference, and enhancing extensibility. Experimental results across a range of languages demonstrate that LoRS-Merging significantly outperforms conventional multi-lingual multi-task training baselines. Our findings suggest that model merging, particularly LoRS-Merging, is a scalable and effective complement to traditional multi-lingual training strategies for S2T applications. </p>
<blockquote>
<p>语言多样性在语音识别到文本（S2T）的任务中，如自动语音识别和翻译，构成了一大挑战。传统多任务训练的方法旨在通过联合优化多种语言的语音识别和翻译任务来解决这一问题。虽然基于这些策略的模型，如whisper，表现出了强大的性能，但它们仍然面临计算成本高、语言干扰、训练配置不佳和扩展性有限的诸多问题。为了克服这些挑战，我们引入了LoRS-Merging（低秩和稀疏模型合并），这是一种旨在高效集成在不同语言或任务上训练的模型的新技术，同时保留性能并降低计算开销。LoRS-Merging结合了低秩和稀疏剪枝，以保留关键结构并消除冗余参数，减轻语言和任务干扰，提高扩展性。跨多种语言的实验结果表明，LoRS-Merging显著优于传统多语种多任务训练基线。我们的研究结果表明，模型合并，尤其是LoRS-Merging，是S2T应用中传统多语种训练策略的可扩展和有效的补充。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17380v1">PDF</a> 13 pages, submitted to ACL 2025</p>
<p><strong>Summary</strong></p>
<p>语言多样性在语音转文本（S2T）任务中是一大挑战，如自动语音识别和翻译。传统多任务训练旨在通过联合优化多种语言和翻译任务来解决这一问题。虽然基于这些策略构建的模型如Whisper表现出强大的性能，但它们仍面临计算成本高、语言干扰、训练配置不佳和扩展性有限等问题。为了克服这些挑战，我们引入了LoRS-Merging（低秩和稀疏模型合并），这是一种设计用于高效集成不同语言或任务训练模型的新技术，同时保持性能并降低计算开销。LoRS-Merging结合低秩和稀疏剪枝，保留关键结构，消除冗余参数，减轻语言和任务干扰，提高扩展性。跨多种语言的实验结果表明，LoRS-Merging显著优于传统多语言多任务训练基线。我们的研究结果表明，模型合并，尤其是LoRS-Merging，是S2T应用中传统多语言训练策略的可扩展和有效的补充。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语言多样性在语音转文本任务中构成重大挑战。</li>
<li>传统多任务训练策略旨在通过联合优化多种语言和翻译任务来解决这一挑战。</li>
<li>虽然现有模型如Whisper性能强大，但仍存在计算成本高、语言干扰和扩展性有限等问题。</li>
<li>LoRS-Merging技术旨在通过高效集成不同语言或任务的训练模型来克服这些挑战。</li>
<li>LoRS-Merging结合低秩和稀疏剪枝，以保留关键结构并消除冗余参数。</li>
<li>LoRS-Merging能减轻语言和任务间的干扰，提高模型的扩展性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17380">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-a951e7f6c65b72a31f20eaaec8ae202f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-14c7cb6d6488c9352a7f842e385f3f21.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3ac6825b1d9c0026c1f7b41a51bbe40.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Baichuan-Audio-A-Unified-Framework-for-End-to-End-Speech-Interaction"><a href="#Baichuan-Audio-A-Unified-Framework-for-End-to-End-Speech-Interaction" class="headerlink" title="Baichuan-Audio: A Unified Framework for End-to-End Speech Interaction"></a>Baichuan-Audio: A Unified Framework for End-to-End Speech Interaction</h2><p><strong>Authors:Tianpeng Li, Jun Liu, Tao Zhang, Yuanbo Fang, Da Pan, Mingrui Wang, Zheng Liang, Zehuan Li, Mingan Lin, Guosheng Dong, Jianhua Xu, Haoze Sun, Zenan Zhou, Weipeng Chen</strong></p>
<p>We introduce Baichuan-Audio, an end-to-end audio large language model that seamlessly integrates audio understanding and generation. It features a text-guided aligned speech generation mechanism, enabling real-time speech interaction with both comprehension and generation capabilities. Baichuan-Audio leverages a pre-trained ASR model, followed by multi-codebook discretization of speech at a frame rate of 12.5 Hz. This multi-codebook setup ensures that speech tokens retain both semantic and acoustic information. To further enhance modeling, an independent audio head is employed to process audio tokens, effectively capturing their unique characteristics. To mitigate the loss of intelligence during pre-training and preserve the original capabilities of the LLM, we propose a two-stage pre-training strategy that maintains language understanding while enhancing audio modeling. Following alignment, the model excels in real-time speech-based conversation and exhibits outstanding question-answering capabilities, demonstrating its versatility and efficiency. The proposed model demonstrates superior performance in real-time spoken dialogue and exhibits strong question-answering abilities. Our code, model and training data are available at <a target="_blank" rel="noopener" href="https://github.com/baichuan-inc/Baichuan-Audio">https://github.com/baichuan-inc/Baichuan-Audio</a> </p>
<blockquote>
<p>我们推出百川音频（Baichuan-Audio）——端到端的音频大语言模型，它无缝集成了音频理解和生成。该模型具备文本引导的对齐语音生成机制，能够实现具有理解和生成能力的实时语音交互。百川音频利用预训练的语音识别模型，随后以12.5Hz的帧率进行多代码簿离散化语音。这种多代码簿设置确保语音标记保留语义和声音信息。为了进一步提高建模效果，采用独立的音频头来处理音频标记，有效地捕捉其独特特征。为了减轻预训练过程中的智力损失并保留大语言模型的原始能力，我们提出了两阶段预训练策略，既保持语言理解又增强音频建模。对齐后，该模型在实时语音对话中表现出色，具有出色的问答能力，体现了其通用性和效率。所提出的模型在实时口语对话中表现出卓越性能，展现出强大的问答能力。我们的代码、模型和训练数据可在 <a target="_blank" rel="noopener" href="https://github.com/baichuan-inc/Baichuan-Audio">https://github.com/baichuan-inc/Baichuan-Audio</a> 找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17239v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>巴川音频模型是一个端到端的音频大语言模型，融合了音频理解和生成能力，实现实时语音交互。模型具备文本指导对齐的语音生成机制和多码本离散化技术，能有效捕捉语音的语义和声音信息。采用独立音频头处理音频标记，并采用两阶段预训练策略增强建模效果并维护语言理解功能。模型具有优秀的实时对话性能，展现了强大的问答能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>巴川音频模型是一个端到端的音频大语言模型，融合了音频理解和生成能力。</li>
<li>模型具备文本指导对齐的语音生成机制，实现实时语音交互。</li>
<li>多码本离散化技术确保语音标记捕捉语义和声音信息。</li>
<li>独立音频头处理音频标记，提升模型性能。</li>
<li>采用两阶段预训练策略增强建模效果，同时维护语言理解功能。</li>
<li>模型在实时对话中表现出优秀的性能，具备强大的问答能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17239">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-4e65f1fb528ef368b90aa438759522d3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fb90520650bcb0fa719fc644ddec4d2e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-094f0a0bedcf5742f279254e52431328.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8658fe40d30f7b1f7ff1b7d64af37eea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07220ade929d30e1efc8ef08be33fa8e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ad397b9225b4b44275d63854a6c0184a.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="NatSGLD-A-Dataset-with-Speech-Gesture-Logic-and-Demonstration-for-Robot-Learning-in-Natural-Human-Robot-Interaction"><a href="#NatSGLD-A-Dataset-with-Speech-Gesture-Logic-and-Demonstration-for-Robot-Learning-in-Natural-Human-Robot-Interaction" class="headerlink" title="NatSGLD: A Dataset with Speech, Gesture, Logic, and Demonstration for   Robot Learning in Natural Human-Robot Interaction"></a>NatSGLD: A Dataset with Speech, Gesture, Logic, and Demonstration for   Robot Learning in Natural Human-Robot Interaction</h2><p><strong>Authors:Snehesh Shrestha, Yantian Zha, Saketh Banagiri, Ge Gao, Yiannis Aloimonos, Cornelia Fermüller</strong></p>
<p>Recent advances in multimodal Human-Robot Interaction (HRI) datasets emphasize the integration of speech and gestures, allowing robots to absorb explicit knowledge and tacit understanding. However, existing datasets primarily focus on elementary tasks like object pointing and pushing, limiting their applicability to complex domains. They prioritize simpler human command data but place less emphasis on training robots to correctly interpret tasks and respond appropriately. To address these gaps, we present the NatSGLD dataset, which was collected using a Wizard of Oz (WoZ) method, where participants interacted with a robot they believed to be autonomous. NatSGLD records humans’ multimodal commands (speech and gestures), each paired with a demonstration trajectory and a Linear Temporal Logic (LTL) formula that provides a ground-truth interpretation of the commanded tasks. This dataset serves as a foundational resource for research at the intersection of HRI and machine learning. By providing multimodal inputs and detailed annotations, NatSGLD enables exploration in areas such as multimodal instruction following, plan recognition, and human-advisable reinforcement learning from demonstrations. We release the dataset and code under the MIT License at <a target="_blank" rel="noopener" href="https://www.snehesh.com/natsgld/">https://www.snehesh.com/natsgld/</a> to support future HRI research. </p>
<blockquote>
<p>近期多模态人机互动（HRI）数据集的进步强调语音和手势的整合，让机器人能够吸收明确的知识和隐含的理解。然而，现有的数据集主要专注于基本的任务，如指向物体和推动物体，限制了它们在复杂领域的应用。它们优先处理更简单的人类命令数据，但较少强调训练机器人正确解释任务并适当回应。为了解决这些空白，我们推出了NatSGLD数据集，该数据集是采用“奥兹巫师”（WoZ）方法收集的，参与者与他们认为具有自主性的机器人进行了互动。NatSGLD记录了人类的多模态命令（语音和手势），每个命令都与演示轨迹和线性时序逻辑（LTL）公式配对，为命令任务提供了真实地面的解释。该数据集是HRI和机器学习交叉研究的基础资源。通过提供多模态输入和详细注释，NatSGLD可以在多模态指令跟踪、计划识别和基于演示的人类可建议强化学习等领域进行探索。我们在<a target="_blank" rel="noopener" href="https://www.snehesh.com/natsgld/%E4%B8%8B%E4%BB%A5MIT%E8%AE%B8%E5%8F%AF%E8%AF%81%E7%9A%84%E5%BD%A2%E5%BC%8F%E5%8F%91%E5%B8%83%E6%95%B0%E6%8D%AE%E9%9B%86%E5%92%8C%E4%BB%A3%E7%A0%81%EF%BC%8C%E4%BB%A5%E6%94%AF%E6%8C%81%E6%9C%AA%E6%9D%A5%E7%9A%84%E4%BA%BA%E6%9C%BA%E4%BA%92%E5%8A%A8%E7%A0%94%E7%A9%B6%E3%80%82">https://www.snehesh.com/natsgld/下以MIT许可证的形式发布数据集和代码，以支持未来的人机互动研究。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.16718v1">PDF</a> arXiv admin note: substantial text overlap with arXiv:2403.02274</p>
<p><strong>Summary</strong></p>
<p>近期多模态人机交互（HRI）数据集的发展强调了语音和手势的集成，使机器人能够吸收显性知识和隐性理解。然而，现有数据集主要关注基础任务，如指向和推动物体，限制了它们在复杂领域的应用。NatSGLD数据集的推出解决了这一问题，它采用参与者误以为机器人是自主的“Wizard of Oz”（WoZ）方法收集而成。NatSGLD记录了人类的多模态命令（包括语音和手势），每个命令都配有演示轨迹和线性时序逻辑（LTL）公式，为命令任务提供了真实解读。该数据集为HRI和机器学习交叉领域的研究提供了基础资源，通过提供多模态输入和详细注释，支持对多模态指令跟踪、计划识别和人类可指导的强化学习等领域的探索。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态HRI数据集融合语音和手势，增强机器人对知识的吸收和理解。</li>
<li>现有数据集多关注基础任务，限制了其在复杂场景的应用。</li>
<li>NatSGLD数据集采用WoZ方法收集，包含人类多模态命令、演示轨迹和LTL公式。</li>
<li>NatSGLD为HRI和机器学习研究提供基础资源。</li>
<li>数据集支持多模态指令跟踪、计划识别和强化学习等领域的探索。</li>
<li>NatSGLD数据集和代码已发布在<a target="_blank" rel="noopener" href="https://www.snehesh.com/natsgld/%EF%BC%8C%E4%BE%9B%E6%9C%AA%E6%9D%A5HRI%E7%A0%94%E7%A9%B6%E4%BD%BF%E7%94%A8%E3%80%82">https://www.snehesh.com/natsgld/，供未来HRI研究使用。</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.16718">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-01e6731a6d950c83a6cfd0e8603270d2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2fbe2585eaec0a28a544f09950b10230.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a69ba83e8fdf84be4dc5fcaf5f45b0f.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="MemeIntel-Explainable-Detection-of-Propagandistic-and-Hateful-Memes"><a href="#MemeIntel-Explainable-Detection-of-Propagandistic-and-Hateful-Memes" class="headerlink" title="MemeIntel: Explainable Detection of Propagandistic and Hateful Memes"></a>MemeIntel: Explainable Detection of Propagandistic and Hateful Memes</h2><p><strong>Authors:Mohamed Bayan Kmainasi, Abul Hasnat, Md Arid Hasan, Ali Ezzat Shahroor, Firoj Alam</strong></p>
<p>The proliferation of multimodal content on social media presents significant challenges in understanding and moderating complex, context-dependent issues such as misinformation, hate speech, and propaganda. While efforts have been made to develop resources and propose new methods for automatic detection, limited attention has been given to label detection and the generation of explanation-based rationales for predicted labels. To address this challenge, we introduce MemeIntel, an explanation-enhanced dataset for propaganda memes in Arabic and hateful memes in English, making it the first large-scale resource for these tasks. To solve these tasks, we propose a multi-stage optimization approach and train Vision-Language Models (VLMs). Our results demonstrate that this approach significantly improves performance over the base model for both \textbf{label detection} and explanation generation, outperforming the current state-of-the-art with an absolute improvement of ~3% on ArMeme and ~7% on Hateful Memes. For reproducibility and future research, we aim to make the MemeIntel dataset and experimental resources publicly available. </p>
<blockquote>
<p>社交媒体上多模态内容的激增，为理解和调节复杂、依赖于情境的议题（如虚假信息、仇恨言论和宣传等）带来了重大挑战。虽然已有人努力开发资源和提出自动检测的新方法，但对标签检测以及基于预测的标签生成解释理由的关注有限。为了应对这一挑战，我们推出了MemeIntel，这是一个针对阿拉伯语的宣传标语和英文的仇恨言论标签增强的数据集，成为这些任务的首个大规模资源。为了解决这些任务，我们提出了多阶段优化方法并训练了视觉语言模型（VLMs）。我们的结果表明，这种方法在标签检测和解释生成方面都显著提高了基础模型的性能，相较于当前最先进的技术，在ArMeme上绝对提高了约3%，在仇恨言论标签上提高了约7%。为了可复制性和未来研究，我们计划将MemeIntel数据集和实验资源公开提供。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.16612v1">PDF</a> disinformation, misinformation, factuality, harmfulness, fake news,   propaganda, hateful meme, multimodality, text, images</p>
<p><strong>Summary</strong></p>
<p>社交媒体上多模态内容的普及，给理解和调控诸如虚假信息、仇恨言论和宣传等复杂、依赖情境的议题带来了重大挑战。尽管已有资源和新方法的开发用于自动检测，但在标签检测和预测标签的解释性依据生成方面关注有限。为应对这一挑战，我们推出了MemeIntel数据集，包含阿拉伯语的宣传标语和英文的仇恨标语解释增强功能，成为首个大规模资源用于这些任务。我们采用多阶段优化方法和训练视觉语言模型（VLMs）来解决这些任务。结果表明，该方法在标签检测和解释生成方面较基础模型有显著提升，相较于当前最前沿技术，在ArMeme上绝对提升约3%，在仇恨标语上提升约7%。我们旨在将MemeIntel数据集和实验资源公开，以供复制和未来研究使用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>社交媒体多模态内容带来理解和调控复杂、依赖情境的议题的挑战。</li>
<li>当前自动检测方面存在标签检测和解释性依据生成的关注不足。</li>
<li>引入MemeIntel数据集，包含阿拉伯语和英语的宣传标语和仇恨标语解释增强功能。</li>
<li>采用多阶段优化方法和训练视觉语言模型（VLMs）解决相关任务。</li>
<li>方法在标签检测和解释生成方面较基础模型有显著提升。</li>
<li>与当前最前沿技术相比，在ArMeme和仇恨标语方面取得显著进步。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.16612">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3aaf80d9fe0b7b9e1e1eef13747c19bf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-24b89536fb3dd77fa966c87dda7982ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-90b796bd395a410564ec373d19f53326.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f7d7c964fff0a512abc3b4c226179603.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7e1a0640d8bd71d13052324ac60c1109.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Speech-Enhancement-Using-Continuous-Embeddings-of-Neural-Audio-Codec"><a href="#Speech-Enhancement-Using-Continuous-Embeddings-of-Neural-Audio-Codec" class="headerlink" title="Speech Enhancement Using Continuous Embeddings of Neural Audio Codec"></a>Speech Enhancement Using Continuous Embeddings of Neural Audio Codec</h2><p><strong>Authors:Haoyang Li, Jia Qi Yip, Tianyu Fan, Eng Siong Chng</strong></p>
<p>Recent advancements in Neural Audio Codec (NAC) models have inspired their use in various speech processing tasks, including speech enhancement (SE). In this work, we propose a novel, efficient SE approach by leveraging the pre-quantization output of a pretrained NAC encoder. Unlike prior NAC-based SE methods, which process discrete speech tokens using Language Models (LMs), we perform SE within the continuous embedding space of the pretrained NAC, which is highly compressed along the time dimension for efficient representation. Our lightweight SE model, optimized through an embedding-level loss, delivers results comparable to SE baselines trained on larger datasets, with a significantly lower real-time factor of 0.005. Additionally, our method achieves a low GMAC of 3.94, reducing complexity 18-fold compared to Sepformer in a simulated cloud-based audio transmission environment. This work highlights a new, efficient NAC-based SE solution, particularly suitable for cloud applications where NAC is used to compress audio before transmission.   Copyright 20XX IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting&#x2F;republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. </p>
<blockquote>
<p>近期神经网络音频编解码器（NAC）模型的进展激发了其在各种语音处理任务中的应用，包括语音增强（SE）。在这项工作中，我们提出了一种利用预训练NAC编码器的预量化输出进行高效SE的新方法。与以前基于NAC的SE方法不同，后者使用语言模型（LM）处理离散的语音标记，我们在预训练的NAC的连续嵌入空间内执行SE，该嵌入空间在时间维度上进行了高度压缩以进行有效表示。我们的轻量级SE模型通过嵌入层损失进行优化，其结果表明即使与在更大数据集上训练的SE基线相比，也能取得相当的结果，实时因子为0.005。此外，我们的方法在模拟的云音频传输环境中实现了较低的GMAC值3.94，与Sepformer相比复杂度降低了18倍。这项工作是首次展示了适用于使用NAC进行音频压缩和传输的云应用程序的高效解决方案。版权声明：本论文版权归IEEE所有。允许个人使用本材料。如需用于其他用途（无论当前还是未来媒体），包括但不限于为广告或宣传目的重印或重新发布本材料、创建新的集体作品、转售或重新分发到服务器或列表、或在其他作品中使用本受版权保护作品的任何部分，都必须获得IEEE的许可。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.16240v1">PDF</a> Accepted to ICASSP 2025</p>
<p><strong>摘要</strong></p>
<p>近期神经网络音频编码（NAC）模型的进展为其在语音处理任务中的应用提供了可能，包括语音增强（SE）。本研究提出了一种新的高效SE方法，该方法利用预训练NAC的预量化输出。与先前基于NAC的SE方法不同，我们不在离散语音标记上使用语言模型（LMs）进行处理，而是在预训练NAC的连续嵌入空间内执行SE，该空间在时间维度上高度压缩以实现高效表示。我们的轻量化SE模型通过嵌入级损失进行优化，可提供与在更大数据集上训练的SE基线相当的结果，并且实时因子低得多，为0.005。此外，我们的方法在模拟的云基于音频传输环境中实现了3.94的GMAC，与Sepformer相比减少了18倍复杂性。本研究强调了新的高效NAC-基于SE解决方案，尤其适合在NAC用于音频传输前压缩的云应用程序中使用。</p>
<p><strong>要点</strong></p>
<ol>
<li>NAC模型最新进展为语音增强等语音处理任务提供了新的应用可能性。</li>
<li>提出一种高效SE方法，利用预训练NAC的预量化输出进行语音增强。</li>
<li>与传统方法不同，本方法不在离散语音标记上使用语言模型处理，而是在连续嵌入空间内执行SE。</li>
<li>模型优化通过嵌入级损失进行，实时因子低（0.005），性能与大型数据集训练的SE基线相当。</li>
<li>该方法在模拟的云环境中实现低GMAC（3.94），相较于Sepformer减少了复杂性。</li>
<li>本研究适用于云应用程序中的音频压缩和传输场景。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.16240">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-9ac0f07d6f35eb26cff4c11368381ae7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-707c2405a61498ed0371a12450458091.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c95baecf7f190cc481490b58b6430c1d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-78752534f590bb45f7114c56292efae2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4639273e345c3805d1c7d0b0f0fa8343.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-034588c798c07612262656838e28004b.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Improving-Speech-Enhancement-by-Cross-and-Sub-band-Processing-with-State-Space-Model"><a href="#Improving-Speech-Enhancement-by-Cross-and-Sub-band-Processing-with-State-Space-Model" class="headerlink" title="Improving Speech Enhancement by Cross- and Sub-band Processing with   State Space Model"></a>Improving Speech Enhancement by Cross- and Sub-band Processing with   State Space Model</h2><p><strong>Authors:Jizhen Li, Weiping Tu, Yuhong Yang, Xinmeng Xu, Yiqun Zhang, Yanzhen Ren</strong></p>
<p>Recently, the state space model (SSM) represented by Mamba has shown remarkable performance in long-term sequence modeling tasks, including speech enhancement. However, due to substantial differences in sub-band features, applying the same SSM to all sub-bands limits its inference capability. Additionally, when processing each time frame of the time-frequency representation, the SSM may forget certain high-frequency information of low energy, making the restoration of structure in the high-frequency bands challenging. For this reason, we propose Cross- and Sub-band Mamba (CSMamba). To assist the SSM in handling different sub-band features flexibly, we propose a band split block that splits the full-band into four sub-bands with different widths based on their information similarity. We then allocate independent weights to each sub-band, thereby reducing the inference burden on the SSM. Furthermore, to mitigate the forgetting of low-energy information in the high-frequency bands by the SSM, we introduce a spectrum restoration block that enhances the representation of the cross-band features from multiple perspectives. Experimental results on the DNS Challenge 2021 dataset demonstrate that CSMamba outperforms several state-of-the-art (SOTA) speech enhancement methods in three objective evaluation metrics with fewer parameters. </p>
<blockquote>
<p>最近，以Mamba为代表的状态空间模型（SSM）在长期序列建模任务中，包括语音增强，都表现出了显著的性能。然而，由于子带特征之间存在巨大差异，将相同的SSM应用于所有子带会限制其推理能力。此外，在处理时频表示的每个时间帧时，SSM可能会忘记某些低能量的高频信息，使得恢复高频带中的结构具有挑战性。因此，我们提出了跨带和子带Mamba（CSMamba）。为了帮助SSM灵活处理不同的子带特征，我们提出了一个带分割块，该块根据信息相似性将全频带分割成四个不同宽度的子带。然后我们对每个子带分配独立权重，从而减轻SSM的推理负担。此外，为了减轻SSM在高频带中忘记低能量信息的问题，我们引入了一个频谱恢复块，该块从多个角度增强了跨带特征的表现。在DNS Challenge 2021数据集上的实验结果表明，CSMamba在三个客观评价指标上优于几种最先进的语音增强方法，且参数更少。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.16207v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>Mamba代表的州空间模型（SSM）在长期序列建模任务中表现优异，包括语音增强。但应用同一SSM处理所有子带存在局限性。为此，提出跨带和子带Mamba（CSMamba），通过带分割块将全频带分为不同宽度的子带，并为每个子带分配独立权重，减轻SSM的推理负担。同时，通过频谱恢复块增强跨带特征表示，减少SSM在高频带中遗忘低能量信息的问题。在DNS Challenge 2021数据集上的实验结果表明，CSMamba在三个客观评价指标上优于多种最新语音增强方法，且参数更少。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Mamba代表的州空间模型（SSM）在语音增强等长期序列建模任务中表现出色。</li>
<li>应用同一SSM处理所有子带存在局限性，因为子带特征差异显著。</li>
<li>提出CSMamba模型，通过带分割块将全频带分为不同宽度的子带。</li>
<li>为每个子带分配独立权重，以减轻SSM的推理负担。</li>
<li>频谱恢复块用于增强跨带特征表示，减少SSM在高频带中遗忘低能量信息的问题。</li>
<li>实验结果表明，CSMamba在客观评价指标上优于多种最新语音增强方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.16207">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-d68abef25be5cb001f648baef8aae410.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f0d474369c3358d2158373c681816a71.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-38071304e3dbc2712b05ce565c8809a0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3eb4e86504d5cac66b1f4b82042b172e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-51f84ed0f125288ab87d1ea90a871cd6.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Understanding-Zero-shot-Rare-Word-Recognition-Improvements-Through-LLM-Integration"><a href="#Understanding-Zero-shot-Rare-Word-Recognition-Improvements-Through-LLM-Integration" class="headerlink" title="Understanding Zero-shot Rare Word Recognition Improvements Through LLM   Integration"></a>Understanding Zero-shot Rare Word Recognition Improvements Through LLM   Integration</h2><p><strong>Authors:Haoxuan Wang</strong></p>
<p>In this study, we investigate the integration of a large language model (LLM) with an automatic speech recognition (ASR) system, specifically focusing on enhancing rare word recognition performance. Using a 190,000-hour dataset primarily sourced from YouTube, pre-processed with Whisper V3 pseudo-labeling, we demonstrate that the LLM-ASR architecture outperforms traditional Zipformer-Transducer models in the zero-shot rare word recognition task, after training on a large dataset. Our analysis reveals that the LLM contributes significantly to improvements in rare word error rate (R-WER), while the speech encoder primarily determines overall transcription performance (Orthographic Word Error Rate, O-WER, and Normalized Word Error Rate, N-WER). Through extensive ablation studies, we highlight the importance of adapter integration in aligning speech encoder outputs with the LLM’s linguistic capabilities. Furthermore, we emphasize the critical role of high-quality labeled data in achieving optimal performance. These findings provide valuable insights into the synergy between LLM-based ASR architectures, paving the way for future advancements in large-scale LLM-based speech recognition systems. </p>
<blockquote>
<p>在这项研究中，我们探究了大型语言模型（LLM）与自动语音识别（ASR）系统的融合，特别关注提高罕见词识别性能。我们使用主要来源于YouTube的19万小时数据集，经过Whisper V3伪标签进行预处理，证明在大型数据集训练后，LLM-ASR架构在零样本罕见词识别任务中优于传统的Zipformer-Transducer模型。我们的分析表明，LLM对降低罕见词错误率（R-WER）做出了显著贡献，而语音编码器主要决定整体转录性能（正字法词错误率、O-WER和归一化词错误率、N-WER）。通过广泛的消融研究，我们强调了适配器集成在将语音编码器输出与LLM的语言能力对齐中的重要性。此外，我们强调了高质量标记数据在实现最佳性能中的关键作用。这些发现揭示了LLM为基础的ASR架构之间的协同价值，为未来大规模LLM为基础的语音识别系统的进步铺平了道路。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.16142v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本研究探讨了大型语言模型（LLM）与自动语音识别（ASR）系统的融合，特别是在提高罕见词识别性能方面的应用。通过使用主要来自YouTube的190,000小时数据集，结合Whisper V3伪标签进行预处理，研究证明LLM-ASR架构在零样本罕见词识别任务中优于传统的Zipformer-Transducer模型。分析显示LLM对降低罕见词错误率（R-WER）有显著贡献，而语音编码器主要决定整体转录性能（包括正字法词错误率（O-WER）和归一化词错误率（N-WER）。通过广泛的消融研究，强调了适配器集成在将语音编码器输出与LLM的语言能力对齐中的重要性。同时，研究强调了高质量标记数据在实现最佳性能方面的关键作用。这些发现对于理解LLM在ASR架构中的协同作用具有重要意义，为大规模LLM语音识别系统的未来发展铺平了道路。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM与ASR系统的结合能显著提高罕见词的识别性能。</li>
<li>使用YouTuBe数据的190,000小时大型数据集进行训练有助于LLM-ASR架构的表现。</li>
<li>LLM对降低罕见词错误率有重要贡献。</li>
<li>语音编码器在决定整体转录性能中起关键作用。</li>
<li>适配器集成在LLM和语音编码器之间发挥重要作用。</li>
<li>高质量的标记数据对于实现最佳性能至关重要。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.16142">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-394dd0b00ad4fd1b9e25f8bba28d905e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c9f9332e2cc4af9e04fd75a0c19d4e9.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Generative-AI-Framework-for-3D-Object-Generation-in-Augmented-Reality"><a href="#Generative-AI-Framework-for-3D-Object-Generation-in-Augmented-Reality" class="headerlink" title="Generative AI Framework for 3D Object Generation in Augmented Reality"></a>Generative AI Framework for 3D Object Generation in Augmented Reality</h2><p><strong>Authors:Majid Behravan</strong></p>
<p>This thesis presents a framework that integrates state-of-the-art generative AI models for real-time creation of three-dimensional (3D) objects in augmented reality (AR) environments. The primary goal is to convert diverse inputs, such as images and speech, into accurate 3D models, enhancing user interaction and immersion. Key components include advanced object detection algorithms, user-friendly interaction techniques, and robust AI models like Shap-E for 3D generation. Leveraging Vision Language Models (VLMs) and Large Language Models (LLMs), the system captures spatial details from images and processes textual information to generate comprehensive 3D objects, seamlessly integrating virtual objects into real-world environments. The framework demonstrates applications across industries such as gaming, education, retail, and interior design. It allows players to create personalized in-game assets, customers to see products in their environments before purchase, and designers to convert real-world objects into 3D models for real-time visualization. A significant contribution is democratizing 3D model creation, making advanced AI tools accessible to a broader audience, fostering creativity and innovation. The framework addresses challenges like handling multilingual inputs, diverse visual data, and complex environments, improving object detection and model generation accuracy, as well as loading 3D models in AR space in real-time. In conclusion, this thesis integrates generative AI and AR for efficient 3D model generation, enhancing accessibility and paving the way for innovative applications and improved user interactions in AR environments. </p>
<blockquote>
<p>本论文提出了一个集成最新生成式人工智能模型的框架，用于在增强现实（AR）环境中实时创建三维（3D）对象。主要目标是将图像和语音等多样化的输入转化为准确的三维模型，增强用户交互和沉浸感。关键组件包括先进的物体检测算法、用户友好的交互技术以及用于三维生成的三维模型（如Shap-E）。通过利用视觉语言模型（VLMs）和大型语言模型（LLMs），该系统能够捕捉图像中的空间细节，处理文本信息以生成全面的三维物体，无缝地将虚拟物体集成到真实世界环境中。该框架展示了在游戏、教育、零售和室内设计等行业的应用。它允许玩家创建个性化的游戏资产，让客户在购买前看到产品在环境中的效果，设计师将现实世界物体转化为三维模型进行实时可视化。一个重要的贡献是使三维模型创建民主化，使更广泛的受众能够使用先进的AI工具，促进创造力和创新。该框架解决了处理多语言输入、多样化的视觉数据和复杂环境等挑战，提高了物体检测和模型生成的准确性，以及实时在AR空间中加载三维模型的能力。总之，本论文将生成式人工智能和AR技术相结合，实现高效的三维模型生成，提高可访问性，为AR环境中的创新应用和增强用户交互铺平道路。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.15869v1">PDF</a> </p>
<p><strong>摘要</strong><br>    该论文提出一个融合最前沿生成式人工智能模型的框架，用于在增强现实环境中实时创建三维物体。主要目标是将图像和语音等多样化输入转化为精确的三维模型，增强用户互动与沉浸感。关键组件包括高级物体检测算法、用户友好互动技术和强大的三维生成人工智能模型，如Shap-E。借助视觉语言模型和大语言模型，系统捕捉图像的空间细节并处理文本信息，以生成综合三维物体，无缝集成虚拟物体到真实环境。该框架展示了在游戏、教育、零售和室内设计等行业的应用。它允许玩家创建个性化游戏资产，客户在购买前看到产品在环境中的样子，以及设计师将真实世界物体转化为三维模型进行实时可视化。一个重大贡献是使三维模型创建民主化，使先进的AI工具对更广泛的受众群体可用，促进创造力和创新。该框架解决了处理多语言输入、多样化的视觉数据和复杂环境等挑战，提高了物体检测和模型生成的准确性，以及实时在AR空间中加载三维模型的能力。总之，该论文成功整合生成式人工智能和增强现实技术，实现高效的三维模型生成，提高可及性并为创新应用和增强现实环境中的用户互动铺平道路。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>论文提出了一个整合最前沿生成式AI模型的框架，用于在AR环境中实时创建三维物体。</li>
<li>框架的主要目标是将多样化的输入（如图像和语音）转化为精确的三维模型。</li>
<li>框架包括高级物体检测算法、用户友好互动技术和强大的三维生成AI模型。</li>
<li>利用视觉语言模型和大语言模型，实现虚拟物体与真实环境的无缝集成。</li>
<li>框架展示了在游戏、教育、零售和室内设计等多个行业的应用潜力。</li>
<li>该框架有助于民主化三维模型创建，使先进的AI工具对更广泛的用户群体可用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.15869">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ab0bc3bbbfd2ed16a0e96af25887beae.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Pragmatic-Reasoning-improves-LLM-Code-Generation"><a href="#Pragmatic-Reasoning-improves-LLM-Code-Generation" class="headerlink" title="Pragmatic Reasoning improves LLM Code Generation"></a>Pragmatic Reasoning improves LLM Code Generation</h2><p><strong>Authors:Zhuchen Cao, Sven Apel, Adish Singla, Vera Demberg</strong></p>
<p>Large Language Models (LLMs) have demonstrated impressive potential in translating natural language (NL) instructions into program code. However, user instructions often contain inherent ambiguities, making it challenging for LLMs to generate code that accurately reflects the user’s true intent. To address this challenge, researchers have proposed to produce multiple candidates of the program code and then rerank them to identify the best solution. In this paper, we propose CodeRSA, a novel code candidate reranking mechanism built upon the Rational Speech Act (RSA) framework, designed to guide LLMs toward more comprehensive pragmatic reasoning about user intent. We evaluate CodeRSA using one of the latest LLMs on a popular code generation dataset. Our experiment results show that CodeRSA consistently outperforms common baselines, surpasses the state-of-the-art approach in most cases, and demonstrates robust overall performance. These findings underscore the effectiveness of integrating pragmatic reasoning into code candidate reranking, offering a promising direction for enhancing code generation quality in LLMs. </p>
<blockquote>
<p>大型语言模型（LLM）在将自然语言（NL）指令翻译成程序代码方面表现出了令人印象深刻的潜力。然而，用户指令通常包含固有的歧义性，这使得对于大型语言模型来说，生成准确反映用户真实意图的代码具有挑战性。为了应对这一挑战，研究人员建议生成多个程序代码候选，然后对它们进行重新排序以找出最佳解决方案。在本文中，我们提出了CodeRSA，这是一种基于理性言语行为（RSA）框架的新型代码候选重新排序机制，旨在引导大型语言模型对用户意图进行更全面、实用主义的推理。我们使用最新的大型语言模型之一在一个流行的代码生成数据集上评估了CodeRSA。实验结果表明，CodeRSA在大多数情况下均表现优异，并且在多数情况下优于最先进的方法，显示出稳健的整体性能。这些发现强调了将实用主义推理整合到代码候选重新排序中的有效性，为提升大型语言模型中的代码生成质量提供了有前景的方向。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.15835v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在将自然语言指令翻译成程序代码方面展现出巨大潜力，但用户指令中常存在固有歧义，使LLM难以准确反映用户真实意图地生成代码。为解决此挑战，研究者提议生成程序代码的多重候选，并运用CodeRSA这一基于理性言语行为（RSA）框架的新型代码候选重排机制对候选进行排序以找到最佳解决方案。评估实验结果表明，CodeRSA在多数情况下表现优于常规基准和最新前沿方法，展现出稳健的整体性能。这表明将语用推理融入代码候选重排可有效提升LLM的代码生成质量。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）在自然语言翻译代码方面展现出潜力。</li>
<li>用户指令中的歧义给LLM准确生成代码带来挑战。</li>
<li>为解决这一挑战，研究者提议生成代码的多重候选并进行排序。</li>
<li>CodeRSA是一种基于理性言语行为（RSA）框架的新型代码候选重排机制。</li>
<li>CodeRSA在评估实验中表现优异，超越常规基准和前沿方法。</li>
<li>实验结果证明了将语用推理融入代码候选重排的有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.15835">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a0438d654c48f4f2fc41fafe0c96c556.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f0d4655c541712b64ae74b815232dd95.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca3bdf58fd5ef203c69dc97548c34a72.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Retrieval-Augmented-Speech-Recognition-Approach-for-Domain-Challenges"><a href="#Retrieval-Augmented-Speech-Recognition-Approach-for-Domain-Challenges" class="headerlink" title="Retrieval-Augmented Speech Recognition Approach for Domain Challenges"></a>Retrieval-Augmented Speech Recognition Approach for Domain Challenges</h2><p><strong>Authors:Peng Shen, Xugang Lu, Hisashi Kawai</strong></p>
<p>Speech recognition systems often face challenges due to domain mismatch, particularly in real-world applications where domain-specific data is unavailable because of data accessibility and confidentiality constraints. Inspired by Retrieval-Augmented Generation (RAG) techniques for large language models (LLMs), this paper introduces a LLM-based retrieval-augmented speech recognition method that incorporates domain-specific textual data at the inference stage to enhance recognition performance. Rather than relying on domain-specific textual data during the training phase, our model is trained to learn how to utilize textual information provided in prompts for LLM decoder to improve speech recognition performance. Benefiting from the advantages of the RAG retrieval mechanism, our approach efficiently accesses locally available domain-specific documents, ensuring a convenient and effective process for solving domain mismatch problems. Experiments conducted on the CSJ database demonstrate that the proposed method significantly improves speech recognition accuracy and achieves state-of-the-art results on the CSJ dataset, even without relying on the full training data. </p>
<blockquote>
<p>语音识别系统经常面临因领域不匹配而带来的挑战，特别是在现实应用中，由于数据可访问性和保密性约束，往往无法获得特定领域的语音数据。受大型语言模型（LLM）的检索增强生成（RAG）技术的启发，本文介绍了一种基于LLM的检索增强语音识别方法，该方法在推理阶段结合特定领域的文本数据，以提高识别性能。与其他方法不同，我们的模型不是在训练阶段依赖特定领域的文本数据，而是学习如何利用提示中的文本信息，以促使LLM解码器提高语音识别性能。得益于RAG检索机制的优势，我们的方法能够有效地访问本地可用的特定领域文档，确保解决领域不匹配问题的过程既方便又高效。在CSJ数据库上进行的实验表明，该方法显著提高了语音识别精度，在CSJ数据集上达到了最先进的识别效果，甚至在不依赖全部训练数据的情况下也是如此。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.15264v1">PDF</a> </p>
<p><strong>Summary</strong>：</p>
<p>本文提出了一种基于大语言模型（LLM）的检索增强语音识别方法，该方法在推理阶段融入领域特定文本数据，以提高语音识别性能。该方法无需在训练阶段依赖领域特定文本数据，而是学习如何利用提示中的文本信息，改善语音识别效果。实验证明，该方法能显著提高语音识别准确率，达到CSJ数据集上的最新水平，且无需依赖全部训练数据。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>该论文解决了语音识别系统中的领域不匹配问题，特别是在数据获取和保密性受限的现实中。</li>
<li>通过结合大语言模型（LLM）和检索增强生成（RAG）技术，论文提出了一种新的语音识别方法。</li>
<li>该方法在推理阶段融入领域特定文本数据，而不是在训练阶段。</li>
<li>方法的核心在于学习如何利用提示中的文本信息，改善语音识别效果。</li>
<li>论文采用RAG检索机制，能高效访问本地可用的领域特定文档。</li>
<li>实验证明，该方法能显著提高语音识别准确率，达到最新水平。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.15264">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-87968c188ddea404610974386096b3e4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-81867d91435f9cee822b0bb90f72c461.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1aaab7cc3f980fcace86bbf43d3f233a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d1a21a8672b96cc51c761100cb525d56.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-65de1087f77945b5d5a0dc791dcc6466.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-478aaf7d881446049035f88eb2850545.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Enhancing-Speech-Large-Language-Models-with-Prompt-Aware-Mixture-of-Audio-Encoders"><a href="#Enhancing-Speech-Large-Language-Models-with-Prompt-Aware-Mixture-of-Audio-Encoders" class="headerlink" title="Enhancing Speech Large Language Models with Prompt-Aware Mixture of   Audio Encoders"></a>Enhancing Speech Large Language Models with Prompt-Aware Mixture of   Audio Encoders</h2><p><strong>Authors:Weiqiao Shan, Yuang Li, Yuhao Zhang, Yingfeng Luo, Chen Xu, Xiaofeng Zhao, Long Meng, Yunfei Lu, Min Zhang, Hao Yang, Tong Xiao, Jingbo Zhu</strong></p>
<p>Connecting audio encoders with large language models (LLMs) allows the LLM to perform various audio understanding tasks, such as automatic speech recognition (ASR) and audio captioning (AC). Most research focuses on training an adapter layer to generate a unified audio feature for the LLM. However, different tasks may require distinct features that emphasize either semantic or acoustic aspects, making task-specific audio features more desirable. In this paper, we propose Prompt-aware Mixture (PaM) to enhance the Speech LLM that uses multiple audio encoders. Our approach involves using different experts to extract different features based on the prompt that indicates different tasks. Experiments demonstrate that with PaM, only one Speech LLM surpasses the best performances achieved by all single-encoder Speech LLMs on ASR, Speaker Number Verification, and AC tasks. PaM also outperforms other feature fusion baselines, such as concatenation and averaging. </p>
<blockquote>
<p>将音频编码器与大型语言模型（LLM）相连接，可以使LLM执行各种音频理解任务，例如自动语音识别（ASR）和音频描述（AC）。大多数研究集中于训练适配器层以生成用于LLM的统一音频特征。然而，不同的任务可能需要强调语义或声学方面的不同特征，这使得任务特定的音频特征更为理想。在本文中，我们提出使用提示感知混合（PaM）技术增强使用多个音频编码器的语音LLM。我们的方法包括根据提示使用不同的专家来提取不同的特征，这些提示表明了不同的任务。实验表明，使用PaM的单一语音LLM在ASR、说话人数量验证和AC任务上的表现超过了所有单编码器语音LLM的最佳性能。PaM还优于其他特征融合基线方法，如拼接和平均化方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.15178v1">PDF</a> 12 pages,4 figures, 7 tables</p>
<p><strong>Summary</strong><br>基于大型语言模型（LLM）的音频编码器连接使得LLM能够进行音频理解任务，如语音识别（ASR）和音频描述（AC）。尽管大多数研究集中在训练一个统一的音频特征生成适配器层，但不同的任务可能需要强调语义或声学方面的不同特征，因此任务特定的音频特征更为理想。本文提出了基于提示感知混合（PaM）的语音LLM增强方法，该方法使用多个音频编码器提取不同的特征，根据提示针对不同的任务使用不同的专家。实验表明，采用PaM的单一语音LLM在ASR、说话人数验证和AC任务上的性能超过了所有单一编码器语音LLM的最佳性能，并优于其他特征融合基线方法，如拼接和平均。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>音频编码器与大型语言模型的结合使得语言模型能够执行多种音频理解任务，如语音识别和音频描述。</li>
<li>大多数研究集中在训练适配器层以生成统一的音频特征，但任务特定音频特征的重要性被忽视。</li>
<li>本文提出一种基于提示感知混合（PaM）的方法，以增强语音LLM的性能。</li>
<li>PaM使用多个音频编码器，根据任务提示提取不同的特征。</li>
<li>实验表明，使用PaM的语音LLM在多个任务上超越了最佳的单编码器语音LLM性能。</li>
<li>PaM方法优于其他特征融合方法，如简单的拼接和平均。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.15178">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4d38eb8b7624848e484d1b66d0995748.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0dfea113910f27f093691d5fbe9379f0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d33e239884d6de4fc6d45ee3864b060b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-84556732b02df940e8b1057f48d3b858.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a96d285529d5e9c988e3610f5fc1e221.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-17b506b20775114af40948f3e329094b.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Unveiling-Reasoning-Thresholds-in-Language-Models-Scaling-Fine-Tuning-and-Interpretability-through-Attention-Maps"><a href="#Unveiling-Reasoning-Thresholds-in-Language-Models-Scaling-Fine-Tuning-and-Interpretability-through-Attention-Maps" class="headerlink" title="Unveiling Reasoning Thresholds in Language Models: Scaling, Fine-Tuning,   and Interpretability through Attention Maps"></a>Unveiling Reasoning Thresholds in Language Models: Scaling, Fine-Tuning,   and Interpretability through Attention Maps</h2><p><strong>Authors:Yen-Che Hsiao, Abhishek Dutta</strong></p>
<p>This study investigates the in-context learning capabilities of various decoder-only transformer-based language models with different model sizes and training data, including GPT2, SmolLM2, OpenELM, TinyLlama, Stable LM, and Gemma 2. We identify a critical parameter threshold (~1.6 billion), beyond which reasoning performance improves significantly in tasks such as commonsense reasoning in multiple-choice question answering and deductive reasoning. Specifically, models above this threshold achieve better success rates in chain-of-thought (CoT) prompting for deductive reasoning tasks, especially those requiring longer reasoning chains, such as proof by contradiction and disjunction elimination. To address limitations in sub-threshold models, we demonstrate that fine-tuning with task-specific exemplars substantially enhances reasoning performance, enabling accurate CoT generation even without additional exemplars in the prompt for tasks with shorter reasoning chains. Finally, our analysis of attention maps reveals that models capable of generating correct CoTs exhibit higher token-level attention scores on subsequent correct tokens and the correct parts of speech, providing interpretability insights into reasoning processes. These findings collectively advance understanding of reasoning capabilities in decoder-only transformer-based models. The code is available at: <a target="_blank" rel="noopener" href="https://github.com/AnnonymousForPapers/CoT_Reasoning_Test">https://github.com/AnnonymousForPapers/CoT_Reasoning_Test</a>. </p>
<blockquote>
<p>本研究调查了不同规模的解码器专用基于转换器的语言模型以及使用不同训练数据的模型在各种上下文学习功能方面的表现，包括GPT2、SmolLM2、OpenELM、TinyLlama、Stable LM和Gemma 2。我们确定了临界参数阈值（约1.6亿），超出该阈值后，在多选问答中的常识推理和演绎推理等任务中的推理性能会显著提高。具体来说，超过此阈值的模型在演绎推理任务的思维链提示中取得了更高的成功率，特别是在需要较长推理链的任务中，如反证法和析取消除法。为了解决低于阈值模型的局限性，我们证明使用特定任务的示例进行微调可以大大提高推理性能，即使在没有额外的简短推理链提示中也能准确生成思维链。最后，通过对注意力地图的分析，我们发现能够生成正确思维链的模型在随后的正确令牌和正确的词性方面表现出更高的令牌级注意力得分，这为理解推理过程提供了可解释性洞察。这些发现共同推动了我们对基于解码器的转换器模型推理能力的理解。代码可用在：<a target="_blank" rel="noopener" href="https://github.com/AnnonymousForPapers/CoT_Reasoning_Test%E3%80%82">https://github.com/AnnonymousForPapers/CoT_Reasoning_Test。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.15120v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本研究探讨了不同规模与训练数据的解码器只变压器语言模型的上下文学习能力，包括GPT2、SmolLM2、OpenELM等模型。研究发现一个关键参数阈值（约1.6亿），超过此阈值的模型在多选问答与演绎推理等任务中的常识推理性能显著提升。特别是通过思维链提示法，超过阈值的模型在需要较长推理链的任务中表现优异。对于未达到阈值的模型，通过特定任务的微调可以极大地增强推理性能。此外，通过分析注意力地图发现，能正确生成思维链的模型在后续正确令牌和正确词类上的令牌级别注意力得分更高，为理解推理过程提供了可解释性洞察。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究涉及多种解码器只变压器语言模型，包括GPT2等，探讨了它们的上下文学习能力。</li>
<li>发现模型性能的关键参数阈值（约1.6亿），超过此阈值的模型在常识推理任务中表现更优秀。</li>
<li>思维链提示法在需要较长推理链的任务中特别有效。</li>
<li>对未达到性能阈值的模型进行特定任务微调可以显著提高推理性能。</li>
<li>分析注意力地图发现正确生成思维链的模型展现出更高的令牌级别注意力得分。</li>
<li>研究提供了对解码器只变压器模型的推理能力的深入理解。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.15120">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-bafc0515edfd14d0d7b69a70d5ef7530.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6615e3058ab322bfaa9d641b4a5e752a.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="VLAS-Vision-Language-Action-Model-With-Speech-Instructions-For-Customized-Robot-Manipulation"><a href="#VLAS-Vision-Language-Action-Model-With-Speech-Instructions-For-Customized-Robot-Manipulation" class="headerlink" title="VLAS: Vision-Language-Action Model With Speech Instructions For   Customized Robot Manipulation"></a>VLAS: Vision-Language-Action Model With Speech Instructions For   Customized Robot Manipulation</h2><p><strong>Authors:Wei Zhao, Pengxiang Ding, Min Zhang, Zhefei Gong, Shuanghao Bai, Han Zhao, Donglin Wang</strong></p>
<p>Vision-language-action models (VLAs) have become increasingly popular in robot manipulation for their end-to-end design and remarkable performance. However, existing VLAs rely heavily on vision-language models (VLMs) that only support text-based instructions, neglecting the more natural speech modality for human-robot interaction. Traditional speech integration methods usually involves a separate speech recognition system, which complicates the model and introduces error propagation. Moreover, the transcription procedure would lose non-semantic information in the raw speech, such as voiceprint, which may be crucial for robots to successfully complete customized tasks. To overcome above challenges, we propose VLAS, a novel end-to-end VLA that integrates speech recognition directly into the robot policy model. VLAS allows the robot to understand spoken commands through inner speech-text alignment and produces corresponding actions to fulfill the task. We also present two new datasets, SQA and CSI, to support a three-stage tuning process for speech instructions, which empowers VLAS with the ability of multimodal interaction across text, image, speech, and robot actions. Taking a step further, a voice retrieval-augmented generation (RAG) paradigm is designed to enable our model to effectively handle tasks that require individual-specific knowledge. Our extensive experiments show that VLAS can effectively accomplish robot manipulation tasks with diverse speech commands, offering a seamless and customized interaction experience. </p>
<blockquote>
<p>视觉语言行动模型（VLAs）因其端到端的设计和卓越性能而在机器人操作领域越来越受欢迎。然而，现有的VLAs严重依赖于仅支持文本指令的视觉语言模型（VLMs），忽视了人类与机器人交互中更自然的语音模式。传统的语音集成方法通常需要一个独立的语音识别系统，这增加了模型的复杂性并引入了误差传播。此外，转录过程会丢失原始语音中的非语义信息，如声纹，这对于机器人成功完成定制任务可能是至关重要的。为了克服上述挑战，我们提出了VLAS，这是一种新型的端到端VLA，它直接将语音识别集成到机器人策略模型中。VLAS允许机器人通过内部语音文本对齐理解口头命令，并产生相应的行动来完成任务。我们还推出了两个新数据集SQA和CSI，以支持语音指令的三阶段调整过程，这使VLAS具备文本、图像、语音和机器人行动之间的跨模态交互能力。更进一步的是，设计了一种声音检索增强生成（RAG）范式，使我们的模型能够有效处理需要个人特定知识的任务。我们的广泛实验表明，VLAS可以有效地完成具有多种语音命令的机器人操作任务，提供无缝且定制化的交互体验。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13508v2">PDF</a> Accepted as a conference paper at ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了视觉语言行动模型（VLAs）在机器人操作中的最新应用。针对现有VLAs主要依赖文本指令而忽视语音模式的问题，提出了一种新型端到端的VLA模型VLAS，将语音识别直接集成到机器人策略模型中。VLAS能通过内部语音文本对齐理解口语指令，并产生相应的行动来完成任务。此外，还推出了两个新数据集SQA和CSI，支持语音指令的三阶段调整过程，使VLAS具备跨文本、图像、语音和机器人行动的多媒体交互能力。最后，通过引入声音检索增强生成（RAG）范式，使模型能够处理需要个性化知识的任务。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视觉语言行动模型（VLAs）在机器人操作中的应用日益普及，具有端到端的设计和卓越性能。</li>
<li>现有VLAs主要依赖文本指令，忽视了更自然的语音模式的人机交互方式。</li>
<li>新型VLAS模型将语音识别直接集成到机器人策略模型中，理解口语指令并产生相应行动。</li>
<li>VLAS通过内部语音文本对齐实现口语指令理解。</li>
<li>推出了SQA和CSI两个新数据集，支持语音指令的三阶段调整过程，增强VLAS的多媒体交互能力。</li>
<li>VLAS能够处理需要个性化知识的任务，引入声音检索增强生成（RAG）范式。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13508">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-495948d6a743c703d1c1bb0a7781aa28.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-09406fcc63a46cd60c63870aa3fead1f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ba13a25fefefda2b4a1af18ab7b3ce3c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad6aff11d02c3d0d6335b7cb9e5a1cb8.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="VINP-Variational-Bayesian-Inference-with-Neural-Speech-Prior-for-Joint-ASR-Effective-Speech-Dereverberation-and-Blind-RIR-Identification"><a href="#VINP-Variational-Bayesian-Inference-with-Neural-Speech-Prior-for-Joint-ASR-Effective-Speech-Dereverberation-and-Blind-RIR-Identification" class="headerlink" title="VINP: Variational Bayesian Inference with Neural Speech Prior for Joint   ASR-Effective Speech Dereverberation and Blind RIR Identification"></a>VINP: Variational Bayesian Inference with Neural Speech Prior for Joint   ASR-Effective Speech Dereverberation and Blind RIR Identification</h2><p><strong>Authors:Pengyu Wang, Ying Fang, Xiaofei Li</strong></p>
<p>Reverberant speech, denoting the speech signal degraded by the process of reverberation, contains crucial knowledge of both anechoic source speech and room impulse response (RIR). This work proposes a variational Bayesian inference (VBI) framework with neural speech prior (VINP) for joint speech dereverberation and blind RIR identification. In VINP, a probabilistic signal model is constructed in the time-frequency (T-F) domain based on convolution transfer function (CTF) approximation. For the first time, we propose using an arbitrary discriminative dereverberation deep neural network (DNN) to predict the prior distribution of anechoic speech within a probabilistic model. By integrating both reverberant speech and the anechoic speech prior, VINP yields the maximum a posteriori (MAP) and maximum likelihood (ML) estimations of the anechoic speech spectrum and CTF filter, respectively. After simple transformations, the waveforms of anechoic speech and RIR are estimated. Moreover, VINP is effective for automatic speech recognition (ASR) systems, which sets it apart from most deep learning (DL)-based single-channel dereverberation approaches. Experiments on single-channel speech dereverberation demonstrate that VINP reaches an advanced level in most metrics related to human perception and displays unquestionable state-of-the-art (SOTA) performance in ASR-related metrics. For blind RIR identification, experiments indicate that VINP attains the SOTA level in blind estimation of reverberation time at 60 dB (RT60) and direct-to-reverberation ratio (DRR). Codes and audio samples are available online. </p>
<blockquote>
<p>带有混响的语音信号包含了关于无混响源语音和房间冲击响应（RIR）的关键知识。本研究提出了一种基于神经语音先验（VINP）的变贝叶斯推断（VBI）框架，用于联合语音去混响和盲RIR识别。在VINP中，基于卷积传递函数（CTF）近似值在时频（T-F）域中构建概率信号模型。我们首次提出使用任意判别去混响深度神经网络（DNN）在概率模型中预测无混响语音的先验分布。通过整合混响语音和无混响语音先验，VINP得到无混响语音谱和CTF滤波器的最大后验（MAP）和最大似然（ML）估计。经过简单变换，可以估算出无混响语音和RIR的波形。此外，VINP对于自动语音识别（ASR）系统也有效，这与大多数基于深度学习的单通道去混响方法相区别。在单通道语音去混响方面的实验表明，VINP在大多数与人类感知相关的指标中达到了先进水平，并且在与ASR相关的指标中表现出了无可争议的最先进性能。对于盲RIR识别，实验表明，VINP在60分贝（RT60）的混响时间盲估计和直接混响比（DRR）方面达到了最先进水平。代码和音频样本可在网上获得。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.07205v2">PDF</a> Submitted to IEEE&#x2F;ACM Trans. on TASLP</p>
<p><strong>摘要</strong></p>
<p>基于神经语音先验的变分贝叶斯推理框架被提出，用于联合语音去混响和盲房间脉冲响应识别。该框架在时频域构建了基于卷积传递函数近似的概率信号模型。本研究首次提出使用任意判别去混响深度神经网络来预测概率模型中的无混响语音先验分布。通过结合混响语音和无混响语音先验，该框架提供了无混响语音谱和卷积传递函数滤波器的最大后验和最大似然估计。经过简单变换，可估算出无混响语音和房间脉冲响应的波形。此外，该框架对于自动语音识别系统也有效，使其有别于大多数基于深度学习的单通道去混响方法。实验表明，该框架在人类感知相关的大多数指标上达到了先进水平和自动语音识别相关指标的最新性能。对于盲房间脉冲响应识别，实验表明该框架在盲估计混响时间直接比在（DRR）和房间脉冲响应的混响时间（RT60）方面达到了最新水平。相关代码和音频样本可在网上获取。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>提出的变分贝叶斯推理框架结合了神经语音先验，用于联合语音去混响和盲房间脉冲响应识别。</li>
<li>构建了基于卷积传递函数近似的概率信号模型，在时频域进行分析。</li>
<li>使用任意判别去混响深度神经网络预测无混响语音的先验分布。</li>
<li>该方法提供了无混响语音谱和卷积传递函数滤波器的最大后验和最大似然估计。</li>
<li>框架对自动语音识别系统有效，区别于大多数单通道去混响的深度学习方法。</li>
<li>实验表明，该框架在人类感知相关指标上达到先进水平，且在自动语音识别相关指标上表现最佳。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.07205">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-6ffca12842c8f4a63c6f576fda4bc6f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea812fe21f4940fde73a93158c1d8760.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ab8d8083f73aecbf707ea2724ad84601.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-97d39fb86c8b2300c823bbe41181f3fc.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Llasa-Scaling-Train-Time-and-Inference-Time-Compute-for-Llama-based-Speech-Synthesis"><a href="#Llasa-Scaling-Train-Time-and-Inference-Time-Compute-for-Llama-based-Speech-Synthesis" class="headerlink" title="Llasa: Scaling Train-Time and Inference-Time Compute for Llama-based   Speech Synthesis"></a>Llasa: Scaling Train-Time and Inference-Time Compute for Llama-based   Speech Synthesis</h2><p><strong>Authors:Zhen Ye, Xinfa Zhu, Chi-Min Chan, Xinsheng Wang, Xu Tan, Jiahe Lei, Yi Peng, Haohe Liu, Yizhu Jin, Zheqi Dai, Hongzhan Lin, Jianyi Chen, Xingjian Du, Liumeng Xue, Yunlin Chen, Zhifei Li, Lei Xie, Qiuqiang Kong, Yike Guo, Wei Xue</strong></p>
<p>Recent advances in text-based large language models (LLMs), particularly in the GPT series and the o1 model, have demonstrated the effectiveness of scaling both training-time and inference-time compute. However, current state-of-the-art TTS systems leveraging LLMs are often multi-stage, requiring separate models (e.g., diffusion models after LLM), complicating the decision of whether to scale a particular model during training or testing. This work makes the following contributions: First, we explore the scaling of train-time and inference-time compute for speech synthesis. Second, we propose a simple framework Llasa for speech synthesis that employs a single-layer vector quantizer (VQ) codec and a single Transformer architecture to fully align with standard LLMs such as Llama. Our experiments reveal that scaling train-time compute for Llasa consistently improves the naturalness of synthesized speech and enables the generation of more complex and accurate prosody patterns. Furthermore, from the perspective of scaling inference-time compute, we employ speech understanding models as verifiers during the search, finding that scaling inference-time compute shifts the sampling modes toward the preferences of specific verifiers, thereby improving emotional expressiveness, timbre consistency, and content accuracy. In addition, we released the checkpoint and training code for our TTS model (1B, 3B, 8B) and codec model publicly available. </p>
<blockquote>
<p>最近，基于文本的大型语言模型（LLM）的最新进展，特别是GPT系列和O1模型，证明了在训练时间和推理时间计算方面的可扩展性非常有效。然而，当前利用LLM的最先进TTS系统通常是多阶段的，需要单独模型（例如LLM之后的扩散模型），这使得在训练或测试期间扩展特定模型变得更加复杂决策。这项工作做出了以下贡献：首先，我们探索了语音合成的训练时间和推理时间计算的可扩展性。其次，我们提出了一种用于语音合成的简单框架Llasa，它采用单层向量量化（VQ）编解码器和单一Transformer架构，以完全符合Llama等标准LLM。我们的实验表明，扩大Llasa的训练时间计算始终提高了合成语音的自然度，并可以生成更复杂和准确的声音模式。此外，从扩展推理时间计算的角度来看，我们在搜索过程中采用了语音理解模型作为验证器，发现扩大推理时间计算使采样模式转向特定验证器的偏好，从而提高情感表现力、音色一致性和内容准确性。另外，我们公开发布了TTS模型（1B、3B、8B）和编解码器模型的检查点和训练代码。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.04128v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探讨了基于文本的大型语言模型（LLMs）在语音合成中的训练和推理时间计算规模化的研究。提出了一种名为Llasa的语音合成框架，采用单层向量量化编码器和单一的Transformer架构，与标准LLMs如Llama完全对齐。实验表明，增加训练时间计算规模可以提高合成语音的自然度，并生成更复杂、更准确的语调模式。从推理时间计算规模的角度来看，采用语音理解模型作为验证器进行搜索，发现增加推理时间计算规模会使采样模式转向特定验证器的偏好，从而提高情感表达、音色一致性和内容准确性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文本介绍了LLMs在语音合成中的最新进展，特别是GPT系列和o1模型的有效性。</li>
<li>当前先进的TTS系统利用LLMs通常是多阶段的，需要单独模型（如扩散模型后的LLM），这使得在训练和测试期间选择模型规模变得复杂。</li>
<li>提出了一个名为Llasa的语音合成框架，采用单一层向量量化编码器和单一的Transformer架构与标准LLMs对齐。</li>
<li>增加训练时间计算规模可以提高合成语音的自然度和生成更复杂、更准确的语调模式。</li>
<li>从推理时间计算规模的角度来看，增加计算规模会使采样模式更符合特定语音理解模型的偏好，提高情感表达、音色一致性和内容准确性。</li>
<li>公开提供了TTS模型（1B、3B、8B）和编码器模型的检查点和训练代码。</li>
<li>整体而言，该研究为语音合成的计算规模化和性能优化提供了新的见解和工具。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.04128">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-73e7aba508d71adf2faf33d7f539032e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8dade7875c113493333280e518621dc5.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="ToxiLab-How-Well-Do-Open-Source-LLMs-Generate-Synthetic-Toxicity-Data"><a href="#ToxiLab-How-Well-Do-Open-Source-LLMs-Generate-Synthetic-Toxicity-Data" class="headerlink" title="ToxiLab: How Well Do Open-Source LLMs Generate Synthetic Toxicity Data?"></a>ToxiLab: How Well Do Open-Source LLMs Generate Synthetic Toxicity Data?</h2><p><strong>Authors:Zheng Hui, Zhaoxiao Guo, Hang Zhao, Juanyong Duan, Lin Ai, Yinheng Li, Julia Hirschberg, Congrui Huang</strong></p>
<p>Effective toxic content detection relies heavily on high-quality and diverse data, which serve as the foundation for robust content moderation models. Synthetic data has become a common approach for training models across various NLP tasks. However, its effectiveness remains uncertain for highly subjective tasks like hate speech detection, with previous research yielding mixed results. This study explores the potential of open-source LLMs for harmful data synthesis, utilizing controlled prompting and supervised fine-tuning techniques to enhance data quality and diversity. We systematically evaluated 6 open source LLMs on 5 datasets, assessing their ability to generate diverse, high-quality harmful data while minimizing hallucination and duplication. Our results show that Mistral consistently outperforms other open models, and supervised fine-tuning significantly enhances data reliability and diversity. We further analyze the trade-offs between prompt-based vs. fine-tuned toxic data synthesis, discuss real-world deployment challenges, and highlight ethical considerations. Our findings demonstrate that fine-tuned open source LLMs provide scalable and cost-effective solutions to augment toxic content detection datasets, paving the way for more accessible and transparent content moderation tools. </p>
<blockquote>
<p>有效的有毒内容检测在很大程度上依赖于高质量和多样化的数据，这些数据为构建稳健的内容管理模型提供了基础。合成数据已成为各种NLP任务中训练模型的常见方法。然而，对于像仇恨言论检测这样的高度主观任务，其有效性尚不确定，之前的研究结果喜忧参半。本研究探讨了开源大型语言模型在有害数据合成方面的潜力，利用受控提示和监督微调技术来提高数据的质量和多样性。我们在五个数据集上系统地评估了六个开源大型语言模型，评估它们生成多样化、高质量的有害数据的能力，同时尽量减少幻觉和重复。我们的结果表明，Mistral始终优于其他开放模型，监督微调显著提高了数据的可靠性和多样性。我们进一步分析了基于提示与微调有毒数据合成的权衡，讨论了现实部署挑战，并强调了伦理考量。我们的研究结果表明，经过微调后的开源大型语言模型为增强有毒内容检测数据集提供了可扩展且经济实惠的解决方案，为更可访问和透明的内容管理工具的铺设道路。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.15175v4">PDF</a> 14 pages</p>
<p><strong>Summary</strong></p>
<p>本文探讨了开源大型语言模型（LLMs）在合成有害数据方面的潜力，研究通过受控提示和监督微调技术提高数据质量和多样性。实验评估了6个开源LLMs在5个数据集上的表现，结果显示Mistral表现最佳，监督微调可显著提高数据可靠性和多样性。研究还分析了提示与微调之间的权衡，并讨论了实际部署的挑战和伦理考量。研究结果表明，微调的开源LLMs为增强有毒内容检测数据集提供了可扩展且经济实惠的解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>高质量和多样化的数据是建立稳健内容审核模型的基础。</li>
<li>合成数据在NLP任务中广泛使用，但在主观任务如仇恨言论检测中的有效性尚不确定。</li>
<li>开源LLMs在有害数据合成方面具有潜力。</li>
<li>通过受控提示和监督微调技术可以提高数据质量和多样性。</li>
<li>Mistral在评估中表现最佳。</li>
<li>监督微调可显著提高数据可靠性和多样性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.15175">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-886de0fd6fe0397114d365d0e7247f23.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-29f3956b38f04e633b8ff6646584aa6b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5d54a78ad900e6702b1afa6b8f97dce9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5a9013b41d9626c28ec9853f0c9552e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c09b3750e6e9e8f3c889381487706b1d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f7dc5efa7d9df0d3ca2232a7f0afa7eb.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Just-KIDDIN-Knowledge-Infusion-and-Distillation-for-Detection-of-INdecent-Memes"><a href="#Just-KIDDIN-Knowledge-Infusion-and-Distillation-for-Detection-of-INdecent-Memes" class="headerlink" title="Just KIDDIN: Knowledge Infusion and Distillation for Detection of   INdecent Memes"></a>Just KIDDIN: Knowledge Infusion and Distillation for Detection of   INdecent Memes</h2><p><strong>Authors:Rahul Garg, Trilok Padhi, Hemang Jain, Ugur Kursuncu, Ponnurangam Kumaraguru</strong></p>
<p>Toxicity identification in online multimodal environments remains a challenging task due to the complexity of contextual connections across modalities (e.g., textual and visual). In this paper, we propose a novel framework that integrates Knowledge Distillation (KD) from Large Visual Language Models (LVLMs) and knowledge infusion to enhance the performance of toxicity detection in hateful memes. Our approach extracts sub-knowledge graphs from ConceptNet, a large-scale commonsense Knowledge Graph (KG) to be infused within a compact VLM framework. The relational context between toxic phrases in captions and memes, as well as visual concepts in memes enhance the model’s reasoning capabilities. Experimental results from our study on two hate speech benchmark datasets demonstrate superior performance over the state-of-the-art baselines across AU-ROC, F1, and Recall with improvements of 1.1%, 7%, and 35%, respectively. Given the contextual complexity of the toxicity detection task, our approach showcases the significance of learning from both explicit (i.e. KG) as well as implicit (i.e. LVLMs) contextual cues incorporated through a hybrid neurosymbolic approach. This is crucial for real-world applications where accurate and scalable recognition of toxic content is critical for creating safer online environments. </p>
<blockquote>
<p>在多模态在线环境中进行毒性识别仍然是一项具有挑战性的任务，因为跨不同模态（例如文本和视觉）的上下文连接复杂性。在本文中，我们提出了一种新型框架，该框架结合了来自大型视觉语言模型（LVLMs）的知识蒸馏（KD）和知识注入，以提高仇恨性meme中的毒性检测性能。我们的方法从ConceptNet（一个大规模常识知识图谱（KG））中提取子知识图谱，并将其注入紧凑的VLM框架中。标题和meme中毒性短语之间的关系上下文以及meme中的视觉概念增强了模型的推理能力。我们在两个仇恨言论基准数据集上进行的实验结果表明，我们的方法在AU-ROC、F1和召回率方面的性能均优于最新基线，分别提高了1.1%、7%和35%。考虑到毒性检测任务的上下文复杂性，我们的方法展示了从显式（即KG）和隐式（即LVLMs）的上下文线索中学习的重要性，这些上下文线索通过混合神经符号方法进行集成。这对于现实世界的应用至关重要，因为在其中准确且可扩展地识别有毒内容对于创建更安全的在线环境至关重要。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.12174v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种结合知识蒸馏（KD）和大视觉语言模型（LVLMs）知识注入的新框架，用于增强在仇恨言论检测中的有毒内容识别性能。该框架从ConceptNet中提取子知识图谱，注入紧凑的VLM框架内。有毒短语与标语和标语中的视觉概念的关联上下文增强了模型的推理能力。实验结果表明，与最先进的基线相比，我们的方法在AU-ROC、F1和召回率方面表现出卓越的性能，分别提高了1.1%、7%和35%。这表明结合显性和隐性上下文线索，通过混合神经符号方法学习的重要性。对于需要准确和可扩展地识别有毒内容的现实世界应用至关重要。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了一种结合知识蒸馏和知识注入的新框架，用于增强有毒内容识别的性能。</li>
<li>利用ConceptNet提取子知识图谱，并将其注入紧凑的VLM框架内。</li>
<li>有毒短语与标语和视觉概念的关联上下文增强了模型的推理能力。</li>
<li>实验结果表明，该方法在AU-ROC、F1和召回率方面优于现有技术。</li>
<li>与最先进的基线相比，该方法的性能提升显著，特别是在召回率方面。</li>
<li>该方法对于现实世界应用至关重要，需要准确和可扩展地识别有毒内容以创建更安全的在线环境。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.12174">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e846b665a42f37f53f9f1fdac2438ee6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b9f4946f51a6b3d70efc809dc73a0b0e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-29a5c0cfe23d8163d02b71f6b5f665de.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Simultaneous-Diarization-and-Separation-of-Meetings-through-the-Integration-of-Statistical-Mixture-Models"><a href="#Simultaneous-Diarization-and-Separation-of-Meetings-through-the-Integration-of-Statistical-Mixture-Models" class="headerlink" title="Simultaneous Diarization and Separation of Meetings through the   Integration of Statistical Mixture Models"></a>Simultaneous Diarization and Separation of Meetings through the   Integration of Statistical Mixture Models</h2><p><strong>Authors:Tobias Cord-Landwehr, Christoph Boeddeker, Reinhold Haeb-Umbach</strong></p>
<p>We propose an approach for simultaneous diarization and separation of meeting data. It consists of a complex Angular Central Gaussian Mixture Model (cACGMM) for speech source separation, and a von-Mises-Fisher Mixture Model (VMFMM) for diarization in a joint statistical framework. Through the integration, both spatial and spectral information are exploited for diarization and separation. We also develop a method for counting the number of active speakers in a segment of a meeting to support block-wise processing. While the total number of speakers in a meeting may be known, it is usually not known on a per-segment level. With the proposed speaker counting, joint diarization and source separation can be done segment-by-segment, and the permutation problem across segments is solved, thus allowing for block-online processing in the future. Experimental results on the LibriCSS meeting corpus show that the integrated approach outperforms a cascaded approach of diarization and speech enhancement in terms of WER, both on a per-segment and on a per-meeting level. </p>
<blockquote>
<p>我们提出了一种同时进行会议数据分化和分离的方法。它包含一个复杂的角中心高斯混合模型（cACGMM）用于语音源分离，以及一个冯·米塞斯·费舍尔混合模型（VMFMM）用于联合统计框架中的分化。通过整合，利用空间和频谱信息进行分化和分离。我们还开发了一种方法，用于计算会议片段中的活动发言人数，以支持分块处理。虽然会议的发言人数总数可能是已知的，但在每一段中通常并不知道。通过提出的发言人计数方法，可以分段进行联合分化和源分离，并解决跨段的排列问题，从而实现在未来的分块在线处理。在LibriCSS会议语料库上的实验结果表明，该集成方法相较于在分段和会议级别上的级联分化与语音增强方法在词错误率（WER）方面表现更优。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.21455v2">PDF</a> Accepted at ICASSP2025</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种同时进行会议数据分离与日程记录的方法。它采用复杂的角中心高斯混合模型（cACGMM）进行语音源分离，并采用冯米塞斯费希尔混合模型（VMFMM）进行联合统计框架中的日程安排。通过整合，利用空间和频谱信息进行日程安排和分离。此外，还开发了一种方法用于计算会议片段中的活动说话人数，以支持分块处理。实验结果表明，该方法在LibriCSS会议语料库上的表现优于级联的日程安排和语音增强方法，无论是在每段还是整个会议层面，词错误率（WER）都有所降低。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了同时实现会议数据分离与日程记录的方法。</li>
<li>采用了复杂的角中心高斯混合模型（cACGMM）进行语音源分离。</li>
<li>采用了冯米塞斯费希尔混合模型（VMFMM）进行日程安排。</li>
<li>通过整合，该方法能够利用空间和频谱信息。</li>
<li>开发了一种计算会议片段中活动说话人数的方法，支持分块处理。</li>
<li>该方法解决了跨片段的排列问题，为未来分块在线处理提供了可能。</li>
<li>在LibriCSS会议语料库上的实验表明，该方法优于级联的日程安排和语音增强方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.21455">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-6cb0b9cfbb25d02178586ca568214589.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f8e0317e0b2eb29c04adceb6088ded5f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7fedddccf8f8fc66cf55e9197a875a41.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Three-in-One-Fast-and-Accurate-Transducer-for-Hybrid-Autoregressive-ASR"><a href="#Three-in-One-Fast-and-Accurate-Transducer-for-Hybrid-Autoregressive-ASR" class="headerlink" title="Three-in-One: Fast and Accurate Transducer for Hybrid-Autoregressive ASR"></a>Three-in-One: Fast and Accurate Transducer for Hybrid-Autoregressive ASR</h2><p><strong>Authors:Hainan Xu, Travis M. Bartley, Vladimir Bataev, Boris Ginsburg</strong></p>
<p>We present Hybrid-Autoregressive INference TrANsducers (HAINAN), a novel architecture for speech recognition that extends the Token-and-Duration Transducer (TDT) model. Trained with randomly masked predictor network outputs, HAINAN supports both autoregressive inference with all network components and non-autoregressive inference without the predictor. Additionally, we propose a novel semi-autoregressive inference paradigm that first generates an initial hypothesis using non-autoregressive inference, followed by refinement steps where each token prediction is regenerated using parallelized autoregression on the initial hypothesis. Experiments on multiple datasets across different languages demonstrate that HAINAN achieves efficiency parity with CTC in non-autoregressive mode and with TDT in autoregressive mode. In terms of accuracy, autoregressive HAINAN outperforms TDT and RNN-T, while non-autoregressive HAINAN significantly outperforms CTC. Semi-autoregressive inference further enhances the model’s accuracy with minimal computational overhead, and even outperforms TDT results in some cases. These results highlight HAINAN’s flexibility in balancing accuracy and speed, positioning it as a strong candidate for real-world speech recognition applications. </p>
<blockquote>
<p>我们提出了混合自回归推理转换器（Hybrid-Autoregressive INference TrANsducers，简称HAINAN），这是一种用于语音识别的新型架构，它扩展了Token-and-Duration Transducer（TDT）模型。通过训练带有随机掩码的预测器网络输出，HAINAN支持包含所有网络组件的自回归推理和非自回归推理（无需预测器）。此外，我们还提出了一种新型半自回归推理范式，首先使用非自回归推理生成初始假设，然后通过并行自回归对初始假设进行精细化处理，重新生成每个符号的预测。在多语言数据集上的实验表明，HAINAN在非自回归模式下与CTC的效率相当，在自回归模式下与TDT的效率相当。在准确性方面，自回归的HAINAN优于TDT和RNN-T，而非自回归的HAINAN则显著优于CTC。半自回归推理进一步提高了模型的准确性，且计算开销最小，在某些情况下甚至超过了TDT的结果。这些结果凸显了HAINAN在平衡准确性和速度方面的灵活性，使其成为现实世界中语音识别应用的有力候选者。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.02597v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了Hybrid-Autoregressive INference TrANsducers（HAINAN）这一新型语音识别架构。HAINAN扩展了Token-and-Duration Transducer（TDT）模型，通过随机屏蔽预测网络输出来进行训练。它支持自回归推理和非自回归推理，并提出了半自回归推理新范式。实验表明，HAINAN在效率和准确性方面表现出色，具有在平衡准确率和速度方面的灵活性，适用于真实世界的语音识别应用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>HAINAN是一种新型的语音识别架构，基于Token-and-Duration Transducer（TDT）模型进行扩展。</li>
<li>HAINAN通过随机屏蔽预测网络输出来进行训练。</li>
<li>HAINAN支持自回归和非自回归两种推理模式。</li>
<li>提出了半自回归推理新范式，首先使用非自回归推理生成初步假设，然后进行基于并行自回归的精细化预测。</li>
<li>实验结果表明，HAINAN在效率和准确性方面表现出色。</li>
<li>自回归HAINAN在准确性方面优于TDT和RNN-T，非自回归HAINAN则显著优于CTC。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.02597">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-49766e8bf898c7aae614fc914b6de07e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-577fbd2ac9729abdec8417d49706cd43.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0eba685dcfd49c0ff99d4c900c71cefa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3bbcf0b3470dc249090ed21b92d400f1.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Vibravox-A-Dataset-of-French-Speech-Captured-with-Body-conduction-Audio-Sensors"><a href="#Vibravox-A-Dataset-of-French-Speech-Captured-with-Body-conduction-Audio-Sensors" class="headerlink" title="Vibravox: A Dataset of French Speech Captured with Body-conduction Audio   Sensors"></a>Vibravox: A Dataset of French Speech Captured with Body-conduction Audio   Sensors</h2><p><strong>Authors:Julien Hauret, Malo Olivier, Thomas Joubaud, Christophe Langrenne, Sarah Poirée, Véronique Zimpfer, Éric Bavu</strong></p>
<p>Vibravox is a dataset compliant with the General Data Protection Regulation (GDPR) containing audio recordings using five different body-conduction audio sensors : two in-ear microphones, two bone conduction vibration pickups and a laryngophone. The dataset also includes audio data from an airborne microphone used as a reference. The Vibravox corpus contains 45 hours of speech samples and physiological sounds recorded by 188 participants under different acoustic conditions imposed by an high order ambisonics 3D spatializer. Annotations about the recording conditions and linguistic transcriptions are also included in the corpus. We conducted a series of experiments on various speech-related tasks, including speech recognition, speech enhancement and speaker verification. These experiments were carried out using state-of-the-art models to evaluate and compare their performances on signals captured by the different audio sensors offered by the Vibravox dataset, with the aim of gaining a better grasp of their individual characteristics. </p>
<blockquote>
<p>Vibravox是一个符合通用数据保护条例（GDPR）要求的数据集，其中包含使用五种不同的身体传导音频传感器录制的音频记录：两个入耳式麦克风，两个骨传导振动拾音器和一个喉头麦克风。该数据集还包括作为参考的空气传播麦克风的音频数据。Vibravox语料库包含由188名参与者在由高级三维空间定位器施加的不同声学条件下录制的45小时语音样本和生理声音。语料库还包括关于录音条件的注释和语言转录。我们在各种语音相关任务上进行了一系列实验，包括语音识别、语音增强和说话人验证。这些实验使用最前沿的模型进行，旨在评估并比较这些模型在Vibravox数据集提供的不同音频传感器捕获的信号上的性能，从而更好地了解它们各自的特性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.11828v3">PDF</a> 23 pages, 42 figures</p>
<p><strong>总结</strong></p>
<p>Vibravox是一个符合通用数据保护条例（GDPR）规定的音频数据集，包含使用五种不同的身体传导音频传感器录制的音频记录，如两个入耳式麦克风、两个骨传导振动拾音器和一个喉头电话。该数据集还包括作为参考的空气传播麦克风的音频数据。Vibravox语料库包含由188名参与者在由高级三维空间化技术设定的高阶环境噪声条件下录制的45小时语音样本和生理声音。语料库还包括关于录音条件和语言转录的注释。我们进行了一系列关于各种语音相关任务的实验，包括语音识别、语音增强和说话人验证，以评估不同音频传感器在Vibravox数据集上的性能并比较其性能，目的是为了更好地了解它们各自的特性。</p>
<p><strong>要点分析</strong></p>
<ol>
<li>Vibravox是一个符合GDPR规定的音频数据集。</li>
<li>数据集使用了五种不同的身体传导音频传感器进行音频录制。</li>
<li>数据集包含45小时的语音样本和生理声音记录。</li>
<li>录音来自188名参与者，在不同的声学条件下进行。</li>
<li>语料库包含关于录音条件和语言转录的注释。</li>
<li>进行了一系列语音相关任务的实验，包括语音识别、语音增强和说话人验证。</li>
<li>实验的目的是评估并比较不同音频传感器的性能，以了解它们各自的特性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.11828">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3467608a99567db4b0a76b150150dd36.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f2a5adc05deb023863067537bbfb508d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e2c6e59cffb097de38fe4caa257ffe50.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7e7c5f6042c48e2396be947bd23eb48c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7585a935a3876282d7624769dbb8865c.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-26/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-26/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-26/GAN/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e286f6622218051d9f34b42aa6c0cfff.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN 方向最新论文已更新，请持续关注 Update in 2025-02-26  Diffusion Models for Tabular Data Challenges, Current Progress, and   Future Directions
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-26
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-26/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-69a3d0e8c8b00a7adcab939af91e6966.jpg" class="responsive-img" alt="无监督/半监督/对比学习">
                        
                        <span class="card-title">无监督/半监督/对比学习</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            无监督/半监督/对比学习 方向最新论文已更新，请持续关注 Update in 2025-02-26  Multi-view Hypergraph-based Contrastive Learning Model for Cold-Start   Micro-video Recommendation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-26
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    无监督/半监督/对比学习
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">无监督/半监督/对比学习</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">18799.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
