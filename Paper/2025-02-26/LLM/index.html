<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-26  Introducing Visual Perception Token into Multimodal Large Language Model">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-1dad80a4c1a092eeeaa1fdaccf907497.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-26
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    81 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-02-26-æ›´æ–°"><a href="#2025-02-26-æ›´æ–°" class="headerlink" title="2025-02-26 æ›´æ–°"></a>2025-02-26 æ›´æ–°</h1><h2 id="Introducing-Visual-Perception-Token-into-Multimodal-Large-Language-Model"><a href="#Introducing-Visual-Perception-Token-into-Multimodal-Large-Language-Model" class="headerlink" title="Introducing Visual Perception Token into Multimodal Large Language Model"></a>Introducing Visual Perception Token into Multimodal Large Language Model</h2><p><strong>Authors:Runpeng Yu, Xinyin Ma, Xinchao Wang</strong></p>
<p>To utilize visual information, Multimodal Large Language Model (MLLM) relies on the perception process of its vision encoder. The completeness and accuracy of visual perception significantly influence the precision of spatial reasoning, fine-grained understanding, and other tasks. However, MLLM still lacks the autonomous capability to control its own visual perception processes, for example, selectively reviewing specific regions of an image or focusing on information related to specific object categories. In this work, we propose the concept of Visual Perception Token, aiming to empower MLLM with a mechanism to control its visual perception processes. We design two types of Visual Perception Tokens, termed the Region Selection Token and the Vision Re-Encoding Token. MLLMs autonomously generate these tokens, just as they generate text, and use them to trigger additional visual perception actions. The Region Selection Token explicitly identifies specific regions in an image that require further perception, while the Vision Re-Encoding Token uses its hidden states as control signals to guide additional visual perception processes. Extensive experiments demonstrate the advantages of these tokens in handling spatial reasoning, improving fine-grained understanding, and other tasks. On average, the introduction of Visual Perception Tokens improves the performance of a 2B model by 23.6%, increasing its score from 0.572 to 0.708, and even outperforms a 7B parameter model by 13.4% (from 0.624). Please check out our repo <a target="_blank" rel="noopener" href="https://github.com/yu-rp/VisualPerceptionToken">https://github.com/yu-rp/VisualPerceptionToken</a> </p>
<blockquote>
<p>ä¸ºäº†åˆ©ç”¨è§†è§‰ä¿¡æ¯ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ä¾èµ–äºå…¶è§†è§‰ç¼–ç å™¨çš„æ„ŸçŸ¥è¿‡ç¨‹ã€‚è§†è§‰æ„ŸçŸ¥çš„å®Œæ•´æ€§å’Œå‡†ç¡®æ€§å¯¹ç©ºé—´æ¨ç†ã€ç²¾ç»†ç†è§£å’Œå…¶ä»–ä»»åŠ¡çš„ç²¾åº¦äº§ç”Ÿé‡å¤§å½±å“ã€‚ç„¶è€Œï¼ŒMLLMä»ç„¶ç¼ºä¹è‡ªä¸»æ§åˆ¶å…¶è§†è§‰æ„ŸçŸ¥è¿‡ç¨‹çš„èƒ½åŠ›ï¼Œä¾‹å¦‚ï¼Œé€‰æ‹©æ€§å›é¡¾å›¾åƒçš„ç‰¹å®šåŒºåŸŸæˆ–å…³æ³¨ä¸ç‰¹å®šå¯¹è±¡ç±»åˆ«ç›¸å…³çš„ä¿¡æ¯ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†è§†è§‰æ„ŸçŸ¥ä»¤ç‰Œçš„æ¦‚å¿µï¼Œæ—¨åœ¨èµ‹äºˆMLLMæ§åˆ¶å…¶è§†è§‰æ„ŸçŸ¥è¿‡ç¨‹çš„æœºåˆ¶ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸¤ç§ç±»å‹çš„è§†è§‰æ„ŸçŸ¥ä»¤ç‰Œï¼Œç§°ä¸ºåŒºåŸŸé€‰æ‹©ä»¤ç‰Œå’Œè§†è§‰é‡æ–°ç¼–ç ä»¤ç‰Œã€‚MLLMè‡ªä¸»åœ°ç”Ÿæˆè¿™äº›ä»¤ç‰Œï¼Œå°±åƒç”Ÿæˆæ–‡æœ¬ä¸€æ ·ï¼Œå¹¶ä½¿ç”¨å®ƒä»¬æ¥è§¦å‘é¢å¤–çš„è§†è§‰æ„ŸçŸ¥åŠ¨ä½œã€‚åŒºåŸŸé€‰æ‹©ä»¤ç‰Œæ˜¾å¼åœ°æ ‡è¯†å›¾åƒä¸­éœ€è¦è¿›ä¸€æ­¥æ„ŸçŸ¥çš„ç‰¹å®šåŒºåŸŸï¼Œè€Œè§†è§‰é‡æ–°ç¼–ç ä»¤ç‰Œåˆ™åˆ©ç”¨å…¶éšè—çŠ¶æ€ä½œä¸ºæ§åˆ¶ä¿¡å·æ¥å¼•å¯¼é¢å¤–çš„è§†è§‰æ„ŸçŸ¥è¿‡ç¨‹ã€‚å¤§é‡å®éªŒè¯æ˜äº†è¿™äº›ä»¤ç‰Œåœ¨å¤„ç†ç©ºé—´æ¨ç†ã€æé«˜ç²¾ç»†ç†è§£å’Œå…¶ä»–ä»»åŠ¡æ–¹é¢çš„ä¼˜åŠ¿ã€‚å¹³å‡è€Œè¨€ï¼Œå¼•å…¥è§†è§‰æ„ŸçŸ¥ä»¤ç‰Œå¯å°†2Bæ¨¡å‹æ€§èƒ½æé«˜23.6%ï¼Œå°†å…¶å¾—åˆ†ä»0.572æé«˜åˆ°0.708ï¼Œç”šè‡³è¶…è¶Š7Bå‚æ•°æ¨¡å‹è¾¾13.4%ï¼ˆä»0.624ï¼‰ã€‚è¯·è®¿é—®æˆ‘ä»¬çš„å­˜å‚¨åº“ <a target="_blank" rel="noopener" href="https://github.com/yu-rp/VisualPerceptionToken">https://github.com/yu-rp/VisualPerceptionToken</a> äº†è§£æ›´å¤šä¿¡æ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17425v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è§†è§‰æ„ŸçŸ¥ä¿¡æ¯å¯¹äºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå½“å‰MLLMä»æ— æ³•è‡ªä¸»æ§åˆ¶è§†è§‰æ„ŸçŸ¥è¿‡ç¨‹ã€‚æœ¬ç ”ç©¶æå‡ºäº†è§†è§‰æ„ŸçŸ¥ä»¤ç‰Œï¼ˆVisual Perception Tokenï¼‰çš„æ¦‚å¿µï¼Œæ—¨åœ¨èµ‹äºˆMLLMæ§åˆ¶å…¶è§†è§‰æ„ŸçŸ¥è¿‡ç¨‹çš„æœºåˆ¶ã€‚æœ¬ç ”ç©¶è®¾è®¡äº†ä¸¤ç§ç±»å‹çš„è§†è§‰æ„ŸçŸ¥ä»¤ç‰Œï¼šåŒºåŸŸé€‰æ‹©ä»¤ç‰Œå’Œè§†è§‰é‡æ–°ç¼–ç ä»¤ç‰Œã€‚åŒºåŸŸé€‰æ‹©ä»¤ç‰Œå¯æ˜ç¡®æ ‡è¯†å›¾åƒä¸­éœ€è¦è¿›ä¸€æ­¥æ„ŸçŸ¥çš„åŒºåŸŸï¼Œè€Œè§†è§‰é‡æ–°ç¼–ç ä»¤ç‰Œåˆ™åˆ©ç”¨å…¶éšè—çŠ¶æ€ä½œä¸ºæ§åˆ¶ä¿¡å·æ¥å¼•å¯¼é¢å¤–çš„è§†è§‰æ„ŸçŸ¥è¿‡ç¨‹ã€‚è¿™äº›ä»¤ç‰Œçš„åº”ç”¨èƒ½å¤Ÿæé«˜ç©ºé—´æ¨ç†ã€ç²¾ç»†ç²’åº¦ç†è§£ç­‰ä»»åŠ¡çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>MLLMçš„è§†è§‰æ„ŸçŸ¥èƒ½åŠ›å¯¹ç©ºé—´æ¨ç†å’Œç²¾ç»†ç²’åº¦ç†è§£ç­‰ä»»åŠ¡è‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰MLLMç¼ºä¹è‡ªä¸»æ§åˆ¶è§†è§‰æ„ŸçŸ¥è¿‡ç¨‹çš„æœºåˆ¶ã€‚</li>
<li>è§†è§‰æ„ŸçŸ¥ä»¤ç‰Œæ—¨åœ¨è§£å†³æ­¤é—®é¢˜ï¼Œå¹¶åˆ†ä¸ºåŒºåŸŸé€‰æ‹©ä»¤ç‰Œå’Œè§†è§‰é‡æ–°ç¼–ç ä»¤ç‰Œä¸¤ç§ç±»å‹ã€‚</li>
<li>åŒºåŸŸé€‰æ‹©ä»¤ç‰Œèƒ½å¤Ÿæ ‡è¯†éœ€è¦è¿›ä¸€æ­¥æ„ŸçŸ¥çš„å›¾åƒåŒºåŸŸã€‚</li>
<li>è§†è§‰é‡æ–°ç¼–ç ä»¤ç‰Œä½¿ç”¨éšè—çŠ¶æ€ä½œä¸ºæ§åˆ¶ä¿¡å·æ¥å¼•å¯¼é¢å¤–çš„è§†è§‰æ„ŸçŸ¥è¿‡ç¨‹ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¼•å…¥è§†è§‰æ„ŸçŸ¥ä»¤ç‰Œåï¼Œæ¨¡å‹æ€§èƒ½æ˜¾è‘—æé«˜ï¼Œç”šè‡³åœ¨æŸäº›æƒ…å†µä¸‹è¶…è¶Šäº†æ›´å¤§çš„æ¨¡å‹ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17425">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fe443e072642c2424ae3189e19fade6b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-969ea101c923254b142c46e66c3f70f8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b7d1117a397746e11d7d978cddd6080f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b47817c95b96e30eab7bfc6b427b819a.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="MLLMs-Know-Where-to-Look-Training-free-Perception-of-Small-Visual-Details-with-Multimodal-LLMs"><a href="#MLLMs-Know-Where-to-Look-Training-free-Perception-of-Small-Visual-Details-with-Multimodal-LLMs" class="headerlink" title="MLLMs Know Where to Look: Training-free Perception of Small Visual   Details with Multimodal LLMs"></a>MLLMs Know Where to Look: Training-free Perception of Small Visual   Details with Multimodal LLMs</h2><p><strong>Authors:Jiarui Zhang, Mahyar Khayatkhoei, Prateek Chhikara, Filip Ilievski</strong></p>
<p>Multimodal Large Language Models (MLLMs) have experienced rapid progress in visual recognition tasks in recent years. Given their potential integration into many critical applications, it is important to understand the limitations of their visual perception. In this work, we study whether MLLMs can perceive small visual details as effectively as large ones when answering questions about images. We observe that their performance is very sensitive to the size of the visual subject of the question, and further show that this effect is in fact causal by conducting an intervention study. Next, we study the attention patterns of MLLMs when answering visual questions, and intriguingly find that they consistently know where to look, even when they provide the wrong answer. Based on these findings, we then propose training-free visual intervention methods that leverage the internal knowledge of any MLLM itself, in the form of attention and gradient maps, to enhance its perception of small visual details. We evaluate our proposed methods on two widely-used MLLMs and seven visual question answering benchmarks and show that they can significantly improve MLLMsâ€™ accuracy without requiring any training. Our results elucidate the risk of applying MLLMs to visual recognition tasks concerning small details and indicate that visual intervention using the modelâ€™s internal state is a promising direction to mitigate this risk. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰è¯†åˆ«ä»»åŠ¡æ–¹é¢å–å¾—äº†å¿«é€Ÿè¿›å±•ã€‚é‰´äºå®ƒä»¬æœ‰å¯èƒ½èå…¥è®¸å¤šå…³é”®åº”ç”¨ï¼Œäº†è§£å®ƒä»¬åœ¨è§†è§‰æ„ŸçŸ¥æ–¹é¢çš„å±€é™æ€§éå¸¸é‡è¦ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶MLLMsåœ¨å›ç­”å…³äºå›¾åƒçš„é—®é¢˜æ—¶ï¼Œæ˜¯å¦èƒ½åƒæ„ŸçŸ¥å¤§å‹ç‰©ä½“ä¸€æ ·æœ‰æ•ˆåœ°æ„ŸçŸ¥å°åˆ°ç»†èŠ‚ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°å®ƒä»¬çš„æ€§èƒ½å¯¹é—®é¢˜ä¸­è§†è§‰ä¸»é¢˜çš„å¤§å°éå¸¸æ•æ„Ÿï¼Œå¹¶é€šè¿‡å¹²é¢„ç ”ç©¶è¿›ä¸€æ­¥è¡¨æ˜è¿™ç§å½±å“å®é™…ä¸Šæ˜¯å› æœçš„ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ç ”ç©¶MLLMsåœ¨å›ç­”è§†è§‰é—®é¢˜æ—¶æ³¨æ„åŠ›çš„åˆ†å¸ƒï¼Œå¹¶æƒŠå¥‡åœ°å‘ç°ï¼Œå³ä½¿ä»–ä»¬ç»™å‡ºé”™è¯¯çš„ç­”æ¡ˆï¼Œä»–ä»¬ä¹Ÿå§‹ç»ˆçŸ¥é“åº”è¯¥çœ‹å‘å“ªé‡Œã€‚åŸºäºè¿™äº›å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†åˆ©ç”¨ä»»ä½•MLLMæœ¬èº«çš„å†…éƒ¨çŸ¥è¯†ï¼ˆä»¥æ³¨æ„åŠ›å’Œæ¢¯åº¦å›¾çš„å½¢å¼ï¼‰è¿›è¡Œå…è®­ç»ƒè§†è§‰å¹²é¢„çš„æ–¹æ³•ï¼Œä»¥æé«˜å…¶å¯¹å°è§†è§‰ç»†èŠ‚çš„è®¤çŸ¥ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªå¹¿æ³›ä½¿ç”¨çš„MLLMså’Œä¸ƒä¸ªè§†è§‰é—®ç­”åŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°äº†æˆ‘ä»¬æå‡ºçš„æ–¹æ³•ï¼Œå¹¶è¡¨æ˜å®ƒä»¬åœ¨ä¸éœ€è¦ä»»ä½•è®­ç»ƒçš„æƒ…å†µä¸‹å¯ä»¥æ˜¾è‘—æé«˜MLLMsçš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœé˜æ˜äº†å°†MLLMsåº”ç”¨äºæ¶‰åŠå°ç»†èŠ‚çš„è§†è§‰è¯†åˆ«ä»»åŠ¡çš„é£é™©ï¼Œå¹¶è¡¨æ˜ä½¿ç”¨æ¨¡å‹å†…éƒ¨çŠ¶æ€è¿›è¡Œè§†è§‰å¹²é¢„æ˜¯ä¸€ä¸ªæœ‰å¸Œæœ›çš„ç¼“è§£è¿™ç§é£é™©çš„æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17422v1">PDF</a> Published as a conference paper at ICLR 2025. Code at:   <a target="_blank" rel="noopener" href="https://github.com/saccharomycetes/mllms_know">https://github.com/saccharomycetes/mllms_know</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å›¾åƒé—®ç­”ä¸­å¯¹å°è§†è§‰ç»†èŠ‚æ„ŸçŸ¥çš„èƒ½åŠ›ï¼Œå¹¶å‘ç°å…¶æ€§èƒ½å¯¹è§†è§‰ä¸»é¢˜çš„å¤§å°éå¸¸æ•æ„Ÿã€‚é€šè¿‡å¹²é¢„ç ”ç©¶å‘ç°ï¼Œè¿™ä¸€å½±å“å…·æœ‰å› æœæ€§ã€‚åŒæ—¶ï¼Œæœ¬æ–‡ç ”ç©¶äº†MLLMsåœ¨å›ç­”è§†è§‰é—®é¢˜æ—¶çš„æ³¨æ„åŠ›æ¨¡å¼ï¼Œå¹¶æå‡ºäº†åŸºäºæ¨¡å‹å†…éƒ¨çŸ¥è¯†ï¼ˆå¦‚æ³¨æ„åŠ›å’Œæ¢¯åº¦å›¾ï¼‰çš„æ— è®­ç»ƒè§†è§‰å¹²é¢„æ–¹æ³•ï¼Œä»¥æé«˜å…¶å¯¹å°è§†è§‰ç»†èŠ‚çš„æ„ŸçŸ¥èƒ½åŠ›ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œè¿™äº›æ–¹æ³•èƒ½æ˜¾è‘—æé«˜MLLMsçš„å‡†ç¡®ç‡ï¼Œä¸”æ— éœ€ä»»ä½•é¢å¤–è®­ç»ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLMsåœ¨å›¾åƒé—®ç­”ä¸­å¯¹å°è§†è§‰ç»†èŠ‚çš„æ„ŸçŸ¥èƒ½åŠ›å—åˆ°å…³æ³¨ã€‚</li>
<li>MLLMsæ€§èƒ½å—è§†è§‰ä¸»é¢˜å¤§å°çš„å½±å“ã€‚</li>
<li>é€šè¿‡å¹²é¢„ç ”ç©¶å‘ç°ï¼Œè§†è§‰ä¸»é¢˜å¤§å°å½±å“å…·æœ‰å› æœæ€§ã€‚</li>
<li>MLLMsåœ¨å›ç­”è§†è§‰é—®é¢˜æ—¶å…·æœ‰ç¨³å®šçš„æ³¨æ„åŠ›æ¨¡å¼ï¼Œå³ä½¿å›ç­”é”™è¯¯ä¹ŸçŸ¥é“åº”è¯¥å…³æ³¨å“ªé‡Œã€‚</li>
<li>æå‡ºäº†åŸºäºæ¨¡å‹å†…éƒ¨çŸ¥è¯†çš„æ— è®­ç»ƒè§†è§‰å¹²é¢„æ–¹æ³•ã€‚</li>
<li>å¹²é¢„æ–¹æ³•åŒ…æ‹¬åˆ©ç”¨æ³¨æ„åŠ›å’Œæ¢¯åº¦å›¾æ¥æé«˜MLLMså¯¹å°è§†è§‰ç»†èŠ‚çš„æ„ŸçŸ¥èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17422">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8b8e1825000b2a91c1ab495044341864.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-38ca5b06e037a468dcd9540487baa357.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-caeeb2dad0633321e956720569288219.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d66a3738a1818727723368d464455acc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-38de5908497e702a52af24348b7c4053.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2ecbea4b0b0ca24f1b98740acc778726.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="LongSpec-Long-Context-Speculative-Decoding-with-Efficient-Drafting-and-Verification"><a href="#LongSpec-Long-Context-Speculative-Decoding-with-Efficient-Drafting-and-Verification" class="headerlink" title="LongSpec: Long-Context Speculative Decoding with Efficient Drafting and   Verification"></a>LongSpec: Long-Context Speculative Decoding with Efficient Drafting and   Verification</h2><p><strong>Authors:Penghui Yang, Cunxiao Du, Fengzhuo Zhang, Haonan Wang, Tianyu Pang, Chao Du, Bo An</strong></p>
<p>Speculative decoding has become a promising technique to mitigate the high inference latency of autoregressive decoding in Large Language Models (LLMs). Despite its promise, the effective application of speculative decoding in LLMs still confronts three key challenges: the increasing memory demands of the draft model, the distribution shift between the short-training corpora and long-context inference, and inefficiencies in attention implementation. In this work, we enhance the performance of speculative decoding in long-context settings by addressing these challenges. First, we propose a memory-efficient draft model with a constant-sized Key-Value (KV) cache. Second, we introduce novel position indices for short-training data, enabling seamless adaptation from short-context training to long-context inference. Finally, we present an innovative attention aggregation method that combines fast implementations for prefix computation with standard attention for tree mask handling, effectively resolving the latency and memory inefficiencies of tree decoding. Our approach achieves strong results on various long-context tasks, including repository-level code completion, long-context summarization, and o1-like long reasoning tasks, demonstrating significant improvements in latency reduction. The code is available at <a target="_blank" rel="noopener" href="https://github.com/sail-sg/LongSpec">https://github.com/sail-sg/LongSpec</a>. </p>
<blockquote>
<p>æ¨æµ‹è§£ç å·²æˆä¸ºä¸€ç§æœ‰å‰é€”çš„æŠ€æœ¯ï¼Œç”¨äºç¼“è§£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­è‡ªå›å½’è§£ç çš„é«˜æ¨ç†å»¶è¿Ÿé—®é¢˜ã€‚å°½ç®¡å‰æ™¯çœ‹å¥½ï¼Œä½†åœ¨LLMä¸­æœ‰æ•ˆåº”ç”¨æ¨æµ‹è§£ç ä»é¢ä¸´ä¸‰ä¸ªå…³é”®æŒ‘æˆ˜ï¼šè‰ç¨¿æ¨¡å‹å¯¹å†…å­˜çš„æ—¥ç›Šéœ€æ±‚ã€çŸ­è®­ç»ƒè¯­æ–™åº“ä¸é•¿ä¸Šä¸‹æ–‡æ¨ç†ä¹‹é—´çš„åˆ†å¸ƒåç§»ä»¥åŠæ³¨æ„åŠ›å®ç°ä¸­çš„æ•ˆç‡ä½ä¸‹ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡è§£å†³è¿™äº›æŒ‘æˆ˜æé«˜äº†é•¿ä¸Šä¸‹æ–‡ç¯å¢ƒä¸­æ¨æµ‹è§£ç çš„æ€§èƒ½ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å…·æœ‰æ’å®šå¤§å°çš„é”®å€¼ï¼ˆKVï¼‰ç¼“å­˜çš„å†…å­˜é«˜æ•ˆè‰ç¨¿æ¨¡å‹ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬ä¸ºçŸ­è®­ç»ƒæ•°æ®å¼•å…¥äº†æ–°å‹ä½ç½®ç´¢å¼•ï¼Œå®ç°äº†ä»çŸ­ä¸Šä¸‹æ–‡è®­ç»ƒåˆ°é•¿ä¸Šä¸‹æ–‡æ¨ç†çš„æ— ç¼é€‚åº”ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ›æ–°çš„æ³¨æ„åŠ›èšåˆæ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†å¿«é€Ÿå‰ç¼€è®¡ç®—å®ç°ä¸ç”¨äºæ ‘æ©ç å¤„ç†çš„æ ‡å‡†æ³¨æ„åŠ›ï¼Œæœ‰æ•ˆåœ°è§£å†³äº†æ ‘è§£ç çš„å»¶è¿Ÿå’Œå†…å­˜æ•ˆç‡ä½ä¸‹é—®é¢˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å„ç§é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ä¸Šå–å¾—äº†è‰¯å¥½çš„æ•ˆæœï¼ŒåŒ…æ‹¬ä»“åº“çº§ä»£ç è¡¥å…¨ã€é•¿ä¸Šä¸‹æ–‡æ‘˜è¦å’Œo1ç±»ä¼¼çš„é•¿æ¨ç†ä»»åŠ¡ï¼Œè¯æ˜äº†åœ¨å»¶è¿Ÿé™ä½æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/sail-sg/LongSpec%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/sail-sg/LongSpecä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17421v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„æŠ•æœºè§£ç æŠ€æœ¯é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†é’ˆå¯¹è¿™äº›æŒ‘æˆ˜çš„è§£å†³æ–¹æ¡ˆã€‚é€šè¿‡ä¼˜åŒ–å†…å­˜æ•ˆç‡ã€é€‚åº”é•¿è¯­å¢ƒè®­ç»ƒå’Œå¼•å…¥æ–°å‹æ³¨æ„åŠ›èšåˆæ–¹æ³•ï¼Œæœ¬æ–‡æå‡ºçš„æ–¹æ¡ˆåœ¨å¤šç§é•¿è¯­å¢ƒä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æˆæœï¼ŒåŒ…æ‹¬ä»“åº“çº§ä»£ç è¡¥å…¨ã€é•¿è¯­å¢ƒæ‘˜è¦å’Œé•¿æ¨ç†ä»»åŠ¡ç­‰ã€‚æœ‰æ•ˆé™ä½äº†å»¶è¿Ÿå¹¶æé«˜æ€§èƒ½ã€‚ç›¸å…³ä»£ç å¯åœ¨GitHubä¸Šæ‰¾åˆ°ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æŠ•æœºè§£ç æ˜¯ç¼“è§£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é«˜æ¨æ–­å»¶è¿Ÿçš„ä¸€ç§æœ‰å‰é€”çš„æŠ€æœ¯ã€‚</li>
<li>æŠ•æœºè§£ç é¢ä¸´çš„å…³é”®æŒ‘æˆ˜åŒ…æ‹¬æ¨¡å‹å†…å­˜éœ€æ±‚çš„å¢åŠ ã€çŸ­è®­ç»ƒè¯­æ–™åº“ä¸é•¿è¯­å¢ƒæ¨æ–­çš„åˆ†å¸ƒåç§»ä»¥åŠæ³¨æ„åŠ›å®ç°çš„æ•ˆç‡é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§å†…å­˜é«˜æ•ˆçš„è‰æ¡ˆæ¨¡å‹ï¼Œå…·æœ‰æ’å®šå¤§å°çš„é”®å€¼ï¼ˆKVï¼‰ç¼“å­˜ï¼Œä»¥è§£å†³æ¨¡å‹å†…å­˜éœ€æ±‚é—®é¢˜ã€‚</li>
<li>å¼•å…¥æ–°å‹ä½ç½®ç´¢å¼•ç”¨äºçŸ­è®­ç»ƒæ•°æ®ï¼Œå®ç°ä»çŸ­è¯­å¢ƒè®­ç»ƒåˆ°é•¿è¯­å¢ƒæ¨æ–­çš„æ— ç¼é€‚åº”ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åˆ›æ–°çš„æ³¨æ„åŠ›èšåˆæ–¹æ³•ï¼Œç»“åˆäº†å¿«é€Ÿå‰ç¼€è®¡ç®—å’Œæ ‡å‡†æ³¨æ„åŠ›å¤„ç†æ ‘æ©ç ï¼Œè§£å†³äº†æ ‘è§£ç çš„å»¶è¿Ÿå’Œå†…å­˜æ•ˆç‡é—®é¢˜ã€‚</li>
<li>åœ¨å¤šç§é•¿è¯­å¢ƒä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æˆæœï¼ŒåŒ…æ‹¬ä»£ç è¡¥å…¨ã€é•¿è¯­å¢ƒæ‘˜è¦å’Œé•¿æ¨ç†ä»»åŠ¡ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17421">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fac4d74ce5ee8cbffca4282bccd1f4a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a249ff4fe208d206c98283d5dc29904f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89b6be4f708a6470743a59a9c64a091f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-20cabfb4a4269d12d3914c3bee5781f1.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="From-System-1-to-System-2-A-Survey-of-Reasoning-Large-Language-Models"><a href="#From-System-1-to-System-2-A-Survey-of-Reasoning-Large-Language-Models" class="headerlink" title="From System 1 to System 2: A Survey of Reasoning Large Language Models"></a>From System 1 to System 2: A Survey of Reasoning Large Language Models</h2><p><strong>Authors:Zhong-Zhi Li, Duzhen Zhang, Ming-Liang Zhang, Jiaxin Zhang, Zengyan Liu, Yuxuan Yao, Haotian Xu, Junhao Zheng, Pei-Jie Wang, Xiuyi Chen, Yingying Zhang, Fei Yin, Jiahua Dong, Zhijiang Guo, Le Song, Cheng-Lin Liu</strong></p>
<p>Achieving human-level intelligence requires refining the transition from the fast, intuitive System 1 to the slower, more deliberate System 2 reasoning. While System 1 excels in quick, heuristic decisions, System 2 relies on logical reasoning for more accurate judgments and reduced biases. Foundational Large Language Models (LLMs) excel at fast decision-making but lack the depth for complex reasoning, as they have not yet fully embraced the step-by-step analysis characteristic of true System 2 thinking. Recently, reasoning LLMs like OpenAIâ€™s o1&#x2F;o3 and DeepSeekâ€™s R1 have demonstrated expert-level performance in fields such as mathematics and coding, closely mimicking the deliberate reasoning of System 2 and showcasing human-like cognitive abilities. This survey begins with a brief overview of the progress in foundational LLMs and the early development of System 2 technologies, exploring how their combination has paved the way for reasoning LLMs. Next, we discuss how to construct reasoning LLMs, analyzing their features, the core methods enabling advanced reasoning, and the evolution of various reasoning LLMs. Additionally, we provide an overview of reasoning benchmarks, offering an in-depth comparison of the performance of representative reasoning LLMs. Finally, we explore promising directions for advancing reasoning LLMs and maintain a real-time \href{<a target="_blank" rel="noopener" href="https://github.com/zzli2022/Awesome-Slow-Reason-System%7D%7BGitHub">https://github.com/zzli2022/Awesome-Slow-Reason-System}{GitHub</a> Repository} to track the latest developments. We hope this survey will serve as a valuable resource to inspire innovation and drive progress in this rapidly evolving field. </p>
<blockquote>
<p>å®ç°äººç±»æ°´å¹³çš„æ™ºèƒ½éœ€è¦ç²¾è¿›ä»å¿«é€Ÿç›´è§‰ç³»ç»Ÿ1åˆ°è¾ƒæ…¢ã€æ›´æ…é‡çš„ç³»ç»Ÿ2æ¨ç†çš„è¿‡æ¸¡ã€‚ç³»ç»Ÿ1æ“…é•¿å¿«é€Ÿå¯å‘å¼å†³ç­–ï¼Œè€Œç³»ç»Ÿ2åˆ™ä¾èµ–äºé€»è¾‘æ¨ç†ä»¥åšå‡ºæ›´å‡†ç¡®çš„åˆ¤æ–­å’Œå‡å°‘åè§ã€‚åŸºç¡€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ“…é•¿å¿«é€Ÿå†³ç­–ï¼Œä½†åœ¨å¤æ‚æ¨ç†æ–¹é¢ç¼ºä¹æ·±åº¦ï¼Œå› ä¸ºå®ƒä»¬å°šæœªå®Œå…¨æ¥å—ç³»ç»Ÿ2æ€ç»´çš„é€æ­¥åˆ†æç‰¹å¾ã€‚æœ€è¿‘ï¼ŒåƒOpenAIçš„o1&#x2F;o3å’ŒDeepSeekçš„R1ç­‰æ¨ç†LLMåœ¨æ•°å­¦å’Œç¼–ç¨‹ç­‰é¢†åŸŸå·²ç»å±•ç°å‡ºä¸“å®¶çº§çš„æ€§èƒ½ï¼Œå®ƒä»¬èƒ½å¤Ÿå¾ˆå¥½åœ°æ¨¡ä»¿ç³»ç»Ÿ2çš„æ…é‡æ¨ç†å¹¶å±•ç¤ºå‡ºç±»ä¼¼äººç±»çš„è®¤çŸ¥èƒ½åŠ›ã€‚è¿™ç¯‡ç»¼è¿°é¦–å…ˆç®€è¦æ¦‚è¿°äº†åŸºç¡€LLMå’Œç³»ç»Ÿ2æŠ€æœ¯çš„æ—©æœŸå‘å±•è¿›å±•ï¼Œå¹¶æ¢è®¨äº†å®ƒä»¬çš„ç»“åˆå¦‚ä½•ä¸ºæ¨ç†LLMé“ºå¹³é“è·¯ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬è®¨è®ºå¦‚ä½•æ„å»ºæ¨ç†LLMï¼Œåˆ†æå…¶ç‰¹ç‚¹ã€æ”¯æŒé«˜çº§æ¨ç†çš„æ ¸å¿ƒæ–¹æ³•ä»¥åŠå„ç§æ¨ç†LLMçš„æ¼”å˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¦‚è¿°äº†æ¨ç†åŸºå‡†æµ‹è¯•ï¼Œæ·±å…¥æ¯”è¾ƒäº†ä»£è¡¨æ€§æ¨ç†LLMçš„æ€§èƒ½ã€‚æœ€åï¼Œæˆ‘ä»¬æ¢è®¨äº†æ¨åŠ¨æ¨ç†LLMå‘å±•çš„æœ‰å‰é€”çš„æ–¹å‘ï¼Œå¹¶é€šè¿‡GitHubä»“åº“å®æ—¶è·Ÿè¸ªæœ€æ–°è¿›å±•ã€‚æˆ‘ä»¬å¸Œæœ›è¿™ç¯‡ç»¼è¿°èƒ½ä½œä¸ºå®è´µèµ„æºï¼Œæ¿€å‘åˆ›æ–°å¹¶æ¨åŠ¨è¿™ä¸€å¿«é€Ÿæ¼”å˜é¢†åŸŸçš„è¿›æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17419v1">PDF</a> Slow-thinking, Large Language Models, Human-like Reasoning, Decision   Making in AI, AGI</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ–‡ç« æ¢è®¨äº†å®ç°äººç±»æ™ºèƒ½æ°´å¹³éœ€è¦å®ç°ä»å¿«é€Ÿç›´è§‰ç³»ç»Ÿä¸€ï¼ˆSystem 1ï¼‰åˆ°è¾ƒæ…¢ä½†æ›´æ·±æ€ç†Ÿè™‘çš„ç³»ç»ŸäºŒï¼ˆSystem 2ï¼‰æ¨ç†çš„è½¬å˜ã€‚ç³»ç»Ÿä¸€æ“…é•¿å¿«é€Ÿå†³ç­–ï¼Œè€Œç³»ç»ŸäºŒåˆ™ä¾èµ–é€»è¾‘æ¨ç†ä»¥åšå‡ºæ›´å‡†ç¡®ã€å‡å°‘åè§çš„åˆ¤æ–­ã€‚åŸºç¡€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ“…é•¿å¿«é€Ÿå†³ç­–ï¼Œä½†ç¼ºä¹å¤æ‚æ¨ç†çš„æ·±åº¦ï¼Œæœªèƒ½å®Œå…¨é‡‡ç”¨ç³»ç»ŸäºŒçš„é€æ­¥åˆ†æç‰¹æ€§ã€‚æœ€è¿‘ï¼Œå¦‚OpenAIçš„o1&#x2F;o3å’ŒDeepSeekçš„R1ç­‰æ¨ç†LLMå·²åœ¨æ•°å­¦å’Œç¼–ç ç­‰é¢†åŸŸå±•ç°å‡ºä¸“å®¶çº§è¡¨ç°ï¼Œæ¨¡ä»¿äº†ç³»ç»ŸäºŒçš„æ·±æ€ç†Ÿè™‘æ¨ç†å¹¶å±•ç¤ºäº†äººç±»èˆ¬çš„è®¤çŸ¥èƒ½åŠ›ã€‚æœ¬æ–‡é¦–å…ˆæ¦‚è¿°äº†åŸºç¡€LLMå’Œç³»ç»ŸäºŒæŠ€æœ¯çš„æ—©æœŸå‘å±•ï¼Œå¹¶æ¢è®¨äº†å®ƒä»¬çš„ç»“åˆå¦‚ä½•ä¸ºæ¨ç†LLMé“ºå¹³é“è·¯ã€‚æ¥ç€ï¼Œæˆ‘ä»¬è®¨è®ºäº†å¦‚ä½•æ„å»ºæ¨ç†LLMï¼Œåˆ†æäº†å®ƒä»¬çš„ç‰¹ç‚¹ã€æ ¸å¿ƒæ–¹æ³•å’Œå„ç§æ¨ç†LLMçš„æ¼”å˜ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æ¦‚è¿°äº†æ¨ç†åŸºå‡†æµ‹è¯•ï¼Œæ·±å…¥æ¯”è¾ƒäº†ä»£è¡¨æ€§æ¨ç†LLMçš„æ€§èƒ½ã€‚æœ€åï¼Œæˆ‘ä»¬æ¢è®¨äº†æ¨è¿›æ¨ç†LLMå‘å±•çš„æœ‰å‰é€”çš„æ–¹å‘ï¼Œå¹¶é€šè¿‡GitHubä»“åº“å®æ—¶è·Ÿè¸ªæœ€æ–°å‘å±•ã€‚æœ¬æ–‡æ—¨åœ¨ä¸ºè¿™ä¸€å¿«é€Ÿæ¼”å˜çš„é¢†åŸŸæä¾›æœ‰ä»·å€¼çš„èµ„æºï¼Œæ¿€å‘åˆ›æ–°å¹¶æ¨åŠ¨è¿›å±•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å®ç°äººç±»æ™ºèƒ½éœ€è¦ä»ç³»ç»Ÿä¸€çš„å¿«é€Ÿç›´è§‰å†³ç­–è½¬å˜ä¸ºç³»ç»ŸäºŒçš„ç¼“æ…¢ä¸”æ·±æ€ç†Ÿè™‘çš„æ¨ç†è¿‡ç¨‹ã€‚</li>
<li>ç³»ç»Ÿä¸€æ“…é•¿å¿«é€Ÿå†³ç­–ï¼Œè€Œç³»ç»ŸäºŒåˆ™ä¾§é‡äºé€»è¾‘æ¨ç†ä»¥å‡å°‘åè§ï¼Œæé«˜å‡†ç¡®æ€§ã€‚</li>
<li>åŸºç¡€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è™½æ“…é•¿å¿«é€Ÿå†³ç­–ï¼Œä½†åœ¨å¤æ‚æ¨ç†æ–¹é¢å­˜åœ¨å±€é™ï¼Œå°šæœªå®Œå…¨èå…¥ç³»ç»ŸäºŒçš„é€æ­¥åˆ†æç‰¹æ€§ã€‚</li>
<li>æ¨ç†LLMå¦‚OpenAIçš„o1&#x2F;o3å’ŒDeepSeekçš„R1å·²åœ¨ç‰¹å®šé¢†åŸŸå±•ç°å‡ºä¸“å®¶çº§è¡¨ç°ï¼Œæ¨¡ä»¿äº†ç³»ç»ŸäºŒçš„æ¨ç†è¿‡ç¨‹å¹¶å±•ç¤ºäº†äººç±»èˆ¬çš„è®¤çŸ¥èƒ½åŠ›ã€‚</li>
<li>æ–‡ç« æ¦‚è¿°äº†ä»åŸºç¡€LLMåˆ°æ¨ç†LLMçš„å‘å±•è¿‡ç¨‹ï¼ŒåŒ…æ‹¬å…¶ç‰¹ç‚¹ã€æ ¸å¿ƒæ–¹æ³•å’Œä¸åŒæ¨ç†LLMçš„æ¼”å˜ã€‚</li>
<li>æ¨ç†åŸºå‡†æµ‹è¯•æ˜¾ç¤ºï¼Œä»£è¡¨æ€§æ¨ç†LLMçš„æ€§èƒ½å­˜åœ¨å·®å¼‚ï¼Œè¿™ä¸ºè¯„ä¼°æ¨¡å‹æä¾›äº†æœ‰ä»·å€¼çš„æ¯”è¾ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17419">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-250a3e460a810ae8c621b4bfff393d4c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d99413c0978a3f337fcda955e880cbf3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-382ac5b44e9654c579e21aa157d538cb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fdb2cf03842476c4ce4efffcb56efcf4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f80803e8cebea72cacb8301cdf6cd23.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Reasoning-with-Latent-Thoughts-On-the-Power-of-Looped-Transformers"><a href="#Reasoning-with-Latent-Thoughts-On-the-Power-of-Looped-Transformers" class="headerlink" title="Reasoning with Latent Thoughts: On the Power of Looped Transformers"></a>Reasoning with Latent Thoughts: On the Power of Looped Transformers</h2><p><strong>Authors:Nikunj Saunshi, Nishanth Dikkala, Zhiyuan Li, Sanjiv Kumar, Sashank J. Reddi</strong></p>
<p>Large language models have shown remarkable reasoning abilities and scaling laws suggest that large parameter count, especially along the depth axis, is the primary driver. In this work, we make a stronger claim â€“ many reasoning problems require a large depth but not necessarily many parameters. This unlocks a novel application of looped models for reasoning. Firstly, we show that for many synthetic reasoning problems like addition, $p$-hop induction, and math problems, a $k$-layer transformer looped $L$ times nearly matches the performance of a $kL$-layer non-looped model, and is significantly better than a $k$-layer model. This is further corroborated by theoretical results showing that many such reasoning problems can be solved via iterative algorithms, and thus, can be solved effectively using looped models with nearly optimal depth. Perhaps surprisingly, these benefits also translate to practical settings of language modeling â€“ on many downstream reasoning tasks, a language model with $k$-layers looped $L$ times can be competitive to, if not better than, a $kL$-layer language model. In fact, our empirical analysis reveals an intriguing phenomenon: looped and non-looped models exhibit scaling behavior that depends on their effective depth, akin to the inference-time scaling of chain-of-thought (CoT) reasoning. We further elucidate the connection to CoT reasoning by proving that looped models implicitly generate latent thoughts and can simulate $T$ steps of CoT with $T$ loops. Inspired by these findings, we also present an interesting dichotomy between reasoning and memorization, and design a looping-based regularization that is effective on both fronts. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹å·²ç»å±•ç°å‡ºæ˜¾è‘—çš„æ¨ç†èƒ½åŠ›ï¼Œè€Œè§„æ¨¡å®šå¾‹è¡¨æ˜ï¼Œå¤§é‡çš„å‚æ•°ï¼Œå°¤å…¶æ˜¯æ·±åº¦æ–¹å‘çš„å‚æ•°ï¼Œæ˜¯å…¶ä¸»è¦é©±åŠ¨åŠ›ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ›´å¼ºçƒˆçš„è§‚ç‚¹â€”â€”è®¸å¤šæ¨ç†é—®é¢˜éœ€è¦çš„æ˜¯æ·±åº¦ï¼Œè€Œä¸ä»…ä»…æ˜¯å‚æ•°æ•°é‡ã€‚è¿™å¼€å¯äº†å¾ªç¯æ¨¡å‹åœ¨æ¨ç†æ–¹é¢çš„ä¸€ç§æ–°å‹åº”ç”¨ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è¯æ˜å¯¹äºè®¸å¤šåˆæˆæ¨ç†é—®é¢˜ï¼Œå¦‚åŠ æ³•ã€pé˜¶å½’çº³å’Œæ•°å­¦é—®é¢˜ï¼Œkå±‚å˜å‹å™¨å¾ªç¯Læ¬¡å‡ ä¹å¯ä»¥ä¸kLå±‚éå¾ªç¯æ¨¡å‹åŒ¹é…æ€§èƒ½ï¼Œå¹¶ä¸”æ˜æ˜¾ä¼˜äºkå±‚æ¨¡å‹ã€‚ç†è®ºç»“æœä¹Ÿè¿›ä¸€æ­¥è¯å®äº†è¿™ä¸€ç‚¹ï¼Œè¡¨æ˜è®¸å¤šæ­¤ç±»æ¨ç†é—®é¢˜å¯ä»¥é€šè¿‡è¿­ä»£ç®—æ³•è§£å†³ï¼Œå› æ­¤å¯ä»¥ä½¿ç”¨è¿‘ä¹æœ€ä¼˜æ·±åº¦çš„å¾ªç¯æ¨¡å‹æœ‰æ•ˆåœ°è§£å†³ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œè¿™äº›å¥½å¤„ä¹Ÿé€‚ç”¨äºè¯­è¨€å»ºæ¨¡çš„å®é™…åœºæ™¯â€”â€”åœ¨è®¸å¤šä¸‹æ¸¸æ¨ç†ä»»åŠ¡ä¸­ï¼Œå¾ªç¯kå±‚çš„è¯­è¨€æ¨¡å‹å¯èƒ½ä¸kLå±‚çš„è¯­è¨€æ¨¡å‹ä¸ç›¸ä¸Šä¸‹ï¼Œç”šè‡³æ›´å¥½ã€‚äº‹å®ä¸Šï¼Œæˆ‘ä»¬çš„å®è¯åˆ†ææ­ç¤ºäº†ä¸€ä¸ªæœ‰è¶£çš„ç°è±¡ï¼šå¾ªç¯å’Œéå¾ªç¯æ¨¡å‹çš„æ‰©å±•è¡Œä¸ºå–å†³äºå…¶æœ‰æ•ˆæ·±åº¦ï¼Œç±»ä¼¼äºæ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†çš„æ¨ç†æ—¶é—´æ‰©å±•ã€‚æˆ‘ä»¬é€šè¿‡è¯æ˜å¾ªç¯æ¨¡å‹å¯ä»¥éšå«åœ°ç”Ÿæˆæ½œåœ¨æ€ç»´ï¼Œå¹¶å¯ä»¥ä½¿ç”¨Tä¸ªå¾ªç¯æ¨¡æ‹ŸTæ­¥CoTæ¨ç†ï¼Œè¿›ä¸€æ­¥é˜æ˜äº†ä¸CoTæ¨ç†çš„è”ç³»ã€‚å—æ­¤å‘ç°çš„å¯å‘ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†æ¨ç†å’Œè®°å¿†ä¹‹é—´çš„æœ‰è¶£äºŒå…ƒæ€§ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§åŸºäºå¾ªç¯çš„æ­£åˆ™åŒ–æ–¹æ³•ï¼Œåœ¨ä¸¤ä¸ªæ–¹é¢éƒ½æœ‰æ•ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17416v1">PDF</a> ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹çš„å‡ºè‰²æ¨ç†èƒ½åŠ›ä¸»è¦å¾—ç›Šäºå…¶æ·±åº¦è€Œéå‚æ•°æ•°é‡ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œå¯¹äºè®¸å¤šåˆæˆæ¨ç†é—®é¢˜å’Œå®é™…è¯­è¨€å»ºæ¨¡ä»»åŠ¡ï¼Œå¾ªç¯æ¨¡å‹ï¼ˆå³åå¤æ‰§è¡ŒæŸä¸€æ“ä½œçš„æ¨¡å‹ï¼‰å¯ä»¥é€šè¿‡ä¼˜åŒ–æ·±åº¦æ¥å®ç°è¿‘ä¹æœ€ä½³çš„æ¨ç†æ•ˆæœã€‚æ­¤å¤–ï¼Œå¾ªç¯æ¨¡å‹å±•ç°å‡ºä¸€ç§ä¸â€œæ€ç»´é“¾â€ï¼ˆCoTï¼‰æ¨ç†ç›¸ä¼¼çš„æ¨æ–­æ—¶é—´å°ºåº¦è¡Œä¸ºã€‚æœ¬ç ”ç©¶çš„å¦ä¸€é‡è¦å‘ç°æ˜¯å¾ªç¯æ¨¡å‹å¯ä»¥æ¨¡æ‹ŸCoTçš„å¤šæ­¥æ¨ç†è¿‡ç¨‹ã€‚åŒæ—¶ï¼Œå¾ªç¯æ¨¡å‹åœ¨å¤„ç†æ¨ç†å’Œè®°å¿†ä»»åŠ¡æ—¶è¡¨ç°å‡ºæœ‰æ•ˆæ€§ä¸äºŒé‡æ€§ç‰¹å¾ã€‚æ€»ä¹‹ï¼Œæœ¬ç ”ç©¶ä¸ºè¯­è¨€æ¨¡å‹çš„ä¼˜åŒ–æä¾›äº†æ–°çš„è§†è§’å’Œæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ä¸»è¦ä¾èµ–æ·±åº¦è€Œéå‚æ•°æ•°é‡ã€‚æ·±åº¦æ‰©å±•æ˜¯å®ç°ä¼˜è´¨æ¨ç†çš„å…³é”®ã€‚</li>
<li>å¾ªç¯æ¨¡å‹èƒ½æœ‰æ•ˆè§£å†³åˆæˆæ¨ç†é—®é¢˜ï¼Œå¦‚åŠ æ³•ã€å½’çº³ç­‰æ•°å­¦é—®é¢˜ï¼Œå…¶æ€§èƒ½æ¥è¿‘ç”šè‡³è¶…è¶Šéå¾ªç¯æ¨¡å‹ã€‚</li>
<li>å¾ªç¯æ¨¡å‹é€‚ç”¨äºå®é™…è¯­è¨€å»ºæ¨¡ä»»åŠ¡ï¼Œè¡¨ç°å‡ºè‰¯å¥½çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å¾ªç¯æ¨¡å‹çš„æ€§èƒ½ä¸â€œæ€ç»´é“¾â€ï¼ˆCoTï¼‰æ¨ç†ç›¸ä¼¼ï¼Œå¯æ¨¡æ‹Ÿå¤šæ­¥æ¨ç†è¿‡ç¨‹ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17416">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-3a8ddc07a6a39e8a44defb48822e3137.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9f2036c06b44e8fe60fee6d3beb60f0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f5cbd8d6c9c5a7e1416003930fbf9158.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5682cd92c4b6e22bc331149b70326c19.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-05a3a041ae1e715f59d2472d22f7d83c.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="COSMOS-A-Hybrid-Adaptive-Optimizer-for-Memory-Efficient-Training-of-LLMs"><a href="#COSMOS-A-Hybrid-Adaptive-Optimizer-for-Memory-Efficient-Training-of-LLMs" class="headerlink" title="COSMOS: A Hybrid Adaptive Optimizer for Memory-Efficient Training of   LLMs"></a>COSMOS: A Hybrid Adaptive Optimizer for Memory-Efficient Training of   LLMs</h2><p><strong>Authors:Liming Liu, Zhenghao Xu, Zixuan Zhang, Hao Kang, Zichong Li, Chen Liang, Weizhu Chen, Tuo Zhao</strong></p>
<p>Large Language Models (LLMs) have demonstrated remarkable success across various domains, yet their optimization remains a significant challenge due to the complex and high-dimensional loss landscapes they inhabit. While adaptive optimizers such as AdamW are widely used, they suffer from critical limitations, including an inability to capture interdependencies between coordinates and high memory consumption. Subsequent research, exemplified by SOAP, attempts to better capture coordinate interdependence but incurs greater memory overhead, limiting scalability for massive LLMs. An alternative approach aims to reduce memory consumption through low-dimensional projection, but this leads to substantial approximation errors, resulting in less effective optimization (e.g., in terms of per-token efficiency). In this paper, we propose COSMOS, a novel hybrid optimizer that leverages the varying importance of eigensubspaces in the gradient matrix to achieve memory efficiency without compromising optimization performance. The design of COSMOS is motivated by our empirical insights and practical considerations. Specifically, COSMOS applies SOAP to the leading eigensubspace, which captures the primary optimization dynamics, and MUON to the remaining eigensubspace, which is less critical but computationally expensive to handle with SOAP. This hybrid strategy significantly reduces memory consumption while maintaining robust optimization performance, making it particularly suitable for massive LLMs. Numerical experiments on various datasets and transformer architectures are provided to demonstrate the effectiveness of COSMOS. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/lliu606/COSMOS">https://github.com/lliu606/COSMOS</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ä¸ªé¢†åŸŸéƒ½å–å¾—äº†æ˜¾è‘—çš„æˆå°±ï¼Œç„¶è€Œç”±äºå…¶å¤æ‚çš„ã€é«˜ç»´åº¦çš„æŸå¤±æ™¯è§‚ï¼Œå®ƒä»¬çš„ä¼˜åŒ–ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚è™½ç„¶åƒAdamWè¿™æ ·çš„è‡ªé€‚åº”ä¼˜åŒ–å™¨è¢«å¹¿æ³›åº”ç”¨ï¼Œä½†å®ƒä»¬å­˜åœ¨å…³é”®å±€é™æ€§ï¼ŒåŒ…æ‹¬æ— æ³•æ•æ‰åæ ‡ä¹‹é—´çš„äº’ä¾èµ–å…³ç³»ä»¥åŠé«˜å†…å­˜æ¶ˆè€—ã€‚åç»­çš„ç ”ç©¶ï¼Œå¦‚SOAPï¼Œè¯•å›¾æ›´å¥½åœ°æ•æ‰åæ ‡é—´çš„äº’ä¾èµ–æ€§ï¼Œä½†å´å¸¦æ¥äº†æ›´å¤§çš„å†…å­˜å¼€é”€ï¼Œé™åˆ¶äº†å¤§è§„æ¨¡LLMçš„å¯æ‰©å±•æ€§ã€‚å¦ä¸€ç§æ–¹æ³•æ—¨åœ¨é€šè¿‡ä½ç»´æŠ•å½±æ¥å‡å°‘å†…å­˜æ¶ˆè€—ï¼Œä½†è¿™ä¼šå¯¼è‡´å¤§é‡çš„è¿‘ä¼¼è¯¯å·®ï¼Œä»è€Œå¯¼è‡´ä¼˜åŒ–æ•ˆæœè¾ƒå·®ï¼ˆä¾‹å¦‚ï¼Œåœ¨æ¯æ¬¡ä»¤ç‰Œæ•ˆç‡æ–¹é¢ï¼‰ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†COSMOSï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ··åˆä¼˜åŒ–å™¨ï¼Œå®ƒåˆ©ç”¨æ¢¯åº¦çŸ©é˜µä¸­ä¸åŒç‰¹å¾å­ç©ºé—´çš„é‡è¦æ€§æ¥å®ç°å†…å­˜æ•ˆç‡ï¼Œè€Œä¸æŸå®³ä¼˜åŒ–æ€§èƒ½ã€‚COSMOSçš„è®¾è®¡å—åˆ°æˆ‘ä»¬å®è¯è§è§£å’Œå®é™…è€ƒè™‘çš„å¯å‘ã€‚å…·ä½“æ¥è¯´ï¼ŒCOSMOSå¯¹ä¸»è¦çš„ç‰¹å¾å­ç©ºé—´åº”ç”¨SOAPï¼Œæ•æ‰ä¸»è¦çš„ä¼˜åŒ–åŠ¨æ€ï¼Œå¹¶å¯¹å‰©ä½™çš„ç‰¹å¾å­ç©ºé—´åº”ç”¨MUONï¼Œè¿™éƒ¨åˆ†ç›¸å¯¹ä¸é‚£ä¹ˆå…³é”®ä½†ç”¨SOAPå¤„ç†è®¡ç®—æˆæœ¬è¾ƒé«˜ã€‚è¿™ç§æ··åˆç­–ç•¥æ˜¾è‘—å‡å°‘äº†å†…å­˜æ¶ˆè€—ï¼ŒåŒæ—¶ä¿æŒäº†ç¨³å¥çš„ä¼˜åŒ–æ€§èƒ½ï¼Œä½¿å…¶ç‰¹åˆ«é€‚åˆå¤§è§„æ¨¡LLMã€‚æˆ‘ä»¬åœ¨å„ç§æ•°æ®é›†å’Œè½¬æ¢å™¨æ¶æ„ä¸Šè¿›è¡Œäº†æ•°å€¼å®éªŒï¼Œä»¥è¯æ˜COSMOSçš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/lliu606/COSMOS%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/lliu606/COSMOSè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17410v1">PDF</a> 23 pages, 9 figures, 6 tables</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¼˜åŒ–é¢ä¸´å¤æ‚å¤šç»´çš„æŸå¤±æ™¯è§‚æŒ‘æˆ˜ã€‚ç°æœ‰ä¼˜åŒ–å™¨å¦‚AdamWè™½å¹¿æ³›åº”ç”¨ï¼Œä½†å­˜åœ¨æ— æ³•æ•æ‰åæ ‡é—´ä¾èµ–æ€§å’Œé«˜å†…å­˜æ¶ˆè€—ç­‰å±€é™æ€§ã€‚åç»­ç ”ç©¶å¦‚SOAPè¯•å›¾æ•æ‰åæ ‡ä¾èµ–æ€§ä½†å¢åŠ äº†å†…å­˜å¼€é”€ï¼Œé™åˆ¶äº†å¤§è§„æ¨¡LLMçš„æ‰©å±•æ€§ã€‚å¦ä¸€ç§æ–¹æ³•æ—¨åœ¨é€šè¿‡ä½ç»´æŠ•å½±å‡å°‘å†…å­˜æ¶ˆè€—ï¼Œä½†ä¼šå¯¼è‡´è¾ƒå¤§è¯¯å·®ï¼Œå½±å“ä¼˜åŒ–æ•ˆæœã€‚æœ¬æ–‡æå‡ºCOSMOSä¼˜åŒ–å™¨ï¼Œåˆ©ç”¨æ¢¯åº¦çŸ©é˜µä¸­ä¸åŒç‰¹å¾å­ç©ºé—´çš„é‡è¦æ€§æ¥å®ç°å†…å­˜æ•ˆç‡ä¸ä¼˜åŒ–æ€§èƒ½çš„å¹³è¡¡ã€‚COSMOSå¯¹ä¸»è¦ç‰¹å¾å­ç©ºé—´åº”ç”¨SOAPä¼˜åŒ–å™¨ï¼Œæ•æ‰ä¸»è¦ä¼˜åŒ–åŠ¨æ€ï¼Œå¯¹å‰©ä½™ç‰¹å¾å­ç©ºé—´åº”ç”¨MUONä¼˜åŒ–å™¨ã€‚è¿™ç§æ··åˆç­–ç•¥æ˜¾è‘—é™ä½äº†å†…å­˜æ¶ˆè€—ï¼ŒåŒæ—¶ä¿æŒäº†ç¨³å¥çš„ä¼˜åŒ–æ€§èƒ½ï¼Œå°¤å…¶é€‚ç”¨äºå¤§è§„æ¨¡LLMã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsçš„ä¼˜åŒ–é¢ä¸´å¤šç»´æŸå¤±æ™¯è§‚æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰ä¼˜åŒ–å™¨å¦‚AdamWå­˜åœ¨å±€é™æ€§ï¼Œæ— æ³•æœ‰æ•ˆæ•æ‰åæ ‡é—´ä¾èµ–æ€§å’Œé«˜å†…å­˜æ¶ˆè€—ã€‚</li>
<li>SOAPä¼˜åŒ–å™¨è¯•å›¾è§£å†³åæ ‡ä¾èµ–æ€§ä½†å¢åŠ äº†å†…å­˜å¼€é”€ã€‚</li>
<li>ä½ç»´æŠ•å½±æ–¹æ³•è™½å¯é™ä½å†…å­˜æ¶ˆè€—ï¼Œä½†ä¼šå¯¼è‡´è¾ƒå¤§è¯¯å·®ï¼Œå½±å“ä¼˜åŒ–æ•ˆæœã€‚</li>
<li>COSMOSæ˜¯ä¸€ç§æ–°å‹æ··åˆä¼˜åŒ–å™¨ï¼Œç»“åˆSOAPå’ŒMUONä¼˜åŒ–å™¨ï¼Œé’ˆå¯¹ç‰¹å¾å­ç©ºé—´è¿›è¡Œä¸åŒå¤„ç†ã€‚</li>
<li>COSMOSåœ¨é™ä½å†…å­˜æ¶ˆè€—çš„åŒæ—¶ä¿æŒäº†ä¼˜åŒ–æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17410">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-635c8a68fe819c6977315311c8180107.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Large-Language-Models-are-Powerful-EHR-Encoders"><a href="#Large-Language-Models-are-Powerful-EHR-Encoders" class="headerlink" title="Large Language Models are Powerful EHR Encoders"></a>Large Language Models are Powerful EHR Encoders</h2><p><strong>Authors:Stefan Hegselmann, Georg von Arnim, Tillmann Rheude, Noel Kronenberg, David Sontag, Gerhard Hindricks, Roland Eils, Benjamin Wild</strong></p>
<p>Electronic Health Records (EHRs) offer rich potential for clinical prediction, yet their inherent complexity and heterogeneity pose significant challenges for traditional machine learning approaches. Domain-specific EHR foundation models trained on large collections of unlabeled EHR data have demonstrated promising improvements in predictive accuracy and generalization; however, their training is constrained by limited access to diverse, high-quality datasets and inconsistencies in coding standards and healthcare practices. In this study, we explore the possibility of using general-purpose Large Language Models (LLMs) based embedding methods as EHR encoders. By serializing patient records into structured Markdown text, transforming codes into human-readable descriptors, we leverage the extensive generalization capabilities of LLMs pretrained on vast public corpora, thereby bypassing the need for proprietary medical datasets. We systematically evaluate two state-of-the-art LLM-embedding models, GTE-Qwen2-7B-Instruct and LLM2Vec-Llama3.1-8B-Instruct, across 15 diverse clinical prediction tasks from the EHRSHOT benchmark, comparing their performance to an EHRspecific foundation model, CLIMBR-T-Base, and traditional machine learning baselines. Our results demonstrate that LLM-based embeddings frequently match or exceed the performance of specialized models, even in few-shot settings, and that their effectiveness scales with the size of the underlying LLM and the available context window. Overall, our findings demonstrate that repurposing LLMs for EHR encoding offers a scalable and effective approach for clinical prediction, capable of overcoming the limitations of traditional EHR modeling and facilitating more interoperable and generalizable healthcare applications. </p>
<blockquote>
<p>ç”µå­å¥åº·è®°å½•ï¼ˆEHRsï¼‰åœ¨ä¸´åºŠé¢„æµ‹æ–¹é¢å…·æœ‰å·¨å¤§çš„æ½œåŠ›ï¼Œç„¶è€Œå…¶å›ºæœ‰çš„å¤æ‚æ€§å’Œå¼‚è´¨æ€§ç»™ä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ æ–¹æ³•å¸¦æ¥äº†å·¨å¤§çš„æŒ‘æˆ˜ã€‚é’ˆå¯¹ç‰¹å®šé¢†åŸŸçš„EHRåŸºç¡€æ¨¡å‹é€šè¿‡åœ¨å¤§é‡æœªæ ‡è®°çš„EHRæ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå·²ç»åœ¨é¢„æµ‹ç²¾åº¦å’Œé€šç”¨æ€§æ–¹é¢æ˜¾ç¤ºå‡ºæœ‰å¸Œæœ›çš„æ”¹è¿›ï¼›ç„¶è€Œï¼Œå®ƒä»¬çš„è®­ç»ƒå—åˆ°å¤šæ ·ã€é«˜è´¨é‡æ•°æ®é›†è®¿é—®æœ‰é™ä»¥åŠç¼–ç æ ‡å‡†å’ŒåŒ»ç–—å®è·µä¸ä¸€è‡´çš„åˆ¶çº¦ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†ä½¿ç”¨åŸºäºé€šç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„åµŒå…¥æ–¹æ³•ä½œä¸ºEHRç¼–ç å™¨çš„å¯èƒ½æ€§ã€‚é€šè¿‡å°†æ‚£è€…è®°å½•åºåˆ—åŒ–ä¸ºç»“æ„åŒ–Markdownæ–‡æœ¬ï¼Œå°†ä»£ç è½¬æ¢ä¸ºäººç±»å¯è¯»çš„æè¿°ç¬¦ï¼Œæˆ‘ä»¬å……åˆ†åˆ©ç”¨äº†LLMsåœ¨å¤§é‡å…¬å…±è¯­æ–™åº“ä¸Šçš„é¢„è®­ç»ƒå¸¦æ¥çš„å¹¿æ³›æ³›åŒ–èƒ½åŠ›ï¼Œä»è€Œç»•è¿‡äº†å¯¹ä¸“æœ‰åŒ»ç–—æ•°æ®é›†çš„éœ€æ±‚ã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°è¯„ä¼°äº†ä¸¤ç§æœ€å…ˆè¿›çš„LLMåµŒå…¥æ¨¡å‹ï¼ŒGTE-Qwen2-7B-Instructå’ŒLLM2Vec-Llama3.1-8B-Instructï¼Œåœ¨EHRSHOTåŸºå‡†æµ‹è¯•çš„15ä¸ªä¸åŒä¸´åºŠé¢„æµ‹ä»»åŠ¡ä¸Šï¼Œä¸EHRç‰¹å®šåŸºç¡€æ¨¡å‹CLIMBR-T-Baseå’Œä¼ ç»Ÿæœºå™¨å­¦ä¹ åŸºå‡†è¿›è¡Œäº†æ€§èƒ½æ¯”è¾ƒã€‚ç»“æœè¡¨æ˜ï¼ŒåŸºäºLLMçš„åµŒå…¥é€šå¸¸ä¸ä¸“ç”¨æ¨¡å‹çš„æ€§èƒ½ç›¸åŒ¹é…æˆ–è¶…è¿‡ï¼Œå³ä½¿åœ¨å°‘é‡æ ·æœ¬æƒ…å†µä¸‹ä¹Ÿæ˜¯å¦‚æ­¤ï¼Œå…¶æœ‰æ•ˆæ€§éšç€åŸºç¡€LLMçš„å¤§å°å’Œå¯ç”¨ä¸Šä¸‹æ–‡çª—å£è€Œæ‰©å¤§ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œå°†LLMsé‡æ–°ç”¨äºEHRç¼–ç æä¾›äº†ä¸€ç§å¯æ‰©å±•å’Œæœ‰æ•ˆçš„ä¸´åºŠé¢„æµ‹æ–¹æ³•ï¼Œèƒ½å¤Ÿå…‹æœä¼ ç»ŸEHRå»ºæ¨¡çš„é™åˆ¶ï¼Œä¿ƒè¿›æ›´äº’è”å’Œæ›´é€šç”¨çš„åŒ»ç–—åº”ç”¨ç¨‹åºçš„å¼€å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17403v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†ç”µå­å¥åº·è®°å½•ï¼ˆEHRsï¼‰åœ¨ä¸´åºŠé¢„æµ‹ä¸­çš„ä¸°å¯Œæ½œåŠ›ï¼Œä»¥åŠä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•é¢ä¸´çš„æŒ‘æˆ˜ã€‚ç ”ç©¶é€šè¿‡é‡‡ç”¨åŸºäºå¤§å‹å…¬å…±è¯­æ–™åº“çš„é€šç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä½œä¸ºEHRç¼–ç å™¨æ¥è§£å†³é—®é¢˜ã€‚é€šè¿‡ç»“æ„åŒ–Markdownæ–‡æœ¬å¯¹æ‚£è€…è®°å½•è¿›è¡Œåºåˆ—åŒ–ï¼Œå°†ä»£ç è½¬æ¢ä¸ºäººç±»å¯è¯»çš„æè¿°ç¬¦ï¼Œä»è€Œç»•è¿‡å¯¹ä¸“æœ‰åŒ»ç–—æ•°æ®é›†çš„éœ€æ±‚ã€‚ç³»ç»Ÿè¯„ä¼°è¡¨æ˜ï¼ŒLLMåµŒå…¥æ¨¡å‹åœ¨EHRSHOTåŸºå‡†æµ‹è¯•çš„15é¡¹ä¸åŒä¸´åºŠé¢„æµ‹ä»»åŠ¡ä¸­çš„æ€§èƒ½ä¸ç‰¹å®šäºEHRçš„åŸºç¡€æ¨¡å‹ç›¸æ¯”è¡¨ç°è‰¯å¥½ï¼Œç”šè‡³åœ¨æŸäº›æƒ…å†µä¸‹è¡¨ç°æ›´å¥½ã€‚è¿™è¡¨æ˜å°†LLMsé‡æ–°ç”¨äºEHRç¼–ç æ˜¯ä¸€ç§å¯æ‰©å±•ä¸”æœ‰æ•ˆçš„ä¸´åºŠé¢„æµ‹æ–¹æ³•ï¼Œèƒ½å¤Ÿå…‹æœä¼ ç»ŸEHRå»ºæ¨¡çš„é™åˆ¶ï¼Œä¿ƒè¿›æ›´å…·äº’é€šæ€§å’Œæ³›åŒ–èƒ½åŠ›çš„åŒ»ç–—ä¿å¥åº”ç”¨ç¨‹åºçš„å¼€å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”µå­å¥åº·è®°å½•ï¼ˆEHRsï¼‰åœ¨ä¸´åºŠé¢„æµ‹ä¸­å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œä½†æ•°æ®å¤æ‚æ€§å’Œå¼‚è´¨æ€§ç»™ä¼ ç»Ÿæœºå™¨å­¦ä¹ å¸¦æ¥æŒ‘æˆ˜ã€‚</li>
<li>ç‰¹å®šäºEHRçš„åŸºç¡€æ¨¡å‹åœ¨é¢„æµ‹å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›æ–¹é¢æ˜¾ç¤ºå‡ºæ”¹è¿›çš„å¸Œæœ›ï¼Œä½†å—é™äºæ•°æ®é›†çš„å¤šæ ·æ€§å’Œè´¨é‡ä»¥åŠç¼–ç æ ‡å‡†å’ŒåŒ»ç–—å®è·µçš„ä¸ä¸€è‡´æ€§ã€‚</li>
<li>ç ”ç©¶ä½¿ç”¨é€šç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä½œä¸ºEHRç¼–ç å™¨çš„æ–°æ–¹æ³•ï¼Œåˆ©ç”¨LLMsåœ¨å¤§é‡å…¬å…±è¯­æ–™åº“ä¸Šçš„é¢„è®­ç»ƒè¿›è¡Œç»“æ„åŒ–æ•°æ®å¤„ç†ã€‚</li>
<li>é€šè¿‡å°†æ‚£è€…è®°å½•åºåˆ—åŒ–ä¸ºç»“æ„åŒ–Markdownæ–‡æœ¬å¹¶è½¬æ¢ä»£ç ä¸ºå¯è¯»æè¿°ç¬¦ï¼Œç»•è¿‡å¯¹ä¸“æœ‰åŒ»ç–—æ•°æ®é›†çš„éœ€æ±‚ã€‚</li>
<li>LLMåµŒå…¥æ¨¡å‹åœ¨å¤šç§ä¸´åºŠé¢„æµ‹ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ï¼Œä¸ç‰¹å®šäºEHRçš„æ¨¡å‹ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ï¼Œç”šè‡³åœ¨å°‘æ ·æœ¬æƒ…å†µä¸‹ä¹Ÿèƒ½åŒ¹é…æˆ–è¶…è¶Šå…¶æ€§èƒ½ã€‚</li>
<li>LLMçš„æœ‰æ•ˆæ€§éšç€æ¨¡å‹å¤§å°å’Œå¯ç”¨ä¸Šä¸‹æ–‡çª—å£çš„å¤§å°è€Œå¢åŠ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17403">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-74b0a518a4b17bd9f2ad4616dbe81fcc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cfedd0a7cb65190d72a64a7503e8e23c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89591a01aaa7b4088fef629c9c365d1d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d61db9bbf53dcfc812bf0a801e742f26.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bf61cdd8860ed37ba16bc3d176e4343a.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="On-Relation-Specific-Neurons-in-Large-Language-Models"><a href="#On-Relation-Specific-Neurons-in-Large-Language-Models" class="headerlink" title="On Relation-Specific Neurons in Large Language Models"></a>On Relation-Specific Neurons in Large Language Models</h2><p><strong>Authors:Yihong Liu, Runsheng Chen, Lea Hirlimann, Ahmad Dawar Hakimi, Mingyang Wang, Amir Hossein Kargaran, Sascha Rothe, FranÃ§ois Yvon, Hinrich SchÃ¼tze</strong></p>
<p>In large language models (LLMs), certain neurons can store distinct pieces of knowledge learned during pretraining. While knowledge typically appears as a combination of relations and entities, it remains unclear whether some neurons focus on a relation itself â€“ independent of any entity. We hypothesize such neurons detect a relation in the input text and guide generation involving such a relation. To investigate this, we study the Llama-2 family on a chosen set of relations with a statistics-based method. Our experiments demonstrate the existence of relation-specific neurons. We measure the effect of selectively deactivating candidate neurons specific to relation $r$ on the LLMâ€™s ability to handle (1) facts whose relation is $r$ and (2) facts whose relation is a different relation $râ€™ \neq r$. With respect to their capacity for encoding relation information, we give evidence for the following three properties of relation-specific neurons. $\textbf{(i) Neuron cumulativity.}$ The neurons for $r$ present a cumulative effect so that deactivating a larger portion of them results in the degradation of more facts in $r$. $\textbf{(ii) Neuron versatility.}$ Neurons can be shared across multiple closely related as well as less related relations. Some relation neurons transfer across languages. $\textbf{(iii) Neuron interference.}$ Deactivating neurons specific to one relation can improve LLM generation performance for facts of other relations. We will make our code publicly available at <a target="_blank" rel="noopener" href="https://github.com/cisnlp/relation-specific-neurons">https://github.com/cisnlp/relation-specific-neurons</a>. </p>
<blockquote>
<p>åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­ï¼ŒæŸäº›ç¥ç»å…ƒå¯ä»¥å­˜å‚¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­å­¦åˆ°çš„ä¸åŒçŸ¥è¯†ç‰‡æ®µã€‚è™½ç„¶çŸ¥è¯†é€šå¸¸ä»¥å…³ç³»å’Œå®ä½“çš„ç»„åˆå½¢å¼å‡ºç°ï¼Œä½†å°šä¸æ¸…æ¥šæŸäº›ç¥ç»å…ƒæ˜¯å¦ä¸“æ³¨äºå…³ç³»æœ¬èº«â€”â€”ç‹¬ç«‹äºä»»ä½•å®ä½“ã€‚æˆ‘ä»¬å‡è®¾è¿™æ ·çš„ç¥ç»å…ƒå¯ä»¥æ£€æµ‹è¾“å…¥æ–‡æœ¬ä¸­çš„å…³ç³»ï¼Œå¹¶å¼•å¯¼æ¶‰åŠè¿™ç§å…³ç³»çš„ç”Ÿæˆã€‚ä¸ºäº†è°ƒæŸ¥è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬ä½¿ç”¨ç»Ÿè®¡æ–¹æ³•åœ¨ç ”ç©¶é€‰å®šçš„ä¸€ç»„å…³ç³»ä¸Šç ”ç©¶Llama-2å®¶æ—ã€‚æˆ‘ä»¬çš„å®éªŒè¯æ˜äº†ç‰¹å®šå…³ç³»ç¥ç»å…ƒçš„å­˜åœ¨ã€‚æˆ‘ä»¬æµ‹é‡é€‰æ‹©æ€§åœç”¨ç‰¹å®šäºå…³ç³»$r$çš„å€™é€‰ç¥ç»å…ƒå¯¹LLMå¤„ç†ï¼ˆ1ï¼‰å…³ç³»ä¸º$r$çš„äº‹å®å’Œï¼ˆ2ï¼‰å…³ç³»ä¸ºä¸åŒå…³ç³»$râ€™ \neq r$çš„äº‹å®çš„èƒ½åŠ›çš„å½±å“ã€‚å…³äºå…¶ç¼–ç å…³ç³»ä¿¡æ¯çš„èƒ½åŠ›ï¼Œæˆ‘ä»¬ä¸ºç‰¹å®šå…³ç³»ç¥ç»å…ƒçš„ä»¥ä¸‹ä¸‰ä¸ªå±æ€§æä¾›è¯æ®ã€‚$\textbf{(i) ç¥ç»å…ƒç´¯ç§¯æ€§ã€‚}$é’ˆå¯¹å…³ç³»$r$çš„ç¥ç»å…ƒå…·æœ‰ç´¯ç§¯æ•ˆåº”ï¼Œåœç”¨å…¶ä¸­æ›´å¤šçš„ç¥ç»å…ƒä¼šå¯¼è‡´æ›´å¤šå…³äº$r$çš„äº‹å®é€€åŒ–ã€‚$\textbf{(ii) ç¥ç»å…ƒå¤šåŠŸèƒ½æ€§ã€‚}$ç¥ç»å…ƒå¯ä»¥åœ¨å¤šä¸ªå¯†åˆ‡ç›¸å…³ä»¥åŠä¸å¤ªç›¸å…³çš„å…³ç³»ä¹‹é—´å…±äº«ã€‚æŸäº›å…³ç³»ç¥ç»å…ƒå¯ä»¥åœ¨ä¸åŒè¯­è¨€é—´è½¬ç§»ã€‚$\textbf{(iii) ç¥ç»å…ƒå¹²æ‰°ã€‚}$åœç”¨ç‰¹å®šäºæŸä¸€å…³ç³»çš„ç¥ç»å…ƒå¯èƒ½ä¼šæé«˜LLMå¯¹å…¶å®ƒå…³ç³»äº‹å®ç”Ÿæˆçš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/cisnlp/relation-specific-neurons">https://github.com/cisnlp/relation-specific-neurons</a>ä¸Šå…¬å¼€æä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17355v1">PDF</a> preprint</p>
<p><strong>æ‘˜è¦</strong></p>
<p>åœ¨å¤§è¯­è¨€æ¨¡å‹ä¸­ï¼ŒæŸäº›ç¥ç»å…ƒèƒ½å¤Ÿå­˜å‚¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­å­¦åˆ°çš„ä¸åŒçŸ¥è¯†ç‰‡æ®µã€‚çŸ¥è¯†é€šå¸¸ä»¥å…³ç³»å’Œå®ä½“çš„ç»„åˆå½¢å¼å‡ºç°ï¼Œä½†ç›®å‰å°šä¸æ¸…æ¥šç¥ç»å…ƒæ˜¯å¦ç‹¬ç«‹äºä»»ä½•å®ä½“å…³æ³¨äºå…³ç³»æœ¬èº«ã€‚æˆ‘ä»¬å‡è®¾è¿™ç±»ç¥ç»å…ƒèƒ½å¤Ÿåœ¨è¾“å…¥æ–‡æœ¬ä¸­æ£€æµ‹å…³ç³»å¹¶å¼•å¯¼æ¶‰åŠæ­¤ç±»å…³ç³»çš„ç”Ÿæˆã€‚ä¸ºäº†è°ƒæŸ¥è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬åœ¨Llama-2å®¶æ—ä¸Šé‡‡ç”¨åŸºäºç»Ÿè®¡çš„æ–¹æ³•ï¼Œå¯¹é€‰å®šçš„ä¸€ç»„å…³ç³»è¿›è¡Œç ”ç©¶ã€‚å®éªŒè¯æ˜äº†å­˜åœ¨ç‰¹å®šå…³ç³»çš„ç¥ç»å…ƒã€‚æˆ‘ä»¬æµ‹é‡äº†é€‰æ‹©æ€§åœç”¨ä¸å…³ç³»$r$ç›¸å…³çš„å€™é€‰ç¥ç»å…ƒå¯¹è¯­è¨€æ¨¡å‹å¤„ç†å…³ç³»$r$çš„äº‹å®ä»¥åŠå…³ç³»$râ€™\neq r$çš„äº‹å®çš„å½±å“ã€‚å…³äºå…¶ç¼–ç å…³ç³»ä¿¡æ¯çš„èƒ½åŠ›ï¼Œæˆ‘ä»¬ä¸ºç‰¹å®šå…³ç³»çš„ç¥ç»å…ƒæä¾›äº†ä»¥ä¸‹ä¸‰ä¸ªå±æ€§çš„è¯æ®ã€‚ç¥ç»å…ƒå…·æœ‰ç´¯ç§¯æ€§æ•ˆåº”ï¼Œåœç”¨æ›´å¤šçš„ç¥ç»å…ƒä¼šå¯¼è‡´æ›´å¤šäº‹å®åœ¨å…³ç³»$r$ä¸­é€€åŒ–ï¼›ç¥ç»å…ƒå…·æœ‰é€šç”¨æ€§ï¼Œå¯ä»¥è·¨å¤šä¸ªå…³ç³»å¯†åˆ‡ä»¥åŠä¸å¤ªç›¸å…³çš„å…³ç³»å…±äº«ä½¿ç”¨ï¼ŒæŸäº›å…³ç³»ç¥ç»å…ƒç”šè‡³è·¨è¯­è¨€é€šç”¨ï¼›åœç”¨ç‰¹å®šäºä¸€ä¸ªå…³ç³»çš„ç¥ç»å…ƒå¯èƒ½ä¼šæé«˜è¯­è¨€æ¨¡å‹å¯¹å…¶ä»–å…³ç³»äº‹å®çš„å¤„ç†èƒ½åŠ›ã€‚æˆ‘ä»¬å°†æˆ‘ä»¬çš„ä»£ç å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/cisnlp/relation-specific-neurons%E3%80%82">https://github.com/cisnlp/relation-specific-neuronsã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åœ¨å¤§è¯­è¨€æ¨¡å‹ä¸­ï¼Œç‰¹å®šç¥ç»å…ƒå¯ä»¥å­˜å‚¨é¢„è®­ç»ƒæœŸé—´å­¦åˆ°çš„çŸ¥è¯†ç‰‡æ®µï¼Œè¿™äº›çŸ¥è¯†ä¸ç‰¹å®šçš„å…³ç³»ç›¸å…³è”ã€‚</li>
<li>å­˜åœ¨ä¸“é—¨å¤„ç†ç‰¹å®šå…³ç³»çš„ç¥ç»å…ƒï¼Œè¿™äº›ç¥ç»å…ƒåœ¨æ¨¡å‹å¤„ç†ç›¸å…³äº‹å®æ—¶èµ·ç€é‡è¦ä½œç”¨ã€‚</li>
<li>åœç”¨æ›´å¤šç‰¹å®šå…³ç³»çš„ç¥ç»å…ƒä¼šå¯¼è‡´æ¨¡å‹åœ¨å¤„ç†ç›¸å…³äº‹å®æ—¶æ€§èƒ½ä¸‹é™ã€‚</li>
<li>ç¥ç»å…ƒå…·æœ‰é€šç”¨æ€§ï¼Œå¯ä»¥è·¨å¤šä¸ªå…³ç³»å¯†åˆ‡å’Œä¸å¤ªç›¸å…³çš„å…³ç³»å…±äº«ä½¿ç”¨ï¼Œç”šè‡³å¯ä»¥åœ¨ä¸åŒçš„è¯­è¨€ä¸­å‘æŒ¥ä½œç”¨ã€‚</li>
<li>åœç”¨æŸäº›ç‰¹å®šå…³ç³»çš„ç¥ç»å…ƒå¯èƒ½ä¼šæé«˜æ¨¡å‹åœ¨å…¶ä»–å…³ç³»äº‹å®ä¸Šçš„å¤„ç†èƒ½åŠ›ï¼Œè¿™è¡¨æ˜ç¥ç»å…ƒä¹‹é—´å¯èƒ½å­˜åœ¨å¹²æ‰°æ•ˆåº”ã€‚</li>
<li>å®éªŒç»“æœæ”¯æŒäº†å¤§è¯­è¨€æ¨¡å‹ä¸­ç‰¹å®šå…³ç³»ç¥ç»å…ƒçš„å­˜åœ¨åŠå…¶é‡è¦æ€§è´¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17355">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4b5514233b8255649e670e5618a46d69.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56fa50bfcfb2310582491318d4162bf9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e7fad1a474ac7b66987425e6e13a6113.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cf319d2f14878009cf3cbd0a46fab938.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f2ceca3564857ebbdc2176146410d31.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Time-series-forecasting-based-on-optimized-LLM-for-fault-prediction-in-distribution-power-grid-insulators"><a href="#Time-series-forecasting-based-on-optimized-LLM-for-fault-prediction-in-distribution-power-grid-insulators" class="headerlink" title="Time series forecasting based on optimized LLM for fault prediction in   distribution power grid insulators"></a>Time series forecasting based on optimized LLM for fault prediction in   distribution power grid insulators</h2><p><strong>Authors:JoÃ£o Pedro Matos-Carvalho, Stefano Frizzo Stefenon, Valderi Reis Quietinho Leithardt, Kin-Choong Yow</strong></p>
<p>Surface contamination on electrical grid insulators leads to an increase in leakage current until an electrical discharge occurs, which can result in a power system shutdown. To mitigate the possibility of disruptive faults resulting in a power outage, monitoring contamination and leakage current can help predict the progression of faults. Given this need, this paper proposes a hybrid deep learning (DL) model for predicting the increase in leakage current in high-voltage insulators. The hybrid structure considers a multi-criteria optimization using tree-structured Parzen estimation, an input stage filter for signal noise attenuation combined with a large language model (LLM) applied for time series forecasting. The proposed optimized LLM outperforms state-of-the-art DL models with a root-mean-square error equal to 2.24$\times10^{-4}$ for a short-term horizon and 1.21$\times10^{-3}$ for a medium-term horizon. </p>
<blockquote>
<p>ç”µç½‘ç»ç¼˜å­è¡¨é¢æ±¡æŸ“ä¼šå¯¼è‡´æ³„æ¼ç”µæµå¢åŠ ï¼Œç›´è‡³å‘ç”Ÿç”µå‡»ç©¿ï¼Œå¯èƒ½å¯¼è‡´ç”µåŠ›ç³»ç»Ÿå…³é—­ã€‚ä¸ºäº†å‡å°‘åœç”µç­‰ç ´åæ€§æ•…éšœçš„å¯èƒ½æ€§ï¼Œç›‘æµ‹æ±¡æŸ“å’Œæ³„æ¼ç”µæµæœ‰åŠ©äºé¢„æµ‹æ•…éšœçš„å‘å±•è¶‹åŠ¿ã€‚é‰´äºæ­¤éœ€æ±‚ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºé¢„æµ‹é«˜å‹ç»ç¼˜å­æ³„æ¼ç”µæµå¢åŠ çš„æ··åˆæ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰æ¨¡å‹ã€‚è¯¥æ··åˆç»“æ„é‡‡ç”¨æ ‘ç»“æ„Parzenä¼°è®¡è¿›è¡Œå¤šå‡†åˆ™ä¼˜åŒ–ï¼ŒåŒ…æ‹¬è¾“å…¥é˜¶æ®µæ»¤æ³¢å™¨ç”¨äºä¿¡å·å™ªå£°è¡°å‡ï¼Œå¹¶ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œæ—¶é—´åºåˆ—é¢„æµ‹ã€‚æ‰€æå‡ºçš„ä¼˜åŒ–LLMä¼˜äºæœ€å…ˆè¿›çš„DLæ¨¡å‹ï¼ŒçŸ­æœŸé¢„æµ‹çš„å‡æ–¹æ ¹è¯¯å·®ä¸º2.24Ã—10^-4ï¼Œä¸­æœŸé¢„æµ‹çš„å‡æ–¹æ ¹è¯¯å·®ä¸º1.21Ã—10^-3ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17341v1">PDF</a> </p>
<p><strong>Summary</strong><br>ç»ç¼˜å­è¡¨é¢æ±¡æŸ“ä¼šå¯¼è‡´æ³„æ¼ç”µæµå¢åŠ ï¼Œè¿›è€Œå¼•å‘ç”µåŠ›ç³»ç»Ÿä¸­æ–­æ•…éšœã€‚æœ¬æ–‡æå‡ºä¸€ç§æ··åˆæ·±åº¦å­¦ä¹ æ¨¡å‹æ¥é¢„æµ‹é«˜å‹ç»ç¼˜å­ä¸­æ³„æ¼ç”µæµçš„å¢åŠ ã€‚è¯¥æ¨¡å‹ä½¿ç”¨åŸºäºæ ‘ç»“æ„Parzenä¼°è®¡çš„å¤šå‡†åˆ™ä¼˜åŒ–æŠ€æœ¯ï¼ŒåŒ…æ‹¬ä¿¡å·å™ªå£°è¡°å‡çš„è¾“å…¥é˜¶æ®µæ»¤æ³¢å™¨ä»¥åŠå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œæ—¶é—´åºåˆ—é¢„æµ‹ã€‚æå‡ºçš„ä¼˜åŒ–LLMç›¸è¾ƒäºå…¶ä»–æœ€æ–°æ·±åº¦å­¦ä¹ æ¨¡å‹å…·æœ‰æ›´é«˜çš„æ€§èƒ½ã€‚çŸ­æœŸå’Œä¸­æœŸé¢„æµ‹ç»“æœçš„å‡æ–¹æ ¹è¯¯å·®åˆ†åˆ«ä¸º$ 2.24 \times 10^{-4}$ å’Œ $ 1.21 \times 10^{-3}$ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17341">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-8d206f1ddef63c61aa100ba317fb7113.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-15f0ff7cc92b7248aa656e2a3b65cdb5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1601da9274a8bbe2b434341056102f2a.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Mutual-Reinforcement-of-LLM-Dialogue-Synthesis-and-Summarization-Capabilities-for-Few-Shot-Dialogue-Summarization"><a href="#Mutual-Reinforcement-of-LLM-Dialogue-Synthesis-and-Summarization-Capabilities-for-Few-Shot-Dialogue-Summarization" class="headerlink" title="Mutual Reinforcement of LLM Dialogue Synthesis and Summarization   Capabilities for Few-Shot Dialogue Summarization"></a>Mutual Reinforcement of LLM Dialogue Synthesis and Summarization   Capabilities for Few-Shot Dialogue Summarization</h2><p><strong>Authors:Yen-Ju Lu, Ting-Yao Hu, Hema Swetha Koppula, Hadi Pouransari, Jen-Hao Rick Chang, Yin Xia, Xiang Kong, Qi Zhu, Simon Wang, Oncel Tuzel, Raviteja Vemulapalli</strong></p>
<p>In this work, we propose Mutual Reinforcing Data Synthesis (MRDS) within LLMs to improve few-shot dialogue summarization task. Unlike prior methods that require external knowledge, we mutually reinforce the LLM&#39;s dialogue synthesis and summarization capabilities, allowing them to complement each other during training and enhance overall performances. The dialogue synthesis capability is enhanced by directed preference optimization with preference scoring from summarization capability. The summarization capability is enhanced by the additional high quality dialogue-summary paired data produced by the dialogue synthesis capability. By leveraging the proposed MRDS mechanism, we elicit the internal knowledge of LLM in the format of synthetic data, and use it to augment the few-shot real training dataset. Empirical results demonstrate that our method improves dialogue summarization, achieving a 1.5% increase in ROUGE scores and a 0.3% improvement in BERT scores in few-shot settings. Furthermore, our method attains the highest average scores in human evaluations, surpassing both the pre-trained models and the baselines fine-tuned solely for summarization tasks. </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„ç›¸äº’å¢å¼ºæ•°æ®åˆæˆï¼ˆMRDSï¼‰æ–¹æ³•ï¼Œä»¥æé«˜å°æ ·æœ¬å¯¹è¯æ‘˜è¦ä»»åŠ¡çš„æ•ˆæœã€‚ä¸åŒäºéœ€è¦å¤–éƒ¨çŸ¥è¯†çš„æ–¹æ³•ï¼Œæˆ‘ä»¬é€šè¿‡ç›¸äº’å¢å¼ºLLMçš„å¯¹è¯åˆæˆå’Œæ‘˜è¦èƒ½åŠ›ï¼Œä½¿å®ƒä»¬åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­èƒ½å¤Ÿç›¸äº’è¡¥å……ï¼Œä»è€Œæé«˜æ•´ä½“æ€§èƒ½ã€‚å¯¹è¯åˆæˆèƒ½åŠ›é€šè¿‡æ‘˜è¦èƒ½åŠ›çš„åå¥½è¯„åˆ†è¿›è¡Œå®šå‘åå¥½ä¼˜åŒ–ã€‚æ‘˜è¦èƒ½åŠ›åˆ™é€šè¿‡å¯¹è¯åˆæˆèƒ½åŠ›äº§ç”Ÿçš„é¢å¤–é«˜è´¨é‡å¯¹è¯æ‘˜è¦é…å¯¹æ•°æ®è¿›è¡Œå¢å¼ºã€‚é€šè¿‡åˆ©ç”¨æå‡ºçš„MRDSæœºåˆ¶ï¼Œæˆ‘ä»¬ä»¥åˆæˆæ•°æ®çš„å½¢å¼æ¿€å‘LLMçš„å†…éƒ¨çŸ¥è¯†ï¼Œå¹¶å°†å…¶ç”¨äºæ‰©å……å°æ ·æœ¬çœŸå®è®­ç»ƒæ•°æ®é›†ã€‚ç»éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æé«˜äº†å¯¹è¯æ‘˜è¦çš„æ•ˆæœï¼Œåœ¨å°‘æ ·æœ¬ç¯å¢ƒä¸‹ROUGEå¾—åˆ†æé«˜äº†1.5%ï¼ŒBERTå¾—åˆ†æé«˜äº†0.3%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨äººå·¥è¯„ä¼°ä¸­è·å¾—äº†æœ€é«˜å¹³å‡åˆ†ï¼Œè¶…è¿‡äº†é¢„è®­ç»ƒæ¨¡å‹å’Œä»…é’ˆå¯¹æ‘˜è¦ä»»åŠ¡è¿›è¡Œå¾®è°ƒçš„åŸºç¡€æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17328v1">PDF</a> NAACL 2025 Findings</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†LLMä¸­çš„ç›¸äº’å¼ºåŒ–æ•°æ®åˆæˆï¼ˆMRDSï¼‰æ–¹æ³•ï¼Œæ—¨åœ¨æ”¹è¿›å°‘æ ·æœ¬å¯¹è¯æ‘˜è¦ä»»åŠ¡ã€‚ä¸åŒäºéœ€è¦å¤–éƒ¨çŸ¥è¯†çš„æ–¹æ³•ï¼Œæœ¬æ–‡é€šè¿‡ç›¸äº’å¼ºåŒ–LLMçš„å¯¹è¯åˆæˆå’Œæ‘˜è¦èƒ½åŠ›ï¼Œä½¿å®ƒä»¬åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç›¸äº’è¡¥å……ï¼Œä»è€Œæé«˜æ•´ä½“æ€§èƒ½ã€‚é€šè¿‡å¯¹å¯¹è¯åˆæˆèƒ½åŠ›çš„å®šå‘åå¥½ä¼˜åŒ–å’Œæ‘˜è¦èƒ½åŠ›çš„åå¥½è¯„åˆ†ï¼Œæé«˜äº†ä¸¤è€…çš„è´¨é‡ã€‚é€šè¿‡åˆ©ç”¨æå‡ºçš„MRDSæœºåˆ¶ï¼Œæ¿€å‘LLMçš„å†…éƒ¨çŸ¥è¯†å¹¶ä»¥åˆæˆæ•°æ®çš„å½¢å¼ä½¿ç”¨ï¼Œä»¥æ‰©å……å°‘æ ·æœ¬çœŸå®è®­ç»ƒæ•°æ®é›†ã€‚å®è¯ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æé«˜äº†å¯¹è¯æ‘˜è¦çš„æ•ˆæœï¼Œåœ¨å°‘æ ·æœ¬æ¡ä»¶ä¸‹ROUGEå¾—åˆ†æé«˜1.5%ï¼ŒBERTå¾—åˆ†æé«˜0.3%ã€‚åŒæ—¶ï¼Œè¯¥æ–¹æ³•åœ¨äººç±»è¯„ä¼°ä¸­è·å¾—äº†æœ€é«˜å¹³å‡åˆ†ï¼Œè¶…è¶Šäº†é¢„è®­ç»ƒæ¨¡å‹å’Œä»…é’ˆå¯¹æ‘˜è¦ä»»åŠ¡è¿›è¡Œå¾®è°ƒçš„åŸºç¡€æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•â€”â€”ç›¸äº’å¼ºåŒ–æ•°æ®åˆæˆï¼ˆMRDSï¼‰ç”¨äºæ”¹è¿›LLMåœ¨å°‘æ ·æœ¬å¯¹è¯æ‘˜è¦ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚</li>
<li>MRDSæ–¹æ³•ä¸åŒäºéœ€è¦å¤–éƒ¨çŸ¥è¯†çš„æ–¹æ³•ï¼Œè€Œæ˜¯å¼ºåŒ–äº†LLMå†…éƒ¨çš„å¯¹è¯åˆæˆå’Œæ‘˜è¦èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡å®šå‘åå¥½ä¼˜åŒ–å’Œåå¥½è¯„åˆ†æé«˜äº†å¯¹è¯åˆæˆå’Œæ‘˜è¦çš„è´¨é‡ã€‚</li>
<li>åˆ©ç”¨MRDSæœºåˆ¶æ¿€å‘LLMçš„å†…éƒ¨çŸ¥è¯†å¹¶ä»¥åˆæˆæ•°æ®å½¢å¼ä½¿ç”¨ï¼Œæ‰©å……äº†å°‘æ ·æœ¬çœŸå®è®­ç»ƒæ•°æ®é›†ã€‚</li>
<li>å®è¯ç»“æœè¡¨æ˜ï¼ŒMRDSæ–¹æ³•åœ¨å¯¹è¯æ‘˜è¦ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æ•ˆæœï¼Œæé«˜äº†ROUGEå’ŒBERTå¾—åˆ†ã€‚</li>
<li>MRDSæ–¹æ³•åœ¨äººç±»è¯„ä¼°ä¸­è¡¨ç°æœ€ä½³ï¼Œè¶…è¿‡äº†é¢„è®­ç»ƒæ¨¡å‹å’Œé’ˆå¯¹æ‘˜è¦ä»»åŠ¡çš„åŸºçº¿æ¨¡å‹ã€‚</li>
<li>è¯¥æ–¹æ³•ä¸ºæœªæ¥LLMåœ¨å¯¹è¯æ‘˜è¦ä»»åŠ¡ä¸­çš„æ€§èƒ½æå‡æä¾›äº†æ–°çš„æ€è·¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17328">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-00c07ae51fe6e2ae32a997a6154d0db8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b931c028495d010a7491da3b7819baf5.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Delta-Decompression-for-MoE-based-LLMs-Compression"><a href="#Delta-Decompression-for-MoE-based-LLMs-Compression" class="headerlink" title="Delta Decompression for MoE-based LLMs Compression"></a>Delta Decompression for MoE-based LLMs Compression</h2><p><strong>Authors:Hao Gu, Wei Li, Lujun Li, Qiyuan Zhu, Mark Lee, Shengjie Sun, Wei Xue, Yike Guo</strong></p>
<p>Mixture-of-Experts (MoE) architectures in large language models (LLMs) achieve exceptional performance, but face prohibitive storage and memory requirements. To address these challenges, we present $D^2$-MoE, a new delta decompression compressor for reducing the parameters of MoE LLMs. Based on observations of expert diversity, we decompose their weights into a shared base weight and unique delta weights. Specifically, our method first merges each expertâ€™s weight into the base weight using the Fisher information matrix to capture shared components. Then, we compress delta weights through Singular Value Decomposition (SVD) by exploiting their low-rank properties. Finally, we introduce a semi-dynamical structured pruning strategy for the base weights, combining static and dynamic redundancy analysis to achieve further parameter reduction while maintaining input adaptivity. In this way, our $D^2$-MoE successfully compact MoE LLMs to high compression ratios without additional training. Extensive experiments highlight the superiority of our approach, with over 13% performance gains than other compressors on Mixtral|Phi-3.5|DeepSeek|Qwen2 MoE LLMs at 40$\sim$60% compression rates. Codes are available in <a target="_blank" rel="noopener" href="https://github.com/lliai/D2MoE">https://github.com/lliai/D2MoE</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„ä¸“å®¶æ··åˆï¼ˆMoEï¼‰æ¶æ„å–å¾—äº†å“è¶Šçš„æ€§èƒ½ï¼Œä½†é¢ä¸´ç€å­˜å‚¨å’Œå†…å­˜è¦æ±‚è¿‡é«˜çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†D^2-MoEï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„ç”¨äºå‡å°‘MoE LLMå‚æ•°é‡çš„å¢é‡è§£å‹å‹ç¼©æœºã€‚åŸºäºä¸“å®¶å¤šæ ·æ€§çš„è§‚å¯Ÿï¼Œæˆ‘ä»¬å°†æƒé‡åˆ†è§£ä¸ºå…±äº«åŸºç¡€æƒé‡å’Œç‹¬ç‰¹å¢é‡æƒé‡ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é¦–å…ˆä½¿ç”¨Fisherä¿¡æ¯çŸ©é˜µå°†æ¯ä¸ªä¸“å®¶çš„æƒé‡åˆå¹¶ä¸ºåŸºç¡€æƒé‡ï¼Œä»¥æ•è·å…±äº«ç»„ä»¶ã€‚ç„¶åï¼Œæˆ‘ä»¬é€šè¿‡åˆ©ç”¨å¢é‡æƒé‡çš„ä½ç§©å±æ€§ï¼Œé€šè¿‡å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰è¿›è¡Œå‹ç¼©ã€‚æœ€åï¼Œæˆ‘ä»¬å¯¹åŸºç¡€æƒé‡é‡‡ç”¨åŠåŠ¨æ€ç»“æ„åŒ–å‰ªæç­–ç•¥ï¼Œç»“åˆé™æ€å’ŒåŠ¨æ€å†—ä½™åˆ†æï¼Œä»¥å®ç°è¿›ä¸€æ­¥çš„å‚æ•°ç¼©å‡å¹¶ä¿æŒè¾“å…¥é€‚åº”æ€§ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬çš„D^2-MoEæˆåŠŸåœ°å°†MoE LLMå‹ç¼©åˆ°è¾ƒé«˜çš„å‹ç¼©ç‡ï¼Œä¸”æ— éœ€é¢å¤–çš„è®­ç»ƒã€‚å¤§é‡å®éªŒçªæ˜¾äº†æˆ‘ä»¬æ–¹æ³•çš„ä¼˜åŠ¿ï¼Œåœ¨Mixtral|Phi-3.5|DeepSeek|Qwen2 MoE LLMä¸Šï¼Œä¸å…¶ä»–å‹ç¼©æœºç›¸æ¯”ï¼Œæˆ‘ä»¬åœ¨40%~60%çš„å‹ç¼©ç‡ä¸‹å®ç°äº†è¶…è¿‡13%çš„æ€§èƒ½æå‡ã€‚ç›¸å…³ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/lliai/D2MoE%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/lliai/D2MoEè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17298v1">PDF</a> Work in progress</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„Mixture-of-Expertsï¼ˆMoEï¼‰æ¶æ„çš„å‹ç¼©æ–¹æ³•$D^2$-MoEã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ†è§£ä¸“å®¶æƒé‡ï¼Œåˆ©ç”¨Fisherä¿¡æ¯çŸ©é˜µæ•è·å…±äº«æˆåˆ†ï¼Œå¹¶é€šè¿‡å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰å‹ç¼©ç‹¬ç‰¹æˆåˆ†ï¼Œå®ç°MoEæ¶æ„çš„é«˜æ•ˆå‹ç¼©ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†ä¸€ç§åŠåŠ¨æ€ç»“æ„åŒ–å‰ªæç­–ç•¥ï¼Œè¿›ä¸€æ­¥å‡å°‘äº†å‚æ•°æ•°é‡ï¼ŒåŒæ—¶ä¿æŒäº†æ¨¡å‹çš„é€‚åº”æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰å‹ç¼©æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•çš„æ€§èƒ½ä¼˜åŠ¿æ˜¾è‘—ï¼Œå¯ä»¥åœ¨è¾¾åˆ°é«˜è¾¾å‹ç¼©ç‡çš„èŒƒå›´å†…æå‡æ¨¡å‹æ€§èƒ½ã€‚å…·ä½“å‹ç¼©ç‡é«˜è¾¾è‡³å°‘è‡³åŸæ¥çš„å…­æŠ˜ï¼Œä¸”ä»£ç å·²å…¬å¼€åœ¨GitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯é’ˆå¯¹è¯¥æ–‡æœ¬æå–çš„ä¸ƒä¸ªå…³é”®è¦ç‚¹ï¼š</p>
<ul>
<li>ä»‹ç»äº†ä¸€ç§æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„Mixture-of-Expertsï¼ˆMoEï¼‰æ¶æ„å‹ç¼©æ–¹æ³•ï¼Œåä¸º$D^2$-MoEã€‚è¯¥è®¾è®¡åº”å¯¹å­˜å‚¨å’Œå†…å­˜æ–¹é¢çš„æŒ‘æˆ˜ã€‚</li>
<li>é€šè¿‡åˆ†è§£ä¸“å®¶æƒé‡æ¥æ•è·å…±äº«å’Œç‹¬ç‰¹æˆåˆ†ï¼Œä»¥æé«˜MoEæ¶æ„çš„å‹ç¼©æ•ˆç‡ã€‚è¿™åŸºäºä¸“å®¶çš„å¤šæ ·æ€§å’Œä½¿ç”¨Fisherä¿¡æ¯çŸ©é˜µçš„è§‚å¯Ÿã€‚é€šè¿‡ä½¿ç”¨å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰ï¼Œå‡å°‘æ¨¡å‹å‚æ•°æ•°é‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17298">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f448318c5cf774daad4c62098a7b0500.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e114eab5f983ebb84a101f0ccf158f4a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7fb363c9974ecb48c374987ba520db5d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-921319f86272608bcc5484a9c6a9c1f5.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Benchmarking-Retrieval-Augmented-Generation-in-Multi-Modal-Contexts"><a href="#Benchmarking-Retrieval-Augmented-Generation-in-Multi-Modal-Contexts" class="headerlink" title="Benchmarking Retrieval-Augmented Generation in Multi-Modal Contexts"></a>Benchmarking Retrieval-Augmented Generation in Multi-Modal Contexts</h2><p><strong>Authors:Zhenghao Liu, Xingsheng Zhu, Tianshuo Zhou, Xinyi Zhang, Xiaoyuan Yi, Yukun Yan, Yu Gu, Ge Yu, Maosong Sun</strong></p>
<p>This paper introduces Multi-Modal Retrieval-Augmented Generation (M^2RAG), a benchmark designed to evaluate the effectiveness of Multi-modal Large Language Models (MLLMs) in leveraging knowledge from multi-modal retrieval documents. The benchmark comprises four tasks: image captioning, multi-modal question answering, multi-modal fact verification, and image reranking. All tasks are set in an open-domain setting, requiring RAG models to retrieve query-relevant information from a multi-modal document collection and use it as input context for RAG modeling. To enhance the context utilization capabilities of MLLMs, we also introduce Multi-Modal Retrieval-Augmented Instruction Tuning (MM-RAIT), an instruction tuning method that optimizes MLLMs within multi-modal contexts. Our experiments show that MM-RAIT improves the performance of RAG systems by enabling them to effectively learn from multi-modal contexts. All data and code are available at <a target="_blank" rel="noopener" href="https://github.com/NEUIR/M2RAG">https://github.com/NEUIR/M2RAG</a>. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆM^2RAGï¼‰åŸºå‡†æµ‹è¯•ï¼Œè¯¥åŸºå‡†æµ‹è¯•æ—¨åœ¨è¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨åˆ©ç”¨å¤šæ¨¡æ€æ£€ç´¢æ–‡æ¡£ä¸­çš„çŸ¥è¯†æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…æ‹¬å››é¡¹ä»»åŠ¡ï¼šå›¾åƒæè¿°ã€å¤šæ¨¡æ€é—®ç­”ã€å¤šæ¨¡æ€äº‹å®æ ¸æŸ¥å’Œå›¾åƒé‡æ–°æ’åºã€‚æ‰€æœ‰ä»»åŠ¡éƒ½åœ¨å¼€æ”¾é¢†åŸŸç¯å¢ƒä¸­è®¾ç½®ï¼Œéœ€è¦RAGæ¨¡å‹ä»å¤šæ¨¡æ€æ–‡æ¡£é›†åˆä¸­æ£€ç´¢ä¸æŸ¥è¯¢ç›¸å…³çš„ä¿¡æ¯ï¼Œå¹¶å°†å…¶ç”¨ä½œRAGå»ºæ¨¡çš„è¾“å…¥ä¸Šä¸‹æ–‡ã€‚ä¸ºäº†æé«˜MLLMsçš„ä¸Šä¸‹æ–‡åˆ©ç”¨èƒ½åŠ›ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºæŒ‡ä»¤è°ƒæ•´ï¼ˆMM-RAITï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åœ¨å¤šåª’ä½“ä¸Šä¸‹æ–‡ä¸­ä¼˜åŒ–MLLMsçš„æŒ‡ä»¤è°ƒæ•´æ–¹æ³•ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒMM-RAITé€šè¿‡ä½¿RAGç³»ç»Ÿèƒ½å¤Ÿæœ‰æ•ˆåœ°ä»å¤šåª’ä½“ä¸Šä¸‹æ–‡ä¸­å­¦ä¹ ï¼Œæé«˜äº†å…¶æ€§èƒ½ã€‚æ‰€æœ‰æ•°æ®å’Œä»£ç éƒ½å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/NEUIR/M2RAG%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/NEUIR/M2RAGæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17297v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆM^2RAGï¼‰åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä»å¤šæ¨¡æ€æ£€ç´¢æ–‡æ¡£ä¸­åˆ©ç”¨çŸ¥è¯†çš„æ•ˆæœã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…æ‹¬å›¾åƒæè¿°ã€å¤šæ¨¡æ€é—®ç­”ã€å¤šæ¨¡æ€äº‹å®æ ¸æŸ¥å’Œå›¾åƒé‡æ’åºå››ä¸ªä»»åŠ¡ã€‚æ‰€æœ‰ä»»åŠ¡å‡åœ¨å¼€æ”¾é¢†åŸŸè®¾ç½®ï¼Œè¦æ±‚RAGæ¨¡å‹ä»å¤šæ¨¡æ€æ–‡æ¡£é›†åˆä¸­æ£€ç´¢ä¸æŸ¥è¯¢ç›¸å…³çš„ä¿¡æ¯ï¼Œå¹¶å°†å…¶ç”¨ä½œRAGå»ºæ¨¡çš„è¾“å…¥ä¸Šä¸‹æ–‡ã€‚ä¸ºå¢å¼ºMLLMsåœ¨ä¸Šä¸‹æ–‡åˆ©ç”¨æ–¹é¢çš„èƒ½åŠ›ï¼Œè¿˜ä»‹ç»äº†å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºæŒ‡ä»¤è°ƒæ•´ï¼ˆMM-RAITï¼‰æ–¹æ³•ã€‚å®éªŒè¡¨æ˜ï¼ŒMM-RAITé€šè¿‡ä½¿RAGç³»ç»Ÿèƒ½å¤Ÿæœ‰æ•ˆå­¦ä¹ å¤šæ¨¡æ€ä¸Šä¸‹æ–‡ï¼Œæé«˜äº†å…¶æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>M^2RAGæ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°MLLMsåœ¨å¤šæ¨¡æ€æ£€ç´¢æ–‡æ¡£ä¸­çš„çŸ¥è¯†åˆ©ç”¨æ•ˆæœçš„åŸºå‡†æµ‹è¯•ã€‚</li>
<li>M^2RAGåŒ…å«å›¾åƒæè¿°ã€å¤šæ¨¡æ€é—®ç­”ã€å¤šæ¨¡æ€äº‹å®æ ¸æŸ¥å’Œå›¾åƒé‡æ’åºå››ä¸ªä»»åŠ¡ã€‚</li>
<li>æ‰€æœ‰ä»»åŠ¡å‡åœ¨å¼€æ”¾é¢†åŸŸè®¾ç½®ï¼Œéœ€è¦ä»å¤šæ¨¡æ€æ–‡æ¡£é›†åˆä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯ã€‚</li>
<li>MM-RAITæ˜¯ä¸€ç§ä¼˜åŒ–MLLMsåœ¨å¤šæ¨¡æ€ä¸Šä¸‹æ–‡ä¸­çš„æŒ‡ä»¤è°ƒæ•´æ–¹æ³•ã€‚</li>
<li>MM-RAITé€šè¿‡ä½¿RAGç³»ç»Ÿèƒ½å¤Ÿæœ‰æ•ˆå­¦ä¹ å¤šæ¨¡æ€ä¸Šä¸‹æ–‡ï¼Œæé«˜äº†å…¶æ€§èƒ½ã€‚</li>
<li>M^2RAGçš„æ•°æ®å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/NEUIR/M2RAG%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/NEUIR/M2RAGæ‰¾åˆ°ã€‚</a></li>
<li>MLLMsåœ¨å¤šæ¨¡æ€ä¸Šä¸‹æ–‡ä¸­çš„æ€§èƒ½æå‡å¯¹äºå®ç°æ›´æ™ºèƒ½ã€æ›´å…¨é¢çš„è¯­è¨€æ¨¡å‹å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17297">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-54a6d49c8b648d554dd4ce39614ae861.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0326f5c2e1dc2e4df07b69ca80ea89d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b50b0558853991f7c25c01b44b6f4544.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1c9d6ef2f98ad8a11288689e663a3685.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Measuring-Data-Diversity-for-Instruction-Tuning-A-Systematic-Analysis-and-A-Reliable-Metric"><a href="#Measuring-Data-Diversity-for-Instruction-Tuning-A-Systematic-Analysis-and-A-Reliable-Metric" class="headerlink" title="Measuring Data Diversity for Instruction Tuning: A Systematic Analysis   and A Reliable Metric"></a>Measuring Data Diversity for Instruction Tuning: A Systematic Analysis   and A Reliable Metric</h2><p><strong>Authors:Yuming Yang, Yang Nan, Junjie Ye, Shihan Dou, Xiao Wang, Shuo Li, Huijie Lv, Tao Gui, Qi Zhang, Xuanjing Huang</strong></p>
<p>Data diversity is crucial for the instruction tuning of large language models. Existing studies have explored various diversity-aware data selection methods to construct high-quality datasets and enhance model performance. However, the fundamental problem of precisely defining and measuring data diversity remains underexplored, limiting clear guidance for data engineering. To address this, we systematically analyze 11 existing diversity measurement methods by assessing their correlation with model performance through extensive fine-tuning experiments. Our results indicate that a reliable diversity measure should properly account for both inter-sample differences and the information distribution in the sample space. Building on this, we propose NovelSum, a new diversity metric based on sample-level â€œnovelty.â€ Experiments on both simulated and real-world data show that NovelSum accurately captures diversity variations and achieves a 0.97 correlation with instruction-tuned model performance, highlighting its value in guiding data engineering practices. With NovelSum as an optimization objective, we further develop a greedy, diversity-oriented data selection strategy that outperforms existing approaches, validating both the effectiveness and practical significance of our metric. </p>
<blockquote>
<p>æ•°æ®çš„å¤šæ ·æ€§å¯¹äºå¤§è¯­è¨€æ¨¡å‹çš„æŒ‡ä»¤è°ƒæ•´è‡³å…³é‡è¦ã€‚ç°æœ‰ç ”ç©¶å·²ç»æ¢ç´¢äº†å„ç§æ„è¯†å¤šæ ·æ€§çš„æ•°æ®é€‰æ‹©æ–¹æ³•æ¥æ„å»ºé«˜è´¨é‡çš„æ•°æ®é›†ï¼Œä»¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚ç„¶è€Œï¼Œå…³äºç²¾ç¡®å®šä¹‰å’Œæµ‹é‡æ•°æ®å¤šæ ·æ€§çš„æ ¹æœ¬é—®é¢˜ä»ç„¶è¢«æ¢ç´¢å¾—ä¸å¤Ÿæ·±å…¥ï¼Œè¿™é™åˆ¶äº†æ•°æ®å·¥ç¨‹çš„æ˜ç¡®æŒ‡å¯¼ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬é€šè¿‡å¹¿æ³›çš„å¾®è°ƒå®éªŒè¯„ä¼°äº†ç°æœ‰1 1ç§å¤šæ ·æ€§æµ‹é‡æ–¹æ³•ä¸æ¨¡å‹æ€§èƒ½çš„ç›¸å…³æ€§ï¼Œè¿›è¡Œäº†ç³»ç»Ÿåˆ†æã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå¯é çš„å¤šæ ·æ€§åº¦é‡åº”é€‚å½“åœ°è€ƒè™‘æ ·æœ¬ä¹‹é—´çš„å·®å¼‚ä»¥åŠæ ·æœ¬ç©ºé—´ä¸­çš„ä¿¡æ¯åˆ†å¸ƒã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºæ ·æœ¬çº§â€œæ–°é¢–æ€§â€çš„NovelSumæ–°å¤šæ ·æ€§æŒ‡æ ‡ã€‚åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®æ•°æ®ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒNovelSumå‡†ç¡®åœ°æ•æ‰äº†å¤šæ ·æ€§å˜åŒ–ï¼Œä¸æŒ‡ä»¤è°ƒæ•´æ¨¡å‹æ€§èƒ½çš„ç›¸å…³æ€§è¾¾åˆ°0.9 7ï¼Œå‡¸æ˜¾å…¶åœ¨æŒ‡å¯¼æ•°æ®å·¥ç¨‹å®è·µä¸­çš„ä»·å€¼ã€‚ä»¥NovelSumä¸ºä¼˜åŒ–ç›®æ ‡ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¼€å‘äº†ä¸€ç§è´ªå©ªçš„ã€ä»¥å¤šæ ·æ€§ä¸ºå¯¼å‘çš„æ•°æ®é€‰æ‹©ç­–ç•¥ï¼Œå…¶æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒéªŒè¯äº†æˆ‘ä»¬æŒ‡æ ‡çš„æœ‰æ•ˆæ€§å’Œå®é™…æ„ä¹‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17184v1">PDF</a> 15 pages. The related codes and resources will be released later.   Project page: <a target="_blank" rel="noopener" href="https://github.com/UmeanNever/NovelSum">https://github.com/UmeanNever/NovelSum</a></p>
<p><strong>æ‘˜è¦</strong><br>    æ•°æ®å¤šæ ·æ€§å¯¹äºå¤§è¯­è¨€æ¨¡å‹çš„æŒ‡ä»¤å¾®è°ƒè‡³å…³é‡è¦ã€‚ç°æœ‰ç ”ç©¶å·²æ¢ç´¢äº†å„ç§å¤šæ ·æ€§æ„ŸçŸ¥çš„æ•°æ®é€‰æ‹©æ–¹æ³•æ¥æ„å»ºé«˜è´¨é‡æ•°æ®é›†ï¼Œä»¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚ç„¶è€Œï¼Œç²¾ç¡®å®šä¹‰å’Œæµ‹é‡æ•°æ®å¤šæ ·æ€§çš„åŸºç¡€é—®é¢˜ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ï¼Œå¯¹æ•°æ®å·¥ç¨‹ç¼ºä¹æ˜ç¡®æŒ‡å¯¼ã€‚æœ¬æ–‡ç³»ç»Ÿåœ°åˆ†æäº†11ç§ç°æœ‰çš„å¤šæ ·æ€§æµ‹é‡æ–¹æ³•ï¼Œé€šè¿‡å¤§é‡çš„å¾®è°ƒå®éªŒè¯„ä¼°å®ƒä»¬ä¸æ¨¡å‹æ€§èƒ½çš„ç›¸å…³æ€§ã€‚ç»“æœè¡¨æ˜ï¼Œå¯é çš„å¤šæ ·æ€§æµ‹é‡åº”é€‚å½“è€ƒè™‘æ ·æœ¬é—´çš„å·®å¼‚ä»¥åŠæ ·æœ¬ç©ºé—´ä¸­çš„ä¿¡æ¯åˆ†å¸ƒã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºæ ·æœ¬çº§â€œæ–°é¢–æ€§â€çš„NovelSumæ–°å¤šæ ·æ€§æŒ‡æ ‡ã€‚åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®æ•°æ®ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒNovelSumèƒ½å‡†ç¡®æ•æ‰å¤šæ ·æ€§å˜åŒ–ï¼Œä¸æŒ‡ä»¤è°ƒæ•´æ¨¡å‹æ€§èƒ½çš„ç›¸å…³æ€§è¾¾åˆ°0.97ï¼Œçªæ˜¾å…¶åœ¨æŒ‡å¯¼æ•°æ®å·¥ç¨‹å®è·µä¸­çš„ä»·å€¼ã€‚ä»¥NovelSumä¸ºä¼˜åŒ–ç›®æ ‡ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¼€å‘äº†ä¸€ç§è´ªå©ªçš„ã€ä»¥å¤šæ ·æ€§ä¸ºå¯¼å‘çš„æ•°æ®é€‰æ‹©ç­–ç•¥ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒéªŒè¯äº†æˆ‘ä»¬çš„æŒ‡æ ‡çš„æœ‰æ•ˆæ€§å’Œå®é™…æ„ä¹‰ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ•°æ®å¤šæ ·æ€§å¯¹å¤§è¯­è¨€æ¨¡å‹çš„æŒ‡ä»¤å¾®è°ƒè‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰ç ”ç©¶å·²æ¢ç´¢äº†å¤šç§å¤šæ ·æ€§æ„ŸçŸ¥çš„æ•°æ®é€‰æ‹©æ–¹æ³•ã€‚</li>
<li>æ•°æ®å¤šæ ·æ€§çš„ç²¾ç¡®å®šä¹‰å’Œæµ‹é‡ä»æ˜¯æœªå……åˆ†æ¢ç´¢çš„é¢†åŸŸã€‚</li>
<li>æœ¬æ–‡åˆ†æäº†11ç§å¤šæ ·æ€§æµ‹é‡æ–¹æ³•ï¼Œå¹¶å‘ç°å¯é çš„å¤šæ ·æ€§æµ‹é‡åº”ç»¼åˆè€ƒè™‘æ ·æœ¬é—´å·®å¼‚å’Œæ ·æœ¬ç©ºé—´çš„ä¿¡æ¯åˆ†å¸ƒã€‚</li>
<li>æå‡ºäº†åŸºäºæ ·æœ¬çº§â€œæ–°é¢–æ€§â€çš„æ–°å¤šæ ·æ€§æŒ‡æ ‡NovelSumã€‚</li>
<li>NovelSumèƒ½å‡†ç¡®æ•æ‰å¤šæ ·æ€§å˜åŒ–ï¼Œä¸æ¨¡å‹æ€§èƒ½é«˜åº¦ç›¸å…³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17184">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4e4d7e9953e0d452aec0b15bfeb878db.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-54e13d1819d42fc8227602d59168d40d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c73708b42657d862618f4d0f17d41eb9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef13d395163cfa16cb12d2c25d19dd20.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="The-Role-of-Sparsity-for-Length-Generalization-in-Transformers"><a href="#The-Role-of-Sparsity-for-Length-Generalization-in-Transformers" class="headerlink" title="The Role of Sparsity for Length Generalization in Transformers"></a>The Role of Sparsity for Length Generalization in Transformers</h2><p><strong>Authors:Noah Golowich, Samy Jelassi, David Brandfonbrener, Sham M. Kakade, Eran Malach</strong></p>
<p>Training large language models to predict beyond their training context lengths has drawn much attention in recent years, yet the principles driving such behavior of length generalization remain underexplored. We propose a new theoretical framework to study length generalization for the next-token prediction task, as performed by decoder-only transformers. Conceptually, we show that length generalization occurs as long as each predicted token depends on a small (fixed) number of previous tokens. We formalize such tasks via a notion we call $k$-sparse planted correlation distributions, and show that an idealized model of transformers which generalize attention heads successfully length-generalize on such tasks. As a bonus, our theoretical model justifies certain techniques to modify positional embeddings which have been introduced to improve length generalization, such as position coupling.   We support our theoretical results with experiments on synthetic tasks and natural language, which confirm that a key factor driving length generalization is a &#96;&#96;sparseâ€™â€™ dependency structure of each token on the previous ones. Inspired by our theory, we introduce Predictive Position Coupling, which trains the transformer to predict the position IDs used in a positional coupling approach. Predictive Position Coupling thereby allows us to broaden the array of tasks to which position coupling can successfully be applied to achieve length generalization. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œè®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ä»¥é¢„æµ‹å…¶è®­ç»ƒä¸Šä¸‹æ–‡é•¿åº¦ä¹‹å¤–çš„å†…å®¹å·²ç»å¼•èµ·äº†å¹¿æ³›å…³æ³¨ï¼Œç„¶è€Œé©±åŠ¨è¿™ç§é•¿åº¦æ³›åŒ–è¡Œä¸ºçš„åŸåˆ™ä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ã€‚æˆ‘ä»¬æå‡ºä¸€ä¸ªæ–°çš„ç†è®ºæ¡†æ¶æ¥ç ”ç©¶ä»…è§£ç çš„Transformeræ‰§è¡Œçš„ä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹ä»»åŠ¡çš„é•¿åº¦æ³›åŒ–ã€‚ä»æ¦‚å¿µä¸Šæˆ‘ä»¬è¡¨æ˜ï¼Œåªè¦æ¯ä¸ªé¢„æµ‹çš„ä»¤ç‰Œä¾èµ–äºå‰é¢çš„ä¸€å°ä¸²ï¼ˆå›ºå®šï¼‰ä»¤ç‰Œï¼Œå°±ä¼šå‘ç”Ÿé•¿åº¦æ³›åŒ–ã€‚æˆ‘ä»¬é€šè¿‡æ‰€è°“çš„kç¨€ç–æ¤å…¥ç›¸å…³æ€§åˆ†å¸ƒçš„æ¦‚å¿µæ¥æ­£å¼å®šä¹‰æ­¤ç±»ä»»åŠ¡ï¼Œå¹¶è¡¨æ˜åœ¨ç±»ä¼¼çš„ä»»åŠ¡ä¸Šï¼Œèƒ½å¤ŸæˆåŠŸå®ç°é•¿åº¦æ³›åŒ–çš„é€šç”¨åŒ–æ³¨æ„åŠ›å¤´åœ¨ç†æƒ³åŒ–çš„Transformeræ¨¡å‹ä¸­å¾—åˆ°è¯å®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç†è®ºæ¨¡å‹éªŒè¯äº†ä¸ºäº†æé«˜é•¿åº¦æ³›åŒ–è€Œå¼•å…¥çš„ä¿®æ”¹ä½ç½®åµŒå…¥çš„ä¸€äº›æŠ€æœ¯ï¼Œä¾‹å¦‚ä½ç½®è€¦åˆã€‚æˆ‘ä»¬é€šè¿‡åˆæˆä»»åŠ¡å’Œè‡ªç„¶è¯­è¨€å®éªŒæ¥æ”¯æŒæˆ‘ä»¬çš„ç†è®ºç»“æœï¼Œå®éªŒè¯å®ï¼Œé©±åŠ¨é•¿åº¦æ³›åŒ–çš„ä¸€ä¸ªå…³é”®å› ç´ æ˜¯ä»¤ç‰Œå¯¹å‰ä¸€ä¸ªä»¤ç‰Œçš„â€œç¨€ç–â€ä¾èµ–ç»“æ„ã€‚å—æˆ‘ä»¬ç†è®ºçš„å¯å‘ï¼Œæˆ‘ä»¬å¼•å…¥äº†é¢„æµ‹ä½ç½®è€¦åˆï¼Œå®ƒè®­ç»ƒTransformeré¢„æµ‹ä½ç½®è€¦åˆæ–¹æ³•ä¸­ä½¿ç”¨çš„ä½ç½®IDã€‚é¢„æµ‹ä½ç½®è€¦åˆä»è€Œæ‰©å¤§äº†ä½ç½®è€¦åˆæˆåŠŸåº”ç”¨çš„ä»»åŠ¡èŒƒå›´ï¼Œä»¥å®ç°é•¿åº¦æ³›åŒ–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.16792v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„ç†è®ºæ¡†æ¶æ¥ç ”ç©¶ä»…è§£ç çš„transformeråœ¨ä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹ä»»åŠ¡ä¸­çš„é•¿åº¦æ³›åŒ–è¡Œä¸ºã€‚å®éªŒè¡¨æ˜ï¼Œé•¿åº¦æ³›åŒ–çš„å‘ç”Ÿå–å†³äºæ¯ä¸ªé¢„æµ‹çš„ä»¤ç‰Œä¸å‰é¢ä»¤ç‰Œä¹‹é—´çš„ç¨€ç–ä¾èµ–å…³ç³»ã€‚é€šè¿‡ç†è®ºåˆ†æå’Œå®éªŒéªŒè¯ï¼Œæœ¬æ–‡æ­ç¤ºäº†é•¿åº¦æ³›åŒ–çš„å…³é”®å› ç´ ï¼Œå¹¶æå‡ºäº†é¢„æµ‹ä½ç½®è€¦åˆçš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•è®­ç»ƒå˜å‹å™¨é¢„æµ‹ä½ç½®IDï¼Œä»è€Œæ‰©å¤§äº†ä½ç½®è€¦åˆæˆåŠŸåº”ç”¨çš„ä»»åŠ¡èŒƒå›´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºæ–°çš„ç†è®ºæ¡†æ¶ç ”ç©¶ä»…è§£ç çš„transformeråœ¨é•¿åº¦æ³›åŒ–æ–¹é¢çš„è¡Œä¸ºã€‚</li>
<li>é•¿åº¦æ³›åŒ–çš„å‘ç”Ÿå–å†³äºæ¯ä¸ªé¢„æµ‹çš„ä»¤ç‰Œä¸å‰é¢ä»¤ç‰Œä¹‹é—´çš„ç¨€ç–ä¾èµ–å…³ç³»ã€‚</li>
<li>é€šè¿‡å®éªŒéªŒè¯äº†é•¿åº¦æ³›åŒ–çš„å…³é”®å› ç´ ã€‚</li>
<li>ä»‹ç»äº†ä¸€ç§æ–°çš„æ–¹æ³•â€”â€”é¢„æµ‹ä½ç½®è€¦åˆï¼Œè¯¥æ–¹æ³•è®­ç»ƒå˜å‹å™¨é¢„æµ‹ä½ç½®IDã€‚</li>
<li>é¢„æµ‹ä½ç½®è€¦åˆæ–¹æ³•æœ‰åŠ©äºæ‰©å¤§ä½ç½®è€¦åˆæˆåŠŸåº”ç”¨çš„ä»»åŠ¡èŒƒå›´ã€‚</li>
<li>ç†è®ºåˆ†æå’Œå®éªŒéªŒè¯å…±åŒæ”¯æŒäº†ä¸Šè¿°è§‚ç‚¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.16792">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ce6acfe98bb30a7c7ec6d9f87ee1d435.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="AeroReformer-Aerial-Referring-Transformer-for-UAV-based-Referring-Image-Segmentation"><a href="#AeroReformer-Aerial-Referring-Transformer-for-UAV-based-Referring-Image-Segmentation" class="headerlink" title="AeroReformer: Aerial Referring Transformer for UAV-based Referring Image   Segmentation"></a>AeroReformer: Aerial Referring Transformer for UAV-based Referring Image   Segmentation</h2><p><strong>Authors:Rui Li</strong></p>
<p>As a novel and challenging task, referring segmentation combines computer vision and natural language processing to localize and segment objects based on textual descriptions. While referring image segmentation (RIS) has been extensively studied in natural images, little attention has been given to aerial imagery, particularly from unmanned aerial vehicles (UAVs). The unique challenges of UAV imagery, including complex spatial scales, occlusions, and varying object orientations, render existing RIS approaches ineffective. A key limitation has been the lack of UAV-specific datasets, as manually annotating pixel-level masks and generating textual descriptions is labour-intensive and time-consuming. To address this gap, we design an automatic labelling pipeline that leverages pre-existing UAV segmentation datasets and Multimodal Large Language Models (MLLM) for generating textual descriptions. Furthermore, we propose Aerial Referring Transformer (AeroReformer), a novel framework for UAV referring image segmentation (UAV-RIS), featuring a Vision-Language Cross-Attention Module (VLCAM) for effective cross-modal understanding and a Rotation-Aware Multi-Scale Fusion (RAMSF) decoder to enhance segmentation accuracy in aerial scenes. Extensive experiments on two newly developed datasets demonstrate the superiority of AeroReformer over existing methods, establishing a new benchmark for UAV-RIS. The datasets and code will be publicly available at: <a target="_blank" rel="noopener" href="https://github.com/lironui/AeroReformer">https://github.com/lironui/AeroReformer</a>. </p>
<blockquote>
<p>ä½œä¸ºä¸€é¡¹æ–°é¢–ä¸”å…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå¼•ç”¨åˆ†å‰²ç»“åˆäº†è®¡ç®—æœºè§†è§‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†ï¼Œæ ¹æ®æ–‡æœ¬æè¿°æ¥å®šä½å’Œåˆ†å‰²å¯¹è±¡ã€‚è™½ç„¶å›¾åƒå¼•ç”¨åˆ†å‰²ï¼ˆRISï¼‰åœ¨è‡ªç„¶å›¾åƒä¸­å¾—åˆ°äº†å¹¿æ³›çš„ç ”ç©¶ï¼Œä½†å¯¹èˆªç©ºå›¾åƒçš„å…³æ³¨å´å¾ˆå°‘ï¼Œç‰¹åˆ«æ˜¯æ¥è‡ªæ— äººæœºï¼ˆUAVï¼‰çš„å›¾åƒã€‚æ— äººæœºå›¾åƒå…·æœ‰ç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å¤æ‚çš„ç©ºé—´å°ºåº¦ã€é®æŒ¡å’Œå˜åŒ–çš„å¯¹è±¡æ–¹å‘ï¼Œä½¿å¾—ç°æœ‰çš„RISæ–¹æ³•æ•ˆæœä¸ä½³ã€‚ä¸€ä¸ªå…³é”®çš„é™åˆ¶æ˜¯ç¼ºä¹é’ˆå¯¹æ— äººæœºçš„ç‰¹å®šæ•°æ®é›†ï¼Œå› ä¸ºæ‰‹åŠ¨æ³¨é‡Šåƒç´ çº§è’™ç‰ˆå’Œç”Ÿæˆæ–‡æœ¬æè¿°æ˜¯åŠ³åŠ¨å¯†é›†å‹çš„ï¼Œä¸”è€—æ—¶ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªè‡ªåŠ¨æ ‡æ³¨ç®¡é“ï¼Œè¯¥ç®¡é“åˆ©ç”¨ç°æœ‰çš„æ— äººæœºåˆ†å‰²æ•°æ®é›†å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æ¥ç”Ÿæˆæ–‡æœ¬æè¿°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†æ— äººæœºå¼•ç”¨å›¾åƒåˆ†å‰²ï¼ˆUAV-RISï¼‰çš„æ–°æ¡†æ¶â€œAerialReferring Transformerâ€ï¼ˆAeroReformerï¼‰ï¼Œè¯¥æ¡†æ¶å…·æœ‰è§†è§‰è¯­è¨€è·¨æ³¨æ„åŠ›æ¨¡å—ï¼ˆVLCAMï¼‰ï¼Œå¯å®ç°æœ‰æ•ˆçš„è·¨æ¨¡æ€ç†è§£ï¼Œä»¥åŠæ—‹è½¬æ„ŸçŸ¥å¤šå°ºåº¦èåˆï¼ˆRAMSFï¼‰è§£ç å™¨ï¼Œå¯æé«˜èˆªç©ºåœºæ™¯ä¸­çš„åˆ†å‰²ç²¾åº¦ã€‚åœ¨ä¸¤ä¸ªæ–°å¼€å‘çš„æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒAeroReformerä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸ºUAV-RISå»ºç«‹äº†æ–°çš„åŸºå‡†ã€‚æ•°æ®é›†å’Œä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/lironui/AeroReformer%E5%85%AC%E5%BC%80%E5%8F%AF%E7%94%A8%E3%80%82">https://github.com/lironui/AeroReformerå…¬å¼€å¯ç”¨ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.16680v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>åˆ©ç”¨è®¡ç®—æœºè§†è§‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯ï¼Œå‚ç…§åˆ†å‰²ä»»åŠ¡å¯ä»¥å®ç°å¯¹åŸºäºæ–‡æœ¬æè¿°çš„å¯¹è±¡å®šä½å’Œåˆ†å‰²ã€‚å°½ç®¡è‡ªç„¶å›¾åƒçš„å‚ç…§å›¾åƒåˆ†å‰²ï¼ˆRISï¼‰å·²å¾—åˆ°å¹¿æ³›ç ”ç©¶ï¼Œä½†æ— äººæœºå½±åƒçš„å‚ç…§å›¾åƒåˆ†å‰²ï¼ˆUAV-RISï¼‰å°šæœªå—åˆ°è¶³å¤Ÿé‡è§†ã€‚æ— äººæœºå½±åƒå…·æœ‰ç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼Œå¦‚å¤æ‚çš„ç©ºé—´å°ºåº¦ã€é®æŒ¡å’Œå˜åŒ–çš„å¯¹è±¡æ–¹å‘ï¼Œä½¿å¾—ç°æœ‰çš„RISæ–¹æ³•æ•ˆæœä¸ä½³ã€‚ä¸ºè§£å†³è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬è®¾è®¡äº†åˆ©ç”¨ç°æœ‰æ— äººæœºåˆ†å‰²æ•°æ®é›†å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ç”Ÿæˆæ–‡æœ¬æè¿°çš„è‡ªåŠ¨æ ‡æ³¨ç®¡é“ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†åä¸ºAeroreformerçš„æ–°æ¡†æ¶ï¼Œå…·æœ‰ç”¨äºæœ‰æ•ˆè·¨æ¨¡æ€ç†è§£çš„è§†è§‰è¯­è¨€è·¨æ³¨æ„åŠ›æ¨¡å—ï¼ˆVLCAMï¼‰å’Œç”¨äºå¢å¼ºèˆªç©ºåœºæ™¯åˆ†å‰²å‡†ç¡®åº¦çš„æ—‹è½¬æ„ŸçŸ¥å¤šå°ºåº¦èåˆï¼ˆRAMSFï¼‰è§£ç å™¨ã€‚åœ¨ä¸¤ä¸ªæ–°å¼€å‘æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒAeroreformerä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸ºUAV-RISå»ºç«‹äº†æ–°çš„åŸºå‡†ã€‚æ•°æ®é›†å’Œä»£ç å°†åœ¨lironui&#x2F;Aeroreformerå…¬å¼€å¯ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>å‚ç…§åˆ†å‰²ä»»åŠ¡ç»“åˆäº†è®¡ç®—æœºè§†è§‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†ï¼ŒåŸºäºæ–‡æœ¬æè¿°è¿›è¡Œå¯¹è±¡å®šä½å’Œåˆ†å‰²ã€‚</li>
<li>ç›®å‰å¯¹æ— äººæœºå½±åƒçš„å‚ç…§å›¾åƒåˆ†å‰²ï¼ˆUAV-RISï¼‰ç ”ç©¶ä¸è¶³ã€‚</li>
<li>æ— äººæœºå½±åƒå­˜åœ¨ç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼Œå¦‚å¤æ‚çš„ç©ºé—´å°ºåº¦ã€é®æŒ¡å’Œå¯¹è±¡æ–¹å‘å˜åŒ–ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å› æ— æ³•æœ‰æ•ˆå¤„ç†è¿™äº›æŒ‘æˆ˜è€Œæ•ˆæœä¸ä½³ã€‚</li>
<li>ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œè®¾è®¡äº†åˆ©ç”¨ç°æœ‰æ— äººæœºåˆ†å‰²æ•°æ®é›†å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„è‡ªåŠ¨æ ‡æ³¨ç®¡é“ã€‚</li>
<li>æå‡ºäº†åä¸ºAeroreformerçš„æ–°æ¡†æ¶ï¼Œå…·æœ‰è§†è§‰è¯­è¨€è·¨æ³¨æ„åŠ›æ¨¡å—ï¼ˆVLCAMï¼‰å’Œæ—‹è½¬æ„ŸçŸ¥å¤šå°ºåº¦èåˆï¼ˆRAMSFï¼‰è§£ç å™¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.16680">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f21107119346ab7ad6909dc69e31e370.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-94563a681bd029f13ab4bd71b9df0f30.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1dad80a4c1a092eeeaa1fdaccf907497.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dcd320e41e8da6e9320fd0657b38e59b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d2e30d0732afe262c3d4c30b7eddd609.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f27c20a18690299605e3d93bbd872a17.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="CODESYNC-Synchronizing-Large-Language-Models-with-Dynamic-Code-Evolution-at-Scale"><a href="#CODESYNC-Synchronizing-Large-Language-Models-with-Dynamic-Code-Evolution-at-Scale" class="headerlink" title="CODESYNC: Synchronizing Large Language Models with Dynamic Code   Evolution at Scale"></a>CODESYNC: Synchronizing Large Language Models with Dynamic Code   Evolution at Scale</h2><p><strong>Authors:Chenlong Wang, Zhaoyang Chu, Zhengxiang Cheng, Xuyi Yang, Kaiyue Qiu, Yao Wan, Zhou Zhao, Xuanhua Shi, Dongping Chen</strong></p>
<p>Large Language Models (LLMs) have exhibited exceptional performance in software engineering yet face challenges in adapting to continually evolving code knowledge, particularly regarding the frequent updates of third-party library APIs. This limitation, stemming from static pre-training datasets, often results in non-executable code or implementations with suboptimal safety and efficiency. To this end, this paper introduces CODESYNC, a data engine for identifying outdated code patterns and collecting real-time code knowledge updates from Python third-party libraries. Building upon CODESYNC, we develop CODESYNCBENCH, a comprehensive benchmark for assessing LLMsâ€™ ability to stay synchronized with code evolution, which covers real-world updates for 220 APIs from six Python libraries. Our benchmark offers 3,300 test cases across three evaluation tasks and an update-aware instruction tuning dataset consisting of 2,200 training samples. Extensive experiments on 14 state-of-the-art LLMs reveal that they struggle with dynamic code evolution, even with the support of advanced knowledge updating methods (e.g., DPO, ORPO, and SimPO). We believe that our benchmark can offer a strong foundation for the development of more effective methods for real-time code knowledge updating in the future. The experimental code and dataset are publicly available at: <a target="_blank" rel="noopener" href="https://github.com/Lucky-voyage/Code-Sync">https://github.com/Lucky-voyage/Code-Sync</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è½¯ä»¶å·¥ç¨‹é¢†åŸŸè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä½†åœ¨é€‚åº”ä¸æ–­æ¼”å˜çš„ä»£ç çŸ¥è¯†æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¬¬ä¸‰æ–¹åº“APIé¢‘ç¹æ›´æ–°çš„æƒ…å†µä¸‹ã€‚è¿™ä¸€å±€é™æ€§æºäºé™æ€çš„é¢„è®­ç»ƒæ•°æ®é›†ï¼Œå¾€å¾€å¯¼è‡´ç”Ÿæˆçš„ä»£ç ä¸å¯æ‰§è¡Œï¼Œæˆ–åœ¨å®‰å…¨æ€§å’Œæ•ˆç‡æ–¹é¢å­˜åœ¨æ¬¡ä¼˜å®ç°ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡ä»‹ç»äº†CODESYNCï¼Œä¸€ä¸ªç”¨äºè¯†åˆ«è¿‡æ—¶ä»£ç æ¨¡å¼å¹¶ä»Pythonç¬¬ä¸‰æ–¹åº“ä¸­å®æ—¶æ”¶é›†ä»£ç çŸ¥è¯†æ›´æ–°çš„æ•°æ®å¼•æ“ã€‚åŸºäºCODESYNCï¼Œæˆ‘ä»¬å¼€å‘äº†CODESYNCBENCHï¼Œä¸€ä¸ªå…¨é¢è¯„ä¼°LLMä¸ä»£ç è¿›åŒ–åŒæ­¥èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–äº†æ¥è‡ªå…­ä¸ªPythonåº“çš„220ä¸ªAPIçš„å®æ—¶æ›´æ–°ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•åŒ…å«3300ä¸ªæµ‹è¯•ç”¨ä¾‹ï¼Œæ¶µç›–ä¸‰ä¸ªè¯„ä¼°ä»»åŠ¡ï¼Œä»¥åŠä¸€ä¸ªåŒ…å«2200ä¸ªè®­ç»ƒæ ·æœ¬çš„æ›´æ–°æ„ŸçŸ¥æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ã€‚å¯¹14ç§æœ€æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œå³ä½¿åœ¨å…ˆè¿›çš„æ›´æ–°çŸ¥è¯†æ–¹æ³•ï¼ˆå¦‚DPOã€ORPOå’ŒSimPOï¼‰çš„æ”¯æŒä¸‹ï¼Œå®ƒä»¬åœ¨å¤„ç†åŠ¨æ€ä»£ç è¿›åŒ–æ–¹é¢ä»ç„¶é¢ä¸´å›°éš¾ã€‚æˆ‘ä»¬ç›¸ä¿¡ï¼Œæˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•å¯ä»¥ä¸ºæœªæ¥å¼€å‘æ›´æœ‰æ•ˆçš„å®æ—¶ä»£ç çŸ¥è¯†æ›´æ–°æ–¹æ³•æä¾›åšå®çš„åŸºç¡€ã€‚å®éªŒä»£ç å’Œæ•°æ®é›†å¯åœ¨ä»¥ä¸‹ç½‘å€å…¬å¼€è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/Lucky-voyage/Code-Sync">https://github.com/Lucky-voyage/Code-Sync</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.16645v1">PDF</a> </p>
<p><strong>Summary</strong><br>LLMåœ¨é¢å¯¹æŒç»­æ¼”åŒ–çš„ä»£ç çŸ¥è¯†æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯ç¬¬ä¸‰æ–¹åº“APIçš„é¢‘ç¹æ›´æ–°ã€‚æœ¬æ–‡æå‡ºCODESYNCæ•°æ®å¼•æ“å’ŒCODESYNCBENCHåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ã€‚CODESYNCå¯è¯†åˆ«è¿‡æ—¶ä»£ç æ¨¡å¼å¹¶ä»Pythonç¬¬ä¸‰æ–¹åº“å®æ—¶æ”¶é›†ä»£ç çŸ¥è¯†æ›´æ–°ã€‚CODESYNCBENCHè¯„ä¼°LLMåœ¨ä»£ç è¿›åŒ–ä¸­çš„åŒæ­¥èƒ½åŠ›ï¼Œæ¶µç›–å…­ä¸ªPythonåº“çš„220ä¸ªAPIçš„çœŸå®ä¸–ç•Œæ›´æ–°ã€‚å®éªŒè¡¨æ˜ï¼Œå³ä½¿ä½¿ç”¨å…ˆè¿›çš„çŸ¥è¯†æ›´æ–°æ–¹æ³•ï¼ŒLLMåœ¨åŠ¨æ€ä»£ç æ¼”åŒ–æ–¹é¢ä»é¢ä¸´å›°éš¾ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨è½¯ä»¶å·¥ç¨‹ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†åœ¨é€‚åº”ä¸æ–­æ¼”åŒ–çš„ä»£ç çŸ¥è¯†æ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>ç¬¬ä¸‰æ–¹åº“APIçš„é¢‘ç¹æ›´æ–°ç»™LLMå¸¦æ¥ç‰¹å®šå›°éš¾ã€‚</li>
<li>CODESYNCæ•°æ®å¼•æ“ç”¨äºè¯†åˆ«è¿‡æ—¶ä»£ç æ¨¡å¼å¹¶ä»Pythonç¬¬ä¸‰æ–¹åº“æ”¶é›†å®æ—¶ä»£ç çŸ¥è¯†æ›´æ–°ã€‚</li>
<li>CODESYNCBENCHåŸºå‡†æµ‹è¯•ç”¨äºè¯„ä¼°LLMåœ¨ä»£ç è¿›åŒ–ä¸­çš„åŒæ­¥èƒ½åŠ›ï¼Œæ¶µç›–çœŸå®ä¸–ç•Œæ›´æ–°å’Œå¤šç§è¯„ä»·ä»»åŠ¡ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼Œå³ä½¿ä½¿ç”¨å…ˆè¿›çš„çŸ¥è¯†æ›´æ–°æ–¹æ³•ï¼ŒLLMåœ¨å¤„ç†åŠ¨æ€ä»£ç æ¼”åŒ–æ—¶ä»è¡¨ç°ä¸è¶³ã€‚</li>
<li>æœ¬æ–‡æä¾›çš„åŸºå‡†æµ‹è¯•ä¸ºå¼€å‘æ›´æœ‰æ•ˆçš„å®æ—¶ä»£ç çŸ¥è¯†æ›´æ–°æ–¹æ³•æä¾›äº†åŸºç¡€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.16645">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-14d9dda110c098f8a6cdb8eb030a706f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b80b38f486d2dd00b82467cb865a6624.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a1375bca7f9be60a406ef34b6fef383.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dfafdb4f71839e2596cf0f7a0c01d6f2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b9424fccfd50c738a056f64e0d263e13.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7be38814ad3195c2fbad9656b932d822.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Enhancing-Domain-Specific-Retrieval-Augmented-Generation-Synthetic-Data-Generation-and-Evaluation-using-Reasoning-Models"><a href="#Enhancing-Domain-Specific-Retrieval-Augmented-Generation-Synthetic-Data-Generation-and-Evaluation-using-Reasoning-Models" class="headerlink" title="Enhancing Domain-Specific Retrieval-Augmented Generation: Synthetic Data   Generation and Evaluation using Reasoning Models"></a>Enhancing Domain-Specific Retrieval-Augmented Generation: Synthetic Data   Generation and Evaluation using Reasoning Models</h2><p><strong>Authors:Aryan Jadon, Avinash Patil, Shashank Kumar</strong></p>
<p>Retrieval-Augmented Generation (RAG) systems face significant performance gaps when applied to technical domains requiring precise information extraction from complex documents. Current evaluation methodologies relying on document-level metrics inadequately capture token-resolution retrieval accuracy that is critical for domain-related documents. We propose a framework combining granular evaluation metrics with synthetic data generation to optimize domain-specific RAG performance. First, we introduce token-aware metrics Precision $\Omega$ and Intersection-over-Union (IoU) that quantify context preservation versus information density trade-offs inherent in technical texts. Second, we develop a reasoning model-driven pipeline using instruction-tuned LLMs (DeepSeek-R1, DeepSeek-R1 distilled variants, and Phi-4) to generate context-anchored QA pairs with discontinuous reference spans across three specialized corpora: SEC 10-K filings (finance), biomedical abstracts (PubMed), and APT threat reports (cybersecurity).   Our empirical analysis reveals critical insights: smaller chunks (less than 10 tokens) improve precision by 31-42% (IoU &#x3D; 0.071 vs. baseline 0.053) at recall costs (-18%), while domain-specific embedding strategies yield 22% variance in optimal chunk sizing (5-20 tokens). The DeepSeek-R1-Distill-Qwen-32B model demonstrates superior concept alignment (+14% mean IoU over alternatives), though no configuration universally dominates. Financial texts favor larger chunks for risk factor coverage (Recall &#x3D; 0.81 at size &#x3D; 20), whereas cybersecurity content benefits from atomic segmentation, Precision $\Omega &#x3D; 0.28$ at size &#x3D; 5.   Our code is available on <a target="_blank" rel="noopener" href="https://github.com/aryan-jadon/Synthetic-Data-Generation-and-Evaluation-using-Reasoning-Model">https://github.com/aryan-jadon/Synthetic-Data-Generation-and-Evaluation-using-Reasoning-Model</a> </p>
<blockquote>
<p>æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿåœ¨åº”ç”¨äºéœ€è¦ç²¾ç¡®æå–å¤æ‚æ–‡æ¡£ä¿¡æ¯çš„æŠ€æœ¯é¢†åŸŸæ—¶ï¼Œå­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®è·ã€‚å½“å‰ä¾èµ–æ–‡æ¡£çº§åˆ«çš„è¯„ä¼°æ–¹æ³•ä¸è¶³ä»¥æ•æ‰å¯¹é¢†åŸŸç›¸å…³æ–‡æ¡£è‡³å…³é‡è¦çš„ä»¤ç‰Œè§£ææ£€ç´¢å‡†ç¡®æ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»“åˆç²’åº¦è¯„ä¼°æŒ‡æ ‡å’Œåˆæˆæ•°æ®ç”Ÿæˆçš„æ¡†æ¶ï¼Œä»¥ä¼˜åŒ–ç‰¹å®šé¢†åŸŸçš„RAGæ€§èƒ½ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥äº†ä»¤ç‰Œæ„ŸçŸ¥æŒ‡æ ‡ç²¾åº¦Î©ï¼ˆPrecision Omegaï¼‰å’Œäº¤å¹¶æ¯”ï¼ˆIoUï¼‰ï¼Œä»¥é‡åŒ–æŠ€æœ¯æ–‡æœ¬ä¸­å›ºæœ‰çš„ä¸Šä¸‹æ–‡ä¿ç•™ä¸ä¿¡æ¯å¯†åº¦ä¹‹é—´çš„æƒè¡¡ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬ä½¿ç”¨æŒ‡ä»¤å¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆDeepSeek-R1ã€DeepSeek-R1è’¸é¦å˜ç§å’ŒPhi-4ï¼‰å¼€å‘äº†ä¸€ä¸ªåŸºäºæ¨ç†æ¨¡å‹çš„ç®¡é“ï¼Œç”Ÿæˆä¸ä¸‰ä¸ªä¸“ä¸šè¯­æ–™åº“ç›¸å…³çš„ä¸Šä¸‹æ–‡é”šå®šçš„é—®ç­”å¯¹ï¼šåŒ…æ‹¬SEC 10-KæŠ¥å‘Šï¼ˆé‡‘èï¼‰ã€ç”Ÿç‰©åŒ»å­¦æ‘˜è¦ï¼ˆPubMedï¼‰å’ŒAPTå¨èƒæŠ¥å‘Šï¼ˆç½‘ç»œå®‰å…¨ï¼‰ã€‚æˆ‘ä»¬çš„å®è¯åˆ†ææ­ç¤ºäº†å…³é”®è§è§£ï¼šè¾ƒå°çš„ç‰‡æ®µï¼ˆå°‘äº10ä¸ªä»¤ç‰Œï¼‰é€šè¿‡æé«˜ç²¾åº¦ï¼ˆIoU &#x3D; 0.071ä¸åŸºçº¿0.053ç›¸æ¯”ï¼‰æ¥ä¼˜åŒ–å¬å›ç‡ï¼ˆ-18%ï¼‰ï¼Œè€Œç‰¹å®šé¢†åŸŸçš„åµŒå…¥ç­–ç•¥å¯¼è‡´æœ€ä½³ç‰‡æ®µå¤§å°æœ‰22%çš„å˜åŠ¨èŒƒå›´ï¼ˆ5-20ä¸ªä»¤ç‰Œï¼‰ã€‚DeepSeek-R1-Distill-Qwen-32Bæ¨¡å‹æ˜¾ç¤ºå‡ºæ›´å¥½çš„æ¦‚å¿µå¯¹é½ï¼ˆç›¸å¯¹äºæ›¿ä»£æ¨¡å‹å¹³å‡IoUæé«˜14%ï¼‰ï¼Œä½†æ²¡æœ‰é…ç½®èƒ½å¤Ÿæ™®éå æ®ä¸»å¯¼åœ°ä½ã€‚é‡‘èæ–‡æœ¬å€¾å‘äºè¾ƒå¤§çš„å—ä»¥è¦†ç›–é£é™©å› ç´ ï¼ˆå¤§å°ä¸º20æ—¶çš„å¬å›ç‡ä¸º0.81ï¼‰ï¼Œè€Œç½‘ç»œå®‰å…¨å†…å®¹å—ç›ŠäºåŸå­åˆ†å‰²ï¼ˆå¤§å°ä¸º5æ—¶çš„ç²¾åº¦Î©ä¸º0.28ï¼‰ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/aryan-jadon/Synthetic-Data-Generation-and-Evaluation-using-Reasoning-Model%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/aryan-jadon/Synthetic-Data-Generation-and-Evaluation-using-Reasoning-Modelæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.15854v1">PDF</a> 8 Pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹æŠ€æœ¯é¢†åŸŸçš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿæ€§èƒ½æå‡çš„ç ”ç©¶ã€‚é’ˆå¯¹ç°æœ‰è¯„ä¼°æ–¹æ³•çš„ä¸è¶³ï¼Œæå‡ºäº†ç»“åˆç²¾ç»†è¯„ä¼°æŒ‡æ ‡å’Œåˆæˆæ•°æ®ç”Ÿæˆçš„æ¡†æ¶ã€‚å¼•å…¥tokençº§åˆ«çš„ç²¾åº¦Î©å’Œäº¤é›†æ¯”ï¼ˆIoUï¼‰æŒ‡æ ‡ï¼Œé‡åŒ–æŠ€æœ¯æ–‡æœ¬ä¸­çš„ä¸Šä¸‹æ–‡ä¿ç•™ä¸ä¿¡æ¯å¯†åº¦æƒè¡¡ã€‚åŒæ—¶ï¼Œåˆ©ç”¨æŒ‡ä»¤å¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”Ÿæˆä¸Šä¸‹æ–‡ç›¸å…³çš„é—®ç­”å¯¹ï¼Œæ¶µç›–ä¸‰ä¸ªä¸“ä¸šè¯­æ–™åº“ã€‚å®è¯åˆ†ææ˜¾ç¤ºï¼Œè¾ƒå°çš„tokenå—åœ¨æé«˜ç²¾åº¦æ–¹é¢æ•ˆæœæ˜¾è‘—ï¼Œè€Œé’ˆå¯¹ç‰¹å®šé¢†åŸŸçš„åµŒå…¥ç­–ç•¥å¯¹æœ€ä½³å—å¤§å°æœ‰å½±å“ã€‚ç›¸å…³ä»£ç å·²å…¬å¼€åœ¨GitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰é’ˆå¯¹æŠ€æœ¯é¢†åŸŸçš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿå­˜åœ¨æ€§èƒ½å·®è·ã€‚</li>
<li>ä¼ ç»Ÿæ–‡æ¡£çº§è¯„ä¼°æŒ‡æ ‡æ— æ³•æœ‰æ•ˆæ•æ‰tokençº§åˆ«çš„ç²¾ç¡®æ£€ç´¢å‡†ç¡®åº¦ã€‚</li>
<li>å¼•å…¥token-awareè¯„ä¼°æŒ‡æ ‡å¦‚ç²¾åº¦Î©å’Œäº¤é›†æ¯”ï¼ˆIoUï¼‰ï¼Œè¡¡é‡æŠ€æœ¯æ–‡æœ¬ä¸­çš„ä¸Šä¸‹æ–‡ä¿ç•™ä¸ä¿¡æ¯å¯†åº¦æƒè¡¡ã€‚</li>
<li>æå‡ºç»“åˆç²¾ç»†è¯„ä¼°æŒ‡æ ‡ä¸åˆæˆæ•°æ®ç”Ÿæˆçš„æ¡†æ¶ï¼Œä¼˜åŒ–é¢†åŸŸç‰¹å®šRAGæ€§èƒ½ã€‚</li>
<li>åˆ©ç”¨æŒ‡ä»¤å¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆä¸Šä¸‹æ–‡ç›¸å…³çš„é—®ç­”å¯¹ï¼Œæ¶µç›–é‡‘èã€ç”Ÿç‰©åŒ»å­¦å’Œç½‘ç»œå®‰å…¨é¢†åŸŸçš„ä¸“ä¸šè¯­æ–™åº“ã€‚</li>
<li>å®è¯åˆ†ææ˜¾ç¤ºï¼Œè¾ƒå°çš„tokenå—åœ¨æé«˜ç²¾åº¦æ–¹é¢è¡¨ç°ä¼˜è¶Šï¼Œè€Œè¾ƒå¤§çš„tokenå—åœ¨é‡‘èæ–‡æœ¬ä¸­ç”¨äºé£é™©å› å­è¦†ç›–æ›´ä¸ºæœ‰åˆ©ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.15854">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-89a01e6dfd22d4b9687675e746884f87.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3097ac2504f3996d60d6fc2a28880354.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6606075fdb673f783b9470cbcd11a1e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-481de2fb7defc47da925cd632196030e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-80047ed2ea5d89f19009ed451173e699.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dbf1b902fabb37ea084eddc6630c2258.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="A-Stronger-Mixture-of-Low-Rank-Experts-for-Fine-Tuning-Foundation-Models"><a href="#A-Stronger-Mixture-of-Low-Rank-Experts-for-Fine-Tuning-Foundation-Models" class="headerlink" title="A Stronger Mixture of Low-Rank Experts for Fine-Tuning Foundation Models"></a>A Stronger Mixture of Low-Rank Experts for Fine-Tuning Foundation Models</h2><p><strong>Authors:Mengyang Sun, Yihao Wang, Tao Feng, Dan Zhang, Yifan Zhu, Jie Tang</strong></p>
<p>In order to streamline the fine-tuning of foundation models, Low-Rank Adapters (LoRAs) have been substantially adopted across various fields, including instruction tuning and domain adaptation. The underlying concept of LoRA involves decomposing a full-rank matrix into the product of two lower-rank matrices, which reduces storage consumption and accelerates the training process. Furthermore, to address the limited expressive capacity of LoRA, the Mixture-of-Expert (MoE) has been introduced for incorporating multiple LoRA adapters. The integration of LoRA experts leads to a visible improvement across several downstream scenes. However, the mixture of LoRAs (MoE-LoRA) still exhibits its low robustness during tuning and inferring. Inspired by the Riemannian Preconditioners which train LoRA as a sub-space projector, we propose a new training strategy for MoE-LoRA, to stabilize and boost its feature learning procedure by multi-space projections. Examinations on SGD and AdamW optimizers demonstrate the effectiveness of our methodology. Source code is available at <a target="_blank" rel="noopener" href="https://github.com/THUDM/MoELoRA_Riemannian">https://github.com/THUDM/MoELoRA_Riemannian</a>. </p>
<blockquote>
<p>ä¸ºäº†ä¼˜åŒ–åŸºç¡€æ¨¡å‹çš„å¾®è°ƒè¿‡ç¨‹ï¼Œä½ç§©é€‚é…å™¨ï¼ˆLoRAsï¼‰å·²è¢«å¹¿æ³›åº”ç”¨äºå¤šä¸ªé¢†åŸŸï¼ŒåŒ…æ‹¬æŒ‡ä»¤è°ƒæ•´å’Œé¢†åŸŸé€‚åº”ã€‚LoRAçš„åŸºæœ¬ç†å¿µæ˜¯å°†ä¸€ä¸ªæ»¡ç§©çŸ©é˜µåˆ†è§£ä¸ºä¸¤ä¸ªä½ç§©çŸ©é˜µçš„ä¹˜ç§¯ï¼Œä»è€Œé™ä½å­˜å‚¨æ¶ˆè€—å¹¶åŠ é€Ÿè®­ç»ƒè¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œä¸ºäº†è§£å†³LoRAæœ‰é™çš„è¡¨è¾¾å®¹é‡é—®é¢˜ï¼Œå¼•å…¥äº†æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ–¹æ³•ï¼Œå°†å¤šä¸ªLoRAé€‚é…å™¨ç»“åˆèµ·æ¥ã€‚LoRAä¸“å®¶çš„èåˆåœ¨å¤šä¸ªä¸‹æ¸¸åœºæ™¯ä¸­å¸¦æ¥äº†æ˜æ˜¾çš„æ”¹è¿›ã€‚ç„¶è€Œï¼ŒLoRAçš„æ··åˆï¼ˆMoE-LoRAï¼‰åœ¨è°ƒæ•´å’Œæ¨æ–­è¿‡ç¨‹ä¸­ä»ç„¶è¡¨ç°å‡ºè¾ƒä½çš„é²æ£’æ€§ã€‚å—é»æ›¼é¢„å¤„ç†å™¨è®­ç»ƒçš„å¯å‘ï¼Œæˆ‘ä»¬å°†LoRAè®­ç»ƒä¸ºå­ç©ºé—´æŠ•å½±å™¨ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„MoE-LoRAè®­ç»ƒç­–ç•¥ï¼Œé€šè¿‡å¤šç©ºé—´æŠ•å½±æ¥ç¨³å®šå’Œæå‡å…¶ç‰¹æ€§å­¦ä¹ ç¨‹åºã€‚å¯¹SGDå’ŒAdamWä¼˜åŒ–å™¨çš„æµ‹è¯•è¯æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/THUDM/MoELoRA_Riemannian%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/THUDM/MoELoRA_Riemannianæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.15828v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<pre><code>LoRAsï¼ˆä½ç§©é€‚é…å™¨ï¼‰å·²è¢«å¹¿æ³›åº”ç”¨äºå¤šä¸ªé¢†åŸŸä»¥ä¼˜åŒ–åŸºç¡€æ¨¡å‹çš„å¾®è°ƒè¿‡ç¨‹ã€‚å…¶é€šè¿‡åˆ†è§£å…¨ç§©çŸ©é˜µä»¥é™ä½å­˜å‚¨æ¶ˆè€—å¹¶åŠ é€Ÿè®­ç»ƒã€‚ä¸ºè§£å†³LoRAçš„æœ‰é™è¡¨è¾¾èƒ½åŠ›ï¼Œå¼•å…¥äº†MoEï¼ˆæ··åˆä¸“å®¶ï¼‰æ¥ç»“åˆå¤šä¸ªLoRAé€‚é…å™¨ã€‚å°†LoRAä¸“å®¶é›†æˆåï¼Œåœ¨å¤šä¸ªä¸‹æ¸¸åœºæ™¯ä¸­äº§ç”Ÿäº†æ˜æ˜¾çš„æ”¹è¿›æ•ˆæœã€‚ç„¶è€Œï¼ŒMoE-LoRAï¼ˆæ··åˆä½ç§©é€‚é…å™¨ï¼‰åœ¨è°ƒä¼˜å’Œæ¨æ–­æ—¶ä»è¡¨ç°å‡ºè¾ƒä½çš„ç¨³å¥æ€§ã€‚å—é»æ›¼é¢„å¤„ç†å™¨å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„MoE-LoRAè®­ç»ƒç­–ç•¥ï¼Œé€šè¿‡å¤šç©ºé—´æŠ•å½±æ¥ç¨³å®šå’Œå¢å¼ºå…¶ç‰¹æ€§å­¦ä¹ ç¨‹åºã€‚åœ¨SGDå’ŒAdamWä¼˜åŒ–å™¨ä¸Šçš„å®éªŒéªŒè¯äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚
å¼€æºä»£ç ä½äºï¼š[https://github.com/THUDM/MoELoRA_Riemannianã€‚](https://github.com/THUDM/MoELoRA_Riemannian%E3%80%82)

**Key Takeaways**

1. LoRAsç”¨äºä¼˜åŒ–åŸºç¡€æ¨¡å‹å¾®è°ƒè¿‡ç¨‹ï¼Œå·²å¹¿æ³›åº”ç”¨äºä¸åŒé¢†åŸŸã€‚
2. LoRAsé€šè¿‡åˆ†è§£å…¨ç§©çŸ©é˜µæ¥å‡å°‘å­˜å‚¨æ¶ˆè€—å¹¶åŠ é€Ÿè®­ç»ƒã€‚
3. ä¸ºæå‡LoRAçš„è¡¨è¾¾èƒ½åŠ›ï¼Œå¼•å…¥äº†MoEæ¥ç»“åˆå¤šä¸ªLoRAé€‚é…å™¨ã€‚
4. MoE-LoRAåœ¨å¤šä¸ªä¸‹æ¸¸åœºæ™¯ä¸­è¡¨ç°ä¼˜å¼‚ã€‚
5. MoE-LoRAåœ¨è°ƒä¼˜å’Œæ¨æ–­æ—¶å­˜åœ¨ç¨³å¥æ€§é—®é¢˜ã€‚
6. å—é»æ›¼é¢„å¤„ç†å™¨å¯å‘ï¼Œæå‡ºäº†é€šè¿‡å¤šç©ºé—´æŠ•å½±çš„ç¨³å®šè®­ç»ƒç­–ç•¥ä»¥å¢å¼ºMoE-LoRAçš„ç‰¹æ€§å­¦ä¹ ã€‚
</code></pre>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.15828">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dc2837f7841dacb9dbf39950a82651e2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c26c462f117de33aec6f4d4477c4886b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-75a1e164b442bbc8e2c6768d9995d66a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f46614f2935c5e462d76afa54da3ecd.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Generalizing-From-Short-to-Long-Effective-Data-Synthesis-for-Long-Context-Instruction-Tuning"><a href="#Generalizing-From-Short-to-Long-Effective-Data-Synthesis-for-Long-Context-Instruction-Tuning" class="headerlink" title="Generalizing From Short to Long: Effective Data Synthesis for   Long-Context Instruction Tuning"></a>Generalizing From Short to Long: Effective Data Synthesis for   Long-Context Instruction Tuning</h2><p><strong>Authors:Wenhao Zhu, Pinzhen Chen, Hanxu Hu, Shujian Huang, Fei Yuan, Jiajun Chen, Alexandra Birch</strong></p>
<p>Long-context modelling for large language models (LLMs) has been a key area of recent research because many real world use cases require reasoning over longer inputs such as documents. The focus of research into modelling long context has been on how to model position and there has been little investigation into other important aspects of language modelling such as instruction tuning. Long context training examples are challenging and expensive to create and use. In this paper, we investigate how to design instruction data for the post-training phase of a long context pre-trained model: how much and what type of context is needed for optimal and efficient post-training. Our controlled study reveals that models instruction-tuned on short contexts can effectively generalize to longer ones, while also identifying other critical factors such as instruction difficulty and context composition. Based on these findings, we propose context synthesis, a novel data synthesis framework that leverages off-the-shelf LLMs to generate extended background contexts for high-quality instruction-answer pairs. Experiment results on the document-level benchmark (LongBench) demonstrate that our proposed approach outperforms previous instruction synthesis approaches and comes close to the performance of human-annotated long-context instruction data. The project will be available at: <a target="_blank" rel="noopener" href="https://github.com/NJUNLP/context-synthesis">https://github.com/NJUNLP/context-synthesis</a>. </p>
<blockquote>
<p>å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é•¿æ–‡æœ¬å»ºæ¨¡ä¸€ç›´æ˜¯è¿‘æœŸç ”ç©¶çš„é‡ç‚¹é¢†åŸŸï¼Œå› ä¸ºè®¸å¤šç°å®ä¸–ç•Œçš„åº”ç”¨åœºæ™¯éœ€è¦å¯¹æ–‡æ¡£ç­‰æ›´é•¿çš„è¾“å…¥è¿›è¡Œæ¨ç†ã€‚å…³äºé•¿æ–‡æœ¬å»ºæ¨¡çš„ç ”ç©¶é‡ç‚¹åœ¨äºå¦‚ä½•å»ºæ¨¡ä½ç½®ä¿¡æ¯ï¼Œè€Œå¯¹äºè¯­è¨€å»ºæ¨¡çš„å…¶ä»–é‡è¦æ–¹é¢ï¼Œå¦‚æŒ‡ä»¤å¾®è°ƒçš„ç ”ç©¶åˆ™ç›¸å¯¹è¾ƒå°‘ã€‚é•¿æ–‡æœ¬è®­ç»ƒæ ·æœ¬çš„åˆ›å»ºå’Œä½¿ç”¨å…·æœ‰æŒ‘æˆ˜æ€§å’Œé«˜æ˜‚çš„æˆæœ¬ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å¦‚ä½•ä¸ºé•¿æ–‡æœ¬é¢„è®­ç»ƒæ¨¡å‹çš„è®­ç»ƒåé˜¶æ®µè®¾è®¡æŒ‡ä»¤æ•°æ®ï¼šéœ€è¦å¤šå°‘ä»¥åŠéœ€è¦å“ªç§ç±»å‹çš„ä¸Šä¸‹æ–‡æ‰èƒ½è¾¾åˆ°æœ€ä½³å’Œé«˜æ•ˆçš„è®­ç»ƒåæ•ˆæœã€‚æˆ‘ä»¬çš„å¯¹ç…§ç ”ç©¶è¡¨æ˜ï¼Œåœ¨çŸ­æ–‡æœ¬ä¸Šè°ƒæ ¡çš„æ¨¡å‹å¯ä»¥æœ‰æ•ˆåœ°æ¨å¹¿åˆ°æ›´é•¿çš„æ–‡æœ¬ä¸Šï¼ŒåŒæ—¶ç¡®å®šäº†å…¶ä»–å…³é”®å› ç´ ï¼Œå¦‚æŒ‡ä»¤éš¾åº¦å’Œä¸Šä¸‹æ–‡ç»„åˆã€‚åŸºäºè¿™äº›å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸Šä¸‹æ–‡åˆæˆè¿™ä¸€æ–°å‹æ•°æ®åˆæˆæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨ç°æˆçš„LLMç”Ÿæˆé«˜è´¨é‡çš„æŒ‡ä»¤ç­”æ¡ˆå¯¹æ‰€éœ€çš„é•¿èƒŒæ™¯ä¸Šä¸‹æ–‡ã€‚åœ¨æ–‡æ¡£çº§åŸºå‡†æµ‹è¯•ï¼ˆLongBenchï¼‰ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•ä¼˜äºä»¥å‰çš„æŒ‡ä»¤åˆæˆæ–¹æ³•ï¼Œå¹¶æ¥è¿‘äººç±»æ ‡æ³¨çš„é•¿æ–‡æœ¬æŒ‡ä»¤æ•°æ®çš„æ€§èƒ½ã€‚è¯¥é¡¹ç›®å°†åœ¨ä»¥ä¸‹ç½‘å€æä¾›ï¼š<a target="_blank" rel="noopener" href="https://github.com/NJUNLP/context-synthesis">https://github.com/NJUNLP/context-synthesis</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.15592v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é•¿æ–‡æœ¬å»ºæ¨¡é—®é¢˜ã€‚ç ”ç©¶é‡ç‚¹åœ¨äºå¦‚ä½•è®¾è®¡é€‚ç”¨äºé•¿æ–‡æœ¬é¢„è®­ç»ƒæ¨¡å‹çš„åè®­ç»ƒé˜¶æ®µçš„æŒ‡ä»¤æ•°æ®ï¼ŒåŒ…æ‹¬éœ€è¦å¤šå°‘å’Œä½•ç§ç±»å‹çš„ä¸Šä¸‹æ–‡æ¥å®ç°æœ€ä¼˜å’Œé«˜æ•ˆçš„è®­ç»ƒã€‚ç ”ç©¶å‘ç°ï¼ŒåŸºäºçŸ­æ–‡æœ¬è®­ç»ƒçš„æ¨¡å‹å¯ä»¥æœ‰æ•ˆåœ°æ³›åŒ–åˆ°é•¿æ–‡æœ¬åœºæ™¯ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªåä¸ºâ€œä¸Šä¸‹æ–‡åˆæˆâ€çš„æ–°å‹æ•°æ®åˆæˆæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨ç°æˆçš„LLMç”Ÿæˆé«˜è´¨é‡æŒ‡ä»¤ç­”æ¡ˆå¯¹çš„é«˜èƒŒæ™¯ä¸Šä¸‹æ–‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ–‡æ¡£çº§åˆ«çš„åŸºå‡†æµ‹è¯•ä¸Šä¼˜äºå…ˆå‰çš„æŒ‡ä»¤åˆæˆæ–¹æ³•ï¼Œå¹¶æ¥è¿‘äººç±»æ ‡æ³¨çš„é•¿æ–‡æœ¬æŒ‡ä»¤æ•°æ®æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é•¿æ–‡æœ¬å»ºæ¨¡æ˜¯LLMç ”ç©¶çš„é‡ç‚¹ï¼Œéœ€è¦å¤„ç†æ›´é•¿çš„è¾“å…¥å¦‚æ–‡æ¡£ã€‚</li>
<li>ç›®å‰ç ”ç©¶å¤šå…³æ³¨äºå¦‚ä½•å»ºæ¨¡ä½ç½®ä¿¡æ¯ï¼Œä½†å…¶ä»–è¯­è¨€å»ºæ¨¡çš„é‡è¦æ–¹é¢å¦‚æŒ‡ä»¤è°ƒæ•´å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ã€‚</li>
<li>åˆ›å»ºå’Œä½¿ç”¨é•¿æ–‡æœ¬è®­ç»ƒç¤ºä¾‹å…·æœ‰æŒ‘æˆ˜æ€§å’Œæˆæœ¬ã€‚</li>
<li>æŒ‡ä»¤æ•°æ®çš„è®¾è®¡å¯¹äºé•¿æ–‡æœ¬é¢„è®­ç»ƒæ¨¡å‹çš„åè®­ç»ƒé˜¶æ®µè‡³å…³é‡è¦ã€‚</li>
<li>ç ”ç©¶å‘ç°åŸºäºçŸ­æ–‡æœ¬çš„æ¨¡å‹å¯ä»¥æœ‰æ•ˆåœ°æ³›åŒ–åˆ°é•¿æ–‡æœ¬åœºæ™¯ã€‚</li>
<li>æå‡ºäº†åä¸ºâ€œä¸Šä¸‹æ–‡åˆæˆâ€çš„æ–°å‹æ•°æ®åˆæˆæ¡†æ¶ï¼Œåˆ©ç”¨LLMç”Ÿæˆæ‰©å±•çš„èƒŒæ™¯ä¸Šä¸‹æ–‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.15592">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e4292d27b4cc1081f1edc1eafbc41ca0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-29eac173b31ff627b5c13c27c4bfb17e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f6d4bd1e5d4522299545a1a8809465b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6b51cded874b42cf9615f29ae0742892.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4584d1a3de23b17a70c6a74e7f511881.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Evaluating-Social-Biases-in-LLM-Reasoning"><a href="#Evaluating-Social-Biases-in-LLM-Reasoning" class="headerlink" title="Evaluating Social Biases in LLM Reasoning"></a>Evaluating Social Biases in LLM Reasoning</h2><p><strong>Authors:Xuyang Wu, Jinming Nian, Zhiqiang Tao, Yi Fang</strong></p>
<p>In the recent development of AI reasoning, large language models (LLMs) are trained to automatically generate chain-of-thought reasoning steps, which have demonstrated compelling performance on math and coding tasks. However, when bias is mixed within the reasoning process to form strong logical arguments, it could cause even more harmful results and further induce hallucinations. In this paper, we have evaluated the 8B and 32B variants of DeepSeek-R1 against their instruction tuned counterparts on the BBQ dataset, and investigated the bias that is elicited out and being amplified through reasoning steps. To the best of our knowledge, this empirical study is the first to assess bias issues in LLM reasoning. </p>
<blockquote>
<p>åœ¨äººå·¥æ™ºèƒ½æ¨ç†çš„æœ€è¿‘å‘å±•ä¸­ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç»è¿‡è®­ç»ƒèƒ½å¤Ÿè‡ªåŠ¨äº§ç”Ÿæ€ç»´é“¾æ¨ç†æ­¥éª¤ï¼Œè¿™åœ¨æ•°å­¦å’Œç¼–ç ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†ä»¤äººä¿¡æœçš„æ•ˆæœã€‚ç„¶è€Œï¼Œå½“æ¨ç†è¿‡ç¨‹ä¸­çš„åè§è¢«ç”¨æ¥å½¢æˆå¼ºæœ‰åŠ›çš„è®ºè¯æ—¶ï¼Œå®ƒå¯èƒ½ä¼šäº§ç”Ÿæ›´ç³Ÿç³•çš„ç»“æœå¹¶å¯¼è‡´è¿›ä¸€æ­¥çš„å¹»è§‰ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬åœ¨BBQæ•°æ®é›†ä¸Šè¯„ä¼°äº†DeepSeek-R1çš„8Bå’Œ32Bå˜ç§ä¸å…¶æŒ‡ä»¤è°ƒæ•´åçš„æ¨¡å‹ï¼Œå¹¶ç ”ç©¶äº†é€šè¿‡æ¨ç†æ­¥éª¤æ‰€æ¿€å‘å’Œæ”¾å¤§çš„åè§ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™é¡¹å®è¯ç ”ç©¶é¦–æ¬¡å¯¹LLMæ¨ç†ä¸­çš„åè§é—®é¢˜è¿›è¡Œäº†è¯„ä¼°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.15361v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªåŠ¨ç”Ÿæˆæ€ç»´é“¾æ¨ç†æ­¥éª¤æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œç”¨äºå®Œæˆæ•°å­¦å’Œç¼–ç¨‹ä»»åŠ¡ã€‚ç„¶è€Œï¼Œå½“å­˜åœ¨åè§æ—¶ï¼Œå¯èƒ½å¯¼è‡´æœ‰å®³ç»“æœå¹¶å¼•å‘å¹»è§‰ã€‚æœ¬æ–‡è¯„ä¼°äº†DeepSeek-R1çš„8Bå’Œ32Bç‰ˆæœ¬ä¸æŒ‡ä»¤è°ƒä¼˜åçš„åŒç±»äº§å“åœ¨BBQæ•°æ®é›†ä¸Šçš„è¡¨ç°ï¼Œå¹¶æ¢è®¨äº†é€šè¿‡æ¨ç†æ­¥éª¤å¼•å‘çš„åè§æ”¾å¤§é—®é¢˜ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–æ¬¡å¯¹LLMæ¨ç†ä¸­çš„åè§é—®é¢˜è¿›è¡Œå®è¯ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯è‡ªåŠ¨è¿›è¡Œé“¾å¼æ€ç»´æ¨ç†æ­¥éª¤ï¼Œé€‚ç”¨äºæ•°å­¦å’Œç¼–ç¨‹ä»»åŠ¡ã€‚</li>
<li>åœ¨LLMçš„æ¨ç†è¿‡ç¨‹ä¸­å¼•å…¥åè§ä¼šå¯¼è‡´æ›´ä¸¥é‡çš„åæœï¼ŒåŒ…æ‹¬å¼•å‘å¹»è§‰ã€‚</li>
<li>è¯„ä¼°äº†DeepSeek-R1çš„8Bå’Œ32Bç‰ˆæœ¬ä¸æŒ‡ä»¤è°ƒä¼˜æ¨¡å‹åœ¨BBQæ•°æ®é›†ä¸Šçš„è¡¨ç°å·®å¼‚ã€‚</li>
<li>ç ”ç©¶å‘ç°é€šè¿‡æ¨ç†æ­¥éª¤ä¼šæ”¾å¤§åè§ã€‚</li>
<li>è¯¥ç ”ç©¶æ˜¯é¦–æ¬¡é’ˆå¯¹LLMæ¨ç†ä¸­çš„åè§é—®é¢˜è¿›è¡Œå®è¯ç ”ç©¶ã€‚</li>
<li>å¯¹äºå¦‚ä½•å‡å°‘æˆ–é¿å…LLMä¸­çš„åè§é—®é¢˜ï¼Œéœ€è¦è¿›ä¸€æ­¥çš„ç ”ç©¶å’Œæ¢è®¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.15361">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-aa297a491d822e83c949317b0dda9ff3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e07b3e4b996a5c61ea75375e55961e0e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c080fcf64df0cc1a83fb77033b75578e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-754e2056174c9ad4409ab9cbe3d0cdc9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d6877032c6396f0aa50848f2d141109a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a8f872230d4667fd36276b2eda680e63.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bb15bc8d16dbd32e83fb871db9dfed03.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-26/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-26/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-26/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-4e962cb65e30b43066e790af6db94399.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-26  IGDA Interactive Graph Discovery through Large Language Model Agents
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-26
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-23/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-a47f2f13235fbbbaee0fbc6127180b0e.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-23  OpenCharacter Training Customizable Role-Playing LLMs with Large-Scale   Synthetic Personas
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-23
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">19778.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
