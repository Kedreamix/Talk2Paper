<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Agent">
    <meta name="description" content="Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-26  IGDA Interactive Graph Discovery through Large Language Model Agents">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Agent | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-4e962cb65e30b43066e790af6db94399.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Agent</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Agent/">
                                <span class="chip bg-color">Agent</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                Agent
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-26
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    18.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    76 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-02-26-æ›´æ–°"><a href="#2025-02-26-æ›´æ–°" class="headerlink" title="2025-02-26 æ›´æ–°"></a>2025-02-26 æ›´æ–°</h1><h2 id="IGDA-Interactive-Graph-Discovery-through-Large-Language-Model-Agents"><a href="#IGDA-Interactive-Graph-Discovery-through-Large-Language-Model-Agents" class="headerlink" title="IGDA: Interactive Graph Discovery through Large Language Model Agents"></a>IGDA: Interactive Graph Discovery through Large Language Model Agents</h2><p><strong>Authors:Alex Havrilla, David Alvarez-Melis, Nicolo Fusi</strong></p>
<p>Large language models ($\textbf{LLMs}$) have emerged as a powerful method for discovery. Instead of utilizing numerical data, LLMs utilize associated variable $\textit{semantic metadata}$ to predict variable relationships. Simultaneously, LLMs demonstrate impressive abilities to act as black-box optimizers when given an objective $f$ and sequence of trials. We study LLMs at the intersection of these two capabilities by applying LLMs to the task of $\textit{interactive graph discovery}$: given a ground truth graph $G^*$ capturing variable relationships and a budget of $I$ edge experiments over $R$ rounds, minimize the distance between the predicted graph $\hat{G}_R$ and $G^*$ at the end of the $R$-th round. To solve this task we propose $\textbf{IGDA}$, a LLM-based pipeline incorporating two key components: 1) an LLM uncertainty-driven method for edge experiment selection 2) a local graph update strategy utilizing binary feedback from experiments to improve predictions for unselected neighboring edges. Experiments on eight different real-world graphs show our approach often outperforms all baselines including a state-of-the-art numerical method for interactive graph discovery. Further, we conduct a rigorous series of ablations dissecting the impact of each pipeline component. Finally, to assess the impact of memorization, we apply our interactive graph discovery strategy to a complex, new (as of July 2024) causal graph on protein transcription factors, finding strong performance in a setting where memorization is impossible. Overall, our results show IGDA to be a powerful method for graph discovery complementary to existing numerically driven approaches. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å·²ç»æˆä¸ºä¸€ç§å¼ºå¤§çš„å‘ç°æ–¹æ³•ã€‚LLMsä¸æ˜¯åˆ©ç”¨æ•°å€¼æ•°æ®ï¼Œè€Œæ˜¯åˆ©ç”¨å…³è”å˜é‡è¯­ä¹‰å…ƒæ•°æ®æ¥é¢„æµ‹å˜é‡å…³ç³»ã€‚åŒæ—¶ï¼ŒLLMsåœ¨è¢«èµ‹äºˆç›®æ ‡få’Œä¸€ç³»åˆ—è¯•éªŒæ—¶ï¼Œè¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„ä½œä¸ºé»‘ç®±ä¼˜åŒ–å™¨çš„èƒ½åŠ›ã€‚æˆ‘ä»¬é€šè¿‡å°†LLMsåº”ç”¨äºäº¤äº’å¼å›¾å‘ç°ä»»åŠ¡ï¼Œæ¥ç ”ç©¶è¿™ä¸¤ç§èƒ½åŠ›çš„äº¤é›†ï¼šç»™å®šä¸€ä¸ªæ•è·å˜é‡å…³ç³»çš„çœŸå®å›¾G<em>å’Œä¸€ä¸ªåœ¨Rè½®ä¸­Iæ¡è¾¹å®éªŒé¢„ç®—çš„é™åˆ¶ï¼Œæœ€å°åŒ–åœ¨ç¬¬Rè½®ç»“æŸæ—¶é¢„æµ‹å›¾Gå¸½Rä¸å®é™…å›¾G</em>ä¹‹é—´çš„è·ç¦»ã€‚ä¸ºè§£å†³æ­¤ä»»åŠ¡ï¼Œæˆ‘ä»¬æå‡ºIGDAï¼Œè¿™æ˜¯ä¸€ç§åŸºäºLLMçš„ç®¡é“ï¼ŒåŒ…å«ä¸¤ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ï¼š1ï¼‰ä¸€ç§LLMä¸ç¡®å®šæ€§é©±åŠ¨æ–¹æ³•è¿›è¡Œè¾¹ç¼˜å®éªŒé€‰æ‹©ï¼›2ï¼‰ä¸€ç§åˆ©ç”¨å®éªŒäºŒè¿›åˆ¶åé¦ˆæ›´æ–°å±€éƒ¨å›¾çš„ç­–ç•¥ï¼Œä»¥æé«˜æœªé€‰æ‹©ç›¸é‚»è¾¹ç¼˜çš„é¢„æµ‹ã€‚åœ¨å…«ä¸ªä¸åŒçœŸå®ä¸–ç•Œå›¾å½¢ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é€šå¸¸ä¼˜äºæ‰€æœ‰åŸºçº¿ï¼ŒåŒ…æ‹¬ç”¨äºäº¤äº’å¼å›¾å½¢å‘ç°çš„æœ€æ–°æ•°å€¼æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¿›è¡Œäº†ä¸€ç³»åˆ—ä¸¥æ ¼çš„æ¶ˆèç ”ç©¶ï¼Œåˆ†æäº†æ¯ä¸ªç®¡é“ç»„ä»¶çš„å½±å“ã€‚æœ€åï¼Œä¸ºäº†è¯„ä¼°è®°å¿†çš„å½±å“ï¼Œæˆ‘ä»¬å°†äº¤äº’å¼å›¾å‘ç°ç­–ç•¥åº”ç”¨äºä¸€ä¸ªå¤æ‚çš„ã€å…¨æ–°çš„ï¼ˆæˆªè‡³2024å¹´7æœˆï¼‰è›‹ç™½è´¨è½¬å½•å› å­å› æœå›¾ï¼Œå¹¶åœ¨ä¸å¯èƒ½å‘ç”Ÿè®°å¿†çš„æƒ…å†µä¸‹å–å¾—äº†è‰¯å¥½çš„æ€§èƒ½ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬çš„ç»“æœè¯æ˜äº†IGDAåœ¨å›¾å½¢å‘ç°æ–¹é¢æ˜¯ä¸€ç§å¼ºå¤§çš„æ–¹æ³•ï¼Œä¸ç°æœ‰çš„æ•°å€¼é©±åŠ¨æ–¹æ³•ç›¸è¾…ç›¸æˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17189v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä½œä¸ºä¸€ç§å¼ºå¤§çš„å‘ç°æ–¹æ³•å·²ç»å´­éœ²å¤´è§’ã€‚LLMsåˆ©ç”¨å…³è”å˜é‡è¯­ä¹‰å…ƒæ•°æ®é¢„æµ‹å˜é‡å…³ç³»ï¼Œè€Œéä½¿ç”¨æ•°å€¼æ•°æ®ã€‚åŒæ—¶ï¼ŒLLMsåœ¨ç»™å®šç›®æ ‡å‡½æ•°å’Œä¸€ç³»åˆ—è¯•éªŒæ—¶ï¼Œå±•ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„ä½œä¸ºé»‘ç®±ä¼˜åŒ–å™¨çš„èƒ½åŠ›ã€‚æœ¬ç ”ç©¶åœ¨LLMsçš„è¿™ä¸¤ç§èƒ½åŠ›äº¤é›†å¤„å±•å¼€æ¢ç´¢ï¼Œå°†LLMsåº”ç”¨äºäº¤äº’å¼å›¾å‘ç°ä»»åŠ¡ï¼šç»™å®šçœŸå®å›¾G<em>ï¼Œä»¥åŠIæ¡è¾¹å®éªŒé¢„ç®—ï¼Œåœ¨Rè½®å†…æœ€å°åŒ–é¢„æµ‹å›¾GRä¸G</em>ä¹‹é—´çš„è·ç¦»ã€‚ä¸ºè§£å†³æ­¤ä»»åŠ¡ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºLLMçš„IGDAç®¡é“ï¼ŒåŒ…å«ä¸¤ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ï¼š1ï¼‰LLMä¸ç¡®å®šæ€§é©±åŠ¨çš„è¾¹ç¼˜å®éªŒé€‰æ‹©æ–¹æ³•ï¼›2ï¼‰åˆ©ç”¨å®éªŒäºŒè¿›åˆ¶åé¦ˆæ›´æ–°å±€éƒ¨å›¾ç­–ç•¥ï¼Œä»¥æé«˜æœªé€‰æ‹©ç›¸é‚»è¾¹ç¼˜çš„é¢„æµ‹ã€‚åœ¨å…«ä¸ªä¸åŒçœŸå®ä¸–ç•Œå›¾å½¢ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é€šå¸¸ä¼˜äºæ‰€æœ‰åŸºçº¿ï¼ŒåŒ…æ‹¬äº¤äº’å¼å›¾å½¢å‘ç°é¢†åŸŸæœ€å…ˆè¿›çš„æ•°å€¼æ–¹æ³•ã€‚é€šè¿‡ä¸€ç³»åˆ—ä¸¥è°¨çš„åˆ†æå‰¥ç¦»æ¯ä¸ªç®¡é“ç»„ä»¶çš„å½±å“ï¼Œæˆ‘ä»¬è¯„ä¼°äº†è®°å¿†çš„å½±å“ï¼Œå¹¶å°†äº¤äº’å¼å›¾å½¢å‘ç°ç­–ç•¥åº”ç”¨äºå¤æ‚çš„è›‹ç™½è´¨è½¬å½•å› å­å› æœå›¾ï¼ˆæˆªè‡³2024å¹´7æœˆï¼‰ï¼Œåœ¨æ— è®°å¿†æƒ…å†µä¸‹è¡¨ç°å‡ºå¼ºåŠ²æ€§èƒ½ã€‚æ€»ä½“è€Œè¨€ï¼Œæˆ‘ä»¬çš„ç»“æœè¯æ˜äº†IGDAåœ¨å›¾å‘ç°é¢†åŸŸçš„å¼ºå¤§èƒ½åŠ›ï¼Œæ˜¯å¯¹ç°æœ‰æ•°å€¼é©±åŠ¨æ–¹æ³•çš„æœ‰åŠ›è¡¥å……ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åˆ©ç”¨è¯­ä¹‰å…ƒæ•°æ®é¢„æµ‹å˜é‡å…³ç³»ï¼Œå±•ç°å‡ºå¼ºå¤§çš„å‘ç°èƒ½åŠ›ã€‚</li>
<li>LLMså…¼å…·é»‘ç®±ä¼˜åŒ–å™¨çš„ç‰¹æ€§ï¼Œèƒ½å¤Ÿåœ¨ç»™å®šç›®æ ‡å‡½æ•°å’Œä¸€ç³»åˆ—è¯•éªŒæ—¶è¿›è¡Œä¼˜åŒ–ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºLLMçš„äº¤äº’å¼å›¾å‘ç°æ–¹æ³•IGDAï¼Œç»“åˆä¸ç¡®å®šæ€§é©±åŠ¨çš„è¾¹ç¼˜å®éªŒé€‰æ‹©æ–¹æ³•å’Œå±€éƒ¨å›¾æ›´æ–°ç­–ç•¥ã€‚</li>
<li>åœ¨å¤šä¸ªçœŸå®ä¸–ç•Œå›¾å½¢ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒIGDAæ–¹æ³•é€šå¸¸ä¼˜äºç°æœ‰æ•°å€¼æ–¹æ³•å’Œå…¶ä»–åŸºçº¿ã€‚</li>
<li>IGDAé€šè¿‡ä¸€ç³»åˆ—ä¸¥è°¨çš„åˆ†æè¯„ä¼°äº†æ¯ä¸ªç®¡é“ç»„ä»¶çš„å½±å“ï¼Œå¹¶è¯æ˜å…¶æœ‰æ•ˆæ€§ã€‚</li>
<li>å°†IGDAåº”ç”¨äºå¤æ‚çš„è›‹ç™½è´¨è½¬å½•å› å­å› æœå›¾ï¼Œåœ¨æ— è®°å¿†æƒ…å†µä¸‹è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>IGDAæ˜¯ä¸€ç§å¼ºå¤§çš„å›¾å‘ç°æ–¹æ³•ï¼Œå¯¹ç°æœ‰æ•°å€¼é©±åŠ¨æ–¹æ³•å½¢æˆæœ‰ç›Šè¡¥å……ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17189">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1cf3ada404b5ddc0606009bc36c4ca47.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4eafc3bd929301594c3b25d4c9531450.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee883acbe563648b37ba0adc646ee86c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-163ba46a602d3d183e3b97da248784b7.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="MA2RL-Masked-Autoencoders-for-Generalizable-Multi-Agent-Reinforcement-Learning"><a href="#MA2RL-Masked-Autoencoders-for-Generalizable-Multi-Agent-Reinforcement-Learning" class="headerlink" title="MA2RL: Masked Autoencoders for Generalizable Multi-Agent Reinforcement   Learning"></a>MA2RL: Masked Autoencoders for Generalizable Multi-Agent Reinforcement   Learning</h2><p><strong>Authors:Jinyuan Feng, Min Chen, Zhiqiang Pu, Yifan Xu, Yanyan Liang</strong></p>
<p>To develop generalizable models in multi-agent reinforcement learning, recent approaches have been devoted to discovering task-independent skills for each agent, which generalize across tasks and facilitate agentsâ€™ cooperation. However, particularly in partially observed settings, such approaches struggle with sample efficiency and generalization capabilities due to two primary challenges: (a) How to incorporate global states into coordinating the skills of different agents? (b) How to learn generalizable and consistent skill semantics when each agent only receives partial observations? To address these challenges, we propose a framework called \textbf{M}asked \textbf{A}utoencoders for \textbf{M}ulti-\textbf{A}gent \textbf{R}einforcement \textbf{L}earning (MA2RL), which encourages agents to infer unobserved entities by reconstructing entity-states from the entity perspective. The entity perspective helps MA2RL generalize to diverse tasks with varying agent numbers and action spaces. Specifically, we treat local entity-observations as masked contexts of the global entity-states, and MA2RL can infer the latent representation of dynamically masked entities, facilitating the assignment of task-independent skills and the learning of skill semantics. Extensive experiments demonstrate that MA2RL achieves significant improvements relative to state-of-the-art approaches, demonstrating extraordinary performance, remarkable zero-shot generalization capabilities and advantageous transferability. </p>
<blockquote>
<p>ä¸ºäº†åœ¨å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ä¸­å¼€å‘å¯æ¨å¹¿çš„æ¨¡å‹ï¼Œæœ€è¿‘çš„æ–¹æ³•ä¸“æ³¨äºä¸ºæ¯ä¸ªæ™ºèƒ½ä½“å‘ç°ä»»åŠ¡ç‹¬ç«‹æŠ€èƒ½ï¼Œè¿™äº›æŠ€èƒ½å¯ä»¥è·¨ä»»åŠ¡æ¨å¹¿å¹¶ä¿ƒè¿›æ™ºèƒ½ä½“ä¹‹é—´çš„åˆä½œã€‚ç„¶è€Œï¼Œç‰¹åˆ«æ˜¯åœ¨éƒ¨åˆ†è§‚æµ‹ç¯å¢ƒä¸­ï¼Œè¿™äº›æ–¹æ³•ç”±äºä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜è€Œé¢ä¸´æ ·æœ¬æ•ˆç‡å’Œæ¨å¹¿èƒ½åŠ›çš„é—®é¢˜ï¼š(a) å¦‚ä½•å°†å…¨å±€çŠ¶æ€çº³å…¥ä¸åŒæ™ºèƒ½ä½“çš„æŠ€èƒ½åè°ƒä¸­ï¼Ÿ(b) å½“æ¯ä¸ªæ™ºèƒ½ä½“åªæ¥æ”¶éƒ¨åˆ†è§‚æµ‹æ—¶ï¼Œå¦‚ä½•å­¦ä¹ å’ŒæŒæ¡å¯æ¨å¹¿å’Œä¸€è‡´çš„æŠ€èƒ½è¯­ä¹‰ï¼Ÿ</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17046v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œä¸ºå¼€å‘å¯æ¨å¹¿æ¨¡å‹ï¼Œç°æœ‰æ–¹æ³•è‡´åŠ›äºå‘ç°ä»»åŠ¡ç‹¬ç«‹æŠ€èƒ½ä»¥ä¿ƒè¿›æ™ºèƒ½ä½“é—´çš„åˆä½œã€‚ä½†åœ¨éƒ¨åˆ†è§‚æµ‹åœºæ™¯ä¸­ï¼Œè¿™äº›æ–¹æ³•é¢ä¸´æ ·æœ¬æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›çš„æŒ‘æˆ˜ã€‚é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºåä¸ºMA2RLçš„æ¡†æ¶ï¼Œé€šè¿‡å®ä½“è§†è§’é¼“åŠ±æ™ºèƒ½ä½“æ¨æ–­æœªè§‚æµ‹å®ä½“ï¼Œé‡å»ºå®ä½“çŠ¶æ€ä»¥å®ç°å…¨å±€çŠ¶æ€çš„åè°ƒã€‚è¯¥æ¡†æ¶åœ¨å¤„ç†å…·æœ‰ä¸åŒæ™ºèƒ½ä½“æ•°é‡å’ŒåŠ¨ä½œç©ºé—´çš„ä»»åŠ¡æ—¶å…·æœ‰æ¨å¹¿æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒMA2RLç›¸è¾ƒäºæœ€æ–°æ–¹æ³•å–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€å¼•äººæ³¨ç›®çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›å’Œæœ‰åˆ©çš„å¯è¿ç§»æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MA2RLæ¡†æ¶ç”¨äºå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ä¸­çš„å¯æ¨å¹¿æ¨¡å‹å¼€å‘ã€‚</li>
<li>è¯¥æ¡†æ¶æ—¨åœ¨å‘ç°ä»»åŠ¡ç‹¬ç«‹æŠ€èƒ½ï¼Œä»¥ä¿ƒè¿›æ™ºèƒ½ä½“é—´çš„åˆä½œã€‚</li>
<li>åœ¨éƒ¨åˆ†è§‚æµ‹åœºæ™¯ä¸­ï¼ŒMA2RLè§£å†³äº†æ ·æœ¬æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›çš„æŒ‘æˆ˜ã€‚</li>
<li>MA2RLé€šè¿‡å®ä½“è§†è§’é¼“åŠ±æ™ºèƒ½ä½“æ¨æ–­æœªè§‚æµ‹å®ä½“ã€‚</li>
<li>MA2RLæ¡†æ¶å¤„ç†ä¸åŒæ™ºèƒ½ä½“æ•°é‡å’ŒåŠ¨ä½œç©ºé—´çš„ä»»åŠ¡æ—¶å…·æœ‰æ¨å¹¿æ€§ã€‚</li>
<li>MA2RLæ¡†æ¶é€šè¿‡é‡å»ºå®ä½“çŠ¶æ€å®ç°å…¨å±€çŠ¶æ€çš„åè°ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17046">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4ade8b9a356fe55c0f7ddf35515c6eba.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d659ebe07f86043ed085a3fc0d783102.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a4f790ae2e1ed7f8201e52c8a36b01d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-29ccd8a925df4f47a17dc096073a2c47.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-58a7c2cec77695a0bd35d494f55c70d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f0b299fa0806c522736ed1d0692d2b2.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Leveraging-Large-Language-Models-for-Effective-and-Explainable-Multi-Agent-Credit-Assignment"><a href="#Leveraging-Large-Language-Models-for-Effective-and-Explainable-Multi-Agent-Credit-Assignment" class="headerlink" title="Leveraging Large Language Models for Effective and Explainable   Multi-Agent Credit Assignment"></a>Leveraging Large Language Models for Effective and Explainable   Multi-Agent Credit Assignment</h2><p><strong>Authors:Kartik Nagpal, Dayi Dong, Jean-Baptiste Bouvier, Negar Mehr</strong></p>
<p>Recent work, spanning from autonomous vehicle coordination to in-space assembly, has shown the importance of learning collaborative behavior for enabling robots to achieve shared goals. A common approach for learning this cooperative behavior is to utilize the centralized-training decentralized-execution paradigm. However, this approach also introduces a new challenge: how do we evaluate the contributions of each agentâ€™s actions to the overall success or failure of the team. This credit assignment problem has remained open, and has been extensively studied in the Multi-Agent Reinforcement Learning literature. In fact, humans manually inspecting agent behavior often generate better credit evaluations than existing methods. We combine this observation with recent works which show Large Language Models demonstrate human-level performance at many pattern recognition tasks. Our key idea is to reformulate credit assignment to the two pattern recognition problems of sequence improvement and attribution, which motivates our novel LLM-MCA method. Our approach utilizes a centralized LLM reward-critic which numerically decomposes the environment reward based on the individualized contribution of each agent in the scenario. We then update the agentsâ€™ policy networks based on this feedback. We also propose an extension LLM-TACA where our LLM critic performs explicit task assignment by passing an intermediary goal directly to each agent policy in the scenario. Both our methods far outperform the state-of-the-art on a variety of benchmarks, including Level-Based Foraging, Robotic Warehouse, and our new Spaceworld benchmark which incorporates collision-related safety constraints. As an artifact of our methods, we generate large trajectory datasets with each timestep annotated with per-agent reward information, as sampled from our LLM critics. </p>
<blockquote>
<p>è¿‘æœŸçš„ç ”ç©¶å·¥ä½œï¼Œä»è‡ªåŠ¨é©¾é©¶è½¦è¾†åè°ƒåˆ°å¤ªç©ºå†…ç»„è£…ï¼Œè¡¨æ˜è¦è®©æœºå™¨äººå®ç°å…±äº«ç›®æ ‡ï¼Œå­¦ä¹ åä½œè¡Œä¸ºè‡³å…³é‡è¦ã€‚å­¦ä¹ è¿™ç§åä½œè¡Œä¸ºçš„ä¸€ç§å¸¸è§æ–¹æ³•æ˜¯é‡‡ç”¨é›†ä¸­è®­ç»ƒã€åˆ†æ•£æ‰§è¡Œçš„æ¨¡å¼ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•ä¹Ÿå¸¦æ¥äº†æ–°çš„æŒ‘æˆ˜ï¼šæˆ‘ä»¬å¦‚ä½•è¯„ä¼°æ¯ä¸ªæ™ºèƒ½ä½“è¡ŒåŠ¨å¯¹å›¢é˜Ÿæ•´ä½“æˆåŠŸæˆ–å¤±è´¥çš„è´¡çŒ®ã€‚è¿™ç§åŠŸåŠ³åˆ†é…é—®é¢˜ä»ç„¶å¼€æ”¾ï¼Œå¹¶åœ¨å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ æ–‡çŒ®ä¸­è¿›è¡Œäº†å¹¿æ³›çš„ç ”ç©¶ã€‚äº‹å®ä¸Šï¼Œäººç±»æ‰‹åŠ¨æ£€æŸ¥æ™ºèƒ½ä½“è¡Œä¸ºé€šå¸¸ä¼šäº§ç”Ÿæ¯”ç°æœ‰æ–¹æ³•æ›´å¥½çš„ä¿¡ç”¨è¯„ä¼°ã€‚æˆ‘ä»¬ç»“åˆè¿™ä¸€è§‚å¯Ÿç»“æœä¸è¿‘æœŸçš„ç ”ç©¶å·¥ä½œï¼Œè¿™äº›ç ”ç©¶å·¥ä½œè¡¨æ˜å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è®¸å¤šæ¨¡å¼è¯†åˆ«ä»»åŠ¡ä¸Šè¡¨ç°å‡ºäº†äººç±»æ°´å¹³çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†ä¿¡ç”¨åˆ†é…é‡æ–°åˆ¶å®šä¸ºåºåˆ—æ”¹è¿›å’Œå½’å±çš„ä¸¤ä¸ªæ¨¡å¼è¯†åˆ«é—®é¢˜ï¼Œè¿™æ¿€å‘äº†æˆ‘ä»¬æ–°é¢–LLM-MCAæ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨é›†ä¸­LLMå¥–åŠ±è¯„è®ºå®¶ï¼Œè¯¥è¯„è®ºå®¶æ ¹æ®åœºæ™¯ä¸­æ¯ä¸ªæ™ºèƒ½ä½“çš„ä¸ªæ€§åŒ–è´¡çŒ®æ¥æ•°å€¼åˆ†è§£ç¯å¢ƒå¥–åŠ±ã€‚ç„¶åæˆ‘ä»¬æ ¹æ®è¿™äº›åé¦ˆæ›´æ–°æ™ºèƒ½ä½“çš„ç­–ç•¥ç½‘ç»œã€‚æˆ‘ä»¬è¿˜æå‡ºäº†LLM-TACAæ‰©å±•ï¼Œå…¶ä¸­æˆ‘ä»¬çš„LLMè¯„è®ºå®¶é€šè¿‡ç›´æ¥å‘åœºæ™¯ä¸­çš„æ¯ä¸ªæ™ºèƒ½ä½“ç­–ç•¥ä¼ é€’ä¸­é—´ç›®æ ‡æ¥æ‰§è¡Œæ˜ç¡®çš„ä»»åŠ¡åˆ†é…ã€‚æˆ‘ä»¬çš„ä¸¤ç§æ–¹æ³•åœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°éƒ½å¤§å¤§è¶…è¿‡äº†æœ€æ–°æ°´å¹³ï¼ŒåŒ…æ‹¬åŸºäºçº§åˆ«çš„è§…é£Ÿã€æœºå™¨äººä»“åº“ä»¥åŠæˆ‘ä»¬æ–°çš„å¤ªç©ºä¸–ç•ŒåŸºå‡†æµ‹è¯•ï¼Œè¯¥æµ‹è¯•ç»“åˆäº†ç¢°æ’ç›¸å…³çš„å®‰å…¨çº¦æŸã€‚ä½œä¸ºæˆ‘ä»¬æ–¹æ³•çš„ä¸€ä¸ªæˆæœï¼Œæˆ‘ä»¬ç”Ÿæˆäº†å¤§é‡çš„è½¨è¿¹æ•°æ®é›†ï¼Œæ¯ä¸ªæ—¶é—´æ­¥éƒ½æ³¨æ˜äº†æ¥è‡ªLLMè¯„è®ºå®¶çš„æŒ‰æ™ºèƒ½ä½“å¥–åŠ±ä¿¡æ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.16863v1">PDF</a> 8 pages+Appendix, 6 Figures, AAMAS 2025</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸç ”ç©¶ï¼Œä»è‡ªåŠ¨é©¾é©¶è½¦è¾†åè°ƒåˆ°å¤ªç©ºç»„è£…ï¼Œå‡¸æ˜¾äº†æœºå™¨äººå­¦ä¹ åä½œè¡Œä¸ºçš„é‡è¦æ€§ä»¥å®ç°å…±åŒç›®æ ‡ã€‚å¸¸è§çš„å­¦ä¹ åä½œè¡Œä¸ºæ–¹æ³•æ˜¯é‡‡ç”¨é›†ä¸­è®­ç»ƒåˆ†å¸ƒå¼æ‰§è¡ŒèŒƒå¼ï¼Œä½†è¿™å¼•å‘äº†æ–°çš„æŒ‘æˆ˜ï¼šå¦‚ä½•è¯„ä¼°æ¯ä¸ªæ™ºèƒ½ä½“å¯¹å›¢é˜Ÿæ•´ä½“æˆè´¥çš„è´¡çŒ®ã€‚æœ¬æ–‡æå‡ºå°†ä¿¡ç”¨åˆ†é…é—®é¢˜è½¬åŒ–ä¸ºåºåˆ—æ”¹è¿›å’Œå½’å› ä¸¤ä¸ªæ¨¡å¼è¯†åˆ«é—®é¢˜ï¼Œå¹¶å¼•å…¥äº†LLM-MCAå’ŒLLM-TACAæ–°æ–¹æ³•æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨é›†ä¸­å¼çš„LLMå¥–åŠ±è¯„è®ºå®¶æ¥æ•°å€¼åˆ†è§£ç¯å¢ƒå¥–åŠ±ï¼ŒåŸºäºæ¯ä¸ªæ™ºèƒ½ä½“åœ¨åœºæ™¯ä¸­çš„ä¸ªæ€§åŒ–è´¡çŒ®ã€‚ç„¶åï¼Œæˆ‘ä»¬æ ¹æ®åé¦ˆæ›´æ–°æ™ºèƒ½ä½“çš„ç­–ç•¥ç½‘ç»œã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å‡è¿œè¶…ç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœºå™¨äººåä½œè¡Œä¸ºå¯¹äºå®ç°å…±åŒç›®æ ‡è‡³å…³é‡è¦ï¼Œä»è‡ªåŠ¨é©¾é©¶åˆ°å¤ªç©ºä»»åŠ¡ç­‰å„ä¸ªé¢†åŸŸå‡æ˜¾ç¤ºå‡ºå…¶é‡è¦æ€§ã€‚</li>
<li>é›†ä¸­è®­ç»ƒåˆ†å¸ƒå¼æ‰§è¡ŒèŒƒå¼æ˜¯å¸¸è§çš„æœºå™¨äººåä½œå­¦ä¹ ç­–ç•¥ã€‚</li>
<li>ä¿¡ç”¨åˆ†é…é—®é¢˜æ˜¯ä¸€ä¸ªæ–°çš„æŒ‘æˆ˜ï¼Œæ—¨åœ¨è¯„ä¼°æ¯ä¸ªæ™ºèƒ½ä½“å¯¹å›¢é˜ŸæˆåŠŸçš„è´¡çŒ®ã€‚</li>
<li>LLM-MCAå’ŒLLM-TACAæ–¹æ³•é€šè¿‡å°†ä¿¡ç”¨åˆ†é…é—®é¢˜è½¬åŒ–ä¸ºåºåˆ—æ”¹è¿›å’Œå½’å› æ¨¡å¼è¯†åˆ«é—®é¢˜æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>LLMå¥–åŠ±è¯„è®ºå®¶ç”¨äºæ•°å€¼åˆ†è§£ç¯å¢ƒå¥–åŠ±ï¼ŒåŸºäºæ¯ä¸ªæ™ºèƒ½ä½“åœ¨åœºæ™¯ä¸­çš„ä¸ªæ€§åŒ–è´¡çŒ®ã€‚</li>
<li>æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¿œè¶…ç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.16863">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d2e160a9ed75c36e9081903d4b9db619.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6840eb6cdb6b7b1184b4c1715791e250.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dcbc1b36dfd7586b60a2bb455bcf4735.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59b28be42c5f1b69ea4e2ac8cfca5722.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Multi-Agent-Autonomous-Driving-Systems-with-Large-Language-Models-A-Survey-of-Recent-Advances"><a href="#Multi-Agent-Autonomous-Driving-Systems-with-Large-Language-Models-A-Survey-of-Recent-Advances" class="headerlink" title="Multi-Agent Autonomous Driving Systems with Large Language Models: A   Survey of Recent Advances"></a>Multi-Agent Autonomous Driving Systems with Large Language Models: A   Survey of Recent Advances</h2><p><strong>Authors:Yaozu Wu, Dongyuan Li, Yankai Chen, Renhe Jiang, Henry Peng Zou, Liancheng Fang, Zhen Wang, Philip S. Yu</strong></p>
<p>Autonomous Driving Systems (ADSs) are revolutionizing transportation by reducing human intervention, improving operational efficiency, and enhancing safety. Large Language Models (LLMs), known for their exceptional planning and reasoning capabilities, have been integrated into ADSs to assist with driving decision-making. However, LLM-based single-agent ADSs face three major challenges: limited perception, insufficient collaboration, and high computational demands. To address these issues, recent advancements in LLM-based multi-agent ADSs have focused on improving inter-agent communication and cooperation. This paper provides a frontier survey of LLM-based multi-agent ADSs. We begin with a background introduction to related concepts, followed by a categorization of existing LLM-based approaches based on different agent interaction modes. We then discuss agent-human interactions in scenarios where LLM-based agents engage with humans. Finally, we summarize key applications, datasets, and challenges in this field to support future research (<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/LLM-based_Multi-agent_ADS-3A5C/README.md">https://anonymous.4open.science/r/LLM-based_Multi-agent_ADS-3A5C/README.md</a>). </p>
<blockquote>
<p>è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿï¼ˆADSï¼‰é€šè¿‡å‡å°‘äººä¸ºå¹²é¢„ã€æé«˜è¿è¥æ•ˆç‡ã€å¢å¼ºå®‰å…¨æ€§ï¼Œæ­£åœ¨ä¸ºäº¤é€šè¿è¾“å¸¦æ¥é©å‘½æ€§çš„å˜é©ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»¥å…¶å‡ºè‰²çš„è§„åˆ’å’Œæ¨ç†èƒ½åŠ›è€Œé—»åï¼Œå·²é›†æˆåˆ°ADSä¸­ï¼ŒååŠ©è¿›è¡Œé©¾é©¶å†³ç­–ã€‚ç„¶è€Œï¼ŒåŸºäºLLMçš„å•ä»£ç†ADSé¢ä¸´ä¸‰å¤§æŒ‘æˆ˜ï¼šæ„ŸçŸ¥æœ‰é™ã€åä½œä¸è¶³å’Œè®¡ç®—éœ€æ±‚é«˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼ŒåŸºäºLLMçš„å¤šä»£ç†ADSçš„æœ€æ–°è¿›å±•ä¸»è¦é›†ä¸­åœ¨æ”¹è¿›ä»£ç†ä¹‹é—´çš„é€šä¿¡å’Œåä½œã€‚æœ¬æ–‡å¯¹åŸºäºLLMçš„å¤šä»£ç†ADSè¿›è¡Œäº†å‰æ²¿è°ƒæŸ¥ã€‚é¦–å…ˆä»ç›¸å…³æ¦‚å¿µçš„èƒŒæ™¯ä»‹ç»å¼€å§‹ï¼Œç„¶åæ ¹æ®ä¸åŒä»£ç†äº¤äº’æ¨¡å¼å¯¹ç°æœ‰çš„åŸºäºLLMçš„æ–¹æ³•è¿›è¡Œåˆ†ç±»ã€‚æ¥ç€è®¨è®ºäº†LLMä»£ç†ä¸äººç±»äº¤äº’çš„åœºæ™¯ã€‚æœ€åï¼Œæˆ‘ä»¬æ€»ç»“äº†è¯¥é¢†åŸŸçš„å…³é”®åº”ç”¨ã€æ•°æ®é›†å’ŒæŒ‘æˆ˜ï¼Œä»¥æ”¯æŒæœªæ¥çš„ç ”ç©¶ï¼ˆ<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/LLM-based_Multi-agent_ADS-3A5C/README.md%EF%BC%89%E3%80%82">https://anonymous.4open.science/r/LLM-based_Multi-agent_ADS-3A5C/README.mdï¼‰ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.16804v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è‡ªä¸»é©¾é©¶ç³»ç»Ÿï¼ˆADSï¼‰æ­£é€šè¿‡å‡å°‘äººå·¥å¹²é¢„ã€æé«˜æ“ä½œæ•ˆç‡åŠå®‰å…¨æ€§ï¼Œé©æ–°äº¤é€šè¿è¾“è¡Œä¸šã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»¥å…¶å‡ºè‰²çš„è§„åˆ’ä¸æ¨ç†èƒ½åŠ›è¢«é›†æˆè‡³ADSä¸­ï¼Œè¾…åŠ©é©¾é©¶å†³ç­–ã€‚ç„¶è€Œï¼ŒLLMå•æ™ºèƒ½ä½“ADSé¢ä¸´æ„ŸçŸ¥å±€é™ã€åä½œä¸è¶³åŠé«˜è®¡ç®—éœ€æ±‚ä¸‰å¤§æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œè¿‘æœŸLLMå¤šæ™ºèƒ½ä½“ADSçš„è¿›å±•é›†ä¸­åœ¨æ”¹å–„æ™ºèƒ½ä½“é—´çš„é€šè®¯ä¸åä½œã€‚æœ¬æ–‡å¯¹LLMå¤šæ™ºèƒ½ä½“ADSè¿›è¡Œå‰æ²¿è°ƒæŸ¥ï¼Œå…ˆä»‹ç»ç›¸å…³èƒŒæ™¯çŸ¥è¯†ï¼Œå†æŒ‰ä¸åŒæ™ºèƒ½ä½“äº¤äº’æ¨¡å¼åˆ†ç±»ç°æœ‰çš„LLMæ–¹æ³•ï¼Œè®¨è®ºLLMæ™ºèƒ½ä½“ä¸äººç±»çš„äº¤äº’åœºæ™¯ï¼Œæœ€åæ€»ç»“è¯¥é¢†åŸŸçš„å…³é”®åº”ç”¨ã€æ•°æ®é›†ä¸æŒ‘æˆ˜ï¼Œä»¥æ”¯æŒæœªæ¥ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªä¸»é©¾é©¶ç³»ç»Ÿï¼ˆADSï¼‰æ­£åœ¨æ”¹å˜äº¤é€šè¿è¾“è¡Œä¸šï¼Œé€šè¿‡å‡å°‘äººå·¥å¹²é¢„å’Œæé«˜æ“ä½œæ•ˆç‡åŠå®‰å…¨æ€§ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²è¢«é›†æˆåˆ°ADSä¸­ï¼Œç”¨äºè¾…åŠ©é©¾é©¶å†³ç­–ï¼Œå…·æœ‰å‡ºè‰²çš„è§„åˆ’å’Œæ¨ç†èƒ½åŠ›ã€‚</li>
<li>LLMå•æ™ºèƒ½ä½“ADSé¢ä¸´æ„ŸçŸ¥å±€é™ã€åä½œä¸è¶³å’Œé«˜è®¡ç®—éœ€æ±‚ç­‰æŒ‘æˆ˜ã€‚</li>
<li>LLMå¤šæ™ºèƒ½ä½“ADSçš„è¿‘æœŸè¿›å±•é›†ä¸­åœ¨æ”¹å–„æ™ºèƒ½ä½“é—´çš„é€šä¿¡å’Œåä½œã€‚</li>
<li>æ–‡ç« æä¾›äº†LLMå¤šæ™ºèƒ½ä½“ADSçš„å‰æ²¿è°ƒæŸ¥ï¼ŒåŒ…æ‹¬ç›¸å…³èƒŒæ™¯çŸ¥è¯†ã€æŒ‰ä¸åŒæ™ºèƒ½ä½“äº¤äº’æ¨¡å¼çš„åˆ†ç±»ã€LLMæ™ºèƒ½ä½“ä¸äººç±»çš„äº¤äº’åœºæ™¯ç­‰ã€‚</li>
<li>è¯¥é¢†åŸŸçš„å…³é”®åº”ç”¨ã€æ•°æ®é›†å’ŒæŒ‘æˆ˜è¢«æ€»ç»“ï¼Œä»¥æ”¯æŒæœªæ¥ç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.16804">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-61f9a2d31194f3c67a5dbee813d3cdb9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3e78021ec39d87add716b6a566ff7ea5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f23ea65e3f47d914d71b7a5314a11bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a4aa2fb8b97f744deab839b7955d2701.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-861f527b7db616b0657d2c8b729fd6fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-11495a1d818004b94b4ec56afa9b6075.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="MobileSteward-Integrating-Multiple-App-Oriented-Agents-with-Self-Evolution-to-Automate-Cross-App-Instructions"><a href="#MobileSteward-Integrating-Multiple-App-Oriented-Agents-with-Self-Evolution-to-Automate-Cross-App-Instructions" class="headerlink" title="MobileSteward: Integrating Multiple App-Oriented Agents with   Self-Evolution to Automate Cross-App Instructions"></a>MobileSteward: Integrating Multiple App-Oriented Agents with   Self-Evolution to Automate Cross-App Instructions</h2><p><strong>Authors:Yuxuan Liu, Hongda Sun, Wei Liu, Jian Luan, Bo Du, Rui Yan</strong></p>
<p>Mobile phone agents can assist people in automating daily tasks on their phones, which have emerged as a pivotal research spotlight. However, existing procedure-oriented agents struggle with cross-app instructions, due to the following challenges: (1) complex task relationships, (2) diverse app environment, and (3) error propagation and information loss in multi-step execution. Drawing inspiration from object-oriented programming principles, we recognize that object-oriented solutions is more suitable for cross-app instruction. To address these challenges, we propose a self-evolving multi-agent framework named MobileSteward, which integrates multiple app-oriented StaffAgents coordinated by a centralized StewardAgent. We design three specialized modules in MobileSteward: (1) Dynamic Recruitment generates a scheduling graph guided by information flow to explicitly associate tasks among apps. (2) Assigned Execution assigns the task to app-oriented StaffAgents, each equipped with app-specialized expertise to address the diversity between apps. (3) Adjusted Evaluation conducts evaluation to provide reflection tips or deliver key information, which alleviates error propagation and information loss during multi-step execution. To continuously improve the performance of MobileSteward, we develop a Memory-based Self-evolution mechanism, which summarizes the experience from successful execution, to improve the performance of MobileSteward. We establish the first English Cross-APP Benchmark (CAPBench) in the real-world environment to evaluate the agentsâ€™ capabilities of solving complex cross-app instructions. Experimental results demonstrate that MobileSteward achieves the best performance compared to both single-agent and multi-agent frameworks, highlighting the superiority of MobileSteward in better handling user instructions with diverse complexity. </p>
<blockquote>
<p>æ‰‹æœºä»£ç†å¯ä»¥ååŠ©ç”¨æˆ·è‡ªåŠ¨åŒ–æ‰‹æœºä¸Šçš„æ—¥å¸¸ä»»åŠ¡ï¼Œè¿™å·²æˆä¸ºä¸€ä¸ªå…³é”®çš„ç ”ç©¶çƒ­ç‚¹ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æµç¨‹å¯¼å‘å‹ä»£ç†åœ¨è·¨åº”ç”¨æŒ‡ä»¤æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼ŒåŸå› åŒ…æ‹¬ä»¥ä¸‹å‡ ç‚¹ï¼šï¼ˆ1ï¼‰ä»»åŠ¡å…³ç³»å¤æ‚ï¼Œï¼ˆ2ï¼‰åº”ç”¨ç¯å¢ƒå¤šæ ·ï¼Œä»¥åŠï¼ˆ3ï¼‰å¤šæ­¥æ‰§è¡Œä¸­çš„é”™è¯¯ä¼ æ’­å’Œä¿¡æ¯ä¸¢å¤±ã€‚ä»é¢å‘å¯¹è±¡ç¼–ç¨‹åŸç†ä¸­æ±²å–çµæ„Ÿï¼Œæˆ‘ä»¬è®¤è¯†åˆ°é¢å‘å¯¹è±¡è§£å†³æ–¹æ¡ˆæ›´é€‚åˆè·¨åº”ç”¨æŒ‡ä»¤ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºMobileStewardçš„è‡ªæˆ‘è¿›åŒ–å¤šä»£ç†æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é›†æˆäº†å¤šä¸ªç”±é›†ä¸­åŒ–StewardAgentåè°ƒçš„åº”ç”¨å¯¼å‘å‹StaffAgentsã€‚æˆ‘ä»¬åœ¨MobileStewardä¸­è®¾è®¡äº†ä¸‰ä¸ªä¸“ç”¨æ¨¡å—ï¼šï¼ˆ1ï¼‰åŠ¨æ€æ‹›å‹Ÿç”Ÿæˆä¸€ä¸ªç”±ä¿¡æ¯æµå¼•å¯¼çš„è°ƒåº¦å›¾ï¼Œä»¥æ˜¾å¼å…³è”åº”ç”¨ç¨‹åºä¹‹é—´çš„ä»»åŠ¡ã€‚ï¼ˆ2ï¼‰ä»»åŠ¡åˆ†é…å°†ä»»åŠ¡åˆ†é…ç»™é¢å‘åº”ç”¨çš„StaffAgentsï¼Œæ¯ä¸ªStaffAgentéƒ½é…å¤‡äº†é’ˆå¯¹åº”ç”¨çš„ä¸“ä¸šçŸ¥è¯†ï¼Œä»¥è§£å†³åº”ç”¨ä¹‹é—´çš„å¤šæ ·æ€§é—®é¢˜ã€‚ï¼ˆ3ï¼‰è°ƒæ•´è¯„ä¼°è¿›è¡Œè¯„ä¼°ä»¥æä¾›åé¦ˆæç¤ºæˆ–ä¼ é€’å…³é”®ä¿¡æ¯ï¼Œè¿™å‡è½»äº†å¤šæ­¥æ‰§è¡Œè¿‡ç¨‹ä¸­çš„é”™è¯¯ä¼ æ’­å’Œä¿¡æ¯ä¸¢å¤±ã€‚ä¸ºäº†ä¸æ–­æé«˜MobileStewardçš„æ€§èƒ½ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§åŸºäºå†…å­˜çš„è‡ªæˆ‘è¿›åŒ–æœºåˆ¶ï¼Œè¯¥æœºåˆ¶ä»æˆåŠŸçš„æ‰§è¡Œä¸­æ€»ç»“ç»éªŒï¼Œä»¥æé«˜MobileStewardçš„æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨çœŸå®ç¯å¢ƒä¸­å»ºç«‹äº†é¦–ä¸ªè‹±æ–‡è·¨åº”ç”¨åŸºå‡†æµ‹è¯•ï¼ˆCAPBenchï¼‰ï¼Œä»¥è¯„ä¼°ä»£ç†è§£å†³å¤æ‚è·¨åº”ç”¨æŒ‡ä»¤çš„èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸å•ä»£ç†å’Œå¤šä»£ç†æ¡†æ¶ç›¸æ¯”ï¼ŒMobileStewardå–å¾—äº†æœ€ä½³æ€§èƒ½ï¼Œå‡¸æ˜¾äº†å…¶åœ¨å¤„ç†å…·æœ‰ä¸åŒå¤æ‚æ€§çš„ç”¨æˆ·æŒ‡ä»¤æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.16796v1">PDF</a> Accepted by KDD2025 Research Track</p>
<p><strong>Summary</strong><br>     ç§»åŠ¨æ‰‹æœºä»£ç†èƒ½å¤ŸååŠ©ç”¨æˆ·è‡ªåŠ¨åŒ–å®Œæˆæ—¥å¸¸æ‰‹æœºä»»åŠ¡ï¼Œä½†ç°æœ‰æµç¨‹å¯¼å‘çš„ä»£ç†é¢ä¸´è·¨åº”ç”¨æŒ‡ä»¤çš„æŒ‘æˆ˜ã€‚å—é¢å‘å¯¹è±¡ç¼–ç¨‹åŸåˆ™çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºè‡ªæˆ‘è¿›åŒ–çš„å¤šä»£ç†æ¡†æ¶MobileStewardï¼ŒåŒ…å«åŠ¨æ€æ‹›å‹Ÿã€æŒ‡å®šæ‰§è¡Œå’Œè°ƒæ•´è¯„ä¼°ä¸‰ä¸ªæ¨¡å—ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªåŸºäºå†…å­˜çš„è‡ªæˆ‘è¿›åŒ–æœºåˆ¶æ¥æé«˜æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMobileStewardç›¸è¾ƒäºå•ä»£ç†å’Œå¤šä»£ç†æ¡†æ¶è¡¨ç°å‡ºæœ€ä½³æ€§èƒ½ï¼Œåœ¨å¤„ç†ç”¨æˆ·å¤šæ ·åŒ–å¤æ‚æŒ‡ä»¤æ–¹é¢æ›´å…·ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç§»åŠ¨æ‰‹æœºä»£ç†èƒ½ååŠ©è‡ªåŠ¨åŒ–æ—¥å¸¸ä»»åŠ¡ï¼Œä½†è·¨åº”ç”¨æŒ‡ä»¤æ˜¯ä¸€å¤§æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æµç¨‹å¯¼å‘çš„ä»£ç†é¢ä¸´å¤æ‚ä»»åŠ¡å…³ç³»ã€å¤šæ ·åº”ç”¨ç¯å¢ƒä»¥åŠå¤šæ­¥éª¤æ‰§è¡Œä¸­çš„é”™è¯¯ä¼ æ’­å’Œä¿¡æ¯ä¸¢å¤±ç­‰éš¾é¢˜ã€‚</li>
<li>é¢å‘å¯¹è±¡è§£å†³æ–¹æ¡ˆæ›´é€‚åˆè·¨åº”ç”¨æŒ‡ä»¤ã€‚</li>
<li>MobileStewardæ˜¯ä¸€ä¸ªè‡ªæˆ‘è¿›åŒ–çš„å¤šä»£ç†æ¡†æ¶ï¼ŒåŒ…å«åŠ¨æ€æ‹›å‹Ÿã€æŒ‡å®šæ‰§è¡Œå’Œè°ƒæ•´è¯„ä¼°ä¸‰ä¸ªæ¨¡å—ã€‚</li>
<li>MobileStewardé€šè¿‡ä¿¡æ¯æµåŠ¨æ¥æ˜ç¡®ä»»åŠ¡ä¸åº”ç”¨çš„å…³è”ã€‚</li>
<li>MobileStewardé€šè¿‡åº”ç”¨ç‰¹å®šçš„ä¸“ä¸šçŸ¥è¯†æ¥åº”å¯¹åº”ç”¨é—´çš„å·®å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.16796">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-be4f037dddab7d25828db36f005c57a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a17b66c8f9dd68ba059ad98b9360b8c0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a24afbe0a6e7dd79acee9254204ed40c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e588c0a8811592fa708ca7b7d6aac301.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7fc88ea2deb9aea96e122508c8bc48d3.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Multi-Agent-Stock-Prediction-Systems-Machine-Learning-Models-Simulations-and-Real-Time-Trading-Strategies"><a href="#Multi-Agent-Stock-Prediction-Systems-Machine-Learning-Models-Simulations-and-Real-Time-Trading-Strategies" class="headerlink" title="Multi-Agent Stock Prediction Systems: Machine Learning Models,   Simulations, and Real-Time Trading Strategies"></a>Multi-Agent Stock Prediction Systems: Machine Learning Models,   Simulations, and Real-Time Trading Strategies</h2><p><strong>Authors:Daksh Dave, Gauransh Sawhney, Vikhyat Chauhan</strong></p>
<p>This paper presents a comprehensive study on stock price prediction, leveragingadvanced machine learning (ML) and deep learning (DL) techniques to improve financial forecasting accuracy. The research evaluates the performance of various recurrent neural network (RNN) architectures, including Long Short-Term Memory (LSTM) networks, Gated Recurrent Units (GRU), and attention-based models. These models are assessed for their ability to capture complex temporal dependencies inherent in stock market data. Our findings show that attention-based models outperform other architectures, achieving the highest accuracy by capturing both short and long-term dependencies. This study contributes valuable insights into AI-driven financial forecasting, offering practical guidance for developing more accurate and efficient trading systems. </p>
<blockquote>
<p>æœ¬æ–‡å…¨é¢ç ”ç©¶äº†è‚¡ç¥¨ä»·æ ¼é¢„æµ‹ï¼Œåˆ©ç”¨å…ˆè¿›çš„æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰å’Œæ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰æŠ€æœ¯æé«˜é‡‘èé¢„æµ‹çš„å‡†ç¡®æ€§ã€‚ç ”ç©¶è¯„ä¼°äº†å„ç§å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰æ¶æ„çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬é•¿çŸ­æœŸè®°å¿†ï¼ˆLSTMï¼‰ç½‘ç»œã€é—¨æ§å¾ªç¯å•å…ƒï¼ˆGRUï¼‰å’ŒåŸºäºæ³¨æ„åŠ›çš„æ¨¡å‹ã€‚è¿™äº›æ¨¡å‹è¢«è¯„ä¼°ä¸ºæ•æ‰è‚¡å¸‚æ•°æ®å†…åœ¨å¤æ‚æ—¶é—´ä¾èµ–æ€§çš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒåŸºäºæ³¨æ„åŠ›çš„æ¨¡å‹åœ¨æ•æ‰çŸ­æœŸå’Œé•¿æœŸä¾èµ–æ€§æ–¹é¢è¡¨ç°å‡ºä¼˜äºå…¶ä»–æ¶æ„çš„æ€§èƒ½ï¼Œè·å¾—äº†æœ€é«˜çš„å‡†ç¡®æ€§ã€‚è¿™é¡¹ç ”ç©¶ä¸ºAIé©±åŠ¨çš„é‡‘èé¢„æµ‹æä¾›äº†å®è´µçš„è§è§£ï¼Œå¹¶ä¸ºå¼€å‘æ›´å‡†ç¡®ã€æ›´é«˜æ•ˆçš„äº¤æ˜“ç³»ç»Ÿæä¾›äº†å®é™…æŒ‡å¯¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.15853v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡åˆ©ç”¨å…ˆè¿›çš„æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ æŠ€æœ¯ï¼Œå¯¹è‚¡ç¥¨ä»·æ ¼é¢„æµ‹è¿›è¡Œäº†å…¨é¢ç ”ç©¶ï¼Œæ—¨åœ¨æé«˜é‡‘èé¢„æµ‹çš„å‡†ç¡®åº¦ã€‚ç ”ç©¶è¯„ä¼°äº†ä¸åŒå¾ªç¯ç¥ç»ç½‘ç»œæ¶æ„ï¼ˆåŒ…æ‹¬é•¿çŸ­æ—¶è®°å¿†ç½‘ç»œã€é—¨æ§å¾ªç¯å•å…ƒå’ŒåŸºäºæ³¨æ„åŠ›çš„æ¨¡å‹ï¼‰çš„æ€§èƒ½ï¼Œä»¥æ•æ‰è‚¡å¸‚æ•°æ®å†…åœ¨çš„å¤æ‚æ—¶é—´ä¾èµ–æ€§ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒåŸºäºæ³¨æ„åŠ›çš„æ¨¡å‹è¡¨ç°å‡ºæœ€ä½³æ€§èƒ½ï¼Œèƒ½å¤Ÿæ•æ‰çŸ­æœŸå’Œé•¿æœŸä¾èµ–å…³ç³»ï¼Œä¸ºäººå·¥æ™ºèƒ½é©±åŠ¨çš„é‡‘èé¢„æµ‹æä¾›äº†å®è´µçš„è§è§£ï¼Œå¹¶ä¸ºå¼€å‘æ›´å‡†ç¡®ã€æ›´é«˜æ•ˆçš„äº¤æ˜“ç³»ç»Ÿæä¾›äº†å®é™…æŒ‡å¯¼ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æœ¬æ–‡åˆ©ç”¨æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ æŠ€æœ¯è¿›è¡Œè‚¡ç¥¨ä»·æ ¼é¢„æµ‹ç ”ç©¶ã€‚</li>
<li>ç ”ç©¶è¯„ä¼°äº†å¤šç§å¾ªç¯ç¥ç»ç½‘ç»œæ¶æ„çš„æ€§èƒ½ã€‚</li>
<li>åŸºäºæ³¨æ„åŠ›çš„æ¨¡å‹åœ¨æ•æ‰è‚¡å¸‚æ•°æ®çš„æ—¶é—´ä¾èµ–æ€§æ–¹é¢è¡¨ç°å‡ºæœ€ä½³æ€§èƒ½ã€‚</li>
<li>æ³¨æ„åŠ›æ¨¡å‹èƒ½å¤ŸåŒæ—¶æ•æ‰çŸ­æœŸå’Œé•¿æœŸä¾èµ–å…³ç³»ã€‚</li>
<li>ç ”ç©¶ç»“æœæé«˜äº†é‡‘èé¢„æµ‹çš„å‡†ç¡®åº¦ã€‚</li>
<li>æœ¬æ–‡ä¸ºäººå·¥æ™ºèƒ½é©±åŠ¨çš„é‡‘èé¢„æµ‹æä¾›äº†å®è´µè§è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.15853">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cd308be01c838f9fa45a51fef0971483.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-60307e7c86e546e1e55da71c969133f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-144c953bdd09867c5b231a34357cd5d1.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Leveraging-Dual-Process-Theory-in-Language-Agent-Framework-for-Real-time-Simultaneous-Human-AI-Collaboration"><a href="#Leveraging-Dual-Process-Theory-in-Language-Agent-Framework-for-Real-time-Simultaneous-Human-AI-Collaboration" class="headerlink" title="Leveraging Dual Process Theory in Language Agent Framework for Real-time   Simultaneous Human-AI Collaboration"></a>Leveraging Dual Process Theory in Language Agent Framework for Real-time   Simultaneous Human-AI Collaboration</h2><p><strong>Authors:Shao Zhang, Xihuai Wang, Wenhao Zhang, Chaoran Li, Junru Song, Tingyu Li, Lin Qiu, Xuezhi Cao, Xunliang Cai, Wen Yao, Weinan Zhang, Xinbing Wang, Ying Wen</strong></p>
<p>Agents built on large language models (LLMs) have excelled in turn-by-turn human-AI collaboration but struggle with simultaneous tasks requiring real-time interaction. Latency issues and the challenge of inferring variable human strategies hinder their ability to make autonomous decisions without explicit instructions. Through experiments with current independent System 1 and System 2 methods, we validate the necessity of using Dual Process Theory (DPT) in real-time tasks. We propose DPT-Agent, a novel language agent framework that integrates System 1 and System 2 for efficient real-time simultaneous human-AI collaboration. DPT-Agentâ€™s System 1 uses a Finite-state Machine (FSM) and code-as-policy for fast, intuitive, and controllable decision-making. DPT-Agentâ€™s System 2 integrates Theory of Mind (ToM) and asynchronous reflection to infer human intentions and perform reasoning-based autonomous decisions. We demonstrate the effectiveness of DPT-Agent through further experiments with rule-based agents and human collaborators, showing significant improvements over mainstream LLM-based frameworks. To the best of our knowledge, DPT-Agent is the first language agent framework that achieves successful real-time simultaneous human-AI collaboration autonomously. Code of DPT-Agent can be found in <a target="_blank" rel="noopener" href="https://github.com/sjtu-marl/DPT-Agent">https://github.com/sjtu-marl/DPT-Agent</a>. </p>
<blockquote>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç†äººåœ¨é€è½®çš„äººæœºåä½œä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨éœ€è¦å®æ—¶äº’åŠ¨çš„åŒæ—¶å¤šä»»åŠ¡å¤„ç†æ–¹é¢å­˜åœ¨å›°éš¾ã€‚å»¶è¿Ÿé—®é¢˜å’Œæ¨æ–­å¯å˜äººç±»ç­–ç•¥çš„æŒ‘æˆ˜é˜»ç¢äº†å®ƒä»¬åœ¨æ²¡æœ‰æ˜ç¡®æŒ‡ä»¤çš„æƒ…å†µä¸‹åšå‡ºè‡ªä¸»å†³ç­–çš„èƒ½åŠ›ã€‚æˆ‘ä»¬é€šè¿‡å½“å‰ç‹¬ç«‹çš„System 1å’ŒSystem 2æ–¹æ³•çš„å®éªŒï¼ŒéªŒè¯äº†åœ¨å®æ—¶ä»»åŠ¡ä¸­ä½¿ç”¨åŒè¿‡ç¨‹ç†è®ºï¼ˆDPTï¼‰çš„å¿…è¦æ€§ã€‚æˆ‘ä»¬æå‡ºäº†DPT-Agentï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„è¯­è¨€ä»£ç†æ¡†æ¶ï¼Œå®ƒæ•´åˆäº†System 1å’ŒSystem 2ï¼Œç”¨äºé«˜æ•ˆå®æ—¶çš„åŒæ­¥äººæœºåä½œã€‚DPT-Agentçš„System 1ä½¿ç”¨æœ‰é™çŠ¶æ€æœºï¼ˆFSMï¼‰å’Œä»£ç å³ç­–ç•¥ï¼Œä»¥å®ç°å¿«é€Ÿã€ç›´è§‚å’Œå¯æ§çš„å†³ç­–ã€‚DPT-Agentçš„System 2ç»“åˆäº†å¿ƒæ™ºç†è®ºï¼ˆToMï¼‰å’Œå¼‚æ­¥åå°„ï¼Œä»¥æ¨æ–­äººç±»æ„å›¾å¹¶åŸºäºæ¨ç†è¿›è¡Œè‡ªä¸»å†³ç­–ã€‚æˆ‘ä»¬é€šè¿‡ä¸åŸºäºè§„åˆ™çš„ä»£ç†äººå’Œäººç±»åˆä½œè€…è¿›ä¸€æ­¥å®éªŒï¼Œè¯æ˜äº†DPT-Agentçš„æœ‰æ•ˆæ€§ï¼Œæ˜¾ç¤ºå‡ºåœ¨ä¸»æµLLMæ¡†æ¶ä¸Šçš„æ˜¾è‘—æ”¹è¿›ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒDPT-Agentæ˜¯ç¬¬ä¸€ä¸ªæˆåŠŸå®ç°å®æ—¶åŒæ­¥äººæœºåä½œè‡ªä¸»æ€§çš„è¯­è¨€ä»£ç†æ¡†æ¶ã€‚DPT-Agentçš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/sjtu-marl/DPT-Agent">https://github.com/sjtu-marl/DPT-Agent</a>ä¸­æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.11882v2">PDF</a> Preprint under review. Update the experimental results of the   DeepSeek-R1 series models</p>
<p><strong>Summary</strong>ï¼šåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„ä»£ç†åœ¨é€æ­¥äººæœºåä½œä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨éœ€è¦å®æ—¶äº¤äº’çš„åŒæ—¶ä»»åŠ¡æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚å®éªŒéªŒè¯äº†åŒè¿‡ç¨‹ç†è®ºåœ¨å®æ—¶ä»»åŠ¡ä¸­çš„å¿…è¦æ€§ã€‚æå‡ºDPT-Agentï¼Œä¸€ä¸ªç»“åˆç³»ç»Ÿ1å’Œç³»ç»Ÿ2çš„æ–°å‹è¯­è¨€ä»£ç†æ¡†æ¶ï¼Œç”¨äºé«˜æ•ˆå®æ—¶çš„äººæœºåä½œã€‚DPT-Agentçš„ç³»ç»Ÿ1é‡‡ç”¨æœ‰é™çŠ¶æ€æœºè¿›è¡Œå¿«é€Ÿã€ç›´è§‚å’Œå¯æ§çš„å†³ç­–ã€‚ç³»ç»Ÿ2ç»“åˆäº†å¿ƒæ™ºç†è®ºè¿›è¡Œæ¨ç†å¹¶è‡ªä¸»å†³ç­–ã€‚é€šè¿‡è¿›ä¸€æ­¥çš„å®éªŒè¯æ˜äº†DPT-Agentçš„æœ‰æ•ˆæ€§ã€‚è¿™æ˜¯é¦–æ¬¡å®ç°è‡ªä¸»å®æ—¶äººæœºåä½œçš„è¯­è¨€ä»£ç†æ¡†æ¶ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†åœ¨é€æ­¥äººæœºåä½œä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨éœ€è¦å®æ—¶äº¤äº’çš„åŒæ—¶ä»»åŠ¡æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>åŒè¿‡ç¨‹ç†è®ºåœ¨å®æ—¶ä»»åŠ¡ä¸­çš„å¿…è¦æ€§å¾—åˆ°éªŒè¯ã€‚</li>
<li>DPT-Agentæ˜¯ä¸€ä¸ªæ–°å‹è¯­è¨€ä»£ç†æ¡†æ¶ï¼Œç»“åˆäº†ç³»ç»Ÿ1å’Œç³»ç»Ÿ2ï¼Œç”¨äºé«˜æ•ˆå®æ—¶çš„äººæœºåä½œã€‚</li>
<li>DPT-Agentçš„ç³»ç»Ÿ1é‡‡ç”¨æœ‰é™çŠ¶æ€æœºè¿›è¡Œå†³ç­–ï¼Œå…·æœ‰å¿«é€Ÿã€ç›´è§‚å’Œå¯æ§çš„ç‰¹ç‚¹ã€‚</li>
<li>DPT-Agentçš„ç³»ç»Ÿ2ç»“åˆäº†å¿ƒæ™ºç†è®ºè¿›è¡Œæ¨ç†ï¼Œå¹¶èƒ½è‡ªä¸»å†³ç­–ã€‚</li>
<li>ä¸è§„åˆ™ä»£ç†å’Œäººå·¥åˆä½œè€…è¿›è¡Œçš„å®éªŒè¯æ˜äº†DPT-Agentçš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.11882">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0f7850b7980da4d87a57d9d78568be0a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c5f68db2c393aa24f82d56b3170b910d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3058b4314de5369cdae281c8b2fdf8b6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8e5196adfadaf8e24f594685ec4efeea.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="AgentRefine-Enhancing-Agent-Generalization-through-Refinement-Tuning"><a href="#AgentRefine-Enhancing-Agent-Generalization-through-Refinement-Tuning" class="headerlink" title="AgentRefine: Enhancing Agent Generalization through Refinement Tuning"></a>AgentRefine: Enhancing Agent Generalization through Refinement Tuning</h2><p><strong>Authors:Dayuan Fu, Keqing He, Yejie Wang, Wentao Hong, Zhuoma Gongque, Weihao Zeng, Wei Wang, Jingang Wang, Xunliang Cai, Weiran Xu</strong></p>
<p>Large Language Model (LLM) based agents have proved their ability to perform complex tasks like humans. However, there is still a large gap between open-sourced LLMs and commercial models like the GPT series. In this paper, we focus on improving the agent generalization capabilities of LLMs via instruction tuning. We first observe that the existing agent training corpus exhibits satisfactory results on held-in evaluation sets but fails to generalize to held-out sets. These agent-tuning works face severe formatting errors and are frequently stuck in the same mistake for a long while. We analyze that the poor generalization ability comes from overfitting to several manual agent environments and a lack of adaptation to new situations. They struggle with the wrong action steps and can not learn from the experience but just memorize existing observation-action relations. Inspired by the insight, we propose a novel AgentRefine framework for agent-tuning. The core idea is to enable the model to learn to correct its mistakes via observation in the trajectory. Specifically, we propose an agent synthesis framework to encompass a diverse array of environments and tasks and prompt a strong LLM to refine its error action according to the environment feedback. AgentRefine significantly outperforms state-of-the-art agent-tuning work in terms of generalization ability on diverse agent tasks. It also has better robustness facing perturbation and can generate diversified thought in inference. Our findings establish the correlation between agent generalization and self-refinement and provide a new paradigm for future research. </p>
<blockquote>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç†å·²ç»è¯æ˜äº†å®ƒä»¬æ‰§è¡Œå¤æ‚ä»»åŠ¡çš„èƒ½åŠ›ï¼Œç±»ä¼¼äºäººç±»ã€‚ç„¶è€Œï¼Œå¼€æºLLMå’Œå•†ä¸šæ¨¡å‹ï¼ˆå¦‚GPTç³»åˆ—ï¼‰ä¹‹é—´ä»å­˜åœ¨å¾ˆå¤§å·®è·ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä¸“æ³¨äºé€šè¿‡æŒ‡ä»¤å¾®è°ƒæé«˜LLMçš„ä»£ç†æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬é¦–å…ˆè§‚å¯Ÿåˆ°ï¼Œç°æœ‰çš„ä»£ç†è®­ç»ƒè¯­æ–™åº“åœ¨å†…éƒ¨è¯„ä¼°é›†ä¸Šå–å¾—äº†ä»¤äººæ»¡æ„çš„ç»“æœï¼Œä½†æ— æ³•æ¨å¹¿åˆ°å¤–éƒ¨é›†ã€‚è¿™äº›ä»£ç†è°ƒæ•´å·¥ä½œé¢ä¸´ç€ä¸¥é‡çš„æ ¼å¼é”™è¯¯ï¼Œå¹¶ä¸”ç»å¸¸é•¿æ—¶é—´é™·å…¥åŒæ ·çš„é”™è¯¯ã€‚æˆ‘ä»¬åˆ†æè®¤ä¸ºï¼Œæ³›åŒ–èƒ½åŠ›å·®çš„æ ¹æºåœ¨äºå¯¹å‡ ç§æ‰‹åŠ¨ä»£ç†ç¯å¢ƒçš„è¿‡åº¦æ‹Ÿåˆä»¥åŠå¯¹æ–°æƒ…å†µçš„é€‚åº”ä¸è¶³ã€‚ä»–ä»¬é™·å…¥äº†é”™è¯¯çš„è¡ŒåŠ¨æ­¥éª¤ï¼Œæ— æ³•ä»ç»éªŒä¸­å­¦ä¹ ï¼Œè€Œåªæ˜¯è®°å¿†ç°æœ‰çš„è§‚å¯Ÿ-è¡ŒåŠ¨å…³ç³»ã€‚å—æ­¤å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„AgentRefineæ¡†æ¶ç”¨äºä»£ç†è°ƒæ•´ã€‚æ ¸å¿ƒæ€æƒ³æ˜¯ä½¿æ¨¡å‹èƒ½å¤Ÿé€šè¿‡è½¨è¿¹ä¸­çš„è§‚å¯Ÿå­¦ä¹ çº æ­£è‡ªå·±çš„é”™è¯¯ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä»£ç†åˆæˆæ¡†æ¶ï¼Œä»¥æ¶µç›–å„ç§ç¯å¢ƒå’Œä»»åŠ¡ï¼Œå¹¶æç¤ºå¼ºå¤§çš„LLMæ ¹æ®ç¯å¢ƒåé¦ˆç»†åŒ–å…¶é”™è¯¯è¡ŒåŠ¨ã€‚AgentRefineåœ¨å¤šç§ä»£ç†ä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›æ–¹é¢æ˜¾è‘—ä¼˜äºæœ€æ–°çš„ä»£ç†è°ƒæ•´å·¥ä½œã€‚å®ƒè¿˜å…·æœ‰æ›´å¥½çš„æŠ—æ‰°åŠ¨æ€§ï¼Œå¹¶åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¯ä»¥äº§ç”Ÿå¤šæ ·åŒ–çš„æƒ³æ³•ã€‚æˆ‘ä»¬çš„å‘ç°å»ºç«‹äº†ä»£ç†æ³›åŒ–å’Œè‡ªæˆ‘å®Œå–„ä¹‹é—´çš„å…³è”ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›äº†æ–°çš„èŒƒå¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01702v2">PDF</a> ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç†äººåœ¨å¤æ‚ä»»åŠ¡æ‰§è¡Œæ–¹é¢è¡¨ç°å‡ºäº†ä¸äººç±»ç›¸ä¼¼çš„å®åŠ›ã€‚ç„¶è€Œï¼Œå¼€æºLLMä¸å•†ä¸šæ¨¡å‹å¦‚GPTç³»åˆ—ä¹‹é—´ä»å­˜åœ¨è¾ƒå¤§å·®è·ã€‚æœ¬æ–‡å…³æ³¨é€šè¿‡æŒ‡ä»¤å¾®è°ƒæå‡LLMçš„ä»£ç†æ³›åŒ–èƒ½åŠ›ã€‚ç°æœ‰ä»£ç†è®­ç»ƒè¯­æ–™åº“åœ¨å°é—­è¯„ä¼°é›†ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å¼€æ”¾æµ‹è¯•é›†ä¸Šæ³›åŒ–èƒ½åŠ›è¾ƒå·®ã€‚å—æ­¤å¯å‘ï¼Œæœ¬æ–‡æå‡ºäº†æ–°å‹çš„AgentRefineæ¡†æ¶ç”¨äºä»£ç†å¾®è°ƒã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯ä½¿æ¨¡å‹èƒ½å¤Ÿé€šè¿‡è§‚å¯Ÿè½¨è¿¹å­¦ä¹ çº æ­£é”™è¯¯ã€‚AgentRefineæ˜¾è‘—æå‡äº†ç°æœ‰ä»£ç†è°ƒæ•´å·¥ä½œåœ¨å¤šæ ·åŒ–ä»£ç†ä»»åŠ¡ä¸Šçš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶å…·å¤‡æ›´å¥½çš„é²æ£’æ€§å’Œæ¨ç†æ—¶çš„å¤šæ ·åŒ–æ€ç»´ç”Ÿæˆèƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨å¤æ‚ä»»åŠ¡æ‰§è¡Œæ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†ä¸å•†ä¸šæ¨¡å‹å¦‚GPTç³»åˆ—ç›¸æ¯”ä»å­˜åœ¨ä¸€å®šå·®è·ã€‚</li>
<li>ç°æœ‰LLMä»£ç†è®­ç»ƒæ¨¡å‹åœ¨å°é—­è¯„ä¼°é›†ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å¼€æ”¾æµ‹è¯•é›†ä¸Šæ³›åŒ–èƒ½åŠ›è¾ƒå·®ã€‚</li>
<li>ä»£ç†è®­ç»ƒä¸­çš„æ ¼å¼åŒ–é”™è¯¯å’Œé•¿æœŸé‡å¤é”™è¯¯é—®é¢˜äºŸå¾…è§£å†³ã€‚</li>
<li>é”™è¯¯çš„æ³›åŒ–èƒ½åŠ›æºäºå¯¹æ‰‹åŠ¨ä»£ç†ç¯å¢ƒçš„è¿‡åº¦é€‚åº”å’Œå¯¹æ–°æƒ…å†µçš„ç¼ºä¹é€‚åº”æ€§ã€‚</li>
<li>LLMéš¾ä»¥ä»ç»éªŒä¸­å­¦ä¹ ï¼Œåªèƒ½è®°å¿†ç°æœ‰çš„è§‚å¯Ÿ-è¡ŒåŠ¨å…³ç³»ã€‚</li>
<li>AgentRefineæ¡†æ¶é€šè¿‡ä½¿æ¨¡å‹å­¦ä¹ æ ¹æ®ç¯å¢ƒåé¦ˆçº æ­£é”™è¯¯ï¼Œæ˜¾è‘—æé«˜äº†ä»£ç†è°ƒæ•´å·¥ä½œçš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>AgentRefineåœ¨å¤šæ ·åŒ–ä»£ç†ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå…·å¤‡æ›´å¥½çš„é²æ£’æ€§å’Œæ¨ç†å¤šæ ·åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01702">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2255ed0a5d7d3c56a172d243ce39bcc5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e790c1bbb63d57fa516cb7978a1323bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-435710b79c861a4ca2ec450f7d8973a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4de5db3d55f928025855b461beb43b81.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-750d49d44f60a9a8b4ff9f1ed0375f83.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-776bf9f9537ebf89bd5d7871d8d30a1d.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="TradingAgents-Multi-Agents-LLM-Financial-Trading-Framework"><a href="#TradingAgents-Multi-Agents-LLM-Financial-Trading-Framework" class="headerlink" title="TradingAgents: Multi-Agents LLM Financial Trading Framework"></a>TradingAgents: Multi-Agents LLM Financial Trading Framework</h2><p><strong>Authors:Yijia Xiao, Edward Sun, Di Luo, Wei Wang</strong></p>
<p>Significant progress has been made in automated problem-solving using societies of agents powered by large language models (LLMs). In finance, efforts have largely focused on single-agent systems handling specific tasks or multi-agent frameworks independently gathering data. However, multi-agent systemsâ€™ potential to replicate real-world trading firmsâ€™ collaborative dynamics remains underexplored. TradingAgents proposes a novel stock trading framework inspired by trading firms, featuring LLM-powered agents in specialized roles such as fundamental analysts, sentiment analysts, technical analysts, and traders with varied risk profiles. The framework includes Bull and Bear researcher agents assessing market conditions, a risk management team monitoring exposure, and traders synthesizing insights from debates and historical data to make informed decisions. By simulating a dynamic, collaborative trading environment, this framework aims to improve trading performance. Detailed architecture and extensive experiments reveal its superiority over baseline models, with notable improvements in cumulative returns, Sharpe ratio, and maximum drawdown, highlighting the potential of multi-agent LLM frameworks in financial trading. TradingAgents is available at <a target="_blank" rel="noopener" href="https://github.com/TradingAgents-AI">https://github.com/TradingAgents-AI</a>. </p>
<blockquote>
<p>åœ¨åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„æ™ºèƒ½ä»£ç†ç¤¾ä¼šè§£å†³è‡ªåŠ¨åŒ–é—®é¢˜æ–¹é¢ï¼Œå·²ç»å–å¾—äº†é‡å¤§è¿›å±•ã€‚åœ¨é‡‘èé¢†åŸŸï¼Œç›¸å…³åŠªåŠ›ä¸»è¦é›†ä¸­åœ¨å¤„ç†ç‰¹å®šä»»åŠ¡çš„å•ä¸€ä»£ç†ç³»ç»Ÿï¼Œæˆ–ç‹¬ç«‹æ”¶é›†æ•°æ®çš„å¤šä»£ç†æ¡†æ¶ã€‚ç„¶è€Œï¼Œå¤šä»£ç†ç³»ç»Ÿå¤åˆ¶ç°å®ä¸–ç•Œäº¤æ˜“å…¬å¸åä½œåŠ¨æ€çš„æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ã€ŠTradingAgentsã€‹æå‡ºäº†ä¸€ä¸ªå—äº¤æ˜“å…¬å¸å¯å‘çš„æ–°å‹è‚¡ç¥¨äº¤æ˜“æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨LLMé©±åŠ¨çš„ä»£ç†æ‹…ä»»ä¸“é—¨è§’è‰²ï¼Œå¦‚åŸºæœ¬é¢åˆ†æå¸ˆã€æƒ…ç»ªåˆ†æå¸ˆã€æŠ€æœ¯åˆ†æå¸ˆå’Œä¸åŒé£é™©çŠ¶å†µçš„äº¤æ˜“å‘˜ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬çœ‹æ¶¨å’Œçœ‹è·Œç ”ç©¶å‘˜ä»£ç†è¯„ä¼°å¸‚åœºçŠ¶å†µï¼Œé£é™©ç®¡ç†å›¢é˜Ÿç›‘æ§æ›å…‰åº¦ï¼Œä»¥åŠäº¤æ˜“å‘˜ç»¼åˆè¾©è®ºå’Œå†å²æ•°æ®ä¸­çš„è§è§£ä»¥åšå‡ºæ˜æ™ºçš„å†³ç­–ã€‚é€šè¿‡æ¨¡æ‹ŸåŠ¨æ€åä½œçš„è´¸æ˜“ç¯å¢ƒï¼Œè¯¥æ¡†æ¶æ—¨åœ¨æé«˜äº¤æ˜“æ€§èƒ½ã€‚è¯¦ç»†çš„æ¶æ„å’Œå¹¿æ³›çš„å®éªŒæ˜¾ç¤ºå…¶åœ¨ç´¯ç§¯å›æŠ¥ã€å¤æ™®æ¯”ç‡å’Œæœ€å¤§å›æ’¤ç­‰æ–¹é¢ä¼˜äºåŸºå‡†æ¨¡å‹ï¼Œçªæ˜¾äº†å¤šä»£ç†LLMæ¡†æ¶åœ¨é‡‘èäº¤æ˜“ä¸­çš„æ½œåŠ›ã€‚ã€ŠTradingAgentsã€‹å¯åœ¨ <a target="_blank" rel="noopener" href="https://github.com/TradingAgents-AI">https://github.com/TradingAgents-AI</a> è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20138v4">PDF</a> Multi-Agent AI in the Real World @ AAAI 2025</p>
<p><strong>æ€»ç»“</strong></p>
<p>åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹é©±åŠ¨çš„ç¤¾ä¼šåŒ–ä»£ç†ï¼ˆLLMï¼‰åœ¨è‡ªåŠ¨åŒ–é—®é¢˜è§£å†³æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚é‡‘èé¢†åŸŸè™½å·²æœ‰å•ä¸€ä»£ç†ç³»ç»Ÿå¤„ç†ç‰¹å®šä»»åŠ¡æˆ–å¤šä»£ç†æ¡†æ¶ç‹¬ç«‹æ”¶é›†æ•°æ®çš„ç ”ç©¶ï¼Œä½†å¤šä»£ç†ç³»ç»Ÿå¤åˆ¶çœŸå®äº¤æ˜“å…¬å¸åä½œåŠ¨æ€çš„ç ”ç©¶ä»è¢«å¿½è§†ã€‚TradingAgentsæå‡ºä¸€ä¸ªå—äº¤æ˜“å…¬å¸å¯å‘çš„æ–°å‹è‚¡ç¥¨äº¤æ˜“æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨LLMé©±åŠ¨çš„å¤šä»£ç†ï¼Œåˆ†ä¸ºå¸‚åœºåˆ†æå‘˜ã€æƒ…ç»ªåˆ†æå‘˜ã€æŠ€æœ¯åˆ†æå‘˜åŠé£é™©ä¸åŒçš„äº¤æ˜“å‘˜ç­‰ä¸åŒè§’è‰²ã€‚æ¡†æ¶è¿˜åŒ…æ‹¬Bullå’ŒBearç ”ç©¶ä»£ç†è¯„ä¼°å¸‚åœºçŠ¶å†µï¼Œé£é™©ç®¡ç†å›¢é˜Ÿç›‘æ§æ›å…‰åº¦ï¼Œäº¤æ˜“å‘˜ç»“åˆè¾©è®ºå’Œå†å²æ•°æ®æ¥åšå‡ºæ˜æ™ºå†³ç­–ã€‚é€šè¿‡æ¨¡æ‹ŸåŠ¨æ€åä½œäº¤æ˜“ç¯å¢ƒï¼Œè¯¥æ¡†æ¶æ—¨åœ¨æé«˜äº¤æ˜“æ€§èƒ½ã€‚è¯¦ç»†çš„æ¶æ„å’Œå¹¿æ³›çš„å®éªŒè¡¨æ˜å…¶ä¼˜äºåŸºå‡†æ¨¡å‹ï¼Œåœ¨ç´¯è®¡å›æŠ¥ã€å¤æ™®æ¯”ç‡å’Œæœ€å¤§å›æ’¤æ–¹é¢éƒ½æœ‰æ˜¾è‘—æ”¹å–„ï¼Œçªæ˜¾äº†å¤šä»£ç†LLMæ¡†æ¶åœ¨é‡‘èäº¤æ˜“ä¸­çš„æ½œåŠ›ã€‚TradingAgentsé¡¹ç›®å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/TradingAgents-AI%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/TradingAgents-AIè®¿é—®ã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ul>
<li>LLMé©±åŠ¨çš„ä»£ç†åœ¨è‡ªåŠ¨åŒ–é—®é¢˜è§£å†³æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ã€‚</li>
<li>é‡‘èé¢†åŸŸå·²æœ‰å•ä¸€ä»£ç†ç³»ç»Ÿå’Œå¤šä»£ç†æ¡†æ¶çš„ç ”ç©¶ï¼Œä½†å¤šä»£ç†ç³»ç»Ÿçš„åä½œåŠ¨æ€ä»è¢«å¿½è§†ã€‚</li>
<li>TradingAgentsæå‡ºå—çœŸå®äº¤æ˜“å…¬å¸å¯å‘çš„è‚¡ç¥¨äº¤æ˜“æ¡†æ¶ï¼ŒåŒ…å«å¤šç§LLMé©±åŠ¨çš„å¤šä»£ç†è§’è‰²ã€‚</li>
<li>æ¡†æ¶åŒ…å«å¸‚åœºåˆ†æã€æƒ…ç»ªåˆ†æã€æŠ€æœ¯åˆ†æå’Œé£é™©ç®¡ç†ç­‰å…³é”®ç»„ä»¶ã€‚</li>
<li>é€šè¿‡æ¨¡æ‹ŸåŠ¨æ€åä½œç¯å¢ƒï¼Œè¯¥æ¡†æ¶æ—¨åœ¨æé«˜äº¤æ˜“æ€§èƒ½ã€‚</li>
<li>è¯¦ç»†å®éªŒè¡¨æ˜å…¶ä¼˜äºåŸºå‡†æ¨¡å‹ï¼Œåœ¨ç´¯è®¡å›æŠ¥ã€å¤æ™®æ¯”ç‡å’Œæœ€å¤§å›æ’¤æ–¹é¢æœ‰æ‰€æ”¹å–„ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20138">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ed32b163098d73aaef571ed9618f1c15.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9899c981abc8be1ebf44346399f3f2dd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-34cc5bf9c6b593ef2c17231b3b472346.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92c873ecba85e2d8289a18d1e8f25d0d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eba4b8a51ddba76a7d9ed986fefccd57.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="REGENT-A-Retrieval-Augmented-Generalist-Agent-That-Can-Act-In-Context-in-New-Environments"><a href="#REGENT-A-Retrieval-Augmented-Generalist-Agent-That-Can-Act-In-Context-in-New-Environments" class="headerlink" title="REGENT: A Retrieval-Augmented Generalist Agent That Can Act In-Context   in New Environments"></a>REGENT: A Retrieval-Augmented Generalist Agent That Can Act In-Context   in New Environments</h2><p><strong>Authors:Kaustubh Sridhar, Souradeep Dutta, Dinesh Jayaraman, Insup Lee</strong></p>
<p>Building generalist agents that can rapidly adapt to new environments is a key challenge for deploying AI in the digital and real worlds. Is scaling current agent architectures the most effective way to build generalist agents? We propose a novel approach to pre-train relatively small policies on relatively small datasets and adapt them to unseen environments via in-context learning, without any finetuning. Our key idea is that retrieval offers a powerful bias for fast adaptation. Indeed, we demonstrate that even a simple retrieval-based 1-nearest neighbor agent offers a surprisingly strong baseline for todayâ€™s state-of-the-art generalist agents. From this starting point, we construct a semi-parametric agent, REGENT, that trains a transformer-based policy on sequences of queries and retrieved neighbors. REGENT can generalize to unseen robotics and game-playing environments via retrieval augmentation and in-context learning, achieving this with up to 3x fewer parameters and up to an order-of-magnitude fewer pre-training datapoints, significantly outperforming todayâ€™s state-of-the-art generalist agents. Website: <a target="_blank" rel="noopener" href="https://kaustubhsridhar.github.io/regent-research">https://kaustubhsridhar.github.io/regent-research</a> </p>
<blockquote>
<p>æ„å»ºèƒ½å¤Ÿè¿…é€Ÿé€‚åº”æ–°ç¯å¢ƒçš„é€šç”¨æ™ºèƒ½ä½“æ˜¯äººå·¥æ™ºèƒ½åœ¨æ•°å­—å’ŒçœŸå®ä¸–ç•Œéƒ¨ç½²ä¸­çš„ä¸€é¡¹å…³é”®æŒ‘æˆ˜ã€‚ä½¿ç”¨ç°æœ‰çš„æ™ºèƒ½ä½“æ¶æ„è¿›è¡Œæ‰©å±•æ˜¯æ„å»ºé€šç”¨æ™ºèƒ½ä½“çš„æœ€æœ‰æ•ˆæ–¹å¼å—ï¼Ÿæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œåœ¨ç›¸å¯¹è¾ƒå°çš„æ•°æ®é›†ä¸Šé¢„å…ˆè®­ç»ƒè¾ƒå°çš„ç­–ç•¥ï¼Œå¹¶é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ å°†å…¶é€‚åº”æœªè§è¿‡çš„ç¯å¢ƒï¼Œè€Œæ— éœ€å¾®è°ƒã€‚æˆ‘ä»¬çš„æ ¸å¿ƒæ€æƒ³æ˜¯æ£€ç´¢ä¸ºå¿«é€Ÿé€‚åº”æä¾›äº†å¼ºå¤§çš„åè§ã€‚å®é™…ä¸Šï¼Œæˆ‘ä»¬è¯æ˜ï¼Œå³ä½¿æ˜¯ä¸€ä¸ªç®€å•çš„åŸºäºæ£€ç´¢çš„1-æœ€è¿‘é‚»æ™ºèƒ½ä½“ä¹Ÿä¸ºå½“ä»Šæœ€å…ˆè¿›çš„é€šç”¨æ™ºèƒ½ä½“æä¾›äº†å‡ºäººæ„æ–™çš„å¼ºå¤§åŸºçº¿ã€‚ä»è¿™ä¸ªèµ·ç‚¹å‡ºå‘ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŠå‚æ•°æ™ºèƒ½ä½“REGENTï¼Œå®ƒåœ¨æŸ¥è¯¢åºåˆ—å’Œæ£€ç´¢åˆ°çš„é‚»å±…ä¸Šè®­ç»ƒåŸºäºå˜å‹å™¨çš„ç­–ç•¥ã€‚REGENTå¯ä»¥é€šè¿‡æ£€ç´¢å¢å¼ºå’Œä¸Šä¸‹æ–‡å­¦ä¹ æ¨å¹¿åˆ°æœªè§è¿‡çš„æœºå™¨äººå’Œæ¸¸æˆç¯å¢ƒï¼Œä½¿ç”¨è¾ƒå°‘çš„å‚æ•°ï¼ˆæœ€å¤šä¸‰å€ï¼‰å’Œè¾ƒå°‘çš„é¢„è®­ç»ƒæ•°æ®ç‚¹ï¼ˆæœ€å¤šä¸€ä¸ªæ•°é‡çº§ï¼‰ï¼Œæ˜¾è‘—ä¼˜äºå½“å‰çš„å…ˆè¿›é€šç”¨æ™ºèƒ½ä½“ã€‚ç½‘ç«™ï¼š<a target="_blank" rel="noopener" href="https://kaustubhsridhar.github.io/regent-research">https://kaustubhsridhar.github.io/regent-research</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04759v2">PDF</a> ICLR 2025 Oral, NeurIPS 2024 Workshops on Adaptive Foundation Models   (AFM) and Open World Agents (OWA), 30 pages</p>
<p><strong>Summary</strong><br>æœ¬æ–‡æå‡ºäº†åˆ©ç”¨å°å‹æ”¿ç­–ç½‘ç»œåœ¨å°è§„æ¨¡æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶é€šè¿‡æ£€ç´¢å¢å¼ºå’Œä¸Šä¸‹æ–‡å­¦ä¹ å¿«é€Ÿé€‚åº”æ–°ç¯å¢ƒçš„æ–¹æ³•ã€‚æ–‡ç« å±•ç¤ºäº†åŸºäºæ£€ç´¢çš„æœ€è¿‘é‚»ä»£ç†çš„å¼ºå¤§æ€§èƒ½ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šæ„å»ºäº†ä¸€ä¸ªåŠå‚æ•°åŒ–çš„é€šç”¨ä»£ç†REGENTï¼Œèƒ½å¤Ÿå®ç°å¯¹æœªè§è¿‡çš„æœºå™¨äººå’Œæ¸¸æˆç¯å¢ƒçš„æ³›åŒ–ã€‚ç›¸è¾ƒäºç°æœ‰æŠ€æœ¯ï¼Œè¯¥æ–¹æ³•å…·æœ‰æ›´å°‘å‚æ•°å’Œæ›´å°‘é¢„è®­ç»ƒæ•°æ®ç‚¹çš„ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æå‡ºåˆ©ç”¨å°å‹æ”¿ç­–ç½‘ç»œè¿›è¡Œé¢„è®­ç»ƒå¹¶é€‚åº”æ–°ç¯å¢ƒçš„æ–¹æ³•ã€‚</li>
<li>å±•ç¤ºåŸºäºæ£€ç´¢çš„æœ€è¿‘é‚»ä»£ç†çš„å¼ºå¤§æ€§èƒ½ã€‚</li>
<li>æ„å»ºåŠå‚æ•°åŒ–ä»£ç†REGENTï¼Œé€šè¿‡æ£€ç´¢å¢å¼ºå’Œä¸Šä¸‹æ–‡å­¦ä¹ å®ç°æ³›åŒ–ã€‚</li>
<li>REGENTåœ¨æœºå™¨äººå’Œæ¸¸æˆç¯å¢ƒä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>REGENTæ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯çš„ä¸€èˆ¬ä»£ç†ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.04759">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4e962cb65e30b43066e790af6db94399.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a8e5f274fe3953923dbcd71cd83789d1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e3c814b02969a789e57d6f6af5cd350f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9394196fa8c20e29ea4a84056da71d36.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Comparative-Analysis-of-Multi-Agent-Reinforcement-Learning-Policies-for-Crop-Planning-Decision-Support"><a href="#Comparative-Analysis-of-Multi-Agent-Reinforcement-Learning-Policies-for-Crop-Planning-Decision-Support" class="headerlink" title="Comparative Analysis of Multi-Agent Reinforcement Learning Policies for   Crop Planning Decision Support"></a>Comparative Analysis of Multi-Agent Reinforcement Learning Policies for   Crop Planning Decision Support</h2><p><strong>Authors:Anubha Mahajan, Shreya Hegde, Ethan Shay, Daniel Wu, Aviva Prins</strong></p>
<p>In India, the majority of farmers are classified as small or marginal, making their livelihoods particularly vulnerable to economic losses due to market saturation and climate risks. Effective crop planning can significantly impact their expected income, yet existing decision support systems (DSS) often provide generic recommendations that fail to account for real-time market dynamics and the interactions among multiple farmers. In this paper, we evaluate the viability of three multi-agent reinforcement learning (MARL) approaches for optimizing total farmer income and promoting fairness in crop planning: Independent Q-Learning (IQL), where each farmer acts independently without coordination, Agent-by-Agent (ABA), which sequentially optimizes each farmerâ€™s policy in relation to the others, and the Multi-agent Rollout Policy, which jointly optimizes all farmersâ€™ actions for global reward maximization. Our results demonstrate that while IQL offers computational efficiency with linear runtime, it struggles with coordination among agents, leading to lower total rewards and an unequal distribution of income. Conversely, the Multi-agent Rollout policy achieves the highest total rewards and promotes equitable income distribution among farmers but requires significantly more computational resources, making it less practical for large numbers of agents. ABA strikes a balance between runtime efficiency and reward optimization, offering reasonable total rewards with acceptable fairness and scalability. These findings highlight the importance of selecting appropriate MARL approaches in DSS to provide personalized and equitable crop planning recommendations, advancing the development of more adaptive and farmer-centric agricultural decision-making systems. </p>
<blockquote>
<p>åœ¨å°åº¦ï¼Œå¤§å¤šæ•°å†œæ°‘è¢«å½’ç±»ä¸ºå°å†œæˆ·æˆ–è¾¹ç¼˜å†œæˆ·ï¼Œä»–ä»¬çš„ç”Ÿè®¡ç‰¹åˆ«å®¹æ˜“å—åˆ°å¸‚åœºé¥±å’Œå’Œæ°”å€™é£é™©å¯¼è‡´çš„ç»æµæŸå¤±çš„å½±å“ã€‚æœ‰æ•ˆçš„ä½œç‰©è§„åˆ’ä¼šå¯¹ä»–ä»¬çš„é¢„æœŸæ”¶å…¥äº§ç”Ÿé‡å¤§å½±å“ï¼Œç„¶è€Œç°æœ‰çš„å†³ç­–æ”¯æŒç³»ç»Ÿï¼ˆDSSï¼‰é€šå¸¸æä¾›é€šç”¨çš„å»ºè®®ï¼Œè¿™äº›å»ºè®®æœªèƒ½è€ƒè™‘å®æ—¶å¸‚åœºåŠ¨æ€å’Œå¤šä¸ªå†œæ°‘ä¹‹é—´çš„ç›¸äº’ä½œç”¨ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è¯„ä¼°äº†ä¸‰ç§å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰æ–¹æ³•åœ¨ä½œç‰©è§„åˆ’ä¸­ä¼˜åŒ–å†œæ°‘æ€»æ”¶å…¥å’Œä¿ƒè¿›å…¬å¹³æ–¹é¢çš„å¯è¡Œæ€§ï¼šç‹¬ç«‹Qå­¦ä¹ ï¼ˆIQLï¼‰ï¼Œæ¯ä¸ªå†œæ°‘ç‹¬ç«‹è¡ŒåŠ¨è€Œæ— åè°ƒï¼›é€ä¸ªæ™ºèƒ½ä½“ï¼ˆABAï¼‰ï¼ŒæŒ‰åºä¼˜åŒ–æ¯ä¸ªå†œæ°‘çš„æ”¿ç­–ï¼Œç›¸å¯¹äºå…¶ä»–å†œæ°‘è€Œè¨€ï¼›å¤šæ™ºèƒ½ä½“è¯•ç©ç­–ç•¥ï¼Œè”åˆä¼˜åŒ–æ‰€æœ‰å†œæ°‘çš„è¡Œä¸ºä»¥å®ç°å…¨å±€å¥–åŠ±æœ€å¤§åŒ–ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œè™½ç„¶IQLåœ¨è®¡ç®—æ•ˆç‡æ–¹é¢è¡¨ç°å‡ºçº¿æ€§è¿è¡Œæ—¶çš„ä¼˜åŠ¿ï¼Œä½†åœ¨æ™ºèƒ½ä½“ä¹‹é—´çš„åè°ƒæ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå¯¼è‡´æ€»å¥–åŠ±è¾ƒä½å’Œæ”¶å…¥åˆ†å¸ƒä¸å‡ã€‚ç›¸åï¼Œå¤šæ™ºèƒ½ä½“è¯•ç©ç­–ç•¥è·å¾—äº†æœ€é«˜çš„æ€»å¥–åŠ±ï¼Œä¿ƒè¿›äº†å†œæ°‘ä¹‹é—´çš„å…¬å¹³æ”¶å…¥åˆ†é…ï¼Œä½†éœ€è¦æ›´å¤šçš„è®¡ç®—èµ„æºï¼Œå¯¹äºå¤§é‡æ™ºèƒ½ä½“è€Œè¨€å®ç”¨æ€§è¾ƒä½ã€‚ABAåœ¨è¿è¡Œæ—¶æ•ˆç‡å’Œå¥–åŠ±ä¼˜åŒ–ä¹‹é—´è¾¾åˆ°äº†å¹³è¡¡ï¼Œä»¥åˆç†çš„æ€»å¥–åŠ±å’Œå¯æ¥å—çš„å…¬å¹³æ€§å’Œå¯æ‰©å±•æ€§æä¾›å»ºè®®ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†åœ¨é€‰æ‹©é€‚å½“çš„MARLæ–¹æ³•ä»¥åœ¨DSSä¸­æä¾›ä¸ªæ€§åŒ–å’Œå…¬å¹³çš„ä½œç‰©è§„åˆ’å»ºè®®æ–¹é¢çš„é‡è¦æ€§ï¼Œæ¨åŠ¨äº†æ›´å…·é€‚åº”æ€§å’Œä»¥å†œæ°‘ä¸ºä¸­å¿ƒçš„å†œä¸šå†³ç­–ç³»ç»Ÿçš„å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.02057v2">PDF</a> </p>
<p><strong>Summary</strong><br>     å°åº¦å¤šæ•°å†œæ°‘å±äºå°è§„æ¨¡æˆ–è¾¹ç¼˜å†œæˆ·ï¼Œä»–ä»¬çš„ç”Ÿè®¡æ˜“å—å¸‚åœºé¥±å’Œå’Œæ°”å€™é£é™©çš„å½±å“ã€‚æœ¬æ–‡è¯„ä¼°äº†ä¸‰ç§å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰æ–¹æ³•åœ¨ä¼˜åŒ–å†œæ°‘æ”¶å…¥å’Œä¿ƒè¿›ç§æ¤å…¬å¹³æ–¹é¢çš„å¯è¡Œæ€§ï¼šç‹¬ç«‹Qå­¦ä¹ ï¼ˆIQLï¼‰ã€é€ä¸ªä»£ç†ï¼ˆABAï¼‰å’Œå¤šæ™ºèƒ½ä½“æ»šåŠ¨ç­–ç•¥ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒIQLè®¡ç®—æ•ˆç‡é«˜ä½†ç¼ºä¹åè°ƒï¼Œå¯¼è‡´æ€»æ”¶å…¥è¾ƒä½å’Œæ”¶å…¥åˆ†é…ä¸å‡ï¼›å¤šæ™ºèƒ½ä½“æ»šåŠ¨ç­–ç•¥è™½å®ç°æœ€é«˜æ€»æ”¶å…¥å’Œå…¬å¹³çš„æ”¶å…¥åˆ†é…ï¼Œä½†è®¡ç®—èµ„æºéœ€æ±‚å¤§ï¼Œéš¾ä»¥åº”å¯¹å¤§é‡æ™ºèƒ½ä½“ã€‚ABAåœ¨è¿è¡Œæ—¶æ•ˆç‡å’Œå¥–åŠ±ä¼˜åŒ–ä¹‹é—´å–å¾—å¹³è¡¡ï¼Œæä¾›åˆç†çš„æ€»æ”¶å…¥å’Œå…¬å¹³æ€§ã€‚ç ”ç©¶å¼ºè°ƒäº†ä¸ºDSSé€‰æ‹©åˆé€‚MARLæ–¹æ³•çš„é‡è¦æ€§ï¼Œä»¥æä¾›ä¸ªæ€§åŒ–å…¬å¹³çš„ç§æ¤å»ºè®®ï¼Œæ¨åŠ¨é€‚åº”æ€§æ›´å¼ºã€ä»¥å†œæ°‘ä¸ºä¸­å¿ƒçš„å†œä¸šå†³ç­–ç³»ç»Ÿçš„å¼€å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å°åº¦å¤šæ•°å†œæ°‘é¢ä¸´ç”Ÿè®¡è„†å¼±é—®é¢˜ï¼Œæ€¥éœ€æœ‰æ•ˆçš„å†³ç­–æ”¯æŒç³»ç»Ÿæ¥åº”å¯¹å¸‚åœºé¥±å’Œå’Œæ°”å€™é£é™©ã€‚</li>
<li>å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰åœ¨ä¼˜åŒ–å†œæ°‘æ€»æ”¶å…¥å’Œä¿ƒè¿›ç§æ¤å…¬å¹³æ–¹é¢å…·æœ‰æ½œåŠ›ã€‚</li>
<li>ç‹¬ç«‹Qå­¦ä¹ ï¼ˆIQLï¼‰è™½ç„¶è®¡ç®—æ•ˆç‡é«˜ï¼Œä½†åœ¨åè°ƒå¤šæ™ºèƒ½ä½“æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå¯¼è‡´æ”¶å…¥ä¸å‡ã€‚</li>
<li>å¤šæ™ºèƒ½ä½“æ»šåŠ¨ç­–ç•¥è™½ç„¶èƒ½æœ€å¤§åŒ–æ€»æ”¶å…¥å¹¶ä¿ƒè¿›å…¬å¹³åˆ†é…ï¼Œä½†å¯¹è®¡ç®—èµ„æºéœ€æ±‚å¤§ï¼Œéš¾ä»¥åº”ç”¨äºå¤§é‡æ™ºèƒ½ä½“çš„åœºæ™¯ã€‚</li>
<li>é€ä¸ªä»£ç†ï¼ˆABAï¼‰æ–¹æ³•å®ç°äº†è¿è¡Œæ—¶æ•ˆç‡å’Œå¥–åŠ±ä¼˜åŒ–ä¹‹é—´çš„å¹³è¡¡ï¼Œå…·æœ‰åˆç†çš„æ€»æ”¶å…¥å’Œå…¬å¹³æ€§è¡¨ç°ã€‚</li>
<li>å†³ç­–æ”¯æŒç³»ç»Ÿåº”æä¾›ä¸ªæ€§åŒ–çš„ç§æ¤å»ºè®®ï¼Œä»¥å¢å¼ºç³»ç»Ÿçš„é€‚åº”æ€§å’Œä»¥å†œæ°‘ä¸ºä¸­å¿ƒçš„åŸåˆ™ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.02057">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-49e02cf2018e715364ff826191824bf4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f6fc2d31ace8d934d9b4b521ff3941c7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f383b5ac4359e5b8b22aa9d09669f869.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="GraphTeam-Facilitating-Large-Language-Model-based-Graph-Analysis-via-Multi-Agent-Collaboration"><a href="#GraphTeam-Facilitating-Large-Language-Model-based-Graph-Analysis-via-Multi-Agent-Collaboration" class="headerlink" title="GraphTeam: Facilitating Large Language Model-based Graph Analysis via   Multi-Agent Collaboration"></a>GraphTeam: Facilitating Large Language Model-based Graph Analysis via   Multi-Agent Collaboration</h2><p><strong>Authors:Xin Sky Li, Qizhi Chu, Yubin Chen, Yang Liu, Yaoqi Liu, Zekai Yu, Weize Chen, Chen Qian, Chuan Shi, Cheng Yang</strong></p>
<p>Graphs are widely used for modeling relational data in real-world scenarios, such as social networks and urban computing. Existing LLM-based graph analysis approaches either integrate graph neural networks (GNNs) for specific machine learning tasks, limiting their transferability, or rely solely on LLMsâ€™ internal reasoning ability, resulting in suboptimal performance. To address these limitations, we take advantage of recent advances in LLM-based agents, which have shown capabilities of utilizing external knowledge or tools for problem solving. By simulating human problem-solving strategies such as analogy and collaboration, we propose a multi-agent system based on LLMs named GraphTeam, for graph analysis. GraphTeam consists of five LLM-based agents from three modules, and the agents with different specialities can collaborate with each other to address complex problems. Specifically, (1) input-output normalization module: the question agent extracts and refines four key arguments from the original question, facilitating the problem understanding, and the answer agent organizes the results to meet the output requirement; (2) external knowledge retrieval module: we first build a knowledge base consisting of relevant documentation and experience information, and then the search agent retrieves the most relevant entries for each question. (3) problem-solving module: given the retrieved information from search agent, the coding agent uses established algorithms via programming to generate solutions, and in case the coding agent does not work, the reasoning agent will directly compute the results without programming. Extensive experiments on six graph analysis benchmarks demonstrate that GraphTeam achieves state-of-the-art performance with an average 25.85% improvement over the best baseline in terms of accuracy. The code and data are available at <a target="_blank" rel="noopener" href="https://github.com/BUPT-GAMMA/GraphTeam">https://github.com/BUPT-GAMMA/GraphTeam</a>. </p>
<blockquote>
<p>å›¾è¢«å¹¿æ³›åº”ç”¨äºç°å®ä¸–ç•Œåœºæ™¯ä¸­çš„å…³ç³»æ•°æ®å»ºæ¨¡ï¼Œå¦‚ç¤¾ä¼šç½‘ç»œå’ŒåŸå¸‚è®¡ç®—ã€‚ç°æœ‰çš„åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å›¾åˆ†æè¦ä¹ˆå°†å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰æ•´åˆåˆ°ç‰¹å®šçš„æœºå™¨å­¦ä¹ ä»»åŠ¡ä¸­ï¼Œé™åˆ¶äº†å…¶è¿ç§»èƒ½åŠ›ï¼Œè¦ä¹ˆåªä¾èµ–äºLLMçš„å†…éƒ¨æ¨ç†èƒ½åŠ›ï¼Œå¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬åˆ©ç”¨åŸºäºLLMçš„ä»£ç†çš„æœ€æ–°è¿›å±•ï¼Œè¿™äº›ä»£ç†æ˜¾ç¤ºå‡ºåˆ©ç”¨å¤–éƒ¨çŸ¥è¯†æˆ–å·¥å…·è§£å†³é—®é¢˜çš„èƒ½åŠ›ã€‚é€šè¿‡æ¨¡æ‹Ÿäººç±»çš„ç±»æ¯”å’Œåä½œç­‰è§£å†³é—®é¢˜ç­–ç•¥ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåä¸ºGraphTeamçš„åŸºäºLLMçš„å¤šä»£ç†ç³»ç»Ÿï¼Œç”¨äºå›¾åˆ†æã€‚GraphTeamç”±ä¸‰ä¸ªæ¨¡å—ä¸­çš„äº”ä¸ªåŸºäºLLMçš„ä»£ç†ç»„æˆï¼Œä¸åŒä¸“ä¸šçš„ä»£ç†å¯ä»¥ç›¸äº’åä½œæ¥è§£å†³å¤æ‚é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œï¼ˆ1ï¼‰è¾“å…¥è¾“å‡ºå½’ä¸€åŒ–æ¨¡å—ï¼šé—®é¢˜ä»£ç†ä»åŸå§‹é—®é¢˜ä¸­æå–å¹¶ä¼˜åŒ–å››ä¸ªå…³é”®å‚æ•°ï¼Œä¿ƒè¿›å¯¹é—®é¢˜çš„ç†è§£ï¼Œç­”æ¡ˆä»£ç†åˆ™è´Ÿè´£å°†ç»“æœç»„ç»‡æˆç¬¦åˆè¾“å‡ºè¦æ±‚çš„å½¢å¼ï¼›ï¼ˆ2ï¼‰å¤–éƒ¨çŸ¥è¯†æ£€ç´¢æ¨¡å—ï¼šæˆ‘ä»¬é¦–å…ˆå»ºç«‹ä¸€ä¸ªåŒ…å«ç›¸å…³æ–‡æ¡£å’Œç»éªŒä¿¡æ¯çš„çŸ¥è¯†åº“ï¼Œç„¶åæœç´¢ä»£ç†ä¼šé’ˆå¯¹æ¯ä¸ªé—®é¢˜æ£€ç´¢æœ€ç›¸å…³çš„æ¡ç›®ã€‚ï¼ˆ3ï¼‰é—®é¢˜è§£å†³æ¨¡å—ï¼šç»™å®šæ¥è‡ªæœç´¢ä»£ç†çš„æ£€ç´¢ä¿¡æ¯ï¼Œç¼–ç ä»£ç†é€šè¿‡ä½¿ç”¨ç¼–ç¨‹å»ºç«‹çš„ç®—æ³•æ¥ç”Ÿæˆè§£å†³æ–¹æ¡ˆï¼Œå¦‚æœç¼–ç ä»£ç†æ— æ³•å·¥ä½œï¼Œæ¨ç†ä»£ç†å°†ç›´æ¥è¿›è¡Œè®¡ç®—å¹¶å¾—å‡ºç»“æœã€‚åœ¨å…­ä¸ªå›¾åˆ†æåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒGraphTeamè¾¾åˆ°äº†æœ€æ–°çš„æŠ€æœ¯æ€§èƒ½æ°´å¹³ï¼Œåœ¨å‡†ç¡®æ€§æ–¹é¢ç›¸å¯¹äºæœ€ä½³åŸºå‡†å¹³å‡æé«˜äº†25.85%ã€‚ç›¸å…³ä»£ç å’Œæ•°æ®å¯ä»¥åœ¨<a target="_blank" rel="noopener" href="https://github.com/BUPT-GAMMA/GraphTeam%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/BUPT-GAMMA/GraphTeamæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.18032v4">PDF</a> </p>
<p><strong>Summary</strong><br>    åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ºåŸºç¡€çš„å¤šæ™ºèƒ½ä½“ç³»ç»ŸGraphTeamè¿›è¡Œå›¾åˆ†æï¼Œé€šè¿‡æ¨¡æ‹Ÿäººç±»é—®é¢˜è§£å†³ç­–ç•¥å¦‚ç±»æ¯”å’Œåä½œï¼Œè§£å†³å•ä¸€LLMåœ¨è§£å†³å¤æ‚å›¾åˆ†æä»»åŠ¡æ—¶çš„å±€é™æ€§ã€‚GraphTeamç”±è¾“å…¥è¾“æ­£è§„åŒ–æ¨¡å—ã€å¤–éƒ¨çŸ¥è¯†æ£€ç´¢æ¨¡å—å’Œé—®é¢˜è§£å†³æ¨¡å—ç»„æˆï¼Œæ—¨åœ¨æé«˜å›¾åˆ†æçš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚å…¶åœ¨å…­ä¸ªå›¾åˆ†æåŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æ˜¾è‘—æˆæœï¼Œå¹³å‡æå‡å‡†ç¡®ç‡é«˜è¾¾25.85%ã€‚ç›¸å…³ä»£ç å’Œæ•°æ®å¯åœ¨GitHubä¸Šè·å–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GraphTeamåˆ©ç”¨LLMä¸ºåŸºç¡€çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œè§£å†³äº†ç°æœ‰å›¾åˆ†ææ–¹æ³•çš„å±€é™æ€§ã€‚</li>
<li>GraphTeamé€šè¿‡æ¨¡æ‹Ÿäººç±»é—®é¢˜è§£å†³ç­–ç•¥å¦‚ç±»æ¯”å’Œåä½œï¼Œæé«˜äº†å›¾åˆ†æçš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚</li>
<li>GraphTeamç”±è¾“å…¥è¾“æ­£è§„åŒ–æ¨¡å—ã€å¤–éƒ¨çŸ¥è¯†æ£€ç´¢æ¨¡å—å’Œé—®é¢˜è§£å†³æ¨¡å—ç»„æˆï¼Œå„æ¨¡å—ä¸­çš„æ™ºèƒ½ä½“å…·å¤‡ä¸åŒçš„ä¸“ä¸šåŠŸèƒ½å¹¶èƒ½ç›¸äº’åä½œã€‚</li>
<li>è¾“å…¥è¾“æ­£è§„åŒ–æ¨¡å—é€šè¿‡é—®é¢˜æç‚¼å’Œç»“æœç»„ç»‡ï¼Œä¿ƒè¿›äº†é—®é¢˜çš„ç†è§£å’Œç­”æ¡ˆçš„å‘ˆç°ã€‚</li>
<li>å¤–éƒ¨çŸ¥è¯†æ£€ç´¢æ¨¡å—æ„å»ºäº†çŸ¥è¯†åº“ï¼Œå¹¶é€šè¿‡æœç´¢æ™ºèƒ½ä½“ä¸ºé—®é¢˜æ£€ç´¢æœ€ç›¸å…³çš„ä¿¡æ¯ã€‚</li>
<li>é—®é¢˜è§£å†³æ¨¡å—åˆ©ç”¨ç¼–ç¨‹æ™ºèƒ½ä½“å»ºç«‹çš„ç®—æ³•ç”Ÿæˆè§£å†³æ–¹æ¡ˆï¼Œè‹¥ç¼–ç¨‹æ™ºèƒ½ä½“æ— æ³•å·¥ä½œï¼Œåˆ™æ¨ç†æ™ºèƒ½ä½“ä¼šç›´æ¥è®¡ç®—ç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.18032">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-02e66c52607d7867b2c4aca28da704a5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6174aaec6b371137dfce026a2e408e6c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cd6e32e390d7e1a502518465cfd21472.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="SPA-Bench-A-Comprehensive-Benchmark-for-SmartPhone-Agent-Evaluation"><a href="#SPA-Bench-A-Comprehensive-Benchmark-for-SmartPhone-Agent-Evaluation" class="headerlink" title="SPA-Bench: A Comprehensive Benchmark for SmartPhone Agent Evaluation"></a>SPA-Bench: A Comprehensive Benchmark for SmartPhone Agent Evaluation</h2><p><strong>Authors:Jingxuan Chen, Derek Yuen, Bin Xie, Yuhao Yang, Gongwei Chen, Zhihao Wu, Li Yixing, Xurui Zhou, Weiwen Liu, Shuai Wang, Kaiwen Zhou, Rui Shao, Liqiang Nie, Yasheng Wang, Jianye Hao, Jun Wang, Kun Shao</strong></p>
<p>Smartphone agents are increasingly important for helping users control devices efficiently, with (Multimodal) Large Language Model (MLLM)-based approaches emerging as key contenders. Fairly comparing these agents is essential but challenging, requiring a varied task scope, the integration of agents with different implementations, and a generalisable evaluation pipeline to assess their strengths and weaknesses. In this paper, we present SPA-B ENCH, a comprehensive SmartPhone Agent Benchmark designed to evaluate (M)LLM-based agents in an interactive environment that simulates real-world conditions. SPA-B ENCH offers three key contributions: (1) A diverse set of tasks covering system and third-party apps in both English and Chinese, focusing on features commonly used in daily routines; (2) A plug-and-play framework enabling real-time agent interaction with Android devices, integrating over ten agents with the flexibility to add more; (3) A novel evaluation pipeline that automatically assesses agent performance across multiple dimensions, encompassing seven metrics related to task completion and resource consumption. Our extensive experiments across tasks and agents reveal challenges like interpreting mobile user interfaces, action grounding, memory retention, and execution costs. We propose future research directions to ease these difficulties, moving closer to real-world smartphone agent applications. SPA-B ENCH is available at <a target="_blank" rel="noopener" href="https://ai-agents-2030.github.io/SPA-Bench/">https://ai-agents-2030.github.io/SPA-Bench/</a>. </p>
<blockquote>
<p>æ™ºèƒ½æ‰‹æœºä»£ç†åœ¨å¸®åŠ©ç”¨æˆ·é«˜æ•ˆæ§åˆ¶è®¾å¤‡æ–¹é¢è¶Šæ¥è¶Šé‡è¦ï¼ŒåŸºäºï¼ˆå¤šæ¨¡æ€ï¼‰å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„æ–¹æ³•å·²æˆä¸ºå…³é”®ç«äº‰è€…ã€‚å¯¹è¿™äº›ä»£ç†è¿›è¡Œå…¬å¹³æ¯”è¾ƒæ˜¯éå¸¸å¿…è¦çš„ï¼Œä½†å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œéœ€è¦ä»»åŠ¡èŒƒå›´å¤šæ ·åŒ–ï¼Œé›†æˆä¸åŒå®ç°çš„ä»£ç†ï¼Œä»¥åŠä¸€ä¸ªé€šç”¨çš„è¯„ä¼°æµç¨‹æ¥è¯„ä¼°å®ƒä»¬çš„ä¼˜ç¼ºç‚¹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†SPA-B ENCHï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„æ™ºèƒ½æ‰‹æœºä»£ç†åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨åœ¨æ¨¡æ‹ŸçœŸå®ä¸–ç•Œæ¡ä»¶çš„äº¤äº’å¼ç¯å¢ƒä¸­è¯„ä¼°åŸºäºï¼ˆMï¼‰LLMçš„ä»£ç†ã€‚SPA-B ENCHæä¾›äº†ä¸‰ä¸ªä¸»è¦è´¡çŒ®ï¼šï¼ˆ1ï¼‰æ¶µç›–ç³»ç»Ÿå’Œç¬¬ä¸‰æ–¹åº”ç”¨ç¨‹åºçš„å¤šæ ·åŒ–ä»»åŠ¡ï¼ŒåŒ…æ‹¬è‹±è¯­å’Œä¸­æ–‡ï¼Œé‡ç‚¹æ˜¯åœ¨æ—¥å¸¸ä¾‹è¡Œç¨‹åºä¸­å¸¸ç”¨çš„åŠŸèƒ½ï¼›ï¼ˆ2ï¼‰ä¸€ä¸ªå³æ’å³ç”¨çš„æ¡†æ¶ï¼Œä½¿ä»£ç†èƒ½å¤Ÿå®æ—¶ä¸å®‰å“è®¾å¤‡è¿›è¡Œäº¤äº’ï¼Œé›†æˆäº†åå¤šä¸ªä»£ç†ï¼Œå¹¶å…·å¤‡æ·»åŠ æ›´å¤šä»£ç†çš„çµæ´»æ€§ï¼›ï¼ˆ3ï¼‰ä¸€ä¸ªæ–°çš„è¯„ä¼°æµç¨‹ï¼Œè¯¥æµç¨‹èƒ½å¤Ÿè‡ªåŠ¨ä»å¤šä¸ªç»´åº¦è¯„ä¼°ä»£ç†æ€§èƒ½ï¼ŒåŒ…æ‹¬ä¸ä»»åŠ¡å®Œæˆå’Œèµ„æºæ¶ˆè€—ç›¸å…³çš„ä¸ƒä¸ªæŒ‡æ ‡ã€‚æˆ‘ä»¬åœ¨ä¸åŒä»»åŠ¡å’Œä»£ç†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒæ­ç¤ºäº†è¯¸å¦‚è§£é‡Šç§»åŠ¨ç”¨æˆ·ç•Œé¢ã€åŠ¨ä½œå®šä½ã€è®°å¿†ä¿ç•™å’Œæ‰§è¡Œæˆæœ¬ç­‰æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†æœªæ¥ç ”ç©¶çš„æ–¹å‘æ¥ç¼“è§£è¿™äº›å›°éš¾ï¼Œæ›´æ¥è¿‘ç°å®ä¸–ç•Œçš„æ™ºèƒ½æ‰‹æœºä»£ç†åº”ç”¨ç¨‹åºã€‚SPA-B ENCHå¯åœ¨<a target="_blank" rel="noopener" href="https://ai-agents-2030.github.io/SPA-Bench/%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://ai-agents-2030.github.io/SPA-Bench/ä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.15164v2">PDF</a> ICLR 2025 Spotlight</p>
<p><strong>Summary</strong><br>     æ™ºèƒ½æ‰‹æœºä»£ç†åœ¨å¸®åŠ©ç”¨æˆ·é«˜æ•ˆæ§åˆ¶è®¾å¤‡æ–¹é¢è¶Šæ¥è¶Šé‡è¦ï¼ŒåŸºäºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„æ–¹æ³•æˆä¸ºå…³é”®ç«äº‰è€…ã€‚æœ¬æ–‡æå‡ºäº†SPA-B ENCHæ™ºèƒ½æ‰‹æœºä»£ç†åŸºå‡†æµ‹è¯•å¹³å°ï¼Œè¯¥å¹³å°å¯åœ¨æ¨¡æ‹ŸçœŸå®ç¯å¢ƒçš„äº¤äº’å¼ç¯å¢ƒä¸­è¯„ä¼°MLLMä»£ç†çš„æ€§èƒ½ã€‚è¯¥å¹³å°å…·æœ‰ä¸‰ä¸ªå…³é”®è´¡çŒ®ï¼ŒåŒ…æ‹¬å¤šæ ·åŒ–çš„ä»»åŠ¡é›†ã€å®æ—¶äº¤äº’çš„æ’ä»¶æ¡†æ¶ä»¥åŠè‡ªåŠ¨è¯„ä¼°ä»£ç†æ€§èƒ½çš„è¯„ä»·ç®¡é“ã€‚å®éªŒæ­ç¤ºäº†ç§»åŠ¨ç”¨æˆ·æ¥å£è§£è¯»ã€è¡ŒåŠ¨å®šä½ã€è®°å¿†ä¿ç•™å’Œæ‰§è¡Œæˆæœ¬ç­‰æŒ‘æˆ˜ã€‚è¯¥å¹³å°ä¿ƒè¿›äº†æ™ºèƒ½åŠ©ç†åœ¨å®é™…åº”ç”¨ä¸­çš„å‘å±•ã€‚ </p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŸºäºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„æ™ºèƒ½ä»£ç†åœ¨æ™ºèƒ½æ‰‹æœºæ§åˆ¶æ–¹é¢æ—¥ç›Šé‡è¦ã€‚</li>
<li>SPA-B ENCHæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°æ™ºèƒ½ä»£ç†æ€§èƒ½çš„åŸºå‡†æµ‹è¯•å¹³å°ï¼Œæ¨¡æ‹ŸçœŸå®ç¯å¢ƒã€‚</li>
<li>å¹³å°åŒ…å«å¤šæ ·åŒ–çš„ä»»åŠ¡é›†ï¼Œæ¶µç›–ç³»ç»Ÿå’Œç¬¬ä¸‰æ–¹åº”ç”¨ï¼Œæ”¯æŒä¸­è‹±æ–‡ã€‚</li>
<li>å¹³å°æä¾›äº†ä¸€ä¸ªæ’ä»¶æ¡†æ¶ï¼Œå¯ä¸å®‰å“è®¾å¤‡è¿›è¡Œå®æ—¶äº¤äº’ï¼Œé›†æˆå¤šä¸ªæ™ºèƒ½ä»£ç†ã€‚</li>
<li>å¹³å°é‡‡ç”¨æ–°å‹è¯„ä»·ç®¡é“è‡ªåŠ¨è¯„ä¼°ä»£ç†åœ¨ä»»åŠ¡å®Œæˆå’Œèµ„æºæ¶ˆè€—æ–¹é¢çš„è¡¨ç°ã€‚</li>
<li>å®éªŒæ­ç¤ºäº†ç§»åŠ¨ç”¨æˆ·æ¥å£è§£è¯»ã€è¡ŒåŠ¨å®šä½ã€è®°å¿†ä¿ç•™å’Œæ‰§è¡Œæˆæœ¬ç­‰æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.15164">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4609b1ea159fed062287f3e02bad57b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b293ac1f80e7f7f71923e9bb391073ac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-135efcc03f6301f67419be32036ab131.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7d746ad5a4f1769f4f7f696123e72b13.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ecde3517b3b7f4a14e5693a8b2970b69.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Efficient-Multi-agent-Navigation-with-Lightweight-DRL-Policy"><a href="#Efficient-Multi-agent-Navigation-with-Lightweight-DRL-Policy" class="headerlink" title="Efficient Multi-agent Navigation with Lightweight DRL Policy"></a>Efficient Multi-agent Navigation with Lightweight DRL Policy</h2><p><strong>Authors:Xingrong Diao, Jiankun Wang</strong></p>
<p>In this article, we present an end-to-end collision avoidance policy based on deep reinforcement learning (DRL) for multi-agent systems, demonstrating encouraging outcomes in real-world applications. In particular, our policy calculates the control commands of the agent based on the raw LiDAR observation. In addition, the number of parameters of the proposed basic model is 140,000, and the size of the parameter file is 3.5 MB, which allows the robot to calculate the actions from the CPU alone. We propose a multi-agent training platform based on a physics-based simulator to further bridge the gap between simulation and the real world. The policy is trained on a policy-gradients-based RL algorithm in a dense and messy training environment. A novel reward function is introduced to address the issue of agents choosing suboptimal actions in some common scenarios. Although the data used for training is exclusively from the simulation platform, the policy can be successfully transferred and deployed in real-world robots. Finally, our policy effectively responds to intentional obstructions and avoids collisions. The website is available at <a target="_blank" rel="noopener" href="https://sites.google.com/view/xingrong2024efficient/%E9%A6%96%E9%A1%B5">https://sites.google.com/view/xingrong2024efficient/%E9%A6%96%E9%A1%B5</a>. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰çš„ç«¯åˆ°ç«¯ç¢°æ’é¿å…ç­–ç•¥ï¼Œé€‚ç”¨äºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œå¹¶åœ¨å®é™…åº”ç”¨ä¸­å–å¾—äº†ä»¤äººé¼“èˆçš„æ•ˆæœã€‚æˆ‘ä»¬çš„ç­–ç•¥æ ¹æ®åŸå§‹çš„æ¿€å…‰é›·è¾¾è§‚æµ‹ç»“æœè®¡ç®—æ™ºèƒ½ä½“çš„æ§åˆ¶å‘½ä»¤ã€‚æ­¤å¤–ï¼Œæ‰€æå‡ºçš„åŸºç¡€æ¨¡å‹çš„å‚æ•°æ•°é‡ä¸º14ä¸‡ä¸ªï¼Œå‚æ•°æ–‡ä»¶å¤§å°ä¸º3.5MBï¼Œä½¿å¾—æœºå™¨äººèƒ½å¤Ÿä»…é€šè¿‡CPUè®¡ç®—åŠ¨ä½œã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºç‰©ç†æ¨¡æ‹Ÿå™¨çš„å¤šæ™ºèƒ½ä½“è®­ç»ƒå¹³å°ï¼Œä»¥è¿›ä¸€æ­¥å¼¥åˆä»¿çœŸä¸ç°å®ä¹‹é—´çš„å·®è·ã€‚è¯¥ç­–ç•¥åœ¨å¯†é›†è€Œæ··ä¹±çš„è®­ç»ƒç¯å¢ƒä¸­ï¼Œé‡‡ç”¨åŸºäºç­–ç•¥æ¢¯åº¦çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•è¿›è¡Œè®­ç»ƒã€‚ä¸ºè§£å†³æ™ºèƒ½ä½“åœ¨æŸäº›å¸¸è§åœºæ™¯ä¸­é€‰æ‹©æ¬¡ä¼˜åŠ¨ä½œçš„é—®é¢˜ï¼Œå¼•å…¥äº†ä¸€ç§æ–°å‹å¥–åŠ±å‡½æ•°ã€‚è™½ç„¶è®­ç»ƒæ•°æ®å®Œå…¨æ¥è‡ªä»¿çœŸå¹³å°ï¼Œä½†ç­–ç•¥å¯ä»¥æˆåŠŸè½¬ç§»åˆ°å®é™…æœºå™¨äººä¸­è¿›è¡Œéƒ¨ç½²ã€‚æœ€åï¼Œæˆ‘ä»¬çš„ç­–ç•¥å¯ä»¥æœ‰æ•ˆåœ°åº”å¯¹æœ‰æ„é˜»ç¢çš„æƒ…å†µï¼Œé¿å…ç¢°æ’ã€‚[ç½‘ç«™åœ°å€ä¸ºï¼š<a target="_blank" rel="noopener" href="https://sites.google.com/view/xingrong2024efficient/%E9%A6%96%E9%A1%B5%E3%80%82]">https://sites.google.com/view/xingrong2024efficient/%E9%A6%96%E9%A1%B5ã€‚]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.16370v3">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰çš„ç«¯åˆ°ç«¯ç¢°æ’é¿å…ç­–ç•¥ï¼Œé€‚ç”¨äºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œå¹¶åœ¨å®é™…åº”ç”¨ä¸­è¡¨ç°å‡ºè‰¯å¥½æ•ˆæœã€‚è¯¥ç­–ç•¥æ ¹æ®æ¿€å…‰é›·è¾¾çš„åŸå§‹è§‚æµ‹è®¡ç®—æ™ºèƒ½ä½“çš„æ§åˆ¶å‘½ä»¤ï¼Œåˆ©ç”¨ç‰©ç†æ¨¡æ‹Ÿå™¨å»ºç«‹å¤šæ™ºèƒ½ä½“è®­ç»ƒå¹³å°ä»¥ç¼©å°æ¨¡æ‹Ÿä¸ç°å®çš„å·®è·ã€‚è¯¥ç­–ç•¥é€šè¿‡å¼•å…¥æ–°å‹å¥–åŠ±å‡½æ•°è§£å†³äº†æ™ºèƒ½ä½“åœ¨æŸäº›å¸¸è§åœºæ™¯ä¸­é€‰å–æ¬¡ä¼˜åŠ¨ä½œçš„é—®é¢˜ï¼Œè®­ç»ƒæ•°æ®å…¨éƒ¨æ¥è‡ªä»¿çœŸå¹³å°ï¼Œä½†ç­–ç•¥å¯æˆåŠŸè½¬ç§»å¹¶éƒ¨ç½²åˆ°å®é™…æœºå™¨äººä¸­ï¼Œæœ‰æ•ˆåº”å¯¹æ•…æ„é˜»ç¢å¹¶é¿å…ç¢°æ’ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>åŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰çš„ç¢°æ’é¿å…ç­–ç•¥åœ¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­å®ç°æœ‰æ•ˆåº”ç”¨ã€‚</li>
<li>ç­–ç•¥æ ¹æ®æ¿€å…‰é›·è¾¾è§‚æµ‹æ•°æ®è®¡ç®—æ™ºèƒ½ä½“çš„æ§åˆ¶å‘½ä»¤ã€‚</li>
<li>æå‡ºä¸€ç§å¤šæ™ºèƒ½ä½“è®­ç»ƒå¹³å°ï¼Œåˆ©ç”¨ç‰©ç†æ¨¡æ‹Ÿå™¨ç¼©å°æ¨¡æ‹Ÿä¸ç°å®çš„å·®è·ã€‚</li>
<li>å¼•å…¥æ–°å‹å¥–åŠ±å‡½æ•°è§£å†³æ™ºèƒ½ä½“åœ¨å¸¸è§åœºæ™¯ä¸­é€‰å–æ¬¡ä¼˜åŠ¨ä½œçš„é—®é¢˜ã€‚</li>
<li>è®­ç»ƒæ•°æ®å®Œå…¨æ¥è‡ªä»¿çœŸå¹³å°ï¼Œä½†ç­–ç•¥å¯æˆåŠŸåº”ç”¨äºå®é™…æœºå™¨äººã€‚</li>
<li>ç­–ç•¥å¯¹æ•…æ„é˜»ç¢ä½œå‡ºæœ‰æ•ˆå“åº”å¹¶å…·å¤‡ç¢°æ’é¿å…èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.16370">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7562a334eaf1bc584133b919c9427ec1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d7579066953c1a6e2a0c2c87005eacba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d03b5c2afe85a4ad7eb45c3a0823b42.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ea3cb613c30918897f8e2a381e49fc8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3d5ad92209823946961e4f4e3cee208a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a628c6cf6e254da80512caeec45dcd3f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ae6022ab8897ca778e56171a9885a4ad.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Multi-Agent-Causal-Discovery-Using-Large-Language-Models"><a href="#Multi-Agent-Causal-Discovery-Using-Large-Language-Models" class="headerlink" title="Multi-Agent Causal Discovery Using Large Language Models"></a>Multi-Agent Causal Discovery Using Large Language Models</h2><p><strong>Authors:Hao Duong Le, Xin Xia, Zhang Chen</strong></p>
<p>Causal discovery aims to identify causal relationships between variables and is a critical research area in machine learning. Traditional methods focus on statistical or machine learning algorithms to uncover causal links from structured data, often overlooking the valuable contextual information provided by metadata. Large language models (LLMs) have shown promise in creating unified causal discovery frameworks by incorporating both structured data and metadata. However, their potential in multi-agent settings remains largely unexplored. To address this gap, we introduce the Multi-Agent Causal Discovery Framework (MAC), which consists of two key modules: the Debate-Coding Module (DCM) and the Meta-Debate Module (MDM). The DCM begins with a multi-agent debating and coding process, where agents use both structured data and metadata to collaboratively select the most suitable statistical causal discovery (SCD) method. The selected SCD is then applied to the structured data to generate an initial causal graph. This causal graph is transformed into causal metadata through the Meta Fusion mechanism. With all the metadata, MDM then refines the causal structure by leveraging a multi-agent debating framework. Extensive experiments across five datasets demonstrate that MAC outperforms both traditional statistical causal discovery methods and existing LLM-based approaches, achieving state-of-the-art performance. </p>
<blockquote>
<p>å› æœå‘ç°æ—¨åœ¨ç¡®å®šå˜é‡ä¹‹é—´çš„å› æœå…³ç³»ï¼Œæ˜¯æœºå™¨å­¦ä¹ ä¸­çš„ä¸€ä¸ªå…³é”®ç ”ç©¶é¢†åŸŸã€‚ä¼ ç»Ÿæ–¹æ³•ä¸»è¦å…³æ³¨ç»Ÿè®¡æˆ–æœºå™¨å­¦ä¹ ç®—æ³•ï¼Œä»ç»“æ„åŒ–æ•°æ®ä¸­æŒ–æ˜å› æœå…³ç³»ï¼Œå¾€å¾€å¿½è§†äº†å…ƒæ•°æ®æä¾›çš„å®è´µä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡ç»“åˆç»“æ„åŒ–æ•°æ®å’Œå…ƒæ•°æ®ï¼Œåœ¨åˆ›å»ºç»Ÿä¸€çš„å› æœå‘ç°æ¡†æ¶æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨å¤šæ™ºèƒ½ä½“ç¯å¢ƒä¸­çš„æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¤šæ™ºèƒ½ä½“å› æœå‘ç°æ¡†æ¶ï¼ˆMACï¼‰ï¼Œå®ƒåŒ…æ‹¬ä¸¤ä¸ªå…³é”®æ¨¡å—ï¼šè¾©è®ºç¼–ç æ¨¡å—ï¼ˆDCMï¼‰å’Œå…ƒè¾©è®ºæ¨¡å—ï¼ˆMDMï¼‰ã€‚DCMå¼€å§‹äºå¤šæ™ºèƒ½ä½“è¾©è®ºå’Œç¼–ç è¿‡ç¨‹ï¼Œæ™ºèƒ½ä½“ä½¿ç”¨ç»“æ„åŒ–æ•°æ®å’Œå…ƒæ•°æ®ååŒé€‰æ‹©æœ€åˆé€‚çš„ç»Ÿè®¡å› æœå‘ç°ï¼ˆSCDï¼‰æ–¹æ³•ã€‚ç„¶ååº”ç”¨æ‰€é€‰çš„SCDåˆ°ç»“æ„åŒ–æ•°æ®ä¸Šï¼Œç”Ÿæˆåˆå§‹å› æœå›¾ã€‚è¯¥å› æœå›¾é€šè¿‡å…ƒèåˆæœºåˆ¶è½¬åŒ–ä¸ºå› æœå…ƒæ•°æ®ã€‚åˆ©ç”¨æ‰€æœ‰çš„å…ƒæ•°æ®ï¼ŒMDMç„¶ååˆ©ç”¨å¤šæ™ºèƒ½ä½“è¾©è®ºæ¡†æ¶å¯¹å› æœç»“æ„è¿›è¡Œç»†åŒ–ã€‚åœ¨äº”ä¸ªæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMACä¼˜äºä¼ ç»Ÿçš„ç»Ÿè®¡å› æœå‘ç°æ–¹æ³•å’Œç°æœ‰çš„LLMæ–¹æ³•ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.15073v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å› æœå‘ç°çš„é‡è¦æ€§åŠå…¶åœ¨ä¼ ç»Ÿæ–¹æ³•ä¸­çš„å±€é™æ€§ï¼ŒåŒ…æ‹¬åœ¨æ•°æ®ä¸­çš„å¿½ç•¥å…ƒæ•°æ®é—®é¢˜ã€‚é€šè¿‡å¼•å…¥å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå°†ç»“æ„æ•°æ®å’Œå…ƒæ•°æ®ç»“åˆèµ·æ¥ï¼Œèƒ½å¤Ÿæé«˜å› æœå‘ç°çš„å‡†ç¡®æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šæ™ºèƒ½ä½“å› æœå‘ç°æ¡†æ¶ï¼ˆMACï¼‰ï¼ŒåŒ…å«è¾©è®ºç¼–ç æ¨¡å—ï¼ˆDCMï¼‰å’Œå…ƒè¾©è®ºæ¨¡å—ï¼ˆMDMï¼‰ã€‚DCMåˆ©ç”¨ç»“æ„æ•°æ®å’Œå…ƒæ•°æ®ï¼Œé€šè¿‡å¤šæ™ºèƒ½ä½“è¾©è®ºå’Œç¼–ç è¿‡ç¨‹é€‰æ‹©æœ€åˆé€‚çš„ç»Ÿè®¡å› æœå‘ç°æ–¹æ³•ã€‚ç»è¿‡Meta Fusionæœºåˆ¶çš„è½¬åŒ–ï¼Œè¿™äº›å› æœå…³ç³»æœ€ç»ˆç»è¿‡MDMè¿›è¡Œä¼˜åŒ–å¹¶ç»™å‡ºç»“è®ºã€‚ç»è¿‡å¤šé¡¹å®éªŒè¯æ˜ï¼ŒMACç›¸å¯¹äºä¼ ç»Ÿæ–¹æ³•å’Œç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å› æœå‘ç°é¢†åŸŸå…·æœ‰ä¼˜è¶Šçš„æ€§èƒ½ã€‚å…¶æ–¹æ³•çš„èåˆå¤šç§æ¨¡å‹ä¹‹é—´çš„åˆ›æ–°æ€æƒ³å’Œç‹¬ç‰¹è®¾è®¡ï¼Œä½¿å…¶åœ¨å¤šç§æ•°æ®é›†ä¸Šå®ç°äº†æœ€ä½³æ€§èƒ½ã€‚ </p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯æ–‡æœ¬çš„ä¸»è¦è§è§£ï¼š</p>
<ul>
<li>å› æœå‘ç°æ—¨åœ¨ç¡®å®šå˜é‡ä¹‹é—´çš„å› æœå…³ç³»ï¼Œæ˜¯æœºå™¨å­¦ä¹ ä¸­çš„å…³é”®ç ”ç©¶é¢†åŸŸã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•ä¸»è¦ä¾èµ–ç»Ÿè®¡æˆ–æœºå™¨å­¦ä¹ ç®—æ³•æ¥å‘ç°å› æœè”ç³»ï¼Œä½†å¿½ç•¥äº†å…ƒæ•°æ®æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ä»·å€¼ã€‚å¤§å‹è¯­è¨€æ¨¡å‹èƒ½åœ¨æ­¤å¤„èå…¥ä½œç”¨å·¨å¤§çš„å› ç´ å¹¶åŠ å¼ºé›†æˆç³»ç»Ÿå¯¹äºç¯å¢ƒåŠ¨æ€å› ç´ çš„è§£æå’ŒæŠ“å–èƒ½åŠ›ï¼Œè¿›ä¸€æ­¥æé«˜å› æœå…³ç³»åˆ†æçš„ç²¾å‡†æ€§ã€‚æœ¬ç ”ç©¶ä¸ºæ­¤æå‡ºäº†æ–°çš„å› æœå‘ç°æ¡†æ¶ä»¥é€‚åº”èåˆæ–¹å¼çš„è¦æ±‚ã€‚è¿™ä¸ªæ¡†æ¶å¼•å…¥äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„ä½œç”¨å’Œæ¦‚å¿µæ¡†æ¶çš„èåˆåˆ›æ–°æ–¹æ³•ï¼Œä¸ºè§£å†³è¿™ä¸€é—®é¢˜æä¾›äº†æ–°æ–¹å‘ã€‚ </li>
<li>å¤šæ™ºèƒ½ä½“å› æœå‘ç°æ¡†æ¶ï¼ˆMACï¼‰åŒ…å«ä¸¤ä¸ªå…³é”®æ¨¡å—ï¼šè¾©è®ºç¼–ç æ¨¡å—ï¼ˆDCMï¼‰å’Œå…ƒè¾©è®ºæ¨¡å—ï¼ˆMDMï¼‰ã€‚DCMé€šè¿‡å¤šæ™ºèƒ½ä½“è¾©è®ºå’Œç¼–ç è¿‡ç¨‹é€‰æ‹©æœ€åˆé€‚çš„ç»Ÿè®¡å› æœå‘ç°æ–¹æ³•ï¼›MDMåˆ™åˆ©ç”¨å…ƒæ•°æ®ä¼˜åŒ–å’Œæ”¹è¿›å› æœå…³ç³»ç»“æ„ã€‚è¿™ç§ç»„åˆä½¿å¾—MACæ¡†æ¶èƒ½å¤Ÿå……åˆ†åˆ©ç”¨ç»“æ„æ•°æ®å’Œå…ƒæ•°æ®ï¼Œä»è€Œæé«˜å› æœå‘ç°çš„å‡†ç¡®æ€§ã€‚ </li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒMACåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æœ€ä½³æ€§èƒ½ï¼Œä¼˜äºä¼ ç»Ÿæ–¹æ³•å’Œç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚è¿™è¯æ˜äº†MACæ¡†æ¶çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚é€šè¿‡å¤§é‡å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•çš„ç¨³å®šæ€§å’Œå¯é æ€§ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚è¿™ç§å¤šæ™ºèƒ½ä½“ç³»ç»Ÿå› å…¶æ™ºèƒ½å†³ç­–ã€ç²¾å‡†è®¡ç®—å’Œé€‚åº”æ€§å¼ºçš„ç‰¹ç‚¹è€Œåœ¨ä¸åŒåœºæ™¯ä¸‹å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚ç‰¹åˆ«æ˜¯åœ¨é¢å¯¹å¤æ‚å¤šå˜çš„çœŸå®ä¸–ç•Œæ•°æ®æ—¶ï¼Œå®ƒèƒ½å¤Ÿçµæ´»åº”å¯¹å„ç§æƒ…å†µå¹¶ä¿æŒæ€§èƒ½çš„ç¨³å®šè¾“å‡ºä»¥ç²¾å‡†åŒ–å‘ç°å¤æ‚é—®é¢˜çš„å†…åœ¨è§„å¾‹æ€§çš„å› ç´ è¡¨ç°è¡¨ç°å¾—å‡ºå‡ºæ›´ä¸ºä¼˜å¼‚ä¼˜åŠ¿çªå‡ºçš„ç‰¹ç‚¹ç‰¹æ€§ã€‚ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.15073">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3ba561997a8a70549fe0265e0fabf6d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f4e7ab0b19f8879903d4de73fc68a14.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab310bb94d9800f73ecdb103ed660202.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed4b98db153d2dd9bdc37586ea92be86.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Direct-Multi-Turn-Preference-Optimization-for-Language-Agents"><a href="#Direct-Multi-Turn-Preference-Optimization-for-Language-Agents" class="headerlink" title="Direct Multi-Turn Preference Optimization for Language Agents"></a>Direct Multi-Turn Preference Optimization for Language Agents</h2><p><strong>Authors:Wentao Shi, Mengqi Yuan, Junkang Wu, Qifan Wang, Fuli Feng</strong></p>
<p>Adapting Large Language Models (LLMs) for agent tasks is critical in developing language agents. Direct Preference Optimization (DPO) is a promising technique for this adaptation with the alleviation of compounding errors, offering a means to directly optimize Reinforcement Learning (RL) objectives. However, applying DPO to multi-turn tasks presents challenges due to the inability to cancel the partition function. Overcoming this obstacle involves making the partition function independent of the current state and addressing length disparities between preferred and dis-preferred trajectories. In this light, we replace the policy constraint with the state-action occupancy measure constraint in the RL objective and add length normalization to the Bradley-Terry model, yielding a novel loss function named DMPO for multi-turn agent tasks with theoretical explanations. Extensive experiments on three multi-turn agent task datasets confirm the effectiveness and superiority of the DMPO loss. The code is available at <a target="_blank" rel="noopener" href="https://github.com/swt-user/DMPO">https://github.com/swt-user/DMPO</a>. </p>
<blockquote>
<p>åœ¨å¼€å‘è¯­è¨€ä»£ç†æ—¶ï¼Œå°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€‚åº”ä»£ç†ä»»åŠ¡æ˜¯éå¸¸å…³é”®çš„ã€‚ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰æ˜¯ä¸€ç§æœ‰å‰é€”çš„æŠ€æœ¯ï¼Œå¯ä»¥ç¼“è§£å¤åˆé”™è¯¯ï¼Œä¸ºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç›®æ ‡æä¾›ç›´æ¥ä¼˜åŒ–çš„æ‰‹æ®µã€‚ç„¶è€Œï¼Œåœ¨å¤šè½®ä»»åŠ¡ä¸­åº”ç”¨DPOé¢ä¸´ç€æ— æ³•å–æ¶ˆåˆ†åŒºå‡½æ•°çš„æŒ‘æˆ˜ã€‚å…‹æœè¿™ä¸€éšœç¢éœ€è¦ä½¿åˆ†åŒºå‡½æ•°ç‹¬ç«‹äºå½“å‰çŠ¶æ€å¹¶è§£å†³é¦–é€‰å’Œéé¦–é€‰è½¨è¿¹ä¹‹é—´çš„é•¿åº¦å·®å¼‚ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬ç”¨çŠ¶æ€åŠ¨ä½œå ç”¨ç‡åº¦é‡çº¦æŸæ›¿ä»£ç­–ç•¥çº¦æŸåœ¨RLç›®æ ‡ä¸­ï¼Œå¹¶ä¸ºBradley-Terryæ¨¡å‹æ·»åŠ é•¿åº¦å½’ä¸€åŒ–ï¼Œäº§ç”Ÿä¸€ä¸ªæ–°çš„æŸå¤±å‡½æ•°ï¼Œåä¸ºDMPOï¼Œç”¨äºå¤šè½®ä»£ç†ä»»åŠ¡ï¼Œå¹¶æœ‰ç†è®ºè§£é‡Šã€‚åœ¨ä¸‰ä¸ªå¤šè½®ä»£ç†ä»»åŠ¡æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¯å®äº†DMPOæŸå¤±çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚ä»£ç ä½äºï¼š<a target="_blank" rel="noopener" href="https://github.com/swt-user/DMPO%E3%80%82">https://github.com/swt-user/DMPOã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.14868v5">PDF</a> Accepted by EMNLP 2024 Main</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¼€å‘è¯­è¨€ä»£ç†ä»»åŠ¡ä¸­çš„é€‚åº”è‡³å…³é‡è¦ã€‚ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰æ˜¯ä¸€ç§å…·æœ‰æ½œåŠ›çš„æŠ€æœ¯ï¼Œå¯ä»¥ç¼“è§£ç»„åˆé”™è¯¯çš„é—®é¢˜ï¼Œä¸ºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç›®æ ‡æä¾›ç›´æ¥ä¼˜åŒ–çš„æ‰‹æ®µã€‚ç„¶è€Œï¼Œåœ¨å¤šè½®ä»»åŠ¡ä¸­åº”ç”¨DPOé¢ä¸´æŒ‘æˆ˜ï¼Œå…‹æœè¿™ä¸€éšœç¢éœ€è¦ä½¿åˆ†åŒºå‡½æ•°ç‹¬ç«‹äºå½“å‰çŠ¶æ€å¹¶è§£å†³é¦–é€‰å’Œæœªé€‰è½¨è¿¹ä¹‹é—´çš„é•¿åº¦å·®å¼‚ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ç”¨çŠ¶æ€åŠ¨ä½œå ç”¨åº¦é‡çº¦æŸæ›¿æ¢ç­–ç•¥çº¦æŸï¼Œå¹¶åœ¨Bradley-Terryæ¨¡å‹ä¸­å¢åŠ é•¿åº¦å½’ä¸€åŒ–ï¼Œä¸ºé’ˆå¯¹å¤šè½®ä»£ç†ä»»åŠ¡çš„æŸå¤±å‡½æ•°å‘½åä¸ºDMPOï¼Œå¹¶ç»™å‡ºç†è®ºè§£é‡Šã€‚åœ¨ä¸‰ä¸ªå¤šè½®ä»£ç†ä»»åŠ¡æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¯å®äº†DMPOæŸå¤±çš„æœ‰æ•ˆæ€§ã€‚æœ‰å…³ä»£ç çš„ä¿¡æ¯è¯·è®¿é—® <a target="_blank" rel="noopener" href="https://github.com/swt-user/DMPO%E3%80%82">https://github.com/swt-user/DMPOã€‚</a> </p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¼€å‘è¯­è¨€ä»£ç†ä»»åŠ¡ä¸­çš„é€‚åº”è‡³å…³é‡è¦ã€‚</li>
<li>ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰æ˜¯ä¸€ç§ç”¨äºé€‚åº”LLMçš„æœ‰å‰é€”çš„æŠ€æœ¯ï¼Œå¯ä»¥ç¼“è§£ç»„åˆé”™è¯¯é—®é¢˜ã€‚</li>
<li>åœ¨å¤šè½®ä»»åŠ¡ä¸­åº”ç”¨DPOé¢ä¸´æŒ‘æˆ˜ï¼Œéœ€è§£å†³åˆ†åŒºå‡½æ•°çš„ç‹¬ç«‹æ€§å’Œè½¨è¿¹é•¿åº¦å·®å¼‚é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹æŸå¤±å‡½æ•°DMPOï¼Œç”¨äºå¤šè½®ä»£ç†ä»»åŠ¡ï¼Œç”±çŠ¶æ€åŠ¨ä½œå ç”¨åº¦é‡çº¦æŸå’ŒBradley-Terryæ¨¡å‹çš„é•¿åº¦å½’ä¸€åŒ–ç»„æˆã€‚</li>
<li>åœ¨ä¸‰ä¸ªå¤šè½®ä»£ç†ä»»åŠ¡æ•°æ®é›†ä¸Šçš„å®éªŒè¯å®äº†DMPOæŸå¤±çš„æœ‰æ•ˆæ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.14868">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-95792cef454982d90bfc3520e4be43d9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fcdd56cdcb111d3be30d74d84ef8abae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f3a211736ef50ed166d06bb9df95826.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3483245f4c688c1e4fb89ac356307abc.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Anywhere-A-Multi-Agent-Framework-for-User-Guided-Reliable-and-Diverse-Foreground-Conditioned-Image-Generation"><a href="#Anywhere-A-Multi-Agent-Framework-for-User-Guided-Reliable-and-Diverse-Foreground-Conditioned-Image-Generation" class="headerlink" title="Anywhere: A Multi-Agent Framework for User-Guided, Reliable, and Diverse   Foreground-Conditioned Image Generation"></a>Anywhere: A Multi-Agent Framework for User-Guided, Reliable, and Diverse   Foreground-Conditioned Image Generation</h2><p><strong>Authors:Tianyidan Xie, Rui Ma, Qian Wang, Xiaoqian Ye, Feixuan Liu, Ying Tai, Zhenyu Zhang, Lanjun Wang, Zili Yi</strong></p>
<p>Recent advancements in image-conditioned image generation have demonstrated substantial progress. However, foreground-conditioned image generation remains underexplored, encountering challenges such as compromised object integrity, foreground-background inconsistencies, limited diversity, and reduced control flexibility. These challenges arise from current end-to-end inpainting models, which suffer from inaccurate training masks, limited foreground semantic understanding, data distribution biases, and inherent interference between visual and textual prompts. To overcome these limitations, we present Anywhere, a multi-agent framework that departs from the traditional end-to-end approach. In this framework, each agent is specialized in a distinct aspect, such as foreground understanding, diversity enhancement, object integrity protection, and textual prompt consistency. Our framework is further enhanced with the ability to incorporate optional user textual inputs, perform automated quality assessments, and initiate re-generation as needed. Comprehensive experiments demonstrate that this modular design effectively overcomes the limitations of existing end-to-end models, resulting in higher fidelity, quality, diversity and controllability in foreground-conditioned image generation. Additionally, the Anywhere framework is extensible, allowing it to benefit from future advancements in each individual agent. </p>
<blockquote>
<p>åœ¨å›¾åƒæ¡ä»¶å›¾åƒç”Ÿæˆæ–¹é¢ï¼Œæœ€è¿‘çš„è¿›å±•å·²ç»å–å¾—äº†å®è´¨æ€§çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œå‰æ™¯æ¡ä»¶å›¾åƒç”Ÿæˆä»ç„¶è¢«è¾ƒå°‘æ¢ç´¢ï¼Œé¢ä¸´ç€å¯¹è±¡å®Œæ•´æ€§å—æŸã€å‰æ™¯èƒŒæ™¯ä¸ä¸€è‡´ã€å¤šæ ·æ€§æœ‰é™å’Œæ§åˆ¶çµæ´»æ€§é™ä½ç­‰æŒ‘æˆ˜ã€‚è¿™äº›æŒ‘æˆ˜æºäºå½“å‰ç«¯åˆ°ç«¯çš„è¡¥å…¨æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹å—åˆ°è®­ç»ƒæ©æ¨¡ä¸å‡†ç¡®ã€å‰æ™¯è¯­ä¹‰ç†è§£æœ‰é™ã€æ•°æ®åˆ†å¸ƒåè§ä»¥åŠè§†è§‰å’Œæ–‡å­—æç¤ºä¹‹é—´å›ºæœ‰å¹²æ‰°çš„å½±å“ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Anywhereï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œå®ƒåç¦»äº†ä¼ ç»Ÿçš„ç«¯åˆ°ç«¯æ–¹æ³•ã€‚åœ¨æ­¤æ¡†æ¶ä¸­ï¼Œæ¯ä¸ªæ™ºèƒ½ä½“éƒ½ä¸“é—¨è´Ÿè´£ä¸€ä¸ªç‰¹å®šçš„æ–¹é¢ï¼Œå¦‚å‰æ™¯ç†è§£ã€å¤šæ ·æ€§å¢å¼ºã€å¯¹è±¡å®Œæ•´æ€§ä¿æŠ¤å’Œæ–‡æœ¬æç¤ºä¸€è‡´æ€§ç­‰ã€‚æˆ‘ä»¬çš„æ¡†æ¶è¿˜å¢å¼ºäº†åŠ å…¥å¯é€‰ç”¨æˆ·æ–‡æœ¬è¾“å…¥ã€æ‰§è¡Œè‡ªåŠ¨åŒ–è´¨é‡è¯„ä¼°å’Œæ ¹æ®éœ€è¦å¯åŠ¨é‡æ–°ç”Ÿæˆçš„èƒ½åŠ›ã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼Œè¿™ç§æ¨¡å—åŒ–è®¾è®¡æœ‰æ•ˆåœ°å…‹æœäº†ç°æœ‰ç«¯åˆ°ç«¯æ¨¡å‹çš„å±€é™æ€§ï¼Œåœ¨å‰æ™¯æ¡ä»¶å›¾åƒç”Ÿæˆæ–¹é¢å®ç°äº†æ›´é«˜çš„ä¿çœŸåº¦ã€è´¨é‡ã€å¤šæ ·æ€§å’Œå¯æ§æ€§ã€‚æ­¤å¤–ï¼ŒAnywhereæ¡†æ¶å…·æœ‰å¯æ‰©å±•æ€§ï¼Œå¯ä»¥ä»æ¯ä¸ªç‹¬ç«‹æ™ºèƒ½ä½“çš„æœªæ¥è¿›æ­¥ä¸­å—ç›Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.18598v2">PDF</a> 18 pages, 15 figures, project page:   <a target="_blank" rel="noopener" href="https://anywheremultiagent.github.io/">https://anywheremultiagent.github.io</a>, Accepted at AAAI 2025</p>
<p><strong>æ€»ç»“</strong></p>
<p>è¿‘æœŸï¼Œå›¾åƒæ¡ä»¶å›¾åƒç”Ÿæˆé¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å‰æ™¯æ¡ä»¶å›¾åƒç”Ÿæˆä»ç„¶è¢«è¾ƒå°‘æ¢ç´¢ã€‚é¢ä¸´å¯¹è±¡å®Œæ•´æ€§å—æŸã€å‰æ™¯èƒŒæ™¯ä¸ä¸€è‡´ã€å¤šæ ·æ€§æœ‰é™å’Œæ§åˆ¶çµæ´»æ€§é™ä½ç­‰æŒ‘æˆ˜ã€‚è¿™äº›æŒ‘æˆ˜æºäºå½“å‰ç«¯åˆ°ç«¯çš„å¡«å……æ¨¡å‹ï¼Œå­˜åœ¨è®­ç»ƒæ©æ¨¡ä¸å‡†ç¡®ã€å‰æ™¯è¯­ä¹‰ç†è§£æœ‰é™ã€æ•°æ®åˆ†å¸ƒåè§ä»¥åŠè§†è§‰å’Œæ–‡å­—æç¤ºä¹‹é—´çš„å›ºæœ‰å¹²æ‰°ç­‰é—®é¢˜ã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†Anywhereå¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œå®ƒçªç ´äº†ä¼ ç»Ÿçš„ç«¯åˆ°ç«¯æ–¹æ³•ã€‚åœ¨æ­¤æ¡†æ¶ä¸­ï¼Œæ¯ä¸ªæ™ºèƒ½ä½“éƒ½ä¸“é—¨è´Ÿè´£ä¸€ä¸ªç‰¹å®šçš„æ–¹é¢ï¼Œå¦‚å‰æ™¯ç†è§£ã€å¤šæ ·æ€§å¢å¼ºã€å¯¹è±¡å®Œæ•´æ€§ä¿æŠ¤å’Œæ–‡æœ¬æç¤ºä¸€è‡´æ€§ç­‰ã€‚æˆ‘ä»¬çš„æ¡†æ¶è¿˜å¢å¼ºäº†åŠ å…¥ç”¨æˆ·æ–‡æœ¬è¾“å…¥ã€è¿›è¡Œè‡ªåŠ¨è´¨é‡è¯„ä¼°å’Œéœ€è¦æ—¶å¯åŠ¨é‡æ–°ç”Ÿæˆçš„èƒ½åŠ›ã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼Œè¿™ç§æ¨¡å—åŒ–è®¾è®¡æœ‰æ•ˆåœ°å…‹æœäº†ç°æœ‰ç«¯åˆ°ç«¯æ¨¡å‹çš„å±€é™æ€§ï¼Œåœ¨å‰æ™¯æ¡ä»¶å›¾åƒç”Ÿæˆä¸­å®ç°äº†æ›´é«˜çš„ä¿çœŸåº¦ã€è´¨é‡ã€å¤šæ ·æ€§å’Œå¯æ§æ€§ã€‚æ­¤å¤–ï¼ŒAnywhereæ¡†æ¶å…·æœ‰å¯æ‰©å±•æ€§ï¼Œå¯ä»¥ä»å„ä¸ªç‹¬ç«‹æ™ºèƒ½ä½“çš„æœªæ¥è¿›æ­¥ä¸­å—ç›Šã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å‰æ™¯æ¡ä»¶å›¾åƒç”Ÿæˆé¢ä¸´æŒ‘æˆ˜ï¼Œå¦‚å¯¹è±¡å®Œæ•´æ€§å—æŸå’Œå‰æ™¯èƒŒæ™¯ä¸ä¸€è‡´ã€‚</li>
<li>å½“å‰ç«¯åˆ°ç«¯æ¨¡å‹å­˜åœ¨è®­ç»ƒæ©æ¨¡ä¸å‡†ç¡®ã€å‰æ™¯è¯­ä¹‰ç†è§£æœ‰é™ç­‰é—®é¢˜ã€‚</li>
<li>Anywhereå¤šæ™ºèƒ½ä½“æ¡†æ¶çªç ´ä¼ ç»Ÿæ–¹æ³•ï¼Œæ¯ä¸ªæ™ºèƒ½ä½“å¤„ç†ç‰¹å®šä»»åŠ¡ã€‚</li>
<li>æ¡†æ¶å…·å¤‡ç”¨æˆ·äº¤äº’ã€è‡ªåŠ¨è´¨é‡è¯„ä¼°å’Œå†ç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>æ¨¡å—åŒ–è®¾è®¡æœ‰æ•ˆå…‹æœç°æœ‰ç«¯åˆ°ç«¯æ¨¡å‹å±€é™æ€§ï¼Œæé«˜å›¾åƒç”Ÿæˆçš„ä¿çœŸåº¦ã€è´¨é‡ã€å¤šæ ·æ€§å’Œå¯æ§æ€§ã€‚</li>
<li>Anywhereæ¡†æ¶å…·æœ‰å¯æ‰©å±•æ€§ï¼Œå¯é€‚åº”æœªæ¥æŠ€æœ¯è¿›æ­¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.18598">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c49a98e0690e573df63d40078e6a9eff.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7f95ed592ea08cd455a5a6826f4883b4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1a9f60d36aedf77ab7334ba144579425.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d260be3b72cc45b997c21727ac450a03.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f3d6ae1016be2495b39d832f472a9d4.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Self-Confirming-Transformer-for-Belief-Conditioned-Adaptation-in-Offline-Multi-Agent-Reinforcement-Learning"><a href="#Self-Confirming-Transformer-for-Belief-Conditioned-Adaptation-in-Offline-Multi-Agent-Reinforcement-Learning" class="headerlink" title="Self-Confirming Transformer for Belief-Conditioned Adaptation in Offline   Multi-Agent Reinforcement Learning"></a>Self-Confirming Transformer for Belief-Conditioned Adaptation in Offline   Multi-Agent Reinforcement Learning</h2><p><strong>Authors:Tao Li, Juan Guevara, Xinhong Xie, Quanyan Zhu</strong></p>
<p>Offline reinforcement learning (RL) suffers from the distribution shift between the offline dataset and the online environment. In multi-agent RL (MARL), this distribution shift may arise from the nonstationary opponents in the online testing who display distinct behaviors from those recorded in the offline dataset. Hence, the key to the broader deployment of offline MARL is the online adaptation to nonstationary opponents. Recent advances in foundation models, e.g., large language models, have demonstrated the generalization ability of the transformer, an emerging neural network architecture, in sequence modeling, of which offline RL is a special case. One naturally wonders \textit{whether offline-trained transformer-based RL policies adapt to nonstationary opponents online}. We propose a novel auto-regressive training to equip transformer agents with online adaptability based on the idea of self-augmented pre-conditioning. The transformer agent first learns offline to predict the opponentâ€™s action based on past observations. When deployed online, such a fictitious opponent play, referred to as the belief, is fed back to the transformer, together with other environmental feedback, to generate future actions conditional on the belief. Motivated by self-confirming equilibrium in game theory, the training loss consists of belief consistency loss, requiring the beliefs to match the opponentâ€™s actual actions and best response loss, mandating the agent to behave optimally under the belief. We evaluate the online adaptability of the proposed self-confirming transformer (SCT) in a structured environment, iterated prisonerâ€™s dilemma games, to demonstrate SCTâ€™s belief consistency and equilibrium behaviors as well as more involved multi-particle environments to showcase its superior performance against nonstationary opponents over prior transformers and offline MARL baselines. </p>
<blockquote>
<p>ç¦»çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é¢ä¸´ç€ç¦»çº¿æ•°æ®é›†å’Œåœ¨çº¿ç¯å¢ƒä¹‹é—´çš„åˆ†å¸ƒåç§»é—®é¢˜ã€‚åœ¨å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰ä¸­ï¼Œè¿™ç§åˆ†å¸ƒåç§»å¯èƒ½æºäºåœ¨çº¿æµ‹è¯•ä¸­éç¨³å®šå¯¹æ‰‹çš„å‡ºç°ï¼Œè¿™äº›å¯¹æ‰‹çš„è¡Œä¸ºä¸ç¦»çº¿æ•°æ®é›†ä¸­è®°å½•çš„è¡Œä¸ºæˆªç„¶ä¸åŒã€‚å› æ­¤ï¼Œç¦»çº¿MARLæ›´å¹¿æ³›éƒ¨ç½²çš„å…³é”®åœ¨äºå¯¹åœ¨çº¿éç¨³å®šå¯¹æ‰‹çš„é€‚åº”ã€‚æœ€è¿‘çš„è¿›å±•ï¼Œå¦‚åŸºç¡€æ¨¡å‹ä¸­çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå±•ç¤ºäº†transformerçš„æ³›åŒ–èƒ½åŠ›ã€‚Transformeræ˜¯ä¸€ç§æ–°å…´ç¥ç»ç½‘ç»œæ¶æ„ï¼Œåœ¨åºåˆ—å»ºæ¨¡ä¸­å…·æœ‰ç‰¹æ®Šçš„åº”ç”¨åœºæ™¯ï¼Œå…¶ä¸­åŒ…æ‹¬ç¦»çº¿RLã€‚äººä»¬è‡ªç„¶ä¼šå¥½å¥‡çš„æ˜¯ï¼Œâ€œåŸºäºç¦»çº¿è®­ç»ƒçš„transformerçš„RLç­–ç•¥æ˜¯å¦é€‚åº”åœ¨çº¿çš„éç¨³å®šå¯¹æ‰‹â€ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„è‡ªåŠ¨å›å½’è®­ç»ƒï¼ŒåŸºäºè‡ªæˆ‘å¢å¼ºé¢„å¤„ç†çš„ç†å¿µï¼Œä¸ºtransformeræ™ºèƒ½ä½“æä¾›åœ¨çº¿é€‚åº”æ€§ã€‚Transformeræ™ºèƒ½ä½“é¦–å…ˆå­¦ä¹ åŸºäºè¿‡å»çš„è§‚å¯Ÿé¢„æµ‹å¯¹æ‰‹çš„è¡ŒåŠ¨ã€‚å½“åœ¨çº¿éƒ¨ç½²æ—¶ï¼Œè¿™ç§è™šæ„çš„å¯¹æ‰‹æ¸¸æˆï¼ˆç§°ä¸ºä¿¡å¿µï¼‰ä¼šåé¦ˆç»™transformerï¼Œä»¥åŠå…¶ä»–ç¯å¢ƒåé¦ˆä¸€èµ·ï¼Œæ ¹æ®ä¿¡å¿µç”Ÿæˆæœªæ¥çš„è¡ŒåŠ¨ã€‚å—åšå¼ˆè®ºä¸­çš„è‡ªæˆ‘ç¡®è®¤å‡è¡¡çš„å¯å‘ï¼Œè®­ç»ƒæŸå¤±åŒ…æ‹¬ä¿¡å¿µä¸€è‡´æ€§æŸå¤±ï¼Œè¦æ±‚ä¿¡å¿µä¸å¯¹æ‰‹çš„å®é™…è¡ŒåŠ¨ç›¸åŒ¹é…ï¼Œä»¥åŠæœ€ä½³å“åº”æŸå¤±ï¼Œè¦æ±‚æ™ºèƒ½ä½“åœ¨ä¿¡å¿µä¸‹ä»¥æœ€ä¼˜æ–¹å¼è¡¨ç°ã€‚æˆ‘ä»¬åœ¨ç»“æ„åŒ–ç¯å¢ƒä¸­è¯„ä¼°æ‰€æå‡ºçš„è‡ªæˆ‘ç¡®è®¤å˜å‹å™¨ï¼ˆSCTï¼‰çš„åœ¨çº¿é€‚åº”æ€§ï¼ŒåŒ…æ‹¬é‡å¤çš„å›šå¾’å›°å¢ƒæ¸¸æˆï¼Œä»¥å±•ç¤ºSCTçš„ä¿¡å¿µä¸€è‡´æ€§ã€å‡è¡¡è¡Œä¸ºä»¥åŠåœ¨é¢å¯¹éç¨³å®šå¯¹æ‰‹æ—¶ä¼˜äºå…ˆå‰transformerå’Œç¦»çº¿MARLåŸºå‡†æµ‹è¯•çš„è¡¨ç°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04579v2">PDF</a> </p>
<p><strong>Summary</strong><br>     ç¦»çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é¢ä¸´ç¦»çº¿æ•°æ®é›†å’Œåœ¨çº¿ç¯å¢ƒä¹‹é—´çš„åˆ†å¸ƒåç§»é—®é¢˜ã€‚åœ¨å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰ä¸­ï¼Œè¿™ç§åˆ†å¸ƒåç§»å¯èƒ½æºäºåœ¨çº¿æµ‹è¯•ä¸­çš„éå¹³ç¨³å¯¹æ‰‹ï¼Œä»–ä»¬çš„è¡Œä¸ºä¸ç¦»çº¿æ•°æ®é›†è®°å½•çš„è¡Œä¸ºæˆªç„¶ä¸åŒã€‚å› æ­¤ï¼Œç¦»çº¿MARLå¹¿æ³›éƒ¨ç½²çš„å…³é”®åœ¨äºåœ¨çº¿é€‚åº”éå¹³ç¨³å¯¹æ‰‹çš„èƒ½åŠ›ã€‚æœ€è¿‘ï¼ŒåŸºç¡€æ¨¡å‹ï¼ˆå¦‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼‰ä¸­çš„è¿›å±•å±•ç¤ºäº†transformeråœ¨åºåˆ—å»ºæ¨¡ä¸­çš„æ³›åŒ–èƒ½åŠ›ï¼Œè€Œç¦»çº¿RLæ˜¯å…¶ä¸­çš„ç‰¹ä¾‹ã€‚æœ¬æ–‡æå‡ºäº†åŸºäºè‡ªå¢é¢„å¤„ç†çš„è‡ªåŠ¨å›å½’è®­ç»ƒï¼Œä¸ºtransformeræ™ºèƒ½ä½“é…å¤‡åœ¨çº¿é€‚åº”æ€§ã€‚è¯¥è®­ç»ƒæŸå¤±åŒ…æ‹¬ä¿¡å¿µä¸€è‡´æ€§æŸå¤±å’Œæœ€ä½³å“åº”æŸå¤±ï¼Œè¦æ±‚æ™ºèƒ½ä½“åœ¨ä¿¡å¿µä¸‹è¡¨ç°å‡ºæœ€ä¼˜è¡Œä¸ºã€‚æˆ‘ä»¬åœ¨ç»“æ„åŒ–ç¯å¢ƒä¸­è¯„ä¼°äº†æ‰€æå‡ºçš„è‡ªç¡®è®¤transformerï¼ˆSCTï¼‰çš„åœ¨çº¿é€‚åº”æ€§ï¼Œä»¥å±•ç¤ºå…¶åœ¨é¢å¯¹éå¹³ç¨³å¯¹æ‰‹æ—¶çš„ä¼˜è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¦»çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰é¢ä¸´åœ¨çº¿ç¯å¢ƒä¸ç¦»çº¿æ•°æ®é›†ä¹‹é—´çš„åˆ†å¸ƒåç§»é—®é¢˜ã€‚</li>
<li>éå¹³ç¨³å¯¹æ‰‹åœ¨åœ¨çº¿æµ‹è¯•ä¸­çš„è¡Œä¸ºå¯èƒ½ä¸ç¦»çº¿æ•°æ®é›†è®°å½•çš„è¡Œä¸ºæ˜¾è‘—ä¸åŒã€‚</li>
<li>åœ¨çº¿é€‚åº”éå¹³ç¨³å¯¹æ‰‹æ˜¯ç¦»çº¿MARLå¹¿æ³›éƒ¨ç½²çš„å…³é”®ã€‚</li>
<li>åŸºç¡€æ¨¡å‹çš„è¿›å±•ï¼Œç‰¹åˆ«æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„transformerï¼Œå±•ç¤ºäº†å…¶åœ¨åºåˆ—å»ºæ¨¡ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†åŸºäºè‡ªå¢é¢„å¤„ç†çš„è‡ªåŠ¨å›å½’è®­ç»ƒï¼Œä»¥æé«˜transformeræ™ºèƒ½ä½“çš„åœ¨çº¿é€‚åº”æ€§ã€‚</li>
<li>è®­ç»ƒæŸå¤±åŒ…æ‹¬ä¿¡å¿µä¸€è‡´æ€§æŸå¤±å’Œæœ€ä½³å“åº”æŸå¤±ï¼Œä»¥ç¡®ä¿æ™ºèƒ½ä½“åœ¨ä¿¡å¿µä¸‹è¡¨ç°å‡ºæœ€ä¼˜è¡Œä¸ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2310.04579">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ffeac17dfbd0894d12b91c608abfa61a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-954a343681f8762b4a2eebb802238d6c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ad597789e9ad45000583463d6f458346.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-408e1499c6bbf9f72f0d0bf496110acb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7948e438ff2acd9f68ea46e537fa2be2.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-26/Agent/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-26/Agent/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Agent/">
                                    <span class="chip bg-color">Agent</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-26/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-1ae727e6933c739cea874e5a04534456.jpg" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-26  Large Language Models are Powerful EHR Encoders
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-26
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-26/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-1dad80a4c1a092eeeaa1fdaccf907497.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-26  Introducing Visual Perception Token into Multimodal Large Language Model
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-26
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32102k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
