<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-26  GCC Generative Color Constancy via Diffusing a Color Checker">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-728e0cefbefe5b2e1745f9050740922c.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-26
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    82 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-02-26-æ›´æ–°"><a href="#2025-02-26-æ›´æ–°" class="headerlink" title="2025-02-26 æ›´æ–°"></a>2025-02-26 æ›´æ–°</h1><h2 id="GCC-Generative-Color-Constancy-via-Diffusing-a-Color-Checker"><a href="#GCC-Generative-Color-Constancy-via-Diffusing-a-Color-Checker" class="headerlink" title="GCC: Generative Color Constancy via Diffusing a Color Checker"></a>GCC: Generative Color Constancy via Diffusing a Color Checker</h2><p><strong>Authors:Chen-Wei Chang, Cheng-De Fan, Chia-Che Chang, Yi-Chen Lo, Yu-Chee Tseng, Jiun-Long Huang, Yu-Lun Liu</strong></p>
<p>Color constancy methods often struggle to generalize across different camera sensors due to varying spectral sensitivities. We present GCC, which leverages diffusion models to inpaint color checkers into images for illumination estimation. Our key innovations include (1) a single-step deterministic inference approach that inpaints color checkers reflecting scene illumination, (2) a Laplacian decomposition technique that preserves checker structure while allowing illumination-dependent color adaptation, and (3) a mask-based data augmentation strategy for handling imprecise color checker annotations. GCC demonstrates superior robustness in cross-camera scenarios, achieving state-of-the-art worst-25% error rates of 5.15{\deg} and 4.32{\deg} in bi-directional evaluations. These results highlight our methodâ€™s stability and generalization capability across different camera characteristics without requiring sensor-specific training, making it a versatile solution for real-world applications. </p>
<blockquote>
<p>é¢œè‰²æ’å¸¸æ³•æ–¹æ³•å¾€å¾€ç”±äºä¸åŒçš„å…‰è°±æ•æ„Ÿæ€§è€Œåœ¨ä¸åŒç›¸æœºä¼ æ„Ÿå™¨ä¸Šéš¾ä»¥æ¨å¹¿ã€‚æˆ‘ä»¬æå‡ºäº†GCCï¼Œå®ƒåˆ©ç”¨æ‰©æ•£æ¨¡å‹å°†é¢œè‰²æ¡å¡«å……åˆ°å›¾åƒä¸­è¿›è¡Œå…‰ç…§ä¼°è®¡ã€‚æˆ‘ä»¬çš„å…³é”®åˆ›æ–°åŒ…æ‹¬ï¼š(1) å•æ­¥ç¡®å®šæ€§æ¨ç†æ–¹æ³•ï¼Œç”¨äºå¡«å……åæ˜ åœºæ™¯å…‰ç…§çš„é¢œè‰²æ¡ï¼›(2) æ‹‰æ™®æ‹‰æ–¯åˆ†è§£æŠ€æœ¯ï¼Œèƒ½å¤Ÿåœ¨ä¿ç•™æ£‹ç›˜ç»“æ„çš„åŒæ—¶å®ç°å…‰ç…§ç›¸å…³çš„é¢œè‰²é€‚åº”ï¼›(3) åŸºäºæ©ç çš„å¢å¹¿ç­–ç•¥ï¼Œç”¨äºå¤„ç†ä¸ç²¾ç¡®çš„é¢œè‰²æ¡æ ‡æ³¨ã€‚GCCåœ¨è·¨ç›¸æœºåœºæ™¯ä¸­è¡¨ç°å‡ºå“è¶Šçš„é²æ£’æ€§ï¼Œåœ¨åŒå‘è¯„ä¼°ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æœ€ä½25%é”™è¯¯ç‡ä¸º5.15Â°å’Œ4.32Â°ï¼Œè¿™äº›ç»“æœå‡¸æ˜¾äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸éœ€è¦é’ˆå¯¹ä¼ æ„Ÿå™¨è¿›è¡Œç‰¹å®šè®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œåœ¨ä¸åŒç›¸æœºç‰¹æ€§ä¸Šçš„ç¨³å®šæ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œä½¿å…¶æˆä¸ºç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„é€šç”¨è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17435v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://chenwei891213.github.io/GCC/">https://chenwei891213.github.io/GCC/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†GCCæ–¹æ³•ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹å¯¹å›¾åƒä¸­çš„é¢œè‰²æ£€æŸ¥å™¨è¿›è¡Œä¿®å¤ä»¥è¿›è¡Œç…§æ˜ä¼°è®¡ã€‚ä¸»è¦åˆ›æ–°åŒ…æ‹¬å•æ­¥ç¡®å®šæ€§æ¨ç†æ–¹æ³•ã€æ‹‰æ™®æ‹‰æ–¯åˆ†è§£æŠ€æœ¯å’ŒåŸºäºæ©è†œçš„å¢å¹¿ç­–ç•¥ã€‚GCCåœ¨è·¨ç›¸æœºåœºæ™¯ä¸­è¡¨ç°å‡ºå‡ºè‰²çš„é²æ£’æ€§ï¼Œåœ¨åŒå‘è¯„ä¼°ä¸­å®ç°äº†æœ€ä½25%çš„é”™è¯¯ç‡ä¸º5.15Â°å’Œ4.32Â°ï¼Œå±•ç¤ºäº†å…¶ç¨³å®šæ€§å’Œåœ¨ä¸åŒç›¸æœºç‰¹æ€§ä¸‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œæ— éœ€é’ˆå¯¹ä¼ æ„Ÿå™¨è¿›è¡Œç‰¹å®šè®­ç»ƒï¼Œä½¿å…¶æˆä¸ºçœŸå®ä¸–ç•Œåº”ç”¨çš„é€šç”¨è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GCCåˆ©ç”¨æ‰©æ•£æ¨¡å‹ä¿®å¤å›¾åƒä¸­çš„é¢œè‰²æ£€æŸ¥å™¨ä»¥è¿›è¡Œç…§æ˜ä¼°è®¡ã€‚</li>
<li>é‡‡ç”¨å•æ­¥ç¡®å®šæ€§æ¨ç†æ–¹æ³•ä¿®å¤åæ˜ åœºæ™¯ç…§æ˜çš„é¢œè‰²æ£€æŸ¥å™¨ã€‚</li>
<li>é‡‡ç”¨æ‹‰æ™®æ‹‰æ–¯åˆ†è§£æŠ€æœ¯ä¿ç•™äº†é¢œè‰²æ£€æŸ¥å™¨çš„ç»“æ„ï¼ŒåŒæ—¶å®ç°äº†å…‰ç…§ä¾èµ–çš„è‰²å½©é€‚åº”ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºæ©è†œçš„æ•°æ®å¢å¼ºç­–ç•¥æ¥å¤„ç†ä¸ç²¾ç¡®çš„é¢œè‰²æ£€æŸ¥å™¨æ³¨é‡Šã€‚</li>
<li>GCCåœ¨è·¨ç›¸æœºåœºæ™¯ä¸­è¡¨ç°å‡ºå‡ºè‰²çš„é²æ£’æ€§ã€‚</li>
<li>åœ¨åŒå‘è¯„ä¼°ä¸­ï¼ŒGCCå®ç°äº†æœ€ä½25%çš„é”™è¯¯ç‡ä¸º5.15Â°å’Œ4.32Â°ï¼Œå±•ç°äº†å…¶ä¼˜ç§€æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17435">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a8ef5f606ac96b55d6a8df35dbda6982.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-62fdcb6de01b889d3b47d4dbde00a0f3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7b409f0100cba6a1ef8c19eed16660d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c782312367eb57f5732a46a104b1ae67.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="X-Dancer-Expressive-Music-to-Human-Dance-Video-Generation"><a href="#X-Dancer-Expressive-Music-to-Human-Dance-Video-Generation" class="headerlink" title="X-Dancer: Expressive Music to Human Dance Video Generation"></a>X-Dancer: Expressive Music to Human Dance Video Generation</h2><p><strong>Authors:Zeyuan Chen, Hongyi Xu, Guoxian Song, You Xie, Chenxu Zhang, Xin Chen, Chao Wang, Di Chang, Linjie Luo</strong></p>
<p>We present X-Dancer, a novel zero-shot music-driven image animation pipeline that creates diverse and long-range lifelike human dance videos from a single static image. As its core, we introduce a unified transformer-diffusion framework, featuring an autoregressive transformer model that synthesize extended and music-synchronized token sequences for 2D body, head and hands poses, which then guide a diffusion model to produce coherent and realistic dance video frames. Unlike traditional methods that primarily generate human motion in 3D, X-Dancer addresses data limitations and enhances scalability by modeling a wide spectrum of 2D dance motions, capturing their nuanced alignment with musical beats through readily available monocular videos. To achieve this, we first build a spatially compositional token representation from 2D human pose labels associated with keypoint confidences, encoding both large articulated body movements (e.g., upper and lower body) and fine-grained motions (e.g., head and hands). We then design a music-to-motion transformer model that autoregressively generates music-aligned dance pose token sequences, incorporating global attention to both musical style and prior motion context. Finally we leverage a diffusion backbone to animate the reference image with these synthesized pose tokens through AdaIN, forming a fully differentiable end-to-end framework. Experimental results demonstrate that X-Dancer is able to produce both diverse and characterized dance videos, substantially outperforming state-of-the-art methods in term of diversity, expressiveness and realism. Code and model will be available for research purposes. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†X-Dancerï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹é›¶æ ·æœ¬éŸ³ä¹é©±åŠ¨å›¾åƒåŠ¨ç”»ç®¡é“ï¼Œå®ƒå¯ä»¥ä»å•ä¸ªé™æ€å›¾åƒä¸­åˆ›å»ºå¤šæ ·ä¸”é•¿ç¨‹çš„é€¼çœŸäººç±»èˆè¹ˆè§†é¢‘ã€‚å…¶æ ¸å¿ƒæ˜¯ç»Ÿä¸€transformeræ‰©æ•£æ¡†æ¶ï¼Œå®ƒåŒ…å«ä¸€ä¸ªè‡ªå›å½’transformeræ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿåˆæˆæ‰©å±•çš„å¹¶ä¸éŸ³ä¹åŒæ­¥çš„ä»¤ç‰Œåºåˆ—ï¼Œç”¨äºè¡¨ç¤º2Dèº«ä½“ã€å¤´éƒ¨å’Œæ‰‹éƒ¨å§¿åŠ¿ã€‚ç„¶åè¿™äº›ä»¤ç‰Œåºåˆ—å¼•å¯¼æ‰©æ•£æ¨¡å‹ç”Ÿæˆè¿è´¯ä¸”é€¼çœŸçš„èˆè¹ˆè§†é¢‘å¸§ã€‚ä¸ä¼ ç»Ÿçš„ä¸»è¦åœ¨3Dä¸­ç”Ÿæˆäººç±»è¿åŠ¨çš„æ–¹æ³•ä¸åŒï¼ŒX-Danceré€šè¿‡å»ºæ¨¡å¤§é‡çš„2Dèˆè¹ˆåŠ¨ä½œæ¥è§£å†³æ•°æ®é™åˆ¶å¹¶å¢å¼ºå¯æ‰©å±•æ€§ï¼Œå¹¶é€šè¿‡å¯ç”¨çš„å•ç›®è§†é¢‘æ•æ‰å…¶ä¸éŸ³ä¹èŠ‚å¥çš„å¾®å¦™å¯¹é½ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬é¦–å…ˆæ ¹æ®ä¸å…³é”®ç‚¹ç½®ä¿¡åº¦ç›¸å…³çš„2Däººç±»å§¿åŠ¿æ ‡ç­¾æ„å»ºç©ºé—´ç»„åˆä»¤ç‰Œè¡¨ç¤ºï¼Œç¼–ç å¤§å‹å…³èŠ‚èº«ä½“è¿åŠ¨ï¼ˆä¾‹å¦‚ï¼Œä¸ŠåŠèº«å’Œä¸‹åŠèº«ï¼‰å’Œç²¾ç»†è¿åŠ¨ï¼ˆä¾‹å¦‚ï¼Œå¤´éƒ¨å’Œæ‰‹éƒ¨ï¼‰ã€‚ç„¶åæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªéŸ³ä¹åˆ°è¿åŠ¨çš„transformeræ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯ä»¥è‡ªå›å½’åœ°ç”Ÿæˆä¸éŸ³ä¹å¯¹é½çš„èˆè¹ˆå§¿åŠ¿ä»¤ç‰Œåºåˆ—ï¼Œå¹¶é€šè¿‡å…¨å±€æ³¨æ„åŠ›æ¥å…³æ³¨éŸ³ä¹é£æ ¼å’Œå…ˆå‰çš„è¿åŠ¨ä¸Šä¸‹æ–‡ã€‚æœ€åï¼Œæˆ‘ä»¬åˆ©ç”¨æ‰©æ•£ä¸»å¹²å°†è¿™äº›åˆæˆçš„å§¿åŠ¿ä»¤ç‰Œé€šè¿‡AdaINæŠ€æœ¯ä½¿å‚è€ƒå›¾åƒåŠ¨èµ·æ¥ï¼Œå½¢æˆä¸€ä¸ªå®Œå…¨å¯å¾®åˆ†çš„ç«¯åˆ°ç«¯æ¡†æ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒX-Dancerèƒ½å¤Ÿäº§ç”Ÿå¤šæ ·ä¸”ç‰¹å¾é²œæ˜çš„èˆè¹ˆè§†é¢‘ï¼Œåœ¨å¤šæ ·æ€§ã€è¡¨ç°åŠ›å’Œé€¼çœŸåº¦æ–¹é¢å¤§å¤§ä¼˜äºç°æœ‰å…ˆè¿›æ–¹æ³•ã€‚ä»£ç å’Œæ¨¡å‹å°†ä¾›ç ”ç©¶ä½¿ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17414v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºç»™å®šçš„æ–‡æœ¬æ‘˜è¦ï¼ŒX-Danceræ˜¯ä¸€ä¸ªé›¶æ ·æœ¬éŸ³ä¹é©±åŠ¨å›¾åƒåŠ¨ç”»ç®¡é“ï¼Œèƒ½å¤Ÿä»å•ä¸€é™æ€å›¾åƒåˆ›å»ºå¤šæ ·ä¸”é•¿ç¨‹é€¼çœŸçš„èˆè¹ˆè§†é¢‘ã€‚å…¶æ ¸å¿ƒæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„è½¬æ¢å™¨æ‰©æ•£æ¡†æ¶ï¼ŒåŒ…å«ä¸€ä¸ªè‡ªå›å½’è½¬æ¢å™¨æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åˆæˆæ‰©å±•ä¸”ä¸éŸ³ä¹åŒæ­¥çš„ä»¤ç‰Œåºåˆ—ï¼Œç”¨äºæŒ‡å¯¼æ‰©æ•£æ¨¡å‹äº§ç”Ÿè¿è´¯ä¸”é€¼çœŸçš„èˆè¹ˆè§†é¢‘å¸§ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒX-Dancerä¸»è¦è§£å†³æ•°æ®å±€é™æ€§é—®é¢˜å¹¶é€šè¿‡æ¨¡æ‹Ÿå¤§é‡äºŒç»´èˆè¹ˆåŠ¨ä½œå¢å¼ºå…¶å¯æ‰©å±•æ€§ï¼Œä»è€Œæ•æ‰å…¶ä¸éŸ³ä¹èŠ‚å¥çš„å¾®å¦™å¯¹é½ã€‚ç›®å‰å®éªŒç»“æœè¯å®X-Dancerèƒ½å¤Ÿäº§ç”Ÿå¤šæ ·ä¸”å¯Œæœ‰ç‰¹è‰²çš„èˆè¹ˆè§†é¢‘ï¼Œåœ¨å¤šæ ·æ€§ã€è¡¨ç°åŠ›å’Œé€¼çœŸåº¦æ–¹é¢å¤§å¤§ä¼˜äºç°æœ‰å…ˆè¿›æŠ€æœ¯æ–¹æ³•ã€‚ä»£ç å’Œæ¨¡å‹å¯ä¾›ç ”ç©¶ä¹‹ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>X-Danceræ˜¯ä¸€ä¸ªé›¶æ ·æœ¬éŸ³ä¹é©±åŠ¨å›¾åƒåŠ¨ç”»ç®¡é“ï¼Œèƒ½å¤Ÿåˆ›å»ºå¤šæ ·ä¸”é€¼çœŸçš„èˆè¹ˆè§†é¢‘ã€‚</li>
<li>æ ¸å¿ƒæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„è½¬æ¢å™¨æ‰©æ•£æ¡†æ¶ï¼ŒåŒ…å«è‡ªå›å½’è½¬æ¢å™¨æ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹ã€‚</li>
<li>è‡ªå›å½’è½¬æ¢å™¨æ¨¡å‹åˆæˆä¸éŸ³ä¹åŒæ­¥çš„ä»¤ç‰Œåºåˆ—ï¼Œç”¨äºæŒ‡å¯¼ç”Ÿæˆè¿è´¯ä¸”é€¼çœŸçš„èˆè¹ˆè§†é¢‘å¸§ã€‚</li>
<li>X-Danceré€šè¿‡æ¨¡æ‹ŸäºŒç»´èˆè¹ˆåŠ¨ä½œè§£å†³æ•°æ®å±€é™æ€§é—®é¢˜å¹¶å¢å¼ºå¯æ‰©å±•æ€§ã€‚</li>
<li>å®ƒèƒ½å¤Ÿæ•æ‰èˆè¹ˆåŠ¨ä½œä¸éŸ³ä¹èŠ‚å¥çš„å¾®å¦™å¯¹é½ã€‚</li>
<li>X-Danceräº§ç”Ÿçš„èˆè¹ˆè§†é¢‘åœ¨å¤šæ ·æ€§ã€è¡¨ç°åŠ›å’Œé€¼çœŸåº¦æ–¹é¢ä¼˜äºç°æœ‰æŠ€æœ¯æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17414">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4c49224c45c5dfc4650f031e998a1aa1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d1e7c3f66ec4403475b21ac57858714d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1e7fcd8451d7d8e92d457657a02e76fa.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="DICEPTION-A-Generalist-Diffusion-Model-for-Visual-Perceptual-Tasks"><a href="#DICEPTION-A-Generalist-Diffusion-Model-for-Visual-Perceptual-Tasks" class="headerlink" title="DICEPTION: A Generalist Diffusion Model for Visual Perceptual Tasks"></a>DICEPTION: A Generalist Diffusion Model for Visual Perceptual Tasks</h2><p><strong>Authors:Canyu Zhao, Mingyu Liu, Huanyi Zheng, Muzhi Zhu, Zhiyue Zhao, Hao Chen, Tong He, Chunhua Shen</strong></p>
<p>Our primary goal here is to create a good, generalist perception model that can tackle multiple tasks, within limits on computational resources and training data. To achieve this, we resort to text-to-image diffusion models pre-trained on billions of images. Our exhaustive evaluation metrics demonstrate that DICEPTION effectively tackles multiple perception tasks, achieving performance on par with state-of-the-art models. We achieve results on par with SAM-vit-h using only 0.06% of their data (e.g., 600K vs. 1B pixel-level annotated images). Inspired by Wang et al., DICEPTION formulates the outputs of various perception tasks using color encoding; and we show that the strategy of assigning random colors to different instances is highly effective in both entity segmentation and semantic segmentation. Unifying various perception tasks as conditional image generation enables us to fully leverage pre-trained text-to-image models. Thus, DICEPTION can be efficiently trained at a cost of orders of magnitude lower, compared to conventional models that were trained from scratch. When adapting our model to other tasks, it only requires fine-tuning on as few as 50 images and 1% of its parameters. DICEPTION provides valuable insights and a more promising solution for visual generalist models. </p>
<blockquote>
<p>æˆ‘ä»¬çš„ä¸»è¦ç›®æ ‡æ˜¯åœ¨æœ‰é™çš„è®¡ç®—èµ„æºå’Œè®­ç»ƒæ•°æ®ä¸‹ï¼Œåˆ›å»ºä¸€ä¸ªèƒ½å¤Ÿå¤„ç†å¤šç§ä»»åŠ¡çš„ä¼˜ç§€é€šç”¨æ„ŸçŸ¥æ¨¡å‹ã€‚ä¸ºå®ç°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬é‡‡ç”¨åœ¨æ•°åäº¿å›¾åƒä¸Šé¢„å…ˆè®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ã€‚æˆ‘ä»¬çš„å…¨é¢è¯„ä¼°æŒ‡æ ‡è¡¨æ˜ï¼ŒDICEPTIONæœ‰æ•ˆåœ°è§£å†³äº†å¤šä¸ªæ„ŸçŸ¥ä»»åŠ¡ï¼Œå…¶æ€§èƒ½ä¸æœ€æ–°æ¨¡å‹ç›¸å½“ã€‚æˆ‘ä»¬ä½¿ç”¨ä»…ä¸ºå…¶æ•°æ®é‡çš„0.06%ï¼ˆä¾‹å¦‚ï¼Œ60ä¸‡å¯¹æ¯”1äº¿åƒç´ çº§æ³¨é‡Šå›¾åƒï¼‰å°±è¾¾åˆ°äº†ä¸SAM-vit-hç›¸å½“çš„ç»“æœã€‚DICEPTIONå—åˆ°Wangç­‰äººçš„å¯å‘ï¼Œé€šè¿‡é¢œè‰²ç¼–ç æ¥è¡¨è¿°å„ç§æ„ŸçŸ¥ä»»åŠ¡çš„è¾“å‡ºï¼›æˆ‘ä»¬è¯æ˜ï¼Œä¸ºä¸åŒå®ä¾‹åˆ†é…éšæœºé¢œè‰²çš„ç­–ç•¥åœ¨å®ä½“åˆ†å‰²å’Œè¯­ä¹‰åˆ†å‰²ä¸­éƒ½æä¸ºæœ‰æ•ˆã€‚å°†å„ç§æ„ŸçŸ¥ä»»åŠ¡ç»Ÿä¸€ä¸ºæ¡ä»¶å›¾åƒç”Ÿæˆï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿå……åˆ†åˆ©ç”¨é¢„å…ˆè®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ã€‚å› æ­¤ï¼Œä¸ä»å¤´å¼€å§‹è®­ç»ƒçš„å¸¸è§„æ¨¡å‹ç›¸æ¯”ï¼ŒDICEPTIONçš„è®­ç»ƒæˆæœ¬å¤§å¤§é™ä½ï¼Œæ•ˆç‡æé«˜ã€‚å½“å°†æˆ‘ä»¬çš„æ¨¡å‹é€‚åº”åˆ°å…¶ä»–ä»»åŠ¡æ—¶ï¼Œå®ƒåªéœ€è¦åœ¨å°‘é‡ï¼ˆä»…50å¼ å›¾åƒï¼‰ä¸Šè¿›è¡Œå¾®è°ƒï¼Œä¸”åªéœ€ä½¿ç”¨å…¶å‚æ•°çš„1%ã€‚DICEPTIONä¸ºè§†è§‰é€šç”¨æ¨¡å‹æä¾›äº†æœ‰ä»·å€¼çš„è§è§£å’Œæ›´æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17157v1">PDF</a> 29 pages, 20 figures</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æ—¨åœ¨åˆ›å»ºèƒ½å¤Ÿåœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½çš„é€šç”¨æ„ŸçŸ¥æ¨¡å‹ï¼Œé‡‡ç”¨æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹é¢„è®­ç»ƒäºå¤§é‡å›¾åƒæ•°æ®ã€‚é€šè¿‡å…¨é¢çš„è¯„ä¼°æŒ‡æ ‡ï¼Œæ˜¾ç¤ºDICEPTIONåœ¨å¤šä¸ªæ„ŸçŸ¥ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä¸æœ€æ–°æ¨¡å‹æ€§èƒ½ç›¸å½“ã€‚é€šè¿‡ä½¿ç”¨é¢œè‰²ç¼–ç ç­–ç•¥ï¼ŒDICEPTIONåœ¨å„ç§æ„ŸçŸ¥ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œé€šè¿‡éšæœºé¢œè‰²åŒºåˆ†ä¸åŒå®ä¾‹ã€‚é€šè¿‡å°†å„ç§æ„ŸçŸ¥ä»»åŠ¡ç»Ÿä¸€ä¸ºæ¡ä»¶å›¾åƒç”Ÿæˆï¼Œå……åˆ†åˆ©ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ï¼Œè®­ç»ƒæˆæœ¬å¤§å¹…é™ä½ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹é€‚åº”æ–°ä»»åŠ¡æ—¶ä»…éœ€å¾®è°ƒå°‘é‡å›¾åƒå’Œå‚æ•°ã€‚DICEPTIONä¸ºè§†è§‰é€šç”¨æ¨¡å‹æä¾›äº†æœ‰ä»·å€¼çš„è§è§£å’Œæ›´æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç›®æ ‡åˆ›å»ºèƒ½å¤Ÿåœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½çš„é€šç”¨æ„ŸçŸ¥æ¨¡å‹ï¼Œå—åˆ°è®¡ç®—èµ„æºå’Œè®­ç»ƒæ•°æ®çš„é™åˆ¶ã€‚</li>
<li>é‡‡ç”¨æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒï¼Œæ¶‰åŠå¤§é‡å›¾åƒæ•°æ®ã€‚</li>
<li>DICEPTIONé€šè¿‡å¤šç§æ„ŸçŸ¥ä»»åŠ¡è¡¨ç°å‡ºè‰²ï¼Œä¸æœ€æ–°æ¨¡å‹æ€§èƒ½ç›¸å½“ã€‚</li>
<li>DICEPTIONåˆ©ç”¨é¢œè‰²ç¼–ç ç­–ç•¥æ¥åŒºåˆ†ä¸åŒçš„å®ä¾‹ï¼Œè¿™ä¸€ç­–ç•¥åœ¨å®ä½“åˆ†å‰²å’Œè¯­ä¹‰åˆ†å‰²ä¸­éƒ½æœ‰æ•ˆã€‚</li>
<li>é€šè¿‡å°†æ„ŸçŸ¥ä»»åŠ¡ç»Ÿä¸€ä¸ºæ¡ä»¶å›¾åƒç”Ÿæˆï¼Œå……åˆ†åˆ©ç”¨é¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„ä¼˜åŠ¿ã€‚</li>
<li>è®­ç»ƒæˆæœ¬å¤§å¹…é™ä½ï¼Œä¸ä¼ ç»Ÿä»å¤´å¼€å§‹è®­ç»ƒçš„æ¨¡å‹ç›¸æ¯”æ•ˆæœæ˜¾è‘—ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17157">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b569392874f64f66618517c7dab0c5a2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bfecab689ff93127e3025f23f72140e5.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Diffusion-Models-for-Tabular-Data-Challenges-Current-Progress-and-Future-Directions"><a href="#Diffusion-Models-for-Tabular-Data-Challenges-Current-Progress-and-Future-Directions" class="headerlink" title="Diffusion Models for Tabular Data: Challenges, Current Progress, and   Future Directions"></a>Diffusion Models for Tabular Data: Challenges, Current Progress, and   Future Directions</h2><p><strong>Authors:Zhong Li, Qi Huang, Lincen Yang, Jiayang Shi, Zhao Yang, Niki van Stein, Thomas BÃ¤ck, Matthijs van Leeuwen</strong></p>
<p>In recent years, generative models have achieved remarkable performance across diverse applications, including image generation, text synthesis, audio creation, video generation, and data augmentation. Diffusion models have emerged as superior alternatives to Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs) by addressing their limitations, such as training instability, mode collapse, and poor representation of multimodal distributions. This success has spurred widespread research interest. In the domain of tabular data, diffusion models have begun to showcase similar advantages over GANs and VAEs, achieving significant performance breakthroughs and demonstrating their potential for addressing unique challenges in tabular data modeling. However, while domains like images and time series have numerous surveys summarizing advancements in diffusion models, there remains a notable gap in the literature for tabular data. Despite the increasing interest in diffusion models for tabular data, there has been little effort to systematically review and summarize these developments. This lack of a dedicated survey limits a clear understanding of the challenges, progress, and future directions in this critical area. This survey addresses this gap by providing a comprehensive review of diffusion models for tabular data. Covering works from June 2015, when diffusion models emerged, to December 2024, we analyze nearly all relevant studies, with updates maintained in a \href{<a target="_blank" rel="noopener" href="https://github.com/Diffusion-Model-Leiden/awesome-diffusion-models-for-tabular-data%7D%7BGitHub">https://github.com/Diffusion-Model-Leiden/awesome-diffusion-models-for-tabular-data}{GitHub</a> repository}. Assuming readers possess foundational knowledge of statistics and diffusion models, we employ mathematical formulations to deliver a rigorous and detailed review, aiming to promote developments in this emerging and exciting area. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œç”Ÿæˆæ¨¡å‹åœ¨å¤šç§åº”ç”¨ä¸­éƒ½å–å¾—äº†æ˜¾è‘—çš„æˆæ•ˆï¼ŒåŒ…æ‹¬å›¾åƒç”Ÿæˆã€æ–‡æœ¬åˆæˆã€éŸ³é¢‘åˆ›ä½œã€è§†é¢‘ç”Ÿæˆå’Œæ•°æ®å¢å¼ºã€‚æ‰©æ•£æ¨¡å‹é€šè¿‡è§£å†³ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å’Œå˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEsï¼‰å­˜åœ¨çš„è®­ç»ƒä¸ç¨³å®šã€æ¨¡å¼å´©æºƒä»¥åŠå¤šæ¨¡æ€åˆ†å¸ƒè¡¨ç¤ºä¸ä½³ç­‰å±€é™æ€§ï¼Œæˆä¸ºå…¶ä¼˜ç§€æ›¿ä»£å“ã€‚è¿™ä¸€æˆåŠŸå¼•å‘äº†å¹¿æ³›çš„ç ”ç©¶å…´è¶£ã€‚åœ¨è¡¨æ ¼æ•°æ®é¢†åŸŸï¼Œæ‰©æ•£æ¨¡å‹å¼€å§‹æ˜¾ç¤ºå‡ºå¯¹GANså’ŒVAEsçš„ç±»ä¼¼ä¼˜åŠ¿ï¼Œåœ¨æ€§èƒ½ä¸Šå–å¾—äº†é‡å¤§çªç ´ï¼Œå¹¶æ˜¾ç¤ºå‡ºè§£å†³è¡¨æ ¼æ•°æ®å»ºæ¨¡ä¸­çš„ç‹¬ç‰¹æŒ‘æˆ˜çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå°½ç®¡å›¾åƒå’Œæ—¶é—´åºåˆ—ç­‰é¢†åŸŸæœ‰è®¸å¤šå…³äºæ‰©æ•£æ¨¡å‹çš„è¿›å±•è°ƒæŸ¥ï¼Œä½†è¡¨æ ¼æ•°æ®çš„æ–‡çŒ®ä¸­ä»å­˜åœ¨æ˜æ˜¾ç©ºç™½ã€‚å°½ç®¡å¯¹è¡¨æ ¼æ•°æ®çš„æ‰©æ•£æ¨¡å‹çš„å…´è¶£æ—¥ç›Šå¢åŠ ï¼Œä½†å¾ˆå°‘æœ‰äººç³»ç»Ÿåœ°å›é¡¾å’Œæ€»ç»“è¿™äº›å‘å±•ã€‚ç¼ºä¹ä¸“é—¨çš„è°ƒæŸ¥é™åˆ¶äº†å¯¹è¿™ä¸€å…³é”®é¢†åŸŸçš„æŒ‘æˆ˜ã€è¿›å±•å’Œæœªæ¥æ–¹å‘çš„ç†è§£ã€‚æœ¬è°ƒæŸ¥é€šè¿‡æä¾›è¡¨æ ¼æ•°æ®çš„æ‰©æ•£æ¨¡å‹çš„å…¨é¢å›é¡¾æ¥å¡«è¡¥è¿™ä¸€ç©ºç™½ã€‚æˆ‘ä»¬ä»2015å¹´6æœˆæ‰©æ•£æ¨¡å‹å‡ºç°çš„æ—¶å€™å¼€å§‹ï¼Œåˆ°2024å¹´12æœˆï¼Œåˆ†æäº†å‡ ä¹æ‰€æœ‰çš„ç›¸å…³ç ”ç©¶ï¼Œæ›´æ–°å†…å®¹ç»´æŠ¤åœ¨ä¸€ä¸ªGitHubä»“åº“ä¸­ã€‚å‡è®¾è¯»è€…å…·å¤‡ç»Ÿè®¡å­¦å’Œæ‰©æ•£æ¨¡å‹çš„åŸºç¡€çŸ¥è¯†ï¼Œæˆ‘ä»¬é‡‡ç”¨æ•°å­¦å…¬å¼æ¥æä¾›ä¸¥è°¨è¯¦ç»†çš„è¯„è®ºï¼Œæ—¨åœ¨ä¿ƒè¿›è¿™ä¸€æ–°å…´å’Œæ¿€åŠ¨äººå¿ƒé¢†åŸŸçš„å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17119v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆã€æ–‡æœ¬åˆæˆã€éŸ³é¢‘åˆ›å»ºã€è§†é¢‘ç”Ÿæˆå’Œæ•°æ®å¢å¼ºç­‰å¤šä¸ªé¢†åŸŸå–å¾—çš„æ˜¾è‘—æˆæœã€‚æ‰©æ•£æ¨¡å‹ä½œä¸ºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å’Œå˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEsï¼‰çš„æ›¿ä»£æ–¹æ¡ˆï¼Œè§£å†³äº†å®ƒä»¬çš„è®­ç»ƒä¸ç¨³å®šã€æ¨¡å¼å´©æºƒå’Œå¤šæ¨¡æ€åˆ†å¸ƒè¡¨ç¤ºä¸ä½³ç­‰é—®é¢˜ã€‚æ–‡ç« æŒ‡å‡ºï¼Œæ‰©æ•£æ¨¡å‹åœ¨è¡¨æ ¼æ•°æ®é¢†åŸŸä¹Ÿå¼€å§‹å±•ç°å‡ºä¼˜åŠ¿ï¼Œå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½çªç ´ï¼Œå¹¶æœ‰å¯èƒ½è§£å†³è¡¨æ ¼æ•°æ®å»ºæ¨¡ä¸­çš„ç‹¬ç‰¹æŒ‘æˆ˜ã€‚ç„¶è€Œï¼Œå°½ç®¡å›¾åƒå’Œæ—¶é—´åºåˆ—ç­‰é¢†åŸŸå·²ç»æœ‰è®¸å¤šå…³äºæ‰©æ•£æ¨¡å‹çš„è°ƒæŸ¥ï¼Œä½†è¡¨æ ¼æ•°æ®é¢†åŸŸçš„æ–‡çŒ®ä»å­˜åœ¨æ˜¾è‘—å·®è·ã€‚å°½ç®¡æ‰©æ•£æ¨¡å‹åœ¨è¡¨æ ¼æ•°æ®ä¸­çš„åº”ç”¨è¶Šæ¥è¶Šå—æ¬¢è¿ï¼Œä½†å¾ˆå°‘æœ‰äººç³»ç»Ÿåœ°å›é¡¾å’Œæ€»ç»“è¿™äº›å‘å±•ã€‚æœ¬æ–‡æ—¨åœ¨å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæä¾›ä¸€ä¸ªå…¨é¢çš„æ‰©æ•£æ¨¡å‹è¡¨æ ¼æ•°æ®å›é¡¾ã€‚æ–‡ç« æ¶µç›–äº†ä»2015å¹´æ‰©æ•£æ¨¡å‹å‡ºç°åˆ°2024å¹´ç›¸å…³ç ”ç©¶çš„åŸºæœ¬æƒ…å†µï¼Œå¹¶æ›´æ–°äº†ç»´æŠ¤çš„ç›¸å…³ç ”ç©¶å†…å®¹ï¼Œç›®æ ‡æ˜¯æ¨åŠ¨è¿™ä¸€æ–°å…´é¢†åŸŸçš„è¿›æ­¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹å·²åœ¨å¤šä¸ªé¢†åŸŸåŒ…æ‹¬å›¾åƒç”Ÿæˆç­‰å±•ç°å‡ºæ˜¾è‘—æ€§èƒ½ä¼˜åŠ¿ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹è§£å†³äº†ç”Ÿæˆå¯¹æŠ—ç½‘ç»œå’Œå˜åˆ†è‡ªç¼–ç å™¨çš„æŸäº›å±€é™æ€§ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åœ¨è¡¨æ ¼æ•°æ®é¢†åŸŸä¹Ÿå¼€å§‹å±•ç°ä¼˜åŠ¿ï¼Œå¹¶è§£å†³äº†ç‰¹å®šæŒ‘æˆ˜ã€‚</li>
<li>å°½ç®¡æ‰©æ•£æ¨¡å‹åœ¨è¡¨æ ¼æ•°æ®ä¸­çš„åº”ç”¨è¶Šæ¥è¶Šå—æ¬¢è¿ï¼Œä½†ç›¸å…³ç ”ç©¶ç»¼è¿°ä»å­˜åœ¨ç©ºç™½ã€‚</li>
<li>æœ¬æ–‡æä¾›äº†ä¸€ä¸ªå…¨é¢çš„æ‰©æ•£æ¨¡å‹è¡¨æ ¼æ•°æ®å›é¡¾ï¼Œæ¶µç›–äº†ä»2015å¹´åˆ°2024å¹´çš„ç›¸å…³ç ”ç©¶ã€‚</li>
<li>æ–‡ç« é€šè¿‡æ•°å­¦å…¬å¼è¿›è¡Œäº†ä¸¥æ ¼å’Œè¯¦ç»†çš„è¯„è¿°ï¼Œå‡è®¾è¯»è€…å…·å¤‡ç»Ÿè®¡å’Œæ‰©æ•£æ¨¡å‹çš„åŸºç¡€çŸ¥è¯†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17119">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-50792d2eacd4296041607b1fd946fb8b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a4202ec35b8d8aac25fa9635e818981b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a39fd9d6e10c7e8129b407ce2de8f589.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-130a2b6a0616c01ce9e5b7b4a8e9cd2f.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="SFLD-Reducing-the-content-bias-for-AI-generated-Image-Detection"><a href="#SFLD-Reducing-the-content-bias-for-AI-generated-Image-Detection" class="headerlink" title="SFLD: Reducing the content bias for AI-generated Image Detection"></a>SFLD: Reducing the content bias for AI-generated Image Detection</h2><p><strong>Authors:Seoyeon Gye, Junwon Ko, Hyounguk Shon, Minchan Kwon, Junmo Kim</strong></p>
<p>Identifying AI-generated content is critical for the safe and ethical use of generative AI. Recent research has focused on developing detectors that generalize to unknown generators, with popular methods relying either on high-level features or low-level fingerprints. However, these methods have clear limitations: biased towards unseen content, or vulnerable to common image degradations, such as JPEG compression. To address these issues, we propose a novel approach, SFLD, which incorporates PatchShuffle to integrate high-level semantic and low-level textural information. SFLD applies PatchShuffle at multiple levels, improving robustness and generalization across various generative models. Additionally, current benchmarks face challenges such as low image quality, insufficient content preservation, and limited class diversity. In response, we introduce TwinSynths, a new benchmark generation methodology that constructs visually near-identical pairs of real and synthetic images to ensure high quality and content preservation. Our extensive experiments and analysis show that SFLD outperforms existing methods on detecting a wide variety of fake images sourced from GANs, diffusion models, and TwinSynths, demonstrating the state-of-the-art performance and generalization capabilities to novel generative models. </p>
<blockquote>
<p>è¯†åˆ«äººå·¥æ™ºèƒ½ç”Ÿæˆçš„å†…å®¹å¯¹äºäººå·¥æ™ºèƒ½ç”ŸæˆæŠ€æœ¯çš„å®‰å…¨å’Œé“å¾·ä½¿ç”¨è‡³å…³é‡è¦ã€‚è¿‘æœŸçš„ç ”ç©¶é‡ç‚¹å·²è½¬å‘å¼€å‘èƒ½å¤Ÿæ¨å¹¿åˆ°æœªçŸ¥ç”Ÿæˆå™¨çš„æ£€æµ‹å™¨ï¼Œæµè¡Œçš„æ–¹æ³•ä¾èµ–äºé«˜çº§ç‰¹å¾æˆ–ä½çº§æŒ‡çº¹ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•æœ‰æ˜æ˜¾çš„å±€é™æ€§ï¼šåå‘äºæœªçŸ¥å†…å®¹ï¼Œæˆ–å®¹æ˜“å—åˆ°å¸¸è§å›¾åƒé€€åŒ–çš„å½±å“ï¼Œå¦‚JPEGå‹ç¼©ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•SFLDï¼Œå®ƒç»“åˆäº†PatchShuffleæ¥æ•´åˆé«˜çº§è¯­ä¹‰å’Œä½çº§çº¹ç†ä¿¡æ¯ã€‚SFLDåœ¨å¤šçº§åº”ç”¨PatchShuffleï¼Œæé«˜äº†åœ¨å„ç§ç”Ÿæˆæ¨¡å‹ä¸Šçš„ç¨³å¥æ€§å’Œé€šç”¨æ€§ã€‚æ­¤å¤–ï¼Œå½“å‰åŸºå‡†æµ‹è¯•é¢ä¸´ç€å›¾åƒè´¨é‡ä½ã€å†…å®¹ä¿å­˜ä¸è¶³å’Œç±»åˆ«å¤šæ ·æ€§æœ‰é™ç­‰æŒ‘æˆ˜ã€‚ä½œä¸ºå›åº”ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ–°çš„åŸºå‡†æµ‹è¯•ç”Ÿæˆæ–¹æ³•TwinSynthsï¼Œå®ƒæ„å»ºè§†è§‰ä¸Šå‡ ä¹ç›¸åŒçš„çœŸå®å’Œåˆæˆå›¾åƒå¯¹ï¼Œä»¥ç¡®ä¿é«˜è´¨é‡å’Œå†…å®¹ä¿å­˜ã€‚æˆ‘ä»¬çš„å¹¿æ³›å®éªŒå’Œåˆ†æè¡¨æ˜ï¼ŒSFLDåœ¨æ£€æµ‹æ¥è‡ªGANã€æ‰©æ•£æ¨¡å‹å’ŒTwinSynthsçš„å¤šç§è™šå‡å›¾åƒæ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶åœ¨æ–°å‹ç”Ÿæˆæ¨¡å‹ä¸Šçš„å“è¶Šæ€§èƒ½å’Œé€šç”¨åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17105v1">PDF</a> IEEE&#x2F;CVF WACV 2025, Oral</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹çš„å†…å®¹ç”Ÿæˆæ£€æµ‹æ–¹æ¡ˆSFLDï¼Œæ—¨åœ¨è§£å†³äººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹çš„è¯†åˆ«å’Œè¯„ä¼°é—®é¢˜ã€‚SFLDé€šè¿‡ç»“åˆPatchShuffleæŠ€æœ¯ï¼Œèåˆäº†é«˜çº§è¯­ä¹‰å’Œä½çº§çº¹ç†ä¿¡æ¯ï¼Œæé«˜äº†æ£€æµ‹æ–¹æ³•çš„ç¨³å¥æ€§å’Œå¯¹å„ç§ç”Ÿæˆæ¨¡å‹çš„é€šç”¨æ€§ã€‚åŒæ—¶ï¼Œä¸ºäº†æ”¹è¿›ç°æœ‰è¯„ä¼°æ ‡å‡†çš„ä¸è¶³ï¼Œå¼•å…¥äº†æ–°çš„åŸºå‡†ç”Ÿæˆæ–¹æ³•TwinSynthsï¼Œèƒ½å¤Ÿç”Ÿæˆè§†è§‰ä¸Šä¸çœŸå®å›¾åƒå‡ ä¹ç›¸åŒçš„åˆæˆå›¾åƒï¼Œç¡®ä¿é«˜è´¨é‡å’Œå†…å®¹ä¿ç•™ã€‚å®éªŒè¡¨æ˜ï¼ŒSFLDåœ¨æ£€æµ‹æ¥è‡ªGANsã€æ‰©æ•£æ¨¡å‹å’ŒTwinSynthsçš„è™šå‡å›¾åƒæ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SFLDæ–¹æ³•ç»“åˆäº†PatchShuffleæŠ€æœ¯ï¼Œæ—¨åœ¨è§£å†³AIç”Ÿæˆå†…å®¹æ£€æµ‹çš„é—®é¢˜ã€‚</li>
<li>SFLDèåˆäº†é«˜çº§è¯­ä¹‰å’Œä½çº§çº¹ç†ä¿¡æ¯ï¼Œæé«˜äº†æ£€æµ‹æ–¹æ³•çš„ç¨³å¥æ€§å’Œé€šç”¨æ€§ã€‚</li>
<li>ç°æœ‰æ£€æµ‹æ–¹æ³•çš„å±€é™æ€§åŒ…æ‹¬å¯¹æ–°æœªçŸ¥ç”Ÿæˆå™¨çš„åè§å’Œå¯¹å¸¸è§å›¾åƒé™è´¨çš„è„†å¼±æ€§ã€‚</li>
<li>SFLDé€šè¿‡åœ¨å¤šä¸ªå±‚é¢åº”ç”¨PatchShuffleæŠ€æœ¯æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>å¼•å…¥æ–°çš„åŸºå‡†ç”Ÿæˆæ–¹æ³•TwinSynthsï¼Œä»¥ç”Ÿæˆé«˜è´¨é‡ä¸”å†…å®¹ä¿ç•™çš„åˆæˆå›¾åƒã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒSFLDåœ¨æ£€æµ‹å„ç§è™šå‡å›¾åƒæ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17105">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7df22d1fb09c8bcd5a78dab102c0bd57.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b64ca6d83320ba4fca5d7751bee06d5a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0039f398eda8e02fe09ef34f871e2c8c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f5444f48039dcab804eae7f460ab553.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce0226604461c24203b78f77234ac1d7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dff0e1018a9c7da3873351c53470582f.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="SpecDM-Hyperspectral-Dataset-Synthesis-with-Pixel-level-Semantic-Annotations"><a href="#SpecDM-Hyperspectral-Dataset-Synthesis-with-Pixel-level-Semantic-Annotations" class="headerlink" title="SpecDM: Hyperspectral Dataset Synthesis with Pixel-level Semantic   Annotations"></a>SpecDM: Hyperspectral Dataset Synthesis with Pixel-level Semantic   Annotations</h2><p><strong>Authors:Wendi Liu, Pei Yang, Wenhui Hong, Xiaoguang Mei, Jiayi Ma</strong></p>
<p>In hyperspectral remote sensing field, some downstream dense prediction tasks, such as semantic segmentation (SS) and change detection (CD), rely on supervised learning to improve model performance and require a large amount of manually annotated data for training. However, due to the needs of specific equipment and special application scenarios, the acquisition and annotation of hyperspectral images (HSIs) are often costly and time-consuming. To this end, our work explores the potential of generative diffusion model in synthesizing HSIs with pixel-level annotations. The main idea is to utilize a two-stream VAE to learn the latent representations of images and corresponding masks respectively, learn their joint distribution during the diffusion model training, and finally obtain the image and mask through their respective decoders. To the best of our knowledge, it is the first work to generate high-dimensional HSIs with annotations. Our proposed approach can be applied in various kinds of dataset generation. We select two of the most widely used dense prediction tasks: semantic segmentation and change detection, and generate datasets suitable for these tasks. Experiments demonstrate that our synthetic datasets have a positive impact on the improvement of these downstream tasks. </p>
<blockquote>
<p>åœ¨è¶…å…‰è°±é¥æ„Ÿé¢†åŸŸï¼Œä¸€äº›ä¸‹æ¸¸å¯†é›†é¢„æµ‹ä»»åŠ¡ï¼Œå¦‚è¯­ä¹‰åˆ†å‰²ï¼ˆSSï¼‰å’Œå˜åŒ–æ£€æµ‹ï¼ˆCDï¼‰ï¼Œä¾èµ–äºç›‘ç£å­¦ä¹ æ¥æé«˜æ¨¡å‹æ€§èƒ½ï¼Œå¹¶éœ€è¦å¤§é‡æ‰‹åŠ¨æ³¨é‡Šçš„æ•°æ®è¿›è¡Œè®­ç»ƒã€‚ç„¶è€Œï¼Œç”±äºç‰¹å®šè®¾å¤‡å’Œç‰¹æ®Šåº”ç”¨åœºæ™¯çš„éœ€æ±‚ï¼Œè¶…å…‰è°±å›¾åƒï¼ˆHSIï¼‰çš„è·å–å’Œæ³¨é‡Šå¾€å¾€æˆæœ¬é«˜æ˜‚ã€è€—æ—¶ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬çš„å·¥ä½œæ¢ç´¢äº†ç”Ÿæˆæ‰©æ•£æ¨¡å‹åœ¨åˆæˆå¸¦æœ‰åƒç´ çº§æ³¨é‡Šçš„HSIæ–¹é¢çš„æ½œåŠ›ã€‚ä¸»è¦æ€æƒ³æ˜¯åˆ©ç”¨åŒæµVAEåˆ†åˆ«å­¦ä¹ å›¾åƒå’Œç›¸åº”æ©ç çš„æ½œåœ¨è¡¨ç¤ºï¼Œåœ¨æ‰©æ•£æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­å­¦ä¹ å®ƒä»¬çš„è”åˆåˆ†å¸ƒï¼Œç„¶åé€šè¿‡å„è‡ªçš„è§£ç å™¨è·å¾—å›¾åƒå’Œæ©ç ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€é¡¹ç”Ÿæˆå¸¦æœ‰æ³¨é‡Šçš„é«˜ç»´HSIçš„å·¥ä½œã€‚æˆ‘ä»¬æå‡ºçš„æ–¹æ³•å¯åº”ç”¨äºå„ç§æ•°æ®é›†ç”Ÿæˆã€‚æˆ‘ä»¬é€‰æ‹©äº†ä¸¤ä¸ªæœ€å¹¿æ³›ä½¿ç”¨çš„å¯†é›†é¢„æµ‹ä»»åŠ¡ï¼šè¯­ä¹‰åˆ†å‰²å’Œå˜åŒ–æ£€æµ‹ï¼Œå¹¶ç”Ÿæˆäº†é€‚åˆè¿™äº›ä»»åŠ¡çš„æ•°æ®é›†ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬åˆæˆçš„æ•°æ®é›†å¯¹è¿™äº›ä¸‹æ¸¸ä»»åŠ¡çš„æ”¹è¿›äº§ç”Ÿäº†ç§¯æå½±å“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17056v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢ç´¢äº†åœ¨è¶…å…‰è°±é¥æ„Ÿé¢†åŸŸï¼Œåˆ©ç”¨ç”Ÿæˆæ€§æ‰©æ•£æ¨¡å‹åˆæˆå¸¦æœ‰åƒç´ çº§æ³¨é‡Šçš„é«˜å…‰è°±å›¾åƒï¼ˆHSIsï¼‰çš„æ½œåŠ›ã€‚è¯¥ç ”ç©¶é‡‡ç”¨åŒæµå˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰å­¦ä¹ å›¾åƒå’Œç›¸åº”æ©ç çš„æ½œåœ¨è¡¨ç¤ºï¼Œåœ¨æ‰©æ•£æ¨¡å‹è®­ç»ƒæœŸé—´å­¦ä¹ å®ƒä»¬çš„è”åˆåˆ†å¸ƒï¼Œå¹¶é€šè¿‡å„è‡ªçš„è§£ç å™¨è·å¾—å›¾åƒå’Œæ©ç ã€‚è¯¥ç ”ç©¶é¦–æ¬¡ç”Ÿæˆå¸¦æœ‰æ³¨é‡Šçš„é«˜ç»´HSIsï¼Œå¯¹å¤šç§æ•°æ®é›†ç”Ÿæˆå…·æœ‰åº”ç”¨ä»·å€¼ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯­ä¹‰åˆ†å‰²å’Œå˜åŒ–æ£€æµ‹ç­‰å¯†é›†é¢„æµ‹ä»»åŠ¡ä¸­ã€‚å®éªŒè¡¨æ˜ï¼Œåˆæˆæ•°æ®é›†å¯¹è¿™äº›ä¸‹æ¸¸ä»»åŠ¡æœ‰ç§¯æå½±å“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¶…å…‰è°±é¥æ„Ÿä¸­çš„ä¸‹æ¸¸å¯†é›†é¢„æµ‹ä»»åŠ¡ï¼Œå¦‚è¯­ä¹‰åˆ†å‰²å’Œå˜åŒ–æ£€æµ‹ï¼Œéœ€è¦ä¾èµ–å¤§é‡æ‰‹åŠ¨æ³¨é‡Šæ•°æ®è¿›è¡Œè®­ç»ƒï¼Œä½†è·å–å’Œæ³¨é‡Šé«˜å…‰è°±å›¾åƒï¼ˆHSIsï¼‰æˆæœ¬é«˜æ˜‚ã€è€—æ—¶æ—¶ã€‚</li>
<li>ç ”ç©¶é¦–æ¬¡å°è¯•åˆ©ç”¨ç”Ÿæˆæ€§æ‰©æ•£æ¨¡å‹åˆæˆå¸¦æœ‰åƒç´ çº§æ³¨é‡Šçš„HSIsã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨åŒæµå˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰å­¦ä¹ å›¾åƒå’Œæ©ç çš„æ½œåœ¨è¡¨ç¤ºï¼Œå¹¶å­¦ä¹ å®ƒä»¬çš„è”åˆåˆ†å¸ƒã€‚</li>
<li>é€šè¿‡å„è‡ªçš„è§£ç å™¨è·å¾—åˆæˆå›¾åƒå’Œæ©ç ã€‚</li>
<li>è¯¥æ–¹æ³•å¯åº”ç”¨äºå¤šç§æ•°æ®é›†ç”Ÿæˆï¼Œç‰¹åˆ«æ˜¯è¯­ä¹‰åˆ†å‰²å’Œå˜åŒ–æ£€æµ‹ä»»åŠ¡ã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œåˆæˆçš„æ•°æ®é›†å¯¹æ”¹è¿›ä¸‹æ¸¸ä»»åŠ¡æœ‰ç§¯æå½±å“ã€‚</li>
<li>æ­¤æŠ€æœ¯æœ‰æœ›é™ä½æ•°æ®è·å–å’Œæ³¨é‡Šçš„æˆæœ¬ï¼Œæé«˜æ¨¡å‹åœ¨è¶…å…‰è°±é¥æ„Ÿé¢†åŸŸçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17056">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9334d10412b14afc4a1c5f90d4f15928.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-22de93ee27308208e4c82e6f8128a2fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6aa76b499e9788abc9e7813f988cd65.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8c984181c254d8a4c796a1ab35f0665c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a03a2811a2202fa0c9afce0f60d8a3e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-83096682c1fd710fe82143a95c0093ab.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Autoregressive-Image-Generation-Guided-by-Chains-of-Thought"><a href="#Autoregressive-Image-Generation-Guided-by-Chains-of-Thought" class="headerlink" title="Autoregressive Image Generation Guided by Chains of Thought"></a>Autoregressive Image Generation Guided by Chains of Thought</h2><p><strong>Authors:Miaomiao Cai, Guanjie Wang, Wei Li, Zhijun Tu, Hanting Chen, Shaohui Lin, Jie Hu</strong></p>
<p>In the field of autoregressive (AR) image generation, models based on the â€˜next-token predictionâ€™ paradigm of LLMs have shown comparable performance to diffusion models by reducing inductive biases. However, directly applying LLMs to complex image generation can struggle with reconstructing the structure and details of the image, impacting the accuracy and stability of generation. Additionally, the â€˜next-token predictionâ€™ paradigm in the AR model does not align with the contextual scanning and logical reasoning processes involved in human visual perception, limiting effective image generation. Chain-of-Thought (CoT), as a key reasoning capability of LLMs, utilizes reasoning prompts to guide the model, improving reasoning performance on complex natural language process (NLP) tasks, enhancing accuracy and stability of generation, and helping the model maintain contextual coherence and logical consistency, similar to human reasoning. Inspired by CoT from the field of NLP, we propose autoregressive Image Generation with Thoughtful Reasoning (IGTR) to enhance autoregressive image generation. IGTR adds reasoning prompts without modifying the model structure or raster generation order. Specifically, we design specialized image-related reasoning prompts for AR image generation to simulate the human reasoning process, which enhances contextual reasoning by allowing the model to first perceive overall distribution information before generating the image, and improve generation stability by increasing the inference steps. Compared to the AR method without prompts, our method shows outstanding performance and achieves an approximate improvement of 20%. </p>
<blockquote>
<p>åœ¨è‡ªå›å½’ï¼ˆARï¼‰å›¾åƒç”Ÿæˆé¢†åŸŸï¼ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„â€œä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹â€èŒƒå¼çš„æ¨¡å‹é€šè¿‡å‡å°‘å½’çº³åè§è¡¨ç°å‡ºäº†ä¸æ‰©æ•£æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç›´æ¥å°†å¤§å‹è¯­è¨€æ¨¡å‹åº”ç”¨äºå¤æ‚å›¾åƒç”Ÿæˆæ—¶ï¼Œå¯èƒ½ä¼šé¢ä¸´é‡å»ºå›¾åƒç»“æ„å’Œç»†èŠ‚æ–¹é¢çš„å›°éš¾ï¼Œä»è€Œå½±å“ç”Ÿæˆçš„å‡†ç¡®æ€§å’Œç¨³å®šæ€§ã€‚æ­¤å¤–ï¼Œè‡ªå›å½’æ¨¡å‹ä¸­çš„â€œä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹â€èŒƒå¼å¹¶ä¸ç¬¦åˆäººç±»è§†è§‰æ„ŸçŸ¥æ‰€æ¶‰åŠä¸Šä¸‹æ–‡æ‰«æå’Œé€»è¾‘æ¨ç†è¿‡ç¨‹ï¼Œé™åˆ¶äº†æœ‰æ•ˆçš„å›¾åƒç”Ÿæˆã€‚ä½œä¸ºå¤§å‹è¯­è¨€æ¨¡å‹çš„å…³é”®æ¨ç†èƒ½åŠ›ï¼Œâ€œé“¾å¼æ€ç»´â€ï¼ˆCoTï¼‰åˆ©ç”¨æ¨ç†æç¤ºæ¥å¼•å¯¼æ¨¡å‹ï¼Œæé«˜äº†åœ¨å¤æ‚è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»»åŠ¡ä¸Šçš„æ¨ç†æ€§èƒ½ï¼Œå¢å¼ºäº†ç”Ÿæˆçš„å‡†ç¡®æ€§å’Œç¨³å®šæ€§ï¼Œå¸®åŠ©æ¨¡å‹ä¿æŒä¸Šä¸‹æ–‡è¿è´¯æ€§å’Œé€»è¾‘ä¸€è‡´æ€§ï¼Œç±»ä¼¼äºäººç±»æ¨ç†ã€‚æˆ‘ä»¬ä»è‡ªç„¶è¯­è¨€å¤„ç†çš„é¢†åŸŸå—åˆ°æ€ç»´é“¾çš„å¯å‘ï¼Œæå‡ºäº†å¢å¼ºè‡ªå›å½’å›¾åƒç”Ÿæˆçš„â€œæ·±æ€ç†Ÿè™‘çš„å›¾åƒç”Ÿæˆâ€ï¼ˆIGTRï¼‰ã€‚IGTRé€šè¿‡æ·»åŠ æ¨ç†æç¤ºï¼Œè€Œæ— éœ€ä¿®æ”¹æ¨¡å‹ç»“æ„æˆ–æ …æ ¼ç”Ÿæˆé¡ºåºã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä¸ºARå›¾åƒç”Ÿæˆè®¾è®¡äº†ä¸“é—¨çš„å›¾åƒç›¸å…³æ¨ç†æç¤ºï¼Œä»¥æ¨¡æ‹Ÿäººç±»æ¨ç†è¿‡ç¨‹ï¼Œé€šè¿‡å…è®¸æ¨¡å‹åœ¨ç”Ÿæˆå›¾åƒä¹‹å‰å…ˆæ„ŸçŸ¥æ•´ä½“åˆ†å¸ƒä¿¡æ¯ï¼Œå¢å¼ºä¸Šä¸‹æ–‡æ¨ç†ï¼Œå¹¶é€šè¿‡å¢åŠ æ¨ç†æ­¥éª¤æ¥æé«˜ç”Ÿæˆçš„ç¨³å®šæ€§ã€‚ä¸æ²¡æœ‰æç¤ºçš„ARæ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œå®ç°äº†å¤§çº¦20%çš„æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.16965v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºè‡ªå›å½’ï¼ˆARï¼‰å›¾åƒç”Ÿæˆé¢†åŸŸçš„ç ”ç©¶ï¼Œé€šè¿‡å‡å°‘å½’çº³åè§ï¼ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„â€œä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹â€èŒƒå¼çš„æ¨¡å‹è¡¨ç°å‡ºäº†ä¸æ‰©æ•£æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç›´æ¥å°†LLMsåº”ç”¨äºå¤æ‚çš„å›¾åƒç”Ÿæˆå¯èƒ½ä¼šéš¾ä»¥é‡å»ºå›¾åƒçš„ç»“æ„å’Œç»†èŠ‚ï¼Œå½±å“ç”Ÿæˆçš„å‡†ç¡®æ€§å’Œç¨³å®šæ€§ã€‚æ­¤å¤–ï¼ŒARæ¨¡å‹ä¸­çš„â€œä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹â€èŒƒå¼å¹¶ä¸ç¬¦åˆäººç±»è§†è§‰æ„ŸçŸ¥æ‰€æ¶‰åŠä¸Šä¸‹æ–‡æ‰«æå’Œé€»è¾‘æ¨ç†è¿‡ç¨‹ï¼Œé™åˆ¶äº†æœ‰æ•ˆçš„å›¾åƒç”Ÿæˆã€‚å—è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸä¸­é“¾å¼æ€ç»´ï¼ˆCoTï¼‰çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†å¢å¼ºè‡ªå›å½’å›¾åƒç”Ÿæˆçš„æ€ç»´é©±åŠ¨å›¾åƒç”Ÿæˆï¼ˆIGTRï¼‰æ–¹æ³•ã€‚IGTRé€šè¿‡æ·»åŠ æ¨ç†æç¤ºè€Œæ— éœ€ä¿®æ”¹æ¨¡å‹ç»“æ„æˆ–æ …æ ¼ç”Ÿæˆé¡ºåºï¼Œè®¾è®¡äº†é’ˆå¯¹ARå›¾åƒç”Ÿæˆçš„ä¸“é—¨å›¾åƒç›¸å…³æ¨ç†æç¤ºï¼Œæ¨¡æ‹Ÿäººç±»æ¨ç†è¿‡ç¨‹ï¼Œé€šè¿‡å…è®¸æ¨¡å‹åœ¨ç”Ÿæˆå›¾åƒä¹‹å‰å…ˆæ„ŸçŸ¥æ•´ä½“åˆ†å¸ƒä¿¡æ¯ï¼Œå¢å¼ºäº†ä¸Šä¸‹æ–‡æ¨ç†ï¼Œå¹¶é€šè¿‡å¢åŠ æ¨ç†æ­¥éª¤æé«˜äº†ç”Ÿæˆçš„ç¨³å®šæ€§ã€‚ä¸æ²¡æœ‰æç¤ºçš„ARæ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œå¹¶å®ç°äº†çº¦20%çš„æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsåœ¨è‡ªå›å½’å›¾åƒç”Ÿæˆä¸­è¡¨ç°å‡ºä¸æ‰©æ•£æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ï¼Œä½†å­˜åœ¨å‡†ç¡®æ€§å’Œç¨³å®šæ€§é—®é¢˜ã€‚</li>
<li>â€˜Next-token predictionâ€™ èŒƒå¼åœ¨å›¾åƒç”Ÿæˆä¸­ä¸ç¬¦åˆäººç±»è§†è§‰æ„ŸçŸ¥çš„è‡ªç„¶è¿‡ç¨‹ã€‚</li>
<li>é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ–¹æ³•æé«˜äº†LLMsåœ¨å¤æ‚NLPä»»åŠ¡ä¸­çš„æ¨ç†æ€§èƒ½ã€‚</li>
<li>IGTRæ–¹æ³•é€šè¿‡æ·»åŠ æ¨ç†æç¤ºå¢å¼ºäº†è‡ªå›å½’å›¾åƒç”Ÿæˆï¼Œæé«˜äº†ç”Ÿæˆçš„è´¨é‡å’Œç¨³å®šæ€§ã€‚</li>
<li>IGTRæ¨¡æ‹Ÿäººç±»æ¨ç†è¿‡ç¨‹ï¼Œå…è®¸æ¨¡å‹åœ¨ç”Ÿæˆå‰æ„ŸçŸ¥æ•´ä½“åˆ†å¸ƒä¿¡æ¯ã€‚</li>
<li>IGTRé€šè¿‡å¢åŠ æ¨ç†æ­¥éª¤æ”¹å–„äº†ä¸Šä¸‹æ–‡æ¨ç†å’Œç”Ÿæˆç¨³å®šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.16965">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-3a93a73217ff9ef5c05242d5f811e3f7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2415635db2b2fbbf797b11746eed96f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c888581ef405b3024933fab5311f8f4a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-423b71ae65515bc2b8603162dc210a94.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-39976d4ebc0ad6ff32ac0b98073a01e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-653ebc696baf1913fd330a762c07b7f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5549d9e892faf0501f5fd623c10dfae7.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="MAD-AD-Masked-Diffusion-for-Unsupervised-Brain-Anomaly-Detection"><a href="#MAD-AD-Masked-Diffusion-for-Unsupervised-Brain-Anomaly-Detection" class="headerlink" title="MAD-AD: Masked Diffusion for Unsupervised Brain Anomaly Detection"></a>MAD-AD: Masked Diffusion for Unsupervised Brain Anomaly Detection</h2><p><strong>Authors:Farzad Beizaee, Gregory Lodygensky, Christian Desrosiers, Jose Dolz</strong></p>
<p>Unsupervised anomaly detection in brain images is crucial for identifying injuries and pathologies without access to labels. However, the accurate localization of anomalies in medical images remains challenging due to the inherent complexity and variability of brain structures and the scarcity of annotated abnormal data. To address this challenge, we propose a novel approach that incorporates masking within diffusion models, leveraging their generative capabilities to learn robust representations of normal brain anatomy. During training, our model processes only normal brain MRI scans and performs a forward diffusion process in the latent space that adds noise to the features of randomly-selected patches. Following a dual objective, the model learns to identify which patches are noisy and recover their original features. This strategy ensures that the model captures intricate patterns of normal brain structures while isolating potential anomalies as noise in the latent space. At inference, the model identifies noisy patches corresponding to anomalies and generates a normal counterpart for these patches by applying a reverse diffusion process. Our method surpasses existing unsupervised anomaly detection techniques, demonstrating superior performance in generating accurate normal counterparts and localizing anomalies. The code is available at h<a target="_blank" rel="noopener" href="https://github.com/farzad-bz/MAD-AD">https://github.com/farzad-bz/MAD-AD</a>. </p>
<blockquote>
<p>åœ¨æ— éœ€æ ‡ç­¾çš„æƒ…å†µä¸‹ï¼Œå¯¹è„‘å›¾åƒè¿›è¡Œæ— ç›‘ç£å¼‚å¸¸æ£€æµ‹å¯¹äºè¯†åˆ«æŸä¼¤å’Œç—…ç†è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç”±äºè„‘ç»“æ„æœ¬èº«çš„å¤æ‚æ€§å’Œå¯å˜æ€§ä»¥åŠæ ‡æ³¨å¼‚å¸¸æ•°æ®çš„ç¨€ç¼ºæ€§ï¼ŒåŒ»å­¦å›¾åƒä¸­å¼‚å¸¸çš„å‡†ç¡®å®šä½ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»“åˆæ‰©æ•£æ¨¡å‹æ©æ¨¡çš„æ–°æ–¹æ³•ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›å­¦ä¹ å¯¹æ­£å¸¸è„‘è§£å‰–ç»“æ„çš„ç¨³å¥è¡¨ç¤ºã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ä»…å¤„ç†æ­£å¸¸çš„è„‘éƒ¨MRIæ‰«æï¼Œå¹¶åœ¨æ½œåœ¨ç©ºé—´æ‰§è¡Œæ­£å‘æ‰©æ•£è¿‡ç¨‹ï¼Œå‘éšæœºé€‰æ‹©çš„è¡¥ä¸çš„ç‰¹å¾æ·»åŠ å™ªå£°ã€‚éµå¾ªåŒé‡ç›®æ ‡ï¼Œæ¨¡å‹å­¦ä¼šè¯†åˆ«å“ªäº›è¡¥ä¸æ˜¯å˜ˆæ‚çš„å¹¶æ¢å¤å…¶åŸå§‹ç‰¹å¾ã€‚è¿™ä¸€ç­–ç•¥ç¡®ä¿æ¨¡å‹æ•æ‰æ­£å¸¸è„‘ç»“æ„çš„å¤æ‚æ¨¡å¼ï¼ŒåŒæ—¶å°†æ½œåœ¨çš„å¼‚å¸¸ä½œä¸ºæ½œåœ¨ç©ºé—´ä¸­çš„å™ªå£°è€Œå­¤ç«‹å‡ºæ¥ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹ä¼šè¯†åˆ«å¯¹åº”äºå¼‚å¸¸çš„å˜ˆæ‚è¡¥ä¸ï¼Œå¹¶é€šè¿‡åå‘æ‰©æ•£è¿‡ç¨‹ä¸ºè¿™äº›è¡¥ä¸ç”Ÿæˆæ­£å¸¸å¯¹åº”ç‰©ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¶…è¶Šäº†ç°æœ‰çš„æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹æŠ€æœ¯ï¼Œåœ¨ç”Ÿæˆå‡†ç¡®çš„æ­£å¸¸å¯¹åº”ç‰©å’Œå®šä½å¼‚å¸¸æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/farzad-bz/MAD-AD%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/farzad-bz/MAD-ADè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.16943v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–°å‹æ— ç›‘ç£è„‘å›¾åƒå¼‚å¸¸æ£€æµ‹æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥æ©ç æŠ€æœ¯ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›å­¦ä¹ æ­£å¸¸è„‘ç»“æ„çš„é²æ£’è¡¨ç¤ºã€‚æ¨¡å‹ä»…å¤„ç†æ­£å¸¸è„‘MRIæ‰«æï¼Œé€šè¿‡æ½œåœ¨ç©ºé—´ä¸­çš„æ­£å‘æ‰©æ•£è¿‡ç¨‹æ·»åŠ å™ªå£°åˆ°éšæœºé€‰æ‹©çš„è¡¥ä¸ç‰¹å¾ä¸Šã€‚æ¨¡å‹å­¦ä¹ è¯†åˆ«å“ªäº›è¡¥ä¸æ˜¯å˜ˆæ‚çš„å¹¶æ¢å¤å…¶åŸå§‹ç‰¹å¾ï¼Œä»è€Œåœ¨æ½œåœ¨ç©ºé—´ä¸­æ•æ‰æ­£å¸¸è„‘ç»“æ„çš„å¤æ‚æ¨¡å¼å¹¶å°†æ½œåœ¨å¼‚å¸¸ä½œä¸ºå™ªå£°éš”ç¦»ã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œæ¨¡å‹è¯†åˆ«å¯¹åº”äºå¼‚å¸¸çš„å˜ˆæ‚è¡¥ä¸ï¼Œå¹¶é€šè¿‡åå‘æ‰©æ•£è¿‡ç¨‹ç”Ÿæˆè¿™äº›è¡¥ä¸çš„æ­£å¸¸å¯¹åº”ç‰©ã€‚è¯¥æ–¹æ³•ä¼˜äºç°æœ‰çš„æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹æŠ€æœ¯ï¼Œåœ¨ç”Ÿæˆå‡†ç¡®çš„æ­£å¸¸å¯¹åº”ç‰©å’Œå®šä½å¼‚å¸¸æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹ç»“åˆæ©ç æŠ€æœ¯ç”¨äºæ— ç›‘ç£è„‘å›¾åƒå¼‚å¸¸æ£€æµ‹ã€‚</li>
<li>æ¨¡å‹ä»…å¤„ç†æ­£å¸¸è„‘MRIæ‰«ææ•°æ®ã€‚</li>
<li>é€šè¿‡æ½œåœ¨ç©ºé—´ä¸­çš„æ­£å‘æ‰©æ•£è¿‡ç¨‹æ·»åŠ å™ªå£°åˆ°éšæœºé€‰æ‹©çš„è¡¥ä¸ç‰¹å¾ä¸Šã€‚</li>
<li>æ¨¡å‹èƒ½å¤Ÿè¯†åˆ«å¹¶æ¢å¤æ­£å¸¸ç‰¹å¾çš„å˜ˆæ‚è¡¥ä¸ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨æ½œåœ¨ç©ºé—´ä¸­æ•æ‰æ­£å¸¸è„‘ç»“æ„çš„å¤æ‚æ¨¡å¼ï¼Œå°†å¼‚å¸¸è¡¨ç°ä¸ºå™ªå£°ã€‚</li>
<li>åœ¨æ¨ç†é˜¶æ®µï¼Œæ¨¡å‹èƒ½è¯†åˆ«å¼‚å¸¸çš„å˜ˆæ‚è¡¥ä¸å¹¶ç”Ÿæˆå…¶æ­£å¸¸å¯¹åº”ç‰©ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.16943">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-416145dc3489b27cc482d3b51e1bf256.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6ef5318d210bef55c905e8cf4d2694cb.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="DiffKAN-Inpainting-KAN-based-Diffusion-model-for-brain-tumor-inpainting"><a href="#DiffKAN-Inpainting-KAN-based-Diffusion-model-for-brain-tumor-inpainting" class="headerlink" title="DiffKAN-Inpainting: KAN-based Diffusion model for brain tumor inpainting"></a>DiffKAN-Inpainting: KAN-based Diffusion model for brain tumor inpainting</h2><p><strong>Authors:Tianli Tao, Ziyang Wang, Han Zhang, Theodoros N. Arvanitis, Le Zhang</strong></p>
<p>Brain tumors delay the standard preprocessing workflow for further examination. Brain inpainting offers a viable, although difficult, solution for tumor tissue processing, which is necessary to improve the precision of the diagnosis and treatment. Most conventional U-Net-based generative models, however, often face challenges in capturing the complex, nonlinear latent representations inherent in brain imaging. In order to accomplish high-quality healthy brain tissue reconstruction, this work proposes DiffKAN-Inpainting, an innovative method that blends diffusion models with the Kolmogorov-Arnold Networks architecture. During the denoising process, we introduce the RePaint method and tumor information to generate images with a higher fidelity and smoother margin. Both qualitative and quantitative results demonstrate that as compared to the state-of-the-art methods, our proposed DiffKAN-Inpainting inpaints more detailed and realistic reconstructions on the BraTS dataset. The knowledge gained from ablation study provide insights for future research to balance performance with computing cost. </p>
<blockquote>
<p>è„‘è‚¿ç˜¤ä¼šå»¶è¿Ÿè¿›ä¸€æ­¥çš„æ£€æŸ¥æ ‡å‡†é¢„å¤„ç†å·¥ä½œæµç¨‹ã€‚è„‘è¡¥æŠ€æœ¯æä¾›äº†ä¸€ä¸ªå¯è¡Œçš„è§£å†³æ–¹æ¡ˆï¼Œå°½ç®¡è¿™å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œä½†å¯¹äºå¤„ç†è‚¿ç˜¤ç»„ç»‡ä»¥æé«˜è¯Šæ–­å’Œæ²»ç–—ç²¾åº¦æ˜¯å¿…è¦çš„ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°åŸºäºä¼ ç»ŸU-Netçš„ç”Ÿæˆæ¨¡å‹å¸¸å¸¸é¢ä¸´æ•è·å†…åœ¨äºè„‘æˆåƒä¸­çš„å¤æ‚éçº¿æ€§æ½œåœ¨è¡¨ç¤ºçš„éš¾é¢˜ã€‚ä¸ºäº†å®Œæˆé«˜è´¨é‡çš„è„‘ç»„ç»‡é‡å»ºï¼Œè¿™é¡¹å·¥ä½œæå‡ºäº†DiffKAN-Inpaintingæ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§å°†æ‰©æ•£æ¨¡å‹ä¸Kolmogorov-Arnoldç½‘ç»œæ¶æ„ç›¸ç»“åˆçš„åˆ›æ–°æ–¹æ³•ã€‚åœ¨é™å™ªè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†RePaintæ–¹æ³•å’Œè‚¿ç˜¤ä¿¡æ¯æ¥ç”Ÿæˆå…·æœ‰æ›´é«˜ä¿çœŸåº¦å’Œå¹³æ»‘è¾¹ç¼˜çš„å›¾åƒã€‚å®šæ€§å’Œå®šé‡ç»“æœå‡è¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬æå‡ºçš„DiffKAN-Inpaintingåœ¨BraTSæ•°æ®é›†ä¸Šè¿›è¡Œäº†æ›´è¯¦ç»†å’Œç°å®çš„é‡å»ºã€‚ä»æ¶ˆèç ”ç©¶ä¸­è·å¾—çš„çŸ¥è¯†ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†å¹³è¡¡æ€§èƒ½å’Œè®¡ç®—æˆæœ¬çš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.16771v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºä¸€ç§åä¸ºDiffKAN-Inpaintingçš„æ–°æ–¹æ³•ï¼Œç»“åˆæ‰©æ•£æ¨¡å‹å’ŒKolmogorov-Arnoldç½‘ç»œæ¶æ„ï¼Œç”¨äºé«˜è´¨é‡é‡å»ºå¥åº·è„‘ç»„ç»‡ã€‚è¯¥æ–¹æ³•è§£å†³äº†ä¼ ç»ŸU-Netç”Ÿæˆæ¨¡å‹åœ¨æ•æ‰å¤æ‚éçº¿æ€§æ½œåœ¨è¡¨ç¤ºæ–¹é¢çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†è„‘è‚¿ç˜¤ç»„ç»‡æ—¶ã€‚é€šè¿‡å¼•å…¥RePaintæ–¹æ³•å’Œè‚¿ç˜¤ä¿¡æ¯ï¼Œæé«˜äº†å›¾åƒä¿çœŸåº¦å’Œè¾¹ç¼˜å¹³æ»‘åº¦ã€‚åœ¨BraTSæ•°æ®é›†ä¸Šçš„ç»“æœè¯æ˜ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒDiffKAN-Inpaintingçš„é‡å»ºæ›´ä¸ºè¯¦ç»†å’ŒçœŸå®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ä¸­æŒ‡å‡ºè„‘è‚¿ç˜¤å¯¹é¢„å¤„ç†å·¥ä½œæµç¨‹çš„å½±å“ï¼Œå¼ºè°ƒäº†è„‘è¡¥å…¨æŠ€æœ¯åœ¨æ”¹å–„è¯Šæ–­å’Œæ²»ç–—ç²¾åº¦æ–¹é¢çš„å¿…è¦æ€§ã€‚</li>
<li>ä¼ ç»ŸU-Netç”Ÿæˆæ¨¡å‹åœ¨å¤„ç†å¤æ‚çš„è„‘æˆåƒæ•°æ®æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œéš¾ä»¥æ•æ‰éçº¿æ€§æ½œåœ¨è¡¨ç¤ºã€‚</li>
<li>DiffKAN-Inpaintingæ–¹æ³•ç»“åˆäº†æ‰©æ•£æ¨¡å‹å’ŒKolmogorov-Arnoldç½‘ç»œæ¶æ„ï¼Œæ—¨åœ¨ç”Ÿæˆé«˜è´¨é‡çš„å¥åº·è„‘ç»„ç»‡é‡å»ºã€‚</li>
<li>RePaintæ–¹æ³•å’Œè‚¿ç˜¤ä¿¡æ¯çš„å¼•å…¥æé«˜äº†å›¾åƒç”Ÿæˆçš„ä¿çœŸåº¦å’Œè¾¹ç¼˜å¹³æ»‘åº¦ã€‚</li>
<li>åœ¨BraTSæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDiffKAN-Inpaintingä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å¤Ÿæä¾›æ›´ä¸ºè¯¦ç»†å’ŒçœŸå®çš„é‡å»ºç»“æœã€‚</li>
<li>æ¶ˆèç ”ç©¶ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†å¹³è¡¡æ€§èƒ½å’Œè®¡ç®—æˆæœ¬çš„è§è§£ã€‚</li>
<li>è¯¥æ–¹æ³•å¯¹äºæ”¹å–„è„‘è‚¿ç˜¤è¯Šæ–­å’Œå¤„ç†å…·æœ‰æ½œåœ¨çš„åº”ç”¨ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.16771">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4b3e3c3ec1ae4b6ece50a639567de01b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ecad256aa331c5321d9abc10725ebe4d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-14ddbcf6d0076cdc7a8ef10c3295d56e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-23b320d150f888b0ac664958285dda6b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-864857c43fe0a6a419d2932424a24711.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="DOSE3-Diffusion-based-Out-of-distribution-detection-on-SE-3-trajectories"><a href="#DOSE3-Diffusion-based-Out-of-distribution-detection-on-SE-3-trajectories" class="headerlink" title="DOSE3 : Diffusion-based Out-of-distribution detection on SE(3)   trajectories"></a>DOSE3 : Diffusion-based Out-of-distribution detection on SE(3)   trajectories</h2><p><strong>Authors:Hongzhe Cheng, Tianyou Zheng, Tianyi Zhang, Matthew Johnson-Roberson, Weiming Zhi</strong></p>
<p>Out-of-Distribution(OOD) detection, a fundamental machine learning task aimed at identifying abnormal samples, traditionally requires model retraining for different inlier distributions. While recent research demonstrates the applicability of diffusion models to OOD detection, existing approaches are limited to Euclidean or latent image spaces. Our work extends OOD detection to trajectories in the Special Euclidean Group in 3D ($\mathbb{SE}(3)$), addressing a critical need in computer vision, robotics, and engineering applications that process object pose sequences in $\mathbb{SE}(3)$. We present $\textbf{D}$iffusion-based $\textbf{O}$ut-of-distribution detection on $\mathbb{SE}(3)$ ($\mathbf{DOSE3}$), a novel OOD framework that extends diffusion to a unified sample space of $\mathbb{SE}(3)$ pose sequences. Through extensive validation on multiple benchmark datasets, we demonstrate $\mathbf{DOSE3}$â€™s superior performance compared to state-of-the-art OOD detection frameworks. </p>
<blockquote>
<p>å¼‚å¸¸æ ·æœ¬æ£€æµ‹ï¼ˆOut-of-Distributionï¼ŒOODï¼‰æ˜¯æœºå™¨å­¦ä¹ é¢†åŸŸä¸­çš„ä¸€é¡¹åŸºç¡€ä»»åŠ¡ï¼Œæ—¨åœ¨è¯†åˆ«å¼‚å¸¸æ ·æœ¬ã€‚ä¼ ç»Ÿçš„OODæ£€æµ‹éœ€è¦å¯¹ä¸åŒçš„å†…éƒ¨æ•°æ®åˆ†å¸ƒè¿›è¡Œæ¨¡å‹é‡è®­ç»ƒã€‚è™½ç„¶æœ€è¿‘çš„ç ”ç©¶æ˜¾ç¤ºäº†æ‰©æ•£æ¨¡å‹åœ¨OODæ£€æµ‹ä¸­çš„åº”ç”¨æ½œåŠ›ï¼Œä½†ç°æœ‰æ–¹æ³•ä»…é™äºæ¬§å‡ é‡Œå¾—ç©ºé—´æˆ–æ½œåœ¨å›¾åƒç©ºé—´ã€‚æˆ‘ä»¬çš„å·¥ä½œå°†OODæ£€æµ‹æ‰©å±•åˆ°ä¸‰ç»´ç‰¹æ®Šæ¬§å‡ é‡Œå¾—ç¾¤ï¼ˆ$\mathbb{SE}(3)$ï¼‰ä¸­çš„è½¨è¿¹ï¼Œè§£å†³äº†è®¡ç®—æœºè§†è§‰ã€æœºå™¨äººæŠ€æœ¯å’Œå·¥ç¨‹åº”ç”¨ä¸­å¤„ç†$\mathbb{SE}(3)$å†…å¯¹è±¡å§¿æ€åºåˆ—çš„å…³é”®éœ€æ±‚ã€‚æˆ‘ä»¬æå‡ºäº†åŸºäºæ‰©æ•£çš„$\mathbb{SE}(3)$ä¸Šçš„å¼‚å¸¸æ£€æµ‹ï¼ˆDOSE3ï¼‰æ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„OODæ¡†æ¶ï¼Œå°†æ‰©æ•£æ‰©å±•åˆ°ç»Ÿä¸€çš„$\mathbb{SE}(3)$å§¿æ€åºåˆ—æ ·æœ¬ç©ºé—´ã€‚é€šè¿‡å¤šä¸ªåŸºå‡†æ•°æ®é›†çš„å¤§é‡éªŒè¯ï¼Œæˆ‘ä»¬è¯æ˜äº†DOSE3ç›¸è¾ƒäºæœ€æ–°OODæ£€æµ‹æ¡†æ¶çš„å“è¶Šæ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.16725v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†å°†æ‰©æ•£æ¨¡å‹åº”ç”¨äº$\mathbb{SE}(3)$ä¸Šçš„å¼‚å¸¸æ ·æœ¬æ£€æµ‹é—®é¢˜ã€‚ä¼ ç»Ÿæ–¹æ³•é€šå¸¸éœ€è¦åœ¨ä¸åŒçš„å†…éƒ¨åˆ†å¸ƒä¸Šé‡æ–°è®­ç»ƒæ¨¡å‹ï¼Œè€Œæœ¬æ–‡æå‡ºçš„$\mathbf{DOSE3}$æ¡†æ¶æ‰©å±•äº†æ‰©æ•£æ¨¡å‹ï¼Œç»Ÿä¸€å¤„ç†$\mathbb{SE}(3)$å§¿æ€åºåˆ—çš„æ ·æœ¬ç©ºé—´ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸ŠéªŒè¯äº†å…¶ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡ç ”ç©¶çš„æ˜¯æœºå™¨å­¦ä¹ ä¸­è¯†åˆ«å¼‚å¸¸æ ·æœ¬çš„OODæ£€æµ‹ä»»åŠ¡ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¯¹è±¡å§¿æ€åºåˆ—çš„è®¡ç®—æœºè§†è§‰ã€æœºå™¨äººå·¥ç¨‹å’Œå·¥ç¨‹åº”ç”¨ä¸­çš„OODæ£€æµ‹ã€‚</li>
<li>ä¼ ç»ŸOODæ£€æµ‹æ–¹æ³•é€šå¸¸éœ€è¦é’ˆå¯¹ä¸åŒçš„å†…éƒ¨åˆ†å¸ƒè¿›è¡Œæ¨¡å‹é‡æ–°è®­ç»ƒã€‚</li>
<li>æœ¬æ–‡å°†æ‰©æ•£æ¨¡å‹åº”ç”¨äº$\mathbb{SE}(3)$ä¸Šçš„OODæ£€æµ‹ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„åº”ç”¨æ–¹å‘ã€‚</li>
<li>æå‡ºçš„$\mathbf{DOSE3}$æ¡†æ¶ç»Ÿä¸€å¤„ç†$\mathbb{SE}(3)$å§¿æ€åºåˆ—çš„æ ·æœ¬ç©ºé—´ï¼Œçªç ´äº†ä»¥å¾€ä»…åœ¨æ¬§å‡ é‡Œå¾—æˆ–æ½œåœ¨å›¾åƒç©ºé—´è¿›è¡ŒOODæ£€æµ‹çš„é™åˆ¶ã€‚</li>
<li>é€šè¿‡åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„éªŒè¯ï¼Œè¯æ˜äº†$\mathbf{DOSE3}$æ¡†æ¶ç›¸è¾ƒäºç°æœ‰OODæ£€æµ‹æ¡†æ¶å…·æœ‰ä¼˜è¶Šæ€§ã€‚</li>
<li>$\mathbf{DOSE3}$æ¡†æ¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå°¤å…¶åœ¨å¤„ç†å¯¹è±¡å§¿æ€åºåˆ—çš„è®¡ç®—æœºè§†è§‰å’Œæœºå™¨äººæŠ€æœ¯ä¸­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.16725">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-452a488f2d1f0ab250dee787e3f0278d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-191aaade413bcd4e242e302f2505395e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d2348483452014bc6bc97a17de89a7fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c0fea8320068d1d5797fb57928f0f48.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="High-resolution-Rainy-Image-Synthesis-Learning-from-Rendering"><a href="#High-resolution-Rainy-Image-Synthesis-Learning-from-Rendering" class="headerlink" title="High-resolution Rainy Image Synthesis: Learning from Rendering"></a>High-resolution Rainy Image Synthesis: Learning from Rendering</h2><p><strong>Authors:Kaibin Zhou, Shengjie Zhao, Hao Deng, Lin Zhang</strong></p>
<p>Currently, there are few effective methods for synthesizing a mass of high-resolution rainy images in complex illumination conditions. However, these methods are essential for synthesizing large-scale high-quality paired rainy-clean image datasets, which can train deep learning-based single image rain removal models capable of generalizing to various illumination conditions. Therefore, we propose a practical two-stage learning-from-rendering pipeline for high-resolution rainy image synthesis. The pipeline combines the benefits of the realism of rendering-based methods and the high-efficiency of learning-based methods, providing the possibility of creating large-scale high-quality paired rainy-clean image datasets. In the rendering stage, we use a rendering-based method to create a High-resolution Rainy Image (HRI) dataset, which contains realistic high-resolution paired rainy-clean images of multiple scenes and various illumination conditions. In the learning stage, to learn illumination information from background images for high-resolution rainy image generation, we propose a High-resolution Rainy Image Generation Network (HRIGNet). HRIGNet is designed to introduce a guiding diffusion model in the Latent Diffusion Model, which provides additional guidance information for high-resolution image synthesis. In our experiments, HRIGNet is able to synthesize high-resolution rainy images up to 2048x1024 resolution. Rain removal experiments on real dataset validate that our method can help improve the robustness of deep derainers to real rainy images. To make our work reproducible, source codes and the dataset have been released at <a target="_blank" rel="noopener" href="https://kb824999404.github.io/HRIG/">https://kb824999404.github.io/HRIG/</a>. </p>
<blockquote>
<p>ç›®å‰ï¼Œåœ¨å¤æ‚å…‰ç…§æ¡ä»¶ä¸‹åˆæˆå¤§é‡é«˜åˆ†è¾¨ç‡é›¨å¤©å›¾åƒçš„æœ‰æ•ˆæ–¹æ³•è¿˜æ¯”è¾ƒå°‘ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¯¹äºåˆæˆå¤§è§„æ¨¡é«˜è´¨é‡é…å¯¹é›¨å¤©-æ¸…æ´å›¾åƒæ•°æ®é›†è‡³å…³é‡è¦ï¼Œè¿™äº›æ•°æ®é›†å¯ä»¥è®­ç»ƒåŸºäºæ·±åº¦å­¦ä¹ çš„å•å›¾åƒå»é›¨æ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿæ¨å¹¿è‡³å„ç§å…‰ç…§æ¡ä»¶ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å®ç”¨çš„åŸºäºæ¸²æŸ“çš„ä¸¤é˜¶æ®µå­¦ä¹ ç®¡é“ï¼Œç”¨äºé«˜åˆ†è¾¨ç‡é›¨å¤©å›¾åƒåˆæˆã€‚è¯¥ç®¡é“ç»“åˆäº†åŸºäºæ¸²æŸ“çš„æ–¹æ³•å’ŒåŸºäºå­¦ä¹ çš„æ–¹æ³•çš„ç°å®ä¸»ä¹‰ä¼˜ç‚¹ï¼Œä»¥åŠé«˜æ•ˆç‡ä¼˜ç‚¹ï¼Œåˆ›é€ äº†æ„å»ºå¤§è§„æ¨¡é«˜è´¨é‡é…å¯¹é›¨å¤©-æ¸…æ´å›¾åƒæ•°æ®é›†çš„å¯èƒ½æ€§ã€‚åœ¨æ¸²æŸ“é˜¶æ®µï¼Œæˆ‘ä»¬ä½¿ç”¨åŸºäºæ¸²æŸ“çš„æ–¹æ³•åˆ›å»ºé«˜åˆ†è¾¨ç‡é›¨å¤©å›¾åƒï¼ˆHRIï¼‰æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«å¤šä¸ªåœºæ™¯å’Œå¤šç§å…‰ç…§æ¡ä»¶ä¸‹çš„çœŸå®é«˜åˆ†è¾¨ç‡é…å¯¹é›¨å¤©-æ¸…æ´å›¾åƒã€‚åœ¨å­¦ä¹ é˜¶æ®µï¼Œä¸ºäº†ä»èƒŒæ™¯å›¾åƒä¸­å­¦ä¹ å…‰ç…§ä¿¡æ¯ä»¥ç”Ÿæˆé«˜åˆ†è¾¨ç‡é›¨å¤©å›¾åƒï¼Œæˆ‘ä»¬æå‡ºäº†é«˜åˆ†è¾¨ç‡é›¨å¤©å›¾åƒç”Ÿæˆç½‘ç»œï¼ˆHRIGNetï¼‰ã€‚HRIGNetçš„è®¾è®¡æ˜¯åœ¨æ½œåœ¨æ‰©æ•£æ¨¡å‹ä¸­å¼•å…¥æŒ‡å¯¼æ‰©æ•£æ¨¡å‹ï¼Œä¸ºé«˜åˆ†è¾¨ç‡å›¾åƒåˆæˆæä¾›é¢å¤–çš„æŒ‡å¯¼ä¿¡æ¯ã€‚åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼ŒHRIGNetèƒ½å¤Ÿåˆæˆé«˜è¾¾2048x1024åˆ†è¾¨ç‡çš„é«˜åˆ†è¾¨ç‡é›¨å¤©å›¾åƒã€‚åœ¨çœŸå®æ•°æ®é›†ä¸Šçš„å»é›¨å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥å¸®åŠ©æé«˜æ·±åº¦å»é›¨å™¨å¯¹çœŸå®é›¨å¤©å›¾åƒçš„ç¨³å¥æ€§ã€‚ä¸ºäº†ä½¿æˆ‘ä»¬çš„å·¥ä½œå…·æœ‰å¯é‡å¤æ€§ï¼Œæºä»£ç å’Œæ•°æ®é›†å·²åœ¨<a target="_blank" rel="noopener" href="https://kb824999404.github.io/HRIG/">https://kb824999404.github.io/HRIG/</a>å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.16421v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§å®ç”¨çš„ä¸¤é˜¶æ®µå­¦ä¹ æ¸²æŸ“ç®¡é“ï¼Œç”¨äºé«˜åˆ†è¾¨ç‡é›¨å¤©å›¾åƒåˆæˆã€‚è¯¥æ–¹æ³•ç»“åˆäº†æ¸²æŸ“æ–¹æ³•çš„ç°å®æ€§å’Œå­¦ä¹ æ–¹æ³•çš„æ•ˆç‡ï¼Œå¯ä»¥åˆ›å»ºå¤§è§„æ¨¡é«˜è´¨é‡é…å¯¹é›¨å¤©-æ¸…æ´å›¾åƒæ•°æ®é›†ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µçš„æ¸²æŸ“è¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨åŸºäºæ¸²æŸ“çš„æ–¹æ³•åˆ›å»ºé«˜åˆ†è¾¨ç‡é›¨å¤©å›¾åƒï¼ˆHRIï¼‰æ•°æ®é›†ï¼ŒåŒ…å«å¤šä¸ªåœºæ™¯å’Œå¤šç§ç…§æ˜æ¡ä»¶ä¸‹çš„çœŸå®é«˜åˆ†è¾¨ç‡é…å¯¹é›¨å¤©-æ¸…æ´å›¾åƒã€‚åœ¨ç¬¬äºŒé˜¶æ®µçš„å­¦ä¹ ä¸­ï¼Œä¸ºäº†ä»èƒŒæ™¯å›¾åƒä¸­å­¦ä¹ ç…§æ˜ä¿¡æ¯è¿›è¡Œé«˜åˆ†è¾¨ç‡é›¨å¤©å›¾åƒç”Ÿæˆï¼Œæå‡ºäº†é«˜åˆ†è¾¨ç‡é›¨å¤©å›¾åƒç”Ÿæˆç½‘ç»œï¼ˆHRIGNetï¼‰ã€‚HRIGNetåœ¨æ½œåœ¨æ‰©æ•£æ¨¡å‹ä¸­å¼•å…¥äº†æŒ‡å¯¼æ‰©æ•£æ¨¡å‹ï¼Œä¸ºé«˜åˆ†è¾¨ç‡å›¾åƒåˆæˆæä¾›äº†é¢å¤–çš„æŒ‡å¯¼ä¿¡æ¯ã€‚å®éªŒè¡¨æ˜ï¼ŒHRIGNetèƒ½å¤Ÿåˆæˆé«˜è¾¾2048x1024åˆ†è¾¨ç‡çš„é«˜åˆ†è¾¨ç‡é›¨å¤©å›¾åƒï¼Œå¹¶ä¸”åœ¨çœŸå®æ•°æ®é›†ä¸Šçš„å»é›¨å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•èƒ½æé«˜æ·±åº¦å»é›¨å™¨å¯¹çœŸå®é›¨å¤©å›¾åƒçš„é²æ£’æ€§ã€‚ç›¸å…³æºä»£ç å’Œæ•°æ®é›†å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://kb824999404.github.io/HRIG/">é“¾æ¥</a>ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§å®ç”¨çš„ä¸¤é˜¶æ®µå­¦ä¹ æ¸²æŸ“ç®¡é“ï¼Œç”¨äºåœ¨å¤æ‚å…‰ç…§æ¡ä»¶ä¸‹åˆæˆå¤§é‡é«˜åˆ†è¾¨ç‡é›¨å¤©å›¾åƒã€‚</li>
<li>åˆ›å»ºäº†é«˜åˆ†è¾¨ç‡é›¨å¤©å›¾åƒï¼ˆHRIï¼‰æ•°æ®é›†ï¼ŒåŒ…å«å¤šä¸ªåœºæ™¯å’Œå¤šç§ç…§æ˜æ¡ä»¶ä¸‹çš„çœŸå®é«˜åˆ†è¾¨ç‡é…å¯¹é›¨å¤©-æ¸…æ´å›¾åƒã€‚</li>
<li>æå‡ºäº†é«˜åˆ†è¾¨ç‡é›¨å¤©å›¾åƒç”Ÿæˆç½‘ç»œï¼ˆHRIGNetï¼‰ï¼Œèƒ½å¤Ÿå¼•å…¥æŒ‡å¯¼æ‰©æ•£æ¨¡å‹ä»¥æä¾›é«˜åˆ†è¾¨ç‡å›¾åƒåˆæˆçš„é¢å¤–æŒ‡å¯¼ä¿¡æ¯ã€‚</li>
<li>HRIGNetèƒ½å¤Ÿåˆæˆé«˜è¾¾2048x1024åˆ†è¾¨ç‡çš„é«˜åˆ†è¾¨ç‡é›¨å¤©å›¾åƒã€‚</li>
<li>åœ¨çœŸå®æ•°æ®é›†ä¸Šçš„å»é›¨å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•èƒ½æé«˜æ·±åº¦å»é›¨å™¨å¯¹çœŸå®é›¨å¤©å›¾åƒçš„é²æ£’æ€§ã€‚</li>
<li>å…¬å¼€äº†æºä»£ç å’Œæ•°æ®é›†ï¼Œä¾¿äºå…¶ä»–ç ”ç©¶è€…è¿›è¡Œå¤ç°å’Œç ”ç©¶ã€‚</li>
<li>è¯¥æ–¹æ³•ç»“åˆäº†æ¸²æŸ“æ–¹æ³•çš„ç°å®æ€§å’Œå­¦ä¹ æ–¹æ³•çš„æ•ˆç‡ï¼Œä¸ºåˆ›å»ºå¤§è§„æ¨¡é«˜è´¨é‡é…å¯¹é›¨å¤©-æ¸…æ´å›¾åƒæ•°æ®é›†æä¾›äº†å¯èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.16421">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-79953fb66ed7f1cda121bc9969ca117e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-71aa642987215b0f9338c6196c269cb1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1d8b2d0e72d9f37c72d64e4397ddec69.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c2361e2235345602c8b9f85d49df73ad.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Concept-Corrector-Erase-concepts-on-the-fly-for-text-to-image-diffusion-models"><a href="#Concept-Corrector-Erase-concepts-on-the-fly-for-text-to-image-diffusion-models" class="headerlink" title="Concept Corrector: Erase concepts on the fly for text-to-image diffusion   models"></a>Concept Corrector: Erase concepts on the fly for text-to-image diffusion   models</h2><p><strong>Authors:Zheling Meng, Bo Peng, Xiaochuan Jin, Yueming Lyu, Wei Wang, Jing Dong</strong></p>
<p>Text-to-image diffusion models have demonstrated the underlying risk of generating various unwanted content, such as sexual elements. To address this issue, the task of concept erasure has been introduced, aiming to erase any undesired concepts that the models can generate. Previous methods, whether training-based or training-free, have primarily focused on the input side, i.e. texts. However, they often suffer from incomplete erasure due to limitations in the generalization from limited prompts to diverse image content. In this paper, motivated by the notion that concept erasure on the output side, i.e. generated images, may be more direct and effective, we propose to check concepts based on intermediate-generated images and correct them in the remainder of the generation process. Two key challenges are identified, i.e. determining the presence of target concepts during generation and replacing them on the fly. Leveraging the generation mechanism of diffusion models, we present the Concept Corrector, which incorporates the Generation Check Mechanism and the Concept Removal Attention. This method can identify the generated features associated with target concepts and replace them using pre-defined negative prompts, thereby achieving concept erasure. It requires no changes to model parameters and only relies on a given concept name and its replacement content. To the best of our knowledge, this is the first erasure method based on intermediate-generated images. The experiments on various concepts demonstrate its impressive erasure performance. Code: <a target="_blank" rel="noopener" href="https://github.com/RichardSunnyMeng/ConceptCorrector">https://github.com/RichardSunnyMeng/ConceptCorrector</a>. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹å·²ç»æ˜¾ç¤ºå‡ºç”Ÿæˆå„ç§ä¸éœ€è¦å†…å®¹ï¼ˆå¦‚æ€§å…ƒç´ ï¼‰çš„æ½œåœ¨é£é™©ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå¼•å…¥äº†æ¦‚å¿µæ¶ˆé™¤çš„ä»»åŠ¡ï¼Œæ—¨åœ¨æ¶ˆé™¤æ¨¡å‹å¯èƒ½ç”Ÿæˆçš„ä¸å¿…è¦æ¦‚å¿µã€‚ä»¥å‰çš„æ–¹æ³•ï¼Œæ— è®ºæ˜¯åŸºäºè®­ç»ƒçš„ï¼Œè¿˜æ˜¯å…è®­ç»ƒçš„ï¼Œéƒ½ä¸»è¦é›†ä¸­åœ¨è¾“å…¥ç«¯ï¼Œå³æ–‡æœ¬ä¸Šã€‚ç„¶è€Œï¼Œç”±äºä»æœ‰é™çš„æç¤ºæ¨å¹¿åˆ°å¤šæ ·åŒ–çš„å›¾åƒå†…å®¹çš„å±€é™æ€§ï¼Œå®ƒä»¬é€šå¸¸é­å—ä¸å®Œæ•´çš„æ¶ˆé™¤ç»“æœã€‚æœ¬æ–‡å—åˆ°è¾“å‡ºä¾§ï¼ˆå³ç”Ÿæˆçš„å›¾åƒï¼‰çš„æ¦‚å¿µæ¶ˆé™¤å¯èƒ½æ›´ç›´æ¥æœ‰æ•ˆçš„æ¦‚å¿µçš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºåŸºäºä¸­é—´ç”Ÿæˆçš„å›¾åƒæ£€æŸ¥æ¦‚å¿µå¹¶åœ¨å‰©ä½™ç”Ÿæˆè¿‡ç¨‹ä¸­çº æ­£å®ƒä»¬ã€‚ç¡®å®šäº†ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼Œå³åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ç¡®å®šç›®æ ‡æ¦‚å¿µçš„å­˜åœ¨å¹¶å³æ—¶æ›¿æ¢å®ƒä»¬ã€‚åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆæœºåˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†æ¦‚å¿µæ ¡æ­£å™¨ï¼ˆConcept Correctorï¼‰ï¼Œå®ƒç»“åˆäº†ç”Ÿæˆæ£€æŸ¥æœºåˆ¶å’Œæ¦‚å¿µç§»é™¤æ³¨æ„åŠ›ï¼ˆConcept Removal Attentionï¼‰ã€‚è¿™ç§æ–¹æ³•å¯ä»¥è¯†åˆ«ä¸ç›®æ ‡æ¦‚å¿µç›¸å…³çš„ç”Ÿæˆç‰¹å¾å¹¶ç”¨é¢„å®šä¹‰çš„è´Ÿé¢æç¤ºæ›¿æ¢å®ƒä»¬ï¼Œä»è€Œå®ç°æ¦‚å¿µæ¶ˆé™¤ã€‚å®ƒä¸éœ€è¦å¯¹æ¨¡å‹å‚æ•°è¿›è¡Œä»»ä½•æ›´æ”¹ï¼Œä»…ä¾èµ–äºç»™å®šçš„æ¦‚å¿µåç§°åŠå…¶æ›¿æ¢å†…å®¹ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯åŸºäºä¸­é—´ç”Ÿæˆçš„å›¾åƒçš„é¦–ä¸ªæ¶ˆé™¤æ–¹æ³•ã€‚å¯¹å„ç§æ¦‚å¿µçš„å®éªŒè¯æ˜å…¶ä»¤äººå°è±¡æ·±åˆ»çš„æ¶ˆé™¤æ€§èƒ½ã€‚ä»£ç åœ°å€ï¼š<a target="_blank" rel="noopener" href="https://github.com/RichardSunnyMeng/ConceptCorrector">https://github.com/RichardSunnyMeng/ConceptCorrector</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.16368v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>é’ˆå¯¹æ–‡æœ¬è½¬å›¾åƒæ‰©æ•£æ¨¡å‹å¯èƒ½ç”ŸæˆåŒ…å«ä¸æƒ³è¦çš„å†…å®¹ï¼ˆå¦‚æ€§ç›¸å…³å…ƒç´ ï¼‰çš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†æ¦‚å¿µä¿®æ­£å™¨ï¼ˆConcept Correctorï¼‰ã€‚å®ƒåŸºäºä¸­é—´ç”Ÿæˆçš„å›¾åƒæ£€æŸ¥æ¦‚å¿µå¹¶åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­è¿›è¡Œä¿®æ­£ï¼Œæ— éœ€æ”¹å˜æ¨¡å‹å‚æ•°ï¼Œåªéœ€æä¾›æ¦‚å¿µåç§°å’Œæ›¿æ¢å†…å®¹å³å¯ã€‚æ­¤ä¸ºåŸºäºä¸­é—´ç”Ÿæˆå›¾åƒçš„é¦–ä¸ªæ“¦é™¤æ–¹æ³•ï¼Œå®éªŒè¯æ˜å…¶æ“¦é™¤æ€§èƒ½ä»¤äººå°è±¡æ·±åˆ»ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ–‡æœ¬è½¬å›¾åƒæ‰©æ•£æ¨¡å‹å­˜åœ¨ç”Ÿæˆä¸æƒ³è¦å†…å®¹çš„é£é™©ï¼Œå¦‚æ€§ç›¸å…³å…ƒç´ ã€‚</li>
<li>æ¦‚å¿µä¿®æ­£å™¨ï¼ˆConcept Correctorï¼‰æ—¨åœ¨åŸºäºä¸­é—´ç”Ÿæˆçš„å›¾åƒæ£€æŸ¥å¹¶ä¿®æ­£ä¸éœ€è¦çš„æ¦‚å¿µã€‚</li>
<li>è¯¥æ–¹æ³•æ— éœ€æ”¹å˜æ¨¡å‹å‚æ•°ï¼Œåªéœ€æä¾›æ¦‚å¿µåç§°å’Œæ›¿æ¢å†…å®¹ã€‚</li>
<li>æ¦‚å¿µä¿®æ­£å™¨åŒ…å«ç”Ÿæˆæ£€æŸ¥æœºåˆ¶å’Œæ¦‚å¿µç§»é™¤æ³¨æ„åŠ›ã€‚</li>
<li>æ–¹æ³•èƒ½å¤Ÿè¯†åˆ«ä¸ç›®æ ‡æ¦‚å¿µç›¸å…³çš„ç”Ÿæˆç‰¹å¾ï¼Œå¹¶ä½¿ç”¨é¢„å®šä¹‰çš„è´Ÿé¢æç¤ºè¿›è¡Œæ›¿æ¢ã€‚</li>
<li>æ­¤ä¸ºé¦–ä¸ªåŸºäºä¸­é—´ç”Ÿæˆå›¾åƒçš„æ¦‚å¿µæ“¦é™¤æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.16368">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-67d1d2ad59d92a9096a9dba25b7d97bf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-38209559ac16809401e7de074a3252ab.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d5a74e4000554f9cebc18daa1877bce2.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="DualNeRF-Text-Driven-3D-Scene-Editing-via-Dual-Field-Representation"><a href="#DualNeRF-Text-Driven-3D-Scene-Editing-via-Dual-Field-Representation" class="headerlink" title="DualNeRF: Text-Driven 3D Scene Editing via Dual-Field Representation"></a>DualNeRF: Text-Driven 3D Scene Editing via Dual-Field Representation</h2><p><strong>Authors:Yuxuan Xiong, Yue Shi, Yishun Dou, Bingbing Ni</strong></p>
<p>Recently, denoising diffusion models have achieved promising results in 2D image generation and editing. Instruct-NeRF2NeRF (IN2N) introduces the success of diffusion into 3D scene editing through an â€œIterative dataset updateâ€ (IDU) strategy. Though achieving fascinating results, IN2N suffers from problems of blurry backgrounds and trapping in local optima. The first problem is caused by IN2Nâ€™s lack of efficient guidance for background maintenance, while the second stems from the interaction between image editing and NeRF training during IDU. In this work, we introduce DualNeRF to deal with these problems. We propose a dual-field representation to preserve features of the original scene and utilize them as additional guidance to the model for background maintenance during IDU. Moreover, a simulated annealing strategy is embedded into IDU to endow our model with the power of addressing local optima issues. A CLIP-based consistency indicator is used to further improve the editing quality by filtering out low-quality edits. Extensive experiments demonstrate that our method outperforms previous methods both qualitatively and quantitatively. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œå»å™ªæ‰©æ•£æ¨¡å‹åœ¨2Då›¾åƒç”Ÿæˆå’Œç¼–è¾‘æ–¹é¢å–å¾—äº†æœ‰å‰æ™¯çš„ç»“æœã€‚Instruct-NeRF2NeRFï¼ˆIN2Nï¼‰é€šè¿‡â€œè¿­ä»£æ•°æ®é›†æ›´æ–°â€ï¼ˆIDUï¼‰ç­–ç•¥ï¼Œå°†æ‰©æ•£çš„æˆåŠŸå¼•å…¥3Dåœºæ™¯ç¼–è¾‘ã€‚å°½ç®¡å–å¾—äº†ä»¤äººç€è¿·çš„ç»“æœï¼Œä½†IN2Nä»å­˜åœ¨èƒŒæ™¯æ¨¡ç³Šå’Œé™·å…¥å±€éƒ¨æœ€ä¼˜çš„é—®é¢˜ã€‚ç¬¬ä¸€ä¸ªé—®é¢˜æ˜¯ç”±IN2Nå¯¹èƒŒæ™¯ç»´æŠ¤çš„æœ‰æ•ˆæŒ‡å¯¼ä¸è¶³é€ æˆçš„ï¼Œè€Œç¬¬äºŒä¸ªé—®é¢˜åˆ™æºäºå›¾åƒç¼–è¾‘å’ŒNeRFè®­ç»ƒåœ¨IDUè¿‡ç¨‹ä¸­çš„ç›¸äº’ä½œç”¨ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥DualNeRFæ¥è§£å†³è¿™äº›é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŒåœºè¡¨ç¤ºæ³•ï¼Œä»¥ä¿ç•™åŸå§‹åœºæ™¯çš„ç‰¹å¾ï¼Œå¹¶å°†å…¶ä½œä¸ºé™„åŠ æŒ‡å¯¼ï¼Œç”¨äºåœ¨IDUæœŸé—´è¿›è¡ŒèƒŒæ™¯ç»´æŠ¤ã€‚æ­¤å¤–ï¼Œå°†æ¨¡æ‹Ÿé€€ç«ç­–ç•¥åµŒå…¥åˆ°IDUä¸­ï¼Œä½¿æˆ‘ä»¬çš„æ¨¡å‹å…·æœ‰è§£å†³å±€éƒ¨æœ€ä¼˜é—®é¢˜çš„èƒ½åŠ›ã€‚ä½¿ç”¨åŸºäºCLIPçš„ä¸€è‡´æ€§æŒ‡æ ‡æ¥è¿›ä¸€æ­¥æ”¹å–„ç¼–è¾‘è´¨é‡ï¼Œè¿‡æ»¤æ‰ä½è´¨é‡çš„ç¼–è¾‘ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è´¨é‡å’Œæ•°é‡ä¸Šéƒ½ä¼˜äºä¹‹å‰çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.16302v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è¿‘æœŸäºŒç»´å›¾åƒç”Ÿæˆå’Œç¼–è¾‘ä¸­è¡¨ç°è‰¯å¥½çš„é™å™ªæ‰©æ•£æ¨¡å‹ã€‚Instruct-NeRF2NeRFï¼ˆIN2Nï¼‰é€šè¿‡â€œè¿­ä»£æ•°æ®é›†æ›´æ–°â€ï¼ˆIDUï¼‰ç­–ç•¥å°†æ‰©æ•£æˆåŠŸå¼•å…¥ä¸‰ç»´åœºæ™¯ç¼–è¾‘ã€‚å°½ç®¡å–å¾—äº†ä»¤äººæƒŠå¹çš„ç»“æœï¼Œä½†IN2Nä»å­˜åœ¨èƒŒæ™¯æ¨¡ç³Šå’Œé™·å…¥å±€éƒ¨æœ€ä¼˜çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºDualNeRFè§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡åŒé‡åœºè¡¨ç¤ºæ³•ä¿ç•™åŸå§‹åœºæ™¯ç‰¹å¾ï¼Œå°†å…¶ä½œä¸ºæ¨¡å‹åœ¨IDUæœŸé—´çš„é¢å¤–æŒ‡å¯¼æ¥ç»´æŠ¤èƒŒæ™¯ã€‚åŒæ—¶ï¼Œå°†æ¨¡æ‹Ÿé€€ç«ç­–ç•¥åµŒå…¥IDUä¸­ï¼Œä»¥è§£å†³å±€éƒ¨æœ€ä¼˜é—®é¢˜ã€‚ä½¿ç”¨CLIPä¸€è‡´æ€§æŒ‡æ ‡è¿›ä¸€æ­¥æé«˜äº†ç¼–è¾‘è´¨é‡ï¼Œè¿‡æ»¤æ‰ä½è´¨é‡çš„ç¼–è¾‘ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è´¨é‡å’Œæ•°é‡ä¸Šéƒ½ä¼˜äºä»¥å‰çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Denoising diffusion models have achieved promising results in 2D image generation and editing.</li>
<li>Instruct-NeRF2NeRF (IN2N) introduces diffusion into 3D scene editing via an â€œIterative dataset updateâ€ (IDU) strategy.</li>
<li>IN2N faces challenges of blurry backgrounds and local optima issues.</li>
<li>DualNeRF proposes a dual-field representation to preserve original scene features and guide background maintenance during IDU.</li>
<li>A simulated annealing strategy is embedded into IDU to address local optima issues in the model.</li>
<li>A CLIP-based consistency indicator is used to improve edit quality by filtering out low-quality edits.</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.16302">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-43b46ec21fcdbf2c96531030a8d47efc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa85180418d25229a8491a3776e510b5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb437f5b0b3fcb69d4f0da6c833c302a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-090c4324269982390d90238c2705d884.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="PersGuard-Preventing-Malicious-Personalization-via-Backdoor-Attacks-on-Pre-trained-Text-to-Image-Diffusion-Models"><a href="#PersGuard-Preventing-Malicious-Personalization-via-Backdoor-Attacks-on-Pre-trained-Text-to-Image-Diffusion-Models" class="headerlink" title="PersGuard: Preventing Malicious Personalization via Backdoor Attacks on   Pre-trained Text-to-Image Diffusion Models"></a>PersGuard: Preventing Malicious Personalization via Backdoor Attacks on   Pre-trained Text-to-Image Diffusion Models</h2><p><strong>Authors:Xinwei Liu, Xiaojun Jia, Yuan Xun, Hua Zhang, Xiaochun Cao</strong></p>
<p>Diffusion models (DMs) have revolutionized data generation, particularly in text-to-image (T2I) synthesis. However, the widespread use of personalized generative models raises significant concerns regarding privacy violations and copyright infringement. To address these issues, researchers have proposed adversarial perturbation-based protection techniques. However, these methods have notable limitations, including insufficient robustness against data transformations and the inability to fully eliminate identifiable features of protected objects in the generated output. In this paper, we introduce PersGuard, a novel backdoor-based approach that prevents malicious personalization of specific images. Unlike traditional adversarial perturbation methods, PersGuard implant backdoor triggers into pre-trained T2I models, preventing the generation of customized outputs for designated protected images while allowing normal personalization for unprotected ones. Unfortunately, existing backdoor methods for T2I diffusion models fail to be applied to personalization scenarios due to the different backdoor objectives and the potential backdoor elimination during downstream fine-tuning processes. To address these, we propose three novel backdoor objectives specifically designed for personalization scenarios, coupled with backdoor retention loss engineered to resist downstream fine-tuning. These components are integrated into a unified optimization framework. Extensive experimental evaluations demonstrate PersGuardâ€™s effectiveness in preserving data privacy, even under challenging conditions including gray-box settings, multi-object protection, and facial identity scenarios. Our method significantly outperforms existing techniques, offering a more robust solution for privacy and copyright protection. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰åœ¨æ•°æ®ç”Ÿæˆæ–¹é¢å¸¦æ¥äº†é©å‘½æ€§çš„å˜åŒ–ï¼Œç‰¹åˆ«æ˜¯åœ¨æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰çš„åˆæˆä¸­ã€‚ç„¶è€Œï¼Œä¸ªæ€§åŒ–ç”Ÿæˆæ¨¡å‹çš„å¹¿æ³›ä½¿ç”¨å¼•å‘äº†å…³äºéšç§æ³„éœ²å’Œç‰ˆæƒä¾µçŠ¯çš„é‡å¤§æ‹…å¿§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œç ”ç©¶äººå‘˜å·²ç»æå‡ºäº†åŸºäºå¯¹æŠ—æ€§æ‰°åŠ¨ä¿æŠ¤çš„æŠ€æœ¯ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å­˜åœ¨æ˜æ˜¾çš„å±€é™æ€§ï¼ŒåŒ…æ‹¬å¯¹æŠ—æ•°æ®å˜æ¢çš„ç¨³å¥æ€§ä¸è¶³ï¼Œä»¥åŠæ— æ³•å®Œå…¨æ¶ˆé™¤ç”Ÿæˆè¾“å‡ºä¸­å—ä¿æŠ¤å¯¹è±¡çš„å¯è¯†åˆ«ç‰¹å¾ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°å‹çš„åé—¨æ–¹æ³•â€”â€”PersGuardï¼Œå®ƒå¯é˜²æ­¢ç‰¹å®šå›¾åƒè¢«æ¶æ„ä¸ªæ€§åŒ–ã€‚ä¸åŒäºä¼ ç»Ÿçš„å¯¹æŠ—æ€§æ‰°åŠ¨æ–¹æ³•ï¼ŒPersGuardå°†åé—¨è§¦å‘å™¨æ¤å…¥åˆ°é¢„è®­ç»ƒçš„T2Iæ¨¡å‹ä¸­ï¼Œé˜²æ­¢ä¸ºæŒ‡å®šçš„å—ä¿æŠ¤å›¾åƒç”Ÿæˆä¸ªæ€§åŒ–è¾“å‡ºï¼ŒåŒæ—¶å…è®¸å¯¹éä¿æŠ¤å›¾åƒè¿›è¡Œæ­£å¸¸çš„ä¸ªæ€§åŒ–æ“ä½œã€‚ç„¶è€Œï¼Œç°æœ‰çš„é’ˆå¯¹T2Iæ‰©æ•£æ¨¡å‹çš„åé—¨æ–¹æ³•æ— æ³•åº”ç”¨äºä¸ªæ€§åŒ–åœºæ™¯ï¼Œè¿™æ˜¯ç”±äºåé—¨ç›®æ ‡ä¸åŒï¼Œä»¥åŠåœ¨ä¸‹æ¸¸å¾®è°ƒè¿‡ç¨‹ä¸­æ½œåœ¨çš„åé—¨æ¶ˆé™¤ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸‰ç§ä¸“é—¨ä¸ºä¸ªæ€§åŒ–åœºæ™¯è®¾è®¡çš„å…¨æ–°åé—¨ç›®æ ‡ï¼Œå¹¶ç»“åˆæŠµæŠ—ä¸‹æ¸¸å¾®è°ƒçš„åé—¨ä¿ç•™æŸå¤±ã€‚è¿™äº›ç»„ä»¶è¢«æ•´åˆåˆ°ä¸€ä¸ªç»Ÿä¸€çš„ä¼˜åŒ–æ¡†æ¶ä¸­ã€‚å¤§é‡çš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒPersGuardåœ¨ä¿æŠ¤æ•°æ®éšç§æ–¹é¢éå¸¸æœ‰æ•ˆï¼Œå³ä½¿åœ¨ç°åº¦è®¾ç½®ã€å¤šå¯¹è±¡ä¿æŠ¤å’Œé¢éƒ¨èº«ä»½è¯†åˆ«ç­‰å…·æœ‰æŒ‘æˆ˜æ€§çš„æ¡ä»¶ä¸‹ä¹Ÿæ˜¯å¦‚æ­¤ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œä¸ºéšç§å’Œç‰ˆæƒä¿æŠ¤æä¾›äº†æ›´ç¨³å¥çš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.16167v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰åœ¨æ•°æ®ç”Ÿæˆé¢†åŸŸæ€èµ·äº†ä¸€åœºé©å‘½ï¼Œå°¤å…¶åœ¨æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰çš„åˆæˆä¸­è¡¨ç°å°¤ä¸ºçªå‡ºã€‚ç„¶è€Œï¼Œä¸ªæ€§åŒ–ç”Ÿæˆæ¨¡å‹çš„å¹¿æ³›åº”ç”¨å¼•å‘äº†å…³äºéšç§ä¾µçŠ¯å’Œç‰ˆæƒä¾µæƒçš„ä¸¥é‡æ‹…å¿§ã€‚ä¸ºåº”å¯¹è¿™äº›é—®é¢˜ï¼Œç ”ç©¶è€…æå‡ºäº†åŸºäºå¯¹æŠ—æ€§æ‰°åŠ¨ä¿æŠ¤çš„è§£å†³æ–¹æ¡ˆã€‚ä½†ç°æœ‰æ–¹æ³•å­˜åœ¨æ˜æ˜¾å±€é™æ€§ï¼Œå¦‚æ— æ³•æœ‰æ•ˆå¯¹æŠ—æ•°æ®å˜æ¢ã€æ— æ³•å®Œå…¨æ¶ˆé™¤ä¿æŠ¤å¯¹è±¡åœ¨ç”Ÿæˆè¾“å‡ºä¸­çš„å¯è¯†åˆ«ç‰¹å¾ç­‰ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹åé—¨æ–¹æ³•â€”â€”PersGuardï¼Œå®ƒé€šè¿‡åé—¨è§¦å‘æœºåˆ¶é˜²æ­¢ç‰¹å®šå›¾åƒè¢«æ¶æ„ä¸ªæ€§åŒ–ã€‚ä¸ä¼ ç»Ÿçš„å¯¹æŠ—æ€§æ‰°åŠ¨æ–¹æ³•ä¸åŒï¼ŒPersGuardå°†åé—¨è§¦å‘å™¨æ¤å…¥é¢„è®­ç»ƒçš„T2Iæ¨¡å‹ï¼Œé˜»æ­¢ä¸ºæŒ‡å®šå—ä¿æŠ¤å›¾åƒç”Ÿæˆå®šåˆ¶è¾“å‡ºï¼ŒåŒæ—¶å…è®¸å¯¹æœªå—ä¿æŠ¤å›¾åƒçš„æ­£å¸¸ä¸ªæ€§åŒ–æ“ä½œã€‚ç„¶è€Œï¼Œé’ˆå¯¹æ–‡æœ¬ç”Ÿæˆçš„æ‰©æ•£æ¨¡å‹çš„ç°æœ‰åé—¨æ–¹æ³•æ— æ³•åº”ç”¨äºä¸ªæ€§åŒ–åœºæ™¯ï¼ŒåŸå› åœ¨äºä¸åŒçš„åé—¨ç›®æ ‡å’Œä¸‹æ¸¸å¾®è°ƒè¿‡ç¨‹ä¸­æ½œåœ¨çš„åé—¨æ¶ˆé™¤é—®é¢˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸‰é¡¹ä¸“ä¸ºä¸ªæ€§åŒ–åœºæ™¯è®¾è®¡çš„å…¨æ–°åé—¨ç›®æ ‡ï¼Œå¹¶ç»“åˆæŠ—ä¸‹æ¸¸å¾®è°ƒçš„åé—¨ä¿ç•™æŸå¤±ã€‚è¿™äº›ç»„ä»¶è¢«æ•´åˆåˆ°ä¸€ä¸ªç»Ÿä¸€çš„ä¼˜åŒ–æ¡†æ¶ä¸­ã€‚å¹¿æ³›çš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒPersGuardåœ¨ä¿æŠ¤æ•°æ®éšç§æ–¹é¢æ•ˆæœæ˜¾è‘—ï¼Œå³ä½¿åœ¨ç°ç›’è®¾ç½®ã€å¤šå¯¹è±¡ä¿æŠ¤å’Œé¢éƒ¨èº«ä»½è¯†åˆ«ç­‰åœºæ™¯ä¸‹ä¹Ÿè¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œä¸ºéšç§å’Œç‰ˆæƒä¿æŠ¤æä¾›äº†æ›´ç¨³å¥çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰åœ¨æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰åˆæˆä¸­å®ç°äº†é‡å¤§çªç ´ã€‚</li>
<li>ä¸ªæ€§åŒ–ç”Ÿæˆæ¨¡å‹çš„å¹¿æ³›åº”ç”¨å¼•å‘äº†å…³äºéšç§å’Œç‰ˆæƒçš„æ–°æŒ‘æˆ˜ã€‚</li>
<li>ä¼ ç»Ÿå¯¹æŠ—æ€§æ‰°åŠ¨æ–¹æ³•å­˜åœ¨å±€é™æ€§ï¼Œå¦‚å¯¹æŠ—æ•°æ®å˜æ¢èƒ½åŠ›å¼±ã€æ— æ³•å®Œå…¨æ¶ˆé™¤ä¿æŠ¤å¯¹è±¡çš„è¯†åˆ«ç‰¹å¾ã€‚</li>
<li>PersGuardæ˜¯ä¸€ç§æ–°å‹åé—¨æ–¹æ³•ï¼Œé€šè¿‡æ¤å…¥åé—¨è§¦å‘å™¨é˜²æ­¢ç‰¹å®šå›¾åƒè¢«æ¶æ„ä¸ªæ€§åŒ–ã€‚</li>
<li>ç°æœ‰é’ˆå¯¹T2Iæ‰©æ•£æ¨¡å‹çš„åå°æ–¹æ³•éš¾ä»¥åº”ç”¨äºä¸ªæ€§åŒ–åœºæ™¯ï¼Œå› ä¸ºå­˜åœ¨ä¸åŒçš„åé—¨ç›®æ ‡å’Œä¸‹æ¸¸å¾®è°ƒè¿‡ç¨‹ä¸­çš„åé—¨æ¶ˆé™¤é—®é¢˜ã€‚</li>
<li>ä¸ºåº”å¯¹ä¸Šè¿°é—®é¢˜ï¼Œæå‡ºäº†ä¸‰é¡¹ä¸“ä¸ºä¸ªæ€§åŒ–åœºæ™¯è®¾è®¡çš„åé—¨ç›®æ ‡å’ŒæŠ—ä¸‹æ¸¸å¾®è°ƒçš„åé—¨ä¿ç•™æŸå¤±ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.16167">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-728e0cefbefe5b2e1745f9050740922c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-60cbae12ff1002c627517db817cd533a.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="One-step-Diffusion-Models-with-f-Divergence-Distribution-Matching"><a href="#One-step-Diffusion-Models-with-f-Divergence-Distribution-Matching" class="headerlink" title="One-step Diffusion Models with $f$-Divergence Distribution Matching"></a>One-step Diffusion Models with $f$-Divergence Distribution Matching</h2><p><strong>Authors:Yilun Xu, Weili Nie, Arash Vahdat</strong></p>
<p>Sampling from diffusion models involves a slow iterative process that hinders their practical deployment, especially for interactive applications. To accelerate generation speed, recent approaches distill a multi-step diffusion model into a single-step student generator via variational score distillation, which matches the distribution of samples generated by the student to the teacherâ€™s distribution. However, these approaches use the reverse Kullback-Leibler (KL) divergence for distribution matching which is known to be mode seeking. In this paper, we generalize the distribution matching approach using a novel $f$-divergence minimization framework, termed $f$-distill, that covers different divergences with different trade-offs in terms of mode coverage and training variance. We derive the gradient of the $f$-divergence between the teacher and student distributions and show that it is expressed as the product of their score differences and a weighting function determined by their density ratio. This weighting function naturally emphasizes samples with higher density in the teacher distribution, when using a less mode-seeking divergence. We observe that the popular variational score distillation approach using the reverse-KL divergence is a special case within our framework. Empirically, we demonstrate that alternative $f$-divergences, such as forward-KL and Jensen-Shannon divergences, outperform the current best variational score distillation methods across image generation tasks. In particular, when using Jensen-Shannon divergence, $f$-distill achieves current state-of-the-art one-step generation performance on ImageNet64 and zero-shot text-to-image generation on MS-COCO. Project page: <a target="_blank" rel="noopener" href="https://research.nvidia.com/labs/genair/f-distill">https://research.nvidia.com/labs/genair/f-distill</a> </p>
<blockquote>
<p>ä»æ‰©æ•£æ¨¡å‹ä¸­è¿›è¡Œé‡‡æ ·æ¶‰åŠä¸€ä¸ªç¼“æ…¢è¿­ä»£çš„è¿‡ç¨‹ï¼Œè¿™é˜»ç¢äº†å®ƒä»¬çš„å®é™…åº”ç”¨ï¼Œå°¤å…¶æ˜¯åœ¨äº¤äº’å¼åº”ç”¨ä¸­ã€‚ä¸ºäº†åŠ å¿«ç”Ÿæˆé€Ÿåº¦ï¼Œæœ€è¿‘çš„æ–¹æ³•é€šè¿‡å˜åˆ†åˆ†æ•°è’¸é¦å°†å¤šæ­¥æ‰©æ•£æ¨¡å‹è’¸é¦ä¸ºå•æ­¥å­¦ç”Ÿç”Ÿæˆå™¨ï¼Œä½¿å­¦ç”Ÿç”Ÿæˆçš„æ ·æœ¬åˆ†å¸ƒä¸æ•™å¸ˆçš„åˆ†å¸ƒç›¸åŒ¹é…ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ä½¿ç”¨åå‘Kullback-Leiblerï¼ˆKLï¼‰æ•£åº¦è¿›è¡Œåˆ†å¸ƒåŒ¹é…ï¼Œè€Œåå‘KLæ•£åº¦æ˜¯ä¼—æ‰€å‘¨çŸ¥çš„æ¨¡å¼å¯»æ±‚ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨ä¸€ç§æ–°çš„f-æ•£åº¦æœ€å°åŒ–æ¡†æ¶æ¨å¹¿åˆ†å¸ƒåŒ¹é…æ–¹æ³•ï¼Œç§°ä¸ºf-distillï¼Œè¯¥æ¡†æ¶æ¶µç›–äº†ä¸åŒçš„æ•£åº¦ï¼Œåœ¨æ¨¡å¼è¦†ç›–å’Œè®­ç»ƒæ–¹å·®æ–¹é¢æœ‰ä¸åŒçš„æƒè¡¡ã€‚æˆ‘ä»¬æ¨å¯¼å‡ºæ•™å¸ˆå’Œå­¦ç”Ÿåˆ†å¸ƒä¹‹é—´f-æ•£åº¦çš„æ¢¯åº¦ï¼Œå¹¶æ˜¾ç¤ºå®ƒè¡¨ç¤ºä¸ºä»–ä»¬çš„åˆ†æ•°å·®å¼‚ä¹‹ç§¯å’Œä¸€ä¸ªç”±ä»–ä»¬çš„å¯†åº¦æ¯”ç¡®å®šçš„åŠ æƒå‡½æ•°ã€‚å½“ä½¿ç”¨è¾ƒå°‘çš„æ¨¡å¼å¯»æ±‚æ•£åº¦æ—¶ï¼Œæ­¤åŠ æƒå‡½æ•°ä¼šè‡ªç„¶åœ°å¼ºè°ƒæ•™å¸ˆåˆ†å¸ƒä¸­å¯†åº¦è¾ƒé«˜çš„æ ·æœ¬ã€‚æˆ‘ä»¬å‘ç°ï¼Œä½¿ç”¨åå‘KLæ•£åº¦çš„æµè¡Œå˜åˆ†åˆ†æ•°è’¸é¦æ–¹æ³•æ˜¯æˆ‘ä»¬æ¡†æ¶å†…çš„ç‰¹æ®Šæƒ…å†µã€‚ä»ç»éªŒä¸Šçœ‹ï¼Œæˆ‘ä»¬å‘ç°æ›¿ä»£çš„f-æ•£åº¦ï¼Œå¦‚æ­£å‘KLå’ŒJensen-Shannonæ•£åº¦ï¼Œåœ¨å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¼˜äºå½“å‰æœ€ä½³å˜åˆ†åˆ†æ•°è’¸é¦æ–¹æ³•ã€‚ç‰¹åˆ«æ˜¯ä½¿ç”¨Jensen-Shannonæ•£åº¦æ—¶ï¼Œf-distillåœ¨ImageNet64ä¸Šå®ç°äº†ä¸€æ­¥ç”Ÿæˆçš„æœ€å…ˆè¿›æ€§èƒ½ï¼Œå¹¶åœ¨MS-COCOä¸Šå®ç°äº†é›¶æ ·æœ¬æ–‡æœ¬åˆ°å›¾åƒçš„ç”Ÿæˆã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://research.nvidia.com/labs/genair/f-distill">https://research.nvidia.com/labs/genair/f-distill</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.15681v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡é’ˆå¯¹æ‰©æ•£æ¨¡å‹çš„é‡‡æ ·é€Ÿåº¦æ…¢çš„é—®é¢˜ï¼Œæå‡ºäº†ä½¿ç”¨f-divergenceæœ€å°åŒ–æ¡†æ¶ï¼ˆf-distillï¼‰æ¥åŠ é€Ÿç”Ÿæˆé€Ÿåº¦çš„æ–¹æ³•ã€‚è¯¥æ¡†æ¶å°†å¤šæ­¥æ‰©æ•£æ¨¡å‹ç®€åŒ–ä¸ºå•æ­¥å­¦ç”Ÿç”Ÿæˆå™¨ï¼Œå¹¶é€šè¿‡åŒ¹é…å­¦ç”Ÿä¸æ•™å¸ˆåˆ†å¸ƒæ¥å®ç°åŠ é€Ÿã€‚æ–‡ç« æ¨å¯¼äº†æ•™å¸ˆåˆ†å¸ƒå’Œå­¦ç”Ÿåˆ†å¸ƒä¹‹é—´çš„f-divergenceæ¢¯åº¦ï¼Œå¹¶å±•ç¤ºäº†è¯¥æ¢¯åº¦ä¸ä»–ä»¬çš„åˆ†æ•°å·®å¼‚å’Œå¯†åº¦æ¯”ç‡å†³å®šçš„æƒé‡å‡½æ•°çš„ä¹˜ç§¯è¡¨è¾¾å½¢å¼ã€‚æ­¤å¤–ï¼Œè¯¥æ–‡é€šè¿‡å®è¯ç ”ç©¶å‘ç°ï¼Œä½¿ç”¨ä¸åŒçš„f-divergencesï¼ˆå¦‚æ­£å‘KLå’ŒJensen-Shannonæ•£åº¦ï¼‰å¯ä»¥ä¼˜äºç°æœ‰çš„å˜åˆ†åˆ†æ•°è’¸é¦æ–¹æ³•ï¼Œåœ¨å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šæœ‰æ›´å¥½çš„è¡¨ç°ã€‚ç‰¹åˆ«æ˜¯åœ¨ä½¿ç”¨Jensen-Shannonæ•£åº¦æ—¶ï¼Œf-distillåœ¨ImageNet64å’ŒMS-COCOä¸Šçš„å•æ­¥ç”Ÿæˆæ€§èƒ½å’Œé›¶æ ·æœ¬æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸Šè¾¾åˆ°äº†å½“å‰æœ€ä½³æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹çš„é‡‡æ ·æ˜¯ä¸€ä¸ªæ…¢è¿­ä»£è¿‡ç¨‹ï¼Œé™åˆ¶äº†å…¶å®æ—¶åº”ç”¨å’Œäº¤äº’ä½“éªŒã€‚</li>
<li>é€šè¿‡å˜åˆ†åˆ†æ•°è’¸é¦æ–¹æ³•ï¼Œå¯ä»¥å°†å¤šæ­¥æ‰©æ•£æ¨¡å‹ç®€åŒ–ä¸ºå•æ­¥å­¦ç”Ÿç”Ÿæˆå™¨ï¼Œä»¥æé«˜ç”Ÿæˆé€Ÿåº¦ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä½¿ç”¨åå‘KLæ•£åº¦è¿›è¡Œåˆ†å¸ƒåŒ¹é…ï¼Œä½†å­˜åœ¨æ¨¡å¼å¯»æ±‚çš„é—®é¢˜ã€‚</li>
<li>æå‡ºäº†f-divergenceæœ€å°åŒ–æ¡†æ¶ï¼ˆf-distillï¼‰ï¼ŒåŒ…æ‹¬ä¸åŒçš„æ•£åº¦é€‰æ‹©ï¼Œä»¥æ”¹å–„æ¨¡å¼è¦†ç›–å’Œè®­ç»ƒæ–¹å·®ä¹‹é—´çš„æƒè¡¡ã€‚</li>
<li>f-distillæ¡†æ¶ä¸‹çš„æ•™å¸ˆå’Œå­¦ç”Ÿåˆ†å¸ƒåŒ¹é…é€šè¿‡å¯†åº¦æ¯”ç‡å’Œåˆ†æ•°å·®å¼‚å®ç°ï¼Œå…¶ä¸­æƒé‡å‡½æ•°å¯è‡ªç„¶å¼ºè°ƒæ•™å¸ˆåˆ†å¸ƒä¸­å¯†åº¦è¾ƒé«˜çš„æ ·æœ¬ã€‚</li>
<li>ä½¿ç”¨ä¸åŒçš„f-divergencesï¼ˆå¦‚æ­£å‘KLå’ŒJensen-Shannonæ•£åº¦ï¼‰åœ¨å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šè¡¨ç°å‡ºä¼˜äºç°æœ‰æ–¹æ³•çš„æ€§èƒ½ã€‚</li>
<li>f-distillåœ¨ImageNet64å’ŒMS-COCOä¸Šçš„å•æ­¥ç”Ÿæˆæ€§èƒ½è¾¾åˆ°äº†å½“å‰æœ€ä½³æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.15681">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-168c26395b0f933990c30d4c76543955.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-758646ff0cbf035abb8ee7e794c47bec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-60f58669ce73edba2aeb2bec052dd2a7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-640788e79204c7bf6af616c1cfc27cf2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-282474fb04eb7377a581f6ceeac2d2c5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bf095bde99d15ad0eb04816b1d5dc2cb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e9ef7f26a17a72e14b5049d251bb7dc9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b65760aef9c6fec286ae4ee3670e5d81.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Ambient-Denoising-Diffusion-Generative-Adversarial-Networks-for-Establishing-Stochastic-Object-Models-from-Noisy-Image-Data"><a href="#Ambient-Denoising-Diffusion-Generative-Adversarial-Networks-for-Establishing-Stochastic-Object-Models-from-Noisy-Image-Data" class="headerlink" title="Ambient Denoising Diffusion Generative Adversarial Networks for   Establishing Stochastic Object Models from Noisy Image Data"></a>Ambient Denoising Diffusion Generative Adversarial Networks for   Establishing Stochastic Object Models from Noisy Image Data</h2><p><strong>Authors:Xichen Xu, Wentao Chen, Weimin Zhou</strong></p>
<p>It is widely accepted that medical imaging systems should be objectively assessed via task-based image quality (IQ) measures that ideally account for all sources of randomness in the measured image data, including the variation in the ensemble of objects to be imaged. Stochastic object models (SOMs) that can randomly draw samples from the object distribution can be employed to characterize object variability. To establish realistic SOMs for task-based IQ analysis, it is desirable to employ experimental image data. However, experimental image data acquired from medical imaging systems are subject to measurement noise. Previous work investigated the ability of deep generative models (DGMs) that employ an augmented generative adversarial network (GAN), AmbientGAN, for establishing SOMs from noisy measured image data. Recently, denoising diffusion models (DDMs) have emerged as a leading DGM for image synthesis and can produce superior image quality than GANs. However, original DDMs possess a slow image-generation process because of the Gaussian assumption in the denoising steps. More recently, denoising diffusion GAN (DDGAN) was proposed to permit fast image generation while maintain high generated image quality that is comparable to the original DDMs. In this work, we propose an augmented DDGAN architecture, Ambient DDGAN (ADDGAN), for learning SOMs from noisy image data. Numerical studies that consider clinical computed tomography (CT) images and digital breast tomosynthesis (DBT) images are conducted. The ability of the proposed ADDGAN to learn realistic SOMs from noisy image data is demonstrated. It has been shown that the ADDGAN significantly outperforms the advanced AmbientGAN models for synthesizing high resolution medical images with complex textures. </p>
<blockquote>
<p>æ™®éè®¤ä¸ºï¼ŒåŒ»ç–—æˆåƒç³»ç»Ÿåº”é€šè¿‡åŸºäºä»»åŠ¡çš„å›¾åƒè´¨é‡ï¼ˆIQï¼‰æªæ–½è¿›è¡Œå®¢è§‚è¯„ä¼°ï¼Œè¿™äº›æªæ–½åº”ç†æƒ³åœ°è€ƒè™‘æµ‹é‡å›¾åƒæ•°æ®ä¸­æ‰€æœ‰éšæœºæ€§çš„æ¥æºï¼ŒåŒ…æ‹¬å¾…æˆåƒå¯¹è±¡é›†åˆçš„å˜å¼‚ã€‚å¯ä»¥ä½¿ç”¨éšæœºå¯¹è±¡æ¨¡å‹ï¼ˆSOMsï¼‰ä»å¯¹è±¡åˆ†å¸ƒä¸­éšæœºæŠ½å–æ ·æœ¬ä»¥è¡¨å¾å¯¹è±¡çš„å˜åŒ–æ€§ã€‚ä¸ºäº†é’ˆå¯¹åŸºäºä»»åŠ¡çš„IQåˆ†æå»ºç«‹ç°å®çš„SOMsï¼Œå»ºè®®ä½¿ç”¨å®éªŒå›¾åƒæ•°æ®ã€‚ç„¶è€Œï¼Œä»åŒ»ç–—æˆåƒç³»ç»Ÿè·å¾—çš„å®éªŒå›¾åƒæ•°æ®å­˜åœ¨æµ‹é‡å™ªå£°ã€‚å…ˆå‰çš„å·¥ä½œç ”ç©¶äº†ä½¿ç”¨å¢å¼ºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰çš„æ·±ç”Ÿæˆæ¨¡å‹ï¼ˆDGMï¼‰å»ºç«‹å™ªå£°æµ‹é‡å›¾åƒæ•°æ®çš„SOMsçš„èƒ½åŠ›ã€‚æœ€è¿‘ï¼Œå»å™ªæ‰©æ•£æ¨¡å‹ï¼ˆDDMï¼‰ä½œä¸ºé¢†å…ˆçš„å›¾åƒåˆæˆDGMå‡ºç°ï¼Œå¹¶å¯ä»¥äº§ç”Ÿæ¯”GANæ›´é«˜çš„å›¾åƒè´¨é‡ã€‚ç„¶è€Œï¼ŒåŸå§‹DDMsç”±äºå»å™ªæ­¥éª¤ä¸­çš„é«˜æ–¯å‡è®¾è€Œå…·æœ‰è¾ƒæ…¢çš„å›¾åƒç”Ÿæˆè¿‡ç¨‹ã€‚æœ€è¿‘ï¼Œæå‡ºäº†å»å™ªæ‰©æ•£GANï¼ˆDDGANï¼‰ä»¥å…è®¸å¿«é€Ÿå›¾åƒç”Ÿæˆï¼ŒåŒæ—¶ä¿æŒä¸åŸå§‹DDMsç›¸å½“çš„é«˜ç”Ÿæˆå›¾åƒè´¨é‡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¢å¼ºçš„DDGANæ¶æ„ï¼Œå³ç¯å¢ƒDDGANï¼ˆADDGANï¼‰ï¼Œç”¨äºä»å™ªå£°å›¾åƒæ•°æ®ä¸­å­¦ä¹ SOMsã€‚æˆ‘ä»¬è¿›è¡Œäº†è€ƒè™‘ä¸´åºŠè®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰å›¾åƒå’Œæ•°å­—ä¹³è…ºæ–­å±‚åˆæˆï¼ˆDBTï¼‰å›¾åƒçš„æ•°å€¼ç ”ç©¶ã€‚å±•ç¤ºäº†æ‰€æå‡ºçš„ADDGANä»å™ªå£°å›¾åƒæ•°æ®ä¸­å­¦ä¹ ç°å®SOMsçš„èƒ½åŠ›ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒADDGANåœ¨åˆæˆå…·æœ‰å¤æ‚çº¹ç†çš„é«˜åˆ†è¾¨ç‡åŒ»å­¦å›¾åƒæ–¹é¢æ˜¾è‘—ä¼˜äºå…ˆè¿›çš„AmbientGANæ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.19094v2">PDF</a> SPIE Medical Imaging 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åŸºäºä»»åŠ¡è´¨é‡çš„åŒ»å­¦å½±åƒç³»ç»Ÿè¯„ä¼°æ–¹æ³•ï¼Œä»‹ç»äº†éšæœºå¯¹è±¡æ¨¡å‹ï¼ˆSOMsï¼‰çš„å»ºç«‹è¿‡ç¨‹åŠå…¶åœ¨ä»»åŠ¡è´¨é‡åˆ†æä¸­çš„åº”ç”¨ã€‚æ–‡ç« æŒ‡å‡ºï¼Œå®éªŒå›¾åƒæ•°æ®å—åˆ°æµ‹é‡å™ªå£°çš„å½±å“ï¼Œå› æ­¤éœ€è¦ä½¿ç”¨æ·±åº¦ç”Ÿæˆæ¨¡å‹ï¼ˆDGMsï¼‰å»ºç«‹SOMsã€‚æ–‡ç« ä»‹ç»äº†å»å™ªæ‰©æ•£æ¨¡å‹ï¼ˆDDMsï¼‰å’Œå»å™ªæ‰©æ•£GANï¼ˆDDGANï¼‰çš„ä¼˜åŠ¿å’ŒæŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†ä¸€ç§å¢å¼ºçš„DDGANæ¶æ„â€”â€”Ambient DDGANï¼ˆADDGANï¼‰ï¼Œèƒ½å¤Ÿä»å™ªå£°å›¾åƒæ•°æ®ä¸­å­¦ä¹ ç°å®ä¸–ç•Œçš„SOMsã€‚æ•°å€¼ç ”ç©¶è¯æ˜äº†ADDGANåœ¨åˆæˆé«˜åˆ†è¾¨ç‡åŒ»å­¦å›¾åƒæ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ï¼Œå°¤å…¶åœ¨å¤„ç†å¤æ‚çº¹ç†çš„åŒ»å­¦å›¾åƒæ—¶è¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»ç–—å½±åƒç³»ç»Ÿè¯„ä¼°åº”å½“é‡‡ç”¨åŸºäºä»»åŠ¡è´¨é‡çš„å›¾åƒè´¨é‡ï¼ˆIQï¼‰æªæ–½ï¼Œè€ƒè™‘å›¾åƒæ•°æ®ä¸­çš„æ‰€æœ‰éšæœºæ€§æ¥æºã€‚</li>
<li>éšæœºå¯¹è±¡æ¨¡å‹ï¼ˆSOMsï¼‰å¯ç”¨äºè¡¨å¾å¯¹è±¡å˜åŒ–ï¼Œå¯ä»å¯¹è±¡åˆ†å¸ƒä¸­éšæœºæŠ½å–æ ·æœ¬ã€‚</li>
<li>å®éªŒå›¾åƒæ•°æ®å—åˆ°æµ‹é‡å™ªå£°çš„å½±å“ï¼Œéœ€è¦ä½¿ç”¨æ·±åº¦ç”Ÿæˆæ¨¡å‹ï¼ˆDGMsï¼‰å»ºç«‹ç°å®çš„SOMsã€‚</li>
<li>å»å™ªæ‰©æ•£æ¨¡å‹ï¼ˆDDMsï¼‰å’Œå»å™ªæ‰©æ•£GANï¼ˆDDGANï¼‰æ˜¯ç”¨äºå›¾åƒåˆæˆçš„å…ˆè¿›DGMã€‚</li>
<li>åŸå§‹çš„DDMsç”±äºå»å™ªæ­¥éª¤ä¸­çš„é«˜æ–¯å‡è®¾ï¼Œå›¾åƒç”Ÿæˆè¿‡ç¨‹è¾ƒæ…¢ã€‚</li>
<li>æ–°æå‡ºçš„å»å™ªæ‰©æ•£GANï¼ˆDDGANï¼‰å…è®¸å¿«é€Ÿå›¾åƒç”Ÿæˆï¼ŒåŒæ—¶ä¿æŒä¸åŸå§‹DDMsç›¸å½“çš„é«˜è´¨é‡å›¾åƒç”Ÿæˆèƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.19094">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6ab7ec549d85f3ec83866946fa4175ca.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e286f6622218051d9f34b42aa6c0cfff.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9bbec2dc4d2b19b867c16214a9579421.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4f141bc5650c59804d3f9a2085de1663.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-770238047ef76dab46461ea22f983730.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3cedd86632b8bfe03933c6c1429d22e6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f606185b13845e181b63e9500963a7c1.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="SmartSpatial-Enhancing-the-3D-Spatial-Arrangement-Capabilities-of-Stable-Diffusion-Models-and-Introducing-a-Novel-3D-Spatial-Evaluation-Framework"><a href="#SmartSpatial-Enhancing-the-3D-Spatial-Arrangement-Capabilities-of-Stable-Diffusion-Models-and-Introducing-a-Novel-3D-Spatial-Evaluation-Framework" class="headerlink" title="SmartSpatial: Enhancing the 3D Spatial Arrangement Capabilities of   Stable Diffusion Models and Introducing a Novel 3D Spatial Evaluation   Framework"></a>SmartSpatial: Enhancing the 3D Spatial Arrangement Capabilities of   Stable Diffusion Models and Introducing a Novel 3D Spatial Evaluation   Framework</h2><p><strong>Authors:Mao Xun Huang, Brian J Chan, Hen-Hsen Huang</strong></p>
<p>Stable Diffusion models have made remarkable strides in generating photorealistic images from text prompts but often falter when tasked with accurately representing complex spatial arrangements, particularly involving intricate 3D relationships. To address this limitation, we introduce SmartSpatial, an innovative approach that not only enhances the spatial arrangement capabilities of Stable Diffusion but also fosters AI-assisted creative workflows through 3D-aware conditioning and attention-guided mechanisms. SmartSpatial incorporates depth information injection and cross-attention control to ensure precise object placement, delivering notable improvements in spatial accuracy metrics. In conjunction with SmartSpatial, we present SmartSpatialEval, a comprehensive evaluation framework that bridges computational spatial accuracy with qualitative artistic assessments. Experimental results show that SmartSpatial significantly outperforms existing methods, setting new benchmarks for spatial fidelity in AI-driven art and creativity. </p>
<blockquote>
<p>ç¨³å®šæ‰©æ•£æ¨¡å‹åœ¨æ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆé€¼çœŸçš„å›¾åƒæ–¹é¢å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä½†åœ¨å‡†ç¡®è¡¨ç¤ºå¤æ‚çš„ç©ºé—´æ’åˆ—ï¼Œç‰¹åˆ«æ˜¯æ¶‰åŠç²¾ç»†çš„3Då…³ç³»æ—¶ï¼Œå¾€å¾€ä¼šå‡ºç°å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†SmartSpatialï¼Œè¿™æ˜¯ä¸€ç§åˆ›æ–°çš„æ–¹æ³•ï¼Œä¸ä»…æé«˜äº†ç¨³å®šæ‰©æ•£çš„ç©ºé—´æ’åˆ—èƒ½åŠ›ï¼Œè¿˜é€šè¿‡3Dæ„ŸçŸ¥æ¡ä»¶å’Œæ³¨æ„åŠ›å¼•å¯¼æœºåˆ¶ä¿ƒè¿›äº†AIè¾…åŠ©åˆ›æ„å·¥ä½œæµç¨‹ã€‚SmartSpatialé€šè¿‡æ·±åº¦ä¿¡æ¯æ³¨å…¥å’Œäº¤å‰æ³¨æ„åŠ›æ§åˆ¶ï¼Œç¡®ä¿ç²¾ç¡®çš„å¯¹è±¡æ”¾ç½®ï¼Œåœ¨ç©ºé—´ç²¾åº¦æŒ‡æ ‡æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚ä¸SmartSpatialç›¸ç»“åˆï¼Œæˆ‘ä»¬æ¨å‡ºäº†SmartSpatialEvalï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œå®ƒç»“åˆäº†è®¡ç®—ç©ºé—´ç²¾åº¦ä¸å®šæ€§è‰ºæœ¯è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSmartSpatialæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸ºAIé©±åŠ¨çš„è‰ºæœ¯å’Œåˆ›é€ åŠ›è®¾ç«‹äº†æ–°çš„ç©ºé—´ä¿çœŸåº¦åŸºå‡†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01998v2">PDF</a> 9 pages</p>
<p><strong>Summary</strong></p>
<p>Stable Diffusionæ¨¡å‹åœ¨æ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆé€¼çœŸçš„å›¾åƒæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨è¡¨ç°å¤æ‚çš„ç©ºé—´æ’åˆ—ï¼Œå°¤å…¶æ˜¯æ¶‰åŠç²¾ç»†çš„3Då…³ç³»æ—¶å¸¸å¸¸å‡ºç°é—®é¢˜ã€‚ä¸ºè§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æ¨å‡ºäº†SmartSpatialï¼Œè¿™æ˜¯ä¸€ç§ä¸ä»…å¢å¼ºäº†Stable Diffusionçš„ç©ºé—´æ’åˆ—èƒ½åŠ›ï¼Œè¿˜é€šè¿‡3Dæ„ŸçŸ¥æ¡ä»¶å’Œæ³¨æ„åŠ›å¼•å¯¼æœºåˆ¶ä¿ƒè¿›äº†AIè¾…åŠ©åˆ›æ„å·¥ä½œæµçš„åˆ›æ–°æ–¹æ³•ã€‚SmartSpatialé€šè¿‡æ·±åº¦ä¿¡æ¯æ³¨å…¥å’Œäº¤å‰æ³¨æ„åŠ›æ§åˆ¶ï¼Œç¡®ä¿ç²¾ç¡®çš„å¯¹è±¡æ”¾ç½®ï¼Œåœ¨ç©ºé—´ç²¾åº¦æŒ‡æ ‡æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚é…åˆSmartSpatialï¼Œæˆ‘ä»¬æ¨å‡ºäº†SmartSpatialEvalç»¼åˆè¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†è®¡ç®—ç©ºé—´ç²¾åº¦ä¸å®šæ€§è‰ºæœ¯è¯„ä¼°ç›¸ç»“åˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSmartSpatialæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸ºAIé©±åŠ¨çš„è‰ºæœ¯å’Œåˆ›é€ åŠ›è®¾ç«‹äº†æ–°çš„ç©ºé—´ä¿çœŸåº¦åŸºå‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Stable Diffusionæ¨¡å‹åœ¨ç”Ÿæˆé€¼çœŸå›¾åƒæ–¹é¢æœ‰æ‰€çªç ´ï¼Œä½†åœ¨å¤æ‚ç©ºé—´æ’åˆ—è¡¨ç¤ºä¸Šä»æœ‰å±€é™ã€‚</li>
<li>SmartSpatialæ–¹æ³•è¢«å¼•å…¥ï¼Œä»¥å¢å¼ºæ¨¡å‹åœ¨ç©ºé—´æ’åˆ—æ–¹é¢çš„èƒ½åŠ›ï¼Œå¹¶ä¿ƒè¿›AIè¾…åŠ©çš„åˆ›æ„å·¥ä½œæµç¨‹ã€‚</li>
<li>SmartSpatialé€šè¿‡æ·±åº¦ä¿¡æ¯æ³¨å…¥å’Œäº¤å‰æ³¨æ„åŠ›æ§åˆ¶ç¡®ä¿ç²¾ç¡®å¯¹è±¡æ”¾ç½®ã€‚</li>
<li>SmartSpatialEvalè¯„ä¼°æ¡†æ¶ç”¨äºç»¼åˆè¯„ä¼°è®¡ç®—ç©ºé—´ç²¾åº¦ä¸å®šæ€§è‰ºæœ¯è¯„ä¼°ã€‚</li>
<li>SmartSpatialæ˜¾è‘—æé«˜äº†ç©ºé—´ç²¾åº¦æŒ‡æ ‡ï¼Œç›¸è¾ƒäºç°æœ‰æ–¹æ³•è¡¨ç°å‡ºä¼˜è¶Šæ€§ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºAIé©±åŠ¨çš„è‰ºæœ¯å’Œåˆ›é€ åŠ›è®¾ç«‹äº†æ–°çš„ç©ºé—´ä¿çœŸåº¦åŸºå‡†ã€‚</li>
<li>è¿™ç§æ–¹æ³•æœ‰åŠ©äºæ¨åŠ¨AIåœ¨å›¾åƒç”Ÿæˆé¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¤æ‚ç©ºé—´å…³ç³»æ–¹é¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01998">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-415b6f453f88b27eed6ba39ddb82d28b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7135cd8741ef9117dda8f204817631a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6823e12811863ed75ea8e0a363cbdc1f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d9e69b1089978064351b3dfd6347fb73.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-806c37ed85ed7eaadc8e919ef1f2ee12.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a0bb524963b16ce7a93004f1a6cecbea.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Panoptic-Diffusion-Models-co-generation-of-images-and-segmentation-maps"><a href="#Panoptic-Diffusion-Models-co-generation-of-images-and-segmentation-maps" class="headerlink" title="Panoptic Diffusion Models: co-generation of images and segmentation maps"></a>Panoptic Diffusion Models: co-generation of images and segmentation maps</h2><p><strong>Authors:Yinghan Long, Kaushik Roy</strong></p>
<p>Recently, diffusion models have demonstrated impressive capabilities in text-guided and image-conditioned image generation. However, existing diffusion models cannot simultaneously generate an image and a panoptic segmentation of objects and stuff from the prompt. Incorporating an inherent understanding of shapes and scene layouts can improve the creativity and realism of diffusion models. To address this limitation, we present Panoptic Diffusion Model (PDM), the first model designed to generate both images and panoptic segmentation maps concurrently. PDM bridges the gap between image and text by constructing segmentation layouts that provide detailed, built-in guidance throughout the generation process. This ensures the inclusion of categories mentioned in text prompts and enriches the diversity of segments within the background. We demonstrate the effectiveness of PDM across two architectures: a unified diffusion transformer and a two-stream transformer with a pretrained backbone. We propose a Multi-Scale Patching mechanism to generate high-resolution segmentation maps. Additionally, when ground-truth maps are available, PDM can function as a text-guided image-to-image generation model. Finally, we propose a novel metric for evaluating the quality of generated maps and show that PDM achieves state-of-the-art results in image generation with implicit scene control. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œæ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬å¼•å¯¼å’Œå›¾åƒæ¡ä»¶å›¾åƒç”Ÿæˆæ–¹é¢å±•ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ‰©æ•£æ¨¡å‹æ— æ³•ä»æç¤ºä¸­åŒæ—¶ç”Ÿæˆå›¾åƒå’Œå…¨æ™¯åˆ†å‰²çš„å¯¹è±¡å’Œç´ æã€‚èå…¥å¯¹å½¢çŠ¶å’Œåœºæ™¯å¸ƒå±€çš„å†…åœ¨ç†è§£ï¼Œå¯ä»¥æé«˜æ‰©æ•£æ¨¡å‹çš„åˆ›é€ æ€§å’ŒçœŸå®æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†å…¨æ™¯æ‰©æ•£æ¨¡å‹ï¼ˆPDMï¼‰ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªè®¾è®¡ç”¨æ¥åŒæ—¶ç”Ÿæˆå›¾åƒå’Œå…¨æ™¯åˆ†å‰²å›¾æ¨¡å‹ã€‚PDMé€šè¿‡æ„å»ºåˆ†å‰²å¸ƒå±€æ¥ç¼©å°å›¾åƒå’Œæ–‡æœ¬ä¹‹é—´çš„å·®è·ï¼Œä¸ºç”Ÿæˆè¿‡ç¨‹æä¾›è¯¦ç»†çš„å†…ç½®æŒ‡å¯¼ã€‚è¿™ç¡®ä¿äº†æ–‡æœ¬æç¤ºä¸­æåˆ°çš„ç±»åˆ«çš„åŒ…å«æ€§ï¼Œå¹¶ä¸°å¯Œäº†èƒŒæ™¯ä¸­æ®µçš„å¤šæ ·æ€§ã€‚æˆ‘ä»¬åœ¨ä¸¤ç§æ¶æ„ä¸Šå±•ç¤ºäº†PDMçš„æœ‰æ•ˆæ€§ï¼šç»Ÿä¸€çš„æ‰©æ•£å˜å‹å™¨å’Œå¸¦æœ‰é¢„è®­ç»ƒä¸»å¹²çš„ä¸¤æµå˜å‹å™¨ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šå°ºåº¦è¡¥ä¸æœºåˆ¶æ¥ç”Ÿæˆé«˜åˆ†è¾¨ç‡çš„åˆ†å‰²å›¾ã€‚å¦å¤–ï¼Œå½“å­˜åœ¨çœŸå®åœ°å›¾æ—¶ï¼ŒPDMå¯ä»¥ä½œä¸ºæ–‡æœ¬å¼•å¯¼çš„å›¾åƒåˆ°å›¾åƒç”Ÿæˆæ¨¡å‹å‘æŒ¥ä½œç”¨ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æŒ‡æ ‡æ¥è¯„ä¼°ç”Ÿæˆçš„åœ°å›¾çš„è´¨é‡ï¼Œå¹¶è¯æ˜PDMåœ¨å…·æœ‰éšå¼åœºæ™¯æ§åˆ¶çš„å›¾åƒç”Ÿæˆæ–¹é¢è¾¾åˆ°äº†æœ€æ–°çš„ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.02929v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºPanoptic Diffusion Modelï¼ˆPDMï¼‰çš„æ–°æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨ç”Ÿæˆå›¾åƒçš„åŒæ—¶ç”Ÿæˆå…¨æ™¯åˆ†å‰²å›¾ã€‚PDMé€šè¿‡æ„å»ºåˆ†å‰²å¸ƒå±€æ¥å¼¥è¡¥å›¾åƒå’Œæ–‡æœ¬ä¹‹é—´çš„å·®è·ï¼Œä¸ºç”Ÿæˆè¿‡ç¨‹æä¾›è¯¦ç»†çš„å†…ç½®æŒ‡å¯¼ï¼Œç¡®ä¿åŒ…å«æ–‡æœ¬æç¤ºä¸­çš„ç±»åˆ«ï¼Œå¹¶ä¸°å¯ŒèƒŒæ™¯ä¸­çš„åˆ†æ®µå¤šæ ·æ€§ã€‚æ–‡ç« å±•ç¤ºäº†PDMåœ¨ä¸¤ç§æ¶æ„ä¸‹çš„æœ‰æ•ˆæ€§ï¼Œå¹¶æå‡ºäº†ä¸€ç§å¤šå°ºåº¦è´´ç‰‡æœºåˆ¶æ¥ç”Ÿæˆé«˜åˆ†è¾¨ç‡åˆ†å‰²å›¾ã€‚å½“å­˜åœ¨çœŸå®åœ°å›¾æ—¶ï¼ŒPDMå¯ä»¥ä½œä¸ºæ–‡æœ¬å¼•å¯¼çš„å›¾åƒåˆ°å›¾åƒç”Ÿæˆæ¨¡å‹ã€‚æœ€åï¼Œæ–‡ç« æå‡ºäº†ä¸€ç§è¯„ä»·ç”Ÿæˆå›¾è´¨é‡çš„æ–°æŒ‡æ ‡ï¼Œå¹¶å±•ç¤ºäº†PDMåœ¨å›¾åƒç”Ÿæˆæ–¹é¢çš„å“è¶Šæ€§èƒ½ï¼Œå®ç°äº†éšå¼åœºæ™¯æ§åˆ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Panoptic Diffusion Model (PDM) æ˜¯é¦–ä¸ªèƒ½å¤ŸåŒæ—¶ç”Ÿæˆå›¾åƒå’Œå…¨æ™¯åˆ†å‰²å›¾æ¨¡å‹ã€‚</li>
<li>PDMé€šè¿‡æ„å»ºåˆ†å‰²å¸ƒå±€æ¥èåˆå›¾åƒå’Œæ–‡æœ¬ï¼Œæä¾›ç”Ÿæˆè¿‡ç¨‹ä¸­çš„è¯¦ç»†å†…ç½®æŒ‡å¯¼ã€‚</li>
<li>PDMèƒ½å¤Ÿç¡®ä¿åŒ…å«æ–‡æœ¬æç¤ºä¸­çš„ç±»åˆ«ï¼Œå¹¶ä¸°å¯ŒèƒŒæ™¯ä¸­çš„åˆ†æ®µå¤šæ ·æ€§ã€‚</li>
<li>æ–‡ç« å±•ç¤ºäº†PDMåœ¨ç»Ÿä¸€æ‰©æ•£å˜å‹å™¨å’Œä¸¤æµå˜å‹å™¨æ¶æ„ä¸‹çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>å¤šå°ºåº¦è´´ç‰‡æœºåˆ¶ç”¨äºç”Ÿæˆé«˜åˆ†è¾¨ç‡åˆ†å‰²å›¾ã€‚</li>
<li>å½“å­˜åœ¨çœŸå®åœ°å›¾æ—¶ï¼ŒPDMå¯ä½œä¸ºæ–‡æœ¬å¼•å¯¼çš„å›¾åƒåˆ°å›¾åƒç”Ÿæˆæ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.02929">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-abe43cf5a7e095f29ecdf9f6db420595.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f6b23f6aa374aea9f09a86fb715305f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3e301980af267f04372a61fa4746d6a2.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Cross-Attention-Head-Position-Patterns-Can-Align-with-Human-Visual-Concepts-in-Text-to-Image-Generative-Models"><a href="#Cross-Attention-Head-Position-Patterns-Can-Align-with-Human-Visual-Concepts-in-Text-to-Image-Generative-Models" class="headerlink" title="Cross-Attention Head Position Patterns Can Align with Human Visual   Concepts in Text-to-Image Generative Models"></a>Cross-Attention Head Position Patterns Can Align with Human Visual   Concepts in Text-to-Image Generative Models</h2><p><strong>Authors:Jungwon Park, Jungmin Ko, Dongnam Byun, Jangwon Suh, Wonjong Rhee</strong></p>
<p>Recent text-to-image diffusion models leverage cross-attention layers, which have been effectively utilized to enhance a range of visual generative tasks. However, our understanding of cross-attention layers remains somewhat limited. In this study, we introduce a mechanistic interpretability approach for diffusion models by constructing Head Relevance Vectors (HRVs) that align with human-specified visual concepts. An HRV for a given visual concept has a length equal to the total number of cross-attention heads, with each element representing the importance of the corresponding head for the given visual concept. To validate HRVs as interpretable features, we develop an ordered weakening analysis that demonstrates their effectiveness. Furthermore, we propose concept strengthening and concept adjusting methods and apply them to enhance three visual generative tasks. Our results show that HRVs can reduce misinterpretations of polysemous words in image generation, successfully modify five challenging attributes in image editing, and mitigate catastrophic neglect in multi-concept generation. Overall, our work provides an advancement in understanding cross-attention layers and introduces new approaches for fine-controlling these layers at the head level. </p>
<blockquote>
<p>æœ€è¿‘çš„æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹åˆ©ç”¨äº†äº¤å‰æ³¨æ„åŠ›å±‚ï¼Œè¿™äº›å±‚å·²è¢«æœ‰æ•ˆåœ°ç”¨äºå¢å¼ºå„ç§è§†è§‰ç”Ÿæˆä»»åŠ¡ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å¯¹äº¤å‰æ³¨æ„åŠ›å±‚çš„ç†è§£ä»ç„¶æœ‰é™ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡æ„å»ºä¸äººç±»æŒ‡å®šçš„è§†è§‰æ¦‚å¿µç›¸å¯¹åº”çš„Head Relevance Vectors (HRVs)å¼•å…¥äº†ä¸€ç§æœºåˆ¶è§£é‡Šæ€§æ–¹æ³•ã€‚ç»™å®šè§†è§‰æ¦‚å¿µçš„HRVçš„é•¿åº¦ç­‰äºäº¤å‰æ³¨æ„åŠ›å¤´çš„æ€»æ•°ï¼Œæ¯ä¸ªå…ƒç´ ä»£è¡¨ç›¸åº”å¤´å¯¹äºç»™å®šè§†è§‰æ¦‚å¿µçš„é‡è¦æ€§ã€‚ä¸ºäº†éªŒè¯HRVä½œä¸ºå¯è§£é‡Šç‰¹å¾çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§æœ‰åºçš„å‡å¼±åˆ†ææ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†æ¦‚å¿µåŠ å¼ºå’Œæ¦‚å¿µè°ƒæ•´æ–¹æ³•ï¼Œå¹¶åº”ç”¨äºå¢å¼ºä¸‰ç§è§†è§‰ç”Ÿæˆä»»åŠ¡ã€‚ç»“æœè¡¨æ˜ï¼ŒHRVå¯ä»¥å‡å°‘å›¾åƒç”Ÿæˆä¸­å¯¹å¤šä¹‰è¯çš„è¯¯è§£ï¼ŒæˆåŠŸä¿®æ”¹å›¾åƒç¼–è¾‘ä¸­çš„äº”ä¸ªæŒ‘æˆ˜å±æ€§ï¼Œå¹¶å‡è½»å¤šæ¦‚å¿µç”Ÿæˆä¸­çš„ç¾éš¾æ€§å¿½è§†ã€‚æ€»ä½“è€Œè¨€ï¼Œæˆ‘ä»¬çš„å·¥ä½œæ¨è¿›äº†å¯¹äº¤å‰æ³¨æ„åŠ›å±‚çš„ç†è§£ï¼Œå¹¶ä»‹ç»äº†åœ¨å¤´éƒ¨å±‚é¢ç²¾ç»†æ§åˆ¶è¿™äº›å±‚çš„æ–°æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.02237v3">PDF</a> Accepted by ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢ç´¢äº†æ‰©æ•£æ¨¡å‹çš„è·¨æ³¨æ„åŠ›å±‚æœºåˆ¶ï¼Œé€šè¿‡æ„å»ºä¸äººç±»æŒ‡å®šè§†è§‰æ¦‚å¿µå¯¹é½çš„Head Relevance Vectorsï¼ˆHRVsï¼‰æ¥æé«˜æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚ç ”ç©¶é€šè¿‡æœ‰åºå‡å¼±åˆ†æéªŒè¯äº†HRVsçš„æœ‰æ•ˆæ€§ï¼Œå¹¶æå‡ºæ¦‚å¿µå¼ºåŒ–å’Œæ¦‚å¿µè°ƒæ•´æ–¹æ³•ï¼Œåº”ç”¨äºä¸‰ä¸ªè§†è§‰ç”Ÿæˆä»»åŠ¡ã€‚ç ”ç©¶å‘ç°ï¼ŒHRVsèƒ½å¤Ÿå‡å°‘å¤šä¹‰è¯åœ¨å›¾åƒç”Ÿæˆä¸­çš„è¯¯è§£ï¼ŒæˆåŠŸä¿®æ”¹å›¾åƒç¼–è¾‘ä¸­çš„äº”ä¸ªæŒ‘æˆ˜å±æ€§ï¼Œå¹¶ç¼“è§£å¤šæ¦‚å¿µç”Ÿæˆä¸­çš„ç¾éš¾æ€§å¿½è§†é—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥Head Relevance Vectorsï¼ˆHRVsï¼‰æé«˜æ‰©æ•£æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚</li>
<li>é€šè¿‡æœ‰åºå‡å¼±åˆ†æéªŒè¯äº†HRVsçš„æœ‰æ•ˆæ€§ã€‚</li>
<li>æå‡ºæ¦‚å¿µå¼ºåŒ–å’Œæ¦‚å¿µè°ƒæ•´æ–¹æ³•ï¼Œåº”ç”¨äºè§†è§‰ç”Ÿæˆä»»åŠ¡ã€‚</li>
<li>HRVsèƒ½å¤Ÿå‡å°‘å¤šä¹‰è¯åœ¨å›¾åƒç”Ÿæˆä¸­çš„è¯¯è§£ã€‚</li>
<li>æˆåŠŸä¿®æ”¹å›¾åƒç¼–è¾‘ä¸­çš„å¤šä¸ªæŒ‘æˆ˜å±æ€§ã€‚</li>
<li>ç¼“è§£å¤šæ¦‚å¿µç”Ÿæˆä¸­çš„ç¾éš¾æ€§å¿½è§†é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.02237">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1dca725e830d062bcbf4e1d71c92cf03.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7599c21eb18e31a00c55eee1b82f9d19.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-303543fb2cae857b3804fce74f7df6e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f1145e8b7b13c8823f511ab38a831818.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="A-Grey-box-Attack-against-Latent-Diffusion-Model-based-Image-Editing-by-Posterior-Collapse"><a href="#A-Grey-box-Attack-against-Latent-Diffusion-Model-based-Image-Editing-by-Posterior-Collapse" class="headerlink" title="A Grey-box Attack against Latent Diffusion Model-based Image Editing by   Posterior Collapse"></a>A Grey-box Attack against Latent Diffusion Model-based Image Editing by   Posterior Collapse</h2><p><strong>Authors:Zhongliang Guo, Chun Tong Lei, Lei Fang, Shuai Zhao, Yifei Qian, Jingyu Lin, Zeyu Wang, Cunjian Chen, Ognjen ArandjeloviÄ‡, Chun Pong Lau</strong></p>
<p>Recent advancements in generative AI, particularly Latent Diffusion Models (LDMs), have revolutionized image synthesis and manipulation. However, these generative techniques raises concerns about data misappropriation and intellectual property infringement. Adversarial attacks on machine learning models have been extensively studied, and a well-established body of research has extended these techniques as a benign metric to prevent the underlying misuse of generative AI. Current approaches to safeguarding images from manipulation by LDMs are limited by their reliance on model-specific knowledge and their inability to significantly degrade semantic quality of generated images. In response to these shortcomings, we propose the Posterior Collapse Attack (PCA) based on the observation that VAEs suffer from posterior collapse during training. Our method minimizes dependence on the white-box information of target models to get rid of the implicit reliance on model-specific knowledge. By accessing merely a small amount of LDM parameters, in specific merely the VAE encoder of LDMs, our method causes a substantial semantic collapse in generation quality, particularly in perceptual consistency, and demonstrates strong transferability across various model architectures. Experimental results show that PCA achieves superior perturbation effects on image generation of LDMs with lower runtime and VRAM. Our method outperforms existing techniques, offering a more robust and generalizable solution that is helpful in alleviating the socio-technical challenges posed by the rapidly evolving landscape of generative AI. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆå°¤å…¶æ˜¯æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMsï¼‰ï¼‰çš„æœ€æ–°è¿›å±•å½»åº•æ”¹å˜äº†å›¾åƒåˆæˆå’Œæ“çºµã€‚ç„¶è€Œï¼Œè¿™äº›ç”ŸæˆæŠ€æœ¯å¼•å‘äº†å…³äºæ•°æ®ä¸å½“ä½¿ç”¨å’ŒçŸ¥è¯†äº§æƒä¾µçŠ¯çš„æ‹…å¿§ã€‚æœºå™¨å­¦ä¹ æ¨¡å‹çš„å¯¹æŠ—æ€§æ”»å‡»å·²ç»å¾—åˆ°äº†å¹¿æ³›çš„ç ”ç©¶ï¼Œå¹¶ä¸”å·²æœ‰å¤§é‡ç ”ç©¶å°†è¿™äº›æŠ€æœ¯ä½œä¸ºä¸€ç§è‰¯æ€§æŒ‡æ ‡ï¼Œä»¥é˜²æ­¢ç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„æ½œåœ¨æ»¥ç”¨ã€‚å½“å‰ä¿æŠ¤å›¾åƒå…å—LDMæ“çºµçš„æ–¹æ³•å—é™äºå®ƒä»¬å¯¹ç‰¹å®šæ¨¡å‹çŸ¥è¯†çš„ä¾èµ–ï¼Œä»¥åŠå®ƒä»¬æ— æ³•æ˜¾è‘—é™ä½ç”Ÿæˆå›¾åƒçš„è¯­ä¹‰è´¨é‡ã€‚é’ˆå¯¹è¿™äº›ç¼ºç‚¹ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºåå´©æºƒæ”»å‡»ï¼ˆPCAï¼‰çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ˜¯æ ¹æ®å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEsï¼‰åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šå‡ºç°çš„åå´©æºƒç°è±¡çš„è§‚å¯Ÿè€Œæå‡ºçš„ã€‚æˆ‘ä»¬çš„æ–¹æ³•æœ€å°åŒ–äº†å¯¹ç›®æ ‡æ¨¡å‹ç™½ç›’ä¿¡æ¯çš„ä¾èµ–ï¼Œä»¥æ‘†è„±å¯¹ç‰¹å®šæ¨¡å‹çŸ¥è¯†çš„éšæ€§ä¾èµ–ã€‚é€šè¿‡ä»…è®¿é—®å°‘é‡LDMå‚æ•°ï¼Œç‰¹åˆ«æ˜¯LDMçš„VAEç¼–ç å™¨ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç”Ÿæˆè´¨é‡ä¸Šå¼•èµ·äº†å¤§é‡çš„è¯­ä¹‰å´©æºƒï¼Œç‰¹åˆ«æ˜¯åœ¨æ„ŸçŸ¥ä¸€è‡´æ€§æ–¹é¢ï¼Œå¹¶ä¸”åœ¨å„ç§æ¨¡å‹æ¶æ„ä¹‹é—´è¡¨ç°å‡ºå¼ºå¤§çš„å¯è½¬ç§»æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPCAåœ¨å›¾åƒç”Ÿæˆæ–¹é¢å®ç°äº†å¯¹LDMsçš„å“è¶Šæ‰°åŠ¨æ•ˆæœï¼Œå…·æœ‰æ›´ä½çš„è¿è¡Œæ—¶é—´å’ŒVRAMã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œæä¾›äº†ä¸€ç§æ›´ç¨³å¥å’Œå¯æ¨å¹¿çš„è§£å†³æ–¹æ¡ˆï¼Œæœ‰åŠ©äºç¼“è§£ç”Ÿæˆå¼äººå·¥æ™ºèƒ½å¿«é€Ÿæ¼”å˜æ‰€å¸¦æ¥çš„ç¤¾ä¼šæŠ€æœ¯æŒ‘æˆ˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.10901v3">PDF</a> 21 pages, 7 figures, 10 tables</p>
<p><strong>Summary</strong><br>     è¿‘æœŸç”Ÿæˆå¼AIæŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMsï¼‰åœ¨å›¾åƒåˆæˆå’Œæ“çºµæ–¹é¢çš„è¿›å±•å¼•å‘äº†æ•°æ®ä¸å½“ä½¿ç”¨å’ŒçŸ¥è¯†äº§æƒä¾µçŠ¯çš„æ‹…å¿§ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæå‡ºåŸºäºå˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEsï¼‰è®­ç»ƒè¿‡ç¨‹ä¸­çš„åå´©æºƒç°è±¡çš„Posterior Collapse Attackï¼ˆPCAï¼‰æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡å°‘é‡LDMå‚æ•°ï¼Œç‰¹åˆ«æ˜¯VAEç¼–ç å™¨çš„è®¿é—®ï¼Œå®ç°äº†ç”Ÿæˆè´¨é‡çš„è¯­ä¹‰å´©æºƒï¼Œç‰¹åˆ«æ˜¯åœ¨æ„ŸçŸ¥ä¸€è‡´æ€§æ–¹é¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPCAå¯¹LDMçš„å›¾åƒç”Ÿæˆå®ç°äº†å“è¶Šçš„æ‰°åŠ¨æ•ˆæœï¼Œè¿è¡Œæ—¶é—´çŸ­ï¼Œè™šæ‹Ÿå†…å­˜ï¼ˆVRAMï¼‰éœ€æ±‚ä½ï¼Œä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œä¸ºç¼“è§£ç”Ÿæˆå¼AIå¿«é€Ÿå‘å±•å¸¦æ¥çš„ç¤¾ä¼šæŠ€æœ¯æŒ‘æˆ˜æä¾›äº†æ›´ç¨³å¥å’Œé€šç”¨çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LDMçš„å¿«é€Ÿå‘å±•æ¨åŠ¨äº†å›¾åƒåˆæˆå’Œæ“çºµçš„è¿›æ­¥ï¼Œä½†å¸¦æ¥äº†æ•°æ®æ»¥ç”¨å’ŒçŸ¥è¯†äº§æƒé—®é¢˜çš„æ‹…å¿§ã€‚</li>
<li>å¯¹æŠ—æ”»å‡»æ˜¯è¯„ä¼°æœºå™¨å­¦ä¹ æ¨¡å‹å®‰å…¨æ€§çš„é‡è¦æ‰‹æ®µã€‚</li>
<li>PCAæ–¹æ³•åŸºäºVAEè®­ç»ƒä¸­çš„åå´©æºƒç°è±¡æå‡ºï¼Œå‡å°‘å¯¹ç›®æ ‡æ¨¡å‹çš„ä¾èµ–ã€‚</li>
<li>PCAé€šè¿‡è®¿é—®å°‘é‡LDMå‚æ•°ï¼ˆç‰¹åˆ«æ˜¯VAEç¼–ç å™¨ï¼‰å®ç°è¯­ä¹‰è´¨é‡æ˜¾è‘—ä¸‹é™ã€‚</li>
<li>PCAæ–¹æ³•åœ¨æ„ŸçŸ¥ä¸€è‡´æ€§æ–¹é¢è¡¨ç°å°¤ä¸ºçªå‡ºï¼Œå±•ç¤ºäº†å¯¹å„ç§æ¨¡å‹æ¶æ„çš„å¼ºå¤§å¯è½¬ç§»æ€§ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒPCAåœ¨è¿è¡Œæ—¶å’ŒVRAMéœ€æ±‚æ–¹é¢ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.10901">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-86aefd3766a436bb3240745eb2241944.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6fc922dd8d446f1c4a87d0e85b911040.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d7ef0b8cf8f9056d64eceab17d615b56.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a7217d6a1c6be6a761f6c7049526ed4e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c7fbbb59e13183db5f48557f5aede31.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-26/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-26/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-26/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-864857c43fe0a6a419d2932424a24711.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-26  MDN Mamba-Driven Dualstream Network For Medical Hyperspectral Image   Segmentation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-26
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-26/NeRF/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-02-26\./crop_NeRF/2502.14931v1/page_4_0.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-26  Semantic Neural Radiance Fields for Multi-Date Satellite Data
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-26
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30341.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
