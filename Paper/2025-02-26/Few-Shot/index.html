<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-26  Large Language Models are Powerful EHR Encoders">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-1ae727e6933c739cea874e5a04534456.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-26
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    83 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-02-26-æ›´æ–°"><a href="#2025-02-26-æ›´æ–°" class="headerlink" title="2025-02-26 æ›´æ–°"></a>2025-02-26 æ›´æ–°</h1><h2 id="Large-Language-Models-are-Powerful-EHR-Encoders"><a href="#Large-Language-Models-are-Powerful-EHR-Encoders" class="headerlink" title="Large Language Models are Powerful EHR Encoders"></a>Large Language Models are Powerful EHR Encoders</h2><p><strong>Authors:Stefan Hegselmann, Georg von Arnim, Tillmann Rheude, Noel Kronenberg, David Sontag, Gerhard Hindricks, Roland Eils, Benjamin Wild</strong></p>
<p>Electronic Health Records (EHRs) offer rich potential for clinical prediction, yet their inherent complexity and heterogeneity pose significant challenges for traditional machine learning approaches. Domain-specific EHR foundation models trained on large collections of unlabeled EHR data have demonstrated promising improvements in predictive accuracy and generalization; however, their training is constrained by limited access to diverse, high-quality datasets and inconsistencies in coding standards and healthcare practices. In this study, we explore the possibility of using general-purpose Large Language Models (LLMs) based embedding methods as EHR encoders. By serializing patient records into structured Markdown text, transforming codes into human-readable descriptors, we leverage the extensive generalization capabilities of LLMs pretrained on vast public corpora, thereby bypassing the need for proprietary medical datasets. We systematically evaluate two state-of-the-art LLM-embedding models, GTE-Qwen2-7B-Instruct and LLM2Vec-Llama3.1-8B-Instruct, across 15 diverse clinical prediction tasks from the EHRSHOT benchmark, comparing their performance to an EHRspecific foundation model, CLIMBR-T-Base, and traditional machine learning baselines. Our results demonstrate that LLM-based embeddings frequently match or exceed the performance of specialized models, even in few-shot settings, and that their effectiveness scales with the size of the underlying LLM and the available context window. Overall, our findings demonstrate that repurposing LLMs for EHR encoding offers a scalable and effective approach for clinical prediction, capable of overcoming the limitations of traditional EHR modeling and facilitating more interoperable and generalizable healthcare applications. </p>
<blockquote>
<p>ç”µå­å¥åº·è®°å½•ï¼ˆEHRsï¼‰åœ¨ä¸´åºŠé¢„æµ‹æ–¹é¢å…·æœ‰å·¨å¤§çš„æ½œåŠ›ï¼Œç„¶è€Œå…¶å›ºæœ‰çš„å¤æ‚æ€§å’Œå¼‚è´¨æ€§ç»™ä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ æ–¹æ³•å¸¦æ¥äº†å·¨å¤§çš„æŒ‘æˆ˜ã€‚åŸºäºå¤§å‹æ— æ ‡ç­¾EHRæ•°æ®çš„é¢†åŸŸç‰¹å®šEHRåŸºç¡€æ¨¡å‹å·²ç»åœ¨é¢„æµ‹å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›æ–¹é¢æ˜¾ç¤ºå‡ºæœ‰å¸Œæœ›çš„æ”¹è¿›ï¼›ç„¶è€Œï¼Œå®ƒä»¬çš„è®­ç»ƒå—åˆ°å¤šæ ·ã€é«˜è´¨é‡æ•°æ®é›†è®¿é—®å—é™ä»¥åŠç¼–ç æ ‡å‡†å’ŒåŒ»ç–—ä¿å¥å®è·µä¸ä¸€è‡´çš„åˆ¶çº¦ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ä½¿ç”¨åŸºäºé€šç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„åµŒå…¥æ–¹æ³•ä½œä¸ºEHRç¼–ç å™¨çš„å¯èƒ½æ€§ã€‚é€šè¿‡å°†æ‚£è€…è®°å½•åºåˆ—åŒ–ä¸ºç»“æ„åŒ–Markdownæ–‡æœ¬ï¼Œå°†ä»£ç è½¬æ¢ä¸ºäººç±»å¯è¯»çš„æè¿°ç¬¦ï¼Œæˆ‘ä»¬å……åˆ†åˆ©ç”¨äº†LLMsåœ¨å¤§é‡å…¬å…±è¯­æ–™åº“ä¸Šçš„é¢„è®­ç»ƒå¸¦æ¥çš„å¹¿æ³›æ³›åŒ–èƒ½åŠ›ï¼Œä»è€Œç»•è¿‡äº†å¯¹ä¸“æœ‰åŒ»ç–—æ•°æ®é›†çš„éœ€æ±‚ã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°è¯„ä¼°äº†ä¸¤ç§æœ€å…ˆè¿›çš„LLMåµŒå…¥æ¨¡å‹â€”â€”GTE-Qwen2-7B-Instructå’ŒLLM2Vec-Llama3.1-8B-Instructï¼Œåœ¨EHRSHOTåŸºå‡†æµ‹è¯•çš„15ä¸ªä¸åŒä¸´åºŠé¢„æµ‹ä»»åŠ¡ä¸Šï¼Œä¸EHRç‰¹å®šåŸºç¡€æ¨¡å‹CLIMBR-T-Baseå’Œä¼ ç»Ÿæœºå™¨å­¦ä¹ åŸºçº¿è¿›è¡Œäº†æ€§èƒ½æ¯”è¾ƒã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒåŸºäºLLMçš„åµŒå…¥ç»å¸¸ä¸ä¸“ä¸šåŒ–æ¨¡å‹çš„æ€§èƒ½ç›¸åŒ¹é…ç”šè‡³æ›´å¥½ï¼Œå³ä½¿åœ¨å°‘æ ·æœ¬æƒ…å†µä¸‹ä¹Ÿæ˜¯å¦‚æ­¤ï¼Œå®ƒä»¬çš„æœ‰æ•ˆæ€§éšç€åº•å±‚LLMçš„å¤§å°å’Œå¯ç”¨ä¸Šä¸‹æ–‡çª—å£çš„å¤§å°è€Œæ‰©å±•ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå°†LLMsé‡æ–°ç”¨äºEHRç¼–ç ä¸ºä¸´åºŠé¢„æµ‹æä¾›äº†ä¸€ç§å¯æ‰©å±•å’Œæœ‰æ•ˆçš„é€”å¾„ï¼Œèƒ½å¤Ÿå…‹æœä¼ ç»ŸEHRå»ºæ¨¡çš„é™åˆ¶ï¼Œä¿ƒè¿›æ›´äº’è”å’Œæ›´é€šç”¨çš„åŒ»ç–—ä¿å¥åº”ç”¨ç¨‹åºçš„å¼€å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17403v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬ç ”ç©¶æ¢ç´¢äº†ä½¿ç”¨é€šç”¨çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä½œä¸ºç”µå­å¥åº·è®°å½•ï¼ˆEHRsï¼‰ç¼–ç å™¨çš„å¯èƒ½æ€§ã€‚é€šè¿‡å°†æ‚£è€…è®°å½•åºåˆ—åŒ–ä¸ºç»“æ„åŒ–Markdownæ–‡æœ¬ï¼Œå°†ä»£ç è½¬æ¢ä¸ºäººç±»å¯è¯»çš„æè¿°ç¬¦ï¼Œç ”ç©¶åˆ©ç”¨åœ¨å¤§é‡å…¬å…±è¯­æ–™åº“ä¸Šé¢„è®­ç»ƒLLMsçš„å¹¿æ³›æ³›åŒ–èƒ½åŠ›ï¼Œä»è€Œç»•è¿‡å¯¹ä¸“æœ‰åŒ»ç–—æ•°æ®é›†çš„éœ€æ±‚ã€‚ç³»ç»Ÿè¯„ä¼°äº†ä¸¤ç§å…ˆè¿›LLMåµŒå…¥æ¨¡å‹åœ¨EHRSHOTåŸºå‡†æµ‹è¯•çš„15ä¸ªä¸åŒä¸´åºŠé¢„æµ‹ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œå¹¶ä¸EHRç‰¹å®šåŸºç¡€æ¨¡å‹CLIMBR-T-Baseå’Œä¼ ç»Ÿæœºå™¨å­¦ä¹ åŸºçº¿è¿›è¡Œäº†æ¯”è¾ƒã€‚ç»“æœè¡¨æ˜ï¼ŒLLMåŸºäºåµŒå…¥çš„è¡¨ç¤ºåœ¨å¾ˆå¤šæƒ…å†µä¸‹ä¸ä¸“é—¨æ¨¡å‹çš„æ€§èƒ½ç›¸åŒ¹é…ç”šè‡³è¶…è¶Šï¼Œå³ä½¿åœ¨å°‘æ ·æœ¬æƒ…å†µä¸‹ä¹Ÿæ˜¯å¦‚æ­¤ï¼Œä¸”å…¶æœ‰æ•ˆæ€§éšç€åº•å±‚LLMçš„å¤§å°å’Œå¯ç”¨ä¸Šä¸‹æ–‡çª—å£çš„å¤§å°è€Œæé«˜ã€‚æ€»ä½“è€Œè¨€ï¼Œç ”ç©¶å‘ç°å°†LLMsç”¨äºEHRç¼–ç æä¾›äº†ä¸€ç§å¯æ‰©å±•å’Œæœ‰æ•ˆçš„ä¸´åºŠé¢„æµ‹æ–¹æ³•ï¼Œèƒ½å¤Ÿå…‹æœä¼ ç»ŸEHRå»ºæ¨¡çš„é™åˆ¶ï¼Œä¿ƒè¿›æ›´äº’æ“ä½œå’Œæ›´é€šç”¨çš„åŒ»ç–—ä¿å¥åº”ç”¨ç¨‹åºçš„å¼€å‘ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç”µå­å¥åº·è®°å½•ï¼ˆEHRsï¼‰è•´å«ä¸°å¯Œçš„ä¸´åºŠé¢„æµ‹æ½œåŠ›ï¼Œä½†å…¶å¤æ‚æ€§å’Œå¼‚è´¨æ€§ä¸ºä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•å¸¦æ¥æŒ‘æˆ˜ã€‚</li>
<li>åŸŸåç‰¹å®šçš„EHRåŸºç¡€æ¨¡å‹å·²åœ¨é¢„æµ‹å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›æ–¹é¢æ˜¾ç¤ºå‡ºæœ‰å¸Œæœ›çš„æ”¹è¿›ï¼Œä½†å…¶è®­ç»ƒå—é™äºå¤šæ ·ã€é«˜è´¨é‡æ•°æ®é›†çš„ä¸ç¨³å®šå’Œç¼–ç æ ‡å‡†å’ŒåŒ»ç–—ä¿å¥å®è·µçš„ä¸ä¸€è‡´ã€‚</li>
<li>ç ”ç©¶æ¢ç´¢äº†ä½¿ç”¨é€šç”¨çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä½œä¸ºEHRç¼–ç å™¨çš„å¯èƒ½æ€§ï¼Œåˆ©ç”¨LLMsåœ¨å¤§é‡å…¬å…±è¯­æ–™åº“ä¸Šçš„é¢„è®­ç»ƒæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡åºåˆ—åŒ–æ‚£è€…è®°å½•å’Œè½¬æ¢ä»£ç ä¸ºäººç±»å¯è¯»çš„æè¿°ç¬¦ï¼Œç ”ç©¶ç»•è¿‡äº†å¯¹ä¸“æœ‰åŒ»ç–—æ•°æ®é›†çš„éœ€æ±‚ã€‚</li>
<li>LLMåµŒå…¥æ¨¡å‹åœ¨EHRSHOTåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä¸ä¸“é—¨æ¨¡å‹ç›¸åŒ¹é…ç”šè‡³è¶…è¶Šã€‚</li>
<li>LLMsçš„æœ‰æ•ˆæ€§åœ¨å°‘æ ·æœ¬æƒ…å†µä¸‹å°¤ä¸ºæ˜¾è‘—ï¼Œä¸”éšç€æ¨¡å‹å¤§å°å’Œä¸Šä¸‹æ–‡çª—å£çš„å¢åŠ è€Œæé«˜ã€‚</li>
<li>ä½¿ç”¨LLMsè¿›è¡ŒEHRç¼–ç ä¸ºä¸´åºŠé¢„æµ‹æä¾›äº†ä¸€ç§å¯æ‰©å±•å’Œæœ‰æ•ˆçš„æ–¹æ³•ï¼Œèƒ½å¤Ÿå…‹æœä¼ ç»ŸEHRå»ºæ¨¡çš„é™åˆ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17403">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-74b0a518a4b17bd9f2ad4616dbe81fcc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cfedd0a7cb65190d72a64a7503e8e23c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89591a01aaa7b4088fef629c9c365d1d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d61db9bbf53dcfc812bf0a801e742f26.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bf61cdd8860ed37ba16bc3d176e4343a.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="FIG-Forward-Inverse-Generation-for-Low-Resource-Domain-specific-Event-Detection"><a href="#FIG-Forward-Inverse-Generation-for-Low-Resource-Domain-specific-Event-Detection" class="headerlink" title="FIG: Forward-Inverse Generation for Low-Resource Domain-specific Event   Detection"></a>FIG: Forward-Inverse Generation for Low-Resource Domain-specific Event   Detection</h2><p><strong>Authors:Tanmay Parekh, Yuxuan Dong, Lucas Bandarkar, Artin Kim, I-Hung Hsu, Kai-Wei Chang, Nanyun Peng</strong></p>
<p>Event Detection (ED) is the task of identifying typed event mentions of interest from natural language text, which benefits domain-specific reasoning in biomedical, legal, and epidemiological domains. However, procuring supervised data for thousands of events for various domains is a laborious and expensive task. To this end, existing works have explored synthetic data generation via forward (generating labels for unlabeled sentences) and inverse (generating sentences from generated labels) generations. However, forward generation often produces noisy labels, while inverse generation struggles with domain drift and incomplete event annotations. To address these challenges, we introduce FIG, a hybrid approach that leverages inverse generation for high-quality data synthesis while anchoring it to domain-specific cues extracted via forward generation on unlabeled target data. FIG further enhances its synthetic data by adding missing annotations through forward generation-based refinement. Experimentation on three ED datasets from diverse domains reveals that FIG outperforms the best baseline achieving average gains of 3.3% F1 and 5.4% F1 in the zero-shot and few-shot settings respectively. Analyzing the generated trigger hit rate and human evaluation substantiates FIGâ€™s superior domain alignment and data quality compared to existing baselines. </p>
<blockquote>
<p>äº‹ä»¶æ£€æµ‹ï¼ˆEDï¼‰æ˜¯ä»è‡ªç„¶è¯­è¨€æ–‡æœ¬ä¸­è¯†åˆ«å‡ºæœ‰è¶£çš„äº‹ä»¶ç±»å‹æåŠçš„ä»»åŠ¡ï¼Œè¿™æœ‰åŠ©äºç”Ÿç‰©åŒ»å­¦ã€æ³•å¾‹å’Œæµè¡Œç—…å­¦ç­‰é¢†åŸŸçš„ç‰¹å®šé¢†åŸŸæ¨ç†ã€‚ç„¶è€Œï¼Œä¸ºå„ç§é¢†åŸŸæˆåƒä¸Šä¸‡çš„äº‹ä»¶è·å–ç›‘ç£æ•°æ®æ˜¯ä¸€é¡¹è€—æ—¶ä¸”æ˜‚è´µçš„ä»»åŠ¡ã€‚ä¸ºæ­¤ï¼Œç°æœ‰ç ”ç©¶å·²ç»æ¢ç´¢äº†æ­£å‘ç”Ÿæˆï¼ˆä¸ºæœªæ ‡è®°çš„å¥å­ç”Ÿæˆæ ‡ç­¾ï¼‰å’Œé€†å‘ç”Ÿæˆï¼ˆä»ç”Ÿæˆçš„æ ‡ç­¾ç”Ÿæˆå¥å­ï¼‰çš„åˆæˆæ•°æ®ç”Ÿæˆæ–¹æ³•ã€‚ç„¶è€Œï¼Œæ­£å‘ç”Ÿæˆå¾€å¾€ä¼šç”Ÿæˆå™ªå£°æ ‡ç­¾ï¼Œè€Œé€†å‘ç”Ÿæˆåˆ™é¢ä¸´é¢†åŸŸæ¼‚ç§»å’Œäº‹ä»¶æ³¨è§£ä¸å®Œæ•´çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†FIGï¼Œè¿™æ˜¯ä¸€ç§æ··åˆæ–¹æ³•ï¼Œå®ƒåˆ©ç”¨é€†å‘ç”Ÿæˆè¿›è¡Œé«˜è´¨é‡æ•°æ®åˆæˆï¼ŒåŒæ—¶ä»¥é€šè¿‡æ­£å‘ç”Ÿæˆåœ¨æœªæ ‡è®°çš„ç›®æ ‡æ•°æ®ä¸Šæå–çš„é¢†åŸŸç‰¹å®šçº¿ç´¢ä¸ºåŸºç¡€è¿›è¡Œé”šå®šã€‚FIGè¿˜é€šè¿‡åŸºäºæ­£å‘ç”Ÿæˆçš„ç»†åŒ–æ¥è¡¥å……ç¼ºå¤±çš„æ³¨é‡Šï¼Œè¿›ä¸€æ­¥å¢å¼ºå…¶åˆæˆæ•°æ®ã€‚åœ¨ä¸‰ä¸ªæ¥è‡ªä¸åŒé¢†åŸŸçš„äº‹ä»¶æ£€æµ‹æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸æœ€ä½³åŸºçº¿ç›¸æ¯”ï¼ŒFIGåœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬è®¾ç½®ä¸‹åˆ†åˆ«å®ç°äº†å¹³å‡F1å¾—åˆ†æé«˜3.3%å’Œ5.4%ã€‚å¯¹ç”Ÿæˆè§¦å‘å‘½ä¸­ç‡çš„åˆ†æå’Œäººå·¥è¯„ä¼°è¯å®ï¼Œä¸ç°æœ‰åŸºçº¿ç›¸æ¯”ï¼ŒFIGåœ¨é¢†åŸŸå¯¹é½å’Œæ•°æ®è´¨é‡æ–¹é¢å…·æœ‰ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17394v1">PDF</a> Under review at ACL ARR Feb 2025</p>
<p><strong>Summary</strong></p>
<p>äº‹ä»¶æ£€æµ‹ï¼ˆEDï¼‰æ˜¯ä»è‡ªç„¶è¯­è¨€æ–‡æœ¬ä¸­è¯†åˆ«ç‰¹å®šäº‹ä»¶æåŠçš„ä»»åŠ¡ï¼Œå¯¹ç‰¹å®šé¢†åŸŸï¼ˆå¦‚ç”Ÿç‰©åŒ»å­¦ã€æ³•å¾‹å’Œæµè¡Œç—…å­¦é¢†åŸŸï¼‰çš„æ¨ç†å…·æœ‰é‡å¤§æ„ä¹‰ã€‚ç„¶è€Œï¼Œä¸ºæ•°åƒä¸ªäº‹ä»¶è·å–ç›‘ç£æ•°æ®æ˜¯ä¸€ä¸ªè‰°å·¨ä¸”æ˜‚è´µçš„ä»»åŠ¡ã€‚ç°æœ‰ç ”ç©¶å·²å°è¯•é€šè¿‡æ­£å‘ç”Ÿæˆï¼ˆä¸ºæœªæ ‡è®°çš„å¥å­ç”Ÿæˆæ ‡ç­¾ï¼‰å’Œé€†å‘ç”Ÿæˆï¼ˆä»ç”Ÿæˆçš„æ ‡ç­¾ç”Ÿæˆå¥å­ï¼‰çš„æ–¹å¼è¿›è¡Œåˆæˆæ•°æ®ç”Ÿæˆã€‚ä½†æ­£å‘ç”Ÿæˆå¾€å¾€äº§ç”Ÿå™ªå£°æ ‡ç­¾ï¼Œè€Œé€†å‘ç”Ÿæˆåˆ™é¢ä¸´é¢†åŸŸæ¼‚ç§»å’Œäº‹ä»¶æ ‡æ³¨ä¸å®Œæ•´çš„é—®é¢˜ã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†FIGæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨é€†å‘ç”Ÿæˆè¿›è¡Œé«˜è´¨é‡æ•°æ®åˆæˆï¼ŒåŒæ—¶é€šè¿‡æ­£å‘ç”Ÿæˆåœ¨æœªæ ‡è®°çš„ç›®æ ‡æ•°æ®ä¸Šæå–çš„ç‰¹å®šé¢†åŸŸçº¿ç´¢è¿›è¡Œé”šå®šã€‚æ­¤å¤–ï¼ŒFIGè¿˜é€šè¿‡æ­£å‘ç”Ÿæˆè¿›è¡Œç²¾ç‚¼æ¥è¡¥å……ç¼ºå¤±çš„æ ‡æ³¨ã€‚åœ¨ä¸‰ä¸ªæ¥è‡ªä¸åŒé¢†åŸŸçš„EDæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸æœ€ä½³åŸºçº¿ç›¸æ¯”ï¼ŒFIGåœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬åœºæ™¯ä¸‹å¹³å‡F1å¾—åˆ†åˆ†åˆ«æé«˜äº†3.3%å’Œ5.4%ã€‚å¯¹ç”Ÿæˆè§¦å‘å‘½ä¸­ç‡å’Œäººå·¥è¯„ä¼°çš„åˆ†æè¯å®äº†FIGç›¸è¾ƒäºç°æœ‰åŸºçº¿åœ¨é¢†åŸŸå¯¹é½å’Œæ•°æ®è´¨é‡æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äº‹ä»¶æ£€æµ‹ï¼ˆEDï¼‰æ˜¯ä»æ–‡æœ¬ä¸­è¯†åˆ«ç‰¹å®šäº‹ä»¶çš„ä»»åŠ¡ï¼Œå¯¹ç‰¹å®šé¢†åŸŸæœ‰é‡è¦æ„ä¹‰ã€‚</li>
<li>ä¸ºäº‹ä»¶è·å–ç›‘ç£æ•°æ®æ—¢å›°éš¾åˆæ˜‚è´µã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¦‚æ­£å‘å’Œé€†å‘æ•°æ®ç”Ÿæˆå­˜åœ¨æŒ‘æˆ˜ï¼Œå¦‚å™ªå£°æ ‡ç­¾å’Œé¢†åŸŸæ¼‚ç§»é—®é¢˜ã€‚</li>
<li>å¼•å…¥çš„FIGæ–¹æ³•ç»“åˆäº†æ­£å‘å’Œé€†å‘ç”Ÿæˆï¼Œæ—¨åœ¨æé«˜æ•°æ®åˆæˆçš„è´¨é‡å’Œé¢†åŸŸç‰¹å¼‚æ€§ã€‚</li>
<li>é€šè¿‡å®éªŒéªŒè¯ï¼ŒFIGåœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬åœºæ™¯ä¸‹å‡ä¼˜äºç°æœ‰åŸºçº¿ã€‚</li>
<li>ç”Ÿæˆè§¦å‘å‘½ä¸­ç‡åˆ†æå’Œäººå·¥è¯„ä¼°è¯æ˜äº†FIGåœ¨é¢†åŸŸå¯¹é½å’Œæ•°æ®è´¨é‡æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17394">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-007571303f402535d7b515f24fe5c43d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b70b95ed0aa6bd754b6cf92d2e40191e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0db55f60ae961d1a85c615deff02384e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a5a25f0e744897d0faa03408ab8c39ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f2c102bbc1d378fbef8c9f7c1d7d62a.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Mutual-Reinforcement-of-LLM-Dialogue-Synthesis-and-Summarization-Capabilities-for-Few-Shot-Dialogue-Summarization"><a href="#Mutual-Reinforcement-of-LLM-Dialogue-Synthesis-and-Summarization-Capabilities-for-Few-Shot-Dialogue-Summarization" class="headerlink" title="Mutual Reinforcement of LLM Dialogue Synthesis and Summarization   Capabilities for Few-Shot Dialogue Summarization"></a>Mutual Reinforcement of LLM Dialogue Synthesis and Summarization   Capabilities for Few-Shot Dialogue Summarization</h2><p><strong>Authors:Yen-Ju Lu, Ting-Yao Hu, Hema Swetha Koppula, Hadi Pouransari, Jen-Hao Rick Chang, Yin Xia, Xiang Kong, Qi Zhu, Simon Wang, Oncel Tuzel, Raviteja Vemulapalli</strong></p>
<p>In this work, we propose Mutual Reinforcing Data Synthesis (MRDS) within LLMs to improve few-shot dialogue summarization task. Unlike prior methods that require external knowledge, we mutually reinforce the LLM&#39;s dialogue synthesis and summarization capabilities, allowing them to complement each other during training and enhance overall performances. The dialogue synthesis capability is enhanced by directed preference optimization with preference scoring from summarization capability. The summarization capability is enhanced by the additional high quality dialogue-summary paired data produced by the dialogue synthesis capability. By leveraging the proposed MRDS mechanism, we elicit the internal knowledge of LLM in the format of synthetic data, and use it to augment the few-shot real training dataset. Empirical results demonstrate that our method improves dialogue summarization, achieving a 1.5% increase in ROUGE scores and a 0.3% improvement in BERT scores in few-shot settings. Furthermore, our method attains the highest average scores in human evaluations, surpassing both the pre-trained models and the baselines fine-tuned solely for summarization tasks. </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„ç›¸äº’å¼ºåŒ–æ•°æ®åˆæˆï¼ˆMRDSï¼‰æ–¹æ³•ï¼Œä»¥æé«˜å°æ ·å¯¹è¯æ‘˜è¦ä»»åŠ¡çš„æ•ˆæœã€‚ä¸åŒäºéœ€è¦å¤–éƒ¨çŸ¥è¯†çš„å…ˆå‰æ–¹æ³•ï¼Œæˆ‘ä»¬ç›¸äº’å¼ºåŒ–LLMçš„å¯¹è¯åˆæˆå’Œæ‘˜è¦èƒ½åŠ›ï¼Œä½¿å®ƒä»¬åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­èƒ½å¤Ÿç›¸äº’è¡¥å……ï¼Œä»è€Œæé«˜æ•´ä½“æ€§èƒ½ã€‚å¯¹è¯åˆæˆèƒ½åŠ›é€šè¿‡åŸºäºæ‘˜è¦èƒ½åŠ›çš„åå¥½è¯„åˆ†è¿›è¡Œå®šå‘åå¥½ä¼˜åŒ–è€Œå¢å¼ºã€‚æ‘˜è¦èƒ½åŠ›é€šè¿‡å¯¹å¯¹è¯åˆæˆèƒ½åŠ›äº§ç”Ÿçš„é¢å¤–é«˜è´¨é‡å¯¹è¯æ‘˜è¦é…å¯¹æ•°æ®è¿›è¡Œå¢å¼ºã€‚é€šè¿‡åˆ©ç”¨æå‡ºçš„MRDSæœºåˆ¶ï¼Œæˆ‘ä»¬ä»¥åˆæˆæ•°æ®çš„å½¢å¼æ¿€å‘LLMçš„å†…éƒ¨çŸ¥è¯†ï¼Œå¹¶å°†å…¶ç”¨äºæ‰©å……å°æ ·çœŸå®è®­ç»ƒæ•°æ®é›†ã€‚ç»éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æé«˜äº†å¯¹è¯æ‘˜è¦çš„æ•ˆæœï¼Œåœ¨å°æ ·è®¾ç½®ä¸‹ROUGEå¾—åˆ†æé«˜äº†1.5%ï¼ŒBERTå¾—åˆ†æé«˜äº†0.3%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨äººç±»è¯„ä¼°ä¸­è·å¾—äº†æœ€é«˜å¹³å‡åˆ†ï¼Œè¶…è¿‡äº†ä»…é’ˆå¯¹æ‘˜è¦ä»»åŠ¡è¿›è¡Œé¢„è®­ç»ƒçš„æ¨¡å‹å’ŒåŸºçº¿æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17328v1">PDF</a> NAACL 2025 Findings</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºäº†åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­é‡‡ç”¨ç›¸äº’å¼ºåŒ–æ•°æ®åˆæˆï¼ˆMRDSï¼‰æ–¹æ³•ï¼Œä»¥æé«˜å°‘æ ·æœ¬å¯¹è¯æ‘˜è¦ä»»åŠ¡çš„æ•ˆæœã€‚ä¸åŒäºéœ€è¦å¤–éƒ¨çŸ¥è¯†çš„æ–¹æ³•ï¼Œæˆ‘ä»¬ç›¸äº’å¼ºåŒ–LLMçš„å¯¹è¯åˆæˆå’Œæ‘˜è¦èƒ½åŠ›ï¼Œä½¿å…¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç›¸äº’è¡¥å……ï¼Œä»è€Œæé«˜æ•´ä½“æ€§èƒ½ã€‚é€šè¿‡å¯¹è¯åˆæˆçš„å®šå‘åå¥½ä¼˜åŒ–å’Œæ‘˜è¦èƒ½åŠ›çš„åå¥½è¯„åˆ†ï¼Œå¢å¼ºäº†å¯¹è¯åˆæˆèƒ½åŠ›ã€‚åŒæ—¶ï¼Œé€šè¿‡å¯¹è¯åˆæˆäº§ç”Ÿçš„é™„åŠ é«˜è´¨é‡å¯¹è¯æ‘˜è¦é…å¯¹æ•°æ®ï¼Œæé«˜äº†æ‘˜è¦èƒ½åŠ›ã€‚åˆ©ç”¨æå‡ºçš„MRDSæœºåˆ¶ï¼Œæˆ‘ä»¬ä»¥åˆæˆæ•°æ®çš„å½¢å¼æ¿€å‘LLMçš„å†…éƒ¨çŸ¥è¯†ï¼Œå¹¶å°†å…¶ç”¨äºå¢å¼ºå°‘é‡çš„çœŸå®è®­ç»ƒæ•°æ®é›†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æé«˜äº†å¯¹è¯æ‘˜è¦çš„æ•ˆæœï¼Œåœ¨å°‘æ ·æœ¬ç¯å¢ƒä¸‹ROUGEå¾—åˆ†æé«˜äº†1.5%ï¼ŒBERTå¾—åˆ†æé«˜äº†0.3%ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨äººç±»è¯„ä¼°ä¸­è·å¾—äº†æœ€é«˜å¹³å‡åˆ†ï¼Œè¶…è¶Šäº†é¢„è®­ç»ƒæ¨¡å‹å’Œä»…é’ˆå¯¹æ‘˜è¦ä»»åŠ¡è¿›è¡Œå¾®è°ƒçš„åŸºç¡€æ¨¡å‹ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•â€”â€”ç›¸äº’å¼ºåŒ–æ•°æ®åˆæˆï¼ˆMRDSï¼‰ï¼Œç”¨äºæ”¹è¿›å°‘æ ·æœ¬å¯¹è¯æ‘˜è¦ä»»åŠ¡ã€‚</li>
<li>MRDSæ–¹æ³•ä¸åŒäºä¾èµ–å¤–éƒ¨çŸ¥è¯†çš„æ–¹æ³•ï¼Œå®ƒé€šè¿‡ç›¸äº’å¼ºåŒ–å¯¹è¯åˆæˆå’Œæ‘˜è¦èƒ½åŠ›æ¥æå‡LLMçš„æ€§èƒ½ã€‚</li>
<li>å¯¹è¯åˆæˆèƒ½åŠ›é€šè¿‡å®šå‘åå¥½ä¼˜åŒ–å’Œæ‘˜è¦èƒ½åŠ›çš„åå¥½è¯„åˆ†å¾—åˆ°å¢å¼ºã€‚</li>
<li>åˆ©ç”¨MRDSæœºåˆ¶æ¿€å‘LLMçš„å†…éƒ¨çŸ¥è¯†ï¼Œå¹¶ä»¥åˆæˆæ•°æ®çš„å½¢å¼ç”¨äºå¢å¼ºçœŸå®è®­ç»ƒæ•°æ®é›†ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒMRDSæ–¹æ³•åœ¨å°‘æ ·æœ¬ç¯å¢ƒä¸‹æé«˜äº†å¯¹è¯æ‘˜è¦çš„ROUGEå’ŒBERTå¾—åˆ†ã€‚</li>
<li>MRDSæ–¹æ³•åœ¨äººç±»è¯„ä¼°ä¸­è¡¨ç°æœ€ä½³ï¼Œè¶…è¶Šäº†é¢„è®­ç»ƒæ¨¡å‹å’Œé’ˆå¯¹æ‘˜è¦ä»»åŠ¡çš„åŸºçº¿æ¨¡å‹ã€‚</li>
<li>è¯¥æ–¹æ³•å¼ºè°ƒäº†å†…éƒ¨çŸ¥è¯†åœ¨å°‘æ ·æœ¬å­¦ä¹ ä¸­çš„é‡è¦æ€§ï¼Œå¹¶å±•ç¤ºäº†å¦‚ä½•åˆ©ç”¨LLMçš„å†…éƒ¨çŸ¥è¯†æ¥æé«˜æ‘˜è¦ä»»åŠ¡çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17328">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-00c07ae51fe6e2ae32a997a6154d0db8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b931c028495d010a7491da3b7819baf5.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Alpha-SQL-Zero-Shot-Text-to-SQL-using-Monte-Carlo-Tree-Search"><a href="#Alpha-SQL-Zero-Shot-Text-to-SQL-using-Monte-Carlo-Tree-Search" class="headerlink" title="Alpha-SQL: Zero-Shot Text-to-SQL using Monte Carlo Tree Search"></a>Alpha-SQL: Zero-Shot Text-to-SQL using Monte Carlo Tree Search</h2><p><strong>Authors:Boyan Li, Jiayi Zhang, Ju Fan, Yanwei Xu, Chong Chen, Nan Tang, Yuyu Luo</strong></p>
<p>Text-to-SQL, which enables natural language interaction with databases, serves as a pivotal method across diverse industries. With new, more powerful large language models (LLMs) emerging every few months, fine-tuning has become incredibly costly, labor-intensive, and error-prone. As an alternative, zero-shot Text-to-SQL, which leverages the growing knowledge and reasoning capabilities encoded in LLMs without task-specific fine-tuning, presents a promising and more challenging direction. To address this challenge, we propose Alpha-SQL, a novel approach that leverages a Monte Carlo Tree Search (MCTS) framework to iteratively infer SQL construction actions based on partial SQL query states. To enhance the frameworkâ€™s reasoning capabilities, we introduce LLM-as-Action-Model to dynamically generate SQL construction actions during the MCTS process, steering the search toward more promising SQL queries. Moreover, Alpha-SQL employs a self-supervised reward function to evaluate the quality of candidate SQL queries, ensuring more accurate and efficient query generation. Experimental results show that Alpha-SQL achieves 69.7% execution accuracy on the BIRD development set, using a 32B open-source LLM without fine-tuning. Alpha-SQL outperforms the best previous zero-shot approach based on GPT-4o by 2.5% on the BIRD development set. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°SQLçš„æŠ€æœ¯ï¼Œèƒ½å¤Ÿå®ç°ä¸æ•°æ®åº“çš„è‡ªç„¶è¯­è¨€äº¤äº’ï¼Œæ˜¯å„è¡Œä¸šå…³é”®çš„æ–¹æ³•ã€‚éšç€æ¯éš”å‡ ä¸ªæœˆå°±ä¼šå‡ºç°æ–°çš„ã€æ›´å¼ºå¤§çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå¾®è°ƒå˜å¾—éå¸¸æ˜‚è´µã€åŠ³åŠ¨å¯†é›†å‹ä¸”æ˜“å‡ºé”™ã€‚ä½œä¸ºä¸€ç§æ›¿ä»£æ–¹æ¡ˆï¼Œé›¶å‡»æ–‡æœ¬åˆ°SQLï¼Œåˆ©ç”¨LLMsä¸­ä¸æ–­å¢é•¿çš„çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ï¼Œæ— éœ€ç‰¹å®šä»»åŠ¡çš„å¾®è°ƒï¼Œå±•ç°äº†ä¸€ä¸ªæœ‰å‰æ™¯ä¸”æ›´å…·æŒ‘æˆ˜æ€§çš„æ–¹å‘ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†Alpha-SQLè¿™ä¸€æ–°æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰æ¡†æ¶æ¥åŸºäºéƒ¨åˆ†SQLæŸ¥è¯¢çŠ¶æ€è¿­ä»£æ¨æ–­SQLæ„å»ºåŠ¨ä½œã€‚ä¸ºäº†æé«˜æ¡†æ¶çš„æ¨ç†èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼•å…¥äº†LLM-ä½œä¸ºè¡ŒåŠ¨æ¨¡å‹æ¥åœ¨MCTSè¿‡ç¨‹ä¸­åŠ¨æ€ç”ŸæˆSQLæ„å»ºåŠ¨ä½œï¼Œå¼•å¯¼æœç´¢æœç€æ›´æœ‰å‰é€”çš„SQLæŸ¥è¯¢è¿›è¡Œã€‚æ­¤å¤–ï¼ŒAlpha-SQLé‡‡ç”¨è‡ªæˆ‘ç›‘ç£çš„å¥–åŠ±å‡½æ•°æ¥è¯„ä¼°å€™é€‰SQLæŸ¥è¯¢çš„è´¨é‡ï¼Œç¡®ä¿æ›´å‡†ç¡®ã€é«˜æ•ˆçš„æŸ¥è¯¢ç”Ÿæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAlpha-SQLåœ¨BIRDå¼€å‘é›†ä¸Šè¾¾åˆ°äº†69.7%çš„æ‰§è¡Œå‡†ç¡®ç‡ï¼Œä½¿ç”¨çš„æ˜¯æœªè¿›è¡Œå¾®è°ƒçš„å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ã€‚Alpha-SQLåœ¨BIRDå¼€å‘é›†ä¸Šçš„è¡¨ç°ä¼˜äºä¹‹å‰çš„åŸºäºGPT-4oçš„æœ€ä½³é›¶å†²å‡»æ–¹æ³•æé«˜äº†2.5%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17248v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬æè¿°äº†Text-to-SQLçš„é‡è¦æ€§ä»¥åŠå¦‚ä½•åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å®ç°é›¶æ ·æœ¬Text-to-SQLçš„æ–¹æ³•ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•çš„ä¸è¶³ï¼Œæå‡ºäº†ä¸€ç§åä¸ºAlpha-SQLçš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆè’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰å’ŒLLMï¼Œé€šè¿‡è‡ªæˆ‘ç›‘ç£å¥–åŠ±å‡½æ•°è¯„ä¼°å€™é€‰SQLæŸ¥è¯¢è´¨é‡ï¼Œå®ç°äº†æ›´é«˜çš„æ‰§è¡Œå‡†ç¡®ç‡å’Œæ›´é«˜æ•ˆçš„æŸ¥è¯¢ç”Ÿæˆã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAlpha-SQLåœ¨ä¸è¿›è¡Œå¾®è°ƒçš„æƒ…å†µä¸‹ï¼Œåœ¨BIRDå¼€å‘é›†ä¸Šè¾¾åˆ°äº†69.7%çš„æ‰§è¡Œå‡†ç¡®ç‡ï¼Œå¹¶ä¼˜äºä¹‹å‰çš„GPT-4oçš„é›¶æ ·æœ¬æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Text-to-SQLæ˜¯è‡ªç„¶è¯­è¨€ä¸æ•°æ®åº“äº¤äº’çš„é‡è¦æ–¹æ³•ï¼Œå¹¿æ³›åº”ç”¨äºä¸åŒè¡Œä¸šã€‚</li>
<li>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œä»»åŠ¡ç‰¹å®šå¾®è°ƒçš„æˆæœ¬ã€åŠ³åŠ¨å¼ºåº¦å’Œé”™è¯¯ç‡éƒ½åœ¨å¢åŠ ã€‚</li>
<li>Alpha-SQLæ˜¯ä¸€ç§æ–°çš„é›¶æ ·æœ¬Text-to-SQLæ–¹æ³•ï¼Œç»“åˆäº†è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰å’ŒLLMã€‚</li>
<li>Alpha-SQLé€šè¿‡LLMç”ŸæˆSQLæ„å»ºåŠ¨ä½œï¼Œæé«˜æ¨ç†èƒ½åŠ›ã€‚</li>
<li>Alpha-SQLä½¿ç”¨è‡ªæˆ‘ç›‘ç£å¥–åŠ±å‡½æ•°è¯„ä¼°å€™é€‰SQLæŸ¥è¯¢è´¨é‡ï¼Œç¡®ä¿æ›´å‡†ç¡®ã€é«˜æ•ˆçš„æŸ¥è¯¢ç”Ÿæˆã€‚</li>
<li>Alpha-SQLåœ¨ä¸è¿›è¡Œå¾®è°ƒçš„æƒ…å†µä¸‹ï¼Œåœ¨BIRDå¼€å‘é›†ä¸Šå®ç°äº†69.7%çš„æ‰§è¡Œå‡†ç¡®ç‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17248">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d5e3eee7dc536c617f0e9d1b59fe8533.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1ae727e6933c739cea874e5a04534456.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b7e3a9d10147c98fb960a17143267b9e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc820fde87d6280d9dd90e4645065fb1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a48ba8758df3d6a60a8e8b13ea02f576.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Evaluating-the-Effectiveness-of-Large-Language-Models-in-Automated-News-Article-Summarization"><a href="#Evaluating-the-Effectiveness-of-Large-Language-Models-in-Automated-News-Article-Summarization" class="headerlink" title="Evaluating the Effectiveness of Large Language Models in Automated News   Article Summarization"></a>Evaluating the Effectiveness of Large Language Models in Automated News   Article Summarization</h2><p><strong>Authors:Lionel Richy Panlap Houamegni, Fatih Gedikli</strong></p>
<p>The automation of news analysis and summarization presents a promising solution to the challenge of processing and analyzing vast amounts of information prevalent in todayâ€™s information society. Large Language Models (LLMs) have demonstrated the capability to transform vast amounts of textual data into concise and easily comprehensible summaries, offering an effective solution to the problem of information overload and providing users with a quick overview of relevant information. A particularly significant application of this technology lies in supply chain risk analysis. Companies must monitor the news about their suppliers and respond to incidents for several critical reasons, including compliance with laws and regulations, risk management, and maintaining supply chain resilience. This paper develops an automated news summarization system for supply chain risk analysis using LLMs. The proposed solution aggregates news from various sources, summarizes them using LLMs, and presents the condensed information to users in a clear and concise format. This approach enables companies to optimize their information processing and make informed decisions. Our study addresses two main research questions: (1) Are LLMs effective in automating news summarization, particularly in the context of supply chain risk analysis? (2) How effective are various LLMs in terms of readability, duplicate detection, and risk identification in their summarization quality? In this paper, we conducted an offline study using a range of publicly available LLMs at the time and complemented it with a user study focused on the top performing systems of the offline experiments to evaluate their effectiveness further. Our results demonstrate that LLMs, particularly Few-Shot GPT-4o mini, offer significant improvements in summary quality and risk identification. </p>
<blockquote>
<p>æ–°é—»åˆ†æä¸æ‘˜è¦çš„è‡ªåŠ¨åŒ–ä¸ºå½“ä»Šä¿¡æ¯ç¤¾ä¼šæ™®éå­˜åœ¨çš„æµ·é‡ä¿¡æ¯å¤„ç†ä¸åˆ†ææŒ‘æˆ˜æä¾›äº†å‰æ™¯å…‰æ˜çš„è§£å†³æ–¹æ¡ˆã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿå°†å¤§é‡æ–‡æœ¬æ•°æ®è½¬åŒ–ä¸ºç®€æ´ã€æ˜“äºç†è§£çš„æ‘˜è¦ï¼Œä¸ºè§£å†³ä¿¡æ¯è¿‡è½½é—®é¢˜æä¾›äº†æœ‰æ•ˆæ–¹æ³•ï¼Œå¹¶ä¸ºç”¨æˆ·æä¾›äº†ç›¸å…³ä¿¡æ¯çš„å¿«é€Ÿæ¦‚è§ˆã€‚è¿™é¡¹æŠ€æœ¯åœ¨ä¾›åº”é“¾é£é™©åˆ†æä¸­çš„åº”ç”¨å°¤ä¸ºæ˜¾è‘—ã€‚å…¬å¸å¿…é¡»ç›‘æ§æœ‰å…³å…¶ä¾›åº”å•†çš„æ–°é—»å¹¶å¯¹äº‹ä»¶åšå‡ºå“åº”ï¼Œå‡ºäºå¤šä¸ªå…³é”®åŸå› ï¼ŒåŒ…æ‹¬éµå®ˆæ³•å¾‹æ³•è§„ã€é£é™©ç®¡ç†å’Œä¿æŒä¾›åº”é“¾éŸ§æ€§ã€‚æœ¬æ–‡å¼€å‘äº†ä¸€ä¸ªç”¨äºä¾›åº”é“¾é£é™©åˆ†æçš„è‡ªåŠ¨åŒ–æ–°é—»æ‘˜è¦ç³»ç»Ÿï¼Œé‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ã€‚æ‰€æå‡ºçš„è§£å†³æ–¹æ¡ˆä»å„ç§æ¥æºèšåˆæ–°é—»ï¼Œä½¿ç”¨LLMè¿›è¡Œæ‘˜è¦ï¼Œå¹¶ä»¥æ¸…æ™°ç®€æ´çš„æ ¼å¼å‘ç”¨æˆ·å‘ˆç°æµ“ç¼©çš„ä¿¡æ¯ã€‚è¿™ç§æ–¹æ³•ä½¿å…¬å¸èƒ½å¤Ÿä¼˜åŒ–å…¶ä¿¡æ¯å¤„ç†å¹¶åšå‡ºæ˜æ™ºçš„å†³ç­–ã€‚æˆ‘ä»¬çš„ç ”ç©¶è§£å†³äº†ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼šï¼ˆ1ï¼‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªåŠ¨åŒ–æ–°é—»æ‘˜è¦ä¸­æ˜¯å¦æœ‰æ•ˆï¼Œç‰¹åˆ«æ˜¯åœ¨ä¾›åº”é“¾é£é™©åˆ†æçš„èƒŒæ™¯ä¸‹ï¼Ÿï¼ˆ2ï¼‰åœ¨å¯è¯»æ€§ã€é‡å¤æ£€æµ‹å’Œé£é™©è¯†åˆ«æ–¹é¢ï¼Œå„ç§å¤§å‹è¯­è¨€æ¨¡å‹çš„æ‘˜è¦è´¨é‡æ•ˆæœå¦‚ä½•ï¼Ÿåœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†å½“æ—¶å¯ç”¨çš„å„ç§å…¬å¼€å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº†ç¦»çº¿ç ”ç©¶ï¼Œå¹¶é€šè¿‡ä»¥è¡¨ç°æœ€ä½³çš„ç³»ç»Ÿä¸ºä¸­å¿ƒçš„ç”¨æˆ·ç ”ç©¶è¿›ä¸€æ­¥è¯„ä¼°äº†å…¶æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå°¤å…¶æ˜¯Few-Shot GPT-4o miniï¼Œåœ¨æ‘˜è¦è´¨é‡å’Œé£é™©è¯†åˆ«æ–¹é¢æä¾›äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17136v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ–°é—»åˆ†æä¸æ‘˜è¦è‡ªåŠ¨åŒ–æ–¹é¢å±•ç°å‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œä¸ºè§£å†³ä¿¡æ¯ç¤¾ä¼šä¸­çš„æµ·é‡ä¿¡æ¯å¤„ç†ä¸åˆ†ææŒ‘æˆ˜æä¾›äº†æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚LLMsèƒ½å¤Ÿå°†å¤§é‡æ–‡æœ¬æ•°æ®è½¬åŒ–ä¸ºç®€æ´æ˜“æ‡‚çš„æ‘˜è¦ï¼Œæœ‰æ•ˆåº”å¯¹ä¿¡æ¯è¿‡è½½é—®é¢˜ï¼Œä¸ºç”¨æˆ·æä¾›ç›¸å…³ä¿¡æ¯å¿«é€Ÿæ¦‚è§ˆã€‚æœ¬ç ”ç©¶åœ¨ä¾›åº”é“¾é£é™©åˆ†æä¸­åˆ©ç”¨æ­¤æŠ€æœ¯ï¼Œå¼€å‘è‡ªåŠ¨åŒ–æ–°é—»æ‘˜è¦ç³»ç»Ÿã€‚é€šè¿‡æ–°é—»æ¥æºèšåˆã€LLMsæ‘˜è¦å‘ˆç°ï¼Œå¸®åŠ©å…¬å¸ä¼˜åŒ–ä¿¡æ¯å¤„ç†å¹¶åšå‡ºæ˜æ™ºå†³ç­–ã€‚ç ”ç©¶è¯å®LLMsï¼Œå°¤å…¶æ˜¯Few-Shot GPT-4o miniï¼Œåœ¨æ‘˜è¦è´¨é‡å’Œé£é™©è¯†åˆ«æ–¹é¢æœ‰æ˜¾è‘—æé«˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯ä»¥è½¬åŒ–å¤§é‡æ–‡æœ¬æ•°æ®ä¸ºç®€æ´æ‘˜è¦ï¼Œæœ‰æ•ˆåº”å¯¹ä¿¡æ¯è¿‡è½½ã€‚</li>
<li>åœ¨ä¾›åº”é“¾é£é™©åˆ†æä¸­ï¼ŒLLMsçš„åº”ç”¨æœ‰åŠ©äºå…¬å¸ç›‘æ§ä¾›åº”å•†æ–°é—»å¹¶å“åº”å„ç§äº‹ä»¶ã€‚</li>
<li>ç ”ç©¶é€šè¿‡è‡ªåŠ¨åŒ–æ–°é—»æ‘˜è¦ç³»ç»Ÿï¼Œèšåˆæ–°é—»æ¥æºå¹¶ä½¿ç”¨LLMsè¿›è¡Œæ‘˜è¦å‘ˆç°ã€‚</li>
<li>ç ”ç©¶ä¸»è¦è§£å†³ä¸¤ä¸ªé—®é¢˜ï¼šLLMsåœ¨è‡ªåŠ¨åŒ–æ–°é—»æ‘˜è¦ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä»¥åŠä¸åŒLLMsåœ¨å¯è¯»æ€§ã€å»é‡å’Œé£é™©è¯„ä¼°æ–¹é¢çš„æ•ˆæœã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨ç¦»çº¿ç ”ç©¶åŠç”¨æˆ·ç ”ç©¶çš„æ–¹å¼ï¼Œè¯„ä¼°äº†å„ç³»ç»Ÿçš„æœ‰æ•ˆæ€§ã€‚</li>
<li>ç»“æœæ˜¾ç¤ºï¼ŒLLMsæ˜¾è‘—æé«˜æ‘˜è¦è´¨é‡å’Œé£é™©è¯†åˆ«èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17136">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e9ee34c9b172ec431518bd78529aabbd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dc9b160ea2dfc9d9a3d44baa6ac669ff.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Code-Summarization-Beyond-Function-Level"><a href="#Code-Summarization-Beyond-Function-Level" class="headerlink" title="Code Summarization Beyond Function Level"></a>Code Summarization Beyond Function Level</h2><p><strong>Authors:Vladimir Makharev, Vladimir Ivanov</strong></p>
<p>Code summarization is a critical task in natural language processing and software engineering, which aims to generate concise descriptions of source code. Recent advancements have improved the quality of these summaries, enhancing code readability and maintainability. However, the content of a repository or a class has not been considered in function code summarization. This study investigated the effectiveness of code summarization models beyond the function level, exploring the impact of class and repository contexts on the summary quality. The study involved revising benchmarks for evaluating models at class and repository levels, assessing baseline models, and evaluating LLMs with in-context learning to determine the enhancement of summary quality with additional context. The findings revealed that the fine-tuned state-of-the-art CodeT5+ base model excelled in code summarization, while incorporating few-shot learning and retrieved code chunks from RAG significantly enhanced the performance of LLMs in this task. Notably, the Deepseek Coder 1.3B and Starcoder2 15B models demonstrated substantial improvements in metrics such as BLEURT, METEOR, and BLEU-4 at both class and repository levels. Repository-level summarization exhibited promising potential but necessitates significant computational resources and gains from the inclusion of structured context. Lastly, we employed the recent SIDE code summarization metric in our evaluation. This study contributes to refining strategies for prompt engineering, few-shot learning, and RAG, addressing gaps in benchmarks for code summarization at various levels. Finally, we publish all study details, code, datasets, and results of evaluation in the GitHub repository available at <a target="_blank" rel="noopener" href="https://github.com/kilimanj4r0/code-summarization-beyond-function-level">https://github.com/kilimanj4r0/code-summarization-beyond-function-level</a>. </p>
<blockquote>
<p>ä»£ç æ‘˜è¦ä»»åŠ¡æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†å’Œè½¯ä»¶å·¥ç¨‹ä¸­çš„å…³é”®ä»»åŠ¡ï¼Œæ—¨åœ¨ç”Ÿæˆæºä»£ç çš„ç®€æ´æè¿°ã€‚æœ€è¿‘çš„è¿›å±•æé«˜äº†è¿™äº›æ‘˜è¦çš„è´¨é‡ï¼Œå¢å¼ºäº†ä»£ç çš„å¯è¯»æ€§å’Œå¯ç»´æŠ¤æ€§ã€‚ç„¶è€Œï¼Œåœ¨å‡½æ•°ä»£ç æ‘˜è¦ä¸­ï¼Œä»“åº“æˆ–ç±»çš„å†…å®¹å¹¶æœªå¾—åˆ°è€ƒè™‘ã€‚æœ¬ç ”ç©¶æ—¨åœ¨æ¢è®¨å‡½æ•°çº§åˆ«ä»¥å¤–çš„ä»£ç æ‘˜è¦æ¨¡å‹çš„æœ‰æ•ˆæ€§ï¼Œæ¢ç´¢ç±»å’Œä»“åº“ä¸Šä¸‹æ–‡å¯¹æ‘˜è¦è´¨é‡çš„å½±å“ã€‚è¯¥ç ”ç©¶åŒ…æ‹¬ä¿®è®¢ç”¨äºç±»å’Œä»“åº“çº§åˆ«è¯„ä¼°æ¨¡å‹çš„åŸºå‡†æµ‹è¯•ï¼Œè¯„ä¼°åŸºçº¿æ¨¡å‹ï¼Œå¹¶è¯„ä¼°å…·æœ‰ä¸Šä¸‹æ–‡å­¦ä¹ çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä»¥ç¡®å®šé™„åŠ ä¸Šä¸‹æ–‡å¯¹æ‘˜è¦è´¨é‡çš„æå‡ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç»è¿‡å¾®è°ƒçš„æœ€å…ˆè¿›çš„åŸºç¡€æ¨¡å‹CodeT5+åœ¨ä»£ç æ‘˜è¦æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œè€Œèå…¥å°æ ·æœ¬å­¦ä¹ å’Œä»RAGæ£€ç´¢çš„ä»£ç ç‰‡æ®µåˆ™æ˜¾è‘—æé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ­¤ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒDeepseek Coder 1.3Bå’ŒStarcoder2 15Bæ¨¡å‹åœ¨ç±»çº§åˆ«å’Œä»“åº“çº§åˆ«çš„BLEURTã€METEORå’ŒBLEU-4æŒ‡æ ‡ä¸Šå‡å®ç°äº†æ˜¾è‘—æ”¹è¿›ã€‚ä»“åº“çº§åˆ«çš„æ‘˜è¦å±•ç°å‡ºäº†å·¨å¤§çš„æ½œåŠ›ï¼Œä½†éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºï¼Œå¹¶ä»ç»“æ„åŒ–ä¸Šä¸‹æ–‡ä¸­è·ç›Šã€‚æœ€åï¼Œæˆ‘ä»¬åœ¨è¯„ä¼°ä¸­é‡‡ç”¨äº†æœ€æ–°çš„SIDEä»£ç æ‘˜è¦æŒ‡æ ‡ã€‚æœ¬ç ”ç©¶æœ‰åŠ©äºå®Œå–„æç¤ºå·¥ç¨‹ã€å°æ ·æœ¬å­¦ä¹ å’ŒRAGçš„ç­–ç•¥ï¼Œå¡«è¡¥äº†å„çº§ä»£ç æ‘˜è¦çš„åŸºå‡†æµ‹è¯•ç©ºç™½ã€‚æœ€åï¼Œæˆ‘ä»¬åœ¨GitHubä»“åº“ä¸­å…¬å¼€äº†æ‰€æœ‰ç ”ç©¶ç»†èŠ‚ã€ä»£ç ã€æ•°æ®é›†å’Œè¯„ä¼°ç»“æœï¼š<a target="_blank" rel="noopener" href="https://github.com/kilimanj4r0/code-summarization-beyond-function-level">https://github.com/kilimanj4r0/code-summarization-beyond-function-level</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.16704v1">PDF</a> Accepted to LLM4Code @ ICSEâ€™25; 8 pages, 3 figures, 4 tables</p>
<p><strong>Summary</strong><br>ä»£ç æ‘˜è¦å¤„ç†æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†å’Œè½¯ä»¶å·¥ç¨‹ä¸­çš„å…³é”®ä»»åŠ¡ï¼Œæ—¨åœ¨ç”Ÿæˆå¯¹æºä»£ç çš„ç®€æ´æè¿°ã€‚è¿‘æœŸæŠ€æœ¯çš„å‘å±•æé«˜äº†æ‘˜è¦çš„è´¨é‡ï¼Œå¢å¼ºäº†ä»£ç çš„å¯è¯»æ€§å’Œå¯ç»´æŠ¤æ€§ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†è¶…è¶Šå‡½æ•°çº§åˆ«çš„ä»£ç æ‘˜è¦æ¨¡å‹çš„æœ‰æ•ˆæ€§ï¼Œæ¢ç´¢äº†ç±»ä¸Šä¸‹æ–‡å’Œå­˜å‚¨åº“ä¸Šä¸‹æ–‡å¯¹æ‘˜è¦è´¨é‡çš„å½±å“ã€‚ç ”ç©¶å‘ç°ï¼Œç»è¿‡å¾®è°ƒçš„æœ€å…ˆè¿›çš„CodeT5+åŸºç¡€æ¨¡å‹åœ¨ä»£ç æ‘˜è¦ä¸­è¡¨ç°å‡ºè‰²ï¼Œè€Œèå…¥å°‘é‡å­¦ä¹ å’Œä»RAGæ£€ç´¢çš„ä»£ç ç‰‡æ®µåˆ™è¿›ä¸€æ­¥æå‡äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ­¤ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚Deepseek Coder 1.3Bå’ŒStarcoder2 15Bæ¨¡å‹åœ¨ç±»çº§åˆ«å’Œå­˜å‚¨åº“çº§åˆ«çš„åº¦é‡æ ‡å‡†ä¸Šå‡æœ‰æ˜¾è‘—æ”¹è¿›ã€‚æœ¬ç ”ç©¶å¯¹æç¤ºå·¥ç¨‹ã€å°‘é‡å­¦ä¹ å’ŒRAGçš„ç­–ç•¥è¿›è¡Œäº†ä¼˜åŒ–ï¼Œå¡«è¡¥äº†å„çº§ä»£ç æ‘˜è¦çš„åŸºå‡†æµ‹è¯•ç©ºç™½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»£ç æ‘˜è¦å¤„ç†æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†å’Œè½¯ä»¶å·¥ç¨‹ä¸­çš„å…³é”®ä»»åŠ¡ã€‚</li>
<li>ç ”ç©¶æ¢è®¨äº†è¶…è¶Šå‡½æ•°çº§åˆ«çš„ä»£ç æ‘˜è¦æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>ç±»ä¸Šä¸‹æ–‡å’Œå­˜å‚¨åº“ä¸Šä¸‹æ–‡å¯¹æ‘˜è¦è´¨é‡æœ‰å½±å“ã€‚</li>
<li>CodeT5+åŸºç¡€æ¨¡å‹åœ¨ä»£ç æ‘˜è¦ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>èå…¥å°‘é‡å­¦ä¹ å’ŒRAGæ£€ç´¢çš„ä»£ç ç‰‡æ®µæå‡äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„è¡¨ç°ã€‚</li>
<li>Deepseek Coderå’ŒStarcoderæ¨¡å‹åœ¨ç±»çº§åˆ«å’Œå­˜å‚¨åº“çº§åˆ«çš„åº¦é‡æ ‡å‡†ä¸Šæœ‰æ˜¾è‘—æ”¹è¿›ã€‚</li>
<li>ç ”ç©¶æˆæœåŒ…æ‹¬ç­–ç•¥ä¼˜åŒ–ã€åŸºå‡†æµ‹è¯•å¡«è¡¥ä»¥åŠGitHubå­˜å‚¨åº“ä¸­çš„ç ”ç©¶ç»†èŠ‚ã€ä»£ç ã€æ•°æ®é›†å’Œè¯„ä¼°ç»“æœå‘å¸ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.16704">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fe8209e4c213d8be158821cbb16e6dc5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a6aad10764a437ede4e5252c27c043f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8ee8080a26fd061677dffed6583c1d1c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d6df761ad4ad7675f77e26d7e08c518.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1bf4d0764616d7cb4d3764055228280b.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="MetaSym-A-Symplectic-Meta-learning-Framework-for-Physical-Intelligence"><a href="#MetaSym-A-Symplectic-Meta-learning-Framework-for-Physical-Intelligence" class="headerlink" title="MetaSym: A Symplectic Meta-learning Framework for Physical Intelligence"></a>MetaSym: A Symplectic Meta-learning Framework for Physical Intelligence</h2><p><strong>Authors:Pranav Vaidhyanathan, Aristotelis Papatheodorou, Mark T. Mitchison, Natalia Ares, Ioannis Havoutis</strong></p>
<p>Scalable and generalizable physics-aware deep learning has long been considered a significant challenge with various applications across diverse domains ranging from robotics to molecular dynamics. Central to almost all physical systems are symplectic forms, the geometric backbone that underpins fundamental invariants like energy and momentum. In this work, we introduce a novel deep learning architecture, MetaSym. In particular, MetaSym combines a strong symplectic inductive bias obtained from a symplectic encoder and an autoregressive decoder with meta-attention. This principled design ensures that core physical invariants remain intact while allowing flexible, data-efficient adaptation to system heterogeneities. We benchmark MetaSym on highly varied datasets such as a high-dimensional spring mesh system (Otness et al., 2021), an open quantum system with dissipation and measurement backaction, and robotics-inspired quadrotor dynamics. Our results demonstrate superior performance in modeling dynamics under few-shot adaptation, outperforming state-of-the-art baselines with far larger models. </p>
<blockquote>
<p>å¯æ‰©å±•ä¸”å¯æ³›åŒ–çš„ç‰©ç†æ„ŸçŸ¥æ·±åº¦å­¦ä¹ é•¿æœŸä»¥æ¥è¢«è§†ä¸ºä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå…¶åœ¨ä»æœºå™¨äººå­¦åˆ°åˆ†å­åŠ¨åŠ›å­¦ç­‰å¤šä¸ªé¢†åŸŸéƒ½æœ‰å¹¿æ³›çš„åº”ç”¨ã€‚å‡ ä¹æ‰€æœ‰ç‰©ç†ç³»ç»Ÿçš„æ ¸å¿ƒéƒ½æ˜¯è¾›å½¢å¼ï¼Œå®ƒæ˜¯æ”¯æ’‘èƒ½é‡å’ŒåŠ¨é‡ç­‰åŸºæœ¬ä¸å˜é‡çš„å‡ ä½•ä¸»å¹²ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°å‹æ·±åº¦å­¦ä¹ æ¶æ„MetaSymã€‚å…·ä½“æ¥è¯´ï¼ŒMetaSymç»“åˆäº†ç”±è¾›ç¼–ç å™¨è·å¾—çš„å¼ºå¤§è¾›å½’çº³åè§å’Œå¸¦æœ‰å…ƒæ³¨æ„çš„è‡ªå›å½’è§£ç å™¨ã€‚è¿™ç§æœ‰åŸåˆ™çš„è®¾è®¡ç¡®ä¿æ ¸å¿ƒç‰©ç†ä¸å˜é‡ä¿æŒå®Œæ•´ï¼ŒåŒæ—¶å…è®¸çµæ´»ã€æ•°æ®é«˜æ•ˆåœ°é€‚åº”ç³»ç»Ÿå¼‚è´¨æ€§ã€‚æˆ‘ä»¬åœ¨é«˜åº¦ä¸åŒçš„æ•°æ®é›†ä¸Šå¯¹MetaSymè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œä¾‹å¦‚é«˜ç»´å¼¹ç°§ç½‘æ ¼ç³»ç»Ÿï¼ˆOtnessç­‰äººï¼Œ2021å¹´ï¼‰ã€å…·æœ‰è€—æ•£å’Œæµ‹é‡åä½œç”¨çš„å¼€æ”¾é‡å­ç³»ç»Ÿï¼Œä»¥åŠå—æœºå™¨äººå¯å‘çš„å››æ—‹ç¿¼é£è¡Œå™¨åŠ¨åŠ›å­¦ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œåœ¨å°‘æ ·æœ¬é€‚åº”çš„æƒ…å†µä¸‹ï¼ŒMetaSymåœ¨å»ºæ¨¡åŠ¨åŠ›å­¦æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œè¶…è¶Šäº†ä½¿ç”¨æ›´å¤§æ¨¡å‹çš„æœ€æ–°åŸºçº¿æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.16667v1">PDF</a> 8+10 pages, 5 figures, 4 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„æ·±åº¦å­¦ä¹ æ¶æ„MetaSymï¼Œè¯¥æ¶æ„ç»“åˆäº†ä»è¾›ç¼–ç å™¨å’Œè‡ªå›å½’è§£ç å™¨å¾—åˆ°çš„å¼ºå¤§è¾›å½’çº³åè§ä»¥åŠå…ƒæ³¨æ„æœºåˆ¶ã€‚è¯¥è®¾è®¡èƒ½å¤Ÿåœ¨ä¿æŒæ ¸å¿ƒç‰©ç†ä¸å˜é‡çš„åŒæ—¶ï¼Œçµæ´»é€‚åº”ç³»ç»Ÿå¼‚è´¨æ€§å¹¶é«˜æ•ˆåˆ©ç”¨æ•°æ®ã€‚MetaSymåœ¨å¤šç§æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜è¶Šï¼Œå¦‚é«˜ç»´å¼¹ç°§ç½‘æ ¼ç³»ç»Ÿã€å…·æœ‰è€—æ•£å’Œæµ‹é‡åä½œç”¨çš„å¼€æ”¾é‡å­ç³»ç»Ÿä»¥åŠå—æœºå™¨äººå¯å‘çš„å››æ—‹ç¿¼é£è¡Œå™¨åŠ¨åŠ›å­¦ã€‚å…¶åœ¨å°‘æ ·æœ¬é€‚åº”å»ºæ¨¡åŠ¨åŠ›å­¦æ–¹é¢çš„æ€§èƒ½ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„åŸºçº¿æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MetaSymæ˜¯ä¸€ç§æ–°å‹çš„æ·±åº¦å­¦ä¹ æ¶æ„ï¼Œç»“åˆäº†è¾›ç¼–ç å™¨å’Œè‡ªå›å½’è§£ç å™¨ä»¥åŠå…ƒæ³¨æ„æœºåˆ¶ã€‚</li>
<li>è¯¥æ¶æ„æ—¨åœ¨ç¡®ä¿ç‰©ç†ç³»ç»Ÿçš„æ ¸å¿ƒä¸å˜æ€§è´¨ï¼ˆå¦‚èƒ½é‡å’ŒåŠ¨é‡ï¼‰åœ¨æ¨¡å‹å­¦ä¹ ä¸­å¾—åˆ°ä¿æŒã€‚</li>
<li>MetaSymèƒ½å¤Ÿåœ¨ä¿æŒç‰©ç†ä¸å˜æ€§çš„åŒæ—¶ï¼Œçµæ´»é€‚åº”ç³»ç»Ÿå¼‚è´¨æ€§å¹¶é«˜æ•ˆåˆ©ç”¨æ•°æ®ã€‚</li>
<li>åœ¨å¤šç§æ•°æ®é›†ä¸Šè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬é«˜ç»´å¼¹ç°§ç½‘æ ¼ç³»ç»Ÿã€å¼€æ”¾é‡å­ç³»ç»Ÿå’Œå››æ—‹ç¿¼é£è¡Œå™¨åŠ¨åŠ›å­¦ã€‚</li>
<li>MetaSymåœ¨å°‘æ ·æœ¬é€‚åº”å»ºæ¨¡åŠ¨åŠ›å­¦æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>è¯¥æ¶æ„ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„åŸºçº¿æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.16667">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6434da3050204ac602c25f3da137f127.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a7d970c928a31437ba0255450830406b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-629ce2ed937bcd7d582608496327e73c.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Few-shot-Continual-Relation-Extraction-via-Open-Information-Extraction"><a href="#Few-shot-Continual-Relation-Extraction-via-Open-Information-Extraction" class="headerlink" title="Few-shot Continual Relation Extraction via Open Information Extraction"></a>Few-shot Continual Relation Extraction via Open Information Extraction</h2><p><strong>Authors:Thiem Nguyen, Anh Nguyen, Quyen Tran, Tu Vu, Diep Nguyen, Linh Ngo, Thien Nguyen</strong></p>
<p>Typically, Few-shot Continual Relation Extraction (FCRE) models must balance retaining prior knowledge while adapting to new tasks with extremely limited data. However, real-world scenarios may also involve unseen or undetermined relations that existing methods still struggle to handle. To address these challenges, we propose a novel approach that leverages the Open Information Extraction concept of Knowledge Graph Construction (KGC). Our method not only exposes models to all possible pairs of relations, including determined and undetermined labels not available in the training set, but also enriches model knowledge with diverse relation descriptions, thereby enhancing knowledge retention and adaptability in this challenging scenario. In the perspective of KGC, this is the first work explored in the setting of Continual Learning, allowing efficient expansion of the graph as the data evolves. Experimental results demonstrate our superior performance compared to other state-of-the-art FCRE baselines, as well as the efficiency in handling dynamic graph construction in this setting. </p>
<blockquote>
<p>é€šå¸¸ï¼ŒFew-shot Continual Relation Extractionï¼ˆFCREï¼‰æ¨¡å‹éœ€è¦åœ¨ä¿ç•™å…ˆå‰çŸ¥è¯†çš„åŒæ—¶ï¼Œé€‚åº”æ–°ä»»åŠ¡ä¸”æ•°æ®é‡æåº¦æœ‰é™ã€‚ç„¶è€Œï¼Œç°å®ä¸–ç•Œåœºæ™¯è¿˜å¯èƒ½æ¶‰åŠæœªè§æˆ–æœªç¡®å®šçš„å…³ç³»ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†è¿™äº›åœºæ™¯æ—¶ä»é¢ä¸´å›°éš¾ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨çŸ¥è¯†å›¾è°±æ„å»ºï¼ˆKGCï¼‰ä¸­çš„å¼€æ”¾ä¿¡æ¯æå–æ¦‚å¿µçš„æ–°æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…ä½¿æ¨¡å‹æ¥è§¦åˆ°æ‰€æœ‰å¯èƒ½çš„å…³ç³»å¯¹ï¼ŒåŒ…æ‹¬è®­ç»ƒé›†ä¸­ä¸å­˜åœ¨çš„ç¡®å®šå’Œæœªç¡®å®šçš„æ ‡ç­¾ï¼Œè€Œä¸”è¿˜é€šè¿‡å¤šæ ·çš„å…³ç³»æè¿°ä¸°å¯Œæ¨¡å‹çŸ¥è¯†ï¼Œä»è€Œåœ¨è¿™ç§å…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ä¸­æé«˜çŸ¥è¯†ä¿ç•™å’Œé€‚åº”æ€§ã€‚ä»KGCçš„è§’åº¦æ¥çœ‹ï¼Œè¿™æ˜¯è¿ç»­å­¦ä¹ ç¯å¢ƒä¸­é¦–æ¬¡æ¢ç´¢çš„å·¥ä½œï¼Œå…è®¸éšç€æ•°æ®çš„æ¼”å˜æœ‰æ•ˆåœ°æ‰©å±•å›¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸å…¶ä»–å…ˆè¿›çš„FCREåŸºçº¿ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ€§èƒ½ä¼˜è¶Šï¼Œå¹¶ä¸”åœ¨æ­¤ç¯å¢ƒä¸­å¤„ç†åŠ¨æ€å›¾æ„å»ºçš„æ•ˆç‡å¾ˆé«˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.16648v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹Few-shot Continual Relation Extractionï¼ˆFCREï¼‰æ¨¡å‹çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åˆ©ç”¨çŸ¥è¯†å›¾è°±æ„å»ºï¼ˆKGCï¼‰çš„å¼€æ”¾ä¿¡æ¯æå–æ¦‚å¿µçš„æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•ä¸ä»…ä½¿æ¨¡å‹æ¥è§¦åˆ°æ‰€æœ‰å¯èƒ½çš„å…³ç³»å¯¹ï¼ŒåŒ…æ‹¬è®­ç»ƒé›†ä¸­ä¸å¯ç”¨çš„ç¡®å®šå’Œæœªç¡®å®šæ ‡ç­¾ï¼Œè¿˜é€šè¿‡å¤šæ ·çš„å…³ç³»æè¿°ä¸°å¯Œæ¨¡å‹çŸ¥è¯†ï¼Œä»è€Œå¢å¼ºçŸ¥è¯†ä¿ç•™å’Œé€‚åº”æ€§ã€‚è¿™æ˜¯çŸ¥è¯†å›¾è°±æ„å»ºè§†è§’åœ¨æŒç»­å­¦ä¹ é¢†åŸŸä¸­çš„é¦–æ¬¡æ¢ç´¢ï¼Œèƒ½å¤Ÿå®ç°éšç€æ•°æ®æ¼”å˜çš„é«˜æ•ˆå›¾æ‰©å±•ã€‚å®éªŒç»“æœè¯æ˜äº†è¯¥æ–¹æ³•ç›¸è¾ƒäºå…¶ä»–å…ˆè¿›FCREåŸºå‡†çº¿çš„ä¼˜è¶Šæ€§ï¼Œä»¥åŠåœ¨å¤„ç†åŠ¨æ€å›¾æ„å»ºæ–¹é¢çš„æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FCREæ¨¡å‹éœ€è¦åœ¨ä¿ç•™å…ˆéªŒçŸ¥è¯†çš„åŒæ—¶é€‚åº”æ–°ä»»åŠ¡ï¼Œå°¤å…¶åœ¨æ•°æ®æåº¦æœ‰é™çš„æƒ…å†µä¸‹ã€‚</li>
<li>ç°æœ‰æ–¹æ³•éš¾ä»¥å¤„ç†æœªè§æˆ–æœªç¡®å®šçš„å…³ç³»ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œåˆ©ç”¨KGCçš„å¼€æ”¾ä¿¡æ¯æå–æ¦‚å¿µæ¥åº”å¯¹è¿™äº›æŒ‘æˆ˜ã€‚</li>
<li>æ¨¡å‹å¯ä»¥æ¥è§¦åˆ°æ‰€æœ‰å¯èƒ½çš„å…³ç³»å¯¹ï¼ŒåŒ…æ‹¬è®­ç»ƒé›†ä¸­ä¸å¯ç”¨çš„ç¡®å®šå’Œæœªç¡®å®šæ ‡ç­¾ã€‚</li>
<li>æ–¹æ³•é€šè¿‡å¤šæ ·çš„å…³ç³»æè¿°ä¸°å¯Œæ¨¡å‹çŸ¥è¯†ï¼Œå¢å¼ºçŸ¥è¯†ä¿ç•™å’Œé€‚åº”æ€§ã€‚</li>
<li>è¿™æ˜¯çŸ¥è¯†å›¾è°±æ„å»ºè§†è§’åœ¨æŒç»­å­¦ä¹ é¢†åŸŸçš„é¦–æ¬¡æ¢ç´¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.16648">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b41817cb0695ddccedf7fca81d259266.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a8a3548505de091ad33bf2e57af5976.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="TimePFN-Effective-Multivariate-Time-Series-Forecasting-with-Synthetic-Data"><a href="#TimePFN-Effective-Multivariate-Time-Series-Forecasting-with-Synthetic-Data" class="headerlink" title="TimePFN: Effective Multivariate Time Series Forecasting with Synthetic   Data"></a>TimePFN: Effective Multivariate Time Series Forecasting with Synthetic   Data</h2><p><strong>Authors:Ege Onur Taga, M. Emrullah Ildiz, Samet Oymak</strong></p>
<p>The diversity of time series applications and scarcity of domain-specific data highlight the need for time-series models with strong few-shot learning capabilities. In this work, we propose a novel training scheme and a transformer-based architecture, collectively referred to as TimePFN, for multivariate time-series (MTS) forecasting. TimePFN is based on the concept of Prior-data Fitted Networks (PFN), which aims to approximate Bayesian inference. Our approach consists of (1) generating synthetic MTS data through diverse Gaussian process kernels and the linear coregionalization method, and (2) a novel MTS architecture capable of utilizing both temporal and cross-channel dependencies across all input patches. We evaluate TimePFN on several benchmark datasets and demonstrate that it outperforms the existing state-of-the-art models for MTS forecasting in both zero-shot and few-shot settings. Notably, fine-tuning TimePFN with as few as 500 data points nearly matches full dataset training error, and even 50 data points yield competitive results. We also find that TimePFN exhibits strong univariate forecasting performance, attesting to its generalization ability. Overall, this work unlocks the power of synthetic data priors for MTS forecasting and facilitates strong zero- and few-shot forecasting performance. </p>
<blockquote>
<p>æ—¶é—´åºåˆ—åº”ç”¨çš„å¤šæ ·æ€§å’Œç‰¹å®šé¢†åŸŸæ•°æ®çš„ç¨€ç¼ºæ€§ï¼Œå‡¸æ˜¾äº†éœ€è¦å…·æœ‰å¼ºå¤§å°‘é‡å­¦ä¹ èƒ½åŠ›çš„æ—¶é—´åºåˆ—æ¨¡å‹ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„è®­ç»ƒæ–¹æ¡ˆå’ŒåŸºäºå˜å‹å™¨çš„æ¶æ„ï¼Œä¸¤è€…ç»Ÿç§°ä¸ºTimePFNï¼Œç”¨äºå¤šå…ƒæ—¶é—´åºåˆ—ï¼ˆMTSï¼‰é¢„æµ‹ã€‚TimePFNåŸºäºå…ˆéªŒæ•°æ®æ‹Ÿåˆç½‘ç»œï¼ˆPFNï¼‰çš„æ¦‚å¿µï¼Œæ—¨åœ¨è¿‘ä¼¼è´å¶æ–¯æ¨æ–­ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬ï¼ˆ1ï¼‰é€šè¿‡å¤šæ ·çš„é«˜æ–¯è¿‡ç¨‹æ ¸å’Œçº¿æ€§å…±åŒºåŸŸåŒ–æ–¹æ³•ç”ŸæˆåˆæˆMTSæ•°æ®ï¼Œï¼ˆ2ï¼‰ä¸€ç§æ–°å‹çš„MTSæ¶æ„ï¼Œèƒ½å¤Ÿåˆ©ç”¨æ‰€æœ‰è¾“å…¥è¡¥ä¸ä¸­çš„æ—¶é—´ä¾èµ–æ€§å’Œè·¨é€šé“ä¾èµ–æ€§ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¯„ä¼°äº†TimePFNï¼Œç»“æœè¡¨æ˜ï¼Œå®ƒåœ¨é›¶æ ·æœ¬å’Œå°‘é‡æ ·æœ¬çš„æƒ…å†µä¸‹ï¼Œéƒ½ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›çš„MTSé¢„æµ‹æ¨¡å‹ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä½¿ç”¨ä»…500ä¸ªæ•°æ®ç‚¹å¯¹TimePFNè¿›è¡Œå¾®è°ƒï¼Œå…¶è¯¯å·®å‡ ä¹åŒ¹é…å…¨æ•°æ®é›†è®­ç»ƒè¯¯å·®ï¼Œç”šè‡³ä½¿ç”¨50ä¸ªæ•°æ®ç‚¹ä¹Ÿèƒ½è·å¾—å…·æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚æˆ‘ä»¬è¿˜å‘ç°TimePFNåœ¨å•å˜é‡é¢„æµ‹æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶æ³›åŒ–èƒ½åŠ›ã€‚æ€»çš„æ¥è¯´ï¼Œè¿™é¡¹å·¥ä½œé‡Šæ”¾äº†åˆæˆæ•°æ®å…ˆéªŒåœ¨MTSé¢„æµ‹ä¸­çš„æ½œåŠ›ï¼Œå¹¶å®ç°äº†å¼ºå¤§çš„é›¶æ ·æœ¬å’Œå°‘é‡æ ·æœ¬é¢„æµ‹æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.16294v1">PDF</a> To appear in AAAI-2025 as a conference paper</p>
<p><strong>Summary</strong></p>
<p>æ—¶é—´åºåˆ—åº”ç”¨çš„å¤šæ ·æ€§å’Œç‰¹å®šé¢†åŸŸæ•°æ®çš„ç¨€ç¼ºæ€§çªæ˜¾å‡ºéœ€è¦å…·æœ‰å¼ºå¤§å°‘æ ·æœ¬å­¦ä¹ èƒ½åŠ›çš„æ—¶é—´åºåˆ—æ¨¡å‹ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå…ˆéªŒæ•°æ®æ‹Ÿåˆç½‘ç»œï¼ˆPFNï¼‰æ¦‚å¿µçš„æ–°å‹è®­ç»ƒæ–¹æ¡ˆå’ŒåŸºäºå˜å‹å™¨çš„æ¶æ„ï¼Œç»Ÿç§°ä¸ºTimePFNï¼Œç”¨äºå¤šå…ƒæ—¶é—´åºåˆ—ï¼ˆMTSï¼‰é¢„æµ‹ã€‚TimePFNé€šè¿‡ç”ŸæˆåˆæˆMTSæ•°æ®å’Œåˆ©ç”¨å…ˆè¿›æ¶æ„å®ç°å¼ºå¤§çš„é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬é¢„æµ‹æ€§èƒ½ï¼Œå±•ç¤ºäº†å‡ºè‰²çš„é¢„æµ‹æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ—¶é—´åºåˆ—åº”ç”¨çš„å¤šæ ·æ€§å’Œç‰¹å®šé¢†åŸŸæ•°æ®çš„ç¨€ç¼ºæ€§å‡¸æ˜¾å‡ºå¯¹å…·æœ‰å¼ºå¤§å°‘æ ·æœ¬å­¦ä¹ èƒ½åŠ›æ¨¡å‹çš„è¿«åˆ‡éœ€æ±‚ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†TimePFNæ¨¡å‹ï¼Œç»“åˆäº†æ–°å‹è®­ç»ƒæ–¹æ¡ˆå’ŒåŸºäºå˜å‹å™¨çš„æ¶æ„ã€‚</li>
<li>TimePFNåŸºäºå…ˆéªŒæ•°æ®æ‹Ÿåˆç½‘ç»œï¼ˆPFNï¼‰æ¦‚å¿µï¼Œæ—¨åœ¨è¿‘ä¼¼è´å¶æ–¯æ¨ç†ã€‚</li>
<li>é€šè¿‡ç”ŸæˆåˆæˆMTSæ•°æ®å’Œåˆ©ç”¨å…ˆè¿›æ¶æ„ï¼ŒTimePFNå®ç°äº†å¼ºå¤§çš„é¢„æµ‹æ€§èƒ½ã€‚</li>
<li>TimePFNåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›çš„MTSé¢„æµ‹æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬è®¾ç½®ä¸‹ã€‚</li>
<li>TimePFNä»…ä½¿ç”¨å°‘é‡æ•°æ®ç‚¹ï¼ˆå¦‚500ä¸ªï¼‰è¿›è¡Œå¾®è°ƒå³å¯æ¥è¿‘å…¨æ•°æ®é›†è®­ç»ƒçš„é”™è¯¯ç‡ï¼Œç”šè‡³ä½¿ç”¨50ä¸ªæ•°æ®ç‚¹ä¹Ÿèƒ½å–å¾—æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚</li>
<li>TimePFNå±•ç°å‡ºå¼ºå¤§çš„å•å˜é‡é¢„æµ‹æ€§èƒ½ï¼Œè¯æ˜äº†å…¶æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.16294">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7babbe90763294fc46d0ab55e7ca01e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1ec3317004cb7755fe0b3003160b7051.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b249619f5b8eb652cd72d9f0758630c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a03814cc966e17c8a342fd4611efa4cd.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Enhancing-LLMs-for-Identifying-and-Prioritizing-Important-Medical-Jargons-from-Electronic-Health-Record-Notes-Utilizing-Data-Augmentation"><a href="#Enhancing-LLMs-for-Identifying-and-Prioritizing-Important-Medical-Jargons-from-Electronic-Health-Record-Notes-Utilizing-Data-Augmentation" class="headerlink" title="Enhancing LLMs for Identifying and Prioritizing Important Medical   Jargons from Electronic Health Record Notes Utilizing Data Augmentation"></a>Enhancing LLMs for Identifying and Prioritizing Important Medical   Jargons from Electronic Health Record Notes Utilizing Data Augmentation</h2><p><strong>Authors:Won Seok Jang, Sharmin Sultana, Zonghai Yao, Hieu Tran, Zhichao Yang, Sunjae Kwon, Hong Yu</strong></p>
<p>Objective: OpenNotes enables patients to access EHR notes, but medical jargon can hinder comprehension. To improve understanding, we evaluated closed- and open-source LLMs for extracting and prioritizing key medical terms using prompting, fine-tuning, and data augmentation.   Materials and Methods: We assessed LLMs on 106 expert-annotated EHR notes, experimenting with (i) general vs. structured prompts, (ii) zero-shot vs. few-shot prompting, (iii) fine-tuning, and (iv) data augmentation. To enhance open-source models in low-resource settings, we used ChatGPT for data augmentation and applied ranking techniques. We incrementally increased the augmented dataset size (10 to 10,000) and conducted 5-fold cross-validation, reporting F1 score and Mean Reciprocal Rank (MRR).   Results and Discussion: Fine-tuning and data augmentation improved performance over other strategies. GPT-4 Turbo achieved the highest F1 (0.433), while Mistral7B with data augmentation had the highest MRR (0.746). Open-source models, when fine-tuned or augmented, outperformed closed-source models. Notably, the best F1 and MRR scores did not always align. Few-shot prompting outperformed zero-shot in vanilla models, and structured prompts yielded different preferences across models. Fine-tuning improved zero-shot performance but sometimes degraded few-shot performance. Data augmentation performed comparably or better than other methods.   Conclusion: Our evaluation highlights the effectiveness of prompting, fine-tuning, and data augmentation in improving model performance for medical jargon extraction in low-resource scenarios. </p>
<blockquote>
<p>ç›®æ ‡ï¼šOpenNotesè®©æ‚£è€…èƒ½å¤Ÿè®¿é—®ç”µå­å¥åº·è®°å½•ï¼ˆEHRï¼‰ç¬”è®°ï¼Œä½†åŒ»ç–—æœ¯è¯­å¯èƒ½ä¼šå¦¨ç¢ç†è§£ã€‚ä¸ºäº†æ”¹å–„æ‚£è€…çš„ç†è§£ï¼Œæˆ‘ä»¬è¯„ä¼°äº†ç”¨äºæå–å’Œä¼˜å…ˆæ’åºå…³é”®åŒ»å­¦æœ¯è¯­çš„é—­æºå’Œå¼€æºçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œé‡‡ç”¨æç¤ºã€å¾®è°ƒå’Œæ•°æ®å¢å¼ºç­‰æ–¹æ³•ã€‚ææ–™å’Œæ–¹æ³•ï¼šæˆ‘ä»¬åœ¨106ä»½ä¸“å®¶æ³¨é‡Šçš„EHRç¬”è®°ä¸Šè¯„ä¼°äº†LLMsï¼Œå°è¯•äº†ï¼ˆiï¼‰é€šç”¨æç¤ºä¸ç»“æ„åŒ–æç¤ºï¼Œï¼ˆiiï¼‰é›¶æ ·æœ¬æç¤ºä¸å°‘æ ·æœ¬æç¤ºï¼Œï¼ˆiiiï¼‰å¾®è°ƒï¼Œä»¥åŠï¼ˆivï¼‰æ•°æ®å¢å¼ºã€‚ä¸ºäº†åœ¨ä½èµ„æºç¯å¢ƒä¸­å¢å¼ºå¼€æºæ¨¡å‹çš„æ•ˆæœï¼Œæˆ‘ä»¬åˆ©ç”¨ChatGPTè¿›è¡Œæ•°æ®å¢å¼ºï¼Œå¹¶åº”ç”¨æ’åºæŠ€æœ¯ã€‚æˆ‘ä»¬é€æ­¥å¢åŠ äº†å¢å¼ºæ•°æ®é›†çš„å¤§å°ï¼ˆä»10åˆ°10,000ï¼‰ï¼Œè¿›è¡Œäº†5å€äº¤å‰éªŒè¯ï¼Œå¹¶æŠ¥å‘Šäº†F1åˆ†æ•°å’Œå¹³å‡å€’æ•°æ’åï¼ˆMRRï¼‰ã€‚ç»“æœå’Œè®¨è®ºï¼šç›¸è¾ƒäºå…¶ä»–ç­–ç•¥ï¼Œå¾®è°ƒå’Œæ•°æ®å¢å¼ºæé«˜äº†æ€§èƒ½ã€‚GPT-4 Turboè·å¾—äº†æœ€é«˜çš„F1åˆ†æ•°ï¼ˆ0.433ï¼‰ï¼Œè€ŒMistral7Bé€šè¿‡æ•°æ®å¢å¼ºè·å¾—äº†æœ€é«˜çš„MRRï¼ˆ0.746ï¼‰ã€‚å½“å¼€æºæ¨¡å‹ç»è¿‡å¾®è°ƒæˆ–å¢å¼ºæ—¶ï¼Œå…¶æ€§èƒ½è¶…è¿‡äº†é—­æºæ¨¡å‹ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæœ€ä½³çš„F1åˆ†æ•°å’ŒMRRå¹¶ä¸æ€»æ˜¯å¯¹é½ã€‚å°‘æ ·æœ¬æç¤ºåœ¨åŸå§‹æ¨¡å‹ä¸­è¡¨ç°ä¼˜äºé›¶æ ·æœ¬æç¤ºï¼Œç»“æ„åŒ–æç¤ºåœ¨ä¸åŒæ¨¡å‹ä¸­äº§ç”Ÿä¸åŒçš„åå¥½ã€‚å¾®è°ƒæé«˜äº†é›¶æ ·æœ¬æ€§èƒ½ï¼Œä½†æœ‰æ—¶ä¼šé™ä½å°‘æ ·æœ¬æ€§èƒ½ã€‚æ•°æ®å¢å¼ºçš„è¡¨ç°ä¸å…¶ä»–æ–¹æ³•ç›¸å½“æˆ–æ›´å¥½ã€‚ç»“è®ºï¼šæˆ‘ä»¬çš„è¯„ä¼°å¼ºè°ƒäº†æç¤ºã€å¾®è°ƒå’Œæ•°æ®å¢å¼ºåœ¨æ”¹è¿›ä½èµ„æºåœºæ™¯ä¸­åŒ»å­¦æœ¯è¯­æå–æ¨¡å‹æ€§èƒ½çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.16022v1">PDF</a> 21pages, 5 figures, 4 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†OpenNotesç³»ç»Ÿä¸‹æ‚£è€…è·å–ç”µå­å¥åº·è®°å½•ï¼ˆEHRï¼‰ç¬”è®°æ—¶é‡åˆ°çš„åŒ»å­¦æœ¯è¯­ç†è§£éš¾é¢˜ã€‚ç ”ç©¶å›¢é˜Ÿè¯„ä¼°äº†é—­æºå’Œå¼€æºçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æå–å’Œä¼˜å…ˆæ’åºå…³é”®åŒ»å­¦æœ¯è¯­æ–¹é¢çš„è¡¨ç°ï¼Œå…·ä½“é‡‡ç”¨äº†æç¤ºã€å¾®è°ƒã€æ•°æ®å¢å¼ºç­‰æ–¹æ³•ã€‚ç»“æœæ˜¾ç¤ºï¼Œå¾®è°ƒå’Œæ•°æ®å¢å¼ºèƒ½æé«˜æ¨¡å‹æ€§èƒ½ï¼ŒGPT-4 Turboåœ¨F1åˆ†æ•°ä¸Šè¡¨ç°æœ€ä½³ï¼Œè€ŒMistral7Båœ¨å¹³å‡å€’æ•°æ’åï¼ˆMRRï¼‰ä¸Šè¡¨ç°æœ€ä½³ã€‚æ­¤å¤–ï¼Œå¼€æºæ¨¡å‹åœ¨å¾®è°ƒæˆ–æ•°æ®å¢å¼ºåè¡¨ç°ä¼˜äºé—­æºæ¨¡å‹ã€‚ç ”ç©¶è¿˜å‘ç°ï¼Œå°‘æ ·æœ¬æç¤ºé€šå¸¸ä¼˜äºé›¶æ ·æœ¬ï¼Œç»“æ„åŒ–æç¤ºåœ¨ä¸åŒæ¨¡å‹ä¸­æœ‰ä¸åŒåå¥½ï¼Œå¾®è°ƒè™½èƒ½æé«˜é›¶æ ·æœ¬æ€§èƒ½ä½†æœ‰æ—¶ä¼šé™ä½å°‘æ ·æœ¬æ€§èƒ½ã€‚æ€»ä½“è€Œè¨€ï¼Œæç¤ºã€å¾®è°ƒã€æ•°æ®å¢å¼ºç­‰æ–¹æ³•åœ¨åŒ»å­¦æœ¯è¯­æå–æ–¹é¢æ•ˆæœæ˜¾è‘—ï¼Œç‰¹åˆ«æ˜¯åœ¨èµ„æºæœ‰é™çš„åœºæ™¯ä¸‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OpenNoteså…è®¸æ‚£è€…è®¿é—®ç”µå­å¥åº·è®°å½•ï¼ˆEHRï¼‰ç¬”è®°ï¼Œä½†åŒ»å­¦æœ¯è¯­çš„å¤æ‚æ€§å¯èƒ½é˜»ç¢ç†è§£ã€‚</li>
<li>ç ”ç©¶è¯„ä¼°äº†é—­æºå’Œå¼€æºçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æå–å’Œæ’åºå…³é”®åŒ»å­¦æœ¯è¯­æ–¹é¢çš„è¡¨ç°ã€‚</li>
<li>ä½¿ç”¨äº†æç¤ºã€å¾®è°ƒã€æ•°æ®å¢å¼ºç­‰æ–¹æ³•æ¥æ”¹å–„æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>GPT-4 Turboåœ¨F1åˆ†æ•°ä¸Šè¡¨ç°æœ€ä½³ï¼Œè€ŒMistral7Båœ¨å¹³å‡å€’æ•°æ’åï¼ˆMRRï¼‰ä¸Šé¢†å…ˆã€‚</li>
<li>å¼€æºæ¨¡å‹åœ¨å¾®è°ƒæˆ–æ•°æ®å¢å¼ºåè¡¨ç°ä¼˜äºé—­æºæ¨¡å‹ã€‚</li>
<li>å°‘æ ·æœ¬æç¤ºé€šå¸¸æ¯”é›¶æ ·æœ¬æç¤ºæ›´æœ‰æ•ˆï¼Œç»“æ„åŒ–æç¤ºèƒ½æé«˜æ¨¡å‹æ€§èƒ½ä½†ä¸åŒæ¨¡å‹è¡¨ç°ä¸åŒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.16022">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8c9dc397742ef574027a3610a960f0a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-35740911d587ff17db7d6bc11781608c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f51ce4c78a0ef5f740f0d06fe9761fc5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-26da0512aecf25bd3998960496ea89c8.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="MoMa-A-Modular-Deep-Learning-Framework-for-Material-Property-Prediction"><a href="#MoMa-A-Modular-Deep-Learning-Framework-for-Material-Property-Prediction" class="headerlink" title="MoMa: A Modular Deep Learning Framework for Material Property Prediction"></a>MoMa: A Modular Deep Learning Framework for Material Property Prediction</h2><p><strong>Authors:Botian Wang, Yawen Ouyang, Yaohui Li, Yiqun Wang, Haorui Cui, Jianbing Zhang, Xiaonan Wang, Wei-Ying Ma, Hao Zhou</strong></p>
<p>Deep learning methods for material property prediction have been widely explored to advance materials discovery. However, the prevailing pre-train then fine-tune paradigm often fails to address the inherent diversity and disparity of material tasks. To overcome these challenges, we introduce MoMa, a Modular framework for Materials that first trains specialized modules across a wide range of tasks and then adaptively composes synergistic modules tailored to each downstream scenario. Evaluation across 17 datasets demonstrates the superiority of MoMa, with a substantial 14% average improvement over the strongest baseline. Few-shot and continual learning experiments further highlight MoMaâ€™s potential for real-world applications. Pioneering a new paradigm of modular material learning, MoMa will be open-sourced to foster broader community collaboration. </p>
<blockquote>
<p>æ·±åº¦å­¦ä¹ åœ¨ææ–™å±æ€§é¢„æµ‹æ–¹é¢çš„åº”ç”¨å·²å¾—åˆ°å¹¿æ³›æ¢ç´¢ï¼Œä»¥æ¨åŠ¨ææ–™å‘ç°çš„å‘å±•ã€‚ç„¶è€Œï¼Œæµè¡Œçš„é¢„è®­ç»ƒå¾®è°ƒæ¨¡å¼å¾€å¾€æ— æ³•è§£å†³ææ–™ä»»åŠ¡çš„å›ºæœ‰å¤šæ ·æ€§å’Œå·®å¼‚æ€§ã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†MoMaï¼Œè¿™æ˜¯ä¸€ä¸ªé¢å‘ææ–™çš„æ¨¡å—åŒ–æ¡†æ¶ï¼Œå®ƒé¦–å…ˆè®­ç»ƒé€‚ç”¨äºå„ç§ä»»åŠ¡çš„ä¸“ç”¨æ¨¡å—ï¼Œç„¶åè‡ªé€‚åº”åœ°ç»„åˆé’ˆå¯¹æ¯ä¸ªä¸‹æ¸¸åœºæ™¯çš„ååŒæ¨¡å—ã€‚åœ¨17ä¸ªæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¯æ˜äº†MoMaçš„ä¼˜è¶Šæ€§ï¼Œç›¸è¾ƒäºæœ€å¼ºçš„åŸºçº¿æ¨¡å‹ï¼Œå…¶å¹³å‡æ”¹è¿›äº†é«˜è¾¾14%ã€‚å°æ ·æœ¬å’ŒæŒç»­å­¦ä¹ çš„å®éªŒè¿›ä¸€æ­¥çªæ˜¾äº†MoMaåœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„æ½œåŠ›ã€‚ä½œä¸ºæ¨¡å—åŒ–ææ–™å­¦ä¹ çš„æ–°èŒƒå¼å…ˆé©±ï¼ŒMoMaå°†å¼€æºä»¥ä¿ƒè¿›æ›´å¹¿æ³›çš„ç¤¾åŒºåˆä½œã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.15483v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ææ–™å±æ€§é¢„æµ‹çš„æ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨æ¨åŠ¨ææ–™å‘ç°æ–¹é¢å¾—åˆ°äº†å¹¿æ³›çš„ç ”ç©¶ã€‚ç„¶è€Œï¼Œå½“å‰å…ˆé¢„è®­ç»ƒå†å¾®è°ƒçš„æ¨¡å¼å¾€å¾€æ— æ³•åº”å¯¹ææ–™ä»»åŠ¡çš„å†…åœ¨å¤šæ ·æ€§å’Œå·®å¼‚æ€§ã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºMoMaæ¨¡å—åŒ–çš„ææ–™å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é¦–å…ˆå¯¹å„ç§ä»»åŠ¡è¿›è¡Œä¸“ä¸šåŒ–æ¨¡å—è®­ç»ƒï¼Œç„¶åé’ˆå¯¹æ¯ç§ä¸‹æ¸¸åœºæ™¯è‡ªé€‚åº”åœ°ç»„åˆååŒæ¨¡å—ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºMoMaçš„ä¼˜è¶Šæ€§ï¼Œç›¸å¯¹äºæœ€å¼ºåŸºçº¿æœ‰å¹³å‡æé«˜ç™¾åˆ†ä¹‹åå››çš„æ•ˆæœã€‚å°‘æ ·æœ¬å’ŒæŒç»­å­¦ä¹ çš„å®éªŒè¿›ä¸€æ­¥å‡¸æ˜¾äº†MoMaåœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„æ½œåŠ›ã€‚MoMaå¼€åˆ›äº†æ¨¡å—åŒ–ææ–™å­¦ä¹ çš„æ–°èŒƒå¼ï¼Œå¹¶å°†å¼€æºä»¥ä¿ƒè¿›æ›´å¹¿æ³›çš„ç¤¾åŒºåˆä½œã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰é¢„è®­ç»ƒå¾®è°ƒæ¨¡å‹åœ¨ææ–™å­¦ä¹ ä»»åŠ¡ä¸­çš„è¡¨ç°æœ‰å¾…æé«˜ã€‚</li>
<li>MoMaæ˜¯ä¸€ä¸ªå…¨æ–°çš„ææ–™å­¦ä¹ æ¡†æ¶ï¼Œé‡‡ç”¨æ¨¡å—åŒ–è®¾è®¡ä»¥é€‚åº”ä¸åŒçš„ææ–™ä»»åŠ¡ã€‚</li>
<li>MoMaé€šè¿‡è®­ç»ƒä¸“ä¸šæ¨¡å—å¹¶è‡ªé€‚åº”ç»„åˆååŒæ¨¡å—æ¥æé«˜æ€§èƒ½ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºMoMaç›¸å¯¹äºç°æœ‰æ–¹æ³•å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚</li>
<li>MoMaåœ¨å°‘æ ·æœ¬å’ŒæŒç»­å­¦ä¹ æ–¹é¢çš„è¡¨ç°çªå‡ºå…¶å®é™…åº”ç”¨æ½œåŠ›ã€‚</li>
<li>MoMaå°†å¼€æºä»¥ä¿ƒè¿›æ›´å¹¿æ³›çš„ç¤¾åŒºåˆä½œå’Œè¿›ä¸€æ­¥å‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.15483">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6e6539210e968776afb64847f09f39ec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5fc390130116b6925fa6a461008e3fe4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3e773a0fbd0487f6e1ff3ee8efbbabdd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1ab815da1904d1b42bbe91cc91854c5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5e6a71e1a88d8a1da2d46e96a4b21883.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Few-shot-Species-Range-Estimation"><a href="#Few-shot-Species-Range-Estimation" class="headerlink" title="Few-shot Species Range Estimation"></a>Few-shot Species Range Estimation</h2><p><strong>Authors:Christian Lange, Max Hamilton, Elijah Cole, Alexander Shepard, Samuel Heinrich, Angela Zhu, Subhransu Maji, Grant Van Horn, Oisin Mac Aodha</strong></p>
<p>Knowing where a particular species can or cannot be found on Earth is crucial for ecological research and conservation efforts. By mapping the spatial ranges of all species, we would obtain deeper insights into how global biodiversity is affected by climate change and habitat loss. However, accurate range estimates are only available for a relatively small proportion of all known species. For the majority of the remaining species, we often only have a small number of records denoting the spatial locations where they have previously been observed. We outline a new approach for few-shot species range estimation to address the challenge of accurately estimating the range of a species from limited data. During inference, our model takes a set of spatial locations as input, along with optional metadata such as text or an image, and outputs a species encoding that can be used to predict the range of a previously unseen species in feed-forward manner. We validate our method on two challenging benchmarks, where we obtain state-of-the-art range estimation performance, in a fraction of the compute time, compared to recent alternative approaches. </p>
<blockquote>
<p>äº†è§£ç‰¹å®šç‰©ç§åœ¨åœ°çƒä¸Šå¯ä»¥æˆ–ä¸èƒ½è¢«å‘ç°çš„ä½ç½®å¯¹äºç”Ÿæ€ç ”ç©¶å’Œä¿æŠ¤å·¥ä½œè‡³å…³é‡è¦ã€‚é€šè¿‡ç»˜åˆ¶æ‰€æœ‰ç‰©ç§çš„ç©ºé—´åˆ†å¸ƒèŒƒå›´å›¾ï¼Œæˆ‘ä»¬å°†æ›´æ·±å…¥åœ°äº†è§£å…¨çƒç”Ÿç‰©å¤šæ ·æ€§å¦‚ä½•å—åˆ°æ°”å€™å˜åŒ–å’Œæ –æ¯åœ°ä¸§å¤±çš„å½±å“ã€‚ç„¶è€Œï¼Œå‡†ç¡®çš„èŒƒå›´ä¼°è®¡ä»…é€‚ç”¨äºå·²çŸ¥ç‰©ç§ä¸­çš„ä¸€å°éƒ¨åˆ†ã€‚å¯¹äºå¤§å¤šæ•°å‰©ä½™ç‰©ç§ï¼Œæˆ‘ä»¬é€šå¸¸åªæœ‰å°‘æ•°è®°å½•æ ‡æ³¨äº†å®ƒä»¬ä¹‹å‰è§‚å¯Ÿåˆ°çš„ç©ºé—´ä½ç½®ã€‚æˆ‘ä»¬æ¦‚è¿°äº†ä¸€ç§æ–°çš„å°‘é‡ç‰©ç§èŒƒå›´ä¼°è®¡æ–¹æ³•ï¼Œä»¥è§£å†³ä»æœ‰é™æ•°æ®ä¸­å‡†ç¡®ä¼°è®¡ç‰©ç§èŒƒå›´çš„æŒ‘æˆ˜ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ä»¥ä¸€ç»„ç©ºé—´ä½ç½®ä½œä¸ºè¾“å…¥ï¼Œä»¥åŠå¯é€‰çš„å…ƒæ•°æ®ï¼Œå¦‚æ–‡æœ¬æˆ–å›¾åƒï¼Œå¹¶è¾“å‡ºä¸€ä¸ªç‰©ç§ç¼–ç ï¼Œè¯¥ç¼–ç å¯ç”¨äºä»¥å‰æ‰€æœªè§çš„ç‰©ç§çš„èŒƒå›´é¢„æµ‹ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸ŠéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œä¸ä½¿ç”¨æœ€è¿‘çš„æ›¿ä»£æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬åœ¨è®¡ç®—æ—¶é—´ä¸Šå¤§å¤§ç¼©çŸ­ï¼Œå¹¶è·å¾—äº†æœ€å…ˆè¿›çš„èŒƒå›´ä¼°è®¡æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14977v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>è¯¥æ–‡æŒ‡å‡ºç‰©ç§ç©ºé—´åˆ†å¸ƒæ˜ å°„å¯¹ç”Ÿæ€ç ”ç©¶å’Œä¿æŠ¤å·¥ä½œçš„é‡è¦æ€§ã€‚ç„¶è€Œï¼Œç›®å‰å‡†ç¡®çš„ç‰©ç§åˆ†å¸ƒèŒƒå›´æ•°æ®åªæ¶µç›–äº†ä¸€å°éƒ¨åˆ†å·²çŸ¥ç‰©ç§ã€‚å¯¹äºå¤§å¤šæ•°ç‰©ç§ï¼Œæˆ‘ä»¬åªæœ‰å°‘æ•°è®°å½•è¡¨æ˜å®ƒä»¬æ›¾åœ¨å“ªäº›åœ°ç‚¹å‡ºç°è¿‡ã€‚å› æ­¤ï¼Œæ–‡ç« æå‡ºäº†ä¸€ç§åŸºäºå°‘é‡æ•°æ®çš„ç‰©ç§åˆ†å¸ƒèŒƒå›´ä¼°è®¡æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿé€šè¿‡æ¥æ”¶ç©ºé—´ä½ç½®å’Œå¯é€‰å…ƒæ•°æ®ï¼ˆå¦‚æ–‡æœ¬æˆ–å›¾åƒï¼‰ä½œä¸ºè¾“å…¥ï¼Œè¾“å‡ºä¸€ä¸ªç‰©ç§ç¼–ç ï¼Œä»¥é¢„æµ‹æœªè§ç‰©ç§çš„åˆ†å¸ƒèŒƒå›´ã€‚åœ¨ä¸¤é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•åœ¨è¿ç®—æ—¶é—´å¤§å¹…å‡å°‘çš„æƒ…å†µä¸‹ï¼Œä»å–å¾—äº†æœ€å…ˆè¿›çš„èŒƒå›´ä¼°è®¡æ€§èƒ½ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>ç‰©ç§ç©ºé—´åˆ†å¸ƒå¯¹ç”Ÿæ€ç ”ç©¶å’Œä¿æŠ¤è‡³å…³é‡è¦ã€‚</li>
<li>ç›®å‰å‡†ç¡®çš„ç‰©ç§åˆ†å¸ƒèŒƒå›´æ•°æ®ä»…é™äºä¸€å°éƒ¨åˆ†å·²çŸ¥ç‰©ç§ã€‚</li>
<li>å¯¹äºå¤§å¤šæ•°ç‰©ç§ï¼Œåªæœ‰å°‘æ•°è®°å½•è¡¨æ˜å®ƒä»¬æ›¾åœ¨å“ªäº›åœ°ç‚¹å‡ºç°è¿‡ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºå°‘é‡æ•°æ®çš„ç‰©ç§åˆ†å¸ƒèŒƒå›´ä¼°è®¡æ–°æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡æ¥æ”¶ç©ºé—´ä½ç½®å’Œå¯é€‰å…ƒæ•°æ®ä½œä¸ºè¾“å…¥ï¼Œè¾“å‡ºä¸€ä¸ªç‰©ç§ç¼–ç ã€‚</li>
<li>æ–¹æ³•èƒ½å¤Ÿåœ¨é¢„æµ‹æœªè§ç‰©ç§çš„åˆ†å¸ƒèŒƒå›´æ—¶ï¼Œä»¥é¢„æµ‹æ¨æ–­çš„æ–¹å¼è¿›è¡Œåº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14977">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8287a5c4e12415ceba4f05c3fbf9d716.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-334f488c64c136b6999f325e6c033b01.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-df4545e72aa0db7f1dd9136134978292.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-57378ede8370aba20fe92ef87d467895.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6cba238f2211c91e26fad0857cb35dc.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="CAPE-Covariate-Adjusted-Pre-Training-for-Epidemic-Time-Series-Forecasting"><a href="#CAPE-Covariate-Adjusted-Pre-Training-for-Epidemic-Time-Series-Forecasting" class="headerlink" title="CAPE: Covariate-Adjusted Pre-Training for Epidemic Time Series   Forecasting"></a>CAPE: Covariate-Adjusted Pre-Training for Epidemic Time Series   Forecasting</h2><p><strong>Authors:Zewen Liu, Juntong Ni, Max S. Y. Lau, Wei Jin</strong></p>
<p>Accurate forecasting of epidemic infection trajectories is crucial for safeguarding public health. However, limited data availability during emerging outbreaks and the complex interaction between environmental factors and disease dynamics present significant challenges for effective forecasting. In response, we introduce CAPE, a novel epidemic pre-training framework designed to harness extensive disease datasets from diverse regions and integrate environmental factors directly into the modeling process for more informed decision-making on downstream diseases. Based on a covariate adjustment framework, CAPE utilizes pre-training combined with hierarchical environment contrasting to identify universal patterns across diseases while estimating latent environmental influences. We have compiled a diverse collection of epidemic time series datasets and validated the effectiveness of CAPE under various evaluation scenarios, including full-shot, few-shot, zero-shot, cross-location, and cross-disease settings, where it outperforms the leading baseline by an average of 9.9% in full-shot and 14.3% in zero-shot settings. The code will be released upon acceptance. </p>
<blockquote>
<p>ä¼ æŸ“ç—…æ„ŸæŸ“è½¨è¿¹çš„å‡†ç¡®é¢„æµ‹å¯¹äºä¿éšœå…¬å…±å«ç”Ÿè‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œåœ¨æ–°å‹ç–«æƒ…çˆ†å‘æœŸé—´æ•°æ®å¯ç”¨æ€§çš„æœ‰é™æ€§ä»¥åŠç¯å¢ƒå› å­ä¸ç–¾ç—…åŠ¨æ€ä¹‹é—´çš„å¤æ‚äº¤äº’ï¼Œä¸ºæœ‰æ•ˆçš„é¢„æµ‹å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†CAPEï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹ä¼ æŸ“ç—…é¢„è®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨åˆ©ç”¨æ¥è‡ªä¸åŒåœ°åŒºçš„å¹¿æ³›ç–¾ç—…æ•°æ®é›†ï¼Œå¹¶å°†ç¯å¢ƒå› å­ç›´æ¥æ•´åˆåˆ°å»ºæ¨¡è¿‡ç¨‹ä¸­ï¼Œä»¥ä¸ºä¸‹æ¸¸ç–¾ç—…çš„å†³ç­–æä¾›æ›´æœ‰ä¾æ®çš„å†³ç­–ã€‚åŸºäºåå˜é‡è°ƒæ•´æ¡†æ¶ï¼ŒCAPEåˆ©ç”¨é¢„è®­ç»ƒç»“åˆåˆ†å±‚ç¯å¢ƒå¯¹æ¯”ï¼Œä»¥è¯†åˆ«ç–¾ç—…ä¹‹é—´çš„é€šç”¨æ¨¡å¼ï¼ŒåŒæ—¶ä¼°è®¡æ½œåœ¨çš„ç¯å¢ƒå½±å“ã€‚æˆ‘ä»¬å·²ç¼–è¯‘äº†å¤šç§ä¼ æŸ“ç—…æ—¶é—´åºåˆ—æ•°æ®é›†ï¼Œå¹¶åœ¨å„ç§è¯„ä¼°åœºæ™¯ä¸‹éªŒè¯äº†CAPEçš„æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬å…¨æ•°æ®ã€å°æ•°æ®ã€é›¶æ•°æ®ã€è·¨åœ°ç‚¹å’Œè·¨ç–¾ç—…è®¾ç½®ã€‚åœ¨å…¨æ•°æ®è®¾ç½®ä¸­ï¼Œå®ƒæ¯”é¢†å…ˆåŸºçº¿é«˜å‡º9.9%ï¼›åœ¨é›¶æ•°æ®è®¾ç½®ä¸­ï¼Œé«˜å‡º14.3%ã€‚ä»£ç å°†åœ¨æ¥å—åå‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03393v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>CAPEæ˜¯ä¸€ä¸ªæ–°å‹æµè¡Œç—…é¢„è®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨åˆ©ç”¨æ¥è‡ªä¸åŒåœ°åŒºçš„ä¸°å¯Œç–¾ç—…æ•°æ®é›†ï¼Œå¹¶å°†ç¯å¢ƒå› ç´ ç›´æ¥çº³å…¥å»ºæ¨¡è¿‡ç¨‹ä¸­ï¼Œä¸ºä¸‹æ¸¸ç–¾ç—…çš„å†³ç­–æä¾›æ›´æœ‰æ ¹æ®çš„é¢„æµ‹ã€‚CAPEåŸºäºåå˜é‡è°ƒæ•´æ¡†æ¶ï¼Œé‡‡ç”¨é¢„è®­ç»ƒå’Œå±‚æ¬¡ç¯å¢ƒå¯¹æ¯”æŠ€æœ¯ï¼Œè¯†åˆ«ç–¾ç—…é—´çš„é€šç”¨æ¨¡å¼ï¼ŒåŒæ—¶è¯„ä¼°æ½œåœ¨ç¯å¢ƒå½±å“ã€‚åœ¨å¤šç§è¯„ä¼°åœºæ™¯ä¸­ï¼ŒCAPEè¡¨ç°ä¼˜å¼‚ï¼Œå¹³å‡è¶…å‡ºé¢†å…ˆåŸºçº¿9.9%ï¼ˆå…¨æ•°æ®åœºæ™¯ï¼‰å’Œ14.3%ï¼ˆé›¶æ•°æ®åœºæ™¯ï¼‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CAPEæ˜¯ä¸€ä¸ªæµè¡Œç—…é¢„è®­ç»ƒæ¡†æ¶ï¼Œç”¨äºé¢„æµ‹æ„ŸæŸ“è½¨è¿¹ã€‚</li>
<li>å®ƒç»“åˆäº†æ¥è‡ªä¸åŒåœ°åŒºçš„ä¸°å¯Œç–¾ç—…æ•°æ®é›†ã€‚</li>
<li>CAPEå°†ç¯å¢ƒå› ç´ ç›´æ¥çº³å…¥å»ºæ¨¡è¿‡ç¨‹ã€‚</li>
<li>åŸºäºåå˜é‡è°ƒæ•´æ¡†æ¶ï¼ŒCAPEé‡‡ç”¨é¢„è®­ç»ƒå’Œå±‚æ¬¡ç¯å¢ƒå¯¹æ¯”æŠ€æœ¯ã€‚</li>
<li>CAPEåœ¨ä¸åŒè¯„ä¼°åœºæ™¯ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒåŒ…æ‹¬å…¨æ•°æ®ã€å°‘æ•°æ®ã€é›¶æ•°æ®ã€è·¨åœ°ç‚¹å’Œè·¨ç–¾ç—…è®¾ç½®ã€‚</li>
<li>ä¸é¢†å…ˆåŸºçº¿ç›¸æ¯”ï¼ŒCAPEåœ¨å…¨æ•°æ®åœºæ™¯ä¸­å¹³å‡è¡¨ç°è¶…å‡º9.9%ï¼Œåœ¨é›¶æ•°æ®åœºæ™¯ä¸­å¹³å‡è¶…å‡º14.3%ã€‚</li>
<li>ä»£ç å°†åœ¨æ¥å—åå‘å¸ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03393">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-11eecbad75f2db6377b288c1ddb51373.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4fa23534cab0786091679ef7d71a16aa.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Modeling-Multi-modal-Cross-interaction-for-Multi-label-Few-shot-Image-Classification-Based-on-Local-Feature-Selection"><a href="#Modeling-Multi-modal-Cross-interaction-for-Multi-label-Few-shot-Image-Classification-Based-on-Local-Feature-Selection" class="headerlink" title="Modeling Multi-modal Cross-interaction for Multi-label Few-shot Image   Classification Based on Local Feature Selection"></a>Modeling Multi-modal Cross-interaction for Multi-label Few-shot Image   Classification Based on Local Feature Selection</h2><p><strong>Authors:Kun Yan, Zied Bouraoui, Fangyun Wei, Chang Xu, Ping Wang, Shoaib Jameel, Steven Schockaert</strong></p>
<p>The aim of multi-label few-shot image classification (ML-FSIC) is to assign semantic labels to images, in settings where only a small number of training examples are available for each label. A key feature of the multi-label setting is that an image often has several labels, which typically refer to objects appearing in different regions of the image. When estimating label prototypes, in a metric-based setting, it is thus important to determine which regions are relevant for which labels, but the limited amount of training data and the noisy nature of local features make this highly challenging. As a solution, we propose a strategy in which label prototypes are gradually refined. First, we initialize the prototypes using word embeddings, which allows us to leverage prior knowledge about the meaning of the labels. Second, taking advantage of these initial prototypes, we then use a Loss Change Measurement (LCM) strategy to select the local features from the training images (i.e. the support set) that are most likely to be representative of a given label. Third, we construct the final prototype of the label by aggregating these representative local features using a multi-modal cross-interaction mechanism, which again relies on the initial word embedding-based prototypes. Experiments on COCO, PASCAL VOC, NUS-WIDE, and iMaterialist show that our model substantially improves the current state-of-the-art. </p>
<blockquote>
<p>å¤šæ ‡ç­¾å°‘æ ·æœ¬å›¾åƒåˆ†ç±»ï¼ˆML-FSICï¼‰çš„ç›®æ ‡æ˜¯åœ¨æ¯ä¸ªæ ‡ç­¾åªæœ‰å°‘é‡è®­ç»ƒæ ·æœ¬çš„æƒ…å†µä¸‹ï¼Œä¸ºå›¾åƒåˆ†é…è¯­ä¹‰æ ‡ç­¾ã€‚å¤šæ ‡ç­¾è®¾ç½®çš„ä¸€ä¸ªå…³é”®ç‰¹å¾æ˜¯å›¾åƒé€šå¸¸å…·æœ‰å¤šä¸ªæ ‡ç­¾ï¼Œè¿™äº›æ ‡ç­¾é€šå¸¸æŒ‡ä»£å›¾åƒä¸åŒåŒºåŸŸä¸­å‡ºç°çš„å¯¹è±¡ã€‚åœ¨åŸºäºåº¦é‡çš„ç¯å¢ƒä¸­ä¼°è®¡æ ‡ç­¾åŸå‹æ—¶ï¼Œç¡®å®šå“ªäº›åŒºåŸŸä¸å“ªäº›æ ‡ç­¾ç›¸å…³éå¸¸é‡è¦ï¼Œä½†è®­ç»ƒæ•°æ®çš„æœ‰é™æ€§å’Œå±€éƒ¨ç‰¹å¾çš„å™ªå£°æ€§è´¨ä½¿å¾—è¿™æå…·æŒ‘æˆ˜æ€§ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é€æ­¥ä¼˜åŒ–æ ‡ç­¾åŸå‹çš„ç­–ç•¥ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨è¯åµŒå…¥åˆå§‹åŒ–åŸå‹ï¼Œä»¥ä¾¿åˆ©ç”¨æœ‰å…³æ ‡ç­¾å«ä¹‰çš„å…ˆéªŒçŸ¥è¯†ã€‚å…¶æ¬¡ï¼Œåˆ©ç”¨è¿™äº›åˆå§‹åŸå‹ï¼Œæˆ‘ä»¬ä½¿ç”¨æŸå¤±å˜åŒ–æµ‹é‡ï¼ˆLCMï¼‰ç­–ç•¥ä»è®­ç»ƒå›¾åƒï¼ˆå³æ”¯æŒé›†ï¼‰ä¸­é€‰æ‹©æœ€å¯èƒ½ä»£è¡¨ç»™å®šæ ‡ç­¾çš„å±€éƒ¨ç‰¹å¾ã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡å¤šæ¨¡æ€äº¤å‰äº¤äº’æœºåˆ¶èšåˆè¿™äº›ä»£è¡¨æ€§å±€éƒ¨ç‰¹å¾æ¥æ„å»ºæ ‡ç­¾çš„æœ€ç»ˆåŸå‹ï¼Œè¿™åŒæ ·ä¾èµ–äºæœ€åˆçš„åŸºäºè¯åµŒå…¥çš„åŸå‹ã€‚åœ¨COCOã€PASCAL VOCã€NUS-WIDEå’ŒiMaterialistä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å¤§å¤§æ”¹è¿›äº†å½“å‰çš„æœ€ä½³æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13732v2">PDF</a> In Transactions on Multimedia Computing Communications and   Applications. arXiv admin note: text overlap with arXiv:2112.01037</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤šæ ‡ç­¾å°‘æ ·æœ¬å›¾åƒåˆ†ç±»ï¼ˆML-FSICï¼‰çš„ç›®æ ‡ï¼Œå³åœ¨æ¯ä¸ªæ ‡ç­¾åªæœ‰å°‘é‡è®­ç»ƒæ ·æœ¬çš„æƒ…å†µä¸‹ï¼Œä¸ºå›¾åƒåˆ†é…è¯­ä¹‰æ ‡ç­¾ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§é€æ­¥ä¼˜åŒ–æ ‡ç­¾åŸå‹çš„æ–¹æ³•ï¼Œé¦–å…ˆåˆ©ç”¨è¯åµŒå…¥åˆå§‹åŒ–åŸå‹ï¼Œç„¶åä½¿ç”¨å±€éƒ¨ç‰¹å¾é€‰æ‹©ç­–ç•¥ï¼ˆLoss Change Measurementï¼ŒLCMï¼‰é€‰æ‹©æœ€æœ‰å¯èƒ½ä»£è¡¨ç»™å®šæ ‡ç­¾çš„å±€éƒ¨ç‰¹å¾ï¼Œæœ€åé€šè¿‡å¤šæ¨¡æ€äº¤å‰äº¤äº’æœºåˆ¶æ„å»ºæœ€ç»ˆçš„æ ‡ç­¾åŸå‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨COCOã€PASCAL VOCã€NUS-WIDEå’ŒiMaterialistç­‰å¤šä¸ªæ•°æ®é›†ä¸Šå‡æ˜¾è‘—æé«˜äº†å½“å‰æŠ€æœ¯æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ ‡ç­¾å°‘æ ·æœ¬å›¾åƒåˆ†ç±»ï¼ˆML-FSICï¼‰çš„ç›®æ ‡æ˜¯åœ¨å°‘é‡è®­ç»ƒæ ·æœ¬çš„æƒ…å†µä¸‹ï¼Œä¸ºå›¾åƒåˆ†é…å¤šä¸ªè¯­ä¹‰æ ‡ç­¾ã€‚</li>
<li>å›¾åƒé€šå¸¸å…·æœ‰å¤šä¸ªæ ‡ç­¾ï¼Œè¿™äº›æ ‡ç­¾é€šå¸¸æŒ‡ä»£å›¾åƒä¸­ä¸åŒåŒºåŸŸçš„ç‰©ä½“ã€‚</li>
<li>åœ¨ä¼°è®¡æ ‡ç­¾åŸå‹æ—¶ï¼Œéœ€è¦ç¡®å®šå“ªäº›åŒºåŸŸä¸å“ªäº›æ ‡ç­¾ç›¸å…³ï¼Œä½†ç”±äºè®­ç»ƒæ•°æ®æœ‰é™å’Œå±€éƒ¨ç‰¹å¾çš„å™ªå£°æ€§è´¨ï¼Œè¿™å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§é€æ­¥ä¼˜åŒ–æ ‡ç­¾åŸå‹çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬ä½¿ç”¨è¯åµŒå…¥åˆå§‹åŒ–åŸå‹ã€åˆ©ç”¨Loss Change Measurementï¼ˆLCMï¼‰ç­–ç•¥é€‰æ‹©å±€éƒ¨ç‰¹å¾å’Œæ„å»ºæœ€ç»ˆæ ‡ç­¾åŸå‹ã€‚</li>
<li>è¯åµŒå…¥å…è®¸åˆ©ç”¨æ ‡ç­¾çš„å…ˆéªŒçŸ¥è¯†ï¼ŒLCMç­–ç•¥æœ‰åŠ©äºé€‰æ‹©æœ€å¯èƒ½ä»£è¡¨ç»™å®šæ ‡ç­¾çš„å±€éƒ¨ç‰¹å¾ã€‚</li>
<li>å¤šæ¨¡æ€äº¤å‰äº¤äº’æœºåˆ¶ç”¨äºèšåˆä»£è¡¨æ€§å±€éƒ¨ç‰¹å¾ï¼Œè¿›ä¸€æ­¥ä¾èµ–äºåˆå§‹çš„è¯åµŒå…¥åŸå‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13732">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f1b2bb12381dfd4cff798709741958e5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fe1022b5c81b4d29225382c1fccd5171.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Manta-Enhancing-Mamba-for-Few-Shot-Action-Recognition-of-Long-Sub-Sequence"><a href="#Manta-Enhancing-Mamba-for-Few-Shot-Action-Recognition-of-Long-Sub-Sequence" class="headerlink" title="Manta: Enhancing Mamba for Few-Shot Action Recognition of Long   Sub-Sequence"></a>Manta: Enhancing Mamba for Few-Shot Action Recognition of Long   Sub-Sequence</h2><p><strong>Authors:Wenbo Huang, Jinghui Zhang, Guang Li, Lei Zhang, Shuoyuan Wang, Fang Dong, Jiahui Jin, Takahiro Ogawa, Miki Haseyama</strong></p>
<p>In few-shot action recognition (FSAR), long sub-sequences of video naturally express entire actions more effectively. However, the high computational complexity of mainstream Transformer-based methods limits their application. Recent Mamba demonstrates efficiency in modeling long sequences, but directly applying Mamba to FSAR overlooks the importance of local feature modeling and alignment. Moreover, long sub-sequences within the same class accumulate intra-class variance, which adversely impacts FSAR performance. To solve these challenges, we propose a Matryoshka MAmba and CoNtrasTive LeArning framework (Manta). Firstly, the Matryoshka Mamba introduces multiple Inner Modules to enhance local feature representation, rather than directly modeling global features. An Outer Module captures dependencies of timeline between these local features for implicit temporal alignment. Secondly, a hybrid contrastive learning paradigm, combining both supervised and unsupervised methods, is designed to mitigate the negative effects of intra-class variance accumulation. The Matryoshka Mamba and the hybrid contrastive learning paradigm operate in two parallel branches within Manta, enhancing Mamba for FSAR of long sub-sequence. Manta achieves new state-of-the-art performance on prominent benchmarks, including SSv2, Kinetics, UCF101, and HMDB51. Extensive empirical studies prove that Manta significantly improves FSAR of long sub-sequence from multiple perspectives. </p>
<blockquote>
<p>åœ¨å°‘æ ·æœ¬åŠ¨ä½œè¯†åˆ«ï¼ˆFSARï¼‰ä¸­ï¼Œè§†é¢‘çš„é•¿å­åºåˆ—æ›´è‡ªç„¶åœ°è¡¨è¾¾äº†æ•´ä¸ªåŠ¨ä½œã€‚ç„¶è€Œï¼Œä¸»æµåŸºäºTransformerçš„æ–¹æ³•çš„é«˜è®¡ç®—å¤æ‚åº¦é™åˆ¶äº†å…¶åº”ç”¨ã€‚æœ€è¿‘çš„Mambaåœ¨å»ºæ¨¡é•¿åºåˆ—æ–¹é¢å±•ç¤ºäº†æ•ˆç‡ï¼Œä½†ç›´æ¥å°†Mambaåº”ç”¨äºFSARå¿½ç•¥äº†å±€éƒ¨ç‰¹å¾å»ºæ¨¡å’Œå¯¹é½çš„é‡è¦æ€§ã€‚æ­¤å¤–ï¼ŒåŒä¸€ç±»åˆ«å†…çš„é•¿å­åºåˆ—ä¼šç´¯ç§¯ç±»å†…æ–¹å·®ï¼Œè¿™å¯¹FSARæ€§èƒ½äº§ç”Ÿä¸åˆ©å½±å“ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†Matryoshka Mambaå’Œå¯¹æ¯”å­¦ä¹ æ¡†æ¶ï¼ˆMantaï¼‰ã€‚é¦–å…ˆï¼ŒMatryoshka Mambaå¼•å…¥äº†å¤šä¸ªå†…éƒ¨æ¨¡å—æ¥å¢å¼ºå±€éƒ¨ç‰¹å¾è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ç›´æ¥å»ºæ¨¡å…¨å±€ç‰¹å¾ã€‚å¤–éƒ¨æ¨¡å—æ•è·è¿™äº›å±€éƒ¨ç‰¹å¾ä¹‹é—´æ—¶é—´çº¿çš„ä¾èµ–æ€§ï¼Œä»¥è¿›è¡Œéšå¼æ—¶é—´å¯¹é½ã€‚å…¶æ¬¡ï¼Œç»“åˆæœ‰ç›‘ç£å’Œæ— ç›‘ç£æ–¹æ³•çš„æ··åˆå¯¹æ¯”å­¦ä¹ èŒƒå¼æ—¨åœ¨å‡è½»ç±»å†…æ–¹å·®ç´¯ç§¯çš„è´Ÿé¢å½±å“ã€‚Matryoshka Mambaå’Œæ··åˆå¯¹æ¯”å­¦ä¹ èŒƒå¼åœ¨Mantaçš„ä¸¤ä¸ªå¹¶è¡Œåˆ†æ”¯ä¸­è¿è¡Œï¼Œå¢å¼ºäº†Mambaå¯¹é•¿å­åºåˆ—çš„FSARèƒ½åŠ›ã€‚Mantaåœ¨SSv2ã€Kineticsã€UCF101å’ŒHMDB51ç­‰ä¸»æµåŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼Œå¹¿æ³›çš„å®è¯ç ”ç©¶è¯æ˜ï¼ŒMantaä»å¤šä¸ªè§’åº¦æ˜¾è‘—æé«˜äº†é•¿å­åºåˆ—çš„FSARæ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07481v4">PDF</a> Accepted by AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå°‘æ•°é•œå¤´åŠ¨ä½œè¯†åˆ«çš„ç ”ç©¶èƒŒæ™¯ï¼Œæœ¬æ–‡ä¸»è¦é’ˆå¯¹ä¸»æµTransformeræ–¹æ³•åœ¨å»ºæ¨¡é•¿åºåˆ—è§†é¢‘æ—¶çš„é«˜è®¡ç®—å¤æ‚åº¦é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªåä¸ºMantaçš„æ–°æ¡†æ¶ã€‚Mantaé€šè¿‡å¼•å…¥Matryoshka Mambaå’Œæ··åˆå¯¹æ¯”å­¦ä¹ èŒƒå¼æ¥è§£å†³ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ã€‚Matryoshka Mambaè®¾è®¡æœ‰å¤šä¸ªå†…éƒ¨æ¨¡å—æ¥å¢å¼ºå±€éƒ¨ç‰¹å¾è¡¨ç¤ºï¼Œè€Œéç›´æ¥å»ºæ¨¡å…¨å±€ç‰¹å¾ï¼ŒåŒæ—¶é€šè¿‡å¤–éƒ¨æ¨¡å—æ•æ‰è¿™äº›å±€éƒ¨ç‰¹å¾çš„æ—¶é—´çº¿ä¾èµ–æ€§ä»¥å®ç°éšå¼æ—¶é—´å¯¹é½ã€‚æ··åˆå¯¹æ¯”å­¦ä¹ èŒƒå¼ç»“åˆäº†ç›‘ç£å’Œæ— ç›‘ç£æ–¹æ³•ï¼Œä»¥å‡è½»åŒä¸€ç±»åˆ«å†…æ–¹å·®ç§¯ç´¯å¯¹å°‘æ•°é•œå¤´åŠ¨ä½œè¯†åˆ«æ€§èƒ½çš„è´Ÿé¢å½±å“ã€‚Mantaåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•é›†ä¸Šå®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å°‘æ•°é•œå¤´åŠ¨ä½œè¯†åˆ«ä¸­ï¼Œé•¿å­åºåˆ—è§†é¢‘æ›´æœ‰æ•ˆè¡¨è¾¾å®Œæ•´åŠ¨ä½œã€‚</li>
<li>ä¸»æµTransformeræ–¹æ³•åœ¨è®¡ç®—å¤æ‚åº¦ä¸Šå­˜åœ¨é—®é¢˜ï¼Œéš¾ä»¥åº”ç”¨äºé•¿å­åºåˆ—çš„å»ºæ¨¡ã€‚</li>
<li>Mambaè™½åœ¨å»ºæ¨¡é•¿åºåˆ—ä¸Šæœ‰æ•ˆç‡ï¼Œä½†å¿½ç•¥å±€éƒ¨ç‰¹å¾å»ºæ¨¡å’Œå¯¹é½çš„é‡è¦æ€§ã€‚</li>
<li>åŒç±»é•¿å­åºåˆ—çš„ç´¯ç§¯ä¼šé€ æˆç±»å†…æ–¹å·®ï¼Œå½±å“è¯†åˆ«æ€§èƒ½ã€‚</li>
<li>Matryoshka Mambaé€šè¿‡å¼•å…¥å¤šä¸ªå†…éƒ¨æ¨¡å—å¢å¼ºå±€éƒ¨ç‰¹å¾è¡¨ç¤ºï¼Œå¤–éƒ¨æ¨¡å—å®ç°æ—¶é—´å¯¹é½ã€‚</li>
<li>æ··åˆå¯¹æ¯”å­¦ä¹ èŒƒå¼ç»“åˆç›‘ç£å’Œæ— ç›‘ç£æ–¹æ³•ï¼Œå‡è½»ç±»å†…æ–¹å·®ç§¯ç´¯çš„è´Ÿé¢å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.07481">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f48a716b41cddf407f9a3cbb4f6bdd2c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d16c2681d6549d6322cdf09a77cba898.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b7850902a9c3912646aa3a165d6fc9b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1d20f12b2ec1bc52f161cf0f0f664e5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d3efc2920012bc9e302b18d829345f0e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-27e490f47ca16f70e9a33df1c71fe152.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-34c6e325a9bcc51cecd7512552bf34d5.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="GraphCLIP-Enhancing-Transferability-in-Graph-Foundation-Models-for-Text-Attributed-Graphs"><a href="#GraphCLIP-Enhancing-Transferability-in-Graph-Foundation-Models-for-Text-Attributed-Graphs" class="headerlink" title="GraphCLIP: Enhancing Transferability in Graph Foundation Models for   Text-Attributed Graphs"></a>GraphCLIP: Enhancing Transferability in Graph Foundation Models for   Text-Attributed Graphs</h2><p><strong>Authors:Yun Zhu, Haizhou Shi, Xiaotang Wang, Yongchao Liu, Yaoke Wang, Boci Peng, Chuntao Hong, Siliang Tang</strong></p>
<p>Recently, research on Text-Attributed Graphs (TAGs) has gained significant attention due to the prevalence of free-text node features in real-world applications and the advancements in Large Language Models (LLMs) that bolster TAG methodologies. However, current TAG approaches face two primary challenges: (i) Heavy reliance on label information and (ii) Limited cross-domain zero&#x2F;few-shot transferability. These issues constrain the scaling of both data and model size, owing to high labor costs and scaling laws, complicating the development of graph foundation models with strong transferability. In this work, we propose the GraphCLIP framework to address these challenges by learning graph foundation models with strong cross-domain zero&#x2F;few-shot transferability through a self-supervised contrastive graph-summary pretraining method. Specifically, we generate and curate large-scale graph-summary pair data with the assistance of LLMs, and introduce a novel graph-summary pretraining method, combined with invariant learning, to enhance graph foundation models with strong cross-domain zero-shot transferability. For few-shot learning, we propose a novel graph prompt tuning technique aligned with our pretraining objective to mitigate catastrophic forgetting and minimize learning costs. Extensive experiments show the superiority of GraphCLIP in both zero-shot and few-shot settings, while evaluations across various downstream tasks confirm the versatility of GraphCLIP. Our code is available at: <a target="_blank" rel="noopener" href="https://github.com/ZhuYun97/GraphCLIP">https://github.com/ZhuYun97/GraphCLIP</a> </p>
<blockquote>
<p>è¿‘æœŸï¼Œå…³äºæ–‡æœ¬å±æ€§å›¾ï¼ˆTAGsï¼‰çš„ç ”ç©¶å—åˆ°äº†å¹¿æ³›å…³æ³¨ï¼Œè¿™æ˜¯ç”±äºç°å®ä¸–ç•Œåº”ç”¨ä¸­è‡ªç”±æ–‡æœ¬èŠ‚ç‚¹ç‰¹å¾çš„æ™®åŠä»¥åŠæ”¯æŒTAGæ–¹æ³•çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿›å±•ã€‚ç„¶è€Œï¼Œå½“å‰çš„TAGæ–¹æ³•é¢ä¸´ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼š(i)ä¸¥é‡ä¾èµ–æ ‡ç­¾ä¿¡æ¯ï¼›(ii)è·¨åŸŸé›¶&#x2F;å°‘æ ·æœ¬è¿ç§»èƒ½åŠ›æœ‰é™ã€‚è¿™äº›é—®é¢˜ç”±äºé«˜æ˜‚çš„äººå·¥æˆæœ¬å’Œè§„æ¨¡æ‰©å±•å®šå¾‹è€Œé™åˆ¶äº†æ•°æ®å’Œæ¨¡å‹è§„æ¨¡çš„æ‰©å±•ï¼Œä»è€ŒåŠ å‰§äº†å…·æœ‰å¼ºå¤§è¿ç§»èƒ½åŠ›çš„å›¾åŸºç¡€æ¨¡å‹çš„å¼€å‘éš¾åº¦ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºGraphCLIPæ¡†æ¶æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œé€šè¿‡è‡ªç›‘ç£å¯¹æ¯”å›¾æ‘˜è¦é¢„è®­ç»ƒæ–¹æ³•æ¥å­¦ä¹ å…·æœ‰å¼ºå¤§è·¨åŸŸé›¶&#x2F;å°‘æ ·æœ¬è¿ç§»èƒ½åŠ›çš„å›¾åŸºç¡€æ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å€ŸåŠ©LLMsç”Ÿæˆå’Œç²¾é€‰å¤§è§„æ¨¡å›¾æ‘˜è¦é…å¯¹æ•°æ®ï¼Œå¹¶å¼•å…¥ä¸€ç§æ–°å‹çš„å›¾æ‘˜è¦é¢„è®­ç»ƒæ–¹æ³•ä¸ä¸å˜å­¦ä¹ ç›¸ç»“åˆï¼Œä»¥å¢å¼ºå›¾åŸºç¡€æ¨¡å‹çš„è·¨åŸŸé›¶æ ·æœ¬è¿ç§»èƒ½åŠ›ã€‚å¯¹äºå°‘æ ·æœ¬å­¦ä¹ ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸é¢„è®­ç»ƒç›®æ ‡å¯¹é½çš„å›¾æç¤ºè°ƒæ•´æŠ€æœ¯ï¼Œä»¥å‡è½»ç¾éš¾æ€§é—å¿˜å¹¶æœ€å°åŒ–å­¦ä¹ æˆæœ¬ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒGraphCLIPåœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬è®¾ç½®ä¸­éƒ½è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œè€Œåœ¨å„ç§ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„è¯„ä¼°åˆ™è¯å®äº†GraphCLIPçš„é€šç”¨æ€§ã€‚æˆ‘ä»¬çš„ä»£ç å¯ç”¨åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/ZhuYun97/GraphCLIP">https://github.com/ZhuYun97/GraphCLIP</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.10329v4">PDF</a> Accepted to WWWâ€™25</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹æ–‡æœ¬å±æ€§å›¾ï¼ˆTAGsï¼‰çš„ç ”ç©¶è¿›å±•åŠå…¶é¢ä¸´çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å¯¹æ•°æ®æ ‡ç­¾çš„è¿‡åº¦ä¾èµ–å’Œè·¨åŸŸé›¶&#x2F;å°‘æ ·æœ¬è¿ç§»èƒ½åŠ›æœ‰é™çš„é—®é¢˜ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæå‡ºäº†GraphCLIPæ¡†æ¶ï¼Œé€šè¿‡è‡ªç›‘ç£å¯¹æ¯”å›¾æ‘˜è¦é¢„è®­ç»ƒæ–¹æ³•ï¼Œå­¦ä¹ å…·æœ‰å¼ºå¤§è·¨åŸŸé›¶&#x2F;å°‘æ ·æœ¬è¿ç§»èƒ½åŠ›çš„å›¾åŸºç¡€æ¨¡å‹ã€‚åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆå’Œç­›é€‰å¤§è§„æ¨¡å›¾æ‘˜è¦å¯¹æ•°æ®ï¼Œç»“åˆä¸å˜å­¦ä¹ ï¼Œå¢å¼ºæ¨¡å‹çš„é›¶æ ·æœ¬è¿ç§»èƒ½åŠ›ã€‚é’ˆå¯¹å°‘æ ·æœ¬å­¦ä¹ ï¼Œæå‡ºäº†ä¸€ç§ä¸é¢„è®­ç»ƒç›®æ ‡å¯¹é½çš„å›¾æç¤ºè°ƒæ•´æŠ€æœ¯ï¼Œä»¥å‡è½»ç¾éš¾æ€§é—å¿˜å¹¶å‡å°‘å­¦ä¹ æˆæœ¬ã€‚å®éªŒè¯æ˜GraphCLIPåœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬è®¾ç½®ä¸­çš„ä¼˜è¶Šæ€§ï¼Œå¹¶åœ¨å„ç§ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„è¯„ä¼°ç¡®è®¤äº†å…¶é€šç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Text-Attributed Graphs (TAGs)ç ”ç©¶å—åˆ°å…³æ³¨ï¼Œä½†é¢ä¸´ä¾èµ–æ ‡ç­¾ä¿¡æ¯å’Œè·¨åŸŸè¿ç§»èƒ½åŠ›æœ‰é™ä¸¤å¤§æŒ‘æˆ˜ã€‚</li>
<li>GraphCLIPæ¡†æ¶é€šè¿‡è‡ªç›‘ç£å¯¹æ¯”å›¾æ‘˜è¦é¢„è®­ç»ƒæ–¹æ³•è§£å†³è¿™äº›é—®é¢˜ï¼Œæé«˜æ¨¡å‹çš„è¿ç§»èƒ½åŠ›ã€‚</li>
<li>åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆå’Œç­›é€‰å¤§è§„æ¨¡å›¾æ‘˜è¦å¯¹æ•°æ®ï¼Œå¢å¼ºæ¨¡å‹çš„é›¶æ ·æœ¬è¿ç§»èƒ½åŠ›ã€‚</li>
<li>GraphCLIPç»“åˆä¸å˜å­¦ä¹ æ¥æå‡æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>é’ˆå¯¹å°‘æ ·æœ¬å­¦ä¹ ï¼ŒGraphCLIPæå‡ºå›¾æç¤ºè°ƒæ•´æŠ€æœ¯ï¼Œå‡è½»ç¾éš¾æ€§é—å¿˜å¹¶å‡å°‘å­¦ä¹ æˆæœ¬ã€‚</li>
<li>å®éªŒè¯æ˜GraphCLIPåœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬è®¾ç½®ä¸­çš„ä¼˜è¶Šæ€§ã€‚</li>
<li>GraphCLIPåœ¨å„ç§ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„è¯„ä¼°ç»“æœè¯æ˜äº†å…¶é€šç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.10329">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-cbb56d9d09ff746dcafcc515c85501e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-90565eb4fc93a92d404aabde5e0df3d5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1862f11ef721f1779d45322857343a25.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-78935b866551015645b23cf4d495d827.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-42438623bb4111bbe55f85ea65885596.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Packet-Inspection-Transformer-A-Self-Supervised-Journey-to-Unseen-Malware-Detection-with-Few-Samples"><a href="#Packet-Inspection-Transformer-A-Self-Supervised-Journey-to-Unseen-Malware-Detection-with-Few-Samples" class="headerlink" title="Packet Inspection Transformer: A Self-Supervised Journey to Unseen   Malware Detection with Few Samples"></a>Packet Inspection Transformer: A Self-Supervised Journey to Unseen   Malware Detection with Few Samples</h2><p><strong>Authors:Kyle Stein, Arash Mahyari, Guillermo Francia III, Eman El-Sheikh</strong></p>
<p>As networks continue to expand and become more interconnected, the need for novel malware detection methods becomes more pronounced. Traditional security measures are increasingly inadequate against the sophistication of modern cyber attacks. Deep Packet Inspection (DPI) has been pivotal in enhancing network security, offering an in-depth analysis of network traffic that surpasses conventional monitoring techniques. DPI not only examines the metadata of network packets, but also dives into the actual content being carried within the packet payloads, providing a comprehensive view of the data flowing through networks. While the integration of advanced deep learning techniques with DPI has introduced modern methodologies into malware detection and network traffic classification, state-of-the-art supervised learning approaches are limited by their reliance on large amounts of annotated data and their inability to generalize to novel, unseen malware threats. To address these limitations, this paper leverages the recent advancements in self-supervised learning (SSL) and few-shot learning (FSL). Our proposed self-supervised approach trains a transformer via SSL to learn the embedding of packet content, including payload, from vast amounts of unlabeled data by masking portions of packets, leading to a learned representation that generalizes to various downstream tasks. Once the representation is extracted from the packets, they are used to train a malware detection algorithm. The representation obtained from the transformer is then used to adapt the malware detector to novel types of attacks using few-shot learning approaches. Our experimental results demonstrate that our method achieves classification accuracies of up to 94.76% on the UNSW-NB15 dataset and 83.25% on the CIC-IoT23 dataset. </p>
<blockquote>
<p>éšç€ç½‘ç»œçš„ä¸æ–­æ‰©å¼ å’Œç›¸äº’è¿æ¥ï¼Œå¯¹æ–°å‹æ¶æ„è½¯ä»¶æ£€æµ‹æ–¹æ³•çš„éœ€è¦å˜å¾—æ›´åŠ è¿«åˆ‡ã€‚ä¼ ç»Ÿçš„å®‰å…¨æªæ–½è¶Šæ¥è¶Šéš¾ä»¥åº”å¯¹ç°ä»£ç½‘ç»œæ”»å‡»çš„å¤æ‚æ€§ã€‚æ·±åº¦åŒ…æ£€æµ‹ï¼ˆDPIï¼‰åœ¨å¢å¼ºç½‘ç»œå®‰å…¨æ–¹é¢å‘æŒ¥äº†å…³é”®ä½œç”¨ï¼Œæä¾›äº†ä¸€ç§å¯¹ç½‘ç»œæµé‡è¿›è¡Œæ·±å…¥åˆ†æçš„æŠ€æœ¯ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿç›‘æ§æŠ€æœ¯ã€‚DPIä¸ä»…æ£€æŸ¥ç½‘ç»œåŒ…çš„å…ƒæ•°æ®ï¼Œè¿˜æ·±å…¥ç ”ç©¶åŒ…æœ‰æ•ˆè´Ÿè½½ä¸­çš„å®é™…å†…å®¹ï¼Œæä¾›äº†é€šè¿‡ç½‘ç»œæµåŠ¨çš„æ•°æ®çš„å…¨é¢è§†å›¾ã€‚è™½ç„¶å°†å…ˆè¿›çš„æ·±åº¦å­¦ä¹ æŠ€æœ¯ä¸DPIç›¸ç»“åˆå·²ç»ä¸ºæ¶æ„è½¯ä»¶æ£€æµ‹å’Œç½‘ç»œæµé‡åˆ†ç±»å¼•å…¥äº†ç°ä»£æ–¹æ³•ï¼Œä½†æœ€å…ˆè¿›çš„ç›‘ç£å­¦ä¹ æ–¹æ³•å—é™äºå®ƒä»¬å¯¹å¤§é‡æ³¨é‡Šæ•°æ®çš„ä¾èµ–ä»¥åŠå®ƒä»¬å¯¹æ–°å‹æœªè§æ¶æ„å¨èƒçš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡åˆ©ç”¨è‡ªæˆ‘ç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰å’Œå°‘é•œå¤´å­¦ä¹ ï¼ˆFSLï¼‰çš„æœ€æ–°è¿›å±•ã€‚æˆ‘ä»¬æå‡ºçš„è‡ªæˆ‘ç›‘ç£æ–¹æ³•é€šè¿‡SSLè®­ç»ƒä¸€ä¸ªå˜å‹å™¨ï¼Œå­¦ä¹ åŒ…æ‹¬æœ‰æ•ˆè´Ÿè½½åœ¨å†…çš„æ•°æ®åŒ…å†…å®¹çš„åµŒå…¥ï¼Œé€šè¿‡æ©ç›–æ•°æ®åŒ…çš„éƒ¨åˆ†å†…å®¹ï¼Œä»å¤§é‡æ— æ ‡ç­¾æ•°æ®ä¸­å­¦ä¹ è¡¨ç¤ºï¼Œè¿™æœ‰åŠ©äºé€‚åº”å„ç§ä¸‹æ¸¸ä»»åŠ¡ã€‚ä¸€æ—¦ä»æ•°æ®åŒ…ä¸­æå–å‡ºè¡¨ç¤ºå½¢å¼ï¼Œå®ƒä»¬å°±è¢«ç”¨æ¥è®­ç»ƒæ¶æ„è½¯ä»¶æ£€æµ‹ç®—æ³•ã€‚ä»å˜å‹å™¨è·å¾—çš„è¡¨ç¤ºå½¢å¼ç„¶åè¢«ç”¨äºåˆ©ç”¨å°‘é•œå¤´å­¦ä¹ æ–¹æ³•é€‚åº”æ–°å‹æ”»å‡»ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨UNSW-NB15æ•°æ®é›†ä¸Šè¾¾åˆ°äº†é«˜è¾¾94.76%çš„åˆ†ç±»ç²¾åº¦ï¼Œåœ¨CIC-IoT23æ•°æ®é›†ä¸Šè¾¾åˆ°äº†83.25%çš„åˆ†ç±»ç²¾åº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.18219v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼šéšç€ç½‘ç»œä¸æ–­æ‰©å±•å’Œç›¸äº’è¿æ¥ï¼Œæ–°å‹æ¶æ„è½¯ä»¶æ£€æµ‹æ–¹æ³•çš„å¿…è¦æ€§æ›´åŠ çªå‡ºã€‚æ·±åº¦åŒ…æ£€æµ‹ï¼ˆDPIï¼‰å¯¹äºå¢å¼ºç½‘ç»œå®‰å…¨æ€§è‡³å…³é‡è¦ï¼Œå®ƒæä¾›äº†å¯¹ç½‘ç»œæµé‡çš„æ·±å…¥åˆ†æï¼Œè¶…è¶Šäº†ä¼ ç»Ÿç›‘æ§æŠ€æœ¯ã€‚æœ¬æ–‡åˆ©ç”¨è‡ªæˆ‘ç›‘ç£å­¦ä¹ å’Œå°‘æ ·æœ¬å­¦ä¹ çš„æœ€æ–°è¿›å±•ï¼Œé€šè¿‡è‡ªæˆ‘ç›‘ç£æ–¹æ³•è®­ç»ƒä¸€ä¸ªå˜å‹å™¨æ¥å­¦ä¹ æ•°æ®åŒ…å†…å®¹çš„åµŒå…¥ï¼ŒåŒ…æ‹¬è´Ÿè½½ï¼Œé€šè¿‡æ©ç›–éƒ¨åˆ†æ•°æ®åŒ…ä»å¤§é‡æœªæ ‡è®°çš„æ•°æ®ä¸­å­¦ä¹ è¡¨ç¤ºï¼Œç„¶åå°†è¯¥è¡¨ç¤ºç”¨äºè®­ç»ƒæ¶æ„è½¯ä»¶æ£€æµ‹ç®—æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨UNSW-NB15æ•°æ®é›†ä¸Šè¾¾åˆ°äº†94.76%çš„åˆ†ç±»å‡†ç¡®ç‡ï¼Œåœ¨CIC-IoT23æ•°æ®é›†ä¸Šè¾¾åˆ°äº†83.25%çš„åˆ†ç±»å‡†ç¡®ç‡ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ç½‘ç»œäº’è”æ€§çš„å¢å¼ºå‡¸æ˜¾äº†æ–°å‹æ¶æ„è½¯ä»¶æ£€æµ‹æ–¹æ³•çš„å¿…è¦æ€§ã€‚</li>
<li>ä¼ ç»Ÿå®‰å…¨æªæ–½å¯¹ç°ä»£ç½‘ç»œæ”»å‡»çš„å¤æ‚æ€§è¶Šæ¥è¶Šä¸è¶³å¤Ÿåº”å¯¹ã€‚</li>
<li>æ·±åº¦åŒ…æ£€æµ‹ï¼ˆDPIï¼‰æ˜¯ä¸€ç§å¢å¼ºç½‘ç»œå®‰å…¨çš„å…³é”®æŠ€æœ¯ï¼Œèƒ½å¤Ÿæ·±å…¥åˆ†æç½‘ç»œæµé‡ã€‚</li>
<li>DPIä¸ä»…åˆ†æç½‘ç»œåŒ…çš„å…ƒæ•°æ®ï¼Œè¿˜æ·±å…¥å®é™…å†…å®¹ï¼Œæä¾›å…¨é¢çš„ç½‘ç»œæ•°æ®æµè§†å›¾ã€‚</li>
<li>æ·±åº¦å­¦ä¹ å’ŒDPIçš„ç»“åˆä¸ºæ¶æ„è½¯ä»¶æ£€æµ‹å’Œæµé‡åˆ†ç±»æä¾›äº†ç°ä»£æ–¹æ³•ã€‚</li>
<li>å½“å‰æœ€å…ˆè¿›çš„ç›‘ç£å­¦ä¹ æ–¹æ³•å—é™äºéœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®å’Œæ— æ³•æ³›åŒ–åˆ°æ–°å‹å¨èƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.18219">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bcc83883366eddcde9bbfb1aa68721fc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e301b46dc5830d0b3d043546d5749519.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e27fded5d5e75cf8c2f24ad1970dbf4c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-10498d334da71e691e186b5e291bc831.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a39e5d724d5b613f597cc5b287c65915.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Iterative-Repair-with-Weak-Verifiers-for-Few-shot-Transfer-in-KBQA-with-Unanswerability"><a href="#Iterative-Repair-with-Weak-Verifiers-for-Few-shot-Transfer-in-KBQA-with-Unanswerability" class="headerlink" title="Iterative Repair with Weak Verifiers for Few-shot Transfer in KBQA with   Unanswerability"></a>Iterative Repair with Weak Verifiers for Few-shot Transfer in KBQA with   Unanswerability</h2><p><strong>Authors:Riya Sawhney, Samrat Yadav, Indrajit Bhattacharya,  Mausam</strong></p>
<p>Real-world applications of KBQA require models to handle unanswerable questions with a limited volume of in-domain labeled training data. We propose the novel task of few-shot transfer for KBQA with unanswerable questions and contribute two new datasets for performance evaluation. We present FUn-FuSIC - a novel solution for our task that extends FuSIC KBQA, the state-of-the-art few-shot transfer model for answerable-only KBQA. We first note that FuSIC-KBQAâ€™s iterative repair makes a strong assumption that all questions are unanswerable. As a remedy, we propose Feedback for Unanswerability (FUn), which uses iterative repair using feedback from a suite of strong and weak verifiers, and an adaptation of self consistency for unanswerabilty to better assess the answerability of a question. Our experiments show that FUn-FuSIC significantly outperforms suitable adaptations of multiple LLM based and supervised SoTA models on our task, while establishing a new SoTA for answerable few-shot transfer as well. </p>
<blockquote>
<p>ç°å®ä¸–ç•Œä¸­çš„KBQAåº”ç”¨è¦æ±‚æ¨¡å‹èƒ½å¤Ÿå¤„ç†æ— æ³•å›ç­”çš„é—®é¢˜ï¼Œå¹¶ä¸”åœ¨é¢†åŸŸå†…éƒ¨åªæœ‰æœ‰é™é‡çš„æ ‡è®°è®­ç»ƒæ•°æ®ã€‚æˆ‘ä»¬é’ˆå¯¹æ— æ³•å›ç­”çš„KBQAæå‡ºäº†æ–°é¢–çš„å°æ ·æœ¬è¿ç§»ä»»åŠ¡ï¼Œå¹¶ä¸ºæ€§èƒ½è¯„ä¼°è´¡çŒ®äº†ä¸¤ä¸ªæ–°æ•°æ®é›†ã€‚æˆ‘ä»¬ä»‹ç»äº†FUN-FuSICâ€”â€”ä¸€ç§é’ˆå¯¹æˆ‘ä»¬ä»»åŠ¡çš„æ–°å‹è§£å†³æ–¹æ¡ˆï¼Œå®ƒæ‰©å±•äº†FuSIC KBQAï¼ˆé’ˆå¯¹ä»…å¯å›ç­”KBQAçš„æœ€å…ˆè¿›çš„å°æ ·æœ¬è¿ç§»æ¨¡å‹ï¼‰ã€‚æˆ‘ä»¬é¦–å…ˆæ³¨æ„åˆ°ï¼ŒFuSIC-KBQAçš„è¿­ä»£ä¿®å¤å‡è®¾æ‰€æœ‰é—®é¢˜éƒ½æ˜¯æ— æ³•å›ç­”çš„ï¼Œè¿™æ˜¯ä¸€ä¸ªå¾ˆå¼ºçš„å‡è®¾ã€‚ä½œä¸ºè¡¥æ•‘æªæ–½ï¼Œæˆ‘ä»¬æå‡ºäº†é’ˆå¯¹æ— æ³•å›ç­”çš„é—®é¢˜çš„åé¦ˆï¼ˆFUNï¼‰ï¼Œå®ƒä½¿ç”¨æ¥è‡ªä¸€ç³»åˆ—å¼ºéªŒè¯å™¨å’Œå¼±éªŒè¯å™¨çš„åé¦ˆè¿›è¡Œè¿­ä»£ä¿®å¤ï¼Œå¹¶è‡ªé€‚åº”åœ°è°ƒæ•´è‡ªæˆ‘ä¸€è‡´æ€§ä»¥æ›´å¥½åœ°è¯„ä¼°é—®é¢˜çš„å¯å›ç­”æ€§ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒFUN-FuSICåœ¨æˆ‘ä»¬çš„ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºå¤šä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹å’Œç›‘ç£çš„å½“å‰æœ€ä½³æ¨¡å‹ï¼ˆSoTAï¼‰çš„é€‚å½“æ”¹ç¼–ç‰ˆæœ¬ï¼ŒåŒæ—¶ä¸ºæˆ‘ä»¬å»ºç«‹äº†æ–°çš„å¯å›ç­”çš„å°æ ·æœ¬è¿ç§»çš„å½“å‰æœ€ä½³æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.14313v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>é’ˆå¯¹KBQAï¼ˆçŸ¥è¯†åº“é—®ç­”ï¼‰çš„å®é™…åº”ç”¨ï¼Œéœ€è¦æ¨¡å‹å¤„ç†ä¸å¯å›ç­”çš„é—®é¢˜ï¼Œå¹¶åœ¨æœ‰é™çš„é¢†åŸŸå†…æ ‡ç­¾è®­ç»ƒæ•°æ®ä¸‹å·¥ä½œã€‚æœ¬æ–‡æå‡ºäº†é’ˆå¯¹KBQAä¸å¯å›ç­”é—®é¢˜è¿›è¡Œå°‘æ ·æœ¬è¿ç§»çš„æ–°ä»»åŠ¡ï¼Œå¹¶è´¡çŒ®äº†ä¸¤ä¸ªæ–°çš„æ•°æ®é›†ç”¨äºæ€§èƒ½è¯„ä¼°ã€‚æˆ‘ä»¬æå‡ºäº†FUN-FuSICçš„æ–°è§£å†³æ–¹æ¡ˆï¼Œå®ƒæ˜¯FuSIC KBQAçš„æ‰©å±•ï¼Œé€‚ç”¨äºä»…å›ç­”é—®é¢˜çš„å°‘æ ·æœ¬è¿ç§»æ¨¡å‹ã€‚æˆ‘ä»¬å‘ç°FuSIC-KBQAçš„è¿­ä»£ä¿®å¤å‡è®¾æ‰€æœ‰é—®é¢˜éƒ½æ˜¯ä¸å¯å›ç­”çš„ï¼Œè¿™å­˜åœ¨å±€é™æ€§ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†é’ˆå¯¹ä¸å¯å›ç­”æ€§çš„åé¦ˆï¼ˆFUNï¼‰ï¼Œåˆ©ç”¨ä¸€ç³»åˆ—å¼ºéªŒè¯å™¨å’Œå¼±éªŒè¯å™¨çš„åé¦ˆè¿›è¡Œè¿­ä»£ä¿®å¤ï¼Œå¹¶è‡ªé€‚åº”åœ°è°ƒæ•´è‡ªæˆ‘ä¸€è‡´æ€§ä»¥æ›´å¥½åœ°è¯„ä¼°é—®é¢˜çš„å¯å›ç­”æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒFUN-FuSICåœ¨æˆ‘ä»¬çš„ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºå¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå½“å‰æœ€ä½³ç›‘ç£æ¨¡å‹çš„é€‚åº”æ€§ç‰ˆæœ¬ï¼ŒåŒæ—¶ä¸ºå¯å›ç­”é—®é¢˜çš„å°‘æ ·æœ¬è¿ç§»å»ºç«‹äº†æ–°çš„æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>é’ˆå¯¹KBQAçš„ä¸å¯å›ç­”é—®é¢˜ï¼Œæå‡ºäº†å°‘æ ·æœ¬è¿ç§»çš„æ–°ä»»åŠ¡ã€‚</li>
<li>è´¡çŒ®äº†ä¸¤ä¸ªæ–°çš„æ•°æ®é›†ç”¨äºè¯„ä¼°è¯¥ä»»åŠ¡çš„æ€§èƒ½ã€‚</li>
<li>æ‰©å±•äº†FuSIC KBQAæ¨¡å‹ï¼Œæå‡ºäº†FUN-FuSICè§£å†³æ–¹æ¡ˆæ¥å¤„ç†ä¸å¯å›ç­”é—®é¢˜çš„å°‘æ ·æœ¬è¿ç§»ã€‚</li>
<li>æŒ‡å‡ºFuSIC-KBQAçš„è¿­ä»£ä¿®å¤å‡è®¾å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>æå‡ºäº†é’ˆå¯¹ä¸å¯å›ç­”æ€§çš„åé¦ˆï¼ˆFUNï¼‰ï¼Œé€šè¿‡åˆ©ç”¨ä¸€ç³»åˆ—å¼ºéªŒè¯å™¨å’Œå¼±éªŒè¯å™¨çš„åé¦ˆè¿›è¡Œè¿­ä»£ä¿®å¤ã€‚</li>
<li>é€šè¿‡å®éªŒéªŒè¯äº†FUN-FuSICåœ¨ä»»åŠ¡ä¸Šçš„ä¼˜è¶Šæ€§ï¼ŒåŒæ—¶å»ºç«‹äº†æ–°çš„å°‘æ ·æœ¬è¿ç§»æœ€ä½³æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.14313">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6f6371182ee979f72bfebbaa5a27a0bb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3ea931b769c8450292c10b6a261bc740.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-602ed04ed957e020bcb077c9d4837d8e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36f0c1e09c8f11db10b36ef76a86da8c.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Cross-domain-Multi-modal-Few-shot-Object-Detection-via-Rich-Text"><a href="#Cross-domain-Multi-modal-Few-shot-Object-Detection-via-Rich-Text" class="headerlink" title="Cross-domain Multi-modal Few-shot Object Detection via Rich Text"></a>Cross-domain Multi-modal Few-shot Object Detection via Rich Text</h2><p><strong>Authors:Zeyu Shangguan, Daniel Seita, Mohammad Rostami</strong></p>
<p>Cross-modal feature extraction and integration have led to steady performance improvements in few-shot learning tasks due to generating richer features. However, existing multi-modal object detection (MM-OD) methods degrade when facing significant domain-shift and are sample insufficient. We hypothesize that rich text information could more effectively help the model to build a knowledge relationship between the vision instance and its language description and can help mitigate domain shift. Specifically, we study the Cross-Domain few-shot generalization of MM-OD (CDMM-FSOD) and propose a meta-learning based multi-modal few-shot object detection method that utilizes rich text semantic information as an auxiliary modality to achieve domain adaptation in the context of FSOD. Our proposed network contains (i) a multi-modal feature aggregation module that aligns the vision and language support feature embeddings and (ii) a rich text semantic rectify module that utilizes bidirectional text feature generation to reinforce multi-modal feature alignment and thus to enhance the modelâ€™s language understanding capability. We evaluate our model on common standard cross-domain object detection datasets and demonstrate that our approach considerably outperforms existing FSOD methods. </p>
<blockquote>
<p>è·¨æ¨¡æ€ç‰¹å¾æå–å’Œèåˆç”±äºç”Ÿæˆäº†æ›´ä¸°å¯Œçš„ç‰¹å¾ï¼Œåœ¨å°‘æ ·æœ¬å­¦ä¹ ä»»åŠ¡ä¸­å¸¦æ¥äº†ç¨³å®šçš„æ€§èƒ½æå‡ã€‚ç„¶è€Œï¼Œå½“é¢ä¸´æ˜¾è‘—çš„é¢†åŸŸè¿ç§»å’Œæ ·æœ¬ä¸è¶³æ—¶ï¼Œç°æœ‰çš„å¤šæ¨¡æ€ç›®æ ‡æ£€æµ‹ï¼ˆMM-ODï¼‰æ–¹æ³•ä¼šæ€§èƒ½ä¸‹é™ã€‚æˆ‘ä»¬å‡è®¾ä¸°å¯Œçš„æ–‡æœ¬ä¿¡æ¯å¯ä»¥æ›´æœ‰æ•ˆåœ°å¸®åŠ©æ¨¡å‹å»ºç«‹è§†è§‰å®ä¾‹ä¸å…¶è¯­è¨€æè¿°ä¹‹é—´çš„çŸ¥è¯†å…³ç³»ï¼Œå¹¶æœ‰åŠ©äºç¼“è§£é¢†åŸŸè¿ç§»ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ç ”ç©¶äº†è·¨åŸŸå°‘æ ·æœ¬æ³›åŒ–çš„MM-ODï¼ˆCDMM-FSODï¼‰ï¼Œå¹¶æå‡ºäº†ä¸€ç§åŸºäºå…ƒå­¦ä¹ çš„å¤šæ¨¡æ€å°‘æ ·æœ¬ç›®æ ‡æ£€æµ‹æ–¹æ³•ï¼Œåˆ©ç”¨ä¸°å¯Œçš„æ–‡æœ¬è¯­ä¹‰ä¿¡æ¯ä½œä¸ºè¾…åŠ©æ¨¡æ€æ¥å®ç°FSODé¢†åŸŸçš„è‡ªé€‚åº”ã€‚æˆ‘ä»¬æå‡ºçš„ç½‘ç»œåŒ…å«ï¼ˆiï¼‰ä¸€ç§å¤šæ¨¡æ€ç‰¹å¾èšåˆæ¨¡å—ï¼Œç”¨äºå¯¹é½è§†è§‰å’Œè¯­è¨€æ”¯æŒç‰¹å¾åµŒå…¥ï¼›ï¼ˆiiï¼‰ä¸€ç§ä¸°å¯Œçš„æ–‡æœ¬è¯­ä¹‰æ ¡æ­£æ¨¡å—ï¼Œåˆ©ç”¨åŒå‘æ–‡æœ¬ç‰¹å¾ç”Ÿæˆæ¥åŠ å¼ºå¤šæ¨¡æ€ç‰¹å¾å¯¹é½ï¼Œä»è€Œæé«˜æ¨¡å‹çš„è¯­è¨€ç†è§£èƒ½åŠ›å’Œè·¨é¢†åŸŸé€‚åº”èƒ½åŠ›ã€‚æˆ‘ä»¬åœ¨å¸¸è§çš„æ ‡å‡†è·¨åŸŸç›®æ ‡æ£€æµ‹æ•°æ®é›†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ¨¡å‹ï¼Œç»“æœè¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰çš„FSODæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.16188v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†è·¨æ¨¡æ€ç‰¹å¾æå–ä¸èåˆåœ¨å°‘æ ·æœ¬å­¦ä¹ ä»»åŠ¡ä¸­çš„æŒç»­æ€§èƒ½æå‡ï¼Œå¹¶æŒ‡å‡ºå¤šæ¨¡æ€ç›®æ ‡æ£€æµ‹ï¼ˆMM-ODï¼‰æ–¹æ³•åœ¨é¢ä¸´æ˜¾è‘—é¢†åŸŸåç§»å’Œæ ·æœ¬ä¸è¶³æ—¶çš„å±€é™æ€§ã€‚ç ”ç©¶å‡è®¾ä¸°å¯Œæ–‡æœ¬ä¿¡æ¯èƒ½æœ‰æ•ˆå¸®åŠ©æ¨¡å‹å»ºç«‹è§†è§‰å®ä¾‹ä¸è¯­è¨€æè¿°ä¹‹é—´çš„çŸ¥è¯†å…³ç³»ï¼Œå¹¶æœ‰åŠ©äºç¼“è§£é¢†åŸŸåç§»é—®é¢˜ã€‚æ–‡ç« ç ”ç©¶äº†è·¨åŸŸå°‘æ ·æœ¬æ³›åŒ–çš„å¤šæ¨¡æ€ç›®æ ‡æ£€æµ‹ï¼ˆCDMM-FSODï¼‰ï¼Œå¹¶æå‡ºä¸€ç§åŸºäºå…ƒå­¦ä¹ çš„å¤šæ¨¡æ€å°‘æ ·æœ¬ç›®æ ‡æ£€æµ‹æ–¹æ³•ï¼Œåˆ©ç”¨ä¸°å¯Œæ–‡æœ¬è¯­ä¹‰ä¿¡æ¯ä½œä¸ºè¾…åŠ©æ¨¡æ€æ¥å®ç°é¢†åŸŸé€‚åº”ï¼Œé’ˆå¯¹FSODã€‚æ‰€æç½‘ç»œåŒ…æ‹¬å¤šæ¨¡æ€ç‰¹å¾èšåˆæ¨¡å—å’Œä¸°å¯Œæ–‡æœ¬è¯­ä¹‰æ ¡æ­£æ¨¡å—ï¼Œå‰è€…å¯¹é½è§†è§‰å’Œè¯­è¨€æ”¯æŒç‰¹å¾åµŒå…¥ï¼Œåè€…åˆ©ç”¨åŒå‘æ–‡æœ¬ç‰¹å¾ç”Ÿæˆå¼ºåŒ–å¤šæ¨¡æ€ç‰¹å¾å¯¹é½ï¼Œæé«˜æ¨¡å‹çš„è¯­è¨€ç†è§£åŠ›ã€‚åœ¨è·¨åŸŸç›®æ ‡æ£€æµ‹æ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰FSODæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è·¨æ¨¡æ€ç‰¹å¾æå–ä¸èåˆåœ¨å°‘æ ·æœ¬å­¦ä¹ ä»»åŠ¡ä¸­æŒç»­æ¨åŠ¨æ€§èƒ½æå‡ã€‚</li>
<li>å¤šæ¨¡æ€ç›®æ ‡æ£€æµ‹ï¼ˆMM-ODï¼‰æ–¹æ³•åœ¨é¢ä¸´æ˜¾è‘—é¢†åŸŸåç§»å’Œæ ·æœ¬ä¸è¶³æ—¶æ€§èƒ½ä¸‹é™ã€‚</li>
<li>ä¸°å¯Œæ–‡æœ¬ä¿¡æ¯èƒ½æœ‰æ•ˆå¸®åŠ©æ¨¡å‹å»ºç«‹è§†è§‰å®ä¾‹ä¸è¯­è¨€æè¿°ä¹‹é—´çš„çŸ¥è¯†å…³ç³»ã€‚</li>
<li>æ–‡ç« ç ”ç©¶äº†è·¨åŸŸå°‘æ ·æœ¬æ³›åŒ–çš„å¤šæ¨¡æ€ç›®æ ‡æ£€æµ‹ï¼ˆCDMM-FSODï¼‰ã€‚</li>
<li>æå‡ºä¸€ç§åŸºäºå…ƒå­¦ä¹ çš„å¤šæ¨¡æ€å°‘æ ·æœ¬ç›®æ ‡æ£€æµ‹æ–¹æ³•ï¼Œç»“åˆä¸°å¯Œæ–‡æœ¬è¯­ä¹‰ä¿¡æ¯ä½œä¸ºè¾…åŠ©æ¨¡æ€å®ç°é¢†åŸŸé€‚åº”ã€‚</li>
<li>æ‰€æç½‘ç»œåŒ…æ‹¬å¤šæ¨¡æ€ç‰¹å¾èšåˆæ¨¡å—ï¼Œç”¨äºå¯¹é½è§†è§‰å’Œè¯­è¨€ç‰¹å¾åµŒå…¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.16188">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-85a4de71be3b941e9adfb228ec9e55f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-166e3a4b3a5aa523408277a6f8373cbc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ea5b9efa72ab4c621978060a74d66d5c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2f3049da544342f8ce39dc0e7e2d7b8d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8dc3d406b5b833b4d020643fbf9a2b95.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="PromptAid-Prompt-Exploration-Perturbation-Testing-and-Iteration-using-Visual-Analytics-for-Large-Language-Models"><a href="#PromptAid-Prompt-Exploration-Perturbation-Testing-and-Iteration-using-Visual-Analytics-for-Large-Language-Models" class="headerlink" title="PromptAid: Prompt Exploration, Perturbation, Testing and Iteration using   Visual Analytics for Large Language Models"></a>PromptAid: Prompt Exploration, Perturbation, Testing and Iteration using   Visual Analytics for Large Language Models</h2><p><strong>Authors:Aditi Mishra, Utkarsh Soni, Anjana Arunkumar, Jinbin Huang, Bum Chul Kwon, Chris Bryan</strong></p>
<p>Large Language Models (LLMs) have gained widespread popularity due to their ability to perform ad-hoc Natural Language Processing (NLP) tasks with a simple natural language prompt. Part of the appeal for LLMs is their approachability to the general public, including individuals with no prior technical experience in NLP techniques. However, natural language prompts can vary significantly in terms of their linguistic structure, context, and other semantics. Modifying one or more of these aspects can result in significant differences in task performance. Non-expert users may find it challenging to identify the changes needed to improve a prompt, especially when they lack domain-specific knowledge and lack appropriate feedback. To address this challenge, we present PromptAid, a visual analytics system designed to interactively create, refine, and test prompts through exploration, perturbation, testing, and iteration. PromptAid uses multiple, coordinated visualizations which allow users to improve prompts by using the three strategies: keyword perturbations, paraphrasing perturbations, and obtaining the best set of in-context few-shot examples. PromptAid was designed through an iterative prototyping process involving NLP experts and was evaluated through quantitative and qualitative assessments for LLMs. Our findings indicate that PromptAid helps users to iterate over prompt template alterations with less cognitive overhead, generate diverse prompts with help of recommendations, and analyze the performance of the generated prompts while surpassing existing state-of-the-art prompting interfaces in performance. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å› å…¶èƒ½å¤Ÿé€šè¿‡ç®€å•çš„è‡ªç„¶è¯­è¨€æç¤ºæ‰§è¡Œä¸“é¡¹è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»»åŠ¡è€Œå¹¿å—æ¬¢è¿ã€‚LLMçš„å¸å¼•åŠ›ä¹‹ä¸€åœ¨äºå®ƒä»¬å¯¹æ™®é€šå¤§ä¼—çš„äº²å’ŒåŠ›ï¼ŒåŒ…æ‹¬é‚£äº›æ²¡æœ‰å…ˆå‰NLPæŠ€æœ¯ç»éªŒçš„äººã€‚ç„¶è€Œï¼Œè‡ªç„¶è¯­è¨€çš„æç¤ºåœ¨å®ƒä»¬çš„è¯­è¨€ç»“æ„ã€ä¸Šä¸‹æ–‡å’Œå…¶ä»–è¯­ä¹‰æ–¹é¢å¯èƒ½å­˜åœ¨å¾ˆå¤§å·®å¼‚ã€‚ä¿®æ”¹è¿™äº›æ–¹é¢çš„ä¸€ä¸ªæˆ–å¤šä¸ªå› ç´ å¯èƒ½ä¼šå¯¼è‡´ä»»åŠ¡æ€§èƒ½çš„é‡å¤§å·®å¼‚ã€‚éä¸“ä¸šç”¨æˆ·å¯èƒ½ä¼šå‘ç°å¾ˆéš¾ç¡®å®šæ”¹è¿›æç¤ºæ‰€éœ€çš„æ›´æ”¹ï¼Œå°¤å…¶æ˜¯åœ¨ä»–ä»¬ç¼ºä¹ç‰¹å®šé¢†åŸŸçŸ¥è¯†å’Œé€‚å½“çš„åé¦ˆæ—¶ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†PromptAidï¼Œä¸€ä¸ªè§†è§‰åˆ†æç³»ç»Ÿï¼Œæ—¨åœ¨é€šè¿‡æ¢ç´¢ã€æ‰°åŠ¨ã€æµ‹è¯•å’Œè¿­ä»£æ¥äº¤äº’å¼åœ°åˆ›å»ºã€å®Œå–„å’Œè°ƒæ•´æç¤ºã€‚PromptAidä½¿ç”¨å¤šä¸ªåè°ƒçš„å¯è§†åŒ–æç¤ºï¼Œå…è®¸ç”¨æˆ·é€šè¿‡ä¸‰ç§ç­–ç•¥æ”¹è¿›æç¤ºï¼šå…³é”®è¯æ‰°åŠ¨ã€åŒä¹‰æ›¿æ¢æ‰°åŠ¨å’Œè·å–æœ€ä½³çš„ä¸Šä¸‹æ–‡å°‘é‡ç¤ºä¾‹ã€‚PromptAidçš„è®¾è®¡è¿‡ç¨‹æ˜¯é€šè¿‡æ¶‰åŠNLPä¸“å®¶çš„è¿­ä»£åŸå‹åˆ¶ä½œè¿‡ç¨‹å®Œæˆçš„ï¼Œå¹¶é€šè¿‡é’ˆå¯¹LLMçš„å®šé‡å’Œå®šæ€§è¯„ä¼°è¿›è¡Œäº†è¯„ä¼°ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒPromptAidæœ‰åŠ©äºç”¨æˆ·å‡å°‘è®¤çŸ¥è´Ÿæ‹…åœ°è¿­ä»£è°ƒæ•´æç¤ºæ¨¡æ¿ï¼Œå€ŸåŠ©æ¨èç”Ÿæˆå¤šæ ·åŒ–çš„æç¤ºï¼Œå¹¶åˆ†æç”Ÿæˆçš„æç¤ºçš„æ€§èƒ½ï¼ŒåŒæ—¶åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†ç°æœ‰çš„å…ˆè¿›æç¤ºç•Œé¢ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2304.01964v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡è‡ªç„¶è¯­è¨€æç¤ºæ‰§è¡Œå³å…´çš„è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»»åŠ¡çš„èƒ½åŠ›è€Œå¹¿å—æ¬¢è¿ã€‚LLMçš„å¸å¼•åŠ›åœ¨äºå®ƒå¯¹å…¬ä¼—çš„äº²å’ŒåŠ›ï¼ŒåŒ…æ‹¬æ²¡æœ‰NLPæŠ€æœ¯ç»éªŒçš„ä¸ªäººã€‚ç„¶è€Œï¼Œè‡ªç„¶è¯­è¨€çš„æç¤ºåœ¨è¯­è¨€è¡¨è¾¾å’Œç»“æ„ä¸Šæœ‰å¾ˆå¤§çš„å·®å¼‚ï¼Œå¯èƒ½å¯¼è‡´ä»»åŠ¡æ‰§è¡Œæ•ˆæœçš„ä¸åŒã€‚éä¸“ä¸šç”¨æˆ·å¯èƒ½éš¾ä»¥è¯†åˆ«éœ€è¦æ”¹è¿›çš„æç¤ºå˜åŒ–ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¼ºä¹ç‰¹å®šé¢†åŸŸçŸ¥è¯†å’Œé€‚å½“åé¦ˆçš„æƒ…å†µä¸‹ã€‚ä¸ºè§£å†³æ­¤æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†PromptAidï¼Œä¸€ä¸ªè§†è§‰åˆ†æç³»ç»Ÿï¼Œæ—¨åœ¨é€šè¿‡æ¢ç´¢ã€å¾®è°ƒã€æµ‹è¯•å’Œè¿­ä»£çš„æ–¹å¼ï¼Œä»¥äº¤äº’æ–¹å¼åˆ›å»ºå’Œç»†åŒ–æç¤ºã€‚PromptAidåˆ©ç”¨å¤šç§åè°ƒå¯è§†åŒ–å·¥å…·ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿé€šè¿‡ä¸‰ç§ç­–ç•¥æ”¹è¿›æç¤ºï¼šå…³é”®å­—æ‰°åŠ¨ã€åŒä¹‰æ›¿æ¢æ‰°åŠ¨å’Œè·å–æœ€ä½³çš„ä¸Šä¸‹æ–‡å°‘é‡ç¤ºä¾‹ã€‚PromptAidçš„è®¾è®¡ç»è¿‡äº†NLPä¸“å®¶çš„è¿­ä»£åŸå‹åˆ¶ä½œè¿‡ç¨‹ï¼Œå¹¶é€šè¿‡å®šé‡å’Œå®šæ€§è¯„ä¼°è¿›è¡ŒéªŒè¯ï¼Œè¡¨æ˜å…¶èƒ½å¸®åŠ©ç”¨æˆ·é€šè¿‡æ›´å°‘çš„è®¤çŸ¥è´Ÿæ‹…æ¥è¿­ä»£æç¤ºæ¨¡æ¿çš„ä¿®æ”¹ï¼Œå€ŸåŠ©æ¨èç”Ÿæˆå¤šæ ·åŒ–çš„æç¤ºå¹¶åˆ†æç”Ÿæˆæç¤ºçš„æ€§èƒ½è¡¨ç°ï¼Œè¶…è¶Šäº†ç°æœ‰çš„å…ˆè¿›æç¤ºç•Œé¢åœ¨æ€§èƒ½æ–¹é¢çš„è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”±äºå…¶ä½¿ç”¨è‡ªç„¶è¯­è¨€æç¤ºæ‰§è¡ŒNLPä»»åŠ¡çš„èƒ½åŠ›è€Œå—åˆ°å¹¿æ³›æ¬¢è¿ã€‚</li>
<li>è‡ªç„¶è¯­è¨€æç¤ºçš„å·®å¼‚ï¼ˆå¦‚è¯­è¨€ç»“æ„ã€ä¸Šä¸‹æ–‡å’Œè¯­ä¹‰ï¼‰å¯èƒ½å¯¼è‡´ä»»åŠ¡æ‰§è¡Œæ•ˆæœçš„æ˜¾è‘—ä¸åŒã€‚</li>
<li>éä¸“ä¸šç”¨æˆ·åœ¨æ”¹è¿›è¯­è¨€æç¤ºæ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¼ºä¹é¢†åŸŸçŸ¥è¯†å’Œåé¦ˆçš„æƒ…å†µä¸‹ã€‚</li>
<li>PromptAidæ˜¯ä¸€ä¸ªè§†è§‰åˆ†æç³»ç»Ÿï¼Œæ—¨åœ¨é€šè¿‡äº¤äº’æ–¹å¼å¸®åŠ©ç”¨æˆ·åˆ›å»ºã€ç»†åŒ–å’Œæµ‹è¯•è¯­è¨€æç¤ºã€‚</li>
<li>PromptAidåˆ©ç”¨å¤šç§åè°ƒå¯è§†åŒ–å·¥å…·ï¼Œæ”¯æŒä¸‰ç§æ”¹è¿›æç¤ºçš„ç­–ç•¥ï¼šå…³é”®å­—æ‰°åŠ¨ã€åŒä¹‰æ›¿æ¢æ‰°åŠ¨å’Œè·å–æœ€ä½³ä¸Šä¸‹æ–‡å°‘é‡ç¤ºä¾‹ã€‚</li>
<li>PromptAidçš„è®¾è®¡ç»è¿‡äº†NLPä¸“å®¶çš„è¿­ä»£åŸå‹åˆ¶ä½œè¿‡ç¨‹ï¼Œå¹¶é€šè¿‡å®šé‡å’Œå®šæ€§è¯„ä¼°éªŒè¯å…¶æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2304.01964">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c2c596fe2f2454b913067ae004f83757.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae0799b01c16fb6ef71db226ebfb0c88.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2c2f87576ac3065eb8782d7f62ebd7e1.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-26/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-26/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-26/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-ccdd65aa023a3f3a7229dd8c9e6f225e.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-26  UniDB A Unified Diffusion Bridge Framework via Stochastic Optimal   Control
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-26
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-26/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-4e962cb65e30b43066e790af6db94399.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-26  IGDA Interactive Graph Discovery through Large Language Model Agents
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-26
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">31879.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
