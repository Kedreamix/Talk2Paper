<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-02-26  Large Language Models are Powerful EHR Encoders">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-1ae727e6933c739cea874e5a04534456.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-02-26
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-03-12
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    20.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    83 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-02-26-更新"><a href="#2025-02-26-更新" class="headerlink" title="2025-02-26 更新"></a>2025-02-26 更新</h1><h2 id="Large-Language-Models-are-Powerful-EHR-Encoders"><a href="#Large-Language-Models-are-Powerful-EHR-Encoders" class="headerlink" title="Large Language Models are Powerful EHR Encoders"></a>Large Language Models are Powerful EHR Encoders</h2><p><strong>Authors:Stefan Hegselmann, Georg von Arnim, Tillmann Rheude, Noel Kronenberg, David Sontag, Gerhard Hindricks, Roland Eils, Benjamin Wild</strong></p>
<p>Electronic Health Records (EHRs) offer rich potential for clinical prediction, yet their inherent complexity and heterogeneity pose significant challenges for traditional machine learning approaches. Domain-specific EHR foundation models trained on large collections of unlabeled EHR data have demonstrated promising improvements in predictive accuracy and generalization; however, their training is constrained by limited access to diverse, high-quality datasets and inconsistencies in coding standards and healthcare practices. In this study, we explore the possibility of using general-purpose Large Language Models (LLMs) based embedding methods as EHR encoders. By serializing patient records into structured Markdown text, transforming codes into human-readable descriptors, we leverage the extensive generalization capabilities of LLMs pretrained on vast public corpora, thereby bypassing the need for proprietary medical datasets. We systematically evaluate two state-of-the-art LLM-embedding models, GTE-Qwen2-7B-Instruct and LLM2Vec-Llama3.1-8B-Instruct, across 15 diverse clinical prediction tasks from the EHRSHOT benchmark, comparing their performance to an EHRspecific foundation model, CLIMBR-T-Base, and traditional machine learning baselines. Our results demonstrate that LLM-based embeddings frequently match or exceed the performance of specialized models, even in few-shot settings, and that their effectiveness scales with the size of the underlying LLM and the available context window. Overall, our findings demonstrate that repurposing LLMs for EHR encoding offers a scalable and effective approach for clinical prediction, capable of overcoming the limitations of traditional EHR modeling and facilitating more interoperable and generalizable healthcare applications. </p>
<blockquote>
<p>电子健康记录（EHRs）在临床预测方面具有巨大的潜力，然而其固有的复杂性和异质性给传统的机器学习方法带来了巨大的挑战。基于大型无标签EHR数据的领域特定EHR基础模型已经在预测准确性和泛化能力方面显示出有希望的改进；然而，它们的训练受到多样、高质量数据集访问受限以及编码标准和医疗保健实践不一致的制约。在这项研究中，我们探索了使用基于通用大语言模型（LLMs）的嵌入方法作为EHR编码器的可能性。通过将患者记录序列化为结构化Markdown文本，将代码转换为人类可读的描述符，我们充分利用了LLMs在大量公共语料库上的预训练带来的广泛泛化能力，从而绕过了对专有医疗数据集的需求。我们系统地评估了两种最先进的LLM嵌入模型——GTE-Qwen2-7B-Instruct和LLM2Vec-Llama3.1-8B-Instruct，在EHRSHOT基准测试的15个不同临床预测任务上，与EHR特定基础模型CLIMBR-T-Base和传统机器学习基线进行了性能比较。我们的结果表明，基于LLM的嵌入经常与专业化模型的性能相匹配甚至更好，即使在少样本情况下也是如此，它们的有效性随着底层LLM的大小和可用上下文窗口的大小而扩展。总的来说，我们的研究结果表明，将LLMs重新用于EHR编码为临床预测提供了一种可扩展和有效的途径，能够克服传统EHR建模的限制，促进更互联和更通用的医疗保健应用程序的开发。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17403v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>本研究探索了使用通用的大型语言模型（LLMs）作为电子健康记录（EHRs）编码器的可能性。通过将患者记录序列化为结构化Markdown文本，将代码转换为人类可读的描述符，研究利用在大量公共语料库上预训练LLMs的广泛泛化能力，从而绕过对专有医疗数据集的需求。系统评估了两种先进LLM嵌入模型在EHRSHOT基准测试的15个不同临床预测任务上的性能，并与EHR特定基础模型CLIMBR-T-Base和传统机器学习基线进行了比较。结果表明，LLM基于嵌入的表示在很多情况下与专门模型的性能相匹配甚至超越，即使在少样本情况下也是如此，且其有效性随着底层LLM的大小和可用上下文窗口的大小而提高。总体而言，研究发现将LLMs用于EHR编码提供了一种可扩展和有效的临床预测方法，能够克服传统EHR建模的限制，促进更互操作和更通用的医疗保健应用程序的开发。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>电子健康记录（EHRs）蕴含丰富的临床预测潜力，但其复杂性和异质性为传统机器学习方法带来挑战。</li>
<li>域名特定的EHR基础模型已在预测准确性和泛化能力方面显示出有希望的改进，但其训练受限于多样、高质量数据集的不稳定和编码标准和医疗保健实践的不一致。</li>
<li>研究探索了使用通用的大型语言模型（LLMs）作为EHR编码器的可能性，利用LLMs在大量公共语料库上的预训练泛化能力。</li>
<li>通过序列化患者记录和转换代码为人类可读的描述符，研究绕过了对专有医疗数据集的需求。</li>
<li>LLM嵌入模型在EHRSHOT基准测试中表现出强大的性能，与专门模型相匹配甚至超越。</li>
<li>LLMs的有效性在少样本情况下尤为显著，且随着模型大小和上下文窗口的增加而提高。</li>
<li>使用LLMs进行EHR编码为临床预测提供了一种可扩展和有效的方法，能够克服传统EHR建模的限制。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17403">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-74b0a518a4b17bd9f2ad4616dbe81fcc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cfedd0a7cb65190d72a64a7503e8e23c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89591a01aaa7b4088fef629c9c365d1d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d61db9bbf53dcfc812bf0a801e742f26.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bf61cdd8860ed37ba16bc3d176e4343a.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="FIG-Forward-Inverse-Generation-for-Low-Resource-Domain-specific-Event-Detection"><a href="#FIG-Forward-Inverse-Generation-for-Low-Resource-Domain-specific-Event-Detection" class="headerlink" title="FIG: Forward-Inverse Generation for Low-Resource Domain-specific Event   Detection"></a>FIG: Forward-Inverse Generation for Low-Resource Domain-specific Event   Detection</h2><p><strong>Authors:Tanmay Parekh, Yuxuan Dong, Lucas Bandarkar, Artin Kim, I-Hung Hsu, Kai-Wei Chang, Nanyun Peng</strong></p>
<p>Event Detection (ED) is the task of identifying typed event mentions of interest from natural language text, which benefits domain-specific reasoning in biomedical, legal, and epidemiological domains. However, procuring supervised data for thousands of events for various domains is a laborious and expensive task. To this end, existing works have explored synthetic data generation via forward (generating labels for unlabeled sentences) and inverse (generating sentences from generated labels) generations. However, forward generation often produces noisy labels, while inverse generation struggles with domain drift and incomplete event annotations. To address these challenges, we introduce FIG, a hybrid approach that leverages inverse generation for high-quality data synthesis while anchoring it to domain-specific cues extracted via forward generation on unlabeled target data. FIG further enhances its synthetic data by adding missing annotations through forward generation-based refinement. Experimentation on three ED datasets from diverse domains reveals that FIG outperforms the best baseline achieving average gains of 3.3% F1 and 5.4% F1 in the zero-shot and few-shot settings respectively. Analyzing the generated trigger hit rate and human evaluation substantiates FIG’s superior domain alignment and data quality compared to existing baselines. </p>
<blockquote>
<p>事件检测（ED）是从自然语言文本中识别出有趣的事件类型提及的任务，这有助于生物医学、法律和流行病学等领域的特定领域推理。然而，为各种领域成千上万的事件获取监督数据是一项耗时且昂贵的任务。为此，现有研究已经探索了正向生成（为未标记的句子生成标签）和逆向生成（从生成的标签生成句子）的合成数据生成方法。然而，正向生成往往会生成噪声标签，而逆向生成则面临领域漂移和事件注解不完整的问题。为了解决这些挑战，我们引入了FIG，这是一种混合方法，它利用逆向生成进行高质量数据合成，同时以通过正向生成在未标记的目标数据上提取的领域特定线索为基础进行锚定。FIG还通过基于正向生成的细化来补充缺失的注释，进一步增强其合成数据。在三个来自不同领域的事件检测数据集上的实验表明，与最佳基线相比，FIG在零样本和少样本设置下分别实现了平均F1得分提高3.3%和5.4%。对生成触发命中率的分析和人工评估证实，与现有基线相比，FIG在领域对齐和数据质量方面具有优势。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17394v1">PDF</a> Under review at ACL ARR Feb 2025</p>
<p><strong>Summary</strong></p>
<p>事件检测（ED）是从自然语言文本中识别特定事件提及的任务，对特定领域（如生物医学、法律和流行病学领域）的推理具有重大意义。然而，为数千个事件获取监督数据是一个艰巨且昂贵的任务。现有研究已尝试通过正向生成（为未标记的句子生成标签）和逆向生成（从生成的标签生成句子）的方式进行合成数据生成。但正向生成往往产生噪声标签，而逆向生成则面临领域漂移和事件标注不完整的问题。为了克服这些挑战，我们引入了FIG方法，该方法利用逆向生成进行高质量数据合成，同时通过正向生成在未标记的目标数据上提取的特定领域线索进行锚定。此外，FIG还通过正向生成进行精炼来补充缺失的标注。在三个来自不同领域的ED数据集上的实验表明，与最佳基线相比，FIG在零样本和少样本场景下平均F1得分分别提高了3.3%和5.4%。对生成触发命中率和人工评估的分析证实了FIG相较于现有基线在领域对齐和数据质量方面的优越性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>事件检测（ED）是从文本中识别特定事件的任务，对特定领域有重要意义。</li>
<li>为事件获取监督数据既困难又昂贵。</li>
<li>现有方法如正向和逆向数据生成存在挑战，如噪声标签和领域漂移问题。</li>
<li>引入的FIG方法结合了正向和逆向生成，旨在提高数据合成的质量和领域特异性。</li>
<li>通过实验验证，FIG在零样本和少样本场景下均优于现有基线。</li>
<li>生成触发命中率分析和人工评估证明了FIG在领域对齐和数据质量方面的优越性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17394">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-007571303f402535d7b515f24fe5c43d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b70b95ed0aa6bd754b6cf92d2e40191e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0db55f60ae961d1a85c615deff02384e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a5a25f0e744897d0faa03408ab8c39ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f2c102bbc1d378fbef8c9f7c1d7d62a.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Mutual-Reinforcement-of-LLM-Dialogue-Synthesis-and-Summarization-Capabilities-for-Few-Shot-Dialogue-Summarization"><a href="#Mutual-Reinforcement-of-LLM-Dialogue-Synthesis-and-Summarization-Capabilities-for-Few-Shot-Dialogue-Summarization" class="headerlink" title="Mutual Reinforcement of LLM Dialogue Synthesis and Summarization   Capabilities for Few-Shot Dialogue Summarization"></a>Mutual Reinforcement of LLM Dialogue Synthesis and Summarization   Capabilities for Few-Shot Dialogue Summarization</h2><p><strong>Authors:Yen-Ju Lu, Ting-Yao Hu, Hema Swetha Koppula, Hadi Pouransari, Jen-Hao Rick Chang, Yin Xia, Xiang Kong, Qi Zhu, Simon Wang, Oncel Tuzel, Raviteja Vemulapalli</strong></p>
<p>In this work, we propose Mutual Reinforcing Data Synthesis (MRDS) within LLMs to improve few-shot dialogue summarization task. Unlike prior methods that require external knowledge, we mutually reinforce the LLM&#39;s dialogue synthesis and summarization capabilities, allowing them to complement each other during training and enhance overall performances. The dialogue synthesis capability is enhanced by directed preference optimization with preference scoring from summarization capability. The summarization capability is enhanced by the additional high quality dialogue-summary paired data produced by the dialogue synthesis capability. By leveraging the proposed MRDS mechanism, we elicit the internal knowledge of LLM in the format of synthetic data, and use it to augment the few-shot real training dataset. Empirical results demonstrate that our method improves dialogue summarization, achieving a 1.5% increase in ROUGE scores and a 0.3% improvement in BERT scores in few-shot settings. Furthermore, our method attains the highest average scores in human evaluations, surpassing both the pre-trained models and the baselines fine-tuned solely for summarization tasks. </p>
<blockquote>
<p>在这项工作中，我们提出了大型语言模型（LLM）中的相互强化数据合成（MRDS）方法，以提高小样对话摘要任务的效果。不同于需要外部知识的先前方法，我们相互强化LLM的对话合成和摘要能力，使它们在训练过程中能够相互补充，从而提高整体性能。对话合成能力通过基于摘要能力的偏好评分进行定向偏好优化而增强。摘要能力通过对对话合成能力产生的额外高质量对话摘要配对数据进行增强。通过利用提出的MRDS机制，我们以合成数据的形式激发LLM的内部知识，并将其用于扩充小样真实训练数据集。经验结果表明，我们的方法提高了对话摘要的效果，在小样设置下ROUGE得分提高了1.5%，BERT得分提高了0.3%。此外，我们的方法在人类评估中获得了最高平均分，超过了仅针对摘要任务进行预训练的模型和基线模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17328v1">PDF</a> NAACL 2025 Findings</p>
<p><strong>摘要</strong></p>
<p>本文提出了在大型语言模型（LLM）中采用相互强化数据合成（MRDS）方法，以提高少样本对话摘要任务的效果。不同于需要外部知识的方法，我们相互强化LLM的对话合成和摘要能力，使其在训练过程中相互补充，从而提高整体性能。通过对话合成的定向偏好优化和摘要能力的偏好评分，增强了对话合成能力。同时，通过对话合成产生的附加高质量对话摘要配对数据，提高了摘要能力。利用提出的MRDS机制，我们以合成数据的形式激发LLM的内部知识，并将其用于增强少量的真实训练数据集。实验结果表明，该方法提高了对话摘要的效果，在少样本环境下ROUGE得分提高了1.5%，BERT得分提高了0.3%。此外，该方法在人类评估中获得了最高平均分，超越了预训练模型和仅针对摘要任务进行微调的基础模型。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>提出了一种新的方法——相互强化数据合成（MRDS），用于改进少样本对话摘要任务。</li>
<li>MRDS方法不同于依赖外部知识的方法，它通过相互强化对话合成和摘要能力来提升LLM的性能。</li>
<li>对话合成能力通过定向偏好优化和摘要能力的偏好评分得到增强。</li>
<li>利用MRDS机制激发LLM的内部知识，并以合成数据的形式用于增强真实训练数据集。</li>
<li>实验结果表明，MRDS方法在少样本环境下提高了对话摘要的ROUGE和BERT得分。</li>
<li>MRDS方法在人类评估中表现最佳，超越了预训练模型和针对摘要任务的基线模型。</li>
<li>该方法强调了内部知识在少样本学习中的重要性，并展示了如何利用LLM的内部知识来提高摘要任务的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17328">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-00c07ae51fe6e2ae32a997a6154d0db8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b931c028495d010a7491da3b7819baf5.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Alpha-SQL-Zero-Shot-Text-to-SQL-using-Monte-Carlo-Tree-Search"><a href="#Alpha-SQL-Zero-Shot-Text-to-SQL-using-Monte-Carlo-Tree-Search" class="headerlink" title="Alpha-SQL: Zero-Shot Text-to-SQL using Monte Carlo Tree Search"></a>Alpha-SQL: Zero-Shot Text-to-SQL using Monte Carlo Tree Search</h2><p><strong>Authors:Boyan Li, Jiayi Zhang, Ju Fan, Yanwei Xu, Chong Chen, Nan Tang, Yuyu Luo</strong></p>
<p>Text-to-SQL, which enables natural language interaction with databases, serves as a pivotal method across diverse industries. With new, more powerful large language models (LLMs) emerging every few months, fine-tuning has become incredibly costly, labor-intensive, and error-prone. As an alternative, zero-shot Text-to-SQL, which leverages the growing knowledge and reasoning capabilities encoded in LLMs without task-specific fine-tuning, presents a promising and more challenging direction. To address this challenge, we propose Alpha-SQL, a novel approach that leverages a Monte Carlo Tree Search (MCTS) framework to iteratively infer SQL construction actions based on partial SQL query states. To enhance the framework’s reasoning capabilities, we introduce LLM-as-Action-Model to dynamically generate SQL construction actions during the MCTS process, steering the search toward more promising SQL queries. Moreover, Alpha-SQL employs a self-supervised reward function to evaluate the quality of candidate SQL queries, ensuring more accurate and efficient query generation. Experimental results show that Alpha-SQL achieves 69.7% execution accuracy on the BIRD development set, using a 32B open-source LLM without fine-tuning. Alpha-SQL outperforms the best previous zero-shot approach based on GPT-4o by 2.5% on the BIRD development set. </p>
<blockquote>
<p>文本到SQL的技术，能够实现与数据库的自然语言交互，是各行业关键的方法。随着每隔几个月就会出现新的、更强大的大型语言模型（LLM），微调变得非常昂贵、劳动密集型且易出错。作为一种替代方案，零击文本到SQL，利用LLMs中不断增长的知识和推理能力，无需特定任务的微调，展现了一个有前景且更具挑战性的方向。为了应对这一挑战，我们提出了Alpha-SQL这一新方法，它利用蒙特卡洛树搜索（MCTS）框架来基于部分SQL查询状态迭代推断SQL构建动作。为了提高框架的推理能力，我们引入了LLM-作为行动模型来在MCTS过程中动态生成SQL构建动作，引导搜索朝着更有前途的SQL查询进行。此外，Alpha-SQL采用自我监督的奖励函数来评估候选SQL查询的质量，确保更准确、高效的查询生成。实验结果表明，Alpha-SQL在BIRD开发集上达到了69.7%的执行准确率，使用的是未进行微调的开源大型语言模型。Alpha-SQL在BIRD开发集上的表现优于之前的基于GPT-4o的最佳零冲击方法提高了2.5%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17248v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>文本描述了Text-to-SQL的重要性以及如何利用大型语言模型（LLMs）实现零样本Text-to-SQL的方法。针对现有方法的不足，提出了一种名为Alpha-SQL的新方法，该方法结合蒙特卡洛树搜索（MCTS）和LLM，通过自我监督奖励函数评估候选SQL查询质量，实现了更高的执行准确率和更高效的查询生成。实验结果显示，Alpha-SQL在不进行微调的情况下，在BIRD开发集上达到了69.7%的执行准确率，并优于之前的GPT-4o的零样本方法。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Text-to-SQL是自然语言与数据库交互的重要方法，广泛应用于不同行业。</li>
<li>随着大型语言模型（LLMs）的快速发展，任务特定微调的成本、劳动强度和错误率都在增加。</li>
<li>Alpha-SQL是一种新的零样本Text-to-SQL方法，结合了蒙特卡洛树搜索（MCTS）和LLM。</li>
<li>Alpha-SQL通过LLM生成SQL构建动作，提高推理能力。</li>
<li>Alpha-SQL使用自我监督奖励函数评估候选SQL查询质量，确保更准确、高效的查询生成。</li>
<li>Alpha-SQL在不进行微调的情况下，在BIRD开发集上实现了69.7%的执行准确率。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17248">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d5e3eee7dc536c617f0e9d1b59fe8533.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1ae727e6933c739cea874e5a04534456.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b7e3a9d10147c98fb960a17143267b9e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc820fde87d6280d9dd90e4645065fb1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a48ba8758df3d6a60a8e8b13ea02f576.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Evaluating-the-Effectiveness-of-Large-Language-Models-in-Automated-News-Article-Summarization"><a href="#Evaluating-the-Effectiveness-of-Large-Language-Models-in-Automated-News-Article-Summarization" class="headerlink" title="Evaluating the Effectiveness of Large Language Models in Automated News   Article Summarization"></a>Evaluating the Effectiveness of Large Language Models in Automated News   Article Summarization</h2><p><strong>Authors:Lionel Richy Panlap Houamegni, Fatih Gedikli</strong></p>
<p>The automation of news analysis and summarization presents a promising solution to the challenge of processing and analyzing vast amounts of information prevalent in today’s information society. Large Language Models (LLMs) have demonstrated the capability to transform vast amounts of textual data into concise and easily comprehensible summaries, offering an effective solution to the problem of information overload and providing users with a quick overview of relevant information. A particularly significant application of this technology lies in supply chain risk analysis. Companies must monitor the news about their suppliers and respond to incidents for several critical reasons, including compliance with laws and regulations, risk management, and maintaining supply chain resilience. This paper develops an automated news summarization system for supply chain risk analysis using LLMs. The proposed solution aggregates news from various sources, summarizes them using LLMs, and presents the condensed information to users in a clear and concise format. This approach enables companies to optimize their information processing and make informed decisions. Our study addresses two main research questions: (1) Are LLMs effective in automating news summarization, particularly in the context of supply chain risk analysis? (2) How effective are various LLMs in terms of readability, duplicate detection, and risk identification in their summarization quality? In this paper, we conducted an offline study using a range of publicly available LLMs at the time and complemented it with a user study focused on the top performing systems of the offline experiments to evaluate their effectiveness further. Our results demonstrate that LLMs, particularly Few-Shot GPT-4o mini, offer significant improvements in summary quality and risk identification. </p>
<blockquote>
<p>新闻分析与摘要的自动化为当今信息社会普遍存在的海量信息处理与分析挑战提供了前景光明的解决方案。大型语言模型（LLM）能够将大量文本数据转化为简洁、易于理解的摘要，为解决信息过载问题提供了有效方法，并为用户提供了相关信息的快速概览。这项技术在供应链风险分析中的应用尤为显著。公司必须监控有关其供应商的新闻并对事件做出响应，出于多个关键原因，包括遵守法律法规、风险管理和保持供应链韧性。本文开发了一个用于供应链风险分析的自动化新闻摘要系统，采用大型语言模型。所提出的解决方案从各种来源聚合新闻，使用LLM进行摘要，并以清晰简洁的格式向用户呈现浓缩的信息。这种方法使公司能够优化其信息处理并做出明智的决策。我们的研究解决了两个主要问题：（1）大型语言模型在自动化新闻摘要中是否有效，特别是在供应链风险分析的背景下？（2）在可读性、重复检测和风险识别方面，各种大型语言模型的摘要质量效果如何？在本文中，我们使用了当时可用的各种公开大型语言模型进行了离线研究，并通过以表现最佳的系统为中心的用户研究进一步评估了其有效性。我们的结果表明，大型语言模型，尤其是Few-Shot GPT-4o mini，在摘要质量和风险识别方面提供了显著的改进。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17136v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLMs）在新闻分析与摘要自动化方面展现出巨大的潜力，为解决信息社会中的海量信息处理与分析挑战提供了有前景的解决方案。LLMs能够将大量文本数据转化为简洁易懂的摘要，有效应对信息过载问题，为用户提供相关信息快速概览。本研究在供应链风险分析中利用此技术，开发自动化新闻摘要系统。通过新闻来源聚合、LLMs摘要呈现，帮助公司优化信息处理并做出明智决策。研究证实LLMs，尤其是Few-Shot GPT-4o mini，在摘要质量和风险识别方面有显著提高。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）可以转化大量文本数据为简洁摘要，有效应对信息过载。</li>
<li>在供应链风险分析中，LLMs的应用有助于公司监控供应商新闻并响应各种事件。</li>
<li>研究通过自动化新闻摘要系统，聚合新闻来源并使用LLMs进行摘要呈现。</li>
<li>研究主要解决两个问题：LLMs在自动化新闻摘要中的有效性，以及不同LLMs在可读性、去重和风险评估方面的效果。</li>
<li>研究采用离线研究及用户研究的方式，评估了各系统的有效性。</li>
<li>结果显示，LLMs显著提高摘要质量和风险识别能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17136">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-e9ee34c9b172ec431518bd78529aabbd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dc9b160ea2dfc9d9a3d44baa6ac669ff.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Code-Summarization-Beyond-Function-Level"><a href="#Code-Summarization-Beyond-Function-Level" class="headerlink" title="Code Summarization Beyond Function Level"></a>Code Summarization Beyond Function Level</h2><p><strong>Authors:Vladimir Makharev, Vladimir Ivanov</strong></p>
<p>Code summarization is a critical task in natural language processing and software engineering, which aims to generate concise descriptions of source code. Recent advancements have improved the quality of these summaries, enhancing code readability and maintainability. However, the content of a repository or a class has not been considered in function code summarization. This study investigated the effectiveness of code summarization models beyond the function level, exploring the impact of class and repository contexts on the summary quality. The study involved revising benchmarks for evaluating models at class and repository levels, assessing baseline models, and evaluating LLMs with in-context learning to determine the enhancement of summary quality with additional context. The findings revealed that the fine-tuned state-of-the-art CodeT5+ base model excelled in code summarization, while incorporating few-shot learning and retrieved code chunks from RAG significantly enhanced the performance of LLMs in this task. Notably, the Deepseek Coder 1.3B and Starcoder2 15B models demonstrated substantial improvements in metrics such as BLEURT, METEOR, and BLEU-4 at both class and repository levels. Repository-level summarization exhibited promising potential but necessitates significant computational resources and gains from the inclusion of structured context. Lastly, we employed the recent SIDE code summarization metric in our evaluation. This study contributes to refining strategies for prompt engineering, few-shot learning, and RAG, addressing gaps in benchmarks for code summarization at various levels. Finally, we publish all study details, code, datasets, and results of evaluation in the GitHub repository available at <a target="_blank" rel="noopener" href="https://github.com/kilimanj4r0/code-summarization-beyond-function-level">https://github.com/kilimanj4r0/code-summarization-beyond-function-level</a>. </p>
<blockquote>
<p>代码摘要任务是自然语言处理和软件工程中的关键任务，旨在生成源代码的简洁描述。最近的进展提高了这些摘要的质量，增强了代码的可读性和可维护性。然而，在函数代码摘要中，仓库或类的内容并未得到考虑。本研究旨在探讨函数级别以外的代码摘要模型的有效性，探索类和仓库上下文对摘要质量的影响。该研究包括修订用于类和仓库级别评估模型的基准测试，评估基线模型，并评估具有上下文学习的大型语言模型，以确定附加上下文对摘要质量的提升。研究结果表明，经过微调的最先进的基础模型CodeT5+在代码摘要方面表现出色，而融入小样本学习和从RAG检索的代码片段则显著提高了大型语言模型在此任务上的性能。值得注意的是，Deepseek Coder 1.3B和Starcoder2 15B模型在类级别和仓库级别的BLEURT、METEOR和BLEU-4指标上均实现了显著改进。仓库级别的摘要展现出了巨大的潜力，但需要大量的计算资源，并从结构化上下文中获益。最后，我们在评估中采用了最新的SIDE代码摘要指标。本研究有助于完善提示工程、小样本学习和RAG的策略，填补了各级代码摘要的基准测试空白。最后，我们在GitHub仓库中公开了所有研究细节、代码、数据集和评估结果：<a target="_blank" rel="noopener" href="https://github.com/kilimanj4r0/code-summarization-beyond-function-level">https://github.com/kilimanj4r0/code-summarization-beyond-function-level</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.16704v1">PDF</a> Accepted to LLM4Code @ ICSE’25; 8 pages, 3 figures, 4 tables</p>
<p><strong>Summary</strong><br>代码摘要处理是自然语言处理和软件工程中的关键任务，旨在生成对源代码的简洁描述。近期技术的发展提高了摘要的质量，增强了代码的可读性和可维护性。本研究探讨了超越函数级别的代码摘要模型的有效性，探索了类上下文和存储库上下文对摘要质量的影响。研究发现，经过微调的最先进的CodeT5+基础模型在代码摘要中表现出色，而融入少量学习和从RAG检索的代码片段则进一步提升了大型语言模型在此任务中的表现。Deepseek Coder 1.3B和Starcoder2 15B模型在类级别和存储库级别的度量标准上均有显著改进。本研究对提示工程、少量学习和RAG的策略进行了优化，填补了各级代码摘要的基准测试空白。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>代码摘要处理是自然语言处理和软件工程中的关键任务。</li>
<li>研究探讨了超越函数级别的代码摘要模型的有效性。</li>
<li>类上下文和存储库上下文对摘要质量有影响。</li>
<li>CodeT5+基础模型在代码摘要中表现优异。</li>
<li>融入少量学习和RAG检索的代码片段提升了大型语言模型的表现。</li>
<li>Deepseek Coder和Starcoder模型在类级别和存储库级别的度量标准上有显著改进。</li>
<li>研究成果包括策略优化、基准测试填补以及GitHub存储库中的研究细节、代码、数据集和评估结果发布。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.16704">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-fe8209e4c213d8be158821cbb16e6dc5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a6aad10764a437ede4e5252c27c043f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8ee8080a26fd061677dffed6583c1d1c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d6df761ad4ad7675f77e26d7e08c518.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1bf4d0764616d7cb4d3764055228280b.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="MetaSym-A-Symplectic-Meta-learning-Framework-for-Physical-Intelligence"><a href="#MetaSym-A-Symplectic-Meta-learning-Framework-for-Physical-Intelligence" class="headerlink" title="MetaSym: A Symplectic Meta-learning Framework for Physical Intelligence"></a>MetaSym: A Symplectic Meta-learning Framework for Physical Intelligence</h2><p><strong>Authors:Pranav Vaidhyanathan, Aristotelis Papatheodorou, Mark T. Mitchison, Natalia Ares, Ioannis Havoutis</strong></p>
<p>Scalable and generalizable physics-aware deep learning has long been considered a significant challenge with various applications across diverse domains ranging from robotics to molecular dynamics. Central to almost all physical systems are symplectic forms, the geometric backbone that underpins fundamental invariants like energy and momentum. In this work, we introduce a novel deep learning architecture, MetaSym. In particular, MetaSym combines a strong symplectic inductive bias obtained from a symplectic encoder and an autoregressive decoder with meta-attention. This principled design ensures that core physical invariants remain intact while allowing flexible, data-efficient adaptation to system heterogeneities. We benchmark MetaSym on highly varied datasets such as a high-dimensional spring mesh system (Otness et al., 2021), an open quantum system with dissipation and measurement backaction, and robotics-inspired quadrotor dynamics. Our results demonstrate superior performance in modeling dynamics under few-shot adaptation, outperforming state-of-the-art baselines with far larger models. </p>
<blockquote>
<p>可扩展且可泛化的物理感知深度学习长期以来被视为一项具有挑战性的任务，其在从机器人学到分子动力学等多个领域都有广泛的应用。几乎所有物理系统的核心都是辛形式，它是支撑能量和动量等基本不变量的几何主干。在这项工作中，我们介绍了一种新型深度学习架构MetaSym。具体来说，MetaSym结合了由辛编码器获得的强大辛归纳偏见和带有元注意的自回归解码器。这种有原则的设计确保核心物理不变量保持完整，同时允许灵活、数据高效地适应系统异质性。我们在高度不同的数据集上对MetaSym进行了基准测试，例如高维弹簧网格系统（Otness等人，2021年）、具有耗散和测量反作用的开放量子系统，以及受机器人启发的四旋翼飞行器动力学。我们的结果表明，在少样本适应的情况下，MetaSym在建模动力学方面表现出卓越的性能，超越了使用更大模型的最新基线水平。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.16667v1">PDF</a> 8+10 pages, 5 figures, 4 tables</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种新型的深度学习架构MetaSym，该架构结合了从辛编码器和自回归解码器得到的强大辛归纳偏见以及元注意机制。该设计能够在保持核心物理不变量的同时，灵活适应系统异质性并高效利用数据。MetaSym在多种数据集上的表现优越，如高维弹簧网格系统、具有耗散和测量反作用的开放量子系统以及受机器人启发的四旋翼飞行器动力学。其在少样本适应建模动力学方面的性能优于当前最先进的基线模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MetaSym是一种新型的深度学习架构，结合了辛编码器和自回归解码器以及元注意机制。</li>
<li>该架构旨在确保物理系统的核心不变性质（如能量和动量）在模型学习中得到保持。</li>
<li>MetaSym能够在保持物理不变性的同时，灵活适应系统异质性并高效利用数据。</li>
<li>在多种数据集上进行了基准测试，包括高维弹簧网格系统、开放量子系统和四旋翼飞行器动力学。</li>
<li>MetaSym在少样本适应建模动力学方面表现出卓越性能。</li>
<li>该架构优于当前最先进的基线模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.16667">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6434da3050204ac602c25f3da137f127.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a7d970c928a31437ba0255450830406b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-629ce2ed937bcd7d582608496327e73c.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Few-shot-Continual-Relation-Extraction-via-Open-Information-Extraction"><a href="#Few-shot-Continual-Relation-Extraction-via-Open-Information-Extraction" class="headerlink" title="Few-shot Continual Relation Extraction via Open Information Extraction"></a>Few-shot Continual Relation Extraction via Open Information Extraction</h2><p><strong>Authors:Thiem Nguyen, Anh Nguyen, Quyen Tran, Tu Vu, Diep Nguyen, Linh Ngo, Thien Nguyen</strong></p>
<p>Typically, Few-shot Continual Relation Extraction (FCRE) models must balance retaining prior knowledge while adapting to new tasks with extremely limited data. However, real-world scenarios may also involve unseen or undetermined relations that existing methods still struggle to handle. To address these challenges, we propose a novel approach that leverages the Open Information Extraction concept of Knowledge Graph Construction (KGC). Our method not only exposes models to all possible pairs of relations, including determined and undetermined labels not available in the training set, but also enriches model knowledge with diverse relation descriptions, thereby enhancing knowledge retention and adaptability in this challenging scenario. In the perspective of KGC, this is the first work explored in the setting of Continual Learning, allowing efficient expansion of the graph as the data evolves. Experimental results demonstrate our superior performance compared to other state-of-the-art FCRE baselines, as well as the efficiency in handling dynamic graph construction in this setting. </p>
<blockquote>
<p>通常，Few-shot Continual Relation Extraction（FCRE）模型需要在保留先前知识的同时，适应新任务且数据量极度有限。然而，现实世界场景还可能涉及未见或未确定的关系，现有方法在处理这些场景时仍面临困难。为了应对这些挑战，我们提出了一种利用知识图谱构建（KGC）中的开放信息提取概念的新方法。我们的方法不仅使模型接触到所有可能的关系对，包括训练集中不存在的确定和未确定的标签，而且还通过多样的关系描述丰富模型知识，从而在这种具有挑战性的场景中提高知识保留和适应性。从KGC的角度来看，这是连续学习环境中首次探索的工作，允许随着数据的演变有效地扩展图。实验结果表明，与其他先进的FCRE基线相比，我们的性能优越，并且在此环境中处理动态图构建的效率很高。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.16648v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对Few-shot Continual Relation Extraction（FCRE）模型的挑战，提出了一种利用知识图谱构建（KGC）的开放信息提取概念的新方法。该方法不仅使模型接触到所有可能的关系对，包括训练集中不可用的确定和未确定标签，还通过多样的关系描述丰富模型知识，从而增强知识保留和适应性。这是知识图谱构建视角在持续学习领域中的首次探索，能够实现随着数据演变的高效图扩展。实验结果证明了该方法相较于其他先进FCRE基准线的优越性，以及在处理动态图构建方面的效率。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FCRE模型需要在保留先验知识的同时适应新任务，尤其在数据极度有限的情况下。</li>
<li>现有方法难以处理未见或未确定的关系。</li>
<li>提出了一种新方法，利用KGC的开放信息提取概念来应对这些挑战。</li>
<li>模型可以接触到所有可能的关系对，包括训练集中不可用的确定和未确定标签。</li>
<li>方法通过多样的关系描述丰富模型知识，增强知识保留和适应性。</li>
<li>这是知识图谱构建视角在持续学习领域的首次探索。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.16648">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-b41817cb0695ddccedf7fca81d259266.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a8a3548505de091ad33bf2e57af5976.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="TimePFN-Effective-Multivariate-Time-Series-Forecasting-with-Synthetic-Data"><a href="#TimePFN-Effective-Multivariate-Time-Series-Forecasting-with-Synthetic-Data" class="headerlink" title="TimePFN: Effective Multivariate Time Series Forecasting with Synthetic   Data"></a>TimePFN: Effective Multivariate Time Series Forecasting with Synthetic   Data</h2><p><strong>Authors:Ege Onur Taga, M. Emrullah Ildiz, Samet Oymak</strong></p>
<p>The diversity of time series applications and scarcity of domain-specific data highlight the need for time-series models with strong few-shot learning capabilities. In this work, we propose a novel training scheme and a transformer-based architecture, collectively referred to as TimePFN, for multivariate time-series (MTS) forecasting. TimePFN is based on the concept of Prior-data Fitted Networks (PFN), which aims to approximate Bayesian inference. Our approach consists of (1) generating synthetic MTS data through diverse Gaussian process kernels and the linear coregionalization method, and (2) a novel MTS architecture capable of utilizing both temporal and cross-channel dependencies across all input patches. We evaluate TimePFN on several benchmark datasets and demonstrate that it outperforms the existing state-of-the-art models for MTS forecasting in both zero-shot and few-shot settings. Notably, fine-tuning TimePFN with as few as 500 data points nearly matches full dataset training error, and even 50 data points yield competitive results. We also find that TimePFN exhibits strong univariate forecasting performance, attesting to its generalization ability. Overall, this work unlocks the power of synthetic data priors for MTS forecasting and facilitates strong zero- and few-shot forecasting performance. </p>
<blockquote>
<p>时间序列应用的多样性和特定领域数据的稀缺性，凸显了需要具有强大少量学习能力的时间序列模型。在这项工作中，我们提出了一种新型的训练方案和基于变压器的架构，两者统称为TimePFN，用于多元时间序列（MTS）预测。TimePFN基于先验数据拟合网络（PFN）的概念，旨在近似贝叶斯推断。我们的方法包括（1）通过多样的高斯过程核和线性共区域化方法生成合成MTS数据，（2）一种新型的MTS架构，能够利用所有输入补丁中的时间依赖性和跨通道依赖性。我们在多个基准数据集上评估了TimePFN，结果表明，它在零样本和少量样本的情况下，都优于现有的最先进的MTS预测模型。值得注意的是，使用仅500个数据点对TimePFN进行微调，其误差几乎匹配全数据集训练误差，甚至使用50个数据点也能获得具有竞争力的结果。我们还发现TimePFN在单变量预测方面表现出强大的性能，证明了其泛化能力。总的来说，这项工作释放了合成数据先验在MTS预测中的潜力，并实现了强大的零样本和少量样本预测性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.16294v1">PDF</a> To appear in AAAI-2025 as a conference paper</p>
<p><strong>Summary</strong></p>
<p>时间序列应用的多样性和特定领域数据的稀缺性突显出需要具有强大少样本学习能力的时间序列模型。本文提出了一种基于先验数据拟合网络（PFN）概念的新型训练方案和基于变压器的架构，统称为TimePFN，用于多元时间序列（MTS）预测。TimePFN通过生成合成MTS数据和利用先进架构实现强大的零样本和少样本预测性能，展示了出色的预测效果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>时间序列应用的多样性和特定领域数据的稀缺性凸显出对具有强大少样本学习能力模型的迫切需求。</li>
<li>本文提出了TimePFN模型，结合了新型训练方案和基于变压器的架构。</li>
<li>TimePFN基于先验数据拟合网络（PFN）概念，旨在近似贝叶斯推理。</li>
<li>通过生成合成MTS数据和利用先进架构，TimePFN实现了强大的预测性能。</li>
<li>TimePFN在多个基准数据集上的表现优于现有的最先进的MTS预测模型，特别是在零样本和少样本设置下。</li>
<li>TimePFN仅使用少量数据点（如500个）进行微调即可接近全数据集训练的错误率，甚至使用50个数据点也能取得有竞争力的结果。</li>
<li>TimePFN展现出强大的单变量预测性能，证明了其泛化能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.16294">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-7babbe90763294fc46d0ab55e7ca01e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1ec3317004cb7755fe0b3003160b7051.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b249619f5b8eb652cd72d9f0758630c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a03814cc966e17c8a342fd4611efa4cd.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Enhancing-LLMs-for-Identifying-and-Prioritizing-Important-Medical-Jargons-from-Electronic-Health-Record-Notes-Utilizing-Data-Augmentation"><a href="#Enhancing-LLMs-for-Identifying-and-Prioritizing-Important-Medical-Jargons-from-Electronic-Health-Record-Notes-Utilizing-Data-Augmentation" class="headerlink" title="Enhancing LLMs for Identifying and Prioritizing Important Medical   Jargons from Electronic Health Record Notes Utilizing Data Augmentation"></a>Enhancing LLMs for Identifying and Prioritizing Important Medical   Jargons from Electronic Health Record Notes Utilizing Data Augmentation</h2><p><strong>Authors:Won Seok Jang, Sharmin Sultana, Zonghai Yao, Hieu Tran, Zhichao Yang, Sunjae Kwon, Hong Yu</strong></p>
<p>Objective: OpenNotes enables patients to access EHR notes, but medical jargon can hinder comprehension. To improve understanding, we evaluated closed- and open-source LLMs for extracting and prioritizing key medical terms using prompting, fine-tuning, and data augmentation.   Materials and Methods: We assessed LLMs on 106 expert-annotated EHR notes, experimenting with (i) general vs. structured prompts, (ii) zero-shot vs. few-shot prompting, (iii) fine-tuning, and (iv) data augmentation. To enhance open-source models in low-resource settings, we used ChatGPT for data augmentation and applied ranking techniques. We incrementally increased the augmented dataset size (10 to 10,000) and conducted 5-fold cross-validation, reporting F1 score and Mean Reciprocal Rank (MRR).   Results and Discussion: Fine-tuning and data augmentation improved performance over other strategies. GPT-4 Turbo achieved the highest F1 (0.433), while Mistral7B with data augmentation had the highest MRR (0.746). Open-source models, when fine-tuned or augmented, outperformed closed-source models. Notably, the best F1 and MRR scores did not always align. Few-shot prompting outperformed zero-shot in vanilla models, and structured prompts yielded different preferences across models. Fine-tuning improved zero-shot performance but sometimes degraded few-shot performance. Data augmentation performed comparably or better than other methods.   Conclusion: Our evaluation highlights the effectiveness of prompting, fine-tuning, and data augmentation in improving model performance for medical jargon extraction in low-resource scenarios. </p>
<blockquote>
<p>目标：OpenNotes让患者能够访问电子健康记录（EHR）笔记，但医疗术语可能会妨碍理解。为了改善患者的理解，我们评估了用于提取和优先排序关键医学术语的闭源和开源的大型语言模型（LLMs），采用提示、微调和数据增强等方法。材料和方法：我们在106份专家注释的EHR笔记上评估了LLMs，尝试了（i）通用提示与结构化提示，（ii）零样本提示与少样本提示，（iii）微调，以及（iv）数据增强。为了在低资源环境中增强开源模型的效果，我们利用ChatGPT进行数据增强，并应用排序技术。我们逐步增加了增强数据集的大小（从10到10,000），进行了5倍交叉验证，并报告了F1分数和平均倒数排名（MRR）。结果和讨论：相较于其他策略，微调和数据增强提高了性能。GPT-4 Turbo获得了最高的F1分数（0.433），而Mistral7B通过数据增强获得了最高的MRR（0.746）。当开源模型经过微调或增强时，其性能超过了闭源模型。值得注意的是，最佳的F1分数和MRR并不总是对齐。少样本提示在原始模型中表现优于零样本提示，结构化提示在不同模型中产生不同的偏好。微调提高了零样本性能，但有时会降低少样本性能。数据增强的表现与其他方法相当或更好。结论：我们的评估强调了提示、微调和数据增强在改进低资源场景中医学术语提取模型性能的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.16022v1">PDF</a> 21pages, 5 figures, 4 tables</p>
<p><strong>Summary</strong></p>
<p>本文探讨了OpenNotes系统下患者获取电子健康记录（EHR）笔记时遇到的医学术语理解难题。研究团队评估了闭源和开源的大型语言模型（LLMs）在提取和优先排序关键医学术语方面的表现，具体采用了提示、微调、数据增强等方法。结果显示，微调和数据增强能提高模型性能，GPT-4 Turbo在F1分数上表现最佳，而Mistral7B在平均倒数排名（MRR）上表现最佳。此外，开源模型在微调或数据增强后表现优于闭源模型。研究还发现，少样本提示通常优于零样本，结构化提示在不同模型中有不同偏好，微调虽能提高零样本性能但有时会降低少样本性能。总体而言，提示、微调、数据增强等方法在医学术语提取方面效果显著，特别是在资源有限的场景下。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OpenNotes允许患者访问电子健康记录（EHR）笔记，但医学术语的复杂性可能阻碍理解。</li>
<li>研究评估了闭源和开源的大型语言模型（LLMs）在提取和排序关键医学术语方面的表现。</li>
<li>使用了提示、微调、数据增强等方法来改善模型性能。</li>
<li>GPT-4 Turbo在F1分数上表现最佳，而Mistral7B在平均倒数排名（MRR）上领先。</li>
<li>开源模型在微调或数据增强后表现优于闭源模型。</li>
<li>少样本提示通常比零样本提示更有效，结构化提示能提高模型性能但不同模型表现不同。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.16022">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8c9dc397742ef574027a3610a960f0a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-35740911d587ff17db7d6bc11781608c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f51ce4c78a0ef5f740f0d06fe9761fc5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-26da0512aecf25bd3998960496ea89c8.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="MoMa-A-Modular-Deep-Learning-Framework-for-Material-Property-Prediction"><a href="#MoMa-A-Modular-Deep-Learning-Framework-for-Material-Property-Prediction" class="headerlink" title="MoMa: A Modular Deep Learning Framework for Material Property Prediction"></a>MoMa: A Modular Deep Learning Framework for Material Property Prediction</h2><p><strong>Authors:Botian Wang, Yawen Ouyang, Yaohui Li, Yiqun Wang, Haorui Cui, Jianbing Zhang, Xiaonan Wang, Wei-Ying Ma, Hao Zhou</strong></p>
<p>Deep learning methods for material property prediction have been widely explored to advance materials discovery. However, the prevailing pre-train then fine-tune paradigm often fails to address the inherent diversity and disparity of material tasks. To overcome these challenges, we introduce MoMa, a Modular framework for Materials that first trains specialized modules across a wide range of tasks and then adaptively composes synergistic modules tailored to each downstream scenario. Evaluation across 17 datasets demonstrates the superiority of MoMa, with a substantial 14% average improvement over the strongest baseline. Few-shot and continual learning experiments further highlight MoMa’s potential for real-world applications. Pioneering a new paradigm of modular material learning, MoMa will be open-sourced to foster broader community collaboration. </p>
<blockquote>
<p>深度学习在材料属性预测方面的应用已得到广泛探索，以推动材料发现的发展。然而，流行的预训练微调模式往往无法解决材料任务的固有多样性和差异性。为了克服这些挑战，我们引入了MoMa，这是一个面向材料的模块化框架，它首先训练适用于各种任务的专用模块，然后自适应地组合针对每个下游场景的协同模块。在17个数据集上的评估证明了MoMa的优越性，相较于最强的基线模型，其平均改进了高达14%。小样本和持续学习的实验进一步突显了MoMa在现实世界应用中的潜力。作为模块化材料学习的新范式先驱，MoMa将开源以促进更广泛的社区合作。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.15483v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>材料属性预测的深度学习方法在推动材料发现方面得到了广泛的研究。然而，当前先预训练再微调的模式往往无法应对材料任务的内在多样性和差异性。为了克服这些挑战，我们推出MoMa模块化的材料学习框架，该框架首先对各种任务进行专业化模块训练，然后针对每种下游场景自适应地组合协同模块。在多个数据集上的评估显示MoMa的优越性，相对于最强基线有平均提高百分之十四的效果。少样本和持续学习的实验进一步凸显了MoMa在现实世界应用中的潜力。MoMa开创了模块化材料学习的新范式，并将开源以促进更广泛的社区合作。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前预训练微调模型在材料学习任务中的表现有待提高。</li>
<li>MoMa是一个全新的材料学习框架，采用模块化设计以适应不同的材料任务。</li>
<li>MoMa通过训练专业模块并自适应组合协同模块来提高性能。</li>
<li>在多个数据集上的评估显示MoMa相对于现有方法具有显著优势。</li>
<li>MoMa在少样本和持续学习方面的表现突出其实际应用潜力。</li>
<li>MoMa将开源以促进更广泛的社区合作和进一步发展。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.15483">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6e6539210e968776afb64847f09f39ec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5fc390130116b6925fa6a461008e3fe4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3e773a0fbd0487f6e1ff3ee8efbbabdd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1ab815da1904d1b42bbe91cc91854c5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5e6a71e1a88d8a1da2d46e96a4b21883.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Few-shot-Species-Range-Estimation"><a href="#Few-shot-Species-Range-Estimation" class="headerlink" title="Few-shot Species Range Estimation"></a>Few-shot Species Range Estimation</h2><p><strong>Authors:Christian Lange, Max Hamilton, Elijah Cole, Alexander Shepard, Samuel Heinrich, Angela Zhu, Subhransu Maji, Grant Van Horn, Oisin Mac Aodha</strong></p>
<p>Knowing where a particular species can or cannot be found on Earth is crucial for ecological research and conservation efforts. By mapping the spatial ranges of all species, we would obtain deeper insights into how global biodiversity is affected by climate change and habitat loss. However, accurate range estimates are only available for a relatively small proportion of all known species. For the majority of the remaining species, we often only have a small number of records denoting the spatial locations where they have previously been observed. We outline a new approach for few-shot species range estimation to address the challenge of accurately estimating the range of a species from limited data. During inference, our model takes a set of spatial locations as input, along with optional metadata such as text or an image, and outputs a species encoding that can be used to predict the range of a previously unseen species in feed-forward manner. We validate our method on two challenging benchmarks, where we obtain state-of-the-art range estimation performance, in a fraction of the compute time, compared to recent alternative approaches. </p>
<blockquote>
<p>了解特定物种在地球上可以或不能被发现的位置对于生态研究和保护工作至关重要。通过绘制所有物种的空间分布范围图，我们将更深入地了解全球生物多样性如何受到气候变化和栖息地丧失的影响。然而，准确的范围估计仅适用于已知物种中的一小部分。对于大多数剩余物种，我们通常只有少数记录标注了它们之前观察到的空间位置。我们概述了一种新的少量物种范围估计方法，以解决从有限数据中准确估计物种范围的挑战。在推理过程中，我们的模型以一组空间位置作为输入，以及可选的元数据，如文本或图像，并输出一个物种编码，该编码可用于以前所未见的物种的范围预测。我们在两个具有挑战性的基准测试上验证了我们的方法，与使用最近的替代方法相比，我们在计算时间上大大缩短，并获得了最先进的范围估计性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14977v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>该文指出物种空间分布映射对生态研究和保护工作的重要性。然而，目前准确的物种分布范围数据只涵盖了一小部分已知物种。对于大多数物种，我们只有少数记录表明它们曾在哪些地点出现过。因此，文章提出了一种基于少量数据的物种分布范围估计新方法，该方法能够通过接收空间位置和可选元数据（如文本或图像）作为输入，输出一个物种编码，以预测未见物种的分布范围。在两项具有挑战性的基准测试中，该方法在运算时间大幅减少的情况下，仍取得了最先进的范围估计性能。</p>
<p><strong>要点</strong></p>
<ol>
<li>物种空间分布对生态研究和保护至关重要。</li>
<li>目前准确的物种分布范围数据仅限于一小部分已知物种。</li>
<li>对于大多数物种，只有少数记录表明它们曾在哪些地点出现过。</li>
<li>提出了一种基于少量数据的物种分布范围估计新方法。</li>
<li>该方法通过接收空间位置和可选元数据作为输入，输出一个物种编码。</li>
<li>方法能够在预测未见物种的分布范围时，以预测推断的方式进行应用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14977">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-8287a5c4e12415ceba4f05c3fbf9d716.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-334f488c64c136b6999f325e6c033b01.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-df4545e72aa0db7f1dd9136134978292.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-57378ede8370aba20fe92ef87d467895.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6cba238f2211c91e26fad0857cb35dc.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="CAPE-Covariate-Adjusted-Pre-Training-for-Epidemic-Time-Series-Forecasting"><a href="#CAPE-Covariate-Adjusted-Pre-Training-for-Epidemic-Time-Series-Forecasting" class="headerlink" title="CAPE: Covariate-Adjusted Pre-Training for Epidemic Time Series   Forecasting"></a>CAPE: Covariate-Adjusted Pre-Training for Epidemic Time Series   Forecasting</h2><p><strong>Authors:Zewen Liu, Juntong Ni, Max S. Y. Lau, Wei Jin</strong></p>
<p>Accurate forecasting of epidemic infection trajectories is crucial for safeguarding public health. However, limited data availability during emerging outbreaks and the complex interaction between environmental factors and disease dynamics present significant challenges for effective forecasting. In response, we introduce CAPE, a novel epidemic pre-training framework designed to harness extensive disease datasets from diverse regions and integrate environmental factors directly into the modeling process for more informed decision-making on downstream diseases. Based on a covariate adjustment framework, CAPE utilizes pre-training combined with hierarchical environment contrasting to identify universal patterns across diseases while estimating latent environmental influences. We have compiled a diverse collection of epidemic time series datasets and validated the effectiveness of CAPE under various evaluation scenarios, including full-shot, few-shot, zero-shot, cross-location, and cross-disease settings, where it outperforms the leading baseline by an average of 9.9% in full-shot and 14.3% in zero-shot settings. The code will be released upon acceptance. </p>
<blockquote>
<p>传染病感染轨迹的准确预测对于保障公共卫生至关重要。然而，在新型疫情爆发期间数据可用性的有限性以及环境因子与疾病动态之间的复杂交互，为有效的预测带来了重大挑战。为了应对这些挑战，我们引入了CAPE，这是一个新型传染病预训练框架，旨在利用来自不同地区的广泛疾病数据集，并将环境因子直接整合到建模过程中，以为下游疾病的决策提供更有依据的决策。基于协变量调整框架，CAPE利用预训练结合分层环境对比，以识别疾病之间的通用模式，同时估计潜在的环境影响。我们已编译了多种传染病时间序列数据集，并在各种评估场景下验证了CAPE的有效性，包括全数据、小数据、零数据、跨地点和跨疾病设置。在全数据设置中，它比领先基线高出9.9%；在零数据设置中，高出14.3%。代码将在接受后发布。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03393v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>CAPE是一个新型流行病预训练框架，旨在利用来自不同地区的丰富疾病数据集，并将环境因素直接纳入建模过程中，为下游疾病的决策提供更有根据的预测。CAPE基于协变量调整框架，采用预训练和层次环境对比技术，识别疾病间的通用模式，同时评估潜在环境影响。在多种评估场景中，CAPE表现优异，平均超出领先基线9.9%（全数据场景）和14.3%（零数据场景）。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CAPE是一个流行病预训练框架，用于预测感染轨迹。</li>
<li>它结合了来自不同地区的丰富疾病数据集。</li>
<li>CAPE将环境因素直接纳入建模过程。</li>
<li>基于协变量调整框架，CAPE采用预训练和层次环境对比技术。</li>
<li>CAPE在不同评估场景中表现优异，包括全数据、少数据、零数据、跨地点和跨疾病设置。</li>
<li>与领先基线相比，CAPE在全数据场景中平均表现超出9.9%，在零数据场景中平均超出14.3%。</li>
<li>代码将在接受后发布。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03393">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-11eecbad75f2db6377b288c1ddb51373.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4fa23534cab0786091679ef7d71a16aa.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Modeling-Multi-modal-Cross-interaction-for-Multi-label-Few-shot-Image-Classification-Based-on-Local-Feature-Selection"><a href="#Modeling-Multi-modal-Cross-interaction-for-Multi-label-Few-shot-Image-Classification-Based-on-Local-Feature-Selection" class="headerlink" title="Modeling Multi-modal Cross-interaction for Multi-label Few-shot Image   Classification Based on Local Feature Selection"></a>Modeling Multi-modal Cross-interaction for Multi-label Few-shot Image   Classification Based on Local Feature Selection</h2><p><strong>Authors:Kun Yan, Zied Bouraoui, Fangyun Wei, Chang Xu, Ping Wang, Shoaib Jameel, Steven Schockaert</strong></p>
<p>The aim of multi-label few-shot image classification (ML-FSIC) is to assign semantic labels to images, in settings where only a small number of training examples are available for each label. A key feature of the multi-label setting is that an image often has several labels, which typically refer to objects appearing in different regions of the image. When estimating label prototypes, in a metric-based setting, it is thus important to determine which regions are relevant for which labels, but the limited amount of training data and the noisy nature of local features make this highly challenging. As a solution, we propose a strategy in which label prototypes are gradually refined. First, we initialize the prototypes using word embeddings, which allows us to leverage prior knowledge about the meaning of the labels. Second, taking advantage of these initial prototypes, we then use a Loss Change Measurement (LCM) strategy to select the local features from the training images (i.e. the support set) that are most likely to be representative of a given label. Third, we construct the final prototype of the label by aggregating these representative local features using a multi-modal cross-interaction mechanism, which again relies on the initial word embedding-based prototypes. Experiments on COCO, PASCAL VOC, NUS-WIDE, and iMaterialist show that our model substantially improves the current state-of-the-art. </p>
<blockquote>
<p>多标签少样本图像分类（ML-FSIC）的目标是在每个标签只有少量训练样本的情况下，为图像分配语义标签。多标签设置的一个关键特征是图像通常具有多个标签，这些标签通常指代图像不同区域中出现的对象。在基于度量的环境中估计标签原型时，确定哪些区域与哪些标签相关非常重要，但训练数据的有限性和局部特征的噪声性质使得这极具挑战性。为解决这一问题，我们提出了一种逐步优化标签原型的策略。首先，我们使用词嵌入初始化原型，以便利用有关标签含义的先验知识。其次，利用这些初始原型，我们使用损失变化测量（LCM）策略从训练图像（即支持集）中选择最可能代表给定标签的局部特征。最后，我们通过多模态交叉交互机制聚合这些代表性局部特征来构建标签的最终原型，这同样依赖于最初的基于词嵌入的原型。在COCO、PASCAL VOC、NUS-WIDE和iMaterialist上的实验表明，我们的模型大大改进了当前的最佳水平。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13732v2">PDF</a> In Transactions on Multimedia Computing Communications and   Applications. arXiv admin note: text overlap with arXiv:2112.01037</p>
<p><strong>Summary</strong></p>
<p>本文介绍了多标签少样本图像分类（ML-FSIC）的目标，即在每个标签只有少量训练样本的情况下，为图像分配语义标签。文章提出了一种逐步优化标签原型的方法，首先利用词嵌入初始化原型，然后使用局部特征选择策略（Loss Change Measurement，LCM）选择最有可能代表给定标签的局部特征，最后通过多模态交叉交互机制构建最终的标签原型。实验结果表明，该方法在COCO、PASCAL VOC、NUS-WIDE和iMaterialist等多个数据集上均显著提高了当前技术水平。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多标签少样本图像分类（ML-FSIC）的目标是在少量训练样本的情况下，为图像分配多个语义标签。</li>
<li>图像通常具有多个标签，这些标签通常指代图像中不同区域的物体。</li>
<li>在估计标签原型时，需要确定哪些区域与哪些标签相关，但由于训练数据有限和局部特征的噪声性质，这具有挑战性。</li>
<li>提出了一种逐步优化标签原型的方法，包括使用词嵌入初始化原型、利用Loss Change Measurement（LCM）策略选择局部特征和构建最终标签原型。</li>
<li>词嵌入允许利用标签的先验知识，LCM策略有助于选择最可能代表给定标签的局部特征。</li>
<li>多模态交叉交互机制用于聚合代表性局部特征，进一步依赖于初始的词嵌入原型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13732">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f1b2bb12381dfd4cff798709741958e5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fe1022b5c81b4d29225382c1fccd5171.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Manta-Enhancing-Mamba-for-Few-Shot-Action-Recognition-of-Long-Sub-Sequence"><a href="#Manta-Enhancing-Mamba-for-Few-Shot-Action-Recognition-of-Long-Sub-Sequence" class="headerlink" title="Manta: Enhancing Mamba for Few-Shot Action Recognition of Long   Sub-Sequence"></a>Manta: Enhancing Mamba for Few-Shot Action Recognition of Long   Sub-Sequence</h2><p><strong>Authors:Wenbo Huang, Jinghui Zhang, Guang Li, Lei Zhang, Shuoyuan Wang, Fang Dong, Jiahui Jin, Takahiro Ogawa, Miki Haseyama</strong></p>
<p>In few-shot action recognition (FSAR), long sub-sequences of video naturally express entire actions more effectively. However, the high computational complexity of mainstream Transformer-based methods limits their application. Recent Mamba demonstrates efficiency in modeling long sequences, but directly applying Mamba to FSAR overlooks the importance of local feature modeling and alignment. Moreover, long sub-sequences within the same class accumulate intra-class variance, which adversely impacts FSAR performance. To solve these challenges, we propose a Matryoshka MAmba and CoNtrasTive LeArning framework (Manta). Firstly, the Matryoshka Mamba introduces multiple Inner Modules to enhance local feature representation, rather than directly modeling global features. An Outer Module captures dependencies of timeline between these local features for implicit temporal alignment. Secondly, a hybrid contrastive learning paradigm, combining both supervised and unsupervised methods, is designed to mitigate the negative effects of intra-class variance accumulation. The Matryoshka Mamba and the hybrid contrastive learning paradigm operate in two parallel branches within Manta, enhancing Mamba for FSAR of long sub-sequence. Manta achieves new state-of-the-art performance on prominent benchmarks, including SSv2, Kinetics, UCF101, and HMDB51. Extensive empirical studies prove that Manta significantly improves FSAR of long sub-sequence from multiple perspectives. </p>
<blockquote>
<p>在少样本动作识别（FSAR）中，视频的长子序列更自然地表达了整个动作。然而，主流基于Transformer的方法的高计算复杂度限制了其应用。最近的Mamba在建模长序列方面展示了效率，但直接将Mamba应用于FSAR忽略了局部特征建模和对齐的重要性。此外，同一类别内的长子序列会累积类内方差，这对FSAR性能产生不利影响。为了解决这些挑战，我们提出了Matryoshka Mamba和对比学习框架（Manta）。首先，Matryoshka Mamba引入了多个内部模块来增强局部特征表示，而不是直接建模全局特征。外部模块捕获这些局部特征之间时间线的依赖性，以进行隐式时间对齐。其次，结合有监督和无监督方法的混合对比学习范式旨在减轻类内方差累积的负面影响。Matryoshka Mamba和混合对比学习范式在Manta的两个并行分支中运行，增强了Mamba对长子序列的FSAR能力。Manta在SSv2、Kinetics、UCF101和HMDB51等主流基准测试上达到了最新水平，广泛的实证研究证明，Manta从多个角度显著提高了长子序列的FSAR性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07481v4">PDF</a> Accepted by AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>基于少数镜头动作识别的研究背景，本文主要针对主流Transformer方法在建模长序列视频时的高计算复杂度问题，提出了一个名为Manta的新框架。Manta通过引入Matryoshka Mamba和混合对比学习范式来解决现有方法的局限性。Matryoshka Mamba设计有多个内部模块来增强局部特征表示，而非直接建模全局特征，同时通过外部模块捕捉这些局部特征的时间线依赖性以实现隐式时间对齐。混合对比学习范式结合了监督和无监督方法，以减轻同一类别内方差积累对少数镜头动作识别性能的负面影响。Manta在多个基准测试集上实现了卓越的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>少数镜头动作识别中，长子序列视频更有效表达完整动作。</li>
<li>主流Transformer方法在计算复杂度上存在问题，难以应用于长子序列的建模。</li>
<li>Mamba虽在建模长序列上有效率，但忽略局部特征建模和对齐的重要性。</li>
<li>同类长子序列的累积会造成类内方差，影响识别性能。</li>
<li>Matryoshka Mamba通过引入多个内部模块增强局部特征表示，外部模块实现时间对齐。</li>
<li>混合对比学习范式结合监督和无监督方法，减轻类内方差积累的负面影响。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.07481">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f48a716b41cddf407f9a3cbb4f6bdd2c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d16c2681d6549d6322cdf09a77cba898.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b7850902a9c3912646aa3a165d6fc9b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1d20f12b2ec1bc52f161cf0f0f664e5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d3efc2920012bc9e302b18d829345f0e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-27e490f47ca16f70e9a33df1c71fe152.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-34c6e325a9bcc51cecd7512552bf34d5.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="GraphCLIP-Enhancing-Transferability-in-Graph-Foundation-Models-for-Text-Attributed-Graphs"><a href="#GraphCLIP-Enhancing-Transferability-in-Graph-Foundation-Models-for-Text-Attributed-Graphs" class="headerlink" title="GraphCLIP: Enhancing Transferability in Graph Foundation Models for   Text-Attributed Graphs"></a>GraphCLIP: Enhancing Transferability in Graph Foundation Models for   Text-Attributed Graphs</h2><p><strong>Authors:Yun Zhu, Haizhou Shi, Xiaotang Wang, Yongchao Liu, Yaoke Wang, Boci Peng, Chuntao Hong, Siliang Tang</strong></p>
<p>Recently, research on Text-Attributed Graphs (TAGs) has gained significant attention due to the prevalence of free-text node features in real-world applications and the advancements in Large Language Models (LLMs) that bolster TAG methodologies. However, current TAG approaches face two primary challenges: (i) Heavy reliance on label information and (ii) Limited cross-domain zero&#x2F;few-shot transferability. These issues constrain the scaling of both data and model size, owing to high labor costs and scaling laws, complicating the development of graph foundation models with strong transferability. In this work, we propose the GraphCLIP framework to address these challenges by learning graph foundation models with strong cross-domain zero&#x2F;few-shot transferability through a self-supervised contrastive graph-summary pretraining method. Specifically, we generate and curate large-scale graph-summary pair data with the assistance of LLMs, and introduce a novel graph-summary pretraining method, combined with invariant learning, to enhance graph foundation models with strong cross-domain zero-shot transferability. For few-shot learning, we propose a novel graph prompt tuning technique aligned with our pretraining objective to mitigate catastrophic forgetting and minimize learning costs. Extensive experiments show the superiority of GraphCLIP in both zero-shot and few-shot settings, while evaluations across various downstream tasks confirm the versatility of GraphCLIP. Our code is available at: <a target="_blank" rel="noopener" href="https://github.com/ZhuYun97/GraphCLIP">https://github.com/ZhuYun97/GraphCLIP</a> </p>
<blockquote>
<p>近期，关于文本属性图（TAGs）的研究受到了广泛关注，这是由于现实世界应用中自由文本节点特征的普及以及支持TAG方法的大型语言模型（LLMs）的进展。然而，当前的TAG方法面临两个主要挑战：(i)严重依赖标签信息；(ii)跨域零&#x2F;少样本迁移能力有限。这些问题由于高昂的人工成本和规模扩展定律而限制了数据和模型规模的扩展，从而加剧了具有强大迁移能力的图基础模型的开发难度。在这项工作中，我们提出GraphCLIP框架来解决这些挑战，通过自监督对比图摘要预训练方法来学习具有强大跨域零&#x2F;少样本迁移能力的图基础模型。具体来说，我们借助LLMs生成和精选大规模图摘要配对数据，并引入一种新型的图摘要预训练方法与不变学习相结合，以增强图基础模型的跨域零样本迁移能力。对于少样本学习，我们提出了一种与预训练目标对齐的图提示调整技术，以减轻灾难性遗忘并最小化学习成本。大量实验表明，GraphCLIP在零样本和少样本设置中都表现出卓越性能，而在各种下游任务上的评估则证实了GraphCLIP的通用性。我们的代码可用在：<a target="_blank" rel="noopener" href="https://github.com/ZhuYun97/GraphCLIP">https://github.com/ZhuYun97/GraphCLIP</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.10329v4">PDF</a> Accepted to WWW’25</p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对文本属性图（TAGs）的研究进展及其面临的挑战，包括对数据标签的过度依赖和跨域零&#x2F;少样本迁移能力有限的问题。为应对这些挑战，提出了GraphCLIP框架，通过自监督对比图摘要预训练方法，学习具有强大跨域零&#x2F;少样本迁移能力的图基础模型。利用大型语言模型生成和筛选大规模图摘要对数据，结合不变学习，增强模型的零样本迁移能力。针对少样本学习，提出了一种与预训练目标对齐的图提示调整技术，以减轻灾难性遗忘并减少学习成本。实验证明GraphCLIP在零样本和少样本设置中的优越性，并在各种下游任务中的评估确认了其通用性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Text-Attributed Graphs (TAGs)研究受到关注，但面临依赖标签信息和跨域迁移能力有限两大挑战。</li>
<li>GraphCLIP框架通过自监督对比图摘要预训练方法解决这些问题，提高模型的迁移能力。</li>
<li>利用大型语言模型生成和筛选大规模图摘要对数据，增强模型的零样本迁移能力。</li>
<li>GraphCLIP结合不变学习来提升模型的性能。</li>
<li>针对少样本学习，GraphCLIP提出图提示调整技术，减轻灾难性遗忘并减少学习成本。</li>
<li>实验证明GraphCLIP在零样本和少样本设置中的优越性。</li>
<li>GraphCLIP在各种下游任务中的评估结果证明了其通用性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.10329">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-cbb56d9d09ff746dcafcc515c85501e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-90565eb4fc93a92d404aabde5e0df3d5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1862f11ef721f1779d45322857343a25.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-78935b866551015645b23cf4d495d827.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-42438623bb4111bbe55f85ea65885596.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Packet-Inspection-Transformer-A-Self-Supervised-Journey-to-Unseen-Malware-Detection-with-Few-Samples"><a href="#Packet-Inspection-Transformer-A-Self-Supervised-Journey-to-Unseen-Malware-Detection-with-Few-Samples" class="headerlink" title="Packet Inspection Transformer: A Self-Supervised Journey to Unseen   Malware Detection with Few Samples"></a>Packet Inspection Transformer: A Self-Supervised Journey to Unseen   Malware Detection with Few Samples</h2><p><strong>Authors:Kyle Stein, Arash Mahyari, Guillermo Francia III, Eman El-Sheikh</strong></p>
<p>As networks continue to expand and become more interconnected, the need for novel malware detection methods becomes more pronounced. Traditional security measures are increasingly inadequate against the sophistication of modern cyber attacks. Deep Packet Inspection (DPI) has been pivotal in enhancing network security, offering an in-depth analysis of network traffic that surpasses conventional monitoring techniques. DPI not only examines the metadata of network packets, but also dives into the actual content being carried within the packet payloads, providing a comprehensive view of the data flowing through networks. While the integration of advanced deep learning techniques with DPI has introduced modern methodologies into malware detection and network traffic classification, state-of-the-art supervised learning approaches are limited by their reliance on large amounts of annotated data and their inability to generalize to novel, unseen malware threats. To address these limitations, this paper leverages the recent advancements in self-supervised learning (SSL) and few-shot learning (FSL). Our proposed self-supervised approach trains a transformer via SSL to learn the embedding of packet content, including payload, from vast amounts of unlabeled data by masking portions of packets, leading to a learned representation that generalizes to various downstream tasks. Once the representation is extracted from the packets, they are used to train a malware detection algorithm. The representation obtained from the transformer is then used to adapt the malware detector to novel types of attacks using few-shot learning approaches. Our experimental results demonstrate that our method achieves classification accuracies of up to 94.76% on the UNSW-NB15 dataset and 83.25% on the CIC-IoT23 dataset. </p>
<blockquote>
<p>随着网络的不断扩张和相互连接，对新型恶意软件检测方法的需要变得更加迫切。传统的安全措施越来越难以应对现代网络攻击的复杂性。深度包检测（DPI）在增强网络安全方面发挥了关键作用，提供了一种对网络流量进行深入分析的技术，超越了传统监控技术。DPI不仅检查网络包的元数据，还深入研究包有效负载中的实际内容，提供了通过网络流动的数据的全面视图。虽然将先进的深度学习技术与DPI相结合已经为恶意软件检测和网络流量分类引入了现代方法，但最先进的监督学习方法受限于它们对大量注释数据的依赖以及它们对新型未见恶意威胁的泛化能力。为了解决这个问题，本文利用自我监督学习（SSL）和少镜头学习（FSL）的最新进展。我们提出的自我监督方法通过SSL训练一个变压器，学习包括有效负载在内的数据包内容的嵌入，通过掩盖数据包的部分内容，从大量无标签数据中学习表示，这有助于适应各种下游任务。一旦从数据包中提取出表示形式，它们就被用来训练恶意软件检测算法。从变压器获得的表示形式然后被用于利用少镜头学习方法适应新型攻击。我们的实验结果表明，我们的方法在UNSW-NB15数据集上达到了高达94.76%的分类精度，在CIC-IoT23数据集上达到了83.25%的分类精度。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.18219v2">PDF</a> </p>
<p><strong>Summary</strong>：随着网络不断扩展和相互连接，新型恶意软件检测方法的必要性更加突出。深度包检测（DPI）对于增强网络安全性至关重要，它提供了对网络流量的深入分析，超越了传统监控技术。本文利用自我监督学习和少样本学习的最新进展，通过自我监督方法训练一个变压器来学习数据包内容的嵌入，包括负载，通过掩盖部分数据包从大量未标记的数据中学习表示，然后将该表示用于训练恶意软件检测算法。实验结果表明，该方法在UNSW-NB15数据集上达到了94.76%的分类准确率，在CIC-IoT23数据集上达到了83.25%的分类准确率。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>网络互联性的增强凸显了新型恶意软件检测方法的必要性。</li>
<li>传统安全措施对现代网络攻击的复杂性越来越不足够应对。</li>
<li>深度包检测（DPI）是一种增强网络安全的关键技术，能够深入分析网络流量。</li>
<li>DPI不仅分析网络包的元数据，还深入实际内容，提供全面的网络数据流视图。</li>
<li>深度学习和DPI的结合为恶意软件检测和流量分类提供了现代方法。</li>
<li>当前最先进的监督学习方法受限于需要大量标注数据和无法泛化到新型威胁。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.18219">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-bcc83883366eddcde9bbfb1aa68721fc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e301b46dc5830d0b3d043546d5749519.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e27fded5d5e75cf8c2f24ad1970dbf4c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-10498d334da71e691e186b5e291bc831.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a39e5d724d5b613f597cc5b287c65915.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Iterative-Repair-with-Weak-Verifiers-for-Few-shot-Transfer-in-KBQA-with-Unanswerability"><a href="#Iterative-Repair-with-Weak-Verifiers-for-Few-shot-Transfer-in-KBQA-with-Unanswerability" class="headerlink" title="Iterative Repair with Weak Verifiers for Few-shot Transfer in KBQA with   Unanswerability"></a>Iterative Repair with Weak Verifiers for Few-shot Transfer in KBQA with   Unanswerability</h2><p><strong>Authors:Riya Sawhney, Samrat Yadav, Indrajit Bhattacharya,  Mausam</strong></p>
<p>Real-world applications of KBQA require models to handle unanswerable questions with a limited volume of in-domain labeled training data. We propose the novel task of few-shot transfer for KBQA with unanswerable questions and contribute two new datasets for performance evaluation. We present FUn-FuSIC - a novel solution for our task that extends FuSIC KBQA, the state-of-the-art few-shot transfer model for answerable-only KBQA. We first note that FuSIC-KBQA’s iterative repair makes a strong assumption that all questions are unanswerable. As a remedy, we propose Feedback for Unanswerability (FUn), which uses iterative repair using feedback from a suite of strong and weak verifiers, and an adaptation of self consistency for unanswerabilty to better assess the answerability of a question. Our experiments show that FUn-FuSIC significantly outperforms suitable adaptations of multiple LLM based and supervised SoTA models on our task, while establishing a new SoTA for answerable few-shot transfer as well. </p>
<blockquote>
<p>现实世界中的KBQA应用要求模型能够处理无法回答的问题，并且在领域内部只有有限量的标记训练数据。我们针对无法回答的KBQA提出了新颖的小样本迁移任务，并为性能评估贡献了两个新数据集。我们介绍了FUN-FuSIC——一种针对我们任务的新型解决方案，它扩展了FuSIC KBQA（针对仅可回答KBQA的最先进的小样本迁移模型）。我们首先注意到，FuSIC-KBQA的迭代修复假设所有问题都是无法回答的，这是一个很强的假设。作为补救措施，我们提出了针对无法回答的问题的反馈（FUN），它使用来自一系列强验证器和弱验证器的反馈进行迭代修复，并自适应地调整自我一致性以更好地评估问题的可回答性。我们的实验表明，FUN-FuSIC在我们的任务上显著优于多个基于大型语言模型和监督的当前最佳模型（SoTA）的适当改编版本，同时为我们建立了新的可回答的小样本迁移的当前最佳水平。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.14313v2">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>针对KBQA（知识库问答）的实际应用，需要模型处理不可回答的问题，并在有限的领域内标签训练数据下工作。本文提出了针对KBQA不可回答问题进行少样本迁移的新任务，并贡献了两个新的数据集用于性能评估。我们提出了FUN-FuSIC的新解决方案，它是FuSIC KBQA的扩展，适用于仅回答问题的少样本迁移模型。我们发现FuSIC-KBQA的迭代修复假设所有问题都是不可回答的，这存在局限性。为解决这一问题，我们提出了针对不可回答性的反馈（FUN），利用一系列强验证器和弱验证器的反馈进行迭代修复，并自适应地调整自我一致性以更好地评估问题的可回答性。实验表明，FUN-FuSIC在我们的任务上显著优于多个大型语言模型（LLM）和当前最佳监督模型的适应性版本，同时为可回答问题的少样本迁移建立了新的最佳性能。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>针对KBQA的不可回答问题，提出了少样本迁移的新任务。</li>
<li>贡献了两个新的数据集用于评估该任务的性能。</li>
<li>扩展了FuSIC KBQA模型，提出了FUN-FuSIC解决方案来处理不可回答问题的少样本迁移。</li>
<li>指出FuSIC-KBQA的迭代修复假设存在局限性。</li>
<li>提出了针对不可回答性的反馈（FUN），通过利用一系列强验证器和弱验证器的反馈进行迭代修复。</li>
<li>通过实验验证了FUN-FuSIC在任务上的优越性，同时建立了新的少样本迁移最佳性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.14313">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-6f6371182ee979f72bfebbaa5a27a0bb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3ea931b769c8450292c10b6a261bc740.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-602ed04ed957e020bcb077c9d4837d8e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36f0c1e09c8f11db10b36ef76a86da8c.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Cross-domain-Multi-modal-Few-shot-Object-Detection-via-Rich-Text"><a href="#Cross-domain-Multi-modal-Few-shot-Object-Detection-via-Rich-Text" class="headerlink" title="Cross-domain Multi-modal Few-shot Object Detection via Rich Text"></a>Cross-domain Multi-modal Few-shot Object Detection via Rich Text</h2><p><strong>Authors:Zeyu Shangguan, Daniel Seita, Mohammad Rostami</strong></p>
<p>Cross-modal feature extraction and integration have led to steady performance improvements in few-shot learning tasks due to generating richer features. However, existing multi-modal object detection (MM-OD) methods degrade when facing significant domain-shift and are sample insufficient. We hypothesize that rich text information could more effectively help the model to build a knowledge relationship between the vision instance and its language description and can help mitigate domain shift. Specifically, we study the Cross-Domain few-shot generalization of MM-OD (CDMM-FSOD) and propose a meta-learning based multi-modal few-shot object detection method that utilizes rich text semantic information as an auxiliary modality to achieve domain adaptation in the context of FSOD. Our proposed network contains (i) a multi-modal feature aggregation module that aligns the vision and language support feature embeddings and (ii) a rich text semantic rectify module that utilizes bidirectional text feature generation to reinforce multi-modal feature alignment and thus to enhance the model’s language understanding capability. We evaluate our model on common standard cross-domain object detection datasets and demonstrate that our approach considerably outperforms existing FSOD methods. </p>
<blockquote>
<p>跨模态特征提取和融合由于生成了更丰富的特征，在少样本学习任务中带来了稳定的性能提升。然而，当面临显著的领域迁移和样本不足时，现有的多模态目标检测（MM-OD）方法会性能下降。我们假设丰富的文本信息可以更有效地帮助模型建立视觉实例与其语言描述之间的知识关系，并有助于缓解领域迁移。具体来说，我们研究了跨域少样本泛化的MM-OD（CDMM-FSOD），并提出了一种基于元学习的多模态少样本目标检测方法，利用丰富的文本语义信息作为辅助模态来实现FSOD领域的自适应。我们提出的网络包含（i）一种多模态特征聚合模块，用于对齐视觉和语言支持特征嵌入；（ii）一种丰富的文本语义校正模块，利用双向文本特征生成来加强多模态特征对齐，从而提高模型的语言理解能力和跨领域适应能力。我们在常见的标准跨域目标检测数据集上评估了我们的模型，结果表明我们的方法显著优于现有的FSOD方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.16188v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探讨了跨模态特征提取与融合在少样本学习任务中的持续性能提升，并指出多模态目标检测（MM-OD）方法在面临显著领域偏移和样本不足时的局限性。研究假设丰富文本信息能有效帮助模型建立视觉实例与语言描述之间的知识关系，并有助于缓解领域偏移问题。文章研究了跨域少样本泛化的多模态目标检测（CDMM-FSOD），并提出一种基于元学习的多模态少样本目标检测方法，利用丰富文本语义信息作为辅助模态来实现领域适应，针对FSOD。所提网络包括多模态特征聚合模块和丰富文本语义校正模块，前者对齐视觉和语言支持特征嵌入，后者利用双向文本特征生成强化多模态特征对齐，提高模型的语言理解力。在跨域目标检测数据集上的评估显示，该方法显著优于现有FSOD方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>跨模态特征提取与融合在少样本学习任务中持续推动性能提升。</li>
<li>多模态目标检测（MM-OD）方法在面临显著领域偏移和样本不足时性能下降。</li>
<li>丰富文本信息能有效帮助模型建立视觉实例与语言描述之间的知识关系。</li>
<li>文章研究了跨域少样本泛化的多模态目标检测（CDMM-FSOD）。</li>
<li>提出一种基于元学习的多模态少样本目标检测方法，结合丰富文本语义信息作为辅助模态实现领域适应。</li>
<li>所提网络包括多模态特征聚合模块，用于对齐视觉和语言特征嵌入。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.16188">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-85a4de71be3b941e9adfb228ec9e55f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-166e3a4b3a5aa523408277a6f8373cbc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ea5b9efa72ab4c621978060a74d66d5c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2f3049da544342f8ce39dc0e7e2d7b8d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8dc3d406b5b833b4d020643fbf9a2b95.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="PromptAid-Prompt-Exploration-Perturbation-Testing-and-Iteration-using-Visual-Analytics-for-Large-Language-Models"><a href="#PromptAid-Prompt-Exploration-Perturbation-Testing-and-Iteration-using-Visual-Analytics-for-Large-Language-Models" class="headerlink" title="PromptAid: Prompt Exploration, Perturbation, Testing and Iteration using   Visual Analytics for Large Language Models"></a>PromptAid: Prompt Exploration, Perturbation, Testing and Iteration using   Visual Analytics for Large Language Models</h2><p><strong>Authors:Aditi Mishra, Utkarsh Soni, Anjana Arunkumar, Jinbin Huang, Bum Chul Kwon, Chris Bryan</strong></p>
<p>Large Language Models (LLMs) have gained widespread popularity due to their ability to perform ad-hoc Natural Language Processing (NLP) tasks with a simple natural language prompt. Part of the appeal for LLMs is their approachability to the general public, including individuals with no prior technical experience in NLP techniques. However, natural language prompts can vary significantly in terms of their linguistic structure, context, and other semantics. Modifying one or more of these aspects can result in significant differences in task performance. Non-expert users may find it challenging to identify the changes needed to improve a prompt, especially when they lack domain-specific knowledge and lack appropriate feedback. To address this challenge, we present PromptAid, a visual analytics system designed to interactively create, refine, and test prompts through exploration, perturbation, testing, and iteration. PromptAid uses multiple, coordinated visualizations which allow users to improve prompts by using the three strategies: keyword perturbations, paraphrasing perturbations, and obtaining the best set of in-context few-shot examples. PromptAid was designed through an iterative prototyping process involving NLP experts and was evaluated through quantitative and qualitative assessments for LLMs. Our findings indicate that PromptAid helps users to iterate over prompt template alterations with less cognitive overhead, generate diverse prompts with help of recommendations, and analyze the performance of the generated prompts while surpassing existing state-of-the-art prompting interfaces in performance. </p>
<blockquote>
<p>大型语言模型（LLM）因其能够通过简单的自然语言提示执行专项自然语言处理（NLP）任务而广受欢迎。LLM的吸引力之一在于它们对普通大众的亲和力，包括那些没有先前NLP技术经验的人。然而，自然语言的提示在它们的语言结构、上下文和其他语义方面可能存在很大差异。修改这些方面的一个或多个因素可能会导致任务性能的重大差异。非专业用户可能会发现很难确定改进提示所需的更改，尤其是在他们缺乏特定领域知识和适当的反馈时。为了应对这一挑战，我们推出了PromptAid，一个视觉分析系统，旨在通过探索、扰动、测试和迭代来交互式地创建、完善和调整提示。PromptAid使用多个协调的可视化提示，允许用户通过三种策略改进提示：关键词扰动、同义替换扰动和获取最佳的上下文少量示例。PromptAid的设计过程是通过涉及NLP专家的迭代原型制作过程完成的，并通过针对LLM的定量和定性评估进行了评估。我们的研究结果表明，PromptAid有助于用户减少认知负担地迭代调整提示模板，借助推荐生成多样化的提示，并分析生成的提示的性能，同时在性能上超越了现有的先进提示界面。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2304.01964v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）通过自然语言提示执行即兴的自然语言处理（NLP）任务的能力而广受欢迎。LLM的吸引力在于它对公众的亲和力，包括没有NLP技术经验的个人。然而，自然语言的提示在语言表达和结构上有很大的差异，可能导致任务执行效果的不同。非专业用户可能难以识别需要改进的提示变化，特别是在缺乏特定领域知识和适当反馈的情况下。为解决此挑战，我们推出了PromptAid，一个视觉分析系统，旨在通过探索、微调、测试和迭代的方式，以交互方式创建和细化提示。PromptAid利用多种协调可视化工具，使用户能够通过三种策略改进提示：关键字扰动、同义替换扰动和获取最佳的上下文少量示例。PromptAid的设计经过了NLP专家的迭代原型制作过程，并通过定量和定性评估进行验证，表明其能帮助用户通过更少的认知负担来迭代提示模板的修改，借助推荐生成多样化的提示并分析生成提示的性能表现，超越了现有的先进提示界面在性能方面的表现。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）由于其使用自然语言提示执行NLP任务的能力而受到广泛欢迎。</li>
<li>自然语言提示的差异（如语言结构、上下文和语义）可能导致任务执行效果的显著不同。</li>
<li>非专业用户在改进语言提示方面面临挑战，特别是在缺乏领域知识和反馈的情况下。</li>
<li>PromptAid是一个视觉分析系统，旨在通过交互方式帮助用户创建、细化和测试语言提示。</li>
<li>PromptAid利用多种协调可视化工具，支持三种改进提示的策略：关键字扰动、同义替换扰动和获取最佳上下文少量示例。</li>
<li>PromptAid的设计经过了NLP专家的迭代原型制作过程，并通过定量和定性评估验证其有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2304.01964">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-c2c596fe2f2454b913067ae004f83757.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae0799b01c16fb6ef71db226ebfb0c88.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2c2f87576ac3065eb8782d7f62ebd7e1.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-26/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-26/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-26/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-ccdd65aa023a3f3a7229dd8c9e6f225e.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-02-26  UniDB A Unified Diffusion Bridge Framework via Stochastic Optimal   Control
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-26
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-26/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-4e962cb65e30b43066e790af6db94399.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-02-26  IGDA Interactive Graph Discovery through Large Language Model Agents
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-26
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">15437.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
