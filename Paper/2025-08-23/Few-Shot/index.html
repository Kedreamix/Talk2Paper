<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-08-23  Amortized In-Context Mixed Effect Transformer Models A Zero-Shot   Approach for Pharmacokinetics">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-8e9808ff564f5e19ce16e0846f0654a1.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-08-23
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-09-08
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    12.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    50 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-08-23-更新"><a href="#2025-08-23-更新" class="headerlink" title="2025-08-23 更新"></a>2025-08-23 更新</h1><h2 id="Amortized-In-Context-Mixed-Effect-Transformer-Models-A-Zero-Shot-Approach-for-Pharmacokinetics"><a href="#Amortized-In-Context-Mixed-Effect-Transformer-Models-A-Zero-Shot-Approach-for-Pharmacokinetics" class="headerlink" title="Amortized In-Context Mixed Effect Transformer Models: A Zero-Shot   Approach for Pharmacokinetics"></a>Amortized In-Context Mixed Effect Transformer Models: A Zero-Shot   Approach for Pharmacokinetics</h2><p><strong>Authors:César Ali Ojeda Marin, Wilhelm Huisinga, Purity Kavwele, Niklas Hartung</strong></p>
<p>Accurate dose-response forecasting under sparse sampling is central to precision pharmacotherapy. We present the Amortized In-Context Mixed-Effect Transformer (AICMET) model, a transformer-based latent-variable framework that unifies mechanistic compartmental priors with amortized in-context Bayesian inference. AICMET is pre-trained on hundreds of thousands of synthetic pharmacokinetic trajectories with Ornstein-Uhlenbeck priors over the parameters of compartment models, endowing the model with strong inductive biases and enabling zero-shot adaptation to new compounds. At inference time, the decoder conditions on the collective context of previously profiled trial participants, generating calibrated posterior predictions for newly enrolled patients after a few early drug concentration measurements. This capability collapses traditional model-development cycles from weeks to hours while preserving some degree of expert modelling. Experiments across public datasets show that AICMET attains state-of-the-art predictive accuracy and faithfully quantifies inter-patient variability – outperforming both nonlinear mixed-effects baselines and recent neural ODE variants. Our results highlight the feasibility of transformer-based, population-aware neural architectures as offering a new alternative for bespoke pharmacokinetic modeling pipelines, charting a path toward truly population-aware personalized dosing regimens. </p>
<blockquote>
<p>在稀疏采样下进行准确的剂量-反应预测是精准药物治疗的核心。我们提出了平均上下文混合效应变压器（AICMET）模型，这是一个基于变压器的潜在变量框架，它将机械室模型先验与平均上下文贝叶斯推理相结合。AICMET在数以万计的人工药物代谢轨迹上进行预训练，这些轨迹带有Ornstein-Uhlenbeck关于室模型参数的先验，为模型提供了强烈的归纳偏见，并实现了对新化合物的零样本适应。在推理阶段，解码器根据先前分析过的试验参与者的集体上下文进行调整，在少数早期药物浓度测量后，为刚入学的新患者生成校准的后验预测。这种能力将传统的模型开发周期从数周缩短到数小时，同时保留一定程度的专家建模。在公开数据集上的实验表明，AICMET达到了先进的预测精度，并真实地量化了患者间的差异——优于非线性混合效应基准和最新的神经ODE变体。我们的结果强调了基于变压器的、具有人群意识的神经网络架构的可行性，为定制的药物代谢动力学建模管道提供了新的选择，为真正具有人群意识个性化给药方案铺平了道路。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15659v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p> AICMET模型是一种基于转化器的潜变量框架，它将机械室模型先验与摊销上下文贝叶斯推理相结合，实现了稀疏采样下的精准剂量反应预测。该模型经过百万级合成药物代谢动力学轨迹的预训练，并具备Ornstein-Uhlenbeck先验知识，可快速适应新化合物。在推断阶段，该解码器根据先前试验参与者的集体上下文进行条件处理，生成新纳入患者经几次早期药物浓度测量后的校准后预测。此能力使得模型开发周期从数周缩短至数小时，同时保持了专家建模的一定程度。实验表明，AICMET在公共数据集上达到了最先进的预测精度，并忠实量化了患者间的变异性，优于非线性混合效应基准和最新的神经ODE变体。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AICMET模型是一个基于转化器的潜变量框架，结合了机械室模型先验和摊销上下文贝叶斯推理。</li>
<li>模型经过大量合成药物代谢动力学轨迹的预训练，具备Ornstein-Uhlenbeck先验知识。</li>
<li>AICMET可以快速适应新化合物，并在推断阶段根据先前试验参与者的集体上下文进行条件处理。</li>
<li>该模型能够在稀疏采样下进行精准剂量反应预测。</li>
<li>AICMET达到了先进的预测精度，并忠实量化了患者间的变异性。</li>
<li>与非线性混合效应基准和神经ODE变体相比，AICMET表现出优越性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15659">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-ea439aa091202bca24c089b57d414411.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-13ef1c417662ec98846a30640d2a0255.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7e821497e1e2d78a6425f6cebc56c887.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2134f364689f733b9cb6948f31e7cf00.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Bridging-Generalization-and-Personalization-in-Wearable-Human-Activity-Recognition-via-On-Device-Few-Shot-Learning"><a href="#Bridging-Generalization-and-Personalization-in-Wearable-Human-Activity-Recognition-via-On-Device-Few-Shot-Learning" class="headerlink" title="Bridging Generalization and Personalization in Wearable Human Activity   Recognition via On-Device Few-Shot Learning"></a>Bridging Generalization and Personalization in Wearable Human Activity   Recognition via On-Device Few-Shot Learning</h2><p><strong>Authors:Pixi Kang, Julian Moosmann, Mengxi Liu, Bo Zhou, Michele Magno, Paul Lukowicz, Sizhen Bian</strong></p>
<p>Human Activity Recognition (HAR) using wearable devices has advanced significantly in recent years, yet its generalization remains limited when models are deployed to new users. This degradation in performance is primarily due to user-induced concept drift (UICD), highlighting the importance of efficient personalization. In this paper, we present a hybrid framework that first generalizes across users and then rapidly adapts to individual users using few-shot learning directly on-device. By updating only the classifier layer with user-specific data, our method achieves robust personalization with minimal computational and memory overhead. We implement this framework on the energy-efficient RISC-V-based GAP9 microcontroller and validate it across three diverse HAR scenarios: RecGym, QVAR-Gesture, and Ultrasound-Gesture. Post-deployment adaptation yields consistent accuracy improvements of 3.73%, 17.38%, and 3.70% respectively. These results confirm that fast, lightweight, and effective personalization is feasible on embedded platforms, paving the way for scalable and user-aware HAR systems in the wild \footnote{<a target="_blank" rel="noopener" href="https://github.com/kangpx/onlineTiny2023%7D">https://github.com/kangpx/onlineTiny2023}</a>. </p>
<blockquote>
<p>使用可穿戴设备进行人类活动识别（HAR）的研究近年来取得了显著进展，但当模型部署给新用户时，其泛化能力仍然有限。这种性能下降主要是由于用户引起的概念漂移（UICD），从而突出了有效个性化的重要性。在本文中，我们提出了一种混合框架，该框架首先实现跨用户泛化，然后直接使用少量数据在设备上进行快速的个人用户适应。通过仅使用用户特定数据更新分类器层，我们的方法可以在计算量和内存开销极小的情况下实现稳健的个性化。我们在能效高的RISC-V基的GAP9微控制器上实现了该框架，并在三种不同的HAR场景：RecGym、QVAR-Gesture和Ultrasound-Gesture中进行了验证。部署后的适应分别带来了3.73%、17.38%和3.70%的准确性改善。这些结果证实了嵌入式平台上快速、轻便、有效的个性化是可行的，为野外可扩展和用户感知的HAR系统铺平了道路。[在线资源：<a target="_blank" rel="noopener" href="https://github.com/kangpx/onlineTiny2023]%E3%80%82">https://github.com/kangpx/onlineTiny2023]。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15413v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>HAR技术在可穿戴设备上已有了显著的发展，但模型的部署在面对新用户时其泛化能力受限。为解决因用户导致的概念漂移（UICD）问题，凸显个性化效率的重要性，本文提出了混合框架。该框架先实现用户间的泛化，然后借助少数样本学习迅速适应个别用户，且仅通过用户特定数据更新分类器层，实现稳健的个性化，同时计算与内存开销较小。该框架在节能RISC-V基GAP9微控制器上实现，并在RecGym、QVAR-Gesture和Ultrasound-Gesture三个不同HAR场景中验证。部署后的适应分别带来3.73%、17.38%和3.70%的准确性提升。结果表明嵌入式平台上可实现快速、轻便、高效的个性化，为野外可扩展和用户感知的HAR系统铺平了道路。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>人机交互识别（HAR）技术在可穿戴设备上的泛化能力面临挑战，特别是在面对新用户时。</li>
<li>用户导致的概念漂移（UICD）是限制HAR技术泛化的主要原因之一。</li>
<li>提出的混合框架先实现跨用户的泛化，然后通过少数样本学习迅速适应个别用户。</li>
<li>该框架通过仅更新分类器层来实现个性化，同时保持较低的计算和内存开销。</li>
<li>在RISC-V基GAP9微控制器上实现了该框架，验证了其在实际应用中的有效性。</li>
<li>在三个不同的HAR场景中验证该框架，包括RecGym、QVAR-Gesture和Ultrasound-Gesture。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15413">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-831cd3ded6fa5ac005a3f4883bb7ad54.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-16e24959a44ec0159ec473a1c85cc2ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-739a8d3ea27b95505ff4dc3a8622afee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a176505d6f8c28f9d1f5cb6b88977706.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="In-Context-Iterative-Policy-Improvement-for-Dynamic-Manipulation"><a href="#In-Context-Iterative-Policy-Improvement-for-Dynamic-Manipulation" class="headerlink" title="In-Context Iterative Policy Improvement for Dynamic Manipulation"></a>In-Context Iterative Policy Improvement for Dynamic Manipulation</h2><p><strong>Authors:Mark Van der Merwe, Devesh Jha</strong></p>
<p>Attention-based architectures trained on internet-scale language data have demonstrated state of the art reasoning ability for various language-based tasks, such as logic problems and textual reasoning. Additionally, these Large Language Models (LLMs) have exhibited the ability to perform few-shot prediction via in-context learning, in which input-output examples provided in the prompt are generalized to new inputs. This ability furthermore extends beyond standard language tasks, enabling few-shot learning for general patterns. In this work, we consider the application of in-context learning with pre-trained language models for dynamic manipulation. Dynamic manipulation introduces several crucial challenges, including increased dimensionality, complex dynamics, and partial observability. To address this, we take an iterative approach, and formulate our in-context learning problem to predict adjustments to a parametric policy based on previous interactions. We show across several tasks in simulation and on a physical robot that utilizing in-context learning outperforms alternative methods in the low data regime. Video summary of this work and experiments can be found <a target="_blank" rel="noopener" href="https://youtu.be/2inxpdrq74U?si=dAdDYsUEr25nZvRn">https://youtu.be/2inxpdrq74U?si=dAdDYsUEr25nZvRn</a>. </p>
<blockquote>
<p>基于互联网规模语言数据训练的注意力架构在各种基于语言的任务上表现出了最先进的推理能力，如逻辑问题和文本推理。此外，这些大型语言模型（LLM）还表现出了通过上下文学习进行少样本预测的能力，其中在提示中提供的输入输出示例可以推广到新的输入。这种能力还超越了标准语言任务，能够实现一般模式的少样本学习。在这项工作中，我们考虑了使用预训练语言模型进行上下文学习的应用，用于动态操作。动态操作带来了几个关键挑战，包括维度增加、动力复杂和局部可观测性。为了解决这一问题，我们采用迭代方法，并制定相应的上下文学习问题，以根据之前的交互预测参数策略的调整。我们在模拟和实体机器人上的多个任务中都表明，利用上下文学习优于在低数据环境下的其他方法。这项工作和实验的视频摘要请参见：<a target="_blank" rel="noopener" href="https://youtu.be/2inxpdrq74U?si=dAdDYsUEr25nZvRn">链接</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15021v1">PDF</a> 14 pages. Accepted at CoRL 2025</p>
<p><strong>Summary</strong></p>
<p>互联网规模的语料库训练的基于注意力机制的架构展现出先进语言任务中的逻辑理解和文本推理能力。此外，大型语言模型具备通过上下文学习的少量样本预测能力，可推广至新输入。本研究探讨将预训练语言模型的上下文学习应用于动态操纵。动态操纵面临高维性、复杂动态及部分可观等挑战。本研究采取迭代方法，解决基于之前交互调整参数策略的上下文学习问题。模拟任务和实际机器人实验证明，在数据稀缺情况下，上下文学习方法优于其他方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>基于注意力机制的架构在互联网规模的语料库上展现出强大的逻辑理解和文本推理能力。</li>
<li>大型语言模型具备通过上下文学习的少量样本预测能力。</li>
<li>预训练语言模型的上下文学习被应用于动态操纵领域。</li>
<li>动态操纵面临高维性、复杂动态及部分可观性等挑战。</li>
<li>研究采用迭代方法解决基于之前交互调整参数策略的上下文学习问题。</li>
<li>模拟任务显示上下文学习方法在数据稀缺情况下的优越性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15021">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-e78a45f1a9ad2e62ef1c3af12f67cabb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-21108ab8299f08dfc0b92de1f85ef28c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-976f31de346ee78c33c9d10cf491857c.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="MCPTox-A-Benchmark-for-Tool-Poisoning-Attack-on-Real-World-MCP-Servers"><a href="#MCPTox-A-Benchmark-for-Tool-Poisoning-Attack-on-Real-World-MCP-Servers" class="headerlink" title="MCPTox: A Benchmark for Tool Poisoning Attack on Real-World MCP Servers"></a>MCPTox: A Benchmark for Tool Poisoning Attack on Real-World MCP Servers</h2><p><strong>Authors:Zhiqiang Wang, Yichao Gao, Yanting Wang, Suyuan Liu, Haifeng Sun, Haoran Cheng, Guanquan Shi, Haohua Du, Xiangyang Li</strong></p>
<p>By providing a standardized interface for LLM agents to interact with external tools, the Model Context Protocol (MCP) is quickly becoming a cornerstone of the modern autonomous agent ecosystem. However, it creates novel attack surfaces due to untrusted external tools. While prior work has focused on attacks injected through external tool outputs, we investigate a more fundamental vulnerability: Tool Poisoning, where malicious instructions are embedded within a tool’s metadata without execution. To date, this threat has been primarily demonstrated through isolated cases, lacking a systematic, large-scale evaluation.   We introduce MCPTox, the first benchmark to systematically evaluate agent robustness against Tool Poisoning in realistic MCP settings. MCPTox is constructed upon 45 live, real-world MCP servers and 353 authentic tools. To achieve this, we design three distinct attack templates to generate a comprehensive suite of 1312 malicious test cases by few-shot learning, covering 10 categories of potential risks. Our evaluation on 20 prominent LLM agents setting reveals a widespread vulnerability to Tool Poisoning, with o1-mini, achieving an attack success rate of 72.8%. We find that more capable models are often more susceptible, as the attack exploits their superior instruction-following abilities. Finally, the failure case analysis reveals that agents rarely refuse these attacks, with the highest refused rate (Claude-3.7-Sonnet) less than 3%, demonstrating that existing safety alignment is ineffective against malicious actions that use legitimate tools for unauthorized operation. Our findings create a crucial empirical baseline for understanding and mitigating this widespread threat, and we release MCPTox for the development of verifiably safer AI agents. Our dataset is available at an anonymized repository: \textit{<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/AAAI26-7C02%7D">https://anonymous.4open.science/r/AAAI26-7C02}</a>. </p>
<blockquote>
<p>模型上下文协议（MCP）通过为LLM代理提供一个与外部工具交互的标准接口，迅速成为现代自主代理生态系统的基础。然而，由于不受信任的外部工具，它会产生新的攻击面。虽然以前的工作主要集中在通过外部工具输出注入的攻击上，但我们调查了一个更基本的漏洞：工具中毒，恶意指令嵌入在工具元数据中而不执行。迄今为止，这一威胁主要通过孤立案例演示，缺乏系统、大规模的评价。我们引入了MCPTox，这是第一个系统地评估代理在现实MCP环境中抵御工具中毒的稳健性的基准测试。MCPTox建立在45个实时、真实的MCP服务器和353个真实工具之上。为此，我们设计了三种独特的攻击模板，通过少量学习生成了1312个恶意测试用例的综合套件，涵盖10类潜在风险。我们对20个突出的LLM代理环境的评估显示，工具中毒漏洞普遍存在，其中o1-mini的攻击成功率达到72.8%。我们发现，更强大的模型往往更容易受到攻击，因为攻击利用了它们出色的指令执行能力。最后，失败案例分析显示，代理很少会拒绝这些攻击，拒绝率最高的代理（Claude-3.7-Sonnet）不到3%，这表明现有的安全对齐对于使用合法工具进行未经授权操作的恶意行为是无效的。我们的发现对于理解和缓解这一广泛威胁创造了重要的实证基线，我们发布MCPTox以促进开发可验证的更安全的AI代理。我们的数据集可在匿名仓库中找到：[<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/AAAI26-7C0">https://anonymous.4open.science/r/AAAI26-7C0</a><br>]中提出了一个重要的问题：“这个仓库会持续更新和维护吗？”回应中提到他们会持续关注相关的安全漏洞并进行相应的更新和改进措施来保证数据集的质量和安全性确保用户在使用过程中的安全性和准确性同时强调了这是一个持续性的过程他们也将与其他研究者和安全专家合作以确保数据集的准确性和完整性以支持对人工智能安全性问题的深入研究并帮助开发者创建更安全的AI系统。我们的数据集可以适应人工智能的进步并提供持久的价值这是一个至关重要的经验性基准可以让我们更深入地了解工具中毒问题并指导未来的AI开发工作以解决各种潜在的威胁随着人工智能的发展和应用场景的扩展这些问题将会变得越来越重要我们也希望能够与同行共同推动人工智能安全性的进步并促进人工智能的健康发展。](<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/AAAI26-7C%E)%E4%BB%A5%E4%B8%8B%E6%98%AF%E8%BF%99%E6%AE%B5%E7%BF%BB%E8%AF%91%E7%9A%84%E6%9B%B4%E5%AE%8C%E6%95%B4%E7%89%88%E6%9C%AC%EF%BC%9A">https://anonymous.4open.science/r/AAAI26-7C%E)以下是这段翻译的更完整版本：</a></p>
</blockquote>
<p><strong>翻译</strong></p>
<p>模型上下文协议（MCP）为LLM代理提供了一个与外部工具交互的标准接口，已经成为现代自主代理生态系统的重要基石。然而，它同时也带来了新的安全隐患，因为不受信任的外界工具可能会产生新型攻击面。之前的研究主要关注通过外部工具输出进行的攻击，但我们发现了一种更为基础的漏洞——工具中毒。在这个漏洞中，恶意指令被嵌入到工具的元数据中而不被执行。尽管已有一些孤立案例的展示，但这一威胁缺乏系统、大规模的评价。</p>
<p>为了解决这个问题，我们引入了MCPTox基准测试，它是第一个能够系统地评估代理在真实MCP环境下抵御工具中毒的稳健性的测试平台。MCPTox建立在45个实时、真实的MCP服务器和353个真实工具的基础上。我们设计了三种独特的攻击模板，通过少量学习生成了涵盖10类潜在风险的1312个恶意测试用例的综合套件。</p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14925v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>模型上下文协议（MCP）为LLM代理与外部工具交互提供了标准化接口，成为现代自主代理生态系统的重要组成部分。然而，由于不受信任的外部工具，它会产生新的攻击面。本文主要研究一种名为“工具中毒”的新型威胁，即恶意指令嵌入在工具的元数据中而无需执行。为了系统地评估代理对工具中毒的鲁棒性，我们引入了MCPTox，它是第一个在真实MCP环境下评估代理鲁棒性的基准测试。我们的实验基于45个实时MCP服务器和353个真实工具进行构建，通过三种独特的攻击模板生成了涵盖10种潜在风险类别的1312个恶意测试用例。对20种主流的LLM代理环境的评估显示，工具中毒攻击普遍存在，o1-mini的攻击成功率高达72.8%。我们发现更强大的模型往往更容易受到攻击，因为攻击正是利用了他们出色的指令执行能力。最后，失败案例分析表明，代理很少拒绝这些攻击，最高拒绝率（Claude-3.7-Sonnet）不到3%，表明现有的安全对齐对于使用合法工具进行未经授权操作恶意行为无效。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>模型上下文协议（MCP）为LLM代理与外部工具交互提供了标准化接口，成为现代自主代理生态系统的重要基石。</li>
<li>由于不受信任的外部工具，MCP产生了新的攻击面，出现了一种新的威胁——“工具中毒”。</li>
<li>MCPTox是首个用于在真实MCP环境下评估代理对工具中毒鲁棒性的基准测试。</li>
<li>实验基于45个实时MCP服务器和353个真实工具构建，生成了涵盖10种潜在风险类别的1312个恶意测试用例。</li>
<li>对20种主流LLM代理环境的评估显示，工具中毒攻击普遍存在，且更强大的模型更易受到攻击。</li>
<li>现有安全对齐措施对于工具中毒攻击效果不佳，代理很少能够拒绝这些攻击。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14925">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-027df6f96e4cae40a4056d77bacb81c8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8c71636c8c8b563a7ad96d90934e4da2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-57f43cd38d8c52c6b3781f94eb45b6a2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-de5511bdf3b1643cd3451ae5622d4375.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3288588e3ded7f04ac3c0ecd621b45a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45a23b8441014f15afb35a7b13d32be6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3e4c8f63a81d016d95168b7e7ee16ade.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="DictAS-A-Framework-for-Class-Generalizable-Few-Shot-Anomaly-Segmentation-via-Dictionary-Lookup"><a href="#DictAS-A-Framework-for-Class-Generalizable-Few-Shot-Anomaly-Segmentation-via-Dictionary-Lookup" class="headerlink" title="DictAS: A Framework for Class-Generalizable Few-Shot Anomaly   Segmentation via Dictionary Lookup"></a>DictAS: A Framework for Class-Generalizable Few-Shot Anomaly   Segmentation via Dictionary Lookup</h2><p><strong>Authors:Zhen Qu, Xian Tao, Xinyi Gong, ShiChen Qu, Xiaopei Zhang, Xingang Wang, Fei Shen, Zhengtao Zhang, Mukesh Prasad, Guiguang Ding</strong></p>
<p>Recent vision-language models (e.g., CLIP) have demonstrated remarkable class-generalizable ability to unseen classes in few-shot anomaly segmentation (FSAS), leveraging supervised prompt learning or fine-tuning on seen classes. However, their cross-category generalization largely depends on prior knowledge of real seen anomaly samples. In this paper, we propose a novel framework, namely DictAS, which enables a unified model to detect visual anomalies in unseen object categories without any retraining on the target data, only employing a few normal reference images as visual prompts. The insight behind DictAS is to transfer dictionary lookup capabilities to the FSAS task for unseen classes via self-supervised learning, instead of merely memorizing the normal and abnormal feature patterns from the training set. Specifically, DictAS mainly consists of three components: (1) Dictionary Construction - to simulate the index and content of a real dictionary using features from normal reference images. (2) Dictionary Lookup - to retrieve queried region features from the dictionary via a sparse lookup strategy. When a query feature cannot be retrieved, it is classified as an anomaly. (3) Query Discrimination Regularization - to enhance anomaly discrimination by making abnormal features harder to retrieve from the dictionary. To achieve this, Contrastive Query Constraint and Text Alignment Constraint are further proposed. Extensive experiments on seven public industrial and medical datasets demonstrate that DictAS consistently outperforms state-of-the-art FSAS methods. </p>
<blockquote>
<p>最近的视觉语言模型（例如CLIP）在少样本异常分割（FSAS）中展示了显著的类别泛化能力，这得益于在可见类别上进行的监督提示学习或微调。然而，它们的跨类别泛化很大程度上依赖于真实可见异常样本的先验知识。在本文中，我们提出了一种新型框架，名为DictAS，它能够在目标数据上无需任何再训练，仅使用少量正常参考图像作为视觉提示，即可检测未见对象类别的视觉异常。DictAS的见解是通过自我监督学习，将字典查找能力转移到FSAS任务中的未见类别，而不是仅仅从训练集中记忆正常和异常的特征模式。具体来说，DictAS主要由三个部分组成：（1）字典构建——使用正常参考图像的特征模拟真实字典的索引和内容。（2）字典查找——通过稀疏查找策略从字典中检索查询区域特征。当无法检索到查询特征时，它会被归类为异常。（3）查询判别正则化——通过使异常特征更难从字典中检索，以增强异常判别。为此，进一步提出了对比查询约束和文本对齐约束。在七个公共工业和医疗数据集上的大量实验表明，DictAS始终优于最新的FSAS方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13560v2">PDF</a> Accepted by ICCV 2025, Project: <a target="_blank" rel="noopener" href="https://github.com/xiaozhen228/DictAS">https://github.com/xiaozhen228/DictAS</a></p>
<p><strong>Summary</strong></p>
<p>本文提出了一种名为DictAS的新框架，用于在未训练的目标数据上检测未见对象类别中的视觉异常。该框架通过模拟真实字典的索引和内容，利用正常参考图像的特征构建字典，并通过稀疏检索策略检索查询区域特征。当无法检索到查询特征时，将其分类为异常。此外，还通过对比查询约束和文本对齐约束增强异常检测能力。在七个公共工业和医疗数据集上的实验表明，DictAS在未见类别异常分割（FSAS）任务上始终优于现有方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DictAS框架用于在未训练的目标数据上检测未见对象类别中的视觉异常。</li>
<li>DictAS利用正常参考图像的特征构建字典，模拟真实字典的索引和内容。</li>
<li>通过稀疏检索策略检索查询区域特征，无法检索到的特征被视为异常。</li>
<li>通过对比查询约束和文本对齐约束增强异常检测能力。</li>
<li>DictAS框架包括三个主要组件：字典构建、字典查找和查询判别正则化。</li>
<li>实验表明，DictAS在七个公共数据集上的一致表现优于现有FSAS方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13560">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-e49a66ab2261e31b24def0d322fa2b50.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-401706d57dbafcb784033b9856ecf607.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7d2ffe3a4f50d5311fbe18eae2380d36.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a379e50efa4f973d4e20403d7a00f4d.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="CC-Time-Cross-Model-and-Cross-Modality-Time-Series-Forecasting"><a href="#CC-Time-Cross-Model-and-Cross-Modality-Time-Series-Forecasting" class="headerlink" title="CC-Time: Cross-Model and Cross-Modality Time Series Forecasting"></a>CC-Time: Cross-Model and Cross-Modality Time Series Forecasting</h2><p><strong>Authors:Peng Chen, Yihang Wang, Yang Shu, Yunyao Cheng, Kai Zhao, Zhongwen Rao, Lujia Pan, Bin Yang, Chenjuan Guo</strong></p>
<p>With the success of pre-trained language models (PLMs) in various application fields beyond natural language processing, language models have raised emerging attention in the field of time series forecasting (TSF) and have shown great prospects. However, current PLM-based TSF methods still fail to achieve satisfactory prediction accuracy matching the strong sequential modeling power of language models. To address this issue, we propose Cross-Model and Cross-Modality Learning with PLMs for time series forecasting (CC-Time). We explore the potential of PLMs for time series forecasting from two aspects: 1) what time series features could be modeled by PLMs, and 2) whether relying solely on PLMs is sufficient for building time series models. In the first aspect, CC-Time incorporates cross-modality learning to model temporal dependency and channel correlations in the language model from both time series sequences and their corresponding text descriptions. In the second aspect, CC-Time further proposes the cross-model fusion block to adaptively integrate knowledge from the PLMs and time series model to form a more comprehensive modeling of time series patterns. Extensive experiments on nine real-world datasets demonstrate that CC-Time achieves state-of-the-art prediction accuracy in both full-data training and few-shot learning situations. </p>
<blockquote>
<p>随着预训练语言模型（PLMs）在自然语言处理以外的各种应用领域的成功，语言模型在时间序列预测（TSF）领域引起了广泛的关注，并显示出巨大的潜力。然而，当前的基于PLM的时间序列预测方法仍然无法实现对时间序列模式进行强有力建模的满意预测精度。为了解决这一问题，我们提出了基于PLM的跨模型跨模态时间序列预测方法（CC-Time）。我们从两个方面探讨了PLM在时间序列预测中的潜力：1）PLM可以建模哪些时间序列特征；2）仅依赖PLM是否足以构建时间序列模型。在第一方面，CC-Time结合了跨模态学习，从时间序列序列及其相应的文本描述中，对语言模型中的时间依赖性和通道相关性进行建模。在第二方面，CC-Time进一步提出了跨模型融合模块，以自适应地整合来自PLM和时序模型的知识，形成对时序模式更全面建模。在九个真实世界数据集上的大量实验表明，CC-Time在全数据训练和少样本学习情况下均达到了最新的预测精度。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.12235v2">PDF</a> </p>
<p><strong>Summary</strong><br>基于预训练语言模型（PLMs）在自然语言处理以外领域的应用成功，时间序列预测（TSF）领域开始关注语言模型并展现出巨大潜力。针对当前PLM-based TSF方法预测精度不足的问题，提出Cross-Model和Cross-Modality Learning with PLMs for time series forecasting（CC-Time）。从两个方面探索PLM在TSF中的潜力：一是PLM可建模的时间序列特征；二是仅依赖PLM是否足以构建时间序列模型。CC-Time通过跨模态学习建模时间序列序列和对应文本描述中的时间依赖和通道相关性，并提出跨模型融合块以自适应地整合PLM和TSF模型的知识，以更全面建模时间序列模式。在九个真实数据集上的实验表明，CC-Time在完全数据训练和少样本学习情况下均实现了最先进的预测精度。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>预训练语言模型（PLMs）在多个领域取得成功，引发时间序列预测（TSF）领域的关注。</li>
<li>当前PLM-based TSF方法预测精度不足，需要新的方法改进。</li>
<li>提出CC-Time方法，从两个方面探索PLM在TSF中的潜力：时间序列特征的建模和构建时间序列模型的充分性。</li>
<li>CC-Time通过跨模态学习建模时间序列序列和文本描述中的时间依赖和通道相关性。</li>
<li>CC-Time提出跨模型融合块，自适应整合PLM和TSF模型知识，实现更全面建模时间序列模式。</li>
<li>在九个真实数据集上的实验表明，CC-Time在多种情况下实现了最先进的预测精度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.12235">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-687c1a6dfe880300d526e122ccfdb413.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-039d5e60ec124b362ec8ede5270afae8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8e9808ff564f5e19ce16e0846f0654a1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dfbbfdc89027582bc4d94ccedfa3c8e8.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Prescriptive-Agents-based-on-RAG-for-Automated-Maintenance-PARAM"><a href="#Prescriptive-Agents-based-on-RAG-for-Automated-Maintenance-PARAM" class="headerlink" title="Prescriptive Agents based on RAG for Automated Maintenance (PARAM)"></a>Prescriptive Agents based on RAG for Automated Maintenance (PARAM)</h2><p><strong>Authors:Chitranshu Harbola, Anupam Purwar</strong></p>
<p>Industrial machinery maintenance requires timely intervention to prevent catastrophic failures and optimize operational efficiency. This paper presents an integrated Large Language Model (LLM)-based intelligent system for prescriptive maintenance that extends beyond traditional anomaly detection to provide actionable maintenance recommendations. Building upon our prior LAMP framework for numerical data analysis, we develop a comprehensive solution that combines bearing vibration frequency analysis with multi agentic generation for intelligent maintenance planning. Our approach serializes bearing vibration data (BPFO, BPFI, BSF, FTF frequencies) into natural language for LLM processing, enabling few-shot anomaly detection with high accuracy. The system classifies fault types (inner race, outer race, ball&#x2F;roller, cage faults) and assesses severity levels. A multi-agentic component processes maintenance manuals using vector embeddings and semantic search, while also conducting web searches to retrieve comprehensive procedural knowledge and access up-to-date maintenance practices for more accurate and in-depth recommendations. The Gemini model then generates structured maintenance recommendations includes immediate actions, inspection checklists, corrective measures, parts requirements, and timeline specifications. Experimental validation in bearing vibration datasets demonstrates effective anomaly detection and contextually relevant maintenance guidance. The system successfully bridges the gap between condition monitoring and actionable maintenance planning, providing industrial practitioners with intelligent decision support. This work advances the application of LLMs in industrial maintenance, offering a scalable framework for prescriptive maintenance across machinery components and industrial sectors. </p>
<blockquote>
<p>工业机械维护需要及时的干预来预防灾难性故障并优化运营效率。本文提出了一种基于大型语言模型（LLM）的集成智能系统，用于预防性维护，该系统超越了传统的异常检测，提供了可行的维护建议。在先前用于数值数据分析的LAMP框架的基础上，我们开发了一种综合解决方案，该方案将轴承振动频率分析与多智能体生成相结合，用于智能维护规划。我们的方法将轴承振动数据（BPFO、BPFI、BSF、FTF频率）序列化为自然语言以供LLM处理，从而实现高准确度的少量异常检测。该系统对故障类型（内圈、外圈、球&#x2F;滚珠、笼故障）进行分类并评估严重程度。多智能体组件使用向量嵌入和语义搜索处理维护手册，同时进行网络搜索以检索全面的程序知识并获取最新维护实践，以提供更准确和深入的推荐。然后，Gemini模型生成结构化维护建议，包括立即行动、检查清单、纠正措施、零件要求和时间表规范。在轴承振动数据集上的实验验证表明，系统能够有效地检测异常并提供与上下文相关的维护指导。该系统成功地弥合了状况监测和可操作维护规划之间的鸿沟，为工业从业者提供智能决策支持。这项工作推动了LLM在工业维护领域的应用，为机械部件和工业部门的预防性维护提供了一个可扩展的框架。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04714v2">PDF</a> </p>
<p><strong>Summary</strong>:<br>工业机械维护需要及时的干预来预防灾难性故障并优化运营效率。本文提出了一种基于大型语言模型（LLM）的集成智能系统，用于预防性维护，该系统不仅超越了传统的异常检测，而且提供了可操作的维护建议。该系统结合轴承振动频率分析和多智能体生成，实现了智能化的维护计划。通过序列化轴承振动数据并进行自然语言处理，实现了高准确度的少数异常检测。系统可以分类故障类型并评估严重程度。此外，该系统还通过向量嵌入和语义搜索处理维护手册，进行网络搜索以获取全面的程序性知识和最新的维护实践，以提供更准确和深入的推荐。通过轴承振动数据集的实验验证，该系统实现了有效的异常检测和与上下文相关的维护指导。这项工作推动了LLM在工业维护领域的应用，为机械部件和工业部门提供了可扩展的预防性维护框架。</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>大型语言模型（LLM）用于工业机械的智能维护系统。</li>
<li>系统结合轴承振动频率分析与多智能体生成，提供维护建议。</li>
<li>通过自然语言处理实现少数异常检测的高准确性。</li>
<li>系统能分类故障类型和评估严重程度。</li>
<li>通过处理维护手册和网络搜索，系统提供更准确和深入的维护建议。</li>
<li>实验验证表明，系统实现了有效的异常检测和与上下文相关的维护指导。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04714">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-99f24ef02a7e7288b085b18a70597849.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-833d53ccf48f02fd0f28e242433f157a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-659d4eb6d0e68fcd3a48ee5cc5b2568d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e444f740ceaeb9b313bbe1260e335924.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-97150f109527f773126066ce74a320b0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="AraReasoner-Evaluating-Reasoning-Based-LLMs-for-Arabic-NLP"><a href="#AraReasoner-Evaluating-Reasoning-Based-LLMs-for-Arabic-NLP" class="headerlink" title="AraReasoner: Evaluating Reasoning-Based LLMs for Arabic NLP"></a>AraReasoner: Evaluating Reasoning-Based LLMs for Arabic NLP</h2><p><strong>Authors:Ahmed Hasanaath, Aisha Alansari, Ahmed Ashraf, Chafik Salmane, Hamzah Luqman, Saad Ezzini</strong></p>
<p>Large language models (LLMs) have shown remarkable progress in reasoning abilities and general natural language processing (NLP) tasks, yet their performance on Arabic data, characterized by rich morphology, diverse dialects, and complex script, remains underexplored. This paper presents a comprehensive benchmarking study of multiple reasoning-focused LLMs, with a special emphasis on the newly introduced DeepSeek models, across a suite of fifteen Arabic NLP tasks. We experiment with various strategies, including zero-shot, few-shot, and fine-tuning. This allows us to systematically evaluate performance on datasets covering a range of applications to examine their capacity for linguistic reasoning under different levels of complexity. Our experiments reveal several key findings. First, carefully selecting just three in-context examples delivers an average uplift of over 13 F1 points on classification tasks-boosting sentiment analysis from 35.3% to 87.5% and paraphrase detection from 56.1% to 87.0%. Second, reasoning-focused DeepSeek architectures outperform a strong GPT o4-mini baseline by an average of 12 F1 points on complex inference tasks in the zero-shot setting. Third, LoRA-based fine-tuning yields up to an additional 8 points in F1 and BLEU compared to equivalent increases in model scale. The code is available at <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/AraReasoner41299">https://anonymous.4open.science/r/AraReasoner41299</a> </p>
<blockquote>
<p>大型语言模型（LLM）在推理能力和自然语言处理（NLP）的一般任务上取得了显著的进步，然而，它们在处理阿拉伯数据方面的表现，这些阿拉伯数据以丰富的形态、多样的方言和复杂的脚本为特征，仍被研究得不够透彻。本文全面评估了多个注重推理的LLM模型，特别强调新推出的DeepSeek模型，涵盖十五项阿拉伯语NLP任务。我们尝试了多种策略，包括零样本、少样本和微调。这使我们能够系统地评估数据集上的性能，涵盖各种应用程序，以检查它们在不同复杂程度下的语言推理能力。我们的实验揭示了几个关键发现。首先，只需精心选择三个上下文实例，就可以在分类任务上平均提高超过13个F1点——情感分析从35.3%提高到87.5%，而改述检测从56.1%提高到87.0%。其次，注重推理的DeepSeek架构在零样本设置下的复杂推理任务上平均比强大的GPT o4-mini基线高出12个F1点。第三，基于LoRA的微调与模型规模的等效增长相比，额外提高了高达8个F1点和BLEU值。代码可用<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/AraReasoner41299%E6%9F%A5%E8%AF%A2%E3%80%82">https://anonymous.4open.science/r/AraReasoner41299查询。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08768v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLMs）在具有丰富形态、多样方言和复杂脚本的阿拉伯语数据上性能尚待探索。本文通过一系列阿拉伯NLP任务全面评估多个注重推理的LLMs，尤其是新推出的DeepSeek模型。研究发现，精选三个实例的few-shot方式能提高分类任务的平均F1分数超过13点；DeepSeek架构在零样本设置下比GPT o4-mini基线平均高出12 F1点；基于LoRA的微调技术相比模型规模的增加可提高最多8点的F1和BLEU分数。相关代码已公开分享。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>在阿拉伯语NLP任务中，大型语言模型（LLMs）的性能仍有待充分探索。</li>
<li>通过一系列实验评估了多种推理型LLMs，尤其是新出现的DeepSeek模型。</li>
<li>精选三个实例的few-shot方式显著提高分类任务性能。</li>
<li>DeepSeek架构在零样本设置下表现出优越性能。</li>
<li>LoRA微调技术能有效提升模型性能。</li>
<li>实验结果详细说明了不同策略对阿拉伯语NLP任务性能的影响。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08768">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-85909eb455fa75a12e4c53f27797c2cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-98eb100cc13fdad5032bac7377f6403e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-49480addd61a026709ac53c33ba9859f.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Embodied-Long-Horizon-Manipulation-with-Closed-loop-Code-Generation-and-Incremental-Few-shot-Adaptation"><a href="#Embodied-Long-Horizon-Manipulation-with-Closed-loop-Code-Generation-and-Incremental-Few-shot-Adaptation" class="headerlink" title="Embodied Long Horizon Manipulation with Closed-loop Code Generation and   Incremental Few-shot Adaptation"></a>Embodied Long Horizon Manipulation with Closed-loop Code Generation and   Incremental Few-shot Adaptation</h2><p><strong>Authors:Yuan Meng, Xiangtong Yao, Haihui Ye, Yirui Zhou, Shengqiang Zhang, Zhenguo Sun, Xukun Li, Zhenshan Bing, Alois Knoll</strong></p>
<p>Embodied long-horizon manipulation requires robotic systems to process multimodal inputs-such as vision and natural language-and translate them into executable actions. However, existing learning-based approaches often depend on large, task-specific datasets and struggle to generalize to unseen scenarios. Recent methods have explored using large language models (LLMs) as high-level planners that decompose tasks into subtasks using natural language and guide pretrained low-level controllers. Yet, these approaches assume perfect execution from low-level policies, which is unrealistic in real-world environments with noise or suboptimal behaviors. To overcome this, we fully discard the pretrained low-level policy and instead use the LLM to directly generate executable code plans within a closed-loop framework. Our planner employs chain-of-thought (CoT)-guided few-shot learning with incrementally structured examples to produce robust and generalizable task plans. Complementing this, a reporter evaluates outcomes using RGB-D and delivers structured feedback, enabling recovery from misalignment and replanning under partial observability. This design eliminates per-step inference, reduces computational overhead, and limits error accumulation that was observed in previous methods. Our framework achieves state-of-the-art performance on 30+ diverse seen and unseen long-horizon tasks across LoHoRavens, CALVIN, Franka Kitchen, and cluttered real-world settings. </p>
<blockquote>
<p>实现具有长期视角的操作需要机器人系统处理多模态输入，如视觉和自然语言，并将其翻译成可执行的行动。然而，现有的基于学习的方法通常依赖于大型、特定任务的数据集，并且在未见场景中的泛化能力较差。最近的方法已经尝试使用大型语言模型（LLM）作为高级规划器，利用自然语言将任务分解成子任务，并引导预训练的低级控制器。然而，这些方法假设低级策略可以完美执行，这在现实世界中存在噪声或非最优行为的情况下是不切实际的。为了克服这一问题，我们完全抛弃了预训练的低级策略，而是使用LLM直接在闭环框架内生成可执行的代码计划。我们的规划器采用基于思维链（CoT）引导的小样本学习，通过增量结构化示例来产生稳健且可泛化的任务计划。作为补充，一个报告者使用RGB-D评估结果并提供结构化反馈，从而实现部分观测下的误对齐恢复和重新规划。这种设计消除了逐步推理，减少了计算开销，并限制了之前在其它方法中观察到的误差累积。我们的框架在LoHoRavens、CALVIN、Franka厨房和杂乱的真实世界环境中，实现了30多个不同已见和未见长期任务的最佳性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21969v3">PDF</a> update ICRA 6 page</p>
<p><strong>Summary</strong></p>
<p>基于多模态输入的长期规划操控需要机器人系统处理视觉和自然语言等多种模式输入，并将其转化为可执行动作。现有学习法依赖于特定任务的大量数据集，难以推广到未见场景。近期方法尝试使用大型语言模型作为高级规划器，但假设低级策略的完美执行并不现实。为此，我们抛弃预训练的低级策略，改用大型语言模型直接生成封闭循环框架内的可执行代码计划。此外，结合使用RGB-D进行结果评估并提供结构化反馈，促进误对齐情况下的恢复与部分观察下的重新规划。此设计消除了逐步推理，减少了计算开销，并限制了先前方法观察到的误差累积。我们的框架在多个长周期任务上实现了卓越性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>机器人系统需要处理多模态输入，如视觉和自然语言，转化为可执行动作。</li>
<li>现有方法依赖于大量特定任务数据集，难以适应新场景。</li>
<li>大型语言模型用于高级任务规划，但完美执行假设不现实。</li>
<li>抛弃预训练的低级策略，直接使用大型语言模型生成代码计划。</li>
<li>结合RGB-D进行结果评估并提供结构化反馈。</li>
<li>该设计减少计算开销和误差累积。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21969">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-477e5b2528e669fb1d9aaa17210cad7a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d34edef9b411ac4c1c28be81f232fe2e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-de8b11308af9cce233e856bbc878d26f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-506702e2a4341bcdd7004138af3c3667.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-88d516f7bc4a2ca9c12ad57a136bebea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a5a6b9f2350360c95e1ce5bfd4a7db9c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-77846ad7794efa874f08b1c4773e28f0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Revisiting-Out-of-Distribution-Detection-in-Real-time-Object-Detection-From-Benchmark-Pitfalls-to-a-New-Mitigation-Paradigm"><a href="#Revisiting-Out-of-Distribution-Detection-in-Real-time-Object-Detection-From-Benchmark-Pitfalls-to-a-New-Mitigation-Paradigm" class="headerlink" title="Revisiting Out-of-Distribution Detection in Real-time Object Detection:   From Benchmark Pitfalls to a New Mitigation Paradigm"></a>Revisiting Out-of-Distribution Detection in Real-time Object Detection:   From Benchmark Pitfalls to a New Mitigation Paradigm</h2><p><strong>Authors:Changshun Wu, Weicheng He, Chih-Hong Cheng, Xiaowei Huang, Saddek Bensalem</strong></p>
<p>Out-of-distribution (OoD) inputs pose a persistent challenge to deep learning models, often triggering overconfident predictions on non-target objects. While prior work has primarily focused on refining scoring functions and adjusting test-time thresholds, such algorithmic improvements offer only incremental gains. We argue that a rethinking of the entire development lifecycle is needed to mitigate these risks effectively. This work addresses two overlooked dimensions of OoD detection in object detection. First, we reveal fundamental flaws in widely used evaluation benchmarks: contrary to their design intent, up to 13% of objects in the OoD test sets actually belong to in-distribution classes, and vice versa. These quality issues severely distort the reported performance of existing methods and contribute to their high false positive rates. Second, we introduce a novel training-time mitigation paradigm that operates independently of external OoD detectors. Instead of relying solely on post-hoc scoring, we fine-tune the detector using a carefully synthesized OoD dataset that semantically resembles in-distribution objects. This process shapes a defensive decision boundary by suppressing objectness on OoD objects, leading to a 91% reduction in hallucination error of a YOLO model on BDD-100K. Our methodology generalizes across detection paradigms such as YOLO, Faster R-CNN, and RT-DETR, and supports few-shot adaptation. Together, these contributions offer a principled and effective way to reduce OoD-induced hallucination in object detectors. Code and data are available at: <a target="_blank" rel="noopener" href="https://gricad-gitlab.univ-grenoble-alpes.fr/dnn-safety/m-hood">https://gricad-gitlab.univ-grenoble-alpes.fr/dnn-safety/m-hood</a>. </p>
<blockquote>
<p>分布外（Out-of-distribution，OoD）输入对深度学习模型构成持续挑战，通常会对非目标对象做出过于自信的预测。虽然先前的工作主要集中于改进评分功能和调整测试时间阈值，但此类算法改进只能带来增量收益。我们认为，为了有效减轻这些风险，需要重新思考整个开发生命周期。这项工作解决了对象检测中分布外检测（OoD检测）被忽略的两个维度。首先，我们揭示了广泛使用的评估基准中存在的根本问题：与他们的设计初衷相反，高达13%的对象属于分布内类别，反之亦然。这些问题严重影响了现有方法的报告性能，并导致了较高的误报率。其次，我们引入了一种新的训练时间缓解模式，该模式独立于外部分布外检测器运行。我们不是依赖事后评分，而是使用精心合成的分布外数据集对检测器进行微调，该数据集在语义上类似于分布内对象。这一过程通过抑制分布外对象上的对象性来形成防御决策边界，导致YOLO模型在BDD-100K上的幻觉误差降低了91%。我们的方法适用于YOLO、Faster R-CNN和RT-DETR等检测范式，并支持少量镜头适应。这些贡献共同提供了一种有原则且有效的减少对象检测器中的分布外输入引起的幻觉的方法。代码和数据可在：<a target="_blank" rel="noopener" href="https://gricad-gitlab.univ-grenoble-alpes.fr/dnn-safety/m-hood%E3%80%82">https://gricad-gitlab.univ-grenoble-alpes.fr/dnn-safety/m-hood。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07330v3">PDF</a> Expanded journal version of our IROS 2025 paper, adding automated OoD   benchmarking, generalization to multiple object detectors, few-shot   fine-tuning, and in-depth analysis</p>
<p><strong>摘要</strong><br>深度学习方法面临来自分布外（Out-of-Distribution，OoD）输入的持续挑战，容易在非目标对象上产生过度自信的预测。现有研究主要关注评分函数的改进和测试时阈值的调整，但这些算法上的改进只带来了有限的提升。本文重新思考了开发周期中的两个被忽视方面，以解决OoD检测中的关键问题。首先，我们发现常用的评估基准存在根本问题：测试集中有高达13%的对象实际上是分布内的类别，这与设计初衷相反。这些问题严重影响了现有方法的性能报告并导致了高误报率。其次，我们提出了一种新颖的在线缓解范式，独立于外部OoD检测器运行。我们不再依赖事后评分，而是使用精心合成的类似于分布内对象的OoD数据集微调检测器。通过压制OoD对象上的对象性，塑造防御决策边界，YOLO模型的幻觉错误减少了91%。我们的方法适用于YOLO、Faster R-CNN和RT-DETR等检测范式，并支持少量样本适应。这些贡献为解决深度学习方法在对象检测中的分布外输入问题提供了有效的解决方案。更多信息和资源可在链接中找到：<a target="_blank" rel="noopener" href="https://gricad-gitlab.univ-grenoble-alpes.fr/dnn-safety/m-hood">https://gricad-gitlab.univ-grenoble-alpes.fr/dnn-safety/m-hood</a>。</p>
<p><strong>要点归纳</strong></p>
<p>一、现有的对象检测模型面临来自分布外的输入挑战，会导致对非目标对象的过度自信预测。这带来了对新方法的迫切需求。</p>
<p>二、传统的评估基准存在质量缺陷：设计初衷与实际存在差异，可能导致高达13%的对象类别被误报为分布内或分布外。这对评估模型性能造成了严重扭曲。我们提出了相应的解决方法来改善这个问题。</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07330">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-c6c4c66eb6766776627010ba4e243a49.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f657f2faea724bd7f1d739deef4ee9c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f0e15772a987df8ec33a28eae01a7dc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-20b1c1a4e1e4e4be5e39eaaf7f3a59e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b4af4548204b148ca3e5d717348f18b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-731a8fc9e488b35a920a95578f9ada87.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3d654934ef966007a679f6377ad964f2.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Label-Anything-Multi-Class-Few-Shot-Semantic-Segmentation-with-Visual-Prompts"><a href="#Label-Anything-Multi-Class-Few-Shot-Semantic-Segmentation-with-Visual-Prompts" class="headerlink" title="Label Anything: Multi-Class Few-Shot Semantic Segmentation with Visual   Prompts"></a>Label Anything: Multi-Class Few-Shot Semantic Segmentation with Visual   Prompts</h2><p><strong>Authors:Pasquale De Marinis, Nicola Fanelli, Raffaele Scaringi, Emanuele Colonna, Giuseppe Fiameni, Gennaro Vessio, Giovanna Castellano</strong></p>
<p>Few-shot semantic segmentation aims to segment objects from previously unseen classes using only a limited number of labeled examples. In this paper, we introduce Label Anything, a novel transformer-based architecture designed for multi-prompt, multi-way few-shot semantic segmentation. Our approach leverages diverse visual prompts – points, bounding boxes, and masks – to create a highly flexible and generalizable framework that significantly reduces annotation burden while maintaining high accuracy. Label Anything makes three key contributions: ($\textit{i}$) we introduce a new task formulation that relaxes conventional few-shot segmentation constraints by supporting various types of prompts, multi-class classification, and enabling multiple prompts within a single image; ($\textit{ii}$) we propose a novel architecture based on transformers and attention mechanisms; and ($\textit{iii}$) we design a versatile training procedure allowing our model to operate seamlessly across different $N$-way $K$-shot and prompt-type configurations with a single trained model. Our extensive experimental evaluation on the widely used COCO-$20^i$ benchmark demonstrates that Label Anything achieves state-of-the-art performance among existing multi-way few-shot segmentation methods, while significantly outperforming leading single-class models when evaluated in multi-class settings. Code and trained models are available at <a target="_blank" rel="noopener" href="https://github.com/pasqualedem/LabelAnything">https://github.com/pasqualedem/LabelAnything</a>. </p>
<blockquote>
<p>少量样本语义分割旨在使用有限的标注样本对之前未见过的类别进行对象分割。在本文中，我们介绍了Label Anything，这是一种基于transformer的新型架构，专为多提示、多类别少量样本语义分割设计。我们的方法利用多样化的视觉提示——点、边界框和蒙版，创建一个高度灵活和可推广的框架，在保持高准确性的同时，大大降低了标注负担。Label Anything做出了三个主要贡献：（i）我们引入了一种新的任务形式化方法，通过支持各种提示类型、多类分类，并在单个图像内支持多个提示，放宽了传统的少量样本分割约束；（ii）我们提出了一种基于transformer和注意力机制的新型架构；（iii）我们设计了一种通用训练程序，使我们的模型能够在不同的N路K样本和提示类型配置中使用单个训练模型无缝运行。我们在广泛使用的COCO-20i基准测试上的大量实验评估表明，Label Anything在现有的多类别少量样本分割方法中实现了最先进的性能，并且在多类别设置中显著优于领先的单类模型。相关代码和训练好的模型可在<a target="_blank" rel="noopener" href="https://github.com/pasqualedem/LabelAnything%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/pasqualedem/LabelAnything找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.02075v4">PDF</a> ECAI 2025 - 28th European Conference on Artificial Intelligence</p>
<p><strong>Summary</strong></p>
<p>本文介绍了基于Transformer架构的Label Anything模型，用于多提示、多类别少样本语义分割。该模型利用多样化的视觉提示（点、边界框和掩码）创建了一个灵活且可推广的框架，显著减少了标注负担，同时保持了高准确性。Label Anything对少样本语义分割领域做出了三项关键贡献：引入支持多种提示、多类别分类的新任务形式；提出基于Transformer和注意力机制的新架构；设计了一种通用训练程序，使模型能够在不同的N-way K-shot和提示类型配置中无缝运行。在广泛使用的COCO-20i基准测试中，Label Anything在现有的多类别少样本分割方法中取得了最佳性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Label Anything是一个基于Transformer的少样本语义分割模型，支持多提示和多类别分类。</li>
<li>该模型利用点、边界框和掩码等多样化的视觉提示，创建了一个灵活且可推广的框架。</li>
<li>Label Anything显著减少了标注负担，同时保持了高准确性。</li>
<li>该模型做出了三项关键贡献：引入新的任务形式、新的架构设计和通用训练程序。</li>
<li>Label Anything在COCO-20i基准测试中表现最佳，证明了其有效性。</li>
<li>模型支持不同的N-way K-shot和提示类型配置，具有广泛的应用性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.02075">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-943de758e838f749339981471007a3c8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6e32b2483843529dfb32bb4f52c67317.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de0a205bdd88658ebc95bfca2df6d6fe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5a6b5232cb693b7b99e2bfc6e8617a49.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-23/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-23/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-23/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-26cbea695aa2d6f406a4930a75641d36.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-08-23  Are Virtual DES Images a Valid Alternative to the Real Ones?
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-23
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-23/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-20b6292f2fd919823d826d9270025daf.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-08-23  Distributed Detection of Adversarial Attacks in Multi-Agent   Reinforcement Learning with Continuous Action Space
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-23
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30666.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
