<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-08-23  Intern-S1 A Scientific Multimodal Foundation Model">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-149c94281c63fab0be861c1249be1603.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-08-23
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-08-25
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    22.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    93 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-08-23-更新"><a href="#2025-08-23-更新" class="headerlink" title="2025-08-23 更新"></a>2025-08-23 更新</h1><h2 id="Intern-S1-A-Scientific-Multimodal-Foundation-Model"><a href="#Intern-S1-A-Scientific-Multimodal-Foundation-Model" class="headerlink" title="Intern-S1: A Scientific Multimodal Foundation Model"></a>Intern-S1: A Scientific Multimodal Foundation Model</h2><p><strong>Authors:Lei Bai, Zhongrui Cai, Maosong Cao, Weihan Cao, Chiyu Chen, Haojiong Chen, Kai Chen, Pengcheng Chen, Ying Chen, Yongkang Chen, Yu Cheng, Yu Cheng, Pei Chu, Tao Chu, Erfei Cui, Ganqu Cui, Long Cui, Ziyun Cui, Nianchen Deng, Ning Ding, Nanqin Dong, Peijie Dong, Shihan Dou, Sinan Du, Haodong Duan, Caihua Fan, Ben Gao, Changjiang Gao, Jianfei Gao, Songyang Gao, Yang Gao, Zhangwei Gao, Jiaye Ge, Qiming Ge, Lixin Gu, Yuzhe Gu, Aijia Guo, Qipeng Guo, Xu Guo, Conghui He, Junjun He, Yili Hong, Siyuan Hou, Caiyu Hu, Hanglei Hu, Jucheng Hu, Ming Hu, Zhouqi Hua, Haian Huang, Junhao Huang, Xu Huang, Zixian Huang, Zhe Jiang, Lingkai Kong, Linyang Li, Peiji Li, Pengze Li, Shuaibin Li, Tianbin Li, Wei Li, Yuqiang Li, Dahua Lin, Junyao Lin, Tianyi Lin, Zhishan Lin, Hongwei Liu, Jiangning Liu, Jiyao Liu, Junnan Liu, Kai Liu, Kaiwen Liu, Kuikun Liu, Shichun Liu, Shudong Liu, Wei Liu, Xinyao Liu, Yuhong Liu, Zhan Liu, Yinquan Lu, Haijun Lv, Hongxia Lv, Huijie Lv, Qidang Lv, Ying Lv, Chengqi Lyu, Chenglong Ma, Jianpeng Ma, Ren Ma, Runmin Ma, Runyuan Ma, Xinzhu Ma, Yichuan Ma, Zihan Ma, Sixuan Mi, Junzhi Ning, Wenchang Ning, Xinle Pang, Jiahui Peng, Runyu Peng, Yu Qiao, Jiantao Qiu, Xiaoye Qu, Yuan Qu, Yuchen Ren, Fukai Shang, Wenqi Shao, Junhao Shen, Shuaike Shen, Chunfeng Song, Demin Song, Diping Song, Chenlin Su, Weijie Su, Weigao Sun, Yu Sun, Qian Tan, Cheng Tang, Huanze Tang, Kexian Tang, Shixiang Tang, Jian Tong, Aoran Wang, Bin Wang, Dong Wang, Lintao Wang, Rui Wang, Weiyun Wang, Wenhai Wang, Yi Wang, Ziyi Wang, Ling-I Wu, Wen Wu, Yue Wu, Zijian Wu, Linchen Xiao, Shuhao Xing, Chao Xu, Huihui Xu, Jun Xu, Ruiliang Xu, Wanghan Xu, GanLin Yang, Yuming Yang, Haochen Ye, Jin Ye, Shenglong Ye, Jia Yu, Jiashuo Yu, Jing Yu, Fei Yuan, Bo Zhang, Chao Zhang, Chen Zhang, Hongjie Zhang, Jin Zhang, Qiaosheng Zhang, Qiuyinzhe Zhang, Songyang Zhang, Taolin Zhang, Wenlong Zhang, Wenwei Zhang, Yechen Zhang, Ziyang Zhang, Haiteng Zhao, Qian Zhao, Xiangyu Zhao, Xiangyu Zhao, Bowen Zhou, Dongzhan Zhou, Peiheng Zhou, Yuhao Zhou, Yunhua Zhou, Dongsheng Zhu, Lin Zhu, Yicheng Zou</strong></p>
<p>In recent years, a plethora of open-source foundation models have emerged, achieving remarkable progress in some widely attended fields, with performance being quite close to that of closed-source models. However, in high-value but more challenging scientific professional fields, either the fields still rely on expert models, or the progress of general foundation models lags significantly compared to those in popular areas, far from sufficient for transforming scientific research and leaving substantial gap between open-source models and closed-source models in these scientific domains. To mitigate this gap and explore a step further toward Artificial General Intelligence (AGI), we introduce Intern-S1, a specialized generalist equipped with general understanding and reasoning capabilities with expertise to analyze multiple science modal data. Intern-S1 is a multimodal Mixture-of-Experts (MoE) model with 28 billion activated parameters and 241 billion total parameters, continually pre-trained on 5T tokens, including over 2.5T tokens from scientific domains. In the post-training stage, Intern-S1 undergoes offline and then online reinforcement learning (RL) in InternBootCamp, where we propose Mixture-of-Rewards (MoR) to synergize the RL training on more than 1000 tasks simultaneously. Through integrated innovations in algorithms, data, and training systems, Intern-S1 achieved top-tier performance in online RL training.On comprehensive evaluation benchmarks, Intern-S1 demonstrates competitive performance on general reasoning tasks among open-source models and significantly outperforms open-source models in scientific domains, surpassing closed-source state-of-the-art models in professional tasks, such as molecular synthesis planning, reaction condition prediction, predicting thermodynamic stabilities for crystals. Our models are available at <a target="_blank" rel="noopener" href="https://huggingface.co/internlm/Intern-S1">https://huggingface.co/internlm/Intern-S1</a>. </p>
<blockquote>
<p>近年来，大量的开源基础模型不断涌现，在一些广泛关注的领域取得了显著的进展，其性能与闭源模型相当接近。然而，在高价值但更具挑战性的科学专业领域，这些领域仍然依赖于专家模型，或者通用基础模型的进展与流行领域相比滞后显著，远远不足以改变科学研究，从而在这些科学领域的开源模型和闭源模型之间产生了巨大的差距。为了缩小这一差距，并进一步朝着通用人工智能（AGI）的方向发展，我们引入了Intern-S1，这是一个具备通用理解和推理能力，同时拥有分析多种科学模态数据的专业通才。Intern-S1是一个多模态的专家混合（MoE）模型，拥有28亿个激活参数和总共241亿个参数，在5T标记上持续进行预训练，其中包括来自科学领域的超过2.5T标记。在训练后阶段，Intern-S1在InternBootCamp中经历了离线然后是在线的强化学习（RL）训练。我们提出了奖励混合（MoR）方法来协同在超过1000个任务上同时进行RL训练。通过算法、数据和训练系统的综合创新，Intern-S1在在线RL训练中取得了顶尖性能。在全面的评估基准测试中，Intern-S1在通用推理任务中展示了与开源模型的竞争力，并在科学领域显著优于开源模型，在专业任务中超越了闭源的最先进模型，如分子合成规划、反应条件预测、晶体热力学稳定性预测等。我们的模型可在<a target="_blank" rel="noopener" href="https://huggingface.co/internlm/intern-s1%E8%8E%B7%E5%8F%96%E3%80%82">https://huggingface.co/internlm/Intern-S1获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15763v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该文介绍了新兴的开源基础模型 Intern-S1，它在一些广受关注的领域取得了引人注目的进展，性能接近闭源模型。然而，在高价值的科学专业领域中，开源基础模型的进展仍然显著滞后。为了缩小差距并朝着通用人工智能（AGI）迈出一步，推出了 Intern-S1 模型，该模型具备通用理解和推理能力，并具备分析多种科学模态数据的专业知识。通过算法、数据和训练系统的综合创新，Intern-S1 在在线强化学习训练中取得了顶尖性能，并在综合评估基准测试中展现了竞争力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>开源基础模型在许多领域性能接近闭源模型，但在高价值的科学专业领域存在显著差距。</li>
<li>Intern-S1 是一款多模态的混合专家（MoE）模型，具备通用理解和推理能力，并具备分析多种科学模态数据的专业知识。</li>
<li>Intern-S1 通过持续预训练在 5T 标记符号上，包括超过 2.5T 来自科学领域的标记符号。</li>
<li>Intern-S1 在在线强化学习训练阶段采用了混合物奖励（MoR）方法，同步进行超过 1000 项任务的强化学习训练。</li>
<li>通过算法、数据和训练系统的综合创新，Intern-S1 在在线强化学习训练中实现了顶尖性能。</li>
<li>Intern-S1 在通用推理任务上展现了与开源模型的竞争力，并在科学领域显著优于其他开源模型。</li>
<li>Intern-S1 在专业任务上超越了闭源的最先进模型，如分子合成规划、反应条件预测和晶体热力学稳定性预测等。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15763">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6f8341d09ccdc18c4e86303af60f0537.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6237b1566b477a31cf689954aaa1a61b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b20ed790b9759ad242c7c81834ae428b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-78591ae6302e57a931d5853acb766c0a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee6ed39d102291a6c3c34e80ea4ed775.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Response-and-Prompt-Evaluation-to-Prevent-Parasocial-Relationships-with-Chatbots"><a href="#Response-and-Prompt-Evaluation-to-Prevent-Parasocial-Relationships-with-Chatbots" class="headerlink" title="Response and Prompt Evaluation to Prevent Parasocial Relationships with   Chatbots"></a>Response and Prompt Evaluation to Prevent Parasocial Relationships with   Chatbots</h2><p><strong>Authors:Emma Rath, Stuart Armstrong, Rebecca Gorman</strong></p>
<p>The development of parasocial relationships with AI agents has severe, and in some cases, tragic effects for human well-being. Yet preventing such dynamics is challenging: parasocial cues often emerge gradually in private conversations, and not all forms of emotional engagement are inherently harmful. We address this challenge by introducing a simple response evaluation framework, created by repurposing a state-of-the-art language model, that evaluates ongoing conversations for parasocial cues in real time. To test the feasibility of this approach, we constructed a small synthetic dataset of thirty dialogues spanning parasocial, sycophantic, and neutral conversations. Iterative evaluation with five stage testing successfully identified all parasocial conversations while avoiding false positives under a tolerant unanimity rule, with detection typically occurring within the first few exchanges. These findings provide preliminary evidence that evaluation agents can provide a viable solution for the prevention of parasocial relations. </p>
<blockquote>
<p>与人工智能代理建立准社会关系的发展给人类福祉带来了严重，甚至在某些情况下带来了悲剧性的影响。然而，阻止这种动态发展具有挑战性：准社会线索通常会在私人对话中逐渐显现，并非所有形式的情感参与都是有害的。我们通过引入一个简单的反应评估框架来解决这一挑战，该框架由最先语言模型改造而成，用于实时评估对话中是否存在准社会线索。为了验证该方法的可行性，我们构建了包含三十个对话的小型合成数据集，涵盖了准社会、谄媚和中性对话。通过五个阶段的测试进行迭代评估，成功地识别出了所有准社会对话，在宽容一致规则下避免了误报，检测通常发生在最初的几次交流中。这些发现提供了初步证据，表明评估代理可以提供一种防止准社会关系的可行解决方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15748v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探讨了与人工智能代理发展寄生社会关系所带来的严重甚至悲剧性影响。针对这一挑战，提出一个简单的响应评估框架，通过改造当前最先进的语言模型，以实时评估对话中的寄生社会线索。通过构建包含寄生社会、谄媚和中性对话的三十个对话的小型合成数据集进行测试，成功识别出所有寄生社会对话，并在容忍一致性的规则下避免误报。初步证据表明评估代理可为预防寄生社会关系提供可行解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>与人工智能代理发展寄生社会关系可能对人类福祉产生严重影响。</li>
<li>寄生社会线索在私人对话中逐渐显现，并非所有情感参与都是有害的。</li>
<li>引入了一个简单的响应评估框架，利用先进语言模型实时评估对话中的寄生社会线索。</li>
<li>通过构建包含不同类型对话的合成数据集进行测试，验证了该框架的有效性。</li>
<li>在容忍一致性的规则下，该框架能成功识别寄生社会对话，且通常能在前几轮交流中即完成检测。</li>
<li>评估代理在预防寄生社会关系方面具有潜在应用价值。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15748">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-43c92f2d4d7ab75c70594a76a2d13deb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d04191d34c14b82e8046c84e639f554c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a0a226e622a266d645d2a988c71232f0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="End-to-End-Agentic-RAG-System-Training-for-Traceable-Diagnostic-Reasoning"><a href="#End-to-End-Agentic-RAG-System-Training-for-Traceable-Diagnostic-Reasoning" class="headerlink" title="End-to-End Agentic RAG System Training for Traceable Diagnostic   Reasoning"></a>End-to-End Agentic RAG System Training for Traceable Diagnostic   Reasoning</h2><p><strong>Authors:Qiaoyu Zheng, Yuze Sun, Chaoyi Wu, Weike Zhao, Pengcheng Qiu, Yongguo Yu, Kun Sun, Yanfeng Wang, Ya Zhang, Weidi Xie</strong></p>
<p>Accurate diagnosis with medical large language models is hindered by knowledge gaps and hallucinations. Retrieval and tool-augmented methods help, but their impact is limited by weak use of external knowledge and poor feedback-reasoning traceability. To address these challenges, We introduce Deep-DxSearch, an agentic RAG system trained end-to-end with reinforcement learning (RL) that enables steer tracebale retrieval-augmented reasoning for medical diagnosis. In Deep-DxSearch, we first construct a large-scale medical retrieval corpus comprising patient records and reliable medical knowledge sources to support retrieval-aware reasoning across diagnostic scenarios. More crutially, we frame the LLM as the core agent and the retrieval corpus as its environment, using tailored rewards on format, retrieval, reasoning structure, and diagnostic accuracy, thereby evolving the agentic RAG policy from large-scale data through RL.   Experiments demonstrate that our end-to-end agentic RL training framework consistently outperforms prompt-engineering and training-free RAG approaches across multiple data centers. After training, Deep-DxSearch achieves substantial gains in diagnostic accuracy, surpassing strong diagnostic baselines such as GPT-4o, DeepSeek-R1, and other medical-specific frameworks for both common and rare disease diagnosis under in-distribution and out-of-distribution settings. Moreover, ablation studies on reward design and retrieval corpus components confirm their critical roles, underscoring the uniqueness and effectiveness of our approach compared with traditional implementations. Finally, case studies and interpretability analyses highlight improvements in Deep-DxSearch’s diagnostic policy, providing deeper insight into its performance gains and supporting clinicians in delivering more reliable and precise preliminary diagnoses. See <a target="_blank" rel="noopener" href="https://github.com/MAGIC-AI4Med/Deep-DxSearch">https://github.com/MAGIC-AI4Med/Deep-DxSearch</a>. </p>
<blockquote>
<p>使用医疗大型语言模型进行准确诊断受到知识差距和幻觉的阻碍。检索和工具增强方法有所帮助，但它们的影响受限于外部知识利用不足和反馈推理追踪能力弱。为了解决这些挑战，我们引入了Deep-DxSearch，这是一个通过强化学习（RL）进行端到端训练的主动推理图系统，可实现用于医疗诊断的追踪检索增强推理。在Deep-DxSearch中，我们首先构建了一个大规模医疗检索语料库，包含患者记录和可靠的医疗知识来源，以支持跨诊断场景的检索感知推理。更重要的是，我们将大型语言模型作为核心主体，将检索语料库作为其环境，针对格式、检索、推理结构和诊断准确性制定专门奖励，从而通过强化学习推动主动推理图策略从大规模数据中进化。实验表明，我们的端到端主动强化学习训练框架在多个数据中心始终优于基于提示和训练免费的主动推理图方法。经过训练，Deep-DxSearch在诊断准确性方面取得了显著进步，超越了强诊断基线，如GPT-4o、DeepSeek-R1和其他针对常见和罕见疾病诊断的医疗专用框架，无论在内部分布还是外部分布环境中均表现优异。此外，关于奖励设计和检索语料库组件的消融研究证实了它们的关键作用，强调了我们的方法与传统实施方式的独特性和有效性。最后，案例研究和解释性分析突显了Deep-DxSearch诊断策略的改进，为其性能提升提供了更深入的了解，并支持临床医生提供更可靠和精确的早期诊断。更多信息请参见<a target="_blank" rel="noopener" href="https://github.com/MAGIC-AI4Med/Deep-DxSearch%E3%80%82">https://github.com/MAGIC-AI4Med/Deep-DxSearch。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15746v1">PDF</a> 35 pages, 5 figures, 3 tables</p>
<p><strong>Summary</strong></p>
<p>基于大规模医疗语言模型的诊断存在知识缺口和幻觉的问题。检索和工具增强方法有所帮助，但其影响受限于外部知识利用不足和反馈推理追踪能力弱。为解决这些挑战，我们推出Deep-DxSearch，这是一个采用强化学习（RL）进行端到端训练的能动型RAG系统，可实现追踪可检索的推理辅助医疗诊断。Deep-DxSearch首先构建大规模医疗检索语料库，包含患者记录和可靠医疗知识源，以支持跨诊断场景的检索感知推理。更为关键的是，我们将大型语言模型作为核心主体，将检索语料库作为环境，通过格式、检索、推理结构和诊断准确性的定制奖励，使能动型RAG策略从大规模数据中通过RL进行进化。实验表明，我们的端到端能动型RL训练框架在多个数据中心均优于提示工程和免训练RAG方法。经过训练，Deep-DxSearch在诊断准确性方面取得了实质性进展，超越了包括GPT-4o、DeepSeek-R1在内的强劲诊断基线以及其他医疗特定框架，在常见和罕见疾病的诊断中均表现出色，无论处于分布内还是分布外环境。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型在医疗诊断中的应用受到知识缺口和幻觉的挑战。</li>
<li>检索和工具增强方法虽有帮助，但受限于外部知识利用不足和推理反馈追踪能力弱。</li>
<li>Deep-DxSearch是一个基于强化学习训练的能动型RAG系统，支持检索增强的推理过程。</li>
<li>Deep-DxSearch构建大规模医疗检索语料库以支持跨诊断场景的检索感知推理。</li>
<li>Deep-DxSearch将大型语言模型作为核心主体，结合定制奖励进行训练，以提高诊断准确性。</li>
<li>实验表明Deep-DxSearch在多个数据中心均优于其他方法，并能在常见和罕见疾病的诊断中表现出色。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15746">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-d1bb8f786cad4e2d5361a4b48148892a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e16115757b3c0fd580e293a4e959ec92.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a888b967c7d1c1b107fd8d54ea7bf485.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Benchmarking-Computer-Science-Survey-Generation"><a href="#Benchmarking-Computer-Science-Survey-Generation" class="headerlink" title="Benchmarking Computer Science Survey Generation"></a>Benchmarking Computer Science Survey Generation</h2><p><strong>Authors:Weihang Su, Anzhe Xie, Qingyao Ai, Jianming Long, Jiaxin Mao, Ziyi Ye, Yiqun Liu</strong></p>
<p>Scientific survey articles play a vital role in summarizing research progress, yet their manual creation is becoming increasingly infeasible due to the rapid growth of academic literature. While large language models (LLMs) offer promising capabilities for automating this process, progress in this area is hindered by the absence of standardized benchmarks and evaluation protocols. To address this gap, we introduce SurGE (Survey Generation Evaluation), a new benchmark for evaluating scientific survey generation in the computer science domain. SurGE consists of (1) a collection of test instances, each including a topic description, an expert-written survey, and its full set of cited references, and (2) a large-scale academic corpus of over one million papers that serves as the retrieval pool. In addition, we propose an automated evaluation framework that measures generated surveys across four dimensions: information coverage, referencing accuracy, structural organization, and content quality. Our evaluation of diverse LLM-based approaches shows that survey generation remains highly challenging, even for advanced self-reflection frameworks. These findings highlight the complexity of the task and the necessity for continued research. We have open-sourced all the code, data, and models at: <a target="_blank" rel="noopener" href="https://github.com/oneal2000/SurGE">https://github.com/oneal2000/SurGE</a> </p>
<blockquote>
<p>科学综述文章在总结研究进展方面起着至关重要的作用，然而由于其数量的快速增长，手动编写变得越来越不可行。虽然大型语言模型（LLM）为自动化这个过程提供了有前景的能力，但这一领域的进展受到缺乏标准化基准测试和评估协议的阻碍。为了填补这一空白，我们引入了SurGE（综述生成评估）基准测试，这是一个用于评估计算机科学领域科学综述生成情况的新基准测试。SurGE包括：（1）测试实例集，每个实例包括主题描述、专家撰写的综述及其全套引文；（2）作为检索池的大规模学术语料库，包含超过一百万的论文。此外，我们提出了一个自动化评估框架，从四个维度衡量生成的综述：信息覆盖、引用准确性、结构组织和内容质量。我们对多种基于LLM的方法的评估表明，即使是对于先进的自我反思框架来说，生成综述仍然极具挑战性。这些发现突显了任务的复杂性以及继续研究的必要性。我们已经公开所有代码、数据和模型在：<a target="_blank" rel="noopener" href="https://github.com/oneal2000/SurGE">https://github.com/oneal2000/SurGE</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15658v1">PDF</a> </p>
<p><strong>Summary</strong><br>学术综述文章在总结研究进展方面起着重要作用，但手动创建综述越来越不可行。大型语言模型（LLM）为自动化此过程提供了潜力，但缺乏标准化基准测试和评估协议阻碍了进展。为解决这一空白，我们推出了SurGE（综述生成评估）基准测试，用于评估计算机科学领域的学术综述生成能力。SurGE包括一组测试实例和一个大规模学术文献库，同时提供自动评估框架来衡量生成的综述在信息覆盖、引用准确性、结构组织和内容质量等方面的表现。我们对不同的LLM方法进行了评估，发现即使是高级自我反思框架，综述生成仍然极具挑战性。这些发现凸显了任务的复杂性以及持续研究的必要性。相关代码、数据和模型已开源。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>学术综述文章在总结研究进展中起重要作用，但手动创建变得困难。</li>
<li>大型语言模型（LLM）为自动化综述生成提供了潜力。</li>
<li>当前缺乏标准化基准测试和评估协议来评估LLM在学术综述生成方面的性能。</li>
<li>SurGE基准测试包括测试实例和大规模学术文献库，用于评估科学综述生成能力。</li>
<li>SurGE提供自动评估框架来衡量生成的综述在信息覆盖、引用准确性等方面的表现。</li>
<li>LLM方法在综述生成方面仍然面临挑战，凸显了任务的复杂性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15658">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-149c94281c63fab0be861c1249be1603.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bd7151693f752650f1b872a8d9d91a69.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="On-the-Effectiveness-of-Graph-Reordering-for-Accelerating-Approximate-Nearest-Neighbor-Search-on-GPU"><a href="#On-the-Effectiveness-of-Graph-Reordering-for-Accelerating-Approximate-Nearest-Neighbor-Search-on-GPU" class="headerlink" title="On the Effectiveness of Graph Reordering for Accelerating Approximate   Nearest Neighbor Search on GPU"></a>On the Effectiveness of Graph Reordering for Accelerating Approximate   Nearest Neighbor Search on GPU</h2><p><strong>Authors:Yutaro Oguri, Mai Nishimura, Yusuke Matsui</strong></p>
<p>We present the first systematic investigation of graph reordering effects for graph-based Approximate Nearest Neighbor Search (ANNS) on a GPU. While graph-based ANNS has become the dominant paradigm for modern AI applications, recent approaches focus on algorithmic innovations while neglecting memory layout considerations that significantly affect execution time. Our unified evaluation framework enables comprehensive evaluation of diverse reordering strategies across different graph indices through a graph adapter that converts arbitrary graph topologies into a common representation and a GPU-optimized graph traversal engine. We conduct a comprehensive analysis across diverse datasets and state-of-the-art graph indices, introducing analysis metrics that quantify the relationship between structural properties and memory layout effectiveness. Our GPU-targeted reordering achieves up to 15$%$ QPS improvements while preserving search accuracy, demonstrating that memory layout optimization operates orthogonally to existing algorithmic innovations. We will release all code upon publication to facilitate reproducibility and foster further research. </p>
<blockquote>
<p>我们对基于图的近似最近邻搜索（ANNS）在GPU上的图重排效应进行了首次系统研究。虽然基于图的ANNS已成为现代AI应用的主导范式，但最近的方法主要集中在算法创新上，而忽视了内存布局考虑，这显著影响了执行时间。我们的统一评估框架通过图形适配器（将任意图形拓扑转换为通用表示形式）和GPU优化的图形遍历引擎，全面评估了不同图形索引的不同重排序策略。我们在不同数据集和最新图形索引上进行了全面的分析，引入了分析度量标准，以量化结构属性与内存布局有效性之间的关系。我们针对GPU进行的重排序在保持搜索准确性的同时，实现了高达15%的查询每秒（QPS）改进，这表明内存布局优化与现有的算法创新是垂直运行的。我们会在发表时公开所有代码，以促进可重复性和进一步的研究。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15436v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文首次对基于GPU的图形重排序在图形近似最近邻搜索（ANNS）中的影响进行了系统研究。研究发现在现代AI应用中，虽然基于图形的ANNS已成为主流范式，但最近的算法更多地关注算法创新，而忽视了内存布局对执行时间的显著影响。本文提供了一个统一的评估框架，能够全面评估不同重排序策略在不同图形索引上的表现。通过图形适配器和GPU优化的图形遍历引擎，该框架能够将任意图形拓扑转换为通用表示形式。本文全面分析了不同数据集和最新图形索引之间的关系，并引入了量化结构特性和内存布局有效性的分析指标。通过针对GPU的重排序优化，实现了查询速率（QPS）提高达15%，同时保证了搜索精度。这表明内存布局优化与现有的算法创新是相辅相成的。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>本文首次系统地研究了图形重排序在基于GPU的图形近似最近邻搜索（ANNS）中的影响。</li>
<li>研究指出，尽管基于图形的ANNS在现代AI应用中占据主导地位，但最近的算法更多地关注算法创新，忽视了内存布局的重要性。</li>
<li>提供了一个统一的评估框架，用于全面评估不同重排序策略在不同图形索引上的表现。</li>
<li>通过图形适配器和GPU优化的图形遍历引擎，任意图形拓扑可转换为通用表示形式。</li>
<li>研究全面分析了不同数据集和最新图形索引之间的关系，引入了分析结构特性和内存布局有效性的量化指标。</li>
<li>通过针对GPU的重排序优化，实现了查询速率（QPS）的提高，最高可达15%。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15436">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-2bea5250e879d514c939f5e4aacf3f77.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e313829f7a68e4c8973af3aecad97608.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1771e6ff9907104721b045215dba0e47.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-74ebff8e3918cb23fe11746ab355c450.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="DIO-Refining-Mutual-Information-and-Causal-Chain-to-Enhance-Machine-Abstract-Reasoning-Ability"><a href="#DIO-Refining-Mutual-Information-and-Causal-Chain-to-Enhance-Machine-Abstract-Reasoning-Ability" class="headerlink" title="DIO: Refining Mutual Information and Causal Chain to Enhance Machine   Abstract Reasoning Ability"></a>DIO: Refining Mutual Information and Causal Chain to Enhance Machine   Abstract Reasoning Ability</h2><p><strong>Authors:Ruizhuo Song, Beiming Yuan</strong></p>
<p>Despite the outstanding performance of current deep learning models across various domains, their fundamental bottleneck in abstract reasoning remains unresolved. To address this challenge, the academic community has introduced Raven’s Progressive Matrices (RPM) problems as an authoritative benchmark for evaluating the abstract reasoning capabilities of deep learning algorithms, with a focus on core intelligence dimensions such as abstract reasoning, pattern recognition, and complex problem-solving. Therefore, this paper centers on solving RPM problems, aiming to contribute to enhancing the abstract reasoning abilities of machine intelligence. Firstly, this paper adopts a &#96;&#96;causal chain modeling’’ perspective to systematically analyze the complete causal chain in RPM tasks: image $\rightarrow$ abstract attributes $\rightarrow$ progressive attribute patterns $\rightarrow$ pattern consistency $\rightarrow$ correct answer. Based on this analysis, the network architecture of the baseline model DIO is designed. However, experiments reveal that the optimization objective formulated for DIO, namely maximizing the variational lower bound of mutual information between the context and the correct option, fails to enable the model to genuinely acquire the predefined human reasoning logic. This is attributed to two main reasons: the tightness of the lower bound significantly impacts the effectiveness of mutual information maximization, and mutual information, as a statistical measure, does not capture the causal relationship between subjects and objects. To overcome these limitations, this paper progressively proposes three improvement methods: </p>
<blockquote>
<p>尽管当前深度学习模型在各种领域表现出卓越的性能，它们在抽象推理方面的根本瓶颈仍未解决。为了应对这一挑战，学术界引入了Raven的渐进矩阵（RPM）问题作为评估深度学习算法抽象推理能力的权威基准测试，重点关注抽象推理、模式识别和复杂问题解决等核心智力维度。因此，本文专注于解决RPM问题，旨在提高机器智能的抽象推理能力。首先，本文采用“因果链建模”的角度，系统地分析了RPM任务中的完整因果链：图像$\rightarrow$抽象属性$\rightarrow$渐进属性模式$\rightarrow$模式一致性$\rightarrow$正确答案。基于此分析，设计了基线模型DIO的网络架构。然而，实验表明，为DIO制定的优化目标，即最大化上下文与正确选项之间的互信息的变分下界，未能使模型真正获得预先设定的人类推理逻辑。这主要归因于两个原因：下界的紧密性显著影响了互信息最大化的有效性；作为统计量度的互信息并不能捕捉主体与对象之间的因果关系。为了克服这些局限性，本文逐步提出了三种改进方法：</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15387v1">PDF</a> 15 pages, 9 figures, 8 tables</p>
<p><strong>Summary</strong><br>深度学习模型虽然在多个领域表现出色，但在抽象推理方面仍存在瓶颈。为应对这一挑战，学术界引入了Raven’s Progressive Matrices（RPM）问题作为评估深度学习算法抽象推理能力的权威基准测试。本文专注于解决RPM问题，旨在提高机器智能的抽象推理能力。该文采用因果链建模方法，分析RPM任务的完整因果链，并设计基线模型DIO的网络架构。然而，实验表明，DIO的优化目标未能使模型真正获得预设的人类推理逻辑。原因在于变分下界的紧密性影响了信息最大化的有效性，且信息最大化作为统计度量手段未能捕捉主体与对象之间的因果关系。为克服这些局限，本文逐步提出三种改进方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>深度学习模型在抽象推理方面存在瓶颈，需要引入新的评估基准如RPM问题来评估其能力。</li>
<li>本文通过因果链建模分析RPM问题的完整因果链，并设计基线模型DIO。</li>
<li>DIO的优化目标未能使模型真正获得预设的人类推理逻辑，主要原因是变分下界的紧密性和信息最大化的有效性问题。</li>
<li>信息最大化作为统计度量手段无法捕捉主体与对象之间的因果关系。</li>
<li>为改进模型性能，本文提出了三种策略：优化变分下界、结合使用多种推理方法和强化学习、以及构建更加复杂的网络结构来增强模型的抽象推理能力。</li>
<li>解决RPM问题对于提高机器智能的抽象推理能力具有重要意义。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15387">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9f4157e6d38b295a01167a71e0cdb021.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2e6f4a6deae6ae50bf3812a754edefcf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b753210c8e2f98ef9b811c7def2ddf9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0184ee4d30aec0efd872e35e1ee6822f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f4c20a71e65def04c35f4eaf6f8b495.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b46e3e60dd0bb37185e4719641844967.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Unveiling-Trust-in-Multimodal-Large-Language-Models-Evaluation-Analysis-and-Mitigation"><a href="#Unveiling-Trust-in-Multimodal-Large-Language-Models-Evaluation-Analysis-and-Mitigation" class="headerlink" title="Unveiling Trust in Multimodal Large Language Models: Evaluation,   Analysis, and Mitigation"></a>Unveiling Trust in Multimodal Large Language Models: Evaluation,   Analysis, and Mitigation</h2><p><strong>Authors:Yichi Zhang, Yao Huang, Yifan Wang, Yitong Sun, Chang Liu, Zhe Zhao, Zhengwei Fang, Huanran Chen, Xiao Yang, Xingxing Wei, Hang Su, Yinpeng Dong, Jun Zhu</strong></p>
<p>The trustworthiness of Multimodal Large Language Models (MLLMs) remains an intense concern despite the significant progress in their capabilities. Existing evaluation and mitigation approaches often focus on narrow aspects and overlook risks introduced by the multimodality. To tackle these challenges, we propose MultiTrust-X, a comprehensive benchmark for evaluating, analyzing, and mitigating the trustworthiness issues of MLLMs. We define a three-dimensional framework, encompassing five trustworthiness aspects which include truthfulness, robustness, safety, fairness, and privacy; two novel risk types covering multimodal risks and cross-modal impacts; and various mitigation strategies from the perspectives of data, model architecture, training, and inference algorithms. Based on the taxonomy, MultiTrust-X includes 32 tasks and 28 curated datasets, enabling holistic evaluations over 30 open-source and proprietary MLLMs and in-depth analysis with 8 representative mitigation methods. Our extensive experiments reveal significant vulnerabilities in current models, including a gap between trustworthiness and general capabilities, as well as the amplification of potential risks in base LLMs by both multimodal training and inference. Moreover, our controlled analysis uncovers key limitations in existing mitigation strategies that, while some methods yield improvements in specific aspects, few effectively address overall trustworthiness, and many introduce unexpected trade-offs that compromise model utility. These findings also provide practical insights for future improvements, such as the benefits of reasoning to better balance safety and performance. Based on these insights, we introduce a Reasoning-Enhanced Safety Alignment (RESA) approach that equips the model with chain-of-thought reasoning ability to discover the underlying risks, achieving state-of-the-art results. </p>
<blockquote>
<p>尽管多模态大型语言模型（MLLMs）的能力已经取得了重大进展，但其可信度仍然是一个备受关注的问题。现有的评估和缓解方法往往侧重于狭窄的方面，忽视了多模态引入的风险。为了应对这些挑战，我们提出了MultiTrust-X，这是一个全面评估、分析和缓解MLLMs可信度问题的基准测试。我们定义了一个三维框架，包含五个可信度方面，即真实性、稳健性、安全性、公平性和隐私性；两种新型风险类型，包括多模态风险和跨模态影响；以及从数据、模型结构、训练和推理算法等角度的多种缓解策略。基于分类学，MultiTrust-X包括32个任务和28个精选数据集，能够对30个开源和专有MLLM进行整体评估，并使用8种具有代表性的缓解方法进行深入分析。我们的大量实验揭示了当前模型的重要漏洞，包括可信度与通用能力之间的鸿沟，以及基于LLM的基础训练中推理和推断的多模态带来的潜在风险的放大。此外，我们的受控分析揭示了现有缓解策略的关键局限性，即虽然一些方法在特定方面有所改进，但几乎没有什么能有效地解决整体可信度问题，而且许多方法引入了意外的权衡，从而损害了模型的实用性。这些发现还为未来的改进提供了实际见解，例如通过推理来更好地平衡安全性和性能的好处。基于这些见解，我们引入了一种增强型安全对齐推理（RESA）方法，该方法赋予模型链式思维推理能力，以发现潜在风险，实现最新结果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15370v1">PDF</a> For Appendix, please refer to arXiv:2406.07057</p>
<p><strong>Summary</strong></p>
<p>文本提出了针对多模态大型语言模型（MLLMs）的信任问题的一种全面的基准测试方案MultiTrust-X。文章详细描述了所提出的方案的构建方法和重要特点，包括对MLLMs的可信度的五个方面的评价：真实性、稳健性、安全性、公平性和隐私性；涵盖多模态风险和跨模态影响的新风险类型；以及从数据、模型架构、训练和推理算法等角度的缓解策略。基于这些分类，MultiTrust-X包括32项任务和28个精选数据集，能够对30个开源和专有MLLM进行全面评估，并使用8种具有代表性的缓解方法进行深入分析。文章还介绍了基于实验和分析的发现，包括当前模型中的显著漏洞和未来改进的实际见解。最后提出了一种新的方法RESA来增强模型的安全性和性能平衡。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态大型语言模型（MLLMs）的可信性仍是人们关注的重点，尽管其能力已取得了显著进展。</li>
<li>现有评估和缓解方法往往关注狭窄的方面，忽视了多模态引入的风险。</li>
<li>MultiTrust-X是一个全面的基准测试，用于评估、分析和缓解MLLMs的可信性问题。它定义了一个包含五个可信方面的三维框架，包括真实性、稳健性、安全性、公平性和隐私性。</li>
<li>MultiTrust-X还包括涵盖多模态风险和跨模态影响的新风险类型以及多种缓解策略。</li>
<li>实验表明，当前模型存在显著漏洞，包括可信性与一般能力之间的鸿沟，以及基础LLMs中潜在风险的放大。这些风险在多模态训练和推理中尤为明显。</li>
<li>现有的缓解策略存在局限性，一些方法仅在特定方面有所改善，而少数方法能有效解决整体的可信性问题，并可能引入影响模型实用性的意外权衡。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15370">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d5bfcc58d265f1a640a3136578e3056e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-132316f58e4fb3d8b422ad515a45e558.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9db2886d8199b65a7a62242c9fa49c16.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ce219559fe8fa8cfb981c662bb3f9ba.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bcd456c1e0d3c2a511250562e8a2ed70.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="KG-EDAS-A-Meta-Metric-Framework-for-Evaluating-Knowledge-Graph-Completion-Models"><a href="#KG-EDAS-A-Meta-Metric-Framework-for-Evaluating-Knowledge-Graph-Completion-Models" class="headerlink" title="KG-EDAS: A Meta-Metric Framework for Evaluating Knowledge Graph   Completion Models"></a>KG-EDAS: A Meta-Metric Framework for Evaluating Knowledge Graph   Completion Models</h2><p><strong>Authors:Haji Gul, Abul Ghani Naim, Ajaz Ahmad Bhat</strong></p>
<p>Knowledge Graphs (KGs) enable applications in various domains such as semantic search, recommendation systems, and natural language processing. KGs are often incomplete, missing entities and relations, an issue addressed by Knowledge Graph Completion (KGC) methods that predict missing elements. Different evaluation metrics, such as Mean Reciprocal Rank (MRR), Mean Rank (MR), and Hit@k, are commonly used to assess the performance of such KGC models. A major challenge in evaluating KGC models, however, lies in comparing their performance across multiple datasets and metrics. A model may outperform others on one dataset but underperform on another, making it difficult to determine overall superiority. Moreover, even within a single dataset, different metrics such as MRR and Hit@1 can yield conflicting rankings, where one model excels in MRR while another performs better in Hit@1, further complicating model selection for downstream tasks. These inconsistencies hinder holistic comparisons and highlight the need for a unified meta-metric that integrates performance across all metrics and datasets to enable a more reliable and interpretable evaluation framework. To address this need, we propose KG Evaluation based on Distance from Average Solution (EDAS), a robust and interpretable meta-metric that synthesizes model performance across multiple datasets and diverse evaluation criteria into a single normalized score ($M_i \in [0,1]$). Unlike traditional metrics that focus on isolated aspects of performance, EDAS offers a global perspective that supports more informed model selection and promotes fairness in cross-dataset evaluation. Experimental results on benchmark datasets such as FB15k-237 and WN18RR demonstrate that EDAS effectively integrates multi-metric, multi-dataset performance into a unified ranking, offering a consistent, robust, and generalizable framework for evaluating KGC models. </p>
<blockquote>
<p>知识图谱（KGs）可应用于各种领域，如语义搜索、推荐系统和自然语言处理。知识图谱往往是不完整的，缺少实体和关系，这一问题通过知识图谱补全（KGC）方法得到解决，这些方法能够预测缺失元素。通常使用诸如平均倒数排名（MRR）、平均排名（MR）和命中@k等评估指标来评估此类KGC模型的性能。然而，评估KGC模型的主要挑战在于比较它们在多个数据集和指标上的性能。一个模型可能在某个数据集上优于其他模型，但在另一个数据集上表现较差，这使得难以确定其总体优势。此外，即使在单个数据集中，不同的指标（如MRR和命中@1）也可能产生冲突的排名，一个模型可能在MRR中表现优异，而在命中@1中表现较好，这进一步使下游任务中的模型选择复杂化。这些不一致性阻碍了全面的比较，并强调了需要一个统一的度量标准，该标准能够整合所有指标和数据集的性能，从而建立一个更可靠和可解释的评价框架。为了应对这一需求，我们提出了基于距离平均解（EDAS）的KG评估方法，这是一种稳健且可解释的度量标准，可将模型在多个数据集和不同评价指标上的性能综合成一个单一的归一化分数（Mi∈[0,1]）。与传统的孤立关注性能方面的指标不同，EDAS提供了全局视角，支持更明智的模型选择，并促进了跨数据集的公平评估。在FB15k-237和WN18RR等基准数据集上的实验结果表明，EDAS有效地将多指标、多数据集的性能集成到一个统一的排名中，为评估KGC模型提供了一个一致、稳健和通用的框架。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15357v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了知识图谱（KGs）在各领域的应用，如语义搜索、推荐系统和自然语言处理。知识图谱常常不完整，缺少实体和关系，知识图谱补全（KGC）方法可预测缺失元素。使用不同的评估指标，如平均倒数排名（MRR）、平均排名（MR）和命中@k，来评估KGC模型的性能。然而，在多个数据集和指标上比较KGC模型的性能是一个主要挑战。此外，即使在单个数据集中，不同的指标也可能产生冲突的排名结果。针对这一需求，本文提出了基于距离平均解（EDAS）的KG评估方法，该方法将模型在多个数据集和多种评价指标上的性能综合为一个单一的归一化分数，为更可靠和可解释的评价框架提供了支持。实验结果表明，EDAS能有效整合多指标、多数据集的性能，为评价KGC模型提供了一个一致、稳健和通用的框架。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>知识图谱（KGs）在多个领域有广泛应用，但存在不完整性问题，需要知识图谱补全（KGC）方法预测缺失元素。</li>
<li>评估KGC模型的性能通常使用多个数据集和评价指标，但比较这些性能是一个挑战。</li>
<li>存在多种评价KGC模型的指标，但不同指标可能导致冲突的结果。</li>
<li>EDAS是一种新的评估方法，能将模型在多个数据集和多种评价指标上的性能综合为一个单一的归一化分数。</li>
<li>EDAS提供了一个全局视角，支持更明智的模型选择和跨数据集的公平评价。</li>
<li>实验结果表明，EDAS能有效整合多指标和多数据集的性能评价。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15357">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-014ce7980990530133c10a8843ec78d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-319a530a73ee91d3f3abce7872cacd29.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f73f9e77b982d241fa3eae9edc29cd1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-884e0039e8c5e2a7956a94f3686552d3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8af0259e3a2558c3b0462aaef2ff76fb.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="REG4Rec-Reasoning-Enhanced-Generative-Model-for-Large-Scale-Recommendation-Systems"><a href="#REG4Rec-Reasoning-Enhanced-Generative-Model-for-Large-Scale-Recommendation-Systems" class="headerlink" title="REG4Rec: Reasoning-Enhanced Generative Model for Large-Scale   Recommendation Systems"></a>REG4Rec: Reasoning-Enhanced Generative Model for Large-Scale   Recommendation Systems</h2><p><strong>Authors:Haibo Xing, Hao Deng, Yucheng Mao, Jinxin Hu, Yi Xu, Hao Zhang, Jiahao Wang, Shizhun Wang, Yu Zhang, Xiaoyi Zeng, Jing Zhang</strong></p>
<p>Sequential recommendation aims to predict a user’s next action in large-scale recommender systems. While traditional methods often suffer from insufficient information interaction, recent generative recommendation models partially address this issue by directly generating item predictions. To better capture user intents, recent studies have introduced a reasoning process into generative recommendation, significantly improving recommendation performance. However, these approaches are constrained by the singularity of item semantic representations, facing challenges such as limited diversity in reasoning pathways and insufficient reliability in the reasoning process. To tackle these issues, we introduce REG4Rec, a reasoning-enhanced generative model that constructs multiple dynamic semantic reasoning paths alongside a self-reflection process, ensuring high-confidence recommendations. Specifically, REG4Rec utilizes an MoE-based parallel quantization codebook (MPQ) to generate multiple unordered semantic tokens for each item, thereby constructing a larger-scale diverse reasoning space. Furthermore, to enhance the reliability of reasoning, we propose a training reasoning enhancement stage, which includes Preference Alignment for Reasoning (PARS) and a Multi-Step Reward Augmentation (MSRA) strategy. PARS uses reward functions tailored for recommendation to enhance reasoning and reflection, while MSRA introduces future multi-step actions to improve overall generalization. During inference, Consistency-Oriented Self-Reflection for Pruning (CORP) is proposed to discard inconsistent reasoning paths, preventing the propagation of erroneous reasoning. Lastly, we develop an efficient offline training strategy for large-scale recommendation. Experiments on real-world datasets and online evaluations show that REG4Rec delivers outstanding performance and substantial practical value. </p>
<blockquote>
<p>序列推荐旨在在大规模推荐系统中预测用户的下一个行为。虽然传统方法常常因信息交互不足而受限，但最近的生成推荐模型通过直接生成项目预测部分地解决了这个问题。为了更好地捕捉用户意图，最近的研究将推理过程引入生成推荐，显著提高了推荐性能。然而，这些方法受到项目语义表示单一性的限制，面临推理路径多样性有限和推理过程可靠性不足等挑战。为了解决这些问题，我们引入了REG4Rec，这是一种增强推理的生成模型，它构建了多个动态语义推理路径，并伴随着一个自我反思过程，确保高置信度的推荐。具体来说，REG4Rec使用一个基于MoE的并行量化代码本（MPQ）来为每个项目生成多个无序语义标记，从而构建一个更大规模的多样化推理空间。此外，为了提高推理的可靠性，我们提出了一种训练推理增强阶段，包括用于推理的偏好对齐（PARS）和多步奖励增强（MSRA）策略。PARS使用针对推荐的奖励函数来增强推理和反思，而MSRA通过引入未来的多步行动来提高整体泛化能力。在推理过程中，我们提出了面向一致性的自我反思修剪（CORP）来丢弃不一致的推理路径，防止错误推理的传播。最后，我们为大规模推荐开发了一种高效的离线训练策略。在现实数据集上的实验和在线评估表明，REG4Rec表现出卓越的性能和巨大的实用价值。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15308v1">PDF</a> </p>
<p><strong>Summary</strong>：<br>新一代推荐系统REG4Rec结合生成式推荐与动态语义推理路径，以提高用户意图捕捉和推荐性能。通过构建多个动态语义推理路径和自反思过程，解决单一语义表示带来的挑战。REG4Rec使用基于MoE的并行量化代码本生成多个无序语义令牌，构建大规模多样化推理空间。同时，通过训练增强推理阶段和一致性导向的自反思策略，提高推理的可靠性和准确性。实验证明，REG4Rec在真实数据集和在线评估中表现出卓越性能。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>REG4Rec结合生成式推荐与动态语义推理，旨在提高捕捉用户意图和推荐性能。</li>
<li>通过构建多个动态语义推理路径，解决单一语义表示的限制和挑战。</li>
<li>使用基于MoE的并行量化代码本生成多个无序语义令牌，创建大规模多样化推理空间。</li>
<li>通过训练增强推理阶段，包括偏好对齐推理和奖励增强策略，提高推理可靠性。</li>
<li>在推理过程中引入一致性导向的自反思策略，避免错误推理的传播。</li>
<li>REG4Rec采用高效离线训练策略，适用于大规模推荐场景。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15308">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-baf9de2ee41d8c3088aba597445b7d06.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1cb3462ddf8590da77885761b5f35781.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56a5150b27ae9bdab8a06f3adc518f91.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="RATopo-Improving-Lane-Topology-Reasoning-via-Redundancy-Assignment"><a href="#RATopo-Improving-Lane-Topology-Reasoning-via-Redundancy-Assignment" class="headerlink" title="RATopo: Improving Lane Topology Reasoning via Redundancy Assignment"></a>RATopo: Improving Lane Topology Reasoning via Redundancy Assignment</h2><p><strong>Authors:Han Li, Shaofei Huang, Longfei Xu, Yulu Gao, Beipeng Mu, Si Liu</strong></p>
<p>Lane topology reasoning plays a critical role in autonomous driving by modeling the connections among lanes and the topological relationships between lanes and traffic elements. Most existing methods adopt a first-detect-then-reason paradigm, where topological relationships are supervised based on the one-to-one assignment results obtained during the detection stage. This supervision strategy results in suboptimal topology reasoning performance due to the limited range of valid supervision. In this paper, we propose RATopo, a Redundancy Assignment strategy for lane Topology reasoning that enables quantity-rich and geometry-diverse topology supervision. Specifically, we restructure the Transformer decoder by swapping the cross-attention and self-attention layers. This allows redundant lane predictions to be retained before suppression, enabling effective one-to-many assignment. We also instantiate multiple parallel cross-attention blocks with independent parameters, which further enhances the diversity of detected lanes. Extensive experiments on OpenLane-V2 demonstrate that our RATopo strategy is model-agnostic and can be seamlessly integrated into existing topology reasoning frameworks, consistently improving both lane-lane and lane-traffic topology performance. </p>
<blockquote>
<p>车道拓扑推理在自动驾驶中扮演着重要角色，它通过建模车道之间的连接以及车道与交通元素之间的拓扑关系来实现。现有大多数方法采用先检测再推理的模式，其中拓扑关系是基于检测阶段获得的一一对应关系结果进行监督的。这种监督策略由于有效的监督范围有限，导致拓扑推理性能不佳。在本文中，我们提出了RATopo，这是一种用于车道拓扑推理的冗余分配策略，能够实现丰富数量和多样几何形状的拓扑监督。具体来说，我们通过交换Transformer解码器的交叉注意力和自注意力层来重建其结构。这允许在抑制之前保留冗余的车道预测，从而实现有效的一对多分配。我们还实例化了具有独立参数的多个并行交叉注意力块，这进一步增强了检测到的车道的多样性。在OpenLane-V2上的大量实验表明，我们的RATopo策略与模型无关，可以无缝集成到现有的拓扑推理框架中，持续提高车道与车道以及车道与交通的拓扑性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15272v1">PDF</a> Accepted by ACM MM 2025</p>
<p><strong>摘要</strong></p>
<p>车道拓扑推理在自动驾驶中起到关键作用，通过对车道间连接以及车道与交通元素间的拓扑关系进行建模来实现。现有方法通常采用先检测后推理的模式，基于检测阶段获得的一一对应关系结果来监督拓扑关系。这种监督策略因有效监督范围有限而导致拓扑推理性能不佳。本文提出RATopo，一种车道拓扑推理中的冗余分配策略，实现丰富数量和几何多样的拓扑监督。具体来说，我们重构Transformer解码器，通过交换交叉注意力和自注意力层来保留冗余车道预测，实现有效的一对多分配。我们还实例化具有独立参数的多个并行交叉注意力块，进一步提高检测到的车道的多样性。在OpenLane-V2上的大量实验表明，我们的RATopo策略具有模型无关性，可以无缝集成到现有的拓扑推理框架中，持续提高车道与车道、车道与交通的拓扑性能。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>车道拓扑推理在自动驾驶中扮演关键角色，通过对车道间以及车道与交通元素的拓扑关系进行建模来实现。</li>
<li>现有方法的监督策略因有效监督范围有限导致拓扑推理性能不佳。</li>
<li>RATopo策略采用冗余分配，实现丰富数量和几何多样的拓扑监督。</li>
<li>重构Transformer解码器，通过交换交叉注意力和自注意力层来保留冗余车道预测。</li>
<li>实例化多个并行交叉注意力块，提高检测到的车道的多样性。</li>
<li>RATopo策略可以无缝集成到现有拓扑推理框架中。</li>
<li>RATopo策略能提高车道与车道、车道与交通的拓扑推理性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15272">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-15315cf3fb40c91f73502c9c02db460a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3501fd3de915b77d62db3ee073d0553d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8af89a55d628e386b5a85c617e43cef0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Select-to-Know-An-Internal-External-Knowledge-Self-Selection-Framework-for-Domain-Specific-Question-Answering"><a href="#Select-to-Know-An-Internal-External-Knowledge-Self-Selection-Framework-for-Domain-Specific-Question-Answering" class="headerlink" title="Select to Know: An Internal-External Knowledge Self-Selection Framework   for Domain-Specific Question Answering"></a>Select to Know: An Internal-External Knowledge Self-Selection Framework   for Domain-Specific Question Answering</h2><p><strong>Authors:Bolei He, Xinran He, Run Shao, Shanfu Shu, Xianwei Xue, Mingquan Cheng, Haifeng Li, Zhenhua Ling</strong></p>
<p>Large Language Models (LLMs) perform well in general QA but often struggle in domain-specific scenarios. Retrieval-Augmented Generation (RAG) introduces external knowledge but suffers from hallucinations and latency due to noisy retrievals. Continued pretraining internalizes domain knowledge but is costly and lacks cross-domain flexibility. We attribute this challenge to the long-tail distribution of domain knowledge, which leaves partial yet useful internal knowledge underutilized. We further argue that knowledge acquisition should be progressive, mirroring human learning: first understanding concepts, then applying them to complex reasoning. To address this, we propose Selct2Know (S2K), a cost-effective framework that internalizes domain knowledge through an internal-external knowledge self-selection strategy and selective supervised fine-tuning. We also introduce a structured reasoning data generation pipeline and integrate GRPO to enhance reasoning ability. Experiments on medical, legal, and financial QA benchmarks show that S2K consistently outperforms existing methods and matches domain-pretrained LLMs with significantly lower cost. </p>
<blockquote>
<p>大型语言模型（LLM）在一般问答任务中表现良好，但在特定领域的场景中经常面临挑战。检索增强生成（RAG）引入了外部知识，但由于检索结果中的噪声而容易出现虚构和延迟。持续预训练可以内化领域知识，但成本高昂且缺乏跨领域灵活性。我们将这一挑战归因于领域知识的长尾分布，这使得部分有用的内部知识未得到充分利用。我们进一步认为，知识获取应该是一个渐进的过程，反映人类学习的方式：首先理解概念，然后将其应用于复杂推理。为了解决这一问题，我们提出了Selct2Know（S2K）这一成本效益高的框架，它通过内部和外部知识的自我选择策略以及选择性监督微调来内化领域知识。我们还引入了一个结构化推理数据生成管道，并整合了GRPO技术以提高推理能力。在医疗、法律和财务问答基准测试上的实验表明，S2K始终优于现有方法，并以更低的成本与领域预训练LLM相匹配。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15213v1">PDF</a> EMNLP2025 Findings</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在一般问答任务中表现良好，但在特定领域场景中常遇到困难。检索增强生成（RAG）引入外部知识，但存在幻象和延迟问题。继续预训练可以内化领域知识，但成本高昂且缺乏跨领域灵活性。本文认为这是由于领域知识的长尾分布导致部分内部知识未被充分利用。知识获取应循序渐进，先理解概念，再应用于复杂推理。为解决这一问题，本文提出Selct2Know（S2K）框架，通过内外部知识自选策略和选择性监督微调来内化领域知识。同时引入结构化推理数据生成管道并结合GRPO增强推理能力。在医疗、法律和财务问答基准测试上的实验表明，S2K持续优于现有方法，并以较低成本匹配领域预训练LLM。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs在特定领域场景中面临挑战，需要更有效的方式内化领域知识。</li>
<li>RAG虽引入外部知识，但存在幻象和延迟问题。</li>
<li>继续预训练虽然能内化领域知识，但成本高昂且缺乏跨领域灵活性。</li>
<li>领域知识的长尾分布导致部分内部知识未被充分利用。</li>
<li>知识获取应循序渐进，先理解概念再应用复杂推理。</li>
<li>S2K框架通过内外部知识自选策略和选择性监督微调来内化领域知识，提高性能并降低成本。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15213">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-611a6567f3035bdafd56becc4ff290a0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6cb895834f25af6d89f3227364dc23bc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1b978f4f7fcd1a5114a27b65fb25a8e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d84a6621529e377ab1a47ca8ee38261.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-86d3467265a4508ef430aa4a135cb494.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f87c14da69d4fa2ccaa40990678153e.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Fin-PRM-A-Domain-Specialized-Process-Reward-Model-for-Financial-Reasoning-in-Large-Language-Models"><a href="#Fin-PRM-A-Domain-Specialized-Process-Reward-Model-for-Financial-Reasoning-in-Large-Language-Models" class="headerlink" title="Fin-PRM: A Domain-Specialized Process Reward Model for Financial   Reasoning in Large Language Models"></a>Fin-PRM: A Domain-Specialized Process Reward Model for Financial   Reasoning in Large Language Models</h2><p><strong>Authors:Yuanchen Zhou, Shuo Jiang, Jie Zhu, Junhui Li, Lifan Guo, Feng Chen, Chi Zhang</strong></p>
<p>Process Reward Models (PRMs) have emerged as a promising framework for supervising intermediate reasoning in large language models (LLMs), yet existing PRMs are primarily trained on general or Science, Technology, Engineering, and Mathematics (STEM) domains and fall short in domain-specific contexts such as finance, where reasoning is more structured, symbolic, and sensitive to factual and regulatory correctness. We introduce \textbf{Fin-PRM}, a domain-specialized, trajectory-aware PRM tailored to evaluate intermediate reasoning steps in financial tasks. Fin-PRM integrates step-level and trajectory-level reward supervision, enabling fine-grained evaluation of reasoning traces aligned with financial logic. We apply Fin-PRM in both offline and online reward learning settings, supporting three key applications: (i) selecting high-quality reasoning trajectories for distillation-based supervised fine-tuning, (ii) providing dense process-level rewards for reinforcement learning, and (iii) guiding reward-informed Best-of-N inference at test time. Experimental results on financial reasoning benchmarks, including CFLUE and FinQA, demonstrate that Fin-PRM consistently outperforms general-purpose PRMs and strong domain baselines in trajectory selection quality. Downstream models trained with Fin-PRM yield substantial improvements with baselines, with gains of 12.9% in supervised learning, 5.2% in reinforcement learning, and 5.1% in test-time performance. These findings highlight the value of domain-specialized reward modeling for aligning LLMs with expert-level financial reasoning. Our project resources will be available at <a target="_blank" rel="noopener" href="https://github.com/aliyun/qwen-dianjin">https://github.com/aliyun/qwen-dianjin</a>. </p>
<blockquote>
<p>过程奖励模型（PRMs）已成为监督大型语言模型（LLMs）中的中间推理的有前途的框架，但现有的PRM主要训练于通用或科学、技术、工程和数学（STEM）领域，而在金融等特定领域的上下文中表现不足，这里的推理更加结构化、符号化，并且对事实和法规的正确性更加敏感。我们引入了<strong>Fin-PRM</strong>，这是一个针对金融任务中的中间推理步骤进行评估的域专业化、轨迹感知的PRM。Fin-PRM集成了步骤级和轨迹级的奖励监督，能够实现与金融逻辑对齐的推理轨迹的精细评估。我们在离线奖励学习和在线奖励学习环境中都应用了Fin-PRM，支持三个关键应用：（i）选择基于蒸馏的有监督微调的高质量推理轨迹，（ii）为强化学习提供密集的过程级奖励，（iii）在测试时指导基于奖励的Best-of-N推理。在金融推理基准测试上的实验结果，包括CFLUE和FinQA，证明了Fin-PRM在轨迹选择质量上始终优于通用PRM和强基线。使用Fin-PRM训练的下游模型与基线相比实现了显著改进，监督学习提高了12.9%，强化学习提高了5.2%，测试时性能提高了5.1%。这些发现凸显了领域专业化奖励模型对于将大型语言模型与专家级金融推理对齐的价值。我们的项目资源将在<a target="_blank" rel="noopener" href="https://github.com/aliyun/qwen-dianjin">https://github.com/aliyun/qwen-dianjin</a>上提供。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15202v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对金融领域特殊需求的进程奖励模型（Fin-PRM）。该模型能够在金融任务的推理过程中进行精细化评估，结合了步骤级和轨迹级的奖励监督，符合金融逻辑。实验结果表明，Fin-PRM在财务推理基准测试上的表现优于通用进程奖励模型和强基准线，能提高下游模型的轨迹选择质量。通过Fin-PRM，下游模型的性能得到了显著提高，在监督学习、强化学习和测试时间性能方面的提升分别达到了12.9%、5.2%和5.1%。这凸显了针对金融领域进行专项奖励建模的价值。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Fin-PRM是一个针对金融领域的专业化进程奖励模型，用于评估金融任务中的推理过程。</li>
<li>Fin-PRM结合了步骤级和轨迹级的奖励监督，以精细的方式评估推理轨迹，与金融逻辑相符。</li>
<li>Fin-PRM在财务推理基准测试上的表现优于通用进程奖励模型和强基准线。</li>
<li>Fin-PRM能提高下游模型的轨迹选择质量，包括用于监督学习、强化学习和测试时间的推理轨迹选择。</li>
<li>通过Fin-PRM，下游模型的性能得到了显著提高，包括在监督学习、强化学习和测试时间方面的提升。</li>
<li>实验结果证明了针对特定领域（如金融）进行奖励建模的重要性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15202">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-545aa4f6f5248d09774e002dfe6aff64.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9dab5e23a22de821731e549e3319b69f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f0dbfa9077986a46a7533349bb534f9e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be4d05025f9fa88c555f18febdf9f814.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="PuzzleClone-An-SMT-Powered-Framework-for-Synthesizing-Verifiable-Data"><a href="#PuzzleClone-An-SMT-Powered-Framework-for-Synthesizing-Verifiable-Data" class="headerlink" title="PuzzleClone: An SMT-Powered Framework for Synthesizing Verifiable Data"></a>PuzzleClone: An SMT-Powered Framework for Synthesizing Verifiable Data</h2><p><strong>Authors:Kai Xiong, Yanwei Huang, Rongjunchen Zhang, Kun Chen, Haipang Wu</strong></p>
<p>High-quality mathematical and logical datasets with verifiable answers are essential for strengthening the reasoning capabilities of large language models (LLMs). While recent data augmentation techniques have facilitated the creation of large-scale benchmarks, existing LLM-generated datasets often suffer from limited reliability, diversity, and scalability. To address these challenges, we introduce PuzzleClone, a formal framework for synthesizing verifiable data at scale using Satisfiability Modulo Theories (SMT). Our approach features three key innovations: (1) encoding seed puzzles into structured logical specifications, (2) generating scalable variants through systematic variable and constraint randomization, and (3) ensuring validity via a reproduction mechanism. Applying PuzzleClone, we construct a curated benchmark comprising over 83K diverse and programmatically validated puzzles. The generated puzzles span a wide spectrum of difficulty and formats, posing significant challenges to current state-of-the-art models. We conduct post training (SFT and RL) on PuzzleClone datasets. Experimental results show that training on PuzzleClone yields substantial improvements not only on PuzzleClone testset but also on logic and mathematical benchmarks. Post training raises PuzzleClone average from 14.4 to 56.2 and delivers consistent improvements across 7 logic and mathematical benchmarks up to 12.5 absolute percentage points (AMC2023 from 52.5 to 65.0). Our code and data are available at <a target="_blank" rel="noopener" href="https://github.com/puzzleclone">https://github.com/puzzleclone</a>. </p>
<blockquote>
<p>高质量的数学和逻辑数据集对于增强大型语言模型（LLM）的推理能力至关重要。虽然最近的数据增强技术有助于创建大规模基准测试，但现有的LLM生成的数据集往往存在可靠性、多样性和可扩展性方面的局限性。为了应对这些挑战，我们引入了PuzzleClone，这是一个使用可满足性模理论（SMT）大规模合成可验证数据的正式框架。我们的方法有三个关键创新点：（1）将种子谜题编码为结构化的逻辑规范，（2）通过系统的变量和约束随机化生成可扩展的变体，（3）通过复制机制确保有效性。应用PuzzleClone，我们构建了一个包含超过83000个多样且程序验证有效的谜题的精选基准测试。生成的谜题难度各异，形式多样，对当前最先进的模型构成了重大挑战。我们在PuzzleClone数据集上进行训练后（SFT和RL）。实验结果表明，在PuzzleClone数据集上进行训练不仅提高了其在PuzzleClone测试集上的表现，而且在逻辑和数学基准测试上也有显著改善。训练后，PuzzleClone的平均分数从14.4提高到56.2，在7个逻辑和数学基准测试中均取得了一致的改进，绝对百分点最高提高了12.5（AMC2023从52.5提高到65.0）。我们的代码和数据可在<a target="_blank" rel="noopener" href="https://github.com/puzzleclone%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/puzzleclone找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15180v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于可验证答案的高质量数学和逻辑数据集对于加强大型语言模型（LLM）的推理能力至关重要。针对现有LLM生成数据集在可靠性、多样性和可扩展性方面的局限性，本文提出了PuzzleClone框架，通过可满足性模理论（SMT）大规模合成可验证数据。PuzzleClone通过三个关键创新点实现了突破：将种子谜题编码为结构化逻辑规范、通过系统变量和约束随机化生成可扩展变体，以及通过复制机制确保有效性。应用PuzzleClone框架构建了一个包含超过8.3万个多样化和程序验证的谜题基准测试集。实验结果表明，在PuzzleClone数据集上进行训练不仅提高了其在PuzzleClone测试集上的性能，而且在逻辑和数学基准测试上也取得了显著改进。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>高质量数学和逻辑数据集对加强LLM推理能力至关重要。</li>
<li>现有LLM生成数据集存在可靠性、多样性和可扩展性问题。</li>
<li>PuzzleClone框架通过SMT合成可验证数据，具有三个关键创新点。</li>
<li>PuzzleClone构建了包含超过8.3万个谜题的大规模基准测试集。</li>
<li>在PuzzleClone数据集上训练LLM，不仅提高了其在测试集上的性能，也改进了逻辑和数学基准测试成绩。</li>
<li>PuzzleClone平均成绩从14.4提高到56.2，在7个逻辑和数学基准测试中实现了一致性改进。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15180">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-28f1bddd067d2ab326b0f1be5c485ec4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9118212d41d053a1db7780101755332b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4573ab35fabe0ee9725776cd46347238.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-047613e95caa41543f12b152d660e996.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dce28d40554dcf5f23fbf05f15b9f699.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d7fc4713133ae2bfa175081556b8242.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="ContextualLVLM-Agent-A-Holistic-Framework-for-Multi-Turn-Visually-Grounded-Dialogue-and-Complex-Instruction-Following"><a href="#ContextualLVLM-Agent-A-Holistic-Framework-for-Multi-Turn-Visually-Grounded-Dialogue-and-Complex-Instruction-Following" class="headerlink" title="ContextualLVLM-Agent: A Holistic Framework for Multi-Turn   Visually-Grounded Dialogue and Complex Instruction Following"></a>ContextualLVLM-Agent: A Holistic Framework for Multi-Turn   Visually-Grounded Dialogue and Complex Instruction Following</h2><p><strong>Authors:Seungmin Han, Haeun Kwon, Ji-jun Park, Taeyang Yoon</strong></p>
<p>Despite significant advancements in Large Language Models (LLMs) and Large Vision-Language Models (LVLMs), current models still face substantial challenges in handling complex, multi-turn, and visually-grounded tasks that demand deep reasoning, sustained contextual understanding, entity tracking, and multi-step instruction following. Existing benchmarks often fall short in capturing the dynamism and intricacies of real-world multi-modal interactions, leading to issues such as context loss and visual hallucinations. To address these limitations, we introduce MMDR-Bench (Multi-Modal Dialogue Reasoning Benchmark), a novel dataset comprising 300 meticulously designed complex multi-turn dialogue scenarios, each averaging 5-7 turns and evaluated across six core dimensions including visual entity tracking and reasoning depth. Furthermore, we propose CoLVLM Agent (Contextual LVLM Agent), a holistic framework that enhances existing LVLMs with advanced reasoning and instruction following capabilities through an iterative “memory-perception-planning-execution” cycle, requiring no extensive re-training of the underlying models. Our extensive experiments on MMDR-Bench demonstrate that CoLVLM Agent consistently achieves superior performance, attaining an average human evaluation score of 4.03, notably surpassing state-of-the-art commercial models like GPT-4o (3.92) and Gemini 1.5 Pro (3.85). The framework exhibits significant advantages in reasoning depth, instruction adherence, and error suppression, and maintains robust performance over extended dialogue turns, validating the effectiveness of its modular design and iterative approach for complex multi-modal interactions. </p>
<blockquote>
<p>尽管大型语言模型（LLM）和大型视觉语言模型（LVLM）取得了重大进展，但当前模型在处理复杂、多轮、视觉基础的任务时仍面临巨大挑战，这些任务需要深度推理、持续上下文理解、实体跟踪以及多步骤指令执行。现有的基准测试通常无法捕捉真实世界多模态交互的动态性和复杂性，导致诸如上下文丢失和视觉幻觉等问题。为了解决这些局限性，我们推出了MMDR-Bench（多模态对话推理基准测试），这是一个新的数据集，包含了300个精心设计的复杂多轮对话场景，每个场景平均有5-7轮对话，并在六个核心维度进行评估，包括视觉实体跟踪和推理深度等。此外，我们提出了CoLVLM Agent（上下文LVLM代理）这一全面框架，它通过迭代“记忆-感知-规划-执行”循环，增强了现有LVLMs的推理和指令执行能力，无需对底层模型进行大量再训练。我们在MMDR-Bench上进行的大量实验表明，CoLVLM Agent始终实现卓越性能，平均人类评价得分为4.03，显著超过了最先进的商业模型，如GPT-4o（3.92）和Gemini 1.5 Pro（3.85）。该框架在推理深度、指令遵循和错误抑制方面表现出显著优势，并在较长的对话轮次中保持稳健性能，验证了其在复杂多模态交互中的模块化设计和迭代方法的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15164v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了大型语言模型（LLMs）和大型视觉语言模型（LVLMs）的最新进展，指出了它们在处理复杂、多轮、视觉定位的任务方面的挑战。为此，引入MMDR-Bench数据集和CoLVLM Agent框架。MMDR-Bench包含300个精心设计的复杂多轮对话场景，评估核心维度包括视觉实体跟踪和推理深度。CoLVLM Agent通过迭代“记忆-感知-规划-执行”循环增强现有LVLMs的推理和指令遵循能力，无需对底层模型进行大量再训练。在MMDR-Bench上的实验表明，CoLVLM Agent性能卓越，平均人类评价得分为4.03，超越了GPT-4o（3.92）和Gemini 1.5 Pro（3.85）。该框架在推理深度、指令遵循和错误抑制方面表现出显著优势，并在多轮对话中保持稳健性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型和视觉语言模型在处理复杂、多轮、视觉定位的任务时仍面临挑战。</li>
<li>MMDR-Bench数据集旨在解决这一挑战，包含复杂多轮对话场景，强调视觉实体跟踪和推理深度。</li>
<li>CoLVLM Agent框架通过迭代循环增强现有LVLMs的推理和指令遵循能力，无需大量再训练。</li>
<li>CoLVLM Agent在MMDR-Bench上的实验表现优于GPT-4o和Gemini 1.5 Pro等现有模型。</li>
<li>该框架在推理深度、指令遵循和错误抑制方面表现出显著优势。</li>
<li>CoLVLM Agent在多轮对话中保持稳健性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15164">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b9f34b70ddaf3d8a49ffa0603f471cfe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-31cece34279ee1c4d34074a9393529ee.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3fc19417e0f1d62bfd6e8593cda009c7.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Mobile-Agent-v3-Foundamental-Agents-for-GUI-Automation"><a href="#Mobile-Agent-v3-Foundamental-Agents-for-GUI-Automation" class="headerlink" title="Mobile-Agent-v3: Foundamental Agents for GUI Automation"></a>Mobile-Agent-v3: Foundamental Agents for GUI Automation</h2><p><strong>Authors:Jiabo Ye, Xi Zhang, Haiyang Xu, Haowei Liu, Junyang Wang, Zhaoqing Zhu, Ziwei Zheng, Feiyu Gao, Junjie Cao, Zhengxi Lu, Jitong Liao, Qi Zheng, Fei Huang, Jingren Zhou, Ming Yan</strong></p>
<p>This paper introduces GUI-Owl, a foundational GUI agent model that achieves state-of-the-art performance among open-source end-to-end models on ten GUI benchmarks across desktop and mobile environments, covering grounding, question answering, planning, decision-making, and procedural knowledge. GUI-Owl-7B achieves 66.4 on AndroidWorld and 29.4 on OSWorld. Building on this, we propose Mobile-Agent-v3, a general-purpose GUI agent framework that further improves performance to 73.3 on AndroidWorld and 37.7 on OSWorld, setting a new state-of-the-art for open-source GUI agent frameworks. GUI-Owl incorporates three key innovations: (1) Large-scale Environment Infrastructure: a cloud-based virtual environment spanning Android, Ubuntu, macOS, and Windows, enabling our Self-Evolving GUI Trajectory Production framework. This generates high-quality interaction data via automated query generation and correctness validation, leveraging GUI-Owl to refine trajectories iteratively, forming a self-improving loop. It supports diverse data pipelines and reduces manual annotation. (2) Diverse Foundational Agent Capabilities: by integrating UI grounding, planning, action semantics, and reasoning patterns, GUI-Owl supports end-to-end decision-making and can act as a modular component in multi-agent systems. (3) Scalable Environment RL: we develop a scalable reinforcement learning framework with fully asynchronous training for real-world alignment. We also introduce Trajectory-aware Relative Policy Optimization (TRPO) for online RL, achieving 34.9 on OSWorld. GUI-Owl and Mobile-Agent-v3 are open-sourced at <a target="_blank" rel="noopener" href="https://github.com/X-PLUG/MobileAgent">https://github.com/X-PLUG/MobileAgent</a>. </p>
<blockquote>
<p>本文介绍了GUI-Owl，这是一个基础GUI代理模型，它在桌面和移动环境的十个GUI基准测试上实现了开源端到端模型的最新性能，涵盖了接地、问答、规划、决策和程序知识。GUI-Owl-7B在AndroidWorld上达到66.4，在OSWorld上达到29.4。在此基础上，我们提出了通用GUI代理框架Mobile-Agent-v3，它进一步提高了性能，在AndroidWorld上达到73.3，在OSWorld上达到37.7，为开源GUI代理框架创造了新的最新纪录。GUI-Owl融合了三大创新点：（1）大规模环境基础设施：一个跨越Android、Ubuntu、macOS和Windows的基于云端的虚拟环境，使我们的自我进化GUI轨迹生产框架成为可能。该框架通过自动化查询生成和正确性验证生成高质量交互数据，利用GUI-Owl迭代地优化轨迹，形成一个自我改进循环。它支持各种数据管道，减少手动注释。（2）多样化的基础代理功能：通过集成UI接地、规划、动作语义和推理模式，GUI-Owl支持端到端的决策制定，可以作为多代理系统中的模块化组件。（3）可扩展的环境强化学习：我们开发了一个可扩展的强化学习框架，具有完全异步训练以符合现实世界的对齐。我们还引入了轨迹感知相对策略优化（TRPO）进行在线强化学习，在OSWorld上实现34.9的成绩。GUI-Owl和Mobile-Agent-v3已在<a target="_blank" rel="noopener" href="https://github.com/X-PLUG/Mobileagent%E5%BC%80%E6%BA%90%E3%80%82">https://github.com/X-PLUG/Mobileagent开源。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15144v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>本文介绍了GUI-Owl，这是一个基础GUI代理模型，在桌面和移动环境的十个GUI基准测试上实现了开创性的性能。它在AndroidWorld上的得分为66.4，在OSWorld上的得分为29.4。在此基础上，提出了通用GUI代理框架Mobile-Agent-v3，进一步提高了性能，在AndroidWorld上得分73.3，在OSWorld上得分37.7，为开源GUI代理框架创造了新的世界纪录。GUI-Owl包含三个关键创新点：大规模环境基础设施、多样化的基础代理功能和可扩展的环境强化学习。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>GUI-Owl模型：介绍了GUI-Owl，一个性能出色的开源端到端GUI代理模型，它在多个GUI基准测试中表现出色。</li>
<li>Mobile-Agent-v3框架：提出的Mobile-Agent-v3框架进一步提高了GUI代理的性能，并在AndroidWorld和OSWorld上达到了新的世界纪录。</li>
<li>大规模环境基础设施：通过云端的虚拟环境，实现了自我进化的GUI轨迹生产框架，减少了手动标注，并支持多种数据管道。</li>
<li>多样化的基础代理能力：GUI-Owl集成了UI定位、规划、动作语义和推理模式，支持端到端的决策制定，并可作为多代理系统中的模块化组件。</li>
<li>可扩展的环境强化学习：开发了具有完全异步训练的可扩展强化学习框架，并引入了轨迹感知相对策略优化（TRPO）进行在线强化学习。</li>
<li>GUI-Owl和Mobile-Agent-v3的开源性：这两个模型和框架都是开源的，方便公众访问和贡献。</li>
<li>模型与框架的应用前景：GUI-Owl和Mobile-Agent-v3在移动和桌面环境中的GUI代理任务上表现出色，为未来的人机交互技术提供了新的方向。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15144">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-80c97cadc0da7730513a65955695be8c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a88af975441203b8a9da9f9fd7299d7f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8c532b38d8f80847c616baa04ff12c73.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a5aeaa9cb8684922bcd597a5f2ea0a43.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6366c4e2b849c160d1da1ff1330d53bd.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="On-the-need-to-perform-comprehensive-evaluations-of-automated-program-repair-benchmarks-Sorald-case-study"><a href="#On-the-need-to-perform-comprehensive-evaluations-of-automated-program-repair-benchmarks-Sorald-case-study" class="headerlink" title="On the need to perform comprehensive evaluations of automated program   repair benchmarks: Sorald case study"></a>On the need to perform comprehensive evaluations of automated program   repair benchmarks: Sorald case study</h2><p><strong>Authors:Sumudu Liyanage, Sherlock A. Licorish, Markus Wagner, Stephen G. MacDonell</strong></p>
<p>In supporting the development of high-quality software, especially necessary in the era of LLMs, automated program repair (APR) tools aim to improve code quality by automatically addressing violations detected by static analysis profilers. Previous research tends to evaluate APR tools only for their ability to clear violations, neglecting their potential introduction of new (sometimes severe) violations, changes to code functionality and degrading of code structure. There is thus a need for research to develop and assess comprehensive evaluation frameworks for APR tools. This study addresses this research gap, and evaluates Sorald (a state-of-the-art APR tool) as a proof of concept. Sorald’s effectiveness was evaluated in repairing 3,529 SonarQube violations across 30 rules within 2,393 Java code snippets extracted from Stack Overflow. Outcomes show that while Sorald fixes specific rule violations, it introduced 2,120 new faults (32 bugs, 2088 code smells), reduced code functional correctness–as evidenced by a 24% unit test failure rate–and degraded code structure, demonstrating the utility of our framework. Findings emphasize the need for evaluation methodologies that capture the full spectrum of APR tool effects, including side effects, to ensure their safe and effective adoption. </p>
<blockquote>
<p>在支持高质量软件开发的时代，特别是在大型语言模型（LLMs）的时代，自动化程序修复（APR）工具旨在通过自动解决静态分析分析器检测到的违规情况来提高代码质量。以往的研究往往只评估APR工具清除违规的能力，忽视了它们可能引入新的（有时是严重的）违规情况、对代码功能的改变以及代码结构的退化。因此，有必要进行研究，开发和评估APR工具的全面评估框架。本研究解决了这一研究空白，并作为概念验证评估了最新颖的APR工具Sorald。在修复从Stack Overflow提取的3,529个SonarQube违规情况（涉及30条规则，涵盖2,393个Java代码片段）方面，对Sorald的有效性进行了评估。结果表明，虽然Sorald能够修复特定的规则违规情况，但它引入了2,120个新故障（包括32个错误，2,088个代码异味），降低了代码功能的正确性（有24%的单元测试失败率作为证据），并破坏了代码结构，证明了我们的框架的实用性。研究结果强调需要一种评估方法，该方法能够捕捉到APR工具的所有影响，包括副作用，以确保其安全和有效的采用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15135v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该文探讨了在LLM时代高质量软件开发中自动化程序修复工具的重要性及其评价框架的缺失。文章以Sorald这一前沿APR工具为例，通过对其修复SonarQube检测到的3,529个违规实例的效果进行评估，发现虽然Sorald能够修复特定规则违规，但同时也引入了新的缺陷（包括32个错误和大量代码异味），并对代码功能正确性产生负面影响（单元测试失败率达到了24%），同时导致代码结构退化。因此，文章强调了开发全面评估APR工具的重要性，特别是需要评估方法能够捕捉工具的所有影响，包括副作用，以确保其安全和有效的采用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自动化程序修复工具旨在提高代码质量，特别是在LLM时代显得尤为重要。</li>
<li>现有研究往往只关注APR工具修复违规的能力，而忽视了其可能引入新的缺陷、改变代码功能以及破坏代码结构的风险。</li>
<li>Sorald作为前沿APR工具的代表，在修复特定违规实例的同时，也引入了大量新的缺陷和代码异味。</li>
<li>Sorald对代码功能正确性产生了负面影响，导致单元测试失败率上升。</li>
<li>代码结构在修复过程中可能出现退化现象。</li>
<li>当前研究需要开发全面的评估框架来全面评估APR工具的影响，包括其对代码质量、功能性和结构性的影响。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15135">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-0583bcfdb6703ac2cc553b5dbd135c90.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b0cf0d35917c2273b5b32eedcce95b38.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3e4f7a4ffb3806f7a90b623502402d4b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f7cafd50143d7bee641570612590e49f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f949cd5f933ed7dac4f89bde1171ea8.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Don’t-Think-Twice-Over-Reasoning-Impairs-Confidence-Calibration"><a href="#Don’t-Think-Twice-Over-Reasoning-Impairs-Confidence-Calibration" class="headerlink" title="Don’t Think Twice! Over-Reasoning Impairs Confidence Calibration"></a>Don’t Think Twice! Over-Reasoning Impairs Confidence Calibration</h2><p><strong>Authors:Romain Lacombe, Kerrie Wu, Eddie Dilworth</strong></p>
<p>Large Language Models deployed as question answering tools require robust calibration to avoid overconfidence. We systematically evaluate how reasoning capabilities and budget affect confidence assessment accuracy, using the ClimateX dataset (Lacombe et al., 2023) and expanding it to human and planetary health. Our key finding challenges the “test-time scaling” paradigm: while recent reasoning LLMs achieve 48.7% accuracy in assessing expert confidence, increasing reasoning budgets consistently impairs rather than improves calibration. Extended reasoning leads to systematic overconfidence that worsens with longer thinking budgets, producing diminishing and negative returns beyond modest computational investments. Conversely, search-augmented generation dramatically outperforms pure reasoning, achieving 89.3% accuracy by retrieving relevant evidence. Our results suggest that information access, rather than reasoning depth or inference budget, may be the critical bottleneck for improved confidence calibration of knowledge-intensive tasks. </p>
<blockquote>
<p>大型语言模型作为问答工具部署时，需要进行稳健的校准以避免过度自信。我们系统地评估了推理能力和预算对信心评估精度的影响，使用了ClimateX数据集（Lacombe等人，2023年），并将其扩展到人类和星球健康领域。我们的关键发现对“测试时缩放”范式提出了挑战：虽然最近的推理大型语言模型在评估专家信心方面达到了48.7%的准确度，但增加推理预算却会损害而不是改善校准。过度推理导致系统过度自信，随着思考预算的增加而恶化，超出适度计算投资后产生收益递减和负面回报。相反，搜索增强生成法显著优于纯推理法，通过检索相关证据达到了89.3%的准确度。我们的结果表明，信息获取可能而不是推理深度或推理预算成为改进知识密集型任务信心校准的关键瓶颈。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15050v1">PDF</a> Published at ICML 2025 Workshop on Reliable and Responsible   Foundation Models</p>
<p><strong>Summary</strong></p>
<p>大型语言模型作为问答工具需要可靠的校准以避免过度自信。本文通过ClimateX数据集（Lacombe等人，2023年）系统评估推理能力和预算对信心评估准确性的影响，并扩展到人类和地球健康领域。研究发现，增加推理预算并不会改善校准，反而可能导致系统性过度自信，并且在较长的思考预算下产生负面回报。相反，搜索增强生成法显著优于纯推理法，通过检索相关证据达到89.3%的准确率。这表明信息获取可能是改进知识密集型任务信心校准的关键瓶颈，而非推理深度或推理预算。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型作为问答工具需要校准以避免过度自信。</li>
<li>增加推理预算不一定会提高信心评估的准确性，反而可能导致系统性过度自信。</li>
<li>搜索增强生成法通过检索相关证据显著提高信心评估的准确率。</li>
<li>信息获取是改进知识密集型任务信心校准的关键。</li>
<li>推理深度并非改善信心校准的主要因素。</li>
<li>适度的计算投资可以获得较好的信心评估效果，过多的计算资源可能导致负面回报。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15050">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-19af5689bcd8895607883d6ad5241a80.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-52ca86482f459a221ab6bd72287e78a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73c54e61acaf5ebbe0e66e84eeabe59c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d740cd23c54ddf313fc82ca8fb5bc9f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a5b09182a8735a2885c64a9e8599fbed.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="In-Context-Iterative-Policy-Improvement-for-Dynamic-Manipulation"><a href="#In-Context-Iterative-Policy-Improvement-for-Dynamic-Manipulation" class="headerlink" title="In-Context Iterative Policy Improvement for Dynamic Manipulation"></a>In-Context Iterative Policy Improvement for Dynamic Manipulation</h2><p><strong>Authors:Mark Van der Merwe, Devesh Jha</strong></p>
<p>Attention-based architectures trained on internet-scale language data have demonstrated state of the art reasoning ability for various language-based tasks, such as logic problems and textual reasoning. Additionally, these Large Language Models (LLMs) have exhibited the ability to perform few-shot prediction via in-context learning, in which input-output examples provided in the prompt are generalized to new inputs. This ability furthermore extends beyond standard language tasks, enabling few-shot learning for general patterns. In this work, we consider the application of in-context learning with pre-trained language models for dynamic manipulation. Dynamic manipulation introduces several crucial challenges, including increased dimensionality, complex dynamics, and partial observability. To address this, we take an iterative approach, and formulate our in-context learning problem to predict adjustments to a parametric policy based on previous interactions. We show across several tasks in simulation and on a physical robot that utilizing in-context learning outperforms alternative methods in the low data regime. Video summary of this work and experiments can be found <a target="_blank" rel="noopener" href="https://youtu.be/2inxpdrq74U?si=dAdDYsUEr25nZvRn">https://youtu.be/2inxpdrq74U?si=dAdDYsUEr25nZvRn</a>. </p>
<blockquote>
<p>基于互联网规模语言数据训练的注意力架构在各种语言任务上展现出了最前沿的推理能力，如逻辑问题和文本推理。此外，这些大型语言模型（LLM）还表现出通过上下文学习进行少样本预测的能力，其中根据提示提供的输入输出例子可以推广到新的输入。这种能力还超越了标准语言任务，使少样本学习能够应用于一般模式。在这项工作中，我们考虑了使用预训练语言模型进行上下文学习的应用，用于动态操作。动态操作带来了几个关键挑战，包括维度增加、动态复杂性和部分可观察性。为了解决这个问题，我们采取了一种迭代方法，并将我们的上下文学习问题制定为基于先前交互预测参数策略的调整。我们在模拟和实体机器人上的多个任务中都表明，利用上下文学习优于低数据环境下的其他方法。该工作和实验的视频摘要可以在<a target="_blank" rel="noopener" href="https://youtu.be/2inxpdrq74U?si=dAdDYsUEr25nZvRn%E6%89%BE%E5%88%B0%E3%80%82">https://youtu.be/2inxpdrq74U?si=dAdDYsUEr25nZvRn找到。</a></p>
</blockquote>
<p><strong>简化说明（非正式版）</strong>：</p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15021v1">PDF</a> 14 pages. Accepted at CoRL 2025</p>
<p><strong>Summary</strong></p>
<p>预训练的语言模型借助互联网规模的语料库进行训练，已显示出卓越的推理能力，可在各种语言任务中进行逻辑和文本推理。它们能通过上下文学习进行少量样本预测，该能力适用于更一般的模式。本文探讨在动态操控场景下应用上下文学习的预训练语言模型。动态操控面临高维性、复杂动态和局部观测等挑战。通过迭代方法并预测基于之前交互的参数策略的微调，我们发现在模拟任务和实际机器人任务中，使用上下文学习的效果优于低数据环境下的其他方法。相关视频摘要和实验可通过链接查看：<a target="_blank" rel="noopener" href="https://youtu.be/2inxpdrq74U?si=dAdDYsUEr25nZvRn">视频链接</a>。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>预训练语言模型在大量语言数据上训练后，展现出卓越的语言推理能力。</li>
<li>这些模型能通过上下文学习进行少量样本预测，适用于多种任务。</li>
<li>在动态操控场景中，上下文学习具有应用价值，但面临高维性、复杂动态和局部观测等挑战。</li>
<li>采用迭代方法预测基于之前交互的参数策略的微调来解决这些挑战。</li>
<li>在模拟和真实机器人实验中，上下文学习的效果优于其他方法，特别是在低数据环境下。</li>
<li>视频摘要提供了对工作方法和实验结果的直观展示。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15021">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-e78a45f1a9ad2e62ef1c3af12f67cabb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-21108ab8299f08dfc0b92de1f85ef28c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-976f31de346ee78c33c9d10cf491857c.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="NVIDIA-Nemotron-Nano-2-An-Accurate-and-Efficient-Hybrid-Mamba-Transformer-Reasoning-Model"><a href="#NVIDIA-Nemotron-Nano-2-An-Accurate-and-Efficient-Hybrid-Mamba-Transformer-Reasoning-Model" class="headerlink" title="NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid   Mamba-Transformer Reasoning Model"></a>NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid   Mamba-Transformer Reasoning Model</h2><p><strong>Authors: NVIDIA,  :, Aarti Basant, Abhijit Khairnar, Abhijit Paithankar, Abhinav Khattar, Adithya Renduchintala, Aditya Malte, Akhiad Bercovich, Akshay Hazare, Alejandra Rico, Aleksander Ficek, Alex Kondratenko, Alex Shaposhnikov, Alexander Bukharin, Ali Taghibakhshi, Amelia Barton, Ameya Sunil Mahabaleshwarkar, Amy Shen, Andrew Tao, Ann Guan, Anna Shors, Anubhav Mandarwal, Arham Mehta, Arun Venkatesan, Ashton Sharabiani, Ashwath Aithal, Ashwin Poojary, Ayush Dattagupta, Balaram Buddharaju, Banghua Zhu, Barnaby Simkin, Bilal Kartal, Bita Darvish Rouhani, Bobby Chen, Boris Ginsburg, Brandon Norick, Brian Yu, Bryan Catanzaro, Charles Wang, Charlie Truong, Chetan Mungekar, Chintan Patel, Chris Alexiuk, Christian Munley, Christopher Parisien, Dan Su, Daniel Afrimi, Daniel Korzekwa, Daniel Rohrer, Daria Gitman, David Mosallanezhad, Deepak Narayanan, Dima Rekesh, Dina Yared, Dmytro Pykhtar, Dong Ahn, Duncan Riach, Eileen Long, Elliott Ning, Eric Chung, Erick Galinkin, Evelina Bakhturina, Gargi Prasad, Gerald Shen, Haifeng Qian, Haim Elisha, Harsh Sharma, Hayley Ross, Helen Ngo, Herman Sahota, Hexin Wang, Hoo Chang Shin, Hua Huang, Iain Cunningham, Igor Gitman, Ivan Moshkov, Jaehun Jung, Jan Kautz, Jane Polak Scowcroft, Jared Casper, Jian Zhang, Jiaqi Zeng, Jimmy Zhang, Jinze Xue, Jocelyn Huang, Joey Conway, John Kamalu, Jonathan Cohen, Joseph Jennings, Julien Veron Vialard, Junkeun Yi, Jupinder Parmar, Kari Briski, Katherine Cheung, Katherine Luna, Keith Wyss, Keshav Santhanam, Kezhi Kong, Krzysztof Pawelec, Kumar Anik, Kunlun Li, Kushan Ahmadian, Lawrence McAfee, Laya Sleiman, Leon Derczynski, Luis Vega, Maer Rodrigues de Melo, Makesh Narsimhan Sreedhar, Marcin Chochowski, Mark Cai, Markus Kliegl, Marta Stepniewska-Dziubinska, Matvei Novikov, Mehrzad Samadi, Meredith Price, Meriem Boubdir, Michael Boone, Michael Evans, Michal Bien, Michal Zawalski, Miguel Martinez, Mike Chrzanowski, Mohammad Shoeybi, Mostofa Patwary, Namit Dhameja, Nave Assaf, Negar Habibi, Nidhi Bhatia, Nikki Pope, Nima Tajbakhsh, Nirmal Kumar Juluru, Oleg Rybakov, Oleksii Hrinchuk, Oleksii Kuchaiev, Oluwatobi Olabiyi, Pablo Ribalta, Padmavathy Subramanian, Parth Chadha, Pavlo Molchanov, Peter Dykas, Peter Jin, Piotr Bialecki, Piotr Januszewski, Pradeep Thalasta, Prashant Gaikwad, Prasoon Varshney, Pritam Gundecha, Przemek Tredak, Rabeeh Karimi Mahabadi, Rajen Patel, Ran El-Yaniv, Ranjit Rajan, Ria Cheruvu, Rima Shahbazyan, Ritika Borkar, Ritu Gala, Roger Waleffe, Ruoxi Zhang, Russell J. Hewett, Ryan Prenger, Sahil Jain, Samuel Kriman, Sanjeev Satheesh, Saori Kaji, Sarah Yurick, Saurav Muralidharan, Sean Narenthiran, Seonmyeong Bak, Sepehr Sameni, Seungju Han, Shanmugam Ramasamy, Shaona Ghosh, Sharath Turuvekere Sreenivas, Shelby Thomas, Shizhe Diao, Shreya Gopal, Shrimai Prabhumoye, Shubham Toshniwal, Shuoyang Ding, Siddharth Singh, Siddhartha Jain, Somshubra Majumdar, Soumye Singhal, Stefania Alborghetti, Syeda Nahida Akter, Terry Kong, Tim Moon, Tomasz Hliwiak, Tomer Asida, Tony Wang, Tugrul Konuk, Twinkle Vashishth, Tyler Poon, Udi Karpas, Vahid Noroozi, Venkat Srinivasan, Vijay Korthikanti, Vikram Fugro, Vineeth Kalluru, Vitaly Kurin, Vitaly Lavrukhin, Wasi Uddin Ahmad, Wei Du, Wonmin Byeon, Ximing Lu, Xin Dong, Yashaswi Karnati, Yejin Choi, Yian Zhang, Ying Lin, Yonggan Fu, Yoshi Suhara, Zhen Dong, Zhiyu Li, Zhongbo Zhu, Zijia Chen</strong></p>
<p>We introduce Nemotron-Nano-9B-v2, a hybrid Mamba-Transformer language model designed to increase throughput for reasoning workloads while achieving state-of-the-art accuracy compared to similarly-sized models. Nemotron-Nano-9B-v2 builds on the Nemotron-H architecture, in which the majority of the self-attention layers in the common Transformer architecture are replaced with Mamba-2 layers, to achieve improved inference speed when generating the long thinking traces needed for reasoning. We create Nemotron-Nano-9B-v2 by first pre-training a 12-billion-parameter model (Nemotron-Nano-12B-v2-Base) on 20 trillion tokens using an FP8 training recipe. After aligning Nemotron-Nano-12B-v2-Base, we employ the Minitron strategy to compress and distill the model with the goal of enabling inference on up to 128k tokens on a single NVIDIA A10G GPU (22GiB of memory, bfloat16 precision). Compared to existing similarly-sized models (e.g., Qwen3-8B), we show that Nemotron-Nano-9B-v2 achieves on-par or better accuracy on reasoning benchmarks while achieving up to 6x higher inference throughput in reasoning settings like 8k input and 16k output tokens. We are releasing Nemotron-Nano-9B-v2, Nemotron-Nano12B-v2-Base, and Nemotron-Nano-9B-v2-Base checkpoints along with the majority of our pre- and post-training datasets on Hugging Face. </p>
<blockquote>
<p>我们介绍了Nemotron-Nano-9B-v2，这是一款混合了Mamba-Transformer的语言模型，旨在提高推理工作负载的吞吐量，同时与类似规模的模型相比实现最先进的准确性。Nemotron-Nano-9B-v2建立在Nemotron-H架构的基础上，将Transformer架构中大部分的自注意力层替换为Mamba-2层，从而在生成用于推理的长思考轨迹时实现更快的推理速度。我们通过首先在20万亿个令牌上使用FP8训练配方预训练一个12亿参数模型（Nemotron-Nano-12B-v2-Base）来创建Nemotron-Nano-9B-v2。在对Nemotron-Nano-12B-v2-Base进行对齐后，我们采用Minitron策略来压缩和蒸馏模型，旨在能够在单个NVIDIA A10G GPU（具有22GB内存，bfloat16精度）上进行高达128k令牌的推理。与现有的类似规模模型（例如Qwen3-8B）相比，我们在推理基准测试上展示了Nemotron-Nano-9B-v2实现了相当的或更好的准确性，同时在如8k输入和16k输出令牌的推理设置中实现了高达6倍的推理吞吐量。我们将Nemotron-Nano-9B-v2、Nemotron-Nano12B-v2-Base和Nemotron-Nano-9B-v2-Base检查点以及我们的大部分预训练和后续训练数据集在Hugging Face上发布。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14444v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于Mamba-Transformer架构的Nemotron-Nano-9B-v2混合语言模型旨在提高推理工作负载的吞吐量，同时与同类模型相比实现最先进的准确性。该模型通过采用Nemotron-H架构，将Transformer架构中的大部分自注意力层替换为Mamba-2层，以实现在生成推理所需的长思考轨迹时的更快推理速度。通过预训练一个12亿参数的模型（Nemotron-Nano-12B-v2-Base），并在使用FP8训练配方处理过的20万亿个令牌上对其进行训练，创建了Nemotron-Nano-9B-v2。采用Minitron策略对模型进行压缩和蒸馏，可在单个NVIDIA A10G GPU上实现对高达128k令牌的推理。与现有类似规模的模型相比，Nemotron-Nano-9B-v2在推理基准测试上实现了相当或更好的准确性，同时在8k输入和16k输出令牌等推理环境中实现了高达6倍的推理吞吐量。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Nemotron-Nano-9B-v2是一个混合Mamba-Transformer语言模型，旨在提高推理工作负载的吞吐量和准确性。</li>
<li>该模型基于Nemotron-H架构，将自注意力层替换为Mamba-2层以实现更快的推理速度。</li>
<li>模型预训练在包含20万亿令牌的巨大数据集上进行，采用了FP8训练配方。</li>
<li>通过使用Minitron策略进行压缩和蒸馏，模型可以在单个NVIDIA A10G GPU上处理高达128k令牌的推理任务。</li>
<li>与类似规模的模型相比，Nemotron-Nano-9B-v2在推理基准测试中表现出色。</li>
<li>该模型实现了高达6倍的推理吞吐量，特别是在处理大规模输入和输出令牌时。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14444">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-55f97bf17c1a24b75a1fbc40b0cbf9ad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7fa504cec80d675359af61cce57535a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa73dae471819e017a5cc665c1f7aee0.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="ThinkTuning-Instilling-Cognitive-Reflections-without-Distillation"><a href="#ThinkTuning-Instilling-Cognitive-Reflections-without-Distillation" class="headerlink" title="ThinkTuning: Instilling Cognitive Reflections without Distillation"></a>ThinkTuning: Instilling Cognitive Reflections without Distillation</h2><p><strong>Authors:Aswin RRV, Jacob Dineen, Divij Handa, Md Nayem Uddin, Mihir Parmar, Chitta Baral, Ben Zhou</strong></p>
<p>Recent advances in test-time scaling have led to the emergence of thinking LLMs that exhibit self-reflective behaviors and multi-step reasoning. While RL drives this self-improvement paradigm, a recent study (Gandhi et al., 2025) shows that RL alone does not truly instill these new reasoning abilities - it merely draws out behaviors already present in the base models. This raises a question: How can we train the models that don’t exhibit such thinking behavior to develop it in the first place? To this end, we propose ThinkTuning, a GRPO-based interactive training approach where we augment the rollouts of a student model with the guidance from a teacher model. A simple idea from classroom practice inspires our method: a teacher poses a problem, lets the student try an answer, then gives corrective feedback – enough to point the mind in the right direction and then show the solution. Each piece of feedback reshapes the student’s thoughts, leading them to arrive at the correct solution. Similarly, we find that this type of implicit supervision through feedback from a teacher model of the same size improves the reasoning capabilities of the student model. In particular, on average, our method shows a 3.85% improvement over zero-shot baselines across benchmarks, and on MATH-500, AIME and GPQA-Diamond it shows 2.08%, 2.23% and 3.99% improvements over the vanilla-GRPO baseline. Source code is available at <a target="_blank" rel="noopener" href="https://github.com/3rdAT/ThinkTuning">https://github.com/3rdAT/ThinkTuning</a>. </p>
<blockquote>
<p>近期测试时间缩放技术的进展，催生出了一种具备自我反思行为和跨步骤推理能力的思考式大型语言模型（LLMs）。虽然强化学习（RL）推动了这种自我改进模式的发展，但最近的一项研究（Gandhi等人，2025）表明，单纯依靠RL并不能真正赋予这些新的推理能力——它只是激发了在基础模型中已经存在的行为。这就产生了一个问题：如何训练那些没有这种思维行为的模型，让它们首先具备这种能力呢？为此，我们提出了ThinkTuning，这是一种基于GRPO的交互式训练方法，我们通过在学员模型的rollouts中加入教师模型的指导来增强其功能。我们的方法灵感来自于课堂实践的简单想法：教师提出一个问题，让学生尝试回答，然后给出纠正反馈，足以指引正确的方向并展示解决方案。每一份反馈都会重塑学生的思路，引导他们找到正确的答案。同样，我们发现，通过教师模型的反馈进行这种隐式监督，能够改善学员模型的推理能力。尤其是我们的方法在平均基准测试上比零基准线高出3.85%的改进。在MATH-500、AIME和GPQA-Diamond上，相对于普通的GRPO基线，我们的方法分别实现了2.08%、2.23%和3.99%的改进。源代码可在<a target="_blank" rel="noopener" href="https://github.com/3rdAT/ThinkTuning">https://github.com/3rdAT/ThinkTuning</a>获取。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07616v2">PDF</a> EMNLP 2025 (Main Conference)</p>
<p><strong>Summary</strong></p>
<p>本文介绍了近期测试时间扩展技术的进步推动了具备自我反思行为和跨步推理的思考式LLM的出现。然而，研究发现仅通过强化学习（RL）并不能真正赋予模型这些新推理能力，只能调动基础模型中已有的能力。因此，提出ThinkTuning训练方法，这是一种基于GRPO的交互式训练方法，通过教师模型的指导来增强学生模型的rollouts。该方法受到课堂实践的启发，通过教师提出问题和提供反馈，引导学生思考并找到正确答案。同样地，本文通过教师模型给予同样规模的隐式监督反馈，提升了学生模型的推理能力。ThinkTuning方法相较于零基准线平均提升了3.85%，在MATH-500、AIME和GPQA-Diamond任务上分别提升了2.08%、2.23%和3.99%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>测试时间扩展技术的进展推动了思考式LLM的出现，这些模型展现出自我反思行为和跨步推理能力。</li>
<li>强化学习（RL）并不能真正赋予模型新的推理能力，而只能调动基础模型已有的能力。</li>
<li>提出ThinkTuning训练方法，这是一种基于GRPO的交互式训练方式，旨在通过教师模型的指导来提升学生模型的推理能力。</li>
<li>ThinkTuning方法受到课堂实践的启发，通过教师的反馈来引导学生思考并找到问题的答案。</li>
<li>教师模型给予隐式监督反馈，有助于提升学生模型的推理能力。</li>
<li>ThinkTuning方法相较于零基准线平均提升了3.85%的推理能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07616">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-4a27e03da626dce684e6ddb82f248df1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ccd28137d612c00d12112dd39483cb29.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-23/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-23/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-23/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-f1ff94757caca643ac10254ed3e86f94.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM 方向最新论文已更新，请持续关注 Update in 2025-08-23  End-to-End Agentic RAG System Training for Traceable Diagnostic   Reasoning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-23
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-23/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-4a1220d90ca9cc844fe9d58293966c16.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2025-08-23  DIFFA Large Language Diffusion Models Can Listen and Understand
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-23
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">26551.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
