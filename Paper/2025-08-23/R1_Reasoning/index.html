<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-23  Intern-S1 A Scientific Multimodal Foundation Model">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-149c94281c63fab0be861c1249be1603.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-23
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-25
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    22.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    93 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-23-æ›´æ–°"><a href="#2025-08-23-æ›´æ–°" class="headerlink" title="2025-08-23 æ›´æ–°"></a>2025-08-23 æ›´æ–°</h1><h2 id="Intern-S1-A-Scientific-Multimodal-Foundation-Model"><a href="#Intern-S1-A-Scientific-Multimodal-Foundation-Model" class="headerlink" title="Intern-S1: A Scientific Multimodal Foundation Model"></a>Intern-S1: A Scientific Multimodal Foundation Model</h2><p><strong>Authors:Lei Bai, Zhongrui Cai, Maosong Cao, Weihan Cao, Chiyu Chen, Haojiong Chen, Kai Chen, Pengcheng Chen, Ying Chen, Yongkang Chen, Yu Cheng, Yu Cheng, Pei Chu, Tao Chu, Erfei Cui, Ganqu Cui, Long Cui, Ziyun Cui, Nianchen Deng, Ning Ding, Nanqin Dong, Peijie Dong, Shihan Dou, Sinan Du, Haodong Duan, Caihua Fan, Ben Gao, Changjiang Gao, Jianfei Gao, Songyang Gao, Yang Gao, Zhangwei Gao, Jiaye Ge, Qiming Ge, Lixin Gu, Yuzhe Gu, Aijia Guo, Qipeng Guo, Xu Guo, Conghui He, Junjun He, Yili Hong, Siyuan Hou, Caiyu Hu, Hanglei Hu, Jucheng Hu, Ming Hu, Zhouqi Hua, Haian Huang, Junhao Huang, Xu Huang, Zixian Huang, Zhe Jiang, Lingkai Kong, Linyang Li, Peiji Li, Pengze Li, Shuaibin Li, Tianbin Li, Wei Li, Yuqiang Li, Dahua Lin, Junyao Lin, Tianyi Lin, Zhishan Lin, Hongwei Liu, Jiangning Liu, Jiyao Liu, Junnan Liu, Kai Liu, Kaiwen Liu, Kuikun Liu, Shichun Liu, Shudong Liu, Wei Liu, Xinyao Liu, Yuhong Liu, Zhan Liu, Yinquan Lu, Haijun Lv, Hongxia Lv, Huijie Lv, Qidang Lv, Ying Lv, Chengqi Lyu, Chenglong Ma, Jianpeng Ma, Ren Ma, Runmin Ma, Runyuan Ma, Xinzhu Ma, Yichuan Ma, Zihan Ma, Sixuan Mi, Junzhi Ning, Wenchang Ning, Xinle Pang, Jiahui Peng, Runyu Peng, Yu Qiao, Jiantao Qiu, Xiaoye Qu, Yuan Qu, Yuchen Ren, Fukai Shang, Wenqi Shao, Junhao Shen, Shuaike Shen, Chunfeng Song, Demin Song, Diping Song, Chenlin Su, Weijie Su, Weigao Sun, Yu Sun, Qian Tan, Cheng Tang, Huanze Tang, Kexian Tang, Shixiang Tang, Jian Tong, Aoran Wang, Bin Wang, Dong Wang, Lintao Wang, Rui Wang, Weiyun Wang, Wenhai Wang, Yi Wang, Ziyi Wang, Ling-I Wu, Wen Wu, Yue Wu, Zijian Wu, Linchen Xiao, Shuhao Xing, Chao Xu, Huihui Xu, Jun Xu, Ruiliang Xu, Wanghan Xu, GanLin Yang, Yuming Yang, Haochen Ye, Jin Ye, Shenglong Ye, Jia Yu, Jiashuo Yu, Jing Yu, Fei Yuan, Bo Zhang, Chao Zhang, Chen Zhang, Hongjie Zhang, Jin Zhang, Qiaosheng Zhang, Qiuyinzhe Zhang, Songyang Zhang, Taolin Zhang, Wenlong Zhang, Wenwei Zhang, Yechen Zhang, Ziyang Zhang, Haiteng Zhao, Qian Zhao, Xiangyu Zhao, Xiangyu Zhao, Bowen Zhou, Dongzhan Zhou, Peiheng Zhou, Yuhao Zhou, Yunhua Zhou, Dongsheng Zhu, Lin Zhu, Yicheng Zou</strong></p>
<p>In recent years, a plethora of open-source foundation models have emerged, achieving remarkable progress in some widely attended fields, with performance being quite close to that of closed-source models. However, in high-value but more challenging scientific professional fields, either the fields still rely on expert models, or the progress of general foundation models lags significantly compared to those in popular areas, far from sufficient for transforming scientific research and leaving substantial gap between open-source models and closed-source models in these scientific domains. To mitigate this gap and explore a step further toward Artificial General Intelligence (AGI), we introduce Intern-S1, a specialized generalist equipped with general understanding and reasoning capabilities with expertise to analyze multiple science modal data. Intern-S1 is a multimodal Mixture-of-Experts (MoE) model with 28 billion activated parameters and 241 billion total parameters, continually pre-trained on 5T tokens, including over 2.5T tokens from scientific domains. In the post-training stage, Intern-S1 undergoes offline and then online reinforcement learning (RL) in InternBootCamp, where we propose Mixture-of-Rewards (MoR) to synergize the RL training on more than 1000 tasks simultaneously. Through integrated innovations in algorithms, data, and training systems, Intern-S1 achieved top-tier performance in online RL training.On comprehensive evaluation benchmarks, Intern-S1 demonstrates competitive performance on general reasoning tasks among open-source models and significantly outperforms open-source models in scientific domains, surpassing closed-source state-of-the-art models in professional tasks, such as molecular synthesis planning, reaction condition prediction, predicting thermodynamic stabilities for crystals. Our models are available at <a target="_blank" rel="noopener" href="https://huggingface.co/internlm/Intern-S1">https://huggingface.co/internlm/Intern-S1</a>. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œå¤§é‡çš„å¼€æºåŸºç¡€æ¨¡å‹ä¸æ–­æ¶Œç°ï¼Œåœ¨ä¸€äº›å¹¿æ³›å…³æ³¨çš„é¢†åŸŸå–å¾—äº†æ˜¾è‘—çš„è¿›å±•ï¼Œå…¶æ€§èƒ½ä¸é—­æºæ¨¡å‹ç›¸å½“æ¥è¿‘ã€‚ç„¶è€Œï¼Œåœ¨é«˜ä»·å€¼ä½†æ›´å…·æŒ‘æˆ˜æ€§çš„ç§‘å­¦ä¸“ä¸šé¢†åŸŸï¼Œè¿™äº›é¢†åŸŸä»ç„¶ä¾èµ–äºä¸“å®¶æ¨¡å‹ï¼Œæˆ–è€…é€šç”¨åŸºç¡€æ¨¡å‹çš„è¿›å±•ä¸æµè¡Œé¢†åŸŸç›¸æ¯”æ»åæ˜¾è‘—ï¼Œè¿œè¿œä¸è¶³ä»¥æ”¹å˜ç§‘å­¦ç ”ç©¶ï¼Œä»è€Œåœ¨è¿™äº›ç§‘å­¦é¢†åŸŸçš„å¼€æºæ¨¡å‹å’Œé—­æºæ¨¡å‹ä¹‹é—´äº§ç”Ÿäº†å·¨å¤§çš„å·®è·ã€‚ä¸ºäº†ç¼©å°è¿™ä¸€å·®è·ï¼Œå¹¶è¿›ä¸€æ­¥æœç€é€šç”¨äººå·¥æ™ºèƒ½ï¼ˆAGIï¼‰çš„æ–¹å‘å‘å±•ï¼Œæˆ‘ä»¬å¼•å…¥äº†Intern-S1ï¼Œè¿™æ˜¯ä¸€ä¸ªå…·å¤‡é€šç”¨ç†è§£å’Œæ¨ç†èƒ½åŠ›ï¼ŒåŒæ—¶æ‹¥æœ‰åˆ†æå¤šç§ç§‘å­¦æ¨¡æ€æ•°æ®çš„ä¸“ä¸šé€šæ‰ã€‚Intern-S1æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€çš„ä¸“å®¶æ··åˆï¼ˆMoEï¼‰æ¨¡å‹ï¼Œæ‹¥æœ‰28äº¿ä¸ªæ¿€æ´»å‚æ•°å’Œæ€»å…±241äº¿ä¸ªå‚æ•°ï¼Œåœ¨5Tæ ‡è®°ä¸ŠæŒç»­è¿›è¡Œé¢„è®­ç»ƒï¼Œå…¶ä¸­åŒ…æ‹¬æ¥è‡ªç§‘å­¦é¢†åŸŸçš„è¶…è¿‡2.5Tæ ‡è®°ã€‚åœ¨è®­ç»ƒåé˜¶æ®µï¼ŒIntern-S1åœ¨InternBootCampä¸­ç»å†äº†ç¦»çº¿ç„¶åæ˜¯åœ¨çº¿çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒã€‚æˆ‘ä»¬æå‡ºäº†å¥–åŠ±æ··åˆï¼ˆMoRï¼‰æ–¹æ³•æ¥ååŒåœ¨è¶…è¿‡1000ä¸ªä»»åŠ¡ä¸ŠåŒæ—¶è¿›è¡ŒRLè®­ç»ƒã€‚é€šè¿‡ç®—æ³•ã€æ•°æ®å’Œè®­ç»ƒç³»ç»Ÿçš„ç»¼åˆåˆ›æ–°ï¼ŒIntern-S1åœ¨åœ¨çº¿RLè®­ç»ƒä¸­å–å¾—äº†é¡¶å°–æ€§èƒ½ã€‚åœ¨å…¨é¢çš„è¯„ä¼°åŸºå‡†æµ‹è¯•ä¸­ï¼ŒIntern-S1åœ¨é€šç”¨æ¨ç†ä»»åŠ¡ä¸­å±•ç¤ºäº†ä¸å¼€æºæ¨¡å‹çš„ç«äº‰åŠ›ï¼Œå¹¶åœ¨ç§‘å­¦é¢†åŸŸæ˜¾è‘—ä¼˜äºå¼€æºæ¨¡å‹ï¼Œåœ¨ä¸“ä¸šä»»åŠ¡ä¸­è¶…è¶Šäº†é—­æºçš„æœ€å…ˆè¿›æ¨¡å‹ï¼Œå¦‚åˆ†å­åˆæˆè§„åˆ’ã€ååº”æ¡ä»¶é¢„æµ‹ã€æ™¶ä½“çƒ­åŠ›å­¦ç¨³å®šæ€§é¢„æµ‹ç­‰ã€‚æˆ‘ä»¬çš„æ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/internlm/intern-s1%E8%8E%B7%E5%8F%96%E3%80%82">https://huggingface.co/internlm/Intern-S1è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15763v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†æ–°å…´çš„å¼€æºåŸºç¡€æ¨¡å‹ Intern-S1ï¼Œå®ƒåœ¨ä¸€äº›å¹¿å—å…³æ³¨çš„é¢†åŸŸå–å¾—äº†å¼•äººæ³¨ç›®çš„è¿›å±•ï¼Œæ€§èƒ½æ¥è¿‘é—­æºæ¨¡å‹ã€‚ç„¶è€Œï¼Œåœ¨é«˜ä»·å€¼çš„ç§‘å­¦ä¸“ä¸šé¢†åŸŸä¸­ï¼Œå¼€æºåŸºç¡€æ¨¡å‹çš„è¿›å±•ä»ç„¶æ˜¾è‘—æ»åã€‚ä¸ºäº†ç¼©å°å·®è·å¹¶æœç€é€šç”¨äººå·¥æ™ºèƒ½ï¼ˆAGIï¼‰è¿ˆå‡ºä¸€æ­¥ï¼Œæ¨å‡ºäº† Intern-S1 æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å…·å¤‡é€šç”¨ç†è§£å’Œæ¨ç†èƒ½åŠ›ï¼Œå¹¶å…·å¤‡åˆ†æå¤šç§ç§‘å­¦æ¨¡æ€æ•°æ®çš„ä¸“ä¸šçŸ¥è¯†ã€‚é€šè¿‡ç®—æ³•ã€æ•°æ®å’Œè®­ç»ƒç³»ç»Ÿçš„ç»¼åˆåˆ›æ–°ï¼ŒIntern-S1 åœ¨åœ¨çº¿å¼ºåŒ–å­¦ä¹ è®­ç»ƒä¸­å–å¾—äº†é¡¶å°–æ€§èƒ½ï¼Œå¹¶åœ¨ç»¼åˆè¯„ä¼°åŸºå‡†æµ‹è¯•ä¸­å±•ç°äº†ç«äº‰åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼€æºåŸºç¡€æ¨¡å‹åœ¨è®¸å¤šé¢†åŸŸæ€§èƒ½æ¥è¿‘é—­æºæ¨¡å‹ï¼Œä½†åœ¨é«˜ä»·å€¼çš„ç§‘å­¦ä¸“ä¸šé¢†åŸŸå­˜åœ¨æ˜¾è‘—å·®è·ã€‚</li>
<li>Intern-S1 æ˜¯ä¸€æ¬¾å¤šæ¨¡æ€çš„æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¨¡å‹ï¼Œå…·å¤‡é€šç”¨ç†è§£å’Œæ¨ç†èƒ½åŠ›ï¼Œå¹¶å…·å¤‡åˆ†æå¤šç§ç§‘å­¦æ¨¡æ€æ•°æ®çš„ä¸“ä¸šçŸ¥è¯†ã€‚</li>
<li>Intern-S1 é€šè¿‡æŒç»­é¢„è®­ç»ƒåœ¨ 5T æ ‡è®°ç¬¦å·ä¸Šï¼ŒåŒ…æ‹¬è¶…è¿‡ 2.5T æ¥è‡ªç§‘å­¦é¢†åŸŸçš„æ ‡è®°ç¬¦å·ã€‚</li>
<li>Intern-S1 åœ¨åœ¨çº¿å¼ºåŒ–å­¦ä¹ è®­ç»ƒé˜¶æ®µé‡‡ç”¨äº†æ··åˆç‰©å¥–åŠ±ï¼ˆMoRï¼‰æ–¹æ³•ï¼ŒåŒæ­¥è¿›è¡Œè¶…è¿‡ 1000 é¡¹ä»»åŠ¡çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒã€‚</li>
<li>é€šè¿‡ç®—æ³•ã€æ•°æ®å’Œè®­ç»ƒç³»ç»Ÿçš„ç»¼åˆåˆ›æ–°ï¼ŒIntern-S1 åœ¨åœ¨çº¿å¼ºåŒ–å­¦ä¹ è®­ç»ƒä¸­å®ç°äº†é¡¶å°–æ€§èƒ½ã€‚</li>
<li>Intern-S1 åœ¨é€šç”¨æ¨ç†ä»»åŠ¡ä¸Šå±•ç°äº†ä¸å¼€æºæ¨¡å‹çš„ç«äº‰åŠ›ï¼Œå¹¶åœ¨ç§‘å­¦é¢†åŸŸæ˜¾è‘—ä¼˜äºå…¶ä»–å¼€æºæ¨¡å‹ã€‚</li>
<li>Intern-S1 åœ¨ä¸“ä¸šä»»åŠ¡ä¸Šè¶…è¶Šäº†é—­æºçš„æœ€å…ˆè¿›æ¨¡å‹ï¼Œå¦‚åˆ†å­åˆæˆè§„åˆ’ã€ååº”æ¡ä»¶é¢„æµ‹å’Œæ™¶ä½“çƒ­åŠ›å­¦ç¨³å®šæ€§é¢„æµ‹ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15763">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6f8341d09ccdc18c4e86303af60f0537.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6237b1566b477a31cf689954aaa1a61b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b20ed790b9759ad242c7c81834ae428b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-78591ae6302e57a931d5853acb766c0a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee6ed39d102291a6c3c34e80ea4ed775.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Response-and-Prompt-Evaluation-to-Prevent-Parasocial-Relationships-with-Chatbots"><a href="#Response-and-Prompt-Evaluation-to-Prevent-Parasocial-Relationships-with-Chatbots" class="headerlink" title="Response and Prompt Evaluation to Prevent Parasocial Relationships with   Chatbots"></a>Response and Prompt Evaluation to Prevent Parasocial Relationships with   Chatbots</h2><p><strong>Authors:Emma Rath, Stuart Armstrong, Rebecca Gorman</strong></p>
<p>The development of parasocial relationships with AI agents has severe, and in some cases, tragic effects for human well-being. Yet preventing such dynamics is challenging: parasocial cues often emerge gradually in private conversations, and not all forms of emotional engagement are inherently harmful. We address this challenge by introducing a simple response evaluation framework, created by repurposing a state-of-the-art language model, that evaluates ongoing conversations for parasocial cues in real time. To test the feasibility of this approach, we constructed a small synthetic dataset of thirty dialogues spanning parasocial, sycophantic, and neutral conversations. Iterative evaluation with five stage testing successfully identified all parasocial conversations while avoiding false positives under a tolerant unanimity rule, with detection typically occurring within the first few exchanges. These findings provide preliminary evidence that evaluation agents can provide a viable solution for the prevention of parasocial relations. </p>
<blockquote>
<p>ä¸äººå·¥æ™ºèƒ½ä»£ç†å»ºç«‹å‡†ç¤¾ä¼šå…³ç³»çš„å‘å±•ç»™äººç±»ç¦ç¥‰å¸¦æ¥äº†ä¸¥é‡ï¼Œç”šè‡³åœ¨æŸäº›æƒ…å†µä¸‹å¸¦æ¥äº†æ‚²å‰§æ€§çš„å½±å“ã€‚ç„¶è€Œï¼Œé˜»æ­¢è¿™ç§åŠ¨æ€å‘å±•å…·æœ‰æŒ‘æˆ˜æ€§ï¼šå‡†ç¤¾ä¼šçº¿ç´¢é€šå¸¸ä¼šåœ¨ç§äººå¯¹è¯ä¸­é€æ¸æ˜¾ç°ï¼Œå¹¶éæ‰€æœ‰å½¢å¼çš„æƒ…æ„Ÿå‚ä¸éƒ½æ˜¯æœ‰å®³çš„ã€‚æˆ‘ä»¬é€šè¿‡å¼•å…¥ä¸€ä¸ªç®€å•çš„ååº”è¯„ä¼°æ¡†æ¶æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œè¯¥æ¡†æ¶ç”±æœ€å…ˆè¯­è¨€æ¨¡å‹æ”¹é€ è€Œæˆï¼Œç”¨äºå®æ—¶è¯„ä¼°å¯¹è¯ä¸­æ˜¯å¦å­˜åœ¨å‡†ç¤¾ä¼šçº¿ç´¢ã€‚ä¸ºäº†éªŒè¯è¯¥æ–¹æ³•çš„å¯è¡Œæ€§ï¼Œæˆ‘ä»¬æ„å»ºäº†åŒ…å«ä¸‰åä¸ªå¯¹è¯çš„å°å‹åˆæˆæ•°æ®é›†ï¼Œæ¶µç›–äº†å‡†ç¤¾ä¼šã€è°„åªšå’Œä¸­æ€§å¯¹è¯ã€‚é€šè¿‡äº”ä¸ªé˜¶æ®µçš„æµ‹è¯•è¿›è¡Œè¿­ä»£è¯„ä¼°ï¼ŒæˆåŠŸåœ°è¯†åˆ«å‡ºäº†æ‰€æœ‰å‡†ç¤¾ä¼šå¯¹è¯ï¼Œåœ¨å®½å®¹ä¸€è‡´è§„åˆ™ä¸‹é¿å…äº†è¯¯æŠ¥ï¼Œæ£€æµ‹é€šå¸¸å‘ç”Ÿåœ¨æœ€åˆçš„å‡ æ¬¡äº¤æµä¸­ã€‚è¿™äº›å‘ç°æä¾›äº†åˆæ­¥è¯æ®ï¼Œè¡¨æ˜è¯„ä¼°ä»£ç†å¯ä»¥æä¾›ä¸€ç§é˜²æ­¢å‡†ç¤¾ä¼šå…³ç³»çš„å¯è¡Œè§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15748v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†ä¸äººå·¥æ™ºèƒ½ä»£ç†å‘å±•å¯„ç”Ÿç¤¾ä¼šå…³ç³»æ‰€å¸¦æ¥çš„ä¸¥é‡ç”šè‡³æ‚²å‰§æ€§å½±å“ã€‚é’ˆå¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæå‡ºä¸€ä¸ªç®€å•çš„å“åº”è¯„ä¼°æ¡†æ¶ï¼Œé€šè¿‡æ”¹é€ å½“å‰æœ€å…ˆè¿›çš„è¯­è¨€æ¨¡å‹ï¼Œä»¥å®æ—¶è¯„ä¼°å¯¹è¯ä¸­çš„å¯„ç”Ÿç¤¾ä¼šçº¿ç´¢ã€‚é€šè¿‡æ„å»ºåŒ…å«å¯„ç”Ÿç¤¾ä¼šã€è°„åªšå’Œä¸­æ€§å¯¹è¯çš„ä¸‰åä¸ªå¯¹è¯çš„å°å‹åˆæˆæ•°æ®é›†è¿›è¡Œæµ‹è¯•ï¼ŒæˆåŠŸè¯†åˆ«å‡ºæ‰€æœ‰å¯„ç”Ÿç¤¾ä¼šå¯¹è¯ï¼Œå¹¶åœ¨å®¹å¿ä¸€è‡´æ€§çš„è§„åˆ™ä¸‹é¿å…è¯¯æŠ¥ã€‚åˆæ­¥è¯æ®è¡¨æ˜è¯„ä¼°ä»£ç†å¯ä¸ºé¢„é˜²å¯„ç”Ÿç¤¾ä¼šå…³ç³»æä¾›å¯è¡Œè§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¸äººå·¥æ™ºèƒ½ä»£ç†å‘å±•å¯„ç”Ÿç¤¾ä¼šå…³ç³»å¯èƒ½å¯¹äººç±»ç¦ç¥‰äº§ç”Ÿä¸¥é‡å½±å“ã€‚</li>
<li>å¯„ç”Ÿç¤¾ä¼šçº¿ç´¢åœ¨ç§äººå¯¹è¯ä¸­é€æ¸æ˜¾ç°ï¼Œå¹¶éæ‰€æœ‰æƒ…æ„Ÿå‚ä¸éƒ½æ˜¯æœ‰å®³çš„ã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªç®€å•çš„å“åº”è¯„ä¼°æ¡†æ¶ï¼Œåˆ©ç”¨å…ˆè¿›è¯­è¨€æ¨¡å‹å®æ—¶è¯„ä¼°å¯¹è¯ä¸­çš„å¯„ç”Ÿç¤¾ä¼šçº¿ç´¢ã€‚</li>
<li>é€šè¿‡æ„å»ºåŒ…å«ä¸åŒç±»å‹å¯¹è¯çš„åˆæˆæ•°æ®é›†è¿›è¡Œæµ‹è¯•ï¼ŒéªŒè¯äº†è¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>åœ¨å®¹å¿ä¸€è‡´æ€§çš„è§„åˆ™ä¸‹ï¼Œè¯¥æ¡†æ¶èƒ½æˆåŠŸè¯†åˆ«å¯„ç”Ÿç¤¾ä¼šå¯¹è¯ï¼Œä¸”é€šå¸¸èƒ½åœ¨å‰å‡ è½®äº¤æµä¸­å³å®Œæˆæ£€æµ‹ã€‚</li>
<li>è¯„ä¼°ä»£ç†åœ¨é¢„é˜²å¯„ç”Ÿç¤¾ä¼šå…³ç³»æ–¹é¢å…·æœ‰æ½œåœ¨åº”ç”¨ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15748">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-43c92f2d4d7ab75c70594a76a2d13deb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d04191d34c14b82e8046c84e639f554c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a0a226e622a266d645d2a988c71232f0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="End-to-End-Agentic-RAG-System-Training-for-Traceable-Diagnostic-Reasoning"><a href="#End-to-End-Agentic-RAG-System-Training-for-Traceable-Diagnostic-Reasoning" class="headerlink" title="End-to-End Agentic RAG System Training for Traceable Diagnostic   Reasoning"></a>End-to-End Agentic RAG System Training for Traceable Diagnostic   Reasoning</h2><p><strong>Authors:Qiaoyu Zheng, Yuze Sun, Chaoyi Wu, Weike Zhao, Pengcheng Qiu, Yongguo Yu, Kun Sun, Yanfeng Wang, Ya Zhang, Weidi Xie</strong></p>
<p>Accurate diagnosis with medical large language models is hindered by knowledge gaps and hallucinations. Retrieval and tool-augmented methods help, but their impact is limited by weak use of external knowledge and poor feedback-reasoning traceability. To address these challenges, We introduce Deep-DxSearch, an agentic RAG system trained end-to-end with reinforcement learning (RL) that enables steer tracebale retrieval-augmented reasoning for medical diagnosis. In Deep-DxSearch, we first construct a large-scale medical retrieval corpus comprising patient records and reliable medical knowledge sources to support retrieval-aware reasoning across diagnostic scenarios. More crutially, we frame the LLM as the core agent and the retrieval corpus as its environment, using tailored rewards on format, retrieval, reasoning structure, and diagnostic accuracy, thereby evolving the agentic RAG policy from large-scale data through RL.   Experiments demonstrate that our end-to-end agentic RL training framework consistently outperforms prompt-engineering and training-free RAG approaches across multiple data centers. After training, Deep-DxSearch achieves substantial gains in diagnostic accuracy, surpassing strong diagnostic baselines such as GPT-4o, DeepSeek-R1, and other medical-specific frameworks for both common and rare disease diagnosis under in-distribution and out-of-distribution settings. Moreover, ablation studies on reward design and retrieval corpus components confirm their critical roles, underscoring the uniqueness and effectiveness of our approach compared with traditional implementations. Finally, case studies and interpretability analyses highlight improvements in Deep-DxSearchâ€™s diagnostic policy, providing deeper insight into its performance gains and supporting clinicians in delivering more reliable and precise preliminary diagnoses. See <a target="_blank" rel="noopener" href="https://github.com/MAGIC-AI4Med/Deep-DxSearch">https://github.com/MAGIC-AI4Med/Deep-DxSearch</a>. </p>
<blockquote>
<p>ä½¿ç”¨åŒ»ç–—å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå‡†ç¡®è¯Šæ–­å—åˆ°çŸ¥è¯†å·®è·å’Œå¹»è§‰çš„é˜»ç¢ã€‚æ£€ç´¢å’Œå·¥å…·å¢å¼ºæ–¹æ³•æœ‰æ‰€å¸®åŠ©ï¼Œä½†å®ƒä»¬çš„å½±å“å—é™äºå¤–éƒ¨çŸ¥è¯†åˆ©ç”¨ä¸è¶³å’Œåé¦ˆæ¨ç†è¿½è¸ªèƒ½åŠ›å¼±ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†Deep-DxSearchï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒçš„ä¸»åŠ¨æ¨ç†å›¾ç³»ç»Ÿï¼Œå¯å®ç°ç”¨äºåŒ»ç–—è¯Šæ–­çš„è¿½è¸ªæ£€ç´¢å¢å¼ºæ¨ç†ã€‚åœ¨Deep-DxSearchä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆæ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡åŒ»ç–—æ£€ç´¢è¯­æ–™åº“ï¼ŒåŒ…å«æ‚£è€…è®°å½•å’Œå¯é çš„åŒ»ç–—çŸ¥è¯†æ¥æºï¼Œä»¥æ”¯æŒè·¨è¯Šæ–­åœºæ™¯çš„æ£€ç´¢æ„ŸçŸ¥æ¨ç†ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬å°†å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºæ ¸å¿ƒä¸»ä½“ï¼Œå°†æ£€ç´¢è¯­æ–™åº“ä½œä¸ºå…¶ç¯å¢ƒï¼Œé’ˆå¯¹æ ¼å¼ã€æ£€ç´¢ã€æ¨ç†ç»“æ„å’Œè¯Šæ–­å‡†ç¡®æ€§åˆ¶å®šä¸“é—¨å¥–åŠ±ï¼Œä»è€Œé€šè¿‡å¼ºåŒ–å­¦ä¹ æ¨åŠ¨ä¸»åŠ¨æ¨ç†å›¾ç­–ç•¥ä»å¤§è§„æ¨¡æ•°æ®ä¸­è¿›åŒ–ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç«¯åˆ°ç«¯ä¸»åŠ¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ¡†æ¶åœ¨å¤šä¸ªæ•°æ®ä¸­å¿ƒå§‹ç»ˆä¼˜äºåŸºäºæç¤ºå’Œè®­ç»ƒå…è´¹çš„ä¸»åŠ¨æ¨ç†å›¾æ–¹æ³•ã€‚ç»è¿‡è®­ç»ƒï¼ŒDeep-DxSearchåœ¨è¯Šæ–­å‡†ç¡®æ€§æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›æ­¥ï¼Œè¶…è¶Šäº†å¼ºè¯Šæ–­åŸºçº¿ï¼Œå¦‚GPT-4oã€DeepSeek-R1å’Œå…¶ä»–é’ˆå¯¹å¸¸è§å’Œç½•è§ç–¾ç—…è¯Šæ–­çš„åŒ»ç–—ä¸“ç”¨æ¡†æ¶ï¼Œæ— è®ºåœ¨å†…éƒ¨åˆ†å¸ƒè¿˜æ˜¯å¤–éƒ¨åˆ†å¸ƒç¯å¢ƒä¸­å‡è¡¨ç°ä¼˜å¼‚ã€‚æ­¤å¤–ï¼Œå…³äºå¥–åŠ±è®¾è®¡å’Œæ£€ç´¢è¯­æ–™åº“ç»„ä»¶çš„æ¶ˆèç ”ç©¶è¯å®äº†å®ƒä»¬çš„å…³é”®ä½œç”¨ï¼Œå¼ºè°ƒäº†æˆ‘ä»¬çš„æ–¹æ³•ä¸ä¼ ç»Ÿå®æ–½æ–¹å¼çš„ç‹¬ç‰¹æ€§å’Œæœ‰æ•ˆæ€§ã€‚æœ€åï¼Œæ¡ˆä¾‹ç ”ç©¶å’Œè§£é‡Šæ€§åˆ†æçªæ˜¾äº†Deep-DxSearchè¯Šæ–­ç­–ç•¥çš„æ”¹è¿›ï¼Œä¸ºå…¶æ€§èƒ½æå‡æä¾›äº†æ›´æ·±å…¥çš„äº†è§£ï¼Œå¹¶æ”¯æŒä¸´åºŠåŒ»ç”Ÿæä¾›æ›´å¯é å’Œç²¾ç¡®çš„æ—©æœŸè¯Šæ–­ã€‚æ›´å¤šä¿¡æ¯è¯·å‚è§<a target="_blank" rel="noopener" href="https://github.com/MAGIC-AI4Med/Deep-DxSearch%E3%80%82">https://github.com/MAGIC-AI4Med/Deep-DxSearchã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15746v1">PDF</a> 35 pages, 5 figures, 3 tables</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§è§„æ¨¡åŒ»ç–—è¯­è¨€æ¨¡å‹çš„è¯Šæ–­å­˜åœ¨çŸ¥è¯†ç¼ºå£å’Œå¹»è§‰çš„é—®é¢˜ã€‚æ£€ç´¢å’Œå·¥å…·å¢å¼ºæ–¹æ³•æœ‰æ‰€å¸®åŠ©ï¼Œä½†å…¶å½±å“å—é™äºå¤–éƒ¨çŸ¥è¯†åˆ©ç”¨ä¸è¶³å’Œåé¦ˆæ¨ç†è¿½è¸ªèƒ½åŠ›å¼±ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºDeep-DxSearchï¼Œè¿™æ˜¯ä¸€ä¸ªé‡‡ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒçš„èƒ½åŠ¨å‹RAGç³»ç»Ÿï¼Œå¯å®ç°è¿½è¸ªå¯æ£€ç´¢çš„æ¨ç†è¾…åŠ©åŒ»ç–—è¯Šæ–­ã€‚Deep-DxSearché¦–å…ˆæ„å»ºå¤§è§„æ¨¡åŒ»ç–—æ£€ç´¢è¯­æ–™åº“ï¼ŒåŒ…å«æ‚£è€…è®°å½•å’Œå¯é åŒ»ç–—çŸ¥è¯†æºï¼Œä»¥æ”¯æŒè·¨è¯Šæ–­åœºæ™¯çš„æ£€ç´¢æ„ŸçŸ¥æ¨ç†ã€‚æ›´ä¸ºå…³é”®çš„æ˜¯ï¼Œæˆ‘ä»¬å°†å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºæ ¸å¿ƒä¸»ä½“ï¼Œå°†æ£€ç´¢è¯­æ–™åº“ä½œä¸ºç¯å¢ƒï¼Œé€šè¿‡æ ¼å¼ã€æ£€ç´¢ã€æ¨ç†ç»“æ„å’Œè¯Šæ–­å‡†ç¡®æ€§çš„å®šåˆ¶å¥–åŠ±ï¼Œä½¿èƒ½åŠ¨å‹RAGç­–ç•¥ä»å¤§è§„æ¨¡æ•°æ®ä¸­é€šè¿‡RLè¿›è¡Œè¿›åŒ–ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç«¯åˆ°ç«¯èƒ½åŠ¨å‹RLè®­ç»ƒæ¡†æ¶åœ¨å¤šä¸ªæ•°æ®ä¸­å¿ƒå‡ä¼˜äºæç¤ºå·¥ç¨‹å’Œå…è®­ç»ƒRAGæ–¹æ³•ã€‚ç»è¿‡è®­ç»ƒï¼ŒDeep-DxSearchåœ¨è¯Šæ–­å‡†ç¡®æ€§æ–¹é¢å–å¾—äº†å®è´¨æ€§è¿›å±•ï¼Œè¶…è¶Šäº†åŒ…æ‹¬GPT-4oã€DeepSeek-R1åœ¨å†…çš„å¼ºåŠ²è¯Šæ–­åŸºçº¿ä»¥åŠå…¶ä»–åŒ»ç–—ç‰¹å®šæ¡†æ¶ï¼Œåœ¨å¸¸è§å’Œç½•è§ç–¾ç—…çš„è¯Šæ–­ä¸­å‡è¡¨ç°å‡ºè‰²ï¼Œæ— è®ºå¤„äºåˆ†å¸ƒå†…è¿˜æ˜¯åˆ†å¸ƒå¤–ç¯å¢ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŒ»ç–—è¯Šæ–­ä¸­çš„åº”ç”¨å—åˆ°çŸ¥è¯†ç¼ºå£å’Œå¹»è§‰çš„æŒ‘æˆ˜ã€‚</li>
<li>æ£€ç´¢å’Œå·¥å…·å¢å¼ºæ–¹æ³•è™½æœ‰å¸®åŠ©ï¼Œä½†å—é™äºå¤–éƒ¨çŸ¥è¯†åˆ©ç”¨ä¸è¶³å’Œæ¨ç†åé¦ˆè¿½è¸ªèƒ½åŠ›å¼±ã€‚</li>
<li>Deep-DxSearchæ˜¯ä¸€ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„èƒ½åŠ¨å‹RAGç³»ç»Ÿï¼Œæ”¯æŒæ£€ç´¢å¢å¼ºçš„æ¨ç†è¿‡ç¨‹ã€‚</li>
<li>Deep-DxSearchæ„å»ºå¤§è§„æ¨¡åŒ»ç–—æ£€ç´¢è¯­æ–™åº“ä»¥æ”¯æŒè·¨è¯Šæ–­åœºæ™¯çš„æ£€ç´¢æ„ŸçŸ¥æ¨ç†ã€‚</li>
<li>Deep-DxSearchå°†å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºæ ¸å¿ƒä¸»ä½“ï¼Œç»“åˆå®šåˆ¶å¥–åŠ±è¿›è¡Œè®­ç»ƒï¼Œä»¥æé«˜è¯Šæ–­å‡†ç¡®æ€§ã€‚</li>
<li>å®éªŒè¡¨æ˜Deep-DxSearchåœ¨å¤šä¸ªæ•°æ®ä¸­å¿ƒå‡ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œå¹¶èƒ½åœ¨å¸¸è§å’Œç½•è§ç–¾ç—…çš„è¯Šæ–­ä¸­è¡¨ç°å‡ºè‰²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15746">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d1bb8f786cad4e2d5361a4b48148892a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e16115757b3c0fd580e293a4e959ec92.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a888b967c7d1c1b107fd8d54ea7bf485.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Benchmarking-Computer-Science-Survey-Generation"><a href="#Benchmarking-Computer-Science-Survey-Generation" class="headerlink" title="Benchmarking Computer Science Survey Generation"></a>Benchmarking Computer Science Survey Generation</h2><p><strong>Authors:Weihang Su, Anzhe Xie, Qingyao Ai, Jianming Long, Jiaxin Mao, Ziyi Ye, Yiqun Liu</strong></p>
<p>Scientific survey articles play a vital role in summarizing research progress, yet their manual creation is becoming increasingly infeasible due to the rapid growth of academic literature. While large language models (LLMs) offer promising capabilities for automating this process, progress in this area is hindered by the absence of standardized benchmarks and evaluation protocols. To address this gap, we introduce SurGE (Survey Generation Evaluation), a new benchmark for evaluating scientific survey generation in the computer science domain. SurGE consists of (1) a collection of test instances, each including a topic description, an expert-written survey, and its full set of cited references, and (2) a large-scale academic corpus of over one million papers that serves as the retrieval pool. In addition, we propose an automated evaluation framework that measures generated surveys across four dimensions: information coverage, referencing accuracy, structural organization, and content quality. Our evaluation of diverse LLM-based approaches shows that survey generation remains highly challenging, even for advanced self-reflection frameworks. These findings highlight the complexity of the task and the necessity for continued research. We have open-sourced all the code, data, and models at: <a target="_blank" rel="noopener" href="https://github.com/oneal2000/SurGE">https://github.com/oneal2000/SurGE</a> </p>
<blockquote>
<p>ç§‘å­¦ç»¼è¿°æ–‡ç« åœ¨æ€»ç»“ç ”ç©¶è¿›å±•æ–¹é¢èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œç„¶è€Œç”±äºå…¶æ•°é‡çš„å¿«é€Ÿå¢é•¿ï¼Œæ‰‹åŠ¨ç¼–å†™å˜å¾—è¶Šæ¥è¶Šä¸å¯è¡Œã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ºè‡ªåŠ¨åŒ–è¿™ä¸ªè¿‡ç¨‹æä¾›äº†æœ‰å‰æ™¯çš„èƒ½åŠ›ï¼Œä½†è¿™ä¸€é¢†åŸŸçš„è¿›å±•å—åˆ°ç¼ºä¹æ ‡å‡†åŒ–åŸºå‡†æµ‹è¯•å’Œè¯„ä¼°åè®®çš„é˜»ç¢ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†SurGEï¼ˆç»¼è¿°ç”Ÿæˆè¯„ä¼°ï¼‰åŸºå‡†æµ‹è¯•ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°è®¡ç®—æœºç§‘å­¦é¢†åŸŸç§‘å­¦ç»¼è¿°ç”Ÿæˆæƒ…å†µçš„æ–°åŸºå‡†æµ‹è¯•ã€‚SurGEåŒ…æ‹¬ï¼šï¼ˆ1ï¼‰æµ‹è¯•å®ä¾‹é›†ï¼Œæ¯ä¸ªå®ä¾‹åŒ…æ‹¬ä¸»é¢˜æè¿°ã€ä¸“å®¶æ’°å†™çš„ç»¼è¿°åŠå…¶å…¨å¥—å¼•æ–‡ï¼›ï¼ˆ2ï¼‰ä½œä¸ºæ£€ç´¢æ± çš„å¤§è§„æ¨¡å­¦æœ¯è¯­æ–™åº“ï¼ŒåŒ…å«è¶…è¿‡ä¸€ç™¾ä¸‡çš„è®ºæ–‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªè‡ªåŠ¨åŒ–è¯„ä¼°æ¡†æ¶ï¼Œä»å››ä¸ªç»´åº¦è¡¡é‡ç”Ÿæˆçš„ç»¼è¿°ï¼šä¿¡æ¯è¦†ç›–ã€å¼•ç”¨å‡†ç¡®æ€§ã€ç»“æ„ç»„ç»‡å’Œå†…å®¹è´¨é‡ã€‚æˆ‘ä»¬å¯¹å¤šç§åŸºäºLLMçš„æ–¹æ³•çš„è¯„ä¼°è¡¨æ˜ï¼Œå³ä½¿æ˜¯å¯¹äºå…ˆè¿›çš„è‡ªæˆ‘åæ€æ¡†æ¶æ¥è¯´ï¼Œç”Ÿæˆç»¼è¿°ä»ç„¶æå…·æŒ‘æˆ˜æ€§ã€‚è¿™äº›å‘ç°çªæ˜¾äº†ä»»åŠ¡çš„å¤æ‚æ€§ä»¥åŠç»§ç»­ç ”ç©¶çš„å¿…è¦æ€§ã€‚æˆ‘ä»¬å·²ç»å…¬å¼€æ‰€æœ‰ä»£ç ã€æ•°æ®å’Œæ¨¡å‹åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/oneal2000/SurGE">https://github.com/oneal2000/SurGE</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15658v1">PDF</a> </p>
<p><strong>Summary</strong><br>å­¦æœ¯ç»¼è¿°æ–‡ç« åœ¨æ€»ç»“ç ”ç©¶è¿›å±•æ–¹é¢èµ·ç€é‡è¦ä½œç”¨ï¼Œä½†æ‰‹åŠ¨åˆ›å»ºç»¼è¿°è¶Šæ¥è¶Šä¸å¯è¡Œã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ºè‡ªåŠ¨åŒ–æ­¤è¿‡ç¨‹æä¾›äº†æ½œåŠ›ï¼Œä½†ç¼ºä¹æ ‡å‡†åŒ–åŸºå‡†æµ‹è¯•å’Œè¯„ä¼°åè®®é˜»ç¢äº†è¿›å±•ã€‚ä¸ºè§£å†³è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æ¨å‡ºäº†SurGEï¼ˆç»¼è¿°ç”Ÿæˆè¯„ä¼°ï¼‰åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°è®¡ç®—æœºç§‘å­¦é¢†åŸŸçš„å­¦æœ¯ç»¼è¿°ç”Ÿæˆèƒ½åŠ›ã€‚SurGEåŒ…æ‹¬ä¸€ç»„æµ‹è¯•å®ä¾‹å’Œä¸€ä¸ªå¤§è§„æ¨¡å­¦æœ¯æ–‡çŒ®åº“ï¼ŒåŒæ—¶æä¾›è‡ªåŠ¨è¯„ä¼°æ¡†æ¶æ¥è¡¡é‡ç”Ÿæˆçš„ç»¼è¿°åœ¨ä¿¡æ¯è¦†ç›–ã€å¼•ç”¨å‡†ç¡®æ€§ã€ç»“æ„ç»„ç»‡å’Œå†…å®¹è´¨é‡ç­‰æ–¹é¢çš„è¡¨ç°ã€‚æˆ‘ä»¬å¯¹ä¸åŒçš„LLMæ–¹æ³•è¿›è¡Œäº†è¯„ä¼°ï¼Œå‘ç°å³ä½¿æ˜¯é«˜çº§è‡ªæˆ‘åæ€æ¡†æ¶ï¼Œç»¼è¿°ç”Ÿæˆä»ç„¶æå…·æŒ‘æˆ˜æ€§ã€‚è¿™äº›å‘ç°å‡¸æ˜¾äº†ä»»åŠ¡çš„å¤æ‚æ€§ä»¥åŠæŒç»­ç ”ç©¶çš„å¿…è¦æ€§ã€‚ç›¸å…³ä»£ç ã€æ•°æ®å’Œæ¨¡å‹å·²å¼€æºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å­¦æœ¯ç»¼è¿°æ–‡ç« åœ¨æ€»ç»“ç ”ç©¶è¿›å±•ä¸­èµ·é‡è¦ä½œç”¨ï¼Œä½†æ‰‹åŠ¨åˆ›å»ºå˜å¾—å›°éš¾ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ºè‡ªåŠ¨åŒ–ç»¼è¿°ç”Ÿæˆæä¾›äº†æ½œåŠ›ã€‚</li>
<li>å½“å‰ç¼ºä¹æ ‡å‡†åŒ–åŸºå‡†æµ‹è¯•å’Œè¯„ä¼°åè®®æ¥è¯„ä¼°LLMåœ¨å­¦æœ¯ç»¼è¿°ç”Ÿæˆæ–¹é¢çš„æ€§èƒ½ã€‚</li>
<li>SurGEåŸºå‡†æµ‹è¯•åŒ…æ‹¬æµ‹è¯•å®ä¾‹å’Œå¤§è§„æ¨¡å­¦æœ¯æ–‡çŒ®åº“ï¼Œç”¨äºè¯„ä¼°ç§‘å­¦ç»¼è¿°ç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>SurGEæä¾›è‡ªåŠ¨è¯„ä¼°æ¡†æ¶æ¥è¡¡é‡ç”Ÿæˆçš„ç»¼è¿°åœ¨ä¿¡æ¯è¦†ç›–ã€å¼•ç”¨å‡†ç¡®æ€§ç­‰æ–¹é¢çš„è¡¨ç°ã€‚</li>
<li>LLMæ–¹æ³•åœ¨ç»¼è¿°ç”Ÿæˆæ–¹é¢ä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼Œå‡¸æ˜¾äº†ä»»åŠ¡çš„å¤æ‚æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15658">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-149c94281c63fab0be861c1249be1603.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bd7151693f752650f1b872a8d9d91a69.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="On-the-Effectiveness-of-Graph-Reordering-for-Accelerating-Approximate-Nearest-Neighbor-Search-on-GPU"><a href="#On-the-Effectiveness-of-Graph-Reordering-for-Accelerating-Approximate-Nearest-Neighbor-Search-on-GPU" class="headerlink" title="On the Effectiveness of Graph Reordering for Accelerating Approximate   Nearest Neighbor Search on GPU"></a>On the Effectiveness of Graph Reordering for Accelerating Approximate   Nearest Neighbor Search on GPU</h2><p><strong>Authors:Yutaro Oguri, Mai Nishimura, Yusuke Matsui</strong></p>
<p>We present the first systematic investigation of graph reordering effects for graph-based Approximate Nearest Neighbor Search (ANNS) on a GPU. While graph-based ANNS has become the dominant paradigm for modern AI applications, recent approaches focus on algorithmic innovations while neglecting memory layout considerations that significantly affect execution time. Our unified evaluation framework enables comprehensive evaluation of diverse reordering strategies across different graph indices through a graph adapter that converts arbitrary graph topologies into a common representation and a GPU-optimized graph traversal engine. We conduct a comprehensive analysis across diverse datasets and state-of-the-art graph indices, introducing analysis metrics that quantify the relationship between structural properties and memory layout effectiveness. Our GPU-targeted reordering achieves up to 15$%$ QPS improvements while preserving search accuracy, demonstrating that memory layout optimization operates orthogonally to existing algorithmic innovations. We will release all code upon publication to facilitate reproducibility and foster further research. </p>
<blockquote>
<p>æˆ‘ä»¬å¯¹åŸºäºå›¾çš„è¿‘ä¼¼æœ€è¿‘é‚»æœç´¢ï¼ˆANNSï¼‰åœ¨GPUä¸Šçš„å›¾é‡æ’æ•ˆåº”è¿›è¡Œäº†é¦–æ¬¡ç³»ç»Ÿç ”ç©¶ã€‚è™½ç„¶åŸºäºå›¾çš„ANNSå·²æˆä¸ºç°ä»£AIåº”ç”¨çš„ä¸»å¯¼èŒƒå¼ï¼Œä½†æœ€è¿‘çš„æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨ç®—æ³•åˆ›æ–°ä¸Šï¼Œè€Œå¿½è§†äº†å†…å­˜å¸ƒå±€è€ƒè™‘ï¼Œè¿™æ˜¾è‘—å½±å“äº†æ‰§è¡Œæ—¶é—´ã€‚æˆ‘ä»¬çš„ç»Ÿä¸€è¯„ä¼°æ¡†æ¶é€šè¿‡å›¾å½¢é€‚é…å™¨ï¼ˆå°†ä»»æ„å›¾å½¢æ‹“æ‰‘è½¬æ¢ä¸ºé€šç”¨è¡¨ç¤ºå½¢å¼ï¼‰å’ŒGPUä¼˜åŒ–çš„å›¾å½¢éå†å¼•æ“ï¼Œå…¨é¢è¯„ä¼°äº†ä¸åŒå›¾å½¢ç´¢å¼•çš„ä¸åŒé‡æ’åºç­–ç•¥ã€‚æˆ‘ä»¬åœ¨ä¸åŒæ•°æ®é›†å’Œæœ€æ–°å›¾å½¢ç´¢å¼•ä¸Šè¿›è¡Œäº†å…¨é¢çš„åˆ†æï¼Œå¼•å…¥äº†åˆ†æåº¦é‡æ ‡å‡†ï¼Œä»¥é‡åŒ–ç»“æ„å±æ€§ä¸å†…å­˜å¸ƒå±€æœ‰æ•ˆæ€§ä¹‹é—´çš„å…³ç³»ã€‚æˆ‘ä»¬é’ˆå¯¹GPUè¿›è¡Œçš„é‡æ’åºåœ¨ä¿æŒæœç´¢å‡†ç¡®æ€§çš„åŒæ—¶ï¼Œå®ç°äº†é«˜è¾¾15%çš„æŸ¥è¯¢æ¯ç§’ï¼ˆQPSï¼‰æ”¹è¿›ï¼Œè¿™è¡¨æ˜å†…å­˜å¸ƒå±€ä¼˜åŒ–ä¸ç°æœ‰çš„ç®—æ³•åˆ›æ–°æ˜¯å‚ç›´è¿è¡Œçš„ã€‚æˆ‘ä»¬ä¼šåœ¨å‘è¡¨æ—¶å…¬å¼€æ‰€æœ‰ä»£ç ï¼Œä»¥ä¿ƒè¿›å¯é‡å¤æ€§å’Œè¿›ä¸€æ­¥çš„ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15436v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡é¦–æ¬¡å¯¹åŸºäºGPUçš„å›¾å½¢é‡æ’åºåœ¨å›¾å½¢è¿‘ä¼¼æœ€è¿‘é‚»æœç´¢ï¼ˆANNSï¼‰ä¸­çš„å½±å“è¿›è¡Œäº†ç³»ç»Ÿç ”ç©¶ã€‚ç ”ç©¶å‘ç°åœ¨ç°ä»£AIåº”ç”¨ä¸­ï¼Œè™½ç„¶åŸºäºå›¾å½¢çš„ANNSå·²æˆä¸ºä¸»æµèŒƒå¼ï¼Œä½†æœ€è¿‘çš„ç®—æ³•æ›´å¤šåœ°å…³æ³¨ç®—æ³•åˆ›æ–°ï¼Œè€Œå¿½è§†äº†å†…å­˜å¸ƒå±€å¯¹æ‰§è¡Œæ—¶é—´çš„æ˜¾è‘—å½±å“ã€‚æœ¬æ–‡æä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„è¯„ä¼°æ¡†æ¶ï¼Œèƒ½å¤Ÿå…¨é¢è¯„ä¼°ä¸åŒé‡æ’åºç­–ç•¥åœ¨ä¸åŒå›¾å½¢ç´¢å¼•ä¸Šçš„è¡¨ç°ã€‚é€šè¿‡å›¾å½¢é€‚é…å™¨å’ŒGPUä¼˜åŒ–çš„å›¾å½¢éå†å¼•æ“ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿå°†ä»»æ„å›¾å½¢æ‹“æ‰‘è½¬æ¢ä¸ºé€šç”¨è¡¨ç¤ºå½¢å¼ã€‚æœ¬æ–‡å…¨é¢åˆ†æäº†ä¸åŒæ•°æ®é›†å’Œæœ€æ–°å›¾å½¢ç´¢å¼•ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶å¼•å…¥äº†é‡åŒ–ç»“æ„ç‰¹æ€§å’Œå†…å­˜å¸ƒå±€æœ‰æ•ˆæ€§çš„åˆ†ææŒ‡æ ‡ã€‚é€šè¿‡é’ˆå¯¹GPUçš„é‡æ’åºä¼˜åŒ–ï¼Œå®ç°äº†æŸ¥è¯¢é€Ÿç‡ï¼ˆQPSï¼‰æé«˜è¾¾15%ï¼ŒåŒæ—¶ä¿è¯äº†æœç´¢ç²¾åº¦ã€‚è¿™è¡¨æ˜å†…å­˜å¸ƒå±€ä¼˜åŒ–ä¸ç°æœ‰çš„ç®—æ³•åˆ›æ–°æ˜¯ç›¸è¾…ç›¸æˆçš„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡é¦–æ¬¡ç³»ç»Ÿåœ°ç ”ç©¶äº†å›¾å½¢é‡æ’åºåœ¨åŸºäºGPUçš„å›¾å½¢è¿‘ä¼¼æœ€è¿‘é‚»æœç´¢ï¼ˆANNSï¼‰ä¸­çš„å½±å“ã€‚</li>
<li>ç ”ç©¶æŒ‡å‡ºï¼Œå°½ç®¡åŸºäºå›¾å½¢çš„ANNSåœ¨ç°ä»£AIåº”ç”¨ä¸­å æ®ä¸»å¯¼åœ°ä½ï¼Œä½†æœ€è¿‘çš„ç®—æ³•æ›´å¤šåœ°å…³æ³¨ç®—æ³•åˆ›æ–°ï¼Œå¿½è§†äº†å†…å­˜å¸ƒå±€çš„é‡è¦æ€§ã€‚</li>
<li>æä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„è¯„ä¼°æ¡†æ¶ï¼Œç”¨äºå…¨é¢è¯„ä¼°ä¸åŒé‡æ’åºç­–ç•¥åœ¨ä¸åŒå›¾å½¢ç´¢å¼•ä¸Šçš„è¡¨ç°ã€‚</li>
<li>é€šè¿‡å›¾å½¢é€‚é…å™¨å’ŒGPUä¼˜åŒ–çš„å›¾å½¢éå†å¼•æ“ï¼Œä»»æ„å›¾å½¢æ‹“æ‰‘å¯è½¬æ¢ä¸ºé€šç”¨è¡¨ç¤ºå½¢å¼ã€‚</li>
<li>ç ”ç©¶å…¨é¢åˆ†æäº†ä¸åŒæ•°æ®é›†å’Œæœ€æ–°å›¾å½¢ç´¢å¼•ä¹‹é—´çš„å…³ç³»ï¼Œå¼•å…¥äº†åˆ†æç»“æ„ç‰¹æ€§å’Œå†…å­˜å¸ƒå±€æœ‰æ•ˆæ€§çš„é‡åŒ–æŒ‡æ ‡ã€‚</li>
<li>é€šè¿‡é’ˆå¯¹GPUçš„é‡æ’åºä¼˜åŒ–ï¼Œå®ç°äº†æŸ¥è¯¢é€Ÿç‡ï¼ˆQPSï¼‰çš„æé«˜ï¼Œæœ€é«˜å¯è¾¾15%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15436">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2bea5250e879d514c939f5e4aacf3f77.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e313829f7a68e4c8973af3aecad97608.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1771e6ff9907104721b045215dba0e47.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-74ebff8e3918cb23fe11746ab355c450.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="DIO-Refining-Mutual-Information-and-Causal-Chain-to-Enhance-Machine-Abstract-Reasoning-Ability"><a href="#DIO-Refining-Mutual-Information-and-Causal-Chain-to-Enhance-Machine-Abstract-Reasoning-Ability" class="headerlink" title="DIO: Refining Mutual Information and Causal Chain to Enhance Machine   Abstract Reasoning Ability"></a>DIO: Refining Mutual Information and Causal Chain to Enhance Machine   Abstract Reasoning Ability</h2><p><strong>Authors:Ruizhuo Song, Beiming Yuan</strong></p>
<p>Despite the outstanding performance of current deep learning models across various domains, their fundamental bottleneck in abstract reasoning remains unresolved. To address this challenge, the academic community has introduced Ravenâ€™s Progressive Matrices (RPM) problems as an authoritative benchmark for evaluating the abstract reasoning capabilities of deep learning algorithms, with a focus on core intelligence dimensions such as abstract reasoning, pattern recognition, and complex problem-solving. Therefore, this paper centers on solving RPM problems, aiming to contribute to enhancing the abstract reasoning abilities of machine intelligence. Firstly, this paper adopts a &#96;&#96;causal chain modelingâ€™â€™ perspective to systematically analyze the complete causal chain in RPM tasks: image $\rightarrow$ abstract attributes $\rightarrow$ progressive attribute patterns $\rightarrow$ pattern consistency $\rightarrow$ correct answer. Based on this analysis, the network architecture of the baseline model DIO is designed. However, experiments reveal that the optimization objective formulated for DIO, namely maximizing the variational lower bound of mutual information between the context and the correct option, fails to enable the model to genuinely acquire the predefined human reasoning logic. This is attributed to two main reasons: the tightness of the lower bound significantly impacts the effectiveness of mutual information maximization, and mutual information, as a statistical measure, does not capture the causal relationship between subjects and objects. To overcome these limitations, this paper progressively proposes three improvement methods: </p>
<blockquote>
<p>å°½ç®¡å½“å‰æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨å„ç§é¢†åŸŸè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œå®ƒä»¬åœ¨æŠ½è±¡æ¨ç†æ–¹é¢çš„æ ¹æœ¬ç“¶é¢ˆä»æœªè§£å†³ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œå­¦æœ¯ç•Œå¼•å…¥äº†Ravençš„æ¸è¿›çŸ©é˜µï¼ˆRPMï¼‰é—®é¢˜ä½œä¸ºè¯„ä¼°æ·±åº¦å­¦ä¹ ç®—æ³•æŠ½è±¡æ¨ç†èƒ½åŠ›çš„æƒå¨åŸºå‡†æµ‹è¯•ï¼Œé‡ç‚¹å…³æ³¨æŠ½è±¡æ¨ç†ã€æ¨¡å¼è¯†åˆ«å’Œå¤æ‚é—®é¢˜è§£å†³ç­‰æ ¸å¿ƒæ™ºåŠ›ç»´åº¦ã€‚å› æ­¤ï¼Œæœ¬æ–‡ä¸“æ³¨äºè§£å†³RPMé—®é¢˜ï¼Œæ—¨åœ¨æé«˜æœºå™¨æ™ºèƒ½çš„æŠ½è±¡æ¨ç†èƒ½åŠ›ã€‚é¦–å…ˆï¼Œæœ¬æ–‡é‡‡ç”¨â€œå› æœé“¾å»ºæ¨¡â€çš„è§’åº¦ï¼Œç³»ç»Ÿåœ°åˆ†æäº†RPMä»»åŠ¡ä¸­çš„å®Œæ•´å› æœé“¾ï¼šå›¾åƒ$\rightarrow$æŠ½è±¡å±æ€§$\rightarrow$æ¸è¿›å±æ€§æ¨¡å¼$\rightarrow$æ¨¡å¼ä¸€è‡´æ€§$\rightarrow$æ­£ç¡®ç­”æ¡ˆã€‚åŸºäºæ­¤åˆ†æï¼Œè®¾è®¡äº†åŸºçº¿æ¨¡å‹DIOçš„ç½‘ç»œæ¶æ„ã€‚ç„¶è€Œï¼Œå®éªŒè¡¨æ˜ï¼Œä¸ºDIOåˆ¶å®šçš„ä¼˜åŒ–ç›®æ ‡ï¼Œå³æœ€å¤§åŒ–ä¸Šä¸‹æ–‡ä¸æ­£ç¡®é€‰é¡¹ä¹‹é—´çš„äº’ä¿¡æ¯çš„å˜åˆ†ä¸‹ç•Œï¼Œæœªèƒ½ä½¿æ¨¡å‹çœŸæ­£è·å¾—é¢„å…ˆè®¾å®šçš„äººç±»æ¨ç†é€»è¾‘ã€‚è¿™ä¸»è¦å½’å› äºä¸¤ä¸ªåŸå› ï¼šä¸‹ç•Œçš„ç´§å¯†æ€§æ˜¾è‘—å½±å“äº†äº’ä¿¡æ¯æœ€å¤§åŒ–çš„æœ‰æ•ˆæ€§ï¼›ä½œä¸ºç»Ÿè®¡é‡åº¦çš„äº’ä¿¡æ¯å¹¶ä¸èƒ½æ•æ‰ä¸»ä½“ä¸å¯¹è±¡ä¹‹é—´çš„å› æœå…³ç³»ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæœ¬æ–‡é€æ­¥æå‡ºäº†ä¸‰ç§æ”¹è¿›æ–¹æ³•ï¼š</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15387v1">PDF</a> 15 pages, 9 figures, 8 tables</p>
<p><strong>Summary</strong><br>æ·±åº¦å­¦ä¹ æ¨¡å‹è™½ç„¶åœ¨å¤šä¸ªé¢†åŸŸè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æŠ½è±¡æ¨ç†æ–¹é¢ä»å­˜åœ¨ç“¶é¢ˆã€‚ä¸ºåº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œå­¦æœ¯ç•Œå¼•å…¥äº†Ravenâ€™s Progressive Matricesï¼ˆRPMï¼‰é—®é¢˜ä½œä¸ºè¯„ä¼°æ·±åº¦å­¦ä¹ ç®—æ³•æŠ½è±¡æ¨ç†èƒ½åŠ›çš„æƒå¨åŸºå‡†æµ‹è¯•ã€‚æœ¬æ–‡ä¸“æ³¨äºè§£å†³RPMé—®é¢˜ï¼Œæ—¨åœ¨æé«˜æœºå™¨æ™ºèƒ½çš„æŠ½è±¡æ¨ç†èƒ½åŠ›ã€‚è¯¥æ–‡é‡‡ç”¨å› æœé“¾å»ºæ¨¡æ–¹æ³•ï¼Œåˆ†æRPMä»»åŠ¡çš„å®Œæ•´å› æœé“¾ï¼Œå¹¶è®¾è®¡åŸºçº¿æ¨¡å‹DIOçš„ç½‘ç»œæ¶æ„ã€‚ç„¶è€Œï¼Œå®éªŒè¡¨æ˜ï¼ŒDIOçš„ä¼˜åŒ–ç›®æ ‡æœªèƒ½ä½¿æ¨¡å‹çœŸæ­£è·å¾—é¢„è®¾çš„äººç±»æ¨ç†é€»è¾‘ã€‚åŸå› åœ¨äºå˜åˆ†ä¸‹ç•Œçš„ç´§å¯†æ€§å½±å“äº†ä¿¡æ¯æœ€å¤§åŒ–çš„æœ‰æ•ˆæ€§ï¼Œä¸”ä¿¡æ¯æœ€å¤§åŒ–ä½œä¸ºç»Ÿè®¡åº¦é‡æ‰‹æ®µæœªèƒ½æ•æ‰ä¸»ä½“ä¸å¯¹è±¡ä¹‹é—´çš„å› æœå…³ç³»ã€‚ä¸ºå…‹æœè¿™äº›å±€é™ï¼Œæœ¬æ–‡é€æ­¥æå‡ºä¸‰ç§æ”¹è¿›æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨æŠ½è±¡æ¨ç†æ–¹é¢å­˜åœ¨ç“¶é¢ˆï¼Œéœ€è¦å¼•å…¥æ–°çš„è¯„ä¼°åŸºå‡†å¦‚RPMé—®é¢˜æ¥è¯„ä¼°å…¶èƒ½åŠ›ã€‚</li>
<li>æœ¬æ–‡é€šè¿‡å› æœé“¾å»ºæ¨¡åˆ†æRPMé—®é¢˜çš„å®Œæ•´å› æœé“¾ï¼Œå¹¶è®¾è®¡åŸºçº¿æ¨¡å‹DIOã€‚</li>
<li>DIOçš„ä¼˜åŒ–ç›®æ ‡æœªèƒ½ä½¿æ¨¡å‹çœŸæ­£è·å¾—é¢„è®¾çš„äººç±»æ¨ç†é€»è¾‘ï¼Œä¸»è¦åŸå› æ˜¯å˜åˆ†ä¸‹ç•Œçš„ç´§å¯†æ€§å’Œä¿¡æ¯æœ€å¤§åŒ–çš„æœ‰æ•ˆæ€§é—®é¢˜ã€‚</li>
<li>ä¿¡æ¯æœ€å¤§åŒ–ä½œä¸ºç»Ÿè®¡åº¦é‡æ‰‹æ®µæ— æ³•æ•æ‰ä¸»ä½“ä¸å¯¹è±¡ä¹‹é—´çš„å› æœå…³ç³»ã€‚</li>
<li>ä¸ºæ”¹è¿›æ¨¡å‹æ€§èƒ½ï¼Œæœ¬æ–‡æå‡ºäº†ä¸‰ç§ç­–ç•¥ï¼šä¼˜åŒ–å˜åˆ†ä¸‹ç•Œã€ç»“åˆä½¿ç”¨å¤šç§æ¨ç†æ–¹æ³•å’Œå¼ºåŒ–å­¦ä¹ ã€ä»¥åŠæ„å»ºæ›´åŠ å¤æ‚çš„ç½‘ç»œç»“æ„æ¥å¢å¼ºæ¨¡å‹çš„æŠ½è±¡æ¨ç†èƒ½åŠ›ã€‚</li>
<li>è§£å†³RPMé—®é¢˜å¯¹äºæé«˜æœºå™¨æ™ºèƒ½çš„æŠ½è±¡æ¨ç†èƒ½åŠ›å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15387">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9f4157e6d38b295a01167a71e0cdb021.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2e6f4a6deae6ae50bf3812a754edefcf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b753210c8e2f98ef9b811c7def2ddf9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0184ee4d30aec0efd872e35e1ee6822f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f4c20a71e65def04c35f4eaf6f8b495.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b46e3e60dd0bb37185e4719641844967.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Unveiling-Trust-in-Multimodal-Large-Language-Models-Evaluation-Analysis-and-Mitigation"><a href="#Unveiling-Trust-in-Multimodal-Large-Language-Models-Evaluation-Analysis-and-Mitigation" class="headerlink" title="Unveiling Trust in Multimodal Large Language Models: Evaluation,   Analysis, and Mitigation"></a>Unveiling Trust in Multimodal Large Language Models: Evaluation,   Analysis, and Mitigation</h2><p><strong>Authors:Yichi Zhang, Yao Huang, Yifan Wang, Yitong Sun, Chang Liu, Zhe Zhao, Zhengwei Fang, Huanran Chen, Xiao Yang, Xingxing Wei, Hang Su, Yinpeng Dong, Jun Zhu</strong></p>
<p>The trustworthiness of Multimodal Large Language Models (MLLMs) remains an intense concern despite the significant progress in their capabilities. Existing evaluation and mitigation approaches often focus on narrow aspects and overlook risks introduced by the multimodality. To tackle these challenges, we propose MultiTrust-X, a comprehensive benchmark for evaluating, analyzing, and mitigating the trustworthiness issues of MLLMs. We define a three-dimensional framework, encompassing five trustworthiness aspects which include truthfulness, robustness, safety, fairness, and privacy; two novel risk types covering multimodal risks and cross-modal impacts; and various mitigation strategies from the perspectives of data, model architecture, training, and inference algorithms. Based on the taxonomy, MultiTrust-X includes 32 tasks and 28 curated datasets, enabling holistic evaluations over 30 open-source and proprietary MLLMs and in-depth analysis with 8 representative mitigation methods. Our extensive experiments reveal significant vulnerabilities in current models, including a gap between trustworthiness and general capabilities, as well as the amplification of potential risks in base LLMs by both multimodal training and inference. Moreover, our controlled analysis uncovers key limitations in existing mitigation strategies that, while some methods yield improvements in specific aspects, few effectively address overall trustworthiness, and many introduce unexpected trade-offs that compromise model utility. These findings also provide practical insights for future improvements, such as the benefits of reasoning to better balance safety and performance. Based on these insights, we introduce a Reasoning-Enhanced Safety Alignment (RESA) approach that equips the model with chain-of-thought reasoning ability to discover the underlying risks, achieving state-of-the-art results. </p>
<blockquote>
<p>å°½ç®¡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„èƒ½åŠ›å·²ç»å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†å…¶å¯ä¿¡åº¦ä»ç„¶æ˜¯ä¸€ä¸ªå¤‡å—å…³æ³¨çš„é—®é¢˜ã€‚ç°æœ‰çš„è¯„ä¼°å’Œç¼“è§£æ–¹æ³•å¾€å¾€ä¾§é‡äºç‹­çª„çš„æ–¹é¢ï¼Œå¿½è§†äº†å¤šæ¨¡æ€å¼•å…¥çš„é£é™©ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†MultiTrust-Xï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢è¯„ä¼°ã€åˆ†æå’Œç¼“è§£MLLMså¯ä¿¡åº¦é—®é¢˜çš„åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªä¸‰ç»´æ¡†æ¶ï¼ŒåŒ…å«äº”ä¸ªå¯ä¿¡åº¦æ–¹é¢ï¼Œå³çœŸå®æ€§ã€ç¨³å¥æ€§ã€å®‰å…¨æ€§ã€å…¬å¹³æ€§å’Œéšç§æ€§ï¼›ä¸¤ç§æ–°å‹é£é™©ç±»å‹ï¼ŒåŒ…æ‹¬å¤šæ¨¡æ€é£é™©å’Œè·¨æ¨¡æ€å½±å“ï¼›ä»¥åŠä»æ•°æ®ã€æ¨¡å‹ç»“æ„ã€è®­ç»ƒå’Œæ¨ç†ç®—æ³•ç­‰è§’åº¦çš„å¤šç§ç¼“è§£ç­–ç•¥ã€‚åŸºäºåˆ†ç±»å­¦ï¼ŒMultiTrust-XåŒ…æ‹¬32ä¸ªä»»åŠ¡å’Œ28ä¸ªç²¾é€‰æ•°æ®é›†ï¼Œèƒ½å¤Ÿå¯¹30ä¸ªå¼€æºå’Œä¸“æœ‰MLLMè¿›è¡Œæ•´ä½“è¯„ä¼°ï¼Œå¹¶ä½¿ç”¨8ç§å…·æœ‰ä»£è¡¨æ€§çš„ç¼“è§£æ–¹æ³•è¿›è¡Œæ·±å…¥åˆ†æã€‚æˆ‘ä»¬çš„å¤§é‡å®éªŒæ­ç¤ºäº†å½“å‰æ¨¡å‹çš„é‡è¦æ¼æ´ï¼ŒåŒ…æ‹¬å¯ä¿¡åº¦ä¸é€šç”¨èƒ½åŠ›ä¹‹é—´çš„é¸¿æ²Ÿï¼Œä»¥åŠåŸºäºLLMçš„åŸºç¡€è®­ç»ƒä¸­æ¨ç†å’Œæ¨æ–­çš„å¤šæ¨¡æ€å¸¦æ¥çš„æ½œåœ¨é£é™©çš„æ”¾å¤§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„å—æ§åˆ†ææ­ç¤ºäº†ç°æœ‰ç¼“è§£ç­–ç•¥çš„å…³é”®å±€é™æ€§ï¼Œå³è™½ç„¶ä¸€äº›æ–¹æ³•åœ¨ç‰¹å®šæ–¹é¢æœ‰æ‰€æ”¹è¿›ï¼Œä½†å‡ ä¹æ²¡æœ‰ä»€ä¹ˆèƒ½æœ‰æ•ˆåœ°è§£å†³æ•´ä½“å¯ä¿¡åº¦é—®é¢˜ï¼Œè€Œä¸”è®¸å¤šæ–¹æ³•å¼•å…¥äº†æ„å¤–çš„æƒè¡¡ï¼Œä»è€ŒæŸå®³äº†æ¨¡å‹çš„å®ç”¨æ€§ã€‚è¿™äº›å‘ç°è¿˜ä¸ºæœªæ¥çš„æ”¹è¿›æä¾›äº†å®é™…è§è§£ï¼Œä¾‹å¦‚é€šè¿‡æ¨ç†æ¥æ›´å¥½åœ°å¹³è¡¡å®‰å…¨æ€§å’Œæ€§èƒ½çš„å¥½å¤„ã€‚åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¢å¼ºå‹å®‰å…¨å¯¹é½æ¨ç†ï¼ˆRESAï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èµ‹äºˆæ¨¡å‹é“¾å¼æ€ç»´æ¨ç†èƒ½åŠ›ï¼Œä»¥å‘ç°æ½œåœ¨é£é™©ï¼Œå®ç°æœ€æ–°ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15370v1">PDF</a> For Appendix, please refer to arXiv:2406.07057</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬æå‡ºäº†é’ˆå¯¹å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„ä¿¡ä»»é—®é¢˜çš„ä¸€ç§å…¨é¢çš„åŸºå‡†æµ‹è¯•æ–¹æ¡ˆMultiTrust-Xã€‚æ–‡ç« è¯¦ç»†æè¿°äº†æ‰€æå‡ºçš„æ–¹æ¡ˆçš„æ„å»ºæ–¹æ³•å’Œé‡è¦ç‰¹ç‚¹ï¼ŒåŒ…æ‹¬å¯¹MLLMsçš„å¯ä¿¡åº¦çš„äº”ä¸ªæ–¹é¢çš„è¯„ä»·ï¼šçœŸå®æ€§ã€ç¨³å¥æ€§ã€å®‰å…¨æ€§ã€å…¬å¹³æ€§å’Œéšç§æ€§ï¼›æ¶µç›–å¤šæ¨¡æ€é£é™©å’Œè·¨æ¨¡æ€å½±å“çš„æ–°é£é™©ç±»å‹ï¼›ä»¥åŠä»æ•°æ®ã€æ¨¡å‹æ¶æ„ã€è®­ç»ƒå’Œæ¨ç†ç®—æ³•ç­‰è§’åº¦çš„ç¼“è§£ç­–ç•¥ã€‚åŸºäºè¿™äº›åˆ†ç±»ï¼ŒMultiTrust-XåŒ…æ‹¬32é¡¹ä»»åŠ¡å’Œ28ä¸ªç²¾é€‰æ•°æ®é›†ï¼Œèƒ½å¤Ÿå¯¹30ä¸ªå¼€æºå’Œä¸“æœ‰MLLMè¿›è¡Œå…¨é¢è¯„ä¼°ï¼Œå¹¶ä½¿ç”¨8ç§å…·æœ‰ä»£è¡¨æ€§çš„ç¼“è§£æ–¹æ³•è¿›è¡Œæ·±å…¥åˆ†æã€‚æ–‡ç« è¿˜ä»‹ç»äº†åŸºäºå®éªŒå’Œåˆ†æçš„å‘ç°ï¼ŒåŒ…æ‹¬å½“å‰æ¨¡å‹ä¸­çš„æ˜¾è‘—æ¼æ´å’Œæœªæ¥æ”¹è¿›çš„å®é™…è§è§£ã€‚æœ€åæå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•RESAæ¥å¢å¼ºæ¨¡å‹çš„å®‰å…¨æ€§å’Œæ€§èƒ½å¹³è¡¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å¯ä¿¡æ€§ä»æ˜¯äººä»¬å…³æ³¨çš„é‡ç‚¹ï¼Œå°½ç®¡å…¶èƒ½åŠ›å·²å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚</li>
<li>ç°æœ‰è¯„ä¼°å’Œç¼“è§£æ–¹æ³•å¾€å¾€å…³æ³¨ç‹­çª„çš„æ–¹é¢ï¼Œå¿½è§†äº†å¤šæ¨¡æ€å¼•å…¥çš„é£é™©ã€‚</li>
<li>MultiTrust-Xæ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°ã€åˆ†æå’Œç¼“è§£MLLMsçš„å¯ä¿¡æ€§é—®é¢˜ã€‚å®ƒå®šä¹‰äº†ä¸€ä¸ªåŒ…å«äº”ä¸ªå¯ä¿¡æ–¹é¢çš„ä¸‰ç»´æ¡†æ¶ï¼ŒåŒ…æ‹¬çœŸå®æ€§ã€ç¨³å¥æ€§ã€å®‰å…¨æ€§ã€å…¬å¹³æ€§å’Œéšç§æ€§ã€‚</li>
<li>MultiTrust-Xè¿˜åŒ…æ‹¬æ¶µç›–å¤šæ¨¡æ€é£é™©å’Œè·¨æ¨¡æ€å½±å“çš„æ–°é£é™©ç±»å‹ä»¥åŠå¤šç§ç¼“è§£ç­–ç•¥ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œå½“å‰æ¨¡å‹å­˜åœ¨æ˜¾è‘—æ¼æ´ï¼ŒåŒ…æ‹¬å¯ä¿¡æ€§ä¸ä¸€èˆ¬èƒ½åŠ›ä¹‹é—´çš„é¸¿æ²Ÿï¼Œä»¥åŠåŸºç¡€LLMsä¸­æ½œåœ¨é£é™©çš„æ”¾å¤§ã€‚è¿™äº›é£é™©åœ¨å¤šæ¨¡æ€è®­ç»ƒå’Œæ¨ç†ä¸­å°¤ä¸ºæ˜æ˜¾ã€‚</li>
<li>ç°æœ‰çš„ç¼“è§£ç­–ç•¥å­˜åœ¨å±€é™æ€§ï¼Œä¸€äº›æ–¹æ³•ä»…åœ¨ç‰¹å®šæ–¹é¢æœ‰æ‰€æ”¹å–„ï¼Œè€Œå°‘æ•°æ–¹æ³•èƒ½æœ‰æ•ˆè§£å†³æ•´ä½“çš„å¯ä¿¡æ€§é—®é¢˜ï¼Œå¹¶å¯èƒ½å¼•å…¥å½±å“æ¨¡å‹å®ç”¨æ€§çš„æ„å¤–æƒè¡¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15370">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d5bfcc58d265f1a640a3136578e3056e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-132316f58e4fb3d8b422ad515a45e558.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9db2886d8199b65a7a62242c9fa49c16.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ce219559fe8fa8cfb981c662bb3f9ba.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bcd456c1e0d3c2a511250562e8a2ed70.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="KG-EDAS-A-Meta-Metric-Framework-for-Evaluating-Knowledge-Graph-Completion-Models"><a href="#KG-EDAS-A-Meta-Metric-Framework-for-Evaluating-Knowledge-Graph-Completion-Models" class="headerlink" title="KG-EDAS: A Meta-Metric Framework for Evaluating Knowledge Graph   Completion Models"></a>KG-EDAS: A Meta-Metric Framework for Evaluating Knowledge Graph   Completion Models</h2><p><strong>Authors:Haji Gul, Abul Ghani Naim, Ajaz Ahmad Bhat</strong></p>
<p>Knowledge Graphs (KGs) enable applications in various domains such as semantic search, recommendation systems, and natural language processing. KGs are often incomplete, missing entities and relations, an issue addressed by Knowledge Graph Completion (KGC) methods that predict missing elements. Different evaluation metrics, such as Mean Reciprocal Rank (MRR), Mean Rank (MR), and Hit@k, are commonly used to assess the performance of such KGC models. A major challenge in evaluating KGC models, however, lies in comparing their performance across multiple datasets and metrics. A model may outperform others on one dataset but underperform on another, making it difficult to determine overall superiority. Moreover, even within a single dataset, different metrics such as MRR and Hit@1 can yield conflicting rankings, where one model excels in MRR while another performs better in Hit@1, further complicating model selection for downstream tasks. These inconsistencies hinder holistic comparisons and highlight the need for a unified meta-metric that integrates performance across all metrics and datasets to enable a more reliable and interpretable evaluation framework. To address this need, we propose KG Evaluation based on Distance from Average Solution (EDAS), a robust and interpretable meta-metric that synthesizes model performance across multiple datasets and diverse evaluation criteria into a single normalized score ($M_i \in [0,1]$). Unlike traditional metrics that focus on isolated aspects of performance, EDAS offers a global perspective that supports more informed model selection and promotes fairness in cross-dataset evaluation. Experimental results on benchmark datasets such as FB15k-237 and WN18RR demonstrate that EDAS effectively integrates multi-metric, multi-dataset performance into a unified ranking, offering a consistent, robust, and generalizable framework for evaluating KGC models. </p>
<blockquote>
<p>çŸ¥è¯†å›¾è°±ï¼ˆKGsï¼‰å¯åº”ç”¨äºå„ç§é¢†åŸŸï¼Œå¦‚è¯­ä¹‰æœç´¢ã€æ¨èç³»ç»Ÿå’Œè‡ªç„¶è¯­è¨€å¤„ç†ã€‚çŸ¥è¯†å›¾è°±å¾€å¾€æ˜¯ä¸å®Œæ•´çš„ï¼Œç¼ºå°‘å®ä½“å’Œå…³ç³»ï¼Œè¿™ä¸€é—®é¢˜é€šè¿‡çŸ¥è¯†å›¾è°±è¡¥å…¨ï¼ˆKGCï¼‰æ–¹æ³•å¾—åˆ°è§£å†³ï¼Œè¿™äº›æ–¹æ³•èƒ½å¤Ÿé¢„æµ‹ç¼ºå¤±å…ƒç´ ã€‚é€šå¸¸ä½¿ç”¨è¯¸å¦‚å¹³å‡å€’æ•°æ’åï¼ˆMRRï¼‰ã€å¹³å‡æ’åï¼ˆMRï¼‰å’Œå‘½ä¸­@kç­‰è¯„ä¼°æŒ‡æ ‡æ¥è¯„ä¼°æ­¤ç±»KGCæ¨¡å‹çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¯„ä¼°KGCæ¨¡å‹çš„ä¸»è¦æŒ‘æˆ˜åœ¨äºæ¯”è¾ƒå®ƒä»¬åœ¨å¤šä¸ªæ•°æ®é›†å’ŒæŒ‡æ ‡ä¸Šçš„æ€§èƒ½ã€‚ä¸€ä¸ªæ¨¡å‹å¯èƒ½åœ¨æŸä¸ªæ•°æ®é›†ä¸Šä¼˜äºå…¶ä»–æ¨¡å‹ï¼Œä½†åœ¨å¦ä¸€ä¸ªæ•°æ®é›†ä¸Šè¡¨ç°è¾ƒå·®ï¼Œè¿™ä½¿å¾—éš¾ä»¥ç¡®å®šå…¶æ€»ä½“ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼Œå³ä½¿åœ¨å•ä¸ªæ•°æ®é›†ä¸­ï¼Œä¸åŒçš„æŒ‡æ ‡ï¼ˆå¦‚MRRå’Œå‘½ä¸­@1ï¼‰ä¹Ÿå¯èƒ½äº§ç”Ÿå†²çªçš„æ’åï¼Œä¸€ä¸ªæ¨¡å‹å¯èƒ½åœ¨MRRä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè€Œåœ¨å‘½ä¸­@1ä¸­è¡¨ç°è¾ƒå¥½ï¼Œè¿™è¿›ä¸€æ­¥ä½¿ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„æ¨¡å‹é€‰æ‹©å¤æ‚åŒ–ã€‚è¿™äº›ä¸ä¸€è‡´æ€§é˜»ç¢äº†å…¨é¢çš„æ¯”è¾ƒï¼Œå¹¶å¼ºè°ƒäº†éœ€è¦ä¸€ä¸ªç»Ÿä¸€çš„åº¦é‡æ ‡å‡†ï¼Œè¯¥æ ‡å‡†èƒ½å¤Ÿæ•´åˆæ‰€æœ‰æŒ‡æ ‡å’Œæ•°æ®é›†çš„æ€§èƒ½ï¼Œä»è€Œå»ºç«‹ä¸€ä¸ªæ›´å¯é å’Œå¯è§£é‡Šçš„è¯„ä»·æ¡†æ¶ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€éœ€æ±‚ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºè·ç¦»å¹³å‡è§£ï¼ˆEDASï¼‰çš„KGè¯„ä¼°æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§ç¨³å¥ä¸”å¯è§£é‡Šçš„åº¦é‡æ ‡å‡†ï¼Œå¯å°†æ¨¡å‹åœ¨å¤šä¸ªæ•°æ®é›†å’Œä¸åŒè¯„ä»·æŒ‡æ ‡ä¸Šçš„æ€§èƒ½ç»¼åˆæˆä¸€ä¸ªå•ä¸€çš„å½’ä¸€åŒ–åˆ†æ•°ï¼ˆMiâˆˆ[0,1]ï¼‰ã€‚ä¸ä¼ ç»Ÿçš„å­¤ç«‹å…³æ³¨æ€§èƒ½æ–¹é¢çš„æŒ‡æ ‡ä¸åŒï¼ŒEDASæä¾›äº†å…¨å±€è§†è§’ï¼Œæ”¯æŒæ›´æ˜æ™ºçš„æ¨¡å‹é€‰æ‹©ï¼Œå¹¶ä¿ƒè¿›äº†è·¨æ•°æ®é›†çš„å…¬å¹³è¯„ä¼°ã€‚åœ¨FB15k-237å’ŒWN18RRç­‰åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒEDASæœ‰æ•ˆåœ°å°†å¤šæŒ‡æ ‡ã€å¤šæ•°æ®é›†çš„æ€§èƒ½é›†æˆåˆ°ä¸€ä¸ªç»Ÿä¸€çš„æ’åä¸­ï¼Œä¸ºè¯„ä¼°KGCæ¨¡å‹æä¾›äº†ä¸€ä¸ªä¸€è‡´ã€ç¨³å¥å’Œé€šç”¨çš„æ¡†æ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15357v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†çŸ¥è¯†å›¾è°±ï¼ˆKGsï¼‰åœ¨å„é¢†åŸŸçš„åº”ç”¨ï¼Œå¦‚è¯­ä¹‰æœç´¢ã€æ¨èç³»ç»Ÿå’Œè‡ªç„¶è¯­è¨€å¤„ç†ã€‚çŸ¥è¯†å›¾è°±å¸¸å¸¸ä¸å®Œæ•´ï¼Œç¼ºå°‘å®ä½“å’Œå…³ç³»ï¼ŒçŸ¥è¯†å›¾è°±è¡¥å…¨ï¼ˆKGCï¼‰æ–¹æ³•å¯é¢„æµ‹ç¼ºå¤±å…ƒç´ ã€‚ä½¿ç”¨ä¸åŒçš„è¯„ä¼°æŒ‡æ ‡ï¼Œå¦‚å¹³å‡å€’æ•°æ’åï¼ˆMRRï¼‰ã€å¹³å‡æ’åï¼ˆMRï¼‰å’Œå‘½ä¸­@kï¼Œæ¥è¯„ä¼°KGCæ¨¡å‹çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œåœ¨å¤šä¸ªæ•°æ®é›†å’ŒæŒ‡æ ‡ä¸Šæ¯”è¾ƒKGCæ¨¡å‹çš„æ€§èƒ½æ˜¯ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œå³ä½¿åœ¨å•ä¸ªæ•°æ®é›†ä¸­ï¼Œä¸åŒçš„æŒ‡æ ‡ä¹Ÿå¯èƒ½äº§ç”Ÿå†²çªçš„æ’åç»“æœã€‚é’ˆå¯¹è¿™ä¸€éœ€æ±‚ï¼Œæœ¬æ–‡æå‡ºäº†åŸºäºè·ç¦»å¹³å‡è§£ï¼ˆEDASï¼‰çš„KGè¯„ä¼°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†æ¨¡å‹åœ¨å¤šä¸ªæ•°æ®é›†å’Œå¤šç§è¯„ä»·æŒ‡æ ‡ä¸Šçš„æ€§èƒ½ç»¼åˆä¸ºä¸€ä¸ªå•ä¸€çš„å½’ä¸€åŒ–åˆ†æ•°ï¼Œä¸ºæ›´å¯é å’Œå¯è§£é‡Šçš„è¯„ä»·æ¡†æ¶æä¾›äº†æ”¯æŒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEDASèƒ½æœ‰æ•ˆæ•´åˆå¤šæŒ‡æ ‡ã€å¤šæ•°æ®é›†çš„æ€§èƒ½ï¼Œä¸ºè¯„ä»·KGCæ¨¡å‹æä¾›äº†ä¸€ä¸ªä¸€è‡´ã€ç¨³å¥å’Œé€šç”¨çš„æ¡†æ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>çŸ¥è¯†å›¾è°±ï¼ˆKGsï¼‰åœ¨å¤šä¸ªé¢†åŸŸæœ‰å¹¿æ³›åº”ç”¨ï¼Œä½†å­˜åœ¨ä¸å®Œæ•´æ€§é—®é¢˜ï¼Œéœ€è¦çŸ¥è¯†å›¾è°±è¡¥å…¨ï¼ˆKGCï¼‰æ–¹æ³•é¢„æµ‹ç¼ºå¤±å…ƒç´ ã€‚</li>
<li>è¯„ä¼°KGCæ¨¡å‹çš„æ€§èƒ½é€šå¸¸ä½¿ç”¨å¤šä¸ªæ•°æ®é›†å’Œè¯„ä»·æŒ‡æ ‡ï¼Œä½†æ¯”è¾ƒè¿™äº›æ€§èƒ½æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚</li>
<li>å­˜åœ¨å¤šç§è¯„ä»·KGCæ¨¡å‹çš„æŒ‡æ ‡ï¼Œä½†ä¸åŒæŒ‡æ ‡å¯èƒ½å¯¼è‡´å†²çªçš„ç»“æœã€‚</li>
<li>EDASæ˜¯ä¸€ç§æ–°çš„è¯„ä¼°æ–¹æ³•ï¼Œèƒ½å°†æ¨¡å‹åœ¨å¤šä¸ªæ•°æ®é›†å’Œå¤šç§è¯„ä»·æŒ‡æ ‡ä¸Šçš„æ€§èƒ½ç»¼åˆä¸ºä¸€ä¸ªå•ä¸€çš„å½’ä¸€åŒ–åˆ†æ•°ã€‚</li>
<li>EDASæä¾›äº†ä¸€ä¸ªå…¨å±€è§†è§’ï¼Œæ”¯æŒæ›´æ˜æ™ºçš„æ¨¡å‹é€‰æ‹©å’Œè·¨æ•°æ®é›†çš„å…¬å¹³è¯„ä»·ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒEDASèƒ½æœ‰æ•ˆæ•´åˆå¤šæŒ‡æ ‡å’Œå¤šæ•°æ®é›†çš„æ€§èƒ½è¯„ä»·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15357">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-014ce7980990530133c10a8843ec78d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-319a530a73ee91d3f3abce7872cacd29.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f73f9e77b982d241fa3eae9edc29cd1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-884e0039e8c5e2a7956a94f3686552d3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8af0259e3a2558c3b0462aaef2ff76fb.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="REG4Rec-Reasoning-Enhanced-Generative-Model-for-Large-Scale-Recommendation-Systems"><a href="#REG4Rec-Reasoning-Enhanced-Generative-Model-for-Large-Scale-Recommendation-Systems" class="headerlink" title="REG4Rec: Reasoning-Enhanced Generative Model for Large-Scale   Recommendation Systems"></a>REG4Rec: Reasoning-Enhanced Generative Model for Large-Scale   Recommendation Systems</h2><p><strong>Authors:Haibo Xing, Hao Deng, Yucheng Mao, Jinxin Hu, Yi Xu, Hao Zhang, Jiahao Wang, Shizhun Wang, Yu Zhang, Xiaoyi Zeng, Jing Zhang</strong></p>
<p>Sequential recommendation aims to predict a userâ€™s next action in large-scale recommender systems. While traditional methods often suffer from insufficient information interaction, recent generative recommendation models partially address this issue by directly generating item predictions. To better capture user intents, recent studies have introduced a reasoning process into generative recommendation, significantly improving recommendation performance. However, these approaches are constrained by the singularity of item semantic representations, facing challenges such as limited diversity in reasoning pathways and insufficient reliability in the reasoning process. To tackle these issues, we introduce REG4Rec, a reasoning-enhanced generative model that constructs multiple dynamic semantic reasoning paths alongside a self-reflection process, ensuring high-confidence recommendations. Specifically, REG4Rec utilizes an MoE-based parallel quantization codebook (MPQ) to generate multiple unordered semantic tokens for each item, thereby constructing a larger-scale diverse reasoning space. Furthermore, to enhance the reliability of reasoning, we propose a training reasoning enhancement stage, which includes Preference Alignment for Reasoning (PARS) and a Multi-Step Reward Augmentation (MSRA) strategy. PARS uses reward functions tailored for recommendation to enhance reasoning and reflection, while MSRA introduces future multi-step actions to improve overall generalization. During inference, Consistency-Oriented Self-Reflection for Pruning (CORP) is proposed to discard inconsistent reasoning paths, preventing the propagation of erroneous reasoning. Lastly, we develop an efficient offline training strategy for large-scale recommendation. Experiments on real-world datasets and online evaluations show that REG4Rec delivers outstanding performance and substantial practical value. </p>
<blockquote>
<p>åºåˆ—æ¨èæ—¨åœ¨åœ¨å¤§è§„æ¨¡æ¨èç³»ç»Ÿä¸­é¢„æµ‹ç”¨æˆ·çš„ä¸‹ä¸€ä¸ªè¡Œä¸ºã€‚è™½ç„¶ä¼ ç»Ÿæ–¹æ³•å¸¸å¸¸å› ä¿¡æ¯äº¤äº’ä¸è¶³è€Œå—é™ï¼Œä½†æœ€è¿‘çš„ç”Ÿæˆæ¨èæ¨¡å‹é€šè¿‡ç›´æ¥ç”Ÿæˆé¡¹ç›®é¢„æµ‹éƒ¨åˆ†åœ°è§£å†³äº†è¿™ä¸ªé—®é¢˜ã€‚ä¸ºäº†æ›´å¥½åœ°æ•æ‰ç”¨æˆ·æ„å›¾ï¼Œæœ€è¿‘çš„ç ”ç©¶å°†æ¨ç†è¿‡ç¨‹å¼•å…¥ç”Ÿæˆæ¨èï¼Œæ˜¾è‘—æé«˜äº†æ¨èæ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å—åˆ°é¡¹ç›®è¯­ä¹‰è¡¨ç¤ºå•ä¸€æ€§çš„é™åˆ¶ï¼Œé¢ä¸´æ¨ç†è·¯å¾„å¤šæ ·æ€§æœ‰é™å’Œæ¨ç†è¿‡ç¨‹å¯é æ€§ä¸è¶³ç­‰æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†REG4Recï¼Œè¿™æ˜¯ä¸€ç§å¢å¼ºæ¨ç†çš„ç”Ÿæˆæ¨¡å‹ï¼Œå®ƒæ„å»ºäº†å¤šä¸ªåŠ¨æ€è¯­ä¹‰æ¨ç†è·¯å¾„ï¼Œå¹¶ä¼´éšç€ä¸€ä¸ªè‡ªæˆ‘åæ€è¿‡ç¨‹ï¼Œç¡®ä¿é«˜ç½®ä¿¡åº¦çš„æ¨èã€‚å…·ä½“æ¥è¯´ï¼ŒREG4Recä½¿ç”¨ä¸€ä¸ªåŸºäºMoEçš„å¹¶è¡Œé‡åŒ–ä»£ç æœ¬ï¼ˆMPQï¼‰æ¥ä¸ºæ¯ä¸ªé¡¹ç›®ç”Ÿæˆå¤šä¸ªæ— åºè¯­ä¹‰æ ‡è®°ï¼Œä»è€Œæ„å»ºä¸€ä¸ªæ›´å¤§è§„æ¨¡çš„å¤šæ ·åŒ–æ¨ç†ç©ºé—´ã€‚æ­¤å¤–ï¼Œä¸ºäº†æé«˜æ¨ç†çš„å¯é æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è®­ç»ƒæ¨ç†å¢å¼ºé˜¶æ®µï¼ŒåŒ…æ‹¬ç”¨äºæ¨ç†çš„åå¥½å¯¹é½ï¼ˆPARSï¼‰å’Œå¤šæ­¥å¥–åŠ±å¢å¼ºï¼ˆMSRAï¼‰ç­–ç•¥ã€‚PARSä½¿ç”¨é’ˆå¯¹æ¨èçš„å¥–åŠ±å‡½æ•°æ¥å¢å¼ºæ¨ç†å’Œåæ€ï¼Œè€ŒMSRAé€šè¿‡å¼•å…¥æœªæ¥çš„å¤šæ­¥è¡ŒåŠ¨æ¥æé«˜æ•´ä½“æ³›åŒ–èƒ½åŠ›ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†é¢å‘ä¸€è‡´æ€§çš„è‡ªæˆ‘åæ€ä¿®å‰ªï¼ˆCORPï¼‰æ¥ä¸¢å¼ƒä¸ä¸€è‡´çš„æ¨ç†è·¯å¾„ï¼Œé˜²æ­¢é”™è¯¯æ¨ç†çš„ä¼ æ’­ã€‚æœ€åï¼Œæˆ‘ä»¬ä¸ºå¤§è§„æ¨¡æ¨èå¼€å‘äº†ä¸€ç§é«˜æ•ˆçš„ç¦»çº¿è®­ç»ƒç­–ç•¥ã€‚åœ¨ç°å®æ•°æ®é›†ä¸Šçš„å®éªŒå’Œåœ¨çº¿è¯„ä¼°è¡¨æ˜ï¼ŒREG4Recè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½å’Œå·¨å¤§çš„å®ç”¨ä»·å€¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15308v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>æ–°ä¸€ä»£æ¨èç³»ç»ŸREG4Recç»“åˆç”Ÿæˆå¼æ¨èä¸åŠ¨æ€è¯­ä¹‰æ¨ç†è·¯å¾„ï¼Œä»¥æé«˜ç”¨æˆ·æ„å›¾æ•æ‰å’Œæ¨èæ€§èƒ½ã€‚é€šè¿‡æ„å»ºå¤šä¸ªåŠ¨æ€è¯­ä¹‰æ¨ç†è·¯å¾„å’Œè‡ªåæ€è¿‡ç¨‹ï¼Œè§£å†³å•ä¸€è¯­ä¹‰è¡¨ç¤ºå¸¦æ¥çš„æŒ‘æˆ˜ã€‚REG4Recä½¿ç”¨åŸºäºMoEçš„å¹¶è¡Œé‡åŒ–ä»£ç æœ¬ç”Ÿæˆå¤šä¸ªæ— åºè¯­ä¹‰ä»¤ç‰Œï¼Œæ„å»ºå¤§è§„æ¨¡å¤šæ ·åŒ–æ¨ç†ç©ºé—´ã€‚åŒæ—¶ï¼Œé€šè¿‡è®­ç»ƒå¢å¼ºæ¨ç†é˜¶æ®µå’Œä¸€è‡´æ€§å¯¼å‘çš„è‡ªåæ€ç­–ç•¥ï¼Œæé«˜æ¨ç†çš„å¯é æ€§å’Œå‡†ç¡®æ€§ã€‚å®éªŒè¯æ˜ï¼ŒREG4Recåœ¨çœŸå®æ•°æ®é›†å’Œåœ¨çº¿è¯„ä¼°ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>REG4Recç»“åˆç”Ÿæˆå¼æ¨èä¸åŠ¨æ€è¯­ä¹‰æ¨ç†ï¼Œæ—¨åœ¨æé«˜æ•æ‰ç”¨æˆ·æ„å›¾å’Œæ¨èæ€§èƒ½ã€‚</li>
<li>é€šè¿‡æ„å»ºå¤šä¸ªåŠ¨æ€è¯­ä¹‰æ¨ç†è·¯å¾„ï¼Œè§£å†³å•ä¸€è¯­ä¹‰è¡¨ç¤ºçš„é™åˆ¶å’ŒæŒ‘æˆ˜ã€‚</li>
<li>ä½¿ç”¨åŸºäºMoEçš„å¹¶è¡Œé‡åŒ–ä»£ç æœ¬ç”Ÿæˆå¤šä¸ªæ— åºè¯­ä¹‰ä»¤ç‰Œï¼Œåˆ›å»ºå¤§è§„æ¨¡å¤šæ ·åŒ–æ¨ç†ç©ºé—´ã€‚</li>
<li>é€šè¿‡è®­ç»ƒå¢å¼ºæ¨ç†é˜¶æ®µï¼ŒåŒ…æ‹¬åå¥½å¯¹é½æ¨ç†å’Œå¥–åŠ±å¢å¼ºç­–ç•¥ï¼Œæé«˜æ¨ç†å¯é æ€§ã€‚</li>
<li>åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¼•å…¥ä¸€è‡´æ€§å¯¼å‘çš„è‡ªåæ€ç­–ç•¥ï¼Œé¿å…é”™è¯¯æ¨ç†çš„ä¼ æ’­ã€‚</li>
<li>REG4Recé‡‡ç”¨é«˜æ•ˆç¦»çº¿è®­ç»ƒç­–ç•¥ï¼Œé€‚ç”¨äºå¤§è§„æ¨¡æ¨èåœºæ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15308">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-baf9de2ee41d8c3088aba597445b7d06.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1cb3462ddf8590da77885761b5f35781.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56a5150b27ae9bdab8a06f3adc518f91.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="RATopo-Improving-Lane-Topology-Reasoning-via-Redundancy-Assignment"><a href="#RATopo-Improving-Lane-Topology-Reasoning-via-Redundancy-Assignment" class="headerlink" title="RATopo: Improving Lane Topology Reasoning via Redundancy Assignment"></a>RATopo: Improving Lane Topology Reasoning via Redundancy Assignment</h2><p><strong>Authors:Han Li, Shaofei Huang, Longfei Xu, Yulu Gao, Beipeng Mu, Si Liu</strong></p>
<p>Lane topology reasoning plays a critical role in autonomous driving by modeling the connections among lanes and the topological relationships between lanes and traffic elements. Most existing methods adopt a first-detect-then-reason paradigm, where topological relationships are supervised based on the one-to-one assignment results obtained during the detection stage. This supervision strategy results in suboptimal topology reasoning performance due to the limited range of valid supervision. In this paper, we propose RATopo, a Redundancy Assignment strategy for lane Topology reasoning that enables quantity-rich and geometry-diverse topology supervision. Specifically, we restructure the Transformer decoder by swapping the cross-attention and self-attention layers. This allows redundant lane predictions to be retained before suppression, enabling effective one-to-many assignment. We also instantiate multiple parallel cross-attention blocks with independent parameters, which further enhances the diversity of detected lanes. Extensive experiments on OpenLane-V2 demonstrate that our RATopo strategy is model-agnostic and can be seamlessly integrated into existing topology reasoning frameworks, consistently improving both lane-lane and lane-traffic topology performance. </p>
<blockquote>
<p>è½¦é“æ‹“æ‰‘æ¨ç†åœ¨è‡ªåŠ¨é©¾é©¶ä¸­æ‰®æ¼”ç€é‡è¦è§’è‰²ï¼Œå®ƒé€šè¿‡å»ºæ¨¡è½¦é“ä¹‹é—´çš„è¿æ¥ä»¥åŠè½¦é“ä¸äº¤é€šå…ƒç´ ä¹‹é—´çš„æ‹“æ‰‘å…³ç³»æ¥å®ç°ã€‚ç°æœ‰å¤§å¤šæ•°æ–¹æ³•é‡‡ç”¨å…ˆæ£€æµ‹å†æ¨ç†çš„æ¨¡å¼ï¼Œå…¶ä¸­æ‹“æ‰‘å…³ç³»æ˜¯åŸºäºæ£€æµ‹é˜¶æ®µè·å¾—çš„ä¸€ä¸€å¯¹åº”å…³ç³»ç»“æœè¿›è¡Œç›‘ç£çš„ã€‚è¿™ç§ç›‘ç£ç­–ç•¥ç”±äºæœ‰æ•ˆçš„ç›‘ç£èŒƒå›´æœ‰é™ï¼Œå¯¼è‡´æ‹“æ‰‘æ¨ç†æ€§èƒ½ä¸ä½³ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†RATopoï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºè½¦é“æ‹“æ‰‘æ¨ç†çš„å†—ä½™åˆ†é…ç­–ç•¥ï¼Œèƒ½å¤Ÿå®ç°ä¸°å¯Œæ•°é‡å’Œå¤šæ ·å‡ ä½•å½¢çŠ¶çš„æ‹“æ‰‘ç›‘ç£ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é€šè¿‡äº¤æ¢Transformerè§£ç å™¨çš„äº¤å‰æ³¨æ„åŠ›å’Œè‡ªæ³¨æ„åŠ›å±‚æ¥é‡å»ºå…¶ç»“æ„ã€‚è¿™å…è®¸åœ¨æŠ‘åˆ¶ä¹‹å‰ä¿ç•™å†—ä½™çš„è½¦é“é¢„æµ‹ï¼Œä»è€Œå®ç°æœ‰æ•ˆçš„ä¸€å¯¹å¤šåˆ†é…ã€‚æˆ‘ä»¬è¿˜å®ä¾‹åŒ–äº†å…·æœ‰ç‹¬ç«‹å‚æ•°çš„å¤šä¸ªå¹¶è¡Œäº¤å‰æ³¨æ„åŠ›å—ï¼Œè¿™è¿›ä¸€æ­¥å¢å¼ºäº†æ£€æµ‹åˆ°çš„è½¦é“çš„å¤šæ ·æ€§ã€‚åœ¨OpenLane-V2ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„RATopoç­–ç•¥ä¸æ¨¡å‹æ— å…³ï¼Œå¯ä»¥æ— ç¼é›†æˆåˆ°ç°æœ‰çš„æ‹“æ‰‘æ¨ç†æ¡†æ¶ä¸­ï¼ŒæŒç»­æé«˜è½¦é“ä¸è½¦é“ä»¥åŠè½¦é“ä¸äº¤é€šçš„æ‹“æ‰‘æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15272v1">PDF</a> Accepted by ACM MM 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>è½¦é“æ‹“æ‰‘æ¨ç†åœ¨è‡ªåŠ¨é©¾é©¶ä¸­èµ·åˆ°å…³é”®ä½œç”¨ï¼Œé€šè¿‡å¯¹è½¦é“é—´è¿æ¥ä»¥åŠè½¦é“ä¸äº¤é€šå…ƒç´ é—´çš„æ‹“æ‰‘å…³ç³»è¿›è¡Œå»ºæ¨¡æ¥å®ç°ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸é‡‡ç”¨å…ˆæ£€æµ‹åæ¨ç†çš„æ¨¡å¼ï¼ŒåŸºäºæ£€æµ‹é˜¶æ®µè·å¾—çš„ä¸€ä¸€å¯¹åº”å…³ç³»ç»“æœæ¥ç›‘ç£æ‹“æ‰‘å…³ç³»ã€‚è¿™ç§ç›‘ç£ç­–ç•¥å› æœ‰æ•ˆç›‘ç£èŒƒå›´æœ‰é™è€Œå¯¼è‡´æ‹“æ‰‘æ¨ç†æ€§èƒ½ä¸ä½³ã€‚æœ¬æ–‡æå‡ºRATopoï¼Œä¸€ç§è½¦é“æ‹“æ‰‘æ¨ç†ä¸­çš„å†—ä½™åˆ†é…ç­–ç•¥ï¼Œå®ç°ä¸°å¯Œæ•°é‡å’Œå‡ ä½•å¤šæ ·çš„æ‹“æ‰‘ç›‘ç£ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é‡æ„Transformerè§£ç å™¨ï¼Œé€šè¿‡äº¤æ¢äº¤å‰æ³¨æ„åŠ›å’Œè‡ªæ³¨æ„åŠ›å±‚æ¥ä¿ç•™å†—ä½™è½¦é“é¢„æµ‹ï¼Œå®ç°æœ‰æ•ˆçš„ä¸€å¯¹å¤šåˆ†é…ã€‚æˆ‘ä»¬è¿˜å®ä¾‹åŒ–å…·æœ‰ç‹¬ç«‹å‚æ•°çš„å¤šä¸ªå¹¶è¡Œäº¤å‰æ³¨æ„åŠ›å—ï¼Œè¿›ä¸€æ­¥æé«˜æ£€æµ‹åˆ°çš„è½¦é“çš„å¤šæ ·æ€§ã€‚åœ¨OpenLane-V2ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„RATopoç­–ç•¥å…·æœ‰æ¨¡å‹æ— å…³æ€§ï¼Œå¯ä»¥æ— ç¼é›†æˆåˆ°ç°æœ‰çš„æ‹“æ‰‘æ¨ç†æ¡†æ¶ä¸­ï¼ŒæŒç»­æé«˜è½¦é“ä¸è½¦é“ã€è½¦é“ä¸äº¤é€šçš„æ‹“æ‰‘æ€§èƒ½ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è½¦é“æ‹“æ‰‘æ¨ç†åœ¨è‡ªåŠ¨é©¾é©¶ä¸­æ‰®æ¼”å…³é”®è§’è‰²ï¼Œé€šè¿‡å¯¹è½¦é“é—´ä»¥åŠè½¦é“ä¸äº¤é€šå…ƒç´ çš„æ‹“æ‰‘å…³ç³»è¿›è¡Œå»ºæ¨¡æ¥å®ç°ã€‚</li>
<li>ç°æœ‰æ–¹æ³•çš„ç›‘ç£ç­–ç•¥å› æœ‰æ•ˆç›‘ç£èŒƒå›´æœ‰é™å¯¼è‡´æ‹“æ‰‘æ¨ç†æ€§èƒ½ä¸ä½³ã€‚</li>
<li>RATopoç­–ç•¥é‡‡ç”¨å†—ä½™åˆ†é…ï¼Œå®ç°ä¸°å¯Œæ•°é‡å’Œå‡ ä½•å¤šæ ·çš„æ‹“æ‰‘ç›‘ç£ã€‚</li>
<li>é‡æ„Transformerè§£ç å™¨ï¼Œé€šè¿‡äº¤æ¢äº¤å‰æ³¨æ„åŠ›å’Œè‡ªæ³¨æ„åŠ›å±‚æ¥ä¿ç•™å†—ä½™è½¦é“é¢„æµ‹ã€‚</li>
<li>å®ä¾‹åŒ–å¤šä¸ªå¹¶è¡Œäº¤å‰æ³¨æ„åŠ›å—ï¼Œæé«˜æ£€æµ‹åˆ°çš„è½¦é“çš„å¤šæ ·æ€§ã€‚</li>
<li>RATopoç­–ç•¥å¯ä»¥æ— ç¼é›†æˆåˆ°ç°æœ‰æ‹“æ‰‘æ¨ç†æ¡†æ¶ä¸­ã€‚</li>
<li>RATopoç­–ç•¥èƒ½æé«˜è½¦é“ä¸è½¦é“ã€è½¦é“ä¸äº¤é€šçš„æ‹“æ‰‘æ¨ç†æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15272">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-15315cf3fb40c91f73502c9c02db460a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3501fd3de915b77d62db3ee073d0553d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8af89a55d628e386b5a85c617e43cef0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Select-to-Know-An-Internal-External-Knowledge-Self-Selection-Framework-for-Domain-Specific-Question-Answering"><a href="#Select-to-Know-An-Internal-External-Knowledge-Self-Selection-Framework-for-Domain-Specific-Question-Answering" class="headerlink" title="Select to Know: An Internal-External Knowledge Self-Selection Framework   for Domain-Specific Question Answering"></a>Select to Know: An Internal-External Knowledge Self-Selection Framework   for Domain-Specific Question Answering</h2><p><strong>Authors:Bolei He, Xinran He, Run Shao, Shanfu Shu, Xianwei Xue, Mingquan Cheng, Haifeng Li, Zhenhua Ling</strong></p>
<p>Large Language Models (LLMs) perform well in general QA but often struggle in domain-specific scenarios. Retrieval-Augmented Generation (RAG) introduces external knowledge but suffers from hallucinations and latency due to noisy retrievals. Continued pretraining internalizes domain knowledge but is costly and lacks cross-domain flexibility. We attribute this challenge to the long-tail distribution of domain knowledge, which leaves partial yet useful internal knowledge underutilized. We further argue that knowledge acquisition should be progressive, mirroring human learning: first understanding concepts, then applying them to complex reasoning. To address this, we propose Selct2Know (S2K), a cost-effective framework that internalizes domain knowledge through an internal-external knowledge self-selection strategy and selective supervised fine-tuning. We also introduce a structured reasoning data generation pipeline and integrate GRPO to enhance reasoning ability. Experiments on medical, legal, and financial QA benchmarks show that S2K consistently outperforms existing methods and matches domain-pretrained LLMs with significantly lower cost. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¸€èˆ¬é—®ç­”ä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨ç‰¹å®šé¢†åŸŸçš„åœºæ™¯ä¸­ç»å¸¸é¢ä¸´æŒ‘æˆ˜ã€‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å¼•å…¥äº†å¤–éƒ¨çŸ¥è¯†ï¼Œä½†ç”±äºæ£€ç´¢ç»“æœä¸­çš„å™ªå£°è€Œå®¹æ˜“å‡ºç°è™šæ„å’Œå»¶è¿Ÿã€‚æŒç»­é¢„è®­ç»ƒå¯ä»¥å†…åŒ–é¢†åŸŸçŸ¥è¯†ï¼Œä½†æˆæœ¬é«˜æ˜‚ä¸”ç¼ºä¹è·¨é¢†åŸŸçµæ´»æ€§ã€‚æˆ‘ä»¬å°†è¿™ä¸€æŒ‘æˆ˜å½’å› äºé¢†åŸŸçŸ¥è¯†çš„é•¿å°¾åˆ†å¸ƒï¼Œè¿™ä½¿å¾—éƒ¨åˆ†æœ‰ç”¨çš„å†…éƒ¨çŸ¥è¯†æœªå¾—åˆ°å……åˆ†åˆ©ç”¨ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è®¤ä¸ºï¼ŒçŸ¥è¯†è·å–åº”è¯¥æ˜¯ä¸€ä¸ªæ¸è¿›çš„è¿‡ç¨‹ï¼Œåæ˜ äººç±»å­¦ä¹ çš„æ–¹å¼ï¼šé¦–å…ˆç†è§£æ¦‚å¿µï¼Œç„¶åå°†å…¶åº”ç”¨äºå¤æ‚æ¨ç†ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Selct2Knowï¼ˆS2Kï¼‰è¿™ä¸€æˆæœ¬æ•ˆç›Šé«˜çš„æ¡†æ¶ï¼Œå®ƒé€šè¿‡å†…éƒ¨å’Œå¤–éƒ¨çŸ¥è¯†çš„è‡ªæˆ‘é€‰æ‹©ç­–ç•¥ä»¥åŠé€‰æ‹©æ€§ç›‘ç£å¾®è°ƒæ¥å†…åŒ–é¢†åŸŸçŸ¥è¯†ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªç»“æ„åŒ–æ¨ç†æ•°æ®ç”Ÿæˆç®¡é“ï¼Œå¹¶æ•´åˆäº†GRPOæŠ€æœ¯ä»¥æé«˜æ¨ç†èƒ½åŠ›ã€‚åœ¨åŒ»ç–—ã€æ³•å¾‹å’Œè´¢åŠ¡é—®ç­”åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒS2Kå§‹ç»ˆä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶ä»¥æ›´ä½çš„æˆæœ¬ä¸é¢†åŸŸé¢„è®­ç»ƒLLMç›¸åŒ¹é…ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15213v1">PDF</a> EMNLP2025 Findings</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¸€èˆ¬é—®ç­”ä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨ç‰¹å®šé¢†åŸŸåœºæ™¯ä¸­å¸¸é‡åˆ°å›°éš¾ã€‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å¼•å…¥å¤–éƒ¨çŸ¥è¯†ï¼Œä½†å­˜åœ¨å¹»è±¡å’Œå»¶è¿Ÿé—®é¢˜ã€‚ç»§ç»­é¢„è®­ç»ƒå¯ä»¥å†…åŒ–é¢†åŸŸçŸ¥è¯†ï¼Œä½†æˆæœ¬é«˜æ˜‚ä¸”ç¼ºä¹è·¨é¢†åŸŸçµæ´»æ€§ã€‚æœ¬æ–‡è®¤ä¸ºè¿™æ˜¯ç”±äºé¢†åŸŸçŸ¥è¯†çš„é•¿å°¾åˆ†å¸ƒå¯¼è‡´éƒ¨åˆ†å†…éƒ¨çŸ¥è¯†æœªè¢«å……åˆ†åˆ©ç”¨ã€‚çŸ¥è¯†è·å–åº”å¾ªåºæ¸è¿›ï¼Œå…ˆç†è§£æ¦‚å¿µï¼Œå†åº”ç”¨äºå¤æ‚æ¨ç†ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºSelct2Knowï¼ˆS2Kï¼‰æ¡†æ¶ï¼Œé€šè¿‡å†…å¤–éƒ¨çŸ¥è¯†è‡ªé€‰ç­–ç•¥å’Œé€‰æ‹©æ€§ç›‘ç£å¾®è°ƒæ¥å†…åŒ–é¢†åŸŸçŸ¥è¯†ã€‚åŒæ—¶å¼•å…¥ç»“æ„åŒ–æ¨ç†æ•°æ®ç”Ÿæˆç®¡é“å¹¶ç»“åˆGRPOå¢å¼ºæ¨ç†èƒ½åŠ›ã€‚åœ¨åŒ»ç–—ã€æ³•å¾‹å’Œè´¢åŠ¡é—®ç­”åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒS2KæŒç»­ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶ä»¥è¾ƒä½æˆæœ¬åŒ¹é…é¢†åŸŸé¢„è®­ç»ƒLLMã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsåœ¨ç‰¹å®šé¢†åŸŸåœºæ™¯ä¸­é¢ä¸´æŒ‘æˆ˜ï¼Œéœ€è¦æ›´æœ‰æ•ˆçš„æ–¹å¼å†…åŒ–é¢†åŸŸçŸ¥è¯†ã€‚</li>
<li>RAGè™½å¼•å…¥å¤–éƒ¨çŸ¥è¯†ï¼Œä½†å­˜åœ¨å¹»è±¡å’Œå»¶è¿Ÿé—®é¢˜ã€‚</li>
<li>ç»§ç»­é¢„è®­ç»ƒè™½ç„¶èƒ½å†…åŒ–é¢†åŸŸçŸ¥è¯†ï¼Œä½†æˆæœ¬é«˜æ˜‚ä¸”ç¼ºä¹è·¨é¢†åŸŸçµæ´»æ€§ã€‚</li>
<li>é¢†åŸŸçŸ¥è¯†çš„é•¿å°¾åˆ†å¸ƒå¯¼è‡´éƒ¨åˆ†å†…éƒ¨çŸ¥è¯†æœªè¢«å……åˆ†åˆ©ç”¨ã€‚</li>
<li>çŸ¥è¯†è·å–åº”å¾ªåºæ¸è¿›ï¼Œå…ˆç†è§£æ¦‚å¿µå†åº”ç”¨å¤æ‚æ¨ç†ã€‚</li>
<li>S2Kæ¡†æ¶é€šè¿‡å†…å¤–éƒ¨çŸ¥è¯†è‡ªé€‰ç­–ç•¥å’Œé€‰æ‹©æ€§ç›‘ç£å¾®è°ƒæ¥å†…åŒ–é¢†åŸŸçŸ¥è¯†ï¼Œæé«˜æ€§èƒ½å¹¶é™ä½æˆæœ¬ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15213">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-611a6567f3035bdafd56becc4ff290a0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6cb895834f25af6d89f3227364dc23bc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1b978f4f7fcd1a5114a27b65fb25a8e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d84a6621529e377ab1a47ca8ee38261.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-86d3467265a4508ef430aa4a135cb494.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f87c14da69d4fa2ccaa40990678153e.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Fin-PRM-A-Domain-Specialized-Process-Reward-Model-for-Financial-Reasoning-in-Large-Language-Models"><a href="#Fin-PRM-A-Domain-Specialized-Process-Reward-Model-for-Financial-Reasoning-in-Large-Language-Models" class="headerlink" title="Fin-PRM: A Domain-Specialized Process Reward Model for Financial   Reasoning in Large Language Models"></a>Fin-PRM: A Domain-Specialized Process Reward Model for Financial   Reasoning in Large Language Models</h2><p><strong>Authors:Yuanchen Zhou, Shuo Jiang, Jie Zhu, Junhui Li, Lifan Guo, Feng Chen, Chi Zhang</strong></p>
<p>Process Reward Models (PRMs) have emerged as a promising framework for supervising intermediate reasoning in large language models (LLMs), yet existing PRMs are primarily trained on general or Science, Technology, Engineering, and Mathematics (STEM) domains and fall short in domain-specific contexts such as finance, where reasoning is more structured, symbolic, and sensitive to factual and regulatory correctness. We introduce \textbf{Fin-PRM}, a domain-specialized, trajectory-aware PRM tailored to evaluate intermediate reasoning steps in financial tasks. Fin-PRM integrates step-level and trajectory-level reward supervision, enabling fine-grained evaluation of reasoning traces aligned with financial logic. We apply Fin-PRM in both offline and online reward learning settings, supporting three key applications: (i) selecting high-quality reasoning trajectories for distillation-based supervised fine-tuning, (ii) providing dense process-level rewards for reinforcement learning, and (iii) guiding reward-informed Best-of-N inference at test time. Experimental results on financial reasoning benchmarks, including CFLUE and FinQA, demonstrate that Fin-PRM consistently outperforms general-purpose PRMs and strong domain baselines in trajectory selection quality. Downstream models trained with Fin-PRM yield substantial improvements with baselines, with gains of 12.9% in supervised learning, 5.2% in reinforcement learning, and 5.1% in test-time performance. These findings highlight the value of domain-specialized reward modeling for aligning LLMs with expert-level financial reasoning. Our project resources will be available at <a target="_blank" rel="noopener" href="https://github.com/aliyun/qwen-dianjin">https://github.com/aliyun/qwen-dianjin</a>. </p>
<blockquote>
<p>è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMsï¼‰å·²æˆä¸ºç›‘ç£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„ä¸­é—´æ¨ç†çš„æœ‰å‰é€”çš„æ¡†æ¶ï¼Œä½†ç°æœ‰çš„PRMä¸»è¦è®­ç»ƒäºé€šç”¨æˆ–ç§‘å­¦ã€æŠ€æœ¯ã€å·¥ç¨‹å’Œæ•°å­¦ï¼ˆSTEMï¼‰é¢†åŸŸï¼Œè€Œåœ¨é‡‘èç­‰ç‰¹å®šé¢†åŸŸçš„ä¸Šä¸‹æ–‡ä¸­è¡¨ç°ä¸è¶³ï¼Œè¿™é‡Œçš„æ¨ç†æ›´åŠ ç»“æ„åŒ–ã€ç¬¦å·åŒ–ï¼Œå¹¶ä¸”å¯¹äº‹å®å’Œæ³•è§„çš„æ­£ç¡®æ€§æ›´åŠ æ•æ„Ÿã€‚æˆ‘ä»¬å¼•å…¥äº†<strong>Fin-PRM</strong>ï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹é‡‘èä»»åŠ¡ä¸­çš„ä¸­é—´æ¨ç†æ­¥éª¤è¿›è¡Œè¯„ä¼°çš„åŸŸä¸“ä¸šåŒ–ã€è½¨è¿¹æ„ŸçŸ¥çš„PRMã€‚Fin-PRMé›†æˆäº†æ­¥éª¤çº§å’Œè½¨è¿¹çº§çš„å¥–åŠ±ç›‘ç£ï¼Œèƒ½å¤Ÿå®ç°ä¸é‡‘èé€»è¾‘å¯¹é½çš„æ¨ç†è½¨è¿¹çš„ç²¾ç»†è¯„ä¼°ã€‚æˆ‘ä»¬åœ¨ç¦»çº¿å¥–åŠ±å­¦ä¹ å’Œåœ¨çº¿å¥–åŠ±å­¦ä¹ ç¯å¢ƒä¸­éƒ½åº”ç”¨äº†Fin-PRMï¼Œæ”¯æŒä¸‰ä¸ªå…³é”®åº”ç”¨ï¼šï¼ˆiï¼‰é€‰æ‹©åŸºäºè’¸é¦çš„æœ‰ç›‘ç£å¾®è°ƒçš„é«˜è´¨é‡æ¨ç†è½¨è¿¹ï¼Œï¼ˆiiï¼‰ä¸ºå¼ºåŒ–å­¦ä¹ æä¾›å¯†é›†çš„è¿‡ç¨‹çº§å¥–åŠ±ï¼Œï¼ˆiiiï¼‰åœ¨æµ‹è¯•æ—¶æŒ‡å¯¼åŸºäºå¥–åŠ±çš„Best-of-Næ¨ç†ã€‚åœ¨é‡‘èæ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœï¼ŒåŒ…æ‹¬CFLUEå’ŒFinQAï¼Œè¯æ˜äº†Fin-PRMåœ¨è½¨è¿¹é€‰æ‹©è´¨é‡ä¸Šå§‹ç»ˆä¼˜äºé€šç”¨PRMå’Œå¼ºåŸºçº¿ã€‚ä½¿ç”¨Fin-PRMè®­ç»ƒçš„ä¸‹æ¸¸æ¨¡å‹ä¸åŸºçº¿ç›¸æ¯”å®ç°äº†æ˜¾è‘—æ”¹è¿›ï¼Œç›‘ç£å­¦ä¹ æé«˜äº†12.9%ï¼Œå¼ºåŒ–å­¦ä¹ æé«˜äº†5.2%ï¼Œæµ‹è¯•æ—¶æ€§èƒ½æé«˜äº†5.1%ã€‚è¿™äº›å‘ç°å‡¸æ˜¾äº†é¢†åŸŸä¸“ä¸šåŒ–å¥–åŠ±æ¨¡å‹å¯¹äºå°†å¤§å‹è¯­è¨€æ¨¡å‹ä¸ä¸“å®¶çº§é‡‘èæ¨ç†å¯¹é½çš„ä»·å€¼ã€‚æˆ‘ä»¬çš„é¡¹ç›®èµ„æºå°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/aliyun/qwen-dianjin">https://github.com/aliyun/qwen-dianjin</a>ä¸Šæä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15202v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹é‡‘èé¢†åŸŸç‰¹æ®Šéœ€æ±‚çš„è¿›ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆFin-PRMï¼‰ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨é‡‘èä»»åŠ¡çš„æ¨ç†è¿‡ç¨‹ä¸­è¿›è¡Œç²¾ç»†åŒ–è¯„ä¼°ï¼Œç»“åˆäº†æ­¥éª¤çº§å’Œè½¨è¿¹çº§çš„å¥–åŠ±ç›‘ç£ï¼Œç¬¦åˆé‡‘èé€»è¾‘ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFin-PRMåœ¨è´¢åŠ¡æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜äºé€šç”¨è¿›ç¨‹å¥–åŠ±æ¨¡å‹å’Œå¼ºåŸºå‡†çº¿ï¼Œèƒ½æé«˜ä¸‹æ¸¸æ¨¡å‹çš„è½¨è¿¹é€‰æ‹©è´¨é‡ã€‚é€šè¿‡Fin-PRMï¼Œä¸‹æ¸¸æ¨¡å‹çš„æ€§èƒ½å¾—åˆ°äº†æ˜¾è‘—æé«˜ï¼Œåœ¨ç›‘ç£å­¦ä¹ ã€å¼ºåŒ–å­¦ä¹ å’Œæµ‹è¯•æ—¶é—´æ€§èƒ½æ–¹é¢çš„æå‡åˆ†åˆ«è¾¾åˆ°äº†12.9%ã€5.2%å’Œ5.1%ã€‚è¿™å‡¸æ˜¾äº†é’ˆå¯¹é‡‘èé¢†åŸŸè¿›è¡Œä¸“é¡¹å¥–åŠ±å»ºæ¨¡çš„ä»·å€¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Fin-PRMæ˜¯ä¸€ä¸ªé’ˆå¯¹é‡‘èé¢†åŸŸçš„ä¸“ä¸šåŒ–è¿›ç¨‹å¥–åŠ±æ¨¡å‹ï¼Œç”¨äºè¯„ä¼°é‡‘èä»»åŠ¡ä¸­çš„æ¨ç†è¿‡ç¨‹ã€‚</li>
<li>Fin-PRMç»“åˆäº†æ­¥éª¤çº§å’Œè½¨è¿¹çº§çš„å¥–åŠ±ç›‘ç£ï¼Œä»¥ç²¾ç»†çš„æ–¹å¼è¯„ä¼°æ¨ç†è½¨è¿¹ï¼Œä¸é‡‘èé€»è¾‘ç›¸ç¬¦ã€‚</li>
<li>Fin-PRMåœ¨è´¢åŠ¡æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜äºé€šç”¨è¿›ç¨‹å¥–åŠ±æ¨¡å‹å’Œå¼ºåŸºå‡†çº¿ã€‚</li>
<li>Fin-PRMèƒ½æé«˜ä¸‹æ¸¸æ¨¡å‹çš„è½¨è¿¹é€‰æ‹©è´¨é‡ï¼ŒåŒ…æ‹¬ç”¨äºç›‘ç£å­¦ä¹ ã€å¼ºåŒ–å­¦ä¹ å’Œæµ‹è¯•æ—¶é—´çš„æ¨ç†è½¨è¿¹é€‰æ‹©ã€‚</li>
<li>é€šè¿‡Fin-PRMï¼Œä¸‹æ¸¸æ¨¡å‹çš„æ€§èƒ½å¾—åˆ°äº†æ˜¾è‘—æé«˜ï¼ŒåŒ…æ‹¬åœ¨ç›‘ç£å­¦ä¹ ã€å¼ºåŒ–å­¦ä¹ å’Œæµ‹è¯•æ—¶é—´æ–¹é¢çš„æå‡ã€‚</li>
<li>å®éªŒç»“æœè¯æ˜äº†é’ˆå¯¹ç‰¹å®šé¢†åŸŸï¼ˆå¦‚é‡‘èï¼‰è¿›è¡Œå¥–åŠ±å»ºæ¨¡çš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15202">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-545aa4f6f5248d09774e002dfe6aff64.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9dab5e23a22de821731e549e3319b69f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f0dbfa9077986a46a7533349bb534f9e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be4d05025f9fa88c555f18febdf9f814.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="PuzzleClone-An-SMT-Powered-Framework-for-Synthesizing-Verifiable-Data"><a href="#PuzzleClone-An-SMT-Powered-Framework-for-Synthesizing-Verifiable-Data" class="headerlink" title="PuzzleClone: An SMT-Powered Framework for Synthesizing Verifiable Data"></a>PuzzleClone: An SMT-Powered Framework for Synthesizing Verifiable Data</h2><p><strong>Authors:Kai Xiong, Yanwei Huang, Rongjunchen Zhang, Kun Chen, Haipang Wu</strong></p>
<p>High-quality mathematical and logical datasets with verifiable answers are essential for strengthening the reasoning capabilities of large language models (LLMs). While recent data augmentation techniques have facilitated the creation of large-scale benchmarks, existing LLM-generated datasets often suffer from limited reliability, diversity, and scalability. To address these challenges, we introduce PuzzleClone, a formal framework for synthesizing verifiable data at scale using Satisfiability Modulo Theories (SMT). Our approach features three key innovations: (1) encoding seed puzzles into structured logical specifications, (2) generating scalable variants through systematic variable and constraint randomization, and (3) ensuring validity via a reproduction mechanism. Applying PuzzleClone, we construct a curated benchmark comprising over 83K diverse and programmatically validated puzzles. The generated puzzles span a wide spectrum of difficulty and formats, posing significant challenges to current state-of-the-art models. We conduct post training (SFT and RL) on PuzzleClone datasets. Experimental results show that training on PuzzleClone yields substantial improvements not only on PuzzleClone testset but also on logic and mathematical benchmarks. Post training raises PuzzleClone average from 14.4 to 56.2 and delivers consistent improvements across 7 logic and mathematical benchmarks up to 12.5 absolute percentage points (AMC2023 from 52.5 to 65.0). Our code and data are available at <a target="_blank" rel="noopener" href="https://github.com/puzzleclone">https://github.com/puzzleclone</a>. </p>
<blockquote>
<p>é«˜è´¨é‡çš„æ•°å­¦å’Œé€»è¾‘æ•°æ®é›†å¯¹äºå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›è‡³å…³é‡è¦ã€‚è™½ç„¶æœ€è¿‘çš„æ•°æ®å¢å¼ºæŠ€æœ¯æœ‰åŠ©äºåˆ›å»ºå¤§è§„æ¨¡åŸºå‡†æµ‹è¯•ï¼Œä½†ç°æœ‰çš„LLMç”Ÿæˆçš„æ•°æ®é›†å¾€å¾€å­˜åœ¨å¯é æ€§ã€å¤šæ ·æ€§å’Œå¯æ‰©å±•æ€§æ–¹é¢çš„å±€é™æ€§ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†PuzzleCloneï¼Œè¿™æ˜¯ä¸€ä¸ªä½¿ç”¨å¯æ»¡è¶³æ€§æ¨¡ç†è®ºï¼ˆSMTï¼‰å¤§è§„æ¨¡åˆæˆå¯éªŒè¯æ•°æ®çš„æ­£å¼æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•æœ‰ä¸‰ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šï¼ˆ1ï¼‰å°†ç§å­è°œé¢˜ç¼–ç ä¸ºç»“æ„åŒ–çš„é€»è¾‘è§„èŒƒï¼Œï¼ˆ2ï¼‰é€šè¿‡ç³»ç»Ÿçš„å˜é‡å’Œçº¦æŸéšæœºåŒ–ç”Ÿæˆå¯æ‰©å±•çš„å˜ä½“ï¼Œï¼ˆ3ï¼‰é€šè¿‡å¤åˆ¶æœºåˆ¶ç¡®ä¿æœ‰æ•ˆæ€§ã€‚åº”ç”¨PuzzleCloneï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŒ…å«è¶…è¿‡83000ä¸ªå¤šæ ·ä¸”ç¨‹åºéªŒè¯æœ‰æ•ˆçš„è°œé¢˜çš„ç²¾é€‰åŸºå‡†æµ‹è¯•ã€‚ç”Ÿæˆçš„è°œé¢˜éš¾åº¦å„å¼‚ï¼Œå½¢å¼å¤šæ ·ï¼Œå¯¹å½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹æ„æˆäº†é‡å¤§æŒ‘æˆ˜ã€‚æˆ‘ä»¬åœ¨PuzzleCloneæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒåï¼ˆSFTå’ŒRLï¼‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨PuzzleCloneæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒä¸ä»…æé«˜äº†å…¶åœ¨PuzzleCloneæµ‹è¯•é›†ä¸Šçš„è¡¨ç°ï¼Œè€Œä¸”åœ¨é€»è¾‘å’Œæ•°å­¦åŸºå‡†æµ‹è¯•ä¸Šä¹Ÿæœ‰æ˜¾è‘—æ”¹å–„ã€‚è®­ç»ƒåï¼ŒPuzzleCloneçš„å¹³å‡åˆ†æ•°ä»14.4æé«˜åˆ°56.2ï¼Œåœ¨7ä¸ªé€»è¾‘å’Œæ•°å­¦åŸºå‡†æµ‹è¯•ä¸­å‡å–å¾—äº†ä¸€è‡´çš„æ”¹è¿›ï¼Œç»å¯¹ç™¾åˆ†ç‚¹æœ€é«˜æé«˜äº†12.5ï¼ˆAMC2023ä»52.5æé«˜åˆ°65.0ï¼‰ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/puzzleclone%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/puzzlecloneæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15180v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¯éªŒè¯ç­”æ¡ˆçš„é«˜è´¨é‡æ•°å­¦å’Œé€»è¾‘æ•°æ®é›†å¯¹äºåŠ å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›è‡³å…³é‡è¦ã€‚é’ˆå¯¹ç°æœ‰LLMç”Ÿæˆæ•°æ®é›†åœ¨å¯é æ€§ã€å¤šæ ·æ€§å’Œå¯æ‰©å±•æ€§æ–¹é¢çš„å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºäº†PuzzleCloneæ¡†æ¶ï¼Œé€šè¿‡å¯æ»¡è¶³æ€§æ¨¡ç†è®ºï¼ˆSMTï¼‰å¤§è§„æ¨¡åˆæˆå¯éªŒè¯æ•°æ®ã€‚PuzzleCloneé€šè¿‡ä¸‰ä¸ªå…³é”®åˆ›æ–°ç‚¹å®ç°äº†çªç ´ï¼šå°†ç§å­è°œé¢˜ç¼–ç ä¸ºç»“æ„åŒ–é€»è¾‘è§„èŒƒã€é€šè¿‡ç³»ç»Ÿå˜é‡å’Œçº¦æŸéšæœºåŒ–ç”Ÿæˆå¯æ‰©å±•å˜ä½“ï¼Œä»¥åŠé€šè¿‡å¤åˆ¶æœºåˆ¶ç¡®ä¿æœ‰æ•ˆæ€§ã€‚åº”ç”¨PuzzleCloneæ¡†æ¶æ„å»ºäº†ä¸€ä¸ªåŒ…å«è¶…è¿‡8.3ä¸‡ä¸ªå¤šæ ·åŒ–å’Œç¨‹åºéªŒè¯çš„è°œé¢˜åŸºå‡†æµ‹è¯•é›†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨PuzzleCloneæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒä¸ä»…æé«˜äº†å…¶åœ¨PuzzleCloneæµ‹è¯•é›†ä¸Šçš„æ€§èƒ½ï¼Œè€Œä¸”åœ¨é€»è¾‘å’Œæ•°å­¦åŸºå‡†æµ‹è¯•ä¸Šä¹Ÿå–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é«˜è´¨é‡æ•°å­¦å’Œé€»è¾‘æ•°æ®é›†å¯¹åŠ å¼ºLLMæ¨ç†èƒ½åŠ›è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰LLMç”Ÿæˆæ•°æ®é›†å­˜åœ¨å¯é æ€§ã€å¤šæ ·æ€§å’Œå¯æ‰©å±•æ€§é—®é¢˜ã€‚</li>
<li>PuzzleCloneæ¡†æ¶é€šè¿‡SMTåˆæˆå¯éªŒè¯æ•°æ®ï¼Œå…·æœ‰ä¸‰ä¸ªå…³é”®åˆ›æ–°ç‚¹ã€‚</li>
<li>PuzzleCloneæ„å»ºäº†åŒ…å«è¶…è¿‡8.3ä¸‡ä¸ªè°œé¢˜çš„å¤§è§„æ¨¡åŸºå‡†æµ‹è¯•é›†ã€‚</li>
<li>åœ¨PuzzleCloneæ•°æ®é›†ä¸Šè®­ç»ƒLLMï¼Œä¸ä»…æé«˜äº†å…¶åœ¨æµ‹è¯•é›†ä¸Šçš„æ€§èƒ½ï¼Œä¹Ÿæ”¹è¿›äº†é€»è¾‘å’Œæ•°å­¦åŸºå‡†æµ‹è¯•æˆç»©ã€‚</li>
<li>PuzzleCloneå¹³å‡æˆç»©ä»14.4æé«˜åˆ°56.2ï¼Œåœ¨7ä¸ªé€»è¾‘å’Œæ•°å­¦åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†ä¸€è‡´æ€§æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15180">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-28f1bddd067d2ab326b0f1be5c485ec4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9118212d41d053a1db7780101755332b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4573ab35fabe0ee9725776cd46347238.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-047613e95caa41543f12b152d660e996.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dce28d40554dcf5f23fbf05f15b9f699.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d7fc4713133ae2bfa175081556b8242.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="ContextualLVLM-Agent-A-Holistic-Framework-for-Multi-Turn-Visually-Grounded-Dialogue-and-Complex-Instruction-Following"><a href="#ContextualLVLM-Agent-A-Holistic-Framework-for-Multi-Turn-Visually-Grounded-Dialogue-and-Complex-Instruction-Following" class="headerlink" title="ContextualLVLM-Agent: A Holistic Framework for Multi-Turn   Visually-Grounded Dialogue and Complex Instruction Following"></a>ContextualLVLM-Agent: A Holistic Framework for Multi-Turn   Visually-Grounded Dialogue and Complex Instruction Following</h2><p><strong>Authors:Seungmin Han, Haeun Kwon, Ji-jun Park, Taeyang Yoon</strong></p>
<p>Despite significant advancements in Large Language Models (LLMs) and Large Vision-Language Models (LVLMs), current models still face substantial challenges in handling complex, multi-turn, and visually-grounded tasks that demand deep reasoning, sustained contextual understanding, entity tracking, and multi-step instruction following. Existing benchmarks often fall short in capturing the dynamism and intricacies of real-world multi-modal interactions, leading to issues such as context loss and visual hallucinations. To address these limitations, we introduce MMDR-Bench (Multi-Modal Dialogue Reasoning Benchmark), a novel dataset comprising 300 meticulously designed complex multi-turn dialogue scenarios, each averaging 5-7 turns and evaluated across six core dimensions including visual entity tracking and reasoning depth. Furthermore, we propose CoLVLM Agent (Contextual LVLM Agent), a holistic framework that enhances existing LVLMs with advanced reasoning and instruction following capabilities through an iterative â€œmemory-perception-planning-executionâ€ cycle, requiring no extensive re-training of the underlying models. Our extensive experiments on MMDR-Bench demonstrate that CoLVLM Agent consistently achieves superior performance, attaining an average human evaluation score of 4.03, notably surpassing state-of-the-art commercial models like GPT-4o (3.92) and Gemini 1.5 Pro (3.85). The framework exhibits significant advantages in reasoning depth, instruction adherence, and error suppression, and maintains robust performance over extended dialogue turns, validating the effectiveness of its modular design and iterative approach for complex multi-modal interactions. </p>
<blockquote>
<p>å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†å½“å‰æ¨¡å‹åœ¨å¤„ç†å¤æ‚ã€å¤šè½®ã€è§†è§‰åŸºç¡€çš„ä»»åŠ¡æ—¶ä»é¢ä¸´å·¨å¤§æŒ‘æˆ˜ï¼Œè¿™äº›ä»»åŠ¡éœ€è¦æ·±åº¦æ¨ç†ã€æŒç»­ä¸Šä¸‹æ–‡ç†è§£ã€å®ä½“è·Ÿè¸ªä»¥åŠå¤šæ­¥éª¤æŒ‡ä»¤æ‰§è¡Œã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•é€šå¸¸æ— æ³•æ•æ‰çœŸå®ä¸–ç•Œå¤šæ¨¡æ€äº¤äº’çš„åŠ¨æ€æ€§å’Œå¤æ‚æ€§ï¼Œå¯¼è‡´è¯¸å¦‚ä¸Šä¸‹æ–‡ä¸¢å¤±å’Œè§†è§‰å¹»è§‰ç­‰é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æ¨å‡ºäº†MMDR-Benchï¼ˆå¤šæ¨¡æ€å¯¹è¯æ¨ç†åŸºå‡†æµ‹è¯•ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„æ•°æ®é›†ï¼ŒåŒ…å«äº†300ä¸ªç²¾å¿ƒè®¾è®¡çš„å¤æ‚å¤šè½®å¯¹è¯åœºæ™¯ï¼Œæ¯ä¸ªåœºæ™¯å¹³å‡æœ‰5-7è½®å¯¹è¯ï¼Œå¹¶åœ¨å…­ä¸ªæ ¸å¿ƒç»´åº¦è¿›è¡Œè¯„ä¼°ï¼ŒåŒ…æ‹¬è§†è§‰å®ä½“è·Ÿè¸ªå’Œæ¨ç†æ·±åº¦ç­‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†CoLVLM Agentï¼ˆä¸Šä¸‹æ–‡LVLMä»£ç†ï¼‰è¿™ä¸€å…¨é¢æ¡†æ¶ï¼Œå®ƒé€šè¿‡è¿­ä»£â€œè®°å¿†-æ„ŸçŸ¥-è§„åˆ’-æ‰§è¡Œâ€å¾ªç¯ï¼Œå¢å¼ºäº†ç°æœ‰LVLMsçš„æ¨ç†å’ŒæŒ‡ä»¤æ‰§è¡Œèƒ½åŠ›ï¼Œæ— éœ€å¯¹åº•å±‚æ¨¡å‹è¿›è¡Œå¤§é‡å†è®­ç»ƒã€‚æˆ‘ä»¬åœ¨MMDR-Benchä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒCoLVLM Agentå§‹ç»ˆå®ç°å“è¶Šæ€§èƒ½ï¼Œå¹³å‡äººç±»è¯„ä»·å¾—åˆ†ä¸º4.03ï¼Œæ˜¾è‘—è¶…è¿‡äº†æœ€å…ˆè¿›çš„å•†ä¸šæ¨¡å‹ï¼Œå¦‚GPT-4oï¼ˆ3.92ï¼‰å’ŒGemini 1.5 Proï¼ˆ3.85ï¼‰ã€‚è¯¥æ¡†æ¶åœ¨æ¨ç†æ·±åº¦ã€æŒ‡ä»¤éµå¾ªå’Œé”™è¯¯æŠ‘åˆ¶æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œå¹¶åœ¨è¾ƒé•¿çš„å¯¹è¯è½®æ¬¡ä¸­ä¿æŒç¨³å¥æ€§èƒ½ï¼ŒéªŒè¯äº†å…¶åœ¨å¤æ‚å¤šæ¨¡æ€äº¤äº’ä¸­çš„æ¨¡å—åŒ–è®¾è®¡å’Œè¿­ä»£æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15164v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„æœ€æ–°è¿›å±•ï¼ŒæŒ‡å‡ºäº†å®ƒä»¬åœ¨å¤„ç†å¤æ‚ã€å¤šè½®ã€è§†è§‰å®šä½çš„ä»»åŠ¡æ–¹é¢çš„æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œå¼•å…¥MMDR-Benchæ•°æ®é›†å’ŒCoLVLM Agentæ¡†æ¶ã€‚MMDR-BenchåŒ…å«300ä¸ªç²¾å¿ƒè®¾è®¡çš„å¤æ‚å¤šè½®å¯¹è¯åœºæ™¯ï¼Œè¯„ä¼°æ ¸å¿ƒç»´åº¦åŒ…æ‹¬è§†è§‰å®ä½“è·Ÿè¸ªå’Œæ¨ç†æ·±åº¦ã€‚CoLVLM Agenté€šè¿‡è¿­ä»£â€œè®°å¿†-æ„ŸçŸ¥-è§„åˆ’-æ‰§è¡Œâ€å¾ªç¯å¢å¼ºç°æœ‰LVLMsçš„æ¨ç†å’ŒæŒ‡ä»¤éµå¾ªèƒ½åŠ›ï¼Œæ— éœ€å¯¹åº•å±‚æ¨¡å‹è¿›è¡Œå¤§é‡å†è®­ç»ƒã€‚åœ¨MMDR-Benchä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCoLVLM Agentæ€§èƒ½å“è¶Šï¼Œå¹³å‡äººç±»è¯„ä»·å¾—åˆ†ä¸º4.03ï¼Œè¶…è¶Šäº†GPT-4oï¼ˆ3.92ï¼‰å’ŒGemini 1.5 Proï¼ˆ3.85ï¼‰ã€‚è¯¥æ¡†æ¶åœ¨æ¨ç†æ·±åº¦ã€æŒ‡ä»¤éµå¾ªå’Œé”™è¯¯æŠ‘åˆ¶æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œå¹¶åœ¨å¤šè½®å¯¹è¯ä¸­ä¿æŒç¨³å¥æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¤æ‚ã€å¤šè½®ã€è§†è§‰å®šä½çš„ä»»åŠ¡æ—¶ä»é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>MMDR-Benchæ•°æ®é›†æ—¨åœ¨è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼ŒåŒ…å«å¤æ‚å¤šè½®å¯¹è¯åœºæ™¯ï¼Œå¼ºè°ƒè§†è§‰å®ä½“è·Ÿè¸ªå’Œæ¨ç†æ·±åº¦ã€‚</li>
<li>CoLVLM Agentæ¡†æ¶é€šè¿‡è¿­ä»£å¾ªç¯å¢å¼ºç°æœ‰LVLMsçš„æ¨ç†å’ŒæŒ‡ä»¤éµå¾ªèƒ½åŠ›ï¼Œæ— éœ€å¤§é‡å†è®­ç»ƒã€‚</li>
<li>CoLVLM Agentåœ¨MMDR-Benchä¸Šçš„å®éªŒè¡¨ç°ä¼˜äºGPT-4oå’ŒGemini 1.5 Proç­‰ç°æœ‰æ¨¡å‹ã€‚</li>
<li>è¯¥æ¡†æ¶åœ¨æ¨ç†æ·±åº¦ã€æŒ‡ä»¤éµå¾ªå’Œé”™è¯¯æŠ‘åˆ¶æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚</li>
<li>CoLVLM Agentåœ¨å¤šè½®å¯¹è¯ä¸­ä¿æŒç¨³å¥æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15164">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b9f34b70ddaf3d8a49ffa0603f471cfe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-31cece34279ee1c4d34074a9393529ee.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3fc19417e0f1d62bfd6e8593cda009c7.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Mobile-Agent-v3-Foundamental-Agents-for-GUI-Automation"><a href="#Mobile-Agent-v3-Foundamental-Agents-for-GUI-Automation" class="headerlink" title="Mobile-Agent-v3: Foundamental Agents for GUI Automation"></a>Mobile-Agent-v3: Foundamental Agents for GUI Automation</h2><p><strong>Authors:Jiabo Ye, Xi Zhang, Haiyang Xu, Haowei Liu, Junyang Wang, Zhaoqing Zhu, Ziwei Zheng, Feiyu Gao, Junjie Cao, Zhengxi Lu, Jitong Liao, Qi Zheng, Fei Huang, Jingren Zhou, Ming Yan</strong></p>
<p>This paper introduces GUI-Owl, a foundational GUI agent model that achieves state-of-the-art performance among open-source end-to-end models on ten GUI benchmarks across desktop and mobile environments, covering grounding, question answering, planning, decision-making, and procedural knowledge. GUI-Owl-7B achieves 66.4 on AndroidWorld and 29.4 on OSWorld. Building on this, we propose Mobile-Agent-v3, a general-purpose GUI agent framework that further improves performance to 73.3 on AndroidWorld and 37.7 on OSWorld, setting a new state-of-the-art for open-source GUI agent frameworks. GUI-Owl incorporates three key innovations: (1) Large-scale Environment Infrastructure: a cloud-based virtual environment spanning Android, Ubuntu, macOS, and Windows, enabling our Self-Evolving GUI Trajectory Production framework. This generates high-quality interaction data via automated query generation and correctness validation, leveraging GUI-Owl to refine trajectories iteratively, forming a self-improving loop. It supports diverse data pipelines and reduces manual annotation. (2) Diverse Foundational Agent Capabilities: by integrating UI grounding, planning, action semantics, and reasoning patterns, GUI-Owl supports end-to-end decision-making and can act as a modular component in multi-agent systems. (3) Scalable Environment RL: we develop a scalable reinforcement learning framework with fully asynchronous training for real-world alignment. We also introduce Trajectory-aware Relative Policy Optimization (TRPO) for online RL, achieving 34.9 on OSWorld. GUI-Owl and Mobile-Agent-v3 are open-sourced at <a target="_blank" rel="noopener" href="https://github.com/X-PLUG/MobileAgent">https://github.com/X-PLUG/MobileAgent</a>. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†GUI-Owlï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºç¡€GUIä»£ç†æ¨¡å‹ï¼Œå®ƒåœ¨æ¡Œé¢å’Œç§»åŠ¨ç¯å¢ƒçš„åä¸ªGUIåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†å¼€æºç«¯åˆ°ç«¯æ¨¡å‹çš„æœ€æ–°æ€§èƒ½ï¼Œæ¶µç›–äº†æ¥åœ°ã€é—®ç­”ã€è§„åˆ’ã€å†³ç­–å’Œç¨‹åºçŸ¥è¯†ã€‚GUI-Owl-7Båœ¨AndroidWorldä¸Šè¾¾åˆ°66.4ï¼Œåœ¨OSWorldä¸Šè¾¾åˆ°29.4ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†é€šç”¨GUIä»£ç†æ¡†æ¶Mobile-Agent-v3ï¼Œå®ƒè¿›ä¸€æ­¥æé«˜äº†æ€§èƒ½ï¼Œåœ¨AndroidWorldä¸Šè¾¾åˆ°73.3ï¼Œåœ¨OSWorldä¸Šè¾¾åˆ°37.7ï¼Œä¸ºå¼€æºGUIä»£ç†æ¡†æ¶åˆ›é€ äº†æ–°çš„æœ€æ–°çºªå½•ã€‚GUI-Owlèåˆäº†ä¸‰å¤§åˆ›æ–°ç‚¹ï¼šï¼ˆ1ï¼‰å¤§è§„æ¨¡ç¯å¢ƒåŸºç¡€è®¾æ–½ï¼šä¸€ä¸ªè·¨è¶ŠAndroidã€Ubuntuã€macOSå’ŒWindowsçš„åŸºäºäº‘ç«¯çš„è™šæ‹Ÿç¯å¢ƒï¼Œä½¿æˆ‘ä»¬çš„è‡ªæˆ‘è¿›åŒ–GUIè½¨è¿¹ç”Ÿäº§æ¡†æ¶æˆä¸ºå¯èƒ½ã€‚è¯¥æ¡†æ¶é€šè¿‡è‡ªåŠ¨åŒ–æŸ¥è¯¢ç”Ÿæˆå’Œæ­£ç¡®æ€§éªŒè¯ç”Ÿæˆé«˜è´¨é‡äº¤äº’æ•°æ®ï¼Œåˆ©ç”¨GUI-Owlè¿­ä»£åœ°ä¼˜åŒ–è½¨è¿¹ï¼Œå½¢æˆä¸€ä¸ªè‡ªæˆ‘æ”¹è¿›å¾ªç¯ã€‚å®ƒæ”¯æŒå„ç§æ•°æ®ç®¡é“ï¼Œå‡å°‘æ‰‹åŠ¨æ³¨é‡Šã€‚ï¼ˆ2ï¼‰å¤šæ ·åŒ–çš„åŸºç¡€ä»£ç†åŠŸèƒ½ï¼šé€šè¿‡é›†æˆUIæ¥åœ°ã€è§„åˆ’ã€åŠ¨ä½œè¯­ä¹‰å’Œæ¨ç†æ¨¡å¼ï¼ŒGUI-Owlæ”¯æŒç«¯åˆ°ç«¯çš„å†³ç­–åˆ¶å®šï¼Œå¯ä»¥ä½œä¸ºå¤šä»£ç†ç³»ç»Ÿä¸­çš„æ¨¡å—åŒ–ç»„ä»¶ã€‚ï¼ˆ3ï¼‰å¯æ‰©å±•çš„ç¯å¢ƒå¼ºåŒ–å­¦ä¹ ï¼šæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå¯æ‰©å±•çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå…·æœ‰å®Œå…¨å¼‚æ­¥è®­ç»ƒä»¥ç¬¦åˆç°å®ä¸–ç•Œçš„å¯¹é½ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†è½¨è¿¹æ„ŸçŸ¥ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆTRPOï¼‰è¿›è¡Œåœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼Œåœ¨OSWorldä¸Šå®ç°34.9çš„æˆç»©ã€‚GUI-Owlå’ŒMobile-Agent-v3å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/X-PLUG/Mobileagent%E5%BC%80%E6%BA%90%E3%80%82">https://github.com/X-PLUG/Mobileagentå¼€æºã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15144v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†GUI-Owlï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºç¡€GUIä»£ç†æ¨¡å‹ï¼Œåœ¨æ¡Œé¢å’Œç§»åŠ¨ç¯å¢ƒçš„åä¸ªGUIåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†å¼€åˆ›æ€§çš„æ€§èƒ½ã€‚å®ƒåœ¨AndroidWorldä¸Šçš„å¾—åˆ†ä¸º66.4ï¼Œåœ¨OSWorldä¸Šçš„å¾—åˆ†ä¸º29.4ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæå‡ºäº†é€šç”¨GUIä»£ç†æ¡†æ¶Mobile-Agent-v3ï¼Œè¿›ä¸€æ­¥æé«˜äº†æ€§èƒ½ï¼Œåœ¨AndroidWorldä¸Šå¾—åˆ†73.3ï¼Œåœ¨OSWorldä¸Šå¾—åˆ†37.7ï¼Œä¸ºå¼€æºGUIä»£ç†æ¡†æ¶åˆ›é€ äº†æ–°çš„ä¸–ç•Œçºªå½•ã€‚GUI-OwlåŒ…å«ä¸‰ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šå¤§è§„æ¨¡ç¯å¢ƒåŸºç¡€è®¾æ–½ã€å¤šæ ·åŒ–çš„åŸºç¡€ä»£ç†åŠŸèƒ½å’Œå¯æ‰©å±•çš„ç¯å¢ƒå¼ºåŒ–å­¦ä¹ ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>GUI-Owlæ¨¡å‹ï¼šä»‹ç»äº†GUI-Owlï¼Œä¸€ä¸ªæ€§èƒ½å‡ºè‰²çš„å¼€æºç«¯åˆ°ç«¯GUIä»£ç†æ¨¡å‹ï¼Œå®ƒåœ¨å¤šä¸ªGUIåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ã€‚</li>
<li>Mobile-Agent-v3æ¡†æ¶ï¼šæå‡ºçš„Mobile-Agent-v3æ¡†æ¶è¿›ä¸€æ­¥æé«˜äº†GUIä»£ç†çš„æ€§èƒ½ï¼Œå¹¶åœ¨AndroidWorldå’ŒOSWorldä¸Šè¾¾åˆ°äº†æ–°çš„ä¸–ç•Œçºªå½•ã€‚</li>
<li>å¤§è§„æ¨¡ç¯å¢ƒåŸºç¡€è®¾æ–½ï¼šé€šè¿‡äº‘ç«¯çš„è™šæ‹Ÿç¯å¢ƒï¼Œå®ç°äº†è‡ªæˆ‘è¿›åŒ–çš„GUIè½¨è¿¹ç”Ÿäº§æ¡†æ¶ï¼Œå‡å°‘äº†æ‰‹åŠ¨æ ‡æ³¨ï¼Œå¹¶æ”¯æŒå¤šç§æ•°æ®ç®¡é“ã€‚</li>
<li>å¤šæ ·åŒ–çš„åŸºç¡€ä»£ç†èƒ½åŠ›ï¼šGUI-Owlé›†æˆäº†UIå®šä½ã€è§„åˆ’ã€åŠ¨ä½œè¯­ä¹‰å’Œæ¨ç†æ¨¡å¼ï¼Œæ”¯æŒç«¯åˆ°ç«¯çš„å†³ç­–åˆ¶å®šï¼Œå¹¶å¯ä½œä¸ºå¤šä»£ç†ç³»ç»Ÿä¸­çš„æ¨¡å—åŒ–ç»„ä»¶ã€‚</li>
<li>å¯æ‰©å±•çš„ç¯å¢ƒå¼ºåŒ–å­¦ä¹ ï¼šå¼€å‘äº†å…·æœ‰å®Œå…¨å¼‚æ­¥è®­ç»ƒçš„å¯æ‰©å±•å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå¹¶å¼•å…¥äº†è½¨è¿¹æ„ŸçŸ¥ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆTRPOï¼‰è¿›è¡Œåœ¨çº¿å¼ºåŒ–å­¦ä¹ ã€‚</li>
<li>GUI-Owlå’ŒMobile-Agent-v3çš„å¼€æºæ€§ï¼šè¿™ä¸¤ä¸ªæ¨¡å‹å’Œæ¡†æ¶éƒ½æ˜¯å¼€æºçš„ï¼Œæ–¹ä¾¿å…¬ä¼—è®¿é—®å’Œè´¡çŒ®ã€‚</li>
<li>æ¨¡å‹ä¸æ¡†æ¶çš„åº”ç”¨å‰æ™¯ï¼šGUI-Owlå’ŒMobile-Agent-v3åœ¨ç§»åŠ¨å’Œæ¡Œé¢ç¯å¢ƒä¸­çš„GUIä»£ç†ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä¸ºæœªæ¥çš„äººæœºäº¤äº’æŠ€æœ¯æä¾›äº†æ–°çš„æ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15144">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-80c97cadc0da7730513a65955695be8c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a88af975441203b8a9da9f9fd7299d7f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8c532b38d8f80847c616baa04ff12c73.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a5aeaa9cb8684922bcd597a5f2ea0a43.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6366c4e2b849c160d1da1ff1330d53bd.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="On-the-need-to-perform-comprehensive-evaluations-of-automated-program-repair-benchmarks-Sorald-case-study"><a href="#On-the-need-to-perform-comprehensive-evaluations-of-automated-program-repair-benchmarks-Sorald-case-study" class="headerlink" title="On the need to perform comprehensive evaluations of automated program   repair benchmarks: Sorald case study"></a>On the need to perform comprehensive evaluations of automated program   repair benchmarks: Sorald case study</h2><p><strong>Authors:Sumudu Liyanage, Sherlock A. Licorish, Markus Wagner, Stephen G. MacDonell</strong></p>
<p>In supporting the development of high-quality software, especially necessary in the era of LLMs, automated program repair (APR) tools aim to improve code quality by automatically addressing violations detected by static analysis profilers. Previous research tends to evaluate APR tools only for their ability to clear violations, neglecting their potential introduction of new (sometimes severe) violations, changes to code functionality and degrading of code structure. There is thus a need for research to develop and assess comprehensive evaluation frameworks for APR tools. This study addresses this research gap, and evaluates Sorald (a state-of-the-art APR tool) as a proof of concept. Soraldâ€™s effectiveness was evaluated in repairing 3,529 SonarQube violations across 30 rules within 2,393 Java code snippets extracted from Stack Overflow. Outcomes show that while Sorald fixes specific rule violations, it introduced 2,120 new faults (32 bugs, 2088 code smells), reduced code functional correctnessâ€“as evidenced by a 24% unit test failure rateâ€“and degraded code structure, demonstrating the utility of our framework. Findings emphasize the need for evaluation methodologies that capture the full spectrum of APR tool effects, including side effects, to ensure their safe and effective adoption. </p>
<blockquote>
<p>åœ¨æ”¯æŒé«˜è´¨é‡è½¯ä»¶å¼€å‘çš„æ—¶ä»£ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ—¶ä»£ï¼Œè‡ªåŠ¨åŒ–ç¨‹åºä¿®å¤ï¼ˆAPRï¼‰å·¥å…·æ—¨åœ¨é€šè¿‡è‡ªåŠ¨è§£å†³é™æ€åˆ†æåˆ†æå™¨æ£€æµ‹åˆ°çš„è¿è§„æƒ…å†µæ¥æé«˜ä»£ç è´¨é‡ã€‚ä»¥å¾€çš„ç ”ç©¶å¾€å¾€åªè¯„ä¼°APRå·¥å…·æ¸…é™¤è¿è§„çš„èƒ½åŠ›ï¼Œå¿½è§†äº†å®ƒä»¬å¯èƒ½å¼•å…¥æ–°çš„ï¼ˆæœ‰æ—¶æ˜¯ä¸¥é‡çš„ï¼‰è¿è§„æƒ…å†µã€å¯¹ä»£ç åŠŸèƒ½çš„æ”¹å˜ä»¥åŠä»£ç ç»“æ„çš„é€€åŒ–ã€‚å› æ­¤ï¼Œæœ‰å¿…è¦è¿›è¡Œç ”ç©¶ï¼Œå¼€å‘å’Œè¯„ä¼°APRå·¥å…·çš„å…¨é¢è¯„ä¼°æ¡†æ¶ã€‚æœ¬ç ”ç©¶è§£å†³äº†è¿™ä¸€ç ”ç©¶ç©ºç™½ï¼Œå¹¶ä½œä¸ºæ¦‚å¿µéªŒè¯è¯„ä¼°äº†æœ€æ–°é¢–çš„APRå·¥å…·Soraldã€‚åœ¨ä¿®å¤ä»Stack Overflowæå–çš„3,529ä¸ªSonarQubeè¿è§„æƒ…å†µï¼ˆæ¶‰åŠ30æ¡è§„åˆ™ï¼Œæ¶µç›–2,393ä¸ªJavaä»£ç ç‰‡æ®µï¼‰æ–¹é¢ï¼Œå¯¹Soraldçš„æœ‰æ•ˆæ€§è¿›è¡Œäº†è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼Œè™½ç„¶Soraldèƒ½å¤Ÿä¿®å¤ç‰¹å®šçš„è§„åˆ™è¿è§„æƒ…å†µï¼Œä½†å®ƒå¼•å…¥äº†2,120ä¸ªæ–°æ•…éšœï¼ˆåŒ…æ‹¬32ä¸ªé”™è¯¯ï¼Œ2,088ä¸ªä»£ç å¼‚å‘³ï¼‰ï¼Œé™ä½äº†ä»£ç åŠŸèƒ½çš„æ­£ç¡®æ€§ï¼ˆæœ‰24%çš„å•å…ƒæµ‹è¯•å¤±è´¥ç‡ä½œä¸ºè¯æ®ï¼‰ï¼Œå¹¶ç ´åäº†ä»£ç ç»“æ„ï¼Œè¯æ˜äº†æˆ‘ä»¬çš„æ¡†æ¶çš„å®ç”¨æ€§ã€‚ç ”ç©¶ç»“æœå¼ºè°ƒéœ€è¦ä¸€ç§è¯„ä¼°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ•æ‰åˆ°APRå·¥å…·çš„æ‰€æœ‰å½±å“ï¼ŒåŒ…æ‹¬å‰¯ä½œç”¨ï¼Œä»¥ç¡®ä¿å…¶å®‰å…¨å’Œæœ‰æ•ˆçš„é‡‡ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15135v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æ¢è®¨äº†åœ¨LLMæ—¶ä»£é«˜è´¨é‡è½¯ä»¶å¼€å‘ä¸­è‡ªåŠ¨åŒ–ç¨‹åºä¿®å¤å·¥å…·çš„é‡è¦æ€§åŠå…¶è¯„ä»·æ¡†æ¶çš„ç¼ºå¤±ã€‚æ–‡ç« ä»¥Soraldè¿™ä¸€å‰æ²¿APRå·¥å…·ä¸ºä¾‹ï¼Œé€šè¿‡å¯¹å…¶ä¿®å¤SonarQubeæ£€æµ‹åˆ°çš„3,529ä¸ªè¿è§„å®ä¾‹çš„æ•ˆæœè¿›è¡Œè¯„ä¼°ï¼Œå‘ç°è™½ç„¶Soraldèƒ½å¤Ÿä¿®å¤ç‰¹å®šè§„åˆ™è¿è§„ï¼Œä½†åŒæ—¶ä¹Ÿå¼•å…¥äº†æ–°çš„ç¼ºé™·ï¼ˆåŒ…æ‹¬32ä¸ªé”™è¯¯å’Œå¤§é‡ä»£ç å¼‚å‘³ï¼‰ï¼Œå¹¶å¯¹ä»£ç åŠŸèƒ½æ­£ç¡®æ€§äº§ç”Ÿè´Ÿé¢å½±å“ï¼ˆå•å…ƒæµ‹è¯•å¤±è´¥ç‡è¾¾åˆ°äº†24%ï¼‰ï¼ŒåŒæ—¶å¯¼è‡´ä»£ç ç»“æ„é€€åŒ–ã€‚å› æ­¤ï¼Œæ–‡ç« å¼ºè°ƒäº†å¼€å‘å…¨é¢è¯„ä¼°APRå·¥å…·çš„é‡è¦æ€§ï¼Œç‰¹åˆ«æ˜¯éœ€è¦è¯„ä¼°æ–¹æ³•èƒ½å¤Ÿæ•æ‰å·¥å…·çš„æ‰€æœ‰å½±å“ï¼ŒåŒ…æ‹¬å‰¯ä½œç”¨ï¼Œä»¥ç¡®ä¿å…¶å®‰å…¨å’Œæœ‰æ•ˆçš„é‡‡ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨åŒ–ç¨‹åºä¿®å¤å·¥å…·æ—¨åœ¨æé«˜ä»£ç è´¨é‡ï¼Œç‰¹åˆ«æ˜¯åœ¨LLMæ—¶ä»£æ˜¾å¾—å°¤ä¸ºé‡è¦ã€‚</li>
<li>ç°æœ‰ç ”ç©¶å¾€å¾€åªå…³æ³¨APRå·¥å…·ä¿®å¤è¿è§„çš„èƒ½åŠ›ï¼Œè€Œå¿½è§†äº†å…¶å¯èƒ½å¼•å…¥æ–°çš„ç¼ºé™·ã€æ”¹å˜ä»£ç åŠŸèƒ½ä»¥åŠç ´åä»£ç ç»“æ„çš„é£é™©ã€‚</li>
<li>Soraldä½œä¸ºå‰æ²¿APRå·¥å…·çš„ä»£è¡¨ï¼Œåœ¨ä¿®å¤ç‰¹å®šè¿è§„å®ä¾‹çš„åŒæ—¶ï¼Œä¹Ÿå¼•å…¥äº†å¤§é‡æ–°çš„ç¼ºé™·å’Œä»£ç å¼‚å‘³ã€‚</li>
<li>Soraldå¯¹ä»£ç åŠŸèƒ½æ­£ç¡®æ€§äº§ç”Ÿäº†è´Ÿé¢å½±å“ï¼Œå¯¼è‡´å•å…ƒæµ‹è¯•å¤±è´¥ç‡ä¸Šå‡ã€‚</li>
<li>ä»£ç ç»“æ„åœ¨ä¿®å¤è¿‡ç¨‹ä¸­å¯èƒ½å‡ºç°é€€åŒ–ç°è±¡ã€‚</li>
<li>å½“å‰ç ”ç©¶éœ€è¦å¼€å‘å…¨é¢çš„è¯„ä¼°æ¡†æ¶æ¥å…¨é¢è¯„ä¼°APRå·¥å…·çš„å½±å“ï¼ŒåŒ…æ‹¬å…¶å¯¹ä»£ç è´¨é‡ã€åŠŸèƒ½æ€§å’Œç»“æ„æ€§çš„å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15135">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-0583bcfdb6703ac2cc553b5dbd135c90.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b0cf0d35917c2273b5b32eedcce95b38.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3e4f7a4ffb3806f7a90b623502402d4b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f7cafd50143d7bee641570612590e49f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f949cd5f933ed7dac4f89bde1171ea8.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Donâ€™t-Think-Twice-Over-Reasoning-Impairs-Confidence-Calibration"><a href="#Donâ€™t-Think-Twice-Over-Reasoning-Impairs-Confidence-Calibration" class="headerlink" title="Donâ€™t Think Twice! Over-Reasoning Impairs Confidence Calibration"></a>Donâ€™t Think Twice! Over-Reasoning Impairs Confidence Calibration</h2><p><strong>Authors:Romain Lacombe, Kerrie Wu, Eddie Dilworth</strong></p>
<p>Large Language Models deployed as question answering tools require robust calibration to avoid overconfidence. We systematically evaluate how reasoning capabilities and budget affect confidence assessment accuracy, using the ClimateX dataset (Lacombe et al., 2023) and expanding it to human and planetary health. Our key finding challenges the â€œtest-time scalingâ€ paradigm: while recent reasoning LLMs achieve 48.7% accuracy in assessing expert confidence, increasing reasoning budgets consistently impairs rather than improves calibration. Extended reasoning leads to systematic overconfidence that worsens with longer thinking budgets, producing diminishing and negative returns beyond modest computational investments. Conversely, search-augmented generation dramatically outperforms pure reasoning, achieving 89.3% accuracy by retrieving relevant evidence. Our results suggest that information access, rather than reasoning depth or inference budget, may be the critical bottleneck for improved confidence calibration of knowledge-intensive tasks. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºé—®ç­”å·¥å…·éƒ¨ç½²æ—¶ï¼Œéœ€è¦è¿›è¡Œç¨³å¥çš„æ ¡å‡†ä»¥é¿å…è¿‡åº¦è‡ªä¿¡ã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°è¯„ä¼°äº†æ¨ç†èƒ½åŠ›å’Œé¢„ç®—å¯¹ä¿¡å¿ƒè¯„ä¼°ç²¾åº¦çš„å½±å“ï¼Œä½¿ç”¨äº†ClimateXæ•°æ®é›†ï¼ˆLacombeç­‰äººï¼Œ2023å¹´ï¼‰ï¼Œå¹¶å°†å…¶æ‰©å±•åˆ°äººç±»å’Œæ˜Ÿçƒå¥åº·é¢†åŸŸã€‚æˆ‘ä»¬çš„å…³é”®å‘ç°å¯¹â€œæµ‹è¯•æ—¶ç¼©æ”¾â€èŒƒå¼æå‡ºäº†æŒ‘æˆ˜ï¼šè™½ç„¶æœ€è¿‘çš„æ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¯„ä¼°ä¸“å®¶ä¿¡å¿ƒæ–¹é¢è¾¾åˆ°äº†48.7%çš„å‡†ç¡®åº¦ï¼Œä½†å¢åŠ æ¨ç†é¢„ç®—å´ä¼šæŸå®³è€Œä¸æ˜¯æ”¹å–„æ ¡å‡†ã€‚è¿‡åº¦æ¨ç†å¯¼è‡´ç³»ç»Ÿè¿‡åº¦è‡ªä¿¡ï¼Œéšç€æ€è€ƒé¢„ç®—çš„å¢åŠ è€Œæ¶åŒ–ï¼Œè¶…å‡ºé€‚åº¦è®¡ç®—æŠ•èµ„åäº§ç”Ÿæ”¶ç›Šé€’å‡å’Œè´Ÿé¢å›æŠ¥ã€‚ç›¸åï¼Œæœç´¢å¢å¼ºç”Ÿæˆæ³•æ˜¾è‘—ä¼˜äºçº¯æ¨ç†æ³•ï¼Œé€šè¿‡æ£€ç´¢ç›¸å…³è¯æ®è¾¾åˆ°äº†89.3%çš„å‡†ç¡®åº¦ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œä¿¡æ¯è·å–å¯èƒ½è€Œä¸æ˜¯æ¨ç†æ·±åº¦æˆ–æ¨ç†é¢„ç®—æˆä¸ºæ”¹è¿›çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¿¡å¿ƒæ ¡å‡†çš„å…³é”®ç“¶é¢ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15050v1">PDF</a> Published at ICML 2025 Workshop on Reliable and Responsible   Foundation Models</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºé—®ç­”å·¥å…·éœ€è¦å¯é çš„æ ¡å‡†ä»¥é¿å…è¿‡åº¦è‡ªä¿¡ã€‚æœ¬æ–‡é€šè¿‡ClimateXæ•°æ®é›†ï¼ˆLacombeç­‰äººï¼Œ2023å¹´ï¼‰ç³»ç»Ÿè¯„ä¼°æ¨ç†èƒ½åŠ›å’Œé¢„ç®—å¯¹ä¿¡å¿ƒè¯„ä¼°å‡†ç¡®æ€§çš„å½±å“ï¼Œå¹¶æ‰©å±•åˆ°äººç±»å’Œåœ°çƒå¥åº·é¢†åŸŸã€‚ç ”ç©¶å‘ç°ï¼Œå¢åŠ æ¨ç†é¢„ç®—å¹¶ä¸ä¼šæ”¹å–„æ ¡å‡†ï¼Œåè€Œå¯èƒ½å¯¼è‡´ç³»ç»Ÿæ€§è¿‡åº¦è‡ªä¿¡ï¼Œå¹¶ä¸”åœ¨è¾ƒé•¿çš„æ€è€ƒé¢„ç®—ä¸‹äº§ç”Ÿè´Ÿé¢å›æŠ¥ã€‚ç›¸åï¼Œæœç´¢å¢å¼ºç”Ÿæˆæ³•æ˜¾è‘—ä¼˜äºçº¯æ¨ç†æ³•ï¼Œé€šè¿‡æ£€ç´¢ç›¸å…³è¯æ®è¾¾åˆ°89.3%çš„å‡†ç¡®ç‡ã€‚è¿™è¡¨æ˜ä¿¡æ¯è·å–å¯èƒ½æ˜¯æ”¹è¿›çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¿¡å¿ƒæ ¡å‡†çš„å…³é”®ç“¶é¢ˆï¼Œè€Œéæ¨ç†æ·±åº¦æˆ–æ¨ç†é¢„ç®—ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºé—®ç­”å·¥å…·éœ€è¦æ ¡å‡†ä»¥é¿å…è¿‡åº¦è‡ªä¿¡ã€‚</li>
<li>å¢åŠ æ¨ç†é¢„ç®—ä¸ä¸€å®šä¼šæé«˜ä¿¡å¿ƒè¯„ä¼°çš„å‡†ç¡®æ€§ï¼Œåè€Œå¯èƒ½å¯¼è‡´ç³»ç»Ÿæ€§è¿‡åº¦è‡ªä¿¡ã€‚</li>
<li>æœç´¢å¢å¼ºç”Ÿæˆæ³•é€šè¿‡æ£€ç´¢ç›¸å…³è¯æ®æ˜¾è‘—æé«˜ä¿¡å¿ƒè¯„ä¼°çš„å‡†ç¡®ç‡ã€‚</li>
<li>ä¿¡æ¯è·å–æ˜¯æ”¹è¿›çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¿¡å¿ƒæ ¡å‡†çš„å…³é”®ã€‚</li>
<li>æ¨ç†æ·±åº¦å¹¶éæ”¹å–„ä¿¡å¿ƒæ ¡å‡†çš„ä¸»è¦å› ç´ ã€‚</li>
<li>é€‚åº¦çš„è®¡ç®—æŠ•èµ„å¯ä»¥è·å¾—è¾ƒå¥½çš„ä¿¡å¿ƒè¯„ä¼°æ•ˆæœï¼Œè¿‡å¤šçš„è®¡ç®—èµ„æºå¯èƒ½å¯¼è‡´è´Ÿé¢å›æŠ¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15050">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-19af5689bcd8895607883d6ad5241a80.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-52ca86482f459a221ab6bd72287e78a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73c54e61acaf5ebbe0e66e84eeabe59c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d740cd23c54ddf313fc82ca8fb5bc9f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a5b09182a8735a2885c64a9e8599fbed.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="In-Context-Iterative-Policy-Improvement-for-Dynamic-Manipulation"><a href="#In-Context-Iterative-Policy-Improvement-for-Dynamic-Manipulation" class="headerlink" title="In-Context Iterative Policy Improvement for Dynamic Manipulation"></a>In-Context Iterative Policy Improvement for Dynamic Manipulation</h2><p><strong>Authors:Mark Van der Merwe, Devesh Jha</strong></p>
<p>Attention-based architectures trained on internet-scale language data have demonstrated state of the art reasoning ability for various language-based tasks, such as logic problems and textual reasoning. Additionally, these Large Language Models (LLMs) have exhibited the ability to perform few-shot prediction via in-context learning, in which input-output examples provided in the prompt are generalized to new inputs. This ability furthermore extends beyond standard language tasks, enabling few-shot learning for general patterns. In this work, we consider the application of in-context learning with pre-trained language models for dynamic manipulation. Dynamic manipulation introduces several crucial challenges, including increased dimensionality, complex dynamics, and partial observability. To address this, we take an iterative approach, and formulate our in-context learning problem to predict adjustments to a parametric policy based on previous interactions. We show across several tasks in simulation and on a physical robot that utilizing in-context learning outperforms alternative methods in the low data regime. Video summary of this work and experiments can be found <a target="_blank" rel="noopener" href="https://youtu.be/2inxpdrq74U?si=dAdDYsUEr25nZvRn">https://youtu.be/2inxpdrq74U?si=dAdDYsUEr25nZvRn</a>. </p>
<blockquote>
<p>åŸºäºäº’è”ç½‘è§„æ¨¡è¯­è¨€æ•°æ®è®­ç»ƒçš„æ³¨æ„åŠ›æ¶æ„åœ¨å„ç§è¯­è¨€ä»»åŠ¡ä¸Šå±•ç°å‡ºäº†æœ€å‰æ²¿çš„æ¨ç†èƒ½åŠ›ï¼Œå¦‚é€»è¾‘é—®é¢˜å’Œæ–‡æœ¬æ¨ç†ã€‚æ­¤å¤–ï¼Œè¿™äº›å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿˜è¡¨ç°å‡ºé€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ è¿›è¡Œå°‘æ ·æœ¬é¢„æµ‹çš„èƒ½åŠ›ï¼Œå…¶ä¸­æ ¹æ®æç¤ºæä¾›çš„è¾“å…¥è¾“å‡ºä¾‹å­å¯ä»¥æ¨å¹¿åˆ°æ–°çš„è¾“å…¥ã€‚è¿™ç§èƒ½åŠ›è¿˜è¶…è¶Šäº†æ ‡å‡†è¯­è¨€ä»»åŠ¡ï¼Œä½¿å°‘æ ·æœ¬å­¦ä¹ èƒ½å¤Ÿåº”ç”¨äºä¸€èˆ¬æ¨¡å¼ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è€ƒè™‘äº†ä½¿ç”¨é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹è¿›è¡Œä¸Šä¸‹æ–‡å­¦ä¹ çš„åº”ç”¨ï¼Œç”¨äºåŠ¨æ€æ“ä½œã€‚åŠ¨æ€æ“ä½œå¸¦æ¥äº†å‡ ä¸ªå…³é”®æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ç»´åº¦å¢åŠ ã€åŠ¨æ€å¤æ‚æ€§å’Œéƒ¨åˆ†å¯è§‚å¯Ÿæ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬é‡‡å–äº†ä¸€ç§è¿­ä»£æ–¹æ³•ï¼Œå¹¶å°†æˆ‘ä»¬çš„ä¸Šä¸‹æ–‡å­¦ä¹ é—®é¢˜åˆ¶å®šä¸ºåŸºäºå…ˆå‰äº¤äº’é¢„æµ‹å‚æ•°ç­–ç•¥çš„è°ƒæ•´ã€‚æˆ‘ä»¬åœ¨æ¨¡æ‹Ÿå’Œå®ä½“æœºå™¨äººä¸Šçš„å¤šä¸ªä»»åŠ¡ä¸­éƒ½è¡¨æ˜ï¼Œåˆ©ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ ä¼˜äºä½æ•°æ®ç¯å¢ƒä¸‹çš„å…¶ä»–æ–¹æ³•ã€‚è¯¥å·¥ä½œå’Œå®éªŒçš„è§†é¢‘æ‘˜è¦å¯ä»¥åœ¨<a target="_blank" rel="noopener" href="https://youtu.be/2inxpdrq74U?si=dAdDYsUEr25nZvRn%E6%89%BE%E5%88%B0%E3%80%82">https://youtu.be/2inxpdrq74U?si=dAdDYsUEr25nZvRnæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>ç®€åŒ–è¯´æ˜ï¼ˆéæ­£å¼ç‰ˆï¼‰</strong>ï¼š</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15021v1">PDF</a> 14 pages. Accepted at CoRL 2025</p>
<p><strong>Summary</strong></p>
<p>é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹å€ŸåŠ©äº’è”ç½‘è§„æ¨¡çš„è¯­æ–™åº“è¿›è¡Œè®­ç»ƒï¼Œå·²æ˜¾ç¤ºå‡ºå“è¶Šçš„æ¨ç†èƒ½åŠ›ï¼Œå¯åœ¨å„ç§è¯­è¨€ä»»åŠ¡ä¸­è¿›è¡Œé€»è¾‘å’Œæ–‡æœ¬æ¨ç†ã€‚å®ƒä»¬èƒ½é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ è¿›è¡Œå°‘é‡æ ·æœ¬é¢„æµ‹ï¼Œè¯¥èƒ½åŠ›é€‚ç”¨äºæ›´ä¸€èˆ¬çš„æ¨¡å¼ã€‚æœ¬æ–‡æ¢è®¨åœ¨åŠ¨æ€æ“æ§åœºæ™¯ä¸‹åº”ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ çš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ã€‚åŠ¨æ€æ“æ§é¢ä¸´é«˜ç»´æ€§ã€å¤æ‚åŠ¨æ€å’Œå±€éƒ¨è§‚æµ‹ç­‰æŒ‘æˆ˜ã€‚é€šè¿‡è¿­ä»£æ–¹æ³•å¹¶é¢„æµ‹åŸºäºä¹‹å‰äº¤äº’çš„å‚æ•°ç­–ç•¥çš„å¾®è°ƒï¼Œæˆ‘ä»¬å‘ç°åœ¨æ¨¡æ‹Ÿä»»åŠ¡å’Œå®é™…æœºå™¨äººä»»åŠ¡ä¸­ï¼Œä½¿ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ çš„æ•ˆæœä¼˜äºä½æ•°æ®ç¯å¢ƒä¸‹çš„å…¶ä»–æ–¹æ³•ã€‚ç›¸å…³è§†é¢‘æ‘˜è¦å’Œå®éªŒå¯é€šè¿‡é“¾æ¥æŸ¥çœ‹ï¼š<a target="_blank" rel="noopener" href="https://youtu.be/2inxpdrq74U?si=dAdDYsUEr25nZvRn">è§†é¢‘é“¾æ¥</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹åœ¨å¤§é‡è¯­è¨€æ•°æ®ä¸Šè®­ç»ƒåï¼Œå±•ç°å‡ºå“è¶Šçš„è¯­è¨€æ¨ç†èƒ½åŠ›ã€‚</li>
<li>è¿™äº›æ¨¡å‹èƒ½é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ è¿›è¡Œå°‘é‡æ ·æœ¬é¢„æµ‹ï¼Œé€‚ç”¨äºå¤šç§ä»»åŠ¡ã€‚</li>
<li>åœ¨åŠ¨æ€æ“æ§åœºæ™¯ä¸­ï¼Œä¸Šä¸‹æ–‡å­¦ä¹ å…·æœ‰åº”ç”¨ä»·å€¼ï¼Œä½†é¢ä¸´é«˜ç»´æ€§ã€å¤æ‚åŠ¨æ€å’Œå±€éƒ¨è§‚æµ‹ç­‰æŒ‘æˆ˜ã€‚</li>
<li>é‡‡ç”¨è¿­ä»£æ–¹æ³•é¢„æµ‹åŸºäºä¹‹å‰äº¤äº’çš„å‚æ•°ç­–ç•¥çš„å¾®è°ƒæ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚</li>
<li>åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®æœºå™¨äººå®éªŒä¸­ï¼Œä¸Šä¸‹æ–‡å­¦ä¹ çš„æ•ˆæœä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½æ•°æ®ç¯å¢ƒä¸‹ã€‚</li>
<li>è§†é¢‘æ‘˜è¦æä¾›äº†å¯¹å·¥ä½œæ–¹æ³•å’Œå®éªŒç»“æœçš„ç›´è§‚å±•ç¤ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15021">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-e78a45f1a9ad2e62ef1c3af12f67cabb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-21108ab8299f08dfc0b92de1f85ef28c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-976f31de346ee78c33c9d10cf491857c.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="NVIDIA-Nemotron-Nano-2-An-Accurate-and-Efficient-Hybrid-Mamba-Transformer-Reasoning-Model"><a href="#NVIDIA-Nemotron-Nano-2-An-Accurate-and-Efficient-Hybrid-Mamba-Transformer-Reasoning-Model" class="headerlink" title="NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid   Mamba-Transformer Reasoning Model"></a>NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid   Mamba-Transformer Reasoning Model</h2><p><strong>Authors: NVIDIA,  :, Aarti Basant, Abhijit Khairnar, Abhijit Paithankar, Abhinav Khattar, Adithya Renduchintala, Aditya Malte, Akhiad Bercovich, Akshay Hazare, Alejandra Rico, Aleksander Ficek, Alex Kondratenko, Alex Shaposhnikov, Alexander Bukharin, Ali Taghibakhshi, Amelia Barton, Ameya Sunil Mahabaleshwarkar, Amy Shen, Andrew Tao, Ann Guan, Anna Shors, Anubhav Mandarwal, Arham Mehta, Arun Venkatesan, Ashton Sharabiani, Ashwath Aithal, Ashwin Poojary, Ayush Dattagupta, Balaram Buddharaju, Banghua Zhu, Barnaby Simkin, Bilal Kartal, Bita Darvish Rouhani, Bobby Chen, Boris Ginsburg, Brandon Norick, Brian Yu, Bryan Catanzaro, Charles Wang, Charlie Truong, Chetan Mungekar, Chintan Patel, Chris Alexiuk, Christian Munley, Christopher Parisien, Dan Su, Daniel Afrimi, Daniel Korzekwa, Daniel Rohrer, Daria Gitman, David Mosallanezhad, Deepak Narayanan, Dima Rekesh, Dina Yared, Dmytro Pykhtar, Dong Ahn, Duncan Riach, Eileen Long, Elliott Ning, Eric Chung, Erick Galinkin, Evelina Bakhturina, Gargi Prasad, Gerald Shen, Haifeng Qian, Haim Elisha, Harsh Sharma, Hayley Ross, Helen Ngo, Herman Sahota, Hexin Wang, Hoo Chang Shin, Hua Huang, Iain Cunningham, Igor Gitman, Ivan Moshkov, Jaehun Jung, Jan Kautz, Jane Polak Scowcroft, Jared Casper, Jian Zhang, Jiaqi Zeng, Jimmy Zhang, Jinze Xue, Jocelyn Huang, Joey Conway, John Kamalu, Jonathan Cohen, Joseph Jennings, Julien Veron Vialard, Junkeun Yi, Jupinder Parmar, Kari Briski, Katherine Cheung, Katherine Luna, Keith Wyss, Keshav Santhanam, Kezhi Kong, Krzysztof Pawelec, Kumar Anik, Kunlun Li, Kushan Ahmadian, Lawrence McAfee, Laya Sleiman, Leon Derczynski, Luis Vega, Maer Rodrigues de Melo, Makesh Narsimhan Sreedhar, Marcin Chochowski, Mark Cai, Markus Kliegl, Marta Stepniewska-Dziubinska, Matvei Novikov, Mehrzad Samadi, Meredith Price, Meriem Boubdir, Michael Boone, Michael Evans, Michal Bien, Michal Zawalski, Miguel Martinez, Mike Chrzanowski, Mohammad Shoeybi, Mostofa Patwary, Namit Dhameja, Nave Assaf, Negar Habibi, Nidhi Bhatia, Nikki Pope, Nima Tajbakhsh, Nirmal Kumar Juluru, Oleg Rybakov, Oleksii Hrinchuk, Oleksii Kuchaiev, Oluwatobi Olabiyi, Pablo Ribalta, Padmavathy Subramanian, Parth Chadha, Pavlo Molchanov, Peter Dykas, Peter Jin, Piotr Bialecki, Piotr Januszewski, Pradeep Thalasta, Prashant Gaikwad, Prasoon Varshney, Pritam Gundecha, Przemek Tredak, Rabeeh Karimi Mahabadi, Rajen Patel, Ran El-Yaniv, Ranjit Rajan, Ria Cheruvu, Rima Shahbazyan, Ritika Borkar, Ritu Gala, Roger Waleffe, Ruoxi Zhang, Russell J. Hewett, Ryan Prenger, Sahil Jain, Samuel Kriman, Sanjeev Satheesh, Saori Kaji, Sarah Yurick, Saurav Muralidharan, Sean Narenthiran, Seonmyeong Bak, Sepehr Sameni, Seungju Han, Shanmugam Ramasamy, Shaona Ghosh, Sharath Turuvekere Sreenivas, Shelby Thomas, Shizhe Diao, Shreya Gopal, Shrimai Prabhumoye, Shubham Toshniwal, Shuoyang Ding, Siddharth Singh, Siddhartha Jain, Somshubra Majumdar, Soumye Singhal, Stefania Alborghetti, Syeda Nahida Akter, Terry Kong, Tim Moon, Tomasz Hliwiak, Tomer Asida, Tony Wang, Tugrul Konuk, Twinkle Vashishth, Tyler Poon, Udi Karpas, Vahid Noroozi, Venkat Srinivasan, Vijay Korthikanti, Vikram Fugro, Vineeth Kalluru, Vitaly Kurin, Vitaly Lavrukhin, Wasi Uddin Ahmad, Wei Du, Wonmin Byeon, Ximing Lu, Xin Dong, Yashaswi Karnati, Yejin Choi, Yian Zhang, Ying Lin, Yonggan Fu, Yoshi Suhara, Zhen Dong, Zhiyu Li, Zhongbo Zhu, Zijia Chen</strong></p>
<p>We introduce Nemotron-Nano-9B-v2, a hybrid Mamba-Transformer language model designed to increase throughput for reasoning workloads while achieving state-of-the-art accuracy compared to similarly-sized models. Nemotron-Nano-9B-v2 builds on the Nemotron-H architecture, in which the majority of the self-attention layers in the common Transformer architecture are replaced with Mamba-2 layers, to achieve improved inference speed when generating the long thinking traces needed for reasoning. We create Nemotron-Nano-9B-v2 by first pre-training a 12-billion-parameter model (Nemotron-Nano-12B-v2-Base) on 20 trillion tokens using an FP8 training recipe. After aligning Nemotron-Nano-12B-v2-Base, we employ the Minitron strategy to compress and distill the model with the goal of enabling inference on up to 128k tokens on a single NVIDIA A10G GPU (22GiB of memory, bfloat16 precision). Compared to existing similarly-sized models (e.g., Qwen3-8B), we show that Nemotron-Nano-9B-v2 achieves on-par or better accuracy on reasoning benchmarks while achieving up to 6x higher inference throughput in reasoning settings like 8k input and 16k output tokens. We are releasing Nemotron-Nano-9B-v2, Nemotron-Nano12B-v2-Base, and Nemotron-Nano-9B-v2-Base checkpoints along with the majority of our pre- and post-training datasets on Hugging Face. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†Nemotron-Nano-9B-v2ï¼Œè¿™æ˜¯ä¸€æ¬¾æ··åˆäº†Mamba-Transformerçš„è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨æé«˜æ¨ç†å·¥ä½œè´Ÿè½½çš„ååé‡ï¼ŒåŒæ—¶ä¸ç±»ä¼¼è§„æ¨¡çš„æ¨¡å‹ç›¸æ¯”å®ç°æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ã€‚Nemotron-Nano-9B-v2å»ºç«‹åœ¨Nemotron-Hæ¶æ„çš„åŸºç¡€ä¸Šï¼Œå°†Transformeræ¶æ„ä¸­å¤§éƒ¨åˆ†çš„è‡ªæ³¨æ„åŠ›å±‚æ›¿æ¢ä¸ºMamba-2å±‚ï¼Œä»è€Œåœ¨ç”Ÿæˆç”¨äºæ¨ç†çš„é•¿æ€è€ƒè½¨è¿¹æ—¶å®ç°æ›´å¿«çš„æ¨ç†é€Ÿåº¦ã€‚æˆ‘ä»¬é€šè¿‡é¦–å…ˆåœ¨20ä¸‡äº¿ä¸ªä»¤ç‰Œä¸Šä½¿ç”¨FP8è®­ç»ƒé…æ–¹é¢„è®­ç»ƒä¸€ä¸ª12äº¿å‚æ•°æ¨¡å‹ï¼ˆNemotron-Nano-12B-v2-Baseï¼‰æ¥åˆ›å»ºNemotron-Nano-9B-v2ã€‚åœ¨å¯¹Nemotron-Nano-12B-v2-Baseè¿›è¡Œå¯¹é½åï¼Œæˆ‘ä»¬é‡‡ç”¨Minitronç­–ç•¥æ¥å‹ç¼©å’Œè’¸é¦æ¨¡å‹ï¼Œæ—¨åœ¨èƒ½å¤Ÿåœ¨å•ä¸ªNVIDIA A10G GPUï¼ˆå…·æœ‰22GBå†…å­˜ï¼Œbfloat16ç²¾åº¦ï¼‰ä¸Šè¿›è¡Œé«˜è¾¾128kä»¤ç‰Œçš„æ¨ç†ã€‚ä¸ç°æœ‰çš„ç±»ä¼¼è§„æ¨¡æ¨¡å‹ï¼ˆä¾‹å¦‚Qwen3-8Bï¼‰ç›¸æ¯”ï¼Œæˆ‘ä»¬åœ¨æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šå±•ç¤ºäº†Nemotron-Nano-9B-v2å®ç°äº†ç›¸å½“çš„æˆ–æ›´å¥½çš„å‡†ç¡®æ€§ï¼ŒåŒæ—¶åœ¨å¦‚8kè¾“å…¥å’Œ16kè¾“å‡ºä»¤ç‰Œçš„æ¨ç†è®¾ç½®ä¸­å®ç°äº†é«˜è¾¾6å€çš„æ¨ç†ååé‡ã€‚æˆ‘ä»¬å°†Nemotron-Nano-9B-v2ã€Nemotron-Nano12B-v2-Baseå’ŒNemotron-Nano-9B-v2-Baseæ£€æŸ¥ç‚¹ä»¥åŠæˆ‘ä»¬çš„å¤§éƒ¨åˆ†é¢„è®­ç»ƒå’Œåç»­è®­ç»ƒæ•°æ®é›†åœ¨Hugging Faceä¸Šå‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14444v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºMamba-Transformeræ¶æ„çš„Nemotron-Nano-9B-v2æ··åˆè¯­è¨€æ¨¡å‹æ—¨åœ¨æé«˜æ¨ç†å·¥ä½œè´Ÿè½½çš„ååé‡ï¼ŒåŒæ—¶ä¸åŒç±»æ¨¡å‹ç›¸æ¯”å®ç°æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ã€‚è¯¥æ¨¡å‹é€šè¿‡é‡‡ç”¨Nemotron-Hæ¶æ„ï¼Œå°†Transformeræ¶æ„ä¸­çš„å¤§éƒ¨åˆ†è‡ªæ³¨æ„åŠ›å±‚æ›¿æ¢ä¸ºMamba-2å±‚ï¼Œä»¥å®ç°åœ¨ç”Ÿæˆæ¨ç†æ‰€éœ€çš„é•¿æ€è€ƒè½¨è¿¹æ—¶çš„æ›´å¿«æ¨ç†é€Ÿåº¦ã€‚é€šè¿‡é¢„è®­ç»ƒä¸€ä¸ª12äº¿å‚æ•°çš„æ¨¡å‹ï¼ˆNemotron-Nano-12B-v2-Baseï¼‰ï¼Œå¹¶åœ¨ä½¿ç”¨FP8è®­ç»ƒé…æ–¹å¤„ç†è¿‡çš„20ä¸‡äº¿ä¸ªä»¤ç‰Œä¸Šå¯¹å…¶è¿›è¡Œè®­ç»ƒï¼Œåˆ›å»ºäº†Nemotron-Nano-9B-v2ã€‚é‡‡ç”¨Minitronç­–ç•¥å¯¹æ¨¡å‹è¿›è¡Œå‹ç¼©å’Œè’¸é¦ï¼Œå¯åœ¨å•ä¸ªNVIDIA A10G GPUä¸Šå®ç°å¯¹é«˜è¾¾128kä»¤ç‰Œçš„æ¨ç†ã€‚ä¸ç°æœ‰ç±»ä¼¼è§„æ¨¡çš„æ¨¡å‹ç›¸æ¯”ï¼ŒNemotron-Nano-9B-v2åœ¨æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†ç›¸å½“æˆ–æ›´å¥½çš„å‡†ç¡®æ€§ï¼ŒåŒæ—¶åœ¨8kè¾“å…¥å’Œ16kè¾“å‡ºä»¤ç‰Œç­‰æ¨ç†ç¯å¢ƒä¸­å®ç°äº†é«˜è¾¾6å€çš„æ¨ç†ååé‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Nemotron-Nano-9B-v2æ˜¯ä¸€ä¸ªæ··åˆMamba-Transformerè¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨æé«˜æ¨ç†å·¥ä½œè´Ÿè½½çš„ååé‡å’Œå‡†ç¡®æ€§ã€‚</li>
<li>è¯¥æ¨¡å‹åŸºäºNemotron-Hæ¶æ„ï¼Œå°†è‡ªæ³¨æ„åŠ›å±‚æ›¿æ¢ä¸ºMamba-2å±‚ä»¥å®ç°æ›´å¿«çš„æ¨ç†é€Ÿåº¦ã€‚</li>
<li>æ¨¡å‹é¢„è®­ç»ƒåœ¨åŒ…å«20ä¸‡äº¿ä»¤ç‰Œçš„å·¨å¤§æ•°æ®é›†ä¸Šè¿›è¡Œï¼Œé‡‡ç”¨äº†FP8è®­ç»ƒé…æ–¹ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨Minitronç­–ç•¥è¿›è¡Œå‹ç¼©å’Œè’¸é¦ï¼Œæ¨¡å‹å¯ä»¥åœ¨å•ä¸ªNVIDIA A10G GPUä¸Šå¤„ç†é«˜è¾¾128kä»¤ç‰Œçš„æ¨ç†ä»»åŠ¡ã€‚</li>
<li>ä¸ç±»ä¼¼è§„æ¨¡çš„æ¨¡å‹ç›¸æ¯”ï¼ŒNemotron-Nano-9B-v2åœ¨æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ã€‚</li>
<li>è¯¥æ¨¡å‹å®ç°äº†é«˜è¾¾6å€çš„æ¨ç†ååé‡ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤§è§„æ¨¡è¾“å…¥å’Œè¾“å‡ºä»¤ç‰Œæ—¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14444">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-55f97bf17c1a24b75a1fbc40b0cbf9ad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7fa504cec80d675359af61cce57535a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa73dae471819e017a5cc665c1f7aee0.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="ThinkTuning-Instilling-Cognitive-Reflections-without-Distillation"><a href="#ThinkTuning-Instilling-Cognitive-Reflections-without-Distillation" class="headerlink" title="ThinkTuning: Instilling Cognitive Reflections without Distillation"></a>ThinkTuning: Instilling Cognitive Reflections without Distillation</h2><p><strong>Authors:Aswin RRV, Jacob Dineen, Divij Handa, Md Nayem Uddin, Mihir Parmar, Chitta Baral, Ben Zhou</strong></p>
<p>Recent advances in test-time scaling have led to the emergence of thinking LLMs that exhibit self-reflective behaviors and multi-step reasoning. While RL drives this self-improvement paradigm, a recent study (Gandhi et al., 2025) shows that RL alone does not truly instill these new reasoning abilities - it merely draws out behaviors already present in the base models. This raises a question: How can we train the models that donâ€™t exhibit such thinking behavior to develop it in the first place? To this end, we propose ThinkTuning, a GRPO-based interactive training approach where we augment the rollouts of a student model with the guidance from a teacher model. A simple idea from classroom practice inspires our method: a teacher poses a problem, lets the student try an answer, then gives corrective feedback â€“ enough to point the mind in the right direction and then show the solution. Each piece of feedback reshapes the studentâ€™s thoughts, leading them to arrive at the correct solution. Similarly, we find that this type of implicit supervision through feedback from a teacher model of the same size improves the reasoning capabilities of the student model. In particular, on average, our method shows a 3.85% improvement over zero-shot baselines across benchmarks, and on MATH-500, AIME and GPQA-Diamond it shows 2.08%, 2.23% and 3.99% improvements over the vanilla-GRPO baseline. Source code is available at <a target="_blank" rel="noopener" href="https://github.com/3rdAT/ThinkTuning">https://github.com/3rdAT/ThinkTuning</a>. </p>
<blockquote>
<p>è¿‘æœŸæµ‹è¯•æ—¶é—´ç¼©æ”¾æŠ€æœ¯çš„è¿›å±•ï¼Œå‚¬ç”Ÿå‡ºäº†ä¸€ç§å…·å¤‡è‡ªæˆ‘åæ€è¡Œä¸ºå’Œè·¨æ­¥éª¤æ¨ç†èƒ½åŠ›çš„æ€è€ƒå¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ã€‚è™½ç„¶å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¨åŠ¨äº†è¿™ç§è‡ªæˆ‘æ”¹è¿›æ¨¡å¼çš„å‘å±•ï¼Œä½†æœ€è¿‘çš„ä¸€é¡¹ç ”ç©¶ï¼ˆGandhiç­‰äººï¼Œ2025ï¼‰è¡¨æ˜ï¼Œå•çº¯ä¾é RLå¹¶ä¸èƒ½çœŸæ­£èµ‹äºˆè¿™äº›æ–°çš„æ¨ç†èƒ½åŠ›â€”â€”å®ƒåªæ˜¯æ¿€å‘äº†åœ¨åŸºç¡€æ¨¡å‹ä¸­å·²ç»å­˜åœ¨çš„è¡Œä¸ºã€‚è¿™å°±äº§ç”Ÿäº†ä¸€ä¸ªé—®é¢˜ï¼šå¦‚ä½•è®­ç»ƒé‚£äº›æ²¡æœ‰è¿™ç§æ€ç»´è¡Œä¸ºçš„æ¨¡å‹ï¼Œè®©å®ƒä»¬é¦–å…ˆå…·å¤‡è¿™ç§èƒ½åŠ›å‘¢ï¼Ÿä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ThinkTuningï¼Œè¿™æ˜¯ä¸€ç§åŸºäºGRPOçš„äº¤äº’å¼è®­ç»ƒæ–¹æ³•ï¼Œæˆ‘ä»¬é€šè¿‡åœ¨å­¦å‘˜æ¨¡å‹çš„rolloutsä¸­åŠ å…¥æ•™å¸ˆæ¨¡å‹çš„æŒ‡å¯¼æ¥å¢å¼ºå…¶åŠŸèƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•çµæ„Ÿæ¥è‡ªäºè¯¾å ‚å®è·µçš„ç®€å•æƒ³æ³•ï¼šæ•™å¸ˆæå‡ºä¸€ä¸ªé—®é¢˜ï¼Œè®©å­¦ç”Ÿå°è¯•å›ç­”ï¼Œç„¶åç»™å‡ºçº æ­£åé¦ˆï¼Œè¶³ä»¥æŒ‡å¼•æ­£ç¡®çš„æ–¹å‘å¹¶å±•ç¤ºè§£å†³æ–¹æ¡ˆã€‚æ¯ä¸€ä»½åé¦ˆéƒ½ä¼šé‡å¡‘å­¦ç”Ÿçš„æ€è·¯ï¼Œå¼•å¯¼ä»–ä»¬æ‰¾åˆ°æ­£ç¡®çš„ç­”æ¡ˆã€‚åŒæ ·ï¼Œæˆ‘ä»¬å‘ç°ï¼Œé€šè¿‡æ•™å¸ˆæ¨¡å‹çš„åé¦ˆè¿›è¡Œè¿™ç§éšå¼ç›‘ç£ï¼Œèƒ½å¤Ÿæ”¹å–„å­¦å‘˜æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚å°¤å…¶æ˜¯æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¹³å‡åŸºå‡†æµ‹è¯•ä¸Šæ¯”é›¶åŸºå‡†çº¿é«˜å‡º3.85%çš„æ”¹è¿›ã€‚åœ¨MATH-500ã€AIMEå’ŒGPQA-Diamondä¸Šï¼Œç›¸å¯¹äºæ™®é€šçš„GRPOåŸºçº¿ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åˆ†åˆ«å®ç°äº†2.08%ã€2.23%å’Œ3.99%çš„æ”¹è¿›ã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/3rdAT/ThinkTuning">https://github.com/3rdAT/ThinkTuning</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07616v2">PDF</a> EMNLP 2025 (Main Conference)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è¿‘æœŸæµ‹è¯•æ—¶é—´æ‰©å±•æŠ€æœ¯çš„è¿›æ­¥æ¨åŠ¨äº†å…·å¤‡è‡ªæˆ‘åæ€è¡Œä¸ºå’Œè·¨æ­¥æ¨ç†çš„æ€è€ƒå¼LLMçš„å‡ºç°ã€‚ç„¶è€Œï¼Œç ”ç©¶å‘ç°ä»…é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¹¶ä¸èƒ½çœŸæ­£èµ‹äºˆæ¨¡å‹è¿™äº›æ–°æ¨ç†èƒ½åŠ›ï¼Œåªèƒ½è°ƒåŠ¨åŸºç¡€æ¨¡å‹ä¸­å·²æœ‰çš„èƒ½åŠ›ã€‚å› æ­¤ï¼Œæå‡ºThinkTuningè®­ç»ƒæ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºGRPOçš„äº¤äº’å¼è®­ç»ƒæ–¹æ³•ï¼Œé€šè¿‡æ•™å¸ˆæ¨¡å‹çš„æŒ‡å¯¼æ¥å¢å¼ºå­¦ç”Ÿæ¨¡å‹çš„rolloutsã€‚è¯¥æ–¹æ³•å—åˆ°è¯¾å ‚å®è·µçš„å¯å‘ï¼Œé€šè¿‡æ•™å¸ˆæå‡ºé—®é¢˜å’Œæä¾›åé¦ˆï¼Œå¼•å¯¼å­¦ç”Ÿæ€è€ƒå¹¶æ‰¾åˆ°æ­£ç¡®ç­”æ¡ˆã€‚åŒæ ·åœ°ï¼Œæœ¬æ–‡é€šè¿‡æ•™å¸ˆæ¨¡å‹ç»™äºˆåŒæ ·è§„æ¨¡çš„éšå¼ç›‘ç£åé¦ˆï¼Œæå‡äº†å­¦ç”Ÿæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚ThinkTuningæ–¹æ³•ç›¸è¾ƒäºé›¶åŸºå‡†çº¿å¹³å‡æå‡äº†3.85%ï¼Œåœ¨MATH-500ã€AIMEå’ŒGPQA-Diamondä»»åŠ¡ä¸Šåˆ†åˆ«æå‡äº†2.08%ã€2.23%å’Œ3.99%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æµ‹è¯•æ—¶é—´æ‰©å±•æŠ€æœ¯çš„è¿›å±•æ¨åŠ¨äº†æ€è€ƒå¼LLMçš„å‡ºç°ï¼Œè¿™äº›æ¨¡å‹å±•ç°å‡ºè‡ªæˆ‘åæ€è¡Œä¸ºå’Œè·¨æ­¥æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¹¶ä¸èƒ½çœŸæ­£èµ‹äºˆæ¨¡å‹æ–°çš„æ¨ç†èƒ½åŠ›ï¼Œè€Œåªèƒ½è°ƒåŠ¨åŸºç¡€æ¨¡å‹å·²æœ‰çš„èƒ½åŠ›ã€‚</li>
<li>æå‡ºThinkTuningè®­ç»ƒæ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºGRPOçš„äº¤äº’å¼è®­ç»ƒæ–¹å¼ï¼Œæ—¨åœ¨é€šè¿‡æ•™å¸ˆæ¨¡å‹çš„æŒ‡å¯¼æ¥æå‡å­¦ç”Ÿæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ThinkTuningæ–¹æ³•å—åˆ°è¯¾å ‚å®è·µçš„å¯å‘ï¼Œé€šè¿‡æ•™å¸ˆçš„åé¦ˆæ¥å¼•å¯¼å­¦ç”Ÿæ€è€ƒå¹¶æ‰¾åˆ°é—®é¢˜çš„ç­”æ¡ˆã€‚</li>
<li>æ•™å¸ˆæ¨¡å‹ç»™äºˆéšå¼ç›‘ç£åé¦ˆï¼Œæœ‰åŠ©äºæå‡å­¦ç”Ÿæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ThinkTuningæ–¹æ³•ç›¸è¾ƒäºé›¶åŸºå‡†çº¿å¹³å‡æå‡äº†3.85%çš„æ¨ç†èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07616">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-4a27e03da626dce684e6ddb82f248df1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ccd28137d612c00d12112dd39483cb29.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-23/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-23/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-23/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-f1ff94757caca643ac10254ed3e86f94.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-23  End-to-End Agentic RAG System Training for Traceable Diagnostic   Reasoning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-23
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-23/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-4a1220d90ca9cc844fe9d58293966c16.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-23  DIFFA Large Language Diffusion Models Can Listen and Understand
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-23
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">26551.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
