<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-23  End-to-End Agentic RAG System Training for Traceable Diagnostic   Reasoning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-f1ff94757caca643ac10254ed3e86f94.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-23
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-08
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    13.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    55 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-23-æ›´æ–°"><a href="#2025-08-23-æ›´æ–°" class="headerlink" title="2025-08-23 æ›´æ–°"></a>2025-08-23 æ›´æ–°</h1><h2 id="End-to-End-Agentic-RAG-System-Training-for-Traceable-Diagnostic-Reasoning"><a href="#End-to-End-Agentic-RAG-System-Training-for-Traceable-Diagnostic-Reasoning" class="headerlink" title="End-to-End Agentic RAG System Training for Traceable Diagnostic   Reasoning"></a>End-to-End Agentic RAG System Training for Traceable Diagnostic   Reasoning</h2><p><strong>Authors:Qiaoyu Zheng, Yuze Sun, Chaoyi Wu, Weike Zhao, Pengcheng Qiu, Yongguo Yu, Kun Sun, Yanfeng Wang, Ya Zhang, Weidi Xie</strong></p>
<p>Accurate diagnosis with medical large language models is hindered by knowledge gaps and hallucinations. Retrieval and tool-augmented methods help, but their impact is limited by weak use of external knowledge and poor feedback-reasoning traceability. To address these challenges, We introduce Deep-DxSearch, an agentic RAG system trained end-to-end with reinforcement learning (RL) that enables steer tracebale retrieval-augmented reasoning for medical diagnosis. In Deep-DxSearch, we first construct a large-scale medical retrieval corpus comprising patient records and reliable medical knowledge sources to support retrieval-aware reasoning across diagnostic scenarios. More crutially, we frame the LLM as the core agent and the retrieval corpus as its environment, using tailored rewards on format, retrieval, reasoning structure, and diagnostic accuracy, thereby evolving the agentic RAG policy from large-scale data through RL.   Experiments demonstrate that our end-to-end agentic RL training framework consistently outperforms prompt-engineering and training-free RAG approaches across multiple data centers. After training, Deep-DxSearch achieves substantial gains in diagnostic accuracy, surpassing strong diagnostic baselines such as GPT-4o, DeepSeek-R1, and other medical-specific frameworks for both common and rare disease diagnosis under in-distribution and out-of-distribution settings. Moreover, ablation studies on reward design and retrieval corpus components confirm their critical roles, underscoring the uniqueness and effectiveness of our approach compared with traditional implementations. Finally, case studies and interpretability analyses highlight improvements in Deep-DxSearchâ€™s diagnostic policy, providing deeper insight into its performance gains and supporting clinicians in delivering more reliable and precise preliminary diagnoses. See <a target="_blank" rel="noopener" href="https://github.com/MAGIC-AI4Med/Deep-DxSearch">https://github.com/MAGIC-AI4Med/Deep-DxSearch</a>. </p>
<blockquote>
<p>ä½¿ç”¨åŒ»ç–—å¤§å‹è¯­è¨€æ¨¡å‹çš„å‡†ç¡®è¯Šæ–­å—åˆ°çŸ¥è¯†å·®è·å’Œå¹»è§‰çš„é˜»ç¢ã€‚æ£€ç´¢å’Œå·¥å…·å¢å¼ºæ–¹æ³•æœ‰æ‰€å¸®åŠ©ï¼Œä½†å…¶å½±å“å—é™äºå¤–éƒ¨çŸ¥è¯†åˆ©ç”¨ä¸è¶³å’Œåé¦ˆæ¨ç†å¯è¿½æº¯æ€§è¾ƒå·®ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†Deep-DxSearchï¼Œè¿™æ˜¯ä¸€ä¸ªä»¥å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒçš„ä¸»åŠ¨æ¨ç†å›¾ç³»ç»Ÿï¼ˆRAGï¼‰ï¼Œå¯å®ç°ç”¨äºåŒ»å­¦è¯Šæ–­çš„å¯è¿½æº¯æ£€ç´¢å¢å¼ºæ¨ç†ã€‚åœ¨Deep-DxSearchä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆæ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡åŒ»å­¦æ£€ç´¢è¯­æ–™åº“ï¼ŒåŒ…å«æ‚£è€…è®°å½•å’Œå¯é åŒ»å­¦çŸ¥è¯†æºï¼Œä»¥æ”¯æŒè·¨è¯Šæ–­åœºæ™¯çš„æ£€ç´¢æ„ŸçŸ¥æ¨ç†ã€‚æ›´ä¸ºå…³é”®çš„æ˜¯ï¼Œæˆ‘ä»¬å°†å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºæ ¸å¿ƒä¸»ä½“ï¼Œå°†æ£€ç´¢è¯­æ–™åº“ä½œä¸ºå…¶ç¯å¢ƒï¼Œé’ˆå¯¹æ ¼å¼ã€æ£€ç´¢ã€æ¨ç†ç»“æ„å’Œè¯Šæ–­å‡†ç¡®æ€§è®¾è®¡å®šåˆ¶å¥–åŠ±ï¼Œä»è€Œé€šè¿‡å¼ºåŒ–å­¦ä¹ æ¨åŠ¨ä¸»åŠ¨æ¨ç†å›¾ç­–ç•¥ä»å¤§è§„æ¨¡æ•°æ®ä¸­è¿›åŒ–ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç«¯åˆ°ç«¯ä¸»åŠ¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ¡†æ¶åœ¨å¤šæ•°æ®ä¸­å¿ƒå‡è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¿‡åŸºäºæç¤ºå’Œå…è®­ç»ƒRAGæ–¹æ³•ã€‚ç»è¿‡è®­ç»ƒåï¼ŒDeep-DxSearchåœ¨è¯Šæ–­å‡†ç¡®æ€§æ–¹é¢å–å¾—äº†å®è´¨æ€§æå‡ï¼Œåœ¨åˆ†å¸ƒå†…å’Œåˆ†å¸ƒå¤–è®¾ç½®ä¸‹å‡è¶…è¶Šäº†å¼ºå¤§çš„è¯Šæ–­åŸºçº¿ï¼Œå¦‚GPT-4oã€DeepSeek-R1å’Œå…¶ä»–åŒ»å­¦ä¸“ç”¨æ¡†æ¶ï¼Œæ— è®ºæ˜¯å¸¸è§ç–¾ç—…è¿˜æ˜¯ç½•è§ç–¾ç—…çš„è¯Šæ–­å‡å¦‚æ­¤ã€‚æ­¤å¤–ï¼Œå¯¹å¥–åŠ±è®¾è®¡å’Œæ£€ç´¢è¯­æ–™åº“ç»„ä»¶çš„æ¶ˆèç ”ç©¶è¯å®äº†å®ƒä»¬çš„å…³é”®ä½œç”¨ï¼Œå¼ºè°ƒäº†æˆ‘ä»¬çš„æ–¹æ³•ä¸ä¼ ç»Ÿå®æ–½ç›¸æ¯”çš„ç‹¬ç‰¹æ€§å’Œæœ‰æ•ˆæ€§ã€‚æœ€åï¼Œæ¡ˆä¾‹ç ”ç©¶å’Œå¯è§£é‡Šæ€§åˆ†æçªå‡ºäº†Deep-DxSearchè¯Šæ–­ç­–ç•¥çš„æ”¹è¿›ï¼Œä¸ºæ·±å…¥äº†è§£å…¶æ€§èƒ½æå‡æä¾›äº†ä¾æ®ï¼Œå¹¶æ”¯æŒä¸´åºŠåŒ»ç”Ÿæä¾›æ›´å¯é å’Œç²¾ç¡®çš„æ—©æœŸè¯Šæ–­ã€‚æ›´å¤šä¿¡æ¯è¯·å‚è§<a target="_blank" rel="noopener" href="https://github.com/MAGIC-AI4Med/DeepDxSearch%E3%80%82">https://github.com/MAGIC-AI4Med/DeepDxSearchã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15746v1">PDF</a> 35 pages, 5 figures, 3 tables</p>
<p><strong>Summary</strong></p>
<p>åœ¨åŒ»ç–—å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œå‡†ç¡®è¯Šæ–­æ—¶ï¼Œå­˜åœ¨çŸ¥è¯†ç¼ºå£å’Œè™šæ„é—®é¢˜ã€‚è™½ç„¶æ£€ç´¢å’Œå·¥å…·å¢å¼ºæ–¹æ³•æœ‰æ‰€å¸®åŠ©ï¼Œä½†å®ƒä»¬å¯¹å¤–éƒ¨çŸ¥è¯†åˆ©ç”¨ä¸è¶³ï¼Œåé¦ˆæ¨ç†å¯è¿½æº¯æ€§ä¸ä½³ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºDeep-DxSearchï¼Œä¸€ä¸ªç»è¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„è‡ªä¸»RAGç³»ç»Ÿï¼Œå¯å®ç°å¯è¿½è¸ªæ£€ç´¢å¢å¼ºæ¨ç†ï¼Œç”¨äºåŒ»ç–—è¯Šæ–­ã€‚Deep-DxSearchæ„å»ºå¤§è§„æ¨¡åŒ»å­¦æ£€ç´¢è¯­æ–™åº“ï¼Œæ¶µç›–æ‚£è€…è®°å½•å’Œå¯é åŒ»å­¦çŸ¥è¯†æºï¼Œä»¥æ”¯æŒè·¨è¯Šæ–­åœºæ™¯çš„æ£€ç´¢æ„ŸçŸ¥æ¨ç†ã€‚å…³é”®çš„æ˜¯ï¼Œæˆ‘ä»¬ä»¥LLMä¸ºæ ¸å¿ƒä»£ç†ï¼Œæ£€ç´¢è¯­æ–™åº“ä¸ºå…¶ç¯å¢ƒï¼Œä½¿ç”¨æ ¼å¼ã€æ£€ç´¢ã€æ¨ç†ç»“æ„å’Œè¯Šæ–­å‡†ç¡®æ€§çš„å®šåˆ¶å¥–åŠ±ï¼Œé€šè¿‡å¤§è§„æ¨¡æ•°æ®å¼ºåŒ–ä»£ç†ç­–ç•¥ã€‚å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„ç»ˆç«¯åˆ°ç»ˆç«¯è‡ªä¸»RLè®­ç»ƒæ¡†æ¶åœ¨å¤šä¸ªæ•°æ®ä¸­å¿ƒå‡ä¼˜äºæç¤ºå·¥ç¨‹å’Œå…è®­ç»ƒRAGæ–¹æ³•ã€‚Deep-DxSearchåœ¨è®­ç»ƒåè¯Šæ–­å‡†ç¡®æ€§å¤§å¤§æé«˜ï¼Œè¶…è¶Šå¼ºå¤§çš„è¯Šæ–­åŸºçº¿ï¼Œå¦‚GPT-4oã€DeepSeek-R1å’Œå…¶ä»–åŒ»ç–—ç‰¹å®šæ¡†æ¶ï¼Œé€‚ç”¨äºå¸¸è§å’Œç½•è§ç–¾ç—…çš„è¯Šæ–­ï¼Œåœ¨å†…éƒ¨å’Œå¤–éƒ¨åˆ†å¸ƒè®¾ç½®ä¸‹å‡è¡¨ç°ä¼˜å¼‚ã€‚å¥–åŠ±è®¾è®¡å’Œæ£€ç´¢è¯­æ–™åº“ç»„ä»¶çš„æ¶ˆèç ”ç©¶è¯å®äº†å®ƒä»¬çš„é‡è¦æ€§ï¼Œçªæ˜¾äº†ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”çš„ç‹¬ç‰¹æ€§å’Œæœ‰æ•ˆæ€§ã€‚æœ€ç»ˆï¼Œæ¡ˆä¾‹ç ”ç©¶å’Œè§£é‡Šæ€§åˆ†æçªæ˜¾äº†Deep-DxSearchè¯Šæ–­ç­–ç•¥çš„æ”¹è¿›ï¼Œä¸ºæ€§èƒ½æå‡æä¾›äº†æ·±å…¥äº†è§£ï¼Œå¹¶æ”¯æŒåŒ»ç”Ÿæä¾›æ›´å¯é å’Œç²¾ç¡®çš„æ—©æœŸè¯Šæ–­ã€‚è¯¦æƒ…è§[ç½‘ç«™é“¾æ¥]ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»ç–—å¤§è¯­è¨€æ¨¡å‹åœ¨è¯Šæ–­ä¸­é¢ä¸´çŸ¥è¯†ç¼ºå£å’Œè™šæ„æŒ‘æˆ˜ã€‚</li>
<li>Deep-DxSearché€šè¿‡æ„å»ºå¤§è§„æ¨¡åŒ»å­¦æ£€ç´¢è¯­æ–™åº“æ¥æ”¯æŒè·¨è¯Šæ–­åœºæ™¯çš„æ£€ç´¢æ„ŸçŸ¥æ¨ç†ã€‚</li>
<li>LLMä½œä¸ºæ ¸å¿ƒä»£ç†ï¼Œä¸æ£€ç´¢è¯­æ–™åº“äº’åŠ¨ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›è¡Œè®­ç»ƒã€‚</li>
<li>Deep-DxSearchåœ¨å¤šç§è®¾ç½®ä¸­è¶…è¶Šäº†å…¶ä»–è¯Šæ–­å’ŒåŒ»å­¦ç‰¹å®šæ¡†æ¶ã€‚</li>
<li>æ¶ˆèç ”ç©¶è¯å®äº†å¥–åŠ±è®¾è®¡å’Œæ£€ç´¢è¯­æ–™åº“ç»„ä»¶çš„é‡è¦æ€§ã€‚</li>
<li>Deep-DxSearchæ”¹è¿›äº†è¯Šæ–­ç­–ç•¥ï¼Œæé«˜äº†è¯Šæ–­å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15746">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d1bb8f786cad4e2d5361a4b48148892a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e16115757b3c0fd580e293a4e959ec92.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a888b967c7d1c1b107fd8d54ea7bf485.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="EcomMMMU-Strategic-Utilization-of-Visuals-for-Robust-Multimodal-E-Commerce-Models"><a href="#EcomMMMU-Strategic-Utilization-of-Visuals-for-Robust-Multimodal-E-Commerce-Models" class="headerlink" title="EcomMMMU: Strategic Utilization of Visuals for Robust Multimodal   E-Commerce Models"></a>EcomMMMU: Strategic Utilization of Visuals for Robust Multimodal   E-Commerce Models</h2><p><strong>Authors:Xinyi Ling, Hanwen Du, Zhihui Zhu, Xia Ning</strong></p>
<p>E-commerce platforms are rich in multimodal data, featuring a variety of images that depict product details. However, this raises an important question: do these images always enhance product understanding, or can they sometimes introduce redundancy or degrade performance? Existing datasets are limited in both scale and design, making it difficult to systematically examine this question. To this end, we introduce EcomMMMU, an e-commerce multimodal multitask understanding dataset with 406,190 samples and 8,989,510 images. EcomMMMU is comprised of multi-image visual-language data designed with 8 essential tasks and a specialized VSS subset to benchmark the capability of multimodal large language models (MLLMs) to effectively utilize visual content. Analysis on EcomMMMU reveals that product images do not consistently improve performance and can, in some cases, degrade it. This indicates that MLLMs may struggle to effectively leverage rich visual content for e-commerce tasks. Building on these insights, we propose SUMEI, a data-driven method that strategically utilizes multiple images via predicting visual utilities before using them for downstream tasks. Comprehensive experiments demonstrate the effectiveness and robustness of SUMEI. The data and code are available through <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/submission25">https://anonymous.4open.science/r/submission25</a>. </p>
<blockquote>
<p>ç”µå­å•†åŠ¡å¹³å°æ‹¥æœ‰ä¸°å¯Œçš„å¤šæ¨¡æ€æ•°æ®ï¼Œå…¶ä¸­åŒ…å«äº†å±•ç¤ºäº§å“ç»†èŠ‚çš„å„ç§å›¾åƒã€‚ç„¶è€Œï¼Œè¿™å¼•å‘äº†ä¸€ä¸ªé‡è¦é—®é¢˜ï¼šè¿™äº›å›¾åƒæ˜¯å¦æ€»æ˜¯èƒ½å¢å¼ºå¯¹äº§å“çš„ç†è§£ï¼Œè¿˜æ˜¯æœ‰æ—¶ä¼šå¼•å…¥å†—ä½™ä¿¡æ¯æˆ–é™ä½æ€§èƒ½ï¼Ÿç°æœ‰æ•°æ®é›†åœ¨è§„æ¨¡å’Œè®¾è®¡ä¸Šéƒ½å­˜åœ¨å±€é™æ€§ï¼Œå¾ˆéš¾ç³»ç»Ÿåœ°ç ”ç©¶è¿™ä¸ªé—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†EcomMMMUç”µå­å•†åŠ¡å¤šæ¨¡æ€å¤šä»»åŠ¡ç†è§£æ•°æ®é›†ï¼ŒåŒ…å«406,190ä¸ªæ ·æœ¬å’Œ8,989,510å¼ å›¾åƒã€‚EcomMMMUç”±å¤šå›¾åƒè§†è§‰è¯­è¨€æ•°æ®ç»„æˆï¼Œè®¾è®¡äº†8ä¸ªåŸºæœ¬ä»»åŠ¡ï¼Œå¹¶æœ‰ä¸€ä¸ªä¸“é—¨çš„VSSå­é›†ï¼Œä»¥è¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æœ‰æ•ˆåˆ©ç”¨è§†è§‰å†…å®¹çš„èƒ½åŠ›ã€‚å¯¹EcomMMMUçš„åˆ†æè¡¨æ˜ï¼Œäº§å“å›¾åƒå¹¶ä¸æ€»èƒ½æé«˜æ€§èƒ½ï¼Œæœ‰æ—¶ç”šè‡³ä¼šé™ä½æ€§èƒ½ã€‚è¿™è¡¨æ˜MLLMsåœ¨æœ‰æ•ˆåˆ©ç”¨ä¸°å¯Œçš„è§†è§‰å†…å®¹æ–¹é¢å¯èƒ½é¢ä¸´å›°éš¾ã€‚åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†SUMEIæ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡é¢„æµ‹è§†è§‰æ•ˆç”¨ï¼Œæœ‰é€‰æ‹©åœ°åˆ©ç”¨å¤šä¸ªå›¾åƒï¼Œç„¶åå†å°†å…¶ç”¨äºä¸‹æ¸¸ä»»åŠ¡ã€‚å¤§é‡çš„å®éªŒè¯æ˜äº†SUMEIçš„æœ‰æ•ˆæ€§å’Œç¨³å¥æ€§ã€‚ç›¸å…³æ•°æ®å’Œä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/submission25">åŒ¿åç½‘å€é“¾æ¥</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15721v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>     ç”µå•†å¹³å°ä¸Šä¸°å¯Œçš„å¤šæ¨¡æ€æ•°æ®å«æœ‰å¤§é‡å•†å“å›¾ç‰‡å±•ç¤ºä¿¡æ¯ã€‚ä½†é—®é¢˜åœ¨äºï¼Œè¿™äº›å›¾ç‰‡æ˜¯å¦æ€»æ˜¯èƒ½å¢å¼ºå¯¹å•†å“çš„ç†è§£ï¼Œæˆ–è€…æ˜¯å¦ä¼šå¼•å…¥å†—ä½™ä¿¡æ¯ã€é™ä½æ€§èƒ½ï¼Ÿç°æœ‰æ•°æ®é›†åœ¨è§„æ¨¡å’Œè®¾è®¡ä¸Šçš„å±€é™æ€§ï¼Œä½¿å¾—éš¾ä»¥ç³»ç»Ÿåœ°ç ”ç©¶è¿™ä¸€é—®é¢˜ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†EcomMMMUç”µå•†å¤šæ¨¡æ€å¤šä»»åŠ¡ç†è§£æ•°æ®é›†ï¼ŒåŒ…å«40ä¸‡6åƒå¤šæ¡æ ·æœ¬å’Œè¶…è¿‡å…«ç™¾ä¸‡å¼ å›¾ç‰‡ã€‚å®ƒæ¶µç›–äº†å¤šå›¾åƒè§†è§‰è¯­è¨€æ•°æ®è®¾è®¡çš„å…«ä¸ªåŸºæœ¬ä»»åŠ¡å’Œä¸€ä¸ªä¸“é—¨çš„VSSå­é›†ï¼Œæ—¨åœ¨è¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹æœ‰æ•ˆåˆ©ç”¨è§†è§‰å†…å®¹çš„èƒ½åŠ›ã€‚å¯¹EcomMMMUçš„åˆ†ææ˜¾ç¤ºï¼Œå•†å“å›¾ç‰‡å¹¶ä¸æ€»æ˜¯èƒ½æé«˜æ€§èƒ½ï¼Œæœ‰æ—¶ç”šè‡³ä¼šé™ä½æ€§èƒ½ã€‚è¿™è¡¨æ˜å¤§å‹è¯­è¨€æ¨¡å‹å¯èƒ½éš¾ä»¥æœ‰æ•ˆåœ°åˆ©ç”¨ä¸°å¯Œçš„è§†è§‰å†…å®¹å®Œæˆç”µå•†ä»»åŠ¡ã€‚åŸºäºæ­¤æ´å¯Ÿï¼Œæˆ‘ä»¬æå‡ºSUMEIæ–¹æ³•ï¼Œå®ƒé€šè¿‡é¢„æµ‹è§†è§‰æ•ˆç”¨ï¼Œæœ‰é’ˆå¯¹æ€§åœ°ä½¿ç”¨å¤šå¼ å›¾ç‰‡æ¥å®Œæˆä¸‹æ¸¸ä»»åŠ¡ã€‚ç»è¿‡ç»¼åˆå®éªŒéªŒè¯ï¼ŒSUMEIå…·æœ‰æœ‰æ•ˆæ€§å’Œç¨³å¥æ€§ã€‚ç›¸å…³æ•°æ®å’Œä»£ç å¯é€šè¿‡é“¾æ¥<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/submission25%E8%8E%B7%E5%8F%96%E3%80%82">https://anonymous.4open.science/r/submission25è·å–ã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç”µå•†å¹³å°ä¸Šå•†å“å›¾ç‰‡ä¸°å¯Œï¼Œä½†å›¾ç‰‡æ˜¯å¦æœ‰åŠ©äºå¢å¼ºå¯¹å•†å“çš„ç†è§£å°šå­˜ç–‘é—®ã€‚</li>
<li>å½“å‰æ•°æ®é›†åœ¨è§„æ¨¡å’Œè®¾è®¡ä¸Šçš„å±€é™æ€§ï¼Œä½¿å¾—éš¾ä»¥ç³»ç»Ÿåœ°ç ”ç©¶è¿™ä¸€é—®é¢˜ã€‚</li>
<li>å¼•å…¥EcomMMMUæ•°æ®é›†ï¼ŒåŒ…å«å¤šæ¨¡æ€å¤šä»»åŠ¡è®¾è®¡ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åˆ©ç”¨è§†è§‰å†…å®¹çš„èƒ½åŠ›ã€‚</li>
<li>åˆ†ææ˜¾ç¤ºå•†å“å›¾ç‰‡å¹¶ä¸æ€»æ˜¯èƒ½æé«˜æ€§èƒ½ï¼Œæœ‰æ—¶å¯èƒ½é™ä½æ€§èƒ½ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æœ‰æ•ˆåˆ©ç”¨ä¸°å¯Œè§†è§‰å†…å®¹æ–¹é¢å¯èƒ½å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºSUMEIæ–¹æ³•ï¼Œé€šè¿‡é¢„æµ‹è§†è§‰æ•ˆç”¨æœ‰é’ˆå¯¹æ€§åœ°ä½¿ç”¨å¤šå¼ å›¾ç‰‡å®Œæˆä¸‹æ¸¸ä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15721">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cc6f3a0df8d7ffc3eb53ed50dd381849.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-06443c7ec872e7293906bf46a2981f28.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e31de151d189e08cf560c857cdceffbd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ea205949a6fc9405c318fb3dba6bb1f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-842970e813385bd35daa9198ee2c16e5.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="StreamMem-Query-Agnostic-KV-Cache-Memory-for-Streaming-Video-Understanding"><a href="#StreamMem-Query-Agnostic-KV-Cache-Memory-for-Streaming-Video-Understanding" class="headerlink" title="StreamMem: Query-Agnostic KV Cache Memory for Streaming Video   Understanding"></a>StreamMem: Query-Agnostic KV Cache Memory for Streaming Video   Understanding</h2><p><strong>Authors:Yanlai Yang, Zhuokai Zhao, Satya Narayan Shukla, Aashu Singh, Shlok Kumar Mishra, Lizhu Zhang, Mengye Ren</strong></p>
<p>Multimodal large language models (MLLMs) have made significant progress in visual-language reasoning, but their ability to efficiently handle long videos remains limited. Despite recent advances in long-context MLLMs, storing and attending to the key-value (KV) cache for long visual contexts incurs substantial memory and computational overhead. Existing visual compression methods require either encoding the entire visual context before compression or having access to the questions in advance, which is impractical for long video understanding and multi-turn conversational settings. In this work, we propose StreamMem, a query-agnostic KV cache memory mechanism for streaming video understanding. Specifically, StreamMem encodes new video frames in a streaming manner, compressing the KV cache using attention scores between visual tokens and generic query tokens, while maintaining a fixed-size KV memory to enable efficient question answering (QA) in memory-constrained, long-video scenarios. Evaluation on three long video understanding and two streaming video question answering benchmarks shows that StreamMem achieves state-of-the-art performance in query-agnostic KV cache compression and is competitive with query-aware compression approaches. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰è¯­è¨€æ¨ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å…¶é«˜æ•ˆå¤„ç†é•¿è§†é¢‘çš„èƒ½åŠ›ä»ç„¶æœ‰é™ã€‚å°½ç®¡è¿‘æœŸåœ¨é•¿è¯­å¢ƒMLLMsæ–¹é¢æœ‰æ‰€è¿›å±•ï¼Œä½†ä¸ºé•¿è§†è§‰è¯­å¢ƒå­˜å‚¨å’Œå…³æ³¨é”®å€¼ï¼ˆKVï¼‰ç¼“å­˜ä¼šäº§ç”Ÿå¤§é‡å†…å­˜å’Œè®¡ç®—å¼€é”€ã€‚ç°æœ‰è§†è§‰å‹ç¼©æ–¹æ³•è¦ä¹ˆéœ€è¦åœ¨å‹ç¼©ä¹‹å‰å¯¹æ•´ä¸ªè§†è§‰ç¯å¢ƒè¿›è¡Œç¼–ç ï¼Œè¦ä¹ˆéœ€è¦æå‰äº†è§£é—®é¢˜ï¼Œè¿™å¯¹äºé•¿è§†é¢‘ç†è§£å’Œå¤šå›åˆå¯¹è¯ç¯å¢ƒæ¥è¯´å¹¶ä¸å®ç”¨ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†StreamMemï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºæµå¼è§†é¢‘ç†è§£çš„æŸ¥è¯¢æ— å…³é”®å€¼ç¼“å­˜å†…å­˜æœºåˆ¶ã€‚å…·ä½“æ¥è¯´ï¼ŒStreamMemä»¥æµå¼æ–¹å¼ç¼–ç æ–°è§†é¢‘å¸§ï¼Œä½¿ç”¨è§†è§‰ä»¤ç‰Œå’Œé€šç”¨æŸ¥è¯¢ä»¤ç‰Œä¹‹é—´çš„æ³¨æ„åŠ›åˆ†æ•°å‹ç¼©é”®å€¼ç¼“å­˜ï¼ŒåŒæ—¶ä¿æŒå›ºå®šå¤§å°çš„é”®å€¼å†…å­˜ï¼Œä»¥åœ¨å†…å­˜å—é™çš„é•¿è§†é¢‘åœºæ™¯ä¸­å®ç°é«˜æ•ˆé—®ç­”ï¼ˆQAï¼‰ã€‚åœ¨ä¸‰ä¸ªé•¿è§†é¢‘ç†è§£å’Œä¸¤ä¸ªæµå¼è§†é¢‘é—®ç­”åŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒStreamMemåœ¨æŸ¥è¯¢æ— å…³çš„é”®å€¼ç¼“å­˜å‹ç¼©æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œå¹¶ä¸”åœ¨æŸ¥è¯¢æ„ŸçŸ¥å‹ç¼©æ–¹æ³•ä¸­è¡¨ç°å…·æœ‰ç«äº‰åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15717v1">PDF</a> 15 pages, 3 figures</p>
<p><strong>Summary</strong></p>
<p>MLLMåœ¨å¤„ç†é•¿è§†é¢‘æ–¹é¢å­˜åœ¨æ•ˆç‡é™åˆ¶ï¼Œç‰¹åˆ«æ˜¯åœ¨å­˜å‚¨å’Œå…³æ³¨é•¿è§†è§‰ä¸Šä¸‹æ–‡çš„å…³é”®å€¼ç¼“å­˜æ—¶ä¼šäº§ç”Ÿå¤§é‡çš„å†…å­˜å’Œè®¡ç®—å¼€é”€ã€‚ç°æœ‰è§†è§‰å‹ç¼©æ–¹æ³•éœ€è¦æå‰å¯¹æ•´ä¸ªè§†è§‰ä¸Šä¸‹æ–‡è¿›è¡Œç¼–ç æˆ–æå‰è·å–é—®é¢˜ï¼Œè¿™å¯¹äºé•¿è§†é¢‘ç†è§£å’Œå¤šè½®å¯¹è¯åœºæ™¯æ¥è¯´ä¸åˆ‡å®é™…ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºStreamMemçš„æŸ¥è¯¢æ— å…³çš„å…³é”®å€¼ç¼“å­˜è®°å¿†æœºåˆ¶ï¼Œä»¥æµå¼å¤„ç†è§†é¢‘ç†è§£ã€‚å®ƒé€šè¿‡å…³æ³¨è§†è§‰ä»¤ç‰Œå’Œé€šç”¨æŸ¥è¯¢ä»¤ç‰Œä¹‹é—´çš„æ³¨æ„åŠ›åˆ†æ•°æ¥å‹ç¼©å…³é”®å€¼ç¼“å­˜ï¼ŒåŒæ—¶ä¿æŒå›ºå®šå¤§å°çš„å…³é”®å€¼å†…å­˜ï¼Œä»¥åœ¨å†…å­˜å—é™çš„é•¿è§†é¢‘åœºæ™¯ä¸­å®ç°é«˜æ•ˆçš„é—®é¢˜å›ç­”ã€‚åœ¨ä¸‰ä¸ªé•¿è§†é¢‘ç†è§£å’Œä¸¤ä¸ªæµå¼è§†é¢‘é—®ç­”åŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒStreamMemåœ¨æŸ¥è¯¢æ— å…³çš„å…³é”®å€¼ç¼“å­˜å‹ç¼©æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œå¹¶ä¸”åœ¨æŸ¥è¯¢æ„ŸçŸ¥å‹ç¼©æ–¹æ³•ä¸­å…·æœ‰å¾ˆå¼ºçš„ç«äº‰åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLMåœ¨å¤„ç†é•¿è§†é¢‘æ—¶å­˜åœ¨æ•ˆç‡é™åˆ¶ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†è§†è§‰ä¸Šä¸‹æ–‡çš„å…³é”®å€¼ç¼“å­˜æ—¶é¢ä¸´å†…å­˜å’Œè®¡ç®—å¼€é”€çš„æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰è§†è§‰å‹ç¼©æ–¹æ³•åœ¨å¤„ç†é•¿è§†é¢‘ç†è§£å’Œå¤šè½®å¯¹è¯åœºæ™¯æ—¶å­˜åœ¨å›°éš¾ï¼Œéœ€è¦æå‰å¯¹è§†è§‰ä¸Šä¸‹æ–‡è¿›è¡Œç¼–ç æˆ–æå‰è·å–é—®é¢˜ã€‚</li>
<li>StreamMemæ˜¯ä¸€ç§æŸ¥è¯¢æ— å…³çš„å…³é”®å€¼ç¼“å­˜è®°å¿†æœºåˆ¶ï¼Œä»¥æµå¼å¤„ç†è§†é¢‘ç†è§£ï¼Œèƒ½å¤Ÿå‹ç¼©å…³é”®å€¼ç¼“å­˜å¹¶ç»´æŒå›ºå®šå¤§å°çš„å…³é”®å€¼å†…å­˜ã€‚</li>
<li>StreamMemé€šè¿‡å…³æ³¨è§†è§‰ä»¤ç‰Œå’Œé€šç”¨æŸ¥è¯¢ä»¤ç‰Œä¹‹é—´çš„æ³¨æ„åŠ›åˆ†æ•°æ¥å®ç°é«˜æ•ˆçš„è§†é¢‘ç†è§£ã€‚</li>
<li>StreamMemåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå®ç°äº†æœ€æ–°çš„æŠ€æœ¯æ€§èƒ½ï¼Œåœ¨æŸ¥è¯¢æ„ŸçŸ¥å‹ç¼©æ–¹æ³•ä¸­å…·æœ‰å¾ˆå¼ºçš„ç«äº‰åŠ›ã€‚</li>
<li>StreamMemé€‚ç”¨äºå†…å­˜å—é™çš„é•¿è§†é¢‘åœºæ™¯ä¸­çš„é«˜æ•ˆé—®é¢˜å›ç­”ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15717">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-84d7d70ece0f5a451bf042986592922d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3a8ccecef93be255c1f43afea49cc93e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad8c856fbaba7a3fa93640bb0fdd200a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7b32da0b8cb7d5d77003f3d030d0eadc.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="LLaSO-A-Foundational-Framework-for-Reproducible-Research-in-Large-Language-and-Speech-Model"><a href="#LLaSO-A-Foundational-Framework-for-Reproducible-Research-in-Large-Language-and-Speech-Model" class="headerlink" title="LLaSO: A Foundational Framework for Reproducible Research in Large   Language and Speech Model"></a>LLaSO: A Foundational Framework for Reproducible Research in Large   Language and Speech Model</h2><p><strong>Authors:Yirong Sun, Yizhong Geng, Peidong Wei, Yanjun Chen, Jinghan Yang, Rongfei Chen, Wei Zhang, Xiaoyu Shen</strong></p>
<p>The development of Large Speech-Language Models (LSLMs) has been slowed by fragmented architectures and a lack of transparency, hindering the systematic comparison and reproducibility of research. Unlike in the vision-language domain, the LSLM field suffers from the common practice of releasing model weights without their corresponding training data and configurations. To address these critical gaps, we introduce LLaSO, the first fully open, end-to-end framework for large-scale speech-language modeling. LLaSO provides the community with three essential resources: (1) LLaSO-Align, a 12M-instance speech-text alignment corpus; (2) LLaSO-Instruct, a 13.5M-instance multi-task instruction-tuning dataset; and (3) LLaSO-Eval, a reproducible benchmark for standardized evaluation. To validate our framework, we build and release LLaSO-Base, a 3.8B-parameter reference model trained exclusively on our public data. It achieves a normalized score of 0.72, establishing a strong, reproducible baseline that surpasses comparable models. Our analysis reveals that while broader training coverage enhances performance, significant generalization gaps persist on unseen tasks, particularly in pure audio scenarios. By releasing the complete stack of data, benchmarks, and models, LLaSO establishes a foundational open standard to unify research efforts and accelerate community-driven progress in LSLMs. We release the code, dataset, pretrained models, and results in <a target="_blank" rel="noopener" href="https://github.com/EIT-NLP/LLaSO">https://github.com/EIT-NLP/LLaSO</a>. </p>
<blockquote>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLSLMï¼‰çš„å‘å±•å—åˆ°äº†æ¶æ„åˆ†æ•£å’Œé€æ˜åº¦ä¸è¶³çš„é˜»ç¢ï¼Œè¿™å¦¨ç¢äº†ç ”ç©¶çš„ç³»ç»Ÿæ¯”è¾ƒå’Œå¯é‡å¤æ€§ã€‚ä¸è§†è§‰è¯­è¨€é¢†åŸŸä¸åŒï¼ŒLSLMé¢†åŸŸæ™®éå­˜åœ¨å‘å¸ƒæ¨¡å‹æƒé‡è€Œæ²¡æœ‰ç›¸åº”è®­ç»ƒæ•°æ®å’Œé…ç½®çš„åšæ³•ã€‚ä¸ºäº†è§£å†³è¿™äº›å…³é”®å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†LLaSOï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå®Œå…¨å¼€æ”¾çš„å¤§è§„æ¨¡è¯­è¨€å»ºæ¨¡ç«¯åˆ°ç«¯æ¡†æ¶ã€‚LLaSOä¸ºç¤¾åŒºæä¾›äº†ä¸‰ç§é‡è¦èµ„æºï¼šï¼ˆ1ï¼‰LLaSO-Alignï¼Œä¸€ä¸ªåŒ…å«1200ä¸‡å®ä¾‹çš„è¯­éŸ³æ–‡æœ¬å¯¹é½è¯­æ–™åº“ï¼›ï¼ˆ2ï¼‰LLaSO-Instructï¼Œä¸€ä¸ªåŒ…å«1350ä¸‡å®ä¾‹çš„å¤šä»»åŠ¡æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†ï¼›ï¼ˆ3ï¼‰LLaSO-Evalï¼Œä¸€ä¸ªå¯å¤ç°çš„æ ‡å‡†åŒ–è¯„ä¼°åŸºå‡†ã€‚ä¸ºäº†éªŒè¯æˆ‘ä»¬çš„æ¡†æ¶ï¼Œæˆ‘ä»¬æ„å»ºå¹¶å‘å¸ƒäº†LLaSO-Baseï¼Œè¿™æ˜¯ä¸€ä¸ªä»…åœ¨æˆ‘ä»¬å…¬å¼€æ•°æ®ä¸Šè®­ç»ƒçš„38äº¿å‚æ•°å‚è€ƒæ¨¡å‹ï¼Œå…¶å½’ä¸€åŒ–å¾—åˆ†ä¸º0.72ï¼Œå»ºç«‹äº†å¼ºå¤§ä¸”å¯å¤ç°çš„åŸºçº¿ï¼Œè¶…è¶Šäº†åŒç±»æ¨¡å‹ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œè™½ç„¶æ›´å¹¿æ³›çš„è®­ç»ƒè¦†ç›–æœ‰åŠ©äºæé«˜æ€§èƒ½ï¼Œä½†åœ¨æœªè§ä»»åŠ¡ä¸Šä»ç„¶å­˜åœ¨é‡å¤§çš„æ³›åŒ–å·®è·ï¼Œç‰¹åˆ«æ˜¯åœ¨çº¯éŸ³é¢‘åœºæ™¯ä¸­ã€‚é€šè¿‡å‘å¸ƒæ•°æ®ã€åŸºå‡†æµ‹è¯•å’Œæ¨¡å‹çš„å®Œæ•´å †æ ˆï¼ŒLLaSOå»ºç«‹äº†ä¸€ä¸ªç»Ÿä¸€çš„å¼€æ”¾æ ‡å‡†ï¼Œä»¥æ•´åˆç ”ç©¶åŠªåŠ›å¹¶åŠ é€ŸLSLMçš„ç¤¾åŒºé©±åŠ¨è¿›å±•ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/EIT-NLP/LLaSO">https://github.com/EIT-NLP/LLaSO</a>å‘å¸ƒä»£ç ã€æ•°æ®é›†ã€é¢„è®­ç»ƒæ¨¡å‹å’Œç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15418v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§è¯­éŸ³è¯­è¨€æ¨¡å‹ï¼ˆLSLMï¼‰çš„å‘å±•å—åˆ°äº†æ¶æ„ç¢ç‰‡åŒ–åŠç¼ºä¹é€æ˜åº¦çš„å½±å“ï¼Œé˜»ç¢äº†ç ”ç©¶çš„ç³»ç»Ÿæ¯”è¾ƒå’Œå¯é‡å¤æ€§ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºLLaSOï¼Œé¦–ä¸ªå®Œå…¨å¼€æ”¾çš„å¤§è§„æ¨¡è¯­éŸ³è¯­è¨€å»ºæ¨¡ç«¯åˆ°ç«¯æ¡†æ¶ã€‚LLaSOä¸ºç¤¾åŒºæä¾›ä¸‰å¤§èµ„æºï¼šLLaSO-Alignè¯­éŸ³æ–‡æœ¬å¯¹é½è¯­æ–™åº“ã€LLaSO-Instructå¤šä»»åŠ¡æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†åŠLLaSO-Evalå¯é‡å¤åŸºå‡†æµ‹è¯•å¹³å°ï¼Œç”¨äºæ ‡å‡†åŒ–è¯„ä¼°ã€‚åŸºäºè¯¥æ¡†æ¶ï¼Œæˆ‘ä»¬æ„å»ºäº†ä»…ä½¿ç”¨å…¬å¼€æ•°æ®è®­ç»ƒçš„LLaSO-BaseåŸºå‡†æ¨¡å‹ï¼Œå‚æ•°è§„æ¨¡è¾¾3.8Bï¼Œå½’ä¸€åŒ–å¾—åˆ†ä¸º0.72ï¼Œåœ¨å¯æ¯”è¾ƒæ¨¡å‹ä¸­è¡¨ç°ä¼˜å¼‚ä¸”å¯å¤åˆ¶æ€§å¼ºã€‚ç ”ç©¶åˆ†æè¡¨æ˜ï¼Œå°½ç®¡æ›´å¹¿æ³›çš„è®­ç»ƒè¦†ç›–æœ‰åŠ©äºæå‡æ€§èƒ½ï¼Œä½†åœ¨æœªè§ä»»åŠ¡ä¸Šä»å­˜åœ¨é‡å¤§æ³›åŒ–å·®è·ï¼Œç‰¹åˆ«æ˜¯åœ¨çº¯éŸ³é¢‘åœºæ™¯ä¸­ã€‚æˆ‘ä»¬å‘å¸ƒæ•´å¥—æ•°æ®ã€åŸºå‡†æµ‹è¯•åŠæ¨¡å‹ï¼ŒLLaSOç¡®ç«‹äº†ä¸€ä¸ªç»Ÿä¸€ç ”ç©¶åŠªåŠ›ã€åŠ é€ŸLSLMç¤¾åŒºé©±åŠ¨å‘å±•çš„å¼€æ”¾æ ‡å‡†ã€‚ç›¸å…³ä»£ç ã€æ•°æ®é›†ã€é¢„è®­ç»ƒæ¨¡å‹åŠç»“æœå·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/EIT-NLP/LLaSO%E3%80%82">https://github.com/EIT-NLP/LLaSOã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>LSLMé¢†åŸŸå‘å±•å—åˆ°æ¶æ„ç¢ç‰‡åŒ–å’Œé€æ˜åº¦ç¼ºä¹çš„é˜»ç¢ï¼Œå½±å“ç ”ç©¶çš„æ¯”è¾ƒå’Œå¯é‡å¤æ€§ã€‚</li>
<li>æ¨å‡ºLLaSOæ¡†æ¶ï¼Œä¸ºLSLMç ”ç©¶æä¾›å…¨é¢å¼€æ”¾çš„èµ„æºï¼ŒåŒ…æ‹¬è¯­æ–™åº“ã€æ•°æ®é›†å’Œè¯„ä¼°å¹³å°ã€‚</li>
<li>åŸºäºLLaSOæ¡†æ¶å»ºç«‹LLaSO-Baseæ¨¡å‹ï¼Œåœ¨æ ‡å‡†åŒ–æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>ç ”ç©¶å‘ç°æ›´å¹¿æ³›çš„è®­ç»ƒè¦†ç›–æœ‰åŠ©äºæå‡æ€§èƒ½ï¼Œä½†æœªè§ä»»åŠ¡ä¸Šçš„æ³›åŒ–èƒ½åŠ›ä»æœ‰å·®è·ã€‚</li>
<li>LLaSOçš„å»ºç«‹ä¸ºLSLMç ”ç©¶æä¾›äº†ç»Ÿä¸€çš„å¼€æ”¾æ ‡å‡†ï¼Œæœ‰åŠ©äºæ•´åˆç ”ç©¶åŠ›é‡ï¼ŒåŠ é€Ÿå‘å±•ã€‚</li>
<li>LLaSOæ¡†æ¶åŠç›¸å…³èµ„æºå·²å®Œå…¨å…¬å¼€ï¼Œæ–¹ä¾¿ç ”ç©¶äººå‘˜ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15418">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-82ea5c2f9cc714d5d14d195629163951.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bc53c3768e8b27710c2e77db6445dbae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a922bc7fd3be110ed015c6ec1cadbbef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0e9abfef04ff1dea1d2cae41e78fe66f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2faae3ae7d4fb343b0e33e464162f12c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0dfeb191cd1071d3bfd758c417c5d9b1.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Improving-LLMs-for-Machine-Translation-Using-Synthetic-Preference-Data"><a href="#Improving-LLMs-for-Machine-Translation-Using-Synthetic-Preference-Data" class="headerlink" title="Improving LLMs for Machine Translation Using Synthetic Preference Data"></a>Improving LLMs for Machine Translation Using Synthetic Preference Data</h2><p><strong>Authors:Dario Vajda, Domen VreÅ¡, Marko Robnik-Å ikonja</strong></p>
<p>Large language models have emerged as effective machine translation systems. In this paper, we explore how a general instruction-tuned large language model can be improved for machine translation using relatively few easily produced data resources. Using Slovene as a use case, we improve the GaMS-9B-Instruct model using Direct Preference Optimization (DPO) training on a programmatically curated and enhanced subset of a public dataset. As DPO requires pairs of quality-ranked instances, we generated its training dataset by translating English Wikipedia articles using two LLMs, GaMS-9B-Instruct and EuroLLM-9B-Instruct. We ranked the resulting translations based on heuristics coupled with automatic evaluation metrics such as COMET. The evaluation shows that our fine-tuned model outperforms both models involved in the dataset generation. In comparison to the baseline models, the fine-tuned model achieved a COMET score gain of around 0.04 and 0.02, respectively, on translating Wikipedia articles. It also more consistently avoids language and formatting errors. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹å·²ç»ä½œä¸ºæœ‰æ•ˆçš„æœºå™¨ç¿»è¯‘ç³»ç»Ÿå‡ºç°ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†å¦‚ä½•é€šè¿‡åˆ©ç”¨ç›¸å¯¹å®¹æ˜“è·å–çš„æ•°æ®èµ„æºï¼Œå¯¹é€šç”¨æŒ‡ä»¤è°ƒæ•´çš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œæ”¹è¿›ï¼Œä»¥æé«˜å…¶æœºå™¨ç¿»è¯‘çš„æ€§èƒ½ã€‚ä»¥æ–¯æ´›æ–‡å°¼äºšè¯­ä¸ºä½¿ç”¨æ¡ˆä¾‹ï¼Œæˆ‘ä»¬é€šè¿‡å¯¹å…¬å…±æ•°æ®é›†çš„ç¼–ç¨‹ç­–åˆ’å’Œå¢å¼ºå­é›†è¿›è¡Œç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰è®­ç»ƒï¼Œæ”¹è¿›äº†GaMS-9B-Instructæ¨¡å‹ã€‚ç”±äºDPOéœ€è¦æˆå¯¹çš„è´¨é‡æ’åå®ä¾‹ï¼Œæˆ‘ä»¬é€šè¿‡ä½¿ç”¨ä¸¤ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹GaMS-9B-Instructå’ŒEuroLLM-9B-Instructç¿»è¯‘è‹±æ–‡ç»´åŸºç™¾ç§‘æ–‡ç« æ¥ç”Ÿæˆå…¶è®­ç»ƒæ•°æ®é›†ã€‚æˆ‘ä»¬æ ¹æ®ä¸è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡ï¼ˆå¦‚COMETï¼‰ç»“åˆçš„å¯å‘å¼æ–¹æ³•ï¼Œå¯¹æ‰€å¾—ç¿»è¯‘è¿›è¡Œæ’åã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬å¾®è°ƒçš„æ¨¡å‹åœ¨ç¿»è¯‘ç»´åŸºç™¾ç§‘æ–‡ç« æ–¹é¢çš„è¡¨ç°ä¼˜äºå‚ä¸æ•°æ®é›†ç”Ÿæˆçš„è¿™ä¸¤ä¸ªæ¨¡å‹ã€‚ä¸åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼Œå¾®è°ƒæ¨¡å‹åœ¨ç»´åŸºç™¾ç§‘æ–‡ç« ç¿»è¯‘ä¸Šçš„COMETå¾—åˆ†åˆ†åˆ«æé«˜äº†çº¦0.04å’Œ0.02ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜èƒ½æ›´ä¸€è‡´åœ°é¿å…è¯­è¨€å’Œæ ¼å¼é”™è¯¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14951v1">PDF</a> Paper with individual presentation at LUHME workshop at ECAI 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹å·²æˆä¸ºæœ‰æ•ˆçš„æœºå™¨ç¿»è¯‘ç³»ç»Ÿã€‚æœ¬æ–‡æ¢ç´¢äº†å¦‚ä½•ä½¿ç”¨å°‘é‡æ˜“è·å–çš„æ•°æ®èµ„æºæ”¹è¿›é€šç”¨æŒ‡ä»¤è°ƒæ•´çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚ä»¥æ–¯æ´›æ–‡å°¼äºšè¯­ä¸ºä¾‹ï¼Œæˆ‘ä»¬æ”¹è¿›äº†GaMS-9B-Instructæ¨¡å‹ï¼Œé‡‡ç”¨Direct Preference Optimizationï¼ˆDPOï¼‰è®­ç»ƒæ³•å¯¹å…¬å¼€æ•°æ®é›†çš„ä¸€ä¸ªç¨‹åºåŒ–ç²¾é€‰å¢å¼ºå­é›†è¿›è¡Œè®­ç»ƒã€‚ç”±äºDPOéœ€è¦è´¨é‡æ’åºçš„å®ä¾‹å¯¹ï¼Œæˆ‘ä»¬é€šè¿‡ä½¿ç”¨ä¸¤ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹GaMS-9B-Instructå’ŒEuroLLM-9B-Instructç¿»è¯‘è‹±æ–‡ç»´åŸºç™¾ç§‘æ–‡ç« æ¥ç”Ÿæˆå…¶è®­ç»ƒæ•°æ®é›†ã€‚æˆ‘ä»¬æ ¹æ®å¯å‘å¼æ–¹æ³•å’Œè‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡ï¼ˆå¦‚COMETï¼‰å¯¹ç¿»è¯‘ç»“æœè¿›è¡Œäº†æ’åã€‚è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç²¾ç»†è°ƒæ•´æ¨¡å‹ä¼˜äºå‚ä¸æ•°æ®é›†ç”Ÿæˆçš„æ¨¡å‹ã€‚ä¸åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼Œç²¾ç»†è°ƒæ•´æ¨¡å‹åœ¨ç¿»è¯‘ç»´åŸºç™¾ç§‘æ–‡ç« æ—¶çš„COMETå¾—åˆ†åˆ†åˆ«æé«˜äº†çº¦0.04å’Œ0.02ã€‚å®ƒè¿˜æ›´ä¸€è‡´åœ°é¿å…äº†è¯­è¨€å’Œæ ¼å¼é”™è¯¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æœºå™¨ç¿»è¯‘é¢†åŸŸè¡¨ç°å“è¶Šã€‚</li>
<li>ä½¿ç”¨æ–¯æ´›æ–‡å°¼äºšè¯­ä½œä¸ºæ¡ˆä¾‹ï¼Œå¯¹é€šç”¨æŒ‡ä»¤è°ƒæ•´çš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº†æ”¹è¿›ã€‚</li>
<li>é‡‡ç”¨Direct Preference Optimizationï¼ˆDPOï¼‰è®­ç»ƒæ³•ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>è®­ç»ƒæ•°æ®é›†æ˜¯é€šè¿‡ä¸¤ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ç¿»è¯‘è‹±æ–‡ç»´åŸºç™¾ç§‘æ–‡ç« å¹¶ä½¿ç”¨è´¨é‡è¯„ä¼°æ’åç”Ÿæˆçš„ã€‚</li>
<li>ç²¾ç»†è°ƒæ•´æ¨¡å‹çš„æ€§èƒ½ä¼˜äºå‚ä¸æ•°æ®é›†ç”Ÿæˆçš„æ¨¡å‹ã€‚</li>
<li>ä¸åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼Œç²¾ç»†è°ƒæ•´æ¨¡å‹åœ¨COMETå¾—åˆ†ä¸Šæœ‰æ‰€æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14951">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fc1fb53c175154743717b632ffaa72e0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-825f27310517f93e5920a087f52a9705.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0332fb1aed2860ff291766fbd867f85e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-03e10e461ba503baab747103bc7244ce.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0f32a6762dca887bcb5a1b8177ce326e.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="NVIDIA-Nemotron-Nano-2-An-Accurate-and-Efficient-Hybrid-Mamba-Transformer-Reasoning-Model"><a href="#NVIDIA-Nemotron-Nano-2-An-Accurate-and-Efficient-Hybrid-Mamba-Transformer-Reasoning-Model" class="headerlink" title="NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid   Mamba-Transformer Reasoning Model"></a>NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid   Mamba-Transformer Reasoning Model</h2><p><strong>Authors: NVIDIA,  :, Aarti Basant, Abhijit Khairnar, Abhijit Paithankar, Abhinav Khattar, Adithya Renduchintala, Aditya Malte, Akhiad Bercovich, Akshay Hazare, Alejandra Rico, Aleksander Ficek, Alex Kondratenko, Alex Shaposhnikov, Alexander Bukharin, Ali Taghibakhshi, Amelia Barton, Ameya Sunil Mahabaleshwarkar, Amy Shen, Andrew Tao, Ann Guan, Anna Shors, Anubhav Mandarwal, Arham Mehta, Arun Venkatesan, Ashton Sharabiani, Ashwath Aithal, Ashwin Poojary, Ayush Dattagupta, Balaram Buddharaju, Banghua Zhu, Barnaby Simkin, Bilal Kartal, Bita Darvish Rouhani, Bobby Chen, Boris Ginsburg, Brandon Norick, Brian Yu, Bryan Catanzaro, Charles Wang, Charlie Truong, Chetan Mungekar, Chintan Patel, Chris Alexiuk, Christian Munley, Christopher Parisien, Dan Su, Daniel Afrimi, Daniel Korzekwa, Daniel Rohrer, Daria Gitman, David Mosallanezhad, Deepak Narayanan, Dima Rekesh, Dina Yared, Dmytro Pykhtar, Dong Ahn, Duncan Riach, Eileen Long, Elliott Ning, Eric Chung, Erick Galinkin, Evelina Bakhturina, Gargi Prasad, Gerald Shen, Haifeng Qian, Haim Elisha, Harsh Sharma, Hayley Ross, Helen Ngo, Herman Sahota, Hexin Wang, Hoo Chang Shin, Hua Huang, Iain Cunningham, Igor Gitman, Ivan Moshkov, Jaehun Jung, Jan Kautz, Jane Polak Scowcroft, Jared Casper, Jian Zhang, Jiaqi Zeng, Jimmy Zhang, Jinze Xue, Jocelyn Huang, Joey Conway, John Kamalu, Jonathan Cohen, Joseph Jennings, Julien Veron Vialard, Junkeun Yi, Jupinder Parmar, Kari Briski, Katherine Cheung, Katherine Luna, Keith Wyss, Keshav Santhanam, Kezhi Kong, Krzysztof Pawelec, Kumar Anik, Kunlun Li, Kushan Ahmadian, Lawrence McAfee, Laya Sleiman, Leon Derczynski, Luis Vega, Maer Rodrigues de Melo, Makesh Narsimhan Sreedhar, Marcin Chochowski, Mark Cai, Markus Kliegl, Marta Stepniewska-Dziubinska, Matvei Novikov, Mehrzad Samadi, Meredith Price, Meriem Boubdir, Michael Boone, Michael Evans, Michal Bien, Michal Zawalski, Miguel Martinez, Mike Chrzanowski, Mohammad Shoeybi, Mostofa Patwary, Namit Dhameja, Nave Assaf, Negar Habibi, Nidhi Bhatia, Nikki Pope, Nima Tajbakhsh, Nirmal Kumar Juluru, Oleg Rybakov, Oleksii Hrinchuk, Oleksii Kuchaiev, Oluwatobi Olabiyi, Pablo Ribalta, Padmavathy Subramanian, Parth Chadha, Pavlo Molchanov, Peter Dykas, Peter Jin, Piotr Bialecki, Piotr Januszewski, Pradeep Thalasta, Prashant Gaikwad, Prasoon Varshney, Pritam Gundecha, Przemek Tredak, Rabeeh Karimi Mahabadi, Rajen Patel, Ran El-Yaniv, Ranjit Rajan, Ria Cheruvu, Rima Shahbazyan, Ritika Borkar, Ritu Gala, Roger Waleffe, Ruoxi Zhang, Russell J. Hewett, Ryan Prenger, Sahil Jain, Samuel Kriman, Sanjeev Satheesh, Saori Kaji, Sarah Yurick, Saurav Muralidharan, Sean Narenthiran, Seonmyeong Bak, Sepehr Sameni, Seungju Han, Shanmugam Ramasamy, Shaona Ghosh, Sharath Turuvekere Sreenivas, Shelby Thomas, Shizhe Diao, Shreya Gopal, Shrimai Prabhumoye, Shubham Toshniwal, Shuoyang Ding, Siddharth Singh, Siddhartha Jain, Somshubra Majumdar, Soumye Singhal, Stefania Alborghetti, Syeda Nahida Akter, Terry Kong, Tim Moon, Tomasz Hliwiak, Tomer Asida, Tony Wang, Tugrul Konuk, Twinkle Vashishth, Tyler Poon, Udi Karpas, Vahid Noroozi, Venkat Srinivasan, Vijay Korthikanti, Vikram Fugro, Vineeth Kalluru, Vitaly Kurin, Vitaly Lavrukhin, Wasi Uddin Ahmad, Wei Du, Wonmin Byeon, Ximing Lu, Xin Dong, Yashaswi Karnati, Yejin Choi, Yian Zhang, Ying Lin, Yonggan Fu, Yoshi Suhara, Zhen Dong, Zhiyu Li, Zhongbo Zhu, Zijia Chen</strong></p>
<p>We introduce Nemotron-Nano-9B-v2, a hybrid Mamba-Transformer language model designed to increase throughput for reasoning workloads while achieving state-of-the-art accuracy compared to similarly-sized models. Nemotron-Nano-9B-v2 builds on the Nemotron-H architecture, in which the majority of the self-attention layers in the common Transformer architecture are replaced with Mamba-2 layers, to achieve improved inference speed when generating the long thinking traces needed for reasoning. We create Nemotron-Nano-9B-v2 by first pre-training a 12-billion-parameter model (Nemotron-Nano-12B-v2-Base) on 20 trillion tokens using an FP8 training recipe. After aligning Nemotron-Nano-12B-v2-Base, we employ the Minitron strategy to compress and distill the model with the goal of enabling inference on up to 128k tokens on a single NVIDIA A10G GPU (22GiB of memory, bfloat16 precision). Compared to existing similarly-sized models (e.g., Qwen3-8B), we show that Nemotron-Nano-9B-v2 achieves on-par or better accuracy on reasoning benchmarks while achieving up to 6x higher inference throughput in reasoning settings like 8k input and 16k output tokens. We are releasing Nemotron-Nano-9B-v2, Nemotron-Nano12B-v2-Base, and Nemotron-Nano-9B-v2-Base checkpoints along with the majority of our pre- and post-training datasets on Hugging Face. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†Nemotron-Nano-9B-v2ï¼Œè¿™æ˜¯ä¸€ç§æ··åˆMamba-Transformerè¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨æé«˜æ¨ç†å·¥ä½œè´Ÿè½½çš„ååé‡ï¼ŒåŒæ—¶ä¸ç±»ä¼¼è§„æ¨¡çš„æ¨¡å‹ç›¸æ¯”å®ç°æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ã€‚Nemotron-Nano-9B-v2å»ºç«‹åœ¨Nemotron-Hæ¶æ„çš„åŸºç¡€ä¸Šï¼Œå°†Transformeræ¶æ„ä¸­å¤§éƒ¨åˆ†çš„è‡ªæ³¨æ„åŠ›å±‚æ›¿æ¢ä¸ºMamba-2å±‚ï¼Œä»¥å®ç°åœ¨ç”Ÿæˆæ¨ç†æ‰€éœ€çš„é•¿æ€è€ƒè½¨è¿¹æ—¶çš„æ›´å¿«æ¨ç†é€Ÿåº¦ã€‚æˆ‘ä»¬é€šè¿‡é¦–å…ˆåœ¨20ä¸‡äº¿ä¸ªä»¤ç‰Œä¸Šä½¿ç”¨FP8è®­ç»ƒé…æ–¹é¢„è®­ç»ƒä¸€ä¸ª12äº¿å‚æ•°æ¨¡å‹ï¼ˆNemotron-Nano-12B-v2-Baseï¼‰æ¥åˆ›å»ºNemotron-Nano-9B-v2ã€‚åœ¨å¯¹Nemotron-Nano-12B-v2-Baseè¿›è¡Œå¯¹é½åï¼Œæˆ‘ä»¬é‡‡ç”¨Minitronç­–ç•¥æ¥å‹ç¼©å’Œè’¸é¦æ¨¡å‹ï¼Œæ—¨åœ¨èƒ½å¤Ÿåœ¨å•ä¸ªNVIDIA A10G GPUï¼ˆå…·æœ‰22GiBå†…å­˜ï¼Œbfloat16ç²¾åº¦ï¼‰ä¸Šè¿›è¡Œæœ€å¤šè¾¾128kä»¤ç‰Œçš„æ¨ç†ã€‚ä¸ç°æœ‰çš„ç±»ä¼¼è§„æ¨¡æ¨¡å‹ï¼ˆä¾‹å¦‚Qwen3-8Bï¼‰ç›¸æ¯”ï¼Œæˆ‘ä»¬åœ¨æ¨ç†åŸºå‡†æµ‹è¯•ä¸­æ˜¾ç¤ºï¼ŒNemotron-Nano-9B-v2å®ç°äº†ç›¸å½“æˆ–æ›´å¥½çš„å‡†ç¡®æ€§ï¼ŒåŒæ—¶åœ¨å¦‚8kè¾“å…¥å’Œ16kè¾“å‡ºä»¤ç‰Œçš„æ¨ç†è®¾ç½®ä¸­å®ç°äº†é«˜è¾¾6å€çš„æ¨ç†ååé‡ã€‚æˆ‘ä»¬å°†Nemotron-Nano-9B-v2ã€Nemotron-Nano12B-v2-Baseä»¥åŠNemotron-Nano-9B-v2çš„æ£€æŸ¥ç‚¹è¿åŒæˆ‘ä»¬å¤§éƒ¨åˆ†é¢„è®­ç»ƒå’Œåç»­è®­ç»ƒæ•°æ®é›†ä¸€èµ·åœ¨Hugging Faceä¸Šå‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14444v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†Nemotron-Nano-9B-v2çš„è¯ç”ŸèƒŒæ™¯å’Œä¼˜åŠ¿ã€‚è¿™æ˜¯ä¸€æ¬¾åŸºäºMamba-Transformeræ¶æ„çš„è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡ç”¨Mamba-2å±‚æ›¿ä»£Transformeræ¶æ„ä¸­çš„å¤§éƒ¨åˆ†è‡ªæ³¨æ„åŠ›å±‚ï¼Œæ—¨åœ¨æå‡å¤„ç†æ¨ç†ä»»åŠ¡æ—¶çš„æ•ˆç‡ä¸å‡†ç¡®åº¦ã€‚å…¶è®­ç»ƒæ•°æ®é‡‡ç”¨å¤§è§„æ¨¡æ•°æ®é›†è®­ç»ƒå¹¶ä½¿ç”¨äº†FP8è®­ç»ƒé…æ–¹ï¼Œèƒ½å¤Ÿåœ¨å•ä¸ªNVIDIA A10G GPUä¸Šå®ç°é«˜è¾¾128kæ ‡è®°çš„æ¨ç†èƒ½åŠ›ã€‚ç›¸è¾ƒäºå…¶ä»–ç±»ä¼¼è§„æ¨¡çš„æ¨¡å‹ï¼Œå®ƒåœ¨æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°å“è¶Šï¼Œæé«˜äº†æ¨ç†é€Ÿåº¦ã€‚æ–‡ç« æœ€åå°†æ¨¡å‹å’Œé¢„è®­ç»ƒæ•°æ®é›†å‘å¸ƒåœ¨Hugging Faceä¸Šä¾›å¤§ä¼—ä½¿ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯å…³äºæ–‡æœ¬çš„å…³é”®è§è§£ï¼š</p>
<ol>
<li>Nemotron-Nano-9B-v2æ˜¯ä¸€ä¸ªåŸºäºMamba-Transformeræ¶æ„çš„è¯­è¨€æ¨¡å‹ï¼Œç”¨äºæé«˜æ¨ç†å·¥ä½œè´Ÿè½½çš„ååé‡å¹¶ä¿æŒä¸ç±»ä¼¼è§„æ¨¡æ¨¡å‹ç›¸æ¯”çš„æœ€ä½³å‡†ç¡®æ€§ã€‚</li>
<li>è¯¥æ¨¡å‹åŸºäºNemotron-Hæ¶æ„æ„å»ºï¼Œè¯¥æ¶æ„é€šè¿‡æ›¿æ¢Transformeræ¶æ„ä¸­çš„å¤§éƒ¨åˆ†è‡ªæ³¨æ„åŠ›å±‚ä¸ºMamba-2å±‚ï¼Œä»¥å®ç°ç”Ÿæˆé•¿æ€è€ƒè½¨è¿¹æ‰€éœ€çš„æ¨ç†è¿‡ç¨‹ä¸­çš„æ”¹è¿›æ¨æ–­é€Ÿåº¦ã€‚</li>
<li>æ¨¡å‹é€šè¿‡å¤§è§„æ¨¡æ•°æ®é›†è¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶é‡‡ç”¨FP8è®­ç»ƒé…æ–¹æ¥æå‡æ€§èƒ½ã€‚</li>
<li>è¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨å•ä¸ªNVIDIA A10G GPUä¸Šå¤„ç†é«˜è¾¾128kçš„æ ‡è®°æ¨ç†ä»»åŠ¡ï¼Œç›®æ ‡æ˜¯ä¸ºäº†å®ç°åœ¨å†…å­˜ä½¿ç”¨ç‡å’Œå¤„ç†é€Ÿåº¦ä¹‹é—´çš„ä¼˜åŒ–å¹³è¡¡ã€‚</li>
<li>æ¨¡å‹è¡¨ç°å‡ºä¼˜äºåŒç±»æ¨¡å‹çš„æ¨ç†é€Ÿåº¦æ€§èƒ½è¡¨ç°ã€‚ç›¸å¯¹äºåŒæ ·è§„æ¨¡çš„æ¨¡å‹ï¼ˆå¦‚Qwen3-8Bï¼‰ï¼Œåœ¨å¦‚è¾“å…¥ä¸º8kæ ‡è®°å’Œè¾“å‡ºä¸º16kæ ‡è®°çš„æ¨ç†ä»»åŠ¡ä¸Šå¯ä»¥è¾¾åˆ°æ›´é«˜çš„æ¨ç†é€Ÿåº¦ï¼ˆé«˜è¾¾6å€ï¼‰ã€‚ </li>
<li>ä½œè€…å·²ç»å°†è¯¥æ¨¡å‹å’Œä¸»è¦çš„é¢„è®­ç»ƒå’Œåç»­è®­ç»ƒæ•°æ®é›†åœ¨Hugging Faceä¸Šå‘å¸ƒï¼Œä»¥ä¾›å¤§ä¼—è®¿é—®å’Œä½¿ç”¨ã€‚è¿™å°†ä¿ƒè¿›æœªæ¥æ¨¡å‹çš„ç ”ç©¶å’Œå‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14444">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-55f97bf17c1a24b75a1fbc40b0cbf9ad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7fa504cec80d675359af61cce57535a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa73dae471819e017a5cc665c1f7aee0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="TabulaX-Leveraging-Large-Language-Models-for-Multi-Class-Table-Transformations"><a href="#TabulaX-Leveraging-Large-Language-Models-for-Multi-Class-Table-Transformations" class="headerlink" title="TabulaX: Leveraging Large Language Models for Multi-Class Table   Transformations"></a>TabulaX: Leveraging Large Language Models for Multi-Class Table   Transformations</h2><p><strong>Authors:Arash Dargahi Nobari, Davood Rafiei</strong></p>
<p>The integration of tabular data from diverse sources is often hindered by inconsistencies in formatting and representation, posing significant challenges for data analysts and personal digital assistants. Existing methods for automating tabular data transformations are limited in scope, often focusing on specific types of transformations or lacking interpretability. In this paper, we introduce TabulaX, a novel framework that leverages Large Language Models (LLMs) for multi-class column-level tabular transformations. TabulaX first classifies input columns into four transformation types (string-based, numerical, algorithmic, and general) and then applies tailored methods to generate human-interpretable transformation functions, such as numeric formulas or programming code. This approach enhances transparency and allows users to understand and modify the mappings. Through extensive experiments on real-world datasets from various domains, we demonstrate that TabulaX outperforms existing state-of-the-art approaches in terms of accuracy, supports a broader class of transformations, and generates interpretable transformations that can be efficiently applied. </p>
<blockquote>
<p>ä»ä¸åŒæ¥æºæ•´åˆè¡¨æ ¼æ•°æ®å¸¸å¸¸å› æ ¼å¼å’Œè¡¨ç¤ºçš„ä¸ä¸€è‡´æ€§è€Œå—é˜»ï¼Œå¯¹æ•°æ®åˆ†æå¸ˆå’Œä¸ªäººæ•°å­—åŠ©ç†æ„æˆé‡å¤§æŒ‘æˆ˜ã€‚ç°æœ‰çš„è‡ªåŠ¨åŒ–è¡¨æ ¼æ•°æ®è½¬æ¢æ–¹æ³•èŒƒå›´æœ‰é™ï¼Œé€šå¸¸åªä¸“æ³¨äºç‰¹å®šç±»å‹çš„è½¬æ¢æˆ–ç¼ºä¹å¯è§£é‡Šæ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†TabulaXï¼Œä¸€ä¸ªåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œå¤šç±»åˆ—çº§è¡¨æ ¼è½¬æ¢çš„æ–°å‹æ¡†æ¶ã€‚TabulaXé¦–å…ˆå°†å¯¹è¾“å…¥åˆ—è¿›è¡Œå››ç§è½¬æ¢ç±»å‹ï¼ˆåŸºäºå­—ç¬¦ä¸²ã€æ•°å€¼ã€ç®—æ³•å’Œé€šç”¨ï¼‰çš„åˆ†ç±»ï¼Œç„¶ååº”ç”¨å®šåˆ¶çš„æ–¹æ³•ç”Ÿæˆäººç±»å¯è§£é‡Šè½¬æ¢å‡½æ•°ï¼Œå¦‚æ•°å€¼å…¬å¼æˆ–ç¨‹åºä»£ç ã€‚è¿™ç§æ–¹æ³•æé«˜äº†é€æ˜åº¦ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿç†è§£å’Œä¿®æ”¹æ˜ å°„å…³ç³»ã€‚é€šè¿‡åœ¨ä¸åŒé¢†åŸŸçœŸå®æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†TabulaXåœ¨å‡†ç¡®æ€§æ–¹é¢ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œæ”¯æŒæ›´å¹¿æ³›çš„è½¬æ¢ç±»å‹ï¼Œå¹¶ç”Ÿæˆå¯è§£é‡Šçš„è½¬æ¢ï¼Œå¯é«˜æ•ˆåº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.17110v2">PDF</a> </p>
<p><strong>Summary</strong><br>æ•°æ®ä»ä¸åŒæ¥æºçš„è¡¨æ ¼é›†æˆå¸¸å¸¸å—åˆ°æ ¼å¼å’Œè¡¨ç¤ºä¸ä¸€è‡´æ€§çš„é˜»ç¢ï¼Œç»™æ•°æ®åˆ†æäººå‘˜å’Œä¸ªäººæ•°å­—åŠ©ç†å¸¦æ¥é‡å¤§æŒ‘æˆ˜ã€‚ç°æœ‰çš„è‡ªåŠ¨åŒ–è¡¨æ ¼æ•°æ®è½¬æ¢æ–¹æ³•èŒƒå›´æœ‰é™ï¼Œé€šå¸¸åªå…³æ³¨ç‰¹å®šç±»å‹çš„è½¬æ¢æˆ–ç¼ºä¹å¯è§£é‡Šæ€§ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªæ–°å‹æ¡†æ¶TabulaXï¼Œå®ƒåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œå¤šç±»åˆ—çº§è¡¨æ ¼è½¬æ¢ã€‚TabulaXé¦–å…ˆæ ¹æ®è¾“å…¥çš„åˆ—å°†å…¶åˆ†ä¸ºå››ç±»ï¼ˆåŸºäºå­—ç¬¦ä¸²çš„ã€æ•°å­—çš„ã€ç®—æ³•çš„å’Œä¸€èˆ¬çš„ï¼‰ï¼Œç„¶åé‡‡ç”¨ç›¸åº”çš„æ–¹æ³•ç”Ÿæˆäººç±»å¯è§£é‡Šæ€§çš„è½¬æ¢å‡½æ•°ï¼Œå¦‚æ•°å€¼å…¬å¼æˆ–ç¨‹åºä»£ç ã€‚æ­¤æ–¹æ³•å¢å¼ºäº†é€æ˜åº¦å¹¶å…è®¸ç”¨æˆ·ç†è§£å’Œä¿®æ”¹æ˜ å°„å…³ç³»ã€‚åœ¨ç°å®ä¸–ç•Œçš„å¤šä¸ªé¢†åŸŸæ•°æ®é›†ä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒTabulaXåœ¨å‡†ç¡®æ€§æ–¹é¢ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ¡ˆï¼Œæ”¯æŒæ›´å¹¿æ³›çš„è½¬æ¢ç±»å‹ï¼Œå¹¶èƒ½ç”Ÿæˆå¯è§£é‡Šçš„è½¬æ¢ï¼Œå¯æœ‰æ•ˆåœ°åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¡¨æ ¼æ•°æ®é›†æˆé¢ä¸´æ ¼å¼å’Œè¡¨ç¤ºä¸ä¸€è‡´çš„æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰è‡ªåŠ¨åŒ–è¡¨æ ¼æ•°æ®è½¬æ¢æ–¹æ³•å…·æœ‰å±€é™æ€§ã€‚</li>
<li>TabulaXåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œå¤šç±»åˆ—çº§è¡¨æ ¼è½¬æ¢ã€‚</li>
<li>TabulaXå°†è¾“å…¥åˆ—åˆ†ä¸ºå››ç±»ï¼šåŸºäºå­—ç¬¦ä¸²çš„ã€æ•°å­—çš„ã€ç®—æ³•çš„å’Œä¸€èˆ¬çš„ã€‚</li>
<li>TabulaXç”Ÿæˆäººç±»å¯è§£é‡Šæ€§çš„è½¬æ¢å‡½æ•°ï¼Œå¦‚æ•°å€¼å…¬å¼æˆ–ç¨‹åºä»£ç ã€‚</li>
<li>TabulaXåœ¨å‡†ç¡®æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ¡ˆï¼Œå¹¶æ”¯æŒæ›´å¹¿æ³›çš„è½¬æ¢ç±»å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.17110">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-6feaac4031eeba7511d50f8106010e04.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-de8288e7a097284361c0a947ec64989b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e602c36a54ab83f83e112f46dca2b95e.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Development-of-Pre-Trained-Transformer-based-Models-for-the-Nepali-Language"><a href="#Development-of-Pre-Trained-Transformer-based-Models-for-the-Nepali-Language" class="headerlink" title="Development of Pre-Trained Transformer-based Models for the Nepali   Language"></a>Development of Pre-Trained Transformer-based Models for the Nepali   Language</h2><p><strong>Authors:Prajwal Thapa, Jinu Nyachhyon, Mridul Sharma, Bal Krishna Bal</strong></p>
<p>Transformer-based pre-trained language models have dominated the field of Natural Language Processing (NLP) for quite some time now. However, the Nepali language, spoken by approximately 32 million people worldwide, remains significantly underrepresented in this domain. This underrepresentation is primarily attributed to the scarcity of monolingual data corpora and limited available resources for the Nepali language. While existing efforts have predominantly concentrated on basic encoder-based models, there is a notable gap in the exploration of decoder-based architectures. To address this gap, we have collected 27.5 GB of Nepali text data, approximately 2.4x larger than any previously available Nepali language corpus. Leveraging this data, we pre-trained three different models i.e., BERT, RoBERTa, and GPT-2, exclusively for the Nepali Language. Furthermore, we performed instruction tuning and explored its potential for monolingual Nepali data, providing a foundation for future research. Our models outperformed the existing best model by 2 points on Nep-gLUE benchmark, scoring 95.60 and also outperformed existing models on text generation tasks, demonstrating improvements in both understanding and generating Nepali text. </p>
<blockquote>
<p>åŸºäºTransformerçš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸå·²ç»å æ®ä¸»å¯¼åœ°ä½å¾ˆé•¿æ—¶é—´äº†ã€‚ç„¶è€Œï¼Œä¸–ç•Œä¸Šçº¦æœ‰3200ä¸‡äººä½¿ç”¨çš„å°¼æ³Šå°”è¯­åœ¨è¿™ä¸ªé¢†åŸŸä»ç„¶æ˜¾è‘—ç¼ºä¹ä»£è¡¨æ€§ã€‚è¿™ç§ä»£è¡¨æ€§ä¸è¶³ä¸»è¦å½’å› äºå•è¯­æ•°æ®è¯­æ–™åº“çš„ç¨€ç¼ºä»¥åŠå°¼æ³Šå°”è¯­å¯ç”¨èµ„æºçš„æœ‰é™ã€‚è™½ç„¶ç°æœ‰çš„åŠªåŠ›ä¸»è¦é›†ä¸­åœ¨åŸºæœ¬çš„ç¼–ç å™¨æ¨¡å‹ä¸Šï¼Œä½†åœ¨è§£ç å™¨æ¶æ„çš„æ¢ç´¢æ–¹é¢ä»å­˜åœ¨æ˜æ˜¾çš„å·®è·ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ”¶é›†äº†27.5GBçš„å°¼æ³Šå°”æ–‡æœ¬æ•°æ®ï¼Œå¤§çº¦æ˜¯ä¹‹å‰ä»»ä½•å¯ç”¨çš„å°¼æ³Šå°”è¯­è¯­æ–™åº“çš„2.4å€ã€‚åˆ©ç”¨è¿™äº›æ•°æ®ï¼Œæˆ‘ä»¬é’ˆå¯¹å°¼æ³Šå°”è¯­é¢„è®­ç»ƒäº†ä¸‰ç§ä¸åŒçš„æ¨¡å‹ï¼Œå³BERTã€RoBERTaå’ŒGPT-2ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯¹æŒ‡ä»¤è¿›è¡Œäº†è°ƒæ•´ï¼Œå¹¶æ¢ç´¢äº†å…¶åœ¨å•è¯­å°¼æ³Šå°”è¯­æ•°æ®ä¸­çš„æ½œåŠ›ï¼Œä¸ºæœªæ¥ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨Nep-gLUEåŸºå‡†æµ‹è¯•ä¸Šçš„å¾—åˆ†æ¯”ç°æœ‰æœ€ä½³æ¨¡å‹é«˜å‡º2åˆ†ï¼Œå¾—åˆ†ä¸º95.60ï¼Œå¹¶ä¸”åœ¨æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸Šä¹Ÿè¶…è¶Šäº†ç°æœ‰æ¨¡å‹ï¼Œè¿™è¡¨æ˜æˆ‘ä»¬åœ¨ç†è§£å’Œç”Ÿæˆå°¼æ³Šå°”è¯­æ–‡æœ¬æ–¹é¢éƒ½æœ‰æ‰€æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.15734v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼šåŸºäºTransformerçš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå æ®ä¸»å¯¼åœ°ä½å·²ä¹…ï¼Œä½†å°¼æ³Šå°”è¯­åœ¨è¯¥é¢†åŸŸå—åˆ°ä¸¥é‡å¿½è§†ã€‚è¿™ä¸»è¦å½’å› äºç¼ºä¹å•è¯­æ•°æ®è¯­æ–™åº“å’Œå¯ç”¨çš„å°¼æ³Šå°”è¯­èµ„æºæœ‰é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ”¶é›†äº†è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„å°¼æ³Šå°”è¯­æ–‡æœ¬æ•°æ®é›†ï¼Œå¹¶åˆ©ç”¨è¯¥æ•°æ®é›†å¯¹BERTã€RoBERTaå’ŒGPT-2ç­‰ä¸‰ç§æ¨¡å‹è¿›è¡Œäº†è®­ç»ƒã€‚åœ¨æŒ‡å¯¼å¾®è°ƒä¸‹ï¼Œè¯¥æ¨¡å‹åœ¨å°¼æ³Šå°”è¯­è¨€ç†è§£å’Œæ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸Šéƒ½è¶…è¶Šäº†ç°æœ‰çš„æ¨¡å‹è¡¨ç°ã€‚åœ¨Nep-gLUEåŸºå‡†æµ‹è¯•ä¸­ï¼Œå¾—åˆ†é«˜å‡ºç›®å‰æœ€ä½³æ¨¡å‹ä¸¤åˆ†ï¼Œè¾¾åˆ°äº†95.6çš„é«˜åˆ†ã€‚è¯¥æ¨¡å‹çš„è®­ç»ƒå’Œç ”å‘ä¸ºåç»­ç›¸å…³ç ”ç©¶å¥ å®šäº†åšå®åŸºç¡€ã€‚ </p>
<p><strong>Key Takeaways</strong>ï¼š </p>
<ul>
<li>Transformer-basedé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå ä¸»å¯¼åœ°ä½ï¼Œä½†å°¼æ³Šå°”è¯­çš„ç›¸å…³ç ”ç©¶ååˆ†åŒ®ä¹ã€‚ </li>
<li>ç¼ºä¹å•è¯­æ•°æ®è¯­æ–™åº“å’Œå¯ç”¨çš„å°¼æ³Šå°”è¯­èµ„æºæ˜¯å°¼æ³Šå°”è¯­åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå—å¿½è§†çš„ä¸»è¦åŸå› ã€‚ </li>
<li>ç ”ç©¶äººå‘˜æ”¶é›†äº†è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„å°¼æ³Šå°”è¯­æ–‡æœ¬æ•°æ®é›†ï¼Œç”¨äºè®­ç»ƒæ¨¡å‹ã€‚ </li>
<li>åˆ©ç”¨è¯¥æ•°æ®é›†è®­ç»ƒçš„æ¨¡å‹åŒ…æ‹¬BERTã€RoBERTaå’ŒGPT-2ç­‰ä¸‰ç§æ¨¡å‹ã€‚ </li>
<li>é€šè¿‡æŒ‡å¯¼å¾®è°ƒï¼Œè¿™äº›æ¨¡å‹åœ¨ç†è§£å’Œç”Ÿæˆå°¼æ³Šå°”è¯­æ–‡æœ¬æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚ </li>
<li>åœ¨Nep-gLUEåŸºå‡†æµ‹è¯•ä¸­ï¼Œè¿™äº›æ¨¡å‹çš„å¾—åˆ†é«˜äºç°æœ‰æœ€ä½³æ¨¡å‹ä¸¤åˆ†ï¼Œè¾¾åˆ°95.6çš„é«˜åˆ†ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.15734">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2a8fdbb595be4fd20cf24dc8d7ecac98.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9b7ee24e95a60115998df2fc64e9712b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ba18a1a83e652844362962f84bc9cd49.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fe905b8e9fcc6422e5d7b7e4678565a4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6abdfe1b6fffe9c4274f8521b933d283.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Knowledge-Guided-Prompt-Learning-for-Request-Quality-Assurance-in-Public-Code-Review"><a href="#Knowledge-Guided-Prompt-Learning-for-Request-Quality-Assurance-in-Public-Code-Review" class="headerlink" title="Knowledge-Guided Prompt Learning for Request Quality Assurance in Public   Code Review"></a>Knowledge-Guided Prompt Learning for Request Quality Assurance in Public   Code Review</h2><p><strong>Authors:Lin Li, Xinchun Yu, Xinyu Chen, Peng Liang</strong></p>
<p>Public Code Review (PCR) is developed in the Software Question Answering (SQA) community, assisting developers in exploring high-quality and efficient review services. Current methods on PCR mainly focus on the reviewerâ€™s perspective, including finding a capable reviewer, predicting comment quality, and recommending&#x2F;generating review comments. However, it is not well studied that how to satisfy the review necessity requests posted by developers which can increase their visibility, which in turn acts as a prerequisite for better review responses. To this end, we propose K nowledge-guided P rompt learning for P ublic Code Review (KP-PCR) to achieve developer-based code review request quality assurance (i.e., predicting request necessity and recommending tags subtask). Specifically, we reformulate the two subtasks via 1) text prompt tuning which converts both of them into a Masked Language Model (MLM) by constructing prompt templates using hard prompt; and 2) knowledge and code prefix tuning which introduces knowledge guidance from fine-tuned large language models by soft prompt, and uses program dependence graph to characterize code snippets. Finally, both of the request necessity prediction and tag recommendation subtasks output predicted results through an answer engineering module. In addition, we further analysis the time complexity of our KP-PCR that has lightweight prefix based the operation of introducing knowledge guidance. Experimental results on the PCR dataset for the period 2011-2023 demonstrate that our KP-PCR outperforms baselines by 2.3%-8.4% in the request necessity prediction and by 1.4%-6.9% in the tag recommendation. The code implementation is released at <a target="_blank" rel="noopener" href="https://github.com/WUT-IDEA/KP-PCR">https://github.com/WUT-IDEA/KP-PCR</a>. </p>
<blockquote>
<p>å…¬å…±ä»£ç å®¡æŸ¥ï¼ˆPCRï¼‰æ˜¯åœ¨è½¯ä»¶é—®ç­”ï¼ˆSQAï¼‰ç¤¾åŒºä¸­å¼€å‘çš„ï¼Œæ—¨åœ¨å¸®åŠ©å¼€å‘è€…æ¢ç´¢é«˜è´¨é‡å’Œé«˜æ•ˆçš„å®¡æŸ¥æœåŠ¡ã€‚å½“å‰PCRçš„ä¸»è¦æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨å®¡æŸ¥è€…çš„è§’åº¦ï¼ŒåŒ…æ‹¬å¯»æ‰¾èƒ½åŠ›å¼ºçš„å®¡æŸ¥è€…ã€é¢„æµ‹è¯„è®ºè´¨é‡å’Œæ¨è&#x2F;ç”Ÿæˆå®¡æŸ¥è¯„è®ºã€‚ç„¶è€Œï¼Œå¦‚ä½•æ»¡è¶³å¼€å‘è€…æå‡ºçš„å®¡æŸ¥éœ€æ±‚è¯·æ±‚å¹¶æœªå¾—åˆ°å¾ˆå¥½çš„ç ”ç©¶ï¼Œè€Œè¿™äº›è¯·æ±‚çš„å¢åŠ å¯ä»¥æé«˜å…¶å¯è§åº¦ï¼Œä»è€Œæ›´å¥½åœ°è·å¾—å®¡æŸ¥å›åº”ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºçŸ¥è¯†å¼•å¯¼æç¤ºå­¦ä¹ å…¬å…±ä»£ç å®¡æŸ¥ï¼ˆKP-PCRï¼‰ï¼Œä»¥å®ç°åŸºäºå¼€å‘è€…çš„ä»£ç å®¡æŸ¥è¯·æ±‚è´¨é‡ä¿è¯ï¼ˆå³é¢„æµ‹è¯·æ±‚å¿…è¦æ€§å’Œæ¨èæ ‡ç­¾å­ä»»åŠ¡ï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é€šè¿‡1ï¼‰æ–‡æœ¬æç¤ºè°ƒæ•´ï¼Œå°†è¿™ä¸¤ä¸ªå­ä»»åŠ¡è½¬æ¢ä¸ºæ©ç è¯­è¨€æ¨¡å‹ï¼ˆMLMï¼‰ï¼Œé€šè¿‡æ„å»ºç¡¬æç¤ºçš„æç¤ºæ¨¡æ¿æ¥å®ç°ï¼›2ï¼‰çŸ¥è¯†å’Œä»£ç å‰ç¼€è°ƒæ•´ï¼Œå¼•å…¥ç»è¿‡å¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹çš„çŸ¥è¯†æŒ‡å¯¼ï¼Œä½¿ç”¨è½¯æç¤ºå’Œç¨‹åºä¾èµ–å›¾æ¥è¡¨å¾ä»£ç ç‰‡æ®µã€‚æœ€åï¼Œè¯·æ±‚å¿…è¦æ€§é¢„æµ‹å’Œæ ‡ç­¾æ¨èå­ä»»åŠ¡éƒ½é€šè¿‡ç­”æ¡ˆå·¥ç¨‹æ¨¡å—è¾“å‡ºé¢„æµ‹ç»“æœã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥åˆ†æäº†æˆ‘ä»¬çš„KP-PCRçš„æ—¶é—´å¤æ‚åº¦ï¼Œå®ƒå…·æœ‰åŸºäºå¼•å…¥çŸ¥è¯†æŒ‡å¯¼çš„å‰ç¼€çš„è½»é‡çº§æ“ä½œã€‚åœ¨2011-2023å¹´çš„PCRæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„KP-PCRåœ¨è¯·æ±‚å¿…è¦æ€§é¢„æµ‹æ–¹é¢ä¼˜äºåŸºçº¿2.3%-8.4%ï¼Œåœ¨æ ‡ç­¾æ¨èæ–¹é¢ä¼˜äºåŸºçº¿1.4%-6.9%ã€‚ä»£ç å®ç°å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/WUT-IDEA/KP-PCR%E3%80%82">https://github.com/WUT-IDEA/KP-PCRã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.21673v3">PDF</a> 27 pages, 5 images, 12 tables, Manuscript revision submitted to a   journal (2025)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å…¬å…±ä»£ç å®¡æŸ¥ï¼ˆPCRï¼‰åœ¨è½¯ä»¶é—®ç­”ï¼ˆSQAï¼‰ç¤¾åŒºçš„å‘å±•æƒ…å†µï¼Œå¹¶æŒ‡å‡ºå½“å‰PCRæ–¹æ³•ä¸»è¦å…³æ³¨è¯„å®¡è€…çš„è§’åº¦ã€‚ä¸ºæ»¡è¶³å¼€å‘è€…æå‡ºçš„å®¡æŸ¥éœ€æ±‚è¯·æ±‚ï¼Œæé«˜å¯è§æ€§å¹¶è·å–æ›´å¥½çš„å®¡æŸ¥å›åº”ï¼Œæå‡ºäº†çŸ¥è¯†å¼•å¯¼æç¤ºå­¦ä¹ å…¬å…±ä»£ç å®¡æŸ¥ï¼ˆKP-PCRï¼‰æ–¹æ³•ã€‚KP-PCRé€šè¿‡æ–‡æœ¬æç¤ºè°ƒæ•´å’ŒçŸ¥è¯†ä¸ä»£ç å‰ç¼€è°ƒæ•´ä¸¤ä¸ªæ­¥éª¤ï¼Œå°†è¯·æ±‚å¿…è¦æ€§é¢„æµ‹å’Œæ ‡ç­¾æ¨èå­ä»»åŠ¡è½¬åŒ–ä¸ºæ©ç è¯­è¨€æ¨¡å‹ï¼ˆMLMï¼‰çš„ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜åˆ†æäº†KP-PCRçš„æ—¶é—´å¤æ‚åº¦ï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯äº†å…¶åœ¨è¯·æ±‚å¿…è¦æ€§é¢„æµ‹å’Œæ ‡ç­¾æ¨èæ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å…¬å…±ä»£ç å®¡æŸ¥ï¼ˆPCRï¼‰åœ¨è½¯ä»¶é—®ç­”ï¼ˆSQAï¼‰ç¤¾åŒºä¸­å¾—åˆ°å‘å±•ï¼Œä¸»è¦å…³æ³¨è¯„å®¡è€…çš„è§’åº¦ï¼ŒåŒ…æ‹¬å¯»æ‰¾èƒ½åŠ›å¼ºçš„è¯„å®¡è€…ã€é¢„æµ‹è¯„è®ºè´¨é‡å’Œç”Ÿæˆæ¨èè¯„è®ºã€‚</li>
<li>KP-PCRæ–¹æ³•æ—¨åœ¨æ»¡è¶³å¼€å‘è€…çš„å®¡æŸ¥éœ€æ±‚è¯·æ±‚ï¼Œæé«˜å…¶åœ¨PCRä¸­çš„å¯è§æ€§ï¼Œä»è€Œè·å–æ›´å¥½çš„å®¡æŸ¥å›åº”ã€‚</li>
<li>KP-PCRé€šè¿‡æ–‡æœ¬æç¤ºè°ƒæ•´å’ŒçŸ¥è¯†ä¸ä»£ç å‰ç¼€è°ƒæ•´ä¸¤ä¸ªæ­¥éª¤ï¼Œå°†è¯·æ±‚å¿…è¦æ€§é¢„æµ‹å’Œæ ‡ç­¾æ¨èå­ä»»åŠ¡è½¬åŒ–ä¸ºæ©ç è¯­è¨€æ¨¡å‹ï¼ˆMLMï¼‰çš„ä»»åŠ¡ã€‚</li>
<li>KP-PCRå¼•å…¥çŸ¥è¯†æŒ‡å¯¼ï¼Œä½¿ç”¨ç¨‹åºä¾èµ–å›¾æ¥è¡¨å¾ä»£ç ç‰‡æ®µï¼Œå¹¶é€šè¿‡è½»é‡çº§çš„å‰ç¼€æ“ä½œå®ç°ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒKP-PCRåœ¨è¯·æ±‚å¿…è¦æ€§é¢„æµ‹å’Œæ ‡ç­¾æ¨èæ–¹é¢ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚</li>
<li>KP-PCRçš„ä»£ç å®ç°å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/WUT-IDEA/KP-PCR%E3%80%82">https://github.com/WUT-IDEA/KP-PCRã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.21673">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f1ff94757caca643ac10254ed3e86f94.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8b80222e7d56efa066e25f14385eba5a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ad1bb7da15b3f7c0ba3b28b76a5096b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bfacefe38eabe8d6de06a17f44ef5ba4.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Optimizing-Cross-Client-Domain-Coverage-for-Federated-Instruction-Tuning-of-Large-Language-Models"><a href="#Optimizing-Cross-Client-Domain-Coverage-for-Federated-Instruction-Tuning-of-Large-Language-Models" class="headerlink" title="Optimizing Cross-Client Domain Coverage for Federated Instruction Tuning   of Large Language Models"></a>Optimizing Cross-Client Domain Coverage for Federated Instruction Tuning   of Large Language Models</h2><p><strong>Authors:Zezhou Wang, Yaxin Du, Xingjun Ma, Yugang Jiang, Zhuzhong Qian, Siheng Chen</strong></p>
<p>Federated domain-specific instruction tuning (FedDIT) for large language models (LLMs) aims to enhance performance in specialized domains using distributed private and limited data, yet identifying key performance drivers and optimal augmentation strategies remains challenging. We empirically establish that cross-client domain coverage, rather than data heterogeneity, is the pivotal factor. We then introduce FedDCA, an algorithm that explicitly maximizes this coverage through diversity-oriented client center selection and retrieval-based augmentation, constructing diverse, non-redundant cross-client instruction sets. Extensive experiments across multiple domains demonstrate FedDCAâ€™s superiority over eleven baselines, achieving performance gains of up to 29.19% and domain coverage improvements of 4.82%-21.36%. FedDCA maintains its effectiveness in diverse and challenging scenarios, including data selection, held-out settings where task-specific public data is scarce and various data heterogeneity, with manageable privacy risks. This work clarifies critical FedDIT dynamics and presents FedDCA as an effective, privacy-preserving, and scalable solution for advancing domain-specific LLM tuning. </p>
<blockquote>
<p>é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è”é‚¦ç‰¹å®šé¢†åŸŸæŒ‡ä»¤è°ƒæ•´ï¼ˆFedDITï¼‰æ—¨åœ¨åˆ©ç”¨åˆ†å¸ƒå¼ç§æœ‰å’Œæœ‰é™æ•°æ®æé«˜åœ¨ç‰¹å®šé¢†åŸŸçš„æ€§èƒ½ï¼Œç„¶è€Œï¼Œç¡®å®šå…³é”®æ€§èƒ½é©±åŠ¨å› ç´ å’Œæœ€ä½³å¢å¼ºç­–ç•¥ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬é€šè¿‡å®è¯ç ”ç©¶è¯å®ï¼Œè·¨å®¢æˆ·ç«¯é¢†åŸŸè¦†ç›–ï¼ˆè€Œéæ•°æ®å¼‚è´¨æ€§ï¼‰æ˜¯å…³é”®è¦ç´ ã€‚éšåï¼Œæˆ‘ä»¬å¼•å…¥äº†FedDCAç®—æ³•ï¼Œè¯¥ç®—æ³•é€šè¿‡é¢å‘å¤šæ ·æ€§çš„å®¢æˆ·ç«¯ä¸­å¿ƒé€‰æ‹©å’Œæ£€ç´¢å¢å¼ºæ³•ï¼Œæ˜¾å¼åœ°æœ€å¤§åŒ–è¿™ç§è¦†ç›–ï¼Œæ„å»ºå¤šæ ·åŒ–ã€éå†—ä½™çš„è·¨å®¢æˆ·ç«¯æŒ‡ä»¤é›†ã€‚è·¨å¤šä¸ªé¢†åŸŸçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒFedDCAåœ¨è¶…è¿‡11ä¸ªåŸºå‡†æµ‹è¯•ä¸­å…·æœ‰ä¼˜åŠ¿ï¼Œæ€§èƒ½æå‡å¹…åº¦é«˜è¾¾29.19%ï¼Œé¢†åŸŸè¦†ç›–æ”¹å–„å¹…åº¦åœ¨4.82%~21.36%ä¹‹é—´ã€‚FedDCAåœ¨å¤šæ ·åŒ–çš„æŒ‘æˆ˜åœºæ™¯ä¸­ä¿æŒäº†å…¶æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬æ•°æ®é€‰æ‹©ã€ä»»åŠ¡ç‰¹å®šå…¬å¼€æ•°æ®ç¨€ç¼ºçš„ä¿ç•™è®¾ç½®ä»¥åŠå„ç§æ•°æ®å¼‚è´¨æ€§ï¼ŒåŒæ—¶å¯æ§éšç§é£é™©ã€‚è¿™é¡¹å·¥ä½œæ˜ç¡®äº†å…³é”®çš„FedDITåŠ¨æ€ï¼Œå¹¶å±•ç¤ºäº†FedDCAä½œä¸ºä¸€ç§æœ‰æ•ˆã€ä¿æŠ¤éšç§å’Œå¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆï¼Œå¯æ¨åŠ¨ç‰¹å®šé¢†åŸŸçš„LLMè°ƒä¼˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.20135v6">PDF</a> EMNLP 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è”é‚¦ç‰¹å®šåŸŸæŒ‡ä»¤è°ƒæ•´ï¼ˆFedDITï¼‰æ—¨åœ¨åˆ©ç”¨åˆ†å¸ƒå¼ç§æœ‰å’Œæœ‰é™æ•°æ®æé«˜åœ¨ç‰¹å®šé¢†åŸŸçš„æ€§èƒ½ï¼Œä½†ç¡®å®šå…³é”®æ€§èƒ½é©±åŠ¨å› ç´ å’Œæœ€ä½³å¢å¼ºç­–ç•¥ä»å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬å®è¯åœ°è¯æ˜äº†è·¨å®¢æˆ·ç«¯çš„åŸŸè¦†ç›–æ˜¯å…³é”®å› ç´ ï¼Œè€Œä¸æ˜¯æ•°æ®å¼‚è´¨æ€§ã€‚æ¥ç€å¼•å…¥äº†FedDCAç®—æ³•ï¼Œå®ƒé€šè¿‡é¢å‘å¤šæ ·æ€§çš„å®¢æˆ·ç«¯ä¸­å¿ƒé€‰æ‹©å’Œæ£€ç´¢å¢å¼ºæ–¹æ³•ï¼Œæ˜¾å¼åœ°æœ€å¤§åŒ–è¿™ç§è¦†ç›–ï¼Œæ„å»ºå¤šæ ·ä¸”éå†—ä½™çš„è·¨å®¢æˆ·ç«¯æŒ‡ä»¤é›†ã€‚åœ¨å¤šé¢†åŸŸçš„å¹¿æ³›å®éªŒè¯æ˜ï¼ŒFedDCAä¼˜äºåä¸€ç§åŸºçº¿æ–¹æ³•ï¼Œå®ç°äº†é«˜è¾¾29.19%çš„æ€§èƒ½æå‡å’Œ4.82%-21.36%çš„åŸŸè¦†ç›–æ”¹å–„ã€‚FedDCAåœ¨æ•°æ®é€‰æ‹©ã€ä»»åŠ¡ç‰¹å®šå…¬å¼€æ•°æ®ç¨€ç¼ºçš„ä¿ç•™è®¾ç½®ä»¥åŠå¤šç§æ•°æ®å¼‚è´¨æ€§ç­‰å¤šæ ·å’Œå¤æ‚åœºæ™¯ä¸­ä¿æŒå…¶æœ‰æ•ˆæ€§ï¼ŒåŒæ—¶ç®¡ç†éšç§é£é™©ã€‚æœ¬ç ”ç©¶æ˜ç¡®äº†å…³é”®çš„FedDITåŠ¨æ€ï¼Œå¹¶æå‡ºFedDCAæ˜¯ä¸€ç§æœ‰æ•ˆã€éšç§ä¿æŠ¤ã€å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆï¼Œå¯æ¨åŠ¨ç‰¹å®šé¢†åŸŸçš„LLMè°ƒä¼˜ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è”é‚¦ç‰¹å®šåŸŸæŒ‡ä»¤è°ƒæ•´ï¼ˆFedDITï¼‰æ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç‰¹å®šé¢†åŸŸçš„æ€§èƒ½ã€‚</li>
<li>è·¨å®¢æˆ·ç«¯çš„åŸŸè¦†ç›–æ˜¯æå‡LLMæ€§èƒ½çš„å…³é”®å› ç´ ã€‚</li>
<li>FedDCAç®—æ³•é€šè¿‡å¤šæ ·æ€§å¯¼å‘çš„å®¢æˆ·ç«¯ä¸­å¿ƒé€‰æ‹©å’Œæ£€ç´¢å¢å¼ºæ–¹æ³•ï¼Œæœ€å¤§åŒ–è·¨å®¢æˆ·ç«¯çš„åŸŸè¦†ç›–ã€‚</li>
<li>FedDCAåœ¨å¤šä¸ªé¢†åŸŸå®éªŒä¸­è¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•æœ‰é«˜è¾¾29.19%çš„æ€§èƒ½æå‡ã€‚</li>
<li>FedDCAåœ¨æ•°æ®é€‰æ‹©ã€ä»»åŠ¡ç‰¹å®šå…¬å¼€æ•°æ®ç¨€ç¼ºä»¥åŠæ•°æ®å¼‚è´¨æ€§ç­‰å¤æ‚åœºæ™¯ä¸­è¡¨ç°ç¨³å¥ã€‚</li>
<li>FedDCAèƒ½æœ‰æ•ˆç®¡ç†éšç§é£é™©ã€‚</li>
<li>æ­¤ç ”ç©¶æ˜ç¡®äº†å…³é”®çš„FedDITåŠ¨æ€ï¼Œå¹¶æå‡ºFedDCAä½œä¸ºä¸€ç§æœ‰æ•ˆã€å¯æ‰©å±•åˆ°ç‰¹å®šé¢†åŸŸçš„LLMè°ƒä¼˜è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.20135">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-1613f6279a50213b24c2a7fd7d84d250.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-819299dca0151a8448ab2d8094fc50e8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-15d61d0606b635ddd0aace74426c72c5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-300873ceaaa6af410fadabf405f4affe.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Judging-the-Judges-Evaluating-Alignment-and-Vulnerabilities-in-LLMs-as-Judges"><a href="#Judging-the-Judges-Evaluating-Alignment-and-Vulnerabilities-in-LLMs-as-Judges" class="headerlink" title="Judging the Judges: Evaluating Alignment and Vulnerabilities in   LLMs-as-Judges"></a>Judging the Judges: Evaluating Alignment and Vulnerabilities in   LLMs-as-Judges</h2><p><strong>Authors:Aman Singh Thakur, Kartik Choudhary, Venkat Srinik Ramayapally, Sankaran Vaidyanathan, Dieuwke Hupkes</strong></p>
<p>Offering a promising solution to the scalability challenges associated with human evaluation, the LLM-as-a-judge paradigm is rapidly gaining traction as an approach to evaluating large language models (LLMs). However, there are still many open questions about the strengths and weaknesses of this paradigm, and what potential biases it may hold. In this paper, we present a comprehensive study of the performance of various LLMs acting as judges, focusing on a clean scenario in which inter-human agreement is high. Investigating thirteen judge models of different model sizes and families, judging answers of nine different â€˜examtaker modelsâ€™ - both base and instruction-tuned - we find that only the best (and largest) models achieve reasonable alignment with humans. However, they are still quite far behind inter-human agreement and their assigned scores may still differ with up to 5 points from human-assigned scores. In terms of their ranking of the nine exam-taker models, instead, also smaller models and even the lexical metric contains may provide a reasonable signal. Through error analysis and other studies, we identify vulnerabilities in judge models, such as their sensitivity to prompt complexity and length, and a tendency toward leniency. The fact that even the best judges differ from humans in this comparatively simple setup suggest that caution may be wise when using judges in more complex setups. Lastly, our research rediscovers the importance of using alignment metrics beyond simple percent alignment, showing that judges with high percent agreement can still assign vastly different scores. </p>
<blockquote>
<p>é’ˆå¯¹ä¸äººç±»è¯„ä¼°ç›¸å…³çš„å¯æ‰©å±•æ€§æŒ‘æˆ˜ï¼ŒLLMä½œä¸ºæ³•å®˜çš„èŒƒå¼ä¸ºè§£å†³è¯¥é—®é¢˜æä¾›äº†å‰æ™¯å¹¿é˜”çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶ä½œä¸ºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¸€ç§æ–¹æ³•è¿…é€Ÿè·å¾—äº†æ”¯æŒã€‚ç„¶è€Œï¼Œå…³äºè¯¥èŒƒå¼çš„ä¼˜ç¼ºç‚¹åŠå…¶å¯èƒ½å­˜åœ¨çš„æ½œåœ¨åè§ï¼Œä»æœ‰è®¸å¤šæ‚¬è€Œæœªå†³çš„é—®é¢˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¯¹ä½œä¸ºæ³•å®˜çš„å„ç§LLMçš„æ€§èƒ½è¿›è¡Œäº†å…¨é¢çš„ç ”ç©¶ï¼Œé‡ç‚¹å…³æ³¨äººç±»ä¹‹é—´å…±è¯†åº¦è¾ƒé«˜çš„æ¸…æ´åœºæ™¯ã€‚æˆ‘ä»¬è°ƒæŸ¥äº†13ä¸ªä¸åŒè§„æ¨¡å’Œå®¶æ—çš„æ³•å®˜æ¨¡å‹ï¼Œè¯„ä¼°äº†9ä¸ªä¸åŒå‚åŠ è€ƒè¯•æ¨¡å‹çš„ç­”æ¡ˆï¼ˆåŒ…æ‹¬åŸºç¡€æ¨¡å‹å’ŒæŒ‡ä»¤è°ƒä¼˜æ¨¡å‹ï¼‰ï¼Œæˆ‘ä»¬å‘ç°åªæœ‰æœ€å¥½çš„ï¼ˆä¹Ÿæ˜¯æœ€å¤§çš„ï¼‰æ¨¡å‹æ‰èƒ½å®ç°ä¸äººç±»çš„åˆç†å¯¹é½ã€‚ç„¶è€Œï¼Œå®ƒä»¬ä»ç„¶è¿œè¿œè½åäºäººç±»ä¹‹é—´çš„å…±è¯†ï¼Œå…¶åˆ†é…çš„åˆ†æ•°å¯èƒ½ä¸äººç±»åˆ†é…çš„åˆ†æ•°ç›¸å·®é«˜è¾¾5åˆ†ã€‚è€Œåœ¨å¯¹ä¹ä¸ªè€ƒè¯•æ¨¡å‹çš„æ’åæ–¹é¢ï¼Œè¾ƒå°çš„æ¨¡å‹ç”šè‡³è¯æ±‡åº¦é‡ä¹Ÿå¯èƒ½æä¾›åˆç†çš„ä¿¡å·ã€‚é€šè¿‡é”™è¯¯åˆ†æå’Œå…¶ä»–ç ”ç©¶ï¼Œæˆ‘ä»¬å‘ç°äº†æ³•å®˜æ¨¡å‹çš„æ¼æ´ï¼Œå¦‚å®ƒä»¬å¯¹æç¤ºçš„å¤æ‚æ€§å’Œé•¿åº¦çš„æ•æ„Ÿæ€§ï¼Œä»¥åŠå€¾å‘äºå®½å®¹çš„è¶‹åŠ¿ã€‚å³ä½¿åœ¨ç›¸å¯¹ç®€å•çš„è®¾ç½®ä¸­ï¼Œå³ä½¿æœ€å¥½çš„æ³•å®˜ä¹Ÿä¸äººç±»å­˜åœ¨å·®å¼‚ï¼Œå› æ­¤åœ¨æ›´å¤æ‚çš„è®¾ç½®ä¸­ä½¿ç”¨æ³•å®˜æ—¶å¯èƒ½éœ€è¦è°¨æ…ã€‚æœ€åï¼Œæˆ‘ä»¬çš„ç ”ç©¶é‡æ–°å‘ç°äº†ä½¿ç”¨è¶…è¶Šç®€å•ç™¾åˆ†æ¯”å¯¹é½çš„å¯¹é½æŒ‡æ ‡çš„é‡è¦æ€§ï¼Œè¡¨æ˜å³ä½¿ç™¾åˆ†æ¯”åè®®å¾ˆé«˜çš„æ³•å®˜ä»ç„¶å¯ä»¥åˆ†é…æˆªç„¶ä¸åŒçš„åˆ†æ•°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.12624v6">PDF</a> <a target="_blank" rel="noopener" href="https://aclanthology.org/2025.gem-1.33/">https://aclanthology.org/2025.gem-1.33/</a></p>
<p><strong>Summary</strong></p>
<p>LLMä½œä¸ºè¯„ä¼°è€…çš„æ¨¡å¼åœ¨è§£å†³è¯­è¨€æ¨¡å‹è¯„ä¼°çš„å¯æ‰©å±•æ€§æŒ‘æˆ˜æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚æœ¬æ–‡å…¨é¢ç ”ç©¶äº†ä¸åŒå¤§å°çš„LLMæ¨¡å‹ä½œä¸ºè¯„å§”çš„è¡¨ç°ï¼Œå‘ç°åªæœ‰æœ€ä¼˜ç§€çš„æ¨¡å‹æ‰èƒ½è¾¾åˆ°ä¸äººç±»ä¹‹é—´çš„åˆç†å¯¹é½ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹ä»ç„¶ä¸äººç±»è¯„ä¼°å­˜åœ¨å·®è·ï¼Œä¸”å­˜åœ¨å¯¹æç¤ºå¤æ‚æ€§å’Œé•¿åº¦çš„æ•æ„Ÿæ€§å’Œå®½æ¾å€¾å‘ç­‰æ¼æ´ã€‚å› æ­¤ï¼Œåœ¨æ›´å¤æ‚çš„è®¾ç½®ä¸­ä½¿ç”¨è¿™äº›æ¨¡å‹æ—¶éœ€è¦è°¨æ…ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å‘ç°ç®€å•çš„å¯¹é½ç™¾åˆ†æ¯”ä¸èƒ½å®Œå…¨åæ˜ æ¨¡å‹çš„çœŸå®æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLM-as-a-judgeæ¨¡å¼è§£å†³äº†äººç±»è¯„ä¼°å¯æ‰©å±•æ€§çš„æŒ‘æˆ˜ã€‚</li>
<li>åªæœ‰æœ€ä¼˜ç§€çš„LLMæ¨¡å‹æ‰èƒ½å®ç°ä¸äººç±»çš„åˆç†å¯¹é½ã€‚</li>
<li>ä¼˜ç§€çš„LLMæ¨¡å‹ä»ä¸äººç±»è¯„ä¼°å­˜åœ¨å·®è·ï¼Œéœ€è°¨æ…ä½¿ç”¨äºå¤æ‚ç¯å¢ƒã€‚</li>
<li>LLMä½œä¸ºè¯„å§”å­˜åœ¨å¯¹æç¤ºå¤æ‚æ€§å’Œé•¿åº¦çš„æ•æ„Ÿæ€§å’Œå®½æ¾å€¾å‘ç­‰æ¼æ´ã€‚</li>
<li>ç®€å•çš„å¯¹é½ç™¾åˆ†æ¯”ä¸èƒ½å®Œå…¨åæ˜ æ¨¡å‹çš„çœŸå®æ€§èƒ½ã€‚</li>
<li>ç ”ç©¶é‡æ–°å‘ç°äº†ä½¿ç”¨è¶…è¶Šç®€å•ç™¾åˆ†æ¯”å¯¹é½çš„å¯¹é½æŒ‡æ ‡çš„é‡è¦æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.12624">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5f4181eacf3b0e4b24712602e2a48695.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e63ac5b0c7c98e34730daa017ce5573.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-804295d52fd69db447d6025bb8ead2fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-815a69f56e7c1a65e2ac378f4d4d23c2.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="CRISPR-GPT-for-Agentic-Automation-of-Gene-editing-Experiments"><a href="#CRISPR-GPT-for-Agentic-Automation-of-Gene-editing-Experiments" class="headerlink" title="CRISPR-GPT for Agentic Automation of Gene-editing Experiments"></a>CRISPR-GPT for Agentic Automation of Gene-editing Experiments</h2><p><strong>Authors:Yuanhao Qu, Kaixuan Huang, Ming Yin, Kanghong Zhan, Dyllan Liu, Di Yin, Henry C. Cousins, William A. Johnson, Xiaotong Wang, Mihir Shah, Russ B. Altman, Denny Zhou, Mengdi Wang, Le Cong</strong></p>
<p>The introduction of genome engineering technology has transformed biomedical research, making it possible to make precise changes to genetic information. However, creating an efficient gene-editing system requires a deep understanding of CRISPR technology, and the complex experimental systems under investigation. While Large Language Models (LLMs) have shown promise in various tasks, they often lack specific knowledge and struggle to accurately solve biological design problems. In this work, we introduce CRISPR-GPT, an LLM agent augmented with domain knowledge and external tools to automate and enhance the design process of CRISPR-based gene-editing experiments. CRISPR-GPT leverages the reasoning ability of LLMs to facilitate the process of selecting CRISPR systems, designing guide RNAs, recommending cellular delivery methods, drafting protocols, and designing validation experiments to confirm editing outcomes. We showcase the potential of CRISPR-GPT for assisting non-expert researchers with gene-editing experiments from scratch and validate the agentâ€™s effectiveness in a real-world use case. Furthermore, we explore the ethical and regulatory considerations associated with automated gene-editing design, highlighting the need for responsible and transparent use of these tools. Our work aims to bridge the gap between beginner biological researchers and CRISPR genome engineering techniques, and demonstrate the potential of LLM agents in facilitating complex biological discovery tasks. The published version of this draft is available at <a target="_blank" rel="noopener" href="https://www.nature.com/articles/s41551-025-01463-z">https://www.nature.com/articles/s41551-025-01463-z</a>. </p>
<blockquote>
<p>åŸºå› ç»„å·¥ç¨‹æŠ€æœ¯çš„å¼•å…¥å·²ç»å½»åº•æ”¹å˜äº†ç”Ÿç‰©åŒ»å­¦ç ”ç©¶ï¼Œä½¿å¾—å¯¹é—ä¼ ä¿¡æ¯è¿›è¡Œç²¾ç¡®ä¿®æ”¹æˆä¸ºå¯èƒ½ã€‚ç„¶è€Œï¼Œè¦åˆ›å»ºä¸€ä¸ªé«˜æ•ˆçš„åŸºå› ç¼–è¾‘ç³»ç»Ÿï¼Œéœ€è¦æ·±å…¥äº†è§£CRISPRæŠ€æœ¯ä»¥åŠå¤æ‚çš„å®éªŒç³»ç»Ÿã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ç§ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å®ƒä»¬é€šå¸¸ç¼ºä¹ç‰¹å®šçŸ¥è¯†ï¼Œéš¾ä»¥å‡†ç¡®è§£å†³ç”Ÿç‰©è®¾è®¡é—®é¢˜ã€‚åœ¨æˆ‘ä»¬çš„å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†CRISPR-GPTï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡é¢†åŸŸçŸ¥è¯†å’Œå¤–éƒ¨å·¥å…·å¢å¼ºåŠŸèƒ½çš„å¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†ï¼Œå¯ä»¥è‡ªåŠ¨åŒ–å’Œå¢å¼ºCRISPRåŸºå› ç¼–è¾‘å®éªŒçš„è®¾è®¡è¿‡ç¨‹ã€‚CRISPR-GPTåˆ©ç”¨LLMçš„æ¨ç†èƒ½åŠ›æ¥ä¿ƒè¿›é€‰æ‹©CRISPRç³»ç»Ÿã€è®¾è®¡å¼•å¯¼RNAã€æ¨èç»†èƒä¼ é€’æ–¹æ³•ã€èµ·è‰åè®®ä»¥åŠè®¾è®¡éªŒè¯å®éªŒä»¥ç¡®è®¤ç¼–è¾‘ç»“æœçš„è¿‡ç¨‹ã€‚æˆ‘ä»¬å±•ç¤ºäº†CRISPR-GPTåœ¨ååŠ©éä¸“ä¸šç ”ç©¶äººå‘˜è¿›è¡ŒåŸºå› ç¼–è¾‘å®éªŒæ–¹é¢çš„æ½œåŠ›ï¼Œå¹¶é€šè¿‡å®é™…ä½¿ç”¨æ¡ˆä¾‹éªŒè¯äº†è¯¥ä»£ç†çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¢è®¨äº†ä¸è‡ªåŠ¨åŒ–åŸºå› ç¼–è¾‘è®¾è®¡ç›¸å…³çš„ä¼¦ç†å’Œç›‘ç®¡é—®é¢˜ï¼Œå¼ºè°ƒè¿™äº›å·¥å…·éœ€è¦è´Ÿè´£ä»»å’Œé€æ˜åœ°ä½¿ç”¨ã€‚æˆ‘ä»¬çš„å·¥ä½œæ—¨åœ¨ç¼©å°åˆçº§ç”Ÿç‰©ç ”ç©¶äººå‘˜ä¸CRISPRåŸºå› ç»„å·¥ç¨‹æŠ€æœ¯ä¹‹é—´çš„å·®è·ï¼Œå¹¶å±•ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†åœ¨ä¿ƒè¿›å¤æ‚ç”Ÿç‰©å­¦å‘ç°ä»»åŠ¡ä¸­çš„æ½œåŠ›ã€‚è¯¥è®ºæ–‡çš„å‘å¸ƒç‰ˆæœ¬å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://www.nature.com/articles/s41551-025-01463-z%E8%AE%BF%E9%97%AE%E3%80%82">https://www.nature.com/articles/s41551-025-01463-zè®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.18021v2">PDF</a> Accepted to Nature Biomedical Engineering</p>
<p><strong>Summary</strong>ï¼šCRISPRåŸºå› ç¼–è¾‘æŠ€æœ¯é©å‘½æ€§åœ°æ”¹å˜äº†ç”Ÿç‰©åŒ»å­¦ç ”ç©¶ã€‚ä¸ºäº†æé«˜åŸºå› ç¼–è¾‘çš„æ•ˆç‡ï¼Œæœ¬æ–‡å¼•å…¥CRISPR-GPTï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æŠ€æœ¯å’Œå¤–éƒ¨å·¥å…·è¾…åŠ©CRISPRåŸºå› ç¼–è¾‘å®éªŒè®¾è®¡çš„ç³»ç»Ÿã€‚CRISPR-GPTå¯å¸®åŠ©éä¸“ä¸šç ”ç©¶äººå‘˜å®Œæˆä»å®éªŒè®¾è®¡åˆ°éªŒè¯çš„å…¨è¿‡ç¨‹ï¼Œå¹¶æ¢è®¨äº†è‡ªåŠ¨åŒ–åŸºå› ç¼–è¾‘è®¾è®¡çš„ä¼¦ç†å’Œç›‘ç®¡é—®é¢˜ã€‚æœ¬æ–‡æ—¨åœ¨ç¼©å°åˆå­¦è€…ä¸CRISPRåŸºå› å·¥ç¨‹æŠ€æœ¯çš„å·®è·ï¼Œå±•ç¤ºLLMåœ¨ä¿ƒè¿›å¤æ‚ç”Ÿç‰©å­¦å‘ç°ä»»åŠ¡ä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>CRISPRåŸºå› ç¼–è¾‘æŠ€æœ¯çš„å¼•å…¥ä¸ºç”Ÿç‰©åŒ»å­¦ç ”ç©¶å¸¦æ¥äº†å·¨å¤§çš„å˜é©ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æŠ€æœ¯å¯æé«˜åŸºå› ç¼–è¾‘æ•ˆç‡ï¼Œä½†ä»ç¼ºä¹ç‰¹å®šé¢†åŸŸçš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>CRISPR-GPTç»“åˆäº†CRISPRæŠ€æœ¯å’ŒLLMçš„ä¼˜åŠ¿ï¼Œè¾…åŠ©å®ŒæˆåŸºå› ç¼–è¾‘å®éªŒçš„å…¨è¿‡ç¨‹ã€‚</li>
<li>CRISPR-GPTå…·å¤‡é€‰æ‹©CRISPRç³»ç»Ÿã€è®¾è®¡å¼•å¯¼RNAã€æ¨èç»†èƒä¼ é€’æ–¹æ³•ã€èµ·è‰åè®®å’Œè®¾è®¡éªŒè¯å®éªŒçš„èƒ½åŠ›ã€‚</li>
<li>CRISPR-GPTæœ‰åŠ©äºéä¸“ä¸šç ”ç©¶äººå‘˜è¿›è¡ŒåŸºå› ç¼–è¾‘å®éªŒï¼ŒéªŒè¯äº†å…¶åœ¨çœŸå®ä¸–ç•Œä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>è‡ªåŠ¨åŒ–åŸºå› ç¼–è¾‘è®¾è®¡éœ€è¦è€ƒè™‘ä¼¦ç†å’Œç›‘ç®¡é—®é¢˜ï¼Œéœ€è¦è´Ÿè´£ä»»å’Œé€æ˜åœ°ä½¿ç”¨è¿™äº›å·¥å…·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.18021">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-805fa45ff43a8920f0d29e82a9d8225b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4168d85011ac37c931c3111397507eac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a25e251e7d93830c61bedad54ab676cb.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-23/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-23/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-23/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-20b6292f2fd919823d826d9270025daf.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-23  Distributed Detection of Adversarial Attacks in Multi-Agent   Reinforcement Learning with Continuous Action Space
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-23
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-23/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-149c94281c63fab0be861c1249be1603.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-23  Intern-S1 A Scientific Multimodal Foundation Model
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-23
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">28051.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
