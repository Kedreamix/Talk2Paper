<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM 方向最新论文已更新，请持续关注 Update in 2025-08-23  End-to-End Agentic RAG System Training for Traceable Diagnostic   Reasoning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-f1ff94757caca643ac10254ed3e86f94.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-08-23
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-09-08
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    13.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    55 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-08-23-更新"><a href="#2025-08-23-更新" class="headerlink" title="2025-08-23 更新"></a>2025-08-23 更新</h1><h2 id="End-to-End-Agentic-RAG-System-Training-for-Traceable-Diagnostic-Reasoning"><a href="#End-to-End-Agentic-RAG-System-Training-for-Traceable-Diagnostic-Reasoning" class="headerlink" title="End-to-End Agentic RAG System Training for Traceable Diagnostic   Reasoning"></a>End-to-End Agentic RAG System Training for Traceable Diagnostic   Reasoning</h2><p><strong>Authors:Qiaoyu Zheng, Yuze Sun, Chaoyi Wu, Weike Zhao, Pengcheng Qiu, Yongguo Yu, Kun Sun, Yanfeng Wang, Ya Zhang, Weidi Xie</strong></p>
<p>Accurate diagnosis with medical large language models is hindered by knowledge gaps and hallucinations. Retrieval and tool-augmented methods help, but their impact is limited by weak use of external knowledge and poor feedback-reasoning traceability. To address these challenges, We introduce Deep-DxSearch, an agentic RAG system trained end-to-end with reinforcement learning (RL) that enables steer tracebale retrieval-augmented reasoning for medical diagnosis. In Deep-DxSearch, we first construct a large-scale medical retrieval corpus comprising patient records and reliable medical knowledge sources to support retrieval-aware reasoning across diagnostic scenarios. More crutially, we frame the LLM as the core agent and the retrieval corpus as its environment, using tailored rewards on format, retrieval, reasoning structure, and diagnostic accuracy, thereby evolving the agentic RAG policy from large-scale data through RL.   Experiments demonstrate that our end-to-end agentic RL training framework consistently outperforms prompt-engineering and training-free RAG approaches across multiple data centers. After training, Deep-DxSearch achieves substantial gains in diagnostic accuracy, surpassing strong diagnostic baselines such as GPT-4o, DeepSeek-R1, and other medical-specific frameworks for both common and rare disease diagnosis under in-distribution and out-of-distribution settings. Moreover, ablation studies on reward design and retrieval corpus components confirm their critical roles, underscoring the uniqueness and effectiveness of our approach compared with traditional implementations. Finally, case studies and interpretability analyses highlight improvements in Deep-DxSearch’s diagnostic policy, providing deeper insight into its performance gains and supporting clinicians in delivering more reliable and precise preliminary diagnoses. See <a target="_blank" rel="noopener" href="https://github.com/MAGIC-AI4Med/Deep-DxSearch">https://github.com/MAGIC-AI4Med/Deep-DxSearch</a>. </p>
<blockquote>
<p>使用医疗大型语言模型的准确诊断受到知识差距和幻觉的阻碍。检索和工具增强方法有所帮助，但其影响受限于外部知识利用不足和反馈推理可追溯性较差。为了解决这些挑战，我们引入了Deep-DxSearch，这是一个以强化学习（RL）进行端到端训练的主动推理图系统（RAG），可实现用于医学诊断的可追溯检索增强推理。在Deep-DxSearch中，我们首先构建了一个大规模医学检索语料库，包含患者记录和可靠医学知识源，以支持跨诊断场景的检索感知推理。更为关键的是，我们将大型语言模型作为核心主体，将检索语料库作为其环境，针对格式、检索、推理结构和诊断准确性设计定制奖励，从而通过强化学习推动主动推理图策略从大规模数据中进化。实验表明，我们的端到端主动强化学习训练框架在多数据中心均表现优异，超过基于提示和免训练RAG方法。经过训练后，Deep-DxSearch在诊断准确性方面取得了实质性提升，在分布内和分布外设置下均超越了强大的诊断基线，如GPT-4o、DeepSeek-R1和其他医学专用框架，无论是常见疾病还是罕见疾病的诊断均如此。此外，对奖励设计和检索语料库组件的消融研究证实了它们的关键作用，强调了我们的方法与传统实施相比的独特性和有效性。最后，案例研究和可解释性分析突出了Deep-DxSearch诊断策略的改进，为深入了解其性能提升提供了依据，并支持临床医生提供更可靠和精确的早期诊断。更多信息请参见<a target="_blank" rel="noopener" href="https://github.com/MAGIC-AI4Med/DeepDxSearch%E3%80%82">https://github.com/MAGIC-AI4Med/DeepDxSearch。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15746v1">PDF</a> 35 pages, 5 figures, 3 tables</p>
<p><strong>Summary</strong></p>
<p>在医疗大语言模型进行准确诊断时，存在知识缺口和虚构问题。虽然检索和工具增强方法有所帮助，但它们对外部知识利用不足，反馈推理可追溯性不佳。为解决这些问题，我们推出Deep-DxSearch，一个经过强化学习训练的自主RAG系统，可实现可追踪检索增强推理，用于医疗诊断。Deep-DxSearch构建大规模医学检索语料库，涵盖患者记录和可靠医学知识源，以支持跨诊断场景的检索感知推理。关键的是，我们以LLM为核心代理，检索语料库为其环境，使用格式、检索、推理结构和诊断准确性的定制奖励，通过大规模数据强化代理策略。实验证明，我们的终端到终端自主RL训练框架在多个数据中心均优于提示工程和免训练RAG方法。Deep-DxSearch在训练后诊断准确性大大提高，超越强大的诊断基线，如GPT-4o、DeepSeek-R1和其他医疗特定框架，适用于常见和罕见疾病的诊断，在内部和外部分布设置下均表现优异。奖励设计和检索语料库组件的消融研究证实了它们的重要性，突显了与传统方法相比的独特性和有效性。最终，案例研究和解释性分析突显了Deep-DxSearch诊断策略的改进，为性能提升提供了深入了解，并支持医生提供更可靠和精确的早期诊断。详情见[网站链接]。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>医疗大语言模型在诊断中面临知识缺口和虚构挑战。</li>
<li>Deep-DxSearch通过构建大规模医学检索语料库来支持跨诊断场景的检索感知推理。</li>
<li>LLM作为核心代理，与检索语料库互动，通过强化学习进行训练。</li>
<li>Deep-DxSearch在多种设置中超越了其他诊断和医学特定框架。</li>
<li>消融研究证实了奖励设计和检索语料库组件的重要性。</li>
<li>Deep-DxSearch改进了诊断策略，提高了诊断准确性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15746">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d1bb8f786cad4e2d5361a4b48148892a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e16115757b3c0fd580e293a4e959ec92.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a888b967c7d1c1b107fd8d54ea7bf485.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="EcomMMMU-Strategic-Utilization-of-Visuals-for-Robust-Multimodal-E-Commerce-Models"><a href="#EcomMMMU-Strategic-Utilization-of-Visuals-for-Robust-Multimodal-E-Commerce-Models" class="headerlink" title="EcomMMMU: Strategic Utilization of Visuals for Robust Multimodal   E-Commerce Models"></a>EcomMMMU: Strategic Utilization of Visuals for Robust Multimodal   E-Commerce Models</h2><p><strong>Authors:Xinyi Ling, Hanwen Du, Zhihui Zhu, Xia Ning</strong></p>
<p>E-commerce platforms are rich in multimodal data, featuring a variety of images that depict product details. However, this raises an important question: do these images always enhance product understanding, or can they sometimes introduce redundancy or degrade performance? Existing datasets are limited in both scale and design, making it difficult to systematically examine this question. To this end, we introduce EcomMMMU, an e-commerce multimodal multitask understanding dataset with 406,190 samples and 8,989,510 images. EcomMMMU is comprised of multi-image visual-language data designed with 8 essential tasks and a specialized VSS subset to benchmark the capability of multimodal large language models (MLLMs) to effectively utilize visual content. Analysis on EcomMMMU reveals that product images do not consistently improve performance and can, in some cases, degrade it. This indicates that MLLMs may struggle to effectively leverage rich visual content for e-commerce tasks. Building on these insights, we propose SUMEI, a data-driven method that strategically utilizes multiple images via predicting visual utilities before using them for downstream tasks. Comprehensive experiments demonstrate the effectiveness and robustness of SUMEI. The data and code are available through <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/submission25">https://anonymous.4open.science/r/submission25</a>. </p>
<blockquote>
<p>电子商务平台拥有丰富的多模态数据，其中包含了展示产品细节的各种图像。然而，这引发了一个重要问题：这些图像是否总是能增强对产品的理解，还是有时会引入冗余信息或降低性能？现有数据集在规模和设计上都存在局限性，很难系统地研究这个问题。为此，我们推出了EcomMMMU电子商务多模态多任务理解数据集，包含406,190个样本和8,989,510张图像。EcomMMMU由多图像视觉语言数据组成，设计了8个基本任务，并有一个专门的VSS子集，以评估多模态大型语言模型（MLLMs）有效利用视觉内容的能力。对EcomMMMU的分析表明，产品图像并不总能提高性能，有时甚至会降低性能。这表明MLLMs在有效利用丰富的视觉内容方面可能面临困难。基于这些见解，我们提出了SUMEI方法，该方法通过预测视觉效用，有选择地利用多个图像，然后再将其用于下游任务。大量的实验证明了SUMEI的有效性和稳健性。相关数据和代码可通过<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/submission25">匿名网址链接</a>获取。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15721v1">PDF</a> </p>
<p><strong>摘要</strong><br>     电商平台上丰富的多模态数据含有大量商品图片展示信息。但问题在于，这些图片是否总是能增强对商品的理解，或者是否会引入冗余信息、降低性能？现有数据集在规模和设计上的局限性，使得难以系统地研究这一问题。因此，我们推出了EcomMMMU电商多模态多任务理解数据集，包含40万6千多条样本和超过八百万张图片。它涵盖了多图像视觉语言数据设计的八个基本任务和一个专门的VSS子集，旨在评估多模态大型语言模型有效利用视觉内容的能力。对EcomMMMU的分析显示，商品图片并不总是能提高性能，有时甚至会降低性能。这表明大型语言模型可能难以有效地利用丰富的视觉内容完成电商任务。基于此洞察，我们提出SUMEI方法，它通过预测视觉效用，有针对性地使用多张图片来完成下游任务。经过综合实验验证，SUMEI具有有效性和稳健性。相关数据和代码可通过链接<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/submission25%E8%8E%B7%E5%8F%96%E3%80%82">https://anonymous.4open.science/r/submission25获取。</a></p>
<p><strong>关键见解</strong></p>
<ol>
<li>电商平台上商品图片丰富，但图片是否有助于增强对商品的理解尚存疑问。</li>
<li>当前数据集在规模和设计上的局限性，使得难以系统地研究这一问题。</li>
<li>引入EcomMMMU数据集，包含多模态多任务设计，旨在评估大型语言模型利用视觉内容的能力。</li>
<li>分析显示商品图片并不总是能提高性能，有时可能降低性能。</li>
<li>大型语言模型在有效利用丰富视觉内容方面可能存在挑战。</li>
<li>提出SUMEI方法，通过预测视觉效用有针对性地使用多张图片完成下游任务。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15721">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-cc6f3a0df8d7ffc3eb53ed50dd381849.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-06443c7ec872e7293906bf46a2981f28.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e31de151d189e08cf560c857cdceffbd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ea205949a6fc9405c318fb3dba6bb1f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-842970e813385bd35daa9198ee2c16e5.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="StreamMem-Query-Agnostic-KV-Cache-Memory-for-Streaming-Video-Understanding"><a href="#StreamMem-Query-Agnostic-KV-Cache-Memory-for-Streaming-Video-Understanding" class="headerlink" title="StreamMem: Query-Agnostic KV Cache Memory for Streaming Video   Understanding"></a>StreamMem: Query-Agnostic KV Cache Memory for Streaming Video   Understanding</h2><p><strong>Authors:Yanlai Yang, Zhuokai Zhao, Satya Narayan Shukla, Aashu Singh, Shlok Kumar Mishra, Lizhu Zhang, Mengye Ren</strong></p>
<p>Multimodal large language models (MLLMs) have made significant progress in visual-language reasoning, but their ability to efficiently handle long videos remains limited. Despite recent advances in long-context MLLMs, storing and attending to the key-value (KV) cache for long visual contexts incurs substantial memory and computational overhead. Existing visual compression methods require either encoding the entire visual context before compression or having access to the questions in advance, which is impractical for long video understanding and multi-turn conversational settings. In this work, we propose StreamMem, a query-agnostic KV cache memory mechanism for streaming video understanding. Specifically, StreamMem encodes new video frames in a streaming manner, compressing the KV cache using attention scores between visual tokens and generic query tokens, while maintaining a fixed-size KV memory to enable efficient question answering (QA) in memory-constrained, long-video scenarios. Evaluation on three long video understanding and two streaming video question answering benchmarks shows that StreamMem achieves state-of-the-art performance in query-agnostic KV cache compression and is competitive with query-aware compression approaches. </p>
<blockquote>
<p>多模态大型语言模型（MLLMs）在视觉语言推理方面取得了显著进展，但其高效处理长视频的能力仍然有限。尽管近期在长语境MLLMs方面有所进展，但为长视觉语境存储和关注键值（KV）缓存会产生大量内存和计算开销。现有视觉压缩方法要么需要在压缩之前对整个视觉环境进行编码，要么需要提前了解问题，这对于长视频理解和多回合对话环境来说并不实用。在这项工作中，我们提出了StreamMem，这是一种用于流式视频理解的查询无关键值缓存内存机制。具体来说，StreamMem以流式方式编码新视频帧，使用视觉令牌和通用查询令牌之间的注意力分数压缩键值缓存，同时保持固定大小的键值内存，以在内存受限的长视频场景中实现高效问答（QA）。在三个长视频理解和两个流式视频问答基准测试上的评估表明，StreamMem在查询无关的键值缓存压缩方面达到了最新技术水平，并且在查询感知压缩方法中表现具有竞争力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15717v1">PDF</a> 15 pages, 3 figures</p>
<p><strong>Summary</strong></p>
<p>MLLM在处理长视频方面存在效率限制，特别是在存储和关注长视觉上下文的关键值缓存时会产生大量的内存和计算开销。现有视觉压缩方法需要提前对整个视觉上下文进行编码或提前获取问题，这对于长视频理解和多轮对话场景来说不切实际。本研究提出了一种名为StreamMem的查询无关的关键值缓存记忆机制，以流式处理视频理解。它通过关注视觉令牌和通用查询令牌之间的注意力分数来压缩关键值缓存，同时保持固定大小的关键值内存，以在内存受限的长视频场景中实现高效的问题回答。在三个长视频理解和两个流式视频问答基准测试上的评估表明，StreamMem在查询无关的关键值缓存压缩方面达到了最新技术水平，并且在查询感知压缩方法中具有很强的竞争力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLM在处理长视频时存在效率限制，特别是在处理视觉上下文的关键值缓存时面临内存和计算开销的挑战。</li>
<li>现有视觉压缩方法在处理长视频理解和多轮对话场景时存在困难，需要提前对视觉上下文进行编码或提前获取问题。</li>
<li>StreamMem是一种查询无关的关键值缓存记忆机制，以流式处理视频理解，能够压缩关键值缓存并维持固定大小的关键值内存。</li>
<li>StreamMem通过关注视觉令牌和通用查询令牌之间的注意力分数来实现高效的视频理解。</li>
<li>StreamMem在多个基准测试上表现优异，实现了最新的技术性能，在查询感知压缩方法中具有很强的竞争力。</li>
<li>StreamMem适用于内存受限的长视频场景中的高效问题回答。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15717">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-84d7d70ece0f5a451bf042986592922d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3a8ccecef93be255c1f43afea49cc93e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad8c856fbaba7a3fa93640bb0fdd200a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7b32da0b8cb7d5d77003f3d030d0eadc.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="LLaSO-A-Foundational-Framework-for-Reproducible-Research-in-Large-Language-and-Speech-Model"><a href="#LLaSO-A-Foundational-Framework-for-Reproducible-Research-in-Large-Language-and-Speech-Model" class="headerlink" title="LLaSO: A Foundational Framework for Reproducible Research in Large   Language and Speech Model"></a>LLaSO: A Foundational Framework for Reproducible Research in Large   Language and Speech Model</h2><p><strong>Authors:Yirong Sun, Yizhong Geng, Peidong Wei, Yanjun Chen, Jinghan Yang, Rongfei Chen, Wei Zhang, Xiaoyu Shen</strong></p>
<p>The development of Large Speech-Language Models (LSLMs) has been slowed by fragmented architectures and a lack of transparency, hindering the systematic comparison and reproducibility of research. Unlike in the vision-language domain, the LSLM field suffers from the common practice of releasing model weights without their corresponding training data and configurations. To address these critical gaps, we introduce LLaSO, the first fully open, end-to-end framework for large-scale speech-language modeling. LLaSO provides the community with three essential resources: (1) LLaSO-Align, a 12M-instance speech-text alignment corpus; (2) LLaSO-Instruct, a 13.5M-instance multi-task instruction-tuning dataset; and (3) LLaSO-Eval, a reproducible benchmark for standardized evaluation. To validate our framework, we build and release LLaSO-Base, a 3.8B-parameter reference model trained exclusively on our public data. It achieves a normalized score of 0.72, establishing a strong, reproducible baseline that surpasses comparable models. Our analysis reveals that while broader training coverage enhances performance, significant generalization gaps persist on unseen tasks, particularly in pure audio scenarios. By releasing the complete stack of data, benchmarks, and models, LLaSO establishes a foundational open standard to unify research efforts and accelerate community-driven progress in LSLMs. We release the code, dataset, pretrained models, and results in <a target="_blank" rel="noopener" href="https://github.com/EIT-NLP/LLaSO">https://github.com/EIT-NLP/LLaSO</a>. </p>
<blockquote>
<p>大规模语言模型（LSLM）的发展受到了架构分散和透明度不足的阻碍，这妨碍了研究的系统比较和可重复性。与视觉语言领域不同，LSLM领域普遍存在发布模型权重而没有相应训练数据和配置的做法。为了解决这些关键差距，我们推出了LLaSO，这是第一个完全开放的大规模语言建模端到端框架。LLaSO为社区提供了三种重要资源：（1）LLaSO-Align，一个包含1200万实例的语音文本对齐语料库；（2）LLaSO-Instruct，一个包含1350万实例的多任务指令微调数据集；（3）LLaSO-Eval，一个可复现的标准化评估基准。为了验证我们的框架，我们构建并发布了LLaSO-Base，这是一个仅在我们公开数据上训练的38亿参数参考模型，其归一化得分为0.72，建立了强大且可复现的基线，超越了同类模型。我们的分析表明，虽然更广泛的训练覆盖有助于提高性能，但在未见任务上仍然存在重大的泛化差距，特别是在纯音频场景中。通过发布数据、基准测试和模型的完整堆栈，LLaSO建立了一个统一的开放标准，以整合研究努力并加速LSLM的社区驱动进展。我们在<a target="_blank" rel="noopener" href="https://github.com/EIT-NLP/LLaSO">https://github.com/EIT-NLP/LLaSO</a>发布代码、数据集、预训练模型和结果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15418v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>大语音语言模型（LSLM）的发展受到了架构碎片化及缺乏透明度的影响，阻碍了研究的系统比较和可重复性。针对这一问题，我们推出LLaSO，首个完全开放的大规模语音语言建模端到端框架。LLaSO为社区提供三大资源：LLaSO-Align语音文本对齐语料库、LLaSO-Instruct多任务指令调整数据集及LLaSO-Eval可重复基准测试平台，用于标准化评估。基于该框架，我们构建了仅使用公开数据训练的LLaSO-Base基准模型，参数规模达3.8B，归一化得分为0.72，在可比较模型中表现优异且可复制性强。研究分析表明，尽管更广泛的训练覆盖有助于提升性能，但在未见任务上仍存在重大泛化差距，特别是在纯音频场景中。我们发布整套数据、基准测试及模型，LLaSO确立了一个统一研究努力、加速LSLM社区驱动发展的开放标准。相关代码、数据集、预训练模型及结果已发布在<a target="_blank" rel="noopener" href="https://github.com/EIT-NLP/LLaSO%E3%80%82">https://github.com/EIT-NLP/LLaSO。</a></p>
<p><strong>关键见解</strong></p>
<ol>
<li>LSLM领域发展受到架构碎片化和透明度缺乏的阻碍，影响研究的比较和可重复性。</li>
<li>推出LLaSO框架，为LSLM研究提供全面开放的资源，包括语料库、数据集和评估平台。</li>
<li>基于LLaSO框架建立LLaSO-Base模型，在标准化测试中表现优异。</li>
<li>研究发现更广泛的训练覆盖有助于提升性能，但未见任务上的泛化能力仍有差距。</li>
<li>LLaSO的建立为LSLM研究提供了统一的开放标准，有助于整合研究力量，加速发展。</li>
<li>LLaSO框架及相关资源已完全公开，方便研究人员使用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15418">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-82ea5c2f9cc714d5d14d195629163951.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bc53c3768e8b27710c2e77db6445dbae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a922bc7fd3be110ed015c6ec1cadbbef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0e9abfef04ff1dea1d2cae41e78fe66f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2faae3ae7d4fb343b0e33e464162f12c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0dfeb191cd1071d3bfd758c417c5d9b1.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Improving-LLMs-for-Machine-Translation-Using-Synthetic-Preference-Data"><a href="#Improving-LLMs-for-Machine-Translation-Using-Synthetic-Preference-Data" class="headerlink" title="Improving LLMs for Machine Translation Using Synthetic Preference Data"></a>Improving LLMs for Machine Translation Using Synthetic Preference Data</h2><p><strong>Authors:Dario Vajda, Domen Vreš, Marko Robnik-Šikonja</strong></p>
<p>Large language models have emerged as effective machine translation systems. In this paper, we explore how a general instruction-tuned large language model can be improved for machine translation using relatively few easily produced data resources. Using Slovene as a use case, we improve the GaMS-9B-Instruct model using Direct Preference Optimization (DPO) training on a programmatically curated and enhanced subset of a public dataset. As DPO requires pairs of quality-ranked instances, we generated its training dataset by translating English Wikipedia articles using two LLMs, GaMS-9B-Instruct and EuroLLM-9B-Instruct. We ranked the resulting translations based on heuristics coupled with automatic evaluation metrics such as COMET. The evaluation shows that our fine-tuned model outperforms both models involved in the dataset generation. In comparison to the baseline models, the fine-tuned model achieved a COMET score gain of around 0.04 and 0.02, respectively, on translating Wikipedia articles. It also more consistently avoids language and formatting errors. </p>
<blockquote>
<p>大型语言模型已经作为有效的机器翻译系统出现。在本文中，我们探讨了如何通过利用相对容易获取的数据资源，对通用指令调整的大型语言模型进行改进，以提高其机器翻译的性能。以斯洛文尼亚语为使用案例，我们通过对公共数据集的编程策划和增强子集进行直接偏好优化（DPO）训练，改进了GaMS-9B-Instruct模型。由于DPO需要成对的质量排名实例，我们通过使用两个大型语言模型GaMS-9B-Instruct和EuroLLM-9B-Instruct翻译英文维基百科文章来生成其训练数据集。我们根据与自动评估指标（如COMET）结合的启发式方法，对所得翻译进行排名。评估结果表明，我们微调的模型在翻译维基百科文章方面的表现优于参与数据集生成的这两个模型。与基线模型相比，微调模型在维基百科文章翻译上的COMET得分分别提高了约0.04和0.02。此外，它还能更一致地避免语言和格式错误。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14951v1">PDF</a> Paper with individual presentation at LUHME workshop at ECAI 2025</p>
<p><strong>Summary</strong></p>
<p>大型语言模型已成为有效的机器翻译系统。本文探索了如何使用少量易获取的数据资源改进通用指令调整的大型语言模型。以斯洛文尼亚语为例，我们改进了GaMS-9B-Instruct模型，采用Direct Preference Optimization（DPO）训练法对公开数据集的一个程序化精选增强子集进行训练。由于DPO需要质量排序的实例对，我们通过使用两个大型语言模型GaMS-9B-Instruct和EuroLLM-9B-Instruct翻译英文维基百科文章来生成其训练数据集。我们根据启发式方法和自动评估指标（如COMET）对翻译结果进行了排名。评估表明，我们的精细调整模型优于参与数据集生成的模型。与基线模型相比，精细调整模型在翻译维基百科文章时的COMET得分分别提高了约0.04和0.02。它还更一致地避免了语言和格式错误。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型在机器翻译领域表现卓越。</li>
<li>使用斯洛文尼亚语作为案例，对通用指令调整的大型语言模型进行了改进。</li>
<li>采用Direct Preference Optimization（DPO）训练法优化模型性能。</li>
<li>训练数据集是通过两个大型语言模型翻译英文维基百科文章并使用质量评估排名生成的。</li>
<li>精细调整模型的性能优于参与数据集生成的模型。</li>
<li>与基线模型相比，精细调整模型在COMET得分上有所提高。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14951">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-fc1fb53c175154743717b632ffaa72e0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-825f27310517f93e5920a087f52a9705.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0332fb1aed2860ff291766fbd867f85e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-03e10e461ba503baab747103bc7244ce.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0f32a6762dca887bcb5a1b8177ce326e.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="NVIDIA-Nemotron-Nano-2-An-Accurate-and-Efficient-Hybrid-Mamba-Transformer-Reasoning-Model"><a href="#NVIDIA-Nemotron-Nano-2-An-Accurate-and-Efficient-Hybrid-Mamba-Transformer-Reasoning-Model" class="headerlink" title="NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid   Mamba-Transformer Reasoning Model"></a>NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid   Mamba-Transformer Reasoning Model</h2><p><strong>Authors: NVIDIA,  :, Aarti Basant, Abhijit Khairnar, Abhijit Paithankar, Abhinav Khattar, Adithya Renduchintala, Aditya Malte, Akhiad Bercovich, Akshay Hazare, Alejandra Rico, Aleksander Ficek, Alex Kondratenko, Alex Shaposhnikov, Alexander Bukharin, Ali Taghibakhshi, Amelia Barton, Ameya Sunil Mahabaleshwarkar, Amy Shen, Andrew Tao, Ann Guan, Anna Shors, Anubhav Mandarwal, Arham Mehta, Arun Venkatesan, Ashton Sharabiani, Ashwath Aithal, Ashwin Poojary, Ayush Dattagupta, Balaram Buddharaju, Banghua Zhu, Barnaby Simkin, Bilal Kartal, Bita Darvish Rouhani, Bobby Chen, Boris Ginsburg, Brandon Norick, Brian Yu, Bryan Catanzaro, Charles Wang, Charlie Truong, Chetan Mungekar, Chintan Patel, Chris Alexiuk, Christian Munley, Christopher Parisien, Dan Su, Daniel Afrimi, Daniel Korzekwa, Daniel Rohrer, Daria Gitman, David Mosallanezhad, Deepak Narayanan, Dima Rekesh, Dina Yared, Dmytro Pykhtar, Dong Ahn, Duncan Riach, Eileen Long, Elliott Ning, Eric Chung, Erick Galinkin, Evelina Bakhturina, Gargi Prasad, Gerald Shen, Haifeng Qian, Haim Elisha, Harsh Sharma, Hayley Ross, Helen Ngo, Herman Sahota, Hexin Wang, Hoo Chang Shin, Hua Huang, Iain Cunningham, Igor Gitman, Ivan Moshkov, Jaehun Jung, Jan Kautz, Jane Polak Scowcroft, Jared Casper, Jian Zhang, Jiaqi Zeng, Jimmy Zhang, Jinze Xue, Jocelyn Huang, Joey Conway, John Kamalu, Jonathan Cohen, Joseph Jennings, Julien Veron Vialard, Junkeun Yi, Jupinder Parmar, Kari Briski, Katherine Cheung, Katherine Luna, Keith Wyss, Keshav Santhanam, Kezhi Kong, Krzysztof Pawelec, Kumar Anik, Kunlun Li, Kushan Ahmadian, Lawrence McAfee, Laya Sleiman, Leon Derczynski, Luis Vega, Maer Rodrigues de Melo, Makesh Narsimhan Sreedhar, Marcin Chochowski, Mark Cai, Markus Kliegl, Marta Stepniewska-Dziubinska, Matvei Novikov, Mehrzad Samadi, Meredith Price, Meriem Boubdir, Michael Boone, Michael Evans, Michal Bien, Michal Zawalski, Miguel Martinez, Mike Chrzanowski, Mohammad Shoeybi, Mostofa Patwary, Namit Dhameja, Nave Assaf, Negar Habibi, Nidhi Bhatia, Nikki Pope, Nima Tajbakhsh, Nirmal Kumar Juluru, Oleg Rybakov, Oleksii Hrinchuk, Oleksii Kuchaiev, Oluwatobi Olabiyi, Pablo Ribalta, Padmavathy Subramanian, Parth Chadha, Pavlo Molchanov, Peter Dykas, Peter Jin, Piotr Bialecki, Piotr Januszewski, Pradeep Thalasta, Prashant Gaikwad, Prasoon Varshney, Pritam Gundecha, Przemek Tredak, Rabeeh Karimi Mahabadi, Rajen Patel, Ran El-Yaniv, Ranjit Rajan, Ria Cheruvu, Rima Shahbazyan, Ritika Borkar, Ritu Gala, Roger Waleffe, Ruoxi Zhang, Russell J. Hewett, Ryan Prenger, Sahil Jain, Samuel Kriman, Sanjeev Satheesh, Saori Kaji, Sarah Yurick, Saurav Muralidharan, Sean Narenthiran, Seonmyeong Bak, Sepehr Sameni, Seungju Han, Shanmugam Ramasamy, Shaona Ghosh, Sharath Turuvekere Sreenivas, Shelby Thomas, Shizhe Diao, Shreya Gopal, Shrimai Prabhumoye, Shubham Toshniwal, Shuoyang Ding, Siddharth Singh, Siddhartha Jain, Somshubra Majumdar, Soumye Singhal, Stefania Alborghetti, Syeda Nahida Akter, Terry Kong, Tim Moon, Tomasz Hliwiak, Tomer Asida, Tony Wang, Tugrul Konuk, Twinkle Vashishth, Tyler Poon, Udi Karpas, Vahid Noroozi, Venkat Srinivasan, Vijay Korthikanti, Vikram Fugro, Vineeth Kalluru, Vitaly Kurin, Vitaly Lavrukhin, Wasi Uddin Ahmad, Wei Du, Wonmin Byeon, Ximing Lu, Xin Dong, Yashaswi Karnati, Yejin Choi, Yian Zhang, Ying Lin, Yonggan Fu, Yoshi Suhara, Zhen Dong, Zhiyu Li, Zhongbo Zhu, Zijia Chen</strong></p>
<p>We introduce Nemotron-Nano-9B-v2, a hybrid Mamba-Transformer language model designed to increase throughput for reasoning workloads while achieving state-of-the-art accuracy compared to similarly-sized models. Nemotron-Nano-9B-v2 builds on the Nemotron-H architecture, in which the majority of the self-attention layers in the common Transformer architecture are replaced with Mamba-2 layers, to achieve improved inference speed when generating the long thinking traces needed for reasoning. We create Nemotron-Nano-9B-v2 by first pre-training a 12-billion-parameter model (Nemotron-Nano-12B-v2-Base) on 20 trillion tokens using an FP8 training recipe. After aligning Nemotron-Nano-12B-v2-Base, we employ the Minitron strategy to compress and distill the model with the goal of enabling inference on up to 128k tokens on a single NVIDIA A10G GPU (22GiB of memory, bfloat16 precision). Compared to existing similarly-sized models (e.g., Qwen3-8B), we show that Nemotron-Nano-9B-v2 achieves on-par or better accuracy on reasoning benchmarks while achieving up to 6x higher inference throughput in reasoning settings like 8k input and 16k output tokens. We are releasing Nemotron-Nano-9B-v2, Nemotron-Nano12B-v2-Base, and Nemotron-Nano-9B-v2-Base checkpoints along with the majority of our pre- and post-training datasets on Hugging Face. </p>
<blockquote>
<p>我们介绍了Nemotron-Nano-9B-v2，这是一种混合Mamba-Transformer语言模型，旨在提高推理工作负载的吞吐量，同时与类似规模的模型相比实现最先进的准确性。Nemotron-Nano-9B-v2建立在Nemotron-H架构的基础上，将Transformer架构中大部分的自注意力层替换为Mamba-2层，以实现在生成推理所需的长思考轨迹时的更快推理速度。我们通过首先在20万亿个令牌上使用FP8训练配方预训练一个12亿参数模型（Nemotron-Nano-12B-v2-Base）来创建Nemotron-Nano-9B-v2。在对Nemotron-Nano-12B-v2-Base进行对齐后，我们采用Minitron策略来压缩和蒸馏模型，旨在能够在单个NVIDIA A10G GPU（具有22GiB内存，bfloat16精度）上进行最多达128k令牌的推理。与现有的类似规模模型（例如Qwen3-8B）相比，我们在推理基准测试中显示，Nemotron-Nano-9B-v2实现了相当或更好的准确性，同时在如8k输入和16k输出令牌的推理设置中实现了高达6倍的推理吞吐量。我们将Nemotron-Nano-9B-v2、Nemotron-Nano12B-v2-Base以及Nemotron-Nano-9B-v2的检查点连同我们大部分预训练和后续训练数据集一起在Hugging Face上发布。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14444v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文主要介绍了Nemotron-Nano-9B-v2的诞生背景和优势。这是一款基于Mamba-Transformer架构的语言模型，通过用Mamba-2层替代Transformer架构中的大部分自注意力层，旨在提升处理推理任务时的效率与准确度。其训练数据采用大规模数据集训练并使用了FP8训练配方，能够在单个NVIDIA A10G GPU上实现高达128k标记的推理能力。相较于其他类似规模的模型，它在推理任务上表现卓越，提高了推理速度。文章最后将模型和预训练数据集发布在Hugging Face上供大众使用。</p>
<p><strong>Key Takeaways</strong></p>
<p>以下是关于文本的关键见解：</p>
<ol>
<li>Nemotron-Nano-9B-v2是一个基于Mamba-Transformer架构的语言模型，用于提高推理工作负载的吞吐量并保持与类似规模模型相比的最佳准确性。</li>
<li>该模型基于Nemotron-H架构构建，该架构通过替换Transformer架构中的大部分自注意力层为Mamba-2层，以实现生成长思考轨迹所需的推理过程中的改进推断速度。</li>
<li>模型通过大规模数据集进行预训练，并采用FP8训练配方来提升性能。</li>
<li>该模型能够在单个NVIDIA A10G GPU上处理高达128k的标记推理任务，目标是为了实现在内存使用率和处理速度之间的优化平衡。</li>
<li>模型表现出优于同类模型的推理速度性能表现。相对于同样规模的模型（如Qwen3-8B），在如输入为8k标记和输出为16k标记的推理任务上可以达到更高的推理速度（高达6倍）。 </li>
<li>作者已经将该模型和主要的预训练和后续训练数据集在Hugging Face上发布，以供大众访问和使用。这将促进未来模型的研究和发展。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14444">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-55f97bf17c1a24b75a1fbc40b0cbf9ad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7fa504cec80d675359af61cce57535a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa73dae471819e017a5cc665c1f7aee0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="TabulaX-Leveraging-Large-Language-Models-for-Multi-Class-Table-Transformations"><a href="#TabulaX-Leveraging-Large-Language-Models-for-Multi-Class-Table-Transformations" class="headerlink" title="TabulaX: Leveraging Large Language Models for Multi-Class Table   Transformations"></a>TabulaX: Leveraging Large Language Models for Multi-Class Table   Transformations</h2><p><strong>Authors:Arash Dargahi Nobari, Davood Rafiei</strong></p>
<p>The integration of tabular data from diverse sources is often hindered by inconsistencies in formatting and representation, posing significant challenges for data analysts and personal digital assistants. Existing methods for automating tabular data transformations are limited in scope, often focusing on specific types of transformations or lacking interpretability. In this paper, we introduce TabulaX, a novel framework that leverages Large Language Models (LLMs) for multi-class column-level tabular transformations. TabulaX first classifies input columns into four transformation types (string-based, numerical, algorithmic, and general) and then applies tailored methods to generate human-interpretable transformation functions, such as numeric formulas or programming code. This approach enhances transparency and allows users to understand and modify the mappings. Through extensive experiments on real-world datasets from various domains, we demonstrate that TabulaX outperforms existing state-of-the-art approaches in terms of accuracy, supports a broader class of transformations, and generates interpretable transformations that can be efficiently applied. </p>
<blockquote>
<p>从不同来源整合表格数据常常因格式和表示的不一致性而受阻，对数据分析师和个人数字助理构成重大挑战。现有的自动化表格数据转换方法范围有限，通常只专注于特定类型的转换或缺乏可解释性。在本文中，我们介绍了TabulaX，一个利用大型语言模型（LLM）进行多类列级表格转换的新型框架。TabulaX首先将对输入列进行四种转换类型（基于字符串、数值、算法和通用）的分类，然后应用定制的方法生成人类可解释转换函数，如数值公式或程序代码。这种方法提高了透明度，使用户能够理解和修改映射关系。通过在不同领域真实数据集上的大量实验，我们证明了TabulaX在准确性方面优于现有最先进的方法，支持更广泛的转换类型，并生成可解释的转换，可高效应用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.17110v2">PDF</a> </p>
<p><strong>Summary</strong><br>数据从不同来源的表格集成常常受到格式和表示不一致性的阻碍，给数据分析人员和个人数字助理带来重大挑战。现有的自动化表格数据转换方法范围有限，通常只关注特定类型的转换或缺乏可解释性。本文介绍了一个新型框架TabulaX，它利用大型语言模型（LLM）进行多类列级表格转换。TabulaX首先根据输入的列将其分为四类（基于字符串的、数字的、算法的和一般的），然后采用相应的方法生成人类可解释性的转换函数，如数值公式或程序代码。此方法增强了透明度并允许用户理解和修改映射关系。在现实世界的多个领域数据集上进行的大量实验表明，TabulaX在准确性方面优于现有最先进的方案，支持更广泛的转换类型，并能生成可解释的转换，可有效地应用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>表格数据集成面临格式和表示不一致的挑战。</li>
<li>现有自动化表格数据转换方法具有局限性。</li>
<li>TabulaX利用大型语言模型（LLM）进行多类列级表格转换。</li>
<li>TabulaX将输入列分为四类：基于字符串的、数字的、算法的和一般的。</li>
<li>TabulaX生成人类可解释性的转换函数，如数值公式或程序代码。</li>
<li>TabulaX在准确性方面优于现有方案，并支持更广泛的转换类型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.17110">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-6feaac4031eeba7511d50f8106010e04.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-de8288e7a097284361c0a947ec64989b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e602c36a54ab83f83e112f46dca2b95e.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Development-of-Pre-Trained-Transformer-based-Models-for-the-Nepali-Language"><a href="#Development-of-Pre-Trained-Transformer-based-Models-for-the-Nepali-Language" class="headerlink" title="Development of Pre-Trained Transformer-based Models for the Nepali   Language"></a>Development of Pre-Trained Transformer-based Models for the Nepali   Language</h2><p><strong>Authors:Prajwal Thapa, Jinu Nyachhyon, Mridul Sharma, Bal Krishna Bal</strong></p>
<p>Transformer-based pre-trained language models have dominated the field of Natural Language Processing (NLP) for quite some time now. However, the Nepali language, spoken by approximately 32 million people worldwide, remains significantly underrepresented in this domain. This underrepresentation is primarily attributed to the scarcity of monolingual data corpora and limited available resources for the Nepali language. While existing efforts have predominantly concentrated on basic encoder-based models, there is a notable gap in the exploration of decoder-based architectures. To address this gap, we have collected 27.5 GB of Nepali text data, approximately 2.4x larger than any previously available Nepali language corpus. Leveraging this data, we pre-trained three different models i.e., BERT, RoBERTa, and GPT-2, exclusively for the Nepali Language. Furthermore, we performed instruction tuning and explored its potential for monolingual Nepali data, providing a foundation for future research. Our models outperformed the existing best model by 2 points on Nep-gLUE benchmark, scoring 95.60 and also outperformed existing models on text generation tasks, demonstrating improvements in both understanding and generating Nepali text. </p>
<blockquote>
<p>基于Transformer的预训练语言模型在自然语言处理（NLP）领域已经占据主导地位很长时间了。然而，世界上约有3200万人使用的尼泊尔语在这个领域仍然显著缺乏代表性。这种代表性不足主要归因于单语数据语料库的稀缺以及尼泊尔语可用资源的有限。虽然现有的努力主要集中在基本的编码器模型上，但在解码器架构的探索方面仍存在明显的差距。为了弥补这一差距，我们收集了27.5GB的尼泊尔文本数据，大约是之前任何可用的尼泊尔语语料库的2.4倍。利用这些数据，我们针对尼泊尔语预训练了三种不同的模型，即BERT、RoBERTa和GPT-2。此外，我们对指令进行了调整，并探索了其在单语尼泊尔语数据中的潜力，为未来研究奠定了基础。我们的模型在Nep-gLUE基准测试上的得分比现有最佳模型高出2分，得分为95.60，并且在文本生成任务上也超越了现有模型，这表明我们在理解和生成尼泊尔语文本方面都有所改进。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.15734v2">PDF</a> </p>
<p><strong>Summary</strong>：基于Transformer的预训练语言模型在自然语言处理领域占据主导地位已久，但尼泊尔语在该领域受到严重忽视。这主要归因于缺乏单语数据语料库和可用的尼泊尔语资源有限。为了解决这个问题，我们收集了迄今为止最大的尼泊尔语文本数据集，并利用该数据集对BERT、RoBERTa和GPT-2等三种模型进行了训练。在指导微调下，该模型在尼泊尔语言理解和文本生成任务上都超越了现有的模型表现。在Nep-gLUE基准测试中，得分高出目前最佳模型两分，达到了95.6的高分。该模型的训练和研发为后续相关研究奠定了坚实基础。 </p>
<p><strong>Key Takeaways</strong>： </p>
<ul>
<li>Transformer-based预训练语言模型在自然语言处理领域占主导地位，但尼泊尔语的相关研究十分匮乏。 </li>
<li>缺乏单语数据语料库和可用的尼泊尔语资源是尼泊尔语在自然语言处理领域受忽视的主要原因。 </li>
<li>研究人员收集了迄今为止最大的尼泊尔语文本数据集，用于训练模型。 </li>
<li>利用该数据集训练的模型包括BERT、RoBERTa和GPT-2等三种模型。 </li>
<li>通过指导微调，这些模型在理解和生成尼泊尔语文本方面表现优异。 </li>
<li>在Nep-gLUE基准测试中，这些模型的得分高于现有最佳模型两分，达到95.6的高分。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.15734">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2a8fdbb595be4fd20cf24dc8d7ecac98.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9b7ee24e95a60115998df2fc64e9712b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ba18a1a83e652844362962f84bc9cd49.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fe905b8e9fcc6422e5d7b7e4678565a4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6abdfe1b6fffe9c4274f8521b933d283.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Knowledge-Guided-Prompt-Learning-for-Request-Quality-Assurance-in-Public-Code-Review"><a href="#Knowledge-Guided-Prompt-Learning-for-Request-Quality-Assurance-in-Public-Code-Review" class="headerlink" title="Knowledge-Guided Prompt Learning for Request Quality Assurance in Public   Code Review"></a>Knowledge-Guided Prompt Learning for Request Quality Assurance in Public   Code Review</h2><p><strong>Authors:Lin Li, Xinchun Yu, Xinyu Chen, Peng Liang</strong></p>
<p>Public Code Review (PCR) is developed in the Software Question Answering (SQA) community, assisting developers in exploring high-quality and efficient review services. Current methods on PCR mainly focus on the reviewer’s perspective, including finding a capable reviewer, predicting comment quality, and recommending&#x2F;generating review comments. However, it is not well studied that how to satisfy the review necessity requests posted by developers which can increase their visibility, which in turn acts as a prerequisite for better review responses. To this end, we propose K nowledge-guided P rompt learning for P ublic Code Review (KP-PCR) to achieve developer-based code review request quality assurance (i.e., predicting request necessity and recommending tags subtask). Specifically, we reformulate the two subtasks via 1) text prompt tuning which converts both of them into a Masked Language Model (MLM) by constructing prompt templates using hard prompt; and 2) knowledge and code prefix tuning which introduces knowledge guidance from fine-tuned large language models by soft prompt, and uses program dependence graph to characterize code snippets. Finally, both of the request necessity prediction and tag recommendation subtasks output predicted results through an answer engineering module. In addition, we further analysis the time complexity of our KP-PCR that has lightweight prefix based the operation of introducing knowledge guidance. Experimental results on the PCR dataset for the period 2011-2023 demonstrate that our KP-PCR outperforms baselines by 2.3%-8.4% in the request necessity prediction and by 1.4%-6.9% in the tag recommendation. The code implementation is released at <a target="_blank" rel="noopener" href="https://github.com/WUT-IDEA/KP-PCR">https://github.com/WUT-IDEA/KP-PCR</a>. </p>
<blockquote>
<p>公共代码审查（PCR）是在软件问答（SQA）社区中开发的，旨在帮助开发者探索高质量和高效的审查服务。当前PCR的主要方法主要集中在审查者的角度，包括寻找能力强的审查者、预测评论质量和推荐&#x2F;生成审查评论。然而，如何满足开发者提出的审查需求请求并未得到很好的研究，而这些请求的增加可以提高其可见度，从而更好地获得审查回应。为此，我们提出知识引导提示学习公共代码审查（KP-PCR），以实现基于开发者的代码审查请求质量保证（即预测请求必要性和推荐标签子任务）。具体来说，我们通过1）文本提示调整，将这两个子任务转换为掩码语言模型（MLM），通过构建硬提示的提示模板来实现；2）知识和代码前缀调整，引入经过微调的大型语言模型的知识指导，使用软提示和程序依赖图来表征代码片段。最后，请求必要性预测和标签推荐子任务都通过答案工程模块输出预测结果。此外，我们进一步分析了我们的KP-PCR的时间复杂度，它具有基于引入知识指导的前缀的轻量级操作。在2011-2023年的PCR数据集上的实验结果表明，我们的KP-PCR在请求必要性预测方面优于基线2.3%-8.4%，在标签推荐方面优于基线1.4%-6.9%。代码实现已发布在<a target="_blank" rel="noopener" href="https://github.com/WUT-IDEA/KP-PCR%E3%80%82">https://github.com/WUT-IDEA/KP-PCR。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.21673v3">PDF</a> 27 pages, 5 images, 12 tables, Manuscript revision submitted to a   journal (2025)</p>
<p><strong>Summary</strong></p>
<p>本文介绍了公共代码审查（PCR）在软件问答（SQA）社区的发展情况，并指出当前PCR方法主要关注评审者的角度。为满足开发者提出的审查需求请求，提高可见性并获取更好的审查回应，提出了知识引导提示学习公共代码审查（KP-PCR）方法。KP-PCR通过文本提示调整和知识与代码前缀调整两个步骤，将请求必要性预测和标签推荐子任务转化为掩码语言模型（MLM）的任务。此外，文章还分析了KP-PCR的时间复杂度，并通过实验验证了其在请求必要性预测和标签推荐方面的优越性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>公共代码审查（PCR）在软件问答（SQA）社区中得到发展，主要关注评审者的角度，包括寻找能力强的评审者、预测评论质量和生成推荐评论。</li>
<li>KP-PCR方法旨在满足开发者的审查需求请求，提高其在PCR中的可见性，从而获取更好的审查回应。</li>
<li>KP-PCR通过文本提示调整和知识与代码前缀调整两个步骤，将请求必要性预测和标签推荐子任务转化为掩码语言模型（MLM）的任务。</li>
<li>KP-PCR引入知识指导，使用程序依赖图来表征代码片段，并通过轻量级的前缀操作实现。</li>
<li>实验结果表明，KP-PCR在请求必要性预测和标签推荐方面优于基线方法。</li>
<li>KP-PCR的代码实现已发布在<a target="_blank" rel="noopener" href="https://github.com/WUT-IDEA/KP-PCR%E3%80%82">https://github.com/WUT-IDEA/KP-PCR。</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.21673">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f1ff94757caca643ac10254ed3e86f94.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8b80222e7d56efa066e25f14385eba5a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ad1bb7da15b3f7c0ba3b28b76a5096b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bfacefe38eabe8d6de06a17f44ef5ba4.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Optimizing-Cross-Client-Domain-Coverage-for-Federated-Instruction-Tuning-of-Large-Language-Models"><a href="#Optimizing-Cross-Client-Domain-Coverage-for-Federated-Instruction-Tuning-of-Large-Language-Models" class="headerlink" title="Optimizing Cross-Client Domain Coverage for Federated Instruction Tuning   of Large Language Models"></a>Optimizing Cross-Client Domain Coverage for Federated Instruction Tuning   of Large Language Models</h2><p><strong>Authors:Zezhou Wang, Yaxin Du, Xingjun Ma, Yugang Jiang, Zhuzhong Qian, Siheng Chen</strong></p>
<p>Federated domain-specific instruction tuning (FedDIT) for large language models (LLMs) aims to enhance performance in specialized domains using distributed private and limited data, yet identifying key performance drivers and optimal augmentation strategies remains challenging. We empirically establish that cross-client domain coverage, rather than data heterogeneity, is the pivotal factor. We then introduce FedDCA, an algorithm that explicitly maximizes this coverage through diversity-oriented client center selection and retrieval-based augmentation, constructing diverse, non-redundant cross-client instruction sets. Extensive experiments across multiple domains demonstrate FedDCA’s superiority over eleven baselines, achieving performance gains of up to 29.19% and domain coverage improvements of 4.82%-21.36%. FedDCA maintains its effectiveness in diverse and challenging scenarios, including data selection, held-out settings where task-specific public data is scarce and various data heterogeneity, with manageable privacy risks. This work clarifies critical FedDIT dynamics and presents FedDCA as an effective, privacy-preserving, and scalable solution for advancing domain-specific LLM tuning. </p>
<blockquote>
<p>针对大型语言模型（LLM）的联邦特定领域指令调整（FedDIT）旨在利用分布式私有和有限数据提高在特定领域的性能，然而，确定关键性能驱动因素和最佳增强策略仍然具有挑战性。我们通过实证研究证实，跨客户端领域覆盖（而非数据异质性）是关键要素。随后，我们引入了FedDCA算法，该算法通过面向多样性的客户端中心选择和检索增强法，显式地最大化这种覆盖，构建多样化、非冗余的跨客户端指令集。跨多个领域的广泛实验表明，FedDCA在超过11个基准测试中具有优势，性能提升幅度高达29.19%，领域覆盖改善幅度在4.82%~21.36%之间。FedDCA在多样化的挑战场景中保持了其有效性，包括数据选择、任务特定公开数据稀缺的保留设置以及各种数据异质性，同时可控隐私风险。这项工作明确了关键的FedDIT动态，并展示了FedDCA作为一种有效、保护隐私和可扩展的解决方案，可推动特定领域的LLM调优。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.20135v6">PDF</a> EMNLP 2025</p>
<p><strong>摘要</strong></p>
<p>针对大型语言模型（LLM）的联邦特定域指令调整（FedDIT）旨在利用分布式私有和有限数据提高在特定领域的性能，但确定关键性能驱动因素和最佳增强策略仍具有挑战性。我们实证地证明了跨客户端的域覆盖是关键因素，而不是数据异质性。接着引入了FedDCA算法，它通过面向多样性的客户端中心选择和检索增强方法，显式地最大化这种覆盖，构建多样且非冗余的跨客户端指令集。在多领域的广泛实验证明，FedDCA优于十一种基线方法，实现了高达29.19%的性能提升和4.82%-21.36%的域覆盖改善。FedDCA在数据选择、任务特定公开数据稀缺的保留设置以及多种数据异质性等多样和复杂场景中保持其有效性，同时管理隐私风险。本研究明确了关键的FedDIT动态，并提出FedDCA是一种有效、隐私保护、可扩展的解决方案，可推动特定领域的LLM调优。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>联邦特定域指令调整（FedDIT）旨在提高大型语言模型（LLM）在特定领域的性能。</li>
<li>跨客户端的域覆盖是提升LLM性能的关键因素。</li>
<li>FedDCA算法通过多样性导向的客户端中心选择和检索增强方法，最大化跨客户端的域覆盖。</li>
<li>FedDCA在多个领域实验中表现出优越性能，相较于基线方法有高达29.19%的性能提升。</li>
<li>FedDCA在数据选择、任务特定公开数据稀缺以及数据异质性等复杂场景中表现稳健。</li>
<li>FedDCA能有效管理隐私风险。</li>
<li>此研究明确了关键的FedDIT动态，并提出FedDCA作为一种有效、可扩展到特定领域的LLM调优解决方案。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.20135">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-1613f6279a50213b24c2a7fd7d84d250.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-819299dca0151a8448ab2d8094fc50e8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-15d61d0606b635ddd0aace74426c72c5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-300873ceaaa6af410fadabf405f4affe.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Judging-the-Judges-Evaluating-Alignment-and-Vulnerabilities-in-LLMs-as-Judges"><a href="#Judging-the-Judges-Evaluating-Alignment-and-Vulnerabilities-in-LLMs-as-Judges" class="headerlink" title="Judging the Judges: Evaluating Alignment and Vulnerabilities in   LLMs-as-Judges"></a>Judging the Judges: Evaluating Alignment and Vulnerabilities in   LLMs-as-Judges</h2><p><strong>Authors:Aman Singh Thakur, Kartik Choudhary, Venkat Srinik Ramayapally, Sankaran Vaidyanathan, Dieuwke Hupkes</strong></p>
<p>Offering a promising solution to the scalability challenges associated with human evaluation, the LLM-as-a-judge paradigm is rapidly gaining traction as an approach to evaluating large language models (LLMs). However, there are still many open questions about the strengths and weaknesses of this paradigm, and what potential biases it may hold. In this paper, we present a comprehensive study of the performance of various LLMs acting as judges, focusing on a clean scenario in which inter-human agreement is high. Investigating thirteen judge models of different model sizes and families, judging answers of nine different ‘examtaker models’ - both base and instruction-tuned - we find that only the best (and largest) models achieve reasonable alignment with humans. However, they are still quite far behind inter-human agreement and their assigned scores may still differ with up to 5 points from human-assigned scores. In terms of their ranking of the nine exam-taker models, instead, also smaller models and even the lexical metric contains may provide a reasonable signal. Through error analysis and other studies, we identify vulnerabilities in judge models, such as their sensitivity to prompt complexity and length, and a tendency toward leniency. The fact that even the best judges differ from humans in this comparatively simple setup suggest that caution may be wise when using judges in more complex setups. Lastly, our research rediscovers the importance of using alignment metrics beyond simple percent alignment, showing that judges with high percent agreement can still assign vastly different scores. </p>
<blockquote>
<p>针对与人类评估相关的可扩展性挑战，LLM作为法官的范式为解决该问题提供了前景广阔的解决方案，并作为评估大型语言模型（LLM）的一种方法迅速获得了支持。然而，关于该范式的优缺点及其可能存在的潜在偏见，仍有许多悬而未决的问题。在本文中，我们对作为法官的各种LLM的性能进行了全面的研究，重点关注人类之间共识度较高的清洁场景。我们调查了13个不同规模和家族的法官模型，评估了9个不同参加考试模型的答案（包括基础模型和指令调优模型），我们发现只有最好的（也是最大的）模型才能实现与人类的合理对齐。然而，它们仍然远远落后于人类之间的共识，其分配的分数可能与人类分配的分数相差高达5分。而在对九个考试模型的排名方面，较小的模型甚至词汇度量也可能提供合理的信号。通过错误分析和其他研究，我们发现了法官模型的漏洞，如它们对提示的复杂性和长度的敏感性，以及倾向于宽容的趋势。即使在相对简单的设置中，即使最好的法官也与人类存在差异，因此在更复杂的设置中使用法官时可能需要谨慎。最后，我们的研究重新发现了使用超越简单百分比对齐的对齐指标的重要性，表明即使百分比协议很高的法官仍然可以分配截然不同的分数。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.12624v6">PDF</a> <a target="_blank" rel="noopener" href="https://aclanthology.org/2025.gem-1.33/">https://aclanthology.org/2025.gem-1.33/</a></p>
<p><strong>Summary</strong></p>
<p>LLM作为评估者的模式在解决语言模型评估的可扩展性挑战方面展现出巨大潜力。本文全面研究了不同大小的LLM模型作为评委的表现，发现只有最优秀的模型才能达到与人类之间的合理对齐。然而，这些模型仍然与人类评估存在差距，且存在对提示复杂性和长度的敏感性和宽松倾向等漏洞。因此，在更复杂的设置中使用这些模型时需要谨慎。此外，研究还发现简单的对齐百分比不能完全反映模型的真实性能。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLM-as-a-judge模式解决了人类评估可扩展性的挑战。</li>
<li>只有最优秀的LLM模型才能实现与人类的合理对齐。</li>
<li>优秀的LLM模型仍与人类评估存在差距，需谨慎使用于复杂环境。</li>
<li>LLM作为评委存在对提示复杂性和长度的敏感性和宽松倾向等漏洞。</li>
<li>简单的对齐百分比不能完全反映模型的真实性能。</li>
<li>研究重新发现了使用超越简单百分比对齐的对齐指标的重要性。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.12624">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5f4181eacf3b0e4b24712602e2a48695.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e63ac5b0c7c98e34730daa017ce5573.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-804295d52fd69db447d6025bb8ead2fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-815a69f56e7c1a65e2ac378f4d4d23c2.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="CRISPR-GPT-for-Agentic-Automation-of-Gene-editing-Experiments"><a href="#CRISPR-GPT-for-Agentic-Automation-of-Gene-editing-Experiments" class="headerlink" title="CRISPR-GPT for Agentic Automation of Gene-editing Experiments"></a>CRISPR-GPT for Agentic Automation of Gene-editing Experiments</h2><p><strong>Authors:Yuanhao Qu, Kaixuan Huang, Ming Yin, Kanghong Zhan, Dyllan Liu, Di Yin, Henry C. Cousins, William A. Johnson, Xiaotong Wang, Mihir Shah, Russ B. Altman, Denny Zhou, Mengdi Wang, Le Cong</strong></p>
<p>The introduction of genome engineering technology has transformed biomedical research, making it possible to make precise changes to genetic information. However, creating an efficient gene-editing system requires a deep understanding of CRISPR technology, and the complex experimental systems under investigation. While Large Language Models (LLMs) have shown promise in various tasks, they often lack specific knowledge and struggle to accurately solve biological design problems. In this work, we introduce CRISPR-GPT, an LLM agent augmented with domain knowledge and external tools to automate and enhance the design process of CRISPR-based gene-editing experiments. CRISPR-GPT leverages the reasoning ability of LLMs to facilitate the process of selecting CRISPR systems, designing guide RNAs, recommending cellular delivery methods, drafting protocols, and designing validation experiments to confirm editing outcomes. We showcase the potential of CRISPR-GPT for assisting non-expert researchers with gene-editing experiments from scratch and validate the agent’s effectiveness in a real-world use case. Furthermore, we explore the ethical and regulatory considerations associated with automated gene-editing design, highlighting the need for responsible and transparent use of these tools. Our work aims to bridge the gap between beginner biological researchers and CRISPR genome engineering techniques, and demonstrate the potential of LLM agents in facilitating complex biological discovery tasks. The published version of this draft is available at <a target="_blank" rel="noopener" href="https://www.nature.com/articles/s41551-025-01463-z">https://www.nature.com/articles/s41551-025-01463-z</a>. </p>
<blockquote>
<p>基因组工程技术的引入已经彻底改变了生物医学研究，使得对遗传信息进行精确修改成为可能。然而，要创建一个高效的基因编辑系统，需要深入了解CRISPR技术以及复杂的实验系统。虽然大型语言模型（LLM）在各种任务中显示出潜力，但它们通常缺乏特定知识，难以准确解决生物设计问题。在我们的工作中，我们介绍了CRISPR-GPT，这是一个通过领域知识和外部工具增强功能的大型语言模型代理，可以自动化和增强CRISPR基因编辑实验的设计过程。CRISPR-GPT利用LLM的推理能力来促进选择CRISPR系统、设计引导RNA、推荐细胞传递方法、起草协议以及设计验证实验以确认编辑结果的过程。我们展示了CRISPR-GPT在协助非专业研究人员进行基因编辑实验方面的潜力，并通过实际使用案例验证了该代理的有效性。此外，我们还探讨了与自动化基因编辑设计相关的伦理和监管问题，强调这些工具需要负责任和透明地使用。我们的工作旨在缩小初级生物研究人员与CRISPR基因组工程技术之间的差距，并展示大型语言模型代理在促进复杂生物学发现任务中的潜力。该论文的发布版本可通过<a target="_blank" rel="noopener" href="https://www.nature.com/articles/s41551-025-01463-z%E8%AE%BF%E9%97%AE%E3%80%82">https://www.nature.com/articles/s41551-025-01463-z访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.18021v2">PDF</a> Accepted to Nature Biomedical Engineering</p>
<p><strong>Summary</strong>：CRISPR基因编辑技术革命性地改变了生物医学研究。为了提高基因编辑的效率，本文引入CRISPR-GPT，这是一种利用大型语言模型（LLM）技术和外部工具辅助CRISPR基因编辑实验设计的系统。CRISPR-GPT可帮助非专业研究人员完成从实验设计到验证的全过程，并探讨了自动化基因编辑设计的伦理和监管问题。本文旨在缩小初学者与CRISPR基因工程技术的差距，展示LLM在促进复杂生物学发现任务中的潜力。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>CRISPR基因编辑技术的引入为生物医学研究带来了巨大的变革。</li>
<li>大型语言模型（LLM）技术可提高基因编辑效率，但仍缺乏特定领域的解决方案。</li>
<li>CRISPR-GPT结合了CRISPR技术和LLM的优势，辅助完成基因编辑实验的全过程。</li>
<li>CRISPR-GPT具备选择CRISPR系统、设计引导RNA、推荐细胞传递方法、起草协议和设计验证实验的能力。</li>
<li>CRISPR-GPT有助于非专业研究人员进行基因编辑实验，验证了其在真实世界中的有效性。</li>
<li>自动化基因编辑设计需要考虑伦理和监管问题，需要负责任和透明地使用这些工具。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.18021">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-805fa45ff43a8920f0d29e82a9d8225b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4168d85011ac37c931c3111397507eac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a25e251e7d93830c61bedad54ab676cb.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-23/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-23/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-23/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-20b6292f2fd919823d826d9270025daf.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-08-23  Distributed Detection of Adversarial Attacks in Multi-Agent   Reinforcement Learning with Continuous Action Space
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-23
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-23/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-149c94281c63fab0be861c1249be1603.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-08-23  Intern-S1 A Scientific Multimodal Foundation Model
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-23
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">28051.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
