<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Agent">
    <meta name="description" content="Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-23  Distributed Detection of Adversarial Attacks in Multi-Agent   Reinforcement Learning with Continuous Action Space">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Agent | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-20b6292f2fd919823d826d9270025daf.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Agent</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Agent/">
                                <span class="chip bg-color">Agent</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                Agent
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-23
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-08
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    16.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    65 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-23-æ›´æ–°"><a href="#2025-08-23-æ›´æ–°" class="headerlink" title="2025-08-23 æ›´æ–°"></a>2025-08-23 æ›´æ–°</h1><h2 id="Distributed-Detection-of-Adversarial-Attacks-in-Multi-Agent-Reinforcement-Learning-with-Continuous-Action-Space"><a href="#Distributed-Detection-of-Adversarial-Attacks-in-Multi-Agent-Reinforcement-Learning-with-Continuous-Action-Space" class="headerlink" title="Distributed Detection of Adversarial Attacks in Multi-Agent   Reinforcement Learning with Continuous Action Space"></a>Distributed Detection of Adversarial Attacks in Multi-Agent   Reinforcement Learning with Continuous Action Space</h2><p><strong>Authors:Kiarash Kazari, Ezzeldin Shereen, GyÃ¶rgy DÃ¡n</strong></p>
<p>We address the problem of detecting adversarial attacks against cooperative multi-agent reinforcement learning with continuous action space. We propose a decentralized detector that relies solely on the local observations of the agents and makes use of a statistical characterization of the normal behavior of observable agents. The proposed detector utilizes deep neural networks to approximate the normal behavior of agents as parametric multivariate Gaussian distributions. Based on the predicted density functions, we define a normality score and provide a characterization of its mean and variance. This characterization allows us to employ a two-sided CUSUM procedure for detecting deviations of the normality score from its mean, serving as a detector of anomalous behavior in real-time. We evaluate our scheme on various multi-agent PettingZoo benchmarks against different state-of-the-art attack methods, and our results demonstrate the effectiveness of our method in detecting impactful adversarial attacks. Particularly, it outperforms the discrete counterpart by achieving AUC-ROC scores of over 0.95 against the most impactful attacks in all evaluated environments. </p>
<blockquote>
<p>æˆ‘ä»¬è§£å†³é’ˆå¯¹å…·æœ‰è¿ç»­åŠ¨ä½œç©ºé—´çš„åˆä½œå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ çš„å¯¹æŠ—æ”»å‡»æ£€æµ‹é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ†æ•£å¼æ£€æµ‹å™¨ï¼Œå®ƒä»…ä¾èµ–äºæ™ºèƒ½ä½“çš„å±€éƒ¨è§‚å¯Ÿï¼Œå¹¶åˆ©ç”¨å¯è§‚å¯Ÿæ™ºèƒ½ä½“çš„æ­£å¸¸è¡Œä¸ºçš„ç»Ÿè®¡ç‰¹å¾ã€‚æ‰€æå‡ºçš„æ£€æµ‹å™¨åˆ©ç”¨æ·±åº¦ç¥ç»ç½‘ç»œæ¥é€¼è¿‘æ™ºèƒ½ä½“çš„æ­£å¸¸è¡Œä¸ºï¼Œå°†å…¶è¡¨ç¤ºä¸ºå‚æ•°åŒ–å¤šå…ƒé«˜æ–¯åˆ†å¸ƒã€‚åŸºäºé¢„æµ‹çš„å¯†åº¦å‡½æ•°ï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªæ­£å¸¸æ€§å¾—åˆ†ï¼Œå¹¶ç»™å‡ºäº†å…¶å‡å€¼å’Œæ–¹å·®çš„ç‰¹å¾æè¿°ã€‚è¿™ç§ç‰¹å¾æè¿°ä½¿æˆ‘ä»¬èƒ½å¤Ÿé‡‡ç”¨åŒä¾§CUSUMç¨‹åºæ¥æ£€æµ‹æ­£å¸¸æ€§å¾—åˆ†ä¸å…¶å‡å€¼ä¹‹é—´çš„åå·®ï¼Œä»è€Œå®æ—¶æ£€æµ‹å¼‚å¸¸è¡Œä¸ºã€‚æˆ‘ä»¬åœ¨å„ç§å¤šæ™ºèƒ½ä½“PettingZooåŸºå‡†æµ‹è¯•ä¸Šå¯¹å„ç§å…ˆè¿›çš„æ”»å‡»æ–¹æ³•è¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœè¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ£€æµ‹æœ‰å½±å“åŠ›çš„å¯¹æŠ—æ”»å‡»æ–¹é¢éå¸¸æœ‰æ•ˆã€‚å°¤å…¶å€¼å¾—ä¸€æçš„æ˜¯ï¼Œä¸ä¼ ç»Ÿçš„ç¦»æ•£æ£€æµ‹æ–¹æ³•ç›¸æ¯”ï¼Œå®ƒåœ¨æ‰€æœ‰è¯„ä¼°ç¯å¢ƒä¸­å¯¹æœ€å…·å½±å“åŠ›çš„æ”»å‡»å–å¾—äº†è¶…è¿‡0..çš„AUC-ROCå¾—åˆ†ï¼ˆ95åˆ†ä»¥ä¸Šï¼‰ã€‚ç‰¹åˆ«æ˜¯åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ç¯å¢ƒä¸­ï¼Œå®ƒçš„æ€§èƒ½ä¼˜åŠ¿æ›´ä¸ºæ˜æ˜¾ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15764v1">PDF</a> Accepted for publication at ECAI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡è§£å†³é’ˆå¯¹åˆä½œå‹å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ çš„è¿ç»­åŠ¨ä½œç©ºé—´ä¸­çš„å¯¹æŠ—æ”»å‡»æ£€æµ‹é—®é¢˜ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§åŸºäºå±€éƒ¨è§‚æµ‹çš„åˆ†æ•£å¼æ£€æµ‹å™¨ï¼Œåˆ©ç”¨å¯è§‚æµ‹æ™ºèƒ½ä½“çš„æ­£å¸¸è¡Œä¸ºçš„ç»Ÿè®¡ç‰¹å¾è¿›è¡Œæ£€æµ‹ã€‚æ£€æµ‹å™¨åˆ©ç”¨æ·±åº¦ç¥ç»ç½‘ç»œè¿‘ä¼¼æ™ºèƒ½ä½“çš„æ­£å¸¸è¡Œä¸ºä½œä¸ºå‚æ•°åŒ–çš„å¤šå…ƒé«˜æ–¯åˆ†å¸ƒã€‚åŸºäºé¢„æµ‹çš„å¯†åº¦å‡½æ•°ï¼Œå®šä¹‰äº†æ­£å¸¸è¡Œä¸ºè¯„åˆ†å¹¶ç»™å‡ºäº†å…¶å‡å€¼å’Œæ–¹å·®çš„ç‰¹å¾æè¿°ã€‚è¿™ç§ç‰¹å¾æè¿°å…è®¸é‡‡ç”¨åŒä¾§CUSUMç¨‹åºæ£€æµ‹æ­£å¸¸è¯„åˆ†å‡å€¼ä¸Šçš„åå·®ï¼Œå®æ—¶æ£€æµ‹å¼‚å¸¸è¡Œä¸ºã€‚è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ¡ˆåœ¨å¤šæ™ºèƒ½ä½“PettingZooåŸºå‡†æµ‹è¯•ä¸­èƒ½æœ‰æ•ˆæ£€æµ‹å„ç§æœ€å…ˆè¿›çš„æ”»å‡»æ–¹æ³•ä¸­çš„å½±å“æ€§å¯¹æŠ—æ”»å‡»ï¼Œç‰¹åˆ«æ˜¯åœ¨è¿ç»­åŠ¨ä½œç©ºé—´ä¸­ï¼Œå¯¹æœ€å…·å½±å“åŠ›çš„æ”»å‡»çš„AUC-ROCå¾—åˆ†è¶…è¿‡0.95ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥æ–‡æœ¬è§£å†³äº†æ£€æµ‹å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ è¿ç»­åŠ¨ä½œç©ºé—´ä¸­çš„å¯¹æŠ—æ”»å‡»çš„é—®é¢˜ã€‚</li>
<li>æå‡ºä¸€ç§åŸºäºå±€éƒ¨è§‚æµ‹çš„åˆ†æ•£å¼æ£€æµ‹å™¨æ¥è¯†åˆ«æ”»å‡»ã€‚</li>
<li>æ£€æµ‹å™¨é€šè¿‡æ·±åº¦ç¥ç»ç½‘ç»œå»ºæ¨¡æ™ºèƒ½ä½“çš„æ­£å¸¸è¡Œä¸ºä¸ºå‚æ•°åŒ–çš„å¤šå…ƒé«˜æ–¯åˆ†å¸ƒã€‚</li>
<li>å®šä¹‰äº†æ­£å¸¸è¡Œä¸ºè¯„åˆ†å¹¶æè¿°äº†å…¶å‡å€¼å’Œæ–¹å·®çš„ç‰¹å¾ã€‚</li>
<li>é‡‡ç”¨åŒä¾§CUSUMç¨‹åºå®æ—¶æ£€æµ‹æ­£å¸¸è¯„åˆ†ä¸­çš„åå·®ä»¥è¯†åˆ«å¼‚å¸¸è¡Œä¸ºã€‚</li>
<li>åœ¨å¤šæ™ºèƒ½ä½“PettingZooåŸºå‡†æµ‹è¯•ä¸­è¿›è¡Œäº†è¯„ä¼°ï¼Œè¯æ˜è¯¥æ–¹æ³•å¯¹å½±å“æ€§å¯¹æŠ—æ”»å‡»æœ‰æ•ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15764">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-70b31fdc49f69ad6b2399ff86318a1b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-524c5dab2df10d672d0533ff96a6d513.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55a4a9e73a9e229c9fd01502e5d87daa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a9db5c9bb6b38d1033baee5424cda52a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ee0510aaaebfce93f58fa963e78cce1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f571d436d8bde4a9c30860da30c25cc9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-752bc4765eff60dba6f3c603ba082326.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="â€œDoes-the-cafe-entrance-look-accessible-Where-is-the-door-â€-Towards-Geospatial-AI-Agents-for-Visual-Inquiries"><a href="#â€œDoes-the-cafe-entrance-look-accessible-Where-is-the-door-â€-Towards-Geospatial-AI-Agents-for-Visual-Inquiries" class="headerlink" title="â€œDoes the cafe entrance look accessible? Where is the door?â€ Towards   Geospatial AI Agents for Visual Inquiries"></a>â€œDoes the cafe entrance look accessible? Where is the door?â€ Towards   Geospatial AI Agents for Visual Inquiries</h2><p><strong>Authors:Jon E. Froehlich, Jared Hwang, Zeyu Wang, John S. Oâ€™Meara, Xia Su, William Huang, Yang Zhang, Alex Fiannaca, Philip Nelson, Shaun Kane</strong></p>
<p>Interactive digital maps have revolutionized how people travel and learn about the world; however, they rely on pre-existing structured data in GIS databases (e.g., road networks, POI indices), limiting their ability to address geo-visual questions related to what the world looks like. We introduce our vision for Geo-Visual Agentsâ€“multimodal AI agents capable of understanding and responding to nuanced visual-spatial inquiries about the world by analyzing large-scale repositories of geospatial images, including streetscapes (e.g., Google Street View), place-based photos (e.g., TripAdvisor, Yelp), and aerial imagery (e.g., satellite photos) combined with traditional GIS data sources. We define our vision, describe sensing and interaction approaches, provide three exemplars, and enumerate key challenges and opportunities for future work. </p>
<blockquote>
<p>äº¤äº’å¼æ•°å­—åœ°å›¾å·²ç»å½»åº•æ”¹å˜äº†äººä»¬æ—…è¡Œå’Œäº†è§£ä¸–ç•Œçš„æ–¹å¼ï¼›ç„¶è€Œï¼Œå®ƒä»¬ä¾èµ–äºGISæ•°æ®åº“ä¸­çš„é¢„å…ˆå­˜åœ¨çš„ç»“æ„åŒ–æ•°æ®ï¼ˆä¾‹å¦‚ï¼Œé“è·¯ç½‘ç»œã€POIç´¢å¼•ï¼‰ï¼Œé™åˆ¶äº†å®ƒä»¬è§£å†³ä¸åœ°ç†è§†è§‰ç›¸å…³çš„ä¸–ç•Œå¤–è§‚é—®é¢˜ã€‚æˆ‘ä»¬ä»‹ç»äº†å¯¹åœ°ç†è§†è§‰ä»£ç†çš„æ„¿æ™¯â€”â€”å¤šæ¨¡æ€äººå·¥æ™ºèƒ½ä»£ç†ï¼Œèƒ½å¤Ÿåˆ†æå¤§è§„æ¨¡åœ°ç†ç©ºé—´å›¾åƒå­˜å‚¨åº“ï¼ŒåŒ…æ‹¬è¡—é“æ™¯è§‚ï¼ˆä¾‹å¦‚è°·æ­Œè¡—æ™¯ï¼‰ã€åŸºäºåœ°ç‚¹çš„ç…§ç‰‡ï¼ˆä¾‹å¦‚TripAdvisorã€Yelpï¼‰å’Œèˆªç©ºå›¾åƒï¼ˆä¾‹å¦‚å«æ˜Ÿç…§ç‰‡ï¼‰ï¼Œå¹¶ç»“åˆä¼ ç»ŸGISæ•°æ®æºï¼Œç†è§£å’Œå›ç­”å…³äºä¸–ç•Œçš„å¾®å¦™è§†è§‰ç©ºé—´æŸ¥è¯¢ã€‚æˆ‘ä»¬å®šä¹‰äº†æˆ‘ä»¬çš„æ„¿æ™¯ï¼Œæè¿°äº†æ„ŸçŸ¥å’Œäº¤äº’æ–¹æ³•ï¼Œæä¾›äº†ä¸‰ä¸ªç¤ºä¾‹ï¼Œå¹¶åˆ—ä¸¾äº†æœªæ¥å·¥ä½œçš„å…³é”®æŒ‘æˆ˜å’Œæœºé‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15752v1">PDF</a> Accepted to the ICCVâ€™25 Workshop â€œVision Foundation Models and   Generative AI for Accessibility: Challenges and Opportunitiesâ€</p>
<p><strong>Summary</strong></p>
<p>äº¤äº’å¼æ•°å­—åœ°å›¾å·²ç»æ”¹å˜äº†äººä»¬æ—…è¡Œå’Œäº†è§£ä¸–ç•Œçš„æ–¹å¼ï¼Œä½†å®ƒä»¬ä¾èµ–äºGISæ•°æ®åº“ä¸­çš„é¢„å…ˆç»“æ„åŒ–æ•°æ®ï¼Œé™åˆ¶äº†è§£å†³ä¸åœ°ç†è§†è§‰ç›¸å…³çš„ä¸–ç•Œå¤–è§‚é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºåœ°ç†è§†è§‰ä»£ç†çš„æ„¿æ™¯â€”â€”å¤šæ¨¡æ€AIä»£ç†èƒ½å¤Ÿåˆ†æå¤§è§„æ¨¡åœ°ç†ç©ºé—´å›¾åƒå­˜å‚¨åº“ï¼Œç†è§£å¹¶å›åº”å…³äºä¸–ç•Œçš„å¾®å¦™è§†è§‰ç©ºé—´æŸ¥è¯¢ï¼ŒåŒ…æ‹¬è¡—æ™¯ï¼ˆå¦‚è°·æ­Œè¡—æ™¯ï¼‰ã€åŸºäºåœ°ç‚¹çš„ç…§ç‰‡ï¼ˆå¦‚çŒ«é€”é¹°ã€é›…è™æœ¬åœ°ï¼‰å’Œç©ºä¸­å›¾åƒï¼ˆå¦‚å«æ˜Ÿç…§ç‰‡ï¼‰ä¸ä¼ ç»Ÿçš„GISæ•°æ®æºç›¸ç»“åˆã€‚æœ¬æ–‡å®šä¹‰äº†è¿™ä¸ªæ„¿æ™¯ï¼Œæè¿°äº†æ„ŸçŸ¥å’Œäº’åŠ¨æ–¹æ³•ï¼Œç»™å‡ºäº†ä¸‰ä¸ªèŒƒä¾‹ï¼Œå¹¶åˆ—ä¸¾äº†æœªæ¥å·¥ä½œçš„å…³é”®æŒ‘æˆ˜å’Œæœºé‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äº¤äº’å¼æ•°å­—åœ°å›¾å·²æ”¹å˜æ—…è¡Œå’Œäº†è§£ä¸–ç•Œçš„æ–¹å¼ã€‚</li>
<li>åœ°ç†è§†è§‰ä»£ç†å…·å¤‡ç†è§£å›åº”å…³äºä¸–ç•Œçš„å¾®å¦™è§†è§‰ç©ºé—´æŸ¥è¯¢çš„èƒ½åŠ›ã€‚</li>
<li>åœ°ç†è§†è§‰ä»£ç†ä¾èµ–äºå¤§è§„æ¨¡åœ°ç†ç©ºé—´å›¾åƒå­˜å‚¨åº“è¿›è¡Œåˆ†æã€‚</li>
<li>å¼•å…¥åœ°ç†è§†è§‰ä»£ç†èƒ½å¤Ÿæ•´åˆå¤šç§æ•°æ®æºï¼ŒåŒ…æ‹¬è¡—æ™¯ã€åŸºäºåœ°ç‚¹çš„ç…§ç‰‡å’Œç©ºä¸­å›¾åƒç­‰ã€‚</li>
<li>åœ°ç†è§†è§‰ä»£ç†å…·æœ‰æ½œåœ¨çš„å¤šæ¨¡æ€äº¤äº’èƒ½åŠ›ã€‚</li>
<li>å®ç°åœ°ç†è§†è§‰ä»£ç†é¢ä¸´çš„å…³é”®æŒ‘æˆ˜åŒ…æ‹¬æŠ€æœ¯éš¾é¢˜å’Œæ•°æ®é›†æˆé—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15752">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7830525ee6ff8700ce6494e55e52f741.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="End-to-End-Agentic-RAG-System-Training-for-Traceable-Diagnostic-Reasoning"><a href="#End-to-End-Agentic-RAG-System-Training-for-Traceable-Diagnostic-Reasoning" class="headerlink" title="End-to-End Agentic RAG System Training for Traceable Diagnostic   Reasoning"></a>End-to-End Agentic RAG System Training for Traceable Diagnostic   Reasoning</h2><p><strong>Authors:Qiaoyu Zheng, Yuze Sun, Chaoyi Wu, Weike Zhao, Pengcheng Qiu, Yongguo Yu, Kun Sun, Yanfeng Wang, Ya Zhang, Weidi Xie</strong></p>
<p>Accurate diagnosis with medical large language models is hindered by knowledge gaps and hallucinations. Retrieval and tool-augmented methods help, but their impact is limited by weak use of external knowledge and poor feedback-reasoning traceability. To address these challenges, We introduce Deep-DxSearch, an agentic RAG system trained end-to-end with reinforcement learning (RL) that enables steer tracebale retrieval-augmented reasoning for medical diagnosis. In Deep-DxSearch, we first construct a large-scale medical retrieval corpus comprising patient records and reliable medical knowledge sources to support retrieval-aware reasoning across diagnostic scenarios. More crutially, we frame the LLM as the core agent and the retrieval corpus as its environment, using tailored rewards on format, retrieval, reasoning structure, and diagnostic accuracy, thereby evolving the agentic RAG policy from large-scale data through RL.   Experiments demonstrate that our end-to-end agentic RL training framework consistently outperforms prompt-engineering and training-free RAG approaches across multiple data centers. After training, Deep-DxSearch achieves substantial gains in diagnostic accuracy, surpassing strong diagnostic baselines such as GPT-4o, DeepSeek-R1, and other medical-specific frameworks for both common and rare disease diagnosis under in-distribution and out-of-distribution settings. Moreover, ablation studies on reward design and retrieval corpus components confirm their critical roles, underscoring the uniqueness and effectiveness of our approach compared with traditional implementations. Finally, case studies and interpretability analyses highlight improvements in Deep-DxSearchâ€™s diagnostic policy, providing deeper insight into its performance gains and supporting clinicians in delivering more reliable and precise preliminary diagnoses. See <a target="_blank" rel="noopener" href="https://github.com/MAGIC-AI4Med/Deep-DxSearch">https://github.com/MAGIC-AI4Med/Deep-DxSearch</a>. </p>
<blockquote>
<p>ç²¾ç¡®è¯Šæ–­ä¸åŒ»ç–—å¤§å‹è¯­è¨€æ¨¡å‹ä¹‹é—´å­˜åœ¨çŸ¥è¯†å·®è·å’Œå¹»è§‰çš„éšœç¢ã€‚æ£€ç´¢å’Œå·¥å…·å¢å¼ºæ–¹æ³•æœ‰æ‰€å¸®åŠ©ï¼Œä½†å®ƒä»¬çš„å½±å“å—é™äºå¤–éƒ¨çŸ¥è¯†åˆ©ç”¨ä¸è¶³å’Œåé¦ˆæ¨ç†è¿½è¸ªèƒ½åŠ›å¼±ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†Deep-DxSearchï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒçš„ä¸»åŠ¨æ¨ç†å›¾ï¼ˆRAGï¼‰ç³»ç»Ÿï¼Œèƒ½å¤Ÿå®ç°ç”¨äºåŒ»å­¦è¯Šæ–­çš„å¼•å¯¼è¿½è¸ªå¢å¼ºæ¨ç†ã€‚åœ¨Deep-DxSearchä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆæ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡åŒ»å­¦æ£€ç´¢è¯­æ–™åº“ï¼ŒåŒ…å«æ‚£è€…è®°å½•å’Œå¯é åŒ»å­¦çŸ¥è¯†æ¥æºï¼Œä»¥æ”¯æŒè·¨è¯Šæ–­åœºæ™¯çš„æ£€ç´¢æ„ŸçŸ¥æ¨ç†ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬å°†å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºæ ¸å¿ƒä¸»ä½“ï¼Œå°†æ£€ç´¢è¯­æ–™åº“ä½œä¸ºå…¶ç¯å¢ƒï¼Œä½¿ç”¨é’ˆå¯¹æ ¼å¼ã€æ£€ç´¢ã€æ¨ç†ç»“æ„å’Œè¯Šæ–­å‡†ç¡®åº¦çš„å®šåˆ¶å¥–åŠ±ï¼Œä»è€Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ä»å¤§è§„æ¨¡æ•°æ®ä¸­æ¼”åŒ–ä¸»åŠ¨æ¨ç†å›¾ç­–ç•¥ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç«¯åˆ°ç«¯ä¸»åŠ¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ¡†æ¶åœ¨å¤šä¸ªæ•°æ®ä¸­å¿ƒå§‹ç»ˆä¼˜äºåŸºäºæç¤ºå’Œè®­ç»ƒå…è´¹çš„RAGæ–¹æ³•ã€‚ç»è¿‡è®­ç»ƒï¼ŒDeep-DxSearchåœ¨è¯Šæ–­å‡†ç¡®æ€§æ–¹é¢å–å¾—äº†å®è´¨æ€§è¿›å±•ï¼Œè¶…è¶Šäº†å¼ºå¤§çš„è¯Šæ–­åŸºçº¿ï¼Œå¦‚GPT-4oã€DeepSeek-R1å’Œå…¶ä»–é’ˆå¯¹å¸¸è§å’Œç½•è§ç–¾ç—…è¯Šæ–­çš„åŒ»å­¦ç‰¹å®šæ¡†æ¶ï¼Œæ— è®ºæ˜¯åœ¨å†…éƒ¨æ•°æ®åˆ†å¸ƒè¿˜æ˜¯å¤–éƒ¨æ•°æ®åˆ†å¸ƒç¯å¢ƒä¸­å‡è¡¨ç°ä¼˜å¼‚ã€‚æ­¤å¤–ï¼Œå…³äºå¥–åŠ±è®¾è®¡å’Œæ£€ç´¢è¯­æ–™åº“ç»„ä»¶çš„æ¶ˆèç ”ç©¶è¯å®äº†å®ƒä»¬çš„å…³é”®ä½œç”¨ï¼Œå¼ºè°ƒäº†æˆ‘ä»¬çš„æ–¹æ³•ä¸ä¼ ç»Ÿå®æ–½ç›¸æ¯”çš„ç‹¬ç‰¹æ€§å’Œæœ‰æ•ˆæ€§ã€‚æœ€åï¼Œæ¡ˆä¾‹ç ”ç©¶å’Œå¯è§£é‡Šæ€§åˆ†æçªå‡ºäº†Deep-DxSearchè¯Šæ–­ç­–ç•¥çš„æ”¹è¿›ï¼Œæä¾›äº†å¯¹å…¶æ€§èƒ½æå‡çš„æ›´æ·±å…¥è§è§£ï¼Œå¹¶æ”¯æŒä¸´åºŠåŒ»ç”Ÿæä¾›æ›´å¯é å’Œç²¾ç¡®çš„æ—©æœŸè¯Šæ–­ã€‚è¯¦è§<a target="_blank" rel="noopener" href="https://github.com/MAGIC-AI4Med/Deep-DxSearch%E3%80%82">https://github.com/MAGIC-AI4Med/Deep-DxSearchã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15746v1">PDF</a> 35 pages, 5 figures, 3 tables</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŒ»å­¦è¯Šæ–­ä¸­çš„å‡†ç¡®åº”ç”¨å—åˆ°çŸ¥è¯†å·®è·å’Œå¹»è§‰çš„åˆ¶çº¦ã€‚è™½ç„¶æ£€ç´¢å’Œå·¥å…·å¢å¼ºæ–¹æ³•æœ‰åŠ©äºæ”¹è¿›ï¼Œä½†å®ƒä»¬å—åˆ°å¤–éƒ¨çŸ¥è¯†åˆ©ç”¨ä¸è¶³å’Œåé¦ˆæ¨ç†å¯è¿½æº¯æ€§å·®çš„é™åˆ¶ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºDeep-DxSearchï¼Œä¸€ä¸ªä»¥å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„ä¸»ä½“é—´æ£€ç´¢å¢å¼ºæ¨ç†ï¼ˆRAGï¼‰ç³»ç»Ÿï¼Œç”¨äºåŒ»å­¦è¯Šæ–­ã€‚Deep-DxSearchæ„å»ºå¤§è§„æ¨¡åŒ»å­¦æ£€ç´¢è¯­æ–™åº“ï¼Œæ¶µç›–æ‚£è€…è®°å½•å’Œå¯é åŒ»å­¦çŸ¥è¯†æºï¼Œæ”¯æŒè·¨è¯Šæ–­åœºæ™¯çš„æ£€ç´¢æ„ŸçŸ¥æ¨ç†ã€‚å°†å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºæ ¸å¿ƒä¸»ä½“ï¼Œæ£€ç´¢è¯­æ–™åº“ä½œä¸ºå…¶ç¯å¢ƒï¼Œé€šè¿‡é’ˆå¯¹æ ¼å¼ã€æ£€ç´¢ã€æ¨ç†ç»“æ„å’Œè¯Šæ–­å‡†ç¡®æ€§çš„å¥–åŠ±æœºåˆ¶ï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ è¿›è¡Œä¸»ä½“é—´RAGç­–ç•¥çš„è®­ç»ƒä¸ä¼˜åŒ–ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç»ˆç«¯ä¸»ä½“å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ¡†æ¶åœ¨å¤šä¸­å¿ƒæ•°æ®ä¸­è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œè¯Šæ–­å‡†ç¡®ç‡å¤§å¹…æå‡ï¼Œè¶…è¶Šäº†GPT-4oã€DeepSeek-R1ç­‰å¼ºå¤§è¯Šæ–­åŸºçº¿ä»¥åŠä¼ ç»ŸåŒ»å­¦ç‰¹å®šæ¡†æ¶ï¼Œé€‚ç”¨äºå¸¸è§å’Œç½•è§ç–¾ç—…çš„è¯Šæ–­ï¼Œæ— è®ºæ˜¯åœ¨å†…éƒ¨è¿˜æ˜¯å¤–éƒ¨åˆ†å¸ƒç¯å¢ƒä¸‹å‡è¡¨ç°ä¼˜å¼‚ã€‚æ­¤å¤–ï¼Œå¥–åŠ±è®¾è®¡å’Œæ£€ç´¢è¯­æ–™åº“ç»„ä»¶çš„æ¶ˆé™¤ç ”ç©¶è¯å®äº†å…¶å…³é”®ä½œç”¨ï¼Œå‡¸æ˜¾æˆ‘ä»¬æ–¹æ³•çš„ç‹¬ç‰¹æ€§å’Œæœ‰æ•ˆæ€§ã€‚æœ€ç»ˆï¼Œæ¡ˆä¾‹ç ”ç©¶å’Œè§£é‡Šæ€§åˆ†ææ­ç¤ºäº†Deep-DxSearchè¯Šæ–­ç­–ç•¥çš„æ”¹è¿›ï¼Œä¸ºæ€§èƒ½æå‡æä¾›äº†æ·±å…¥è§è§£ï¼Œæ”¯æŒä¸´åºŠåŒ»ç”Ÿæä¾›æ›´å¯é å’Œç²¾ç¡®çš„æ—©æœŸè¯Šæ–­ã€‚æ›´å¤šä¿¡æ¯è¯·å‚è§<a target="_blank" rel="noopener" href="https://github.com/MAGIC-AI4Med/Deep-DxSearch%E3%80%82">https://github.com/MAGIC-AI4Med/Deep-DxSearchã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŒ»å­¦è¯Šæ–­ä¸­é¢ä¸´çŸ¥è¯†å·®è·å’Œå¹»è§‰çš„æŒ‘æˆ˜ã€‚</li>
<li>Deep-DxSearchæ˜¯ä¸€ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ çš„ä¸»ä½“é—´æ£€ç´¢å¢å¼ºæ¨ç†ç³»ç»Ÿï¼Œç”¨äºåŒ»å­¦è¯Šæ–­ã€‚</li>
<li>Deep-DxSearchæ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„åŒ»å­¦æ£€ç´¢è¯­æ–™åº“æ¥æ”¯æŒè·¨è¯Šæ–­åœºæ™¯çš„æ£€ç´¢æ„ŸçŸ¥æ¨ç†ã€‚</li>
<li>Deep-DxSearché€šè¿‡é’ˆå¯¹å¤šä¸ªæ–¹é¢çš„å¥–åŠ±æœºåˆ¶ï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ ¸å¿ƒä¸»ä½“å’Œç­–ç•¥ã€‚</li>
<li>å®éªŒè¡¨æ˜Deep-DxSearchåœ¨è¯Šæ–­å‡†ç¡®ç‡ä¸Šè¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œé€‚ç”¨äºå¤šç§ç¯å¢ƒå’Œç–¾ç—…ç±»å‹ã€‚</li>
<li>å¥–åŠ±è®¾è®¡å’Œæ£€ç´¢è¯­æ–™åº“ç»„ä»¶çš„é‡è¦æ€§é€šè¿‡æ¶ˆé™¤ç ”ç©¶å¾—åˆ°è¯å®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15746">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d1bb8f786cad4e2d5361a4b48148892a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e16115757b3c0fd580e293a4e959ec92.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a888b967c7d1c1b107fd8d54ea7bf485.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Understanding-Action-Effects-through-Instrumental-Empowerment-in-Multi-Agent-Reinforcement-Learning"><a href="#Understanding-Action-Effects-through-Instrumental-Empowerment-in-Multi-Agent-Reinforcement-Learning" class="headerlink" title="Understanding Action Effects through Instrumental Empowerment in   Multi-Agent Reinforcement Learning"></a>Understanding Action Effects through Instrumental Empowerment in   Multi-Agent Reinforcement Learning</h2><p><strong>Authors:Ardian Selmonaj, Miroslav Strupl, Oleg Szehr, Alessandro Antonucci</strong></p>
<p>To reliably deploy Multi-Agent Reinforcement Learning (MARL) systems, it is crucial to understand individual agent behaviors within a team. While prior work typically evaluates overall team performance based on explicit reward signals or learned value functions, it is unclear how to infer agent contributions in the absence of any value feedback. In this work, we investigate whether meaningful insights into agent behaviors can be extracted that are consistent with the underlying value functions, solely by analyzing the policy distribution. Inspired by the phenomenon that intelligent agents tend to pursue convergent instrumental values, which generally increase the likelihood of task success, we introduce Intended Cooperation Values (ICVs), a method based on information-theoretic Shapley values for quantifying each agentâ€™s causal influence on their co-playersâ€™ instrumental empowerment. Specifically, ICVs measure an agentâ€™s action effect on its teammatesâ€™ policies by assessing their decision uncertainty and preference alignment. The analysis across cooperative and competitive MARL environments reveals the extent to which agents adopt similar or diverse strategies. By comparing action effects between policies and value functions, our method identifies which agent behaviors are beneficial to team success, either by fostering deterministic decisions or by preserving flexibility for future action choices. Our proposed method offers novel insights into cooperation dynamics and enhances explainability in MARL systems. </p>
<blockquote>
<p>åœ¨éƒ¨ç½²å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMulti-Agent Reinforcement Learningï¼Œç®€ç§°MARLï¼‰ç³»ç»Ÿæ—¶ï¼Œç†è§£å›¢é˜Ÿä¸­å•ä¸ªæ™ºèƒ½ä½“çš„è¡Œä¸ºè‡³å…³é‡è¦ã€‚ä»¥å¾€çš„ç ”ç©¶é€šå¸¸åŸºäºæ˜ç¡®çš„å¥–åŠ±ä¿¡å·æˆ–å­¦ä¹ åˆ°çš„ä»·å€¼å‡½æ•°æ¥è¯„ä¼°å›¢é˜Ÿçš„æ•´ä½“æ€§èƒ½ï¼Œä½†åœ¨æ²¡æœ‰ä»»ä½•ä»·å€¼åé¦ˆçš„æƒ…å†µä¸‹ï¼Œå¦‚ä½•æ¨æ–­æ™ºèƒ½ä½“çš„è´¡çŒ®å°šä¸æ¸…æ¥šã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬è°ƒæŸ¥æ˜¯å¦ä»…é€šè¿‡åˆ†ææ”¿ç­–åˆ†å¸ƒå°±èƒ½æå–å‡ºä¸åº•å±‚ä»·å€¼å‡½æ•°ä¸€è‡´çš„æ™ºèƒ½ä½“è¡Œä¸ºçš„æ·±åˆ»è§è§£ã€‚å—æ™ºèƒ½ä½“å€¾å‘äºè¿½æ±‚è¶‹åŒçš„å·¥å…·æ€§ä»·å€¼è¿™ä¸€ç°è±¡çš„å¯å‘ï¼Œè¿™ç§ä»·å€¼é€šå¸¸ä¼šå¢åŠ ä»»åŠ¡æˆåŠŸçš„å¯èƒ½æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºä¿¡æ¯è®ºçš„Shapleyä»·å€¼çš„é¢„æœŸåˆä½œä»·å€¼ï¼ˆIntended Cooperation Valuesï¼Œç®€ç§°ICVsï¼‰æ–¹æ³•ï¼Œç”¨äºé‡åŒ–æ¯ä¸ªæ™ºèƒ½ä½“å¯¹å…¶é˜Ÿå‹å·¥å…·æ€§èµ‹èƒ½çš„å› æœå½±å“ã€‚å…·ä½“æ¥è¯´ï¼ŒICVsé€šè¿‡è¯„ä¼°å…¶é˜Ÿå‹çš„å†³ç­–ä¸ç¡®å®šæ€§å’Œåå¥½ä¸€è‡´æ€§æ¥è¡¡é‡æ™ºèƒ½ä½“çš„è¡Œä¸ºå¯¹é˜Ÿå‹æ”¿ç­–çš„å½±å“ã€‚åœ¨åˆä½œå’Œç«äº‰æ€§çš„MARLç¯å¢ƒä¸­çš„åˆ†ææ­ç¤ºäº†æ™ºèƒ½ä½“é‡‡ç”¨ç›¸ä¼¼æˆ–ä¸åŒç­–ç•¥çš„ç¨‹åº¦ã€‚é€šè¿‡æ¯”è¾ƒæ”¿ç­–å’Œä»·å€¼å‡½æ•°ä¹‹é—´çš„è¡Œä¸ºæ•ˆæœï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç¡®å®šäº†å“ªäº›æ™ºèƒ½ä½“è¡Œä¸ºå¯¹å›¢é˜ŸæˆåŠŸæœ‰ç›Šï¼Œæ˜¯é€šè¿‡ä¿ƒè¿›ç¡®å®šæ€§å†³ç­–è¿˜æ˜¯ä¿ç•™æœªæ¥è¡ŒåŠ¨é€‰æ‹©çš„çµæ´»æ€§æ¥å®ç°çš„ã€‚æˆ‘ä»¬æå‡ºçš„æ–¹æ³•ä¸ºåˆä½œåŠ¨åŠ›å­¦æä¾›äº†æ–°çš„è§è§£ï¼Œå¹¶å¢å¼ºäº†MARLç³»ç»Ÿçš„å¯è§£é‡Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15652v1">PDF</a> European Conference on Artificial Intelligence (ECAI) 2025</p>
<p><strong>Summary</strong></p>
<p>å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰ç³»ç»Ÿéƒ¨ç½²ä¸­ï¼Œç†è§£å›¢é˜Ÿå†…ä¸ªä½“æ™ºèƒ½è¡Œä¸ºè‡³å…³é‡è¦ã€‚åœ¨ç¼ºä¹ä»·å€¼åé¦ˆçš„æƒ…å†µä¸‹ï¼Œé€šè¿‡åˆ†ææ”¿ç­–åˆ†å¸ƒæ¥æ¨æ–­æ™ºèƒ½ä½“è¡Œä¸ºå…·æœ‰é‡è¦æ„ä¹‰ã€‚æœ¬ç ”ç©¶å—åˆ°æ™ºèƒ½ä½“è¿½æ±‚æ”¶æ•›æ€§å·¥å…·ä»·å€¼çš„å¯å‘ï¼Œå¼•å…¥æ„å›¾åˆä½œä»·å€¼ï¼ˆICVsï¼‰æ–¹æ³•ï¼ŒåŸºäºä¿¡æ¯è®ºçš„æ²™æ™®åˆ©å€¼é‡åŒ–æ¯ä¸ªæ™ºèƒ½ä½“å¯¹åŒä¼´å·¥å…·èµ‹èƒ½çš„å› æœå½±å“ã€‚ICVsé€šè¿‡è¯„ä¼°åŒä¼´å†³ç­–ä¸ç¡®å®šæ€§å’Œåå¥½ä¸€è‡´æ€§æ¥è¡¡é‡æ™ºèƒ½ä½“è¡ŒåŠ¨å¯¹åŒä¼´æ”¿ç­–çš„å½±å“ã€‚åˆ†æåˆä½œå’Œç«äº‰æ€§MARLç¯å¢ƒæ­ç¤ºäº†æ™ºèƒ½ä½“é‡‡å–ç›¸ä¼¼æˆ–ä¸åŒç­–ç•¥çš„ç¨‹åº¦ã€‚é€šè¿‡æ¯”è¾ƒæ”¿ç­–å’Œä»·å€¼å‡½æ•°ä¹‹é—´çš„è¡ŒåŠ¨æ•ˆåº”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç¡®å®šäº†å“ªäº›æ™ºèƒ½ä½“è¡Œä¸ºå¯¹å›¢é˜ŸæˆåŠŸæœ‰ç›Šï¼Œè¦ä¹ˆé€šè¿‡åŸ¹å…»ç¡®å®šæ€§å†³ç­–ï¼Œè¦ä¹ˆé€šè¿‡ä¿ç•™æœªæ¥è¡ŒåŠ¨é€‰æ‹©çš„çµæ´»æ€§ã€‚è¯¥ç ”ç©¶ä¸ºåˆä½œåŠ¨åŠ›å­¦æä¾›äº†æ–°çš„è§è§£ï¼Œæé«˜äº†MARLç³»ç»Ÿçš„å¯è§£é‡Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åœ¨å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰ç³»ç»Ÿéƒ¨ç½²ä¸­ï¼Œç†è§£ä¸ªä½“æ™ºèƒ½è¡Œä¸ºå¯¹å›¢é˜Ÿè‡³å…³é‡è¦ã€‚</li>
<li>ç¼ºä¹ä»·å€¼åé¦ˆçš„æƒ…å†µä¸‹ï¼Œå¯é€šè¿‡åˆ†ææ”¿ç­–åˆ†å¸ƒæ¨æ–­æ™ºèƒ½ä½“è¡Œä¸ºã€‚</li>
<li>æ™ºèƒ½ä½“è¿½æ±‚æ”¶æ•›æ€§å·¥å…·ä»·å€¼ï¼Œæœ¬ç ”ç©¶åŸºäºæ­¤å¼•å…¥æ„å›¾åˆä½œä»·å€¼ï¼ˆICVsï¼‰æ–¹æ³•ã€‚</li>
<li>ICVsåŸºäºä¿¡æ¯è®ºçš„æ²™æ™®åˆ©å€¼é‡åŒ–æ¯ä¸ªæ™ºèƒ½ä½“å¯¹åŒä¼´å·¥å…·èµ‹èƒ½çš„å› æœå½±å“ã€‚</li>
<li>ICVsé€šè¿‡åˆ†ææ™ºèƒ½ä½“è¡ŒåŠ¨å¯¹åŒä¼´æ”¿ç­–çš„å½±å“æ¥è¯„ä¼°å†³ç­–ä¸ç¡®å®šæ€§å’Œåå¥½ä¸€è‡´æ€§ã€‚</li>
<li>åˆ†ææ˜¾ç¤ºåˆä½œå’Œç«äº‰æ€§MARLç¯å¢ƒä¸­æ™ºèƒ½ä½“é‡‡å–ä¸åŒç­–ç•¥çš„ç¨‹åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15652">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2e5ffe47926dccef258d767158f4b1e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-27a0a7d79979f807008d366da3742699.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-85c621a56847c922b6d846fb637163d0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fb42dcd2b4b1b49da35d8ccdebc98ef0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-22c9942835930d17481b72a8a27610fa.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Super-additive-Cooperation-in-Language-Model-Agents"><a href="#Super-additive-Cooperation-in-Language-Model-Agents" class="headerlink" title="Super-additive Cooperation in Language Model Agents"></a>Super-additive Cooperation in Language Model Agents</h2><p><strong>Authors:Filippo Tonini, Lukas Galke</strong></p>
<p>With the prospect of autonomous artificial intelligence (AI) agents, studying their tendency for cooperative behavior becomes an increasingly relevant topic. This study is inspired by the super-additive cooperation theory, where the combined effects of repeated interactions and inter-group rivalry have been argued to be the cause for cooperative tendencies found in humans. We devised a virtual tournament where language model agents, grouped into teams, face each other in a Prisonerâ€™s Dilemma game. By simulating both internal team dynamics and external competition, we discovered that this blend substantially boosts both overall and initial, one-shot cooperation levels (the tendency to cooperate in one-off interactions). This research provides a novel framework for large language models to strategize and act in complex social scenarios and offers evidence for how intergroup competition can, counter-intuitively, result in more cooperative behavior. These insights are crucial for designing future multi-agent AI systems that can effectively work together and better align with human values. Source code is available at <a target="_blank" rel="noopener" href="https://github.com/pippot/Superadditive-cooperation-LLMs">https://github.com/pippot/Superadditive-cooperation-LLMs</a>. </p>
<blockquote>
<p>éšç€äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰è‡ªä¸»ä»£ç†çš„å‰æ™¯å±•ç°ï¼Œç ”ç©¶å…¶åˆä½œè¡Œä¸ºçš„å€¾å‘æˆä¸ºä¸€ä¸ªè¶Šæ¥è¶Šé‡è¦çš„è¯é¢˜ã€‚æœ¬ç ”ç©¶å—åˆ°è¶…åŠ æ€§åˆä½œç†è®ºçš„å¯å‘ï¼Œè¯¥ç†è®ºè®¤ä¸ºé‡å¤äº’åŠ¨çš„è”åˆæ•ˆåº”å’Œå›¢é˜Ÿä¹‹é—´çš„ç«äº‰æ˜¯å¯¼è‡´äººç±»åˆä½œå€¾å‘çš„åŸå› ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªè™šæ‹Ÿé”¦æ ‡èµ›ï¼Œè¯­è¨€æ¨¡å‹ä»£ç†è¢«åˆ†æˆå›¢é˜Ÿï¼Œåœ¨å›šå¾’å›°å¢ƒæ¸¸æˆä¸­ç›¸äº’å¯¹æŠ—ã€‚é€šè¿‡æ¨¡æ‹Ÿå›¢é˜Ÿå†…éƒ¨åŠ¨æ€å’Œå¤–éƒ¨ç«äº‰ï¼Œæˆ‘ä»¬å‘ç°è¿™ç§ç»“åˆæ˜¾è‘—æé«˜äº†æ•´ä½“å’Œåˆæ¬¡ã€å•æ¬¡åˆä½œçš„æ°´å¹³ï¼ˆåœ¨ä¸€æ¬¡æ€§äº’åŠ¨ä¸­åˆä½œçš„å€¾å‘ï¼‰ã€‚è¯¥ç ”ç©¶ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚ç¤¾ä¼šåœºæ™¯ä¸­çš„ç­–ç•¥åˆ¶å®šå’Œè¡Œä¸ºæä¾›äº†ä¸€ä¸ªæ–°çš„æ¡†æ¶ï¼Œå¹¶æä¾›äº†è¯æ®ï¼Œè¯æ˜å›¢é˜Ÿé—´çš„ç«äº‰å¦‚ä½•å‡ºäººæ„æ–™åœ°å¯¼è‡´æ›´å¤šçš„åˆä½œè¡Œä¸ºã€‚è¿™äº›è§è§£å¯¹äºè®¾è®¡æœªæ¥èƒ½å¤ŸååŒå·¥ä½œçš„å¤šæ™ºèƒ½ä½“AIç³»ç»Ÿï¼Œä½¿å…¶æ›´å¥½åœ°ç¬¦åˆäººç±»ä»·å€¼è§‚è‡³å…³é‡è¦ã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/pippot/Superadditive-cooperation-LLMs%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/pippot/Superadditive-cooperation-LLMsä¸­æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15510v1">PDF</a> FAIEMA 2025</p>
<p><strong>æ€»ç»“</strong><br>    æœ¬ç ”ç©¶åŸºäºè¶…åŠ æ€§åˆä½œç†è®ºï¼Œæ¢è®¨è¯­è¨€æ¨¡å‹ä»£ç†åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­åˆä½œçš„è¶‹åŠ¿ã€‚è®¾è®¡è™šæ‹Ÿç«èµ›ç¯å¢ƒæ¨¡æ‹Ÿå›¢é˜Ÿå†…éƒ¨å’Œå¤–éƒ¨ç«äº‰å¯¹è¯­è¨€æ¨¡å‹ä»£ç†åœ¨å›šå¾’å›°å¢ƒæ¸¸æˆä¸­çš„è¡¨ç°ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç«äº‰å’Œåˆä½œçš„ç»“åˆèƒ½æ˜¾è‘—æé«˜å•æ¬¡å’Œæ€»ä½“åˆä½œæ°´å¹³ã€‚è¿™ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚ç¤¾ä¼šåœºæ™¯ä¸­çš„ç­–ç•¥åˆ¶å®šå’Œè¡Œä¸ºæä¾›äº†æ–°æ¡†æ¶ï¼Œå¹¶ä¸ºæœªæ¥å¤šæ™ºèƒ½ä½“AIç³»ç»Ÿçš„è®¾è®¡å’Œäººç±»ä»·å€¼è§‚çš„èåˆæä¾›äº†é‡è¦å¯ç¤ºã€‚ä»£ç å…¬å¼€åœ¨GitHubä¸Šã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åŸºäºè¶…åŠ æ€§åˆä½œç†è®ºï¼Œæ¢è®¨äº†è¯­è¨€æ¨¡å‹ä»£ç†çš„åˆä½œè¡Œä¸ºã€‚</li>
<li>è®¾è®¡è™šæ‹Ÿç«èµ›ç¯å¢ƒæ¨¡æ‹Ÿè¯­è¨€æ¨¡å‹ä»£ç†çš„å›¢é˜ŸåŠ¨æ€å’Œå¤–éƒ¨ç«äº‰ã€‚</li>
<li>å‘ç°å†…éƒ¨å’Œå¤–éƒ¨å› ç´ çš„ç»“åˆæé«˜äº†å•æ¬¡å’Œæ€»ä½“åˆä½œæ°´å¹³ã€‚</li>
<li>ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚ç¤¾ä¼šåœºæ™¯ä¸­çš„ç­–ç•¥åˆ¶å®šæä¾›äº†æ–°æ¡†æ¶ã€‚</li>
<li>ç ”ç©¶ç»“æœè¯æ˜å›¢é˜Ÿé—´çš„ç«äº‰å¯ä»¥æé«˜åˆä½œè¡Œä¸ºï¼Œè¿™æ˜¯ä¸ç›´è§‰ç›¸åçš„ç°è±¡ã€‚</li>
<li>æ­¤ç ”ç©¶å¯¹è®¾è®¡æœªæ¥çš„å¤šæ™ºèƒ½ä½“AIç³»ç»Ÿå…·æœ‰é‡è¦æ„ä¹‰ã€‚è¿™äº›ç³»ç»Ÿå¯ä»¥æ›´å¥½åœ°ä¸äººç±»ååŒå·¥ä½œå¹¶æ›´å¥½åœ°ä½“ç°äººç±»ä»·å€¼è§‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15510">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6f0e15026ac94e107178b28dc333d2a0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc6e564bc866b03b7b395a9fe58827e8.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="From-Bits-to-Boardrooms-A-Cutting-Edge-Multi-Agent-LLM-Framework-for-Business-Excellence"><a href="#From-Bits-to-Boardrooms-A-Cutting-Edge-Multi-Agent-LLM-Framework-for-Business-Excellence" class="headerlink" title="From Bits to Boardrooms: A Cutting-Edge Multi-Agent LLM Framework for   Business Excellence"></a>From Bits to Boardrooms: A Cutting-Edge Multi-Agent LLM Framework for   Business Excellence</h2><p><strong>Authors:Zihao Wang, Junming Zhang</strong></p>
<p>Large Language Models (LLMs) have shown promising potential in business applications, particularly in enterprise decision support and strategic planning, yet current approaches often struggle to reconcile intricate operational analyses with overarching strategic goals across diverse market environments, leading to fragmented workflows and reduced collaboration across organizational levels. This paper introduces BusiAgent, a novel multi-agent framework leveraging LLMs for advanced decision-making in complex corporate environments. BusiAgent integrates three core innovations: an extended Continuous Time Markov Decision Process (CTMDP) for dynamic agent modeling, a generalized entropy measure to optimize collaborative efficiency, and a multi-level Stackelberg game to handle hierarchical decision processes. Additionally, contextual Thompson sampling is employed for prompt optimization, supported by a comprehensive quality assurance system to mitigate errors. Extensive empirical evaluations across diverse business scenarios validate BusiAgentâ€™s efficacy, demonstrating its capacity to generate coherent, client-focused solutions that smoothly integrate granular insights with high-level strategy, significantly outperforming established approaches in both solution quality and user satisfaction. By fusing cutting-edge AI technologies with deep business insights, BusiAgent marks a substantial step forward in AI-driven enterprise decision-making, empowering organizations to navigate complex business landscapes more effectively. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å•†ä¸šåº”ç”¨æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¼ä¸šå†³ç­–æ”¯æŒå’Œæˆ˜ç•¥è§„åˆ’æ–¹é¢ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•å¸¸å¸¸éš¾ä»¥åœ¨å¤šæ ·çš„å¸‚åœºç¯å¢ƒä¸­ï¼Œå°†å¤æ‚çš„æ“ä½œåˆ†æä¸é«˜çº§æˆ˜ç•¥ç›®æ ‡ç›¸åè°ƒï¼Œå¯¼è‡´å·¥ä½œæµç¨‹ç¢ç‰‡åŒ–ï¼Œä¸”å„çº§ç»„ç»‡é—´çš„åä½œå‡å°‘ã€‚æœ¬æ–‡ä»‹ç»äº†BusiAgentï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œåˆ©ç”¨LLMåœ¨å¤æ‚çš„å•†ä¸šç¯å¢ƒä¸­è¿›è¡Œé«˜çº§å†³ç­–ã€‚BusiAgenté›†æˆäº†ä¸‰é¡¹æ ¸å¿ƒåˆ›æ–°ï¼šé‡‡ç”¨æ‰©å±•çš„è¿ç»­æ—¶é—´é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆCTMDPï¼‰è¿›è¡ŒåŠ¨æ€æ™ºèƒ½ä½“å»ºæ¨¡ï¼Œä½¿ç”¨å¹¿ä¹‰ç†µåº¦é‡ä¼˜åŒ–åä½œæ•ˆç‡ï¼Œä»¥åŠé‡‡ç”¨å¤šå±‚æ¬¡æ–¯å¡”å…‹å°”ä¼¯æ ¼åšå¼ˆå¤„ç†åˆ†å±‚å†³ç­–è¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œè¿˜é‡‡ç”¨ä¸Šä¸‹æ–‡æ±¤æ™®æ£®é‡‡æ ·è¿›è¡Œæç¤ºä¼˜åŒ–ï¼Œå¹¶ç”±å…¨é¢çš„è´¨é‡ä¿è¯ç³»ç»Ÿæä¾›æ”¯æŒä»¥å‡è½»é”™è¯¯ã€‚åœ¨å¤šç§å•†ä¸šåœºæ™¯ä¸‹çš„å¹¿æ³›å®è¯è¯„ä¼°éªŒè¯äº†BusiAgentçš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜å…¶èƒ½å¤Ÿç”Ÿæˆè¿è´¯ã€ä»¥å®¢æˆ·ä¸ºä¸­å¿ƒè§£å†³æ–¹æ¡ˆçš„èƒ½åŠ›ï¼Œèƒ½å¤Ÿé¡ºç•…åœ°å°†é¢—ç²’çŠ¶è§è§£ä¸é«˜çº§ç­–ç•¥ç›¸ç»“åˆï¼Œåœ¨è§£å†³æ–¹æ¡ˆè´¨é‡å’Œç”¨æˆ·æ»¡æ„åº¦æ–¹é¢éƒ½å¤§å¤§ä¼˜äºç°æœ‰æ–¹æ³•ã€‚é€šè¿‡å°†å‰æ²¿äººå·¥æ™ºèƒ½æŠ€æœ¯ä¸æ·±åˆ»çš„å•†ä¸šæ´å¯Ÿç›¸ç»“åˆï¼ŒBusiAgentåœ¨AIé©±åŠ¨çš„ä¼ä¸šå†³ç­–æ–¹é¢å–å¾—äº†é‡å¤§è¿›æ­¥ï¼Œä½¿ç»„ç»‡èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åº”å¯¹å¤æ‚çš„å•†ä¸šç¯å¢ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15447v1">PDF</a> Accepted by ECAI 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¼ä¸šå†³ç­–æ”¯æŒå’Œæˆ˜ç•¥è§„åˆ’æ–¹é¢çš„åº”ç”¨å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†åœ¨å¤æ‚çš„å•†ä¸šç¯å¢ƒä¸­ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥å°†è¯¦ç»†çš„æ“ä½œåˆ†æä¸é«˜çº§æˆ˜ç•¥ç›®æ ‡ç›¸ç»“åˆï¼Œå¯¼è‡´å·¥ä½œæµç¨‹ç¢ç‰‡åŒ–å¹¶é™ä½äº†ç»„ç»‡å†…çš„åä½œæ•ˆç‡ã€‚æœ¬æ–‡æå‡ºBusiAgentï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå¤æ‚ä¼ä¸šç¯å¢ƒå†³ç­–åˆ¶å®šçš„æ–°å‹å¤šæ™ºèƒ½ä½“æ¡†æ¶ã€‚BusiAgenté›†æˆäº†ä¸‰é¡¹æ ¸å¿ƒåˆ›æ–°ï¼šé‡‡ç”¨æ‰©å±•çš„è¿ç»­æ—¶é—´é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹è¿›è¡ŒåŠ¨æ€æ™ºèƒ½ä½“å»ºæ¨¡ã€ä½¿ç”¨å¹¿ä¹‰ç†µåº¦é‡ä¼˜åŒ–åä½œæ•ˆç‡ä»¥åŠåˆ©ç”¨å¤šå±‚æ¬¡æ–¯å¡”å…‹å°”ä¼¯æ ¼åšå¼ˆå¤„ç†åˆ†å±‚å†³ç­–è¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œè¿˜é‡‡ç”¨æƒ…å¢ƒåŒ–æ±¤æ™®æ£®é‡‡æ ·æ³•è¿›è¡Œæç¤ºä¼˜åŒ–ï¼Œå¹¶ç”±å…¨é¢çš„è´¨é‡ä¿è¯ç³»ç»Ÿæä¾›æ”¯æŒä»¥å‡è½»é”™è¯¯ã€‚åœ¨å¤šç§å•†ä¸šåœºæ™¯ä¸‹çš„å¹¿æ³›å®è¯è¯„ä¼°éªŒè¯äº†BusiAgentçš„æœ‰æ•ˆæ€§ï¼Œè¡¨æ˜å…¶åœ¨è§£å†³æ–¹æ¡ˆè´¨é‡å’Œç”¨æˆ·æ»¡æ„åº¦æ–¹é¢å‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œèƒ½å¤Ÿç”Ÿæˆè¿è´¯ã€ä»¥å®¢æˆ·ä¸ºä¸­å¿ƒä¸”èƒ½å°†è¯¦ç»†è§è§£ä¸é«˜çº§æˆ˜ç•¥é¡ºåˆ©ç»“åˆçš„è§£å†³æ–¹æ¡ˆã€‚é€šè¿‡èåˆå°–ç«¯äººå·¥æ™ºèƒ½æŠ€æœ¯ä¸æ·±å…¥çš„ä¼ä¸šæ´å¯Ÿï¼ŒBusiAgentæ ‡å¿—ç€äººå·¥æ™ºèƒ½é©±åŠ¨çš„ä¼ä¸šå†³ç­–åˆ¶å®šå–å¾—äº†é‡å¤§è¿›æ­¥ï¼Œå¸®åŠ©ç»„ç»‡æ›´æœ‰æ•ˆåœ°åº”å¯¹å¤æ‚çš„å•†ä¸šç¯å¢ƒæŒ‘æˆ˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸šåŠ¡åº”ç”¨ä¸­æœ‰å·¨å¤§æ½œåŠ›ï¼Œå°¤å…¶åœ¨å†³ç­–æ”¯æŒå’Œæˆ˜ç•¥è§„åˆ’æ–¹é¢ã€‚</li>
<li>å½“å‰æ–¹æ³•éš¾ä»¥åœ¨å¤æ‚ç¯å¢ƒä¸­æ•´åˆæ“ä½œåˆ†æä¸é«˜çº§æˆ˜ç•¥ç›®æ ‡ã€‚</li>
<li>BusiAgentæ˜¯ä¸€ä¸ªæ–°å‹çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œä¼ä¸šå†³ç­–åˆ¶å®šã€‚</li>
<li>BusiAgenté›†æˆäº†ä¸‰é¡¹æ ¸å¿ƒåˆ›æ–°æŠ€æœ¯ï¼šè¿ç»­æ—¶é—´é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ã€å¹¿ä¹‰ç†µä¼˜åŒ–å’Œå¤šå±‚æ¬¡æ–¯å¡”å…‹å°”ä¼¯æ ¼åšå¼ˆã€‚</li>
<li>BusiAgenté‡‡ç”¨æƒ…å¢ƒåŒ–æ±¤æ™®æ£®é‡‡æ ·æ³•è¿›è¡Œæç¤ºä¼˜åŒ–ï¼Œå…·æœ‰å…¨é¢çš„è´¨é‡ä¿è¯ç³»ç»Ÿã€‚</li>
<li>åœ¨å¤šç§å•†ä¸šåœºæ™¯ä¸‹çš„è¯„ä¼°ä¸­ï¼ŒBusiAgentè¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œæ˜¾è‘—æé«˜äº†è§£å†³æ–¹æ¡ˆè´¨é‡å’Œç”¨æˆ·æ»¡æ„åº¦ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15447">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3d63e9e50b7931e2b8ec6e0e71c62fb7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de212b9b03a5cdd96e7970c1761dea6c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-295f2b69de52793e655b1d8cf450ca5b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f9f70a77bddf4cef476d2c905268e224.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5cdfc8c8bbcd3a5fcd308cd6db011f32.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9e7fe383e9108738ea7f821667e20f1.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="See-it-Say-it-Sorted-Agentic-System-for-Compositional-Diagram-Generation"><a href="#See-it-Say-it-Sorted-Agentic-System-for-Compositional-Diagram-Generation" class="headerlink" title="See it. Say it. Sorted: Agentic System for Compositional Diagram   Generation"></a>See it. Say it. Sorted: Agentic System for Compositional Diagram   Generation</h2><p><strong>Authors:Hantao Zhang, Jingyang Liu, Ed Li</strong></p>
<p>We study sketch-to-diagram generation: converting rough hand sketches into precise, compositional diagrams. Diffusion models excel at photorealism but struggle with the spatial precision, alignment, and symbolic structure required for flowcharts. We introduce See it. Say it. Sorted., a training-free agentic system that couples a Vision-Language Model (VLM) with Large Language Models (LLMs) to produce editable Scalable Vector Graphics (SVG) programs. The system runs an iterative loop in which a Critic VLM proposes a small set of qualitative, relational edits; multiple candidate LLMs synthesize SVG updates with diverse strategies (conservative-&gt;aggressive, alternative, focused); and a Judge VLM selects the best candidate, ensuring stable improvement. This design prioritizes qualitative reasoning over brittle numerical estimates, preserves global constraints (e.g., alignment, connectivity), and naturally supports human-in-the-loop corrections. On 10 sketches derived from flowcharts in published papers, our method more faithfully reconstructs layout and structure than two frontier closed-source image generation LLMs (GPT-5 and Gemini-2.5-Pro), accurately composing primitives (e.g., multi-headed arrows) without inserting unwanted text. Because outputs are programmatic SVGs, the approach is readily extensible to presentation tools (e.g., PowerPoint) via APIs and can be specialized with improved prompts and task-specific tools. The codebase is open-sourced at <a target="_blank" rel="noopener" href="https://github.com/hantaoZhangrichard/see_it_say_it_sorted.git">https://github.com/hantaoZhangrichard/see_it_say_it_sorted.git</a>. </p>
<blockquote>
<p>æˆ‘ä»¬ç ”ç©¶äº†ä»è‰å›¾åˆ°å›¾è¡¨ç”Ÿæˆçš„ç ”ç©¶ï¼šå°†ç²—ç•¥çš„æ‰‹ç»˜è‰å›¾è½¬åŒ–ä¸ºç²¾ç¡®ã€ç»„åˆå¼çš„å›¾è¡¨ã€‚æ‰©æ•£æ¨¡å‹åœ¨ç…§ç‰‡å†™å®æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æµç¨‹å›¾æ‰€éœ€çš„ç©ºé—´ç²¾åº¦ã€å¯¹é½å’Œç¬¦å·ç»“æ„æ–¹é¢å­˜åœ¨å›°éš¾ã€‚æˆ‘ä»¬æ¨å‡ºäº†â€œSee it. Say it. Sorted.â€ç³»ç»Ÿï¼Œè¿™æ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„ä¸»ä½“ç³»ç»Ÿï¼Œå°†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç›¸ç»“åˆï¼Œç”Ÿæˆå¯ç¼–è¾‘çš„çŸ¢é‡å›¾å½¢ï¼ˆSVGï¼‰ç¨‹åºã€‚è¯¥ç³»ç»Ÿè¿è¡Œä¸€ä¸ªè¿­ä»£å¾ªç¯ï¼Œå…¶ä¸­æ‰¹è¯„å®¶VLMæå‡ºä¸€å°éƒ¨åˆ†å®šæ€§å…³ç³»ç¼–è¾‘ï¼›å¤šä¸ªå€™é€‰LLMä½¿ç”¨ä¸åŒç­–ç•¥ï¼ˆä¿å®ˆåˆ°æ¿€è¿›ã€æ›¿ä»£ã€ä¸“æ³¨ï¼‰åˆæˆSVGæ›´æ–°ï¼›æ³•å®˜VLMé€‰æ‹©æœ€ä½³å€™é€‰è€…ï¼Œç¡®ä¿ç¨³å®šçš„æ”¹è¿›ã€‚è¿™ç§è®¾è®¡ä¼˜å…ˆè€ƒè™‘å®šæ€§æ¨ç†è€Œéè„†å¼±çš„æ•°å€¼ä¼°è®¡ï¼Œä¿ç•™å…¨å±€çº¦æŸï¼ˆä¾‹å¦‚å¯¹é½ã€è¿æ¥ï¼‰ï¼Œå¹¶è‡ªç„¶åœ°æ”¯æŒäººä¸ºå¾ªç¯æ ¡æ­£ã€‚åœ¨å¯¹æ¥è‡ªå·²å‘è¡¨è®ºæ–‡çš„10ä¸ªæµç¨‹å›¾è‰å›¾çš„å¤„ç†ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ¯”å‰æ²¿çš„å°é—­å¼å›¾åƒç”ŸæˆLLMï¼ˆGPT-5å’ŒGemini-2.5 Proï¼‰æ›´å¿ å®åœ°é‡å»ºå¸ƒå±€å’Œç»“æ„ï¼Œèƒ½å¤Ÿå‡†ç¡®åœ°ç»„åˆåŸºæœ¬å…ƒç´ ï¼ˆä¾‹å¦‚å¤šå¤´ç®­å¤´ï¼‰ï¼Œè€Œä¸ä¼šæ’å…¥ä¸éœ€è¦çš„æ–‡æœ¬ã€‚ç”±äºè¾“å‡ºæ˜¯ç¨‹åºåŒ–çš„SVGï¼Œå› æ­¤è¯¥æ–¹æ³•å¯ä»¥è½»æ¾é€šè¿‡APIæ‰©å±•åˆ°æ¼”ç¤ºå·¥å…·ï¼ˆä¾‹å¦‚PowerPointï¼‰ï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡æ”¹è¿›æç¤ºå’Œä»»åŠ¡ç‰¹å®šå·¥å…·è¿›è¡Œä¸“é—¨åŒ–ã€‚è¯¥ä»£ç åº“å·²å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/hantaoZhangrichard/see_it_say_it_sorted.git%E3%80%82">https://github.com/hantaoZhangrichard/see_it_say_it_sorted.gitã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15222v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºè‰å›¾åˆ°å›¾è¡¨ç”Ÿæˆçš„ç ”ç©¶ï¼Œé€šè¿‡æ— è®­ç»ƒçš„ç³»ç»ŸSee it. Say it. Sorted.ï¼Œå°†ç²—ç•¥çš„æ‰‹ç»˜è‰å›¾è½¬åŒ–ä¸ºç²¾ç¡®çš„ç»„æˆå›¾è¡¨ã€‚ç³»ç»Ÿé‡‡ç”¨è¿­ä»£å¾ªç¯çš„æ–¹å¼ï¼Œé€šè¿‡ä¸‰ä¸ªç»„ä»¶â€”â€”Critic VLMæå‡ºä¿®æ”¹å»ºè®®ï¼Œå¤šä¸ªå€™é€‰LLMåˆæˆSVGæ›´æ–°å†…å®¹ï¼ŒJudge VLMé€‰æ‹©æœ€ä½³å€™é€‰â€”â€”ç¡®ä¿æŒç»­æ”¹è¿›ã€‚è¯¥ç³»ç»Ÿæ³¨é‡å®šæ€§æ¨ç†ï¼Œå…¼é¡¾å…¨å±€çº¦æŸå’Œäººä¸ºæ ¡æ­£åŠŸèƒ½ã€‚ç›¸è¾ƒäºå…¶ä»–å‰æ²¿çš„é—­æºå›¾åƒç”ŸæˆLLMsï¼Œè¯¥ç³»ç»Ÿèƒ½æ›´å‡†ç¡®åœ°é‡æ„æµç¨‹å’Œç»“æ„ï¼Œå‡†ç¡®ç»„åˆåŸºæœ¬å…ƒç´ ã€‚è¾“å‡ºçš„ç¨‹åºåŒ–SVGæ ¼å¼ä½¿å…¶æ˜“äºæ‰©å±•åˆ°æ¼”ç¤ºå·¥å…·å¦‚PowerPointç­‰ã€‚ç›¸å…³ä»£ç å·²å¼€æºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç ”ç©¶è‰å›¾åˆ°å›¾è¡¨ç”ŸæˆæŠ€æœ¯ï¼Œå°†ç²—ç•¥æ‰‹ç»˜è‰å›¾è½¬åŒ–ä¸ºç²¾ç¡®å›¾è¡¨ã€‚</li>
<li>æå‡ºæ— è®­ç»ƒçš„ç³»ç»ŸSee it. Say it. Sorted.ï¼ŒåŒ…å«ä¸‰ä¸ªç»„ä»¶ï¼šCritic VLMã€å¤šä¸ªå€™é€‰LLMå’ŒJudge VLMã€‚</li>
<li>ç³»ç»Ÿæ³¨é‡å®šæ€§æ¨ç†ï¼Œå…¼é¡¾å…¨å±€çº¦æŸå’Œäººä¸ºæ ¡æ­£åŠŸèƒ½ã€‚</li>
<li>ä¸å…¶ä»–å‰æ²¿LLMsç›¸æ¯”ï¼Œè¯¥ç³»ç»Ÿèƒ½æ›´å‡†ç¡®åœ°é‡æ„å›¾è¡¨å¸ƒå±€å’Œç»“æ„ã€‚</li>
<li>è¾“å‡ºç¨‹åºåŒ–çš„SVGæ ¼å¼ä½¿å¾—åº”ç”¨åœºæ™¯å…·æœ‰å¹¿æ³›æ€§å’Œå¯æ‰©å±•æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15222">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f74dfde820fa8f822759ea1919ac78b6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0371cdf5f20cd938fa3a4c512c43617e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-22615bd766eaf130be902e26f5ac964f.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Mobile-Agent-v3-Foundamental-Agents-for-GUI-Automation"><a href="#Mobile-Agent-v3-Foundamental-Agents-for-GUI-Automation" class="headerlink" title="Mobile-Agent-v3: Foundamental Agents for GUI Automation"></a>Mobile-Agent-v3: Foundamental Agents for GUI Automation</h2><p><strong>Authors:Jiabo Ye, Xi Zhang, Haiyang Xu, Haowei Liu, Junyang Wang, Zhaoqing Zhu, Ziwei Zheng, Feiyu Gao, Junjie Cao, Zhengxi Lu, Jitong Liao, Qi Zheng, Fei Huang, Jingren Zhou, Ming Yan</strong></p>
<p>This paper introduces GUI-Owl, a foundational GUI agent model that achieves state-of-the-art performance among open-source end-to-end models on ten GUI benchmarks across desktop and mobile environments, covering grounding, question answering, planning, decision-making, and procedural knowledge. GUI-Owl-7B achieves 66.4 on AndroidWorld and 29.4 on OSWorld. Building on this, we propose Mobile-Agent-v3, a general-purpose GUI agent framework that further improves performance to 73.3 on AndroidWorld and 37.7 on OSWorld, setting a new state-of-the-art for open-source GUI agent frameworks. GUI-Owl incorporates three key innovations: (1) Large-scale Environment Infrastructure: a cloud-based virtual environment spanning Android, Ubuntu, macOS, and Windows, enabling our Self-Evolving GUI Trajectory Production framework. This generates high-quality interaction data via automated query generation and correctness validation, leveraging GUI-Owl to refine trajectories iteratively, forming a self-improving loop. It supports diverse data pipelines and reduces manual annotation. (2) Diverse Foundational Agent Capabilities: by integrating UI grounding, planning, action semantics, and reasoning patterns, GUI-Owl supports end-to-end decision-making and can act as a modular component in multi-agent systems. (3) Scalable Environment RL: we develop a scalable reinforcement learning framework with fully asynchronous training for real-world alignment. We also introduce Trajectory-aware Relative Policy Optimization (TRPO) for online RL, achieving 34.9 on OSWorld. GUI-Owl and Mobile-Agent-v3 are open-sourced at <a target="_blank" rel="noopener" href="https://github.com/X-PLUG/MobileAgent">https://github.com/X-PLUG/MobileAgent</a>. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†GUI-Owlï¼Œè¿™æ˜¯ä¸€ç§åŸºç¡€GUIä»£ç†æ¨¡å‹ï¼Œåœ¨æ¡Œé¢å’Œç§»åŠ¨ç¯å¢ƒçš„åä¸ªGUIåŸºå‡†æµ‹è¯•ä¸­ï¼Œå®ƒåœ¨å¼€æºç«¯åˆ°ç«¯æ¨¡å‹ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ¶µç›–äº†æ¥åœ°ã€é—®ç­”ã€è§„åˆ’ã€å†³ç­–å’Œç¨‹åºçŸ¥è¯†ã€‚GUI-Owl-7Båœ¨AndroidWorldä¸Šå¾—åˆ†ä¸º66.4ï¼Œåœ¨OSWorldä¸Šå¾—åˆ†ä¸º29.4ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†Mobile-Agent-v3ï¼Œè¿™æ˜¯ä¸€ä¸ªé€šç”¨çš„GUIä»£ç†æ¡†æ¶ï¼Œè¿›ä¸€æ­¥æé«˜äº†æ€§èƒ½ï¼Œåœ¨AndroidWorldä¸Šè¾¾åˆ°73.3ï¼Œåœ¨OSWorldä¸Šè¾¾åˆ°37.7ï¼Œä¸ºå¼€æºGUIä»£ç†æ¡†æ¶åˆ›é€ äº†æ–°çš„æœ€å…ˆè¿›çš„æ°´å¹³ã€‚GUI-Owlæœ‰ä¸‰ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šï¼ˆ1ï¼‰å¤§è§„æ¨¡ç¯å¢ƒåŸºç¡€è®¾æ–½ï¼šä¸€ä¸ªè·¨è¶ŠAndroidã€Ubuntuã€macOSå’ŒWindowsçš„åŸºäºäº‘çš„è™šæ‹Ÿç¯å¢ƒï¼Œä½¿æˆ‘ä»¬çš„è‡ªæˆ‘è¿›åŒ–GUIè½¨è¿¹ç”Ÿäº§æ¡†æ¶æˆä¸ºå¯èƒ½ã€‚å®ƒé€šè¿‡è‡ªåŠ¨åŒ–æŸ¥è¯¢ç”Ÿæˆå’Œæ­£ç¡®æ€§éªŒè¯ç”Ÿæˆé«˜è´¨é‡äº¤äº’æ•°æ®ï¼Œåˆ©ç”¨GUI-Owlè¿­ä»£ä¼˜åŒ–è½¨è¿¹ï¼Œå½¢æˆä¸€ä¸ªè‡ªæˆ‘æ”¹è¿›å¾ªç¯ã€‚å®ƒæ”¯æŒå¤šæ ·åŒ–çš„æ•°æ®ç®¡é“ï¼Œå‡å°‘æ‰‹åŠ¨æ³¨é‡Šã€‚ï¼ˆ2ï¼‰å¤šæ ·åŒ–çš„åŸºç¡€ä»£ç†åŠŸèƒ½ï¼šé€šè¿‡é›†æˆUIæ¥åœ°ã€è§„åˆ’ã€åŠ¨ä½œè¯­ä¹‰å’Œæ¨ç†æ¨¡å¼ï¼ŒGUI-Owlæ”¯æŒç«¯åˆ°ç«¯çš„å†³ç­–åˆ¶å®šï¼Œå¹¶å¯ä»¥ä½œä¸ºå¤šä»£ç†ç³»ç»Ÿä¸­çš„æ¨¡å—åŒ–ç»„ä»¶ã€‚ ï¼ˆ3ï¼‰å¯æ‰©å±•çš„ç¯å¢ƒå¼ºåŒ–å­¦ä¹ ï¼šæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå¯æ‰©å±•çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå…·æœ‰å®Œå…¨å¼‚æ­¥è®­ç»ƒä»¥å®ç°ç°å®ä¸–ç•Œå¯¹é½ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†è½¨è¿¹æ„ŸçŸ¥ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆTRPOï¼‰è¿›è¡Œåœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼Œåœ¨OSWorldä¸Šå®ç°34.9å¾—åˆ†ã€‚GUI-Owlå’ŒMobile-Agent-v3åœ¨<a target="_blank" rel="noopener" href="https://github.com/X-PLUG/MobileAgent">https://github.com/X-PLUG/MobileAgent</a>ä¸Šå¼€æºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15144v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>GUI-Owlæ˜¯ä¸€æ¬¾åŸºç¡€GUIä»£ç†æ¨¡å‹ï¼Œåœ¨æ¡Œé¢å’Œç§»åŠ¨ç¯å¢ƒçš„åä¸ªGUIåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚é€šè¿‡å¼•å…¥å¤§å‹ç¯å¢ƒåŸºç¡€è®¾æ–½ã€å¤šæ ·åŒ–çš„åŸºç¡€ä»£ç†èƒ½åŠ›å’Œå¯æ‰©å±•çš„ç¯å¢ƒå¼ºåŒ–å­¦ä¹ ï¼ŒGUI-Owlä¸æ–­æå‡è‡ªæˆ‘è¿›åŒ–è½¨è¿¹ç”Ÿäº§æ¡†æ¶çš„è´¨é‡ã€‚å…¶å¼€æºç‰ˆæœ¬ä¸ºç ”ç©¶è€…æä¾›äº†ä¸€ä¸ªå¼ºå¤§çš„å·¥å…·ã€‚Mobile-Agent-v3åœ¨æ­¤åŸºç¡€ä¸Šè¿›ä¸€æ­¥æé«˜æ€§èƒ½ï¼Œå±•ç¤ºäº†å…¶å‡ºè‰²çš„GUIä»£ç†æ¡†æ¶ä¼˜åŠ¿ã€‚è¿™äº›æ¨¡å‹ä¸ºåˆ›å»ºæ›´åŠ æ™ºèƒ½ã€è‡ªä¸»çš„äº¤äº’ç³»ç»Ÿå¼€è¾Ÿäº†æ–°çš„å¯èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GUI-Owlæ¨¡å‹åœ¨æ¡Œé¢å’Œç§»åŠ¨ç¯å¢ƒçš„GUIåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>GUI-Owlå…·å¤‡å¤§å‹ç¯å¢ƒåŸºç¡€è®¾æ–½ï¼Œæ”¯æŒå¤šæ ·åŒ–æ•°æ®ç®¡é“å’Œå‡å°‘æ‰‹åŠ¨æ ‡æ³¨ã€‚</li>
<li>é€šè¿‡ç»“åˆUIæ¥åœ°ã€è§„åˆ’ã€åŠ¨ä½œè¯­ä¹‰å’Œæ¨ç†æ¨¡å¼ï¼ŒGUI-Owlæ”¯æŒç«¯åˆ°ç«¯çš„å†³ç­–åˆ¶å®šã€‚</li>
<li>Mobile-Agent-v3ä½œä¸ºé€šç”¨GUIä»£ç†æ¡†æ¶ï¼Œæé«˜äº†æ€§èƒ½å¹¶è®¾å®šäº†æ–°çš„å¼€æºGUIä»£ç†æ¡†æ¶æ ‡å‡†ã€‚</li>
<li>GUI-Owlé‡‡ç”¨è‡ªæˆ‘è¿›åŒ–è½¨è¿¹ç”Ÿäº§æ¡†æ¶ï¼Œå½¢æˆè‡ªæˆ‘æ”¹è¿›å¾ªç¯ï¼Œæå‡äº¤äº’æ•°æ®è´¨é‡ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ æ¡†æ¶å…·å¤‡å¯æ‰©å±•æ€§å’Œå®Œå…¨å¼‚æ­¥è®­ç»ƒç‰¹æ€§ï¼Œé€‚ç”¨äºçœŸå®ä¸–ç•Œåœºæ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15144">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-80c97cadc0da7730513a65955695be8c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a88af975441203b8a9da9f9fd7299d7f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8c532b38d8f80847c616baa04ff12c73.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a5aeaa9cb8684922bcd597a5f2ea0a43.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6366c4e2b849c160d1da1ff1330d53bd.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="FinAgentBench-A-Benchmark-Dataset-for-Agentic-Retrieval-in-Financial-Question-Answering"><a href="#FinAgentBench-A-Benchmark-Dataset-for-Agentic-Retrieval-in-Financial-Question-Answering" class="headerlink" title="FinAgentBench: A Benchmark Dataset for Agentic Retrieval in Financial   Question Answering"></a>FinAgentBench: A Benchmark Dataset for Agentic Retrieval in Financial   Question Answering</h2><p><strong>Authors:Chanyeol Choi, Jihoon Kwon, Alejandro Lopez-Lira, Chaewoon Kim, Minjae Kim, Juneha Hwang, Jaeseon Ha, Hojun Choi, Suyeol Yun, Yongjin Kim, Yongjae Lee</strong></p>
<p>Accurate information retrieval (IR) is critical in the financial domain, where investors must identify relevant information from large collections of documents. Traditional IR methods-whether sparse or dense-often fall short in retrieval accuracy, as it requires not only capturing semantic similarity but also performing fine-grained reasoning over document structure and domain-specific knowledge. Recent advances in large language models (LLMs) have opened up new opportunities for retrieval with multi-step reasoning, where the model ranks passages through iterative reasoning about which information is most relevant to a given query. However, there exists no benchmark to evaluate such capabilities in the financial domain. To address this gap, we introduce FinAgentBench, the first large-scale benchmark for evaluating retrieval with multi-step reasoning in finance â€“ a setting we term agentic retrieval. The benchmark consists of 3,429 expert-annotated examples on S&amp;P-100 listed firms and assesses whether LLM agents can (1) identify the most relevant document type among candidates, and (2) pinpoint the key passage within the selected document. Our evaluation framework explicitly separates these two reasoning steps to address context limitations. This design enables to provide a quantitative basis for understanding retrieval-centric LLM behavior in finance. We evaluate a suite of state-of-the-art models and further demonstrated how targeted fine-tuning can significantly improve agentic retrieval performance. Our benchmark provides a foundation for studying retrieval-centric LLM behavior in complex, domain-specific tasks for finance. We will release the dataset publicly upon acceptance of the paper and plan to expand and share dataset for the full S&amp;P 500 and beyond. </p>
<blockquote>
<p>å‡†ç¡®çš„æƒ…æŠ¥æ£€ç´¢ï¼ˆIRï¼‰åœ¨é‡‘èé¢†åŸŸè‡³å…³é‡è¦ï¼ŒæŠ•èµ„è€…å¿…é¡»åœ¨å¤§é‡çš„æ–‡æ¡£é›†åˆä¸­æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚ä¼ ç»Ÿçš„IRæ–¹æ³•ï¼Œæ— è®ºç¨€ç–è¿˜æ˜¯å¯†é›†ï¼Œå¾€å¾€åœ¨æ£€ç´¢å‡†ç¡®æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå› ä¸ºå®ƒä¸ä»…éœ€è¦æ•æ‰è¯­ä¹‰ç›¸ä¼¼æ€§ï¼Œè¿˜éœ€è¦å¯¹æ–‡æ¡£ç»“æ„å’Œç‰¹å®šé¢†åŸŸçš„çŸ¥è¯†è¿›è¡Œç²¾ç»†çš„æ¨ç†ã€‚è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•ä¸ºå…·æœ‰å¤šæ­¥æ¨ç†çš„æ£€ç´¢æä¾›äº†æ–°çš„æœºä¼šï¼Œè¯¥æ¨¡å‹é€šè¿‡è¿­ä»£æ¨ç†å¯¹ä¸ç»™å®šæŸ¥è¯¢æœ€ç›¸å…³çš„ä¿¡æ¯è¿›è¡Œæ’åã€‚ç„¶è€Œï¼Œé‡‘èé¢†åŸŸå°šæœªæœ‰åŸºå‡†æµ‹è¯•æ¥è¯„ä¼°è¿™ç§èƒ½åŠ›ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†FinAgentBenchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºè¯„ä¼°é‡‘èé¢†åŸŸä¸­å…·æœ‰å¤šæ­¥æ¨ç†çš„æ£€ç´¢çš„å¤§å‹åŸºå‡†æµ‹è¯•â€”â€”æˆ‘ä»¬ç§°ä¹‹ä¸ºä»£ç†æ£€ç´¢ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…å«3429ä¸ªå…³äºæ ‡æ™®100ä¸Šå¸‚å…¬å¸çš„ä¸“å®¶æ³¨é‡Šç¤ºä¾‹ï¼Œå¹¶è¯„ä¼°LLMä»£ç†æ˜¯å¦èƒ½ï¼ˆ1ï¼‰åœ¨å€™é€‰è€…ä¸­è¯†åˆ«å‡ºæœ€ç›¸å…³çš„æ–‡æ¡£ç±»å‹ï¼Œï¼ˆ2ï¼‰åœ¨æ‰€é€‰æ–‡æ¡£ä¸­å®šä½å…³é”®æ®µè½ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ¡†æ¶æ˜ç¡®åœ°å°†è¿™ä¸¤ä¸ªæ¨ç†æ­¥éª¤åˆ†å¼€ï¼Œä»¥è§£å†³ä¸Šä¸‹æ–‡é™åˆ¶é—®é¢˜ã€‚è¿™ç§è®¾è®¡èƒ½å¤Ÿæä¾›å®šé‡ä¾æ®ï¼Œä»¥äº†è§£é‡‘èé¢†åŸŸä¸­ä»¥æ£€ç´¢ä¸ºä¸­å¿ƒçš„LLMçš„è¡Œä¸ºã€‚æˆ‘ä»¬è¯„ä¼°äº†ä¸€ç³»åˆ—æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œå¹¶è¿›ä¸€æ­¥å±•ç¤ºäº†æœ‰é’ˆå¯¹æ€§çš„å¾®è°ƒå¦‚ä½•æ˜¾ç€æé«˜ä»£ç†æ£€ç´¢æ€§èƒ½ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•ä¸ºç ”ç©¶é‡‘èé¢†åŸŸçš„å¤æ‚ç‰¹å®šä»»åŠ¡çš„ä»¥æ£€ç´¢ä¸ºä¸­å¿ƒçš„LLMè¡Œä¸ºæä¾›äº†åŸºç¡€ã€‚è®ºæ–‡æ¥å—åï¼Œæˆ‘ä»¬å°†å…¬å¼€å‘å¸ƒæ•°æ®é›†ï¼Œå¹¶è®¡åˆ’å¯¹æ ‡æ™®500æŒ‡æ•°åŠä»¥åçš„æ•°æ®é›†è¿›è¡Œæ‰©å±•å’Œå…±äº«ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14052v2">PDF</a> 6 pages</p>
<p><strong>Summary</strong><br>     é‡‘èé¢†åŸŸçš„ä¿¡æ¯æ£€ç´¢ï¼ˆIRï¼‰è‡³å…³é‡è¦ï¼ŒæŠ•èµ„è€…éœ€ä»å¤§é‡æ–‡æ¡£ä¸­æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚ä¼ ç»ŸIRæ–¹æ³•å¸¸å› æœªèƒ½æ•æ‰è¯­ä¹‰ç›¸ä¼¼æ€§åŠè¿›è¡Œç²¾ç»†çš„æ–‡æ¡£ç»“æ„å’Œé¢†åŸŸç‰¹å®šçŸ¥è¯†æ¨ç†è€Œå¯¼è‡´æ£€ç´¢ç²¾åº¦ä¸è¶³ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•ä¸ºå…·æœ‰å¤šæ­¥æ¨ç†çš„æ£€ç´¢æä¾›äº†æ–°çš„æœºä¼šï¼Œè¯¥æ¨¡å‹é€šè¿‡è¿­ä»£æ¨ç†å¯¹ä¸ç»™å®šæŸ¥è¯¢æœ€ç›¸å…³çš„ä¿¡æ¯è¿›è¡Œæ’åã€‚ç„¶è€Œï¼Œé‡‘èé¢†åŸŸç¼ºä¹è¯„ä¼°è¿™ç§èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚ä¸ºè§£å†³æ­¤ç©ºç™½ï¼Œæˆ‘ä»¬æ¨å‡ºäº†FinAgentBenchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºè¯„ä¼°é‡‘èé¢†åŸŸä¸­å…·æœ‰å¤šæ­¥æ¨ç†çš„æ£€ç´¢èƒ½åŠ›çš„å¤§è§„æ¨¡åŸºå‡†æµ‹è¯•â€”â€”æˆ‘ä»¬ç§°ä¹‹ä¸ºagenticæ£€ç´¢ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…å«3429ä¸ªå…³äºæ ‡æ™®100å®¶ä¸Šå¸‚å…¬å¸çš„ä¸“å®¶æ³¨é‡Šç¤ºä¾‹ï¼Œè¯„ä¼°LLMä»£ç†æ˜¯å¦èƒ½ï¼ˆ1ï¼‰åœ¨å€™é€‰æ–‡æ¡£ä¸­è¯†åˆ«å‡ºæœ€ç›¸å…³çš„æ–‡æ¡£ç±»å‹ï¼Œï¼ˆ2ï¼‰åœ¨æ‰€é€‰æ–‡æ¡£ä¸­å®šä½å…³é”®æ®µè½ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ¡†æ¶æ˜ç¡®åœ°å°†è¿™ä¸¤ä¸ªæ¨ç†æ­¥éª¤åˆ†å¼€ï¼Œä»¥è§£å†³ä¸Šä¸‹æ–‡å±€é™æ€§ã€‚è¿™ç§è®¾è®¡æœ‰åŠ©äºä¸ºç†è§£é‡‘èé¢†åŸŸä¸­ä»¥æ£€ç´¢ä¸ºä¸­å¿ƒçš„LLMè¡Œä¸ºæä¾›å®šé‡ä¾æ®ã€‚æˆ‘ä»¬è¯„ä¼°äº†ä¸€ç³»åˆ—æœ€æ–°æ¨¡å‹ï¼Œå¹¶å±•ç¤ºäº†å¦‚ä½•æœ‰é’ˆå¯¹æ€§åœ°å¾®è°ƒä»¥æ˜¾è‘—æ”¹å–„agenticæ£€ç´¢æ€§èƒ½ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•ä¸ºç ”ç©¶é‡‘èé¢†åŸŸçš„å¤æ‚ã€ç‰¹å®šä»»åŠ¡ä¸­çš„æ£€ç´¢ä¸­å¿ƒLLMè¡Œä¸ºå¥ å®šäº†åŸºç¡€ã€‚æˆ‘ä»¬å°†åœ¨è®ºæ–‡è¢«æ¥å—åå…¬å¼€å‘å¸ƒæ•°æ®é›†ï¼Œå¹¶è®¡åˆ’æ‰©å±•åˆ°æ•´ä¸ªæ ‡æ™®500æŒ‡æ•°åŠæ›´å¹¿æ³›é¢†åŸŸã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é‡‘èé¢†åŸŸçš„ä¿¡æ¯æ£€ç´¢ï¼ˆIRï¼‰è‡³å…³é‡è¦ï¼Œéœ€è¦å‡†ç¡®æ•æ‰è¯­ä¹‰ç›¸ä¼¼æ€§ã€æ–‡æ¡£ç»“æ„å’Œé¢†åŸŸçŸ¥è¯†ã€‚</li>
<li>ä¼ ç»ŸIRæ–¹æ³•åœ¨æŸäº›æƒ…å†µä¸‹è¡¨ç°ä¸è¶³ï¼Œéœ€è¦æ–°çš„æ–¹æ³•æé«˜æ£€ç´¢ç²¾åº¦ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯ä»¥é€šè¿‡å¤šæ­¥æ¨ç†æé«˜æ£€ç´¢èƒ½åŠ›ï¼Œé€šè¿‡è¿­ä»£æ¨ç†æ’åç›¸å…³ä¿¡æ¯ã€‚</li>
<li>é‡‘èé¢†åŸŸç¼ºä¹è¯„ä¼°LLMè¿›è¡Œå¤šæ­¥æ¨ç†æ£€ç´¢çš„åŸºå‡†æµ‹è¯•ã€‚</li>
<li>æ¨å‡ºFinAgentBenchåŸºå‡†æµ‹è¯•ï¼Œè¯„ä¼°LLMåœ¨è¯†åˆ«æœ€ç›¸å…³æ–‡æ¡£ç±»å‹å’Œå®šä½å…³é”®æ®µè½æ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>è¯„ä¼°æ¡†æ¶æ˜ç¡®åŒºåˆ†ä¸¤ä¸ªæ¨ç†æ­¥éª¤ï¼Œè§£å†³ä¸Šä¸‹æ–‡å±€é™æ€§é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14052">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fd69921bb210b3b00028254d96dded0a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7edd011ecb55ab8f5f1dc0a0d3bc0c4d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cdc646c6400897963657abaae90790d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b17b2b1e8956436fd5c1ebad0b07f26.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e63ef8bc6c0f1a43c6d8c26f11621086.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Preacher-Paper-to-Video-Agentic-System"><a href="#Preacher-Paper-to-Video-Agentic-System" class="headerlink" title="Preacher: Paper-to-Video Agentic System"></a>Preacher: Paper-to-Video Agentic System</h2><p><strong>Authors:Jingwei Liu, Ling Yang, Hao Luo, Fan Wang, Hongyan Li, Mengdi Wang</strong></p>
<p>The paper-to-video task converts a research paper into a structured video abstract, distilling key concepts, methods, and conclusions into an accessible, well-organized format. While state-of-the-art video generation models demonstrate potential, they are constrained by limited context windows, rigid video duration constraints, limited stylistic diversity, and an inability to represent domain-specific knowledge. To address these limitations, we introduce Preacher, the first paper-to-video agentic system. Preacher employs a topdown approach to decompose, summarize, and reformulate the paper, followed by bottom-up video generation, synthesizing diverse video segments into a coherent abstract. To align cross-modal representations, we define key scenes and introduce a Progressive Chain of Thought (P-CoT) for granular, iterative planning. Preacher successfully generates high-quality video abstracts across five research fields, demonstrating expertise beyond current video generation models. Code will be released at: <a target="_blank" rel="noopener" href="https://github.com/GenVerse/Paper2Video">https://github.com/GenVerse/Paper2Video</a> </p>
<blockquote>
<p>è¿™ç¯‡è®ºæ–‡å°†ç ”ç©¶è®ºæ–‡è½¬åŒ–ä¸ºç»“æ„åŒ–çš„è§†é¢‘æ‘˜è¦ï¼Œæç‚¼å…³é”®æ¦‚å¿µã€æ–¹æ³•å’Œç»“è®ºï¼Œä½¿å…¶æ˜“äºç†è§£å¹¶ä»¥è‰¯å¥½çš„ç»„ç»‡æ–¹å¼å‘ˆç°ã€‚å°½ç®¡æœ€å…ˆè¿›çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å®ƒä»¬å—é™äºæœ‰é™çš„ä¸Šä¸‹æ–‡çª—å£ã€ä¸¥æ ¼çš„è§†é¢‘æŒç»­æ—¶é—´é™åˆ¶ã€æœ‰é™çš„é£æ ¼å¤šæ ·æ€§å’Œæ— æ³•è¡¨ç¤ºç‰¹å®šé¢†åŸŸçŸ¥è¯†ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†Preacherï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªè®ºæ–‡åˆ°è§†é¢‘çš„æ™ºèƒ½ç³»ç»Ÿã€‚Preacheré‡‡ç”¨è‡ªä¸Šè€Œä¸‹çš„æ–¹æ³•åˆ†è§£ã€æ€»ç»“å’Œé‡æ„è®ºæ–‡ï¼Œç„¶åè¿›è¡Œè‡ªä¸‹è€Œä¸Šçš„è§†é¢‘ç”Ÿæˆï¼Œå°†å¤šæ ·åŒ–çš„è§†é¢‘ç‰‡æ®µåˆæˆä¸€ä¸ªè¿è´¯çš„æ‘˜è¦ã€‚ä¸ºäº†å¯¹é½è·¨æ¨¡æ€è¡¨ç¤ºï¼Œæˆ‘ä»¬å®šä¹‰äº†å…³é”®åœºæ™¯å¹¶å¼•å…¥äº†æ¸è¿›æ€ç»´é“¾ï¼ˆP-CoTï¼‰è¿›è¡Œç²¾ç»†çš„è¿­ä»£è§„åˆ’ã€‚PreacheræˆåŠŸåœ°åœ¨äº”ä¸ªç ”ç©¶é¢†åŸŸç”Ÿæˆäº†é«˜è´¨é‡çš„è§†é¢‘æ‘˜è¦ï¼Œå±•ç¤ºäº†è¶…è¶Šå½“å‰è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„ä¸“é•¿ã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/GenVerse/Paper2Video%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/GenVerse/Paper2Videoä¸Šå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09632v4">PDF</a> Include some mistakes</p>
<p><strong>Summary</strong><br>æ–‡æœ¬ä¸­çš„ç ”ç©¶è®ºæ–‡æ¢è®¨äº†å°†è®ºæ–‡è½¬æ¢ä¸ºè§†é¢‘æ‘˜è¦çš„æŠ€æœ¯ï¼Œè¯¦ç»†ä»‹ç»äº†å¦‚ä½•æŠ½å–å…³é”®æ¦‚å¿µã€æ–¹æ³•å’Œç»“è®ºå¹¶å°†å…¶åˆ¶ä½œæˆæœ‰æ¡ç†çš„è§†é¢‘ã€‚ä¸ºçªç ´å½“å‰è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„å±€é™ï¼Œä¾‹å¦‚æœ‰é™çš„è¯­å¢ƒçª—å£ã€è§†é¢‘æ—¶é•¿é™åˆ¶å’Œç¼ºä¹ç‰¹å®šçš„çŸ¥è¯†è¡¨ç¤ºç­‰ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„è§£å†³æ–¹æ¡ˆã€‚ä»‹ç»äº†åä¸ºPreacherçš„ç³»ç»Ÿï¼Œå®ƒæ˜¯ä¸€ä¸ªçº¸åª’è½¬æ¢è§†é¢‘çš„ä»£ç†äººç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿé€šè¿‡è‡ªä¸Šè€Œä¸‹æ–¹å¼å¤„ç†æ–‡çŒ®åˆ†è§£å’Œæ‘˜è¦ç¼–å†™è¿‡ç¨‹ï¼Œç„¶åè¿›è¡Œè‡ªä¸‹è€Œä¸Šçš„è§†é¢‘ç”Ÿæˆè¿‡ç¨‹ï¼Œå¹¶èåˆå¤šç§è§†é¢‘ç‰‡æ®µç”Ÿæˆè¿è´¯çš„æ‘˜è¦ã€‚é€šè¿‡å®šä¹‰å…³é”®åœºæ™¯å¹¶å¼•å…¥æ¸è¿›é“¾å¼æ€ç»´æ–¹æ³•ï¼ˆP-CoTï¼‰ï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿå®Œæˆç²¾ç»†ã€è¿­ä»£è§„åˆ’è¿‡ç¨‹ï¼Œå®ç°è·¨æ¨¡æ€è¡¨ç¤ºçš„åŒ¹é…ã€‚PreacheræˆåŠŸç”Ÿæˆäº†äº”ä¸ªç ”ç©¶é¢†åŸŸçš„è§†é¢‘æ‘˜è¦ï¼Œå±•ç°äº†è¶…è¶Šç°æœ‰è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„ä¸“ä¸šèƒ½åŠ›ã€‚æ›´å¤šä¿¡æ¯å¯é€šè¿‡è®¿é—®ç›¸å…³ä»£ç åº“è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/GenVerse/Paper2Video">https://github.com/GenVerse/Paper2Video</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç ”ç©¶è®ºæ–‡è½¬åŒ–ä¸ºè§†é¢‘æ‘˜è¦æŠ€æœ¯ä»‹ç»ã€‚</li>
<li>å½“å‰è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„å±€é™æ€§åŒ…æ‹¬æœ‰é™çš„è¯­å¢ƒçª—å£ã€è§†é¢‘æ—¶é•¿é™åˆ¶å’Œç¼ºä¹ç‰¹å®šçŸ¥è¯†è¡¨ç¤ºç­‰ã€‚</li>
<li>Preacherç³»ç»Ÿæ˜¯ä¸€ç§æ–°çš„çº¸åª’è½¬æ¢è§†é¢‘çš„ä»£ç†äººç³»ç»Ÿï¼Œé‡‡ç”¨è‡ªä¸Šè€Œä¸‹å’Œè‡ªä¸‹è€Œä¸Šçš„å¤„ç†æ–¹å¼ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09632">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b7637cddb75ef082363a079b2076fee2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2c35f78f977b7b562e643467f781ae79.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aabd58efacb3021bc328b855ffb6879f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-796ef1f12ffd673831080a00b4094a7f.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Agoran-An-Agentic-Open-Marketplace-for-6G-RAN-Automation"><a href="#Agoran-An-Agentic-Open-Marketplace-for-6G-RAN-Automation" class="headerlink" title="Agoran: An Agentic Open Marketplace for 6G RAN Automation"></a>Agoran: An Agentic Open Marketplace for 6G RAN Automation</h2><p><strong>Authors:Ilias Chatzistefanidis, Navid Nikaein, Andrea Leone, Ali Maatouk, Leandros Tassiulas, Roberto Morabito, Ioannis Pitsiorlas, Marios Kountouris</strong></p>
<p>Next-generation mobile networks must reconcile the often-conflicting goals of multiple service owners. However, todayâ€™s network slice controllers remain rigid, policy-bound, and unaware of the business context. We introduce Agoran Service and Resource Broker (SRB), an agentic marketplace that brings stakeholders directly into the operational loop. Inspired by the ancient Greek agora, Agoran distributes authority across three autonomous AI branches: a Legislative branch that answers compliance queries using retrieval-augmented Large Language Models (LLMs); an Executive branch that maintains real-time situational awareness through a watcher-updated vector database; and a Judicial branch that evaluates each agent message with a rule-based Trust Score, while arbitrating LLMs detect malicious behavior and apply real-time incentives to restore trust. Stakeholder-side Negotiation Agents and the SRB-side Mediator Agent negotiate feasible, Pareto-optimal offers produced by a multi-objective optimizer, reaching a consensus intent in a single round, which is then deployed to Open and AI RAN controllers. Deployed on a private 5G testbed and evaluated with realistic traces of vehicle mobility, Agoran achieved significant gains: (i) a 37% increase in throughput of eMBB slices, (ii) a 73% reduction in latency of URLLC slices, and concurrently (iii) an end-to-end 8.3% saving in PRB usage compared to a static baseline. An 1B-parameter Llama model, fine-tuned for five minutes on 100 GPT-4 dialogues, recovers approximately 80% of GPT-4.1â€™s decision quality, while operating within 6 GiB of memory and converging in only 1.3 seconds. These results establish Agoran as a concrete, standards-aligned path toward ultra-flexible, stakeholder-centric 6G networks. A live demo is presented <a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=h7vEyMu2f5w%5C&ab_channel=BubbleRAN">https://www.youtube.com/watch?v=h7vEyMu2f5w\&amp;ab_channel=BubbleRAN</a>. </p>
<blockquote>
<p>ä¸‹ä¸€ä»£ç§»åŠ¨ç½‘ç»œå¿…é¡»åè°ƒå¤šä¸ªæœåŠ¡æ‰€æœ‰è€…ç»å¸¸å†²çªçš„ç›®æ ‡ã€‚ç„¶è€Œï¼Œå½“å‰çš„ç½‘ç»œåˆ‡ç‰‡æ§åˆ¶å™¨ä»ç„¶åƒµåŒ–ã€å—æ”¿ç­–é™åˆ¶ï¼Œä¸”ä¸äº†è§£ä¸šåŠ¡ä¸Šä¸‹æ–‡ã€‚æˆ‘ä»¬å¼•å…¥äº†AgoranæœåŠ¡èµ„æºç»çºªäººï¼ˆSRBï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªä»£ç†å¸‚åœºï¼Œå®ƒç›´æ¥å°†åˆ©ç›Šç›¸å…³è€…çº³å…¥è¿è¥å¾ªç¯ã€‚Agoranä»¥å¤å¸Œè…Šçš„å¸‚é›†ä¸ºçµæ„Ÿï¼Œå°†æƒåŠ›åˆ†æ•£åˆ°ä¸‰ä¸ªè‡ªä¸»çš„äººå·¥æ™ºèƒ½åˆ†æ”¯æœºæ„ï¼šç«‹æ³•éƒ¨é—¨ä½¿ç”¨å¢å¼ºæ£€ç´¢çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å›ç­”åˆè§„æŸ¥è¯¢ï¼›è¡Œæ”¿éƒ¨é—¨é€šè¿‡è§‚å¯Ÿè€…æ›´æ–°çš„å‘é‡æ•°æ®åº“ç»´æŒå®æ—¶æƒ…å†µäº†è§£ï¼›å¸æ³•éƒ¨é—¨è¯„ä¼°æ¯ä¸ªä»£ç†æ¶ˆæ¯ï¼Œé‡‡ç”¨åŸºäºè§„åˆ™çš„ä¿¡ä»»è¯„åˆ†ï¼ŒåŒæ—¶ä»²è£LLMæ£€æµ‹æ¶æ„è¡Œä¸ºå¹¶åº”ç”¨å®æ—¶æ¿€åŠ±æ¥æ¢å¤ä¿¡ä»»ã€‚åˆ©ç›Šç›¸å…³æ–¹è°ˆåˆ¤ä»£ç†äººå’ŒSRBè°ƒè§£äººä»£ç†è°ˆåˆ¤ç”±å¤šç›®æ ‡ä¼˜åŒ–å™¨äº§ç”Ÿçš„å¯è¡Œä¸”å¸•ç´¯æ‰˜æœ€ä¼˜æŠ¥ä»·ï¼Œåœ¨å•è½®è°ˆåˆ¤ä¸­è¾¾æˆå…±è¯†æ„å›¾ï¼Œç„¶åéƒ¨ç½²åˆ°å¼€æ”¾å’ŒAI RANæ§åˆ¶å™¨ã€‚åœ¨ç§æœ‰5Gæµ‹è¯•åºŠä¸Šéƒ¨ç½²ï¼Œå¹¶ç”¨å®é™…çš„è½¦è¾†ç§»åŠ¨è½¨è¿¹è¿›è¡Œè¯„ä¼°ï¼ŒAgoranå–å¾—äº†æ˜¾è‘—æˆæ•ˆï¼šï¼ˆiï¼‰eMBBåˆ‡ç‰‡çš„ååé‡æé«˜äº†37%ï¼Œï¼ˆiiï¼‰URLLCåˆ‡ç‰‡çš„å»¶è¿Ÿå‡å°‘äº†73%ï¼ŒåŒæ—¶ï¼ˆiiiï¼‰ä¸é™æ€åŸºçº¿ç›¸æ¯”ï¼Œåœ¨PRBä½¿ç”¨ä¸Šå®ç°äº†ç«¯åˆ°ç«¯8.3%çš„èŠ‚çœã€‚ä¸€ä¸ª1000äº¿å‚æ•°çš„Llamaæ¨¡å‹ï¼Œåœ¨100ä¸ªGPT-4å¯¹è¯ä¸Šè¿›è¡Œäº”åˆ†é’Ÿå¾®è°ƒï¼Œèƒ½å¤Ÿæ¢å¤å¤§çº¦80%çš„GPT-4.1å†³ç­–è´¨é‡ï¼ŒåŒæ—¶åœ¨6GiBå†…å­˜å†…è¿è¡Œï¼Œå¹¶åœ¨ä»…1.3ç§’å†…æ”¶æ•›ã€‚è¿™äº›ç»“æœè¯æ˜äº†Agoranä½œä¸ºå®ç°è¶…çµæ´»ã€ä»¥åˆ©ç›Šç›¸å…³è€…ä¸ºä¸­å¿ƒçš„6Gç½‘ç»œçš„åˆ‡å®ã€ç¬¦åˆæ ‡å‡†çš„é“è·¯ã€‚ç°åœºæ¼”ç¤ºè¯·å‚è§ï¼š<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=h7vEyMu2f5w&ab_channel=BubbleRAN%E3%80%82">https://www.youtube.com/watch?v=h7vEyMu2f5w&ab_channel&#x3D;BubbleRANã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09159v2">PDF</a> Pre-print submitted to Computer Networks AI-for-6G</p>
<p><strong>Summary</strong><br>     ä¸‹ä¸€ä»£ç§»åŠ¨ç½‘ç»œéœ€è¦åè°ƒå¤šä¸ªæœåŠ¡æ‰€æœ‰è€…ç»å¸¸ç›¸äº’å†²çªçš„ç›®æ ‡ã€‚é’ˆå¯¹å½“å‰ç½‘ç»œåˆ‡ç‰‡æ§åˆ¶å™¨çš„åƒµåŒ–ã€å—æ”¿ç­–é™åˆ¶ä¸”ä¸äº†è§£ä¸šåŠ¡ä¸Šä¸‹æ–‡çš„é—®é¢˜ï¼Œæå‡ºäº†AgoranæœåŠ¡å’Œèµ„æºç»çºªäººï¼ˆSRBï¼‰ä½œä¸ºä¸­ä»‹å¸‚åœºï¼Œå°†åˆ©ç›Šç›¸å…³è€…ç›´æ¥çº³å…¥æ“ä½œå¾ªç¯ã€‚Agorané€šè¿‡ç«‹æ³•åˆ†æ”¯ã€è¡Œæ”¿åˆ†æ”¯å’Œå¸æ³•åˆ†æ”¯ä¸‰ä¸ªè‡ªä¸»äººå·¥æ™ºèƒ½åˆ†æ”¯åˆ†é…æƒåŠ›ï¼Œå®ç°äº†çµæ´»çš„ç½‘ç»œç®¡ç†ã€‚é€šè¿‡éƒ¨ç½²åœ¨ç§æœ‰5Gæµ‹è¯•åºŠä¸Šï¼Œç”¨çœŸå®çš„è½¦è¾†ç§»åŠ¨è½¨è¿¹è¿›è¡Œè¯„ä¼°ï¼ŒAgoranå–å¾—äº†æ˜¾è‘—æˆæ•ˆã€‚æ­¤å¤–ï¼Œå…¶LLamaæ¨¡å‹åœ¨GPT-4çš„åŸºç¡€ä¸Šå¾®è°ƒåè¡¨ç°å‡ºè‰¯å¥½çš„å†³ç­–è´¨é‡ã€‚è¿™äº›æˆæœæ ‡å¿—ç€Agoranæˆä¸ºä¸€æ¡å®ç°è¶…çµæ´»ã€ä»¥åˆ©ç›Šç›¸å…³è€…ä¸ºä¸­å¿ƒçš„6Gç½‘ç»œçš„åˆ‡å®å¯è¡Œè·¯å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¸‹ä¸€ä»£ç§»åŠ¨ç½‘ç»œéœ€è¦è§£å†³å¤šä¸ªæœåŠ¡æ‰€æœ‰è€…ä¹‹é—´çš„å†²çªç›®æ ‡ã€‚</li>
<li>å½“å‰ç½‘ç»œåˆ‡ç‰‡æ§åˆ¶å™¨å­˜åœ¨åƒµåŒ–ã€å—æ”¿ç­–é™åˆ¶ä¸”ä¸äº†è§£ä¸šåŠ¡ä¸Šä¸‹æ–‡çš„é—®é¢˜ã€‚</li>
<li>Agoranä½œä¸ºä¸€ä¸ªä¸­ä»‹å¸‚åœºï¼Œå°†åˆ©ç›Šç›¸å…³è€…çº³å…¥æ“ä½œå¾ªç¯ä¸­ã€‚</li>
<li>Agorané€šè¿‡ç«‹æ³•ã€è¡Œæ”¿å’Œå¸æ³•ä¸‰ä¸ªè‡ªä¸»äººå·¥æ™ºèƒ½åˆ†æ”¯åˆ†é…æƒåŠ›ï¼Œå®ç°çµæ´»çš„ç½‘ç»œç®¡ç†ã€‚</li>
<li>Agoranåœ¨ç§æœ‰5Gæµ‹è¯•åºŠä¸Šçš„è¯„ä¼°ç»“æœæ˜¾ç¤ºå‡ºæ˜¾è‘—æˆæ•ˆï¼ŒåŒ…æ‹¬eMBBåˆ‡ç‰‡ååé‡å¢åŠ 37%ã€URLLCåˆ‡ç‰‡å»¶è¿Ÿå‡å°‘73%ï¼Œå¹¶å®ç°äº†PRBä½¿ç”¨ç«¯åˆ°ç«¯èŠ‚çœ8.3%ã€‚</li>
<li>Agoranä½¿ç”¨çš„LLamaæ¨¡å‹åœ¨GPT-4çš„åŸºç¡€ä¸Šè¡¨ç°å‡ºè‰¯å¥½çš„å†³ç­–è´¨é‡ã€‚</li>
<li>Agoranæœ‰æœ›æˆä¸ºå®ç°è¶…çµæ´»ã€ä»¥åˆ©ç›Šç›¸å…³è€…ä¸ºä¸­å¿ƒçš„6Gç½‘ç»œçš„åˆ‡å®è·¯å¾„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09159">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d348f72cf09fd7f1400b12f80bf19a8c.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="UAV-ON-A-Benchmark-for-Open-World-Object-Goal-Navigation-with-Aerial-Agents"><a href="#UAV-ON-A-Benchmark-for-Open-World-Object-Goal-Navigation-with-Aerial-Agents" class="headerlink" title="UAV-ON: A Benchmark for Open-World Object Goal Navigation with Aerial   Agents"></a>UAV-ON: A Benchmark for Open-World Object Goal Navigation with Aerial   Agents</h2><p><strong>Authors:Jianqiang Xiao, Yuexuan Sun, Yixin Shao, Boxi Gan, Rongqiang Liu, Yanjing Wu, Weili Gua, Xiang Deng</strong></p>
<p>Aerial navigation is a fundamental yet underexplored capability in embodied intelligence, enabling agents to operate in large-scale, unstructured environments where traditional navigation paradigms fall short. However, most existing research follows the Vision-and-Language Navigation (VLN) paradigm, which heavily depends on sequential linguistic instructions, limiting its scalability and autonomy. To address this gap, we introduce UAV-ON, a benchmark for large-scale Object Goal Navigation (ObjectNav) by aerial agents in open-world environments, where agents operate based on high-level semantic goals without relying on detailed instructional guidance as in VLN. UAV-ON comprises 14 high-fidelity Unreal Engine environments with diverse semantic regions and complex spatial layouts, covering urban, natural, and mixed-use settings. It defines 1270 annotated target objects, each characterized by an instance-level instruction that encodes category, physical footprint, and visual descriptors, allowing grounded reasoning. These instructions serve as semantic goals, introducing realistic ambiguity and complex reasoning challenges for aerial agents. To evaluate the benchmark, we implement several baseline methods, including Aerial ObjectNav Agent (AOA), a modular policy that integrates instruction semantics with egocentric observations for long-horizon, goal-directed exploration. Empirical results show that all baselines struggle in this setting, highlighting the compounded challenges of aerial navigation and semantic goal grounding. UAV-ON aims to advance research on scalable UAV autonomy driven by semantic goal descriptions in complex real-world environments. </p>
<blockquote>
<p>æ— äººæœºå¯¼èˆªæ˜¯æ™ºèƒ½ä½“ï¼ˆembodied intelligenceï¼‰ä¸­çš„ä¸€é¡¹åŸºæœ¬ä½†å°šæœªè¢«å……åˆ†æ¢ç´¢çš„èƒ½åŠ›ï¼Œä½¿å¾—æ™ºèƒ½ä½“èƒ½å¤Ÿåœ¨å¤§è§„æ¨¡ã€éç»“æ„åŒ–ç¯å¢ƒä¸­è¿›è¡Œè¿è¡Œï¼Œè€Œè¿™äº›ç¯å¢ƒä¸­ä¼ ç»Ÿçš„å¯¼èˆªæ¨¡å¼å¹¶ä¸é€‚ç”¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤§å¤šæ•°ç ”ç©¶éƒ½éµå¾ªè§†è§‰ä¸è¯­è¨€å¯¼èˆªï¼ˆVLNï¼‰æ¨¡å¼ï¼Œè¯¥æ¨¡å¼ä¸¥é‡ä¾èµ–äºè¿ç»­çš„è¯­è¨€æŒ‡ä»¤ï¼Œé™åˆ¶äº†å…¶å¯æ‰©å±•æ€§å’Œè‡ªä¸»æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†UAV-ONåŸºå‡†æµ‹è¯•ï¼Œè¿™æ˜¯æ— äººæœºåœ¨å¤§è§„æ¨¡å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­è¿›è¡Œç›®æ ‡ç‰©ä½“å¯¼èˆªï¼ˆObjectNavï¼‰çš„åŸºå‡†æµ‹è¯•ã€‚åœ¨è¿™ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œæ™ºèƒ½ä½“åŸºäºé«˜çº§è¯­ä¹‰ç›®æ ‡è¿›è¡Œæ“ä½œï¼Œä¸å†ä¾èµ–äºåƒVLNé‚£æ ·è¯¦ç»†çš„æŒ‡å¯¼æ€§æŒ‡ä»¤ã€‚UAV-ONåŒ…å«ä½¿ç”¨é«˜è´¨é‡çš„æ¸¸æˆå¼•æ“â€”â€”Unreal Engineæ¨¡æ‹Ÿåˆ¶ä½œçš„14ä¸ªé«˜åº¦é€¼çœŸçš„ç¯å¢ƒï¼Œå…¶ä¸­åŒ…å«å¤šæ ·åŒ–çš„è¯­ä¹‰åŒºåŸŸå’Œå¤æ‚çš„ç©ºé—´å¸ƒå±€ï¼Œæ¶µç›–åŸå¸‚ã€è‡ªç„¶å’Œæ··åˆç”¨é€”åœºæ™¯ã€‚å®ƒå®šä¹‰äº†æœ‰æ ‡æ³¨çš„1270ä¸ªç›®æ ‡å¯¹è±¡ï¼Œæ¯ä¸ªå¯¹è±¡éƒ½å¯ä»¥é€šè¿‡å®ä¾‹çº§åˆ«çš„æŒ‡ä»¤è¿›è¡Œç‰¹å¾åŒ–æè¿°ï¼Œè¿™äº›æŒ‡ä»¤åŒ…å«äº†ç±»åˆ«ã€ç‰©ç†è¶³è¿¹å’Œè§†è§‰æè¿°ç¬¦ï¼Œå…è®¸åŸºäºå®é™…æƒ…å¢ƒè¿›è¡Œæ¨ç†ã€‚è¿™äº›æŒ‡ä»¤ä½œä¸ºè¯­ä¹‰ç›®æ ‡å­˜åœ¨ï¼Œä¸ºæ— äººæœºå¼•å…¥äº†ç°å®å­˜åœ¨çš„æ¨¡ç³Šæ€§å’Œå¤æ‚çš„æ¨ç†æŒ‘æˆ˜ã€‚ä¸ºäº†è¯„ä¼°è¿™ä¸ªåŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬å®æ–½äº†å‡ ç§åŸºçº¿æ–¹æ³•ï¼ŒåŒ…æ‹¬ç©ºä¸­ç›®æ ‡å¯¼èˆªä»£ç†ï¼ˆAOAï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ¨¡å—åŒ–ç­–ç•¥ï¼Œå®ƒå°†æŒ‡ä»¤è¯­ä¹‰ä¸è‡ªæˆ‘ä¸­å¿ƒè§‚å¯Ÿç›¸ç»“åˆï¼Œä»¥å®ç°é•¿æœŸã€ç›®æ ‡å¯¼å‘çš„æ¢ç´¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æœ‰åŸºçº¿æ–¹æ³•åœ¨è¿™ä¸ªç¯å¢ƒä¸‹éƒ½é¢ä¸´æŒ‘æˆ˜ï¼Œå‡¸æ˜¾äº†æ— äººæœºå¯¼èˆªå’Œè¯­ä¹‰ç›®æ ‡å®šä½çš„æŒ‘æˆ˜æ€§ã€‚UAV-ONçš„ç›®æ ‡æ˜¯æ¨è¿›åŸºäºå¤æ‚ç°å®ç¯å¢ƒä¸­çš„è¯­ä¹‰ç›®æ ‡æè¿°çš„å¯æ‰©å±•æ— äººæœºè‡ªä¸»æ€§ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.00288v3">PDF</a> Accepted to ACM MM Dataset Track 2025</p>
<p><strong>Summary</strong></p>
<p>åœ¨ä½“æ™ºèƒ½ä¸­ï¼Œç©ºä¸­å¯¼èˆªæ˜¯ä¸€é¡¹åŸºç¡€ä¸”å°šæœªå……åˆ†ç ”ç©¶çš„èƒ½åŠ›ï¼Œä½¿ä»£ç†èƒ½å¤Ÿåœ¨å¤§è§„æ¨¡ã€éç»“æ„åŒ–ç¯å¢ƒä¸­è¿ä½œï¼Œè€Œä¼ ç»Ÿçš„å¯¼èˆªèŒƒå¼åœ¨è¿™æ–¹é¢è¡¨ç°ä¸è¶³ã€‚é’ˆå¯¹æ­¤ç ”ç©¶é¢†åŸŸçš„ä¸è¶³ï¼Œæˆ‘ä»¬æ¨å‡ºæ— äººæœºç›®æ ‡å¯¼èˆªåŸºå‡†æµ‹è¯•ï¼ˆUAV-ONï¼‰ï¼Œè¯¥æµ‹è¯•æ¨¡æ‹Ÿå¤§è§„æ¨¡ç©ºä¸­ä»£ç†ç‰©ä½“ç›®æ ‡å¯¼èˆªï¼ˆObjectNavï¼‰åœ¨å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­çš„åœºæ™¯ã€‚UAV-ONåŒ…å«å…·æœ‰å¤šæ ·è¯­ä¹‰åŒºåŸŸå’Œå¤æ‚ç©ºé—´å¸ƒå±€çš„é«˜ä¿çœŸUnreal Engineç¯å¢ƒï¼Œæ¶µç›–åŸå¸‚ã€è‡ªç„¶å’Œæ··åˆç”¨é€”åœºæ™¯ã€‚å®ƒå®šä¹‰äº†æ ‡æ³¨çš„ç›®æ ‡ç‰©ä½“ï¼Œæ¯ä¸ªç‰©ä½“éƒ½æœ‰å®ä¾‹çº§åˆ«çš„æŒ‡ä»¤ï¼ŒåŒ…æ‹¬ç±»åˆ«ã€ç‰©ç†è¶³è¿¹å’Œè§†è§‰æè¿°ç¬¦ï¼Œå…è®¸åŸºäºæŒ‡ä»¤è¿›è¡Œæ¨ç†ã€‚ä¸ºäº†è¯„ä¼°åŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬å®æ–½äº†åŒ…æ‹¬ç©ºä¸­ç›®æ ‡å¯¼èˆªä»£ç†ï¼ˆAOAï¼‰åœ¨å†…çš„å‡ ç§åŸºçº¿æ–¹æ³•ã€‚ç»“æœè¡¨æ˜ï¼Œæ‰€æœ‰åŸºçº¿æ–¹æ³•åœ¨è¿™ä¸ªç¯å¢ƒä¸­éƒ½é¢ä¸´æŒ‘æˆ˜ï¼Œçªæ˜¾äº†ç©ºä¸­å¯¼èˆªå’Œè¯­ä¹‰ç›®æ ‡æ¥åœ°æŠ€æœ¯çš„å¤æ‚æ€§ã€‚UAV-ONæ—¨åœ¨æ¨åŠ¨å¤æ‚ç°å®ç¯å¢ƒä¸­åŸºäºè¯­ä¹‰ç›®æ ‡æè¿°çš„æ— äººæœºè‡ªä¸»æ€§ç ”ç©¶çš„å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç©ºä¸­å¯¼èˆªåœ¨ä½“æ™ºèƒ½ä¸­æ˜¯ä¸€ä¸ªå…³é”®ä¸”å°šæœªå®Œå…¨ç ”ç©¶çš„é¢†åŸŸã€‚ä¼ ç»Ÿçš„å¯¼èˆªæ–¹å¼åœ¨å¤§è§„æ¨¡éç»“æ„åŒ–ç¯å¢ƒä¸­éš¾ä»¥æ»¡è¶³éœ€æ±‚ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç¼ºï¼Œå¼•å…¥äº†æ— äººæœºç›®æ ‡å¯¼èˆªåŸºå‡†æµ‹è¯•ï¼ˆUAV-ONï¼‰ã€‚</li>
<li>UAV-ONåŒ…å«å¤šæ ·åŒ–çš„é«˜ä¿çœŸç¯å¢ƒæ¨¡æ‹Ÿï¼Œæ¶µç›–åŸå¸‚ã€è‡ªç„¶å’Œæ··åˆç”¨é€”åœºæ™¯ã€‚æä¾›äº†ä¸°å¯Œçš„è¯­ä¹‰åŒºåŸŸå’Œå¤æ‚ç©ºé—´å¸ƒå±€çš„æŒ‘æˆ˜ã€‚</li>
<li>æ¯ä¸ªç›®æ ‡ç‰©ä½“éƒ½æœ‰è¯¦ç»†çš„å®ä¾‹çº§åˆ«æŒ‡ä»¤ï¼ŒåŒ…æ‹¬ç±»åˆ«ã€ç‰©ç†å±æ€§å’Œè§†è§‰æè¿°ç­‰ï¼Œè¿™ä¸ºç©ºä¸­å¯¼èˆªæä¾›äº†è¯­ä¹‰ç›®æ ‡å¯¼å‘çš„æ¨ç†åŸºç¡€ã€‚è¿™å¢åŠ äº†çœŸå®ä¸–ç•Œçš„å¤æ‚æ€§å¹¶å¼•å…¥äº†è¯­ä¹‰æ¨¡ç³Šæ€§æŒ‘æˆ˜ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.00288">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a94497b0830d1b520d7eb0cc95035ca7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-20b6292f2fd919823d826d9270025daf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-665cf57b6e6180cafb3f5c8310ac5b2b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f306caeb85df17078d97caa642e71172.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="WebEvolver-Enhancing-Web-Agent-Self-Improvement-with-Coevolving-World-Model"><a href="#WebEvolver-Enhancing-Web-Agent-Self-Improvement-with-Coevolving-World-Model" class="headerlink" title="WebEvolver: Enhancing Web Agent Self-Improvement with Coevolving World   Model"></a>WebEvolver: Enhancing Web Agent Self-Improvement with Coevolving World   Model</h2><p><strong>Authors:Tianqing Fang, Hongming Zhang, Zhisong Zhang, Kaixin Ma, Wenhao Yu, Haitao Mi, Dong Yu</strong></p>
<p>Agent self-improvement, where the backbone Large Language Model (LLM) of the agent are trained on trajectories sampled autonomously based on their own policies, has emerged as a promising approach for enhancing performance. Recent advancements, particularly in web environments, face a critical limitation: their performance will reach a stagnation point during autonomous learning cycles, hindering further improvement. We argue that this stems from limited exploration of the web environment and insufficient exploitation of pre-trained web knowledge in LLMs. To improve the performance of self-improvement, we propose a novel framework that introduces a co-evolving World Model LLM. This world model predicts the next observation based on the current observation and action within the web environment. Leveraging LLMsâ€™ pretrained knowledge of abundant web content, the World Model serves dual roles: (1) as a virtual web server generating self-instructed training data to continuously refine the agentâ€™s policy, and (2) as an imagination engine during inference, enabling look-ahead simulation to guide action selection for the agent LLM. Experiments in real-world web environments (Mind2Web-Live, WebVoyager, and GAIA-web) show a 10% performance gain over existing self-evolving agents, demonstrating the efficacy and generalizability of our approach, without using any distillation from more powerful close-sourced models. Our work establishes the necessity of integrating world models into autonomous agent frameworks to unlock sustained adaptability. Code is available at <a target="_blank" rel="noopener" href="https://github.com/Tencent/SelfEvolvingAgent">https://github.com/Tencent/SelfEvolvingAgent</a> </p>
<blockquote>
<p>ä»£ç†è‡ªæˆ‘æ”¹è¿›ä½œä¸ºä¸€ç§å¢å¼ºæ€§èƒ½çš„æœ‰å‰é€”çš„æ–¹æ³•å·²ç»å´­éœ²å¤´è§’ï¼Œå…¶ä¸­ä»£ç†çš„åç«¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜¯åœ¨æ ¹æ®è‡ªä¸»æ”¿ç­–é‡‡æ ·çš„è½¨è¿¹ä¸Šè¿›è¡Œè®­ç»ƒçš„ã€‚æœ€è¿‘çš„è¿›å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨ç½‘ç»œç¯å¢ƒä¸­ï¼Œé¢ä¸´ä¸€ä¸ªå…³é”®çš„å±€é™æ€§ï¼šå®ƒä»¬åœ¨è‡ªä¸»å­¦ä¹ å‘¨æœŸä¸­çš„æ€§èƒ½ä¼šè¾¾åˆ°åœæ»ç‚¹ï¼Œé˜»ç¢äº†è¿›ä¸€æ­¥çš„æ”¹è¿›ã€‚æˆ‘ä»¬è®¤ä¸ºè¿™æºäºç½‘ç»œç¯å¢ƒçš„æœ‰é™æ¢ç´¢å’Œå¯¹LLMä¸­é¢„è®­ç»ƒç½‘ç»œçŸ¥è¯†çš„ä¸å……åˆ†åˆ©ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.21024v2">PDF</a> EMNLP 2025 Main Conference</p>
<p><strong>Summary</strong></p>
<p>åŸºäºè‡ªä¸»ç­–ç•¥çš„è½¨è¿¹é‡‡æ ·ï¼ŒAgentè‡ªæˆ‘æ”¹è¿›æ–¹æ³•å·²ç»å´­éœ²å¤´è§’ï¼Œç‰¹åˆ«æ˜¯åœ¨ç½‘ç»œç¯å¢ƒä¸­ã€‚ç„¶è€Œï¼Œå…¶æ€§èƒ½åœ¨è‡ªä¸»å­¦ä¹ å‘¨æœŸä¸­ä¼šè¾¾åˆ°åœæ»ç‚¹ã€‚æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹ååŒè¿›åŒ–ä¸–ç•Œæ¨¡å‹LLMæ¡†æ¶ï¼Œåˆ©ç”¨LLMçš„é¢„è®­ç»ƒç½‘ç»œçŸ¥è¯†ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªè§‚å¯Ÿç»“æœï¼Œä½œä¸ºè™šæ‹ŸæœåŠ¡å™¨ç”Ÿæˆè‡ªæˆ‘æŒ‡å¯¼çš„è®­ç»ƒæ•°æ®ï¼Œå¹¶ç”¨äºæŒç»­ä¼˜åŒ–agentçš„ç­–ç•¥ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨çœŸå®ç½‘ç»œç¯å¢ƒä¸­çš„æ€§èƒ½æ¯”ç°æœ‰è‡ªè¿›åŒ–agentæé«˜äº†10%ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Agentè‡ªæˆ‘æ”¹è¿›æ–¹æ³•åŸºäºè‡ªä¸»ç­–ç•¥çš„è½¨è¿¹é‡‡æ ·ï¼Œæ—¨åœ¨æé«˜æ€§èƒ½ã€‚</li>
<li>ç½‘ç»œç¯å¢ƒä¸­çš„è‡ªä¸»å­¦ä¹ æ–¹æ³•å­˜åœ¨æ€§èƒ½åœæ»é—®é¢˜ã€‚</li>
<li>æ–°å‹ååŒè¿›åŒ–ä¸–ç•Œæ¨¡å‹LLMæ¡†æ¶è¢«æå‡ºï¼Œä»¥è§£å†³æ€§èƒ½åœæ»é—®é¢˜ã€‚</li>
<li>è¯¥æ¡†æ¶åˆ©ç”¨LLMçš„é¢„è®­ç»ƒç½‘ç»œçŸ¥è¯†ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªè§‚å¯Ÿç»“æœã€‚</li>
<li>ä¸–ç•Œæ¨¡å‹ä½œä¸ºè™šæ‹ŸæœåŠ¡å™¨ç”Ÿæˆè‡ªæˆ‘æŒ‡å¯¼çš„è®­ç»ƒæ•°æ®ï¼Œä»¥ä¼˜åŒ–agentç­–ç•¥ã€‚</li>
<li>å®éªŒè¯æ˜è¯¥æ¡†æ¶åœ¨çœŸå®ç½‘ç»œç¯å¢ƒä¸­æ€§èƒ½ä¼˜è¶Šï¼Œæ¯”ç°æœ‰è‡ªè¿›åŒ–agentæé«˜10%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.21024">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ae1891a1bf6f62cb38a9c8d24916c341.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e6b7c299ad7e3aec7cdeefb2997b9e17.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="VerifiAgent-a-Unified-Verification-Agent-in-Language-Model-Reasoning"><a href="#VerifiAgent-a-Unified-Verification-Agent-in-Language-Model-Reasoning" class="headerlink" title="VerifiAgent: a Unified Verification Agent in Language Model Reasoning"></a>VerifiAgent: a Unified Verification Agent in Language Model Reasoning</h2><p><strong>Authors:Jiuzhou Han, Wray Buntine, Ehsan Shareghi</strong></p>
<p>Large language models demonstrate remarkable reasoning capabilities but often produce unreliable or incorrect responses. Existing verification methods are typically model-specific or domain-restricted, requiring significant computational resources and lacking scalability across diverse reasoning tasks. To address these limitations, we propose VerifiAgent, a unified verification agent that integrates two levels of verification: meta-verification, which assesses completeness and consistency in model responses, and tool-based adaptive verification, where VerifiAgent autonomously selects appropriate verification tools based on the reasoning type, including mathematical, logical, or commonsense reasoning. This adaptive approach ensures both efficiency and robustness across different verification scenarios. Experimental results show that VerifiAgent outperforms baseline verification methods (e.g., deductive verifier, backward verifier) among all reasoning tasks. Additionally, it can further enhance reasoning accuracy by leveraging feedback from verification results. VerifiAgent can also be effectively applied to inference scaling, achieving better results with fewer generated samples and costs compared to existing process reward models in the mathematical reasoning domain. Code is available at <a target="_blank" rel="noopener" href="https://github.com/Jiuzhouh/VerifiAgent">https://github.com/Jiuzhouh/VerifiAgent</a> </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹å±•ç°å‡ºæƒŠäººçš„æ¨ç†èƒ½åŠ›ï¼Œä½†å¸¸å¸¸äº§ç”Ÿä¸å¯é æˆ–é”™è¯¯çš„å›åº”ã€‚ç°æœ‰çš„éªŒè¯æ–¹æ³•é€šå¸¸æ˜¯æ¨¡å‹ç‰¹å®šæˆ–é¢†åŸŸé™åˆ¶çš„ï¼Œéœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºï¼Œå¹¶ä¸”åœ¨ä¸åŒçš„æ¨ç†ä»»åŠ¡ä¸­ç¼ºä¹å¯æ‰©å±•æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†VerifiAgentï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„éªŒè¯ä»£ç†ï¼Œå®ƒæ•´åˆäº†ä¸¤ä¸ªå±‚æ¬¡çš„éªŒè¯ï¼šå…ƒéªŒè¯ï¼Œè¯„ä¼°æ¨¡å‹å“åº”çš„å®Œæ•´æ€§å’Œä¸€è‡´æ€§ï¼›ä»¥åŠåŸºäºå·¥å…·çš„è‡ªé€‚åº”éªŒè¯ï¼ŒVerifiAgentæ ¹æ®æ¨ç†ç±»å‹ï¼ˆåŒ…æ‹¬æ•°å­¦ã€é€»è¾‘æˆ–å¸¸è¯†æ¨ç†ï¼‰è‡ªä¸»é€‰æ‹©åˆé€‚çš„éªŒè¯å·¥å…·ã€‚è¿™ç§è‡ªé€‚åº”æ–¹æ³•ç¡®ä¿äº†ä¸åŒéªŒè¯åœºæ™¯ä¸‹çš„æ•ˆç‡å’Œç¨³å¥æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVerifiAgentåœ¨æ‰€æœ‰æ¨ç†ä»»åŠ¡ä¸­çš„æ€§èƒ½ä¼˜äºåŸºå‡†éªŒè¯æ–¹æ³•ï¼ˆä¾‹å¦‚æ¼”ç»éªŒè¯å™¨ã€å‘åéªŒè¯å™¨ï¼‰ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜å¯ä»¥é€šè¿‡åˆ©ç”¨éªŒè¯ç»“æœçš„åé¦ˆæ¥æé«˜æ¨ç†å‡†ç¡®æ€§ã€‚VerifiAgentè¿˜å¯ä»¥æœ‰æ•ˆåœ°åº”ç”¨äºæ¨ç†ç¼©æ”¾ï¼Œä¸æ•°å­¦æ¨ç†é¢†åŸŸä¸­çš„ç°æœ‰æµç¨‹å¥–åŠ±æ¨¡å‹ç›¸æ¯”ï¼Œä½¿ç”¨æ›´å°‘çš„ç”Ÿæˆæ ·æœ¬å’Œæˆæœ¬å®ç°æ›´å¥½çš„ç»“æœã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Jiuzhouh/VerifiAgent">https://github.com/Jiuzhouh/VerifiAgent</a>è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00406v2">PDF</a> EMNLP 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹å…·å¤‡å‡ºè‰²çš„æ¨ç†èƒ½åŠ›ï¼Œä½†å¸¸äº§ç”Ÿä¸å¯é æˆ–é”™è¯¯çš„å›åº”ã€‚ç°æœ‰éªŒè¯æ–¹æ³•é€šå¸¸æ¨¡å‹ç‰¹å®šæˆ–å±€é™äºç‰¹å®šé¢†åŸŸï¼Œéœ€è¦å¤§é‡è®¡ç®—èµ„æºï¼Œä¸”åœ¨è·¨ä¸åŒæ¨ç†ä»»åŠ¡æ—¶ç¼ºä¹å¯æ‰©å±•æ€§ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºVerifiAgentç»Ÿä¸€éªŒè¯ä»£ç†ï¼Œå®ƒæ•´åˆäº†ä¸¤ç§å±‚æ¬¡çš„éªŒè¯ï¼šå…ƒéªŒè¯ï¼Œè¯„ä¼°æ¨¡å‹å›åº”çš„å®Œæ•´æ€§å’Œä¸€è‡´æ€§ï¼›ä»¥åŠåŸºäºå·¥å…·çš„è‡ªé€‚åº”éªŒè¯ï¼ŒVerifiAgentæ ¹æ®æ¨ç†ç±»å‹ï¼ˆå¦‚æ•°å­¦ã€é€»è¾‘æˆ–å¸¸è¯†æ¨ç†ï¼‰è‡ªä¸»é€‰æ‹©åˆé€‚çš„éªŒè¯å·¥å…·ã€‚è¿™ç§è‡ªé€‚åº”æ–¹æ³•ç¡®ä¿äº†ä¸åŒéªŒè¯åœºæ™¯ä¸‹çš„æ•ˆç‡å’Œç¨³å¥æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVerifiAgentåœ¨æ‰€æœ‰æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ä¼˜äºåŸºå‡†éªŒè¯æ–¹æ³•ï¼ˆå¦‚æ¼”ç»éªŒè¯å™¨ã€åå‘éªŒè¯å™¨ç­‰ï¼‰ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜èƒ½é€šè¿‡åˆ©ç”¨éªŒè¯ç»“æœåé¦ˆè¿›ä¸€æ­¥æé«˜æ¨ç†å‡†ç¡®æ€§ã€‚VerifiAgentè¿˜å¯æœ‰æ•ˆåº”ç”¨äºæ¨ç†æ‰©å±•ï¼Œä¸ç°æœ‰æ•°å­¦æ¨ç†é¢†åŸŸçš„æµç¨‹å¥–åŠ±æ¨¡å‹ç›¸æ¯”ï¼Œä½¿ç”¨æ›´å°‘çš„ç”Ÿæˆæ ·æœ¬å’Œæˆæœ¬å®ç°æ›´å¥½çš„ç»“æœã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>VerifiAgentæ˜¯ä¸€ä¸ªç»Ÿä¸€éªŒè¯ä»£ç†ï¼Œç”¨äºéªŒè¯å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„å“åº”ã€‚</li>
<li>å®ƒç»“åˆäº†å…ƒéªŒè¯å’Œå·¥å…·åŸºäºè‡ªé€‚åº”éªŒè¯ä¸¤ç§å±‚æ¬¡çš„éªŒè¯æ–¹æ³•ã€‚</li>
<li>å…ƒéªŒè¯è¯„ä¼°æ¨¡å‹å›åº”çš„å®Œæ•´æ€§å’Œä¸€è‡´æ€§ã€‚</li>
<li>åŸºäºå·¥å…·çš„è‡ªé€‚åº”éªŒè¯èƒ½æ ¹æ®æ¨ç†ç±»å‹è‡ªä¸»é€‰æ‹©éªŒè¯å·¥å…·ã€‚</li>
<li>VerifiAgentåœ¨å¤šç§æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºå…¶ä»–åŸºå‡†éªŒè¯æ–¹æ³•ã€‚</li>
<li>VerifiAgentå¯ä»¥åˆ©ç”¨éªŒè¯ç»“æœåé¦ˆæé«˜æ¨ç†å‡†ç¡®æ€§ã€‚</li>
<li>VerifiAgentå¯åº”ç”¨äºæ¨ç†æ‰©å±•ï¼Œå¹¶åœ¨æ•°å­¦æ¨ç†é¢†åŸŸå®ç°äº†æ›´å¥½çš„ç»“æœå’Œæˆæœ¬æ•ˆç›Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00406">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f2a07a8d82c9077a74ba448e6b0a6e23.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4e4d9a33af3f5c76d628d348a7ed72e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ae37539217938c8fe458ac7c586ca71.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fdf4c219ba76a86ef57a1f0c18ad9452.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-303892384c31c559331c2bcd10ffe7ab.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="CRISPR-GPT-for-Agentic-Automation-of-Gene-editing-Experiments"><a href="#CRISPR-GPT-for-Agentic-Automation-of-Gene-editing-Experiments" class="headerlink" title="CRISPR-GPT for Agentic Automation of Gene-editing Experiments"></a>CRISPR-GPT for Agentic Automation of Gene-editing Experiments</h2><p><strong>Authors:Yuanhao Qu, Kaixuan Huang, Ming Yin, Kanghong Zhan, Dyllan Liu, Di Yin, Henry C. Cousins, William A. Johnson, Xiaotong Wang, Mihir Shah, Russ B. Altman, Denny Zhou, Mengdi Wang, Le Cong</strong></p>
<p>The introduction of genome engineering technology has transformed biomedical research, making it possible to make precise changes to genetic information. However, creating an efficient gene-editing system requires a deep understanding of CRISPR technology, and the complex experimental systems under investigation. While Large Language Models (LLMs) have shown promise in various tasks, they often lack specific knowledge and struggle to accurately solve biological design problems. In this work, we introduce CRISPR-GPT, an LLM agent augmented with domain knowledge and external tools to automate and enhance the design process of CRISPR-based gene-editing experiments. CRISPR-GPT leverages the reasoning ability of LLMs to facilitate the process of selecting CRISPR systems, designing guide RNAs, recommending cellular delivery methods, drafting protocols, and designing validation experiments to confirm editing outcomes. We showcase the potential of CRISPR-GPT for assisting non-expert researchers with gene-editing experiments from scratch and validate the agentâ€™s effectiveness in a real-world use case. Furthermore, we explore the ethical and regulatory considerations associated with automated gene-editing design, highlighting the need for responsible and transparent use of these tools. Our work aims to bridge the gap between beginner biological researchers and CRISPR genome engineering techniques, and demonstrate the potential of LLM agents in facilitating complex biological discovery tasks. The published version of this draft is available at <a target="_blank" rel="noopener" href="https://www.nature.com/articles/s41551-025-01463-z">https://www.nature.com/articles/s41551-025-01463-z</a>. </p>
<blockquote>
<p>åŸºå› ç»„å·¥ç¨‹æŠ€æœ¯çš„å¼•å…¥å·²ç»æ”¹å˜äº†ç”Ÿç‰©åŒ»å­¦ç ”ç©¶ï¼Œä½¿å¾—å¯¹é—ä¼ ä¿¡æ¯è¿›è¡Œç²¾ç¡®æ”¹å˜æˆä¸ºå¯èƒ½ã€‚ç„¶è€Œï¼Œåˆ›å»ºä¸€ä¸ªé«˜æ•ˆçš„åŸºå› ç¼–è¾‘ç³»ç»Ÿéœ€è¦æ·±å…¥äº†è§£CRISPRæŠ€æœ¯ä»¥åŠå¤æ‚çš„å®éªŒç³»ç»Ÿã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ½œåŠ›ï¼Œä½†å®ƒä»¬é€šå¸¸ç¼ºä¹ç‰¹å®šçŸ¥è¯†ï¼Œåœ¨è§£å†³ç”Ÿç‰©è®¾è®¡é—®é¢˜æ—¶éš¾ä»¥åšåˆ°ç²¾ç¡®ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†CRISPR-GPTï¼Œè¿™æ˜¯ä¸€ä¸ªç»“åˆäº†é¢†åŸŸçŸ¥è¯†å’Œå¤–éƒ¨å·¥å…·çš„LLMä»£ç†ï¼Œæ—¨åœ¨è‡ªåŠ¨åŒ–å¹¶å¢å¼ºCRISPRåŸºå› ç¼–è¾‘å®éªŒçš„è®¾è®¡è¿‡ç¨‹ã€‚CRISPR-GPTåˆ©ç”¨LLMçš„æ¨ç†èƒ½åŠ›ï¼Œä¿ƒè¿›é€‰æ‹©CRISPRç³»ç»Ÿã€è®¾è®¡å¼•å¯¼RNAã€æ¨èç»†èƒä¼ é€’æ–¹æ³•ã€èµ·è‰åè®®ä»¥åŠè®¾è®¡éªŒè¯å®éªŒæ¥ç¡®è®¤ç¼–è¾‘ç»“æœçš„è¿‡ç¨‹ã€‚æˆ‘ä»¬å±•ç¤ºäº†CRISPR-GPTåœ¨éä¸“å®¶ç ”ç©¶äººå‘˜ä¸­è¿›è¡ŒåŸºå› ç¼–è¾‘å®éªŒæ–¹é¢çš„æ½œåŠ›ï¼Œå¹¶é€šè¿‡å®é™…ä½¿ç”¨æ¡ˆä¾‹éªŒè¯äº†è¯¥ä»£ç†çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ¢è®¨äº†ä¸è‡ªåŠ¨åŒ–åŸºå› ç¼–è¾‘è®¾è®¡ç›¸å…³çš„ä¼¦ç†å’Œç›‘ç®¡é—®é¢˜ï¼Œå¼ºè°ƒè¿™äº›å·¥å…·éœ€è¦è´Ÿè´£ä»»å’Œé€æ˜åœ°ä½¿ç”¨ã€‚æˆ‘ä»¬çš„å·¥ä½œæ—¨åœ¨ç¼©å°åˆçº§ç”Ÿç‰©ç ”ç©¶äººå‘˜ä¸CRISPRåŸºå› ç»„å·¥ç¨‹æŠ€æœ¯ä¹‹é—´çš„å·®è·ï¼Œå¹¶å±•ç¤ºLLMä»£ç†åœ¨ä¿ƒè¿›å¤æ‚ç”Ÿç‰©å­¦å‘ç°ä»»åŠ¡ä¸­çš„æ½œåŠ›ã€‚æœ¬æ–‡çš„å‘å¸ƒç‰ˆæœ¬å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://www.nature.com/articles/s41551-025-01463-z%E8%8E%B7%E5%BE%97%E3%80%82">https://www.nature.com/articles/s41551-025-01463-zè·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.18021v2">PDF</a> Accepted to Nature Biomedical Engineering</p>
<p><strong>Summary</strong><br>     åŸºå› ç¼–è¾‘æŠ€æœ¯å¼•å…¥ç”Ÿç‰©åŒ»å­¦ç ”ç©¶ï¼Œä¸ºç²¾å‡†æ”¹å˜é—ä¼ ä¿¡æ¯æä¾›äº†å¯èƒ½ã€‚æœ¬ç ”ç©¶æå‡ºCRISPR-GPTï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆé¢†åŸŸçŸ¥è¯†å’Œå¤–éƒ¨å·¥å…·çš„å¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†ï¼Œæ—¨åœ¨è‡ªåŠ¨åŒ–å¹¶ä¼˜åŒ–CRISPRåŸºå› ç¼–è¾‘å®éªŒè®¾è®¡è¿‡ç¨‹ã€‚CRISPR-GPTèƒ½å¤Ÿåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ååŠ©é€‰æ‹©CRISPRç³»ç»Ÿã€è®¾è®¡å¯¼å‘RNAã€æ¨èç»†èƒä¼ é€’æ–¹æ³•ã€èµ·è‰åè®®ä»¥åŠè®¾è®¡éªŒè¯å®éªŒä»¥ç¡®è®¤ç¼–è¾‘ç»“æœã€‚æ­¤æ¨¡å‹å±•ç°å‡ºåœ¨çœŸå®æ¡ˆä¾‹ä¸­ååŠ©éä¸“ä¸šç ”ç©¶äººå‘˜è¿›è¡ŒåŸºå› ç¼–è¾‘å®éªŒçš„æ½œåŠ›ï¼Œå¹¶æ¢è®¨äº†è‡ªåŠ¨åŒ–åŸºå› ç¼–è¾‘è®¾è®¡çš„ä¼¦ç†å’Œæ³•è§„è€ƒé‡ã€‚æœ¬ç ”ç©¶æ—¨åœ¨ç¼©å°åˆå­¦è€…ä¸CRISPRåŸºå› ç»„å·¥ç¨‹æŠ€æœ¯ä¹‹é—´çš„å·®è·ï¼Œå¹¶å±•ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†åœ¨ä¿ƒè¿›å¤æ‚ç”Ÿç‰©å­¦å‘ç°ä»»åŠ¡ä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŸºå› ç¼–è¾‘æŠ€æœ¯å·²æ·±åˆ»æ”¹å˜ç”Ÿç‰©åŒ»å­¦ç ”ç©¶ï¼ŒCRISPRæŠ€æœ¯æ˜¯å…¶ä¸­çš„æ ¸å¿ƒéƒ¨åˆ†ã€‚</li>
<li>CRISPR-GPTæ˜¯ä¸€ä¸ªç»“åˆäº†é¢†åŸŸçŸ¥è¯†å’Œå¤–éƒ¨å·¥å…·çš„å¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†ï¼Œæ—¨åœ¨ä¼˜åŒ–CRISPRåŸºå› ç¼–è¾‘å®éªŒçš„è®¾è®¡è¿‡ç¨‹ã€‚</li>
<li>CRISPR-GPTèƒ½ååŠ©é€‰æ‹©CRISPRç³»ç»Ÿã€è®¾è®¡å¯¼å‘RNAã€æ¨èç»†èƒä¼ é€’æ–¹æ³•ã€èµ·è‰åè®®ä»¥åŠè®¾è®¡éªŒè¯å®éªŒã€‚</li>
<li>CRISPR-GPTåœ¨éä¸“ä¸šç ”ç©¶äººå‘˜ä¸­çš„å®é™…åº”ç”¨å¾—åˆ°å±•ç¤ºï¼ŒéªŒè¯äº†å…¶åœ¨çœŸå®æ¡ˆä¾‹ä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>æ­¤ç ”ç©¶æ¢è®¨äº†è‡ªåŠ¨åŒ–åŸºå› ç¼–è¾‘è®¾è®¡çš„ä¼¦ç†å’Œæ³•è§„è€ƒé‡ã€‚</li>
<li>CRISPR-GPTæ—¨åœ¨ç¼©å°åˆå­¦è€…ä¸å¤æ‚çš„åŸºå› ç»„å·¥ç¨‹æŠ€æœ¯ä¹‹é—´çš„å·®è·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.18021">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-805fa45ff43a8920f0d29e82a9d8225b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4168d85011ac37c931c3111397507eac.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a25e251e7d93830c61bedad54ab676cb.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-23/Agent/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-23/Agent/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Agent/">
                                    <span class="chip bg-color">Agent</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-23/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-8e9808ff564f5e19ce16e0846f0654a1.jpg" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-23  Amortized In-Context Mixed Effect Transformer Models A Zero-Shot   Approach for Pharmacokinetics
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-23
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-23/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-f1ff94757caca643ac10254ed3e86f94.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-23  End-to-End Agentic RAG System Training for Traceable Diagnostic   Reasoning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-23
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">27768.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
