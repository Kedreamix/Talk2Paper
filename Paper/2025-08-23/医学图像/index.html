<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-23  Hessian-based lightweight neural network for brain vessel segmentation   on a minimal training dataset">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-85b76ff49610b77e13804b71b66ab455.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-23
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-25
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    18.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    78 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-23-æ›´æ–°"><a href="#2025-08-23-æ›´æ–°" class="headerlink" title="2025-08-23 æ›´æ–°"></a>2025-08-23 æ›´æ–°</h1><h2 id="Hessian-based-lightweight-neural-network-for-brain-vessel-segmentation-on-a-minimal-training-dataset"><a href="#Hessian-based-lightweight-neural-network-for-brain-vessel-segmentation-on-a-minimal-training-dataset" class="headerlink" title="Hessian-based lightweight neural network for brain vessel segmentation   on a minimal training dataset"></a>Hessian-based lightweight neural network for brain vessel segmentation   on a minimal training dataset</h2><p><strong>Authors:Alexandra Bernadotte, Elfimov Nikita, Mikhail Shutov, Ivan Menshikov</strong></p>
<p>Accurate segmentation of blood vessels in brain magnetic resonance angiography (MRA) is essential for successful surgical procedures, such as aneurysm repair or bypass surgery. Currently, annotation is primarily performed through manual segmentation or classical methods, such as the Frangi filter, which often lack sufficient accuracy. Neural networks have emerged as powerful tools for medical image segmentation, but their development depends on well-annotated training datasets. However, there is a notable lack of publicly available MRA datasets with detailed brain vessel annotations.   To address this gap, we propose a novel semi-supervised learning lightweight neural network with Hessian matrices on board for 3D segmentation of complex structures such as tubular structures, which we named HessNet. The solution is a Hessian-based neural network with only 6000 parameters. HessNet can run on the CPU and significantly reduces the resource requirements for training neural networks. The accuracy of vessel segmentation on a minimal training dataset reaches state-of-the-art results. It helps us create a large, semi-manually annotated brain vessel dataset of brain MRA images based on the IXI dataset (annotated 200 images). Annotation was performed by three experts under the supervision of three neurovascular surgeons after applying HessNet. It provides high accuracy of vessel segmentation and allows experts to focus only on the most complex important cases. The dataset is available at <a target="_blank" rel="noopener" href="https://git.scinalytics.com/terilat/VesselDatasetPartly">https://git.scinalytics.com/terilat/VesselDatasetPartly</a>. </p>
<blockquote>
<p>åœ¨è„‘éƒ¨ç£å…±æŒ¯è¡€ç®¡é€ å½±ï¼ˆMRAï¼‰ä¸­ï¼Œå¯¹è¡€ç®¡è¿›è¡Œç²¾ç¡®åˆ†å‰²å¯¹äºæˆåŠŸçš„æ‰‹æœ¯ç¨‹åºè‡³å…³é‡è¦ï¼Œä¾‹å¦‚åŠ¨è„‰ç˜¤ä¿®å¤æˆ–æ­æ¡¥æ‰‹æœ¯ã€‚ç›®å‰ï¼Œæ³¨é‡Šä¸»è¦é€šè¿‡æ‰‹åŠ¨åˆ†å‰²æˆ–ç»å…¸æ–¹æ³•ï¼ˆå¦‚Frangiæ»¤æ³¢å™¨ï¼‰è¿›è¡Œï¼Œä½†è¿™äº›æ–¹æ³•å¾€å¾€ç¼ºä¹è¶³å¤Ÿçš„å‡†ç¡®æ€§ã€‚ç¥ç»ç½‘ç»œå·²ä½œä¸ºåŒ»å­¦å›¾åƒåˆ†å‰²çš„å¼ºå¤§å·¥å…·å‡ºç°ï¼Œä½†å…¶å‘å±•å–å†³äºç»è¿‡è‰¯å¥½æ³¨é‡Šçš„è®­ç»ƒæ•°æ®é›†ã€‚ç„¶è€Œï¼Œå­˜åœ¨æ˜æ˜¾çš„ç¼ºä¹å¸¦æœ‰è¯¦ç»†è„‘è¡€ç®¡æ³¨é‡Šçš„å…¬å¼€MRAæ•°æ®é›†ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹åŠç›‘ç£å­¦ä¹ è½»é‡çº§ç¥ç»ç½‘ç»œï¼Œè¯¥ç½‘ç»œåœ¨æ¿ä¸Šé…å¤‡äº†HessiançŸ©é˜µï¼Œç”¨äºå¯¹ç®¡çŠ¶ç»“æ„ç­‰å¤æ‚ç»“æ„è¿›è¡Œ3Dåˆ†å‰²ï¼Œæˆ‘ä»¬å°†å…¶å‘½åä¸ºHessNetã€‚è¯¥è§£å†³æ–¹æ¡ˆæ˜¯åŸºäºHessiançš„ç¥ç»ç½‘ç»œï¼Œä»…æœ‰600relatedå‚æ•°ã€‚HessNetå¯åœ¨CPUä¸Šè¿è¡Œï¼Œæ˜¾ç€é™ä½äº†è®­ç»ƒç¥ç»ç½‘ç»œæ‰€éœ€çš„èµ„æºè¦æ±‚ã€‚åœ¨æœ€å°è®­ç»ƒæ•°æ®é›†ä¸Šçš„è¡€ç®¡åˆ†å‰²ç²¾åº¦è¾¾åˆ°äº†ä¸šç•Œé¢†å…ˆçš„ç»“æœã€‚å®ƒå¸®åŠ©æˆ‘ä»¬åŸºäºIXIæ•°æ®é›†åˆ›å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„åŠæ‰‹åŠ¨æ³¨é‡Šçš„è„‘éƒ¨è¡€ç®¡æ•°æ®é›†ï¼ˆæ³¨é‡Šäº†200å¼ å›¾åƒï¼‰ã€‚åœ¨ç¥ç»è¡€ç®¡å¤–ç§‘åŒ»ç”Ÿçš„ç›‘ç£ä¸‹ï¼Œä¸‰åä¸“å®¶åœ¨åº”ç”¨äº†HessNetä¹‹åè¿›è¡Œäº†æ³¨é‡Šã€‚å®ƒæä¾›äº†é«˜ç²¾åº¦çš„è¡€ç®¡åˆ†å‰²ï¼Œå¹¶å…è®¸ä¸“å®¶ä¸“æ³¨äºæœ€å¤æ‚ä¸”æœ€é‡è¦çš„ç—…ä¾‹ã€‚æ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://git.scinalytics.com/terilat/VesselDatasetPartly%E6%89%BE%E5%88%B0%E3%80%82">https://git.scinalytics.com/terilat/VesselDatasetPartlyæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15660v1">PDF</a> 11 pages, 2 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨è„‘ç£å…±æŒ¯è¡€ç®¡é€ å½±ï¼ˆMRAï¼‰ä¸­å‡†ç¡®åˆ†å‰²è¡€ç®¡çš„é‡è¦æ€§ï¼Œå¹¶æŒ‡å‡ºç›®å‰ä¸»è¦ä¾èµ–æ‰‹åŠ¨åˆ†å‰²æˆ–ç»å…¸æ–¹æ³•ï¼ˆå¦‚Frangiæ»¤æ³¢å™¨ï¼‰ï¼Œä½†å‡†ç¡®æ€§ä¸è¶³ã€‚ç¥ç»ç½‘ç»œåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­å…·æœ‰æ½œåŠ›ï¼Œä½†ç¼ºä¹è¯¦ç»†çš„MRAæ•°æ®é›†ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºHessiançŸ©é˜µçš„åŠç›‘ç£å­¦ä¹ è½»é‡åŒ–ç¥ç»ç½‘ç»œï¼ˆHessNetï¼‰ï¼Œç”¨äº3Då¤æ‚ç»“æ„ï¼ˆå¦‚ç®¡çŠ¶ç»“æ„ï¼‰çš„åˆ†å‰²ã€‚HessNetä»…åŒ…å«6000ä¸ªå‚æ•°ï¼Œå¯åœ¨CPUä¸Šè¿è¡Œï¼Œæ˜¾è‘—é™ä½äº†è®­ç»ƒç¥ç»ç½‘ç»œæ‰€éœ€çš„èµ„æºã€‚åœ¨å°å‹è®­ç»ƒæ•°æ®é›†ä¸Šçš„è¡€ç®¡åˆ†å‰²ç²¾åº¦è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚åˆ©ç”¨HessNetï¼ŒåŸºäºIXIæ•°æ®é›†åˆ›å»ºäº†ä¸€ä¸ªå¤§å‹åŠæ‰‹åŠ¨æ³¨é‡Šçš„è„‘MRAå›¾åƒè„‘è¡€ç®¡æ•°æ®é›†ï¼Œä¸“å®¶å¯åœ¨ç¥ç»è¡€ç®¡å¤–ç§‘åŒ»ç”Ÿçš„ç›‘ç£ä¸‹æ ‡æ³¨å›¾åƒï¼Œç¡®ä¿é«˜ç²¾ç¡®åº¦ã€‚æ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://git.scinalytics.com/terilat/VesselDatasetPartly">é“¾æ¥</a>è·å–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å‡†ç¡®çš„å¤§è„‘è¡€ç®¡åˆ†å‰²å¯¹æˆåŠŸè¿›è¡Œå¤–ç§‘æ‰‹æœ¯ï¼ˆå¦‚åŠ¨è„‰ç˜¤ä¿®å¤å’Œæ­æ¡¥æ‰‹æœ¯ï¼‰è‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰æ–¹æ³•ï¼ˆå¦‚æ‰‹åŠ¨åˆ†å‰²å’ŒFrangiæ»¤æ³¢å™¨ï¼‰åœ¨è¡€ç®¡åˆ†å‰²æ–¹é¢çš„å‡†ç¡®æ€§ä¸è¶³ã€‚</li>
<li>ç¥ç»ç½‘ç»œåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­å…·æœ‰ä¼˜åŠ¿ï¼Œä½†éœ€è¦è‰¯å¥½çš„æ³¨é‡Šè®­ç»ƒæ•°æ®é›†ã€‚</li>
<li>ç¼ºä¹å…·æœ‰è¯¦ç»†å¤§è„‘è¡€ç®¡æ³¨é‡Šçš„å…¬å¼€MRAæ•°æ®é›†ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºHessiançŸ©é˜µçš„åŠç›‘ç£å­¦ä¹ è½»é‡åŒ–ç¥ç»ç½‘ç»œï¼ˆHessNetï¼‰ï¼Œç”¨äºå¤æ‚ç»“æ„ï¼ˆå¦‚ç®¡çŠ¶ç»“æ„ï¼‰çš„3Dåˆ†å‰²ã€‚</li>
<li>HessNetå¯åœ¨CPUä¸Šè¿è¡Œï¼Œæ˜¾è‘—é™ä½è®­ç»ƒç¥ç»ç½‘ç»œæ‰€éœ€çš„èµ„æºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15660">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e8c2b256eab2d4d548cb90012f1f28c3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0e1e5dd1f04a5164ff0765d9a7822a21.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Lunar-geochemistry-from-X-ray-line-flux-ratios-using-CLASS-on-Chandrayaan-2"><a href="#Lunar-geochemistry-from-X-ray-line-flux-ratios-using-CLASS-on-Chandrayaan-2" class="headerlink" title="Lunar geochemistry from X-ray line flux ratios using CLASS on   Chandrayaan 2"></a>Lunar geochemistry from X-ray line flux ratios using CLASS on   Chandrayaan 2</h2><p><strong>Authors:R. Kumar, Y. Rai, S. Srijan, A. Bansal, Ameya V Singh, A. Kumar, H. Mhatre, M. Goyal, S. Swain, S. Patidar, Aditya P Saikia, A. Ahmad, S. Narendranath, Netra S Pillai, R. Kashyap, V. Bhalerao</strong></p>
<p>Global lunar chemical maps are essential for understanding the origin and evolution of the Moon, its surface characteristics, and its potential for resource extraction. Lunar elemental abundance maps have been derived using X-ray and gamma ray spectroscopy previously but are limited in coverage or have coarse spatial resolution. Here we used X-ray fluorescence line intensity of O, Mg, Al, Si, Ca and Fe derived from five years of data from the Chandrayaan-2 Large Area Soft X-ray Spectrometer (CLASS) to generate global O&#x2F;Si, Mg&#x2F;Si, Al&#x2F;Si, Mg&#x2F;Al, Ca&#x2F;Si and Fe&#x2F;Si line intensity ratio maps at a resolution of 5.3 km&#x2F;pixel. We have developed an independent data analysis methodology for CLASS, based on open source Python packages. Our analysis shows that the Mg&#x2F;Al map best represents the geochemical differences between the major terranes, consistent with the findings of the Apollo 15 and 16 X-ray Fluorescence Spectrometer (XRS) maps. We have also shown a good correlation of the line intensity ratios with the abundance ratios from CLASS using published elemental abundance maps. Further, we apply Gaussian mixture models to the Mg&#x2F;Si vs Al&#x2F;Si density maps to map geochemically distinct regions on the Moon that could be of interest for future investigations. </p>
<blockquote>
<p>å…¨çƒæœˆçƒåŒ–å­¦åœ°å›¾å¯¹äºäº†è§£æœˆçƒçš„èµ·æºå’Œæ¼”åŒ–ã€æœˆçƒè¡¨é¢ç‰¹å¾ä»¥åŠå…¶èµ„æºæå–æ½œåŠ›è‡³å…³é‡è¦ã€‚ä¹‹å‰å·²ç»ä½¿ç”¨Xå°„çº¿å’Œä¼½é©¬å°„çº¿å…‰è°±æ³•å¾—åˆ°äº†æœˆçƒå…ƒç´ ä¸°åº¦åœ°å›¾ï¼Œä½†å…¶è¦†ç›–èŒƒå›´æœ‰é™æˆ–ç©ºé—´åˆ†è¾¨ç‡è¾ƒä½ã€‚è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨äº†æ¥è‡ªâ€œå«¦å¨¥äºŒå·â€å¤§å‹åŒºåŸŸè½¯Xå°„çº¿å…‰è°±ä»ªï¼ˆCLASSï¼‰äº”å¹´çš„æ•°æ®ï¼Œä»ä¸­æå–äº†æ°§ã€é•ã€é“ã€ç¡…ã€é’™å’Œé“çš„Xå°„çº¿è§å…‰çº¿å¼ºåº¦ï¼Œç”Ÿæˆäº†å…¨çƒO&#x2F;Siã€Mg&#x2F;Siã€Al&#x2F;Siã€Mg&#x2F;Alã€Ca&#x2F;Siå’ŒFe&#x2F;Siçº¿å¼ºåº¦æ¯”ç‡åœ°å›¾ï¼Œåˆ†è¾¨ç‡ä¸ºæ¯åƒç´ 5.3å…¬é‡Œã€‚æˆ‘ä»¬åŸºäºå¼€æºPythonåŒ…å¼€å‘äº†ä¸€ç§ç‹¬ç«‹çš„CLASSæ•°æ®åˆ†ææ–¹æ³•ã€‚åˆ†æè¡¨æ˜ï¼ŒMg&#x2F;Alåœ°å›¾æœ€èƒ½åæ˜ ä¸»è¦åœ°å½¢çš„åœ°çƒåŒ–å­¦å·®å¼‚ï¼Œè¿™ä¸é˜¿æ³¢ç½—15å·å’Œ16å·Xå°„çº¿è§å…‰å…‰è°±ä»ªï¼ˆXRSï¼‰åœ°å›¾çš„å‘ç°ç›¸ä¸€è‡´ã€‚æˆ‘ä»¬è¿˜å±•ç¤ºäº†çº¿å¼ºåº¦æ¯”ç‡ä¸CLASSå‘å¸ƒçš„å…ƒç´ ä¸°åº¦åœ°å›¾ä¸­çš„ä¸°åº¦æ¯”ç‡ä¹‹é—´çš„è‰¯å¥½ç›¸å…³æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯¹Mg&#x2F;Siä¸Al&#x2F;Siå¯†åº¦å›¾åº”ç”¨é«˜æ–¯æ··åˆæ¨¡å‹ï¼Œåœ¨æœˆçƒä¸Šç»˜åˆ¶å‡ºåœ°çƒåŒ–å­¦ç‰¹å¾æ˜æ˜¾çš„åŒºåŸŸï¼Œè¿™äº›åŒºåŸŸå¯èƒ½å¯¹æœªæ¥çš„ç ”ç©¶æ„Ÿå…´è¶£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15563v1">PDF</a> 12 pages, 9 figures. Submitted to the Planetary Science Journal</p>
<p><strong>Summary</strong></p>
<p>å…¨çƒæœˆçƒåŒ–å­¦åœ°å›¾å¯¹äºäº†è§£æœˆçƒçš„èµ·æºå’Œæ¼”åŒ–ã€è¡¨é¢ç‰¹å¾ä»¥åŠèµ„æºæå–æ½œåŠ›è‡³å…³é‡è¦ã€‚ä»¥å¾€åˆ©ç”¨Xå°„çº¿å’Œä¼½é©¬å°„çº¿å…‰è°±æ³•å¾—åˆ°æœˆçƒå…ƒç´ ä¸°åº¦åœ°å›¾ï¼Œä½†è¦†ç›–èŒƒå›´æœ‰é™æˆ–ç©ºé—´åˆ†è¾¨ç‡è¾ƒç²—ã€‚æœ¬ç ”ç©¶åˆ©ç”¨æ¥è‡ªâ€œæœˆèˆ¹äºŒå·â€å¤§å‹åŒºåŸŸè½¯Xå°„çº¿å…‰è°±ä»ªï¼ˆCLASSï¼‰äº”å¹´çš„æ•°æ®ï¼Œç”Ÿæˆäº†å…¨çƒO&#x2F;Siã€Mg&#x2F;Siã€Al&#x2F;Siã€Mg&#x2F;Alã€Ca&#x2F;Siå’ŒFe&#x2F;Siçº¿å¼ºåº¦æ¯”ç‡åœ°å›¾ï¼Œç©ºé—´åˆ†è¾¨ç‡ä¸º5.3å…¬é‡Œ&#x2F;åƒç´ ã€‚ç ”ç©¶é‡‡ç”¨åŸºäºå¼€æºPythonåŒ…ç‹¬ç«‹æ•°æ®åˆ†ææ–¹æ³•ï¼Œå‘ç°Mg&#x2F;Alåœ°å›¾æœ€èƒ½åæ˜ ä¸»è¦åœ°å½¢çš„åœ°çƒåŒ–å­¦å·®å¼‚ï¼Œå¹¶ä¸é˜¿æ³¢ç½—15å·å’Œ16å·çš„Xå°„çº¿è§å…‰å…‰è°±ä»ªï¼ˆXRSï¼‰åœ°å›¾çš„å‘ç°ç›¸ä¸€è‡´ã€‚æ­¤å¤–ï¼Œçº¿å¼ºåº¦æ¯”ç‡ä¸CLASSå‘å¸ƒçš„å…ƒç´ ä¸°åº¦åœ°å›¾ä¹‹é—´ä¹Ÿå‘ˆç°å‡ºè‰¯å¥½çš„ç›¸å…³æ€§ã€‚ç ”ç©¶è¿˜åº”ç”¨é«˜æ–¯æ··åˆæ¨¡å‹å¯¹Mg&#x2F;Siä¸Al&#x2F;Siå¯†åº¦åœ°å›¾è¿›è¡Œåˆ†æï¼Œä»¥ç»˜åˆ¶æœˆçƒä¸Šåœ°çƒåŒ–å­¦ç‰¹å¾ä¸åŒçš„åŒºåŸŸï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›å…´è¶£ç‚¹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å…¨çƒæœˆçƒåŒ–å­¦åœ°å›¾å¯¹ç†è§£æœˆçƒç‰¹æ€§åŠèµ„æºæå–è‡³å…³é‡è¦ã€‚</li>
<li>åˆ©ç”¨æœˆèˆ¹äºŒå·CLASSæ•°æ®ç”Ÿæˆäº†é«˜åˆ†è¾¨æœˆçƒå…ƒç´ æ¯”ä¾‹åœ°å›¾ã€‚</li>
<li>ç‹¬ç«‹æ•°æ®åˆ†ææ–¹æ³•åŸºäºå¼€æºPythonåŒ…ã€‚</li>
<li>Mg&#x2F;Alåœ°å›¾åæ˜ ä¸»è¦åœ°å½¢åœ°çƒåŒ–å­¦å·®å¼‚ï¼Œä¸é˜¿æ³¢ç½—XRSåœ°å›¾ä¸€è‡´ã€‚</li>
<li>çº¿å¼ºåº¦æ¯”ç‡ä¸å…ƒç´ ä¸°åº¦åœ°å›¾é—´å­˜åœ¨è‰¯å¥½ç›¸å…³æ€§ã€‚</li>
<li>é«˜æ–¯æ··åˆæ¨¡å‹ç”¨äºåˆ†æMg&#x2F;Siä¸Al&#x2F;Siå¯†åº¦åœ°å›¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15563">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bb54fabd1165128d5a19f4279b97c80c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-173f01fd1813e1aad4b65256de2f9724.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-85b76ff49610b77e13804b71b66ab455.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ef8b56575b52e623bfd28334f55c2e73.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2de44e7cffbbb87f236550c408651ff9.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="LGMSNet-Thinning-a-medical-image-segmentation-model-via-dual-level-multiscale-fusion"><a href="#LGMSNet-Thinning-a-medical-image-segmentation-model-via-dual-level-multiscale-fusion" class="headerlink" title="LGMSNet: Thinning a medical image segmentation model via dual-level   multiscale fusion"></a>LGMSNet: Thinning a medical image segmentation model via dual-level   multiscale fusion</h2><p><strong>Authors:Chengqi Dong, Fenghe Tang, Rongge Mao, Xinpei Gao, S. Kevin Zhou</strong></p>
<p>Medical image segmentation plays a pivotal role in disease diagnosis and treatment planning, particularly in resource-constrained clinical settings where lightweight and generalizable models are urgently needed. However, existing lightweight models often compromise performance for efficiency and rarely adopt computationally expensive attention mechanisms, severely restricting their global contextual perception capabilities. Additionally, current architectures neglect the channel redundancy issue under the same convolutional kernels in medical imaging, which hinders effective feature extraction. To address these challenges, we propose LGMSNet, a novel lightweight framework based on local and global dual multiscale that achieves state-of-the-art performance with minimal computational overhead. LGMSNet employs heterogeneous intra-layer kernels to extract local high-frequency information while mitigating channel redundancy. In addition, the model integrates sparse transformer-convolutional hybrid branches to capture low-frequency global information. Extensive experiments across six public datasets demonstrate LGMSNetâ€™s superiority over existing state-of-the-art methods. In particular, LGMSNet maintains exceptional performance in zero-shot generalization tests on four unseen datasets, underscoring its potential for real-world deployment in resource-limited medical scenarios. The whole project code is in <a target="_blank" rel="noopener" href="https://github.com/cq-dong/LGMSNet">https://github.com/cq-dong/LGMSNet</a>. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²åœ¨ç–¾ç—…è¯Šæ–­å’Œæ²»ç–—è®¡åˆ’åˆ¶å®šä¸­å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨èµ„æºå—é™çš„ä¸´åºŠç¯å¢ƒä¸­ï¼Œæ€¥éœ€è½»ä¾¿ä¸”é€šç”¨çš„æ¨¡å‹ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è½»ä¾¿æ¨¡å‹å¾€å¾€ä¸ºäº†åœ¨æ•ˆç‡ä¸Šåšå‡ºå¦¥åè€Œç‰ºç‰²äº†æ€§èƒ½ï¼Œå¹¶ä¸”å¾ˆå°‘é‡‡ç”¨è®¡ç®—æ˜‚è´µçš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œè¿™ä¸¥é‡é™åˆ¶äº†å®ƒä»¬çš„å…¨å±€ä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œå½“å‰æ¶æ„å¿½ç•¥äº†åŒ»å­¦å½±åƒä¸­åŒä¸€å·ç§¯æ ¸ä¸‹çš„é€šé“å†—ä½™é—®é¢˜ï¼Œè¿™é˜»ç¢äº†æœ‰æ•ˆçš„ç‰¹å¾æå–ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†LGMSNetï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå±€éƒ¨å’Œå…¨å±€åŒé‡å¤šå°ºåº¦çš„æ–°å‹è½»ä¾¿æ¡†æ¶ï¼Œä»¥æœ€å°çš„è®¡ç®—å¼€é”€å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚LGMSNeté‡‡ç”¨å¼‚è´¨å†…å±‚æ ¸æ¥æå–å±€éƒ¨é«˜é¢‘ä¿¡æ¯ï¼ŒåŒæ—¶å‡è½»é€šé“å†—ä½™ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹ç»“åˆäº†ç¨€ç–çš„transformerå·ç§¯æ··åˆåˆ†æ”¯æ¥æ•è·ä½é¢‘å…¨å±€ä¿¡æ¯ã€‚åœ¨å…­ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒLGMSNetåœ¨ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚ç‰¹åˆ«æ˜¯åœ¨å››ä¸ªæœªè§æ•°æ®é›†ä¸Šçš„é›¶æ ·æœ¬æ³›åŒ–æµ‹è¯•ä¸­ï¼ŒLGMSNetä¿æŒäº†å‡ºè‰²çš„æ€§èƒ½ï¼Œçªæ˜¾å…¶åœ¨èµ„æºå—é™çš„åŒ»å­¦åœºæ™¯ä¸­è¿›è¡Œå®é™…éƒ¨ç½²çš„æ½œåŠ›ã€‚æ•´ä¸ªé¡¹ç›®ä»£ç ä½äº<a target="_blank" rel="noopener" href="https://github.com/cq-dong/LGMSNet%E3%80%82">https://github.com/cq-dong/LGMSNetã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15476v1">PDF</a> Accepted by ECAI 2025</p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒåˆ†å‰²åœ¨ç–¾ç—…è¯Šæ–­å’Œæ²»ç–—è®¡åˆ’ä¸­å…·æœ‰è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨èµ„æºå—é™çš„ä¸´åºŠç¯å¢ƒä¸­ã€‚ç°æœ‰è½»é‡åŒ–æ¨¡å‹é€šå¸¸éœ€è¦åœ¨æ€§èƒ½å’Œæ•ˆç‡ä¹‹é—´åšå‡ºå¦¥åï¼Œå¹¶ä¸”å¾ˆå°‘é‡‡ç”¨è®¡ç®—é‡è¾ƒå¤§çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œè¿™ä¸¥é‡é™åˆ¶äº†å…¶å…¨å±€ä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†LGMSNetï¼Œä¸€ç§åŸºäºå±€éƒ¨å’Œå…¨å±€åŒå¤šå°ºåº¦çš„è½»é‡åŒ–æ¡†æ¶ï¼Œä»¥æœ€å°çš„è®¡ç®—å¼€é”€å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚LGMSNeté‡‡ç”¨å¼‚è´¨å†…æ ¸æ¥æå–å±€éƒ¨é«˜é¢‘ä¿¡æ¯å¹¶å‡è½»é€šé“å†—ä½™é—®é¢˜ï¼ŒåŒæ—¶ç»“åˆç¨€ç–transformer-å·ç§¯æ··åˆåˆ†æ”¯æ•è·ä½é¢‘å…¨å±€ä¿¡æ¯ã€‚åœ¨å…­ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒLGMSNetåœ¨æœªè§æ•°æ®é›†ä¸Šçš„é›¶æ ·æœ¬æ³›åŒ–æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œçªæ˜¾å…¶åœ¨èµ„æºå—é™åŒ»å­¦åœºæ™¯ä¸­çš„å®é™…åº”ç”¨æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²åœ¨ç–¾ç—…è¯Šæ–­å’Œæ²»ç–—è®¡åˆ’ä¸­èµ·é‡è¦ä½œç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨èµ„æºå—é™ç¯å¢ƒä¸­ã€‚</li>
<li>ç°æœ‰è½»é‡åŒ–æ¨¡å‹éœ€è¦åœ¨æ€§èƒ½å’Œæ•ˆç‡ä¹‹é—´è¿›è¡Œæƒè¡¡ï¼Œä¸”ç¼ºä¹å…¨å±€ä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›ã€‚</li>
<li>LGMSNetæ˜¯ä¸€ç§åŸºäºå±€éƒ¨å’Œå…¨å±€åŒå¤šå°ºåº¦çš„è½»é‡åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>LGMSNeté‡‡ç”¨å¼‚è´¨å†…æ ¸æå–å±€éƒ¨é«˜é¢‘ä¿¡æ¯å¹¶å‡è½»é€šé“å†—ä½™ã€‚</li>
<li>æ¨¡å‹ç»“åˆç¨€ç–transformer-å·ç§¯æ··åˆåˆ†æ”¯æ•è·ä½é¢‘å…¨å±€ä¿¡æ¯ã€‚</li>
<li>åœ¨å¤šä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜LGMSNetå…·æœ‰å“è¶Šæ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨æœªè§æ•°æ®é›†çš„é›¶æ ·æœ¬æ³›åŒ–æµ‹è¯•ä¸­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15476">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-25ca82867e3f3e7d85c4420295d1fb61.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c3e78a3dcf8ee39b96c06605a07db111.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2e39bb91ad713f51e00f1d509c0cb3f2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d8b22167f8f0d9e9ba6db671e523fc33.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bb6d35aed56fc790b27b8c51734706bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6bd6a2e0bf94011bed177eced4cbb96d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d15c61b0f85afeaef97f519ae4eb66d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50ee63021da9ab7e1ce24f4eae03c08a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b35f2010b7575195fd3b9cd1c275d433.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Bridging-Generalization-and-Personalization-in-Wearable-Human-Activity-Recognition-via-On-Device-Few-Shot-Learning"><a href="#Bridging-Generalization-and-Personalization-in-Wearable-Human-Activity-Recognition-via-On-Device-Few-Shot-Learning" class="headerlink" title="Bridging Generalization and Personalization in Wearable Human Activity   Recognition via On-Device Few-Shot Learning"></a>Bridging Generalization and Personalization in Wearable Human Activity   Recognition via On-Device Few-Shot Learning</h2><p><strong>Authors:Pixi Kang, Julian Moosmann, Mengxi Liu, Bo Zhou, Michele Magno, Paul Lukowicz, Sizhen Bian</strong></p>
<p>Human Activity Recognition (HAR) using wearable devices has advanced significantly in recent years, yet its generalization remains limited when models are deployed to new users. This degradation in performance is primarily due to user-induced concept drift (UICD), highlighting the importance of efficient personalization. In this paper, we present a hybrid framework that first generalizes across users and then rapidly adapts to individual users using few-shot learning directly on-device. By updating only the classifier layer with user-specific data, our method achieves robust personalization with minimal computational and memory overhead. We implement this framework on the energy-efficient RISC-V-based GAP9 microcontroller and validate it across three diverse HAR scenarios: RecGym, QVAR-Gesture, and Ultrasound-Gesture. Post-deployment adaptation yields consistent accuracy improvements of 3.73%, 17.38%, and 3.70% respectively. These results confirm that fast, lightweight, and effective personalization is feasible on embedded platforms, paving the way for scalable and user-aware HAR systems in the wild \footnote{<a target="_blank" rel="noopener" href="https://github.com/kangpx/onlineTiny2023%7D">https://github.com/kangpx/onlineTiny2023}</a>. </p>
<blockquote>
<p>ä½¿ç”¨å¯ç©¿æˆ´è®¾å¤‡è¿›è¡Œäººç±»æ´»åŠ¨è¯†åˆ«ï¼ˆHARï¼‰è¿‘å¹´æ¥å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å½“æ¨¡å‹éƒ¨ç½²ç»™æ–°ç”¨æˆ·æ—¶ï¼Œå…¶é€šç”¨æ€§ä»ç„¶æœ‰é™ã€‚è¿™ç§æ€§èƒ½ä¸‹é™ä¸»è¦æ˜¯ç”±äºç”¨æˆ·å¼•èµ·çš„æ¦‚å¿µæ¼‚ç§»ï¼ˆUICDï¼‰æ‰€å¯¼è‡´çš„ï¼Œè¿™å¼ºè°ƒäº†æœ‰æ•ˆä¸ªæ€§åŒ–æ–¹æ³•çš„é‡è¦æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ··åˆæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é¦–å…ˆå®ç°è·¨ç”¨æˆ·é€šç”¨åŒ–ï¼Œç„¶åä½¿ç”¨å°‘é‡æ ·æœ¬å­¦ä¹ ç›´æ¥åœ¨è®¾å¤‡ä¸Šå¿«é€Ÿé€‚åº”ä¸ªåˆ«ç”¨æˆ·ã€‚é€šè¿‡ä»…ä½¿ç”¨ç”¨æˆ·ç‰¹å®šæ•°æ®æ›´æ–°åˆ†ç±»å™¨å±‚ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥åœ¨è®¡ç®—é‡å’Œå†…å­˜å¼€é”€æå°çš„æƒ…å†µä¸‹å®ç°ç¨³å¥çš„ä¸ªæ€§åŒ–ã€‚æˆ‘ä»¬åœ¨èƒ½æ•ˆé«˜çš„RISC-VåŸºGAP9å¾®æ§åˆ¶å™¨ä¸Šå®ç°äº†è¯¥æ¡†æ¶ï¼Œå¹¶åœ¨ä¸‰ç§ä¸åŒçš„HARåœºæ™¯ä¸­è¿›è¡Œäº†éªŒè¯ï¼šRecGymã€QVARæ‰‹åŠ¿å’Œè¶…å£°æ‰‹åŠ¿ã€‚éƒ¨ç½²åçš„é€‚åº”åˆ†åˆ«äº§ç”Ÿäº†ç¨³å®šçš„å‡†ç¡®æ€§æé«˜3.73%ã€æé«˜é«˜è¾¾ä¸ªäººæœ¬èº«çš„åˆæ¬¡æµ‹å‡†ç¡®åº¦ä¹‹å‰ä¸Šå‡èƒ½è¿›æ­¥ä¸‰ç‚¹çš„è¿™ç§åŸºæœ¬æ›¿ä»£ä¸åŒåŒ–å­¦è´µæ ¼çš„æ¼”ç»ƒä¸å®é™…ç¨æœ‰æŸå¤±ä¸ä¸€æ ·å ååˆ†é‡è¦çš„ç ”ç©¶æˆæœè®¤å¯è¿™è¡¨æ˜åµŒå…¥å¼å¹³å°ä¸Šå¿«é€Ÿã€è½»ä¾¿ã€æœ‰æ•ˆçš„ä¸ªæ€§åŒ–æ–¹æ³•æ˜¯å¯è¡Œçš„ï¼Œä¸ºé‡å¤–å¯æ‰©å±•å’Œç”¨æˆ·æ„ŸçŸ¥çš„HARç³»ç»Ÿé“ºå¹³äº†é“è·¯ã€‚å¦‚éœ€æ›´å¤šä¿¡æ¯ï¼Œè¯·è®¿é—®æˆ‘ä»¬çš„GitHubé¡µé¢ï¼š[<a target="_blank" rel="noopener" href="https://github.com/kangpx/onlineTiny2023]%E3%80%82">https://github.com/kangpx/onlineTiny2023]ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15413v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ··åˆæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é¦–å…ˆå®ç°è·¨ç”¨æˆ·æ³›åŒ–ï¼Œç„¶åé€šè¿‡å°‘é‡æ ·æœ¬å­¦ä¹ å¿«é€Ÿé€‚åº”ä¸ªä½“ç”¨æˆ·ã€‚é€šè¿‡ä»…æ›´æ–°åˆ†ç±»å™¨å±‚ä»¥åŒ…å«ç”¨æˆ·ç‰¹å®šæ•°æ®ï¼Œè¯¥æ–¹æ³•å®ç°äº†å…·æœ‰æœ€å°è®¡ç®—å’Œå†…å­˜å¼€é”€çš„ç¨³å¥ä¸ªæ€§åŒ–ã€‚åœ¨RISC-Væ¶æ„çš„GAP9å¾®æ§åˆ¶å™¨ä¸Šå®ç°äº†è¯¥æ¡†æ¶ï¼Œå¹¶åœ¨ä¸‰ç§ä¸åŒçš„HARåœºæ™¯ä¸­è¿›è¡Œäº†éªŒè¯ï¼Œæ˜¾ç¤ºå‡ºæœ‰æ•ˆæ€§å’Œå¯é æ€§ã€‚è¯¥æ–¹æ³•åœ¨éƒ¨ç½²åå¿«é€Ÿé€‚åº”ä¸åŒåœºæ™¯çš„èƒ½åŠ›ä½¿HARç³»ç»Ÿçš„æ³›åŒ–èƒ½åŠ›å¾—åˆ°æé«˜ã€‚å› æ­¤ï¼Œå®ç°åœ¨åµŒå…¥å¼å¹³å°ä¸Šçš„å¿«é€Ÿã€è½»é‡çº§ä¸”æœ‰æ•ˆçš„ä¸ªæ€§åŒ–æ˜¯å¯è¡Œçš„ã€‚å…·ä½“éªŒè¯æ–¹æ³•å¯ä»¥å‚è€ƒè¯¥è®ºæ–‡çš„GitHubé¡µé¢^<a target="_blank" rel="noopener" href="https://github.com/kangpx/onlineTiny2023">é“¾æ¥ä¸ºé¢å¤–æ³¨é‡Šï¼Œå·²è·³è¿‡å…¶å†…å®¹</a>ã€‚è¯¥å·¥ä½œä¸ºæœªæ¥åœ¨çœŸå®ä¸–ç•Œç¯å¢ƒä¸‹æ„å»ºå¯æ‰©å±•å’Œç”¨æˆ·æ„ŸçŸ¥çš„HARç³»ç»Ÿé“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯è¯¥æ–‡æœ¬çš„å…³é”®è¦ç‚¹ï¼š</p>
<ul>
<li>æå‡ºäº†ä¸€ç§æ··åˆæ¡†æ¶ï¼Œèƒ½å¤Ÿè·¨ç”¨æˆ·æ³›åŒ–å¹¶å¿«é€Ÿé€‚åº”ä¸ªä½“ç”¨æˆ·ã€‚</li>
<li>é€šè¿‡ä»…æ›´æ–°åˆ†ç±»å™¨å±‚å®ç°ç¨³å¥ä¸ªæ€§åŒ–ï¼Œå‡å°‘è®¡ç®—å’Œå†…å­˜å¼€é”€ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15413">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-831cd3ded6fa5ac005a3f4883bb7ad54.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-16e24959a44ec0159ec473a1c85cc2ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-739a8d3ea27b95505ff4dc3a8622afee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a176505d6f8c28f9d1f5cb6b88977706.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Bladder-Cancer-Diagnosis-with-Deep-Learning-A-Multi-Task-Framework-and-Online-Platform"><a href="#Bladder-Cancer-Diagnosis-with-Deep-Learning-A-Multi-Task-Framework-and-Online-Platform" class="headerlink" title="Bladder Cancer Diagnosis with Deep Learning: A Multi-Task Framework and   Online Platform"></a>Bladder Cancer Diagnosis with Deep Learning: A Multi-Task Framework and   Online Platform</h2><p><strong>Authors:Jinliang Yu, Mingduo Xie, Yue Wang, Tianfan Fu, Xianglai Xu, Jiajun Wang</strong></p>
<p>Clinical cystoscopy, the current standard for bladder cancer diagnosis, suffers from significant reliance on physician expertise, leading to variability and subjectivity in diagnostic outcomes. There is an urgent need for objective, accurate, and efficient computational approaches to improve bladder cancer diagnostics.   Leveraging recent advancements in deep learning, this study proposes an integrated multi-task deep learning framework specifically designed for bladder cancer diagnosis from cystoscopic images. Our framework includes a robust classification model using EfficientNet-B0 enhanced with Convolutional Block Attention Module (CBAM), an advanced segmentation model based on ResNet34-UNet++ architecture with self-attention mechanisms and attention gating, and molecular subtyping using ConvNeXt-Tiny to classify molecular markers such as HER-2 and Ki-67. Additionally, we introduce a Gradio-based online diagnostic platform integrating all developed models, providing intuitive features including multi-format image uploads, bilingual interfaces, and dynamic threshold adjustments.   Extensive experimentation demonstrates the effectiveness of our methods, achieving outstanding accuracy (93.28%), F1-score (82.05%), and AUC (96.41%) for classification tasks, and exceptional segmentation performance indicated by a Dice coefficient of 0.9091. The online platform significantly improved the accuracy, efficiency, and accessibility of clinical bladder cancer diagnostics, enabling practical and user-friendly deployment. The code is publicly available.   Our multi-task framework and integrated online tool collectively advance the field of intelligent bladder cancer diagnosis by improving clinical reliability, supporting early tumor detection, and enabling real-time diagnostic feedback. These contributions mark a significant step toward AI-assisted decision-making in urology. </p>
<blockquote>
<p>ä¸´åºŠè†€èƒ±é•œæ£€æŸ¥æ˜¯ç›®å‰è†€èƒ±ç™Œè¯Šæ–­çš„æ ‡å‡†ï¼Œä½†å…¶å¯¹åŒ»ç”Ÿä¸“ä¸šçŸ¥è¯†çš„ä¾èµ–æ€§å¼ºï¼Œå¯¼è‡´è¯Šæ–­ç»“æœå­˜åœ¨å˜å¼‚æ€§å’Œä¸»è§‚æ€§ã€‚å› æ­¤ï¼Œæ€¥éœ€å®¢è§‚ã€å‡†ç¡®ã€é«˜æ•ˆçš„è®¡ç®—æ–¹æ³•æ¥æ”¹è¿›è†€èƒ±ç™Œçš„è¯Šæ–­ã€‚æœ¬ç ”ç©¶åˆ©ç”¨æ·±åº¦å­¦ä¹ é¢†åŸŸçš„æœ€æ–°è¿›å±•ï¼Œæå‡ºäº†ä¸€ä¸ªä¸“ä¸ºè†€èƒ±ç™Œè¯Šæ–­è®¾è®¡çš„å¤šä»»åŠ¡æ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸€ä¸ªä½¿ç”¨EfficientNet-B0å¹¶ç»“åˆå·ç§¯å—æ³¨æ„åŠ›æ¨¡å—ï¼ˆCBAMï¼‰çš„ç¨³å¥åˆ†ç±»æ¨¡å‹ï¼Œä¸€ä¸ªåŸºäºResNet34-UNet++æ¶æ„å¹¶ä½¿ç”¨è‡ªæ³¨æ„æœºåˆ¶å’Œæ³¨æ„åŠ›é—¨çš„é«˜çº§åˆ†å‰²æ¨¡å‹ï¼Œä»¥åŠä½¿ç”¨ConvNeXt-Tinyè¿›è¡Œåˆ†å­äºšå‹çš„åˆ†ç±»ï¼Œå¦‚HER-2å’ŒKi-67ç­‰åˆ†å­æ ‡è®°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªåŸºäºGradioçš„åœ¨çº¿è¯Šæ–­å¹³å°ï¼Œè¯¥å¹³å°é›†æˆäº†æ‰€æœ‰å·²å¼€å‘çš„æ¨¡å‹ï¼Œæä¾›äº†å¤šæ ¼å¼å›¾åƒä¸Šä¼ ã€åŒè¯­ç•Œé¢å’ŒåŠ¨æ€é˜ˆå€¼è°ƒæ•´ç­‰ç›´è§‚åŠŸèƒ½ã€‚å¹¿æ³›çš„å®éªŒè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œåœ¨åˆ†ç±»ä»»åŠ¡ä¸­å–å¾—äº†å‡ºè‰²çš„å‡†ç¡®ç‡ï¼ˆ93.28%ï¼‰ã€F1åˆ†æ•°ï¼ˆ82.05%ï¼‰å’ŒAUCï¼ˆ96.41%ï¼‰ï¼Œåˆ†å‰²æ€§èƒ½ä¹Ÿååˆ†çªå‡ºï¼ŒDiceç³»æ•°ä¸º0.9091ã€‚åœ¨çº¿å¹³å°æ˜¾è‘—æé«˜äº†è†€èƒ±ç™Œè¯Šæ–­çš„å‡†ç¡®æ€§ã€æ•ˆç‡å’Œå¯åŠæ€§ï¼Œå®ç°äº†å®ç”¨ä¸”ç”¨æˆ·å‹å¥½çš„éƒ¨ç½²ã€‚ç›¸å…³ä»£ç å·²å…¬å¼€å¯ç”¨ã€‚æˆ‘ä»¬çš„å¤šä»»åŠ¡æ¡†æ¶å’Œé›†æˆåœ¨çº¿å·¥å…·å…±åŒæ¨åŠ¨äº†æ™ºèƒ½è†€èƒ±ç™Œè¯Šæ–­é¢†åŸŸçš„å‘å±•ï¼Œæé«˜äº†ä¸´åºŠå¯é æ€§ï¼Œæ”¯æŒæ—©æœŸè‚¿ç˜¤æ£€æµ‹ï¼Œå¹¶èƒ½å¤Ÿå®ç°å®æ—¶è¯Šæ–­åé¦ˆã€‚è¿™äº›è´¡çŒ®æ ‡å¿—ç€åœ¨æ³Œå°¿ç§‘é¢†åŸŸå®ç°äººå·¥æ™ºèƒ½è¾…åŠ©å†³ç­–çš„é‡å¤§è¿›æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15379v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æœ¬ä»‹ç»äº†é’ˆå¯¹è†€èƒ±ç™Œè¯Šæ–­çš„æ™ºèƒ½å¤šä»»åŠ¡æ·±åº¦å­¦ä¹ æ¡†æ¶ä»¥åŠåŸºäºè¯¥æ¡†æ¶çš„åœ¨çº¿è¯Šæ–­å¹³å°ã€‚è¯¥æ¡†æ¶åˆ©ç”¨æ·±åº¦å­¦ä¹ æŠ€æœ¯ï¼Œé€šè¿‡é›†æˆåˆ†ç±»æ¨¡å‹ã€åˆ†å‰²æ¨¡å‹å’Œåˆ†å­åˆ†å‹æŠ€æœ¯ï¼Œå®ç°äº†è†€èƒ±ç™Œè¯Šæ–­çš„å®¢è§‚åŒ–ã€ç²¾ç¡®æ€§å’Œé«˜æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œå¼€å‘äº†ä¸€ç§åŸºäºGradioçš„åœ¨çº¿è¯Šæ–­å¹³å°ï¼Œæé«˜äº†è¯Šæ–­çš„å‡†ç¡®æ€§ã€æ•ˆç‡å’Œå¯åŠæ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè¯¥æ–¹æ³•å…·æœ‰è‰¯å¥½çš„æ€§èƒ½è¡¨ç°ï¼Œå¹¶å…¬å¼€äº†ç›¸å…³ä»£ç ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¸´åºŠè†€èƒ±ç™Œè¯Šæ–­ç°çŠ¶å­˜åœ¨ä¾èµ–åŒ»ç”Ÿç»éªŒå’Œä¸»è§‚åˆ¤æ–­çš„é—®é¢˜ï¼Œéœ€è¦å®¢è§‚ã€å‡†ç¡®å’Œé«˜æ•ˆçš„è®¡ç®—æ–¹æ³•æ¥æ”¹è¿›ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„å¤šä»»åŠ¡æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºè†€èƒ±ç™Œè¯Šæ–­ã€‚åŒ…æ‹¬ç”¨äºåˆ†ç±»çš„EfficientNet-B0ä¸CBAMç»“åˆæ¨¡å‹ã€ç”¨äºåˆ†å‰²çš„ResNet34-UNet++æ¶æ„ä»¥åŠç”¨äºåˆ†å­åˆ†å‹çš„ConvNeXt-Tinyæ¨¡å‹ã€‚</li>
<li>å¼€å‘äº†åŸºäºGradioçš„åœ¨çº¿è¯Šæ–­å¹³å°ï¼Œé›†æˆäº†æ‰€æœ‰å¼€å‘çš„æ¨¡å‹ï¼Œå¹¶æä¾›äº†å¤šæ ¼å¼å›¾åƒä¸Šä¼ ã€åŒè¯­ç•Œé¢å’ŒåŠ¨æ€é˜ˆå€¼è°ƒæ•´ç­‰ç›´è§‚åŠŸèƒ½ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åˆ†ç±»ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè¾ƒé«˜çš„å‡†ç¡®æ€§ã€F1åˆ†æ•°å’ŒAUCå€¼ï¼Œåœ¨åˆ†å‰²ä»»åŠ¡ä¸Šä¹Ÿå–å¾—äº†ä¼˜å¼‚çš„Diceç³»æ•°ã€‚</li>
<li>åœ¨çº¿å¹³å°æ˜¾è‘—æé«˜äº†è†€èƒ±ç™Œè¯Šæ–­çš„å‡†ç¡®æ€§ã€æ•ˆç‡å’Œå¯åŠæ€§ï¼Œå®ç°äº†å®ç”¨ä¸”ç”¨æˆ·å‹å¥½çš„éƒ¨ç½²ã€‚</li>
<li>ä»£ç å·²å…¬å¼€ï¼Œä¸ºæ™ºèƒ½è†€èƒ±ç™Œè¯Šæ–­é¢†åŸŸçš„ç ”ç©¶å’Œåº”ç”¨æä¾›äº†æœ‰ä»·å€¼çš„èµ„æºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15379">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-531dd75dff3a709e286b18b268a1fb6a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-368c7238fcb7cd8031b8c41bcbf8d7a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bc276dde7519e823897569b5d534f2be.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="TPA-Temporal-Prompt-Alignment-for-Fetal-Congenital-Heart-Defect-Classification"><a href="#TPA-Temporal-Prompt-Alignment-for-Fetal-Congenital-Heart-Defect-Classification" class="headerlink" title="TPA: Temporal Prompt Alignment for Fetal Congenital Heart Defect   Classification"></a>TPA: Temporal Prompt Alignment for Fetal Congenital Heart Defect   Classification</h2><p><strong>Authors:Darya Taratynova, Alya Almsouti, Beknur Kalmakhanbet, Numan Saeed, Mohammad Yaqub</strong></p>
<p>Congenital heart defect (CHD) detection in ultrasound videos is hindered by image noise and probe positioning variability. While automated methods can reduce operator dependence, current machine learning approaches often neglect temporal information, limit themselves to binary classification, and do not account for prediction calibration. We propose Temporal Prompt Alignment (TPA), a method leveraging foundation image-text model and prompt-aware contrastive learning to classify fetal CHD on cardiac ultrasound videos. TPA extracts features from each frame of video subclips using an image encoder, aggregates them with a trainable temporal extractor to capture heart motion, and aligns the video representation with class-specific text prompts via a margin-hinge contrastive loss. To enhance calibration for clinical reliability, we introduce a Conditional Variational Autoencoder Style Modulation (CVAESM) module, which learns a latent style vector to modulate embeddings and quantifies classification uncertainty. Evaluated on a private dataset for CHD detection and on a large public dataset, EchoNet-Dynamic, for systolic dysfunction, TPA achieves state-of-the-art macro F1 scores of 85.40% for CHD diagnosis, while also reducing expected calibration error by 5.38% and adaptive ECE by 6.8%. On EchoNet-Dynamicâ€™s three-class task, it boosts macro F1 by 4.73% (from 53.89% to 58.62%). Temporal Prompt Alignment (TPA) is a framework for fetal congenital heart defect (CHD) classification in ultrasound videos that integrates temporal modeling, prompt-aware contrastive learning, and uncertainty quantification. </p>
<blockquote>
<p>å…ˆå¤©æ€§å¿ƒè„ç¼ºé™·ï¼ˆCHDï¼‰çš„è¶…å£°è§†é¢‘æ£€æµ‹å—åˆ°å›¾åƒå™ªå£°å’Œæ¢å¤´å®šä½å¯å˜æ€§çš„é˜»ç¢ã€‚è™½ç„¶è‡ªåŠ¨åŒ–æ–¹æ³•å¯ä»¥å‡å°‘å¯¹æ“ä½œå‘˜çš„ä¾èµ–ï¼Œä½†å½“å‰çš„æœºå™¨å­¦ä¹ æ–¹æ³•å¸¸å¸¸å¿½ç•¥äº†æ—¶é—´ä¿¡æ¯ï¼Œä»…é™äºäºŒå…ƒåˆ†ç±»ï¼Œå¹¶ä¸”æ²¡æœ‰è€ƒè™‘åˆ°é¢„æµ‹æ ¡å‡†ã€‚æˆ‘ä»¬æå‡ºäº†Temporal Prompt Alignmentï¼ˆTPAï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨åŸºç¡€å›¾åƒæ–‡æœ¬æ¨¡å‹å’Œæç¤ºæ„ŸçŸ¥å¯¹æ¯”å­¦ä¹ æ¥å¯¹èƒå„¿CHDè¿›è¡Œå¿ƒè„è¶…å£°è§†é¢‘åˆ†ç±»ã€‚TPAé€šè¿‡å›¾åƒç¼–ç å™¨ä»è§†é¢‘çš„æ¯ä¸ªå¸§ä¸­æå–ç‰¹å¾ï¼Œå¹¶ä½¿ç”¨å¯è®­ç»ƒçš„æ—¶é—´æå–å™¨æ¥æ•æ‰å¿ƒè„è¿åŠ¨ï¼Œå¹¶é€šè¿‡è¾¹è·é“°é“¾å¯¹æ¯”æŸå¤±å°†è§†é¢‘è¡¨ç¤ºä¸ç‰¹å®šç±»åˆ«çš„æ–‡æœ¬æç¤ºå¯¹é½ã€‚ä¸ºäº†æé«˜ä¸´åºŠå¯é æ€§çš„æ ¡å‡†ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ¡ä»¶å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨é£æ ¼è°ƒåˆ¶ï¼ˆCVAESMï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—å­¦ä¹ æ½œåœ¨çš„é£æ ¼å‘é‡æ¥è°ƒåˆ¶åµŒå…¥å¹¶é‡åŒ–åˆ†ç±»çš„ä¸ç¡®å®šæ€§ã€‚åœ¨ç”¨äºCHDæ£€æµ‹çš„ç§æœ‰æ•°æ®é›†å’Œç”¨äºæ”¶ç¼©åŠŸèƒ½éšœç¢çš„å¤§å‹å…¬å…±æ•°æ®é›†EchoNet-Dynamicä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼ŒTPAè¾¾åˆ°äº†æœ€å…ˆè¿›çš„å®è§‚F1åˆ†æ•°ï¼ŒCHDè¯Šæ–­çš„F1åˆ†æ•°ä¸º85.40%ï¼ŒåŒæ—¶é™ä½äº†æœŸæœ›æ ¡å‡†è¯¯å·®5.38%å’Œè‡ªé€‚åº”ECE 6.8%ã€‚åœ¨EchoNet-Dynamicçš„ä¸‰ç±»ä»»åŠ¡ä¸­ï¼Œå®ƒæé«˜äº†å®è§‚F1åˆ†æ•°4.73%ï¼ˆä»53.89%åˆ°58.62%ï¼‰ã€‚Temporal Prompt Alignmentï¼ˆTPAï¼‰æ˜¯ä¸€ä¸ªç”¨äºè¶…å£°è§†é¢‘ä¸­èƒå„¿å…ˆå¤©æ€§å¿ƒè„ç¼ºé™·ï¼ˆCHDï¼‰åˆ†ç±»çš„æ¡†æ¶ï¼Œå®ƒé›†æˆäº†æ—¶é—´å»ºæ¨¡ã€æç¤ºæ„ŸçŸ¥å¯¹æ¯”å­¦ä¹ å’Œä¸ç¡®å®šæ€§é‡åŒ–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15298v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºå›¾åƒ-æ–‡æœ¬æ¨¡å‹å’Œæç¤ºæ„ŸçŸ¥å¯¹æ¯”å­¦ä¹ çš„å…ˆå¤©æ€§å¿ƒè„ç¼ºé™·ï¼ˆCHDï¼‰åˆ†ç±»æ–¹æ³•ï¼Œç§°ä¸ºTemporal Prompt Alignmentï¼ˆTPAï¼‰ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿä»è¶…å£°è§†é¢‘ä¸­æå–ç‰¹å¾ï¼Œæ•æ‰å¿ƒè„è¿åŠ¨ï¼Œå¹¶ä¸ç±»åˆ«ç‰¹å®šçš„æ–‡æœ¬æç¤ºå¯¹é½ã€‚ä¸ºå¢å¼ºåˆ†ç±»çš„ä¸´åºŠå¯é æ€§ï¼Œå¼•å…¥æ¡ä»¶å˜åˆ†è‡ªç¼–ç å™¨é£æ ¼è°ƒåˆ¶æ¨¡å—æ¥å­¦ä¹ æ½œåœ¨çš„é£æ ¼å‘é‡ä»¥è°ƒæ•´åµŒå…¥å¹¶é‡åŒ–åˆ†ç±»çš„ä¸ç¡®å®šæ€§ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒTPAåœ¨ç§äººæ•°æ®é›†ä¸Šçš„CHDæ£€æµ‹ä»¥åŠå…¬å…±æ•°æ®é›†EchoNet-Dynamicä¸Šçš„æ”¶ç¼©åŠŸèƒ½éšœç¢å‡è¾¾åˆ°äº†å…ˆè¿›æ°´å¹³ï¼Œå¹¶åœ¨å¤šç±»ä»»åŠ¡ä¸Šå®ç°äº†å®è§‚F1åˆ†æ•°çš„æ˜¾è‘—æå‡ã€‚æ€»ä½“æ¥è¯´ï¼ŒTemporal Prompt Alignmentæ˜¯ä¸€ä¸ªç»“åˆäº†æ—¶é—´å»ºæ¨¡ã€æç¤ºæ„ŸçŸ¥å¯¹æ¯”å­¦ä¹ å’Œä¸ç¡®å®šæ€§é‡åŒ–çš„æ¡†æ¶ï¼Œç”¨äºèƒå„¿å…ˆå¤©æ€§å¿ƒè„ç¼ºé™·çš„åˆ†ç±»ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å…ˆå¤©æ€§å¿ƒè„ç¼ºé™·ï¼ˆCHDï¼‰åœ¨è¶…å£°è§†é¢‘æ£€æµ‹ä¸­é¢ä¸´å›¾åƒå™ªå£°å’Œæ¢å¤´å®šä½å¯å˜æ€§çš„æŒ‘æˆ˜ã€‚</li>
<li>å½“å‰æœºå™¨å­¦ä¹ æ–¹æ³•å¿½ç•¥äº†æ—¶é—´ä¿¡æ¯ï¼Œä»…é™äºäºŒå…ƒåˆ†ç±»ï¼Œå¹¶ä¸”æœªè€ƒè™‘é¢„æµ‹æ ¡å‡†ã€‚</li>
<li>æå‡ºä¸€ç§åä¸ºTemporal Prompt Alignmentï¼ˆTPAï¼‰çš„æ–¹æ³•ï¼Œåˆ©ç”¨å›¾åƒ-æ–‡æœ¬æ¨¡å‹å’Œæç¤ºæ„ŸçŸ¥å¯¹æ¯”å­¦ä¹ å¯¹å…ˆå¤©æ€§å¿ƒè„ç¼ºé™·è¿›è¡Œåˆ†ç±»ã€‚</li>
<li>TPAä½¿ç”¨å›¾åƒç¼–ç å™¨ä»è§†é¢‘å­å‰ªè¾‘çš„æ¯ä¸€å¸§ä¸­æå–ç‰¹å¾ï¼Œå¹¶ç”¨å¯è®­ç»ƒçš„æ—¶é—´æå–å™¨æ•æ‰å¿ƒè„è¿åŠ¨ã€‚</li>
<li>é€šè¿‡å¼•å…¥æ¡ä»¶å˜åˆ†è‡ªç¼–ç å™¨é£æ ¼è°ƒåˆ¶æ¨¡å—ï¼ˆCVAESMï¼‰ï¼Œå¢å¼ºåˆ†ç±»çš„ä¸´åºŠå¯é æ€§å¹¶é‡åŒ–åˆ†ç±»çš„ä¸ç¡®å®šæ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15298">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-030996daf9bd8cb52b94e81ec5ee0eb4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f9c9db43549a41cab485975de73cf808.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c1009bc92687fed60d1a4721a21222e3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-52c859f24dd0635029fbebedaf093932.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Conditional-Cube-Attack-on-Round-Reduced-ASCON"><a href="#Conditional-Cube-Attack-on-Round-Reduced-ASCON" class="headerlink" title="Conditional Cube Attack on Round-Reduced ASCON"></a>Conditional Cube Attack on Round-Reduced ASCON</h2><p><strong>Authors:Zheng Li, Xiaoyang Dong, Xiaoyun Wang</strong></p>
<p>This paper evaluates the secure level of authenticated encryption \textsc{Ascon} against cube-like method. \textsc{Ascon} submitted by Dobraunig \emph{et<del>al.} is one of 16 survivors of the 3rd round CAESAR competition. The cube-like method is first used by Dinur \emph{et</del>al.} to analyze Keccak keyed modes. At CT-RSA 2015, Dobraunig \emph{et<del>al.} applied this method to 5&#x2F;6-round reduced \textsc{Ascon}, whose structure is similar to Keccak keyed modes. However, for \textsc{Ascon} the non-linear layer is more complex and state is much smaller, which make it hard for the attackers to select enough cube variables that do not multiply with each other after the first round. This seems to be the reason why the best previous key-recovery attack is on 6-round \textsc{Ascon}, while for Keccak keyed modes (Keccak-MAC and Keyak) the attacked round is no less than 7-round.   In this paper, we generalize the conditional cube attack proposed by Huang \emph{et</del>al.}, and find new cubes depending on some key bit conditions for 5&#x2F;6-round reduced \textsc{Ascon}, and translate the previous theoretic 6-round attack with $2^{66}$ time complexity to a practical one with $2^{40}$ time complexity. Moreover, we propose the first 7-round key-recovery attack on \textsc{Ascon}. By introducing \emph{the cube-like key-subset technique}, we divide the full key space into many subsets according to different key conditions. For each key subset, we launch the cube tester to determine if the key falls into it. Finally, we recover the full key space by testing all the key subsets. The total time complexity is about $2^{103.9}$. In addition, for a weak-key subset, whose size is $2^{117}$, the attack is more efficient and costs only $2^{77}$ time complexity. Those attacks do not threaten the full round (12 rounds) \textsc{Ascon}. </p>
<blockquote>
<p>æœ¬æ–‡è¯„ä¼°äº†è®¤è¯åŠ å¯†ç®—æ³•\Ascoå¯¹æŠ—ç«‹æ–¹ä½“æ”»å‡»çš„å®‰å…¨çº§åˆ«ã€‚ç”±Dobraunigç­‰äººæäº¤çš„\Ascoæ˜¯ç¬¬ä¸‰è½®CAESARç«èµ›ä¸­å¹¸å­˜çš„16ä¸ªç®—æ³•ä¹‹ä¸€ã€‚ç«‹æ–¹ä½“æ”»å‡»æ–¹æ³•æœ€åˆç”±Dinurç­‰äººç”¨äºåˆ†æKeccakå¯†é’¥æ¨¡å¼ã€‚åœ¨CT-RSA 2015å¹´ä¼šè®®ä¸Šï¼ŒDobraunigç­‰äººå°†æ­¤æ–¹æ³•åº”ç”¨äºç»è¿‡å‡å°‘çš„5&#x2F;6è½®\Ascoï¼Œå…¶ç»“æ„ä¸Keccakå¯†é’¥æ¨¡å¼ç›¸ä¼¼ã€‚ç„¶è€Œï¼Œå¯¹äº\Ascoæ¥è¯´ï¼Œå…¶éçº¿æ€§å±‚æ›´åŠ å¤æ‚ä¸”çŠ¶æ€æ›´å°ï¼Œè¿™ä½¿å¾—æ”»å‡»è€…éš¾ä»¥é€‰æ‹©è¶³å¤Ÿçš„ç«‹æ–¹ä½“å˜é‡ï¼Œè¿™äº›å˜é‡åœ¨ç¬¬ä¸€è½®ä¹‹åä¸ä¼šç›¸äº’ç›¸ä¹˜ã€‚è¿™ä¼¼ä¹å°±æ˜¯ä¸ºä»€ä¹ˆä¹‹å‰çš„æœ€ä½³å¯†é’¥æ¢å¤æ”»å‡»é’ˆå¯¹çš„æ˜¯ç»è¿‡å‡å°‘çš„6è½®\Ascoçš„åŸå› ï¼Œè€Œå¯¹äºKeccakå¯†é’¥æ¨¡å¼ï¼ˆKeccak-MACå’ŒKeyakï¼‰ï¼Œæ”»å‡»è½®æ¬¡ä¸å°‘äºç¬¬7è½®ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ¨å¹¿äº†Huangç­‰äººæå‡ºçš„æ¡ä»¶ç«‹æ–¹ä½“æ”»å‡»ï¼Œå¹¶é’ˆå¯¹ç»è¿‡å‡å°‘çš„5&#x2F;6è½®\Ascoæ‰¾åˆ°äº†ä¾èµ–äºæŸäº›å¯†é’¥ä½æ¡ä»¶çš„æ–°çš„ç«‹æ–¹ä½“ï¼Œå¹¶å°†ä¹‹å‰çš„ç†è®ºä¸Šçš„é’ˆå¯¹ç»è¿‡å‡å°‘çš„6è½®æ”»å‡»çš„$2^{66}$çš„æ—¶é—´å¤æ‚åº¦è½¬åŒ–ä¸ºå…·æœ‰å®é™…å¯è¡Œæ€§çš„$2^{40}$çš„æ—¶é—´å¤æ‚åº¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é¦–æ¬¡æå‡ºäº†é’ˆå¯¹\Ascoçš„é’ˆå¯¹ç»è¿‡å‡å°‘çš„7è½®çš„å¯†é’¥æ¢å¤æ”»å‡»ã€‚é€šè¿‡å¼•å…¥ç«‹æ–¹ä½“ç±»ä¼¼å¯†é’¥å­é›†æŠ€æœ¯ï¼Œæˆ‘ä»¬æ ¹æ®ä¸åŒçš„å¯†é’¥æ¡ä»¶å°†å®Œæ•´çš„å¯†é’¥ç©ºé—´åˆ’åˆ†ä¸ºå¤šä¸ªå­é›†ã€‚å¯¹äºæ¯ä¸ªå¯†é’¥å­é›†ï¼Œæˆ‘ä»¬å¯åŠ¨ç«‹æ–¹ä½“æµ‹è¯•å™¨æ¥ç¡®å®šå¯†é’¥æ˜¯å¦å±äºå®ƒã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡æµ‹è¯•æ‰€æœ‰å¯†é’¥å­é›†æ¥æ¢å¤å®Œæ•´çš„å¯†é’¥ç©ºé—´ã€‚æ€»çš„æ—¶é—´å¤æ‚åº¦å¤§çº¦æ˜¯$2^{103.9}$ã€‚æ­¤å¤–ï¼Œå¯¹äºå¤§å°ä¸º$2^{117}$çš„å¼±å¯†é’¥å­é›†ï¼Œæ”»å‡»æ›´ä¸ºæœ‰æ•ˆä¸”ä»…è€—æ—¶$2^{77}$çš„æ—¶é—´å¤æ‚åº¦ã€‚è¿™äº›æ”»å‡»ä¸ä¼šå¨èƒåˆ°ç»è¿‡å®Œæ•´è®¾è®¡ï¼ˆæ€»å…±è¿›è¡Œä¸‰è½®åŠ è§£ï¼‰çš„å®Œæ•´çš„åŸå§‹ç‰ˆçš„Asconç®—æ³•çš„å®‰å…¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15172v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>æœ¬æ–‡è¯„ä¼°äº†èº«ä»½éªŒè¯åŠ å¯†æ–¹æ³•AscÃ³nçš„å®‰å…¨æ€§ï¼Œé‡‡ç”¨ç«‹æ–¹ä½“æ”»å‡»æ–¹æ³•è¿›è¡Œç ”ç©¶ã€‚æ–‡ç« å¯¹AscÃ³nè¿›è¡Œäº†æ¡ä»¶ç«‹æ–¹ä½“æ”»å‡»ï¼Œæå‡ºäº†æ–°çš„ç«‹æ–¹ä½“ç»“æ„ï¼Œå¹¶å°†ç†è®ºä¸Šçš„å…­è½®æ”»å‡»è½¬åŒ–ä¸ºå®é™…æ”»å‡»ï¼Œé™ä½äº†æ—¶é—´å¤æ‚åº¦ã€‚åŒæ—¶ï¼Œæ–‡ç« è¿˜æå‡ºäº†é’ˆå¯¹AscÃ³nçš„é¦–ä¸ªä¸ƒè½®å¯†é’¥æ¢å¤æ”»å‡»ï¼Œé€šè¿‡å¼•å…¥ç«‹æ–¹ä½“å¯†é’¥å­é›†æŠ€æœ¯ï¼Œå°†æ•´ä¸ªå¯†é’¥ç©ºé—´åˆ’åˆ†ä¸ºå¤šä¸ªå­é›†è¿›è¡Œæµ‹è¯•ï¼Œæœ€ç»ˆæ¢å¤å‡ºå®Œæ•´çš„å¯†é’¥ç©ºé—´ã€‚ä½†æ”»å‡»å¹¶ä¸å¨èƒåˆ°å®Œæ•´çš„AscÃ³nå®‰å…¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>AscÃ³né€šè¿‡äº†ç«‹æ–¹ä½“æ”»å‡»æ–¹æ³•çš„è¯„ä¼°ã€‚</li>
<li>æ¡ä»¶ç«‹æ–¹ä½“æ”»å‡»åº”ç”¨äºAscÃ³nçš„æ–°ç«‹æ–¹ä½“ç»“æ„æå‡ºã€‚</li>
<li>å°†ç†è®ºä¸Šçš„å…­è½®æ”»å‡»è½¬åŒ–ä¸ºå®é™…æ”»å‡»ï¼Œæ—¶é—´å¤æ‚åº¦é™ä½ã€‚</li>
<li>é¦–æ¬¡æå‡ºäº†é’ˆå¯¹AscÃ³nçš„ä¸ƒè½®å¯†é’¥æ¢å¤æ”»å‡»ã€‚</li>
<li>é€šè¿‡ç«‹æ–¹ä½“å¯†é’¥å­é›†æŠ€æœ¯å°†æ•´ä¸ªå¯†é’¥ç©ºé—´åˆ’åˆ†ä¸ºå¤šä¸ªå­é›†è¿›è¡Œæµ‹è¯•ã€‚</li>
<li>æœ€ç»ˆæˆåŠŸæ¢å¤å‡ºå®Œæ•´çš„å¯†é’¥ç©ºé—´ï¼Œæ€»æ—¶é—´å¤æ‚åº¦çº¦ä¸º$2^{103.9}$ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15172">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a0c8def00d742f948c37657aec6f9d7a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a03f874477c439a1184ec848c99de2d4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a65dd172e3e5a4715789b619f22ea7f7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1301aff1037872fa0ce3351763bd68ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c1cf1c7f6869b63684e8fe7c8e700fcf.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Cohort-Aware-Agents-for-Individualized-Lung-Cancer-Risk-Prediction-Using-a-Retrieval-Augmented-Model-Selection-Framework"><a href="#Cohort-Aware-Agents-for-Individualized-Lung-Cancer-Risk-Prediction-Using-a-Retrieval-Augmented-Model-Selection-Framework" class="headerlink" title="Cohort-Aware Agents for Individualized Lung Cancer Risk Prediction Using   a Retrieval-Augmented Model Selection Framework"></a>Cohort-Aware Agents for Individualized Lung Cancer Risk Prediction Using   a Retrieval-Augmented Model Selection Framework</h2><p><strong>Authors:Chongyu Qu, Allen J. Luna, Thomas Z. Li, Junchao Zhu, Junlin Guo, Juming Xiong, Kim L. Sandler, Bennett A. Landman, Yuankai Huo</strong></p>
<p>Accurate lung cancer risk prediction remains challenging due to substantial variability across patient populations and clinical settings â€“ no single model performs best for all cohorts. To address this, we propose a personalized lung cancer risk prediction agent that dynamically selects the most appropriate model for each patient by combining cohort-specific knowledge with modern retrieval and reasoning techniques. Given a patientâ€™s CT scan and structured metadata â€“ including demographic, clinical, and nodule-level features â€“ the agent first performs cohort retrieval using FAISS-based similarity search across nine diverse real-world cohorts to identify the most relevant patient population from a multi-institutional database. Second, a Large Language Model (LLM) is prompted with the retrieved cohort and its associated performance metrics to recommend the optimal prediction algorithm from a pool of eight representative models, including classical linear risk models (e.g., Mayo, Brock), temporally-aware models (e.g., TDVIT, DLSTM), and multi-modal computer vision-based approaches (e.g., Liao, Sybil, DLS, DLI). This two-stage agent pipeline â€“ retrieval via FAISS and reasoning via LLM â€“ enables dynamic, cohort-aware risk prediction personalized to each patientâ€™s profile. Building on this architecture, the agent supports flexible and cohort-driven model selection across diverse clinical populations, offering a practical path toward individualized risk assessment in real-world lung cancer screening. </p>
<blockquote>
<p>ç²¾ç¡®é¢„æµ‹è‚ºç™Œé£é™©ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå› ä¸ºä¸åŒæ‚£è€…ç¾¤ä½“å’Œä¸´åºŠç¯å¢ƒä¹‹é—´å­˜åœ¨å¾ˆå¤§çš„å·®å¼‚â€”â€”æ²¡æœ‰ä»»ä½•å•ä¸€æ¨¡å‹é€‚ç”¨äºæ‰€æœ‰ç¾¤ä½“ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸ªæ€§åŒ–çš„è‚ºç™Œé£é™©é¢„æµ‹ä»£ç†ï¼Œè¯¥ä»£ç†é€šè¿‡ç»“åˆç‰¹å®šäººç¾¤çš„çŸ¥è¯†ä¸ç°ä»£æ£€ç´¢å’Œæ¨ç†æŠ€æœ¯ï¼ŒåŠ¨æ€ä¸ºæ¯ä½æ‚£è€…é€‰æ‹©æœ€åˆé€‚çš„æ¨¡å‹ã€‚ç»™å®šæ‚£è€…çš„CTæ‰«æå’Œç»“æ„åŒ–å…ƒæ•°æ®ï¼ˆåŒ…æ‹¬äººå£ç»Ÿè®¡ã€ä¸´åºŠå’Œç»“èŠ‚çº§ç‰¹å¾ï¼‰ï¼Œä»£ç†é¦–å…ˆä½¿ç”¨åŸºäºFAISSçš„ç›¸ä¼¼æ€§æœç´¢åœ¨ä¹ä¸ªå¤šæ ·åŒ–çš„çœŸå®ä¸–ç•Œç¾¤ä½“ä¸­è¿›è¡Œäººç¾¤æ£€ç´¢ï¼Œä»¥è¯†åˆ«ä¸å¤šæœºæ„æ•°æ®åº“ä¸­æœ€ç›¸å…³çš„äººç¾¤ã€‚å…¶æ¬¡ï¼Œä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ ¹æ®æ£€ç´¢åˆ°çš„ç¾¤ä½“åŠå…¶ç›¸å…³æ€§èƒ½æŒ‡æ ‡ï¼Œä»å…«ä¸ªä»£è¡¨æ€§æ¨¡å‹ä¸­æ¨èæœ€ä½³çš„é¢„æµ‹ç®—æ³•ï¼ŒåŒ…æ‹¬ç»å…¸çº¿æ€§é£é™©æ¨¡å‹ï¼ˆä¾‹å¦‚Mayoã€Brockï¼‰ã€æ—¶é—´æ„ŸçŸ¥æ¨¡å‹ï¼ˆä¾‹å¦‚TDVITã€DLSTMï¼‰å’Œå¤šæ¨¡æ€è®¡ç®—æœºè§†è§‰æ–¹æ³•ï¼ˆä¾‹å¦‚Liaoã€Sybilã€DLSã€DLIï¼‰ã€‚è¿™ç§ä¸¤é˜¶æ®µçš„ä»£ç†ç®¡é“â€”â€”é€šè¿‡FAISSè¿›è¡Œæ£€ç´¢ï¼Œé€šè¿‡LLMè¿›è¡Œæ¨ç†â€”â€”å®ç°äº†é’ˆå¯¹æ¯ä¸ªæ‚£è€…èµ„æ–™çš„åŠ¨æ€ã€äººç¾¤æ„ŸçŸ¥é£é™©é¢„æµ‹ã€‚åŸºäºè¿™ä¸€æ¶æ„ï¼Œä»£ç†æ”¯æŒåœ¨å¤šæ ·åŒ–çš„ä¸´åºŠäººç¾¤ä¸­å®ç°çµæ´»å’ŒåŸºäºäººç¾¤æ¨¡å‹çš„é€‰æ‹©ï¼Œä¸ºçœŸå®ä¸–ç•Œè‚ºç™Œç­›æŸ¥ä¸­çš„ä¸ªæ€§åŒ–é£é™©è¯„ä¼°æä¾›äº†å®é™…è·¯å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14940v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†ä¸€ç§ä¸ªæ€§åŒ–è‚ºç™Œé£é™©é¢„æµ‹æ¨¡å‹ï¼Œå®ƒé€šè¿‡ç»“åˆæ‚£è€…äººç¾¤ç‰¹å®šçš„çŸ¥è¯†ä»¥åŠç°ä»£æ£€ç´¢å’Œæ¨ç†æŠ€æœ¯ï¼Œé’ˆå¯¹æ¯ä¸ªæ‚£è€…åŠ¨æ€é€‰æ‹©æœ€åˆé€‚çš„é¢„æµ‹æ¨¡å‹ã€‚æ¨¡å‹é¦–å…ˆé€šè¿‡åŸºäºFAISSçš„ç›¸ä¼¼æ€§æœç´¢åœ¨å¤šä¸ªçœŸå®ä¸–ç•Œçš„æ‚£è€…äººç¾¤ä¸­æ£€ç´¢æœ€ç›¸å…³çš„æ‚£è€…ç¾¤ä½“ï¼Œç„¶åä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨èæœ€ä½³çš„é¢„æµ‹ç®—æ³•ã€‚è¿™ç§ä¸¤é˜¶æ®µæ¨¡å‹ç®¡é“å®ç°äº†é’ˆå¯¹æ¯ä¸ªæ‚£è€…ä¸ªäººèµ„æ–™çš„åŠ¨æ€ã€äººç¾¤æ„ŸçŸ¥çš„é£é™©é¢„æµ‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‚ºç™Œé£é™©é¢„æµ‹å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºä¸åŒæ‚£è€…äººç¾¤å’Œä¸´åºŠç¯å¢ƒä¹‹é—´å­˜åœ¨å¾ˆå¤§çš„å˜å¼‚æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ä¸ªæ€§åŒ–è‚ºç™Œé£é™©é¢„æµ‹æ¨¡å‹ï¼Œèƒ½ç»“åˆç‰¹å®šäººç¾¤çŸ¥è¯†å’Œç°ä»£æ£€ç´¢åŠæ¨ç†æŠ€æœ¯ä¸ºæ‚£è€…åŠ¨æ€é€‰æ‹©æœ€åˆé€‚çš„é¢„æµ‹æ¨¡å‹ã€‚</li>
<li>è¯¥æ¨¡å‹é¦–å…ˆä½¿ç”¨åŸºäºFAISSçš„ç›¸ä¼¼æ€§æœç´¢åœ¨å¤šä¸ªçœŸå®ä¸–ç•Œçš„äººç¾¤ä¸­æ£€ç´¢æœ€ç›¸å…³çš„æ‚£è€…ç¾¤ä½“ã€‚</li>
<li>åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨èæœ€ä½³é¢„æµ‹ç®—æ³•ï¼Œä»å…«ä¸ªä»£è¡¨æ€§æ¨¡å‹ä¸­é€‰å–ã€‚</li>
<li>æ¨¡å‹åŒ…æ‹¬ç»å…¸çº¿æ€§é£é™©æ¨¡å‹ã€æ—¶é—´æ„ŸçŸ¥æ¨¡å‹å’ŒåŸºäºå¤šæ¨¡æ€è®¡ç®—æœºè§†è§‰çš„æ–¹æ³•ã€‚</li>
<li>è¯¥æ¨¡å‹ç®¡é“é€šè¿‡åŠ¨æ€ã€äººç¾¤æ„ŸçŸ¥çš„é£é™©é¢„æµ‹å®ç°äº†ä¸ªæ€§åŒ–çš„é£é™©è¯„ä¼°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14940">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-02643eda2817bcee9fc6aee66b94dec2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-401cfeec5ffc0725a1de2c129cf933f8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6993cc35db0e246b9fb29b865fea4bef.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-536954ecb68d3f79f317f64d583fc7cb.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="TOM-An-Open-Source-Tongue-Segmentation-Method-with-Multi-Teacher-Distillation-and-Task-Specific-Data-Augmentation"><a href="#TOM-An-Open-Source-Tongue-Segmentation-Method-with-Multi-Teacher-Distillation-and-Task-Specific-Data-Augmentation" class="headerlink" title="TOM: An Open-Source Tongue Segmentation Method with Multi-Teacher   Distillation and Task-Specific Data Augmentation"></a>TOM: An Open-Source Tongue Segmentation Method with Multi-Teacher   Distillation and Task-Specific Data Augmentation</h2><p><strong>Authors:Jiacheng Xie, Ziyang Zhang, Biplab Poudel, Congyu Guo, Yang Yu, Guanghui An, Xiaoting Tang, Lening Zhao, Chunhui Xu, Dong Xu</strong></p>
<p>Tongue imaging serves as a valuable diagnostic tool, particularly in Traditional Chinese Medicine (TCM). The quality of tongue surface segmentation significantly affects the accuracy of tongue image classification and subsequent diagnosis in intelligent tongue diagnosis systems. However, existing research on tongue image segmentation faces notable limitations, and there is a lack of robust and user-friendly segmentation tools. This paper proposes a tongue image segmentation model (TOM) based on multi-teacher knowledge distillation. By incorporating a novel diffusion-based data augmentation method, we enhanced the generalization ability of the segmentation model while reducing its parameter size. Notably, after reducing the parameter count by 96.6% compared to the teacher models, the student model still achieves an impressive segmentation performance of 95.22% mIoU. Furthermore, we packaged and deployed the trained model as both an online and offline segmentation tool (available at <a target="_blank" rel="noopener" href="https://itongue.cn/">https://itongue.cn/</a>), allowing TCM practitioners and researchers to use it without any programming experience. We also present a case study on TCM constitution classification using segmented tongue patches. Experimental results demonstrate that training with tongue patches yields higher classification performance and better interpretability than original tongue images. To our knowledge, this is the first open-source and freely available tongue image segmentation tool. </p>
<blockquote>
<p>èˆŒæˆåƒä½œä¸ºä¸€ç§è¯Šæ–­å·¥å…·ï¼Œåœ¨ä¸­åŒ»ä¸­å…·æœ‰é‡è¦ä»·å€¼ã€‚èˆŒè¡¨é¢åˆ†å‰²çš„è´¨é‡æ˜¾è‘—å½±å“æ™ºèƒ½èˆŒè¯Šæ–­ç³»ç»Ÿä¸­èˆŒå›¾åƒåˆ†ç±»å’Œéšåè¯Šæ–­çš„å‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œç°æœ‰çš„èˆŒå›¾åƒåˆ†å‰²ç ”ç©¶é¢ä¸´æ˜¾è‘—å±€é™æ€§ï¼Œç¼ºä¹ç¨³å¥ä¸”ç”¨æˆ·å‹å¥½çš„åˆ†å‰²å·¥å…·ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¤šæ•™å¸ˆçŸ¥è¯†è’¸é¦çš„èˆŒå›¾åƒåˆ†å‰²æ¨¡å‹ï¼ˆTOMï¼‰ã€‚é€šè¿‡å¼•å…¥ä¸€ç§æ–°å‹åŸºäºæ‰©æ•£çš„æ•°æ®å¢å¼ºæ–¹æ³•ï¼Œæˆ‘ä»¬æé«˜äº†åˆ†å‰²æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼ŒåŒæ—¶å‡å°äº†å…¶å‚æ•°è§„æ¨¡ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä¸æ•™å¸ˆæ¨¡å‹ç›¸æ¯”ï¼Œæˆ‘ä»¬å°†å‚æ•°æ•°é‡å‡å°‘äº†96.6%ï¼Œå­¦ç”Ÿæ¨¡å‹ä»å®ç°äº†95.22%çš„mIoUåˆ†å‰²æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†è®­ç»ƒå¥½çš„æ¨¡å‹æ‰“åŒ…éƒ¨ç½²ä¸ºä¸€ä¸ªåœ¨çº¿å’Œç¦»çº¿çš„åˆ†å‰²å·¥å…·ï¼ˆå¯åœ¨[<a target="_blank" rel="noopener" href="https://itongue.cn/%E8%AE%BF%E9%97%AE%EF%BC%89%EF%BC%8C%E4%BD%BF%E4%B8%AD%E5%8C%BB%E4%BB%8E%E4%B8%9A%E8%80%85%E5%92%8C%E7%A0%94%E7%A9%B6%E4%BA%BA%E5%91%98%E6%97%A0%E9%9C%80%E7%BC%96%E7%A8%8B%E7%BB%8F%E9%AA%8C%E5%8D%B3%E5%8F%AF%E4%BD%BF%E7%94%A8%E3%80%82%E6%88%91%E4%BB%AC%E8%BF%98%E5%B1%95%E7%A4%BA%E4%BA%86%E4%B8%80%E9%A1%B9%E5%85%B3%E4%BA%8E%E4%BD%BF%E7%94%A8%E5%88%86%E5%89%B2%E8%88%8C%E7%89%87%E8%BF%9B%E8%A1%8C%E4%B8%AD%E5%8C%BB%E4%BD%93%E8%B4%A8%E5%88%86%E7%B1%BB%E7%9A%84%E6%A1%88%E4%BE%8B%E7%A0%94%E7%A9%B6%E3%80%82%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C%E8%A1%A8%E6%98%8E%EF%BC%8C%E4%BD%BF%E7%94%A8%E8%88%8C%E7%89%87%E8%BF%9B%E8%A1%8C%E8%AE%AD%E7%BB%83%E5%8F%AF%E4%BB%A5%E8%8E%B7%E5%BE%97%E6%AF%94%E4%BD%BF%E7%94%A8%E5%8E%9F%E5%A7%8B%E8%88%8C%E5%9B%BE%E5%83%8F%E6%9B%B4%E9%AB%98%E7%9A%84%E5%88%86%E7%B1%BB%E6%80%A7%E8%83%BD%E5%92%8C%E6%9B%B4%E5%A5%BD%E7%9A%84%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7%E3%80%82%E6%8D%AE%E6%88%91%E4%BB%AC%E6%89%80%E7%9F%A5%EF%BC%8C%E8%BF%99%E6%98%AF%E7%AC%AC%E4%B8%80%E4%B8%AA%E5%BC%80%E6%BA%90%E4%B8%94%E5%85%8D%E8%B4%B9%E4%BD%BF%E7%94%A8%E7%9A%84%E8%88%8C%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E5%B7%A5%E5%85%B7%E3%80%82">https://itongue.cn/è®¿é—®ï¼‰ï¼Œä½¿ä¸­åŒ»ä»ä¸šè€…å’Œç ”ç©¶äººå‘˜æ— éœ€ç¼–ç¨‹ç»éªŒå³å¯ä½¿ç”¨ã€‚æˆ‘ä»¬è¿˜å±•ç¤ºäº†ä¸€é¡¹å…³äºä½¿ç”¨åˆ†å‰²èˆŒç‰‡è¿›è¡Œä¸­åŒ»ä½“è´¨åˆ†ç±»çš„æ¡ˆä¾‹ç ”ç©¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨èˆŒç‰‡è¿›è¡Œè®­ç»ƒå¯ä»¥è·å¾—æ¯”ä½¿ç”¨åŸå§‹èˆŒå›¾åƒæ›´é«˜çš„åˆ†ç±»æ€§èƒ½å’Œæ›´å¥½çš„å¯è§£é‡Šæ€§ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå¼€æºä¸”å…è´¹ä½¿ç”¨çš„èˆŒå›¾åƒåˆ†å‰²å·¥å…·ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14932v1">PDF</a> Tongue segmentation, data augmentation, synthetic data for AI   training, prompt engineering, Segment Anything Model, knowledge distillation,   tongue classification</p>
<p><strong>Summary</strong><br>     èˆŒè±¡æˆåƒåœ¨ä¸­åŒ»è¯Šæ–­ä¸­å…·æœ‰é‡è¦ä»·å€¼ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¤šæ•™å¸ˆçŸ¥è¯†è’¸é¦çš„èˆŒå›¾åƒåˆ†å‰²æ¨¡å‹ï¼ˆTOMï¼‰ï¼Œé€šè¿‡å¼•å…¥æ‰©æ•£å¼æ•°æ®å¢å¼ºæ–¹æ³•ï¼Œæé«˜äº†åˆ†å‰²æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å¹¶å‡å°äº†å‚æ•°è§„æ¨¡ã€‚è¯¥æ¨¡å‹ä¸ä»…åœ¨çº¿å’Œçº¿ä¸‹å‡å¯ä½œä¸ºåˆ†å‰²å·¥å…·ä½¿ç”¨ï¼Œè€Œä¸”å®ç°äº†è¾ƒé«˜çš„åˆ†å‰²æ€§èƒ½ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å±•ç¤ºäº†ä½¿ç”¨åˆ†å‰²èˆŒæ–‘è¿›è¡Œä¸­åŒ»ä½“è´¨åˆ†ç±»çš„æ¡ˆä¾‹ç ”ç©¶ï¼Œç»“æœè¡¨æ˜ä½¿ç”¨èˆŒæ–‘è¿›è¡Œè®­ç»ƒå¯ä»¥æé«˜åˆ†ç±»æ€§èƒ½å’Œè§£é‡Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>èˆŒè±¡æˆåƒåœ¨ä¸­åŒ»è¯Šæ–­ä¸­å…·é‡è¦ä»·å€¼ã€‚</li>
<li>ç°æœ‰èˆŒå›¾åƒåˆ†å‰²æŠ€æœ¯å­˜åœ¨å±€é™æ€§å’Œç¼ºä¹ç¨³å¥ã€ç”¨æˆ·å‹å¥½çš„åˆ†å‰²å·¥å…·ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¤šæ•™å¸ˆçŸ¥è¯†è’¸é¦çš„èˆŒå›¾åƒåˆ†å‰²æ¨¡å‹ï¼ˆTOMï¼‰ã€‚</li>
<li>å¼•å…¥æ‰©æ•£å¼æ•°æ®å¢å¼ºæ–¹æ³•ï¼Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å¹¶å‡å°å‚æ•°è§„æ¨¡ã€‚</li>
<li>æ¨¡å‹å®ç°è¾ƒé«˜çš„åˆ†å‰²æ€§èƒ½ï¼Œè¾¾åˆ°95.22%çš„mIoUã€‚</li>
<li>è¯¥æ¨¡å‹è¢«æ‰“åŒ…å¹¶éƒ¨ç½²ä¸ºåœ¨çº¿å’Œçº¿ä¸‹åˆ†å‰²å·¥å…·ï¼Œç½‘å€ä¸º<a target="_blank" rel="noopener" href="https://itongue.cn/%E3%80%82">https://itongue.cn/ã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14932">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-dc2c526f5bf91b3533e3f34f9bae8fe7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dcad6da5076022ee8ef1e0faa7254fb1.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Pixels-Under-Pressure-Exploring-Fine-Tuning-Paradigms-for-Foundation-Models-in-High-Resolution-Medical-Imaging"><a href="#Pixels-Under-Pressure-Exploring-Fine-Tuning-Paradigms-for-Foundation-Models-in-High-Resolution-Medical-Imaging" class="headerlink" title="Pixels Under Pressure: Exploring Fine-Tuning Paradigms for Foundation   Models in High-Resolution Medical Imaging"></a>Pixels Under Pressure: Exploring Fine-Tuning Paradigms for Foundation   Models in High-Resolution Medical Imaging</h2><p><strong>Authors:Zahra TehraniNasab, Amar Kumar, Tal Arbel</strong></p>
<p>Advancements in diffusion-based foundation models have improved text-to-image generation, yet most efforts have been limited to low-resolution settings. As high-resolution image synthesis becomes increasingly essential for various applications, particularly in medical imaging domains, fine-tuning emerges as a crucial mechanism for adapting these powerful pre-trained models to task-specific requirements and data distributions. In this work, we present a systematic study, examining the impact of various fine-tuning techniques on image generation quality when scaling to high resolution 512x512 pixels. We benchmark a diverse set of fine-tuning methods, including full fine-tuning strategies and parameter-efficient fine-tuning (PEFT). We dissect how different fine-tuning methods influence key quality metrics, including Fr&#39;echet Inception Distance (FID), Vendi score, and prompt-image alignment. We also evaluate the utility of generated images in a downstream classification task under data-scarce conditions, demonstrating that specific fine-tuning strategies improve both generation fidelity and downstream performance when synthetic images are used for classifier training and evaluation on real images. Our code is accessible through the project website - <a target="_blank" rel="noopener" href="https://tehraninasab.github.io/PixelUPressure/">https://tehraninasab.github.io/PixelUPressure/</a>. </p>
<blockquote>
<p>åŸºäºæ‰©æ•£çš„åŸºç¡€æ¨¡å‹çš„è¿›æ­¥å·²ç»æé«˜äº†æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„èƒ½åŠ›ï¼Œä½†å¤§å¤šæ•°åŠªåŠ›éƒ½å±€é™äºä½åˆ†è¾¨ç‡ç¯å¢ƒã€‚éšç€é«˜åˆ†è¾¨ç‡å›¾åƒåˆæˆåœ¨å„ä¸ªé¢†åŸŸï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»å­¦å½±åƒé¢†åŸŸå˜å¾—è¶Šæ¥è¶Šé‡è¦ï¼Œå¾®è°ƒä½œä¸ºä¸€ç§é€‚åº”è¿™äº›å¼ºå¤§é¢„è®­ç»ƒæ¨¡å‹ä»¥æ»¡è¶³ç‰¹å®šä»»åŠ¡å’Œæ•°æ®åˆ†å¸ƒéœ€æ±‚çš„æœºåˆ¶æ˜¾å¾—è‡³å…³é‡è¦ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è¿›è¡Œäº†ç³»ç»Ÿç ”ç©¶ï¼Œæ¢è®¨äº†å„ç§å¾®è°ƒæŠ€æœ¯å¯¹å›¾åƒç”Ÿæˆè´¨é‡çš„å½±å“ï¼Œç‰¹åˆ«æ˜¯åœ¨æ‰©å±•åˆ°é«˜åˆ†è¾¨ç‡ï¼ˆ512x512åƒç´ ï¼‰æ—¶ã€‚æˆ‘ä»¬å¯¹ä¸€ç³»åˆ—å¾®è°ƒæ–¹æ³•è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬å…¨å¾®è°ƒç­–ç•¥å’Œå‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰ã€‚æˆ‘ä»¬åˆ†æäº†ä¸åŒçš„å¾®è°ƒæ–¹æ³•å¦‚ä½•å½±å“å…³é”®çš„è´¨é‡æŒ‡æ ‡ï¼ŒåŒ…æ‹¬FrÃ©chet Inception Distanceï¼ˆFIDï¼‰ã€Vendiåˆ†æ•°å’Œæç¤ºå›¾åƒå¯¹é½ã€‚æˆ‘ä»¬è¿˜è¯„ä¼°äº†åœ¨æ•°æ®ç¨€ç¼ºæ¡ä»¶ä¸‹ç”Ÿæˆå›¾åƒåœ¨ä¸‹æ¸¸åˆ†ç±»ä»»åŠ¡ä¸­çš„å®ç”¨æ€§ï¼Œè¯æ˜äº†ç‰¹å®šçš„å¾®è°ƒç­–ç•¥åœ¨æé«˜ç”Ÿæˆä¿çœŸåº¦å’Œä¸‹æ¸¸æ€§èƒ½æ–¹é¢çš„ä½œç”¨ï¼Œå°¤å…¶æ˜¯åœ¨ä½¿ç”¨åˆæˆå›¾åƒè¿›è¡Œåˆ†ç±»å™¨è®­ç»ƒå’Œè¯„ä¼°çœŸå®å›¾åƒæ—¶ã€‚æˆ‘ä»¬çš„ä»£ç å¯é€šè¿‡é¡¹ç›®ç½‘ç«™è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://tehraninasab.github.io/PixelUPressure/">ç½‘ç«™é“¾æ¥</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14931v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ç ”ç©¶äº†ä¸åŒç²¾ç»†è°ƒæ•´æŠ€æœ¯å¯¹é«˜åˆ†è¾¨ç‡ï¼ˆ512x512åƒç´ ï¼‰å›¾åƒç”Ÿæˆè´¨é‡çš„å½±å“ã€‚æ–‡ç« æ¢è®¨äº†å¤šç§ç²¾ç»†è°ƒæ•´æ–¹æ³•ï¼ŒåŒ…æ‹¬å…¨ç²¾ç»†è°ƒæ•´ç­–ç•¥å’Œå‚æ•°é«˜æ•ˆç²¾ç»†è°ƒæ•´ï¼ˆPEFTï¼‰ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œä¸åŒçš„ç²¾ç»†è°ƒæ•´æ–¹æ³•ä¼šå½±å“å…³é”®è´¨é‡æŒ‡æ ‡ï¼Œå¦‚FrÃ©chet Inception Distanceï¼ˆFIDï¼‰ã€Vendiåˆ†æ•°å’Œæç¤ºå›¾åƒå¯¹é½ç­‰ã€‚æ­¤å¤–ï¼Œåœ¨æ•°æ®ç¨€ç¼ºçš„æ¡ä»¶ä¸‹ï¼Œå¯¹ç”Ÿæˆå›¾åƒåœ¨ä¸‹æ¸¸åˆ†ç±»ä»»åŠ¡ä¸­çš„å®ç”¨æ€§è¿›è¡Œäº†è¯„ä¼°ï¼Œè¯æ˜äº†ç‰¹å®šçš„ç²¾ç»†è°ƒæ•´ç­–ç•¥åœ¨æé«˜ç”Ÿæˆå›¾åƒä¿çœŸåº¦å’Œä¸‹æ¸¸æ€§èƒ½æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£åŸºç¡€æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ–¹é¢çš„è¿›å±•å·²æœ‰æ‰€æå‡ï¼Œä½†å¤§å¤šæ•°åŠªåŠ›ä»…é™äºä½åˆ†è¾¨ç‡è®¾ç½®ã€‚</li>
<li>é«˜åˆ†è¾¨ç‡å›¾åƒåˆæˆåœ¨å„ç§åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»å­¦å½±åƒé¢†åŸŸå˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚</li>
<li>ç²¾ç»†è°ƒæ•´æ˜¯é€‚åº”è¿™äº›å¼ºå¤§é¢„è®­ç»ƒæ¨¡å‹ä»¥ç¬¦åˆç‰¹å®šä»»åŠ¡å’Œæ•°æ®å¤„ç†åˆ†å¸ƒçš„å…³é”®æœºåˆ¶ã€‚</li>
<li>ç ”ç©¶å¯¹å¤šç§ç²¾ç»†è°ƒæ•´æ–¹æ³•è¿›è¡Œäº†ç³»ç»Ÿç ”ç©¶ï¼ŒåŒ…æ‹¬å…¨ç²¾ç»†è°ƒæ•´å’Œå‚æ•°é«˜æ•ˆç²¾ç»†è°ƒæ•´ï¼ˆPEFTï¼‰ã€‚</li>
<li>ä¸åŒç²¾ç»†è°ƒæ•´æ–¹æ³•ä¼šå½±å“å›¾åƒç”Ÿæˆçš„å…³é”®è´¨é‡æŒ‡æ ‡ï¼Œå¦‚FrÃ©chet Inception Distanceï¼ˆFIDï¼‰ã€Vendiåˆ†æ•°å’Œæç¤ºå›¾åƒå¯¹é½ã€‚</li>
<li>åœ¨æ•°æ®ç¨€ç¼ºçš„æ¡ä»¶ä¸‹ï¼Œç”Ÿæˆå›¾åƒåœ¨ä¸‹æ¸¸åˆ†ç±»ä»»åŠ¡ä¸­çš„å®ç”¨æ€§å¾—åˆ°äº†è¯„ä¼°ã€‚</li>
<li>ç‰¹å®šçš„ç²¾ç»†è°ƒæ•´ç­–ç•¥åœ¨æé«˜ç”Ÿæˆå›¾åƒçš„ä¿çœŸåº¦å’Œä¸‹æ¸¸æ€§èƒ½æ–¹é¢éƒ½è¡¨ç°å‡ºäº†æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14931">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-24b5e255333bc23fe8e7fd6b3303f695.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d298fd1b29c541fbe1a1072c0de8e9b7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8437485d2fa7378f378692998b70984b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-254bac17c7d446ad0532493b44de52cf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-76cda3b8341cd4a1101be33ae98eed77.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-385b0047c6b1694df75a6a175bb5b15b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-10b945a712957721da21e7664d62509f.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="LV-Net-Anatomy-aware-lateral-ventricle-shape-modeling-with-a-case-study-on-Alzheimerâ€™s-disease"><a href="#LV-Net-Anatomy-aware-lateral-ventricle-shape-modeling-with-a-case-study-on-Alzheimerâ€™s-disease" class="headerlink" title="LV-Net: Anatomy-aware lateral ventricle shape modeling with a case study   on Alzheimerâ€™s disease"></a>LV-Net: Anatomy-aware lateral ventricle shape modeling with a case study   on Alzheimerâ€™s disease</h2><p><strong>Authors:Wonjung Park, Suhyun Ahn, Jinah Park</strong></p>
<p>Lateral ventricle (LV) shape analysis holds promise as a biomarker for neurological diseases; however, challenges remain due to substantial shape variability across individuals and segmentation difficulties arising from limited MRI resolution. We introduce LV-Net, a novel framework for producing individualized 3D LV meshes from brain MRI by deforming an anatomy-aware joint LV-hippocampus template mesh. By incorporating anatomical relationships embedded within the joint template, LV-Net reduces boundary segmentation artifacts and improves reconstruction robustness. In addition, by classifying the vertices of the template mesh based on their anatomical adjacency, our method enhances point correspondence across subjects, leading to more accurate LV shape statistics. We demonstrate that LV-Net achieves superior reconstruction accuracy, even in the presence of segmentation imperfections, and delivers more reliable shape descriptors across diverse datasets. Finally, we apply LV-Net to Alzheimerâ€™s disease analysis, identifying LV subregions that show significantly associations with the disease relative to cognitively normal controls. The codes for LV shape modeling are available at <a target="_blank" rel="noopener" href="https://github.com/PWonjung/LV_Shape_Modeling">https://github.com/PWonjung/LV_Shape_Modeling</a>. </p>
<blockquote>
<p>ä¾§è„‘å®¤ï¼ˆLVï¼‰å½¢æ€åˆ†æä½œä¸ºç¥ç»ç–¾ç—…çš„ç”Ÿç‰©æ ‡å¿—ç‰©å…·æœ‰å¹¿é˜”å‰æ™¯ï¼Œä½†ç”±äºä¸ªä½“é—´å½¢æ€å·®å¼‚è¾ƒå¤§ä»¥åŠMRIåˆ†è¾¨ç‡æœ‰é™å¯¼è‡´çš„åˆ†å‰²å›°éš¾ï¼Œä»å­˜åœ¨æŒ‘æˆ˜ã€‚æˆ‘ä»¬å¼•å…¥äº†LV-Netï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œé€šè¿‡å˜å½¢ä¸€ä¸ªäº†è§£ç»“æ„çš„è”åˆLV-æµ·é©¬æ¨¡æ¿ç½‘æ ¼ï¼Œä»è„‘éƒ¨MRIç”Ÿæˆä¸ªæ€§åŒ–çš„3D LVç½‘æ ¼ã€‚é€šè¿‡èå…¥è”åˆæ¨¡æ¿å†…çš„ç»“æ„å…³ç³»ï¼ŒLV-Netå‡å°‘äº†è¾¹ç•Œåˆ†å‰²ä¼ªå½±ï¼Œæé«˜äº†é‡å»ºçš„ç¨³å¥æ€§ã€‚æ­¤å¤–ï¼Œé€šè¿‡åŸºäºå…¶ç»“æ„é‚»æ¥å¯¹æ¨¡æ¿ç½‘æ ¼çš„é¡¶ç‚¹è¿›è¡Œåˆ†ç±»ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¢å¼ºäº†ä¸åŒå—è¯•è€…é—´çš„ç‚¹å¯¹åº”å…³ç³»ï¼Œä»è€Œå¾—åˆ°æ›´å‡†ç¡®çš„LVå½¢æ€ç»Ÿè®¡ã€‚æˆ‘ä»¬è¯æ˜ï¼Œå³ä½¿åœ¨å­˜åœ¨åˆ†å‰²ç¼ºé™·çš„æƒ…å†µä¸‹ï¼ŒLV-Netä¹Ÿèƒ½å®ç°è¾ƒé«˜çš„é‡å»ºç²¾åº¦ï¼Œå¹¶åœ¨å„ç§æ•°æ®é›†ä¸­æä¾›æ›´å¯é çš„å½¢çŠ¶æè¿°ç¬¦ã€‚æœ€åï¼Œæˆ‘ä»¬å°†LV-Netåº”ç”¨äºé˜¿å°”èŒ¨æµ·é»˜ç—…åˆ†æï¼Œè¯†åˆ«å‡ºä¸è®¤çŸ¥æ­£å¸¸å¯¹ç…§ç›¸æ¯”ï¼Œä¸ç–¾ç—…æ˜¾è‘—ç›¸å…³çš„LVå­åŒºåŸŸã€‚LVå½¢æ€å»ºæ¨¡çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/PWonjung/LV_Shape_Modeling%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/PWonjung/LV_Shape_Modelingæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06055v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºåŒ»å­¦å›¾åƒçš„ç ”ç©¶ï¼ŒLVï¼ˆä¾§è„‘å®¤ï¼‰å½¢çŠ¶åˆ†æä½œä¸ºç¥ç»æ€§ç–¾ç—…çš„ç”Ÿç‰©æ ‡å¿—ç‰©å…·æœ‰æ½œåŠ›ï¼Œä½†ä»ç„¶å­˜åœ¨æŒ‘æˆ˜ï¼Œå¦‚ä¸ªä½“é—´å½¢çŠ¶å·®å¼‚å¤§ä»¥åŠMRIåˆ†è¾¨ç‡é™åˆ¶å¯¼è‡´çš„åˆ†å‰²å›°éš¾ã€‚æœ¬æ–‡å¼•å…¥LV-Netæ¡†æ¶ï¼Œé€šè¿‡å˜å½¢è§£å‰–è”åˆLV-æµ·é©¬æ¨¡æ¿ç½‘æ ¼ï¼Œä»è„‘éƒ¨MRIç”Ÿæˆä¸ªæ€§åŒ–3D LVç½‘æ ¼ã€‚LV-Netåˆ©ç”¨è”åˆæ¨¡æ¿ä¸­çš„è§£å‰–å…³ç³»ï¼Œå‡å°‘äº†è¾¹ç•Œåˆ†å‰²ä¼ªå½±ï¼Œæé«˜äº†é‡å»ºçš„ç¨³å¥æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•é€šè¿‡å¯¹æ¨¡æ¿ç½‘æ ¼çš„é¡¶ç‚¹æŒ‰å…¶è§£å‰–éƒ¨ä½è¿›è¡Œåˆ†ç±»ï¼Œå¢å¼ºäº†è·¨ä¸»ä½“çš„ç‚¹å¯¹åº”å…³ç³»ï¼Œä»è€Œè·å¾—æ›´å‡†ç¡®çš„LVå½¢çŠ¶ç»Ÿè®¡ä¿¡æ¯ã€‚å®éªŒè¡¨æ˜ï¼ŒLV-Netå³ä½¿åœ¨å­˜åœ¨åˆ†å‰²ç¼ºé™·çš„æƒ…å†µä¸‹ä¹Ÿèƒ½å®ç°è¾ƒé«˜çš„é‡å»ºç²¾åº¦ï¼Œå¹¶ä¸”åœ¨ä¸åŒçš„æ•°æ®é›†ä¸­æä¾›æ›´å¯é çš„å½¢çŠ¶æè¿°ã€‚æœ€åï¼ŒLV-Netåœ¨é˜¿å°”èŒ¨æµ·é»˜ç—…åˆ†æä¸­çš„åº”ç”¨ï¼Œè¯†åˆ«å‡ºä¸æ­£å¸¸è®¤çŸ¥å¯¹ç…§ç»„ç›¸æ¯”ï¼Œä¸ç–¾ç—…å¯†åˆ‡ç›¸å…³çš„LVå­åŒºåŸŸã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LVï¼ˆä¾§è„‘å®¤ï¼‰å½¢çŠ¶åˆ†æä½œä¸ºç¥ç»æ€§ç–¾ç—…çš„ç”Ÿç‰©æ ‡å¿—ç‰©å…·æœ‰æ½œåŠ›ï¼Œä½†ä»é¢ä¸´ä¸ªä½“é—´å½¢çŠ¶å·®å¼‚åŠMRIåˆ†è¾¨ç‡é™åˆ¶çš„æŒ‘æˆ˜ã€‚</li>
<li>LV-Netæ¡†æ¶èƒ½å¤Ÿç”Ÿæˆä¸ªæ€§åŒ–çš„3D LVç½‘æ ¼ï¼Œé€šè¿‡å˜å½¢è§£å‰–è”åˆLV-æµ·é©¬æ¨¡æ¿ç½‘æ ¼å®ç°ã€‚</li>
<li>LV-Netåˆ©ç”¨è§£å‰–å…³ç³»å‡å°‘è¾¹ç•Œåˆ†å‰²ä¼ªå½±ï¼Œæé«˜é‡å»ºç¨³å¥æ€§ã€‚</li>
<li>é€šè¿‡åˆ†ç±»æ¨¡æ¿ç½‘æ ¼çš„é¡¶ç‚¹ï¼ŒLV-Netå¢å¼ºè·¨ä¸»ä½“ç‚¹å¯¹åº”å…³ç³»ï¼Œè·å¾—æ›´å‡†ç¡®LVå½¢çŠ¶ç»Ÿè®¡ã€‚</li>
<li>LV-Netå®ç°é«˜é‡å»ºç²¾åº¦ï¼Œå³ä½¿åœ¨å­˜åœ¨åˆ†å‰²ç¼ºé™·çš„æƒ…å†µä¸‹ã€‚</li>
<li>LV-Netæä¾›å¯é å½¢çŠ¶æè¿°ï¼Œé€‚ç”¨äºä¸åŒæ•°æ®é›†ã€‚</li>
<li>LV-NetæˆåŠŸåº”ç”¨äºé˜¿å°”èŒ¨æµ·é»˜ç—…åˆ†æï¼Œè¯†åˆ«å‡ºä¸ç–¾ç—…ç›¸å…³çš„LVå­åŒºåŸŸã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06055">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-33c0526247fb38285ada9022f7d84318.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cbaaee67cc94ff96ddb71af6a305f93e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1210d3c13b4375a4d2010cbd19704150.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-77d5acbe19955fed079a0ed47b1db8eb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b66ea3cfcfafc6271a4766387036dd0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Discriminating-Distal-Ischemic-Stroke-from-Seizure-Induced-Stroke-Mimics-Using-Dynamic-Susceptibility-Contrast-MRI"><a href="#Discriminating-Distal-Ischemic-Stroke-from-Seizure-Induced-Stroke-Mimics-Using-Dynamic-Susceptibility-Contrast-MRI" class="headerlink" title="Discriminating Distal Ischemic Stroke from Seizure-Induced Stroke Mimics   Using Dynamic Susceptibility Contrast MRI"></a>Discriminating Distal Ischemic Stroke from Seizure-Induced Stroke Mimics   Using Dynamic Susceptibility Contrast MRI</h2><p><strong>Authors:Marijn Borghouts, Richard McKinley, Manuel KÃ¶stner, Josien Pluim, Roland Wiest, Ruisheng Su</strong></p>
<p>Distinguishing acute ischemic strokes (AIS) from stroke mimics (SMs), particularly in cases involving medium and small vessel occlusions, remains a significant diagnostic challenge. While computed tomography (CT) based protocols are commonly used in emergency settings, their sensitivity for detecting distal occlusions is limited. This study explores the potential of magnetic resonance perfusion (MRP) imaging as a tool for differentiating distal AIS from epileptic seizures, a prevalent SM. Using a retrospective dataset of 162 patients (129 AIS, 33 seizures), we extracted region-wise perfusion map descriptors (PMDs) from dynamic susceptibility contrast (DSC) images. Statistical analyses identified several brain regions, located mainly in the temporal and occipital lobe, exhibiting significant group differences in certain PMDs. Hemispheric asymmetry analyses further highlighted these regions as discriminative. A logistic regression model trained on PMDs achieved an area under the receiver operating characteristic (AUROC) curve of 0.90, and an area under the precision recall curve (AUPRC) of 0.74, with a specificity of 92% and a sensitivity of 73%, suggesting strong performance in distinguishing distal AIS from seizures. These findings support further exploration of MRP-based PMDs as interpretable features for distinguishing true strokes from various mimics. The code is openly available at our GitHub <a target="_blank" rel="noopener" href="https://github.com/Marijn311/PMD_extraction_and_analysis%7Bgithub.com/Marijn311/PMD/_extraction/_and/_analysis">https://github.com/Marijn311/PMD_extraction_and_analysis{github.com/Marijn311/PMD\_extraction\_and\_analysis</a> </p>
<blockquote>
<p>åœ¨æ€¥æ€§ç¼ºè¡€æ€§ä¸­é£ï¼ˆAISï¼‰ä¸ä¸­é£æ¨¡ä»¿è€…ï¼ˆSMsï¼‰ä¹‹é—´è¿›è¡ŒåŒºåˆ†ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¶‰åŠä¸­ã€å°è¡€ç®¡é—­å¡çš„æƒ…å†µä¸‹ï¼Œä»ç„¶æ˜¯ä¸€ä¸ªé‡è¦çš„è¯Šæ–­æŒ‘æˆ˜ã€‚è™½ç„¶åŸºäºè®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰çš„åè®®åœ¨ç´§æ€¥æƒ…å†µä¸‹å¸¸ç”¨ï¼Œä½†å…¶åœ¨æ£€æµ‹è¿œç«¯é—­å¡æ–¹é¢çš„æ•æ„Ÿæ€§æœ‰é™ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†ç£å…±æŒ¯çŒæ³¨ï¼ˆMRPï¼‰æˆåƒåœ¨åŒºåˆ†è¿œç«¯AISä¸å¸¸è§SMç™«ç—«å‘ä½œæ–¹é¢çš„æ½œåŠ›ã€‚é€šè¿‡å›é¡¾åˆ†æåŒ…å«129ä¾‹AISå’Œ33ä¾‹ç™«ç—«å‘ä½œçš„162ä¾‹æ‚£è€…æ•°æ®é›†ï¼Œæˆ‘ä»¬ä»åŠ¨æ€æ˜“æ„Ÿæ€§å¯¹æ¯”å›¾åƒä¸­æå–åŒºåŸŸçŒæ³¨æ˜ å°„æè¿°ç¬¦ï¼ˆPMDsï¼‰ã€‚ç»Ÿè®¡åˆ†æå‘ç°ä¸»è¦ä½äºé¢å¶å’Œæ•å¶çš„æŸäº›å¤§è„‘åŒºåŸŸåœ¨æŸäº›PMDsä¸Šå­˜åœ¨æ˜¾è‘—çš„ç¾¤ä½“å·®å¼‚ã€‚åŠçƒä¸å¯¹ç§°åˆ†æè¿›ä¸€æ­¥å¼ºè°ƒäº†è¿™äº›åŒºåŸŸçš„é‰´åˆ«ä½œç”¨ã€‚ä½¿ç”¨PMDè®­ç»ƒçš„é€»è¾‘å›å½’æ¨¡å‹åœ¨å—è¯•è€…å·¥ä½œç‰¹å¾æ›²çº¿ï¼ˆAUROCï¼‰ä¸‹çš„é¢ç§¯è¾¾åˆ°0.90ï¼Œåœ¨ç²¾ç¡®åº¦å¬å›æ›²çº¿ï¼ˆAUPRCï¼‰ä¸‹çš„é¢ç§¯è¾¾åˆ°0.74ï¼Œç‰¹å¼‚åº¦ä¸º92%ï¼Œçµæ•åº¦ä¸º73%ï¼Œåœ¨åŒºåˆ†è¿œç«¯AISä¸ç™«ç—«å‘ä½œæ–¹é¢è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚è¿™äº›å‘ç°æ”¯æŒè¿›ä¸€æ­¥æ¢ç´¢åŸºäºMRPçš„PMDä½œä¸ºåŒºåˆ†çœŸå®ä¸­é£ä¸å„ç§æ¨¡ä»¿è€…çš„å¯è§£é‡Šç‰¹å¾ã€‚ä»£ç å…¬å¼€åœ¨æˆ‘ä»¬çš„GitHubä¸Šå¯ç”¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/Marijn311/PMD_extraction_and_analysis">github.com&#x2F;Marijn311&#x2F;PMD_extraction_and_analysis</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04404v2">PDF</a> Accepted to SWITCH2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶æ¢è®¨äº†ç£å…±æŒ¯çŒæ³¨ï¼ˆMRPï¼‰æˆåƒåœ¨åŒºåˆ†è¿œç«¯æ€¥æ€§ç¼ºè¡€æ€§è„‘å’ä¸­ï¼ˆAISï¼‰ä¸å¸¸è§çš„å’ä¸­æ¨¡ä»¿ï¼ˆSMï¼‰â€”â€”ç™«ç—«å‘ä½œä¸ºä¾‹çš„æ½œåŠ›ã€‚é€šè¿‡å¯¹åŠ¨æ€ç£æ•æ„Ÿå¯¹æ¯”ï¼ˆDSCï¼‰å›¾åƒçš„åŒºåŸŸçŒæ³¨å›¾æè¿°ç¬¦ï¼ˆPMDsï¼‰çš„åˆ†æï¼Œç ”ç©¶åœ¨ä½äºé¢å¶å’Œæ•å¶ç­‰ä¸»è¦åŒºåŸŸçš„ç‰¹å®šPMDsè¡¨ç°å‡ºæ˜¾è‘—çš„ç»„é—´å·®å¼‚ã€‚ä¸€ä¸ªåŸºäºPMDsçš„é€»è¾‘å›å½’æ¨¡å‹åœ¨åŒºåˆ†è¿œç«¯AISä¸ç™«ç—«å‘ä½œæ–¹é¢è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ï¼Œå…¶ROCæ›²çº¿ä¸‹çš„é¢ç§¯ä¸º0.90ï¼Œç²¾ç¡®åº¦å¬å›æ›²çº¿ä¸‹çš„é¢ç§¯ä¸º0.74ï¼Œç‰¹å¼‚åº¦ä¸º92%ï¼Œæ•æ„Ÿåº¦ä¸º73%ã€‚è¿™ä¸ºè¿›ä¸€æ­¥æ¢ç´¢åŸºäºMRPçš„PMDsä½œä¸ºåŒºåˆ†çœŸå®å’ä¸­ä¸å„ç§æ¨¡ä»¿çš„å¯è§£é‡Šç‰¹å¾æä¾›äº†æ”¯æŒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ€¥æ€§ç¼ºè¡€æ€§è„‘å’ä¸­ï¼ˆAISï¼‰ä¸å’ä¸­æ¨¡ä»¿ï¼ˆSMï¼‰ä¹‹é—´çš„è¯Šæ–­ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¶‰åŠä¸­å°è¡€ç®¡é—­å¡çš„æƒ…å†µä¸‹ã€‚</li>
<li>è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰åœ¨æ£€æµ‹è¿œç«¯é—­å¡æ–¹é¢çš„çµæ•åº¦æœ‰é™ã€‚</li>
<li>æœ¬ç ”ç©¶ä½¿ç”¨ç£å…±æŒ¯çŒæ³¨ï¼ˆMRPï¼‰æˆåƒæ¥åŒºåˆ†è¿œç«¯AISå’Œå¸¸è§çš„SMâ€”â€”ç™«ç—«å‘ä½œã€‚</li>
<li>é€šè¿‡åŠ¨æ€ç£æ•æ„Ÿå¯¹æ¯”ï¼ˆDSCï¼‰å›¾åƒåˆ†æï¼Œå‘ç°æŸäº›å¤§è„‘åŒºåŸŸçš„çŒæ³¨å›¾æè¿°ç¬¦ï¼ˆPMDsï¼‰å­˜åœ¨æ˜¾è‘—çš„ç»„é—´å·®å¼‚ã€‚</li>
<li>è¿™äº›å·®å¼‚åœ¨é¢å¶å’Œæ•å¶ç­‰ä¸»è¦åŒºåŸŸå°¤ä¸ºæ˜æ˜¾ã€‚</li>
<li>åŸºäºPMDsçš„é€»è¾‘å›å½’æ¨¡å‹åœ¨åŒºåˆ†è¿œç«¯AISä¸ç™«ç—«å‘ä½œæ–¹é¢è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ï¼ŒROCæ›²çº¿ä¸‹çš„é¢ç§¯ä¸º0.90ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04404">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-bce5b49a639e97b110dfcc487a5fe8a2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6b795dafe913f2a29bc42196e705d42a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8432043833d4cc8e8270253d155b091f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-828cdd3087da5565b3ef7747149e2388.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Cross-Modality-Masked-Learning-for-Survival-Prediction-in-ICI-Treated-NSCLC-Patients"><a href="#Cross-Modality-Masked-Learning-for-Survival-Prediction-in-ICI-Treated-NSCLC-Patients" class="headerlink" title="Cross-Modality Masked Learning for Survival Prediction in ICI Treated   NSCLC Patients"></a>Cross-Modality Masked Learning for Survival Prediction in ICI Treated   NSCLC Patients</h2><p><strong>Authors:Qilong Xing, Zikai Song, Bingxin Gong, Lian Yang, Junqing Yu, Wei Yang</strong></p>
<p>Accurate prognosis of non-small cell lung cancer (NSCLC) patients undergoing immunotherapy is essential for personalized treatment planning, enabling informed patient decisions, and improving both treatment outcomes and quality of life. However, the lack of large, relevant datasets and effective multi-modal feature fusion strategies pose significant challenges in this domain. To address these challenges, we present a large-scale dataset and introduce a novel framework for multi-modal feature fusion aimed at enhancing the accuracy of survival prediction. The dataset comprises 3D CT images and corresponding clinical records from NSCLC patients treated with immune checkpoint inhibitors (ICI), along with progression-free survival (PFS) and overall survival (OS) data. We further propose a cross-modality masked learning approach for medical feature fusion, consisting of two distinct branches, each tailored to its respective modality: a Slice-Depth Transformer for extracting 3D features from CT images and a graph-based Transformer for learning node features and relationships among clinical variables in tabular data. The fusion process is guided by a masked modality learning strategy, wherein the model utilizes the intact modality to reconstruct missing components. This mechanism improves the integration of modality-specific features, fostering more effective inter-modality relationships and feature interactions. Our approach demonstrates superior performance in multi-modal integration for NSCLC survival prediction, surpassing existing methods and setting a new benchmark for prognostic models in this context. </p>
<blockquote>
<p>å¯¹æ¥å—å…ç–«æ²»ç–—çš„éå°ç»†èƒè‚ºç™Œï¼ˆNSCLCï¼‰æ‚£è€…è¿›è¡Œå‡†ç¡®çš„é¢„åè¯„ä¼°å¯¹äºä¸ªæ€§åŒ–æ²»ç–—è®¡åˆ’çš„åˆ¶å®šã€æ‚£è€…å†³ç­–çš„çŸ¥æƒ…ä»¥åŠæ”¹å–„æ²»ç–—ç»“æœå’Œç”Ÿæ´»è´¨é‡è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç¼ºä¹ç›¸å…³çš„å¤§å‹æ•°æ®é›†å’Œæœ‰æ•ˆçš„å¤šæ¨¡å¼ç‰¹å¾èåˆç­–ç•¥ï¼Œç»™è¿™ä¸€é¢†åŸŸå¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„æ•°æ®é›†ï¼Œå¹¶ä»‹ç»äº†ä¸€ä¸ªæ—¨åœ¨æé«˜ç”Ÿå­˜é¢„æµ‹å‡†ç¡®æ€§çš„å¤šæ¨¡å¼ç‰¹å¾èåˆçš„æ–°æ¡†æ¶ã€‚æ•°æ®é›†åŒ…å«æ¥å—å…ç–«æ£€æŸ¥ç‚¹æŠ‘åˆ¶å‰‚ï¼ˆICIï¼‰æ²»ç–—çš„NSCLCæ‚£è€…çš„3D CTå›¾åƒå’Œç›¸åº”çš„ä¸´åºŠè®°å½•ï¼Œä»¥åŠæ— è¿›å±•ç”Ÿå­˜ï¼ˆPFSï¼‰å’Œæ€»ç”Ÿå­˜ï¼ˆOSï¼‰æ•°æ®ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§ç”¨äºåŒ»å­¦ç‰¹å¾èåˆçš„è·¨æ¨¡æ€æ©è†œå­¦ä¹ æ–¹æ³•ï¼ŒåŒ…æ‹¬ä¸¤ä¸ªé’ˆå¯¹å…¶å„è‡ªæ¨¡æ€çš„åˆ†æ”¯ï¼šä¸€ä¸ªåˆ‡ç‰‡æ·±åº¦è½¬æ¢å™¨ï¼Œç”¨äºä»CTå›¾åƒä¸­æå–3Dç‰¹å¾ï¼›ä¸€ä¸ªåŸºäºå›¾çš„è½¬æ¢å™¨ï¼Œç”¨äºå­¦ä¹ èŠ‚ç‚¹ç‰¹å¾å’Œè¡¨æ ¼æ•°æ®ä¸­çš„ä¸´åºŠå˜é‡ä¹‹é—´çš„å…³ç³»ã€‚èåˆè¿‡ç¨‹ç”±æ©è†œæ¨¡æ€å­¦ä¹ ç­–ç•¥å¼•å¯¼ï¼Œè¯¥ç­–ç•¥ä½¿æ¨¡å‹åˆ©ç”¨å®Œæ•´æ¨¡æ€æ¥é‡å»ºç¼ºå¤±ç»„ä»¶ã€‚è¿™ç§æœºåˆ¶æ”¹å–„äº†æ¨¡æ€ç‰¹å®šç‰¹å¾çš„é›†æˆï¼Œä¿ƒè¿›äº†æ›´æœ‰æ•ˆçš„è·¨æ¨¡æ€å…³ç³»å’Œç‰¹å¾äº¤äº’ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨NSCLCç”Ÿå­˜é¢„æµ‹çš„å¤šæ¨¡å¼é›†æˆä¸­å±•ç¤ºäº†å“è¶Šçš„æ€§èƒ½ï¼Œè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œä¸ºè¿™ä¸€èƒŒæ™¯ä¸‹çš„é¢„åæ¨¡å‹è®¾å®šäº†æ–°çš„åŸºå‡†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06994v2">PDF</a> MICCAI 2025</p>
<p><strong>æ‘˜è¦</strong><br>è‚ºç™Œç²¾å‡†é¢„åé¢„æµ‹åœ¨ä¸ªæ€§åŒ–æ²»ç–—ã€æ‚£è€…å†³ç­–ä»¥åŠæ²»ç–—æˆæœä¸è´¨é‡æå‡ç­‰æ–¹é¢éƒ½å…·æœ‰é‡è¦ä½œç”¨ã€‚é¢å¯¹ç›¸å…³æ•°æ®é›†ç¼ºå¤±ä»¥åŠå¤šæ¨¡æ€ç‰¹å¾èåˆç­–ç•¥çš„å±€é™æ€§ç­‰æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå¤§å‹æ•°æ®é›†ä¸æ–°å‹å¤šæ¨¡æ€ç‰¹å¾èåˆæ¡†æ¶æ¥æé«˜ç”Ÿå­˜é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚æ•°æ®é›†æ¶µç›–äº†æ¥å—å…ç–«æ£€æŸ¥ç‚¹æŠ‘åˆ¶å‰‚æ²»ç–—çš„éå°ç»†èƒè‚ºç™Œæ‚£è€…çš„3D CTå›¾åƒã€ç›¸å…³ä¸´åºŠè®°å½•ä»¥åŠæ— è¿›å±•ç”Ÿå­˜å’Œæ€»ç”Ÿå­˜æ•°æ®ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†è·¨æ¨¡æ€æ©è†œå­¦ä¹ æ–¹æ³•è¿›è¡ŒåŒ»å­¦ç‰¹å¾èåˆï¼ŒåŒ…æ‹¬é’ˆå¯¹ä¸¤ç§ä¸åŒæ¨¡æ€çš„ä¸“é—¨åˆ†æ”¯ï¼šç”¨äºä»CTå›¾åƒä¸­æå–3Dç‰¹å¾çš„Slice-Depth Transformerï¼Œä»¥åŠç”¨äºå­¦ä¹ ä¸´åºŠå˜é‡èŠ‚ç‚¹ç‰¹å¾å’Œå…³ç³»çš„å›¾åŸºTransformerã€‚èåˆè¿‡ç¨‹ç”±æ©è†œæ¨¡æ€å­¦ä¹ ç­–ç•¥å¼•å¯¼ï¼Œè¯¥ç­–ç•¥ä½¿æ¨¡å‹åˆ©ç”¨å®Œæ•´æ¨¡æ€æ¥é‡å»ºç¼ºå¤±éƒ¨åˆ†ï¼Œæé«˜äº†æ¨¡æ€ç‰¹å®šç‰¹å¾çš„é›†æˆï¼Œä¿ƒè¿›äº†è·¨æ¨¡æ€çš„æœ‰æ•ˆå…³ç³»å’Œç‰¹å¾äº¤äº’ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨NSCLCç”Ÿå­˜é¢„æµ‹çš„å¤šæ¨¡æ€èåˆä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œä¸ºè¿™ä¸€é¢†åŸŸçš„é¢„åæ¨¡å‹è®¾å®šäº†æ–°çš„åŸºå‡†ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>éå°ç»†èƒè‚ºç™Œçš„ç²¾å‡†é¢„åé¢„æµ‹å¯¹ä¸ªæ€§åŒ–æ²»ç–—ä¸æ‚£è€…å†³ç­–è‡³å…³é‡è¦ã€‚</li>
<li>ç¼ºä¹å¤§å‹ç›¸å…³æ•°æ®é›†å’Œå¤šæ¨¡æ€ç‰¹å¾èåˆç­–ç•¥æ˜¯å½“å‰çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§å¤§å‹æ•°æ®é›†ï¼ŒåŒ…å«æ¥å—å…ç–«æ£€æŸ¥ç‚¹æŠ‘åˆ¶å‰‚æ²»ç–—çš„NSCLCæ‚£è€…çš„3Då›¾åƒå’Œä¸´åºŠè®°å½•ã€‚</li>
<li>ä»‹ç»äº†ä¸€ç§æ–°å‹å¤šæ¨¡æ€ç‰¹å¾èåˆæ¡†æ¶ï¼Œæ—¨åœ¨æé«˜ç”Ÿå­˜é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚</li>
<li>æå‡ºäº†è·¨æ¨¡æ€æ©è†œå­¦ä¹ æ–¹æ³•è¿›è¡ŒåŒ»å­¦ç‰¹å¾èåˆï¼ŒåŒ…æ‹¬Slice-Depth Transformerå’Œå›¾åŸºTransformerã€‚</li>
<li>èåˆè¿‡ç¨‹é‡‡ç”¨æ©è†œæ¨¡æ€å­¦ä¹ ç­–ç•¥ï¼Œæé«˜äº†æ¨¡æ€ç‰¹å¾çš„é›†æˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06994">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-246ea51e25e85e52ed37d318368844f2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f6a91c6b1ccc6d77ad6e8601895211e4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42ad508ede0e6fb7a61e6d81ea77d246.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="MCA-RG-Enhancing-LLMs-with-Medical-Concept-Alignment-for-Radiology-Report-Generation"><a href="#MCA-RG-Enhancing-LLMs-with-Medical-Concept-Alignment-for-Radiology-Report-Generation" class="headerlink" title="MCA-RG: Enhancing LLMs with Medical Concept Alignment for Radiology   Report Generation"></a>MCA-RG: Enhancing LLMs with Medical Concept Alignment for Radiology   Report Generation</h2><p><strong>Authors:Qilong Xing, Zikai Song, Youjia Zhang, Na Feng, Junqing Yu, Wei Yang</strong></p>
<p>Despite significant advancements in adapting Large Language Models (LLMs) for radiology report generation (RRG), clinical adoption remains challenging due to difficulties in accurately mapping pathological and anatomical features to their corresponding text descriptions. Additionally, semantic agnostic feature extraction further hampers the generation of accurate diagnostic reports. To address these challenges, we introduce Medical Concept Aligned Radiology Report Generation (MCA-RG), a knowledge-driven framework that explicitly aligns visual features with distinct medical concepts to enhance the report generation process. MCA-RG utilizes two curated concept banks: a pathology bank containing lesion-related knowledge, and an anatomy bank with anatomical descriptions. The visual features are aligned with these medical concepts and undergo tailored enhancement. We further propose an anatomy-based contrastive learning procedure to improve the generalization of anatomical features, coupled with a matching loss for pathological features to prioritize clinically relevant regions. Additionally, a feature gating mechanism is employed to filter out low-quality concept features. Finally, the visual features are corresponding to individual medical concepts, and are leveraged to guide the report generation process. Experiments on two public benchmarks (MIMIC-CXR and CheXpert Plus) demonstrate that MCA-RG achieves superior performance, highlighting its effectiveness in radiology report generation. </p>
<blockquote>
<p>å°½ç®¡åœ¨å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€‚åº”äºæ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆï¼ˆRRGï¼‰æ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†ç”±äºå°†ç—…ç†å’Œè§£å‰–ç‰¹å¾å‡†ç¡®æ˜ å°„åˆ°ç›¸åº”çš„æ–‡æœ¬æè¿°ä¸­çš„å›°éš¾ï¼Œå…¶åœ¨ä¸´åºŠä¸Šçš„åº”ç”¨ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æ­¤å¤–ï¼Œè¯­ä¹‰æ— å…³çš„ç‰¹å¾æå–è¿›ä¸€æ­¥é˜»ç¢äº†å‡†ç¡®è¯Šæ–­æŠ¥å‘Šçš„ç”Ÿæˆã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŒ»å­¦æ¦‚å¿µå¯¹é½æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆï¼ˆMCA-RGï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªçŸ¥è¯†é©±åŠ¨æ¡†æ¶ï¼Œé€šè¿‡æ˜ç¡®åœ°å°†è§†è§‰ç‰¹å¾ä¸ç‹¬ç‰¹çš„åŒ»å­¦æ¦‚å¿µå¯¹é½ï¼Œä»¥å¢å¼ºæŠ¥å‘Šç”Ÿæˆè¿‡ç¨‹ã€‚MCA-RGåˆ©ç”¨ä¸¤ä¸ªç²¾é€‰çš„æ¦‚å¿µåº“ï¼šä¸€ä¸ªåŒ…å«ç—…å˜ç›¸å…³çŸ¥è¯†çš„ç—…ç†åº“å’Œä¸€ä¸ªåŒ…å«è§£å‰–æè¿°çš„è§£å‰–åº“ã€‚è§†è§‰ç‰¹å¾ä¸è¿™äº›åŒ»å­¦æ¦‚å¿µå¯¹é½ï¼Œå¹¶è¿›è¡Œæœ‰é’ˆå¯¹æ€§çš„å¢å¼ºã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ä¸€ç§åŸºäºè§£å‰–å­¦çš„å¯¹æ¯”å­¦ä¹ ç¨‹åºï¼Œä»¥æé«˜è§£å‰–ç‰¹å¾çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶ç»“åˆç—…ç†ç‰¹å¾çš„åŒ¹é…æŸå¤±æ¥ä¼˜å…ˆå¤„ç†ä¸´åºŠç›¸å…³åŒºåŸŸã€‚æ­¤å¤–ï¼Œè¿˜é‡‡ç”¨äº†ç‰¹å¾é—¨æ§æœºåˆ¶æ¥è¿‡æ»¤æ‰ä½è´¨é‡çš„æ¦‚å¿µç‰¹å¾ã€‚æœ€åï¼Œå°†è§†è§‰ç‰¹å¾ä¸å•ä¸ªåŒ»å­¦æ¦‚å¿µç›¸å¯¹åº”ï¼Œå¹¶ç”¨äºæŒ‡å¯¼æŠ¥å‘Šç”Ÿæˆè¿‡ç¨‹ã€‚åœ¨ä¸¤ä¸ªå…¬å…±åŸºå‡†æµ‹è¯•ï¼ˆMIMIC-CXRå’ŒCheXpert Plusï¼‰ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMCA-RGå–å¾—äº†å“è¶Šçš„æ€§èƒ½ï¼Œçªæ˜¾å…¶åœ¨æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06992v2">PDF</a> MICCAI 2025</p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒæŠ¥å‘Šç”Ÿæˆé¢†åŸŸè™½ç„¶å·²æœ‰å¤§å‹è¯­è¨€æ¨¡å‹çš„åº”ç”¨å°è¯•ï¼Œä½†ç”±äºç—…ç†å’Œè§£å‰–ç‰¹å¾ä¸æ–‡æœ¬æè¿°ä¹‹é—´çš„æ˜ å°„å›°éš¾ä»¥åŠè¯­ä¹‰æ— å…³çš„ç‰¹å¾æå–é—®é¢˜ï¼Œä¸´åºŠé‡‡çº³ä»é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºåŒ»å­¦æ¦‚å¿µå¯¹é½çš„æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆæ¡†æ¶ï¼ˆMCA-RGï¼‰ï¼Œé€šè¿‡æ˜ç¡®å¯¹é½è§†è§‰ç‰¹å¾ä¸ç‰¹å®šçš„åŒ»å­¦æ¦‚å¿µæ¥å¼ºåŒ–æŠ¥å‘Šç”Ÿæˆè¿‡ç¨‹ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ä¸¤ä¸ªç²¾é€‰çš„æ¦‚å¿µåº“ï¼šåŒ…å«ç—…å˜ç›¸å…³çŸ¥è¯†çš„ç—…ç†å­¦åº“å’ŒåŒ…å«è§£å‰–å­¦æè¿°çš„è§£å‰–å­¦åº“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMCA-RGåœ¨å…¬å¼€æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜è¶Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLMsåœ¨æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆä¸­çš„åº”ç”¨é¢ä¸´ç—…ç†å’Œè§£å‰–ç‰¹å¾ä¸æ–‡æœ¬æ˜ å°„å›°éš¾çš„é—®é¢˜ã€‚</li>
<li>MCA-RGæ˜¯ä¸€ä¸ªçŸ¥è¯†é©±åŠ¨æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œé€šè¿‡æ˜ç¡®å¯¹é½è§†è§‰ç‰¹å¾ä¸åŒ»å­¦æ¦‚å¿µæ¥å¼ºåŒ–æŠ¥å‘Šç”Ÿæˆã€‚</li>
<li>æ¡†æ¶åŒ…å«ä¸¤ä¸ªæ¦‚å¿µåº“ï¼šç—…ç†å­¦åº“å’Œè§£å‰–å­¦åº“ï¼Œç”¨äºå¯¹é½è§†è§‰ç‰¹å¾ã€‚</li>
<li>è§£å‰–å­¦å¯¹æ¯”å­¦ä¹ ç¨‹åºå’ŒåŒ¹é…æŸå¤±ç”¨äºæ”¹å–„ç‰¹å¾æ³›åŒ–èƒ½åŠ›å’Œå…³æ³¨ä¸´åºŠé‡è¦åŒºåŸŸã€‚</li>
<li>ç‰¹å¾é—¨æ§æœºåˆ¶ç”¨äºè¿‡æ»¤ä½è´¨é‡æ¦‚å¿µç‰¹å¾ã€‚</li>
<li>å®éªŒç»“æœè¯æ˜MCA-RGåœ¨æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆä¸­çš„ä¼˜è¶Šæ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06992">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4ab59bf4a3c405444b804ad988e4e236.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-872b9774df4bb8a8be8b77965a2f750f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9dfa6df4a223444bbe292cc0df5aa4d3.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Physics-Driven-Autoregressive-State-Space-Models-for-Medical-Image-Reconstruction"><a href="#Physics-Driven-Autoregressive-State-Space-Models-for-Medical-Image-Reconstruction" class="headerlink" title="Physics-Driven Autoregressive State Space Models for Medical Image   Reconstruction"></a>Physics-Driven Autoregressive State Space Models for Medical Image   Reconstruction</h2><p><strong>Authors:Bilal Kabas, Fuat Arslan, Valiyeh A. Nezhad, Saban Ozturk, Emine U. Saritas, Tolga Ã‡ukur</strong></p>
<p>Medical image reconstruction from undersampled acquisitions is an ill-posed inverse problem requiring accurate recovery of anatomical structures from incomplete measurements. Physics-driven (PD) network models have gained prominence for this task by integrating data-consistency mechanisms with learned priors, enabling improved performance over purely data-driven approaches. However, reconstruction quality still hinges on the networkâ€™s ability to disentangle artifacts from true anatomical signals-both of which exhibit complex, multi-scale contextual structure. Convolutional neural networks (CNNs) capture local correlations but often struggle with non-local dependencies. While transformers aim to alleviate this limitation, practical implementations involve design compromises to reduce computational cost by balancing local and non-local sensitivity, occasionally resulting in performance comparable to CNNs. To address these challenges, we propose MambaRoll, a novel physics-driven autoregressive state space model (SSM) for high-fidelity and efficient image reconstruction. MambaRoll employs an unrolled architecture where each cascade autoregressively predicts finer-scale feature maps conditioned on coarser-scale representations, enabling consistent multi-scale context propagation. Each stage is built on a hierarchy of scale-specific PD-SSM modules that capture spatial dependencies while enforcing data consistency through residual correction. To further improve scale-aware learning, we introduce a Deep Multi-Scale Decoding (DMSD) loss, which provides supervision at intermediate spatial scales in alignment with the autoregressive design. Demonstrations on accelerated MRI and sparse-view CT reconstructions show that MambaRoll consistently outperforms state-of-the-art CNN-, transformer-, and SSM-based methods. </p>
<blockquote>
<p>ä»æ¬ é‡‡æ ·é‡‡é›†ä¸­è¿›è¡ŒåŒ»å­¦å›¾åƒé‡å»ºæ˜¯ä¸€ä¸ªä¸é€‚å®šçš„é€†é—®é¢˜ï¼Œéœ€è¦ä»ä¸å®Œå…¨çš„æµ‹é‡ä¸­å‡†ç¡®æ¢å¤è§£å‰–ç»“æ„ã€‚ç‰©ç†é©±åŠ¨ï¼ˆPDï¼‰ç½‘ç»œæ¨¡å‹é€šè¿‡æ•´åˆæ•°æ®ä¸€è‡´æ€§å’Œå­¦ä¹ å…ˆéªŒæœºåˆ¶è€Œåœ¨æ­¤ä»»åŠ¡ä¸­å´­éœ²å¤´è§’ï¼Œç›¸æ¯”çº¯ç²¹çš„æ•°æ®é©±åŠ¨æ–¹æ³•å®ç°äº†æ€§èƒ½æå‡ã€‚ç„¶è€Œï¼Œé‡å»ºè´¨é‡ä»ç„¶å–å†³äºç½‘ç»œåŒºåˆ†ä¼ªå½±å’ŒçœŸå®è§£å‰–ä¿¡å·çš„èƒ½åŠ›ï¼Œè¿™ä¸¤è€…éƒ½è¡¨ç°å‡ºå¤æ‚çš„å¤šå°ºåº¦ä¸Šä¸‹æ–‡ç»“æ„ã€‚å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰èƒ½å¤Ÿæ•æ‰å±€éƒ¨ç›¸å…³æ€§ï¼Œä½†ç»å¸¸å¯¹éå±€éƒ¨ä¾èµ–æ€§å¤„ç†å¾—åƒåŠ›ã€‚è™½ç„¶å˜å‹å™¨æ—¨åœ¨ç¼“è§£è¿™ä¸€å±€é™æ€§ï¼Œä½†å®é™…åº”ç”¨ä¸­çš„å®ç°éœ€è¦åœ¨å‡å°‘è®¡ç®—æˆæœ¬çš„åŒæ—¶å¹³è¡¡å±€éƒ¨å’Œéå±€éƒ¨æ•æ„Ÿæ€§ï¼Œå¶å°”ä¼šå¯¼è‡´æ€§èƒ½ä¸CNNç›¸å½“ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†MambaRollï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºé«˜ä¿çœŸå’Œé«˜æ•ˆå›¾åƒé‡å»ºçš„æ–°å‹ç‰©ç†é©±åŠ¨è‡ªå›å½’çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰ã€‚MambaRollé‡‡ç”¨å±•å¼€æ¶æ„ï¼Œå…¶ä¸­æ¯ä¸ªçº§è”è‡ªå›å½’é¢„æµ‹åŸºäºè¾ƒç²—ç³™å°ºåº¦è¡¨ç¤ºçš„è¾ƒç»†å°ºåº¦ç‰¹å¾å›¾ï¼Œä»è€Œå®ç°ä¸€è‡´çš„å¤šå°ºåº¦ä¸Šä¸‹æ–‡ä¼ æ’­ã€‚æ¯ä¸ªé˜¶æ®µéƒ½å»ºç«‹åœ¨å±‚æ¬¡åŒ–çš„å°ºåº¦ç‰¹å®šPD-SSMæ¨¡å—ä¸Šï¼Œè¿™äº›æ¨¡å—èƒ½å¤Ÿæ•è·ç©ºé—´ä¾èµ–æ€§ï¼ŒåŒæ—¶é€šè¿‡æ®‹å·®ä¿®æ­£æ¥å¼ºåˆ¶æ•°æ®ä¸€è‡´æ€§ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜å°ºåº¦æ„ŸçŸ¥å­¦ä¹ ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ·±åº¦å¤šå°ºåº¦è§£ç ï¼ˆDMSDï¼‰æŸå¤±ï¼Œè¯¥æŸå¤±åœ¨è‡ªå›å½’è®¾è®¡çš„ä¸­é—´ç©ºé—´å°ºåº¦ä¸Šæä¾›ç›‘ç£ã€‚åœ¨åŠ é€ŸMRIå’Œç¨€ç–è§†å›¾CTé‡å»ºçš„æ¼”ç¤ºä¸­ï¼ŒMambaRollå§‹ç»ˆä¼˜äºæœ€å…ˆè¿›çš„CNNã€å˜å‹å™¨å’ŒSSMæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.09331v3">PDF</a> 10 pages, 10 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºMambaRollçš„æ–°å‹ç‰©ç†é©±åŠ¨è‡ªå›å½’çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰ï¼Œç”¨äºé«˜ä¿çœŸå’Œé«˜æ•ˆçš„åŒ»å­¦å›¾åƒé‡å»ºã€‚è¯¥æ¨¡å‹é‡‡ç”¨è§£å·æ¶æ„ï¼Œæ¯ä¸ªé˜¶æ®µè‡ªå›å½’åœ°é¢„æµ‹æ›´ç²¾ç»†å°ºåº¦çš„ç‰¹å¾å›¾ï¼Œè¿™äº›é¢„æµ‹åŸºäºè¾ƒç²—ç³™å°ºåº¦çš„è¡¨ç¤ºï¼Œä»è€Œå®ç°ä¸€è‡´çš„å¤šå°ºåº¦ä¸Šä¸‹æ–‡ä¼ æ’­ã€‚æ¯ä¸ªé˜¶æ®µéƒ½å»ºç«‹åœ¨å±‚æ¬¡åŒ–çš„å°ºåº¦ç‰¹å®šPD-SSMæ¨¡å—ä¸Šï¼Œè¿™äº›æ¨¡å—åœ¨æ•è·ç©ºé—´ä¾èµ–æ€§çš„åŒæ—¶ï¼Œé€šè¿‡æ®‹å·®æ ¡æ­£å¼ºåˆ¶æ‰§è¡Œæ•°æ®ä¸€è‡´æ€§ã€‚ä¸ºè¿›ä¸€æ­¥æé«˜å°ºåº¦æ„ŸçŸ¥å­¦ä¹ ï¼Œå¼•å…¥æ·±åº¦å¤šå°ºåº¦è§£ç ï¼ˆDMSDï¼‰æŸå¤±ï¼Œåœ¨è‡ªå›å½’è®¾è®¡å¯¹é½çš„ä¸­é—´ç©ºé—´å°ºåº¦ä¸Šæä¾›ç›‘ç£ã€‚åœ¨åŠ é€ŸMRIå’Œç¨€ç–è§†å›¾CTé‡å»ºçš„æ¼”ç¤ºä¸­ï¼ŒMambaRollä¸€è‡´åœ°ä¼˜äºåŸºäºCNNã€transformerå’ŒSSMçš„æ–¹æ³•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒé‡å»ºæ˜¯ä»ä¸å®Œå…¨æµ‹é‡ä¸­å‡†ç¡®æ¢å¤è§£å‰–ç»“æ„çš„ä¸€ä¸ªä¸é€‚å®šé€†é—®é¢˜ã€‚</li>
<li>ç‰©ç†é©±åŠ¨ï¼ˆPDï¼‰ç½‘ç»œæ¨¡å‹é€šè¿‡æ•´åˆæ•°æ®ä¸€è‡´æ€§ä¸å­¦ä¹ å…ˆéªŒï¼Œåœ¨åŒ»å­¦å›¾åƒé‡å»ºä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ï¼ˆå¦‚å·ç§¯ç¥ç»ç½‘ç»œå’Œå˜å‹å™¨ï¼‰åœ¨è§£å†³éå±€éƒ¨ä¾èµ–æ€§å’Œå¤šå°ºåº¦ä¸Šä¸‹æ–‡ç»“æ„æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>MambaRollæ¨¡å‹é‡‡ç”¨è‡ªå›å½’çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰æ¶æ„ï¼Œæ—¨åœ¨å®ç°é«˜ä¿çœŸå’Œé«˜æ•ˆçš„å›¾åƒé‡å»ºã€‚</li>
<li>MambaRollåœ¨æ¯ä¸ªé˜¶æ®µä½¿ç”¨å±‚æ¬¡åŒ–çš„å°ºåº¦ç‰¹å®šPD-SSMæ¨¡å—ï¼Œæ•è·ç©ºé—´ä¾èµ–æ€§å¹¶å¼ºåˆ¶æ‰§è¡Œæ•°æ®ä¸€è‡´æ€§ã€‚</li>
<li>å¼•å…¥æ·±åº¦å¤šå°ºåº¦è§£ç ï¼ˆDMSDï¼‰æŸå¤±ï¼Œä»¥æ”¹è¿›å°ºåº¦æ„ŸçŸ¥å­¦ä¹ å¹¶ç¬¦åˆè‡ªå›å½’è®¾è®¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.09331">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1de9fc933a4964be4d76fba26a831bb0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0e9a09825a1dd78af938042a11c2a2a3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae5a8d36dc036d075467db1dcfc56574.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1770dcf3a4d3f41b383313db5778a6d1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b116141a43095928fe2764299ddee60.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Kinetic-Diffusion-Rotation-Algorithm-for-Dose-Estimation-in-Electron-Beam-Therapy"><a href="#Kinetic-Diffusion-Rotation-Algorithm-for-Dose-Estimation-in-Electron-Beam-Therapy" class="headerlink" title="Kinetic-Diffusion-Rotation Algorithm for Dose Estimation in Electron   Beam Therapy"></a>Kinetic-Diffusion-Rotation Algorithm for Dose Estimation in Electron   Beam Therapy</h2><p><strong>Authors:Klaas Willems, Vince Maes, Zhirui Tang, Giovanni Samaey</strong></p>
<p>Monte Carlo methods are state-of-the-art when it comes to dosimetric computations in radiotherapy. However, the execution time of these methods suffers in high-collisional regimes. We address this problem by introducing a kinetic-diffusion particle tracing scheme. This algorithm, first proposed in the context of neutral transport in fusion energy, relies on explicit simulation of the kinetic motion in low-collisional regimes and dynamically switches to motion based on a random walk in high-collisional regimes. The random walk motion maintains the first two moments (mean and variance) of the kinetic motion. We derive an analytic formula for the mean kinetic motion and discuss the addition of a multiple scattering distribution to the algorithm. In contrast to neutral transport, the radiation transfer setting does not readily admit to an analytical expression for the variance of the kinetic motion, and we therefore resort to the use of a lookup table. We test the algorithm for dosimetric computations in radiation therapy on a 2D CT scan of a lung patient. Using a simple particle model, our Python implementation of the algorithm is nearly 33 times faster than an equivalent kinetic simulation at the cost of a small modeling error. </p>
<blockquote>
<p>è’™ç‰¹å¡æ´›æ–¹æ³•åœ¨æ”¾å°„æ²»ç–—å‰‚é‡è®¡ç®—æ–¹é¢å±äºæœ€å…ˆè¿›æŠ€æœ¯ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•çš„æ‰§è¡Œæ—¶é—´åœ¨ç¢°æ’ç‡è¾ƒé«˜çš„æƒ…å†µä¸‹ä¼šå—åˆ°å¾ˆå¤§å½±å“ã€‚æˆ‘ä»¬é€šè¿‡å¼•å…¥ä¸€ç§åŠ¨åŠ›å­¦æ‰©æ•£ç²’å­è¿½è¸ªæ–¹æ¡ˆæ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚è¯¥ç®—æ³•æœ€åˆåœ¨æ ¸èšå˜èƒ½é‡ä¸­çš„ä¸­æ€§ç²’å­ä¼ è¾“èƒŒæ™¯ä¸‹æå‡ºï¼Œä¾èµ–äºåœ¨ä½ç¢°æ’ç‡ç¯å¢ƒä¸­æ˜¾å¼æ¨¡æ‹ŸåŠ¨åŠ›å­¦è¿åŠ¨ï¼Œå¹¶åœ¨é«˜ç¢°æ’ç‡ç¯å¢ƒä¸­åŠ¨æ€åˆ‡æ¢åˆ°åŸºäºéšæœºæ¸¸èµ°çš„è¿åŠ¨ã€‚éšæœºæ¸¸èµ°è¿åŠ¨ä¿æŒäº†åŠ¨åŠ›å­¦è¿åŠ¨çš„å‰ä¸¤ä¸ªçŸ©ï¼ˆå‡å€¼å’Œæ–¹å·®ï¼‰ã€‚æˆ‘ä»¬ä¸ºå¹³å‡åŠ¨åŠ›å­¦è¿åŠ¨æ¨å¯¼äº†ä¸€ä¸ªåˆ†æå…¬å¼ï¼Œå¹¶è®¨è®ºäº†å‘ç®—æ³•ä¸­æ·»åŠ å¤šé‡æ•£å°„åˆ†å¸ƒçš„é—®é¢˜ã€‚ä¸ä¸­æ€§ç²’å­ä¼ è¾“ä¸åŒï¼Œè¾å°„ä¼ è¾“è®¾ç½®ä¸ä¾¿äºæ¥å—åŠ¨åŠ›å­¦è¿åŠ¨çš„æ–¹å·®çš„åˆ†æè¡¨è¾¾å¼ï¼Œå› æ­¤æˆ‘ä»¬ä½¿ç”¨æŸ¥æ‰¾è¡¨ã€‚æˆ‘ä»¬åœ¨è‚ºéƒ¨æ‚£è€…çš„2D CTæ‰«æä¸Šæµ‹è¯•äº†è¯¥ç®—æ³•åœ¨æ”¾å°„æ²»ç–—å‰‚é‡è®¡ç®—ä¸­çš„åº”ç”¨ã€‚ä½¿ç”¨ç®€å•çš„ç²’å­æ¨¡å‹ï¼Œæˆ‘ä»¬çš„ç®—æ³•Pythonå®ç°æ¯”ç­‰æ•ˆåŠ¨åŠ›å­¦æ¨¡æ‹Ÿé€Ÿåº¦å¿«è¿‘33å€ï¼Œåªéœ€ä»˜å‡ºè¾ƒå°çš„å»ºæ¨¡è¯¯å·®ä»£ä»·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.05063v3">PDF</a> </p>
<p><strong>Summary</strong><br>     é’ˆå¯¹æ”¾ç–—ä¸­çš„å‰‚é‡è®¡ç®—ï¼Œæå‡ºä¸€ç§åŸºäºåŠ¨åŠ›å­¦æ‰©æ•£ç²’å­è¿½è¸ªæ–¹æ¡ˆçš„è’™ç‰¹å¡ç½—æ–¹æ³•æ”¹è¿›ç®—æ³•ã€‚è¯¥ç®—æ³•åœ¨ä½ç¢°æ’ä½“åˆ¶ä¸‹æ˜¾å¼æ¨¡æ‹ŸåŠ¨åŠ›å­¦è¿åŠ¨ï¼Œå¹¶åœ¨é«˜ç¢°æ’ä½“åˆ¶ä¸‹é‡‡ç”¨åŸºäºéšæœºæ¸¸èµ°çš„è¿åŠ¨ã€‚é€šè¿‡æ¨å¯¼å¹³å‡åŠ¨åŠ›å­¦è¿åŠ¨çš„è§£æå…¬å¼ï¼Œå¹¶åŠ å…¥å¤šé‡æ•£å°„åˆ†å¸ƒï¼Œåœ¨è¾å°„ä¼ è¾“ç¯å¢ƒä¸­å®ç°äº†å¿«é€Ÿä¸”ç›¸å¯¹å‡†ç¡®çš„å‰‚é‡è®¡ç®—ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è’™ç‰¹å¡ç½—æ–¹æ³•åœ¨æ”¾ç–—å‰‚é‡è®¡ç®—ä¸­å¤„äºå‰æ²¿åœ°ä½ï¼Œä½†åœ¨é«˜ç¢°æ’ä½“åˆ¶ä¸‹æ‰§è¡Œæ—¶é—´è¾ƒé•¿ã€‚</li>
<li>æå‡ºä¸€ç§åŸºäºåŠ¨åŠ›å­¦æ‰©æ•£ç²’å­è¿½è¸ªæ–¹æ¡ˆçš„ç®—æ³•ï¼Œè§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>è¯¥ç®—æ³•åœ¨ä½ç¢°æ’ä½“åˆ¶ä¸‹æ˜¾å¼æ¨¡æ‹ŸåŠ¨åŠ›å­¦è¿åŠ¨ï¼Œå¹¶åœ¨é«˜ç¢°æ’ä½“åˆ¶ä¸‹é‡‡ç”¨éšæœºæ¸¸èµ°è¿åŠ¨ã€‚</li>
<li>ç®—æ³•é€šè¿‡æ¨å¯¼å¹³å‡åŠ¨åŠ›å­¦è¿åŠ¨çš„è§£æå…¬å¼ï¼Œå¹¶åŠ å…¥å¤šé‡æ•£å°„åˆ†å¸ƒï¼Œæé«˜è®¡ç®—å‡†ç¡®æ€§ã€‚</li>
<li>è¾å°„ä¼ è¾“ç¯å¢ƒä¸æ”¯æŒå¯¹åŠ¨åŠ›å­¦è¿åŠ¨çš„æ–¹å·®è¿›è¡Œè§£æè¡¨è¾¾å¼è®¡ç®—ï¼Œå› æ­¤é‡‡ç”¨æŸ¥æ‰¾è¡¨ã€‚</li>
<li>åœ¨äºŒç»´è‚ºéƒ¨CTæ‰«æä¸Šæµ‹è¯•è¯¥ç®—æ³•è¿›è¡Œæ”¾ç–—å‰‚é‡è®¡ç®—ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.05063">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-375d598d512b74db1258117d8c679465.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-52fe3f0b5d558ed2fbfcd39872b2a201.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="From-Few-to-More-Scribble-based-Medical-Image-Segmentation-via-Masked-Context-Modeling-and-Continuous-Pseudo-Labels"><a href="#From-Few-to-More-Scribble-based-Medical-Image-Segmentation-via-Masked-Context-Modeling-and-Continuous-Pseudo-Labels" class="headerlink" title="From Few to More: Scribble-based Medical Image Segmentation via Masked   Context Modeling and Continuous Pseudo Labels"></a>From Few to More: Scribble-based Medical Image Segmentation via Masked   Context Modeling and Continuous Pseudo Labels</h2><p><strong>Authors:Zhisong Wang, Yiwen Ye, Ziyang Chen, Minglei Shu, Yanning Zhang, Yong Xia</strong></p>
<p>Scribble-based weakly supervised segmentation methods have shown promising results in medical image segmentation, significantly reducing annotation costs. However, existing approaches often rely on auxiliary tasks to enforce semantic consistency and use hard pseudo labels for supervision, overlooking the unique challenges faced by models trained with sparse annotations. These models must predict pixel-wise segmentation maps from limited data, making it crucial to handle varying levels of annotation richness effectively. In this paper, we propose MaCo, a weakly supervised model designed for medical image segmentation, based on the principle of â€œfrom few to more.â€ MaCo leverages Masked Context Modeling (MCM) and Continuous Pseudo Labels (CPL). MCM employs an attention-based masking strategy to perturb the input image, ensuring that the modelâ€™s predictions align with those of the original image. CPL converts scribble annotations into continuous pixel-wise labels by applying an exponential decay function to distance maps, producing confidence maps that represent the likelihood of each pixel belonging to a specific category, rather than relying on hard pseudo labels. We evaluate MaCo on three public datasets, comparing it with other weakly supervised methods. Our results show that MaCo outperforms competing methods across all datasets, establishing a new record in weakly supervised medical image segmentation. </p>
<blockquote>
<p>åŸºäºæ¶‚ç”»å¼±ç›‘ç£çš„åˆ†å‰²æ–¹æ³•åœ¨æ±½è½¦å›¾åƒåˆ†å‰²æ–¹é¢å±•ç°å‡ºäº†è‰¯å¥½çš„ç»“æœï¼Œæ˜¾è‘—é™ä½äº†æ ‡æ³¨æˆæœ¬ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–è¾…åŠ©ä»»åŠ¡æ¥å¼ºåˆ¶æ‰§è¡Œè¯­ä¹‰ä¸€è‡´æ€§ï¼Œå¹¶ä½¿ç”¨ç¡¬ä¼ªæ ‡ç­¾è¿›è¡Œç›‘ç£ï¼Œä»è€Œå¿½ç•¥äº†ç”¨ç¨€ç–æ³¨é‡Šè®­ç»ƒçš„æ¨¡å‹æ‰€é¢ä¸´çš„ç‹¬ç‰¹æŒ‘æˆ˜ã€‚è¿™äº›æ¨¡å‹å¿…é¡»ä»æœ‰é™æ•°æ®ä¸­é¢„æµ‹åƒç´ çº§çš„åˆ†å‰²å›¾ï¼Œå› æ­¤æœ‰æ•ˆå¤„ç†ä¸åŒçº§åˆ«çš„æ³¨é‡Šä¸°å¯Œåº¦è‡³å…³é‡è¦ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†MaCoï¼Œè¿™æ˜¯ä¸€ç§ä¸ºåŒ»å­¦å›¾åƒåˆ†å‰²è®¾è®¡çš„å¼±ç›‘ç£æ¨¡å‹ï¼ŒåŸºäºâ€œä»å°‘åˆ°å¤šâ€çš„åŸåˆ™ã€‚MaCoåˆ©ç”¨æ©æ¨¡ä¸Šä¸‹æ–‡å»ºæ¨¡ï¼ˆMCMï¼‰å’Œè¿ç»­ä¼ªæ ‡ç­¾ï¼ˆCPLï¼‰ã€‚MCMé‡‡ç”¨åŸºäºæ³¨æ„åŠ›çš„æ©æ¨¡ç­–ç•¥æ¥æ‰°åŠ¨è¾“å…¥å›¾åƒï¼Œç¡®ä¿æ¨¡å‹çš„é¢„æµ‹ä¸åŸå§‹å›¾åƒçš„é¢„æµ‹ä¸€è‡´ã€‚CPLé€šè¿‡åº”ç”¨æŒ‡æ•°è¡°å‡å‡½æ•°å°†æ¶‚ç”»æ³¨é‡Šè½¬æ¢ä¸ºè¿ç»­çš„åƒç´ çº§æ ‡ç­¾ï¼Œç”Ÿæˆç½®ä¿¡å›¾ï¼Œè¡¨ç¤ºæ¯ä¸ªåƒç´ å±äºç‰¹å®šç±»åˆ«çš„å¯èƒ½æ€§ï¼Œè€Œä¸æ˜¯ä¾èµ–ç¡¬ä¼ªæ ‡ç­¾ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªå…¬å…±æ•°æ®é›†ä¸Šè¯„ä¼°äº†MaCoï¼Œå¹¶å°†å…¶ä¸å…¶ä»–å¼±ç›‘ç£æ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒã€‚ç»“æœè¡¨æ˜ï¼ŒMaCoåœ¨æ‰€æœ‰æ•°æ®é›†ä¸Šçš„è¡¨ç°å‡ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œåœ¨å¼±ç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹é¢åˆ›é€ äº†æ–°çš„çºªå½•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.12814v2">PDF</a> 13 pages, 10 figures, 10 tables, JBHI</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¼±ç›‘ç£çš„åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹MaCoï¼Œé‡‡ç”¨â€œä»å°‘åˆ°å¤šâ€çš„è®¾è®¡åŸåˆ™ã€‚æ¨¡å‹ç»“åˆMasked Context Modelingï¼ˆMCMï¼‰å’ŒContinuous Pseudo Labelsï¼ˆCPLï¼‰æŠ€æœ¯ï¼Œæœ‰æ•ˆå¤„ç†ç¨€ç–æ ‡æ³¨æ•°æ®ï¼Œé¢„æµ‹åƒç´ çº§åˆ†å‰²å›¾ã€‚å®éªŒè¯æ˜ï¼ŒMaCoåœ¨ä¸‰ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„è¡¨ç°å‡è¶…è¶Šå…¶ä»–å¼±ç›‘ç£æ–¹æ³•ï¼Œåˆ›ä¸‹äº†æ–°çš„çºªå½•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Scribble-basedå¼±ç›‘ç£åˆ†å‰²æ–¹æ³•åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­å±•ç°å‡ºé™ä½æ ‡æ³¨æˆæœ¬çš„ä¼˜åŠ¿ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¸¸å€ŸåŠ©è¾…åŠ©ä»»åŠ¡æ¥åŠ å¼ºè¯­ä¹‰ä¸€è‡´æ€§ï¼Œä½†å¿½è§†äº†ç¨€ç–æ ‡æ³¨æ•°æ®å¸¦æ¥çš„æŒ‘æˆ˜ã€‚</li>
<li>MaCoæ¨¡å‹åˆ©ç”¨Masked Context Modelingï¼ˆMCMï¼‰å’ŒContinuous Pseudo Labelsï¼ˆCPLï¼‰æŠ€æœ¯åº”å¯¹æŒ‘æˆ˜ã€‚</li>
<li>MCMé€šè¿‡æ³¨æ„åŠ›æœºåˆ¶å¹²æ‰°è¾“å…¥å›¾åƒï¼Œç¡®ä¿æ¨¡å‹é¢„æµ‹ä¸åŸå§‹å›¾åƒä¸€è‡´ã€‚</li>
<li>CPLå°†æ¶‚é¸¦æ ‡æ³¨è½¬åŒ–ä¸ºè¿ç»­çš„åƒç´ çº§æ ‡ç­¾ï¼Œé€šè¿‡æŒ‡æ•°è¡°å‡å‡½æ•°ç”Ÿæˆç½®ä¿¡å›¾ã€‚</li>
<li>MaCoæ¨¡å‹åœ¨ä¸‰ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–å¼±ç›‘ç£æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.12814">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-ccdb8f6ce9b8801210c1a602ffc13349.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0fe36c1b35274c09d7ac96da113f4174.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-31c19c70afa172eb462b47d3cafeadf3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1de92d5bdab8f19bc9b36b6fcd2b2988.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bb65e798f2f120f8dffcb2c51208edd9.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Fast-DDPM-Fast-Denoising-Diffusion-Probabilistic-Models-for-Medical-Image-to-Image-Generation"><a href="#Fast-DDPM-Fast-Denoising-Diffusion-Probabilistic-Models-for-Medical-Image-to-Image-Generation" class="headerlink" title="Fast-DDPM: Fast Denoising Diffusion Probabilistic Models for Medical   Image-to-Image Generation"></a>Fast-DDPM: Fast Denoising Diffusion Probabilistic Models for Medical   Image-to-Image Generation</h2><p><strong>Authors:Hongxu Jiang, Muhammad Imran, Teng Zhang, Yuyin Zhou, Muxuan Liang, Kuang Gong, Wei Shao</strong></p>
<p>Denoising diffusion probabilistic models (DDPMs) have achieved unprecedented success in computer vision. However, they remain underutilized in medical imaging, a field crucial for disease diagnosis and treatment planning. This is primarily due to the high computational cost associated with (1) the use of large number of time steps (e.g., 1,000) in diffusion processes and (2) the increased dimensionality of medical images, which are often 3D or 4D. Training a diffusion model on medical images typically takes days to weeks, while sampling each image volume takes minutes to hours. To address this challenge, we introduce Fast-DDPM, a simple yet effective approach capable of improving training speed, sampling speed, and generation quality simultaneously. Unlike DDPM, which trains the image denoiser across 1,000 time steps, Fast-DDPM trains and samples using only 10 time steps. The key to our method lies in aligning the training and sampling procedures to optimize time-step utilization. Specifically, we introduced two efficient noise schedulers with 10 time steps: one with uniform time step sampling and another with non-uniform sampling. We evaluated Fast-DDPM across three medical image-to-image generation tasks: multi-image super-resolution, image denoising, and image-to-image translation. Fast-DDPM outperformed DDPM and current state-of-the-art methods based on convolutional networks and generative adversarial networks in all tasks. Additionally, Fast-DDPM reduced the training time to 0.2x and the sampling time to 0.01x compared to DDPM. Our code is publicly available at: <a target="_blank" rel="noopener" href="https://github.com/mirthAI/Fast-DDPM">https://github.com/mirthAI/Fast-DDPM</a>. </p>
<blockquote>
<p>å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMsï¼‰åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸå–å¾—äº†å‰æ‰€æœªæœ‰çš„æˆåŠŸã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨åŒ»å­¦æˆåƒé¢†åŸŸçš„åº”ç”¨ä»ç„¶è¢«ä½ä¼°ï¼Œè€ŒåŒ»å­¦æˆåƒå¯¹äºç–¾ç—…è¯Šæ–­å’Œæ²»ç–—è®¡åˆ’è‡³å…³é‡è¦ã€‚è¿™ä¸»è¦æ˜¯å› ä¸ºä¸ï¼ˆ1ï¼‰æ‰©æ•£è¿‡ç¨‹ä¸­ä½¿ç”¨çš„å¤§é‡æ—¶é—´æ­¥æ•°ï¼ˆä¾‹å¦‚ï¼Œ1000æ­¥ï¼‰å’Œï¼ˆ2ï¼‰åŒ»å­¦å›¾åƒå¢åŠ çš„ç»´åº¦ï¼ˆé€šå¸¸ä¸º3Dæˆ–4Dï¼‰ç›¸å…³çš„è®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚åœ¨åŒ»å­¦å›¾åƒä¸Šè®­ç»ƒæ‰©æ•£æ¨¡å‹é€šå¸¸éœ€è¦å‡ å¤©åˆ°å‡ å‘¨çš„æ—¶é—´ï¼Œè€Œå¯¹æ¯ä¸ªå›¾åƒä½“ç§¯è¿›è¡Œé‡‡æ ·åˆ™éœ€è¦å‡ åˆ†é’Ÿåˆ°å‡ å°æ—¶ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†Fast-DDPMï¼Œè¿™æ˜¯ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•ï¼Œå¯ä»¥åŒæ—¶æé«˜è®­ç»ƒé€Ÿåº¦ã€é‡‡æ ·é€Ÿåº¦å’Œç”Ÿæˆè´¨é‡ã€‚ä¸DDPMä¸åŒï¼Œå®ƒåœ¨æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­è·¨ä¸€åƒä¸ªæ—¶é—´æ­¥è®­ç»ƒå›¾åƒå»å™ªå™¨ï¼Œè€ŒFast-DDPMä»…ä½¿ç”¨åä¸ªæ—¶é—´æ­¥è¿›è¡Œè®­ç»ƒå’Œé‡‡æ ·ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„å…³é”®åœ¨äºå¯¹é½è®­ç»ƒå’Œé‡‡æ ·è¿‡ç¨‹ä»¥ä¼˜åŒ–æ—¶é—´æ­¥é•¿çš„åˆ©ç”¨ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸¤ç§å…·æœ‰åä¸ªæ—¶é—´æ­¥çš„é«˜æ•ˆå™ªå£°è°ƒåº¦å™¨ï¼šä¸€ç§å…·æœ‰å‡åŒ€æ—¶é—´æ­¥é•¿é‡‡æ ·ï¼Œå¦ä¸€ç§å…·æœ‰éå‡åŒ€é‡‡æ ·ã€‚æˆ‘ä»¬åœ¨ä¸‰é¡¹åŒ»å­¦å›¾åƒåˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­è¯„ä¼°äº†Fast-DDPMï¼šå¤šå›¾åƒè¶…åˆ†è¾¨ç‡ã€å›¾åƒå»å™ªå’Œå›¾åƒåˆ°å›¾åƒè½¬æ¢ã€‚Fast-DDPMåœ¨æ‰€æœ‰ä»»åŠ¡ä¸­éƒ½ä¼˜äºDDPMå’ŒåŸºäºå·ç§¯ç½‘ç»œå’Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œçš„æœ€å…ˆè¿›æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒFast-DDPMå°†è®­ç»ƒæ—¶é—´ç¼©çŸ­è‡³DDPMçš„0.2å€ï¼Œé‡‡æ ·æ—¶é—´ç¼©çŸ­è‡³DDPMçš„0.01å€ã€‚æˆ‘ä»¬çš„ä»£ç å…¬å¼€åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/mirthAI/Fast-DDPM">https://github.com/mirthAI/Fast-DDPM</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.14802v3">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    å¿«é€ŸDDPMæ¨¡å‹ä¼˜åŒ–äº†è®­ç»ƒä¸é‡‡æ ·è¿‡ç¨‹ï¼Œèƒ½åœ¨åŒ»ç–—å›¾åƒé¢†åŸŸä»¥è¾ƒå°‘çš„æ­¥éª¤å®ç°é«˜è´¨é‡çš„å›¾åƒç”Ÿæˆã€‚ç›¸è¾ƒäºDDPMæ¨¡å‹ä½¿ç”¨çš„ä¸€åƒæ­¥æ‰©æ•£è¿‡ç¨‹ï¼Œå¿«é€ŸDDPMä»…ä½¿ç”¨åä¸ªæ­¥éª¤å³å¯å®Œæˆè®­ç»ƒå’Œé‡‡æ ·è¿‡ç¨‹ã€‚æ¨¡å‹åŒ…å«ä¸¤ä¸ªå™ªå£°è°ƒåº¦å™¨ä»¥æé«˜æ•ˆç‡ï¼ŒåŒ…æ‹¬å‡åŒ€å’Œéå‡åŒ€é‡‡æ ·æ­¥éª¤ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨å¤šç§åŒ»ç–—å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­ï¼Œå¿«é€ŸDDPMæ¨¡å‹å‡ä¼˜äºDDPMæ¨¡å‹å’Œå½“å‰å…¶ä»–å…ˆè¿›çš„å·ç§¯ç½‘ç»œæ¨¡å‹ä¸ç”Ÿæˆå¯¹æŠ—ç½‘ç»œæ¨¡å‹ï¼Œå¹¶ä¸”åœ¨è®­ç»ƒä¸é‡‡æ ·æ—¶é—´ä¸Šæ˜¾è‘—ç¼©çŸ­äº†åŸæœ‰æ¨¡å‹æ‰€éœ€çš„æ—¶é—´ã€‚å…¶ä»£ç å·²å…¬å¼€åˆ†äº«äºç½‘å€ï¼š<a target="_blank" rel="noopener" href="https://github.com/mirthAI/Fast-DDPM%E3%80%82">https://github.com/mirthAI/Fast-DDPMã€‚</a></p>
<p><strong>å…³é”®è¦ç‚¹</strong></p>
<ol>
<li>å¿«é€ŸDDPMæ¨¡å‹é€‚ç”¨äºåŒ»ç–—å›¾åƒé¢†åŸŸï¼Œé€šè¿‡ä¼˜åŒ–è®­ç»ƒå’Œé‡‡æ ·è¿‡ç¨‹æé«˜äº†æ•ˆç‡ã€‚</li>
<li>ä¸DDPMæ¨¡å‹ç›¸æ¯”ï¼Œå¿«é€ŸDDPMé€šè¿‡ä½¿ç”¨æ›´å°‘çš„æ­¥éª¤å®ç°äº†è®­ç»ƒå’Œé‡‡æ ·è¿‡ç¨‹çš„æé€Ÿã€‚é€šè¿‡ä¸¤ä¸ªé«˜æ•ˆçš„å™ªå£°è°ƒåº¦å™¨ï¼ˆå‡åŒ€ä¸éå‡åŒ€é‡‡æ ·æ­¥éª¤ï¼‰æé«˜äº†æ—¶é—´æ­¥é•¿çš„åˆ©ç”¨ç‡ã€‚</li>
<li>åœ¨å¤šç§åŒ»ç–—å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­ï¼Œå¿«é€ŸDDPMæ¨¡å‹è¡¨ç°å‡ºè¶…è¶Šå…¶ä»–å…ˆè¿›æ¨¡å‹çš„ä¼˜åŠ¿ï¼Œå¦‚å¤šå›¾åƒè¶…åˆ†è¾¨ç‡ã€å›¾åƒå»å™ªå’Œå›¾åƒè½¬æ¢ä»»åŠ¡ç­‰ã€‚</li>
<li>ä¸å…¶ä»–æ¨¡å‹ç›¸æ¯”ï¼Œå¿«é€ŸDDPMæ¨¡å‹æ˜¾è‘—ç¼©çŸ­äº†è®­ç»ƒæ—¶é—´å’Œé‡‡æ ·æ—¶é—´ã€‚è¿™å¯¹äºå®é™…åº”ç”¨ä¸­çš„å¿«é€Ÿå“åº”å’Œé«˜æ•ˆå¤„ç†è‡³å…³é‡è¦ã€‚</li>
<li>è¯¥æ¨¡å‹çš„å…¬å¼€ä»£ç å¯ä¾›ç ”ç©¶è€…å’Œå¼€å‘è€…ä½¿ç”¨ï¼Œè¿›ä¸€æ­¥æ¨åŠ¨äº†å…¶åœ¨åŒ»ç–—å›¾åƒå¤„ç†é¢†åŸŸçš„åº”ç”¨å’Œå‘å±•ã€‚</li>
<li>å¿«é€ŸDDPMæ¨¡å‹çš„å¼•å…¥ä¸ºåŒ»ç–—å›¾åƒå¤„ç†é¢†åŸŸå¸¦æ¥äº†æ–°çš„è§†è§’å’Œè§£å†³æ–¹æ¡ˆï¼Œæœ‰æœ›æ¨åŠ¨è¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.14802">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-66f3f979a11c37d422d7825cf371bd3f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-49dcefa62fbe94e1da22230d4282a004.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b6fe4f49d2628659ff7ddd8f9e5919e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-491c4c5948d4b29c67ee3f8e573971c4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-da26bd395e39747c5897d0ecd8ca1045.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e5cc024d598392bc76826984d4bab24b.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-23/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-23/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-23/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-891dee2cf88e1c7f060e56ae86df1f95.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-23  Sadeed Advancing Arabic Diacritization Through Small Language Model
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-23
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-23/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-c71b5e238d840e076feb4e10f2993009.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-23  CineScale Free Lunch in High-Resolution Cinematic Visual Generation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-23
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">26551.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
