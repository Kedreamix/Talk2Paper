<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-08-23  CineScale Free Lunch in High-Resolution Cinematic Visual Generation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-c71b5e238d840e076feb4e10f2993009.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-08-23
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-09-08
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    13.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    56 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-08-23-更新"><a href="#2025-08-23-更新" class="headerlink" title="2025-08-23 更新"></a>2025-08-23 更新</h1><h2 id="CineScale-Free-Lunch-in-High-Resolution-Cinematic-Visual-Generation"><a href="#CineScale-Free-Lunch-in-High-Resolution-Cinematic-Visual-Generation" class="headerlink" title="CineScale: Free Lunch in High-Resolution Cinematic Visual Generation"></a>CineScale: Free Lunch in High-Resolution Cinematic Visual Generation</h2><p><strong>Authors:Haonan Qiu, Ning Yu, Ziqi Huang, Paul Debevec, Ziwei Liu</strong></p>
<p>Visual diffusion models achieve remarkable progress, yet they are typically trained at limited resolutions due to the lack of high-resolution data and constrained computation resources, hampering their ability to generate high-fidelity images or videos at higher resolutions. Recent efforts have explored tuning-free strategies to exhibit the untapped potential higher-resolution visual generation of pre-trained models. However, these methods are still prone to producing low-quality visual content with repetitive patterns. The key obstacle lies in the inevitable increase in high-frequency information when the model generates visual content exceeding its training resolution, leading to undesirable repetitive patterns deriving from the accumulated errors. In this work, we propose CineScale, a novel inference paradigm to enable higher-resolution visual generation. To tackle the various issues introduced by the two types of video generation architectures, we propose dedicated variants tailored to each. Unlike existing baseline methods that are confined to high-resolution T2I and T2V generation, CineScale broadens the scope by enabling high-resolution I2V and V2V synthesis, built atop state-of-the-art open-source video generation frameworks. Extensive experiments validate the superiority of our paradigm in extending the capabilities of higher-resolution visual generation for both image and video models. Remarkably, our approach enables 8k image generation without any fine-tuning, and achieves 4k video generation with only minimal LoRA fine-tuning. Generated video samples are available at our website: <a target="_blank" rel="noopener" href="https://eyeline-labs.github.io/CineScale/">https://eyeline-labs.github.io/CineScale/</a>. </p>
<blockquote>
<p>视觉扩散模型取得了显著的进步，但由于缺乏高分辨率数据和受限的计算资源，它们通常只在有限的分辨率上进行训练，这阻碍了它们生成更高分辨率的高保真图像或视频的能力。最近的研究致力于探索无需调整的策略，以展示预训练模型在更高分辨率视觉生成方面的潜力。然而，这些方法仍然容易产生重复模式的低质量视觉内容。关键障碍在于当模型生成超过其训练分辨率的视觉内容时，高频信息的必然增加会导致由累积误差引起的不可取的重复模式。在这项工作中，我们提出了CineScale，这是一种新型推理范式，可实现更高分辨率的视觉生成。为了解决由两种视频生成架构引入的各种问题，我们针对每种架构提出了专用变体。与仅限于高分辨率T2I和T2V生成的现有基准方法不同，CineScale通过支持高分辨率I2V和V2V合成来扩大范围，建立在最新开源视频生成框架之上。大量实验验证了我们的范式在扩展图像和视频模型的高分辨率生成能力方面的优越性。值得注意的是，我们的方法能够在无需微调的情况下实现8k图像生成，并且只需进行微小的LoRA微调即可实现4k视频生成。生成的视频样本可在我们的网站上进行查看：<a target="_blank" rel="noopener" href="https://eyeline-labs.github.io/CineScale/%E3%80%82">https://eyeline-labs.github.io/CineScale/。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15774v1">PDF</a> CineScale is an extended work of FreeScale (ICCV 2025). Project Page:   <a target="_blank" rel="noopener" href="https://eyeline-labs.github.io/CineScale/">https://eyeline-labs.github.io/CineScale/</a>, Code Repo:   <a target="_blank" rel="noopener" href="https://github.com/Eyeline-Labs/CineScale">https://github.com/Eyeline-Labs/CineScale</a></p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为CineScale的新型推理范式，旨在实现更高分辨率的视觉生成。该范式解决了由于模型生成超过训练分辨率的视觉内容时高频信息不可避免增加而导致的问题，避免了产生重复模式。CineScale不仅限于高分辨率的T2I和T2V生成，还能实现高分辨率的I2V和V2V合成。实验证明，该范式在图像和视频模型的更高分辨率视觉生成能力方面具有优越性，能够在不进行微调的情况下生成8k图像，并且只需最小的LoRA微调即可实现4k视频生成。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CineScale是一种新型推理范式，旨在实现更高分辨率的视觉生成。</li>
<li>该方法解决了在生成超过训练分辨率的视觉内容时产生的重复模式问题。</li>
<li>CineScale不仅适用于高分辨率的T2I和T2V生成，还支持高分辨率的I2V和V2V合成。</li>
<li>实验证明CineScale在图像和视频模型的更高分辨率视觉生成方面具有优越性。</li>
<li>CineScale能够在不进行微调的情况下生成8k图像。</li>
<li>只需最小的LoRA微调，CineScale即可实现4k视频生成。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15774">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-72613e33ecc86e68d31c6c55cc0b0a5c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eee40583539fd23e15032c2c496c12be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c2069e2f2c4e5020e923803a40a14226.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1978d2d0726bf3439670a2f23dacf32c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a8dcaa90351012af1ebd8957aafca51f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a7f2337011bb3549cd5bdeebf816c703.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Visual-Autoregressive-Modeling-for-Instruction-Guided-Image-Editing"><a href="#Visual-Autoregressive-Modeling-for-Instruction-Guided-Image-Editing" class="headerlink" title="Visual Autoregressive Modeling for Instruction-Guided Image Editing"></a>Visual Autoregressive Modeling for Instruction-Guided Image Editing</h2><p><strong>Authors:Qingyang Mao, Qi Cai, Yehao Li, Yingwei Pan, Mingyue Cheng, Ting Yao, Qi Liu, Tao Mei</strong></p>
<p>Recent advances in diffusion models have brought remarkable visual fidelity to instruction-guided image editing. However, their global denoising process inherently entangles the edited region with the entire image context, leading to unintended spurious modifications and compromised adherence to editing instructions. In contrast, autoregressive models offer a distinct paradigm by formulating image synthesis as a sequential process over discrete visual tokens. Their causal and compositional mechanism naturally circumvents the adherence challenges of diffusion-based methods. In this paper, we present VAREdit, a visual autoregressive (VAR) framework that reframes image editing as a next-scale prediction problem. Conditioned on source image features and text instructions, VAREdit generates multi-scale target features to achieve precise edits. A core challenge in this paradigm is how to effectively condition the source image tokens. We observe that finest-scale source features cannot effectively guide the prediction of coarser target features. To bridge this gap, we introduce a Scale-Aligned Reference (SAR) module, which injects scale-matched conditioning information into the first self-attention layer. VAREdit demonstrates significant advancements in both editing adherence and efficiency. On standard benchmarks, it outperforms leading diffusion-based methods by 30%+ higher GPT-Balance score. Moreover, it completes a $512\times512$ editing in 1.2 seconds, making it 2.2$\times$ faster than the similarly sized UltraEdit. The models are available at <a target="_blank" rel="noopener" href="https://github.com/HiDream-ai/VAREdit">https://github.com/HiDream-ai/VAREdit</a>. </p>
<blockquote>
<p>近期扩散模型（Diffusion Models）的进展为指令导向的图像编辑带来了显著的视觉保真度。然而，其全局去噪过程固有地将编辑区域与整个图像上下文纠缠在一起，导致出现意外的虚假修改以及不符合编辑指令的情况。相比之下，自回归模型（Autoregressive Models）通过为离散视觉符号制定一个序列过程来提供独特的范式，实现了图像合成。它们的因果和组合机制自然地避免了基于扩散的方法的遵循挑战。在本文中，我们提出了VAREdit，一个视觉自回归（VAR）框架，它将图像编辑重新定义为下一个尺度的预测问题。在源图像特征和文本指令的条件下，VAREdit生成多尺度目标特征来实现精确编辑。在此范式中的核心挑战是如何有效地调节源图像符号。我们发现最精细尺度的源特征无法有效地指导较粗糙目标特征的预测。为了弥补这一差距，我们引入了尺度对齐参考（SAR）模块，该模块将尺度匹配的调节信息注入到第一层自我关注中。VAREdit在编辑的遵循性和效率方面都取得了显著的进步。在标准基准测试中，它的GPT平衡得分比领先的扩散模型高出30%以上。此外，它能在1.2秒内完成一个512x512的编辑任务，相比之下比相同规模的UltraEdit快2.2倍。模型可以在<a target="_blank" rel="noopener" href="https://github.com/HiDream-ai/VAREdit%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/HiDream-ai/VAREdit找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15772v1">PDF</a> Source codes and models are available at   <a target="_blank" rel="noopener" href="https://github.com/HiDream-ai/VAREdit">https://github.com/HiDream-ai/VAREdit</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了基于视觉自回归（VAR）框架的图像编辑技术VAREdit。该技术将图像编辑重新定义为下一个尺度预测问题，通过源图像特征和文本指令生成多尺度目标特征来实现精确编辑。为弥补最精细尺度源特征无法有效引导较粗目标特征预测的缺陷，引入了规模对齐参考（SAR）模块，将规模匹配的调节信息注入第一层自注意力中。相较于领先的扩散模型，VAREdit在编辑贴合度和效率上具有显著优势。它在标准基准测试上的GPT-Balance得分高出30%以上，且完成一次512x512的编辑仅需1.2秒，相较于类似规模的UltraEdit加速约两倍。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>VAREdit利用视觉自回归框架进行图像编辑，将编辑定义为下一个尺度预测问题。</li>
<li>VAREdit通过结合源图像特征和文本指令生成多尺度目标特征，实现精确编辑。</li>
<li>引入规模对齐参考（SAR）模块，以解决最精细尺度源特征无法有效引导较粗目标特征预测的问题。</li>
<li>VAREdit在编辑贴合度和效率方面显著优于现有扩散模型。</li>
<li>VAREdit在标准测试中的GPT-Balance得分高出30%以上。</li>
<li>VAREdit完成一次512x512的编辑仅需1.2秒，相较于类似规模的编辑器，效率更高。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15772">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-13d23621db80188712c9307e624edbcb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b1fbf565289113329bf17a2c75602deb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-068aa3fcf0647fb08e9c7ff101f1edba.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8cfb6e71ce741eadad1f1e1b3a7b99e3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8018b44231e0aa61695e4b8c45a1479a.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Probability-Density-from-Latent-Diffusion-Models-for-Out-of-Distribution-Detection"><a href="#Probability-Density-from-Latent-Diffusion-Models-for-Out-of-Distribution-Detection" class="headerlink" title="Probability Density from Latent Diffusion Models for Out-of-Distribution   Detection"></a>Probability Density from Latent Diffusion Models for Out-of-Distribution   Detection</h2><p><strong>Authors:Joonas Järve, Karl Kaspar Haavel, Meelis Kull</strong></p>
<p>Despite rapid advances in AI, safety remains the main bottleneck to deploying machine-learning systems. A critical safety component is out-of-distribution detection: given an input, decide whether it comes from the same distribution as the training data. In generative models, the most natural OOD score is the data likelihood. Actually, under the assumption of uniformly distributed OOD data, the likelihood is even the optimal OOD detector, as we show in this work. However, earlier work reported that likelihood often fails in practice, raising doubts about its usefulness. We explore whether, in practice, the representation space also suffers from the inability to learn good density estimation for OOD detection, or if it is merely a problem of the pixel space typically used in generative models. To test this, we trained a Variational Diffusion Model not on images, but on the representation space of a pre-trained ResNet-18 to assess the performance of our likelihood-based detector in comparison to state-of-the-art methods from the OpenOOD suite. </p>
<blockquote>
<p>尽管人工智能领域取得了飞速发展，安全仍是部署机器学习系统的主瓶颈。一个关键的安全组件是异常检测：给定输入，确定其是否来自与训练数据相同的分布。在生成模型中，最自然的异常值分数是数据概率。实际上，在假设异常值数据均匀分布的情况下，概率甚至是最佳的异常检测器，正如我们在本工作中所展示的。然而，早期的研究报告指出，在实践中概率通常会失效，这引发了对其有用性的怀疑。我们探索的是在实践中，表示空间是否也因无法进行良好的密度估计来进行异常检测而受到影响，还是仅仅是因为生成模型中通常使用的像素空间的问题。为了测试这一点，我们训练了一个变分扩散模型，不使用图像，而是使用预训练的ResNet-1</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15737v1">PDF</a> ECAI 2025</p>
<p><strong>Summary</strong><br>     尽管人工智能发展迅速，但安全仍是部署机器学习系统的主瓶颈。关键的安全组件是分布外检测：给定输入，确定其是否来自与训练数据相同的分布。在生成模型中，最自然的OOD分数是数据可能性。实际上，在假设OOD数据均匀分布的情况下，可能性甚至是最佳的OOD检测器，我们在工作中也证明了这一点。然而，早期的研究报告指出，在实际操作中可能性经常会失效，引发了对其实用性的质疑。我们探究在实操中是否是表示空间遭受了无法学习良好的密度估计以进行OOD检测的困扰，还是仅仅是生成模型中通常使用的像素空间的问题。为了测试这一点，我们训练了一个变分扩散模型，不是对图像进行训练，而是对预训练ResNet-18的表示空间进行评估，以比较基于可能性的检测器与OpenOOD套件中的最新方法的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AI的快速发展面临安全瓶颈，尤其是部署机器学习系统的安全。</li>
<li>分布外检测是机器学习安全中的关键组件，用于判断输入是否来自训练数据分布。</li>
<li>在生成模型中，数据可能性是最自然的OOD检测指标，但在实践中经常被发现在性能上的问题。</li>
<li>变分扩散模型在表示空间而非图像上进行训练，以评估基于可能性的检测器的性能。</li>
<li>现有的对可能性的质疑部分源于实操中的问题，可能与表示空间或像素空间的特性有关。</li>
<li>本研究通过对比实验来探究在表示空间中进行OOD检测是否也会遭遇密度估计问题。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15737">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-fe019682510d54cb3942a9f7190a6662.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8b39657e351481403947c19d13cfc5b9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4a54387d4728c50d350e78eba02f5454.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-02f86b889c2040022a92199ba4dbf7da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f73282a3a938095008ffc77ea9485a2.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Pathology-Informed-Latent-Diffusion-Model-for-Anomaly-Detection-in-Lymph-Node-Metastasis"><a href="#Pathology-Informed-Latent-Diffusion-Model-for-Anomaly-Detection-in-Lymph-Node-Metastasis" class="headerlink" title="Pathology-Informed Latent Diffusion Model for Anomaly Detection in Lymph   Node Metastasis"></a>Pathology-Informed Latent Diffusion Model for Anomaly Detection in Lymph   Node Metastasis</h2><p><strong>Authors:Jiamu Wang, Keunho Byeon, Jinsol Song, Anh Nguyen, Sangjeong Ahn, Sung Hak Lee, Jin Tae Kwak</strong></p>
<p>Anomaly detection is an emerging approach in digital pathology for its ability to efficiently and effectively utilize data for disease diagnosis. While supervised learning approaches deliver high accuracy, they rely on extensively annotated datasets, suffering from data scarcity in digital pathology. Unsupervised anomaly detection, however, offers a viable alternative by identifying deviations from normal tissue distributions without requiring exhaustive annotations. Recently, denoising diffusion probabilistic models have gained popularity in unsupervised anomaly detection, achieving promising performance in both natural and medical imaging datasets. Building on this, we incorporate a vision-language model with a diffusion model for unsupervised anomaly detection in digital pathology, utilizing histopathology prompts during reconstruction. Our approach employs a set of pathology-related keywords associated with normal tissues to guide the reconstruction process, facilitating the differentiation between normal and abnormal tissues. To evaluate the effectiveness of the proposed method, we conduct experiments on a gastric lymph node dataset from a local hospital and assess its generalization ability under domain shift using a public breast lymph node dataset. The experimental results highlight the potential of the proposed method for unsupervised anomaly detection across various organs in digital pathology. Code: <a target="_blank" rel="noopener" href="https://github.com/QuIIL/AnoPILaD">https://github.com/QuIIL/AnoPILaD</a>. </p>
<blockquote>
<p>异常检测是数字病理中一个新兴的方法，因为它能够高效且有效地利用数据进行疾病诊断。虽然监督学习方法可以提供较高的准确性，但它们依赖于大量标注的数据集，而在数字病理中常常面临数据稀缺的问题。然而，无监督的异常检测提供了一个可行的替代方案，它可以通过识别正常组织分布中的偏差来进行异常检测，无需详尽的标注。最近，去噪扩散概率模型在无监督异常检测中获得了普及，在自然和医学成像数据集上取得了有希望的性能。在此基础上，我们将一个视觉语言模型与扩散模型相结合，用于数字病理中的无监督异常检测，并在重建过程中利用病理提示。我们的方法采用与正常组织相关的一组病理学关键词来引导重建过程，从而便于区分正常组织和异常组织。为了评估所提出方法的有效性，我们在当地医院的胃淋巴结数据集上进行了实验，并使用公共的乳腺淋巴结数据集来评估其在领域迁移下的泛化能力。实验结果突出了所提出方法在数字病理中用于各种器官的无监督异常检测的潜力。代码地址：<a target="_blank" rel="noopener" href="https://github.com/QuIIL/AnoPILaD">https://github.com/QuIIL/AnoPILaD</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15236v1">PDF</a> </p>
<p><strong>Summary</strong><br>     数字病理中的异常检测新兴方法能有效利用数据进行疾病诊断。监督学习方法虽准确但依赖大量标注数据，在数字病理中面临数据稀缺问题。无监督异常检测通过识别正常组织分布的偏差，无需详尽标注，提供了可行替代方案。近期，降噪扩散概率模型在无监督异常检测中受到关注，在自然和医学成像数据集上表现出良好性能。在此基础上，我们结合视觉语言模型和扩散模型进行数字病理的无监督异常检测，重建过程中利用组织病理学提示。通过一系列与正常组织相关的病理学关键词引导重建过程，有助于区分正常组织与异常组织。对本地医院的胃淋巴节点数据集进行试验并对公开乳腺淋巴节点数据集进行跨域评估，凸显了该方法的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>异常检测是数字病理学中的新兴方法，用于高效且有效地利用数据进行疾病诊断。</li>
<li>监督学习方法在数字病理中面临数据稀缺问题。</li>
<li>无监督异常检测通过识别正常组织分布的偏差，为数字病理提供了可行替代方案。</li>
<li>降噪扩散概率模型在无监督异常检测中受到关注，表现良好。</li>
<li>结合视觉语言模型和扩散模型进行数字病理的无监督异常检测，利用组织病理学提示提升检测准确性。</li>
<li>通过关键词引导重建过程，有助于区分正常组织与异常组织。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15236">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5a412df1e2323af076ad13c45fe93b60.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a8c69cf25a20c78e9a65f7a27ef5376.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f83a65fabd8f03cfe77ff5ec5821b0f0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d5f246d12e6545d2c5404912c413923c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8f31c192d6418fc22224b3e72c40362c.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="See-it-Say-it-Sorted-Agentic-System-for-Compositional-Diagram-Generation"><a href="#See-it-Say-it-Sorted-Agentic-System-for-Compositional-Diagram-Generation" class="headerlink" title="See it. Say it. Sorted: Agentic System for Compositional Diagram   Generation"></a>See it. Say it. Sorted: Agentic System for Compositional Diagram   Generation</h2><p><strong>Authors:Hantao Zhang, Jingyang Liu, Ed Li</strong></p>
<p>We study sketch-to-diagram generation: converting rough hand sketches into precise, compositional diagrams. Diffusion models excel at photorealism but struggle with the spatial precision, alignment, and symbolic structure required for flowcharts. We introduce See it. Say it. Sorted., a training-free agentic system that couples a Vision-Language Model (VLM) with Large Language Models (LLMs) to produce editable Scalable Vector Graphics (SVG) programs. The system runs an iterative loop in which a Critic VLM proposes a small set of qualitative, relational edits; multiple candidate LLMs synthesize SVG updates with diverse strategies (conservative-&gt;aggressive, alternative, focused); and a Judge VLM selects the best candidate, ensuring stable improvement. This design prioritizes qualitative reasoning over brittle numerical estimates, preserves global constraints (e.g., alignment, connectivity), and naturally supports human-in-the-loop corrections. On 10 sketches derived from flowcharts in published papers, our method more faithfully reconstructs layout and structure than two frontier closed-source image generation LLMs (GPT-5 and Gemini-2.5-Pro), accurately composing primitives (e.g., multi-headed arrows) without inserting unwanted text. Because outputs are programmatic SVGs, the approach is readily extensible to presentation tools (e.g., PowerPoint) via APIs and can be specialized with improved prompts and task-specific tools. The codebase is open-sourced at <a target="_blank" rel="noopener" href="https://github.com/hantaoZhangrichard/see_it_say_it_sorted.git">https://github.com/hantaoZhangrichard/see_it_say_it_sorted.git</a>. </p>
<blockquote>
<p>我们研究了草图到图表生成的任务，即将粗略的手绘草图转化为精确、组合式的图表。扩散模型在照片写实方面表现出色，但在流程图所需的空间精度、对齐和符号结构方面遇到了困难。我们推出了“所见即所说.已排序”，这是一个无需训练的主体系统，它将视觉语言模型（VLM）与大型语言模型（LLM）相结合，生成可编辑的矢量图形（SVG）程序。该系统运行一个迭代循环，其中批评家VLM提出一小部分定性关系编辑；多个候选LLM使用不同策略（保守到激进、替代、专注）合成SVG更新；法官VLM选择最佳候选者，确保稳定改进。这种设计优先考虑定性推理而非脆弱的数值估计，保留全局约束（例如对齐、连接），并天然支持人类参与循环校正。在来自已发表论文的10个流程图草图上，我们的方法比两个前沿的闭源图像生成LLM（GPT-5和Gemini-2.5-Pro）更忠实地重建了布局和结构，能够准确地组合原始元素（例如多头箭头），而不会插入不需要的文本。由于输出是程序化的SVG，因此该方法可以通过API轻松扩展到演示工具（例如PowerPoint），并且可以通过改进提示和任务特定工具进行专门化。该代码库已在<a target="_blank" rel="noopener" href="https://github.com/hantaoZhangrichard/see_it_say_it_sorted.git%E5%BC%80%E6%BA%90%E3%80%82">https://github.com/hantaoZhangrichard/see_it_say_it_sorted.git开源。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15222v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文研究了草图到图表的生成问题，即将粗糙的手绘草图转化为精确的组成图表。研究团队引入了一个名为“See it. Say it. Sorted.”的无训练代理系统，结合了视觉语言模型（VLM）和大型语言模型（LLM），以生成可编辑的矢量图形（SVG）程序。该系统通过迭代过程，由批判性VLM提出定性关系编辑建议，多个候选LLM采用不同策略合成SVG更新，并由判断性VLM选择最佳候选，确保持续改进。该方法重视定性推理，保留全局约束，如对齐和连接性，并支持人工介入修正。在基于论文中10个草拟的流程图测试上，该方法能更忠实地重建布局和结构，相较于两大前沿图像生成LLM（GPT-5和Gemini-2.5-Pro）更具优势，能准确组合基本元素而不添加无关文字。其输出为程序化SVG，易于扩展到演示工具如PowerPoint等。相关代码已开源。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究团队提出了一种将草图转化为精确图表的方法，使用“See it. Say it. Sorted.”系统结合VLM和LLM技术。</li>
<li>该系统通过迭代过程，能够提出编辑建议、合成SVG更新并选择最佳候选，从而持续提高图表质量。</li>
<li>系统重视定性推理，能够保留全局约束，如对齐和连接性，并支持人工修正。</li>
<li>与其他前沿LLM相比，该方法在重建布局和结构方面表现更优秀，能准确组合基本元素。</li>
<li>输出为程序化SVG格式，易于集成到各种演示工具中，如PowerPoint。</li>
<li>该方法具有可扩展性，可通过改进提示和任务特定工具进行专业化。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15222">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f74dfde820fa8f822759ea1919ac78b6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0371cdf5f20cd938fa3a4c512c43617e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-22615bd766eaf130be902e26f5ac964f.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MeSS-City-Mesh-Guided-Outdoor-Scene-Generation-with-Cross-View-Consistent-Diffusion"><a href="#MeSS-City-Mesh-Guided-Outdoor-Scene-Generation-with-Cross-View-Consistent-Diffusion" class="headerlink" title="MeSS: City Mesh-Guided Outdoor Scene Generation with Cross-View   Consistent Diffusion"></a>MeSS: City Mesh-Guided Outdoor Scene Generation with Cross-View   Consistent Diffusion</h2><p><strong>Authors:Xuyang Chen, Zhijun Zhai, Kaixuan Zhou, Zengmao Wang, Jianan He, Dong Wang, Yanfeng Zhang, mingwei Sun, Rüdiger Westermann, Konrad Schindler, Liqiu Meng</strong></p>
<p>Mesh models have become increasingly accessible for numerous cities; however, the lack of realistic textures restricts their application in virtual urban navigation and autonomous driving. To address this, this paper proposes MeSS (Meshbased Scene Synthesis) for generating high-quality, styleconsistent outdoor scenes with city mesh models serving as the geometric prior. While image and video diffusion models can leverage spatial layouts (such as depth maps or HD maps) as control conditions to generate street-level perspective views, they are not directly applicable to 3D scene generation. Video diffusion models excel at synthesizing consistent view sequences that depict scenes but often struggle to adhere to predefined camera paths or align accurately with rendered control videos. In contrast, image diffusion models, though unable to guarantee cross-view visual consistency, can produce more geometry-aligned results when combined with ControlNet. Building on this insight, our approach enhances image diffusion models by improving cross-view consistency. The pipeline comprises three key stages: first, we generate geometrically consistent sparse views using Cascaded Outpainting ControlNets; second, we propagate denser intermediate views via a component dubbed AGInpaint; and third, we globally eliminate visual inconsistencies (e.g., varying exposure) using the GCAlign module. Concurrently with generation, a 3D Gaussian Splatting (3DGS) scene is reconstructed by initializing Gaussian balls on the mesh surface. Our method outperforms existing approaches in both geometric alignment and generation quality. Once synthesized, the scene can be rendered in diverse styles through relighting and style transfer techniques. </p>
<blockquote>
<p>网格模型已经为众多城市提供了越来越便捷的访问方式，然而，由于缺乏逼真的纹理，其在虚拟城市导航和自动驾驶方面的应用受到限制。为解决这一问题，本文提出了MeSS（基于网格的场景合成）方法，该方法以城市网格模型作为几何先验，生成高质量、风格一致的外景。图像和视频扩散模型可以利用空间布局（如深度图或高清地图）作为控制条件来生成街道级透视视图，但它们并不直接适用于3D场景生成。视频扩散模型擅长合成一致的场景视图序列，但往往难以遵循预定的相机路径或与渲染的控制视频准确对齐。相比之下，图像扩散模型虽然无法保证跨视图的视觉一致性，但当与ControlNet结合时，可以产生更贴合几何的结果。基于此见解，我们的方法通过改进图像扩散模型的跨视图一致性来增强其性能。该管道包括三个阶段：首先，我们使用级联外画ControlNets生成几何一致的稀疏视图；其次，我们通过名为AGInpaint的组件传播更密集的中间视图；最后，我们使用GCAlign模块全局消除视觉不一致（例如，曝光不同）。在生成的同时，通过在网格表面上初始化高斯球来重建3D高斯喷绘（3DGS）场景。我们的方法在几何对齐和生成质量方面都优于现有方法。场景合成后，可以通过重新照明和风格转换技术以多种风格进行渲染。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15169v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种名为MeSS的方法，用于生成高质量、风格一致的户外场景。该方法以城市网格模型作为几何先验，结合图像扩散模型和控制网络（ControlNet），生成几何对齐的视图。通过三个阶段处理，包括生成几何一致的稀疏视图、传播中间密集视图以及消除全局视觉不一致性。同时，通过三维高斯喷绘（3DGS）技术重建场景，实现多种风格的渲染。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>城市网格模型作为几何先验，为虚拟城市导航和自动驾驶应用提供了基础。</li>
<li>图像扩散模型与控制网络（ControlNet）结合，提高了跨视图的一致性。</li>
<li>MeSS方法包括三个阶段：生成几何一致的稀疏视图、传播中间密集视图、消除视觉不一致性。</li>
<li>3DGS技术用于重建场景，实现多种风格的渲染。</li>
<li>MeSS方法在几何对齐和生成质量方面优于现有方法。</li>
<li>合成场景可以通过照明和风格转换技术进行不同风格的渲染。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15169">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-926c1a3fb69a681177f24ffa5f41beee.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3a4d8faceee8b7e2bb8f6f4662186092.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c09b33b9d1a412c50df633512c99ba56.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7b856fbc2ca3d4cb8d9db9dee9e1fb4e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-12b798ac6e3954efb7f2a1c8ae65d37a.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="TAIGen-Training-Free-Adversarial-Image-Generation-via-Diffusion-Models"><a href="#TAIGen-Training-Free-Adversarial-Image-Generation-via-Diffusion-Models" class="headerlink" title="TAIGen: Training-Free Adversarial Image Generation via Diffusion Models"></a>TAIGen: Training-Free Adversarial Image Generation via Diffusion Models</h2><p><strong>Authors:Susim Roy, Anubhooti Jain, Mayank Vatsa, Richa Singh</strong></p>
<p>Adversarial attacks from generative models often produce low-quality images and require substantial computational resources. Diffusion models, though capable of high-quality generation, typically need hundreds of sampling steps for adversarial generation. This paper introduces TAIGen, a training-free black-box method for efficient adversarial image generation. TAIGen produces adversarial examples using only 3-20 sampling steps from unconditional diffusion models. Our key finding is that perturbations injected during the mixing step interval achieve comparable attack effectiveness without processing all timesteps. We develop a selective RGB channel strategy that applies attention maps to the red channel while using GradCAM-guided perturbations on green and blue channels. This design preserves image structure while maximizing misclassification in target models. TAIGen maintains visual quality with PSNR above 30 dB across all tested datasets. On ImageNet with VGGNet as source, TAIGen achieves 70.6% success against ResNet, 80.8% against MNASNet, and 97.8% against ShuffleNet. The method generates adversarial examples 10x faster than existing diffusion-based attacks. Our method achieves the lowest robust accuracy, indicating it is the most impactful attack as the defense mechanism is least successful in purifying the images generated by TAIGen. </p>
<blockquote>
<p>对抗性攻击生成模型通常会产生低质量的图像，并需要大量的计算资源。尽管扩散模型能够进行高质量生成，但通常需要进行数百步采样才能进行对抗性生成。本文介绍了TAIGen，这是一种无需训练的黑盒方法，可高效生成对抗性图像。TAIGen仅使用无条件扩散模型的3-20步采样即可产生对抗性实例。我们的关键发现是在混合步骤间隔中注入扰动，在不处理所有时间步长的情况下实现了相当的攻击效果。我们开发了一种选择性RGB通道策略，对红色通道应用注意力图，同时对绿色和蓝色通道使用GradCAM引导的扰动。这种设计保留了图像结构，同时最大限度地提高了目标模型的误分类率。TAIGen在所有测试数据集上保持PSNR高于30 dB的视觉质量。在ImageNet上使用VGGNet作为源时，TAIGen对ResNet的成功率为70.6%，对MNASNet的成功率为80.8%，对ShuffleNet的成功率高达97.8%。该方法生成的对抗性实例的速度比现有的基于扩散的攻击快10倍。我们的方法达到了最低的稳健准确率，这表明它是最具影响力的攻击，因为防御机制在净化TAIGen生成的图像时效果最不明显。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15020v1">PDF</a> Accepted at ICCVW-CV4BIOM 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种名为TAIGen的训练无关的黑盒方法，用于高效生成对抗性图像。该方法利用无条件扩散模型，仅通过3-20个采样步骤即可生成对抗样本。研究发现在混合步骤间隔期间注入扰动，可在不处理所有时间步的情况下实现相当的攻击效果。该方法采用选择性RGB通道策略，对红通道应用注意力映射，同时对绿蓝通道采用GradCAM引导的扰动。此方法在保持图像结构的同时，最大化目标模型的误分类。TAIGen在保持视觉质量的同时，生成对抗样本的速度是现有扩散攻击的10倍。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TAIGen是一种训练无关的黑盒方法，用于高效生成对抗性图像。</li>
<li>该方法利用无条件扩散模型，通过较少的采样步骤（3-20步）生成对抗样本。</li>
<li>研究发现，在混合步骤间隔期间注入扰动可达成有效的攻击。</li>
<li>采用选择性RGB通道策略，对红、绿、蓝通道分别处理，以提高攻击效果并保持图像质量。</li>
<li>TAIGen在多个数据集上的PSNR值均超过30dB，保持较高的视觉质量。</li>
<li>在ImageNet数据集上，TAIGen对ResNet、MNASNet和ShuffleNet的成功率分别为70.6%、80.8%和97.8%。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15020">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f4aab2b078a24982a3ca5fb6bb11e06a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93d339abe9d4f08f4b18b3803189f7c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-806ebcccd6112f3926cfca5c49735ecc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-18cd6406edd57cdd7dfa7fa1313904e6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5d1bfee740b4a4df3f7bc822b11abba0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c71e4d02a985f18bd5c313dd4a66d5f9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6a6fa7ccc8cc2531796bd69bed502b2b.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Latent-Interpolation-Learning-Using-Diffusion-Models-for-Cardiac-Volume-Reconstruction"><a href="#Latent-Interpolation-Learning-Using-Diffusion-Models-for-Cardiac-Volume-Reconstruction" class="headerlink" title="Latent Interpolation Learning Using Diffusion Models for Cardiac Volume   Reconstruction"></a>Latent Interpolation Learning Using Diffusion Models for Cardiac Volume   Reconstruction</h2><p><strong>Authors:Niklas Bubeck, Suprosanna Shit, Chen Chen, Can Zhao, Pengfei Guo, Dong Yang, Georg Zitzlsberger, Daguang Xu, Bernhard Kainz, Daniel Rueckert, Jiazhen Pan</strong></p>
<p>Cardiac Magnetic Resonance (CMR) imaging is a critical tool for diagnosing and managing cardiovascular disease, yet its utility is often limited by the sparse acquisition of 2D short-axis slices, resulting in incomplete volumetric information. Accurate 3D reconstruction from these sparse slices is essential for comprehensive cardiac assessment, but existing methods face challenges, including reliance on predefined interpolation schemes (e.g., linear or spherical), computational inefficiency, and dependence on additional semantic inputs such as segmentation labels or motion data. To address these limitations, we propose a novel Cardiac Latent Interpolation Diffusion (CaLID) framework that introduces three key innovations. First, we present a data-driven interpolation scheme based on diffusion models, which can capture complex, non-linear relationships between sparse slices and improves reconstruction accuracy. Second, we design a computationally efficient method that operates in the latent space and speeds up 3D whole-heart upsampling time by a factor of 24, reducing computational overhead compared to previous methods. Third, with only sparse 2D CMR images as input, our method achieves SOTA performance against baseline methods, eliminating the need for auxiliary input such as morphological guidance, thus simplifying workflows. We further extend our method to 2D+T data, enabling the effective modeling of spatiotemporal dynamics and ensuring temporal coherence. Extensive volumetric evaluations and downstream segmentation tasks demonstrate that CaLID achieves superior reconstruction quality and efficiency. By addressing the fundamental limitations of existing approaches, our framework advances the state of the art for spatio and spatiotemporal whole-heart reconstruction, offering a robust and clinically practical solution for cardiovascular imaging. </p>
<blockquote>
<p>心脏磁共振（CMR）成像在心血管疾病的诊断和治疗中起着至关重要的作用，但其效用往往受到二维短轴切片稀疏采集的限制，导致体积信息不完整。从这些稀疏切片中进行准确的3D重建对于全面的心脏评估至关重要，但现有方法面临挑战，包括依赖于预定义的插值方案（例如线性或球形）、计算效率低下以及对额外的语义输入（如分割标签或运动数据）的依赖。为了解决这些局限性，我们提出了一种新型的心脏潜在插值扩散（CaLID）框架，引入了三项关键创新。首先，我们提出了一种基于扩散模型的数据驱动插值方案，可以捕捉稀疏切片之间复杂的非线性关系，提高重建精度。其次，我们设计了一种在潜在空间运行的高效方法，将3D全心上采样时间加快24倍，与以前的方法相比，减少了计算开销。第三，我们的方法仅使用稀疏的2DCMR图像作为输入，就达到了与基线方法相比的最佳性能，无需辅助输入（如形态引导），从而简化了工作流程。我们还将我们的方法扩展到2D+T数据，能够有效地对时空动态进行建模，确保时间连贯性。大量的体积评估和下游分割任务表明，CaLID在重建质量和效率方面达到了领先水平。通过解决现有方法的基本局限性，我们的框架在时空和时空动态全心重建方面推动了最新技术，为心血管成像提供了稳健且实用的临床解决方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13826v3">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>心脏磁共振（CMR）成像在诊断和治疗心血管疾病方面具有重要意义，但其效用常因二维短轴切片的稀疏采集而受到限制，导致体积信息不完整。准确的从稀疏切片中进行三维重建对于全面的心脏评估至关重要，但现有方法面临挑战，包括依赖预设的插值方案（如线性或球形）、计算效率低下以及对额外的语义输入（如分割标签或运动数据）的依赖。为解决这些局限性，我们提出了全新的心脏潜在插值扩散（CaLID）框架，引入了三项关键创新。首先，我们基于扩散模型提出了数据驱动插值方案，该方案可以捕捉稀疏切片之间的复杂非线性关系，提高重建精度。其次，我们设计了一种在潜在空间高效运行的方法，将心脏3D上采样的时间缩短了24倍，降低了计算开销。第三，我们的方法仅使用稀疏的2DCMR图像作为输入，即可达到优于基线方法的性能，无需辅助输入，从而简化了工作流程。我们还将方法扩展到了二维加时间数据，能够有效地对时空动态进行建模，确保时间连贯性。大量的体积评估和下游分割任务表明，CaLID在重建质量和效率方面达到了卓越的水平。通过解决现有方法的基本局限性，我们的框架在空间和时空全心脏重建方面达到了最新水平，为心血管成像提供了稳健且实用的解决方案。</p>
<p><strong>要点</strong></p>
<ol>
<li>CMR成像在心血管疾病诊断和管理中至关重要，但受限于二维切片的稀疏采集导致的体积信息不完整。</li>
<li>现有三维重建方法面临依赖预设插值方案、计算效率低下和对额外语义输入的依赖等挑战。</li>
<li>CaLID框架引入数据驱动插值方案，提高重建精度，且无需额外的辅助输入。</li>
<li>CaLID在潜在空间操作，计算效率高，大大缩短了三维心脏上采样的时间。</li>
<li>CaLID能够扩展到二维加时间数据，有效建模时空动态，确保时间连贯性。</li>
<li>广泛的评估和下游任务显示CaLID在重建质量和效率方面达到卓越水平。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13826">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-8e4059179698f84ec7b7acc198cc9b38.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b3c2aacf31192a2998523e57c1e2452.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9db76104ad21d445d80009b4eb9b5fd5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7b3e8fbd563620fc41599e9edf300cdb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b595fe2d116ad3426b59833e331eab7.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Synthesizing-Near-Boundary-OOD-Samples-for-Out-of-Distribution-Detection"><a href="#Synthesizing-Near-Boundary-OOD-Samples-for-Out-of-Distribution-Detection" class="headerlink" title="Synthesizing Near-Boundary OOD Samples for Out-of-Distribution Detection"></a>Synthesizing Near-Boundary OOD Samples for Out-of-Distribution Detection</h2><p><strong>Authors:Jinglun Li, Kaixun Jiang, Zhaoyu Chen, Bo Lin, Yao Tang, Weifeng Ge, Wenqiang Zhang</strong></p>
<p>Pre-trained vision-language models have exhibited remarkable abilities in detecting out-of-distribution (OOD) samples. However, some challenging OOD samples, which lie close to in-distribution (InD) data in image feature space, can still lead to misclassification. The emergence of foundation models like diffusion models and multimodal large language models (MLLMs) offers a potential solution to this issue. In this work, we propose SynOOD, a novel approach that harnesses foundation models to generate synthetic, challenging OOD data for fine-tuning CLIP models, thereby enhancing boundary-level discrimination between InD and OOD samples. Our method uses an iterative in-painting process guided by contextual prompts from MLLMs to produce nuanced, boundary-aligned OOD samples. These samples are refined through noise adjustments based on gradients from OOD scores like the energy score, effectively sampling from the InD&#x2F;OOD boundary. With these carefully synthesized images, we fine-tune the CLIP image encoder and negative label features derived from the text encoder to strengthen connections between near-boundary OOD samples and a set of negative labels. Finally, SynOOD achieves state-of-the-art performance on the large-scale ImageNet benchmark, with minimal increases in parameters and runtime. Our approach significantly surpasses existing methods, and the code is available at <a target="_blank" rel="noopener" href="https://github.com/Jarvisgivemeasuit/SynOOD">https://github.com/Jarvisgivemeasuit/SynOOD</a>. </p>
<blockquote>
<p>预训练的语言视觉模型在检测分布外（OOD）样本方面表现出了显著的能力。然而，一些在图像特征空间上与分布内（InD）数据相近的具有挑战性的OOD样本，仍可能导致误分类。扩散模型和多媒体语言模型等基础模型的涌现为解决这一问题提供了潜在解决方案。在这项工作中，我们提出了SynOOD，这是一种利用基础模型生成合成、具有挑战性的OOD数据对CLIP模型进行微调的新方法，从而提高了InD和OOD样本之间的边界级别判别能力。我们的方法使用由多媒体语言模型提供的上下文提示引导的迭代填充过程来生成微妙的、与边界对齐的OOD样本。这些样本通过基于OOD分数（如能量分数）的梯度进行噪声调整来细化，有效地从InD&#x2F;OOD边界进行采样。通过仔细合成的图像，我们微调CLIP图像编码器和从文本编码器获得的负标签特征，以加强近边界OOD样本与一组负标签之间的联系。最后，SynOOD在大型ImageNet基准测试中实现了最先进的性能，参数和运行时几乎没有增加。我们的方法显著优于现有方法，代码可访问<a target="_blank" rel="noopener" href="https://github.com/Jarvisgivemeasuit/SynOOD%E3%80%82">https://github.com/Jarvisgivemeasuit/SynOOD。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10225v3">PDF</a> Accepted by ICCV 2025 (Highlight)</p>
<p><strong>摘要</strong><br>    预训练视听模型在检测异常分布（OOD）样本方面表现出卓越能力，但对于接近正常分布数据的OOD样本，仍存在误分类风险。本文引入扩散模型等多模态基础模型，提出SynOOD方法，生成合成异常分布数据对CLIP模型进行微调，提高正常与异常样本间的边界级别判别能力。该方法通过迭代修复过程与多模态大型语言模型的上下文提示相结合，生成微妙的边界对齐OOD样本。这些样本基于异常分数（如能量分数）的梯度进行噪声调整，有效采样正常分布与异常分布边界。通过精细合成的图像，我们微调CLIP图像编码器与来自文本编码器的负标签特征，强化近边界OOD样本与一系列负标签之间的连接。最终，SynOOD在大型ImageNet基准测试中达到领先水平，且参数与运行时间增加微小。该方法显著优于现有技术，代码已公开。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>预训练视听模型在检测OOD样本方面具有出色能力，但对于接近正常分布数据的OOD样本仍可能误分类。</li>
<li>引入扩散模型等多模态基础模型是解决这一问题的一种潜在方法。</li>
<li>SynOOD方法利用基础模型生成合成OOD数据，用于微调CLIP模型，提高正常与异常样本间的边界识别。</li>
<li>SynOOD通过迭代修复过程与上下文提示生成微妙的边界对齐OOD样本。</li>
<li>这些样本基于异常分数（如能量分数）的梯度进行噪声调整，从而更好地反映正常与异常分布的边界。</li>
<li>通过精细合成的图像，SynOOD微调了CLIP图像编码器和负标签特征，强化了近边界OOD样本与负标签之间的连接。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10225">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c71b5e238d840e076feb4e10f2993009.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c9eff2f1034c6fe8cfa8da20ff80c095.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-014152f40cd33b577bc10e2c7878cd59.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-681933d97e047dcacb3d67817f780db4.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="CopyrightShield-Enhancing-Diffusion-Model-Security-against-Copyright-Infringement-Attacks"><a href="#CopyrightShield-Enhancing-Diffusion-Model-Security-against-Copyright-Infringement-Attacks" class="headerlink" title="CopyrightShield: Enhancing Diffusion Model Security against Copyright   Infringement Attacks"></a>CopyrightShield: Enhancing Diffusion Model Security against Copyright   Infringement Attacks</h2><p><strong>Authors:Zhixiang Guo, Siyuan Liang, Aishan Liu, Dacheng Tao</strong></p>
<p>Diffusion models have attracted significant attention due to its exceptional data generation capabilities in fields such as image synthesis. However, recent studies have shown that diffusion models are vulnerable to copyright infringement attacks, where attackers inject strategically modified non-infringing images into the training set, inducing the model to generate infringing content under the prompt of specific poisoned captions. To address this issue, we first propose a defense framework, CopyrightShield, to defend against the above attack. Specifically, we analyze the memorization mechanism of diffusion models and find that attacks exploit the model’s overfitting to specific spatial positions and prompts, causing it to reproduce poisoned samples under backdoor triggers. Based on this, we propose a poisoned sample detection method using spatial masking and data attribution to quantify poisoning risk and accurately identify hidden backdoor samples. To further mitigate memorization of poisoned features, we introduce an adaptive optimization strategy that integrates a dynamic penalty term into the training loss, reducing reliance on infringing features while preserving generative performance. Experimental results demonstrate that CopyrightShield significantly improves poisoned sample detection performance across two attack scenarios, achieving average F1-scores of 0.665, retarding the First-Attack Epoch (FAE) of 115.2% and decreasing the Copyright Infringement Rate (CIR) by 56.7%. Compared to the SoTA backdoor defense in diffusion models, the defense effect is improved by about 25%, showcasing its superiority and practicality in enhancing the security of diffusion models. </p>
<blockquote>
<p>扩散模型因其图像合成等领域的出色数据生成能力而备受关注。然而，最近的研究表明，扩散模型容易受到版权侵犯攻击。攻击者将战略修改的的非侵权图像注入训练集，诱导模型在特定中毒标题的提示下生成侵权内容。为了解决这一问题，我们首先提出了一个防御框架CopyrightShield来防范上述攻击。具体来说，我们分析了扩散模型的记忆机制，并发现攻击者利用模型对特定空间位置和提示的过度拟合，在后门触发下复制中毒样本。基于此，我们提出了一种使用空间掩蔽和数据归属的中毒样本检测方法，以量化中毒风险并准确识别隐藏的后门样本。为了进一步减轻对中毒特征的记忆，我们引入了一种自适应优化策略，将动态惩罚项集成到训练损失中，从而在保留生成性能的同时减少了对侵权特征的依赖。实验结果表明，CopyrightShield在两种攻击场景下显著提高了中毒样本的检测性能，平均F1分数为0.665，First-Attack Epoch（FAE）延迟了115.2%，并且降低了版权侵犯率（CIR）约56.7%。与当前最先进的扩散模型后门防御相比，防御效果提高了约25%，证明了其在提高扩散模型安全性方面的优越性和实用性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.01528v2">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>扩散模型因其图像合成等领域的出色数据生成能力而备受关注。然而，近期研究表明，扩散模型易受版权侵犯攻击。攻击者将策略性修改的非侵权图像注入训练集，诱导模型在特定中毒标题的提示下生成侵权内容。为解决这一问题，我们提出防御框架CopyrightShield。通过分析扩散模型的记忆机制，我们发现攻击是利用模型对特定空间位置和提示的过拟合，导致在后门触发下重现中毒样本。基于此，我们提出一种使用空间掩码和数据归属的中毒样本检测方法，以量化中毒风险并准确识别隐藏的后门样本。为进一步减轻对中毒特征的记忆，我们引入自适应优化策略，将动态惩罚项集成到训练损失中，减少了对侵权特征的依赖，同时保持了生成性能。实验结果表明，CopyrightShield在两种攻击场景下显著提高了中毒样本检测性能，平均F1分数为0.665，First-Attack Epoch（FAE）延迟了115.2%，版权侵权率（CIR）降低了56.7%。与当前扩散模型中的后门防御技术相比，其防御效果提高了约25%，显示出其在提高扩散模型安全性方面的优势和实用性。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>扩散模型因其数据生成能力而在图像合成等领域受到广泛关注，但易受版权侵犯攻击。</li>
<li>攻击者通过注入战略修改的非侵权图像和特定中毒标题，诱导模型生成侵权内容。</li>
<li>CopyrightShield防御框架被提出，通过分析和利用扩散模型的记忆机制来对抗这种攻击。</li>
<li>提出一种使用空间掩码和数据归属的中毒样本检测方法，以检测和识别中毒样本。</li>
<li>为减轻对中毒特征的记忆，引入自适应优化策略，集成动态惩罚项到训练损失中。</li>
<li>实验证明CopyrightShield显著提高了中毒样本检测性能，并降低了版权侵权率。</li>
<li>与现有技术相比，CopyrightShield的防御效果有显著提高，显示出其在增强扩散模型安全性方面的优势和实用性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.01528">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-355483a02bd15cf402dd4ff392fd4c03.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2ee2b09d5e15f7f3e648d63ca98a8356.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e81f81940f0416987030d7f0aea40c1a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7ddcc4fec8607c57f739b5cb3016e8d1.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Vulnerabilities-in-AI-generated-Image-Detection-The-Challenge-of-Adversarial-Attacks"><a href="#Vulnerabilities-in-AI-generated-Image-Detection-The-Challenge-of-Adversarial-Attacks" class="headerlink" title="Vulnerabilities in AI-generated Image Detection: The Challenge of   Adversarial Attacks"></a>Vulnerabilities in AI-generated Image Detection: The Challenge of   Adversarial Attacks</h2><p><strong>Authors:Yunfeng Diao, Naixin Zhai, Changtao Miao, Zitong Yu, Xingxing Wei, Xun Yang, Meng Wang</strong></p>
<p>Recent advancements in image synthesis, particularly with the advent of GAN and Diffusion models, have amplified public concerns regarding the dissemination of disinformation. To address such concerns, numerous AI-generated Image (AIGI) Detectors have been proposed and achieved promising performance in identifying fake images. However, there still lacks a systematic understanding of the adversarial robustness of AIGI detectors. In this paper, we examine the vulnerability of state-of-the-art AIGI detectors against adversarial attack under white-box and black-box settings, which has been rarely investigated so far. To this end, we propose a new method to attack AIGI detectors. First, inspired by the obvious difference between real images and fake images in the frequency domain, we add perturbations under the frequency domain to push the image away from its original frequency distribution. Second, we explore the full posterior distribution of the surrogate model to further narrow this gap between heterogeneous AIGI detectors, e.g. transferring adversarial examples across CNNs and ViTs. This is achieved by introducing a novel post-train Bayesian strategy that turns a single surrogate into a Bayesian one, capable of simulating diverse victim models using one pre-trained surrogate, without the need for re-training. We name our method as Frequency-based Post-train Bayesian Attack, or FPBA. Through FPBA, we show that adversarial attack is truly a real threat to AIGI detectors, because FPBA can deliver successful black-box attacks across models, generators, defense methods, and even evade cross-generator detection, which is a crucial real-world detection scenario. The code will be shared upon acceptance. </p>
<blockquote>
<p>近期图像合成领域的进展，尤其是生成对抗网络（GAN）和扩散模型的出现，加剧了公众对传播虚假信息的担忧。为了解决这些担忧，已经提出了许多人工智能生成的图像（AIGI）检测器，并且在识别虚假图像方面取得了令人瞩目的性能。然而，人们对于AIGI检测器的对抗性稳健性仍缺乏系统的理解。在本文中，我们研究了最先进的AIGI检测器在白盒和黑盒设置下对抗攻击的脆弱性，这一领域迄今为止很少被研究。为此，我们提出了一种攻击AIGI检测器的新方法。首先，受真实图像和虚假图像在频域上明显差异的启发，我们在频域中添加扰动，使图像远离其原始频率分布。其次，我们探索了代理模型的后验分布，以进一步缩小不同AIGI检测器之间的差距，例如，在卷积神经网络（CNN）和视觉转换器（ViT）之间转移对抗样本。这是通过引入一种新的后训练贝叶斯策略实现的，该策略将单个代理转变为贝叶斯代理，能够利用一个预训练的代理模拟多种受害者模型，而无需重新训练。我们将我们的方法命名为基于频率的后训练贝叶斯攻击（FPBA）。通过FPBA，我们证明对抗攻击确实对AIGI检测器构成真正的威胁，因为FPBA可以成功地进行跨模型、生成器、防御方法的黑盒攻击，甚至能逃避跨生成器检测，这是关键的现实世界检测场景。代码将在接受后共享。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.20836v4">PDF</a> </p>
<p><strong>摘要</strong><br>随着GAN和Diffusion模型的出现，图像合成技术的最新进展加剧了公众对虚假信息传播的担忧。为应对这些担忧，已经提出了许多AI生成的图像（AIGI）检测器，并在识别虚假图像方面取得了有希望的性能。然而，关于AIGI检测器的对抗性稳健性仍存在系统性的理解不足。本文研究了最先进的AIGI检测器在白盒和黑盒设置下对抗对抗性攻击的脆弱性，这一领域迄今为止很少被研究。为此，我们提出了一种新的攻击AIGI检测器的方法。首先，我们受到真实图像和虚假图像在频域中明显差异的启发，在频域中添加扰动使图像偏离其原始频率分布。其次，我们探索了代理模型的后验分布，以进一步缩小不同AIGI检测器之间的鸿沟。这是通过引入一种新的后训练贝叶斯策略实现的，该策略将单一的代理模型转变为贝叶斯模型，能够模拟使用单一预训练代理的多个受害者模型，无需重新训练。我们称我们的方法为基于频率的后训练贝叶斯攻击（FPBA）。通过FPBA，我们证明了对抗性攻击确实对AIGI检测器构成威胁，因为FPBA可以在不同的模型、生成器、防御方法和跨生成器检测中成功实施黑盒攻击，从而逃避现实世界的检测场景。代码将在接受后共享。</p>
<p><strong>要点</strong></p>
<ol>
<li>最新图像合成技术引发公众对虚假信息传播担忧，AIGI检测器用于识别虚假图像。</li>
<li>当前缺乏关于AIGI检测器对抗性稳健性的系统性理解。</li>
<li>提出了在白盒和黑盒设置下对抗攻击AIGI检测器的新方法——FPBA。</li>
<li>FPBA通过频域扰动攻击检测器，并探索代理模型的后验分布来缩小不同检测器间的差距。</li>
<li>FPBA成功实施黑盒攻击，威胁到AIGI检测器的稳健性，可应用于不同模型、生成器、防御方法和跨生成器检测场景。</li>
<li>所提方法具有广泛应用潜力，可有效逃避现实世界的检测。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.20836">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-05516adc7e53e9da8989586850e5864c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1b75a81980c7c1fd6fa81fd641c70c25.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7c7506823da3f9dac8db5c459d8aace1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-23f50b063da8714d3ca2591c44b226bc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1c57d195cdf3ca8ae2fc7ef7d95d68ab.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-042413491ebb6df9da458ec6cd8da3f3.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Fast-DDPM-Fast-Denoising-Diffusion-Probabilistic-Models-for-Medical-Image-to-Image-Generation"><a href="#Fast-DDPM-Fast-Denoising-Diffusion-Probabilistic-Models-for-Medical-Image-to-Image-Generation" class="headerlink" title="Fast-DDPM: Fast Denoising Diffusion Probabilistic Models for Medical   Image-to-Image Generation"></a>Fast-DDPM: Fast Denoising Diffusion Probabilistic Models for Medical   Image-to-Image Generation</h2><p><strong>Authors:Hongxu Jiang, Muhammad Imran, Teng Zhang, Yuyin Zhou, Muxuan Liang, Kuang Gong, Wei Shao</strong></p>
<p>Denoising diffusion probabilistic models (DDPMs) have achieved unprecedented success in computer vision. However, they remain underutilized in medical imaging, a field crucial for disease diagnosis and treatment planning. This is primarily due to the high computational cost associated with (1) the use of large number of time steps (e.g., 1,000) in diffusion processes and (2) the increased dimensionality of medical images, which are often 3D or 4D. Training a diffusion model on medical images typically takes days to weeks, while sampling each image volume takes minutes to hours. To address this challenge, we introduce Fast-DDPM, a simple yet effective approach capable of improving training speed, sampling speed, and generation quality simultaneously. Unlike DDPM, which trains the image denoiser across 1,000 time steps, Fast-DDPM trains and samples using only 10 time steps. The key to our method lies in aligning the training and sampling procedures to optimize time-step utilization. Specifically, we introduced two efficient noise schedulers with 10 time steps: one with uniform time step sampling and another with non-uniform sampling. We evaluated Fast-DDPM across three medical image-to-image generation tasks: multi-image super-resolution, image denoising, and image-to-image translation. Fast-DDPM outperformed DDPM and current state-of-the-art methods based on convolutional networks and generative adversarial networks in all tasks. Additionally, Fast-DDPM reduced the training time to 0.2x and the sampling time to 0.01x compared to DDPM. Our code is publicly available at: <a target="_blank" rel="noopener" href="https://github.com/mirthAI/Fast-DDPM">https://github.com/mirthAI/Fast-DDPM</a>. </p>
<blockquote>
<p>去噪扩散概率模型（DDPM）在计算机视觉领域取得了前所未有的成功。然而，它们在医学影像这一对疾病诊断和治疗计划至关重要的领域却未得到充分利用。这主要是因为与（1）扩散过程中使用大量时间步数（例如，高达一千步）和（2）医学图像增加的维度（通常为三维或四维）相关的计算成本很高。在医学图像上训练扩散模型通常需要数天至数周的时间，而对每个图像体积进行采样则需要数分钟至数小时的时间。为了应对这一挑战，我们引入了Fast-DDPM，这是一种简单而有效的方法，可以同时提高训练速度、采样速度和生成质量。与DDPM不同，DDPM是在一千个时间步长上训练图像去噪器，而Fast-DDPM则仅使用十个时间步长进行训练和采样。我们的方法的关键在于对齐训练和采样程序以优化时间步长的利用。具体来说，我们引入了两种具有十个时间步长的有效噪声调度器：一种具有均匀时间步长采样，另一种具有非均匀采样。我们在三项医学图像到图像生成任务中评估了Fast-DDPM：多图像超分辨率、图像去噪和图像到图像转换。Fast-DDPM在所有任务中都优于DDPM和基于卷积网络和生成对抗网络的最先进方法。此外，与DDPM相比，Fast-DDPM将训练时间缩短至原来的0.2倍，采样时间缩短至原来的0.01倍。我们的代码可在以下网址公开访问：<a target="_blank" rel="noopener" href="https://github.com/mirthAI/Fast-DDPM">https://github.com/mirthAI/Fast-DDPM</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.14802v3">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>扩散概率模型（DDPM）在计算机视觉领域取得了前所未有的成功，但在医学成像领域的应用仍然有限。这是因为医学图像的维度通常较高，为三维或四维，且扩散过程中需要大量的时间步骤（例如1000步），导致计算成本高昂。针对这一问题，我们提出了Fast-DDPM方法，该方法能在训练速度、采样速度生成质量方面同时实现改进。与DDPM不同的是，Fast-DDPM仅使用10个时间步骤进行训练和采样。我们引入了两种高效噪声调度器，一种采用均匀时间步长采样，另一种采用非均匀采样。在三项医学图像生成任务中，Fast-DDPM表现优异，包括多图像超分辨率、图像去噪和图像到图像的转换。与DDPM和当前最先进的卷积网络和生成对抗网络相比，Fast-DDPM在所有任务中的表现均有所超越。此外，Fast-DDPM将训练时间缩短至DDPM的0.2倍，采样时间缩短至DDPM的0.01倍。我们的代码已公开发布在：<a target="_blank" rel="noopener" href="https://github.com/mirthAI/Fast-DDPM%E3%80%82">https://github.com/mirthAI/Fast-DDPM。</a> </p>
<p><strong>要点摘要</strong></p>
<ol>
<li>DDPM在计算机视觉领域表现出色，但在医学成像领域因高计算成本而受到限制。</li>
<li>Fast-DDPM方法旨在解决这一问题，通过优化时间步长利用，显著提高了训练速度和采样速度。</li>
<li>Fast-DDPM仅使用10个时间步骤进行训练和采样，引入两种噪声调度器以实现高效采样。</li>
<li>在三项医学图像生成任务中，Fast-DDPM表现优于DDPM和其他最先进的方法。</li>
<li>Fast-DDPM将训练时间和采样时间都大幅度减少。</li>
<li>公开的代码可在<a target="_blank" rel="noopener" href="https://github.com/mirthAI/Fast-DDPM%E4%B8%8A%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/mirthAI/Fast-DDPM上访问。</a> </li>
<li>Fast-DDPM为医学成像领域带来了新的可能性，有望改善疾病诊断和治疗计划。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.14802">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-66f3f979a11c37d422d7825cf371bd3f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-49dcefa62fbe94e1da22230d4282a004.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b6fe4f49d2628659ff7ddd8f9e5919e0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-491c4c5948d4b29c67ee3f8e573971c4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-da26bd395e39747c5897d0ecd8ca1045.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e5cc024d598392bc76826984d4bab24b.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-23/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-23/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-23/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-85b76ff49610b77e13804b71b66ab455.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-08-23  Hessian-based lightweight neural network for brain vessel segmentation   on a minimal training dataset
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-23
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-23/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-03bea3cfcc4280abe86b1689b675ddf4.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2025-08-23  GOGS High-Fidelity Geometry and Relighting for Glossy Objects via   Gaussian Surfels
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-23
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30191.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
