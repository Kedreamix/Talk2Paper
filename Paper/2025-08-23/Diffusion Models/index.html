<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-23  CineScale Free Lunch in High-Resolution Cinematic Visual Generation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-c71b5e238d840e076feb4e10f2993009.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-23
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-08
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    13.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    56 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-23-æ›´æ–°"><a href="#2025-08-23-æ›´æ–°" class="headerlink" title="2025-08-23 æ›´æ–°"></a>2025-08-23 æ›´æ–°</h1><h2 id="CineScale-Free-Lunch-in-High-Resolution-Cinematic-Visual-Generation"><a href="#CineScale-Free-Lunch-in-High-Resolution-Cinematic-Visual-Generation" class="headerlink" title="CineScale: Free Lunch in High-Resolution Cinematic Visual Generation"></a>CineScale: Free Lunch in High-Resolution Cinematic Visual Generation</h2><p><strong>Authors:Haonan Qiu, Ning Yu, Ziqi Huang, Paul Debevec, Ziwei Liu</strong></p>
<p>Visual diffusion models achieve remarkable progress, yet they are typically trained at limited resolutions due to the lack of high-resolution data and constrained computation resources, hampering their ability to generate high-fidelity images or videos at higher resolutions. Recent efforts have explored tuning-free strategies to exhibit the untapped potential higher-resolution visual generation of pre-trained models. However, these methods are still prone to producing low-quality visual content with repetitive patterns. The key obstacle lies in the inevitable increase in high-frequency information when the model generates visual content exceeding its training resolution, leading to undesirable repetitive patterns deriving from the accumulated errors. In this work, we propose CineScale, a novel inference paradigm to enable higher-resolution visual generation. To tackle the various issues introduced by the two types of video generation architectures, we propose dedicated variants tailored to each. Unlike existing baseline methods that are confined to high-resolution T2I and T2V generation, CineScale broadens the scope by enabling high-resolution I2V and V2V synthesis, built atop state-of-the-art open-source video generation frameworks. Extensive experiments validate the superiority of our paradigm in extending the capabilities of higher-resolution visual generation for both image and video models. Remarkably, our approach enables 8k image generation without any fine-tuning, and achieves 4k video generation with only minimal LoRA fine-tuning. Generated video samples are available at our website: <a target="_blank" rel="noopener" href="https://eyeline-labs.github.io/CineScale/">https://eyeline-labs.github.io/CineScale/</a>. </p>
<blockquote>
<p>è§†è§‰æ‰©æ•£æ¨¡å‹å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä½†ç”±äºç¼ºä¹é«˜åˆ†è¾¨ç‡æ•°æ®å’Œå—é™çš„è®¡ç®—èµ„æºï¼Œå®ƒä»¬é€šå¸¸åªåœ¨æœ‰é™çš„åˆ†è¾¨ç‡ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè¿™é˜»ç¢äº†å®ƒä»¬ç”Ÿæˆæ›´é«˜åˆ†è¾¨ç‡çš„é«˜ä¿çœŸå›¾åƒæˆ–è§†é¢‘çš„èƒ½åŠ›ã€‚æœ€è¿‘çš„ç ”ç©¶è‡´åŠ›äºæ¢ç´¢æ— éœ€è°ƒæ•´çš„ç­–ç•¥ï¼Œä»¥å±•ç¤ºé¢„è®­ç»ƒæ¨¡å‹åœ¨æ›´é«˜åˆ†è¾¨ç‡è§†è§‰ç”Ÿæˆæ–¹é¢çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ä»ç„¶å®¹æ˜“äº§ç”Ÿé‡å¤æ¨¡å¼çš„ä½è´¨é‡è§†è§‰å†…å®¹ã€‚å…³é”®éšœç¢åœ¨äºå½“æ¨¡å‹ç”Ÿæˆè¶…è¿‡å…¶è®­ç»ƒåˆ†è¾¨ç‡çš„è§†è§‰å†…å®¹æ—¶ï¼Œé«˜é¢‘ä¿¡æ¯çš„å¿…ç„¶å¢åŠ ä¼šå¯¼è‡´ç”±ç´¯ç§¯è¯¯å·®å¼•èµ·çš„ä¸å¯å–çš„é‡å¤æ¨¡å¼ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†CineScaleï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ¨ç†èŒƒå¼ï¼Œå¯å®ç°æ›´é«˜åˆ†è¾¨ç‡çš„è§†è§‰ç”Ÿæˆã€‚ä¸ºäº†è§£å†³ç”±ä¸¤ç§è§†é¢‘ç”Ÿæˆæ¶æ„å¼•å…¥çš„å„ç§é—®é¢˜ï¼Œæˆ‘ä»¬é’ˆå¯¹æ¯ç§æ¶æ„æå‡ºäº†ä¸“ç”¨å˜ä½“ã€‚ä¸ä»…é™äºé«˜åˆ†è¾¨ç‡T2Iå’ŒT2Vç”Ÿæˆçš„ç°æœ‰åŸºå‡†æ–¹æ³•ä¸åŒï¼ŒCineScaleé€šè¿‡æ”¯æŒé«˜åˆ†è¾¨ç‡I2Vå’ŒV2Våˆæˆæ¥æ‰©å¤§èŒƒå›´ï¼Œå»ºç«‹åœ¨æœ€æ–°å¼€æºè§†é¢‘ç”Ÿæˆæ¡†æ¶ä¹‹ä¸Šã€‚å¤§é‡å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„èŒƒå¼åœ¨æ‰©å±•å›¾åƒå’Œè§†é¢‘æ¨¡å‹çš„é«˜åˆ†è¾¨ç‡ç”Ÿæˆèƒ½åŠ›æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿåœ¨æ— éœ€å¾®è°ƒçš„æƒ…å†µä¸‹å®ç°8kå›¾åƒç”Ÿæˆï¼Œå¹¶ä¸”åªéœ€è¿›è¡Œå¾®å°çš„LoRAå¾®è°ƒå³å¯å®ç°4kè§†é¢‘ç”Ÿæˆã€‚ç”Ÿæˆçš„è§†é¢‘æ ·æœ¬å¯åœ¨æˆ‘ä»¬çš„ç½‘ç«™ä¸Šè¿›è¡ŒæŸ¥çœ‹ï¼š<a target="_blank" rel="noopener" href="https://eyeline-labs.github.io/CineScale/%E3%80%82">https://eyeline-labs.github.io/CineScale/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15774v1">PDF</a> CineScale is an extended work of FreeScale (ICCV 2025). Project Page:   <a target="_blank" rel="noopener" href="https://eyeline-labs.github.io/CineScale/">https://eyeline-labs.github.io/CineScale/</a>, Code Repo:   <a target="_blank" rel="noopener" href="https://github.com/Eyeline-Labs/CineScale">https://github.com/Eyeline-Labs/CineScale</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºCineScaleçš„æ–°å‹æ¨ç†èŒƒå¼ï¼Œæ—¨åœ¨å®ç°æ›´é«˜åˆ†è¾¨ç‡çš„è§†è§‰ç”Ÿæˆã€‚è¯¥èŒƒå¼è§£å†³äº†ç”±äºæ¨¡å‹ç”Ÿæˆè¶…è¿‡è®­ç»ƒåˆ†è¾¨ç‡çš„è§†è§‰å†…å®¹æ—¶é«˜é¢‘ä¿¡æ¯ä¸å¯é¿å…å¢åŠ è€Œå¯¼è‡´çš„é—®é¢˜ï¼Œé¿å…äº†äº§ç”Ÿé‡å¤æ¨¡å¼ã€‚CineScaleä¸ä»…é™äºé«˜åˆ†è¾¨ç‡çš„T2Iå’ŒT2Vç”Ÿæˆï¼Œè¿˜èƒ½å®ç°é«˜åˆ†è¾¨ç‡çš„I2Vå’ŒV2Våˆæˆã€‚å®éªŒè¯æ˜ï¼Œè¯¥èŒƒå¼åœ¨å›¾åƒå’Œè§†é¢‘æ¨¡å‹çš„æ›´é«˜åˆ†è¾¨ç‡è§†è§‰ç”Ÿæˆèƒ½åŠ›æ–¹é¢å…·æœ‰ä¼˜è¶Šæ€§ï¼Œèƒ½å¤Ÿåœ¨ä¸è¿›è¡Œå¾®è°ƒçš„æƒ…å†µä¸‹ç”Ÿæˆ8kå›¾åƒï¼Œå¹¶ä¸”åªéœ€æœ€å°çš„LoRAå¾®è°ƒå³å¯å®ç°4kè§†é¢‘ç”Ÿæˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CineScaleæ˜¯ä¸€ç§æ–°å‹æ¨ç†èŒƒå¼ï¼Œæ—¨åœ¨å®ç°æ›´é«˜åˆ†è¾¨ç‡çš„è§†è§‰ç”Ÿæˆã€‚</li>
<li>è¯¥æ–¹æ³•è§£å†³äº†åœ¨ç”Ÿæˆè¶…è¿‡è®­ç»ƒåˆ†è¾¨ç‡çš„è§†è§‰å†…å®¹æ—¶äº§ç”Ÿçš„é‡å¤æ¨¡å¼é—®é¢˜ã€‚</li>
<li>CineScaleä¸ä»…é€‚ç”¨äºé«˜åˆ†è¾¨ç‡çš„T2Iå’ŒT2Vç”Ÿæˆï¼Œè¿˜æ”¯æŒé«˜åˆ†è¾¨ç‡çš„I2Vå’ŒV2Våˆæˆã€‚</li>
<li>å®éªŒè¯æ˜CineScaleåœ¨å›¾åƒå’Œè§†é¢‘æ¨¡å‹çš„æ›´é«˜åˆ†è¾¨ç‡è§†è§‰ç”Ÿæˆæ–¹é¢å…·æœ‰ä¼˜è¶Šæ€§ã€‚</li>
<li>CineScaleèƒ½å¤Ÿåœ¨ä¸è¿›è¡Œå¾®è°ƒçš„æƒ…å†µä¸‹ç”Ÿæˆ8kå›¾åƒã€‚</li>
<li>åªéœ€æœ€å°çš„LoRAå¾®è°ƒï¼ŒCineScaleå³å¯å®ç°4kè§†é¢‘ç”Ÿæˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15774">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-72613e33ecc86e68d31c6c55cc0b0a5c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eee40583539fd23e15032c2c496c12be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c2069e2f2c4e5020e923803a40a14226.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1978d2d0726bf3439670a2f23dacf32c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a8dcaa90351012af1ebd8957aafca51f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a7f2337011bb3549cd5bdeebf816c703.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Visual-Autoregressive-Modeling-for-Instruction-Guided-Image-Editing"><a href="#Visual-Autoregressive-Modeling-for-Instruction-Guided-Image-Editing" class="headerlink" title="Visual Autoregressive Modeling for Instruction-Guided Image Editing"></a>Visual Autoregressive Modeling for Instruction-Guided Image Editing</h2><p><strong>Authors:Qingyang Mao, Qi Cai, Yehao Li, Yingwei Pan, Mingyue Cheng, Ting Yao, Qi Liu, Tao Mei</strong></p>
<p>Recent advances in diffusion models have brought remarkable visual fidelity to instruction-guided image editing. However, their global denoising process inherently entangles the edited region with the entire image context, leading to unintended spurious modifications and compromised adherence to editing instructions. In contrast, autoregressive models offer a distinct paradigm by formulating image synthesis as a sequential process over discrete visual tokens. Their causal and compositional mechanism naturally circumvents the adherence challenges of diffusion-based methods. In this paper, we present VAREdit, a visual autoregressive (VAR) framework that reframes image editing as a next-scale prediction problem. Conditioned on source image features and text instructions, VAREdit generates multi-scale target features to achieve precise edits. A core challenge in this paradigm is how to effectively condition the source image tokens. We observe that finest-scale source features cannot effectively guide the prediction of coarser target features. To bridge this gap, we introduce a Scale-Aligned Reference (SAR) module, which injects scale-matched conditioning information into the first self-attention layer. VAREdit demonstrates significant advancements in both editing adherence and efficiency. On standard benchmarks, it outperforms leading diffusion-based methods by 30%+ higher GPT-Balance score. Moreover, it completes a $512\times512$ editing in 1.2 seconds, making it 2.2$\times$ faster than the similarly sized UltraEdit. The models are available at <a target="_blank" rel="noopener" href="https://github.com/HiDream-ai/VAREdit">https://github.com/HiDream-ai/VAREdit</a>. </p>
<blockquote>
<p>è¿‘æœŸæ‰©æ•£æ¨¡å‹ï¼ˆDiffusion Modelsï¼‰çš„è¿›å±•ä¸ºæŒ‡ä»¤å¯¼å‘çš„å›¾åƒç¼–è¾‘å¸¦æ¥äº†æ˜¾è‘—çš„è§†è§‰ä¿çœŸåº¦ã€‚ç„¶è€Œï¼Œå…¶å…¨å±€å»å™ªè¿‡ç¨‹å›ºæœ‰åœ°å°†ç¼–è¾‘åŒºåŸŸä¸æ•´ä¸ªå›¾åƒä¸Šä¸‹æ–‡çº ç¼ åœ¨ä¸€èµ·ï¼Œå¯¼è‡´å‡ºç°æ„å¤–çš„è™šå‡ä¿®æ”¹ä»¥åŠä¸ç¬¦åˆç¼–è¾‘æŒ‡ä»¤çš„æƒ…å†µã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œè‡ªå›å½’æ¨¡å‹ï¼ˆAutoregressive Modelsï¼‰é€šè¿‡ä¸ºç¦»æ•£è§†è§‰ç¬¦å·åˆ¶å®šä¸€ä¸ªåºåˆ—è¿‡ç¨‹æ¥æä¾›ç‹¬ç‰¹çš„èŒƒå¼ï¼Œå®ç°äº†å›¾åƒåˆæˆã€‚å®ƒä»¬çš„å› æœå’Œç»„åˆæœºåˆ¶è‡ªç„¶åœ°é¿å…äº†åŸºäºæ‰©æ•£çš„æ–¹æ³•çš„éµå¾ªæŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†VAREditï¼Œä¸€ä¸ªè§†è§‰è‡ªå›å½’ï¼ˆVARï¼‰æ¡†æ¶ï¼Œå®ƒå°†å›¾åƒç¼–è¾‘é‡æ–°å®šä¹‰ä¸ºä¸‹ä¸€ä¸ªå°ºåº¦çš„é¢„æµ‹é—®é¢˜ã€‚åœ¨æºå›¾åƒç‰¹å¾å’Œæ–‡æœ¬æŒ‡ä»¤çš„æ¡ä»¶ä¸‹ï¼ŒVAREditç”Ÿæˆå¤šå°ºåº¦ç›®æ ‡ç‰¹å¾æ¥å®ç°ç²¾ç¡®ç¼–è¾‘ã€‚åœ¨æ­¤èŒƒå¼ä¸­çš„æ ¸å¿ƒæŒ‘æˆ˜æ˜¯å¦‚ä½•æœ‰æ•ˆåœ°è°ƒèŠ‚æºå›¾åƒç¬¦å·ã€‚æˆ‘ä»¬å‘ç°æœ€ç²¾ç»†å°ºåº¦çš„æºç‰¹å¾æ— æ³•æœ‰æ•ˆåœ°æŒ‡å¯¼è¾ƒç²—ç³™ç›®æ ‡ç‰¹å¾çš„é¢„æµ‹ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†å°ºåº¦å¯¹é½å‚è€ƒï¼ˆSARï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—å°†å°ºåº¦åŒ¹é…çš„è°ƒèŠ‚ä¿¡æ¯æ³¨å…¥åˆ°ç¬¬ä¸€å±‚è‡ªæˆ‘å…³æ³¨ä¸­ã€‚VAREditåœ¨ç¼–è¾‘çš„éµå¾ªæ€§å’Œæ•ˆç‡æ–¹é¢éƒ½å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­ï¼Œå®ƒçš„GPTå¹³è¡¡å¾—åˆ†æ¯”é¢†å…ˆçš„æ‰©æ•£æ¨¡å‹é«˜å‡º30%ä»¥ä¸Šã€‚æ­¤å¤–ï¼Œå®ƒèƒ½åœ¨1.2ç§’å†…å®Œæˆä¸€ä¸ª512x512çš„ç¼–è¾‘ä»»åŠ¡ï¼Œç›¸æ¯”ä¹‹ä¸‹æ¯”ç›¸åŒè§„æ¨¡çš„UltraEditå¿«2.2å€ã€‚æ¨¡å‹å¯ä»¥åœ¨<a target="_blank" rel="noopener" href="https://github.com/HiDream-ai/VAREdit%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/HiDream-ai/VAREditæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15772v1">PDF</a> Source codes and models are available at   <a target="_blank" rel="noopener" href="https://github.com/HiDream-ai/VAREdit">https://github.com/HiDream-ai/VAREdit</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºè§†è§‰è‡ªå›å½’ï¼ˆVARï¼‰æ¡†æ¶çš„å›¾åƒç¼–è¾‘æŠ€æœ¯VAREditã€‚è¯¥æŠ€æœ¯å°†å›¾åƒç¼–è¾‘é‡æ–°å®šä¹‰ä¸ºä¸‹ä¸€ä¸ªå°ºåº¦é¢„æµ‹é—®é¢˜ï¼Œé€šè¿‡æºå›¾åƒç‰¹å¾å’Œæ–‡æœ¬æŒ‡ä»¤ç”Ÿæˆå¤šå°ºåº¦ç›®æ ‡ç‰¹å¾æ¥å®ç°ç²¾ç¡®ç¼–è¾‘ã€‚ä¸ºå¼¥è¡¥æœ€ç²¾ç»†å°ºåº¦æºç‰¹å¾æ— æ³•æœ‰æ•ˆå¼•å¯¼è¾ƒç²—ç›®æ ‡ç‰¹å¾é¢„æµ‹çš„ç¼ºé™·ï¼Œå¼•å…¥äº†è§„æ¨¡å¯¹é½å‚è€ƒï¼ˆSARï¼‰æ¨¡å—ï¼Œå°†è§„æ¨¡åŒ¹é…çš„è°ƒèŠ‚ä¿¡æ¯æ³¨å…¥ç¬¬ä¸€å±‚è‡ªæ³¨æ„åŠ›ä¸­ã€‚ç›¸è¾ƒäºé¢†å…ˆçš„æ‰©æ•£æ¨¡å‹ï¼ŒVAREditåœ¨ç¼–è¾‘è´´åˆåº¦å’Œæ•ˆç‡ä¸Šå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚å®ƒåœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šçš„GPT-Balanceå¾—åˆ†é«˜å‡º30%ä»¥ä¸Šï¼Œä¸”å®Œæˆä¸€æ¬¡512x512çš„ç¼–è¾‘ä»…éœ€1.2ç§’ï¼Œç›¸è¾ƒäºç±»ä¼¼è§„æ¨¡çš„UltraEditåŠ é€Ÿçº¦ä¸¤å€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>VAREditåˆ©ç”¨è§†è§‰è‡ªå›å½’æ¡†æ¶è¿›è¡Œå›¾åƒç¼–è¾‘ï¼Œå°†ç¼–è¾‘å®šä¹‰ä¸ºä¸‹ä¸€ä¸ªå°ºåº¦é¢„æµ‹é—®é¢˜ã€‚</li>
<li>VAREdité€šè¿‡ç»“åˆæºå›¾åƒç‰¹å¾å’Œæ–‡æœ¬æŒ‡ä»¤ç”Ÿæˆå¤šå°ºåº¦ç›®æ ‡ç‰¹å¾ï¼Œå®ç°ç²¾ç¡®ç¼–è¾‘ã€‚</li>
<li>å¼•å…¥è§„æ¨¡å¯¹é½å‚è€ƒï¼ˆSARï¼‰æ¨¡å—ï¼Œä»¥è§£å†³æœ€ç²¾ç»†å°ºåº¦æºç‰¹å¾æ— æ³•æœ‰æ•ˆå¼•å¯¼è¾ƒç²—ç›®æ ‡ç‰¹å¾é¢„æµ‹çš„é—®é¢˜ã€‚</li>
<li>VAREditåœ¨ç¼–è¾‘è´´åˆåº¦å’Œæ•ˆç‡æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ‰©æ•£æ¨¡å‹ã€‚</li>
<li>VAREditåœ¨æ ‡å‡†æµ‹è¯•ä¸­çš„GPT-Balanceå¾—åˆ†é«˜å‡º30%ä»¥ä¸Šã€‚</li>
<li>VAREditå®Œæˆä¸€æ¬¡512x512çš„ç¼–è¾‘ä»…éœ€1.2ç§’ï¼Œç›¸è¾ƒäºç±»ä¼¼è§„æ¨¡çš„ç¼–è¾‘å™¨ï¼Œæ•ˆç‡æ›´é«˜ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15772">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-13d23621db80188712c9307e624edbcb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b1fbf565289113329bf17a2c75602deb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-068aa3fcf0647fb08e9c7ff101f1edba.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8cfb6e71ce741eadad1f1e1b3a7b99e3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8018b44231e0aa61695e4b8c45a1479a.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Probability-Density-from-Latent-Diffusion-Models-for-Out-of-Distribution-Detection"><a href="#Probability-Density-from-Latent-Diffusion-Models-for-Out-of-Distribution-Detection" class="headerlink" title="Probability Density from Latent Diffusion Models for Out-of-Distribution   Detection"></a>Probability Density from Latent Diffusion Models for Out-of-Distribution   Detection</h2><p><strong>Authors:Joonas JÃ¤rve, Karl Kaspar Haavel, Meelis Kull</strong></p>
<p>Despite rapid advances in AI, safety remains the main bottleneck to deploying machine-learning systems. A critical safety component is out-of-distribution detection: given an input, decide whether it comes from the same distribution as the training data. In generative models, the most natural OOD score is the data likelihood. Actually, under the assumption of uniformly distributed OOD data, the likelihood is even the optimal OOD detector, as we show in this work. However, earlier work reported that likelihood often fails in practice, raising doubts about its usefulness. We explore whether, in practice, the representation space also suffers from the inability to learn good density estimation for OOD detection, or if it is merely a problem of the pixel space typically used in generative models. To test this, we trained a Variational Diffusion Model not on images, but on the representation space of a pre-trained ResNet-18 to assess the performance of our likelihood-based detector in comparison to state-of-the-art methods from the OpenOOD suite. </p>
<blockquote>
<p>å°½ç®¡äººå·¥æ™ºèƒ½é¢†åŸŸå–å¾—äº†é£é€Ÿå‘å±•ï¼Œå®‰å…¨ä»æ˜¯éƒ¨ç½²æœºå™¨å­¦ä¹ ç³»ç»Ÿçš„ä¸»ç“¶é¢ˆã€‚ä¸€ä¸ªå…³é”®çš„å®‰å…¨ç»„ä»¶æ˜¯å¼‚å¸¸æ£€æµ‹ï¼šç»™å®šè¾“å…¥ï¼Œç¡®å®šå…¶æ˜¯å¦æ¥è‡ªä¸è®­ç»ƒæ•°æ®ç›¸åŒçš„åˆ†å¸ƒã€‚åœ¨ç”Ÿæˆæ¨¡å‹ä¸­ï¼Œæœ€è‡ªç„¶çš„å¼‚å¸¸å€¼åˆ†æ•°æ˜¯æ•°æ®æ¦‚ç‡ã€‚å®é™…ä¸Šï¼Œåœ¨å‡è®¾å¼‚å¸¸å€¼æ•°æ®å‡åŒ€åˆ†å¸ƒçš„æƒ…å†µä¸‹ï¼Œæ¦‚ç‡ç”šè‡³æ˜¯æœ€ä½³çš„å¼‚å¸¸æ£€æµ‹å™¨ï¼Œæ­£å¦‚æˆ‘ä»¬åœ¨æœ¬å·¥ä½œä¸­æ‰€å±•ç¤ºçš„ã€‚ç„¶è€Œï¼Œæ—©æœŸçš„ç ”ç©¶æŠ¥å‘ŠæŒ‡å‡ºï¼Œåœ¨å®è·µä¸­æ¦‚ç‡é€šå¸¸ä¼šå¤±æ•ˆï¼Œè¿™å¼•å‘äº†å¯¹å…¶æœ‰ç”¨æ€§çš„æ€€ç–‘ã€‚æˆ‘ä»¬æ¢ç´¢çš„æ˜¯åœ¨å®è·µä¸­ï¼Œè¡¨ç¤ºç©ºé—´æ˜¯å¦ä¹Ÿå› æ— æ³•è¿›è¡Œè‰¯å¥½çš„å¯†åº¦ä¼°è®¡æ¥è¿›è¡Œå¼‚å¸¸æ£€æµ‹è€Œå—åˆ°å½±å“ï¼Œè¿˜æ˜¯ä»…ä»…æ˜¯å› ä¸ºç”Ÿæˆæ¨¡å‹ä¸­é€šå¸¸ä½¿ç”¨çš„åƒç´ ç©ºé—´çš„é—®é¢˜ã€‚ä¸ºäº†æµ‹è¯•è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªå˜åˆ†æ‰©æ•£æ¨¡å‹ï¼Œä¸ä½¿ç”¨å›¾åƒï¼Œè€Œæ˜¯ä½¿ç”¨é¢„è®­ç»ƒçš„ResNet-1</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15737v1">PDF</a> ECAI 2025</p>
<p><strong>Summary</strong><br>     å°½ç®¡äººå·¥æ™ºèƒ½å‘å±•è¿…é€Ÿï¼Œä½†å®‰å…¨ä»æ˜¯éƒ¨ç½²æœºå™¨å­¦ä¹ ç³»ç»Ÿçš„ä¸»ç“¶é¢ˆã€‚å…³é”®çš„å®‰å…¨ç»„ä»¶æ˜¯åˆ†å¸ƒå¤–æ£€æµ‹ï¼šç»™å®šè¾“å…¥ï¼Œç¡®å®šå…¶æ˜¯å¦æ¥è‡ªä¸è®­ç»ƒæ•°æ®ç›¸åŒçš„åˆ†å¸ƒã€‚åœ¨ç”Ÿæˆæ¨¡å‹ä¸­ï¼Œæœ€è‡ªç„¶çš„OODåˆ†æ•°æ˜¯æ•°æ®å¯èƒ½æ€§ã€‚å®é™…ä¸Šï¼Œåœ¨å‡è®¾OODæ•°æ®å‡åŒ€åˆ†å¸ƒçš„æƒ…å†µä¸‹ï¼Œå¯èƒ½æ€§ç”šè‡³æ˜¯æœ€ä½³çš„OODæ£€æµ‹å™¨ï¼Œæˆ‘ä»¬åœ¨å·¥ä½œä¸­ä¹Ÿè¯æ˜äº†è¿™ä¸€ç‚¹ã€‚ç„¶è€Œï¼Œæ—©æœŸçš„ç ”ç©¶æŠ¥å‘ŠæŒ‡å‡ºï¼Œåœ¨å®é™…æ“ä½œä¸­å¯èƒ½æ€§ç»å¸¸ä¼šå¤±æ•ˆï¼Œå¼•å‘äº†å¯¹å…¶å®ç”¨æ€§çš„è´¨ç–‘ã€‚æˆ‘ä»¬æ¢ç©¶åœ¨å®æ“ä¸­æ˜¯å¦æ˜¯è¡¨ç¤ºç©ºé—´é­å—äº†æ— æ³•å­¦ä¹ è‰¯å¥½çš„å¯†åº¦ä¼°è®¡ä»¥è¿›è¡ŒOODæ£€æµ‹çš„å›°æ‰°ï¼Œè¿˜æ˜¯ä»…ä»…æ˜¯ç”Ÿæˆæ¨¡å‹ä¸­é€šå¸¸ä½¿ç”¨çš„åƒç´ ç©ºé—´çš„é—®é¢˜ã€‚ä¸ºäº†æµ‹è¯•è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªå˜åˆ†æ‰©æ•£æ¨¡å‹ï¼Œä¸æ˜¯å¯¹å›¾åƒè¿›è¡Œè®­ç»ƒï¼Œè€Œæ˜¯å¯¹é¢„è®­ç»ƒResNet-18çš„è¡¨ç¤ºç©ºé—´è¿›è¡Œè¯„ä¼°ï¼Œä»¥æ¯”è¾ƒåŸºäºå¯èƒ½æ€§çš„æ£€æµ‹å™¨ä¸OpenOODå¥—ä»¶ä¸­çš„æœ€æ–°æ–¹æ³•çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AIçš„å¿«é€Ÿå‘å±•é¢ä¸´å®‰å…¨ç“¶é¢ˆï¼Œå°¤å…¶æ˜¯éƒ¨ç½²æœºå™¨å­¦ä¹ ç³»ç»Ÿçš„å®‰å…¨ã€‚</li>
<li>åˆ†å¸ƒå¤–æ£€æµ‹æ˜¯æœºå™¨å­¦ä¹ å®‰å…¨ä¸­çš„å…³é”®ç»„ä»¶ï¼Œç”¨äºåˆ¤æ–­è¾“å…¥æ˜¯å¦æ¥è‡ªè®­ç»ƒæ•°æ®åˆ†å¸ƒã€‚</li>
<li>åœ¨ç”Ÿæˆæ¨¡å‹ä¸­ï¼Œæ•°æ®å¯èƒ½æ€§æ˜¯æœ€è‡ªç„¶çš„OODæ£€æµ‹æŒ‡æ ‡ï¼Œä½†åœ¨å®è·µä¸­ç»å¸¸è¢«å‘ç°åœ¨æ€§èƒ½ä¸Šçš„é—®é¢˜ã€‚</li>
<li>å˜åˆ†æ‰©æ•£æ¨¡å‹åœ¨è¡¨ç¤ºç©ºé—´è€Œéå›¾åƒä¸Šè¿›è¡Œè®­ç»ƒï¼Œä»¥è¯„ä¼°åŸºäºå¯èƒ½æ€§çš„æ£€æµ‹å™¨çš„æ€§èƒ½ã€‚</li>
<li>ç°æœ‰çš„å¯¹å¯èƒ½æ€§çš„è´¨ç–‘éƒ¨åˆ†æºäºå®æ“ä¸­çš„é—®é¢˜ï¼Œå¯èƒ½ä¸è¡¨ç¤ºç©ºé—´æˆ–åƒç´ ç©ºé—´çš„ç‰¹æ€§æœ‰å…³ã€‚</li>
<li>æœ¬ç ”ç©¶é€šè¿‡å¯¹æ¯”å®éªŒæ¥æ¢ç©¶åœ¨è¡¨ç¤ºç©ºé—´ä¸­è¿›è¡ŒOODæ£€æµ‹æ˜¯å¦ä¹Ÿä¼šé­é‡å¯†åº¦ä¼°è®¡é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15737">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fe019682510d54cb3942a9f7190a6662.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8b39657e351481403947c19d13cfc5b9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4a54387d4728c50d350e78eba02f5454.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-02f86b889c2040022a92199ba4dbf7da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f73282a3a938095008ffc77ea9485a2.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Pathology-Informed-Latent-Diffusion-Model-for-Anomaly-Detection-in-Lymph-Node-Metastasis"><a href="#Pathology-Informed-Latent-Diffusion-Model-for-Anomaly-Detection-in-Lymph-Node-Metastasis" class="headerlink" title="Pathology-Informed Latent Diffusion Model for Anomaly Detection in Lymph   Node Metastasis"></a>Pathology-Informed Latent Diffusion Model for Anomaly Detection in Lymph   Node Metastasis</h2><p><strong>Authors:Jiamu Wang, Keunho Byeon, Jinsol Song, Anh Nguyen, Sangjeong Ahn, Sung Hak Lee, Jin Tae Kwak</strong></p>
<p>Anomaly detection is an emerging approach in digital pathology for its ability to efficiently and effectively utilize data for disease diagnosis. While supervised learning approaches deliver high accuracy, they rely on extensively annotated datasets, suffering from data scarcity in digital pathology. Unsupervised anomaly detection, however, offers a viable alternative by identifying deviations from normal tissue distributions without requiring exhaustive annotations. Recently, denoising diffusion probabilistic models have gained popularity in unsupervised anomaly detection, achieving promising performance in both natural and medical imaging datasets. Building on this, we incorporate a vision-language model with a diffusion model for unsupervised anomaly detection in digital pathology, utilizing histopathology prompts during reconstruction. Our approach employs a set of pathology-related keywords associated with normal tissues to guide the reconstruction process, facilitating the differentiation between normal and abnormal tissues. To evaluate the effectiveness of the proposed method, we conduct experiments on a gastric lymph node dataset from a local hospital and assess its generalization ability under domain shift using a public breast lymph node dataset. The experimental results highlight the potential of the proposed method for unsupervised anomaly detection across various organs in digital pathology. Code: <a target="_blank" rel="noopener" href="https://github.com/QuIIL/AnoPILaD">https://github.com/QuIIL/AnoPILaD</a>. </p>
<blockquote>
<p>å¼‚å¸¸æ£€æµ‹æ˜¯æ•°å­—ç—…ç†ä¸­ä¸€ä¸ªæ–°å…´çš„æ–¹æ³•ï¼Œå› ä¸ºå®ƒèƒ½å¤Ÿé«˜æ•ˆä¸”æœ‰æ•ˆåœ°åˆ©ç”¨æ•°æ®è¿›è¡Œç–¾ç—…è¯Šæ–­ã€‚è™½ç„¶ç›‘ç£å­¦ä¹ æ–¹æ³•å¯ä»¥æä¾›è¾ƒé«˜çš„å‡†ç¡®æ€§ï¼Œä½†å®ƒä»¬ä¾èµ–äºå¤§é‡æ ‡æ³¨çš„æ•°æ®é›†ï¼Œè€Œåœ¨æ•°å­—ç—…ç†ä¸­å¸¸å¸¸é¢ä¸´æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ã€‚ç„¶è€Œï¼Œæ— ç›‘ç£çš„å¼‚å¸¸æ£€æµ‹æä¾›äº†ä¸€ä¸ªå¯è¡Œçš„æ›¿ä»£æ–¹æ¡ˆï¼Œå®ƒå¯ä»¥é€šè¿‡è¯†åˆ«æ­£å¸¸ç»„ç»‡åˆ†å¸ƒä¸­çš„åå·®æ¥è¿›è¡Œå¼‚å¸¸æ£€æµ‹ï¼Œæ— éœ€è¯¦å°½çš„æ ‡æ³¨ã€‚æœ€è¿‘ï¼Œå»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹åœ¨æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹ä¸­è·å¾—äº†æ™®åŠï¼Œåœ¨è‡ªç„¶å’ŒåŒ»å­¦æˆåƒæ•°æ®é›†ä¸Šå–å¾—äº†æœ‰å¸Œæœ›çš„æ€§èƒ½ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å°†ä¸€ä¸ªè§†è§‰è¯­è¨€æ¨¡å‹ä¸æ‰©æ•£æ¨¡å‹ç›¸ç»“åˆï¼Œç”¨äºæ•°å­—ç—…ç†ä¸­çš„æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹ï¼Œå¹¶åœ¨é‡å»ºè¿‡ç¨‹ä¸­åˆ©ç”¨ç—…ç†æç¤ºã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨ä¸æ­£å¸¸ç»„ç»‡ç›¸å…³çš„ä¸€ç»„ç—…ç†å­¦å…³é”®è¯æ¥å¼•å¯¼é‡å»ºè¿‡ç¨‹ï¼Œä»è€Œä¾¿äºåŒºåˆ†æ­£å¸¸ç»„ç»‡å’Œå¼‚å¸¸ç»„ç»‡ã€‚ä¸ºäº†è¯„ä¼°æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬åœ¨å½“åœ°åŒ»é™¢çš„èƒƒæ·‹å·´ç»“æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Œå¹¶ä½¿ç”¨å…¬å…±çš„ä¹³è…ºæ·‹å·´ç»“æ•°æ®é›†æ¥è¯„ä¼°å…¶åœ¨é¢†åŸŸè¿ç§»ä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœçªå‡ºäº†æ‰€æå‡ºæ–¹æ³•åœ¨æ•°å­—ç—…ç†ä¸­ç”¨äºå„ç§å™¨å®˜çš„æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹çš„æ½œåŠ›ã€‚ä»£ç åœ°å€ï¼š<a target="_blank" rel="noopener" href="https://github.com/QuIIL/AnoPILaD">https://github.com/QuIIL/AnoPILaD</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15236v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æ•°å­—ç—…ç†ä¸­çš„å¼‚å¸¸æ£€æµ‹æ–°å…´æ–¹æ³•èƒ½æœ‰æ•ˆåˆ©ç”¨æ•°æ®è¿›è¡Œç–¾ç—…è¯Šæ–­ã€‚ç›‘ç£å­¦ä¹ æ–¹æ³•è™½å‡†ç¡®ä½†ä¾èµ–å¤§é‡æ ‡æ³¨æ•°æ®ï¼Œåœ¨æ•°å­—ç—…ç†ä¸­é¢ä¸´æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹é€šè¿‡è¯†åˆ«æ­£å¸¸ç»„ç»‡åˆ†å¸ƒçš„åå·®ï¼Œæ— éœ€è¯¦å°½æ ‡æ³¨ï¼Œæä¾›äº†å¯è¡Œæ›¿ä»£æ–¹æ¡ˆã€‚è¿‘æœŸï¼Œé™å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹åœ¨æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹ä¸­å—åˆ°å…³æ³¨ï¼Œåœ¨è‡ªç„¶å’ŒåŒ»å­¦æˆåƒæ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰¯å¥½æ€§èƒ½ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬ç»“åˆè§†è§‰è¯­è¨€æ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹è¿›è¡Œæ•°å­—ç—…ç†çš„æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹ï¼Œé‡å»ºè¿‡ç¨‹ä¸­åˆ©ç”¨ç»„ç»‡ç—…ç†å­¦æç¤ºã€‚é€šè¿‡ä¸€ç³»åˆ—ä¸æ­£å¸¸ç»„ç»‡ç›¸å…³çš„ç—…ç†å­¦å…³é”®è¯å¼•å¯¼é‡å»ºè¿‡ç¨‹ï¼Œæœ‰åŠ©äºåŒºåˆ†æ­£å¸¸ç»„ç»‡ä¸å¼‚å¸¸ç»„ç»‡ã€‚å¯¹æœ¬åœ°åŒ»é™¢çš„èƒƒæ·‹å·´èŠ‚ç‚¹æ•°æ®é›†è¿›è¡Œè¯•éªŒå¹¶å¯¹å…¬å¼€ä¹³è…ºæ·‹å·´èŠ‚ç‚¹æ•°æ®é›†è¿›è¡Œè·¨åŸŸè¯„ä¼°ï¼Œå‡¸æ˜¾äº†è¯¥æ–¹æ³•çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼‚å¸¸æ£€æµ‹æ˜¯æ•°å­—ç—…ç†å­¦ä¸­çš„æ–°å…´æ–¹æ³•ï¼Œç”¨äºé«˜æ•ˆä¸”æœ‰æ•ˆåœ°åˆ©ç”¨æ•°æ®è¿›è¡Œç–¾ç—…è¯Šæ–­ã€‚</li>
<li>ç›‘ç£å­¦ä¹ æ–¹æ³•åœ¨æ•°å­—ç—…ç†ä¸­é¢ä¸´æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚</li>
<li>æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹é€šè¿‡è¯†åˆ«æ­£å¸¸ç»„ç»‡åˆ†å¸ƒçš„åå·®ï¼Œä¸ºæ•°å­—ç—…ç†æä¾›äº†å¯è¡Œæ›¿ä»£æ–¹æ¡ˆã€‚</li>
<li>é™å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹åœ¨æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹ä¸­å—åˆ°å…³æ³¨ï¼Œè¡¨ç°è‰¯å¥½ã€‚</li>
<li>ç»“åˆè§†è§‰è¯­è¨€æ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹è¿›è¡Œæ•°å­—ç—…ç†çš„æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹ï¼Œåˆ©ç”¨ç»„ç»‡ç—…ç†å­¦æç¤ºæå‡æ£€æµ‹å‡†ç¡®æ€§ã€‚</li>
<li>é€šè¿‡å…³é”®è¯å¼•å¯¼é‡å»ºè¿‡ç¨‹ï¼Œæœ‰åŠ©äºåŒºåˆ†æ­£å¸¸ç»„ç»‡ä¸å¼‚å¸¸ç»„ç»‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15236">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5a412df1e2323af076ad13c45fe93b60.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a8c69cf25a20c78e9a65f7a27ef5376.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f83a65fabd8f03cfe77ff5ec5821b0f0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d5f246d12e6545d2c5404912c413923c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8f31c192d6418fc22224b3e72c40362c.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="See-it-Say-it-Sorted-Agentic-System-for-Compositional-Diagram-Generation"><a href="#See-it-Say-it-Sorted-Agentic-System-for-Compositional-Diagram-Generation" class="headerlink" title="See it. Say it. Sorted: Agentic System for Compositional Diagram   Generation"></a>See it. Say it. Sorted: Agentic System for Compositional Diagram   Generation</h2><p><strong>Authors:Hantao Zhang, Jingyang Liu, Ed Li</strong></p>
<p>We study sketch-to-diagram generation: converting rough hand sketches into precise, compositional diagrams. Diffusion models excel at photorealism but struggle with the spatial precision, alignment, and symbolic structure required for flowcharts. We introduce See it. Say it. Sorted., a training-free agentic system that couples a Vision-Language Model (VLM) with Large Language Models (LLMs) to produce editable Scalable Vector Graphics (SVG) programs. The system runs an iterative loop in which a Critic VLM proposes a small set of qualitative, relational edits; multiple candidate LLMs synthesize SVG updates with diverse strategies (conservative-&gt;aggressive, alternative, focused); and a Judge VLM selects the best candidate, ensuring stable improvement. This design prioritizes qualitative reasoning over brittle numerical estimates, preserves global constraints (e.g., alignment, connectivity), and naturally supports human-in-the-loop corrections. On 10 sketches derived from flowcharts in published papers, our method more faithfully reconstructs layout and structure than two frontier closed-source image generation LLMs (GPT-5 and Gemini-2.5-Pro), accurately composing primitives (e.g., multi-headed arrows) without inserting unwanted text. Because outputs are programmatic SVGs, the approach is readily extensible to presentation tools (e.g., PowerPoint) via APIs and can be specialized with improved prompts and task-specific tools. The codebase is open-sourced at <a target="_blank" rel="noopener" href="https://github.com/hantaoZhangrichard/see_it_say_it_sorted.git">https://github.com/hantaoZhangrichard/see_it_say_it_sorted.git</a>. </p>
<blockquote>
<p>æˆ‘ä»¬ç ”ç©¶äº†è‰å›¾åˆ°å›¾è¡¨ç”Ÿæˆçš„ä»»åŠ¡ï¼Œå³å°†ç²—ç•¥çš„æ‰‹ç»˜è‰å›¾è½¬åŒ–ä¸ºç²¾ç¡®ã€ç»„åˆå¼çš„å›¾è¡¨ã€‚æ‰©æ•£æ¨¡å‹åœ¨ç…§ç‰‡å†™å®æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æµç¨‹å›¾æ‰€éœ€çš„ç©ºé—´ç²¾åº¦ã€å¯¹é½å’Œç¬¦å·ç»“æ„æ–¹é¢é‡åˆ°äº†å›°éš¾ã€‚æˆ‘ä»¬æ¨å‡ºäº†â€œæ‰€è§å³æ‰€è¯´.å·²æ’åºâ€ï¼Œè¿™æ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„ä¸»ä½“ç³»ç»Ÿï¼Œå®ƒå°†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç›¸ç»“åˆï¼Œç”Ÿæˆå¯ç¼–è¾‘çš„çŸ¢é‡å›¾å½¢ï¼ˆSVGï¼‰ç¨‹åºã€‚è¯¥ç³»ç»Ÿè¿è¡Œä¸€ä¸ªè¿­ä»£å¾ªç¯ï¼Œå…¶ä¸­æ‰¹è¯„å®¶VLMæå‡ºä¸€å°éƒ¨åˆ†å®šæ€§å…³ç³»ç¼–è¾‘ï¼›å¤šä¸ªå€™é€‰LLMä½¿ç”¨ä¸åŒç­–ç•¥ï¼ˆä¿å®ˆåˆ°æ¿€è¿›ã€æ›¿ä»£ã€ä¸“æ³¨ï¼‰åˆæˆSVGæ›´æ–°ï¼›æ³•å®˜VLMé€‰æ‹©æœ€ä½³å€™é€‰è€…ï¼Œç¡®ä¿ç¨³å®šæ”¹è¿›ã€‚è¿™ç§è®¾è®¡ä¼˜å…ˆè€ƒè™‘å®šæ€§æ¨ç†è€Œéè„†å¼±çš„æ•°å€¼ä¼°è®¡ï¼Œä¿ç•™å…¨å±€çº¦æŸï¼ˆä¾‹å¦‚å¯¹é½ã€è¿æ¥ï¼‰ï¼Œå¹¶å¤©ç„¶æ”¯æŒäººç±»å‚ä¸å¾ªç¯æ ¡æ­£ã€‚åœ¨æ¥è‡ªå·²å‘è¡¨è®ºæ–‡çš„10ä¸ªæµç¨‹å›¾è‰å›¾ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ¯”ä¸¤ä¸ªå‰æ²¿çš„é—­æºå›¾åƒç”ŸæˆLLMï¼ˆGPT-5å’ŒGemini-2.5-Proï¼‰æ›´å¿ å®åœ°é‡å»ºäº†å¸ƒå±€å’Œç»“æ„ï¼Œèƒ½å¤Ÿå‡†ç¡®åœ°ç»„åˆåŸå§‹å…ƒç´ ï¼ˆä¾‹å¦‚å¤šå¤´ç®­å¤´ï¼‰ï¼Œè€Œä¸ä¼šæ’å…¥ä¸éœ€è¦çš„æ–‡æœ¬ã€‚ç”±äºè¾“å‡ºæ˜¯ç¨‹åºåŒ–çš„SVGï¼Œå› æ­¤è¯¥æ–¹æ³•å¯ä»¥é€šè¿‡APIè½»æ¾æ‰©å±•åˆ°æ¼”ç¤ºå·¥å…·ï¼ˆä¾‹å¦‚PowerPointï¼‰ï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡æ”¹è¿›æç¤ºå’Œä»»åŠ¡ç‰¹å®šå·¥å…·è¿›è¡Œä¸“é—¨åŒ–ã€‚è¯¥ä»£ç åº“å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/hantaoZhangrichard/see_it_say_it_sorted.git%E5%BC%80%E6%BA%90%E3%80%82">https://github.com/hantaoZhangrichard/see_it_say_it_sorted.gitå¼€æºã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15222v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†è‰å›¾åˆ°å›¾è¡¨çš„ç”Ÿæˆé—®é¢˜ï¼Œå³å°†ç²—ç³™çš„æ‰‹ç»˜è‰å›¾è½¬åŒ–ä¸ºç²¾ç¡®çš„ç»„æˆå›¾è¡¨ã€‚ç ”ç©¶å›¢é˜Ÿå¼•å…¥äº†ä¸€ä¸ªåä¸ºâ€œSee it. Say it. Sorted.â€çš„æ— è®­ç»ƒä»£ç†ç³»ç»Ÿï¼Œç»“åˆäº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œä»¥ç”Ÿæˆå¯ç¼–è¾‘çš„çŸ¢é‡å›¾å½¢ï¼ˆSVGï¼‰ç¨‹åºã€‚è¯¥ç³»ç»Ÿé€šè¿‡è¿­ä»£è¿‡ç¨‹ï¼Œç”±æ‰¹åˆ¤æ€§VLMæå‡ºå®šæ€§å…³ç³»ç¼–è¾‘å»ºè®®ï¼Œå¤šä¸ªå€™é€‰LLMé‡‡ç”¨ä¸åŒç­–ç•¥åˆæˆSVGæ›´æ–°ï¼Œå¹¶ç”±åˆ¤æ–­æ€§VLMé€‰æ‹©æœ€ä½³å€™é€‰ï¼Œç¡®ä¿æŒç»­æ”¹è¿›ã€‚è¯¥æ–¹æ³•é‡è§†å®šæ€§æ¨ç†ï¼Œä¿ç•™å…¨å±€çº¦æŸï¼Œå¦‚å¯¹é½å’Œè¿æ¥æ€§ï¼Œå¹¶æ”¯æŒäººå·¥ä»‹å…¥ä¿®æ­£ã€‚åœ¨åŸºäºè®ºæ–‡ä¸­10ä¸ªè‰æ‹Ÿçš„æµç¨‹å›¾æµ‹è¯•ä¸Šï¼Œè¯¥æ–¹æ³•èƒ½æ›´å¿ å®åœ°é‡å»ºå¸ƒå±€å’Œç»“æ„ï¼Œç›¸è¾ƒäºä¸¤å¤§å‰æ²¿å›¾åƒç”ŸæˆLLMï¼ˆGPT-5å’ŒGemini-2.5-Proï¼‰æ›´å…·ä¼˜åŠ¿ï¼Œèƒ½å‡†ç¡®ç»„åˆåŸºæœ¬å…ƒç´ è€Œä¸æ·»åŠ æ— å…³æ–‡å­—ã€‚å…¶è¾“å‡ºä¸ºç¨‹åºåŒ–SVGï¼Œæ˜“äºæ‰©å±•åˆ°æ¼”ç¤ºå·¥å…·å¦‚PowerPointç­‰ã€‚ç›¸å…³ä»£ç å·²å¼€æºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶å›¢é˜Ÿæå‡ºäº†ä¸€ç§å°†è‰å›¾è½¬åŒ–ä¸ºç²¾ç¡®å›¾è¡¨çš„æ–¹æ³•ï¼Œä½¿ç”¨â€œSee it. Say it. Sorted.â€ç³»ç»Ÿç»“åˆVLMå’ŒLLMæŠ€æœ¯ã€‚</li>
<li>è¯¥ç³»ç»Ÿé€šè¿‡è¿­ä»£è¿‡ç¨‹ï¼Œèƒ½å¤Ÿæå‡ºç¼–è¾‘å»ºè®®ã€åˆæˆSVGæ›´æ–°å¹¶é€‰æ‹©æœ€ä½³å€™é€‰ï¼Œä»è€ŒæŒç»­æé«˜å›¾è¡¨è´¨é‡ã€‚</li>
<li>ç³»ç»Ÿé‡è§†å®šæ€§æ¨ç†ï¼Œèƒ½å¤Ÿä¿ç•™å…¨å±€çº¦æŸï¼Œå¦‚å¯¹é½å’Œè¿æ¥æ€§ï¼Œå¹¶æ”¯æŒäººå·¥ä¿®æ­£ã€‚</li>
<li>ä¸å…¶ä»–å‰æ²¿LLMç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨é‡å»ºå¸ƒå±€å’Œç»“æ„æ–¹é¢è¡¨ç°æ›´ä¼˜ç§€ï¼Œèƒ½å‡†ç¡®ç»„åˆåŸºæœ¬å…ƒç´ ã€‚</li>
<li>è¾“å‡ºä¸ºç¨‹åºåŒ–SVGæ ¼å¼ï¼Œæ˜“äºé›†æˆåˆ°å„ç§æ¼”ç¤ºå·¥å…·ä¸­ï¼Œå¦‚PowerPointã€‚</li>
<li>è¯¥æ–¹æ³•å…·æœ‰å¯æ‰©å±•æ€§ï¼Œå¯é€šè¿‡æ”¹è¿›æç¤ºå’Œä»»åŠ¡ç‰¹å®šå·¥å…·è¿›è¡Œä¸“ä¸šåŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15222">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f74dfde820fa8f822759ea1919ac78b6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0371cdf5f20cd938fa3a4c512c43617e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-22615bd766eaf130be902e26f5ac964f.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MeSS-City-Mesh-Guided-Outdoor-Scene-Generation-with-Cross-View-Consistent-Diffusion"><a href="#MeSS-City-Mesh-Guided-Outdoor-Scene-Generation-with-Cross-View-Consistent-Diffusion" class="headerlink" title="MeSS: City Mesh-Guided Outdoor Scene Generation with Cross-View   Consistent Diffusion"></a>MeSS: City Mesh-Guided Outdoor Scene Generation with Cross-View   Consistent Diffusion</h2><p><strong>Authors:Xuyang Chen, Zhijun Zhai, Kaixuan Zhou, Zengmao Wang, Jianan He, Dong Wang, Yanfeng Zhang, mingwei Sun, RÃ¼diger Westermann, Konrad Schindler, Liqiu Meng</strong></p>
<p>Mesh models have become increasingly accessible for numerous cities; however, the lack of realistic textures restricts their application in virtual urban navigation and autonomous driving. To address this, this paper proposes MeSS (Meshbased Scene Synthesis) for generating high-quality, styleconsistent outdoor scenes with city mesh models serving as the geometric prior. While image and video diffusion models can leverage spatial layouts (such as depth maps or HD maps) as control conditions to generate street-level perspective views, they are not directly applicable to 3D scene generation. Video diffusion models excel at synthesizing consistent view sequences that depict scenes but often struggle to adhere to predefined camera paths or align accurately with rendered control videos. In contrast, image diffusion models, though unable to guarantee cross-view visual consistency, can produce more geometry-aligned results when combined with ControlNet. Building on this insight, our approach enhances image diffusion models by improving cross-view consistency. The pipeline comprises three key stages: first, we generate geometrically consistent sparse views using Cascaded Outpainting ControlNets; second, we propagate denser intermediate views via a component dubbed AGInpaint; and third, we globally eliminate visual inconsistencies (e.g., varying exposure) using the GCAlign module. Concurrently with generation, a 3D Gaussian Splatting (3DGS) scene is reconstructed by initializing Gaussian balls on the mesh surface. Our method outperforms existing approaches in both geometric alignment and generation quality. Once synthesized, the scene can be rendered in diverse styles through relighting and style transfer techniques. </p>
<blockquote>
<p>ç½‘æ ¼æ¨¡å‹å·²ç»ä¸ºä¼—å¤šåŸå¸‚æä¾›äº†è¶Šæ¥è¶Šä¾¿æ·çš„è®¿é—®æ–¹å¼ï¼Œç„¶è€Œï¼Œç”±äºç¼ºä¹é€¼çœŸçš„çº¹ç†ï¼Œå…¶åœ¨è™šæ‹ŸåŸå¸‚å¯¼èˆªå’Œè‡ªåŠ¨é©¾é©¶æ–¹é¢çš„åº”ç”¨å—åˆ°é™åˆ¶ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†MeSSï¼ˆåŸºäºç½‘æ ¼çš„åœºæ™¯åˆæˆï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä»¥åŸå¸‚ç½‘æ ¼æ¨¡å‹ä½œä¸ºå‡ ä½•å…ˆéªŒï¼Œç”Ÿæˆé«˜è´¨é‡ã€é£æ ¼ä¸€è‡´çš„å¤–æ™¯ã€‚å›¾åƒå’Œè§†é¢‘æ‰©æ•£æ¨¡å‹å¯ä»¥åˆ©ç”¨ç©ºé—´å¸ƒå±€ï¼ˆå¦‚æ·±åº¦å›¾æˆ–é«˜æ¸…åœ°å›¾ï¼‰ä½œä¸ºæ§åˆ¶æ¡ä»¶æ¥ç”Ÿæˆè¡—é“çº§é€è§†è§†å›¾ï¼Œä½†å®ƒä»¬å¹¶ä¸ç›´æ¥é€‚ç”¨äº3Dåœºæ™¯ç”Ÿæˆã€‚è§†é¢‘æ‰©æ•£æ¨¡å‹æ“…é•¿åˆæˆä¸€è‡´çš„åœºæ™¯è§†å›¾åºåˆ—ï¼Œä½†å¾€å¾€éš¾ä»¥éµå¾ªé¢„å®šçš„ç›¸æœºè·¯å¾„æˆ–ä¸æ¸²æŸ“çš„æ§åˆ¶è§†é¢‘å‡†ç¡®å¯¹é½ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå›¾åƒæ‰©æ•£æ¨¡å‹è™½ç„¶æ— æ³•ä¿è¯è·¨è§†å›¾çš„è§†è§‰ä¸€è‡´æ€§ï¼Œä½†å½“ä¸ControlNetç»“åˆæ—¶ï¼Œå¯ä»¥äº§ç”Ÿæ›´è´´åˆå‡ ä½•çš„ç»“æœã€‚åŸºäºæ­¤è§è§£ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡æ”¹è¿›å›¾åƒæ‰©æ•£æ¨¡å‹çš„è·¨è§†å›¾ä¸€è‡´æ€§æ¥å¢å¼ºå…¶æ€§èƒ½ã€‚è¯¥ç®¡é“åŒ…æ‹¬ä¸‰ä¸ªé˜¶æ®µï¼šé¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨çº§è”å¤–ç”»ControlNetsç”Ÿæˆå‡ ä½•ä¸€è‡´çš„ç¨€ç–è§†å›¾ï¼›å…¶æ¬¡ï¼Œæˆ‘ä»¬é€šè¿‡åä¸ºAGInpaintçš„ç»„ä»¶ä¼ æ’­æ›´å¯†é›†çš„ä¸­é—´è§†å›¾ï¼›æœ€åï¼Œæˆ‘ä»¬ä½¿ç”¨GCAlignæ¨¡å—å…¨å±€æ¶ˆé™¤è§†è§‰ä¸ä¸€è‡´ï¼ˆä¾‹å¦‚ï¼Œæ›å…‰ä¸åŒï¼‰ã€‚åœ¨ç”Ÿæˆçš„åŒæ—¶ï¼Œé€šè¿‡åœ¨ç½‘æ ¼è¡¨é¢ä¸Šåˆå§‹åŒ–é«˜æ–¯çƒæ¥é‡å»º3Dé«˜æ–¯å–·ç»˜ï¼ˆ3DGSï¼‰åœºæ™¯ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å‡ ä½•å¯¹é½å’Œç”Ÿæˆè´¨é‡æ–¹é¢éƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚åœºæ™¯åˆæˆåï¼Œå¯ä»¥é€šè¿‡é‡æ–°ç…§æ˜å’Œé£æ ¼è½¬æ¢æŠ€æœ¯ä»¥å¤šç§é£æ ¼è¿›è¡Œæ¸²æŸ“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15169v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºMeSSçš„æ–¹æ³•ï¼Œç”¨äºç”Ÿæˆé«˜è´¨é‡ã€é£æ ¼ä¸€è‡´çš„æˆ·å¤–åœºæ™¯ã€‚è¯¥æ–¹æ³•ä»¥åŸå¸‚ç½‘æ ¼æ¨¡å‹ä½œä¸ºå‡ ä½•å…ˆéªŒï¼Œç»“åˆå›¾åƒæ‰©æ•£æ¨¡å‹å’Œæ§åˆ¶ç½‘ç»œï¼ˆControlNetï¼‰ï¼Œç”Ÿæˆå‡ ä½•å¯¹é½çš„è§†å›¾ã€‚é€šè¿‡ä¸‰ä¸ªé˜¶æ®µå¤„ç†ï¼ŒåŒ…æ‹¬ç”Ÿæˆå‡ ä½•ä¸€è‡´çš„ç¨€ç–è§†å›¾ã€ä¼ æ’­ä¸­é—´å¯†é›†è§†å›¾ä»¥åŠæ¶ˆé™¤å…¨å±€è§†è§‰ä¸ä¸€è‡´æ€§ã€‚åŒæ—¶ï¼Œé€šè¿‡ä¸‰ç»´é«˜æ–¯å–·ç»˜ï¼ˆ3DGSï¼‰æŠ€æœ¯é‡å»ºåœºæ™¯ï¼Œå®ç°å¤šç§é£æ ¼çš„æ¸²æŸ“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŸå¸‚ç½‘æ ¼æ¨¡å‹ä½œä¸ºå‡ ä½•å…ˆéªŒï¼Œä¸ºè™šæ‹ŸåŸå¸‚å¯¼èˆªå’Œè‡ªåŠ¨é©¾é©¶åº”ç”¨æä¾›äº†åŸºç¡€ã€‚</li>
<li>å›¾åƒæ‰©æ•£æ¨¡å‹ä¸æ§åˆ¶ç½‘ç»œï¼ˆControlNetï¼‰ç»“åˆï¼Œæé«˜äº†è·¨è§†å›¾çš„ä¸€è‡´æ€§ã€‚</li>
<li>MeSSæ–¹æ³•åŒ…æ‹¬ä¸‰ä¸ªé˜¶æ®µï¼šç”Ÿæˆå‡ ä½•ä¸€è‡´çš„ç¨€ç–è§†å›¾ã€ä¼ æ’­ä¸­é—´å¯†é›†è§†å›¾ã€æ¶ˆé™¤è§†è§‰ä¸ä¸€è‡´æ€§ã€‚</li>
<li>3DGSæŠ€æœ¯ç”¨äºé‡å»ºåœºæ™¯ï¼Œå®ç°å¤šç§é£æ ¼çš„æ¸²æŸ“ã€‚</li>
<li>MeSSæ–¹æ³•åœ¨å‡ ä½•å¯¹é½å’Œç”Ÿæˆè´¨é‡æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>åˆæˆåœºæ™¯å¯ä»¥é€šè¿‡ç…§æ˜å’Œé£æ ¼è½¬æ¢æŠ€æœ¯è¿›è¡Œä¸åŒé£æ ¼çš„æ¸²æŸ“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15169">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-926c1a3fb69a681177f24ffa5f41beee.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3a4d8faceee8b7e2bb8f6f4662186092.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c09b33b9d1a412c50df633512c99ba56.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7b856fbc2ca3d4cb8d9db9dee9e1fb4e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-12b798ac6e3954efb7f2a1c8ae65d37a.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="TAIGen-Training-Free-Adversarial-Image-Generation-via-Diffusion-Models"><a href="#TAIGen-Training-Free-Adversarial-Image-Generation-via-Diffusion-Models" class="headerlink" title="TAIGen: Training-Free Adversarial Image Generation via Diffusion Models"></a>TAIGen: Training-Free Adversarial Image Generation via Diffusion Models</h2><p><strong>Authors:Susim Roy, Anubhooti Jain, Mayank Vatsa, Richa Singh</strong></p>
<p>Adversarial attacks from generative models often produce low-quality images and require substantial computational resources. Diffusion models, though capable of high-quality generation, typically need hundreds of sampling steps for adversarial generation. This paper introduces TAIGen, a training-free black-box method for efficient adversarial image generation. TAIGen produces adversarial examples using only 3-20 sampling steps from unconditional diffusion models. Our key finding is that perturbations injected during the mixing step interval achieve comparable attack effectiveness without processing all timesteps. We develop a selective RGB channel strategy that applies attention maps to the red channel while using GradCAM-guided perturbations on green and blue channels. This design preserves image structure while maximizing misclassification in target models. TAIGen maintains visual quality with PSNR above 30 dB across all tested datasets. On ImageNet with VGGNet as source, TAIGen achieves 70.6% success against ResNet, 80.8% against MNASNet, and 97.8% against ShuffleNet. The method generates adversarial examples 10x faster than existing diffusion-based attacks. Our method achieves the lowest robust accuracy, indicating it is the most impactful attack as the defense mechanism is least successful in purifying the images generated by TAIGen. </p>
<blockquote>
<p>å¯¹æŠ—æ€§æ”»å‡»ç”Ÿæˆæ¨¡å‹é€šå¸¸ä¼šäº§ç”Ÿä½è´¨é‡çš„å›¾åƒï¼Œå¹¶éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºã€‚å°½ç®¡æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿè¿›è¡Œé«˜è´¨é‡ç”Ÿæˆï¼Œä½†é€šå¸¸éœ€è¦è¿›è¡Œæ•°ç™¾æ­¥é‡‡æ ·æ‰èƒ½è¿›è¡Œå¯¹æŠ—æ€§ç”Ÿæˆã€‚æœ¬æ–‡ä»‹ç»äº†TAIGenï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„é»‘ç›’æ–¹æ³•ï¼Œå¯é«˜æ•ˆç”Ÿæˆå¯¹æŠ—æ€§å›¾åƒã€‚TAIGenä»…ä½¿ç”¨æ— æ¡ä»¶æ‰©æ•£æ¨¡å‹çš„3-20æ­¥é‡‡æ ·å³å¯äº§ç”Ÿå¯¹æŠ—æ€§å®ä¾‹ã€‚æˆ‘ä»¬çš„å…³é”®å‘ç°æ˜¯åœ¨æ··åˆæ­¥éª¤é—´éš”ä¸­æ³¨å…¥æ‰°åŠ¨ï¼Œåœ¨ä¸å¤„ç†æ‰€æœ‰æ—¶é—´æ­¥é•¿çš„æƒ…å†µä¸‹å®ç°äº†ç›¸å½“çš„æ”»å‡»æ•ˆæœã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ç§é€‰æ‹©æ€§RGBé€šé“ç­–ç•¥ï¼Œå¯¹çº¢è‰²é€šé“åº”ç”¨æ³¨æ„åŠ›å›¾ï¼ŒåŒæ—¶å¯¹ç»¿è‰²å’Œè“è‰²é€šé“ä½¿ç”¨GradCAMå¼•å¯¼çš„æ‰°åŠ¨ã€‚è¿™ç§è®¾è®¡ä¿ç•™äº†å›¾åƒç»“æ„ï¼ŒåŒæ—¶æœ€å¤§é™åº¦åœ°æé«˜äº†ç›®æ ‡æ¨¡å‹çš„è¯¯åˆ†ç±»ç‡ã€‚TAIGenåœ¨æ‰€æœ‰æµ‹è¯•æ•°æ®é›†ä¸Šä¿æŒPSNRé«˜äº30 dBçš„è§†è§‰è´¨é‡ã€‚åœ¨ImageNetä¸Šä½¿ç”¨VGGNetä½œä¸ºæºæ—¶ï¼ŒTAIGenå¯¹ResNetçš„æˆåŠŸç‡ä¸º70.6%ï¼Œå¯¹MNASNetçš„æˆåŠŸç‡ä¸º80.8%ï¼Œå¯¹ShuffleNetçš„æˆåŠŸç‡é«˜è¾¾97.8%ã€‚è¯¥æ–¹æ³•ç”Ÿæˆçš„å¯¹æŠ—æ€§å®ä¾‹çš„é€Ÿåº¦æ¯”ç°æœ‰çš„åŸºäºæ‰©æ•£çš„æ”»å‡»å¿«10å€ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æœ€ä½çš„ç¨³å¥å‡†ç¡®ç‡ï¼Œè¿™è¡¨æ˜å®ƒæ˜¯æœ€å…·å½±å“åŠ›çš„æ”»å‡»ï¼Œå› ä¸ºé˜²å¾¡æœºåˆ¶åœ¨å‡€åŒ–TAIGenç”Ÿæˆçš„å›¾åƒæ—¶æ•ˆæœæœ€ä¸æ˜æ˜¾ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15020v1">PDF</a> Accepted at ICCVW-CV4BIOM 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºTAIGençš„è®­ç»ƒæ— å…³çš„é»‘ç›’æ–¹æ³•ï¼Œç”¨äºé«˜æ•ˆç”Ÿæˆå¯¹æŠ—æ€§å›¾åƒã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ— æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œä»…é€šè¿‡3-20ä¸ªé‡‡æ ·æ­¥éª¤å³å¯ç”Ÿæˆå¯¹æŠ—æ ·æœ¬ã€‚ç ”ç©¶å‘ç°åœ¨æ··åˆæ­¥éª¤é—´éš”æœŸé—´æ³¨å…¥æ‰°åŠ¨ï¼Œå¯åœ¨ä¸å¤„ç†æ‰€æœ‰æ—¶é—´æ­¥çš„æƒ…å†µä¸‹å®ç°ç›¸å½“çš„æ”»å‡»æ•ˆæœã€‚è¯¥æ–¹æ³•é‡‡ç”¨é€‰æ‹©æ€§RGBé€šé“ç­–ç•¥ï¼Œå¯¹çº¢é€šé“åº”ç”¨æ³¨æ„åŠ›æ˜ å°„ï¼ŒåŒæ—¶å¯¹ç»¿è“é€šé“é‡‡ç”¨GradCAMå¼•å¯¼çš„æ‰°åŠ¨ã€‚æ­¤æ–¹æ³•åœ¨ä¿æŒå›¾åƒç»“æ„çš„åŒæ—¶ï¼Œæœ€å¤§åŒ–ç›®æ ‡æ¨¡å‹çš„è¯¯åˆ†ç±»ã€‚TAIGenåœ¨ä¿æŒè§†è§‰è´¨é‡çš„åŒæ—¶ï¼Œç”Ÿæˆå¯¹æŠ—æ ·æœ¬çš„é€Ÿåº¦æ˜¯ç°æœ‰æ‰©æ•£æ”»å‡»çš„10å€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TAIGenæ˜¯ä¸€ç§è®­ç»ƒæ— å…³çš„é»‘ç›’æ–¹æ³•ï¼Œç”¨äºé«˜æ•ˆç”Ÿæˆå¯¹æŠ—æ€§å›¾åƒã€‚</li>
<li>è¯¥æ–¹æ³•åˆ©ç”¨æ— æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡è¾ƒå°‘çš„é‡‡æ ·æ­¥éª¤ï¼ˆ3-20æ­¥ï¼‰ç”Ÿæˆå¯¹æŠ—æ ·æœ¬ã€‚</li>
<li>ç ”ç©¶å‘ç°ï¼Œåœ¨æ··åˆæ­¥éª¤é—´éš”æœŸé—´æ³¨å…¥æ‰°åŠ¨å¯è¾¾æˆæœ‰æ•ˆçš„æ”»å‡»ã€‚</li>
<li>é‡‡ç”¨é€‰æ‹©æ€§RGBé€šé“ç­–ç•¥ï¼Œå¯¹çº¢ã€ç»¿ã€è“é€šé“åˆ†åˆ«å¤„ç†ï¼Œä»¥æé«˜æ”»å‡»æ•ˆæœå¹¶ä¿æŒå›¾åƒè´¨é‡ã€‚</li>
<li>TAIGenåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„PSNRå€¼å‡è¶…è¿‡30dBï¼Œä¿æŒè¾ƒé«˜çš„è§†è§‰è´¨é‡ã€‚</li>
<li>åœ¨ImageNetæ•°æ®é›†ä¸Šï¼ŒTAIGenå¯¹ResNetã€MNASNetå’ŒShuffleNetçš„æˆåŠŸç‡åˆ†åˆ«ä¸º70.6%ã€80.8%å’Œ97.8%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15020">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f4aab2b078a24982a3ca5fb6bb11e06a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93d339abe9d4f08f4b18b3803189f7c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-806ebcccd6112f3926cfca5c49735ecc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-18cd6406edd57cdd7dfa7fa1313904e6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5d1bfee740b4a4df3f7bc822b11abba0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c71e4d02a985f18bd5c313dd4a66d5f9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6a6fa7ccc8cc2531796bd69bed502b2b.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Latent-Interpolation-Learning-Using-Diffusion-Models-for-Cardiac-Volume-Reconstruction"><a href="#Latent-Interpolation-Learning-Using-Diffusion-Models-for-Cardiac-Volume-Reconstruction" class="headerlink" title="Latent Interpolation Learning Using Diffusion Models for Cardiac Volume   Reconstruction"></a>Latent Interpolation Learning Using Diffusion Models for Cardiac Volume   Reconstruction</h2><p><strong>Authors:Niklas Bubeck, Suprosanna Shit, Chen Chen, Can Zhao, Pengfei Guo, Dong Yang, Georg Zitzlsberger, Daguang Xu, Bernhard Kainz, Daniel Rueckert, Jiazhen Pan</strong></p>
<p>Cardiac Magnetic Resonance (CMR) imaging is a critical tool for diagnosing and managing cardiovascular disease, yet its utility is often limited by the sparse acquisition of 2D short-axis slices, resulting in incomplete volumetric information. Accurate 3D reconstruction from these sparse slices is essential for comprehensive cardiac assessment, but existing methods face challenges, including reliance on predefined interpolation schemes (e.g., linear or spherical), computational inefficiency, and dependence on additional semantic inputs such as segmentation labels or motion data. To address these limitations, we propose a novel Cardiac Latent Interpolation Diffusion (CaLID) framework that introduces three key innovations. First, we present a data-driven interpolation scheme based on diffusion models, which can capture complex, non-linear relationships between sparse slices and improves reconstruction accuracy. Second, we design a computationally efficient method that operates in the latent space and speeds up 3D whole-heart upsampling time by a factor of 24, reducing computational overhead compared to previous methods. Third, with only sparse 2D CMR images as input, our method achieves SOTA performance against baseline methods, eliminating the need for auxiliary input such as morphological guidance, thus simplifying workflows. We further extend our method to 2D+T data, enabling the effective modeling of spatiotemporal dynamics and ensuring temporal coherence. Extensive volumetric evaluations and downstream segmentation tasks demonstrate that CaLID achieves superior reconstruction quality and efficiency. By addressing the fundamental limitations of existing approaches, our framework advances the state of the art for spatio and spatiotemporal whole-heart reconstruction, offering a robust and clinically practical solution for cardiovascular imaging. </p>
<blockquote>
<p>å¿ƒè„ç£å…±æŒ¯ï¼ˆCMRï¼‰æˆåƒåœ¨å¿ƒè¡€ç®¡ç–¾ç—…çš„è¯Šæ–­å’Œæ²»ç–—ä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œä½†å…¶æ•ˆç”¨å¾€å¾€å—åˆ°äºŒç»´çŸ­è½´åˆ‡ç‰‡ç¨€ç–é‡‡é›†çš„é™åˆ¶ï¼Œå¯¼è‡´ä½“ç§¯ä¿¡æ¯ä¸å®Œæ•´ã€‚ä»è¿™äº›ç¨€ç–åˆ‡ç‰‡ä¸­è¿›è¡Œå‡†ç¡®çš„3Dé‡å»ºå¯¹äºå…¨é¢çš„å¿ƒè„è¯„ä¼°è‡³å…³é‡è¦ï¼Œä½†ç°æœ‰æ–¹æ³•é¢ä¸´æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ä¾èµ–äºé¢„å®šä¹‰çš„æ’å€¼æ–¹æ¡ˆï¼ˆä¾‹å¦‚çº¿æ€§æˆ–çƒå½¢ï¼‰ã€è®¡ç®—æ•ˆç‡ä½ä¸‹ä»¥åŠå¯¹é¢å¤–çš„è¯­ä¹‰è¾“å…¥ï¼ˆå¦‚åˆ†å‰²æ ‡ç­¾æˆ–è¿åŠ¨æ•°æ®ï¼‰çš„ä¾èµ–ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„å¿ƒè„æ½œåœ¨æ’å€¼æ‰©æ•£ï¼ˆCaLIDï¼‰æ¡†æ¶ï¼Œå¼•å…¥äº†ä¸‰é¡¹å…³é”®åˆ›æ–°ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ•°æ®é©±åŠ¨æ’å€¼æ–¹æ¡ˆï¼Œå¯ä»¥æ•æ‰ç¨€ç–åˆ‡ç‰‡ä¹‹é—´å¤æ‚çš„éçº¿æ€§å…³ç³»ï¼Œæé«˜é‡å»ºç²¾åº¦ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åœ¨æ½œåœ¨ç©ºé—´è¿è¡Œçš„é«˜æ•ˆæ–¹æ³•ï¼Œå°†3Då…¨å¿ƒä¸Šé‡‡æ ·æ—¶é—´åŠ å¿«24å€ï¼Œä¸ä»¥å‰çš„æ–¹æ³•ç›¸æ¯”ï¼Œå‡å°‘äº†è®¡ç®—å¼€é”€ã€‚ç¬¬ä¸‰ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä»…ä½¿ç”¨ç¨€ç–çš„2DCMRå›¾åƒä½œä¸ºè¾“å…¥ï¼Œå°±è¾¾åˆ°äº†ä¸åŸºçº¿æ–¹æ³•ç›¸æ¯”çš„æœ€ä½³æ€§èƒ½ï¼Œæ— éœ€è¾…åŠ©è¾“å…¥ï¼ˆå¦‚å½¢æ€å¼•å¯¼ï¼‰ï¼Œä»è€Œç®€åŒ–äº†å·¥ä½œæµç¨‹ã€‚æˆ‘ä»¬è¿˜å°†æˆ‘ä»¬çš„æ–¹æ³•æ‰©å±•åˆ°2D+Tæ•°æ®ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°å¯¹æ—¶ç©ºåŠ¨æ€è¿›è¡Œå»ºæ¨¡ï¼Œç¡®ä¿æ—¶é—´è¿è´¯æ€§ã€‚å¤§é‡çš„ä½“ç§¯è¯„ä¼°å’Œä¸‹æ¸¸åˆ†å‰²ä»»åŠ¡è¡¨æ˜ï¼ŒCaLIDåœ¨é‡å»ºè´¨é‡å’Œæ•ˆç‡æ–¹é¢è¾¾åˆ°äº†é¢†å…ˆæ°´å¹³ã€‚é€šè¿‡è§£å†³ç°æœ‰æ–¹æ³•çš„åŸºæœ¬å±€é™æ€§ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨æ—¶ç©ºå’Œæ—¶ç©ºåŠ¨æ€å…¨å¿ƒé‡å»ºæ–¹é¢æ¨åŠ¨äº†æœ€æ–°æŠ€æœ¯ï¼Œä¸ºå¿ƒè¡€ç®¡æˆåƒæä¾›äº†ç¨³å¥ä¸”å®ç”¨çš„ä¸´åºŠè§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13826v3">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¿ƒè„ç£å…±æŒ¯ï¼ˆCMRï¼‰æˆåƒåœ¨è¯Šæ–­å’Œæ²»ç–—å¿ƒè¡€ç®¡ç–¾ç—…æ–¹é¢å…·æœ‰é‡è¦æ„ä¹‰ï¼Œä½†å…¶æ•ˆç”¨å¸¸å› äºŒç»´çŸ­è½´åˆ‡ç‰‡çš„ç¨€ç–é‡‡é›†è€Œå—åˆ°é™åˆ¶ï¼Œå¯¼è‡´ä½“ç§¯ä¿¡æ¯ä¸å®Œæ•´ã€‚å‡†ç¡®çš„ä»ç¨€ç–åˆ‡ç‰‡ä¸­è¿›è¡Œä¸‰ç»´é‡å»ºå¯¹äºå…¨é¢çš„å¿ƒè„è¯„ä¼°è‡³å…³é‡è¦ï¼Œä½†ç°æœ‰æ–¹æ³•é¢ä¸´æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ä¾èµ–é¢„è®¾çš„æ’å€¼æ–¹æ¡ˆï¼ˆå¦‚çº¿æ€§æˆ–çƒå½¢ï¼‰ã€è®¡ç®—æ•ˆç‡ä½ä¸‹ä»¥åŠå¯¹é¢å¤–çš„è¯­ä¹‰è¾“å…¥ï¼ˆå¦‚åˆ†å‰²æ ‡ç­¾æˆ–è¿åŠ¨æ•°æ®ï¼‰çš„ä¾èµ–ã€‚ä¸ºè§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†å…¨æ–°çš„å¿ƒè„æ½œåœ¨æ’å€¼æ‰©æ•£ï¼ˆCaLIDï¼‰æ¡†æ¶ï¼Œå¼•å…¥äº†ä¸‰é¡¹å…³é”®åˆ›æ–°ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åŸºäºæ‰©æ•£æ¨¡å‹æå‡ºäº†æ•°æ®é©±åŠ¨æ’å€¼æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆå¯ä»¥æ•æ‰ç¨€ç–åˆ‡ç‰‡ä¹‹é—´çš„å¤æ‚éçº¿æ€§å…³ç³»ï¼Œæé«˜é‡å»ºç²¾åº¦ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åœ¨æ½œåœ¨ç©ºé—´é«˜æ•ˆè¿è¡Œçš„æ–¹æ³•ï¼Œå°†å¿ƒè„3Dä¸Šé‡‡æ ·çš„æ—¶é—´ç¼©çŸ­äº†24å€ï¼Œé™ä½äº†è®¡ç®—å¼€é”€ã€‚ç¬¬ä¸‰ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä»…ä½¿ç”¨ç¨€ç–çš„2DCMRå›¾åƒä½œä¸ºè¾“å…¥ï¼Œå³å¯è¾¾åˆ°ä¼˜äºåŸºçº¿æ–¹æ³•çš„æ€§èƒ½ï¼Œæ— éœ€è¾…åŠ©è¾“å…¥ï¼Œä»è€Œç®€åŒ–äº†å·¥ä½œæµç¨‹ã€‚æˆ‘ä»¬è¿˜å°†æ–¹æ³•æ‰©å±•åˆ°äº†äºŒç»´åŠ æ—¶é—´æ•°æ®ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°å¯¹æ—¶ç©ºåŠ¨æ€è¿›è¡Œå»ºæ¨¡ï¼Œç¡®ä¿æ—¶é—´è¿è´¯æ€§ã€‚å¤§é‡çš„ä½“ç§¯è¯„ä¼°å’Œä¸‹æ¸¸åˆ†å‰²ä»»åŠ¡è¡¨æ˜ï¼ŒCaLIDåœ¨é‡å»ºè´¨é‡å’Œæ•ˆç‡æ–¹é¢è¾¾åˆ°äº†å“è¶Šçš„æ°´å¹³ã€‚é€šè¿‡è§£å†³ç°æœ‰æ–¹æ³•çš„åŸºæœ¬å±€é™æ€§ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨ç©ºé—´å’Œæ—¶ç©ºå…¨å¿ƒè„é‡å»ºæ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼Œä¸ºå¿ƒè¡€ç®¡æˆåƒæä¾›äº†ç¨³å¥ä¸”å®ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>CMRæˆåƒåœ¨å¿ƒè¡€ç®¡ç–¾ç—…è¯Šæ–­å’Œç®¡ç†ä¸­è‡³å…³é‡è¦ï¼Œä½†å—é™äºäºŒç»´åˆ‡ç‰‡çš„ç¨€ç–é‡‡é›†å¯¼è‡´çš„ä½“ç§¯ä¿¡æ¯ä¸å®Œæ•´ã€‚</li>
<li>ç°æœ‰ä¸‰ç»´é‡å»ºæ–¹æ³•é¢ä¸´ä¾èµ–é¢„è®¾æ’å€¼æ–¹æ¡ˆã€è®¡ç®—æ•ˆç‡ä½ä¸‹å’Œå¯¹é¢å¤–è¯­ä¹‰è¾“å…¥çš„ä¾èµ–ç­‰æŒ‘æˆ˜ã€‚</li>
<li>CaLIDæ¡†æ¶å¼•å…¥æ•°æ®é©±åŠ¨æ’å€¼æ–¹æ¡ˆï¼Œæé«˜é‡å»ºç²¾åº¦ï¼Œä¸”æ— éœ€é¢å¤–çš„è¾…åŠ©è¾“å…¥ã€‚</li>
<li>CaLIDåœ¨æ½œåœ¨ç©ºé—´æ“ä½œï¼Œè®¡ç®—æ•ˆç‡é«˜ï¼Œå¤§å¤§ç¼©çŸ­äº†ä¸‰ç»´å¿ƒè„ä¸Šé‡‡æ ·çš„æ—¶é—´ã€‚</li>
<li>CaLIDèƒ½å¤Ÿæ‰©å±•åˆ°äºŒç»´åŠ æ—¶é—´æ•°æ®ï¼Œæœ‰æ•ˆå»ºæ¨¡æ—¶ç©ºåŠ¨æ€ï¼Œç¡®ä¿æ—¶é—´è¿è´¯æ€§ã€‚</li>
<li>å¹¿æ³›çš„è¯„ä¼°å’Œä¸‹æ¸¸ä»»åŠ¡æ˜¾ç¤ºCaLIDåœ¨é‡å»ºè´¨é‡å’Œæ•ˆç‡æ–¹é¢è¾¾åˆ°å“è¶Šæ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13826">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8e4059179698f84ec7b7acc198cc9b38.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b3c2aacf31192a2998523e57c1e2452.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9db76104ad21d445d80009b4eb9b5fd5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7b3e8fbd563620fc41599e9edf300cdb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b595fe2d116ad3426b59833e331eab7.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Synthesizing-Near-Boundary-OOD-Samples-for-Out-of-Distribution-Detection"><a href="#Synthesizing-Near-Boundary-OOD-Samples-for-Out-of-Distribution-Detection" class="headerlink" title="Synthesizing Near-Boundary OOD Samples for Out-of-Distribution Detection"></a>Synthesizing Near-Boundary OOD Samples for Out-of-Distribution Detection</h2><p><strong>Authors:Jinglun Li, Kaixun Jiang, Zhaoyu Chen, Bo Lin, Yao Tang, Weifeng Ge, Wenqiang Zhang</strong></p>
<p>Pre-trained vision-language models have exhibited remarkable abilities in detecting out-of-distribution (OOD) samples. However, some challenging OOD samples, which lie close to in-distribution (InD) data in image feature space, can still lead to misclassification. The emergence of foundation models like diffusion models and multimodal large language models (MLLMs) offers a potential solution to this issue. In this work, we propose SynOOD, a novel approach that harnesses foundation models to generate synthetic, challenging OOD data for fine-tuning CLIP models, thereby enhancing boundary-level discrimination between InD and OOD samples. Our method uses an iterative in-painting process guided by contextual prompts from MLLMs to produce nuanced, boundary-aligned OOD samples. These samples are refined through noise adjustments based on gradients from OOD scores like the energy score, effectively sampling from the InD&#x2F;OOD boundary. With these carefully synthesized images, we fine-tune the CLIP image encoder and negative label features derived from the text encoder to strengthen connections between near-boundary OOD samples and a set of negative labels. Finally, SynOOD achieves state-of-the-art performance on the large-scale ImageNet benchmark, with minimal increases in parameters and runtime. Our approach significantly surpasses existing methods, and the code is available at <a target="_blank" rel="noopener" href="https://github.com/Jarvisgivemeasuit/SynOOD">https://github.com/Jarvisgivemeasuit/SynOOD</a>. </p>
<blockquote>
<p>é¢„è®­ç»ƒçš„è¯­è¨€è§†è§‰æ¨¡å‹åœ¨æ£€æµ‹åˆ†å¸ƒå¤–ï¼ˆOODï¼‰æ ·æœ¬æ–¹é¢è¡¨ç°å‡ºäº†æ˜¾è‘—çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œä¸€äº›åœ¨å›¾åƒç‰¹å¾ç©ºé—´ä¸Šä¸åˆ†å¸ƒå†…ï¼ˆInDï¼‰æ•°æ®ç›¸è¿‘çš„å…·æœ‰æŒ‘æˆ˜æ€§çš„OODæ ·æœ¬ï¼Œä»å¯èƒ½å¯¼è‡´è¯¯åˆ†ç±»ã€‚æ‰©æ•£æ¨¡å‹å’Œå¤šåª’ä½“è¯­è¨€æ¨¡å‹ç­‰åŸºç¡€æ¨¡å‹çš„æ¶Œç°ä¸ºè§£å†³è¿™ä¸€é—®é¢˜æä¾›äº†æ½œåœ¨è§£å†³æ–¹æ¡ˆã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†SynOODï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨åŸºç¡€æ¨¡å‹ç”Ÿæˆåˆæˆã€å…·æœ‰æŒ‘æˆ˜æ€§çš„OODæ•°æ®å¯¹CLIPæ¨¡å‹è¿›è¡Œå¾®è°ƒçš„æ–°æ–¹æ³•ï¼Œä»è€Œæé«˜äº†InDå’ŒOODæ ·æœ¬ä¹‹é—´çš„è¾¹ç•Œçº§åˆ«åˆ¤åˆ«èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨ç”±å¤šåª’ä½“è¯­è¨€æ¨¡å‹æä¾›çš„ä¸Šä¸‹æ–‡æç¤ºå¼•å¯¼çš„è¿­ä»£å¡«å……è¿‡ç¨‹æ¥ç”Ÿæˆå¾®å¦™çš„ã€ä¸è¾¹ç•Œå¯¹é½çš„OODæ ·æœ¬ã€‚è¿™äº›æ ·æœ¬é€šè¿‡åŸºäºOODåˆ†æ•°ï¼ˆå¦‚èƒ½é‡åˆ†æ•°ï¼‰çš„æ¢¯åº¦è¿›è¡Œå™ªå£°è°ƒæ•´æ¥ç»†åŒ–ï¼Œæœ‰æ•ˆåœ°ä»InD&#x2F;OODè¾¹ç•Œè¿›è¡Œé‡‡æ ·ã€‚é€šè¿‡ä»”ç»†åˆæˆçš„å›¾åƒï¼Œæˆ‘ä»¬å¾®è°ƒCLIPå›¾åƒç¼–ç å™¨å’Œä»æ–‡æœ¬ç¼–ç å™¨è·å¾—çš„è´Ÿæ ‡ç­¾ç‰¹å¾ï¼Œä»¥åŠ å¼ºè¿‘è¾¹ç•ŒOODæ ·æœ¬ä¸ä¸€ç»„è´Ÿæ ‡ç­¾ä¹‹é—´çš„è”ç³»ã€‚æœ€åï¼ŒSynOODåœ¨å¤§å‹ImageNetåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå‚æ•°å’Œè¿è¡Œæ—¶å‡ ä¹æ²¡æœ‰å¢åŠ ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä»£ç å¯è®¿é—®<a target="_blank" rel="noopener" href="https://github.com/Jarvisgivemeasuit/SynOOD%E3%80%82">https://github.com/Jarvisgivemeasuit/SynOODã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10225v3">PDF</a> Accepted by ICCV 2025 (Highlight)</p>
<p><strong>æ‘˜è¦</strong><br>    é¢„è®­ç»ƒè§†å¬æ¨¡å‹åœ¨æ£€æµ‹å¼‚å¸¸åˆ†å¸ƒï¼ˆOODï¼‰æ ·æœ¬æ–¹é¢è¡¨ç°å‡ºå“è¶Šèƒ½åŠ›ï¼Œä½†å¯¹äºæ¥è¿‘æ­£å¸¸åˆ†å¸ƒæ•°æ®çš„OODæ ·æœ¬ï¼Œä»å­˜åœ¨è¯¯åˆ†ç±»é£é™©ã€‚æœ¬æ–‡å¼•å…¥æ‰©æ•£æ¨¡å‹ç­‰å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ï¼Œæå‡ºSynOODæ–¹æ³•ï¼Œç”Ÿæˆåˆæˆå¼‚å¸¸åˆ†å¸ƒæ•°æ®å¯¹CLIPæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œæé«˜æ­£å¸¸ä¸å¼‚å¸¸æ ·æœ¬é—´çš„è¾¹ç•Œçº§åˆ«åˆ¤åˆ«èƒ½åŠ›ã€‚è¯¥æ–¹æ³•é€šè¿‡è¿­ä»£ä¿®å¤è¿‡ç¨‹ä¸å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡æç¤ºç›¸ç»“åˆï¼Œç”Ÿæˆå¾®å¦™çš„è¾¹ç•Œå¯¹é½OODæ ·æœ¬ã€‚è¿™äº›æ ·æœ¬åŸºäºå¼‚å¸¸åˆ†æ•°ï¼ˆå¦‚èƒ½é‡åˆ†æ•°ï¼‰çš„æ¢¯åº¦è¿›è¡Œå™ªå£°è°ƒæ•´ï¼Œæœ‰æ•ˆé‡‡æ ·æ­£å¸¸åˆ†å¸ƒä¸å¼‚å¸¸åˆ†å¸ƒè¾¹ç•Œã€‚é€šè¿‡ç²¾ç»†åˆæˆçš„å›¾åƒï¼Œæˆ‘ä»¬å¾®è°ƒCLIPå›¾åƒç¼–ç å™¨ä¸æ¥è‡ªæ–‡æœ¬ç¼–ç å™¨çš„è´Ÿæ ‡ç­¾ç‰¹å¾ï¼Œå¼ºåŒ–è¿‘è¾¹ç•ŒOODæ ·æœ¬ä¸ä¸€ç³»åˆ—è´Ÿæ ‡ç­¾ä¹‹é—´çš„è¿æ¥ã€‚æœ€ç»ˆï¼ŒSynOODåœ¨å¤§å‹ImageNetåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°é¢†å…ˆæ°´å¹³ï¼Œä¸”å‚æ•°ä¸è¿è¡Œæ—¶é—´å¢åŠ å¾®å°ã€‚è¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œä»£ç å·²å…¬å¼€ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>é¢„è®­ç»ƒè§†å¬æ¨¡å‹åœ¨æ£€æµ‹OODæ ·æœ¬æ–¹é¢å…·æœ‰å‡ºè‰²èƒ½åŠ›ï¼Œä½†å¯¹äºæ¥è¿‘æ­£å¸¸åˆ†å¸ƒæ•°æ®çš„OODæ ·æœ¬ä»å¯èƒ½è¯¯åˆ†ç±»ã€‚</li>
<li>å¼•å…¥æ‰©æ•£æ¨¡å‹ç­‰å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹æ˜¯è§£å†³è¿™ä¸€é—®é¢˜çš„ä¸€ç§æ½œåœ¨æ–¹æ³•ã€‚</li>
<li>SynOODæ–¹æ³•åˆ©ç”¨åŸºç¡€æ¨¡å‹ç”ŸæˆåˆæˆOODæ•°æ®ï¼Œç”¨äºå¾®è°ƒCLIPæ¨¡å‹ï¼Œæé«˜æ­£å¸¸ä¸å¼‚å¸¸æ ·æœ¬é—´çš„è¾¹ç•Œè¯†åˆ«ã€‚</li>
<li>SynOODé€šè¿‡è¿­ä»£ä¿®å¤è¿‡ç¨‹ä¸ä¸Šä¸‹æ–‡æç¤ºç”Ÿæˆå¾®å¦™çš„è¾¹ç•Œå¯¹é½OODæ ·æœ¬ã€‚</li>
<li>è¿™äº›æ ·æœ¬åŸºäºå¼‚å¸¸åˆ†æ•°ï¼ˆå¦‚èƒ½é‡åˆ†æ•°ï¼‰çš„æ¢¯åº¦è¿›è¡Œå™ªå£°è°ƒæ•´ï¼Œä»è€Œæ›´å¥½åœ°åæ˜ æ­£å¸¸ä¸å¼‚å¸¸åˆ†å¸ƒçš„è¾¹ç•Œã€‚</li>
<li>é€šè¿‡ç²¾ç»†åˆæˆçš„å›¾åƒï¼ŒSynOODå¾®è°ƒäº†CLIPå›¾åƒç¼–ç å™¨å’Œè´Ÿæ ‡ç­¾ç‰¹å¾ï¼Œå¼ºåŒ–äº†è¿‘è¾¹ç•ŒOODæ ·æœ¬ä¸è´Ÿæ ‡ç­¾ä¹‹é—´çš„è¿æ¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10225">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c71b5e238d840e076feb4e10f2993009.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c9eff2f1034c6fe8cfa8da20ff80c095.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-014152f40cd33b577bc10e2c7878cd59.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-681933d97e047dcacb3d67817f780db4.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="CopyrightShield-Enhancing-Diffusion-Model-Security-against-Copyright-Infringement-Attacks"><a href="#CopyrightShield-Enhancing-Diffusion-Model-Security-against-Copyright-Infringement-Attacks" class="headerlink" title="CopyrightShield: Enhancing Diffusion Model Security against Copyright   Infringement Attacks"></a>CopyrightShield: Enhancing Diffusion Model Security against Copyright   Infringement Attacks</h2><p><strong>Authors:Zhixiang Guo, Siyuan Liang, Aishan Liu, Dacheng Tao</strong></p>
<p>Diffusion models have attracted significant attention due to its exceptional data generation capabilities in fields such as image synthesis. However, recent studies have shown that diffusion models are vulnerable to copyright infringement attacks, where attackers inject strategically modified non-infringing images into the training set, inducing the model to generate infringing content under the prompt of specific poisoned captions. To address this issue, we first propose a defense framework, CopyrightShield, to defend against the above attack. Specifically, we analyze the memorization mechanism of diffusion models and find that attacks exploit the modelâ€™s overfitting to specific spatial positions and prompts, causing it to reproduce poisoned samples under backdoor triggers. Based on this, we propose a poisoned sample detection method using spatial masking and data attribution to quantify poisoning risk and accurately identify hidden backdoor samples. To further mitigate memorization of poisoned features, we introduce an adaptive optimization strategy that integrates a dynamic penalty term into the training loss, reducing reliance on infringing features while preserving generative performance. Experimental results demonstrate that CopyrightShield significantly improves poisoned sample detection performance across two attack scenarios, achieving average F1-scores of 0.665, retarding the First-Attack Epoch (FAE) of 115.2% and decreasing the Copyright Infringement Rate (CIR) by 56.7%. Compared to the SoTA backdoor defense in diffusion models, the defense effect is improved by about 25%, showcasing its superiority and practicality in enhancing the security of diffusion models. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹å› å…¶å›¾åƒåˆæˆç­‰é¢†åŸŸçš„å‡ºè‰²æ•°æ®ç”Ÿæˆèƒ½åŠ›è€Œå¤‡å—å…³æ³¨ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œæ‰©æ•£æ¨¡å‹å®¹æ˜“å—åˆ°ç‰ˆæƒä¾µçŠ¯æ”»å‡»ã€‚æ”»å‡»è€…å°†æˆ˜ç•¥ä¿®æ”¹çš„çš„éä¾µæƒå›¾åƒæ³¨å…¥è®­ç»ƒé›†ï¼Œè¯±å¯¼æ¨¡å‹åœ¨ç‰¹å®šä¸­æ¯’æ ‡é¢˜çš„æç¤ºä¸‹ç”Ÿæˆä¾µæƒå†…å®¹ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬é¦–å…ˆæå‡ºäº†ä¸€ä¸ªé˜²å¾¡æ¡†æ¶CopyrightShieldæ¥é˜²èŒƒä¸Šè¿°æ”»å‡»ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åˆ†æäº†æ‰©æ•£æ¨¡å‹çš„è®°å¿†æœºåˆ¶ï¼Œå¹¶å‘ç°æ”»å‡»è€…åˆ©ç”¨æ¨¡å‹å¯¹ç‰¹å®šç©ºé—´ä½ç½®å’Œæç¤ºçš„è¿‡åº¦æ‹Ÿåˆï¼Œåœ¨åé—¨è§¦å‘ä¸‹å¤åˆ¶ä¸­æ¯’æ ·æœ¬ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä½¿ç”¨ç©ºé—´æ©è”½å’Œæ•°æ®å½’å±çš„ä¸­æ¯’æ ·æœ¬æ£€æµ‹æ–¹æ³•ï¼Œä»¥é‡åŒ–ä¸­æ¯’é£é™©å¹¶å‡†ç¡®è¯†åˆ«éšè—çš„åé—¨æ ·æœ¬ã€‚ä¸ºäº†è¿›ä¸€æ­¥å‡è½»å¯¹ä¸­æ¯’ç‰¹å¾çš„è®°å¿†ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è‡ªé€‚åº”ä¼˜åŒ–ç­–ç•¥ï¼Œå°†åŠ¨æ€æƒ©ç½šé¡¹é›†æˆåˆ°è®­ç»ƒæŸå¤±ä¸­ï¼Œä»è€Œåœ¨ä¿ç•™ç”Ÿæˆæ€§èƒ½çš„åŒæ—¶å‡å°‘äº†å¯¹ä¾µæƒç‰¹å¾çš„ä¾èµ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCopyrightShieldåœ¨ä¸¤ç§æ”»å‡»åœºæ™¯ä¸‹æ˜¾è‘—æé«˜äº†ä¸­æ¯’æ ·æœ¬çš„æ£€æµ‹æ€§èƒ½ï¼Œå¹³å‡F1åˆ†æ•°ä¸º0.665ï¼ŒFirst-Attack Epochï¼ˆFAEï¼‰å»¶è¿Ÿäº†115.2%ï¼Œå¹¶ä¸”é™ä½äº†ç‰ˆæƒä¾µçŠ¯ç‡ï¼ˆCIRï¼‰çº¦56.7%ã€‚ä¸å½“å‰æœ€å…ˆè¿›çš„æ‰©æ•£æ¨¡å‹åé—¨é˜²å¾¡ç›¸æ¯”ï¼Œé˜²å¾¡æ•ˆæœæé«˜äº†çº¦25%ï¼Œè¯æ˜äº†å…¶åœ¨æé«˜æ‰©æ•£æ¨¡å‹å®‰å…¨æ€§æ–¹é¢çš„ä¼˜è¶Šæ€§å’Œå®ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.01528v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ‰©æ•£æ¨¡å‹å› å…¶å›¾åƒåˆæˆç­‰é¢†åŸŸçš„å‡ºè‰²æ•°æ®ç”Ÿæˆèƒ½åŠ›è€Œå¤‡å—å…³æ³¨ã€‚ç„¶è€Œï¼Œè¿‘æœŸç ”ç©¶è¡¨æ˜ï¼Œæ‰©æ•£æ¨¡å‹æ˜“å—ç‰ˆæƒä¾µçŠ¯æ”»å‡»ã€‚æ”»å‡»è€…å°†ç­–ç•¥æ€§ä¿®æ”¹çš„éä¾µæƒå›¾åƒæ³¨å…¥è®­ç»ƒé›†ï¼Œè¯±å¯¼æ¨¡å‹åœ¨ç‰¹å®šä¸­æ¯’æ ‡é¢˜çš„æç¤ºä¸‹ç”Ÿæˆä¾µæƒå†…å®¹ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºé˜²å¾¡æ¡†æ¶CopyrightShieldã€‚é€šè¿‡åˆ†ææ‰©æ•£æ¨¡å‹çš„è®°å¿†æœºåˆ¶ï¼Œæˆ‘ä»¬å‘ç°æ”»å‡»æ˜¯åˆ©ç”¨æ¨¡å‹å¯¹ç‰¹å®šç©ºé—´ä½ç½®å’Œæç¤ºçš„è¿‡æ‹Ÿåˆï¼Œå¯¼è‡´åœ¨åé—¨è§¦å‘ä¸‹é‡ç°ä¸­æ¯’æ ·æœ¬ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºä¸€ç§ä½¿ç”¨ç©ºé—´æ©ç å’Œæ•°æ®å½’å±çš„ä¸­æ¯’æ ·æœ¬æ£€æµ‹æ–¹æ³•ï¼Œä»¥é‡åŒ–ä¸­æ¯’é£é™©å¹¶å‡†ç¡®è¯†åˆ«éšè—çš„åé—¨æ ·æœ¬ã€‚ä¸ºè¿›ä¸€æ­¥å‡è½»å¯¹ä¸­æ¯’ç‰¹å¾çš„è®°å¿†ï¼Œæˆ‘ä»¬å¼•å…¥è‡ªé€‚åº”ä¼˜åŒ–ç­–ç•¥ï¼Œå°†åŠ¨æ€æƒ©ç½šé¡¹é›†æˆåˆ°è®­ç»ƒæŸå¤±ä¸­ï¼Œå‡å°‘äº†å¯¹ä¾µæƒç‰¹å¾çš„ä¾èµ–ï¼ŒåŒæ—¶ä¿æŒäº†ç”Ÿæˆæ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCopyrightShieldåœ¨ä¸¤ç§æ”»å‡»åœºæ™¯ä¸‹æ˜¾è‘—æé«˜äº†ä¸­æ¯’æ ·æœ¬æ£€æµ‹æ€§èƒ½ï¼Œå¹³å‡F1åˆ†æ•°ä¸º0.665ï¼ŒFirst-Attack Epochï¼ˆFAEï¼‰å»¶è¿Ÿäº†115.2%ï¼Œç‰ˆæƒä¾µæƒç‡ï¼ˆCIRï¼‰é™ä½äº†56.7%ã€‚ä¸å½“å‰æ‰©æ•£æ¨¡å‹ä¸­çš„åé—¨é˜²å¾¡æŠ€æœ¯ç›¸æ¯”ï¼Œå…¶é˜²å¾¡æ•ˆæœæé«˜äº†çº¦25%ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨æé«˜æ‰©æ•£æ¨¡å‹å®‰å…¨æ€§æ–¹é¢çš„ä¼˜åŠ¿å’Œå®ç”¨æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹å› å…¶æ•°æ®ç”Ÿæˆèƒ½åŠ›è€Œåœ¨å›¾åƒåˆæˆç­‰é¢†åŸŸå—åˆ°å¹¿æ³›å…³æ³¨ï¼Œä½†æ˜“å—ç‰ˆæƒä¾µçŠ¯æ”»å‡»ã€‚</li>
<li>æ”»å‡»è€…é€šè¿‡æ³¨å…¥æˆ˜ç•¥ä¿®æ”¹çš„éä¾µæƒå›¾åƒå’Œç‰¹å®šä¸­æ¯’æ ‡é¢˜ï¼Œè¯±å¯¼æ¨¡å‹ç”Ÿæˆä¾µæƒå†…å®¹ã€‚</li>
<li>CopyrightShieldé˜²å¾¡æ¡†æ¶è¢«æå‡ºï¼Œé€šè¿‡åˆ†æå’Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„è®°å¿†æœºåˆ¶æ¥å¯¹æŠ—è¿™ç§æ”»å‡»ã€‚</li>
<li>æå‡ºä¸€ç§ä½¿ç”¨ç©ºé—´æ©ç å’Œæ•°æ®å½’å±çš„ä¸­æ¯’æ ·æœ¬æ£€æµ‹æ–¹æ³•ï¼Œä»¥æ£€æµ‹å’Œè¯†åˆ«ä¸­æ¯’æ ·æœ¬ã€‚</li>
<li>ä¸ºå‡è½»å¯¹ä¸­æ¯’ç‰¹å¾çš„è®°å¿†ï¼Œå¼•å…¥è‡ªé€‚åº”ä¼˜åŒ–ç­–ç•¥ï¼Œé›†æˆåŠ¨æ€æƒ©ç½šé¡¹åˆ°è®­ç»ƒæŸå¤±ä¸­ã€‚</li>
<li>å®éªŒè¯æ˜CopyrightShieldæ˜¾è‘—æé«˜äº†ä¸­æ¯’æ ·æœ¬æ£€æµ‹æ€§èƒ½ï¼Œå¹¶é™ä½äº†ç‰ˆæƒä¾µæƒç‡ã€‚</li>
<li>ä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”ï¼ŒCopyrightShieldçš„é˜²å¾¡æ•ˆæœæœ‰æ˜¾è‘—æé«˜ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨å¢å¼ºæ‰©æ•£æ¨¡å‹å®‰å…¨æ€§æ–¹é¢çš„ä¼˜åŠ¿å’Œå®ç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.01528">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-355483a02bd15cf402dd4ff392fd4c03.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2ee2b09d5e15f7f3e648d63ca98a8356.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e81f81940f0416987030d7f0aea40c1a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7ddcc4fec8607c57f739b5cb3016e8d1.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Vulnerabilities-in-AI-generated-Image-Detection-The-Challenge-of-Adversarial-Attacks"><a href="#Vulnerabilities-in-AI-generated-Image-Detection-The-Challenge-of-Adversarial-Attacks" class="headerlink" title="Vulnerabilities in AI-generated Image Detection: The Challenge of   Adversarial Attacks"></a>Vulnerabilities in AI-generated Image Detection: The Challenge of   Adversarial Attacks</h2><p><strong>Authors:Yunfeng Diao, Naixin Zhai, Changtao Miao, Zitong Yu, Xingxing Wei, Xun Yang, Meng Wang</strong></p>
<p>Recent advancements in image synthesis, particularly with the advent of GAN and Diffusion models, have amplified public concerns regarding the dissemination of disinformation. To address such concerns, numerous AI-generated Image (AIGI) Detectors have been proposed and achieved promising performance in identifying fake images. However, there still lacks a systematic understanding of the adversarial robustness of AIGI detectors. In this paper, we examine the vulnerability of state-of-the-art AIGI detectors against adversarial attack under white-box and black-box settings, which has been rarely investigated so far. To this end, we propose a new method to attack AIGI detectors. First, inspired by the obvious difference between real images and fake images in the frequency domain, we add perturbations under the frequency domain to push the image away from its original frequency distribution. Second, we explore the full posterior distribution of the surrogate model to further narrow this gap between heterogeneous AIGI detectors, e.g. transferring adversarial examples across CNNs and ViTs. This is achieved by introducing a novel post-train Bayesian strategy that turns a single surrogate into a Bayesian one, capable of simulating diverse victim models using one pre-trained surrogate, without the need for re-training. We name our method as Frequency-based Post-train Bayesian Attack, or FPBA. Through FPBA, we show that adversarial attack is truly a real threat to AIGI detectors, because FPBA can deliver successful black-box attacks across models, generators, defense methods, and even evade cross-generator detection, which is a crucial real-world detection scenario. The code will be shared upon acceptance. </p>
<blockquote>
<p>è¿‘æœŸå›¾åƒåˆæˆé¢†åŸŸçš„è¿›å±•ï¼Œå°¤å…¶æ˜¯ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰å’Œæ‰©æ•£æ¨¡å‹çš„å‡ºç°ï¼ŒåŠ å‰§äº†å…¬ä¼—å¯¹ä¼ æ’­è™šå‡ä¿¡æ¯çš„æ‹…å¿§ã€‚ä¸ºäº†è§£å†³è¿™äº›æ‹…å¿§ï¼Œå·²ç»æå‡ºäº†è®¸å¤šäººå·¥æ™ºèƒ½ç”Ÿæˆçš„å›¾åƒï¼ˆAIGIï¼‰æ£€æµ‹å™¨ï¼Œå¹¶ä¸”åœ¨è¯†åˆ«è™šå‡å›¾åƒæ–¹é¢å–å¾—äº†ä»¤äººç©ç›®çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œäººä»¬å¯¹äºAIGIæ£€æµ‹å™¨çš„å¯¹æŠ—æ€§ç¨³å¥æ€§ä»ç¼ºä¹ç³»ç»Ÿçš„ç†è§£ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†æœ€å…ˆè¿›çš„AIGIæ£€æµ‹å™¨åœ¨ç™½ç›’å’Œé»‘ç›’è®¾ç½®ä¸‹å¯¹æŠ—æ”»å‡»çš„è„†å¼±æ€§ï¼Œè¿™ä¸€é¢†åŸŸè¿„ä»Šä¸ºæ­¢å¾ˆå°‘è¢«ç ”ç©¶ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ”»å‡»AIGIæ£€æµ‹å™¨çš„æ–°æ–¹æ³•ã€‚é¦–å…ˆï¼Œå—çœŸå®å›¾åƒå’Œè™šå‡å›¾åƒåœ¨é¢‘åŸŸä¸Šæ˜æ˜¾å·®å¼‚çš„å¯å‘ï¼Œæˆ‘ä»¬åœ¨é¢‘åŸŸä¸­æ·»åŠ æ‰°åŠ¨ï¼Œä½¿å›¾åƒè¿œç¦»å…¶åŸå§‹é¢‘ç‡åˆ†å¸ƒã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ä»£ç†æ¨¡å‹çš„åéªŒåˆ†å¸ƒï¼Œä»¥è¿›ä¸€æ­¥ç¼©å°ä¸åŒAIGIæ£€æµ‹å™¨ä¹‹é—´çš„å·®è·ï¼Œä¾‹å¦‚ï¼Œåœ¨å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’Œè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰ä¹‹é—´è½¬ç§»å¯¹æŠ—æ ·æœ¬ã€‚è¿™æ˜¯é€šè¿‡å¼•å…¥ä¸€ç§æ–°çš„åè®­ç»ƒè´å¶æ–¯ç­–ç•¥å®ç°çš„ï¼Œè¯¥ç­–ç•¥å°†å•ä¸ªä»£ç†è½¬å˜ä¸ºè´å¶æ–¯ä»£ç†ï¼Œèƒ½å¤Ÿåˆ©ç”¨ä¸€ä¸ªé¢„è®­ç»ƒçš„ä»£ç†æ¨¡æ‹Ÿå¤šç§å—å®³è€…æ¨¡å‹ï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚æˆ‘ä»¬å°†æˆ‘ä»¬çš„æ–¹æ³•å‘½åä¸ºåŸºäºé¢‘ç‡çš„åè®­ç»ƒè´å¶æ–¯æ”»å‡»ï¼ˆFPBAï¼‰ã€‚é€šè¿‡FPBAï¼Œæˆ‘ä»¬è¯æ˜å¯¹æŠ—æ”»å‡»ç¡®å®å¯¹AIGIæ£€æµ‹å™¨æ„æˆçœŸæ­£çš„å¨èƒï¼Œå› ä¸ºFPBAå¯ä»¥æˆåŠŸåœ°è¿›è¡Œè·¨æ¨¡å‹ã€ç”Ÿæˆå™¨ã€é˜²å¾¡æ–¹æ³•çš„é»‘ç›’æ”»å‡»ï¼Œç”šè‡³èƒ½é€ƒé¿è·¨ç”Ÿæˆå™¨æ£€æµ‹ï¼Œè¿™æ˜¯å…³é”®çš„ç°å®ä¸–ç•Œæ£€æµ‹åœºæ™¯ã€‚ä»£ç å°†åœ¨æ¥å—åå…±äº«ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.20836v4">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>éšç€GANå’ŒDiffusionæ¨¡å‹çš„å‡ºç°ï¼Œå›¾åƒåˆæˆæŠ€æœ¯çš„æœ€æ–°è¿›å±•åŠ å‰§äº†å…¬ä¼—å¯¹è™šå‡ä¿¡æ¯ä¼ æ’­çš„æ‹…å¿§ã€‚ä¸ºåº”å¯¹è¿™äº›æ‹…å¿§ï¼Œå·²ç»æå‡ºäº†è®¸å¤šAIç”Ÿæˆçš„å›¾åƒï¼ˆAIGIï¼‰æ£€æµ‹å™¨ï¼Œå¹¶åœ¨è¯†åˆ«è™šå‡å›¾åƒæ–¹é¢å–å¾—äº†æœ‰å¸Œæœ›çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå…³äºAIGIæ£€æµ‹å™¨çš„å¯¹æŠ—æ€§ç¨³å¥æ€§ä»å­˜åœ¨ç³»ç»Ÿæ€§çš„ç†è§£ä¸è¶³ã€‚æœ¬æ–‡ç ”ç©¶äº†æœ€å…ˆè¿›çš„AIGIæ£€æµ‹å™¨åœ¨ç™½ç›’å’Œé»‘ç›’è®¾ç½®ä¸‹å¯¹æŠ—å¯¹æŠ—æ€§æ”»å‡»çš„è„†å¼±æ€§ï¼Œè¿™ä¸€é¢†åŸŸè¿„ä»Šä¸ºæ­¢å¾ˆå°‘è¢«ç ”ç©¶ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ”»å‡»AIGIæ£€æµ‹å™¨çš„æ–¹æ³•ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å—åˆ°çœŸå®å›¾åƒå’Œè™šå‡å›¾åƒåœ¨é¢‘åŸŸä¸­æ˜æ˜¾å·®å¼‚çš„å¯å‘ï¼Œåœ¨é¢‘åŸŸä¸­æ·»åŠ æ‰°åŠ¨ä½¿å›¾åƒåç¦»å…¶åŸå§‹é¢‘ç‡åˆ†å¸ƒã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ä»£ç†æ¨¡å‹çš„åéªŒåˆ†å¸ƒï¼Œä»¥è¿›ä¸€æ­¥ç¼©å°ä¸åŒAIGIæ£€æµ‹å™¨ä¹‹é—´çš„é¸¿æ²Ÿã€‚è¿™æ˜¯é€šè¿‡å¼•å…¥ä¸€ç§æ–°çš„åè®­ç»ƒè´å¶æ–¯ç­–ç•¥å®ç°çš„ï¼Œè¯¥ç­–ç•¥å°†å•ä¸€çš„ä»£ç†æ¨¡å‹è½¬å˜ä¸ºè´å¶æ–¯æ¨¡å‹ï¼Œèƒ½å¤Ÿæ¨¡æ‹Ÿä½¿ç”¨å•ä¸€é¢„è®­ç»ƒä»£ç†çš„å¤šä¸ªå—å®³è€…æ¨¡å‹ï¼Œæ— éœ€é‡æ–°è®­ç»ƒã€‚æˆ‘ä»¬ç§°æˆ‘ä»¬çš„æ–¹æ³•ä¸ºåŸºäºé¢‘ç‡çš„åè®­ç»ƒè´å¶æ–¯æ”»å‡»ï¼ˆFPBAï¼‰ã€‚é€šè¿‡FPBAï¼Œæˆ‘ä»¬è¯æ˜äº†å¯¹æŠ—æ€§æ”»å‡»ç¡®å®å¯¹AIGIæ£€æµ‹å™¨æ„æˆå¨èƒï¼Œå› ä¸ºFPBAå¯ä»¥åœ¨ä¸åŒçš„æ¨¡å‹ã€ç”Ÿæˆå™¨ã€é˜²å¾¡æ–¹æ³•å’Œè·¨ç”Ÿæˆå™¨æ£€æµ‹ä¸­æˆåŠŸå®æ–½é»‘ç›’æ”»å‡»ï¼Œä»è€Œé€ƒé¿ç°å®ä¸–ç•Œçš„æ£€æµ‹åœºæ™¯ã€‚ä»£ç å°†åœ¨æ¥å—åå…±äº«ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>æœ€æ–°å›¾åƒåˆæˆæŠ€æœ¯å¼•å‘å…¬ä¼—å¯¹è™šå‡ä¿¡æ¯ä¼ æ’­æ‹…å¿§ï¼ŒAIGIæ£€æµ‹å™¨ç”¨äºè¯†åˆ«è™šå‡å›¾åƒã€‚</li>
<li>å½“å‰ç¼ºä¹å…³äºAIGIæ£€æµ‹å™¨å¯¹æŠ—æ€§ç¨³å¥æ€§çš„ç³»ç»Ÿæ€§ç†è§£ã€‚</li>
<li>æå‡ºäº†åœ¨ç™½ç›’å’Œé»‘ç›’è®¾ç½®ä¸‹å¯¹æŠ—æ”»å‡»AIGIæ£€æµ‹å™¨çš„æ–°æ–¹æ³•â€”â€”FPBAã€‚</li>
<li>FPBAé€šè¿‡é¢‘åŸŸæ‰°åŠ¨æ”»å‡»æ£€æµ‹å™¨ï¼Œå¹¶æ¢ç´¢ä»£ç†æ¨¡å‹çš„åéªŒåˆ†å¸ƒæ¥ç¼©å°ä¸åŒæ£€æµ‹å™¨é—´çš„å·®è·ã€‚</li>
<li>FPBAæˆåŠŸå®æ–½é»‘ç›’æ”»å‡»ï¼Œå¨èƒåˆ°AIGIæ£€æµ‹å™¨çš„ç¨³å¥æ€§ï¼Œå¯åº”ç”¨äºä¸åŒæ¨¡å‹ã€ç”Ÿæˆå™¨ã€é˜²å¾¡æ–¹æ³•å’Œè·¨ç”Ÿæˆå™¨æ£€æµ‹åœºæ™¯ã€‚</li>
<li>æ‰€ææ–¹æ³•å…·æœ‰å¹¿æ³›åº”ç”¨æ½œåŠ›ï¼Œå¯æœ‰æ•ˆé€ƒé¿ç°å®ä¸–ç•Œçš„æ£€æµ‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.20836">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-05516adc7e53e9da8989586850e5864c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1b75a81980c7c1fd6fa81fd641c70c25.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7c7506823da3f9dac8db5c459d8aace1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-23f50b063da8714d3ca2591c44b226bc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1c57d195cdf3ca8ae2fc7ef7d95d68ab.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-042413491ebb6df9da458ec6cd8da3f3.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Fast-DDPM-Fast-Denoising-Diffusion-Probabilistic-Models-for-Medical-Image-to-Image-Generation"><a href="#Fast-DDPM-Fast-Denoising-Diffusion-Probabilistic-Models-for-Medical-Image-to-Image-Generation" class="headerlink" title="Fast-DDPM: Fast Denoising Diffusion Probabilistic Models for Medical   Image-to-Image Generation"></a>Fast-DDPM: Fast Denoising Diffusion Probabilistic Models for Medical   Image-to-Image Generation</h2><p><strong>Authors:Hongxu Jiang, Muhammad Imran, Teng Zhang, Yuyin Zhou, Muxuan Liang, Kuang Gong, Wei Shao</strong></p>
<p>Denoising diffusion probabilistic models (DDPMs) have achieved unprecedented success in computer vision. However, they remain underutilized in medical imaging, a field crucial for disease diagnosis and treatment planning. This is primarily due to the high computational cost associated with (1) the use of large number of time steps (e.g., 1,000) in diffusion processes and (2) the increased dimensionality of medical images, which are often 3D or 4D. Training a diffusion model on medical images typically takes days to weeks, while sampling each image volume takes minutes to hours. To address this challenge, we introduce Fast-DDPM, a simple yet effective approach capable of improving training speed, sampling speed, and generation quality simultaneously. Unlike DDPM, which trains the image denoiser across 1,000 time steps, Fast-DDPM trains and samples using only 10 time steps. The key to our method lies in aligning the training and sampling procedures to optimize time-step utilization. Specifically, we introduced two efficient noise schedulers with 10 time steps: one with uniform time step sampling and another with non-uniform sampling. We evaluated Fast-DDPM across three medical image-to-image generation tasks: multi-image super-resolution, image denoising, and image-to-image translation. Fast-DDPM outperformed DDPM and current state-of-the-art methods based on convolutional networks and generative adversarial networks in all tasks. Additionally, Fast-DDPM reduced the training time to 0.2x and the sampling time to 0.01x compared to DDPM. Our code is publicly available at: <a target="_blank" rel="noopener" href="https://github.com/mirthAI/Fast-DDPM">https://github.com/mirthAI/Fast-DDPM</a>. </p>
<blockquote>
<p>å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMï¼‰åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸå–å¾—äº†å‰æ‰€æœªæœ‰çš„æˆåŠŸã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨åŒ»å­¦å½±åƒè¿™ä¸€å¯¹ç–¾ç—…è¯Šæ–­å’Œæ²»ç–—è®¡åˆ’è‡³å…³é‡è¦çš„é¢†åŸŸå´æœªå¾—åˆ°å……åˆ†åˆ©ç”¨ã€‚è¿™ä¸»è¦æ˜¯å› ä¸ºä¸ï¼ˆ1ï¼‰æ‰©æ•£è¿‡ç¨‹ä¸­ä½¿ç”¨å¤§é‡æ—¶é—´æ­¥æ•°ï¼ˆä¾‹å¦‚ï¼Œé«˜è¾¾ä¸€åƒæ­¥ï¼‰å’Œï¼ˆ2ï¼‰åŒ»å­¦å›¾åƒå¢åŠ çš„ç»´åº¦ï¼ˆé€šå¸¸ä¸ºä¸‰ç»´æˆ–å››ç»´ï¼‰ç›¸å…³çš„è®¡ç®—æˆæœ¬å¾ˆé«˜ã€‚åœ¨åŒ»å­¦å›¾åƒä¸Šè®­ç»ƒæ‰©æ•£æ¨¡å‹é€šå¸¸éœ€è¦æ•°å¤©è‡³æ•°å‘¨çš„æ—¶é—´ï¼Œè€Œå¯¹æ¯ä¸ªå›¾åƒä½“ç§¯è¿›è¡Œé‡‡æ ·åˆ™éœ€è¦æ•°åˆ†é’Ÿè‡³æ•°å°æ—¶çš„æ—¶é—´ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†Fast-DDPMï¼Œè¿™æ˜¯ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•ï¼Œå¯ä»¥åŒæ—¶æé«˜è®­ç»ƒé€Ÿåº¦ã€é‡‡æ ·é€Ÿåº¦å’Œç”Ÿæˆè´¨é‡ã€‚ä¸DDPMä¸åŒï¼ŒDDPMæ˜¯åœ¨ä¸€åƒä¸ªæ—¶é—´æ­¥é•¿ä¸Šè®­ç»ƒå›¾åƒå»å™ªå™¨ï¼Œè€ŒFast-DDPMåˆ™ä»…ä½¿ç”¨åä¸ªæ—¶é—´æ­¥é•¿è¿›è¡Œè®­ç»ƒå’Œé‡‡æ ·ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„å…³é”®åœ¨äºå¯¹é½è®­ç»ƒå’Œé‡‡æ ·ç¨‹åºä»¥ä¼˜åŒ–æ—¶é—´æ­¥é•¿çš„åˆ©ç”¨ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸¤ç§å…·æœ‰åä¸ªæ—¶é—´æ­¥é•¿çš„æœ‰æ•ˆå™ªå£°è°ƒåº¦å™¨ï¼šä¸€ç§å…·æœ‰å‡åŒ€æ—¶é—´æ­¥é•¿é‡‡æ ·ï¼Œå¦ä¸€ç§å…·æœ‰éå‡åŒ€é‡‡æ ·ã€‚æˆ‘ä»¬åœ¨ä¸‰é¡¹åŒ»å­¦å›¾åƒåˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­è¯„ä¼°äº†Fast-DDPMï¼šå¤šå›¾åƒè¶…åˆ†è¾¨ç‡ã€å›¾åƒå»å™ªå’Œå›¾åƒåˆ°å›¾åƒè½¬æ¢ã€‚Fast-DDPMåœ¨æ‰€æœ‰ä»»åŠ¡ä¸­éƒ½ä¼˜äºDDPMå’ŒåŸºäºå·ç§¯ç½‘ç»œå’Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œçš„æœ€å…ˆè¿›æ–¹æ³•ã€‚æ­¤å¤–ï¼Œä¸DDPMç›¸æ¯”ï¼ŒFast-DDPMå°†è®­ç»ƒæ—¶é—´ç¼©çŸ­è‡³åŸæ¥çš„0.2å€ï¼Œé‡‡æ ·æ—¶é—´ç¼©çŸ­è‡³åŸæ¥çš„0.01å€ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨ä»¥ä¸‹ç½‘å€å…¬å¼€è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/mirthAI/Fast-DDPM">https://github.com/mirthAI/Fast-DDPM</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.14802v3">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMï¼‰åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸå–å¾—äº†å‰æ‰€æœªæœ‰çš„æˆåŠŸï¼Œä½†åœ¨åŒ»å­¦æˆåƒé¢†åŸŸçš„åº”ç”¨ä»ç„¶æœ‰é™ã€‚è¿™æ˜¯å› ä¸ºåŒ»å­¦å›¾åƒçš„ç»´åº¦é€šå¸¸è¾ƒé«˜ï¼Œä¸ºä¸‰ç»´æˆ–å››ç»´ï¼Œä¸”æ‰©æ•£è¿‡ç¨‹ä¸­éœ€è¦å¤§é‡çš„æ—¶é—´æ­¥éª¤ï¼ˆä¾‹å¦‚1000æ­¥ï¼‰ï¼Œå¯¼è‡´è®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Fast-DDPMæ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½åœ¨è®­ç»ƒé€Ÿåº¦ã€é‡‡æ ·é€Ÿåº¦ç”Ÿæˆè´¨é‡æ–¹é¢åŒæ—¶å®ç°æ”¹è¿›ã€‚ä¸DDPMä¸åŒçš„æ˜¯ï¼ŒFast-DDPMä»…ä½¿ç”¨10ä¸ªæ—¶é—´æ­¥éª¤è¿›è¡Œè®­ç»ƒå’Œé‡‡æ ·ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸¤ç§é«˜æ•ˆå™ªå£°è°ƒåº¦å™¨ï¼Œä¸€ç§é‡‡ç”¨å‡åŒ€æ—¶é—´æ­¥é•¿é‡‡æ ·ï¼Œå¦ä¸€ç§é‡‡ç”¨éå‡åŒ€é‡‡æ ·ã€‚åœ¨ä¸‰é¡¹åŒ»å­¦å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­ï¼ŒFast-DDPMè¡¨ç°ä¼˜å¼‚ï¼ŒåŒ…æ‹¬å¤šå›¾åƒè¶…åˆ†è¾¨ç‡ã€å›¾åƒå»å™ªå’Œå›¾åƒåˆ°å›¾åƒçš„è½¬æ¢ã€‚ä¸DDPMå’Œå½“å‰æœ€å…ˆè¿›çš„å·ç§¯ç½‘ç»œå’Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œç›¸æ¯”ï¼ŒFast-DDPMåœ¨æ‰€æœ‰ä»»åŠ¡ä¸­çš„è¡¨ç°å‡æœ‰æ‰€è¶…è¶Šã€‚æ­¤å¤–ï¼ŒFast-DDPMå°†è®­ç»ƒæ—¶é—´ç¼©çŸ­è‡³DDPMçš„0.2å€ï¼Œé‡‡æ ·æ—¶é—´ç¼©çŸ­è‡³DDPMçš„0.01å€ã€‚æˆ‘ä»¬çš„ä»£ç å·²å…¬å¼€å‘å¸ƒåœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/mirthAI/Fast-DDPM%E3%80%82">https://github.com/mirthAI/Fast-DDPMã€‚</a> </p>
<p><strong>è¦ç‚¹æ‘˜è¦</strong></p>
<ol>
<li>DDPMåœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨åŒ»å­¦æˆåƒé¢†åŸŸå› é«˜è®¡ç®—æˆæœ¬è€Œå—åˆ°é™åˆ¶ã€‚</li>
<li>Fast-DDPMæ–¹æ³•æ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ï¼Œé€šè¿‡ä¼˜åŒ–æ—¶é—´æ­¥é•¿åˆ©ç”¨ï¼Œæ˜¾è‘—æé«˜äº†è®­ç»ƒé€Ÿåº¦å’Œé‡‡æ ·é€Ÿåº¦ã€‚</li>
<li>Fast-DDPMä»…ä½¿ç”¨10ä¸ªæ—¶é—´æ­¥éª¤è¿›è¡Œè®­ç»ƒå’Œé‡‡æ ·ï¼Œå¼•å…¥ä¸¤ç§å™ªå£°è°ƒåº¦å™¨ä»¥å®ç°é«˜æ•ˆé‡‡æ ·ã€‚</li>
<li>åœ¨ä¸‰é¡¹åŒ»å­¦å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­ï¼ŒFast-DDPMè¡¨ç°ä¼˜äºDDPMå’Œå…¶ä»–æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</li>
<li>Fast-DDPMå°†è®­ç»ƒæ—¶é—´å’Œé‡‡æ ·æ—¶é—´éƒ½å¤§å¹…åº¦å‡å°‘ã€‚</li>
<li>å…¬å¼€çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/mirthAI/Fast-DDPM%E4%B8%8A%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/mirthAI/Fast-DDPMä¸Šè®¿é—®ã€‚</a> </li>
<li>Fast-DDPMä¸ºåŒ»å­¦æˆåƒé¢†åŸŸå¸¦æ¥äº†æ–°çš„å¯èƒ½æ€§ï¼Œæœ‰æœ›æ”¹å–„ç–¾ç—…è¯Šæ–­å’Œæ²»ç–—è®¡åˆ’ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.14802">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-66f3f979a11c37d422d7825cf371bd3f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-49dcefa62fbe94e1da22230d4282a004.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b6fe4f49d2628659ff7ddd8f9e5919e0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-491c4c5948d4b29c67ee3f8e573971c4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-da26bd395e39747c5897d0ecd8ca1045.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e5cc024d598392bc76826984d4bab24b.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-23/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-23/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-23/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-85b76ff49610b77e13804b71b66ab455.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-23  Hessian-based lightweight neural network for brain vessel segmentation   on a minimal training dataset
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-23
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-23/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-03bea3cfcc4280abe86b1689b675ddf4.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-23  GOGS High-Fidelity Geometry and Relighting for Glossy Objects via   Gaussian Surfels
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-23
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32306k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
