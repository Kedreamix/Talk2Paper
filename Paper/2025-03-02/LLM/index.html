<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM 方向最新论文已更新，请持续关注 Update in 2025-03-02  Relation Also Knows Rethinking the Recall and Editing of Factual   Associations in Auto-Regressive Transformer Language Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-358197163960f55274f7e9639fcd1709.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-03-02
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-03-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    7.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    30 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-03-02-更新"><a href="#2025-03-02-更新" class="headerlink" title="2025-03-02 更新"></a>2025-03-02 更新</h1><h2 id="Relation-Also-Knows-Rethinking-the-Recall-and-Editing-of-Factual-Associations-in-Auto-Regressive-Transformer-Language-Models"><a href="#Relation-Also-Knows-Rethinking-the-Recall-and-Editing-of-Factual-Associations-in-Auto-Regressive-Transformer-Language-Models" class="headerlink" title="Relation Also Knows: Rethinking the Recall and Editing of Factual   Associations in Auto-Regressive Transformer Language Models"></a>Relation Also Knows: Rethinking the Recall and Editing of Factual   Associations in Auto-Regressive Transformer Language Models</h2><p><strong>Authors:Xiyu Liu, Zhengxiao Liu, Naibin Gu, Zheng Lin, Wanli Ma, Ji Xiang, Weiping Wang</strong></p>
<p>The storage and recall of factual associations in auto-regressive transformer language models (LMs) have drawn a great deal of attention, inspiring knowledge editing by directly modifying the located model weights. Most editing works achieve knowledge editing under the guidance of existing interpretations of knowledge recall that mainly focus on subject knowledge. However, these interpretations are seriously flawed, neglecting relation information and leading to the over-generalizing problem for editing. In this work, we discover a novel relation-focused perspective to interpret the knowledge recall of transformer LMs during inference and apply it on single knowledge editing to avoid over-generalizing. Experimental results on the dataset supplemented with a new R-Specificity criterion demonstrate that our editing approach significantly alleviates over-generalizing while remaining competitive on other criteria, breaking the domination of subject-focused editing for future research. </p>
<blockquote>
<p>自动回归变换器语言模型（LMs）中的事实关联存储和回忆已经引起了广泛关注，这激发了通过直接修改定位模型权重进行知识编辑的灵感。大多数编辑工作是在知识回忆的现有解释指导下完成知识编辑的，这些解释主要集中在主题知识上。然而，这些解释存在严重缺陷，忽略了关系信息，导致编辑过程中出现过度概括的问题。在这项工作中，我们发现了一种新型的关系导向视角来解读推理过程中变换器LMs的知识回忆，并将其应用于单一知识编辑中以避免过度概括。在新增数据集上进行的实验补充了新的R特异性标准（R-Specificity criterion），结果表明我们的编辑方法显著减轻了过度概括的问题，同时在其他标准上仍具有竞争力，打破了以主题为中心的编辑在未来研究中的主导地位。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.15091v3">PDF</a> Accepted by AAAI25</p>
<p><strong>Summary</strong>：<br>本文关注自回归Transformer语言模型中事实关联知识的存储和回忆问题，并指出当前知识编辑方法主要集中在主题知识上，忽略关系信息导致过度泛化问题。本文提出了一种新型的关系聚焦视角来解读Transformer模型在推理过程中的知识回忆，并将其应用于单一知识编辑以避免过度泛化。实验结果表明，该方法在补充数据集和新提出的R-Specificity标准上显著减轻了过度泛化问题，同时在其他标准上仍具有竞争力。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>文章探讨了Transformer语言模型中事实关联知识的存储和回忆问题。</li>
<li>当前知识编辑方法主要关注主题知识，导致过度泛化问题。</li>
<li>文章提出了一种新型的关系聚焦视角来解读Transformer模型在推理过程中的知识回忆。</li>
<li>该方法应用于单一知识编辑，以避免过度泛化。</li>
<li>实验结果表明，该方法在特定数据集和新标准上表现优异，显著减轻了过度泛化问题。</li>
<li>该方法在多个标准上仍具有竞争力，为未来的研究提供了新的方向。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.15091">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-35a4d1ad51671e3e0d6ef20a8411778d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fee26da49c8313c3a8a21f5bd19957cc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e06b9499c65bd09299f422a231a90e5b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f7921d748106a0e5f66f5d9927ee0f1f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-66c99ab4449455656b7406c5c36e6740.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-37119858d85dc16b28481b9945634f7c.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="NV-Embed-Improved-Techniques-for-Training-LLMs-as-Generalist-Embedding-Models"><a href="#NV-Embed-Improved-Techniques-for-Training-LLMs-as-Generalist-Embedding-Models" class="headerlink" title="NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding   Models"></a>NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding   Models</h2><p><strong>Authors:Chankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad Shoeybi, Bryan Catanzaro, Wei Ping</strong></p>
<p>Decoder-only LLM-based embedding models are beginning to outperform BERT or T5-based embedding models in general-purpose text embedding tasks, including dense vector-based retrieval. In this work, we introduce NV-Embed, incorporating architectural designs, training procedures, and curated datasets to significantly enhance the performance of LLM as a versatile embedding model, while maintaining its simplicity and reproducibility. For model architecture, we propose a latent attention layer to obtain pooled embeddings, which consistently improves retrieval and downstream task accuracy compared to mean pooling or using the last <EOS> token embedding from LLMs. To enhance representation learning, we remove the causal attention mask of LLMs during contrastive training. For training algorithm, we introduce a two-stage contrastive instruction-tuning method. It first applies contrastive training with instructions on retrieval datasets, utilizing in-batch negatives and curated hard negative examples. At stage-2, it blends various non-retrieval into instruction tuning, which not only enhances non-retrieval task accuracy but also improves retrieval performance. For training data, we utilize the hard-negative mining, synthetic data generation and existing public available datasets to boost the performance of embedding model. By combining these techniques, our NV-Embed-v1 and NV-Embed-v2 models obtained the No.1 position on the MTEB leaderboard (as of May 24 and August 30, 2024, respectively) across 56 tasks, demonstrating the sustained effectiveness of the proposed methods over time. It also achieved the highest scores in the Long Doc section and the second-highest scores in the QA section of the AIR Benchmark, which covers a range of out-of-domain information retrieval topics beyond those in MTEB. We further provide the analysis of model compression techniques for generalist embedding models. </p>
<blockquote>
<p>基于解码器的LLM嵌入模型开始在通用文本嵌入任务中超越基于BERT或T5的嵌入模型，包括密集向量检索。在这项工作中，我们引入了NV-Embed，结合了架构设计、训练过程和精选数据集，以显著增强LLM作为通用嵌入模型的性能，同时保持其简洁性和可重复性。在模型架构方面，我们提出了一个潜在注意力层来获得池化嵌入，与基于LLM的平均池化或使用最后一个<EOS>令牌嵌入相比，它始终提高了检索和下游任务的准确性。为了增强表示学习，我们在对比训练期间移除了LLM的因果注意力掩码。在训练算法方面，我们引入了一种两阶段的对比指令微调方法。它首先在检索数据集上应用带有指令的对比训练，利用批次内的负样本和精选的硬负样本。在第二阶段，它将各种非检索任务融入指令微调中，这不仅提高了非检索任务的准确性，也提高了检索性能。在训练数据方面，我们利用硬负样本挖掘、合成数据生成和现有的公开可用数据集来提升嵌入模型的性能。通过结合这些技术，我们的NV-Embed-v1和NV-Embed-v2模型在56项任务中位列首位（分别截至2024年5月24日和8月30日），这证明了所提出的方法随着时间的推移具有持续的有效性。此外，它在AIR基准测试中长文档部分得分最高，问答部分得分排名第二，涵盖了超出MTEB范围的跨域信息检索主题。我们还进一步分析了针对通用嵌入模型的模型压缩技术。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.17428v3">PDF</a> ICLR 2025 (Spotlight). We open-source the model at:   <a target="_blank" rel="noopener" href="https://huggingface.co/nvidia/NV-Embed-v2">https://huggingface.co/nvidia/NV-Embed-v2</a></p>
<p><strong>Summary</strong></p>
<p>基于解码器的大型语言模型（LLM）的嵌入模型开始在某些通用的文本嵌入任务中表现优于BERT或T5模型。本研究介绍了NV-Embed，它通过架构设计、训练程序和精选数据集显著提高了LLM作为通用嵌入模型的表现，同时保持了其简单性和可复现性。通过引入潜伏注意力层获取池化嵌入，提出移除LLM中的因果注意力掩码进行对比训练等方法提高了检索和下游任务的准确性。此外，本研究还引入了两阶段的对比指令微调方法。经过硬负样本挖掘、合成数据生成和现有公开数据集的使用，NV-Embed模型在多个任务上取得了领先的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM-based嵌入模型在通用文本嵌入任务上开始超越BERT或T5模型。</li>
<li>NV-Embed通过改进架构设计、训练程序和精选数据集提高了LLM的表现。</li>
<li>潜伏注意力层的引入提高了检索和下游任务的准确性。</li>
<li>对比训练过程中移除了LLM的因果注意力掩码以增强表示学习。</li>
<li>两阶段的对比指令微调方法提高了非检索任务准确性和检索性能。</li>
<li>NV-Embed模型通过硬负样本挖掘、合成数据生成等技术在多个任务上取得领先性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.17428">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-146ed9c90253145b36a5a40094c56c3c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-14863976b1cc28b6b49d14f836b7c6d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-09ae21737cbc0d92b4a9763d60064cfa.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Vikhr-Constructing-a-State-of-the-art-Bilingual-Open-Source-Instruction-Following-Large-Language-Model-for-Russian"><a href="#Vikhr-Constructing-a-State-of-the-art-Bilingual-Open-Source-Instruction-Following-Large-Language-Model-for-Russian" class="headerlink" title="Vikhr: Constructing a State-of-the-art Bilingual Open-Source   Instruction-Following Large Language Model for Russian"></a>Vikhr: Constructing a State-of-the-art Bilingual Open-Source   Instruction-Following Large Language Model for Russian</h2><p><strong>Authors:Aleksandr Nikolich, Konstantin Korolev, Sergei Bratchikov, Igor Kiselev, Artem Shelmanov</strong></p>
<p>There has been a surge in developing various Large Language Models (LLMs). However, text generation for languages other than English often faces significant challenges, including poor generation quality and reduced computational performance due to the disproportionate representation of tokens in the model’s vocabulary. In this work, we address these issues by developing a pipeline for adapting English-oriented pre-trained models to other languages and constructing efficient bilingual LLMs. Using this pipeline, we construct Vikhr, a state-of-the-art bilingual open-source instruction-following LLM designed specifically for the Russian language. “Vikhr” refers to the name of the Mistral LLM series and means a “strong gust of wind.” Unlike previous Russian-language models that typically rely on LoRA adapters on top of English-oriented models, sacrificing performance for lower training costs, Vikhr features an adapted tokenizer vocabulary and undergoes continued pre-training and instruction tuning of all weights. This not only enhances the model’s performance but also significantly improves its computational and contextual efficiency. The remarkable performance of Vikhr across various Russian-language benchmarks can also be attributed to our efforts in expanding instruction datasets and corpora for continued pre-training. Vikhr not only sets a new state of the art among open-source LLMs for Russian but even outperforms some proprietary closed-source models on certain benchmarks. The model weights, instruction sets, and code are publicly available. </p>
<blockquote>
<p>在开发各种大型语言模型（LLM）方面出现了激增。然而，对于非英语语言的文本生成常常面临巨大的挑战，包括生成质量较差和计算性能下降，这是由于模型中词汇的代表性不均衡所导致的。在这项工作中，我们通过开发一个用于适应面向英语的预训练模型以适应其他语言的管道，并构建高效的双语LLM来解决这些问题。使用该管道，我们构建了Vikhr，这是一个最先进的双语开源指令遵循LLM，专门为俄语设计。“Vikhr”指的是Mistral LLM系列的名字，意味着“一阵大风”。不同于通常依赖于英语导向模型的LoRA适配器以降低训练成本为牺牲的先前俄语模型，Vikhr具有适应的标记器词汇表，并经历了所有权重的持续预训练和指令调整。这不仅提高了模型的性能，还大大提高了其计算和上下文效率。Vikhr在多种俄语基准测试中的出色表现也可归功于我们扩大指令数据集和语料库以进行持续预训练的努力。Vikhr不仅在开源LLM中树立了俄语的新标杆，而且在某些基准测试中甚至超越了某些专有闭源模型。模型权重、指令集和代码均已公开可用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.13929v5">PDF</a> Accepted at WMRL @ EMNLP-2024</p>
<p><strong>Summary</strong></p>
<p>本摘要介绍了开发适应非英语语言的大型语言模型（LLM）所面临的挑战，包括生成质量不佳和计算性能下降等问题。为解决这些问题，研究者开发了一种适应英语预训练模型到其他语言的管道，并构建了高效的双语LLM。在此基础上，研究团队构建了面向俄语的双语开源指令遵循LLM——Vikhr。与传统的基于英语模型的适配器不同，Vikhr采用适应的词汇表并继续对所有权重进行预训练和指令微调，从而提高了模型的性能和计算效率。其在俄语基准测试上的卓越表现得益于扩充的指令数据集和持续预训练语料库的努力。Vikhr不仅在开源LLM中树立了俄语的新标杆，而且在某些基准测试上甚至超过了某些专有闭源模型。模型的权重、指令集和代码已公开可用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）在非英语语言生成方面面临挑战，包括生成质量和计算性能问题。</li>
<li>研究者通过开发适应英语预训练模型到其他语言的管道来解决这些问题。</li>
<li>Vikhr是一种面向俄语的先进双语开源指令遵循LLM。</li>
<li>Vikhr采用适应的词汇表并继续对所有权重进行预训练和指令微调，以提高模型的性能和计算效率。</li>
<li>Vikhr在俄语基准测试上表现出卓越的性能，超过了某些专有闭源模型。</li>
<li>Vikhr的模型权重、指令集和代码已公开可用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.13929">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-c990153ee0a50b600aee839648f5b549.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c8a564baa06e970ab5d4c6063e18ffa5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-358197163960f55274f7e9639fcd1709.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6c0cdd04ea6b884428fbb7124e9cca99.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-354542348aaceb7f5964342b618dac8d.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Meta-Prompting-for-AI-Systems"><a href="#Meta-Prompting-for-AI-Systems" class="headerlink" title="Meta Prompting for AI Systems"></a>Meta Prompting for AI Systems</h2><p><strong>Authors:Yifan Zhang, Yang Yuan, Andrew Chi-Chih Yao</strong></p>
<p>We introduce Meta Prompting (MP), a prompting paradigm designed to enhance the utilization of large language models (LLMs) and AI systems in complex problem-solving and data interaction. Grounded in type theory and category theory, Meta Prompting prioritizes structural and syntactical considerations over traditional content-centric methods. In this work, we formally define Meta Prompting, delineate its distinctions from few-shot prompting, and demonstrate its effectiveness across various AI applications. In particular, we show that Meta Prompting can decompose intricate reasoning tasks into simpler sub-problems, thereby improving token efficiency and enabling fairer comparisons with conventional few-shot techniques. Furthermore, we extend this framework to prompting tasks, allowing LLMs to recursively self-generate refined prompts in a metaprogramming-like manner. Empirical evaluations reveal that a Qwen-72B base language model equipped with Meta Prompting-without additional instruction tuning-achieves a PASS@1 accuracy of 46.3% on MATH problems, surpassing a supervised fine-tuned counterpart, 83.5% accuracy on GSM8K, and a 100% success rate on Game of 24 tasks using GPT-4. The code is available at <a target="_blank" rel="noopener" href="https://github.com/meta-prompting/meta-prompting">https://github.com/meta-prompting/meta-prompting</a>. </p>
<blockquote>
<p>我们介绍了Meta Prompting（MP），这是一种旨在提高大型语言模型（LLM）和AI系统在复杂问题解决和数据交互中的利用率的提示范式。Meta Prompting基于类型理论和范畴理论，优先考虑结构和句法因素，而非传统的以内容为中心的方法。在这项工作中，我们正式定义了Meta Prompting，阐述了它与少量样本提示的区别，并在各种AI应用程序中展示了其有效性。我们特别表明，Meta Prompting能够将复杂的推理任务分解成更简单的子问题，从而提高令牌效率，并与传统的少量样本技术进行更公平的比较。此外，我们将此框架扩展到提示任务，使LLM能够以一种类似元编程的方式递归地自我生成精细的提示。实证评估表明，配备Meta Prompting的Qwen-72B基础语言模型，无需额外的指令调整，在MATH问题上的PASS@1准确率达到了46.3%，超越了经过监督精细调整的对手；在GSM8K上的准确率为83.5%，而在Game of 24任务上的成功率为100%，使用的是GPT-4。代码可在<a target="_blank" rel="noopener" href="https://github.com/meta-prompting/meta-prompting%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/meta-prompting/meta-prompting上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11482v7">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了Meta Prompting（MP）这一新的提示范式，旨在提高大型语言模型（LLM）和人工智能系统在复杂问题求解和数据交互中的使用效率。该研究基于类型理论和范畴理论，强调结构和句法因素，超越传统的内容中心方法。文章正式定义了Meta Prompting，阐述了其与few-shot提示的区别，并在各种AI应用程序中证明了其有效性。特别是，Meta Prompting能够分解复杂的推理任务为更简单的子问题，提高了令牌效率，并与传统的few-shot技术进行了更公平的比较。此外，该框架扩展到提示任务中，使LLMs能够以类似元编程的方式递归地自我生成精细提示。实证评估表明，配备Meta Prompting的Qwen-72B基础语言模型在MATH问题上达到46.3%的PASS@1准确率，超越了经监督微调的对标模型；在GSM8K上达到83.5%的准确率；在Game of 24任务中使用GPT-4实现100%的成功率。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Meta Prompting（MP）是一种新型的提示范式，旨在提高大型语言模型（LLM）在复杂问题求解和数据交互中的效率。</li>
<li>Meta Prompting基于类型理论和范畴理论，注重结构和句法因素，与传统的以内容为中心的提示方法有所不同。</li>
<li>Meta Prompting能将复杂的推理任务分解成更简单的子问题，提高令牌效率，并与few-shot技术进行比较。</li>
<li>该框架可扩展到提示任务中，使LLMs能够自我生成精细提示，类似于元编程的方式。</li>
<li>配备Meta Prompting的Qwen-72B模型在MATH问题上表现优异，达到46.3%的PASS@1准确率。</li>
<li>在GSM8K测试中，该模型的准确率为83.5%。</li>
<li>在Game of 24任务中，使用GPT-4的模型实现了100%的成功率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2311.11482">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-c2473965861ec2e18f1030b7e87631a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2936a2aa278dd76f575a31126fdb7d2d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e58a244970242c50e89c2e6e57992a96.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Recommendations-by-Concise-User-Profiles-from-Review-Text"><a href="#Recommendations-by-Concise-User-Profiles-from-Review-Text" class="headerlink" title="Recommendations by Concise User Profiles from Review Text"></a>Recommendations by Concise User Profiles from Review Text</h2><p><strong>Authors:Ghazaleh Haratinezhad Torbati, Anna Tigunova, Andrew Yates, Gerhard Weikum</strong></p>
<p>Recommender systems perform well for popular items and users with ample interactions (likes, ratings etc.). This work addresses the difficult and underexplored case of users who have very sparse interactions but post informative review texts. This setting naturally calls for encoding user-specific text with large language models (LLM). However, feeding the full text of all reviews through an LLM has a weak signal-to-noise ratio and incurs high costs of processed tokens. This paper addresses these two issues. It presents a light-weight framework, called CUP, which first computes concise user profiles and feeds only these into the training of transformer-based recommenders. For user profiles, we devise various techniques to select the most informative cues from noisy reviews. Experiments, with book reviews data, show that fine-tuning a small language model with judiciously constructed profiles achieves the best performance, even in comparison to LLM-generated rankings. </p>
<blockquote>
<p>推荐系统对于流行物品和拥有大量互动（如喜欢、评分等）的用户表现良好。这项工作解决了用户互动很少但发布的信息丰富的评论文本这一困难且未被充分研究的案例。这种情况自然地要求使用大型语言模型（LLM）对特定用户的文本进行编码。然而，将所有评论的完整文本通过LLM处理，信号与噪声之比很低，并且会产生高昂的处理令牌成本。本论文解决了这两个问题。它提出了一个轻量级的框架，称为CUP，该框架首先计算简洁的用户配置文件，并将其仅输入到基于转换器的推荐者的训练中。对于用户配置文件，我们设计了各种技术从嘈杂的评论中选择最具有信息性的线索。使用书评数据的实验表明，使用精心构建的配置文件对小型语言模型进行微调，即使与LLM生成的排名相比，也能实现最佳性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01314v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文关注用户交互稀疏但发布有信息含量的评论文本的情况。针对这种情况，提出了一种轻量级的框架CUP，通过计算简洁的用户配置文件并将其输入到基于转换器的推荐器训练中来解决使用大型语言模型（LLM）编码用户特定文本的问题。实验表明，使用精心构建的用户配置文件微调小型语言模型，即使在大型语言模型生成的排名面前，也能取得最佳性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>推荐系统对于交互丰富的用户和流行项目表现良好，但对于交互稀疏但评论信息丰富的用户面临挑战。</li>
<li>在这种情况下，使用大型语言模型（LLM）编码用户特定文本是必要的。</li>
<li>直接将所有评论文本输入LLM存在信号与噪音比低和令牌处理成本高的问题。</li>
<li>提出的CUP框架通过计算简洁的用户配置文件来解决上述问题。</li>
<li>选择最有信息量的线索来构建用户配置文件是解决噪音评论的关键。</li>
<li>实验表明，使用精心构建的用户配置文件微调小型语言模型可以取得最佳性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2311.01314">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8d8b1016057648df24d307732d45abc0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0d4615eed9b39dddc6f08509fd18a6c1.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Self-Confirming-Transformer-for-Belief-Conditioned-Adaptation-in-Offline-Multi-Agent-Reinforcement-Learning"><a href="#Self-Confirming-Transformer-for-Belief-Conditioned-Adaptation-in-Offline-Multi-Agent-Reinforcement-Learning" class="headerlink" title="Self-Confirming Transformer for Belief-Conditioned Adaptation in Offline   Multi-Agent Reinforcement Learning"></a>Self-Confirming Transformer for Belief-Conditioned Adaptation in Offline   Multi-Agent Reinforcement Learning</h2><p><strong>Authors:Tao Li, Juan Guevara, Xinhong Xie, Quanyan Zhu</strong></p>
<p>Offline reinforcement learning (RL) suffers from the distribution shift between the offline dataset and the online environment. In multi-agent RL (MARL), this distribution shift may arise from the nonstationary opponents in the online testing who display distinct behaviors from those recorded in the offline dataset. Hence, the key to the broader deployment of offline MARL is the online adaptation to nonstationary opponents. Recent advances in foundation models, e.g., large language models, have demonstrated the generalization ability of the transformer, an emerging neural network architecture, in sequence modeling, of which offline RL is a special case. One naturally wonders \textit{whether offline-trained transformer-based RL policies adapt to nonstationary opponents online}. We propose a novel auto-regressive training to equip transformer agents with online adaptability based on the idea of self-augmented pre-conditioning. The transformer agent first learns offline to predict the opponent’s action based on past observations. When deployed online, such a fictitious opponent play, referred to as the belief, is fed back to the transformer, together with other environmental feedback, to generate future actions conditional on the belief. Motivated by self-confirming equilibrium in game theory, the training loss consists of belief consistency loss, requiring the beliefs to match the opponent’s actual actions and best response loss, mandating the agent to behave optimally under the belief. We evaluate the online adaptability of the proposed self-confirming transformer (SCT) in a structured environment, iterated prisoner’s dilemma games, to demonstrate SCT’s belief consistency and equilibrium behaviors as well as more involved multi-particle environments to showcase its superior performance against nonstationary opponents over prior transformers and offline MARL baselines. </p>
<blockquote>
<p>离线强化学习（RL）面临着离线数据集和在线环境之间的分布偏移问题。在多智能体强化学习（MARL）中，这种分布偏移可能源于在线测试中非稳定对手的出现，这些对手的行为与离线数据集中记录的行为截然不同。因此，离线MARL更广泛部署的关键在于对在线非稳定对手的适应。最近的基金模型（如大型语言模型）的进展证明了变压器这种新兴神经网络架构在序列建模中的泛化能力，离线RL是其中的特例。人们自然会好奇“基于离线训练的变压器RL策略是否适应在线的非稳定对手”。我们提出了一种新颖的自动回归训练，基于自我增强预处理的理念，为变压器智能体提供在线适应性。变压器智能体首先离线学习基于过去观察预测对手行动。当在线部署时，这种虚构的对手游戏（称为信念）会反馈给变压器，以及其他环境反馈一起，以信念为条件生成未来行动。受博弈论中自我确认均衡的启发，训练损失包括信念一致性损失，要求信念与对手的实际行动相匹配，以及最佳响应损失，要求智能体在信念下表现出最优行为。我们在结构化环境中评估了所提出自我确认变压器（SCT）的在线适应性，在反复囚徒困境游戏中展示了SCT的信念一致性、均衡行为以及在更复杂的多粒子环境中相对于先前变压器和离线MARL基准的优越性能表现，对抗非稳定对手。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04579v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该文本探讨了离线强化学习（RL）面临的在线数据与离线数据集分布转移的问题。在多智能体强化学习（MARL）中，这种分布转移可能源于在线测试中的非稳定对手，他们的行为与离线数据集记录的行为不同。因此，离线MARL更广泛部署的关键在于在线适应非稳定对手的能力。最近，基础模型的进展，如大型语言模型，显示了transformer在序列建模中的泛化能力，离线RL是其中的特例。本文提出了基于自增强预处理的自回归训练，为transformer智能体提供在线适应性。该训练损失包括信念一致性损失和最佳响应损失，要求信念与对手的实际行动相匹配，以及智能体在信念下的行为要最优。在结构化环境、重复的囚徒困境游戏以及更复杂的多粒子环境中评估了所提出的自确认transformer（SCT）的在线适应性，展示了SCT在应对非稳定对手时的优越性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>离线强化学习（RL）面临在线数据与离线数据集分布转移的问题。</li>
<li>在多智能体强化学习（MARL）中，非稳定对手导致的分布转移是一个重要挑战。</li>
<li>Transformer模型在序列建模中展现出强大的泛化能力。</li>
<li>提出了基于自增强预处理的自回归训练，以增强transformer智能体的在线适应性。</li>
<li>训练损失包括信念一致性损失和最佳响应损失，确保智能体能够适应非稳定对手的行为。</li>
<li>在结构化环境和多粒子环境中评估了自确认transformer（SCT）的在线适应性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2310.04579">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ffeac17dfbd0894d12b91c608abfa61a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-954a343681f8762b4a2eebb802238d6c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad597789e9ad45000583463d6f458346.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-408e1499c6bbf9f72f0d0bf496110acb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7948e438ff2acd9f68ea46e537fa2be2.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Which-Transformer-to-Favor-A-Comparative-Analysis-of-Efficiency-in-Vision-Transformers"><a href="#Which-Transformer-to-Favor-A-Comparative-Analysis-of-Efficiency-in-Vision-Transformers" class="headerlink" title="Which Transformer to Favor: A Comparative Analysis of Efficiency in   Vision Transformers"></a>Which Transformer to Favor: A Comparative Analysis of Efficiency in   Vision Transformers</h2><p><strong>Authors:Tobias Christian Nauen, Sebastian Palacio, Federico Raue, Andreas Dengel</strong></p>
<p>Self-attention in Transformers comes with a high computational cost because of their quadratic computational complexity, but their effectiveness in addressing problems in language and vision has sparked extensive research aimed at enhancing their efficiency. However, diverse experimental conditions, spanning multiple input domains, prevent a fair comparison based solely on reported results, posing challenges for model selection. To address this gap in comparability, we perform a large-scale benchmark of more than 45 models for image classification, evaluating key efficiency aspects, including accuracy, speed, and memory usage. Our benchmark provides a standardized baseline for efficiency-oriented transformers. We analyze the results based on the Pareto front – the boundary of optimal models. Surprisingly, despite claims of other models being more efficient, ViT remains Pareto optimal across multiple metrics. We observe that hybrid attention-CNN models exhibit remarkable inference memory- and parameter-efficiency. Moreover, our benchmark shows that using a larger model in general is more efficient than using higher resolution images. Thanks to our holistic evaluation, we provide a centralized resource for practitioners and researchers, facilitating informed decisions when selecting or developing efficient transformers. </p>
<blockquote>
<p>Transformer中的自注意力机制由于二次计算复杂度而带来了较高的计算成本，但其在语言和视觉问题处理中的有效性引发了旨在提高其效率的广泛研究。然而，跨越多个输入域的多种实验条件仅基于报告结果而无法进行公平比较，这给模型选择带来了挑战。为了解决可比性方面的这一差距，我们对超过45个模型进行了大规模图像分类基准测试，评估了包括准确性、速度和内存使用在内的关键效率方面。我们的基准测试为面向效率的变压器提供了标准化基线。我们根据帕累托前沿（最优模型边界）分析结果。令人惊讶的是，尽管有其他模型效率更高的说法，但ViT在多个指标上仍然是帕累托最优。我们发现，混合注意力CNN模型在推理内存和参数效率方面表现出色。此外，我们的基准测试表明，一般来说，使用较大的模型比使用更高分辨率的图像更加高效。由于我们进行了全面评估，因此为实践者和研究人员提供了一个集中资源，有助于他们在选择或开发高效变压器时做出明智的决策。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09372v4">PDF</a> v3: new models, analysis of scaling behaviors; v4: WACV 2025 camera   ready version, appendix added</p>
<p><strong>Summary</strong></p>
<p>本文探讨了Transformer中的自注意力机制虽然计算成本高，但其处理语言和视觉问题的有效性引发了广泛的研究以提高其效率。由于不同输入领域的实验条件差异，仅基于报告结果难以进行公平比较，给模型选择带来挑战。为此，我们对超过45种模型进行了大规模图像分类基准测试，评估了准确性、速度和内存使用等关键效率方面。基准测试提供了面向效率的变压器标准化基线。根据帕累托最优模型的分析，令人惊讶的是，尽管有其他模型声称效率更高，但ViT在多个指标上仍是帕累托最优。我们的基准测试显示，使用较大的模型通常比在更高分辨率的图像上工作更为高效。我们的整体评估为实践者和研究者提供了集中资源，有助于在选择或开发高效变压器时做出明智的决策。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformer中的自注意力机制具有高的计算成本，但其在语言和视觉问题处理中的有效性引发了效率提升的研究。</li>
<li>不同输入领域的实验条件差异导致难以公平比较模型性能，为模型选择带来挑战。</li>
<li>对超过45种模型进行大规模图像分类基准测试，包括关键效率方面的评估，如准确性、速度和内存使用。</li>
<li>基准测试提供了面向效率的变压器的标准化基线。</li>
<li>ViT在多个效率指标上表现优异，成为帕累托最优模型。</li>
<li>杂交注意力-CNN模型在推理内存和参数效率方面表现出色。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2308.09372">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-9b9b06c8598e3ec737fe90506bfb5a5b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-91b390aa9d7b33e0e879e8f413e94881.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ce0aaf8ac953cc4684d7f2415da248bb.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-02/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-02/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-02/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-e2092ad58bfbfc3cc1ba032c97a32bc5.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-03-02  Future-Proofing Class-Incremental Learning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-02
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-01/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-fe000b5a090c30db09aa2742c15cbb91.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2025-03-01  InsTaG Learning Personalized 3D Talking Head from Few-Second Video
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-01
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">16190.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
