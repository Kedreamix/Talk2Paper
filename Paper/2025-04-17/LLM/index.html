<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-17  TextArena">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-42543d31f9f13eed5c771f66a7161d60.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-17
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-26
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    81 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-17-æ›´æ–°"><a href="#2025-04-17-æ›´æ–°" class="headerlink" title="2025-04-17 æ›´æ–°"></a>2025-04-17 æ›´æ–°</h1><h2 id="TextArena"><a href="#TextArena" class="headerlink" title="TextArena"></a>TextArena</h2><p><strong>Authors:Leon Guertler, Bobby Cheng, Simon Yu, Bo Liu, Leshem Choshen, Cheston Tan</strong></p>
<p>TextArena is an open-source collection of competitive text-based games for training and evaluation of agentic behavior in Large Language Models (LLMs). It spans 57+ unique environments (including single-player, two-player, and multi-player setups) and allows for easy evaluation of model capabilities via an online-play system (against humans and other submitted models) with real-time TrueSkill scores. Traditional benchmarks rarely assess dynamic social skills such as negotiation, theory of mind, and deception, creating a gap that TextArena addresses. Designed with research, community and extensibility in mind, TextArena emphasizes ease of adding new games, adapting the framework, testing models, playing against the models, and training models. Detailed documentation of environments, games, leaderboard, and examples are available on <a target="_blank" rel="noopener" href="https://github.com/LeonGuertler/TextArena">https://github.com/LeonGuertler/TextArena</a> and <a target="_blank" rel="noopener" href="https://www.textarena.ai/">https://www.textarena.ai/</a>. </p>
<blockquote>
<p>TextArenaæ˜¯ä¸€ä¸ªåŸºäºæ–‡æœ¬çš„æ¸¸æˆé›†åˆï¼Œç”¨äºè®­ç»ƒå’Œè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„æ™ºèƒ½è¡Œä¸ºã€‚å®ƒæ¶µç›–57ä¸ªä»¥ä¸Šçš„ç‹¬ç‰¹ç¯å¢ƒï¼ˆåŒ…æ‹¬å•äººã€ä¸¤äººå’Œå¤šäººçš„è®¾ç½®ï¼‰ï¼Œå¹¶é€šè¿‡åœ¨çº¿æ¸¸æˆç³»ç»Ÿï¼ˆä¸äººç±»å’Œå…¶ä»–æäº¤æ¨¡å‹å¯¹æŠ—ï¼‰è¿›è¡Œå®æ—¶TrueSkillè¯„åˆ†ï¼Œå¯ä»¥è½»æ¾åœ°è¯„ä¼°æ¨¡å‹çš„èƒ½åŠ›ã€‚ä¼ ç»ŸåŸºå‡†æµ‹è¯•å¾ˆå°‘è¯„ä¼°è¯¸å¦‚è°ˆåˆ¤ã€å¿ƒæ™ºç†è®ºå’Œæ¬ºéª—ç­‰åŠ¨æ€ç¤¾äº¤æŠ€èƒ½ï¼Œä»è€Œç•™ä¸‹äº†ç©ºç™½ï¼Œè€ŒTextArenaæ­£æ˜¯ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜è€Œè®¾è®¡çš„ã€‚æœ¬ç€ç ”ç©¶ã€ç¤¾åŒºå’Œå¯æ‰©å±•æ€§çš„åŸåˆ™ï¼ŒTextArenaå¼ºè°ƒæ˜“äºæ·»åŠ æ–°æ¸¸æˆã€é€‚åº”æ¡†æ¶ã€æµ‹è¯•æ¨¡å‹ã€ä¸æ¨¡å‹å¯¹æˆ˜å’Œè®­ç»ƒæ¨¡å‹ç­‰ç‰¹ç‚¹ã€‚å…³äºç¯å¢ƒã€æ¸¸æˆã€æ’è¡Œæ¦œå’Œç¤ºä¾‹çš„è¯¦ç»†æ–‡æ¡£å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/LeonGuertler/TextArena%E5%92%8Chttps://www.textarena.ai/%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/LeonGuertler/TextArenaå’Œhttps://www.textarena.ai/ä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11442v1">PDF</a> work in progress; 5 pages, 3 figures</p>
<p><strong>Summary</strong><br>     æ–‡æœ¬Arenaæ˜¯ä¸€ä¸ªé¢å‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¼€æºæ–‡æœ¬æ¸¸æˆé›†åˆï¼Œæ—¨åœ¨è®­ç»ƒå’Œè¯„ä¼°ä»£ç†è¡Œä¸ºã€‚å®ƒåŒ…å«è¶…è¿‡57ä¸ªç‹¬ç‰¹çš„ç¯å¢ƒï¼Œæä¾›åœ¨çº¿æ¸¸æˆç³»ç»Ÿï¼Œä¾¿äºè¯„ä¼°æ¨¡å‹èƒ½åŠ›å¹¶ä¸äººç±»å’Œå…¶ä»–æäº¤æ¨¡å‹è¿›è¡Œå®æ—¶TrueSkillè¯„åˆ†ã€‚å®ƒè§£å†³äº†ä¼ ç»ŸåŸºå‡†æµ‹è¯•å¾ˆå°‘è¯„ä¼°çš„åŠ¨æ€ç¤¾äº¤æŠ€èƒ½é—®é¢˜ï¼Œå¦‚è°ˆåˆ¤ã€å¿ƒæ™ºç†è®ºå’Œæ¬ºéª—ç­‰ã€‚è®¾è®¡æ³¨é‡ç ”ç©¶ã€ç¤¾åŒºå’Œå¯æ‰©å±•æ€§ï¼Œæ˜“äºæ·»åŠ æ–°æ¸¸æˆã€é€‚åº”æ¡†æ¶ã€æµ‹è¯•æ¨¡å‹ã€ä¸æ¨¡å‹äº’åŠ¨å’Œè®­ç»ƒæ¨¡å‹ã€‚æ›´å¤šä¿¡æ¯è¯¦è§ç›¸å…³ç½‘ç«™ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TextArenaæ˜¯ä¸€ä¸ªé’ˆå¯¹LLMçš„å¼€æºæ–‡æœ¬æ¸¸æˆé›†åˆï¼Œä¸“æ³¨äºè®­ç»ƒå’Œè¯„ä¼°ä»£ç†è¡Œä¸ºã€‚</li>
<li>å®ƒåŒ…å«å¤šç§ç¯å¢ƒï¼ŒåŒ…æ‹¬å•äººã€ä¸¤äººå’Œå¤šç©å®¶è®¾ç½®ã€‚</li>
<li>TextArenaæä¾›äº†åœ¨çº¿æ¸¸æˆç³»ç»Ÿï¼Œå¯è½»æ¾è¯„ä¼°æ¨¡å‹èƒ½åŠ›ï¼Œå¹¶ä¸äººç±»å’Œå…¶ä»–æ¨¡å‹è¿›è¡Œå®æ—¶TrueSkillè¯„åˆ†ã€‚</li>
<li>å®ƒè§£å†³äº†ä¼ ç»ŸåŸºå‡†æµ‹è¯•æœªå……åˆ†è¯„ä¼°çš„åŠ¨æ€ç¤¾äº¤æŠ€èƒ½ï¼Œå¦‚è°ˆåˆ¤ã€å¿ƒæ™ºç†è®ºå’Œæ¬ºéª—ç­‰ã€‚</li>
<li>TextArenaè®¾è®¡æ³¨é‡ç ”ç©¶ã€ç¤¾åŒºå’Œå¯æ‰©å±•æ€§ï¼Œæ˜“äºä½¿ç”¨ã€‚</li>
<li>è¯¥å¹³å°æä¾›äº†è¯¦ç»†çš„æ–‡æ¡£å’Œç¯å¢ƒç¤ºä¾‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11442">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b036cdd247c4b910d03a7eca2e9355ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d14bd1a8696347fcda5f13ed7bc2417b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ed31026652e010dc60d30259bbf8c56c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-89e6b77645d6f51ce19089bd6923095e.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="A-Dual-Space-Framework-for-General-Knowledge-Distillation-of-Large-Language-Models"><a href="#A-Dual-Space-Framework-for-General-Knowledge-Distillation-of-Large-Language-Models" class="headerlink" title="A Dual-Space Framework for General Knowledge Distillation of Large   Language Models"></a>A Dual-Space Framework for General Knowledge Distillation of Large   Language Models</h2><p><strong>Authors:Xue Zhang, Songming Zhang, Yunlong Liang, Fandong Meng, Yufeng Chen, Jinan Xu, Jie Zhou</strong></p>
<p>Knowledge distillation (KD) is a promising solution to compress large language models (LLMs) by transferring their knowledge to smaller models. During this process, white-box KD methods usually minimize the distance between the output distributions of the teacher model and the student model to transfer more information. However, we reveal that the current white-box KD framework exhibits two limitations: a) bridging probability distributions from different output spaces will limit the similarity between the teacher model and the student model; b) this framework cannot be applied to LLMs with different vocabularies. One of the root causes for these limitations is that the distributions from the teacher and the student for KD are output by different prediction heads, which yield distributions in different output spaces and dimensions. Therefore, in this paper, we propose a dual-space knowledge distillation (DSKD) framework that unifies the prediction heads of the teacher and the student models for KD. Specifically, we first introduce two projectors with ideal initialization to project the teacher&#x2F;student hidden states into the student&#x2F;teacher representation spaces. After this, the hidden states from different models can share the same head and unify the output spaces of the distributions. Furthermore, we develop an exact token alignment (ETA) algorithm to align the same tokens in two differently-tokenized sequences. Based on the above, our DSKD framework is a general KD framework that supports both off-policy and on-policy KD, and KD between any two LLMs regardless of their vocabularies. Extensive experiments on instruction-following, mathematical reasoning, and code generation benchmarks show that DSKD significantly outperforms existing methods based on the current white-box KD framework and surpasses other cross-tokenizer KD methods for LLMs with different vocabularies. </p>
<blockquote>
<p>çŸ¥è¯†è’¸é¦ï¼ˆKDï¼‰æ˜¯ä¸€ç§æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„çŸ¥è¯†è½¬ç§»åˆ°å°å‹æ¨¡å‹æ¥å‹ç¼©å®ƒä»¬ã€‚åœ¨æ­¤è¿‡ç¨‹ä¸­ï¼Œç™½ç›’KDæ–¹æ³•é€šå¸¸é€šè¿‡æœ€å°åŒ–æ•™å¸ˆæ¨¡å‹å’Œå­¦ç”Ÿæ¨¡å‹çš„è¾“å‡ºåˆ†å¸ƒä¹‹é—´çš„è·ç¦»æ¥è½¬ç§»æ›´å¤šä¿¡æ¯ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬æ­ç¤ºäº†å½“å‰ç™½ç›’KDæ¡†æ¶çš„ä¸¤ä¸ªå±€é™æ€§ï¼šaï¼‰ä»ä¸åŒè¾“å‡ºç©ºé—´æ„å»ºæ¦‚ç‡åˆ†å¸ƒä¼šé™åˆ¶æ•™å¸ˆæ¨¡å‹å’Œå­¦ç”Ÿæ¨¡å‹ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼›bï¼‰è¯¥æ¡†æ¶æ— æ³•åº”ç”¨äºå…·æœ‰ä¸åŒè¯æ±‡è¡¨çš„LLMã€‚è¿™äº›å±€é™æ€§çš„ä¸€ä¸ªæ ¹æœ¬åŸå› æ˜¯ï¼Œç”¨äºKDçš„æ•™å¸ˆå’Œå­¦ç”Ÿæ¨¡å‹è¾“å‡ºçš„åˆ†å¸ƒæ¥è‡ªä¸åŒçš„é¢„æµ‹å¤´ï¼Œä»è€Œäº§ç”Ÿä¸åŒçš„è¾“å‡ºç©ºé—´å’Œç»´åº¦ã€‚å› æ­¤ï¼Œæœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€æ•™å¸ˆæ¨¡å‹å’Œå­¦ç”Ÿæ¨¡å‹é¢„æµ‹å¤´çš„åŒç©ºé—´çŸ¥è¯†è’¸é¦ï¼ˆDSKDï¼‰æ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆå¼•å…¥ä¸¤ä¸ªå…·æœ‰ç†æƒ³åˆå§‹åŒ–çš„æŠ•å½±å™¨ï¼Œå°†æ•™å¸ˆ&#x2F;å­¦ç”Ÿçš„éšè—çŠ¶æ€æŠ•å½±åˆ°å­¦ç”Ÿ&#x2F;æ•™å¸ˆçš„è¡¨ç¤ºç©ºé—´ã€‚æ­¤åï¼Œæ¥è‡ªä¸åŒæ¨¡å‹çš„éšè—çŠ¶æ€å¯ä»¥å…±äº«åŒä¸€ä¸ªå¤´éƒ¨å¹¶ç»Ÿä¸€åˆ†å¸ƒçš„è¾“ç©ºé—´ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§ç²¾ç¡®ä»¤ç‰Œå¯¹é½ï¼ˆETAï¼‰ç®—æ³•æ¥å¯¹é½ä¸¤ä¸ªä¸åŒæ ‡è®°çš„åºåˆ—ä¸­çš„ç›¸åŒä»¤ç‰Œã€‚åŸºäºä»¥ä¸Šå†…å®¹ï¼Œæˆ‘ä»¬çš„DSKDæ¡†æ¶æ˜¯ä¸€ä¸ªé€šç”¨KDæ¡†æ¶ï¼Œæ”¯æŒç¦»çº¿ç­–ç•¥KDå’Œåœ¨çº¿ç­–ç•¥KDï¼Œä»¥åŠä»»ä½•ä¸¤ä¸ªLLMä¹‹é—´çš„KDï¼Œæ— è®ºå…¶è¯æ±‡è¡¨å¦‚ä½•ã€‚åœ¨æŒ‡ä»¤éµå¾ªã€æ•°å­¦æ¨ç†å’Œä»£ç ç”ŸæˆåŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDSKDæ˜¾è‘—ä¼˜äºåŸºäºå½“å‰ç™½ç›’KDæ¡†æ¶çš„æ–¹æ³•å’Œå…¶ä»–ç”¨äºå…·æœ‰ä¸åŒè¯æ±‡è¡¨çš„LLMçš„è·¨ä»¤ç‰Œå™¨KDæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11426v1">PDF</a> 19 pages, 9 figures, 11 tables, under review. Code is available at:   <a target="_blank" rel="noopener" href="https://github.com/songmzhang/DSKDv2">https://github.com/songmzhang/DSKDv2</a>. arXiv admin note: text overlap with   arXiv:2406.17328</p>
<p><strong>æ‘˜è¦</strong></p>
<p>çŸ¥è¯†è’¸é¦ï¼ˆKDï¼‰æ˜¯ä¸€ç§å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„çŸ¥è¯†è½¬ç§»åˆ°å°å‹æ¨¡å‹ä¸Šçš„æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆã€‚å½“å‰çš„ç™½ç›’KDæ–¹æ³•é€šå¸¸é€šè¿‡æœ€å°åŒ–æ•™å¸ˆæ¨¡å‹å’Œå­¦ç”Ÿæ¨¡å‹è¾“å‡ºåˆ†å¸ƒä¹‹é—´çš„è·ç¦»æ¥ä¼ é€’æ›´å¤šä¿¡æ¯ã€‚ç„¶è€Œï¼Œæœ¬æ–‡æ­ç¤ºäº†å½“å‰ç™½ç›’KDæ¡†æ¶çš„ä¸¤ä¸ªå±€é™æ€§ï¼šaï¼‰æ¡¥æ¥ä¸åŒè¾“å‡ºç©ºé—´çš„æ¦‚ç‡åˆ†å¸ƒå°†é™åˆ¶æ•™å¸ˆæ¨¡å‹å’Œå­¦ç”Ÿæ¨¡å‹ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼›bï¼‰è¯¥æ¡†æ¶ä¸èƒ½åº”ç”¨äºå…·æœ‰ä¸åŒè¯æ±‡è¡¨çš„LLMã€‚æœ¬æ–‡æå‡ºä¸€ç§åŒç©ºé—´çŸ¥è¯†è’¸é¦ï¼ˆDSKDï¼‰æ¡†æ¶ï¼Œé€šè¿‡ç»Ÿä¸€æ•™å¸ˆæ¨¡å‹å’Œå­¦ç”Ÿæ¨¡å‹çš„é¢„æµ‹å¤´æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆå¼•å…¥ä¸¤ä¸ªç†æƒ³åˆå§‹åŒ–çš„æŠ•å½±ä»ªï¼Œå°†å­¦ç”Ÿ&#x2F;æ•™å¸ˆçš„éšè—çŠ¶æ€æŠ•å½±åˆ°æ•™å¸ˆ&#x2F;å­¦ç”Ÿçš„è¡¨ç¤ºç©ºé—´ä¸­ã€‚ä¹‹åï¼Œæ¥è‡ªä¸åŒæ¨¡å‹çš„éšè—çŠ¶æ€å¯ä»¥ä½¿ç”¨ç›¸åŒçš„å¤´éƒ¨å¹¶ç»Ÿä¸€è¾“å‡ºç©ºé—´çš„åˆ†å¸ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§ç²¾ç¡®çš„ä»¤ç‰Œå¯¹é½ï¼ˆETAï¼‰ç®—æ³•ï¼Œä»¥å¯¹é½ä¸¤ä¸ªä¸åŒæ ‡è®°çš„åºåˆ—ä¸­çš„ç›¸åŒä»¤ç‰Œã€‚åŸºäºä»¥ä¸Šå†…å®¹ï¼Œæˆ‘ä»¬çš„DSKDæ¡†æ¶æ˜¯ä¸€ä¸ªé€šç”¨çš„KDæ¡†æ¶ï¼Œæ”¯æŒç¦»ç­–ç•¥å’Œå³ç­–ç•¥KDï¼Œä»¥åŠä»»ä½•ä¸¤ä¸ªLLMä¹‹é—´çš„KDï¼Œæ— è®ºå…¶è¯æ±‡è¡¨å¦‚ä½•ã€‚åœ¨æŒ‡ä»¤éµå¾ªã€æ•°å­¦æ¨ç†å’Œä»£ç ç”ŸæˆåŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDSKDæ˜¾è‘—ä¼˜äºåŸºäºå½“å‰ç™½ç›’KDæ¡†æ¶çš„æ–¹æ³•ï¼Œå¹¶åœ¨å…·æœ‰ä¸åŒè¯æ±‡è¡¨çš„LLMçš„è·¨ä»¤ç‰ŒåŒ–KDæ–¹æ³•ä¸­è¡¨ç°ä¼˜è¶Šã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>çŸ¥è¯†è’¸é¦ï¼ˆKDï¼‰æ˜¯ä¸€ç§ç”¨äºå‹ç¼©å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–¹æ³•ï¼Œå¯é€šè¿‡å°†çŸ¥è¯†ä»å¤§å‹æ¨¡å‹è½¬ç§»åˆ°å°å‹æ¨¡å‹æ¥å®ç°ã€‚</li>
<li>å½“å‰çš„ç™½ç›’KDæ–¹æ³•å­˜åœ¨å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†ä¸åŒè¾“å‡ºç©ºé—´å’Œè¯æ±‡è¡¨çš„LLMæ—¶ã€‚</li>
<li>åŒç©ºé—´çŸ¥è¯†è’¸é¦ï¼ˆDSKDï¼‰æ¡†æ¶è¢«æå‡ºä»¥è§£å†³è¿™äº›é—®é¢˜ï¼Œé€šè¿‡ç»Ÿä¸€æ•™å¸ˆæ¨¡å‹å’Œå­¦ç”Ÿæ¨¡å‹çš„é¢„æµ‹å¤´ã€‚</li>
<li>DSKDä½¿ç”¨æŠ•å½±ä»ªæ¥è½¬æ¢æ•™å¸ˆæˆ–å­¦ç”Ÿçš„éšè—çŠ¶æ€ï¼Œä»¥ä¾¿å®ƒä»¬å¯ä»¥åœ¨åŒä¸€è¾“å‡ºç©ºé—´ä¸­æ¯”è¾ƒã€‚</li>
<li>DSKDå¼•å…¥äº†ä¸€ç§ç²¾ç¡®çš„ä»¤ç‰Œå¯¹é½ï¼ˆETAï¼‰ç®—æ³•ï¼Œä»¥å¤„ç†ä¸åŒæ ‡è®°çš„åºåˆ—ä¸­çš„ä»¤ç‰Œå¯¹é½é—®é¢˜ã€‚</li>
<li>DSKDæ¡†æ¶æ”¯æŒç¦»ç­–ç•¥å’Œå³ç­–ç•¥KDï¼Œä»¥åŠä»»ä½•ä¸¤ä¸ªLLMä¹‹é—´çš„KDï¼Œä¸å—è¯æ±‡è¡¨é™åˆ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11426">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-45c10d8ecd2985beb1d24f772dbc139c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f1e3ec56d64ad3ba238062ace5b5e34.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de919d1ea01854024b11e73266a32398.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-27424f862f55d43366e2a798d269acea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f82031493dbdaec03c577143a43c1adf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e39b4716e9bb6cb5bb320b89adb6ad18.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Reinforcing-Compositional-Retrieval-Retrieving-Step-by-Step-for-Composing-Informative-Contexts"><a href="#Reinforcing-Compositional-Retrieval-Retrieving-Step-by-Step-for-Composing-Informative-Contexts" class="headerlink" title="Reinforcing Compositional Retrieval: Retrieving Step-by-Step for   Composing Informative Contexts"></a>Reinforcing Compositional Retrieval: Retrieving Step-by-Step for   Composing Informative Contexts</h2><p><strong>Authors:Quanyu Long, Jianda Chen, Zhengyuan Liu, Nancy F. Chen, Wenya Wang, Sinno Jialin Pan</strong></p>
<p>Large Language Models (LLMs) have demonstrated remarkable capabilities across numerous tasks, yet they often rely on external context to handle complex tasks. While retrieval-augmented frameworks traditionally focus on selecting top-ranked documents in a single pass, many real-world scenarios demand compositional retrieval, where multiple sources must be combined in a coordinated manner. In this work, we propose a tri-encoder sequential retriever that models this process as a Markov Decision Process (MDP), decomposing the probability of retrieving a set of elements into a sequence of conditional probabilities and allowing each retrieval step to be conditioned on previously selected examples. We train the retriever in two stages: first, we efficiently construct supervised sequential data for initial policy training; we then refine the policy to align with the LLMâ€™s preferences using a reward grounded in the structural correspondence of generated programs. Experimental results show that our method consistently and significantly outperforms baselines, underscoring the importance of explicitly modeling inter-example dependencies. These findings highlight the potential of compositional retrieval for tasks requiring multiple pieces of evidence or examples. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šä¸ªä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†å“è¶Šçš„èƒ½åŠ›ï¼Œä½†å®ƒä»¬é€šå¸¸ä¾èµ–äºå¤–éƒ¨ä¸Šä¸‹æ–‡æ¥å¤„ç†å¤æ‚ä»»åŠ¡ã€‚è™½ç„¶æ£€ç´¢å¢å¼ºæ¡†æ¶ä¼ ç»Ÿä¸Šä¾§é‡äºåœ¨å•æ¬¡ä¼ é€’ä¸­é€‰æ‹©æ’åé å‰çš„æ–‡æ¡£ï¼Œä½†è®¸å¤šç°å®ä¸–ç•Œåœºæ™¯éœ€è¦ç»„åˆå¼æ£€ç´¢ï¼Œå…¶ä¸­å¿…é¡»å°†å¤šä¸ªæºä»¥åè°ƒçš„æ–¹å¼è¿›è¡Œç»„åˆã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸‰ç¼–ç å™¨é¡ºåºæ£€ç´¢å™¨ï¼Œå®ƒå°†è¿™ä¸€è¿‡ç¨‹å»ºæ¨¡ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ï¼Œå°†æ£€ç´¢ä¸€ç»„å…ƒç´ çš„æ¦‚ç‡åˆ†è§£ä¸ºä¸€ç³»åˆ—æ¡ä»¶æ¦‚ç‡ï¼Œå¹¶å…è®¸æ¯ä¸ªæ£€ç´¢æ­¥éª¤éƒ½åŸºäºå…ˆå‰é€‰æ‹©çš„ä¾‹å­ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªé˜¶æ®µè®­ç»ƒæ£€ç´¢å™¨ï¼šé¦–å…ˆï¼Œæˆ‘ä»¬æœ‰æ•ˆåœ°æ„å»ºæœ‰ç›‘ç£çš„åºåˆ—æ•°æ®ç”¨äºåˆå§‹ç­–ç•¥è®­ç»ƒï¼›ç„¶åï¼Œæˆ‘ä»¬é€šè¿‡å¯¹ç”Ÿæˆç¨‹åºçš„ç»“æ„å¯¹åº”æ€§è¿›è¡Œå¥–åŠ±ï¼Œå¯¹ç­–ç•¥è¿›è¡Œå¾®è°ƒï¼Œä½¿å…¶ä¸LLMçš„åå¥½ä¿æŒä¸€è‡´ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å§‹ç»ˆä¸”æ˜¾è‘—ä¼˜äºåŸºå‡†æ–¹æ³•ï¼Œå¼ºè°ƒæ˜¾å¼å»ºæ¨¡ç¤ºä¾‹é—´ä¾èµ–å…³ç³»çš„é‡è¦æ€§ã€‚è¿™äº›å‘ç°çªæ˜¾äº†ç»„åˆæ£€ç´¢å¯¹äºéœ€è¦å¤šä¸ªè¯æ®æˆ–ç¤ºä¾‹çš„ä»»åŠ¡çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11420v1">PDF</a> 19 pages, 8 figures</p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶ä¾èµ–äºå¤–éƒ¨ä¸Šä¸‹æ–‡ï¼Œä½†ä¼ ç»Ÿæ£€ç´¢å¢å¼ºæ¡†æ¶ä¸»è¦å…³æ³¨å•ä¸€æ£€ç´¢è¿‡ç¨‹çš„é€‰æ‹©æ–‡æ¡£ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ä¸‰ç¼–ç å™¨é¡ºåºæ£€ç´¢å™¨ï¼Œå°†æ£€ç´¢è¿‡ç¨‹å»ºæ¨¡ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ï¼Œå…è®¸æ¯ä¸ªæ£€ç´¢æ­¥éª¤åŸºäºå…ˆå‰é€‰æ‹©çš„ç¤ºä¾‹è¿›è¡Œæ¡ä»¶åŒ–ï¼Œä»è€Œå®ç°å¤šæºåè°ƒç»„åˆã€‚é€šè¿‡ä¸¤ä¸ªé˜¶æ®µè®­ç»ƒæ£€ç´¢å™¨ï¼šé¦–å…ˆæ„å»ºç›‘ç£åºåˆ—æ•°æ®è¿›è¡Œåˆå§‹ç­–ç•¥è®­ç»ƒï¼Œç„¶åä½¿ç”¨åŸºäºç”Ÿæˆç¨‹åºç»“æ„å¯¹åº”æ€§çš„å¥–åŠ±å¯¹ç­–ç•¥è¿›è¡Œå¾®è°ƒï¼Œä»¥ä¸LLMåå¥½å¯¹é½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å§‹ç»ˆä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œå¼ºè°ƒæ˜ç¡®å»ºæ¨¡å®ä¾‹é—´ä¾èµ–å…³ç³»çš„é‡è¦æ€§ã€‚è¿™äº›å‘ç°å‡¸æ˜¾äº†ç»„åˆæ£€ç´¢å¯¹äºéœ€è¦å¤šä¸ªè¯æ®æˆ–ç¤ºä¾‹çš„ä»»åŠ¡çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLMåœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶ä¾èµ–å¤–éƒ¨ä¸Šä¸‹æ–‡ã€‚</li>
<li>ä¼ ç»Ÿæ£€ç´¢å¢å¼ºæ¡†æ¶ä¸»è¦å…³æ³¨å•ä¸€æ£€ç´¢è¿‡ç¨‹çš„é€‰æ‹©æ–‡æ¡£ï¼Œä½†çœŸå®åœºæ™¯éœ€è¦å¤šæºåè°ƒç»„åˆã€‚</li>
<li>æå‡ºäº†ä¸€ç§ä¸‰ç¼–ç å™¨é¡ºåºæ£€ç´¢å™¨ï¼Œå°†æ£€ç´¢è¿‡ç¨‹å»ºæ¨¡ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ã€‚</li>
<li>è¯¥æ–¹æ³•å…è®¸æ¯ä¸ªæ£€ç´¢æ­¥éª¤åŸºäºå…ˆå‰é€‰æ‹©çš„ç¤ºä¾‹è¿›è¡Œæ¡ä»¶åŒ–ã€‚</li>
<li>é€šè¿‡ä¸¤ä¸ªé˜¶æ®µè®­ç»ƒæ£€ç´¢å™¨ï¼šåˆå§‹ç­–ç•¥è®­ç»ƒå’Œä¸LLMåå¥½å¯¹é½çš„ç­–ç•¥å¾®è°ƒã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œå¼ºè°ƒæ˜ç¡®å»ºæ¨¡å®ä¾‹é—´ä¾èµ–å…³ç³»çš„é‡è¦æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11420">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ef3ba91e258c4b53488da6c99377252c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e7d6cc8db775d3b4057b2902bf964bd0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f6db088bc20f9834af1713c56577d4fc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f2aad415fa0ab6bce4d1866ff6273585.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0e79ea6a305a8bf235e8b7604804489e.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="DataDecide-How-to-Predict-Best-Pretraining-Data-with-Small-Experiments"><a href="#DataDecide-How-to-Predict-Best-Pretraining-Data-with-Small-Experiments" class="headerlink" title="DataDecide: How to Predict Best Pretraining Data with Small Experiments"></a>DataDecide: How to Predict Best Pretraining Data with Small Experiments</h2><p><strong>Authors:Ian Magnusson, Nguyen Tai, Ben Bogin, David Heineman, Jena D. Hwang, Luca Soldaini, Akshita Bhagia, Jiacheng Liu, Dirk Groeneveld, Oyvind Tafjord, Noah A. Smith, Pang Wei Koh, Jesse Dodge</strong></p>
<p>Because large language models are expensive to pretrain on different datasets, using smaller-scale experiments to decide on data is crucial for reducing costs. Which benchmarks and methods of making decisions from observed performance at small scale most accurately predict the datasets that yield the best large models? To empower open exploration of this question, we release models, data, and evaluations in DataDecide â€“ the most extensive open suite of models over differences in data and scale. We conduct controlled pretraining experiments across 25 corpora with differing sources, deduplication, and filtering up to 100B tokens, model sizes up to 1B parameters, and 3 random seeds. We find that the ranking of models at a single, small size (e.g., 150M parameters) is a strong baseline for predicting best models at our larger target scale (1B) (~80% of com parisons correct). No scaling law methods among 8 baselines exceed the compute-decision frontier of single-scale predictions, but DataDecide can measure improvement in future scaling laws. We also identify that using continuous likelihood metrics as proxies in small experiments makes benchmarks including MMLU, ARC, HellaSwag, MBPP, and HumanEval &gt;80% predictable at the target 1B scale with just 0.01% of the compute. </p>
<blockquote>
<p>ç”±äºåœ¨ä¸åŒçš„æ•°æ®é›†ä¸Šé¢„è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹æˆæœ¬é«˜æ˜‚ï¼Œå› æ­¤ä½¿ç”¨å°è§„æ¨¡å®éªŒæ¥å†³å®šæ•°æ®å¯¹é™ä½æˆæœ¬è‡³å…³é‡è¦ã€‚é‚£ä¹ˆï¼Œä»è§‚å¯Ÿåˆ°çš„æ€§èƒ½è¡¨ç°ä¸­åšå‡ºå†³ç­–ï¼Œå“ªäº›åŸºå‡†æµ‹è¯•å’Œæ–¹æ³•èƒ½æœ€å‡†ç¡®åœ°é¢„æµ‹äº§ç”Ÿæœ€ä½³å¤§å‹æ¨¡å‹çš„æ•°æ®é›†ï¼Ÿä¸ºäº†æ¨åŠ¨å¯¹è¿™ä¸€é—®é¢˜çš„å¼€æ”¾æ¢ç´¢ï¼Œæˆ‘ä»¬åœ¨DataDecideä¸­å‘å¸ƒäº†æ¨¡å‹ã€æ•°æ®ä»¥åŠè¯„ä¼°æ–¹æ³•â€”â€”è¿™æ˜¯å…³äºæ•°æ®å’Œè§„æ¨¡å·®å¼‚çš„æœ€å…¨é¢çš„å¼€æ”¾æ¨¡å‹å¥—ä»¶ã€‚æˆ‘ä»¬åœ¨25ä¸ªä¸åŒæ¥æºã€ç»è¿‡å»é‡å’Œè¿‡æ»¤çš„è¯­æ–™åº“ä¸Šè¿›è¡Œäº†å—æ§çš„é¢„è®­ç»ƒå®éªŒï¼Œæ¶‰åŠå¤šè¾¾100Bçš„ä»¤ç‰Œã€é«˜è¾¾1Bå‚æ•°çš„æ¨¡å‹ä»¥åŠ3ä¸ªéšæœºç§å­ã€‚æˆ‘ä»¬å‘ç°ï¼Œå•ä¸€å°å°ºå¯¸ï¼ˆä¾‹å¦‚å«æœ‰150Må‚æ•°çš„æ¨¡å‹ï¼‰çš„æ¨¡å‹æ’åæ˜¯é¢„æµ‹æˆ‘ä»¬åœ¨æ›´å¤§çš„ç›®æ ‡è§„æ¨¡ï¼ˆ1Bï¼‰ä¸Šçš„æœ€ä½³æ¨¡å‹çš„å¼ºå¤§åŸºçº¿ï¼ˆçº¦80%çš„æ¯”è¾ƒæ˜¯æ­£ç¡®çš„ï¼‰ã€‚åœ¨8ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œæ²¡æœ‰æ‰©å±•å®šå¾‹æ–¹æ³•è¶…è¿‡å•ä¸€è§„æ¨¡é¢„æµ‹çš„è®¡ç®—å†³ç­–å‰æ²¿ï¼Œä½†DataDecideå¯ä»¥è¡¡é‡æœªæ¥æ‰©å±•å®šå¾‹çš„æ”¹è¿›æƒ…å†µã€‚æˆ‘ä»¬è¿˜å‘ç°ï¼Œåœ¨å°è§„æ¨¡å®éªŒä¸­é‡‡ç”¨è¿ç»­å¯èƒ½æ€§åº¦é‡ä½œä¸ºä»£ç†æŒ‡æ ‡ï¼Œä½¿å¾—åŒ…æ‹¬MMLUã€ARCã€HellaSwagã€MBPPå’ŒHumanEvalåœ¨å†…çš„åŸºå‡†æµ‹è¯•åœ¨ç›®æ ‡è§„æ¨¡ä¸º1Bæ—¶ä»…ä½¿ç”¨è®¡ç®—é‡çš„ä¸‡åˆ†ä¹‹ä¸€å°±èƒ½è¾¾åˆ°è¶…è¿‡80%çš„å¯é¢„æµ‹æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11393v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ä½¿ç”¨å°è§„æ¨¡å®éªŒæ¥ç­›é€‰å¤§å‹è¯­è¨€æ¨¡å‹çš„é¢„è®­ç»ƒæ•°æ®é›†æ˜¯å…³é”®ï¼Œå¯æœ‰æ•ˆé™ä½æˆæœ¬ã€‚æœ¬æ–‡é€šè¿‡å®éªŒå¯¹æ¯”ï¼Œå‘ç°åœ¨å°è§„æ¨¡æ•°æ®ä¸Šçš„æ€§èƒ½æ’åå¯¹é¢„æµ‹å¤§å‹æ¨¡å‹çš„æ€§èƒ½å…·æœ‰è¾ƒå¼ºçš„å‚è€ƒä»·å€¼ã€‚å¹¶é‡Šæ”¾DataDecideæ•°æ®é›†ä½œä¸ºç ”ç©¶å’Œæ¯”è¾ƒçš„å¼€æ”¾å¹³å°ã€‚é€šè¿‡å¯¹æ¯”å®éªŒå‘ç°ï¼Œåˆ©ç”¨è¿ç»­å¯èƒ½æ€§åº¦é‡ä½œä¸ºå°å®éªŒçš„ä»£ç†æŒ‡æ ‡å¯æœ‰æ•ˆé¢„æµ‹å¤§å‹æ¨¡å‹çš„æ€§èƒ½ã€‚å› æ­¤ä½¿ç”¨é€‚åˆçš„å·¥å…·å’Œæ–¹æ³•èƒ½åœ¨è¾ƒä½çš„è®¡ç®—æˆæœ¬ä¸‹æœ‰æ•ˆåœ°è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚ </p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯è¯¥æ–‡æœ¬çš„å‡ ä¸ªä¸»è¦è§‚ç‚¹å’Œå¯ç¤ºï¼š</p>
<ol>
<li>å¤§è§„æ¨¡çš„è¯­è¨€æ¨¡å‹åœ¨å¤šç§æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒä»£ä»·æ˜‚è´µï¼Œå› æ­¤å°è§„æ¨¡å®éªŒçš„é€‰æ‹©å¯¹äºé™ä½æˆæœ¬è‡³å…³é‡è¦ã€‚</li>
<li>é€šè¿‡å®éªŒå‘ç°ï¼Œå°è§„æ¨¡æ•°æ®ä¸Šçš„æ¨¡å‹æ€§èƒ½æ’åæ˜¯é¢„æµ‹å¤§å‹æ¨¡å‹æ€§èƒ½çš„æœ‰æ•ˆæ–¹æ³•ã€‚</li>
<li>DataDecideæ•°æ®é›†çš„å‘å¸ƒä¸ºç ”ç©¶å’Œæ¯”è¾ƒä¸åŒæ•°æ®å’Œè§„æ¨¡ä¸‹çš„æ¨¡å‹æä¾›äº†å¼€æ”¾å¹³å°ã€‚</li>
<li>åœ¨å°è§„æ¨¡å®éªŒä¸­ï¼Œä½¿ç”¨è¿ç»­å¯èƒ½æ€§åº¦é‡ä½œä¸ºä»£ç†æŒ‡æ ‡èƒ½æœ‰æ•ˆé¢„æµ‹å¤§å‹æ¨¡å‹çš„æ€§èƒ½è¡¨ç°ã€‚ </li>
<li>åœ¨é€‰æ‹©æ•°æ®é›†çš„å†³ç­–è¿‡ç¨‹ä¸­ï¼Œéœ€è¦æƒè¡¡å„ç§å› ç´ å¦‚æ•°æ®æ¥æºã€å»é‡å’Œè¿‡æ»¤ç­‰ã€‚ </li>
<li>æœ¬æ–‡ç ”ç©¶çš„æ¨¡å‹å‚æ•°è§„æ¨¡ä»å°åˆ°å¤§çš„è¿‡ç¨‹æ˜¾ç¤ºå‡ºäº†ä¸€å®šçš„è§„å¾‹ï¼Œå¯¹ä»Šåçš„æ¨¡å‹ç¼©æ”¾ç ”ç©¶æœ‰å‚è€ƒæ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11393">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-87e1bfdd7ecb5bc0326315c0018850f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-68d5ae305aee70833b4f3cee7415d41d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-591a39e94d1d894007f8226bd3c540df.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="OpenTuringBench-An-Open-Model-based-Benchmark-and-Framework-for-Machine-Generated-Text-Detection-and-Attribution"><a href="#OpenTuringBench-An-Open-Model-based-Benchmark-and-Framework-for-Machine-Generated-Text-Detection-and-Attribution" class="headerlink" title="OpenTuringBench: An Open-Model-based Benchmark and Framework for   Machine-Generated Text Detection and Attribution"></a>OpenTuringBench: An Open-Model-based Benchmark and Framework for   Machine-Generated Text Detection and Attribution</h2><p><strong>Authors:Lucio La Cava, Andrea Tagarelli</strong></p>
<p>Open Large Language Models (OLLMs) are increasingly leveraged in generative AI applications, posing new challenges for detecting their outputs. We propose OpenTuringBench, a new benchmark based on OLLMs, designed to train and evaluate machine-generated text detectors on the Turing Test and Authorship Attribution problems. OpenTuringBench focuses on a representative set of OLLMs, and features a number of challenging evaluation tasks, including human&#x2F;machine-manipulated texts, out-of-domain texts, and texts from previously unseen models. We also provide OTBDetector, a contrastive learning framework to detect and attribute OLLM-based machine-generated texts. Results highlight the relevance and varying degrees of difficulty of the OpenTuringBench tasks, with our detector achieving remarkable capabilities across the various tasks and outperforming most existing detectors. Resources are available on the OpenTuringBench Hugging Face repository at <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/MLNTeam-Unical/OpenTuringBench">https://huggingface.co/datasets/MLNTeam-Unical/OpenTuringBench</a> </p>
<blockquote>
<p>å¼€æ”¾å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆOLLMsï¼‰åœ¨ç”Ÿæˆå¼äººå·¥æ™ºèƒ½åº”ç”¨ä¸­çš„ä½¿ç”¨è¶Šæ¥è¶Šå¹¿æ³›ï¼Œè¿™å¯¹å…¶è¾“å‡ºæ£€æµ‹æå‡ºäº†æ–°çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†åŸºäºOLLMsçš„OpenTuringBenchæ–°åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨åœ¨å›¾çµæµ‹è¯•å’Œä½œè€…å½’å±é—®é¢˜ä¸Šè®­ç»ƒå’Œè¯„ä¼°æœºå™¨ç”Ÿæˆçš„æ–‡æœ¬æ£€æµ‹å™¨ã€‚OpenTuringBenchå…³æ³¨OLLMsçš„ä»£è¡¨æ€§é›†åˆï¼Œå¹¶åŒ…å«ä¸€ç³»åˆ—å…·æœ‰æŒ‘æˆ˜æ€§çš„è¯„ä¼°ä»»åŠ¡ï¼ŒåŒ…æ‹¬äººä¸ºæ“æ§æ–‡æœ¬ã€é¢†åŸŸå¤–æ–‡æœ¬ä»¥åŠæ¥è‡ªä¹‹å‰æœªè§æ¨¡å‹çš„æ–‡æœ¬ã€‚æˆ‘ä»¬è¿˜æä¾›äº†OTBDetectorå¯¹æ¯”å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºæ£€æµ‹å¹¶å½’å±OLLMæœºå™¨ç”Ÿæˆçš„æ–‡æœ¬ã€‚ç»“æœçªæ˜¾äº†OpenTuringBenchä»»åŠ¡çš„ç°å®æ„ä¹‰ä»¥åŠä¸åŒéš¾åº¦çš„å±‚æ¬¡ï¼Œæˆ‘ä»¬çš„æ£€æµ‹å™¨åœ¨å„ç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œå¹¶è¶…è¶Šäº†å¤§å¤šæ•°ç°æœ‰æ£€æµ‹å™¨ã€‚èµ„æºå¯åœ¨OpenTuringBench Hugging Faceå­˜å‚¨åº“ä¸­æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/MLNTeam-Unical/OpenTuringBench">https://huggingface.co/datasets/MLNTeam-Unical/OpenTuringBench</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11369v1">PDF</a> Under review with ARR</p>
<p><strong>Summary</strong></p>
<p>OpenTuringBenchæ˜¯ä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åŸºå‡†æµ‹è¯•å¹³å°ï¼Œæ—¨åœ¨è®­ç»ƒå’Œè¯„ä¼°å¯¹æœºå™¨ç”Ÿæˆæ–‡æœ¬çš„è¯†åˆ«èƒ½åŠ›ã€‚è¯¥å¹³å°åŒ…å«ä¸€ç³»åˆ—å…·æœ‰ä»£è¡¨æ€§çš„è¯„ä¼°ä»»åŠ¡ï¼Œå¯ä»¥å¤„ç†åŒ…æ‹¬äººä¸ºæˆ–æœºå™¨æ“æ§æ–‡æœ¬ã€éåŸŸæ–‡æœ¬å’Œä¹‹å‰æœªè§æ¨¡å‹çš„æ–‡æœ¬ç­‰å†…å®¹ã€‚OpenTuringBenchè¿˜æä¾›äº†OTBDetectoræ£€æµ‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç”¨äºæ£€æµ‹å’Œå½’å› LLMåŸºç¡€æœºå™¨ç”Ÿæˆæ–‡æœ¬ã€‚ç°æœ‰ç»“æœè¡¨æ˜OpenTuringBenchä»»åŠ¡çš„å¤šæ ·æ€§å’Œéš¾åº¦ï¼Œå…¶æ£€æµ‹å™¨åœ¨å„ç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›å¹¶è¶…è¶Šäº†å¤§å¤šæ•°ç°æœ‰æ£€æµ‹å™¨ã€‚èµ„æºå¯åœ¨OpenTuringBench Hugging Faceä»“åº“è·å–ã€‚^[åŸºäºæä¾›æ–‡æœ¬çš„ç²¾ç®€æ€»ç»“]^</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>OpenTuringBenchæ˜¯ä¸€ä¸ªé’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åŸºå‡†æµ‹è¯•å¹³å°ã€‚</li>
<li>å®ƒè®¾è®¡ç”¨äºè®­ç»ƒå’Œè¯„ä¼°æœºå™¨ç”Ÿæˆæ–‡æœ¬çš„æ£€æµ‹å™¨ã€‚</li>
<li>å¹³å°åŒ…å«ä¸€ç³»åˆ—è¯„ä¼°ä»»åŠ¡ï¼Œæ¶µç›–äººä¸ºæˆ–æœºå™¨æ“æ§æ–‡æœ¬ã€éåŸŸæ–‡æœ¬å’ŒæœªçŸ¥æ¨¡å‹æ–‡æœ¬ç­‰å†…å®¹ã€‚</li>
<li>OpenTuringBenchæä¾›äº†OTBDetectoræ£€æµ‹æ¡†æ¶æ¥æ£€æµ‹å’Œå½’å› LLMç”Ÿæˆçš„æ–‡æœ¬ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11369">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1ce222c183a92f505072fdaa2ab2d48f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c65cc45d536516a3a959289ea80a39f.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Teaching-Large-Language-Models-to-Reason-through-Learning-and-Forgetting"><a href="#Teaching-Large-Language-Models-to-Reason-through-Learning-and-Forgetting" class="headerlink" title="Teaching Large Language Models to Reason through Learning and Forgetting"></a>Teaching Large Language Models to Reason through Learning and Forgetting</h2><p><strong>Authors:Tianwei Ni, Allen Nie, Sapana Chaudhary, Yao Liu, Huzefa Rangwala, Rasool Fakoor</strong></p>
<p>Leveraging inference-time search in large language models has proven effective in further enhancing a trained modelâ€™s capability to solve complex mathematical and reasoning problems. However, this approach significantly increases computational costs and inference time, as the model must generate and evaluate multiple candidate solutions to identify a viable reasoning path. To address this, we propose an effective approach that integrates search capabilities directly into the model by fine-tuning it using both successful (learning) and failed reasoning paths (forgetting) derived from diverse search methods. While fine-tuning the model with these data might seem straightforward, we identify a critical issue: the modelâ€™s search capability tends to degrade rapidly if fine-tuning is performed naively. We show that this degradation can be substantially mitigated by employing a smaller learning rate. Extensive experiments on the challenging Game-of-24 and Countdown mathematical reasoning benchmarks show that our approach not only outperforms both standard fine-tuning and inference-time search baselines but also significantly reduces inference time by 180$\times$. </p>
<blockquote>
<p>åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„æ¨ç†æ—¶é—´æœç´¢ï¼Œå·²è¢«è¯æ˜å¯ä»¥è¿›ä¸€æ­¥æé«˜è®­ç»ƒæ¨¡å‹è§£å†³å¤æ‚æ•°å­¦å’Œæ¨ç†é—®é¢˜çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•æ˜¾è‘—å¢åŠ äº†è®¡ç®—æˆæœ¬å’Œæ¨ç†æ—¶é—´ï¼Œå› ä¸ºæ¨¡å‹å¿…é¡»ç”Ÿæˆå¹¶è¯„ä¼°å¤šä¸ªå€™é€‰è§£å†³æ–¹æ¡ˆæ¥è¯†åˆ«å¯è¡Œçš„æ¨ç†è·¯å¾„ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å°†æœç´¢èƒ½åŠ›ç›´æ¥é›†æˆåˆ°æ¨¡å‹ä¸­çš„æœ‰æ•ˆæ–¹æ³•ï¼Œé€šè¿‡å¾®è°ƒæ¨¡å‹ï¼Œä½¿ç”¨æ¥è‡ªä¸åŒæœç´¢æ–¹æ³•çš„æˆåŠŸï¼ˆå­¦ä¹ ï¼‰å’Œå¤±è´¥æ¨ç†è·¯å¾„ï¼ˆé—å¿˜ï¼‰ã€‚è™½ç„¶ç”¨è¿™äº›æ•°æ®å¾®è°ƒæ¨¡å‹çœ‹ä¼¼ç®€å•ï¼Œä½†æˆ‘ä»¬å‘ç°äº†ä¸€ä¸ªå…³é”®é—®é¢˜ï¼šå¦‚æœç›²ç›®è¿›è¡Œå¾®è°ƒï¼Œæ¨¡å‹çš„æœç´¢èƒ½åŠ›å¾€å¾€ä¼šè¿…é€Ÿä¸‹é™ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œé€šè¿‡é‡‡ç”¨è¾ƒå°çš„å­¦ä¹ ç‡ï¼Œå¯ä»¥å¤§å¤§å‡è½»è¿™ç§é€€åŒ–ã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„24ç‚¹æ¸¸æˆå’Œå€’è®¡æ—¶æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•çš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…ä¼˜äºæ ‡å‡†å¾®è°ƒæ–¹æ³•å’Œæ¨ç†æ—¶é—´æœç´¢åŸºå‡†æµ‹è¯•ï¼Œè€Œä¸”é€šè¿‡å°†æ¨ç†æ—¶é—´å‡å°‘180å€ï¼Œæ˜¾è‘—å‡å°‘äº†æ¨ç†æ—¶é—´ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11364v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¨ç†æ—¶é—´æœç´¢æ–¹é¢çš„åº”ç”¨å·²è¯æ˜å¯æœ‰æ•ˆæé«˜è®­ç»ƒæ¨¡å‹è§£å†³å¤æ‚æ•°å­¦å’Œæ¨ç†é—®é¢˜çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•æ˜¾è‘—å¢åŠ äº†è®¡ç®—æˆæœ¬å’Œæ¨ç†æ—¶é—´ï¼Œå› ä¸ºæ¨¡å‹å¿…é¡»ç”Ÿæˆå¹¶è¯„ä¼°å¤šä¸ªå€™é€‰è§£å†³æ–¹æ¡ˆæ¥è¯†åˆ«å¯è¡Œçš„æ¨ç†è·¯å¾„ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å°†æœç´¢èƒ½åŠ›ç›´æ¥é›†æˆåˆ°æ¨¡å‹ä¸­çš„æœ‰æ•ˆæ–¹æ³•ï¼Œé€šè¿‡ç²¾ç»†è°ƒæ•´ä»å¤šç§æœç´¢æ–¹æ³•ä¸­è·å¾—çš„å­¦ä¹ æˆåŠŸä¸å¤±è´¥æ¨ç†è·¯å¾„çš„æ•°æ®ã€‚è™½ç„¶ç”¨è¿™äº›æ•°æ®è°ƒæ•´æ¨¡å‹çœ‹ä¼¼ç®€å•ï¼Œä½†æˆ‘ä»¬å‘ç°äº†ä¸€ä¸ªå…³é”®é—®é¢˜ï¼šå¦‚æœç›²ç›®è¿›è¡Œå¾®è°ƒï¼Œæ¨¡å‹çš„æœç´¢èƒ½åŠ›ä¼šè¿…é€Ÿä¸‹é™ã€‚æˆ‘ä»¬å±•ç¤ºé€šè¿‡é‡‡ç”¨è¾ƒå°çš„å­¦ä¹ ç‡å¯ä»¥å¤§å¤§ç¼“è§£è¿™ç§é€€åŒ–é—®é¢˜ã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„Game-of-24å’Œå€’è®¡æ—¶æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…ä¼˜äºæ ‡å‡†å¾®è°ƒæ–¹æ³•å’Œæ¨ç†æ—¶é—´æœç´¢åŸºå‡†æµ‹è¯•ï¼Œè€Œä¸”å°†æ¨ç†æ—¶é—´å‡å°‘äº†180å€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¨ç†æ—¶é—´æœç´¢åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­å¢å¼ºäº†è§£å†³å¤æ‚æ•°å­¦å’Œæ¨ç†é—®é¢˜çš„èƒ½åŠ›ï¼Œä½†å¢åŠ äº†è®¡ç®—æˆæœ¬å’Œæ¨ç†æ—¶é—´ã€‚</li>
<li>é€šè¿‡ç²¾ç»†è°ƒæ•´æ¨¡å‹ï¼Œé›†æˆæœç´¢èƒ½åŠ›å¯ä»¥ç›´æ¥æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>åœ¨ä½¿ç”¨ä¸åŒçš„æœç´¢æ–¹æ³•è·å–æ•°æ®æ—¶ï¼Œæ¨¡å‹è‹¥ç›²ç›®è¿›è¡Œå¾®è°ƒä¼šå¯¼è‡´æœç´¢èƒ½åŠ›è¿…é€Ÿä¸‹é™ã€‚</li>
<li>é‡‡ç”¨è¾ƒå°çš„å­¦ä¹ ç‡å¯ä»¥æœ‰æ•ˆç¼“è§£æ¨¡å‹æœç´¢èƒ½åŠ›çš„é€€åŒ–é—®é¢˜ã€‚</li>
<li>ç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•å’Œæ¨ç†æ—¶é—´æœç´¢ï¼Œæå‡ºçš„æ–¹æ³•åœ¨æ€§èƒ½ä¸Šæ›´èƒœä¸€ç­¹ã€‚</li>
<li>æ‰€æå‡ºçš„æ–¹æ³•æ˜¾è‘—å‡å°‘äº†æ¨ç†æ—¶é—´ï¼Œè¾¾åˆ°äº†180å€çš„é€Ÿåº¦æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11364">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b10d9745584f69750a9624c79a2e8d23.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8b3b7ea3942140105c693bdfabfa785a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-41adb84ed68e74a63a8994dc0aace7f8.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Kimina-Prover-Preview-Towards-Large-Formal-Reasoning-Models-with-Reinforcement-Learning"><a href="#Kimina-Prover-Preview-Towards-Large-Formal-Reasoning-Models-with-Reinforcement-Learning" class="headerlink" title="Kimina-Prover Preview: Towards Large Formal Reasoning Models with   Reinforcement Learning"></a>Kimina-Prover Preview: Towards Large Formal Reasoning Models with   Reinforcement Learning</h2><p><strong>Authors:Haiming Wang, Mert Unsal, Xiaohan Lin, Mantas Baksys, Junqi Liu, Marco Dos Santos, Flood Sung, Marina Vinyes, Zhenzhe Ying, Zekai Zhu, Jianqiao Lu, Hugues de SaxcÃ©, Bolton Bailey, Chendong Song, Chenjun Xiao, Dehao Zhang, Ebony Zhang, Frederick Pu, Han Zhu, Jiawei Liu, Jonas Bayer, Julien Michel, Longhui Yu, LÃ©o Dreyfus-Schmidt, Lewis Tunstall, Luigi Pagani, Moreira Machado, Pauline Bourigault, Ran Wang, Stanislas Polu, Thibaut Barroyer, Wen-Ding Li, Yazhe Niu, Yann Fleureau, Yangyang Hu, Zhouliang Yu, Zihan Wang, Zhilin Yang, Zhengying Liu, Jia Li</strong></p>
<p>We introduce Kimina-Prover Preview, a large language model that pioneers a novel reasoning-driven exploration paradigm for formal theorem proving, as showcased in this preview release. Trained with a large-scale reinforcement learning pipeline from Qwen2.5-72B, Kimina-Prover demonstrates strong performance in Lean 4 proof generation by employing a structured reasoning pattern we term \textit{formal reasoning pattern}. This approach allows the model to emulate human problem-solving strategies in Lean, iteratively generating and refining proof steps. Kimina-Prover sets a new state-of-the-art on the miniF2F benchmark, reaching 80.7% with pass@8192. Beyond improved benchmark performance, our work yields several key insights: (1) Kimina-Prover exhibits high sample efficiency, delivering strong results even with minimal sampling (pass@1) and scaling effectively with computational budget, stemming from its unique reasoning pattern and RL training; (2) we demonstrate clear performance scaling with model size, a trend previously unobserved for neural theorem provers in formal mathematics; (3) the learned reasoning style, distinct from traditional search algorithms, shows potential to bridge the gap between formal verification and informal mathematical intuition. We open source distilled versions with 1.5B and 7B parameters of Kimina-Prover </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºKimina-Prover Previewç‰ˆï¼Œè¿™æ˜¯ä¸€æ¬¾å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œåœ¨æ­£å¼å®šç†è¯æ˜æ–¹é¢å¼€åˆ›äº†ä¸€ç§æ–°é¢–çš„åŸºäºæ¨ç†çš„æ¢ç´¢æ¨¡å¼ï¼Œæ­¤é¢„è§ˆç‰ˆå±•ç¤ºäº†å…¶è¡¨ç°ã€‚Kimina-Proveré€šè¿‡é‡‡ç”¨Qwen2.5-72Bçš„å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ ç®¡é“è¿›è¡Œè®­ç»ƒï¼Œåœ¨é‡‡ç”¨æˆ‘ä»¬ç§°ä¹‹ä¸ºâ€œå½¢å¼æ¨ç†æ¨¡å¼â€çš„ç»“æ„åŒ–æ¨ç†æ¨¡å¼æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚è¿™ç§æ–¹æ³•ä½¿æ¨¡å‹èƒ½å¤Ÿæ¨¡ä»¿äººç±»åœ¨Leanä¸­çš„é—®é¢˜è§£å†³ç­–ç•¥ï¼Œé€šè¿‡è¿­ä»£ç”Ÿæˆå’Œç²¾ç‚¼è¯æ˜æ­¥éª¤ã€‚Kimina-Proveråœ¨miniF2FåŸºå‡†æµ‹è¯•ä¸­åˆ›é€ äº†æ–°çš„æœ€å…ˆè¿›çš„æˆç»©ï¼Œè¾¾åˆ°äº†pass@8192çš„80.7%ã€‚é™¤äº†æé«˜åŸºå‡†æµ‹è¯•æ€§èƒ½å¤–ï¼Œæˆ‘ä»¬çš„å·¥ä½œè¿˜è·å¾—äº†å‡ ä¸ªå…³é”®è§è§£ï¼šï¼ˆ1ï¼‰Kimina-Proverå…·æœ‰è¾ƒé«˜çš„æ ·æœ¬æ•ˆç‡ï¼Œå³ä½¿åœ¨æœ€å°é‡‡æ ·é‡ï¼ˆpass@1ï¼‰çš„æƒ…å†µä¸‹ä¹Ÿèƒ½äº§ç”Ÿå¼ºå¤§çš„ç»“æœï¼Œå¹¶ä¸”éšç€è®¡ç®—é¢„ç®—çš„æœ‰æ•ˆæ‰©å±•ï¼Œè¿™æºäºå…¶ç‹¬ç‰¹çš„æ¨ç†æ¨¡å¼å’Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼›ï¼ˆ2ï¼‰æˆ‘ä»¬è¯æ˜äº†æ¨¡å‹è§„æ¨¡ä¸æ€§èƒ½ä¹‹é—´çš„æ˜ç¡®å…³ç³»ï¼Œè¿™å¯¹äºå½¢å¼æ•°å­¦ä¸­çš„ç¥ç»å®šç†è¯æ˜å™¨è€Œè¨€æ˜¯å‰æ‰€æœªæœ‰çš„è¶‹åŠ¿ï¼›ï¼ˆ3ï¼‰å­¦åˆ°çš„æ¨ç†é£æ ¼ä¸ä¼ ç»Ÿçš„æœç´¢ç®—æ³•æˆªç„¶ä¸åŒï¼Œæ˜¾ç¤ºå‡ºå¼¥åˆå½¢å¼éªŒè¯ä¸éæ­£å¼æ•°å­¦ç›´è§‰ä¹‹é—´å·®è·çš„æ½œåŠ›ã€‚æˆ‘ä»¬å…¬å¼€äº†ç»è¿‡æç‚¼çš„Kimina-Proverç‰ˆæœ¬ï¼Œå‚æ•°åˆ†åˆ«ä¸º1.5Bå’Œ7Bã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11354v1">PDF</a> 22 pages</p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹Kimina-Proveråœ¨å½¢å¼åŒ–å®šç†è¯æ˜é¢†åŸŸå®ç°äº†æ–°çš„çªç ´ã€‚å®ƒé‡‡ç”¨äº†ä¸€ç§æ–°é¢–çš„é€»è¾‘é©±åŠ¨æ¢ç´¢èŒƒå¼ï¼Œèƒ½å¤Ÿåœ¨Lean 4è¯æ˜ç”Ÿæˆä¸­å±•ç°å¼ºå¤§çš„æ€§èƒ½ã€‚Kimina-Proverå±•ç°å‡ºé«˜æ ·æœ¬æ•ˆç‡å’Œæ¸…æ™°çš„æ€§èƒ½è§„æ¨¡æ‰©å±•æ€§ï¼Œå¹¶æœ‰æœ›ç¼©å°å½¢å¼éªŒè¯ä¸ç›´è§‚æ•°å­¦ä¹‹é—´çš„å·®è·ã€‚æˆ‘ä»¬å¼€æºäº†è’¸é¦ç‰ˆçš„Kimina-Proverï¼Œå‚æ•°åˆ†åˆ«ä¸º1.5Bå’Œ7Bã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Kimina-Proveræ˜¯ä¸€ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œé‡‡ç”¨æ–°é¢–çš„é€»è¾‘é©±åŠ¨æ¢ç´¢èŒƒå¼è¿›è¡Œå½¢å¼å®šç†è¯æ˜ã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨Lean 4è¯æ˜ç”Ÿæˆä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œé€šè¿‡ä¸€ç§ç§°ä¸ºâ€œå½¢å¼åŒ–æ¨ç†æ¨¡å¼â€çš„ç»“æ„åŒ–æ¨ç†æ¨¡å¼æ¥æ¨¡æ‹Ÿäººç±»è§£å†³é—®é¢˜çš„ç­–ç•¥ã€‚</li>
<li>Kimina-Proveråœ¨miniF2FåŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æ–°çš„æŠ€æœ¯æ°´å¹³ï¼Œè¾¾åˆ°80.7%çš„å‡†ç¡®ç‡ã€‚</li>
<li>Kimina-Proverå±•ç°å‡ºé«˜æ ·æœ¬æ•ˆç‡å’Œè®¡ç®—é¢„ç®—çš„æœ‰æ•ˆæ‰©å±•æ€§ã€‚</li>
<li>æ¨¡å‹å¤§å°ä¸æ€§èƒ½ä¹‹é—´å­˜åœ¨æ¸…æ™°çš„æ­£ç›¸å…³å…³ç³»ï¼Œè¿™åœ¨ä¹‹å‰çš„ç¥ç»å®šç†è¯æ˜å™¨ä¸­æœªè¢«è§‚å¯Ÿåˆ°ã€‚</li>
<li>Kimina-Proveræ‰€å±•ç°çš„æ¨ç†é£æ ¼æœ‰æœ›ç¼©å°å½¢å¼éªŒè¯å’Œç›´è§‚æ•°å­¦ä¹‹é—´çš„å·®è·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11354">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9ed07dd198e12dcd7b9703aba6e3df3d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e0d5284a83425a4889e6bf046975eebf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-06a159bbd6fa2a5b76350f927c68709f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cdc759d1b9e92edfb482c412b795050a.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="The-Obvious-Invisible-Threat-LLM-Powered-GUI-Agentsâ€™-Vulnerability-to-Fine-Print-Injections"><a href="#The-Obvious-Invisible-Threat-LLM-Powered-GUI-Agentsâ€™-Vulnerability-to-Fine-Print-Injections" class="headerlink" title="The Obvious Invisible Threat: LLM-Powered GUI Agentsâ€™ Vulnerability to   Fine-Print Injections"></a>The Obvious Invisible Threat: LLM-Powered GUI Agentsâ€™ Vulnerability to   Fine-Print Injections</h2><p><strong>Authors:Chaoran Chen, Zhiping Zhang, Bingcan Guo, Shang Ma, Ibrahim Khalilov, Simret A Gebreegziabher, Yanfang Ye, Ziang Xiao, Yaxing Yao, Tianshi Li, Toby Jia-Jun Li</strong></p>
<p>A Large Language Model (LLM) powered GUI agent is a specialized autonomous system that performs tasks on the userâ€™s behalf according to high-level instructions. It does so by perceiving and interpreting the graphical user interfaces (GUIs) of relevant apps, often visually, inferring necessary sequences of actions, and then interacting with GUIs by executing the actions such as clicking, typing, and tapping. To complete real-world tasks, such as filling forms or booking services, GUI agents often need to process and act on sensitive user data. However, this autonomy introduces new privacy and security risks. Adversaries can inject malicious content into the GUIs that alters agent behaviors or induces unintended disclosures of private information. These attacks often exploit the discrepancy between visual saliency for agents and human users, or the agentâ€™s limited ability to detect violations of contextual integrity in task automation. In this paper, we characterized six types of such attacks, and conducted an experimental study to test these attacks with six state-of-the-art GUI agents, 234 adversarial webpages, and 39 human participants. Our findings suggest that GUI agents are highly vulnerable, particularly to contextually embedded threats. Moreover, human users are also susceptible to many of these attacks, indicating that simple human oversight may not reliably prevent failures. This misalignment highlights the need for privacy-aware agent design. We propose practical defense strategies to inform the development of safer and more reliable GUI agents. </p>
<blockquote>
<p>ä¸€ä¸ªç”±å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„å›¾ç”¨ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†æ˜¯ä¸€ä¸ªä¸“é—¨è®¾è®¡çš„è‡ªä¸»ç³»ç»Ÿï¼Œèƒ½å¤Ÿæ ¹æ®é«˜çº§æŒ‡ä»¤ä»£è¡¨ç”¨æˆ·æ‰§è¡Œä»»åŠ¡ã€‚å®ƒé€šè¿‡æ„ŸçŸ¥å’Œè§£è¯»ç›¸å…³åº”ç”¨çš„å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ï¼Œé€šå¸¸æ˜¯é€šè¿‡è§†è§‰æ–¹å¼ï¼Œæ¨æ–­å‡ºå¿…è¦çš„è¡ŒåŠ¨åºåˆ—ï¼Œç„¶åé€šè¿‡æ‰§è¡Œç‚¹å‡»ã€é”®å…¥å’Œè§¦æ§ç­‰è¡ŒåŠ¨ä¸GUIè¿›è¡Œäº¤äº’ã€‚ä¸ºäº†å®Œæˆç°å®ä¸–ç•Œä¸­çš„ä»»åŠ¡ï¼Œå¦‚å¡«å†™è¡¨æ ¼æˆ–é¢„è®¢æœåŠ¡ï¼ŒGUIä»£ç†é€šå¸¸éœ€è¦å¤„ç†å’Œæ“ä½œæ•æ„Ÿçš„ç”¨æˆ·æ•°æ®ã€‚ç„¶è€Œï¼Œè¿™ç§è‡ªä¸»æ€§å¸¦æ¥äº†æ–°çš„éšç§å’Œå®‰å…¨é£é™©ã€‚æ”»å‡»è€…å¯ä»¥åœ¨GUIä¸­æ³¨å…¥æ¶æ„å†…å®¹ï¼Œæ”¹å˜ä»£ç†è¡Œä¸ºæˆ–è¯±å¯¼æ„å¤–æ³„éœ²ç§äººä¿¡æ¯ã€‚è¿™äº›æ”»å‡»é€šå¸¸åˆ©ç”¨ä»£ç†å’Œäººç±»ç”¨æˆ·ä¹‹é—´è§†è§‰æ˜¾è‘—æ€§çš„å·®å¼‚ï¼Œæˆ–è€…ä»£ç†åœ¨ä»»åŠ¡è‡ªåŠ¨åŒ–ä¸­æ£€æµ‹è¿åä¸Šä¸‹æ–‡å®Œæ•´æ€§çš„æœ‰é™èƒ½åŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æè¿°äº†å…­ç§æ­¤ç±»æ”»å‡»ï¼Œå¹¶é€šè¿‡å®éªŒç ”ç©¶äº†è¿™äº›æ”»å‡»ä¸å…­ç§æœ€å…ˆè¿›çš„GUIä»£ç†ã€234ä¸ªå¯¹æŠ—æ€§ç½‘é¡µå’Œ39åäººç±»å‚ä¸è€…çš„æµ‹è¯•ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒGUIä»£ç†é«˜åº¦è„†å¼±ï¼Œç‰¹åˆ«æ˜¯å®¹æ˜“å—åˆ°ä¸Šä¸‹æ–‡åµŒå…¥çš„å¨èƒã€‚æ­¤å¤–ï¼Œäººç±»ç”¨æˆ·ä¹Ÿæ˜“å—è¿™äº›æ”»å‡»çš„å½±å“ï¼Œè¡¨æ˜ç®€å•çš„äººä¸ºç›‘ç£å¯èƒ½æ— æ³•å¯é åœ°é˜²æ­¢å¤±è´¥ã€‚è¿™ç§ä¸åŒ¹é…çªæ˜¾äº†éœ€è¦è®¾è®¡å…·æœ‰éšç§æ„è¯†çš„ä»£ç†ã€‚æˆ‘ä»¬æå‡ºå®ç”¨çš„é˜²å¾¡ç­–ç•¥ï¼Œä»¥æŒ‡å¯¼å¼€å‘æ›´å®‰å…¨ã€æ›´å¯é çš„GUIä»£ç†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11281v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>LLMé©±åŠ¨çš„å†…è§†ç”¨æˆ·æ¥å£è‡ªåŠ¨åŒ–ä»£ç†èƒ½é€šè¿‡ç†è§£å¹¶æ“ä½œå›¾å½¢ç”¨æˆ·æ¥å£ï¼ˆGUIï¼‰æ‰§è¡Œç”¨æˆ·ä»»åŠ¡ï¼Œä½†è‡ªä¸»æ“ä½œå¸¦æ¥éšç§å’Œå®‰å…¨é—®é¢˜ã€‚æ”»å‡»è€…å¯èƒ½æ³¨å…¥æ¶æ„å†…å®¹æ”¹å˜ä»£ç†è¡Œä¸ºæˆ–æ³„éœ²ç§äººä¿¡æ¯ã€‚å®éªŒå‘ç°è¿™ç±»æ”»å‡»ååˆ†å±é™©ä¸”å¸¸è§äºçœŸå®ä»»åŠ¡è¯­å¢ƒï¼Œè€Œè‡ªåŠ¨åŒ–è®¾è®¡ä¹Ÿä½¿äººä¸ºå®¡æŸ¥ä¸èƒ½é¿å…æ”»å‡»é£é™©ã€‚æˆ‘ä»¬éœ€è¦åŠ å¼ºå†…è§†ç”¨æˆ·æ¥å£ä»£ç†çš„å®‰å…¨æ€§å’Œå¯é æ€§ï¼Œå¯¹æ­¤æˆ‘ä»¬æå‡ºäº†å…·ä½“çš„é˜²å¾¡ç­–ç•¥ã€‚</p>
<p><strong>å…³é”®è¦ç‚¹</strong></p>
<ol>
<li>LLMé©±åŠ¨çš„å†…è§†ç”¨æˆ·æ¥å£ä»£ç†å…·æœ‰å¤„ç†çœŸå®ä¸–ç•Œä»»åŠ¡çš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬å¡«å†™è¡¨å•å’Œé¢„è®¢æœåŠ¡ã€‚åœ¨æ­¤è¿‡ç¨‹ä¸­æ¶‰åŠå¤„ç†ç”¨æˆ·çš„æ•æ„Ÿæ•°æ®ã€‚</li>
<li>GUIä»£ç†çš„è‡ªä¸»æ“ä½œå¸¦æ¥äº†éšç§å’Œå®‰å…¨é—®é¢˜é£é™©ã€‚æ”»å‡»è€…å¯èƒ½é€šè¿‡æ³¨å…¥æ¶æ„å†…å®¹æ”¹å˜ä»£ç†è¡Œä¸ºæˆ–æ³„éœ²ç§äººä¿¡æ¯ã€‚æ”»å‡»ç±»å‹å¤šæ ·ï¼ŒåŒ…æ‹¬è§†è§‰æ˜¾è‘—æ€§å·®å¼‚æ”»å‡»å’Œä¸Šä¸‹æ–‡å®Œæ•´æ€§æ”»å‡»ç­‰ã€‚</li>
<li>å®éªŒå‘ç°GUIä»£ç†é«˜åº¦å®¹æ˜“å—åˆ°æ”»å‡»ï¼Œç‰¹åˆ«æ˜¯åœ¨çœŸå®ä»»åŠ¡è¯­å¢ƒä¸­ã€‚äººç±»ç”¨æˆ·åŒæ ·å®¹æ˜“å—åˆ°è¿™äº›æ”»å‡»çš„å½±å“ï¼Œè¿™è¡¨æ˜ç®€å•çš„äººä¸ºç›‘ç£å¯èƒ½æ— æ³•å¯é åœ°é˜²æ­¢å¤±è´¥ã€‚</li>
<li>GUIä»£ç†çš„è®¾è®¡å’Œéƒ¨ç½²éœ€è¦è€ƒè™‘åˆ°éšç§å’Œå®‰å…¨é—®é¢˜ï¼ŒåŒ…æ‹¬ä»£ç†å¯¹ç”¨æˆ·æ•°æ®çš„å¤„ç†æ–¹å¼å’Œå®‰å…¨ä¿æŠ¤æœºåˆ¶ç­‰ã€‚åŒæ—¶ä¹Ÿéœ€è¦æ³¨æ„å…¶åœ¨æ“ä½œè¿‡ç¨‹ä¸­çš„å¤±è¯¯æƒ…å†µå¤„ç†å’Œæ•°æ®æ”¶é›†å®¡è®¡çš„é—®é¢˜ã€‚éœ€è¦æœ‰åŠæ³•å¸®åŠ©è¯†åˆ«å’ŒæŠµå¾¡è¿™ç§ç±»å‹çš„å¨èƒä»¥é™ä½æ”»å‡»å½±å“å¹¶ä¿éšœç”¨æˆ·æ•°æ®å®‰å…¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11281">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0704ceaa6dd8d5666c6339c7e6025740.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-147fe5d54e8c807958c792e5b4d9cb3c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fe2242c083ccd4c09b862c7229791332.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d80ed7a23f585527c13a43d5b9fbbab0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Distillation-Supervised-Convolutional-Low-Rank-Adaptation-for-Efficient-Image-Super-Resolution"><a href="#Distillation-Supervised-Convolutional-Low-Rank-Adaptation-for-Efficient-Image-Super-Resolution" class="headerlink" title="Distillation-Supervised Convolutional Low-Rank Adaptation for Efficient   Image Super-Resolution"></a>Distillation-Supervised Convolutional Low-Rank Adaptation for Efficient   Image Super-Resolution</h2><p><strong>Authors:Xinning Chai, Yao Zhang, Yuxuan Zhang, Zhengxue Cheng, Yingsheng Qin, Yucai Yang, Li Song</strong></p>
<p>Convolutional neural networks (CNNs) have been widely used in efficient image super-resolution. However, for CNN-based methods, performance gains often require deeper networks and larger feature maps, which increase complexity and inference costs. Inspired by LoRAâ€™s success in fine-tuning large language models, we explore its application to lightweight models and propose Distillation-Supervised Convolutional Low-Rank Adaptation (DSCLoRA), which improves model performance without increasing architectural complexity or inference costs. Specifically, we integrate ConvLoRA into the efficient SR network SPAN by replacing the SPAB module with the proposed SConvLB module and incorporating ConvLoRA layers into both the pixel shuffle block and its preceding convolutional layer. DSCLoRA leverages low-rank decomposition for parameter updates and employs a spatial feature affinity-based knowledge distillation strategy to transfer second-order statistical information from teacher models (pre-trained SPAN) to student models (ours). This method preserves the core knowledge of lightweight models and facilitates optimal solution discovery under certain conditions. Experiments on benchmark datasets show that DSCLoRA improves PSNR and SSIM over SPAN while maintaining its efficiency and competitive image quality. Notably, DSCLoRA ranked first in the Overall Performance Track of the NTIRE 2025 Efficient Super-Resolution Challenge. Our code and models are made publicly available at <a target="_blank" rel="noopener" href="https://github.com/Yaozzz666/DSCF-SR">https://github.com/Yaozzz666/DSCF-SR</a>. </p>
<blockquote>
<p>å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰åœ¨é«˜æ•ˆçš„å›¾åƒè¶…åˆ†è¾¨ç‡æŠ€æœ¯ä¸­å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ã€‚ç„¶è€Œï¼Œå¯¹äºåŸºäºCNNçš„æ–¹æ³•è€Œè¨€ï¼Œæ€§èƒ½çš„æå‡é€šå¸¸éœ€è¦æ›´æ·±çš„ç½‘ç»œå’Œæ›´å¤§çš„ç‰¹å¾å›¾ï¼Œè¿™å¢åŠ äº†å¤æ‚æ€§å’Œæ¨ç†æˆæœ¬ã€‚å—LoRAåœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹å¾®è°ƒä¸­æˆåŠŸçš„å¯å‘ï¼Œæˆ‘ä»¬æ¢ç´¢äº†å…¶åœ¨è½»é‡çº§æ¨¡å‹ä¸­çš„åº”ç”¨ï¼Œå¹¶æå‡ºäº†è’¸é¦ç›‘ç£å·ç§¯ä½ç§©é€‚åº”ï¼ˆDSCLoRAï¼‰ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨ä¸å¢åŠ æ¶æ„å¤æ‚æ€§æˆ–æ¨ç†æˆæœ¬çš„æƒ…å†µä¸‹æé«˜æ¨¡å‹æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†ConvLoRAé›†æˆåˆ°é«˜æ•ˆçš„SRç½‘ç»œSPANä¸­ï¼Œé€šè¿‡ç”¨æ‰€æå‡ºçš„SConvLBæ¨¡å—æ›¿æ¢SPABæ¨¡å—ï¼Œå¹¶å°†ConvLoRAå±‚é›†æˆåˆ°åƒç´ æ´—ç‰Œå—åŠå…¶å‰é¢çš„å·ç§¯å±‚ä¸­ã€‚DSCLoRAåˆ©ç”¨ä½ç§©åˆ†è§£è¿›è¡Œå‚æ•°æ›´æ–°ï¼Œå¹¶é‡‡ç”¨åŸºäºç©ºé—´ç‰¹å¾äº²å’ŒåŠ›çš„çŸ¥è¯†è’¸é¦ç­–ç•¥ï¼Œä»æ•™å¸ˆæ¨¡å‹ï¼ˆé¢„è®­ç»ƒçš„SPANï¼‰å‘å­¦ç”Ÿæ¨¡å‹ï¼ˆæˆ‘ä»¬çš„æ¨¡å‹ï¼‰ä¼ é€’äºŒé˜¶ç»Ÿè®¡ä¿¡æ¯ã€‚è¿™ç§æ–¹æ³•ä¿ç•™äº†è½»é‡çº§æ¨¡å‹çš„æ ¸å¿ƒçŸ¥è¯†ï¼Œå¹¶åœ¨ä¸€å®šæ¡ä»¶ä¸‹ä¿ƒè¿›äº†æœ€ä¼˜è§£çš„å‘ç°ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDSCLoRAåœ¨ä¿æŒSPANæ•ˆç‡å’Œå›¾åƒè´¨é‡ç«äº‰åŠ›çš„åŒæ—¶ï¼Œæé«˜äº†PSNRå’ŒSSIMã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒDSCLoRAåœ¨NTIRE 2025é«˜æ•ˆè¶…åˆ†è¾¨ç‡æŒ‘æˆ˜çš„æ€»ä½“æ€§èƒ½è½¨é“ä¸­æ’åç¬¬ä¸€ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/Yaozzz666/DSCF-SR%E4%B8%8A%E5%85%AC%E5%BC%80%E5%8F%AF%E7%94%A8%E3%80%82">https://github.com/Yaozzz666/DSCF-SRä¸Šå…¬å¼€å¯ç”¨ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11271v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰çš„é«˜æ•ˆå›¾åƒè¶…åˆ†è¾¨ç‡æŠ€æœ¯ã€‚ä¸ºäº†æé«˜æ¨¡å‹æ€§èƒ½ï¼Œæ–‡ç« æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•DSCLoRAï¼Œå®ƒç»“åˆäº†LoRAæŠ€æœ¯åœ¨è½»é‡çº§æ¨¡å‹ä¸­çš„åº”ç”¨ï¼Œé€šè¿‡ä½ç§©åˆ†è§£è¿›è¡Œå‚æ•°æ›´æ–°ï¼Œå¹¶é‡‡ç”¨åŸºäºç©ºé—´ç‰¹å¾äº²å’Œæ€§çš„çŸ¥è¯†è’¸é¦ç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDSCLoRAåœ¨ä¿æŒé«˜æ•ˆå’Œå›¾åƒè´¨é‡ç«äº‰åŠ›çš„åŒæ—¶ï¼Œæé«˜äº†å›¾åƒè¶…åˆ†è¾¨ç‡çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•åœ¨NTIRE 2025é«˜æ•ˆè¶…åˆ†è¾¨ç‡æŒ‘æˆ˜èµ›ä¸­æ€»ä½“æ€§èƒ½æ’åç¬¬ä¸€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å·²å¹¿æ³›åº”ç”¨äºé«˜æ•ˆå›¾åƒè¶…åˆ†è¾¨ç‡æŠ€æœ¯ã€‚</li>
<li>DSCLoRAç»“åˆäº†LoRAæŠ€æœ¯åº”ç”¨äºè½»é‡çº§æ¨¡å‹ï¼Œä»¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>DSCLoRAä½¿ç”¨ä½ç§©åˆ†è§£è¿›è¡Œå‚æ•°æ›´æ–°ï¼Œå¹¶é‡‡ç”¨åŸºäºç©ºé—´ç‰¹å¾äº²å’Œæ€§çš„çŸ¥è¯†è’¸é¦ç­–ç•¥ã€‚</li>
<li>DSCLoRAæ”¹è¿›äº†å›¾åƒè¶…åˆ†è¾¨ç‡çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†æ¨¡å‹çš„æ•ˆç‡å’Œå›¾åƒè´¨é‡ç«äº‰åŠ›ã€‚</li>
<li>DSCLoRAåœ¨NTIRE 2025é«˜æ•ˆè¶…åˆ†è¾¨ç‡æŒ‘æˆ˜èµ›ä¸­æ€»ä½“æ€§èƒ½æ’åç¬¬ä¸€ã€‚</li>
<li>æ–‡ç« æå‡ºäº†ä¸€ä¸ªåä¸ºDSCLoRAçš„æ–°æ–¹æ³•ï¼Œé€šè¿‡æ›¿æ¢SPABæ¨¡å—å¹¶æ•´åˆConvLoRAå±‚æ¥ä¼˜åŒ–ç°æœ‰SRç½‘ç»œï¼ˆSPANï¼‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11271">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-019d5ea13079c20284bf91b4794ff8a0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-85ff8d796ebb18bb93424ee009150d94.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a559afdc1c4512da6a8516506a8d777e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-80a54acd1fc52b9b303b58e8c775e6dd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-38093366d7af5542f25d216b37b95365.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-18171e0a93f2068be7516062d637c527.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-35c72eba6db02dab34de6abbf0f9fb02.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="When-is-Task-Vector-Provably-Effective-for-Model-Editing-A-Generalization-Analysis-of-Nonlinear-Transformers"><a href="#When-is-Task-Vector-Provably-Effective-for-Model-Editing-A-Generalization-Analysis-of-Nonlinear-Transformers" class="headerlink" title="When is Task Vector Provably Effective for Model Editing? A   Generalization Analysis of Nonlinear Transformers"></a>When is Task Vector Provably Effective for Model Editing? A   Generalization Analysis of Nonlinear Transformers</h2><p><strong>Authors:Hongkang Li, Yihua Zhang, Shuai Zhang, Meng Wang, Sijia Liu, Pin-Yu Chen</strong></p>
<p>Task arithmetic refers to editing the pre-trained model by adding a weighted sum of task vectors, each of which is the weight update from the pre-trained model to fine-tuned models for certain tasks. This approach recently gained attention as a computationally efficient inference method for model editing, e.g., multi-task learning, forgetting, and out-of-domain generalization capabilities. However, the theoretical understanding of why task vectors can execute various conceptual operations remains limited, due to the highly non-convexity of training Transformer-based models. To the best of our knowledge, this paper provides the first theoretical characterization of the generalization guarantees of task vector methods on nonlinear Transformers. We consider a conceptual learning setting, where each task is a binary classification problem based on a discriminative pattern. We theoretically prove the effectiveness of task addition in simultaneously learning a set of irrelevant or aligned tasks, as well as the success of task negation in unlearning one task from irrelevant or contradictory tasks. Moreover, we prove the proper selection of linear coefficients for task arithmetic to achieve guaranteed generalization to out-of-domain tasks. All of our theoretical results hold for both dense-weight parameters and their low-rank approximations. Although established in a conceptual setting, our theoretical findings were validated on a practical machine unlearning task using the large language model Phi-1.5 (1.3B). </p>
<blockquote>
<p>ä»»åŠ¡ç®—æœ¯æŒ‡çš„æ˜¯é€šè¿‡æ·»åŠ ä»»åŠ¡å‘é‡çš„åŠ æƒå’Œæ¥ç¼–è¾‘é¢„è®­ç»ƒæ¨¡å‹ï¼Œå…¶ä¸­æ¯ä¸ªä»»åŠ¡å‘é‡éƒ½æ˜¯é¢„è®­ç»ƒæ¨¡å‹åˆ°é’ˆå¯¹æŸäº›ä»»åŠ¡çš„å¾®è°ƒæ¨¡å‹çš„æƒé‡æ›´æ–°ã€‚ä½œä¸ºä¸€ç§è®¡ç®—æ•ˆç‡é«˜çš„æ¨ç†æ–¹æ³•ï¼Œè¿™ç§æŠ€æœ¯åœ¨æ¨¡å‹ç¼–è¾‘ä¸­å¼•èµ·äº†å…³æ³¨ï¼Œä¾‹å¦‚å¤šä»»åŠ¡å­¦ä¹ ã€é—å¿˜å’Œè·¨åŸŸæ³›åŒ–èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç”±äºè®­ç»ƒåŸºäºTransformerçš„æ¨¡å‹é«˜åº¦éå‡¸æ€§ï¼Œå…³äºä»»åŠ¡å‘é‡ä¸ºä½•èƒ½å¤Ÿæ‰§è¡Œå„ç§æ¦‚å¿µæ“ä½œçš„ç†è®ºç†è§£ä»ç„¶æœ‰é™ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæœ¬æ–‡é¦–æ¬¡å¯¹ä»»åŠ¡å‘é‡æ–¹æ³•åœ¨éçº¿æ€§Transformerä¸Šçš„æ³›åŒ–ä¿è¯è¿›è¡Œäº†ç†è®ºè¡¨å¾ã€‚æˆ‘ä»¬è€ƒè™‘ä¸€ä¸ªæ¦‚å¿µå­¦ä¹ åœºæ™¯ï¼Œå…¶ä¸­æ¯ä¸ªä»»åŠ¡éƒ½æ˜¯åŸºäºåˆ¤åˆ«æ¨¡å¼çš„äºŒå…ƒåˆ†ç±»é—®é¢˜ã€‚æˆ‘ä»¬ä»ç†è®ºä¸Šè¯æ˜äº†åŒæ—¶å­¦ä¹ ä¸€ç»„ä¸ç›¸å…³æˆ–å¯¹é½çš„ä»»åŠ¡æ—¶æ·»åŠ ä»»åŠ¡çš„æœ‰æ•ˆæ€§ï¼Œä»¥åŠä»ä¸ç›¸å…³æˆ–çŸ›ç›¾çš„ä»»åŠ¡ä¸­é—å¿˜ä¸€ä¸ªä»»åŠ¡æ—¶å¦å®šä»»åŠ¡çš„æˆåŠŸã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜äº†é€‰æ‹©é€‚å½“çš„çº¿æ€§ç³»æ•°è¿›è¡Œä»»åŠ¡ç®—æœ¯è¿ç®—ï¼Œä»¥å®ç°è·¨åŸŸä»»åŠ¡çš„ä¿è¯æ³›åŒ–ã€‚æˆ‘ä»¬çš„ç†è®ºç»“æœå¯¹å¯†é›†æƒé‡å‚æ•°åŠå…¶ä½ç§©è¿‘ä¼¼éƒ½æˆç«‹ã€‚è™½ç„¶è¿™äº›ç†è®ºç»“æœæ˜¯åœ¨æ¦‚å¿µåœºæ™¯ä¸‹å»ºç«‹çš„ï¼Œä½†æˆ‘ä»¬é€šè¿‡åœ¨å®é™…çš„æœºå™¨é—å¿˜ä»»åŠ¡ä¸­ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹Phi-1.5ï¼ˆè§„æ¨¡ä¸º1.3Bï¼‰è¿›è¡Œäº†éªŒè¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10957v1">PDF</a> Published at ICLR 2025 as an oral paper</p>
<p><strong>Summary</strong>ï¼šä»»åŠ¡ç®—æœ¯é€šè¿‡æ·»åŠ ä»»åŠ¡å‘é‡çš„åŠ æƒå’Œæ¥ç¼–è¾‘é¢„è®­ç»ƒæ¨¡å‹ï¼Œè¿™äº›ä»»åŠ¡å‘é‡æ˜¯é¢„è®­ç»ƒæ¨¡å‹åˆ°ç‰¹å®šä»»åŠ¡çš„å¾®è°ƒæ¨¡å‹çš„æƒé‡æ›´æ–°ã€‚æœ¬æ–‡æä¾›äº†ä»»åŠ¡å‘é‡æ–¹æ³•åœ¨éçº¿æ€§Transformerä¸Šçš„æ³›åŒ–ä¿è¯çš„ç†è®ºè¡¨å¾ï¼Œå¹¶è¯æ˜äº†ä»»åŠ¡æ·»åŠ å’Œå¦å®šçš„æœ‰æ•ˆæ€§ã€‚åŒæ—¶ï¼Œæœ¬æ–‡è¯æ˜äº†ä»»åŠ¡ç®—æœ¯ä¸­é€‰æ‹©çº¿æ€§ç³»æ•°çš„æ­£ç¡®æ€§ï¼Œä»¥å®ç°è·¨åŸŸä»»åŠ¡çš„æ³›åŒ–ä¿è¯ã€‚è¿™äº›ç†è®ºç»“æœé€‚ç”¨äºå¯†é›†æƒé‡å‚æ•°åŠå…¶ä½ç§©è¿‘ä¼¼ã€‚è™½ç„¶æ˜¯åœ¨æ¦‚å¿µä¸Šå»ºç«‹çš„ï¼Œä½†è¿™äº›ç†è®ºå‘ç°å·²åœ¨å¤§å‹è¯­è¨€æ¨¡å‹Phi-1.5ä¸Šè¿›è¡Œäº†éªŒè¯ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ä»»åŠ¡ç®—æœ¯é€šè¿‡æ·»åŠ åŠ æƒä»»åŠ¡å‘é‡ç¼–è¾‘é¢„è®­ç»ƒæ¨¡å‹ï¼Œæé«˜è®¡ç®—æ•ˆç‡ã€‚</li>
<li>æœ¬æ–‡é¦–æ¬¡ä¸ºä»»åŠ¡å‘é‡æ–¹æ³•åœ¨éçº¿æ€§Transformerä¸Šçš„æ³›åŒ–æä¾›ç†è®ºä¿è¯ã€‚</li>
<li>è¯æ˜äº†ä»»åŠ¡æ·»åŠ å’Œå¦å®šçš„æœ‰æ•ˆæ€§ï¼Œåœ¨åŒæ—¶å­¦ä¹ ä¸€ç»„ä¸ç›¸å…³æˆ–å¯¹é½çš„ä»»åŠ¡ä»¥åŠä»ä¸é‡è¦æˆ–çŸ›ç›¾çš„ä»»åŠ¡ä¸­é—å¿˜ä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</li>
<li>è¯æ˜äº†åœ¨ä»»åŠ¡ç®—æœ¯ä¸­æ­£ç¡®é€‰æ‹©çº¿æ€§ç³»æ•°çš„é‡è¦æ€§ï¼Œä»¥å®ç°è·¨åŸŸä»»åŠ¡çš„æ³›åŒ–ä¿è¯ã€‚</li>
<li>ç†è®ºç»“æœé€‚ç”¨äºå¯†é›†æƒé‡å‚æ•°å’Œä½ç§©è¿‘ä¼¼ã€‚</li>
<li>åœ¨æ¦‚å¿µéªŒè¯çš„åŸºç¡€ä¸Šï¼Œé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹Phi-1.5éªŒè¯äº†è¿™äº›ç†è®ºå‘ç°ã€‚</li>
<li>è¿™äº›ç†è®ºæˆæœä¸ºæ¨¡å‹ç¼–è¾‘æä¾›äº†æ–°çš„ç†è§£å’Œè§†è§’ï¼Œæœ‰åŠ©äºè¿›ä¸€æ­¥æé«˜æ¨¡å‹çš„æ€§èƒ½å’Œåº”ç”¨èŒƒå›´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10957">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-af2bc23dca1a1fc701e67aeb4469f272.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-140e7311cd9096fe1fc8bbfc104d0a16.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b5c5334347e339b483664df3b27a7cc7.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="ReasonDrive-Efficient-Visual-Question-Answering-for-Autonomous-Vehicles-with-Reasoning-Enhanced-Small-Vision-Language-Models"><a href="#ReasonDrive-Efficient-Visual-Question-Answering-for-Autonomous-Vehicles-with-Reasoning-Enhanced-Small-Vision-Language-Models" class="headerlink" title="ReasonDrive: Efficient Visual Question Answering for Autonomous Vehicles   with Reasoning-Enhanced Small Vision-Language Models"></a>ReasonDrive: Efficient Visual Question Answering for Autonomous Vehicles   with Reasoning-Enhanced Small Vision-Language Models</h2><p><strong>Authors:Amirhosein Chahe, Lifeng Zhou</strong></p>
<p>Vision-language models (VLMs) show promise for autonomous driving but often lack transparent reasoning capabilities that are critical for safety. We investigate whether explicitly modeling reasoning during fine-tuning enhances VLM performance on driving decision tasks. Using GPT-4o, we generate structured reasoning chains for driving scenarios from the DriveLM benchmark with category-specific prompting strategies. We compare reasoning-based fine-tuning, answer-only fine-tuning, and baseline instruction-tuned models across multiple small VLM families (Llama 3.2, Llava 1.5, and Qwen 2.5VL). Our results demonstrate that reasoning-based fine-tuning consistently outperforms alternatives, with Llama3.2-11B-reason achieving the highest performance. Models fine-tuned with reasoning show substantial improvements in accuracy and text generation quality, suggesting explicit reasoning enhances internal representations for driving decisions. These findings highlight the importance of transparent decision processes in safety-critical domains and offer a promising direction for developing more interpretable autonomous driving systems. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è‡ªåŠ¨é©¾é©¶æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œä½†åœ¨å…³é”®çš„é€æ˜åº¦æ¨ç†èƒ½åŠ›æ–¹é¢å¾€å¾€å­˜åœ¨ä¸è¶³ï¼Œè¿™å¯¹å®‰å…¨æ€§è‡³å…³é‡è¦ã€‚æˆ‘ä»¬è°ƒæŸ¥äº†åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­æ˜¾å¼å»ºæ¨¡æ¨ç†æ˜¯å¦æœ‰åŠ©äºæå‡é©¾é©¶å†³ç­–ä»»åŠ¡çš„VLMæ€§èƒ½ã€‚æˆ‘ä»¬ä½¿ç”¨GPT-4oä¸ºDriveLMåŸºå‡†æµ‹è¯•ä¸­çš„é©¾é©¶åœºæ™¯ç”Ÿæˆç»“æ„åŒ–æ¨ç†é“¾ï¼Œå¹¶é‡‡ç”¨äº†ç±»åˆ«ç‰¹å®šçš„æç¤ºç­–ç•¥ã€‚æˆ‘ä»¬æ¯”è¾ƒäº†åŸºäºæ¨ç†çš„å¾®è°ƒã€ä»…ç­”æ¡ˆçš„å¾®è°ƒä»¥åŠåŸºäºæŒ‡ä»¤çš„åŸºçº¿æ¨¡å‹çš„æ€§èƒ½å·®å¼‚ï¼Œæ¶‰åŠå¤šä¸ªå°å‹VLMå®¶æ—ï¼ˆLlama 3.2ã€Llava 1.5å’ŒQwen 2.5VLï¼‰ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒåŸºäºæ¨ç†çš„å¾®è°ƒå§‹ç»ˆä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œå…¶ä¸­Llama3.2-11B-reasonçš„è¡¨ç°æœ€ä½³ã€‚ç»è¿‡æ¨ç†è°ƒæ ¡çš„æ¨¡å‹åœ¨å‡†ç¡®æ€§å’Œæ–‡æœ¬ç”Ÿæˆè´¨é‡æ–¹é¢å–å¾—äº†å®è´¨æ€§çš„è¿›æ­¥ï¼Œè¿™è¡¨æ˜æ˜ç¡®çš„æ¨ç†å¢å¼ºäº†é©¾é©¶å†³ç­–çš„å†…éƒ¨è¡¨ç¤ºã€‚è¿™äº›å‘ç°çªæ˜¾äº†å®‰å…¨å…³é”®é¢†åŸŸä¸­é€æ˜å†³ç­–è¿‡ç¨‹çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºå¼€å‘æ›´å…·è§£é‡Šæ€§çš„è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿæä¾›äº†æœ‰å‰æ™¯çš„æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10757v1">PDF</a> </p>
<p><strong>Summary</strong><br>VLMåœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸå…·æœ‰æ½œåŠ›ï¼Œä½†ç¼ºä¹å…³é”®çš„é€æ˜æ¨ç†èƒ½åŠ›ã€‚æœ¬ç ”ç©¶é€šè¿‡å¾®è°ƒè¿‡ç¨‹æ˜¾å¼å»ºæ¨¡æ¨ç†ï¼Œå¢å¼ºVLMåœ¨é©¾é©¶å†³ç­–ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚ä½¿ç”¨GPT-4oç”Ÿæˆç»“æ„åŒ–æ¨ç†é“¾ï¼Œå¯¹æ¯”ä¸åŒæ¨¡å‹è¡¨ç°ã€‚ç»“æœæ˜¾ç¤ºï¼ŒåŸºäºæ¨ç†çš„å¾®è°ƒæ–¹å¼è¡¨ç°æœ€ä½³ï¼Œå‡†ç¡®åº¦å’Œæ–‡æœ¬ç”Ÿæˆè´¨é‡æ˜¾è‘—æé«˜ã€‚è¿™è¡¨æ˜æ˜¾å¼æ¨ç†æœ‰åŠ©äºæå‡é©¾é©¶å†³ç­–çš„å†…éƒ¨è¡¨å¾ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLMåœ¨è‡ªåŠ¨é©¾é©¶ä¸­æœ‰åº”ç”¨æ½œåŠ›ï¼Œä½†ç¼ºä¹é€æ˜æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ç ”ç©¶é€šè¿‡å¾®è°ƒè¿‡ç¨‹æ˜¾å¼å»ºæ¨¡æ¨ç†ï¼Œä»¥å¢å¼ºVLMåœ¨é©¾é©¶å†³ç­–ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>ä½¿ç”¨GPT-4oç”Ÿæˆç»“æ„åŒ–æ¨ç†é“¾ï¼Œç”¨äºé©¾é©¶åœºæ™¯ã€‚</li>
<li>å¯¹æ¯”äº†åŸºäºæ¨ç†çš„å¾®è°ƒã€ä»…å›ç­”å¾®è°ƒåŠåŸºå‡†æŒ‡ä»¤å¾®è°ƒæ¨¡å‹çš„è¡¨ç°ã€‚</li>
<li>åŸºäºæ¨ç†çš„å¾®è°ƒæ–¹å¼è¡¨ç°æœ€ä½³ï¼ŒLlama3.2-11B-reasonæ¨¡å‹æ€§èƒ½æœ€é«˜ã€‚</li>
<li>æ˜¾å¼æ¨ç†æœ‰åŠ©äºæå‡é©¾é©¶å†³ç­–çš„å‡†ç¡®åº¦å’Œæ–‡æœ¬ç”Ÿæˆè´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10757">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-680ebfa4b3798c05ad18c99ec18655ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-771f7cc0da05f138ca249c067e85e18a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6df2340cb97c37b38a15f19075e6e8bb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-42543d31f9f13eed5c771f66a7161d60.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-76f282bf50ac098de27cc1d6bf1983ec.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Improving-In-Context-Learning-with-Reasoning-Distillation"><a href="#Improving-In-Context-Learning-with-Reasoning-Distillation" class="headerlink" title="Improving In-Context Learning with Reasoning Distillation"></a>Improving In-Context Learning with Reasoning Distillation</h2><p><strong>Authors:Nafis Sadeq, Xin Xu, Zhouhang Xie, Julian McAuley, Byungkyu Kang, Prarit Lamba, Xiang Gao</strong></p>
<p>Language models rely on semantic priors to perform in-context learning, which leads to poor performance on tasks involving inductive reasoning. Instruction-tuning methods based on imitation learning can superficially enhance the in-context learning performance of language models, but they often fail to improve the modelâ€™s understanding of the underlying rules that connect inputs and outputs in few-shot demonstrations. We propose ReDis, a reasoning distillation technique designed to improve the inductive reasoning capabilities of language models. Through a careful combination of data augmentation, filtering, supervised fine-tuning, and alignment, ReDis achieves significant performance improvements across a diverse range of tasks, including 1D-ARC, List Function, ACRE, and MiniSCAN. Experiments on three language model backbones show that ReDis outperforms equivalent few-shot prompting baselines across all tasks and even surpasses the teacher model, GPT-4o, in some cases. ReDis, based on the LLaMA-3 backbone, achieves relative improvements of 23.2%, 2.8%, and 66.6% over GPT-4o on 1D-ARC, ACRE, and MiniSCAN, respectively, within a similar hypothesis search space. The code, dataset, and model checkpoints will be made available at <a target="_blank" rel="noopener" href="https://github.com/NafisSadeq/reasoning-distillation.git">https://github.com/NafisSadeq/reasoning-distillation.git</a>. </p>
<blockquote>
<p>è¯­è¨€æ¨¡å‹ä¾èµ–äºè¯­ä¹‰å…ˆéªŒæ¥è¿›è¡Œä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œè¿™å¯¼è‡´åœ¨å¤„ç†æ¶‰åŠå½’çº³æ¨ç†çš„ä»»åŠ¡æ—¶è¡¨ç°ä¸ä½³ã€‚åŸºäºæ¨¡ä»¿å­¦ä¹ çš„æŒ‡ä»¤è°ƒæ•´æ–¹æ³•è¡¨é¢ä¸Šå¯ä»¥å¢å¼ºè¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ï¼Œä½†å®ƒä»¬å¾€å¾€æ— æ³•æé«˜æ¨¡å‹å¯¹å°‘æ•°æ¼”ç¤ºä¸­è¾“å…¥å’Œè¾“å‡ºä¹‹é—´åŸºæœ¬è§„åˆ™çš„ç†è§£ã€‚æˆ‘ä»¬æå‡ºäº†ReDisï¼Œè¿™æ˜¯ä¸€ç§æ¨ç†è’¸é¦æŠ€æœ¯ï¼Œæ—¨åœ¨æé«˜è¯­è¨€æ¨¡å‹çš„å½’çº³æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡æ•°æ®å¢å¼ºã€è¿‡æ»¤ã€ç›‘ç£å¾®è°ƒå’Œå¯¹é½çš„ç²¾å¿ƒç»“åˆï¼ŒReDisåœ¨å¤šç§ä»»åŠ¡ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ï¼ŒåŒ…æ‹¬1D-ARCã€List Functionã€ACREå’ŒMiniSCANã€‚åœ¨ä¸‰ä¸ªè¯­è¨€æ¨¡å‹ä¸»å¹²ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒReDisåœ¨æ‰€æœ‰ä»»åŠ¡ä¸Šçš„è¡¨ç°éƒ½è¶…è¿‡äº†ç­‰æ•ˆçš„å°‘é‡æç¤ºåŸºå‡†æµ‹è¯•ï¼Œå¹¶ä¸”åœ¨æŸäº›æƒ…å†µä¸‹ç”šè‡³è¶…è¶Šäº†æ•™å¸ˆæ¨¡å‹GPT-4oã€‚åŸºäºLLaMA-3ä¸»å¹²çš„ReDisåœ¨1D-ARCã€ACREå’ŒMiniSCANä¸Šç›¸å¯¹äºGPT-4oåˆ†åˆ«å®ç°äº†23.2%ã€2.8%å’Œ66.6%çš„ç›¸å¯¹æ”¹è¿›ï¼Œåœ¨ç±»ä¼¼çš„å‡è®¾æœç´¢ç©ºé—´å†…ã€‚ä»£ç ã€æ•°æ®é›†å’Œæ¨¡å‹æ£€æŸ¥ç‚¹å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/NafisSadeq/reasoning-distillation.git%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/NafisSadeq/reasoning-distillation.gitä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10647v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯­è¨€æ¨¡å‹ä¾èµ–è¯­ä¹‰å…ˆéªŒè¿›è¡Œä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œè¿™å¯¼è‡´åœ¨æ¶‰åŠå½’çº³æ¨ç†çš„ä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ã€‚åŸºäºæ¨¡ä»¿å­¦ä¹ çš„æŒ‡ä»¤è°ƒæ•´æ–¹æ³•è™½ç„¶å¯ä»¥è¡¨é¢ä¸Šæé«˜è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ï¼Œä½†å¾€å¾€æ— æ³•æé«˜æ¨¡å‹å¯¹å°‘æ•°æ¼”ç¤ºä¸­è¾“å…¥å’Œè¾“å‡ºä¹‹é—´åŸºç¡€è§„åˆ™çš„ç†è§£ã€‚æœ¬æ–‡æå‡ºä¸€ç§åä¸ºReDisçš„æ¨ç†è’¸é¦æŠ€æœ¯ï¼Œæ—¨åœ¨æé«˜è¯­è¨€æ¨¡å‹çš„å½’çº³æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡æ•°æ®å¢å¼ºã€è¿‡æ»¤ã€ç›‘ç£å¾®è°ƒå’Œå¯¹é½çš„ç²¾å¿ƒè®¾è®¡ï¼ŒReDisåœ¨åŒ…æ‹¬1D-ARCã€List Functionã€ACREå’ŒMiniSCANç­‰å¤šé¡¹ä»»åŠ¡ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚å®éªŒè¡¨æ˜ï¼ŒReDisåœ¨ä¸‰ä¸ªè¯­è¨€æ¨¡å‹ä¸»å¹²ç½‘ä¸Šå‡è¶…è¶ŠåŒç­‰æ°´å¹³çš„å°‘æ ·æœ¬æç¤ºåŸºå‡†çº¿ï¼Œå¹¶åœ¨æŸäº›æƒ…å†µä¸‹ç”šè‡³è¶…è¶Šäº†æ•™å¸ˆæ¨¡å‹GPT-4oã€‚ä»¥LLaMA-3ä¸ºä¸»å¹²ç½‘çš„ReDisåœ¨1D-ARCã€ACREå’ŒMiniSCANä¸Šç›¸å¯¹äºGPT-4oçš„æ”¹è¿›åˆ†åˆ«ä¸º23.2%ã€2.8%å’Œ66.6%ï¼Œä¸”ä½äºç›¸ä¼¼çš„å‡è®¾æœç´¢ç©ºé—´å†…ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­è¨€æ¨¡å‹ä¾èµ–è¯­ä¹‰å…ˆéªŒè¿›è¡Œä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œåœ¨æ¶‰åŠå½’çº³æ¨ç†çš„ä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ã€‚</li>
<li>æŒ‡ä»¤è°ƒæ•´æ–¹æ³•è™½ç„¶èƒ½æé«˜ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ï¼Œä½†éš¾ä»¥æé«˜æ¨¡å‹å¯¹åŸºç¡€è§„åˆ™çš„ç†è§£ã€‚</li>
<li>æå‡ºä¸€ç§åä¸ºReDisçš„æ¨ç†è’¸é¦æŠ€æœ¯ï¼Œé€šè¿‡æ•°æ®å¢å¼ºã€è¿‡æ»¤ã€ç›‘ç£å¾®è°ƒå’Œå¯¹é½æé«˜è¯­è¨€æ¨¡å‹å½’çº³æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ReDisåœ¨å¤šç§ä»»åŠ¡ä¸Šå®ç°æ˜¾è‘—æ€§èƒ½æå‡ï¼ŒåŒ…æ‹¬1D-ARCã€List Functionã€ACREå’ŒMiniSCANã€‚</li>
<li>ReDisåœ¨ä¸åŒè¯­è¨€æ¨¡å‹ä¸»å¹²ç½‘ä¸Šå‡è¶…è¶Šå°‘æ ·æœ¬æç¤ºåŸºå‡†çº¿ï¼Œéƒ¨åˆ†æƒ…å†µä¸‹è¶…è¶Šæ•™å¸ˆæ¨¡å‹GPT-4oã€‚</li>
<li>ä»¥LLaMA-3ä¸ºä¸»å¹²ç½‘çš„ReDisåœ¨ç›¸å¯¹GPT-4oçš„æ”¹è¿›ä¸Šè¾¾åˆ°æ˜¾è‘—æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10647">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f90cda74598c8660c157a905f75b4360.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e9db694b375b0f1be6b93129b108f775.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dc90b54ce525ca22384b9366a9451c56.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f78e453174dceba41599f0b4ce3bd937.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Who-is-More-Bayesian-Humans-or-ChatGPT"><a href="#Who-is-More-Bayesian-Humans-or-ChatGPT" class="headerlink" title="Who is More Bayesian: Humans or ChatGPT?"></a>Who is More Bayesian: Humans or ChatGPT?</h2><p><strong>Authors:Tianshi Mu, Pranjal Rawat, John Rust, Chengjun Zhang, Qixuan Zhong</strong></p>
<p>We compare the performance of human and artificially intelligent (AI) decision makers in simple binary classification tasks where the optimal decision rule is given by Bayes Rule. We reanalyze choices of human subjects gathered from laboratory experiments conducted by El-Gamal and Grether and Holt and Smith. We confirm that while overall, Bayes Rule represents the single best model for predicting human choices, subjects are heterogeneous and a significant share of them make suboptimal choices that reflect judgement biases described by Kahneman and Tversky that include the <code>representativeness heuristic&#39;&#39; (excessive weight on the evidence from the sample relative to the prior) and </code>conservatismâ€™â€™ (excessive weight on the prior relative to the sample). We compare the performance of AI subjects gathered from recent versions of large language models (LLMs) including several versions of ChatGPT. These general-purpose generative AI chatbots are not specifically trained to do well in narrow decision making tasks, but are trained instead as &#96;&#96;language predictorsâ€™â€™ using a large corpus of textual data from the web. We show that ChatGPT is also subject to biases that result in suboptimal decisions. However we document a rapid evolution in the performance of ChatGPT from sub-human performance for early versions (ChatGPT 3.5) to superhuman and nearly perfect Bayesian classifications in the latest versions (ChatGPT 4o). </p>
<blockquote>
<p>æˆ‘ä»¬æ¯”è¾ƒäº†äººç±»å’Œäººå·¥æ™ºèƒ½ï¼ˆAIï¼‰å†³ç­–è€…åœ¨ç®€å•äºŒå…ƒåˆ†ç±»ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå…¶ä¸­æœ€ä½³å†³ç­–è§„åˆ™ç”±è´å¶æ–¯å®šç†ç»™å‡ºã€‚æˆ‘ä»¬å¯¹El-Gamalã€Gretherä»¥åŠ Holtå’ŒSmithä»å®éªŒå®¤å®éªŒä¸­æ”¶é›†çš„äººç±»é€‰æ‹©è¿›è¡Œäº†é‡æ–°åˆ†æã€‚æˆ‘ä»¬ç¡®è®¤ï¼Œè™½ç„¶æ€»ä½“ä¸Šè´å¶æ–¯å®šç†æ˜¯é¢„æµ‹äººç±»é€‰æ‹©çš„æœ€ä½³æ¨¡å‹ï¼Œä½†å—è¯•è€…ä¹‹é—´å­˜åœ¨å¼‚è´¨æ€§ï¼Œç›¸å½“ä¸€éƒ¨åˆ†äººåšå‡ºäº†åæ˜ å¡å†…æ›¼å’Œç‰¹ç»´å°”æ–¯åŸºæ‰€æè¿°çš„åˆ¤æ–­åè§çš„æ¬¡ä¼˜é€‰æ‹©ï¼ŒåŒ…æ‹¬â€œä»£è¡¨æ€§å¯å‘å¼â€ï¼ˆæ ·æœ¬è¯æ®ç›¸å¯¹äºå…ˆéªŒè¯æ®è€Œè¨€è¿‡åº¦é‡è§†ï¼‰å’Œâ€œä¿å®ˆä¸»ä¹‰â€ï¼ˆè¿‡åº¦é‡è§†å…ˆéªŒè¯æ®ç›¸å¯¹äºæ ·æœ¬ï¼‰ã€‚æˆ‘ä»¬æ¯”è¾ƒäº†ä»æœ€è¿‘ç‰ˆæœ¬çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­æ”¶é›†çš„äººå·¥æ™ºèƒ½ä¸»ä½“çš„è¡¨ç°ï¼ŒåŒ…æ‹¬å‡ ä¸ªç‰ˆæœ¬çš„ChatGPTã€‚è¿™äº›é€šç”¨ç”Ÿæˆå¼AIèŠå¤©æœºå™¨äººå¹¶æ²¡æœ‰ä¸“é—¨é’ˆå¯¹ç‹­çª„çš„å†³ç­–ä»»åŠ¡è¿›è¡Œè®­ç»ƒï¼Œè€Œæ˜¯ä½œä¸ºâ€œè¯­è¨€é¢„æµ‹å™¨â€ä½¿ç”¨æ¥è‡ªç½‘ç»œçš„å¤§é‡æ–‡æœ¬æ•°æ®è¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬è¡¨æ˜ChatGPTä¹Ÿå­˜åœ¨å¯¼è‡´æ¬¡ä¼˜å†³ç­–çš„åè§ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬è®°å½•äº†ChatGPTæ€§èƒ½çš„å¿«é€Ÿè¿›åŒ–ï¼Œä»æ—©æœŸç‰ˆæœ¬çš„æ¬¡äººç±»æ€§èƒ½ï¼ˆChatGPT 3.5ï¼‰åˆ°æœ€æ–°ç‰ˆæœ¬çš„è¶…äººç±»å’Œè¿‘ä¹å®Œç¾çš„è´å¶æ–¯åˆ†ç±»ï¼ˆChatGPT 4oï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10636v1">PDF</a> 86 pages, 19 figures</p>
<p><strong>Summary</strong>ï¼šå¯¹æ¯”äººç±»ä¸äººå·¥æ™ºèƒ½åœ¨åŸºäºè´å¶æ–¯è§„åˆ™çš„ç®€å•äºŒå…ƒåˆ†ç±»ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚å‘ç°è™½ç„¶è´å¶æ–¯è§„åˆ™æ˜¯é¢„æµ‹äººç±»é€‰æ‹©çš„æœ€ä½³æ¨¡å‹ï¼Œä½†å­˜åœ¨ä¸ªä½“å·®å¼‚ï¼Œéƒ¨åˆ†äººç±»å­˜åœ¨åˆ¤æ–­åå·®ã€‚åŒæ—¶å‘ç°é€šç”¨ç”Ÿæˆå¼AIèŠå¤©æœºå™¨äººChatGPTä¹Ÿå­˜åœ¨å†³ç­–åå·®é—®é¢˜ï¼Œä½†å…¶æ€§èƒ½åœ¨æœ€æ–°ç‰ˆæœ¬ä¸­å·²æ¥è¿‘ç”šè‡³è¶…è¶Šäººç±»ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¯¹æ¯”äº†äººç±»å’Œäººå·¥æ™ºèƒ½åœ¨äºŒå…ƒåˆ†ç±»ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚</li>
<li>è´å¶æ–¯è§„åˆ™æ˜¯é¢„æµ‹äººç±»é€‰æ‹©çš„æœ€ä½³æ¨¡å‹ã€‚</li>
<li>äººç±»åœ¨å†³ç­–æ—¶å­˜åœ¨ä¸ªä½“å·®å¼‚ï¼Œéƒ¨åˆ†äººä¼šå—åˆ°åˆ¤æ–­åå·®çš„å½±å“ã€‚</li>
<li>ChatGPTç­‰é€šç”¨ç”Ÿæˆå¼AIèŠå¤©æœºå™¨äººåœ¨å†³ç­–ä»»åŠ¡ä¸­ä¹Ÿä¼šè¡¨ç°å‡ºåå·®ã€‚</li>
<li>ChatGPTçš„æ€§èƒ½åœ¨ä¸æ–­å‡çº§ä¸­ï¼Œæœ€æ–°ç‰ˆæœ¬å·²æ¥è¿‘æˆ–è¾¾åˆ°è¶…äººç±»çš„è´å¶æ–¯åˆ†ç±»æ€§èƒ½ã€‚</li>
<li>AIå’Œäººç±»çš„å†³ç­–è¿‡ç¨‹éƒ½å—åˆ°å…ˆå‰ä¿¡æ¯çš„å½±å“ï¼Œä½†è¿‡åº¦ä¾èµ–å…ˆå‰ä¿¡æ¯ä¹Ÿå¯èƒ½å¯¼è‡´åå·®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10636">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f61e5cb80b28606636c7cd9205436e2e.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="InternVL3-Exploring-Advanced-Training-and-Test-Time-Recipes-for-Open-Source-Multimodal-Models"><a href="#InternVL3-Exploring-Advanced-Training-and-Test-Time-Recipes-for-Open-Source-Multimodal-Models" class="headerlink" title="InternVL3: Exploring Advanced Training and Test-Time Recipes for   Open-Source Multimodal Models"></a>InternVL3: Exploring Advanced Training and Test-Time Recipes for   Open-Source Multimodal Models</h2><p><strong>Authors:Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Yuchen Duan, Hao Tian, Weijie Su, Jie Shao, Zhangwei Gao, Erfei Cui, Yue Cao, Yangzhou Liu, Xingguang Wei, Hongjie Zhang, Haomin Wang, Weiye Xu, Hao Li, Jiahao Wang, Dengnian Chen, Songze Li, Yinan He, Tan Jiang, Jiapeng Luo, Yi Wang, Conghui He, Botian Shi, Xingcheng Zhang, Wenqi Shao, Junjun He, Yingtong Xiong, Wenwen Qu, Peng Sun, Penglong Jiao, Han Lv, Lijun Wu, Kaipeng Zhang, Huipeng Deng, Jiaye Ge, Kai Chen, Limin Wang, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, Wenhai Wang</strong></p>
<p>We introduce InternVL3, a significant advancement in the InternVL series featuring a native multimodal pre-training paradigm. Rather than adapting a text-only large language model (LLM) into a multimodal large language model (MLLM) that supports visual inputs, InternVL3 jointly acquires multimodal and linguistic capabilities from both diverse multimodal data and pure-text corpora during a single pre-training stage. This unified training paradigm effectively addresses the complexities and alignment challenges commonly encountered in conventional post-hoc training pipelines for MLLMs. To further improve performance and scalability, InternVL3 incorporates variable visual position encoding (V2PE) to support extended multimodal contexts, employs advanced post-training techniques such as supervised fine-tuning (SFT) and mixed preference optimization (MPO), and adopts test-time scaling strategies alongside an optimized training infrastructure. Extensive empirical evaluations demonstrate that InternVL3 delivers superior performance across a wide range of multi-modal tasks. In particular, InternVL3-78B achieves a score of 72.2 on the MMMU benchmark, setting a new state-of-the-art among open-source MLLMs. Its capabilities remain highly competitive with leading proprietary models, including ChatGPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro, while also maintaining strong pure-language proficiency. In pursuit of open-science principles, we will publicly release both the training data and model weights to foster further research and development in next-generation MLLMs. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†InternVL3ï¼Œè¿™æ˜¯InternVLç³»åˆ—çš„ä¸€ä¸ªé‡å¤§è¿›å±•ï¼Œå®ƒé‡‡ç”¨äº†ä¸€ç§åŸç”Ÿå¤šæ¨¡æ€é¢„è®­ç»ƒèŒƒå¼ã€‚InternVL3å¹¶æ²¡æœ‰å°†ä»…æ–‡æœ¬çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ”¹ç¼–ä¸ºæ”¯æŒè§†è§‰è¾“å…¥çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œè€Œæ˜¯åœ¨å•ä¸ªé¢„è®­ç»ƒé˜¶æ®µä»å¤šæ ·åŒ–çš„å¤šæ¨¡æ€æ•°æ®å’Œçº¯æ–‡æœ¬è¯­æ–™åº“ä¸­åŒæ—¶è·å¾—å¤šæ¨¡æ€å’Œè¯­è¨€èƒ½åŠ›ã€‚è¿™ç§ç»Ÿä¸€çš„è®­ç»ƒèŒƒå¼æœ‰æ•ˆåœ°è§£å†³äº†ä¼ ç»ŸMLLMåå¤„ç†è®­ç»ƒç®¡é“ä¸­å¸¸è§åˆ°çš„å¤æ‚æ€§å’Œå¯¹é½æŒ‘æˆ˜ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜æ€§èƒ½å’Œå¯æ‰©å±•æ€§ï¼ŒInternVL3å¼•å…¥äº†å¯å˜è§†è§‰ä½ç½®ç¼–ç ï¼ˆV2PEï¼‰ä»¥æ”¯æŒæ‰©å±•çš„å¤šæ¨¡æ€ä¸Šä¸‹æ–‡ï¼Œé‡‡ç”¨äº†å…ˆè¿›çš„åè®­ç»ƒæŠ€æœ¯ï¼Œå¦‚ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œæ··åˆåå¥½ä¼˜åŒ–ï¼ˆMPOï¼‰ï¼Œå¹¶é‡‡ç”¨äº†æµ‹è¯•æ—¶ç¼©æ”¾ç­–ç•¥ä»¥åŠä¼˜åŒ–çš„è®­ç»ƒåŸºç¡€è®¾æ–½ã€‚å¤§é‡çš„å®è¯è¯„ä¼°è¡¨æ˜ï¼ŒInternVL3åœ¨å¹¿æ³›çš„å¤šæ¨¡æ€ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚ç‰¹åˆ«æ˜¯ï¼ŒInternVL3-78Båœ¨MMMUåŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†72.2çš„åˆ†æ•°ï¼Œåœ¨å¼€æºMLLMä¸­åˆ›é€ äº†æ–°çš„ä¸–ç•Œçºªå½•ã€‚å…¶èƒ½åŠ›ä¸é¢†å…ˆçš„ä¸“æœ‰æ¨¡å‹ä¿æŒé«˜åº¦ç«äº‰ï¼ŒåŒ…æ‹¬ChatGPT-4oã€Claude 3.5 Sonnetå’ŒGemini 2.5 Proï¼ŒåŒæ—¶ä¿æŒå¼ºå¤§çš„çº¯è¯­è¨€ç†Ÿç»ƒç¨‹åº¦ã€‚æˆ‘ä»¬è¿½æ±‚å¼€æ”¾ç§‘å­¦åŸåˆ™ï¼Œå°†å…¬å¼€å‘å¸ƒè®­ç»ƒæ•°æ®å’Œæ¨¡å‹æƒé‡ï¼Œä»¥ä¿ƒè¿›ä¸‹ä¸€ä»£MLLMçš„ç ”ç©¶å’Œå‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10479v2">PDF</a> Technical Report</p>
<p><strong>æ‘˜è¦</strong><br>    æ¨å‡ºå…¨æ–°å¤šæ¨¡æ€é¢„è®­ç»ƒæ¨¡å‹InternVL3ï¼Œé›†æˆæ–‡æœ¬å’Œè§†è§‰ä¿¡æ¯ï¼Œæ‰“ç ´ä¼ ç»ŸLLMåœ¨è§†è§‰æ”¯æŒé¢†åŸŸçš„å±€é™æ€§ã€‚åœ¨å•ä¸€é¢„è®­ç»ƒé˜¶æ®µå†…èåˆè·¨æ¨¡æ€è¯­æ–™å’Œçº¯æ–‡æœ¬åº“ã€‚æ–°å¼•å…¥V2PEæŠ€æœ¯åŠåè®­ç»ƒæŠ€æœ¯ï¼ˆå¦‚SFTå’ŒMPOï¼‰ï¼Œç»“åˆæµ‹è¯•æ—¶é—´ç­–ç•¥åŠä¼˜åŒ–è®­ç»ƒåŸºç¡€è®¾æ–½ï¼Œå¤§å¹…æå‡å¤šæ¨¡æ€ä»»åŠ¡æ€§èƒ½ã€‚å¯¹æ¯”ä¼—å¤šä¸»æµå¼€æºæ¨¡å‹åŠå¤§å‹æ¨¡å‹æ€§èƒ½å¼ºåŠ²ã€‚è®¡åˆ’å…¬å¼€è®­ç»ƒæ•°æ®å’Œæ¨¡å‹æƒé‡ï¼Œæ¨åŠ¨ä¸‹ä¸€ä»£MLLMç ”ç©¶å‘å±•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ul>
<li>InternVL3æ˜¯é¦–ä¸ªé‡‡ç”¨åŸç”Ÿå¤šæ¨¡æ€é¢„è®­ç»ƒèŒƒå¼çš„æ¨¡å‹ï¼Œå®ç°äº†æ–‡æœ¬å’Œè§†è§‰ä¿¡æ¯çš„é›†æˆã€‚</li>
<li>åœ¨å•ä¸€é¢„è®­ç»ƒé˜¶æ®µå†…èåˆè·¨æ¨¡æ€è¯­æ–™å’Œçº¯æ–‡æœ¬åº“ï¼Œè§£å†³äº†ä¼ ç»ŸLLMå‘MLLMè½¬æ¢ä¸­çš„å¤æ‚æ€§åŠå¯¹é½æŒ‘æˆ˜ã€‚</li>
<li>é‡‡ç”¨V2PEæŠ€æœ¯ä»¥æ”¯æŒæ›´å¹¿æ³›çš„å¤šæ¨¡æ€ä¸Šä¸‹æ–‡ï¼Œç»“åˆå…ˆè¿›çš„åè®­ç»ƒæŠ€æœ¯å’Œæµ‹è¯•æ—¶é—´ç­–ç•¥ï¼Œæé«˜æ€§èƒ½å’Œå¯æ‰©å±•æ€§ã€‚</li>
<li>åœ¨å¹¿æ³›çš„è·¨æ¨¡æ€ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨MMMUåŸºå‡†æµ‹è¯•ä¸­å¾—åˆ†é¢†å…ˆã€‚</li>
<li>ä¸å…¶ä»–ä¸»æµå¼€æºMLLMæ¨¡å‹å’Œå¤§å‹æ¨¡å‹ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10479">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b46fbc4f34a187eea159cbd0dfc37dfa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1b25fb58da24b0eaf6bca0b60aec112.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1adb70d0220bb7a5215393f1048cfd04.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fd170d85bdcc93ac61b4d30eabdedc15.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="GUI-R1-A-Generalist-R1-Style-Vision-Language-Action-Model-For-GUI-Agents"><a href="#GUI-R1-A-Generalist-R1-Style-Vision-Language-Action-Model-For-GUI-Agents" class="headerlink" title="GUI-R1 : A Generalist R1-Style Vision-Language Action Model For GUI   Agents"></a>GUI-R1 : A Generalist R1-Style Vision-Language Action Model For GUI   Agents</h2><p><strong>Authors:Xiaobo Xia, Run Luo</strong></p>
<p>Existing efforts in building Graphical User Interface (GUI) agents largely rely on the training paradigm of supervised fine-tuning on Large Vision-Language Models (LVLMs). However, this approach not only demands extensive amounts of training data but also struggles to effectively understand GUI screenshots and generalize to unseen interfaces. The issue significantly limits its application in real-world scenarios, especially for high-level tasks. Inspired by Reinforcement Fine-Tuning (RFT) in large reasoning models (e.g., DeepSeek-R1), which efficiently enhances the problem-solving capabilities of large language models in real-world settings, we propose \name, the first reinforcement learning framework designed to enhance the GUI capabilities of LVLMs in high-level real-world task scenarios, through unified action space rule modeling. By leveraging a small amount of carefully curated high-quality data across multiple platforms (including Windows, Linux, MacOS, Android, and Web) and employing policy optimization algorithms such as Group Relative Policy Optimization (GRPO) to update the model, \name achieves superior performance using only 0.02% of the data (3K vs. 13M) compared to previous state-of-the-art methods like OS-Atlas across eight benchmarks spanning three different platforms (mobile, desktop, and web). These results demonstrate the immense potential of reinforcement learning based on unified action space rule modeling in improving the execution capabilities of LVLMs for real-world GUI agent tasks. </p>
<blockquote>
<p>å½“å‰æ„å»ºå›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†çš„å·¥ä½œä¸»è¦ä¾èµ–äºåœ¨å¤§è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰ä¸Šè¿›è¡Œç›‘ç£ç²¾ç»†è°ƒæ•´çš„è®­ç»ƒèŒƒå¼ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•ä¸ä»…éœ€æ±‚å¤§é‡çš„è®­ç»ƒæ•°æ®ï¼Œè€Œä¸”åœ¨ç†è§£GUIæˆªå›¾å’Œæ³›åŒ–åˆ°æœªè§è¿‡çš„ç•Œé¢ä¸Šä¹Ÿé¢ä¸´å›°éš¾ã€‚è¿™ä¸€é—®é¢˜æå¤§åœ°é™åˆ¶äº†å…¶åœ¨ç°å®åœºæ™¯ä¸­çš„åº”ç”¨ï¼Œå°¤å…¶æ˜¯é«˜çº§ä»»åŠ¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10458v2">PDF</a> </p>
<p><strong>Summary</strong><br>åŸºäºå¼ºåŒ–å­¦ä¹ çš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹GUIèƒ½åŠ›å¢å¼ºæ¡†æ¶æå‡ºä¸€ç§æ–°å‹çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡ç»Ÿä¸€åŠ¨ä½œç©ºé—´è§„åˆ™å»ºæ¨¡ï¼Œæ—¨åœ¨æé«˜å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨çœŸå®ä¸–ç•Œé«˜çº§ä»»åŠ¡åœºæ™¯ä¸‹çš„GUIèƒ½åŠ›ã€‚è¯¥æ¡†æ¶ä½¿ç”¨å°‘é‡ç²¾å¿ƒæŒ‘é€‰çš„é«˜è´¨é‡è·¨å¹³å°æ•°æ®ï¼Œåˆ©ç”¨ç­–ç•¥ä¼˜åŒ–ç®—æ³•å¯¹æ¨¡å‹è¿›è¡Œæ›´æ–°ï¼Œå¹¶åœ¨å¤šä¸ªå¹³å°çš„å…«ä¸ªåŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†å“è¶Šçš„æ€§èƒ½è¡¨ç°ã€‚è¿™æ˜¾ç¤ºå‡ºå¼ºåŒ–å­¦ä¹ åœ¨ç»Ÿä¸€åŠ¨ä½œç©ºé—´è§„åˆ™å»ºæ¨¡æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥æ–‡æœ¬ä»‹ç»äº†ä¸€ç§æ–°å‹çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨çœŸå®ä¸–ç•Œé«˜çº§ä»»åŠ¡åœºæ™¯ä¸‹çš„GUIèƒ½åŠ›ã€‚</li>
<li>è¯¥æ¡†æ¶é‡‡ç”¨ç»Ÿä¸€åŠ¨ä½œç©ºé—´è§„åˆ™å»ºæ¨¡ï¼Œä»¥æé«˜æ¨¡å‹çš„é€šç”¨æ€§å’Œé€‚åº”æ€§ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨å°‘é‡é«˜è´¨é‡æ•°æ®ï¼Œè¯¥æ¡†æ¶å®ç°äº†å¯¹æ¨¡å‹çš„æ›´æ–°å’Œä¼˜åŒ–ã€‚</li>
<li>è¯¥æ¡†æ¶åœ¨å¤šä¸ªå¹³å°çš„å…«ä¸ªåŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†å“è¶Šçš„æ€§èƒ½è¡¨ç°ã€‚</li>
<li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ¡†æ¶åœ¨æ•°æ®é‡éœ€æ±‚æ–¹é¢å¤§å¹…é™ä½ï¼Œåªéœ€0.02%çš„æ•°æ®å³å¯è¾¾åˆ°ä¼˜è¶Šæ€§èƒ½ã€‚</li>
<li>è¯¥æ¡†æ¶çš„æ½œåŠ›å·¨å¤§ï¼Œå¯ä¸ºæœªæ¥çœŸå®ä¸–ç•ŒGUIä»£ç†ä»»åŠ¡æä¾›å¼ºå¤§çš„æ‰§è¡Œèƒ½åŠ›å’Œæ›´å¹¿æ³›çš„åº”ç”¨èŒƒå›´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10458">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-9a9e0a42308a982366e9279cfb053bbd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-452117247a099f01923cad779a0c3f47.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0979f294f71f855149b4b7942ea5c51d.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="MultiLoKo-a-multilingual-local-knowledge-benchmark-for-LLMs-spanning-31-languages"><a href="#MultiLoKo-a-multilingual-local-knowledge-benchmark-for-LLMs-spanning-31-languages" class="headerlink" title="MultiLoKo: a multilingual local knowledge benchmark for LLMs spanning 31   languages"></a>MultiLoKo: a multilingual local knowledge benchmark for LLMs spanning 31   languages</h2><p><strong>Authors:Dieuwke Hupkes, Nikolay Bogoychev</strong></p>
<p>We present MultiLoKo, a new benchmark for evaluating multilinguality in LLMs covering 31 languages. MultiLoKo consists of three partitions: a main partition consisting of 500 questions per language, separately sourced to be locally relevant to the specific language, and two translated partitions, containing human-authored translations from 30 non-English languages to English and vice versa. For comparison, we also release corresponding machine-authored translations. The data is equally distributed over two splits: a dev split and a blind, out-of-distribution test split. MultiLoKo can be used to study a variety of questions regarding the multilinguality of LLMs as well as meta-questions about multilingual benchmark creation. We compute MultiLoKo scores for 11 base and chat models marketed to be multilingual and study their average performance, their performance parity across languages, how much their ability to answer questions depends on the question language, and which languages are most difficult. None of the models we studied performs well on MultiLoKo, as indicated by low average scores as well as large differences between the best and worst scoring languages. Furthermore, we find a substantial effect of the question language, indicating sub-optimal knowledge transfer between languages. Lastly, we find that using local vs English-translated data can result in differences more than 20 points for the best performing models, drastically change the estimated difficulty of some languages. For using machines instead of human translations, we find a weaker effect on ordering of language difficulty, a larger difference in model rankings, and a substantial drop in estimated performance for all models. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†MultiLoKoï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„è¯„ä¼°LLMå¤šè¯­è¨€èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–31ç§è¯­è¨€ã€‚MultiLoKoç”±ä¸‰ä¸ªåˆ†åŒºç»„æˆï¼šä¸€ä¸ªä¸»åˆ†åŒºï¼ŒåŒ…å«é’ˆå¯¹æ¯ç§è¯­è¨€å•ç‹¬é‡‡é›†çš„500ä¸ªé—®é¢˜ï¼Œè¿™äº›é—®é¢˜ä¸å½“åœ°è¯­è¨€ç›¸å…³ï¼›ä¸¤ä¸ªç¿»è¯‘åˆ†åŒºï¼ŒåŒ…å«ä»30ç§éè‹±è¯­åˆ°è‹±è¯­å’Œä»è‹±è¯­åˆ°è¿™äº›éè‹±è¯­çš„æœºå™¨ç¿»è¯‘å’Œäººç±»ç¿»è¯‘ã€‚ä¸ºäº†è¿›è¡Œæ¯”è¾ƒï¼Œæˆ‘ä»¬è¿˜å‘å¸ƒäº†ç›¸åº”çš„æœºå™¨ç¿»è¯‘ç‰ˆæœ¬ã€‚æ•°æ®è¢«å‡åŒ€åœ°åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼šå¼€å‘é›†å’Œç›²æ€çš„ã€è¶…å‡ºåˆ†å¸ƒèŒƒå›´çš„æµ‹è¯•é›†ã€‚MultiLoKoå¯ç”¨äºç ”ç©¶å…³äºLLMå¤šè¯­è¨€èƒ½åŠ›çš„å„ç§é—®é¢˜ä»¥åŠå…³äºå¤šè¯­è¨€åŸºå‡†æµ‹è¯•åˆ›å»ºçš„å…ƒé—®é¢˜ã€‚æˆ‘ä»¬ä¸ºå¸‚åœºä¸Šå®£ä¼ ä¸ºæ”¯æŒå¤šè¯­è¨€çš„11ä¸ªåŸºç¡€æ¨¡å‹å’ŒèŠå¤©æ¨¡å‹è®¡ç®—äº†MultiLoKoå¾—åˆ†ï¼Œç ”ç©¶äº†å®ƒä»¬çš„å¹³å‡æ€§èƒ½ã€è·¨è¯­è¨€çš„æ€§èƒ½ä¸€è‡´æ€§ã€å®ƒä»¬å›ç­”é—®é¢˜çš„èƒ½åŠ›å¯¹é—®é¢˜è¯­è¨€çš„ä¾èµ–ç¨‹åº¦ï¼Œä»¥åŠå“ªäº›è¯­è¨€æœ€éš¾ã€‚æˆ‘ä»¬æ‰€ç ”ç©¶çš„æ¨¡å‹åœ¨MultiLoKoä¸Šçš„è¡¨ç°å‡ä¸ä½³ï¼Œè¿™ä½“ç°åœ¨å¹³å‡åˆ†æ•°è¾ƒä½ä»¥åŠæœ€ä½³å’Œæœ€å·®å¾—åˆ†è¯­è¨€ä¹‹é—´çš„å·®å¼‚è¾ƒå¤§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°é—®é¢˜è¯­è¨€æœ‰å¾ˆå¤§çš„å½±å“ï¼Œè¡¨æ˜è¯­è¨€ä¹‹é—´çš„çŸ¥è¯†è½¬ç§»å¹¶ä¸ç†æƒ³ã€‚æœ€åï¼Œæˆ‘ä»¬å‘ç°ä½¿ç”¨æœ¬åœ°æ•°æ®ç›¸å¯¹äºè‹±è¯­ç¿»è¯‘æ•°æ®ä¼šå¯¼è‡´æœ€ä½³è¡¨ç°æ¨¡å‹çš„å·®å¼‚è¶…è¿‡20åˆ†ï¼Œè¿™ä¼šæå¤§åœ°æ”¹å˜å¯¹æŸäº›è¯­è¨€éš¾åº¦çš„ä¼°è®¡ã€‚å¦‚æœä½¿ç”¨æœºå™¨ç¿»è¯‘è€Œä¸æ˜¯äººå·¥ç¿»è¯‘ï¼Œæˆ‘ä»¬å‘ç°å¯¹è¯­è¨€éš¾åº¦æ’åºçš„å½±å“è¾ƒå°ï¼Œæ¨¡å‹æ’åå·®å¼‚è¾ƒå¤§ï¼Œå¯¹æ‰€æœ‰æ¨¡å‹çš„é¢„ä¼°æ€§èƒ½æœ‰å¤§å¹…ä¸‹é™ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10356v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æå‡ºäº†ä¸€ç§æ–°çš„è·¨31ç§è¯­è¨€çš„LLMå¤šè¯­è¨€æ€§èƒ½è¯„ä¼°åŸºå‡†ï¼šMultiLoKoã€‚å®ƒåŒ…æ‹¬ä¸‰ä¸ªåˆ†åŒºï¼Œä¸»è¦ç”¨äºè¯„ä¼°LLMçš„å¤šè¯­è¨€èƒ½åŠ›ï¼Œå¹¶åŒ…æ‹¬äººç±»ç¿»è¯‘å’Œæœºå™¨ç¿»è¯‘çš„æ•°æ®ã€‚é€šè¿‡MultiLoKoåŸºå‡†æµ‹è¯•ï¼Œå‘ç°ç°æœ‰LLMæ¨¡å‹åœ¨å¤šè¯­è¨€æ€§èƒ½ä¸Šå­˜åœ¨ä¸è¶³ï¼Œä¸åŒè¯­è¨€é—´çš„çŸ¥è¯†è¿ç§»æ•ˆæœæœ‰å¾…æé«˜ï¼Œä¸”ä½¿ç”¨æœ¬åœ°æ•°æ®ä¸è‹±è¯­ç¿»è¯‘æ•°æ®å¯¹æ¨¡å‹æ€§èƒ½è¯„ä¼°å½±å“è¾ƒå¤§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æå‡ºäº†MultiLoKoåŸºå‡†ï¼Œæ¶µç›–31ç§è¯­è¨€ï¼Œç”¨äºè¯„ä¼°LLMçš„å¤šè¯­è¨€èƒ½åŠ›ã€‚</li>
<li>MultiLoKoåŒ…å«ä¸‰ä¸ªåˆ†åŒºï¼šä¸»åˆ†åŒºå’Œä¸¤ç§ç¿»è¯‘åˆ†åŒºï¼ˆäººç±»ç¿»è¯‘å’Œæœºå™¨ç¿»è¯‘ï¼‰ã€‚</li>
<li>LLMåœ¨å¤šè¯­è¨€æ€§èƒ½ä¸Šè¡¨ç°ä¸è¶³ï¼Œå¹³å‡åˆ†æ•°è¾ƒä½ï¼Œä¸”æœ€ä½³å’Œæœ€å·®è¯­è¨€ä¹‹é—´çš„æ€§èƒ½å·®å¼‚è¾ƒå¤§ã€‚</li>
<li>ä¸åŒè¯­è¨€é—´çš„çŸ¥è¯†è¿ç§»æ•ˆæœæœ‰å¾…æé«˜ï¼Œé—®é¢˜è¯­è¨€å¯¹æ¨¡å‹å›ç­”èƒ½åŠ›æœ‰è¾ƒå¤§å½±å“ã€‚</li>
<li>ä½¿ç”¨æœ¬åœ°æ•°æ®ä¸è‹±è¯­ç¿»è¯‘æ•°æ®å¯¹æ¨¡å‹æ€§èƒ½è¯„ä¼°å½±å“æ˜¾è‘—ï¼Œæœ€ä½³æ¨¡å‹æ€§èƒ½å·®å¼‚è¶…è¿‡20åˆ†ã€‚</li>
<li>ç›¸è¾ƒäºæœºå™¨ç¿»è¯‘ï¼Œäººç±»ç¿»è¯‘å¯¹è¯­è¨€éš¾åº¦æ’åºå½±å“æ›´å¤§ï¼Œæ¨¡å‹æ’åå·®å¼‚æ›´æ˜æ˜¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10356">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-475737c69c1a6860273a826b1c253f80.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a1e7c126be9d1bc1f8d188a2dfa69f23.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c10060aacc343df7a62d6bd31920ebc.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Breaking-the-Data-Barrier-â€“-Building-GUI-Agents-Through-Task-Generalization"><a href="#Breaking-the-Data-Barrier-â€“-Building-GUI-Agents-Through-Task-Generalization" class="headerlink" title="Breaking the Data Barrier â€“ Building GUI Agents Through Task   Generalization"></a>Breaking the Data Barrier â€“ Building GUI Agents Through Task   Generalization</h2><p><strong>Authors:Junlei Zhang, Zichen Ding, Chang Ma, Zijie Chen, Qiushi Sun, Zhenzhong Lan, Junxian He</strong></p>
<p>Graphical User Interface (GUI) agents offer cross-platform solutions for automating complex digital tasks, with significant potential to transform productivity workflows. However, their performance is often constrained by the scarcity of high-quality trajectory data. To address this limitation, we propose training Vision Language Models (VLMs) on data-rich, reasoning-intensive tasks during a dedicated mid-training stage, and then examine how incorporating these tasks facilitates generalization to GUI planning scenarios. Specifically, we explore a range of tasks with readily available instruction-tuning data, including GUI perception, multimodal reasoning, and textual reasoning. Through extensive experiments across 11 mid-training tasks, we demonstrate that: (1) Task generalization proves highly effective, yielding substantial improvements across most settings. For instance, multimodal mathematical reasoning enhances performance on AndroidWorld by an absolute 6.3%. Remarkably, text-only mathematical data significantly boosts GUI web agent performance, achieving a 5.6% improvement on WebArena and 5.4% improvement on AndroidWorld, underscoring notable cross-modal generalization from text-based to visual domains; (2) Contrary to prior assumptions, GUI perception data - previously considered closely aligned with GUI agent tasks and widely utilized for training - has a comparatively limited impact on final performance; (3) Building on these insights, we identify the most effective mid-training tasks and curate optimized mixture datasets, resulting in absolute performance gains of 8.0% on WebArena and 12.2% on AndroidWorld. Our work provides valuable insights into cross-domain knowledge transfer for GUI agents and offers a practical approach to addressing data scarcity challenges in this emerging field. The code, data and models will be available at <a target="_blank" rel="noopener" href="https://github.com/hkust-nlp/GUIMid">https://github.com/hkust-nlp/GUIMid</a>. </p>
<blockquote>
<p>å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†æä¾›è·¨å¹³å°è§£å†³æ–¹æ¡ˆï¼Œç”¨äºè‡ªåŠ¨åŒ–å¤æ‚çš„æ•°å­—ä»»åŠ¡ï¼Œå…·æœ‰æ”¹å˜ç”Ÿäº§åŠ›å·¥ä½œæµç¨‹çš„å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„è¡¨ç°å¾€å¾€å—åˆ°é«˜è´¨é‡è½¨è¿¹æ•°æ®ç¨€ç¼ºçš„åˆ¶çº¦ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºåœ¨ä¸“é—¨çš„ä¸­é—´è®­ç»ƒé˜¶æ®µï¼Œåœ¨æ•°æ®ä¸°å¯Œã€æ¨ç†å¯†é›†çš„ä»»åŠ¡ä¸Šè®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ï¼Œç„¶åç ”ç©¶å¦‚ä½•å°†è¿™äº›ä»»åŠ¡çº³å…¥ä»¥æ¨åŠ¨å¯¹GUIè§„åˆ’åœºæ™¯çš„æ³›åŒ–èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ä¸€ç³»åˆ—å…·æœ‰å¯è·å¾—çš„æŒ‡ä»¤è°ƒæ•´æ•°æ®çš„ä»»åŠ¡ï¼ŒåŒ…æ‹¬GUIæ„ŸçŸ¥ã€å¤šæ¨¡æ€æ¨ç†å’Œæ–‡æœ¬æ¨ç†ã€‚é€šè¿‡å¯¹11ä¸ªä¸­é—´è®­ç»ƒä»»åŠ¡çš„å¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬è¯æ˜ï¼šï¼ˆ1ï¼‰ä»»åŠ¡æ³›åŒ–è¯æ˜éå¸¸æœ‰æ•ˆï¼Œåœ¨å¤§å¤šæ•°è®¾ç½®ä¸‹éƒ½å–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚ä¾‹å¦‚ï¼Œå¤šæ¨¡æ€æ•°å­¦æ¨ç†åœ¨AndroidWorldä¸Šçš„è¡¨ç°æé«˜äº†ç»å¯¹6.3%ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåªæœ‰æ–‡æœ¬çš„æ•°å­¦æ•°æ®æ˜¾è‘—æé«˜äº†GUIç½‘ç»œä»£ç†çš„æ€§èƒ½ï¼Œåœ¨ç½‘ç»œé¢†åŸŸï¼ˆWebArenaï¼‰ä¸Šæé«˜äº†5.6%ï¼Œåœ¨AndroidWorldä¸Šæé«˜äº†5.4%ï¼Œçªæ˜¾äº†ä»æ–‡æœ¬åˆ°è§†è§‰é¢†åŸŸçš„è·¨æ¨¡æ€æ³›åŒ–çš„é‡è¦æ€§ï¼›ï¼ˆ2ï¼‰ä¸å…ˆå‰çš„å‡è®¾ç›¸åï¼ŒGUIæ„ŸçŸ¥æ•°æ®â€”â€”ä¹‹å‰è¢«è®¤ä¸ºä¸GUIä»£ç†ä»»åŠ¡ç´§å¯†ç›¸å…³å¹¶å¹¿æ³›ç”¨äºè®­ç»ƒâ€”â€”å¯¹æœ€ç»ˆæ€§èƒ½çš„å½±å“ç›¸å¯¹æœ‰é™ï¼›ï¼ˆ3ï¼‰åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬ç¡®å®šäº†æœ€æœ‰æ•ˆçš„ä¸­é—´è®­ç»ƒä»»åŠ¡å¹¶ä¼˜åŒ–äº†æ··åˆæ•°æ®é›†ï¼Œåœ¨ç½‘ç»œé¢†åŸŸï¼ˆWebArenaï¼‰ä¸Šå–å¾—äº†ç»å¯¹æ€§èƒ½æå‡8.0%ï¼Œåœ¨AndroidWorldä¸Šå–å¾—äº†ç»å¯¹æ€§èƒ½æå‡12.2%ã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ºGUIä»£ç†çš„è·¨åŸŸçŸ¥è¯†è¿ç§»æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ï¼Œå¹¶ä¸ºè§£å†³è¿™ä¸€æ–°å…´é¢†åŸŸä¸­çš„æ•°æ®ç¨€ç¼ºæŒ‘æˆ˜æä¾›äº†å®ç”¨æ–¹æ³•ã€‚ä»£ç ã€æ•°æ®å’Œæ¨¡å‹å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/hkust-nlp/GUIMid%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/hkust-nlp/GUIMidä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10127v2">PDF</a> 24 pages, 11 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†åœ¨è‡ªåŠ¨åŒ–å¤æ‚æ•°å­—ä»»åŠ¡æ–¹é¢çš„è·¨å¹³å°è§£å†³æ–¹æ¡ˆæ½œåŠ›ï¼Œå¹¶æŒ‡å‡ºå…¶æ€§èƒ½å—é™äºé«˜è´¨é‡è½¨è¿¹æ•°æ®çš„ç¨€ç¼ºæ€§ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºäº†åœ¨ä¸“é—¨çš„ä¸­é—´è®­ç»ƒé˜¶æ®µå¯¹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è¿›è¡Œä¸°å¯Œæ•°æ®ã€æ¨ç†å¯†é›†å‹ä»»åŠ¡çš„è®­ç»ƒï¼Œå¹¶æ¢è®¨äº†è¿™äº›ä»»åŠ¡å¦‚ä½•ä¿ƒè¿›å¯¹GUIè§„åˆ’åœºæ™¯çš„æ³›åŒ–ã€‚å®éªŒè¡¨æ˜ï¼Œä»»åŠ¡æ³›åŒ–æ•ˆæœæ˜¾è‘—ï¼Œå¤šæ•°è®¾ç½®ä¸‹å‡æœ‰å®è´¨æ€§æ”¹å–„ã€‚æ–‡æœ¬æ•°å­¦æ•°æ®å¯¹GUIç½‘é¡µä»£ç†æ€§èƒ½çš„æå‡æ˜¾è‘—ï¼Œè¡¨ç°å‡ºè·¨æ¨¡æ€æ³›åŒ–çš„èƒ½åŠ›ã€‚ç ”ç©¶å›¢é˜Ÿè¿˜ç¡®å®šäº†æœ€æœ‰æ•ˆçš„ä¸­é—´è®­ç»ƒä»»åŠ¡å¹¶ä¼˜åŒ–äº†æ··åˆæ•°æ®é›†ï¼Œå®ç°äº†æ€§èƒ½ä¸Šçš„ç»å¯¹æå‡ã€‚è¯¥ç ”ç©¶ä¸ºGUIä»£ç†çš„è·¨åŸŸçŸ¥è¯†è¿ç§»æä¾›äº†å®è´µè§è§£ï¼Œå¹¶ä¸ºè§£å†³è¯¥é¢†åŸŸçš„æ•°æ®ç¨€ç¼ºæŒ‘æˆ˜æä¾›äº†å®ç”¨æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GUIä»£ç†å…·æœ‰è‡ªåŠ¨åŒ–å¤æ‚æ•°å­—ä»»åŠ¡çš„è·¨å¹³å°è§£å†³æ–¹æ¡ˆæ½œåŠ›ï¼Œå—æ•°æ®é™åˆ¶åˆ¶çº¦ã€‚</li>
<li>æå‡ºäº†åœ¨ä¸“é—¨çš„ä¸­é—´è®­ç»ƒé˜¶æ®µä½¿ç”¨ä¸°å¯Œçš„æ•°æ®å’Œæ¨ç†å¯†é›†å‹ä»»åŠ¡è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ã€‚</li>
<li>ä»»åŠ¡æ³›åŒ–æ˜¾è‘—æé«˜æ€§èƒ½ï¼Œå¹¶åœ¨å¤šæ•°è®¾ç½®ä¸­è¡¨ç°å®è´¨æ€§æ”¹å–„ã€‚æ–‡æœ¬æ•°å­¦æ•°æ®æ˜¾è‘—æå‡GUIç½‘é¡µä»£ç†æ€§èƒ½ã€‚</li>
<li>æœ€æœ‰æ•ˆçš„ä¸­é—´è®­ç»ƒä»»åŠ¡å’Œä¼˜åŒ–åçš„æ··åˆæ•°æ®é›†å¸¦æ¥ç»å¯¹æ€§èƒ½æå‡ã€‚</li>
<li>ç ”ç©¶ç»“æœå¼ºè°ƒè·¨æ¨¡æ€æ³›åŒ–çš„é‡è¦æ€§ï¼Œä¸ºGUIä»£ç†çš„è·¨åŸŸçŸ¥è¯†è¿ç§»æä¾›äº†æœ‰ä»·å€¼è§è§£ã€‚</li>
<li>ç ”ç©¶è§£å†³äº†GUIä»£ç†é¢†åŸŸçš„æ•°æ®ç¨€ç¼ºæŒ‘æˆ˜ï¼Œæä¾›äº†ä¸€ä¸ªå®ç”¨æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10127">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-ec1e01eefff05666c60eb7517204a35a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5e8fca516064e09908f5d82a5493484f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d11e8f3278ea7d538fd56988431440fb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6e6f84dfa434bcbb7c20aa76a5eddc9a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-54162ceb000ca3b11b6105002ff0ac77.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Short-Path-Prompting-in-LLMs-Analyzing-Reasoning-Instability-and-Solutions-for-Robust-Performance"><a href="#Short-Path-Prompting-in-LLMs-Analyzing-Reasoning-Instability-and-Solutions-for-Robust-Performance" class="headerlink" title="Short-Path Prompting in LLMs: Analyzing Reasoning Instability and   Solutions for Robust Performance"></a>Short-Path Prompting in LLMs: Analyzing Reasoning Instability and   Solutions for Robust Performance</h2><p><strong>Authors:Zuoli Tang, Junjie Ou, Kaiqin Hu, Chunwei Wu, Zhaoxin Huan, Chilin Fu, Xiaolu Zhang, Jun Zhou, Chenliang Li</strong></p>
<p>Recent years have witnessed significant progress in large language modelsâ€™ (LLMs) reasoning, which is largely due to the chain-of-thought (CoT) approaches, allowing models to generate intermediate reasoning steps before reaching the final answer. Building on these advances, state-of-the-art LLMs are instruction-tuned to provide long and detailed CoT pathways when responding to reasoning-related questions. However, human beings are naturally cognitive misers and will prompt language models to give rather short responses, thus raising a significant conflict with CoT reasoning. In this paper, we delve into how LLMsâ€™ reasoning performance changes when users provide short-path prompts. The results and analysis reveal that language models can reason effectively and robustly without explicit CoT prompts, while under short-path prompting, LLMsâ€™ reasoning ability drops significantly and becomes unstable, even on grade-school problems. To address this issue, we propose two approaches: an instruction-guided approach and a fine-tuning approach, both designed to effectively manage the conflict. Experimental results show that both methods achieve high accuracy, providing insights into the trade-off between instruction adherence and reasoning accuracy in current models. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œè¿™å¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯ç”±äºæ€ç»´é“¾ï¼ˆCoTï¼‰æ–¹æ³•çš„åº”ç”¨ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨è¾¾åˆ°æœ€ç»ˆç­”æ¡ˆä¹‹å‰äº§ç”Ÿä¸­é—´æ¨ç†æ­¥éª¤ã€‚åŸºäºè¿™äº›è¿›å±•ï¼Œæœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹é€šè¿‡æŒ‡ä»¤è°ƒæ•´ï¼Œèƒ½å¤Ÿåœ¨å›ç­”æ¨ç†ç›¸å…³é—®é¢˜æ—¶æä¾›é•¿å’Œè¯¦ç»†çš„æ€ç»´é“¾è·¯å¾„ã€‚ç„¶è€Œï¼Œäººç±»å¤©ç”Ÿæ˜¯è®¤çŸ¥åå•¬è€…ï¼Œä¼šä¿ƒä½¿è¯­è¨€æ¨¡å‹ç»™å‡ºç›¸å¯¹ç®€çŸ­çš„å›åº”ï¼Œä»è€Œä¸æ€ç»´é“¾æ¨ç†äº§ç”Ÿé‡å¤§å†²çªã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ·±å…¥ç ”ç©¶äº†å½“ç”¨æˆ·æä¾›çŸ­è·¯æç¤ºæ—¶ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†æ€§èƒ½å¦‚ä½•å˜åŒ–ã€‚ç»“æœå’Œåˆ†æè¡¨æ˜ï¼Œè¯­è¨€æ¨¡å‹å¯ä»¥åœ¨æ²¡æœ‰æ˜ç¡®çš„æ€ç»´é“¾æç¤ºçš„æƒ…å†µä¸‹è¿›è¡Œæœ‰æ•ˆçš„æ¨ç†ï¼Œè€Œåœ¨çŸ­è·¯æç¤ºä¸‹ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ä¼šæ˜¾è‘—ä¸‹é™ï¼Œç”šè‡³åœ¨å°å­¦é—®é¢˜ä¸Šä¹Ÿå˜å¾—ä¸ç¨³å®šã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ç§è§£å†³æ–¹æ³•ï¼šæŒ‡ä»¤å¼•å¯¼æ–¹æ³•å’Œå¾®è°ƒæ–¹æ³•ï¼Œè¿™ä¸¤ç§æ–¹æ³•éƒ½æ—¨åœ¨æœ‰æ•ˆåœ°ç®¡ç†å†²çªã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸¤ç§æ–¹æ³•éƒ½å®ç°äº†è¾ƒé«˜çš„å‡†ç¡®æ€§ï¼Œä¸ºå½“å‰æ¨¡å‹ä¸­æŒ‡ä»¤éµå¾ªå’Œæ¨ç†å‡†ç¡®æ€§ä¹‹é—´çš„æƒè¡¡æä¾›äº†è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.09586v1">PDF</a> Under review</p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡é“¾å¼æ€ç»´ï¼ˆCoTï¼‰è¿›è¡Œæ¨ç†æ–¹é¢çš„æ˜¾è‘—è¿›æ­¥å¾—åˆ°äº†å¹¿æ³›è®¤å¯ã€‚ç„¶è€Œï¼Œå½“ç”¨æˆ·ä½¿ç”¨ç®€çŸ­çš„æŒ‡ä»¤æç¤ºæ—¶ï¼ŒLLMçš„æ¨ç†èƒ½åŠ›ä¼šæ˜¾è‘—ä¸‹é™å¹¶å˜å¾—ä¸ç¨³å®šã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶è€…æå‡ºäº†æŒ‡ä»¤å¼•å¯¼æ–¹æ³•å’Œå¾®è°ƒæ–¹æ³•ä¸¤ç§æ–¹æ³•ï¼Œå®éªŒç»“æœè¡¨æ˜è¿™ä¸¤ç§æ–¹æ³•å‡èƒ½æœ‰æ•ˆè§£å†³æŒ‡ä»¤éµå¾ªä¸æ¨ç†å‡†ç¡®æ€§ä¹‹é—´çš„æƒè¡¡é—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMé€šè¿‡CoTè¿›è¡Œæ¨ç†å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚</li>
<li>ç”¨æˆ·ç®€çŸ­æŒ‡ä»¤æç¤ºä¼šå¯¼è‡´LLMæ¨ç†èƒ½åŠ›æ˜¾è‘—ä¸‹é™ã€‚</li>
<li>åœ¨å¤„ç†ç®€å•çš„å¸¸è¯†é—®é¢˜æ—¶ï¼ŒLLMåœ¨ç®€çŸ­æŒ‡ä»¤ä¸‹çš„æ¨ç†èƒ½åŠ›ä¹Ÿä¼šå—åˆ°å½±å“ã€‚</li>
<li>æŒ‡ä»¤å¼•å¯¼æ–¹æ³•å’Œå¾®è°ƒæ–¹æ³•å‡èƒ½æœ‰æ•ˆè§£å†³æŒ‡ä»¤éµå¾ªä¸æ¨ç†å‡†ç¡®æ€§ä¹‹é—´çš„æƒè¡¡é—®é¢˜ã€‚</li>
<li>æŒ‡ä»¤å¼•å¯¼æ–¹æ³•æ³¨é‡å¼•å¯¼LLMéµå¾ªæŒ‡ä»¤çš„åŒæ—¶ä¿æŒæ¨ç†èƒ½åŠ›ã€‚</li>
<li>å¾®è°ƒæ–¹æ³•é€šè¿‡å¯¹æ¨¡å‹çš„ä¼˜åŒ–æ¥æé«˜å…¶é€‚åº”ç®€çŸ­æŒ‡ä»¤çš„èƒ½åŠ›ï¼Œä»è€Œæé«˜æ¨ç†å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.09586">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c0b5509f7c874c05ca190607916fa4bb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f13b874691544f10c79f847177d99a4c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-41bbde1a8c6d72710d80df16e92794d6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cd8e160334179ca52acbf4f3fe917e7a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-49069048dfe79aa5deb5555e039f5a4d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3ef77acc586d03f4b8543ef111ef4a60.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="VideoAds-for-Fast-Paced-Video-Understanding-Where-Opensource-Foundation-Models-Beat-GPT-4o-Gemini-1-5-Pro"><a href="#VideoAds-for-Fast-Paced-Video-Understanding-Where-Opensource-Foundation-Models-Beat-GPT-4o-Gemini-1-5-Pro" class="headerlink" title="VideoAds for Fast-Paced Video Understanding: Where Opensource Foundation   Models Beat GPT-4o &amp; Gemini-1.5 Pro"></a>VideoAds for Fast-Paced Video Understanding: Where Opensource Foundation   Models Beat GPT-4o &amp; Gemini-1.5 Pro</h2><p><strong>Authors:Zheyuan Zhang, Monica Dou, Linkai Peng, Hongyi Pan, Ulas Bagci, Boqing Gong</strong></p>
<p>Advertisement videos serve as a rich and valuable source of purpose-driven information, encompassing high-quality visual, textual, and contextual cues designed to engage viewers. They are often more complex than general videos of similar duration due to their structured narratives and rapid scene transitions, posing significant challenges to multi-modal large language models (MLLMs). In this work, we introduce VideoAds, the first dataset tailored for benchmarking the performance of MLLMs on advertisement videos. VideoAds comprises well-curated advertisement videos with complex temporal structures, accompanied by \textbf{manually} annotated diverse questions across three core tasks: visual finding, video summary, and visual reasoning. We propose a quantitative measure to compare VideoAds against existing benchmarks in terms of video complexity. Through extensive experiments, we find that Qwen2.5-VL-72B, an opensource MLLM, achieves 73.35% accuracy on VideoAds, outperforming GPT-4o (66.82%) and Gemini-1.5 Pro (69.66%); the two proprietary models especially fall behind the opensource model in video summarization and reasoning, but perform the best in visual finding. Notably, human experts easily achieve a remarkable accuracy of 94.27%. These results underscore the necessity of advancing MLLMsâ€™ temporal modeling capabilities and highlight VideoAds as a potentially pivotal benchmark for future research in understanding video that requires high FPS sampling. The dataset and evaluation code will be publicly available at <a target="_blank" rel="noopener" href="https://videoadsbenchmark.netlify.app/">https://videoadsbenchmark.netlify.app</a>. </p>
<blockquote>
<p>å¹¿å‘Šè§†é¢‘ä½œä¸ºç›®çš„é©±åŠ¨ä¿¡æ¯çš„ä¸°å¯Œä¸”å®è´µçš„æ¥æºï¼ŒåŒ…å«äº†ä¸ºå¸å¼•è§‚ä¼—è€Œè®¾è®¡çš„é«˜è´¨é‡è§†è§‰ã€æ–‡æœ¬å’Œä¸Šä¸‹æ–‡çº¿ç´¢ã€‚ç”±äºå®ƒä»¬å…·æœ‰ç»“æ„åŒ–çš„å™äº‹å’Œå¿«é€Ÿçš„åœºæ™¯è½¬æ¢ï¼Œé€šå¸¸æ¯”ç±»ä¼¼æ—¶é•¿çš„æ™®é€šè§†é¢‘æ›´åŠ å¤æ‚ï¼Œç»™å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†VideoAdsï¼Œè¿™æ˜¯ä¸“é—¨ä¸ºè¯„ä¼°å¹¿å‘Šè§†é¢‘ä¸ŠMLLMsæ€§èƒ½è€Œå®šåˆ¶çš„ç¬¬ä¸€ä¸ªæ•°æ®é›†ã€‚VideoAdsåŒ…å«äº†ç²¾å¿ƒæŒ‘é€‰çš„å¹¿å‘Šè§†é¢‘ï¼Œå…·æœ‰å¤æ‚çš„æ—¶åºç»“æ„ï¼Œä»¥åŠé’ˆå¯¹ä¸‰ä¸ªæ ¸å¿ƒä»»åŠ¡çš„æ‰‹åŠ¨æ³¨é‡Šçš„å¤šæ ·åŒ–é—®é¢˜ï¼šè§†è§‰æŸ¥æ‰¾ã€è§†é¢‘æ‘˜è¦å’Œè§†è§‰æ¨ç†ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å®šé‡åº¦é‡æ–¹æ³•ï¼Œå°†VideoAdsä¸ç°æœ‰åŸºå‡†æµ‹è¯•åœ¨è§†é¢‘å¤æ‚åº¦æ–¹é¢è¿›è¡Œæ¯”è¾ƒã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼Œæˆ‘ä»¬å‘ç°å¼€æºMLLM Qwen2.5-VL-72Båœ¨VideoAdsä¸Šè¾¾åˆ°äº†73.35%çš„å‡†ç¡®ç‡ï¼Œè¶…è¿‡äº†GPT-4oï¼ˆ66.82%ï¼‰å’ŒGemini-1.5 Proï¼ˆ69.66%ï¼‰ï¼›è¿™ä¸¤ä¸ªä¸“æœ‰æ¨¡å‹å°¤å…¶åœ¨è§†é¢‘æ‘˜è¦å’Œæ¨ç†æ–¹é¢è½åäºå¼€æºæ¨¡å‹ï¼Œä½†åœ¨è§†è§‰æŸ¥æ‰¾æ–¹é¢è¡¨ç°æœ€ä½³ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œäººç±»ä¸“å®¶å¾ˆå®¹æ˜“è¾¾åˆ°94.27%çš„æ˜¾è‘—å‡†ç¡®ç‡ã€‚è¿™äº›ç»“æœå¼ºè°ƒäº†æé«˜MLLMsæ—¶åºå»ºæ¨¡èƒ½åŠ›çš„å¿…è¦æ€§ï¼Œå¹¶çªå‡ºäº†VideoAdsä½œä¸ºæœªæ¥ç ”ç©¶ç†è§£è§†é¢‘çš„é‡è¦åŸºå‡†æµ‹è¯•ï¼Œè¯¥æµ‹è¯•éœ€è¦é«˜FPSé‡‡æ ·ã€‚æ•°æ®é›†å’Œè¯„ä¼°ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://videoadsbenchmark.netlify.appä¸Šå…¬å¼€å¯ç”¨./">https://videoadsbenchmark.netlify.appä¸Šå…¬å¼€å¯ç”¨ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.09282v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¹¿å‘Šè§†é¢‘æ˜¯ç›®çš„é©±åŠ¨ä¿¡æ¯çš„é‡è¦æ¥æºï¼ŒåŒ…å«é«˜è´¨é‡è§†è§‰ã€æ–‡æœ¬å’Œä¸Šä¸‹æ–‡çº¿ç´¢ï¼Œæ—¨åœ¨å¸å¼•è§‚ä¼—ã€‚ç”±äºå…¶ç»“æ„åŒ–å™äº‹å’Œå¿«é€Ÿåœºæ™¯è½¬æ¢ï¼Œå¯¹å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ„æˆé‡å¤§æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æ¨å‡ºVideoAdsæ•°æ®é›†ï¼Œä¸“ä¸ºè¯„ä¼°MLLMsåœ¨å¹¿å‘Šè§†é¢‘ä¸Šçš„æ€§èƒ½è€Œè®¾è®¡ã€‚VideoAdsåŒ…å«ç²¾å¿ƒæŒ‘é€‰çš„å¹¿å‘Šè§†é¢‘ï¼Œå…·æœ‰å¤æ‚çš„æ—¶é—´ç»“æ„ï¼Œå¹¶è¾…ä»¥æ‰‹åŠ¨æ ‡æ³¨çš„å¤šæ ·æ€§é—®é¢˜ï¼Œæ¶µç›–è§†è§‰å‘ç°ã€è§†é¢‘æ‘˜è¦å’Œè§†è§‰æ¨ç†ä¸‰ä¸ªæ ¸å¿ƒä»»åŠ¡ã€‚é€šè¿‡å¹¿æ³›å®éªŒå‘ç°ï¼Œå¼€æºMLLM Qwen2.5-VL-72Båœ¨VideoAdsä¸Šè¡¨ç°æœ€ä½³ï¼Œå‡†ç¡®ç‡ä¸º73.35%ï¼Œä¼˜äºGPT-4oï¼ˆ66.82%ï¼‰å’ŒGemini-1.5 Proï¼ˆ69.66%ï¼‰ã€‚è¿™äº›ç»“æœçªæ˜¾äº†æé«˜MLLMsçš„æ—¶é—´å»ºæ¨¡èƒ½åŠ›çš„å¿…è¦æ€§ï¼Œå¹¶çªå‡ºäº†VideoAdsä½œä¸ºæœªæ¥ç ”ç©¶çš„å…³é”®åŸºå‡†çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¹¿å‘Šè§†é¢‘åŒ…å«ä¸°å¯Œè§†è§‰ã€æ–‡æœ¬å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ŒæŒ‘æˆ˜å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ã€‚</li>
<li>VideoAdsæ•°æ®é›†ä¸“ä¸ºè¯„ä¼°MLLMsåœ¨å¹¿å‘Šè§†é¢‘ä¸Šçš„æ€§èƒ½è®¾è®¡ï¼ŒåŒ…å«å¤æ‚æ—¶é—´ç»“æ„å’Œæ‰‹åŠ¨æ ‡æ³¨çš„é—®é¢˜ã€‚</li>
<li>VideoAdsæ¶µç›–è§†è§‰å‘ç°ã€è§†é¢‘æ‘˜è¦å’Œè§†è§‰æ¨ç†ä¸‰ä¸ªæ ¸å¿ƒä»»åŠ¡ã€‚</li>
<li>å¼€æºMLLM Qwen2.5-VL-72Båœ¨VideoAdsä¸Šè¡¨ç°æœ€ä½³ï¼Œå‡†ç¡®ç‡ä¸º73.35%ã€‚</li>
<li>GPT-4oå’ŒGemini-1.5 Proåœ¨è§†é¢‘æ‘˜è¦å’Œæ¨ç†æ–¹é¢ç›¸å¯¹è¾ƒå¼±ï¼Œä½†åœ¨è§†è§‰å‘ç°æ–¹é¢è¡¨ç°æœ€ä½³ã€‚</li>
<li>äººä¸“å®¶çš„å‡†ç¡®ç‡é«˜è¾¾94.27%ï¼Œçªæ˜¾äº†æé«˜MLLMsæ—¶é—´å»ºæ¨¡èƒ½åŠ›çš„å¿…è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.09282">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b3b2180d6882d724b3eeb63cf8682945.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ddaa31129259bf09c0cbb01b0004a03d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-464ec5ac70915653fe4b6e3b58823bd8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-614f50404e3daac37ae04deac2ef3d87.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-52551950764345252d6f9ad810dd6f09.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="MDIT-A-Model-free-Data-Interpolation-Method-for-Diverse-Instruction-Tuning"><a href="#MDIT-A-Model-free-Data-Interpolation-Method-for-Diverse-Instruction-Tuning" class="headerlink" title="MDIT: A Model-free Data Interpolation Method for Diverse Instruction   Tuning"></a>MDIT: A Model-free Data Interpolation Method for Diverse Instruction   Tuning</h2><p><strong>Authors:Yangning Li, Zihua Lan, Lv Qingsong, Yinghui Li, Hai-Tao Zheng</strong></p>
<p>As Large Language Models (LLMs) are increasingly applied across various tasks, instruction tuning has emerged as a critical method for enhancing model performance. However, current data management strategies face substantial challenges in generating diverse and comprehensive data, restricting further improvements in model performance. To address this gap, we propose MDIT, a novel model-free data interpolation method for diverse instruction tuning, which generates varied and high-quality instruction data by performing task interpolation. Moreover, it contains diversity-based clustering strategies to ensure the diversity of the training data. Extensive experiments show that our method achieves superior performance in multiple benchmark tasks. The LLMs finetuned with MDIT show significant improvements in numerous tasks such as general question answering, math reasoning, and code generation. MDIT offers an efficient and automatic data synthetic method, generating diverse instruction data without depending on external resources while expanding the application potential of LLMs in complex environments. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„é¡¹ä»»åŠ¡ä¸­çš„åº”ç”¨è¶Šæ¥è¶Šå¹¿æ³›ï¼ŒæŒ‡ä»¤å¾®è°ƒä½œä¸ºä¸€ç§æé«˜æ¨¡å‹æ€§èƒ½çš„å…³é”®æ–¹æ³•åº”è¿è€Œç”Ÿã€‚ç„¶è€Œï¼Œå½“å‰çš„æ•°æ®ç®¡ç†ç­–ç•¥åœ¨ç”Ÿæˆå¤šæ ·ä¸”å…¨é¢çš„æ•°æ®æ–¹é¢é¢ä¸´ç€å·¨å¤§çš„æŒ‘æˆ˜ï¼Œé™åˆ¶äº†æ¨¡å‹æ€§èƒ½çš„è¿›ä¸€æ­¥æé«˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†MDITï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºå¤šæ ·æŒ‡ä»¤å¾®è°ƒçš„æ–°å‹æ— æ¨¡å‹æ•°æ®å†…æ’æ–¹æ³•ã€‚å®ƒé€šè¿‡ä»»åŠ¡å†…æ’ç”Ÿæˆå¤šæ ·ä¸”é«˜è´¨é‡çš„ä»»åŠ¡æŒ‡ä»¤æ•°æ®ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜åŒ…å«åŸºäºå¤šæ ·æ€§çš„èšç±»ç­–ç•¥ï¼Œä»¥ç¡®ä¿è®­ç»ƒæ•°æ®çš„å¤šæ ·æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†ä»»åŠ¡ä¸Šå®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚ä½¿ç”¨MDITå¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šé¡¹ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ˜¾è‘—æ”¹è¿›ï¼Œå¦‚é€šç”¨é—®ç­”ã€æ•°å­¦æ¨ç†å’Œä»£ç ç”Ÿæˆã€‚MDITæä¾›äº†ä¸€ç§é«˜æ•ˆã€è‡ªåŠ¨çš„æ•°æ®åˆæˆæ–¹æ³•ï¼Œèƒ½å¤Ÿç”Ÿæˆå¤šæ ·çš„æŒ‡ä»¤æ•°æ®ï¼Œæ— éœ€ä¾èµ–å¤–éƒ¨èµ„æºï¼Œæ‰©å¤§äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚ç¯å¢ƒä¸­çš„åº”ç”¨æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07288v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åº”ç”¨æ—¥ç›Šå¹¿æ³›ï¼ŒæŒ‡ä»¤å¾®è°ƒä½œä¸ºæå‡æ¨¡å‹æ€§èƒ½çš„å…³é”®æ–¹æ³•é¢ä¸´æ•°æ®å¤šæ ·æ€§åŠå…¨é¢æ€§çš„æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºMDITï¼Œä¸€ç§ç”¨äºå¤šæ ·æŒ‡ä»¤å¾®è°ƒçš„æ— æ¨¡å‹æ•°æ®æ’å€¼æ–¹æ³•ã€‚å®ƒé€šè¿‡ä»»åŠ¡æ’å€¼ç”Ÿæˆå¤šæ ·ä¸”é«˜è´¨é‡çš„ä»»åŠ¡æŒ‡ä»¤æ•°æ®ï¼Œå¹¶å¼•å…¥åŸºäºå¤šæ ·æ€§çš„èšç±»ç­–ç•¥ç¡®ä¿è®­ç»ƒæ•°æ®çš„å¤šæ ·æ€§ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œä½¿ç”¨MDITè¿›è¡Œå¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é€šç”¨é—®ç­”ã€æ•°å­¦æ¨ç†å’Œä»£ç ç”Ÿæˆç­‰å¤šé¡¹ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚MDITæä¾›äº†ä¸€ç§é«˜æ•ˆè‡ªåŠ¨çš„æ•°æ®åˆæˆæ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨ä¸ä¾èµ–å¤–éƒ¨èµ„æºçš„æƒ…å†µä¸‹ç”Ÿæˆå¤šæ ·çš„æŒ‡ä»¤æ•°æ®ï¼Œæ‹“å±•äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚ç¯å¢ƒä¸‹çš„åº”ç”¨æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æŒ‡ä»¤å¾®è°ƒæ˜¯æå‡å¤§å‹è¯­è¨€æ¨¡å‹æ€§èƒ½çš„å…³é”®æ–¹æ³•ã€‚</li>
<li>å½“å‰æ•°æ®ç®¡ç†æ–¹æ³•åœ¨ç”Ÿæˆå¤šæ ·å’Œå…¨é¢çš„æ•°æ®æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>MDITæ˜¯ä¸€ç§æ— æ¨¡å‹æ•°æ®æ’å€¼æ–¹æ³•ï¼Œç”¨äºç”Ÿæˆå¤šæ ·ä¸”é«˜è´¨é‡çš„ä»»åŠ¡æŒ‡ä»¤æ•°æ®ã€‚</li>
<li>MDITé€šè¿‡ä»»åŠ¡æ’å€¼å®ç°æ•°æ®ç”Ÿæˆï¼Œå¹¶å¼•å…¥åŸºäºå¤šæ ·æ€§çš„èšç±»ç­–ç•¥ã€‚</li>
<li>å®éªŒè¯æ˜MDITåœ¨å¤šä¸ªåŸºå‡†ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜è¶Šã€‚</li>
<li>ä½¿ç”¨MDITè¿›è¡Œå¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šé¡¹ä»»åŠ¡ä¸­å–å¾—æ˜¾è‘—æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07288">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d0595e5ff6b553b5b0427a88031efc5b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-02294fca9339cb39fa23eec7735dbd64.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2bf13231fd2f3427da81cdee4c15c69f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bbb052ac495cd40a6d696553b73aa203.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-17/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-17/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-17/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-d1860704f6cde213a9d9d218195fabcd.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-17  The Obvious Invisible Threat LLM-Powered GUI Agents' Vulnerability to   Fine-Print Injections
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-17
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-17/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-68a83c71fd7b3c98241ad990201a9a50.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-17  DeepMath-103K A Large-Scale, Challenging, Decontaminated, and   Verifiable Mathematical Dataset for Advancing Reasoning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-17
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">16663.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
