<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-04-17  Aligning Generative Denoising with Discriminative Objectives Unleashes   Diffusion for Visual Perception">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-c526efb9290b57bfd298d9bcdafe865a.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-04-17
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    16.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    66 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-04-17-更新"><a href="#2025-04-17-更新" class="headerlink" title="2025-04-17 更新"></a>2025-04-17 更新</h1><h2 id="Aligning-Generative-Denoising-with-Discriminative-Objectives-Unleashes-Diffusion-for-Visual-Perception"><a href="#Aligning-Generative-Denoising-with-Discriminative-Objectives-Unleashes-Diffusion-for-Visual-Perception" class="headerlink" title="Aligning Generative Denoising with Discriminative Objectives Unleashes   Diffusion for Visual Perception"></a>Aligning Generative Denoising with Discriminative Objectives Unleashes   Diffusion for Visual Perception</h2><p><strong>Authors:Ziqi Pang, Xin Xu, Yu-Xiong Wang</strong></p>
<p>With the success of image generation, generative diffusion models are increasingly adopted for discriminative tasks, as pixel generation provides a unified perception interface. However, directly repurposing the generative denoising process for discriminative objectives reveals critical gaps rarely addressed previously. Generative models tolerate intermediate sampling errors if the final distribution remains plausible, but discriminative tasks require rigorous accuracy throughout, as evidenced in challenging multi-modal tasks like referring image segmentation. Motivated by this gap, we analyze and enhance alignment between generative diffusion processes and perception tasks, focusing on how perception quality evolves during denoising. We find: (1) earlier denoising steps contribute disproportionately to perception quality, prompting us to propose tailored learning objectives reflecting varying timestep contributions; (2) later denoising steps show unexpected perception degradation, highlighting sensitivity to training-denoising distribution shifts, addressed by our diffusion-tailored data augmentation; and (3) generative processes uniquely enable interactivity, serving as controllable user interfaces adaptable to correctional prompts in multi-round interactions. Our insights significantly improve diffusion-based perception models without architectural changes, achieving state-of-the-art performance on depth estimation, referring image segmentation, and generalist perception tasks. Code available at <a target="_blank" rel="noopener" href="https://github.com/ziqipang/ADDP">https://github.com/ziqipang/ADDP</a>. </p>
<blockquote>
<p>随着图像生成的成功，生成扩散模型越来越多地被用于判别任务，因为像素生成提供了一个统一的感知接口。然而，直接将生成去噪过程用于判别目标会暴露出之前很少解决的关键差距。生成模型如果最终分布仍然可行的话，可以容忍中间采样错误，但判别任务需要始终严格的准确性，如指代图像分割等具有挑战性的多模式任务所证明。受此差距的驱动，我们分析和提高了生成扩散过程和感知任务之间的对齐性，重点关注去噪过程中感知质量如何发展。我们发现：（1）早期的去噪步骤对感知质量的贡献不成比例，促使我们提出反映不同时间步长贡献的定制学习目标；（2）后期的去噪步骤显示出意外的感知退化，突出显示对训练-去噪分布变化的敏感性，这可以通过我们针对扩散定制的数据增强来解决；（3）生成过程具有独特的交互性，可作为可控制的用户界面，适应多轮交互中的纠正提示。我们的见解在不需要架构更改的情况下，显著改进了基于扩散的感知模型，在深度估计、指代图像分割和通用感知任务上实现了最先进的性能。代码可访问 <a target="_blank" rel="noopener" href="https://github.com/ziqipang/ADDP%E3%80%82">https://github.com/ziqipang/ADDP。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11457v1">PDF</a> ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>本文探讨了生成性扩散模型在判别任务中的应用，分析了将生成性去噪过程直接应用于判别目标时存在的问题。针对这些问题，本文重点研究了生成扩散过程与感知任务之间的对齐问题，并发现早期去噪步骤对感知质量的贡献不均等，提出了反映不同时间步长贡献的学习目标。此外，还发现后期去噪步骤会出现意外的感知退化问题，对此本文通过扩散定制的数据增强方法来解决。最后，本文还探讨了生成过程的交互性特点，实现了多轮交互中的纠正提示功能。这些发现有助于提高扩散模型的感知性能，在深度估计、引用图像分割和通用感知任务上实现了最先进的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>生成性扩散模型在判别任务中的应用逐渐增多，但直接将生成性去噪过程用于判别任务存在关键差距。</li>
<li>早期去噪步骤对感知质量的贡献较大，提出需反映不同时间步长贡献的学习目标。</li>
<li>后期去噪步骤可能出现意外的感知退化，这主要是由于训练与去噪分布之间的变化，需要通过扩散定制的数据增强来解决。</li>
<li>生成过程具有交互性特点，适用于多轮交互中的纠正提示。</li>
<li>这些发现显著提高扩散模型的感知性能，实现深度估计、引用图像分割和通用感知任务的先进性能。</li>
<li>相关工作代码已公开在GitHub上共享。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11457">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0849a67bed0f1393e18f2c94587e05c8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0ffe77c99d2cadd63899ccd87c2d03da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3e489b1c58ef31caeac7c4f1fcec9a66.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f704214dfa4357a28861e56f7b250c9.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="ADT-Tuning-Diffusion-Models-with-Adversarial-Supervision"><a href="#ADT-Tuning-Diffusion-Models-with-Adversarial-Supervision" class="headerlink" title="ADT: Tuning Diffusion Models with Adversarial Supervision"></a>ADT: Tuning Diffusion Models with Adversarial Supervision</h2><p><strong>Authors:Dazhong Shen, Guanglu Song, Yi Zhang, Bingqi Ma, Lujundong Li, Dongzhi Jiang, Zhuofan Zong, Yu Liu</strong></p>
<p>Diffusion models have achieved outstanding image generation by reversing a forward noising process to approximate true data distributions. During training, these models predict diffusion scores from noised versions of true samples in a single forward pass, while inference requires iterative denoising starting from white noise. This training-inference divergences hinder the alignment between inference and training data distributions, due to potential prediction biases and cumulative error accumulation. To address this problem, we propose an intuitive but effective fine-tuning framework, called Adversarial Diffusion Tuning (ADT), by stimulating the inference process during optimization and aligning the final outputs with training data by adversarial supervision. Specifically, to achieve robust adversarial training, ADT features a siamese-network discriminator with a fixed pre-trained backbone and lightweight trainable parameters, incorporates an image-to-image sampling strategy to smooth discriminative difficulties, and preserves the original diffusion loss to prevent discriminator hacking. In addition, we carefully constrain the backward-flowing path for back-propagating gradients along the inference path without incurring memory overload or gradient explosion. Finally, extensive experiments on Stable Diffusion models (v1.5, XL, and v3), demonstrate that ADT significantly improves both distribution alignment and image quality. </p>
<blockquote>
<p>扩散模型通过逆转正向噪声过程来近似真实数据分布，从而实现了出色的图像生成。在训练过程中，这些模型在一次前向传递中从真实样本的噪声版本预测扩散分数，而推理则需要从白噪声开始进行迭代去噪。由于潜在的预测偏差和累积误差的累积，这种训练与推理之间的差异阻碍了推理和训练数据分布之间的对齐。为了解决这一问题，我们提出了一种直观而有效的微调框架，称为对抗性扩散调整（ADT），通过优化过程中刺激推理过程，并通过对抗性监督使最终输出与训练数据对齐。具体而言，为了进行稳健的对抗性训练，ADT使用一个带有固定预训练主干和轻量级可训练参数的Siamese网络鉴别器，采用图像到图像的采样策略来平滑鉴别难度，并保留原始扩散损失以防止鉴别器被攻击。此外，我们小心地约束反向传播路径，以便在推理路径上反向传播梯度，而不会导致内存过载或梯度爆炸。最后，对Stable Diffusion模型（v1.5、XL和v3）的大量实验表明，ADT显著提高了分布对齐和图像质量。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11423v1">PDF</a> </p>
<p><strong>摘要</strong><br>    扩散模型通过反转前向噪声过程来近似真实数据分布，实现了出色的图像生成。训练过程中，模型从真实样本的噪声版本预测扩散分数，推理则需要从白噪声开始进行迭代去噪。这种训练与推理的偏差可能导致预测偏差和累积误差的累积，进而影响推理与训练数据分布的对齐。为解决这一问题，我们提出了名为“对抗性扩散调整”（ADT）的微调框架，通过优化过程中刺激推理过程并实现最终输出与训练数据的对齐。ADT采用具有固定预训练主干和轻量级可训练参数的孪生网络鉴别器，采用图像到图像的采样策略来解决鉴别困难，并保留原始扩散损失以防止鉴别器黑客攻击。此外，我们仔细约束反向传播梯度在推理路径上的向后流动路径，避免内存过载或梯度爆炸。在Stable Diffusion模型上的实验表明，ADT显著提高了分布对齐和图像质量。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>扩散模型通过反转前向噪声过程来生成图像。</li>
<li>训练过程中模型预测扩散分数，而推理需要从白噪声迭代去噪。</li>
<li>这种训练与推理的偏差可能影响数据分布的对齐。</li>
<li>提出的ADT框架通过对抗性监督来优化推理与训练数据对齐。</li>
<li>ADT采用孪生网络鉴别器，具有固定预训练主干和轻量级可训练参数。</li>
<li>ADT采用图像到图像的采样策略来解决鉴别困难，并保留原始扩散损失。</li>
<li>实验表明，ADT提高了分布对齐和图像质量。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11423">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-91bd3c36c92d45dd090e7f49ce798103.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c9e0b5e22e82ea29aa78ac0ac078419.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1276a930fa09d1075b37bf26a13ae309.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Autoregressive-Distillation-of-Diffusion-Transformers"><a href="#Autoregressive-Distillation-of-Diffusion-Transformers" class="headerlink" title="Autoregressive Distillation of Diffusion Transformers"></a>Autoregressive Distillation of Diffusion Transformers</h2><p><strong>Authors:Yeongmin Kim, Sotiris Anagnostidis, Yuming Du, Edgar Schönfeld, Jonas Kohler, Markos Georgopoulos, Albert Pumarola, Ali Thabet, Artsiom Sanakoyeu</strong></p>
<p>Diffusion models with transformer architectures have demonstrated promising capabilities in generating high-fidelity images and scalability for high resolution. However, iterative sampling process required for synthesis is very resource-intensive. A line of work has focused on distilling solutions to probability flow ODEs into few-step student models. Nevertheless, existing methods have been limited by their reliance on the most recent denoised samples as input, rendering them susceptible to exposure bias. To address this limitation, we propose AutoRegressive Distillation (ARD), a novel approach that leverages the historical trajectory of the ODE to predict future steps. ARD offers two key benefits: 1) it mitigates exposure bias by utilizing a predicted historical trajectory that is less susceptible to accumulated errors, and 2) it leverages the previous history of the ODE trajectory as a more effective source of coarse-grained information. ARD modifies the teacher transformer architecture by adding token-wise time embedding to mark each input from the trajectory history and employs a block-wise causal attention mask for training. Furthermore, incorporating historical inputs only in lower transformer layers enhances performance and efficiency. We validate the effectiveness of ARD in a class-conditioned generation on ImageNet and T2I synthesis. Our model achieves a $5\times$ reduction in FID degradation compared to the baseline methods while requiring only 1.1% extra FLOPs on ImageNet-256. Moreover, ARD reaches FID of 1.84 on ImageNet-256 in merely 4 steps and outperforms the publicly available 1024p text-to-image distilled models in prompt adherence score with a minimal drop in FID compared to the teacher. Project page: <a target="_blank" rel="noopener" href="https://github.com/alsdudrla10/ARD">https://github.com/alsdudrla10/ARD</a>. </p>
<blockquote>
<p>基于Transformer架构的扩散模型在生成高保真图像和高分辨率方面表现出有前景的能力。然而，合成所需的迭代采样过程非常耗费资源。一系列研究专注于将概率流常微分方程（ODEs）的解决方案蒸馏成少数步骤的学生模型。然而，现有方法受限于它们对最新去噪样本的依赖，使其容易受到暴露偏差的影响。为了解决这一局限性，我们提出了AutoRegressive Distillation（ARD）这一新方法，它利用ODE的历史轨迹来预测未来步骤。ARD有两个主要优点：1）它通过利用预测的历史轨迹（不太容易受累积误差影响）来缓解暴露偏差；2）它利用ODE轨迹的先前历史作为更有效的粗粒度信息来源。ARD通过添加标记每个输入来自轨迹历史的token级时间嵌入来修改教师transformer架构，并采用块级因果注意力掩码进行训练。此外，仅在较低层的transformer中纳入历史输入可提高性能和效率。我们在ImageNet上的类别条件生成和T2I合成上验证了ARD的有效性。与基准方法相比，我们的模型在FID退化方面实现了5倍的减少，同时在ImageNet-256上仅需要额外的1.1% FLOPs。此外，ARD在ImageNet-256上仅需4步即可达到FID为1.84，并且在提示遵循得分方面优于公开可用的1024p文本到图像蒸馏模型，同时与教师的FID相比几乎没有下降。项目页面：<a target="_blank" rel="noopener" href="https://github.com/alsdudrla10/ARD%E3%80%82">https://github.com/alsdudrla10/ARD。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11295v1">PDF</a> CVPR 2025 Oral</p>
<p><strong>摘要</strong><br>基于扩散模型的自回归蒸馏（ARD）方法通过利用ODE历史轨迹来预测未来步骤，解决了迭代采样过程中的资源密集型和暴露偏差问题。ARD通过添加时间嵌入和块级因果注意力掩码来修改教师转换器架构，并将历史输入仅纳入较低层转换器中以提高性能和效率。在ImageNet和T2I合成上的类条件生成验证表明，ARD在减少FID降解方面比基线方法提高了5倍，同时在ImageNet-256上仅需要额外1.1%的浮点运算。此外，ARD在仅有四步的情况下达到了ImageNet-256的FID为1.84，并在提示遵循得分方面超越了公开的1024p文本到图像蒸馏模型。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>扩散模型具有生成高保真图像和良好扩展性的潜力。</li>
<li>迭代采样过程对于图像合成非常资源密集。</li>
<li>当前方法依赖于最近的去噪样本作为输入，容易受到暴露偏差的影响。</li>
<li>提出了一种新的自回归蒸馏（ARD）方法，利用ODE的历史轨迹来预测未来步骤。</li>
<li>ARD通过添加时间嵌入和块级因果注意力掩码修改教师转换器架构。</li>
<li>ARD通过结合历史输入在下层转换器中实现高性能和效率。</li>
<li>ARD在ImageNet和T2I合成上的表现优于基线方法，显著减少了FID降解，并在提示遵循得分方面表现出色。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11295">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0bbd7ec7feb2545633261a494a73a4f8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a7d1abb3ca5bbe8ab3518d443b2bb1ef.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c60f1b996b03ef39ccf19e091a39d527.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8592fe0985903a7cc0655033d1803932.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3987172d96372878d014e8a6415085ec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8eebff7940e18a69adf747a0a199da56.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Taming-Consistency-Distillation-for-Accelerated-Human-Image-Animation"><a href="#Taming-Consistency-Distillation-for-Accelerated-Human-Image-Animation" class="headerlink" title="Taming Consistency Distillation for Accelerated Human Image Animation"></a>Taming Consistency Distillation for Accelerated Human Image Animation</h2><p><strong>Authors:Xiang Wang, Shiwei Zhang, Hangjie Yuan, Yujie Wei, Yingya Zhang, Changxin Gao, Yuehuan Wang, Nong Sang</strong></p>
<p>Recent advancements in human image animation have been propelled by video diffusion models, yet their reliance on numerous iterative denoising steps results in high inference costs and slow speeds. An intuitive solution involves adopting consistency models, which serve as an effective acceleration paradigm through consistency distillation. However, simply employing this strategy in human image animation often leads to quality decline, including visual blurring, motion degradation, and facial distortion, particularly in dynamic regions. In this paper, we propose the DanceLCM approach complemented by several enhancements to improve visual quality and motion continuity at low-step regime: (1) segmented consistency distillation with an auxiliary light-weight head to incorporate supervision from real video latents, mitigating cumulative errors resulting from single full-trajectory generation; (2) a motion-focused loss to centre on motion regions, and explicit injection of facial fidelity features to improve face authenticity. Extensive qualitative and quantitative experiments demonstrate that DanceLCM achieves results comparable to state-of-the-art video diffusion models with a mere 2-4 inference steps, significantly reducing the inference burden without compromising video quality. The code and models will be made publicly available. </p>
<blockquote>
<p>近期人类图像动画技术的进展得益于视频扩散模型。然而，它们依赖于大量的迭代去噪步骤，导致推理成本高昂和速度缓慢。一种直观的解决方案是采用一致性模型，通过一致性蒸馏成为一种有效的加速范式。然而，仅仅在人类图像动画中采用这种策略往往会导致质量下降，包括视觉模糊、运动退化以及面部失真，特别是在动态区域。在本文中，我们提出了DanceLCM方法，并辅以几项增强功能，以在低步骤制度下提高视觉质量和运动连续性：（1）分段一致性蒸馏，辅以辅助的轻量级头，以融入真实视频潜变量的监督信息，缓解由单一全轨迹生成导致的累积误差；（2）以运动为中心的损失函数，专注于运动区域，并显式注入面部保真特征以提高面部真实性。大量的定性和定量实验表明，DanceLCM在仅使用2-4个推理步骤的情况下，就能达到与最先进的视频扩散模型相当的结果，显著减少了推理负担，同时不妥协视频质量。代码和模型将公开发布。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11143v1">PDF</a> </p>
<p><strong>Summary</strong><br>     视频扩散模型推动了人像动画的最新发展，但其依赖于大量的迭代去噪步骤导致推理成本高、速度慢。采用一致性模型作为有效的加速范式可以通过一致性蒸馏来解决这个问题，但简单应用于人像动画往往会导致质量下降，包括视觉模糊、运动退化以及面部失真，特别是在动态区域。本文提出了结合多种改进的DanceLCM方法，在低步状态下提高视觉质量和运动连续性：（1）通过辅助的轻量级头进行分段一致性蒸馏，以融入真实视频潜变量的监督信息，缓解单一全轨迹生成导致的累积误差；（2）以运动为中心的损失聚焦于运动区域，并显式注入面部保真特征以提高面部真实性。大量定性和定量实验表明，DanceLCM仅需2-4步推理即可获得与最先进的视频扩散模型相当的结果，显著降低了推理负担，且不损害视频质量。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视频扩散模型在人像动画领域取得最新进展，但存在推理成本高和速度慢的问题。</li>
<li>一致性模型能有效加速视频扩散模型的推理过程。</li>
<li>单纯采用一致性模型会导致质量下降，包括视觉模糊、运动退化以及面部失真。</li>
<li>DanceLCM方法通过分段一致性蒸馏和面部保真特征的注入来提高视觉质量和运动连续性。</li>
<li>DanceLCM方法在低步状态下实现高速推理，仅需2-4步即可获得与先进模型相当的结果。</li>
<li>DanceLCM方法显著降低了推理负担，同时保持视频质量不损失。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11143">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-0f474d693afc43aeb4dc52df1682b746.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-753e61cd3cd107492f519618c6c1076d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4ff7e842f554322ad6c2fb31838f6dd7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-14501c5741a33c3fb80563b01604d923.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="AnimeDL-2M-Million-Scale-AI-Generated-Anime-Image-Detection-and-Localization-in-Diffusion-Era"><a href="#AnimeDL-2M-Million-Scale-AI-Generated-Anime-Image-Detection-and-Localization-in-Diffusion-Era" class="headerlink" title="AnimeDL-2M: Million-Scale AI-Generated Anime Image Detection and   Localization in Diffusion Era"></a>AnimeDL-2M: Million-Scale AI-Generated Anime Image Detection and   Localization in Diffusion Era</h2><p><strong>Authors:Chenyang Zhu, Xing Zhang, Yuyang Sun, Ching-Chun Chang, Isao Echizen</strong></p>
<p>Recent advances in image generation, particularly diffusion models, have significantly lowered the barrier for creating sophisticated forgeries, making image manipulation detection and localization (IMDL) increasingly challenging. While prior work in IMDL has focused largely on natural images, the anime domain remains underexplored-despite its growing vulnerability to AI-generated forgeries. Misrepresentations of AI-generated images as hand-drawn artwork, copyright violations, and inappropriate content modifications pose serious threats to the anime community and industry. To address this gap, we propose AnimeDL-2M, the first large-scale benchmark for anime IMDL with comprehensive annotations. It comprises over two million images including real, partially manipulated, and fully AI-generated samples. Experiments indicate that models trained on existing IMDL datasets of natural images perform poorly when applied to anime images, highlighting a clear domain gap between anime and natural images. To better handle IMDL tasks in anime domain, we further propose AniXplore, a novel model tailored to the visual characteristics of anime imagery. Extensive evaluations demonstrate that AniXplore achieves superior performance compared to existing methods. Dataset and code can be found in <a target="_blank" rel="noopener" href="https://flytweety.github.io/AnimeDL2M/">https://flytweety.github.io/AnimeDL2M/</a>. </p>
<blockquote>
<p>图像生成领域的最新进展，尤其是扩散模型，极大地降低了创建复杂伪造作品的门槛，使得图像操纵检测与定位（IMDL）越来越具有挑战性。尽管先前在IMDL方面的工作主要集中在自然图像上，但动漫领域仍然被忽视，尽管它越来越容易受到AI生成的伪造作品的威胁。将AI生成的图像误表示为手绘艺术品、版权侵犯和不适当的内容修改对动漫社区和行业构成严重威胁。为了弥补这一空白，我们提出了AnimeDL-2M，这是首个用于动漫IMDL的大规模基准测试，包含全面注释。它包含超过两百万张图像，包括真实、部分操纵和完全AI生成的样本。实验表明，在自然人图像IMDL数据集上训练的模型在应用于动漫图像时表现不佳，这突出了动漫和自然人图像之间的明显领域差距。为了更好地处理动漫领域的IMDL任务，我们进一步提出了AniXplore，这是一个针对动漫图像视觉特征量身定制的新模型。大量评估表明，与现有方法相比，AniXplore实现了卓越的性能。数据集和代码可在<a target="_blank" rel="noopener" href="https://flytweety.github.io/AnimeDL2M/%E6%89%BE%E5%88%B0%E3%80%82">https://flytweety.github.io/AnimeDL2M/找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11015v1">PDF</a> </p>
<p><strong>Summary</strong><br>     近期图像生成技术，特别是扩散模型的发展，使得创建高级伪造作品的门槛大幅降低，使得图像操作检测与定位（IMDL）面临越来越大的挑战。以往的研究主要关注自然图像领域，但动漫领域却被忽略。虽然动漫的虚假AI生成图像易被用作手绘作品诈骗等违法行为，对动漫界和产业构成严重威胁。针对这一问题，推出首个大型动漫IMDL基准测试数据集AnimeDL-2M，含有超过两百万图片样本。研究结果显示，在动漫图像上训练的模型表现不佳，显示动漫与自然图像领域间的明显差距。为此推出针对动漫视觉特性的新模型AniXplore，评估显示其表现优越于现有方法。更多信息可通过<a target="_blank" rel="noopener" href="https://flytweety.github.io/AnimeDL2M/">链接</a>获取。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>扩散模型降低了创建高级伪造作品的难度，使图像操作检测与定位（IMDL）更具挑战性。</li>
<li>动漫领域的IMDL研究相对缺乏，存在巨大的研究空间。</li>
<li>动漫的虚假AI生成图像存在误当作手绘作品的法律风险。</li>
<li>推出首个大型动漫IMDL数据集AnimeDL-2M，含有超过两百万图片样本。</li>
<li>在动漫图像上训练的模型表现欠佳，表明动漫与常规自然图像领域的差距。</li>
<li>提出针对动漫视觉特性的新模型AniXplore。</li>
<li>AniXplore模型相较于现有方法表现优越。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11015">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e1c7b50f1b455178261dd15b14250b4b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8992976fc1aa15fc7f78e4d85c26d0d8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9cac0dc7205cf7fdb6c41056585ab09d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70c499791398dcc15996dee8835fa376.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-29f8d14632ade71f6b529587479d3426.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="TMCIR-Token-Merge-Benefits-Composed-Image-Retrieval"><a href="#TMCIR-Token-Merge-Benefits-Composed-Image-Retrieval" class="headerlink" title="TMCIR: Token Merge Benefits Composed Image Retrieval"></a>TMCIR: Token Merge Benefits Composed Image Retrieval</h2><p><strong>Authors:Chaoyang Wang, Zeyu Zhang, Long Teng, Zijun Li, Shichao Kan</strong></p>
<p>Composed Image Retrieval (CIR) retrieves target images using a multi-modal query that combines a reference image with text describing desired modifications. The primary challenge is effectively fusing this visual and textual information. Current cross-modal feature fusion approaches for CIR exhibit an inherent bias in intention interpretation. These methods tend to disproportionately emphasize either the reference image features (visual-dominant fusion) or the textual modification intent (text-dominant fusion through image-to-text conversion). Such an imbalanced representation often fails to accurately capture and reflect the actual search intent of the user in the retrieval results. To address this challenge, we propose TMCIR, a novel framework that advances composed image retrieval through two key innovations: 1) Intent-Aware Cross-Modal Alignment. We first fine-tune CLIP encoders contrastively using intent-reflecting pseudo-target images, synthesized from reference images and textual descriptions via a diffusion model. This step enhances the encoder ability of text to capture nuanced intents in textual descriptions. 2) Adaptive Token Fusion. We further fine-tune all encoders contrastively by comparing adaptive token-fusion features with the target image. This mechanism dynamically balances visual and textual representations within the contrastive learning pipeline, optimizing the composed feature for retrieval. Extensive experiments on Fashion-IQ and CIRR datasets demonstrate that TMCIR significantly outperforms state-of-the-art methods, particularly in capturing nuanced user intent. </p>
<blockquote>
<p>图像组合检索（CIR）使用多模态查询检索目标图像，该查询将参考图像与描述所需修改的文本相结合。主要挑战是如何有效地融合这种视觉和文本信息。当前用于CIR的跨模态特征融合方法表现出意图解释的固有偏见。这些方法往往过分强调参考图像特征（视觉主导融合）或文本修改意图（通过图像到文本的转换实现文本主导融合）。这种不平衡的表示通常在检索结果中无法准确捕获和反映用户的实际搜索意图。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10995v1">PDF</a> arXiv admin note: text overlap with arXiv:2310.05473 by other authors</p>
<p><strong>Summary</strong></p>
<p>一种融合文本与图像信息的跨模态检索方法。目前CIR存在视觉和文本信息融合的问题，通常会出现过度偏向参考图像或文本描述的偏向性问题，难以准确捕捉用户意图。本文提出的TMCIR框架通过两种关键创新方法解决这一问题：一是意图感知的跨模态对齐，利用扩散模型合成反映意图的伪目标图像对CLIP编码器进行微调；二是自适应令牌融合，通过对比自适应令牌融合特征与目标图像对编码器进行进一步微调。实验证明，TMCIR显著优于当前的主流方法，尤其在于捕捉用户细微意图的能力上。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>CIR利用多模态查询（结合参考图像和描述文本）检索目标图像。</li>
<li>当前CIR方法的挑战在于如何有效融合视觉和文本信息。</li>
<li>存在的方法倾向于偏向参考图像或文本描述，导致无法准确捕捉用户意图。</li>
<li>TMCIR框架通过两种创新方法解决这一问题：意图感知的跨模态对齐和自适应令牌融合。</li>
<li>意图感知的跨模态对齐通过合成反映意图的伪目标图像对CLIP编码器进行微调。</li>
<li>自适应令牌融合通过对比特征与目标图像对编码器进行微调，实现视觉和文本信息的动态平衡。</li>
<li>实验证明，TMCIR在捕捉用户细微意图和检索性能上显著优于当前的主流方法。</li>
<li>TMCIR框架在Fashion-IQ和CIRR数据集上的实验表现优异。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10995">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-9260b82f6a9f0121dacf24a1215fa811.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-47466a58a91dcc6cbd6c2e0d48dc7ce2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c526efb9290b57bfd298d9bcdafe865a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b6c1eacc22f15cb42b848d974e08cef7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-019b0f26132d258921676e52796fe977.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-09811b87c317db7bc2c378475e7d8a5d.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Bringing-together-invertible-UNets-with-invertible-attention-modules-for-memory-efficient-diffusion-models"><a href="#Bringing-together-invertible-UNets-with-invertible-attention-modules-for-memory-efficient-diffusion-models" class="headerlink" title="Bringing together invertible UNets with invertible attention modules for   memory-efficient diffusion models"></a>Bringing together invertible UNets with invertible attention modules for   memory-efficient diffusion models</h2><p><strong>Authors:Karan Jain, Mohammad Nayeem Teli</strong></p>
<p>Diffusion models have recently gained state of the art performance on many image generation tasks. However, most models require significant computational resources to achieve this. This becomes apparent in the application of medical image synthesis due to the 3D nature of medical datasets like CT-scans, MRIs, electron microscope, etc. In this paper we propose a novel architecture for a single GPU memory-efficient training for diffusion models for high dimensional medical datasets. The proposed model is built by using an invertible UNet architecture with invertible attention modules. This leads to the following two contributions: 1. denoising diffusion models and thus enabling memory usage to be independent of the dimensionality of the dataset, and 2. reducing the energy usage during training. While this new model can be applied to a multitude of image generation tasks, we showcase its memory-efficiency on the 3D BraTS2020 dataset leading to up to 15% decrease in peak memory consumption during training with comparable results to SOTA while maintaining the image quality. </p>
<blockquote>
<p>扩散模型最近在许多图像生成任务上达到了最先进的性能。然而，大多数模型为了实现这一点需要大量的计算资源。这在医学图像合成的应用中变得尤为明显，因为医学数据集如CT扫描、MRI、电子显微镜等的三维性质。在本文中，我们提出了一种针对高维医学数据集扩散模型的单GPU内存高效训练的新型架构。该模型采用可逆UNet架构和可逆注意力模块构建。这带来了以下两个贡献：1.降噪扩散模型，从而使内存使用量与数据集维度的无关性成为可能；2.减少训练过程中的能耗。虽然这种新型模型可以应用于多种图像生成任务，但我们展示了其在3D BraTS2020数据集上的内存效率，在训练过程中峰值内存使用率降低了高达15%，同时保持与最新技术相当的结果和图像质量。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10883v1">PDF</a> </p>
<p><strong>Summary</strong><br>     扩散模型在图像生成任务上取得了最新性能表现，但其需要巨大的计算资源。对于医疗图像合成应用而言，由于其涉及高维医学数据集如CT扫描、MRI等，这一问题尤为突出。本文提出了一种针对扩散模型的新型单GPU内存高效训练架构，采用可逆UNet架构和可逆注意力模块，为医疗数据集提供了去噪扩散模型。此模型可应用于多种图像生成任务，展示了其在BraTS数据集上的内存效率，减少了训练过程中的峰值内存使用量，同时在维持图像质量的情况下达到了一流的性能表现。其降低了训练和计算时的能源成本以及改善了可复用性成为关键的两个优势。我们相信扩散模型的轻量级计算解决方案不仅将使现代神经退行性疾病的三维动态成像成为可能，还将为医学研究和诊断带来重大突破。这一突破将为医学领域带来深远的影响。 </p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>扩散模型在高维医疗数据集上表现出卓越性能。针对图像生成任务特别突出。对于高维医学数据集如CT扫描和MRI等，其性能表现尤为显著。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10883">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c59b09a2c867cd62dd6eb2750f75a886.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="PT-Mark-Invisible-Watermarking-for-Text-to-image-Diffusion-Models-via-Semantic-aware-Pivotal-Tuning"><a href="#PT-Mark-Invisible-Watermarking-for-Text-to-image-Diffusion-Models-via-Semantic-aware-Pivotal-Tuning" class="headerlink" title="PT-Mark: Invisible Watermarking for Text-to-image Diffusion Models via   Semantic-aware Pivotal Tuning"></a>PT-Mark: Invisible Watermarking for Text-to-image Diffusion Models via   Semantic-aware Pivotal Tuning</h2><p><strong>Authors:Yaopeng Wang, Huiyu Xu, Zhibo Wang, Jiacheng Du, Zhichao Li, Yiming Li, Qiu Wang, Kui Ren</strong></p>
<p>Watermarking for diffusion images has drawn considerable attention due to the widespread use of text-to-image diffusion models and the increasing need for their copyright protection. Recently, advanced watermarking techniques, such as Tree Ring, integrate watermarks by embedding traceable patterns (e.g., Rings) into the latent distribution during the diffusion process. Such methods disrupt the original semantics of the generated images due to the inevitable distribution shift caused by the watermarks, thereby limiting their practicality, particularly in digital art creation. In this work, we present Semantic-aware Pivotal Tuning Watermarks (PT-Mark), a novel invisible watermarking method that preserves both the semantics of diffusion images and the traceability of the watermark. PT-Mark preserves the original semantics of the watermarked image by gradually aligning the generation trajectory with the original (pivotal) trajectory while maintaining the traceable watermarks during whole diffusion denoising process. To achieve this, we first compute the salient regions of the watermark at each diffusion denoising step as a spatial prior to identify areas that can be aligned without disrupting the watermark pattern. Guided by the region, we then introduce an additional pivotal tuning branch that optimizes the text embedding to align the semantics while preserving the watermarks. Extensive evaluations demonstrate that PT-Mark can preserve the original semantics of the diffusion images while integrating robust watermarks. It achieves a 10% improvement in the performance of semantic preservation (i.e., SSIM, PSNR, and LPIPS) compared to state-of-the-art watermarking methods, while also showing comparable robustness against real-world perturbations and four times greater efficiency. </p>
<blockquote>
<p>针对扩散图像的水印技术得到了广泛关注，这主要是由于文本到图像的扩散模型的广泛应用和对版权保护的不断增长的需求。最近，先进的水印技术，如Tree Ring，通过在扩散过程中将可追踪的模式（例如圆环）嵌入潜在分布来集成水印。这些方法由于水印引起的不可避免的分布偏移而破坏了生成图像的原语意，从而限制了它们的实用性，特别是在数字艺术创作中。在这项工作中，我们提出了语义感知的枢轴调整水印（PT-Mark），这是一种新的不可见水印方法，能够同时保留扩散图像语义和水印的可追踪性。PT-Mark通过在整个扩散去噪过程中保持可追踪的水印，同时逐步调整生成轨迹与原始（枢轴）轨迹对齐，从而保留水印图像的原语意。为了实现这一点，我们首先在每个扩散去噪步骤中计算水印的显著区域作为空间先验，以识别可以不对齐而不会影响水印模式的区域。在该区域的指导下，然后我们引入了一个额外的枢轴调整分支，优化文本嵌入以对齐语义同时保留水印。广泛评估表明，PT-Mark能够在集成稳健水印的同时保留扩散图像的原语意。与最先进的水印方法相比，它在语义保留性能（即结构相似性度量、峰值信噪比和结构相似性度量指数）上提高了10%，同时显示出对现实世界扰动的相当鲁棒性并提高四倍效率。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10853v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>扩散图像水印因其广泛应用于文本到图像的扩散模型及版权保护需求而受到关注。最新的水印技术，如Tree Ring，通过嵌入可追溯模式（例如环形）到扩散过程的潜在分布中进行集成。但这种方法会破坏生成图像的原语义，限制了其实用性。在此，我们提出一种新型不可见水印方法——语义感知关键调整水印（PT-Mark），它既能保留扩散图像语义，又能追溯水印。PT-Mark通过逐步调整生成轨迹与原始（关键）轨迹对齐，同时在整个扩散去噪过程中保持可追溯的水印，从而保留水印图像的原语义。为实现这一目标，我们首先在每个扩散去噪步骤中计算水印的显著区域作为空间先验，以识别可对齐的区域而不干扰水印图案。在该区域的引导下，我们引入了一个额外的关键调整分支，优化文本嵌入以对齐语义同时保留水印。评估表明，PT-Mark在保留扩散图像原语义的同时集成了稳健的水印，在语义保留性能上较现有水印方法提高了10%，同时显示了对现实世界扰动的可比性鲁棒性和四倍效率。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>扩散图像水印因文本-图像扩散模型广泛应用和版权保护需求而受到关注。</li>
<li>现有方法如Tree Ring会破坏生成图像的原语义，限制其实用性。</li>
<li>PT-Mark是一种新型不可见水印方法，能同时保留扩散图像的语义和追溯水印。</li>
<li>PT-Mark通过逐步对齐生成轨迹与原始轨迹，同时保持整个扩散去噪过程中的水印来实现语义保留。</li>
<li>PT-Mark引入空间先验和关键调整分支以提高语义保留和水印的稳健性。</li>
<li>评估显示PT-Mark在语义保留性能上较现有方法有所提升，同时效率更高。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10853">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-c8eff5bc5841efaa1393f66480527004.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-50b0658721b6a93677247e04d42ae815.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-658c04a0563e4a55e8caaecbde0a6840.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5ecd0d79a7cc989cd7cf043ca2a508c4.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="GaSLight-Gaussian-Splats-for-Spatially-Varying-Lighting-in-HDR"><a href="#GaSLight-Gaussian-Splats-for-Spatially-Varying-Lighting-in-HDR" class="headerlink" title="GaSLight: Gaussian Splats for Spatially-Varying Lighting in HDR"></a>GaSLight: Gaussian Splats for Spatially-Varying Lighting in HDR</h2><p><strong>Authors:Christophe Bolduc, Yannick Hold-Geoffroy, Zhixin Shu, Jean-François Lalonde</strong></p>
<p>We present GaSLight, a method that generates spatially-varying lighting from regular images. Our method proposes using HDR Gaussian Splats as light source representation, marking the first time regular images can serve as light sources in a 3D renderer. Our two-stage process first enhances the dynamic range of images plausibly and accurately by leveraging the priors embedded in diffusion models. Next, we employ Gaussian Splats to model 3D lighting, achieving spatially variant lighting. Our approach yields state-of-the-art results on HDR estimations and their applications in illuminating virtual objects and scenes. To facilitate the benchmarking of images as light sources, we introduce a novel dataset of calibrated and unsaturated HDR to evaluate images as light sources. We assess our method using a combination of this novel dataset and an existing dataset from the literature. The code to reproduce our method will be available upon acceptance. </p>
<blockquote>
<p>我们提出了GaSLight方法，该方法可以从常规图像生成空间变化的光照。我们的方法建议使用HDR高斯Splats作为光源表示，这是首次将常规图像作为3D渲染器的光源。我们的两阶段过程首先利用扩散模型中嵌入的先验知识，以合理且准确的方式增强图像的动态范围。接下来，我们采用高斯Splats对3D照明进行建模，实现空间变化的光照。我们的方法在HDR估计及其用于照明虚拟对象和场景的应用方面产生了最先进的成果。为了对图像作为光源进行基准测试，我们引入了一个新型校准和不饱和HDR数据集来评估图像作为光源。我们使用这个新型数据集和文献中的现有数据集来评估我们的方法。接受后将提供复制我们方法的代码。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10809v1">PDF</a> </p>
<p><strong>Summary</strong><br>     我们提出了一种名为GaSLight的方法，该方法可以从常规图像生成空间变化的光照。我们使用HDR高斯Splats作为光源表示，这是首次将常规图像作为光源用于三维渲染。我们的两步过程首先利用扩散模型中的先验知识，以合理且准确的方式增强图像的动态范围。然后，我们使用高斯Splats对三维照明进行建模，实现空间变化的光照。我们的方法在HDR估计及其用于照明虚拟对象和场景方面的应用方面达到了最新水平。为了方便图像作为光源的基准测试，我们引入了一个新型的校准和不饱和HDR数据集来评估图像作为光源。我们使用这个新数据集和文献中的现有数据集来评估我们的方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GaSLight方法能从常规图像生成空间变化的光照。</li>
<li>HDR高斯Splats首次被用作光源表示，用于三维渲染。</li>
<li>该方法采用两步过程，首先增强图像动态范围，然后利用高斯Splats进行三维照明建模。</li>
<li>方法在HDR估计及其应用于虚拟对象和场景的照明方面达到了最新水平。</li>
<li>为了评估图像作为光源的效果，引入了一个新型的校准和不饱和HDR数据集。</li>
<li>该方法结合了新数据集和现有数据集进行评估。</li>
<li>方法代码在接受后将公开可用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10809">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-bd1b6add52d68b6d05cee2576476a970.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a587eee72f338f28224d08dea7b8d954.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-faad0dc0950fb458ed849d09cbe96dc9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b148832e6da27501a698c1717c440602.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-95ce0b37073da87d139eff1cacc105cc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-606d27e2d875c7ceeb31eb0cba8ec175.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="H3AE-High-Compression-High-Speed-and-High-Quality-AutoEncoder-for-Video-Diffusion-Models"><a href="#H3AE-High-Compression-High-Speed-and-High-Quality-AutoEncoder-for-Video-Diffusion-Models" class="headerlink" title="H3AE: High Compression, High Speed, and High Quality AutoEncoder for   Video Diffusion Models"></a>H3AE: High Compression, High Speed, and High Quality AutoEncoder for   Video Diffusion Models</h2><p><strong>Authors:Yushu Wu, Yanyu Li, Ivan Skorokhodov, Anil Kag, Willi Menapace, Sharath Girish, Aliaksandr Siarohin, Yanzhi Wang, Sergey Tulyakov</strong></p>
<p>Autoencoder (AE) is the key to the success of latent diffusion models for image and video generation, reducing the denoising resolution and improving efficiency. However, the power of AE has long been underexplored in terms of network design, compression ratio, and training strategy. In this work, we systematically examine the architecture design choices and optimize the computation distribution to obtain a series of efficient and high-compression video AEs that can decode in real time on mobile devices. We also unify the design of plain Autoencoder and image-conditioned I2V VAE, achieving multifunctionality in a single network. In addition, we find that the widely adopted discriminative losses, i.e., GAN, LPIPS, and DWT losses, provide no significant improvements when training AEs at scale. We propose a novel latent consistency loss that does not require complicated discriminator design or hyperparameter tuning, but provides stable improvements in reconstruction quality. Our AE achieves an ultra-high compression ratio and real-time decoding speed on mobile while outperforming prior art in terms of reconstruction metrics by a large margin. We finally validate our AE by training a DiT on its latent space and demonstrate fast, high-quality text-to-video generation capability. </p>
<blockquote>
<p>自动编码器（AE）是潜扩散模型在图像和视频生成方面成功的关键，可以降低降噪分辨率并提高效率。然而，关于自动编码器的网络设计、压缩比和训练策略等方面的潜力一直被低估。在这项工作中，我们系统地研究了架构设计选择，优化了计算分配，获得了一系列高效的高压缩视频自动编码器，能够在移动设备上实时解码。我们还统一了普通自动编码器和图像条件I2V VAE的设计，在单个网络中实现了多功能性。此外，我们发现广泛采用的判别损失，即GAN、LPIPS和DWT损失，在训练大规模AE时并没有提供显著的改进。我们提出了一种新的潜在一致性损失，它不需要复杂的鉴别器设计或超参数调整，但在重建质量方面提供了稳定的改进。我们的自动编码器在移动设备上实现了超高的压缩比和实时解码速度，同时在重建指标方面大大超越了先前的技术。最后，我们通过在其潜在空间上训练DiT来验证我们的自动编码器，并展示了快速、高质量的文字到视频生成能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10567v1">PDF</a> 8 pages, 4 figures, 6 tables</p>
<p><strong>Summary</strong></p>
<p>自动编码器（AE）是潜在扩散模型在图像和视频生成中成功的关键，它通过降低去噪分辨率并提高效率来实现这一点。然而，关于自动编码器的网络设计、压缩比和训练策略等方面的潜力一直被忽视。在这项工作中，我们系统地研究了架构设计的选择，优化了计算分布，获得了一系列高效且高压缩的视频自动编码器，能够在移动设备上实时解码。我们还统一了普通自动编码器和图像条件I2V VAE的设计，实现单一网络的多功能性。此外，我们发现广泛采用的判别损失（如GAN、LPIPS和DWT损失）在训练大规模AE时并未提供显著改进。我们提出了一种新型潜在一致性损失，该损失不需要复杂的鉴别器设计或超参数调整，但在重建质量上提供了稳定的改进。我们的自动编码器在具有超高压缩比和实时解码速度的同时，在重建指标上大幅度超越了先前技术。最后，我们通过在其潜在空间上训练DiT来验证我们的自动编码器，并展示了快速、高质量的文字到视频生成能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自动编码器（AE）是潜在扩散模型成功的关键，尤其在图像和视频生成领域。</li>
<li>通过优化网络设计、压缩比和训练策略，提高了视频自动编码器的效率和性能。</li>
<li>实现了一种实时解码的移动设备上的高效、高压缩视频自动编码器。</li>
<li>融合了普通自动编码器和图像条件I2V VAE的设计，增强了单一网络的多功能性。</li>
<li>传统的判别损失在训练大规模AE时效果有限。</li>
<li>提出了一种新型潜在一致性损失，能有效提升自动编码器的重建质量。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10567">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-0d721cc0ecdc88fab6d4fdd4030db761.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b64eda8b08a1cfa35bf5f1e7b769ba17.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4bd1bf90ffa5d1916a3438862d0ac1d3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c8fadabc7dd7fe57d1e65f5c5b403661.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b59dc8846a299cc1af237a8292d44155.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9ab7725147cabe8642890df1add6fa2b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7cd7d5d1002b3e29ca67d96b661ca801.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="OctGPT-Octree-based-Multiscale-Autoregressive-Models-for-3D-Shape-Generation"><a href="#OctGPT-Octree-based-Multiscale-Autoregressive-Models-for-3D-Shape-Generation" class="headerlink" title="OctGPT: Octree-based Multiscale Autoregressive Models for 3D Shape   Generation"></a>OctGPT: Octree-based Multiscale Autoregressive Models for 3D Shape   Generation</h2><p><strong>Authors:Si-Tong Wei, Rui-Huan Wang, Chuan-Zhi Zhou, Baoquan Chen, Peng-Shuai Wang</strong></p>
<p>Autoregressive models have achieved remarkable success across various domains, yet their performance in 3D shape generation lags significantly behind that of diffusion models. In this paper, we introduce OctGPT, a novel multiscale autoregressive model for 3D shape generation that dramatically improves the efficiency and performance of prior 3D autoregressive approaches, while rivaling or surpassing state-of-the-art diffusion models. Our method employs a serialized octree representation to efficiently capture the hierarchical and spatial structures of 3D shapes. Coarse geometry is encoded via octree structures, while fine-grained details are represented by binary tokens generated using a vector quantized variational autoencoder (VQVAE), transforming 3D shapes into compact multiscale binary sequences suitable for autoregressive prediction. To address the computational challenges of handling long sequences, we incorporate octree-based transformers enhanced with 3D rotary positional encodings, scale-specific embeddings, and token-parallel generation schemes. These innovations reduce training time by 13 folds and generation time by 69 folds, enabling the efficient training of high-resolution 3D shapes, e.g.,$1024^3$, on just four NVIDIA 4090 GPUs only within days. OctGPT showcases exceptional versatility across various tasks, including text-, sketch-, and image-conditioned generation, as well as scene-level synthesis involving multiple objects. Extensive experiments demonstrate that OctGPT accelerates convergence and improves generation quality over prior autoregressive methods, offering a new paradigm for high-quality, scalable 3D content creation. Our code and trained models are available at <a target="_blank" rel="noopener" href="https://github.com/octree-nn/octgpt">https://github.com/octree-nn/octgpt</a>. </p>
<blockquote>
<p>自回归模型已经在多个领域取得了显著的成功，但在3D形状生成方面的性能却远远落后于扩散模型。在本文中，我们介绍了OctGPT，这是一种用于3D形状生成的新型多尺度自回归模型，它大大提高了先前3D自回归方法的效率和性能，同时与最先进的扩散模型相匹敌甚至超越。我们的方法采用序列化八叉树表示形式，以有效地捕获3D形状的层次结构和空间结构。粗几何结构通过八叉树结构进行编码，而精细细节则由使用向量量化变分自动编码器（VQVAE）生成的二进制令牌表示，将3D形状转换为适合自回归预测的多尺度二进制序列。为了解决处理长序列的计算挑战，我们结合了基于八叉树的变压器，并增强了3D旋转位置编码、尺度特定嵌入和令牌并行生成方案。这些创新将训练时间缩短了13倍，生成时间缩短了69倍，使得在仅四台NVIDIA 4090 GPU上在几天内对高分辨率3D形状（例如1024^3）进行有效训练成为可能。OctGPT在各种任务中展示了出色的通用性，包括文本、草图和图像条件下的生成，以及涉及多个对象的场景级别合成。大量实验表明，OctGPT加速了收敛，提高了生成质量，为高质量、可扩展的3D内容创建提供了新的范式。我们的代码和训练好的模型可在<a target="_blank" rel="noopener" href="https://github.com/octree-nn/octgpt%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/octree-nn/octgpt上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.09975v2">PDF</a> SIGGRAPH 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了OctGPT模型，这是一种用于三维形状生成的新型多尺度自回归模型。该模型通过采用序列化八叉树表示法，有效捕捉三维形状的层次和空间结构，改进了先前的三维自回归方法的效率和性能，同时与最先进的扩散模型相抗衡或更胜一筹。OctGPT具有出色的多任务适应性，包括文本、草图、图像条件生成以及涉及多个对象的场景级别合成。它通过创新的技术手段，如基于八叉树的变压器、三维旋转位置编码、尺度特定嵌入和令牌并行生成方案，解决了处理长序列的计算挑战。该模型为高质量、可伸缩的三维内容创建提供了新的范例。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OctGPT是一个用于3D形状生成的多尺度自回归模型，显著提高了先前自回归方法的效率和性能。</li>
<li>OctGPT采用了序列化八叉树表示法，有效捕捉了3D形状的层次和空间结构。</li>
<li>OctGPT可与最先进的扩散模型相抗衡或表现更佳。</li>
<li>OctGPT具有出色的多任务适应性，包括文本、草图、图像条件生成和场景级别合成。</li>
<li>通过采用基于八叉树的变压器等创新技术，解决了处理长序列的计算挑战。</li>
<li>OctGPT实现了高效的训练，能够在仅四天内在四台NVIDIA 4090 GPU上训练高分辨率（例如1024^3）的3D形状。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.09975">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-510495a30a76a8367e0998c6e8924c97.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee13c13531eb57544bc391d2986841b8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5797e9b6bff5f87db57e40e7fd37c868.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5758c207f83189b9417bd59194aa3ec8.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="COP-GEN-Beta-Unified-Generative-Modelling-of-COPernicus-Imagery-Thumbnails"><a href="#COP-GEN-Beta-Unified-Generative-Modelling-of-COPernicus-Imagery-Thumbnails" class="headerlink" title="COP-GEN-Beta: Unified Generative Modelling of COPernicus Imagery   Thumbnails"></a>COP-GEN-Beta: Unified Generative Modelling of COPernicus Imagery   Thumbnails</h2><p><strong>Authors:Miguel Espinosa, Valerio Marsocci, Yuru Jia, Elliot J. Crowley, Mikolaj Czerkawski</strong></p>
<p>In remote sensing, multi-modal data from various sensors capturing the same scene offers rich opportunities, but learning a unified representation across these modalities remains a significant challenge. Traditional methods have often been limited to single or dual-modality approaches. In this paper, we introduce COP-GEN-Beta, a generative diffusion model trained on optical, radar, and elevation data from the Major TOM dataset. What sets COP-GEN-Beta apart is its ability to map any subset of modalities to any other, enabling zero-shot modality translation after training. This is achieved through a sequence-based diffusion transformer, where each modality is controlled by its own timestep embedding. We extensively evaluate COP-GEN-Beta on thumbnail images from the Major TOM dataset, demonstrating its effectiveness in generating high-quality samples. Qualitative and quantitative evaluations validate the model’s performance, highlighting its potential as a powerful pre-trained model for future remote sensing tasks. </p>
<blockquote>
<p>在遥感领域，从不同传感器捕捉同一场景的多模态数据提供了丰富的机会，但在这些模态上学习统一表征仍是一个巨大的挑战。传统方法通常局限于单一或双模态方法。在本文中，我们介绍了COP-GEN-Beta，这是一个在Major TOM数据集的光学、雷达和海拔数据上训练的生成型扩散模型。COP-GEN-Beta的特殊之处在于，它能够将任何模态子集映射到其他模态，实现训练后的零样本模态转换。这是通过基于序列的扩散变压器实现的，其中每个模态由自己的时间步长嵌入控制。我们在Major TOM数据集的缩略图图像上对COP-GEN-Beta进行了广泛评估，证明了它在生成高质量样本方面的有效性。定性和定量评估验证了该模型的性能，突显了其在未来遥感任务中作为强大预训练模型的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08548v2">PDF</a> Accepted at CVPR 2025 Workshop MORSE</p>
<p><strong>Summary</strong></p>
<p>多模态数据在遥感领域提供了丰富的机会，但学习跨模态的统一表示仍然是一个挑战。本文介绍了一种名为COP-GEN-Beta的生成型扩散模型，该模型能够在Major TOM数据集的光学、雷达和地形数据上进行训练。COP-GEN-Beta的突出特点是能够实现任意模态子集之间的映射，训练后能够实现零样本模态转换。这通过基于序列的扩散变换器实现，每个模态由自己的时间步长嵌入控制。在Major TOM数据集缩略图上的广泛评估证明了COP-GEN-Beta生成高质量样本的有效性。该模型表现出色，有望成为未来遥感任务的强大预训练模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态数据在遥感中具有丰富机会，但跨模态统一表示学习具有挑战性。</li>
<li>COP-GEN-Beta是一种生成型扩散模型，能在多种模态数据上进行训练。</li>
<li>COP-GEN-Beta能够实现任意模态子集之间的映射，实现零样本模态转换。</li>
<li>通过序列扩散变换器实现模型，每个模态具有自己的时间步长嵌入控制。</li>
<li>在Major TOM数据集缩略图上的评估证明了COP-GEN-Beta生成高质量样本的能力。</li>
<li>模型的性能通过定性和定量评估得到验证。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08548">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4c05634df87f5c36cac77c259973a745.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef5906db11a65bfe599ee2788ea14a33.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c7675003f5763fde5e7dd7089fd5379.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="CyclePose-–-Leveraging-Cycle-Consistency-for-Annotation-Free-Nuclei-Segmentation-in-Fluorescence-Microscopy"><a href="#CyclePose-–-Leveraging-Cycle-Consistency-for-Annotation-Free-Nuclei-Segmentation-in-Fluorescence-Microscopy" class="headerlink" title="CyclePose – Leveraging Cycle-Consistency for Annotation-Free Nuclei   Segmentation in Fluorescence Microscopy"></a>CyclePose – Leveraging Cycle-Consistency for Annotation-Free Nuclei   Segmentation in Fluorescence Microscopy</h2><p><strong>Authors:Jonas Utz, Stefan Vocht, Anne Tjorven Buessen, Dennis Possart, Fabian Wagner, Mareike Thies, Mingxuan Gu, Stefan Uderhardt, Katharina Breininger</strong></p>
<p>In recent years, numerous neural network architectures specifically designed for the instance segmentation of nuclei in microscopic images have been released. These models embed nuclei-specific priors to outperform generic architectures like U-Nets; however, they require large annotated datasets, which are often not available. Generative models (GANs, diffusion models) have been used to compensate for this by synthesizing training data. These two-stage approaches are computationally expensive, as first a generative model and then a segmentation model has to be trained. We propose CyclePose, a hybrid framework integrating synthetic data generation and segmentation training. CyclePose builds on a CycleGAN architecture, which allows unpaired translation between microscopy images and segmentation masks. We embed a segmentation model into CycleGAN and leverage a cycle consistency loss for self-supervision. Without annotated data, CyclePose outperforms other weakly or unsupervised methods on two public datasets. Code is available at <a target="_blank" rel="noopener" href="https://github.com/jonasutz/CyclePose">https://github.com/jonasutz/CyclePose</a> </p>
<blockquote>
<p>近年来，专为显微图像中的细胞核实例分割设计的神经网络架构已经陆续发布。这些模型会嵌入特定的细胞核先验知识，以超越U-Net等通用架构的性能；然而，它们需要大量标注数据集，而这些数据通常并不可用。生成模型（GANs、扩散模型）已被用来通过合成训练数据来弥补这一缺陷。这些两阶段的方法计算成本高昂，因为首先需要训练一个生成模型，然后训练一个分割模型。我们提出了CyclePose，这是一个集成合成数据生成和分割训练于一体的混合框架。CyclePose基于CycleGAN架构，允许显微图像和分割掩膜之间的无配对转换。我们将分割模型嵌入到CycleGAN中，并利用循环一致性损失进行自监督。无需标注数据，CyclePose在两个公共数据集上的表现优于其他弱监督或无监督方法。相关代码可通过<a target="_blank" rel="noopener" href="https://github.com/jonasutz/CyclePose%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/jonasutz/CyclePose获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.11266v2">PDF</a> under review for MICCAI 2025</p>
<p><strong>摘要</strong></p>
<p>近期发布多种专为微观图像核实例分割设计的神经网络架构，这些模型通过嵌入核特异性先验知识，性能优于通用架构（如U-Net），但它们需要大量标注数据集，而这通常难以获取。生成模型（GANs，扩散模型）已用于合成训练数据以弥补这一缺陷。这些两阶段方法计算成本高昂，需先训练生成模型，再训练分割模型。我们提出CyclePose，一个集成合成数据生成和分割训练的混合框架。CyclePose基于CycleGAN架构，实现显微镜图像和分割掩膜之间的无配对转换。我们在CycleGAN中嵌入分割模型，并利用循环一致性损失实现自监督。无需标注数据，CyclePose在公开数据集上的表现优于其他弱监督或无监督方法。相关代码已发布至<a target="_blank" rel="noopener" href="https://github.com/jonasutz/CyclePose%E3%80%82">https://github.com/jonasutz/CyclePose。</a></p>
<p><strong>关键见解</strong></p>
<ol>
<li>近年出现多种专为微观图像核实例分割设计的神经网络架构，但需求大量标注数据。</li>
<li>生成模型（如GANs和扩散模型）已被用于合成训练数据以克服数据缺乏的问题。</li>
<li>当前方法多为两阶段，计算成本高昂，需分别训练生成模型和分割模型。</li>
<li>提出CyclePose混合框架，整合合成数据生成和分割训练。</li>
<li>CyclePose基于CycleGAN架构，实现图像和分割掩膜之间的无配对转换。</li>
<li>CyclePose在无需标注数据的情况下，公开数据集上的表现优于其他弱监督或无监督方法。</li>
<li>CyclePose的相关代码已发布至GitHub供公众访问。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.11266">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-92d24cad787164b85359f5cee761086d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-91dc9dfebf13ac8bec1e5481171d74f7.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="RORem-Training-a-Robust-Object-Remover-with-Human-in-the-Loop"><a href="#RORem-Training-a-Robust-Object-Remover-with-Human-in-the-Loop" class="headerlink" title="RORem: Training a Robust Object Remover with Human-in-the-Loop"></a>RORem: Training a Robust Object Remover with Human-in-the-Loop</h2><p><strong>Authors:Ruibin Li, Tao Yang, Song Guo, Lei Zhang</strong></p>
<p>Despite the significant advancements, existing object removal methods struggle with incomplete removal, incorrect content synthesis and blurry synthesized regions, resulting in low success rates. Such issues are mainly caused by the lack of high-quality paired training data, as well as the self-supervised training paradigm adopted in these methods, which forces the model to in-paint the masked regions, leading to ambiguity between synthesizing the masked objects and restoring the background. To address these issues, we propose a semi-supervised learning strategy with human-in-the-loop to create high-quality paired training data, aiming to train a Robust Object Remover (RORem). We first collect 60K training pairs from open-source datasets to train an initial object removal model for generating removal samples, and then utilize human feedback to select a set of high-quality object removal pairs, with which we train a discriminator to automate the following training data generation process. By iterating this process for several rounds, we finally obtain a substantial object removal dataset with over 200K pairs. Fine-tuning the pre-trained stable diffusion model with this dataset, we obtain our RORem, which demonstrates state-of-the-art object removal performance in terms of both reliability and image quality. Particularly, RORem improves the object removal success rate over previous methods by more than 18%. The dataset, source code and trained model are available at <a target="_blank" rel="noopener" href="https://github.com/leeruibin/RORem">https://github.com/leeruibin/RORem</a>. </p>
<blockquote>
<p>尽管取得了重大进展，现有的物体移除方法仍存在困扰，例如去除不完全、内容合成不正确以及合成区域模糊，导致成功率较低。这些问题主要是由于缺乏高质量配对训练数据以及这些方法所采用的自监督训练范式导致的。自监督训练范式迫使模型对遮罩区域进行填充，导致合成遮罩物体和恢复背景之间产生模糊。为了解决这些问题，我们提出了一种半监督学习策略，并引入人类参与来创建高质量配对训练数据，旨在训练一个稳健物体移除器（RORem）。我们首先从公开数据源收集6万组训练配对数据，用于训练初始物体移除模型以生成移除样本，然后利用人类反馈选择一组高质量的物体移除配对数据，并使用这些数据训练一个鉴别器，以自动化后续的训练数据生成过程。经过几轮迭代后，我们最终获得了一个包含超过20万组配对数据的物体移除数据集。使用此数据集对预训练的稳定扩散模型进行微调，我们得到了RORem，它在可靠性和图像质量方面实现了最先进的物体移除性能。特别地，RORem将物体移除成功率提高了超过18%。数据集、源代码和训练好的模型均可在<a target="_blank" rel="noopener" href="https://github.com/leeruibin/RORem">https://github.com/leeruibin/RORem</a>上找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.00740v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种半监督学习策略，结合人工参与，创建高质量配对训练数据，旨在训练一个稳健的对象移除器（RORem）。通过迭代收集训练样本和人类反馈选择高质量的数据对，训练判别器自动化后续的训练数据生成过程。使用此数据集微调预训练的稳定扩散模型，获得具有先进性能的RORem，在可靠性和图像质量方面均表现优异，对象移除成功率较之前的方法提高了18%以上。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>现有对象移除方法存在不完整移除、内容合成不正确和合成区域模糊等问题，导致成功率低。</li>
<li>问题主要源于缺乏高质量配对训练数据和自监督训练范式。</li>
<li>提出一种半监督学习策略，结合人工参与创建高质量配对训练数据，以训练稳健的对象移除器（RORem）。</li>
<li>通过迭代收集训练样本和人类反馈选择数据对，训练判别器自动化后续数据生成。</li>
<li>RORem在可靠性和图像质量方面表现优异，对象移除成功率较之前的方法提高18%以上。</li>
<li>RORem模型、数据集和源代码均可在指定GitHub仓库中找到。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.00740">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-bd5ab457d363dd6072bf068057b4d076.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-60878675ad0033dc47bb8471462cf934.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f722f4161e3bb58b6c1261c619403de0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c48cd5acb064bff8293616686cf7ff8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-62e631ad6caf2fb30e89a432e95cce18.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-17980883c30b9aa63f07feb36883739d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca4c0ca7d933c0ed4bb655144410357e.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="V-LASIK-Consistent-Glasses-Removal-from-Videos-Using-Synthetic-Data"><a href="#V-LASIK-Consistent-Glasses-Removal-from-Videos-Using-Synthetic-Data" class="headerlink" title="V-LASIK: Consistent Glasses-Removal from Videos Using Synthetic Data"></a>V-LASIK: Consistent Glasses-Removal from Videos Using Synthetic Data</h2><p><strong>Authors:Rotem Shalev-Arkushin, Aharon Azulay, Tavi Halperin, Eitan Richardson, Amit H. Bermano, Ohad Fried</strong></p>
<p>Diffusion-based generative models have recently shown remarkable image and video editing capabilities. However, local video editing, particularly removal of small attributes like glasses, remains a challenge. Existing methods either alter the videos excessively, generate unrealistic artifacts, or fail to perform the requested edit consistently throughout the video. In this work, we focus on consistent and identity-preserving removal of glasses in videos, using it as a case study for consistent local attribute removal in videos. Due to the lack of paired data, we adopt a weakly supervised approach and generate synthetic imperfect data, using an adjusted pretrained diffusion model. We show that despite data imperfection, by learning from our generated data and leveraging the prior of pretrained diffusion models, our model is able to perform the desired edit consistently while preserving the original video content. Furthermore, we exemplify the generalization ability of our method to other local video editing tasks by applying it successfully to facial sticker-removal. Our approach demonstrates significant improvement over existing methods, showcasing the potential of leveraging synthetic data and strong video priors for local video editing tasks. </p>
<blockquote>
<p>基于扩散的生成模型最近在图像和视频编辑功能方面表现出了显著的实力。然而，局部视频编辑，特别是去除像眼镜这样的小属性，仍然是一个挑战。现有方法要么过度修改视频，产生不真实的伪影，要么无法在视频中持续执行所需的编辑。在这项工作中，我们专注于在视频中一致且保持身份地去除眼镜，将其作为视频中一致局部属性去除的个案研究。由于缺乏配对数据，我们采用弱监督方法并生成合成的不完美数据，使用调整过的预训练扩散模型。我们表明，尽管数据存在不完美之处，但通过从生成的数据中学习并利用预训练扩散模型的先验知识，我们的模型能够一致地执行所需的编辑操作同时保留原始视频内容。此外，我们通过成功将其应用于面部贴纸去除，展示了该方法在其他局部视频编辑任务上的泛化能力。我们的方法相较于现有方法有明显的改进，展示了利用合成数据和强大的视频先验信息在局部视频编辑任务中的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.14510v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了基于扩散模型的视频编辑技术，特别是在处理局部视频编辑方面的挑战，如眼镜去除等。研究团队针对这一问题，采用弱监督方法，利用调整后的预训练扩散模型生成合成不完美数据。即使数据存在缺陷，模型也能从生成的数据中学习，并利用预训练扩散模型的先验知识，实现对视频的连续眼镜去除，同时保留视频内容的原始性。此外，该研究还展示了该方法在其他局部视频编辑任务中的泛化能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型在图像和视频编辑中显示出强大的能力。</li>
<li>局部视频编辑（如眼镜去除）仍然存在挑战。</li>
<li>现有方法往往会导致视频内容过度修改、生成不真实伪影或无法持续执行编辑请求。</li>
<li>研究提出了一种弱监督方法，利用调整后的预训练扩散模型生成合成不完美数据来解决眼镜去除问题。</li>
<li>即使数据存在缺陷，模型也能学习并成功执行眼镜去除操作，同时保持视频内容的连贯性和真实性。</li>
<li>该方法具有泛化到其他局部视频编辑任务的能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.14510">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-fcffb4ad15b031ef4863dd44c77f1cc7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0627b5d4d5e6240674ef2491f25f1b50.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f2f673abe429906f4c431a3371320779.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef108bafcc15a0178cb8ed60ce6801dc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9276f52b454052b1e5c3b422175fd9f1.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Financial-Models-in-Generative-Art-Black-Scholes-Inspired-Concept-Blending-in-Text-to-Image-Diffusion"><a href="#Financial-Models-in-Generative-Art-Black-Scholes-Inspired-Concept-Blending-in-Text-to-Image-Diffusion" class="headerlink" title="Financial Models in Generative Art: Black-Scholes-Inspired Concept   Blending in Text-to-Image Diffusion"></a>Financial Models in Generative Art: Black-Scholes-Inspired Concept   Blending in Text-to-Image Diffusion</h2><p><strong>Authors:Divya Kothandaraman, Ming Lin, Dinesh Manocha</strong></p>
<p>We introduce a novel approach for concept blending in pretrained text-to-image diffusion models, aiming to generate images at the intersection of multiple text prompts. At each time step during diffusion denoising, our algorithm forecasts predictions w.r.t. the generated image and makes informed text conditioning decisions. Central to our method is the unique analogy between diffusion models, which are rooted in non-equilibrium thermodynamics, and the Black-Scholes model for financial option pricing. By drawing parallels between key variables in both domains, we derive a robust algorithm for concept blending that capitalizes on the Markovian dynamics of the Black-Scholes framework. Our text-based concept blending algorithm is data-efficient, meaning it does not need additional training. Furthermore, it operates without human intervention or hyperparameter tuning. We highlight the benefits of our approach by comparing it qualitatively and quantitatively to other text based concept blending techniques, including linear interpolation, alternating prompts, step-wise prompt switching, and CLIP-guided prompt selection across various scenarios such as single object per text prompt, multiple objects per text prompt and objects against backgrounds. Our work shows that financially inspired techniques can enhance text-to-image concept blending in generative AI, paving the way for broader innovation. Code is available at <a target="_blank" rel="noopener" href="https://github.com/divyakraman/BlackScholesDiffusion2024">https://github.com/divyakraman/BlackScholesDiffusion2024</a>. </p>
<blockquote>
<p>我们介绍了一种在预训练文本到图像扩散模型中进行概念融合的新方法，旨在生成多个文本提示的交叉点的图像。在扩散去噪的每个时间步长中，我们的算法预测生成的图像并做出明智的文本条件决策。我们的方法的核心在于扩散模型与非平衡热力学的独特类比，以及与金融期权定价的Black-Scholes模型之间的关联。通过比较两个领域中的关键变量，我们推导出了一个稳健的概念融合算法，该算法充分利用了Black-Scholes框架的马尔可夫动力学。我们的基于文本的概念融合算法具有数据效率，意味着它不需要额外的训练。此外，它可以在没有人工干预或超参数调整的情况下运行。我们通过与其他基于文本的概念融合技术进行比较，突出了我们方法的好处，包括线性插值、交替提示、逐步提示切换以及CLIP引导的提示选择等。比较场景包括每个文本提示单个对象、每个文本提示多个对象以及对象与背景。我们的研究表明，金融启发技术可以增强生成人工智能中的文本到图像概念融合，为更广泛的创新铺平道路。代码可在<a target="_blank" rel="noopener" href="https://github.com/divyakraman/BlackScholesDiffusion2024%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/divyakraman/BlackScholesDiffusion2024找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.13685v2">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>本文介绍了一种在预训练文本到图像扩散模型中进行概念融合的新方法，旨在生成多个文本提示的交叉点的图像。在扩散去噪的每个时间步长中，我们的算法预测生成的图像并做出明智的文本条件决策。我们的方法的核心在于扩散模型与非平衡态热力学之间的独特类比，以及与金融期权定价的Black-Scholes模型之间的类比。通过比较两个领域的关键变量，我们推导出一种概念融合算法，该算法利用Black-Scholes框架的马尔可夫动力学。我们的基于文本的概融算法非常注重数据效率，无需额外的训练。此外，它无需人工干预或超参数调整。我们通过与其他基于文本的概融合技术（包括线性插值、交替提示、逐步提示切换和CLIP引导提示选择等）在各种场景下的定性和定量比较，突出了我们方法的优势。本文展示了金融启发技术可以增强生成人工智能中的文本到图像概念融合，为更广泛的创新铺平了道路。相关代码已公开发布于<a target="_blank" rel="noopener" href="https://github.com/divyakraman/BlackScholesDiffusion2024%E3%80%82">https://github.com/divyakraman/BlackScholesDiffusion2024。</a></p>
<p><strong>关键见解</strong></p>
<ol>
<li>提出了一种新颖的文本到图像扩散模型中的概念融合方法，旨在生成多个文本提示交叉点的图像。</li>
<li>通过将扩散模型与Black-Scholes金融期权定价模型进行类比，提出了一种稳健的概念融合算法。</li>
<li>该算法在数据效率方面表现出色，无需额外的训练，而且不需要人工干预或超参数调整。</li>
<li>在各种场景下与现有的文本概念融合技术进行了比较，证明了其优势。</li>
<li>展示了金融启发技术在生成人工智能中的文本到图像概念融合的潜力。</li>
<li>为更广泛的创新和进一步的研发提供了新的视角和思路。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.13685">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-a3b5e6e3afd9d743adf8f7a2bf0587fa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-145524306308ccadc0c6a8e73d3dd294.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-17/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-17/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-17/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-19a9242d00a4221999fdc99b6231fe60.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-04-17  Aligning Generative Denoising with Discriminative Objectives Unleashes   Diffusion for Visual Perception
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-17
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-17/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-de11ea7140d240861a6ee879dd40aea2.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2025-04-17  Explicit and Implicit Representations in AI-based 3D Reconstruction for   Radiology A systematic literature review
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-17
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">29301k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
