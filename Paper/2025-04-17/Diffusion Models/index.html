<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-17  Aligning Generative Denoising with Discriminative Objectives Unleashes   Diffusion for Visual Perception">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-c526efb9290b57bfd298d9bcdafe865a.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-17
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    16.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    66 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-17-æ›´æ–°"><a href="#2025-04-17-æ›´æ–°" class="headerlink" title="2025-04-17 æ›´æ–°"></a>2025-04-17 æ›´æ–°</h1><h2 id="Aligning-Generative-Denoising-with-Discriminative-Objectives-Unleashes-Diffusion-for-Visual-Perception"><a href="#Aligning-Generative-Denoising-with-Discriminative-Objectives-Unleashes-Diffusion-for-Visual-Perception" class="headerlink" title="Aligning Generative Denoising with Discriminative Objectives Unleashes   Diffusion for Visual Perception"></a>Aligning Generative Denoising with Discriminative Objectives Unleashes   Diffusion for Visual Perception</h2><p><strong>Authors:Ziqi Pang, Xin Xu, Yu-Xiong Wang</strong></p>
<p>With the success of image generation, generative diffusion models are increasingly adopted for discriminative tasks, as pixel generation provides a unified perception interface. However, directly repurposing the generative denoising process for discriminative objectives reveals critical gaps rarely addressed previously. Generative models tolerate intermediate sampling errors if the final distribution remains plausible, but discriminative tasks require rigorous accuracy throughout, as evidenced in challenging multi-modal tasks like referring image segmentation. Motivated by this gap, we analyze and enhance alignment between generative diffusion processes and perception tasks, focusing on how perception quality evolves during denoising. We find: (1) earlier denoising steps contribute disproportionately to perception quality, prompting us to propose tailored learning objectives reflecting varying timestep contributions; (2) later denoising steps show unexpected perception degradation, highlighting sensitivity to training-denoising distribution shifts, addressed by our diffusion-tailored data augmentation; and (3) generative processes uniquely enable interactivity, serving as controllable user interfaces adaptable to correctional prompts in multi-round interactions. Our insights significantly improve diffusion-based perception models without architectural changes, achieving state-of-the-art performance on depth estimation, referring image segmentation, and generalist perception tasks. Code available at <a target="_blank" rel="noopener" href="https://github.com/ziqipang/ADDP">https://github.com/ziqipang/ADDP</a>. </p>
<blockquote>
<p>éšç€å›¾åƒç”Ÿæˆçš„æˆåŠŸï¼Œç”Ÿæˆæ‰©æ•£æ¨¡å‹è¶Šæ¥è¶Šå¤šåœ°è¢«ç”¨äºåˆ¤åˆ«ä»»åŠ¡ï¼Œå› ä¸ºåƒç´ ç”Ÿæˆæä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„æ„ŸçŸ¥æ¥å£ã€‚ç„¶è€Œï¼Œç›´æ¥å°†ç”Ÿæˆå»å™ªè¿‡ç¨‹ç”¨äºåˆ¤åˆ«ç›®æ ‡ä¼šæš´éœ²å‡ºä¹‹å‰å¾ˆå°‘è§£å†³çš„å…³é”®å·®è·ã€‚ç”Ÿæˆæ¨¡å‹å¦‚æœæœ€ç»ˆåˆ†å¸ƒä»ç„¶å¯è¡Œçš„è¯ï¼Œå¯ä»¥å®¹å¿ä¸­é—´é‡‡æ ·é”™è¯¯ï¼Œä½†åˆ¤åˆ«ä»»åŠ¡éœ€è¦å§‹ç»ˆä¸¥æ ¼çš„å‡†ç¡®æ€§ï¼Œå¦‚æŒ‡ä»£å›¾åƒåˆ†å‰²ç­‰å…·æœ‰æŒ‘æˆ˜æ€§çš„å¤šæ¨¡å¼ä»»åŠ¡æ‰€è¯æ˜ã€‚å—æ­¤å·®è·çš„é©±åŠ¨ï¼Œæˆ‘ä»¬åˆ†æå’Œæé«˜äº†ç”Ÿæˆæ‰©æ•£è¿‡ç¨‹å’Œæ„ŸçŸ¥ä»»åŠ¡ä¹‹é—´çš„å¯¹é½æ€§ï¼Œé‡ç‚¹å…³æ³¨å»å™ªè¿‡ç¨‹ä¸­æ„ŸçŸ¥è´¨é‡å¦‚ä½•å‘å±•ã€‚æˆ‘ä»¬å‘ç°ï¼šï¼ˆ1ï¼‰æ—©æœŸçš„å»å™ªæ­¥éª¤å¯¹æ„ŸçŸ¥è´¨é‡çš„è´¡çŒ®ä¸æˆæ¯”ä¾‹ï¼Œä¿ƒä½¿æˆ‘ä»¬æå‡ºåæ˜ ä¸åŒæ—¶é—´æ­¥é•¿è´¡çŒ®çš„å®šåˆ¶å­¦ä¹ ç›®æ ‡ï¼›ï¼ˆ2ï¼‰åæœŸçš„å»å™ªæ­¥éª¤æ˜¾ç¤ºå‡ºæ„å¤–çš„æ„ŸçŸ¥é€€åŒ–ï¼Œçªå‡ºæ˜¾ç¤ºå¯¹è®­ç»ƒ-å»å™ªåˆ†å¸ƒå˜åŒ–çš„æ•æ„Ÿæ€§ï¼Œè¿™å¯ä»¥é€šè¿‡æˆ‘ä»¬é’ˆå¯¹æ‰©æ•£å®šåˆ¶çš„æ•°æ®å¢å¼ºæ¥è§£å†³ï¼›ï¼ˆ3ï¼‰ç”Ÿæˆè¿‡ç¨‹å…·æœ‰ç‹¬ç‰¹çš„äº¤äº’æ€§ï¼Œå¯ä½œä¸ºå¯æ§åˆ¶çš„ç”¨æˆ·ç•Œé¢ï¼Œé€‚åº”å¤šè½®äº¤äº’ä¸­çš„çº æ­£æç¤ºã€‚æˆ‘ä»¬çš„è§è§£åœ¨ä¸éœ€è¦æ¶æ„æ›´æ”¹çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—æ”¹è¿›äº†åŸºäºæ‰©æ•£çš„æ„ŸçŸ¥æ¨¡å‹ï¼Œåœ¨æ·±åº¦ä¼°è®¡ã€æŒ‡ä»£å›¾åƒåˆ†å‰²å’Œé€šç”¨æ„ŸçŸ¥ä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä»£ç å¯è®¿é—® <a target="_blank" rel="noopener" href="https://github.com/ziqipang/ADDP%E3%80%82">https://github.com/ziqipang/ADDPã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11457v1">PDF</a> ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†ç”Ÿæˆæ€§æ‰©æ•£æ¨¡å‹åœ¨åˆ¤åˆ«ä»»åŠ¡ä¸­çš„åº”ç”¨ï¼Œåˆ†æäº†å°†ç”Ÿæˆæ€§å»å™ªè¿‡ç¨‹ç›´æ¥åº”ç”¨äºåˆ¤åˆ«ç›®æ ‡æ—¶å­˜åœ¨çš„é—®é¢˜ã€‚é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡é‡ç‚¹ç ”ç©¶äº†ç”Ÿæˆæ‰©æ•£è¿‡ç¨‹ä¸æ„ŸçŸ¥ä»»åŠ¡ä¹‹é—´çš„å¯¹é½é—®é¢˜ï¼Œå¹¶å‘ç°æ—©æœŸå»å™ªæ­¥éª¤å¯¹æ„ŸçŸ¥è´¨é‡çš„è´¡çŒ®ä¸å‡ç­‰ï¼Œæå‡ºäº†åæ˜ ä¸åŒæ—¶é—´æ­¥é•¿è´¡çŒ®çš„å­¦ä¹ ç›®æ ‡ã€‚æ­¤å¤–ï¼Œè¿˜å‘ç°åæœŸå»å™ªæ­¥éª¤ä¼šå‡ºç°æ„å¤–çš„æ„ŸçŸ¥é€€åŒ–é—®é¢˜ï¼Œå¯¹æ­¤æœ¬æ–‡é€šè¿‡æ‰©æ•£å®šåˆ¶çš„æ•°æ®å¢å¼ºæ–¹æ³•æ¥è§£å†³ã€‚æœ€åï¼Œæœ¬æ–‡è¿˜æ¢è®¨äº†ç”Ÿæˆè¿‡ç¨‹çš„äº¤äº’æ€§ç‰¹ç‚¹ï¼Œå®ç°äº†å¤šè½®äº¤äº’ä¸­çš„çº æ­£æç¤ºåŠŸèƒ½ã€‚è¿™äº›å‘ç°æœ‰åŠ©äºæé«˜æ‰©æ•£æ¨¡å‹çš„æ„ŸçŸ¥æ€§èƒ½ï¼Œåœ¨æ·±åº¦ä¼°è®¡ã€å¼•ç”¨å›¾åƒåˆ†å‰²å’Œé€šç”¨æ„ŸçŸ¥ä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆæ€§æ‰©æ•£æ¨¡å‹åœ¨åˆ¤åˆ«ä»»åŠ¡ä¸­çš„åº”ç”¨é€æ¸å¢å¤šï¼Œä½†ç›´æ¥å°†ç”Ÿæˆæ€§å»å™ªè¿‡ç¨‹ç”¨äºåˆ¤åˆ«ä»»åŠ¡å­˜åœ¨å…³é”®å·®è·ã€‚</li>
<li>æ—©æœŸå»å™ªæ­¥éª¤å¯¹æ„ŸçŸ¥è´¨é‡çš„è´¡çŒ®è¾ƒå¤§ï¼Œæå‡ºéœ€åæ˜ ä¸åŒæ—¶é—´æ­¥é•¿è´¡çŒ®çš„å­¦ä¹ ç›®æ ‡ã€‚</li>
<li>åæœŸå»å™ªæ­¥éª¤å¯èƒ½å‡ºç°æ„å¤–çš„æ„ŸçŸ¥é€€åŒ–ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºè®­ç»ƒä¸å»å™ªåˆ†å¸ƒä¹‹é—´çš„å˜åŒ–ï¼Œéœ€è¦é€šè¿‡æ‰©æ•£å®šåˆ¶çš„æ•°æ®å¢å¼ºæ¥è§£å†³ã€‚</li>
<li>ç”Ÿæˆè¿‡ç¨‹å…·æœ‰äº¤äº’æ€§ç‰¹ç‚¹ï¼Œé€‚ç”¨äºå¤šè½®äº¤äº’ä¸­çš„çº æ­£æç¤ºã€‚</li>
<li>è¿™äº›å‘ç°æ˜¾è‘—æé«˜æ‰©æ•£æ¨¡å‹çš„æ„ŸçŸ¥æ€§èƒ½ï¼Œå®ç°æ·±åº¦ä¼°è®¡ã€å¼•ç”¨å›¾åƒåˆ†å‰²å’Œé€šç”¨æ„ŸçŸ¥ä»»åŠ¡çš„å…ˆè¿›æ€§èƒ½ã€‚</li>
<li>ç›¸å…³å·¥ä½œä»£ç å·²å…¬å¼€åœ¨GitHubä¸Šå…±äº«ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11457">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0849a67bed0f1393e18f2c94587e05c8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0ffe77c99d2cadd63899ccd87c2d03da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3e489b1c58ef31caeac7c4f1fcec9a66.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f704214dfa4357a28861e56f7b250c9.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="ADT-Tuning-Diffusion-Models-with-Adversarial-Supervision"><a href="#ADT-Tuning-Diffusion-Models-with-Adversarial-Supervision" class="headerlink" title="ADT: Tuning Diffusion Models with Adversarial Supervision"></a>ADT: Tuning Diffusion Models with Adversarial Supervision</h2><p><strong>Authors:Dazhong Shen, Guanglu Song, Yi Zhang, Bingqi Ma, Lujundong Li, Dongzhi Jiang, Zhuofan Zong, Yu Liu</strong></p>
<p>Diffusion models have achieved outstanding image generation by reversing a forward noising process to approximate true data distributions. During training, these models predict diffusion scores from noised versions of true samples in a single forward pass, while inference requires iterative denoising starting from white noise. This training-inference divergences hinder the alignment between inference and training data distributions, due to potential prediction biases and cumulative error accumulation. To address this problem, we propose an intuitive but effective fine-tuning framework, called Adversarial Diffusion Tuning (ADT), by stimulating the inference process during optimization and aligning the final outputs with training data by adversarial supervision. Specifically, to achieve robust adversarial training, ADT features a siamese-network discriminator with a fixed pre-trained backbone and lightweight trainable parameters, incorporates an image-to-image sampling strategy to smooth discriminative difficulties, and preserves the original diffusion loss to prevent discriminator hacking. In addition, we carefully constrain the backward-flowing path for back-propagating gradients along the inference path without incurring memory overload or gradient explosion. Finally, extensive experiments on Stable Diffusion models (v1.5, XL, and v3), demonstrate that ADT significantly improves both distribution alignment and image quality. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹é€šè¿‡é€†è½¬æ­£å‘å™ªå£°è¿‡ç¨‹æ¥è¿‘ä¼¼çœŸå®æ•°æ®åˆ†å¸ƒï¼Œä»è€Œå®ç°äº†å‡ºè‰²çš„å›¾åƒç”Ÿæˆã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œè¿™äº›æ¨¡å‹åœ¨ä¸€æ¬¡å‰å‘ä¼ é€’ä¸­ä»çœŸå®æ ·æœ¬çš„å™ªå£°ç‰ˆæœ¬é¢„æµ‹æ‰©æ•£åˆ†æ•°ï¼Œè€Œæ¨ç†åˆ™éœ€è¦ä»ç™½å™ªå£°å¼€å§‹è¿›è¡Œè¿­ä»£å»å™ªã€‚ç”±äºæ½œåœ¨çš„é¢„æµ‹åå·®å’Œç´¯ç§¯è¯¯å·®çš„ç´¯ç§¯ï¼Œè¿™ç§è®­ç»ƒä¸æ¨ç†ä¹‹é—´çš„å·®å¼‚é˜»ç¢äº†æ¨ç†å’Œè®­ç»ƒæ•°æ®åˆ†å¸ƒä¹‹é—´çš„å¯¹é½ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç›´è§‚è€Œæœ‰æ•ˆçš„å¾®è°ƒæ¡†æ¶ï¼Œç§°ä¸ºå¯¹æŠ—æ€§æ‰©æ•£è°ƒæ•´ï¼ˆADTï¼‰ï¼Œé€šè¿‡ä¼˜åŒ–è¿‡ç¨‹ä¸­åˆºæ¿€æ¨ç†è¿‡ç¨‹ï¼Œå¹¶é€šè¿‡å¯¹æŠ—æ€§ç›‘ç£ä½¿æœ€ç»ˆè¾“å‡ºä¸è®­ç»ƒæ•°æ®å¯¹é½ã€‚å…·ä½“è€Œè¨€ï¼Œä¸ºäº†è¿›è¡Œç¨³å¥çš„å¯¹æŠ—æ€§è®­ç»ƒï¼ŒADTä½¿ç”¨ä¸€ä¸ªå¸¦æœ‰å›ºå®šé¢„è®­ç»ƒä¸»å¹²å’Œè½»é‡çº§å¯è®­ç»ƒå‚æ•°çš„Siameseç½‘ç»œé‰´åˆ«å™¨ï¼Œé‡‡ç”¨å›¾åƒåˆ°å›¾åƒçš„é‡‡æ ·ç­–ç•¥æ¥å¹³æ»‘é‰´åˆ«éš¾åº¦ï¼Œå¹¶ä¿ç•™åŸå§‹æ‰©æ•£æŸå¤±ä»¥é˜²æ­¢é‰´åˆ«å™¨è¢«æ”»å‡»ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°å¿ƒåœ°çº¦æŸåå‘ä¼ æ’­è·¯å¾„ï¼Œä»¥ä¾¿åœ¨æ¨ç†è·¯å¾„ä¸Šåå‘ä¼ æ’­æ¢¯åº¦ï¼Œè€Œä¸ä¼šå¯¼è‡´å†…å­˜è¿‡è½½æˆ–æ¢¯åº¦çˆ†ç‚¸ã€‚æœ€åï¼Œå¯¹Stable Diffusionæ¨¡å‹ï¼ˆv1.5ã€XLå’Œv3ï¼‰çš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒADTæ˜¾è‘—æé«˜äº†åˆ†å¸ƒå¯¹é½å’Œå›¾åƒè´¨é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11423v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    æ‰©æ•£æ¨¡å‹é€šè¿‡åè½¬å‰å‘å™ªå£°è¿‡ç¨‹æ¥è¿‘ä¼¼çœŸå®æ•°æ®åˆ†å¸ƒï¼Œå®ç°äº†å‡ºè‰²çš„å›¾åƒç”Ÿæˆã€‚è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹ä»çœŸå®æ ·æœ¬çš„å™ªå£°ç‰ˆæœ¬é¢„æµ‹æ‰©æ•£åˆ†æ•°ï¼Œæ¨ç†åˆ™éœ€è¦ä»ç™½å™ªå£°å¼€å§‹è¿›è¡Œè¿­ä»£å»å™ªã€‚è¿™ç§è®­ç»ƒä¸æ¨ç†çš„åå·®å¯èƒ½å¯¼è‡´é¢„æµ‹åå·®å’Œç´¯ç§¯è¯¯å·®çš„ç´¯ç§¯ï¼Œè¿›è€Œå½±å“æ¨ç†ä¸è®­ç»ƒæ•°æ®åˆ†å¸ƒçš„å¯¹é½ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åä¸ºâ€œå¯¹æŠ—æ€§æ‰©æ•£è°ƒæ•´â€ï¼ˆADTï¼‰çš„å¾®è°ƒæ¡†æ¶ï¼Œé€šè¿‡ä¼˜åŒ–è¿‡ç¨‹ä¸­åˆºæ¿€æ¨ç†è¿‡ç¨‹å¹¶å®ç°æœ€ç»ˆè¾“å‡ºä¸è®­ç»ƒæ•°æ®çš„å¯¹é½ã€‚ADTé‡‡ç”¨å…·æœ‰å›ºå®šé¢„è®­ç»ƒä¸»å¹²å’Œè½»é‡çº§å¯è®­ç»ƒå‚æ•°çš„å­ªç”Ÿç½‘ç»œé‰´åˆ«å™¨ï¼Œé‡‡ç”¨å›¾åƒåˆ°å›¾åƒçš„é‡‡æ ·ç­–ç•¥æ¥è§£å†³é‰´åˆ«å›°éš¾ï¼Œå¹¶ä¿ç•™åŸå§‹æ‰©æ•£æŸå¤±ä»¥é˜²æ­¢é‰´åˆ«å™¨é»‘å®¢æ”»å‡»ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä»”ç»†çº¦æŸåå‘ä¼ æ’­æ¢¯åº¦åœ¨æ¨ç†è·¯å¾„ä¸Šçš„å‘åæµåŠ¨è·¯å¾„ï¼Œé¿å…å†…å­˜è¿‡è½½æˆ–æ¢¯åº¦çˆ†ç‚¸ã€‚åœ¨Stable Diffusionæ¨¡å‹ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒADTæ˜¾è‘—æé«˜äº†åˆ†å¸ƒå¯¹é½å’Œå›¾åƒè´¨é‡ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹é€šè¿‡åè½¬å‰å‘å™ªå£°è¿‡ç¨‹æ¥ç”Ÿæˆå›¾åƒã€‚</li>
<li>è®­ç»ƒè¿‡ç¨‹ä¸­æ¨¡å‹é¢„æµ‹æ‰©æ•£åˆ†æ•°ï¼Œè€Œæ¨ç†éœ€è¦ä»ç™½å™ªå£°è¿­ä»£å»å™ªã€‚</li>
<li>è¿™ç§è®­ç»ƒä¸æ¨ç†çš„åå·®å¯èƒ½å½±å“æ•°æ®åˆ†å¸ƒçš„å¯¹é½ã€‚</li>
<li>æå‡ºçš„ADTæ¡†æ¶é€šè¿‡å¯¹æŠ—æ€§ç›‘ç£æ¥ä¼˜åŒ–æ¨ç†ä¸è®­ç»ƒæ•°æ®å¯¹é½ã€‚</li>
<li>ADTé‡‡ç”¨å­ªç”Ÿç½‘ç»œé‰´åˆ«å™¨ï¼Œå…·æœ‰å›ºå®šé¢„è®­ç»ƒä¸»å¹²å’Œè½»é‡çº§å¯è®­ç»ƒå‚æ•°ã€‚</li>
<li>ADTé‡‡ç”¨å›¾åƒåˆ°å›¾åƒçš„é‡‡æ ·ç­–ç•¥æ¥è§£å†³é‰´åˆ«å›°éš¾ï¼Œå¹¶ä¿ç•™åŸå§‹æ‰©æ•£æŸå¤±ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒADTæé«˜äº†åˆ†å¸ƒå¯¹é½å’Œå›¾åƒè´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11423">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-91bd3c36c92d45dd090e7f49ce798103.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c9e0b5e22e82ea29aa78ac0ac078419.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1276a930fa09d1075b37bf26a13ae309.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Autoregressive-Distillation-of-Diffusion-Transformers"><a href="#Autoregressive-Distillation-of-Diffusion-Transformers" class="headerlink" title="Autoregressive Distillation of Diffusion Transformers"></a>Autoregressive Distillation of Diffusion Transformers</h2><p><strong>Authors:Yeongmin Kim, Sotiris Anagnostidis, Yuming Du, Edgar SchÃ¶nfeld, Jonas Kohler, Markos Georgopoulos, Albert Pumarola, Ali Thabet, Artsiom Sanakoyeu</strong></p>
<p>Diffusion models with transformer architectures have demonstrated promising capabilities in generating high-fidelity images and scalability for high resolution. However, iterative sampling process required for synthesis is very resource-intensive. A line of work has focused on distilling solutions to probability flow ODEs into few-step student models. Nevertheless, existing methods have been limited by their reliance on the most recent denoised samples as input, rendering them susceptible to exposure bias. To address this limitation, we propose AutoRegressive Distillation (ARD), a novel approach that leverages the historical trajectory of the ODE to predict future steps. ARD offers two key benefits: 1) it mitigates exposure bias by utilizing a predicted historical trajectory that is less susceptible to accumulated errors, and 2) it leverages the previous history of the ODE trajectory as a more effective source of coarse-grained information. ARD modifies the teacher transformer architecture by adding token-wise time embedding to mark each input from the trajectory history and employs a block-wise causal attention mask for training. Furthermore, incorporating historical inputs only in lower transformer layers enhances performance and efficiency. We validate the effectiveness of ARD in a class-conditioned generation on ImageNet and T2I synthesis. Our model achieves a $5\times$ reduction in FID degradation compared to the baseline methods while requiring only 1.1% extra FLOPs on ImageNet-256. Moreover, ARD reaches FID of 1.84 on ImageNet-256 in merely 4 steps and outperforms the publicly available 1024p text-to-image distilled models in prompt adherence score with a minimal drop in FID compared to the teacher. Project page: <a target="_blank" rel="noopener" href="https://github.com/alsdudrla10/ARD">https://github.com/alsdudrla10/ARD</a>. </p>
<blockquote>
<p>åŸºäºTransformeræ¶æ„çš„æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé«˜ä¿çœŸå›¾åƒå’Œé«˜åˆ†è¾¨ç‡æ–¹é¢è¡¨ç°å‡ºæœ‰å‰æ™¯çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œåˆæˆæ‰€éœ€çš„è¿­ä»£é‡‡æ ·è¿‡ç¨‹éå¸¸è€—è´¹èµ„æºã€‚ä¸€ç³»åˆ—ç ”ç©¶ä¸“æ³¨äºå°†æ¦‚ç‡æµå¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEsï¼‰çš„è§£å†³æ–¹æ¡ˆè’¸é¦æˆå°‘æ•°æ­¥éª¤çš„å­¦ç”Ÿæ¨¡å‹ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å—é™äºå®ƒä»¬å¯¹æœ€æ–°å»å™ªæ ·æœ¬çš„ä¾èµ–ï¼Œä½¿å…¶å®¹æ˜“å—åˆ°æš´éœ²åå·®çš„å½±å“ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†AutoRegressive Distillationï¼ˆARDï¼‰è¿™ä¸€æ–°æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨ODEçš„å†å²è½¨è¿¹æ¥é¢„æµ‹æœªæ¥æ­¥éª¤ã€‚ARDæœ‰ä¸¤ä¸ªä¸»è¦ä¼˜ç‚¹ï¼š1ï¼‰å®ƒé€šè¿‡åˆ©ç”¨é¢„æµ‹çš„å†å²è½¨è¿¹ï¼ˆä¸å¤ªå®¹æ˜“å—ç´¯ç§¯è¯¯å·®å½±å“ï¼‰æ¥ç¼“è§£æš´éœ²åå·®ï¼›2ï¼‰å®ƒåˆ©ç”¨ODEè½¨è¿¹çš„å…ˆå‰å†å²ä½œä¸ºæ›´æœ‰æ•ˆçš„ç²—ç²’åº¦ä¿¡æ¯æ¥æºã€‚ARDé€šè¿‡æ·»åŠ æ ‡è®°æ¯ä¸ªè¾“å…¥æ¥è‡ªè½¨è¿¹å†å²çš„tokençº§æ—¶é—´åµŒå…¥æ¥ä¿®æ”¹æ•™å¸ˆtransformeræ¶æ„ï¼Œå¹¶é‡‡ç”¨å—çº§å› æœæ³¨æ„åŠ›æ©ç è¿›è¡Œè®­ç»ƒã€‚æ­¤å¤–ï¼Œä»…åœ¨è¾ƒä½å±‚çš„transformerä¸­çº³å…¥å†å²è¾“å…¥å¯æé«˜æ€§èƒ½å’Œæ•ˆç‡ã€‚æˆ‘ä»¬åœ¨ImageNetä¸Šçš„ç±»åˆ«æ¡ä»¶ç”Ÿæˆå’ŒT2Iåˆæˆä¸ŠéªŒè¯äº†ARDçš„æœ‰æ•ˆæ€§ã€‚ä¸åŸºå‡†æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨FIDé€€åŒ–æ–¹é¢å®ç°äº†5å€çš„å‡å°‘ï¼ŒåŒæ—¶åœ¨ImageNet-256ä¸Šä»…éœ€è¦é¢å¤–çš„1.1% FLOPsã€‚æ­¤å¤–ï¼ŒARDåœ¨ImageNet-256ä¸Šä»…éœ€4æ­¥å³å¯è¾¾åˆ°FIDä¸º1.84ï¼Œå¹¶ä¸”åœ¨æç¤ºéµå¾ªå¾—åˆ†æ–¹é¢ä¼˜äºå…¬å¼€å¯ç”¨çš„1024pæ–‡æœ¬åˆ°å›¾åƒè’¸é¦æ¨¡å‹ï¼ŒåŒæ—¶ä¸æ•™å¸ˆçš„FIDç›¸æ¯”å‡ ä¹æ²¡æœ‰ä¸‹é™ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://github.com/alsdudrla10/ARD%E3%80%82">https://github.com/alsdudrla10/ARDã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11295v1">PDF</a> CVPR 2025 Oral</p>
<p><strong>æ‘˜è¦</strong><br>åŸºäºæ‰©æ•£æ¨¡å‹çš„è‡ªå›å½’è’¸é¦ï¼ˆARDï¼‰æ–¹æ³•é€šè¿‡åˆ©ç”¨ODEå†å²è½¨è¿¹æ¥é¢„æµ‹æœªæ¥æ­¥éª¤ï¼Œè§£å†³äº†è¿­ä»£é‡‡æ ·è¿‡ç¨‹ä¸­çš„èµ„æºå¯†é›†å‹å’Œæš´éœ²åå·®é—®é¢˜ã€‚ARDé€šè¿‡æ·»åŠ æ—¶é—´åµŒå…¥å’Œå—çº§å› æœæ³¨æ„åŠ›æ©ç æ¥ä¿®æ”¹æ•™å¸ˆè½¬æ¢å™¨æ¶æ„ï¼Œå¹¶å°†å†å²è¾“å…¥ä»…çº³å…¥è¾ƒä½å±‚è½¬æ¢å™¨ä¸­ä»¥æé«˜æ€§èƒ½å’Œæ•ˆç‡ã€‚åœ¨ImageNetå’ŒT2Iåˆæˆä¸Šçš„ç±»æ¡ä»¶ç”ŸæˆéªŒè¯è¡¨æ˜ï¼ŒARDåœ¨å‡å°‘FIDé™è§£æ–¹é¢æ¯”åŸºçº¿æ–¹æ³•æé«˜äº†5å€ï¼ŒåŒæ—¶åœ¨ImageNet-256ä¸Šä»…éœ€è¦é¢å¤–1.1%çš„æµ®ç‚¹è¿ç®—ã€‚æ­¤å¤–ï¼ŒARDåœ¨ä»…æœ‰å››æ­¥çš„æƒ…å†µä¸‹è¾¾åˆ°äº†ImageNet-256çš„FIDä¸º1.84ï¼Œå¹¶åœ¨æç¤ºéµå¾ªå¾—åˆ†æ–¹é¢è¶…è¶Šäº†å…¬å¼€çš„1024pæ–‡æœ¬åˆ°å›¾åƒè’¸é¦æ¨¡å‹ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹å…·æœ‰ç”Ÿæˆé«˜ä¿çœŸå›¾åƒå’Œè‰¯å¥½æ‰©å±•æ€§çš„æ½œåŠ›ã€‚</li>
<li>è¿­ä»£é‡‡æ ·è¿‡ç¨‹å¯¹äºå›¾åƒåˆæˆéå¸¸èµ„æºå¯†é›†ã€‚</li>
<li>å½“å‰æ–¹æ³•ä¾èµ–äºæœ€è¿‘çš„å»å™ªæ ·æœ¬ä½œä¸ºè¾“å…¥ï¼Œå®¹æ˜“å—åˆ°æš´éœ²åå·®çš„å½±å“ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„è‡ªå›å½’è’¸é¦ï¼ˆARDï¼‰æ–¹æ³•ï¼Œåˆ©ç”¨ODEçš„å†å²è½¨è¿¹æ¥é¢„æµ‹æœªæ¥æ­¥éª¤ã€‚</li>
<li>ARDé€šè¿‡æ·»åŠ æ—¶é—´åµŒå…¥å’Œå—çº§å› æœæ³¨æ„åŠ›æ©ç ä¿®æ”¹æ•™å¸ˆè½¬æ¢å™¨æ¶æ„ã€‚</li>
<li>ARDé€šè¿‡ç»“åˆå†å²è¾“å…¥åœ¨ä¸‹å±‚è½¬æ¢å™¨ä¸­å®ç°é«˜æ€§èƒ½å’Œæ•ˆç‡ã€‚</li>
<li>ARDåœ¨ImageNetå’ŒT2Iåˆæˆä¸Šçš„è¡¨ç°ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œæ˜¾è‘—å‡å°‘äº†FIDé™è§£ï¼Œå¹¶åœ¨æç¤ºéµå¾ªå¾—åˆ†æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11295">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0bbd7ec7feb2545633261a494a73a4f8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a7d1abb3ca5bbe8ab3518d443b2bb1ef.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c60f1b996b03ef39ccf19e091a39d527.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8592fe0985903a7cc0655033d1803932.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3987172d96372878d014e8a6415085ec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8eebff7940e18a69adf747a0a199da56.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Taming-Consistency-Distillation-for-Accelerated-Human-Image-Animation"><a href="#Taming-Consistency-Distillation-for-Accelerated-Human-Image-Animation" class="headerlink" title="Taming Consistency Distillation for Accelerated Human Image Animation"></a>Taming Consistency Distillation for Accelerated Human Image Animation</h2><p><strong>Authors:Xiang Wang, Shiwei Zhang, Hangjie Yuan, Yujie Wei, Yingya Zhang, Changxin Gao, Yuehuan Wang, Nong Sang</strong></p>
<p>Recent advancements in human image animation have been propelled by video diffusion models, yet their reliance on numerous iterative denoising steps results in high inference costs and slow speeds. An intuitive solution involves adopting consistency models, which serve as an effective acceleration paradigm through consistency distillation. However, simply employing this strategy in human image animation often leads to quality decline, including visual blurring, motion degradation, and facial distortion, particularly in dynamic regions. In this paper, we propose the DanceLCM approach complemented by several enhancements to improve visual quality and motion continuity at low-step regime: (1) segmented consistency distillation with an auxiliary light-weight head to incorporate supervision from real video latents, mitigating cumulative errors resulting from single full-trajectory generation; (2) a motion-focused loss to centre on motion regions, and explicit injection of facial fidelity features to improve face authenticity. Extensive qualitative and quantitative experiments demonstrate that DanceLCM achieves results comparable to state-of-the-art video diffusion models with a mere 2-4 inference steps, significantly reducing the inference burden without compromising video quality. The code and models will be made publicly available. </p>
<blockquote>
<p>è¿‘æœŸäººç±»å›¾åƒåŠ¨ç”»æŠ€æœ¯çš„è¿›å±•å¾—ç›Šäºè§†é¢‘æ‰©æ•£æ¨¡å‹ã€‚ç„¶è€Œï¼Œå®ƒä»¬ä¾èµ–äºå¤§é‡çš„è¿­ä»£å»å™ªæ­¥éª¤ï¼Œå¯¼è‡´æ¨ç†æˆæœ¬é«˜æ˜‚å’Œé€Ÿåº¦ç¼“æ…¢ã€‚ä¸€ç§ç›´è§‚çš„è§£å†³æ–¹æ¡ˆæ˜¯é‡‡ç”¨ä¸€è‡´æ€§æ¨¡å‹ï¼Œé€šè¿‡ä¸€è‡´æ€§è’¸é¦æˆä¸ºä¸€ç§æœ‰æ•ˆçš„åŠ é€ŸèŒƒå¼ã€‚ç„¶è€Œï¼Œä»…ä»…åœ¨äººç±»å›¾åƒåŠ¨ç”»ä¸­é‡‡ç”¨è¿™ç§ç­–ç•¥å¾€å¾€ä¼šå¯¼è‡´è´¨é‡ä¸‹é™ï¼ŒåŒ…æ‹¬è§†è§‰æ¨¡ç³Šã€è¿åŠ¨é€€åŒ–ä»¥åŠé¢éƒ¨å¤±çœŸï¼Œç‰¹åˆ«æ˜¯åœ¨åŠ¨æ€åŒºåŸŸã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†DanceLCMæ–¹æ³•ï¼Œå¹¶è¾…ä»¥å‡ é¡¹å¢å¼ºåŠŸèƒ½ï¼Œä»¥åœ¨ä½æ­¥éª¤åˆ¶åº¦ä¸‹æé«˜è§†è§‰è´¨é‡å’Œè¿åŠ¨è¿ç»­æ€§ï¼šï¼ˆ1ï¼‰åˆ†æ®µä¸€è‡´æ€§è’¸é¦ï¼Œè¾…ä»¥è¾…åŠ©çš„è½»é‡çº§å¤´ï¼Œä»¥èå…¥çœŸå®è§†é¢‘æ½œå˜é‡çš„ç›‘ç£ä¿¡æ¯ï¼Œç¼“è§£ç”±å•ä¸€å…¨è½¨è¿¹ç”Ÿæˆå¯¼è‡´çš„ç´¯ç§¯è¯¯å·®ï¼›ï¼ˆ2ï¼‰ä»¥è¿åŠ¨ä¸ºä¸­å¿ƒçš„æŸå¤±å‡½æ•°ï¼Œä¸“æ³¨äºè¿åŠ¨åŒºåŸŸï¼Œå¹¶æ˜¾å¼æ³¨å…¥é¢éƒ¨ä¿çœŸç‰¹å¾ä»¥æé«˜é¢éƒ¨çœŸå®æ€§ã€‚å¤§é‡çš„å®šæ€§å’Œå®šé‡å®éªŒè¡¨æ˜ï¼ŒDanceLCMåœ¨ä»…ä½¿ç”¨2-4ä¸ªæ¨ç†æ­¥éª¤çš„æƒ…å†µä¸‹ï¼Œå°±èƒ½è¾¾åˆ°ä¸æœ€å…ˆè¿›çš„è§†é¢‘æ‰©æ•£æ¨¡å‹ç›¸å½“çš„ç»“æœï¼Œæ˜¾è‘—å‡å°‘äº†æ¨ç†è´Ÿæ‹…ï¼ŒåŒæ—¶ä¸å¦¥åè§†é¢‘è´¨é‡ã€‚ä»£ç å’Œæ¨¡å‹å°†å…¬å¼€å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11143v1">PDF</a> </p>
<p><strong>Summary</strong><br>     è§†é¢‘æ‰©æ•£æ¨¡å‹æ¨åŠ¨äº†äººåƒåŠ¨ç”»çš„æœ€æ–°å‘å±•ï¼Œä½†å…¶ä¾èµ–äºå¤§é‡çš„è¿­ä»£å»å™ªæ­¥éª¤å¯¼è‡´æ¨ç†æˆæœ¬é«˜ã€é€Ÿåº¦æ…¢ã€‚é‡‡ç”¨ä¸€è‡´æ€§æ¨¡å‹ä½œä¸ºæœ‰æ•ˆçš„åŠ é€ŸèŒƒå¼å¯ä»¥é€šè¿‡ä¸€è‡´æ€§è’¸é¦æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†ç®€å•åº”ç”¨äºäººåƒåŠ¨ç”»å¾€å¾€ä¼šå¯¼è‡´è´¨é‡ä¸‹é™ï¼ŒåŒ…æ‹¬è§†è§‰æ¨¡ç³Šã€è¿åŠ¨é€€åŒ–ä»¥åŠé¢éƒ¨å¤±çœŸï¼Œç‰¹åˆ«æ˜¯åœ¨åŠ¨æ€åŒºåŸŸã€‚æœ¬æ–‡æå‡ºäº†ç»“åˆå¤šç§æ”¹è¿›çš„DanceLCMæ–¹æ³•ï¼Œåœ¨ä½æ­¥çŠ¶æ€ä¸‹æé«˜è§†è§‰è´¨é‡å’Œè¿åŠ¨è¿ç»­æ€§ï¼šï¼ˆ1ï¼‰é€šè¿‡è¾…åŠ©çš„è½»é‡çº§å¤´è¿›è¡Œåˆ†æ®µä¸€è‡´æ€§è’¸é¦ï¼Œä»¥èå…¥çœŸå®è§†é¢‘æ½œå˜é‡çš„ç›‘ç£ä¿¡æ¯ï¼Œç¼“è§£å•ä¸€å…¨è½¨è¿¹ç”Ÿæˆå¯¼è‡´çš„ç´¯ç§¯è¯¯å·®ï¼›ï¼ˆ2ï¼‰ä»¥è¿åŠ¨ä¸ºä¸­å¿ƒçš„æŸå¤±èšç„¦äºè¿åŠ¨åŒºåŸŸï¼Œå¹¶æ˜¾å¼æ³¨å…¥é¢éƒ¨ä¿çœŸç‰¹å¾ä»¥æé«˜é¢éƒ¨çœŸå®æ€§ã€‚å¤§é‡å®šæ€§å’Œå®šé‡å®éªŒè¡¨æ˜ï¼ŒDanceLCMä»…éœ€2-4æ­¥æ¨ç†å³å¯è·å¾—ä¸æœ€å…ˆè¿›çš„è§†é¢‘æ‰©æ•£æ¨¡å‹ç›¸å½“çš„ç»“æœï¼Œæ˜¾è‘—é™ä½äº†æ¨ç†è´Ÿæ‹…ï¼Œä¸”ä¸æŸå®³è§†é¢‘è´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†é¢‘æ‰©æ•£æ¨¡å‹åœ¨äººåƒåŠ¨ç”»é¢†åŸŸå–å¾—æœ€æ–°è¿›å±•ï¼Œä½†å­˜åœ¨æ¨ç†æˆæœ¬é«˜å’Œé€Ÿåº¦æ…¢çš„é—®é¢˜ã€‚</li>
<li>ä¸€è‡´æ€§æ¨¡å‹èƒ½æœ‰æ•ˆåŠ é€Ÿè§†é¢‘æ‰©æ•£æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ã€‚</li>
<li>å•çº¯é‡‡ç”¨ä¸€è‡´æ€§æ¨¡å‹ä¼šå¯¼è‡´è´¨é‡ä¸‹é™ï¼ŒåŒ…æ‹¬è§†è§‰æ¨¡ç³Šã€è¿åŠ¨é€€åŒ–ä»¥åŠé¢éƒ¨å¤±çœŸã€‚</li>
<li>DanceLCMæ–¹æ³•é€šè¿‡åˆ†æ®µä¸€è‡´æ€§è’¸é¦å’Œé¢éƒ¨ä¿çœŸç‰¹å¾çš„æ³¨å…¥æ¥æé«˜è§†è§‰è´¨é‡å’Œè¿åŠ¨è¿ç»­æ€§ã€‚</li>
<li>DanceLCMæ–¹æ³•åœ¨ä½æ­¥çŠ¶æ€ä¸‹å®ç°é«˜é€Ÿæ¨ç†ï¼Œä»…éœ€2-4æ­¥å³å¯è·å¾—ä¸å…ˆè¿›æ¨¡å‹ç›¸å½“çš„ç»“æœã€‚</li>
<li>DanceLCMæ–¹æ³•æ˜¾è‘—é™ä½äº†æ¨ç†è´Ÿæ‹…ï¼ŒåŒæ—¶ä¿æŒè§†é¢‘è´¨é‡ä¸æŸå¤±ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11143">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0f474d693afc43aeb4dc52df1682b746.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-753e61cd3cd107492f519618c6c1076d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4ff7e842f554322ad6c2fb31838f6dd7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-14501c5741a33c3fb80563b01604d923.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="AnimeDL-2M-Million-Scale-AI-Generated-Anime-Image-Detection-and-Localization-in-Diffusion-Era"><a href="#AnimeDL-2M-Million-Scale-AI-Generated-Anime-Image-Detection-and-Localization-in-Diffusion-Era" class="headerlink" title="AnimeDL-2M: Million-Scale AI-Generated Anime Image Detection and   Localization in Diffusion Era"></a>AnimeDL-2M: Million-Scale AI-Generated Anime Image Detection and   Localization in Diffusion Era</h2><p><strong>Authors:Chenyang Zhu, Xing Zhang, Yuyang Sun, Ching-Chun Chang, Isao Echizen</strong></p>
<p>Recent advances in image generation, particularly diffusion models, have significantly lowered the barrier for creating sophisticated forgeries, making image manipulation detection and localization (IMDL) increasingly challenging. While prior work in IMDL has focused largely on natural images, the anime domain remains underexplored-despite its growing vulnerability to AI-generated forgeries. Misrepresentations of AI-generated images as hand-drawn artwork, copyright violations, and inappropriate content modifications pose serious threats to the anime community and industry. To address this gap, we propose AnimeDL-2M, the first large-scale benchmark for anime IMDL with comprehensive annotations. It comprises over two million images including real, partially manipulated, and fully AI-generated samples. Experiments indicate that models trained on existing IMDL datasets of natural images perform poorly when applied to anime images, highlighting a clear domain gap between anime and natural images. To better handle IMDL tasks in anime domain, we further propose AniXplore, a novel model tailored to the visual characteristics of anime imagery. Extensive evaluations demonstrate that AniXplore achieves superior performance compared to existing methods. Dataset and code can be found in <a target="_blank" rel="noopener" href="https://flytweety.github.io/AnimeDL2M/">https://flytweety.github.io/AnimeDL2M/</a>. </p>
<blockquote>
<p>å›¾åƒç”Ÿæˆé¢†åŸŸçš„æœ€æ–°è¿›å±•ï¼Œå°¤å…¶æ˜¯æ‰©æ•£æ¨¡å‹ï¼Œæå¤§åœ°é™ä½äº†åˆ›å»ºå¤æ‚ä¼ªé€ ä½œå“çš„é—¨æ§›ï¼Œä½¿å¾—å›¾åƒæ“çºµæ£€æµ‹ä¸å®šä½ï¼ˆIMDLï¼‰è¶Šæ¥è¶Šå…·æœ‰æŒ‘æˆ˜æ€§ã€‚å°½ç®¡å…ˆå‰åœ¨IMDLæ–¹é¢çš„å·¥ä½œä¸»è¦é›†ä¸­åœ¨è‡ªç„¶å›¾åƒä¸Šï¼Œä½†åŠ¨æ¼«é¢†åŸŸä»ç„¶è¢«å¿½è§†ï¼Œå°½ç®¡å®ƒè¶Šæ¥è¶Šå®¹æ˜“å—åˆ°AIç”Ÿæˆçš„ä¼ªé€ ä½œå“çš„å¨èƒã€‚å°†AIç”Ÿæˆçš„å›¾åƒè¯¯è¡¨ç¤ºä¸ºæ‰‹ç»˜è‰ºæœ¯å“ã€ç‰ˆæƒä¾µçŠ¯å’Œä¸é€‚å½“çš„å†…å®¹ä¿®æ”¹å¯¹åŠ¨æ¼«ç¤¾åŒºå’Œè¡Œä¸šæ„æˆä¸¥é‡å¨èƒã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†AnimeDL-2Mï¼Œè¿™æ˜¯é¦–ä¸ªç”¨äºåŠ¨æ¼«IMDLçš„å¤§è§„æ¨¡åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«å…¨é¢æ³¨é‡Šã€‚å®ƒåŒ…å«è¶…è¿‡ä¸¤ç™¾ä¸‡å¼ å›¾åƒï¼ŒåŒ…æ‹¬çœŸå®ã€éƒ¨åˆ†æ“çºµå’Œå®Œå…¨AIç”Ÿæˆçš„æ ·æœ¬ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨è‡ªç„¶äººå›¾åƒIMDLæ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹åœ¨åº”ç”¨äºåŠ¨æ¼«å›¾åƒæ—¶è¡¨ç°ä¸ä½³ï¼Œè¿™çªå‡ºäº†åŠ¨æ¼«å’Œè‡ªç„¶äººå›¾åƒä¹‹é—´çš„æ˜æ˜¾é¢†åŸŸå·®è·ã€‚ä¸ºäº†æ›´å¥½åœ°å¤„ç†åŠ¨æ¼«é¢†åŸŸçš„IMDLä»»åŠ¡ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†AniXploreï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹åŠ¨æ¼«å›¾åƒè§†è§‰ç‰¹å¾é‡èº«å®šåˆ¶çš„æ–°æ¨¡å‹ã€‚å¤§é‡è¯„ä¼°è¡¨æ˜ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒAniXploreå®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚æ•°æ®é›†å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://flytweety.github.io/AnimeDL2M/%E6%89%BE%E5%88%B0%E3%80%82">https://flytweety.github.io/AnimeDL2M/æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11015v1">PDF</a> </p>
<p><strong>Summary</strong><br>     è¿‘æœŸå›¾åƒç”ŸæˆæŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯æ‰©æ•£æ¨¡å‹çš„å‘å±•ï¼Œä½¿å¾—åˆ›å»ºé«˜çº§ä¼ªé€ ä½œå“çš„é—¨æ§›å¤§å¹…é™ä½ï¼Œä½¿å¾—å›¾åƒæ“ä½œæ£€æµ‹ä¸å®šä½ï¼ˆIMDLï¼‰é¢ä¸´è¶Šæ¥è¶Šå¤§çš„æŒ‘æˆ˜ã€‚ä»¥å¾€çš„ç ”ç©¶ä¸»è¦å…³æ³¨è‡ªç„¶å›¾åƒé¢†åŸŸï¼Œä½†åŠ¨æ¼«é¢†åŸŸå´è¢«å¿½ç•¥ã€‚è™½ç„¶åŠ¨æ¼«çš„è™šå‡AIç”Ÿæˆå›¾åƒæ˜“è¢«ç”¨ä½œæ‰‹ç»˜ä½œå“è¯ˆéª—ç­‰è¿æ³•è¡Œä¸ºï¼Œå¯¹åŠ¨æ¼«ç•Œå’Œäº§ä¸šæ„æˆä¸¥é‡å¨èƒã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæ¨å‡ºé¦–ä¸ªå¤§å‹åŠ¨æ¼«IMDLåŸºå‡†æµ‹è¯•æ•°æ®é›†AnimeDL-2Mï¼Œå«æœ‰è¶…è¿‡ä¸¤ç™¾ä¸‡å›¾ç‰‡æ ·æœ¬ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œåœ¨åŠ¨æ¼«å›¾åƒä¸Šè®­ç»ƒçš„æ¨¡å‹è¡¨ç°ä¸ä½³ï¼Œæ˜¾ç¤ºåŠ¨æ¼«ä¸è‡ªç„¶å›¾åƒé¢†åŸŸé—´çš„æ˜æ˜¾å·®è·ã€‚ä¸ºæ­¤æ¨å‡ºé’ˆå¯¹åŠ¨æ¼«è§†è§‰ç‰¹æ€§çš„æ–°æ¨¡å‹AniXploreï¼Œè¯„ä¼°æ˜¾ç¤ºå…¶è¡¨ç°ä¼˜è¶Šäºç°æœ‰æ–¹æ³•ã€‚æ›´å¤šä¿¡æ¯å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://flytweety.github.io/AnimeDL2M/">é“¾æ¥</a>è·å–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æ‰©æ•£æ¨¡å‹é™ä½äº†åˆ›å»ºé«˜çº§ä¼ªé€ ä½œå“çš„éš¾åº¦ï¼Œä½¿å›¾åƒæ“ä½œæ£€æµ‹ä¸å®šä½ï¼ˆIMDLï¼‰æ›´å…·æŒ‘æˆ˜æ€§ã€‚</li>
<li>åŠ¨æ¼«é¢†åŸŸçš„IMDLç ”ç©¶ç›¸å¯¹ç¼ºä¹ï¼Œå­˜åœ¨å·¨å¤§çš„ç ”ç©¶ç©ºé—´ã€‚</li>
<li>åŠ¨æ¼«çš„è™šå‡AIç”Ÿæˆå›¾åƒå­˜åœ¨è¯¯å½“ä½œæ‰‹ç»˜ä½œå“çš„æ³•å¾‹é£é™©ã€‚</li>
<li>æ¨å‡ºé¦–ä¸ªå¤§å‹åŠ¨æ¼«IMDLæ•°æ®é›†AnimeDL-2Mï¼Œå«æœ‰è¶…è¿‡ä¸¤ç™¾ä¸‡å›¾ç‰‡æ ·æœ¬ã€‚</li>
<li>åœ¨åŠ¨æ¼«å›¾åƒä¸Šè®­ç»ƒçš„æ¨¡å‹è¡¨ç°æ¬ ä½³ï¼Œè¡¨æ˜åŠ¨æ¼«ä¸å¸¸è§„è‡ªç„¶å›¾åƒé¢†åŸŸçš„å·®è·ã€‚</li>
<li>æå‡ºé’ˆå¯¹åŠ¨æ¼«è§†è§‰ç‰¹æ€§çš„æ–°æ¨¡å‹AniXploreã€‚</li>
<li>AniXploreæ¨¡å‹ç›¸è¾ƒäºç°æœ‰æ–¹æ³•è¡¨ç°ä¼˜è¶Šã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11015">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e1c7b50f1b455178261dd15b14250b4b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8992976fc1aa15fc7f78e4d85c26d0d8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9cac0dc7205cf7fdb6c41056585ab09d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70c499791398dcc15996dee8835fa376.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-29f8d14632ade71f6b529587479d3426.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="TMCIR-Token-Merge-Benefits-Composed-Image-Retrieval"><a href="#TMCIR-Token-Merge-Benefits-Composed-Image-Retrieval" class="headerlink" title="TMCIR: Token Merge Benefits Composed Image Retrieval"></a>TMCIR: Token Merge Benefits Composed Image Retrieval</h2><p><strong>Authors:Chaoyang Wang, Zeyu Zhang, Long Teng, Zijun Li, Shichao Kan</strong></p>
<p>Composed Image Retrieval (CIR) retrieves target images using a multi-modal query that combines a reference image with text describing desired modifications. The primary challenge is effectively fusing this visual and textual information. Current cross-modal feature fusion approaches for CIR exhibit an inherent bias in intention interpretation. These methods tend to disproportionately emphasize either the reference image features (visual-dominant fusion) or the textual modification intent (text-dominant fusion through image-to-text conversion). Such an imbalanced representation often fails to accurately capture and reflect the actual search intent of the user in the retrieval results. To address this challenge, we propose TMCIR, a novel framework that advances composed image retrieval through two key innovations: 1) Intent-Aware Cross-Modal Alignment. We first fine-tune CLIP encoders contrastively using intent-reflecting pseudo-target images, synthesized from reference images and textual descriptions via a diffusion model. This step enhances the encoder ability of text to capture nuanced intents in textual descriptions. 2) Adaptive Token Fusion. We further fine-tune all encoders contrastively by comparing adaptive token-fusion features with the target image. This mechanism dynamically balances visual and textual representations within the contrastive learning pipeline, optimizing the composed feature for retrieval. Extensive experiments on Fashion-IQ and CIRR datasets demonstrate that TMCIR significantly outperforms state-of-the-art methods, particularly in capturing nuanced user intent. </p>
<blockquote>
<p>å›¾åƒç»„åˆæ£€ç´¢ï¼ˆCIRï¼‰ä½¿ç”¨å¤šæ¨¡æ€æŸ¥è¯¢æ£€ç´¢ç›®æ ‡å›¾åƒï¼Œè¯¥æŸ¥è¯¢å°†å‚è€ƒå›¾åƒä¸æè¿°æ‰€éœ€ä¿®æ”¹çš„æ–‡æœ¬ç›¸ç»“åˆã€‚ä¸»è¦æŒ‘æˆ˜æ˜¯å¦‚ä½•æœ‰æ•ˆåœ°èåˆè¿™ç§è§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯ã€‚å½“å‰ç”¨äºCIRçš„è·¨æ¨¡æ€ç‰¹å¾èåˆæ–¹æ³•è¡¨ç°å‡ºæ„å›¾è§£é‡Šçš„å›ºæœ‰åè§ã€‚è¿™äº›æ–¹æ³•å¾€å¾€è¿‡åˆ†å¼ºè°ƒå‚è€ƒå›¾åƒç‰¹å¾ï¼ˆè§†è§‰ä¸»å¯¼èåˆï¼‰æˆ–æ–‡æœ¬ä¿®æ”¹æ„å›¾ï¼ˆé€šè¿‡å›¾åƒåˆ°æ–‡æœ¬çš„è½¬æ¢å®ç°æ–‡æœ¬ä¸»å¯¼èåˆï¼‰ã€‚è¿™ç§ä¸å¹³è¡¡çš„è¡¨ç¤ºé€šå¸¸åœ¨æ£€ç´¢ç»“æœä¸­æ— æ³•å‡†ç¡®æ•è·å’Œåæ˜ ç”¨æˆ·çš„å®é™…æœç´¢æ„å›¾ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10995v1">PDF</a> arXiv admin note: text overlap with arXiv:2310.05473 by other authors</p>
<p><strong>Summary</strong></p>
<p>ä¸€ç§èåˆæ–‡æœ¬ä¸å›¾åƒä¿¡æ¯çš„è·¨æ¨¡æ€æ£€ç´¢æ–¹æ³•ã€‚ç›®å‰CIRå­˜åœ¨è§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯èåˆçš„é—®é¢˜ï¼Œé€šå¸¸ä¼šå‡ºç°è¿‡åº¦åå‘å‚è€ƒå›¾åƒæˆ–æ–‡æœ¬æè¿°çš„åå‘æ€§é—®é¢˜ï¼Œéš¾ä»¥å‡†ç¡®æ•æ‰ç”¨æˆ·æ„å›¾ã€‚æœ¬æ–‡æå‡ºçš„TMCIRæ¡†æ¶é€šè¿‡ä¸¤ç§å…³é”®åˆ›æ–°æ–¹æ³•è§£å†³è¿™ä¸€é—®é¢˜ï¼šä¸€æ˜¯æ„å›¾æ„ŸçŸ¥çš„è·¨æ¨¡æ€å¯¹é½ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹åˆæˆåæ˜ æ„å›¾çš„ä¼ªç›®æ ‡å›¾åƒå¯¹CLIPç¼–ç å™¨è¿›è¡Œå¾®è°ƒï¼›äºŒæ˜¯è‡ªé€‚åº”ä»¤ç‰Œèåˆï¼Œé€šè¿‡å¯¹æ¯”è‡ªé€‚åº”ä»¤ç‰Œèåˆç‰¹å¾ä¸ç›®æ ‡å›¾åƒå¯¹ç¼–ç å™¨è¿›è¡Œè¿›ä¸€æ­¥å¾®è°ƒã€‚å®éªŒè¯æ˜ï¼ŒTMCIRæ˜¾è‘—ä¼˜äºå½“å‰çš„ä¸»æµæ–¹æ³•ï¼Œå°¤å…¶åœ¨äºæ•æ‰ç”¨æˆ·ç»†å¾®æ„å›¾çš„èƒ½åŠ›ä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>CIRåˆ©ç”¨å¤šæ¨¡æ€æŸ¥è¯¢ï¼ˆç»“åˆå‚è€ƒå›¾åƒå’Œæè¿°æ–‡æœ¬ï¼‰æ£€ç´¢ç›®æ ‡å›¾åƒã€‚</li>
<li>å½“å‰CIRæ–¹æ³•çš„æŒ‘æˆ˜åœ¨äºå¦‚ä½•æœ‰æ•ˆèåˆè§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯ã€‚</li>
<li>å­˜åœ¨çš„æ–¹æ³•å€¾å‘äºåå‘å‚è€ƒå›¾åƒæˆ–æ–‡æœ¬æè¿°ï¼Œå¯¼è‡´æ— æ³•å‡†ç¡®æ•æ‰ç”¨æˆ·æ„å›¾ã€‚</li>
<li>TMCIRæ¡†æ¶é€šè¿‡ä¸¤ç§åˆ›æ–°æ–¹æ³•è§£å†³è¿™ä¸€é—®é¢˜ï¼šæ„å›¾æ„ŸçŸ¥çš„è·¨æ¨¡æ€å¯¹é½å’Œè‡ªé€‚åº”ä»¤ç‰Œèåˆã€‚</li>
<li>æ„å›¾æ„ŸçŸ¥çš„è·¨æ¨¡æ€å¯¹é½é€šè¿‡åˆæˆåæ˜ æ„å›¾çš„ä¼ªç›®æ ‡å›¾åƒå¯¹CLIPç¼–ç å™¨è¿›è¡Œå¾®è°ƒã€‚</li>
<li>è‡ªé€‚åº”ä»¤ç‰Œèåˆé€šè¿‡å¯¹æ¯”ç‰¹å¾ä¸ç›®æ ‡å›¾åƒå¯¹ç¼–ç å™¨è¿›è¡Œå¾®è°ƒï¼Œå®ç°è§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯çš„åŠ¨æ€å¹³è¡¡ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒTMCIRåœ¨æ•æ‰ç”¨æˆ·ç»†å¾®æ„å›¾å’Œæ£€ç´¢æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºå½“å‰çš„ä¸»æµæ–¹æ³•ã€‚</li>
<li>TMCIRæ¡†æ¶åœ¨Fashion-IQå’ŒCIRRæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨ç°ä¼˜å¼‚ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10995">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9260b82f6a9f0121dacf24a1215fa811.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-47466a58a91dcc6cbd6c2e0d48dc7ce2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c526efb9290b57bfd298d9bcdafe865a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b6c1eacc22f15cb42b848d974e08cef7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-019b0f26132d258921676e52796fe977.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-09811b87c317db7bc2c378475e7d8a5d.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Bringing-together-invertible-UNets-with-invertible-attention-modules-for-memory-efficient-diffusion-models"><a href="#Bringing-together-invertible-UNets-with-invertible-attention-modules-for-memory-efficient-diffusion-models" class="headerlink" title="Bringing together invertible UNets with invertible attention modules for   memory-efficient diffusion models"></a>Bringing together invertible UNets with invertible attention modules for   memory-efficient diffusion models</h2><p><strong>Authors:Karan Jain, Mohammad Nayeem Teli</strong></p>
<p>Diffusion models have recently gained state of the art performance on many image generation tasks. However, most models require significant computational resources to achieve this. This becomes apparent in the application of medical image synthesis due to the 3D nature of medical datasets like CT-scans, MRIs, electron microscope, etc. In this paper we propose a novel architecture for a single GPU memory-efficient training for diffusion models for high dimensional medical datasets. The proposed model is built by using an invertible UNet architecture with invertible attention modules. This leads to the following two contributions: 1. denoising diffusion models and thus enabling memory usage to be independent of the dimensionality of the dataset, and 2. reducing the energy usage during training. While this new model can be applied to a multitude of image generation tasks, we showcase its memory-efficiency on the 3D BraTS2020 dataset leading to up to 15% decrease in peak memory consumption during training with comparable results to SOTA while maintaining the image quality. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹æœ€è¿‘åœ¨è®¸å¤šå›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°æ¨¡å‹ä¸ºäº†å®ç°è¿™ä¸€ç‚¹éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºã€‚è¿™åœ¨åŒ»å­¦å›¾åƒåˆæˆçš„åº”ç”¨ä¸­å˜å¾—å°¤ä¸ºæ˜æ˜¾ï¼Œå› ä¸ºåŒ»å­¦æ•°æ®é›†å¦‚CTæ‰«æã€MRIã€ç”µå­æ˜¾å¾®é•œç­‰çš„ä¸‰ç»´æ€§è´¨ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é’ˆå¯¹é«˜ç»´åŒ»å­¦æ•°æ®é›†æ‰©æ•£æ¨¡å‹çš„å•GPUå†…å­˜é«˜æ•ˆè®­ç»ƒçš„æ–°å‹æ¶æ„ã€‚è¯¥æ¨¡å‹é‡‡ç”¨å¯é€†UNetæ¶æ„å’Œå¯é€†æ³¨æ„åŠ›æ¨¡å—æ„å»ºã€‚è¿™å¸¦æ¥äº†ä»¥ä¸‹ä¸¤ä¸ªè´¡çŒ®ï¼š1.é™å™ªæ‰©æ•£æ¨¡å‹ï¼Œä»è€Œä½¿å†…å­˜ä½¿ç”¨é‡ä¸æ•°æ®é›†ç»´åº¦çš„æ— å…³æ€§æˆä¸ºå¯èƒ½ï¼›2.å‡å°‘è®­ç»ƒè¿‡ç¨‹ä¸­çš„èƒ½è€—ã€‚è™½ç„¶è¿™ç§æ–°å‹æ¨¡å‹å¯ä»¥åº”ç”¨äºå¤šç§å›¾åƒç”Ÿæˆä»»åŠ¡ï¼Œä½†æˆ‘ä»¬å±•ç¤ºäº†å…¶åœ¨3D BraTS2020æ•°æ®é›†ä¸Šçš„å†…å­˜æ•ˆç‡ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å³°å€¼å†…å­˜ä½¿ç”¨ç‡é™ä½äº†é«˜è¾¾15%ï¼ŒåŒæ—¶ä¿æŒä¸æœ€æ–°æŠ€æœ¯ç›¸å½“çš„ç»“æœå’Œå›¾åƒè´¨é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10883v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—äº†æœ€æ–°æ€§èƒ½è¡¨ç°ï¼Œä½†å…¶éœ€è¦å·¨å¤§çš„è®¡ç®—èµ„æºã€‚å¯¹äºåŒ»ç–—å›¾åƒåˆæˆåº”ç”¨è€Œè¨€ï¼Œç”±äºå…¶æ¶‰åŠé«˜ç»´åŒ»å­¦æ•°æ®é›†å¦‚CTæ‰«æã€MRIç­‰ï¼Œè¿™ä¸€é—®é¢˜å°¤ä¸ºçªå‡ºã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹æ‰©æ•£æ¨¡å‹çš„æ–°å‹å•GPUå†…å­˜é«˜æ•ˆè®­ç»ƒæ¶æ„ï¼Œé‡‡ç”¨å¯é€†UNetæ¶æ„å’Œå¯é€†æ³¨æ„åŠ›æ¨¡å—ï¼Œä¸ºåŒ»ç–—æ•°æ®é›†æä¾›äº†å»å™ªæ‰©æ•£æ¨¡å‹ã€‚æ­¤æ¨¡å‹å¯åº”ç”¨äºå¤šç§å›¾åƒç”Ÿæˆä»»åŠ¡ï¼Œå±•ç¤ºäº†å…¶åœ¨BraTSæ•°æ®é›†ä¸Šçš„å†…å­˜æ•ˆç‡ï¼Œå‡å°‘äº†è®­ç»ƒè¿‡ç¨‹ä¸­çš„å³°å€¼å†…å­˜ä½¿ç”¨é‡ï¼ŒåŒæ—¶åœ¨ç»´æŒå›¾åƒè´¨é‡çš„æƒ…å†µä¸‹è¾¾åˆ°äº†ä¸€æµçš„æ€§èƒ½è¡¨ç°ã€‚å…¶é™ä½äº†è®­ç»ƒå’Œè®¡ç®—æ—¶çš„èƒ½æºæˆæœ¬ä»¥åŠæ”¹å–„äº†å¯å¤ç”¨æ€§æˆä¸ºå…³é”®çš„ä¸¤ä¸ªä¼˜åŠ¿ã€‚æˆ‘ä»¬ç›¸ä¿¡æ‰©æ•£æ¨¡å‹çš„è½»é‡çº§è®¡ç®—è§£å†³æ–¹æ¡ˆä¸ä»…å°†ä½¿ç°ä»£ç¥ç»é€€è¡Œæ€§ç–¾ç—…çš„ä¸‰ç»´åŠ¨æ€æˆåƒæˆä¸ºå¯èƒ½ï¼Œè¿˜å°†ä¸ºåŒ»å­¦ç ”ç©¶å’Œè¯Šæ–­å¸¦æ¥é‡å¤§çªç ´ã€‚è¿™ä¸€çªç ´å°†ä¸ºåŒ»å­¦é¢†åŸŸå¸¦æ¥æ·±è¿œçš„å½±å“ã€‚ </p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æ‰©æ•£æ¨¡å‹åœ¨é«˜ç»´åŒ»ç–—æ•°æ®é›†ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚é’ˆå¯¹å›¾åƒç”Ÿæˆä»»åŠ¡ç‰¹åˆ«çªå‡ºã€‚å¯¹äºé«˜ç»´åŒ»å­¦æ•°æ®é›†å¦‚CTæ‰«æå’ŒMRIç­‰ï¼Œå…¶æ€§èƒ½è¡¨ç°å°¤ä¸ºæ˜¾è‘—ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10883">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c59b09a2c867cd62dd6eb2750f75a886.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="PT-Mark-Invisible-Watermarking-for-Text-to-image-Diffusion-Models-via-Semantic-aware-Pivotal-Tuning"><a href="#PT-Mark-Invisible-Watermarking-for-Text-to-image-Diffusion-Models-via-Semantic-aware-Pivotal-Tuning" class="headerlink" title="PT-Mark: Invisible Watermarking for Text-to-image Diffusion Models via   Semantic-aware Pivotal Tuning"></a>PT-Mark: Invisible Watermarking for Text-to-image Diffusion Models via   Semantic-aware Pivotal Tuning</h2><p><strong>Authors:Yaopeng Wang, Huiyu Xu, Zhibo Wang, Jiacheng Du, Zhichao Li, Yiming Li, Qiu Wang, Kui Ren</strong></p>
<p>Watermarking for diffusion images has drawn considerable attention due to the widespread use of text-to-image diffusion models and the increasing need for their copyright protection. Recently, advanced watermarking techniques, such as Tree Ring, integrate watermarks by embedding traceable patterns (e.g., Rings) into the latent distribution during the diffusion process. Such methods disrupt the original semantics of the generated images due to the inevitable distribution shift caused by the watermarks, thereby limiting their practicality, particularly in digital art creation. In this work, we present Semantic-aware Pivotal Tuning Watermarks (PT-Mark), a novel invisible watermarking method that preserves both the semantics of diffusion images and the traceability of the watermark. PT-Mark preserves the original semantics of the watermarked image by gradually aligning the generation trajectory with the original (pivotal) trajectory while maintaining the traceable watermarks during whole diffusion denoising process. To achieve this, we first compute the salient regions of the watermark at each diffusion denoising step as a spatial prior to identify areas that can be aligned without disrupting the watermark pattern. Guided by the region, we then introduce an additional pivotal tuning branch that optimizes the text embedding to align the semantics while preserving the watermarks. Extensive evaluations demonstrate that PT-Mark can preserve the original semantics of the diffusion images while integrating robust watermarks. It achieves a 10% improvement in the performance of semantic preservation (i.e., SSIM, PSNR, and LPIPS) compared to state-of-the-art watermarking methods, while also showing comparable robustness against real-world perturbations and four times greater efficiency. </p>
<blockquote>
<p>é’ˆå¯¹æ‰©æ•£å›¾åƒçš„æ°´å°æŠ€æœ¯å¾—åˆ°äº†å¹¿æ³›å…³æ³¨ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºæ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹çš„å¹¿æ³›åº”ç”¨å’Œå¯¹ç‰ˆæƒä¿æŠ¤çš„ä¸æ–­å¢é•¿çš„éœ€æ±‚ã€‚æœ€è¿‘ï¼Œå…ˆè¿›çš„æ°´å°æŠ€æœ¯ï¼Œå¦‚Tree Ringï¼Œé€šè¿‡åœ¨æ‰©æ•£è¿‡ç¨‹ä¸­å°†å¯è¿½è¸ªçš„æ¨¡å¼ï¼ˆä¾‹å¦‚åœ†ç¯ï¼‰åµŒå…¥æ½œåœ¨åˆ†å¸ƒæ¥é›†æˆæ°´å°ã€‚è¿™äº›æ–¹æ³•ç”±äºæ°´å°å¼•èµ·çš„ä¸å¯é¿å…çš„åˆ†å¸ƒåç§»è€Œç ´åäº†ç”Ÿæˆå›¾åƒçš„åŸè¯­æ„ï¼Œä»è€Œé™åˆ¶äº†å®ƒä»¬çš„å®ç”¨æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°å­—è‰ºæœ¯åˆ›ä½œä¸­ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†è¯­ä¹‰æ„ŸçŸ¥çš„æ¢è½´è°ƒæ•´æ°´å°ï¼ˆPT-Markï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„ä¸å¯è§æ°´å°æ–¹æ³•ï¼Œèƒ½å¤ŸåŒæ—¶ä¿ç•™æ‰©æ•£å›¾åƒè¯­ä¹‰å’Œæ°´å°çš„å¯è¿½è¸ªæ€§ã€‚PT-Marké€šè¿‡åœ¨æ•´ä¸ªæ‰©æ•£å»å™ªè¿‡ç¨‹ä¸­ä¿æŒå¯è¿½è¸ªçš„æ°´å°ï¼ŒåŒæ—¶é€æ­¥è°ƒæ•´ç”Ÿæˆè½¨è¿¹ä¸åŸå§‹ï¼ˆæ¢è½´ï¼‰è½¨è¿¹å¯¹é½ï¼Œä»è€Œä¿ç•™æ°´å°å›¾åƒçš„åŸè¯­æ„ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬é¦–å…ˆåœ¨æ¯ä¸ªæ‰©æ•£å»å™ªæ­¥éª¤ä¸­è®¡ç®—æ°´å°çš„æ˜¾è‘—åŒºåŸŸä½œä¸ºç©ºé—´å…ˆéªŒï¼Œä»¥è¯†åˆ«å¯ä»¥ä¸å¯¹é½è€Œä¸ä¼šå½±å“æ°´å°æ¨¡å¼çš„åŒºåŸŸã€‚åœ¨è¯¥åŒºåŸŸçš„æŒ‡å¯¼ä¸‹ï¼Œç„¶åæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªé¢å¤–çš„æ¢è½´è°ƒæ•´åˆ†æ”¯ï¼Œä¼˜åŒ–æ–‡æœ¬åµŒå…¥ä»¥å¯¹é½è¯­ä¹‰åŒæ—¶ä¿ç•™æ°´å°ã€‚å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒPT-Markèƒ½å¤Ÿåœ¨é›†æˆç¨³å¥æ°´å°çš„åŒæ—¶ä¿ç•™æ‰©æ•£å›¾åƒçš„åŸè¯­æ„ã€‚ä¸æœ€å…ˆè¿›çš„æ°´å°æ–¹æ³•ç›¸æ¯”ï¼Œå®ƒåœ¨è¯­ä¹‰ä¿ç•™æ€§èƒ½ï¼ˆå³ç»“æ„ç›¸ä¼¼æ€§åº¦é‡ã€å³°å€¼ä¿¡å™ªæ¯”å’Œç»“æ„ç›¸ä¼¼æ€§åº¦é‡æŒ‡æ•°ï¼‰ä¸Šæé«˜äº†10%ï¼ŒåŒæ—¶æ˜¾ç¤ºå‡ºå¯¹ç°å®ä¸–ç•Œæ‰°åŠ¨çš„ç›¸å½“é²æ£’æ€§å¹¶æé«˜å››å€æ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10853v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ‰©æ•£å›¾åƒæ°´å°å› å…¶å¹¿æ³›åº”ç”¨äºæ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹åŠç‰ˆæƒä¿æŠ¤éœ€æ±‚è€Œå—åˆ°å…³æ³¨ã€‚æœ€æ–°çš„æ°´å°æŠ€æœ¯ï¼Œå¦‚Tree Ringï¼Œé€šè¿‡åµŒå…¥å¯è¿½æº¯æ¨¡å¼ï¼ˆä¾‹å¦‚ç¯å½¢ï¼‰åˆ°æ‰©æ•£è¿‡ç¨‹çš„æ½œåœ¨åˆ†å¸ƒä¸­è¿›è¡Œé›†æˆã€‚ä½†è¿™ç§æ–¹æ³•ä¼šç ´åç”Ÿæˆå›¾åƒçš„åŸè¯­ä¹‰ï¼Œé™åˆ¶äº†å…¶å®ç”¨æ€§ã€‚åœ¨æ­¤ï¼Œæˆ‘ä»¬æå‡ºä¸€ç§æ–°å‹ä¸å¯è§æ°´å°æ–¹æ³•â€”â€”è¯­ä¹‰æ„ŸçŸ¥å…³é”®è°ƒæ•´æ°´å°ï¼ˆPT-Markï¼‰ï¼Œå®ƒæ—¢èƒ½ä¿ç•™æ‰©æ•£å›¾åƒè¯­ä¹‰ï¼Œåˆèƒ½è¿½æº¯æ°´å°ã€‚PT-Marké€šè¿‡é€æ­¥è°ƒæ•´ç”Ÿæˆè½¨è¿¹ä¸åŸå§‹ï¼ˆå…³é”®ï¼‰è½¨è¿¹å¯¹é½ï¼ŒåŒæ—¶åœ¨æ•´ä¸ªæ‰©æ•£å»å™ªè¿‡ç¨‹ä¸­ä¿æŒå¯è¿½æº¯çš„æ°´å°ï¼Œä»è€Œä¿ç•™æ°´å°å›¾åƒçš„åŸè¯­ä¹‰ã€‚ä¸ºå®ç°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬é¦–å…ˆåœ¨æ¯ä¸ªæ‰©æ•£å»å™ªæ­¥éª¤ä¸­è®¡ç®—æ°´å°çš„æ˜¾è‘—åŒºåŸŸä½œä¸ºç©ºé—´å…ˆéªŒï¼Œä»¥è¯†åˆ«å¯å¯¹é½çš„åŒºåŸŸè€Œä¸å¹²æ‰°æ°´å°å›¾æ¡ˆã€‚åœ¨è¯¥åŒºåŸŸçš„å¼•å¯¼ä¸‹ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªé¢å¤–çš„å…³é”®è°ƒæ•´åˆ†æ”¯ï¼Œä¼˜åŒ–æ–‡æœ¬åµŒå…¥ä»¥å¯¹é½è¯­ä¹‰åŒæ—¶ä¿ç•™æ°´å°ã€‚è¯„ä¼°è¡¨æ˜ï¼ŒPT-Markåœ¨ä¿ç•™æ‰©æ•£å›¾åƒåŸè¯­ä¹‰çš„åŒæ—¶é›†æˆäº†ç¨³å¥çš„æ°´å°ï¼Œåœ¨è¯­ä¹‰ä¿ç•™æ€§èƒ½ä¸Šè¾ƒç°æœ‰æ°´å°æ–¹æ³•æé«˜äº†10%ï¼ŒåŒæ—¶æ˜¾ç¤ºäº†å¯¹ç°å®ä¸–ç•Œæ‰°åŠ¨çš„å¯æ¯”æ€§é²æ£’æ€§å’Œå››å€æ•ˆç‡ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ‰©æ•£å›¾åƒæ°´å°å› æ–‡æœ¬-å›¾åƒæ‰©æ•£æ¨¡å‹å¹¿æ³›åº”ç”¨å’Œç‰ˆæƒä¿æŠ¤éœ€æ±‚è€Œå—åˆ°å…³æ³¨ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¦‚Tree Ringä¼šç ´åç”Ÿæˆå›¾åƒçš„åŸè¯­ä¹‰ï¼Œé™åˆ¶å…¶å®ç”¨æ€§ã€‚</li>
<li>PT-Markæ˜¯ä¸€ç§æ–°å‹ä¸å¯è§æ°´å°æ–¹æ³•ï¼Œèƒ½åŒæ—¶ä¿ç•™æ‰©æ•£å›¾åƒçš„è¯­ä¹‰å’Œè¿½æº¯æ°´å°ã€‚</li>
<li>PT-Marké€šè¿‡é€æ­¥å¯¹é½ç”Ÿæˆè½¨è¿¹ä¸åŸå§‹è½¨è¿¹ï¼ŒåŒæ—¶ä¿æŒæ•´ä¸ªæ‰©æ•£å»å™ªè¿‡ç¨‹ä¸­çš„æ°´å°æ¥å®ç°è¯­ä¹‰ä¿ç•™ã€‚</li>
<li>PT-Markå¼•å…¥ç©ºé—´å…ˆéªŒå’Œå…³é”®è°ƒæ•´åˆ†æ”¯ä»¥æé«˜è¯­ä¹‰ä¿ç•™å’Œæ°´å°çš„ç¨³å¥æ€§ã€‚</li>
<li>è¯„ä¼°æ˜¾ç¤ºPT-Markåœ¨è¯­ä¹‰ä¿ç•™æ€§èƒ½ä¸Šè¾ƒç°æœ‰æ–¹æ³•æœ‰æ‰€æå‡ï¼ŒåŒæ—¶æ•ˆç‡æ›´é«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10853">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c8eff5bc5841efaa1393f66480527004.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-50b0658721b6a93677247e04d42ae815.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-658c04a0563e4a55e8caaecbde0a6840.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5ecd0d79a7cc989cd7cf043ca2a508c4.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="GaSLight-Gaussian-Splats-for-Spatially-Varying-Lighting-in-HDR"><a href="#GaSLight-Gaussian-Splats-for-Spatially-Varying-Lighting-in-HDR" class="headerlink" title="GaSLight: Gaussian Splats for Spatially-Varying Lighting in HDR"></a>GaSLight: Gaussian Splats for Spatially-Varying Lighting in HDR</h2><p><strong>Authors:Christophe Bolduc, Yannick Hold-Geoffroy, Zhixin Shu, Jean-FranÃ§ois Lalonde</strong></p>
<p>We present GaSLight, a method that generates spatially-varying lighting from regular images. Our method proposes using HDR Gaussian Splats as light source representation, marking the first time regular images can serve as light sources in a 3D renderer. Our two-stage process first enhances the dynamic range of images plausibly and accurately by leveraging the priors embedded in diffusion models. Next, we employ Gaussian Splats to model 3D lighting, achieving spatially variant lighting. Our approach yields state-of-the-art results on HDR estimations and their applications in illuminating virtual objects and scenes. To facilitate the benchmarking of images as light sources, we introduce a novel dataset of calibrated and unsaturated HDR to evaluate images as light sources. We assess our method using a combination of this novel dataset and an existing dataset from the literature. The code to reproduce our method will be available upon acceptance. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†GaSLightæ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥ä»å¸¸è§„å›¾åƒç”Ÿæˆç©ºé—´å˜åŒ–çš„å…‰ç…§ã€‚æˆ‘ä»¬çš„æ–¹æ³•å»ºè®®ä½¿ç”¨HDRé«˜æ–¯Splatsä½œä¸ºå…‰æºè¡¨ç¤ºï¼Œè¿™æ˜¯é¦–æ¬¡å°†å¸¸è§„å›¾åƒä½œä¸º3Dæ¸²æŸ“å™¨çš„å…‰æºã€‚æˆ‘ä»¬çš„ä¸¤é˜¶æ®µè¿‡ç¨‹é¦–å…ˆåˆ©ç”¨æ‰©æ•£æ¨¡å‹ä¸­åµŒå…¥çš„å…ˆéªŒçŸ¥è¯†ï¼Œä»¥åˆç†ä¸”å‡†ç¡®çš„æ–¹å¼å¢å¼ºå›¾åƒçš„åŠ¨æ€èŒƒå›´ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬é‡‡ç”¨é«˜æ–¯Splatså¯¹3Dç…§æ˜è¿›è¡Œå»ºæ¨¡ï¼Œå®ç°ç©ºé—´å˜åŒ–çš„å…‰ç…§ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨HDRä¼°è®¡åŠå…¶ç”¨äºç…§æ˜è™šæ‹Ÿå¯¹è±¡å’Œåœºæ™¯çš„åº”ç”¨æ–¹é¢äº§ç”Ÿäº†æœ€å…ˆè¿›çš„æˆæœã€‚ä¸ºäº†å¯¹å›¾åƒä½œä¸ºå…‰æºè¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°å‹æ ¡å‡†å’Œä¸é¥±å’ŒHDRæ•°æ®é›†æ¥è¯„ä¼°å›¾åƒä½œä¸ºå…‰æºã€‚æˆ‘ä»¬ä½¿ç”¨è¿™ä¸ªæ–°å‹æ•°æ®é›†å’Œæ–‡çŒ®ä¸­çš„ç°æœ‰æ•°æ®é›†æ¥è¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•ã€‚æ¥å—åå°†æä¾›å¤åˆ¶æˆ‘ä»¬æ–¹æ³•çš„ä»£ç ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10809v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºGaSLightçš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥ä»å¸¸è§„å›¾åƒç”Ÿæˆç©ºé—´å˜åŒ–çš„å…‰ç…§ã€‚æˆ‘ä»¬ä½¿ç”¨HDRé«˜æ–¯Splatsä½œä¸ºå…‰æºè¡¨ç¤ºï¼Œè¿™æ˜¯é¦–æ¬¡å°†å¸¸è§„å›¾åƒä½œä¸ºå…‰æºç”¨äºä¸‰ç»´æ¸²æŸ“ã€‚æˆ‘ä»¬çš„ä¸¤æ­¥è¿‡ç¨‹é¦–å…ˆåˆ©ç”¨æ‰©æ•£æ¨¡å‹ä¸­çš„å…ˆéªŒçŸ¥è¯†ï¼Œä»¥åˆç†ä¸”å‡†ç¡®çš„æ–¹å¼å¢å¼ºå›¾åƒçš„åŠ¨æ€èŒƒå›´ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨é«˜æ–¯Splatså¯¹ä¸‰ç»´ç…§æ˜è¿›è¡Œå»ºæ¨¡ï¼Œå®ç°ç©ºé—´å˜åŒ–çš„å…‰ç…§ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨HDRä¼°è®¡åŠå…¶ç”¨äºç…§æ˜è™šæ‹Ÿå¯¹è±¡å’Œåœºæ™¯æ–¹é¢çš„åº”ç”¨æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚ä¸ºäº†æ–¹ä¾¿å›¾åƒä½œä¸ºå…‰æºçš„åŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°å‹çš„æ ¡å‡†å’Œä¸é¥±å’ŒHDRæ•°æ®é›†æ¥è¯„ä¼°å›¾åƒä½œä¸ºå…‰æºã€‚æˆ‘ä»¬ä½¿ç”¨è¿™ä¸ªæ–°æ•°æ®é›†å’Œæ–‡çŒ®ä¸­çš„ç°æœ‰æ•°æ®é›†æ¥è¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GaSLightæ–¹æ³•èƒ½ä»å¸¸è§„å›¾åƒç”Ÿæˆç©ºé—´å˜åŒ–çš„å…‰ç…§ã€‚</li>
<li>HDRé«˜æ–¯Splatsé¦–æ¬¡è¢«ç”¨ä½œå…‰æºè¡¨ç¤ºï¼Œç”¨äºä¸‰ç»´æ¸²æŸ“ã€‚</li>
<li>è¯¥æ–¹æ³•é‡‡ç”¨ä¸¤æ­¥è¿‡ç¨‹ï¼Œé¦–å…ˆå¢å¼ºå›¾åƒåŠ¨æ€èŒƒå›´ï¼Œç„¶ååˆ©ç”¨é«˜æ–¯Splatsè¿›è¡Œä¸‰ç»´ç…§æ˜å»ºæ¨¡ã€‚</li>
<li>æ–¹æ³•åœ¨HDRä¼°è®¡åŠå…¶åº”ç”¨äºè™šæ‹Ÿå¯¹è±¡å’Œåœºæ™¯çš„ç…§æ˜æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚</li>
<li>ä¸ºäº†è¯„ä¼°å›¾åƒä½œä¸ºå…‰æºçš„æ•ˆæœï¼Œå¼•å…¥äº†ä¸€ä¸ªæ–°å‹çš„æ ¡å‡†å’Œä¸é¥±å’ŒHDRæ•°æ®é›†ã€‚</li>
<li>è¯¥æ–¹æ³•ç»“åˆäº†æ–°æ•°æ®é›†å’Œç°æœ‰æ•°æ®é›†è¿›è¡Œè¯„ä¼°ã€‚</li>
<li>æ–¹æ³•ä»£ç åœ¨æ¥å—åå°†å…¬å¼€å¯ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10809">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bd1b6add52d68b6d05cee2576476a970.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a587eee72f338f28224d08dea7b8d954.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-faad0dc0950fb458ed849d09cbe96dc9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b148832e6da27501a698c1717c440602.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-95ce0b37073da87d139eff1cacc105cc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-606d27e2d875c7ceeb31eb0cba8ec175.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="H3AE-High-Compression-High-Speed-and-High-Quality-AutoEncoder-for-Video-Diffusion-Models"><a href="#H3AE-High-Compression-High-Speed-and-High-Quality-AutoEncoder-for-Video-Diffusion-Models" class="headerlink" title="H3AE: High Compression, High Speed, and High Quality AutoEncoder for   Video Diffusion Models"></a>H3AE: High Compression, High Speed, and High Quality AutoEncoder for   Video Diffusion Models</h2><p><strong>Authors:Yushu Wu, Yanyu Li, Ivan Skorokhodov, Anil Kag, Willi Menapace, Sharath Girish, Aliaksandr Siarohin, Yanzhi Wang, Sergey Tulyakov</strong></p>
<p>Autoencoder (AE) is the key to the success of latent diffusion models for image and video generation, reducing the denoising resolution and improving efficiency. However, the power of AE has long been underexplored in terms of network design, compression ratio, and training strategy. In this work, we systematically examine the architecture design choices and optimize the computation distribution to obtain a series of efficient and high-compression video AEs that can decode in real time on mobile devices. We also unify the design of plain Autoencoder and image-conditioned I2V VAE, achieving multifunctionality in a single network. In addition, we find that the widely adopted discriminative losses, i.e., GAN, LPIPS, and DWT losses, provide no significant improvements when training AEs at scale. We propose a novel latent consistency loss that does not require complicated discriminator design or hyperparameter tuning, but provides stable improvements in reconstruction quality. Our AE achieves an ultra-high compression ratio and real-time decoding speed on mobile while outperforming prior art in terms of reconstruction metrics by a large margin. We finally validate our AE by training a DiT on its latent space and demonstrate fast, high-quality text-to-video generation capability. </p>
<blockquote>
<p>è‡ªåŠ¨ç¼–ç å™¨ï¼ˆAEï¼‰æ˜¯æ½œæ‰©æ•£æ¨¡å‹åœ¨å›¾åƒå’Œè§†é¢‘ç”Ÿæˆæ–¹é¢æˆåŠŸçš„å…³é”®ï¼Œå¯ä»¥é™ä½é™å™ªåˆ†è¾¨ç‡å¹¶æé«˜æ•ˆç‡ã€‚ç„¶è€Œï¼Œå…³äºè‡ªåŠ¨ç¼–ç å™¨çš„ç½‘ç»œè®¾è®¡ã€å‹ç¼©æ¯”å’Œè®­ç»ƒç­–ç•¥ç­‰æ–¹é¢çš„æ½œåŠ›ä¸€ç›´è¢«ä½ä¼°ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°ç ”ç©¶äº†æ¶æ„è®¾è®¡é€‰æ‹©ï¼Œä¼˜åŒ–äº†è®¡ç®—åˆ†é…ï¼Œè·å¾—äº†ä¸€ç³»åˆ—é«˜æ•ˆçš„é«˜å‹ç¼©è§†é¢‘è‡ªåŠ¨ç¼–ç å™¨ï¼Œèƒ½å¤Ÿåœ¨ç§»åŠ¨è®¾å¤‡ä¸Šå®æ—¶è§£ç ã€‚æˆ‘ä»¬è¿˜ç»Ÿä¸€äº†æ™®é€šè‡ªåŠ¨ç¼–ç å™¨å’Œå›¾åƒæ¡ä»¶I2V VAEçš„è®¾è®¡ï¼Œåœ¨å•ä¸ªç½‘ç»œä¸­å®ç°äº†å¤šåŠŸèƒ½æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°å¹¿æ³›é‡‡ç”¨çš„åˆ¤åˆ«æŸå¤±ï¼Œå³GANã€LPIPSå’ŒDWTæŸå¤±ï¼Œåœ¨è®­ç»ƒå¤§è§„æ¨¡AEæ—¶å¹¶æ²¡æœ‰æä¾›æ˜¾è‘—çš„æ”¹è¿›ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ½œåœ¨ä¸€è‡´æ€§æŸå¤±ï¼Œå®ƒä¸éœ€è¦å¤æ‚çš„é‰´åˆ«å™¨è®¾è®¡æˆ–è¶…å‚æ•°è°ƒæ•´ï¼Œä½†åœ¨é‡å»ºè´¨é‡æ–¹é¢æä¾›äº†ç¨³å®šçš„æ”¹è¿›ã€‚æˆ‘ä»¬çš„è‡ªåŠ¨ç¼–ç å™¨åœ¨ç§»åŠ¨è®¾å¤‡ä¸Šå®ç°äº†è¶…é«˜çš„å‹ç¼©æ¯”å’Œå®æ—¶è§£ç é€Ÿåº¦ï¼ŒåŒæ—¶åœ¨é‡å»ºæŒ‡æ ‡æ–¹é¢å¤§å¤§è¶…è¶Šäº†å…ˆå‰çš„æŠ€æœ¯ã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡åœ¨å…¶æ½œåœ¨ç©ºé—´ä¸Šè®­ç»ƒDiTæ¥éªŒè¯æˆ‘ä»¬çš„è‡ªåŠ¨ç¼–ç å™¨ï¼Œå¹¶å±•ç¤ºäº†å¿«é€Ÿã€é«˜è´¨é‡çš„æ–‡å­—åˆ°è§†é¢‘ç”Ÿæˆèƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10567v1">PDF</a> 8 pages, 4 figures, 6 tables</p>
<p><strong>Summary</strong></p>
<p>è‡ªåŠ¨ç¼–ç å™¨ï¼ˆAEï¼‰æ˜¯æ½œåœ¨æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒå’Œè§†é¢‘ç”Ÿæˆä¸­æˆåŠŸçš„å…³é”®ï¼Œå®ƒé€šè¿‡é™ä½å»å™ªåˆ†è¾¨ç‡å¹¶æé«˜æ•ˆç‡æ¥å®ç°è¿™ä¸€ç‚¹ã€‚ç„¶è€Œï¼Œå…³äºè‡ªåŠ¨ç¼–ç å™¨çš„ç½‘ç»œè®¾è®¡ã€å‹ç¼©æ¯”å’Œè®­ç»ƒç­–ç•¥ç­‰æ–¹é¢çš„æ½œåŠ›ä¸€ç›´è¢«å¿½è§†ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°ç ”ç©¶äº†æ¶æ„è®¾è®¡çš„é€‰æ‹©ï¼Œä¼˜åŒ–äº†è®¡ç®—åˆ†å¸ƒï¼Œè·å¾—äº†ä¸€ç³»åˆ—é«˜æ•ˆä¸”é«˜å‹ç¼©çš„è§†é¢‘è‡ªåŠ¨ç¼–ç å™¨ï¼Œèƒ½å¤Ÿåœ¨ç§»åŠ¨è®¾å¤‡ä¸Šå®æ—¶è§£ç ã€‚æˆ‘ä»¬è¿˜ç»Ÿä¸€äº†æ™®é€šè‡ªåŠ¨ç¼–ç å™¨å’Œå›¾åƒæ¡ä»¶I2V VAEçš„è®¾è®¡ï¼Œå®ç°å•ä¸€ç½‘ç»œçš„å¤šåŠŸèƒ½æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°å¹¿æ³›é‡‡ç”¨çš„åˆ¤åˆ«æŸå¤±ï¼ˆå¦‚GANã€LPIPSå’ŒDWTæŸå¤±ï¼‰åœ¨è®­ç»ƒå¤§è§„æ¨¡AEæ—¶å¹¶æœªæä¾›æ˜¾è‘—æ”¹è¿›ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ½œåœ¨ä¸€è‡´æ€§æŸå¤±ï¼Œè¯¥æŸå¤±ä¸éœ€è¦å¤æ‚çš„é‰´åˆ«å™¨è®¾è®¡æˆ–è¶…å‚æ•°è°ƒæ•´ï¼Œä½†åœ¨é‡å»ºè´¨é‡ä¸Šæä¾›äº†ç¨³å®šçš„æ”¹è¿›ã€‚æˆ‘ä»¬çš„è‡ªåŠ¨ç¼–ç å™¨åœ¨å…·æœ‰è¶…é«˜å‹ç¼©æ¯”å’Œå®æ—¶è§£ç é€Ÿåº¦çš„åŒæ—¶ï¼Œåœ¨é‡å»ºæŒ‡æ ‡ä¸Šå¤§å¹…åº¦è¶…è¶Šäº†å…ˆå‰æŠ€æœ¯ã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡åœ¨å…¶æ½œåœ¨ç©ºé—´ä¸Šè®­ç»ƒDiTæ¥éªŒè¯æˆ‘ä»¬çš„è‡ªåŠ¨ç¼–ç å™¨ï¼Œå¹¶å±•ç¤ºäº†å¿«é€Ÿã€é«˜è´¨é‡çš„æ–‡å­—åˆ°è§†é¢‘ç”Ÿæˆèƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨ç¼–ç å™¨ï¼ˆAEï¼‰æ˜¯æ½œåœ¨æ‰©æ•£æ¨¡å‹æˆåŠŸçš„å…³é”®ï¼Œå°¤å…¶åœ¨å›¾åƒå’Œè§†é¢‘ç”Ÿæˆé¢†åŸŸã€‚</li>
<li>é€šè¿‡ä¼˜åŒ–ç½‘ç»œè®¾è®¡ã€å‹ç¼©æ¯”å’Œè®­ç»ƒç­–ç•¥ï¼Œæé«˜äº†è§†é¢‘è‡ªåŠ¨ç¼–ç å™¨çš„æ•ˆç‡å’Œæ€§èƒ½ã€‚</li>
<li>å®ç°äº†ä¸€ç§å®æ—¶è§£ç çš„ç§»åŠ¨è®¾å¤‡ä¸Šçš„é«˜æ•ˆã€é«˜å‹ç¼©è§†é¢‘è‡ªåŠ¨ç¼–ç å™¨ã€‚</li>
<li>èåˆäº†æ™®é€šè‡ªåŠ¨ç¼–ç å™¨å’Œå›¾åƒæ¡ä»¶I2V VAEçš„è®¾è®¡ï¼Œå¢å¼ºäº†å•ä¸€ç½‘ç»œçš„å¤šåŠŸèƒ½æ€§ã€‚</li>
<li>ä¼ ç»Ÿçš„åˆ¤åˆ«æŸå¤±åœ¨è®­ç»ƒå¤§è§„æ¨¡AEæ—¶æ•ˆæœæœ‰é™ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹æ½œåœ¨ä¸€è‡´æ€§æŸå¤±ï¼Œèƒ½æœ‰æ•ˆæå‡è‡ªåŠ¨ç¼–ç å™¨çš„é‡å»ºè´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10567">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0d721cc0ecdc88fab6d4fdd4030db761.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b64eda8b08a1cfa35bf5f1e7b769ba17.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4bd1bf90ffa5d1916a3438862d0ac1d3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c8fadabc7dd7fe57d1e65f5c5b403661.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b59dc8846a299cc1af237a8292d44155.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9ab7725147cabe8642890df1add6fa2b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7cd7d5d1002b3e29ca67d96b661ca801.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="OctGPT-Octree-based-Multiscale-Autoregressive-Models-for-3D-Shape-Generation"><a href="#OctGPT-Octree-based-Multiscale-Autoregressive-Models-for-3D-Shape-Generation" class="headerlink" title="OctGPT: Octree-based Multiscale Autoregressive Models for 3D Shape   Generation"></a>OctGPT: Octree-based Multiscale Autoregressive Models for 3D Shape   Generation</h2><p><strong>Authors:Si-Tong Wei, Rui-Huan Wang, Chuan-Zhi Zhou, Baoquan Chen, Peng-Shuai Wang</strong></p>
<p>Autoregressive models have achieved remarkable success across various domains, yet their performance in 3D shape generation lags significantly behind that of diffusion models. In this paper, we introduce OctGPT, a novel multiscale autoregressive model for 3D shape generation that dramatically improves the efficiency and performance of prior 3D autoregressive approaches, while rivaling or surpassing state-of-the-art diffusion models. Our method employs a serialized octree representation to efficiently capture the hierarchical and spatial structures of 3D shapes. Coarse geometry is encoded via octree structures, while fine-grained details are represented by binary tokens generated using a vector quantized variational autoencoder (VQVAE), transforming 3D shapes into compact multiscale binary sequences suitable for autoregressive prediction. To address the computational challenges of handling long sequences, we incorporate octree-based transformers enhanced with 3D rotary positional encodings, scale-specific embeddings, and token-parallel generation schemes. These innovations reduce training time by 13 folds and generation time by 69 folds, enabling the efficient training of high-resolution 3D shapes, e.g.,$1024^3$, on just four NVIDIA 4090 GPUs only within days. OctGPT showcases exceptional versatility across various tasks, including text-, sketch-, and image-conditioned generation, as well as scene-level synthesis involving multiple objects. Extensive experiments demonstrate that OctGPT accelerates convergence and improves generation quality over prior autoregressive methods, offering a new paradigm for high-quality, scalable 3D content creation. Our code and trained models are available at <a target="_blank" rel="noopener" href="https://github.com/octree-nn/octgpt">https://github.com/octree-nn/octgpt</a>. </p>
<blockquote>
<p>è‡ªå›å½’æ¨¡å‹å·²ç»åœ¨å¤šä¸ªé¢†åŸŸå–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†åœ¨3Då½¢çŠ¶ç”Ÿæˆæ–¹é¢çš„æ€§èƒ½å´è¿œè¿œè½åäºæ‰©æ•£æ¨¡å‹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†OctGPTï¼Œè¿™æ˜¯ä¸€ç§ç”¨äº3Då½¢çŠ¶ç”Ÿæˆçš„æ–°å‹å¤šå°ºåº¦è‡ªå›å½’æ¨¡å‹ï¼Œå®ƒå¤§å¤§æé«˜äº†å…ˆå‰3Dè‡ªå›å½’æ–¹æ³•çš„æ•ˆç‡å’Œæ€§èƒ½ï¼ŒåŒæ—¶ä¸æœ€å…ˆè¿›çš„æ‰©æ•£æ¨¡å‹ç›¸åŒ¹æ•Œç”šè‡³è¶…è¶Šã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨åºåˆ—åŒ–å…«å‰æ ‘è¡¨ç¤ºå½¢å¼ï¼Œä»¥æœ‰æ•ˆåœ°æ•è·3Då½¢çŠ¶çš„å±‚æ¬¡ç»“æ„å’Œç©ºé—´ç»“æ„ã€‚ç²—å‡ ä½•ç»“æ„é€šè¿‡å…«å‰æ ‘ç»“æ„è¿›è¡Œç¼–ç ï¼Œè€Œç²¾ç»†ç»†èŠ‚åˆ™ç”±ä½¿ç”¨å‘é‡é‡åŒ–å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ï¼ˆVQVAEï¼‰ç”Ÿæˆçš„äºŒè¿›åˆ¶ä»¤ç‰Œè¡¨ç¤ºï¼Œå°†3Då½¢çŠ¶è½¬æ¢ä¸ºé€‚åˆè‡ªå›å½’é¢„æµ‹çš„å¤šå°ºåº¦äºŒè¿›åˆ¶åºåˆ—ã€‚ä¸ºäº†è§£å†³å¤„ç†é•¿åºåˆ—çš„è®¡ç®—æŒ‘æˆ˜ï¼Œæˆ‘ä»¬ç»“åˆäº†åŸºäºå…«å‰æ ‘çš„å˜å‹å™¨ï¼Œå¹¶å¢å¼ºäº†3Dæ—‹è½¬ä½ç½®ç¼–ç ã€å°ºåº¦ç‰¹å®šåµŒå…¥å’Œä»¤ç‰Œå¹¶è¡Œç”Ÿæˆæ–¹æ¡ˆã€‚è¿™äº›åˆ›æ–°å°†è®­ç»ƒæ—¶é—´ç¼©çŸ­äº†13å€ï¼Œç”Ÿæˆæ—¶é—´ç¼©çŸ­äº†69å€ï¼Œä½¿å¾—åœ¨ä»…å››å°NVIDIA 4090 GPUä¸Šåœ¨å‡ å¤©å†…å¯¹é«˜åˆ†è¾¨ç‡3Då½¢çŠ¶ï¼ˆä¾‹å¦‚1024^3ï¼‰è¿›è¡Œæœ‰æ•ˆè®­ç»ƒæˆä¸ºå¯èƒ½ã€‚OctGPTåœ¨å„ç§ä»»åŠ¡ä¸­å±•ç¤ºäº†å‡ºè‰²çš„é€šç”¨æ€§ï¼ŒåŒ…æ‹¬æ–‡æœ¬ã€è‰å›¾å’Œå›¾åƒæ¡ä»¶ä¸‹çš„ç”Ÿæˆï¼Œä»¥åŠæ¶‰åŠå¤šä¸ªå¯¹è±¡çš„åœºæ™¯çº§åˆ«åˆæˆã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒOctGPTåŠ é€Ÿäº†æ”¶æ•›ï¼Œæé«˜äº†ç”Ÿæˆè´¨é‡ï¼Œä¸ºé«˜è´¨é‡ã€å¯æ‰©å±•çš„3Då†…å®¹åˆ›å»ºæä¾›äº†æ–°çš„èŒƒå¼ã€‚æˆ‘ä»¬çš„ä»£ç å’Œè®­ç»ƒå¥½çš„æ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/octree-nn/octgpt%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/octree-nn/octgptä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.09975v2">PDF</a> SIGGRAPH 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†OctGPTæ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºä¸‰ç»´å½¢çŠ¶ç”Ÿæˆçš„æ–°å‹å¤šå°ºåº¦è‡ªå›å½’æ¨¡å‹ã€‚è¯¥æ¨¡å‹é€šè¿‡é‡‡ç”¨åºåˆ—åŒ–å…«å‰æ ‘è¡¨ç¤ºæ³•ï¼Œæœ‰æ•ˆæ•æ‰ä¸‰ç»´å½¢çŠ¶çš„å±‚æ¬¡å’Œç©ºé—´ç»“æ„ï¼Œæ”¹è¿›äº†å…ˆå‰çš„ä¸‰ç»´è‡ªå›å½’æ–¹æ³•çš„æ•ˆç‡å’Œæ€§èƒ½ï¼ŒåŒæ—¶ä¸æœ€å…ˆè¿›çš„æ‰©æ•£æ¨¡å‹ç›¸æŠ—è¡¡æˆ–æ›´èƒœä¸€ç­¹ã€‚OctGPTå…·æœ‰å‡ºè‰²çš„å¤šä»»åŠ¡é€‚åº”æ€§ï¼ŒåŒ…æ‹¬æ–‡æœ¬ã€è‰å›¾ã€å›¾åƒæ¡ä»¶ç”Ÿæˆä»¥åŠæ¶‰åŠå¤šä¸ªå¯¹è±¡çš„åœºæ™¯çº§åˆ«åˆæˆã€‚å®ƒé€šè¿‡åˆ›æ–°çš„æŠ€æœ¯æ‰‹æ®µï¼Œå¦‚åŸºäºå…«å‰æ ‘çš„å˜å‹å™¨ã€ä¸‰ç»´æ—‹è½¬ä½ç½®ç¼–ç ã€å°ºåº¦ç‰¹å®šåµŒå…¥å’Œä»¤ç‰Œå¹¶è¡Œç”Ÿæˆæ–¹æ¡ˆï¼Œè§£å†³äº†å¤„ç†é•¿åºåˆ—çš„è®¡ç®—æŒ‘æˆ˜ã€‚è¯¥æ¨¡å‹ä¸ºé«˜è´¨é‡ã€å¯ä¼¸ç¼©çš„ä¸‰ç»´å†…å®¹åˆ›å»ºæä¾›äº†æ–°çš„èŒƒä¾‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OctGPTæ˜¯ä¸€ä¸ªç”¨äº3Då½¢çŠ¶ç”Ÿæˆçš„å¤šå°ºåº¦è‡ªå›å½’æ¨¡å‹ï¼Œæ˜¾è‘—æé«˜äº†å…ˆå‰è‡ªå›å½’æ–¹æ³•çš„æ•ˆç‡å’Œæ€§èƒ½ã€‚</li>
<li>OctGPTé‡‡ç”¨äº†åºåˆ—åŒ–å…«å‰æ ‘è¡¨ç¤ºæ³•ï¼Œæœ‰æ•ˆæ•æ‰äº†3Då½¢çŠ¶çš„å±‚æ¬¡å’Œç©ºé—´ç»“æ„ã€‚</li>
<li>OctGPTå¯ä¸æœ€å…ˆè¿›çš„æ‰©æ•£æ¨¡å‹ç›¸æŠ—è¡¡æˆ–è¡¨ç°æ›´ä½³ã€‚</li>
<li>OctGPTå…·æœ‰å‡ºè‰²çš„å¤šä»»åŠ¡é€‚åº”æ€§ï¼ŒåŒ…æ‹¬æ–‡æœ¬ã€è‰å›¾ã€å›¾åƒæ¡ä»¶ç”Ÿæˆå’Œåœºæ™¯çº§åˆ«åˆæˆã€‚</li>
<li>é€šè¿‡é‡‡ç”¨åŸºäºå…«å‰æ ‘çš„å˜å‹å™¨ç­‰åˆ›æ–°æŠ€æœ¯ï¼Œè§£å†³äº†å¤„ç†é•¿åºåˆ—çš„è®¡ç®—æŒ‘æˆ˜ã€‚</li>
<li>OctGPTå®ç°äº†é«˜æ•ˆçš„è®­ç»ƒï¼Œèƒ½å¤Ÿåœ¨ä»…å››å¤©å†…åœ¨å››å°NVIDIA 4090 GPUä¸Šè®­ç»ƒé«˜åˆ†è¾¨ç‡ï¼ˆä¾‹å¦‚1024^3ï¼‰çš„3Då½¢çŠ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.09975">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-510495a30a76a8367e0998c6e8924c97.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee13c13531eb57544bc391d2986841b8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5797e9b6bff5f87db57e40e7fd37c868.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5758c207f83189b9417bd59194aa3ec8.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="COP-GEN-Beta-Unified-Generative-Modelling-of-COPernicus-Imagery-Thumbnails"><a href="#COP-GEN-Beta-Unified-Generative-Modelling-of-COPernicus-Imagery-Thumbnails" class="headerlink" title="COP-GEN-Beta: Unified Generative Modelling of COPernicus Imagery   Thumbnails"></a>COP-GEN-Beta: Unified Generative Modelling of COPernicus Imagery   Thumbnails</h2><p><strong>Authors:Miguel Espinosa, Valerio Marsocci, Yuru Jia, Elliot J. Crowley, Mikolaj Czerkawski</strong></p>
<p>In remote sensing, multi-modal data from various sensors capturing the same scene offers rich opportunities, but learning a unified representation across these modalities remains a significant challenge. Traditional methods have often been limited to single or dual-modality approaches. In this paper, we introduce COP-GEN-Beta, a generative diffusion model trained on optical, radar, and elevation data from the Major TOM dataset. What sets COP-GEN-Beta apart is its ability to map any subset of modalities to any other, enabling zero-shot modality translation after training. This is achieved through a sequence-based diffusion transformer, where each modality is controlled by its own timestep embedding. We extensively evaluate COP-GEN-Beta on thumbnail images from the Major TOM dataset, demonstrating its effectiveness in generating high-quality samples. Qualitative and quantitative evaluations validate the modelâ€™s performance, highlighting its potential as a powerful pre-trained model for future remote sensing tasks. </p>
<blockquote>
<p>åœ¨é¥æ„Ÿé¢†åŸŸï¼Œä»ä¸åŒä¼ æ„Ÿå™¨æ•æ‰åŒä¸€åœºæ™¯çš„å¤šæ¨¡æ€æ•°æ®æä¾›äº†ä¸°å¯Œçš„æœºä¼šï¼Œä½†åœ¨è¿™äº›æ¨¡æ€ä¸Šå­¦ä¹ ç»Ÿä¸€è¡¨å¾ä»æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚ä¼ ç»Ÿæ–¹æ³•é€šå¸¸å±€é™äºå•ä¸€æˆ–åŒæ¨¡æ€æ–¹æ³•ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†COP-GEN-Betaï¼Œè¿™æ˜¯ä¸€ä¸ªåœ¨Major TOMæ•°æ®é›†çš„å…‰å­¦ã€é›·è¾¾å’Œæµ·æ‹”æ•°æ®ä¸Šè®­ç»ƒçš„ç”Ÿæˆå‹æ‰©æ•£æ¨¡å‹ã€‚COP-GEN-Betaçš„ç‰¹æ®Šä¹‹å¤„åœ¨äºï¼Œå®ƒèƒ½å¤Ÿå°†ä»»ä½•æ¨¡æ€å­é›†æ˜ å°„åˆ°å…¶ä»–æ¨¡æ€ï¼Œå®ç°è®­ç»ƒåçš„é›¶æ ·æœ¬æ¨¡æ€è½¬æ¢ã€‚è¿™æ˜¯é€šè¿‡åŸºäºåºåˆ—çš„æ‰©æ•£å˜å‹å™¨å®ç°çš„ï¼Œå…¶ä¸­æ¯ä¸ªæ¨¡æ€ç”±è‡ªå·±çš„æ—¶é—´æ­¥é•¿åµŒå…¥æ§åˆ¶ã€‚æˆ‘ä»¬åœ¨Major TOMæ•°æ®é›†çš„ç¼©ç•¥å›¾å›¾åƒä¸Šå¯¹COP-GEN-Betaè¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ï¼Œè¯æ˜äº†å®ƒåœ¨ç”Ÿæˆé«˜è´¨é‡æ ·æœ¬æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚å®šæ€§å’Œå®šé‡è¯„ä¼°éªŒè¯äº†è¯¥æ¨¡å‹çš„æ€§èƒ½ï¼Œçªæ˜¾äº†å…¶åœ¨æœªæ¥é¥æ„Ÿä»»åŠ¡ä¸­ä½œä¸ºå¼ºå¤§é¢„è®­ç»ƒæ¨¡å‹çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08548v2">PDF</a> Accepted at CVPR 2025 Workshop MORSE</p>
<p><strong>Summary</strong></p>
<p>å¤šæ¨¡æ€æ•°æ®åœ¨é¥æ„Ÿé¢†åŸŸæä¾›äº†ä¸°å¯Œçš„æœºä¼šï¼Œä½†å­¦ä¹ è·¨æ¨¡æ€çš„ç»Ÿä¸€è¡¨ç¤ºä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºCOP-GEN-Betaçš„ç”Ÿæˆå‹æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨Major TOMæ•°æ®é›†çš„å…‰å­¦ã€é›·è¾¾å’Œåœ°å½¢æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒã€‚COP-GEN-Betaçš„çªå‡ºç‰¹ç‚¹æ˜¯èƒ½å¤Ÿå®ç°ä»»æ„æ¨¡æ€å­é›†ä¹‹é—´çš„æ˜ å°„ï¼Œè®­ç»ƒåèƒ½å¤Ÿå®ç°é›¶æ ·æœ¬æ¨¡æ€è½¬æ¢ã€‚è¿™é€šè¿‡åŸºäºåºåˆ—çš„æ‰©æ•£å˜æ¢å™¨å®ç°ï¼Œæ¯ä¸ªæ¨¡æ€ç”±è‡ªå·±çš„æ—¶é—´æ­¥é•¿åµŒå…¥æ§åˆ¶ã€‚åœ¨Major TOMæ•°æ®é›†ç¼©ç•¥å›¾ä¸Šçš„å¹¿æ³›è¯„ä¼°è¯æ˜äº†COP-GEN-Betaç”Ÿæˆé«˜è´¨é‡æ ·æœ¬çš„æœ‰æ•ˆæ€§ã€‚è¯¥æ¨¡å‹è¡¨ç°å‡ºè‰²ï¼Œæœ‰æœ›æˆä¸ºæœªæ¥é¥æ„Ÿä»»åŠ¡çš„å¼ºå¤§é¢„è®­ç»ƒæ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€æ•°æ®åœ¨é¥æ„Ÿä¸­å…·æœ‰ä¸°å¯Œæœºä¼šï¼Œä½†è·¨æ¨¡æ€ç»Ÿä¸€è¡¨ç¤ºå­¦ä¹ å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>COP-GEN-Betaæ˜¯ä¸€ç§ç”Ÿæˆå‹æ‰©æ•£æ¨¡å‹ï¼Œèƒ½åœ¨å¤šç§æ¨¡æ€æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒã€‚</li>
<li>COP-GEN-Betaèƒ½å¤Ÿå®ç°ä»»æ„æ¨¡æ€å­é›†ä¹‹é—´çš„æ˜ å°„ï¼Œå®ç°é›¶æ ·æœ¬æ¨¡æ€è½¬æ¢ã€‚</li>
<li>é€šè¿‡åºåˆ—æ‰©æ•£å˜æ¢å™¨å®ç°æ¨¡å‹ï¼Œæ¯ä¸ªæ¨¡æ€å…·æœ‰è‡ªå·±çš„æ—¶é—´æ­¥é•¿åµŒå…¥æ§åˆ¶ã€‚</li>
<li>åœ¨Major TOMæ•°æ®é›†ç¼©ç•¥å›¾ä¸Šçš„è¯„ä¼°è¯æ˜äº†COP-GEN-Betaç”Ÿæˆé«˜è´¨é‡æ ·æœ¬çš„èƒ½åŠ›ã€‚</li>
<li>æ¨¡å‹çš„æ€§èƒ½é€šè¿‡å®šæ€§å’Œå®šé‡è¯„ä¼°å¾—åˆ°éªŒè¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08548">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4c05634df87f5c36cac77c259973a745.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef5906db11a65bfe599ee2788ea14a33.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c7675003f5763fde5e7dd7089fd5379.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="CyclePose-â€“-Leveraging-Cycle-Consistency-for-Annotation-Free-Nuclei-Segmentation-in-Fluorescence-Microscopy"><a href="#CyclePose-â€“-Leveraging-Cycle-Consistency-for-Annotation-Free-Nuclei-Segmentation-in-Fluorescence-Microscopy" class="headerlink" title="CyclePose â€“ Leveraging Cycle-Consistency for Annotation-Free Nuclei   Segmentation in Fluorescence Microscopy"></a>CyclePose â€“ Leveraging Cycle-Consistency for Annotation-Free Nuclei   Segmentation in Fluorescence Microscopy</h2><p><strong>Authors:Jonas Utz, Stefan Vocht, Anne Tjorven Buessen, Dennis Possart, Fabian Wagner, Mareike Thies, Mingxuan Gu, Stefan Uderhardt, Katharina Breininger</strong></p>
<p>In recent years, numerous neural network architectures specifically designed for the instance segmentation of nuclei in microscopic images have been released. These models embed nuclei-specific priors to outperform generic architectures like U-Nets; however, they require large annotated datasets, which are often not available. Generative models (GANs, diffusion models) have been used to compensate for this by synthesizing training data. These two-stage approaches are computationally expensive, as first a generative model and then a segmentation model has to be trained. We propose CyclePose, a hybrid framework integrating synthetic data generation and segmentation training. CyclePose builds on a CycleGAN architecture, which allows unpaired translation between microscopy images and segmentation masks. We embed a segmentation model into CycleGAN and leverage a cycle consistency loss for self-supervision. Without annotated data, CyclePose outperforms other weakly or unsupervised methods on two public datasets. Code is available at <a target="_blank" rel="noopener" href="https://github.com/jonasutz/CyclePose">https://github.com/jonasutz/CyclePose</a> </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œä¸“ä¸ºæ˜¾å¾®å›¾åƒä¸­çš„ç»†èƒæ ¸å®ä¾‹åˆ†å‰²è®¾è®¡çš„ç¥ç»ç½‘ç»œæ¶æ„å·²ç»é™†ç»­å‘å¸ƒã€‚è¿™äº›æ¨¡å‹ä¼šåµŒå…¥ç‰¹å®šçš„ç»†èƒæ ¸å…ˆéªŒçŸ¥è¯†ï¼Œä»¥è¶…è¶ŠU-Netç­‰é€šç”¨æ¶æ„çš„æ€§èƒ½ï¼›ç„¶è€Œï¼Œå®ƒä»¬éœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®é›†ï¼Œè€Œè¿™äº›æ•°æ®é€šå¸¸å¹¶ä¸å¯ç”¨ã€‚ç”Ÿæˆæ¨¡å‹ï¼ˆGANsã€æ‰©æ•£æ¨¡å‹ï¼‰å·²è¢«ç”¨æ¥é€šè¿‡åˆæˆè®­ç»ƒæ•°æ®æ¥å¼¥è¡¥è¿™ä¸€ç¼ºé™·ã€‚è¿™äº›ä¸¤é˜¶æ®µçš„æ–¹æ³•è®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œå› ä¸ºé¦–å…ˆéœ€è¦è®­ç»ƒä¸€ä¸ªç”Ÿæˆæ¨¡å‹ï¼Œç„¶åè®­ç»ƒä¸€ä¸ªåˆ†å‰²æ¨¡å‹ã€‚æˆ‘ä»¬æå‡ºäº†CyclePoseï¼Œè¿™æ˜¯ä¸€ä¸ªé›†æˆåˆæˆæ•°æ®ç”Ÿæˆå’Œåˆ†å‰²è®­ç»ƒäºä¸€ä½“çš„æ··åˆæ¡†æ¶ã€‚CyclePoseåŸºäºCycleGANæ¶æ„ï¼Œå…è®¸æ˜¾å¾®å›¾åƒå’Œåˆ†å‰²æ©è†œä¹‹é—´çš„æ— é…å¯¹è½¬æ¢ã€‚æˆ‘ä»¬å°†åˆ†å‰²æ¨¡å‹åµŒå…¥åˆ°CycleGANä¸­ï¼Œå¹¶åˆ©ç”¨å¾ªç¯ä¸€è‡´æ€§æŸå¤±è¿›è¡Œè‡ªç›‘ç£ã€‚æ— éœ€æ ‡æ³¨æ•°æ®ï¼ŒCyclePoseåœ¨ä¸¤ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–å¼±ç›‘ç£æˆ–æ— ç›‘ç£æ–¹æ³•ã€‚ç›¸å…³ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/jonasutz/CyclePose%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/jonasutz/CyclePoseè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.11266v2">PDF</a> under review for MICCAI 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>è¿‘æœŸå‘å¸ƒå¤šç§ä¸“ä¸ºå¾®è§‚å›¾åƒæ ¸å®ä¾‹åˆ†å‰²è®¾è®¡çš„ç¥ç»ç½‘ç»œæ¶æ„ï¼Œè¿™äº›æ¨¡å‹é€šè¿‡åµŒå…¥æ ¸ç‰¹å¼‚æ€§å…ˆéªŒçŸ¥è¯†ï¼Œæ€§èƒ½ä¼˜äºé€šç”¨æ¶æ„ï¼ˆå¦‚U-Netï¼‰ï¼Œä½†å®ƒä»¬éœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®é›†ï¼Œè€Œè¿™é€šå¸¸éš¾ä»¥è·å–ã€‚ç”Ÿæˆæ¨¡å‹ï¼ˆGANsï¼Œæ‰©æ•£æ¨¡å‹ï¼‰å·²ç”¨äºåˆæˆè®­ç»ƒæ•°æ®ä»¥å¼¥è¡¥è¿™ä¸€ç¼ºé™·ã€‚è¿™äº›ä¸¤é˜¶æ®µæ–¹æ³•è®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œéœ€å…ˆè®­ç»ƒç”Ÿæˆæ¨¡å‹ï¼Œå†è®­ç»ƒåˆ†å‰²æ¨¡å‹ã€‚æˆ‘ä»¬æå‡ºCyclePoseï¼Œä¸€ä¸ªé›†æˆåˆæˆæ•°æ®ç”Ÿæˆå’Œåˆ†å‰²è®­ç»ƒçš„æ··åˆæ¡†æ¶ã€‚CyclePoseåŸºäºCycleGANæ¶æ„ï¼Œå®ç°æ˜¾å¾®é•œå›¾åƒå’Œåˆ†å‰²æ©è†œä¹‹é—´çš„æ— é…å¯¹è½¬æ¢ã€‚æˆ‘ä»¬åœ¨CycleGANä¸­åµŒå…¥åˆ†å‰²æ¨¡å‹ï¼Œå¹¶åˆ©ç”¨å¾ªç¯ä¸€è‡´æ€§æŸå¤±å®ç°è‡ªç›‘ç£ã€‚æ— éœ€æ ‡æ³¨æ•°æ®ï¼ŒCyclePoseåœ¨å…¬å¼€æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–å¼±ç›‘ç£æˆ–æ— ç›‘ç£æ–¹æ³•ã€‚ç›¸å…³ä»£ç å·²å‘å¸ƒè‡³<a target="_blank" rel="noopener" href="https://github.com/jonasutz/CyclePose%E3%80%82">https://github.com/jonasutz/CyclePoseã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è¿‘å¹´å‡ºç°å¤šç§ä¸“ä¸ºå¾®è§‚å›¾åƒæ ¸å®ä¾‹åˆ†å‰²è®¾è®¡çš„ç¥ç»ç½‘ç»œæ¶æ„ï¼Œä½†éœ€æ±‚å¤§é‡æ ‡æ³¨æ•°æ®ã€‚</li>
<li>ç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚GANså’Œæ‰©æ•£æ¨¡å‹ï¼‰å·²è¢«ç”¨äºåˆæˆè®­ç»ƒæ•°æ®ä»¥å…‹æœæ•°æ®ç¼ºä¹çš„é—®é¢˜ã€‚</li>
<li>å½“å‰æ–¹æ³•å¤šä¸ºä¸¤é˜¶æ®µï¼Œè®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œéœ€åˆ†åˆ«è®­ç»ƒç”Ÿæˆæ¨¡å‹å’Œåˆ†å‰²æ¨¡å‹ã€‚</li>
<li>æå‡ºCyclePoseæ··åˆæ¡†æ¶ï¼Œæ•´åˆåˆæˆæ•°æ®ç”Ÿæˆå’Œåˆ†å‰²è®­ç»ƒã€‚</li>
<li>CyclePoseåŸºäºCycleGANæ¶æ„ï¼Œå®ç°å›¾åƒå’Œåˆ†å‰²æ©è†œä¹‹é—´çš„æ— é…å¯¹è½¬æ¢ã€‚</li>
<li>CyclePoseåœ¨æ— éœ€æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹ï¼Œå…¬å¼€æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–å¼±ç›‘ç£æˆ–æ— ç›‘ç£æ–¹æ³•ã€‚</li>
<li>CyclePoseçš„ç›¸å…³ä»£ç å·²å‘å¸ƒè‡³GitHubä¾›å…¬ä¼—è®¿é—®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.11266">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-92d24cad787164b85359f5cee761086d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-91dc9dfebf13ac8bec1e5481171d74f7.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="RORem-Training-a-Robust-Object-Remover-with-Human-in-the-Loop"><a href="#RORem-Training-a-Robust-Object-Remover-with-Human-in-the-Loop" class="headerlink" title="RORem: Training a Robust Object Remover with Human-in-the-Loop"></a>RORem: Training a Robust Object Remover with Human-in-the-Loop</h2><p><strong>Authors:Ruibin Li, Tao Yang, Song Guo, Lei Zhang</strong></p>
<p>Despite the significant advancements, existing object removal methods struggle with incomplete removal, incorrect content synthesis and blurry synthesized regions, resulting in low success rates. Such issues are mainly caused by the lack of high-quality paired training data, as well as the self-supervised training paradigm adopted in these methods, which forces the model to in-paint the masked regions, leading to ambiguity between synthesizing the masked objects and restoring the background. To address these issues, we propose a semi-supervised learning strategy with human-in-the-loop to create high-quality paired training data, aiming to train a Robust Object Remover (RORem). We first collect 60K training pairs from open-source datasets to train an initial object removal model for generating removal samples, and then utilize human feedback to select a set of high-quality object removal pairs, with which we train a discriminator to automate the following training data generation process. By iterating this process for several rounds, we finally obtain a substantial object removal dataset with over 200K pairs. Fine-tuning the pre-trained stable diffusion model with this dataset, we obtain our RORem, which demonstrates state-of-the-art object removal performance in terms of both reliability and image quality. Particularly, RORem improves the object removal success rate over previous methods by more than 18%. The dataset, source code and trained model are available at <a target="_blank" rel="noopener" href="https://github.com/leeruibin/RORem">https://github.com/leeruibin/RORem</a>. </p>
<blockquote>
<p>å°½ç®¡å–å¾—äº†é‡å¤§è¿›å±•ï¼Œç°æœ‰çš„ç‰©ä½“ç§»é™¤æ–¹æ³•ä»å­˜åœ¨å›°æ‰°ï¼Œä¾‹å¦‚å»é™¤ä¸å®Œå…¨ã€å†…å®¹åˆæˆä¸æ­£ç¡®ä»¥åŠåˆæˆåŒºåŸŸæ¨¡ç³Šï¼Œå¯¼è‡´æˆåŠŸç‡è¾ƒä½ã€‚è¿™äº›é—®é¢˜ä¸»è¦æ˜¯ç”±äºç¼ºä¹é«˜è´¨é‡é…å¯¹è®­ç»ƒæ•°æ®ä»¥åŠè¿™äº›æ–¹æ³•æ‰€é‡‡ç”¨çš„è‡ªç›‘ç£è®­ç»ƒèŒƒå¼å¯¼è‡´çš„ã€‚è‡ªç›‘ç£è®­ç»ƒèŒƒå¼è¿«ä½¿æ¨¡å‹å¯¹é®ç½©åŒºåŸŸè¿›è¡Œå¡«å……ï¼Œå¯¼è‡´åˆæˆé®ç½©ç‰©ä½“å’Œæ¢å¤èƒŒæ™¯ä¹‹é—´äº§ç”Ÿæ¨¡ç³Šã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŠç›‘ç£å­¦ä¹ ç­–ç•¥ï¼Œå¹¶å¼•å…¥äººç±»å‚ä¸æ¥åˆ›å»ºé«˜è´¨é‡é…å¯¹è®­ç»ƒæ•°æ®ï¼Œæ—¨åœ¨è®­ç»ƒä¸€ä¸ªç¨³å¥ç‰©ä½“ç§»é™¤å™¨ï¼ˆRORemï¼‰ã€‚æˆ‘ä»¬é¦–å…ˆä»å…¬å¼€æ•°æ®æºæ”¶é›†6ä¸‡ç»„è®­ç»ƒé…å¯¹æ•°æ®ï¼Œç”¨äºè®­ç»ƒåˆå§‹ç‰©ä½“ç§»é™¤æ¨¡å‹ä»¥ç”Ÿæˆç§»é™¤æ ·æœ¬ï¼Œç„¶ååˆ©ç”¨äººç±»åé¦ˆé€‰æ‹©ä¸€ç»„é«˜è´¨é‡çš„ç‰©ä½“ç§»é™¤é…å¯¹æ•°æ®ï¼Œå¹¶ä½¿ç”¨è¿™äº›æ•°æ®è®­ç»ƒä¸€ä¸ªé‰´åˆ«å™¨ï¼Œä»¥è‡ªåŠ¨åŒ–åç»­çš„è®­ç»ƒæ•°æ®ç”Ÿæˆè¿‡ç¨‹ã€‚ç»è¿‡å‡ è½®è¿­ä»£åï¼Œæˆ‘ä»¬æœ€ç»ˆè·å¾—äº†ä¸€ä¸ªåŒ…å«è¶…è¿‡20ä¸‡ç»„é…å¯¹æ•°æ®çš„ç‰©ä½“ç§»é™¤æ•°æ®é›†ã€‚ä½¿ç”¨æ­¤æ•°æ®é›†å¯¹é¢„è®­ç»ƒçš„ç¨³å®šæ‰©æ•£æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œæˆ‘ä»¬å¾—åˆ°äº†RORemï¼Œå®ƒåœ¨å¯é æ€§å’Œå›¾åƒè´¨é‡æ–¹é¢å®ç°äº†æœ€å…ˆè¿›çš„ç‰©ä½“ç§»é™¤æ€§èƒ½ã€‚ç‰¹åˆ«åœ°ï¼ŒRORemå°†ç‰©ä½“ç§»é™¤æˆåŠŸç‡æé«˜äº†è¶…è¿‡18%ã€‚æ•°æ®é›†ã€æºä»£ç å’Œè®­ç»ƒå¥½çš„æ¨¡å‹å‡å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/leeruibin/RORem">https://github.com/leeruibin/RORem</a>ä¸Šæ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.00740v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŠç›‘ç£å­¦ä¹ ç­–ç•¥ï¼Œç»“åˆäººå·¥å‚ä¸ï¼Œåˆ›å»ºé«˜è´¨é‡é…å¯¹è®­ç»ƒæ•°æ®ï¼Œæ—¨åœ¨è®­ç»ƒä¸€ä¸ªç¨³å¥çš„å¯¹è±¡ç§»é™¤å™¨ï¼ˆRORemï¼‰ã€‚é€šè¿‡è¿­ä»£æ”¶é›†è®­ç»ƒæ ·æœ¬å’Œäººç±»åé¦ˆé€‰æ‹©é«˜è´¨é‡çš„æ•°æ®å¯¹ï¼Œè®­ç»ƒåˆ¤åˆ«å™¨è‡ªåŠ¨åŒ–åç»­çš„è®­ç»ƒæ•°æ®ç”Ÿæˆè¿‡ç¨‹ã€‚ä½¿ç”¨æ­¤æ•°æ®é›†å¾®è°ƒé¢„è®­ç»ƒçš„ç¨³å®šæ‰©æ•£æ¨¡å‹ï¼Œè·å¾—å…·æœ‰å…ˆè¿›æ€§èƒ½çš„RORemï¼Œåœ¨å¯é æ€§å’Œå›¾åƒè´¨é‡æ–¹é¢å‡è¡¨ç°ä¼˜å¼‚ï¼Œå¯¹è±¡ç§»é™¤æˆåŠŸç‡è¾ƒä¹‹å‰çš„æ–¹æ³•æé«˜äº†18%ä»¥ä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰å¯¹è±¡ç§»é™¤æ–¹æ³•å­˜åœ¨ä¸å®Œæ•´ç§»é™¤ã€å†…å®¹åˆæˆä¸æ­£ç¡®å’ŒåˆæˆåŒºåŸŸæ¨¡ç³Šç­‰é—®é¢˜ï¼Œå¯¼è‡´æˆåŠŸç‡ä½ã€‚</li>
<li>é—®é¢˜ä¸»è¦æºäºç¼ºä¹é«˜è´¨é‡é…å¯¹è®­ç»ƒæ•°æ®å’Œè‡ªç›‘ç£è®­ç»ƒèŒƒå¼ã€‚</li>
<li>æå‡ºä¸€ç§åŠç›‘ç£å­¦ä¹ ç­–ç•¥ï¼Œç»“åˆäººå·¥å‚ä¸åˆ›å»ºé«˜è´¨é‡é…å¯¹è®­ç»ƒæ•°æ®ï¼Œä»¥è®­ç»ƒç¨³å¥çš„å¯¹è±¡ç§»é™¤å™¨ï¼ˆRORemï¼‰ã€‚</li>
<li>é€šè¿‡è¿­ä»£æ”¶é›†è®­ç»ƒæ ·æœ¬å’Œäººç±»åé¦ˆé€‰æ‹©æ•°æ®å¯¹ï¼Œè®­ç»ƒåˆ¤åˆ«å™¨è‡ªåŠ¨åŒ–åç»­æ•°æ®ç”Ÿæˆã€‚</li>
<li>RORemåœ¨å¯é æ€§å’Œå›¾åƒè´¨é‡æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œå¯¹è±¡ç§»é™¤æˆåŠŸç‡è¾ƒä¹‹å‰çš„æ–¹æ³•æé«˜18%ä»¥ä¸Šã€‚</li>
<li>RORemæ¨¡å‹ã€æ•°æ®é›†å’Œæºä»£ç å‡å¯åœ¨æŒ‡å®šGitHubä»“åº“ä¸­æ‰¾åˆ°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.00740">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-bd5ab457d363dd6072bf068057b4d076.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-60878675ad0033dc47bb8471462cf934.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f722f4161e3bb58b6c1261c619403de0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c48cd5acb064bff8293616686cf7ff8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-62e631ad6caf2fb30e89a432e95cce18.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-17980883c30b9aa63f07feb36883739d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca4c0ca7d933c0ed4bb655144410357e.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="V-LASIK-Consistent-Glasses-Removal-from-Videos-Using-Synthetic-Data"><a href="#V-LASIK-Consistent-Glasses-Removal-from-Videos-Using-Synthetic-Data" class="headerlink" title="V-LASIK: Consistent Glasses-Removal from Videos Using Synthetic Data"></a>V-LASIK: Consistent Glasses-Removal from Videos Using Synthetic Data</h2><p><strong>Authors:Rotem Shalev-Arkushin, Aharon Azulay, Tavi Halperin, Eitan Richardson, Amit H. Bermano, Ohad Fried</strong></p>
<p>Diffusion-based generative models have recently shown remarkable image and video editing capabilities. However, local video editing, particularly removal of small attributes like glasses, remains a challenge. Existing methods either alter the videos excessively, generate unrealistic artifacts, or fail to perform the requested edit consistently throughout the video. In this work, we focus on consistent and identity-preserving removal of glasses in videos, using it as a case study for consistent local attribute removal in videos. Due to the lack of paired data, we adopt a weakly supervised approach and generate synthetic imperfect data, using an adjusted pretrained diffusion model. We show that despite data imperfection, by learning from our generated data and leveraging the prior of pretrained diffusion models, our model is able to perform the desired edit consistently while preserving the original video content. Furthermore, we exemplify the generalization ability of our method to other local video editing tasks by applying it successfully to facial sticker-removal. Our approach demonstrates significant improvement over existing methods, showcasing the potential of leveraging synthetic data and strong video priors for local video editing tasks. </p>
<blockquote>
<p>åŸºäºæ‰©æ•£çš„ç”Ÿæˆæ¨¡å‹æœ€è¿‘åœ¨å›¾åƒå’Œè§†é¢‘ç¼–è¾‘åŠŸèƒ½æ–¹é¢è¡¨ç°å‡ºäº†æ˜¾è‘—çš„å®åŠ›ã€‚ç„¶è€Œï¼Œå±€éƒ¨è§†é¢‘ç¼–è¾‘ï¼Œç‰¹åˆ«æ˜¯å»é™¤åƒçœ¼é•œè¿™æ ·çš„å°å±æ€§ï¼Œä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•è¦ä¹ˆè¿‡åº¦ä¿®æ”¹è§†é¢‘ï¼Œäº§ç”Ÿä¸çœŸå®çš„ä¼ªå½±ï¼Œè¦ä¹ˆæ— æ³•åœ¨è§†é¢‘ä¸­æŒç»­æ‰§è¡Œæ‰€éœ€çš„ç¼–è¾‘ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä¸“æ³¨äºåœ¨è§†é¢‘ä¸­ä¸€è‡´ä¸”ä¿æŒèº«ä»½åœ°å»é™¤çœ¼é•œï¼Œå°†å…¶ä½œä¸ºè§†é¢‘ä¸­ä¸€è‡´å±€éƒ¨å±æ€§å»é™¤çš„ä¸ªæ¡ˆç ”ç©¶ã€‚ç”±äºç¼ºä¹é…å¯¹æ•°æ®ï¼Œæˆ‘ä»¬é‡‡ç”¨å¼±ç›‘ç£æ–¹æ³•å¹¶ç”Ÿæˆåˆæˆçš„ä¸å®Œç¾æ•°æ®ï¼Œä½¿ç”¨è°ƒæ•´è¿‡çš„é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œå°½ç®¡æ•°æ®å­˜åœ¨ä¸å®Œç¾ä¹‹å¤„ï¼Œä½†é€šè¿‡ä»ç”Ÿæˆçš„æ•°æ®ä¸­å­¦ä¹ å¹¶åˆ©ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†ï¼Œæˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿä¸€è‡´åœ°æ‰§è¡Œæ‰€éœ€çš„ç¼–è¾‘æ“ä½œåŒæ—¶ä¿ç•™åŸå§‹è§†é¢‘å†…å®¹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡æˆåŠŸå°†å…¶åº”ç”¨äºé¢éƒ¨è´´çº¸å»é™¤ï¼Œå±•ç¤ºäº†è¯¥æ–¹æ³•åœ¨å…¶ä»–å±€éƒ¨è§†é¢‘ç¼–è¾‘ä»»åŠ¡ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç›¸è¾ƒäºç°æœ‰æ–¹æ³•æœ‰æ˜æ˜¾çš„æ”¹è¿›ï¼Œå±•ç¤ºäº†åˆ©ç”¨åˆæˆæ•°æ®å’Œå¼ºå¤§çš„è§†é¢‘å…ˆéªŒä¿¡æ¯åœ¨å±€éƒ¨è§†é¢‘ç¼–è¾‘ä»»åŠ¡ä¸­çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.14510v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºæ‰©æ•£æ¨¡å‹çš„è§†é¢‘ç¼–è¾‘æŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å±€éƒ¨è§†é¢‘ç¼–è¾‘æ–¹é¢çš„æŒ‘æˆ˜ï¼Œå¦‚çœ¼é•œå»é™¤ç­‰ã€‚ç ”ç©¶å›¢é˜Ÿé’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œé‡‡ç”¨å¼±ç›‘ç£æ–¹æ³•ï¼Œåˆ©ç”¨è°ƒæ•´åçš„é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ç”Ÿæˆåˆæˆä¸å®Œç¾æ•°æ®ã€‚å³ä½¿æ•°æ®å­˜åœ¨ç¼ºé™·ï¼Œæ¨¡å‹ä¹Ÿèƒ½ä»ç”Ÿæˆçš„æ•°æ®ä¸­å­¦ä¹ ï¼Œå¹¶åˆ©ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†ï¼Œå®ç°å¯¹è§†é¢‘çš„è¿ç»­çœ¼é•œå»é™¤ï¼ŒåŒæ—¶ä¿ç•™è§†é¢‘å†…å®¹çš„åŸå§‹æ€§ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å±•ç¤ºäº†è¯¥æ–¹æ³•åœ¨å…¶ä»–å±€éƒ¨è§†é¢‘ç¼–è¾‘ä»»åŠ¡ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒå’Œè§†é¢‘ç¼–è¾‘ä¸­æ˜¾ç¤ºå‡ºå¼ºå¤§çš„èƒ½åŠ›ã€‚</li>
<li>å±€éƒ¨è§†é¢‘ç¼–è¾‘ï¼ˆå¦‚çœ¼é•œå»é™¤ï¼‰ä»ç„¶å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¾€å¾€ä¼šå¯¼è‡´è§†é¢‘å†…å®¹è¿‡åº¦ä¿®æ”¹ã€ç”Ÿæˆä¸çœŸå®ä¼ªå½±æˆ–æ— æ³•æŒç»­æ‰§è¡Œç¼–è¾‘è¯·æ±‚ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§å¼±ç›‘ç£æ–¹æ³•ï¼Œåˆ©ç”¨è°ƒæ•´åçš„é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ç”Ÿæˆåˆæˆä¸å®Œç¾æ•°æ®æ¥è§£å†³çœ¼é•œå»é™¤é—®é¢˜ã€‚</li>
<li>å³ä½¿æ•°æ®å­˜åœ¨ç¼ºé™·ï¼Œæ¨¡å‹ä¹Ÿèƒ½å­¦ä¹ å¹¶æˆåŠŸæ‰§è¡Œçœ¼é•œå»é™¤æ“ä½œï¼ŒåŒæ—¶ä¿æŒè§†é¢‘å†…å®¹çš„è¿è´¯æ€§å’ŒçœŸå®æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•å…·æœ‰æ³›åŒ–åˆ°å…¶ä»–å±€éƒ¨è§†é¢‘ç¼–è¾‘ä»»åŠ¡çš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.14510">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fcffb4ad15b031ef4863dd44c77f1cc7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0627b5d4d5e6240674ef2491f25f1b50.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f2f673abe429906f4c431a3371320779.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef108bafcc15a0178cb8ed60ce6801dc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9276f52b454052b1e5c3b422175fd9f1.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Financial-Models-in-Generative-Art-Black-Scholes-Inspired-Concept-Blending-in-Text-to-Image-Diffusion"><a href="#Financial-Models-in-Generative-Art-Black-Scholes-Inspired-Concept-Blending-in-Text-to-Image-Diffusion" class="headerlink" title="Financial Models in Generative Art: Black-Scholes-Inspired Concept   Blending in Text-to-Image Diffusion"></a>Financial Models in Generative Art: Black-Scholes-Inspired Concept   Blending in Text-to-Image Diffusion</h2><p><strong>Authors:Divya Kothandaraman, Ming Lin, Dinesh Manocha</strong></p>
<p>We introduce a novel approach for concept blending in pretrained text-to-image diffusion models, aiming to generate images at the intersection of multiple text prompts. At each time step during diffusion denoising, our algorithm forecasts predictions w.r.t. the generated image and makes informed text conditioning decisions. Central to our method is the unique analogy between diffusion models, which are rooted in non-equilibrium thermodynamics, and the Black-Scholes model for financial option pricing. By drawing parallels between key variables in both domains, we derive a robust algorithm for concept blending that capitalizes on the Markovian dynamics of the Black-Scholes framework. Our text-based concept blending algorithm is data-efficient, meaning it does not need additional training. Furthermore, it operates without human intervention or hyperparameter tuning. We highlight the benefits of our approach by comparing it qualitatively and quantitatively to other text based concept blending techniques, including linear interpolation, alternating prompts, step-wise prompt switching, and CLIP-guided prompt selection across various scenarios such as single object per text prompt, multiple objects per text prompt and objects against backgrounds. Our work shows that financially inspired techniques can enhance text-to-image concept blending in generative AI, paving the way for broader innovation. Code is available at <a target="_blank" rel="noopener" href="https://github.com/divyakraman/BlackScholesDiffusion2024">https://github.com/divyakraman/BlackScholesDiffusion2024</a>. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†ä¸€ç§åœ¨é¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­è¿›è¡Œæ¦‚å¿µèåˆçš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨ç”Ÿæˆå¤šä¸ªæ–‡æœ¬æç¤ºçš„äº¤å‰ç‚¹çš„å›¾åƒã€‚åœ¨æ‰©æ•£å»å™ªçš„æ¯ä¸ªæ—¶é—´æ­¥é•¿ä¸­ï¼Œæˆ‘ä»¬çš„ç®—æ³•é¢„æµ‹ç”Ÿæˆçš„å›¾åƒå¹¶åšå‡ºæ˜æ™ºçš„æ–‡æœ¬æ¡ä»¶å†³ç­–ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„æ ¸å¿ƒåœ¨äºæ‰©æ•£æ¨¡å‹ä¸éå¹³è¡¡çƒ­åŠ›å­¦çš„ç‹¬ç‰¹ç±»æ¯”ï¼Œä»¥åŠä¸é‡‘èæœŸæƒå®šä»·çš„Black-Scholesæ¨¡å‹ä¹‹é—´çš„å…³è”ã€‚é€šè¿‡æ¯”è¾ƒä¸¤ä¸ªé¢†åŸŸä¸­çš„å…³é”®å˜é‡ï¼Œæˆ‘ä»¬æ¨å¯¼å‡ºäº†ä¸€ä¸ªç¨³å¥çš„æ¦‚å¿µèåˆç®—æ³•ï¼Œè¯¥ç®—æ³•å……åˆ†åˆ©ç”¨äº†Black-Scholesæ¡†æ¶çš„é©¬å°”å¯å¤«åŠ¨åŠ›å­¦ã€‚æˆ‘ä»¬çš„åŸºäºæ–‡æœ¬çš„æ¦‚å¿µèåˆç®—æ³•å…·æœ‰æ•°æ®æ•ˆç‡ï¼Œæ„å‘³ç€å®ƒä¸éœ€è¦é¢å¤–çš„è®­ç»ƒã€‚æ­¤å¤–ï¼Œå®ƒå¯ä»¥åœ¨æ²¡æœ‰äººå·¥å¹²é¢„æˆ–è¶…å‚æ•°è°ƒæ•´çš„æƒ…å†µä¸‹è¿è¡Œã€‚æˆ‘ä»¬é€šè¿‡ä¸å…¶ä»–åŸºäºæ–‡æœ¬çš„æ¦‚å¿µèåˆæŠ€æœ¯è¿›è¡Œæ¯”è¾ƒï¼Œçªå‡ºäº†æˆ‘ä»¬æ–¹æ³•çš„å¥½å¤„ï¼ŒåŒ…æ‹¬çº¿æ€§æ’å€¼ã€äº¤æ›¿æç¤ºã€é€æ­¥æç¤ºåˆ‡æ¢ä»¥åŠCLIPå¼•å¯¼çš„æç¤ºé€‰æ‹©ç­‰ã€‚æ¯”è¾ƒåœºæ™¯åŒ…æ‹¬æ¯ä¸ªæ–‡æœ¬æç¤ºå•ä¸ªå¯¹è±¡ã€æ¯ä¸ªæ–‡æœ¬æç¤ºå¤šä¸ªå¯¹è±¡ä»¥åŠå¯¹è±¡ä¸èƒŒæ™¯ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œé‡‘èå¯å‘æŠ€æœ¯å¯ä»¥å¢å¼ºç”Ÿæˆäººå·¥æ™ºèƒ½ä¸­çš„æ–‡æœ¬åˆ°å›¾åƒæ¦‚å¿µèåˆï¼Œä¸ºæ›´å¹¿æ³›çš„åˆ›æ–°é“ºå¹³é“è·¯ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/divyakraman/BlackScholesDiffusion2024%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/divyakraman/BlackScholesDiffusion2024æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.13685v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åœ¨é¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­è¿›è¡Œæ¦‚å¿µèåˆçš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨ç”Ÿæˆå¤šä¸ªæ–‡æœ¬æç¤ºçš„äº¤å‰ç‚¹çš„å›¾åƒã€‚åœ¨æ‰©æ•£å»å™ªçš„æ¯ä¸ªæ—¶é—´æ­¥é•¿ä¸­ï¼Œæˆ‘ä»¬çš„ç®—æ³•é¢„æµ‹ç”Ÿæˆçš„å›¾åƒå¹¶åšå‡ºæ˜æ™ºçš„æ–‡æœ¬æ¡ä»¶å†³ç­–ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„æ ¸å¿ƒåœ¨äºæ‰©æ•£æ¨¡å‹ä¸éå¹³è¡¡æ€çƒ­åŠ›å­¦ä¹‹é—´çš„ç‹¬ç‰¹ç±»æ¯”ï¼Œä»¥åŠä¸é‡‘èæœŸæƒå®šä»·çš„Black-Scholesæ¨¡å‹ä¹‹é—´çš„ç±»æ¯”ã€‚é€šè¿‡æ¯”è¾ƒä¸¤ä¸ªé¢†åŸŸçš„å…³é”®å˜é‡ï¼Œæˆ‘ä»¬æ¨å¯¼å‡ºä¸€ç§æ¦‚å¿µèåˆç®—æ³•ï¼Œè¯¥ç®—æ³•åˆ©ç”¨Black-Scholesæ¡†æ¶çš„é©¬å°”å¯å¤«åŠ¨åŠ›å­¦ã€‚æˆ‘ä»¬çš„åŸºäºæ–‡æœ¬çš„æ¦‚èç®—æ³•éå¸¸æ³¨é‡æ•°æ®æ•ˆç‡ï¼Œæ— éœ€é¢å¤–çš„è®­ç»ƒã€‚æ­¤å¤–ï¼Œå®ƒæ— éœ€äººå·¥å¹²é¢„æˆ–è¶…å‚æ•°è°ƒæ•´ã€‚æˆ‘ä»¬é€šè¿‡ä¸å…¶ä»–åŸºäºæ–‡æœ¬çš„æ¦‚èåˆæŠ€æœ¯ï¼ˆåŒ…æ‹¬çº¿æ€§æ’å€¼ã€äº¤æ›¿æç¤ºã€é€æ­¥æç¤ºåˆ‡æ¢å’ŒCLIPå¼•å¯¼æç¤ºé€‰æ‹©ç­‰ï¼‰åœ¨å„ç§åœºæ™¯ä¸‹çš„å®šæ€§å’Œå®šé‡æ¯”è¾ƒï¼Œçªå‡ºäº†æˆ‘ä»¬æ–¹æ³•çš„ä¼˜åŠ¿ã€‚æœ¬æ–‡å±•ç¤ºäº†é‡‘èå¯å‘æŠ€æœ¯å¯ä»¥å¢å¼ºç”Ÿæˆäººå·¥æ™ºèƒ½ä¸­çš„æ–‡æœ¬åˆ°å›¾åƒæ¦‚å¿µèåˆï¼Œä¸ºæ›´å¹¿æ³›çš„åˆ›æ–°é“ºå¹³äº†é“è·¯ã€‚ç›¸å…³ä»£ç å·²å…¬å¼€å‘å¸ƒäº<a target="_blank" rel="noopener" href="https://github.com/divyakraman/BlackScholesDiffusion2024%E3%80%82">https://github.com/divyakraman/BlackScholesDiffusion2024ã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­çš„æ¦‚å¿µèåˆæ–¹æ³•ï¼Œæ—¨åœ¨ç”Ÿæˆå¤šä¸ªæ–‡æœ¬æç¤ºäº¤å‰ç‚¹çš„å›¾åƒã€‚</li>
<li>é€šè¿‡å°†æ‰©æ•£æ¨¡å‹ä¸Black-Scholesé‡‘èæœŸæƒå®šä»·æ¨¡å‹è¿›è¡Œç±»æ¯”ï¼Œæå‡ºäº†ä¸€ç§ç¨³å¥çš„æ¦‚å¿µèåˆç®—æ³•ã€‚</li>
<li>è¯¥ç®—æ³•åœ¨æ•°æ®æ•ˆç‡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œæ— éœ€é¢å¤–çš„è®­ç»ƒï¼Œè€Œä¸”ä¸éœ€è¦äººå·¥å¹²é¢„æˆ–è¶…å‚æ•°è°ƒæ•´ã€‚</li>
<li>åœ¨å„ç§åœºæ™¯ä¸‹ä¸ç°æœ‰çš„æ–‡æœ¬æ¦‚å¿µèåˆæŠ€æœ¯è¿›è¡Œäº†æ¯”è¾ƒï¼Œè¯æ˜äº†å…¶ä¼˜åŠ¿ã€‚</li>
<li>å±•ç¤ºäº†é‡‘èå¯å‘æŠ€æœ¯åœ¨ç”Ÿæˆäººå·¥æ™ºèƒ½ä¸­çš„æ–‡æœ¬åˆ°å›¾åƒæ¦‚å¿µèåˆçš„æ½œåŠ›ã€‚</li>
<li>ä¸ºæ›´å¹¿æ³›çš„åˆ›æ–°å’Œè¿›ä¸€æ­¥çš„ç ”å‘æä¾›äº†æ–°çš„è§†è§’å’Œæ€è·¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.13685">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-a3b5e6e3afd9d743adf8f7a2bf0587fa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-145524306308ccadc0c6a8e73d3dd294.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-17/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-17/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-17/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-19a9242d00a4221999fdc99b6231fe60.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-17  Aligning Generative Denoising with Discriminative Objectives Unleashes   Diffusion for Visual Perception
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-17
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-17/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-de11ea7140d240861a6ee879dd40aea2.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-17  Explicit and Implicit Representations in AI-based 3D Reconstruction for   Radiology A systematic literature review
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-17
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">29301k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
