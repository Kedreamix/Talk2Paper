<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-04-17  TerraMind Large-Scale Generative Multimodality for Earth Observation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-682db554604070c09fa35e0e49152148.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-04-17
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-04-28
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    6.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    28 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-04-17-更新"><a href="#2025-04-17-更新" class="headerlink" title="2025-04-17 更新"></a>2025-04-17 更新</h1><h2 id="TerraMind-Large-Scale-Generative-Multimodality-for-Earth-Observation"><a href="#TerraMind-Large-Scale-Generative-Multimodality-for-Earth-Observation" class="headerlink" title="TerraMind: Large-Scale Generative Multimodality for Earth Observation"></a>TerraMind: Large-Scale Generative Multimodality for Earth Observation</h2><p><strong>Authors:Johannes Jakubik, Felix Yang, Benedikt Blumenstiel, Erik Scheurer, Rocco Sedona, Stefano Maurogiovanni, Jente Bosmans, Nikolaos Dionelis, Valerio Marsocci, Niklas Kopp, Rahul Ramachandran, Paolo Fraccaro, Thomas Brunschwiler, Gabriele Cavallaro, Juan Bernabe-Moreno, Nicolas Longépé</strong></p>
<p>We present TerraMind, the first any-to-any generative, multimodal foundation model for Earth observation (EO). Unlike other multimodal models, TerraMind is pretrained on dual-scale representations combining both token-level and pixel-level data across modalities. On a token level, TerraMind encodes high-level contextual information to learn cross-modal relationships, while on a pixel level, TerraMind leverages fine-grained representations to capture critical spatial nuances. We pretrained TerraMind on nine geospatial modalities of a global, large-scale dataset. In this paper, we demonstrate that (i) TerraMind’s dual-scale early fusion approach unlocks a range of zero-shot and few-shot applications for Earth observation, (ii) TerraMind introduces “Thinking-in-Modalities” (TiM) – the capability of generating additional artificial data during finetuning and inference to improve the model output – and (iii) TerraMind achieves beyond state-of-the-art performance in community-standard benchmarks for EO like PANGAEA. The pretraining dataset, the model weights, and our code is open-sourced under a permissive license. </p>
<blockquote>
<p>我们推出TerraMind，这是首个面向地球观测（EO）的任意到任意生成、多模式基础模型。与其他多模式模型不同，TerraMind是在双尺度表示上预训练的，结合了跨模式的令牌级别和像素级别的数据。在令牌级别上，TerraMind编码高级上下文信息来学习跨模式关系，而在像素级别上，TerraMind利用精细表示来捕捉关键的空间细微差别。我们在全球大规模数据的九个地理空间模式上预训练了TerraMind。在本文中，我们证明（i）TerraMind的双尺度早期融合方法解锁了一系列零样本和少样本的地球观测应用，（ii）TerraMind引入了“思考模式”（Thinking-in-Modalities）的能力——在微调过程中和推理期间生成额外的合成数据，以改善模型输出——以及（iii）TerraMind在像PANGAEA这样的地球观测社区标准基准测试中达到了超越现有技术的性能。预训练数据集、模型权重和我们的代码都是在许可下开源的。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11171v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>TerraMind是首个针对地球观测的任意输入-任意生成、多模态基础模型。它采用双尺度表示法，结合标记级别和像素级别的数据跨模态进行预训练。TerraMind的预训练数据集包含全球大规模数据的九种地理空间模态。本文展示了TerraMind的多种零样本和少样本应用、其特有的“模态思考”（TiM）能力以及其在地球观测社区标准基准测试中的卓越性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TerraMind是首个地球观测的多模态基础模型，支持任意输入和生成。</li>
<li>TerraMind采用独特的双尺度预训练方法，结合标记和像素级别的数据。</li>
<li>模型能在标记级别编码高级上下文信息，学习跨模态关系。</li>
<li>在像素级别，TerraMind利用精细表示来捕捉关键的空间细微差别。</li>
<li>TerraMind的“模态思考”（TiM）能力可在微调及推理过程中生成额外的人工数据，改善模型输出。</li>
<li>TerraMind在地球观测的社区标准基准测试中实现了超越现有技术水平的性能。</li>
<li>TerraMind的预训练数据集、模型权重和代码已开源。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11171">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9205e9d6a88eb9d6aba99f9224243888.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-669adae5470be44358a88b27fdebd2e5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0905d58a88dfc548be28b1ad933178cb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1350d9d9a5fcf66a5002c5395525ebcd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5afbb7cbc5ea855d062167fc201e2458.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f9e5bd4b9a38561ff8eb9e512c039c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-68a4b53117f6335abab2c266efc0d6f3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-856636072a2e03ae50298229c2132de7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-875df7cd550ea8587c1d66f3794d03fb.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="TSAL-Few-shot-Text-Segmentation-Based-on-Attribute-Learning"><a href="#TSAL-Few-shot-Text-Segmentation-Based-on-Attribute-Learning" class="headerlink" title="TSAL: Few-shot Text Segmentation Based on Attribute Learning"></a>TSAL: Few-shot Text Segmentation Based on Attribute Learning</h2><p><strong>Authors:Chenming Li, Chengxu Liu, Yuanting Fan, Xiao Jin, Xingsong Hou, Xueming Qian</strong></p>
<p>Recently supervised learning rapidly develops in scene text segmentation. However, the lack of high-quality datasets and the high cost of pixel annotation greatly limit the development of them. Considering the well-performed few-shot learning methods for downstream tasks, we investigate the application of the few-shot learning method to scene text segmentation. We propose TSAL, which leverages CLIP’s prior knowledge to learn text attributes for segmentation. To fully utilize the semantic and texture information in the image, a visual-guided branch is proposed to separately extract text and background features. To reduce data dependency and improve text detection accuracy, the adaptive prompt-guided branch employs effective adaptive prompt templates to capture various text attributes. To enable adaptive prompts capture distinctive text features and complex background distribution, we propose Adaptive Feature Alignment module(AFA). By aligning learnable tokens of different attributes with visual features and prompt prototypes, AFA enables adaptive prompts to capture both general and distinctive attribute information. TSAL can capture the unique attributes of text and achieve precise segmentation using only few images. Experiments demonstrate that our method achieves SOTA performance on multiple text segmentation datasets under few-shot settings and show great potential in text-related domains. </p>
<blockquote>
<p>近期，有监督学习在场景文本分割领域得到了快速发展。然而，高质量数据集的缺乏以及像素注释的高成本极大地限制了其发展。考虑到下游任务中表现良好的小样本学习方法，我们研究了小样本学习方法在场景文本分割中的应用。我们提出TSAL，它利用CLIP的先验知识来学习文本属性进行分割。为了充分利用图像中的语义和纹理信息，我们提出了视觉引导分支来分别提取文本和背景特征。为了减少数据依赖并提高文本检测精度，自适应提示引导分支采用有效的自适应提示模板来捕获各种文本属性。为了使自适应提示能够捕捉独特的文本特征和复杂的背景分布，我们提出了自适应特征对齐模块（AFA）。通过将对不同属性的可学习标记与视觉特征和提示原型进行对齐，AFA使自适应提示能够捕获一般和独特的属性信息。TSAL可以捕捉文本的独特属性，并使用仅少量的图像实现精确分割。实验表明，我们的方法在多个文本分割数据集的小样本设置下达到了最先进的性能，并在文本相关领域具有巨大的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11164v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>近期监督学习在场景文本分割领域快速发展，但缺乏高质量数据集和像素标注的高成本限制了其发展。研究将少样本学习方法应用于场景文本分割，提出TSAL方法，利用CLIP的先验知识学习文本属性进行分割。为充分利用图像中的语义和纹理信息，提出视觉引导分支以分别提取文本和背景特征。为降低数据依赖并提高文本检测精度，采用自适应提示引导分支，使用有效的自适应提示模板捕获各种文本属性。为使自适应提示捕捉独特的文本特征和复杂的背景分布，提出Adaptive Feature Alignment模块（AFA）。通过对齐不同属性的学习标记和视觉特征以及提示原型，AFA使自适应提示能够捕捉通用和独特的属性信息。TSAL仅使用少量图像即可捕捉文本的独特属性，并实现精确分割。实验表明，该方法在多个文本分割数据集上实现了SOTA性能，并在文本相关领域具有巨大潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>监督学习在场景文本分割中发展快速，受限因素包括高质量数据集缺乏和像素标注的高成本。</li>
<li>研究将少样本学习方法应用于场景文本分割，提出TSAL方法。</li>
<li>TSAL利用CLIP的先验知识学习文本属性进行分割。</li>
<li>为充分利用图像中的语义和纹理信息，提出视觉引导分支。</li>
<li>自适应提示引导分支用于降低数据依赖并提高文本检测精度。</li>
<li>引入Adaptive Feature Alignment模块（AFA），帮助自适应提示捕捉独特的文本特征和复杂的背景分布。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11164">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f090360409f36e42be7bf097244ee6dd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9477ea4119ddc94999a43138836925c5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d64a51320cede4c2ec38bf602d41bb4a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56b592b3ad23ce6acfaba7e0bd7b93bd.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Meta-learning-For-Few-Shot-Time-Series-Crop-Type-Classification-A-Benchmark-On-The-EuroCropsML-Dataset"><a href="#Meta-learning-For-Few-Shot-Time-Series-Crop-Type-Classification-A-Benchmark-On-The-EuroCropsML-Dataset" class="headerlink" title="Meta-learning For Few-Shot Time Series Crop Type Classification: A   Benchmark On The EuroCropsML Dataset"></a>Meta-learning For Few-Shot Time Series Crop Type Classification: A   Benchmark On The EuroCropsML Dataset</h2><p><strong>Authors:Joana Reuss, Jan Macdonald, Simon Becker, Konrad Schultka, Lorenz Richter, Marco Körner</strong></p>
<p>Spatial imbalances in crop type data pose significant challenges for accurate classification in remote sensing applications. Algorithms aiming at transferring knowledge from data-rich to data-scarce tasks have thus surged in popularity. However, despite their effectiveness in previous evaluations, their performance in challenging real-world applications is unclear and needs to be evaluated. This study benchmarks transfer learning and several meta-learning algorithms, including (First-Order) Model-Agnostic Meta-Learning ((FO)-MAML), Almost No Inner Loop (ANIL), and Task-Informed Meta-Learning (TIML), on the real-world EuroCropsML time series dataset, which combines farmer-reported crop data with Sentinel-2 satellite observations from Estonia, Latvia, and Portugal. Our findings indicate that MAML-based meta-learning algorithms achieve slightly higher accuracy compared to simpler transfer learning methods when applied to crop type classification tasks in Estonia after pre-training on data from Latvia. However, this improvement comes at the cost of increased computational demands and training time. Moreover, we find that the transfer of knowledge between geographically disparate regions, such as Estonia and Portugal, poses significant challenges to all investigated algorithms. These insights underscore the trade-offs between accuracy and computational resource requirements in selecting machine learning methods for real-world crop type classification tasks and highlight the difficulties of transferring knowledge between different regions of the Earth. To facilitate future research in this domain, we present the first comprehensive benchmark for evaluating transfer and meta-learning methods for crop type classification under real-world conditions. The corresponding code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/dida-do/eurocrops-meta-learning">https://github.com/dida-do/eurocrops-meta-learning</a>. </p>
<blockquote>
<p>农作物类型数据在空间上的不平衡给遥感应用中的准确分类带来了重大挑战。因此，旨在从数据丰富任务向数据稀缺任务转移知识的算法变得非常受欢迎。然而，尽管这些算法在之前的评估中非常有效，它们在具有挑战性的现实世界应用中的表现尚不清楚，需要进行评估。本研究对迁移学习和几种元学习算法进行了基准测试，包括（一阶）模型无关元学习（FO-MAML）、几乎无内环（ANIL）和任务信息元学习（TIML），基准测试是在现实世界的EuroCropsML时间序列数据集上进行的，该数据集结合了农民报告的作物数据与来自爱沙尼亚、拉脱维亚和葡萄牙的Sentinel-2卫星观测数据。我们的研究发现，基于MAML的元学习算法在爱沙尼亚的作物类型分类任务上，与较简单的迁移学习方法相比，取得了略高的精度，这些算法是在拉脱维亚的数据上进行预训练之后应用的。然而，这一改进是以增加计算需求和训练时间为代价的。此外，我们发现，在地理上相距较远的地区之间转移知识，如爱沙尼亚和葡萄牙，给所有调查过的算法都带来了重大挑战。这些见解强调了选择机器学习方法进行现实世界作物类型分类任务时在准确性和计算资源需求之间的权衡，并突出了在不同地区之间转移知识的困难。为了促进该领域未来的研究，我们提供了第一个全面的基准测试，以评估现实世界条件下作物类型分类的迁移和元学习方法。相应的代码可在<a target="_blank" rel="noopener" href="https://github.com/dida-do/eurocrops-meta-learning%E5%85%AC%E5%BC%80%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/dida-do/eurocrops-meta-learning公开获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11022v1">PDF</a> 19 pages, 7 figures, 12 tables</p>
<p><strong>Summary</strong></p>
<p>该文本研究了作物类型数据中的空间不平衡现象对遥感应用中的准确分类带来的挑战。为了应对这一问题，采用基于知识转移的算法变得越来越流行。本文在真实的EuroCropsML时间序列数据集上评估了迁移学习和几种元学习算法的性能，该数据集结合了农民报告的作物数据与Sentinel-2卫星观测数据，来自爱沙尼亚、拉脱维亚和葡萄牙。研究发现，基于MAML的元学习算法在爱沙尼亚的作物类型分类任务上略优于简单的迁移学习方法，但这需要更高的计算需求和更长的训练时间。此外，不同地理区域间的知识转移存在挑战。这些见解强调了选择机器学习方法进行实际作物类型分类时在准确性和计算资源要求之间的权衡，并突出了在地球不同地区转移知识的困难。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>空间不平衡的作物类型数据对遥感应用的准确分类构成挑战。</li>
<li>知识转移算法在处理数据稀缺任务时受到欢迎。</li>
<li>在真实的EuroCropsML数据集上评估了迁移学习和多种元学习算法。</li>
<li>基于MAML的元学习算法在爱沙尼亚的作物分类任务上表现出较高的准确性，但计算需求较高。</li>
<li>地理区域间的知识转移存在挑战。</li>
<li>在选择机器学习方法进行实际作物类型分类时，需要权衡准确性和计算资源要求。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11022">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-a1091ad571ea48a1ef6becf5bd756567.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-67505a7be9b7993975f2795bb584eba7.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Adaptive-Decision-Boundary-for-Few-Shot-Class-Incremental-Learning"><a href="#Adaptive-Decision-Boundary-for-Few-Shot-Class-Incremental-Learning" class="headerlink" title="Adaptive Decision Boundary for Few-Shot Class-Incremental Learning"></a>Adaptive Decision Boundary for Few-Shot Class-Incremental Learning</h2><p><strong>Authors:Linhao Li, Yongzhang Tan, Siyuan Yang, Hao Cheng, Yongfeng Dong, Liang Yang</strong></p>
<p>Few-Shot Class-Incremental Learning (FSCIL) aims to continuously learn new classes from a limited set of training samples without forgetting knowledge of previously learned classes. Conventional FSCIL methods typically build a robust feature extractor during the base training session with abundant training samples and subsequently freeze this extractor, only fine-tuning the classifier in subsequent incremental phases. However, current strategies primarily focus on preventing catastrophic forgetting, considering only the relationship between novel and base classes, without paying attention to the specific decision spaces of each class. To address this challenge, we propose a plug-and-play Adaptive Decision Boundary Strategy (ADBS), which is compatible with most FSCIL methods. Specifically, we assign a specific decision boundary to each class and adaptively adjust these boundaries during training to optimally refine the decision spaces for the classes in each session. Furthermore, to amplify the distinctiveness between classes, we employ a novel inter-class constraint loss that optimizes the decision boundaries and prototypes for each class. Extensive experiments on three benchmarks, namely CIFAR100, miniImageNet, and CUB200, demonstrate that incorporating our ADBS method with existing FSCIL techniques significantly improves performance, achieving overall state-of-the-art results. </p>
<blockquote>
<p>少量类别增量学习（FSCIL）旨在从有限的训练样本中持续学习新类别，同时不遗忘已学类别的知识。传统的FSCIL方法通常在基础训练会话中利用丰富的训练样本构建强大的特征提取器，随后冻结该提取器，仅在后续的增量阶段微调分类器。然而，当前策略主要集中在防止灾难性遗忘上，只考虑新类别与基础类别之间的关系，而没有关注每个类别的特定决策空间。为了解决这一挑战，我们提出了一种即插即用的自适应决策边界策略（ADBS），它与大多数FSCIL方法兼容。具体来说，我们为每个类别分配一个特定的决策边界，并在训练过程中自适应地调整这些边界，以最优方式细化每个会话中类别的决策空间。此外，为了放大类别之间的差异性，我们采用了一种新型类间约束损失，该损失优化了决策边界和每个类的原型。在CIFAR100、miniImageNet和CUB200三个基准测试上的大量实验表明，将我们的ADBS方法与现有的FSCIL技术相结合，可以显著提高性能，达到最新最先进的水平。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10976v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该文本介绍了Few-Shot类增量学习（FSCIL）的目标和现有方法的问题。文中指出FSCIL旨在从有限的训练样本中持续学习新类别，同时不遗忘已学类别的知识。传统FSCIL方法通常会在基础训练阶段构建强大的特征提取器，并在后续增量阶段仅微调分类器。然而，当前策略主要关注防止灾难性遗忘，考虑新类别与基础类别之间的关系，而忽视了每个类别的特定决策空间。为解决这一问题，提出了兼容大多数FSCIL方法的即插即用自适应决策边界策略（ADBS）。通过为每个类别分配特定的决策边界，并在训练过程中自适应地调整这些边界，以优化每个会话中的类别决策空间。此外，还采用了一种新型的类间约束损失，以优化每个类别的决策边界和原型，放大类之间的差异性。在CIFAR100、miniImageNet和CUB200三个基准测试上的实验表明，将ADBS方法与现有FSCIL技术相结合，可显著提高性能，达到最新状态。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Few-Shot类增量学习（FSCIL）的目标是持续学习新类别，从有限的训练样本中，同时保持对旧知识的记忆。</li>
<li>传统FSCIL方法主要关注防止灾难性遗忘，忽视了每个类别的特定决策空间。</li>
<li>提出的Adaptive Decision Boundary Strategy（ADBS）策略为每个类别分配特定的决策边界，并在训练过程中自适应调整这些边界。</li>
<li>ADBS策略通过优化决策边界和原型，提高了类之间的区分度。</li>
<li>ADBS策略与大多数FSCIL方法兼容，可以显著提高性能。</li>
<li>在多个基准测试上，结合ADBS方法和现有FSCIL技术的性能达到了最新状态。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10976">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b7c72da87dd5499f7e9284d536a341c4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-31ea2229f48d443464a3c69e8764baf8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd38f190db35eac8afab0dc6ac8be6f3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-86068c9a39d4bb320337e2985d14de2f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7886339b3032dc74057e2df9865880d2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-641b12cd60d19891e58f1df6ad29a79c.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="NTIRE-2025-Challenge-on-Cross-Domain-Few-Shot-Object-Detection-Methods-and-Results"><a href="#NTIRE-2025-Challenge-on-Cross-Domain-Few-Shot-Object-Detection-Methods-and-Results" class="headerlink" title="NTIRE 2025 Challenge on Cross-Domain Few-Shot Object Detection: Methods   and Results"></a>NTIRE 2025 Challenge on Cross-Domain Few-Shot Object Detection: Methods   and Results</h2><p><strong>Authors:Yuqian Fu, Xingyu Qiu, Bin Ren, Yanwei Fu, Radu Timofte, Nicu Sebe, Ming-Hsuan Yang, Luc Van Gool, Kaijin Zhang, Qingpeng Nong, Xiugang Dong, Hong Gao, Xiangsheng Zhou, Jiancheng Pan, Yanxing Liu, Xiao He, Jiahao Li, Yuze Sun, Xiaomeng Huang, Zhenyu Zhang, Ran Ma, Yuhan Liu, Zijian Zhuang, Shuai Yi, Yixiong Zou, Lingyi Hong, Mingxi Chen, Runze Li, Xingdong Sheng, Wenqiang Zhang, Weisen Chen, Yongxin Yan, Xinguo Chen, Yuanjie Shao, Zhengrong Zuo, Nong Sang, Hao Wu, Haoran Sun, Shuming Hu, Yan Zhang, Zhiguang Shi, Yu Zhang, Chao Chen, Tao Wang, Da Feng, Linhai Zhuo, Ziming Lin, Yali Huang, Jie Me, Yiming Yang, Mi Guo, Mingyuan Jiu, Mingliang Xu, Maomao Xiong, Qunshu Zhang, Xinyu Cao, Yuqing Yang, Dianmo Sheng, Xuanpu Zhao, Zhiyu Li, Xuyang Ding, Wenqian Li</strong></p>
<p>Cross-Domain Few-Shot Object Detection (CD-FSOD) poses significant challenges to existing object detection and few-shot detection models when applied across domains. In conjunction with NTIRE 2025, we organized the 1st CD-FSOD Challenge, aiming to advance the performance of current object detectors on entirely novel target domains with only limited labeled data. The challenge attracted 152 registered participants, received submissions from 42 teams, and concluded with 13 teams making valid final submissions. Participants approached the task from diverse perspectives, proposing novel models that achieved new state-of-the-art (SOTA) results under both open-source and closed-source settings. In this report, we present an overview of the 1st NTIRE 2025 CD-FSOD Challenge, highlighting the proposed solutions and summarizing the results submitted by the participants. </p>
<blockquote>
<p>跨域小样本目标检测（CD-FSOD）给现有的目标检测和少样本检测模型带来了重大挑战，尤其是在跨域应用时。结合NTIRE 2025，我们组织了首届CD-FSOD挑战赛，旨在提高当前目标检测器在仅有少量标注数据的新目标域上的性能。该挑战赛吸引了152名注册参赛者，收到42个团队的提交，最终有13个团队提交了有效的最终作品。参赛者从不同角度入手，提出新型模型，在开源和闭源环境下均取得了最新的最新结果。本报告将概述NTIRE 2025首届CD-FSOD挑战赛，重点介绍提出的解决方案，并总结参赛者提交的结果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10685v1">PDF</a> accepted by CVPRW 25 @ NTIRE</p>
<p><strong>Summary</strong></p>
<p>面向跨域小样本目标检测（CD-FSOD）的挑战性问题，NTIRE 2025首次举办了CD-FSOD挑战赛，旨在提高当前目标检测器在全新目标域有限标签数据下的性能。吸引了大量参与者提交新型模型和方法，部分模型在开放源代码和封闭源代码环境中都达到了最新的技术水平。本报告详细介绍了此次挑战赛的参与者提交的方案及结果。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>CD-FSOD挑战赛旨在解决跨域小样本目标检测的难题。</li>
<li>挑战赛吸引了大量参与者提交新型模型和方法。</li>
<li>部分模型在开放源代码和封闭源代码环境中都达到了最新的技术水平。</li>
<li>参赛者从不同的角度采用了多元化的策略来应对挑战。</li>
<li>比赛结果显示了在有限的标签数据下实现高准确率的可能性。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10685">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-e02e1ea348b95554a79c62423921a02b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-550a55638a71ba02728bc99a532ce286.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-72d3d4042504c0fd8a2aabff721f0207.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8e2007539576505fc5c4e352430baeb3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3aeacaea6326988dcc8594349fb478e4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4bdefd34e1567e0b9365cc1269b05528.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Improving-In-Context-Learning-with-Reasoning-Distillation"><a href="#Improving-In-Context-Learning-with-Reasoning-Distillation" class="headerlink" title="Improving In-Context Learning with Reasoning Distillation"></a>Improving In-Context Learning with Reasoning Distillation</h2><p><strong>Authors:Nafis Sadeq, Xin Xu, Zhouhang Xie, Julian McAuley, Byungkyu Kang, Prarit Lamba, Xiang Gao</strong></p>
<p>Language models rely on semantic priors to perform in-context learning, which leads to poor performance on tasks involving inductive reasoning. Instruction-tuning methods based on imitation learning can superficially enhance the in-context learning performance of language models, but they often fail to improve the model’s understanding of the underlying rules that connect inputs and outputs in few-shot demonstrations. We propose ReDis, a reasoning distillation technique designed to improve the inductive reasoning capabilities of language models. Through a careful combination of data augmentation, filtering, supervised fine-tuning, and alignment, ReDis achieves significant performance improvements across a diverse range of tasks, including 1D-ARC, List Function, ACRE, and MiniSCAN. Experiments on three language model backbones show that ReDis outperforms equivalent few-shot prompting baselines across all tasks and even surpasses the teacher model, GPT-4o, in some cases. ReDis, based on the LLaMA-3 backbone, achieves relative improvements of 23.2%, 2.8%, and 66.6% over GPT-4o on 1D-ARC, ACRE, and MiniSCAN, respectively, within a similar hypothesis search space. The code, dataset, and model checkpoints will be made available at <a target="_blank" rel="noopener" href="https://github.com/NafisSadeq/reasoning-distillation.git">https://github.com/NafisSadeq/reasoning-distillation.git</a>. </p>
<blockquote>
<p>语言模型依赖语义先验来进行上下文学习，这导致在处理涉及归纳推理的任务时表现不佳。基于模仿学习的指令调整方法表面上看似增强了语言模型的上下文学习能力，但它们往往未能改善模型对少量演示中输入和输出之间基本规则的理解。我们提出了ReDis，这是一种推理蒸馏技术，旨在提高语言模型的归纳推理能力。通过数据增强、过滤、监督微调和对齐的精心结合，ReDis在多种任务上实现了显著的性能提升，包括1D-ARC、List Function、ACRE和MiniSCAN。在三个语言模型主干上的实验表明，ReDis在所有任务上的表现都超过了等效的少量提示基线，甚至在某些情况下超越了教师模型GPT-4o。基于LLaMA-3主干的ReDis在类似假设搜索空间内相对于GPT-4o在1D-ARC、ACRE和MiniSCAN上分别实现了23.2%、2.8%和66.6%的相对改进。代码、数据集和模型检查点将在<a target="_blank" rel="noopener" href="https://github.com/NafisSadeq/reasoning-distillation.git%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/NafisSadeq/reasoning-distillation.git上提供。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10647v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探讨了语言模型在涉及归纳推理任务时的性能问题。为解决这一问题，提出了一种名为ReDis的推理蒸馏技术。该技术通过数据增强、过滤、监督微调和对齐等步骤，显著提高了语言模型在多种任务上的归纳推理能力。实验表明，ReDis在多个任务上超越了少样本提示基线，甚至在某些情况下超越了教师模型GPT-4o。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语言模型依赖语义先验进行上下文学习，这在涉及归纳推理的任务上表现不佳。</li>
<li>指令调整方法虽然能提升语言模型的上下文学习性能，但难以提高模型对少数演示中输入输出规则的理解。</li>
<li>提出了ReDis推理蒸馏技术，旨在提高语言模型的归纳推理能力。</li>
<li>ReDis通过数据增强、过滤、监督微调和对齐等步骤实现性能提升。</li>
<li>ReDis在多种任务上表现出显著性能改进，包括1D-ARC、List Function、ACRE和MiniSCAN。</li>
<li>实验表明，ReDis在三个不同的语言模型骨架上均超越了少样本提示基线，并在某些情况下超越了教师模型GPT-4o。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10647">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f90cda74598c8660c157a905f75b4360.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e9db694b375b0f1be6b93129b108f775.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dc90b54ce525ca22384b9366a9451c56.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f78e453174dceba41599f0b4ce3bd937.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="MalMixer-Few-Shot-Malware-Classification-with-Retrieval-Augmented-Semi-Supervised-Learning"><a href="#MalMixer-Few-Shot-Malware-Classification-with-Retrieval-Augmented-Semi-Supervised-Learning" class="headerlink" title="MalMixer: Few-Shot Malware Classification with Retrieval-Augmented   Semi-Supervised Learning"></a>MalMixer: Few-Shot Malware Classification with Retrieval-Augmented   Semi-Supervised Learning</h2><p><strong>Authors:Jiliang Li, Yifan Zhang, Yu Huang, Kevin Leach</strong></p>
<p>Recent growth and proliferation of malware has tested practitioners’ ability to promptly classify new samples according to malware families. In contrast to labor-intensive reverse engineering efforts, machine learning approaches have demonstrated increased speed and accuracy. However, most existing deep-learning malware family classifiers must be calibrated using a large number of samples that are painstakingly manually analyzed before training. Furthermore, as novel malware samples arise that are beyond the scope of the training set, additional reverse engineering effort must be employed to update the training set. The sheer volume of new samples found in the wild creates substantial pressure on practitioners’ ability to reverse engineer enough malware to adequately train modern classifiers. In this paper, we present MalMixer, a malware family classifier using semi-supervised learning that achieves high accuracy with sparse training data. We present a novel domain-knowledge-aware technique for augmenting malware feature representations, enhancing few-shot performance of semi-supervised malware family classification. We show that MalMixer achieves state-of-the-art performance in few-shot malware family classification settings. Our research confirms the feasibility and effectiveness of lightweight, domain-knowledge-aware feature augmentation methods and highlights the capabilities of similar semi-supervised classifiers in addressing malware classification issues. </p>
<blockquote>
<p>最近恶意软件的增长和扩散考验了从业人员根据恶意软件家族及时对新样本进行分类的能力。与劳动密集型的逆向工程努力相比，机器学习的方法已经显示出更高的速度和准确性。然而，大多数现有的深度学习的恶意软件家族分类器需要使用大量样本进行校准，这些样本在训练之前需要进行繁琐的手动分析。此外，随着超出训练集范围的新的恶意软件样本的出现，必须采用更多的逆向工程工作来更新训练集。野外发现的新样本的数量给从业人员带来了巨大的压力，他们必须逆向工程足够多的恶意软件以充分训练现代分类器。在本文中，我们提出了使用半监督学习的恶意软件家族分类器MalMixer，它在稀疏的训练数据下就能实现较高的准确性。我们提出了一种新颖的基于领域知识的增强技术，用于增强恶意软件特征表示，提高了半监督恶意软件家族分类的小样本性能。我们证明了MalMixer在少量恶意软件家族分类场景中达到了最先进的性能。我们的研究证实了轻量级、基于领域知识的特征增强方法的可行性和有效性，并突出了类似的半监督分类器在解决恶意软件分类问题方面的能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.13213v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>这篇论文介绍了MalMixer，一种使用半监督学习的高效的恶意软件家族分类器。它能够在训练数据稀疏的情况下实现高准确率。通过采用一种新颖的域知识感知技术增强恶意软件特征表示，提高半监督恶意软件家族的小样本分类性能。研究证实了轻量级、域知识感知的特征增强方法在实际应用中的可行性和有效性，并突出了类似半监督分类器在解决恶意软件分类问题方面的能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MalMixer是一种高效的恶意软件家族分类器，采用半监督学习方法，可在训练数据稀疏的情况下实现高准确率。</li>
<li>提出了新颖的域知识感知技术，用于增强恶意软件特征表示，提高小样本分类性能。</li>
<li>研究证实了轻量级特征增强方法在处理恶意软件分类问题时的有效性。</li>
<li>MalMixer在少样本恶意软件家族分类设置中达到最先进的性能。</li>
<li>域知识感知技术在恶意软件分类中的应用展现了巨大的潜力。</li>
<li>半监督学习在解决恶意软件分类问题方面具有优势。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.13213">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-20509b66a96f9058f0ffe33b4255ab2f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-32bb27d88ec1de00ca71c257db318708.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-910bf5ea638196af9cb760921eea872a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-85e209844c377defc431f96d9a81487f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-682db554604070c09fa35e0e49152148.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-17/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-17/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-17/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-4c7675003f5763fde5e7dd7089fd5379.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-04-17  COP-GEN-Beta Unified Generative Modelling of COPernicus Imagery   Thumbnails
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-17
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-17/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-d1860704f6cde213a9d9d218195fabcd.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-04-17  The Obvious Invisible Threat LLM-Powered GUI Agents' Vulnerability to   Fine-Print Injections
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-17
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">16668k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
