<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-17  Aligning Generative Denoising with Discriminative Objectives Unleashes   Diffusion for Visual Perception">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-04-17\./crop_åŒ»å­¦å›¾åƒ/2504.11286v1/page_5_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-17
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-19
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    17.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    72 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-17-æ›´æ–°"><a href="#2025-04-17-æ›´æ–°" class="headerlink" title="2025-04-17 æ›´æ–°"></a>2025-04-17 æ›´æ–°</h1><h2 id="Aligning-Generative-Denoising-with-Discriminative-Objectives-Unleashes-Diffusion-for-Visual-Perception"><a href="#Aligning-Generative-Denoising-with-Discriminative-Objectives-Unleashes-Diffusion-for-Visual-Perception" class="headerlink" title="Aligning Generative Denoising with Discriminative Objectives Unleashes   Diffusion for Visual Perception"></a>Aligning Generative Denoising with Discriminative Objectives Unleashes   Diffusion for Visual Perception</h2><p><strong>Authors:Ziqi Pang, Xin Xu, Yu-Xiong Wang</strong></p>
<p>With the success of image generation, generative diffusion models are increasingly adopted for discriminative tasks, as pixel generation provides a unified perception interface. However, directly repurposing the generative denoising process for discriminative objectives reveals critical gaps rarely addressed previously. Generative models tolerate intermediate sampling errors if the final distribution remains plausible, but discriminative tasks require rigorous accuracy throughout, as evidenced in challenging multi-modal tasks like referring image segmentation. Motivated by this gap, we analyze and enhance alignment between generative diffusion processes and perception tasks, focusing on how perception quality evolves during denoising. We find: (1) earlier denoising steps contribute disproportionately to perception quality, prompting us to propose tailored learning objectives reflecting varying timestep contributions; (2) later denoising steps show unexpected perception degradation, highlighting sensitivity to training-denoising distribution shifts, addressed by our diffusion-tailored data augmentation; and (3) generative processes uniquely enable interactivity, serving as controllable user interfaces adaptable to correctional prompts in multi-round interactions. Our insights significantly improve diffusion-based perception models without architectural changes, achieving state-of-the-art performance on depth estimation, referring image segmentation, and generalist perception tasks. Code available at <a target="_blank" rel="noopener" href="https://github.com/ziqipang/ADDP">https://github.com/ziqipang/ADDP</a>. </p>
<blockquote>
<p>éšç€å›¾åƒç”Ÿæˆçš„æˆåŠŸï¼Œç”Ÿæˆæ€§æ‰©æ•£æ¨¡å‹ï¼ˆGenerative Diffusion Modelsï¼‰åœ¨åˆ¤åˆ«ä»»åŠ¡ä¸­çš„åº”ç”¨è¶Šæ¥è¶Šå¹¿æ³›ï¼Œå› ä¸ºåƒç´ ç”Ÿæˆæä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„æ„ŸçŸ¥æ¥å£ã€‚ç„¶è€Œï¼Œç›´æ¥å°†ç”Ÿæˆå»å™ªè¿‡ç¨‹åº”ç”¨äºåˆ¤åˆ«ç›®æ ‡å´æš´éœ²å‡ºä¹‹å‰å¾ˆå°‘è§£å†³çš„å…³é”®å·®è·ã€‚ç”Ÿæˆæ¨¡å‹å¦‚æœæœ€ç»ˆåˆ†å¸ƒä»ç„¶å¯è¡Œï¼Œå¯ä»¥å®¹å¿ä¸­é—´é‡‡æ ·è¿‡ç¨‹ä¸­çš„é”™è¯¯ï¼Œä½†åˆ¤åˆ«ä»»åŠ¡éœ€è¦åœ¨æ•´ä¸ªè¿‡ç¨‹ä¸­ä¸¥æ ¼ä¿æŒå‡†ç¡®æ€§ï¼Œå°±åƒåœ¨å›¾åƒå¼•ç”¨åˆ†å‰²ç­‰å¤šæ¨¡å¼ä»»åŠ¡ä¸­æ‰€æ˜¾ç¤ºçš„é‚£æ ·ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¯¹ç”Ÿæˆæ‰©æ•£è¿‡ç¨‹å’Œæ„ŸçŸ¥ä»»åŠ¡ä¹‹é—´çš„å¯¹é½è¿›è¡Œäº†åˆ†æå’Œæ”¹è¿›ï¼Œé‡ç‚¹ç ”ç©¶å»å™ªè¿‡ç¨‹ä¸­æ„ŸçŸ¥è´¨é‡å¦‚ä½•æ¼”å˜ã€‚æˆ‘ä»¬å‘ç°ï¼šï¼ˆ1ï¼‰æ—©æœŸçš„å»å™ªæ­¥éª¤å¯¹æ„ŸçŸ¥è´¨é‡çš„è´¡çŒ®ä¸æˆæ¯”ä¾‹ï¼Œä¿ƒä½¿æˆ‘ä»¬æå‡ºåæ˜ ä¸åŒæ—¶é—´æ­¥è´¡çŒ®çš„å­¦ä¹ ç›®æ ‡ï¼›ï¼ˆ2ï¼‰åœ¨è¾ƒåçš„å»å™ªæ­¥éª¤ä¸­å‡ºç°äº†æ„å¤–çš„æ„ŸçŸ¥è´¨é‡ä¸‹é™ï¼Œè¿™çªæ˜¾äº†å¯¹è®­ç»ƒå»å™ªåˆ†å¸ƒå˜åŒ–çš„æ•æ„Ÿæ€§ï¼Œæˆ‘ä»¬çš„æ‰©æ•£å®šåˆ¶æ•°æ®å¢å¼ºè§£å†³äº†è¿™ä¸€é—®é¢˜ï¼›ï¼ˆ3ï¼‰ç”Ÿæˆè¿‡ç¨‹å…·æœ‰ç‹¬ç‰¹çš„äº¤äº’æ€§ï¼Œå¯ä½œä¸ºå¯æ§çš„ç”¨æˆ·ç•Œé¢ï¼Œé€‚åº”å¤šè½®äº¤äº’ä¸­çš„æ ¡æ­£æç¤ºã€‚æˆ‘ä»¬çš„è§è§£åœ¨ä¸æ”¹å˜æ¶æ„çš„æƒ…å†µä¸‹æ˜¾è‘—æ”¹è¿›äº†åŸºäºæ‰©æ•£çš„æ„ŸçŸ¥æ¨¡å‹ï¼Œåœ¨æ·±åº¦ä¼°è®¡ã€å›¾åƒå¼•ç”¨åˆ†å‰²å’Œé€šç”¨æ„ŸçŸ¥ä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä»£ç å¯é€šè¿‡ä»¥ä¸‹ç½‘å€è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/ziqipang/ADDP">https://github.com/ziqipang/ADDP</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11457v1">PDF</a> ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå›¾åƒç”Ÿæˆçš„æˆåŠŸï¼Œç”Ÿæˆæ€§æ‰©æ•£æ¨¡å‹åœ¨åˆ¤åˆ«ä»»åŠ¡ä¸­çš„åº”ç”¨æ—¥ç›Šå¹¿æ³›ï¼Œåƒç´ ç”Ÿæˆæä¾›äº†ç»Ÿä¸€çš„æ„ŸçŸ¥æ¥å£ã€‚æœ¬æ–‡åˆ†æäº†å°†ç”Ÿæˆæ€§å»å™ªè¿‡ç¨‹ç›´æ¥åº”ç”¨äºåˆ¤åˆ«ä»»åŠ¡æ—¶å­˜åœ¨çš„å…³é”®å·®è·ï¼Œæå‡ºäº†ä¸€ç³»åˆ—é’ˆå¯¹æ‰©æ•£è¿‡ç¨‹å’Œæ„ŸçŸ¥ä»»åŠ¡ä¹‹é—´å¯¹é½æ€§çš„æ”¹è¿›ç­–ç•¥ã€‚ç ”ç©¶å‘ç°æ—©æœŸå»å™ªæ­¥éª¤å¯¹æ„ŸçŸ¥è´¨é‡çš„è´¡çŒ®è¾ƒå¤§ï¼Œæå‡ºäº†åæ˜ ä¸åŒæ—¶é—´æ­¥è´¡çŒ®çš„å­¦ä¹ ç›®æ ‡ï¼›åæœŸå»å™ªæ­¥éª¤ä¼šå‡ºç°æ„ŸçŸ¥é€€åŒ–ï¼Œé€šè¿‡æ‰©æ•£å®šåˆ¶çš„æ•°æ®å¢å¼ºè§£å†³è®­ç»ƒå»å™ªåˆ†å¸ƒåç§»é—®é¢˜ï¼›ç”Ÿæˆè¿‡ç¨‹å…·æœ‰ç‹¬ç‰¹çš„äº¤äº’æ€§ï¼Œå¯ä½œä¸ºå¤šè½®äº¤äº’ä¸­çš„å¯æ ¡æ­£æç¤ºã€‚è¿™äº›è§è§£åœ¨ä¸æ”¹å˜æ¶æ„çš„æƒ…å†µä¸‹æ˜¾è‘—æé«˜äº†åŸºäºæ‰©æ•£çš„æ„ŸçŸ¥æ¨¡å‹æ€§èƒ½ï¼Œåœ¨æ·±åº¦ä¼°è®¡ã€å¼•ç”¨å›¾åƒåˆ†å‰²å’Œé€šç”¨æ„ŸçŸ¥ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€æ–°æ€§èƒ½æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆæ€§æ‰©æ•£æ¨¡å‹åœ¨åˆ¤åˆ«ä»»åŠ¡ä¸­çš„åº”ç”¨é€æ¸æ™®åŠï¼Œä½†ä»å­˜åœ¨å…³é”®å·®è·ã€‚</li>
<li>æ—©æœŸå»å™ªæ­¥éª¤å¯¹æ„ŸçŸ¥è´¨é‡çš„è´¡çŒ®è¾ƒå¤§ï¼Œéœ€è¦é’ˆå¯¹æ€§åœ°è®¾è®¡å­¦ä¹ ç›®æ ‡ã€‚</li>
<li>åæœŸå»å™ªæ­¥éª¤å­˜åœ¨æ„ŸçŸ¥é€€åŒ–é—®é¢˜ï¼Œè®­ç»ƒå»å™ªåˆ†å¸ƒåç§»æ˜¯ä¸»è¦åŸå› ï¼Œå¯é€šè¿‡æ•°æ®å¢å¼ºè§£å†³ã€‚</li>
<li>ç”Ÿæˆè¿‡ç¨‹å…·æœ‰äº¤äº’æ€§ä¼˜åŠ¿ï¼Œå¯ä½œä¸ºå¤šè½®äº¤äº’ä¸­çš„å¯æ ¡æ­£æç¤ºã€‚</li>
<li>é’ˆå¯¹ä»¥ä¸Šå‘ç°ï¼Œæå‡ºçš„ç­–ç•¥åœ¨ä¸æ”¹å˜æ¨¡å‹æ¶æ„çš„æƒ…å†µä¸‹æ˜¾è‘—æé«˜äº†æ„ŸçŸ¥æ€§èƒ½ã€‚</li>
<li>åœ¨æ·±åº¦ä¼°è®¡ã€å¼•ç”¨å›¾åƒåˆ†å‰²å’Œé€šç”¨æ„ŸçŸ¥ä»»åŠ¡ä¸Šå–å¾—äº†æœ€æ–°æ€§èƒ½æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11457">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-17\./crop_åŒ»å­¦å›¾åƒ/2504.11457v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-17\./crop_åŒ»å­¦å›¾åƒ/2504.11457v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-17\./crop_åŒ»å­¦å›¾åƒ/2504.11457v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-17\./crop_åŒ»å­¦å›¾åƒ/2504.11457v1/page_4_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="PARTFIELD-Learning-3D-Feature-Fields-for-Part-Segmentation-and-Beyond"><a href="#PARTFIELD-Learning-3D-Feature-Fields-for-Part-Segmentation-and-Beyond" class="headerlink" title="PARTFIELD: Learning 3D Feature Fields for Part Segmentation and Beyond"></a>PARTFIELD: Learning 3D Feature Fields for Part Segmentation and Beyond</h2><p><strong>Authors:Minghua Liu, Mikaela Angelina Uy, Donglai Xiang, Hao Su, Sanja Fidler, Nicholas Sharp, Jun Gao</strong></p>
<p>We propose PartField, a feedforward approach for learning part-based 3D features, which captures the general concept of parts and their hierarchy without relying on predefined templates or text-based names, and can be applied to open-world 3D shapes across various modalities. PartField requires only a 3D feedforward pass at inference time, significantly improving runtime and robustness compared to prior approaches. Our model is trained by distilling 2D and 3D part proposals from a mix of labeled datasets and image segmentations on large unsupervised datasets, via a contrastive learning formulation. It produces a continuous feature field which can be clustered to yield a hierarchical part decomposition. Comparisons show that PartField is up to 20% more accurate and often orders of magnitude faster than other recent class-agnostic part-segmentation methods. Beyond single-shape part decomposition, consistency in the learned field emerges across shapes, enabling tasks such as co-segmentation and correspondence, which we demonstrate in several applications of these general-purpose, hierarchical, and consistent 3D feature fields. Check our Webpage! <a target="_blank" rel="noopener" href="https://research.nvidia.com/labs/toronto-ai/partfield-release/">https://research.nvidia.com/labs/toronto-ai/partfield-release/</a> </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†PartFieldï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå‰é¦ˆçš„å­¦ä¹ å±€éƒ¨ä¸‰ç»´ç‰¹å¾çš„æ–¹æ³•ã€‚PartFieldæ•æ‰äº†éƒ¨ä»¶çš„ä¸€èˆ¬æ¦‚å¿µå’Œå®ƒä»¬çš„å±‚æ¬¡ç»“æ„ï¼Œè€Œæ— éœ€ä¾èµ–é¢„å…ˆå®šä¹‰çš„æ¨¡æ¿æˆ–åŸºäºæ–‡æœ¬çš„åç§°ï¼Œå¹¶å¯åº”ç”¨äºå„ç§æ¨¡æ€çš„å¼€æ”¾ä¸–ç•Œä¸‰ç»´å½¢çŠ¶ã€‚PartFieldåœ¨æ¨ç†æ—¶é—´åªéœ€è¦è¿›è¡Œä¸‰ç»´å‰é¦ˆä¼ é€’ï¼Œä¸å…ˆå‰çš„æ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—æé«˜äº†è¿è¡Œæ—¶é—´å’Œç¨³å¥æ€§ã€‚æˆ‘ä»¬çš„æ¨¡å‹é€šè¿‡ä»æ··åˆæ ‡è®°æ•°æ®é›†å’Œå¤§å‹æ— ç›‘ç£æ•°æ®é›†ä¸Šçš„å›¾åƒåˆ†å‰²ä¸­è’¸é¦äºŒç»´å’Œä¸‰ç»´éƒ¨ä»¶æè®®ï¼Œé€šè¿‡å¯¹æ¯”å­¦ä¹ å…¬å¼è¿›è¡Œè®­ç»ƒã€‚å®ƒäº§ç”Ÿäº†ä¸€ä¸ªè¿ç»­çš„ç‰¹å¾åœºï¼Œå¯ä»¥å¯¹å…¶è¿›è¡Œèšç±»ä»¥äº§ç”Ÿå±‚æ¬¡åŒ–çš„éƒ¨ä»¶åˆ†è§£ã€‚æ¯”è¾ƒæ˜¾ç¤ºï¼ŒPartFieldçš„å‡†ç¡®åº¦é«˜è¾¾å…¶ä»–æœ€è¿‘çš„ç±»åˆ«æ— å…³éƒ¨ä»¶åˆ†å‰²æ–¹æ³•çš„20%ï¼Œå¹¶ä¸”é€šå¸¸å¿«å‡ ä¸ªæ•°é‡çº§ã€‚é™¤äº†å•ä¸ªå½¢çŠ¶çš„éƒ¨ä»¶åˆ†è§£ä¹‹å¤–ï¼Œå­¦ä¹ å­—æ®µä¹‹é—´çš„ä¸€è‡´æ€§è¿˜ä½“ç°åœ¨å„ç§å½¢çŠ¶ä¹‹é—´ï¼Œä½¿å…±åˆ†å‰²å’Œå¯¹åº”å…³ç³»ç­‰ä»»åŠ¡æˆä¸ºå¯èƒ½ï¼Œæˆ‘ä»¬åœ¨è¿™äº›é€šç”¨ã€å±‚æ¬¡åŒ–å’Œä¸€è‡´çš„ä¸‰ç»´ç‰¹å¾å­—æ®µçš„å¤šä¸ªåº”ç”¨ä¸­å±•ç¤ºäº†è¿™ä¸€ç‚¹ã€‚è¯·è®¿é—®æˆ‘ä»¬çš„ç½‘é¡µäº†è§£è¯¦æƒ…ï¼<a target="_blank" rel="noopener" href="https://research.nvidia.com/labs/toronto-ai/partfield-release/">https://research.nvidia.com/labs/toronto-ai/partfield-release/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11451v1">PDF</a> <a target="_blank" rel="noopener" href="https://research.nvidia.com/labs/toronto-ai/partfield-release/">https://research.nvidia.com/labs/toronto-ai/partfield-release/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†PartFieldæ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå‰é¦ˆçš„å­¦ä¹ éƒ¨åˆ†ä¸‰ç»´ç‰¹å¾çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæ•æ‰éƒ¨åˆ†çš„ä¸€èˆ¬æ¦‚å¿µå’Œå®ƒä»¬çš„å±‚æ¬¡ç»“æ„ï¼Œæ— éœ€ä¾èµ–é¢„å…ˆå®šä¹‰çš„æ¨¡æ¿æˆ–åŸºäºæ–‡æœ¬çš„åç§°ï¼Œå¹¶å¯åº”ç”¨äºå„ç§æ¨¡æ€çš„å¼€æ”¾ä¸–ç•Œä¸‰ç»´å½¢çŠ¶ã€‚PartFieldåœ¨æ¨ç†æ—¶é—´ä»…éœ€è¿›è¡Œä¸‰ç»´å‰é¦ˆä¼ é€’ï¼Œä¸å…ˆå‰çš„æ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—æé«˜äº†è¿è¡Œæ—¶é—´å’Œç¨³å¥æ€§ã€‚è¯¥æ¨¡å‹é€šè¿‡è’¸é¦æ¥è‡ªæ··åˆæ ‡è®°æ•°æ®é›†å’Œå›¾åƒåˆ†å‰²çš„å¤§å‹æ— ç›‘ç£æ•°æ®é›†çš„äºŒç»´å’Œä¸‰ç»´éƒ¨åˆ†å»ºè®®ï¼Œé€šè¿‡å¯¹æ¯”å­¦ä¹ å…¬å¼è¿›è¡Œè®­ç»ƒã€‚å®ƒäº§ç”Ÿäº†ä¸€ä¸ªè¿ç»­çš„ç‰¹å¾åœºï¼Œå¯ä»¥å¯¹å…¶è¿›è¡Œèšç±»ä»¥è·å¾—å±‚æ¬¡åŒ–çš„éƒ¨åˆ†åˆ†è§£ã€‚ä¸å…¶ä»–æœ€æ–°çš„ç±»æ— å…³çš„éƒ¨åˆ†åˆ†å‰²æ–¹æ³•ç›¸æ¯”ï¼ŒPartFieldçš„å‡†ç¡®åº¦é«˜è¾¾20%ï¼Œå¹¶ä¸”åœ¨è®¸å¤šæƒ…å†µä¸‹ï¼Œé€Ÿåº¦æ¯”å…¶ä»–æ–¹æ³•å¿«å‡ ä¸ªæ•°é‡çº§ã€‚æ­¤å¤–ï¼Œå­¦ä¹ åˆ°çš„åœºçš„ä¸€è‡´æ€§è¶…è¶Šäº†å•ä¸€å½¢çŠ¶çš„éƒ¨åˆ†åˆ†è§£ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿåœ¨å½¢çŠ¶ä¹‹é—´è¿›è¡Œå…±åŒåˆ†å‰²å’Œå¯¹åº”ï¼Œæˆ‘ä»¬åœ¨å‡ ä¸ªåº”ç”¨ä¸­éƒ½è¯æ˜äº†è¿™ä¸€ç‚¹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PartFieldæ˜¯ä¸€ç§åŸºäºå‰é¦ˆçš„å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºå­¦ä¹ éƒ¨åˆ†ä¸‰ç»´ç‰¹å¾ã€‚</li>
<li>å®ƒèƒ½å¤Ÿæ•æ‰éƒ¨åˆ†çš„ä¸€èˆ¬æ¦‚å¿µå’Œå±‚æ¬¡ç»“æ„ï¼Œæ— éœ€ä¾èµ–é¢„å…ˆå®šä¹‰çš„æ¨¡æ¿æˆ–åŸºäºæ–‡æœ¬çš„åç§°ã€‚</li>
<li>PartFieldé€‚ç”¨äºå„ç§æ¨¡æ€çš„å¼€æ”¾ä¸–ç•Œä¸‰ç»´å½¢çŠ¶ã€‚</li>
<li>PartFieldåœ¨æ¨ç†æ—¶é—´ä»…éœ€è¿›è¡Œä¸‰ç»´å‰é¦ˆä¼ é€’ï¼Œæé«˜äº†è¿è¡Œæ—¶é—´å’Œç¨³å¥æ€§ã€‚</li>
<li>PartFieldé€šè¿‡æ··åˆæ ‡è®°æ•°æ®é›†å’Œå›¾åƒåˆ†å‰²çš„å¤§å‹æ— ç›‘ç£æ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚</li>
<li>å®ƒäº§ç”Ÿäº†ä¸€ä¸ªè¿ç»­çš„ç‰¹å¾åœºï¼Œå¯ä»¥å¯¹å…¶è¿›è¡Œèšç±»ä»¥è·å¾—å±‚æ¬¡åŒ–çš„éƒ¨åˆ†åˆ†è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11451">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-17\./crop_åŒ»å­¦å›¾åƒ/2504.11451v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-17\./crop_åŒ»å­¦å›¾åƒ/2504.11451v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-17\./crop_åŒ»å­¦å›¾åƒ/2504.11451v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-17\./crop_åŒ»å­¦å›¾åƒ/2504.11451v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-17\./crop_åŒ»å­¦å›¾åƒ/2504.11451v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="From-Gaze-to-Insight-Bridging-Human-Visual-Attention-and-Vision-Language-Model-Explanation-for-Weakly-Supervised-Medical-Image-Segmentation"><a href="#From-Gaze-to-Insight-Bridging-Human-Visual-Attention-and-Vision-Language-Model-Explanation-for-Weakly-Supervised-Medical-Image-Segmentation" class="headerlink" title="From Gaze to Insight: Bridging Human Visual Attention and Vision   Language Model Explanation for Weakly-Supervised Medical Image Segmentation"></a>From Gaze to Insight: Bridging Human Visual Attention and Vision   Language Model Explanation for Weakly-Supervised Medical Image Segmentation</h2><p><strong>Authors:Jingkun Chen, Haoran Duan, Xiao Zhang, Boyan Gao, Tao Tan, Vicente Grau, Jungong Han</strong></p>
<p>Medical image segmentation remains challenging due to the high cost of pixel-level annotations for training. In the context of weak supervision, clinician gaze data captures regions of diagnostic interest; however, its sparsity limits its use for segmentation. In contrast, vision-language models (VLMs) provide semantic context through textual descriptions but lack the explanation precision required. Recognizing that neither source alone suffices, we propose a teacher-student framework that integrates both gaze and language supervision, leveraging their complementary strengths. Our key insight is that gaze data indicates where clinicians focus during diagnosis, while VLMs explain why those regions are significant. To implement this, the teacher model first learns from gaze points enhanced by VLM-generated descriptions of lesion morphology, establishing a foundation for guiding the student model. The teacher then directs the student through three strategies: (1) Multi-scale feature alignment to fuse visual cues with textual semantics; (2) Confidence-weighted consistency constraints to focus on reliable predictions; (3) Adaptive masking to limit error propagation in uncertain areas. Experiments on the Kvasir-SEG, NCI-ISBI, and ISIC datasets show that our method achieves Dice scores of 80.78%, 80.53%, and 84.22%, respectively-improving 3-5% over gaze baselines without increasing the annotation burden. By preserving correlations among predictions, gaze data, and lesion descriptions, our framework also maintains clinical interpretability. This work illustrates how integrating human visual attention with AI-generated semantic context can effectively overcome the limitations of individual weak supervision signals, thereby advancing the development of deployable, annotation-efficient medical AI systems. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/jingkunchen/FGI.git">https://github.com/jingkunchen/FGI.git</a>. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå› ä¸ºåƒç´ çº§æ ‡æ³¨çš„è®­ç»ƒæˆæœ¬å¾ˆé«˜ã€‚åœ¨å¼±ç›‘ç£çš„èƒŒæ™¯ä¸‹ï¼ŒåŒ»ç”Ÿè§†çº¿æ•°æ®èƒ½å¤Ÿæ•æ‰è¯Šæ–­æ—¶çš„æ„Ÿå…´è¶£åŒºåŸŸï¼Œä½†å…¶ç¨€ç–æ€§é™åˆ¶äº†å…¶åœ¨åˆ†å‰²ä¸­çš„åº”ç”¨ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰é€šè¿‡æ–‡æœ¬æè¿°æä¾›è¯­ä¹‰ä¸Šä¸‹æ–‡ï¼Œä½†ç¼ºä¹å¿…è¦çš„è§£é‡Šç²¾åº¦ã€‚æˆ‘ä»¬è®¤è¯†åˆ°ï¼Œå•ä¸€æ¥æºçš„ä¿¡æ¯ä¸è¶³ä»¥æ»¡è¶³éœ€æ±‚ï¼Œå› æ­¤æå‡ºäº†ä¸€ç§èåˆè§†çº¿å’Œè¯­è¨€ç›‘ç£çš„æ•™å¸ˆ-å­¦ç”Ÿæ¡†æ¶ï¼Œä»¥å‘æŒ¥äºŒè€…çš„äº’è¡¥ä¼˜åŠ¿ã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯ï¼Œè§†çº¿æ•°æ®å¯ä»¥æŒ‡ç¤ºåŒ»ç”Ÿåœ¨è¯Šæ–­æ—¶çš„å…³æ³¨ç„¦ç‚¹ï¼Œè€ŒVLMåˆ™è§£é‡Šè¿™äº›åŒºåŸŸçš„é‡è¦æ€§ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæ•™å¸ˆæ¨¡å‹é¦–å…ˆä»ç”±VLMç”Ÿæˆçš„ç—…å˜å½¢æ€æè¿°å¢å¼ºçš„è§†çº¿ç‚¹ä¸­å­¦ä¹ ï¼Œä¸ºå¼•å¯¼å­¦ç”Ÿæ¨¡å‹å¥ å®šåŸºç¡€ã€‚ç„¶åï¼Œæ•™å¸ˆé€šè¿‡ä¸‰ç§ç­–ç•¥å¼•å¯¼å­¦ç”Ÿï¼šï¼ˆ1ï¼‰å¤šå°ºåº¦ç‰¹å¾å¯¹é½ï¼Œèåˆè§†è§‰çº¿ç´¢å’Œæ–‡æœ¬è¯­ä¹‰ï¼›ï¼ˆ2ï¼‰åŸºäºç½®ä¿¡åº¦çš„ä¸€è‡´æ€§çº¦æŸï¼Œä¸“æ³¨äºå¯é é¢„æµ‹ï¼›ï¼ˆ3ï¼‰è‡ªé€‚åº”æ©ç ï¼Œä»¥é™åˆ¶ä¸ç¡®å®šåŒºåŸŸçš„è¯¯å·®ä¼ æ’­ã€‚åœ¨Kvasir-SEGã€NCI-ISBIå’ŒISICæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åˆ†åˆ«å®ç°äº†80.78%ã€80.53%å’Œ84.22%çš„Diceå¾—åˆ†ï¼Œåœ¨ä¸éœ€è¦å¢åŠ æ ‡æ³¨è´Ÿæ‹…çš„æƒ…å†µä¸‹æ¯”åŸºäºè§†çº¿çš„åŸºçº¿æé«˜äº†3-5%ã€‚é€šè¿‡ä¿æŒé¢„æµ‹ã€è§†çº¿æ•°æ®å’Œç—…å˜æè¿°ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œæˆ‘ä»¬çš„æ¡†æ¶è¿˜ä¿æŒäº†ä¸´åºŠå¯è§£é‡Šæ€§ã€‚è¿™é¡¹å·¥ä½œè¯´æ˜äº†å¦‚ä½•å°†äººç±»è§†è§‰æ³¨æ„åŠ›ä¸AIç”Ÿæˆçš„è¯­ä¹‰ä¸Šä¸‹æ–‡ç›¸ç»“åˆï¼Œä»è€Œæœ‰æ•ˆå…‹æœå•ä¸ªå¼±ç›‘ç£ä¿¡å·çš„å±€é™æ€§ï¼Œæ¨åŠ¨å¯éƒ¨ç½²ã€æ ‡æ³¨æ•ˆç‡é«˜çš„åŒ»ç–—AIç³»ç»Ÿçš„å‘å±•ã€‚ä»£ç å¯ç”¨åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/jingkunchen/FGI.git%E3%80%82">https://github.com/jingkunchen/FGI.gitã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11368v1">PDF</a> 10 pages, 5 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æ¢è®¨åŒ»å­¦å›¾åƒåˆ†å‰²é¢ä¸´çš„æŒ‘æˆ˜ï¼Œä¸»è¦ç”±äºè®­ç»ƒæ—¶çš„åƒç´ çº§æ ‡æ³¨æˆæœ¬é«˜ã€‚ç ”ç©¶æå‡ºä¸€ç§èåˆåŒ»å¸ˆæ³¨è§†æ•°æ®å’Œè¯­è¨€ç›‘ç£çš„æ•™ä¸å­¦æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¼±ç›‘ç£ä¸‹æ³¨è§†æ•°æ®çš„ç¨€ç–æ€§å’Œè¯­è¨€æ¨¡å‹çš„è§£é‡Šç²¾åº¦ä¸è¶³çš„é—®é¢˜ã€‚é€šè¿‡ç»“åˆæ³¨è§†æ•°æ®åæ˜ åŒ»å¸ˆè¯Šæ–­æ—¶çš„å…³æ³¨ç‚¹ï¼Œä»¥åŠè¯­è¨€æ¨¡å‹æä¾›çš„è¯­ä¹‰ä¸Šä¸‹æ–‡ï¼Œæé«˜åŒ»å­¦å›¾åƒåˆ†å‰²çš„ç²¾åº¦å’Œä¸´åºŠå¯è§£é‡Šæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨Kvasir-SEGã€NCI-ISBIå’ŒISICæ•°æ®é›†ä¸Šåˆ†åˆ«å®ç°äº†80.78%ã€80.53%å’Œ84.22%çš„Diceå¾—åˆ†ï¼Œè¾ƒä»…ä½¿ç”¨æ³¨è§†æ•°æ®çš„æ–¹æ³•æé«˜äº†3-5%ï¼Œä¸”æœªå¢åŠ æ ‡æ³¨è´Ÿæ‹…ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²é¢ä¸´é«˜æˆæœ¬æ ‡æ³¨çš„æŒ‘æˆ˜ï¼Œå¼±ç›‘ç£ä¸‹æ³¨è§†æ•°æ®å’Œè¯­è¨€æ¨¡å‹çš„äº’è¡¥æ€§æˆä¸ºç ”ç©¶ç„¦ç‚¹ã€‚</li>
<li>åŒ»å¸ˆæ³¨è§†æ•°æ®åæ˜ è¯Šæ–­å…³æ³¨ç‚¹ï¼Œä½†ç¨€ç–æ€§é™åˆ¶äº†å…¶åœ¨åˆ†å‰²ä¸­çš„åº”ç”¨ã€‚</li>
<li>è¯­è¨€æ¨¡å‹æä¾›è¯­ä¹‰ä¸Šä¸‹æ–‡ï¼Œä½†ç¼ºä¹è§£é‡Šç²¾ç¡®åº¦ã€‚</li>
<li>æå‡ºç»“åˆæ³¨è§†æ•°æ®å’Œè¯­è¨€ç›‘ç£çš„æ•™ä¸å­¦æ¡†æ¶ï¼Œæ•´åˆä¸¤è€…ä¼˜åŠ¿ã€‚</li>
<li>æ•™å¸ˆæ¨¡å‹é€šè¿‡èåˆæ³¨è§†ç‚¹å’Œç”±è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„ç—…å˜å½¢æ€æè¿°æ¥å­¦ä¹ ï¼Œä¸ºå­¦ç”Ÿæ¨¡å‹æä¾›æŒ‡å¯¼ã€‚</li>
<li>å®æ–½å¤šå°ºåº¦ç‰¹å¾å¯¹é½ã€ç½®ä¿¡åº¦åŠ æƒä¸€è‡´æ€§çº¦æŸå’Œè‡ªé€‚åº”æ©ç ç­‰ç­–ç•¥ï¼Œæé«˜åˆ†å‰²ç²¾åº¦å’Œä¸´åºŠå¯è§£é‡Šæ€§ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å®ç°äº†è¾ƒé«˜çš„Diceå¾—åˆ†ï¼Œå¹¶ä¿æŒäº†ä¸´åºŠå¯è§£é‡Šæ€§ï¼Œä¸ºéƒ¨ç½²é«˜æ•ˆçš„åŒ»å­¦äººå·¥æ™ºèƒ½ç³»ç»Ÿæä¾›äº†æ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11368">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-17\./crop_åŒ»å­¦å›¾åƒ/2504.11368v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-17\./crop_åŒ»å­¦å›¾åƒ/2504.11368v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-17\./crop_åŒ»å­¦å›¾åƒ/2504.11368v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Explicit-and-Implicit-Representations-in-AI-based-3D-Reconstruction-for-Radiology-A-systematic-literature-review"><a href="#Explicit-and-Implicit-Representations-in-AI-based-3D-Reconstruction-for-Radiology-A-systematic-literature-review" class="headerlink" title="Explicit and Implicit Representations in AI-based 3D Reconstruction for   Radiology: A systematic literature review"></a>Explicit and Implicit Representations in AI-based 3D Reconstruction for   Radiology: A systematic literature review</h2><p><strong>Authors:Yuezhe Yang, Boyu Yang, Yaqian Wang, Yang He, Xingbo Dong, Zhe Jin</strong></p>
<p>The demand for high-quality medical imaging in clinical practice and assisted diagnosis has made 3D reconstruction in radiological imaging a key research focus. Artificial intelligence (AI) has emerged as a promising approach to enhancing reconstruction accuracy while reducing acquisition and processing time, thereby minimizing patient radiation exposure and discomfort and ultimately benefiting clinical diagnosis. This review explores state-of-the-art AI-based 3D reconstruction algorithms in radiological imaging, categorizing them into explicit and implicit approaches based on their underlying principles. Explicit methods include point-based, volume-based, and Gaussian representations, while implicit methods encompass implicit prior embedding and neural radiance fields. Additionally, we examine commonly used evaluation metrics and benchmark datasets. Finally, we discuss the current state of development, key challenges, and future research directions in this evolving field. Our project available on: <a target="_blank" rel="noopener" href="https://github.com/Bean-Young/AI4Med">https://github.com/Bean-Young/AI4Med</a>. </p>
<blockquote>
<p>åœ¨ä¸´åºŠå®è·µå’Œè¾…åŠ©è¯Šæ–­ä¸­å¯¹é«˜è´¨é‡åŒ»å­¦æˆåƒçš„éœ€æ±‚ä½¿å¾—æ”¾å°„å­¦æˆåƒä¸­çš„3Dé‡å»ºæˆä¸ºå…³é”®çš„ç ”ç©¶ç„¦ç‚¹ã€‚äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰å·²æˆä¸ºä¸€ç§æœ‰å‰é€”çš„æ–¹æ³•ï¼Œå¯ä»¥æé«˜é‡å»ºç²¾åº¦ï¼ŒåŒæ—¶å‡å°‘é‡‡é›†å’Œå¤„ç†æ—¶é—´ï¼Œä»è€Œæœ€å°åŒ–æ‚£è€…çš„è¾å°„æš´éœ²å’Œä¸é€‚æ„Ÿï¼Œå¹¶æœ€ç»ˆæœ‰ç›Šäºä¸´åºŠè¯Šæ–­ã€‚æœ¬æ–‡ç»¼è¿°äº†åŸºäºäººå·¥æ™ºèƒ½çš„æ”¾å°„å­¦æˆåƒä¸­3Dé‡å»ºç®—æ³•çš„æœ€æ–°è¿›å±•ï¼Œæ ¹æ®å®ƒä»¬çš„åŸºæœ¬åŸç†å°†å®ƒä»¬åˆ†ä¸ºæ˜¾å¼æ–¹æ³•å’Œéšå¼æ–¹æ³•ã€‚æ˜¾å¼æ–¹æ³•åŒ…æ‹¬ç‚¹åŸºã€ä½“ç§¯åŸºå’Œé«˜æ–¯è¡¨ç¤ºæ³•ï¼Œè€Œéšå¼æ–¹æ³•åŒ…æ‹¬éšå¼å…ˆéªŒåµŒå…¥å’Œç¥ç»è¾å°„åœºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä»‹ç»äº†å¸¸ç”¨çš„è¯„ä¼°æŒ‡æ ‡å’ŒåŸºå‡†æ•°æ®é›†ã€‚æœ€åï¼Œæˆ‘ä»¬è®¨è®ºäº†è¯¥é¢†åŸŸçš„å½“å‰å‘å±•çŠ¶æ€ã€å…³é”®æŒ‘æˆ˜å’Œæœªæ¥ç ”ç©¶æ–¹å‘ã€‚æˆ‘ä»¬çš„é¡¹ç›®å¯é€šè¿‡ä»¥ä¸‹ç½‘å€è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/Bean-Young/AI4Med">https://github.com/Bean-Young/AI4Med</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11349v1">PDF</a> 43 pages, 5 figures, submit to Medical Image Analysis</p>
<p><strong>Summary</strong></p>
<p>äººå·¥æ™ºèƒ½åœ¨åŒ»å­¦å½±åƒå­¦ä¸‰ç»´é‡å»ºä¸­çš„åº”ç”¨å‰æ™¯å¹¿é˜”ã€‚é€šè¿‡æé«˜é‡å»ºç²¾åº¦ã€ç¼©çŸ­é‡‡é›†å’Œå¤„ç†æ—¶é—´ï¼ŒAIèƒ½å¤Ÿå‡å°‘æ‚£è€…è¾å°„æš´éœ²å’Œä¸é€‚æ„Ÿï¼Œä¸ºä¸´åºŠè¯Šæ–­å¸¦æ¥å¥½å¤„ã€‚æœ¬æ–‡ç»¼è¿°äº†æœ€æ–°çš„AIä¸‰ç»´é‡å»ºç®—æ³•ï¼Œåˆ†ä¸ºæ˜¾å¼æ–¹æ³•å’Œéšå¼æ–¹æ³•ä¸¤å¤§ç±»ï¼Œå¹¶æ¢è®¨äº†å¸¸ç”¨çš„è¯„ä¼°æŒ‡æ ‡å’ŒåŸºå‡†æ•°æ®é›†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å½±åƒå­¦ä¸­å¯¹é«˜è´¨é‡ä¸‰ç»´é‡å»ºçš„éœ€æ±‚ä¿ƒä½¿AIåœ¨è¯¥é¢†åŸŸçš„ç ”ç©¶æˆä¸ºé‡ç‚¹ã€‚</li>
<li>AIèƒ½å¤Ÿæé«˜ä¸‰ç»´é‡å»ºçš„ç²¾åº¦ï¼ŒåŒæ—¶ç¼©çŸ­é‡‡é›†å’Œå¤„ç†æ—¶é—´ã€‚</li>
<li>AIåœ¨åŒ»å­¦å½±åƒå­¦ä¸‰ç»´é‡å»ºä¸­çš„åº”ç”¨èƒ½å¤Ÿå‡å°‘æ‚£è€…çš„è¾å°„æš´éœ²å’Œä¸é€‚æ„Ÿã€‚</li>
<li>å½“å‰çš„AIä¸‰ç»´é‡å»ºç®—æ³•å¯åˆ†ä¸ºæ˜¾å¼æ–¹æ³•å’Œéšå¼æ–¹æ³•ï¼Œå…¶ä¸­æ˜¾å¼æ–¹æ³•åŒ…æ‹¬ç‚¹åŸºã€ä½“ç§¯åŸºå’Œé«˜æ–¯è¡¨ç¤ºæ³•ï¼Œéšå¼æ–¹æ³•åŒ…æ‹¬éšå¼å…ˆéªŒåµŒå…¥å’Œç¥ç»è¾å°„åœºã€‚</li>
<li>åœ¨è¯„ä¼°AIä¸‰ç»´é‡å»ºç®—æ³•æ—¶ï¼Œå¸¸ç”¨çš„è¯„ä¼°æŒ‡æ ‡å’ŒåŸºå‡†æ•°æ®é›†ä¹Ÿå¾ˆé‡è¦ã€‚</li>
<li>å½“å‰ï¼ŒAIåœ¨åŒ»å­¦å½±åƒå­¦ä¸‰ç»´é‡å»ºé¢†åŸŸä»é¢ä¸´ä¸€äº›æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11349">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-17\./crop_åŒ»å­¦å›¾åƒ/2504.11349v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-17\./crop_åŒ»å­¦å›¾åƒ/2504.11349v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-17\./crop_åŒ»å­¦å›¾åƒ/2504.11349v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Uncertainty-Estimation-for-Trust-Attribution-to-Speed-of-Sound-Reconstruction-with-Variational-Networks"><a href="#Uncertainty-Estimation-for-Trust-Attribution-to-Speed-of-Sound-Reconstruction-with-Variational-Networks" class="headerlink" title="Uncertainty Estimation for Trust Attribution to Speed-of-Sound   Reconstruction with Variational Networks"></a>Uncertainty Estimation for Trust Attribution to Speed-of-Sound   Reconstruction with Variational Networks</h2><p><strong>Authors:Sonia Laguna, Lin Zhang, Can Deniz Bezek, Monika Farkas, Dieter Schweizer, Rahel A. Kubik-Huch, Orcun Goksel</strong></p>
<p>Speed-of-sound (SoS) is a biomechanical characteristic of tissue, and its imaging can provide a promising biomarker for diagnosis. Reconstructing SoS images from ultrasound acquisitions can be cast as a limited-angle computed-tomography problem, with Variational Networks being a promising model-based deep learning solution. Some acquired data frames may, however, get corrupted by noise due to, e.g., motion, lack of contact, and acoustic shadows, which in turn negatively affects the resulting SoS reconstructions. We propose to use the uncertainty in SoS reconstructions to attribute trust to each individual acquired frame. Given multiple acquisitions, we then use an uncertainty based automatic selection among these retrospectively, to improve diagnostic decisions. We investigate uncertainty estimation based on Monte Carlo Dropout and Bayesian Variational Inference. We assess our automatic frame selection method for differential diagnosis of breast cancer, distinguishing between benign fibroadenoma and malignant carcinoma. We evaluate 21 lesions classified as BI-RADS~4, which represents suspicious cases for probable malignancy. The most trustworthy frame among four acquisitions of each lesion was identified using uncertainty based criteria. Selecting a frame informed by uncertainty achieved an area under curve of 76% and 80% for Monte Carlo Dropout and Bayesian Variational Inference, respectively, superior to any uncertainty-uninformed baselines with the best one achieving 64%. A novel use of uncertainty estimation is proposed for selecting one of multiple data acquisitions for further processing and decision making. </p>
<blockquote>
<p>å£°é€Ÿï¼ˆSoSï¼‰æ˜¯ç»„ç»‡çš„ç”Ÿç‰©åŠ›å­¦ç‰¹å¾ï¼Œå…¶æˆåƒä¸ºè¯Šæ–­æä¾›äº†ä¸€ä¸ªæœ‰å‰æ™¯çš„ç”Ÿç‰©æ ‡å¿—ç‰©ã€‚ä»è¶…å£°é‡‡é›†é‡å»ºå£°é€Ÿå›¾åƒå¯ä»¥çœ‹ä½œæ˜¯ä¸€ä¸ªæœ‰é™è§’åº¦çš„è®¡ç®—æœºæ–­å±‚æ‰«æé—®é¢˜ï¼ŒåŸºäºæ¨¡å‹çš„æ·±åº¦å­¦ä¹ æ–¹æ³•å˜åˆ†ç½‘ç»œæ˜¯ä¸€ä¸ªæœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œç”±äºè¿åŠ¨ã€æ¥è§¦ä¸è‰¯å’Œå£°éŸ³é˜´å½±ç­‰åŸå› ï¼ŒæŸäº›è·å–çš„æ•°æ®å¸§å¯èƒ½ä¼šè¢«å™ªå£°æŸåï¼Œè¿™åè¿‡æ¥åˆä¼šå¯¹æ‰€å¾—çš„å£°é€Ÿé‡å»ºç»“æœäº§ç”Ÿè´Ÿé¢å½±å“ã€‚æˆ‘ä»¬æè®®åˆ©ç”¨å£°é€Ÿé‡å»ºä¸­çš„ä¸ç¡®å®šæ€§æ¥ç»™æ¯ä¸ªå•ç‹¬è·å–çš„æ•°æ®å¸§èµ‹äºˆä¿¡ä»»åº¦ã€‚åœ¨å¤šæ¬¡é‡‡é›†çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬åˆ©ç”¨åŸºäºä¸ç¡®å®šæ€§çš„è‡ªåŠ¨é€‰æ‹©æ–¹æ³•ï¼Œå¯¹å®ƒä»¬è¿›è¡Œå›é¡¾æ€§é€‰æ‹©ï¼Œä»¥æ”¹è¿›è¯Šæ–­å†³ç­–ã€‚æˆ‘ä»¬ç ”ç©¶äº†åŸºäºè’™ç‰¹å¡æ´›åˆ é™¤å’Œè´å¶æ–¯å˜åˆ†æ¨æ–­çš„ä¸ç¡®å®šæ€§ä¼°è®¡ã€‚æˆ‘ä»¬è¯„ä¼°äº†æˆ‘ä»¬çš„è‡ªåŠ¨æ•°æ®å¸§é€‰æ‹©æ–¹æ³•ï¼Œç”¨äºä¹³è…ºç™Œçš„é‰´åˆ«è¯Šæ–­ï¼ŒåŒºåˆ†è‰¯æ€§çº¤ç»´è…ºç˜¤å’Œæ¶æ€§ç™Œã€‚æˆ‘ä»¬è¯„ä¼°äº†21ä¸ªè¢«åˆ†ç±»ä¸ºBI-RADS 4çš„ç—…å˜ï¼Œè¿™ä»£è¡¨å¯ç–‘çš„æ¶æ€§ç—…ä¾‹ã€‚åˆ©ç”¨ä¸ç¡®å®šæ€§æ ‡å‡†ç¡®å®šäº†æ¯ä¸ªç—…å˜å››ä¸ªé‡‡é›†æ•°æ®ä¸­æœ€å¯ä¿¡èµ–çš„ä¸€å¸§ã€‚åŸºäºä¸ç¡®å®šæ€§çš„å¸§é€‰æ‹©è¾¾åˆ°äº†è’™ç‰¹å¡æ´›åˆ é™¤å’Œè´å¶æ–¯å˜åˆ†æ¨æ–­çš„æ›²çº¿ä¸‹é¢ç§¯åˆ†åˆ«ä¸º76%å’Œ80%ï¼Œä¼˜äºä»»ä½•ä¸è€ƒè™‘ä¸ç¡®å®šæ€§çš„åŸºçº¿ï¼Œå…¶ä¸­è¡¨ç°æœ€å¥½çš„åŸºçº¿è¾¾åˆ°64%ã€‚ä¸ç¡®å®šæ€§ä¼°è®¡çš„æ–°åº”ç”¨è¢«æå‡ºç”¨äºä»å¤šæ¬¡æ•°æ®é‡‡é›†ä¸­é€‰æ‹©ä¸€å¸§è¿›è¡Œè¿›ä¸€æ­¥å¤„ç†å’Œå†³ç­–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11307v1">PDF</a> Published at the International Journal of Computer Assisted Radiology   and Surgery. Presented at the 16th International Conference on Information   Processing in Computer-Assisted Interventions 2025</p>
<p><strong>Summary</strong></p>
<p>è¶…å£°æˆåƒä¸­çš„å£°é€Ÿï¼ˆSoSï¼‰æˆåƒä½œä¸ºä¸€ç§æœ‰å‰æ™¯çš„ç”Ÿç‰©æ ‡å¿—ç‰©è¯Šæ–­æ–¹æ³•ï¼Œèƒ½å¤Ÿä»è¶…å£°æ³¢æ•°æ®ä¸­é‡å»ºSoSå›¾åƒã€‚ç„¶è€Œï¼Œç”±äºå™ªå£°å¹²æ‰°ã€è¿åŠ¨ã€æ¥è§¦ä¸è‰¯å’Œå£°å½±ç­‰åŸå› ï¼Œè·å–çš„æ•°æ®å¸§å¯èƒ½ä¼šå—åˆ°ç ´åï¼Œå½±å“SoSé‡å»ºç»“æœã€‚æœ¬ç ”ç©¶åˆ©ç”¨SoSé‡å»ºä¸­çš„ä¸ç¡®å®šæ€§æ¥è¯„ä¼°æ¯ä¸ªè·å–çš„æ•°æ®å¸§çš„å¯ä¿¡åº¦ï¼Œå¹¶åŸºäºä¸ç¡®å®šæ€§å¯¹å¤šæ¬¡é‡‡é›†çš„æ•°æ®è¿›è¡Œè‡ªåŠ¨ç­›é€‰ï¼Œä»¥æé«˜è¯Šæ–­å†³ç­–çš„å¯é æ€§ã€‚æœ¬ç ”ç©¶é‡‡ç”¨è’™ç‰¹å¡æ´›Dropoutå’Œè´å¶æ–¯å˜åˆ†æ¨æ–­è¿›è¡Œä¸ç¡®å®šæ€§è¯„ä¼°ï¼Œå¹¶å¯¹ä¹³è…ºç—…å˜è¿›è¡Œé‰´åˆ«è¯Šæ–­ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒåŸºäºä¸ç¡®å®šæ€§çš„è‡ªåŠ¨ç­›é€‰èƒ½å¤Ÿæœ‰æ•ˆåŒºåˆ†è‰¯æ€§çº¤ç»´è…ºç˜¤å’Œæ¶æ€§ç™Œã€‚åœ¨BI-RADS4åˆ†ç±»çš„ç–‘ä¼¼æ¶æ€§ç—…å˜ä¸­ï¼Œé€šè¿‡ä¸ç¡®å®šæ€§æ ‡å‡†ç­›é€‰æœ€å¯é çš„æ•°æ®å¸§ï¼Œä½¿ç”¨è’™ç‰¹å¡æ´›Dropoutå’Œè´å¶æ–¯å˜åˆ†æ¨æ–­çš„AUCåˆ†åˆ«è¾¾åˆ°äº†76%å’Œ80%ï¼Œä¼˜äºæœªè€ƒè™‘ä¸ç¡®å®šæ€§çš„åŸºçº¿æ–¹æ³•ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§æ–°å‹çš„ä¸ç¡®å®šæ€§è¯„ä¼°æ–¹æ³•ï¼Œç”¨äºä»å¤šæ¬¡æ•°æ®é‡‡é›†ä¸­é€‰æ‹©ä¸€ä¸ªç”¨äºè¿›ä¸€æ­¥å¤„ç†å’Œå†³ç­–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å£°é€Ÿï¼ˆSoSï¼‰æˆåƒä½œä¸ºè¯Šæ–­çš„ç”Ÿç‰©æ ‡å¿—ç‰©å…·æœ‰æ½œåŠ›ã€‚</li>
<li>æ•°æ®å¸§å¯èƒ½å› å™ªå£°ã€è¿åŠ¨ç­‰å› ç´ è€Œå—ç ´åï¼Œå½±å“SoSé‡å»ºã€‚</li>
<li>åˆ©ç”¨ä¸ç¡®å®šæ€§è¯„ä¼°æ•°æ®å¸§å¯ä¿¡åº¦ï¼Œå¯¹å¤šæ¬¡é‡‡é›†çš„æ•°æ®è¿›è¡Œè‡ªåŠ¨ç­›é€‰ã€‚</li>
<li>é‡‡ç”¨è’™ç‰¹å¡æ´›Dropoutå’Œè´å¶æ–¯å˜åˆ†æ¨æ–­è¿›è¡Œä¸ç¡®å®šæ€§è¯„ä¼°ã€‚</li>
<li>åœ¨ä¹³è…ºç—…å˜é‰´åˆ«è¯Šæ–­ä¸­ï¼ŒåŸºäºä¸ç¡®å®šæ€§çš„è‡ªåŠ¨ç­›é€‰æ•ˆæœè‰¯å¥½ã€‚</li>
<li>æœ€ä½³æ–¹æ³•å®ç°è¾ƒé«˜AUCï¼Œä¼˜äºä¸è€ƒè™‘ä¸ç¡®å®šæ€§çš„åŸºçº¿æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11307">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-17\./crop_åŒ»å­¦å›¾åƒ/2504.11307v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-17\./crop_åŒ»å­¦å›¾åƒ/2504.11307v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-17\./crop_åŒ»å­¦å›¾åƒ/2504.11307v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-17\./crop_åŒ»å­¦å›¾åƒ/2504.11307v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Efficient-Medical-Image-Restoration-via-Reliability-Guided-Learning-in-Frequency-Domain"><a href="#Efficient-Medical-Image-Restoration-via-Reliability-Guided-Learning-in-Frequency-Domain" class="headerlink" title="Efficient Medical Image Restoration via Reliability Guided Learning in   Frequency Domain"></a>Efficient Medical Image Restoration via Reliability Guided Learning in   Frequency Domain</h2><p><strong>Authors:Pengcheng Zheng, Kecheng Chen, Jiaxin Huang, Bohao Chen, Ju Liu, Yazhou Ren, Xiaorong Pu</strong></p>
<p>Medical image restoration tasks aim to recover high-quality images from degraded observations, exhibiting emergent desires in many clinical scenarios, such as low-dose CT image denoising, MRI super-resolution, and MRI artifact removal. Despite the success achieved by existing deep learning-based restoration methods with sophisticated modules, they struggle with rendering computationally-efficient reconstruction results. Moreover, they usually ignore the reliability of the restoration results, which is much more urgent in medical systems. To alleviate these issues, we present LRformer, a Lightweight Transformer-based method via Reliability-guided learning in the frequency domain. Specifically, inspired by the uncertainty quantification in Bayesian neural networks (BNNs), we develop a Reliable Lesion-Semantic Prior Producer (RLPP). RLPP leverages Monte Carlo (MC) estimators with stochastic sampling operations to generate sufficiently-reliable priors by performing multiple inferences on the foundational medical image segmentation model, MedSAM. Additionally, instead of directly incorporating the priors in the spatial domain, we decompose the cross-attention (CA) mechanism into real symmetric and imaginary anti-symmetric parts via fast Fourier transform (FFT), resulting in the design of the Guided Frequency Cross-Attention (GFCA) solver. By leveraging the conjugated symmetric property of FFT, GFCA reduces the computational complexity of naive CA by nearly half. Extensive experimental results in various tasks demonstrate the superiority of the proposed LRformer in both effectiveness and efficiency. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒæ¢å¤ä»»åŠ¡æ—¨åœ¨ä»é€€åŒ–çš„è§‚å¯Ÿä¸­æ¢å¤é«˜è´¨é‡å›¾åƒï¼Œè¿™åœ¨è®¸å¤šä¸´åºŠåœºæ™¯ä¸­è¡¨ç°å‡ºäº†è¿«åˆ‡çš„éœ€æ±‚ï¼Œä¾‹å¦‚ä½å‰‚é‡CTå›¾åƒå»å™ªã€MRIè¶…åˆ†è¾¨ç‡å’ŒMRIä¼ªå½±å»é™¤ã€‚å°½ç®¡ç°æœ‰çš„åŸºäºæ·±åº¦å­¦ä¹ çš„æ¢å¤æ–¹æ³•ä½¿ç”¨å¤æ‚çš„æ¨¡å—å–å¾—äº†æˆåŠŸï¼Œä½†å®ƒä»¬éš¾ä»¥ç”Ÿæˆè®¡ç®—æ•ˆç‡é«˜çš„é‡å»ºç»“æœã€‚æ­¤å¤–ï¼Œå®ƒä»¬é€šå¸¸å¿½ç•¥äº†åŒ»ç–—ç³»ç»Ÿä¸­æ¢å¤ç»“æœçš„å¯é æ€§ï¼Œè¿™ä¸€ç‚¹æ›´åŠ è¿«åˆ‡ã€‚ä¸ºäº†ç¼“è§£è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†LRformerï¼Œè¿™æ˜¯ä¸€ç§åŸºäºè½»é‡çº§Transformerçš„å¯é æ€§å¼•å¯¼é¢‘ç‡åŸŸå­¦ä¹ æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œå—åˆ°è´å¶æ–¯ç¥ç»ç½‘ç»œï¼ˆBNNsï¼‰ä¸­çš„ä¸ç¡®å®šæ€§é‡åŒ–çš„å¯å‘ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§å¯é çš„ç—…ç¶è¯­ä¹‰å…ˆéªŒç”Ÿäº§è€…ï¼ˆRLPPï¼‰ã€‚RLPPåˆ©ç”¨è’™ç‰¹å¡æ´›ï¼ˆMCï¼‰ä¼°è®¡å™¨å’Œéšæœºé‡‡æ ·æ“ä½œï¼Œé€šè¿‡å¯¹åŸºç¡€åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹MedSAMè¿›è¡Œå¤šæ¬¡æ¨ç†ï¼Œç”Ÿæˆè¶³å¤Ÿå¯é çš„å…ˆéªŒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ²¡æœ‰ç›´æ¥åœ¨ç©ºé—´åŸŸä¸­èå…¥è¿™äº›å…ˆéªŒçŸ¥è¯†ï¼Œè€Œæ˜¯é€šè¿‡å¿«é€Ÿå‚…é‡Œå¶å˜æ¢ï¼ˆFFTï¼‰å°†äº¤å‰æ³¨æ„åŠ›ï¼ˆCAï¼‰æœºåˆ¶åˆ†è§£ä¸ºå®å¯¹ç§°å’Œè™šåå¯¹ç§°ä¸¤éƒ¨åˆ†ï¼Œä»è€Œè®¾è®¡å‡ºå¯¼å‘é¢‘ç‡äº¤å‰æ³¨æ„åŠ›ï¼ˆGFCAï¼‰æ±‚è§£å™¨ã€‚åˆ©ç”¨FFTçš„å…±è½­å¯¹ç§°å±æ€§ï¼ŒGFCAå°†åŸå§‹CAçš„è®¡ç®—å¤æ‚åº¦é™ä½äº†è¿‘ä¸€åŠã€‚åœ¨å¤šä¸ªä»»åŠ¡ä¸Šçš„å¤§é‡å®éªŒç»“æœè¯æ˜äº†LRformeråœ¨æœ‰æ•ˆæ€§å’Œæ•ˆç‡æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11286v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†é’ˆå¯¹åŒ»å­¦å›¾åƒæ¢å¤ä»»åŠ¡çš„ä¸€ç§æ–°å‹æ–¹æ³•â€”â€”LRformerã€‚è¯¥æ–¹æ³•ç»“åˆäº†è½»é‡çº§Transformerã€å¯é æ€§æŒ‡å¯¼å­¦ä¹ å’Œé¢‘åŸŸæŠ€æœ¯æ¥è§£å†³åŒ»å­¦å›¾åƒæ¢å¤é—®é¢˜ï¼Œå¹¶åœ¨å¤šä¸ªä»»åŠ¡ä¸­å±•ç¤ºäº†å…¶é«˜æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒæ¢å¤ä»»åŠ¡æ—¨åœ¨ä»é€€åŒ–çš„è§‚å¯Ÿä¸­æ¢å¤é«˜è´¨é‡å›¾åƒï¼Œåœ¨è®¸å¤šä¸´åºŠåœºæ™¯ä¸­å…·æœ‰é‡è¦ä½œç”¨ã€‚</li>
<li>ç°æœ‰æ·±åº¦å­¦ä¹ æ¢å¤æ–¹æ³•åœ¨å¤æ‚æ¨¡å—ä¸Šå–å¾—äº†æˆåŠŸï¼Œä½†åœ¨è®¡ç®—æ•ˆç‡æ–¹é¢å­˜åœ¨é—®é¢˜ï¼Œä¸”å¿½è§†äº†æ¢å¤ç»“æœçš„å¯é æ€§ã€‚</li>
<li>LRformeræ˜¯ä¸€ç§åŸºäºè½»é‡çº§Transformerçš„æ–¹æ³•ï¼Œé€šè¿‡å¯é æ€§æŒ‡å¯¼å­¦ä¹ æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>LRformerå—åˆ°è´å¶æ–¯ç¥ç»ç½‘ç»œä¸ç¡®å®šæ€§çš„å¯å‘ï¼Œå¼€å‘äº†å¯é çš„ç—…ç¶è¯­ä¹‰å…ˆéªŒç”Ÿæˆå™¨ï¼ˆRLPPï¼‰ã€‚</li>
<li>RLPPåˆ©ç”¨è’™ç‰¹å¡æ´›ä¼°è®¡å™¨å’Œéšæœºé‡‡æ ·æ“ä½œç”Ÿæˆå¯é çš„å…ˆéªŒï¼Œé€šè¿‡å¯¹åŸºç¡€åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹MedSAMè¿›è¡Œå¤šæ¬¡æ¨æ–­æ¥å®ç°ã€‚</li>
<li>LRformeré€šè¿‡å¿«é€Ÿå‚…é‡Œå¶å˜æ¢ï¼ˆFFTï¼‰å°†äº¤å‰æ³¨æ„æœºåˆ¶åˆ†è§£ä¸ºå®å¯¹ç§°å’Œè™šåå¯¹ç§°éƒ¨åˆ†ï¼Œè®¾è®¡å‡ºå¯¼å‘é¢‘ç‡äº¤å‰æ³¨æ„ï¼ˆGFCAï¼‰æ±‚è§£å™¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11286">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-17\./crop_åŒ»å­¦å›¾åƒ/2504.11286v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-17\./crop_åŒ»å­¦å›¾åƒ/2504.11286v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-17\./crop_åŒ»å­¦å›¾åƒ/2504.11286v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-17\./crop_åŒ»å­¦å›¾åƒ/2504.11286v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-17\./crop_åŒ»å­¦å›¾åƒ/2504.11286v1/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-17\./crop_åŒ»å­¦å›¾åƒ/2504.11286v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="PraNet-V2-Dual-Supervised-Reverse-Attention-for-Medical-Image-Segmentation"><a href="#PraNet-V2-Dual-Supervised-Reverse-Attention-for-Medical-Image-Segmentation" class="headerlink" title="PraNet-V2: Dual-Supervised Reverse Attention for Medical Image   Segmentation"></a>PraNet-V2: Dual-Supervised Reverse Attention for Medical Image   Segmentation</h2><p><strong>Authors:Bo-Cheng Hu, Ge-Peng Ji, Dian Shao, Deng-Ping Fan</strong></p>
<p>Accurate medical image segmentation is essential for effective diagnosis and treatment. Previously, PraNet-V1 was proposed to enhance polyp segmentation by introducing a reverse attention (RA) module that utilizes background information. However, PraNet-V1 struggles with multi-class segmentation tasks. To address this limitation, we propose PraNet-V2, which, compared to PraNet-V1, effectively performs a broader range of tasks including multi-class segmentation. At the core of PraNet-V2 is the Dual-Supervised Reverse Attention (DSRA) module, which incorporates explicit background supervision, independent background modeling, and semantically enriched attention fusion. Our PraNet-V2 framework demonstrates strong performance on four polyp segmentation datasets. Additionally, by integrating DSRA to iteratively enhance foreground segmentation results in three state-of-the-art semantic segmentation models, we achieve up to a 1.36% improvement in mean Dice score. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/ai4colonoscopy/PraNet-V2/tree/main/binary_seg/jittor">https://github.com/ai4colonoscopy/PraNet-V2/tree/main/binary_seg/jittor</a>. </p>
<blockquote>
<p>å‡†ç¡®çš„åŒ»å­¦å›¾åƒåˆ†å‰²å¯¹äºæœ‰æ•ˆçš„è¯Šæ–­å’Œæ²»ç–—è‡³å…³é‡è¦ã€‚æ­¤å‰ï¼Œæå‡ºäº†PraNet-V1ï¼Œé€šè¿‡å¼•å…¥åå‘æ³¨æ„ï¼ˆRAï¼‰æ¨¡å—åˆ©ç”¨èƒŒæ™¯ä¿¡æ¯æ¥å¢å¼ºæ¯è‚‰åˆ†å‰²ã€‚ç„¶è€Œï¼ŒPraNet-V1åœ¨å¤šç±»åˆ«åˆ†å‰²ä»»åŠ¡æ–¹é¢å­˜åœ¨å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†PraNet-V2ã€‚ä¸PraNet-V1ç›¸æ¯”ï¼ŒPraNet-V2æ›´æœ‰æ•ˆåœ°æ‰§è¡Œæ›´å¹¿æ³›çš„ä»»åŠ¡ï¼ŒåŒ…æ‹¬å¤šç±»åˆ«åˆ†å‰²ã€‚PraNet-V2çš„æ ¸å¿ƒæ˜¯åŒé‡ç›‘ç£åå‘æ³¨æ„ï¼ˆDSRAï¼‰æ¨¡å—ï¼Œå®ƒç»“åˆäº†æ˜ç¡®çš„èƒŒæ™¯ç›‘ç£ã€ç‹¬ç«‹çš„èƒŒæ™¯å»ºæ¨¡å’Œè¯­ä¹‰ä¸°å¯Œçš„æ³¨æ„åŠ›èåˆã€‚æˆ‘ä»¬çš„PraNet-V2æ¡†æ¶åœ¨å››ä¸ªæ¯è‚‰åˆ†å‰²æ•°æ®é›†ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œé€šè¿‡å°†DSRAé›†æˆåˆ°ä¸‰ä¸ªæœ€å…ˆè¿›çš„è¯­ä¹‰åˆ†å‰²æ¨¡å‹ä¸­ï¼Œä»¥è¿­ä»£æ–¹å¼æé«˜å‰æ™¯åˆ†å‰²ç»“æœï¼Œæˆ‘ä»¬å®ç°äº†å¹³å‡Diceå¾—åˆ†é«˜è¾¾1.36%çš„æ”¹è¿›ã€‚ä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/ai4colonoscopy/PraNet-V2/tree/main/binary_seg/jittor%E3%80%82">https://github.com/ai4colonoscopy/PraNet-V2/tree/main/binary_seg&#x2F;jittorã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10986v1">PDF</a> Technical report (4 tables 3 figures 8 pages)</p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒå‡†ç¡®åˆ†å‰²å¯¹äºæœ‰æ•ˆè¯Šæ–­å’Œæ²»ç–—è‡³å…³é‡è¦ã€‚é’ˆå¯¹PraNet-V1åœ¨å¤šç±»åˆ«åˆ†å‰²ä»»åŠ¡ä¸Šçš„å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†PraNet-V2ã€‚è¯¥æ¨¡å‹å¼•å…¥åŒé‡ç›‘ç£åå‘æ³¨æ„åŠ›ï¼ˆDSRAï¼‰æ¨¡å—ï¼Œå®ç°æ›´å¹¿æ³›çš„ä»»åŠ¡èŒƒå›´ï¼ŒåŒ…æ‹¬å¤šç±»åˆ«åˆ†å‰²ã€‚é€šè¿‡ç»“åˆDSRAæ¨¡å—è¿­ä»£å¢å¼ºå‰æ™¯åˆ†å‰²ç»“æœï¼ŒPraNet-V2åœ¨å››ä¸ªæ¯è‚‰åˆ†å‰²æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä¸”åœ¨ä¸‰ç§ä¸»æµè¯­ä¹‰åˆ†å‰²æ¨¡å‹ä¸­å®ç°äº†å¹³å‡Diceå¾—åˆ†çš„æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²å¯¹äºè¯Šæ–­å’Œæ²»ç–—çš„å‡†ç¡®æ€§è‡³å…³é‡è¦ã€‚</li>
<li>PraNet-V2æ˜¯å¯¹PraNet-V1çš„æ”¹è¿›ï¼Œèƒ½å¤Ÿå¤„ç†å¤šç±»åˆ«åˆ†å‰²ä»»åŠ¡ã€‚</li>
<li>DSRAæ¨¡å—æ˜¯PraNet-V2çš„æ ¸å¿ƒï¼Œå®ƒé€šè¿‡æ˜ç¡®çš„èƒŒæ™¯ç›‘ç£ã€ç‹¬ç«‹èƒŒæ™¯å»ºæ¨¡å’Œè¯­ä¹‰ä¸°å¯Œçš„æ³¨æ„åŠ›èåˆæ¥å®ç°æœ‰æ•ˆåˆ†å‰²ã€‚</li>
<li>PraNet-V2åœ¨å››ä¸ªæ¯è‚‰åˆ†å‰²æ•°æ®é›†ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚</li>
<li>é€šè¿‡ç»“åˆDSRAæ¨¡å—è¿­ä»£å¢å¼ºå‰æ™¯åˆ†å‰²ç»“æœï¼Œåœ¨ä¸‰ç§è¯­ä¹‰åˆ†å‰²æ¨¡å‹ä¸­å®ç°äº†æ€§èƒ½æå‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10986">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-17\./crop_åŒ»å­¦å›¾åƒ/2504.10986v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-17\./crop_åŒ»å­¦å›¾åƒ/2504.10986v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-17\./crop_åŒ»å­¦å›¾åƒ/2504.10986v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-17\./crop_åŒ»å­¦å›¾åƒ/2504.10986v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-17\./crop_åŒ»å­¦å›¾åƒ/2504.10986v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Embedding-Radiomics-into-Vision-Transformers-for-Multimodal-Medical-Image-Classification"><a href="#Embedding-Radiomics-into-Vision-Transformers-for-Multimodal-Medical-Image-Classification" class="headerlink" title="Embedding Radiomics into Vision Transformers for Multimodal Medical   Image Classification"></a>Embedding Radiomics into Vision Transformers for Multimodal Medical   Image Classification</h2><p><strong>Authors:Zhenyu Yang, Haiming Zhu, Rihui Zhang, Haipeng Zhang, Jianliang Wang, Chunhao Wang, Minbin Chen, Fang-Fang Yin</strong></p>
<p>Background: Deep learning has significantly advanced medical image analysis, with Vision Transformers (ViTs) offering a powerful alternative to convolutional models by modeling long-range dependencies through self-attention. However, ViTs are inherently data-intensive and lack domain-specific inductive biases, limiting their applicability in medical imaging. In contrast, radiomics provides interpretable, handcrafted descriptors of tissue heterogeneity but suffers from limited scalability and integration into end-to-end learning frameworks. In this work, we propose the Radiomics-Embedded Vision Transformer (RE-ViT) that combines radiomic features with data-driven visual embeddings within a ViT backbone.   Purpose: To develop a hybrid RE-ViT framework that integrates radiomics and patch-wise ViT embeddings through early fusion, enhancing robustness and performance in medical image classification.   Methods: Following the standard ViT pipeline, images were divided into patches. For each patch, handcrafted radiomic features were extracted and fused with linearly projected pixel embeddings. The fused representations were normalized, positionally encoded, and passed to the ViT encoder. A learnable [CLS] token aggregated patch-level information for classification. We evaluated RE-ViT on three public datasets (including BUSI, ChestXray2017, and Retinal OCT) using accuracy, macro AUC, sensitivity, and specificity. RE-ViT was benchmarked against CNN-based (VGG-16, ResNet) and hybrid (TransMed) models.   Results: RE-ViT achieved state-of-the-art results: on BUSI, AUC&#x3D;0.950+&#x2F;-0.011; on ChestXray2017, AUC&#x3D;0.989+&#x2F;-0.004; on Retinal OCT, AUC&#x3D;0.986+&#x2F;-0.001, which outperforms other comparison models.   Conclusions: The RE-ViT framework effectively integrates radiomics with ViT architectures, demonstrating improved performance and generalizability across multimodal medical image classification tasks. </p>
<blockquote>
<p>èƒŒæ™¯ï¼šæ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒåˆ†ææ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œè€ŒVision Transformersï¼ˆViTsï¼‰é€šè¿‡è‡ªæ³¨æ„åŠ›æœºåˆ¶å»ºæ¨¡é•¿è·ç¦»ä¾èµ–å…³ç³»ï¼Œä¸ºå·ç§¯æ¨¡å‹æä¾›äº†ä¸€ç§å¼ºå¤§çš„æ›¿ä»£æ–¹æ¡ˆã€‚ç„¶è€Œï¼ŒViTsæœ¬è´¨ä¸Šéœ€è¦å¤§é‡çš„æ•°æ®ï¼Œå¹¶ä¸”ç¼ºä¹ç‰¹å®šé¢†åŸŸçš„å½’çº³åè§ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨åŒ»å­¦æˆåƒä¸­çš„åº”ç”¨ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæ”¾å°„ç»„å­¦æä¾›äº†ç»„ç»‡å¼‚è´¨æ€§çš„å¯è§£é‡Šã€æ‰‹å·¥æè¿°ç¬¦ï¼Œä½†å—é™äºå¯æ‰©å±•æ€§å’Œæ•´åˆåˆ°ç«¯åˆ°ç«¯å­¦ä¹ æ¡†æ¶çš„èƒ½åŠ›ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Radiomics-Embedded Vision Transformerï¼ˆRE-ViTï¼‰ï¼Œå®ƒå°†æ”¾å°„ç»„å­¦ç‰¹å¾ä¸æ•°æ®é©±åŠ¨çš„è§†è§‰åµŒå…¥ç›¸ç»“åˆï¼Œåœ¨ä¸€ä¸ªViTä¸»å¹²ä¸­ã€‚ç›®çš„ï¼šå¼€å‘ä¸€ä¸ªæ··åˆRE-ViTæ¡†æ¶ï¼Œé€šè¿‡æ—©æœŸèåˆæ•´åˆæ”¾å°„ç»„å­¦å’Œè¡¥ä¸çº§ViTåµŒå…¥ï¼Œæé«˜åŒ»å­¦å›¾åƒåˆ†ç±»çš„ç¨³å¥æ€§å’Œæ€§èƒ½ã€‚æ–¹æ³•ï¼šéµå¾ªæ ‡å‡†çš„ViTç®¡é“ï¼Œå°†å›¾åƒåˆ†æˆè¡¥ä¸ã€‚å¯¹äºæ¯ä¸ªè¡¥ä¸ï¼Œæå–æ‰‹å·¥åˆ¶ä½œçš„æ”¾å°„ç»„å­¦ç‰¹å¾ï¼Œå¹¶ä¸çº¿æ€§æŠ•å½±çš„åƒç´ åµŒå…¥èåˆã€‚èåˆåçš„è¡¨ç¤ºå½¢å¼ç»è¿‡å½’ä¸€åŒ–ã€ä½ç½®ç¼–ç åï¼Œä¼ é€’ç»™ViTç¼–ç å™¨ã€‚ä¸€ä¸ªå¯å­¦ä¹ çš„[CLS]æ ‡è®°èšåˆè¡¥ä¸çº§åˆ«çš„ä¿¡æ¯ç”¨äºåˆ†ç±»ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªå…¬å…±æ•°æ®é›†ï¼ˆåŒ…æ‹¬BUSIã€ChestXray2017å’ŒRetinal OCTï¼‰ä¸Šè¯„ä¼°äº†RE-ViTçš„æ€§èƒ½ï¼Œä½¿ç”¨äº†å‡†ç¡®åº¦ã€å®AUCã€çµæ•åº¦å’Œç‰¹å¼‚åº¦ã€‚RE-ViTä¸åŸºäºCNNï¼ˆVGG-16ã€ResNetï¼‰å’Œæ··åˆï¼ˆTransMedï¼‰æ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒã€‚ç»“æœï¼šRE-ViTè¾¾åˆ°äº†æœ€æ–°æ°´å¹³çš„ç»“æœï¼šåœ¨BUSIä¸Šï¼ŒAUC&#x3D;0.950+&#x2F;-0.011ï¼›åœ¨ChestXray2017ä¸Šï¼ŒAUC&#x3D;0.989+&#x2F;-0.004ï¼›åœ¨è§†ç½‘è†œOCTä¸Šï¼ŒAUC&#x3D;0.986+&#x2F;-0.001ï¼Œä¼˜äºå…¶ä»–å¯¹æ¯”æ¨¡å‹ã€‚ç»“è®ºï¼šRE-ViTæ¡†æ¶æœ‰æ•ˆåœ°å°†æ”¾å°„ç»„å­¦ä¸ViTæ¶æ„ç›¸ç»“åˆï¼Œæ˜¾ç¤ºå‡ºåœ¨å¤šæ¨¡æ€åŒ»å­¦å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸Šçš„æ”¹è¿›æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10916v1">PDF</a> 27 pages, 3 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§èåˆæ”¾å°„ç»„å­¦ä¸Vision Transformerï¼ˆViTï¼‰çš„æ··åˆæ¡†æ¶RE-ViTï¼Œé€šè¿‡æ—©æœŸèåˆæ”¾å°„ç»„å­¦ç‰¹å¾å’Œæ•°æ®é©±åŠ¨è§†è§‰åµŒå…¥ï¼Œæé«˜åœ¨åŒ»å­¦å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­çš„ç¨³å¥æ€§å’Œæ€§èƒ½ã€‚åœ¨ä¸‰ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRE-ViTè¾¾åˆ°äº†å…ˆè¿›æ°´å¹³ï¼Œå¹¶ä¼˜äºå…¶ä»–å¯¹æ¯”æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RE-ViTç»“åˆäº†æ”¾å°„ç»„å­¦ç‰¹å¾å’ŒVision Transformerï¼ˆViTï¼‰çš„ä¼˜åŠ¿ï¼Œå½¢æˆäº†ä¸€ç§æ··åˆæ¡†æ¶ç”¨äºåŒ»å­¦å›¾åƒåˆ†æã€‚</li>
<li>é€šè¿‡æ—©æœŸèåˆæ”¾å°„ç»„å­¦ç‰¹å¾å’ŒViTåµŒå…¥ï¼ŒRE-ViTæé«˜äº†åŒ»å­¦å›¾åƒåˆ†ç±»çš„ç¨³å¥æ€§å’Œæ€§èƒ½ã€‚</li>
<li>RE-ViTåœ¨ä¸‰ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„è¡¨ç°è¾¾åˆ°äº†å…ˆè¿›æ°´å¹³ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚</li>
<li>ä¸å…¶ä»–æ¨¡å‹ç›¸æ¯”ï¼ŒRE-ViTå±•ç°å‡ºæ›´å¼ºçš„æ€§èƒ½ã€‚</li>
<li>RE-ViTæ¡†æ¶å…·æœ‰è‰¯å¥½çš„é€šç”¨æ€§ï¼Œå¯åº”ç”¨äºå¤šç§æ¨¡æ€çš„åŒ»å­¦å›¾åƒåˆ†ç±»ä»»åŠ¡ã€‚</li>
<li>èåˆè¡¨ç¤ºç»è¿‡å½’ä¸€åŒ–ã€ä½ç½®ç¼–ç åä¼ é€’ç»™ViTç¼–ç å™¨è¿›è¡Œå¤„ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10916">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-17\./crop_åŒ»å­¦å›¾åƒ/2504.10916v1/page_0_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Bringing-together-invertible-UNets-with-invertible-attention-modules-for-memory-efficient-diffusion-models"><a href="#Bringing-together-invertible-UNets-with-invertible-attention-modules-for-memory-efficient-diffusion-models" class="headerlink" title="Bringing together invertible UNets with invertible attention modules for   memory-efficient diffusion models"></a>Bringing together invertible UNets with invertible attention modules for   memory-efficient diffusion models</h2><p><strong>Authors:Karan Jain, Mohammad Nayeem Teli</strong></p>
<p>Diffusion models have recently gained state of the art performance on many image generation tasks. However, most models require significant computational resources to achieve this. This becomes apparent in the application of medical image synthesis due to the 3D nature of medical datasets like CT-scans, MRIs, electron microscope, etc. In this paper we propose a novel architecture for a single GPU memory-efficient training for diffusion models for high dimensional medical datasets. The proposed model is built by using an invertible UNet architecture with invertible attention modules. This leads to the following two contributions: 1. denoising diffusion models and thus enabling memory usage to be independent of the dimensionality of the dataset, and 2. reducing the energy usage during training. While this new model can be applied to a multitude of image generation tasks, we showcase its memory-efficiency on the 3D BraTS2020 dataset leading to up to 15% decrease in peak memory consumption during training with comparable results to SOTA while maintaining the image quality. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹æœ€è¿‘åœ¨è®¸å¤šå›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°æ¨¡å‹ä¸ºäº†å®ç°è¿™ä¸€ç‚¹éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºã€‚è¿™åœ¨åŒ»å­¦å›¾åƒåˆæˆçš„åº”ç”¨ä¸­å˜å¾—å°¤ä¸ºæ˜æ˜¾ï¼Œå› ä¸ºåŒ»å­¦æ•°æ®é›†å¦‚CTæ‰«æã€MRIã€ç”µå­æ˜¾å¾®é•œç­‰å…·æœ‰3Dç‰¹æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é’ˆå¯¹é«˜ç»´åŒ»å­¦æ•°æ®é›†æ‰©æ•£æ¨¡å‹çš„å•GPUå†…å­˜é«˜æ•ˆè®­ç»ƒçš„æ–°å‹æ¶æ„ã€‚è¯¥æ¨¡å‹é‡‡ç”¨å¯é€†UNetæ¶æ„å’Œå¯é€†æ³¨æ„åŠ›æ¨¡å—æ„å»ºã€‚è¿™å¸¦æ¥äº†ä»¥ä¸‹ä¸¤ä¸ªè´¡çŒ®ï¼š1. é™å™ªæ‰©æ•£æ¨¡å‹ï¼Œä»è€Œä½¿å†…å­˜ä½¿ç”¨ä¸æ•°æ®é›†ç»´åº¦æ— å…³ï¼Œå¹¶é™ä½äº†è®­ç»ƒè¿‡ç¨‹ä¸­çš„èƒ½è€—ã€‚è™½ç„¶è¿™ä¸ªæ–°æ¨¡å‹å¯ä»¥åº”ç”¨äºå¤šç§å›¾åƒç”Ÿæˆä»»åŠ¡ï¼Œä½†æˆ‘ä»¬åœ¨3D BraTS2020æ•°æ®é›†ä¸Šå±•ç¤ºäº†å…¶å†…å­˜æ•ˆç‡ï¼Œåœ¨ä¿æŒå›¾åƒè´¨é‡çš„åŒæ—¶ï¼Œè®­ç»ƒæœŸé—´çš„å³°å€¼å†…å­˜ä½¿ç”¨é‡å‡å°‘äº†é«˜è¾¾15%ï¼ŒåŒæ—¶è·å¾—äº†ä¸æœ€æ–°æŠ€æœ¯ç›¸å½“çš„ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10883v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æå‡ºäº†ä¸€ç§ç”¨äºæ‰©æ•£æ¨¡å‹çš„æ–°å‹æ¶æ„ï¼Œè¯¥æ¶æ„é€‚ç”¨äºé«˜ç»´åŒ»å­¦æ•°æ®é›†çš„å•GPUå†…å­˜é«˜æ•ˆè®­ç»ƒã€‚è¯¥æ¨¡å‹é‡‡ç”¨å¯é€†UNetæ¶æ„å’Œå¯é€†æ³¨æ„åŠ›æ¨¡å—ï¼Œèƒ½å¤Ÿå®ç°å»å™ªæ‰©æ•£æ¨¡å‹ï¼Œä½¿å†…å­˜ä½¿ç”¨ä¸æ•°æ®é›†ç»´åº¦æ— å…³ï¼Œå¹¶å‡å°‘è®­ç»ƒè¿‡ç¨‹ä¸­çš„èƒ½è€—ã€‚åœ¨3D BraTS2020æ•°æ®é›†ä¸Šï¼Œæ–°æ¨¡å‹è¡¨ç°å‡ºè¾ƒé«˜çš„å†…å­˜æ•ˆç‡ï¼Œè®­ç»ƒæ—¶çš„å³°å€¼å†…å­˜ä½¿ç”¨é‡å‡å°‘äº†é«˜è¾¾15%ï¼ŒåŒæ—¶ä¿æŒå›¾åƒè´¨é‡ä¸æœ€æ–°æŠ€æœ¯ç›¸å½“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šè¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œä½†å¤§å¤šæ•°æ¨¡å‹éœ€è¦å·¨å¤§çš„è®¡ç®—èµ„æºã€‚</li>
<li>åŒ»å­¦å›¾åƒåˆæˆåº”ç”¨ä¸­ï¼Œç”±äºåŒ»å­¦æ•°æ®é›†ï¼ˆå¦‚CTæ‰«æã€MRIã€ç”µå­æ˜¾å¾®é•œç­‰ï¼‰çš„3Dç‰¹æ€§ï¼Œè®¡ç®—èµ„æºéœ€æ±‚å°¤ä¸ºæ˜¾è‘—ã€‚</li>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°å‹æ¶æ„ï¼Œé€‚ç”¨äºé«˜ç»´åŒ»å­¦æ•°æ®é›†çš„æ‰©æ•£æ¨¡å‹çš„å•ä¸ªGPUå†…å­˜é«˜æ•ˆè®­ç»ƒã€‚</li>
<li>æ–°æ¨¡å‹é‡‡ç”¨å¯é€†UNetæ¶æ„å’Œå¯é€†æ³¨æ„åŠ›æ¨¡å—ï¼Œå®ç°äº†å»å™ªæ‰©æ•£æ¨¡å‹ã€‚</li>
<li>è¯¥æ¨¡å‹ä½¿å†…å­˜ä½¿ç”¨ä¸æ•°æ®é›†ç»´åº¦æ— å…³ï¼Œå¹¶é™ä½äº†è®­ç»ƒè¿‡ç¨‹ä¸­çš„èƒ½è€—ã€‚</li>
<li>åœ¨3D BraTS2020æ•°æ®é›†ä¸Šï¼Œæ–°æ¨¡å‹å±•ç¤ºäº†è¾ƒé«˜çš„å†…å­˜æ•ˆç‡ï¼Œè®­ç»ƒæ—¶çš„å³°å€¼å†…å­˜ä½¿ç”¨é‡å‡å°‘äº†é«˜è¾¾15%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10883">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-17\./crop_åŒ»å­¦å›¾åƒ/2504.10883v1/page_0_0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="The-origin-of-X-ray-intra-day-variability-in-HBL-PKS-2155-304"><a href="#The-origin-of-X-ray-intra-day-variability-in-HBL-PKS-2155-304" class="headerlink" title="The origin of X-ray intra-day variability in HBL PKS 2155-304"></a>The origin of X-ray intra-day variability in HBL PKS 2155-304</h2><p><strong>Authors:W. Hu, J. L. Kang, J. X. Wang, G. C. Xiao, G. W. Ren</strong></p>
<p>The origin and physics of X-ray intra-day variability (IDV) in blazars, which is a long-standing issue, is studied by modelling the broad-band X-ray spectrum, the light curves (LCs), and the Fourier time lags. We present the timing analysis of three archived XMM-Newton observations with a total exposure of $&gt;80$ ks of PKS 2155-304, which is one of the brightest and most studied HBLs in the X-ray band. For each observation, we constructed averaged X-ray spectra in 0.5-10 keV band, as well as 100 s binned LCs in various sub-bands. We performed the Bayesian power spectral density (PSD) analysis and Fourier time-lag analyses of the variable LCs. The results are carefully modelled in the context of a multi-zone jet model. PSD analysis reveals that the X-ray variability can be characterised by red noise. The lag-frequency spectra measured in two observations show only the soft or negative lags, with the magnitude of the lags increasing as the frequency decreases. For another observation, the lag-frequency spectra are characterised by small positive or zero time lags at the lowest frequencies, which drops to negative values at higher frequencies. The magnitude of the soft lags ranges from $\sim5$ to $\sim40$ minutes, and increases with the energy difference of two compared LCs. The observed X-ray spectra and lag-frequency spectra can both be successfully described by our proposed two-zone model, with the physical parameters constrained in a fully acceptable space. Moreover, the LC profiles at different energy bands can be satisfactorily reproduced by only varying the injection rate of the energetic electrons. The IDV of PKS 2155-304 should be caused by the injection of energetic electrons, and accelerated by shocks formed in a weakly magnetised jet. </p>
<blockquote>
<p>å¯¹äºé—ªçƒæºä¸­çš„Xå°„çº¿æ—¥é—´å˜åŒ–ï¼ˆIDVï¼‰çš„èµ·æºå’Œç‰©ç†æ€§è´¨ï¼Œä¸€ç›´æ˜¯ä¸€ä¸ªé•¿æœŸå­˜åœ¨çš„é—®é¢˜ã€‚é€šè¿‡å®½å¸¦Xå°„çº¿å…‰è°±ã€å…‰å˜æ›²çº¿ï¼ˆLCsï¼‰å’Œå‚…é‡Œå¶æ—¶é—´å»¶è¿Ÿè¿›è¡Œå»ºæ¨¡ç ”ç©¶ã€‚æˆ‘ä»¬å±•ç¤ºäº†PKS 2155-304çš„ä¸‰æ¬¡å­˜æ¡£XMM-Newtonè§‚æµ‹çš„æ—¶åºåˆ†æï¼Œæ€»æ›å…‰æ—¶é—´è¶…è¿‡80ksï¼Œè¿™æ˜¯Xå°„çº¿æ³¢æ®µä¸­æœ€æ˜äº®ã€ç ”ç©¶æœ€å¤šçš„HBLä¹‹ä¸€ã€‚å¯¹äºæ¯æ¬¡è§‚æµ‹ï¼Œæˆ‘ä»¬åœ¨0.5-10keVæ³¢æ®µæ„å»ºäº†å¹³å‡Xå°„çº¿å…‰è°±ï¼Œä»¥åŠå„ç§å­æ³¢æ®µä¸­çš„æ¯100ç§’åˆ†ç®±å…‰å˜æ›²çº¿ã€‚æˆ‘ä»¬å¯¹å¯å˜å…‰å˜æ›²çº¿è¿›è¡Œäº†è´å¶æ–¯åŠŸç‡è°±å¯†åº¦ï¼ˆPSDï¼‰åˆ†æå’Œå‚…é‡Œå¶æ—¶é—´å»¶è¿Ÿåˆ†æã€‚ç»“åˆå¤šåŒºå–·å°„æ¨¡å‹ï¼Œæˆ‘ä»¬å¯¹ç»“æœè¿›è¡Œäº†ä»”ç»†å»ºæ¨¡ã€‚PSDåˆ†æè¡¨æ˜ï¼ŒXå°„çº¿å˜åŒ–å¯ä»¥ç”¨çº¢å™ªå£°æ¥è¡¨å¾ã€‚ä¸¤æ¬¡è§‚æµ‹æµ‹å¾—çš„æ»åé¢‘ç‡å…‰è°±ä»…æ˜¾ç¤ºè½¯æ»åæˆ–è´Ÿæ»åï¼Œæ»åçš„å¹…åº¦éšç€é¢‘ç‡çš„é™ä½è€Œå¢åŠ ã€‚å¯¹äºå¦ä¸€æ¬¡è§‚æµ‹ï¼Œæ»åé¢‘ç‡å…‰è°±åœ¨æœ€ä½é¢‘ç‡ä¸‹è¡¨ç°ä¸ºè¾ƒå°çš„æ­£æ—¶æ»æˆ–é›¶æ—¶æ»ï¼Œç„¶ååœ¨æ›´é«˜é¢‘ç‡æ—¶é™ä¸ºè´Ÿæ•°ã€‚è½¯æ»åçš„å¹…åº¦èŒƒå›´ä»çº¦5åˆ†é’Ÿåˆ°çº¦40åˆ†é’Ÿï¼Œå¹¶éšç€ä¸¤ä¸ªæ¯”è¾ƒå…‰å˜æ›²çº¿çš„èƒ½é‡å·®è€Œå¢åŠ ã€‚æ‰€æå‡ºçš„ä¸¤åŒºæ¨¡å‹èƒ½å¤ŸæˆåŠŸæè¿°è§‚æµ‹åˆ°çš„Xå°„çº¿å…‰è°±å’Œæ»åé¢‘ç‡å…‰è°±ï¼Œç‰©ç†å‚æ•°åœ¨å®Œå…¨å¯æ¥å—çš„èŒƒå›´å†…å—åˆ°é™åˆ¶ã€‚æ­¤å¤–ï¼Œé€šè¿‡ä»…æ”¹å˜é«˜èƒ½ç”µå­çš„æ³¨å…¥ç‡ï¼Œå¯ä»¥ä»¤äººæ»¡æ„åœ°å†ç°ä¸åŒèƒ½å¸¦çš„å…‰å˜æ›²çº¿è½®å»“ã€‚PKS 2155-304çš„IDVåº”ç”±é«˜èƒ½ç”µå­çš„æ³¨å…¥å¼•èµ·ï¼Œå¹¶åœ¨å¼±ç£åŒ–å–·å°„æµä¸­å½¢æˆçš„å†²å‡»æ³¢çš„æ¨åŠ¨ä¸‹åŠ é€Ÿã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10773v1">PDF</a> Accepted for publication in A&amp;A, 13pages, 11 figures</p>
<p><strong>Summary</strong></p>
<p>PKS 2155-304çš„Xå°„çº¿å†…æ—¥å˜åŒ–ï¼ˆIDVï¼‰çš„æºå¤´å’Œç‰©ç†æœºåˆ¶é€šè¿‡å¯¹å…¶å®½å¸¦Xå°„çº¿è°±ã€å…‰å˜æ›²çº¿å’Œå‚…é‡Œå¶æ—¶é—´æ»åè¿›è¡Œå»ºæ¨¡ç ”ç©¶ã€‚é‡‡ç”¨è´å¶æ–¯åŠŸç‡è°±å¯†åº¦åˆ†æå’Œå‚…é‡Œå¶æ—¶é—´æ»ååˆ†æç­‰æ–¹æ³•ï¼Œå¯¹å…¶è§‚æµ‹æ•°æ®è¿›è¡Œæ—¶åºåˆ†æã€‚ç»“æœæ˜¾ç¤ºï¼ŒXå°„çº¿å˜ç‡å¯ç”±çº¢è‰²å™ªå£°ç‰¹å¾æè¿°ï¼›æ—¶é—´æ»åéšé¢‘ç‡é™ä½è€Œå¢åŠ ï¼Œæˆ–åœ¨é«˜é¢‘ç‡æ—¶ä¸ºé›¶æˆ–è½¬ä¸ºè´Ÿå€¼ã€‚è¿™äº›ç°è±¡å¯é€šè¿‡æå‡ºçš„ä¸¤åŒºæ¨¡å‹æˆåŠŸæè¿°ï¼Œå…¶ä¸­ç‰©ç†å‚æ•°åœ¨ä¸€å®šçš„å¯æ¥å—ç©ºé—´å†…å—åˆ°é™åˆ¶ã€‚å…‰å˜æ›²çº¿ä¸åŒèƒ½å¸¦çš„è½®å»“å¯ä»¥é€šè¿‡ä»…æ”¹å˜é«˜èƒ½ç”µå­çš„æ³¨å…¥ç‡æ¥å¤åˆ¶ã€‚å› æ­¤ï¼ŒPKS 2155-304çš„IDVå¯èƒ½ç”±é«˜èƒ½ç”µå­çš„æ³¨å…¥å¼•èµ·ï¼Œå¹¶åœ¨å¼±ç£åŒ–å–·æµä¸­çš„æ¿€æ³¢ä½œç”¨ä¸‹åŠ é€Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¯¹PKS 2155-304è¿›è¡Œäº†é•¿æœŸè§‚å¯Ÿçš„Xå°„çº¿å†…æ—¥å˜åŒ–ï¼ˆIDVï¼‰ç ”ç©¶ã€‚</li>
<li>é€šè¿‡åˆ†æå…‰å˜æ›²çº¿å’Œå‚…é‡Œå¶æ—¶é—´æ»åï¼Œæ­ç¤ºäº†Xå°„çº¿å˜ç‡çš„ç‰¹å¾ã€‚</li>
<li>è´å¶æ–¯åŠŸç‡è°±å¯†åº¦åˆ†ææ˜¾ç¤ºXå°„çº¿å˜ç‡å…·æœ‰çº¢è‰²å™ªå£°ç‰¹å¾ã€‚</li>
<li>æ—¶é—´æ»åéšé¢‘ç‡å˜åŒ–çš„ç°è±¡å¯é€šè¿‡ä¸¤åŒºæ¨¡å‹æˆåŠŸæè¿°ã€‚</li>
<li>å…‰å˜æ›²çº¿ä¸åŒèƒ½å¸¦çš„è½®å»“å·®å¼‚å¯ä»¥é€šè¿‡é«˜èƒ½ç”µå­æ³¨å…¥ç‡çš„å˜åŒ–æ¥è§£é‡Šã€‚</li>
<li>IDVå¯èƒ½ç”±é«˜èƒ½ç”µå­åœ¨å¼±ç£åŒ–å–·æµä¸­çš„æ³¨å…¥å’Œæ¿€æ³¢åŠ é€Ÿå¼•èµ·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10773">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-17\./crop_åŒ»å­¦å›¾åƒ/2504.10773v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-17\./crop_åŒ»å­¦å›¾åƒ/2504.10773v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-17\./crop_åŒ»å­¦å›¾åƒ/2504.10773v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="SilVar-Med-A-Speech-Driven-Visual-Language-Model-for-Explainable-Abnormality-Detection-in-Medical-Imaging"><a href="#SilVar-Med-A-Speech-Driven-Visual-Language-Model-for-Explainable-Abnormality-Detection-in-Medical-Imaging" class="headerlink" title="SilVar-Med: A Speech-Driven Visual Language Model for Explainable   Abnormality Detection in Medical Imaging"></a>SilVar-Med: A Speech-Driven Visual Language Model for Explainable   Abnormality Detection in Medical Imaging</h2><p><strong>Authors:Tan-Hanh Pham, Chris Ngo, Trong-Duong Bui, Minh Luu Quang, Tan-Huong Pham, Truong-Son Hy</strong></p>
<p>Medical Visual Language Models have shown great potential in various healthcare applications, including medical image captioning and diagnostic assistance. However, most existing models rely on text-based instructions, limiting their usability in real-world clinical environments especially in scenarios such as surgery, text-based interaction is often impractical for physicians. In addition, current medical image analysis models typically lack comprehensive reasoning behind their predictions, which reduces their reliability for clinical decision-making. Given that medical diagnosis errors can have life-changing consequences, there is a critical need for interpretable and rational medical assistance. To address these challenges, we introduce an end-to-end speech-driven medical VLM, SilVar-Med, a multimodal medical image assistant that integrates speech interaction with VLMs, pioneering the task of voice-based communication for medical image analysis. In addition, we focus on the interpretation of the reasoning behind each prediction of medical abnormalities with a proposed reasoning dataset. Through extensive experiments, we demonstrate a proof-of-concept study for reasoning-driven medical image interpretation with end-to-end speech interaction. We believe this work will advance the field of medical AI by fostering more transparent, interactive, and clinically viable diagnostic support systems. Our code and dataset are publicly available at SiVar-Med. </p>
<blockquote>
<p>åŒ»ç–—è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å„ç§åŒ»ç–—å¥åº·åº”ç”¨ä¸­æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ï¼ŒåŒ…æ‹¬åŒ»å­¦å›¾åƒæè¿°å’Œè¯Šæ–­è¾…åŠ©ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ¨¡å‹ä¾èµ–äºæ–‡æœ¬æŒ‡ä»¤ï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨çœŸå®ä¸–ç•Œä¸´åºŠç¯å¢ƒä¸­çš„å®ç”¨æ€§ï¼Œå°¤å…¶æ˜¯åœ¨æ‰‹æœ¯ç­‰åœºæ™¯ä¸­ï¼ŒåŸºäºæ–‡æœ¬çš„äº¤äº’å¯¹åŒ»ç”Ÿæ¥è¯´é€šå¸¸ä¸åˆ‡å®é™…ã€‚æ­¤å¤–ï¼Œå½“å‰çš„åŒ»å­¦å›¾åƒåˆ†ææ¨¡å‹é€šå¸¸ç¼ºä¹é¢„æµ‹èƒŒåçš„ç»¼åˆæ¨ç†ï¼Œè¿™é™ä½äº†å®ƒä»¬åœ¨ä¸´åºŠå†³ç­–ä¸­çš„å¯é æ€§ã€‚é‰´äºåŒ»ç–—è¯Šæ–­é”™è¯¯å¯èƒ½ä¼šå¸¦æ¥æ”¹å˜ç”Ÿå‘½çš„åæœï¼Œå› æ­¤éœ€è¦å¯è§£é‡Šå’Œåˆç†çš„åŒ»ç–—è¾…åŠ©ã€‚é’ˆå¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç«¯åˆ°ç«¯çš„è¯­éŸ³é©±åŠ¨åŒ»ç–—è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆSilVar-Medï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å¤šæ¨¡å¼åŒ»å­¦å›¾åƒåŠ©ç†ï¼Œå®ƒå°†è¯­éŸ³äº¤äº’ä¸è§†è§‰è¯­è¨€æ¨¡å‹é›†æˆåœ¨ä¸€èµ·ï¼Œç‡å…ˆå®ŒæˆåŸºäºè¯­éŸ³çš„åŒ»å­¦å›¾åƒåˆ†æé€šä¿¡ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç€é‡äºé€šè¿‡æå‡ºçš„æ¨ç†æ•°æ®é›†è§£é‡ŠåŒ»å­¦å¼‚å¸¸é¢„æµ‹èƒŒåçš„æ¨ç†ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼Œæˆ‘ä»¬å±•ç¤ºäº†ç«¯åˆ°ç«¯è¯­éŸ³äº¤äº’é©±åŠ¨çš„æ¨ç†é©±åŠ¨åŒ»å­¦å›¾åƒè§£é‡Šçš„æ¦‚å¿µéªŒè¯ç ”ç©¶ã€‚æˆ‘ä»¬ç›¸ä¿¡è¿™é¡¹å·¥ä½œå°†é€šè¿‡ä¿ƒè¿›æ›´é€æ˜ã€äº’åŠ¨å’Œä¸´åºŠå¯è¡Œçš„è¯Šæ–­æ”¯æŒç³»ç»Ÿæ¥æ¨åŠ¨åŒ»ç–—äººå·¥æ™ºèƒ½é¢†åŸŸçš„å‘å±•ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®é›†å¯åœ¨SiVar-Medä¸Šå…¬å¼€è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10642v1">PDF</a> CVPR Multimodal Algorithmic Reasoning Workshop 2025 - SilVarMed</p>
<p><strong>æ‘˜è¦</strong></p>
<p>åŒ»ç–—è§†è§‰è¯­è¨€æ¨¡å‹åœ¨åŒ»ç–—åº”ç”¨çš„å„ä¸ªé¢†åŸŸå±•ç°å‡ºäº†å·¨å¤§çš„æ½œåŠ›ï¼ŒåŒ…æ‹¬åŒ»å­¦å›¾åƒæè¿°å’Œè¯Šæ–­è¾…åŠ©ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ¨¡å‹ä¾èµ–äºæ–‡æœ¬æŒ‡ä»¤ï¼Œè¿™åœ¨ç°å®ä¸–ç•Œçš„ä¸´åºŠç¯å¢ƒä¸­å°¤å…¶ä¸å®ç”¨ï¼Œå¦‚åœ¨æ‰‹æœ¯ç­‰åœºæ™¯ä¸­ï¼ŒåŸºäºæ–‡æœ¬çš„äº¤äº’å¯¹åŒ»ç”Ÿæ¥è¯´å¹¶ä¸æ–¹ä¾¿ã€‚æ­¤å¤–ï¼Œå½“å‰çš„åŒ»å­¦å›¾åƒåˆ†ææ¨¡å‹é€šå¸¸ç¼ºä¹å¯¹å…¶é¢„æµ‹çš„å…¨é¢ç†è§£ï¼Œè¿™é™ä½äº†å®ƒä»¬åœ¨ä¸´åºŠå†³ç­–ä¸­çš„å¯é æ€§ã€‚è€ƒè™‘åˆ°åŒ»ç–—è¯Šæ–­é”™è¯¯å¯èƒ½ä¼šå¸¦æ¥æ”¹å˜ç”Ÿå‘½çš„åæœï¼Œè¿«åˆ‡éœ€è¦å¯è§£é‡Šå’Œåˆç†çš„åŒ»ç–—è¾…åŠ©ã€‚é’ˆå¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç«¯åˆ°ç«¯çš„è¯­éŸ³é©±åŠ¨åŒ»ç–—è§†è§‰è¯­è¨€æ¨¡å‹SilVar-Medï¼Œè¿™æ˜¯ä¸€ç§å°†è¯­éŸ³äº¤äº’ä¸è§†è§‰è¯­è¨€æ¨¡å‹ç›¸ç»“åˆçš„å¤šæ¨¡å¼åŒ»å­¦å›¾åƒè¾…åŠ©å·¥å…·ï¼Œç‡å…ˆå®ç°äº†åŸºäºè¯­éŸ³çš„åŒ»å­¦å›¾åƒåˆ†æé€šä¿¡ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡ç‚¹å…³æ³¨æ¯ä¸ªåŒ»å­¦å¼‚å¸¸é¢„æµ‹èƒŒåçš„è§£é‡Šï¼Œå¹¶æ¨å‡ºäº†ä¸€ä¸ªè§£é‡Šæ€§æ•°æ®é›†ã€‚é€šè¿‡å¤§é‡å®éªŒï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä»¥è§£é‡Šä¸ºå¯¼å‘çš„åŒ»å­¦å›¾åƒè§£è¯»ä¸ç«¯åˆ°ç«¯è¯­éŸ³äº¤äº’çš„æ¦‚å¿µéªŒè¯ç ”ç©¶ã€‚æˆ‘ä»¬ç›¸ä¿¡è¿™é¡¹å·¥ä½œå°†é€šè¿‡ä¿ƒè¿›æ›´é€æ˜ã€äº’åŠ¨å’Œä¸´åºŠå¯è¡Œçš„è¯Šæ–­æ”¯æŒç³»ç»Ÿæ¥æ¨åŠ¨åŒ»ç–—äººå·¥æ™ºèƒ½é¢†åŸŸçš„å‘å±•ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®é›†å·²åœ¨SiVar-Medä¸Šå…¬å¼€å¯ç”¨ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åŒ»ç–—è§†è§‰è¯­è¨€æ¨¡å‹åœ¨åŒ»ç–—åº”ç”¨ä¸­å…·æœ‰å·¨å¤§æ½œåŠ›ï¼ŒåŒ…æ‹¬åŒ»å­¦å›¾åƒæè¿°å’Œè¯Šæ–­è¾…åŠ©ã€‚</li>
<li>ç°æœ‰æ¨¡å‹ä¸»è¦ä¾èµ–äºæ–‡æœ¬æŒ‡ä»¤ï¼Œè¿™åœ¨ç°å®ä¸´åºŠç¯å¢ƒä¸­å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>åŒ»å­¦å›¾åƒåˆ†ææ¨¡å‹ç¼ºä¹å…¨é¢çš„é¢„æµ‹ç†è§£ï¼Œå½±å“ä¸´åºŠå†³ç­–çš„å¯é æ€§ã€‚</li>
<li>è¯­éŸ³é©±åŠ¨çš„åŒ»ç–—è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚SilVar-Medï¼‰é€šè¿‡è¯­éŸ³äº¤äº’æä¾›å¤šæ¨¡å¼åŒ»å­¦å›¾åƒè¾…åŠ©ã€‚</li>
<li>SilVar-Medç‡å…ˆå®ç°åŸºäºè¯­éŸ³çš„åŒ»å­¦å›¾åƒåˆ†æé€šä¿¡ä»»åŠ¡ï¼Œå¼ºè°ƒé¢„æµ‹èƒŒåçš„è§£é‡Šæ€§ã€‚</li>
<li>é€šè¿‡æ¨å‡ºè§£é‡Šæ€§æ•°æ®é›†ï¼ŒåŠ å¼ºå¯¹åŒ»å­¦å¼‚å¸¸é¢„æµ‹èƒŒåç†ç”±çš„å…³æ³¨ã€‚</li>
<li>æ¦‚å¿µéªŒè¯ç ”ç©¶è¡¨æ˜ï¼Œä»¥è§£é‡Šä¸ºå¯¼å‘çš„åŒ»å­¦å›¾åƒè§£è¯»ä¸ç«¯åˆ°ç«¯è¯­éŸ³äº¤äº’ç›¸ç»“åˆçš„æ–¹æ³•å…·æœ‰å¯è¡Œæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10642">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-17\./crop_åŒ»å­¦å›¾åƒ/2504.10642v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-17\./crop_åŒ»å­¦å›¾åƒ/2504.10642v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-17\./crop_åŒ»å­¦å›¾åƒ/2504.10642v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-17\./crop_åŒ»å­¦å›¾åƒ/2504.10642v1/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-17\./crop_åŒ»å­¦å›¾åƒ/2504.10642v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="PathSeqSAM-Sequential-Modeling-for-Pathology-Image-Segmentation-with-SAM2"><a href="#PathSeqSAM-Sequential-Modeling-for-Pathology-Image-Segmentation-with-SAM2" class="headerlink" title="PathSeqSAM: Sequential Modeling for Pathology Image Segmentation with   SAM2"></a>PathSeqSAM: Sequential Modeling for Pathology Image Segmentation with   SAM2</h2><p><strong>Authors:Mingyang Zhu, Yinting Liu, Mingyu Li, Jiacheng Wang</strong></p>
<p>Current methods for pathology image segmentation typically treat 2D slices independently, ignoring valuable cross-slice information. We present PathSeqSAM, a novel approach that treats 2D pathology slices as sequential video frames using SAM2â€™s memory mechanisms. Our method introduces a distance-aware attention mechanism that accounts for variable physical distances between slices and employs LoRA for domain adaptation. Evaluated on the KPI Challenge 2024 dataset for glomeruli segmentation, PathSeqSAM demonstrates improved segmentation quality, particularly in challenging cases that benefit from cross-slice context. We have publicly released our code at <a target="_blank" rel="noopener" href="https://github.com/JackyyyWang/PathSeqSAM">https://github.com/JackyyyWang/PathSeqSAM</a>. </p>
<blockquote>
<p>ç°æœ‰çš„ç—…ç†å­¦å›¾åƒåˆ†å‰²æ–¹æ³•é€šå¸¸ç‹¬ç«‹å¤„ç†2Dåˆ‡ç‰‡ï¼Œå¿½ç•¥äº†æœ‰ä»·å€¼çš„è·¨åˆ‡ç‰‡ä¿¡æ¯ã€‚æˆ‘ä»¬æå‡ºäº†PathSeqSAMï¼Œè¿™æ˜¯ä¸€ç§æ–°æ–¹æ³•ï¼Œåˆ©ç”¨SAM2çš„è®°å¿†æœºåˆ¶å°†2Dç—…ç†å­¦åˆ‡ç‰‡è§†ä¸ºè¿ç»­çš„è§†é¢‘å¸§ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸€ç§è·ç¦»æ„ŸçŸ¥æ³¨æ„åŠ›æœºåˆ¶ï¼Œè¯¥æœºåˆ¶è€ƒè™‘äº†åˆ‡ç‰‡ä¹‹é—´çš„ç‰©ç†è·ç¦»å˜åŒ–ï¼Œå¹¶é‡‡ç”¨äº†LoRAè¿›è¡ŒåŸŸè‡ªé€‚åº”ã€‚åœ¨è‚¾å°çƒåˆ†å‰²çš„KPI Challenge 2024æ•°æ®é›†ä¸Šè¯„ä¼°çš„PathSeqSAMï¼Œæ˜¾ç¤ºå‡ºåˆ†å‰²è´¨é‡çš„æå‡ï¼Œç‰¹åˆ«æ˜¯åœ¨å—ç›Šäºè·¨åˆ‡ç‰‡ä¸Šä¸‹æ–‡çš„æŒ‘æˆ˜æ€§æ¡ˆä¾‹ä¸­ã€‚æˆ‘ä»¬çš„ä»£ç å·²å…¬å¼€å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/JackyyyWang/PathSeqSAM">https://github.com/JackyyyWang/PathSeqSAM</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10526v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒåˆ†å‰²å½“å‰æ–¹æ³•å¤šå¿½ç•¥è·¨åˆ‡ç‰‡ä¿¡æ¯ï¼Œç‹¬ç«‹å¤„ç†2Dåˆ‡ç‰‡ã€‚æœ¬æ–‡æå‡ºPathSeqSAMæ–°æ–¹æ³•ï¼Œåˆ©ç”¨SAM2çš„è®°å¿†æœºåˆ¶å°†2Dç—…ç†åˆ‡ç‰‡è§†ä½œè¿ç»­è§†é¢‘å¸§å¤„ç†ã€‚è¯¥æ–¹æ³•å¼•å…¥è·ç¦»æ„ŸçŸ¥æ³¨æ„åŠ›æœºåˆ¶ï¼Œè€ƒè™‘åˆ‡ç‰‡é—´ä¸åŒç‰©ç†è·ç¦»ï¼Œå¹¶é‡‡ç”¨LoRAè¿›è¡Œé¢†åŸŸé€‚é…ã€‚åœ¨KPI Challenge 2024è‚¾å°çƒåˆ†å‰²æ•°æ®é›†ä¸Šè¯„ä¼°ï¼ŒPathSeqSAMæé«˜äº†åˆ†å‰²è´¨é‡ï¼Œç‰¹åˆ«æ˜¯åœ¨å—ç›Šäºè·¨åˆ‡ç‰‡ä¸Šä¸‹æ–‡çš„æŒ‘æˆ˜æ¡ˆä¾‹ä¸­ã€‚ä»£ç å·²å…¬å¼€äº<a target="_blank" rel="noopener" href="https://github.com/JackyyyWang/PathSeqSAM%E3%80%82">https://github.com/JackyyyWang/PathSeqSAMã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹æ³•å¤šå¿½ç•¥è·¨åˆ‡ç‰‡ä¿¡æ¯ã€‚</li>
<li>PathSeqSAMæ–¹æ³•å°†2Dç—…ç†åˆ‡ç‰‡è§†ä½œè¿ç»­è§†é¢‘å¸§å¤„ç†ã€‚</li>
<li>PathSeqSAMåˆ©ç”¨SAM2çš„è®°å¿†æœºåˆ¶ã€‚</li>
<li>PathSeqSAMå¼•å…¥è·ç¦»æ„ŸçŸ¥æ³¨æ„åŠ›æœºåˆ¶ã€‚</li>
<li>PathSeqSAMè€ƒè™‘åˆ‡ç‰‡é—´çš„ç‰©ç†è·ç¦»ã€‚</li>
<li>PathSeqSAMé‡‡ç”¨LoRAè¿›è¡Œé¢†åŸŸé€‚é…ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10526">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-17\./crop_åŒ»å­¦å›¾åƒ/2504.10526v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-17\./crop_åŒ»å­¦å›¾åƒ/2504.10526v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-17\./crop_åŒ»å­¦å›¾åƒ/2504.10526v1/page_2_0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="SlicerNNInteractive-A-3D-Slicer-extension-for-nnInteractive"><a href="#SlicerNNInteractive-A-3D-Slicer-extension-for-nnInteractive" class="headerlink" title="SlicerNNInteractive: A 3D Slicer extension for nnInteractive"></a>SlicerNNInteractive: A 3D Slicer extension for nnInteractive</h2><p><strong>Authors:Coen de Vente, Kiran Vaidhya Venkadesh, Bram van Ginneken, Clara I. SÃ¡nchez</strong></p>
<p>SlicerNNInteractive integrates nnInteractive, a state-of-the-art promptable deep learning-based framework for 3D image segmentation, into the widely used 3D Slicer platform. Our extension implements a client-server architecture that decouples computationally intensive model inference from the client-side interface. Therefore, SlicerNNInteractive eliminates heavy hardware constraints on the client-side and enables better operating system compatibility than existing plugins for nnInteractive. Running both the client and server-side on a single machine is also possible, offering flexibility across different deployment scenarios. The extension provides an intuitive user interface with all interaction types available in the original framework (point, bounding box, scribble, and lasso prompts), while including a comprehensive set of keyboard shortcuts for efficient workflow. </p>
<blockquote>
<p>SlicerNNInteractiveå°†nnInteractiveï¼ˆä¸€ç§ç”¨äº3Då›¾åƒåˆ†å‰²çš„å…ˆè¿›æç¤ºå¼æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼‰é›†æˆåˆ°å¹¿æ³›ä½¿ç”¨çš„3D Slicerå¹³å°ä¸­ã€‚æˆ‘ä»¬çš„æ‰©å±•å®ç°äº†ä¸€ç§å®¢æˆ·ç«¯-æœåŠ¡å™¨æ¶æ„ï¼Œè¯¥æ¶æ„å°†è®¡ç®—å¯†é›†å‹çš„æ¨¡å‹æ¨ç†ä¸å®¢æˆ·ç«¯ç•Œé¢è§£è€¦ã€‚å› æ­¤ï¼ŒSlicerNNInteractiveæ¶ˆé™¤äº†å®¢æˆ·ç«¯ç«¯çš„ç¹é‡ç¡¬ä»¶çº¦æŸï¼Œå¹¶ä¸”ä¸ç°æœ‰çš„nnInteractiveæ’ä»¶ç›¸æ¯”ï¼Œå…·æœ‰æ›´å¥½çš„æ“ä½œç³»ç»Ÿå…¼å®¹æ€§ã€‚åœ¨å•æœºä¸ŠåŒæ—¶è¿è¡Œå®¢æˆ·ç«¯å’ŒæœåŠ¡å™¨ç«¯ä¹Ÿæ˜¯å¯èƒ½çš„ï¼Œä¸ºä¸åŒçš„éƒ¨ç½²åœºæ™¯æä¾›äº†çµæ´»æ€§ã€‚è¯¥æ‰©å±•æä¾›äº†ä¸€ä¸ªç›´è§‚çš„ç”¨æˆ·ç•Œé¢ï¼Œæ‹¥æœ‰åŸå§‹æ¡†æ¶ä¸­æ‰€æœ‰å¯ç”¨çš„äº¤äº’ç±»å‹ï¼ˆç‚¹ã€è¾¹ç•Œæ¡†ã€æ¶‚é¸¦å’Œå¥—ç´¢æç¤ºï¼‰ï¼ŒåŒæ—¶åŒ…æ‹¬ä¸€ç»„å…¨é¢çš„é”®ç›˜å¿«æ·é”®ï¼Œä»¥æé«˜å·¥ä½œæ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07991v2">PDF</a> 5 pages, 2 figures</p>
<p><strong>Summary</strong></p>
<p>SlicerNNInteractiveå°†nnInteractiveè¿™ä¸€å…ˆè¿›çš„å¯æç¤ºçš„æ·±åº¦å­¦ä¹ ä¸‰ç»´å›¾åƒåˆ†å‰²æ¡†æ¶é›†æˆåˆ°å¹¿æ³›ä½¿ç”¨çš„3D Slicerå¹³å°ä¸­ã€‚å®ƒé€šè¿‡å®¢æˆ·ç«¯-æœåŠ¡å™¨æ¶æ„å®ç°æ¨¡å‹æ¨ç†ä¸å®¢æˆ·ç«¯ç•Œé¢ä¹‹é—´çš„è§£è€¦ï¼Œæ¶ˆé™¤äº†å®¢æˆ·ç«¯ç¡¬ä»¶çš„ç¹é‡çº¦æŸï¼Œæé«˜äº†æ“ä½œç³»ç»Ÿçš„å…¼å®¹æ€§ã€‚åŒæ—¶ï¼Œè¯¥æ‰©å±•æä¾›äº†ç›´è§‚çš„ç”¨æˆ·ç•Œé¢å’Œä¸°å¯Œçš„äº¤äº’ç±»å‹ï¼ŒåŒ…æ‹¬é”®ç›˜å¿«æ·é”®ï¼Œä»¥æé«˜å·¥ä½œæ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SlicerNNInteractiveé›†æˆäº†nnInteractiveæ¡†æ¶åˆ°3D Slicerå¹³å°ã€‚</li>
<li>é‡‡ç”¨å®¢æˆ·ç«¯-æœåŠ¡å™¨æ¶æ„ï¼Œå®ç°æ¨¡å‹æ¨ç†ä¸å®¢æˆ·ç«¯ç•Œé¢çš„è§£è€¦ã€‚</li>
<li>æ¶ˆé™¤äº†å®¢æˆ·ç«¯ç¡¬ä»¶çš„ç¹é‡çº¦æŸï¼Œæé«˜äº†æ“ä½œç³»ç»Ÿå…¼å®¹æ€§ã€‚</li>
<li>æ‰©å±•äº†ç›´è§‚çš„ç”¨æˆ·ç•Œé¢å’Œä¸°å¯Œçš„äº¤äº’ç±»å‹ã€‚</li>
<li>é”®ç›˜å¿«æ·é”®æé«˜äº†å·¥ä½œæ•ˆç‡ã€‚</li>
<li>æ”¯æŒåœ¨å•æœºä¸ŠåŒæ—¶è¿è¡Œå®¢æˆ·ç«¯å’ŒæœåŠ¡å™¨ç«¯ï¼Œé€‚åº”ä¸åŒéƒ¨ç½²åœºæ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07991">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-17\./crop_åŒ»å­¦å›¾åƒ/2504.07991v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-17\./crop_åŒ»å­¦å›¾åƒ/2504.07991v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-17\./crop_åŒ»å­¦å›¾åƒ/2504.07991v2/page_3_0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Core-Excited-States-of-Linear-and-Bent-Uranyl-Complexes-Insights-from-High-Energy-Resolution-X-ray-Spectroscopy-and-Relativistic-Quantum-Chemistry"><a href="#Core-Excited-States-of-Linear-and-Bent-Uranyl-Complexes-Insights-from-High-Energy-Resolution-X-ray-Spectroscopy-and-Relativistic-Quantum-Chemistry" class="headerlink" title="Core-Excited States of Linear and Bent Uranyl Complexes: Insights from   High-Energy Resolution X-ray Spectroscopy and Relativistic Quantum Chemistry"></a>Core-Excited States of Linear and Bent Uranyl Complexes: Insights from   High-Energy Resolution X-ray Spectroscopy and Relativistic Quantum Chemistry</h2><p><strong>Authors:Wilken Aldair Misael, Lucia Amidani, Juliane MÃ¤rz, Elena F. Bazarkina, Kristina O. Kvashnina, ValÃ©rie Vallet, AndrÃ© Severo Pereira Gomes</strong></p>
<p>Advanced X-ray spectroscopic techniques are widely recognized as state-of-the-art tools for probing the electronic structure, bonding, and chemical environments of the heaviest elements in the periodic table. In this study, we employ X-ray absorption near-edge structure measurements in high-energy resolution fluorescence detection (HERFD-XANES) mode to investigate the core states arising from excitations out of the U 3d${_{3&#x2F;2}}$ (M$_4$ edge) levels for molecular complexes in which the uranyl moiety deviates from linearity to varying degrees, and in particular systems containing the UO$_2$Cl$_2$ group such as UO$_2$Cl$_2$.n(H$_2$O) and UO$_2$Cl$_2$(phen)$_2$, which in the latter case exhibits a pronounced O-U-O bending angle. These U M$_4$ edge HERFD-XANES spectra are compared to those of other linear (Cs$_2$UO$_2$Cl$_4$) or pseudo-linear ([UO$_2$(NO$_3$)$_2$.n(H$_2$O)]) uranyl complexes. This evaluation is complemented by ab initio relativistic quantum chemistry simulations using 2-component Time-Dependent Density Functional Theory (TD-DFT) with the CAM-B3LYP functional, employing the Tamm-Dancoff approximation (2c-TDA). Our 2c-TDA simulations show modest deviations from the HERFD-XANES data, with peak splittings differing by less than 1 eV from experimental values. These core-excited states were further characterized by Natural Transition Orbital (NTO) analysis. Overall, our results highlight the influence of equatorial ligands on the spectroscopic signatures, particularly pronounced in UO$_2$Cl$_2$(phen)$<em>2$, where the U 3d${</em>{3&#x2F;2}} \rightarrow$ $5f$ $\sigma{_u}^{*}$ satellite transition appears at lower energies compared to the other systems studied. </p>
<blockquote>
<p>é«˜çº§Xå°„çº¿å…‰è°±æŠ€æœ¯è¢«å…¬è®¤ä¸ºæ˜¯æ¢ç´¢å‘¨æœŸè¡¨ä¸­é‡å…ƒç´ çš„ç”µå­ç»“æ„ã€é”®åˆå’ŒåŒ–å­¦ç¯å¢ƒçš„æœ€æ–°å·¥å…·ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨é«˜èƒ½é‡åˆ†è¾¨ç‡è§å…‰æ£€æµ‹ï¼ˆHERFDï¼‰æ¨¡å¼ä¸‹çš„Xå°„çº¿å¸æ”¶è¿‘è¾¹ç¼˜ç»“æ„æµ‹é‡æŠ€æœ¯ï¼Œç ”ç©¶åç¦»çº¿æ€§çš„é“€é…°åˆ†å­å¤åˆç‰©çš„æ ¸å¿ƒçŠ¶æ€ï¼Œç‰¹åˆ«æ˜¯å«æœ‰UO2Cl2åŸºå›¢çš„ç³»ç»Ÿï¼Œå¦‚UO2Cl2.nï¼ˆH2Oï¼‰å’ŒUO2Cl2ï¼ˆphenï¼‰2ã€‚åœ¨åä¸€ç§æƒ…å†µä¸‹ï¼Œè¡¨ç°å‡ºæ˜æ˜¾çš„O-U-Oå¼¯æ›²è§’ã€‚æˆ‘ä»¬å°†è¿™äº›UM4è¾¹ç¼˜çš„HERFD-XANESå…‰è°±ä¸çº¿æ€§é“€é…°é…åˆç‰©ï¼ˆå¦‚Cs2UO2Cl4ï¼‰æˆ–ä¼ªçº¿æ€§é…åˆç‰©ï¼ˆå¦‚UO2ï¼ˆNO3ï¼‰2.nï¼ˆH2Oï¼‰ï¼‰çš„å…‰è°±è¿›è¡Œæ¯”è¾ƒã€‚è¿™ä¸€è¯„ä¼°è¾…ä»¥åŸºäºæ—¶é—´çš„ä»å¤´è®¡ç®—ç›¸å¯¹è®ºé‡å­åŒ–å­¦æ¨¡æ‹Ÿï¼Œé‡‡ç”¨å«æ—¶å¯†åº¦æ³›å‡½ç†è®ºï¼ˆTD-DFTï¼‰çš„CAM-B3LYPåŠŸèƒ½ï¼Œé‡‡ç”¨Tamm-Dancoffè¿‘ä¼¼ï¼ˆ2c-TDAï¼‰ã€‚æˆ‘ä»¬çš„2c-TDAæ¨¡æ‹Ÿä¸HERFD-XANESæ•°æ®ç•¥æœ‰åå·®ï¼Œå³°å€¼åˆ†è£‚ä¸å®éªŒå€¼ç›¸å·®ä¸åˆ°1ç”µå­ä¼ç‰¹ã€‚è¿™äº›æ ¸å¿ƒæ¿€å‘æ€è¿›ä¸€æ­¥é€šè¿‡è‡ªç„¶è·ƒè¿è½¨é“ï¼ˆNTOï¼‰åˆ†æè¡¨å¾ã€‚æ€»ä½“è€Œè¨€ï¼Œæˆ‘ä»¬çš„ç»“æœçªå‡ºäº†èµ¤é“é…ä½“å¯¹å…‰è°±ç‰¹å¾çš„å½±å“ï¼Œç‰¹åˆ«æ˜¯åœ¨UO2Cl2ï¼ˆphenï¼‰2ä¸­å°¤ä¸ºæ˜æ˜¾ï¼Œå…¶ä¸­U 3d 3&#x2F;2 â†’ 5f Ïƒu*å«æ˜Ÿè¿‡æ¸¡å‡ºç°åœ¨æ¯”å…¶ä»–ç³»ç»Ÿæ›´ä½çš„èƒ½é‡å¤„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05542v2">PDF</a> 27 pages, 9 figures, 3 tables</p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶é‡‡ç”¨é«˜èƒ½é‡åˆ†è¾¨ç‡è§å…‰æ£€æµ‹ï¼ˆHERFD-XANESï¼‰æ¨¡å¼çš„Xå°„çº¿å¸æ”¶è¿‘è¾¹ç»“æ„æµ‹é‡æŠ€æœ¯ï¼Œå¯¹ä¸åŒç¨‹åº¦åç¦»çº¿æ€§çš„é“€é…°åˆ†å­å¤åˆç‰©çš„æ ¸å¿ƒçŠ¶æ€è¿›è¡Œç ”ç©¶ï¼Œç‰¹åˆ«æ˜¯å¯¹å«æœ‰UO2Cl2åŸºå›¢çš„ç³»ç»Ÿï¼Œå¦‚UO2Cl2Â·n(H2O)å’ŒUO2Cl2(phen)2ã€‚é€šè¿‡ä¸å…¶ä»–çº¿æ€§æˆ–ä¼ªçº¿æ€§é“€é…°å¤åˆç‰©çš„æ¯”è¾ƒï¼Œå¹¶ç»“åˆåŸºäºæ—¶é—´çš„å¯†åº¦æ³›å‡½ç†è®ºï¼ˆTD-DFTï¼‰è¿›è¡Œæ¨¡æ‹Ÿåˆ†æï¼Œæ­ç¤ºäº†èµ¤é“é…ä½“å¯¹å…‰è°±ç‰¹å¾çš„å½±å“ï¼Œç‰¹åˆ«æ˜¯åœ¨UO2Cl2(phen)2ä¸­è¡¨ç°å°¤ä¸ºçªå‡ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬ç ”ç©¶ä½¿ç”¨HERFD-XANESæŠ€æœ¯å¯¹é“€é…°åˆ†å­å¤åˆç‰©çš„ç”µå­ç»“æ„ã€é”®åˆå’ŒåŒ–å­¦ç¯å¢ƒè¿›è¡Œäº†ç ”ç©¶ã€‚</li>
<li>ç ”ç©¶å¯¹è±¡åŒ…æ‹¬ä¸åŒç¨‹åº¦åç¦»çº¿æ€§çš„é“€é…°åˆ†å­å¤åˆç‰©ï¼Œç‰¹åˆ«æ˜¯å«æœ‰UO2Cl2åŸºå›¢çš„ç³»ç»Ÿã€‚</li>
<li>é€šè¿‡ä¸å…¶ä»–çº¿æ€§æˆ–ä¼ªçº¿æ€§é“€é…°å¤åˆç‰©çš„æ¯”è¾ƒï¼Œå¯¹å…‰è°±ç‰¹å¾è¿›è¡Œäº†æ·±å…¥åˆ†æã€‚</li>
<li>ä½¿ç”¨åŸºäºæ—¶é—´çš„å¯†åº¦æ³›å‡½ç†è®ºï¼ˆTD-DFTï¼‰è¿›è¡Œæ¨¡æ‹Ÿåˆ†æï¼Œæ­ç¤ºäº†æ ¸å¿ƒçŠ¶æ€çš„ä¸€äº›ç‰¹å¾ã€‚</li>
<li>æ¨¡æ‹Ÿç»“æœä¸­èµ¤é“é…ä½“çš„å½±å“å°¤ä¸ºçªå‡ºï¼Œç‰¹åˆ«æ˜¯åœ¨UO2Cl2(phen)2ç³»ç»Ÿä¸­ã€‚</li>
<li>U 3d_{3&#x2F;2}è‡³$5f$ $\sigma{_u}^{*}$å«æ˜Ÿè·ƒè¿çš„èƒ½é‡è¾ƒä½ï¼Œä¸å…¶ä»–ç³»ç»Ÿç›¸æ¯”å…·æœ‰æ˜¾è‘—ç‰¹å¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05542">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-17\./crop_åŒ»å­¦å›¾åƒ/2504.05542v2/page_0_0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Mosaic3D-Foundation-Dataset-and-Model-for-Open-Vocabulary-3D-Segmentation"><a href="#Mosaic3D-Foundation-Dataset-and-Model-for-Open-Vocabulary-3D-Segmentation" class="headerlink" title="Mosaic3D: Foundation Dataset and Model for Open-Vocabulary 3D   Segmentation"></a>Mosaic3D: Foundation Dataset and Model for Open-Vocabulary 3D   Segmentation</h2><p><strong>Authors:Junha Lee, Chunghyun Park, Jaesung Choe, Yu-Chiang Frank Wang, Jan Kautz, Minsu Cho, Chris Choy</strong></p>
<p>We tackle open-vocabulary 3D scene understanding by introducing a novel data generation pipeline and training framework. Our method addresses three critical requirements for effective training: precise 3D region segmentation, comprehensive textual descriptions, and sufficient dataset scale. By leveraging state-of-the-art open-vocabulary image segmentation models and region-aware Vision-Language Models, we develop an automatic pipeline that generates high-quality 3D mask-text pairs. Applying this pipeline to multiple 3D scene datasets, we create Mosaic3D-5.6M, a dataset of over 30K annotated scenes with 5.6M mask-text pairs, significantly larger than existing datasets. Building upon this data, we propose Mosaic3D, a foundation model combining a 3D encoder trained with contrastive learning and a lightweight mask decoder for open-vocabulary 3D semantic and instance segmentation. Our approach achieves state-of-the-art results on open-vocabulary 3D semantic and instance segmentation tasks including ScanNet200, Matterport3D, and ScanNet++, with ablation studies validating the effectiveness of our large-scale training data. </p>
<blockquote>
<p>æˆ‘ä»¬é€šè¿‡å¼•å…¥æ–°å‹æ•°æ®ç”Ÿæˆæµç¨‹å’Œè®­ç»ƒæ¡†æ¶æ¥è§£å†³å¼€æ”¾å¼è¯æ±‡è¡¨ä¸­çš„ä¸‰ç»´åœºæ™¯ç†è§£é—®é¢˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ»¡è¶³äº†æœ‰æ•ˆè®­ç»ƒçš„ä¸‰ä¸ªå…³é”®éœ€æ±‚ï¼šç²¾ç¡®çš„3DåŒºåŸŸåˆ†å‰²ã€å…¨é¢çš„æ–‡æœ¬æè¿°å’Œè¶³å¤Ÿçš„æ•°æ®é›†è§„æ¨¡ã€‚æˆ‘ä»¬å€ŸåŠ©æœ€æ–°å…ˆè¿›å¼€æ”¾å¼è¯æ±‡è¡¨çš„å›¾åƒåˆ†å‰²æ¨¡å‹å’ŒåŒºåŸŸæ„ŸçŸ¥è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå¼€å‘äº†ä¸€ä¸ªè‡ªåŠ¨ç®¡é“ï¼Œç”Ÿæˆé«˜è´¨é‡çš„ä¸‰ç»´æ©è†œ-æ–‡æœ¬å¯¹ã€‚å°†æ­¤ç®¡é“åº”ç”¨äºå¤šä¸ªä¸‰ç»´åœºæ™¯æ•°æ®é›†ï¼Œæˆ‘ä»¬åˆ›å»ºäº†Mosaic3D-5.6Mæ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡3ä¸‡ä¸ªæ³¨é‡Šåœºæ™¯å’Œ560ä¸‡ä¸ªæ©è†œ-æ–‡æœ¬å¯¹ï¼Œæ˜¾è‘—å¤§äºç°æœ‰æ•°æ®é›†ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†Mosaic3Dæ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ä¸ªç»“åˆäº†é€šè¿‡å¯¹æ¯”å­¦ä¹ è®­ç»ƒçš„3Dç¼–ç å™¨å’Œç”¨äºå¼€æ”¾å¼è¯æ±‡è¡¨çš„ä¸‰ç»´è¯­ä¹‰å’Œå®ä¾‹åˆ†å‰²çš„è½»é‡çº§æ©è†œè§£ç å™¨çš„åŸºç¡€æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¼€æ”¾å¼è¯æ±‡è¡¨çš„ä¸‰ç»´è¯­ä¹‰å’Œå®ä¾‹åˆ†å‰²ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€æ–°ç»“æœï¼ŒåŒ…æ‹¬ScanNet200ã€Matterport3Då’ŒScanNet++ç­‰æ•°æ®é›†ï¼Œå¹¶è¿›è¡Œäº†æ•ˆæœéªŒè¯çš„å¤§è§„æ¨¡è®­ç»ƒæ•°æ®æ¶ˆèç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.02548v2">PDF</a> project page: <a target="_blank" rel="noopener" href="https://nvlabs.github.io/Mosaic3D/">https://nvlabs.github.io/Mosaic3D/</a></p>
<p><strong>Summary</strong></p>
<p>æ–°ä¸€ä»£æ•°æ®ç”Ÿæˆç®¡é“å’Œè®­ç»ƒæ¡†æ¶è§£å†³äº†å¼€æ”¾è¯æ±‡è¡¨çš„ä¸‰ç»´åœºæ™¯ç†è§£é—®é¢˜ã€‚é€šè¿‡ç²¾ç¡®çš„ä¸‰ç»´åŒºåŸŸåˆ†å‰²ã€å…¨é¢çš„æ–‡æœ¬æè¿°å’Œè¶³å¤Ÿçš„æ•°æ®é›†è§„æ¨¡ï¼Œç”Ÿæˆé«˜è´¨é‡çš„ä¸‰ç»´æ©è†œ-æ–‡æœ¬å¯¹ã€‚åˆ›å»ºäº†Mosaic3D-5.6Mæ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡3ä¸‡æ ‡æ³¨åœºæ™¯å’Œ560ä¸‡æ©è†œ-æ–‡æœ¬å¯¹ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæå‡ºäº†Mosaic3Dæ¨¡å‹ï¼Œç»“åˆä¸‰ç»´ç¼–ç å™¨å¯¹æ¯”å­¦ä¹ å’Œè½»é‡çº§æ©è†œè§£ç å™¨ï¼Œå®ç°å¼€æ”¾è¯æ±‡è¡¨çš„ä¸‰ç»´è¯­ä¹‰å’Œå®ä¾‹åˆ†å‰²çš„æœ€ä½³ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥æ–°å‹æ•°æ®ç”Ÿæˆç®¡é“å’Œè®­ç»ƒæ¡†æ¶ï¼Œè§£å†³å¼€æ”¾è¯æ±‡è¡¨çš„ä¸‰ç»´åœºæ™¯ç†è§£éš¾é¢˜ã€‚</li>
<li>æ–¹æ³•æ»¡è¶³ä¸‰ä¸ªå…³é”®è®­ç»ƒè¦æ±‚ï¼šç²¾ç¡®ä¸‰ç»´åŒºåŸŸåˆ†å‰²ã€å…¨é¢æ–‡æœ¬æè¿°å’Œè¶³å¤Ÿçš„æ•°æ®é›†è§„æ¨¡ã€‚</li>
<li>åˆ©ç”¨å…ˆè¿›å¼€æ”¾è¯æ±‡è¡¨çš„å›¾åƒåˆ†å‰²æ¨¡å‹å’ŒåŒºåŸŸæ„ŸçŸ¥è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œè‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡çš„ä¸‰ç»´æ©è†œ-æ–‡æœ¬å¯¹ã€‚</li>
<li>åˆ›å»ºäº†Mosaic3D-5.6Mæ•°æ®é›†ï¼ŒåŒ…å«å¤§é‡æ ‡æ³¨åœºæ™¯å’Œæ©è†œ-æ–‡æœ¬å¯¹ï¼Œæ˜¾è‘—å¤§äºç°æœ‰æ•°æ®é›†ã€‚</li>
<li>æå‡ºMosaic3Dæ¨¡å‹ï¼Œç»“åˆä¸‰ç»´ç¼–ç å™¨å’Œæ©è†œè§£ç å™¨ï¼Œç”¨äºå¼€æ”¾è¯æ±‡è¡¨çš„ä¸‰ç»´è¯­ä¹‰å’Œå®ä¾‹åˆ†å‰²ã€‚</li>
<li>æ¨¡å‹åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¾¾åˆ°å¼€æ”¾è¯æ±‡è¡¨çš„ä¸‰ç»´è¯­ä¹‰å’Œå®ä¾‹åˆ†å‰²çš„æœ€ä½³ç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.02548">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-17\./crop_åŒ»å­¦å›¾åƒ/2502.02548v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-17\./crop_åŒ»å­¦å›¾åƒ/2502.02548v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-17\./crop_åŒ»å­¦å›¾åƒ/2502.02548v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-17\./crop_åŒ»å­¦å›¾åƒ/2502.02548v2/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-17\./crop_åŒ»å­¦å›¾åƒ/2502.02548v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-17\./crop_åŒ»å­¦å›¾åƒ/2502.02548v2/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-17\./crop_åŒ»å­¦å›¾åƒ/2502.02548v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="AMBER-â€“-Advanced-SegFormer-for-Multi-Band-Image-Segmentation-an-application-to-Hyperspectral-Imaging"><a href="#AMBER-â€“-Advanced-SegFormer-for-Multi-Band-Image-Segmentation-an-application-to-Hyperspectral-Imaging" class="headerlink" title="AMBER â€“ Advanced SegFormer for Multi-Band Image Segmentation: an   application to Hyperspectral Imaging"></a>AMBER â€“ Advanced SegFormer for Multi-Band Image Segmentation: an   application to Hyperspectral Imaging</h2><p><strong>Authors:Andrea Dosi, Massimo Brescia, Stefano Cavuoti, Mariarca Dâ€™Aniello, Michele Delli Veneri, Carlo Donadio, Adriano Ettari, Giuseppe Longo, Alvi Rownok, Luca Sannino, Maria Zampella</strong></p>
<p>Deep learning has revolutionized the field of hyperspectral image (HSI) analysis, enabling the extraction of complex spectral and spatial features. While convolutional neural networks (CNNs) have been the backbone of HSI classification, their limitations in capturing global contextual features have led to the exploration of Vision Transformers (ViTs). This paper introduces AMBER, an advanced SegFormer specifically designed for multi-band image segmentation. AMBER enhances the original SegFormer by incorporating three-dimensional convolutions, custom kernel sizes, and a Funnelizer layer. This architecture enables processing hyperspectral data directly, without requiring spectral dimensionality reduction during preprocessing. Our experiments, conducted on three benchmark datasets (Salinas, Indian Pines, and Pavia University) and on a dataset from the PRISMA satellite, show that AMBER outperforms traditional CNN-based methods in terms of Overall Accuracy, Kappa coefficient, and Average Accuracy on the first three datasets, and achieves state-of-the-art performance on the PRISMA dataset. These findings highlight AMBERâ€™s robustness, adaptability to both airborne and spaceborne data, and its potential as a powerful solution for remote sensing and other domains requiring advanced analysis of high-dimensional data. </p>
<blockquote>
<p>æ·±åº¦å­¦ä¹ å·²ç»å½»åº•æ”¹å˜äº†é«˜å…‰è°±å›¾åƒï¼ˆHSIï¼‰åˆ†æé¢†åŸŸï¼Œèƒ½å¤Ÿå®ç°å¤æ‚å…‰è°±å’Œç©ºé—´ç‰¹å¾çš„æå–ã€‚è™½ç„¶å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å·²æˆä¸ºHSIåˆ†ç±»çš„æ”¯æŸ±ï¼Œä½†åœ¨æ•æ‰å…¨å±€ä¸Šä¸‹æ–‡ç‰¹å¾æ–¹é¢çš„å±€é™æ€§ä¿ƒä½¿äººä»¬æ¢ç´¢è§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰ã€‚æœ¬æ–‡ä»‹ç»äº†AMBERï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºå¤šæ³¢æ®µå›¾åƒåˆ†å‰²è®¾è®¡çš„å…ˆè¿›SegFormerã€‚AMBERé€šè¿‡å¼•å…¥ä¸‰ç»´å·ç§¯ã€è‡ªå®šä¹‰å†…æ ¸å¤§å°å’Œæ¼æ–—å±‚å¢å¼ºäº†åŸå§‹SegFormerã€‚è¯¥æ¶æ„èƒ½å¤Ÿç›´æ¥å¤„ç†é«˜å…‰è°±æ•°æ®ï¼Œæ— éœ€åœ¨é¢„å¤„ç†é˜¶æ®µè¿›è¡Œå…‰è°±ç»´åº¦ç¼©å‡ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ï¼ˆSalinasã€Indian Pineså’ŒPavia Universityï¼‰ä»¥åŠPRISMAå«æ˜Ÿæ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒAMBERåœ¨æ€»ä½“ç²¾åº¦ã€Kappaç³»æ•°å’Œå¹³å‡ç²¾åº¦æ–¹é¢ä¼˜äºä¼ ç»Ÿçš„CNNæ–¹æ³•ï¼Œåœ¨PRISMAæ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¿™äº›å‘ç°çªå‡ºäº†AMBERçš„ç¨³å¥æ€§ï¼Œä»¥åŠå¯¹ç©ºä¸­å’Œå¤ªç©ºæ•°æ®çš„é€‚åº”æ€§ï¼Œä»¥åŠå…¶åœ¨é¥æ„Ÿå’Œå…¶ä»–éœ€è¦é«˜çº§åˆ†æé«˜ç»´æ•°æ®çš„é¢†åŸŸä¸­çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.09386v2">PDF</a> submitted to Neural Computing &amp; Applications (Springer). Accepted   with minor revisions</p>
<p><strong>Summary</strong></p>
<p>æ·±åº¦å­¦ä¹ åœ¨è¶…å…‰è°±å›¾åƒï¼ˆHSIï¼‰åˆ†æé¢†åŸŸå¼•èµ·äº†é©å‘½æ€§çš„å˜é©ï¼Œä½¿å¤æ‚å…‰è°±å’Œç©ºé—´ç‰¹å¾çš„æå–æˆä¸ºå¯èƒ½ã€‚è™½ç„¶å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ˜¯HSIåˆ†ç±»çš„åŸºçŸ³ï¼Œä½†åœ¨æ•æ‰å…¨å±€ä¸Šä¸‹æ–‡ç‰¹å¾æ–¹é¢çš„å±€é™æ€§ä¿ƒä½¿äº†å¯¹è§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰çš„æ¢ç´¢ã€‚æœ¬æ–‡ä»‹ç»äº†AMBERï¼Œè¿™æ˜¯ä¸€ç§ä¸“ä¸ºå¤šæ³¢æ®µå›¾åƒåˆ†å‰²è®¾è®¡çš„å…ˆè¿›SegFormerã€‚AMBERé€šè¿‡å¼•å…¥ä¸‰ç»´å·ç§¯ã€è‡ªå®šä¹‰å†…æ ¸å¤§å°å’Œæ¼æ–—å±‚ï¼ˆFunnelizer layerï¼‰å¢å¼ºäº†åŸå§‹SegFormerã€‚è¯¥æ¶æ„èƒ½å¤Ÿç›´æ¥å¤„ç†è¶…å…‰è°±æ•°æ®ï¼Œæ— éœ€åœ¨é¢„å¤„ç†é˜¶æ®µé™ä½å…‰è°±ç»´åº¦ã€‚å®éªŒè¡¨æ˜ï¼ŒAMBERåœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ï¼ˆSalinasã€Indian Pineså’ŒPavia Universityï¼‰å’ŒPRISMAå«æ˜Ÿæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºä¼ ç»ŸåŸºäºCNNçš„æ–¹æ³•ï¼Œæ•´ä½“ç²¾åº¦ã€Kappaç³»æ•°å’Œå¹³å‡ç²¾åº¦å‡æœ‰æ˜¾è‘—æé«˜ï¼Œå±•ç°äº†å…¶ç¨³å¥æ€§ã€é€‚åº”ç©ºä¸­å’Œå¤ªç©ºæ•°æ®çš„èƒ½åŠ›ï¼Œä»¥åŠä½œä¸ºé¥æ„Ÿå’Œå…¶ä»–éœ€è¦é«˜çº§é«˜ç»´æ•°æ®åˆ†æé¢†åŸŸçš„å¼ºå¤§è§£å†³æ–¹æ¡ˆçš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ åœ¨è¶…å…‰è°±å›¾åƒåˆ†æä¸­å…·æœ‰é©å‘½æ€§ä½œç”¨ï¼Œèƒ½å¤Ÿæå–å¤æ‚çš„å…‰è°±å’Œç©ºé—´ç‰¹å¾ã€‚</li>
<li>è™½ç„¶å·ç§¯ç¥ç»ç½‘ç»œåœ¨HSIåˆ†ç±»ä¸­æ˜¯åŸºçŸ³ï¼Œä½†å®ƒä»¬å­˜åœ¨æ•æ‰å…¨å±€ä¸Šä¸‹æ–‡ç‰¹å¾çš„å±€é™æ€§ã€‚</li>
<li>Vision Transformersï¼ˆViTsï¼‰è¢«æ¢ç´¢ç”¨äºè§£å†³CNNçš„å±€é™æ€§ã€‚</li>
<li>è®ºæ–‡ä»‹ç»äº†AMBERï¼Œä¸€ä¸ªåŸºäºSegFormerçš„é«˜çº§æ¨¡å‹ï¼Œå…·æœ‰ç›´æ¥å¤„ç†è¶…å…‰è°±æ•°æ®çš„èƒ½åŠ›ã€‚</li>
<li>AMBERé€šè¿‡å¼•å…¥ä¸‰ç»´å·ç§¯ã€è‡ªå®šä¹‰å†…æ ¸å¤§å°å’Œæ¼æ–—å±‚å¢å¼ºäº†æ€§èƒ½ã€‚</li>
<li>AMBERåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºä¼ ç»ŸCNNæ–¹æ³•ï¼Œæ˜¾ç¤ºå‡ºå…¶å¼ºå¤§çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.09386">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-17\./crop_åŒ»å­¦å›¾åƒ/2409.09386v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-17\./crop_åŒ»å­¦å›¾åƒ/2409.09386v2/page_4_0.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Real-Time-Image-Analysis-Software-Suitable-for-Resource-Constrained-Computing"><a href="#Real-Time-Image-Analysis-Software-Suitable-for-Resource-Constrained-Computing" class="headerlink" title="Real-Time Image Analysis Software Suitable for Resource-Constrained   Computing"></a>Real-Time Image Analysis Software Suitable for Resource-Constrained   Computing</h2><p><strong>Authors:Alexandre Matov</strong></p>
<p>Methods: We have developed a software suite (DataSet Tracker) for real-time analysis designed to run on computers, smartphones, and smart glasses hardware and suitable for resource-constrained, on-the-fly computing in microscopes without internet connectivity; a demo is available for viewing at datasetanalysis.com. Our objective is to present the community with an integrated, easy to use by all, tool for resolving the complex dynamics of the cytoskeletal meshworks, intracytoplasmic membranous networks, and vesicle trafficking. Our software is optimized for resource-constrained computing and can be installed even on microscopes without internet connectivity.   Results: Our computational platform can provide high-content analyses and functional secondary screening of novel compounds that are in the process of approval, or at a pre-clinical stage of development, and putative combination therapies based on FDA-approved drugs. Importantly, dissecting the mechanisms of drug action with quantitative detail will allow the design of drugs that impede relapse and optimal dose regimens with minimal harmful side effects by carefully exploiting disease-specific aberrations.   Conclusions: DataSet Tracker, the real-time optical flow feature tracking software presented in this contribution, can serve as the base module of an integrated platform of existing and future algorithms for real-time cellular analysis. The computational assay we propose could successfully be applied to evaluate treatment strategies for any human organ. It is our goal to have this integrated tool approved for use in the clinical practice. </p>
<blockquote>
<p><strong>æ–¹æ³•</strong>ï¼šæˆ‘ä»¬å¼€å‘äº†ä¸€å¥—åä¸ºDataSet Trackerçš„å®æ—¶åˆ†æè½¯ä»¶å¥—ä»¶ï¼Œæ—¨åœ¨åœ¨è®¡ç®—æœºã€æ™ºèƒ½æ‰‹æœºå’Œæ™ºèƒ½çœ¼é•œç­‰ç¡¬ä»¶ä¸Šè¿è¡Œï¼Œé€‚ç”¨äºæ— ç½‘ç»œè¿æ¥æ˜¾å¾®é•œä¸‹çš„èµ„æºå—é™ã€å³æ—¶è®¡ç®—ã€‚å¯ä»¥åœ¨datasetanalysis.comä¸ŠæŸ¥çœ‹æ¼”ç¤ºç‰ˆæœ¬ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯å‘ç ”ç©¶ç¾¤ä½“æä¾›ä¸€ä¸ªé›†æˆå·¥å…·ï¼Œè§£å†³ç»†èƒéª¨æ¶ç½‘æ ¼ã€ç»†èƒå†…è†œç½‘ç»œå’Œå›Šæ³¡è¿è¾“çš„å¤æ‚åŠ¨æ€é—®é¢˜ï¼Œè¯¥å·¥å…·æ˜“äºæ‰€æœ‰äººä½¿ç”¨ã€‚æˆ‘ä»¬çš„è½¯ä»¶é’ˆå¯¹èµ„æºå—é™è®¡ç®—è¿›è¡Œäº†ä¼˜åŒ–ï¼Œç”šè‡³å¯ä»¥åœ¨æ²¡æœ‰ç½‘ç»œè¿æ¥çš„æ˜¾å¾®é•œä¸Šå®‰è£…ä½¿ç”¨ã€‚</p>
</blockquote>
<p><strong>ç»“æœ</strong>ï¼šæˆ‘ä»¬çš„è®¡ç®—å¹³å°å¯ä»¥æä¾›é«˜å†…æ¶µåˆ†æä»¥åŠæ–°è¯åœ¨è·æ‰¹è¿‡ç¨‹ä¸­æˆ–å¤„äºé¢„ä¸´åºŠç ”ç©¶é˜¶æ®µçš„åŠŸèƒ½äºŒæ¬¡ç­›é€‰ï¼Œä»¥åŠåŸºäºFDAæ‰¹å‡†è¯ç‰©çš„æ½œåœ¨è”åˆç–—æ³•ã€‚é‡è¦çš„æ˜¯ï¼Œä»¥å®šé‡ç»†èŠ‚åˆ†æè¯ç‰©ä½œç”¨æœºåˆ¶ï¼Œå¯è®¾è®¡é˜»æ­¢å¤å‘è¯ç‰©å’Œæœ€ä½³å‰‚é‡æ–¹æ¡ˆï¼Œé€šè¿‡ç²¾å¿ƒåˆ©ç”¨ç–¾ç—…ç‰¹å¼‚æ€§å¼‚å¸¸è¾¾åˆ°å°†æœ‰å®³å‰¯ä½œç”¨é™è‡³æœ€ä½çš„ç›®çš„ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.15735v9">PDF</a> </p>
<p><strong>Summary</strong><br>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç”¨äºå®æ—¶åˆ†æçš„è½¯ä»¶å¥—ä»¶ï¼ˆDataSet Trackerï¼‰ï¼Œå¯åœ¨è®¡ç®—æœºã€æ™ºèƒ½æ‰‹æœºå’Œæ™ºèƒ½çœ¼é•œä¸Šè¿è¡Œï¼Œé€‚ç”¨äºèµ„æºå—é™ã€æ— éœ€äº’è”ç½‘è¿æ¥çš„æ˜¾å¾®é•œç¯å¢ƒä¸‹çš„ç§»åŠ¨è®¡ç®—ã€‚è¯¥è½¯ä»¶çš„ç›®çš„æ˜¯ä¸ºç¤¾åŒºæä¾›ä¸€ä¸ªé›†æˆå·¥å…·ï¼Œè§£å†³ç»†èƒéª¨æ¶ç½‘æ ¼ã€èƒè´¨å†…è†œç½‘ç»œå’Œå›Šæ³¡è¿è¾“çš„å¤æ‚åŠ¨æ€é—®é¢˜ã€‚è¯¥è½¯ä»¶å¯ä¼˜åŒ–èµ„æºå—é™çš„è®¡ç®—ç¯å¢ƒï¼Œå³ä½¿åœ¨æ— äº’è”ç½‘è¿æ¥çš„æ˜¾å¾®é•œä¸‹ä¹Ÿèƒ½å®‰è£…ä½¿ç”¨ã€‚è®¡ç®—å¹³å°å¯ä»¥æä¾›é«˜å†…å®¹åˆ†æå’ŒåŠŸèƒ½äºŒçº§ç­›é€‰ï¼ŒåŸºäºFDAæ‰¹å‡†çš„è¯ç‰©è¿›è¡Œæ–°å‹åŒ–åˆç‰©çš„é¢„å…ˆä¸´åºŠé˜¶æ®µå¼€å‘ä»¥åŠæ½œåœ¨è”åˆç–—æ³•ã€‚è¯¥è½¯ä»¶èƒ½å¤Ÿè¯¦ç»†å®šé‡åœ°å‰–æè¯ç‰©ä½œç”¨æœºåˆ¶ï¼Œä»¥ä¾¿è®¾è®¡èƒ½å¤Ÿé˜»ç¢å¤å‘ã€æœ€ä½³å‰‚é‡æ–¹æ¡ˆçš„è¯ç‰©ï¼Œå¹¶é€šè¿‡åˆ©ç”¨ç–¾ç—…ç‰¹å¼‚æ€§å¼‚å¸¸æ¥å‡å°‘æœ‰å®³çš„å‰¯ä½œç”¨ã€‚DataSet Trackerå¯ä»¥ä½œä¸ºç°æœ‰å’Œæœªæ¥ç®—æ³•çš„é›†æˆå¹³å°çš„åŸºç¡€æ¨¡å—ï¼Œç”¨äºå®æ—¶ç»†èƒåˆ†æã€‚æ‰€æå‡ºçš„è®¡ç®—æµ‹å®šæ³•å¯æˆåŠŸåº”ç”¨äºè¯„ä¼°ä»»ä½•äººç±»å™¨å®˜çš„æ²»ç–—ç­–ç•¥ï¼Œç›®æ ‡æ˜¯åœ¨ä¸´åºŠå®è·µä¸­è·å¾—æ‰¹å‡†ä½¿ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼€å‘äº†ä¸€ç§å®æ—¶åˆ†æè½¯ä»¶DataSet Trackerï¼Œå¯åœ¨å¤šç§è®¾å¤‡ä¸Šè¿è¡Œï¼ŒåŒ…æ‹¬è®¡ç®—æœºã€æ™ºèƒ½æ‰‹æœºå’Œæ™ºèƒ½çœ¼é•œã€‚</li>
<li>é€‚åˆèµ„æºå—é™ç¯å¢ƒï¼Œå¯åœ¨æ— äº’è”ç½‘è¿æ¥çš„æ˜¾å¾®é•œç¯å¢ƒä¸‹ä½¿ç”¨ã€‚</li>
<li>æ—¨åœ¨è§£å†³ç»†èƒéª¨æ¶ç½‘æ ¼ã€èƒè´¨å†…è†œç½‘ç»œå’Œå›Šæ³¡è¿è¾“ç­‰å¤æ‚åŠ¨æ€é—®é¢˜ã€‚</li>
<li>å¯è¿›è¡Œé«˜å†…å®¹åˆ†æå’ŒåŠŸèƒ½äºŒçº§ç­›é€‰ï¼Œé€‚ç”¨äºæ–°è¯å¼€å‘è¿‡ç¨‹ä¸­çš„è¯ç‰©ç»„åˆç–—æ³•è¯„ä¼°ã€‚</li>
<li>èƒ½å¤Ÿè¯¦ç»†å®šé‡åœ°å‰–æè¯ç‰©ä½œç”¨æœºåˆ¶ï¼Œä¸ºè®¾è®¡å‡å°‘å¤å‘å’Œä¼˜åŒ–å‰‚é‡æ–¹æ¡ˆçš„è¯ç‰©æä¾›ä¾æ®ã€‚</li>
<li>DataSet Trackerå¯ä½œä¸ºé›†æˆå¹³å°çš„åŸºç¡€æ¨¡å—ï¼Œç”¨äºå®æ—¶ç»†èƒåˆ†æçš„æœªæ¥ç®—æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.15735">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-17\./crop_åŒ»å­¦å›¾åƒ/2407.15735v9/page_0_0.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="MedMerge-Merging-Models-for-Effective-Transfer-Learning-to-Medical-Imaging-Tasks"><a href="#MedMerge-Merging-Models-for-Effective-Transfer-Learning-to-Medical-Imaging-Tasks" class="headerlink" title="MedMerge: Merging Models for Effective Transfer Learning to Medical   Imaging Tasks"></a>MedMerge: Merging Models for Effective Transfer Learning to Medical   Imaging Tasks</h2><p><strong>Authors:Ibrahim Almakky, Santosh Sanjeev, Anees Ur Rehman Hashmi, Mohammad Areeb Qazi, Hu Wang, Mohammad Yaqub</strong></p>
<p>Transfer learning has become a powerful tool to initialize deep learning models to achieve faster convergence and higher performance. This is especially useful in the medical imaging analysis domain, where data scarcity limits possible performance gains for deep learning models. Some advancements have been made in boosting the transfer learning performance gain by merging models starting from the same initialization. However, in the medical imaging analysis domain, there is an opportunity to merge models starting from different initializations, thus combining the features learned from different tasks. In this work, we propose MedMerge, a method whereby the weights of different models can be merged, and their features can be effectively utilized to boost performance on a new task. With MedMerge, we learn kernel-level weights that can later be used to merge the models into a single model, even when starting from different initializations. Testing on various medical imaging analysis tasks, we show that our merged model can achieve significant performance gains, with up to 7% improvement on the F1 score. The code implementation of this work is available at github.com&#x2F;BioMedIA-MBZUAI&#x2F;MedMerge. </p>
<blockquote>
<p>è¿ç§»å­¦ä¹ å·²ç»æˆä¸ºåˆå§‹åŒ–æ·±åº¦å­¦ä¹ æ¨¡å‹ä»¥æ›´å¿«æ”¶æ•›å’Œæé«˜æ€§èƒ½çš„å¼ºå¤§å·¥å…·ã€‚è¿™åœ¨åŒ»å­¦æˆåƒåˆ†æé¢†åŸŸå°¤å…¶æœ‰ç”¨ï¼Œå› ä¸ºæ•°æ®ç¨€ç¼ºé™åˆ¶äº†æ·±åº¦å­¦ä¹ æ¨¡å‹çš„æ€§èƒ½æå‡ã€‚ä¸€äº›è¿›æ­¥æ˜¯é€šè¿‡åˆå¹¶ä»åŒä¸€åˆå§‹åŒ–å¼€å§‹çš„æ¨¡å‹æ¥æé«˜è¿ç§»å­¦ä¹ æ€§èƒ½å¢ç›Šè€Œå®ç°çš„ã€‚ç„¶è€Œï¼Œåœ¨åŒ»å­¦æˆåƒåˆ†æé¢†åŸŸï¼Œå­˜åœ¨åˆå¹¶ä»ä¸åŒåˆå§‹åŒ–å¼€å§‹çš„æ¨¡å‹çš„æœºä¼šï¼Œä»è€Œç»“åˆä»ä¸åŒä»»åŠ¡ä¸­å­¦åˆ°çš„ç‰¹å¾ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†MedMergeæ–¹æ³•ï¼Œé€šè¿‡è¯¥æ–¹æ³•å¯ä»¥åˆå¹¶ä¸åŒæ¨¡å‹çš„æƒé‡ï¼Œå¹¶æœ‰æ•ˆåˆ©ç”¨å…¶ç‰¹å¾æ¥æé«˜æ–°ä»»åŠ¡çš„æ€§èƒ½ã€‚é€šè¿‡MedMergeï¼Œæˆ‘ä»¬å­¦ä¹ å†…æ ¸çº§åˆ«çš„æƒé‡ï¼Œè¿™äº›æƒé‡ä»¥åå¯ç”¨äºå°†æ¨¡å‹åˆå¹¶ä¸ºä¸€ä¸ªå•ç‹¬çš„æ¨¡å‹ï¼Œå³ä½¿ä»åˆå§‹çŠ¶æ€æœ‰æ‰€ä¸åŒä¹Ÿå¯ä»¥è¿›è¡Œåˆå¹¶ã€‚é€šè¿‡å¯¹å„ç§åŒ»å­¦æˆåƒåˆ†æä»»åŠ¡çš„æµ‹è¯•ï¼Œæˆ‘ä»¬è¯æ˜åˆå¹¶åçš„æ¨¡å‹èƒ½å¤Ÿå®ç°æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œåœ¨F1åˆ†æ•°ä¸Šæœ€å¤šå¯æé«˜7%ã€‚è¯¥å·¥ä½œçš„ä»£ç å®ç°å¯åœ¨github.com&#x2F;BioMedIA-MBZUAI&#x2F;MedMergeæ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.11646v2">PDF</a> </p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒåˆ†æä¸­ï¼Œè¿ç§»å­¦ä¹ æˆä¸ºåˆå§‹åŒ–æ·±åº¦å­¦ä¹ æ¨¡å‹çš„æœ‰åŠ›å·¥å…·ï¼Œèƒ½åŠ å¿«æ”¶æ•›é€Ÿåº¦å¹¶æé«˜æ€§èƒ½ã€‚æœ¬æ–‡æå‡ºMedMergeæ–¹æ³•ï¼Œé€šè¿‡åˆå¹¶ä¸åŒåˆå§‹åŒ–æ¨¡å‹çš„æƒé‡ï¼Œæœ‰æ•ˆç»“åˆä¸åŒä»»åŠ¡å­¦ä¹ çš„ç‰¹å¾ï¼Œæå‡æ–°ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚é€šè¿‡å†…æ ¸çº§åˆ«çš„æƒé‡å­¦ä¹ ï¼Œå®ç°æ¨¡å‹çš„åˆå¹¶ã€‚å®éªŒè¡¨æ˜ï¼Œåˆå¹¶æ¨¡å‹åœ¨å¤šç§åŒ»å­¦å›¾åƒåˆ†æä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼ŒF1åˆ†æ•°æœ€å¤šå¯æé«˜7%ã€‚ä»£ç å®ç°å·²å‘å¸ƒåœ¨github.com&#x2F;BioMedIA-MBZUAI&#x2F;MedMergeã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿ç§»å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œæœ‰åŠ©äºæå‡æ·±åº¦å­¦ä¹ æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>MedMergeæ–¹æ³•é€šè¿‡åˆå¹¶ä¸åŒåˆå§‹åŒ–æ¨¡å‹çš„æƒé‡ï¼Œå®ç°æ€§èƒ½æå‡ã€‚</li>
<li>MedMergeå¯ä»¥åˆå¹¶ä¸åŒä»»åŠ¡å­¦ä¹ çš„ç‰¹å¾ï¼Œå¢å¼ºæ¨¡å‹åœ¨æ–°ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚</li>
<li>é€šè¿‡å†…æ ¸çº§åˆ«çš„æƒé‡å­¦ä¹ ï¼Œå®ç°æ¨¡å‹çš„åˆå¹¶ï¼Œå³ä½¿æ¨¡å‹ä»ä¸åŒçš„åˆå§‹åŒ–å¼€å§‹ã€‚</li>
<li>MedMergeåœ¨å¤šç§åŒ»å­¦å›¾åƒåˆ†æä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</li>
<li>åˆå¹¶æ¨¡å‹çš„è¡¨ç°ä¼˜äºå•ä¸€æ¨¡å‹ï¼ŒF1åˆ†æ•°æœ€å¤šå¯æé«˜7%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.11646">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-17\./crop_åŒ»å­¦å›¾åƒ/2403.11646v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-17\./crop_åŒ»å­¦å›¾åƒ/2403.11646v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-04-17\./crop_åŒ»å­¦å›¾åƒ/2403.11646v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-17/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-17/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-17/TTS/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-04-17\./crop_TTS/2504.10819v1/page_4_0.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-17  Dopamine Audiobook A Training-free MLLM Agent for Emotional and   Human-like Audiobook Generation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-17
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-17/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-04-17\./crop_Diffusion Models/2504.10995v1/page_3_0.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-17  Aligning Generative Denoising with Discriminative Objectives Unleashes   Diffusion for Visual Perception
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-17
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">16042k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
