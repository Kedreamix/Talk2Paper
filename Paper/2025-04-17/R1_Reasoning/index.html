<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-17  DeepMath-103K A Large-Scale, Challenging, Decontaminated, and   Verifiable Mathematical Dataset for Advancing Reasoning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-68a83c71fd7b3c98241ad990201a9a50.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-17
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    88 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-17-æ›´æ–°"><a href="#2025-04-17-æ›´æ–°" class="headerlink" title="2025-04-17 æ›´æ–°"></a>2025-04-17 æ›´æ–°</h1><h2 id="DeepMath-103K-A-Large-Scale-Challenging-Decontaminated-and-Verifiable-Mathematical-Dataset-for-Advancing-Reasoning"><a href="#DeepMath-103K-A-Large-Scale-Challenging-Decontaminated-and-Verifiable-Mathematical-Dataset-for-Advancing-Reasoning" class="headerlink" title="DeepMath-103K: A Large-Scale, Challenging, Decontaminated, and   Verifiable Mathematical Dataset for Advancing Reasoning"></a>DeepMath-103K: A Large-Scale, Challenging, Decontaminated, and   Verifiable Mathematical Dataset for Advancing Reasoning</h2><p><strong>Authors:Zhiwei He, Tian Liang, Jiahao Xu, Qiuzhi Liu, Xingyu Chen, Yue Wang, Linfeng Song, Dian Yu, Zhenwen Liang, Wenxuan Wang, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, Dong Yu</strong></p>
<p>The capacity for complex mathematical reasoning is a key benchmark for artificial intelligence. While reinforcement learning (RL) applied to LLMs shows promise, progress is significantly hindered by the lack of large-scale training data that is sufficiently challenging, possesses verifiable answer formats suitable for RL, and is free from contamination with evaluation benchmarks. To address these limitations, we introduce DeepMath-103K, a new, large-scale dataset comprising approximately 103K mathematical problems, specifically designed to train advanced reasoning models via RL. DeepMath-103K is curated through a rigorous pipeline involving source analysis, stringent decontamination against numerous benchmarks, and filtering for high difficulty (primarily Levels 5-9), significantly exceeding existing open resources in challenge. Each problem includes a verifiable final answer, enabling rule-based RL, and three distinct R1-generated solutions suitable for diverse training paradigms like supervised fine-tuning or distillation. Spanning a wide range of mathematical topics, DeepMath-103K promotes the development of generalizable reasoning. We demonstrate that models trained on DeepMath-103K achieve significant improvements on challenging mathematical benchmarks, validating its effectiveness. We release DeepMath-103K publicly to facilitate community progress in building more capable AI reasoning systems: <a target="_blank" rel="noopener" href="https://github.com/zwhe99/DeepMath">https://github.com/zwhe99/DeepMath</a>. </p>
<blockquote>
<p>å¤æ‚æ•°å­¦æ¨ç†èƒ½åŠ›æ˜¯äººå·¥æ™ºèƒ½çš„å…³é”®åŸºå‡†ã€‚è™½ç„¶åº”ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹çš„å¢å¼ºå­¦ä¹ ï¼ˆRLï¼‰æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†ç”±äºç¼ºä¹è¶³å¤Ÿå…·æœ‰æŒ‘æˆ˜æ€§ã€æ‹¥æœ‰å¯éªŒè¯ç­”æ¡ˆæ ¼å¼é€‚åˆå¼ºåŒ–å­¦ä¹ ä¸”æœªå—è¯„ä¼°åŸºå‡†æ±¡æŸ“çš„å¤§è§„æ¨¡è®­ç»ƒæ•°æ®ï¼Œè¿›å±•å—åˆ°äº†æ˜¾è‘—é˜»ç¢ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æ¨å‡ºäº†DeepMath-103Kï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼ŒåŒ…å«å¤§çº¦103Kä¸ªæ•°å­¦é—®é¢˜ï¼Œä¸“é—¨è®¾è®¡ç”¨äºé€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒé«˜çº§æ¨ç†æ¨¡å‹ã€‚DeepMath-103Kæ˜¯é€šè¿‡ä¸¥æ ¼çš„ç®¡é“è¿›è¡Œç­–åˆ’çš„ï¼ŒåŒ…æ‹¬æºåˆ†æã€é’ˆå¯¹å¤šä¸ªåŸºå‡†çš„ä¸¥æ ¼å»æ±¡ï¼Œä»¥åŠè¿‡æ»¤é«˜éš¾åº¦ï¼ˆä¸»è¦æ˜¯Level 5-9ï¼‰ï¼Œåœ¨æŒ‘æˆ˜æ€§æ–¹é¢æ˜¾è‘—è¶…è¿‡äº†ç°æœ‰å¼€æ”¾èµ„æºã€‚æ¯ä¸ªé—®é¢˜éƒ½åŒ…æ‹¬ä¸€ä¸ªå¯éªŒè¯çš„æœ€ç»ˆç­”æ¡ˆï¼Œæ”¯æŒåŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ ï¼Œä»¥åŠä¸‰ç§ä¸åŒçš„R1ç”Ÿæˆçš„è§£å†³æ–¹æ¡ˆï¼Œé€‚åˆç›‘ç£å¾®è°ƒæˆ–è’¸é¦ç­‰ä¸åŒçš„è®­ç»ƒèŒƒå¼ã€‚DeepMath-103Kæ¶µç›–äº†å¹¿æ³›çš„æ•°å­¦ä¸»é¢˜ï¼Œä¿ƒè¿›äº†å¯æ¨å¹¿æ¨ç†çš„å‘å±•ã€‚æˆ‘ä»¬è¯æ˜ï¼Œåœ¨DeepMath-103Kä¸Šè®­ç»ƒçš„æ¨¡å‹åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬å…¬å¼€å‘å¸ƒDeepMath-103Kï¼Œä»¥ä¿ƒè¿›ç¤¾åŒºåœ¨æ„å»ºæ›´å¼ºå¤§çš„AIæ¨ç†ç³»ç»Ÿæ–¹é¢çš„è¿›æ­¥ï¼š<a target="_blank" rel="noopener" href="https://github.com/zwhe99/DeepMath">https://github.com/zwhe99/DeepMath</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11456v1">PDF</a> WIP</p>
<p><strong>Summary</strong></p>
<p>æ·±åº¦æ•°å­¦æ¨ç†èƒ½åŠ›æ˜¯äººå·¥æ™ºèƒ½çš„å…³é”®åŸºå‡†ã€‚é’ˆå¯¹å¤§å‹è®­ç»ƒæ•°æ®çš„ç¼ºä¹ä»¥åŠç°æœ‰æ•°æ®é›†ä¸­å­˜åœ¨çš„æŒ‘æˆ˜æ€§é—®é¢˜ä¸è¶³ç­‰é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†DeepMath-103Kæ•°æ®é›†ã€‚è¯¥æ•°æ®é›†åŒ…å«å¤§çº¦103Kä¸ªæ•°å­¦é—®é¢˜ï¼Œä¸“ä¸ºé€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒé«˜çº§æ¨ç†æ¨¡å‹è€Œè®¾è®¡ã€‚DeepMath-103Ké€šè¿‡ä¸¥æ ¼çš„æ•°æ®æ¸…æ´—å’Œç­›é€‰é«˜éš¾åº¦é—®é¢˜ï¼ˆä¸»è¦æ˜¯Level 5-9ï¼‰è¿›è¡Œæ„å»ºï¼Œæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰å¼€æ”¾èµ„æºã€‚æ¯ä¸ªé—®é¢˜éƒ½åŒ…å«å¯éªŒè¯çš„æœ€ç»ˆç­”æ¡ˆå’Œä¸‰ç§ä¸åŒçš„è§£å†³æ–¹æ¡ˆï¼Œé€‚ç”¨äºä¸åŒçš„è®­ç»ƒèŒƒå¼ã€‚DeepMath-103Kæ¶µç›–äº†å¹¿æ³›çš„æ•°å­¦ä¸»é¢˜ï¼Œä¿ƒè¿›äº†é€šç”¨æ¨ç†èƒ½åŠ›çš„å‘å±•ã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦åŸºå‡†æµ‹è¯•ä¸­ï¼Œç»è¿‡DeepMath-103Kè®­ç»ƒçš„æ¨¡å‹å–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚æˆ‘ä»¬å…¬å¼€å‘å¸ƒäº†DeepMath-103Kæ•°æ®é›†ï¼Œä»¥ä¿ƒè¿›ç¤¾åŒºåœ¨æ„å»ºæ›´å…·èƒ½åŠ›çš„AIæ¨ç†ç³»ç»Ÿæ–¹é¢çš„è¿›æ­¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è®­ç»ƒæ•°æ®é›†æ˜¯AIå¤æ‚æ•°å­¦æ¨ç†èƒ½åŠ›å‘å±•çš„é‡è¦é™åˆ¶ã€‚</li>
<li>DeepMath-103Kæ˜¯ä¸€ä¸ªæ–°çš„å¤§å‹æ•°æ®é›†ï¼ŒåŒ…å«çº¦103Kä¸ªæ•°å­¦é—®é¢˜ï¼Œç”¨äºè®­ç»ƒé«˜çº§æ¨ç†æ¨¡å‹ã€‚</li>
<li>DeepMath-103Kæ•°æ®é›†é€šè¿‡ä¸¥æ ¼çš„æ•°æ®æ¸…æ´—æµç¨‹å’Œé«˜éš¾åº¦é—®é¢˜ç­›é€‰è¿›è¡Œæ„å»ºã€‚</li>
<li>æ•°æ®é›†ä¸­çš„æ¯ä¸ªé—®é¢˜éƒ½åŒ…å«å¯éªŒè¯çš„æœ€ç»ˆç­”æ¡ˆå’Œå¤šç§è§£å†³æ–¹æ¡ˆï¼Œé€‚ç”¨äºä¸åŒçš„è®­ç»ƒèŒƒå¼ã€‚</li>
<li>DeepMath-103Kæ•°æ®é›†æ¶µç›–äº†å¹¿æ³›çš„æ•°å­¦ä¸»é¢˜ï¼Œæœ‰åŠ©äºå¼€å‘é€šç”¨æ¨ç†èƒ½åŠ›ã€‚</li>
<li>åœ¨æŒ‘æˆ˜æ€§æ•°å­¦åŸºå‡†æµ‹è¯•ä¸­ï¼Œä½¿ç”¨DeepMath-103Kè®­ç»ƒçš„æ¨¡å‹è¡¨ç°æ˜¾è‘—æ”¹å–„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11456">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-854ee1a27665031ec54607bd995f8e19.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cb38a307ad99b894e509be45715328a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5cca59f69af2adcc73efa38ea7767ca5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e212696084b79b378aacf19cb0e67971.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-72dffce6a25aee91fa5304bca786bd64.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="SimpleAR-Pushing-the-Frontier-of-Autoregressive-Visual-Generation-through-Pretraining-SFT-and-RL"><a href="#SimpleAR-Pushing-the-Frontier-of-Autoregressive-Visual-Generation-through-Pretraining-SFT-and-RL" class="headerlink" title="SimpleAR: Pushing the Frontier of Autoregressive Visual Generation   through Pretraining, SFT, and RL"></a>SimpleAR: Pushing the Frontier of Autoregressive Visual Generation   through Pretraining, SFT, and RL</h2><p><strong>Authors:Junke Wang, Zhi Tian, Xun Wang, Xinyu Zhang, Weilin Huang, Zuxuan Wu, Yu-Gang Jiang</strong></p>
<p>This work presents SimpleAR, a vanilla autoregressive visual generation framework without complex architecure modifications. Through careful exploration of training and inference optimization, we demonstrate that: 1) with only 0.5B parameters, our model can generate 1024x1024 resolution images with high fidelity, and achieve competitive results on challenging text-to-image benchmarks, e.g., 0.59 on GenEval and 79.66 on DPG; 2) both supervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO) training could lead to significant improvements on generation aesthectics and prompt alignment; and 3) when optimized with inference acceleraton techniques like vLLM, the time for SimpleAR to generate an 1024x1024 image could be reduced to around 14 seconds. By sharing these findings and open-sourcing the code, we hope to reveal the potential of autoregressive visual generation and encourage more participation in this research field. Code is available at <a target="_blank" rel="noopener" href="https://github.com/wdrink/SimpleAR">https://github.com/wdrink/SimpleAR</a>. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†SimpleARï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•çš„è‡ªå›å½’è§†è§‰ç”Ÿæˆæ¡†æ¶ï¼Œæ— éœ€è¿›è¡Œå¤æ‚çš„æ¶æ„ä¿®æ”¹ã€‚é€šè¿‡å¯¹è®­ç»ƒå’Œæ¨ç†ä¼˜åŒ–çš„ç²¾å¿ƒæ¢ç´¢ï¼Œæˆ‘ä»¬è¯æ˜ï¼š1ï¼‰ä»…ä½¿ç”¨0.5Bå‚æ•°ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å°±èƒ½ç”Ÿæˆé«˜ä¿çœŸåº¦çš„1024x1024åˆ†è¾¨ç‡å›¾åƒï¼Œå¹¶åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ–‡æœ¬åˆ°å›¾åƒåŸºå‡†æµ‹è¯•ä¸­å–å¾—æœ‰ç«äº‰åŠ›çš„ç»“æœï¼Œä¾‹å¦‚åœ¨GenEvalä¸Šçš„å¾—åˆ†ä¸º0.59ï¼Œåœ¨DPGä¸Šçš„å¾—åˆ†ä¸º79.66ï¼›2ï¼‰é€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œé›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰è®­ç»ƒï¼Œå¯ä»¥åœ¨ç”Ÿæˆç¾å­¦å’Œæç¤ºå¯¹é½æ–¹é¢å–å¾—æ˜¾ç€æ”¹è¿›ï¼›3ï¼‰å½“ä½¿ç”¨è¯¸å¦‚vLLMä¹‹ç±»çš„æ¨ç†åŠ é€ŸæŠ€æœ¯è¿›è¡Œä¼˜åŒ–æ—¶ï¼ŒSimpleARç”Ÿæˆä¸€ä¸ª1024x1024å›¾åƒçš„æ—¶é—´å¯ä»¥ç¼©çŸ­åˆ°å¤§çº¦14ç§’ã€‚æˆ‘ä»¬å¸Œæœ›é€šè¿‡åˆ†äº«è¿™äº›å‘ç°å¹¶å…¬å¼€æºä»£ç ï¼Œæ­ç¤ºè‡ªå›å½’è§†è§‰ç”Ÿæˆçš„æ½œåŠ›ï¼Œå¹¶é¼“åŠ±æ›´å¤šäººå‘˜å‚ä¸è¿™ä¸€ç ”ç©¶é¢†åŸŸã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/wdrink/SimpleAR%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/wdrink/SimpleARä¸­æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11455v1">PDF</a> technical report, work in progress</p>
<p><strong>Summary</strong></p>
<p>SimpleARæ¡†æ¶å±•ç¤ºäº†åœ¨è§†è§‰ç”Ÿæˆé¢†åŸŸçš„ä¼˜ç§€è¡¨ç°ï¼Œå®ƒé‡‡ç”¨åŸºç¡€çš„è‡ªå›å½’æ–¹æ³•ï¼Œæ— éœ€å¤æ‚çš„æ¶æ„ä¿®æ”¹ã€‚é€šè¿‡ä¼˜åŒ–è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ï¼Œè¯¥æ¨¡å‹èƒ½ä»¥é«˜ä¿çœŸåº¦ç”Ÿæˆ1024x1024åˆ†è¾¨ç‡çš„å›¾åƒï¼Œå¹¶åœ¨æ–‡æœ¬åˆ°å›¾åƒçš„åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ‰ç«äº‰åŠ›çš„æˆç»©ã€‚æ­¤å¤–ï¼Œé€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œé›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰è®­ç»ƒï¼Œä»¥åŠåˆ©ç”¨vLLMç­‰æ¨ç†åŠ é€ŸæŠ€æœ¯ï¼ŒSimpleARåœ¨ç”Ÿæˆç¾å­¦å’Œæç¤ºå¯¹é½æ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œå¹¶åŠ å¿«äº†å›¾åƒç”Ÿæˆé€Ÿåº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SimpleARæ˜¯ä¸€ä¸ªåŸºç¡€çš„è‡ªå›å½’è§†è§‰ç”Ÿæˆæ¡†æ¶ï¼Œèƒ½ä»¥é«˜ä¿çœŸåº¦ç”Ÿæˆ1024x1024åˆ†è¾¨ç‡çš„å›¾åƒã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ–‡æœ¬åˆ°å›¾åƒåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†ç«äº‰æ€§çš„ç»“æœã€‚</li>
<li>ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œé›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰è®­ç»ƒæé«˜äº†ç”Ÿæˆå›¾åƒçš„ç¾å­¦å’Œæç¤ºå¯¹é½èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨æ¨ç†åŠ é€ŸæŠ€æœ¯ï¼ˆå¦‚vLLMï¼‰ï¼ŒSimpleARèƒ½å¤Ÿåœ¨çº¦14ç§’å†…ç”Ÿæˆä¸€ä¸ª1024x1024åˆ†è¾¨ç‡çš„å›¾åƒã€‚</li>
<li>SimpleARæ¡†æ¶å…·æœ‰æ½œåŠ›ï¼Œå¯ä»¥é¼“åŠ±æ›´å¤šå‚ä¸æ­¤ç ”ç©¶é¢†åŸŸã€‚</li>
<li>è¯¥æ¨¡å‹çš„ä»£ç å·²ç»å¼€æºï¼Œå¯ä¾›å…¬ä¼—è®¿é—®å’Œä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11455">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-02fbe1376b1a68bfed18e3c396f661b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-14e8b97acd0dc079e57027ce93d10e99.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-88c9c84ac1bffd845a92c9cf2ca43536.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d27913e0d813d5086f195968bee22329.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ddba9a9479f4bb804c2e093778a53510.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a8b72f58915d3d3582a3c783f05a698.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Diffusion-Distillation-With-Direct-Preference-Optimization-For-Efficient-3D-LiDAR-Scene-Completion"><a href="#Diffusion-Distillation-With-Direct-Preference-Optimization-For-Efficient-3D-LiDAR-Scene-Completion" class="headerlink" title="Diffusion Distillation With Direct Preference Optimization For Efficient   3D LiDAR Scene Completion"></a>Diffusion Distillation With Direct Preference Optimization For Efficient   3D LiDAR Scene Completion</h2><p><strong>Authors:An Zhaol, Shengyuan Zhang, Ling Yang, Zejian Li, Jiale Wu, Haoran Xu, AnYang Wei, Perry Pengyun GU Lingyun Sun</strong></p>
<p>The application of diffusion models in 3D LiDAR scene completion is limited due to diffusionâ€™s slow sampling speed. Score distillation accelerates diffusion sampling but with performance degradation, while post-training with direct policy optimization (DPO) boosts performance using preference data. This paper proposes Distillation-DPO, a novel diffusion distillation framework for LiDAR scene completion with preference aligment. First, the student model generates paired completion scenes with different initial noises. Second, using LiDAR scene evaluation metrics as preference, we construct winning and losing sample pairs. Such construction is reasonable, since most LiDAR scene metrics are informative but non-differentiable to be optimized directly. Third, Distillation-DPO optimizes the student model by exploiting the difference in score functions between the teacher and student models on the paired completion scenes. Such procedure is repeated until convergence. Extensive experiments demonstrate that, compared to state-of-the-art LiDAR scene completion diffusion models, Distillation-DPO achieves higher-quality scene completion while accelerating the completion speed by more than 5-fold. Our method is the first to explore adopting preference learning in distillation to the best of our knowledge and provide insights into preference-aligned distillation. Our code is public available on <a target="_blank" rel="noopener" href="https://github.com/happyw1nd/DistillationDPO">https://github.com/happyw1nd/DistillationDPO</a>. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨3Dæ¿€å…‰é›·è¾¾åœºæ™¯è¡¥å…¨ä¸­çš„åº”ç”¨å—é™äºå…¶ç¼“æ…¢çš„é‡‡æ ·é€Ÿåº¦ã€‚åˆ†æ•°è’¸é¦å¯ä»¥åŠ é€Ÿæ‰©æ•£é‡‡æ ·ï¼Œä½†ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ï¼Œè€Œä½¿ç”¨ç›´æ¥ç­–ç•¥ä¼˜åŒ–ï¼ˆDPOï¼‰è¿›è¡Œåè®­ç»ƒåˆ™å¯ä»¥åˆ©ç”¨åå¥½æ•°æ®æå‡æ€§èƒ½ã€‚æœ¬æ–‡æå‡ºäº†Distillation-DPOï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºæ¿€å…‰é›·è¾¾åœºæ™¯è¡¥å…¨çš„å…¨æ–°æ‰©æ•£è’¸é¦æ¡†æ¶ï¼Œå¸¦æœ‰åå¥½å¯¹é½ã€‚é¦–å…ˆï¼Œå­¦ç”Ÿæ¨¡å‹ç”Ÿæˆå¸¦æœ‰ä¸åŒåˆå§‹å™ªå£°çš„é…å¯¹å®Œæˆåœºæ™¯ã€‚å…¶æ¬¡ï¼Œä»¥æ¿€å…‰é›·è¾¾åœºæ™¯è¯„ä¼°æŒ‡æ ‡ä¸ºåå¥½ï¼Œæˆ‘ä»¬æ„å»ºäº†èƒœè€…å’Œè´¥è€…æ ·æœ¬å¯¹ã€‚è¿™ç§æ„å»ºæ˜¯åˆç†çš„ï¼Œå› ä¸ºå¤§å¤šæ•°æ¿€å…‰é›·è¾¾åœºæ™¯æŒ‡æ ‡éƒ½æ˜¯ä¿¡æ¯ä¸°å¯Œçš„ï¼Œä½†æ— æ³•ç›´æ¥ä¼˜åŒ–å…¶å¯åˆ†åŒ–æ€§ã€‚ç¬¬ä¸‰ï¼ŒDistillation-DPOé€šè¿‡åˆ©ç”¨æ•™å¸ˆæ¨¡å‹å’Œå­¦ç”Ÿæ¨¡å‹åœ¨é…å¯¹å®Œæˆåœºæ™¯ä¸Šçš„åˆ†æ•°å‡½æ•°çš„å·®å¼‚æ¥ä¼˜åŒ–å­¦ç”Ÿæ¨¡å‹ã€‚è¿™ç§ç¨‹åºä¼šæŒç»­åˆ°æ”¶æ•›ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„æ¿€å…‰é›·è¾¾åœºæ™¯è¡¥å…¨æ‰©æ•£æ¨¡å‹ç›¸æ¯”ï¼ŒDistillation-DPOå®ç°äº†æ›´é«˜è´¨é‡çš„åœºæ™¯è¡¥å…¨ï¼Œå¹¶å°†è¡¥å…¨é€Ÿåº¦æé«˜äº†5å€ä»¥ä¸Šã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¯é¦–æ¬¡æ¢ç´¢åœ¨è’¸é¦ä¸­é‡‡ç”¨åå¥½å­¦ä¹ ï¼Œå¹¶ä¸ºåå¥½å¯¹é½çš„è’¸é¦æä¾›äº†è§è§£ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/happyw1nd/DistillationDPO%E4%B8%8A%E5%85%AC%E5%BC%80%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/happyw1nd/DistillationDPOä¸Šå…¬å¼€è·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11447v1">PDF</a> Our code is public available on   <a target="_blank" rel="noopener" href="https://github.com/happyw1nd/DistillationDPO">https://github.com/happyw1nd/DistillationDPO</a></p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹åœ¨3Dæ¿€å…‰é›·è¾¾åœºæ™¯è¡¥å…¨ä¸­çš„åº”ç”¨å—é™äºå…¶ç¼“æ…¢çš„é‡‡æ ·é€Ÿåº¦ã€‚åˆ†æ•°è’¸é¦è™½ç„¶å¯ä»¥åŠ é€Ÿæ‰©æ•£é‡‡æ ·ï¼Œä½†ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ï¼Œè€Œåè®­ç»ƒä½¿ç”¨ç›´æ¥ç­–ç•¥ä¼˜åŒ–ï¼ˆDPOï¼‰åˆ™èƒ½åˆ©ç”¨åå¥½æ•°æ®æå‡æ€§èƒ½ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç”¨äºæ¿€å…‰é›·è¾¾åœºæ™¯è¡¥å…¨çš„å…¨æ–°æ‰©æ•£è’¸é¦æ¡†æ¶â€”â€”Distillation-DPOï¼Œå®ƒç»“åˆäº†åå¥½å¯¹é½ã€‚é¦–å…ˆï¼Œå­¦ç”Ÿæ¨¡å‹ç”Ÿæˆå¸¦æœ‰ä¸åŒåˆå§‹å™ªå£°çš„é…å¯¹å®Œæˆåœºæ™¯ã€‚å…¶æ¬¡ï¼Œä»¥æ¿€å…‰é›·è¾¾åœºæ™¯è¯„ä¼°æŒ‡æ ‡ä¸ºåå¥½ï¼Œæ„å»ºèƒœè€…ä¸è´¥è€…æ ·æœ¬å¯¹ã€‚è¿™ç§æ„å»ºæ–¹å¼æ˜¯åˆç†çš„ï¼Œå› ä¸ºå¤§å¤šæ•°æ¿€å…‰é›·è¾¾åœºæ™¯æŒ‡æ ‡éƒ½æ˜¯å¾ˆæœ‰ä»·å€¼çš„ä½†æ— æ³•ç›´æ¥ä¼˜åŒ–ã€‚æœ€åï¼ŒDistillation-DPOé€šè¿‡ä¼˜åŒ–é…å¯¹å®Œæˆåœºæ™¯ä¸Šæ•™å¸ˆæ¨¡å‹å’Œå­¦ç”Ÿæ¨¡å‹ä¹‹é—´çš„åˆ†æ•°å‡½æ•°å·®å¼‚æ¥ä¼˜åŒ–å­¦ç”Ÿæ¨¡å‹ã€‚ç»è¿‡å¤§é‡å®éªŒè¯æ˜ï¼Œä¸æœ€å…ˆè¿›çš„æ¿€å…‰é›·è¾¾åœºæ™¯è¡¥å…¨æ‰©æ•£æ¨¡å‹ç›¸æ¯”ï¼ŒDistillation-DPOåœ¨åŠ é€Ÿè¡¥å…¨é€Ÿåº¦çš„åŒæ—¶ï¼Œå®ç°äº†æ›´é«˜è´¨é‡çš„åœºæ™¯è¡¥å…¨ï¼ŒåŠ é€Ÿäº†è¶…è¿‡5å€ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¯ç¬¬ä¸€ä¸ªå°è¯•åœ¨è’¸é¦è¿‡ç¨‹ä¸­é‡‡ç”¨åå¥½å­¦ä¹ çš„ã€‚æˆ‘ä»¬çš„ä»£ç å·²åœ¨å…¬å¼€å¹³å°ä¸Šå‘å¸ƒï¼š<a target="_blank" rel="noopener" href="https://github.com/happyw1nd/DistillationDPO">https://github.com/happyw1nd/DistillationDPO</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨3D LiDARåœºæ™¯è¡¥å…¨ä¸­çš„é‡‡æ ·é€Ÿåº¦æ…¢ã€‚</li>
<li>åˆ†æ•°è’¸é¦å¯ä»¥åŠ é€Ÿæ‰©æ•£é‡‡æ ·ä½†å¯èƒ½å½±å“æ€§èƒ½ã€‚</li>
<li>ç›´æ¥ç­–ç•¥ä¼˜åŒ–ï¼ˆDPOï¼‰åˆ©ç”¨åå¥½æ•°æ®æå‡æ€§èƒ½ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ‰©æ•£è’¸é¦æ¡†æ¶â€”â€”Distillation-DPOï¼Œç”¨äºæ¿€å…‰é›·è¾¾åœºæ™¯è¡¥å…¨ã€‚</li>
<li>å­¦ç”Ÿæ¨¡å‹ç”Ÿæˆå¸¦æœ‰ä¸åŒåˆå§‹å™ªå£°çš„é…å¯¹å®Œæˆåœºæ™¯ã€‚</li>
<li>é‡‡ç”¨æ¿€å…‰é›·è¾¾åœºæ™¯è¯„ä¼°æŒ‡æ ‡ä½œä¸ºä¼˜åŒ–çš„åå¥½æ ‡å‡†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11447">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-79faa37d360215c3c81728c41a7fbb90.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1500980a1fee5ca0505f9e6ae85b30f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5d9db7427d87df12ae8b42fb6654113d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a044c5542b6839b43f2118a0ef1879bb.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Reinforcing-Compositional-Retrieval-Retrieving-Step-by-Step-for-Composing-Informative-Contexts"><a href="#Reinforcing-Compositional-Retrieval-Retrieving-Step-by-Step-for-Composing-Informative-Contexts" class="headerlink" title="Reinforcing Compositional Retrieval: Retrieving Step-by-Step for   Composing Informative Contexts"></a>Reinforcing Compositional Retrieval: Retrieving Step-by-Step for   Composing Informative Contexts</h2><p><strong>Authors:Quanyu Long, Jianda Chen, Zhengyuan Liu, Nancy F. Chen, Wenya Wang, Sinno Jialin Pan</strong></p>
<p>Large Language Models (LLMs) have demonstrated remarkable capabilities across numerous tasks, yet they often rely on external context to handle complex tasks. While retrieval-augmented frameworks traditionally focus on selecting top-ranked documents in a single pass, many real-world scenarios demand compositional retrieval, where multiple sources must be combined in a coordinated manner. In this work, we propose a tri-encoder sequential retriever that models this process as a Markov Decision Process (MDP), decomposing the probability of retrieving a set of elements into a sequence of conditional probabilities and allowing each retrieval step to be conditioned on previously selected examples. We train the retriever in two stages: first, we efficiently construct supervised sequential data for initial policy training; we then refine the policy to align with the LLMâ€™s preferences using a reward grounded in the structural correspondence of generated programs. Experimental results show that our method consistently and significantly outperforms baselines, underscoring the importance of explicitly modeling inter-example dependencies. These findings highlight the potential of compositional retrieval for tasks requiring multiple pieces of evidence or examples. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šä¸ªä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†æ˜¾è‘—çš„èƒ½åŠ›ï¼Œä½†å®ƒä»¬é€šå¸¸ä¾èµ–äºå¤–éƒ¨ä¸Šä¸‹æ–‡æ¥å¤„ç†å¤æ‚ä»»åŠ¡ã€‚è™½ç„¶æ£€ç´¢å¢å¼ºæ¡†æ¶ä¼ ç»Ÿä¸Šä¾§é‡äºåœ¨å•æ¬¡ä¼ é€’ä¸­é€‰æ‹©æ’åé å‰çš„æ–‡æ¡£ï¼Œä½†è®¸å¤šç°å®ä¸–ç•Œåœºæ™¯éœ€è¦ç»„åˆæ£€ç´¢ï¼Œå¿…é¡»ä»¥åè°ƒçš„æ–¹å¼ç»„åˆå¤šä¸ªæºã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸‰ç¼–ç å™¨é¡ºåºæ£€ç´¢å™¨ï¼Œå®ƒå°†æ­¤è¿‡ç¨‹å»ºæ¨¡ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ï¼Œå°†æ£€ç´¢ä¸€ç»„å…ƒç´ çš„æ¦‚ç‡åˆ†è§£ä¸ºä¸€ç³»åˆ—æ¡ä»¶æ¦‚ç‡ï¼Œå¹¶å…è®¸æ¯ä¸ªæ£€ç´¢æ­¥éª¤éƒ½åŸºäºå…ˆå‰é€‰æ‹©çš„ç¤ºä¾‹è¿›è¡Œæ¡ä»¶è®¾ç½®ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªé˜¶æ®µè®­ç»ƒæ£€ç´¢å™¨ï¼šé¦–å…ˆï¼Œæˆ‘ä»¬æœ‰æ•ˆåœ°æ„å»ºç”¨äºåˆå§‹ç­–ç•¥è®­ç»ƒçš„ç›‘ç£åºåˆ—æ•°æ®ï¼›ç„¶åï¼Œæˆ‘ä»¬ç»†åŒ–ç­–ç•¥ï¼Œä»¥å¥–åŠ±ä¸ºåŸºç¡€ä¸LLMçš„åå¥½å¯¹é½ï¼Œè¯¥å¥–åŠ±åŸºäºç”Ÿæˆç¨‹åºçš„ç»“æ„å¯¹åº”ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å§‹ç»ˆä¸”æ˜¾è‘—åœ°ä¼˜äºåŸºçº¿ï¼Œå¼ºè°ƒæ˜¾å¼å»ºæ¨¡ç¤ºä¾‹é—´ä¾èµ–å…³ç³»çš„é‡è¦æ€§ã€‚è¿™äº›å‘ç°çªæ˜¾äº†ç»„åˆæ£€ç´¢å¯¹äºéœ€è¦å¤šä¸ªè¯æ®æˆ–ç¤ºä¾‹çš„ä»»åŠ¡çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11420v1">PDF</a> 19 pages, 8 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šä¸ªä»»åŠ¡ä¸­å±•ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œä½†å®ƒä»¬é€šå¸¸éœ€è¦å¤–éƒ¨ä¸Šä¸‹æ–‡æ¥å¤„ç†å¤æ‚ä»»åŠ¡ã€‚ä¼ ç»Ÿçš„æ£€ç´¢å¢å¼ºæ¡†æ¶ä¾§é‡äºä¸€æ¬¡æ€§é€‰æ‹©æ’åé å‰çš„æ–‡æ¡£ï¼Œä½†åœ¨ç°å®ä¸–ç•Œçš„è®¸å¤šåœºæ™¯ä¸­ï¼Œéœ€è¦ç»„åˆå¤šä¸ªæ¥æºçš„ä¿¡æ¯è¿›è¡Œåè°ƒå¼æ£€ç´¢ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ä¸‰ç¼–ç å™¨é¡ºåºæ£€ç´¢å™¨ï¼Œå°†æ£€ç´¢è¿‡ç¨‹å»ºæ¨¡ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ï¼Œå°†æ£€ç´¢ä¸€ç»„å…ƒç´ çš„å¯èƒ½æ€§åˆ†è§£ä¸ºä¸€ç³»åˆ—æ¡ä»¶æ¦‚ç‡ï¼Œå¹¶å…è®¸æ¯ä¸ªæ£€ç´¢æ­¥éª¤ä»¥å…ˆå‰é€‰æ‹©çš„ä¾‹å­ä¸ºæ¡ä»¶ã€‚æˆ‘ä»¬åˆ†ä¸¤ä¸ªé˜¶æ®µè®­ç»ƒæ£€ç´¢å™¨ï¼šé¦–å…ˆï¼Œæˆ‘ä»¬æœ‰æ•ˆåœ°æ„å»ºç›‘ç£å¼é¡ºåºæ•°æ®ï¼Œç”¨äºåˆå§‹ç­–ç•¥è®­ç»ƒï¼›ç„¶åï¼Œæˆ‘ä»¬ç»†åŒ–ç­–ç•¥ï¼Œä»¥ä¸LLMçš„åå¥½ç›¸ç¬¦ï¼Œå¹¶ä½¿ç”¨åŸºäºç”Ÿæˆç¨‹åºç»“æ„å¯¹åº”æ€§çš„å¥–åŠ±è¿›è¡Œè®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æŒç»­ä¸”æ˜¾è‘—åœ°ä¼˜äºåŸºå‡†æ–¹æ³•ï¼Œå¼ºè°ƒæ˜ç¡®å»ºæ¨¡å„ç¤ºä¾‹é—´ä¾èµ–å…³ç³»çš„é‡è¦æ€§ã€‚è¿™äº›å‘ç°çªæ˜¾äº†ç»„åˆæ£€ç´¢å¯¹äºéœ€è¦å¤šä¸ªè¯æ®æˆ–ç¤ºä¾‹çš„ä»»åŠ¡çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å¤„ç†å¤æ‚ä»»åŠ¡æ—¶éœ€ä¾èµ–å¤–éƒ¨ä¸Šä¸‹æ–‡ã€‚</li>
<li>ä¼ ç»Ÿæ£€ç´¢å¢å¼ºæ¡†æ¶æ³¨é‡å•ä¸€é€šè¿‡çš„é€‰æ‹©ï¼Œç°å®ä¸–ç•Œä¸­éœ€åè°ƒå¤šä¸ªæ¥æºä¿¡æ¯çš„ç»„åˆæ£€ç´¢ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ä¸‰ç¼–ç å™¨é¡ºåºæ£€ç´¢å™¨ï¼Œå°†æ£€ç´¢è¿‡ç¨‹å»ºæ¨¡ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ã€‚</li>
<li>æ£€ç´¢å™¨è®­ç»ƒåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šåˆå§‹ç­–ç•¥è®­ç»ƒå’Œä¸LLMåå¥½ç›¸ç¬¦çš„ç­–ç•¥ç»†åŒ–ã€‚</li>
<li>ç›‘ç£å¼é¡ºåºæ•°æ®ç”¨äºè®­ç»ƒåˆå§‹ç­–ç•¥ï¼Œç”Ÿæˆç¨‹åºç»“æ„å¯¹åº”æ€§çš„å¥–åŠ±ç”¨äºç»†åŒ–ç­–ç•¥ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºæ‰€ææ–¹æ³•æ˜¾è‘—ä¼˜äºåŸºå‡†æ–¹æ³•ï¼Œå¼ºè°ƒå»ºæ¨¡ç¤ºä¾‹é—´ä¾èµ–å…³ç³»çš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11420">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ef3ba91e258c4b53488da6c99377252c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e7d6cc8db775d3b4057b2902bf964bd0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f6db088bc20f9834af1713c56577d4fc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f2aad415fa0ab6bce4d1866ff6273585.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0e79ea6a305a8bf235e8b7604804489e.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Embodied-World-Models-Emerge-from-Navigational-Task-in-Open-Ended-Environments"><a href="#Embodied-World-Models-Emerge-from-Navigational-Task-in-Open-Ended-Environments" class="headerlink" title="Embodied World Models Emerge from Navigational Task in Open-Ended   Environments"></a>Embodied World Models Emerge from Navigational Task in Open-Ended   Environments</h2><p><strong>Authors:Li Jin, Liu Jia</strong></p>
<p>Understanding how artificial systems can develop spatial awareness and reasoning has long been a challenge in AI research. Traditional models often rely on passive observation, but embodied cognition theory suggests that deeper understanding emerges from active interaction with the environment. This study investigates whether neural networks can autonomously internalize spatial concepts through interaction, focusing on planar navigation tasks. Using Gated Recurrent Units (GRUs) combined with Meta-Reinforcement Learning (Meta-RL), we show that agents can learn to encode spatial properties like direction, distance, and obstacle avoidance. We introduce Hybrid Dynamical Systems (HDS) to model the agent-environment interaction as a closed dynamical system, revealing stable limit cycles that correspond to optimal navigation strategies. Ridge Representation allows us to map navigation paths into a fixed-dimensional behavioral space, enabling comparison with neural states. Canonical Correlation Analysis (CCA) confirms strong alignment between these representations, suggesting that the agentâ€™s neural states actively encode spatial knowledge. Intervention experiments further show that specific neural dimensions are causally linked to navigation performance. This work provides an approach to bridging the gap between action and perception in AI, offering new insights into building adaptive, interpretable models that can generalize across complex environments. The causal validation of neural representations also opens new avenues for understanding and controlling the internal mechanisms of AI systems, pushing the boundaries of how machines learn and reason in dynamic, real-world scenarios. </p>
<blockquote>
<p>ç†è§£äººå·¥ç³»ç»Ÿå¦‚ä½•å‘å±•ç©ºé—´æ„è¯†å’Œæ¨ç†ä¸€ç›´æ˜¯äººå·¥æ™ºèƒ½ç ”ç©¶ä¸­çš„ä¸€é¡¹æŒ‘æˆ˜ã€‚ä¼ ç»Ÿæ¨¡å‹é€šå¸¸ä¾èµ–äºè¢«åŠ¨è§‚å¯Ÿï¼Œä½†å…·èº«è®¤çŸ¥ç†è®ºè¡¨æ˜ï¼Œæ›´æ·±å±‚çš„ç†è§£æ¥è‡ªäºä¸ç¯å¢ƒçš„ä¸»åŠ¨äº’åŠ¨ã€‚æœ¬ç ”ç©¶è°ƒæŸ¥ç¥ç»ç½‘ç»œæ˜¯å¦å¯ä»¥é€šè¿‡äº’åŠ¨è‡ªä¸»å†…åŒ–ç©ºé—´æ¦‚å¿µï¼Œé‡ç‚¹å…³æ³¨å¹³é¢å¯¼èˆªä»»åŠ¡ã€‚é€šè¿‡ä½¿ç”¨é—¨æ§å¾ªç¯å•å…ƒï¼ˆGRUsï¼‰ç»“åˆå…ƒå¼ºåŒ–å­¦ä¹ ï¼ˆMeta-RLï¼‰ï¼Œæˆ‘ä»¬å±•ç¤ºäº†æ™ºèƒ½ä½“å¯ä»¥å­¦ä¹ ç¼–ç æ–¹å‘ã€è·ç¦»å’Œé¿éšœç­‰ç©ºé—´å±æ€§ã€‚æˆ‘ä»¬å¼•å…¥æ··åˆåŠ¨åŠ›ç³»ç»Ÿï¼ˆHDSï¼‰æ¥æ¨¡æ‹Ÿæ™ºèƒ½ä½“ä¸ç¯å¢ƒä¹‹é—´çš„ç›¸äº’ä½œç”¨ä½œä¸ºä¸€ä¸ªå°é—­çš„åŠ¨åŠ›ç³»ç»Ÿï¼Œæ­ç¤ºäº†å¯¹åº”äºæœ€ä½³å¯¼èˆªç­–ç•¥çš„ç¨³å®šæé™å¾ªç¯ã€‚å²­è¡¨ç¤ºæ³•å…è®¸æˆ‘ä»¬å°†å¯¼èˆªè·¯å¾„æ˜ å°„åˆ°å›ºå®šç»´åº¦çš„è¡Œä¸ºç©ºé—´ï¼Œä»¥ä¾¿ä¸ç¥ç»çŠ¶æ€è¿›è¡Œæ¯”è¾ƒã€‚å…¸å‹ç›¸å…³åˆ†æï¼ˆCCAï¼‰è¯å®äº†è¿™äº›è¡¨ç¤ºä¹‹é—´çš„å¼ºçƒˆå¯¹é½ï¼Œè¿™è¡¨æ˜æ™ºèƒ½ä½“çš„ç¥ç»çŠ¶æ€ç§¯æç¼–ç ç©ºé—´çŸ¥è¯†ã€‚å¹²é¢„å®éªŒè¿›ä¸€æ­¥è¡¨æ˜ï¼Œç‰¹å®šçš„ç¥ç»ç»´åº¦ä¸å¯¼èˆªæ€§èƒ½ä¹‹é—´å­˜åœ¨å› æœå…³ç³»ã€‚è¿™é¡¹å·¥ä½œä¸ºå¼¥åˆäººå·¥æ™ºèƒ½ä¸­çš„è¡ŒåŠ¨å’Œæ„ŸçŸ¥ä¹‹é—´çš„å·®è·æä¾›äº†ä¸€ç§æ–¹æ³•ï¼Œä¸ºæ„å»ºèƒ½å¤Ÿé€‚åº”å¹¶è·¨è¶Šå¤æ‚ç¯å¢ƒçš„å¯è§£é‡Šæ¨¡å‹æä¾›äº†æ–°çš„è§è§£ã€‚ç¥ç»è¡¨ç¤ºçš„å› æœéªŒè¯ä¹Ÿå¼€è¾Ÿäº†ç†è§£å’Œæ§åˆ¶äººå·¥æ™ºèƒ½ç³»ç»Ÿå†…éƒ¨æœºåˆ¶çš„æ–°é€”å¾„ï¼Œæ¨åŠ¨äº†æœºå™¨åœ¨åŠ¨æ€ã€ç°å®åœºæ™¯ä¸­çš„å­¦ä¹ å’Œæ¨ç†èƒ½åŠ›çš„è¾¹ç•Œã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11419v1">PDF</a> Research on explainable meta-reinforcement learning AI</p>
<p><strong>Summary</strong><br>     è¯¥ç ”ç©¶ç»“åˆç¥ç»ç½‘ç»œä¸åŠ¨æ€ç³»ç»Ÿç†è®ºï¼Œæ¢ç´¢æœºå™¨åœ¨ç©ºé—´å¯¼èˆªä»»åŠ¡ä¸­å¦‚ä½•è‡ªä¸»å†…åŒ–ç©ºé—´æ¦‚å¿µã€‚é€šè¿‡GRUä¸Meta-RLç»“åˆï¼Œå‘ç°æœºå™¨èƒ½å­¦ä¹ æ–¹å‘ã€è·ç¦»åŠé¿éšœç­‰ç©ºé—´å±æ€§ã€‚å¼•å…¥HDSå»ºæ¨¡ï¼Œæ­ç¤ºç¨³å®šå¾ªç¯å¯¹åº”æœ€ä½³å¯¼èˆªç­–ç•¥ã€‚è¯¥ç ”ç©¶ä¸ºAIé¢†åŸŸçš„è¡ŒåŠ¨ä¸æ„ŸçŸ¥ä¹‹é—´çš„æ¡¥æ¢æä¾›æ–°æ–¹æ³•ï¼Œæœ‰åŠ©äºå»ºç«‹èƒ½é€‚åº”å¤æ‚ç¯å¢ƒçš„å¯è§£é‡Šæ¨¡å‹ï¼Œå¹¶ä¸ºAIç³»ç»Ÿå†…éƒ¨æœºåˆ¶çš„å› æœéªŒè¯å¼€å¯æ–°çš„ç ”ç©¶é€”å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººå·¥ç³»ç»Ÿå¯é€šè¿‡ä¸»åŠ¨ä¸ç¯å¢ƒäº’åŠ¨å‘å±•å‡ºç©ºé—´è®¤çŸ¥ä¸æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ç¥ç»ç½‘ç»œç»“åˆMeta-Reinforcement Learning (Meta-RL)æŠ€æœ¯ï¼Œä½¿æœºå™¨èƒ½è‡ªä¸»å†…åŒ–ç©ºé—´æ¦‚å¿µã€‚</li>
<li>Hybrid Dynamical Systems (HDS)å»ºæ¨¡æ­ç¤ºäº†ç¨³å®šå¾ªç¯ä¸æœ€ä½³å¯¼èˆªç­–ç•¥ä¹‹é—´çš„å…³ç³»ã€‚</li>
<li>Ridge Representationæ–¹æ³•èƒ½å°†å¯¼èˆªè·¯å¾„æ˜ å°„åˆ°å›ºå®šç»´åº¦çš„è¡Œä¸ºç©ºé—´ï¼Œä¾¿äºä¸ç¥ç»çŠ¶æ€è¿›è¡Œæ¯”è¾ƒã€‚</li>
<li>Canonical Correlation Analysis (CCA)è¯å®äº†æœºå™¨ç¥ç»çŠ¶æ€ç§¯æç¼–ç ç©ºé—´çŸ¥è¯†ã€‚</li>
<li>é€šè¿‡å¹²é¢„å®éªŒå‘ç°ç‰¹å®šç¥ç»ç»´åº¦ä¸å¯¼èˆªæ€§èƒ½ä¹‹é—´å­˜åœ¨å› æœå…³ç³»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11419">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-df44e50389395a33a7e24223158de1fc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3d1651ca27f07eec8f7e1df4a89592d8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb311f81726292a5ef611e83607aa96d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fedcd02e3d1d14b4a6411135995a13dc.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Teaching-Large-Language-Models-to-Reason-through-Learning-and-Forgetting"><a href="#Teaching-Large-Language-Models-to-Reason-through-Learning-and-Forgetting" class="headerlink" title="Teaching Large Language Models to Reason through Learning and Forgetting"></a>Teaching Large Language Models to Reason through Learning and Forgetting</h2><p><strong>Authors:Tianwei Ni, Allen Nie, Sapana Chaudhary, Yao Liu, Huzefa Rangwala, Rasool Fakoor</strong></p>
<p>Leveraging inference-time search in large language models has proven effective in further enhancing a trained modelâ€™s capability to solve complex mathematical and reasoning problems. However, this approach significantly increases computational costs and inference time, as the model must generate and evaluate multiple candidate solutions to identify a viable reasoning path. To address this, we propose an effective approach that integrates search capabilities directly into the model by fine-tuning it using both successful (learning) and failed reasoning paths (forgetting) derived from diverse search methods. While fine-tuning the model with these data might seem straightforward, we identify a critical issue: the modelâ€™s search capability tends to degrade rapidly if fine-tuning is performed naively. We show that this degradation can be substantially mitigated by employing a smaller learning rate. Extensive experiments on the challenging Game-of-24 and Countdown mathematical reasoning benchmarks show that our approach not only outperforms both standard fine-tuning and inference-time search baselines but also significantly reduces inference time by 180$\times$. </p>
<blockquote>
<p>åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„æ¨ç†æ—¶é—´æœç´¢ï¼Œå·²è¢«è¯æ˜å¯ä»¥è¿›ä¸€æ­¥å¢å¼ºè®­ç»ƒæ¨¡å‹è§£å†³å¤æ‚æ•°å­¦å’Œæ¨ç†é—®é¢˜çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•ä¼šå¤§å¹…å¢åŠ è®¡ç®—æˆæœ¬å’Œæ¨ç†æ—¶é—´ï¼Œå› ä¸ºæ¨¡å‹éœ€è¦ç”Ÿæˆå¹¶è¯„ä¼°å¤šä¸ªå€™é€‰è§£å†³æ–¹æ¡ˆï¼Œä»¥è¯†åˆ«å¯è¡Œçš„æ¨ç†è·¯å¾„ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å°†æœç´¢èƒ½åŠ›ç›´æ¥é›†æˆåˆ°æ¨¡å‹ä¸­çš„æœ‰æ•ˆæ–¹æ³•ï¼Œé€šè¿‡å¾®è°ƒæ¨¡å‹ï¼Œä½¿ç”¨æ¥è‡ªå¤šç§æœç´¢æ–¹æ³•çš„æˆåŠŸï¼ˆå­¦ä¹ ï¼‰å’Œå¤±è´¥æ¨ç†è·¯å¾„ï¼ˆé—å¿˜ï¼‰ã€‚è™½ç„¶ç”¨è¿™äº›æ•°æ®å¾®è°ƒæ¨¡å‹çœ‹ä¼¼ç®€å•ï¼Œä½†æˆ‘ä»¬å‘ç°äº†ä¸€ä¸ªå…³é”®é—®é¢˜ï¼šå¦‚æœç›²ç›®è¿›è¡Œå¾®è°ƒï¼Œæ¨¡å‹çš„æœç´¢èƒ½åŠ›å¾€å¾€ä¼šè¿…é€Ÿä¸‹é™ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œé€šè¿‡é‡‡ç”¨è¾ƒå°çš„å­¦ä¹ ç‡ï¼Œå¯ä»¥å¤§å¤§ç¼“è§£è¿™ç§é€€åŒ–ã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„â€œäºŒåå››ç‚¹æ¸¸æˆâ€å’Œå€’è®¡æ—¶æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•çš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…ä¼˜äºæ ‡å‡†å¾®è°ƒæ–¹æ³•å’Œæ¨ç†æ—¶é—´æœç´¢åŸºå‡†æµ‹è¯•ï¼Œè€Œä¸”é€šè¿‡å‡å°‘180å€çš„æ¨ç†æ—¶é—´ï¼Œå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11364v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ä¸­åˆ©ç”¨æ¨ç†æ—¶é—´æœç´¢å·²è¢«è¯æ˜å¯è¿›ä¸€æ­¥æé«˜è®­ç»ƒæ¨¡å‹è§£å†³å¤æ‚æ•°å­¦å’Œæ¨ç†é—®é¢˜çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•ä¼šå¢åŠ è®¡ç®—æˆæœ¬å’Œæ¨ç†æ—¶é—´ï¼Œå› ä¸ºæ¨¡å‹éœ€è¦ç”Ÿæˆå¹¶è¯„ä¼°å¤šä¸ªå€™é€‰è§£å†³æ–¹æ¡ˆæ¥è¯†åˆ«å¯è¡Œçš„æ¨ç†è·¯å¾„ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å°†æœç´¢èƒ½åŠ›ç›´æ¥é›†æˆåˆ°æ¨¡å‹ä¸­çš„æœ‰æ•ˆæ–¹æ³•ï¼Œé€šè¿‡å¾®è°ƒæ¨¡å‹ï¼Œä½¿ç”¨æ¥è‡ªä¸åŒæœç´¢æ–¹æ³•çš„æˆåŠŸï¼ˆå­¦ä¹ ï¼‰å’Œå¤±è´¥æ¨ç†è·¯å¾„ï¼ˆé—å¿˜ï¼‰ã€‚è™½ç„¶ä½¿ç”¨è¿™äº›æ•°æ®å¾®è°ƒæ¨¡å‹çœ‹ä¼¼ç®€å•ï¼Œä½†æˆ‘ä»¬å‘ç°äº†ä¸€ä¸ªå…³é”®é—®é¢˜ï¼šå¦‚æœç›²ç›®è¿›è¡Œå¾®è°ƒï¼Œæ¨¡å‹çš„æœç´¢èƒ½åŠ›ä¼šè¿…é€Ÿä¸‹é™ã€‚æˆ‘ä»¬é€šè¿‡ä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡æ¥å±•ç¤ºè¿™ç§é€€åŒ–å¯ä»¥å¤§å¤§ç¼“è§£ã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„Game-of-24å’ŒCountdownæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•çš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…ä¼˜äºæ ‡å‡†å¾®è°ƒæ–¹æ³•å’Œæ¨ç†æ—¶é—´æœç´¢åŸºå‡†æµ‹è¯•ï¼Œè€Œä¸”æ¨ç†æ—¶é—´ä¹Ÿå‡å°‘äº†180å€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¨ç†æ—¶é—´æœç´¢åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­èƒ½å¢å¼ºè§£å†³å¤æ‚æ•°å­¦å’Œæ¨ç†é—®é¢˜çš„èƒ½åŠ›ã€‚</li>
<li>æ¨ç†æ—¶é—´æœç´¢ä¼šå¢åŠ è®¡ç®—æˆæœ¬å’Œæ¨ç†æ—¶é—´ï¼Œå› ä¸ºéœ€è¦è¯„ä¼°å¤šä¸ªå€™é€‰è§£å†³æ–¹æ¡ˆã€‚</li>
<li>ç›´æ¥å°†æœç´¢èƒ½åŠ›é›†æˆåˆ°æ¨¡å‹ä¸­æ˜¯ä¸€ç§æœ‰æ•ˆçš„è§£å†³æ–¹æ³•ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨æˆåŠŸå’Œå¤±è´¥çš„æ¨ç†è·¯å¾„è¿›è¡Œå¾®è°ƒå¯ä»¥æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>ç›²ç›®å¾®è°ƒä¼šå¯¼è‡´æ¨¡å‹æœç´¢èƒ½åŠ›è¿…é€Ÿä¸‹é™ã€‚</li>
<li>ä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡å¯ä»¥æ˜¾è‘—ç¼“è§£æ¨¡å‹æ€§èƒ½çš„é€€åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11364">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b10d9745584f69750a9624c79a2e8d23.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8b3b7ea3942140105c693bdfabfa785a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-41adb84ed68e74a63a8994dc0aace7f8.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Kimina-Prover-Preview-Towards-Large-Formal-Reasoning-Models-with-Reinforcement-Learning"><a href="#Kimina-Prover-Preview-Towards-Large-Formal-Reasoning-Models-with-Reinforcement-Learning" class="headerlink" title="Kimina-Prover Preview: Towards Large Formal Reasoning Models with   Reinforcement Learning"></a>Kimina-Prover Preview: Towards Large Formal Reasoning Models with   Reinforcement Learning</h2><p><strong>Authors:Haiming Wang, Mert Unsal, Xiaohan Lin, Mantas Baksys, Junqi Liu, Marco Dos Santos, Flood Sung, Marina Vinyes, Zhenzhe Ying, Zekai Zhu, Jianqiao Lu, Hugues de SaxcÃ©, Bolton Bailey, Chendong Song, Chenjun Xiao, Dehao Zhang, Ebony Zhang, Frederick Pu, Han Zhu, Jiawei Liu, Jonas Bayer, Julien Michel, Longhui Yu, LÃ©o Dreyfus-Schmidt, Lewis Tunstall, Luigi Pagani, Moreira Machado, Pauline Bourigault, Ran Wang, Stanislas Polu, Thibaut Barroyer, Wen-Ding Li, Yazhe Niu, Yann Fleureau, Yangyang Hu, Zhouliang Yu, Zihan Wang, Zhilin Yang, Zhengying Liu, Jia Li</strong></p>
<p>We introduce Kimina-Prover Preview, a large language model that pioneers a novel reasoning-driven exploration paradigm for formal theorem proving, as showcased in this preview release. Trained with a large-scale reinforcement learning pipeline from Qwen2.5-72B, Kimina-Prover demonstrates strong performance in Lean 4 proof generation by employing a structured reasoning pattern we term \textit{formal reasoning pattern}. This approach allows the model to emulate human problem-solving strategies in Lean, iteratively generating and refining proof steps. Kimina-Prover sets a new state-of-the-art on the miniF2F benchmark, reaching 80.7% with pass@8192. Beyond improved benchmark performance, our work yields several key insights: (1) Kimina-Prover exhibits high sample efficiency, delivering strong results even with minimal sampling (pass@1) and scaling effectively with computational budget, stemming from its unique reasoning pattern and RL training; (2) we demonstrate clear performance scaling with model size, a trend previously unobserved for neural theorem provers in formal mathematics; (3) the learned reasoning style, distinct from traditional search algorithms, shows potential to bridge the gap between formal verification and informal mathematical intuition. We open source distilled versions with 1.5B and 7B parameters of Kimina-Prover </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†Kimina-Prover Previewç‰ˆæœ¬ï¼Œè¿™æ˜¯ä¸€æ¬¾å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œç‡å…ˆé‡‡ç”¨æ–°å‹æ¨ç†é©±åŠ¨çš„æ¢ç´¢æ¨¡å¼æ¥è¿›è¡Œå½¢å¼åŒ–å®šç†è¯æ˜ã€‚Kimina-Proverå€ŸåŠ©æ¥è‡ªQwen 2.5åˆ°72Bçš„å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ æµæ°´çº¿è¿›è¡Œè®­ç»ƒï¼Œé€šè¿‡æˆ‘ä»¬ç§°ä¹‹ä¸ºâ€œå½¢å¼æ¨ç†æ¨¡å¼â€çš„ç»“æ„åŒ–æ¨ç†æ¨¡å¼ï¼Œåœ¨Lean 4è¯æ˜ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚è¿™ç§æ–¹æ³•ä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨Leanä¸­æ¨¡æ‹Ÿäººç±»è§£å†³é—®é¢˜çš„ç­–ç•¥ï¼Œå¹¶å¯ä»¥è¿­ä»£åœ°ç”Ÿæˆå’Œå®Œå–„è¯æ˜æ­¥éª¤ã€‚Kimina-Proveråœ¨miniF2FåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼Œä»¥pass@8192çš„æˆç»©è¾¾åˆ°80.7%ã€‚é™¤äº†æé«˜åŸºå‡†æµ‹è¯•æ€§èƒ½å¤–ï¼Œæˆ‘ä»¬çš„å·¥ä½œè¿˜è·å¾—äº†å‡ ä¸ªå…³é”®è§è§£ï¼šï¼ˆ1ï¼‰Kimina-Proverå±•ç°å‡ºå¾ˆé«˜çš„æ ·æœ¬æ•ˆç‡ï¼Œå³ä½¿åœ¨æœ€ä½é‡‡æ ·æ¡ä»¶ä¸‹ä¹Ÿèƒ½äº§ç”Ÿå¼ºæœ‰åŠ›çš„ç»“æœï¼ˆpass@1ï¼‰ï¼Œå¹¶éšç€è®¡ç®—é¢„ç®—çš„å¢åŠ è€Œæœ‰æ•ˆæ‰©å±•ï¼Œè¿™æºäºå…¶ç‹¬ç‰¹çš„æ¨ç†æ¨¡å¼å’Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼›ï¼ˆ2ï¼‰æˆ‘ä»¬è¯æ˜äº†æ¨¡å‹æ€§èƒ½éšç€æ¨¡å‹è§„æ¨¡çš„æ‰©å¤§è€Œæé«˜ï¼Œè¿™åœ¨å½¢å¼æ•°å­¦çš„ç¥ç»ç½‘ç»œå®šç†è¯æ˜å™¨ä¸­æ­¤å‰å¹¶æœªè§‚å¯Ÿåˆ°è¿™ä¸€è¶‹åŠ¿ï¼›ï¼ˆ3ï¼‰å­¦ä¹ åˆ°çš„æ¨ç†é£æ ¼ä¸ä¼ ç»Ÿæœç´¢ç®—æ³•æˆªç„¶ä¸åŒï¼Œæ˜¾ç¤ºäº†åœ¨å¼¥åˆå½¢å¼éªŒè¯å’Œéæ­£å¼æ•°å­¦ç›´è§‰ä¹‹é—´å·®è·çš„æ½œåŠ›ã€‚æˆ‘ä»¬å…¬å¼€äº†å‚æ•°ä¸º1.5Bå’Œ7Bçš„Kimina-Proverç²¾ç®€ç‰ˆæœ¬ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11354v1">PDF</a> 22 pages</p>
<p><strong>Summary</strong>ï¼š</p>
<p>Kimina-Proveré¢„è§ˆç‰ˆæ˜¯ä¸€æ¬¾å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå®ƒå¼€åˆ›äº†å½¢å¼åŒ–å®šç†è¯æ˜ä¸­çš„æ¨ç†é©±åŠ¨æ¢ç´¢æ¨¡å¼ã€‚è¯¥æ¨¡å‹ä½¿ç”¨Qwen2.5-72Bçš„å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ ç®¡é“è¿›è¡Œè®­ç»ƒï¼Œå±•ç°äº†åœ¨Lean 4è¯æ˜ç”Ÿæˆä¸­çš„å‡ºè‰²è¡¨ç°ã€‚Kiminar Proveré‡‡ç”¨æˆ‘ä»¬ç§°ä¹‹ä¸ºâ€œå½¢å¼æ¨ç†æ¨¡å¼â€çš„ç»“æ„åŒ–æ¨ç†æ¨¡å¼ï¼Œèƒ½å¤Ÿæ¨¡æ‹Ÿäººç±»åœ¨Leanä¸­çš„é—®é¢˜è§£å†³ç­–ç•¥ï¼Œå¹¶è¿­ä»£åœ°ç”Ÿæˆå’Œç»†åŒ–è¯æ˜æ­¥éª¤ã€‚è¯¥æ¨¡å‹åœ¨miniF2FåŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æ–°çš„æŠ€æœ¯é«˜åº¦ï¼Œpass@8192å¾—åˆ†ä¸º80.7%ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>Kimina-Proverå¼•å…¥äº†ä¸€ç§æ–°çš„å½¢å¼åŒ–å®šç†è¯æ˜ä¸­çš„æ¨ç†é©±åŠ¨æ¢ç´¢æ¨¡å¼ã€‚</li>
<li>è¯¥æ¨¡å‹ä½¿ç”¨å¤§è§„æ¨¡çš„å¼ºåŒ–å­¦ä¹ ç®¡é“è¿›è¡Œè®­ç»ƒï¼Œå±•ç°äº†åœ¨Lean 4è¯æ˜ç”Ÿæˆä¸­çš„é«˜æ•ˆæ€§èƒ½ã€‚</li>
<li>Kimina-Proveré‡‡ç”¨äº†å½¢å¼æ¨ç†æ¨¡å¼ï¼Œèƒ½å¤Ÿæ¨¡æ‹Ÿäººç±»çš„è¯æ˜è§£å†³ç­–ç•¥ã€‚</li>
<li>è¯¥æ¨¡å‹å…·æœ‰é«˜æ ·æœ¬æ•ˆç‡ï¼Œå³ä½¿åœ¨æœ€å°çš„é‡‡æ ·ä¸‹ä¹Ÿèƒ½äº§ç”Ÿå¼ºå¤§çš„ç»“æœï¼Œå¹¶èƒ½æœ‰æ•ˆåœ°éšç€è®¡ç®—é¢„ç®—è€Œæ‰©å±•ã€‚</li>
<li>éšç€æ¨¡å‹è§„æ¨¡çš„å¢åŠ ï¼Œæ€§èƒ½å¾—åˆ°äº†æ˜æ˜¾çš„æå‡ï¼Œè¿™åœ¨ä¹‹å‰çš„ç¥ç»å®šç†è¯æ˜å™¨ä¸­æ˜¯æ²¡æœ‰è§‚å¯Ÿåˆ°çš„è¶‹åŠ¿ã€‚</li>
<li>æ‰€å­¦åˆ°çš„æ¨ç†é£æ ¼ä¸ä¼ ç»Ÿæœç´¢ç®—æ³•ä¸åŒï¼Œæœ‰æœ›å¼¥åˆå½¢å¼éªŒè¯ä¸éæ­£å¼æ•°å­¦ç›´è§‰ä¹‹é—´çš„å·®è·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11354">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9ed07dd198e12dcd7b9703aba6e3df3d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0d5284a83425a4889e6bf046975eebf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-06a159bbd6fa2a5b76350f927c68709f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cdc759d1b9e92edfb482c412b795050a.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Nondeterministic-Polynomial-time-Problem-Challenge-An-Ever-Scaling-Reasoning-Benchmark-for-LLMs"><a href="#Nondeterministic-Polynomial-time-Problem-Challenge-An-Ever-Scaling-Reasoning-Benchmark-for-LLMs" class="headerlink" title="Nondeterministic Polynomial-time Problem Challenge: An Ever-Scaling   Reasoning Benchmark for LLMs"></a>Nondeterministic Polynomial-time Problem Challenge: An Ever-Scaling   Reasoning Benchmark for LLMs</h2><p><strong>Authors:Chang Yang, Ruiyu Wang, Junzhe Jiang, Qi Jiang, Qinggang Zhang, Yanchen Deng, Shuxin Li, Shuyue Hu, Bo Li, Florian T. Pokorny, Xiao Huang, Xinrun Wang</strong></p>
<p>Reasoning is the fundamental capability of large language models (LLMs). Due to the rapid progress of LLMs, there are two main issues of current benchmarks: i) these benchmarks can be crushed in a short time (less than 1 year), and ii) these benchmarks may be easily hacked. To handle these issues, we propose the ever-scalingness for building the benchmarks which are uncrushable, unhackable, auto-verifiable and general. This paper presents Nondeterministic Polynomial-time Problem Challenge (NPPC), an ever-scaling reasoning benchmark for LLMs. Specifically, the NPPC has three main modules: i) npgym, which provides a unified interface of 25 well-known NP-complete problems and can generate any number of instances with any levels of complexities, ii) npsolver: which provides a unified interface to evaluate the problem instances with both online and offline models via APIs and local deployments, respectively, and iii) npeval: which provides the comprehensive and ready-to-use tools to analyze the performances of LLMs over different problems, the number of tokens, the aha moments, the reasoning errors and the solution errors. Extensive experiments over widely-used LLMs demonstrate: i) NPPC can successfully decrease the performances of advanced LLMsâ€™ performances to below 10%, demonstrating that NPPC is uncrushable, ii) DeepSeek-R1, Claude-3.7-Sonnet, and o1&#x2F;o3-mini are the most powerful LLMs, where DeepSeek-R1 outperforms Claude-3.7-Sonnet and o1&#x2F;o3-mini in most NP-complete problems considered, and iii) the numbers of tokens, aha moments in the advanced LLMs, e.g., Claude-3.7-Sonnet and DeepSeek-R1, are observed first to increase and then decrease when the problem instances become more and more difficult. We believe that NPPC is the first ever-scaling reasoning benchmark, serving as the uncrushable and unhackable testbed for LLMs toward artificial general intelligence (AGI). </p>
<blockquote>
<p>æ¨ç†æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åŸºæœ¬èƒ½åŠ›ã€‚ç”±äºLLMçš„å¿«é€Ÿå‘å±•ï¼Œå½“å‰åŸºå‡†æµ‹è¯•é¢ä¸´ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼šä¸€æ˜¯è¿™äº›åŸºå‡†æµ‹è¯•å¯ä»¥åœ¨å¾ˆçŸ­çš„æ—¶é—´å†…ï¼ˆä¸åˆ°ä¸€å¹´ï¼‰è¢«è¶…è¶Šï¼ŒäºŒæ˜¯è¿™äº›åŸºå‡†æµ‹è¯•å¾ˆå®¹æ˜“è¢«ç ´è§£ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºæ„å»ºä¸å¯è¶…è¶Šã€ä¸å¯ç ´è§£ã€å¯è‡ªåŠ¨éªŒè¯å’Œé€šç”¨çš„åŸºå‡†æµ‹è¯•ï¼Œä»¥å®ç°æŒç»­æ‰©å±•ã€‚æœ¬æ–‡ä»‹ç»äº†éç¡®å®šæ€§å¤šé¡¹å¼æ—¶é—´é—®é¢˜æŒ‘æˆ˜ï¼ˆNPPCï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹LLMçš„æŒç»­æ‰©å±•æ¨ç†åŸºå‡†æµ‹è¯•ã€‚å…·ä½“æ¥è¯´ï¼ŒNPPCæœ‰ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šä¸€ã€npgymï¼Œå®ƒæä¾›äº†25ä¸ªè‘—åçš„NPå®Œå…¨é—®é¢˜çš„ç»Ÿä¸€æ¥å£ï¼Œå¯ä»¥ç”Ÿæˆä»»ä½•æ•°é‡å’Œä»»ä½•å¤æ‚ç¨‹åº¦çš„å®ä¾‹ï¼›äºŒã€npsolverï¼Œå®ƒé€šè¿‡APIå’Œæœ¬åœ°éƒ¨ç½²æä¾›ç»Ÿä¸€æ¥å£ï¼Œå¯ä»¥è¯„ä¼°é—®é¢˜å®ä¾‹çš„åœ¨çº¿å’Œç¦»çº¿æ¨¡å‹ï¼›ä¸‰ã€npevalï¼Œå®ƒæä¾›å…¨é¢ä¸”ç°æˆå¯ç”¨çš„å·¥å…·ï¼Œåˆ†æLLMåœ¨ä¸åŒé—®é¢˜ã€ä»¤ç‰Œæ•°é‡ã€é¡¿æ‚Ÿæ—¶åˆ»ã€æ¨ç†é”™è¯¯å’Œè§£å†³æ–¹æ¡ˆé”™è¯¯æ–¹é¢çš„æ€§èƒ½ã€‚åœ¨å¹¿æ³›ä½¿ç”¨çš„LLMä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼šä¸€ã€NPPCèƒ½å¤ŸæˆåŠŸåœ°å°†å…ˆè¿›LLMçš„æ€§èƒ½é™ä½åˆ°10%ä»¥ä¸‹ï¼Œè¡¨æ˜NPPCæ˜¯ä¸å¯è¶…è¶Šçš„ï¼›äºŒã€DeepSeek-R1ã€Claude-3.7-Sonnetå’Œo1&#x2F;o3-miniæ˜¯æœ€å¼ºå¤§çš„LLMï¼Œå…¶ä¸­DeepSeek-R1åœ¨å¤§å¤šæ•°è€ƒè™‘çš„NPå®Œå…¨é—®é¢˜ä¸­ä¼˜äºClaude-3.7-Sonnetå’Œo1&#x2F;o3-miniï¼›ä¸‰ã€åœ¨å…ˆè¿›çš„LLMï¼ˆå¦‚Claude-3.7-Sonnetå’ŒDeepSeek-R1ï¼‰ä¸­ï¼Œéšç€é—®é¢˜å®ä¾‹å˜å¾—è¶Šæ¥è¶Šå›°éš¾ï¼Œä»¤ç‰Œæ•°é‡å’Œé¡¿æ‚Ÿæ—¶åˆ»é¦–å…ˆå¢åŠ ç„¶åå‡å°‘ã€‚æˆ‘ä»¬ç›¸ä¿¡ï¼ŒNPPCæ˜¯ç¬¬ä¸€ä¸ªæŒç»­æ‰©å±•çš„æ¨ç†åŸºå‡†æµ‹è¯•ï¼Œä½œä¸ºä¸å¯è¶…è¶Šå’Œä¸å¯ç ´è§£çš„æµ‹è¯•å¹³å°ï¼Œä¸ºLLMå®ç°äººå·¥æ™ºèƒ½é€šç”¨æ€§ï¼ˆAGIï¼‰æä¾›äº†æ”¯æŒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11239v1">PDF</a> Preliminary work, 10 pages for main text</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹çš„æ ¹æœ¬èƒ½åŠ›æ˜¯æ¨ç†èƒ½åŠ›ã€‚ç”±äºå¤§å‹è¯­è¨€æ¨¡å‹å‘å±•è¿…é€Ÿï¼Œå½“å‰å­˜åœ¨çš„åŸºå‡†æµ‹è¯•é¢ä¸´ä¸¤å¤§é—®é¢˜ï¼šä¸€ã€è¿™äº›åŸºå‡†æµ‹è¯•å¯ä»¥åœ¨çŸ­æ—¶é—´å†…ï¼ˆä¸åˆ°ä¸€å¹´ï¼‰è¢«çªç ´ï¼›äºŒã€è¿™äº›åŸºå‡†æµ‹è¯•å®¹æ˜“è¢«æ“çºµã€‚ä¸ºäº†åº”å¯¹è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸å¯æ”»ç ´ã€ä¸å¯æ“çºµã€å¯è‡ªåŠ¨éªŒè¯å’Œé€šç”¨çš„æ°¸æ’æ‰©å±•åŸºå‡†æµ‹è¯•ã€‚æœ¬æ–‡ä»‹ç»äº†éç¡®å®šæ€§å¤šé¡¹å¼æ—¶é—´é—®é¢˜æŒ‘æˆ˜ï¼ˆNPPCï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„æ°¸æ’æ‰©å±•æ¨ç†åŸºå‡†æµ‹è¯•ã€‚NPPCä¸»è¦åŒ…æ‹¬ä¸‰ä¸ªæ¨¡å—ï¼šä¸€ã€npgymï¼Œå®ƒæä¾›äº†25ä¸ªè‘—åçš„NPå®Œå…¨é—®é¢˜çš„ç»Ÿä¸€æ¥å£ï¼Œå¯ä»¥ç”Ÿæˆä»»ä½•æ•°é‡ã€ä»»ä½•å¤æ‚ç¨‹åº¦çš„å®ä¾‹ï¼›äºŒã€npsolverï¼Œå®ƒé€šè¿‡APIå’Œæœ¬åœ°éƒ¨ç½²æä¾›é—®é¢˜å®ä¾‹çš„è¯„ä¼°æ¥å£ï¼Œæ—¢å¯ä»¥è¯„ä¼°åœ¨çº¿æ¨¡å‹ä¹Ÿå¯ä»¥è¯„ä¼°ç¦»çº¿æ¨¡å‹ï¼›ä¸‰ã€npevalï¼Œå®ƒæä¾›äº†å…¨é¢ä¸”ç°æˆå¯ç”¨çš„å·¥å…·ï¼Œç”¨äºåˆ†æå¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸åŒé—®é¢˜ã€ä¸åŒæ ‡è®°æ•°é‡ã€çµæ„Ÿæ—¶åˆ»ã€æ¨ç†é”™è¯¯å’Œè§£å†³æ–¹æ¡ˆé”™è¯¯æ–¹é¢çš„æ€§èƒ½ã€‚å®éªŒè¡¨æ˜NPPCæˆåŠŸåœ°å°†å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½é™ä½åˆ°10%ä»¥ä¸‹ï¼Œè¯æ˜äº†NPPCçš„ä¸å¯æ”»ç ´æ€§ï¼Œå¹¶æ­ç¤ºäº†DeepSeek-R1ç­‰æ¨¡å‹çš„ä¼˜åŠ¿ã€‚æˆ‘ä»¬ç›¸ä¿¡NPPCæ˜¯ç¬¬ä¸€ä¸ªæ°¸æ’çš„æ¨ç†åŸºå‡†æµ‹è¯•ï¼Œä½œä¸ºä¸å¯æ”»ç ´å’Œä¸å¯æ“çºµçš„æµ‹è¯•å¹³å°ï¼Œä¸ºå¤§å‹è¯­è¨€æ¨¡å‹è¿ˆå‘äººå·¥æ™ºèƒ½é€šç”¨æ€§æä¾›æ ‡å‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹çš„æ ¸å¿ƒèƒ½åŠ›æ˜¯æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹çš„åŸºå‡†æµ‹è¯•é¢ä¸´å¯å¿«é€Ÿçªç ´å’Œæ˜“è¢«æ“çºµçš„é—®é¢˜ã€‚</li>
<li>æå‡ºäº†éç¡®å®šæ€§å¤šé¡¹å¼æ—¶é—´é—®é¢˜æŒ‘æˆ˜ï¼ˆNPPCï¼‰ï¼Œä¸€ä¸ªé’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„æ°¸æ’æ‰©å±•æ¨ç†åŸºå‡†æµ‹è¯•ã€‚</li>
<li>NPPCåŒ…å«npgymã€npsolverå’Œnpevalä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼Œåˆ†åˆ«ç”¨äºé—®é¢˜ç”Ÿæˆã€é—®é¢˜è¯„ä¼°åŠæ€§èƒ½åˆ†æã€‚</li>
<li>NPPCèƒ½å¤ŸæˆåŠŸé™ä½å…ˆè¿›å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½è‡³10%ä»¥ä¸‹ï¼Œè¯æ˜äº†å…¶ä¸å¯æ”»ç ´æ€§ã€‚</li>
<li>åœ¨NPPCæµ‹è¯•ä¸­ï¼ŒDeepSeek-R1ç­‰æ¨¡å‹è¡¨ç°ä¼˜å¼‚ï¼Œå…¶ä¸­DeepSeek-R1åœ¨å¤šæ•°NPå®Œå…¨é—®é¢˜ä¸Šä¼˜äºClaude-3.7-Sonnetå’Œo1&#x2F;o3-miniã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11239">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-48020c77b367ddbba66bf38e283912fd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-df7046d0b7cc453750b70d708d06a0af.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-46766b1bf9ca8891c32cd9ec941abe11.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="3DAffordSplat-Efficient-Affordance-Reasoning-with-3D-Gaussians"><a href="#3DAffordSplat-Efficient-Affordance-Reasoning-with-3D-Gaussians" class="headerlink" title="3DAffordSplat: Efficient Affordance Reasoning with 3D Gaussians"></a>3DAffordSplat: Efficient Affordance Reasoning with 3D Gaussians</h2><p><strong>Authors:Zeming wei, Junyi Lin, Yang Liu, Weixing Chen, Jingzhou Luo, Guanbin Li, Liang Lin</strong></p>
<p>3D affordance reasoning is essential in associating human instructions with the functional regions of 3D objects, facilitating precise, task-oriented manipulations in embodied AI. However, current methods, which predominantly depend on sparse 3D point clouds, exhibit limited generalizability and robustness due to their sensitivity to coordinate variations and the inherent sparsity of the data. By contrast, 3D Gaussian Splatting (3DGS) delivers high-fidelity, real-time rendering with minimal computational overhead by representing scenes as dense, continuous distributions. This positions 3DGS as a highly effective approach for capturing fine-grained affordance details and improving recognition accuracy. Nevertheless, its full potential remains largely untapped due to the absence of large-scale, 3DGS-specific affordance datasets. To overcome these limitations, we present 3DAffordSplat, the first large-scale, multi-modal dataset tailored for 3DGS-based affordance reasoning. This dataset includes 23,677 Gaussian instances, 8,354 point cloud instances, and 6,631 manually annotated affordance labels, encompassing 21 object categories and 18 affordance types. Building upon this dataset, we introduce AffordSplatNet, a novel model specifically designed for affordance reasoning using 3DGS representations. AffordSplatNet features an innovative cross-modal structure alignment module that exploits structural consistency priors to align 3D point cloud and 3DGS representations, resulting in enhanced affordance recognition accuracy. Extensive experiments demonstrate that the 3DAffordSplat dataset significantly advances affordance learning within the 3DGS domain, while AffordSplatNet consistently outperforms existing methods across both seen and unseen settings, highlighting its robust generalization capabilities. </p>
<blockquote>
<p>ä¸‰ç»´é€‚æ€§æ¨ç†åœ¨å°†äººç±»æŒ‡ä»¤ä¸ä¸‰ç»´ç‰©ä½“çš„åŠŸèƒ½åŒºåŸŸç›¸å…³è”æ–¹é¢è‡³å…³é‡è¦ï¼Œæœ‰åŠ©äºå®ç°å®ä½“äººå·¥æ™ºèƒ½çš„ç²¾ç¡®ã€ä»»åŠ¡å¯¼å‘å‹æ“ä½œã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•ä¸»è¦ä¾èµ–äºç¨€ç–çš„ä¸‰ç»´ç‚¹äº‘ï¼Œç”±äºå…¶å¯¹åæ ‡å˜åŒ–çš„æ•æ„Ÿæ€§å’Œæ•°æ®æœ¬èº«çš„ç¨€ç–æ€§ï¼Œå…¶é€šç”¨æ€§å’Œç¨³å¥æ€§å—åˆ°é™åˆ¶ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œ3Dé«˜æ–¯æ‹¼è´´ï¼ˆ3DGSï¼‰é€šè¿‡é«˜å¯†åº¦ã€è¿ç»­åˆ†å¸ƒæ¥è¡¨ç¤ºåœºæ™¯ï¼Œä»¥æœ€å°çš„è®¡ç®—å¼€é”€å®ç°é«˜ä¿çœŸã€å®æ—¶æ¸²æŸ“ï¼Œæˆä¸ºæ•è·ç²¾ç»†é€‚æ€§ç»†èŠ‚ã€æé«˜è¯†åˆ«ç²¾åº¦çš„æœ‰æ•ˆæ–¹æ³•ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹å¤§è§„æ¨¡ã€é’ˆå¯¹3DGSçš„é€‚æ€§æ•°æ®é›†ï¼Œå…¶å…¨éƒ¨æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†å¼€å‘ã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æ¨å‡ºäº†3DAffordSplatï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä¸“ä¸ºåŸºäº3DGSçš„é€‚æ€§æ¨ç†å®šåˆ¶çš„å¤§è§„æ¨¡ã€å¤šæ¨¡å¼æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†åŒ…å«23677ä¸ªé«˜æ–¯å®ä¾‹ã€8354ä¸ªç‚¹äº‘å®ä¾‹å’Œ6631ä¸ªæ‰‹åŠ¨æ ‡æ³¨çš„é€‚æ€§æ ‡ç­¾ï¼Œæ¶µç›–21ä¸ªå¯¹è±¡ç±»åˆ«å’Œ18ç§é€‚æ€§ç±»å‹ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å¼•å…¥äº†AffordSplatNetï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºä½¿ç”¨3DGSè¡¨ç¤ºè¿›è¡Œé€‚æ€§æ¨ç†è€Œè®¾è®¡çš„æ–°å‹æ¨¡å‹ã€‚AffordSplatNetå…·æœ‰åˆ›æ–°æ€§çš„è·¨æ¨¡æ€ç»“æ„å¯¹é½æ¨¡å—ï¼Œåˆ©ç”¨ç»“æ„ä¸€è‡´æ€§å…ˆéªŒæ¥å¯¹é½ä¸‰ç»´ç‚¹äº‘å’Œä¸‰ç»´é«˜æ–¯æ‹¼è´´è¡¨ç¤ºï¼Œä»è€Œæé«˜é€‚æ€§è¯†åˆ«ç²¾åº¦ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œ3DAffordSplatæ•°æ®é›†åœ¨ä¸‰ç»´é«˜æ–¯æ‹¼è´´åŸŸå†…æ¨åŠ¨äº†é€‚æ€§å­¦ä¹ çš„å‘å±•ï¼Œè€ŒAffordSplatNetåœ¨å·²çŸ¥å’ŒæœªçŸ¥ç¯å¢ƒä¸­å‡è¡¨ç°å‡ºå¯¹ç°æœ‰æ–¹æ³•çš„ä¼˜è¶Šæ€§ï¼Œå‡¸æ˜¾äº†å…¶ç¨³å¥çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11218v1">PDF</a> The first large-scale 3D Gaussians Affordance Reasoning Benchmark</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸‰ç»´é«˜æ–¯è´´å›¾ï¼ˆ3DGSï¼‰åœ¨ä¸‰ç»´ç‰©ä½“åŠŸèƒ½åŒºåŸŸä¸äººç±»æŒ‡ä»¤å…³è”ä¸­çš„é‡è¦ä½œç”¨ï¼Œå¯¹äºå®ç°ç²¾ç¡®çš„ä»»åŠ¡å¯¼å‘å‹æ“ä½œåœ¨åµŒå…¥å¼äººå·¥æ™ºèƒ½ä¸­å…·æœ‰é‡å¤§æ„ä¹‰ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•ä¸»è¦ä¾èµ–äºç¨€ç–çš„ä¸‰ç»´ç‚¹äº‘ï¼Œè¡¨ç°å‡ºæœ‰é™çš„æ³›åŒ–èƒ½åŠ›å’Œç¨³å¥æ€§ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºäº†é¦–ä¸ªå¤§è§„æ¨¡çš„å¤šæ¨¡å¼æ•°æ®é›†3DAffordSplatï¼Œä»¥åŠåŸºäºè¯¥æ•°æ®é›†çš„AffordSplatNetæ¨¡å‹ã€‚AffordSplatNetè®¾è®¡ä¸“é—¨ç”¨äºåˆ©ç”¨ä¸‰ç»´é«˜æ–¯è´´å›¾è¡¨ç¤ºè¿›è¡Œä»¿å°„æ¨ç†ï¼ŒåŒ…æ‹¬åˆ›æ–°æ€§çš„è·¨æ¨¡æ€ç»“æ„å¯¹é½æ¨¡å—ï¼Œå¯ä»¥åŠ å¼ºç‚¹äº‘å’Œä¸‰ç»´é«˜æ–¯è´´å›¾è¡¨ç¤ºçš„å¯¹é½ï¼Œä»è€Œæé«˜ä»¿å°„è¯†åˆ«ç²¾åº¦ã€‚å®éªŒè¡¨æ˜ï¼ŒAffordSplatNetåœ¨æœªè§è¿‡çš„è®¾ç½®ä¸‹ä¹Ÿè¡¨ç°ä¼˜å¼‚ï¼Œå…·æœ‰ç¨³å¥çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>3D affordance reasoningåœ¨å…³è”äººç±»æŒ‡ä»¤ä¸ä¸‰ç»´ç‰©ä½“åŠŸèƒ½åŒºåŸŸæ–¹é¢è‡³å…³é‡è¦ï¼Œå¯¹åµŒå…¥å¼äººå·¥æ™ºèƒ½çš„ç²¾ç¡®ä»»åŠ¡å¯¼å‘æ“ä½œæœ‰é‡è¦æ„ä¹‰ã€‚</li>
<li>å½“å‰æ–¹æ³•ä¸»è¦ä¾èµ–ç¨€ç–çš„ä¸‰ç»´ç‚¹äº‘ï¼Œå­˜åœ¨æ³›åŒ–èƒ½åŠ›å’Œç¨³å¥æ€§æ–¹é¢çš„å±€é™æ€§ã€‚</li>
<li>3D Gaussian Splatting (3DGS)èƒ½æä¾›é«˜ä¿çœŸã€å®æ—¶çš„æ¸²æŸ“ï¼Œä¸”è®¡ç®—å¼€é”€å°ï¼Œèƒ½å¤Ÿæ•æ‰ç²¾ç»†çš„ä»¿å°„ç»†èŠ‚å¹¶æé«˜è¯†åˆ«ç²¾åº¦ã€‚ä½†å…¶æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†å‘æ˜ã€‚</li>
<li>ç¼ºä¹é’ˆå¯¹ä¸‰ç»´é«˜æ–¯è´´å›¾çš„å¤§è§„æ¨¡ä»¿å°„æ•°æ®é›†æ˜¯é™åˆ¶å…¶åº”ç”¨çš„å…³é”®å› ç´ ä¹‹ä¸€ã€‚ä¸ºæ­¤æœ¬æ–‡æ¨å‡ºäº†é¦–ä¸ªå¤§è§„æ¨¡å¤šæ¨¡å¼æ•°æ®é›†â€”â€”3DAffordSplatã€‚</li>
<li>åŸºäºè¯¥æ•°æ®é›†ï¼Œå¼•å…¥äº†AffordSplatNetæ¨¡å‹ï¼Œä¸“é—¨ç”¨äºåˆ©ç”¨ä¸‰ç»´é«˜æ–¯è´´å›¾è¡¨ç¤ºè¿›è¡Œä»¿å°„æ¨ç†ã€‚åŒ…æ‹¬åˆ›æ–°çš„è·¨æ¨¡æ€ç»“æ„å¯¹é½æ¨¡å—ï¼Œèƒ½åŠ å¼ºç‚¹äº‘å’Œä¸‰ç»´é«˜æ–¯è´´å›¾è¡¨ç¤ºçš„å¯¹é½ã€‚</li>
<li>å®éªŒè¡¨æ˜AffordSplatNetåœ¨ä»¿å°„å­¦ä¹ æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨æœªè§è¿‡çš„è®¾ç½®ä¸‹å…·æœ‰ç¨³å¥çš„æ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ¨¡å‹ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11218">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-216456d2f7ee65a24dfad052cfbfd452.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1585573944d0fdb046da2674e80fc53e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ede83fade29a7d3fc596e6da0f66782f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0526e9dd3ee3b4aa3815dc333195c2a7.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Benchmarking-Next-Generation-Reasoning-Focused-Large-Language-Models-in-Ophthalmology-A-Head-to-Head-Evaluation-on-5-888-Items"><a href="#Benchmarking-Next-Generation-Reasoning-Focused-Large-Language-Models-in-Ophthalmology-A-Head-to-Head-Evaluation-on-5-888-Items" class="headerlink" title="Benchmarking Next-Generation Reasoning-Focused Large Language Models in   Ophthalmology: A Head-to-Head Evaluation on 5,888 Items"></a>Benchmarking Next-Generation Reasoning-Focused Large Language Models in   Ophthalmology: A Head-to-Head Evaluation on 5,888 Items</h2><p><strong>Authors:Minjie Zou, Sahana Srinivasan, Thaddaeus Wai Soon Lo, Ke Zou, Gabriel Dawei Yang, Xuguang Ai, Hyunjae Kim, Maxwell Singer, Fares Antaki, Kelvin Li, Robert Chang, Marcus Tan, David Ziyou Chen, Dianbo Liu, Qingyu Chen, Yih Chung Tham</strong></p>
<p>Recent advances in reasoning-focused large language models (LLMs) mark a shift from general LLMs toward models designed for complex decision-making, a crucial aspect in medicine. However, their performance in specialized domains like ophthalmology remains underexplored. This study comprehensively evaluated and compared the accuracy and reasoning capabilities of four newly developed reasoning-focused LLMs, namely DeepSeek-R1, OpenAI o1, o3-mini, and Gemini 2.0 Flash-Thinking. Each model was assessed using 5,888 multiple-choice ophthalmology exam questions from the MedMCQA dataset in zero-shot setting. Quantitative evaluation included accuracy, Macro-F1, and five text-generation metrics (ROUGE-L, METEOR, BERTScore, BARTScore, and AlignScore), computed against ground-truth reasonings. Average inference time was recorded for a subset of 100 randomly selected questions. Additionally, two board-certified ophthalmologists qualitatively assessed clarity, completeness, and reasoning structure of responses to differential diagnosis questions.O1 (0.902) and DeepSeek-R1 (0.888) achieved the highest accuracy, with o1 also leading in Macro-F1 (0.900). The performance of models across the text-generation metrics varied: O3-mini excelled in ROUGE-L (0.151), o1 in METEOR (0.232), DeepSeek-R1 and o3-mini tied for BERTScore (0.673), DeepSeek-R1 (-4.105) and Gemini 2.0 Flash-Thinking (-4.127) performed best in BARTScore, while o3-mini (0.181) and o1 (0.176) led AlignScore. Inference time across the models varied, with DeepSeek-R1 being slowest (40.4 seconds) and Gemini 2.0 Flash-Thinking fastest (6.7 seconds). Qualitative evaluation revealed that DeepSeek-R1 and Gemini 2.0 Flash-Thinking tended to provide detailed and comprehensive intermediate reasoning, whereas o1 and o3-mini displayed concise and summarized justifications. </p>
<blockquote>
<p>è¿‘æœŸé’ˆå¯¹æ¨ç†çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥æ ‡å¿—ç€ä»é€šç”¨LLMå‘ç”¨äºå¤æ‚å†³ç­–è®¾è®¡çš„æ¨¡å‹è½¬å˜ï¼Œè¿™åœ¨åŒ»å­¦ä¸­æ˜¯ä¸€ä¸ªè‡³å…³é‡è¦çš„æ–¹é¢ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨çœ¼ç§‘ç­‰ç‰¹å®šé¢†åŸŸçš„åº”ç”¨æ€§èƒ½ä»ç„¶è¢«æ¢ç´¢ä¸è¶³ã€‚æœ¬ç ”ç©¶å…¨é¢è¯„ä¼°å¹¶æ¯”è¾ƒäº†å››ç§æœ€æ–°å¼€å‘çš„æ¨ç†å‹LLMçš„å‡†ç¡®æ€§åŠå…¶æ¨ç†èƒ½åŠ›ï¼Œåˆ†åˆ«æ˜¯DeepSeek-R1ã€OpenAI o1ã€o3-miniå’ŒGemini 2.0 Flash-Thinkingã€‚æ¯ä¸ªæ¨¡å‹éƒ½ä½¿ç”¨MedMCQAæ•°æ®é›†ä¸­çš„5,888ä¸ªçœ¼ç§‘è€ƒè¯•é€‰æ‹©é¢˜è¿›è¡Œé›¶æ ·æœ¬è®¾ç½®è¯„ä¼°ã€‚å®šé‡è¯„ä¼°åŒ…æ‹¬å‡†ç¡®æ€§ã€å®F1åˆ†æ•°ä»¥åŠäº”ä¸ªæ–‡æœ¬ç”ŸæˆæŒ‡æ ‡ï¼ˆROUGE-Lã€METEORã€BERTScoreã€BARTScoreå’ŒAlignScoreï¼‰ï¼Œè®¡ç®—æ ‡å‡†æ˜¯åŸºäºå®é™…æ¨ç†ã€‚è®°å½•äº†é’ˆå¯¹éšæœºé€‰æ‹©çš„100ä¸ªé—®é¢˜çš„å¹³å‡æ¨ç†æ—¶é—´ã€‚æ­¤å¤–ï¼Œä¸¤ä½è·å¾—è¯ä¹¦çš„çœ¼ç§‘åŒ»ç”Ÿè¿˜å®šæ€§è¯„ä¼°äº†ç­”æ¡ˆåœ¨é‰´åˆ«è¯Šæ–­é—®é¢˜ä¸Šçš„æ¸…æ™°åº¦ã€å®Œæ•´æ€§å’Œæ¨ç†ç»“æ„ã€‚å…¶ä¸­ï¼ŒO1ï¼ˆå‡†ç¡®ç‡0.902ï¼‰å’ŒDeepSeek-R1ï¼ˆå‡†ç¡®ç‡0.888ï¼‰è·å¾—æœ€é«˜å‡†ç¡®ç‡ï¼Œå¹¶ä¸”åœ¨å®è§‚F1è¯„åˆ†ä¸Šï¼ŒO1ä¹Ÿæ’åç¬¬ä¸€ï¼ˆå¾—åˆ†0.90ï¼‰ã€‚å„æ¨¡å‹åœ¨æ–‡æœ¬ç”ŸæˆæŒ‡æ ‡ä¸Šçš„è¡¨ç°æœ‰æ‰€ä¸åŒï¼šO3-miniåœ¨ROUGE-Lä¸Šè¡¨ç°çªå‡ºï¼ˆå¾—åˆ†0.151ï¼‰ï¼ŒO1åœ¨METEORä¸Šè¡¨ç°ä¼˜å¼‚ï¼ˆå¾—åˆ†0.232ï¼‰ï¼ŒDeepSeek-R1å’Œo3-miniåœ¨BERTScoreä¸Šå¹¶åˆ—ç¬¬ä¸€ï¼ˆå¾—åˆ†å‡ä¸º0.673ï¼‰ï¼Œè€Œåœ¨BARTScoreæ–¹é¢ï¼ŒDeepSeek-R1ï¼ˆ-4.105ï¼‰å’ŒGemini 2.0 Flash-Thinkingï¼ˆ-4.127ï¼‰è¡¨ç°æœ€ä½³ï¼›è€Œåœ¨AlignScoreæ–¹é¢ï¼ŒO3-miniï¼ˆå¾—åˆ†0.181ï¼‰å’ŒO1ï¼ˆå¾—åˆ†0.176ï¼‰é¢†å…ˆã€‚å„æ¨¡å‹çš„æ¨ç†æ—¶é—´æœ‰æ‰€ä¸åŒï¼ŒDeepSeek-R1æœ€æ…¢ï¼ˆ40.4ç§’ï¼‰ï¼Œè€ŒGemini 2.0 Flash-Thinkingæœ€å¿«ï¼ˆä»…6.7ç§’ï¼‰ã€‚å®šæ€§è¯„ä¼°è¡¨æ˜ï¼ŒDeepSeek-R1å’ŒGemini 2.0 Flash-Thinkingçš„ç­”æ¡ˆå€¾å‘äºæä¾›è¯¦ç»†è€Œå…¨é¢çš„ä¸­é—´æ¨ç†è¿‡ç¨‹ï¼Œè€ŒO1å’Œo3-miniåˆ™å±•ç°å‡ºç®€æ´çš„æ€»ç»“æ€§è¯æ˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11186v1">PDF</a> 83 pages, 6 figures, 3 tables, 9 supplementary figures, 7   supplementary tables</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬ç ”ç©¶å…¨é¢è¯„ä¼°äº†å››ç§æœ€æ–°å‘å±•çš„æ¨ç†å¯¼å‘å‹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨çœ¼ç§‘é¢†åŸŸçš„å‡†ç¡®æ€§å’Œæ¨ç†èƒ½åŠ›ï¼ŒåŒ…æ‹¬DeepSeek-R1ã€OpenAI o1ã€o3-miniå’ŒGemini 2.0 Flash-Thinkingã€‚ç ”ç©¶ä½¿ç”¨MedMCQAæ•°æ®é›†ä¸­çš„5,888é“çœ¼ç§‘è€ƒè¯•é€‰æ‹©é¢˜è¿›è¡Œé›¶æ ·æœ¬è®¾ç½®ä¸‹çš„è¯„ä¼°ã€‚é€šè¿‡å‡†ç¡®æ€§ã€Macro-F1ä»¥åŠäº”é¡¹æ–‡æœ¬ç”ŸæˆæŒ‡æ ‡ï¼ˆROUGE-Lã€METEORã€BERTScoreã€BARTScoreå’ŒAlignScoreï¼‰å¯¹æ¨¡å‹è¿›è¡Œå®šé‡è¯„ä»·ï¼Œå¹¶ä¸çœŸå®æ¨ç†è¿›è¡Œå¯¹æ¯”ã€‚æ­¤å¤–ï¼Œè¿˜è®°å½•äº†100é“éšæœºé€‰æ‹©çš„é—®é¢˜çš„å¹³å‡æ¨ç†æ—¶é—´ã€‚åŒæ—¶ï¼Œä¸¤ä½è®¤è¯çœ¼ç§‘åŒ»ç”Ÿå¯¹æ¨¡å‹åœ¨å·®å¼‚è¯Šæ–­é—®é¢˜ä¸Šçš„å›ç­”æ¸…æ™°åº¦ã€å®Œæ•´æ€§å’Œæ¨ç†ç»“æ„è¿›è¡Œäº†å®šæ€§è¯„ä¼°ã€‚ç»“æœæ˜¾ç¤ºï¼ŒO1å’ŒDeepSeek-R1åœ¨å‡†ç¡®æ€§ä¸Šè¡¨ç°æœ€ä½³ï¼Œå…¶ä¸­O1åœ¨Macro-F1ä¸Šä¹Ÿé¢†å…ˆã€‚å„æ¨¡å‹åœ¨æ–‡æœ¬ç”ŸæˆæŒ‡æ ‡ä¸Šçš„è¡¨ç°æœ‰æ‰€ä¸åŒã€‚åœ¨æ¨ç†æ—¶é—´ä¸Šï¼ŒDeepSeek-R1æœ€æ…¢ï¼Œè€ŒGemini 2.0 Flash-Thinkingæœ€å¿«ã€‚å®šæ€§è¯„ä¼°è¡¨æ˜ï¼ŒDeepSeek-R1å’ŒGemini 2.0 Flash-Thinkingçš„æ¨ç†è¿‡ç¨‹è¯¦ç»†å…¨é¢ï¼Œè€ŒO1å’Œo3-miniçš„è®ºè¯ç®€æ´æ¦‚æ‹¬ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è¿‘æœŸå‘å±•çš„æ¨ç†å¯¼å‘å‹å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŒ»å­¦é¢†åŸŸçš„å¤æ‚å†³ç­–èƒ½åŠ›ä¸Šå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚</li>
<li>åœ¨çœ¼ç§‘é¢†åŸŸçš„ç ”ç©¶ä»ç„¶ç›¸å¯¹ä¸è¶³ï¼Œéœ€è¦å¯¹æ¨¡å‹çš„æ€§èƒ½è¿›è¡Œå…¨é¢è¯„ä¼°ã€‚</li>
<li>æœ¬ç ”ç©¶æ¯”è¾ƒäº†å››ç§æ¨ç†å¯¼å‘å‹LLMsçš„å‡†ç¡®æ€§å’Œæ¨ç†èƒ½åŠ›ã€‚</li>
<li>åœ¨å®šé‡è¯„ä¼°ä¸­ï¼ŒO1å’ŒDeepSeek-R1è¡¨ç°å‡ºè¾ƒé«˜çš„å‡†ç¡®æ€§ï¼Œè€ŒO1ä¹Ÿåœ¨Macro-F1ä¸Šé¢†å…ˆã€‚å„æ¨¡å‹åœ¨æ–‡æœ¬ç”ŸæˆæŒ‡æ ‡ä¸Šçš„è¡¨ç°æœ‰æ‰€å·®å¼‚ã€‚</li>
<li>æ¨ç†æ—¶é—´å› æ¨¡å‹è€Œå¼‚ï¼Œå…¶ä¸­DeepSeek-R1æœ€æ…¢è€ŒGemini 2.0 Flash-Thinkingæœ€å¿«ã€‚</li>
<li>å®šæ€§è¯„ä¼°æ˜¾ç¤ºï¼ŒDeepSeek-R1å’ŒGemini 2.0 Flash-Thinkingæä¾›äº†è¯¦ç»†çš„æ¨ç†è¿‡ç¨‹ï¼Œè€ŒO1å’Œo3-miniçš„å›ç­”è¾ƒä¸ºç®€æ´æ¦‚æ‹¬ã€‚è¿™å¯èƒ½å½±å“åœ¨å®é™…åº”ç”¨åœºæ™¯ä¸­çš„æ•ˆæœå’Œéœ€æ±‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11186">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-98ada7c39dffd5e33c4c67da184ccf5f.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Dopamine-Audiobook-A-Training-free-MLLM-Agent-for-Emotional-and-Human-like-Audiobook-Generation"><a href="#Dopamine-Audiobook-A-Training-free-MLLM-Agent-for-Emotional-and-Human-like-Audiobook-Generation" class="headerlink" title="Dopamine Audiobook: A Training-free MLLM Agent for Emotional and   Human-like Audiobook Generation"></a>Dopamine Audiobook: A Training-free MLLM Agent for Emotional and   Human-like Audiobook Generation</h2><p><strong>Authors:Yan Rong, Shan Yang, Guangzhi Lei, Li Liu</strong></p>
<p>Audiobook generation, which creates vivid and emotion-rich audio works, faces challenges in conveying complex emotions, achieving human-like qualities, and aligning evaluations with human preferences. Existing text-to-speech (TTS) methods are often limited to specific scenarios, struggle with emotional transitions, and lack automatic human-aligned evaluation benchmarks, instead relying on either misaligned automated metrics or costly human assessments. To address these issues, we propose Dopamine Audiobook, a new unified training-free system leveraging a multimodal large language model (MLLM) as an AI agent for emotional and human-like audiobook generation and evaluation. Specifically, we first design a flow-based emotion-enhanced framework that decomposes complex emotional speech synthesis into controllable sub-tasks. Then, we propose an adaptive model selection module that dynamically selects the most suitable TTS methods from a set of existing state-of-the-art (SOTA) TTS methods for diverse scenarios. We further enhance emotional expressiveness through paralinguistic augmentation and prosody retrieval at word and utterance levels. For evaluation, we propose a novel GPT-based evaluation framework incorporating self-critique, perspective-taking, and psychological MagicEmo prompts to ensure human-aligned and self-aligned assessments. Experiments show that our method generates long speech with superior emotional expression to SOTA TTS models in various metrics. Importantly, our evaluation framework demonstrates better alignment with human preferences and transferability across audio tasks. Project website with audio samples can be found at <a target="_blank" rel="noopener" href="https://dopamine-audiobook.github.io/">https://dopamine-audiobook.github.io</a>. </p>
<blockquote>
<p>éŸ³é¢‘ä¹¦ç”ŸæˆæŠ€æœ¯èƒ½åˆ›é€ å‡ºç”ŸåŠ¨ã€æƒ…æ„Ÿä¸°å¯Œçš„éŸ³é¢‘ä½œå“ï¼Œä½†åœ¨ä¼ è¾¾å¤æ‚æƒ…æ„Ÿã€å®ç°äººæ€§åŒ–ç‰¹è´¨ã€ä»¥åŠä¸äººç±»åå¥½å¯¹é½è¯„ä¼°æ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚ç°æœ‰çš„æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ–¹æ³•é€šå¸¸å±€é™äºç‰¹å®šåœºæ™¯ï¼Œåœ¨æƒ…æ„Ÿè½¬æ¢æ–¹é¢è¡¨ç°æŒ£æ‰ï¼Œå¹¶ä¸”ç¼ºä¹è‡ªåŠ¨ä¸äººç±»å¯¹é½çš„è¯„ä¼°åŸºå‡†ï¼Œè¿™å¯¼è‡´å®ƒä»¬ä¾èµ–äºé”™ä½è‡ªåŠ¨åŒ–æŒ‡æ ‡æˆ–æˆæœ¬é«˜æ˜‚çš„äººå·¥è¯„ä¼°ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å¤šå·´èƒºéŸ³é¢‘ä¹¦ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„æ— éœ€è®­ç»ƒçš„ç³»ç»Ÿï¼Œåˆ©ç”¨å¤šæ¨¡å¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ä½œä¸ºAIä»£ç†ï¼Œç”¨äºæƒ…æ„Ÿå’Œäººæ€§åŒ–çš„éŸ³é¢‘ä¹¦ç”Ÿæˆå’Œè¯„ä¼°ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆè®¾è®¡äº†ä¸€ä¸ªåŸºäºæµçš„æƒ…æ„Ÿå¢å¼ºæ¡†æ¶ï¼Œå°†å¤æ‚çš„æƒ…æ„Ÿè¯­éŸ³åˆæˆåˆ†è§£æˆå¯æ§çš„å­ä»»åŠ¡ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªè‡ªé€‚åº”æ¨¡å‹é€‰æ‹©æ¨¡å—ï¼Œè¯¥æ¨¡å—å¯ä»¥åŠ¨æ€åœ°ä»ä¸€ç»„æœ€å…ˆè¿›çš„TTSæ–¹æ³•ä¸­é€‰æ‹©æœ€é€‚åˆçš„æ–¹æ³•ï¼Œä»¥é€‚åº”ä¸åŒçš„åœºæ™¯ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥é€šè¿‡è¯è¯­å’Œå¥å­çš„å‰¯è¯­è¨€å¢å¼ºå’Œè¯­è°ƒæ£€ç´¢æ¥å¢å¼ºæƒ…æ„Ÿè¡¨ç°åŠ›ã€‚åœ¨è¯„ä¼°æ–¹é¢ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŸºäºGPTçš„è¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†è‡ªæˆ‘æ‰¹åˆ¤ã€è§‚ç‚¹é‡‡æ‹©å’Œå¿ƒç†MagicEmoæç¤ºï¼Œä»¥ç¡®ä¿ä¸äººç±»å¯¹é½çš„è‡ªæˆ‘è¯„ä¼°ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šç§æŒ‡æ ‡ä¸Šç”Ÿæˆäº†å…·æœ‰ä¼˜è¶Šæƒ…æ„Ÿè¡¨è¾¾èƒ½åŠ›çš„é•¿è¯­éŸ³ï¼Œè¶…è¿‡äº†æœ€å…ˆè¿›çš„TTSæ¨¡å‹ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬çš„è¯„ä¼°æ¡†æ¶ä¸äººç±»åå¥½å¯¹é½æ›´å¥½ï¼Œå¹¶ä¸”åœ¨å„ç§éŸ³é¢‘ä»»åŠ¡ä¹‹é—´å…·æœ‰å¯è¿ç§»æ€§ã€‚é¡¹ç›®ç½‘ç«™å’ŒéŸ³é¢‘æ ·æœ¬å¯åœ¨<a target="_blank" rel="noopener" href="https://dopamine-audiobook.github.ioæ‰¾åˆ°./">https://dopamine-audiobook.github.ioæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11002v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†éŸ³é¢‘ä¹¦ç”Ÿæˆé¢ä¸´çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ä¼ è¾¾å¤æ‚æƒ…ç»ªã€å®ç°äººç±»ç‰¹è´¨å’Œå¯¹é½äººç±»åå¥½è¯„ä¼°ç­‰æ–¹é¢ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†Dopamine Audiobookç³»ç»Ÿï¼Œé‡‡ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºAIä»£ç†ï¼Œè¿›è¡Œæƒ…æ„ŸåŒ–å’Œäººæ€§åŒ–çš„éŸ³é¢‘ä¹¦ç”Ÿæˆä¸è¯„ä¼°ã€‚è®¾è®¡åŸºäºæµçš„æƒ…æ„Ÿå¢å¼ºæ¡†æ¶ï¼Œå°†å¤æ‚çš„æƒ…æ„Ÿè¯­éŸ³åˆæˆåˆ†è§£æˆå¯æ§çš„å­ä»»åŠ¡ã€‚åŒæ—¶ï¼Œæå‡ºè‡ªé€‚åº”æ¨¡å‹é€‰æ‹©æ¨¡å—ï¼Œä»ä¸€ç³»åˆ—å…ˆè¿›çš„TTSæ–¹æ³•ä¸­é€‰æ‹©æœ€é€‚åˆçš„æ–¹æ³•åº”å¯¹ä¸åŒåœºæ™¯ã€‚é€šè¿‡è¯­è¨€å¤–çš„å¢å£°å’Œè¯­è°ƒæ£€ç´¢å¢å¼ºæƒ…æ„Ÿè¡¨è¾¾ã€‚è¯„ä»·æ–¹é¢ï¼Œé‡‡ç”¨åŸºäºGPTçš„è¯„ä»·æ¡†æ¶ï¼Œç»“åˆè‡ªæˆ‘æ‰¹è¯„ã€æ¢ä½æ€è€ƒå’Œå¿ƒç†MagicEmoæç¤ºï¼Œç¡®ä¿ä¸äººç±»åå¥½å’Œè‡ªæˆ‘å¯¹é½çš„è¯„ä¼°ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§æŒ‡æ ‡ä¸Šä¼˜äºç°æœ‰TTSæ¨¡å‹ï¼Œå°¤å…¶åœ¨æƒ…æ„Ÿè¡¨è¾¾æ–¹é¢ã€‚è¯„ä»·æ¡†æ¶æ›´å¥½åœ°ç¬¦åˆäººç±»åå¥½ï¼Œå¹¶åœ¨éŸ³é¢‘ä»»åŠ¡ä¹‹é—´å…·æœ‰å¯è½¬ç§»æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éŸ³é¢‘ä¹¦ç”Ÿæˆé¢ä¸´ä¼ è¾¾å¤æ‚æƒ…ç»ªã€å®ç°äººç±»ç‰¹è´¨å’Œå¯¹é½äººç±»åå¥½è¯„ä¼°çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºDopamine Audiobookç³»ç»Ÿï¼Œåˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œæƒ…æ„ŸåŒ–å’Œäººæ€§åŒ–çš„éŸ³é¢‘ä¹¦ç”Ÿæˆã€‚</li>
<li>è®¾è®¡åŸºäºæµçš„æƒ…æ„Ÿå¢å¼ºæ¡†æ¶ï¼Œå°†å¤æ‚çš„æƒ…æ„Ÿè¯­éŸ³åˆæˆè¿‡ç¨‹åˆ†è§£ä¸ºå¯æ§çš„å­ä»»åŠ¡ã€‚</li>
<li>é‡‡ç”¨è‡ªé€‚åº”æ¨¡å‹é€‰æ‹©æ¨¡å—ï¼Œæ ¹æ®åœºæ™¯é€‰æ‹©æœ€åˆé€‚çš„TTSæ–¹æ³•ã€‚</li>
<li>é€šè¿‡è¯­è¨€å¤–çš„å¢å£°å’Œè¯­è°ƒæ£€ç´¢å¢å¼ºæƒ…æ„Ÿè¡¨è¾¾ã€‚</li>
<li>æå‡ºçš„GPT-basedè¯„ä»·æ¡†æ¶èƒ½ç¡®ä¿ä¸äººç±»åå¥½å’Œè‡ªæˆ‘å¯¹é½çš„è¯„ä¼°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11002">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9a8dc872d5a2ed4e7ff9a0695fc789bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6d8d4a4d66f2f20d86f6c323018040b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-517a5c93b7cfc59d7012fbdf4d176b41.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6b4a2421f8b89c090d153df1751eae1e.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="ARise-Towards-Knowledge-Augmented-Reasoning-via-Risk-Adaptive-Search"><a href="#ARise-Towards-Knowledge-Augmented-Reasoning-via-Risk-Adaptive-Search" class="headerlink" title="ARise: Towards Knowledge-Augmented Reasoning via Risk-Adaptive Search"></a>ARise: Towards Knowledge-Augmented Reasoning via Risk-Adaptive Search</h2><p><strong>Authors:Yize Zhang, Tianshu Wang, Sirui Chen, Kun Wang, Xingyu Zeng, Hongyu Lin, Xianpei Han, Le Sun, Chaochao Lu</strong></p>
<p>Large language models (LLMs) have demonstrated impressive capabilities and are receiving increasing attention to enhance their reasoning through scaling testâ€“time compute. However, their application in openâ€“ended, knowledgeâ€“intensive, complex reasoning scenarios is still limited. Reasoningâ€“oriented methods struggle to generalize to openâ€“ended scenarios due to implicit assumptions of complete world knowledge. Meanwhile, knowledgeâ€“augmented reasoning (KAR) methods fail to address two core challenges: 1) error propagation, where errors in early steps cascade through the chain, and 2) verification bottleneck, where the exploreâ€“exploit tradeoff arises in multiâ€“branch decision processes. To overcome these limitations, we introduce ARise, a novel framework that integrates risk assessment of intermediate reasoning states with dynamic retrievalâ€“augmented generation (RAG) within a Monte Carlo tree search paradigm. This approach enables effective construction and optimization of reasoning plans across multiple maintained hypothesis branches. Experimental results show that ARise significantly outperforms the stateâ€“ofâ€“theâ€“art KAR methods by up to 23.10%, and the latest RAG-equipped large reasoning models by up to 25.37%. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»å±•ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ï¼Œå¹¶è¶Šæ¥è¶Šå—åˆ°å…³æ³¨ï¼Œå¯ä»¥é€šè¿‡æ‰©å¤§æµ‹è¯•æ—¶çš„è®¡ç®—æ¥å¢å¼ºå®ƒä»¬çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨å¼€æ”¾ã€çŸ¥è¯†å¯†é›†ã€å¤æ‚æ¨ç†åœºæ™¯ä¸­çš„åº”ç”¨ä»ç„¶æœ‰é™ã€‚é¢å‘æ¨ç†çš„æ–¹æ³•å¾ˆéš¾æ¨å¹¿åˆ°å¼€æ”¾åœºæ™¯ï¼Œå› ä¸ºå®ƒä»¬éšå«åœ°å‡è®¾äº†å®Œæ•´çš„ä¸–ç•ŒçŸ¥è¯†ã€‚åŒæ—¶ï¼ŒçŸ¥è¯†å¢å¼ºæ¨ç†ï¼ˆKARï¼‰æ–¹æ³•é¢ä¸´ä¸¤ä¸ªæ ¸å¿ƒæŒ‘æˆ˜ï¼š1ï¼‰è¯¯å·®ä¼ æ’­ï¼Œå³æ—©æœŸæ­¥éª¤ä¸­çš„é”™è¯¯åœ¨é“¾ä¸­é€çº§ä¼ æ’­ï¼›2ï¼‰éªŒè¯ç“¶é¢ˆï¼Œå³åœ¨å¤šåˆ†æ”¯å†³ç­–è¿‡ç¨‹ä¸­å‡ºç°äº†æ¢ç´¢-åˆ©ç”¨æƒè¡¡ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ARiseï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œå®ƒå°†ä¸­é—´æ¨ç†çŠ¶æ€çš„é£é™©è¯„ä¼°ä¸è’™ç‰¹å¡ç½—æ ‘æœç´¢èŒƒå¼å†…çš„åŠ¨æ€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç›¸ç»“åˆã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°æ„å»ºå’Œä¼˜åŒ–è·¨å¤šä¸ªç»´æŒå‡è®¾åˆ†æ”¯çš„æ¨ç†è®¡åˆ’ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒARiseæ˜¾è‘—ä¼˜äºæœ€æ–°çš„KARæ–¹æ³•ï¼Œæ€§èƒ½æå‡é«˜è¾¾23.10%ï¼Œå¹¶ä¸”æ¯”æœ€æ–°çš„é…å¤‡RAGçš„å¤§å‹æ¨ç†æ¨¡å‹é«˜å‡º25.37%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10893v1">PDF</a> Project homepage: <a target="_blank" rel="noopener" href="https://opencausalab.github.io/ARise">https://opencausalab.github.io/ARise</a></p>
<p><strong>Summary</strong><br>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†æ–¹é¢è¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ï¼Œå¹¶æ­£åœ¨é€šè¿‡æ‰©å±•æµ‹è¯•æ—¶é—´è®¡ç®—æ¥å¢å¼ºè¿™äº›èƒ½åŠ›ã€‚ç„¶è€Œï¼Œåœ¨å¼€æ”¾å¼ã€çŸ¥è¯†å¯†é›†å‹ã€å¤æ‚æ¨ç†åœºæ™¯ä¸­ï¼Œå®ƒä»¬çš„å®é™…åº”ç”¨ä»å­˜åœ¨å±€é™æ€§ã€‚å½“å‰çš„æ–¹æ³•é¢ä¸´éš¾ä»¥æ¨å¹¿åˆ°å¼€æ”¾å¼åœºæ™¯å’Œè¯¯å·®ä¼ æ’­ä¸éªŒè¯ç“¶é¢ˆç­‰æ ¸å¿ƒæŒ‘æˆ˜ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ARiseè¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œå®ƒé€šè¿‡é£é™©è¯„ä¼°å’ŒåŠ¨æ€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰åœ¨è’™ç‰¹å¡æ´›æ ‘æœç´¢èŒƒå¼å†…çš„æ•´åˆï¼Œæœ‰æ•ˆæ„å»ºå’Œä¼˜åŒ–å¤šä¸ªå‡è®¾åˆ†æ”¯çš„æ¨ç†è®¡åˆ’ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒARiseæ˜¾è‘—ä¼˜äºæœ€æ–°çš„KARæ–¹æ³•å’Œé…å¤‡æœ€æ–°RAGçš„å¤§å‹æ¨ç†æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¢å¼ºæ¨ç†èƒ½åŠ›æ–¹é¢æ­£åœ¨å—åˆ°è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ï¼Œä½†ä»é¢ä¸´åœ¨å¼€æ”¾å¼ã€çŸ¥è¯†å¯†é›†å‹ã€å¤æ‚æ¨ç†åœºæ™¯ä¸­çš„åº”ç”¨å±€é™æ€§ã€‚</li>
<li>å½“å‰çš„æ–¹æ³•éš¾ä»¥å°†æ¨ç†æ¨å¹¿åˆ°å¼€æ”¾å¼åœºæ™¯ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºå®ƒä»¬åŸºäºå®Œæ•´ä¸–ç•ŒçŸ¥è¯†çš„éšå«å‡è®¾ã€‚</li>
<li>çŸ¥è¯†å¢å¼ºæ¨ç†ï¼ˆKARï¼‰æ–¹æ³•é¢ä¸´ä¸¤ä¸ªæ ¸å¿ƒæŒ‘æˆ˜ï¼šè¯¯å·®ä¼ æ’­å’ŒéªŒè¯ç“¶é¢ˆã€‚</li>
<li>ARiseæ¡†æ¶é€šè¿‡æ•´åˆä¸­é—´æ¨ç†çŠ¶æ€çš„é£é™©è¯„ä¼°ä¸åŠ¨æ€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ï¼Œåœ¨è’™ç‰¹å¡æ´›æ ‘æœç´¢èŒƒå¼å†…å…‹æœè¿™äº›æŒ‘æˆ˜ã€‚</li>
<li>ARiseæ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆæ„å»ºå’Œä¼˜åŒ–å¤šä¸ªå‡è®¾åˆ†æ”¯çš„æ¨ç†è®¡åˆ’ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒARiseæ˜¾è‘—ä¼˜äºç°æœ‰çš„KARæ–¹æ³•å’Œæœ€æ–°çš„å¤§å‹æ¨ç†æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10893">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c50ae4d942e2585964499ea4f84904ac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-51a5ecd30097ff5155c8fabc668d001e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b6bd7e9b602455aa0246cc6f20b27a66.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-68a83c71fd7b3c98241ad990201a9a50.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="LayoutCoT-Unleashing-the-Deep-Reasoning-Potential-of-Large-Language-Models-for-Layout-Generation"><a href="#LayoutCoT-Unleashing-the-Deep-Reasoning-Potential-of-Large-Language-Models-for-Layout-Generation" class="headerlink" title="LayoutCoT: Unleashing the Deep Reasoning Potential of Large Language   Models for Layout Generation"></a>LayoutCoT: Unleashing the Deep Reasoning Potential of Large Language   Models for Layout Generation</h2><p><strong>Authors:Hengyu Shi, Junhao Su, Huansheng Ning, Xiaoming Wei, Jialin Gao</strong></p>
<p>Conditional layout generation aims to automatically generate visually appealing and semantically coherent layouts from user-defined constraints. While recent methods based on generative models have shown promising results, they typically require substantial amounts of training data or extensive fine-tuning, limiting their versatility and practical applicability. Alternatively, some training-free approaches leveraging in-context learning with Large Language Models (LLMs) have emerged, but they often suffer from limited reasoning capabilities and overly simplistic ranking mechanisms, which restrict their ability to generate consistently high-quality layouts. To this end, we propose LayoutCoT, a novel approach that leverages the reasoning capabilities of LLMs through a combination of Retrieval-Augmented Generation (RAG) and Chain-of-Thought (CoT) techniques. Specifically, LayoutCoT transforms layout representations into a standardized serialized format suitable for processing by LLMs. A Layout-aware RAG is used to facilitate effective retrieval and generate a coarse layout by LLMs. This preliminary layout, together with the selected exemplars, is then fed into a specially designed CoT reasoning module for iterative refinement, significantly enhancing both semantic coherence and visual quality. We conduct extensive experiments on five public datasets spanning three conditional layout generation tasks. Experimental results demonstrate that LayoutCoT achieves state-of-the-art performance without requiring training or fine-tuning. Notably, our CoT reasoning module enables standard LLMs, even those without explicit deep reasoning abilities, to outperform specialized deep-reasoning models such as deepseek-R1, highlighting the potential of our approach in unleashing the deep reasoning capabilities of LLMs for layout generation tasks. </p>
<blockquote>
<p>æ¡ä»¶å¸ƒå±€ç”Ÿæˆæ—¨åœ¨æ ¹æ®ç”¨æˆ·å®šä¹‰çš„çº¦æŸè‡ªåŠ¨ç”Ÿæˆè§†è§‰å¸å¼•äººä¸”è¯­ä¹‰è¿è´¯çš„å¸ƒå±€ã€‚è™½ç„¶æœ€è¿‘åŸºäºç”Ÿæˆæ¨¡å‹çš„æ–¹æ³•æ˜¾ç¤ºå‡ºæœ‰å‰æ™¯çš„ç»“æœï¼Œä½†å®ƒä»¬é€šå¸¸éœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®æˆ–å¹¿æ³›çš„å¾®è°ƒï¼Œè¿™é™åˆ¶äº†å®ƒä»¬çš„é€šç”¨æ€§å’Œå®é™…é€‚ç”¨æ€§ã€‚å¦ä¸€æ–¹é¢ï¼Œä¸€äº›åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¸Šä¸‹æ–‡å­¦ä¹ çš„æ— è®­ç»ƒæ–¹æ³•å·²ç»å‡ºç°ï¼Œä½†å®ƒä»¬é€šå¸¸å­˜åœ¨æ¨ç†èƒ½åŠ›æœ‰é™å’Œæ’åæœºåˆ¶è¿‡äºç®€å•çš„é—®é¢˜ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬ç”Ÿæˆé«˜è´¨é‡å¸ƒå±€çš„èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†LayoutCoTï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡ç»“åˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œæ€ç»´é“¾ï¼ˆCoTï¼‰æŠ€æœ¯åˆ©ç”¨LLMæ¨ç†èƒ½åŠ›çš„æ–°æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼ŒLayoutCoTå°†å¸ƒå±€è¡¨ç¤ºè½¬æ¢ä¸ºé€‚åˆLLMå¤„ç†çš„æ ‡å‡†åŒ–åºåˆ—åŒ–æ ¼å¼ã€‚ä½¿ç”¨å¸ƒå±€æ„ŸçŸ¥RAGæ¥ä¿ƒè¿›æœ‰æ•ˆæ£€ç´¢ï¼Œå¹¶é€šè¿‡LLMç”Ÿæˆç²—ç•¥å¸ƒå±€ã€‚è¿™ä¸ªåˆæ­¥å¸ƒå±€ä¸é€‰å®šçš„èŒƒä¾‹ä¸€èµ·è¢«è¾“å…¥åˆ°ä¸“é—¨è®¾è®¡çš„CoTæ¨ç†æ¨¡å—ä¸­è¿›è¡Œè¿­ä»£ä¼˜åŒ–ï¼Œæ˜¾è‘—æé«˜äº†è¯­ä¹‰è¿è´¯æ€§å’Œè§†è§‰è´¨é‡ã€‚æˆ‘ä»¬åœ¨äº”ä¸ªå…¬å…±æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œæ¶µç›–ä¸‰ä¸ªæ¡ä»¶å¸ƒå±€ç”Ÿæˆä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLayoutCoTåœ¨ä¸è¿›è¡Œè®­ç»ƒæˆ–å¾®è°ƒçš„æƒ…å†µä¸‹å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„CoTæ¨ç†æ¨¡å—ä½¿æ ‡å‡†LLMï¼ˆå³ä½¿é‚£äº›æ²¡æœ‰æ˜ç¡®çš„æ·±åº¦æ¨ç†èƒ½åŠ›çš„LLMï¼‰èƒ½å¤Ÿè¶…è¶Šæ·±åº¦æ¨ç†æ¨¡å‹ï¼ˆå¦‚deepseek-R1ï¼‰ï¼Œè¿™çªæ˜¾äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨é‡Šæ”¾LLMåœ¨å¸ƒå±€ç”Ÿæˆä»»åŠ¡ä¸­çš„æ·±åº¦æ¨ç†æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10829v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–°æ–¹æ³•LayoutCoTï¼Œç”¨äºç”Ÿæˆæ¡ä»¶å¸ƒå±€ã€‚å®ƒé€šè¿‡ç»“åˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œæ€ç»´é“¾ï¼ˆCoTï¼‰æŠ€æœ¯ï¼Œå°†å¸ƒå±€è¡¨ç¤ºè½¬åŒ–ä¸ºæ ‡å‡†åŒ–åºåˆ—åŒ–æ ¼å¼ï¼Œåˆ©ç”¨LLMçš„æ¨ç†èƒ½åŠ›ç”Ÿæˆé«˜è´¨é‡å¸ƒå±€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLayoutCoTåœ¨æ— éœ€è®­ç»ƒå’Œç²¾ç»†è°ƒæ•´çš„æƒ…å†µä¸‹å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æ¡ä»¶å¸ƒå±€ç”Ÿæˆæ—¨åœ¨ä»ç”¨æˆ·å®šä¹‰çš„çº¦æŸä¸­è‡ªåŠ¨ç”Ÿæˆè§†è§‰å¸å¼•å’Œè¯­ä¹‰è¿è´¯çš„å¸ƒå±€ã€‚</li>
<li>ç°æœ‰æ–¹æ³•éœ€è¦å¤§é‡è®­ç»ƒæ•°æ®æˆ–ç²¾ç»†è°ƒæ•´ï¼Œé™åˆ¶äº†å…¶é€šç”¨æ€§å’Œå®ç”¨æ€§ã€‚</li>
<li>LayoutCoTæ˜¯ä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–°æ–¹æ³•ï¼Œç»“åˆäº†RAGå’ŒCoTæŠ€æœ¯æ¥ç”Ÿæˆå¸ƒå±€ã€‚</li>
<li>LayoutCoTé€šè¿‡å°†å¸ƒå±€è¡¨ç¤ºè½¬åŒ–ä¸ºæ ‡å‡†åŒ–åºåˆ—åŒ–æ ¼å¼ï¼Œåˆ©ç”¨LLMçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>LayoutCoTä½¿ç”¨å¸ƒå±€æ„ŸçŸ¥RAGè¿›è¡Œæœ‰æ•ˆæ£€ç´¢ï¼Œå¹¶é€šè¿‡æ€ç»´é“¾æ¨ç†æ¨¡å—è¿›è¡Œè¿­ä»£ä¼˜åŒ–ï¼Œæé«˜è¯­ä¹‰è¿è´¯æ€§å’Œè§†è§‰è´¨é‡ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒLayoutCoTåœ¨äº”ä¸ªå…¬å…±æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ— éœ€è®­ç»ƒå’Œç²¾ç»†è°ƒæ•´ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10829">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1043f8da91492e98ae567d2e15290f58.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5c5ff8589fa11c815988563a6fd53815.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3e1a1f1680d80ebbb50bad82d282ba1e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4cbb1501f2bde1fef7f486a10fd08d72.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d6abc270cb63421a16a8b148ec01520.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="ReasonDrive-Efficient-Visual-Question-Answering-for-Autonomous-Vehicles-with-Reasoning-Enhanced-Small-Vision-Language-Models"><a href="#ReasonDrive-Efficient-Visual-Question-Answering-for-Autonomous-Vehicles-with-Reasoning-Enhanced-Small-Vision-Language-Models" class="headerlink" title="ReasonDrive: Efficient Visual Question Answering for Autonomous Vehicles   with Reasoning-Enhanced Small Vision-Language Models"></a>ReasonDrive: Efficient Visual Question Answering for Autonomous Vehicles   with Reasoning-Enhanced Small Vision-Language Models</h2><p><strong>Authors:Amirhosein Chahe, Lifeng Zhou</strong></p>
<p>Vision-language models (VLMs) show promise for autonomous driving but often lack transparent reasoning capabilities that are critical for safety. We investigate whether explicitly modeling reasoning during fine-tuning enhances VLM performance on driving decision tasks. Using GPT-4o, we generate structured reasoning chains for driving scenarios from the DriveLM benchmark with category-specific prompting strategies. We compare reasoning-based fine-tuning, answer-only fine-tuning, and baseline instruction-tuned models across multiple small VLM families (Llama 3.2, Llava 1.5, and Qwen 2.5VL). Our results demonstrate that reasoning-based fine-tuning consistently outperforms alternatives, with Llama3.2-11B-reason achieving the highest performance. Models fine-tuned with reasoning show substantial improvements in accuracy and text generation quality, suggesting explicit reasoning enhances internal representations for driving decisions. These findings highlight the importance of transparent decision processes in safety-critical domains and offer a promising direction for developing more interpretable autonomous driving systems. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸæ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œä½†å®ƒä»¬å¾€å¾€ç¼ºä¹å…³é”®çš„é€æ˜æ¨ç†èƒ½åŠ›ä»¥ç¡®ä¿å®‰å…¨ã€‚æˆ‘ä»¬ç ”ç©¶äº†åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­æ˜¾å¼å»ºæ¨¡æ¨ç†æ˜¯å¦æœ‰åŠ©äºæé«˜é©¾é©¶å†³ç­–ä»»åŠ¡çš„VLMæ€§èƒ½ã€‚æˆ‘ä»¬ä½¿ç”¨GPT-4oä¸ºDriveLMåŸºå‡†æµ‹è¯•ä¸­çš„é©¾é©¶åœºæ™¯ç”Ÿæˆç»“æ„åŒ–æ¨ç†é“¾ï¼Œé‡‡ç”¨ç‰¹å®šç±»åˆ«çš„æç¤ºç­–ç•¥ã€‚æˆ‘ä»¬æ¯”è¾ƒäº†åŸºäºæ¨ç†çš„å¾®è°ƒã€ä»…ç­”æ¡ˆçš„å¾®è°ƒä»¥åŠåŸºçº¿æŒ‡ä»¤è°ƒæ•´æ¨¡å‹åœ¨å¤šä¸ªå°å‹VLMå®¶æ—ï¼ˆLlama 3.2ã€Llava 1.5å’ŒQwen 2.5VLï¼‰ä¸­çš„è¡¨ç°ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒåŸºäºæ¨ç†çš„å¾®è°ƒå§‹ç»ˆä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œå…¶ä¸­Llama3.2-11B-reasonè¡¨ç°æœ€ä½³ã€‚é€šè¿‡æ¨ç†è¿›è¡Œå¾®è°ƒçš„æ¨¡å‹åœ¨å‡†ç¡®æ€§å’Œæ–‡æœ¬ç”Ÿæˆè´¨é‡æ–¹é¢å®ç°äº†æ˜¾è‘—çš„æå‡ï¼Œè¿™è¡¨æ˜æ˜ç¡®çš„æ¨ç†å¢å¼ºäº†é©¾é©¶å†³ç­–çš„å†…éƒ¨è¡¨ç¤ºã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†å®‰å…¨å…³é”®é¢†åŸŸä¸­é€æ˜å†³ç­–è¿‡ç¨‹çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºå¼€å‘æ›´å…·è§£é‡Šæ€§çš„è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿæä¾›äº†æœ‰å‰æ™¯çš„æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10757v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†ä¸ºè‡ªåŠ¨é©¾é©¶çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰æ·»åŠ æ˜ç¡®æ¨ç†èƒ½åŠ›çš„å¯è¡Œæ€§ã€‚é€šè¿‡å¯¹GPT-4oçš„ä½¿ç”¨å’Œå¯¹é©¾é©¶åœºæ™¯çš„æ¨ç†é“¾æ„å»ºï¼Œä»¥åŠç»“åˆDriveLMåŸºå‡†æµ‹è¯•ä¸ç‰¹å®šç±»åˆ«çš„æç¤ºç­–ç•¥è¿›è¡Œå¾®è°ƒï¼Œç ”ç©¶å‘ç°åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­åŠ å…¥æ¨ç†èƒ½åŠ›å¯ä»¥æ˜¾è‘—æé«˜VLMåœ¨é©¾é©¶å†³ç­–ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚ä¸åªå›ç­”é—®é¢˜å’ŒåŸºçº¿æŒ‡ä»¤è°ƒèŠ‚æ¨¡å‹ç›¸æ¯”ï¼ŒLlama 3.2è¡¨ç°æœ€å¥½ã€‚è¿™äº›æ¨¡å‹åœ¨ç²¾åº¦å’Œæ–‡æœ¬ç”Ÿæˆè´¨é‡æ–¹é¢éƒ½æœ‰æ˜¾è‘—æé«˜ï¼Œè¡¨æ˜æ˜ç¡®çš„æ¨ç†èƒ½åŠ›å¯ä»¥å¢å¼ºé©¾é©¶å†³ç­–çš„å†…éƒ¨è¡¨ç¤ºã€‚è¿™äº›å‘ç°å¯¹äºå¼€å‘å¯è§£é‡Šæ€§æ›´å¼ºçš„è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿå…·æœ‰é‡è¦æ„ä¹‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>VLMåœ¨è‡ªåŠ¨é©¾é©¶ä¸­æœ‰æ½œåŠ›ï¼Œä½†ç¼ºä¹é€æ˜æ¨ç†èƒ½åŠ›ï¼Œè¿™å¯¹å®‰å…¨æ€§è‡³å…³é‡è¦ã€‚</li>
<li>é€šè¿‡GPT-4oè¿›è¡Œç»“æ„åŒ–æ¨ç†é“¾ç”Ÿæˆï¼Œé’ˆå¯¹é©¾é©¶åœºæ™¯è¿›è¡Œæµ‹è¯•ã€‚</li>
<li>ç›¸è¾ƒäºä»…å›ç­”é—®é¢˜çš„å¾®è°ƒæ–¹å¼åŠåŸºçº¿æŒ‡ä»¤è°ƒèŠ‚æ¨¡å‹ï¼ŒåŸºäºæ¨ç†çš„å¾®è°ƒæ–¹æ³•è¡¨ç°æ›´ä¼˜å¼‚ã€‚</li>
<li>Llama 3.2åœ¨æ¨ç†åŸºç¡€ä¸Šå¾®è°ƒåæ€§èƒ½æœ€ä½³ã€‚</li>
<li>åŠ å…¥æ¨ç†èƒ½åŠ›çš„æ¨¡å‹åœ¨ç²¾åº¦å’Œæ–‡æœ¬ç”Ÿæˆè´¨é‡ä¸Šæœ‰æ˜¾è‘—æé«˜ã€‚</li>
<li>æ˜ç¡®æ¨ç†èƒ½åŠ›èƒ½å¢å¼ºé©¾é©¶å†³ç­–çš„å†…éƒ¨è¡¨ç¤ºã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10757">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-680ebfa4b3798c05ad18c99ec18655ea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-771f7cc0da05f138ca249c067e85e18a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6df2340cb97c37b38a15f19075e6e8bb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-42543d31f9f13eed5c771f66a7161d60.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-76f282bf50ac098de27cc1d6bf1983ec.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Improving-In-Context-Learning-with-Reasoning-Distillation"><a href="#Improving-In-Context-Learning-with-Reasoning-Distillation" class="headerlink" title="Improving In-Context Learning with Reasoning Distillation"></a>Improving In-Context Learning with Reasoning Distillation</h2><p><strong>Authors:Nafis Sadeq, Xin Xu, Zhouhang Xie, Julian McAuley, Byungkyu Kang, Prarit Lamba, Xiang Gao</strong></p>
<p>Language models rely on semantic priors to perform in-context learning, which leads to poor performance on tasks involving inductive reasoning. Instruction-tuning methods based on imitation learning can superficially enhance the in-context learning performance of language models, but they often fail to improve the modelâ€™s understanding of the underlying rules that connect inputs and outputs in few-shot demonstrations. We propose ReDis, a reasoning distillation technique designed to improve the inductive reasoning capabilities of language models. Through a careful combination of data augmentation, filtering, supervised fine-tuning, and alignment, ReDis achieves significant performance improvements across a diverse range of tasks, including 1D-ARC, List Function, ACRE, and MiniSCAN. Experiments on three language model backbones show that ReDis outperforms equivalent few-shot prompting baselines across all tasks and even surpasses the teacher model, GPT-4o, in some cases. ReDis, based on the LLaMA-3 backbone, achieves relative improvements of 23.2%, 2.8%, and 66.6% over GPT-4o on 1D-ARC, ACRE, and MiniSCAN, respectively, within a similar hypothesis search space. The code, dataset, and model checkpoints will be made available at <a target="_blank" rel="noopener" href="https://github.com/NafisSadeq/reasoning-distillation.git">https://github.com/NafisSadeq/reasoning-distillation.git</a>. </p>
<blockquote>
<p>è¯­è¨€æ¨¡å‹ä¾èµ–äºè¯­ä¹‰å…ˆéªŒçŸ¥è¯†æ¥è¿›è¡Œä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œè¿™å¯¼è‡´å®ƒä»¬åœ¨æ¶‰åŠå½’çº³æ¨ç†çš„ä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ã€‚åŸºäºæ¨¡ä»¿å­¦ä¹ çš„æŒ‡ä»¤è°ƒæ•´æ–¹æ³•è¡¨é¢ä¸Šå¯ä»¥å¢å¼ºè¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ï¼Œä½†å®ƒä»¬å¾€å¾€æ— æ³•æé«˜æ¨¡å‹å¯¹å°‘é‡æ¼”ç¤ºä¸­è¾“å…¥å’Œè¾“å‡ºä¹‹é—´åŸºæœ¬è§„åˆ™çš„ç†è§£ã€‚æˆ‘ä»¬æå‡ºäº†ReDisï¼Œä¸€ç§æ—¨åœ¨æé«˜è¯­è¨€æ¨¡å‹å½’çº³æ¨ç†èƒ½åŠ›çš„æ¨ç†è’¸é¦æŠ€æœ¯ã€‚é€šè¿‡æ•°æ®å¢å¼ºã€è¿‡æ»¤ã€ç›‘ç£å¾®è°ƒå’Œå¯¹é½çš„ç²¾å¿ƒç»“åˆï¼ŒReDisåœ¨å¤šç§ä»»åŠ¡ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ï¼ŒåŒ…æ‹¬1D-ARCã€List Functionã€ACREå’ŒMiniSCANã€‚åœ¨ä¸‰å¥—è¯­è¨€æ¨¡å‹ä¸»å¹²ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒReDisåœ¨æ‰€æœ‰ä»»åŠ¡ä¸Šçš„è¡¨ç°éƒ½è¶…è¿‡äº†ç­‰æ•ˆçš„å°‘é‡æç¤ºåŸºçº¿ï¼Œç”šè‡³åœ¨æŸäº›æƒ…å†µä¸‹è¶…è¿‡äº†æ•™å¸ˆæ¨¡å‹GPT-4oã€‚åŸºäºLLaMA-3ä¸»å¹²çš„ReDisåœ¨ç±»ä¼¼å‡è®¾æœç´¢ç©ºé—´å†…ï¼Œåœ¨1D-ARCã€ACREå’ŒMiniSCANä»»åŠ¡ä¸Šç›¸å¯¹äºGPT-4oåˆ†åˆ«å®ç°äº†23.2%ã€2.8%å’Œ66.6%çš„ç›¸å¯¹æ”¹è¿›ã€‚ä»£ç ã€æ•°æ®é›†å’Œæ¨¡å‹æ£€æŸ¥ç‚¹å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/NafisSadeq/reasoning-distillation.git%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/NafisSadeq/reasoning-distillation.gitä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10647v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>è¯­è¨€æ¨¡å‹ä¾èµ–äºè¯­ä¹‰å…ˆéªŒæ¥è¿›è¡Œä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œè¿™åœ¨æ¶‰åŠå½’çº³æ¨ç†çš„ä»»åŠ¡ä¸Šè¡¨ç°è¾ƒå·®ã€‚è™½ç„¶åŸºäºæ¨¡ä»¿å­¦ä¹ çš„æŒ‡ä»¤è°ƒæ•´æ–¹æ³•èƒ½å¤Ÿè¡¨é¢ä¸Šæé«˜è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡å­¦ä¹ æ€§èƒ½ï¼Œä½†å®ƒä»¬å¾€å¾€æ— æ³•æé«˜æ¨¡å‹å¯¹å°‘æ•°æ¼”ç¤ºä¸­è¾“å…¥å’Œè¾“å‡ºä¹‹é—´åŸºç¡€è§„åˆ™çš„ç†è§£ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ReDisæ¨ç†è’¸é¦æŠ€æœ¯ï¼Œæ—¨åœ¨æé«˜è¯­è¨€æ¨¡å‹çš„å½’çº³æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡æ•°æ®å¢å¼ºã€è¿‡æ»¤ã€ç›‘ç£å¾®è°ƒå’Œå¯¹é½çš„ç²¾å¿ƒè®¾è®¡ï¼ŒReDisåœ¨å¤šç§ä»»åŠ¡ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ï¼ŒåŒ…æ‹¬1D-ARCã€List Functionã€ACREå’ŒMiniSCANã€‚åœ¨ç±»ä¼¼å‡è®¾æœç´¢ç©ºé—´å†…ï¼ŒåŸºäºLLaMA-3èƒŒä¹¦çš„ReDisåœ¨GPT-4oè€å¸ˆæ¨¡å‹ä¸Šå®ç°äº†ç›¸å¯¹æ”¹è¿›ï¼Œç›¸å¯¹æ”¹è¿›ç‡åˆ†åˆ«ä¸º1D-ARCçš„23.2%ã€ACREçš„2.8%å’ŒMiniSCANçš„66.6%ã€‚ç›¸å…³ä»£ç ã€æ•°æ®é›†å’Œæ¨¡å‹æ£€æŸ¥ç‚¹å°†å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/NafisSadeq/reasoning-distillation.git%E3%80%82">https://github.com/NafisSadeq/reasoning-distillation.gitã€‚</a></p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è¯­è¨€æ¨¡å‹ä¾èµ–è¯­ä¹‰å…ˆéªŒè¿›è¡Œä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œåœ¨å½’çº³æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°æ¬ ä½³ã€‚</li>
<li>æŒ‡ä»¤è°ƒæ•´æ–¹æ³•è™½èƒ½æé«˜ä¸Šä¸‹æ–‡å­¦ä¹ æ€§èƒ½ï¼Œä½†éš¾ä»¥æé«˜æ¨¡å‹å¯¹ä»»åŠ¡åŸºç¡€è§„åˆ™çš„ç†è§£ã€‚</li>
<li>æå‡ºçš„ReDisæ¨ç†è’¸é¦æŠ€æœ¯ç»“åˆæ•°æ®å¢å¼ºã€è¿‡æ»¤ã€ç›‘ç£å¾®è°ƒå’Œå¯¹é½ï¼Œæ—¨åœ¨æé«˜è¯­è¨€æ¨¡å‹çš„å½’çº³æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ReDisåœ¨å¤šç§ä»»åŠ¡ä¸Šå®ç°æ˜¾è‘—æ€§èƒ½æ”¹è¿›ï¼ŒåŒ…æ‹¬1D-ARCã€List Functionã€ACREå’ŒMiniSCANã€‚</li>
<li>åŸºäºLLaMA-3èƒŒä¹¦çš„ReDisåœ¨æŸäº›æƒ…å†µä¸‹è¶…è¶Šäº†GPT-4oè€å¸ˆæ¨¡å‹ï¼Œç›¸å¯¹æ”¹è¿›ç‡æ˜¾è‘—ã€‚</li>
<li>ReDisçš„å®ç°ä»£ç ã€æ•°æ®é›†å’Œæ¨¡å‹æ£€æŸ¥ç‚¹å°†å…¬å¼€ï¼Œæ–¹ä¾¿åç»­ç ”ç©¶ä½¿ç”¨ã€‚</li>
<li>ReDisçš„ç ”ç©¶å¯¹äºæå‡è¯­è¨€æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œç‰¹åˆ«æ˜¯å½’çº³æ¨ç†èƒ½åŠ›ï¼Œå…·æœ‰ç§¯ææ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10647">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f90cda74598c8660c157a905f75b4360.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e9db694b375b0f1be6b93129b108f775.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dc90b54ce525ca22384b9366a9451c56.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f78e453174dceba41599f0b4ce3bd937.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Beyond-Chains-of-Thought-Benchmarking-Latent-Space-Reasoning-Abilities-in-Large-Language-Models"><a href="#Beyond-Chains-of-Thought-Benchmarking-Latent-Space-Reasoning-Abilities-in-Large-Language-Models" class="headerlink" title="Beyond Chains of Thought: Benchmarking Latent-Space Reasoning Abilities   in Large Language Models"></a>Beyond Chains of Thought: Benchmarking Latent-Space Reasoning Abilities   in Large Language Models</h2><p><strong>Authors:Thilo Hagendorff, Sarah Fabi</strong></p>
<p>Large language models (LLMs) can perform reasoning computations both internally within their latent space and externally by generating explicit token sequences like chains of thought. Significant progress in enhancing reasoning abilities has been made by scaling test-time compute. However, understanding and quantifying model-internal reasoning abilities - the inferential â€œleapsâ€ models make between individual token predictions - remains crucial. This study introduces a benchmark (n &#x3D; 4,000 items) designed to quantify model-internal reasoning in different domains. We achieve this by having LLMs indicate the correct solution to reasoning problems not through descriptive text, but by selecting a specific language of their initial response token that is different from English, the benchmark language. This not only requires models to reason beyond their context window, but also to overrise their default tendency to respond in the same language as the prompt, thereby posing an additional cognitive strain. We evaluate a set of 18 LLMs, showing significant performance variations, with GPT-4.5 achieving the highest accuracy (74.7%), outperforming models like Grok-2 (67.2%), and Llama 3.1 405B (65.6%). Control experiments and difficulty scaling analyses suggest that while LLMs engage in internal reasoning, we cannot rule out heuristic exploitations under certain conditions, marking an area for future investigation. Our experiments demonstrate that LLMs can â€œthinkâ€ via latent-space computations, revealing model-internal inference strategies that need further understanding, especially regarding safety-related concerns such as covert planning, goal-seeking, or deception emerging without explicit token traces. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯ä»¥åœ¨å…¶æ½œåœ¨ç©ºé—´å†…éƒ¨å’Œå¤–éƒ¨è¿›è¡Œæ¨ç†è®¡ç®—ï¼Œé€šè¿‡ç”Ÿæˆæ˜ç¡®çš„ä»¤ç‰Œåºåˆ—ï¼ˆå¦‚æ€ç»´é“¾ï¼‰æ¥å±•ç°ã€‚é€šè¿‡æ‰©å¤§æµ‹è¯•æ—¶çš„è®¡ç®—è§„æ¨¡ï¼Œå·²ç»åœ¨æé«˜æ¨ç†èƒ½åŠ›æ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ã€‚ç„¶è€Œï¼Œç†è§£å’Œé‡åŒ–æ¨¡å‹å†…éƒ¨çš„æ¨ç†èƒ½åŠ›â€”â€”æ¨¡å‹åœ¨ä¸ªåˆ«ä»¤ç‰Œé¢„æµ‹ä¹‹é—´åšå‡ºçš„æ¨æ–­â€œè·³è·ƒâ€â€”â€”ä»ç„¶è‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ä¸ªåŒ…å«4000ä¸ªé¡¹ç›®çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨é‡åŒ–ä¸åŒé¢†åŸŸçš„æ¨¡å‹å†…éƒ¨æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬å®ç°è¿™ä¸€ç‚¹çš„æ–¹å¼æ˜¯ï¼Œè®©å¤§å‹è¯­è¨€æ¨¡å‹ä¸ç”¨æè¿°æ€§æ–‡æœ¬ï¼Œè€Œæ˜¯é€šè¿‡é€‰æ‹©å…¶åˆå§‹å“åº”ä»¤ç‰Œä¸­ä¸åŒäºè‹±è¯­ï¼ˆåŸºå‡†æµ‹è¯•è¯­è¨€ï¼‰çš„ç‰¹å®šè¯­è¨€æ¥æŒ‡ç¤ºæ¨ç†é—®é¢˜çš„æ­£ç¡®ç­”æ¡ˆã€‚è¿™ä¸ä»…è¦æ±‚æ¨¡å‹è¶…è¶Šå…¶ä¸Šä¸‹æ–‡çª—å£è¿›è¡Œæ¨ç†ï¼Œè¿˜è¦æ±‚å…¶å…‹æœé»˜è®¤å€¾å‘ï¼Œç”¨ä¸æç¤ºç›¸åŒçš„è¯­è¨€è¿›è¡Œå›åº”ï¼Œä»è€Œæ„æˆé¢å¤–çš„è®¤çŸ¥è´Ÿæ‹…ã€‚æˆ‘ä»¬è¯„ä¼°äº†18ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæ€§èƒ½è¡¨ç°å­˜åœ¨æ˜¾è‘—å·®è·ï¼ŒGPT-4.5å‡†ç¡®ç‡æœ€é«˜ï¼ˆ74.7%ï¼‰ï¼Œä¼˜äºGrok-2ï¼ˆ67.2%ï¼‰å’ŒLlama 3.1 405Bï¼ˆ65.6%ï¼‰ã€‚æ§åˆ¶å®éªŒå’Œéš¾åº¦åˆ†æè¡¨æ˜ï¼Œè™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ä¼šè¿›è¡Œå†…éƒ¨æ¨ç†ï¼Œä½†åœ¨æŸäº›æ¡ä»¶ä¸‹æˆ‘ä»¬ä¸æ’é™¤å…¶åˆ©ç”¨å¯å‘å¼ç­–ç•¥çš„å¯èƒ½æ€§ï¼Œè¿™æ ‡å¿—ç€æœªæ¥ç ”ç©¶çš„ä¸€ä¸ªæ–¹å‘ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹å¯ä»¥é€šè¿‡æ½œåœ¨ç©ºé—´è®¡ç®—è¿›è¡Œâ€œæ€è€ƒâ€ï¼Œæ­ç¤ºéœ€è¦è¿›ä¸€æ­¥ç†è§£çš„æ¨¡å‹å†…éƒ¨æ¨ç†ç­–ç•¥ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸å®‰å…¨ç›¸å…³çš„æ‹…å¿§æœ‰å…³çš„é—®é¢˜ä¸­ï¼Œå¦‚éšè”½è§„åˆ’ã€ç›®æ ‡å¯»æ±‚æˆ–æ²¡æœ‰æ˜ç¡®ä»¤ç‰Œç—•è¿¹çš„æ¬ºéª—ç­‰ç­–ç•¥çš„å‡ºç°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10615v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿåœ¨å…¶æ½œåœ¨ç©ºé—´å†…éƒ¨å’Œå¤–éƒ¨è¿›è¡Œæ¨ç†è®¡ç®—ï¼Œé€šè¿‡ç”Ÿæˆæ˜ç¡®çš„ä»¤ç‰Œåºåˆ—ï¼ˆå¦‚æ€ç»´é“¾ï¼‰ã€‚è™½ç„¶é€šè¿‡æ‰©å¤§æµ‹è¯•æ—¶çš„è®¡ç®—è§„æ¨¡åœ¨å¢å¼ºæ¨ç†èƒ½åŠ›æ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†ç†è§£å’Œé‡åŒ–æ¨¡å‹å†…éƒ¨çš„æ¨ç†èƒ½åŠ›ä»ç„¶è‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ä¸ªåŒ…å«4000ä¸ªé¡¹ç›®çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨é‡åŒ–ä¸åŒé¢†åŸŸçš„æ¨¡å‹å†…éƒ¨æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬è®©LLMé€šè¿‡é€‰æ‹©ä¸åŒäºåŸºå‡†è¯­è¨€è‹±è¯­çš„åˆå§‹å“åº”ä»¤ç‰Œæ¥æŒ‡ç¤ºæ¨ç†é—®é¢˜çš„æ­£ç¡®è§£å†³æ–¹æ¡ˆï¼Œä»è€Œå®ç°è¿™ä¸€ç‚¹ã€‚è¿™ä¸ä»…è¦æ±‚æ¨¡å‹è¶…è¶Šå…¶ä¸Šä¸‹æ–‡çª—å£è¿›è¡Œæ¨ç†ï¼Œè€Œä¸”è¦æ±‚å…‹æœå…¶é»˜è®¤å€¾å‘ï¼Œå³åœ¨æç¤ºä¸­ä½¿ç”¨ä¸æç¤ºç›¸åŒçš„è¯­è¨€è¿›è¡Œå›åº”ï¼Œä»è€Œæ„æˆé¢å¤–çš„è®¤çŸ¥è´Ÿæ‹…ã€‚æˆ‘ä»¬è¯„ä¼°äº†18ä¸ªLLMï¼Œæ€§èƒ½è¡¨ç°å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼ŒGPT-4.5å‡†ç¡®ç‡æœ€é«˜ï¼ˆ74.7%ï¼‰ï¼Œä¼˜äºGrok-2ï¼ˆ67.2%ï¼‰å’ŒLlama 3.1 405Bï¼ˆ65.6%ï¼‰ã€‚æ§åˆ¶å®éªŒå’Œéš¾åº¦åˆ†æè¡¨æ˜ï¼Œè™½ç„¶LLMä¼šè¿›è¡Œå†…éƒ¨æ¨ç†ï¼Œä½†æˆ‘ä»¬ä¸èƒ½æ’é™¤åœ¨æŸäº›æ¡ä»¶ä¸‹åˆ©ç”¨å¯å‘å¼ç­–ç•¥çš„å¯èƒ½æ€§ï¼Œè¿™ä¸ºæœªæ¥ç ”ç©¶æä¾›äº†æ–¹å‘ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜LLMå¯ä»¥é€šè¿‡æ½œåœ¨ç©ºé—´è®¡ç®—è¿›è¡Œâ€œæ€è€ƒâ€ï¼Œæ­ç¤ºäº†éœ€è¦è¿›ä¸€æ­¥ç†è§£çš„æ¨¡å‹å†…éƒ¨æ¨ç†ç­–ç•¥ï¼Œç‰¹åˆ«æ˜¯åœ¨å®‰å…¨ç›¸å…³é—®é¢˜ä¸Šï¼Œå¦‚éšè”½è§„åˆ’ã€ç›®æ ‡è¿½æ±‚æˆ–æ¬ºéª—ç­‰æ— æ˜ç¡®ä»¤ç‰Œç—•è¿¹çš„é—®é¢˜ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·æœ‰åœ¨æ½œåœ¨ç©ºé—´å†…éƒ¨å’Œå¤–éƒ¨è¿›è¡Œæ¨ç†è®¡ç®—çš„èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡ç”Ÿæˆä»¤ç‰Œåºåˆ—ï¼ˆå¦‚æ€ç»´é“¾ï¼‰æ¥å®Œæˆå¤–éƒ¨æ¨ç†è®¡ç®—ã€‚</li>
<li>åœ¨æµ‹è¯•æ—¶é—´è®¡ç®—è§„æ¨¡æ‰©å¤§çš„æƒ…å†µä¸‹ï¼Œå¢å¼ºæ¨ç†èƒ½åŠ›å–å¾—æ˜¾è‘—è¿›å±•ã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªåŒ…å«4000ä¸ªé¡¹ç›®çš„åŸºå‡†æµ‹è¯•æ¥é‡åŒ–æ¨¡å‹å†…éƒ¨æ¨ç†èƒ½åŠ›ã€‚</li>
<li>LLMéœ€è¦å…‹æœé»˜è®¤çš„è¯­è¨€å€¾å‘ä»¥å®ŒæˆåŸºå‡†æµ‹è¯•ä¸­çš„æ¨ç†é—®é¢˜ã€‚</li>
<li>è¯„ä¼°äº†å¤šä¸ªLLMçš„æ€§èƒ½ï¼Œå‘ç°å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼ŒGPT-4.5è¡¨ç°æœ€ä½³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10615">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3ccac95a0836ed9370aac6dab60007ce.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fe4db6f1b16495ef1a2020450b25728d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-036f3f2d4edf729ce593e4a587afdc13.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3850be37159d9616cb81be51d29caf52.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae3aa960c4d0f1b0b69454e8ec1325a6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-60159eb2b2aa9a035efeedc8e57f1d79.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Weight-Ensembling-Improves-Reasoning-in-Language-Models"><a href="#Weight-Ensembling-Improves-Reasoning-in-Language-Models" class="headerlink" title="Weight Ensembling Improves Reasoning in Language Models"></a>Weight Ensembling Improves Reasoning in Language Models</h2><p><strong>Authors:Xingyu Dang, Christina Baek, Kaiyue Wen, Zico Kolter, Aditi Raghunathan</strong></p>
<p>We investigate a failure mode that arises during the training of reasoning models, where the diversity of generations begins to collapse, leading to suboptimal test-time scaling. Notably, the Pass@1 rate reliably improves during supervised finetuning (SFT), but Pass@k rapidly deteriorates. Surprisingly, a simple intervention of interpolating the weights of the latest SFT checkpoint with an early checkpoint, otherwise known as WiSE-FT, almost completely recovers Pass@k while also improving Pass@1. The WiSE-FT variant achieves better test-time scaling (Best@k, majority vote) and achieves superior results with less data when tuned further by reinforcement learning. Finally, we find that WiSE-FT provides complementary performance gains that cannot be achieved only through diversity-inducing decoding strategies, like temperature scaling. We formalize a bias-variance tradeoff of Pass@k with respect to the expectation and variance of Pass@1 over the test distribution. We find that WiSE-FT can reduce bias and variance simultaneously, while temperature scaling inherently trades-off between bias and variance. </p>
<blockquote>
<p>æˆ‘ä»¬ç ”ç©¶äº†ä¸€ç§åœ¨è®­ç»ƒæ¨ç†æ¨¡å‹è¿‡ç¨‹ä¸­å‡ºç°çš„æ•…éšœæ¨¡å¼ï¼Œè¯¥æ¨¡å¼ä¸‹ç”Ÿæˆçš„å¤šæ ·æ€§å¼€å§‹å´©æºƒï¼Œå¯¼è‡´æµ‹è¯•æ—¶çš„ç¼©æ”¾æ€§èƒ½ä¸‹é™ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè™½ç„¶ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æœŸé—´çš„Pass@1ç‡å¯é åœ°æé«˜ï¼Œä½†Pass@kå´è¿…é€Ÿæ¶åŒ–ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œé€šè¿‡æ’å€¼æœ€æ–°SFTæ£€æŸ¥ç‚¹ä¸æ—©æœŸæ£€æŸ¥ç‚¹çš„æƒé‡è¿›è¡Œç®€å•å¹²é¢„ï¼Œå³æ‰€è°“çš„WiSE-FTï¼Œå‡ ä¹å¯ä»¥å®Œå…¨æ¢å¤Pass@kå¹¶æé«˜Pass@1ã€‚WiSE-FTå˜ä½“å®ç°äº†æ›´å¥½çš„æµ‹è¯•æ—¶é—´ç¼©æ”¾ï¼ˆBest@kï¼Œå¤šæ•°æŠ•ç¥¨ï¼‰ï¼Œå¹¶ä¸”å½“é€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›ä¸€æ­¥è°ƒæ•´æ—¶ï¼Œåœ¨è¾ƒå°‘æ•°æ®çš„æƒ…å†µä¸‹å–å¾—äº†æ›´å¥½çš„ç»“æœã€‚æœ€åï¼Œæˆ‘ä»¬å‘ç°WiSE-FTæä¾›äº†æ— æ³•é€šè¿‡å•çº¯é‡‡ç”¨è¯±å¯¼å¤šæ ·æ€§çš„è§£ç ç­–ç•¥ï¼ˆå¦‚æ¸©åº¦ç¼©æ”¾ï¼‰å®ç°çš„æ€§èƒ½å¢ç›Šã€‚æˆ‘ä»¬æ­£å¼æå‡ºäº†å…³äºPass@kçš„æœŸæœ›åå·®ä¸Pass@1åœ¨æµ‹è¯•åˆ†å¸ƒä¸Šçš„æ–¹å·®ä¹‹é—´çš„æƒè¡¡ã€‚æˆ‘ä»¬å‘ç°WiSE-FTå¯ä»¥åŒæ—¶å‡å°‘åå·®å’Œæ–¹å·®ï¼Œè€Œæ¸©åº¦ç¼©æ”¾åˆ™å›ºæœ‰çš„åœ¨åå·®å’Œæ–¹å·®ä¹‹é—´è¿›è¡Œæƒè¡¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10478v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è®­ç»ƒæ¨ç†æ¨¡å‹æ—¶ä¼šå‡ºç°ä¸€ç§ä¸–ä»£å¤šæ ·æ€§å´©æºƒçš„å¤±æ•ˆæ¨¡å¼ï¼Œå¯¼è‡´æµ‹è¯•æ—¶æ‰©å±•æ€§ä¸ä½³ã€‚å°½ç®¡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰çš„Pass@1ç‡æœ‰æ‰€æé«˜ï¼Œä½†Pass@kå´è¿…é€Ÿæ¶åŒ–ã€‚æœ‰è¶£çš„æ˜¯ï¼Œé€šè¿‡é‡‡ç”¨ä¸€ç§åä¸ºWiSE-FTçš„ç­–ç•¥â€”â€”å³å°†æœ€æ–°SFTæ£€æŸ¥ç‚¹çš„æƒé‡ä¸æ—©æœŸæ£€æŸ¥ç‚¹è¿›è¡Œæ’å€¼â€”â€”å‡ ä¹å¯ä»¥å®Œå…¨æ¢å¤Pass@kå¹¶æ”¹å–„Pass@1ã€‚WiSE-FTå˜ä½“åœ¨æµ‹è¯•æ—¶çš„æ‰©å±•æ€§æ›´ä½³ï¼Œå¹¶ä¸”åœ¨é€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›ä¸€æ­¥è°ƒæ•´æ—¶ï¼Œåœ¨å°‘æ•°æ®æƒ…å†µä¸‹ä¹Ÿèƒ½å–å¾—æ›´å¥½çš„ç»“æœã€‚WiSE-FTæä¾›äº†æ— æ³•é€šè¿‡å•ä¸€æ¸©åº¦ç¼©æ”¾ç­‰è§£ç ç­–ç•¥å®ç°çš„æ€§èƒ½å¢ç›Šã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æå‡ºäº†å…³äºPass@kçš„æœŸæœ›ä¸æ–¹å·®ä¹‹é—´çš„åè§æ–¹å·®æƒè¡¡ï¼Œå¹¶å‘ç°WiSE-FTå¯ä»¥åŒæ—¶å‡å°‘åè§å’Œæ–¹å·®ï¼Œè€Œæ¸©åº¦ç¼©æ”¾åˆ™å­˜åœ¨åè§å’Œæ–¹å·®ä¹‹é—´çš„å›ºæœ‰æƒè¡¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¨ç†æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šå‡ºç°ä¸–ä»£å¤šæ ·æ€§å´©æºƒçš„é—®é¢˜ï¼Œå½±å“æµ‹è¯•æ—¶çš„æ‰©å±•æ€§ã€‚</li>
<li>ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰åœ¨æå‡Pass@1ç‡çš„åŒæ—¶ä¼šå¯¼è‡´Pass@kè¿…é€Ÿæ¶åŒ–ã€‚</li>
<li>WiSE-FTç­–ç•¥â€”â€”å³æ’å€¼å¤„ç†æœ€æ–°ä¸æ—©æœŸæ£€æŸ¥ç‚¹çš„æƒé‡â€”â€”èƒ½æœ‰æ•ˆæ¢å¤Pass@kå¹¶æ”¹å–„Pass@1æ€§èƒ½ã€‚</li>
<li>WiSE-FTå˜ä½“å±•ç°å‡ºæ›´ä½³çš„æµ‹è¯•æ‰©å±•æ€§ï¼Œä¸”å°‘æ•°æ®æƒ…å†µä¸‹å¼ºåŒ–å­¦ä¹ æ•ˆæœæ›´ä½³ã€‚</li>
<li>WiSE-FTæä¾›çš„æ€§èƒ½å¢ç›Šæ— æ³•é€šè¿‡å•ä¸€è§£ç ç­–ç•¥å¦‚æ¸©åº¦ç¼©æ”¾å®ç°ã€‚</li>
<li>è®ºæ–‡æå‡ºäº†Pass@kçš„æœŸæœ›ä¸æ–¹å·®ä¹‹é—´çš„åè§æ–¹å·®æƒè¡¡é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10478">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2a4a8ebae776b1d77842ba8d3d74ffaa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-35ab658160b62d3f0d616864b4337d50.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c217533d16be91c9f8584fdc6592eaa7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-644c79548a4c3a9ed2164f571ce6a777.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="GUI-R1-A-Generalist-R1-Style-Vision-Language-Action-Model-For-GUI-Agents"><a href="#GUI-R1-A-Generalist-R1-Style-Vision-Language-Action-Model-For-GUI-Agents" class="headerlink" title="GUI-R1 : A Generalist R1-Style Vision-Language Action Model For GUI   Agents"></a>GUI-R1 : A Generalist R1-Style Vision-Language Action Model For GUI   Agents</h2><p><strong>Authors:Xiaobo Xia, Run Luo</strong></p>
<p>Existing efforts in building Graphical User Interface (GUI) agents largely rely on the training paradigm of supervised fine-tuning on Large Vision-Language Models (LVLMs). However, this approach not only demands extensive amounts of training data but also struggles to effectively understand GUI screenshots and generalize to unseen interfaces. The issue significantly limits its application in real-world scenarios, especially for high-level tasks. Inspired by Reinforcement Fine-Tuning (RFT) in large reasoning models (e.g., DeepSeek-R1), which efficiently enhances the problem-solving capabilities of large language models in real-world settings, we propose \name, the first reinforcement learning framework designed to enhance the GUI capabilities of LVLMs in high-level real-world task scenarios, through unified action space rule modeling. By leveraging a small amount of carefully curated high-quality data across multiple platforms (including Windows, Linux, MacOS, Android, and Web) and employing policy optimization algorithms such as Group Relative Policy Optimization (GRPO) to update the model, \name achieves superior performance using only 0.02% of the data (3K vs. 13M) compared to previous state-of-the-art methods like OS-Atlas across eight benchmarks spanning three different platforms (mobile, desktop, and web). These results demonstrate the immense potential of reinforcement learning based on unified action space rule modeling in improving the execution capabilities of LVLMs for real-world GUI agent tasks. </p>
<blockquote>
<p>ç°æœ‰æ„å»ºå›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†çš„åŠªåŠ›å¤§å¤šä¾èµ–äºåœ¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰ä¸Šé‡‡ç”¨ç›‘ç£å¾®è°ƒè®­ç»ƒèŒƒå¼ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•ä¸ä»…éœ€æ±‚å¤§é‡çš„è®­ç»ƒæ•°æ®ï¼Œè€Œä¸”åœ¨ç†è§£GUIæˆªå›¾å’Œæ¨å¹¿åˆ°æœªè§è¿‡çš„ç•Œé¢æ—¶é¢ä¸´å›°éš¾ã€‚è¿™ä¸€é—®é¢˜æå¤§åœ°é™åˆ¶äº†å…¶åœ¨ç°å®åœºæ™¯ä¸­çš„åº”ç”¨ï¼Œå°¤å…¶æ˜¯å¯¹äºé«˜çº§ä»»åŠ¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10458v2">PDF</a> </p>
<p><strong>Summary</strong><br>åœ¨æ„å»ºå›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†æ–¹é¢ï¼Œç°æœ‰åŠªåŠ›å¤§å¤šä¾èµ–äºåœ¨å¤§è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰ä¸Šé‡‡ç”¨ç›‘ç£å¾®è°ƒè®­ç»ƒèŒƒå¼ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•ä¸ä»…éœ€æ±‚å¤§é‡çš„è®­ç»ƒæ•°æ®ï¼Œè€Œä¸”åœ¨ç†è§£GUIæˆªå›¾å’Œæ³›åŒ–åˆ°æœªè§è¿‡çš„ç•Œé¢ä¸Šå­˜åœ¨å›°éš¾ã€‚è¿™é™åˆ¶äº†å…¶åœ¨ç°å®ä¸–ç•Œåœºæ™¯ä¸­çš„åº”ç”¨ï¼Œå°¤å…¶æ˜¯åœ¨é«˜çº§ä»»åŠ¡ä¸­ã€‚å—å¤§å‹æ¨ç†æ¨¡å‹ä¸­çš„å¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰çš„å¯å‘ï¼ˆä¾‹å¦‚DeepSeek-R1ï¼‰ï¼Œå®ƒé€šè¿‡é«˜æ•ˆå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç°å®ä¸–ç•Œè®¾ç½®ä¸­çš„é—®é¢˜è§£å†³èƒ½åŠ›ï¼Œæˆ‘ä»¬æå‡ºåä¸ºâ€œåç§°â€çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå®ƒæ˜¯ä¸“ä¸ºæé«˜LVLMsåœ¨é«˜çº§ç°å®ä¸–ç•Œä»»åŠ¡åœºæ™¯ä¸­çš„GUIèƒ½åŠ›è€Œè®¾è®¡çš„ï¼Œé€šè¿‡ç»Ÿä¸€åŠ¨ä½œç©ºé—´è§„åˆ™å»ºæ¨¡ã€‚é€šè¿‡åˆ©ç”¨å¤šä¸ªå¹³å°ï¼ˆåŒ…æ‹¬Windowsã€Linuxã€MacOSã€Androidå’ŒWebï¼‰çš„å°é‡ç²¾å¿ƒæŒ‘é€‰çš„é«˜è´¨é‡æ•°æ®ï¼Œå¹¶é‡‡ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç­‰ç­–ç•¥ä¼˜åŒ–ç®—æ³•æ¥æ›´æ–°æ¨¡å‹ï¼Œâ€œåç§°â€ä»…ä½¿ç”¨0.02%çš„æ•°æ®ï¼ˆ3Kå¯¹13Mï¼‰å°±åœ¨è·¨è¶Šä¸‰ä¸ªä¸åŒå¹³å°ï¼ˆç§»åŠ¨ã€æ¡Œé¢å’Œç½‘é¡µï¼‰çš„å…«ä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å¯¹OS-Atlasç­‰ç°æœ‰å…ˆè¿›æ–¹æ³•çš„å“è¶Šæ€§èƒ½ã€‚è¿™äº›ç»“æœè¯æ˜äº†åŸºäºç»Ÿä¸€åŠ¨ä½œç©ºé—´è§„åˆ™å»ºæ¨¡çš„å¼ºåŒ–å­¦ä¹ åœ¨æå‡LVLMsæ‰§è¡Œç°å®ä¸–ç•ŒGUIä»£ç†ä»»åŠ¡çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰GUIä»£ç†æ„å»ºä¸»è¦ä¾èµ–ç›‘ç£å¾®è°ƒè®­ç»ƒèŒƒå¼ï¼Œéœ€è¦å¤§é‡è®­ç»ƒæ•°æ®ï¼Œä¸”åœ¨ç†è§£å’Œæ³›åŒ–GUIç•Œé¢ä¸Šæœ‰å±€é™æ€§ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ æ¡†æ¶è¢«æå‡ºï¼Œç”¨ä»¥æé«˜LVLMsåœ¨ç°å®ä¸–ç•Œé«˜çº§ä»»åŠ¡åœºæ™¯ä¸­çš„GUIèƒ½åŠ›ï¼Œé€šè¿‡ç»Ÿä¸€åŠ¨ä½œç©ºé—´è§„åˆ™å»ºæ¨¡ã€‚</li>
<li>è¯¥æ¡†æ¶åˆ©ç”¨è·¨å¤šä¸ªå¹³å°çš„å°é‡é«˜è´¨é‡æ•°æ®ï¼Œå¹¶é‡‡ç”¨ç­–ç•¥ä¼˜åŒ–ç®—æ³•æ¥æ›´æ–°æ¨¡å‹ã€‚</li>
<li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ¡†æ¶åœ¨è·¨è¶Šä¸åŒå¹³å°çš„å…«ä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å“è¶Šæ€§èƒ½ï¼Œä»…ä½¿ç”¨æå°‘é‡çš„æ•°æ®ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ åœ¨æå‡LVLMsæ‰§è¡ŒGUIä»£ç†ä»»åŠ¡æ–¹é¢çš„æ½œåŠ›å·¨å¤§ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºGUIä»£ç†çš„æ„å»ºæä¾›äº†ä¸€ç§æ–°çš„ã€æ•°æ®æ•ˆç‡æ›´é«˜çš„æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10458">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-9a9e0a42308a982366e9279cfb053bbd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-452117247a099f01923cad779a0c3f47.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0979f294f71f855149b4b7942ea5c51d.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="VisualPuzzles-Decoupling-Multimodal-Reasoning-Evaluation-from-Domain-Knowledge"><a href="#VisualPuzzles-Decoupling-Multimodal-Reasoning-Evaluation-from-Domain-Knowledge" class="headerlink" title="VisualPuzzles: Decoupling Multimodal Reasoning Evaluation from Domain   Knowledge"></a>VisualPuzzles: Decoupling Multimodal Reasoning Evaluation from Domain   Knowledge</h2><p><strong>Authors:Yueqi Song, Tianyue Ou, Yibo Kong, Zecheng Li, Graham Neubig, Xiang Yue</strong></p>
<p>Current multimodal benchmarks often conflate reasoning with domain-specific knowledge, making it difficult to isolate and evaluate general reasoning abilities in non-expert settings. To address this, we introduce VisualPuzzles, a benchmark that targets visual reasoning while deliberately minimizing reliance on specialized knowledge. VisualPuzzles consists of diverse questions spanning five categories: algorithmic, analogical, deductive, inductive, and spatial reasoning. One major source of our questions is manually translated logical reasoning questions from the Chinese Civil Service Examination. Experiments show that VisualPuzzles requires significantly less intensive domain-specific knowledge and more complex reasoning compared to benchmarks like MMMU, enabling us to better evaluate genuine multimodal reasoning. Evaluations show that state-of-the-art multimodal large language models consistently lag behind human performance on VisualPuzzles, and that strong performance on knowledge-intensive benchmarks does not necessarily translate to success on reasoning-focused, knowledge-light tasks. Additionally, reasoning enhancements such as scaling up inference compute (with â€œthinkingâ€ modes) yield inconsistent gains across models and task types, and we observe no clear correlation between model size and performance. We also found that models exhibit different reasoning and answering patterns on VisualPuzzles compared to benchmarks with heavier emphasis on knowledge. VisualPuzzles offers a clearer lens through which to evaluate reasoning capabilities beyond factual recall and domain knowledge. </p>
<blockquote>
<p>å½“å‰çš„å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•é€šå¸¸å°†æ¨ç†ä¸ç‰¹å®šé¢†åŸŸçš„çŸ¥è¯†æ··æ·†ï¼Œä½¿å¾—åœ¨éä¸“ä¸šç¯å¢ƒä¸­éš”ç¦»å’Œè¯„ä¼°ä¸€èˆ¬çš„æ¨ç†èƒ½åŠ›å˜å¾—å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†VisualPuzzlesï¼Œè¿™æ˜¯ä¸€ä¸ªä»¥è§†è§‰æ¨ç†ä¸ºç›®æ ‡ï¼ŒåŒæ—¶åˆ»æ„å‡å°‘å¯¹ä¸“ä¸šçŸ¥è¯†ä¾èµ–çš„åŸºå‡†æµ‹è¯•ã€‚VisualPuzzlesåŒ…å«äº”ä¸ªç±»åˆ«çš„é—®é¢˜ï¼šç®—æ³•ã€ç±»æ¯”ã€æ¼”ç»ã€å½’çº³å’Œç©ºé—´æ¨ç†ã€‚æˆ‘ä»¬çš„é—®é¢˜ä¸»è¦æ¥æºäºä¸­å›½å…¬åŠ¡å‘˜è€ƒè¯•ä¸­æ‰‹åŠ¨ç¿»è¯‘çš„é€»è¾‘æ¨ç†é—®é¢˜ã€‚å®éªŒè¡¨æ˜ï¼Œä¸MMMUç­‰åŸºå‡†æµ‹è¯•ç›¸æ¯”ï¼ŒVisualPuzzleså¯¹ä¸“ä¸šçŸ¥è¯†çš„è¦æ±‚å¤§å¤§é™ä½ï¼Œä½†éœ€è¦å¯¹æ¨ç†æœ‰æ›´å¤æ‚çš„ç†è§£ã€‚è¯„ä¼°æ˜¾ç¤ºï¼Œæœ€å…ˆè¿›çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨VisualPuzzlesä¸Šçš„è¡¨ç°å§‹ç»ˆè½åäºäººç±»ï¼Œè€Œä¸”åœ¨çŸ¥è¯†å¯†é›†å‹åŸºå‡†æµ‹è¯•ä¸Šçš„å‡ºè‰²è¡¨ç°å¹¶ä¸ä¸€å®šèƒ½åœ¨æ³¨é‡æ¨ç†ã€è½»çŸ¥è¯†çš„ä»»åŠ¡ä¸Šå–å¾—æˆåŠŸã€‚æ­¤å¤–ï¼Œé€šè¿‡æ‰©å¤§æ¨ç†è®¡ç®—ï¼ˆä½¿ç”¨â€œæ€è€ƒâ€æ¨¡å¼ï¼‰ç­‰å¢å¼ºæ¨ç†çš„æ–¹æ³•åœ¨æ¨¡å‹å’Œä»»åŠ¡ç±»å‹ä¹‹é—´äº§ç”Ÿäº†ä¸ä¸€è‡´çš„æ•ˆç›Šï¼Œå¹¶ä¸”æˆ‘ä»¬è§‚å¯Ÿåˆ°æ¨¡å‹å¤§å°ä¸æ€§èƒ½ä¹‹é—´æ²¡æœ‰æ˜æ˜¾çš„ç›¸å…³æ€§ã€‚æˆ‘ä»¬è¿˜å‘ç°ï¼Œä¸æ›´ä¾§é‡äºçŸ¥è¯†çš„åŸºå‡†æµ‹è¯•ç›¸æ¯”ï¼Œæ¨¡å‹åœ¨VisualPuzzlesä¸Šå±•ç°å‡ºä¸åŒçš„æ¨ç†å’Œç­”é¢˜æ¨¡å¼ã€‚VisualPuzzlesæä¾›äº†ä¸€ä¸ªæ›´æ¸…æ™°çš„é€é•œï¼Œå¯ä»¥è¯„ä¼°è¶…è¶Šäº‹å®å›å¿†å’Œé¢†åŸŸçŸ¥è¯†çš„æ¨ç†èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10342v2">PDF</a> 56 pages, 43 figures</p>
<p><strong>Summary</strong></p>
<p>VisualPuzzlesåŸºå‡†æµ‹è¯•æ—¨åœ¨è¯„ä¼°è§†è§‰æ¨ç†èƒ½åŠ›ï¼Œå°½é‡å°‘ä¾èµ–ä¸“ä¸šçŸ¥è¯†ã€‚å®ƒåŒ…æ‹¬æ¶µç›–äº”ç§æ¨ç†ç±»åˆ«çš„å¤šæ ·åŒ–é—®é¢˜ï¼šç®—æ³•ã€ç±»æ¯”ã€æ¼”ç»ã€å½’çº³å’Œç©ºé—´æ¨ç†ã€‚å®éªŒè¡¨æ˜ï¼Œä¸MMMUç­‰åŸºå‡†æµ‹è¯•ç›¸æ¯”ï¼ŒVisualPuzzlesæ›´ä¾§é‡äºæ¨ç†è€Œéä¸“ä¸šçŸ¥è¯†ã€‚è§†è§‰æ¨¡å‹åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸Šçš„è¯„ä¼°è½åäºäººç±»è¡¨ç°ï¼Œä¸”åœ¨çŸ¥è¯†è½»å‹çš„æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°å¹¶ä¸ä¸€è‡´ã€‚å¢å¼ºæ¨ç†èƒ½åŠ›çš„æ‰‹æ®µï¼Œå¦‚æ‰©å¤§æ¨ç†è®¡ç®—è§„æ¨¡æˆ–å¢åŠ â€œæ€è€ƒâ€æ¨¡å¼ï¼Œå¯¹æ¨¡å‹å’Œä»»åŠ¡ç±»å‹çš„è¡¨ç°ä¸ä¸€ã€‚æ­¤åŸºå‡†ä¸ºè¯„ä¼°è¶…è¶Šäº‹å®å›å¿†å’Œé¢†åŸŸçŸ¥è¯†çš„æ¨ç†èƒ½åŠ›æä¾›äº†æ¸…æ™°çš„è§†è§’ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VisualPuzzlesæ˜¯ä¸€ä¸ªé’ˆå¯¹è§†è§‰æ¨ç†çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨å‡å°‘ä¸“ä¸šçŸ¥è¯†ä¾èµ–ã€‚</li>
<li>å®ƒåŒ…å«æ¶µç›–å¤šç§æ¨ç†ç±»åˆ«çš„é¢˜ç›®ï¼ŒåŒ…æ‹¬ç®—æ³•ã€ç±»æ¯”ã€æ¼”ç»ã€å½’çº³å’Œç©ºé—´æ¨ç†ã€‚</li>
<li>ä¸å…¶ä»–åŸºå‡†æµ‹è¯•ç›¸æ¯”ï¼ŒVisualPuzzlesæ›´æ³¨é‡è¯„ä¼°æ¨ç†èƒ½åŠ›è€Œéä¸“ä¸šçŸ¥è¯†ã€‚</li>
<li>å½“å‰æœ€å…ˆè¿›çš„è§†è§‰æ¨¡å‹åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸Šçš„è¡¨ç°è½åäºäººç±»ã€‚</li>
<li>åœ¨çŸ¥è¯†è½»å‹çš„æ¨ç†ä»»åŠ¡ä¸Šï¼Œå¢å¼ºæ¨ç†èƒ½åŠ›çš„æ‰‹æ®µè¡¨ç°ä¸ä¸€ã€‚</li>
<li>æ¨¡å‹åœ¨VisualPuzzlesä¸Šçš„è¡¨ç°ä¸ä¾§é‡çŸ¥è¯†çš„åŸºå‡†æµ‹è¯•å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10342">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-60221d3bdb4984be7cd1dc32b7585f35.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7d978ad9899040c77f89efe8c0764d40.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea435c18c082e0889e2175cf11074b9e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-13f7f96ad625b34e51ee23a2c63e4a07.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e74d6329666e97d1651eb85005f486e2.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Breaking-the-Data-Barrier-â€“-Building-GUI-Agents-Through-Task-Generalization"><a href="#Breaking-the-Data-Barrier-â€“-Building-GUI-Agents-Through-Task-Generalization" class="headerlink" title="Breaking the Data Barrier â€“ Building GUI Agents Through Task   Generalization"></a>Breaking the Data Barrier â€“ Building GUI Agents Through Task   Generalization</h2><p><strong>Authors:Junlei Zhang, Zichen Ding, Chang Ma, Zijie Chen, Qiushi Sun, Zhenzhong Lan, Junxian He</strong></p>
<p>Graphical User Interface (GUI) agents offer cross-platform solutions for automating complex digital tasks, with significant potential to transform productivity workflows. However, their performance is often constrained by the scarcity of high-quality trajectory data. To address this limitation, we propose training Vision Language Models (VLMs) on data-rich, reasoning-intensive tasks during a dedicated mid-training stage, and then examine how incorporating these tasks facilitates generalization to GUI planning scenarios. Specifically, we explore a range of tasks with readily available instruction-tuning data, including GUI perception, multimodal reasoning, and textual reasoning. Through extensive experiments across 11 mid-training tasks, we demonstrate that: (1) Task generalization proves highly effective, yielding substantial improvements across most settings. For instance, multimodal mathematical reasoning enhances performance on AndroidWorld by an absolute 6.3%. Remarkably, text-only mathematical data significantly boosts GUI web agent performance, achieving a 5.6% improvement on WebArena and 5.4% improvement on AndroidWorld, underscoring notable cross-modal generalization from text-based to visual domains; (2) Contrary to prior assumptions, GUI perception data - previously considered closely aligned with GUI agent tasks and widely utilized for training - has a comparatively limited impact on final performance; (3) Building on these insights, we identify the most effective mid-training tasks and curate optimized mixture datasets, resulting in absolute performance gains of 8.0% on WebArena and 12.2% on AndroidWorld. Our work provides valuable insights into cross-domain knowledge transfer for GUI agents and offers a practical approach to addressing data scarcity challenges in this emerging field. The code, data and models will be available at <a target="_blank" rel="noopener" href="https://github.com/hkust-nlp/GUIMid">https://github.com/hkust-nlp/GUIMid</a>. </p>
<blockquote>
<p>å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†æä¾›è·¨å¹³å°çš„è‡ªåŠ¨åŒ–å¤æ‚æ•°å­—ä»»åŠ¡è§£å†³æ–¹æ¡ˆï¼Œå…·æœ‰æ”¹å˜ç”Ÿäº§åŠ›å·¥ä½œæµç¨‹çš„å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„è¡¨ç°å¾€å¾€å—åˆ°é«˜è´¨é‡è½¨è¿¹æ•°æ®ç¨€ç¼ºçš„åˆ¶çº¦ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬å»ºè®®åœ¨ä¸“é—¨çš„ä¸­é—´è®­ç»ƒé˜¶æ®µï¼Œåœ¨æ•°æ®ä¸°å¯Œã€æ¨ç†å¯†é›†çš„ä»»åŠ¡ä¸Šè®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œç„¶åç ”ç©¶å¦‚ä½•å°†è¿™äº›ä»»åŠ¡çº³å…¥GUIè§„åˆ’åœºæ™¯ä»¥ä¿ƒè¿›é€šç”¨åŒ–ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ä¸€ç³»åˆ—å…·æœ‰å¯è·å–æŒ‡ä»¤è°ƒæ•´æ•°æ®çš„ä»»åŠ¡ï¼ŒåŒ…æ‹¬GUIæ„ŸçŸ¥ã€å¤šæ¨¡æ€æ¨ç†å’Œæ–‡æœ¬æ¨ç†ã€‚é€šè¿‡å¯¹11ä¸ªä¸­é—´è®­ç»ƒä»»åŠ¡çš„å¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬è¯æ˜ï¼šï¼ˆ1ï¼‰ä»»åŠ¡é€šç”¨åŒ–éå¸¸æœ‰æ•ˆï¼Œåœ¨å¤§å¤šæ•°è®¾ç½®ä¸­éƒ½å–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚ä¾‹å¦‚ï¼Œå¤šæ¨¡æ€æ•°å­¦æ¨ç†åœ¨AndroidWorldä¸Šçš„è¡¨ç°æé«˜äº†6.3%ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåªæœ‰æ–‡æœ¬çš„æ•°å­¦æ•°æ®æ˜¾è‘—æé«˜äº†GUIç½‘ç»œä»£ç†çš„æ€§èƒ½ï¼Œåœ¨WebArenaä¸Šæé«˜äº†5.6%ï¼Œåœ¨AndroidWorldä¸Šæé«˜äº†5.4%ï¼Œçªæ˜¾å‡ºä»æ–‡æœ¬åˆ°è§†è§‰é¢†åŸŸçš„è·¨æ¨¡æ€é€šç”¨çš„é‡è¦æ€§ï¼›ï¼ˆ2ï¼‰ä¸å…ˆå‰çš„å‡è®¾ç›¸åï¼Œä¹‹å‰è¢«è®¤ä¸ºä¸GUIä»£ç†ä»»åŠ¡ç´§å¯†ç›¸å…³å¹¶å¹¿æ³›ç”¨äºè®­ç»ƒçš„GUIæ„ŸçŸ¥æ•°æ®å¯¹æœ€ç»ˆæ€§èƒ½çš„å½±å“ç›¸å¯¹æœ‰é™ï¼›ï¼ˆ3ï¼‰åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬ç¡®å®šäº†æœ€æœ‰æ•ˆçš„ä¸­é—´è®­ç»ƒä»»åŠ¡å¹¶ç­–åˆ’äº†ä¼˜åŒ–çš„æ··åˆæ•°æ®é›†ï¼Œä»è€Œåœ¨WebArenaä¸Šå®ç°äº†8.0%çš„ç»å¯¹æ€§èƒ½æå‡ï¼Œåœ¨AndroidWorldä¸Šå®ç°äº†12.2%çš„æå‡ã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ºGUIä»£ç†çš„è·¨åŸŸçŸ¥è¯†è½¬ç§»æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ï¼Œå¹¶ä¸ºè§£å†³è¿™ä¸€æ–°å…´é¢†åŸŸçš„æ•°æ®ç¨€ç¼ºæŒ‘æˆ˜æä¾›äº†å®ç”¨æ–¹æ³•ã€‚ç›¸å…³ä»£ç ã€æ•°æ®å’Œæ¨¡å‹å°†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/hkust-nlp/GUIMid%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/hkust-nlp/GUIMidæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10127v2">PDF</a> 24 pages, 11 figures</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†åœ¨è‡ªåŠ¨åŒ–å¤æ‚æ•°å­—ä»»åŠ¡æ–¹é¢çš„è·¨å¹³å°è§£å†³æ–¹æ¡ˆæ½œåŠ›ï¼Œä½†å…¶æ€§èƒ½å¾€å¾€å—åˆ°é«˜è´¨é‡è½¨è¿¹æ•°æ®çš„é™åˆ¶ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºåœ¨ä¸“é—¨çš„ä¸­é—´è®­ç»ƒé˜¶æ®µï¼Œå¯¹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è¿›è¡Œä¸°å¯Œæ•°æ®å’Œé«˜æ¨ç†ä»»åŠ¡è®­ç»ƒï¼Œå¹¶ç ”ç©¶è¿™äº›ä»»åŠ¡å¦‚ä½•ä¿ƒè¿›å¯¹GUIè§„åˆ’åœºæ™¯çš„æ³›åŒ–ã€‚å®éªŒè¯æ˜ï¼Œä»»åŠ¡æ³›åŒ–æ•ˆæœæ˜¾è‘—ï¼Œå¤šæ¨¡æ€æ•°å­¦æ¨ç†åœ¨AndroidWorldä»»åŠ¡ä¸Šçš„æ€§èƒ½æå‡è¾¾6.3%ã€‚æ–‡æœ¬æ•°å­¦æ•°æ®å¯¹GUIç½‘é¡µä»£ç†æ€§èƒ½ä¹Ÿæœ‰æ˜¾è‘—æå‡ã€‚ç ”ç©¶è¿˜æŒ‡å‡ºï¼Œä¸å…ˆå‰å‡è®¾ç›¸åï¼ŒGUIæ„ŸçŸ¥æ•°æ®å¯¹æœ€ç»ˆæ€§èƒ½çš„å½±å“æœ‰é™ã€‚åŸºäºæ­¤ï¼Œç ”ç©¶å›¢é˜Ÿä¼˜åŒ–äº†ä¸­æœŸè®­ç»ƒä»»åŠ¡å’Œæ··åˆæ•°æ®é›†ï¼Œå®ç°äº†WebArenaå’ŒAndroidWorldä»»åŠ¡ä¸Šçš„æ€§èƒ½åˆ†åˆ«æå‡8.0%å’Œ12.2%ã€‚è¯¥ç ”ç©¶ä¸ºGUIä»£ç†çš„è·¨åŸŸçŸ¥è¯†è¿ç§»æä¾›äº†å®è´µè§è§£ï¼Œå¹¶æä¾›äº†è§£å†³è¯¥é¢†åŸŸæ•°æ®ç¨€ç¼ºé—®é¢˜çš„å®ç”¨æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GUIä»£ç†å…·æœ‰è‡ªåŠ¨åŒ–å¤æ‚æ•°å­—ä»»åŠ¡çš„è·¨å¹³å°è§£å†³æ–¹æ¡ˆæ½œåŠ›ã€‚</li>
<li>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨ä¸“é—¨çš„ä¸­æœŸè®­ç»ƒé˜¶æ®µæ¥å—ä¸°å¯Œæ•°æ®å’Œæ¨ç†ä»»åŠ¡è®­ç»ƒï¼Œä»¥æå‡æ€§èƒ½ã€‚</li>
<li>ä»»åŠ¡æ³›åŒ–æ•ˆæœæ˜¾è‘—ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šæ¨¡æ€æ•°å­¦æ¨ç†æ–¹é¢ã€‚æ–‡æœ¬æ•°å­¦æ•°æ®å¯¹GUIä»£ç†æ€§èƒ½æœ‰ç§¯æå½±å“ã€‚</li>
<li>GUIæ„ŸçŸ¥æ•°æ®å¯¹GUIä»£ç†çš„æœ€ç»ˆæ€§èƒ½å½±å“æœ‰é™ã€‚</li>
<li>ä¼˜åŒ–ä¸­æœŸè®­ç»ƒä»»åŠ¡å’Œæ··åˆæ•°æ®é›†å¯æ˜¾è‘—æå‡æ€§èƒ½ã€‚</li>
<li>ç ”ç©¶æä¾›äº†GUIä»£ç†è·¨åŸŸçŸ¥è¯†è¿ç§»çš„è§è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10127">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ec1e01eefff05666c60eb7517204a35a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5e8fca516064e09908f5d82a5493484f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d11e8f3278ea7d538fd56988431440fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6e6f84dfa434bcbb7c20aa76a5eddc9a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-54162ceb000ca3b11b6105002ff0ac77.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-17/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-17/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-17/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-42543d31f9f13eed5c771f66a7161d60.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-17  TextArena
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-17
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-16/Text-to-Motion/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e9cbc94d83f03a08c1fb3a1d669eb2c3.jpg" class="responsive-img" alt="Text-to-Motion">
                        
                        <span class="card-title">Text-to-Motion</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Text-to-Motion æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-16  MoLA Motion Generation and Editing with Latent Diffusion Enhanced by   Adversarial Training
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-16
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Text-to-Motion/" class="post-category">
                                    Text-to-Motion
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Text-to-Motion/">
                        <span class="chip bg-color">Text-to-Motion</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">27685.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
