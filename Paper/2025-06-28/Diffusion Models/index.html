<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-28  SmoothSinger A Conditional Diffusion Model for Singing Voice Synthesis   with Multi-Resolution Architecture">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-2f838e90ed63bb3599ceacd153b10ac8.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-28
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    11.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    48 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-28-æ›´æ–°"><a href="#2025-06-28-æ›´æ–°" class="headerlink" title="2025-06-28 æ›´æ–°"></a>2025-06-28 æ›´æ–°</h1><h2 id="SmoothSinger-A-Conditional-Diffusion-Model-for-Singing-Voice-Synthesis-with-Multi-Resolution-Architecture"><a href="#SmoothSinger-A-Conditional-Diffusion-Model-for-Singing-Voice-Synthesis-with-Multi-Resolution-Architecture" class="headerlink" title="SmoothSinger: A Conditional Diffusion Model for Singing Voice Synthesis   with Multi-Resolution Architecture"></a>SmoothSinger: A Conditional Diffusion Model for Singing Voice Synthesis   with Multi-Resolution Architecture</h2><p><strong>Authors:Kehan Sui, Jinxu Xiang, Fang Jin</strong></p>
<p>Singing voice synthesis (SVS) aims to generate expressive and high-quality vocals from musical scores, requiring precise modeling of pitch, duration, and articulation. While diffusion-based models have achieved remarkable success in image and video generation, their application to SVS remains challenging due to the complex acoustic and musical characteristics of singing, often resulting in artifacts that degrade naturalness. In this work, we propose SmoothSinger, a conditional diffusion model designed to synthesize high quality and natural singing voices. Unlike prior methods that depend on vocoders as a final stage and often introduce distortion, SmoothSinger refines low-quality synthesized audio directly in a unified framework, mitigating the degradation associated with two-stage pipelines. The model adopts a reference-guided dual-branch architecture, using low-quality audio from any baseline system as a reference to guide the denoising process, enabling more expressive and context-aware synthesis. Furthermore, it enhances the conventional U-Net with a parallel low-frequency upsampling path, allowing the model to better capture pitch contours and long term spectral dependencies. To improve alignment during training, we replace reference audio with degraded ground truth audio, addressing temporal mismatch between reference and target signals. Experiments on the Opencpop dataset, a large-scale Chinese singing corpus, demonstrate that SmoothSinger achieves state-of-the-art results in both objective and subjective evaluations. Extensive ablation studies confirm its effectiveness in reducing artifacts and improving the naturalness of synthesized voices. </p>
<blockquote>
<p>å”±æ­Œå£°éŸ³åˆæˆï¼ˆSVSï¼‰æ—¨åœ¨ä»ä¹è°±ç”Ÿæˆè¡¨è¾¾ä¸°å¯Œã€é«˜è´¨é‡çš„æ­Œå£°ï¼Œéœ€è¦ç²¾ç¡®åœ°å¯¹éŸ³è°ƒã€æ—¶é•¿å’Œå‘éŸ³è¿›è¡Œå»ºæ¨¡ã€‚è™½ç„¶åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–¹æ³•åœ¨å›¾åƒå’Œè§†é¢‘ç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†å…¶åœ¨SVSä¸­çš„åº”ç”¨ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œè¿™æ˜¯ç”±äºæ­Œå”±çš„å¤æ‚å£°å­¦å’ŒéŸ³ä¹ç‰¹æ€§ï¼Œç»å¸¸äº§ç”Ÿé™ä½è‡ªç„¶åº¦çš„ä¼ªå½±ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†SmoothSingerï¼Œä¸€ä¸ªç”¨äºåˆæˆé«˜è´¨é‡è‡ªç„¶æ­Œå£°çš„æ¡ä»¶æ‰©æ•£æ¨¡å‹ã€‚ä¸ä»¥å¾€ä¾èµ–äºç¼–ç å™¨ä½œä¸ºæœ€åé˜¶æ®µå¹¶ç»å¸¸å¼•å…¥å¤±çœŸçš„æ–¹æ³•ä¸åŒï¼ŒSmoothSingeråœ¨ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ä¸­ç›´æ¥å¯¹ä½è´¨é‡åˆæˆéŸ³é¢‘è¿›è¡Œç²¾ç»†åŒ–å¤„ç†ï¼Œç¼“è§£äº†ä¸¤é˜¶æ®µæµæ°´çº¿å¸¦æ¥çš„é€€åŒ–ã€‚è¯¥æ¨¡å‹é‡‡ç”¨å‚è€ƒå¼•å¯¼åŒåˆ†æ”¯æ¶æ„ï¼Œä½¿ç”¨ä»»ä½•åŸºçº¿ç³»ç»Ÿäº§ç”Ÿçš„ä½è´¨é‡éŸ³é¢‘ä½œä¸ºå‚è€ƒæ¥å¼•å¯¼å»å™ªè¿‡ç¨‹ï¼Œä»è€Œå®ç°æ›´å…·è¡¨ç°åŠ›å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„åˆæˆã€‚æ­¤å¤–ï¼Œå®ƒå¢å¼ºäº†ä¼ ç»Ÿçš„U-Netç½‘ç»œï¼Œå¢åŠ äº†ä¸€æ¡å¹¶è¡Œä½é¢‘ä¸Šé‡‡æ ·è·¯å¾„ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰éŸ³è°ƒè½®å»“å’Œé•¿æœŸé¢‘è°±ä¾èµ–æ€§ã€‚ä¸ºäº†æé«˜è®­ç»ƒè¿‡ç¨‹ä¸­çš„å¯¹é½æ€§ï¼Œæˆ‘ä»¬ç”¨é€€åŒ–çœŸå®éŸ³é¢‘æ›¿æ¢å‚è€ƒéŸ³é¢‘ï¼Œè§£å†³å‚è€ƒä¿¡å·å’Œç›®æ ‡ä¿¡å·ä¹‹é—´çš„æ—¶é—´ä¸åŒ¹é…é—®é¢˜ã€‚åœ¨å¤§å‹ä¸­æ–‡æ­Œå”±è¯­æ–™åº“Opencpopæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSmoothSingeråœ¨å®¢è§‚å’Œä¸»è§‚è¯„ä¼°ä¸­å‡è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚å¹¿æ³›çš„æ¶ˆèç ”ç©¶è¯å®äº†å…¶åœ¨å‡å°‘ä¼ªå½±å’Œæé«˜åˆæˆå£°éŸ³è‡ªç„¶åº¦æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.21478v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºSmoothSingerçš„æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºåˆæˆé«˜è´¨é‡çš„è‡ªç„¶æ­Œå”±å£°éŸ³ã€‚è¯¥æ¨¡å‹æ—¨åœ¨è§£å†³æ­Œå”±å£°éŸ³åˆæˆä¸­çš„æŒ‘æˆ˜ï¼Œé€šè¿‡é‡‡ç”¨å‚è€ƒå¼•å¯¼çš„åŒåˆ†æ”¯æ¶æ„å’Œå¹¶è¡Œä½é¢‘ä¸Šé‡‡æ ·è·¯å¾„ï¼Œæé«˜äº†éŸ³é¢‘åˆæˆçš„è´¨é‡å’Œè‡ªç„¶åº¦ã€‚æ­¤å¤–ï¼Œé€šè¿‡ç”¨é€€åŒ–çœŸå®éŸ³é¢‘æ›¿æ¢å‚è€ƒéŸ³é¢‘æ¥è§£å†³è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ—¶é—´ä¸åŒ¹é…é—®é¢˜ã€‚åœ¨å¤§å‹ä¸­æ–‡æ­Œå”±è¯­æ–™åº“Opencpopä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSmoothSingeråœ¨å®¢è§‚å’Œä¸»è§‚è¯„ä¼°ä¸­éƒ½è¾¾åˆ°äº†æœ€ä½³æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SmoothSingeræ˜¯ä¸€ä¸ªæ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œæ—¨åœ¨åˆæˆé«˜è´¨é‡çš„è‡ªç„¶æ­Œå”±å£°éŸ³ã€‚</li>
<li>è¯¥æ¨¡å‹è§£å†³äº†æ­Œå”±å£°éŸ³åˆæˆä¸­çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ç²¾ç¡®çš„éŸ³é«˜ã€æ—¶é•¿å’Œå‘éŸ³å»ºæ¨¡ã€‚</li>
<li>SmoothSingeré‡‡ç”¨å‚è€ƒå¼•å¯¼çš„åŒåˆ†æ”¯æ¶æ„ï¼Œç›´æ¥åœ¨ç»Ÿä¸€æ¡†æ¶å†…å¯¹åˆæˆéŸ³é¢‘è¿›è¡Œç²¾ç»†åŒ–å¤„ç†ï¼Œé¿å…äº†ä¸¤ä¸ªé˜¶æ®µç®¡é“å¤„ç†å¸¦æ¥çš„å¤±çœŸã€‚</li>
<li>æ¨¡å‹å¢å¼ºäº†ä¼ ç»Ÿçš„U-Netç½‘ç»œï¼Œå¢åŠ äº†å¹¶è¡Œä½é¢‘ä¸Šé‡‡æ ·è·¯å¾„ï¼Œä»¥æ›´å¥½åœ°æ•æ‰éŸ³é«˜è½®å»“å’Œé•¿æœŸé¢‘è°±ä¾èµ–æ€§ã€‚</li>
<li>é€šè¿‡ç”¨é€€åŒ–çœŸå®éŸ³é¢‘æ›¿æ¢å‚è€ƒéŸ³é¢‘æ¥è§£å†³è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ—¶é—´ä¸åŒ¹é…é—®é¢˜ã€‚</li>
<li>åœ¨å¤§å‹ä¸­æ–‡æ­Œå”±è¯­æ–™åº“Opencpopä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSmoothSingeråœ¨å®¢è§‚å’Œä¸»è§‚è¯„ä¼°ä¸­éƒ½ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.21478">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-83ffaa42116990ea9c56adb6f71b7ccc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-acd5387764313af8518226a323b0787a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b191da138748cc85241126c6b38280a6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-52ac409dd5562c80a4d86a9b73efd41f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-820311175823957ad1fc3ef2ec7204cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f1da01ede1d72895851b04f06a3b545d.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="FairyGen-Storied-Cartoon-Video-from-a-Single-Child-Drawn-Character"><a href="#FairyGen-Storied-Cartoon-Video-from-a-Single-Child-Drawn-Character" class="headerlink" title="FairyGen: Storied Cartoon Video from a Single Child-Drawn Character"></a>FairyGen: Storied Cartoon Video from a Single Child-Drawn Character</h2><p><strong>Authors:Jiayi Zheng, Xiaodong Cun</strong></p>
<p>We propose FairyGen, an automatic system for generating story-driven cartoon videos from a single childâ€™s drawing, while faithfully preserving its unique artistic style. Unlike previous storytelling methods that primarily focus on character consistency and basic motion, FairyGen explicitly disentangles character modeling from stylized background generation and incorporates cinematic shot design to support expressive and coherent storytelling. Given a single character sketch, we first employ an MLLM to generate a structured storyboard with shot-level descriptions that specify environment settings, character actions, and camera perspectives. To ensure visual consistency, we introduce a style propagation adapter that captures the characterâ€™s visual style and applies it to the background, faithfully retaining the characterâ€™s full visual identity while synthesizing style-consistent scenes. A shot design module further enhances visual diversity and cinematic quality through frame cropping and multi-view synthesis based on the storyboard. To animate the story, we reconstruct a 3D proxy of the character to derive physically plausible motion sequences, which are then used to fine-tune an MMDiT-based image-to-video diffusion model. We further propose a two-stage motion customization adapter: the first stage learns appearance features from temporally unordered frames, disentangling identity from motion; the second stage models temporal dynamics using a timestep-shift strategy with frozen identity weights. Once trained, FairyGen directly renders diverse and coherent video scenes aligned with the storyboard. Extensive experiments demonstrate that our system produces animations that are stylistically faithful, narratively structured natural motion, highlighting its potential for personalized and engaging story animation. The code will be available at <a target="_blank" rel="noopener" href="https://github.com/GVCLab/FairyGen">https://github.com/GVCLab/FairyGen</a> </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†FairyGenç³»ç»Ÿï¼Œè¿™æ˜¯ä¸€ä¸ªä»å•ä¸ªå„¿ç«¥ç»˜ç”»è‡ªåŠ¨ç”Ÿæˆæ•…äº‹é©±åŠ¨åŠ¨ç”»å¡é€šè§†é¢‘çš„è‡ªåŠ¨ç³»ç»Ÿï¼ŒåŒæ—¶å¿ å®ä¿ç•™å…¶ç‹¬ç‰¹çš„è‰ºæœ¯é£æ ¼ã€‚ä¸åŒäºä¸»è¦å…³æ³¨è§’è‰²ä¸€è‡´æ€§å’ŒåŸºæœ¬è¿åŠ¨çš„å‰è¿°æ•…äº‹æ–¹æ³•ï¼ŒFairyGenæ˜¾å¼åœ°å°†è§’è‰²å»ºæ¨¡ä¸é£æ ¼åŒ–èƒŒæ™¯ç”ŸæˆåŒºåˆ†å¼€ï¼Œå¹¶ç»“åˆç”µå½±é•œå¤´è®¾è®¡ï¼Œä»¥æ”¯æŒè¡¨ç°åŠ›å’Œè¿è´¯æ€§çš„æ•…äº‹å™è¿°ã€‚ç»™å®šå•ä¸ªè§’è‰²è‰å›¾ï¼Œæˆ‘ä»¬é¦–å…ˆä½¿ç”¨MLLMç”Ÿæˆå…·æœ‰é•œå¤´çº§åˆ«æè¿°çš„ç»“æ„åŒ–æ•…äº‹æ¿ï¼Œè¿™äº›æè¿°æŒ‡å®šç¯å¢ƒè®¾ç½®ã€è§’è‰²åŠ¨ä½œå’Œç›¸æœºè§†è§’ã€‚ä¸ºäº†ç¡®ä¿è§†è§‰ä¸€è‡´æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†é£æ ¼ä¼ æ’­é€‚é…å™¨ï¼Œè¯¥é€‚é…å™¨æ•æ‰è§’è‰²çš„è§†è§‰é£æ ¼å¹¶å°†å…¶åº”ç”¨äºèƒŒæ™¯ï¼Œåœ¨åˆæˆé£æ ¼ä¸€è‡´çš„åœºæ™¯æ—¶å¿ å®ä¿ç•™è§’è‰²çš„å®Œæ•´è§†è§‰èº«ä»½ã€‚é•œå¤´è®¾è®¡æ¨¡å—é€šè¿‡å¸§è£å‰ªå’Œå¤šè§†å›¾åˆæˆè¿›ä¸€æ­¥å¢å¼ºäº†è§†è§‰å¤šæ ·æ€§å’Œç”µå½±è´¨é‡ï¼ŒåŸºäºæ•…äº‹æ¿è¿›è¡Œã€‚ä¸ºäº†åŠ¨ç”»æ•…äº‹ï¼Œæˆ‘ä»¬é‡å»ºäº†è§’è‰²çš„3Dä»£ç†ï¼Œä»¥å¯¼å‡ºç‰©ç†ä¸Šå¯è¡Œçš„è¿åŠ¨åºåˆ—ï¼Œç„¶åä½¿ç”¨è¿™äº›åºåˆ—æ¥å¾®è°ƒåŸºäºMMDiTçš„å›¾åƒåˆ°è§†é¢‘çš„æ‰©æ•£æ¨¡å‹ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ä¸ªä¸¤é˜¶æ®µè¿åŠ¨å®šåˆ¶é€‚é…å™¨ï¼šç¬¬ä¸€é˜¶æ®µä»æ—¶é—´æ— åºçš„å¸§ä¸­å­¦ä¹ å¤–è§‚ç‰¹å¾ï¼Œå°†èº«ä»½ä¸è¿åŠ¨åˆ†å¼€ï¼›ç¬¬äºŒé˜¶æ®µä½¿ç”¨æ—¶é—´æ­¥é•¿ç­–ç•¥å¯¹èº«ä»½æƒé‡è¿›è¡Œå†»ç»“ï¼Œå¯¹ä¸´æ—¶åŠ¨æ€è¿›è¡Œå»ºæ¨¡ã€‚ä¸€æ—¦è®­ç»ƒå®Œæˆï¼ŒFairyGenå°†ç›´æ¥ä¸æ•…äº‹æ¿å¯¹é½ï¼Œå‘ˆç°å¤šæ ·åŒ–å’Œè¿è´¯çš„è§†é¢‘åœºæ™¯ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿäº§ç”Ÿçš„åŠ¨ç”»åœ¨é£æ ¼ä¸Šå¿ å®ã€å™äº‹ç»“æ„è‡ªç„¶ã€è¿åŠ¨è¿è´¯ï¼Œçªæ˜¾å…¶åœ¨ä¸ªæ€§åŒ–ã€å¼•äººå…¥èƒœçš„æ•…äº‹åŠ¨ç”»ä¸­çš„æ½œåŠ›ã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/GVCLab/FairyGen%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/GVCLab/FairyGenä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.21272v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://jayleejia.github.io/FairyGen/">https://jayleejia.github.io/FairyGen/</a> ; Code:   <a target="_blank" rel="noopener" href="https://github.com/GVCLab/FairyGen">https://github.com/GVCLab/FairyGen</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†FairyGenç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿæ ¹æ®å„¿ç«¥å•å¹…ç”»ä½œè‡ªåŠ¨ç”Ÿæˆæ•…äº‹é©±åŠ¨åŠ¨ç”»å¡é€šè§†é¢‘ã€‚ç³»ç»Ÿé€šè¿‡åˆ†ç¦»è§’è‰²å»ºæ¨¡å’ŒèƒŒæ™¯ç”Ÿæˆï¼Œèå…¥ç”µå½±æ‹æ‘„è®¾è®¡ï¼Œå®ç°å…·æœ‰è¡¨ç°åŠ›å’Œè¿è´¯æ€§çš„å™äº‹ã€‚ç»™å®šä¸€ä¸ªè§’è‰²è‰å›¾ï¼Œç³»ç»Ÿåˆ©ç”¨MLLMç”Ÿæˆç»“æ„åŒ–æ•…äº‹æ¿ï¼Œå¹¶é€šè¿‡é£æ ¼ä¼ æ’­é€‚é…å™¨ç¡®ä¿è§†è§‰ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œç³»ç»Ÿè¿˜å…·å¤‡æ‹æ‘„è®¾è®¡æ¨¡å—ï¼Œé€šè¿‡ç”»é¢è£å‰ªå’Œå¤šè§†è§’åˆæˆå¢å¼ºè§†è§‰å¤šæ ·æ€§å’Œç”µå½±è´¨é‡ã€‚é€šè¿‡é‡å»ºè§’è‰²3Dä»£ç†ï¼Œè¡ç”Ÿå‡ºç‰©ç†é€¼çœŸçš„è¿åŠ¨åºåˆ—ï¼Œå†åˆ©ç”¨MMDiTå›¾åƒåˆ°è§†é¢‘çš„æ‰©æ•£æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚è¿åŠ¨å®šåˆ¶é€‚é…å™¨åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼Œç¬¬ä¸€é˜¶æ®µå­¦ä¹ ä»æ—¶åºæ— åºå¸§ä¸­æå–å¤–è§‚ç‰¹å¾ï¼Œå°†èº«ä»½ä¸è¿åŠ¨åˆ†ç¦»ï¼›ç¬¬äºŒé˜¶æ®µé‡‡ç”¨æ—¶é—´æ­¥é•¿ç­–ç•¥å»ºæ¨¡æ—¶åºåŠ¨æ€ï¼Œå†»ç»“èº«ä»½æƒé‡ã€‚å®éªŒè¯æ˜ï¼ŒFairyGenç³»ç»Ÿèƒ½å¤Ÿç”Ÿæˆé£æ ¼å¿ å®ã€å™äº‹è¿è´¯ã€è‡ªç„¶è¿åŠ¨çš„åŠ¨ç”»ï¼Œå±•ç°å‡ºå…¶åœ¨ä¸ªæ€§åŒ–ã€å¼•äººå…¥èƒœçš„æ•…äº‹åŠ¨ç”»é¢†åŸŸçš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FairyGenç³»ç»Ÿèƒ½å¤Ÿæ ¹æ®å„¿ç«¥å•å¹…ç”»ä½œè‡ªåŠ¨ç”Ÿæˆæ•…äº‹é©±åŠ¨åŠ¨ç”»å¡é€šè§†é¢‘ã€‚</li>
<li>ç³»ç»Ÿé€šè¿‡åˆ†ç¦»è§’è‰²å»ºæ¨¡å’ŒèƒŒæ™¯ç”Ÿæˆï¼Œå¹¶èå…¥ç”µå½±æ‹æ‘„è®¾è®¡æ¥æ”¯æŒè¡¨è¾¾å’Œè¿è´¯çš„å™äº‹ã€‚</li>
<li>ä½¿ç”¨MLLMç”Ÿæˆç»“æ„åŒ–æ•…äº‹æ¿ï¼ŒåŒ…æ‹¬åœºæ™¯è®¾ç½®ã€è§’è‰²åŠ¨ä½œå’Œæ‘„å½±è§’åº¦æè¿°ã€‚</li>
<li>é€šè¿‡é£æ ¼ä¼ æ’­é€‚é…å™¨ç¡®ä¿è§†è§‰ä¸€è‡´æ€§ï¼ŒåŒæ—¶åˆæˆé£æ ¼ä¸€è‡´çš„åœºæ™¯ã€‚</li>
<li>æ‹æ‘„è®¾è®¡æ¨¡å—å¢å¼ºäº†è§†è§‰å¤šæ ·æ€§å’Œç”µå½±è´¨é‡ï¼Œé€šè¿‡ç”»é¢è£å‰ªå’Œå¤šè§†è§’åˆæˆå®ç°ã€‚</li>
<li>ç³»ç»Ÿé€šè¿‡é‡å»ºè§’è‰²3Dä»£ç†å’Œç‰©ç†é€¼çœŸçš„è¿åŠ¨åºåˆ—ï¼Œç»“åˆMMDiTå›¾åƒåˆ°è§†é¢‘çš„æ‰©æ•£æ¨¡å‹è¿›è¡ŒåŠ¨ç”»åˆ›ä½œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.21272">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e79a582a22c9ba10628a24dfcc6313e4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19794dced782a89e75c14e5c582023d5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-db72d428af9c21e97e4846ffa27a9647.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b5308a0b32f4c62c46f04dd8654db0ca.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5e830db6f9791f66a904cfcf69ecaced.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ca22e2357e06b1bfb64a84070f553d37.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Learning-to-See-in-the-Extremely-Dark"><a href="#Learning-to-See-in-the-Extremely-Dark" class="headerlink" title="Learning to See in the Extremely Dark"></a>Learning to See in the Extremely Dark</h2><p><strong>Authors:Hai Jiang, Binhao Guan, Zhen Liu, Xiaohong Liu, Jian Yu, Zheng Liu, Songchen Han, Shuaicheng Liu</strong></p>
<p>Learning-based methods have made promising advances in low-light RAW image enhancement, while their capability to extremely dark scenes where the environmental illuminance drops as low as 0.0001 lux remains to be explored due to the lack of corresponding datasets. To this end, we propose a paired-to-paired data synthesis pipeline capable of generating well-calibrated extremely low-light RAW images at three precise illuminance ranges of 0.01-0.1 lux, 0.001-0.01 lux, and 0.0001-0.001 lux, together with high-quality sRGB references to comprise a large-scale paired dataset named See-in-the-Extremely-Dark (SIED) to benchmark low-light RAW image enhancement approaches. Furthermore, we propose a diffusion-based framework that leverages the generative ability and intrinsic denoising property of diffusion models to restore visually pleasing results from extremely low-SNR RAW inputs, in which an Adaptive Illumination Correction Module (AICM) and a color consistency loss are introduced to ensure accurate exposure correction and color restoration. Extensive experiments on the proposed SIED and publicly available benchmarks demonstrate the effectiveness of our method. The code and dataset are available at <a target="_blank" rel="noopener" href="https://github.com/JianghaiSCU/SIED">https://github.com/JianghaiSCU/SIED</a>. </p>
<blockquote>
<p>åŸºäºå­¦ä¹ çš„æ–¹æ³•åœ¨ä½å…‰RAWå›¾åƒå¢å¼ºæ–¹é¢å–å¾—äº†æœ‰å‰æ™¯çš„è¿›å±•ï¼Œä½†ç”±äºç¼ºä¹ç›¸åº”çš„æ•°æ®é›†ï¼Œå…¶åœ¨ç¯å¢ƒç…§åº¦é™è‡³0.0001å‹’å…‹æ–¯çš„ææš—åœºæ™¯ä¸‹çš„èƒ½åŠ›å°šå¾…æ¢ç´¢ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é…å¯¹åˆ°é…å¯¹çš„æ•°æ®åˆæˆç®¡é“ï¼Œèƒ½å¤Ÿç”Ÿæˆæ ¡å‡†è‰¯å¥½çš„æä½å…‰RAWå›¾åƒï¼Œåœ¨0.01-0.1å‹’å…‹æ–¯ã€0.001-0.01å‹’å…‹æ–¯å’Œ0.0001-0.001å‹’å…‹æ–¯çš„ä¸‰ä¸ªç²¾ç¡®ç…§åº¦èŒƒå›´å†…ï¼Œä»¥åŠé«˜è´¨é‡sRGBå‚è€ƒï¼Œæ„æˆå¤§è§„æ¨¡é…å¯¹æ•°æ®é›†ï¼Œåä¸ºSee-in-the-Extremely-Darkï¼ˆSIEDï¼‰ï¼Œä»¥è¯„ä¼°ä½å…‰RAWå›¾åƒå¢å¼ºæ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŸºäºæ‰©æ•£çš„æ¡†æ¶ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›å’Œå†…åœ¨å»å™ªå±æ€§ï¼Œä»æä½çš„SNR RAWè¾“å…¥ä¸­æ¢å¤å‡ºè§†è§‰ä¸Šçš„æ„‰æ‚¦ç»“æœï¼Œå…¶ä¸­å¼•å…¥äº†è‡ªé€‚åº”ç…§æ˜æ ¡æ­£æ¨¡å—ï¼ˆAICMï¼‰å’Œé¢œè‰²ä¸€è‡´æ€§æŸå¤±ï¼Œä»¥ç¡®ä¿å‡†ç¡®çš„æ›å…‰æ ¡æ­£å’Œé¢œè‰²æ¢å¤ã€‚åœ¨æå‡ºçš„SIEDå’Œå…¬å¼€å¯ç”¨åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ä»£ç å’Œæ•°æ®é›†å¯åœ¨[<a target="_blank" rel="noopener" href="https://github.com/JianghaiSCU/SIED%E6%89%BE%E5%88%B0%E3%80%82]">https://github.com/JianghaiSCU/SIEDæ‰¾åˆ°ã€‚]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.21132v1">PDF</a> Accepted by ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬æè¿°äº†åœ¨æä½å…‰ç…§æ¡ä»¶ä¸‹RAWå›¾åƒå¢å¼ºçš„å­¦ä¹ æ–¹æ³•çš„è¿›å±•å’Œé¢ä¸´çš„æŒ‘æˆ˜ã€‚ä¸ºè§£å†³ç¼ºä¹ç›¸åº”æ•°æ®é›†çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ•°æ®åˆæˆç®¡é“ï¼Œèƒ½å¤Ÿç”Ÿæˆç²¾ç¡®æ ¡å‡†çš„æä½å…‰ç…§RAWå›¾åƒä»¥åŠä¸é«˜è´¨é‡sRGBå‚è€ƒå›¾åƒé…å¯¹çš„å¤§è§„æ¨¡æ•°æ®é›†â€œææš—ä¸­çš„è§†è§‰â€ï¼ˆSee-in-the-Extremely-Darkï¼Œç®€ç§° SIEDï¼‰ã€‚æ­¤å¤–ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£çš„æ¡†æ¶ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›å’Œå†…åœ¨å»å™ªç‰¹æ€§æ¥æ¢å¤æä½ä¿¡å™ªæ¯”RAWè¾“å…¥çš„å¯è§†ç»“æœã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æå‡ºçš„ SIED å’Œå…¬å¼€åŸºå‡†æµ‹è¯•ä¸Šéƒ½è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚æ•°æ®é›†å’Œä»£ç å¯åœ¨XXXç½‘ç«™ä¸Šæ‰¾åˆ°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å­¦ä¹ æ–¹æ³•åœ¨ä½å…‰ç…§RAWå›¾åƒå¢å¼ºæ–¹é¢å·²å±•ç°å‡ºå‰æ™¯ï¼Œä½†åœ¨æä½å…‰ç…§åœºæ™¯ï¼ˆç¯å¢ƒç…§åº¦ä½è‡³0.0001 luxï¼‰çš„åº”ç”¨ä»éœ€æ¢ç´¢ã€‚</li>
<li>ç¼ºä¹ç›¸åº”çš„æ•°æ®é›†æ˜¯è¿™ä¸€é¢†åŸŸé¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜ä¹‹ä¸€ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ•°æ®åˆæˆç®¡é“ï¼Œèƒ½å¤Ÿç”Ÿæˆç²¾ç¡®æ ¡å‡†çš„æä½å…‰ç…§RAWå›¾åƒï¼Œå¹¶ä¸é«˜è´¨é‡sRGBå‚è€ƒå›¾åƒé…å¯¹ï¼Œæ„æˆå¤§è§„æ¨¡æ•°æ®é›†SIEDã€‚</li>
<li>ä»‹ç»äº†ä¸€ç§åŸºäºæ‰©æ•£çš„æ¡†æ¶ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›å’Œå»å™ªç‰¹æ€§æ¥å¢å¼ºæä½ä¿¡å™ªæ¯”çš„RAWå›¾åƒã€‚</li>
<li>æå‡ºçš„æ¡†æ¶ä¸­åŒ…å«è‡ªé€‚åº”ç…§æ˜æ ¡æ­£æ¨¡å—ï¼ˆAICMï¼‰å’Œè‰²å½©ä¸€è‡´æ€§æŸå¤±ï¼Œç¡®ä¿å‡†ç¡®çš„æ›å…‰æ ¡æ­£å’Œè‰²å½©æ¢å¤ã€‚</li>
<li>åœ¨æå‡ºçš„SIEDæ•°æ®é›†å’Œå…¬å¼€åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.21132">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-87b946546f18a6775a7ff775a08ae6bc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-78ec070b7d01b0d93358f63e88e6ad01.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca723fb8f5b41e762e7ca0cf9dcc98f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a07ccc703dbf1582320100addd1af26.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8f6b127c4f7b703ebe304543adbc2a61.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b726532a2b564ac30ae7740b18882c06.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Boosting-Domain-Generalized-and-Adaptive-Detection-with-Diffusion-Models-Fitness-Generalization-and-Transferability"><a href="#Boosting-Domain-Generalized-and-Adaptive-Detection-with-Diffusion-Models-Fitness-Generalization-and-Transferability" class="headerlink" title="Boosting Domain Generalized and Adaptive Detection with Diffusion   Models: Fitness, Generalization, and Transferability"></a>Boosting Domain Generalized and Adaptive Detection with Diffusion   Models: Fitness, Generalization, and Transferability</h2><p><strong>Authors:Boyong He, Yuxiang Ji, Zhuoyue Tan, Liaoni Wu</strong></p>
<p>Detectors often suffer from performance drop due to domain gap between training and testing data. Recent methods explore diffusion models applied to domain generalization (DG) and adaptation (DA) tasks, but still struggle with large inference costs and have not yet fully leveraged the capabilities of diffusion models. We propose to tackle these problems by extracting intermediate features from a single-step diffusion process, improving feature collection and fusion to reduce inference time by 75% while enhancing performance on source domains (i.e., Fitness). Then, we construct an object-centered auxiliary branch by applying box-masked images with class prompts to extract robust and domain-invariant features that focus on object. We also apply consistency loss to align the auxiliary and ordinary branch, balancing fitness and generalization while preventing overfitting and improving performance on target domains (i.e., Generalization). Furthermore, within a unified framework, standard detectors are guided by diffusion detectors through feature-level and object-level alignment on source domains (for DG) and unlabeled target domains (for DA), thereby improving cross-domain detection performance (i.e., Transferability). Our method achieves competitive results on 3 DA benchmarks and 5 DG benchmarks. Additionally, experiments on COCO generalization benchmark demonstrate that our method maintains significant advantages and show remarkable efficiency in large domain shifts and low-data scenarios. Our work shows the superiority of applying diffusion models to domain generalized and adaptive detection tasks and offers valuable insights for visual perception tasks across diverse domains. The code is available at \href{<a target="_blank" rel="noopener" href="https://github.com/heboyong/Fitness-Generalization-Transferability%7D%7BFitness-Generalization-Transferability%7D">https://github.com/heboyong/Fitness-Generalization-Transferability}{Fitness-Generalization-Transferability}</a>. </p>
<blockquote>
<p>æ£€æµ‹å™¨å¸¸å¸¸å› ä¸ºè®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®ä¹‹é—´çš„åŸŸå·®è·è€Œæ€§èƒ½ä¸‹é™ã€‚è™½ç„¶æœ€è¿‘çš„æ–¹æ³•å°è¯•å°†æ‰©æ•£æ¨¡å‹åº”ç”¨äºåŸŸæ³›åŒ–ï¼ˆDGï¼‰å’ŒåŸŸé€‚åº”ï¼ˆDAï¼‰ä»»åŠ¡ï¼Œä½†ä»ç„¶é¢ä¸´è¾ƒå¤§çš„æ¨ç†æˆæœ¬ï¼Œå¹¶ä¸”å°šæœªå……åˆ†åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„èƒ½åŠ›ã€‚æˆ‘ä»¬æè®®é€šè¿‡ä»å•æ­¥æ‰©æ•£è¿‡ç¨‹ä¸­æå–ä¸­é—´ç‰¹å¾æ¥è§£å†³è¿™äº›é—®é¢˜ï¼Œæ”¹è¿›ç‰¹å¾æ”¶é›†å’Œèåˆï¼Œå‡å°‘75%çš„æ¨ç†æ—¶é—´ï¼ŒåŒæ—¶æé«˜åœ¨æºåŸŸï¼ˆä¾‹å¦‚ï¼ŒFitnessï¼‰ä¸Šçš„æ€§èƒ½ã€‚æ¥ç€ï¼Œæˆ‘ä»¬é€šè¿‡åº”ç”¨å¸¦æœ‰ç±»åˆ«æç¤ºçš„æ¡†æ©å›¾åƒï¼Œæ„å»ºä¸€ä¸ªä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„è¾…åŠ©åˆ†æ”¯ï¼Œä»¥æå–ç¨³å¥ä¸”åŸŸä¸å˜çš„ç‰¹å¾ï¼Œè¿™äº›ç‰¹å¾ä¸“æ³¨äºå¯¹è±¡ã€‚æˆ‘ä»¬è¿˜åº”ç”¨ä¸€è‡´æ€§æŸå¤±æ¥å¯¹é½è¾…åŠ©åˆ†æ”¯å’Œæ™®é€šåˆ†æ”¯ï¼Œåœ¨é€‚åº”æ€§å’Œæ³›åŒ–æ€§ä¹‹é—´å–å¾—å¹³è¡¡ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œå¹¶åœ¨ç›®æ ‡åŸŸï¼ˆå³ï¼Œæ³›åŒ–ï¼‰ä¸Šæé«˜æ€§èƒ½ã€‚æ­¤å¤–ï¼Œåœ¨ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶å†…ï¼Œæ ‡å‡†æ£€æµ‹å™¨é€šè¿‡æºåŸŸï¼ˆç”¨äºDGï¼‰å’Œæ— æ ‡ç­¾ç›®æ ‡åŸŸï¼ˆç”¨äºDAï¼‰çš„ç‰¹å¾çº§å’Œå¯¹è±¡çº§å¯¹é½ï¼Œç”±æ‰©æ•£æ£€æµ‹å™¨è¿›è¡Œå¼•å¯¼ï¼Œä»è€Œæé«˜è·¨åŸŸæ£€æµ‹æ€§èƒ½ï¼ˆå³ï¼Œå¯è½¬ç§»æ€§ï¼‰ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨3ä¸ªDAåŸºå‡†æµ‹è¯•å’Œ5ä¸ªDGåŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚æ­¤å¤–ï¼Œåœ¨COCOæ³›åŒ–åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¿æŒäº†æ˜¾è‘—ä¼˜åŠ¿ï¼Œå¹¶åœ¨å¤§åŸŸè¿ç§»å’Œä½æ•°æ®åœºæ™¯ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ•ˆç‡ã€‚æˆ‘ä»¬çš„å·¥ä½œå±•ç¤ºäº†å°†æ‰©æ•£æ¨¡å‹åº”ç”¨äºåŸŸæ³›åŒ–å’Œè‡ªé€‚åº”æ£€æµ‹ä»»åŠ¡çš„ä¼˜è¶Šæ€§ï¼Œå¹¶ä¸ºè·¨ä¸åŒåŸŸçš„è§†è§‰æ„ŸçŸ¥ä»»åŠ¡æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚ä»£ç å¯ç”¨åœ¨<a target="_blank" rel="noopener" href="https://github.com/heboyong/Fitness-Generalization-Transferability">Fitness-Generalization-Transferability</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.21042v1">PDF</a> Accepted by ICCV2025. arXiv admin note: text overlap with   arXiv:2503.02101</p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹åœ¨é¢†åŸŸæ³›åŒ–ï¼ˆDGï¼‰å’Œé¢†åŸŸé€‚åº”ï¼ˆDAï¼‰ä»»åŠ¡ä¸­å±•ç°å‡ºæ½œåŠ›ï¼Œä½†é¢ä¸´æ¨ç†æˆæœ¬é«˜å’Œæœªå……åˆ†åˆ©ç”¨æ‰©æ•£æ¨¡å‹èƒ½åŠ›çš„é—®é¢˜ã€‚æœ¬ç ”ç©¶é€šè¿‡æå–å•æ­¥æ‰©æ•£è¿‡ç¨‹çš„ä¸­é—´ç‰¹å¾ï¼Œæ”¹è¿›ç‰¹å¾é‡‡é›†å’Œèåˆï¼Œå‡å°‘æ¨ç†æ—¶é—´å¹¶æé«˜æºåŸŸæ€§èƒ½ã€‚æ„å»ºé¢å‘å¯¹è±¡çš„è¾…åŠ©åˆ†æ”¯ï¼Œé€šè¿‡åº”ç”¨å¸¦æœ‰ç±»åˆ«æç¤ºçš„ç›’æ©å›¾åƒæå–ç¨³å¥ä¸”é¢†åŸŸä¸å˜çš„ç‰¹å¾ã€‚åŒæ—¶åº”ç”¨ä¸€è‡´æ€§æŸå¤±æ¥å¹³è¡¡é€‚åº”æ€§å’Œæ³›åŒ–æ€§ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆå¹¶æé«˜ç›®æ ‡åŸŸæ€§èƒ½ã€‚åœ¨ç»Ÿä¸€æ¡†æ¶ä¸‹ï¼Œæ ‡å‡†æ£€æµ‹å™¨é€šè¿‡æºåŸŸå’Œç›®æ ‡åŸŸçš„ç‰¹å¾å’Œå¯¹è±¡çº§å¯¹é½ï¼Œç”±æ‰©æ•£æ£€æµ‹å™¨å¼•å¯¼ï¼Œæé«˜è·¨åŸŸæ£€æµ‹æ€§èƒ½ã€‚æœ¬ç ”ç©¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¹¶åœ¨å¤§é¢†åŸŸåç§»å’Œå°‘æ•°æ®åœºæ™¯ä¸‹ä¿æŒæ˜¾è‘—ä¼˜åŠ¿ã€‚ç ”ç©¶å±•ç¤ºäº†æ‰©æ•£æ¨¡å‹åœ¨é¢†åŸŸæ³›åŒ–å’Œè‡ªé€‚åº”æ£€æµ‹ä»»åŠ¡ä¸­çš„ä¼˜è¶Šæ€§ï¼Œä¸ºè·¨åŸŸè§†è§‰æ„ŸçŸ¥ä»»åŠ¡æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åº”ç”¨äºé¢†åŸŸæ³›åŒ–å’Œé¢†åŸŸé€‚åº”ä»»åŠ¡ä¸­é¢ä¸´æ¨ç†æˆæœ¬é«˜çš„é—®é¢˜ã€‚</li>
<li>é€šè¿‡æå–å•æ­¥æ‰©æ•£è¿‡ç¨‹çš„ä¸­é—´ç‰¹å¾ï¼Œæ”¹è¿›ç‰¹å¾é‡‡é›†å’Œèåˆï¼Œå‡å°‘æ¨ç†æ—¶é—´å¹¶æé«˜æºåŸŸæ€§èƒ½ã€‚</li>
<li>æ„å»ºé¢å‘å¯¹è±¡çš„è¾…åŠ©åˆ†æ”¯ä»¥æå–ç¨³å¥å’Œé¢†åŸŸä¸å˜çš„ç‰¹å¾ã€‚</li>
<li>åº”ç”¨ä¸€è‡´æ€§æŸå¤±æ¥å¹³è¡¡é€‚åº”æ€§å’Œæ³›åŒ–æ€§ï¼Œæé«˜ç›®æ ‡åŸŸæ€§èƒ½ã€‚</li>
<li>åœ¨ç»Ÿä¸€æ¡†æ¶ä¸‹ï¼Œæ ‡å‡†æ£€æµ‹å™¨é€šè¿‡ç‰¹å¾çº§å’Œå¯¹è±¡çº§å¯¹é½ç”±æ‰©æ•£æ£€æµ‹å™¨å¼•å¯¼ï¼Œæé«˜è·¨åŸŸæ£€æµ‹æ€§èƒ½ã€‚</li>
<li>ç ”ç©¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå±•ç¤ºäº†æ‰©æ•£æ¨¡å‹åœ¨é¢†åŸŸæ³›åŒ–å’Œè‡ªé€‚åº”æ£€æµ‹ä»»åŠ¡çš„ä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.21042">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e7c8ce353e9468aa38ee6ccf00eb70a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ad5fd13c34d21d03acb2c11c4591478.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a86d4aa636f58eb3b9d2b90ec74477c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-984ee22834ea0e6e25b697abfe33bee2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3058b6f1da55aff2ca86b6001beaf094.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-922c2533a842f182a2a6eba2f63cef52.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e85fac9466ac87fc45b540facd8e77be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0111fd4d5df9e8d1209c2af8371780ad.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="DidSee-Diffusion-Based-Depth-Completion-for-Material-Agnostic-Robotic-Perception-and-Manipulation"><a href="#DidSee-Diffusion-Based-Depth-Completion-for-Material-Agnostic-Robotic-Perception-and-Manipulation" class="headerlink" title="DidSee: Diffusion-Based Depth Completion for Material-Agnostic Robotic   Perception and Manipulation"></a>DidSee: Diffusion-Based Depth Completion for Material-Agnostic Robotic   Perception and Manipulation</h2><p><strong>Authors:Wenzhou Lyu, Jialing Lin, Wenqi Ren, Ruihao Xia, Feng Qian, Yang Tang</strong></p>
<p>Commercial RGB-D cameras often produce noisy, incomplete depth maps for non-Lambertian objects. Traditional depth completion methods struggle to generalize due to the limited diversity and scale of training data. Recent advances exploit visual priors from pre-trained text-to-image diffusion models to enhance generalization in dense prediction tasks. However, we find that biases arising from training-inference mismatches in the vanilla diffusion framework significantly impair depth completion performance. Additionally, the lack of distinct visual features in non-Lambertian regions further hinders precise prediction. To address these issues, we propose \textbf{DidSee}, a diffusion-based framework for depth completion on non-Lambertian objects. First, we integrate a rescaled noise scheduler enforcing a zero terminal signal-to-noise ratio to eliminate signal leakage bias. Second, we devise a noise-agnostic single-step training formulation to alleviate error accumulation caused by exposure bias and optimize the model with a task-specific loss. Finally, we incorporate a semantic enhancer that enables joint depth completion and semantic segmentation, distinguishing objects from backgrounds and yielding precise, fine-grained depth maps. DidSee achieves state-of-the-art performance on multiple benchmarks, demonstrates robust real-world generalization, and effectively improves downstream tasks such as category-level pose estimation and robotic grasping.Project page: <a target="_blank" rel="noopener" href="https://wenzhoulyu.github.io/DidSee/">https://wenzhoulyu.github.io/DidSee/</a> </p>
<blockquote>
<p>å•†ä¸šRGB-Dç›¸æœºä¸ºéæœ—ä¼¯ä½“å¯¹è±¡ç”Ÿæˆçš„æ·±åº¦å›¾å¸¸å¸¸å¸¦æœ‰å™ªå£°ä¸”ä¸å®Œæ•´ã€‚ç”±äºè®­ç»ƒæ•°æ®æœ‰é™å¤šæ ·æ€§å’Œè§„æ¨¡ï¼Œä¼ ç»Ÿæ·±åº¦å®Œæˆæ–¹æ³•å¾ˆéš¾å®ç°æ³›åŒ–ã€‚æœ€è¿‘çš„è¿›å±•åˆ©ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„è§†è§‰å…ˆéªŒçŸ¥è¯†ï¼Œä»¥å¢å¼ºå¯†é›†é¢„æµ‹ä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å‘ç°ä»åŸå§‹æ‰©æ•£æ¡†æ¶ä¸­çš„è®­ç»ƒæ¨ç†ä¸åŒ¹é…æ‰€äº§ç”Ÿçš„åè§ä¸¥é‡æŸå®³äº†æ·±åº¦å®Œæˆæ€§èƒ½ã€‚æ­¤å¤–ï¼Œéæœ—ä¼¯ä½“åŒºåŸŸç¼ºä¹ç‹¬ç‰¹çš„è§†è§‰ç‰¹å¾ä¹Ÿé˜»ç¢äº†ç²¾ç¡®é¢„æµ‹ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŸºäºæ‰©æ•£çš„æ·±åº¦å®Œæˆæ¡†æ¶â€œDidSeeâ€ï¼Œé€‚ç”¨äºéæœ—ä¼¯ä½“å¯¹è±¡ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æ•´åˆäº†ä¸€ä¸ªé‡æ–°æ ‡å®šçš„å™ªå£°è°ƒåº¦å™¨ï¼Œå¼ºåˆ¶ç»ˆç«¯ä¿¡å·ä¸å™ªå£°æ¯”ä¸ºé›¶ï¼Œä»¥æ¶ˆé™¤ä¿¡å·æ³„æ¼åè§ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§å™ªå£°æ— å…³çš„å•æ­¥è®­ç»ƒå…¬å¼ï¼Œä»¥å‡å°‘æ›å…‰åè§å¼•èµ·çš„è¯¯å·®ç´¯ç§¯ï¼Œå¹¶ç”¨ç‰¹å®šä»»åŠ¡çš„æŸå¤±æ¥ä¼˜åŒ–æ¨¡å‹ã€‚æœ€åï¼Œæˆ‘ä»¬åŠ å…¥äº†ä¸€ä¸ªè¯­ä¹‰å¢å¼ºå™¨ï¼Œèƒ½å¤Ÿå®ç°è”åˆæ·±åº¦å®Œæˆå’Œè¯­ä¹‰åˆ†å‰²ï¼ŒåŒºåˆ†ç‰©ä½“ä¸èƒŒæ™¯ï¼Œç”Ÿæˆç²¾ç¡®ã€ç²¾ç»†çš„æ·±åº¦å›¾ã€‚DidSeeåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†é¢†å…ˆæ°´å¹³ï¼Œå±•ç°äº†ç¨³å¥çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶èƒ½æœ‰æ•ˆæé«˜ä¸‹æ¸¸ä»»åŠ¡å¦‚ç±»åˆ«çº§å§¿æ€ä¼°è®¡å’Œæœºå™¨äººæŠ“å–ç­‰ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://wenzhoulyu.github.io/DidSee/">https://wenzhoulyu.github.io/DidSee/</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.21034v1">PDF</a> </p>
<p><strong>Summary</strong><br>     é’ˆå¯¹éæœ—ä¼¯ä½“æ·±åº¦å®Œæˆä»»åŠ¡ï¼Œç°æœ‰å•†ä¸šRGB-Dç›¸æœºäº§ç”Ÿçš„æ·±åº¦å›¾å­˜åœ¨å™ªå£°å’Œä¸å®Œæ•´é—®é¢˜ï¼Œä¼ ç»Ÿæ–¹æ³•å› è®­ç»ƒæ•°æ®æœ‰é™è€Œéš¾ä»¥æ¨å¹¿ã€‚æœ€æ–°ç ”ç©¶åˆ©ç”¨é¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹æé«˜å¯†é›†é¢„æµ‹ä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½†ä»å­˜åœ¨è®­ç»ƒä¸æ¨ç†ä¸åŒ¹é…å¯¼è‡´çš„åå·®åŠéæœ—ä¼¯ä½“åŒºåŸŸç¼ºä¹ç‰¹å¾å½±å“ç²¾ç¡®é¢„æµ‹çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæå‡ºåŸºäºæ‰©æ•£æ¨¡å‹çš„æ·±åº¦å®Œæˆæ¡†æ¶DidSeeï¼Œé€šè¿‡è°ƒæ•´å™ªå£°è°ƒåº¦å™¨ã€å•æ­¥è®­ç»ƒå…¬å¼å’Œè¯­ä¹‰å¢å¼ºå™¨ç­‰æŠ€æœ¯ï¼Œå®ç°ç²¾ç¡®ã€ç»†ç²’åº¦çš„æ·±åº¦å›¾ç”Ÿæˆï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€ä½³æ€§èƒ½ï¼Œæœ‰æ•ˆæ¨å¹¿è‡³å®é™…åœºæ™¯å¹¶æå‡ä¸‹æ¸¸ä»»åŠ¡å¦‚ç±»åˆ«çº§åˆ«å§¿æ€ä¼°è®¡å’Œæœºå™¨äººæŠ“å–ç­‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å•†ä¸šRGB-Dç›¸æœºåœ¨éæœ—ä¼¯ä½“å¯¹è±¡ä¸Šäº§ç”Ÿå™ªå£°å’Œä¸å®Œæ•´çš„æ·±åº¦å›¾ã€‚</li>
<li>ä¼ ç»Ÿæ·±åº¦å®Œæˆæ–¹æ³•ç”±äºè®­ç»ƒæ•°æ®æœ‰é™å’Œå¤šæ ·æ€§ä¸è¶³ï¼Œéš¾ä»¥æ¨å¹¿ã€‚</li>
<li>æœ€æ–°ç ”ç©¶åˆ©ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹æé«˜æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>å­˜åœ¨è®­ç»ƒä¸æ¨ç†ä¸åŒ¹é…å¯¼è‡´çš„åå·®å½±å“æ·±åº¦å®Œæˆæ€§èƒ½ã€‚</li>
<li>éæœ—ä¼¯ä½“åŒºåŸŸç¼ºä¹ç‰¹å¾è¿›ä¸€æ­¥é˜»ç¢ç²¾ç¡®é¢„æµ‹ã€‚</li>
<li>æå‡ºçš„DidSeeæ¡†æ¶é€šè¿‡è°ƒæ•´å™ªå£°è°ƒåº¦å™¨ã€å•æ­¥è®­ç»ƒå…¬å¼å’Œè¯­ä¹‰å¢å¼ºå™¨ç­‰æŠ€æœ¯è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>DidSeeæ¡†æ¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€ä½³æ€§èƒ½ï¼Œæœ‰æ•ˆæ¨å¹¿è‡³å®é™…åœºæ™¯å¹¶æå‡ä¸‹æ¸¸ä»»åŠ¡æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.21034">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f7af7b9b4123b9dc3485e4b251b058ca.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-71abf97d1ef8119c3cda210ca936317d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a00faf0615ad43db013e1f103df355b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b34276a3871dfeb995d82ac1cb87b01.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0ad3308ee61ff8917c4804bb404f4934.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="From-Cradle-to-Cane-A-Two-Pass-Framework-for-High-Fidelity-Lifespan-Face-Aging"><a href="#From-Cradle-to-Cane-A-Two-Pass-Framework-for-High-Fidelity-Lifespan-Face-Aging" class="headerlink" title="From Cradle to Cane: A Two-Pass Framework for High-Fidelity Lifespan   Face Aging"></a>From Cradle to Cane: A Two-Pass Framework for High-Fidelity Lifespan   Face Aging</h2><p><strong>Authors:Tao Liu, Dafeng Zhang, Gengchen Li, Shizhuo Liu, Yongqi Song, Senmao Li, Shiqi Yang, Boqian Li, Kai Wang, Yaxing Wang</strong></p>
<p>Face aging has become a crucial task in computer vision, with applications ranging from entertainment to healthcare. However, existing methods struggle with achieving a realistic and seamless transformation across the entire lifespan, especially when handling large age gaps or extreme head poses. The core challenge lies in balancing age accuracy and identity preservationâ€“what we refer to as the Age-ID trade-off. Most prior methods either prioritize age transformation at the expense of identity consistency or vice versa. In this work, we address this issue by proposing a two-pass face aging framework, named Cradle2Cane, based on few-step text-to-image (T2I) diffusion models. The first pass focuses on solving age accuracy by introducing an adaptive noise injection (AdaNI) mechanism. This mechanism is guided by including prompt descriptions of age and gender for the given person as the textual condition. Also, by adjusting the noise level, we can control the strength of aging while allowing more flexibility in transforming the face. However, identity preservation is weakly ensured here to facilitate stronger age transformations. In the second pass, we enhance identity preservation while maintaining age-specific features by conditioning the model on two identity-aware embeddings (IDEmb): SVR-ArcFace and Rotate-CLIP. This pass allows for denoising the transformed image from the first pass, ensuring stronger identity preservation without compromising the aging accuracy. Both passes are jointly trained in an end-to-end way. Extensive experiments on the CelebA-HQ test dataset, evaluated through Face++ and Qwen-VL protocols, show that our Cradle2Cane outperforms existing face aging methods in age accuracy and identity consistency. </p>
<blockquote>
<p>é¢éƒ¨è¡°è€å·²æˆä¸ºè®¡ç®—æœºè§†è§‰ä¸­çš„ä¸€é¡¹é‡è¦ä»»åŠ¡ï¼Œå…¶åº”ç”¨ä»å¨±ä¹åˆ°åŒ»ç–—ä¿å¥éƒ½æœ‰æ¶‰åŠã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•åœ¨å®ç°æ•´ä¸ªç”Ÿå‘½å‘¨æœŸçš„çœŸå®æ— ç¼è½¬æ¢æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¤§å¹´é¾„å·®è·æˆ–æç«¯å¤´éƒ¨å§¿åŠ¿æ—¶ã€‚æ ¸å¿ƒæŒ‘æˆ˜åœ¨äºå¹³è¡¡å¹´é¾„å‡†ç¡®æ€§å’Œèº«ä»½ä¿ç•™ä¹‹é—´çš„æƒè¡¡ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºAge-IDæƒè¡¡ã€‚å¤§å¤šæ•°ä¹‹å‰çš„æ–¹æ³•è¦ä¹ˆä¼˜å…ˆè€ƒè™‘å¹´é¾„è½¬æ¢è€Œå¿½è§†èº«ä»½ä¸€è‡´æ€§ï¼Œè¦ä¹ˆåä¹‹ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡æå‡ºä¸€ä¸ªä¸¤é˜¶æ®µçš„é¢éƒ¨è¡°è€æ¡†æ¶æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè¯¥æ¡†æ¶åä¸ºCradle2Caneï¼ŒåŸºäºå°‘æ­¥éª¤çš„æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹ã€‚ç¬¬ä¸€é˜¶æ®µä¸“æ³¨äºè§£å†³å¹´é¾„å‡†ç¡®æ€§é—®é¢˜ï¼Œé€šè¿‡å¼•å…¥è‡ªé€‚åº”å™ªå£°æ³¨å…¥ï¼ˆAdaNIï¼‰æœºåˆ¶ã€‚è¯¥æœºåˆ¶é€šè¿‡åŒ…æ‹¬ç»™å®šäººçš„å¹´é¾„å’Œæ€§åˆ«çš„æç¤ºæè¿°ä½œä¸ºæ–‡æœ¬æ¡ä»¶æ¥å¼•å¯¼ã€‚æ­¤å¤–ï¼Œé€šè¿‡è°ƒæ•´å™ªå£°æ°´å¹³ï¼Œæˆ‘ä»¬å¯ä»¥æ§åˆ¶è¡°è€çš„å¼ºåº¦ï¼ŒåŒæ—¶å…è®¸é¢éƒ¨è½¬æ¢æ—¶æ›´å¤§çš„çµæ´»æ€§ã€‚ç„¶è€Œï¼Œè¿™é‡Œå¯¹èº«ä»½ä¿ç•™çš„ä¿è¯è¾ƒå¼±ï¼Œä»¥ä¿ƒè¿›æ›´å¼ºçš„å¹´é¾„è½¬æ¢ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæˆ‘ä»¬é€šè¿‡ä½¿æ¨¡å‹ä¾èµ–äºä¸¤ä¸ªèº«ä»½æ„ŸçŸ¥åµŒå…¥ï¼ˆIDEmbï¼‰ï¼šSVR-ArcFaceå’ŒRotate-CLIPï¼Œåœ¨ä¿æŒå¹´é¾„ç‰¹å¾çš„åŒæ—¶å¢å¼ºèº«ä»½ä¿ç•™ã€‚è¿™ä¸€å…³å…è®¸å¯¹ç¬¬ä¸€é˜¶æ®µçš„è½¬æ¢å›¾åƒè¿›è¡Œå»å™ªå¤„ç†ï¼Œç¡®ä¿åœ¨ä¿æŒå¹´é¾„å‡†ç¡®æ€§çš„åŒæ—¶å¢å¼ºèº«ä»½ä¿ç•™ã€‚ä¸¤ä¸ªé˜¶æ®µä»¥ç«¯åˆ°ç«¯çš„æ–¹å¼è¿›è¡Œè”åˆè®­ç»ƒã€‚åœ¨CelebA-HQæµ‹è¯•æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒï¼Œé€šè¿‡Face++å’ŒQwen-VLåè®®è¿›è¡Œè¯„ä¼°ï¼Œç»“æœè¡¨æ˜æˆ‘ä»¬çš„Cradle2Caneåœ¨å¹´é¾„å‡†ç¡®æ€§å’Œèº«ä»½ä¸€è‡´æ€§æ–¹é¢ä¼˜äºç°æœ‰çš„é¢éƒ¨è¡°è€æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20977v1">PDF</a> 30 pages, 12 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ‘˜è¦ä¸“æ³¨äºè®¡ç®—æœºè§†è§‰ä¸­çš„äººè„¸è¡°è€æŠ€æœ¯æŒ‘æˆ˜ã€‚è¯¥æ–‡æå‡ºä¸€ç§æ–°çš„è§£å†³ç­–ç•¥ï¼Œé‡‡ç”¨åŸºäºæ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹çš„åŒé˜¶æ®µæ¡†æ¶ï¼ˆCradle2Caneï¼‰ï¼Œæ—¨åœ¨è§£å†³äººè„¸è¡°è€è¿‡ç¨‹ä¸­çš„å¹´é¾„å‡†ç¡®æ€§å’Œèº«ä»½ä¸€è‡´æ€§ä¹‹é—´çš„æƒè¡¡é—®é¢˜ã€‚é€šè¿‡è‡ªé€‚åº”å™ªå£°æ³¨å…¥æœºåˆ¶æé«˜å¹´é¾„å‡†ç¡®æ€§ï¼Œå¹¶é€šè¿‡èº«ä»½æ„ŸçŸ¥åµŒå…¥å¼ºåŒ–èº«ä»½ä¸€è‡´æ€§ã€‚åœ¨CelebA-HQæµ‹è¯•æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCradle2Caneåœ¨å¹´é¾„å‡†ç¡®æ€§å’Œèº«ä»½ä¸€è‡´æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººè„¸è¡°è€åœ¨è®¡ç®—æœºè§†è§‰ä¸­å…·æœ‰é‡è¦åº”ç”¨ï¼Œä½†ç°æœ‰æ–¹æ³•é¢ä¸´å¹´é¾„å‡†ç¡®æ€§å’Œèº«ä»½ä¸€è‡´æ€§ä¹‹é—´çš„æƒè¡¡æŒ‘æˆ˜ã€‚</li>
<li>Cradle2Caneæ¡†æ¶é‡‡ç”¨åŒé˜¶æ®µç­–ç•¥ï¼Œç¬¬ä¸€é˜¶æ®µä¾§é‡äºæé«˜å¹´é¾„å‡†ç¡®æ€§ï¼Œé€šè¿‡è‡ªé€‚åº”å™ªå£°æ³¨å…¥æœºåˆ¶å®ç°ã€‚ç¬¬äºŒé˜¶æ®µä¸“æ³¨äºå¼ºåŒ–èº«ä»½ä¸€è‡´æ€§ï¼ŒåŒæ—¶ä¿æŒå¹´é¾„ç‰¹å¾ã€‚</li>
<li>é€šè¿‡åœ¨T2Iæ‰©æ•£æ¨¡å‹çš„åŸºç¡€ä¸Šå¼•å…¥æ¡ä»¶æ§åˆ¶ï¼Œå®ç°å¯¹äººè„¸è¡°è€è¿‡ç¨‹çš„çµæ´»è°ƒæ•´ã€‚</li>
<li>ä½¿ç”¨èº«ä»½æ„ŸçŸ¥åµŒå…¥ï¼ˆIDEmbï¼‰æé«˜èº«ä»½ä¸€è‡´æ€§ï¼ŒåŒ…æ‹¬SVR-ArcFaceå’ŒRotate-CLIPã€‚</li>
<li>æ–¹æ³•åœ¨CelebA-HQæµ‹è¯•æ•°æ®é›†ä¸Šè¿›è¡Œå¹¿æ³›å®éªŒéªŒè¯ï¼Œè¡¨ç°å‡ºä¼˜è‰¯æ€§èƒ½ï¼Œå°¤å…¶åœ¨å¹´é¾„å‡†ç¡®æ€§å’Œèº«ä»½ä¸€è‡´æ€§æ–¹é¢ã€‚</li>
<li>æ‰€ææ–¹æ³•çš„ä¼˜ç‚¹åœ¨äºèƒ½å¤Ÿåœ¨ä¸åŒå¹´é¾„æ®µå’Œä¸åŒå¤´éƒ¨å§¿æ€ä¸‹å®ç°æ›´çœŸå®æ— ç¼çš„è½¬æ¢ã€‚è¿™æ˜¯å½“å‰ç ”ç©¶çš„é‡ç‚¹ä¸éš¾ç‚¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20977">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bcdb8e81a20deb8669af4eacea995fb1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bfda41e16e7e91ea3b2ec09b26b33173.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2e5cce7baa5506794182a936284bbabe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-77489068666c36a41343830cd232224d.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="StereoDiff-Stereo-Diffusion-Synergy-for-Video-Depth-Estimation"><a href="#StereoDiff-Stereo-Diffusion-Synergy-for-Video-Depth-Estimation" class="headerlink" title="StereoDiff: Stereo-Diffusion Synergy for Video Depth Estimation"></a>StereoDiff: Stereo-Diffusion Synergy for Video Depth Estimation</h2><p><strong>Authors:Haodong Li, Chen Wang, Jiahui Lei, Kostas Daniilidis, Lingjie Liu</strong></p>
<p>Recent video depth estimation methods achieve great performance by following the paradigm of image depth estimation, i.e., typically fine-tuning pre-trained video diffusion models with massive data. However, we argue that video depth estimation is not a naive extension of image depth estimation. The temporal consistency requirements for dynamic and static regions in videos are fundamentally different. Consistent video depth in static regions, typically backgrounds, can be more effectively achieved via stereo matching across all frames, which provides much stronger global 3D cues. While the consistency for dynamic regions still should be learned from large-scale video depth data to ensure smooth transitions, due to the violation of triangulation constraints. Based on these insights, we introduce StereoDiff, a two-stage video depth estimator that synergizes stereo matching for mainly the static areas with video depth diffusion for maintaining consistent depth transitions in dynamic areas. We mathematically demonstrate how stereo matching and video depth diffusion offer complementary strengths through frequency domain analysis, highlighting the effectiveness of their synergy in capturing the advantages of both. Experimental results on zero-shot, real-world, dynamic video depth benchmarks, both indoor and outdoor, demonstrate StereoDiffâ€™s SoTA performance, showcasing its superior consistency and accuracy in video depth estimation. </p>
<blockquote>
<p>æœ€è¿‘çš„è§†é¢‘æ·±åº¦ä¼°è®¡æ–¹æ³•éµå¾ªå›¾åƒæ·±åº¦ä¼°è®¡çš„æ¨¡å¼ï¼Œå³é€šå¸¸é€šè¿‡å¯¹å¤§é‡æ•°æ®è¿›è¡Œé¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå–å¾—äº†å¾ˆå¥½çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬è®¤ä¸ºè§†é¢‘æ·±åº¦ä¼°è®¡å¹¶ä¸æ˜¯å›¾åƒæ·±åº¦ä¼°è®¡çš„ç®€å•æ‰©å±•ã€‚è§†é¢‘ä¸­çš„åŠ¨æ€å’Œé™æ€åŒºåŸŸçš„æ—¶åºä¸€è‡´æ€§è¦æ±‚å­˜åœ¨æ ¹æœ¬å·®å¼‚ã€‚å¯¹äºé€šå¸¸ä¸ºèƒŒæ™¯çš„é™æ€åŒºåŸŸçš„è§†é¢‘æ·±åº¦ä¸€è‡´æ€§ï¼Œå¯ä»¥é€šè¿‡æ‰€æœ‰å¸§çš„ç«‹ä½“åŒ¹é…æ›´æœ‰æ•ˆåœ°å®ç°ï¼Œè¿™æä¾›äº†æ›´å¼ºçš„å…¨å±€3Dçº¿ç´¢ã€‚è€Œå¯¹äºåŠ¨æ€åŒºåŸŸçš„ä¸€è‡´æ€§ï¼Œç”±äºè¿åäº†ä¸‰è§’çº¦æŸï¼Œä»éœ€è¦ä»å¤§è§„æ¨¡è§†é¢‘æ·±åº¦æ•°æ®ä¸­å­¦ä¹ ï¼Œä»¥ç¡®ä¿å¹³æ»‘è¿‡æ¸¡ã€‚åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬å¼•å…¥äº†StereoDiffï¼Œè¿™æ˜¯ä¸€ç§ä¸¤é˜¶æ®µçš„è§†é¢‘æ·±åº¦ä¼°è®¡å™¨ï¼Œå®ƒå°†ä¸»è¦ç”¨äºé™æ€åŒºåŸŸçš„ç«‹ä½“åŒ¹é…ä¸ç”¨äºä¿æŒåŠ¨æ€åŒºåŸŸä¸­ä¸€è‡´æ·±åº¦è¿‡æ¸¡çš„è§†é¢‘æ·±åº¦æ‰©æ•£ç›¸ç»“åˆã€‚æˆ‘ä»¬é€šè¿‡é¢‘åŸŸåˆ†æä»æ•°å­¦ä¸Šè¯æ˜äº†ç«‹ä½“åŒ¹é…å’Œè§†é¢‘æ·±åº¦æ‰©æ•£å¦‚ä½•æä¾›äº’è¡¥ä¼˜åŠ¿ï¼Œçªæ˜¾äº†å®ƒä»¬åœ¨æ•æ‰å„è‡ªä¼˜åŠ¿ä¸Šçš„ååŒæœ‰æ•ˆæ€§ã€‚åœ¨é›¶æ ·æœ¬ã€ç°å®ä¸–ç•Œã€å®¤å†…å’Œå®¤å¤–çš„åŠ¨æ€è§†é¢‘æ·±åº¦åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒStereoDiffçš„æ€§èƒ½è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼Œå±•ç¤ºäº†å…¶åœ¨è§†é¢‘æ·±åº¦ä¼°è®¡ä¸­çš„å“è¶Šä¸€è‡´æ€§å’Œå‡†ç¡®æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20756v1">PDF</a> Work done in Nov. 2024. Project page: <a target="_blank" rel="noopener" href="https://stereodiff.github.io/">https://stereodiff.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è§†é¢‘æ·±åº¦ä¼°è®¡çš„æ–°æ–¹æ³•StereoDiffï¼Œè¯¥æ–¹æ³•ç»“åˆç«‹ä½“åŒ¹é…å’Œè§†é¢‘æ·±åº¦æ‰©æ•£ä¸¤ä¸ªé˜¶æ®µï¼Œé’ˆå¯¹é™æ€åŒºåŸŸå’ŒåŠ¨æ€åŒºåŸŸçš„ä¸åŒè¦æ±‚å®ç°è§†é¢‘æ·±åº¦ä¼°è®¡ã€‚ç«‹ä½“åŒ¹é…ç”¨äºé™æ€åŒºåŸŸçš„æ·±åº¦ä¼°è®¡ï¼Œæä¾›æ›´å¼ºçš„å…¨å±€3Dçº¿ç´¢ï¼›è§†é¢‘æ·±åº¦æ‰©æ•£åˆ™ç”¨äºåŠ¨æ€åŒºåŸŸï¼Œç¡®ä¿å¹³æ»‘è¿‡æ¸¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒStereoDiffåœ¨é›¶æ ·æœ¬ã€çœŸå®ä¸–ç•Œã€å®¤å†…å¤–çš„åŠ¨æ€è§†é¢‘æ·±åº¦è¯„ä¼°ä¸­è¾¾åˆ°é¢†å…ˆæ°´å¹³ï¼Œå±•ç°å‡ºå“è¶Šçš„ä¸€è‡´æ€§å’Œå‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†é¢‘æ·±åº¦ä¼°è®¡ä¸èƒ½ç®€å•åœ°è§†ä¸ºå›¾åƒæ·±åº¦ä¼°è®¡çš„æ‰©å±•ï¼Œå› ä¸ºè§†é¢‘ä¸­çš„åŠ¨æ€å’Œé™æ€åŒºåŸŸå¯¹æ—¶é—´ä¸€è‡´æ€§çš„è¦æ±‚å­˜åœ¨æ ¹æœ¬å·®å¼‚ã€‚</li>
<li>ç«‹ä½“åŒ¹é…åœ¨é™æ€åŒºåŸŸçš„è§†é¢‘æ·±åº¦ä¼°è®¡ä¸­æ›´æœ‰æ•ˆï¼Œèƒ½æä¾›æ›´å¼ºçš„å…¨å±€3Dçº¿ç´¢ã€‚</li>
<li>åŠ¨æ€åŒºåŸŸçš„æ·±åº¦ä¸€è‡´æ€§éœ€è¦é€šè¿‡å¤§è§„æ¨¡è§†é¢‘æ·±åº¦æ•°æ®å­¦ä¹ å®ç°ï¼Œä»¥ç¡®ä¿å¹³æ»‘è¿‡æ¸¡ã€‚</li>
<li>ä»‹ç»äº†æ–°æ–¹æ³•StereoDiffï¼Œè¯¥æ–¹æ³•ç»“åˆç«‹ä½“åŒ¹é…å’Œè§†é¢‘æ·±åº¦æ‰©æ•£ï¼Œåˆ†åˆ«é’ˆå¯¹é™æ€å’ŒåŠ¨æ€åŒºåŸŸè¿›è¡Œä¼˜åŒ–ã€‚</li>
<li>StereoDiffé€šè¿‡é¢‘ç‡åŸŸåˆ†ææ•°å­¦åœ°è¯æ˜äº†ç«‹ä½“åŒ¹é…å’Œè§†é¢‘æ·±åº¦æ‰©æ•£çš„äº’è¡¥ä¼˜åŠ¿ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒStereoDiffåœ¨è§†é¢‘æ·±åº¦ä¼°è®¡çš„é›¶æ ·æœ¬ã€çœŸå®ä¸–ç•Œã€å®¤å†…å¤–çš„è¯„ä¼°ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå…·æœ‰ä¸€æµçš„ä¸€è‡´æ€§å’Œå‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20756">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-dfe2497f9f90ac4b534a81a9d5d99406.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ca3298d263c11cd5636a9e55ed4526c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f186dca18d6cc0c8f4e82be2d59469e.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="On-Convolutions-Intrinsic-Dimension-and-Diffusion-Models"><a href="#On-Convolutions-Intrinsic-Dimension-and-Diffusion-Models" class="headerlink" title="On Convolutions, Intrinsic Dimension, and Diffusion Models"></a>On Convolutions, Intrinsic Dimension, and Diffusion Models</h2><p><strong>Authors:Kin Kwan Leung, Rasa Hosseinzadeh, Gabriel Loaiza-Ganem</strong></p>
<p>The manifold hypothesis asserts that data of interest in high-dimensional ambient spaces, such as image data, lies on unknown low-dimensional submanifolds. Diffusion models (DMs) â€“ which operate by convolving data with progressively larger amounts of Gaussian noise and then learning to revert this process â€“ have risen to prominence as the most performant generative models, and are known to be able to learn distributions with low-dimensional support. For a given datum in one of these submanifolds, we should thus intuitively expect DMs to have implicitly learned its corresponding local intrinsic dimension (LID), i.e. the dimension of the submanifold it belongs to. Kamkari et al. (2024b) recently showed that this is indeed the case by linking this LID to the rate of change of the log marginal densities of the DM with respect to the amount of added noise, resulting in an LID estimator known as FLIPD. LID estimators such as FLIPD have a plethora of uses, among others they quantify the complexity of a given datum, and can be used to detect outliers, adversarial examples and AI-generated text. FLIPD achieves state-of-the-art performance at LID estimation, yet its theoretical underpinnings are incomplete since Kamkari et al. (2024b) only proved its correctness under the highly unrealistic assumption of affine submanifolds. In this work we bridge this gap by formally proving the correctness of FLIPD under realistic assumptions. Additionally, we show that an analogous result holds when Gaussian convolutions are replaced with uniform ones, and discuss the relevance of this result. </p>
<blockquote>
<p>æµå½¢å‡è®¾è®¤ä¸ºï¼Œåœ¨å›¾åƒæ•°æ®ç­‰æ„Ÿå…´è¶£çš„é«˜ç»´ç¯å¢ƒç©ºé—´ä¸­ï¼Œæ•°æ®ä½äºæœªçŸ¥çš„ä½ç»´å­æµå½¢ä¸Šã€‚æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰é€šè¿‡ç”¨é€æ¸å¢åŠ çš„é«˜æ–¯å™ªå£°å¯¹æ•°æ®è¿›è¡Œå·ç§¯ï¼Œç„¶åå­¦ä¹ åè½¬è¿™ä¸€è¿‡ç¨‹ï¼Œå·²æˆä¸ºæ€§èƒ½æœ€ä½³çš„ç”Ÿæˆæ¨¡å‹ï¼Œå¹¶ä¸”å·²çŸ¥èƒ½å¤Ÿå­¦ä¹ ä½ç»´æ”¯æŒçš„åˆ†å¸ƒã€‚å› æ­¤ï¼Œå¯¹äºå…¶ä¸­ä¸€ä¸ªå­æµå½¢ä¸­çš„ç»™å®šæ•°æ®ï¼Œæˆ‘ä»¬åº”è¯¥ç›´è§‰åœ°æœŸæœ›DMå·²ç»éšå¼å­¦ä¹ äº†å…¶ç›¸åº”çš„å±€éƒ¨å†…è•´ç»´æ•°ï¼ˆLIDï¼‰ï¼Œå³å®ƒæ‰€å±çš„å­æµå½¢çš„ç»´æ•°ã€‚Kamkariç­‰äººï¼ˆ2024bï¼‰æœ€è¿‘é€šè¿‡å°†LIDä¸DMå¯¹æ•°è¾¹ç¼˜å¯†åº¦ç›¸å¯¹äºæ‰€æ·»åŠ å™ªå£°çš„å˜åŒ–ç‡è”ç³»èµ·æ¥ï¼Œè¯æ˜äº†è¿™ä¸€ç‚¹ç¡®å®å¦‚æ­¤ï¼Œä»è€Œå¾—åˆ°äº†ä¸€ä¸ªç§°ä¸ºFLIPDçš„LIDä¼°è®¡å™¨ã€‚åƒFLIPDè¿™æ ·çš„LIDä¼°è®¡å™¨æœ‰å¾ˆå¤šç”¨é€”ï¼Œä¾‹å¦‚é‡åŒ–ç»™å®šæ•°æ®çš„å¤æ‚æ€§ï¼Œå¹¶å¯ç”¨äºæ£€æµ‹å¼‚å¸¸å€¼ã€å¯¹æŠ—æ€§ç¤ºä¾‹å’Œäººå·¥æ™ºèƒ½ç”Ÿæˆçš„æ–‡æœ¬ã€‚FLIPDåœ¨LIDä¼°è®¡æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ï¼Œä½†å…¶ç†è®ºåŸºç¡€å°šä¸å®Œæ•´ï¼Œå› ä¸ºKamkariç­‰äººï¼ˆ2024bï¼‰åªåœ¨é«˜åº¦ä¸åˆ‡å®é™…çš„ä»¿å°„å­æµå½¢çš„å‡è®¾ä¸‹è¯æ˜äº†å…¶æ­£ç¡®æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å»ºç«‹åˆç†çš„å‡è®¾æ­£å¼è¯æ˜äº†FLIPDçš„æ­£ç¡®æ€§ï¼Œä»è€Œå¡«è¡¥äº†è¿™ä¸€ç©ºç™½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯æ˜äº†å½“é«˜æ–¯å·ç§¯è¢«å‡åŒ€å·ç§¯å–ä»£æ—¶ï¼Œä¼šå‡ºç°ç±»ä¼¼çš„ç»“æœï¼Œå¹¶è®¨è®ºäº†è¿™ä¸€ç»“æœçš„ç°å®æ„ä¹‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20705v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰ä¸å±€éƒ¨å†…åœ¨ç»´åº¦ï¼ˆLIDï¼‰ä¹‹é—´çš„å…³ç³»ã€‚æ–‡ç« æŒ‡å‡ºï¼Œç”±äºæ‰©æ•£æ¨¡å‹åœ¨å¤„ç†é«˜ç»´æ•°æ®æ—¶å¯ä»¥å­¦ä¹ ä½ç»´åˆ†å¸ƒï¼Œå› æ­¤åº”è¯¥èƒ½å¤Ÿå­¦ä¹ æ•°æ®çš„å±€éƒ¨å†…åœ¨ç»´åº¦ã€‚è¿‘æœŸçš„ç ”ç©¶å·²ç»é€šè¿‡å…³è”DMsçš„è¾¹é™…å¯¹æ•°å¯†åº¦ä¸æ·»åŠ å™ªå£°çš„é‡æ¥ä¼°è®¡LIDï¼Œäº§ç”Ÿäº†ä¸€ç§åä¸ºFLIPDçš„LIDä¼°è®¡å™¨ã€‚å°½ç®¡FLIPDåœ¨LIDä¼°è®¡æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä½†å…¶ç†è®ºæ”¯æ’‘å¹¶ä¸å®Œæ•´ã€‚æœ¬æ–‡æ—¨åœ¨å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œåœ¨æ›´ç°å®çš„å‡è®¾ä¸‹è¯æ˜FLIPDçš„æ­£ç¡®æ€§ï¼Œå¹¶è®¨è®ºäº†å½“é«˜æ–¯å·ç§¯è¢«å‡åŒ€å·ç§¯æ›¿ä»£æ—¶çš„ç±»ä¼¼ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰åœ¨å¤„ç†é«˜ç»´æ•°æ®æ—¶è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œå¹¶èƒ½å­¦ä¹ æ•°æ®çš„ä½ç»´åˆ†å¸ƒã€‚</li>
<li>å±€éƒ¨å†…åœ¨ç»´åº¦ï¼ˆLIDï¼‰æ˜¯æè¿°æ•°æ®æ‰€å±å­æµå½¢ç»´åº¦çš„æ¦‚å¿µã€‚</li>
<li>FLIPDæ˜¯ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„LIDä¼°è®¡å™¨ï¼Œå®ƒé€šè¿‡å…³è”DMsçš„è¾¹é™…å¯¹æ•°å¯†åº¦ä¸æ·»åŠ å™ªå£°çš„é‡æ¥å·¥ä½œã€‚</li>
<li>FLIPDåœ¨LIDä¼°è®¡æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä½†å…¶åœ¨ä¹‹å‰çš„ç ”ç©¶ä¸­ç†è®ºæ”¯æ’‘ä¸å®Œæ•´ã€‚</li>
<li>æœ¬æ–‡åœ¨æ›´ç°å®çš„å‡è®¾ä¸‹è¯æ˜äº†FLIPDçš„æ­£ç¡®æ€§ï¼Œä¸ºFLIPDçš„ç†è®ºåŸºç¡€æä¾›äº†é‡è¦è¡¥å……ã€‚</li>
<li>é™¤äº†é«˜æ–¯å·ç§¯ï¼Œæœ¬æ–‡è¿˜æ¢è®¨äº†å½“ä½¿ç”¨å‡åŒ€å·ç§¯æ—¶FLIPDçš„ç±»ä¼¼ç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20705">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-029dd5a8a370fa5a485c26716cf521ac.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Diffusion-Tree-Sampling-Scalable-inference-time-alignment-of-diffusion-models"><a href="#Diffusion-Tree-Sampling-Scalable-inference-time-alignment-of-diffusion-models" class="headerlink" title="Diffusion Tree Sampling: Scalable inference-time alignment of diffusion   models"></a>Diffusion Tree Sampling: Scalable inference-time alignment of diffusion   models</h2><p><strong>Authors:Vineet Jain, Kusha Sareen, Mohammad Pedramfar, Siamak Ravanbakhsh</strong></p>
<p>Adapting a pretrained diffusion model to new objectives at inference time remains an open problem in generative modeling. Existing steering methods suffer from inaccurate value estimation, especially at high noise levels, which biases guidance. Moreover, information from past runs is not reused to improve sample quality, resulting in inefficient use of compute. Inspired by the success of Monte Carlo Tree Search, we address these limitations by casting inference-time alignment as a search problem that reuses past computations. We introduce a tree-based approach that samples from the reward-aligned target density by propagating terminal rewards back through the diffusion chain and iteratively refining value estimates with each additional generation. Our proposed method, Diffusion Tree Sampling (DTS), produces asymptotically exact samples from the target distribution in the limit of infinite rollouts, and its greedy variant, Diffusion Tree Search (DTS$^\star$), performs a global search for high reward samples. On MNIST and CIFAR-10 class-conditional generation, DTS matches the FID of the best-performing baseline with up to $10\times$ less compute. In text-to-image generation and language completion tasks, DTS$^\star$ effectively searches for high reward samples that match best-of-N with up to $5\times$ less compute. By reusing information from previous generations, we get an anytime algorithm that turns additional compute into steadily better samples, providing a scalable approach for inference-time alignment of diffusion models. </p>
<blockquote>
<p>åœ¨ç”Ÿæˆæ¨¡å‹ä¸­ï¼Œå°†é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹é€‚åº”äºæ¨ç†æ—¶é—´çš„æ–°ç›®æ ‡ä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾æ€§é—®é¢˜ã€‚ç°æœ‰çš„å¼•å¯¼æ–¹æ³•å­˜åœ¨ä»·å€¼ä¼°è®¡ä¸å‡†ç¡®çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜å™ªå£°æ°´å¹³ä¸‹ï¼Œè¿™ä¼šå¯¼è‡´å¼•å¯¼å‡ºç°åå·®ã€‚æ­¤å¤–ï¼Œè¿‡å»è¿è¡Œçš„ä¿¡æ¯æ²¡æœ‰è¢«é‡æ–°åˆ©ç”¨æ¥æé«˜æ ·æœ¬è´¨é‡ï¼Œå¯¼è‡´è®¡ç®—ä½¿ç”¨æ•ˆç‡ä½ä¸‹ã€‚å—åˆ°è’™ç‰¹å¡æ´›æ ‘æœç´¢æˆåŠŸçš„å¯å‘ï¼Œæˆ‘ä»¬é€šè¿‡å°†æ¨ç†æ—¶é—´å¯¹é½è½¬åŒ–ä¸ºä¸€ä¸ªæœç´¢é—®é¢˜æ¥è§£å†³è¿™äº›é™åˆ¶ï¼Œè¯¥é—®é¢˜å¯é‡ç”¨è¿‡å»çš„è®¡ç®—ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºæ ‘çš„æ–¹æ³•ï¼Œé€šè¿‡åå‘ä¼ æ’­æ‰©æ•£é“¾ä¸­çš„ç»ˆç«¯å¥–åŠ±ï¼Œä»å¥–åŠ±å¯¹é½çš„ç›®æ ‡å¯†åº¦ä¸­è¿›è¡Œé‡‡æ ·ï¼Œå¹¶éšç€æ¯ä¸€ä»£çš„å¢åŠ ï¼Œé€šè¿‡è¿­ä»£æ”¹è¿›ä»·å€¼ä¼°è®¡ã€‚æˆ‘ä»¬æå‡ºçš„æ–¹æ³•â€”â€”æ‰©æ•£æ ‘é‡‡æ ·ï¼ˆDTSï¼‰åœ¨æ— é™è¿­ä»£æ¬¡æ•°çš„æƒ…å†µä¸‹ï¼Œä»ç›®æ ‡åˆ†å¸ƒä¸­äº§ç”Ÿäº†æ¸è¿‘ç²¾ç¡®çš„æ ·æœ¬ï¼›å…¶è´ªå¿ƒå˜ä½“â€”â€”æ‰©æ•£æ ‘æœç´¢ï¼ˆDTS<em>ï¼‰åˆ™åœ¨å…¨çƒèŒƒå›´å†…æœç´¢é«˜å¥–åŠ±æ ·æœ¬ã€‚åœ¨MNISTå’ŒCIFAR-10çš„æ¡ä»¶ç±»ç”Ÿæˆä¸­ï¼ŒDTSä¸è¡¨ç°æœ€ä½³çš„åŸºå‡†çº¿ç›¸åŒ¹é…ï¼Œè®¡ç®—é‡å‡å°‘äº†é«˜è¾¾10å€ã€‚åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå’Œè¯­è¨€å®Œæˆä»»åŠ¡ä¸­ï¼ŒDTS</em>æœ‰æ•ˆåœ°æœç´¢é«˜å¥–åŠ±æ ·æœ¬ï¼Œä¸æœ€ä½³NåŒ¹é…çš„è®¡ç®—é‡å‡å°‘é«˜è¾¾5å€ã€‚é€šè¿‡é‡ç”¨å‰å‡ ä»£çš„ä¿¡æ¯ï¼Œæˆ‘ä»¬è·å¾—äº†ä¸€ç§ä»»ä½•æ—¶å€™éƒ½å¯ä»¥ä½¿ç”¨çš„ç®—æ³•ï¼Œè¯¥ç®—æ³•å¯å°†é¢å¤–çš„è®¡ç®—è½¬åŒ–ä¸ºæ›´å¥½çš„æ ·æœ¬ï¼Œä¸ºæ‰©æ•£æ¨¡å‹åœ¨æ¨ç†æ—¶é—´ä¸Šçš„å¯¹é½æä¾›äº†ä¸€ç§å¯æ‰©å±•çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20701v1">PDF</a> </p>
<p><strong>Summary</strong><br>     é’ˆå¯¹é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹åœ¨æ¨ç†é˜¶æ®µé€‚åº”æ–°ç›®æ ‡çš„é—®é¢˜ï¼Œç°æœ‰å¼•å¯¼æ–¹æ³•å­˜åœ¨ä¼°å€¼ä¸å‡†ç¡®ï¼ˆå°¤å…¶åœ¨é«˜å™ªå£°æ°´å¹³ä¸‹ï¼‰å’Œä¿¡æ¯æœªæœ‰æ•ˆåˆ©ç”¨çš„é—®é¢˜ã€‚æœ¬æ–‡é‡‡ç”¨è’™ç‰¹å¡ç½—æ ‘æœç´¢çš„æ€æƒ³ï¼Œå°†æ¨ç†é˜¶æ®µçš„å¯¹é½é—®é¢˜è½¬åŒ–ä¸ºæœç´¢é—®é¢˜ï¼Œé‡ç”¨è¿‡å»çš„è®¡ç®—æ¥æé«˜æ ·æœ¬è´¨é‡ã€‚æå‡ºçš„åŸºäºæ ‘çš„æ–¹æ³•é€šè¿‡åå‘ä¼ æ’­ç›®æ ‡å¯†åº¦å¥–åŠ±ï¼Œè¿­ä»£ä¼˜åŒ–ä»·å€¼ä¼°è®¡ï¼Œç”Ÿæˆæ¸è¿‘ç²¾ç¡®æ ·æœ¬ã€‚åœ¨MNISTå’ŒCIFAR-10åˆ†ç±»ç”Ÿæˆä»»åŠ¡ä¸Šï¼Œè¯¥æ–¹æ³•ä¸æœ€ä½³åŸºçº¿æ¨¡å‹çš„FIDç›¸åŒ¹é…ï¼Œè®¡ç®—æ•ˆç‡æé«˜10å€ã€‚åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå’Œè¯­è¨€è¡¥å…¨ä»»åŠ¡ä¸­ï¼Œé€šè¿‡æœç´¢é«˜å¥–åŠ±æ ·æœ¬ï¼Œä¸æœ€ä½³åŸºçº¿æ¨¡å‹çš„åŒ¹é…åº¦æ›´é«˜ï¼Œè®¡ç®—æ•ˆç‡æé«˜5å€ã€‚é€šè¿‡é‡ç”¨ä¹‹å‰çš„è®¡ç®—ä¿¡æ¯ï¼Œè¯¥ç®—æ³•åœ¨é¢å¤–è®¡ç®—çš„æƒ…å†µä¸‹å¯ç”Ÿæˆæ›´å¥½çš„æ ·æœ¬ï¼Œä¸ºæ‰©æ•£æ¨¡å‹æ¨ç†é˜¶æ®µçš„å¯¹é½é—®é¢˜æä¾›äº†å¯ä¼¸ç¼©çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨æ¨ç†é˜¶æ®µé€‚åº”æ–°ç›®æ ‡æ—¶é¢ä¸´æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ä¼°å€¼ä¸å‡†ç¡®å’Œä¿¡æ¯æœªæœ‰æ•ˆåˆ©ç”¨çš„é—®é¢˜ã€‚</li>
<li>æœ¬æ–‡é€šè¿‡è’™ç‰¹å¡ç½—æ ‘æœç´¢æ€æƒ³ï¼Œå°†æ¨ç†é˜¶æ®µçš„å¯¹é½é—®é¢˜è½¬åŒ–ä¸ºæœç´¢é—®é¢˜ã€‚</li>
<li>å¼•å…¥åŸºäºæ ‘çš„æ–¹æ³•ï¼Œé€šè¿‡åå‘ä¼ æ’­ç›®æ ‡å¯†åº¦å¥–åŠ±ï¼Œè¿­ä»£ä¼˜åŒ–ä»·å€¼ä¼°è®¡ã€‚</li>
<li>æå‡ºçš„æ–¹æ³•åœ¨MNISTå’ŒCIFAR-10åˆ†ç±»ç”Ÿæˆä»»åŠ¡ä¸ŠåŒ¹é…æœ€ä½³åŸºçº¿æ¨¡å‹çš„FIDï¼Œè®¡ç®—æ•ˆç‡æ˜¾è‘—æé«˜ã€‚</li>
<li>åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå’Œè¯­è¨€è¡¥å…¨ä»»åŠ¡ä¸­ï¼Œé€šè¿‡æœç´¢é«˜å¥–åŠ±æ ·æœ¬ï¼Œæé«˜äº†ä¸æœ€ä½³åŸºçº¿æ¨¡å‹çš„åŒ¹é…åº¦ã€‚</li>
<li>é€šè¿‡é‡ç”¨ä¹‹å‰çš„è®¡ç®—ä¿¡æ¯ï¼Œè¯¥ç®—æ³•èƒ½æ›´æœ‰æ•ˆåœ°ç”Ÿæˆæ ·æœ¬ï¼Œè½¬åŒ–ä¸ºå¯ä¼¸ç¼©çš„è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20701">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-805efeff97e24842012fe54f06acc361.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1256bf7309f8752f4407df7f65c4ab97.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-798653955ae441496ce569a2010bfa4a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7049f69c542de3e2b288a9b1a45c7c34.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="TaxaDiffusion-Progressively-Trained-Diffusion-Model-for-Fine-Grained-Species-Generation"><a href="#TaxaDiffusion-Progressively-Trained-Diffusion-Model-for-Fine-Grained-Species-Generation" class="headerlink" title="TaxaDiffusion: Progressively Trained Diffusion Model for Fine-Grained   Species Generation"></a>TaxaDiffusion: Progressively Trained Diffusion Model for Fine-Grained   Species Generation</h2><p><strong>Authors:Amin Karimi Monsefi, Mridul Khurana, Rajiv Ramnath, Anuj Karpatne, Wei-Lun Chao, Cheng Zhang</strong></p>
<p>We propose TaxaDiffusion, a taxonomy-informed training framework for diffusion models to generate fine-grained animal images with high morphological and identity accuracy. Unlike standard approaches that treat each species as an independent category, TaxaDiffusion incorporates domain knowledge that many species exhibit strong visual similarities, with distinctions often residing in subtle variations of shape, pattern, and color. To exploit these relationships, TaxaDiffusion progressively trains conditioned diffusion models across different taxonomic levels â€“ starting from broad classifications such as Class and Order, refining through Family and Genus, and ultimately distinguishing at the Species level. This hierarchical learning strategy first captures coarse-grained morphological traits shared by species with common ancestors, facilitating knowledge transfer before refining fine-grained differences for species-level distinction. As a result, TaxaDiffusion enables accurate generation even with limited training samples per species. Extensive experiments on three fine-grained animal datasets demonstrate that outperforms existing approaches, achieving superior fidelity in fine-grained animal image generation. Project page: <a target="_blank" rel="noopener" href="https://amink8.github.io/TaxaDiffusion/">https://amink8.github.io/TaxaDiffusion/</a> </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†TaxaDiffusionï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹æ‰©æ•£æ¨¡å‹çš„åˆ†ç±»å­¦å¯å‘è®­ç»ƒæ¡†æ¶ï¼Œç”¨äºç”Ÿæˆå…·æœ‰é«˜ç²¾åº¦å½¢æ€å’Œèº«ä»½ç‰¹å¾çš„ç²¾ç»†åŠ¨ç‰©å›¾åƒã€‚ä¸åŒäºæ ‡å‡†æ–¹æ³•å°†æ¯ä¸ªç‰©ç§è§†ä¸ºç‹¬ç«‹ç±»åˆ«çš„æ–¹å¼ï¼ŒTaxaDiffusionç»“åˆäº†é¢†åŸŸçŸ¥è¯†ï¼Œå³è®¸å¤šç‰©ç§è¡¨ç°å‡ºå¼ºçƒˆçš„è§†è§‰ç›¸ä¼¼æ€§ï¼Œå·®å¼‚é€šå¸¸åœ¨äºå½¢çŠ¶ã€å›¾æ¡ˆå’Œé¢œè‰²çš„å¾®å¦™å˜åŒ–ã€‚ä¸ºäº†åˆ©ç”¨è¿™äº›å…³ç³»ï¼ŒTaxaDiffusionåœ¨ä¸åŒçš„åˆ†ç±»å­¦å±‚æ¬¡ä¸Šé€æ­¥è®­ç»ƒæ¡ä»¶æ‰©æ•£æ¨¡å‹â€”â€”ä»å¤§ç±»å¦‚ç±»å’Œç›®å¼€å§‹ï¼Œé€šè¿‡å®¶æ—å’Œå±è¿›è¡Œç²¾ç‚¼ï¼Œæœ€ç»ˆåœ¨ç‰©ç§æ°´å¹³ä¸Šåšå‡ºåŒºåˆ†ã€‚è¿™ç§å±‚æ¬¡åŒ–çš„å­¦ä¹ ç­–ç•¥é¦–å…ˆæ•æ‰ç”±å…±åŒç¥–å…ˆå…±æœ‰çš„ç²—ç²’åº¦å½¢æ€ç‰¹å¾ï¼Œåœ¨ç»†åŒ–ç‰©ç§æ°´å¹³çš„å·®å¼‚ä¹‹å‰ä¿ƒè¿›çŸ¥è¯†è½¬ç§»ã€‚å› æ­¤ï¼Œå³ä½¿æ¯ä¸ªç‰©ç§çš„è®­ç»ƒæ ·æœ¬æœ‰é™ï¼ŒTaxaDiffusionä¹Ÿèƒ½å®ç°å‡†ç¡®çš„ç”Ÿæˆã€‚åœ¨ä¸‰ä¸ªç²¾ç»†åŠ¨ç‰©æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œå…¶æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨ç²¾ç»†åŠ¨ç‰©å›¾åƒç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ•ˆæœã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://amink8.github.io/TaxaDiffusion/">https://amink8.github.io/TaxaDiffusion/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.01923v2">PDF</a> Accepted to ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†TaxaDiffusionï¼Œä¸€ç§ç”¨äºç”Ÿæˆç²¾ç»†åŠ¨ç‰©å›¾åƒçš„åˆ†ç±»å­¦æŒ‡å¯¼çš„æ‰©æ•£æ¨¡å‹è®­ç»ƒæ¡†æ¶ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒTaxaDiffusioné€šè¿‡åœ¨ä¸åŒåˆ†ç±»ç­‰çº§ä¸Šé€æ­¥è®­ç»ƒæ¡ä»¶æ‰©æ•£æ¨¡å‹æ¥åˆ©ç”¨ç‰©ç§é—´çš„è§†è§‰ç›¸ä¼¼æ€§ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿä»ç²—ç²’åº¦åˆ†ç±»ï¼ˆå¦‚ç±»ã€ç›®ï¼‰å¼€å§‹å­¦ä¹ ï¼Œé€æ¸ç»†åŒ–åˆ°ç§‘ã€å±ï¼Œå¹¶æœ€ç»ˆè¾¾åˆ°ç‰©ç§çº§åˆ«ã€‚è¿™ç§å±‚æ¬¡åŒ–çš„å­¦ä¹ ç­–ç•¥èƒ½å¤Ÿé¦–å…ˆæ•æ‰ç‰©ç§ä¹‹é—´å…±åŒç¥–å…ˆæ‰€å¸¦æ¥çš„ç²—ç²’åº¦å½¢æ€ç‰¹å¾ï¼Œç„¶åå†åŒºåˆ†ç»†å¾®å·®åˆ«ï¼Œè¾¾åˆ°ç‰©ç§çº§åˆ«çš„è¯†åˆ«ã€‚TaxaDiffusionèƒ½å¤Ÿå®ç°åœ¨æœ‰é™çš„è®­ç»ƒæ ·æœ¬ä¸‹ç”Ÿæˆå‡†ç¡®çš„å›¾åƒï¼Œå¹¶åœ¨ä¸‰ä¸ªç²¾ç»†åŠ¨ç‰©æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†å…¶ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TaxaDiffusionæ˜¯ä¸€ä¸ªåˆ©ç”¨åˆ†ç±»å­¦ä¿¡æ¯çš„æ‰©æ•£æ¨¡å‹è®­ç»ƒæ¡†æ¶ï¼Œç”¨äºç”Ÿæˆç²¾ç»†åŠ¨ç‰©å›¾åƒã€‚</li>
<li>ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒTaxaDiffusioné€šè¿‡åœ¨ä¸åŒåˆ†ç±»ç­‰çº§ä¸Šé€æ­¥è®­ç»ƒæ¨¡å‹æ¥åˆ©ç”¨ç‰©ç§é—´çš„è§†è§‰ç›¸ä¼¼æ€§ã€‚</li>
<li>TaxaDiffusioné‡‡ç”¨å±‚æ¬¡åŒ–çš„å­¦ä¹ ç­–ç•¥ï¼Œä»ç²—ç²’åº¦åˆ†ç±»å¼€å§‹ï¼Œé€æ¸ç»†åŒ–åˆ°ç‰©ç§çº§åˆ«ã€‚</li>
<li>è¯¥æ¡†æ¶èƒ½å¤Ÿæ•æ‰ç‰©ç§ä¹‹é—´å…±åŒç¥–å…ˆæ‰€å¸¦æ¥çš„å½¢æ€ç‰¹å¾ï¼Œå¹¶åŒºåˆ†ç»†å¾®å·®åˆ«ã€‚</li>
<li>TaxaDiffusionåœ¨æœ‰é™çš„è®­ç»ƒæ ·æœ¬ä¸‹èƒ½å¤Ÿå®ç°å‡†ç¡®çš„å›¾åƒç”Ÿæˆã€‚</li>
<li>åœ¨ä¸‰ä¸ªç²¾ç»†åŠ¨ç‰©æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†TaxaDiffusionçš„ä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.01923">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-73f145d187010f64098bce07a3006f61.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-77ce4325f19de04cb37491a80b1d45f3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b00a9cc57980a35b7ba72456c8770575.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0983691e0f607e66f14663095f83de5e.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="QuEST-Low-bit-Diffusion-Model-Quantization-via-Efficient-Selective-Finetuning"><a href="#QuEST-Low-bit-Diffusion-Model-Quantization-via-Efficient-Selective-Finetuning" class="headerlink" title="QuEST: Low-bit Diffusion Model Quantization via Efficient Selective   Finetuning"></a>QuEST: Low-bit Diffusion Model Quantization via Efficient Selective   Finetuning</h2><p><strong>Authors:Haoxuan Wang, Yuzhang Shang, Zhihang Yuan, Junyi Wu, Junchi Yan, Yan Yan</strong></p>
<p>The practical deployment of diffusion models is still hindered by the high memory and computational overhead. Although quantization paves a way for model compression and acceleration, existing methods face challenges in achieving low-bit quantization efficiently. In this paper, we identify imbalanced activation distributions as a primary source of quantization difficulty, and propose to adjust these distributions through weight finetuning to be more quantization-friendly. We provide both theoretical and empirical evidence supporting finetuning as a practical and reliable solution. Building on this approach, we further distinguish two critical types of quantized layers: those responsible for retaining essential temporal information and those particularly sensitive to bit-width reduction. By selectively finetuning these layers under both local and global supervision, we mitigate performance degradation while enhancing quantization efficiency. Our method demonstrates its efficacy across three high-resolution image generation tasks, obtaining state-of-the-art performance across multiple bit-width settings. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹çš„å®é™…åº”ç”¨ä»ç„¶å—åˆ°é«˜å†…å­˜å’Œè®¡ç®—å¼€é”€çš„é˜»ç¢ã€‚å°½ç®¡é‡åŒ–å¯ä»¥ä¸ºæ¨¡å‹å‹ç¼©å’ŒåŠ é€Ÿé“ºå¹³é“è·¯ï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨å®ç°ä½ä½é‡åŒ–æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è¯†åˆ«å‡ºå¤±è¡¡çš„æ¿€æ´»åˆ†å¸ƒæ˜¯é‡åŒ–çš„ä¸»è¦å›°éš¾æ¥æºï¼Œå¹¶æå‡ºé€šè¿‡æƒé‡å¾®è°ƒè°ƒæ•´è¿™äº›åˆ†å¸ƒï¼Œä½¿å…¶æ›´åˆ©äºé‡åŒ–ã€‚æˆ‘ä»¬æä¾›äº†ç†è®ºå’Œå®è¯è¯æ®ï¼Œæ”¯æŒå¾®è°ƒä½œä¸ºä¸€ç§å®ç”¨å¯é çš„è§£å†³æ–¹æ¡ˆã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥åŒºåˆ†äº†ä¸¤ç§å…³é”®çš„é‡åŒ–å±‚ï¼šé‚£äº›è´Ÿè´£ä¿ç•™é‡è¦æ—¶é—´ä¿¡æ¯çš„å±‚å’Œé‚£äº›å¯¹ä½å®½å‡å°‘ç‰¹åˆ«æ•æ„Ÿçš„å±‚ã€‚é€šè¿‡å±€éƒ¨å’Œå…¨å±€ç›‘ç£ä¸‹æœ‰é€‰æ‹©åœ°å¯¹è¿™äº›å±‚è¿›è¡Œå¾®è°ƒï¼Œæˆ‘ä»¬åœ¨æé«˜é‡åŒ–æ•ˆç‡çš„åŒæ—¶ç¼“è§£äº†æ€§èƒ½ä¸‹é™ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸‰é«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­è¯æ˜äº†å…¶æœ‰æ•ˆæ€§ï¼Œå¹¶åœ¨å¤šä¸ªä½å®½è®¾ç½®ä¸‹è·å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.03666v4">PDF</a> ICCV 2025. Code is available at   <a target="_blank" rel="noopener" href="https://github.com/hatchetProject/QuEST">https://github.com/hatchetProject/QuEST</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨æ‰©æ•£æ¨¡å‹çš„å®ç”¨éƒ¨ç½²ä¸­é¢ä¸´çš„é«˜å†…å­˜å’Œè®¡ç®—å¼€é”€é—®é¢˜ã€‚é’ˆå¯¹ç°æœ‰é‡åŒ–æ–¹æ³•é¢ä¸´çš„æŒ‘æˆ˜ï¼Œæå‡ºé€šè¿‡æƒé‡å¾®è°ƒè°ƒæ•´ä¸å¹³è¡¡æ¿€æ´»åˆ†å¸ƒä»¥å®ç°å‹å¥½é‡åŒ–ã€‚æœ¬æ–‡åœ¨ç†è®ºæ”¯æ’‘çš„åŒæ—¶è¿›è¡Œå®è¯ç ”ç©¶éªŒè¯å¾®è°ƒæ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œå¯é æ€§ã€‚é€šè¿‡é€‰æ‹©æ€§å¾®è°ƒé‡åŒ–å±‚ï¼Œåœ¨æœ¬åœ°å’Œå…¨å±€ç›‘ç£ä¸‹æé«˜é‡åŒ–æ•ˆç‡å¹¶é™ä½æ€§èƒ½ä¸‹é™çš„é£é™©ã€‚è¯¥æ–¹æ³•åœ¨ä¸‰é¡¹é«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å“è¶Šï¼Œåœ¨ä¸åŒä½å®½è®¾ç½®ä¸‹å‡è¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹éƒ¨ç½²é¢ä¸´é«˜å†…å­˜å’Œè®¡ç®—å¼€é”€é—®é¢˜ã€‚</li>
<li>ç°æœ‰é‡åŒ–æ–¹æ³•é¢ä¸´å®ç°ä½æ¯”ç‰¹é‡åŒ–çš„æŒ‘æˆ˜ã€‚</li>
<li>ä¸å¹³è¡¡çš„æ¿€æ´»åˆ†å¸ƒæ˜¯é‡åŒ–å›°éš¾çš„ä¸»è¦åŸå› ä¹‹ä¸€ã€‚</li>
<li>é€šè¿‡æƒé‡å¾®è°ƒè°ƒæ•´æ¿€æ´»åˆ†å¸ƒä»¥å®ç°å‹å¥½é‡åŒ–ã€‚</li>
<li>æå‡ºäº†é€‰æ‹©æ€§å¾®è°ƒç‰¹å®šé‡åŒ–å±‚çš„æ–¹æ³•ï¼Œä»¥æé«˜é‡åŒ–æ•ˆç‡å’Œæ€§èƒ½ã€‚</li>
<li>æ–¹æ³•åœ¨ä¸‰ä¸ªé«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°ä¼˜è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2402.03666">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-377d14451ebea201c15bb066e9a150b2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2f838e90ed63bb3599ceacd153b10ac8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2f155e65b9c876b0acb10a9d79b6c802.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f8be60991dfe6dea172d98d0c0bff13a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f93b8cfb10211e4ed36e2345eac31f6c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2708e2891246883ca60f3bd7ecea99b9.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-28/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-28/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-28/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-9c77b70bfe8f8f2c5aea0f0193a8961f.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-28  Exploring the Design Space of 3D MLLMs for CT Report Generation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-28
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-28/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-bdee4bec1f766bb22519f88d108fed11.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-28  PanSt3R Multi-view Consistent Panoptic Segmentation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-28
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">26548.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
