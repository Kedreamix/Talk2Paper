<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-28  mTSBench Benchmarking Multivariate Time Series Anomaly Detection and   Model Selection at Scale">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-4c826f5d3ba4bf3338ec4bc057ee9250.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-28
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    80 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-28-æ›´æ–°"><a href="#2025-06-28-æ›´æ–°" class="headerlink" title="2025-06-28 æ›´æ–°"></a>2025-06-28 æ›´æ–°</h1><h2 id="mTSBench-Benchmarking-Multivariate-Time-Series-Anomaly-Detection-and-Model-Selection-at-Scale"><a href="#mTSBench-Benchmarking-Multivariate-Time-Series-Anomaly-Detection-and-Model-Selection-at-Scale" class="headerlink" title="mTSBench: Benchmarking Multivariate Time Series Anomaly Detection and   Model Selection at Scale"></a>mTSBench: Benchmarking Multivariate Time Series Anomaly Detection and   Model Selection at Scale</h2><p><strong>Authors:Xiaona Zhou, Constantin Brif, Ismini Lourentzou</strong></p>
<p>Multivariate time series anomaly detection (MTS-AD) is critical in domains like healthcare, cybersecurity, and industrial monitoring, yet remains challenging due to complex inter-variable dependencies, temporal dynamics, and sparse anomaly labels. We introduce mTSBench, the largest benchmark to date for MTS-AD and unsupervised model selection, spanning 344 labeled time series across 19 datasets and 12 diverse application domains. mTSBench evaluates 24 anomaly detection methods, including large language model (LLM)-based detectors for multivariate time series, and systematically benchmarks unsupervised model selection techniques under standardized conditions. Consistent with prior findings, our results confirm that no single detector excels across datasets, underscoring the importance of model selection. However, even state-of-the-art selection methods remain far from optimal, revealing critical gaps. mTSBench provides a unified evaluation suite to enable rigorous, reproducible comparisons and catalyze future advances in adaptive anomaly detection and robust model selection. </p>
<blockquote>
<p>å¤šå…ƒæ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹ï¼ˆMTS-ADï¼‰åœ¨åŒ»ç–—ä¿å¥ã€ç½‘ç»œå®‰å…¨å’Œå·¥ä¸šç›‘æ§ç­‰é¢†åŸŸè‡³å…³é‡è¦ï¼Œä½†ç”±äºå˜é‡ä¹‹é—´çš„å¤æ‚ä¾èµ–å…³ç³»ã€æ—¶é—´åŠ¨æ€å’Œç¨€ç–çš„å¼‚å¸¸æ ‡ç­¾ï¼Œä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æˆ‘ä»¬æ¨å‡ºäº†mTSBenchï¼Œè¿™æ˜¯è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„MTS-ADå’Œæ— ç›‘ç£æ¨¡å‹é€‰æ‹©åŸºå‡†ï¼Œæ¶µç›–äº†19ä¸ªæ•°æ®é›†çš„344ä¸ªæ ‡è®°æ—¶é—´åºåˆ—å’Œ12ä¸ªå¤šæ ·åŒ–çš„åº”ç”¨é¢†åŸŸã€‚mTSBenchè¯„ä¼°äº†24ç§å¼‚å¸¸æ£€æµ‹æ–¹æ³•ï¼ŒåŒ…æ‹¬åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤šå˜é‡æ—¶é—´åºåˆ—æ£€æµ‹å™¨ï¼Œå¹¶åœ¨æ ‡å‡†åŒ–æ¡ä»¶ä¸‹ç³»ç»Ÿåœ°è¯„ä¼°äº†æ— ç›‘ç£æ¨¡å‹é€‰æ‹©æŠ€æœ¯ã€‚ä¸ä¹‹å‰çš„ç ”ç©¶ç»“æœä¸€è‡´ï¼Œæˆ‘ä»¬çš„ç»“æœè¯å®ï¼Œæ²¡æœ‰ä¸€ç§æ£€æµ‹å™¨èƒ½åœ¨æ‰€æœ‰æ•°æ®é›†ä¸Šéƒ½è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œè¿™å¼ºè°ƒäº†æ¨¡å‹é€‰æ‹©çš„é‡è¦æ€§ã€‚ç„¶è€Œï¼Œå³ä½¿æ˜¯æœ€æ–°ä¸€ä»£çš„é€‰æ‹©æ–¹æ³•ä¹Ÿä»ç„¶è¿œè¿œä¸å¤Ÿç†æƒ³ï¼Œè¿™æ­ç¤ºäº†å…³é”®çš„å·®è·ã€‚mTSBenchæä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„è¯„ä¼°å¥—ä»¶ï¼Œèƒ½å¤Ÿå®ç°ä¸¥æ ¼ä¸”å¯é‡å¤çš„æ¯”è¾ƒï¼Œå¹¶æ¨åŠ¨æœªæ¥åœ¨è‡ªé€‚åº”å¼‚å¸¸æ£€æµ‹å’Œç¨³å¥æ¨¡å‹é€‰æ‹©æ–¹é¢çš„è¿›å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.21550v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>MTS-ADåœ¨åŒ»ç–—ä¿å¥ã€ç½‘ç»œå®‰å…¨å’Œå·¥ä¸šç›‘æ§ç­‰é¢†åŸŸè‡³å…³é‡è¦ï¼Œä½†å¤æ‚å¤šå˜é‡ä¾èµ–å…³ç³»ã€æ—¶é—´åŠ¨æ€å’Œç¨€ç–å¼‚å¸¸æ ‡ç­¾ä½¿å…¶é¢ä¸´æŒ‘æˆ˜ã€‚æˆ‘ä»¬æ¨å‡ºmTSBenchï¼Œè¿™æ˜¯è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„MTS-ADå’Œæ— ç›‘ç£æ¨¡å‹é€‰æ‹©åŸºå‡†æµ‹è¯•å¹³å°ï¼Œæ¶µç›–19ä¸ªæ•°æ®é›†çš„344ä¸ªæ ‡è®°æ—¶é—´åºåˆ—å’Œ12ä¸ªä¸åŒçš„åº”ç”¨é¢†åŸŸã€‚mTSBenchè¯„ä¼°äº†åŒ…æ‹¬åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å†…çš„å¤šç§å¼‚å¸¸æ£€æµ‹æ–¹æ³•ï¼Œå¹¶åœ¨æ ‡å‡†åŒ–æ¡ä»¶ä¸‹ç³»ç»Ÿåœ°è¯„ä¼°äº†æ— ç›‘ç£æ¨¡å‹é€‰æ‹©æŠ€æœ¯ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæ²¡æœ‰ä¸€ç§æ£€æµ‹å™¨åœ¨æ‰€æœ‰æ•°æ®é›†ä¸Šéƒ½è¡¨ç°ä¼˜ç§€ï¼Œçªæ˜¾æ¨¡å‹é€‰æ‹©çš„é‡è¦æ€§ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æœ€ä½³æ¨¡å‹é€‰æ‹©æ–¹æ³•ä»ä¸ç†æƒ³ï¼Œå­˜åœ¨å…³é”®å·®è·ã€‚mTSBenchæä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„è¯„ä¼°å¥—ä»¶ï¼Œå¯å®ç°ä¸¥æ ¼çš„å¯é‡å¤æ€§æ¯”è¾ƒï¼Œå¹¶æ¨åŠ¨è‡ªé€‚åº”å¼‚å¸¸æ£€æµ‹å’Œç¨³å¥æ¨¡å‹é€‰æ‹©çš„æœªæ¥å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>mTSBenchæ˜¯å¤šå…ƒæ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹ï¼ˆMTS-ADï¼‰å’Œæ— ç›‘ç£æ¨¡å‹é€‰æ‹©çš„åŸºå‡†æµ‹è¯•å¹³å°ã€‚</li>
<li>MTS-ADåœ¨å¤šä¸ªé¢†åŸŸè‡³å…³é‡è¦ï¼Œä½†å­˜åœ¨å¤æ‚å¤šå˜é‡ä¾èµ–å…³ç³»ç­‰æŒ‘æˆ˜ã€‚</li>
<li>mTSBenchæ¶µç›–äº†å¤šä¸ªæ•°æ®é›†å’Œé¢†åŸŸçš„å¹¿æ³›æ—¶é—´åºåˆ—æ•°æ®ã€‚</li>
<li>å¤šç§å¼‚å¸¸æ£€æµ‹æ–¹æ³•åœ¨mTSBenchä¸Šå¾—åˆ°è¯„ä¼°ï¼ŒåŒ…æ‹¬åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–¹æ³•ã€‚</li>
<li>æ²¡æœ‰å•ä¸€æ£€æµ‹å™¨åœ¨æ‰€æœ‰æ•°æ®é›†ä¸Šéƒ½è¡¨ç°ä¼˜ç§€ï¼Œçªæ˜¾æ¨¡å‹é€‰æ‹©çš„é‡è¦æ€§ã€‚</li>
<li>ç°æœ‰çš„æœ€ä½³æ¨¡å‹é€‰æ‹©æ–¹æ³•ä»ä¸ç†æƒ³ï¼Œå­˜åœ¨å…³é”®å·®è·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.21550">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-118757aaca3cac570e4d57c9aff7eca0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a216e566689c2dd57edd8eab3ce540b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-06df39e4b5554d4a6adab91da5cf0576.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-15d51cc5d0180b54e79f314b2a6c3ef7.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="PsyLite-Technical-Report"><a href="#PsyLite-Technical-Report" class="headerlink" title="PsyLite Technical Report"></a>PsyLite Technical Report</h2><p><strong>Authors:Fangjun Ding, Renyu Zhang, Xinyu Feng, Chengye Xie, Zheng Zhang, Yanting Zhang</strong></p>
<p>With the rapid development of digital technology, AI-driven psychological counseling has gradually become an important research direction in the field of mental health. However, existing models still have deficiencies in dialogue safety, detailed scenario handling, and lightweight deployment. To address these issues, this study proposes PsyLite, a lightweight psychological counseling large language model agent developed based on the base model InternLM2.5-7B-chat. Through a two-stage training strategy (hybrid distillation data fine-tuning and ORPO preference optimization), PsyLite enhances the modelâ€™s deep-reasoning ability, psychological counseling ability, and safe dialogue ability. After deployment using Ollama and Open WebUI, a custom workflow is created with Pipelines. An innovative conditional RAG is designed to introduce crosstalk humor elements at appropriate times during psychological counseling to enhance user experience and decline dangerous requests to strengthen dialogue safety. Evaluations show that PsyLite outperforms the baseline models in the Chinese general evaluation (CEval), psychological counseling professional evaluation (CPsyCounE), and dialogue safety evaluation (SafeDialBench), particularly in psychological counseling professionalism (CPsyCounE score improvement of 47.6%) and dialogue safety (\safe{} score improvement of 2.4%). Additionally, the model uses quantization technology (GGUF q4_k_m) to achieve low hardware deployment (5GB memory is sufficient for operation), providing a feasible solution for psychological counseling applications in resource-constrained environments. </p>
<blockquote>
<p>éšç€æ•°å­—æŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼ŒAIé©±åŠ¨çš„å¿ƒç†å’¨è¯¢åœ¨å¿ƒç†å¥åº·é¢†åŸŸé€æ¸æˆä¸ºä¸€ä¸ªé‡è¦çš„ç ”ç©¶æ–¹å‘ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ¨¡å‹åœ¨å¯¹è¯å®‰å…¨ã€å…·ä½“æƒ…æ™¯å¤„ç†ä»¥åŠè½»é‡åŒ–éƒ¨ç½²æ–¹é¢ä»ç„¶å­˜åœ¨ç¼ºé™·ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬ç ”ç©¶æå‡ºäº†åŸºäºåŸºç¡€æ¨¡å‹InternLM2.5-7B-chatçš„è½»é‡åŒ–å¿ƒç†å’¨è¯¢å¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†â€”â€”PsyLiteã€‚é€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼ˆæ··åˆè’¸é¦æ•°æ®å¾®è°ƒä¸ORPOåå¥½ä¼˜åŒ–ï¼‰ï¼ŒPsyLiteæå‡äº†æ¨¡å‹çš„æ·±åº¦æ¨ç†èƒ½åŠ›ã€å¿ƒç†å’¨è¯¢èƒ½åŠ›å’Œå®‰å…¨å¯¹è¯èƒ½åŠ›ã€‚ä½¿ç”¨Ollamaå’ŒOpen WebUIè¿›è¡Œéƒ¨ç½²åï¼Œé€šè¿‡Pipelinesåˆ›å»ºäº†è‡ªå®šä¹‰å·¥ä½œæµç¨‹ã€‚è®¾è®¡äº†ä¸€ç§åˆ›æ–°çš„æœ‰æ¡ä»¶RAGï¼Œåœ¨å¿ƒç†å’¨è¯¢çš„é€‚å½“æ—¶å€™å¼•å…¥è·¨å¯¹è¯å¹½é»˜å…ƒç´ ï¼Œä»¥å¢å¼ºç”¨æˆ·ä½“éªŒå¹¶åŠ å¼ºå±é™©è¯·æ±‚çš„æ‹’ç»æ¥å¼ºåŒ–å¯¹è¯å®‰å…¨ã€‚è¯„ä¼°æ˜¾ç¤ºï¼ŒPsyLiteåœ¨ä¸­å›½é€šç”¨è¯„ä¼°ï¼ˆCEvalï¼‰ã€å¿ƒç†å’¨è¯¢ä¸“ä¸šè¯„ä¼°ï¼ˆCPsyCounEï¼‰å’Œå¯¹è¯å®‰å…¨è¯„ä¼°ï¼ˆSafeDialBenchï¼‰æ–¹é¢è¶…è¶Šäº†åŸºçº¿æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨å¿ƒç†å’¨è¯¢ä¸“ä¸šï¼ˆCPsyCounEå¾—åˆ†æé«˜äº†47.6%ï¼‰å’Œå¯¹è¯å®‰å…¨ï¼ˆSafeå¾—åˆ†æé«˜äº†2.4%ï¼‰ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹é‡‡ç”¨é‡åŒ–æŠ€æœ¯ï¼ˆGGUF q4_k_mï¼‰å®ç°ä½ç¡¬ä»¶éƒ¨ç½²ï¼ˆä»…éœ€5GBå†…å­˜å³å¯è¿è¡Œï¼‰ï¼Œä¸ºèµ„æºå—é™ç¯å¢ƒä¸­çš„å¿ƒç†å’¨è¯¢åº”ç”¨æä¾›äº†å¯è¡Œçš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.21536v1">PDF</a> </p>
<p><strong>Summary</strong><br>     éšç€æ•°å­—æŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼ŒAIé©±åŠ¨çš„å¿ƒç†å’¨è¯¢é€æ¸æˆä¸ºå¿ƒç†å¥åº·é¢†åŸŸçš„é‡è¦ç ”ç©¶æ–¹å‘ã€‚æœ¬ç ”ç©¶æå‡ºåŸºäºåŸºç¡€æ¨¡å‹InternLM2.5-7B-chatçš„è½»é‡çº§å¿ƒç†å’¨è¯¢å¤§è¯­è¨€æ¨¡å‹ä»£ç†PsyLiteï¼Œé€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥å’Œåˆ›æ–°æ¡ä»¶RAGè®¾è®¡ï¼Œæå‡æ¨¡å‹çš„æ·±åº¦æ¨ç†èƒ½åŠ›ã€å¿ƒç†å’¨è¯¢èƒ½åŠ›å’Œå®‰å…¨å¯¹è¯èƒ½åŠ›ã€‚è¯„ä»·æ˜¾ç¤ºï¼ŒPsyLiteåœ¨ä¸“ä¸šæ€§è¯„ä¼°å’Œå®‰å…¨å¯¹è¯æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼ŒåŒæ—¶é‡‡ç”¨é‡åŒ–æŠ€æœ¯å®ç°ä½ç¡¬ä»¶éƒ¨ç½²ï¼Œä¸ºèµ„æºå—é™ç¯å¢ƒä¸­çš„å¿ƒç†å’¨è¯¢åº”ç”¨æä¾›å¯è¡Œè§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AIé©±åŠ¨çš„å¿ƒç†å’¨è¯¢æˆä¸ºå¿ƒç†å¥åº·é¢†åŸŸçš„é‡è¦ç ”ç©¶æ–¹å‘ã€‚</li>
<li>PsyLiteæ˜¯åŸºäºåŸºç¡€æ¨¡å‹InternLM2.5-7B-chatçš„è½»é‡çº§å¿ƒç†å’¨è¯¢å¤§è¯­è¨€æ¨¡å‹ä»£ç†ã€‚</li>
<li>PsyLiteé€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥å¢å¼ºæ¨¡å‹çš„æ·±åº¦æ¨ç†ã€å¿ƒç†å’¨è¯¢å’Œå®‰å…¨å¯¹è¯èƒ½åŠ›ã€‚</li>
<li>PsyLiteè®¾è®¡åˆ›æ–°æ¡ä»¶RAGä»¥å¼•å…¥å¯¹è¯å¹½é»˜å…ƒç´ ï¼Œæå‡ç”¨æˆ·ä½“éªŒå’Œå¼ºåŒ–å¯¹è¯å®‰å…¨ã€‚</li>
<li>PsyLiteåœ¨ä¸“ä¸šæ€§è¯„ä¼°ï¼ˆCPsyCounEè¯„åˆ†æå‡47.6%ï¼‰å’Œå®‰å…¨å¯¹è¯è¯„ä¼°ï¼ˆSafeDialBenchè¯„åˆ†æå‡2.4%ï¼‰æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>PsyLiteé‡‡ç”¨é‡åŒ–æŠ€æœ¯å®ç°ä½ç¡¬ä»¶éƒ¨ç½²ï¼Œä»…éœ€5GBå†…å­˜å³å¯è¿è¡Œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.21536">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-991a4ec68cbaab0df4655dae8b37e315.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1fa9aa639d2270a14ad219139f7f6855.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Exploring-the-Design-Space-of-3D-MLLMs-for-CT-Report-Generation"><a href="#Exploring-the-Design-Space-of-3D-MLLMs-for-CT-Report-Generation" class="headerlink" title="Exploring the Design Space of 3D MLLMs for CT Report Generation"></a>Exploring the Design Space of 3D MLLMs for CT Report Generation</h2><p><strong>Authors:Mohammed Baharoon, Jun Ma, Congyu Fang, Augustin Toma, Bo Wang</strong></p>
<p>Multimodal Large Language Models (MLLMs) have emerged as a promising way to automate Radiology Report Generation (RRG). In this work, we systematically investigate the design space of 3D MLLMs, including visual input representation, projectors, Large Language Models (LLMs), and fine-tuning techniques for 3D CT report generation. We also introduce two knowledge-based report augmentation methods that improve performance on the GREEN score by up to 10%, achieving the 2nd place on the MICCAI 2024 AMOS-MM challenge. Our results on the 1,687 cases from the AMOS-MM dataset show that RRG is largely independent of the size of LLM under the same training protocol. We also show that larger volume size does not always improve performance if the original ViT was pre-trained on a smaller volume size. Lastly, we show that using a segmentation mask along with the CT volume improves performance. The code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/bowang-lab/AMOS-MM-Solution">https://github.com/bowang-lab/AMOS-MM-Solution</a> </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å‡ºç°ä¸ºå®ç°è‡ªåŠ¨åŒ–æ”¾å°„æŠ¥å‘Šç”Ÿæˆï¼ˆRRGï¼‰æä¾›äº†ä¸€ç§æœ‰å‰æ™¯çš„æ–¹æ³•ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°ç ”ç©¶äº†3D MLLMçš„è®¾è®¡ç©ºé—´ï¼ŒåŒ…æ‹¬è§†è§‰è¾“å…¥è¡¨ç¤ºã€æŠ•å½±ä»ªã€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»¥åŠç”¨äº3D CTæŠ¥å‘Šç”Ÿæˆçš„å¾®è°ƒæŠ€æœ¯ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†ä¸¤ç§åŸºäºçŸ¥è¯†çš„æŠ¥å‘Šå¢å¼ºæ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•å°†GREENè¯„åˆ†æé«˜äº†é«˜è¾¾10%ï¼Œå¹¶åœ¨MICCAI 2024 AMOS-MMæŒ‘æˆ˜ä¸­è·å¾—äº†ç¬¬äºŒåã€‚æˆ‘ä»¬åœ¨AMOS-MMæ•°æ®é›†ä¸Šçš„1687ä¸ªæ¡ˆä¾‹çš„ç»“æœè¡¨æ˜ï¼Œåœ¨åŒä¸€è®­ç»ƒåè®®ä¸‹ï¼ŒRRGåœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šç‹¬ç«‹äºLLMçš„å¤§å°ã€‚æˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œå¦‚æœåŸå§‹çš„ViTæ˜¯åœ¨è¾ƒå°çš„ä½“ç§¯å¤§å°ä¸Šè¿›è¡Œé¢„è®­ç»ƒçš„ï¼Œé‚£ä¹ˆè¾ƒå¤§çš„ä½“ç§¯å¤§å°å¹¶ä¸ä¸€å®šèƒ½å¤Ÿæé«˜æ€§èƒ½ã€‚æœ€åï¼Œæˆ‘ä»¬è¯æ˜äº†ä½¿ç”¨CTä½“ç§¯ä¸åˆ†å‰²æ©ç ç›¸ç»“åˆå¯ä»¥æé«˜æ€§èƒ½ã€‚ä»£ç å·²å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/bowang-lab/AMOS-MM-Solution%E3%80%82">https://github.com/bowang-lab/AMOS-MM-Solutionã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.21535v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ç ”ç©¶äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è‡ªåŠ¨æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆï¼ˆRRGï¼‰ä¸­çš„åº”ç”¨ã€‚ç³»ç»Ÿæ¢è®¨äº†ä¸‰ç»´MLLMsçš„è®¾è®¡ç©ºé—´ï¼ŒåŒ…æ‹¬è§†è§‰è¾“å…¥è¡¨ç¤ºã€æŠ•å½±ä»ªã€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œé’ˆå¯¹ä¸‰ç»´CTæŠ¥å‘Šç”Ÿæˆçš„å¾®è°ƒæŠ€æœ¯ã€‚æ­¤å¤–ä»‹ç»äº†ä¸¤ç§åŸºäºçŸ¥è¯†çš„æŠ¥å‘Šå¢å¼ºæ–¹æ³•ï¼Œæé«˜äº†åœ¨GREENè¯„åˆ†ä¸Šçš„æ€§èƒ½ï¼Œå¹¶åœ¨MICCAI 2024 AMOS-MMæŒ‘æˆ˜ä¸­è·å¾—äº†ç¬¬äºŒåã€‚åœ¨AMOS-MMæ•°æ®é›†ä¸Šçš„ç»“æœæ˜¾ç¤ºï¼Œåœ¨ç›¸åŒçš„è®­ç»ƒåè®®ä¸‹ï¼ŒRRGåœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šç‹¬ç«‹äºLLMçš„å¤§å°ã€‚å¹¶ä¸”å±•ç¤ºäº†å¯¹é¢„è®­ç»ƒä½¿ç”¨çš„åŸå§‹ä½“ç§¯å¤§å°å¹¶ä¸ä¸€å®šæ€»æ˜¯å½±å“æ€§èƒ½ï¼›åŒæ—¶åˆ©ç”¨åˆ†å‰²æ©ç ä¸CTä½“ç§¯æ•°æ®å¯æé«˜æ€§èƒ½ã€‚ä»£ç å·²å…¬å¼€äº[GitHubåœ°å€]ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆï¼ˆRRGï¼‰é¢†åŸŸå±•ç°å‡ºæ½œåŠ›ã€‚</li>
<li>ç ”ç©¶äº†ä¸‰ç»´MLLMsçš„è®¾è®¡ç©ºé—´ï¼Œæ¶µç›–è§†è§‰è¾“å…¥è¡¨ç¤ºã€æŠ•å½±ä»ªå’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ã€‚</li>
<li>å¼•å…¥ä¸¤ç§çŸ¥è¯†å¢å¼ºæŠ¥å‘Šæ–¹æ³•ï¼Œæå‡æ€§èƒ½å¹¶åœ¨MICCAIæŒ‘æˆ˜ä¸­å–å¾—ä¼˜å¼‚æˆç»©ã€‚</li>
<li>åœ¨AMOS-MMæ•°æ®é›†ä¸Šçš„ç ”ç©¶æ˜¾ç¤ºï¼ŒRRGçš„è¡¨ç°åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šç‹¬ç«‹äºLLMçš„å¤§å°ï¼Œéµå¾ªç›¸åŒçš„è®­ç»ƒåè®®ã€‚</li>
<li>ä½“ç§¯å¤§å°å¹¶éå§‹ç»ˆå½±å“æ€§èƒ½çš„å…³é”®å› ç´ ï¼Œé¢„è®­ç»ƒçš„åŸå§‹ä½“ç§¯å¤§å°ä¸å¿…è¿‡å¤§ã€‚</li>
<li>ä½¿ç”¨åˆ†å‰²æ©ç ä¸CTä½“ç§¯æ•°æ®èƒ½æå‡æ¨¡å‹æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.21535">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bb69e031010be7b4f44bfd72e9268570.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-671d20832c0897e7ab2b10a78f1df217.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-34595bcb9b3df1b66b3ea82b36d3ac7b.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="â€œWhatâ€™s-Up-Doc-â€-Analyzing-How-Users-Seek-Health-Information-in-Large-Scale-Conversational-AI-Datasets"><a href="#â€œWhatâ€™s-Up-Doc-â€-Analyzing-How-Users-Seek-Health-Information-in-Large-Scale-Conversational-AI-Datasets" class="headerlink" title="â€œWhatâ€™s Up, Doc?â€: Analyzing How Users Seek Health Information in   Large-Scale Conversational AI Datasets"></a>â€œWhatâ€™s Up, Doc?â€: Analyzing How Users Seek Health Information in   Large-Scale Conversational AI Datasets</h2><p><strong>Authors:Akshay Paruchuri, Maryam Aziz, Rohit Vartak, Ayman Ali, Best Uchehara, Xin Liu, Ishan Chatterjee, Monica Agrawal</strong></p>
<p>People are increasingly seeking healthcare information from large language models (LLMs) via interactive chatbots, yet the nature and inherent risks of these conversations remain largely unexplored. In this paper, we filter large-scale conversational AI datasets to achieve HealthChat-11K, a curated dataset of 11K real-world conversations composed of 25K user messages. We use HealthChat-11K and a clinician-driven taxonomy for how users interact with LLMs when seeking healthcare information in order to systematically study user interactions across 21 distinct health specialties. Our analysis reveals insights into the nature of how and why users seek health information, such as common interactions, instances of incomplete context, affective behaviors, and interactions (e.g., leading questions) that can induce sycophancy, underscoring the need for improvements in the healthcare support capabilities of LLMs deployed as conversational AI. Code and artifacts to retrieve our analyses and combine them into a curated dataset can be found here: <a target="_blank" rel="noopener" href="https://github.com/yahskapar/HealthChat">https://github.com/yahskapar/HealthChat</a> </p>
<blockquote>
<p>äººä»¬è¶Šæ¥è¶Šå¤šåœ°é€šè¿‡äº¤äº’å¼èŠå¤©æœºå™¨äººä»å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­å¯»æ‰¾åŒ»ç–—ä¿¡æ¯ï¼Œä½†è¿™äº›å¯¹è¯çš„æ€§è´¨å’Œæ½œåœ¨é£é™©åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä»æœªè¢«æ¢ç´¢ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è¿‡æ»¤å¤§è§„æ¨¡å¯¹è¯AIæ•°æ®é›†ï¼Œæ„å»ºå¥åº·èŠå¤©ï¼ˆHealthChatï¼‰-11Kæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«ç”±ç”¨æˆ·æ¶ˆæ¯ç»„æˆçš„ç°å®å¯¹è¯çš„ç²¾é€‰é›†åˆï¼Œå…±è®¡åŒ…å«11Kå¯¹è¯å’Œ25Kæ¡ç”¨æˆ·æ¶ˆæ¯ã€‚æˆ‘ä»¬ä½¿ç”¨å¥åº·èŠå¤©ï¼ˆHealthChatï¼‰-11Kæ•°æ®é›†å’Œä¸´åºŠåŒ»ç”Ÿä¸»å¯¼çš„åˆ†ç±»æ³•æ¥ç ”ç©¶ç”¨æˆ·åœ¨å¯»æ±‚åŒ»ç–—ä¿¡æ¯æ—¶å¦‚ä½•ä¸LLMè¿›è¡Œäº¤äº’ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬èƒ½å¤Ÿç³»ç»Ÿåœ°åˆ†ææ¶‰åŠ21ä¸ªä¸åŒå¥åº·é¢†åŸŸçš„ç”¨æˆ·äº¤äº’ã€‚æˆ‘ä»¬çš„åˆ†ææ­ç¤ºäº†ç”¨æˆ·å¦‚ä½•åŠä¸ºä½•å¯»æ±‚å¥åº·ä¿¡æ¯çš„æ€§è´¨çš„ä¸€äº›è§è§£ï¼Œå¦‚å¸¸è§äº¤äº’ã€ç¼ºå°‘ä¸Šä¸‹æ–‡çš„æƒ…å†µã€æƒ…æ„Ÿè¡Œä¸ºå’Œèƒ½å¤Ÿå¼•èµ·ä¼ªé¡ºè¡Œä¸ºç­‰çš„äº¤äº’ï¼ˆä¾‹å¦‚æé—®æ—¶çš„å€¾å‘æ€§ï¼‰ï¼Œå¼ºè°ƒäº†åœ¨éƒ¨ç½²ä½œä¸ºå¯¹è¯AIçš„å¤§å‹è¯­è¨€æ¨¡å‹æ—¶å¯¹å…¶åŒ»ç–—æ”¯æŒèƒ½åŠ›è¿›è¡Œæ”¹è¿›çš„éœ€è¦ã€‚ä»£ç å’Œç›¸å…³æ–‡ç‰©å¯ç”¨äºæ£€ç´¢æˆ‘ä»¬çš„åˆ†æå¹¶å°†å…¶åˆå¹¶ä¸ºä¸€ä¸ªç²¾é€‰æ•°æ®é›†ï¼Œè¯¦æƒ…å¯è§ï¼š<a target="_blank" rel="noopener" href="https://github.com/yahskapar/HealthChat">https://github.com/yahskapar/HealthChat</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.21532v1">PDF</a> 25 pages, 6 figures, 4 tables, corresponds to initial HealthChat-11K   dataset release</p>
<p><strong>Summary</strong>ï¼šéšç€äººä»¬å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡äº¤äº’å¼èŠå¤©æœºå™¨äººè·å–åŒ»ç–—ä¿¡æ¯çš„éœ€æ±‚å¢åŠ ï¼Œç›¸å…³å¯¹è¯çš„æ€§è´¨å’Œæ½œåœ¨é£é™©å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬ç ”ç©¶é€šè¿‡ç­›é€‰å¤§è§„æ¨¡å¯¹è¯AIæ•°æ®é›†ï¼Œæ„å»ºäº†å¥åº·èŠå¤©æ•°æ®é›†HealthChat-11Kï¼ŒåŒ…å«1.1ä¸‡æ¡çœŸå®å¯¹è¯å’Œ2.5ä¸‡æ¡ç”¨æˆ·æ¶ˆæ¯ã€‚åˆ©ç”¨HealthChat-11Kå’Œä¸´åºŠåŒ»ç”Ÿåˆ¶å®šçš„ç”¨æˆ·ä¸LLMäº¤äº’çš„åˆ†ç±»æ³•ï¼Œå¯¹æ¶‰åŠ21ç§ä¸åŒå¥åº·ä¸“ä¸šçš„ç”¨æˆ·äº¤äº’è¿›è¡Œäº†ç³»ç»Ÿç ”ç©¶ã€‚åˆ†ææ­ç¤ºäº†ç”¨æˆ·å¯»æ±‚å¥åº·ä¿¡æ¯çš„æ–¹å¼å’ŒåŸå› ï¼Œå¦‚å¸¸è§äº¤äº’ã€ä¸Šä¸‹æ–‡ä¸å®Œæ•´çš„æƒ…å†µã€æƒ…æ„Ÿè¡Œä¸ºå’Œè¯±å¯¼å¥‰æ‰¿çš„äº¤äº’ç­‰ã€‚è¿™å¼ºè°ƒäº†æ”¹è¿›éƒ¨ç½²ä¸ºå¯¹è¯å¼AIçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åŒ»ç–—ä¿å¥æ”¯æŒèƒ½åŠ›æ–¹é¢çš„å¿…è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>äººä»¬è¶Šæ¥è¶Šå¤šåœ°é€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œäº¤äº’å¼èŠå¤©æœºå™¨äººè·å–åŒ»ç–—ä¿¡æ¯ã€‚</li>
<li>å¥åº·èŠå¤©æ•°æ®é›†HealthChat-11Kç”±çœŸå®å¯¹è¯æ„æˆï¼Œç”¨äºç ”ç©¶ç”¨æˆ·ä¸LLMçš„äº¤äº’ã€‚</li>
<li>ç”¨æˆ·ä¸LLMäº¤äº’çš„æ–¹å¼æ¶‰åŠå¤šç§å¥åº·ä¸“ä¸šï¼Œç³»ç»Ÿç ”ç©¶å…·æœ‰å¿…è¦æ€§ã€‚</li>
<li>åˆ†ææ­ç¤ºäº†ç”¨æˆ·å¯»æ±‚å¥åº·ä¿¡æ¯æ—¶å¸¸è§äº¤äº’ã€ä¸Šä¸‹æ–‡ä¸å®Œæ•´ç­‰é—®é¢˜ã€‚</li>
<li>ç”¨æˆ·çš„æƒ…æ„Ÿè¡Œä¸ºå’ŒæŸäº›äº¤äº’æ–¹å¼ï¼ˆå¦‚å¼•å¯¼æ€§é—®é¢˜ï¼‰å¯èƒ½è¯±å¯¼ä¸å‡†ç¡®çš„å›ç­”ã€‚</li>
<li>è¿™äº›å‘ç°å¼ºè°ƒäº†æ”¹è¿›LLMåœ¨åŒ»ç–—ä¿å¥æ”¯æŒèƒ½åŠ›æ–¹é¢çš„éœ€æ±‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.21532">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6ff42f5dc758ae439675f70c5c946804.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-91afc16c153c45f70b6f6d519ad23b0f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-087a2fca46d9c6dee1c5589ea561ddd4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c080986609b5a771a14577e3626b2235.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c854fe7e2d729e5b9baf6bb039bd90b6.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Mind2Web-2-Evaluating-Agentic-Search-with-Agent-as-a-Judge"><a href="#Mind2Web-2-Evaluating-Agentic-Search-with-Agent-as-a-Judge" class="headerlink" title="Mind2Web 2: Evaluating Agentic Search with Agent-as-a-Judge"></a>Mind2Web 2: Evaluating Agentic Search with Agent-as-a-Judge</h2><p><strong>Authors:Boyu Gou, Zanming Huang, Yuting Ning, Yu Gu, Michael Lin, Weijian Qi, Andrei Kopanev, Botao Yu, Bernal JimÃ©nez GutiÃ©rrez, Yiheng Shu, Chan Hee Song, Jiaman Wu, Shijie Chen, Hanane Nour Moussa, Tianshu Zhang, Jian Xie, Yifei Li, Tianci Xue, Zeyi Liao, Kai Zhang, Boyuan Zheng, Zhaowei Cai, Viktor Rozgic, Morteza Ziyadi, Huan Sun, Yu Su</strong></p>
<p>Agentic search such as Deep Research systems, where large language models autonomously browse the web, synthesize information, and return comprehensive citation-backed answers, represents a major shift in how users interact with web-scale information. While promising greater efficiency and cognitive offloading, the growing complexity and open-endedness of agentic search have outpaced existing evaluation benchmarks and methodologies, which largely assume short search horizons and static answers. In this paper, we introduce Mind2Web 2, a benchmark of 130 realistic, high-quality, and long-horizon tasks that require real-time web browsing and extensive information synthesis, constructed with over 1,000 hours of human labor. To address the challenge of evaluating time-varying and complex answers, we propose a novel Agent-as-a-Judge framework. Our method constructs task-specific judge agents based on a tree-structured rubric design to automatically assess both answer correctness and source attribution. We conduct a comprehensive evaluation of nine frontier agentic search systems and human performance, along with a detailed error analysis to draw insights for future development. The best-performing system, OpenAI Deep Research, can already achieve 50-70% of human performance while spending half the time, showing a great potential. Altogether, Mind2Web 2 provides a rigorous foundation for developing and benchmarking the next generation of agentic search systems. </p>
<blockquote>
<p>åŸºäºæ·±åº¦ç ”ç©¶ç³»ç»Ÿï¼ˆå¦‚Deep Research systemsï¼‰çš„ä»£ç†æœç´¢ï¼Œå…¶ä¸­å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿè‡ªä¸»æµè§ˆç½‘ç»œã€åˆæˆä¿¡æ¯å¹¶è¿”å›æœ‰å¼•è¯æ”¯æŒçš„å…¨é¢ç­”æ¡ˆï¼Œä»£è¡¨äº†ç”¨æˆ·ä¸ç½‘é¡µè§„æ¨¡ä¿¡æ¯äº¤äº’æ–¹å¼çš„ä¸€å¤§è½¬å˜ã€‚è™½ç„¶æœ‰æœ›å¸¦æ¥æ›´é«˜çš„æ•ˆç‡å’Œè®¤çŸ¥å‡è´Ÿï¼Œä½†ä»£ç†æœç´¢çš„æ—¥ç›Šå¤æ‚æ€§å’Œå¼€æ”¾æ€§å·²ç»è¶…è¶Šäº†ç°æœ‰çš„è¯„ä¼°åŸºå‡†å’Œæ–¹æ³•è®ºï¼Œè¿™äº›æ–¹æ³•å¤§å¤šå‡è®¾æœç´¢è§†é‡è¾ƒçŸ­ä¸”ç­”æ¡ˆé™æ€ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Mind2Web 2ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«130ä¸ªç°å®æ€§å¼ºã€é«˜è´¨é‡ã€é•¿è§†çª—ä»»åŠ¡çš„åŸºå‡†æµ‹è¯•ï¼Œéœ€è¦å®æ—¶ç½‘é¡µæµè§ˆå’Œå¹¿æ³›çš„ä¿¡æ¯ç»¼åˆï¼Œå¹¶èŠ±è´¹äº†è¶…è¿‡1000å°æ—¶çš„äººåŠ›è¿›è¡Œæ„å»ºã€‚ä¸ºäº†åº”å¯¹è¯„ä¼°éšæ—¶é—´å˜åŒ–ä»¥åŠå¤æ‚ç­”æ¡ˆçš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„â€œAgent-as-a-Judgeâ€æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŸºäºæ ‘å½¢è¯„åˆ†è¡¨è®¾è®¡ï¼Œæ„å»ºç‰¹å®šä»»åŠ¡çš„è¯„ä¼°ä»£ç†ï¼Œä»¥è‡ªåŠ¨è¯„ä¼°ç­”æ¡ˆçš„æ­£ç¡®æ€§å’Œæ¥æºå½’å±ã€‚æˆ‘ä»¬å¯¹ä¹ä¸ªå‰æ²¿çš„ä»£ç†æœç´¢ç³»ç»Ÿå’Œäººç±»è¡¨ç°è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œå¹¶è¿›è¡Œäº†è¯¦ç»†çš„è¯¯å·®åˆ†æï¼Œä»¥è·å–å¯¹æœªæ¥å‘å±•çš„è§è§£ã€‚è¡¨ç°æœ€ä½³çš„OpenAI Deep Researchç³»ç»Ÿå·²ç»èƒ½å¤Ÿè¾¾åˆ°äººç±»è¡¨ç°çš„50%-70%ï¼ŒåŒæ—¶æ‰€èŠ±è´¹çš„æ—¶é—´åªæœ‰ä¸€åŠï¼Œæ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ã€‚æ€»çš„æ¥è¯´ï¼ŒMind2Web 2ä¸ºä¸‹ä¸€ä»£ä»£ç†æœç´¢ç³»ç»Ÿçš„å‘å±•å’ŒåŸºå‡†æµ‹è¯•æä¾›äº†åšå®çš„åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.21506v1">PDF</a> Project Homepage: <a target="_blank" rel="noopener" href="https://osu-nlp-group.github.io/Mind2Web2/">https://osu-nlp-group.github.io/Mind2Web2/</a></p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹è‡ªä¸»æµè§ˆç½‘ç»œã€åˆæˆä¿¡æ¯å¹¶è¿”å›æœ‰å¼•è¯æ”¯æŒçš„å…¨é¢ç­”æ¡ˆçš„æœç´¢æ–¹å¼ï¼Œå¦‚æ·±åº¦ç ”ç©¶ç³»ç»Ÿï¼Œä»£è¡¨äº†ç”¨æˆ·ä¸ç½‘é¡µè§„æ¨¡ä¿¡æ¯äº¤äº’æ–¹å¼çš„é‡å¤§è½¬å˜ã€‚æœ¬æ–‡ä»‹ç»äº†Mind2Web 2åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«130ä¸ªéœ€è¦å®æ—¶ç½‘ç»œæµè§ˆå’Œå¤§é‡ä¿¡æ¯åˆæˆçš„çœŸå®ã€é«˜è´¨é‡ã€é•¿æœŸä»»åŠ¡ã€‚ä¸ºåº”å¯¹è¯„ä¼°éšæ—¶é—´å˜åŒ–åŠå¤æ‚ç­”æ¡ˆçš„æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†æ–°é¢–çš„â€œä»£ç†ä½œä¸ºè¯„å§”â€æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŸºäºæ ‘å½¢è¯„åˆ†è¡¨è®¾è®¡ï¼Œæ„å»ºç‰¹å®šä»»åŠ¡çš„è¯„å§”ä»£ç†ï¼Œè‡ªåŠ¨è¯„ä¼°ç­”æ¡ˆçš„æ­£ç¡®æ€§åŠæ¥æºå½’å±ã€‚æœ¬æ–‡å…¨é¢è¯„ä¼°äº†å‰æ²¿çš„ä¹ä¸ªä»£ç†æœç´¢ç³»ç»ŸåŠå…¶ä¸äººç±»è¡¨ç°æ¯”è¾ƒï¼Œè¯¦ç»†åˆ†æäº†é”™è¯¯æ¥æºï¼Œä¸ºæœªæ¥ç ”å‘æä¾›äº†æ·±å…¥è§è§£ã€‚è¡¨ç°æœ€ä½³çš„OpenAIæ·±åº¦ç ”ç©¶ç³»ç»Ÿï¼Œèƒ½åœ¨è€—è´¹ä¸€åŠæ—¶é—´çš„æƒ…å†µä¸‹è¾¾åˆ°äººç±»è¡¨ç°çš„50-70%ï¼Œæ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚æ€»ä½“è€Œè¨€ï¼ŒMind2Web 2ä¸ºä¸‹ä¸€ä»£ä»£ç†æœç´¢ç³»ç»Ÿçš„å¼€å‘å’Œè¯„ä¼°æä¾›äº†ä¸¥æ ¼çš„åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹è‡ªä¸»æµè§ˆç½‘ç»œè¿›è¡Œä¿¡æ¯åˆæˆæ˜¯æœç´¢é¢†åŸŸçš„é‡å¤§è½¬å˜ã€‚</li>
<li>Mind2Web 2åŸºå‡†æµ‹è¯•åŒ…å«ç°å®ç”Ÿæ´»ä¸­çš„é•¿æœŸä»»åŠ¡ï¼Œåæ˜ çœŸå®ä¸–ç•Œçš„å¤æ‚æ€§ã€‚</li>
<li>æå‡ºâ€œä»£ç†ä½œä¸ºè¯„å§”â€æ¡†æ¶ä»¥åº”å¯¹è¯„ä¼°å¤æ‚ç­”æ¡ˆçš„æŒ‘æˆ˜ã€‚</li>
<li>ç‰¹å®šä»»åŠ¡çš„è¯„å§”ä»£ç†å¯è‡ªåŠ¨è¯„ä¼°ç­”æ¡ˆçš„æ­£ç¡®æ€§å’Œæ¥æºå½’å±ã€‚</li>
<li>å¯¹å‰æ²¿çš„ä¹ä¸ªä»£ç†æœç´¢ç³»ç»Ÿè¿›è¡Œå…¨é¢è¯„ä¼°å¹¶ä¸äººç±»è¡¨ç°å¯¹æ¯”ã€‚</li>
<li>OpenAIæ·±åº¦ç ”ç©¶ç³»ç»Ÿå±•ç°äº†å·¨å¤§æ½œåŠ›ï¼Œä¸äººç±»è¡¨ç°å·®è·ç¼©å°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.21506">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-07e2de5f2937cb150b5794be520c4d32.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-48c61a4bba35cdf7b42718f538dee9f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d8d976ca76f1ce8fbeae0d5572c9fee9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d31848de681e0baf7e3363cb8d0899ef.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Efficient-and-Reuseable-Cloud-Configuration-Search-Using-Discovery-Spaces"><a href="#Efficient-and-Reuseable-Cloud-Configuration-Search-Using-Discovery-Spaces" class="headerlink" title="Efficient and Reuseable Cloud Configuration Search Using Discovery   Spaces"></a>Efficient and Reuseable Cloud Configuration Search Using Discovery   Spaces</h2><p><strong>Authors:Michael Johnston, Burkhard Ringlein, Christoph Hagleitner, Alessandro Pomponio, Vassilis Vassiliadis, Christian Pinto, Srikumar Venugopal</strong></p>
<p>Finding the optimal set of cloud resources to deploy a given workload at minimal cost while meeting a defined service level agreement is an active area of research. Combining tens of parameters applicable across a large selection of compute, storage, and services offered by cloud providers with similar numbers of application-specific parameters leads to configuration spaces with millions of deployment options.   In this paper, we propose Discovery Space, an abstraction that formalizes the description of workload configuration problems, and exhibits a set of characteristics required for structured, robust and distributed investigations of large search spaces. We describe a concrete implementation of the Discovery Space abstraction and show that it is generalizable across a diverse set of workloads such as Large Language Model inference and Big Data Analytics.   We demonstrate that our approach enables safe, transparent sharing of data between executions of best-of-breed optimizers increasing the efficiency of optimal configuration detection in large search spaces. We also demonstrate how Discovery Spaces enable transfer and reuse of knowledge across similar search spaces, enabling configuration search speed-ups of over 90%. </p>
<blockquote>
<p>åœ¨äº‘èµ„æºä¸­å¯»æ‰¾æœ€ä¼˜é…ç½®ä»¥éƒ¨ç½²ç»™å®šå·¥ä½œé‡ï¼Œå¹¶åœ¨æ»¡è¶³æ—¢å®šæœåŠ¡æ°´å¹³åè®®çš„åŒæ—¶é™ä½æˆæœ¬æ˜¯å½“å‰ç ”ç©¶çš„çƒ­ç‚¹é¢†åŸŸã€‚äº‘æä¾›å•†æä¾›çš„ä¼—å¤šè®¡ç®—ã€å­˜å‚¨å’ŒæœåŠ¡é€‰é¡¹ä¸­æœ‰æ•°åä¸ªå‚æ•°ï¼Œä»¥åŠä¸ä¹‹ç›¸å…³çš„åº”ç”¨ç¨‹åºç‰¹å®šå‚æ•°ï¼Œå…±åŒæ„æˆäº†å…·æœ‰æ•°ç™¾ä¸‡éƒ¨ç½²é€‰é¡¹çš„é…ç½®ç©ºé—´ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Discovery Spaceçš„æ¦‚å¿µï¼Œè¿™æ˜¯ä¸€ä¸ªå½¢å¼åŒ–æè¿°å·¥ä½œé‡é…ç½®é—®é¢˜çš„æŠ½è±¡æ¦‚å¿µï¼Œå¹¶å±•ç¤ºäº†ä¸€ç³»åˆ—ç”¨äºç»“æ„åŒ–ã€ç¨³å¥å’Œåˆ†å¸ƒå¼æ¢ç´¢å¤§å‹æœç´¢ç©ºé—´æ‰€éœ€çš„ç‰¹æ€§ã€‚æˆ‘ä»¬æè¿°äº†Discovery SpaceæŠ½è±¡çš„å…·ä½“å®ç°ï¼Œå¹¶è¯æ˜å…¶åœ¨å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†å’Œå¤§æ•°æ®åˆ†æç­‰å¤šç§å·¥ä½œè´Ÿè½½ä¸­å…·æœ‰é€šç”¨æ€§ã€‚æˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿåœ¨æœ€ä½³ä¼˜åŒ–å™¨æ‰§è¡Œä¹‹é—´å®‰å…¨ã€é€æ˜åœ°å…±äº«æ•°æ®ï¼Œä»è€Œæé«˜å¤§å‹æœç´¢ç©ºé—´ä¸­æœ€ä½³é…ç½®æ£€æµ‹çš„æ•ˆç‡ã€‚æˆ‘ä»¬è¿˜å±•ç¤ºäº†Discovery Spaceå¦‚ä½•åœ¨ç±»ä¼¼æœç´¢ç©ºé—´ä¹‹é—´å®ç°çŸ¥è¯†å’Œç»éªŒçš„è½¬ç§»å’Œå†åˆ©ç”¨ï¼Œä»è€Œå®ç°é…ç½®æœç´¢é€Ÿåº¦æå‡è¶…è¿‡90%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.21467v1">PDF</a> </p>
<p><strong>Summary</strong><br>äº‘æœåŠ¡èµ„æºéƒ¨ç½²çš„ä¼˜åŒ–é—®é¢˜æ˜¯ä¸€ä¸ªç ”ç©¶çƒ­ç‚¹ã€‚é¢å¯¹ä¼—å¤šäº‘æä¾›å•†æä¾›çš„è®¡ç®—ã€å­˜å‚¨å’ŒæœåŠ¡å‚æ•°ï¼Œä»¥åŠåº”ç”¨ç‰¹å®šçš„å‚æ•°ï¼Œé…ç½®ç©ºé—´æ‹¥æœ‰æ•°ç™¾ä¸‡ç§éƒ¨ç½²é€‰é¡¹ã€‚æœ¬æ–‡æå‡ºDiscovery SpaceæŠ½è±¡ï¼Œä»¥å½¢å¼åŒ–æè¿°å·¥ä½œè´Ÿè½½é…ç½®é—®é¢˜ï¼Œå¹¶å±•ç¤ºå…¶ç”¨äºå¤§è§„æ¨¡æœç´¢ç©ºé—´çš„ç»“æ„åŒ–ã€ç¨³å¥å’Œåˆ†å¸ƒå¼è°ƒæŸ¥æ‰€å¿…éœ€çš„ä¸€ç»„ç‰¹æ€§ã€‚æˆ‘ä»¬çš„å®ç°æ–¹æ³•å¯ä»¥åº”ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†ã€å¤§æ•°æ®åˆ†æç­‰å¤šç§å·¥ä½œè´Ÿè½½ï¼Œå¹¶å¯æé«˜æœ€ä½³ä¼˜åŒ–å™¨çš„æ‰§è¡Œæ•ˆç‡ï¼Œå®ç°å¤§è§„æ¨¡æœç´¢ç©ºé—´ä¸­çš„æœ€ä½³é…ç½®æ£€æµ‹ã€‚æ­¤å¤–ï¼ŒDiscovery Spacesè¿˜å¯ä»¥å®ç°ç±»ä¼¼æœç´¢ç©ºé—´ä¹‹é—´çš„çŸ¥è¯†è¿ç§»å’Œå†åˆ©ç”¨ï¼ŒåŠ é€Ÿé…ç½®æœç´¢é€Ÿåº¦è¶…è¿‡90%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äº‘æœåŠ¡èµ„æºéƒ¨ç½²çš„ä¼˜åŒ–æ˜¯å½“å‰çš„çƒ­é—¨ç ”ç©¶é¢†åŸŸã€‚</li>
<li>å­˜åœ¨å¤§é‡çš„äº‘æä¾›å•†å‚æ•°å’Œåº”ç”¨ç‰¹å®šå‚æ•°ï¼Œå¯¼è‡´é…ç½®ç©ºé—´æ‹¥æœ‰æ•°ç™¾ä¸‡ç§éƒ¨ç½²é€‰é¡¹ã€‚</li>
<li>Discovery Spaceè¢«æå‡ºä½œä¸ºä¸€ç§æŠ½è±¡ï¼Œä»¥å½¢å¼åŒ–æè¿°å·¥ä½œè´Ÿè½½é…ç½®é—®é¢˜ã€‚</li>
<li>Discovery Spaceå…·æœ‰ç”¨äºå¤§è§„æ¨¡æœç´¢ç©ºé—´çš„è°ƒæŸ¥æ‰€å¿…éœ€çš„ç‰¹æ€§ã€‚</li>
<li>å…·ä½“å®æ–½æ–¹æ³•å¯ä»¥åº”ç”¨äºå¤šç§å·¥ä½œè´Ÿè½½ï¼Œå¦‚å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†å’Œå¤§æ•°æ®åˆ†æã€‚</li>
<li>è¯¥æ–¹æ³•æé«˜äº†æœ€ä½³ä¼˜åŒ–å™¨çš„æ‰§è¡Œæ•ˆç‡ï¼Œä½¿æœ€ä½³é…ç½®æ£€æµ‹åœ¨å¤§å‹æœç´¢ç©ºé—´ä¸­æ›´ä¸ºå®¹æ˜“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.21467">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-88b9f926f41b74c268ef006176ae177e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bbc58bf04964dfc0224d97dee61f9ba7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b8c6a222114d51fb09ca9a5c6c32b37.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1053a81d727f8a2e49a5b78bc98475f7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-79d1b64cc22a7073d5874097eeea98e5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5c1c685be9060133522c65aa794f8ac.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="ThinkSound-Chain-of-Thought-Reasoning-in-Multimodal-Large-Language-Models-for-Audio-Generation-and-Editing"><a href="#ThinkSound-Chain-of-Thought-Reasoning-in-Multimodal-Large-Language-Models-for-Audio-Generation-and-Editing" class="headerlink" title="ThinkSound: Chain-of-Thought Reasoning in Multimodal Large Language   Models for Audio Generation and Editing"></a>ThinkSound: Chain-of-Thought Reasoning in Multimodal Large Language   Models for Audio Generation and Editing</h2><p><strong>Authors:Huadai Liu, Jialei Wang, Kaicheng Luo, Wen Wang, Qian Chen, Zhou Zhao, Wei Xue</strong></p>
<p>While end-to-end video-to-audio generation has greatly improved, producing high-fidelity audio that authentically captures the nuances of visual content remains challenging. Like professionals in the creative industries, such generation requires sophisticated reasoning about items such as visual dynamics, acoustic environments, and temporal relationships. We present \textbf{ThinkSound}, a novel framework that leverages Chain-of-Thought (CoT) reasoning to enable stepwise, interactive audio generation and editing for videos. Our approach decomposes the process into three complementary stages: foundational foley generation that creates semantically coherent soundscapes, interactive object-centric refinement through precise user interactions, and targeted editing guided by natural language instructions. At each stage, a multimodal large language model generates contextually aligned CoT reasoning that guides a unified audio foundation model. Furthermore, we introduce \textbf{AudioCoT}, a comprehensive dataset with structured reasoning annotations that establishes connections between visual content, textual descriptions, and sound synthesis. Experiments demonstrate that ThinkSound achieves state-of-the-art performance in video-to-audio generation across both audio metrics and CoT metrics and excels in out-of-distribution Movie Gen Audio benchmark. The demo page is available at <a target="_blank" rel="noopener" href="https://thinksound-demo.github.io/">https://ThinkSound-Demo.github.io</a>. </p>
<blockquote>
<p>ç«¯åˆ°ç«¯è§†é¢‘åˆ°éŸ³é¢‘ç”ŸæˆæŠ€æœ¯è™½ç„¶å·²ç»æœ‰äº†å¾ˆå¤§çš„è¿›æ­¥ï¼Œä½†ç”Ÿæˆé«˜è´¨é‡ã€çœŸå®æ•æ‰è§†è§‰å†…å®¹ç»†å¾®ä¹‹å¤„çš„éŸ³é¢‘ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚å°±åƒåˆ›æ„äº§ä¸šçš„ä¸“ä¸šäººå£«ä¸€æ ·ï¼Œè¿™ç§ç”Ÿæˆéœ€è¦å¤æ‚åœ°æ¨ç†è¯¸å¦‚è§†è§‰åŠ¨æ€ã€å£°å­¦ç¯å¢ƒå’Œæ—¶é—´å…³ç³»ç­‰é¡¹ç›®ã€‚æˆ‘ä»¬æå‡ºäº†\textbf{ThinkSound}ï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨æ€ç»´é“¾ï¼ˆChain-of-Thoughtï¼Œç®€ç§°CoTï¼‰æ¨ç†çš„æ–°æ¡†æ¶ï¼Œå¯å®ç°é’ˆå¯¹è§†é¢‘çš„é€æ­¥äº¤äº’å¼éŸ³é¢‘ç”Ÿæˆå’Œç¼–è¾‘ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†å…¶åˆ†è§£ä¸ºä¸‰ä¸ªäº’è¡¥çš„é˜¶æ®µï¼šåˆ›å»ºè¯­ä¹‰è¿è´¯å£°éŸ³æ™¯è§‚çš„åŸºç¡€éŸ³æ•ˆç”Ÿæˆã€é€šè¿‡ç²¾ç¡®ç”¨æˆ·äº’åŠ¨è¿›è¡Œäº¤äº’å¼å¯¹è±¡ä¸­å¿ƒç»†åŒ–ä»¥åŠç”±è‡ªç„¶è¯­è¨€æŒ‡ä»¤å¼•å¯¼çš„æœ‰é’ˆå¯¹æ€§ç¼–è¾‘ã€‚åœ¨æ¯ä¸ªé˜¶æ®µï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹éƒ½ä¼šäº§ç”Ÿä¸ä¸Šä¸‹æ–‡ç›¸å…³çš„CoTæ¨ç†ï¼Œä»¥æŒ‡å¯¼ç»Ÿä¸€çš„éŸ³é¢‘åŸºç¡€æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¨å‡ºäº†\textbf{AudioCoT}ï¼Œè¿™æ˜¯ä¸€ä¸ªå¸¦æœ‰ç»“æ„åŒ–æ¨ç†æ³¨é‡Šçš„ç»¼åˆæ•°æ®é›†ï¼Œæ—¨åœ¨å»ºç«‹è§†è§‰å†…å®¹ã€æ–‡æœ¬æè¿°å’Œå£°éŸ³åˆæˆä¹‹é—´çš„è”ç³»ã€‚å®éªŒè¡¨æ˜ï¼ŒThinkSoundåœ¨éŸ³é¢‘æŒ‡æ ‡å’ŒCoTæŒ‡æ ‡çš„è§†é¢‘åˆ°éŸ³é¢‘ç”Ÿæˆæ–¹é¢éƒ½è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œå¹¶åœ¨ç”µå½±åŸºå› éŸ³é¢‘åŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°å°¤ä¸ºå‡ºè‰²ã€‚æ¼”ç¤ºé¡µé¢å¯åœ¨<a target="_blank" rel="noopener" href="https://thinksound-demo.github.ioæµè§ˆ./">https://ThinkSound-Demo.github.ioæµè§ˆã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.21448v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤šåª’ä½“æ—¶ä»£çš„éŸ³é¢‘ç”ŸæˆæŠ€æœ¯é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯å¦‚ä½•æ ¹æ®è§†é¢‘å†…å®¹ç”ŸæˆçœŸå®ã€é«˜è´¨é‡çš„å£°éŸ³ã€‚ç°åœ¨å‡ºç°äº†ä¸€ä¸ªåä¸ºThinkSoundçš„æ–°æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨é“¾å¼æ€ç»´æ¨ç†æ¥å®ç°è§†é¢‘çš„å£°éŸ³åˆ†é˜¶æ®µç”Ÿæˆå’Œç¼–è¾‘ã€‚è¿™ä¸ªæ¡†æ¶æœ‰ä¸‰ä¸ªé‡è¦ç¯èŠ‚ï¼šå»ºç«‹åŸºç¡€éŸ³æ™¯ã€äº’åŠ¨å¯¹è±¡ç²¾ç»†åŒ–åŠç›®æ ‡ç¼–è¾‘æŒ‡å¯¼ã€‚åœ¨æ¯ä¸€æ­¥ï¼Œä¸€ä¸ªè·¨æ¨¡æ€çš„å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆè¯­å¢ƒç›¸å…³çš„é“¾å¼æ€ç»´æ¨ç†ï¼Œå¼•å¯¼ç»Ÿä¸€çš„éŸ³é¢‘åŸºç¡€æ¨¡å‹ã€‚åŒæ—¶å¼•å…¥äº†AudioCoTæ•°æ®é›†ï¼Œé€šè¿‡ç»“æ„åŒ–çš„æ¨ç†æ³¨é‡Šå»ºç«‹è§†è§‰å†…å®¹ã€æ–‡æœ¬æè¿°å’Œå£°éŸ³åˆæˆä¹‹é—´çš„è”ç³»ã€‚å®éªŒè¯æ˜ï¼ŒThinkSoundåœ¨è§†é¢‘è½¬éŸ³é¢‘ç”Ÿæˆæ–¹é¢è¾¾åˆ°æœ€ä½³çŠ¶æ€ï¼Œç‰¹åˆ«æ˜¯åœ¨Movie Gen Audioæµ‹è¯•ä¸­è¡¨ç°çªå‡ºã€‚å…·ä½“ç»†èŠ‚å¯é€šè¿‡è®¿é—®å…¶æ¼”ç¤ºé¡µé¢äº†è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è§†é¢‘è½¬éŸ³é¢‘ç”ŸæˆæŠ€æœ¯åœ¨å¤šåª’ä½“æ—¶ä»£é¢ä¸´é«˜ä¿çœŸåº¦éŸ³é¢‘ç”Ÿæˆçš„æŒ‘æˆ˜ã€‚</li>
<li>ThinkSoundæ¡†æ¶é‡‡ç”¨é“¾å¼æ€ç»´æ¨ç†å®ç°è§†é¢‘éŸ³é¢‘çš„åˆ†æ­¥ç”Ÿæˆå’Œç¼–è¾‘ã€‚</li>
<li>ThinkSoundåŒ…å«ä¸‰ä¸ªæ ¸å¿ƒç¯èŠ‚ï¼šå»ºç«‹åŸºç¡€éŸ³æ™¯ã€äº’åŠ¨å¯¹è±¡ç²¾ç»†åŒ–åŠç›®æ ‡ç¼–è¾‘æŒ‡å¯¼ã€‚</li>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¯ä¸€æ­¥ä¸ºéŸ³é¢‘åŸºç¡€æ¨¡å‹æä¾›è¯­å¢ƒç›¸å…³çš„æ¨ç†æŒ‡å¯¼ã€‚</li>
<li>å¼•å…¥AudioCoTæ•°æ®é›†ï¼Œå®ç°è§†è§‰å†…å®¹ã€æ–‡æœ¬æè¿°å’Œå£°éŸ³åˆæˆçš„è¿æ¥ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.21448">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-66dd18fcff033fff71c61daa722e85ba.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5b89de3e30a6b4be0f267e1a3c107e0d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dde4048b0b9c1d5926a7b747883766e1.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Text2Cypher-Across-Languages-Evaluating-Foundational-Models-Beyond-English"><a href="#Text2Cypher-Across-Languages-Evaluating-Foundational-Models-Beyond-English" class="headerlink" title="Text2Cypher Across Languages: Evaluating Foundational Models Beyond   English"></a>Text2Cypher Across Languages: Evaluating Foundational Models Beyond   English</h2><p><strong>Authors:Makbule Gulcin Ozsoy, William Tai</strong></p>
<p>Recent advances in large language models have enabled natural language interfaces that translate user questions into database queries, such as Text2SQL, Text2SPARQL, and Text2Cypher. While these interfaces enhance database accessibility, most research today focuses solely on English, with limited evaluation in other languages. This paper investigates the performance of foundational LLMs on the Text2Cypher task across multiple languages. We create and release a multilingual test set by translating English questions into Spanish and Turkish while preserving the original Cypher queries, enabling fair cross-lingual comparison. We evaluate multiple foundational models using standardized prompts and metrics. Our results show a consistent performance pattern: highest on English, then Spanish, and lowest on Turkish. We attribute this to differences in training data availability and linguistic characteristics. Additionally, we explore the impact of translating task prompts into Spanish and Turkish. Results show little to no change in evaluation metrics, suggesting prompt translation has minor impact. Our findings highlight the need for more inclusive evaluation and development in multilingual query generation. Future work includes schema localization and fine-tuning across diverse languages. </p>
<blockquote>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹çš„è¿›æ­¥ä¸ºç”¨æˆ·é—®é¢˜è½¬åŒ–ä¸ºæ•°æ®åº“æŸ¥è¯¢æä¾›äº†è‡ªç„¶è¯­è¨€æ¥å£ï¼Œå¦‚Text2SQLã€Text2SPARQLå’ŒText2Cypherã€‚è™½ç„¶è¿™äº›æ¥å£å¢å¼ºäº†æ•°æ®åº“çš„å¯è®¿é—®æ€§ï¼Œä½†å½“å‰å¤§å¤šæ•°ç ”ç©¶ä»…ä¸“æ³¨äºè‹±è¯­ï¼Œå¯¹å…¶ä»–è¯­è¨€çš„è¯„ä¼°æœ‰é™ã€‚æœ¬æ–‡ç ”ç©¶äº†åŸºç¡€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è·¨å¤šç§è¯­è¨€çš„Text2Cypherä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚æˆ‘ä»¬é€šè¿‡å°†è‹±è¯­é—®é¢˜ç¿»è¯‘æˆè¥¿ç­ç‰™è¯­å’ŒåœŸè€³å…¶è¯­ï¼ŒåŒæ—¶ä¿ç•™åŸå§‹çš„CypheræŸ¥è¯¢ï¼Œåˆ›å»ºå¹¶å‘å¸ƒäº†ä¸€ä¸ªå¤šè¯­è¨€æµ‹è¯•é›†ï¼Œä»è€Œå®ç°å…¬å¹³çš„è·¨è¯­è¨€æ¯”è¾ƒã€‚æˆ‘ä»¬ä½¿ç”¨æ ‡å‡†åŒ–çš„æç¤ºå’ŒæŒ‡æ ‡å¯¹å¤šä¸ªåŸºç¡€æ¨¡å‹è¿›è¡Œè¯„ä¼°ã€‚ç»“æœå‘ˆç°å‡ºä¸€è‡´çš„æ€§èƒ½æ¨¡å¼ï¼šè‹±è¯­è¡¨ç°æœ€å¥½ï¼Œè¥¿ç­ç‰™è¯­æ¬¡ä¹‹ï¼ŒåœŸè€³å…¶è¯­æœ€å·®ã€‚æˆ‘ä»¬å°†æ­¤å½’å› äºè®­ç»ƒæ•°æ®å¯ç”¨æ€§å’Œè¯­è¨€ç‰¹æ€§çš„å·®å¼‚ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¢ç´¢äº†å°†ä»»åŠ¡æç¤ºç¿»è¯‘æˆè¥¿ç­ç‰™è¯­å’ŒåœŸè€³å…¶è¯­çš„å½±å“ã€‚ç»“æœæ˜¾ç¤ºè¯„ä¼°æŒ‡æ ‡å‡ ä¹æ²¡æœ‰å˜åŒ–ï¼Œæç¤ºç¿»è¯‘çš„å½±å“å¾ˆå°ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œéœ€è¦åœ¨å¤šè¯­è¨€æŸ¥è¯¢ç”Ÿæˆæ–¹é¢è¿›è¡Œæ›´åŒ…å®¹çš„è¯„ä¼°å’Œå¼€å‘ã€‚ä»Šåçš„å·¥ä½œåŒ…æ‹¬æ¨¡å¼æœ¬åœ°åŒ–ä»¥åŠåœ¨å¤šç§è¯­è¨€ä¸Šçš„å¾®è°ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.21445v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹çš„æ–°è¿›å±•ä¸ºç”¨æˆ·æä¾›äº†é€šè¿‡è‡ªç„¶è¯­è¨€ç•Œé¢ç”Ÿæˆæ•°æ®åº“æŸ¥è¯¢çš„èƒ½åŠ›ï¼Œå¦‚Text2SQLã€Text2SPARQLå’ŒText2Cypherç­‰ã€‚å°½ç®¡è¿™äº›æ¥å£å¢å¼ºäº†æ•°æ®åº“çš„è®¿é—®æ€§ï¼Œä½†å½“å‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­äºè‹±è¯­ï¼Œå¯¹å…¶ä»–è¯­è¨€çš„è¯„ä¼°æœ‰é™ã€‚æœ¬æ–‡ç ”ç©¶äº†åŸºç¡€LLMsåœ¨å¤šè¯­è¨€ç¯å¢ƒä¸‹çš„Text2Cypherä»»åŠ¡æ€§èƒ½ã€‚é€šè¿‡ç¿»è¯‘è‹±æ–‡é—®é¢˜åˆ°è¥¿ç­ç‰™æ–‡å’ŒåœŸè€³å…¶æ–‡åŒæ—¶ä¿ç•™åŸå§‹CypheræŸ¥è¯¢ï¼Œåˆ›å»ºå¹¶å‘å¸ƒäº†ä¸€ä¸ªå¤šè¯­è¨€æµ‹è¯•é›†ï¼Œå®ç°äº†è·¨è¯­è¨€çš„å…¬å¹³æ¯”è¾ƒã€‚è¯„ä¼°å¤šä¸ªåŸºç¡€æ¨¡å‹çš„ç»“æœæ˜¾ç¤ºï¼Œè‹±è¯­è¡¨ç°æœ€ä½³ï¼Œè¥¿ç­ç‰™è¯­æ¬¡ä¹‹ï¼ŒåœŸè€³å…¶è¯­æœ€å·®ã€‚è¿™å½’å› äºè®­ç»ƒæ•°æ®å¯ç”¨æ€§å’Œè¯­è¨€ç‰¹æ€§çš„å·®å¼‚ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æ¢è®¨äº†å°†ä»»åŠ¡æç¤ºç¿»è¯‘æˆè¥¿ç­ç‰™æ–‡å’ŒåœŸè€³å…¶æ–‡çš„å½±å“ï¼Œç»“æœæ˜¾ç¤ºè¯„ä¼°æŒ‡æ ‡å‡ ä¹æ²¡æœ‰å˜åŒ–ï¼Œæç¤ºç¿»è¯‘çš„å½±å“è¾ƒå°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿå®ç°è‡ªç„¶è¯­è¨€ç•Œé¢ç”Ÿæˆæ•°æ®åº“æŸ¥è¯¢ã€‚</li>
<li>å½“å‰ç ”ç©¶ä¸»è¦é›†ä¸­äºè‹±è¯­ï¼Œå¯¹å…¶ä»–è¯­è¨€çš„è¯„ä¼°æœ‰é™ã€‚</li>
<li>æœ¬æ–‡ç ”ç©¶äº†åŸºç¡€LLMsåœ¨Text2Cypherä»»åŠ¡ä¸Šçš„å¤šè¯­è¨€æ€§èƒ½ã€‚</li>
<li>åˆ›å»ºå¹¶å‘å¸ƒäº†ä¸€ä¸ªå¤šè¯­è¨€æµ‹è¯•é›†ï¼Œé€šè¿‡ç¿»è¯‘è‹±æ–‡é—®é¢˜åŒæ—¶ä¿ç•™åŸå§‹CypheræŸ¥è¯¢ã€‚</li>
<li>è¯„ä¼°ç»“æœæ˜¾ç¤ºè‹±è¯­è¡¨ç°æœ€ä½³ï¼Œè¥¿ç­ç‰™è¯­æ¬¡ä¹‹ï¼ŒåœŸè€³å…¶è¯­æœ€å·®ã€‚</li>
<li>æ€§èƒ½å·®å¼‚å½’å› äºè®­ç»ƒæ•°æ®å¯ç”¨æ€§å’Œè¯­è¨€ç‰¹æ€§çš„å·®å¼‚ã€‚</li>
<li>ä»»åŠ¡æç¤ºçš„ç¿»è¯‘å¯¹è¯„ä¼°æŒ‡æ ‡å½±å“è¾ƒå°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.21445">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f97cc41e7c872eb3872d6db18633d6aa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-febd9b71eae282ffa763098259c0bfad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-abd06693fbdc3fff56e6c5b9827afeab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f055ddbfb5f44a4b899be6d94ec87d1d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b36dc9f3fd9a45997298f13fd891f20a.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Scalable-Bayesian-Low-Rank-Adaptation-of-Large-Language-Models-via-Stochastic-Variational-Subspace-Inference"><a href="#Scalable-Bayesian-Low-Rank-Adaptation-of-Large-Language-Models-via-Stochastic-Variational-Subspace-Inference" class="headerlink" title="Scalable Bayesian Low-Rank Adaptation of Large Language Models via   Stochastic Variational Subspace Inference"></a>Scalable Bayesian Low-Rank Adaptation of Large Language Models via   Stochastic Variational Subspace Inference</h2><p><strong>Authors:Colin Samplawski, Adam D. Cobb, Manoj Acharya, Ramneet Kaur, Susmit Jha</strong></p>
<p>Despite their widespread use, large language models (LLMs) are known to hallucinate incorrect information and be poorly calibrated. This makes the uncertainty quantification of these models of critical importance, especially in high-stakes domains, such as autonomy and healthcare. Prior work has made Bayesian deep learning-based approaches to this problem more tractable by performing inference over the low-rank adaptation (LoRA) parameters of a fine-tuned model. While effective, these approaches struggle to scale to larger LLMs due to requiring further additional parameters compared to LoRA. In this work we present $\textbf{Scala}$ble $\textbf{B}$ayesian $\textbf{L}$ow-Rank Adaptation via Stochastic Variational Subspace Inference (ScalaBL). We perform Bayesian inference in an $r$-dimensional subspace, for LoRA rank $r$. By repurposing the LoRA parameters as projection matrices, we are able to map samples from this subspace into the full weight space of the LLM. This allows us to learn all the parameters of our approach using stochastic variational inference. Despite the low dimensionality of our subspace, we are able to achieve competitive performance with state-of-the-art approaches while only requiring ${\sim}1000$ additional parameters. Furthermore, it allows us to scale up to the largest Bayesian LLM to date, with four times as a many base parameters as prior work. </p>
<blockquote>
<p>å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¾—åˆ°å¹¿æ³›åº”ç”¨ï¼Œä½†å®ƒä»¬ä¼šè™šæ„é”™è¯¯ä¿¡æ¯ï¼Œå¹¶ä¸”æ ¡å‡†ä¸è‰¯ã€‚è¿™ä½¿å¾—è¿™äº›æ¨¡å‹çš„ä¸ç¡®å®šæ€§é‡åŒ–è‡³å…³é‡è¦ï¼Œå°¤å…¶æ˜¯åœ¨è‡ªä¸»æ€§ã€åŒ»ç–—æŠ¤ç†ç­‰é«˜é£é™©é¢†åŸŸã€‚å…ˆå‰çš„å·¥ä½œé€šè¿‡è´å¶æ–¯æ·±åº¦å­¦ä¹ çš„æ–¹æ³•å¯¹è¿™ä¸ªé—®é¢˜è¿›è¡Œäº†ç®€åŒ–å¤„ç†ï¼Œåœ¨ä¸€ä¸ªç»è¿‡ç²¾ç»†è°ƒæ•´æ¨¡å‹çš„ä½ç§©é€‚é…ï¼ˆLoRAï¼‰å‚æ•°ä¸Šæ‰§è¡Œæ¨ç†ã€‚è™½ç„¶è¿™äº›æ–¹æ³•æœ‰æ•ˆï¼Œä½†ç”±äºç›¸è¾ƒäºLoRAè¿˜éœ€è¦è¿›ä¸€æ­¥çš„é¢å¤–å‚æ•°ï¼Œå› æ­¤åœ¨å¤§è§„æ¨¡LLMä¸Šçš„æ‰©å±•é¢ä¸´æŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†é€šè¿‡éšæœºå˜åˆ†å­ç©ºé—´æ¨ç†å®ç°çš„å¯æ‰©å±•è´å¶æ–¯ä½ç§©é€‚é…ï¼ˆScalaBLï¼‰ã€‚æˆ‘ä»¬åœ¨LoRAç­‰çº§$r$çš„$r$ç»´å­ç©ºé—´è¿›è¡Œè´å¶æ–¯æ¨ç†ã€‚é€šè¿‡å°†LoRAå‚æ•°é‡æ–°ç”¨ä½œæŠ•å½±çŸ©é˜µï¼Œæˆ‘ä»¬èƒ½å¤Ÿå°†ä»è¿™ä¸ªå­ç©ºé—´çš„æ ·æœ¬æ˜ å°„åˆ°LLMçš„å…¨æƒé‡ç©ºé—´ã€‚è¿™ä½¿å¾—æˆ‘ä»¬èƒ½å¤Ÿä½¿ç”¨éšæœºå˜åˆ†æ¨ç†æ¥å­¦ä¹ æˆ‘ä»¬æ–¹æ³•ä¸­çš„æ‰€æœ‰å‚æ•°ã€‚å°½ç®¡æˆ‘ä»¬çš„å­ç©ºé—´ä½ç»´åŒ–ï¼Œä½†æˆ‘ä»¬ä»…éœ€çº¦ä¸€åƒä¸ªé¢å¤–å‚æ•°å³å¯å®ç°ä¸æœ€å…ˆè¿›çš„æ¨¡å‹ç«äº‰çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå®ƒä½¿æˆ‘ä»¬èƒ½å¤Ÿæ‰©å±•åˆ°è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„è´å¶æ–¯LLMæ¨¡å‹ï¼ŒåŸºç¡€å‚æ•°æ˜¯å…ˆå‰å·¥ä½œçš„å››å€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.21408v1">PDF</a> Accepted at UAI 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è™½ç„¶åº”ç”¨å¹¿æ³›ï¼Œä½†å­˜åœ¨ç”Ÿæˆé”™è¯¯ä¿¡æ¯çš„é—®é¢˜ï¼Œå¹¶ä¸”æ¨¡å‹çš„ä¸ç¡®å®šæ€§è¯„ä¼°è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨è‡ªä¸»æ€§å’ŒåŒ»ç–—æŠ¤ç†ç­‰é«˜é£é™©çš„é¢†åŸŸã€‚å…ˆå‰çš„å·¥ä½œé€šè¿‡è´å¶æ–¯æ·±åº¦å­¦ä¹ çš„æ–¹æ³•å¯¹è¿™ä¸ªé—®é¢˜è¿›è¡Œäº†ç®€åŒ–ï¼Œé€šè¿‡å¯¹ç²¾ç»†è°ƒæ•´æ¨¡å‹çš„ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰å‚æ•°è¿›è¡Œæ¨ç†ã€‚å°½ç®¡è¿™ç§æ–¹æ³•æœ‰æ•ˆï¼Œä½†åœ¨æ›´å¤§çš„LLMæ¨¡å‹ä¸Šåº”ç”¨å›°éš¾ï¼Œå› ä¸ºå®ƒä»¬éœ€è¦æ¯”LoRAæ›´å¤šçš„å‚æ•°ã€‚åœ¨æ­¤å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†é€šè¿‡éšæœºå˜åˆ†å­å­ç©ºé—´æ¨ç†å®ç°å¯æ‰©å±•çš„è´å¶æ–¯ä½ç§©é€‚åº”ï¼ˆScalaBLï¼‰ã€‚æˆ‘ä»¬åœ¨rç»´å­ç©ºé—´ä¸­è¿›è¡Œè´å¶æ–¯æ¨ç†ï¼Œå¯¹äºLoRAç­‰çº§rã€‚é€šè¿‡é‡æ–°ä½¿ç”¨LoRAå‚æ•°ä½œä¸ºæŠ•å½±çŸ©é˜µï¼Œæˆ‘ä»¬èƒ½å¤Ÿä»è¿™ä¸ªå­ç©ºé—´æ˜ å°„æ ·æœ¬åˆ°LLMçš„å…¨æƒé‡ç©ºé—´ã€‚è¿™ä½¿å¾—æˆ‘ä»¬èƒ½å¤Ÿä½¿ç”¨éšæœºå˜åˆ†æ¨ç†æ¥å­¦ä¹ æˆ‘ä»¬æ–¹æ³•çš„æ‰€æœ‰å‚æ•°ã€‚å°½ç®¡æˆ‘ä»¬çš„å­ç©ºé—´ä½ç»´åŒ–ï¼Œä½†æˆ‘ä»¬èƒ½å¤Ÿå®ç°ä¸æœ€æ–°æŠ€æœ¯æ–¹æ³•ç›¸å½“çš„ç«äº‰åŠ›ï¼Œå¹¶ä¸”ä»…éœ€çº¦1000ä¸ªé¢å¤–å‚æ•°ã€‚æ­¤å¤–ï¼Œå®ƒå…è®¸æˆ‘ä»¬æ‰©å±•åˆ°è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„è´å¶æ–¯LLMï¼Œå…¶åŸºç¡€å‚æ•°æ˜¯å…ˆå‰å·¥ä½œçš„å››å€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è™½ç„¶åº”ç”¨å¹¿æ³›ï¼Œä½†å­˜åœ¨ç”Ÿæˆé”™è¯¯ä¿¡æ¯çš„é—®é¢˜ï¼Œéœ€è¦è¿›è¡Œä¸ç¡®å®šæ€§è¯„ä¼°ã€‚</li>
<li>å…ˆå‰çš„å·¥ä½œä½¿ç”¨è´å¶æ–¯æ·±åº¦å­¦ä¹ çš„æ–¹æ³•ç®€åŒ–è¿™ä¸ªé—®é¢˜ï¼Œä½†éš¾ä»¥åº”ç”¨äºæ›´å¤§çš„LLMæ¨¡å‹ã€‚</li>
<li>æœ¬å·¥ä½œæå‡ºäº†ScalaBLæ–¹æ³•ï¼Œé€šè¿‡éšæœºå˜åˆ†å­å­ç©ºé—´æ¨ç†å®ç°è´å¶æ–¯ä½ç§©é€‚åº”ã€‚</li>
<li>ScalaBLåœ¨rç»´å­ç©ºé—´ä¸­è¿›è¡Œè´å¶æ–¯æ¨ç†ï¼Œå¹¶é€šè¿‡æŠ•å½±çŸ©é˜µæ˜ å°„æ ·æœ¬åˆ°LLMçš„å…¨æƒé‡ç©ºé—´ã€‚</li>
<li>ScalaBLæ–¹æ³•å…·æœ‰ç«äº‰åŠ›ï¼Œå¹¶ä¸”ä»…éœ€å°‘é‡é¢å¤–å‚æ•°ã€‚</li>
<li>ScalaBLèƒ½å¤Ÿæ‰©å±•åˆ°æ›´å¤§çš„LLMæ¨¡å‹ï¼Œå…¶åŸºç¡€å‚æ•°æ˜¯å…ˆå‰å·¥ä½œçš„å››å€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.21408">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a48bc1f95f1aa780da3bd79e853076a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de26d5871948fbdc644849d7fa28bab3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4c826f5d3ba4bf3338ec4bc057ee9250.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1e3a83d77faaefbfe7779d5143ea81b3.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="TableMoE-Neuro-Symbolic-Routing-for-Structured-Expert-Reasoning-in-Multimodal-Table-Understanding"><a href="#TableMoE-Neuro-Symbolic-Routing-for-Structured-Expert-Reasoning-in-Multimodal-Table-Understanding" class="headerlink" title="TableMoE: Neuro-Symbolic Routing for Structured Expert Reasoning in   Multimodal Table Understanding"></a>TableMoE: Neuro-Symbolic Routing for Structured Expert Reasoning in   Multimodal Table Understanding</h2><p><strong>Authors:Junwen Zhang, Pu Chen, Yin Zhang</strong></p>
<p>Multimodal understanding of tables in real-world contexts is challenging due to the complexity of structure, symbolic density, and visual degradation (blur, skew, watermarking, incomplete structures or fonts, multi-span or hierarchically nested layouts). Existing multimodal large language models (MLLMs) struggle with such WildStruct conditions, resulting in limited performance and poor generalization. To address these challenges, we propose TableMoE, a neuro-symbolic Mixture-of-Connector-Experts (MoCE) architecture specifically designed for robust, structured reasoning over multimodal table data. TableMoE features an innovative Neuro-Symbolic Routing mechanism, which predicts latent semantic token roles (e.g., header, data cell, axis, formula) and dynamically routes table elements to specialized experts (Table-to-HTML, Table-to-JSON, Table-to-Code) using a confidence-aware gating strategy informed by symbolic reasoning graphs. To facilitate effective alignment-driven pretraining, we introduce the large-scale TableMoE-Align dataset, consisting of 1.2M table-HTML-JSON-code quadruples across finance, science, biomedicine and industry, utilized exclusively for model pretraining. For evaluation, we curate and release four challenging WildStruct benchmarks: WMMFinQA, WMMTatQA, WMMTabDialog, and WMMFinanceMath, designed specifically to stress-test models under real-world multimodal degradation and structural complexity. Experimental results demonstrate that TableMoE significantly surpasses existing state-of-the-art models. Extensive ablation studies validate each core component, emphasizing the critical role of Neuro-Symbolic Routing and structured expert alignment. Through qualitative analyses, we further showcase TableMoEâ€™s interpretability and enhanced robustness, underscoring the effectiveness of integrating neuro-symbolic reasoning for multimodal table understanding. </p>
<blockquote>
<p>ç°å®ä¸–ç•Œè¯­å¢ƒä¸­çš„è¡¨æ ¼çš„å¤šæ¨¡æ€ç†è§£æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå…¶å¤æ‚æ€§ä½“ç°åœ¨ç»“æ„ã€ç¬¦å·å¯†åº¦ä»¥åŠè§†è§‰é€€åŒ–ï¼ˆæ¨¡ç³Šã€æ­ªæ–œã€æ°´å°ã€ç»“æ„æˆ–å­—ä½“ä¸å®Œæ•´ã€å¤šè·¨åº¦æˆ–å±‚æ¬¡åµŒå¥—å¸ƒå±€ï¼‰ç­‰æ–¹é¢ã€‚ç°æœ‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨é¢ä¸´WildStructæ¡ä»¶ä¸‹è¡¨ç°åƒåŠ›ï¼Œå¯¼è‡´æ€§èƒ½æœ‰é™å’Œæ³›åŒ–èƒ½åŠ›è¾ƒå·®ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†TableMoEï¼Œè¿™æ˜¯ä¸€ç§ä¸“é—¨è®¾è®¡ç”¨äºåœ¨å¤šæ¨¡æ€è¡¨æ ¼æ•°æ®è¿›è¡Œç¨³å¥ã€ç»“æ„åŒ–æ¨ç†çš„ç¥ç»ç¬¦å·æ··åˆè¿æ¥å™¨ä¸“å®¶ï¼ˆMoCEï¼‰æ¶æ„ã€‚TableMoEå…·æœ‰åˆ›æ–°çš„ç¥ç»ç¬¦å·è·¯ç”±æœºåˆ¶ï¼Œå®ƒé¢„æµ‹æ½œåœ¨è¯­ä¹‰æ ‡è®°è§’è‰²ï¼ˆä¾‹å¦‚ï¼Œè¡¨å¤´ã€æ•°æ®å•å…ƒæ ¼ã€è½´ã€å…¬å¼ï¼‰ï¼Œå¹¶ä½¿ç”¨åŸºäºç½®ä¿¡åº¦çš„é—¨æ§ç­–ç•¥åŠ¨æ€åœ°å°†è¡¨æ ¼å…ƒç´ è·¯ç”±åˆ°ä¸“ä¸šä¸“å®¶ï¼ˆè¡¨æ ¼åˆ°HTMLã€è¡¨æ ¼åˆ°JSONã€è¡¨æ ¼åˆ°ä»£ç ï¼‰ï¼Œè¯¥ç­–ç•¥ç”±ç¬¦å·æ¨ç†å›¾æä¾›ä¿¡æ¯ã€‚ä¸ºäº†ä¿ƒè¿›æœ‰æ•ˆçš„å¯¹é½é©±åŠ¨é¢„è®­ç»ƒï¼Œæˆ‘ä»¬å¼•å…¥äº†å¤§è§„æ¨¡çš„TableMoE-Alignæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«é‡‘èã€ç§‘å­¦ã€ç”Ÿç‰©åŒ»å­¦å’Œå·¥ä¸šé¢†åŸŸçš„120ä¸‡å¼ è¡¨æ ¼HTMLJSONä»£ç å››é‡æ•°æ®ï¼Œä»…ç”¨äºæ¨¡å‹é¢„è®­ç»ƒã€‚ä¸ºäº†è¿›è¡Œè¯„ä¼°ï¼Œæˆ‘ä»¬ç­–åˆ’å¹¶å‘å¸ƒäº†å››ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„WildStructåŸºå‡†æµ‹è¯•ï¼šWMMFinQAã€WMMTatQAã€WMMTabDialogå’ŒWMMFinanceMathï¼Œä¸“é—¨è®¾è®¡ç”¨äºåœ¨ç°å®ä¸–ç•Œçš„å¤šæ¨¡æ€é€€åŒ–å’Œç»“æ„å¤æ‚æ€§æ¡ä»¶ä¸‹å¯¹æ¨¡å‹è¿›è¡Œå‹åŠ›æµ‹è¯•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTableMoEæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ¨¡å‹ã€‚å¹¿æ³›çš„æ¶ˆèç ”ç©¶éªŒè¯äº†æ¯ä¸ªæ ¸å¿ƒç»„ä»¶çš„ä½œç”¨ï¼Œå¼ºè°ƒäº†ç¥ç»ç¬¦å·è·¯ç”±å’Œç»“æ„åŒ–ä¸“å®¶å¯¹é½çš„å…³é”®ä½œç”¨ã€‚é€šè¿‡å®šæ€§åˆ†æï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å±•ç¤ºäº†TableMoEçš„å¯è§£é‡Šæ€§å’Œå¢å¼ºçš„ç¨³å¥æ€§ï¼Œå¼ºè°ƒäº†ç¥ç»ç¬¦å·æ¨ç†åœ¨å¤šæ¨¡æ€è¡¨æ ¼ç†è§£ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.21393v1">PDF</a> 43 pages and 11 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºTableMoEçš„ç¥ç»ç¬¦å·æ··åˆä¸“å®¶ç³»ç»Ÿæ¶æ„ï¼Œç”¨äºè§£å†³çœŸå®ä¸–ç•Œç¯å¢ƒä¸‹è¡¨æ ¼çš„å¤šæ¨¡æ€ç†è§£æŒ‘æˆ˜ã€‚é€šè¿‡ç¥ç»ç¬¦å·è·¯ç”±æœºåˆ¶é¢„æµ‹è¡¨æ ¼å…ƒç´ çš„æ½œåœ¨è¯­ä¹‰è§’è‰²ï¼Œå¹¶å°†å…¶åŠ¨æ€è·¯ç”±è‡³ä¸åŒä¸“å®¶è¿›è¡Œå¤„ç†ã€‚æ­¤å¤–ï¼Œä¸ºäº†æ”¯æŒæœ‰æ•ˆçš„é¢„è®­ç»ƒï¼Œå¼•å…¥äº†å¤§è§„æ¨¡TableMoE-Alignæ•°æ®é›†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTableMoEåœ¨å¤æ‚ç»“æ„ã€ç¬¦å·å¯†åº¦å’Œè§†è§‰é€€åŒ–ç­‰æ¡ä»¶ä¸‹æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>çœŸå®ä¸–ç•Œä¸­çš„è¡¨æ ¼å¤šæ¨¡æ€ç†è§£é¢ä¸´ç»“æ„å¤æ‚æ€§ã€ç¬¦å·å¯†åº¦å’Œè§†è§‰é€€åŒ–ç­‰æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰å¤§è¯­è¨€æ¨¡å‹åœ¨WildStructæ¡ä»¶ä¸‹è¡¨ç°æœ‰é™ï¼Œç¼ºä¹æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>TableMoEï¼šä¸€ç§ç¥ç»ç¬¦å·æ··åˆä¸“å®¶æ¶æ„è¢«æå‡ºï¼Œç”¨äºç¨³å¥çš„ç»“æ„åŒ–å¤šæ¨¡æ€è¡¨æ ¼æ•°æ®æ¨ç†ã€‚</li>
<li>TableMoEé‡‡ç”¨åˆ›æ–°çš„ç¥ç»ç¬¦å·è·¯ç”±æœºåˆ¶ï¼Œé¢„æµ‹è¡¨æ ¼å…ƒç´ çš„è¯­ä¹‰è§’è‰²å¹¶åŠ¨æ€è·¯ç”±è‡³ä¸“å®¶å¤„ç†ã€‚</li>
<li>å¼•å…¥å¤§è§„æ¨¡TableMoE-Alignæ•°æ®é›†ï¼Œç”¨äºæ¨¡å‹çš„æœ‰æ•ˆé¢„è®­ç»ƒã€‚</li>
<li>TableMoEåœ¨ç‰¹å®šè®¾è®¡çš„WildStructåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.21393">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f28f30185beb7439c905f9e6b22313d7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d59f126c7260401c3f7085c956297f0b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a80bce142d2356d01452b208101ca4a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8df855816330e6b8f7548d267856164d.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="DynamicBench-Evaluating-Real-Time-Report-Generation-in-Large-Language-Models"><a href="#DynamicBench-Evaluating-Real-Time-Report-Generation-in-Large-Language-Models" class="headerlink" title="DynamicBench: Evaluating Real-Time Report Generation in Large Language   Models"></a>DynamicBench: Evaluating Real-Time Report Generation in Large Language   Models</h2><p><strong>Authors:Jingyao Li, Hao Sun, Zile Qiao, Yong Jiang, Pengjun Xie, Fei Huang, Hong Xu, Jiaya Jia</strong></p>
<p>Traditional benchmarks for large language models (LLMs) typically rely on static evaluations through storytelling or opinion expression, which fail to capture the dynamic requirements of real-time information processing in contemporary applications. To address this limitation, we present DynamicBench, a benchmark designed to evaluate the proficiency of LLMs in storing and processing up-to-the-minute data. DynamicBench utilizes a dual-path retrieval pipeline, integrating web searches with local report databases. It necessitates domain-specific knowledge, ensuring accurate responses report generation within specialized fields. By evaluating models in scenarios that either provide or withhold external documents, DynamicBench effectively measures their capability to independently process recent information or leverage contextual enhancements. Additionally, we introduce an advanced report generation system adept at managing dynamic information synthesis. Our experimental results confirm the efficacy of our approach, with our method achieving state-of-the-art performance, surpassing GPT4o in document-free and document-assisted scenarios by 7.0% and 5.8%, respectively. The code and data will be made publicly available. </p>
<blockquote>
<p>ä¼ ç»Ÿçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åŸºå‡†æµ‹è¯•é€šå¸¸ä¾èµ–äºé€šè¿‡è®²æ•…äº‹æˆ–è¡¨è¾¾æ„è§æ¥è¿›è¡Œçš„é™æ€è¯„ä¼°ï¼Œè¿™æ— æ³•æ•æ‰å½“ä»£åº”ç”¨ä¸­å®æ—¶ä¿¡æ¯å¤„ç†çš„åŠ¨æ€éœ€æ±‚ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æ¨å‡ºäº†DynamicBenchï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨è¯„ä¼°LLMå­˜å‚¨å’Œå¤„ç†æœ€æ–°æ•°æ®èƒ½åŠ›è®¾è®¡çš„åŸºå‡†æµ‹è¯•ã€‚DynamicBenché‡‡ç”¨åŒè·¯å¾„æ£€ç´¢ç®¡é“ï¼Œå°†ç½‘ç»œæœç´¢ä¸æœ¬åœ°æŠ¥å‘Šæ•°æ®åº“é›†æˆåœ¨ä¸€èµ·ã€‚å®ƒéœ€è¦ç‰¹å®šé¢†åŸŸçš„çŸ¥è¯†ï¼Œä»¥ç¡®ä¿åœ¨ç‰¹å®šé¢†åŸŸç”Ÿæˆå‡†ç¡®çš„å“åº”æŠ¥å‘Šã€‚é€šè¿‡åœ¨æä¾›æˆ–ä¸æä¾›å¤–éƒ¨æ–‡æ¡£çš„åœºæ™¯ä¸‹è¯„ä¼°æ¨¡å‹ï¼ŒDynamicBenchæœ‰æ•ˆåœ°è¡¡é‡äº†å®ƒä»¬ç‹¬ç«‹å¤„ç†æœ€æ–°ä¿¡æ¯æˆ–åˆ©ç”¨ä¸Šä¸‹æ–‡å¢å¼ºçš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªå…ˆè¿›çš„æŠ¥å‘Šç”Ÿæˆç³»ç»Ÿï¼Œæ“…é•¿ç®¡ç†åŠ¨æ€ä¿¡æ¯åˆæˆã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¯å®äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ–‡æ¡£ç‹¬ç«‹å’Œæ–‡æ¡£è¾…åŠ©åœºæ™¯ä¸­åˆ†åˆ«è¶…è¶Šäº†GPT4oï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåˆ†åˆ«æé«˜äº†7.0%å’Œ5.8%ã€‚ä»£ç å’Œæ•°æ®å°†å…¬å¼€å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.21343v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>ä¼ ç»Ÿçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åŸºå‡†æµ‹è¯•ä¸»è¦é€šè¿‡æ•…äº‹å™è¿°æˆ–æ„è§è¡¨è¾¾è¿›è¡Œé™æ€è¯„ä¼°ï¼Œæ— æ³•æ•æ‰å½“ä»£åº”ç”¨ä¸­å®æ—¶ä¿¡æ¯å¤„ç†çš„åŠ¨æ€éœ€æ±‚ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†DynamicBenchï¼Œä¸€ä¸ªæ—¨åœ¨è¯„ä¼°LLMå­˜å‚¨å’Œå¤„ç†æœ€æ–°æ•°æ®èƒ½åŠ›çš„æ–°åŸºå‡†æµ‹è¯•ã€‚DynamicBenchåˆ©ç”¨åŒè·¯å¾„æ£€ç´¢ç®¡é“ï¼Œç»“åˆç½‘ç»œæœç´¢å’Œæœ¬åœ°æŠ¥å‘Šæ•°æ®åº“ã€‚å®ƒè¦æ±‚ç‰¹å®šé¢†åŸŸçš„ä¸“ä¸šçŸ¥è¯†ï¼Œç¡®ä¿åœ¨ä¸“ä¸šé¢†åŸŸå†…å‡†ç¡®ç”ŸæˆæŠ¥å‘Šã€‚é€šè¿‡åœ¨ä¸åŒåœºæ™¯ä¸‹è¯„ä¼°æ¨¡å‹çš„è¡¨ç°ï¼Œå¦‚æä¾›æˆ–ç¦æ­¢å¤–éƒ¨æ–‡æ¡£ï¼ŒDynamicBenchèƒ½å¤Ÿæœ‰æ•ˆåœ°è¡¡é‡æ¨¡å‹ç‹¬ç«‹å¤„ç†æœ€æ–°ä¿¡æ¯æˆ–åˆ©ç”¨ä¸Šä¸‹æ–‡å¢å¼ºçš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªå…ˆè¿›æŠ¥å‘Šç”Ÿæˆç³»ç»Ÿï¼Œèƒ½å¤Ÿå¤„ç†åŠ¨æ€ä¿¡æ¯çš„ç»¼åˆç”Ÿæˆã€‚å®éªŒç»“æœè¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•æœ‰æ•ˆï¼Œä¸”åœ¨æ— æ–‡æ¡£å’Œå¸¦æ–‡æ¡£åœºæ™¯ä¸­åˆ†åˆ«è¶…è¶ŠGPT4oè¾¾7.0%å’Œ5.8%ã€‚ä»£ç å’Œæ•°æ®å°†å…¬å¼€å‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ä¼ ç»Ÿçš„å¤§å‹è¯­è¨€æ¨¡å‹åŸºå‡†æµ‹è¯•ä¸»è¦ä¾èµ–é™æ€è¯„ä¼°æ–¹å¼ï¼Œæ— æ³•é€‚åº”ç°ä»£åº”ç”¨ä¸­å®æ—¶ä¿¡æ¯å¤„ç†çš„éœ€æ±‚ã€‚</li>
<li>DynamicBenchæ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°LLMå­˜å‚¨å’Œå¤„ç†æœ€æ–°æ•°æ®çš„èƒ½åŠ›ã€‚</li>
<li>DynamicBenchç»“åˆäº†ç½‘ç»œæœç´¢å’Œæœ¬åœ°æŠ¥å‘Šæ•°æ®åº“ï¼Œé‡‡ç”¨åŒè·¯å¾„æ£€ç´¢ç®¡é“ã€‚</li>
<li>DynamicBenchè¦æ±‚ç‰¹å®šé¢†åŸŸçš„ä¸“ä¸šçŸ¥è¯†ï¼Œä»¥ç¡®ä¿åœ¨ä¸“ä¸šé¢†åŸŸå†…å‡†ç¡®ç”ŸæˆæŠ¥å‘Šã€‚</li>
<li>DynamicBenchèƒ½å¤Ÿè¡¡é‡æ¨¡å‹ç‹¬ç«‹å¤„ç†æœ€æ–°ä¿¡æ¯çš„èƒ½åŠ›ä»¥åŠåˆ©ç”¨ä¸Šä¸‹æ–‡å¢å¼ºçš„èƒ½åŠ›ã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªå…ˆè¿›çš„æŠ¥å‘Šç”Ÿæˆç³»ç»Ÿï¼Œèƒ½å¤Ÿå¤„ç†åŠ¨æ€ä¿¡æ¯çš„ç»¼åˆç”Ÿæˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.21343">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e0b8b14e303f0775bd213888292965ba.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a10bb64b107c555736b94cd96c4e3afa.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b874465f6d10aa863885dc4eb0158d4e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bee280ad0f6419c69cbcb2133f8eeac3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-094a3526cce4ca86f2d42acfbb4c669b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f09a20dd9025e5774270ea8754dd00d.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Forecasting-Geopolitical-Events-with-a-Sparse-Temporal-Fusion-Transformer-and-Gaussian-Process-Hybrid-A-Case-Study-in-Middle-Eastern-and-U-S-Conflict-Dynamics"><a href="#Forecasting-Geopolitical-Events-with-a-Sparse-Temporal-Fusion-Transformer-and-Gaussian-Process-Hybrid-A-Case-Study-in-Middle-Eastern-and-U-S-Conflict-Dynamics" class="headerlink" title="Forecasting Geopolitical Events with a Sparse Temporal Fusion   Transformer and Gaussian Process Hybrid: A Case Study in Middle Eastern and   U.S. Conflict Dynamics"></a>Forecasting Geopolitical Events with a Sparse Temporal Fusion   Transformer and Gaussian Process Hybrid: A Case Study in Middle Eastern and   U.S. Conflict Dynamics</h2><p><strong>Authors:Hsin-Hsiung Huang, Hayden Hampton</strong></p>
<p>Forecasting geopolitical conflict from data sources like the Global Database of Events, Language, and Tone (GDELT) is a critical challenge for national security. The inherent sparsity, burstiness, and overdispersion of such data cause standard deep learning models, including the Temporal Fusion Transformer (TFT), to produce unreliable long-horizon predictions. We introduce STFT-VNNGP, a hybrid architecture that won the 2023 Algorithms for Threat Detection (ATD) competition by overcoming these limitations. Designed to bridge this gap, our model employs a two-stage process: first, a TFT captures complex temporal dynamics to generate multi-quantile forecasts. These quantiles then serve as informed inputs for a Variational Nearest Neighbor Gaussian Process (VNNGP), which performs principled spatiotemporal smoothing and uncertainty quantification. In a case study forecasting conflict dynamics in the Middle East and the U.S., STFT-VNNGP consistently outperforms a standalone TFT, showing a superior ability to predict the timing and magnitude of bursty event periods, particularly at long-range horizons. This work offers a robust framework for generating more reliable and actionable intelligence from challenging event data, with all code and workflows made publicly available to ensure reproducibility. </p>
<blockquote>
<p>ä»å…¨çƒäº‹ä»¶ã€è¯­è¨€å’Œè¯­è°ƒæ•°æ®åº“ï¼ˆGDELTï¼‰ç­‰æ•°æ®æºé¢„æµ‹åœ°ç¼˜æ”¿æ²»å†²çªæ˜¯å›½å®¶å®‰å…¨é¢ä¸´çš„ä¸€é¡¹å…³é”®æŒ‘æˆ˜ã€‚æ­¤ç±»æ•°æ®å›ºæœ‰çš„ç¨€ç–æ€§ã€çªå‘æ€§å’Œè¿‡åº¦åˆ†æ•£æ€§å¯¼è‡´æ ‡å‡†æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ˆåŒ…æ‹¬æ—¶é—´èåˆè½¬æ¢å™¨ï¼ˆTFTï¼‰ï¼‰äº§ç”Ÿä¸å¯é çš„é•¿æœŸé¢„æµ‹ã€‚æˆ‘ä»¬å¼•å…¥äº†STFT-VNNGPï¼Œè¿™æ˜¯ä¸€ç§æ··åˆæ¶æ„ï¼Œå…‹æœäº†è¿™äº›é™åˆ¶ï¼Œèµ¢å¾—äº†2023å¹´å¨èƒæ£€æµ‹ç®—æ³•ï¼ˆATDï¼‰ç«èµ›ã€‚æˆ‘ä»¬çš„æ¨¡å‹é‡‡ç”¨ä¸¤é˜¶æ®µè¿‡ç¨‹æ¥å¼¥è¡¥è¿™ä¸€å·®è·ï¼šé¦–å…ˆï¼ŒTFTæ•æ‰å¤æ‚çš„æ—¶é—´åŠ¨æ€ä»¥ç”Ÿæˆå¤šåˆ†ä½é¢„æµ‹ã€‚è¿™äº›åˆ†ä½æ•°éšåä½œä¸ºä¿¡æ¯ä¸°å¯Œçš„è¾“å…¥ï¼Œç”¨äºå˜åˆ†æœ€è¿‘é‚»é«˜æ–¯è¿‡ç¨‹ï¼ˆVNNGPï¼‰ï¼Œè¿›è¡Œæœ‰åŸåˆ™çš„æ—¶ç©ºå¹³æ»‘å’Œä¸ç¡®å®šæ€§é‡åŒ–ã€‚åœ¨ä¸€é¡¹é¢„æµ‹ä¸­ä¸œå’Œç¾å›½å†²çªåŠ¨æ€çš„æ¡ˆä¾‹ç ”ç©¶ä¸­ï¼ŒSTFT-VNNGPå§‹ç»ˆä¼˜äºå•ç‹¬çš„TFTï¼Œæ˜¾ç¤ºå‡ºåœ¨é¢„æµ‹çªå‘äº‹ä»¶æ—¶æœŸçš„æ—¶æœºå’Œå¹…åº¦æ–¹é¢å…·æœ‰å“è¶Šèƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨è¿œç¨‹èŒƒå›´å†…ã€‚è¿™é¡¹å·¥ä½œæä¾›äº†ä¸€ä¸ªç¨³å¥çš„æ¡†æ¶ï¼Œå¯ä»¥ä»å…·æœ‰æŒ‘æˆ˜æ€§çš„äº‹ä»¶æ•°æ®ä¸­ç”Ÿæˆæ›´å¯é ã€æ›´å¯æ“ä½œçš„æƒ…æŠ¥ï¼Œæ‰€æœ‰ä»£ç å’Œå·¥ä½œæµç¨‹å‡å…¬å¼€æä¾›ï¼Œä»¥ç¡®ä¿å¯é‡å¤æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20935v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå…¨çƒäº‹ä»¶ã€è¯­è¨€å’Œè¯­è°ƒæ•°æ®åº“ï¼ˆGDELTï¼‰ç­‰æ•°æ®æºçš„åœ°ç†æ”¿æ²»å†²çªé¢„æµ‹æ˜¯å›½å®¶å®‰å…¨é¢†åŸŸçš„å…³é”®æŒ‘æˆ˜ã€‚ç”±äºæ­¤ç±»æ•°æ®å›ºæœ‰çš„ç¨€ç–æ€§ã€çªå‘æ€§å’Œè¿‡åº¦åˆ†æ•£æ€§ï¼Œæ ‡å‡†æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ˆåŒ…æ‹¬æ—¶åºèåˆå˜æ¢å™¨TFTï¼‰åœ¨é¢„æµ‹é•¿æœŸäº‹ä»¶æ—¶è¡¨ç°å‡ºä¸å¯é æ€§ã€‚æˆ‘ä»¬å¼•å…¥STFT-VNNGPæ··åˆæ¶æ„ï¼Œè¯¥æ¶æ„åœ¨å…‹æœè¿™äº›å±€é™çš„åŒæ—¶èµ¢å¾—äº†2023å¹´å¨èƒæ£€æµ‹ç®—æ³•ï¼ˆATDï¼‰ç«èµ›ã€‚è®¾è®¡æ­¤æ¨¡å‹æ˜¯ä¸ºäº†å¡«è¡¥ç©ºç™½ï¼Œå®ƒé€šè¿‡ä¸¤ä¸ªé˜¶æ®µçš„è¿‡ç¨‹æ¥å®ç°ï¼šé¦–å…ˆï¼ŒTFTæ•æ‰å¤æ‚çš„æ—¶æ€åŠ¨æ€ä»¥ç”Ÿæˆå¤šåˆ†ä½æ•°é¢„æµ‹ã€‚è¿™äº›åˆ†ä½æ•°éšåä½œä¸ºä¿¡æ¯è¾“å…¥ç”¨äºå˜åˆ†æœ€è¿‘é‚»é«˜æ–¯è¿‡ç¨‹ï¼ˆVNNGPï¼‰ï¼Œè¿›è¡ŒåŸåˆ™æ€§çš„æ—¶ç©ºå¹³æ»‘å’Œä¸ç¡®å®šæ€§é‡åŒ–ã€‚åœ¨é¢„æµ‹ä¸­ä¸œå’Œç¾å›½å†²çªåŠ¨æ€çš„æ¡ˆä¾‹ç ”ç©¶ä¸­ï¼ŒSTFT-VNNGPæŒç»­ä¼˜äºå•ä¸€çš„TFTï¼Œå±•ç°å‡ºé¢„æµ‹çªå‘äº‹ä»¶æ—¶é—´ç‚¹å’ŒæŒç»­æ—¶é—´çš„å“è¶Šèƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨é•¿æœŸèŒƒå›´å†…ã€‚è¿™é¡¹å·¥ä½œæä¾›äº†ä¸€ä¸ªå¯é çš„æ¡†æ¶ï¼Œå¯ä»¥ä»å…·æœ‰æŒ‘æˆ˜æ€§çš„äº‹ä»¶æ•°æ®ä¸­ç”Ÿæˆæ›´å¯é å’Œæ›´å…·è¡ŒåŠ¨åŠ›çš„æƒ…æŠ¥ï¼Œæ‰€æœ‰ä»£ç å’Œå·¥ä½œæµç¨‹å‡å…¬å¼€æä¾›ä»¥ç¡®ä¿å¯é‡å¤æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åœ°ç¼˜æ”¿æ²»å†²çªçš„é¢„æµ‹æ˜¯ä¸€ä¸ªå…³é”®çš„å›½å®¶å®‰å…¨æŒ‘æˆ˜ï¼Œéœ€è¦ä½¿ç”¨å¯é çš„æ•°æ®åˆ†ææ¨¡å‹æ¥è§£å†³ã€‚</li>
<li>æ ‡å‡†æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨å¤„ç†å¦‚GDELTæ•°æ®æºæ—¶å­˜åœ¨å±€é™æ€§ï¼Œéš¾ä»¥åšå‡ºé•¿æœŸå¯é çš„é¢„æµ‹ã€‚</li>
<li>STFT-VNNGPæ··åˆæ¶æ„é€šè¿‡ç»“åˆTFTå’ŒVNNGPæŠ€æœ¯å…‹æœäº†è¿™äº›æŒ‘æˆ˜ï¼Œæé«˜äº†é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚</li>
<li>STFT-VNNGPæ¶æ„é€šè¿‡æ•æ‰å¤æ‚çš„æ—¶æ€åŠ¨æ€ç”Ÿæˆå¤šåˆ†ä½æ•°é¢„æµ‹ï¼Œç„¶åä½¿ç”¨VNNGPè¿›è¡Œæ—¶ç©ºå¹³æ»‘å’Œä¸ç¡®å®šæ€§é‡åŒ–ã€‚</li>
<li>åœ¨ä¸­ä¸œå’Œç¾å›½çš„å†²çªé¢„æµ‹æ¡ˆä¾‹ç ”ç©¶ä¸­ï¼ŒSTFT-VNNGPè¡¨ç°ä¼˜äºå•ä¸€çš„TFTæ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨é•¿æœŸé¢„æµ‹æ–¹é¢ã€‚</li>
<li>è¯¥å·¥ä½œæä¾›äº†ä¸€ä¸ªå¯é çš„æ¡†æ¶ï¼Œå¯ä»¥ä»å…·æœ‰æŒ‘æˆ˜æ€§çš„äº‹ä»¶æ•°æ®ä¸­ç”Ÿæˆæ›´å¯é å’Œæ›´å…·è¡ŒåŠ¨åŠ›çš„æƒ…æŠ¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20935">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-27c392769f63fa9485f2095669739986.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cf93fcdb47a3a7332b9ef3fbe0909c06.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="DiffuCoder-Understanding-and-Improving-Masked-Diffusion-Models-for-Code-Generation"><a href="#DiffuCoder-Understanding-and-Improving-Masked-Diffusion-Models-for-Code-Generation" class="headerlink" title="DiffuCoder: Understanding and Improving Masked Diffusion Models for Code   Generation"></a>DiffuCoder: Understanding and Improving Masked Diffusion Models for Code   Generation</h2><p><strong>Authors:Shansan Gong, Ruixiang Zhang, Huangjie Zheng, Jiatao Gu, Navdeep Jaitly, Lingpeng Kong, Yizhe Zhang</strong></p>
<p>Diffusion large language models (dLLMs) are compelling alternatives to autoregressive (AR) models because their denoising models operate over the entire sequence. The global planning and iterative refinement features of dLLMs are particularly useful for code generation. However, current training and inference mechanisms for dLLMs in coding are still under-explored. To demystify the decoding behavior of dLLMs and unlock their potential for coding, we systematically investigate their denoising processes and reinforcement learning (RL) methods. We train a 7B dLLM, \textbf{DiffuCoder}, on 130B tokens of code. Using this model as a testbed, we analyze its decoding behavior, revealing how it differs from that of AR models: (1) dLLMs can decide how causal their generation should be without relying on semi-AR decoding, and (2) increasing the sampling temperature diversifies not only token choices but also their generation order. This diversity creates a rich search space for RL rollouts. For RL training, to reduce the variance of token log-likelihood estimates and maintain training efficiency, we propose \textbf{coupled-GRPO}, a novel sampling scheme that constructs complementary mask noise for completions used in training. In our experiments, coupled-GRPO significantly improves DiffuCoderâ€™s performance on code generation benchmarks (+4.4% on EvalPlus) and reduces reliance on AR bias during decoding. Our work provides deeper insight into the machinery of dLLM generation and offers an effective, diffusion-native RL training framework. <a target="_blank" rel="noopener" href="https://github.com/apple/ml-diffucoder">https://github.com/apple/ml-diffucoder</a>. </p>
<blockquote>
<p>æ‰©æ•£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆdLLMsï¼‰æ˜¯ä»¤äººä¿¡æœçš„æ›¿ä»£è‡ªå›å½’ï¼ˆARï¼‰æ¨¡å‹çš„é€‰é¡¹ï¼Œå› ä¸ºå®ƒä»¬çš„é™å™ªæ¨¡å‹åœ¨æ•´ä¸ªåºåˆ—ä¸Šè¿è¡Œã€‚dLLMsçš„å…¨å±€è§„åˆ’å’Œè¿­ä»£ä¼˜åŒ–åŠŸèƒ½å¯¹äºä»£ç ç”Ÿæˆç‰¹åˆ«æœ‰ç”¨ã€‚ç„¶è€Œï¼Œå½“å‰é’ˆå¯¹dLLMsåœ¨ç¼–ç æ–¹é¢çš„è®­ç»ƒå’Œæ¨ç†æœºåˆ¶ä»è¢«ç ”ç©¶å¾—ä¸å¤Ÿé€å½»ã€‚ä¸ºäº†æ­å¼€dLLMsçš„è§£ç è¡Œä¸ºä¹‹è°œå¹¶è§£é”å…¶åœ¨ç¼–ç æ–¹é¢çš„æ½œåŠ›ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°ç ”ç©¶äº†å®ƒä»¬çš„é™å™ªè¿‡ç¨‹å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•ã€‚æˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ª7Bçš„dLLMï¼Œåä¸º<strong>DiffuCoder</strong>ï¼Œåœ¨130Bä¸ªä»£ç æ ‡è®°ä¸Šè¿›è¡Œè®­ç»ƒã€‚ä½¿ç”¨è¯¥æ¨¡å‹ä½œä¸ºæµ‹è¯•å¹³å°ï¼Œæˆ‘ä»¬åˆ†æäº†å…¶è§£ç è¡Œä¸ºï¼Œæ­ç¤ºäº†å®ƒä¸ARæ¨¡å‹çš„ä¸åŒä¹‹å¤„ï¼šï¼ˆ1ï¼‰dLLMå¯ä»¥å†³å®šå…¶ç”Ÿæˆç»“æœçš„å› æœæ€§ï¼Œè€Œæ— éœ€ä¾èµ–åŠè‡ªå›å½’è§£ç ï¼›ï¼ˆ2ï¼‰å¢åŠ é‡‡æ ·æ¸©åº¦ä¸ä»…ä½¿æ ‡è®°é€‰æ‹©å¤šæ ·åŒ–ï¼Œè€Œä¸”ä½¿å…¶ç”Ÿæˆé¡ºåºä¹Ÿå¤šæ ·åŒ–ã€‚è¿™ç§å¤šæ ·æ€§ä¸ºRLå›åˆæä¾›äº†ä¸°å¯Œçš„æœç´¢ç©ºé—´ã€‚å¯¹äºRLè®­ç»ƒï¼Œä¸ºäº†å‡å°‘æ ‡è®°å¯¹æ•°ä¼¼ç„¶ä¼°è®¡çš„æ–¹å·®å¹¶ä¿æŒè®­ç»ƒæ•ˆç‡ï¼Œæˆ‘ä»¬æå‡ºäº†<strong>coupled-GRPO</strong>ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„é‡‡æ ·æ–¹æ¡ˆï¼Œä¸ºè®­ç»ƒä¸­ä½¿ç”¨çš„å®Œæˆéƒ¨åˆ†æ„å»ºäº†äº’è¡¥çš„æ©ç å™ªå£°ã€‚åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼Œcoupled-GRPOæ˜¾è‘—æé«˜äº†DiffuCoderåœ¨ä»£ç ç”ŸæˆåŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½ï¼ˆåœ¨EvalPlusä¸Šæé«˜äº†4.4ï¼…ï¼‰ï¼Œå¹¶å‡å°‘äº†è§£ç è¿‡ç¨‹ä¸­ARåå€šçš„ä¾èµ–ã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ºdLLMç”Ÿæˆæœºåˆ¶æä¾›äº†æ›´æ·±å…¥çš„äº†è§£ï¼Œå¹¶æä¾›äº†ä¸€ä¸ªæœ‰æ•ˆçš„ã€åŸºäºæ‰©æ•£çš„RLè®­ç»ƒæ¡†æ¶ã€‚è¯¦æƒ…è¯·è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/apple/ml-diffucoder%E3%80%82">https://github.com/apple/ml-diffucoderã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20639v2">PDF</a> minor update</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ‰©æ•£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆdLLMsï¼‰ä½œä¸ºå¯¹è‡ªå›å½’ï¼ˆARï¼‰æ¨¡å‹çš„å¸å¼•äººçš„æ›¿ä»£æ–¹æ¡ˆï¼Œå…¶é™å™ªæ¨¡å‹åœ¨æ•´ä¸ªåºåˆ—ä¸Šè¿è¡Œã€‚dLLMsçš„å…¨å±€è§„åˆ’å’Œè¿­ä»£ä¼˜åŒ–åŠŸèƒ½å¯¹ä»£ç ç”Ÿæˆç‰¹åˆ«æœ‰ç”¨ã€‚ç„¶è€Œï¼Œç›®å‰å¯¹äºdLLMsåœ¨ç¼–ç æ–¹é¢çš„è®­ç»ƒå’Œæ¨ç†æœºåˆ¶ä»åœ¨æ¢ç´¢é˜¶æ®µã€‚ä¸ºäº†æ­å¼€dLLMsè§£ç è¡Œä¸ºçš„å¥¥ç§˜å¹¶è§£é”å…¶åœ¨ç¼–ç æ–¹é¢çš„æ½œåŠ›ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°ç ”ç©¶äº†å®ƒä»¬çš„é™å™ªè¿‡ç¨‹å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•ã€‚æˆ‘ä»¬åœ¨130Bä»¤ç‰Œä»£ç ä¸Šè®­ç»ƒäº†ä¸€ä¸ª7Bçš„dLLMæ¨¡å‹DiffuCoderã€‚ä»¥è¯¥æ¨¡å‹ä¸ºæµ‹è¯•å¹³å°ï¼Œæˆ‘ä»¬åˆ†æäº†å…¶è§£ç è¡Œä¸ºï¼Œæ­ç¤ºäº†å…¶ä¸ARæ¨¡å‹çš„ä¸åŒä¹‹å¤„ï¼šdLLMsèƒ½å¤Ÿåœ¨ä¸ä¾èµ–åŠè‡ªå›å½’è§£ç çš„æƒ…å†µä¸‹å†³å®šç”Ÿæˆçš„å› æœæ€§ï¼›å¢åŠ é‡‡æ ·æ¸©åº¦ä¸ä»…ä½¿ä»¤ç‰Œé€‰æ‹©å¤šæ ·åŒ–ï¼Œè€Œä¸”ä½¿ç”Ÿæˆé¡ºåºä¹Ÿå¤šæ ·åŒ–ã€‚è¿™ç§å¤šæ ·æ€§ä¸ºRLå›åˆæä¾›äº†ä¸°å¯Œçš„æœç´¢ç©ºé—´ã€‚é’ˆå¯¹RLè®­ç»ƒï¼Œä¸ºäº†å‡å°‘ä»¤ç‰Œå¯¹æ•°ä¼¼ç„¶ä¼°è®¡çš„æ–¹å·®å¹¶ä¿æŒè®­ç»ƒæ•ˆç‡ï¼Œæˆ‘ä»¬æå‡ºäº†è€¦åˆGRPOï¼ˆcoupled-GRPOï¼‰è¿™ä¸€æ–°çš„é‡‡æ ·æ–¹æ¡ˆï¼Œç”¨äºæ„å»ºè®­ç»ƒä¸­ä½¿ç”¨çš„è¡¥ç æ©ç å™ªå£°ã€‚åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼Œè€¦åˆGRPOæ˜¾è‘—æé«˜äº†DiffuCoderåœ¨ä»£ç ç”ŸæˆåŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½ï¼ˆåœ¨EvalPlusä¸Šæé«˜äº†4.4%ï¼‰ï¼Œå¹¶å‡å°‘äº†è§£ç è¿‡ç¨‹ä¸­ARåå·®çš„ä¾èµ–ã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ºdLLMç”Ÿæˆæœºåˆ¶æä¾›äº†æ›´æ·±å…¥çš„äº†è§£ï¼Œå¹¶æä¾›äº†ä¸€ä¸ªæœ‰æ•ˆçš„æ‰©æ•£åŸç”ŸRLè®­ç»ƒæ¡†æ¶ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ‰©æ•£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆdLLMsï¼‰ä½œä¸ºè‡ªå›å½’ï¼ˆARï¼‰æ¨¡å‹çš„æ›¿ä»£æ–¹æ¡ˆï¼Œå…¶å…¨å±€è§„åˆ’å’Œè¿­ä»£ä¼˜åŒ–ç‰¹æ€§åœ¨ä»£ç ç”Ÿæˆä¸­éå¸¸æœ‰ç”¨ã€‚</li>
<li>dLLMsçš„è§£ç è¡Œä¸ºä¸åŒäºARæ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨ä¸ä¾èµ–åŠè‡ªå›å½’è§£ç çš„æƒ…å†µä¸‹å†³å®šç”Ÿæˆçš„å› æœæ€§ã€‚</li>
<li>å¢åŠ é‡‡æ ·æ¸©åº¦å¯ä»¥å¯¼è‡´æ›´å¤šæ ·åŒ–çš„ä»¤ç‰Œé€‰æ‹©å’Œç”Ÿæˆé¡ºåºï¼Œä¸ºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒåˆ›å»ºä¸°å¯Œçš„æœç´¢ç©ºé—´ã€‚</li>
<li>é’ˆå¯¹RLè®­ç»ƒï¼Œæå‡ºäº†è€¦åˆGRPOï¼ˆcoupled-GRPOï¼‰è¿™ä¸€æ–°çš„é‡‡æ ·æ–¹æ¡ˆï¼Œä»¥æé«˜DiffuCoderåœ¨ä»£ç ç”Ÿæˆæ–¹é¢çš„æ€§èƒ½ã€‚</li>
<li>è€¦åˆGRPOèƒ½å¤Ÿæ˜¾è‘—æé«˜æ¨¡å‹åœ¨ä»£ç ç”ŸæˆåŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½ï¼Œå¹¶å‡å°‘è§£ç è¿‡ç¨‹ä¸­å¯¹ARåå·®çš„ä¾èµ–ã€‚</li>
<li>ç ”ç©¶å·¥ä½œä¸ºdLLMç”Ÿæˆæœºåˆ¶æä¾›äº†æ›´æ·±å…¥çš„äº†è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20639">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b661d512447bdbe4fad5845e74cf08cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1fab6f03b8677a7805ff29cfa9802755.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-894b6e6328ece7c23bc3d00fd9739b61.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc7edce831c554d0a6d32308f871a1e8.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="From-Web-Search-towards-Agentic-Deep-Research-Incentivizing-Search-with-Reasoning-Agents"><a href="#From-Web-Search-towards-Agentic-Deep-Research-Incentivizing-Search-with-Reasoning-Agents" class="headerlink" title="From Web Search towards Agentic Deep Research: Incentivizing Search with   Reasoning Agents"></a>From Web Search towards Agentic Deep Research: Incentivizing Search with   Reasoning Agents</h2><p><strong>Authors:Weizhi Zhang, Yangning Li, Yuanchen Bei, Junyu Luo, Guancheng Wan, Liangwei Yang, Chenxuan Xie, Yuyao Yang, Wei-Chieh Huang, Chunyu Miao, Henry Peng Zou, Xiao Luo, Yusheng Zhao, Yankai Chen, Chunkit Chan, Peilin Zhou, Xinyang Zhang, Chenwei Zhang, Jingbo Shang, Ming Zhang, Yangqiu Song, Irwin King, Philip S. Yu</strong></p>
<p>Information retrieval is a cornerstone of modern knowledge acquisition, enabling billions of queries each day across diverse domains. However, traditional keyword-based search engines are increasingly inadequate for handling complex, multi-step information needs. Our position is that Large Language Models (LLMs), endowed with reasoning and agentic capabilities, are ushering in a new paradigm termed Agentic Deep Research. These systems transcend conventional information search techniques by tightly integrating autonomous reasoning, iterative retrieval, and information synthesis into a dynamic feedback loop. We trace the evolution from static web search to interactive, agent-based systems that plan, explore, and learn. We also introduce a test-time scaling law to formalize the impact of computational depth on reasoning and search. Supported by benchmark results and the rise of open-source implementations, we demonstrate that Agentic Deep Research not only significantly outperforms existing approaches, but is also poised to become the dominant paradigm for future information seeking. All the related resources, including industry products, research papers, benchmark datasets, and open-source implementations, are collected for the community in <a target="_blank" rel="noopener" href="https://github.com/DavidZWZ/Awesome-Deep-Research">https://github.com/DavidZWZ/Awesome-Deep-Research</a>. </p>
<blockquote>
<p>ä¿¡æ¯æ£€ç´¢æ˜¯ç°ä»£çŸ¥è¯†è·å–çš„æ ¸å¿ƒåŸºçŸ³ï¼Œæ¯å¤©å¯ä»¥åœ¨ä¸åŒçš„é¢†åŸŸå¤„ç†æ•°åäº¿çš„æŸ¥è¯¢è¯·æ±‚ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„åŸºäºå…³é”®è¯çš„æœç´¢å¼•æ“è¶Šæ¥è¶Šä¸èƒ½æ»¡è¶³å¤æ‚ã€å¤šæ­¥éª¤çš„ä¿¡æ¯éœ€æ±‚ã€‚æˆ‘ä»¬çš„è§‚ç‚¹æ˜¯ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èµ‹äºˆäº†æ¨ç†å’Œæ™ºèƒ½èƒ½åŠ›ï¼Œæ­£åœ¨å¼€åˆ›ä¸€ç§åä¸ºæ™ºèƒ½æ·±åº¦ç ”ç©¶çš„æ–°èŒƒå¼ã€‚è¿™äº›ç³»ç»Ÿé€šè¿‡ç´§å¯†é›†æˆè‡ªä¸»æ¨ç†ã€è¿­ä»£æ£€ç´¢å’Œä¿¡æ¯åˆæˆåˆ°ä¸€ä¸ªåŠ¨æ€åé¦ˆå¾ªç¯ä¸­ï¼Œä»è€Œè¶…è¶Šäº†ä¼ ç»Ÿçš„ä¿¡æ¯æœç´¢æŠ€æœ¯ã€‚æˆ‘ä»¬è¿½è¸ªäº†ä»é™æ€ç½‘é¡µæœç´¢åˆ°äº¤äº’å¼ã€åŸºäºæ™ºèƒ½ç³»ç»Ÿçš„æ¼”å˜ï¼Œè¿™äº›ç³»ç»Ÿå¯ä»¥è®¡åˆ’ã€æ¢ç´¢å’Œå­¦ä¹ èƒ½åŠ›ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªæµ‹è¯•æ—¶é—´ç¼©æ”¾å®šå¾‹ï¼Œä»¥è§„èŒƒè®¡ç®—æ·±åº¦å¯¹æ¨ç†å’Œæœç´¢çš„å½±å“ã€‚åœ¨åŸºå‡†æµ‹è¯•ç»“æœå’Œå¼€æºå®ç°å…´èµ·çš„æ”¯æŒä¸‹ï¼Œæˆ‘ä»¬è¯æ˜äº†æ™ºèƒ½æ·±åº¦ç ”ç©¶ä¸ä»…æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè€Œä¸”è¿˜å°†æˆä¸ºæœªæ¥ä¿¡æ¯æœç´¢çš„ä¸»å¯¼èŒƒå¼ã€‚æ‰€æœ‰ç›¸å…³èµ„æºï¼ŒåŒ…æ‹¬å·¥ä¸šäº§å“ã€ç ”ç©¶è®ºæ–‡ã€åŸºå‡†æ•°æ®é›†å’Œå¼€æºå®ç°ï¼Œéƒ½æ”¶é›†åœ¨<a target="_blank" rel="noopener" href="https://github.com/DavidZWZ/Awesome-Deep-Research%EF%BC%8C%E4%BE%9B%E7%A4%BE%E5%8C%BA%E4%BD%BF%E7%94%A8%E3%80%82">https://github.com/DavidZWZ/Awesome-Deep-Researchï¼Œä¾›ç¤¾åŒºä½¿ç”¨ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18959v2">PDF</a> </p>
<p><strong>Summary</strong><br>åœ¨ä¿¡æ¯æ£€ç´¢é¢†åŸŸï¼Œä¼ ç»ŸåŸºäºå…³é”®è¯çš„æœç´¢å¼•æ“åœ¨å¤„ç†å¤æ‚ã€å¤šæ­¥éª¤çš„ä¿¡æ¯éœ€æ±‚æ—¶è¶Šæ¥è¶Šä¸è¶³ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‡ºç°æ¨åŠ¨äº†æ–°çš„ç ”ç©¶èŒƒå¼â€”â€”æ™ºèƒ½æ·±åº¦ç ”ç©¶çš„å‘å±•ï¼Œé€šè¿‡ç´§å¯†é›†æˆè‡ªä¸»æ¨ç†ã€è¿­ä»£æ£€ç´¢å’Œä¿¡æ¯åˆæˆï¼Œå®ç°äº†åŠ¨æ€åé¦ˆå¾ªç¯ï¼Œæ˜¾è‘—æé«˜ä¿¡æ¯æ£€ç´¢æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚æ™ºèƒ½æ·±åº¦ç ”ç©¶ä¸ä»…åœ¨ä¿¡æ¯æœç´¢é¢†åŸŸæœ‰é‡è¦çš„çªç ´ï¼Œè€Œä¸”ä¸ºæœªæ¥ä¿¡æ¯æœç´¢æä¾›äº†ä¸»å¯¼èŒƒå¼ã€‚ç›¸å…³èµ„æºéƒ½é›†ç»“åœ¨å¼€æºå¹³å°ä¸Šä¾›ç¤¾åŒºä½¿ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ä¿¡æ¯æ£€ç´¢é¢ä¸´ä¼ ç»Ÿæœç´¢å¼•æ“å¤„ç†å¤æ‚éœ€æ±‚çš„ä¸è¶³ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·å¤‡æ¨ç†å’Œè‡ªä¸»èƒ½åŠ›ï¼Œæ¨åŠ¨æ™ºèƒ½æ·±åº¦ç ”ç©¶çš„å‘å±•ã€‚</li>
<li>æ™ºèƒ½æ·±åº¦ç ”ç©¶é€šè¿‡æ•´åˆè‡ªä¸»æ¨ç†ã€è¿­ä»£æ£€ç´¢å’Œä¿¡æ¯åˆæˆï¼Œå®ç°åŠ¨æ€åé¦ˆå¾ªç¯ã€‚</li>
<li>æ™ºèƒ½æ·±åº¦ç ”ç©¶åœ¨ä¿¡æ¯æœç´¢é¢†åŸŸæœ‰æ˜¾è‘—çªç ´ï¼Œæˆä¸ºæœªæ¥ä¸»å¯¼èŒƒå¼ã€‚</li>
<li>æ™ºèƒ½æ·±åº¦ç ”ç©¶é€šè¿‡æµ‹è¯•æ—¶é—´æ¯”ä¾‹å®šå¾‹å½¢å¼åŒ–è®¡ç®—æ·±åº¦å¯¹æ¨ç†å’Œæœç´¢çš„å½±å“ã€‚</li>
<li>å…¬å¼€åŸºå‡†æµ‹è¯•ç»“æœå’Œå¼€æºå®ç°æœ‰åŠ©äºè¯æ˜æ™ºèƒ½æ·±åº¦ç ”ç©¶çš„ä¼˜è¶Šæ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18959">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-606272d05a87c0533b51ed6302057f9f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-76a584b347a5f8a48347111db296ccc0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-155280869ad2bd126b54c3da5ebc7cd6.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="TracLLM-A-Generic-Framework-for-Attributing-Long-Context-LLMs"><a href="#TracLLM-A-Generic-Framework-for-Attributing-Long-Context-LLMs" class="headerlink" title="TracLLM: A Generic Framework for Attributing Long Context LLMs"></a>TracLLM: A Generic Framework for Attributing Long Context LLMs</h2><p><strong>Authors:Yanting Wang, Wei Zou, Runpeng Geng, Jinyuan Jia</strong></p>
<p>Long context large language models (LLMs) are deployed in many real-world applications such as RAG, agent, and broad LLM-integrated applications. Given an instruction and a long context (e.g., documents, PDF files, webpages), a long context LLM can generate an output grounded in the provided context, aiming to provide more accurate, up-to-date, and verifiable outputs while reducing hallucinations and unsupported claims. This raises a research question: how to pinpoint the texts (e.g., sentences, passages, or paragraphs) in the context that contribute most to or are responsible for the generated output by an LLM? This process, which we call context traceback, has various real-world applications, such as 1) debugging LLM-based systems, 2) conducting post-attack forensic analysis for attacks (e.g., prompt injection attack, knowledge corruption attacks) to an LLM, and 3) highlighting knowledge sources to enhance the trust of users towards outputs generated by LLMs. When applied to context traceback for long context LLMs, existing feature attribution methods such as Shapley have sub-optimal performance and&#x2F;or incur a large computational cost. In this work, we develop TracLLM, the first generic context traceback framework tailored to long context LLMs. Our framework can improve the effectiveness and efficiency of existing feature attribution methods. To improve the efficiency, we develop an informed search based algorithm in TracLLM. We also develop contribution score ensemble&#x2F;denoising techniques to improve the accuracy of TracLLM. Our evaluation results show TracLLM can effectively identify texts in a long context that lead to the output of an LLM. Our code and data are at: <a target="_blank" rel="noopener" href="https://github.com/Wang-Yanting/TracLLM">https://github.com/Wang-Yanting/TracLLM</a>. </p>
<blockquote>
<p>é•¿æœŸä¸Šä¸‹æ–‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²éƒ¨ç½²äºè®¸å¤šçœŸå®ä¸–ç•Œçš„åº”ç”¨ä¸­ï¼Œä¾‹å¦‚RAGã€æ™ºèƒ½ä»£ç†å’Œå¹¿æ³›çš„LLMé›†æˆåº”ç”¨ã€‚ç»™å®šæŒ‡ä»¤å’Œé•¿æœŸä¸Šä¸‹æ–‡ï¼ˆä¾‹å¦‚æ–‡æ¡£ã€PDFæ–‡ä»¶ã€ç½‘é¡µï¼‰ï¼Œé•¿æœŸä¸Šä¸‹æ–‡LLMå¯ä»¥ç”ŸæˆåŸºäºæ‰€æä¾›ä¸Šä¸‹æ–‡çš„è¾“å‡ºï¼Œæ—¨åœ¨æä¾›æ›´å‡†ç¡®ã€æœ€æ–°å’Œå¯éªŒè¯çš„è¾“å‡ºï¼ŒåŒæ—¶å‡å°‘å¹»è§‰å’Œæœªç»è¯å®çš„é™ˆè¿°ã€‚è¿™å¼•å‘äº†ä¸€ä¸ªç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•ç¡®å®šåœ¨ä¸Šä¸‹æ–‡ä¸­è´¡çŒ®æœ€å¤§æˆ–è´Ÿè´£LLMç”Ÿæˆçš„è¾“å‡ºçš„æ–‡æœ¬ï¼ˆä¾‹å¦‚å¥å­ã€æ®µè½æˆ–ç¯‡ç« ï¼‰ï¼Ÿæˆ‘ä»¬å°†æ­¤è¿‡ç¨‹ç§°ä¸ºä¸Šä¸‹æ–‡å›æº¯ï¼Œå…·æœ‰å„ç§çœŸå®ä¸–ç•Œçš„åº”ç”¨ï¼Œä¾‹å¦‚1ï¼‰è°ƒè¯•åŸºäºLLMçš„ç³»ç»Ÿï¼Œ2ï¼‰å¯¹LLMè¿›è¡Œæ”»å‡»åçš„æ”»å‡»å–è¯åˆ†æï¼ˆä¾‹å¦‚æç¤ºæ³¨å…¥æ”»å‡»ã€çŸ¥è¯†è…è´¥æ”»å‡»ï¼‰ï¼Œä»¥åŠ3ï¼‰çªå‡ºæ˜¾ç¤ºçŸ¥è¯†æ¥æºï¼Œä»¥å¢å¼ºç”¨æˆ·å¯¹LLMç”Ÿæˆè¾“å‡ºçš„ä¿¡ä»»ã€‚å½“åº”ç”¨äºé•¿æœŸä¸Šä¸‹æ–‡LLMçš„ä¸Šä¸‹æ–‡å›æº¯æ—¶ï¼Œç°æœ‰çš„ç‰¹å¾å½’å› æ–¹æ³•ï¼ˆå¦‚Shapleyï¼‰è¡¨ç°ä¸ä½³å’Œï¼ˆæˆ–ï¼‰è®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚åœ¨æ­¤å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼€å‘äº†TracLLMï¼Œè¿™æ˜¯ä¸“é—¨é’ˆå¯¹é•¿æœŸä¸Šä¸‹æ–‡LLMçš„é¦–ä¸ªé€šç”¨ä¸Šä¸‹æ–‡å›æº¯æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ¡†æ¶å¯ä»¥æé«˜ç°æœ‰ç‰¹å¾å½’å› æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ã€‚ä¸ºæé«˜æ•ˆç‡ï¼Œæˆ‘ä»¬åœ¨TracLLMä¸­å¼€å‘äº†åŸºäºä¿¡æ¯æœç´¢çš„ç®—æ³•ã€‚æˆ‘ä»¬è¿˜å¼€å‘äº†è´¡çŒ®åˆ†æ•°é›†æˆ&#x2F;é™å™ªæŠ€æœ¯ï¼Œä»¥æé«˜TracLLMçš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬çš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒTracLLMå¯ä»¥æœ‰æ•ˆåœ°è¯†åˆ«é•¿æœŸä¸Šä¸‹æ–‡ä¸­å¯¼è‡´LLMè¾“å‡ºçš„æ–‡æœ¬ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®ä½äºï¼š<a target="_blank" rel="noopener" href="https://github.com/Wang-Yanting/TracLLM%E3%80%82">https://github.com/Wang-Yanting/TracLLMã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04202v3">PDF</a> To appear in USENIX Security Symposium 2025. The code and data are   at: <a target="_blank" rel="noopener" href="https://github.com/Wang-Yanting/TracLLM">https://github.com/Wang-Yanting/TracLLM</a></p>
<p><strong>Summary</strong></p>
<p>é•¿è¯­å¢ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å®é™…åº”ç”¨ä¸­å‘æŒ¥ç€é‡è¦ä½œç”¨ï¼Œä¾‹å¦‚åœ¨RAGã€æ™ºèƒ½ä»£ç†å’Œå¹¿æ³›çš„LLMé›†æˆåº”ç”¨ä¸­ã€‚LLMèƒ½å¤Ÿæ ¹æ®æŒ‡ä»¤å’Œé•¿è¯­å¢ƒï¼ˆå¦‚æ–‡æ¡£ã€PDFæ–‡ä»¶ã€ç½‘é¡µï¼‰ç”Ÿæˆè¾“å‡ºï¼Œæ—¨åœ¨æä¾›æ›´å‡†ç¡®ã€æœ€æ–°å’Œå¯éªŒè¯çš„è¾“å‡ºï¼ŒåŒæ—¶å‡å°‘è™šæ„å’Œæœªç»è¯å®çš„å£°æ˜ã€‚è¿™å¼•å‘äº†ä¸€ä¸ªç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•ç¡®å®šæ–‡æœ¬ï¼ˆå¦‚å¥å­ã€æ®µè½ï¼‰åœ¨è¯­å¢ƒä¸­å¯¹LLMç”Ÿæˆçš„è¾“å‡ºè´¡çŒ®æœ€å¤§ï¼Ÿè¿™ä¸ªè¿‡ç¨‹æˆ‘ä»¬ç§°ä¸ºä¸Šä¸‹æ–‡è¿½æº¯ï¼Œå…·æœ‰å¤šç§å®é™…åº”ç”¨ï¼Œä¾‹å¦‚è°ƒè¯•LLMç³»ç»Ÿã€å¯¹LLMè¿›è¡Œæ”»å‡»åçš„æ³•åŒ»å­¦åˆ†æä»¥åŠçªå‡ºæ˜¾ç¤ºçŸ¥è¯†æ¥æºä»¥å¢å¼ºç”¨æˆ·å¯¹LLMç”Ÿæˆè¾“å‡ºçš„ä¿¡ä»»ã€‚é’ˆå¯¹é•¿è¯­å¢ƒLLMçš„ä¸Šä¸‹æ–‡è¿½æº¯ï¼Œç°æœ‰çš„ç‰¹å¾å½’å› æ–¹æ³•ï¼ˆå¦‚Shapleyæ–¹æ³•ï¼‰æ•ˆæœä¸ç†æƒ³ä¸”è®¡ç®—æˆæœ¬é«˜ã€‚æœ¬æ–‡å¼€å‘äº†TracLLMï¼Œä¸€ä¸ªä¸“ä¸ºé•¿è¯­å¢ƒLLMå®šåˆ¶çš„é€šç”¨ä¸Šä¸‹æ–‡è¿½æº¯æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ¡†æ¶å¯ä»¥æ”¹è¿›ç°æœ‰ç‰¹å¾å½’å› æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ã€‚ä¸ºäº†æé«˜æ•ˆç‡ï¼Œæˆ‘ä»¬åœ¨TracLLMä¸­å¼€å‘äº†åŸºäºä¿¡æ¯çš„æœç´¢ç®—æ³•ã€‚æˆ‘ä»¬è¿˜å¼€å‘äº†è´¡çŒ®åˆ†æ•°é›†åˆ&#x2F;é™å™ªæŠ€æœ¯æ¥æé«˜TracLLMçš„å‡†ç¡®æ€§ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒTracLLMå¯ä»¥æœ‰æ•ˆåœ°è¯†åˆ«é•¿è¯­å¢ƒä¸­å¯¼è‡´LLMè¾“å‡ºçš„æ–‡æœ¬ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs are widely applied in real-world scenarios such as RAG, agent, and integrated applications.</li>
<li>LLMs can generate outputs grounded in provided contexts, aiming for accuracy, up-to-date information, and verifiability.</li>
<li>A research question arises: identifying the texts in the context that contribute most to the output generated by LLMs, known as context traceback.</li>
<li>Context traceback has applications in debugging LLM-based systems, post-attack forensic analysis, and enhancing user trust in LLM outputs by highlighting knowledge sources.</li>
<li>Existing feature attribution methods for context traceback in long context LLMs have sub-optimal performance and high computational costs.</li>
<li>TracLLM, a generic context traceback framework tailored to long context LLMs, is developed to improve effectiveness and efficiency.</li>
<li>TracLLM incorporates an informed search algorithm and contribution score ensemble&#x2F;denoising techniques to enhance accuracy.</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04202">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1f1968b4ad49201ed93fd427e1d05f02.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f013b6f939e53ca40d5a688f2bf64e69.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-68045b6106fec5bfb9494f37f3309866.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Ophora-A-Large-Scale-Data-Driven-Text-Guided-Ophthalmic-Surgical-Video-Generation-Model"><a href="#Ophora-A-Large-Scale-Data-Driven-Text-Guided-Ophthalmic-Surgical-Video-Generation-Model" class="headerlink" title="Ophora: A Large-Scale Data-Driven Text-Guided Ophthalmic Surgical Video   Generation Model"></a>Ophora: A Large-Scale Data-Driven Text-Guided Ophthalmic Surgical Video   Generation Model</h2><p><strong>Authors:Wei Li, Ming Hu, Guoan Wang, Lihao Liu, Kaijin Zhou, Junzhi Ning, Xin Guo, Zongyuan Ge, Lixu Gu, Junjun He</strong></p>
<p>In ophthalmic surgery, developing an AI system capable of interpreting surgical videos and predicting subsequent operations requires numerous ophthalmic surgical videos with high-quality annotations, which are difficult to collect due to privacy concerns and labor consumption. Text-guided video generation (T2V) emerges as a promising solution to overcome this issue by generating ophthalmic surgical videos based on surgeon instructions. In this paper, we present Ophora, a pioneering model that can generate ophthalmic surgical videos following natural language instructions. To construct Ophora, we first propose a Comprehensive Data Curation pipeline to convert narrative ophthalmic surgical videos into a large-scale, high-quality dataset comprising over 160K video-instruction pairs, Ophora-160K. Then, we propose a Progressive Video-Instruction Tuning scheme to transfer rich spatial-temporal knowledge from a T2V model pre-trained on natural video-text datasets for privacy-preserved ophthalmic surgical video generation based on Ophora-160K. Experiments on video quality evaluation via quantitative analysis and ophthalmologist feedback demonstrate that Ophora can generate realistic and reliable ophthalmic surgical videos based on surgeon instructions. We also validate the capability of Ophora for empowering downstream tasks of ophthalmic surgical workflow understanding. Code is available at <a target="_blank" rel="noopener" href="https://github.com/mar-cry/Ophora">https://github.com/mar-cry/Ophora</a>. </p>
<blockquote>
<p>åœ¨çœ¼ç§‘æ‰‹æœ¯ä¸­ï¼Œå¼€å‘ä¸€ä¸ªèƒ½å¤Ÿè§£è¯»æ‰‹æœ¯è§†é¢‘å¹¶é¢„æµ‹åç»­æ“ä½œçš„AIç³»ç»Ÿï¼Œéœ€è¦å¤§é‡çš„å¸¦æœ‰é«˜è´¨é‡æ³¨é‡Šçš„çœ¼ç§‘æ‰‹æœ¯è§†é¢‘ã€‚ç”±äºéšç§é—®é¢˜å’ŒåŠ³åŠ¨æ¶ˆè€—ï¼Œè¿™äº›è§†é¢‘çš„æ”¶é›†éå¸¸å›°éš¾ã€‚æ–‡æœ¬å¼•å¯¼çš„è§†é¢‘ç”Ÿæˆï¼ˆT2Vï¼‰ä½œä¸ºä¸€ç§æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆåº”è¿è€Œç”Ÿï¼Œå®ƒå¯ä»¥æ ¹æ®å¤–ç§‘åŒ»ç”Ÿçš„æŒ‡ä»¤ç”Ÿæˆçœ¼ç§‘æ‰‹æœ¯è§†é¢‘ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å…ˆè¿›çš„æ¨¡å‹â€”â€”Ophoraï¼Œå®ƒå¯ä»¥æ ¹æ®è‡ªç„¶è¯­è¨€æŒ‡ä»¤ç”Ÿæˆçœ¼ç§‘æ‰‹æœ¯è§†é¢‘ã€‚ä¸ºäº†æ„å»ºOphoraï¼Œæˆ‘ä»¬é¦–å…ˆæå‡ºäº†ä¸€ç§å…¨é¢çš„æ•°æ®æ•´ç†ç®¡é“ï¼Œå°†å™è¿°æ€§çœ¼ç§‘æ‰‹æœ¯è§†é¢‘è½¬åŒ–ä¸ºå¤§è§„æ¨¡é«˜è´¨é‡æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡16ä¸‡å¯¹è§†é¢‘æŒ‡ä»¤å¯¹ï¼Œåä¸ºOphora-160Kã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ¸è¿›çš„è§†é¢‘æŒ‡ä»¤è°ƒæ•´æ–¹æ¡ˆï¼Œå°†ä¸°å¯Œçš„æ—¶ç©ºçŸ¥è¯†ä»ä¸€ä¸ªåœ¨å¤©ç„¶è§†é¢‘æ–‡æœ¬æ•°æ®é›†ä¸Šé¢„è®­ç»ƒçš„T2Væ¨¡å‹è½¬ç§»åˆ°åŸºäºOphora-160Kçš„éšç§ä¿æŠ¤çœ¼ç§‘æ‰‹æœ¯è§†é¢‘ç”Ÿæˆä¸­ã€‚é€šè¿‡å¯¹è§†é¢‘è´¨é‡çš„å®šé‡åˆ†æå’Œçœ¼ç§‘åŒ»ç”Ÿçš„åé¦ˆè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒOphoraå¯ä»¥æ ¹æ®å¤–ç§‘åŒ»ç”Ÿçš„æŒ‡ä»¤ç”Ÿæˆç°å®å’Œå¯é çš„çœ¼ç§‘æ‰‹æœ¯è§†é¢‘ã€‚æˆ‘ä»¬è¿˜éªŒè¯äº†Ophoraåœ¨çœ¼ç§‘æ‰‹æœ¯å·¥ä½œæµç¨‹ç†è§£ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„èƒ½åŠ›ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/mar-cry/Ophora%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/mar-cry/Ophoraæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07449v6">PDF</a> Early accepted in MICCAI25</p>
<p><strong>Summary</strong></p>
<p>åŸºäºè‡ªç„¶è¯­è¨€æŒ‡ä»¤ç”Ÿæˆçœ¼ç§‘æ‰‹æœ¯è§†é¢‘çš„æ–°å‹æ¨¡å‹â€”â€”Ophoraã€‚é€šè¿‡å¤§è§„æ¨¡æ•°æ®é›†å’Œé«˜æ•ˆè®­ç»ƒç­–ç•¥ï¼Œå®ç°çœ¼ç§‘æ‰‹æœ¯è§†é¢‘çš„ç”Ÿæˆå’Œä¸‹æ¸¸ä»»åŠ¡åº”ç”¨ã€‚æ¨¡å‹æœ‰åŠ©äºè§£å†³çœ¼ç§‘æ‰‹æœ¯è§†é¢‘æ•°æ®é‡‡é›†å›°éš¾çš„é—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>çœ¼ç§‘æ‰‹æœ¯è§†é¢‘ç”Ÿæˆé¢ä¸´æ•°æ®æ”¶é›†å›°éš¾çš„é—®é¢˜ï¼Œä¸»è¦ç”±äºéšç§æ‹…å¿§å’ŒåŠ³åŠ¨å¯†é›†æ€§ã€‚</li>
<li>Text-guided è§†é¢‘ç”Ÿæˆï¼ˆT2Vï¼‰æŠ€æœ¯ä¸ºè§£å†³æ­¤é—®é¢˜æä¾›äº†æœ‰æ•ˆé€”å¾„ã€‚</li>
<li>Ophoraæ¨¡å‹æ˜¯ä¸€ä¸ªåŸºäºè‡ªç„¶è¯­è¨€æŒ‡ä»¤ç”Ÿæˆçœ¼ç§‘æ‰‹æœ¯è§†é¢‘çš„å¼€åˆ›æ€§æ¨¡å‹ã€‚</li>
<li>Comprehensive Data Curation pipelineç”¨äºå°†å™äº‹çœ¼ç§‘æ‰‹æœ¯è§†é¢‘è½¬åŒ–ä¸ºå¤§è§„æ¨¡é«˜è´¨é‡æ•°æ®é›†â€”â€”Ophora-160Kã€‚</li>
<li>Progressive Video-Instruction Tuningæ–¹æ¡ˆç”¨äºä»é¢„è®­ç»ƒåœ¨è‡ªç„¶è§†é¢‘æ–‡æœ¬æ•°æ®é›†ä¸Šçš„T2Væ¨¡å‹è½¬ç§»ç©ºé—´æ—¶é—´çŸ¥è¯†ï¼Œä»¥å®ç°åŸºäºéšç§ä¿æŠ¤çš„çœ¼ç§‘æ‰‹æœ¯è§†é¢‘ç”Ÿæˆã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒOphoraå¯ä»¥æ ¹æ®å¤–ç§‘åŒ»ç”ŸæŒ‡ä»¤ç”Ÿæˆé€¼çœŸå¯é çš„çœ¼ç§‘æ‰‹æœ¯è§†é¢‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07449">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-23e32262ce9cc710f89be253a8097d8d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a4ec3b71a8c06c8f6d824bb3db7290c8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cca6f327a9e0a5dece7366cde8a1f565.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Generalized-Tensor-based-Parameter-Efficient-Fine-Tuning-via-Lie-Group-Transformations"><a href="#Generalized-Tensor-based-Parameter-Efficient-Fine-Tuning-via-Lie-Group-Transformations" class="headerlink" title="Generalized Tensor-based Parameter-Efficient Fine-Tuning via Lie Group   Transformations"></a>Generalized Tensor-based Parameter-Efficient Fine-Tuning via Lie Group   Transformations</h2><p><strong>Authors:Chongjie Si, Zhiyi Shi, Xuehui Wang, Yichen Xiao, Xiaokang Yang, Wei Shen</strong></p>
<p>Adapting pre-trained foundation models for diverse downstream tasks is a core practice in artificial intelligence. However, the wide range of tasks and high computational costs make full fine-tuning impractical. To overcome this, parameter-efficient fine-tuning (PEFT) methods like LoRA have emerged and are becoming a growing research focus. Despite the success of these methods, they are primarily designed for linear layers, focusing on two-dimensional matrices while largely ignoring higher-dimensional parameter spaces like convolutional kernels. Moreover, directly applying these methods to higher-dimensional parameter spaces often disrupts their structural relationships. Given the rapid advancements in matrix-based PEFT methods, rather than designing a specialized strategy, we propose a generalization that extends matrix-based PEFT methods to higher-dimensional parameter spaces without compromising their structural properties. Specifically, we treat parameters as elements of a Lie group, with updates modeled as perturbations in the corresponding Lie algebra. These perturbations are mapped back to the Lie group through the exponential map, ensuring smooth, consistent updates that preserve the inherent structure of the parameter space. Extensive experiments on computer vision and natural language processing validate the effectiveness and versatility of our approach, demonstrating clear improvements over existing methods. </p>
<blockquote>
<p>åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸï¼Œé€‚åº”é¢„è®­ç»ƒçš„åŸºçŸ³æ¨¡å‹ä»¥æ‰§è¡Œå¤šç§ä¸‹æ¸¸ä»»åŠ¡æ˜¯æ ¸å¿ƒå®è·µã€‚ç„¶è€Œï¼Œä»»åŠ¡èŒƒå›´çš„å¹¿æ³›å’Œè®¡ç®—æˆæœ¬é«˜æ˜‚ä½¿å¾—å…¨é¢å¾®è°ƒå˜å¾—ä¸åˆ‡å®é™…ã€‚ä¸ºäº†å…‹æœè¿™ä¸€éš¾é¢˜ï¼Œå‡ºç°äº†åƒLoRAè¿™æ ·çš„å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•ï¼Œå¹¶é€æ¸æˆä¸ºä¸æ–­å¢é•¿çš„ç ”ç©¶ç„¦ç‚¹ã€‚å°½ç®¡è¿™äº›æ–¹æ³•å–å¾—äº†æˆåŠŸï¼Œä½†å®ƒä»¬ä¸»è¦è®¾è®¡ç”¨äºçº¿æ€§å±‚ï¼Œä¾§é‡äºäºŒç»´çŸ©é˜µï¼Œè€Œå¾€å¾€å¿½ç•¥äº†å·ç§¯æ ¸ç­‰æ›´é«˜ç»´å‚æ•°ç©ºé—´ã€‚æ­¤å¤–ï¼Œå°†è¿™äº›æ–¹æ³•ç›´æ¥åº”ç”¨äºæ›´é«˜ç»´å‚æ•°ç©ºé—´å¾€å¾€ä¼šç ´åå…¶ç»“æ„å…³ç³»ã€‚è€ƒè™‘åˆ°åŸºäºçŸ©é˜µçš„PEFTæ–¹æ³•çš„å¿«é€Ÿå‘å±•ï¼Œæˆ‘ä»¬å¹¶æ²¡æœ‰è®¾è®¡ä¸€ç§ä¸“é—¨ç­–ç•¥ï¼Œè€Œæ˜¯æå‡ºäº†ä¸€ç§é€šç”¨æ–¹æ³•ï¼Œå°†åŸºäºçŸ©é˜µçš„PEFTæ–¹æ³•æ‰©å±•åˆ°æ›´é«˜ç»´å‚æ•°ç©ºé—´ï¼Œè€Œä¸ä¼šå¯¹å®ƒä»¬çš„ç»“æ„å±æ€§é€ æˆæŸå®³ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†å‚æ•°è§†ä¸ºæç¾¤ï¼ˆLie groupï¼‰çš„å…ƒç´ ï¼Œå°†æ›´æ–°å»ºæ¨¡ä¸ºç›¸åº”æä»£æ•°ä¸­çš„æ‰°åŠ¨ã€‚è¿™äº›æ‰°åŠ¨é€šè¿‡æŒ‡æ•°æ˜ å°„æ˜ å°„å›æç¾¤ï¼Œç¡®ä¿å¹³æ»‘ã€ä¸€è‡´çš„æ›´æ–°ï¼Œä¿ç•™å‚æ•°ç©ºé—´çš„å›ºæœ‰ç»“æ„ã€‚åœ¨è®¡ç®—æœºè§†è§‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†æ–¹é¢çš„å¹¿æ³›å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ï¼Œæ˜¾ç¤ºå‡ºç›¸è¾ƒäºç°æœ‰æ–¹æ³•çš„æ˜æ˜¾æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00851v2">PDF</a> 2025 ICCV</p>
<p><strong>Summary</strong></p>
<p>é¢„è®­ç»ƒæ¨¡å‹åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸå¹¿æ³›åº”ç”¨ï¼Œä½†é’ˆå¯¹å¤šç§ä¸‹æ¸¸ä»»åŠ¡çš„é€‚åº”é¢ä¸´è®¡ç®—æˆæœ¬é«˜çš„æŒ‘æˆ˜ã€‚å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•å¦‚LoRAç­‰åº”è¿è€Œç”Ÿã€‚ç„¶è€Œï¼Œç°æœ‰PEFTæ–¹æ³•ä¸»è¦å…³æ³¨äºŒç»´çŸ©é˜µï¼Œå¿½ç•¥é«˜ç»´å‚æ•°ç©ºé—´ã€‚æœ¬æ–‡æå‡ºä¸€ç§é€šç”¨æ–¹æ³•ï¼Œå°†çŸ©é˜µå‹PEFTæ–¹æ³•æ¨å¹¿åˆ°é«˜ç»´å‚æ•°ç©ºé—´ï¼Œé€šè¿‡Lieç¾¤å’ŒLieä»£æ•°ç†è®ºè¿›è¡Œæ›´æ–°æ˜ å°„ï¼Œå®ç°å¹³æ»‘ã€ä¸€è‡´çš„æ›´æ–°ï¼Œä¿æŒå‚æ•°ç©ºé—´çš„å›ºæœ‰ç»“æ„ã€‚å®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨è®¡ç®—æœºè§†è§‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸Šæœ‰æ•ˆä¸”ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢„è®­ç»ƒæ¨¡å‹å¹¿æ³›åº”ç”¨äºå¤šç§ä¸‹æ¸¸ä»»åŠ¡ï¼Œä½†é¢ä¸´è®¡ç®—æˆæœ¬é«˜çš„æŒ‘æˆ˜ã€‚</li>
<li>å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ˜¯è§£å†³æ­¤æŒ‘æˆ˜çš„ä¸€ç§æœ‰æ•ˆæ–¹æ³•ã€‚</li>
<li>ç°æœ‰PEFTæ–¹æ³•ä¸»è¦å…³æ³¨äºŒç»´çŸ©é˜µï¼Œå¿½ç•¥äº†é«˜ç»´å‚æ•°ç©ºé—´ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šç”¨æ–¹æ³•ï¼Œå°†çŸ©é˜µå‹PEFTæ–¹æ³•æ‰©å±•åˆ°é«˜ç»´å‚æ•°ç©ºé—´ã€‚</li>
<li>è¯¥æ–¹æ³•åŸºäºLieç¾¤å’ŒLieä»£æ•°ç†è®ºè¿›è¡Œæ›´æ–°æ˜ å°„ï¼Œç¡®ä¿å¹³æ»‘ã€ä¸€è‡´çš„æ›´æ–°ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨è®¡ç®—æœºè§†è§‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸Šè¡¨ç°å‡ºä¼˜äºç°æœ‰æ–¹æ³•çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00851">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-51161df532052bad1e5ae45448e09efc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-47cc4c78af3f7a469ba264e74d19e7d5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f2e8f4408d32bd19895b3598ffac511.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aba822b3d3f71bdbf83037b3d3374c16.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="STI-Bench-Are-MLLMs-Ready-for-Precise-Spatial-Temporal-World-Understanding"><a href="#STI-Bench-Are-MLLMs-Ready-for-Precise-Spatial-Temporal-World-Understanding" class="headerlink" title="STI-Bench: Are MLLMs Ready for Precise Spatial-Temporal World   Understanding?"></a>STI-Bench: Are MLLMs Ready for Precise Spatial-Temporal World   Understanding?</h2><p><strong>Authors:Yun Li, Yiming Zhang, Tao Lin, XiangRui Liu, Wenxiao Cai, Zheng Liu, Bo Zhao</strong></p>
<p>The use of Multimodal Large Language Models (MLLMs) as an end-to-end solution for Embodied AI and Autonomous Driving has become a prevailing trend. While MLLMs have been extensively studied for visual semantic understanding tasks, their ability to perform precise and quantitative spatial-temporal understanding in real-world applications remains largely unexamined, leading to uncertain prospects. To evaluate modelsâ€™ Spatial-Temporal Intelligence, we introduce STI-Bench, a benchmark designed to evaluate MLLMsâ€™ spatial-temporal understanding through challenging tasks such as estimating and predicting the appearance, pose, displacement, and motion of objects. Our benchmark encompasses a wide range of robot and vehicle operations across desktop, indoor, and outdoor scenarios. The extensive experiments reveals that the state-of-the-art MLLMs still struggle in real-world spatial-temporal understanding, especially in tasks requiring precise distance estimation and motion analysis. </p>
<blockquote>
<p>ä½¿ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä½œä¸ºåµŒå…¥å¼äººå·¥æ™ºèƒ½å’Œè‡ªåŠ¨é©¾é©¶çš„ç«¯åˆ°ç«¯è§£å†³æ–¹æ¡ˆå·²æˆä¸ºä¸€ç§æµè¡Œè¶‹åŠ¿ã€‚è™½ç„¶MLLMsåœ¨è§†è§‰è¯­ä¹‰ç†è§£ä»»åŠ¡æ–¹é¢å·²è¢«å¹¿æ³›ç ”ç©¶ï¼Œä½†å®ƒä»¬åœ¨ç°å®åº”ç”¨ä¸­è¿›è¡Œç²¾ç¡®å’Œå®šé‡æ—¶ç©ºç†è§£çš„èƒ½åŠ›åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå°šæœªå¾—åˆ°æ£€éªŒï¼Œå› æ­¤å‰æ™¯ä¸ç¡®å®šã€‚ä¸ºäº†è¯„ä¼°æ¨¡å‹çš„æ—¶ç©ºæ™ºèƒ½ï¼Œæˆ‘ä»¬å¼•å…¥äº†STI-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨é€šè¿‡ä¼°è®¡å’Œé¢„æµ‹å¯¹è±¡çš„å¤–è§‚ã€å§¿åŠ¿ã€ä½ç§»å’Œè¿åŠ¨ç­‰å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡æ¥è¯„ä¼°MLLMsçš„æ—¶ç©ºç†è§£èƒ½åŠ›ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•æ¶µç›–äº†æ¡Œé¢ã€å®¤å†…å’Œå®¤å¤–åœºæ™¯ä¸­çš„æœºå™¨äººå’Œè½¦è¾†æ“ä½œçš„å¹¿æ³›èŒƒå›´ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæœ€å…ˆè¿›çš„MLLMsåœ¨çœŸå®ä¸–ç•Œçš„æ—¶ç©ºç†è§£æ–¹é¢ä»ç„¶é¢ä¸´å›°éš¾ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦ç²¾ç¡®è·ç¦»ä¼°è®¡å’Œè¿åŠ¨åˆ†æçš„ä»»åŠ¡ä¸­ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.23765v5">PDF</a> </p>
<p><strong>æ€»ç»“</strong></p>
<p>åŸºäºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä½œä¸ºç«¯åˆ°ç«¯çš„è§£å†³æ–¹æ¡ˆï¼Œåœ¨åµŒå…¥å¼äººå·¥æ™ºèƒ½å’Œè‡ªåŠ¨é©¾é©¶é¢†åŸŸå·²æˆä¸ºæµè¡Œè¶‹åŠ¿ã€‚è™½ç„¶MLLMsåœ¨è§†è§‰è¯­ä¹‰ç†è§£ä»»åŠ¡ä¸Šå·²è¢«å¹¿æ³›ç ”ç©¶ï¼Œä½†å®ƒä»¬åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­å®ç°ç²¾ç¡®å’Œå®šé‡æ—¶ç©ºç†è§£çš„èƒ½åŠ›å°šæœªå¾—åˆ°å……åˆ†è€ƒå¯Ÿï¼Œå‰æ™¯å°šä¸ç¡®å®šã€‚ä¸ºäº†è¯„ä¼°æ¨¡å‹çš„æ—¶ç©ºæ™ºèƒ½ï¼Œæˆ‘ä»¬å¼•å…¥äº†STI-BenchåŸºå‡†æµ‹è¯•å¹³å°ï¼Œè¯¥å¹³å°æ—¨åœ¨é€šè¿‡ä¼°è®¡å’Œé¢„æµ‹ç‰©ä½“å¤–è§‚ã€å§¿æ€ã€ä½ç§»å’Œè¿åŠ¨ç­‰å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡æ¥è¯„ä¼°MLLMsçš„æ—¶ç©ºç†è§£èƒ½åŠ›ã€‚è¯¥åŸºå‡†æµ‹è¯•å¹³å°æ¶µç›–äº†æ¡Œé¢ã€å®¤å†…å’Œå®¤å¤–åœºæ™¯ä¸­çš„å¹¿æ³›æœºå™¨äººå’Œè½¦è¾†æ“ä½œã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæœ€å…ˆè¿›çš„MLLMsåœ¨çœŸå®ä¸–ç•Œçš„æ—¶ç©ºç†è§£æ–¹é¢ä»æœ‰å›°éš¾ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦ç²¾ç¡®è·ç¦»ä¼°è®¡å’Œè¿åŠ¨åˆ†æçš„ä»»åŠ¡ä¸­ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ul>
<li>MLLMsåœ¨åµŒå…¥å¼äººå·¥æ™ºèƒ½å’Œè‡ªåŠ¨é©¾é©¶é¢†åŸŸä½œä¸ºç«¯åˆ°ç«¯çš„è§£å†³æ–¹æ¡ˆå·²ç»å˜å¾—éå¸¸æµè¡Œã€‚</li>
<li>è™½ç„¶MLLMsåœ¨è§†è§‰è¯­ä¹‰ç†è§£æ–¹é¢å·²ç»æœ‰æ‰€ç ”ç©¶ï¼Œä½†å®ƒä»¬åœ¨æ—¶ç©ºç†è§£æ–¹é¢çš„èƒ½åŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚</li>
<li>æ¨å‡ºäº†STI-BenchåŸºå‡†æµ‹è¯•å¹³å°æ¥è¯„ä¼°MLLMsçš„æ—¶ç©ºç†è§£èƒ½åŠ›ã€‚</li>
<li>STI-Benchæ¶µç›–å¤šç§æœºå™¨äººå’Œè½¦è¾†æ“ä½œï¼ŒåŒ…æ‹¬æ¡Œé¢ã€å®¤å†…å’Œå®¤å¤–åœºæ™¯ã€‚</li>
<li>å…ˆè¿›çš„MLLMsåœ¨çœŸå®ä¸–ç•Œçš„æ—¶ç©ºç†è§£æ–¹é¢ä»æœ‰å›°éš¾ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.23765">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7821c5de0465fc118f548552908d4e56.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bb32d35a7995b9fe54def618b1913939.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5ea75d5c373d03b5f6ae71d1b0f69c92.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-84fe3613eca291bd0f05b964bde10de7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2ad2dc62bea6aeea7c2f83274a27dfa0.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="OpenNER-1-0-Standardized-Open-Access-Named-Entity-Recognition-Datasets-in-50-Languages"><a href="#OpenNER-1-0-Standardized-Open-Access-Named-Entity-Recognition-Datasets-in-50-Languages" class="headerlink" title="OpenNER 1.0: Standardized Open-Access Named Entity Recognition Datasets   in 50+ Languages"></a>OpenNER 1.0: Standardized Open-Access Named Entity Recognition Datasets   in 50+ Languages</h2><p><strong>Authors:Chester Palen-Michel, Maxwell Pickering, Maya Kruse, Jonne SÃ¤levÃ¤, Constantine Lignos</strong></p>
<p>We present OpenNER 1.0, a standardized collection of openly-available named entity recognition (NER) datasets. OpenNER contains 36 NER corpora that span 52 languages, human-annotated in varying named entity ontologies. We correct annotation format issues, standardize the original datasets into a uniform representation with consistent entity type names across corpora, and provide the collection in a structure that enables research in multilingual and multi-ontology NER. We provide baseline results using three pretrained multilingual language models and two large language models to compare the performance of recent models and facilitate future research in NER. We find that no single model is best in all languages and that significant work remains to obtain high performance from LLMs on the NER task. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†OpenNER 1.0ï¼Œè¿™æ˜¯ä¸€ç»„å…¬å¼€å¯ç”¨çš„å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰æ•°æ®é›†çš„æ ‡å‡†åŒ–é›†åˆã€‚OpenNERåŒ…å«36ä¸ªè·¨52ç§è¯­è¨€çš„NERè¯­æ–™åº“ï¼Œè¿™äº›è¯­æ–™åº“ä»¥ä¸åŒçš„å‘½åå®ä½“æœ¬ä½“è¿›è¡Œäººå·¥æ³¨é‡Šã€‚æˆ‘ä»¬è§£å†³äº†æ³¨é‡Šæ ¼å¼é—®é¢˜ï¼Œå°†åŸå§‹æ•°æ®é›†æ ‡å‡†åŒ–ä¸ºè¯­æ–™åº“é—´å…·æœ‰ä¸€è‡´çš„å®ä½“ç±»å‹åç§°çš„ç»Ÿä¸€è¡¨ç¤ºå½¢å¼ï¼Œå¹¶ä»¥ä¿ƒè¿›å¤šè¯­è¨€å’Œè·¨æœ¬ä½“NERç ”ç©¶çš„ç»“æ„æä¾›é›†åˆã€‚æˆ‘ä»¬ä½¿ç”¨ä¸‰ä¸ªé¢„è®­ç»ƒçš„å¤šè¯­è¨€è¯­è¨€æ¨¡å‹å’Œä¸¤ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹æä¾›åŸºå‡†ç»“æœï¼Œä»¥æ¯”è¾ƒæœ€æ–°æ¨¡å‹çš„æ€§èƒ½å¹¶ä¿ƒè¿›NERçš„æœªæ¥ç ”ç©¶ã€‚æˆ‘ä»¬å‘ç°æ²¡æœ‰ä¸€ç§æ¨¡å‹åœ¨æ‰€æœ‰è¯­è¨€ä¸­éƒ½æ˜¯æœ€ä½³çš„ï¼Œå¹¶ä¸”ä»å¤§å‹è¯­è¨€æ¨¡å‹åœ¨NERä»»åŠ¡ä¸Šè·å¾—é«˜æ€§èƒ½çš„å·¥ä½œä»ç„¶ä»»é‡é“è¿œã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.09587v2">PDF</a> Under review</p>
<p><strong>Summary</strong></p>
<p>OpenNER 1.0æ˜¯ä¸€ä¸ªå…¬å¼€å¯ç”¨çš„å‘½åå®ä½“è¯†åˆ«æ•°æ®é›†æ ‡å‡†åŒ–é›†åˆï¼ŒåŒ…å«è¦†ç›–52ç§è¯­è¨€çš„36ä¸ªNERè¯­æ–™åº“ï¼Œé‡‡ç”¨ä¸åŒå‘½åå®ä½“æœ¬ä½“è¿›è¡Œäººå·¥æ³¨é‡Šã€‚è¯¥é›†åˆè§£å†³äº†æ³¨é‡Šæ ¼å¼é—®é¢˜ï¼Œå°†æ‰€æœ‰æ•°æ®é›†æ ‡å‡†åŒ–ä¸ºå…·æœ‰è·¨è¯­æ–™åº“ä¸€è‡´å®ä½“ç±»å‹åç§°çš„ç»Ÿä¸€è¡¨ç¤ºï¼Œå¹¶ä¸ºç ”ç©¶å’Œå¤šè¯­è¨€å’Œå¤šæœ¬ä½“å‘½åå®ä½“è¯†åˆ«æä¾›äº†ä¾¿åˆ©ã€‚ä½¿ç”¨ä¸‰ä¸ªé¢„è®­ç»ƒçš„å¤šè¯­è¨€æ¨¡å‹å’Œä¸¤ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹æä¾›çš„åŸºå‡†ç»“æœï¼Œå‘ç°æ²¡æœ‰ä¸€ç§æ¨¡å‹åœ¨æ‰€æœ‰è¯­è¨€ä¸­è¡¨ç°æœ€ä½³ï¼Œå¹¶ä¸”åœ¨NERä»»åŠ¡ä¸Šä»å¤§å‹è¯­è¨€æ¨¡å‹ä¸­è·å–é«˜æ€§èƒ½çš„å·¥ä½œä»å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OpenNER 1.0æ˜¯ä¸€ä¸ªåŒ…å«å¤šç§è¯­è¨€çš„å‘½åå®ä½“è¯†åˆ«æ•°æ®é›†çš„æ ‡å‡†åŒ–é›†åˆã€‚</li>
<li>å®ƒåŒ…å«äº†36ä¸ªNERè¯­æ–™åº“ï¼Œè·¨è¶Š52ç§è¯­è¨€ï¼Œå¹¶é‡‡ç”¨äººå·¥æ³¨é‡Šæ–¹å¼ã€‚</li>
<li>OpenNER 1.0è§£å†³äº†æ³¨é‡Šæ ¼å¼ä¸ä¸€è‡´çš„é—®é¢˜ï¼Œå¹¶å°†æ‰€æœ‰æ•°æ®é›†æ ‡å‡†åŒ–ä¸ºå…·æœ‰ä¸€è‡´å®ä½“ç±»å‹åç§°çš„ç»Ÿä¸€è¡¨ç¤ºã€‚</li>
<li>è¯¥é›†åˆä¸ºç ”ç©¶å’Œå¤šè¯­è¨€ã€å¤šæœ¬ä½“çš„å‘½åå®ä½“è¯†åˆ«æä¾›äº†ä¾¿åˆ©ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨ä¸‰ä¸ªé¢„è®­ç»ƒçš„å¤šè¯­è¨€æ¨¡å‹å’Œä¸¤ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹çš„åŸºå‡†ç»“æœè¿›è¡Œæ¯”è¾ƒï¼Œå‘ç°æ²¡æœ‰ä¸€ç§æ¨¡å‹åœ¨æ‰€æœ‰è¯­è¨€ä¸­è¡¨ç°æœ€ä½³ã€‚</li>
<li>åœ¨å‘½åå®ä½“è¯†åˆ«ä»»åŠ¡ä¸Šï¼Œä»å¤§å‹è¯­è¨€æ¨¡å‹ä¸­è·å¾—é«˜æ€§èƒ½çš„å·¥ä½œä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.09587">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0046539c6d5399c724442f63005cf59e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-adb2d56c7efb661cdd8fde7cfaed756b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b192c93b1c8c4ae4b3573bf4199291fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-afc8ad1ad18c9b3c8831e14c21530c3b.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="A-Survey-on-Data-Selection-for-LLM-Instruction-Tuning"><a href="#A-Survey-on-Data-Selection-for-LLM-Instruction-Tuning" class="headerlink" title="A Survey on Data Selection for LLM Instruction Tuning"></a>A Survey on Data Selection for LLM Instruction Tuning</h2><p><strong>Authors:Bolin Zhang, Jiahao Wang, Qianlong Du, Jiajun Zhang, Zhiying Tu, Dianhui Chu</strong></p>
<p>Instruction tuning is a vital step of training large language models (LLM), so how to enhance the effect of instruction tuning has received increased attention. Existing works indicate that the quality of the dataset is more crucial than the quantity during instruction tuning of LLM. Therefore, recently a lot of studies focus on exploring the methods of selecting high-quality subset from instruction datasets, aiming to reduce training costs and enhance the instruction-following capabilities of LLMs. This paper presents a comprehensive survey on data selection for LLM instruction tuning. Firstly, we introduce the wildly used instruction datasets. Then, we propose a new taxonomy of the data selection methods and provide a detailed introduction of recent advances,and the evaluation strategies and results of data selection methods are also elaborated in detail. Finally, we emphasize the open challenges and present new frontiers of this task. </p>
<blockquote>
<p>æŒ‡ä»¤å¾®è°ƒæ˜¯è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é‡è¦æ­¥éª¤ï¼Œå› æ­¤å¦‚ä½•æé«˜æŒ‡ä»¤å¾®è°ƒçš„æ•ˆæœå¼•èµ·äº†è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ã€‚ç°æœ‰å·¥ä½œè¡¨æ˜ï¼Œåœ¨LLMçš„æŒ‡ä»¤å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œæ•°æ®é›†çš„è´¨é‡æ¯”æ•°é‡æ›´é‡è¦ã€‚å› æ­¤ï¼Œæœ€è¿‘çš„ç ”ç©¶é‡ç‚¹ä¸»è¦é›†ä¸­åœ¨æ¢ç´¢ä»æŒ‡ä»¤æ•°æ®é›†ä¸­é€‰æ‹©é«˜è´¨é‡å­é›†çš„æ–¹æ³•ï¼Œæ—¨åœ¨é™ä½è®­ç»ƒæˆæœ¬ï¼Œæé«˜LLMçš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›ã€‚æœ¬æ–‡å¯¹LLMæŒ‡ä»¤è°ƒæ•´ä¸­çš„æ•°æ®é€‰æ‹©è¿›è¡Œäº†å…¨é¢ç»¼è¿°ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä»‹ç»äº†å¹¿æ³›ä½¿ç”¨çš„æŒ‡ä»¤æ•°æ®é›†ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†æ•°æ®é€‰æ‹©æ–¹æ³•çš„æ–°åˆ†ç±»ï¼Œå¹¶å¯¹æœ€æ–°è¿›å±•è¿›è¡Œäº†è¯¦ç»†ä»‹ç»ï¼ŒåŒæ—¶è¯¦ç»†é˜è¿°äº†æ•°æ®é€‰æ‹©æ–¹æ³•çš„è¯„ä¼°ç­–ç•¥å’Œç»“æœã€‚æœ€åï¼Œæˆ‘ä»¬å¼ºè°ƒäº†å¼€æ”¾æŒ‘æˆ˜å¹¶å±•ç¤ºäº†è¿™é¡¹ä»»åŠ¡çš„æ–°å‰æ²¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.05123v2">PDF</a> Accepted by JAIR</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å…¨é¢æ¦‚è¿°äº†ç”¨äºLLMæŒ‡ä»¤è°ƒæ•´çš„æ•°æ®é€‰æ‹©æ–¹æ³•ã€‚æ–‡ç« ä»‹ç»äº†å¹¿æ³›ä½¿ç”¨çš„æŒ‡ä»¤æ•°æ®é›†ï¼Œæå‡ºäº†æ•°æ®é€‰æ‹©æ–¹æ³•çš„æ–°åˆ†ç±»ï¼Œå¹¶è¯¦ç»†é˜è¿°äº†æœ€è¿‘çš„è¿›å±•ã€è¯„ä¼°ç­–ç•¥å’Œç»“æœã€‚æ–‡ç« è¿˜å¼ºè°ƒäº†å½“å‰é¢ä¸´çš„æŒ‘æˆ˜å’Œè¯¥ä»»åŠ¡çš„æ–°å‰æ²¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æŒ‡ä»¤è°ƒæ•´æ˜¯è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹çš„é‡è¦æ­¥éª¤ã€‚</li>
<li>æ•°æ®è´¨é‡æ¯”æ•°é‡åœ¨LLMçš„æŒ‡ä»¤è°ƒæ•´ä¸­æ›´ä¸ºé‡è¦ã€‚</li>
<li>å½“å‰ç ”ç©¶å…³æ³¨äºä»æŒ‡ä»¤æ•°æ®é›†ä¸­é€‰æ‹©é«˜è´¨é‡å­é›†çš„æ–¹æ³•ï¼Œæ—¨åœ¨é™ä½è®­ç»ƒæˆæœ¬å¹¶æé«˜LLMçš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›ã€‚</li>
<li>æ–‡ç« ä»‹ç»äº†å¹¿æ³›ä½¿ç”¨çš„æŒ‡ä»¤æ•°æ®é›†ã€‚</li>
<li>æ–‡ç« æå‡ºäº†æ•°æ®é€‰æ‹©æ–¹æ³•çš„æ–°åˆ†ç±»ï¼Œå¹¶è¯¦ç»†é˜è¿°äº†æœ€è¿‘çš„è¿›å±•ã€‚</li>
<li>æ–‡ç« çš„è¯„ä¼°ç­–ç•¥å’Œç»“æœéƒ¨åˆ†è¯¦ç»†é˜è¿°äº†æ•°æ®é€‰æ‹©æ–¹æ³•çš„è¯„ä¼°ç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2402.05123">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a047d1bc7c25033c0c8155bee0e75e01.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1d4acc036b7ff1250fbb5989a207a9d8.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-28/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-28/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-28/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-606272d05a87c0533b51ed6302057f9f.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-28  Mind2Web 2 Evaluating Agentic Search with Agent-as-a-Judge
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-28
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-28/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-84a1984fcce907fbf742a3fcde2ffe2f.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-28  HalluSegBench Counterfactual Visual Reasoning for Segmentation   Hallucination Evaluation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-28
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">28172.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
