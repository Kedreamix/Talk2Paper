<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-28  Exploring the Design Space of 3D MLLMs for CT Report Generation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-9c77b70bfe8f8f2c5aea0f0193a8961f.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-28
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    80 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-28-æ›´æ–°"><a href="#2025-06-28-æ›´æ–°" class="headerlink" title="2025-06-28 æ›´æ–°"></a>2025-06-28 æ›´æ–°</h1><h2 id="Exploring-the-Design-Space-of-3D-MLLMs-for-CT-Report-Generation"><a href="#Exploring-the-Design-Space-of-3D-MLLMs-for-CT-Report-Generation" class="headerlink" title="Exploring the Design Space of 3D MLLMs for CT Report Generation"></a>Exploring the Design Space of 3D MLLMs for CT Report Generation</h2><p><strong>Authors:Mohammed Baharoon, Jun Ma, Congyu Fang, Augustin Toma, Bo Wang</strong></p>
<p>Multimodal Large Language Models (MLLMs) have emerged as a promising way to automate Radiology Report Generation (RRG). In this work, we systematically investigate the design space of 3D MLLMs, including visual input representation, projectors, Large Language Models (LLMs), and fine-tuning techniques for 3D CT report generation. We also introduce two knowledge-based report augmentation methods that improve performance on the GREEN score by up to 10%, achieving the 2nd place on the MICCAI 2024 AMOS-MM challenge. Our results on the 1,687 cases from the AMOS-MM dataset show that RRG is largely independent of the size of LLM under the same training protocol. We also show that larger volume size does not always improve performance if the original ViT was pre-trained on a smaller volume size. Lastly, we show that using a segmentation mask along with the CT volume improves performance. The code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/bowang-lab/AMOS-MM-Solution">https://github.com/bowang-lab/AMOS-MM-Solution</a> </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä½œä¸ºä¸€ç§æœ‰å‰æ™¯çš„æ–¹æ³•ï¼Œå·²ç»å‡ºç°åœ¨è‡ªåŠ¨æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆï¼ˆRRGï¼‰ä¸­ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°ç ”ç©¶äº†3DMLLMsçš„è®¾è®¡ç©ºé—´ï¼ŒåŒ…æ‹¬è§†è§‰è¾“å…¥è¡¨ç¤ºã€æŠ•å½±ä»ªã€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œç”¨äº3Dè®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰æŠ¥å‘Šç”Ÿæˆçš„å¾®è°ƒæŠ€æœ¯ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†ä¸¤ç§åŸºäºçŸ¥è¯†çš„æŠ¥å‘Šå¢å¼ºæ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•åœ¨GREENè¯„åˆ†ä¸Šçš„æ€§èƒ½æé«˜äº†é«˜è¾¾10%ï¼Œå¹¶åœ¨MICCAI 2024 AMOS-MMæŒ‘æˆ˜ä¸­è·å¾—äº†ç¬¬äºŒåã€‚æˆ‘ä»¬åœ¨AMOS-MMæ•°æ®é›†ä¸Šçš„1687ä¸ªæ¡ˆä¾‹ç»“æœè¡¨æ˜ï¼Œåœ¨åŒä¸€è®­ç»ƒåè®®ä¸‹ï¼ŒRRGåœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šç‹¬ç«‹äºLLMçš„å¤§å°ã€‚æˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œå¦‚æœåŸå§‹ViTæ˜¯åœ¨è¾ƒå°çš„ä½“ç§¯ä¸Šé¢„è®­ç»ƒçš„ï¼Œé‚£ä¹ˆè¾ƒå¤§çš„ä½“ç§¯å¤§å°å¹¶ä¸ä¸€å®šæ€»èƒ½æé«˜æ€§èƒ½ã€‚æœ€åï¼Œæˆ‘ä»¬è¡¨æ˜ï¼Œä½¿ç”¨åˆ†æ®µæ©è†œä¸CTä½“ç§¯ä¸€èµ·å¯ä»¥æé«˜æ€§èƒ½ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/bowang-lab/AMOS-MM-Solution%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/bowang-lab/AMOS-MM-Solutionå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.21535v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†åˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹è‡ªåŠ¨åŒ–ç”Ÿæˆæ”¾å°„å­¦æŠ¥å‘Šçš„æ–¹æ³•ã€‚æ–‡ç« ç³»ç»Ÿåœ°æ¢è®¨äº†ä¸‰ç»´å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„è®¾è®¡ç©ºé—´ï¼ŒåŒ…æ‹¬è§†è§‰è¾“å…¥è¡¨ç¤ºã€æŠ•å½±å™¨ã€å¤§å‹è¯­è¨€æ¨¡å‹å’Œé’ˆå¯¹ä¸‰ç»´CTæŠ¥å‘Šç”Ÿæˆçš„å¾®è°ƒæŠ€æœ¯ã€‚æ­¤å¤–ï¼Œè¿˜ä»‹ç»äº†ä¸¤ç§åŸºäºçŸ¥è¯†çš„æŠ¥å‘Šå¢å¼ºæ–¹æ³•ï¼Œæé«˜äº†GREENæŒ‡æ ‡çš„è¯„åˆ†ï¼Œå¹¶åœ¨MICCAI 2024 AMOS-MMæŒ‘æˆ˜èµ›ä¸­è£è·ç¬¬äºŒåã€‚åœ¨AMOS-MMæ•°æ®é›†ä¸Šï¼Œç»“æœæ˜¾ç¤ºæŠ¥å‘Šç”Ÿæˆåœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šç‹¬ç«‹äºåŒä¸€è®­ç»ƒåè®®ä¸‹çš„è¯­è¨€æ¨¡å‹å¤§å°ï¼Œå¹¶ä¸”æ›´å¤§çš„ä½“ç§¯å¤§å°å¹¶ä¸æ€»æ˜¯èƒ½æé«˜æ€§èƒ½ï¼Œå¦‚æœåŸå§‹ViTæ˜¯åœ¨è¾ƒå°çš„ä½“ç§¯å¤§å°ä¸Šè¿›è¡Œé¢„è®­ç»ƒçš„ã€‚æœ€åï¼Œä½¿ç”¨åˆ†æ®µæ©ç ä¸CTä½“ç§¯ç›¸ç»“åˆå¯ä»¥æé«˜æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆä¸­å±•ç°å‡ºæ½œåŠ›ã€‚</li>
<li>ç ”ç©¶äº†ä¸‰ç»´å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„è®¾è®¡ç©ºé—´ï¼ŒåŒ…æ‹¬è§†è§‰è¾“å…¥ã€æŠ•å½±å™¨ã€å¤§å‹è¯­è¨€æ¨¡å‹å’Œå¾®è°ƒæŠ€æœ¯ã€‚</li>
<li>å¼•å…¥ä¸¤ç§çŸ¥è¯†å¢å¼ºæŠ¥å‘Šæ–¹æ³•ï¼Œæé«˜æ€§èƒ½å¹¶åœ¨ç«èµ›ä¸­å–å¾—ç¬¬äºŒåã€‚</li>
<li>æŠ¥å‘Šç”Ÿæˆå¯¹è¯­è¨€æ¨¡å‹å¤§å°ç›¸å¯¹ç‹¬ç«‹ï¼Œåœ¨åŒä¸€è®­ç»ƒåè®®ä¸‹ã€‚</li>
<li>æ›´å¤§çš„ä½“ç§¯å¤§å°ä¸ä¸€å®šèƒ½æé«˜æ€§èƒ½ï¼Œè‹¥é¢„è®­ç»ƒæ—¶çš„ä½“ç§¯è¾ƒå°ã€‚</li>
<li>ç»“åˆåˆ†æ®µæ©ç ä¸CTä½“ç§¯ä½¿ç”¨èƒ½æé«˜æŠ¥å‘Šç”Ÿæˆçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.21535">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bb69e031010be7b4f44bfd72e9268570.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-671d20832c0897e7ab2b10a78f1df217.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-34595bcb9b3df1b66b3ea82b36d3ac7b.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="HyperSORT-Self-Organising-Robust-Training-with-hyper-networks"><a href="#HyperSORT-Self-Organising-Robust-Training-with-hyper-networks" class="headerlink" title="HyperSORT: Self-Organising Robust Training with hyper-networks"></a>HyperSORT: Self-Organising Robust Training with hyper-networks</h2><p><strong>Authors:Samuel Joutard, Marijn Stollenga, Marc Balle Sanchez, Mohammad Farid Azampour, Raphael Prevost</strong></p>
<p>Medical imaging datasets often contain heterogeneous biases ranging from erroneous labels to inconsistent labeling styles. Such biases can negatively impact deep segmentation networks performance. Yet, the identification and characterization of such biases is a particularly tedious and challenging task. In this paper, we introduce HyperSORT, a framework using a hyper-network predicting UNetsâ€™ parameters from latent vectors representing both the image and annotation variability. The hyper-network parameters and the latent vector collection corresponding to each data sample from the training set are jointly learned. Hence, instead of optimizing a single neural network to fit a dataset, HyperSORT learns a complex distribution of UNet parameters where low density areas can capture noise-specific patterns while larger modes robustly segment organs in differentiated but meaningful manners. We validate our method on two 3D abdominal CT public datasets: first a synthetically perturbed version of the AMOS dataset, and TotalSegmentator, a large scale dataset containing real unknown biases and errors. Our experiments show that HyperSORT creates a structured mapping of the dataset allowing the identification of relevant systematic biases and erroneous samples. Latent space clusters yield UNet parameters performing the segmentation task in accordance with the underlying learned systematic bias. The code and our analysis of the TotalSegmentator dataset are made available: <a target="_blank" rel="noopener" href="https://github.com/ImFusionGmbH/HyperSORT">https://github.com/ImFusionGmbH/HyperSORT</a> </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒæ•°æ®é›†å¸¸å¸¸åŒ…å«ä»é”™è¯¯æ ‡ç­¾åˆ°æ ‡æ³¨é£æ ¼ä¸ä¸€è‡´ç­‰å„ç§å¼‚è´¨åè§ã€‚è¿™äº›åè§å¯èƒ½ä¼šå¯¹æ·±åº¦åˆ†å‰²ç½‘ç»œæ€§èƒ½äº§ç”Ÿè´Ÿé¢å½±å“ã€‚ç„¶è€Œï¼Œè¯†åˆ«å’Œæè¿°è¿™äº›åè§æ˜¯ä¸€é¡¹ç‰¹åˆ«ç¹çä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†HyperSORTï¼Œè¿™æ˜¯ä¸€ä¸ªä½¿ç”¨è¶…ç½‘ç»œé¢„æµ‹UNetå‚æ•°çš„æ¡†æ¶ï¼Œè¯¥å‚æ•°ä»ä»£è¡¨å›¾åƒå’Œæ³¨é‡Šå¯å˜æ€§çš„æ½œåœ¨å‘é‡ä¸­æå–ã€‚è¶…ç½‘ç»œå‚æ•°å’Œä¸è®­ç»ƒé›†ä¸­æ¯ä¸ªæ•°æ®æ ·æœ¬å¯¹åº”çš„æ½œåœ¨å‘é‡é›†åˆæ˜¯è”åˆå­¦ä¹ çš„ã€‚å› æ­¤ï¼ŒHyperSORTä¸æ˜¯ä¼˜åŒ–å•ä¸ªç¥ç»ç½‘ç»œä»¥é€‚åº”æ•°æ®é›†ï¼Œè€Œæ˜¯å­¦ä¹ UNetå‚æ•°çš„å¤æ‚åˆ†å¸ƒï¼Œå…¶ä¸­ä½å¯†åº¦åŒºåŸŸå¯ä»¥æ•è·å™ªå£°ç‰¹å®šæ¨¡å¼ï¼Œè€Œè¾ƒå¤§çš„æ¨¡å¼åˆ™ä»¥æœ‰åŒºåˆ«ä½†æœ‰æ„ä¹‰çš„æ–¹å¼ç¨³å¥åœ°åˆ†å‰²å™¨å®˜ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªä¸‰ç»´è…¹éƒ¨CTå…¬å…±æ•°æ®é›†ä¸ŠéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼šé¦–å…ˆæ˜¯AMOSæ•°æ®é›†çš„åˆæˆæ‰°åŠ¨ç‰ˆæœ¬ï¼Œä»¥åŠåŒ…å«çœŸå®æœªçŸ¥åè§å’Œé”™è¯¯çš„TotalSegmentatorå¤§è§„æ¨¡æ•°æ®é›†ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒHyperSORTåˆ›å»ºäº†ä¸€ä¸ªç»“æ„åŒ–æ•°æ®é›†æ˜ å°„ï¼Œä»è€Œèƒ½å¤Ÿè¯†åˆ«ç›¸å…³çš„ç³»ç»Ÿæ€§åè§å’Œé”™è¯¯æ ·æœ¬ã€‚æ½œåœ¨ç©ºé—´èšç±»äº§ç”ŸUNetå‚æ•°ï¼Œæ ¹æ®å­¦åˆ°çš„ç³»ç»Ÿæ€§åè§æ‰§è¡Œåˆ†å‰²ä»»åŠ¡ã€‚æˆ‘ä»¬çš„ä»£ç å’Œå¯¹TotalSegmentatoræ•°æ®é›†çš„åˆ†æå·²æä¾›ï¼š<a target="_blank" rel="noopener" href="https://github.com/ImFusionGmbH/HyperSORT">https://github.com/ImFusionGmbH/HyperSORT</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.21430v1">PDF</a> Accepted at MICCAI 2025</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºHyperSORTçš„æ¡†æ¶ï¼Œå®ƒé€šè¿‡è¶…ç½‘ç»œé¢„æµ‹UNetå‚æ•°ï¼Œè¿™äº›å‚æ•°ä»ä»£è¡¨å›¾åƒå’Œæ³¨é‡Šå¯å˜æ€§çš„æ½œåœ¨å‘é‡ä¸­æ´¾ç”Ÿå‡ºæ¥ã€‚è¯¥æ¡†æ¶èƒ½è”åˆå­¦ä¹ è¶…ç½‘ç»œå‚æ•°å’Œå¯¹åº”è®­ç»ƒé›†ä¸­æ¯ä¸ªæ•°æ®æ ·æœ¬çš„æ½œåœ¨å‘é‡é›†åˆã€‚HyperSORTèƒ½åœ¨æ•°æ®é›†ä¸Šå­¦ä¹ ä¸€ä¸ªå¤æ‚çš„UNetå‚æ•°åˆ†å¸ƒï¼Œå…¶ä¸­ä½å¯†åº¦åŒºåŸŸå¯ä»¥æ•æ‰å™ªå£°ç‰¹å®šæ¨¡å¼ï¼Œè€Œè¾ƒå¤§çš„æ¨¡å¼èƒ½ä»¥æœ‰æ„ä¹‰çš„æ–¹å¼ç¨³å¥åœ°åˆ†å‰²å™¨å®˜ã€‚å®éªŒè¡¨æ˜ï¼ŒHyperSORTèƒ½å¤Ÿåˆ›å»ºæ•°æ®é›†çš„ç»“æ„æ˜ å°„ï¼Œä»¥è¯†åˆ«ç›¸å…³çš„ç³»ç»Ÿæ€§åè§å’Œé”™è¯¯æ ·æœ¬ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>HyperSORTæ¡†æ¶åˆ©ç”¨è¶…ç½‘ç»œé¢„æµ‹UNetå‚æ•°ï¼Œè¿™äº›å‚æ•°ä»å›¾åƒå’Œæ³¨é‡Šå¯å˜æ€§çš„æ½œåœ¨å‘é‡ä¸­ç”Ÿæˆã€‚</li>
<li>è¯¥æ¡†æ¶è”åˆå­¦ä¹ è¶…ç½‘ç»œå‚æ•°å’Œæ¯ä¸ªè®­ç»ƒæ•°æ®æ ·æœ¬çš„æ½œåœ¨å‘é‡é›†åˆã€‚</li>
<li>HyperSORTå­¦ä¹ ä¸€ä¸ªå¤æ‚çš„UNetå‚æ•°åˆ†å¸ƒï¼Œä»¥é€‚åº”æ•°æ®é›†ï¼Œå…¶ä¸­ä½å¯†åº¦åŒºåŸŸå’Œè¾ƒå¤§çš„æ¨¡å¼åˆ†åˆ«ç”¨äºæ•æ‰å™ªå£°ç‰¹å®šæ¨¡å¼å’Œç¨³å¥åœ°åˆ†å‰²å™¨å®˜ã€‚</li>
<li>HyperSORTèƒ½åˆ›å»ºæ•°æ®é›†çš„ç»“æ„æ˜ å°„ï¼Œä»¥è¯†åˆ«ç›¸å…³çš„ç³»ç»Ÿæ€§åè§å’Œé”™è¯¯æ ·æœ¬ã€‚</li>
<li>é€šè¿‡æ½œåœ¨ç©ºé—´èšç±»ï¼ŒHyperSORTå¯ä»¥æ ¹æ®å­¦åˆ°çš„ç³»ç»Ÿæ€§åè§è¿›è¡Œåˆ†å‰²ä»»åŠ¡ã€‚</li>
<li>HyperSORTåœ¨åˆæˆå¹²æ‰°çš„AMOSæ•°æ®é›†å’ŒåŒ…å«çœŸå®æœªçŸ¥åè§å’Œé”™è¯¯çš„TotalSegmentatoræ•°æ®é›†ä¸Šè¿›è¡Œäº†éªŒè¯ã€‚</li>
<li>HyperSORTçš„ä»£ç å’Œå¯¹TotalSegmentatoræ•°æ®é›†çš„åˆ†æå·²å…¬å¼€å¯ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.21430">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c2bb515dad8a8d0afba965b9f300b425.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3a9fd5cb73a923efd4adf5c9cef4a809.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-949403da2561643977be5da796e9f4a6.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Robust-Deep-Learning-for-Myocardial-Scar-Segmentation-in-Cardiac-MRI-with-Noisy-Labels"><a href="#Robust-Deep-Learning-for-Myocardial-Scar-Segmentation-in-Cardiac-MRI-with-Noisy-Labels" class="headerlink" title="Robust Deep Learning for Myocardial Scar Segmentation in Cardiac MRI   with Noisy Labels"></a>Robust Deep Learning for Myocardial Scar Segmentation in Cardiac MRI   with Noisy Labels</h2><p><strong>Authors:Aida Moafi, Danial Moafi, Evgeny M. Mirkes, Gerry P. McCann, Abbas S. Alatrany, Jayanth R. Arnold, Mostafa Mehdipour Ghazi</strong></p>
<p>The accurate segmentation of myocardial scars from cardiac MRI is essential for clinical assessment and treatment planning. In this study, we propose a robust deep-learning pipeline for fully automated myocardial scar detection and segmentation by fine-tuning state-of-the-art models. The method explicitly addresses challenges of label noise from semi-automatic annotations, data heterogeneity, and class imbalance through the use of Kullback-Leibler loss and extensive data augmentation. We evaluate the modelâ€™s performance on both acute and chronic cases and demonstrate its ability to produce accurate and smooth segmentations despite noisy labels. In particular, our approach outperforms state-of-the-art models like nnU-Net and shows strong generalizability in an out-of-distribution test set, highlighting its robustness across various imaging conditions and clinical tasks. These results establish a reliable foundation for automated myocardial scar quantification and support the broader clinical adoption of deep learning in cardiac imaging. </p>
<blockquote>
<p>å¿ƒè„ç£å…±æŒ¯æˆåƒä¸­å¿ƒè‚Œç˜¢ç—•çš„ç²¾ç¡®åˆ†å‰²å¯¹ä¸´åºŠè¯„ä¼°å’Œæ²»ç–—è®¡åˆ’è‡³å…³é‡è¦ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç¨³å¥çš„æ·±åº¦å­¦ä¹ æµæ°´çº¿ï¼Œé€šè¿‡å¾®è°ƒæœ€å…ˆè¿›æ¨¡å‹å®ç°å…¨è‡ªåŠ¨å¿ƒè‚Œç˜¢ç—•æ£€æµ‹å’Œåˆ†å‰²ã€‚è¯¥æ–¹æ³•é€šè¿‡é‡‡ç”¨Kullback-LeibleræŸå¤±å’Œå¹¿æ³›çš„æ•°æ®å¢å¼ºï¼Œæ˜ç¡®è§£å†³äº†æ¥è‡ªåŠè‡ªåŠ¨æ³¨é‡Šçš„æ ‡ç­¾å™ªå£°ã€æ•°æ®å¼‚è´¨æ€§å’Œç±»åˆ«ä¸å¹³è¡¡ç­‰æŒ‘æˆ˜ã€‚æˆ‘ä»¬å¯¹æ¨¡å‹åœ¨æ€¥æ€§å’Œæ…¢æ€§ç—…ä¾‹ä¸Šçš„è¡¨ç°è¿›è¡Œäº†è¯„ä¼°ï¼Œè¯æ˜äº†å…¶åœ¨æ ‡ç­¾å™ªå£°çš„æƒ…å†µä¸‹ä»èƒ½äº§ç”Ÿå‡†ç¡®ã€å¹³æ»‘åˆ†å‰²çš„èƒ½åŠ›ã€‚å°¤å…¶å€¼å¾—ä¸€æçš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºæœ€å…ˆè¿›æ¨¡å‹ï¼ˆå¦‚nnU-Netï¼‰ï¼Œå¹¶ä¸”åœ¨ç¦»åˆ†å¸ƒæµ‹è¯•é›†ä¸Šè¡¨ç°å‡ºå¾ˆå¼ºçš„æ³›åŒ–èƒ½åŠ›ï¼Œå‡¸æ˜¾äº†å…¶åœ¨å„ç§æˆåƒæ¡ä»¶å’Œä¸´åºŠä»»åŠ¡ä¸­çš„ç¨³å¥æ€§ã€‚è¿™äº›ç»“æœä¸ºè‡ªåŠ¨å¿ƒè‚Œç˜¢ç—•é‡åŒ–å¥ å®šäº†åŸºç¡€ï¼Œå¹¶æ”¯æŒæ·±åº¦å­¦ä¹ åœ¨å¿ƒè„æˆåƒä¸­çš„æ›´å¹¿æ³›ä¸´åºŠåº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.21151v1">PDF</a> MICCAI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶æå‡ºä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„å…¨è‡ªåŠ¨å¿ƒè‚Œç˜¢ç—•æ£€æµ‹å’Œåˆ†å‰²æ–¹æ³•ï¼Œé€šè¿‡å¾®è°ƒå…ˆè¿›æŠ€æœ¯æ¨¡å‹æ¥è§£å†³æ ‡ç­¾å™ªå£°ã€æ•°æ®å¼‚è´¨æ€§å’Œç±»åˆ«ä¸å¹³è¡¡ç­‰æŒ‘æˆ˜ã€‚æ¨¡å‹æ€§èƒ½åœ¨æ€¥æ€§å’Œæ…¢æ€§ç—…ä¾‹ä¸­å‡å¾—åˆ°è¯„ä¼°ï¼Œè¡¨ç°å‡ºå‡†ç¡®å¹³æ»‘çš„åˆ†å‰²èƒ½åŠ›ï¼Œå°½ç®¡æ ‡ç­¾å­˜åœ¨å™ªå£°ã€‚è¯¥ç ”ç©¶çš„æ–¹æ³•åœ¨nnU-Netç­‰æœ€æ–°æ¨¡å‹çš„åŸºç¡€ä¸Šè¡¨ç°æ›´ä¼˜ç§€ï¼Œå¹¶ä¸”åœ¨ç¦»åˆ†å¸ƒæµ‹è¯•é›†ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œæ”¯æŒå…¶åœ¨ä¸åŒæˆåƒæ¡ä»¶å’Œä¸´åºŠä»»åŠ¡ä¸­çš„å¹¿æ³›åº”ç”¨ã€‚è¿™äº›ç»“æœä¸ºè‡ªåŠ¨å¿ƒè‚Œç˜¢ç—•é‡åŒ–å¥ å®šäº†åŸºç¡€ï¼Œæ”¯æŒæ·±åº¦å­¦ä¹ åœ¨å¿ƒè„æˆåƒä¸­çš„ä¸´åºŠå¹¿æ³›åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶æå‡ºä¸€ç§å…¨è‡ªåŠ¨å¿ƒè‚Œç˜¢ç—•æ£€æµ‹å’Œåˆ†å‰²çš„æ·±åº¦å­¦ä¹ ç®¡é“æ–¹æ³•ã€‚</li>
<li>æ–¹æ³•é€šè¿‡å¾®è°ƒå…ˆè¿›æŠ€æœ¯æ¨¡å‹è§£å†³æ ‡ç­¾å™ªå£°ã€æ•°æ®å¼‚è´¨æ€§å’Œç±»åˆ«ä¸å¹³è¡¡ç­‰æŒ‘æˆ˜ã€‚</li>
<li>æ¨¡å‹è¯„ä¼°è¡¨æ˜å…¶åœ¨æ€¥æ€§å’Œæ…¢æ€§ç—…ä¾‹ä¸­å…·æœ‰å‡†ç¡®å’Œå…‰æ»‘çš„åˆ†å‰²èƒ½åŠ›ã€‚</li>
<li>è¯¥æ–¹æ³•ä¼˜äºç°æœ‰æœ€æ–°æ¨¡å‹ï¼Œå¦‚nnU-Netã€‚</li>
<li>æ¨¡å‹åœ¨å¤–éƒ¨æµ‹è¯•é›†ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œé€‚ç”¨äºä¸åŒæˆåƒæ¡ä»¶å’Œä¸´åºŠä»»åŠ¡ã€‚</li>
<li>ç ”ç©¶ç»“æœæ”¯æŒè‡ªåŠ¨å¿ƒè‚Œç˜¢ç—•é‡åŒ–çš„åº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.21151">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-897827746f029f1aa46f02344b993b27.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d73b66d5623894af438c9138c87cda55.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb2d512ab6bc8b898b8bbad275edd8e9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4fdd7ac01b7b23e0cdb41edadc364225.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-47fb6401e53202f0a581857234824cf5.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Detection-of-Breast-Cancer-Lumpectomy-Margin-with-SAM-incorporated-Forward-Forward-Contrastive-Learning"><a href="#Detection-of-Breast-Cancer-Lumpectomy-Margin-with-SAM-incorporated-Forward-Forward-Contrastive-Learning" class="headerlink" title="Detection of Breast Cancer Lumpectomy Margin with SAM-incorporated   Forward-Forward Contrastive Learning"></a>Detection of Breast Cancer Lumpectomy Margin with SAM-incorporated   Forward-Forward Contrastive Learning</h2><p><strong>Authors:Tyler Ward, Xiaoqin Wang, Braxton McFarland, Md Atik Ahamed, Sahar Nozad, Talal Arshad, Hafsa Nebbache, Jin Chen, Abdullah Imran</strong></p>
<p>Complete removal of cancer tumors with a negative specimen margin during lumpectomy is essential in reducing breast cancer recurrence. However, 2D specimen radiography (SR), the current method used to assess intraoperative specimen margin status, has limited accuracy, resulting in nearly a quarter of patients requiring additional surgery. To address this, we propose a novel deep learning framework combining the Segment Anything Model (SAM) with Forward-Forward Contrastive Learning (FFCL), a pre-training strategy leveraging both local and global contrastive learning for patch-level classification of SR images. After annotating SR images with regions of known maligancy, non-malignant tissue, and pathology-confirmed margins, we pre-train a ResNet-18 backbone with FFCL to classify margin status, then reconstruct coarse binary masks to prompt SAM for refined tumor margin segmentation. Our approach achieved an AUC of 0.8455 for margin classification and segmented margins with a 27.4% improvement in Dice similarity over baseline models, while reducing inference time to 47 milliseconds per image. These results demonstrate that FFCL-SAM significantly enhances both the speed and accuracy of intraoperative margin assessment, with strong potential to reduce re-excision rates and improve surgical outcomes in breast cancer treatment. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/tbwa233/FFCL-SAM/">https://github.com/tbwa233/FFCL-SAM/</a>. </p>
<blockquote>
<p>åœ¨ä¿ä¹³æ‰‹æœ¯ä¸­ï¼Œå®Œå…¨ç§»é™¤ç™Œç—‡è‚¿ç˜¤å¹¶å¸¦æœ‰é˜´æ€§æ ‡æœ¬è¾¹ç¼˜æ˜¯é™ä½ä¹³è…ºç™Œå¤å‘çš„å…³é”®ã€‚ç„¶è€Œï¼Œç›®å‰ç”¨äºè¯„ä¼°æœ¯ä¸­æ ‡æœ¬è¾¹ç¼˜çŠ¶æ€çš„äºŒç»´æ ‡æœ¬æ”¾å°„çº¿ç…§ç›¸æœ¯ï¼ˆSRï¼‰çš„å‡†ç¡®åº¦æœ‰é™ï¼Œå¯¼è‡´è¿‘å››åˆ†ä¹‹ä¸€çš„æ‚£è€…éœ€è¦æ¥å—é¢å¤–æ‰‹æœ¯ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†Segment Anything Modelï¼ˆSAMï¼‰å’ŒForward-Forward Contrastive Learningï¼ˆFFCLï¼‰ã€‚FFCLæ˜¯ä¸€ç§é¢„è®­ç»ƒç­–ç•¥ï¼Œåˆ©ç”¨å±€éƒ¨å’Œå…¨å±€å¯¹æ¯”å­¦ä¹ å¯¹SRå›¾åƒè¿›è¡Œè¡¥ä¸çº§åˆ«çš„åˆ†ç±»ã€‚åœ¨å¯¹SRå›¾åƒè¿›è¡Œå·²çŸ¥æ¶æ€§åŒºåŸŸã€éæ¶æ€§ç»„ç»‡å’Œç»ç—…ç†è¯å®çš„è¾¹ç¼˜æ ‡æ³¨åï¼Œæˆ‘ä»¬ä½¿ç”¨FFCLé¢„è®­ç»ƒä¸€ä¸ªResNet-18éª¨å¹²ç½‘æ¥åˆ†ç±»è¾¹ç¼˜çŠ¶æ€ï¼Œç„¶åé‡å»ºç²—ç³™çš„äºŒå€¼æ©è†œæ¥æç¤ºSAMè¿›è¡Œç²¾ç»†çš„è‚¿ç˜¤è¾¹ç¼˜åˆ†å‰²ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨è¾¹ç¼˜åˆ†ç±»æ–¹é¢è¾¾åˆ°äº†0.8455çš„AUCï¼Œåœ¨è¾¹ç¼˜åˆ†å‰²æ–¹é¢ç›¸è¾ƒäºåŸºå‡†æ¨¡å‹åœ¨Diceç›¸ä¼¼åº¦ä¸Šæé«˜äº†27.4%ï¼ŒåŒæ—¶å‡å°‘äº†æ¯å¹…å›¾åƒçš„æ¨ç†æ—¶é—´è‡³47æ¯«ç§’ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒFFCL-SAMæ˜¾è‘—æé«˜äº†æœ¯ä¸­è¾¹ç¼˜è¯„ä¼°çš„é€Ÿåº¦å’Œå‡†ç¡®åº¦ï¼Œå…·æœ‰å¼ºå¤§çš„æ½œåŠ›æ¥é™ä½å†åˆ‡é™¤ç‡å¹¶æ”¹å–„ä¹³è…ºç™Œæ²»ç–—çš„æ‰‹æœ¯æ•ˆæœã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/tbwa233/FFCL-SAM/">https://github.com/tbwa233/FFCL-SAM/</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.21006v1">PDF</a> 19 pages, 7 figures, 3 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å¼ºè°ƒäº†åœ¨ä¹³è…ºç™Œä¿ä¹³æ‰‹æœ¯ä¸­å®Œå…¨å»é™¤è‚¿ç˜¤ã€ç¡®ä¿è´Ÿæ€§æ ‡æœ¬è¾¹ç¼˜çš„é‡è¦æ€§ï¼Œä»¥é™ä½ç™Œç—‡å¤å‘çš„é£é™©ã€‚ç„¶è€Œï¼Œç°æœ‰çš„äºŒç»´æ ‡æœ¬æ”¾å°„çº¿æ£€æµ‹ï¼ˆSRï¼‰åœ¨è¯„ä¼°æ ‡æœ¬è¾¹ç¼˜çŠ¶æ€æ—¶çš„å‡†ç¡®åº¦æœ‰é™ï¼Œçº¦å››åˆ†ä¹‹ä¸€çš„æ‚£è€…éœ€è¦å†æ¬¡æ‰‹æœ¯ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºä¸€ç§ç»“åˆSegment Anything Modelï¼ˆSAMï¼‰ä¸Forward-Forward Contrastive Learningï¼ˆFFCLï¼‰çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚é€šè¿‡æ ‡æ³¨SRå›¾åƒä¸­çš„å·²çŸ¥æ¶æ€§åŒºåŸŸã€éæ¶æ€§ç»„ç»‡å’Œç»ç—…ç†è¯å®çš„è¾¹ç¼˜ï¼Œæˆ‘ä»¬åˆ©ç”¨FFCLè¿›è¡Œè¾¹ç¼˜çŠ¶æ€åˆ†ç±»çš„é¢„è®­ç»ƒï¼Œç„¶åé‡å»ºç²—äºŒè¿›åˆ¶æ©è†œä»¥å¼•å¯¼SAMè¿›è¡Œæ›´ç²¾ç»†çš„è‚¿ç˜¤è¾¹ç¼˜åˆ†å‰²ã€‚è¯¥æ–¹æ³•åœ¨è¾¹ç¼˜åˆ†ç±»æ–¹é¢å–å¾—äº†0.8455çš„AUCå€¼ï¼Œç›¸è¾ƒäºåŸºçº¿æ¨¡å‹ï¼ŒDiceç›¸ä¼¼åº¦æé«˜äº†27.4%ï¼Œä¸”æ¯å¼ å›¾åƒçš„æ¨ç†æ—¶é—´ç¼©çŸ­è‡³47æ¯«ç§’ã€‚è¿™è¡¨æ˜FFCL-SAMèƒ½æ˜¾è‘—æé«˜æœ¯ä¸­è¾¹ç¼˜è¯„ä¼°çš„é€Ÿåº¦å’Œå‡†ç¡®åº¦ï¼Œå…·æœ‰é™ä½å†åˆ‡é™¤ç‡ã€æ”¹å–„ä¹³è…ºç™Œæ²»ç–—æ‰‹æœ¯æ•ˆæœçš„é‡è¦æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å®Œå…¨ç§»é™¤ç™Œç—‡è‚¿ç˜¤å¹¶ç¡®è¯è´Ÿæ€§æ ‡æœ¬è¾¹ç¼˜å¯¹é™ä½ä¹³è…ºç™Œå¤å‘è‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰ä½¿ç”¨çš„äºŒç»´æ ‡æœ¬æ”¾å°„çº¿æ£€æµ‹ï¼ˆSRï¼‰æ–¹æ³•å­˜åœ¨å‡†ç¡®åº¦é—®é¢˜ï¼Œçº¦å››åˆ†ä¹‹ä¸€çš„æ‚£è€…éœ€æ¥å—å†æ¬¡æ‰‹æœ¯ã€‚</li>
<li>æå‡ºçš„æ·±åº¦å­¦ä¹ æ¡†æ¶ç»“åˆäº†Segment Anything Modelï¼ˆSAMï¼‰ä¸Forward-Forward Contrastive Learningï¼ˆFFCLï¼‰ã€‚</li>
<li>é€šè¿‡æ ‡æ³¨SRå›¾åƒå¹¶é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œè¾¹ç¼˜çŠ¶æ€åˆ†ç±»ï¼Œå®ç°äº†è¾ƒé«˜çš„åˆ†ç±»å‡†ç¡®åº¦ã€‚</li>
<li>ä½¿ç”¨é‡å»ºçš„ç²—äºŒè¿›åˆ¶æ©è†œå¼•å¯¼SAMè¿›è¡Œæ›´ç²¾ç»†çš„è‚¿ç˜¤è¾¹ç¼˜åˆ†å‰²ï¼Œæé«˜äº†åˆ†å‰²æ•ˆæœã€‚</li>
<li>FFCL-SAMæ–¹æ³•æé«˜äº†æœ¯ä¸­è¾¹ç¼˜è¯„ä¼°çš„é€Ÿåº¦å’Œå‡†ç¡®åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.21006">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6ba5cab390e1f287799a9ad33e3df777.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55bff2f010173c489385b203c979f0da.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Segment-Anything-in-Pathology-Images-with-Natural-Language"><a href="#Segment-Anything-in-Pathology-Images-with-Natural-Language" class="headerlink" title="Segment Anything in Pathology Images with Natural Language"></a>Segment Anything in Pathology Images with Natural Language</h2><p><strong>Authors:Zhixuan Chen, Junlin Hou, Liqi Lin, Yihui Wang, Yequan Bie, Xi Wang, Yanning Zhou, Ronald Cheong Kin Chan, Hao Chen</strong></p>
<p>Pathology image segmentation is crucial in computational pathology for analyzing histological features relevant to cancer diagnosis and prognosis. However, current methods face major challenges in clinical applications due to limited annotated data and restricted category definitions. To address these limitations, we propose PathSegmentor, the first text-prompted segmentation foundation model designed specifically for pathology images. We also introduce PathSeg , the largest and most comprehensive dataset for pathology segmentation, built from 17 public sources and containing 275k image-mask-label triples across 160 diverse categories. With PathSegmentor, users can perform semantic segmentation using natural language prompts, eliminating the need for laborious spatial inputs such as points or boxes. Extensive experiments demonstrate that PathSegmentor outperforms specialized models with higher accuracy and broader applicability, while maintaining a compact architecture. It significantly surpasses existing spatial- and text-prompted models by 0.145 and 0.429 in overall Dice scores, respectively, showing strong robustness in segmenting complex structures and generalizing to external datasets. Moreover, PathSegmentorâ€™s outputs enhance the interpretability of diagnostic models through feature importance estimation and imaging biomarker discovery, offering pathologists evidence-based support for clinical decision-making. This work advances the development of explainable AI in precision oncology. </p>
<blockquote>
<p>ç—…ç†å­¦å›¾åƒåˆ†å‰²åœ¨è®¡ç®—ç—…ç†å­¦ä¸­å¯¹åˆ†æç™Œç—‡è¯Šæ–­å’Œæ²»ç–—ç›¸å…³çš„ç»„ç»‡å­¦ç‰¹å¾è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç”±äºæ ‡æ³¨æ•°æ®æœ‰é™å’Œç±»åˆ«å®šä¹‰å—é™ï¼Œå½“å‰æ–¹æ³•åœ¨ä¸´åºŠåº”ç”¨ä¸­é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†PathSegmentorï¼Œè¿™æ˜¯é¦–ä¸ªä¸“ä¸ºç—…ç†å­¦å›¾åƒè®¾è®¡çš„æ–‡æœ¬æç¤ºåˆ†å‰²åŸºç¡€æ¨¡å‹ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†PathSegï¼Œè¿™æ˜¯æœ€å¤§çš„ç»¼åˆæ€§ç—…ç†å­¦åˆ†å‰²æ•°æ®é›†ï¼Œç”±17ä¸ªå…¬å¼€æ¥æºæ„å»ºï¼ŒåŒ…å«è·¨è¶Š160ä¸ªä¸åŒç±»åˆ«çš„27.5ä¸‡å¼ å›¾åƒ-æ©è†œ-æ ‡ç­¾ä¸‰å…ƒç»„ã€‚é€šè¿‡PathSegmentorï¼Œç”¨æˆ·å¯ä»¥ä½¿ç”¨è‡ªç„¶è¯­è¨€æç¤ºè¿›è¡Œè¯­ä¹‰åˆ†å‰²ï¼Œæ— éœ€ç¹ççš„ç©ºé—´è¾“å…¥ï¼Œå¦‚ç‚¹æˆ–æ¡†ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒPathSegmentoråœ¨å‡†ç¡®åº¦å’Œé€‚ç”¨æ€§æ–¹é¢è¶…è¶Šäº†ä¸“ä¸šæ¨¡å‹ï¼ŒåŒæ—¶ä¿æŒäº†ç´§å‡‘çš„æ¶æ„ã€‚åœ¨æ€»ä½“Diceå¾—åˆ†ä¸Šï¼Œå®ƒä¸ç°æœ‰çš„ç©ºé—´æç¤ºæ¨¡å‹å’Œæ–‡æœ¬æç¤ºæ¨¡å‹ç›¸æ¯”ï¼Œåˆ†åˆ«é«˜å‡º0.145å’Œ0.429ï¼Œåœ¨åˆ†å‰²å¤æ‚ç»“æ„å’Œé€‚åº”å¤–éƒ¨æ•°æ®é›†æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„ç¨³å¥æ€§ã€‚æ­¤å¤–ï¼ŒPathSegmentorçš„è¾“å‡ºé€šè¿‡ç‰¹å¾é‡è¦æ€§è¯„ä¼°å’Œæˆåƒç”Ÿç‰©æ ‡å¿—ç‰©å‘ç°ï¼Œå¢å¼ºäº†è¯Šæ–­æ¨¡å‹çš„å¯è§£é‡Šæ€§ï¼Œä¸ºç—…ç†å­¦å®¶æä¾›åŸºäºè¯æ®çš„æ”¯æŒï¼Œæœ‰åŠ©äºä¸´åºŠå†³ç­–ã€‚è¿™é¡¹å·¥ä½œæ¨åŠ¨äº†ç²¾å‡†è‚¿ç˜¤å­¦ä¸­å¯è§£é‡Šäººå·¥æ™ºèƒ½çš„å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20988v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç—…ç†å›¾åƒåˆ†å‰²åœ¨è®¡ç®—ç—…ç†å­¦ä¸­çš„é‡è¦æ€§ï¼Œå¹¶æŒ‡å‡ºäº†å½“å‰æ–¹æ³•åœ¨ä¸´åºŠåº”ç”¨ä¸­çš„å±€é™æ€§ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºPathSegmentorçš„æ–‡æœ¬æç¤ºåˆ†å‰²åŸºç¡€æ¨¡å‹ï¼Œå¹¶å¼•å…¥äº†è§„æ¨¡æœ€å¤§çš„ç—…ç†åˆ†å‰²æ•°æ®é›†PathSegã€‚PathSegmentorå…è®¸ç”¨æˆ·é€šè¿‡è‡ªç„¶è¯­è¨€æç¤ºè¿›è¡Œè¯­ä¹‰åˆ†å‰²ï¼Œæ— éœ€ç¹ççš„ç©ºé—´è¾“å…¥ã€‚å®éªŒè¯æ˜ï¼ŒPathSegmentoråœ¨å‡†ç¡®æ€§å’Œé€‚ç”¨æ€§æ–¹é¢è¶…è¶Šäº†ä¸“ä¸šæ¨¡å‹ï¼ŒåŒæ—¶åœ¨æ€»ä½“Diceå¾—åˆ†ä¸Šåˆ†åˆ«æ¯”ç°æœ‰çš„ç©ºé—´æç¤ºå’Œæ–‡æœ¬æç¤ºæ¨¡å‹é«˜å‡º0.145å’Œ0.429ã€‚æ­¤å¤–ï¼ŒPathSegmentorçš„è¾“å‡ºå¢å¼ºäº†è¯Šæ–­æ¨¡å‹çš„è§£é‡Šæ€§ï¼Œä¸ºç—…ç†åŒ»å¸ˆæä¾›åŸºäºè¯æ®çš„æ”¯æŒï¼Œæœ‰åŠ©äºä¸´åºŠå†³ç­–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç—…ç†å›¾åƒåˆ†å‰²åœ¨è®¡ç®—ç—…ç†å­¦ä¸­å¯¹ç™Œç—‡è¯Šæ–­å’Œæ²»ç–—ååº”è¯„ä¼°çš„ç»„ç»‡å­¦ç‰¹å¾åˆ†æè‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰ç—…ç†å›¾åƒåˆ†å‰²æ–¹æ³•åœ¨ä¸´åºŠåº”ç”¨ä¸­é¢ä¸´æœ‰é™æ ‡æ³¨æ•°æ®å’Œç±»åˆ«å®šä¹‰é™åˆ¶çš„æŒ‘æˆ˜ã€‚</li>
<li>PathSegmentoræ˜¯é¦–ä¸ªé’ˆå¯¹ç—…ç†å›¾åƒè®¾è®¡çš„æ–‡æœ¬æç¤ºåˆ†å‰²åŸºç¡€æ¨¡å‹ï¼Œç®€åŒ–äº†å¤æ‚çš„ç©ºé—´è¾“å…¥éœ€æ±‚ã€‚</li>
<li>PathSegæ•°æ®é›†æ˜¯è§„æ¨¡æœ€å¤§çš„ç—…ç†åˆ†å‰²æ•°æ®é›†ï¼ŒåŒ…å«æ¥è‡ª17ä¸ªå…¬å…±æ¥æºçš„27.5ä¸‡å¼ å›¾åƒ-æ©è†œ-æ ‡ç­¾ä¸‰å…ƒç»„ï¼Œæ¶‰åŠ160å¤šä¸ªç±»åˆ«ã€‚</li>
<li>PathSegmentoråœ¨æ€»ä½“Diceå¾—åˆ†ä¸Šæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„ç©ºé—´æç¤ºå’Œæ–‡æœ¬æç¤ºæ¨¡å‹ã€‚</li>
<li>PathSegmentorçš„è¾“å‡ºå¢å¼ºäº†è¯Šæ–­æ¨¡å‹çš„è§£é‡Šæ€§ï¼Œæœ‰åŠ©äºç—…ç†åŒ»å¸ˆè¿›è¡Œä¸´åºŠå†³ç­–ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20988">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8e1733ce272818bc3f8356ec400a1d57.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-aa7a9a0d4a06a7a4ed82ec7b4ffff039.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="AI-Driven-MRI-based-Brain-Tumour-Segmentation-Benchmarking"><a href="#AI-Driven-MRI-based-Brain-Tumour-Segmentation-Benchmarking" class="headerlink" title="AI-Driven MRI-based Brain Tumour Segmentation Benchmarking"></a>AI-Driven MRI-based Brain Tumour Segmentation Benchmarking</h2><p><strong>Authors:Connor Ludwig, Khashayar Namdar, Farzad Khalvati</strong></p>
<p>Medical image segmentation has greatly aided medical diagnosis, with U-Net based architectures and nnU-Net providing state-of-the-art performance. There have been numerous general promptable models and medical variations introduced in recent years, but there is currently a lack of evaluation and comparison of these models across a variety of prompt qualities on a common medical dataset. This research uses Segment Anything Model (SAM), Segment Anything Model 2 (SAM 2), MedSAM, SAM-Med-3D, and nnU-Net to obtain zero-shot inference on the BraTS 2023 adult glioma and pediatrics dataset across multiple prompt qualities for both points and bounding boxes. Several of these models exhibit promising Dice scores, particularly SAM and SAM 2 achieving scores of up to 0.894 and 0.893, respectively when given extremely accurate bounding box prompts which exceeds nnU-Netâ€™s segmentation performance. However, nnU-Net remains the dominant medical image segmentation network due to the impracticality of providing highly accurate prompts to the models. The model and prompt evaluation, as well as the comparison, are extended through fine-tuning SAM, SAM 2, MedSAM, and SAM-Med-3D on the pediatrics dataset. The improvements in point prompt performance after fine-tuning are substantial and show promise for future investigation, but are unable to achieve better segmentation than bounding boxes or nnU-Net. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²å·²ç»æå¤§åœ°è¾…åŠ©äº†åŒ»å­¦è¯Šæ–­ï¼ŒåŸºäºU-Netçš„æ¶æ„å’ŒnnU-Netæä¾›äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¿‘å¹´æ¥å·²ç»å‡ºç°äº†è®¸å¤šé€šç”¨æç¤ºæ¨¡å‹å’ŒåŒ»ç–—å˜ä½“ï¼Œä½†åœ¨ä¸€ä¸ªå…¬å…±åŒ»å­¦æ•°æ®é›†ä¸Šï¼Œå…³äºè¿™äº›æ¨¡å‹åœ¨å„ç§æç¤ºè´¨é‡æ–¹é¢çš„è¯„ä¼°ä¸æ¯”è¾ƒä»ç„¶ç¼ºä¹ã€‚æœ¬ç ”ç©¶ä½¿ç”¨Segment Anything Modelï¼ˆSAMï¼‰ã€Segment Anything Model 2ï¼ˆSAM 2ï¼‰ã€MedSAMã€SAM-Med-3Då’ŒnnU-Netï¼Œåœ¨BraTS 2023æˆäººèƒ¶è´¨ç˜¤å’Œå„¿ç§‘æ•°æ®é›†ä¸Šï¼Œé€šè¿‡å¤šç§æç¤ºè´¨é‡è·å¾—é›¶å°„å‡»æ¨æ–­ç‚¹å’Œè¾¹ç•Œæ¡†ã€‚å…¶ä¸­ä¸€äº›æ¨¡å‹è¡¨ç°å‡ºæœ‰å¸Œæœ›çš„Diceå¾—åˆ†ï¼Œç‰¹åˆ«æ˜¯SAMå’ŒSAM 2åœ¨ç»™å‡ºäº†æå…¶å‡†ç¡®çš„è¾¹ç•Œæ¡†æç¤ºæ—¶ï¼Œåˆ†åˆ«è¾¾åˆ°äº†0.894å’Œ0.893çš„åˆ†æ•°ï¼Œè¶…è¿‡äº†nnU-Netçš„åˆ†å‰²æ€§èƒ½ã€‚ç„¶è€Œï¼Œç”±äºå‘æ¨¡å‹æä¾›é«˜åº¦å‡†ç¡®æç¤ºçš„ä¸åˆ‡å®é™…æ€§ï¼ŒnnU-Netä»ç„¶æ˜¯ä¸»å¯¼çš„åŒ»ç–—å›¾åƒåˆ†å‰²ç½‘ç»œã€‚é€šè¿‡å¯¹SAMã€SAM 2ã€MedSAMå’ŒSAM-Med-3Dåœ¨å„¿ç§‘æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒï¼Œæ¨¡å‹å’Œæç¤ºçš„è¯„ä¼°ä»¥åŠæ¯”è¾ƒå¾—åˆ°äº†æ‰©å±•ã€‚å¾®è°ƒåç‚¹æç¤ºæ€§èƒ½çš„æ”¹è¿›æ˜¯å·¨å¤§çš„ï¼Œæ˜¾ç¤ºå‡ºæœªæ¥ç ”ç©¶çš„å¸Œæœ›ï¼Œä½†æ— æ³•å®ç°åœ¨è¾¹ç•Œæ¡†æˆ–nnU-Netä¸Šçš„æ›´å¥½åˆ†å‰²æ•ˆæœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20786v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†åŒ»å­¦å›¾åƒåˆ†å‰²åœ¨åŒ»ç–—è¯Šæ–­ä¸­çš„åº”ç”¨ï¼Œå¹¶æ¯”è¾ƒäº†å¤šç§æ¨¡å‹ï¼ˆåŒ…æ‹¬U-Netã€SAMã€SAM 2ã€MedSAMã€SAM-Med-3Dç­‰ï¼‰åœ¨BraTS 2023æˆäººèƒ¶è´¨ç˜¤å’Œå„¿ç§‘æ•°æ®é›†ä¸Šçš„è¡¨ç°ã€‚ç ”ç©¶å‘ç°ï¼Œåœ¨æŸäº›æ¡ä»¶ä¸‹ï¼ŒSAMå’ŒSAM 2çš„Diceå¾—åˆ†é«˜äºnnU-Netï¼Œä½†åœ¨æä¾›é«˜åº¦å‡†ç¡®çš„æç¤ºæ–¹é¢ï¼ŒnnU-Netä»æ˜¯ä¸»å¯¼çš„åŒ»ç–—å›¾åƒåˆ†å‰²ç½‘ç»œã€‚é€šè¿‡å¾®è°ƒæ¨¡å‹ï¼Œç‚¹æç¤ºæ€§èƒ½å¾—åˆ°æ˜¾è‘—æ”¹å–„ï¼Œä½†æ— æ³•å®ç°æ¯”è¾¹ç•Œæ¡†æˆ–nnU-Netæ›´å¥½çš„åˆ†å‰²æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>åŒ»ç–—å›¾åƒåˆ†å‰²å¯¹åŒ»ç–—è¯Šæ–­æœ‰é‡è¦ä½œç”¨ï¼Œå¤šç§æ¨¡å‹å¦‚U-Netã€SAMç³»åˆ—ç­‰åœ¨BraTSæ•°æ®é›†ä¸Šå¾—åˆ°åº”ç”¨ã€‚</li>
<li>SAMå’ŒSAM 2åœ¨æŸäº›æ¡ä»¶ä¸‹è¡¨ç°å‡ºè¾ƒé«˜çš„Diceå¾—åˆ†ï¼Œä½†åœ¨æä¾›é«˜åº¦å‡†ç¡®çš„æç¤ºæ–¹é¢ï¼ŒnnU-Netä»æ˜¯ä¸»å¯¼çš„åŒ»ç–—å›¾åƒåˆ†å‰²ç½‘ç»œã€‚</li>
<li>ç‚¹æç¤ºåœ¨å¾®è°ƒåæ€§èƒ½æ˜¾è‘—æ”¹å–„ï¼Œä½†æ— æ³•å®ç°æ¯”è¾¹ç•Œæ¡†æˆ–nnU-Netæ›´å¥½çš„åˆ†å‰²æ•ˆæœã€‚</li>
<li>ç›®å‰ç¼ºä¹å¯¹å„ç§æ¨¡å‹åœ¨ä¸åŒæç¤ºè´¨é‡ä¸‹çš„è¯„ä¼°å’Œæ¯”è¾ƒã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20786">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7d834b0811b649be5a34e68a97d925a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-809891d3129cd78d7e46f048c12a9970.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6949a9fc2ada07631f072ce260854e03.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73ab3c705e16d846a4c8230cbc8b104d.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="MiCo-Multiple-Instance-Learning-with-Context-Aware-Clustering-for-Whole-Slide-Image-Analysis"><a href="#MiCo-Multiple-Instance-Learning-with-Context-Aware-Clustering-for-Whole-Slide-Image-Analysis" class="headerlink" title="MiCo: Multiple Instance Learning with Context-Aware Clustering for Whole   Slide Image Analysis"></a>MiCo: Multiple Instance Learning with Context-Aware Clustering for Whole   Slide Image Analysis</h2><p><strong>Authors:Junjian Li, Hulin Kuang, Jin Liu, Hailin Yue, Mengshen He, Jianxin Wang</strong></p>
<p>Multiple instance learning (MIL) has shown significant promise in histopathology whole slide image (WSI) analysis for cancer diagnosis and prognosis. However, the inherent spatial heterogeneity of WSIs presents critical challenges, as morphologically similar tissue types are often dispersed across distant anatomical regions. Conventional MIL methods struggle to model these scattered tissue distributions and capture cross-regional spatial interactions effectively. To address these limitations, we propose a novel Multiple instance learning framework with Context-Aware Clustering (MiCo), designed to enhance cross-regional intra-tissue correlations and strengthen inter-tissue semantic associations in WSIs. MiCo begins by clustering instances to distill discriminative morphological patterns, with cluster centroids serving as semantic anchors. To enhance cross-regional intra-tissue correlations, MiCo employs a Cluster Route module, which dynamically links instances of the same tissue type across distant regions via feature similarity. These semantic anchors act as contextual hubs, propagating semantic relationships to refine instance-level representations. To eliminate semantic fragmentation and strengthen inter-tissue semantic associations, MiCo integrates a Cluster Reducer module, which consolidates redundant anchors while enhancing information exchange between distinct semantic groups. Extensive experiments on two challenging tasks across nine large-scale public cancer datasets demonstrate the effectiveness of MiCo, showcasing its superiority over state-of-the-art methods. The code is available at <a target="_blank" rel="noopener" href="https://github.com/junjianli106/MiCo">https://github.com/junjianli106/MiCo</a>. </p>
<blockquote>
<p>å¤šå®ä¾‹å­¦ä¹ ï¼ˆMILï¼‰åœ¨ç—…ç†å­¦å…¨åˆ‡ç‰‡å›¾åƒï¼ˆWSIï¼‰åˆ†æä¸­æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œå¯¹äºç™Œç—‡çš„è¯Šæ–­å’Œé¢„åå…·æœ‰é‡è¦çš„åº”ç”¨ä»·å€¼ã€‚ç„¶è€Œï¼ŒWSIå›ºæœ‰çš„ç©ºé—´å¼‚è´¨æ€§å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ï¼Œå› ä¸ºå½¢æ€ç›¸ä¼¼çš„ç»„ç»‡ç±»å‹é€šå¸¸åˆ†å¸ƒåœ¨é¥è¿œçš„è§£å‰–åŒºåŸŸã€‚ä¼ ç»Ÿçš„MILæ–¹æ³•éš¾ä»¥å¯¹è¿™äº›åˆ†æ•£çš„ç»„ç»‡åˆ†å¸ƒè¿›è¡Œå»ºæ¨¡ï¼Œå¹¶æœ‰æ•ˆåœ°æ•è·è·¨åŒºåŸŸçš„ç©ºé—´äº¤äº’ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å…·æœ‰ä¸Šä¸‹æ–‡æ„ŸçŸ¥èšç±»çš„å¤šå®ä¾‹å­¦ä¹ æ¡†æ¶ï¼ˆMiCoï¼‰ï¼Œæ—¨åœ¨å¢å¼ºè·¨åŒºåŸŸçš„ç»„ç»‡å†…ç›¸å…³æ€§å’ŒåŠ å¼ºWSIä¸­çš„ç»„ç»‡é—´è¯­ä¹‰å…³è”ã€‚MiCoé¦–å…ˆé€šè¿‡èšç±»å®ä¾‹æ¥æç‚¼åˆ¤åˆ«æ€§å½¢æ€æ¨¡å¼ï¼Œä»¥èšç±»ä¸­å¿ƒä½œä¸ºè¯­ä¹‰é”šç‚¹ã€‚ä¸ºäº†å¢å¼ºè·¨åŒºåŸŸçš„ç»„ç»‡å†…ç›¸å…³æ€§ï¼ŒMiCoé‡‡ç”¨äº†ä¸€ä¸ªèšç±»è·¯å¾„æ¨¡å—ï¼Œè¯¥æ¨¡å—é€šè¿‡ç‰¹å¾ç›¸ä¼¼æ€§åŠ¨æ€é“¾æ¥åŒä¸€ç»„ç»‡ç±»å‹çš„å®ä¾‹ï¼Œè¿™äº›å®ä¾‹ä½äºä¸åŒçš„åŒºåŸŸã€‚è¿™äº›è¯­ä¹‰é”šç‚¹ä½œä¸ºä¸Šä¸‹æ–‡ä¸­å¿ƒï¼Œä¼ æ’­è¯­ä¹‰å…³ç³»ä»¥ä¼˜åŒ–å®ä¾‹çº§çš„è¡¨ç¤ºã€‚ä¸ºäº†æ¶ˆé™¤è¯­ä¹‰ç¢ç‰‡å¹¶åŠ å¼ºç»„ç»‡é—´çš„è¯­ä¹‰å…³è”ï¼ŒMiCoé›†æˆäº†ä¸€ä¸ªèšç±»ç¼©å‡æ¨¡å—ï¼Œè¯¥æ¨¡å—å·©å›ºäº†å†—ä½™çš„é”šç‚¹ï¼ŒåŒæ—¶å¢å¼ºäº†ä¸åŒè¯­ä¹‰ç»„ä¹‹é—´çš„ä¿¡æ¯äº¤æ¢ã€‚åœ¨ä¸¤ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒï¼Œæ¶‰åŠä¹ä¸ªå¤§è§„æ¨¡å…¬å…±ç™Œç—‡æ•°æ®é›†ï¼Œè¯æ˜äº†MiCoçš„æœ‰æ•ˆæ€§ï¼Œå±•ç¤ºäº†å…¶ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/junjianli106/MiCo%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/junjianli106/MiCoä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18028v2">PDF</a> MICCAI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºå¤šå®ä¾‹å­¦ä¹ æ¡†æ¶çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥èšç±»æ–¹æ³•ï¼ˆMiCoï¼‰ï¼Œç”¨äºå¢å¼ºå…¨è§†é‡å›¾åƒï¼ˆWSIï¼‰åˆ†æä¸­è·¨åŒºåŸŸçš„å†…éƒ¨ç»„ç»‡å…³è”æ€§å’Œç»„ç»‡é—´è¯­ä¹‰å…³è”ã€‚MiCoé€šè¿‡èšç±»å®ä¾‹æ¥æå–åˆ¤åˆ«å½¢æ€æ¨¡å¼ï¼Œå¹¶ä»¥èšç±»ä¸­å¿ƒç‚¹ä½œä¸ºè¯­ä¹‰é”šç‚¹ã€‚é€šè¿‡åŠ¨æ€é“¾æ¥åŒä¸€ç»„ç»‡ç±»å‹çš„å®ä¾‹ï¼ŒMiCoå¢å¼ºäº†è·¨åŒºåŸŸçš„å†…éƒ¨ç»„ç»‡å…³è”æ€§ï¼›åŒæ—¶æ•´åˆCluster Reduceræ¨¡å—ï¼Œå·©å›ºå†—ä½™é”šç‚¹å¹¶å¼ºåŒ–ä¸åŒè¯­ä¹‰ç»„ä¹‹é—´çš„ä¿¡æ¯äº¤æµã€‚åœ¨å¤šä¸ªå¤§å‹å…¬å…±ç™Œç—‡æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMiCoæ–¹æ³•æ•ˆæœæ˜¾è‘—ï¼Œä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MiCoæ˜¯ä¸€ç§åŸºäºå¤šå®ä¾‹å­¦ä¹ æ¡†æ¶çš„æ–¹æ³•ï¼Œç”¨äºè§£å†³å…¨è§†é‡å›¾åƒåˆ†æä¸­çš„ç©ºé—´å¼‚è´¨æ€§æŒ‘æˆ˜ã€‚</li>
<li>MiCoé€šè¿‡ä¸Šä¸‹æ–‡æ„ŸçŸ¥èšç±»å¢å¼ºè·¨åŒºåŸŸçš„å†…éƒ¨ç»„ç»‡å…³è”æ€§ã€‚</li>
<li>èšç±»ä¸­å¿ƒç‚¹ä½œä¸ºè¯­ä¹‰é”šç‚¹ï¼Œå¸®åŠ©è¯†åˆ«å’Œç»„ç»‡å½¢æ€æ¨¡å¼ã€‚</li>
<li>MiCoé‡‡ç”¨Cluster Routeæ¨¡å—ï¼Œé€šè¿‡ç‰¹å¾ç›¸ä¼¼æ€§åŠ¨æ€é“¾æ¥åŒä¸€ç»„ç»‡ç±»å‹çš„å®ä¾‹ã€‚</li>
<li>Cluster Reduceræ¨¡å—ç”¨äºå·©å›ºå†—ä½™é”šç‚¹å¹¶å¼ºåŒ–ä¸åŒè¯­ä¹‰ç»„ä¹‹é—´çš„ä¿¡æ¯äº¤æµã€‚</li>
<li>MiCoåœ¨å¤šä¸ªå¤§å‹å…¬å…±ç™Œç—‡æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18028">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ce994f50789689f7d293c15bba2f1bff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a2a44417b13546c7717930e3ea592cb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-04db297d9c2705240acff13f7e65fce1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f0585a4116a0929b964626466db1dbcf.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Simultaneous-Segmentation-of-Ventricles-and-Normal-Abnormal-White-Matter-Hyperintensities-in-Clinical-MRI-using-Deep-Learning"><a href="#Simultaneous-Segmentation-of-Ventricles-and-Normal-Abnormal-White-Matter-Hyperintensities-in-Clinical-MRI-using-Deep-Learning" class="headerlink" title="Simultaneous Segmentation of Ventricles and Normal&#x2F;Abnormal White Matter   Hyperintensities in Clinical MRI using Deep Learning"></a>Simultaneous Segmentation of Ventricles and Normal&#x2F;Abnormal White Matter   Hyperintensities in Clinical MRI using Deep Learning</h2><p><strong>Authors:Mahdi Bashiri Bawil, Mousa Shamsi, Abolhassan Shakeri Bavil</strong></p>
<p>Multiple sclerosis (MS) diagnosis and monitoring rely heavily on accurate assessment of brain MRI biomarkers, particularly white matter hyperintensities (WMHs) and ventricular changes. Current segmentation approaches suffer from several limitations: they typically segment these structures independently despite their pathophysiological relationship, struggle to differentiate between normal and pathological hyperintensities, and are poorly optimized for anisotropic clinical MRI data. We propose a novel 2D pix2pix-based deep learning framework for simultaneous segmentation of ventricles and WMHs with the unique capability to distinguish between normal periventricular hyperintensities and pathological MS lesions. Our method was developed and validated on FLAIR MRI scans from 300 MS patients. Compared to established methods (SynthSeg, Atlas Matching, BIANCA, LST-LPA, LST-LGA, and WMH-SynthSeg), our approach achieved superior performance for both ventricle segmentation (Dice: 0.801+&#x2F;-0.025, HD95: 18.46+&#x2F;-7.1mm) and WMH segmentation (Dice: 0.624+&#x2F;-0.061, precision: 0.755+&#x2F;-0.161). Furthermore, our method successfully differentiated between normal and abnormal hyperintensities with a Dice coefficient of 0.647. Notably, our approach demonstrated exceptional computational efficiency, completing end-to-end processing in approximately 4 seconds per case, up to 36 times faster than baseline methods, while maintaining minimal resource requirements. This combination of improved accuracy, clinically relevant differentiation capability, and computational efficiency addresses critical limitations in current neuroimaging analysis, potentially enabling integration into routine clinical workflows and enhancing MS diagnosis and monitoring. </p>
<blockquote>
<p>å¤šå‘æ€§ç¡¬åŒ–ç—‡ï¼ˆMSï¼‰çš„è¯Šæ–­å’Œç›‘æµ‹ä¸¥é‡ä¾èµ–äºå¯¹å¤§è„‘MRIç”Ÿç‰©æ ‡å¿—ç‰©çš„å‡†ç¡®è¯„ä¼°ï¼Œç‰¹åˆ«æ˜¯è„‘ç™½è´¨é«˜ä¿¡å·ï¼ˆWMHsï¼‰å’Œè„‘å®¤å˜åŒ–ã€‚å½“å‰çš„åˆ†å‰²æ–¹æ³•å­˜åœ¨å‡ ä¸ªå±€é™æ€§ï¼šå®ƒä»¬é€šå¸¸ç‹¬ç«‹åœ°åˆ†å‰²è¿™äº›ç»“æ„ï¼Œå°½ç®¡å­˜åœ¨ç—…ç†ç”Ÿç†å…³ç³»ï¼›éš¾ä»¥åŒºåˆ†æ­£å¸¸å’Œç—…ç†æ€§é«˜ä¿¡å·ï¼›å¯¹äºå„å‘å¼‚æ€§çš„ä¸´åºŠMRIæ•°æ®ä¼˜åŒ–ä¸è¶³ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„åŸºäºäºŒç»´pix2pixçš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œå¯ä»¥åŒæ—¶åˆ†å‰²è„‘å®¤å’ŒWMHsï¼Œå…·æœ‰åŒºåˆ†æ­£å¸¸è„‘å®¤å‘¨å›´é«˜ä¿¡å·å’Œç—…ç†æ€§MSç—…å˜çš„ç‹¬ç‰¹èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ¥è‡ª300åMSæ‚£è€…çš„FLAIR MRIæ‰«æä¸Šè¿›è¡Œäº†å¼€å‘å’ŒéªŒè¯ã€‚ä¸ç°æœ‰æ–¹æ³•ï¼ˆSynthSegã€Atlas Matchingã€BIANCAã€LST-LPAã€LST-LGAå’ŒWMH-SynthSegï¼‰ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è„‘å®¤åˆ†å‰²ï¼ˆDiceï¼š0.801+&#x2F;-0.025ï¼ŒHD95ï¼š18.46+&#x2F;-7.1mmï¼‰å’ŒWMHåˆ†å‰²ï¼ˆDiceï¼š0.624+&#x2F;-0.061ï¼Œç²¾ç¡®åº¦ï¼š0.755+&#x2F;-0.161ï¼‰æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æˆåŠŸåœ°åŒºåˆ†äº†æ­£å¸¸å’Œå¼‚å¸¸çš„é«˜ä¿¡å·ï¼ŒDiceç³»æ•°ä¸º0.647ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è®¡ç®—æ•ˆç‡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œæ¯ä¾‹ç—…ä¾‹çš„ç«¯åˆ°ç«¯å¤„ç†æ—¶é—´çº¦ä¸º4ç§’ï¼Œæ¯”åŸºçº¿æ–¹æ³•å¿«36å€ï¼ŒåŒæ—¶ä¿æŒæœ€ä½çš„èµ„æºéœ€æ±‚ã€‚è¿™ç§é›†æé«˜å‡†ç¡®æ€§ã€å…·æœ‰ä¸´åºŠæ„ä¹‰çš„é‰´åˆ«èƒ½åŠ›å’Œè®¡ç®—æ•ˆç‡äºä¸€ä½“çš„æ–¹æ³•ï¼Œè§£å†³äº†å½“å‰ç¥ç»å½±åƒåˆ†æçš„å…³é”®å±€é™æ€§ï¼Œæœ‰æœ›èå…¥å¸¸è§„ä¸´åºŠå·¥ä½œæµç¨‹ï¼Œæé«˜MSçš„è¯Šæ–­å’Œç›‘æµ‹æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07123v2">PDF</a> 43 pages, 11 figures, 1 table</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„äºŒç»´pix2pixæ¡†æ¶ï¼Œå¯åŒæ—¶åˆ†å‰²è„‘å®¤å’Œè„‘ç™½è´¨é«˜ä¿¡å·åŒºï¼ˆWMHsï¼‰ï¼Œå¹¶åŒºåˆ†æ­£å¸¸ä¸ç—…ç†æ€§MSç—…å˜ã€‚åœ¨300åMSæ‚£è€…çš„FLAIR MRIæ‰«æä¸ŠéªŒè¯ï¼Œè¯¥æ–¹æ³•çš„æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè®¡ç®—æ•ˆç‡é«˜ï¼Œä¸ºMSè¯Šæ–­å’Œç›‘æµ‹æä¾›äº†é‡è¦æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æ–‡ä¸­æå‡ºäº†ä¸€ç§æ–°å‹çš„åŸºäºæ·±åº¦å­¦ä¹ çš„pix2pixæ¡†æ¶ï¼Œèƒ½åŒæ—¶åˆ†å‰²è„‘å®¤å’ŒWMHsã€‚</li>
<li>è¯¥æ–¹æ³•å¯ä»¥åŒºåˆ†æ­£å¸¸å’Œç—…ç†æ€§çš„MSç—…å˜ã€‚</li>
<li>æ–¹æ³•åœ¨300åMSæ‚£è€…çš„FLAIR MRIæ‰«æä¸Šè¿›è¡Œäº†å¼€å‘å’ŒéªŒè¯ã€‚</li>
<li>ç›¸æ¯”ç°æœ‰æ–¹æ³•ï¼Œæ‰€ææ–¹æ³•åœ¨è„‘å®¤å’ŒWMHsåˆ†å‰²ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•å…·æœ‰é«˜æ•ˆçš„è®¡ç®—èƒ½åŠ›ï¼Œèƒ½åœ¨çº¦4ç§’å†…å®Œæˆæ¡ˆä¾‹çš„ç«¯åˆ°ç«¯å¤„ç†ï¼Œæ¯”åŸºçº¿æ–¹æ³•å¿«36å€ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07123">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-81cef4617349d17c7f0f54942a835953.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="High-Temporal-Consistency-through-Semantic-Similarity-Propagation-in-Semi-Supervised-Video-Semantic-Segmentation-for-Autonomous-Flight"><a href="#High-Temporal-Consistency-through-Semantic-Similarity-Propagation-in-Semi-Supervised-Video-Semantic-Segmentation-for-Autonomous-Flight" class="headerlink" title="High Temporal Consistency through Semantic Similarity Propagation in   Semi-Supervised Video Semantic Segmentation for Autonomous Flight"></a>High Temporal Consistency through Semantic Similarity Propagation in   Semi-Supervised Video Semantic Segmentation for Autonomous Flight</h2><p><strong>Authors:CÃ©dric Vincent, Taehyoung Kim, Henri MeeÃŸ</strong></p>
<p>Semantic segmentation from RGB cameras is essential to the perception of autonomous flying vehicles. The stability of predictions through the captured videos is paramount to their reliability and, by extension, to the trustworthiness of the agents. In this paper, we propose a lightweight video semantic segmentation approach-suited to onboard real-time inference-achieving high temporal consistency on aerial data through Semantic Similarity Propagation across frames. SSP temporally propagates the predictions of an efficient image segmentation model with global registration alignment to compensate for camera movements. It combines the current estimation and the prior prediction with linear interpolation using weights computed from the features similarities of the two frames. Because data availability is a challenge in this domain, we propose a consistency-aware Knowledge Distillation training procedure for sparsely labeled datasets with few annotations. Using a large image segmentation model as a teacher to train the efficient SSP, we leverage the strong correlations between labeled and unlabeled frames in the same training videos to obtain high-quality supervision on all frames. KD-SSP obtains a significant temporal consistency increase over the base image segmentation model of 12.5% and 6.7% TC on UAVid and RuralScapes respectively, with higher accuracy and comparable inference speed. On these aerial datasets, KD-SSP provides a superior segmentation quality and inference speed trade-off than other video methods proposed for general applications and shows considerably higher consistency. Project page: <a target="_blank" rel="noopener" href="https://github.com/FraunhoferIVI/SSP">https://github.com/FraunhoferIVI/SSP</a>. </p>
<blockquote>
<p>ä»RGBç›¸æœºè¿›è¡Œè¯­ä¹‰åˆ†å‰²å¯¹è‡ªä¸»é£è¡Œè½¦è¾†çš„æ„ŸçŸ¥è‡³å…³é‡è¦ã€‚é€šè¿‡æ‹æ‘„è§†é¢‘è¿›è¡Œé¢„æµ‹çš„ç¨³å®šæ€§å¯¹å…¶å¯é æ€§è‡³å…³é‡è¦ï¼Œå¹¶ä¸”ç”±æ­¤å¯ä»¥æ‰©å±•åˆ°æ™ºèƒ½ä½“çš„å¯ä¿¡åº¦ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è½»é‡çº§çš„è§†é¢‘è¯­ä¹‰åˆ†å‰²æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€‚åˆåœ¨è½¦è½½å®æ—¶æ¨ç†ä¸­ä½¿ç”¨ï¼Œé€šè¿‡åœ¨å¸§ä¹‹é—´ä¼ æ’­è¯­ä¹‰ç›¸ä¼¼æ€§æ¥å®ç°é«˜ç©ºæ•°æ®çš„é«˜æ—¶é—´ä¸€è‡´æ€§ã€‚SSPé€šè¿‡å…¨å±€æ³¨å†Œå¯¹é½ä¸´æ—¶ä¼ æ’­é«˜æ•ˆå›¾åƒåˆ†å‰²æ¨¡å‹çš„é¢„æµ‹ï¼Œä»¥è¡¥å¿ç›¸æœºç§»åŠ¨ã€‚å®ƒç»“åˆå½“å‰ä¼°è®¡å’Œå…ˆå‰é¢„æµ‹ï¼Œåˆ©ç”¨ä¸¤ä¸ªå¸§çš„ç‰¹å¾ç›¸ä¼¼æ€§è®¡ç®—çš„æƒé‡è¿›è¡Œçº¿æ€§æ’å€¼ã€‚ç”±äºæœ¬é¢†åŸŸçš„æ•°æ®å¯ç”¨æ€§æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œæˆ‘ä»¬é’ˆå¯¹å°‘é‡æ ‡æ³¨çš„æ•°æ®é›†æå‡ºäº†ä¸€ç§ä¸€è‡´æ€§æ„ŸçŸ¥çš„çŸ¥è¯†è’¸é¦è®­ç»ƒç¨‹åºã€‚æˆ‘ä»¬åˆ©ç”¨å¤§å‹å›¾åƒåˆ†å‰²æ¨¡å‹ä½œä¸ºæ•™å¸ˆæ¥è®­ç»ƒé«˜æ•ˆçš„SSPï¼Œå¹¶å……åˆ†åˆ©ç”¨åŒä¸€è®­ç»ƒè§†é¢‘ä¸­æ ‡æ³¨å’Œæœªæ ‡æ³¨å¸§ä¹‹é—´çš„å¼ºç›¸å…³æ€§ï¼Œå¯¹æ‰€æœ‰å¸§è¿›è¡Œé«˜è´¨é‡ç›‘ç£ã€‚KD-SSPåœ¨UAVidå’ŒRuralScapesä¸Šç›¸è¾ƒäºåŸºç¡€å›¾åƒåˆ†å‰²æ¨¡å‹åˆ†åˆ«å®ç°äº†12.5%å’Œ6.7%çš„æ—¶é—´ä¸€è‡´æ€§å¢é•¿ï¼Œå…·æœ‰æ›´é«˜çš„å‡†ç¡®æ€§å’Œç›¸å½“çš„æ¨ç†é€Ÿåº¦ã€‚åœ¨è¿™äº›èˆªç©ºæ•°æ®é›†ä¸­ï¼ŒKD-SSPç›¸è¾ƒäºå…¶ä»–ä¸ºé€šç”¨åº”ç”¨æå‡ºçš„è§†é¢‘æ–¹æ³•æä¾›äº†æ›´ä¼˜è¶Šçš„åˆ†å‰²è´¨é‡å’Œæ¨ç†é€Ÿåº¦ä¹‹é—´çš„æƒè¡¡ï¼Œå¹¶æ˜¾ç¤ºå‡ºæ˜æ˜¾æ›´é«˜çš„ä¸€è‡´æ€§ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://github.com/FraunhoferIVI/SSP">https://github.com/FraunhoferIVI/SSP</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15676v2">PDF</a> Accepted by CVPR2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§è½»é‡çº§çš„è§†é¢‘è¯­ä¹‰åˆ†å‰²æ–¹æ³•ï¼Œé€‚ç”¨äºè‡ªä¸»é£è¡Œè½¦è¾†çš„å®æ—¶æ¨ç†ã€‚è¯¥æ–¹æ³•é€šè¿‡è¯­ä¹‰ç›¸ä¼¼æ€§ä¼ æ’­ï¼ˆSSPï¼‰æŠ€æœ¯å®ç°äº†é«˜æ—¶ç©ºä¸€è‡´æ€§ï¼Œåœ¨èˆªæ‹æ•°æ®ä¸Šå…·æœ‰è‰¯å¥½çš„è¡¨ç°ã€‚åŒæ—¶ï¼Œä¸ºäº†æé«˜åœ¨æ•°æ®ç¨€ç¼ºç¯å¢ƒä¸‹çš„æ¨¡å‹æ€§èƒ½ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§ä¸€è‡´æ€§æ„ŸçŸ¥çš„çŸ¥è¯†è’¸é¦è®­ç»ƒç¨‹åºã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://github.com/FraunhoferIVI/SSP">é“¾æ¥</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­ä¹‰åˆ†å‰²å¯¹äºè‡ªä¸»é£è¡Œè½¦è¾†çš„æ„ŸçŸ¥è‡³å…³é‡è¦ï¼Œé¢„æµ‹çš„ç¨³å®šæ€§å¯¹äºå…¶å¯é æ€§å’Œä¿¡ä»»åº¦è‡³å…³é‡è¦ã€‚</li>
<li>æå‡ºäº†ä¸€ç§è½»é‡çº§çš„è§†é¢‘è¯­ä¹‰åˆ†å‰²æ–¹æ³•ï¼Œé€‚åˆå®æ—¶æ¨ç†åº”ç”¨ï¼Œèƒ½åœ¨èˆªæ‹æ•°æ®ä¸Šå®ç°é«˜æ—¶ç©ºä¸€è‡´æ€§ã€‚</li>
<li>åˆ©ç”¨è¯­ä¹‰ç›¸ä¼¼æ€§ä¼ æ’­ï¼ˆSSPï¼‰æŠ€æœ¯ç»“åˆå…¨å±€æ³¨å†Œå¯¹é½æ¥è¡¥å¿æ‘„åƒå¤´ç§»åŠ¨ï¼Œé€šè¿‡çº¿æ€§æ’å€¼å°†å½“å‰ä¼°è®¡ä¸å…ˆå‰é¢„æµ‹ç»“åˆèµ·æ¥ã€‚</li>
<li>åœ¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹ï¼Œæå‡ºä¸€è‡´æ€§æ„ŸçŸ¥çš„çŸ¥è¯†è’¸é¦è®­ç»ƒç¨‹åºï¼Œä½¿ç”¨å¤§å‹å›¾åƒåˆ†å‰²æ¨¡å‹ä½œä¸ºæ•™å¸ˆæ¨¡å‹è¿›è¡Œè®­ç»ƒã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡åˆ©ç”¨æœ‰æ ‡ç­¾å’Œæ— æ ‡ç­¾å¸§ä¹‹é—´çš„å¼ºç›¸å…³æ€§ï¼Œå¯¹æ‰€æœ‰å¸§è¿›è¡Œé«˜è´¨é‡ç›‘ç£ã€‚</li>
<li>ä¸åŸºç¡€å›¾åƒåˆ†å‰²æ¨¡å‹ç›¸æ¯”ï¼ŒKD-SSPåœ¨UAVidå’ŒRuralScapesæ•°æ®é›†ä¸Šå®ç°äº†æ˜¾è‘—çš„æ—¶ç©ºä¸€è‡´æ€§æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15676">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-42ab3e614312a162b817a580b6e96c8c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ddd5ca422bf113ac596d59061eacf639.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-03dd255d5ab1baded191215e2d22409f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-35b4b7c551a4ced2f8a86045158857ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-529bfb3bdc8fa1145f4ed4d80d7b4ce4.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Rethinking-Cancer-Gene-Identification-through-Graph-Anomaly-Analysis"><a href="#Rethinking-Cancer-Gene-Identification-through-Graph-Anomaly-Analysis" class="headerlink" title="Rethinking Cancer Gene Identification through Graph Anomaly Analysis"></a>Rethinking Cancer Gene Identification through Graph Anomaly Analysis</h2><p><strong>Authors:Yilong Zang, Lingfei Ren, Yue Li, Zhikang Wang, David Antony Selby, Zheng Wang, Sebastian Josef Vollmer, Hongzhi Yin, Jiangning Song, Junhang Wu</strong></p>
<p>Graph neural networks (GNNs) have shown promise in integrating protein-protein interaction (PPI) networks for identifying cancer genes in recent studies. However, due to the insufficient modeling of the biological information in PPI networks, more faithfully depiction of complex protein interaction patterns for cancer genes within the graph structure remains largely unexplored. This study takes a pioneering step toward bridging biological anomalies in protein interactions caused by cancer genes to statistical graph anomaly. We find a unique graph anomaly exhibited by cancer genes, namely weight heterogeneity, which manifests as significantly higher variance in edge weights of cancer gene nodes within the graph. Additionally, from the spectral perspective, we demonstrate that the weight heterogeneity could lead to the â€œflattening outâ€ of spectral energy, with a concentration towards the extremes of the spectrum. Building on these insights, we propose the HIerarchical-Perspective Graph Neural Network (HIPGNN) that not only determines spectral energy distribution variations on the spectral perspective, but also perceives detailed protein interaction context on the spatial perspective. Extensive experiments are conducted on two reprocessed datasets STRINGdb and CPDB, and the experimental results demonstrate the superiority of HIPGNN. </p>
<blockquote>
<p>å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰åœ¨æ•´åˆè›‹ç™½è´¨-è›‹ç™½è´¨ç›¸äº’ä½œç”¨ï¼ˆPPIï¼‰ç½‘ç»œä»¥è¯†åˆ«ç™Œç—‡åŸºå› æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ã€‚ç„¶è€Œï¼Œç”±äºPPIç½‘ç»œä¸­ç”Ÿç‰©ä¿¡æ¯å»ºæ¨¡çš„ä¸è¶³ï¼Œå¯¹äºå›¾å½¢ç»“æ„å†…ç™Œç—‡åŸºå› çš„å¤æ‚è›‹ç™½è´¨ç›¸äº’ä½œç”¨æ¨¡å¼çš„æ›´å¿ å®æè¿°ä»ç„¶æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬ç ”ç©¶è¿ˆå‡ºäº†å°†è›‹ç™½è´¨ç›¸äº’ä½œç”¨ä¸­çš„ç”Ÿç‰©å¼‚å¸¸ä¸ç»Ÿè®¡å›¾å¼‚å¸¸è”ç³»èµ·æ¥çš„ç¬¬ä¸€æ­¥ã€‚æˆ‘ä»¬å‘ç°ç™Œç—‡åŸºå› è¡¨ç°å‡ºä¸€ç§ç‹¬ç‰¹çš„å›¾å½¢å¼‚å¸¸ï¼Œå³æƒé‡å¼‚è´¨æ€§ï¼Œå®ƒè¡¨ç°ä¸ºå›¾å½¢å†…ç™Œç—‡åŸºå› èŠ‚ç‚¹çš„è¾¹æƒé‡æ–¹å·®æ˜¾è‘—è¾ƒé«˜ã€‚æ­¤å¤–ï¼Œä»å…‰è°±è§’åº¦ï¼Œæˆ‘ä»¬è¯æ˜äº†æƒé‡å¼‚è´¨æ€§å¯èƒ½å¯¼è‡´å…‰è°±èƒ½é‡çš„â€œå¹³å¦åŒ–â€ï¼Œå¹¶é›†ä¸­åœ¨å…‰è°±çš„æç«¯éƒ¨åˆ†ã€‚åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†åˆ†å±‚é€è§†å›¾ç¥ç»ç½‘ç»œï¼ˆHIPGNNï¼‰ï¼Œå®ƒä¸ä»…ä»å…‰è°±è§’åº¦ç¡®å®šå…‰è°±èƒ½é‡åˆ†å¸ƒçš„å˜åŒ–ï¼Œè€Œä¸”åœ¨ç©ºé—´è§’åº¦ä¸Šæ„ŸçŸ¥è¯¦ç»†çš„è›‹ç™½è´¨ç›¸äº’ä½œç”¨ä¸Šä¸‹æ–‡ã€‚åœ¨é‡æ–°å¤„ç†è¿‡çš„STRINGdbå’ŒCPDBä¸¤ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼Œå®éªŒç»“æœè¯æ˜äº†HIPGNNçš„ä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17240v2">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶æ¢ç´¢äº†å›¾ç¥ç»ç½‘ç»œåœ¨ç™Œç—‡åŸºå› è›‹ç™½è´¨äº¤äº’ç½‘ç»œä¸­çš„åº”ç”¨ï¼Œå¹¶å‘ç°ç™Œç—‡åŸºå› åœ¨å›¾ä¸­è¡¨ç°å‡ºç‹¬ç‰¹çš„æƒé‡å¼‚è´¨æ€§ç‰¹å¾ã€‚æå‡ºä¸€ç§å±‚æ¬¡è§†è§’å›¾ç¥ç»ç½‘ç»œï¼ˆHIPGNNï¼‰ï¼Œèƒ½åŒæ—¶ä»å…‰è°±å’Œç©ºé—´è§’åº¦æ„ŸçŸ¥è›‹ç™½è´¨äº¤äº’ä¸Šä¸‹æ–‡ï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯äº†å…¶ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾ç¥ç»ç½‘ç»œåœ¨æ•´åˆç™Œç—‡åŸºå› è›‹ç™½è´¨äº¤äº’ç½‘ç»œæ–¹é¢å±•ç°å‡ºæ½œåŠ›ã€‚</li>
<li>ç™Œç—‡åŸºå› åœ¨å›¾ä¸­è¡¨ç°å‡ºç‹¬ç‰¹çš„æƒé‡å¼‚è´¨æ€§ç‰¹å¾ã€‚</li>
<li>æƒé‡å¼‚è´¨æ€§ä¼šå¯¼è‡´å…‰è°±èƒ½é‡çš„â€œæ‰å¹³åŒ–â€ï¼Œå¹¶å‘å…‰è°±æç«¯é›†ä¸­ã€‚</li>
<li>å±‚æ¬¡è§†è§’å›¾ç¥ç»ç½‘ç»œï¼ˆHIPGNNï¼‰èƒ½åŒæ—¶ä»å…‰è°±å’Œç©ºé—´è§’åº¦å¤„ç†æ•°æ®ã€‚</li>
<li>HIPGNNèƒ½ç¡®å®šå…‰è°±èƒ½é‡çš„åˆ†å¸ƒå˜åŒ–ï¼Œå¹¶æ„ŸçŸ¥è›‹ç™½è´¨äº¤äº’çš„è¯¦ç»†ä¸Šä¸‹æ–‡ã€‚</li>
<li>åœ¨ä¸¤ä¸ªé‡æ–°å¤„ç†çš„æ•°æ®é›†STRINGdbå’ŒCPDBä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17240">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-70ae52749a65d0a3a09f22900c702fbb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0c58e77fd7acf097847876fc160b6d30.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2445cf9287c6cfeb059ca66b886b509b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6866a278637c93ed706933b274be6837.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-47cfcc2b4b581f73195e45d1a7a9af24.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="MvKeTR-Chest-CT-Report-Generation-with-Multi-View-Perception-and-Knowledge-Enhancement"><a href="#MvKeTR-Chest-CT-Report-Generation-with-Multi-View-Perception-and-Knowledge-Enhancement" class="headerlink" title="MvKeTR: Chest CT Report Generation with Multi-View Perception and   Knowledge Enhancement"></a>MvKeTR: Chest CT Report Generation with Multi-View Perception and   Knowledge Enhancement</h2><p><strong>Authors:Xiwei Deng, Xianchun He, Jianfeng Bao, Yudan Zhou, Shuhui Cai, Congbo Cai, Zhong Chen</strong></p>
<p>CT report generation (CTRG) aims to automatically generate diagnostic reports for 3D volumes, relieving cliniciansâ€™ workload and improving patient care. Despite clinical value, existing works fail to effectively incorporate diagnostic information from multiple anatomical views and lack related clinical expertise essential for accurate and reliable diagnosis. To resolve these limitations, we propose a novel Multi-view perception Knowledge-enhanced TansfoRmer (MvKeTR) to mimic the diagnostic workflow of clinicians. Just as radiologists first examine CT scans from multiple planes, a Multi-View Perception Aggregator (MVPA) with view-aware attention is proposed to synthesize diagnostic information from multiple anatomical views effectively. Then, inspired by how radiologists further refer to relevant clinical records to guide diagnostic decision-making, a Cross-Modal Knowledge Enhancer (CMKE) is devised to retrieve the most similar reports based on the query volume to incorporate domain knowledge into the diagnosis procedure. Furthermore, instead of traditional MLPs, we employ Kolmogorov-Arnold Networks (KANs) as the fundamental building blocks of both modules, which exhibit superior parameter efficiency and reduced spectral bias to better capture high-frequency components critical for CT interpretation while mitigating overfitting. Extensive experiments on the public CTRG-Chest-548 K dataset demonstrate that our method outpaces prior state-of-the-art (SOTA) models across almost all metrics. The code is available at <a target="_blank" rel="noopener" href="https://github.com/xiweideng/MvKeTR">https://github.com/xiweideng/MvKeTR</a>. </p>
<blockquote>
<p>CTæŠ¥å‘Šç”Ÿæˆï¼ˆCTRGï¼‰æ—¨åœ¨è‡ªåŠ¨ä¸ºä¸‰ç»´ä½“ç§¯ç”Ÿæˆè¯Šæ–­æŠ¥å‘Šï¼Œå‡è½»ä¸´åºŠåŒ»ç”Ÿçš„å·¥ä½œé‡ï¼Œæé«˜æ‚£è€…æŠ¤ç†çš„è´¨é‡ã€‚å°½ç®¡å…·æœ‰ä¸´åºŠä»·å€¼ï¼Œä½†ç°æœ‰å·¥ä½œæœªèƒ½æœ‰æ•ˆåœ°ç»“åˆå¤šä¸ªè§£å‰–è§†è§’çš„è¯Šæ–­ä¿¡æ¯ï¼Œå¹¶ç¼ºä¹å‡†ç¡®å¯é è¯Šæ–­æ‰€éœ€çš„ç›¸å…³ä¸´åºŠç»éªŒã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å¤šè§†è§’æ„ŸçŸ¥çŸ¥è¯†å¢å¼ºè½¬æ¢å™¨ï¼ˆMvKeTRï¼‰æ¥æ¨¡æ‹Ÿä¸´åºŠåŒ»ç”Ÿçš„è¯Šæ–­å·¥ä½œæµç¨‹ã€‚å°±åƒæ”¾å°„ç§‘åŒ»ç”Ÿé¦–å…ˆä¼šä»å¤šä¸ªå¹³é¢æ£€æŸ¥CTæ‰«æä¸€æ ·ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå…·æœ‰è§†å›¾æ„ŸçŸ¥æ³¨æ„åŠ›çš„å¤šè§†å›¾æ„ŸçŸ¥èšåˆå™¨ï¼ˆMVPAï¼‰ï¼Œä»¥æœ‰æ•ˆåœ°åˆæˆå¤šä¸ªè§£å‰–è§†è§’çš„è¯Šæ–­ä¿¡æ¯ã€‚æ¥ç€ï¼Œå—æ”¾å°„ç§‘åŒ»ç”Ÿå¦‚ä½•è¿›ä¸€æ­¥å‚è€ƒç›¸å…³ç—…å†ä»¥æŒ‡å¯¼è¯Šæ–­å†³ç­–è¿‡ç¨‹çš„å¯å‘ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªè·¨æ¨¡æ€çŸ¥è¯†å¢å¼ºå™¨ï¼ˆCMKEï¼‰ï¼Œä»¥æ ¹æ®æŸ¥è¯¢ä½“ç§¯æ£€ç´¢æœ€ç›¸ä¼¼çš„æŠ¥å‘Šï¼Œå°†é¢†åŸŸçŸ¥è¯†èå…¥è¯Šæ–­è¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œä¸ä¼ ç»Ÿçš„å¤šå±‚æ„ŸçŸ¥å™¨ä¸åŒï¼Œæˆ‘ä»¬é‡‡ç”¨Kolmogorov-Arnoldç½‘ç»œï¼ˆKANsï¼‰ä½œä¸ºä¸¤ä¸ªæ¨¡å—çš„åŸºæœ¬æ„å»ºå—ï¼Œå®ƒä»¬åœ¨å‚æ•°æ•ˆç‡æ–¹é¢è¡¨ç°å‡ºä¼˜åŠ¿ï¼Œå¹¶é™ä½äº†å…‰è°±åå·®ï¼Œä»¥æ›´å¥½åœ°æ•æ‰å¯¹CTè§£é‡Šè‡³å…³é‡è¦çš„é«˜é¢‘æˆåˆ†ï¼ŒåŒæ—¶å‡è½»è¿‡æ‹Ÿåˆã€‚åœ¨å…¬å…±CTRG-Chest-548Kæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å‡ ä¹æ‰€æœ‰æŒ‡æ ‡ä¸Šéƒ½è¶…è¿‡äº†æœ€æ–°çš„å…ˆè¿›æŠ€æœ¯æ¨¡å‹ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/xiweideng/MvKeTR%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/xiweideng/MvKeTRä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.18309v3">PDF</a> Accepted for publication in IEEE Journal of Biomedical and Health   Informatics</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºMvKeTRçš„å¤šè§†è§’æ„ŸçŸ¥çŸ¥è¯†å¢å¼ºè½¬æ¢å™¨ï¼Œæ—¨åœ¨æ¨¡ä»¿åŒ»ç”Ÿçš„è¯Šæ–­æµç¨‹ï¼Œè‡ªåŠ¨ç”Ÿæˆä¸‰ç»´ä½“ç§¯çš„è¯Šæ–­æŠ¥å‘Šã€‚é€šè¿‡ä½¿ç”¨å¤šè§†è§’æ„ŸçŸ¥èšåˆå™¨å’Œè·¨æ¨¡æ€çŸ¥è¯†å¢å¼ºå™¨ï¼Œè¯¥æ–¹æ³•èƒ½æœ‰æ•ˆèåˆå¤šè§†è§’è¯Šæ–­ä¿¡æ¯å¹¶èå…¥é¢†åŸŸçŸ¥è¯†ï¼Œæé«˜è¯Šæ–­çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚é‡‡ç”¨Kolmogorov-Arnoldç½‘ç»œä½œä¸ºæ¨¡å—çš„åŸºæœ¬æ„å»ºå—ï¼Œä»¥æé«˜å‚æ•°æ•ˆç‡å’Œé™ä½å…‰è°±åå·®ï¼Œæ›´å¥½åœ°æ•æ‰CTè§£è¯»ä¸­çš„é«˜é¢‘æˆåˆ†ã€‚åœ¨å…¬å…±CTRG-Chest-548Kæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å‡ ä¹æ‰€æœ‰æŒ‡æ ‡ä¸Šéƒ½è¶…è¿‡äº†ç°æœ‰å…ˆè¿›æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>CTæŠ¥å‘Šç”Ÿæˆ(CTRG)æ—¨åœ¨è‡ªåŠ¨ä¸º3Dä½“ç§¯ç”Ÿæˆè¯Šæ–­æŠ¥å‘Šï¼Œå‡è½»åŒ»ç”Ÿå·¥ä½œé‡ï¼Œæé«˜æ‚£è€…æŠ¤ç†è´¨é‡ã€‚</li>
<li>ç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆèåˆå¤šè§†è§’è¯Šæ–­ä¿¡æ¯ï¼Œç¼ºä¹ä¸´åºŠä¸“ä¸šçŸ¥è¯†ï¼Œå½±å“è¯Šæ–­å’Œå¯é æ€§ã€‚</li>
<li>MvKeTRæ¨¡å‹é€šè¿‡æ¨¡ä»¿åŒ»ç”Ÿè¯Šæ–­æµç¨‹ï¼Œä½¿ç”¨å¤šè§†è§’æ„ŸçŸ¥èšåˆå™¨å’Œè·¨æ¨¡æ€çŸ¥è¯†å¢å¼ºå™¨è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>å¤šè§†è§’æ„ŸçŸ¥èšåˆå™¨(MVPA)å…·æœ‰è§†å›¾æ„ŸçŸ¥æ³¨æ„åŠ›æœºåˆ¶ï¼Œèƒ½æœ‰æ•ˆåˆæˆå¤šè§†è§’è¯Šæ–­ä¿¡æ¯ã€‚</li>
<li>è·¨æ¨¡æ€çŸ¥è¯†å¢å¼ºå™¨(CMKE)é€šè¿‡æ£€ç´¢æœ€ç›¸ä¼¼çš„æŠ¥å‘Šï¼Œå°†é¢†åŸŸçŸ¥è¯†èå…¥è¯Šæ–­è¿‡ç¨‹ã€‚</li>
<li>é‡‡ç”¨Kolmogorov-Arnoldç½‘ç»œ(KANs)ä½œä¸ºæ¨¡å—åŸºæœ¬æ„å»ºå—ï¼Œæé«˜å‚æ•°æ•ˆç‡å’Œé™ä½å…‰è°±åå·®ï¼Œæ›´å¥½åœ°æ•æ‰CTè§£è¯»é«˜é¢‘æˆåˆ†ã€‚</li>
<li>åœ¨å…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMvKeTRæ¨¡å‹åœ¨å„é¡¹æŒ‡æ ‡ä¸Šè¶…è¶Šç°æœ‰å…ˆè¿›æŠ€æœ¯ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.18309">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d46942c519652719992c9e5bbb2b6c51.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-27dd0ff547ed55e987a543dacc94997b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7c3df29c231594b815d03d91928a4506.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="USP-Gaussian-Unifying-Spike-based-Image-Reconstruction-Pose-Correction-and-Gaussian-Splatting"><a href="#USP-Gaussian-Unifying-Spike-based-Image-Reconstruction-Pose-Correction-and-Gaussian-Splatting" class="headerlink" title="USP-Gaussian: Unifying Spike-based Image Reconstruction, Pose Correction   and Gaussian Splatting"></a>USP-Gaussian: Unifying Spike-based Image Reconstruction, Pose Correction   and Gaussian Splatting</h2><p><strong>Authors:Kang Chen, Jiyuan Zhang, Zecheng Hao, Yajing Zheng, Tiejun Huang, Zhaofei Yu</strong></p>
<p>Spike cameras, as an innovative neuromorphic camera that captures scenes with the 0-1 bit stream at 40 kHz, are increasingly employed for the 3D reconstruction task via Neural Radiance Fields (NeRF) or 3D Gaussian Splatting (3DGS). Previous spike-based 3D reconstruction approaches often employ a casecased pipeline: starting with high-quality image reconstruction from spike streams based on established spike-to-image reconstruction algorithms, then progressing to camera pose estimation and 3D reconstruction. However, this cascaded approach suffers from substantial cumulative errors, where quality limitations of initial image reconstructions negatively impact pose estimation, ultimately degrading the fidelity of the 3D reconstruction. To address these issues, we propose a synergistic optimization framework, \textbf{USP-Gaussian}, that unifies spike-based image reconstruction, pose correction, and Gaussian splatting into an end-to-end framework. Leveraging the multi-view consistency afforded by 3DGS and the motion capture capability of the spike camera, our framework enables a joint iterative optimization that seamlessly integrates information between the spike-to-image network and 3DGS. Experiments on synthetic datasets with accurate poses demonstrate that our method surpasses previous approaches by effectively eliminating cascading errors. Moreover, we integrate pose optimization to achieve robust 3D reconstruction in real-world scenarios with inaccurate initial poses, outperforming alternative methods by effectively reducing noise and preserving fine texture details. Our code, data and trained models will be available at <a target="_blank" rel="noopener" href="https://github.com/chenkang455/USP-Gaussian">https://github.com/chenkang455/USP-Gaussian</a>. </p>
<blockquote>
<p>ç¥ç»å½¢æ€ç›¸æœºï¼ˆSpike camerasï¼‰æ˜¯ä¸€ç§åˆ›æ–°å‹çš„ç¥ç»å½¢æ€æ‘„åƒå¤´ï¼Œèƒ½å¤Ÿä»¥æ¯ç§’40åƒèµ«çš„é€Ÿåº¦æ•æ‰åœºæ™¯çš„é›¶åˆ°ä¸€æ¯”ç‰¹æµã€‚å®ƒä»¬æ­£è¶Šæ¥è¶Šå¤šåœ°è¢«ç”¨äºé€šè¿‡ç¥ç»ç½‘ç»œè¾å°„åœºï¼ˆNeRFï¼‰æˆ–ä¸‰ç»´é«˜æ–¯æ¶‚é¸¦ï¼ˆ3DGSï¼‰æ‰§è¡Œä¸‰ç»´é‡å»ºä»»åŠ¡ã€‚ä»¥å‰åŸºäºè„‰å†²çš„ä¸‰ç»´é‡å»ºæ–¹æ³•é€šå¸¸é‡‡ç”¨çº§è”ç®¡é“ï¼šé¦–å…ˆä½¿ç”¨åŸºäºç°æœ‰è„‰å†²åˆ°å›¾åƒé‡å»ºç®—æ³•çš„è„‰å†²æµé«˜è´¨é‡å›¾åƒé‡å»ºï¼Œç„¶åè¿›è¡Œç›¸æœºå§¿æ€ä¼°è®¡å’Œä¸‰ç»´é‡å»ºã€‚ç„¶è€Œï¼Œè¿™ç§çº§è”æ–¹æ³•å­˜åœ¨å¤§é‡ç´¯ç§¯è¯¯å·®é—®é¢˜ï¼Œåˆå§‹å›¾åƒé‡å»ºçš„è´¨é‡é™åˆ¶ä¼šå¯¹å§¿æ€ä¼°è®¡äº§ç”Ÿè´Ÿé¢å½±å“ï¼Œæœ€ç»ˆé™ä½ä¸‰ç»´é‡å»ºçš„ä¿çœŸåº¦ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ååŒä¼˜åŒ–æ¡†æ¶USP-Gaussianï¼Œè¯¥æ¡†æ¶å°†åŸºäºè„‰å†²çš„å›¾åƒé‡å»ºã€å§¿æ€æ ¡æ­£å’Œé«˜æ–¯æ¶‚é¸¦ç»Ÿä¸€åˆ°ä¸€ä¸ªç«¯åˆ°ç«¯çš„æ¡†æ¶ä¸­ã€‚åˆ©ç”¨ä¸‰ç»´é«˜æ–¯æ¶‚é¸¦çš„å¤šè§†è§’ä¸€è‡´æ€§ä»¥åŠè„‰å†²ç›¸æœºçš„è¿åŠ¨æ•æ‰èƒ½åŠ›ï¼Œæˆ‘ä»¬çš„æ¡†æ¶èƒ½å¤Ÿå®ç°è”åˆè¿­ä»£ä¼˜åŒ–ï¼Œæ— ç¼é›†æˆè„‰å†²åˆ°å›¾åƒç½‘ç»œå’Œä¸‰ç»´é«˜æ–¯æ¶‚é¸¦ä¹‹é—´çš„ä¿¡æ¯ã€‚åœ¨å…·æœ‰å‡†ç¡®å§¿æ€çš„åˆæˆæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡æœ‰æ•ˆæ¶ˆé™¤çº§è”è¯¯å·®è¶…è¶Šäº†ä»¥å‰çš„æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é›†æˆäº†å§¿æ€ä¼˜åŒ–ä»¥å®ç°é²æ£’çš„ä¸‰ç»´é‡å»ºï¼Œåœ¨å…·æœ‰ä¸å‡†ç¡®åˆå§‹å§¿æ€çš„ç°å®ä¸–ç•Œåœºæ™¯ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œé€šè¿‡æœ‰æ•ˆé™ä½å™ªå£°å¹¶ä¿ç•™ç²¾ç»†çº¹ç†ç»†èŠ‚è€Œä¼˜äºå…¶ä»–æ–¹æ³•ã€‚æˆ‘ä»¬çš„ä»£ç ã€æ•°æ®å’Œè®­ç»ƒæ¨¡å‹å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/chenkang455/USP-Gaussian%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/chenkang455/USP-Gaussianä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.10504v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰å’Œä¸‰ç»´é«˜æ–¯æ‰©å±•ï¼ˆ3DGSï¼‰æŠ€æœ¯çš„è„‰å†²ç›¸æœº3Dé‡å»ºæ–°æ–¹æ³•â€”â€”USP-GaussianååŒä¼˜åŒ–æ¡†æ¶ã€‚é’ˆå¯¹ä¼ ç»Ÿè„‰å†²ç›¸æœºä¸‰ç»´é‡å»ºè¿‡ç¨‹ä¸­çš„çº§è”è¯¯å·®é—®é¢˜ï¼ŒUSP-Gaussianå®ç°äº†è„‰å†²ç›¸æœºå›¾åƒé‡å»ºã€å§¿æ€æ ¡æ­£å’Œé«˜æ–¯æ‰©å±•çš„ç«¯åˆ°ç«¯é›†æˆä¼˜åŒ–ï¼Œæé«˜äº†é‡å»ºç²¾åº¦å’Œé²æ£’æ€§ã€‚é€šè¿‡åˆ©ç”¨å¤šè§†è§’ä¸€è‡´æ€§å’Œè„‰å†²ç›¸æœºçš„è¿åŠ¨æ•æ‰èƒ½åŠ›ï¼ŒUSP-Gaussianåœ¨åˆæˆæ•°æ®é›†å’ŒçœŸå®ä¸–ç•Œåœºæ™¯ä¸­å‡å±•ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½è¡¨ç°ã€‚ä»£ç å’Œæ¨¡å‹å·²å…¬å¼€äºGitHubã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯åŸºäºæ–‡æœ¬çš„å…³é”®è¦ç‚¹æ€»ç»“ï¼š</p>
<ol>
<li>Spikeç›¸æœºé€šè¿‡æ•æ‰åœºæ™¯ä¸­çš„0-1ä½æµå®ç°åœºæ™¯é‡å»ºï¼Œé¢‘ç‡é«˜è¾¾40 kHzã€‚</li>
<li>ä¼ ç»Ÿè„‰å†²ç›¸æœºä¸‰ç»´é‡å»ºæ–¹æ³•é‡‡ç”¨çº§è”å¤„ç†æµç¨‹ï¼Œå­˜åœ¨ç´¯ç§¯è¯¯å·®é—®é¢˜ã€‚</li>
<li>USP-Gaussianæ¡†æ¶é€šè¿‡æ•´åˆè„‰å†²ç›¸æœºå›¾åƒé‡å»ºã€å§¿æ€æ ¡æ­£å’Œé«˜æ–¯æ‰©å±•æŠ€æœ¯ï¼Œå®ç°äº†ç«¯åˆ°ç«¯çš„ååŒä¼˜åŒ–ã€‚</li>
<li>å¤šè§†è§’ä¸€è‡´æ€§å’Œè„‰å†²ç›¸æœºçš„è¿åŠ¨æ•æ‰èƒ½åŠ›ä¸ºUSP-Gaussianæä¾›äº†å¼ºå¤§æ”¯æŒã€‚</li>
<li>USP-Gaussianåœ¨åˆæˆæ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†å…¶æ¶ˆé™¤çº§è”è¯¯å·®çš„èƒ½åŠ›ã€‚</li>
<li>åœ¨çœŸå®ä¸–ç•Œåœºæ™¯ä¸­ï¼ŒUSP-Gaussiané€šè¿‡é›†æˆå§¿æ€ä¼˜åŒ–å®ç°äº†ç¨³å¥çš„3Dé‡å»ºï¼Œæœ‰æ•ˆå‡å°‘å™ªå£°å¹¶ä¿ç•™ç²¾ç»†çº¹ç†ç»†èŠ‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.10504">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8dad278ac8dc0e654af2cb86b06a7654.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3c65beb1820a635ad35c37d583031357.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ef8cfe3564b80dec156f0766a1392782.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dea501765c8ec401bc15e70da74da758.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-925988d9a8e9a04bbd8dbdbcd1533ac2.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Visual-Prompt-Engineering-for-Vision-Language-Models-in-Radiology"><a href="#Visual-Prompt-Engineering-for-Vision-Language-Models-in-Radiology" class="headerlink" title="Visual Prompt Engineering for Vision Language Models in Radiology"></a>Visual Prompt Engineering for Vision Language Models in Radiology</h2><p><strong>Authors:Stefan Denner, Markus Bujotzek, Dimitrios Bounias, David Zimmerer, Raphael Stock, Klaus Maier-Hein</strong></p>
<p>Medical image classification plays a crucial role in clinical decision-making, yet most models are constrained to a fixed set of predefined classes, limiting their adaptability to new conditions. Contrastive Language-Image Pretraining (CLIP) offers a promising solution by enabling zero-shot classification through multimodal large-scale pretraining. However, while CLIP effectively captures global image content, radiology requires a more localized focus on specific pathology regions to enhance both interpretability and diagnostic accuracy. To address this, we explore the potential of incorporating visual cues into zero-shot classification, embedding visual markers, such as arrows, bounding boxes, and circles, directly into radiological images to guide model attention. Evaluating across four public chest X-ray datasets, we demonstrate that visual markers improve AUROC by up to 0.185, highlighting their effectiveness in enhancing classification performance. Furthermore, attention map analysis confirms that visual cues help models focus on clinically relevant areas, leading to more interpretable predictions.To support further research, we use public datasets and provide our codebase and preprocessing pipeline under <a target="_blank" rel="noopener" href="https://github.com/MIC-DKFZ/VPE-in-Radiology">https://github.com/MIC-DKFZ/VPE-in-Radiology</a>, serving as a reference point for future work on localized classification in medical imaging. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†ç±»åœ¨ä¸´åºŠå†³ç­–ä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œç„¶è€Œå¤§å¤šæ•°æ¨¡å‹éƒ½å±€é™äºä¸€ç»„é¢„å®šä¹‰çš„ç±»åˆ«ï¼Œé™åˆ¶äº†å®ƒä»¬å¯¹æ–°æ¡ä»¶çš„é€‚åº”æ€§ã€‚å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰é€šè¿‡å¤šæ¨¡å¼å¤§è§„æ¨¡é¢„è®­ç»ƒæä¾›äº†é›¶æ ·æœ¬åˆ†ç±»çš„æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œè™½ç„¶CLIPèƒ½å¤Ÿæœ‰æ•ˆåœ°æ•è·å…¨å±€å›¾åƒå†…å®¹ï¼Œä½†æ”¾å°„å­¦éœ€è¦æ›´ä¾§é‡äºç‰¹å®šçš„ç—…ç†åŒºåŸŸï¼Œä»¥æé«˜å¯è§£é‡Šæ€§å’Œè¯Šæ–­å‡†ç¡®æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¢ç´¢å°†è§†è§‰çº¿ç´¢èå…¥é›¶æ ·æœ¬åˆ†ç±»çš„æ½œåŠ›ï¼Œç›´æ¥åœ¨æ”¾å°„å›¾åƒä¸­åµŒå…¥è§†è§‰æ ‡è®°ï¼Œå¦‚ç®­å¤´ã€è¾¹ç•Œæ¡†å’Œåœ†åœˆï¼Œä»¥å¼•å¯¼æ¨¡å‹æ³¨æ„åŠ›ã€‚åœ¨å››ä¸ªå…¬å…±èƒ¸éƒ¨Xå°„çº¿æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œè§†è§‰æ ‡è®°æé«˜äº†AUROCå€¼è¾¾0.185ï¼Œè¯æ˜äº†å®ƒä»¬åœ¨æé«˜åˆ†ç±»æ€§èƒ½æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œæ³¨æ„åŠ›å›¾åˆ†æè¯å®ï¼Œè§†è§‰çº¿ç´¢æœ‰åŠ©äºæ¨¡å‹å…³æ³¨ä¸´åºŠç›¸å…³åŒºåŸŸï¼Œä»è€Œåšå‡ºæ›´å¯é¢„æµ‹çš„è§£é‡Šã€‚ä¸ºäº†æ”¯æŒè¿›ä¸€æ­¥ç ”ç©¶ï¼Œæˆ‘ä»¬ä½¿ç”¨å…¬å…±æ•°æ®é›†ï¼Œå¹¶åœ¨<a target="_blank" rel="noopener" href="https://github.com/MIC-DKFZ/VPE-in-Radiology%E4%B8%8A%E6%8F%90%E4%BE%9B%E6%88%91%E4%BB%AC%E7%9A%84%E4%BB%A3%E7%A0%81%E5%BA%93%E5%92%8C%E9%A2%84%E5%A4%84%E7%90%86%E7%AE%A1%E9%81%93%EF%BC%8C%E4%BD%9C%E4%B8%BA%E6%9C%AA%E6%9D%A5%E5%8C%BB%E5%AD%A6%E6%88%90%E5%83%8F%E4%B8%AD%E5%B1%80%E9%83%A8%E5%8C%96%E5%88%86%E7%B1%BB%E7%A0%94%E7%A9%B6%E7%9A%84%E4%B8%80%E4%B8%AA%E5%8F%82%E8%80%83%E7%82%B9%E3%80%82">https://github.com/MIC-DKFZ/VPE-in-Radiologyä¸Šæä¾›æˆ‘ä»¬çš„ä»£ç åº“å’Œé¢„å¤„ç†ç®¡é“ï¼Œä½œä¸ºæœªæ¥åŒ»å­¦æˆåƒä¸­å±€éƒ¨åŒ–åˆ†ç±»ç ”ç©¶çš„ä¸€ä¸ªå‚è€ƒç‚¹ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.15802v3">PDF</a> Accepted at ECCV 2024 Workshop on Emergent Visual Abilities and   Limits of Foundation Models &amp; Medical Imaging with Deep Learning 2025</p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒåˆ†ç±»åœ¨ä¸´åºŠå†³ç­–ä¸­è‡³å…³é‡è¦ï¼Œä½†å¤§å¤šæ•°æ¨¡å‹å—é™äºå›ºå®šçš„é¢„å®šä¹‰ç±»åˆ«ï¼Œéš¾ä»¥é€‚åº”æ–°æƒ…å†µã€‚å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰é€šè¿‡å¤šæ¨¡æ€å¤§è§„æ¨¡é¢„è®­ç»ƒå®ç°é›¶æ ·æœ¬åˆ†ç±»ï¼Œä¸ºè§£å†³æ­¤é—®é¢˜æä¾›äº†å¸Œæœ›ã€‚ç„¶è€Œï¼ŒCLIPè™½ç„¶èƒ½æ•æ‰å›¾åƒå…¨å±€å†…å®¹ï¼Œä½†æ”¾å°„å­¦éœ€è¦æ›´ä¸“æ³¨äºç‰¹å®šç—…ç†åŒºåŸŸä»¥æé«˜è§£é‡Šå’Œè¯Šæ–­å‡†ç¡®æ€§ã€‚æœ¬ç ”ç©¶æ¢ç´¢å°†è§†è§‰çº¿ç´¢èå…¥é›¶æ ·æœ¬åˆ†ç±»çš„æ½œåŠ›ï¼Œé€šè¿‡åœ¨æ”¾å°„å›¾åƒä¸­åµŒå…¥è§†è§‰æ ‡è®°ï¼ˆå¦‚ç®­å¤´ã€è¾¹ç•Œæ¡†å’Œåœ†åœˆï¼‰æ¥å¼•å¯¼æ¨¡å‹æ³¨æ„åŠ›ã€‚åœ¨å››ä¸ªå…¬å…±èƒ¸éƒ¨Xå°„çº¿æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œè§†è§‰æ ‡è®°æé«˜äº†AUROCè¾¾0.185ï¼ŒéªŒè¯äº†å…¶åœ¨æé«˜åˆ†ç±»æ€§èƒ½æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œæ³¨æ„åŠ›å›¾åˆ†æè¯å®è§†è§‰çº¿ç´¢æœ‰åŠ©äºæ¨¡å‹å…³æ³¨ä¸´åºŠç›¸å…³åŒºåŸŸï¼Œäº§ç”Ÿæ›´å¯è§£é‡Šçš„é¢„æµ‹ã€‚ç›¸å…³ä»£ç å’Œé¢„å¤„ç†ç®¡é“å¯é€šè¿‡é“¾æ¥<a target="_blank" rel="noopener" href="https://github.com/MIC-DKFZ/VPE-in-Radiology%E8%8E%B7%E5%8F%96%EF%BC%8C%E4%B8%BA%E5%8C%BB%E5%AD%A6%E6%88%90%E5%83%8F%E4%B8%AD%E7%9A%84%E5%B1%80%E9%83%A8%E5%8C%96%E5%88%86%E7%B1%BB%E6%8F%90%E4%BE%9B%E6%9C%AA%E6%9D%A5%E7%A0%94%E7%A9%B6%E7%9A%84%E5%8F%82%E8%80%83%E7%82%B9%E3%80%82">https://github.com/MIC-DKFZ/VPE-in-Radiologyè·å–ï¼Œä¸ºåŒ»å­¦æˆåƒä¸­çš„å±€éƒ¨åŒ–åˆ†ç±»æä¾›æœªæ¥ç ”ç©¶çš„å‚è€ƒç‚¹ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†ç±»åœ¨ä¸´åºŠå†³ç­–ä¸­å…·æœ‰é‡è¦ä½œç”¨ï¼Œä½†æ¨¡å‹å—é™äºé¢„å®šä¹‰ç±»åˆ«ï¼Œéœ€è¦æé«˜é€‚åº”æ€§ã€‚</li>
<li>å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰æä¾›é›¶æ ·æœ¬åˆ†ç±»è§£å†³æ–¹æ¡ˆï¼Œä½†å…¨å±€åˆ†ç±»åœ¨æ”¾å°„å­¦ä¸­éœ€æ›´å…³æ³¨ç‰¹å®šåŒºåŸŸã€‚</li>
<li>èå…¥è§†è§‰çº¿ç´¢ï¼ˆå¦‚ç®­å¤´ã€è¾¹ç•Œæ¡†å’Œåœ†åœˆï¼‰å¯æœ‰æ•ˆæé«˜æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒä¸Šçš„åˆ†ç±»æ€§èƒ½ã€‚</li>
<li>åœ¨å››ä¸ªå…¬å…±èƒ¸éƒ¨Xå°„çº¿æ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œä½¿ç”¨è§†è§‰æ ‡è®°æé«˜äº†AUROCå€¼ã€‚</li>
<li>æ³¨æ„åŠ›å›¾åˆ†æè¯å®è§†è§‰çº¿ç´¢æœ‰åŠ©äºæ¨¡å‹å…³æ³¨ä¸´åºŠç›¸å…³åŒºåŸŸï¼Œæå‡é¢„æµ‹çš„å¯è§£é‡Šæ€§ã€‚</li>
<li>ç ”ç©¶è€…æä¾›äº†ä»£ç åº“å’Œé¢„å¤„ç†ç®¡é“ï¼Œä¾¿äºæœªæ¥å¯¹åŒ»å­¦æˆåƒå±€éƒ¨åŒ–åˆ†ç±»çš„ç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.15802">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-86023a0bf024a6223605a8bd55a3c47d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e3f1c454639fc5ef39e86063ccd055c6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4770f82eed916e3e506ae4966491a79d.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="FusionSAM-Visual-Multi-Modal-Learning-with-Segment-Anything"><a href="#FusionSAM-Visual-Multi-Modal-Learning-with-Segment-Anything" class="headerlink" title="FusionSAM: Visual Multi-Modal Learning with Segment Anything"></a>FusionSAM: Visual Multi-Modal Learning with Segment Anything</h2><p><strong>Authors:Daixun Li, Weiying Xie, Mingxiang Cao, Yunke Wang, Yusi Zhang, Leyuan Fang, Yunsong Li, Chang Xu</strong></p>
<p>Multimodal image fusion and semantic segmentation are critical for autonomous driving. Despite advancements, current models often struggle with segmenting densely packed elements due to a lack of comprehensive fusion features for guidance during training. While the Segment Anything Model (SAM) allows precise control during fine-tuning through its flexible prompting encoder, its potential remains largely unexplored in the context of multimodal segmentation for natural images. In this paper, we introduce SAM into multimodal image segmentation for the first time, proposing a novel framework that combines Latent Space Token Generation (LSTG) and Fusion Mask Prompting (FMP) modules. This approach transforms the training methodology for multimodal segmentation from a traditional black-box approach to a controllable, prompt-based mechanism. Specifically, we obtain latent space features for both modalities through vector quantization and embed them into a cross-attention-based inter-domain fusion module to establish long-range dependencies between modalities. We then use these comprehensive fusion features as prompts to guide precise pixel-level segmentation. Extensive experiments on multiple public datasets demonstrate that our method significantly outperforms SAM and SAM2 in multimodal autonomous driving scenarios, achieving an average improvement of 4.1$%$ over the state-of-the-art method in segmentation mIoU, and the performance is also optimized in other multi-modal visual scenes. </p>
<blockquote>
<p>å¤šæ¨¡æ€å›¾åƒèåˆå’Œè¯­ä¹‰åˆ†å‰²å¯¹äºè‡ªåŠ¨é©¾é©¶è‡³å…³é‡è¦ã€‚å°½ç®¡æœ‰æ‰€è¿›å±•ï¼Œä½†å½“å‰æ¨¡å‹åœ¨åˆ†å‰²å¯†é›†å…ƒç´ æ—¶ç»å¸¸é‡åˆ°å›°éš¾ï¼Œå› ä¸ºåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç¼ºä¹å…¨é¢çš„èåˆç‰¹å¾ä½œä¸ºæŒ‡å¯¼ã€‚è™½ç„¶Segment Anything Modelï¼ˆSAMï¼‰é€šè¿‡å…¶çµæ´»çš„æç¤ºç¼–ç å™¨å…è®¸å¾®è°ƒè¿‡ç¨‹ä¸­çš„ç²¾ç¡®æ§åˆ¶ï¼Œä½†åœ¨è‡ªç„¶å›¾åƒçš„å¤šæ¨¡æ€åˆ†å‰²èƒŒæ™¯ä¸‹ï¼Œå…¶åœ¨è¯¥é¢†åŸŸçš„æ½œåŠ›å°šæœªå¾—åˆ°å¹¿æ³›æ¢ç´¢ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é¦–æ¬¡å°†SAMå¼•å…¥å¤šæ¨¡æ€å›¾åƒåˆ†å‰²ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆæ½œåœ¨ç©ºé—´ä»¤ç‰Œç”Ÿæˆï¼ˆLatent Space Token Generationï¼ŒLSTGï¼‰å’Œèåˆæ©ç æç¤ºï¼ˆFusion Mask Promptingï¼ŒFMPï¼‰æ¨¡å—çš„æ–°å‹æ¡†æ¶ã€‚è¯¥æ–¹æ³•å°†å¤šæ¨¡æ€åˆ†å‰²çš„è®­ç»ƒæ–¹æ³•ä»ä¼ ç»Ÿé»‘ç®±æ–¹æ³•è½¬å˜ä¸ºå¯æ§çš„åŸºäºæç¤ºçš„æœºåˆ¶ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é€šè¿‡å‘é‡é‡åŒ–è·å¾—ä¸¤ç§æ¨¡æ€çš„æ½œåœ¨ç©ºé—´ç‰¹å¾ï¼Œå¹¶å°†å…¶åµŒå…¥åŸºäºäº¤å‰æ³¨æ„åŠ›çš„è·¨åŸŸèåˆæ¨¡å—ä¸­ï¼Œä»¥åœ¨æ¨¡æ€ä¹‹é—´å»ºç«‹è¿œç¨‹ä¾èµ–å…³ç³»ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†è¿™äº›å…¨é¢çš„èåˆç‰¹å¾ä½œä¸ºæç¤ºæ¥æŒ‡å¯¼ç²¾ç¡®çš„åƒç´ çº§åˆ†å‰²ã€‚åœ¨å¤šä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šç§æ¨¡æ€çš„è‡ªåŠ¨é©¾é©¶åœºæ™¯ä¸­æ˜¾è‘—ä¼˜äºSAMå’ŒSAM2ï¼Œåœ¨åˆ†å‰²mIoUæ–¹é¢æ¯”æœ€æ–°æŠ€æœ¯å¹³å‡æé«˜äº†4.1%ï¼Œå¹¶ä¸”åœ¨å…¶ä»–å¤šæ¨¡æ€è§†è§‰åœºæ™¯ä¸­çš„æ€§èƒ½ä¹Ÿå¾—åˆ°äº†ä¼˜åŒ–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.13980v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å°†Segment Anything Modelï¼ˆSAMï¼‰å¼•å…¥å¤šæ¨¡æ€å›¾åƒåˆ†å‰²ä¸­ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„ç»“åˆLatent Space Token Generationï¼ˆLSTGï¼‰å’ŒFusion Mask Promptingï¼ˆFMPï¼‰æ¨¡å—çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡å‘é‡é‡åŒ–å’Œè·¨åŸŸèåˆæ¨¡å—ï¼Œè·å¾—ä¸¤ç§æ¨¡æ€çš„æ½œåœ¨ç©ºé—´ç‰¹å¾ï¼Œå»ºç«‹æ¨¡æ€é—´çš„é•¿ç¨‹ä¾èµ–å…³ç³»ï¼Œå¹¶åˆ©ç”¨è¿™äº›ç»¼åˆèåˆç‰¹å¾ä½œä¸ºæç¤ºæ¥æŒ‡å¯¼ç²¾ç¡®çš„åƒç´ çº§åˆ†å‰²ã€‚åœ¨å¤šä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šæ¨¡æ€è‡ªåŠ¨é©¾é©¶åœºæ™¯ä¸­çš„è¡¨ç°æ˜¾è‘—ä¼˜äºSAMå’ŒSAM2ï¼Œåœ¨åˆ†å‰²mIoUä¸Šå¹³å‡æé«˜äº†4.1%ï¼Œå¹¶ä¸”åœ¨å…¶ä»–å¤šæ¨¡æ€è§†è§‰åœºæ™¯ä¸­çš„æ€§èƒ½ä¹Ÿå¾—åˆ°äº†ä¼˜åŒ–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡é¦–æ¬¡å°†Segment Anything Model (SAM) å¼•å…¥å¤šæ¨¡æ€å›¾åƒåˆ†å‰²é¢†åŸŸã€‚</li>
<li>æå‡ºäº†ä¸€ç§ç»“åˆLatent Space Token Generation (LSTG) å’ŒFusion Mask Prompting (FMP) æ¨¡å—çš„æ–°æ¡†æ¶ã€‚</li>
<li>é€šè¿‡å‘é‡é‡åŒ–å’Œè·¨åŸŸèåˆæ¨¡å—ï¼Œè·å¾—ä¸¤ç§æ¨¡æ€çš„æ½œåœ¨ç©ºé—´ç‰¹å¾ã€‚</li>
<li>å»ºç«‹æ¨¡æ€é—´çš„é•¿ç¨‹ä¾èµ–å…³ç³»ï¼Œå¹¶åˆ©ç”¨ç»¼åˆèåˆç‰¹å¾ä½œä¸ºæç¤ºï¼ŒæŒ‡å¯¼ç²¾ç¡®çš„åƒç´ çº§åˆ†å‰²ã€‚</li>
<li>åœ¨å¤šä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šæ¨¡æ€è‡ªåŠ¨é©¾é©¶åœºæ™¯ä¸­çš„è¡¨ç°ä¼˜äºSAMå’ŒSAM2ã€‚</li>
<li>ä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨åˆ†å‰²mIoUä¸Šå¹³å‡æé«˜äº†4.1%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.13980">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f32e5367ecb582cc5f1215e936bd899a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f98feea4e45b750a5fb2a389f9cf9505.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9a3a662b7740f19f649846d1f1eedc12.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a183dfd4a959e82968d79553d11a5da5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c77b70bfe8f8f2c5aea0f0193a8961f.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Improved-Baselines-with-Synchronized-Encoding-for-Universal-Medical-Image-Segmentation"><a href="#Improved-Baselines-with-Synchronized-Encoding-for-Universal-Medical-Image-Segmentation" class="headerlink" title="Improved Baselines with Synchronized Encoding for Universal Medical   Image Segmentation"></a>Improved Baselines with Synchronized Encoding for Universal Medical   Image Segmentation</h2><p><strong>Authors:Sihan Yang, Jiadong Feng, Xuande Mi, Haixia Bi, Hai Zhang, Jian Sun</strong></p>
<p>Large foundation models, known for their strong zero-shot generalization capabilities, can be applied to a wide range of downstream tasks. However, developing foundation models for medical image segmentation poses a significant challenge due to the domain gap between natural and medical images. While fine-tuning techniques based on the Segment Anything Model (SAM) have been explored, they primarily focus on scaling up data or refining inference strategies without incorporating domain-specific architectural designs, limiting their zero-shot performance. To optimize segmentation performance under standard inference settings and provide a strong baseline for future research, we introduce SyncSAM, which employs a synchronized dual-branch encoder that integrates convolution and Transformer features in a synchronized manner to enhance medical image encoding, and a multi-scale dual-branch decoder to preserve image details. SyncSAM is trained on two of the largest medical image segmentation datasets, SA-Med2D-20M and IMed-361M, resulting in a series of pre-trained models for universal medical image segmentation. Experimental results demonstrate that SyncSAM not only achieves state-of-the-art performance on test sets but also exhibits strong zero-shot capabilities on unseen datasets. Code and checkpoints are available at <a target="_blank" rel="noopener" href="https://github.com/Hhankyangg/SyncSAM">https://github.com/Hhankyangg/SyncSAM</a>. </p>
<blockquote>
<p>å¤§å‹åŸºç¡€æ¨¡å‹ä»¥å…¶å¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›è€Œå¹¿æ³›åº”ç”¨äºå„ç§ä¸‹æ¸¸ä»»åŠ¡ã€‚ç„¶è€Œï¼Œç”±äºè‡ªç„¶å›¾åƒä¸åŒ»å­¦å›¾åƒé¢†åŸŸä¹‹é—´çš„å·®è·ï¼Œå¼€å‘ç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²çš„åŸºç¡€æ¨¡å‹æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚è™½ç„¶åŸºäºSegment Anything Modelï¼ˆSAMï¼‰çš„å¾®è°ƒæŠ€æœ¯å·²ç»è¢«æ¢ç´¢ï¼Œä½†å®ƒä»¬ä¸»è¦é›†ä¸­åœ¨æ‰©å¤§æ•°æ®æˆ–æ”¹è¿›æ¨ç†ç­–ç•¥ä¸Šï¼Œè€Œæ²¡æœ‰èå…¥é¢†åŸŸç‰¹å®šçš„æ¶æ„è®¾è®¡ï¼Œä»è€Œé™åˆ¶äº†å…¶é›¶æ ·æœ¬æ€§èƒ½ã€‚ä¸ºäº†ä¼˜åŒ–æ ‡å‡†æ¨ç†è®¾ç½®ä¸‹çš„åˆ†å‰²æ€§èƒ½ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›å¼ºå¤§çš„åŸºçº¿ï¼Œæˆ‘ä»¬å¼•å…¥äº†SyncSAMã€‚SyncSAMé‡‡ç”¨åŒæ­¥åŒåˆ†æ”¯ç¼–ç å™¨ï¼Œä»¥åŒæ­¥çš„æ–¹å¼èåˆå·ç§¯å’ŒTransformerç‰¹å¾ï¼Œä»¥å¢å¼ºåŒ»å­¦å›¾åƒç¼–ç ï¼›å¹¶é‡‡ç”¨å¤šå°ºåº¦åŒåˆ†æ”¯è§£ç å™¨ï¼Œä»¥ä¿ç•™å›¾åƒç»†èŠ‚ã€‚SyncSAMåœ¨ä¸¤å¤§åŒ»å­¦å›¾åƒåˆ†å‰²æ•°æ®é›†SA-Med2D-20Må’ŒIMed-361Mä¸Šè¿›è¡Œè®­ç»ƒï¼Œç”Ÿæˆäº†ä¸€ç³»åˆ—ç”¨äºé€šç”¨åŒ»å­¦å›¾åƒåˆ†å‰²çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSyncSAMä¸ä»…åœ¨æµ‹è¯•é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè€Œä¸”åœ¨æœªè§æ•°æ®é›†ä¸Šè¡¨ç°å‡ºäº†å¼ºå¤§çš„é›¶æ ·æœ¬èƒ½åŠ›ã€‚ç›¸å…³ä»£ç å’Œæ£€æŸ¥ç‚¹å¯è®¿é—®<a target="_blank" rel="noopener" href="https://github.com/Hhankyangg/SyncSAM%E3%80%82">https://github.com/Hhankyangg/SyncSAMã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.09886v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹é€šç”¨æ¨¡å‹å…·æœ‰è‰¯å¥½çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œå¯åº”ç”¨äºå¤šç§ä¸‹æ¸¸ä»»åŠ¡ã€‚ç„¶è€Œï¼Œå°†å…¶åº”ç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²æ—¶é¢ä¸´åŸŸå·®è·é—®é¢˜ã€‚åŸºäºSegment Anything Modelï¼ˆSAMï¼‰çš„å¾®è°ƒæŠ€æœ¯è™½ç„¶å·²è¢«æ¢ç´¢ï¼Œä½†å®ƒä»¬ä¸»è¦å…³æ³¨æ‰©å¤§æ•°æ®è§„æ¨¡æˆ–ä¼˜åŒ–æ¨ç†ç­–ç•¥ï¼Œæœªèå…¥ç‰¹å®šé¢†åŸŸçš„æ¶æ„è®¾è®¡ï¼Œä»è€Œé™åˆ¶äº†é›¶æ ·æœ¬æ€§èƒ½ã€‚ä¸ºåœ¨æ ‡å‡†æ¨ç†è®¾ç½®ä¸‹ä¼˜åŒ–åˆ†å‰²æ€§èƒ½å¹¶ä¸ºæœªæ¥ç ”ç©¶æä¾›å¼ºåŠ²åŸºå‡†çº¿ï¼Œæˆ‘ä»¬æ¨å‡ºSyncSAMï¼Œé‡‡ç”¨åŒæ­¥åŒåˆ†æ”¯ç¼–ç å™¨ï¼Œèåˆå·ç§¯å’ŒTransformerç‰¹å¾ï¼Œå¼ºåŒ–åŒ»å­¦å›¾åƒç¼–ç ï¼›ä»¥åŠå¤šå°ºåº¦åŒåˆ†æ”¯è§£ç å™¨ï¼Œä¿ç•™å›¾åƒç»†èŠ‚ã€‚SyncSAMåœ¨ä¸¤å¤§åŒ»å­¦å›¾åƒåˆ†å‰²æ•°æ®é›†SA-Med2D-20Må’ŒIMed-361Mä¸Šè¿›è¡Œè®­ç»ƒï¼Œç”Ÿæˆä¸€ç³»åˆ—ç”¨äºé€šç”¨åŒ»å­¦å›¾åƒåˆ†å‰²çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚å®éªŒç»“æœè¯æ˜SyncSAMä¸ä»…åœ¨æµ‹è¯•é›†ä¸Šè¾¾åˆ°é¢†å…ˆæ°´å¹³ï¼Œè€Œä¸”åœ¨æœªè§æ•°æ®é›†ä¸Šå±•ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹é€šç”¨æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸Šåº”ç”¨æ—¶å­˜åœ¨åŸŸå·®è·é—®é¢˜ã€‚</li>
<li>åŸºäºSegment Anything Modelï¼ˆSAMï¼‰çš„å¾®è°ƒæŠ€æœ¯è™½æœ‰æ‰€æ¢ç´¢ï¼Œä½†ç¼ºä¹ç‰¹å®šé¢†åŸŸçš„æ¶æ„è®¾è®¡ï¼Œé™åˆ¶äº†é›¶æ ·æœ¬æ€§èƒ½ã€‚</li>
<li>SyncSAMé€šè¿‡åŒæ­¥åŒåˆ†æ”¯ç¼–ç å™¨èåˆå·ç§¯å’ŒTransformerç‰¹å¾ï¼Œå¼ºåŒ–åŒ»å­¦å›¾åƒç¼–ç ã€‚</li>
<li>SyncSAMé‡‡ç”¨å¤šå°ºåº¦åŒåˆ†æ”¯è§£ç å™¨ä»¥ä¿ç•™å›¾åƒç»†èŠ‚ã€‚</li>
<li>SyncSAMåœ¨SA-Med2D-20Må’ŒIMed-361Mä¸¤ä¸ªå¤§å‹åŒ»å­¦å›¾åƒåˆ†å‰²æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒã€‚</li>
<li>SyncSAMå®ç°åŒ»å­¦å›¾åƒåˆ†å‰²çš„é¢†å…ˆæ°´å¹³ï¼Œå¹¶åœ¨æœªè§æ•°æ®é›†ä¸Šå±•ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.09886">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e5da76901a552ea8b1ec9219814a342c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f80d06008cb8fd4f7a0486f719e3bf2e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-88fbc7c84c7f629d2104cd8d4f5aab69.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-516e88cb5d1148ab962d4c26b832a321.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Enhancing-Dynamic-CT-Image-Reconstruction-with-Neural-Fields-and-Optical-Flow"><a href="#Enhancing-Dynamic-CT-Image-Reconstruction-with-Neural-Fields-and-Optical-Flow" class="headerlink" title="Enhancing Dynamic CT Image Reconstruction with Neural Fields and Optical   Flow"></a>Enhancing Dynamic CT Image Reconstruction with Neural Fields and Optical   Flow</h2><p><strong>Authors:Pablo Arratia, Matthias Ehrhardt, Lisa Kreusser</strong></p>
<p>In this paper, we investigate image reconstruction for dynamic Computed Tomography. The motion of the target with respect to the measurement acquisition rate leads to highly resolved in time but highly undersampled in space measurements. Such problems pose a major challenge: not accounting for the dynamics of the process leads to a poor reconstruction with non-realistic motion. Variational approaches that penalize time evolution have been proposed to relate subsequent frames and improve image quality based on classical grid-based discretizations. Neural fields have emerged as a novel way to parameterize the quantity of interest using a neural network with a low-dimensional input, benefiting from being lightweight, continuous, and biased towards smooth representations. The latter property has been exploited when solving dynamic inverse problems with neural fields by minimizing a data-fidelity term only. We investigate and show the benefits of introducing explicit motion regularizers for dynamic inverse problems based on partial differential equations, namely, the optical flow equation, for the optimization of neural fields. We compare it against its unregularized counterpart and show the improvements in the reconstruction. We also compare neural fields against a grid-based solver and show that the former outperforms the latter in terms of PSNR in this task. </p>
<blockquote>
<p>æœ¬æ–‡ç ”ç©¶äº†åŠ¨æ€è®¡ç®—æœºæ–­å±‚æ‰«æçš„å›¾åƒé‡å»ºã€‚ç›®æ ‡ç›¸å¯¹äºæµ‹é‡é‡‡é›†ç‡çš„è¿åŠ¨å¯¼è‡´åœ¨æ—¶é—´ä¸Šçš„åˆ†è¾¨ç‡å¾ˆé«˜ï¼Œä½†åœ¨ç©ºé—´ä¸Šçš„æµ‹é‡ä¸¥é‡ä¸è¶³ã€‚è¿™æ ·çš„é—®é¢˜å¸¦æ¥äº†å¾ˆå¤§çš„æŒ‘æˆ˜ï¼šä¸è€ƒè™‘è¿‡ç¨‹çš„åŠ¨æ€æ€§ä¼šå¯¼è‡´é‡å»ºè´¨é‡ä¸ä½³å’Œå‡ºç°éç°å®çš„è¿åŠ¨ã€‚åŸºäºç»å…¸ç½‘æ ¼ç¦»æ•£åŒ–çš„æ–¹æ³•å·²ç»æå‡ºäº†å¯¹æ—¶é—´æ¼”åŒ–è¿›è¡Œæƒ©ç½šçš„å˜åˆ†æ–¹æ³•ï¼Œä»¥å…³è”åç»­å¸§å¹¶æé«˜å›¾åƒè´¨é‡ã€‚ç¥ç»åœºä½œä¸ºä¸€ç§æ–°çš„æ–¹æ³•å·²ç»å´­éœ²å¤´è§’ï¼Œåˆ©ç”¨ä½ç»´è¾“å…¥çš„ç¥ç»ç½‘ç»œå¯¹æ„Ÿå…´è¶£çš„é‡è¿›è¡Œå‚æ•°åŒ–ï¼Œå¾—ç›Šäºå…¶è½»ä¾¿ã€è¿ç»­ä»¥åŠåå‘å¹³æ»‘è¡¨ç¤ºçš„ç‰¹æ€§ã€‚åœ¨è§£å†³åŠ¨æ€é€†é—®é¢˜æ—¶ï¼Œæˆ‘ä»¬å·²ç»åˆ©ç”¨äº†åè€…çš„å±æ€§ï¼Œé€šè¿‡ä»…æœ€å°åŒ–æ•°æ®ä¿çœŸé¡¹æ¥åˆ©ç”¨ç¥ç»åœºã€‚æˆ‘ä»¬ç ”ç©¶å’Œå±•ç¤ºäº†å¼•å…¥åŸºäºåå¾®åˆ†æ–¹ç¨‹çš„æ˜¾å¼è¿åŠ¨æ­£åˆ™åŒ–å™¨å¯¹äºåŠ¨æ€é€†é—®é¢˜çš„ä¼˜åŒ–å¥½å¤„ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹å…‰æµæ–¹ç¨‹ä¼˜åŒ–ç¥ç»åœºã€‚æˆ‘ä»¬å°†å…¶ä¸æœªæ­£åˆ™åŒ–çš„æ–¹æ³•è¿›è¡Œæ¯”è¾ƒï¼Œå¹¶å±•ç¤ºäº†é‡å»ºè´¨é‡çš„æ”¹è¿›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¯”è¾ƒäº†ç¥ç»åœºå’ŒåŸºäºç½‘æ ¼çš„æ±‚è§£å™¨ï¼Œå¹¶è¯æ˜åœ¨æ­¤ä»»åŠ¡ä¸­ï¼Œç¥ç»åœºåœ¨å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰æ–¹é¢è¡¨ç°ä¼˜äºåŸºäºç½‘æ ¼çš„æ±‚è§£å™¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.01299v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†åŠ¨æ€è®¡ç®—æœºæ–­å±‚æ‰«æçš„å›¾åƒé‡å»ºé—®é¢˜ã€‚ç›®æ ‡è¿åŠ¨ä¸æµ‹é‡é‡‡é›†ç‡çš„ç›¸å¯¹å…³ç³»å¯¼è‡´åœ¨æ—¶é—´ä¸Šçš„åˆ†è¾¨ç‡é«˜è€Œåœ¨ç©ºé—´ä¸Šçš„é‡‡æ ·ä¸è¶³ã€‚å¿½ç•¥è¿‡ç¨‹çš„åŠ¨æ€æ€§ä¼šå¯¼è‡´é‡å»ºè´¨é‡å·®ï¼Œå‡ºç°éç°å®è¿åŠ¨ã€‚ç ”ç©¶äººå‘˜æå‡ºäº†åŸºäºå˜åˆ†çš„æ–¹æ³•ï¼Œé€šè¿‡æƒ©ç½šæ—¶é—´æ¼”åŒ–æ¥å…³è”åç»­å¸§å¹¶åŸºäºç»å…¸ç½‘æ ¼ç¦»æ•£åŒ–æé«˜å›¾åƒè´¨é‡ã€‚ç¥ç»ç½‘ç»œåœºä½œä¸ºä¸€ç§æ–°çš„å‚æ•°åŒ–æ–¹æ³•å‡ºç°ï¼Œä½¿ç”¨ä½ç»´è¾“å…¥å‚æ•°åŒ–æ„Ÿå…´è¶£çš„é‡ï¼Œå…¶ä¼˜åŠ¿åœ¨äºè½»é‡åŒ–ã€è¿ç»­æ€§å’Œåå‘å¹³æ»‘è¡¨ç¤ºã€‚æœ¬ç ”ç©¶æ¢ç´¢å¹¶å±•ç¤ºäº†ä¸ºåŠ¨æ€åé—®é¢˜å¼•å…¥åŸºäºåå¾®åˆ†æ–¹ç¨‹çš„æ˜¾å¼è¿åŠ¨æ­£åˆ™åŒ–å™¨çš„ä¼˜åŠ¿ï¼Œç‰¹åˆ«æ˜¯ä½¿ç”¨å…‰æµæ–¹ç¨‹ä¼˜åŒ–ç¥ç»ç½‘ç»œåœºã€‚ä¸æœªæ­£åˆ™åŒ–çš„æ–¹æ³•ç›¸æ¯”ï¼Œæœ¬ç ”ç©¶æ˜¾ç¤ºé‡å»ºè´¨é‡æœ‰æ‰€æé«˜ã€‚æ­¤å¤–ï¼Œå°†ç¥ç»ç½‘ç»œåœºä¸ç½‘æ ¼æ±‚è§£å™¨è¿›è¡Œæ¯”è¾ƒï¼Œç»“æœæ˜¾ç¤ºç¥ç»ç½‘ç»œåœºåœ¨æ­¤ä»»åŠ¡ä¸­çš„å³°å€¼ä¿¡å™ªæ¯”è¡¨ç°æ›´ä½³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡ç ”ç©¶äº†åŠ¨æ€è®¡ç®—æœºæ–­å±‚æ‰«æä¸­çš„å›¾åƒé‡å»ºé—®é¢˜ï¼Œç‰¹åˆ«å…³æ³¨äº†ç›®æ ‡è¿åŠ¨ä¸æµ‹é‡é‡‡é›†ç‡ä¹‹é—´çš„å…³ç³»ã€‚</li>
<li>å¿½ç•¥è¿‡ç¨‹çš„åŠ¨æ€æ€§åœ¨å›¾åƒé‡å»ºä¸­ä¼šå¯¼è‡´è´¨é‡ä¸‹é™å’Œéç°å®è¿åŠ¨çš„å‡ºç°ã€‚</li>
<li>åŸºäºå˜åˆ†çš„æ–¹æ³•è¢«æå‡ºï¼Œé€šè¿‡å…³è”åç»­å¸§å’ŒåŸºäºç½‘æ ¼çš„ç¦»æ•£åŒ–æ¥æé«˜å›¾åƒè´¨é‡ã€‚</li>
<li>ç¥ç»ç½‘ç»œåœºä½œä¸ºä¸€ç§æ–°çš„å‚æ•°åŒ–æ–¹æ³•å‡ºç°ï¼Œå…·æœ‰è½»é‡åŒ–ã€è¿ç»­æ€§å’Œåå‘å¹³æ»‘è¡¨ç¤ºçš„ä¼˜åŠ¿ã€‚</li>
<li>å¼•å…¥åŸºäºåå¾®åˆ†æ–¹ç¨‹çš„æ˜¾å¼è¿åŠ¨æ­£åˆ™åŒ–å™¨å¯¹äºä¼˜åŒ–ç¥ç»ç½‘ç»œåœºè§£å†³åŠ¨æ€åé—®é¢˜æœ‰ç›Šã€‚</li>
<li>ä¸æœªæ­£åˆ™åŒ–çš„æ–¹æ³•ç›¸æ¯”ï¼Œå¼•å…¥å…‰æµæ–¹ç¨‹æ­£åˆ™åŒ–å™¨çš„å›¾åƒé‡å»ºè´¨é‡æœ‰æ‰€æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.01299">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fb2d4bc915bd0bdcb122915ed99bb987.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-133dce128943ea75b5024963f9e6adfc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-58ff86823409f1acbef8f66901366d41.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="IgCONDA-PET-Weakly-Supervised-PET-Anomaly-Detection-using-Implicitly-Guided-Attention-Conditional-Counterfactual-Diffusion-Modeling-â€“-a-Multi-Center-Multi-Cancer-and-Multi-Tracer-Study"><a href="#IgCONDA-PET-Weakly-Supervised-PET-Anomaly-Detection-using-Implicitly-Guided-Attention-Conditional-Counterfactual-Diffusion-Modeling-â€“-a-Multi-Center-Multi-Cancer-and-Multi-Tracer-Study" class="headerlink" title="IgCONDA-PET: Weakly-Supervised PET Anomaly Detection using   Implicitly-Guided Attention-Conditional Counterfactual Diffusion Modeling â€“   a Multi-Center, Multi-Cancer, and Multi-Tracer Study"></a>IgCONDA-PET: Weakly-Supervised PET Anomaly Detection using   Implicitly-Guided Attention-Conditional Counterfactual Diffusion Modeling â€“   a Multi-Center, Multi-Cancer, and Multi-Tracer Study</h2><p><strong>Authors:Shadab Ahamed, Arman Rahmim</strong></p>
<p>Minimizing the need for pixel-level annotated data to train PET lesion detection and segmentation networks is highly desired and can be transformative, given time and cost constraints associated with expert annotations. Current unsupervised or weakly-supervised anomaly detection methods rely on autoencoder or generative adversarial networks (GANs) trained only on healthy data. While these approaches reduce annotation dependency, GAN-based methods are notably more challenging to train than non-GAN alternatives (such as autoencoders) due to issues such as the simultaneous optimization of two competing networks, mode collapse, and training instability. In this paper, we present the weakly-supervised $\textbf{I}$mplicitly-$\textbf{g}$uided $\textbf{CO}$u$\textbf{N}$terfactual diffusion model for $\textbf{D}$etecting $\textbf{A}$nomalies in $\textbf{PET}$ images (IgCONDA-PET). The solution is developed and validated using PET scans from six retrospective cohorts consisting of a total of 2652 cases (multi-cancer, multi-tracer) containing both local and public datasets (spanning multiple centers). The training is conditioned on image class labels (healthy vs. unhealthy) via attention modules, and we employ implicit diffusion guidance. We perform counterfactual generation which facilitates â€œunhealthy-to-healthyâ€ domain translation by generating a synthetic, healthy version of an unhealthy input image, enabling the detection of anomalies through the calculated differences. The performance of our method was compared against several other deep learning based weakly-supervised or unsupervised methods as well as traditional methods like 41% SUV$_\text{max}$ thresholding. We also highlight the importance of incorporating attention modules in our network for the detection of small anomalies. The code is publicly available at: <a target="_blank" rel="noopener" href="https://github.com/ahxmeds/IgCONDA-PET.git">https://github.com/ahxmeds/IgCONDA-PET.git</a>. </p>
<blockquote>
<p>åœ¨ PET ç—…å˜æ£€æµ‹å’Œåˆ†å‰²ç½‘ç»œè®­ç»ƒä¸­ï¼Œæœ€å°åŒ–å¯¹åƒç´ çº§æ ‡æ³¨æ•°æ®çš„éœ€æ±‚æ˜¯éå¸¸ç†æƒ³ä¸”å…·æœ‰å˜é©æ€§çš„ï¼Œå› ä¸ºè¿™æ ·å¯ä»¥èŠ‚çœæ—¶é—´å’Œæˆæœ¬ï¼Œé¿å…æ˜‚è´µçš„ä¸“å®¶æ ‡æ³¨ã€‚å½“å‰çš„æ— ç›‘ç£æˆ–å¼±ç›‘ç£å¼‚å¸¸æ£€æµ‹æ–¹æ³•ä¾èµ–äºä»…ä½¿ç”¨å¥åº·æ•°æ®è®­ç»ƒçš„è‡ªåŠ¨ç¼–ç å™¨æˆ–ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰ã€‚è™½ç„¶è¿™äº›æ–¹æ³•å‡å°‘äº†æ ‡æ³¨ä¾èµ–ï¼Œä½†åŸºäº GAN çš„æ–¹æ³•ç”±äºå…¶å›ºæœ‰çš„å¤æ‚æ€§ï¼ˆå¦‚ä¸¤ä¸ªç«äº‰ç½‘ç»œçš„åŒæ­¥ä¼˜åŒ–ã€æ¨¡å¼å´©æºƒå’Œè®­ç»ƒä¸ç¨³å®šç­‰ï¼‰ç›¸è¾ƒäºé GAN æ›¿ä»£æ–¹æ¡ˆï¼ˆå¦‚è‡ªåŠ¨ç¼–ç å™¨ï¼‰çš„è®­ç»ƒæ›´å…·æŒ‘æˆ˜æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†å¼±ç›‘ç£éšå¯¼PETå¼‚å¸¸æ£€æµ‹å¯¹æ¯”æ‰©æ•£æ¨¡å‹ï¼ˆIgCONDA-PETï¼‰ã€‚è¯¥è§£å†³æ–¹æ¡ˆæ˜¯ä½¿ç”¨æ¥è‡ªå…­ä¸ªå›é¡¾æ€§é˜Ÿåˆ—çš„ PET æ‰«æå¼€å‘å¹¶éªŒè¯çš„ï¼ŒåŒ…æ‹¬æ€»å…± 2652 ä¾‹å¤šç™Œç—‡ã€å¤šç¤ºè¸ªå‰‚ç—…ä¾‹ï¼ŒåŒ…å«æœ¬åœ°å’Œå…¬å¼€æ•°æ®é›†ï¼ˆæ¶µç›–å¤šä¸ªä¸­å¿ƒï¼‰ã€‚è®­ç»ƒæ˜¯é€šè¿‡å›¾åƒç±»åˆ«æ ‡ç­¾ï¼ˆå¥åº·ä¸å¦ï¼‰é€šè¿‡æ³¨æ„åŠ›æ¨¡å—è¿›è¡Œçš„ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†éšå¼æ‰©æ•£æŒ‡å¯¼ã€‚æˆ‘ä»¬æ‰§è¡Œäº†åäº‹å®ç”Ÿæˆï¼Œé€šè¿‡ç”Ÿæˆä¸å¥åº·è¾“å…¥å›¾åƒçš„åˆæˆå¥åº·ç‰ˆæœ¬ï¼Œä¿ƒè¿›äº†â€œä¸å¥åº·åˆ°å¥åº·â€çš„åŸŸè½¬æ¢ï¼Œå¹¶é€šè¿‡è®¡ç®—å·®å¼‚è¿›è¡Œå¼‚å¸¸æ£€æµ‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸åŸºäºæ·±åº¦å­¦ä¹ çš„å…¶ä»–å¼±ç›‘ç£æˆ–æ— ç›‘ç£æ–¹æ³•ä»¥åŠä¼ ç»Ÿçš„å¦‚ä½¿ç”¨ 41% SUVmax å€¼é˜ˆå€¼çš„æ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒã€‚æˆ‘ä»¬è¿˜å¼ºè°ƒäº†åœ¨ç½‘ç»œä¸­èå…¥æ³¨æ„åŠ›æ¨¡å—ä»¥æ£€æµ‹å°å¼‚å¸¸çš„é‡è¦æ€§ã€‚ä»£ç å¯åœ¨ <a target="_blank" rel="noopener" href="https://github.com/ahxmeds/IgCONDA-PET.git">https://github.com/ahxmeds/IgCONDA-PET.git</a> ä¸Šå…¬å¼€è®¿é—®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.00239v3">PDF</a> 48 pages, 13 figures, 4 tables</p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬ç ”ç©¶æå‡ºä¸€ç§ç”¨äºPETå›¾åƒå¼‚å¸¸æ£€æµ‹çš„å¼±ç›‘ç£éšå¼å¼•å¯¼æ‰©æ•£æ¨¡å‹ï¼ˆIgCONDA-PETï¼‰ã€‚è¯¥æ¨¡å‹åˆ©ç”¨PETæ‰«ææ•°æ®å¼€å‘å¹¶éªŒè¯ï¼Œé€šè¿‡æ³¨æ„åŠ›æ¨¡å—ä»¥å›¾åƒç±»åˆ«æ ‡ç­¾ï¼ˆå¥åº·ä¸éå¥åº·ï¼‰ä¸ºæ¡ä»¶è¿›è¡Œè®­ç»ƒï¼Œé‡‡ç”¨éšå¼æ‰©æ•£å¼•å¯¼å¹¶ç”Ÿæˆåäº‹å®æ•°æ®ï¼Œå®ç°éå¥åº·å›¾åƒå‘å¥åº·å›¾åƒçš„è½¬æ¢ï¼Œä»¥æ£€æµ‹å¼‚å¸¸ã€‚ä¸å¤šç§æ·±åº¦å­¦ä¹ çš„å¼±ç›‘ç£æˆ–æ— ç›‘ç£æ–¹æ³•ä»¥åŠä¼ ç»Ÿçš„SUVmaxé˜ˆå€¼æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•æ€§èƒ½ä¼˜å¼‚ã€‚å…¬å¼€çš„ä»£ç å¯ä¾›ç ”ç©¶ä½¿ç”¨ã€‚è¯¥æ–¹æ³•çš„äº®ç‚¹åœ¨äºæ³¨æ„åŠ›æ¨¡å—çš„ä½¿ç”¨ï¼Œå¯¹äºæ£€æµ‹å°å‹å¼‚å¸¸å…·æœ‰é‡è¦æ„ä¹‰ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§åŸºäºå¼±ç›‘ç£çš„éšå¼å¼•å¯¼æ‰©æ•£æ¨¡å‹ï¼ˆIgCONDA-PETï¼‰ç”¨äºPETå›¾åƒå¼‚å¸¸æ£€æµ‹ã€‚</li>
<li>è¯¥æ¨¡å‹é€šè¿‡æ³¨æ„åŠ›æ¨¡å—ä»¥å›¾åƒç±»åˆ«æ ‡ç­¾ä¸ºæ¡ä»¶è¿›è¡Œè®­ç»ƒï¼Œæé«˜äº†æ£€æµ‹çš„å‡†ç¡®æ€§ã€‚</li>
<li>é‡‡ç”¨äº†éšå¼æ‰©æ•£å¼•å¯¼å’Œåäº‹å®ç”Ÿæˆï¼Œå®ç°äº†éå¥åº·å›¾åƒå‘å¥åº·å›¾åƒçš„è½¬æ¢ï¼Œæœ‰åŠ©äºå¼‚å¸¸æ£€æµ‹ã€‚</li>
<li>æ¨¡å‹æ€§èƒ½ä¼˜äºå¤šç§æ·±åº¦å­¦ä¹ çš„å¼±ç›‘ç£æˆ–æ— ç›‘ç£æ–¹æ³•ä»¥åŠä¼ ç»Ÿçš„SUVmaxé˜ˆå€¼æ–¹æ³•ã€‚</li>
<li>å…¬å¼€çš„ä»£ç ä¸ºç ”ç©¶è€…æä¾›äº†ä¾¿åˆ©ã€‚</li>
<li>æ³¨æ„åŠ›æ¨¡å—çš„ä½¿ç”¨å¯¹äºæ£€æµ‹å°å‹å¼‚å¸¸å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.00239">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-4dea5a4badfaf05a351a0942e1c915e4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ea5879b0416dd4775bc1dc39d0277270.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c1d0702ab0f1c92d333c9d274c50736.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="FluoroSAM-A-Language-promptable-Foundation-Model-for-Flexible-X-ray-Image-Segmentation"><a href="#FluoroSAM-A-Language-promptable-Foundation-Model-for-Flexible-X-ray-Image-Segmentation" class="headerlink" title="FluoroSAM: A Language-promptable Foundation Model for Flexible X-ray   Image Segmentation"></a>FluoroSAM: A Language-promptable Foundation Model for Flexible X-ray   Image Segmentation</h2><p><strong>Authors:Benjamin D. Killeen, Liam J. Wang, Blanca Inigo, Han Zhang, Mehran Armand, Russell H. Taylor, Greg Osgood, Mathias Unberath</strong></p>
<p>Language promptable X-ray image segmentation would enable greater flexibility for human-in-the-loop workflows in diagnostic and interventional precision medicine. Prior efforts have contributed task-specific models capable of solving problems within a narrow scope, but expanding to broader use requires additional data, annotations, and training time. Recently, language-aligned foundation models (LFMs) â€“ machine learning models trained on large amounts of highly variable image and text data thus enabling broad applicability â€“ have emerged as promising tools for automated image analysis. Existing foundation models for medical image analysis focus on scenarios and modalities where large, richly annotated datasets are available. However, the X-ray imaging modality features highly variable image appearance and applications, from diagnostic chest X-rays to interventional fluoroscopy, with varying availability of data. To pave the way toward an LFM for comprehensive and language-aligned analysis of arbitrary medical X-ray images, we introduce FluoroSAM, a language-promptable variant of the Segment Anything Model, trained from scratch on 3M synthetic X-ray images from a wide variety of human anatomies, imaging geometries, and viewing angles. These include pseudo-ground truth masks for 128 organ types and 464 tools with associated text descriptions. FluoroSAM is capable of segmenting myriad anatomical structures and tools based on natural language prompts, thanks to the novel incorporation of vector quantization (VQ) of text embeddings in the training process. We demonstrate FluoroSAMâ€™s performance quantitatively on real X-ray images and showcase on several applications how FluoroSAM is a key enabler for rich human-machine interaction in the X-ray image acquisition and analysis context. Code is available at <a target="_blank" rel="noopener" href="https://github.com/arcadelab/fluorosam">https://github.com/arcadelab/fluorosam</a>. </p>
<blockquote>
<p>è¯­è¨€æç¤ºå‹Xå°„çº¿å›¾åƒåˆ†å‰²å°†åœ¨è¯Šæ–­å’Œä»‹å…¥ç²¾å‡†åŒ»å­¦çš„äººç±»é—­ç¯å·¥ä½œæµç¨‹ä¸­æä¾›æ›´çµæ´»çš„è§£å†³æ–¹æ¡ˆã€‚ä¹‹å‰çš„åŠªåŠ›å·²ç»è´¡çŒ®äº†ä¸€äº›é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹èƒ½å¤Ÿåœ¨ç‹­çª„èŒƒå›´å†…è§£å†³é—®é¢˜ï¼Œä½†è¦æ‰©å±•åˆ°æ›´å¹¿æ³›çš„åº”ç”¨åˆ™éœ€è¦æ›´å¤šçš„æ•°æ®ã€æ³¨é‡Šå’Œè®­ç»ƒæ—¶é—´ã€‚æœ€è¿‘ï¼Œè¯­è¨€å¯¹é½åŸºç¡€æ¨¡å‹ï¼ˆLFMsï¼‰â€”â€”åœ¨å¤§é‡é«˜åº¦å¯å˜çš„å›¾åƒå’Œæ–‡æœ¬æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒçš„æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œèƒ½å¤Ÿå®ç°å¹¿æ³›çš„é€‚ç”¨æ€§â€”â€”å·²æˆä¸ºè‡ªåŠ¨åŒ–å›¾åƒåˆ†æçš„å¾ˆæœ‰å‰é€”çš„å·¥å…·ã€‚ç°æœ‰çš„åŒ»å­¦å›¾åƒåˆ†æåŸºç¡€æ¨¡å‹ä¾§é‡äºå¤§å‹ä¸°å¯Œæ³¨é‡Šæ•°æ®é›†å¯ç”¨çš„åœºæ™¯å’Œæ¨¡å¼ã€‚ç„¶è€Œï¼ŒXå°„çº¿æˆåƒæ¨¡å¼å…·æœ‰é«˜åº¦å¯å˜çš„å›¾åƒå¤–è§‚å’Œåº”ç”¨åœºæ™¯ï¼Œä»è¯Šæ–­èƒ¸éƒ¨Xå°„çº¿åˆ°ä»‹å…¥è§å…‰é•œæ£€æŸ¥ï¼Œæ•°æ®å¯ç”¨æ€§å„ä¸ç›¸åŒã€‚ä¸ºäº†å»ºç«‹ä¸€ç§ç”¨äºä»»æ„åŒ»å­¦Xå°„çº¿å›¾åƒçš„ç»¼åˆå’Œè¯­è¨€å¯¹é½åˆ†æçš„åŸºç¡€æ¨¡å‹ï¼Œæˆ‘ä»¬å¼•å…¥äº†FluoroSAMï¼Œè¿™æ˜¯ä¸€ä¸ªè¯­è¨€æç¤ºå‹çš„åˆ†æ®µä»»ä½•ä¸œè¥¿æ¨¡å‹ï¼ˆSegment Anything Modelï¼‰å˜ç§ï¼Œä»å¤´å¼€å§‹è®­ç»ƒï¼Œä½¿ç”¨äº†æ¥è‡ªå„ç§äººç±»è§£å‰–å­¦ã€æˆåƒå‡ ä½•å’Œè§†è§’çš„300ä¸‡åˆæˆXå°„çº¿å›¾åƒã€‚è¿™äº›åŒ…æ‹¬ç”¨äº128ç§å™¨å®˜ç±»å‹å’Œ464ç§å·¥å…·ç›¸å…³çš„æ–‡æœ¬æè¿°çš„ä¼ªçœŸå®æ©æ¨¡ã€‚ç”±äºè®­ç»ƒè¿‡ç¨‹ä¸­æ–‡æœ¬åµŒå…¥çš„å‘é‡é‡åŒ–ï¼ˆVQï¼‰çš„æ–°é¢–ç»“åˆï¼ŒFluoroSAMèƒ½å¤ŸåŸºäºè‡ªç„¶è¯­è¨€æç¤ºå¯¹å¤šç§è§£å‰–ç»“æ„å’Œå·¥å…·è¿›è¡Œåˆ†å‰²ã€‚æˆ‘ä»¬åœ¨çœŸå®Xå°„çº¿å›¾åƒä¸Šå®šé‡å±•ç¤ºäº†FluoroSAMçš„æ€§èƒ½ï¼Œå¹¶åœ¨å‡ ä¸ªåº”ç”¨åœºæ™¯ä¸­å±•ç¤ºäº†FluoroSAMå¦‚ä½•åœ¨Xå°„çº¿å›¾åƒé‡‡é›†å’Œåˆ†æç¯å¢ƒä¸­å®ç°ä¸°å¯Œçš„äººæœºäº¤äº’çš„å…³é”®åŠŸèƒ½ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/arcadelab/fluorosam%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/arcadelab/fluorosamæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.08059v3">PDF</a> </p>
<p><strong>Summary</strong><br>    è¯­è¨€æç¤ºå‹Xå°„çº¿å›¾åƒåˆ†å‰²æŠ€æœ¯ä¸ºè¯Šæ–­ä¸ä»‹å…¥ç²¾å‡†åŒ»ç–—ä¸­çš„äººæœºåä½œæµç¨‹å¸¦æ¥äº†æ›´å¤§çš„çµæ´»æ€§ã€‚ç°æœ‰ä»»åŠ¡ç‰¹å®šæ¨¡å‹åªèƒ½åœ¨ç‹­çª„èŒƒå›´å†…è§£å†³é—®é¢˜ï¼Œè¦æ‰©å¤§åº”ç”¨èŒƒå›´éœ€å¢åŠ æ•°æ®ã€æ ‡æ³¨å’Œè®­ç»ƒæ—¶é—´ã€‚æ–°å…´çš„è¯­è¨€å¯¹é½åŸºç¡€æ¨¡å‹ï¼ˆLFMsï¼‰å…·å¤‡å¤„ç†å¤§é‡å¤šå˜å›¾åƒå’Œæ–‡æœ¬æ•°æ®çš„èƒ½åŠ›ï¼Œä¸ºåŒ»å­¦å›¾åƒåˆ†ææä¾›äº†å¹¿é˜”é€‚ç”¨çš„å·¥å…·ã€‚é’ˆå¯¹åŒ»å­¦Xå°„çº¿æˆåƒæ¨¡æ€é«˜åº¦å¤šå˜çš„ç‰¹ç‚¹ï¼Œæ¨å‡ºFluoroSAMæ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ç§è¯­è¨€æç¤ºå‹çš„åŸºç¡€æ¨¡å‹å˜ä½“ï¼Œå¯åœ¨å¹¿æ³›çš„åŒ»å­¦Xå°„çº¿å›¾åƒä¸Šè¿›è¡Œè¯­è¨€å¯¹é½åˆ†æã€‚FluoroSAMé€šè¿‡è®­ç»ƒè·å¾—å¯¹å¤šç§äººä½“ç»“æ„ã€æˆåƒå‡ ä½•å’Œè§†è§’çš„300ä¸‡åˆæˆXå°„çº¿å›¾åƒçš„è®¤è¯†ï¼Œå¹¶èå…¥æ–‡æœ¬åµŒå…¥çš„å‘é‡é‡åŒ–ï¼ˆVQï¼‰æ–°æ–¹æ³•ã€‚å®ƒèƒ½æ ¹æ®è‡ªç„¶è¯­è¨€æç¤ºå¯¹å¤šç§è§£å‰–ç»“æ„å’Œå·¥å…·è¿›è¡Œåˆ†å‰²ï¼Œå¹¶åœ¨å¤šä¸ªåº”ç”¨ä¸­å±•ç¤ºäº†å…¶åœ¨ä¸°å¯Œäººæœºäº¤äº’ä¸­çš„å…³é”®ä½œç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­è¨€æç¤ºå‹Xå°„çº¿å›¾åƒåˆ†å‰²æŠ€æœ¯æé«˜äº†è¯Šæ–­ä¸ä»‹å…¥åŒ»ç–—ä¸­çš„äººæœºåä½œæµç¨‹çš„çµæ´»æ€§ã€‚</li>
<li>ç°æœ‰ä»»åŠ¡ç‰¹å®šæ¨¡å‹åœ¨è§£å†³åŒ»å­¦å›¾åƒåˆ†æé—®é¢˜æ—¶å­˜åœ¨å±€é™æ€§ï¼Œéœ€è¦æ‰©å¤§åº”ç”¨èŒƒå›´ã€‚</li>
<li>è¯­è¨€å¯¹é½åŸºç¡€æ¨¡å‹ï¼ˆLFMsï¼‰å…·å¤‡å¤„ç†å¤§é‡å¤šå˜å›¾åƒå’Œæ–‡æœ¬æ•°æ®çš„èƒ½åŠ›ï¼Œä¸ºåŒ»å­¦å›¾åƒåˆ†ææä¾›å¹¿æ³›é€‚ç”¨çš„å·¥å…·ã€‚</li>
<li>FluoroSAMæ˜¯ä¸€ç§è¯­è¨€æç¤ºå‹åŸºç¡€æ¨¡å‹å˜ä½“ï¼Œèƒ½åœ¨å¹¿æ³›çš„åŒ»å­¦Xå°„çº¿å›¾åƒä¸Šè¿›è¡Œè¯­è¨€å¯¹é½åˆ†æã€‚</li>
<li>FluoroSAMé€šè¿‡è®­ç»ƒè·å¾—å¯¹å¤šç§äººä½“ç»“æ„ã€æˆåƒå‡ ä½•å’Œè§†è§’çš„Xå°„çº¿å›¾åƒçš„è®¤è¯†ã€‚</li>
<li>FluoroSAMèå…¥æ–‡æœ¬åµŒå…¥çš„å‘é‡é‡åŒ–ï¼ˆVQï¼‰æ–°æ–¹æ³•ï¼Œèƒ½å¤Ÿæ ¹æ®è‡ªç„¶è¯­è¨€æç¤ºå¯¹å¤šç§è§£å‰–ç»“æ„å’Œå·¥å…·è¿›è¡Œåˆ†å‰²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.08059">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1e6afd118ad42bf38789133db0c0ca95.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6a2cf9a7f2eaa775ec64bbacc9f77f56.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b167a9ce495c58a257c26a5c471fae37.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-978ced4069e3201d51171b1c42c087ad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c514bd2857aceaf1469543443e64c5c5.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Leveraging-Foundation-Models-for-Content-Based-Image-Retrieval-in-Radiology"><a href="#Leveraging-Foundation-Models-for-Content-Based-Image-Retrieval-in-Radiology" class="headerlink" title="Leveraging Foundation Models for Content-Based Image Retrieval in   Radiology"></a>Leveraging Foundation Models for Content-Based Image Retrieval in   Radiology</h2><p><strong>Authors:Stefan Denner, David Zimmerer, Dimitrios Bounias, Markus Bujotzek, Shuhan Xiao, Raphael Stock, Lisa Kausch, Philipp Schader, Tobias Penzkofer, Paul F. JÃ¤ger, Klaus Maier-Hein</strong></p>
<p>Content-based image retrieval (CBIR) has the potential to significantly improve diagnostic aid and medical research in radiology. However, current CBIR systems face limitations due to their specialization to certain pathologies, limiting their utility. On the other hand, several vision foundation models have been shown to produce general-purpose visual features. Therefore, in this work, we propose using vision foundation models as powerful and versatile off-the-shelf feature extractors for content-based image retrieval. Our contributions include: (1) benchmarking a diverse set of vision foundation models on an extensive dataset comprising 1.6 million 2D radiological images across four modalities and 161 pathologies; (2) identifying weakly-supervised models, particularly BiomedCLIP, as highly effective, achieving a achieving a P@1 of up to 0.594 (P@3: 0.590, P@5: 0.588, P@10: 0.583), comparable to specialized CBIR systems but without additional training; (3) conducting an in-depth analysis of the impact of index size on retrieval performance; (4) evaluating the quality of embedding spaces generated by different models; and (5) investigating specific challenges associated with retrieving anatomical versus pathological structures. Despite these challenges, our research underscores the vast potential of foundation models for CBIR in radiology, proposing a shift towards versatile, general-purpose medical image retrieval systems that do not require specific tuning. Our code, dataset splits and embeddings are publicly available under <a target="_blank" rel="noopener" href="https://github.com/MIC-DKFZ/foundation-models-for-cbmir">https://github.com/MIC-DKFZ/foundation-models-for-cbmir</a>. </p>
<blockquote>
<p>åŸºäºå†…å®¹çš„å›¾åƒæ£€ç´¢ï¼ˆCBIRï¼‰åœ¨æ”¾å°„ç§‘çš„è¯Šæ–­å’ŒåŒ»ç–—ç ”ç©¶æ–¹é¢æœ‰ç€å·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå½“å‰çš„CBIRç³»ç»Ÿå› ä¸“é—¨å¤„ç†ç‰¹å®šç—…ç†è€Œå—åˆ°å±€é™ï¼Œå¯¼è‡´å…¶å®ç”¨æ€§å—é™ã€‚å¦ä¸€æ–¹é¢ï¼Œä¸€äº›è§†è§‰åŸºç¡€æ¨¡å‹å·²è¢«è¯æ˜å¯ä»¥äº§ç”Ÿé€šç”¨è§†è§‰ç‰¹å¾ã€‚å› æ­¤ï¼Œåœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºåˆ©ç”¨è§†è§‰åŸºç¡€æ¨¡å‹ä½œä¸ºå¼ºå¤§çš„é€šç”¨å³æ’å³ç”¨ç‰¹å¾æå–å™¨ï¼Œç”¨äºåŸºäºå†…å®¹çš„å›¾åƒæ£€ç´¢ã€‚æˆ‘ä»¬çš„è´¡çŒ®åŒ…æ‹¬ï¼šï¼ˆ1ï¼‰åœ¨åŒ…å«160ä¸‡å¼ è·¨è¶Šå››ç§æ¨¡æ€å’Œ161ç§ç—…ç†çš„2Dæ”¾å°„å›¾åƒçš„å¤§è§„æ¨¡æ•°æ®é›†ä¸Šï¼Œå¯¹ä¸€ç³»åˆ—å¤šæ ·åŒ–çš„è§†è§‰åŸºç¡€æ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼›ï¼ˆ2ï¼‰å‘ç°å¼±ç›‘ç£æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯BiomedCLIPï¼Œè¡¨ç°å‡ºé«˜åº¦æœ‰æ•ˆæ€§ï¼Œæ— éœ€é¢å¤–è®­ç»ƒå³å¯å®ç°é«˜è¾¾0.594çš„P@1ï¼ˆP@3ï¼š0.590ï¼ŒP@5ï¼š0.588ï¼ŒP@10ï¼š0.583ï¼‰ï¼Œä¸ä¸“ç”¨CBIRç³»ç»Ÿçš„æ€§èƒ½ç›¸å½“ï¼›ï¼ˆ3ï¼‰æ·±å…¥åˆ†æç´¢å¼•å¤§å°å¯¹æ£€ç´¢æ€§èƒ½çš„å½±å“ï¼›ï¼ˆ4ï¼‰è¯„ä¼°ä¸åŒæ¨¡å‹ç”Ÿæˆçš„åµŒå…¥ç©ºé—´çš„è´¨é‡ï¼›ï¼ˆ5ï¼‰ç ”ç©¶åœ¨æ£€ç´¢è§£å‰–ç»“æ„ä¸ç—…ç†ç»“æ„æ—¶é¢ä¸´çš„ç‰¹å®šæŒ‘æˆ˜ã€‚å°½ç®¡å­˜åœ¨è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬çš„ç ”ç©¶å¼ºè°ƒäº†åŸºç¡€æ¨¡å‹åœ¨æ”¾å°„ç§‘CBIRä¸­çš„å·¨å¤§æ½œåŠ›ï¼Œæå‡ºäº†å‘é€šç”¨åŒ»ç–—å›¾åƒæ£€ç´¢ç³»ç»Ÿçš„è½¬å˜ï¼Œè¿™äº›ç³»ç»Ÿä¸éœ€è¦ç‰¹å®šè°ƒæ•´å³å¯é€‚åº”å¤šç§ç”¨é€”ã€‚æˆ‘ä»¬çš„ä»£ç ã€æ•°æ®é›†åˆ†å‰²å’ŒåµŒå…¥å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/MIC-DKFZ/foundation-models-for-cbmir%E4%B8%8A%E5%85%AC%E5%BC%80%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/MIC-DKFZ/foundation-models-for-cbmirä¸Šå…¬å¼€è·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.06567v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åŸºäºå†…å®¹çš„å›¾åƒæ£€ç´¢ï¼ˆCBIRï¼‰åœ¨æ”¾å°„å­¦è¯Šæ–­è¾…åŠ©å’ŒåŒ»å­¦ç ”ç©¶ä¸­çš„åº”ç”¨æ½œåŠ›ã€‚é’ˆå¯¹å½“å‰CBIRç³»ç»Ÿä¸“ä¸šåŒ–å¯¼è‡´çš„å±€é™æ€§ï¼Œæå‡ºä½¿ç”¨é€šç”¨è§†è§‰åŸºç¡€æ¨¡å‹ä½œä¸ºåŠŸèƒ½å¼ºå¤§ä¸”é€šç”¨çš„ç¦»çº¿ç‰¹å¾æå–å™¨ï¼Œç”¨äºåŸºäºå†…å®¹çš„å›¾åƒæ£€ç´¢ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒéªŒè¯ï¼Œå‘ç°å¼±ç›‘ç£æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯BiomedCLIPï¼Œåœ¨å¤§å‹æ•°æ®é›†ä¸Šè¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œæ— éœ€é¢å¤–è®­ç»ƒå³å¯ä¸ä¸“ä¸šåŒ–CBIRç³»ç»Ÿç›¸åª²ç¾ã€‚ç ”ç©¶å¼ºè°ƒäº†åŸºç¡€æ¨¡å‹åœ¨æ”¾å°„å­¦CBIRä¸­çš„å·¨å¤§æ½œåŠ›ï¼Œæå€¡å¼€å‘é€šç”¨ã€å¤šç”¨é€”çš„åŒ»å­¦å›¾åƒæ£€ç´¢ç³»ç»Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å†…å®¹åŸºäºå›¾åƒçš„æ£€ç´¢ï¼ˆCBIRï¼‰åœ¨æ”¾å°„å­¦ä¸­æœ‰æ”¹è¿›è¯Šæ–­å’ŒåŒ»å­¦ç ”ç©¶çš„æ½œåŠ›ã€‚</li>
<li>å½“å‰CBIRç³»ç»Ÿå› ä¸“ä¸šåŒ–è€Œå¯¹æŸäº›ç—…ç†çš„å±€é™æ€§ã€‚</li>
<li>æè®®ä½¿ç”¨è§†è§‰åŸºç¡€æ¨¡å‹ä½œä¸ºå¼ºå¤§çš„é€šç”¨ç‰¹å¾æå–å™¨ï¼Œç”¨äºCBIRã€‚</li>
<li>åœ¨åŒ…å«161ç§ç—…ç†çš„1.6ç™¾ä¸‡å¼ 2Dæ”¾å°„å›¾åƒçš„å¤§è§„æ¨¡æ•°æ®é›†ä¸Šï¼Œå¯¹å¤šç§è§†è§‰åŸºç¡€æ¨¡å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚</li>
<li>å¼±ç›‘ç£æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯BiomedCLIPï¼Œè¡¨ç°å‡ºé«˜æ•ˆæœï¼Œæ— éœ€é¢å¤–è®­ç»ƒå³å¯ä¸ä¸“ä¸šåŒ–CBIRç³»ç»Ÿç«äº‰ã€‚</li>
<li>ç ”ç©¶å‘ç°åŸºç¡€æ¨¡å‹åœ¨æ”¾å°„å­¦CBIRä¸­çš„å·¨å¤§æ½œåŠ›ï¼Œé¼“åŠ±å¼€å‘é€šç”¨ã€å¤šç”¨é€”çš„åŒ»å­¦å›¾åƒæ£€ç´¢ç³»ç»Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.06567">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1e80080d7d97bd29ed60ed64de08d1f5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-090e9128cdf9af87468f0d20f1b656e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4299be9122a1518e0ffefe7078d2f6c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b58a42c9f885cfcc87decb1169b8660.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-621fc0925a9dc47f5e2860c651ca9999.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-28/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-28/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-28/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-92e0a1d0fefc5180c1e10867b30f82af.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-28  A Multi-Stage Framework for Multimodal Controllable Speech Synthesis
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-28
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-28/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-2f838e90ed63bb3599ceacd153b10ac8.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-28  SmoothSinger A Conditional Diffusion Model for Singing Voice Synthesis   with Multi-Resolution Architecture
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-28
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">31086.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
