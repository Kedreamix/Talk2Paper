<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Agent">
    <meta name="description" content="Agent 方向最新论文已更新，请持续关注 Update in 2025-06-28  Mind2Web 2 Evaluating Agentic Search with Agent-as-a-Judge">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Agent | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-606272d05a87c0533b51ed6302057f9f.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Agent</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Agent/">
                                <span class="chip bg-color">Agent</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                Agent
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-06-28
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    14.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    58 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-06-28-更新"><a href="#2025-06-28-更新" class="headerlink" title="2025-06-28 更新"></a>2025-06-28 更新</h1><h2 id="Mind2Web-2-Evaluating-Agentic-Search-with-Agent-as-a-Judge"><a href="#Mind2Web-2-Evaluating-Agentic-Search-with-Agent-as-a-Judge" class="headerlink" title="Mind2Web 2: Evaluating Agentic Search with Agent-as-a-Judge"></a>Mind2Web 2: Evaluating Agentic Search with Agent-as-a-Judge</h2><p><strong>Authors:Boyu Gou, Zanming Huang, Yuting Ning, Yu Gu, Michael Lin, Weijian Qi, Andrei Kopanev, Botao Yu, Bernal Jiménez Gutiérrez, Yiheng Shu, Chan Hee Song, Jiaman Wu, Shijie Chen, Hanane Nour Moussa, Tianshu Zhang, Jian Xie, Yifei Li, Tianci Xue, Zeyi Liao, Kai Zhang, Boyuan Zheng, Zhaowei Cai, Viktor Rozgic, Morteza Ziyadi, Huan Sun, Yu Su</strong></p>
<p>Agentic search such as Deep Research systems, where large language models autonomously browse the web, synthesize information, and return comprehensive citation-backed answers, represents a major shift in how users interact with web-scale information. While promising greater efficiency and cognitive offloading, the growing complexity and open-endedness of agentic search have outpaced existing evaluation benchmarks and methodologies, which largely assume short search horizons and static answers. In this paper, we introduce Mind2Web 2, a benchmark of 130 realistic, high-quality, and long-horizon tasks that require real-time web browsing and extensive information synthesis, constructed with over 1,000 hours of human labor. To address the challenge of evaluating time-varying and complex answers, we propose a novel Agent-as-a-Judge framework. Our method constructs task-specific judge agents based on a tree-structured rubric design to automatically assess both answer correctness and source attribution. We conduct a comprehensive evaluation of nine frontier agentic search systems and human performance, along with a detailed error analysis to draw insights for future development. The best-performing system, OpenAI Deep Research, can already achieve 50-70% of human performance while spending half the time, showing a great potential. Altogether, Mind2Web 2 provides a rigorous foundation for developing and benchmarking the next generation of agentic search systems. </p>
<blockquote>
<p>基于深度研究系统（Deep Research systems）的agentic搜索（如自主浏览网络、综合信息和返回以引用为支撑的全面答案等），代表了用户与网页规模信息交互方式的一次重大转变。尽管它带来了更高的效率和认知减负，但agentic搜索日益增长的复杂性和开放性超出了现有的评估基准和方法论，这些方法主要假设搜索视野较短且答案静态。在本文中，我们介绍了Mind2Web 2，这是一组包含130个真实、高质量、长时间视野的任务基准测试，需要实时网页浏览和广泛的信息综合，借助超过1000个小时的人力构建而成。为了应对评估随时间变化和复杂答案的挑战，我们提出了新型的Agent-as-a-Judge框架。我们的方法基于树状评分设计构建特定任务的判断代理，以自动评估答案的正确性和来源归属。我们对九个前沿的agentic搜索系统和人类性能进行了全面评估，并进行了详细的误差分析，以获取未来发展的见解。表现最佳的OpenAI Deep Research系统已经达到了人类性能的50%-70%，同时所花费的时间仅为人类的一半，显示出巨大的潜力。总之，Mind2Web 2为下一代agentic搜索系统的开发和基准测试提供了严格的基础。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.21506v1">PDF</a> Project Homepage: <a target="_blank" rel="noopener" href="https://osu-nlp-group.github.io/Mind2Web2/">https://osu-nlp-group.github.io/Mind2Web2/</a></p>
<p><strong>Summary</strong></p>
<p>大型语言模型自主浏览网络、合成信息并返回综合引用答案的搜索方式，代表着用户与网络规模信息交互的重大转变。然而，这种转变带来的复杂性和开放性超出了现有的评估基准和方法论，这些方法主要假设搜索视野较短且答案静态。为解决此问题，本文引入Mind2Web 2基准测试，包含130个需要实时网络浏览和大量信息合成的真实、高质量、长期任务，通过超过1000小时的人力构建而成。为评估随时间变化和复杂答案，我们提出新颖的“Agent作为法官”框架，基于树形评分设计构建特定任务的评估代理，自动评估答案的正确性和来源归属。我们对前沿的九个代理搜索系统和人类表现进行了全面评估，并进行详细错误分析以指导未来发展。最佳性能的OpenAI Deep Research系统已接近人类表现的50%-70%，同时耗时减半，显示出巨大潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型自主浏览网络并返回综合答案代表搜索方式的重要转变。</li>
<li>现有评估基准和方法论无法适应这种转变带来的复杂性和开放性。</li>
<li>Mind2Web 2基准测试包含真实、高质量、长期任务，用于评估代理搜索系统的性能。</li>
<li>引入“Agent作为法官”框架，以自动评估答案的正确性和来源归属。</li>
<li>对九个前沿代理搜索系统和人类表现进行了全面评估。</li>
<li>最佳性能的OpenAI Deep Research系统已接近人类表现的50%-70%，显示巨大潜力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.21506">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-07e2de5f2937cb150b5794be520c4d32.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48c61a4bba35cdf7b42718f538dee9f4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d8d976ca76f1ce8fbeae0d5572c9fee9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d31848de681e0baf7e3363cb8d0899ef.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Agent-RewardBench-Towards-a-Unified-Benchmark-for-Reward-Modeling-across-Perception-Planning-and-Safety-in-Real-World-Multimodal-Agents"><a href="#Agent-RewardBench-Towards-a-Unified-Benchmark-for-Reward-Modeling-across-Perception-Planning-and-Safety-in-Real-World-Multimodal-Agents" class="headerlink" title="Agent-RewardBench: Towards a Unified Benchmark for Reward Modeling   across Perception, Planning, and Safety in Real-World Multimodal Agents"></a>Agent-RewardBench: Towards a Unified Benchmark for Reward Modeling   across Perception, Planning, and Safety in Real-World Multimodal Agents</h2><p><strong>Authors:Tianyi Men, Zhuoran Jin, Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao</strong></p>
<p>As Multimodal Large Language Models (MLLMs) advance, multimodal agents show promise in real-world tasks like web navigation and embodied intelligence. However, due to limitations in a lack of external feedback, these agents struggle with self-correction and generalization. A promising approach is to use reward models as external feedback, but there is no clear on how to select reward models for agents. Thus, there is an urgent need to build a reward bench targeted at agents. To address these challenges, we propose Agent-RewardBench, a benchmark designed to evaluate reward modeling ability in MLLMs. The benchmark is characterized by three key features: (1) Multiple dimensions and real-world agent scenarios evaluation. It covers perception, planning, and safety with 7 scenarios; (2) Step-level reward evaluation. It allows for the assessment of agent capabilities at the individual steps of a task, providing a more granular view of performance during the planning process; and (3) Appropriately difficulty and high-quality. We carefully sample from 10 diverse models, difficulty control to maintain task challenges, and manual verification to ensure the integrity of the data. Experiments demonstrate that even state-of-the-art multimodal models show limited performance, highlighting the need for specialized training in agent reward modeling. Code is available at github. </p>
<blockquote>
<p>随着多模态大型语言模型（MLLMs）的不断发展，多模态代理在网页导航和体现智能等现实任务中显示出巨大的潜力。然而，由于缺乏外部反馈，这些代理在自我纠正和泛化方面存在困难。使用奖励模型作为外部反馈是一种有前途的方法，但对于如何为代理选择奖励模型还没有明确的方法。因此，迫切需要构建一个面向代理的奖励基准。为了解决这些挑战，我们提出了Agent-RewardBench，这是一个旨在评估MLLM中奖励建模能力的新基准。该基准的特点主要体现在三个方面：1）多维度和现实世界代理场景评估。它涵盖了感知、规划和安全，包括7种场景；2）步骤级奖励评估。它允许对代理在任务各个步骤中的能力进行评估，为规划过程中的性能提供更详细的视图；3）适当的难度和高质量。我们从10个不同的模型中精心抽样，控制难度以保持任务挑战，并手动验证以确保数据的完整性。实验表明，即使是最先进的多模态模型也表现出有限的性能，这突显了对代理奖励建模进行专门训练的需要。代码可在GitHub上获得。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.21252v1">PDF</a> ACL 2025 Main</p>
<p><strong>Summary</strong></p>
<p>随着多模态大型语言模型（MLLMs）的发展，多模态代理在诸如网络导航和身体智能等现实任务中展现出潜力。然而，由于缺乏外部反馈，这些代理在自我修正和泛化方面存在局限性。使用奖励模型作为外部反馈是一种有前途的方法，但目前尚不清楚如何为代理选择奖励模型。因此，急需建立一个面向代理的奖励基准测试。本文提出了面向多模态语言模型的奖励建模能力评估的Agent-RewardBench基准测试。该基准测试具有三个关键特征：多维度和现实世界代理场景评估，涵盖感知、规划和安全共七个场景；步骤级奖励评估，允许对代理任务的各个步骤进行评估，提供规划过程中的更精细的性能视图；以及适当的难度和高品质。实验表明，即使是最新颖的多模态模型性能也有限，这凸显了对代理奖励模型的专项训练需求。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态大型语言模型（MLLMs）在现实世界任务中展现出潜力，如网络导航和身体智能。</li>
<li>由于缺乏外部反馈，多模态代理在自我修正和泛化方面存在局限性。</li>
<li>使用奖励模型作为外部反馈是改善这一问题的有前途的方法。</li>
<li>建立一个面向代理的奖励基准测试（Agent-RewardBench）是解决当前挑战的关键。</li>
<li>Agent-RewardBench基准测试具有三个关键特征：多维度场景评估、步骤级奖励评估以及适当的难度和高品质。</li>
<li>实验表明，当前多模态模型的性能有限，需要专项训练来提高代理奖励建模能力。</li>
<li>代码已公开在GitHub上。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.21252">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-eeb30fe7c7f9a2c1168890e5226cefca.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6d9008643cd066b5e5eef9160ef0f2d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca584cc19fa17f5533de84dd5ab71f75.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef0099dd208c25303f2e060394f5689a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0b0f6cd5a64667e01f4c9a47c55dbaa6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d7a215ded2fa011172d9d71cf35b19e9.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Homogenization-of-Multi-agent-Learning-Dynamics-in-Finite-state-Markov-Games"><a href="#Homogenization-of-Multi-agent-Learning-Dynamics-in-Finite-state-Markov-Games" class="headerlink" title="Homogenization of Multi-agent Learning Dynamics in Finite-state Markov   Games"></a>Homogenization of Multi-agent Learning Dynamics in Finite-state Markov   Games</h2><p><strong>Authors:Yann Kerzreho</strong></p>
<p>This paper introduces a new approach for approximating the learning dynamics of multiple reinforcement learning (RL) agents interacting in a finite-state Markov game. The idea is to rescale the learning process by simultaneously reducing the learning rate and increasing the update frequency, effectively treating the agent’s parameters as a slow-evolving variable influenced by the fast-mixing game state. Under mild assumptions-ergodicity of the state process and continuity of the updates-we prove the convergence of this rescaled process to an ordinary differential equation (ODE). This ODE provides a tractable, deterministic approximation of the agent’s learning dynamics. An implementation of the framework is available at,: <a target="_blank" rel="noopener" href="https://github.com/yannKerzreho/MarkovGameApproximation">https://github.com/yannKerzreho/MarkovGameApproximation</a> </p>
<blockquote>
<p>本文介绍了一种新的方法，用于近似有限状态马尔可夫游戏中多个强化学习（RL）代理的学习动态交互。该思想是通过同时减小学习率和增加更新频率来调整学习过程，有效地将代理参数视为受快速混合游戏状态影响的缓慢变化变量。在状态过程的遍历性和更新的连续性等温和假设下，我们证明了这种调整后的过程收敛到一个常微分方程（ODE）。这个ODE提供了一个易于处理、确定的代理学习动态的近似值。该框架的实现可通过以下网址获取：<a target="_blank" rel="noopener" href="https://github.com/yannKerzreho/MarkovGameApproximation">https://github.com/yannKerzreho/MarkovGameApproximation</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.21079v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该论文提出了一种新的方法，用于近似多个强化学习（RL）代理在有限状态马尔可夫博弈中的学习动态。其主要思想是通过同时减小学习率和增加更新频率来调整学习过程。在该框架下，代理参数被视为由快速混合游戏状态所驱动的缓慢变化变量。在状态过程的遍历性和更新连续性的温和假设下，我们证明了这种调整后的过程收敛到一个常微分方程（ODE）。这个ODE为代理的学习动态提供了一个易于处理的确定性近似。有关该框架的实现在特定的网址上有详细介绍。该网址为：<a target="_blank" rel="noopener" href="https://github.com/yannKerzreho/MarkovGameApproximation">github.com&#x2F;yannKerzreho&#x2F;MarkovGameApproximation</a>。</p>
<p><strong>Key Takeaways</strong></p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.21079">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-872841dbf1d04791b6d29c51f53a0c24.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Evidence-based-diagnostic-reasoning-with-multi-agent-copilot-for-human-pathology"><a href="#Evidence-based-diagnostic-reasoning-with-multi-agent-copilot-for-human-pathology" class="headerlink" title="Evidence-based diagnostic reasoning with multi-agent copilot for human   pathology"></a>Evidence-based diagnostic reasoning with multi-agent copilot for human   pathology</h2><p><strong>Authors:Chengkuan Chen, Luca L. Weishaupt, Drew F. K. Williamson, Richard J. Chen, Tong Ding, Bowen Chen, Anurag Vaidya, Long Phi Le, Guillaume Jaume, Ming Y. Lu, Faisal Mahmood</strong></p>
<p>Pathology is experiencing rapid digital transformation driven by whole-slide imaging and artificial intelligence (AI). While deep learning-based computational pathology has achieved notable success, traditional models primarily focus on image analysis without integrating natural language instruction or rich, text-based context. Current multimodal large language models (MLLMs) in computational pathology face limitations, including insufficient training data, inadequate support and evaluation for multi-image understanding, and a lack of autonomous, diagnostic reasoning capabilities. To address these limitations, we introduce PathChat+, a new MLLM specifically designed for human pathology, trained on over 1 million diverse, pathology-specific instruction samples and nearly 5.5 million question answer turns. Extensive evaluations across diverse pathology benchmarks demonstrated that PathChat+ substantially outperforms the prior PathChat copilot, as well as both state-of-the-art (SOTA) general-purpose and other pathology-specific models. Furthermore, we present SlideSeek, a reasoning-enabled multi-agent AI system leveraging PathChat+ to autonomously evaluate gigapixel whole-slide images (WSIs) through iterative, hierarchical diagnostic reasoning, reaching high accuracy on DDxBench, a challenging open-ended differential diagnosis benchmark, while also capable of generating visually grounded, humanly-interpretable summary reports. </p>
<blockquote>
<p>病理学正在经历由全切片成像和人工智能（AI）驱动的快速数字化转型。虽然基于深度学习的计算病理学已经取得了显著的成就，但传统模型主要侧重于图像分析，没有整合自然语言指令或丰富的文本上下文。当前计算病理学中的多模态大型语言模型（MLLMs）面临一些局限性，包括训练数据不足、对多图像理解的支持和评估不足，以及缺乏自主诊断推理能力。为了解决这些局限性，我们推出了PathChat+，这是一款专门用于人类病理学的新的MLLM，在超过100万个多样化的病理学特定指令样本和近550万个问答回合中进行训练。在多种病理学基准测试上的广泛评估表明，PathChat+显著优于之前的PathChat copilot，以及最先进的（SOTA）通用和其他病理学特定模型。此外，我们推出了SlideSeek，这是一个利用PathChat+的推理功能的多智能体AI系统，可自主评估千兆像素全切片图像（WSIs）通过迭代、分层诊断推理，在DDxBench这一具有挑战性的开放式鉴别诊断基准测试中达到高准确性，并能够生成视觉基础、人类可解释的摘要报告。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20964v1">PDF</a> </p>
<p><strong>Summary</strong><br>     病理学正经历由全切片成像和人工智能驱动的数字化快速转型。虽然深度学习在计算病理学方面取得了显著成功，但传统模型主要关注图像分析，并未整合自然语言指令或丰富的文本背景。针对当前多模式大型语言模型在计算病理学方面面临的训练数据不足、多图像理解支持及评估不足、缺乏自主诊断推理能力等局限，我们推出了专为病理学设计的PathChat+模型。该模型在超过100万份病理学特定指令样本和近550万个问答对话回合中进行训练。在多种病理学基准测试上的广泛评估表明，PathChat+显著优于之前的PathChat助手，以及其他最先进的通用和病理学特定模型。此外，我们还推出了SlideSeek，这是一个利用PathChat+的推理赋能多智能体AI系统，可自主评估千兆像素全切片图像，通过迭代分层诊断推理达到高准确率，同时在开放的差异诊断基准测试DDxBench上表现优异，并能生成视觉化、可被人理解的总结报告。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>病理学正在经历数字化转型，全切片成像和人工智能在其中起到关键作用。</li>
<li>当前计算病理学领域的多模式大型语言模型面临多方面的挑战，包括训练数据不足、多图像理解支持不足等。</li>
<li>PathChat+模型的出现解决了上述问题，并在多种病理学基准测试中表现出优异的性能。</li>
<li>PathChat+模型经过大量病理学特定指令样本和问答对话回合的训练，设计更贴合病理学领域。</li>
<li>SlideSeek是一个利用PathChat+的多智能体AI系统，可自主评估全切片图像，并通过迭代分层诊断推理达到高准确率。</li>
<li>SlideSeek在差异诊断基准测试上表现优异，能够生成人类可理解的报告。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20964">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3c9772c4bbffff405f228790a5321e72.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dbb2a9dca720db7f53f8b0c4d040987f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-609fee2aae0bdf34158314af033e5db2.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="FaSTA-Fast-Slow-Toolpath-Agent-with-Subroutine-Mining-for-Efficient-Multi-turn-Image-Editing"><a href="#FaSTA-Fast-Slow-Toolpath-Agent-with-Subroutine-Mining-for-Efficient-Multi-turn-Image-Editing" class="headerlink" title="FaSTA$^*$: Fast-Slow Toolpath Agent with Subroutine Mining for Efficient   Multi-turn Image Editing"></a>FaSTA$^*$: Fast-Slow Toolpath Agent with Subroutine Mining for Efficient   Multi-turn Image Editing</h2><p><strong>Authors:Advait Gupta, Rishie Raj, Dang Nguyen, Tianyi Zhou</strong></p>
<p>We develop a cost-efficient neurosymbolic agent to address challenging multi-turn image editing tasks such as “Detect the bench in the image while recoloring it to pink. Also, remove the cat for a clearer view and recolor the wall to yellow.’’ It combines the fast, high-level subtask planning by large language models (LLMs) with the slow, accurate, tool-use, and local A$^*$ search per subtask to find a cost-efficient toolpath – a sequence of calls to AI tools. To save the cost of A$^*$ on similar subtasks, we perform inductive reasoning on previously successful toolpaths via LLMs to continuously extract&#x2F;refine frequently used subroutines and reuse them as new tools for future tasks in an adaptive fast-slow planning, where the higher-level subroutines are explored first, and only when they fail, the low-level A$^*$ search is activated. The reusable symbolic subroutines considerably save exploration cost on the same types of subtasks applied to similar images, yielding a human-like fast-slow toolpath agent “FaSTA$^*$’’: fast subtask planning followed by rule-based subroutine selection per subtask is attempted by LLMs at first, which is expected to cover most tasks, while slow A$^*$ search is only triggered for novel and challenging subtasks. By comparing with recent image editing approaches, we demonstrate FaSTA$^*$ is significantly more computationally efficient while remaining competitive with the state-of-the-art baseline in terms of success rate. </p>
<blockquote>
<p>我们开发了一种具有成本效益的神经符号代理，用于解决具有挑战性的多轮图像编辑任务，例如“在图像中检测长凳并将其重新着色为粉红色。另外，移除猫以获得更清晰的视图，并将墙壁重新着色为黄色。”它结合了大型语言模型（LLM）的快速、高级子任务规划与缓慢、准确、工具使用和针对每个子任务的局部A<em>搜索，以找到具有成本效益的工具路径——一系列对AI工具的调用。为了节省在类似子任务上的A</em>成本，我们通过LLM对先前成功的工具路径进行归纳推理，以不断提取&#x2F;完善常用的子程序，并将其重新用作未来任务的新工具，在自适应快慢规划中，首先探索高级子程序，只有在它们失败时，才激活低级的A<em>搜索。可重复使用的符号子程序大大节省了在相似图像上应用相同类型子任务的探索成本，从而产生了一个人类般的快慢工具路径代理“FaSTA</em>”：首先是LLM尝试的快速子任务规划，然后是每个子任务的基于规则的子程序选择，这预计会涵盖大多数任务，而缓慢的A<em>搜索仅针对新的和具有挑战性的子任务触发。通过与最近的图像编辑方法进行比较，我们证明了FaSTA</em>在计算效率上显著更高，同时在成功率方面与最新基线保持竞争力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20911v1">PDF</a> </p>
<p><strong>Summary</strong>：</p>
<p>我们开发了一种成本效益高的神经符号代理，用于解决具有挑战性的多轮图像编辑任务，如检测和重色图像中的板凳同时为图像去除猫咪并给墙壁重新着色等。它结合了大型语言模型的快速高级子任务规划与慢速准确的工具使用和针对每个子任务的局部A<em>（Star）搜索来寻找高效工具路径（即AI工具的调用序列）。我们通过使用LLM对以前成功的工具路径进行归纳推理来节省类似子任务的成本，并在此基础上构建了一种自适应的快速慢速规划方法，该方法首先探索高级子程序，仅在它们失败时才激活低级的A</em>（Star）搜索。这种灵活的符号子程序能够节省同一类型子任务在类似图像上的探索成本，从而创建了一种人类式的快速慢速工具路径代理“FaSTA<em>（快速星）”。大多数任务首先尝试由LLM进行的快速子任务规划，随后根据每个子任务选择基于规则的子程序，只有在遇到新颖且具有挑战性的子任务时才触发慢速的A</em>（Star）搜索。相较于近期的图像编辑方法，FaSTA*在保持高成功率的同时，计算效率显著提高。</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>开发了一种名为FaSTA*（快速星）的成本效益高的神经符号代理用于处理图像编辑任务。</li>
<li>FaSTA<em>结合了大型语言模型的快速高级子任务规划与慢速的A</em>（Star）搜索来寻找高效工具路径。</li>
<li>使用LLM对成功的工具路径进行归纳推理以节省成本并提高处理效率。</li>
<li>FaSTA*具备自适应的快速慢速规划方法，可优先探索高级子程序并在必要时触发低级的详细搜索。</li>
<li>与其他图像编辑方法相比，FaSTA*在计算效率上显著提高，同时保持高成功率。</li>
<li>FaSTA*具有处理复杂和多变的图像编辑任务的能力，如检测和重色图像中的特定物体以及去除特定元素等。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20911">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-1e1d8cf1ff5387609086ba8a93b0cd0a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a76738f64542defe5aec2fee9785e2ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f4393a477c71b33b8dc4a71d3f6b99c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd3761230c0c78345f9b3b38da57370e.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MAGPIE-A-dataset-for-Multi-AGent-contextual-PrIvacy-Evaluation"><a href="#MAGPIE-A-dataset-for-Multi-AGent-contextual-PrIvacy-Evaluation" class="headerlink" title="MAGPIE: A dataset for Multi-AGent contextual PrIvacy Evaluation"></a>MAGPIE: A dataset for Multi-AGent contextual PrIvacy Evaluation</h2><p><strong>Authors:Gurusha Juneja, Alon Albalak, Wenyue Hua, William Yang Wang</strong></p>
<p>The proliferation of LLM-based agents has led to increasing deployment of inter-agent collaboration for tasks like scheduling, negotiation, resource allocation etc. In such systems, privacy is critical, as agents often access proprietary tools and domain-specific databases requiring strict confidentiality. This paper examines whether LLM-based agents demonstrate an understanding of contextual privacy. And, if instructed, do these systems preserve inference time user privacy in non-adversarial multi-turn conversation. Existing benchmarks to evaluate contextual privacy in LLM-agents primarily assess single-turn, low-complexity tasks where private information can be easily excluded. We first present a benchmark - MAGPIE comprising 158 real-life high-stakes scenarios across 15 domains. These scenarios are designed such that complete exclusion of private data impedes task completion yet unrestricted information sharing could lead to substantial losses. We then evaluate the current state-of-the-art LLMs on (a) their understanding of contextually private data and (b) their ability to collaborate without violating user privacy. Empirical experiments demonstrate that current models, including GPT-4o and Claude-2.7-Sonnet, lack robust understanding of contextual privacy, misclassifying private data as shareable 25.2% and 43.6% of the time. In multi-turn conversations, these models disclose private information in 59.9% and 50.5% of cases even under explicit privacy instructions. Furthermore, multi-agent systems fail to complete tasks in 71% of scenarios. These results underscore that current models are not aligned towards both contextual privacy preservation and collaborative task-solving. </p>
<blockquote>
<p>基于大型语言模型（LLM）的代理的普及导致越来越多地使用跨代理协作来完成诸如调度、谈判、资源分配等任务。在这样的系统中，隐私至关重要，因为代理通常会访问需要严格保密的专有工具和特定领域的数据库。本文旨在研究基于LLM的代理是否表现出对上下文隐私的理解。并且，如果受到指示，这些系统在非对抗性多轮对话中是否能在推理时间保护用户隐私。现有用于评估LLM代理中上下文隐私的基准测试主要评估低复杂度、单回合的任务，在这些任务中可以很容易地排除私有信息。我们首先提出了一个基准测试MAGPIE，它包含跨越15个领域的158个现实生活中的高风险场景。这些场景的设计目的是，完全排除私有数据会阻碍任务完成，但无限制的信息共享可能导致重大损失。然后我们对当前最先进的LLM进行了评估：（a）它们对上下文隐私数据的理解；（b）它们在遵守用户隐私的前提下进行协作的能力。实证实验表明，包括GPT-4o和Claude-2.7-Sonnet在内的当前模型缺乏对上下文隐私的稳健理解，它们将私有数据错误地归类为可共享数据的频率高达25.2％和43.6％。在多轮对话中，这些模型在明确隐私指令的情况下，仍有高达59.9％和50.5％的情况泄露了私人信息。此外，在多代理系统中，有高达71％的场景无法完成任务。这些结果强调，当前模型并没有很好地实现上下文隐私保护和协作任务解决之间的平衡。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20737v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>随着LLM基于的代理人的增多，代理人的协作任务（如调度、谈判、资源分配等）也越来越频繁。在这些系统中，隐私至关重要，因为代理人经常访问需要严格保密的专有工具和特定领域的数据库。本文旨在研究LLM是否理解语境隐私并在受到指示时保护推理时间用户的隐私。当前对LLM代理人的隐私评估主要局限于简单的单一任务场景，忽视了实际使用中的复杂性。本文提出MAGPIE评估标准，涵盖真实生活中的高风险场景。实证实验表明，现有模型在理解语境隐私和协作过程中保护用户隐私方面存在不足。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM代理人的普及促进了多代理人协作任务的增长，如调度、谈判和资源分配等。</li>
<li>在这些协作系统中，隐私保护至关重要，因为代理人需要访问专有工具和特定数据库。</li>
<li>当前对LLM理解语境隐私能力的评估标准主要限于简单的单一任务场景。</li>
<li>本文提出了MAGPIE评估标准，涵盖了真实生活中的高风险场景以更好地评估LLM的表现。</li>
<li>实证实验表明，现有LLM模型在理解语境隐私方面存在不足，容易误判私人信息的可分享性。</li>
<li>在多轮对话中，这些模型在受到明确隐私指示时仍会泄露私人信息。</li>
<li>多代理人系统在完成某些任务方面的效率有待提高，特别是在涉及隐私保护的场景中。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20737">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-258774a6f38b2f9f99c29f13ac3b980b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ebd1e3a2ca8b87adf0919a5e2ee294fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1ff4c82b75f28b70ec6133031d0ef22a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4ec59ae2cbc901d27cb8bcd5fc0b5d60.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="From-Web-Search-towards-Agentic-Deep-Research-Incentivizing-Search-with-Reasoning-Agents"><a href="#From-Web-Search-towards-Agentic-Deep-Research-Incentivizing-Search-with-Reasoning-Agents" class="headerlink" title="From Web Search towards Agentic Deep Research: Incentivizing Search with   Reasoning Agents"></a>From Web Search towards Agentic Deep Research: Incentivizing Search with   Reasoning Agents</h2><p><strong>Authors:Weizhi Zhang, Yangning Li, Yuanchen Bei, Junyu Luo, Guancheng Wan, Liangwei Yang, Chenxuan Xie, Yuyao Yang, Wei-Chieh Huang, Chunyu Miao, Henry Peng Zou, Xiao Luo, Yusheng Zhao, Yankai Chen, Chunkit Chan, Peilin Zhou, Xinyang Zhang, Chenwei Zhang, Jingbo Shang, Ming Zhang, Yangqiu Song, Irwin King, Philip S. Yu</strong></p>
<p>Information retrieval is a cornerstone of modern knowledge acquisition, enabling billions of queries each day across diverse domains. However, traditional keyword-based search engines are increasingly inadequate for handling complex, multi-step information needs. Our position is that Large Language Models (LLMs), endowed with reasoning and agentic capabilities, are ushering in a new paradigm termed Agentic Deep Research. These systems transcend conventional information search techniques by tightly integrating autonomous reasoning, iterative retrieval, and information synthesis into a dynamic feedback loop. We trace the evolution from static web search to interactive, agent-based systems that plan, explore, and learn. We also introduce a test-time scaling law to formalize the impact of computational depth on reasoning and search. Supported by benchmark results and the rise of open-source implementations, we demonstrate that Agentic Deep Research not only significantly outperforms existing approaches, but is also poised to become the dominant paradigm for future information seeking. All the related resources, including industry products, research papers, benchmark datasets, and open-source implementations, are collected for the community in <a target="_blank" rel="noopener" href="https://github.com/DavidZWZ/Awesome-Deep-Research">https://github.com/DavidZWZ/Awesome-Deep-Research</a>. </p>
<blockquote>
<p>信息检索是现代知识获取的核心基石，每天能够在不同领域处理数十亿次的查询请求。然而，传统的基于关键词的搜索引擎已经越来越不能满足复杂、多步骤的信息需求。我们的观点是，大型语言模型（LLM）赋予了推理和代理能力，正推动着一种新的范式转变，被称为代理深度研究（Agentic Deep Research）。这些系统通过紧密集成自主推理、迭代检索和信息合成到一个动态反馈循环中，从而超越了传统的信息搜索技术。我们追溯了从静态网页搜索到互动、基于代理的系统的演变，这些系统可以计划、探索和学习的历程。我们还引入了一个测试时间尺度定律，以正式计算深度对推理和搜索的影响。在基准测试结果的支持下，以及开源实现的兴起下，我们证明了代理深度研究不仅显著优于现有方法，而且有望成为未来信息搜索的主导范式。所有相关资源，包括工业产品、研究论文、基准数据集和开源实现，都已收集在<a target="_blank" rel="noopener" href="https://github.com/DavidZWZ/Awesome-Deep-Research%EF%BC%8C%E4%BE%9B%E7%A4%BE%E5%8C%BA%E4%BD%BF%E7%94%A8%E3%80%82">https://github.com/DavidZWZ/Awesome-Deep-Research，供社区使用。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18959v2">PDF</a> </p>
<p><strong>Summary</strong><br>信息检索是现代知识获取的核心，每天处理数十亿次的查询请求，涵盖各种领域。然而，传统的基于关键词的搜索引擎在处理复杂、多步骤的信息需求时越来越显得不足。大型语言模型（LLM）的出现，带来了名为Agentic Deep Research的新范式，该系统通过紧密集成自主推理、迭代检索和信息合成，进入一个动态反馈循环，超越了传统信息搜索技术。我们追踪了从静态网页搜索到交互式、基于代理的系统的演变，这些系统可以计划、探索和自主学习。我们还引入了一个测试时的尺度定律，以正式计算深度对推理和搜索的影响。受基准测试结果和开源实现的兴起支持，我们证明Agentic Deep Research不仅显著优于现有方法，而且已成为未来信息搜索的主导范式。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>信息检索在现代知识获取中扮演重要角色，每天处理大量查询请求。</li>
<li>传统搜索引擎在处理复杂、多步骤信息需求时存在不足。</li>
<li>大型语言模型（LLM）的出现引领了Agentic Deep Research新范式。</li>
<li>Agentic Deep Research通过集成自主推理、迭代检索和信息合成，超越了传统信息搜索技术。</li>
<li>Agentic Deep Research系统具有计划、探索和自主学习的能力。</li>
<li>测试时的尺度定律用于描述计算深度对推理和搜索的影响。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18959">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-606272d05a87c0533b51ed6302057f9f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-76a584b347a5f8a48347111db296ccc0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-155280869ad2bd126b54c3da5ebc7cd6.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="xChemAgents-Agentic-AI-for-Explainable-Quantum-Chemistry"><a href="#xChemAgents-Agentic-AI-for-Explainable-Quantum-Chemistry" class="headerlink" title="xChemAgents: Agentic AI for Explainable Quantum Chemistry"></a>xChemAgents: Agentic AI for Explainable Quantum Chemistry</h2><p><strong>Authors:Can Polat, Mehmet Tuncel, Mustafa Kurban, Erchin Serpedin, Hasan Kurban</strong></p>
<p>Recent progress in multimodal graph neural networks has demonstrated that augmenting atomic XYZ geometries with textual chemical descriptors can enhance predictive accuracy across a range of electronic and thermodynamic properties. However, naively appending large sets of heterogeneous descriptors often degrades performance on tasks sensitive to molecular shape or symmetry, and undermines interpretability. xChemAgents proposes a cooperative agent framework that injects physics-aware reasoning into multimodal property prediction. xChemAgents comprises two language-model-based agents: a Selector, which adaptively identifies a sparse, weighted subset of descriptors relevant to each target, and provides a natural language rationale; and a Validator, which enforces physical constraints such as unit consistency and scaling laws through iterative dialogue. On standard benchmark datasets, xChemAgents achieves up to a 22% reduction in mean absolute error over the state-of-the-art baselines, while producing faithful, human-interpretable explanations. Experiment results highlight the potential of cooperative, self-verifying agents to enhance both accuracy and transparency in foundation-model-driven materials science. The implementation and accompanying dataset are available at <a target="_blank" rel="noopener" href="https://github.com/KurbanIntelligenceLab/xChemAgents">https://github.com/KurbanIntelligenceLab/xChemAgents</a>. </p>
<blockquote>
<p>近期在多模态图神经网络方面的进展表明，通过文本化学描述符增强原子XYZ几何结构可以提高一系列电子和热力学属性的预测精度。然而，盲目添加大量异质描述符通常会降低对分子形状或对称性敏感的任务性能，并破坏可解释性。xChemAgents提出了一种协作代理框架，该框架将物理感知推理注入多模态属性预测中。xChemAgents包含两个基于语言模型的代理：Selector，它自适应地识别与每个目标相关的稀疏加权描述符子集，并提供自然语言依据；以及Validator，它通过迭代对话强制执行单位一致性等物理约束和标度定律。在标准基准数据集上，xChemAgents的均方根误差比最新基线技术降低了高达22%，同时产生忠实且人类可解释的解释。实验结果突出了协作、自我验证的代理在基础模型驱动的材料科学中提高准确性和透明度的潜力。相关实现和伴随数据集可在<a target="_blank" rel="noopener" href="https://github.com/KurbanIntelligenceLab/xChemAgents%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/KurbanIntelligenceLab/xChemAgents找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20574v2">PDF</a> Accepted Paper at ICML 2025 Workshop on MAS</p>
<p><strong>Summary</strong></p>
<p>基于多模态图神经网络的新进展，通过结合原子XYZ几何与文本化学描述符，提高了电子和热力学属性的预测精度。然而，简单添加大量异质描述符会损害对分子形状或对称性敏感的任务性能并降低解释性。xChemAgents提出一种合作代理框架，将物理感知推理注入多模态属性预测中。该框架包含两个基于语言模型的代理：选择器，可自适应识别与目标相关的稀疏加权描述符集，并提供自然语言依据；验证器，通过迭代对话强制执行单位一致性等物理约束。在标准基准测试数据集上，xChemAgents较最新基线技术实现了平均绝对误差最多减少22%，同时产生忠实且易于理解的人类解释。实验结果突显了合作、自我验证的代理在提高基础模型驱动的材料科学准确性和透明度方面的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态图神经网络结合原子XYZ几何与文本化学描述符，提升了电子和热力学属性的预测精度。</li>
<li>简单地添加大量异质描述符可能会影响分子形状或对称性相关的任务性能，并降低模型解释性。</li>
<li>xChemAgents提出合作代理框架，包含选择器和验证器，分别进行关键描述符的筛选和物理约束的验证。</li>
<li>选择器能够自适应识别与目标相关的稀疏加权描述符，并提供自然语言解释。</li>
<li>验证器通过迭代对话强制执行物理约束，如单位一致性等。</li>
<li>在标准数据集上，xChemAgents较现有技术显著提高了预测准确性，并提供了可解释的解释。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20574">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-1300e4c96daefab12f9fc1c073de7907.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1414eebd4ec8e2493e581ad43afe4530.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f152dd18fe370cb1308ac2b622fade1c.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="LLM-Based-Human-Agent-Collaboration-and-Interaction-Systems-A-Survey"><a href="#LLM-Based-Human-Agent-Collaboration-and-Interaction-Systems-A-Survey" class="headerlink" title="LLM-Based Human-Agent Collaboration and Interaction Systems: A Survey"></a>LLM-Based Human-Agent Collaboration and Interaction Systems: A Survey</h2><p><strong>Authors:Henry Peng Zou, Wei-Chieh Huang, Yaozu Wu, Yankai Chen, Chunyu Miao, Hoang Nguyen, Yue Zhou, Weizhi Zhang, Liancheng Fang, Langzhou He, Yangning Li, Dongyuan Li, Renhe Jiang, Xue Liu, Philip S. Yu</strong></p>
<p>Recent advances in large language models (LLMs) have sparked growing interest in building fully autonomous agents. However, fully autonomous LLM-based agents still face significant challenges, including limited reliability due to hallucinations, difficulty in handling complex tasks, and substantial safety and ethical risks, all of which limit their feasibility and trustworthiness in real-world applications. To overcome these limitations, LLM-based human-agent systems (LLM-HAS) incorporate human-provided information, feedback, or control into the agent system to enhance system performance, reliability and safety. These human-agent collaboration systems enable humans and LLM-based agents to collaborate effectively by leveraging their complementary strengths. This paper provides the first comprehensive and structured survey of LLM-HAS. It clarifies fundamental concepts, systematically presents core components shaping these systems, including environment &amp; profiling, human feedback, interaction types, orchestration and communication, explores emerging applications, and discusses unique challenges and opportunities arising from human-AI collaboration. By consolidating current knowledge and offering a structured overview, we aim to foster further research and innovation in this rapidly evolving interdisciplinary field. Paper lists and resources are available at <a target="_blank" rel="noopener" href="https://github.com/HenryPengZou/Awesome-Human-Agent-Collaboration-Interaction-Systems">https://github.com/HenryPengZou/Awesome-Human-Agent-Collaboration-Interaction-Systems</a>. </p>
<blockquote>
<p>近期大型语言模型（LLM）的进展引发了人们对构建完全自主代理人的浓厚兴趣。然而，基于LLM的完全自主代理人仍然面临重大挑战，包括由于幻觉导致的可靠性有限、处理复杂任务的困难以及实质性的安全和道德风险，这些都限制了它们在现实世界应用中的可行性和可信度。为了克服这些局限性，LLM基于的人机代理系统（LLM-HAS）将人类提供的资讯、反馈或控制融入代理系统，以提升系统的性能、可靠性和安全性。这些人机协作系统利用人和基于LLM的代理人的互补优势，使他们能够进行有效的协作。本文对LLM-HAS进行了首次全面和系统的调查。它明确了基本概念，系统地呈现了构成这些系统的核心组件，包括环境分析、人类反馈、交互类型、编排和沟通等，探讨了新兴应用，并讨论了由人机协作产生的独特挑战和机遇。通过整合当前知识并提供结构化概述，我们的目标是促进这一迅速发展的跨学科领域的进一步研究和创新。论文列表和资源可通过<a target="_blank" rel="noopener" href="https://github.com/HenryPengZou/Awesome-Human-Agent-Collaboration-Interaction-Systems%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/HenryPengZou/Awesome-Human-Agent-Collaboration-Interaction-Systems获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.00753v4">PDF</a> Paper lists and resources are available at   <a target="_blank" rel="noopener" href="https://github.com/HenryPengZou/Awesome-Human-Agent-Collaboration-Interaction-Systems">https://github.com/HenryPengZou/Awesome-Human-Agent-Collaboration-Interaction-Systems</a></p>
<p><strong>Summary</strong><br>大型语言模型（LLM）驱动的自主智能代理日益受到关注，但仍面临可靠性、处理复杂任务能力、安全和伦理风险等方面的挑战。为解决这些问题，LLM人类代理系统（LLM-HAS）通过结合人类提供的信息、反馈或控制，增强了系统的性能、可靠性和安全性。该系统能利用人类和LLM智能代理的优势进行有效协作。本文是对LLM-HAS的首个全面结构化综述，明确了基本概念，系统地介绍了系统的核心组件，探讨了新兴应用，并讨论了人机协作带来的独特挑战和机遇。整合现有知识并提供结构化概述，以推动该跨学科领域的进一步研究和创新。资源列表可通过链接访问。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLM驱动的智能代理受到关注，但存在可靠性、任务处理、安全和伦理风险方面的挑战。</li>
<li>LLM-HAS结合人类信息、反馈和控制提升系统性能、可靠性和安全性。</li>
<li>LLM-HAS允许人类和LLM智能代理有效协作，利用各自优势。</li>
<li>本文是对LLM-HAS的全面结构化综述，涵盖基本概念、核心组件、新兴应用和独特挑战与机遇。</li>
<li>资源列表提供进一步研究和参考的链接。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.00753">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-ce6ed1d6faca54a6096918f7e132ff9a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-213c518178ce2cd47ecd90d5bac26828.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a6ba09dfab9e7e6522ab8bb6ee6d01f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9887fb5d1c54c35aefa14aa27bc9c479.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b804404a395280eb9aa569eae1f75ae.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Markets-with-Heterogeneous-Agents-Dynamics-and-Survival-of-Bayesian-vs-No-Regret-Learners"><a href="#Markets-with-Heterogeneous-Agents-Dynamics-and-Survival-of-Bayesian-vs-No-Regret-Learners" class="headerlink" title="Markets with Heterogeneous Agents: Dynamics and Survival of Bayesian vs.   No-Regret Learners"></a>Markets with Heterogeneous Agents: Dynamics and Survival of Bayesian vs.   No-Regret Learners</h2><p><strong>Authors:David Easley, Yoav Kolumbus, Eva Tardos</strong></p>
<p>We analyze the performance of heterogeneous learning agents in asset markets with stochastic payoffs. Our main focus is on comparing Bayesian learners and no-regret learners who compete in markets and identifying the conditions under which each approach is more effective. Surprisingly, we find that low regret is not sufficient for survival: an agent can have regret as low as $O(\log T)$ but still vanish when competing against a Bayesian with a finite prior and any positive prior probability on the correct model. On the other hand, we show that Bayesian learning is fragile, while no-regret learning requires less knowledge of the environment and is therefore more robust. Motivated by the strengths and weaknesses of both approaches, we propose a balanced strategy for utilizing Bayesian updates that improves robustness and adaptability to distribution shifts, providing a step toward a best-of-both-worlds learning approach. The method is general, efficient, and easy to implement. Finally, we formally establish the relationship between the notions of survival and market dominance studied in economics and the framework of regret minimization, thus bridging these theories. More broadly, our work contributes to the understanding of dynamics with heterogeneous types of learning agents and their impact on markets. </p>
<blockquote>
<p>我们分析了在收益随机的资产市场中，不同学习主体的表现。我们的主要关注点是对比在市场中竞争的各种贝叶斯学习者和无后悔学习者，并确定在各种条件下哪种方法更有效。令人惊讶的是，我们发现低后悔并不足以生存：一个主体的后悔可以低到O(logT)，但在与带有有限先验和对正确模型的任何正先验概率的贝叶斯进行竞争时，仍然会消失。另一方面，我们证明了贝叶斯学习是脆弱的，而无后悔学习对环境知识的要求较少，因此更具稳健性。基于这两种方法的优缺点，我们提出了一种利用贝叶斯更新的平衡策略，该策略提高了稳健性和适应分布变化的能力，朝着最佳学习途径迈出了重要一步。该方法具有通用性、高效性和易于实现的特点。最后，我们正式建立了经济学中研究的生存与市场支配力与后悔最小化框架之间的关系，从而弥合了这些理论。更广泛地说，我们的工作有助于理解具有不同学习类型的主体市场动态及其对市场的影响。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.08597v2">PDF</a> Learning in Markets, Heterogeneous Agents, Regret and Survival,   Bayesian Learning, No-Regret Learning, Portfolio Optimization, Kelly Rule,   Distribution Shifts, Robust Bayesian Updates</p>
<p><strong>Summary</strong></p>
<p>在资产市场中分析异质学习主体的表现，并比较贝叶斯学习者和无后悔学习者在竞争市场中的表现，以及在何种条件下哪种方法更有效。研究发现低后悔并不足以保证生存，而贝叶斯学习虽然脆弱但具有适应性强的优点。因此，提出一种平衡策略来利用贝叶斯更新以提高稳健性和适应分布变化的能力。该研究为经济学中生存与市场支配力的理论和学习理论中的后悔最小化框架建立了联系。总之，本研究对理解异质学习主体的市场动态及其对市场的冲击有所贡献。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>分析异质学习主体在资产市场中的表现，特别是贝叶斯学习者和无后悔学习者的对比。</li>
<li>低后悔不能保证生存：即使后悔很小，一个学习主体在竞争市场中也可能会消失。</li>
<li>贝叶斯学习虽然能够提供适应性强的优点，但同时也是脆弱的。</li>
<li>无后悔学习对环境的需求知识较少，因此更为稳健。</li>
<li>提出一种平衡策略，结合贝叶斯更新来提高稳健性和适应分布变化的能力。</li>
<li>正式建立了经济学中的生存与市场支配力和学习理论中的后悔最小化框架之间的关系。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.08597">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-fedda6e5d2ec62c7d67449f8ac17115d.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="UP-VLA-A-Unified-Understanding-and-Prediction-Model-for-Embodied-Agent"><a href="#UP-VLA-A-Unified-Understanding-and-Prediction-Model-for-Embodied-Agent" class="headerlink" title="UP-VLA: A Unified Understanding and Prediction Model for Embodied Agent"></a>UP-VLA: A Unified Understanding and Prediction Model for Embodied Agent</h2><p><strong>Authors:Jianke Zhang, Yanjiang Guo, Yucheng Hu, Xiaoyu Chen, Xiang Zhu, Jianyu Chen</strong></p>
<p>Recent advancements in Vision-Language-Action (VLA) models have leveraged pre-trained Vision-Language Models (VLMs) to improve the generalization capabilities. VLMs, typically pre-trained on vision-language understanding tasks, provide rich semantic knowledge and reasoning abilities. However, prior research has shown that VLMs often focus on high-level semantic content and neglect low-level features, limiting their ability to capture detailed spatial information and understand physical dynamics. These aspects, which are crucial for embodied control tasks, remain underexplored in existing pre-training paradigms. In this paper, we investigate the training paradigm for VLAs, and introduce \textbf{UP-VLA}, a \textbf{U}nified VLA model training with both multi-modal \textbf{U}nderstanding and future \textbf{P}rediction objectives, enhancing both high-level semantic comprehension and low-level spatial understanding. Experimental results show that UP-VLA achieves a 33% improvement on the Calvin ABC-D benchmark compared to the previous state-of-the-art method. Additionally, UP-VLA demonstrates improved success rates in real-world manipulation tasks, particularly those requiring precise spatial information. </p>
<blockquote>
<p>近期Vision-Language-Action（VLA）模型的进步得益于预训练的Vision-Language Models（VLMs）提高了其泛化能力。VLMs通常在视觉语言理解任务上进行预训练，提供了丰富的语义知识和推理能力。然而，先前的研究表明，VLMs往往关注高级语义内容，而忽视低级特征，这限制了它们捕捉详细空间信息以及理解物理动态的能力。这些方面对于实体控制任务至关重要，而在现有的预训练范式中仍然探索不足。在本文中，我们研究了VLA的训练范式，并引入了UP-VLA，这是一种统一的VLA模型训练，具有多模态理解以及未来预测目标，增强了高级语义理解和低级空间理解。实验结果表明，与先前最先进的方法相比，UP-VLA在Calvin ABC-D基准测试上实现了33%的改进。此外，UP-VLA在现实世界操作任务中表现出更高的成功率，尤其是在需要精确空间信息的任务中。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.18867v3">PDF</a> Accepted to ICML2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了Vision-Language-Action（VLA）模型的新进展。为提高模型的泛化能力，利用预训练的Vision-Language Models（VLMs）。然而，现有研究指出VLMs常忽略低层次特征，难以捕捉详细的空间信息并理解物理动态。本文提出一种UP-VLA模型训练范式，结合了多模态理解和未来预测目标，提高高层次语义理解和低层次空间理解。实验结果显示，UP-VLA在Calvin ABC-D基准测试中实现了相较于先前最佳方法33%的提升，并在实际操控任务中表现出更高的成功率，尤其在需要精确空间信息的任务中。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision-Language-Action (VLA)模型结合视觉、语言和动作，提高模型的泛化能力。</li>
<li>利用预训练的Vision-Language Models（VLMs）进行VLA模型的训练。</li>
<li>VLMs虽然能提供丰富的语义知识和推理能力，但常忽略低层次特征，难以捕捉详细的空间信息并理解物理动态。</li>
<li>UP-VLA模型结合了多模态理解和未来预测目标，提高高层次语义理解和低层次空间理解。</li>
<li>UP-VLA在Calvin ABC-D基准测试中表现优异，实现了相较于先前最佳方法33%的提升。</li>
<li>UP-VLA在实际操控任务中表现出更高的成功率，尤其在需要精确空间信息的任务中。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.18867">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-14b1d35d2a3361d1fa0da656740016cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1afe0abb462d8a629371dc6b2ef2871.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48559e3e0d07c1a74fef1c1185645ecb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e8d3e51915582c42343bcf3d428d330a.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="WiS-Platform-Enhancing-Evaluation-of-LLM-Based-Multi-Agent-Systems-Through-Game-Based-Analysis"><a href="#WiS-Platform-Enhancing-Evaluation-of-LLM-Based-Multi-Agent-Systems-Through-Game-Based-Analysis" class="headerlink" title="WiS Platform: Enhancing Evaluation of LLM-Based Multi-Agent Systems   Through Game-Based Analysis"></a>WiS Platform: Enhancing Evaluation of LLM-Based Multi-Agent Systems   Through Game-Based Analysis</h2><p><strong>Authors:Chengwei Hu, Jianhui Zheng, Yancheng He, Hangyu Guo, Junguang Jiang, Han Zhu, Kai Sun, Yuning Jiang, Wenbo Su, Bo Zheng</strong></p>
<p>Recent advancements in autonomous multi-agent systems (MAS) based on large language models (LLMs) have enhanced the application scenarios and improved the capability of LLMs to handle complex tasks. Despite demonstrating effectiveness, existing studies still evidently struggle to evaluate, analysis, and reproducibility of LLM-based MAS. In this paper, to facilitate the research on LLM-based MAS, we introduce an open, scalable, and real-time updated platform for accessing and analyzing the LLM-based MAS based on the games Who is Spy?” (WiS). Our platform is featured with three main worths: (1) a unified model evaluate interface that supports models available on Hugging Face; (2) real-time updated leaderboard for model evaluation; (3) a comprehensive evaluation covering game-winning rates, attacking, defense strategies, and reasoning of LLMs. To rigorously test WiS, we conduct extensive experiments coverage of various open- and closed-source LLMs, we find that different agents exhibit distinct and intriguing behaviors in the game. The experimental results demonstrate the effectiveness and efficiency of our platform in evaluating LLM-based MAS. Our platform and its documentation are publicly available at <a target="_blank" rel="noopener" href="https://whoisspy.ai/">https://whoisspy.ai/</a>. </p>
<blockquote>
<p>近年来，基于大型语言模型（LLM）的自主多智能体系统（MAS）的进展增强了LLM处理复杂任务的应用场景和能力。尽管已经展示了有效性，但现有研究在评估、分析和重现LLM基于的MAS方面仍面临明显挑战。在本文中，为了促进基于LLM的MAS的研究，我们介绍了一个开放、可扩展、实时更新的平台，该平台可用于访问和分析基于“谁是间谍？”（WiS）游戏的LLM基于的MAS。我们的平台有三个主要特点：（1）支持Hugging Face上模型的统一模型评估界面；（2）实时更新的模型评估排行榜；（3）全面评估，包括游戏胜率、攻击、防御策略和LLM的推理。为了对WiS进行严格的测试，我们对各种开源和闭源LLM进行了广泛的实验覆盖，我们发现不同智能体在游戏中表现出独特而有趣的行为。实验结果证明了我们平台在评估LLM基于的MAS方面的有效性和效率。我们的平台和相关文档可在<a target="_blank" rel="noopener" href="https://whoisspy.ai/%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://whoisspy.ai/公开访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.03359v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于大型语言模型（LLM）的自主多智能体系统（MAS）的最新进展增强了LLM处理复杂任务的能力，并拓宽了其应用场景。为便于LLM-based MAS的研究，本文引入了一个开放、可扩展、实时更新的平台，该平台基于“谁是间谍？”（WiS）游戏，具备统一模型评估接口、实时更新排行榜以及对游戏获胜率、攻击、防御策略和LLM推理的全面评估。实验证明，不同智能体在游戏中表现出独特且有趣的行为，验证了平台评估LLM-based MAS的有效性和效率。平台和文档可在<a target="_blank" rel="noopener" href="https://whoisspy.ai/%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://whoisspy.ai/公开访问。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）和多智能体系统（MAS）的结合增强了处理复杂任务的能力，并拓宽了应用场景。</li>
<li>引入了一个基于“谁是间谍？”游戏的开放、可扩展、实时更新的平台，用于访问和分析LLM-based MAS。</li>
<li>平台具备统一模型评估接口，支持Hugging Face上的模型。</li>
<li>平台提供实时更新的排行榜，用于模型评估。</li>
<li>平台全面评估游戏获胜率、攻击和防御策略，以及LLMs的推理能力。</li>
<li>不同智能体在“谁是间谍？”游戏中表现出独特且有趣的行为。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.03359">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f0f75da3212f048e03f79f0ef82a89c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a466325663773ba5e8a4b0d14f14ee7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1d53ff622c937e2ffa3c6c72d14a3f0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1521002edf470720ebeed31259c33db2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-712cfc86581d209f72788ccb92c1586e.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="SceneGenAgent-Precise-Industrial-Scene-Generation-with-Coding-Agent"><a href="#SceneGenAgent-Precise-Industrial-Scene-Generation-with-Coding-Agent" class="headerlink" title="SceneGenAgent: Precise Industrial Scene Generation with Coding Agent"></a>SceneGenAgent: Precise Industrial Scene Generation with Coding Agent</h2><p><strong>Authors:Xiao Xia, Dan Zhang, Zibo Liao, Zhenyu Hou, Tianrui Sun, Jing Li, Ling Fu, Yuxiao Dong</strong></p>
<p>The modeling of industrial scenes is essential for simulations in industrial manufacturing. While large language models (LLMs) have shown significant progress in generating general 3D scenes from textual descriptions, generating industrial scenes with LLMs poses a unique challenge due to their demand for precise measurements and positioning, requiring complex planning over spatial arrangement. To address this challenge, we introduce SceneGenAgent, an LLM-based agent for generating industrial scenes through C# code. SceneGenAgent ensures precise layout planning through a structured and calculable format, layout verification, and iterative refinement to meet the quantitative requirements of industrial scenarios. Experiment results demonstrate that LLMs powered by SceneGenAgent exceed their original performance, reaching up to 81.0% success rate in real-world industrial scene generation tasks and effectively meeting most scene generation requirements. To further enhance accessibility, we construct SceneInstruct, a dataset designed for fine-tuning open-source LLMs to integrate into SceneGenAgent. Experiments show that fine-tuning open-source LLMs on SceneInstruct yields significant performance improvements, with Llama3.1-70B approaching the capabilities of GPT-4o. Our code and data are available at <a target="_blank" rel="noopener" href="https://github.com/THUDM/SceneGenAgent">https://github.com/THUDM/SceneGenAgent</a> . </p>
<blockquote>
<p>工业场景的建模对于工业制造中的模拟至关重要。虽然大型语言模型（LLM）在根据文本描述生成一般的3D场景方面取得了显著进展，但使用LLM生成工业场景却带来独特的挑战，因为它们需要精确的测量和定位，并要求对空间布局进行复杂规划。为了应对这一挑战，我们推出了SceneGenAgent，这是一个基于LLM的通过C#代码生成工业场景的代理。SceneGenAgent通过结构化和可计算格式、布局验证以及迭代优化，确保精确布局规划，以满足工业场景的定量要求。实验结果表明，由SceneGenAgent驱动的大型语言模型超出了其原始性能，在现实世界中的工业场景生成任务中成功率高达81.0%，并有效地满足了大多数场景生成要求。为了进一步提高可及性，我们构建了SceneInstruct数据集，用于微调开源的大型语言模型，以集成到SceneGenAgent中。实验表明，在SceneInstruct上微调开源的大型语言模型会显著提高性能，Llama3.1-70B接近GPT-4o的能力。我们的代码和数据集可在<a target="_blank" rel="noopener" href="https://github.com/THUDM/SceneGenAgent%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/THUDM/SceneGenAgent找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.21909v3">PDF</a> Accepted to ACL 2025</p>
<p><strong>Summary</strong></p>
<p>工业场景建模对于工业制造中的模拟至关重要。尽管大型语言模型（LLMs）在根据文本描述生成一般3D场景方面取得了显著进展，但使用LLMs生成工业场景却是一个独特的挑战，因为工业场景需要精确的测量和定位，需要复杂的空间布局规划。为解决这一挑战，我们推出了SceneGenAgent，这是一个基于LLM的代理，可通过C#代码生成工业场景。SceneGenAgent通过结构化、可计算格式、布局验证和迭代优化确保精确布局规划，以满足工业场景的定量要求。实验结果表明，SceneGenAgent支持的LLM成功率达到了81.0%，能有效满足工业场景生成的需求。为进一步提高可访问性，我们构建了SceneInstruct数据集，用于微调开源LLM以集成到SceneGenAgent中。实验显示，在SceneInstruct上微调开源LLM能显著提高性能，Llama3.1-70B甚至接近GPT-4o的能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>工业场景建模对模拟至关重要，需要精确测量和定位以及复杂的空间布局规划。</li>
<li>LLMs在生成工业场景时面临挑战，但SceneGenAgent提供了一个解决方案，通过C#代码生成精确布局规划。</li>
<li>SceneGenAgent确保精确布局规划通过结构化、可计算格式、布局验证和迭代优化。</li>
<li>SceneGenAgent支持的LLM在真实工业场景生成任务中的成功率达到了81.0%。</li>
<li>SceneInstruct数据集用于微调开源LLM以提高性能并集成到SceneGenAgent中。</li>
<li>在SceneInstruct数据集上微调的LLM性能显著提高。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.21909">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a14c9563340d2e419302fc38222d2065.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-70587ff0d78a4b94057c6c7b0f56c551.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-72e4cf37d50eed03da83c10adb56d573.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e841f69b87b641574798db8c730c5d7d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b2f99cc73431601aa326782025090826.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="MockLLM-A-Multi-Agent-Behavior-Collaboration-Framework-for-Online-Job-Seeking-and-Recruiting"><a href="#MockLLM-A-Multi-Agent-Behavior-Collaboration-Framework-for-Online-Job-Seeking-and-Recruiting" class="headerlink" title="MockLLM: A Multi-Agent Behavior Collaboration Framework for Online Job   Seeking and Recruiting"></a>MockLLM: A Multi-Agent Behavior Collaboration Framework for Online Job   Seeking and Recruiting</h2><p><strong>Authors:Hongda Sun, Hongzhan Lin, Haiyu Yan, Yang Song, Xin Gao, Rui Yan</strong></p>
<p>Online recruitment platforms have reshaped job-seeking and recruiting processes, driving increased demand for applications that enhance person-job matching. Traditional methods generally rely on analyzing textual data from resumes and job descriptions, limiting the dynamic, interactive aspects crucial to effective recruitment. Recent advances in Large Language Models (LLMs) have revealed remarkable potential in simulating adaptive, role-based dialogues, making them well-suited for recruitment scenarios. In this paper, we propose \textbf{MockLLM}, a novel framework to generate and evaluate mock interview interactions. The system consists of two key components: mock interview generation and two-sided evaluation in handshake protocol. By simulating both interviewer and candidate roles, MockLLM enables consistent and collaborative interactions for real-time and two-sided matching. To further improve the matching quality, MockLLM further incorporates reflection memory generation and dynamic strategy modification, refining behaviors based on previous experience. We evaluate MockLLM on real-world data Boss Zhipin, a major Chinese recruitment platform. The experimental results indicate that MockLLM outperforms existing methods in matching accuracy, scalability, and adaptability across job domains, highlighting its potential to advance candidate assessment and online recruitment. </p>
<blockquote>
<p>在线招聘平台已经改变了求职和招聘流程，这引发了对应用程序的更大需求，这些应用程序能够提升人与工作的匹配度。传统方法通常依赖于分析简历和职位描述中的文本数据，而忽略了招聘中至关重要的动态交互方面。大型语言模型（LLM）的最新进展在模拟基于角色的对话中显示出显著潜力，使其非常适合招聘场景。在本文中，我们提出了名为MockLLM的新型框架，用于生成和评估模拟面试交互。该系统由两个关键组件组成：模拟面试生成和握手协议中的双向评估。通过模拟面试官和候选人的角色，MockLLM实现了实时双向匹配的持续协作交互。为了进一步提高匹配质量，MockLLM还结合了反思记忆生成和动态策略修改，根据以往经验优化行为。我们在真实的招聘平台Boss Zhipin上对MockLLM进行了评估。实验结果表明，在匹配准确性、可扩展性和跨职业领域的适应性方面，MockLLM优于现有方法，突显其在候选人评估和在线招聘方面的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.18113v2">PDF</a> Accepted by KDD 2025 Research Track</p>
<p><strong>总结</strong><br>招聘平台的数字化转型促进了人职匹配技术的进步。传统方法主要依赖简历和职位描述的文本数据分析，忽视了招聘过程中的动态互动环节。最近的大型语言模型（LLM）技术可以模拟基于角色的对话，非常适合招聘场景。本文提出一种名为MockLLM的新框架，用于生成和评估模拟面试互动，包括模拟面试和握手协议双向评估两个关键组件。MockLLM通过模拟面试官和候选人角色，实现实时双向匹配和协作互动。此外，MockLLM还融入反思记忆生成和动态策略调整，基于以往经验优化行为。在真实数据Boss Zhipin上的实验结果表明，MockLLM在匹配精度、可扩展性和跨职位领域的适应性方面优于现有方法，具有推动候选人评估和在线招聘的潜力。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>在线招聘平台正在推动人职匹配技术的进步。</li>
<li>传统招聘方法主要依赖静态文本数据分析，缺乏动态互动。</li>
<li>大型语言模型（LLM）技术可以模拟角色对话，适用于招聘场景。</li>
<li>MockLLM框架用于生成和评估模拟面试互动，包含模拟面试和双向评估。</li>
<li>MockLLM实现实时双向匹配和协作互动，模拟面试官和候选人角色。</li>
<li>MockLLM基于以往经验优化行为，提高匹配质量。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.18113">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f03f3e9221562238f1d69c7349f01f7f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e9f09826469e132d736982a08373642a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-670c50df2aee9b3de6490a21a493d309.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3d514b648900e9b841064eb039a41768.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-28/Agent/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-28/Agent/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Agent/">
                                    <span class="chip bg-color">Agent</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-28/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-bb15fab0e13c2d1ff96c357021e20ffc.jpg" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-06-28  Variational Supervised Contrastive Learning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-28
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-28/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-4c826f5d3ba4bf3338ec4bc057ee9250.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM 方向最新论文已更新，请持续关注 Update in 2025-06-28  mTSBench Benchmarking Multivariate Time Series Anomaly Detection and   Model Selection at Scale
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-28
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">28292.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
