<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Agent">
    <meta name="description" content="Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-28  Mind2Web 2 Evaluating Agentic Search with Agent-as-a-Judge">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Agent | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-606272d05a87c0533b51ed6302057f9f.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Agent</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Agent/">
                                <span class="chip bg-color">Agent</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                Agent
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-28
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    14.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    58 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-28-æ›´æ–°"><a href="#2025-06-28-æ›´æ–°" class="headerlink" title="2025-06-28 æ›´æ–°"></a>2025-06-28 æ›´æ–°</h1><h2 id="Mind2Web-2-Evaluating-Agentic-Search-with-Agent-as-a-Judge"><a href="#Mind2Web-2-Evaluating-Agentic-Search-with-Agent-as-a-Judge" class="headerlink" title="Mind2Web 2: Evaluating Agentic Search with Agent-as-a-Judge"></a>Mind2Web 2: Evaluating Agentic Search with Agent-as-a-Judge</h2><p><strong>Authors:Boyu Gou, Zanming Huang, Yuting Ning, Yu Gu, Michael Lin, Weijian Qi, Andrei Kopanev, Botao Yu, Bernal JimÃ©nez GutiÃ©rrez, Yiheng Shu, Chan Hee Song, Jiaman Wu, Shijie Chen, Hanane Nour Moussa, Tianshu Zhang, Jian Xie, Yifei Li, Tianci Xue, Zeyi Liao, Kai Zhang, Boyuan Zheng, Zhaowei Cai, Viktor Rozgic, Morteza Ziyadi, Huan Sun, Yu Su</strong></p>
<p>Agentic search such as Deep Research systems, where large language models autonomously browse the web, synthesize information, and return comprehensive citation-backed answers, represents a major shift in how users interact with web-scale information. While promising greater efficiency and cognitive offloading, the growing complexity and open-endedness of agentic search have outpaced existing evaluation benchmarks and methodologies, which largely assume short search horizons and static answers. In this paper, we introduce Mind2Web 2, a benchmark of 130 realistic, high-quality, and long-horizon tasks that require real-time web browsing and extensive information synthesis, constructed with over 1,000 hours of human labor. To address the challenge of evaluating time-varying and complex answers, we propose a novel Agent-as-a-Judge framework. Our method constructs task-specific judge agents based on a tree-structured rubric design to automatically assess both answer correctness and source attribution. We conduct a comprehensive evaluation of nine frontier agentic search systems and human performance, along with a detailed error analysis to draw insights for future development. The best-performing system, OpenAI Deep Research, can already achieve 50-70% of human performance while spending half the time, showing a great potential. Altogether, Mind2Web 2 provides a rigorous foundation for developing and benchmarking the next generation of agentic search systems. </p>
<blockquote>
<p>åŸºäºæ·±åº¦ç ”ç©¶ç³»ç»Ÿï¼ˆDeep Research systemsï¼‰çš„agenticæœç´¢ï¼ˆå¦‚è‡ªä¸»æµè§ˆç½‘ç»œã€ç»¼åˆä¿¡æ¯å’Œè¿”å›ä»¥å¼•ç”¨ä¸ºæ”¯æ’‘çš„å…¨é¢ç­”æ¡ˆç­‰ï¼‰ï¼Œä»£è¡¨äº†ç”¨æˆ·ä¸ç½‘é¡µè§„æ¨¡ä¿¡æ¯äº¤äº’æ–¹å¼çš„ä¸€æ¬¡é‡å¤§è½¬å˜ã€‚å°½ç®¡å®ƒå¸¦æ¥äº†æ›´é«˜çš„æ•ˆç‡å’Œè®¤çŸ¥å‡è´Ÿï¼Œä½†agenticæœç´¢æ—¥ç›Šå¢é•¿çš„å¤æ‚æ€§å’Œå¼€æ”¾æ€§è¶…å‡ºäº†ç°æœ‰çš„è¯„ä¼°åŸºå‡†å’Œæ–¹æ³•è®ºï¼Œè¿™äº›æ–¹æ³•ä¸»è¦å‡è®¾æœç´¢è§†é‡è¾ƒçŸ­ä¸”ç­”æ¡ˆé™æ€ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Mind2Web 2ï¼Œè¿™æ˜¯ä¸€ç»„åŒ…å«130ä¸ªçœŸå®ã€é«˜è´¨é‡ã€é•¿æ—¶é—´è§†é‡çš„ä»»åŠ¡åŸºå‡†æµ‹è¯•ï¼Œéœ€è¦å®æ—¶ç½‘é¡µæµè§ˆå’Œå¹¿æ³›çš„ä¿¡æ¯ç»¼åˆï¼Œå€ŸåŠ©è¶…è¿‡1000ä¸ªå°æ—¶çš„äººåŠ›æ„å»ºè€Œæˆã€‚ä¸ºäº†åº”å¯¹è¯„ä¼°éšæ—¶é—´å˜åŒ–å’Œå¤æ‚ç­”æ¡ˆçš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†æ–°å‹çš„Agent-as-a-Judgeæ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŸºäºæ ‘çŠ¶è¯„åˆ†è®¾è®¡æ„å»ºç‰¹å®šä»»åŠ¡çš„åˆ¤æ–­ä»£ç†ï¼Œä»¥è‡ªåŠ¨è¯„ä¼°ç­”æ¡ˆçš„æ­£ç¡®æ€§å’Œæ¥æºå½’å±ã€‚æˆ‘ä»¬å¯¹ä¹ä¸ªå‰æ²¿çš„agenticæœç´¢ç³»ç»Ÿå’Œäººç±»æ€§èƒ½è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œå¹¶è¿›è¡Œäº†è¯¦ç»†çš„è¯¯å·®åˆ†æï¼Œä»¥è·å–æœªæ¥å‘å±•çš„è§è§£ã€‚è¡¨ç°æœ€ä½³çš„OpenAI Deep Researchç³»ç»Ÿå·²ç»è¾¾åˆ°äº†äººç±»æ€§èƒ½çš„50%-70%ï¼ŒåŒæ—¶æ‰€èŠ±è´¹çš„æ—¶é—´ä»…ä¸ºäººç±»çš„ä¸€åŠï¼Œæ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ã€‚æ€»ä¹‹ï¼ŒMind2Web 2ä¸ºä¸‹ä¸€ä»£agenticæœç´¢ç³»ç»Ÿçš„å¼€å‘å’ŒåŸºå‡†æµ‹è¯•æä¾›äº†ä¸¥æ ¼çš„åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.21506v1">PDF</a> Project Homepage: <a target="_blank" rel="noopener" href="https://osu-nlp-group.github.io/Mind2Web2/">https://osu-nlp-group.github.io/Mind2Web2/</a></p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹è‡ªä¸»æµè§ˆç½‘ç»œã€åˆæˆä¿¡æ¯å¹¶è¿”å›ç»¼åˆå¼•ç”¨ç­”æ¡ˆçš„æœç´¢æ–¹å¼ï¼Œä»£è¡¨ç€ç”¨æˆ·ä¸ç½‘ç»œè§„æ¨¡ä¿¡æ¯äº¤äº’çš„é‡å¤§è½¬å˜ã€‚ç„¶è€Œï¼Œè¿™ç§è½¬å˜å¸¦æ¥çš„å¤æ‚æ€§å’Œå¼€æ”¾æ€§è¶…å‡ºäº†ç°æœ‰çš„è¯„ä¼°åŸºå‡†å’Œæ–¹æ³•è®ºï¼Œè¿™äº›æ–¹æ³•ä¸»è¦å‡è®¾æœç´¢è§†é‡è¾ƒçŸ­ä¸”ç­”æ¡ˆé™æ€ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæœ¬æ–‡å¼•å…¥Mind2Web 2åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«130ä¸ªéœ€è¦å®æ—¶ç½‘ç»œæµè§ˆå’Œå¤§é‡ä¿¡æ¯åˆæˆçš„çœŸå®ã€é«˜è´¨é‡ã€é•¿æœŸä»»åŠ¡ï¼Œé€šè¿‡è¶…è¿‡1000å°æ—¶çš„äººåŠ›æ„å»ºè€Œæˆã€‚ä¸ºè¯„ä¼°éšæ—¶é—´å˜åŒ–å’Œå¤æ‚ç­”æ¡ˆï¼Œæˆ‘ä»¬æå‡ºæ–°é¢–çš„â€œAgentä½œä¸ºæ³•å®˜â€æ¡†æ¶ï¼ŒåŸºäºæ ‘å½¢è¯„åˆ†è®¾è®¡æ„å»ºç‰¹å®šä»»åŠ¡çš„è¯„ä¼°ä»£ç†ï¼Œè‡ªåŠ¨è¯„ä¼°ç­”æ¡ˆçš„æ­£ç¡®æ€§å’Œæ¥æºå½’å±ã€‚æˆ‘ä»¬å¯¹å‰æ²¿çš„ä¹ä¸ªä»£ç†æœç´¢ç³»ç»Ÿå’Œäººç±»è¡¨ç°è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œå¹¶è¿›è¡Œè¯¦ç»†é”™è¯¯åˆ†æä»¥æŒ‡å¯¼æœªæ¥å‘å±•ã€‚æœ€ä½³æ€§èƒ½çš„OpenAI Deep Researchç³»ç»Ÿå·²æ¥è¿‘äººç±»è¡¨ç°çš„50%-70%ï¼ŒåŒæ—¶è€—æ—¶å‡åŠï¼Œæ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹è‡ªä¸»æµè§ˆç½‘ç»œå¹¶è¿”å›ç»¼åˆç­”æ¡ˆä»£è¡¨æœç´¢æ–¹å¼çš„é‡è¦è½¬å˜ã€‚</li>
<li>ç°æœ‰è¯„ä¼°åŸºå‡†å’Œæ–¹æ³•è®ºæ— æ³•é€‚åº”è¿™ç§è½¬å˜å¸¦æ¥çš„å¤æ‚æ€§å’Œå¼€æ”¾æ€§ã€‚</li>
<li>Mind2Web 2åŸºå‡†æµ‹è¯•åŒ…å«çœŸå®ã€é«˜è´¨é‡ã€é•¿æœŸä»»åŠ¡ï¼Œç”¨äºè¯„ä¼°ä»£ç†æœç´¢ç³»ç»Ÿçš„æ€§èƒ½ã€‚</li>
<li>å¼•å…¥â€œAgentä½œä¸ºæ³•å®˜â€æ¡†æ¶ï¼Œä»¥è‡ªåŠ¨è¯„ä¼°ç­”æ¡ˆçš„æ­£ç¡®æ€§å’Œæ¥æºå½’å±ã€‚</li>
<li>å¯¹ä¹ä¸ªå‰æ²¿ä»£ç†æœç´¢ç³»ç»Ÿå’Œäººç±»è¡¨ç°è¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚</li>
<li>æœ€ä½³æ€§èƒ½çš„OpenAI Deep Researchç³»ç»Ÿå·²æ¥è¿‘äººç±»è¡¨ç°çš„50%-70%ï¼Œæ˜¾ç¤ºå·¨å¤§æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.21506">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-07e2de5f2937cb150b5794be520c4d32.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48c61a4bba35cdf7b42718f538dee9f4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d8d976ca76f1ce8fbeae0d5572c9fee9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d31848de681e0baf7e3363cb8d0899ef.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Agent-RewardBench-Towards-a-Unified-Benchmark-for-Reward-Modeling-across-Perception-Planning-and-Safety-in-Real-World-Multimodal-Agents"><a href="#Agent-RewardBench-Towards-a-Unified-Benchmark-for-Reward-Modeling-across-Perception-Planning-and-Safety-in-Real-World-Multimodal-Agents" class="headerlink" title="Agent-RewardBench: Towards a Unified Benchmark for Reward Modeling   across Perception, Planning, and Safety in Real-World Multimodal Agents"></a>Agent-RewardBench: Towards a Unified Benchmark for Reward Modeling   across Perception, Planning, and Safety in Real-World Multimodal Agents</h2><p><strong>Authors:Tianyi Men, Zhuoran Jin, Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao</strong></p>
<p>As Multimodal Large Language Models (MLLMs) advance, multimodal agents show promise in real-world tasks like web navigation and embodied intelligence. However, due to limitations in a lack of external feedback, these agents struggle with self-correction and generalization. A promising approach is to use reward models as external feedback, but there is no clear on how to select reward models for agents. Thus, there is an urgent need to build a reward bench targeted at agents. To address these challenges, we propose Agent-RewardBench, a benchmark designed to evaluate reward modeling ability in MLLMs. The benchmark is characterized by three key features: (1) Multiple dimensions and real-world agent scenarios evaluation. It covers perception, planning, and safety with 7 scenarios; (2) Step-level reward evaluation. It allows for the assessment of agent capabilities at the individual steps of a task, providing a more granular view of performance during the planning process; and (3) Appropriately difficulty and high-quality. We carefully sample from 10 diverse models, difficulty control to maintain task challenges, and manual verification to ensure the integrity of the data. Experiments demonstrate that even state-of-the-art multimodal models show limited performance, highlighting the need for specialized training in agent reward modeling. Code is available at github. </p>
<blockquote>
<p>éšç€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„ä¸æ–­å‘å±•ï¼Œå¤šæ¨¡æ€ä»£ç†åœ¨ç½‘é¡µå¯¼èˆªå’Œä½“ç°æ™ºèƒ½ç­‰ç°å®ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹å¤–éƒ¨åé¦ˆï¼Œè¿™äº›ä»£ç†åœ¨è‡ªæˆ‘çº æ­£å’Œæ³›åŒ–æ–¹é¢å­˜åœ¨å›°éš¾ã€‚ä½¿ç”¨å¥–åŠ±æ¨¡å‹ä½œä¸ºå¤–éƒ¨åé¦ˆæ˜¯ä¸€ç§æœ‰å‰é€”çš„æ–¹æ³•ï¼Œä½†å¯¹äºå¦‚ä½•ä¸ºä»£ç†é€‰æ‹©å¥–åŠ±æ¨¡å‹è¿˜æ²¡æœ‰æ˜ç¡®çš„æ–¹æ³•ã€‚å› æ­¤ï¼Œè¿«åˆ‡éœ€è¦æ„å»ºä¸€ä¸ªé¢å‘ä»£ç†çš„å¥–åŠ±åŸºå‡†ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†Agent-RewardBenchï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°MLLMä¸­å¥–åŠ±å»ºæ¨¡èƒ½åŠ›çš„æ–°åŸºå‡†ã€‚è¯¥åŸºå‡†çš„ç‰¹ç‚¹ä¸»è¦ä½“ç°åœ¨ä¸‰ä¸ªæ–¹é¢ï¼š1ï¼‰å¤šç»´åº¦å’Œç°å®ä¸–ç•Œä»£ç†åœºæ™¯è¯„ä¼°ã€‚å®ƒæ¶µç›–äº†æ„ŸçŸ¥ã€è§„åˆ’å’Œå®‰å…¨ï¼ŒåŒ…æ‹¬7ç§åœºæ™¯ï¼›2ï¼‰æ­¥éª¤çº§å¥–åŠ±è¯„ä¼°ã€‚å®ƒå…è®¸å¯¹ä»£ç†åœ¨ä»»åŠ¡å„ä¸ªæ­¥éª¤ä¸­çš„èƒ½åŠ›è¿›è¡Œè¯„ä¼°ï¼Œä¸ºè§„åˆ’è¿‡ç¨‹ä¸­çš„æ€§èƒ½æä¾›æ›´è¯¦ç»†çš„è§†å›¾ï¼›3ï¼‰é€‚å½“çš„éš¾åº¦å’Œé«˜è´¨é‡ã€‚æˆ‘ä»¬ä»10ä¸ªä¸åŒçš„æ¨¡å‹ä¸­ç²¾å¿ƒæŠ½æ ·ï¼Œæ§åˆ¶éš¾åº¦ä»¥ä¿æŒä»»åŠ¡æŒ‘æˆ˜ï¼Œå¹¶æ‰‹åŠ¨éªŒè¯ä»¥ç¡®ä¿æ•°æ®çš„å®Œæ•´æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„å¤šæ¨¡æ€æ¨¡å‹ä¹Ÿè¡¨ç°å‡ºæœ‰é™çš„æ€§èƒ½ï¼Œè¿™çªæ˜¾äº†å¯¹ä»£ç†å¥–åŠ±å»ºæ¨¡è¿›è¡Œä¸“é—¨è®­ç»ƒçš„éœ€è¦ã€‚ä»£ç å¯åœ¨GitHubä¸Šè·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.21252v1">PDF</a> ACL 2025 Main</p>
<p><strong>Summary</strong></p>
<p>éšç€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å‘å±•ï¼Œå¤šæ¨¡æ€ä»£ç†åœ¨è¯¸å¦‚ç½‘ç»œå¯¼èˆªå’Œèº«ä½“æ™ºèƒ½ç­‰ç°å®ä»»åŠ¡ä¸­å±•ç°å‡ºæ½œåŠ›ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹å¤–éƒ¨åé¦ˆï¼Œè¿™äº›ä»£ç†åœ¨è‡ªæˆ‘ä¿®æ­£å’Œæ³›åŒ–æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚ä½¿ç”¨å¥–åŠ±æ¨¡å‹ä½œä¸ºå¤–éƒ¨åé¦ˆæ˜¯ä¸€ç§æœ‰å‰é€”çš„æ–¹æ³•ï¼Œä½†ç›®å‰å°šä¸æ¸…æ¥šå¦‚ä½•ä¸ºä»£ç†é€‰æ‹©å¥–åŠ±æ¨¡å‹ã€‚å› æ­¤ï¼Œæ€¥éœ€å»ºç«‹ä¸€ä¸ªé¢å‘ä»£ç†çš„å¥–åŠ±åŸºå‡†æµ‹è¯•ã€‚æœ¬æ–‡æå‡ºäº†é¢å‘å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹çš„å¥–åŠ±å»ºæ¨¡èƒ½åŠ›è¯„ä¼°çš„Agent-RewardBenchåŸºå‡†æµ‹è¯•ã€‚è¯¥åŸºå‡†æµ‹è¯•å…·æœ‰ä¸‰ä¸ªå…³é”®ç‰¹å¾ï¼šå¤šç»´åº¦å’Œç°å®ä¸–ç•Œä»£ç†åœºæ™¯è¯„ä¼°ï¼Œæ¶µç›–æ„ŸçŸ¥ã€è§„åˆ’å’Œå®‰å…¨å…±ä¸ƒä¸ªåœºæ™¯ï¼›æ­¥éª¤çº§å¥–åŠ±è¯„ä¼°ï¼Œå…è®¸å¯¹ä»£ç†ä»»åŠ¡çš„å„ä¸ªæ­¥éª¤è¿›è¡Œè¯„ä¼°ï¼Œæä¾›è§„åˆ’è¿‡ç¨‹ä¸­çš„æ›´ç²¾ç»†çš„æ€§èƒ½è§†å›¾ï¼›ä»¥åŠé€‚å½“çš„éš¾åº¦å’Œé«˜å“è´¨ã€‚å®éªŒè¡¨æ˜ï¼Œå³ä½¿æ˜¯æœ€æ–°é¢–çš„å¤šæ¨¡æ€æ¨¡å‹æ€§èƒ½ä¹Ÿæœ‰é™ï¼Œè¿™å‡¸æ˜¾äº†å¯¹ä»£ç†å¥–åŠ±æ¨¡å‹çš„ä¸“é¡¹è®­ç»ƒéœ€æ±‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ç°å®ä¸–ç•Œä»»åŠ¡ä¸­å±•ç°å‡ºæ½œåŠ›ï¼Œå¦‚ç½‘ç»œå¯¼èˆªå’Œèº«ä½“æ™ºèƒ½ã€‚</li>
<li>ç”±äºç¼ºä¹å¤–éƒ¨åé¦ˆï¼Œå¤šæ¨¡æ€ä»£ç†åœ¨è‡ªæˆ‘ä¿®æ­£å’Œæ³›åŒ–æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>ä½¿ç”¨å¥–åŠ±æ¨¡å‹ä½œä¸ºå¤–éƒ¨åé¦ˆæ˜¯æ”¹å–„è¿™ä¸€é—®é¢˜çš„æœ‰å‰é€”çš„æ–¹æ³•ã€‚</li>
<li>å»ºç«‹ä¸€ä¸ªé¢å‘ä»£ç†çš„å¥–åŠ±åŸºå‡†æµ‹è¯•ï¼ˆAgent-RewardBenchï¼‰æ˜¯è§£å†³å½“å‰æŒ‘æˆ˜çš„å…³é”®ã€‚</li>
<li>Agent-RewardBenchåŸºå‡†æµ‹è¯•å…·æœ‰ä¸‰ä¸ªå…³é”®ç‰¹å¾ï¼šå¤šç»´åº¦åœºæ™¯è¯„ä¼°ã€æ­¥éª¤çº§å¥–åŠ±è¯„ä¼°ä»¥åŠé€‚å½“çš„éš¾åº¦å’Œé«˜å“è´¨ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œå½“å‰å¤šæ¨¡æ€æ¨¡å‹çš„æ€§èƒ½æœ‰é™ï¼Œéœ€è¦ä¸“é¡¹è®­ç»ƒæ¥æé«˜ä»£ç†å¥–åŠ±å»ºæ¨¡èƒ½åŠ›ã€‚</li>
<li>ä»£ç å·²å…¬å¼€åœ¨GitHubä¸Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.21252">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-eeb30fe7c7f9a2c1168890e5226cefca.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6d9008643cd066b5e5eef9160ef0f2d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca584cc19fa17f5533de84dd5ab71f75.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef0099dd208c25303f2e060394f5689a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0b0f6cd5a64667e01f4c9a47c55dbaa6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d7a215ded2fa011172d9d71cf35b19e9.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Homogenization-of-Multi-agent-Learning-Dynamics-in-Finite-state-Markov-Games"><a href="#Homogenization-of-Multi-agent-Learning-Dynamics-in-Finite-state-Markov-Games" class="headerlink" title="Homogenization of Multi-agent Learning Dynamics in Finite-state Markov   Games"></a>Homogenization of Multi-agent Learning Dynamics in Finite-state Markov   Games</h2><p><strong>Authors:Yann Kerzreho</strong></p>
<p>This paper introduces a new approach for approximating the learning dynamics of multiple reinforcement learning (RL) agents interacting in a finite-state Markov game. The idea is to rescale the learning process by simultaneously reducing the learning rate and increasing the update frequency, effectively treating the agentâ€™s parameters as a slow-evolving variable influenced by the fast-mixing game state. Under mild assumptions-ergodicity of the state process and continuity of the updates-we prove the convergence of this rescaled process to an ordinary differential equation (ODE). This ODE provides a tractable, deterministic approximation of the agentâ€™s learning dynamics. An implementation of the framework is available at,: <a target="_blank" rel="noopener" href="https://github.com/yannKerzreho/MarkovGameApproximation">https://github.com/yannKerzreho/MarkovGameApproximation</a> </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç”¨äºè¿‘ä¼¼æœ‰é™çŠ¶æ€é©¬å°”å¯å¤«æ¸¸æˆä¸­å¤šä¸ªå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä»£ç†çš„å­¦ä¹ åŠ¨æ€äº¤äº’ã€‚è¯¥æ€æƒ³æ˜¯é€šè¿‡åŒæ—¶å‡å°å­¦ä¹ ç‡å’Œå¢åŠ æ›´æ–°é¢‘ç‡æ¥è°ƒæ•´å­¦ä¹ è¿‡ç¨‹ï¼Œæœ‰æ•ˆåœ°å°†ä»£ç†å‚æ•°è§†ä¸ºå—å¿«é€Ÿæ··åˆæ¸¸æˆçŠ¶æ€å½±å“çš„ç¼“æ…¢å˜åŒ–å˜é‡ã€‚åœ¨çŠ¶æ€è¿‡ç¨‹çš„éå†æ€§å’Œæ›´æ–°çš„è¿ç»­æ€§ç­‰æ¸©å’Œå‡è®¾ä¸‹ï¼Œæˆ‘ä»¬è¯æ˜äº†è¿™ç§è°ƒæ•´åçš„è¿‡ç¨‹æ”¶æ•›åˆ°ä¸€ä¸ªå¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEï¼‰ã€‚è¿™ä¸ªODEæä¾›äº†ä¸€ä¸ªæ˜“äºå¤„ç†ã€ç¡®å®šçš„ä»£ç†å­¦ä¹ åŠ¨æ€çš„è¿‘ä¼¼å€¼ã€‚è¯¥æ¡†æ¶çš„å®ç°å¯é€šè¿‡ä»¥ä¸‹ç½‘å€è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/yannKerzreho/MarkovGameApproximation">https://github.com/yannKerzreho/MarkovGameApproximation</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.21079v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç”¨äºè¿‘ä¼¼å¤šä¸ªå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä»£ç†åœ¨æœ‰é™çŠ¶æ€é©¬å°”å¯å¤«åšå¼ˆä¸­çš„å­¦ä¹ åŠ¨æ€ã€‚å…¶ä¸»è¦æ€æƒ³æ˜¯é€šè¿‡åŒæ—¶å‡å°å­¦ä¹ ç‡å’Œå¢åŠ æ›´æ–°é¢‘ç‡æ¥è°ƒæ•´å­¦ä¹ è¿‡ç¨‹ã€‚åœ¨è¯¥æ¡†æ¶ä¸‹ï¼Œä»£ç†å‚æ•°è¢«è§†ä¸ºç”±å¿«é€Ÿæ··åˆæ¸¸æˆçŠ¶æ€æ‰€é©±åŠ¨çš„ç¼“æ…¢å˜åŒ–å˜é‡ã€‚åœ¨çŠ¶æ€è¿‡ç¨‹çš„éå†æ€§å’Œæ›´æ–°è¿ç»­æ€§çš„æ¸©å’Œå‡è®¾ä¸‹ï¼Œæˆ‘ä»¬è¯æ˜äº†è¿™ç§è°ƒæ•´åçš„è¿‡ç¨‹æ”¶æ•›åˆ°ä¸€ä¸ªå¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEï¼‰ã€‚è¿™ä¸ªODEä¸ºä»£ç†çš„å­¦ä¹ åŠ¨æ€æä¾›äº†ä¸€ä¸ªæ˜“äºå¤„ç†çš„ç¡®å®šæ€§è¿‘ä¼¼ã€‚æœ‰å…³è¯¥æ¡†æ¶çš„å®ç°åœ¨ç‰¹å®šçš„ç½‘å€ä¸Šæœ‰è¯¦ç»†ä»‹ç»ã€‚è¯¥ç½‘å€ä¸ºï¼š<a target="_blank" rel="noopener" href="https://github.com/yannKerzreho/MarkovGameApproximation">github.com&#x2F;yannKerzreho&#x2F;MarkovGameApproximation</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.21079">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-872841dbf1d04791b6d29c51f53a0c24.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Evidence-based-diagnostic-reasoning-with-multi-agent-copilot-for-human-pathology"><a href="#Evidence-based-diagnostic-reasoning-with-multi-agent-copilot-for-human-pathology" class="headerlink" title="Evidence-based diagnostic reasoning with multi-agent copilot for human   pathology"></a>Evidence-based diagnostic reasoning with multi-agent copilot for human   pathology</h2><p><strong>Authors:Chengkuan Chen, Luca L. Weishaupt, Drew F. K. Williamson, Richard J. Chen, Tong Ding, Bowen Chen, Anurag Vaidya, Long Phi Le, Guillaume Jaume, Ming Y. Lu, Faisal Mahmood</strong></p>
<p>Pathology is experiencing rapid digital transformation driven by whole-slide imaging and artificial intelligence (AI). While deep learning-based computational pathology has achieved notable success, traditional models primarily focus on image analysis without integrating natural language instruction or rich, text-based context. Current multimodal large language models (MLLMs) in computational pathology face limitations, including insufficient training data, inadequate support and evaluation for multi-image understanding, and a lack of autonomous, diagnostic reasoning capabilities. To address these limitations, we introduce PathChat+, a new MLLM specifically designed for human pathology, trained on over 1 million diverse, pathology-specific instruction samples and nearly 5.5 million question answer turns. Extensive evaluations across diverse pathology benchmarks demonstrated that PathChat+ substantially outperforms the prior PathChat copilot, as well as both state-of-the-art (SOTA) general-purpose and other pathology-specific models. Furthermore, we present SlideSeek, a reasoning-enabled multi-agent AI system leveraging PathChat+ to autonomously evaluate gigapixel whole-slide images (WSIs) through iterative, hierarchical diagnostic reasoning, reaching high accuracy on DDxBench, a challenging open-ended differential diagnosis benchmark, while also capable of generating visually grounded, humanly-interpretable summary reports. </p>
<blockquote>
<p>ç—…ç†å­¦æ­£åœ¨ç»å†ç”±å…¨åˆ‡ç‰‡æˆåƒå’Œäººå·¥æ™ºèƒ½ï¼ˆAIï¼‰é©±åŠ¨çš„å¿«é€Ÿæ•°å­—åŒ–è½¬å‹ã€‚è™½ç„¶åŸºäºæ·±åº¦å­¦ä¹ çš„è®¡ç®—ç—…ç†å­¦å·²ç»å–å¾—äº†æ˜¾è‘—çš„æˆå°±ï¼Œä½†ä¼ ç»Ÿæ¨¡å‹ä¸»è¦ä¾§é‡äºå›¾åƒåˆ†æï¼Œæ²¡æœ‰æ•´åˆè‡ªç„¶è¯­è¨€æŒ‡ä»¤æˆ–ä¸°å¯Œçš„æ–‡æœ¬ä¸Šä¸‹æ–‡ã€‚å½“å‰è®¡ç®—ç—…ç†å­¦ä¸­çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰é¢ä¸´ä¸€äº›å±€é™æ€§ï¼ŒåŒ…æ‹¬è®­ç»ƒæ•°æ®ä¸è¶³ã€å¯¹å¤šå›¾åƒç†è§£çš„æ”¯æŒå’Œè¯„ä¼°ä¸è¶³ï¼Œä»¥åŠç¼ºä¹è‡ªä¸»è¯Šæ–­æ¨ç†èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æ¨å‡ºäº†PathChat+ï¼Œè¿™æ˜¯ä¸€æ¬¾ä¸“é—¨ç”¨äºäººç±»ç—…ç†å­¦çš„æ–°çš„MLLMï¼Œåœ¨è¶…è¿‡100ä¸‡ä¸ªå¤šæ ·åŒ–çš„ç—…ç†å­¦ç‰¹å®šæŒ‡ä»¤æ ·æœ¬å’Œè¿‘550ä¸‡ä¸ªé—®ç­”å›åˆä¸­è¿›è¡Œè®­ç»ƒã€‚åœ¨å¤šç§ç—…ç†å­¦åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒPathChat+æ˜¾è‘—ä¼˜äºä¹‹å‰çš„PathChat copilotï¼Œä»¥åŠæœ€å…ˆè¿›çš„ï¼ˆSOTAï¼‰é€šç”¨å’Œå…¶ä»–ç—…ç†å­¦ç‰¹å®šæ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ¨å‡ºäº†SlideSeekï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨PathChat+çš„æ¨ç†åŠŸèƒ½çš„å¤šæ™ºèƒ½ä½“AIç³»ç»Ÿï¼Œå¯è‡ªä¸»è¯„ä¼°åƒå…†åƒç´ å…¨åˆ‡ç‰‡å›¾åƒï¼ˆWSIsï¼‰é€šè¿‡è¿­ä»£ã€åˆ†å±‚è¯Šæ–­æ¨ç†ï¼Œåœ¨DDxBenchè¿™ä¸€å…·æœ‰æŒ‘æˆ˜æ€§çš„å¼€æ”¾å¼é‰´åˆ«è¯Šæ–­åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°é«˜å‡†ç¡®æ€§ï¼Œå¹¶èƒ½å¤Ÿç”Ÿæˆè§†è§‰åŸºç¡€ã€äººç±»å¯è§£é‡Šçš„æ‘˜è¦æŠ¥å‘Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20964v1">PDF</a> </p>
<p><strong>Summary</strong><br>     ç—…ç†å­¦æ­£ç»å†ç”±å…¨åˆ‡ç‰‡æˆåƒå’Œäººå·¥æ™ºèƒ½é©±åŠ¨çš„æ•°å­—åŒ–å¿«é€Ÿè½¬å‹ã€‚è™½ç„¶æ·±åº¦å­¦ä¹ åœ¨è®¡ç®—ç—…ç†å­¦æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†ä¼ ç»Ÿæ¨¡å‹ä¸»è¦å…³æ³¨å›¾åƒåˆ†æï¼Œå¹¶æœªæ•´åˆè‡ªç„¶è¯­è¨€æŒ‡ä»¤æˆ–ä¸°å¯Œçš„æ–‡æœ¬èƒŒæ™¯ã€‚é’ˆå¯¹å½“å‰å¤šæ¨¡å¼å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è®¡ç®—ç—…ç†å­¦æ–¹é¢é¢ä¸´çš„è®­ç»ƒæ•°æ®ä¸è¶³ã€å¤šå›¾åƒç†è§£æ”¯æŒåŠè¯„ä¼°ä¸è¶³ã€ç¼ºä¹è‡ªä¸»è¯Šæ–­æ¨ç†èƒ½åŠ›ç­‰å±€é™ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ä¸“ä¸ºç—…ç†å­¦è®¾è®¡çš„PathChat+æ¨¡å‹ã€‚è¯¥æ¨¡å‹åœ¨è¶…è¿‡100ä¸‡ä»½ç—…ç†å­¦ç‰¹å®šæŒ‡ä»¤æ ·æœ¬å’Œè¿‘550ä¸‡ä¸ªé—®ç­”å¯¹è¯å›åˆä¸­è¿›è¡Œè®­ç»ƒã€‚åœ¨å¤šç§ç—…ç†å­¦åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒPathChat+æ˜¾è‘—ä¼˜äºä¹‹å‰çš„PathChatåŠ©æ‰‹ï¼Œä»¥åŠå…¶ä»–æœ€å…ˆè¿›çš„é€šç”¨å’Œç—…ç†å­¦ç‰¹å®šæ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¨å‡ºäº†SlideSeekï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨PathChat+çš„æ¨ç†èµ‹èƒ½å¤šæ™ºèƒ½ä½“AIç³»ç»Ÿï¼Œå¯è‡ªä¸»è¯„ä¼°åƒå…†åƒç´ å…¨åˆ‡ç‰‡å›¾åƒï¼Œé€šè¿‡è¿­ä»£åˆ†å±‚è¯Šæ–­æ¨ç†è¾¾åˆ°é«˜å‡†ç¡®ç‡ï¼ŒåŒæ—¶åœ¨å¼€æ”¾çš„å·®å¼‚è¯Šæ–­åŸºå‡†æµ‹è¯•DDxBenchä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå¹¶èƒ½ç”Ÿæˆè§†è§‰åŒ–ã€å¯è¢«äººç†è§£çš„æ€»ç»“æŠ¥å‘Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç—…ç†å­¦æ­£åœ¨ç»å†æ•°å­—åŒ–è½¬å‹ï¼Œå…¨åˆ‡ç‰‡æˆåƒå’Œäººå·¥æ™ºèƒ½åœ¨å…¶ä¸­èµ·åˆ°å…³é”®ä½œç”¨ã€‚</li>
<li>å½“å‰è®¡ç®—ç—…ç†å­¦é¢†åŸŸçš„å¤šæ¨¡å¼å¤§å‹è¯­è¨€æ¨¡å‹é¢ä¸´å¤šæ–¹é¢çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬è®­ç»ƒæ•°æ®ä¸è¶³ã€å¤šå›¾åƒç†è§£æ”¯æŒä¸è¶³ç­‰ã€‚</li>
<li>PathChat+æ¨¡å‹çš„å‡ºç°è§£å†³äº†ä¸Šè¿°é—®é¢˜ï¼Œå¹¶åœ¨å¤šç§ç—…ç†å­¦åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚</li>
<li>PathChat+æ¨¡å‹ç»è¿‡å¤§é‡ç—…ç†å­¦ç‰¹å®šæŒ‡ä»¤æ ·æœ¬å’Œé—®ç­”å¯¹è¯å›åˆçš„è®­ç»ƒï¼Œè®¾è®¡æ›´è´´åˆç—…ç†å­¦é¢†åŸŸã€‚</li>
<li>SlideSeekæ˜¯ä¸€ä¸ªåˆ©ç”¨PathChat+çš„å¤šæ™ºèƒ½ä½“AIç³»ç»Ÿï¼Œå¯è‡ªä¸»è¯„ä¼°å…¨åˆ‡ç‰‡å›¾åƒï¼Œå¹¶é€šè¿‡è¿­ä»£åˆ†å±‚è¯Šæ–­æ¨ç†è¾¾åˆ°é«˜å‡†ç¡®ç‡ã€‚</li>
<li>SlideSeekåœ¨å·®å¼‚è¯Šæ–­åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œèƒ½å¤Ÿç”Ÿæˆäººç±»å¯ç†è§£çš„æŠ¥å‘Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20964">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3c9772c4bbffff405f228790a5321e72.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dbb2a9dca720db7f53f8b0c4d040987f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-609fee2aae0bdf34158314af033e5db2.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="FaSTA-Fast-Slow-Toolpath-Agent-with-Subroutine-Mining-for-Efficient-Multi-turn-Image-Editing"><a href="#FaSTA-Fast-Slow-Toolpath-Agent-with-Subroutine-Mining-for-Efficient-Multi-turn-Image-Editing" class="headerlink" title="FaSTA$^*$: Fast-Slow Toolpath Agent with Subroutine Mining for Efficient   Multi-turn Image Editing"></a>FaSTA$^*$: Fast-Slow Toolpath Agent with Subroutine Mining for Efficient   Multi-turn Image Editing</h2><p><strong>Authors:Advait Gupta, Rishie Raj, Dang Nguyen, Tianyi Zhou</strong></p>
<p>We develop a cost-efficient neurosymbolic agent to address challenging multi-turn image editing tasks such as â€œDetect the bench in the image while recoloring it to pink. Also, remove the cat for a clearer view and recolor the wall to yellow.â€™â€™ It combines the fast, high-level subtask planning by large language models (LLMs) with the slow, accurate, tool-use, and local A$^*$ search per subtask to find a cost-efficient toolpath â€“ a sequence of calls to AI tools. To save the cost of A$^*$ on similar subtasks, we perform inductive reasoning on previously successful toolpaths via LLMs to continuously extract&#x2F;refine frequently used subroutines and reuse them as new tools for future tasks in an adaptive fast-slow planning, where the higher-level subroutines are explored first, and only when they fail, the low-level A$^*$ search is activated. The reusable symbolic subroutines considerably save exploration cost on the same types of subtasks applied to similar images, yielding a human-like fast-slow toolpath agent â€œFaSTA$^*$â€™â€™: fast subtask planning followed by rule-based subroutine selection per subtask is attempted by LLMs at first, which is expected to cover most tasks, while slow A$^*$ search is only triggered for novel and challenging subtasks. By comparing with recent image editing approaches, we demonstrate FaSTA$^*$ is significantly more computationally efficient while remaining competitive with the state-of-the-art baseline in terms of success rate. </p>
<blockquote>
<p>æˆ‘ä»¬å¼€å‘äº†ä¸€ç§å…·æœ‰æˆæœ¬æ•ˆç›Šçš„ç¥ç»ç¬¦å·ä»£ç†ï¼Œç”¨äºè§£å†³å…·æœ‰æŒ‘æˆ˜æ€§çš„å¤šè½®å›¾åƒç¼–è¾‘ä»»åŠ¡ï¼Œä¾‹å¦‚â€œåœ¨å›¾åƒä¸­æ£€æµ‹é•¿å‡³å¹¶å°†å…¶é‡æ–°ç€è‰²ä¸ºç²‰çº¢è‰²ã€‚å¦å¤–ï¼Œç§»é™¤çŒ«ä»¥è·å¾—æ›´æ¸…æ™°çš„è§†å›¾ï¼Œå¹¶å°†å¢™å£é‡æ–°ç€è‰²ä¸ºé»„è‰²ã€‚â€å®ƒç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿã€é«˜çº§å­ä»»åŠ¡è§„åˆ’ä¸ç¼“æ…¢ã€å‡†ç¡®ã€å·¥å…·ä½¿ç”¨å’Œé’ˆå¯¹æ¯ä¸ªå­ä»»åŠ¡çš„å±€éƒ¨A<em>æœç´¢ï¼Œä»¥æ‰¾åˆ°å…·æœ‰æˆæœ¬æ•ˆç›Šçš„å·¥å…·è·¯å¾„â€”â€”ä¸€ç³»åˆ—å¯¹AIå·¥å…·çš„è°ƒç”¨ã€‚ä¸ºäº†èŠ‚çœåœ¨ç±»ä¼¼å­ä»»åŠ¡ä¸Šçš„A</em>æˆæœ¬ï¼Œæˆ‘ä»¬é€šè¿‡LLMå¯¹å…ˆå‰æˆåŠŸçš„å·¥å…·è·¯å¾„è¿›è¡Œå½’çº³æ¨ç†ï¼Œä»¥ä¸æ–­æå–&#x2F;å®Œå–„å¸¸ç”¨çš„å­ç¨‹åºï¼Œå¹¶å°†å…¶é‡æ–°ç”¨ä½œæœªæ¥ä»»åŠ¡çš„æ–°å·¥å…·ï¼Œåœ¨è‡ªé€‚åº”å¿«æ…¢è§„åˆ’ä¸­ï¼Œé¦–å…ˆæ¢ç´¢é«˜çº§å­ç¨‹åºï¼Œåªæœ‰åœ¨å®ƒä»¬å¤±è´¥æ—¶ï¼Œæ‰æ¿€æ´»ä½çº§çš„A<em>æœç´¢ã€‚å¯é‡å¤ä½¿ç”¨çš„ç¬¦å·å­ç¨‹åºå¤§å¤§èŠ‚çœäº†åœ¨ç›¸ä¼¼å›¾åƒä¸Šåº”ç”¨ç›¸åŒç±»å‹å­ä»»åŠ¡çš„æ¢ç´¢æˆæœ¬ï¼Œä»è€Œäº§ç”Ÿäº†ä¸€ä¸ªäººç±»èˆ¬çš„å¿«æ…¢å·¥å…·è·¯å¾„ä»£ç†â€œFaSTA</em>â€ï¼šé¦–å…ˆæ˜¯LLMå°è¯•çš„å¿«é€Ÿå­ä»»åŠ¡è§„åˆ’ï¼Œç„¶åæ˜¯æ¯ä¸ªå­ä»»åŠ¡çš„åŸºäºè§„åˆ™çš„å­ç¨‹åºé€‰æ‹©ï¼Œè¿™é¢„è®¡ä¼šæ¶µç›–å¤§å¤šæ•°ä»»åŠ¡ï¼Œè€Œç¼“æ…¢çš„A<em>æœç´¢ä»…é’ˆå¯¹æ–°çš„å’Œå…·æœ‰æŒ‘æˆ˜æ€§çš„å­ä»»åŠ¡è§¦å‘ã€‚é€šè¿‡ä¸æœ€è¿‘çš„å›¾åƒç¼–è¾‘æ–¹æ³•è¿›è¡Œæ¯”è¾ƒï¼Œæˆ‘ä»¬è¯æ˜äº†FaSTA</em>åœ¨è®¡ç®—æ•ˆç‡ä¸Šæ˜¾è‘—æ›´é«˜ï¼ŒåŒæ—¶åœ¨æˆåŠŸç‡æ–¹é¢ä¸æœ€æ–°åŸºçº¿ä¿æŒç«äº‰åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20911v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æˆ‘ä»¬å¼€å‘äº†ä¸€ç§æˆæœ¬æ•ˆç›Šé«˜çš„ç¥ç»ç¬¦å·ä»£ç†ï¼Œç”¨äºè§£å†³å…·æœ‰æŒ‘æˆ˜æ€§çš„å¤šè½®å›¾åƒç¼–è¾‘ä»»åŠ¡ï¼Œå¦‚æ£€æµ‹å’Œé‡è‰²å›¾åƒä¸­çš„æ¿å‡³åŒæ—¶ä¸ºå›¾åƒå»é™¤çŒ«å’ªå¹¶ç»™å¢™å£é‡æ–°ç€è‰²ç­‰ã€‚å®ƒç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹çš„å¿«é€Ÿé«˜çº§å­ä»»åŠ¡è§„åˆ’ä¸æ…¢é€Ÿå‡†ç¡®çš„å·¥å…·ä½¿ç”¨å’Œé’ˆå¯¹æ¯ä¸ªå­ä»»åŠ¡çš„å±€éƒ¨A<em>ï¼ˆStarï¼‰æœç´¢æ¥å¯»æ‰¾é«˜æ•ˆå·¥å…·è·¯å¾„ï¼ˆå³AIå·¥å…·çš„è°ƒç”¨åºåˆ—ï¼‰ã€‚æˆ‘ä»¬é€šè¿‡ä½¿ç”¨LLMå¯¹ä»¥å‰æˆåŠŸçš„å·¥å…·è·¯å¾„è¿›è¡Œå½’çº³æ¨ç†æ¥èŠ‚çœç±»ä¼¼å­ä»»åŠ¡çš„æˆæœ¬ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šæ„å»ºäº†ä¸€ç§è‡ªé€‚åº”çš„å¿«é€Ÿæ…¢é€Ÿè§„åˆ’æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é¦–å…ˆæ¢ç´¢é«˜çº§å­ç¨‹åºï¼Œä»…åœ¨å®ƒä»¬å¤±è´¥æ—¶æ‰æ¿€æ´»ä½çº§çš„A</em>ï¼ˆStarï¼‰æœç´¢ã€‚è¿™ç§çµæ´»çš„ç¬¦å·å­ç¨‹åºèƒ½å¤ŸèŠ‚çœåŒä¸€ç±»å‹å­ä»»åŠ¡åœ¨ç±»ä¼¼å›¾åƒä¸Šçš„æ¢ç´¢æˆæœ¬ï¼Œä»è€Œåˆ›å»ºäº†ä¸€ç§äººç±»å¼çš„å¿«é€Ÿæ…¢é€Ÿå·¥å…·è·¯å¾„ä»£ç†â€œFaSTA<em>ï¼ˆå¿«é€Ÿæ˜Ÿï¼‰â€ã€‚å¤§å¤šæ•°ä»»åŠ¡é¦–å…ˆå°è¯•ç”±LLMè¿›è¡Œçš„å¿«é€Ÿå­ä»»åŠ¡è§„åˆ’ï¼Œéšåæ ¹æ®æ¯ä¸ªå­ä»»åŠ¡é€‰æ‹©åŸºäºè§„åˆ™çš„å­ç¨‹åºï¼Œåªæœ‰åœ¨é‡åˆ°æ–°é¢–ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„å­ä»»åŠ¡æ—¶æ‰è§¦å‘æ…¢é€Ÿçš„A</em>ï¼ˆStarï¼‰æœç´¢ã€‚ç›¸è¾ƒäºè¿‘æœŸçš„å›¾åƒç¼–è¾‘æ–¹æ³•ï¼ŒFaSTA*åœ¨ä¿æŒé«˜æˆåŠŸç‡çš„åŒæ—¶ï¼Œè®¡ç®—æ•ˆç‡æ˜¾è‘—æé«˜ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>å¼€å‘äº†ä¸€ç§åä¸ºFaSTA*ï¼ˆå¿«é€Ÿæ˜Ÿï¼‰çš„æˆæœ¬æ•ˆç›Šé«˜çš„ç¥ç»ç¬¦å·ä»£ç†ç”¨äºå¤„ç†å›¾åƒç¼–è¾‘ä»»åŠ¡ã€‚</li>
<li>FaSTA<em>ç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹çš„å¿«é€Ÿé«˜çº§å­ä»»åŠ¡è§„åˆ’ä¸æ…¢é€Ÿçš„A</em>ï¼ˆStarï¼‰æœç´¢æ¥å¯»æ‰¾é«˜æ•ˆå·¥å…·è·¯å¾„ã€‚</li>
<li>ä½¿ç”¨LLMå¯¹æˆåŠŸçš„å·¥å…·è·¯å¾„è¿›è¡Œå½’çº³æ¨ç†ä»¥èŠ‚çœæˆæœ¬å¹¶æé«˜å¤„ç†æ•ˆç‡ã€‚</li>
<li>FaSTA*å…·å¤‡è‡ªé€‚åº”çš„å¿«é€Ÿæ…¢é€Ÿè§„åˆ’æ–¹æ³•ï¼Œå¯ä¼˜å…ˆæ¢ç´¢é«˜çº§å­ç¨‹åºå¹¶åœ¨å¿…è¦æ—¶è§¦å‘ä½çº§çš„è¯¦ç»†æœç´¢ã€‚</li>
<li>ä¸å…¶ä»–å›¾åƒç¼–è¾‘æ–¹æ³•ç›¸æ¯”ï¼ŒFaSTA*åœ¨è®¡ç®—æ•ˆç‡ä¸Šæ˜¾è‘—æé«˜ï¼ŒåŒæ—¶ä¿æŒé«˜æˆåŠŸç‡ã€‚</li>
<li>FaSTA*å…·æœ‰å¤„ç†å¤æ‚å’Œå¤šå˜çš„å›¾åƒç¼–è¾‘ä»»åŠ¡çš„èƒ½åŠ›ï¼Œå¦‚æ£€æµ‹å’Œé‡è‰²å›¾åƒä¸­çš„ç‰¹å®šç‰©ä½“ä»¥åŠå»é™¤ç‰¹å®šå…ƒç´ ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20911">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1e1d8cf1ff5387609086ba8a93b0cd0a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a76738f64542defe5aec2fee9785e2ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f4393a477c71b33b8dc4a71d3f6b99c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd3761230c0c78345f9b3b38da57370e.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MAGPIE-A-dataset-for-Multi-AGent-contextual-PrIvacy-Evaluation"><a href="#MAGPIE-A-dataset-for-Multi-AGent-contextual-PrIvacy-Evaluation" class="headerlink" title="MAGPIE: A dataset for Multi-AGent contextual PrIvacy Evaluation"></a>MAGPIE: A dataset for Multi-AGent contextual PrIvacy Evaluation</h2><p><strong>Authors:Gurusha Juneja, Alon Albalak, Wenyue Hua, William Yang Wang</strong></p>
<p>The proliferation of LLM-based agents has led to increasing deployment of inter-agent collaboration for tasks like scheduling, negotiation, resource allocation etc. In such systems, privacy is critical, as agents often access proprietary tools and domain-specific databases requiring strict confidentiality. This paper examines whether LLM-based agents demonstrate an understanding of contextual privacy. And, if instructed, do these systems preserve inference time user privacy in non-adversarial multi-turn conversation. Existing benchmarks to evaluate contextual privacy in LLM-agents primarily assess single-turn, low-complexity tasks where private information can be easily excluded. We first present a benchmark - MAGPIE comprising 158 real-life high-stakes scenarios across 15 domains. These scenarios are designed such that complete exclusion of private data impedes task completion yet unrestricted information sharing could lead to substantial losses. We then evaluate the current state-of-the-art LLMs on (a) their understanding of contextually private data and (b) their ability to collaborate without violating user privacy. Empirical experiments demonstrate that current models, including GPT-4o and Claude-2.7-Sonnet, lack robust understanding of contextual privacy, misclassifying private data as shareable 25.2% and 43.6% of the time. In multi-turn conversations, these models disclose private information in 59.9% and 50.5% of cases even under explicit privacy instructions. Furthermore, multi-agent systems fail to complete tasks in 71% of scenarios. These results underscore that current models are not aligned towards both contextual privacy preservation and collaborative task-solving. </p>
<blockquote>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç†çš„æ™®åŠå¯¼è‡´è¶Šæ¥è¶Šå¤šåœ°ä½¿ç”¨è·¨ä»£ç†åä½œæ¥å®Œæˆè¯¸å¦‚è°ƒåº¦ã€è°ˆåˆ¤ã€èµ„æºåˆ†é…ç­‰ä»»åŠ¡ã€‚åœ¨è¿™æ ·çš„ç³»ç»Ÿä¸­ï¼Œéšç§è‡³å…³é‡è¦ï¼Œå› ä¸ºä»£ç†é€šå¸¸ä¼šè®¿é—®éœ€è¦ä¸¥æ ¼ä¿å¯†çš„ä¸“æœ‰å·¥å…·å’Œç‰¹å®šé¢†åŸŸçš„æ•°æ®åº“ã€‚æœ¬æ–‡æ—¨åœ¨ç ”ç©¶åŸºäºLLMçš„ä»£ç†æ˜¯å¦è¡¨ç°å‡ºå¯¹ä¸Šä¸‹æ–‡éšç§çš„ç†è§£ã€‚å¹¶ä¸”ï¼Œå¦‚æœå—åˆ°æŒ‡ç¤ºï¼Œè¿™äº›ç³»ç»Ÿåœ¨éå¯¹æŠ—æ€§å¤šè½®å¯¹è¯ä¸­æ˜¯å¦èƒ½åœ¨æ¨ç†æ—¶é—´ä¿æŠ¤ç”¨æˆ·éšç§ã€‚ç°æœ‰ç”¨äºè¯„ä¼°LLMä»£ç†ä¸­ä¸Šä¸‹æ–‡éšç§çš„åŸºå‡†æµ‹è¯•ä¸»è¦è¯„ä¼°ä½å¤æ‚åº¦ã€å•å›åˆçš„ä»»åŠ¡ï¼Œåœ¨è¿™äº›ä»»åŠ¡ä¸­å¯ä»¥å¾ˆå®¹æ˜“åœ°æ’é™¤ç§æœ‰ä¿¡æ¯ã€‚æˆ‘ä»¬é¦–å…ˆæå‡ºäº†ä¸€ä¸ªåŸºå‡†æµ‹è¯•MAGPIEï¼Œå®ƒåŒ…å«è·¨è¶Š15ä¸ªé¢†åŸŸçš„158ä¸ªç°å®ç”Ÿæ´»ä¸­çš„é«˜é£é™©åœºæ™¯ã€‚è¿™äº›åœºæ™¯çš„è®¾è®¡ç›®çš„æ˜¯ï¼Œå®Œå…¨æ’é™¤ç§æœ‰æ•°æ®ä¼šé˜»ç¢ä»»åŠ¡å®Œæˆï¼Œä½†æ— é™åˆ¶çš„ä¿¡æ¯å…±äº«å¯èƒ½å¯¼è‡´é‡å¤§æŸå¤±ã€‚ç„¶åæˆ‘ä»¬å¯¹å½“å‰æœ€å…ˆè¿›çš„LLMè¿›è¡Œäº†è¯„ä¼°ï¼šï¼ˆaï¼‰å®ƒä»¬å¯¹ä¸Šä¸‹æ–‡éšç§æ•°æ®çš„ç†è§£ï¼›ï¼ˆbï¼‰å®ƒä»¬åœ¨éµå®ˆç”¨æˆ·éšç§çš„å‰æä¸‹è¿›è¡Œåä½œçš„èƒ½åŠ›ã€‚å®è¯å®éªŒè¡¨æ˜ï¼ŒåŒ…æ‹¬GPT-4oå’ŒClaude-2.7-Sonnetåœ¨å†…çš„å½“å‰æ¨¡å‹ç¼ºä¹å¯¹ä¸Šä¸‹æ–‡éšç§çš„ç¨³å¥ç†è§£ï¼Œå®ƒä»¬å°†ç§æœ‰æ•°æ®é”™è¯¯åœ°å½’ç±»ä¸ºå¯å…±äº«æ•°æ®çš„é¢‘ç‡é«˜è¾¾25.2ï¼…å’Œ43.6ï¼…ã€‚åœ¨å¤šè½®å¯¹è¯ä¸­ï¼Œè¿™äº›æ¨¡å‹åœ¨æ˜ç¡®éšç§æŒ‡ä»¤çš„æƒ…å†µä¸‹ï¼Œä»æœ‰é«˜è¾¾59.9ï¼…å’Œ50.5ï¼…çš„æƒ…å†µæ³„éœ²äº†ç§äººä¿¡æ¯ã€‚æ­¤å¤–ï¼Œåœ¨å¤šä»£ç†ç³»ç»Ÿä¸­ï¼Œæœ‰é«˜è¾¾71ï¼…çš„åœºæ™¯æ— æ³•å®Œæˆä»»åŠ¡ã€‚è¿™äº›ç»“æœå¼ºè°ƒï¼Œå½“å‰æ¨¡å‹å¹¶æ²¡æœ‰å¾ˆå¥½åœ°å®ç°ä¸Šä¸‹æ–‡éšç§ä¿æŠ¤å’Œåä½œä»»åŠ¡è§£å†³ä¹‹é—´çš„å¹³è¡¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20737v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>éšç€LLMåŸºäºçš„ä»£ç†äººçš„å¢å¤šï¼Œä»£ç†äººçš„åä½œä»»åŠ¡ï¼ˆå¦‚è°ƒåº¦ã€è°ˆåˆ¤ã€èµ„æºåˆ†é…ç­‰ï¼‰ä¹Ÿè¶Šæ¥è¶Šé¢‘ç¹ã€‚åœ¨è¿™äº›ç³»ç»Ÿä¸­ï¼Œéšç§è‡³å…³é‡è¦ï¼Œå› ä¸ºä»£ç†äººç»å¸¸è®¿é—®éœ€è¦ä¸¥æ ¼ä¿å¯†çš„ä¸“æœ‰å·¥å…·å’Œç‰¹å®šé¢†åŸŸçš„æ•°æ®åº“ã€‚æœ¬æ–‡æ—¨åœ¨ç ”ç©¶LLMæ˜¯å¦ç†è§£è¯­å¢ƒéšç§å¹¶åœ¨å—åˆ°æŒ‡ç¤ºæ—¶ä¿æŠ¤æ¨ç†æ—¶é—´ç”¨æˆ·çš„éšç§ã€‚å½“å‰å¯¹LLMä»£ç†äººçš„éšç§è¯„ä¼°ä¸»è¦å±€é™äºç®€å•çš„å•ä¸€ä»»åŠ¡åœºæ™¯ï¼Œå¿½è§†äº†å®é™…ä½¿ç”¨ä¸­çš„å¤æ‚æ€§ã€‚æœ¬æ–‡æå‡ºMAGPIEè¯„ä¼°æ ‡å‡†ï¼Œæ¶µç›–çœŸå®ç”Ÿæ´»ä¸­çš„é«˜é£é™©åœºæ™¯ã€‚å®è¯å®éªŒè¡¨æ˜ï¼Œç°æœ‰æ¨¡å‹åœ¨ç†è§£è¯­å¢ƒéšç§å’Œåä½œè¿‡ç¨‹ä¸­ä¿æŠ¤ç”¨æˆ·éšç§æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMä»£ç†äººçš„æ™®åŠä¿ƒè¿›äº†å¤šä»£ç†äººåä½œä»»åŠ¡çš„å¢é•¿ï¼Œå¦‚è°ƒåº¦ã€è°ˆåˆ¤å’Œèµ„æºåˆ†é…ç­‰ã€‚</li>
<li>åœ¨è¿™äº›åä½œç³»ç»Ÿä¸­ï¼Œéšç§ä¿æŠ¤è‡³å…³é‡è¦ï¼Œå› ä¸ºä»£ç†äººéœ€è¦è®¿é—®ä¸“æœ‰å·¥å…·å’Œç‰¹å®šæ•°æ®åº“ã€‚</li>
<li>å½“å‰å¯¹LLMç†è§£è¯­å¢ƒéšç§èƒ½åŠ›çš„è¯„ä¼°æ ‡å‡†ä¸»è¦é™äºç®€å•çš„å•ä¸€ä»»åŠ¡åœºæ™¯ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†MAGPIEè¯„ä¼°æ ‡å‡†ï¼Œæ¶µç›–äº†çœŸå®ç”Ÿæ´»ä¸­çš„é«˜é£é™©åœºæ™¯ä»¥æ›´å¥½åœ°è¯„ä¼°LLMçš„è¡¨ç°ã€‚</li>
<li>å®è¯å®éªŒè¡¨æ˜ï¼Œç°æœ‰LLMæ¨¡å‹åœ¨ç†è§£è¯­å¢ƒéšç§æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå®¹æ˜“è¯¯åˆ¤ç§äººä¿¡æ¯çš„å¯åˆ†äº«æ€§ã€‚</li>
<li>åœ¨å¤šè½®å¯¹è¯ä¸­ï¼Œè¿™äº›æ¨¡å‹åœ¨å—åˆ°æ˜ç¡®éšç§æŒ‡ç¤ºæ—¶ä»ä¼šæ³„éœ²ç§äººä¿¡æ¯ã€‚</li>
<li>å¤šä»£ç†äººç³»ç»Ÿåœ¨å®ŒæˆæŸäº›ä»»åŠ¡æ–¹é¢çš„æ•ˆç‡æœ‰å¾…æé«˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¶‰åŠéšç§ä¿æŠ¤çš„åœºæ™¯ä¸­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20737">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-258774a6f38b2f9f99c29f13ac3b980b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ebd1e3a2ca8b87adf0919a5e2ee294fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1ff4c82b75f28b70ec6133031d0ef22a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4ec59ae2cbc901d27cb8bcd5fc0b5d60.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="From-Web-Search-towards-Agentic-Deep-Research-Incentivizing-Search-with-Reasoning-Agents"><a href="#From-Web-Search-towards-Agentic-Deep-Research-Incentivizing-Search-with-Reasoning-Agents" class="headerlink" title="From Web Search towards Agentic Deep Research: Incentivizing Search with   Reasoning Agents"></a>From Web Search towards Agentic Deep Research: Incentivizing Search with   Reasoning Agents</h2><p><strong>Authors:Weizhi Zhang, Yangning Li, Yuanchen Bei, Junyu Luo, Guancheng Wan, Liangwei Yang, Chenxuan Xie, Yuyao Yang, Wei-Chieh Huang, Chunyu Miao, Henry Peng Zou, Xiao Luo, Yusheng Zhao, Yankai Chen, Chunkit Chan, Peilin Zhou, Xinyang Zhang, Chenwei Zhang, Jingbo Shang, Ming Zhang, Yangqiu Song, Irwin King, Philip S. Yu</strong></p>
<p>Information retrieval is a cornerstone of modern knowledge acquisition, enabling billions of queries each day across diverse domains. However, traditional keyword-based search engines are increasingly inadequate for handling complex, multi-step information needs. Our position is that Large Language Models (LLMs), endowed with reasoning and agentic capabilities, are ushering in a new paradigm termed Agentic Deep Research. These systems transcend conventional information search techniques by tightly integrating autonomous reasoning, iterative retrieval, and information synthesis into a dynamic feedback loop. We trace the evolution from static web search to interactive, agent-based systems that plan, explore, and learn. We also introduce a test-time scaling law to formalize the impact of computational depth on reasoning and search. Supported by benchmark results and the rise of open-source implementations, we demonstrate that Agentic Deep Research not only significantly outperforms existing approaches, but is also poised to become the dominant paradigm for future information seeking. All the related resources, including industry products, research papers, benchmark datasets, and open-source implementations, are collected for the community in <a target="_blank" rel="noopener" href="https://github.com/DavidZWZ/Awesome-Deep-Research">https://github.com/DavidZWZ/Awesome-Deep-Research</a>. </p>
<blockquote>
<p>ä¿¡æ¯æ£€ç´¢æ˜¯ç°ä»£çŸ¥è¯†è·å–çš„æ ¸å¿ƒåŸºçŸ³ï¼Œæ¯å¤©èƒ½å¤Ÿåœ¨ä¸åŒé¢†åŸŸå¤„ç†æ•°åäº¿æ¬¡çš„æŸ¥è¯¢è¯·æ±‚ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„åŸºäºå…³é”®è¯çš„æœç´¢å¼•æ“å·²ç»è¶Šæ¥è¶Šä¸èƒ½æ»¡è¶³å¤æ‚ã€å¤šæ­¥éª¤çš„ä¿¡æ¯éœ€æ±‚ã€‚æˆ‘ä»¬çš„è§‚ç‚¹æ˜¯ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èµ‹äºˆäº†æ¨ç†å’Œä»£ç†èƒ½åŠ›ï¼Œæ­£æ¨åŠ¨ç€ä¸€ç§æ–°çš„èŒƒå¼è½¬å˜ï¼Œè¢«ç§°ä¸ºä»£ç†æ·±åº¦ç ”ç©¶ï¼ˆAgentic Deep Researchï¼‰ã€‚è¿™äº›ç³»ç»Ÿé€šè¿‡ç´§å¯†é›†æˆè‡ªä¸»æ¨ç†ã€è¿­ä»£æ£€ç´¢å’Œä¿¡æ¯åˆæˆåˆ°ä¸€ä¸ªåŠ¨æ€åé¦ˆå¾ªç¯ä¸­ï¼Œä»è€Œè¶…è¶Šäº†ä¼ ç»Ÿçš„ä¿¡æ¯æœç´¢æŠ€æœ¯ã€‚æˆ‘ä»¬è¿½æº¯äº†ä»é™æ€ç½‘é¡µæœç´¢åˆ°äº’åŠ¨ã€åŸºäºä»£ç†çš„ç³»ç»Ÿçš„æ¼”å˜ï¼Œè¿™äº›ç³»ç»Ÿå¯ä»¥è®¡åˆ’ã€æ¢ç´¢å’Œå­¦ä¹ çš„å†ç¨‹ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªæµ‹è¯•æ—¶é—´å°ºåº¦å®šå¾‹ï¼Œä»¥æ­£å¼è®¡ç®—æ·±åº¦å¯¹æ¨ç†å’Œæœç´¢çš„å½±å“ã€‚åœ¨åŸºå‡†æµ‹è¯•ç»“æœçš„æ”¯æŒä¸‹ï¼Œä»¥åŠå¼€æºå®ç°çš„å…´èµ·ä¸‹ï¼Œæˆ‘ä»¬è¯æ˜äº†ä»£ç†æ·±åº¦ç ”ç©¶ä¸ä»…æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè€Œä¸”æœ‰æœ›æˆä¸ºæœªæ¥ä¿¡æ¯æœç´¢çš„ä¸»å¯¼èŒƒå¼ã€‚æ‰€æœ‰ç›¸å…³èµ„æºï¼ŒåŒ…æ‹¬å·¥ä¸šäº§å“ã€ç ”ç©¶è®ºæ–‡ã€åŸºå‡†æ•°æ®é›†å’Œå¼€æºå®ç°ï¼Œéƒ½å·²æ”¶é›†åœ¨<a target="_blank" rel="noopener" href="https://github.com/DavidZWZ/Awesome-Deep-Research%EF%BC%8C%E4%BE%9B%E7%A4%BE%E5%8C%BA%E4%BD%BF%E7%94%A8%E3%80%82">https://github.com/DavidZWZ/Awesome-Deep-Researchï¼Œä¾›ç¤¾åŒºä½¿ç”¨ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18959v2">PDF</a> </p>
<p><strong>Summary</strong><br>ä¿¡æ¯æ£€ç´¢æ˜¯ç°ä»£çŸ¥è¯†è·å–çš„æ ¸å¿ƒï¼Œæ¯å¤©å¤„ç†æ•°åäº¿æ¬¡çš„æŸ¥è¯¢è¯·æ±‚ï¼Œæ¶µç›–å„ç§é¢†åŸŸã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„åŸºäºå…³é”®è¯çš„æœç´¢å¼•æ“åœ¨å¤„ç†å¤æ‚ã€å¤šæ­¥éª¤çš„ä¿¡æ¯éœ€æ±‚æ—¶è¶Šæ¥è¶Šæ˜¾å¾—ä¸è¶³ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‡ºç°ï¼Œå¸¦æ¥äº†åä¸ºAgentic Deep Researchçš„æ–°èŒƒå¼ï¼Œè¯¥ç³»ç»Ÿé€šè¿‡ç´§å¯†é›†æˆè‡ªä¸»æ¨ç†ã€è¿­ä»£æ£€ç´¢å’Œä¿¡æ¯åˆæˆï¼Œè¿›å…¥ä¸€ä¸ªåŠ¨æ€åé¦ˆå¾ªç¯ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿä¿¡æ¯æœç´¢æŠ€æœ¯ã€‚æˆ‘ä»¬è¿½è¸ªäº†ä»é™æ€ç½‘é¡µæœç´¢åˆ°äº¤äº’å¼ã€åŸºäºä»£ç†çš„ç³»ç»Ÿçš„æ¼”å˜ï¼Œè¿™äº›ç³»ç»Ÿå¯ä»¥è®¡åˆ’ã€æ¢ç´¢å’Œè‡ªä¸»å­¦ä¹ ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªæµ‹è¯•æ—¶çš„å°ºåº¦å®šå¾‹ï¼Œä»¥æ­£å¼è®¡ç®—æ·±åº¦å¯¹æ¨ç†å’Œæœç´¢çš„å½±å“ã€‚å—åŸºå‡†æµ‹è¯•ç»“æœå’Œå¼€æºå®ç°çš„å…´èµ·æ”¯æŒï¼Œæˆ‘ä»¬è¯æ˜Agentic Deep Researchä¸ä»…æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè€Œä¸”å·²æˆä¸ºæœªæ¥ä¿¡æ¯æœç´¢çš„ä¸»å¯¼èŒƒå¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¿¡æ¯æ£€ç´¢åœ¨ç°ä»£çŸ¥è¯†è·å–ä¸­æ‰®æ¼”é‡è¦è§’è‰²ï¼Œæ¯å¤©å¤„ç†å¤§é‡æŸ¥è¯¢è¯·æ±‚ã€‚</li>
<li>ä¼ ç»Ÿæœç´¢å¼•æ“åœ¨å¤„ç†å¤æ‚ã€å¤šæ­¥éª¤ä¿¡æ¯éœ€æ±‚æ—¶å­˜åœ¨ä¸è¶³ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‡ºç°å¼•é¢†äº†Agentic Deep Researchæ–°èŒƒå¼ã€‚</li>
<li>Agentic Deep Researché€šè¿‡é›†æˆè‡ªä¸»æ¨ç†ã€è¿­ä»£æ£€ç´¢å’Œä¿¡æ¯åˆæˆï¼Œè¶…è¶Šäº†ä¼ ç»Ÿä¿¡æ¯æœç´¢æŠ€æœ¯ã€‚</li>
<li>Agentic Deep Researchç³»ç»Ÿå…·æœ‰è®¡åˆ’ã€æ¢ç´¢å’Œè‡ªä¸»å­¦ä¹ çš„èƒ½åŠ›ã€‚</li>
<li>æµ‹è¯•æ—¶çš„å°ºåº¦å®šå¾‹ç”¨äºæè¿°è®¡ç®—æ·±åº¦å¯¹æ¨ç†å’Œæœç´¢çš„å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18959">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-606272d05a87c0533b51ed6302057f9f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-76a584b347a5f8a48347111db296ccc0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-155280869ad2bd126b54c3da5ebc7cd6.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="xChemAgents-Agentic-AI-for-Explainable-Quantum-Chemistry"><a href="#xChemAgents-Agentic-AI-for-Explainable-Quantum-Chemistry" class="headerlink" title="xChemAgents: Agentic AI for Explainable Quantum Chemistry"></a>xChemAgents: Agentic AI for Explainable Quantum Chemistry</h2><p><strong>Authors:Can Polat, Mehmet Tuncel, Mustafa Kurban, Erchin Serpedin, Hasan Kurban</strong></p>
<p>Recent progress in multimodal graph neural networks has demonstrated that augmenting atomic XYZ geometries with textual chemical descriptors can enhance predictive accuracy across a range of electronic and thermodynamic properties. However, naively appending large sets of heterogeneous descriptors often degrades performance on tasks sensitive to molecular shape or symmetry, and undermines interpretability. xChemAgents proposes a cooperative agent framework that injects physics-aware reasoning into multimodal property prediction. xChemAgents comprises two language-model-based agents: a Selector, which adaptively identifies a sparse, weighted subset of descriptors relevant to each target, and provides a natural language rationale; and a Validator, which enforces physical constraints such as unit consistency and scaling laws through iterative dialogue. On standard benchmark datasets, xChemAgents achieves up to a 22% reduction in mean absolute error over the state-of-the-art baselines, while producing faithful, human-interpretable explanations. Experiment results highlight the potential of cooperative, self-verifying agents to enhance both accuracy and transparency in foundation-model-driven materials science. The implementation and accompanying dataset are available at <a target="_blank" rel="noopener" href="https://github.com/KurbanIntelligenceLab/xChemAgents">https://github.com/KurbanIntelligenceLab/xChemAgents</a>. </p>
<blockquote>
<p>è¿‘æœŸåœ¨å¤šæ¨¡æ€å›¾ç¥ç»ç½‘ç»œæ–¹é¢çš„è¿›å±•è¡¨æ˜ï¼Œé€šè¿‡æ–‡æœ¬åŒ–å­¦æè¿°ç¬¦å¢å¼ºåŸå­XYZå‡ ä½•ç»“æ„å¯ä»¥æé«˜ä¸€ç³»åˆ—ç”µå­å’Œçƒ­åŠ›å­¦å±æ€§çš„é¢„æµ‹ç²¾åº¦ã€‚ç„¶è€Œï¼Œç›²ç›®æ·»åŠ å¤§é‡å¼‚è´¨æè¿°ç¬¦é€šå¸¸ä¼šé™ä½å¯¹åˆ†å­å½¢çŠ¶æˆ–å¯¹ç§°æ€§æ•æ„Ÿçš„ä»»åŠ¡æ€§èƒ½ï¼Œå¹¶ç ´åå¯è§£é‡Šæ€§ã€‚xChemAgentsæå‡ºäº†ä¸€ç§åä½œä»£ç†æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†ç‰©ç†æ„ŸçŸ¥æ¨ç†æ³¨å…¥å¤šæ¨¡æ€å±æ€§é¢„æµ‹ä¸­ã€‚xChemAgentsåŒ…å«ä¸¤ä¸ªåŸºäºè¯­è¨€æ¨¡å‹çš„ä»£ç†ï¼šSelectorï¼Œå®ƒè‡ªé€‚åº”åœ°è¯†åˆ«ä¸æ¯ä¸ªç›®æ ‡ç›¸å…³çš„ç¨€ç–åŠ æƒæè¿°ç¬¦å­é›†ï¼Œå¹¶æä¾›è‡ªç„¶è¯­è¨€ä¾æ®ï¼›ä»¥åŠValidatorï¼Œå®ƒé€šè¿‡è¿­ä»£å¯¹è¯å¼ºåˆ¶æ‰§è¡Œå•ä½ä¸€è‡´æ€§ç­‰ç‰©ç†çº¦æŸå’Œæ ‡åº¦å®šå¾‹ã€‚åœ¨æ ‡å‡†åŸºå‡†æ•°æ®é›†ä¸Šï¼ŒxChemAgentsçš„å‡æ–¹æ ¹è¯¯å·®æ¯”æœ€æ–°åŸºçº¿æŠ€æœ¯é™ä½äº†é«˜è¾¾22%ï¼ŒåŒæ—¶äº§ç”Ÿå¿ å®ä¸”äººç±»å¯è§£é‡Šçš„è§£é‡Šã€‚å®éªŒç»“æœçªå‡ºäº†åä½œã€è‡ªæˆ‘éªŒè¯çš„ä»£ç†åœ¨åŸºç¡€æ¨¡å‹é©±åŠ¨çš„ææ–™ç§‘å­¦ä¸­æé«˜å‡†ç¡®æ€§å’Œé€æ˜åº¦çš„æ½œåŠ›ã€‚ç›¸å…³å®ç°å’Œä¼´éšæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/KurbanIntelligenceLab/xChemAgents%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/KurbanIntelligenceLab/xChemAgentsæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20574v2">PDF</a> Accepted Paper at ICML 2025 Workshop on MAS</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤šæ¨¡æ€å›¾ç¥ç»ç½‘ç»œçš„æ–°è¿›å±•ï¼Œé€šè¿‡ç»“åˆåŸå­XYZå‡ ä½•ä¸æ–‡æœ¬åŒ–å­¦æè¿°ç¬¦ï¼Œæé«˜äº†ç”µå­å’Œçƒ­åŠ›å­¦å±æ€§çš„é¢„æµ‹ç²¾åº¦ã€‚ç„¶è€Œï¼Œç®€å•æ·»åŠ å¤§é‡å¼‚è´¨æè¿°ç¬¦ä¼šæŸå®³å¯¹åˆ†å­å½¢çŠ¶æˆ–å¯¹ç§°æ€§æ•æ„Ÿçš„ä»»åŠ¡æ€§èƒ½å¹¶é™ä½è§£é‡Šæ€§ã€‚xChemAgentsæå‡ºä¸€ç§åˆä½œä»£ç†æ¡†æ¶ï¼Œå°†ç‰©ç†æ„ŸçŸ¥æ¨ç†æ³¨å…¥å¤šæ¨¡æ€å±æ€§é¢„æµ‹ä¸­ã€‚è¯¥æ¡†æ¶åŒ…å«ä¸¤ä¸ªåŸºäºè¯­è¨€æ¨¡å‹çš„ä»£ç†ï¼šé€‰æ‹©å™¨ï¼Œå¯è‡ªé€‚åº”è¯†åˆ«ä¸ç›®æ ‡ç›¸å…³çš„ç¨€ç–åŠ æƒæè¿°ç¬¦é›†ï¼Œå¹¶æä¾›è‡ªç„¶è¯­è¨€ä¾æ®ï¼›éªŒè¯å™¨ï¼Œé€šè¿‡è¿­ä»£å¯¹è¯å¼ºåˆ¶æ‰§è¡Œå•ä½ä¸€è‡´æ€§ç­‰ç‰©ç†çº¦æŸã€‚åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•æ•°æ®é›†ä¸Šï¼ŒxChemAgentsè¾ƒæœ€æ–°åŸºçº¿æŠ€æœ¯å®ç°äº†å¹³å‡ç»å¯¹è¯¯å·®æœ€å¤šå‡å°‘22%ï¼ŒåŒæ—¶äº§ç”Ÿå¿ å®ä¸”æ˜“äºç†è§£çš„äººç±»è§£é‡Šã€‚å®éªŒç»“æœçªæ˜¾äº†åˆä½œã€è‡ªæˆ‘éªŒè¯çš„ä»£ç†åœ¨æé«˜åŸºç¡€æ¨¡å‹é©±åŠ¨çš„ææ–™ç§‘å­¦å‡†ç¡®æ€§å’Œé€æ˜åº¦æ–¹é¢çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å›¾ç¥ç»ç½‘ç»œç»“åˆåŸå­XYZå‡ ä½•ä¸æ–‡æœ¬åŒ–å­¦æè¿°ç¬¦ï¼Œæå‡äº†ç”µå­å’Œçƒ­åŠ›å­¦å±æ€§çš„é¢„æµ‹ç²¾åº¦ã€‚</li>
<li>ç®€å•åœ°æ·»åŠ å¤§é‡å¼‚è´¨æè¿°ç¬¦å¯èƒ½ä¼šå½±å“åˆ†å­å½¢çŠ¶æˆ–å¯¹ç§°æ€§ç›¸å…³çš„ä»»åŠ¡æ€§èƒ½ï¼Œå¹¶é™ä½æ¨¡å‹è§£é‡Šæ€§ã€‚</li>
<li>xChemAgentsæå‡ºåˆä½œä»£ç†æ¡†æ¶ï¼ŒåŒ…å«é€‰æ‹©å™¨å’ŒéªŒè¯å™¨ï¼Œåˆ†åˆ«è¿›è¡Œå…³é”®æè¿°ç¬¦çš„ç­›é€‰å’Œç‰©ç†çº¦æŸçš„éªŒè¯ã€‚</li>
<li>é€‰æ‹©å™¨èƒ½å¤Ÿè‡ªé€‚åº”è¯†åˆ«ä¸ç›®æ ‡ç›¸å…³çš„ç¨€ç–åŠ æƒæè¿°ç¬¦ï¼Œå¹¶æä¾›è‡ªç„¶è¯­è¨€è§£é‡Šã€‚</li>
<li>éªŒè¯å™¨é€šè¿‡è¿­ä»£å¯¹è¯å¼ºåˆ¶æ‰§è¡Œç‰©ç†çº¦æŸï¼Œå¦‚å•ä½ä¸€è‡´æ€§ç­‰ã€‚</li>
<li>åœ¨æ ‡å‡†æ•°æ®é›†ä¸Šï¼ŒxChemAgentsè¾ƒç°æœ‰æŠ€æœ¯æ˜¾è‘—æé«˜äº†é¢„æµ‹å‡†ç¡®æ€§ï¼Œå¹¶æä¾›äº†å¯è§£é‡Šçš„è§£é‡Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20574">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1300e4c96daefab12f9fc1c073de7907.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1414eebd4ec8e2493e581ad43afe4530.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f152dd18fe370cb1308ac2b622fade1c.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="LLM-Based-Human-Agent-Collaboration-and-Interaction-Systems-A-Survey"><a href="#LLM-Based-Human-Agent-Collaboration-and-Interaction-Systems-A-Survey" class="headerlink" title="LLM-Based Human-Agent Collaboration and Interaction Systems: A Survey"></a>LLM-Based Human-Agent Collaboration and Interaction Systems: A Survey</h2><p><strong>Authors:Henry Peng Zou, Wei-Chieh Huang, Yaozu Wu, Yankai Chen, Chunyu Miao, Hoang Nguyen, Yue Zhou, Weizhi Zhang, Liancheng Fang, Langzhou He, Yangning Li, Dongyuan Li, Renhe Jiang, Xue Liu, Philip S. Yu</strong></p>
<p>Recent advances in large language models (LLMs) have sparked growing interest in building fully autonomous agents. However, fully autonomous LLM-based agents still face significant challenges, including limited reliability due to hallucinations, difficulty in handling complex tasks, and substantial safety and ethical risks, all of which limit their feasibility and trustworthiness in real-world applications. To overcome these limitations, LLM-based human-agent systems (LLM-HAS) incorporate human-provided information, feedback, or control into the agent system to enhance system performance, reliability and safety. These human-agent collaboration systems enable humans and LLM-based agents to collaborate effectively by leveraging their complementary strengths. This paper provides the first comprehensive and structured survey of LLM-HAS. It clarifies fundamental concepts, systematically presents core components shaping these systems, including environment &amp; profiling, human feedback, interaction types, orchestration and communication, explores emerging applications, and discusses unique challenges and opportunities arising from human-AI collaboration. By consolidating current knowledge and offering a structured overview, we aim to foster further research and innovation in this rapidly evolving interdisciplinary field. Paper lists and resources are available at <a target="_blank" rel="noopener" href="https://github.com/HenryPengZou/Awesome-Human-Agent-Collaboration-Interaction-Systems">https://github.com/HenryPengZou/Awesome-Human-Agent-Collaboration-Interaction-Systems</a>. </p>
<blockquote>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›å±•å¼•å‘äº†äººä»¬å¯¹æ„å»ºå®Œå…¨è‡ªä¸»ä»£ç†äººçš„æµ“åšå…´è¶£ã€‚ç„¶è€Œï¼ŒåŸºäºLLMçš„å®Œå…¨è‡ªä¸»ä»£ç†äººä»ç„¶é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ç”±äºå¹»è§‰å¯¼è‡´çš„å¯é æ€§æœ‰é™ã€å¤„ç†å¤æ‚ä»»åŠ¡çš„å›°éš¾ä»¥åŠå®è´¨æ€§çš„å®‰å…¨å’Œé“å¾·é£é™©ï¼Œè¿™äº›éƒ½é™åˆ¶äº†å®ƒä»¬åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„å¯è¡Œæ€§å’Œå¯ä¿¡åº¦ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼ŒLLMåŸºäºçš„äººæœºä»£ç†ç³»ç»Ÿï¼ˆLLM-HASï¼‰å°†äººç±»æä¾›çš„èµ„è®¯ã€åé¦ˆæˆ–æ§åˆ¶èå…¥ä»£ç†ç³»ç»Ÿï¼Œä»¥æå‡ç³»ç»Ÿçš„æ€§èƒ½ã€å¯é æ€§å’Œå®‰å…¨æ€§ã€‚è¿™äº›äººæœºåä½œç³»ç»Ÿåˆ©ç”¨äººå’ŒåŸºäºLLMçš„ä»£ç†äººçš„äº’è¡¥ä¼˜åŠ¿ï¼Œä½¿ä»–ä»¬èƒ½å¤Ÿè¿›è¡Œæœ‰æ•ˆçš„åä½œã€‚æœ¬æ–‡å¯¹LLM-HASè¿›è¡Œäº†é¦–æ¬¡å…¨é¢å’Œç³»ç»Ÿçš„è°ƒæŸ¥ã€‚å®ƒæ˜ç¡®äº†åŸºæœ¬æ¦‚å¿µï¼Œç³»ç»Ÿåœ°å‘ˆç°äº†æ„æˆè¿™äº›ç³»ç»Ÿçš„æ ¸å¿ƒç»„ä»¶ï¼ŒåŒ…æ‹¬ç¯å¢ƒåˆ†æã€äººç±»åé¦ˆã€äº¤äº’ç±»å‹ã€ç¼–æ’å’Œæ²Ÿé€šç­‰ï¼Œæ¢è®¨äº†æ–°å…´åº”ç”¨ï¼Œå¹¶è®¨è®ºäº†ç”±äººæœºåä½œäº§ç”Ÿçš„ç‹¬ç‰¹æŒ‘æˆ˜å’Œæœºé‡ã€‚é€šè¿‡æ•´åˆå½“å‰çŸ¥è¯†å¹¶æä¾›ç»“æ„åŒ–æ¦‚è¿°ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯ä¿ƒè¿›è¿™ä¸€è¿…é€Ÿå‘å±•çš„è·¨å­¦ç§‘é¢†åŸŸçš„è¿›ä¸€æ­¥ç ”ç©¶å’Œåˆ›æ–°ã€‚è®ºæ–‡åˆ—è¡¨å’Œèµ„æºå¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/HenryPengZou/Awesome-Human-Agent-Collaboration-Interaction-Systems%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/HenryPengZou/Awesome-Human-Agent-Collaboration-Interaction-Systemsè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.00753v4">PDF</a> Paper lists and resources are available at   <a target="_blank" rel="noopener" href="https://github.com/HenryPengZou/Awesome-Human-Agent-Collaboration-Interaction-Systems">https://github.com/HenryPengZou/Awesome-Human-Agent-Collaboration-Interaction-Systems</a></p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„è‡ªä¸»æ™ºèƒ½ä»£ç†æ—¥ç›Šå—åˆ°å…³æ³¨ï¼Œä½†ä»é¢ä¸´å¯é æ€§ã€å¤„ç†å¤æ‚ä»»åŠ¡èƒ½åŠ›ã€å®‰å…¨å’Œä¼¦ç†é£é™©ç­‰æ–¹é¢çš„æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼ŒLLMäººç±»ä»£ç†ç³»ç»Ÿï¼ˆLLM-HASï¼‰é€šè¿‡ç»“åˆäººç±»æä¾›çš„ä¿¡æ¯ã€åé¦ˆæˆ–æ§åˆ¶ï¼Œå¢å¼ºäº†ç³»ç»Ÿçš„æ€§èƒ½ã€å¯é æ€§å’Œå®‰å…¨æ€§ã€‚è¯¥ç³»ç»Ÿèƒ½åˆ©ç”¨äººç±»å’ŒLLMæ™ºèƒ½ä»£ç†çš„ä¼˜åŠ¿è¿›è¡Œæœ‰æ•ˆåä½œã€‚æœ¬æ–‡æ˜¯å¯¹LLM-HASçš„é¦–ä¸ªå…¨é¢ç»“æ„åŒ–ç»¼è¿°ï¼Œæ˜ç¡®äº†åŸºæœ¬æ¦‚å¿µï¼Œç³»ç»Ÿåœ°ä»‹ç»äº†ç³»ç»Ÿçš„æ ¸å¿ƒç»„ä»¶ï¼Œæ¢è®¨äº†æ–°å…´åº”ç”¨ï¼Œå¹¶è®¨è®ºäº†äººæœºåä½œå¸¦æ¥çš„ç‹¬ç‰¹æŒ‘æˆ˜å’Œæœºé‡ã€‚æ•´åˆç°æœ‰çŸ¥è¯†å¹¶æä¾›ç»“æ„åŒ–æ¦‚è¿°ï¼Œä»¥æ¨åŠ¨è¯¥è·¨å­¦ç§‘é¢†åŸŸçš„è¿›ä¸€æ­¥ç ”ç©¶å’Œåˆ›æ–°ã€‚èµ„æºåˆ—è¡¨å¯é€šè¿‡é“¾æ¥è®¿é—®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLMé©±åŠ¨çš„æ™ºèƒ½ä»£ç†å—åˆ°å…³æ³¨ï¼Œä½†å­˜åœ¨å¯é æ€§ã€ä»»åŠ¡å¤„ç†ã€å®‰å…¨å’Œä¼¦ç†é£é™©æ–¹é¢çš„æŒ‘æˆ˜ã€‚</li>
<li>LLM-HASç»“åˆäººç±»ä¿¡æ¯ã€åé¦ˆå’Œæ§åˆ¶æå‡ç³»ç»Ÿæ€§èƒ½ã€å¯é æ€§å’Œå®‰å…¨æ€§ã€‚</li>
<li>LLM-HASå…è®¸äººç±»å’ŒLLMæ™ºèƒ½ä»£ç†æœ‰æ•ˆåä½œï¼Œåˆ©ç”¨å„è‡ªä¼˜åŠ¿ã€‚</li>
<li>æœ¬æ–‡æ˜¯å¯¹LLM-HASçš„å…¨é¢ç»“æ„åŒ–ç»¼è¿°ï¼Œæ¶µç›–åŸºæœ¬æ¦‚å¿µã€æ ¸å¿ƒç»„ä»¶ã€æ–°å…´åº”ç”¨å’Œç‹¬ç‰¹æŒ‘æˆ˜ä¸æœºé‡ã€‚</li>
<li>èµ„æºåˆ—è¡¨æä¾›è¿›ä¸€æ­¥ç ”ç©¶å’Œå‚è€ƒçš„é“¾æ¥ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.00753">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ce6ed1d6faca54a6096918f7e132ff9a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-213c518178ce2cd47ecd90d5bac26828.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a6ba09dfab9e7e6522ab8bb6ee6d01f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9887fb5d1c54c35aefa14aa27bc9c479.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b804404a395280eb9aa569eae1f75ae.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Markets-with-Heterogeneous-Agents-Dynamics-and-Survival-of-Bayesian-vs-No-Regret-Learners"><a href="#Markets-with-Heterogeneous-Agents-Dynamics-and-Survival-of-Bayesian-vs-No-Regret-Learners" class="headerlink" title="Markets with Heterogeneous Agents: Dynamics and Survival of Bayesian vs.   No-Regret Learners"></a>Markets with Heterogeneous Agents: Dynamics and Survival of Bayesian vs.   No-Regret Learners</h2><p><strong>Authors:David Easley, Yoav Kolumbus, Eva Tardos</strong></p>
<p>We analyze the performance of heterogeneous learning agents in asset markets with stochastic payoffs. Our main focus is on comparing Bayesian learners and no-regret learners who compete in markets and identifying the conditions under which each approach is more effective. Surprisingly, we find that low regret is not sufficient for survival: an agent can have regret as low as $O(\log T)$ but still vanish when competing against a Bayesian with a finite prior and any positive prior probability on the correct model. On the other hand, we show that Bayesian learning is fragile, while no-regret learning requires less knowledge of the environment and is therefore more robust. Motivated by the strengths and weaknesses of both approaches, we propose a balanced strategy for utilizing Bayesian updates that improves robustness and adaptability to distribution shifts, providing a step toward a best-of-both-worlds learning approach. The method is general, efficient, and easy to implement. Finally, we formally establish the relationship between the notions of survival and market dominance studied in economics and the framework of regret minimization, thus bridging these theories. More broadly, our work contributes to the understanding of dynamics with heterogeneous types of learning agents and their impact on markets. </p>
<blockquote>
<p>æˆ‘ä»¬åˆ†æäº†åœ¨æ”¶ç›Šéšæœºçš„èµ„äº§å¸‚åœºä¸­ï¼Œä¸åŒå­¦ä¹ ä¸»ä½“çš„è¡¨ç°ã€‚æˆ‘ä»¬çš„ä¸»è¦å…³æ³¨ç‚¹æ˜¯å¯¹æ¯”åœ¨å¸‚åœºä¸­ç«äº‰çš„å„ç§è´å¶æ–¯å­¦ä¹ è€…å’Œæ— åæ‚”å­¦ä¹ è€…ï¼Œå¹¶ç¡®å®šåœ¨å„ç§æ¡ä»¶ä¸‹å“ªç§æ–¹æ³•æ›´æœ‰æ•ˆã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°ä½åæ‚”å¹¶ä¸è¶³ä»¥ç”Ÿå­˜ï¼šä¸€ä¸ªä¸»ä½“çš„åæ‚”å¯ä»¥ä½åˆ°O(logT)ï¼Œä½†åœ¨ä¸å¸¦æœ‰æœ‰é™å…ˆéªŒå’Œå¯¹æ­£ç¡®æ¨¡å‹çš„ä»»ä½•æ­£å…ˆéªŒæ¦‚ç‡çš„è´å¶æ–¯è¿›è¡Œç«äº‰æ—¶ï¼Œä»ç„¶ä¼šæ¶ˆå¤±ã€‚å¦ä¸€æ–¹é¢ï¼Œæˆ‘ä»¬è¯æ˜äº†è´å¶æ–¯å­¦ä¹ æ˜¯è„†å¼±çš„ï¼Œè€Œæ— åæ‚”å­¦ä¹ å¯¹ç¯å¢ƒçŸ¥è¯†çš„è¦æ±‚è¾ƒå°‘ï¼Œå› æ­¤æ›´å…·ç¨³å¥æ€§ã€‚åŸºäºè¿™ä¸¤ç§æ–¹æ³•çš„ä¼˜ç¼ºç‚¹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨è´å¶æ–¯æ›´æ–°çš„å¹³è¡¡ç­–ç•¥ï¼Œè¯¥ç­–ç•¥æé«˜äº†ç¨³å¥æ€§å’Œé€‚åº”åˆ†å¸ƒå˜åŒ–çš„èƒ½åŠ›ï¼Œæœç€æœ€ä½³å­¦ä¹ é€”å¾„è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚è¯¥æ–¹æ³•å…·æœ‰é€šç”¨æ€§ã€é«˜æ•ˆæ€§å’Œæ˜“äºå®ç°çš„ç‰¹ç‚¹ã€‚æœ€åï¼Œæˆ‘ä»¬æ­£å¼å»ºç«‹äº†ç»æµå­¦ä¸­ç ”ç©¶çš„ç”Ÿå­˜ä¸å¸‚åœºæ”¯é…åŠ›ä¸åæ‚”æœ€å°åŒ–æ¡†æ¶ä¹‹é—´çš„å…³ç³»ï¼Œä»è€Œå¼¥åˆäº†è¿™äº›ç†è®ºã€‚æ›´å¹¿æ³›åœ°è¯´ï¼Œæˆ‘ä»¬çš„å·¥ä½œæœ‰åŠ©äºç†è§£å…·æœ‰ä¸åŒå­¦ä¹ ç±»å‹çš„ä¸»ä½“å¸‚åœºåŠ¨æ€åŠå…¶å¯¹å¸‚åœºçš„å½±å“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.08597v2">PDF</a> Learning in Markets, Heterogeneous Agents, Regret and Survival,   Bayesian Learning, No-Regret Learning, Portfolio Optimization, Kelly Rule,   Distribution Shifts, Robust Bayesian Updates</p>
<p><strong>Summary</strong></p>
<p>åœ¨èµ„äº§å¸‚åœºä¸­åˆ†æå¼‚è´¨å­¦ä¹ ä¸»ä½“çš„è¡¨ç°ï¼Œå¹¶æ¯”è¾ƒè´å¶æ–¯å­¦ä¹ è€…å’Œæ— åæ‚”å­¦ä¹ è€…åœ¨ç«äº‰å¸‚åœºä¸­çš„è¡¨ç°ï¼Œä»¥åŠåœ¨ä½•ç§æ¡ä»¶ä¸‹å“ªç§æ–¹æ³•æ›´æœ‰æ•ˆã€‚ç ”ç©¶å‘ç°ä½åæ‚”å¹¶ä¸è¶³ä»¥ä¿è¯ç”Ÿå­˜ï¼Œè€Œè´å¶æ–¯å­¦ä¹ è™½ç„¶è„†å¼±ä½†å…·æœ‰é€‚åº”æ€§å¼ºçš„ä¼˜ç‚¹ã€‚å› æ­¤ï¼Œæå‡ºä¸€ç§å¹³è¡¡ç­–ç•¥æ¥åˆ©ç”¨è´å¶æ–¯æ›´æ–°ä»¥æé«˜ç¨³å¥æ€§å’Œé€‚åº”åˆ†å¸ƒå˜åŒ–çš„èƒ½åŠ›ã€‚è¯¥ç ”ç©¶ä¸ºç»æµå­¦ä¸­ç”Ÿå­˜ä¸å¸‚åœºæ”¯é…åŠ›çš„ç†è®ºå’Œå­¦ä¹ ç†è®ºä¸­çš„åæ‚”æœ€å°åŒ–æ¡†æ¶å»ºç«‹äº†è”ç³»ã€‚æ€»ä¹‹ï¼Œæœ¬ç ”ç©¶å¯¹ç†è§£å¼‚è´¨å­¦ä¹ ä¸»ä½“çš„å¸‚åœºåŠ¨æ€åŠå…¶å¯¹å¸‚åœºçš„å†²å‡»æœ‰æ‰€è´¡çŒ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ†æå¼‚è´¨å­¦ä¹ ä¸»ä½“åœ¨èµ„äº§å¸‚åœºä¸­çš„è¡¨ç°ï¼Œç‰¹åˆ«æ˜¯è´å¶æ–¯å­¦ä¹ è€…å’Œæ— åæ‚”å­¦ä¹ è€…çš„å¯¹æ¯”ã€‚</li>
<li>ä½åæ‚”ä¸èƒ½ä¿è¯ç”Ÿå­˜ï¼šå³ä½¿åæ‚”å¾ˆå°ï¼Œä¸€ä¸ªå­¦ä¹ ä¸»ä½“åœ¨ç«äº‰å¸‚åœºä¸­ä¹Ÿå¯èƒ½ä¼šæ¶ˆå¤±ã€‚</li>
<li>è´å¶æ–¯å­¦ä¹ è™½ç„¶èƒ½å¤Ÿæä¾›é€‚åº”æ€§å¼ºçš„ä¼˜ç‚¹ï¼Œä½†åŒæ—¶ä¹Ÿæ˜¯è„†å¼±çš„ã€‚</li>
<li>æ— åæ‚”å­¦ä¹ å¯¹ç¯å¢ƒçš„éœ€æ±‚çŸ¥è¯†è¾ƒå°‘ï¼Œå› æ­¤æ›´ä¸ºç¨³å¥ã€‚</li>
<li>æå‡ºä¸€ç§å¹³è¡¡ç­–ç•¥ï¼Œç»“åˆè´å¶æ–¯æ›´æ–°æ¥æé«˜ç¨³å¥æ€§å’Œé€‚åº”åˆ†å¸ƒå˜åŒ–çš„èƒ½åŠ›ã€‚</li>
<li>æ­£å¼å»ºç«‹äº†ç»æµå­¦ä¸­çš„ç”Ÿå­˜ä¸å¸‚åœºæ”¯é…åŠ›å’Œå­¦ä¹ ç†è®ºä¸­çš„åæ‚”æœ€å°åŒ–æ¡†æ¶ä¹‹é—´çš„å…³ç³»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.08597">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-fedda6e5d2ec62c7d67449f8ac17115d.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="UP-VLA-A-Unified-Understanding-and-Prediction-Model-for-Embodied-Agent"><a href="#UP-VLA-A-Unified-Understanding-and-Prediction-Model-for-Embodied-Agent" class="headerlink" title="UP-VLA: A Unified Understanding and Prediction Model for Embodied Agent"></a>UP-VLA: A Unified Understanding and Prediction Model for Embodied Agent</h2><p><strong>Authors:Jianke Zhang, Yanjiang Guo, Yucheng Hu, Xiaoyu Chen, Xiang Zhu, Jianyu Chen</strong></p>
<p>Recent advancements in Vision-Language-Action (VLA) models have leveraged pre-trained Vision-Language Models (VLMs) to improve the generalization capabilities. VLMs, typically pre-trained on vision-language understanding tasks, provide rich semantic knowledge and reasoning abilities. However, prior research has shown that VLMs often focus on high-level semantic content and neglect low-level features, limiting their ability to capture detailed spatial information and understand physical dynamics. These aspects, which are crucial for embodied control tasks, remain underexplored in existing pre-training paradigms. In this paper, we investigate the training paradigm for VLAs, and introduce \textbf{UP-VLA}, a \textbf{U}nified VLA model training with both multi-modal \textbf{U}nderstanding and future \textbf{P}rediction objectives, enhancing both high-level semantic comprehension and low-level spatial understanding. Experimental results show that UP-VLA achieves a 33% improvement on the Calvin ABC-D benchmark compared to the previous state-of-the-art method. Additionally, UP-VLA demonstrates improved success rates in real-world manipulation tasks, particularly those requiring precise spatial information. </p>
<blockquote>
<p>è¿‘æœŸVision-Language-Actionï¼ˆVLAï¼‰æ¨¡å‹çš„è¿›æ­¥å¾—ç›Šäºé¢„è®­ç»ƒçš„Vision-Language Modelsï¼ˆVLMsï¼‰æé«˜äº†å…¶æ³›åŒ–èƒ½åŠ›ã€‚VLMsé€šå¸¸åœ¨è§†è§‰è¯­è¨€ç†è§£ä»»åŠ¡ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œæä¾›äº†ä¸°å¯Œçš„è¯­ä¹‰çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå…ˆå‰çš„ç ”ç©¶è¡¨æ˜ï¼ŒVLMså¾€å¾€å…³æ³¨é«˜çº§è¯­ä¹‰å†…å®¹ï¼Œè€Œå¿½è§†ä½çº§ç‰¹å¾ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬æ•æ‰è¯¦ç»†ç©ºé—´ä¿¡æ¯ä»¥åŠç†è§£ç‰©ç†åŠ¨æ€çš„èƒ½åŠ›ã€‚è¿™äº›æ–¹é¢å¯¹äºå®ä½“æ§åˆ¶ä»»åŠ¡è‡³å…³é‡è¦ï¼Œè€Œåœ¨ç°æœ‰çš„é¢„è®­ç»ƒèŒƒå¼ä¸­ä»ç„¶æ¢ç´¢ä¸è¶³ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†VLAçš„è®­ç»ƒèŒƒå¼ï¼Œå¹¶å¼•å…¥äº†UP-VLAï¼Œè¿™æ˜¯ä¸€ç§ç»Ÿä¸€çš„VLAæ¨¡å‹è®­ç»ƒï¼Œå…·æœ‰å¤šæ¨¡æ€ç†è§£ä»¥åŠæœªæ¥é¢„æµ‹ç›®æ ‡ï¼Œå¢å¼ºäº†é«˜çº§è¯­ä¹‰ç†è§£å’Œä½çº§ç©ºé—´ç†è§£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸å…ˆå‰æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼ŒUP-VLAåœ¨Calvin ABC-DåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†33%çš„æ”¹è¿›ã€‚æ­¤å¤–ï¼ŒUP-VLAåœ¨ç°å®ä¸–ç•Œæ“ä½œä»»åŠ¡ä¸­è¡¨ç°å‡ºæ›´é«˜çš„æˆåŠŸç‡ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦ç²¾ç¡®ç©ºé—´ä¿¡æ¯çš„ä»»åŠ¡ä¸­ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.18867v3">PDF</a> Accepted to ICML2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Vision-Language-Actionï¼ˆVLAï¼‰æ¨¡å‹çš„æ–°è¿›å±•ã€‚ä¸ºæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„Vision-Language Modelsï¼ˆVLMsï¼‰ã€‚ç„¶è€Œï¼Œç°æœ‰ç ”ç©¶æŒ‡å‡ºVLMså¸¸å¿½ç•¥ä½å±‚æ¬¡ç‰¹å¾ï¼Œéš¾ä»¥æ•æ‰è¯¦ç»†çš„ç©ºé—´ä¿¡æ¯å¹¶ç†è§£ç‰©ç†åŠ¨æ€ã€‚æœ¬æ–‡æå‡ºä¸€ç§UP-VLAæ¨¡å‹è®­ç»ƒèŒƒå¼ï¼Œç»“åˆäº†å¤šæ¨¡æ€ç†è§£å’Œæœªæ¥é¢„æµ‹ç›®æ ‡ï¼Œæé«˜é«˜å±‚æ¬¡è¯­ä¹‰ç†è§£å’Œä½å±‚æ¬¡ç©ºé—´ç†è§£ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒUP-VLAåœ¨Calvin ABC-DåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†ç›¸è¾ƒäºå…ˆå‰æœ€ä½³æ–¹æ³•33%çš„æå‡ï¼Œå¹¶åœ¨å®é™…æ“æ§ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ›´é«˜çš„æˆåŠŸç‡ï¼Œå°¤å…¶åœ¨éœ€è¦ç²¾ç¡®ç©ºé—´ä¿¡æ¯çš„ä»»åŠ¡ä¸­ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision-Language-Action (VLA)æ¨¡å‹ç»“åˆè§†è§‰ã€è¯­è¨€å’ŒåŠ¨ä½œï¼Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>åˆ©ç”¨é¢„è®­ç»ƒçš„Vision-Language Modelsï¼ˆVLMsï¼‰è¿›è¡ŒVLAæ¨¡å‹çš„è®­ç»ƒã€‚</li>
<li>VLMsè™½ç„¶èƒ½æä¾›ä¸°å¯Œçš„è¯­ä¹‰çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ï¼Œä½†å¸¸å¿½ç•¥ä½å±‚æ¬¡ç‰¹å¾ï¼Œéš¾ä»¥æ•æ‰è¯¦ç»†çš„ç©ºé—´ä¿¡æ¯å¹¶ç†è§£ç‰©ç†åŠ¨æ€ã€‚</li>
<li>UP-VLAæ¨¡å‹ç»“åˆäº†å¤šæ¨¡æ€ç†è§£å’Œæœªæ¥é¢„æµ‹ç›®æ ‡ï¼Œæé«˜é«˜å±‚æ¬¡è¯­ä¹‰ç†è§£å’Œä½å±‚æ¬¡ç©ºé—´ç†è§£ã€‚</li>
<li>UP-VLAåœ¨Calvin ABC-DåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå®ç°äº†ç›¸è¾ƒäºå…ˆå‰æœ€ä½³æ–¹æ³•33%çš„æå‡ã€‚</li>
<li>UP-VLAåœ¨å®é™…æ“æ§ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ›´é«˜çš„æˆåŠŸç‡ï¼Œå°¤å…¶åœ¨éœ€è¦ç²¾ç¡®ç©ºé—´ä¿¡æ¯çš„ä»»åŠ¡ä¸­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.18867">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-14b1d35d2a3361d1fa0da656740016cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1afe0abb462d8a629371dc6b2ef2871.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48559e3e0d07c1a74fef1c1185645ecb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e8d3e51915582c42343bcf3d428d330a.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="WiS-Platform-Enhancing-Evaluation-of-LLM-Based-Multi-Agent-Systems-Through-Game-Based-Analysis"><a href="#WiS-Platform-Enhancing-Evaluation-of-LLM-Based-Multi-Agent-Systems-Through-Game-Based-Analysis" class="headerlink" title="WiS Platform: Enhancing Evaluation of LLM-Based Multi-Agent Systems   Through Game-Based Analysis"></a>WiS Platform: Enhancing Evaluation of LLM-Based Multi-Agent Systems   Through Game-Based Analysis</h2><p><strong>Authors:Chengwei Hu, Jianhui Zheng, Yancheng He, Hangyu Guo, Junguang Jiang, Han Zhu, Kai Sun, Yuning Jiang, Wenbo Su, Bo Zheng</strong></p>
<p>Recent advancements in autonomous multi-agent systems (MAS) based on large language models (LLMs) have enhanced the application scenarios and improved the capability of LLMs to handle complex tasks. Despite demonstrating effectiveness, existing studies still evidently struggle to evaluate, analysis, and reproducibility of LLM-based MAS. In this paper, to facilitate the research on LLM-based MAS, we introduce an open, scalable, and real-time updated platform for accessing and analyzing the LLM-based MAS based on the games Who is Spy?â€ (WiS). Our platform is featured with three main worths: (1) a unified model evaluate interface that supports models available on Hugging Face; (2) real-time updated leaderboard for model evaluation; (3) a comprehensive evaluation covering game-winning rates, attacking, defense strategies, and reasoning of LLMs. To rigorously test WiS, we conduct extensive experiments coverage of various open- and closed-source LLMs, we find that different agents exhibit distinct and intriguing behaviors in the game. The experimental results demonstrate the effectiveness and efficiency of our platform in evaluating LLM-based MAS. Our platform and its documentation are publicly available at <a target="_blank" rel="noopener" href="https://whoisspy.ai/">https://whoisspy.ai/</a>. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è‡ªä¸»å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰çš„è¿›å±•å¢å¼ºäº†LLMå¤„ç†å¤æ‚ä»»åŠ¡çš„åº”ç”¨åœºæ™¯å’Œèƒ½åŠ›ã€‚å°½ç®¡å·²ç»å±•ç¤ºäº†æœ‰æ•ˆæ€§ï¼Œä½†ç°æœ‰ç ”ç©¶åœ¨è¯„ä¼°ã€åˆ†æå’Œé‡ç°LLMåŸºäºçš„MASæ–¹é¢ä»é¢ä¸´æ˜æ˜¾æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œä¸ºäº†ä¿ƒè¿›åŸºäºLLMçš„MASçš„ç ”ç©¶ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ä¸ªå¼€æ”¾ã€å¯æ‰©å±•ã€å®æ—¶æ›´æ–°çš„å¹³å°ï¼Œè¯¥å¹³å°å¯ç”¨äºè®¿é—®å’Œåˆ†æåŸºäºâ€œè°æ˜¯é—´è°ï¼Ÿâ€ï¼ˆWiSï¼‰æ¸¸æˆçš„LLMåŸºäºçš„MASã€‚æˆ‘ä»¬çš„å¹³å°æœ‰ä¸‰ä¸ªä¸»è¦ç‰¹ç‚¹ï¼šï¼ˆ1ï¼‰æ”¯æŒHugging Faceä¸Šæ¨¡å‹çš„ç»Ÿä¸€æ¨¡å‹è¯„ä¼°ç•Œé¢ï¼›ï¼ˆ2ï¼‰å®æ—¶æ›´æ–°çš„æ¨¡å‹è¯„ä¼°æ’è¡Œæ¦œï¼›ï¼ˆ3ï¼‰å…¨é¢è¯„ä¼°ï¼ŒåŒ…æ‹¬æ¸¸æˆèƒœç‡ã€æ”»å‡»ã€é˜²å¾¡ç­–ç•¥å’ŒLLMçš„æ¨ç†ã€‚ä¸ºäº†å¯¹WiSè¿›è¡Œä¸¥æ ¼çš„æµ‹è¯•ï¼Œæˆ‘ä»¬å¯¹å„ç§å¼€æºå’Œé—­æºLLMè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒè¦†ç›–ï¼Œæˆ‘ä»¬å‘ç°ä¸åŒæ™ºèƒ½ä½“åœ¨æ¸¸æˆä¸­è¡¨ç°å‡ºç‹¬ç‰¹è€Œæœ‰è¶£çš„è¡Œä¸ºã€‚å®éªŒç»“æœè¯æ˜äº†æˆ‘ä»¬å¹³å°åœ¨è¯„ä¼°LLMåŸºäºçš„MASæ–¹é¢çš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ã€‚æˆ‘ä»¬çš„å¹³å°å’Œç›¸å…³æ–‡æ¡£å¯åœ¨<a target="_blank" rel="noopener" href="https://whoisspy.ai/%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://whoisspy.ai/å…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.03359v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è‡ªä¸»å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰çš„æœ€æ–°è¿›å±•å¢å¼ºäº†LLMå¤„ç†å¤æ‚ä»»åŠ¡çš„èƒ½åŠ›ï¼Œå¹¶æ‹“å®½äº†å…¶åº”ç”¨åœºæ™¯ã€‚ä¸ºä¾¿äºLLM-based MASçš„ç ”ç©¶ï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸€ä¸ªå¼€æ”¾ã€å¯æ‰©å±•ã€å®æ—¶æ›´æ–°çš„å¹³å°ï¼Œè¯¥å¹³å°åŸºäºâ€œè°æ˜¯é—´è°ï¼Ÿâ€ï¼ˆWiSï¼‰æ¸¸æˆï¼Œå…·å¤‡ç»Ÿä¸€æ¨¡å‹è¯„ä¼°æ¥å£ã€å®æ—¶æ›´æ–°æ’è¡Œæ¦œä»¥åŠå¯¹æ¸¸æˆè·èƒœç‡ã€æ”»å‡»ã€é˜²å¾¡ç­–ç•¥å’ŒLLMæ¨ç†çš„å…¨é¢è¯„ä¼°ã€‚å®éªŒè¯æ˜ï¼Œä¸åŒæ™ºèƒ½ä½“åœ¨æ¸¸æˆä¸­è¡¨ç°å‡ºç‹¬ç‰¹ä¸”æœ‰è¶£çš„è¡Œä¸ºï¼ŒéªŒè¯äº†å¹³å°è¯„ä¼°LLM-based MASçš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ã€‚å¹³å°å’Œæ–‡æ¡£å¯åœ¨<a target="_blank" rel="noopener" href="https://whoisspy.ai/%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://whoisspy.ai/å…¬å¼€è®¿é—®ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰çš„ç»“åˆå¢å¼ºäº†å¤„ç†å¤æ‚ä»»åŠ¡çš„èƒ½åŠ›ï¼Œå¹¶æ‹“å®½äº†åº”ç”¨åœºæ™¯ã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªåŸºäºâ€œè°æ˜¯é—´è°ï¼Ÿâ€æ¸¸æˆçš„å¼€æ”¾ã€å¯æ‰©å±•ã€å®æ—¶æ›´æ–°çš„å¹³å°ï¼Œç”¨äºè®¿é—®å’Œåˆ†æLLM-based MASã€‚</li>
<li>å¹³å°å…·å¤‡ç»Ÿä¸€æ¨¡å‹è¯„ä¼°æ¥å£ï¼Œæ”¯æŒHugging Faceä¸Šçš„æ¨¡å‹ã€‚</li>
<li>å¹³å°æä¾›å®æ—¶æ›´æ–°çš„æ’è¡Œæ¦œï¼Œç”¨äºæ¨¡å‹è¯„ä¼°ã€‚</li>
<li>å¹³å°å…¨é¢è¯„ä¼°æ¸¸æˆè·èƒœç‡ã€æ”»å‡»å’Œé˜²å¾¡ç­–ç•¥ï¼Œä»¥åŠLLMsçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ä¸åŒæ™ºèƒ½ä½“åœ¨â€œè°æ˜¯é—´è°ï¼Ÿâ€æ¸¸æˆä¸­è¡¨ç°å‡ºç‹¬ç‰¹ä¸”æœ‰è¶£çš„è¡Œä¸ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.03359">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f0f75da3212f048e03f79f0ef82a89c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a466325663773ba5e8a4b0d14f14ee7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1d53ff622c937e2ffa3c6c72d14a3f0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1521002edf470720ebeed31259c33db2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-712cfc86581d209f72788ccb92c1586e.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="SceneGenAgent-Precise-Industrial-Scene-Generation-with-Coding-Agent"><a href="#SceneGenAgent-Precise-Industrial-Scene-Generation-with-Coding-Agent" class="headerlink" title="SceneGenAgent: Precise Industrial Scene Generation with Coding Agent"></a>SceneGenAgent: Precise Industrial Scene Generation with Coding Agent</h2><p><strong>Authors:Xiao Xia, Dan Zhang, Zibo Liao, Zhenyu Hou, Tianrui Sun, Jing Li, Ling Fu, Yuxiao Dong</strong></p>
<p>The modeling of industrial scenes is essential for simulations in industrial manufacturing. While large language models (LLMs) have shown significant progress in generating general 3D scenes from textual descriptions, generating industrial scenes with LLMs poses a unique challenge due to their demand for precise measurements and positioning, requiring complex planning over spatial arrangement. To address this challenge, we introduce SceneGenAgent, an LLM-based agent for generating industrial scenes through C# code. SceneGenAgent ensures precise layout planning through a structured and calculable format, layout verification, and iterative refinement to meet the quantitative requirements of industrial scenarios. Experiment results demonstrate that LLMs powered by SceneGenAgent exceed their original performance, reaching up to 81.0% success rate in real-world industrial scene generation tasks and effectively meeting most scene generation requirements. To further enhance accessibility, we construct SceneInstruct, a dataset designed for fine-tuning open-source LLMs to integrate into SceneGenAgent. Experiments show that fine-tuning open-source LLMs on SceneInstruct yields significant performance improvements, with Llama3.1-70B approaching the capabilities of GPT-4o. Our code and data are available at <a target="_blank" rel="noopener" href="https://github.com/THUDM/SceneGenAgent">https://github.com/THUDM/SceneGenAgent</a> . </p>
<blockquote>
<p>å·¥ä¸šåœºæ™¯çš„å»ºæ¨¡å¯¹äºå·¥ä¸šåˆ¶é€ ä¸­çš„æ¨¡æ‹Ÿè‡³å…³é‡è¦ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ ¹æ®æ–‡æœ¬æè¿°ç”Ÿæˆä¸€èˆ¬çš„3Dåœºæ™¯æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä½¿ç”¨LLMç”Ÿæˆå·¥ä¸šåœºæ™¯å´å¸¦æ¥ç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒä»¬éœ€è¦ç²¾ç¡®çš„æµ‹é‡å’Œå®šä½ï¼Œå¹¶è¦æ±‚å¯¹ç©ºé—´å¸ƒå±€è¿›è¡Œå¤æ‚è§„åˆ’ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†SceneGenAgentï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºLLMçš„é€šè¿‡C#ä»£ç ç”Ÿæˆå·¥ä¸šåœºæ™¯çš„ä»£ç†ã€‚SceneGenAgenté€šè¿‡ç»“æ„åŒ–å’Œå¯è®¡ç®—æ ¼å¼ã€å¸ƒå±€éªŒè¯ä»¥åŠè¿­ä»£ä¼˜åŒ–ï¼Œç¡®ä¿ç²¾ç¡®å¸ƒå±€è§„åˆ’ï¼Œä»¥æ»¡è¶³å·¥ä¸šåœºæ™¯çš„å®šé‡è¦æ±‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç”±SceneGenAgenté©±åŠ¨çš„å¤§å‹è¯­è¨€æ¨¡å‹è¶…å‡ºäº†å…¶åŸå§‹æ€§èƒ½ï¼Œåœ¨ç°å®ä¸–ç•Œä¸­çš„å·¥ä¸šåœºæ™¯ç”Ÿæˆä»»åŠ¡ä¸­æˆåŠŸç‡é«˜è¾¾81.0%ï¼Œå¹¶æœ‰æ•ˆåœ°æ»¡è¶³äº†å¤§å¤šæ•°åœºæ™¯ç”Ÿæˆè¦æ±‚ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜å¯åŠæ€§ï¼Œæˆ‘ä»¬æ„å»ºäº†SceneInstructæ•°æ®é›†ï¼Œç”¨äºå¾®è°ƒå¼€æºçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä»¥é›†æˆåˆ°SceneGenAgentä¸­ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨SceneInstructä¸Šå¾®è°ƒå¼€æºçš„å¤§å‹è¯­è¨€æ¨¡å‹ä¼šæ˜¾è‘—æé«˜æ€§èƒ½ï¼ŒLlama3.1-70Bæ¥è¿‘GPT-4oçš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/THUDM/SceneGenAgent%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/THUDM/SceneGenAgentæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.21909v3">PDF</a> Accepted to ACL 2025</p>
<p><strong>Summary</strong></p>
<p>å·¥ä¸šåœºæ™¯å»ºæ¨¡å¯¹äºå·¥ä¸šåˆ¶é€ ä¸­çš„æ¨¡æ‹Ÿè‡³å…³é‡è¦ã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ ¹æ®æ–‡æœ¬æè¿°ç”Ÿæˆä¸€èˆ¬3Dåœºæ™¯æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä½¿ç”¨LLMsç”Ÿæˆå·¥ä¸šåœºæ™¯å´æ˜¯ä¸€ä¸ªç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼Œå› ä¸ºå·¥ä¸šåœºæ™¯éœ€è¦ç²¾ç¡®çš„æµ‹é‡å’Œå®šä½ï¼Œéœ€è¦å¤æ‚çš„ç©ºé—´å¸ƒå±€è§„åˆ’ã€‚ä¸ºè§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†SceneGenAgentï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºLLMçš„ä»£ç†ï¼Œå¯é€šè¿‡C#ä»£ç ç”Ÿæˆå·¥ä¸šåœºæ™¯ã€‚SceneGenAgenté€šè¿‡ç»“æ„åŒ–ã€å¯è®¡ç®—æ ¼å¼ã€å¸ƒå±€éªŒè¯å’Œè¿­ä»£ä¼˜åŒ–ç¡®ä¿ç²¾ç¡®å¸ƒå±€è§„åˆ’ï¼Œä»¥æ»¡è¶³å·¥ä¸šåœºæ™¯çš„å®šé‡è¦æ±‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSceneGenAgentæ”¯æŒçš„LLMæˆåŠŸç‡è¾¾åˆ°äº†81.0%ï¼Œèƒ½æœ‰æ•ˆæ»¡è¶³å·¥ä¸šåœºæ™¯ç”Ÿæˆçš„éœ€æ±‚ã€‚ä¸ºè¿›ä¸€æ­¥æé«˜å¯è®¿é—®æ€§ï¼Œæˆ‘ä»¬æ„å»ºäº†SceneInstructæ•°æ®é›†ï¼Œç”¨äºå¾®è°ƒå¼€æºLLMä»¥é›†æˆåˆ°SceneGenAgentä¸­ã€‚å®éªŒæ˜¾ç¤ºï¼Œåœ¨SceneInstructä¸Šå¾®è°ƒå¼€æºLLMèƒ½æ˜¾è‘—æé«˜æ€§èƒ½ï¼ŒLlama3.1-70Bç”šè‡³æ¥è¿‘GPT-4oçš„èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å·¥ä¸šåœºæ™¯å»ºæ¨¡å¯¹æ¨¡æ‹Ÿè‡³å…³é‡è¦ï¼Œéœ€è¦ç²¾ç¡®æµ‹é‡å’Œå®šä½ä»¥åŠå¤æ‚çš„ç©ºé—´å¸ƒå±€è§„åˆ’ã€‚</li>
<li>LLMsåœ¨ç”Ÿæˆå·¥ä¸šåœºæ™¯æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œä½†SceneGenAgentæä¾›äº†ä¸€ä¸ªè§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡C#ä»£ç ç”Ÿæˆç²¾ç¡®å¸ƒå±€è§„åˆ’ã€‚</li>
<li>SceneGenAgentç¡®ä¿ç²¾ç¡®å¸ƒå±€è§„åˆ’é€šè¿‡ç»“æ„åŒ–ã€å¯è®¡ç®—æ ¼å¼ã€å¸ƒå±€éªŒè¯å’Œè¿­ä»£ä¼˜åŒ–ã€‚</li>
<li>SceneGenAgentæ”¯æŒçš„LLMåœ¨çœŸå®å·¥ä¸šåœºæ™¯ç”Ÿæˆä»»åŠ¡ä¸­çš„æˆåŠŸç‡è¾¾åˆ°äº†81.0%ã€‚</li>
<li>SceneInstructæ•°æ®é›†ç”¨äºå¾®è°ƒå¼€æºLLMä»¥æé«˜æ€§èƒ½å¹¶é›†æˆåˆ°SceneGenAgentä¸­ã€‚</li>
<li>åœ¨SceneInstructæ•°æ®é›†ä¸Šå¾®è°ƒçš„LLMæ€§èƒ½æ˜¾è‘—æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.21909">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a14c9563340d2e419302fc38222d2065.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-70587ff0d78a4b94057c6c7b0f56c551.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-72e4cf37d50eed03da83c10adb56d573.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e841f69b87b641574798db8c730c5d7d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b2f99cc73431601aa326782025090826.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="MockLLM-A-Multi-Agent-Behavior-Collaboration-Framework-for-Online-Job-Seeking-and-Recruiting"><a href="#MockLLM-A-Multi-Agent-Behavior-Collaboration-Framework-for-Online-Job-Seeking-and-Recruiting" class="headerlink" title="MockLLM: A Multi-Agent Behavior Collaboration Framework for Online Job   Seeking and Recruiting"></a>MockLLM: A Multi-Agent Behavior Collaboration Framework for Online Job   Seeking and Recruiting</h2><p><strong>Authors:Hongda Sun, Hongzhan Lin, Haiyu Yan, Yang Song, Xin Gao, Rui Yan</strong></p>
<p>Online recruitment platforms have reshaped job-seeking and recruiting processes, driving increased demand for applications that enhance person-job matching. Traditional methods generally rely on analyzing textual data from resumes and job descriptions, limiting the dynamic, interactive aspects crucial to effective recruitment. Recent advances in Large Language Models (LLMs) have revealed remarkable potential in simulating adaptive, role-based dialogues, making them well-suited for recruitment scenarios. In this paper, we propose \textbf{MockLLM}, a novel framework to generate and evaluate mock interview interactions. The system consists of two key components: mock interview generation and two-sided evaluation in handshake protocol. By simulating both interviewer and candidate roles, MockLLM enables consistent and collaborative interactions for real-time and two-sided matching. To further improve the matching quality, MockLLM further incorporates reflection memory generation and dynamic strategy modification, refining behaviors based on previous experience. We evaluate MockLLM on real-world data Boss Zhipin, a major Chinese recruitment platform. The experimental results indicate that MockLLM outperforms existing methods in matching accuracy, scalability, and adaptability across job domains, highlighting its potential to advance candidate assessment and online recruitment. </p>
<blockquote>
<p>åœ¨çº¿æ‹›è˜å¹³å°å·²ç»æ”¹å˜äº†æ±‚èŒå’Œæ‹›è˜æµç¨‹ï¼Œè¿™å¼•å‘äº†å¯¹åº”ç”¨ç¨‹åºçš„æ›´å¤§éœ€æ±‚ï¼Œè¿™äº›åº”ç”¨ç¨‹åºèƒ½å¤Ÿæå‡äººä¸å·¥ä½œçš„åŒ¹é…åº¦ã€‚ä¼ ç»Ÿæ–¹æ³•é€šå¸¸ä¾èµ–äºåˆ†æç®€å†å’ŒèŒä½æè¿°ä¸­çš„æ–‡æœ¬æ•°æ®ï¼Œè€Œå¿½ç•¥äº†æ‹›è˜ä¸­è‡³å…³é‡è¦çš„åŠ¨æ€äº¤äº’æ–¹é¢ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•åœ¨æ¨¡æ‹ŸåŸºäºè§’è‰²çš„å¯¹è¯ä¸­æ˜¾ç¤ºå‡ºæ˜¾è‘—æ½œåŠ›ï¼Œä½¿å…¶éå¸¸é€‚åˆæ‹›è˜åœºæ™¯ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åä¸ºMockLLMçš„æ–°å‹æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆå’Œè¯„ä¼°æ¨¡æ‹Ÿé¢è¯•äº¤äº’ã€‚è¯¥ç³»ç»Ÿç”±ä¸¤ä¸ªå…³é”®ç»„ä»¶ç»„æˆï¼šæ¨¡æ‹Ÿé¢è¯•ç”Ÿæˆå’Œæ¡æ‰‹åè®®ä¸­çš„åŒå‘è¯„ä¼°ã€‚é€šè¿‡æ¨¡æ‹Ÿé¢è¯•å®˜å’Œå€™é€‰äººçš„è§’è‰²ï¼ŒMockLLMå®ç°äº†å®æ—¶åŒå‘åŒ¹é…çš„æŒç»­åä½œäº¤äº’ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜åŒ¹é…è´¨é‡ï¼ŒMockLLMè¿˜ç»“åˆäº†åæ€è®°å¿†ç”Ÿæˆå’ŒåŠ¨æ€ç­–ç•¥ä¿®æ”¹ï¼Œæ ¹æ®ä»¥å¾€ç»éªŒä¼˜åŒ–è¡Œä¸ºã€‚æˆ‘ä»¬åœ¨çœŸå®çš„æ‹›è˜å¹³å°Boss Zhipinä¸Šå¯¹MockLLMè¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨åŒ¹é…å‡†ç¡®æ€§ã€å¯æ‰©å±•æ€§å’Œè·¨èŒä¸šé¢†åŸŸçš„é€‚åº”æ€§æ–¹é¢ï¼ŒMockLLMä¼˜äºç°æœ‰æ–¹æ³•ï¼Œçªæ˜¾å…¶åœ¨å€™é€‰äººè¯„ä¼°å’Œåœ¨çº¿æ‹›è˜æ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.18113v2">PDF</a> Accepted by KDD 2025 Research Track</p>
<p><strong>æ€»ç»“</strong><br>æ‹›è˜å¹³å°çš„æ•°å­—åŒ–è½¬å‹ä¿ƒè¿›äº†äººèŒåŒ¹é…æŠ€æœ¯çš„è¿›æ­¥ã€‚ä¼ ç»Ÿæ–¹æ³•ä¸»è¦ä¾èµ–ç®€å†å’ŒèŒä½æè¿°çš„æ–‡æœ¬æ•°æ®åˆ†æï¼Œå¿½è§†äº†æ‹›è˜è¿‡ç¨‹ä¸­çš„åŠ¨æ€äº’åŠ¨ç¯èŠ‚ã€‚æœ€è¿‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æŠ€æœ¯å¯ä»¥æ¨¡æ‹ŸåŸºäºè§’è‰²çš„å¯¹è¯ï¼Œéå¸¸é€‚åˆæ‹›è˜åœºæ™¯ã€‚æœ¬æ–‡æå‡ºä¸€ç§åä¸ºMockLLMçš„æ–°æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆå’Œè¯„ä¼°æ¨¡æ‹Ÿé¢è¯•äº’åŠ¨ï¼ŒåŒ…æ‹¬æ¨¡æ‹Ÿé¢è¯•å’Œæ¡æ‰‹åè®®åŒå‘è¯„ä¼°ä¸¤ä¸ªå…³é”®ç»„ä»¶ã€‚MockLLMé€šè¿‡æ¨¡æ‹Ÿé¢è¯•å®˜å’Œå€™é€‰äººè§’è‰²ï¼Œå®ç°å®æ—¶åŒå‘åŒ¹é…å’Œåä½œäº’åŠ¨ã€‚æ­¤å¤–ï¼ŒMockLLMè¿˜èå…¥åæ€è®°å¿†ç”Ÿæˆå’ŒåŠ¨æ€ç­–ç•¥è°ƒæ•´ï¼ŒåŸºäºä»¥å¾€ç»éªŒä¼˜åŒ–è¡Œä¸ºã€‚åœ¨çœŸå®æ•°æ®Boss Zhipinä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒMockLLMåœ¨åŒ¹é…ç²¾åº¦ã€å¯æ‰©å±•æ€§å’Œè·¨èŒä½é¢†åŸŸçš„é€‚åº”æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå…·æœ‰æ¨åŠ¨å€™é€‰äººè¯„ä¼°å’Œåœ¨çº¿æ‹›è˜çš„æ½œåŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åœ¨çº¿æ‹›è˜å¹³å°æ­£åœ¨æ¨åŠ¨äººèŒåŒ¹é…æŠ€æœ¯çš„è¿›æ­¥ã€‚</li>
<li>ä¼ ç»Ÿæ‹›è˜æ–¹æ³•ä¸»è¦ä¾èµ–é™æ€æ–‡æœ¬æ•°æ®åˆ†æï¼Œç¼ºä¹åŠ¨æ€äº’åŠ¨ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æŠ€æœ¯å¯ä»¥æ¨¡æ‹Ÿè§’è‰²å¯¹è¯ï¼Œé€‚ç”¨äºæ‹›è˜åœºæ™¯ã€‚</li>
<li>MockLLMæ¡†æ¶ç”¨äºç”Ÿæˆå’Œè¯„ä¼°æ¨¡æ‹Ÿé¢è¯•äº’åŠ¨ï¼ŒåŒ…å«æ¨¡æ‹Ÿé¢è¯•å’ŒåŒå‘è¯„ä¼°ã€‚</li>
<li>MockLLMå®ç°å®æ—¶åŒå‘åŒ¹é…å’Œåä½œäº’åŠ¨ï¼Œæ¨¡æ‹Ÿé¢è¯•å®˜å’Œå€™é€‰äººè§’è‰²ã€‚</li>
<li>MockLLMåŸºäºä»¥å¾€ç»éªŒä¼˜åŒ–è¡Œä¸ºï¼Œæé«˜åŒ¹é…è´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.18113">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f03f3e9221562238f1d69c7349f01f7f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e9f09826469e132d736982a08373642a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-670c50df2aee9b3de6490a21a493d309.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3d514b648900e9b841064eb039a41768.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-28/Agent/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-28/Agent/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Agent/">
                                    <span class="chip bg-color">Agent</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-28/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-bb15fab0e13c2d1ff96c357021e20ffc.jpg" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-28  Variational Supervised Contrastive Learning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-28
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-28/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-4c826f5d3ba4bf3338ec4bc057ee9250.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-28  mTSBench Benchmarking Multivariate Time Series Anomaly Detection and   Model Selection at Scale
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-28
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">28292.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
