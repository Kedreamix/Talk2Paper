<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-28  HalluSegBench Counterfactual Visual Reasoning for Segmentation   Hallucination Evaluation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-84a1984fcce907fbf742a3fcde2ffe2f.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-28
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    85 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-28-æ›´æ–°"><a href="#2025-06-28-æ›´æ–°" class="headerlink" title="2025-06-28 æ›´æ–°"></a>2025-06-28 æ›´æ–°</h1><h2 id="HalluSegBench-Counterfactual-Visual-Reasoning-for-Segmentation-Hallucination-Evaluation"><a href="#HalluSegBench-Counterfactual-Visual-Reasoning-for-Segmentation-Hallucination-Evaluation" class="headerlink" title="HalluSegBench: Counterfactual Visual Reasoning for Segmentation   Hallucination Evaluation"></a>HalluSegBench: Counterfactual Visual Reasoning for Segmentation   Hallucination Evaluation</h2><p><strong>Authors:Xinzhuo Li, Adheesh Juvekar, Xingyou Liu, Muntasir Wahed, Kiet A. Nguyen, Ismini Lourentzou</strong></p>
<p>Recent progress in vision-language segmentation has significantly advanced grounded visual understanding. However, these models often exhibit hallucinations by producing segmentation masks for objects not grounded in the image content or by incorrectly labeling irrelevant regions. Existing evaluation protocols for segmentation hallucination primarily focus on label or textual hallucinations without manipulating the visual context, limiting their capacity to diagnose critical failures. In response, we introduce HalluSegBench, the first benchmark specifically designed to evaluate hallucinations in visual grounding through the lens of counterfactual visual reasoning. Our benchmark consists of a novel dataset of 1340 counterfactual instance pairs spanning 281 unique object classes, and a set of newly introduced metrics that quantify hallucination sensitivity under visually coherent scene edits. Experiments on HalluSegBench with state-of-the-art vision-language segmentation models reveal that vision-driven hallucinations are significantly more prevalent than label-driven ones, with models often persisting in false segmentation, highlighting the need for counterfactual reasoning to diagnose grounding fidelity. </p>
<blockquote>
<p>è§†è§‰ä¸è¯­è¨€åˆ†å‰²é¢†åŸŸçš„æœ€æ–°è¿›å±•æå¤§åœ°æ¨åŠ¨äº†åŸºäºå®ä½“çš„è§†è§‰ç†è§£ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹å¾€å¾€ä¼šå‡ºç°å¹»è§‰ï¼Œè¡¨ç°ä¸ºä¸ºå›¾åƒå†…å®¹ä¸­æ²¡æœ‰å®ä½“çš„å¯¹è±¡ç”Ÿæˆåˆ†å‰²æ©ç ï¼Œæˆ–è€…é”™è¯¯åœ°æ ‡è®°ä¸ç›¸å…³çš„åŒºåŸŸã€‚ç°æœ‰çš„åˆ†å‰²å¹»è§‰è¯„ä¼°åè®®ä¸»è¦å…³æ³¨æ ‡ç­¾æˆ–æ–‡æœ¬å¹»è§‰ï¼Œè€Œä¸æ“çºµè§†è§‰ä¸Šä¸‹æ–‡ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬è¯Šæ–­å…³é”®æ•…éšœçš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†HalluSegBenchï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨è®¾è®¡çš„åŸºå‡†æµ‹è¯•ï¼Œé€šè¿‡åäº‹å®è§†è§‰æ¨ç†çš„è§†è§’æ¥è¯„ä¼°è§†è§‰å®šä½ä¸­çš„å¹»è§‰ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•åŒ…æ‹¬ä¸€ä¸ªåŒ…å«1340ä¸ªåäº‹å®å®ä¾‹å¯¹çš„æ–°å‹æ•°æ®é›†ï¼Œè¿™äº›å®ä¾‹æ¶‰åŠ281ä¸ªå”¯ä¸€å¯¹è±¡ç±»åˆ«ï¼Œä»¥åŠä¸€ç»„æ–°å¼•å…¥çš„æŒ‡æ ‡ï¼Œå¯ä»¥åœ¨è§†è§‰ä¸Šè¿è´¯çš„åœºæ™¯ç¼–è¾‘ä¸‹é‡åŒ–å¹»è§‰æ•æ„Ÿæ€§ã€‚åœ¨HalluSegBenchä¸Šä¸æœ€å…ˆè¿›çš„è§†è§‰è¯­è¨€åˆ†å‰²æ¨¡å‹çš„å®éªŒè¡¨æ˜ï¼Œè§†è§‰é©±åŠ¨çš„å¹»è§‰è¿œæ¯”æ ‡ç­¾é©±åŠ¨çš„å¹»è§‰æ™®éå¾—å¤šï¼Œæ¨¡å‹ç»å¸¸å‡ºç°é”™è¯¯çš„åˆ†å‰²ï¼Œè¿™å¼ºè°ƒäº†åäº‹å®æ¨ç†åœ¨è¯Šæ–­å®šä½ä¿çœŸåº¦æ–¹é¢çš„å¿…è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.21546v1">PDF</a> Project webpage: <a target="_blank" rel="noopener" href="https://plan-lab.github.io/hallusegbench/">https://plan-lab.github.io/hallusegbench/</a></p>
<p><strong>Summary</strong><br>è§†è§‰è¯­è¨€åˆ†å‰²é¢†åŸŸçš„æœ€æ–°è¿›å±•æå¤§åœ°æ¨åŠ¨äº†åŸºäºå®é™…è§†è§‰å†…å®¹çš„ç†è§£ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹å¸¸ä¼šå‡ºç°äº§ç”Ÿä¸å›¾åƒå†…å®¹æ— å…³çš„åˆ†å‰²æ©æ¨¡æˆ–é”™è¯¯æ ‡æ³¨æ— å…³åŒºåŸŸç­‰å¹»è§‰ç°è±¡ã€‚ç°æœ‰åˆ†å‰²å¹»è§‰è¯„ä¼°åè®®ä¸»è¦å…³æ³¨æ ‡ç­¾æˆ–æ–‡æœ¬å¹»è§‰ï¼Œè€Œä¸æ¶‰åŠè§†è§‰è¯­å¢ƒçš„å˜åŒ–ï¼Œéš¾ä»¥è¯Šæ–­å…³é”®å¤±è¯¯ã€‚ä¸ºåº”å¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºHalluSegBenchåŸºå‡†æµ‹è¯•ï¼Œä¸“é—¨è¯„ä¼°è§†è§‰å®šä½ä¸­çš„å¹»è§‰ç°è±¡ï¼Œé€šè¿‡åäº‹å®æ¨ç†çš„è§†è§’è¿›è¡Œè¯„ä¼°ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…å«åŒ…å«ç”±åœºæ™¯ç¼–è¾‘ç”Ÿæˆçš„åäº‹å®å®ä¾‹å¯¹çš„æ–°å‹æ•°æ®é›†ä»¥åŠæ–°å¼•å…¥çš„é‡åŒ–å¹»è§‰æ•æ„Ÿåº¦çš„æŒ‡æ ‡ã€‚åœ¨HalluSegBenchåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè§†è§‰é©±åŠ¨çš„å¹»è§‰è¿œæ¯”æ ‡ç­¾é©±åŠ¨çš„å¹»è§‰æ™®éï¼Œæ¨¡å‹ç»å¸¸å‡ºç°é”™è¯¯çš„åˆ†å‰²ç»“æœï¼Œå‡¸æ˜¾äº†åäº‹å®æ¨ç†åœ¨è¯Šæ–­å®šä½ä¿çœŸåº¦æ–¹é¢çš„å¿…è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰è¯­è¨€åˆ†å‰²é¢†åŸŸçš„æœ€æ–°è¿›å±•æ¨åŠ¨äº†åŸºäºå®é™…è§†è§‰å†…å®¹çš„ç†è§£ã€‚</li>
<li>å½“å‰æ¨¡å‹å­˜åœ¨äº§ç”Ÿä¸å›¾åƒå†…å®¹æ— å…³çš„åˆ†å‰²æ©æ¨¡æˆ–é”™è¯¯æ ‡æ³¨æ— å…³åŒºåŸŸç­‰å¹»è§‰ç°è±¡ã€‚</li>
<li>ç°æœ‰è¯„ä¼°åè®®éš¾ä»¥è¯Šæ–­å…³é”®å¤±è¯¯ï¼Œä¸»è¦å…³æ³¨æ ‡ç­¾æˆ–æ–‡æœ¬å¹»è§‰ï¼Œè€Œå¿½è§†è§†è§‰è¯­å¢ƒçš„å˜åŒ–ã€‚</li>
<li>æ¨å‡ºHalluSegBenchåŸºå‡†æµ‹è¯•ï¼Œä¸“é—¨è¯„ä¼°è§†è§‰å®šä½ä¸­çš„å¹»è§‰ç°è±¡ã€‚</li>
<li>è¯¥åŸºå‡†æµ‹è¯•åŒ…å«æ–°å‹åäº‹å®å®ä¾‹å¯¹çš„æ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹åœ¨è§†è§‰è¯­å¢ƒå˜åŒ–ä¸‹çš„è¡¨ç°ã€‚</li>
<li>æ–°å¼•å…¥çš„é‡åŒ–æŒ‡æ ‡ç”¨äºè¯„ä¼°å¹»è§‰æ•æ„Ÿåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.21546">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e227f9932c42cca4b71bf71bb61b0516.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-272944026dfa878afad00bbe14d70110.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8f34b439ef8144adaeae3ba5f492cc04.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="PsyLite-Technical-Report"><a href="#PsyLite-Technical-Report" class="headerlink" title="PsyLite Technical Report"></a>PsyLite Technical Report</h2><p><strong>Authors:Fangjun Ding, Renyu Zhang, Xinyu Feng, Chengye Xie, Zheng Zhang, Yanting Zhang</strong></p>
<p>With the rapid development of digital technology, AI-driven psychological counseling has gradually become an important research direction in the field of mental health. However, existing models still have deficiencies in dialogue safety, detailed scenario handling, and lightweight deployment. To address these issues, this study proposes PsyLite, a lightweight psychological counseling large language model agent developed based on the base model InternLM2.5-7B-chat. Through a two-stage training strategy (hybrid distillation data fine-tuning and ORPO preference optimization), PsyLite enhances the modelâ€™s deep-reasoning ability, psychological counseling ability, and safe dialogue ability. After deployment using Ollama and Open WebUI, a custom workflow is created with Pipelines. An innovative conditional RAG is designed to introduce crosstalk humor elements at appropriate times during psychological counseling to enhance user experience and decline dangerous requests to strengthen dialogue safety. Evaluations show that PsyLite outperforms the baseline models in the Chinese general evaluation (CEval), psychological counseling professional evaluation (CPsyCounE), and dialogue safety evaluation (SafeDialBench), particularly in psychological counseling professionalism (CPsyCounE score improvement of 47.6%) and dialogue safety (\safe{} score improvement of 2.4%). Additionally, the model uses quantization technology (GGUF q4_k_m) to achieve low hardware deployment (5GB memory is sufficient for operation), providing a feasible solution for psychological counseling applications in resource-constrained environments. </p>
<blockquote>
<p>éšç€æ•°å­—æŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼ŒAIé©±åŠ¨çš„å¿ƒç†å’¨è¯¢å·²é€æ¸æˆä¸ºå¿ƒç†å¥åº·é¢†åŸŸçš„é‡è¦ç ”ç©¶æ–¹å‘ã€‚ç„¶è€Œï¼Œç°æœ‰æ¨¡å‹åœ¨å¯¹è¯å®‰å…¨ã€è¯¦ç»†åœºæ™¯å¤„ç†å’Œè½»é‡çº§éƒ¨ç½²æ–¹é¢ä»å­˜åœ¨ä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬ç ”ç©¶æå‡ºäº†PsyLiteï¼Œä¸€ä¸ªåŸºäºInternLM2.5-7B-chatåŸºç¡€æ¨¡å‹å¼€å‘çš„è½»é‡çº§å¿ƒç†å’¨è¯¢å¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†ã€‚é€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼ˆæ··åˆè’¸é¦æ•°æ®å¾®è°ƒä¸ORPOåå¥½ä¼˜åŒ–ï¼‰ï¼ŒPsyLiteå¢å¼ºäº†æ¨¡å‹çš„æ·±åº¦æ¨ç†èƒ½åŠ›ã€å¿ƒç†å’¨è¯¢èƒ½åŠ›å’Œå®‰å…¨å¯¹è¯èƒ½åŠ›ã€‚é€šè¿‡Ollamaå’ŒOpen WebUIè¿›è¡Œéƒ¨ç½²åï¼Œä½¿ç”¨Pipelinesåˆ›å»ºäº†è‡ªå®šä¹‰å·¥ä½œæµç¨‹ã€‚è®¾è®¡äº†ä¸€ç§åˆ›æ–°æ€§çš„æ¡ä»¶RAGï¼Œä»¥åœ¨å¿ƒç†å’¨è¯¢çš„é€‚å½“æ—¶å€™å¼•å…¥äº¤å‰å¯¹è¯å¹½é»˜å…ƒç´ ï¼Œå¢å¼ºç”¨æˆ·ä½“éªŒå¹¶é™ä½å±é™©è¯·æ±‚ä»¥åŠ å¼ºå¯¹è¯å®‰å…¨æ€§ã€‚è¯„ä¼°æ˜¾ç¤ºï¼ŒPsyLiteåœ¨ä¸­å›½é€šç”¨è¯„ä¼°ï¼ˆCEvalï¼‰ã€å¿ƒç†å’¨è¯¢ä¸“ä¸šè¯„ä¼°ï¼ˆCPsyCounEï¼‰å’Œå¯¹è¯å®‰å…¨è¯„ä¼°ï¼ˆSafeDialBenchï¼‰æ–¹é¢ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨å¿ƒç†å’¨è¯¢ä¸“ä¸šï¼ˆCPsyCounEå¾—åˆ†æé«˜äº†47.6%ï¼‰å’Œå¯¹è¯å®‰å…¨ï¼ˆå®‰å…¨å¾—åˆ†æé«˜äº†2.4%ï¼‰ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹é‡‡ç”¨é‡åŒ–æŠ€æœ¯ï¼ˆGGUF q4_k_mï¼‰å®ç°ä½ç¡¬ä»¶éƒ¨ç½²ï¼ˆ5GBå†…å­˜å³å¯è¿è¡Œï¼‰ï¼Œä¸ºèµ„æºå—é™ç¯å¢ƒä¸­çš„å¿ƒç†å’¨è¯¢åº”ç”¨æä¾›äº†å¯è¡Œçš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.21536v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŸºäºæ•°å­—æŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼ŒAIé©±åŠ¨çš„å¿ƒç†å’¨è¯¢å·²æˆä¸ºå¿ƒç†å¥åº·é¢†åŸŸçš„é‡è¦ç ”ç©¶æ–¹å‘ã€‚ä¸ºè§£å†³ç°æœ‰æ¨¡å‹åœ¨å¯¹è¯å®‰å…¨ã€è¯¦ç»†åœºæ™¯å¤„ç†å’Œè½»é‡åŒ–éƒ¨ç½²æ–¹é¢çš„ä¸è¶³ï¼Œæœ¬ç ”ç©¶æå‡ºäº†åŸºäºåŸºç¡€æ¨¡å‹InternLM2.5-7B-chatçš„è½»é‡çº§å¿ƒç†å’¨è¯¢å¤§è¯­è¨€æ¨¡å‹ä»£ç†PsyLiteã€‚é€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼ˆæ··åˆè’¸é¦æ•°æ®å¾®è°ƒä¸ORPOåå¥½ä¼˜åŒ–ï¼‰ï¼ŒPsyLiteå¢å¼ºäº†æ¨¡å‹çš„æ·±åº¦æ¨ç†èƒ½åŠ›ã€å¿ƒç†å’¨è¯¢èƒ½åŠ›å’Œå®‰å…¨å¯¹è¯èƒ½åŠ›ã€‚è¯„ä¼°æ˜¾ç¤ºï¼ŒPsyLiteåœ¨ä¸­æ–‡é€šç”¨è¯„ä¼°ï¼ˆCEvalï¼‰ã€å¿ƒç†å’¨è¯¢ä¸“ä¸šè¯„ä¼°ï¼ˆCPsyCounEï¼‰å’Œå¯¹è¯å®‰å…¨è¯„ä¼°ï¼ˆSafeDialBenchï¼‰ä¸Šè¶…è¶Šäº†åŸºçº¿æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨å¿ƒç†å’¨è¯¢ä¸“ä¸šæ€§å’Œå¯¹è¯å®‰å…¨æ€§æ–¹é¢è¡¨ç°çªå‡ºã€‚æ­¤å¤–ï¼Œæ¨¡å‹é‡‡ç”¨é‡åŒ–æŠ€æœ¯å®ç°ä½ç¡¬ä»¶éƒ¨ç½²ï¼Œä¸ºèµ„æºå—é™ç¯å¢ƒä¸­çš„å¿ƒç†å’¨è¯¢åº”ç”¨æä¾›å¯è¡Œè§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AI-driven psychological counselingå·²æˆä¸ºå¿ƒç†å¥åº·é¢†åŸŸçš„é‡è¦ç ”ç©¶æ–¹å‘ã€‚</li>
<li>ç°æœ‰æ¨¡å‹åœ¨å¯¹è¯å®‰å…¨ã€è¯¦ç»†åœºæ™¯å¤„ç†å’Œè½»é‡åŒ–éƒ¨ç½²æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚</li>
<li>PsyLiteæ¨¡å‹åŸºäºåŸºç¡€æ¨¡å‹InternLM2.5-7B-chatå¼€å‘ï¼Œæ—¨åœ¨è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>PsyLiteé€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥å¢å¼ºäº†æ¨¡å‹çš„æ·±åº¦æ¨ç†èƒ½åŠ›ã€å¿ƒç†å’¨è¯¢èƒ½åŠ›å’Œå®‰å…¨å¯¹è¯èƒ½åŠ›ã€‚</li>
<li>PsyLiteåœ¨ä¸­æ–‡é€šç”¨è¯„ä¼°ã€å¿ƒç†å’¨è¯¢ä¸“ä¸šè¯„ä¼°å’Œå¯¹è¯å®‰å…¨è¯„ä¼°æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>PsyLiteæ¨¡å‹é‡‡ç”¨é‡åŒ–æŠ€æœ¯å®ç°ä½ç¡¬ä»¶éƒ¨ç½²ï¼Œé€‚ç”¨äºèµ„æºå—é™ç¯å¢ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.21536">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-991a4ec68cbaab0df4655dae8b37e315.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1fa9aa639d2270a14ad219139f7f6855.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="ThinkSound-Chain-of-Thought-Reasoning-in-Multimodal-Large-Language-Models-for-Audio-Generation-and-Editing"><a href="#ThinkSound-Chain-of-Thought-Reasoning-in-Multimodal-Large-Language-Models-for-Audio-Generation-and-Editing" class="headerlink" title="ThinkSound: Chain-of-Thought Reasoning in Multimodal Large Language   Models for Audio Generation and Editing"></a>ThinkSound: Chain-of-Thought Reasoning in Multimodal Large Language   Models for Audio Generation and Editing</h2><p><strong>Authors:Huadai Liu, Jialei Wang, Kaicheng Luo, Wen Wang, Qian Chen, Zhou Zhao, Wei Xue</strong></p>
<p>While end-to-end video-to-audio generation has greatly improved, producing high-fidelity audio that authentically captures the nuances of visual content remains challenging. Like professionals in the creative industries, such generation requires sophisticated reasoning about items such as visual dynamics, acoustic environments, and temporal relationships. We present \textbf{ThinkSound}, a novel framework that leverages Chain-of-Thought (CoT) reasoning to enable stepwise, interactive audio generation and editing for videos. Our approach decomposes the process into three complementary stages: foundational foley generation that creates semantically coherent soundscapes, interactive object-centric refinement through precise user interactions, and targeted editing guided by natural language instructions. At each stage, a multimodal large language model generates contextually aligned CoT reasoning that guides a unified audio foundation model. Furthermore, we introduce \textbf{AudioCoT}, a comprehensive dataset with structured reasoning annotations that establishes connections between visual content, textual descriptions, and sound synthesis. Experiments demonstrate that ThinkSound achieves state-of-the-art performance in video-to-audio generation across both audio metrics and CoT metrics and excels in out-of-distribution Movie Gen Audio benchmark. The demo page is available at <a target="_blank" rel="noopener" href="https://thinksound-demo.github.io/">https://ThinkSound-Demo.github.io</a>. </p>
<blockquote>
<p>ç«¯åˆ°ç«¯è§†é¢‘åˆ°éŸ³é¢‘ç”ŸæˆæŠ€æœ¯è™½ç„¶å·²ç»æœ‰äº†å¾ˆå¤§çš„è¿›æ­¥ï¼Œä½†ç”Ÿæˆèƒ½å¤ŸçœŸå®æ•æ‰è§†è§‰å†…å®¹ç»†å¾®ä¹‹å¤„çš„é«˜ä¿çœŸéŸ³é¢‘ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸åˆ›æ„äº§ä¸šçš„ä¸“ä¸šäººå£«ä¸€æ ·ï¼Œè¿™ç§ç”Ÿæˆéœ€è¦å¯¹è§†è§‰åŠ¨æ€ã€å£°å­¦ç¯å¢ƒå’Œæ—¶é—´å…³ç³»ç­‰é¡¹ç›®è¿›è¡Œå¤æ‚æ¨ç†ã€‚æˆ‘ä»¬æå‡ºäº†ThinkSoundæ¡†æ¶ï¼Œå®ƒåˆ©ç”¨æ€ç»´é“¾ï¼ˆChain-of-Thoughtï¼Œç®€ç§°CoTï¼‰æ¨ç†ï¼Œå®ç°è§†é¢‘çš„é€æ­¥äº¤äº’å¼éŸ³é¢‘ç”Ÿæˆå’Œç¼–è¾‘ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†è¿™ä¸€è¿‡ç¨‹åˆ†è§£ä¸ºä¸‰ä¸ªäº’è¡¥é˜¶æ®µï¼šåˆ›å»ºè¯­ä¹‰è¿è´¯å£°éŸ³åœºæ™¯çš„åŸºç¡€éŸ³æ•ˆç”Ÿæˆï¼Œé€šè¿‡ç²¾ç¡®ç”¨æˆ·äº’åŠ¨è¿›è¡Œäº¤äº’å¼ä»¥ç‰©ä½“ä¸ºä¸­å¿ƒçš„ç»†åŒ–ï¼Œä»¥åŠç”±è‡ªç„¶è¯­è¨€æŒ‡ä»¤å¼•å¯¼çš„ç›®æ ‡ç¼–è¾‘ã€‚åœ¨æ¯ä¸ªé˜¶æ®µï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹éƒ½ä¼šäº§ç”Ÿä¸ä¸Šä¸‹æ–‡ç›¸å…³çš„CoTæ¨ç†ï¼Œå¼•å¯¼ç»Ÿä¸€çš„éŸ³é¢‘åŸºç¡€æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†AudioCoTæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«ç»“æ„åŒ–æ¨ç†æ³¨é‡Šï¼Œå»ºç«‹äº†è§†è§‰å†…å®¹ã€æ–‡æœ¬æè¿°å’Œå£°éŸ³åˆæˆä¹‹é—´çš„è¿æ¥ã€‚å®éªŒè¡¨æ˜ï¼ŒThinkSoundåœ¨éŸ³é¢‘æŒ‡æ ‡å’ŒCoTæŒ‡æ ‡ä¸Šçš„è§†é¢‘åˆ°éŸ³é¢‘ç”Ÿæˆæ€§èƒ½å‡è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼Œå¹¶åœ¨Movie Gen AudioåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ã€‚æ¼”ç¤ºé¡µé¢å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://thinksound-demo.github.ioè®¿é—®./">https://ThinkSound-Demo.github.ioè®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.21448v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥é¡¹ç›®æå‡ºäº†ThinkSoundæ¡†æ¶ï¼Œç»“åˆChain-of-Thoughtï¼ˆCoTï¼‰æ¨ç†å®ç°äº†è§†é¢‘çš„äº¤äº’å¼éŸ³é¢‘ç”Ÿæˆå’Œç¼–è¾‘ã€‚é¡¹ç›®åŒ…å«ä¸‰ä¸ªäº’è¡¥é˜¶æ®µï¼šç”Ÿæˆè¯­ä¹‰è¿è´¯çš„éŸ³æ•ˆç¯å¢ƒã€é€šè¿‡ç²¾ç¡®ç”¨æˆ·äº¤äº’è¿›è¡Œäº’åŠ¨å¯¹è±¡ä¸­å¿ƒçš„ç²¾ç»†åŒ–è°ƒæ•´ä»¥åŠç”±è‡ªç„¶è¯­è¨€æŒ‡ä»¤å¼•å¯¼çš„ç›®æ ‡ç¼–è¾‘ã€‚åœ¨æ¯ä¸ªé˜¶æ®µï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹äº§ç”Ÿä¸ä¸Šä¸‹æ–‡ä¸€è‡´çš„CoTæ¨ç†ï¼ŒæŒ‡å¯¼ç»Ÿä¸€çš„éŸ³é¢‘åŸºç¡€æ¨¡å‹ã€‚åŒæ—¶ï¼Œå¼•å…¥AudioCoTæ•°æ®é›†ï¼Œå»ºç«‹äº†è§†è§‰å†…å®¹ã€æ–‡æœ¬æè¿°å’Œå£°éŸ³åˆæˆä¹‹é—´çš„è¿æ¥ã€‚ThinkSoundåœ¨è§†é¢‘è½¬éŸ³é¢‘ç”Ÿæˆä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€æ–°æ°´å¹³çš„è¡¨ç°ï¼Œå¹¶æ“…é•¿å¤„ç†Movie Gen Audioçš„éå¸¸è§„åŸºå‡†æµ‹è¯•åœºæ™¯ã€‚æ­¤å¤–ï¼Œæä¾›äº†æ¼”ç¤ºé¡µé¢ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¡¹ç›®å®ç°äº†ThinkSoundæ¡†æ¶ï¼Œæ—¨åœ¨ä»è§†é¢‘ä¸­ç”Ÿæˆé«˜è´¨é‡çš„éŸ³é¢‘å†…å®¹ï¼Œæ¶µç›–ä¸‰ä¸ªä¸»è¦é˜¶æ®µã€‚</li>
<li>è¯¥æ¡†æ¶ä½¿ç”¨Chain-of-Thoughtï¼ˆCoTï¼‰æ¨ç†æ¥å®ç°æ›´æ™ºèƒ½çš„è§†é¢‘éŸ³é¢‘ç”Ÿæˆè¿‡ç¨‹ã€‚é€šè¿‡æ­¤æ–¹å¼æŒ‡å¯¼å£°éŸ³åˆæˆçš„æ¯ä¸ªé˜¶æ®µï¼Œåˆ›å»ºè¯­å¢ƒç›¸å…³ä¸”å…·æœ‰è¿è´¯æ€§çš„éŸ³æ•ˆã€‚</li>
<li>é¡¹ç›®å¼•å…¥äº†AudioCoTæ•°æ®é›†ï¼Œç”¨äºè®­ç»ƒæ¨¡å‹å¹¶ç†è§£è§†è§‰å†…å®¹ã€æ–‡æœ¬æè¿°å’Œå£°éŸ³åˆæˆä¹‹é—´çš„å…³è”ã€‚è¿™å¯¹äºæé«˜æ¨¡å‹çš„æ€§èƒ½è‡³å…³é‡è¦ã€‚</li>
<li>è¯¥æ¡†æ¶è¡¨ç°å‡ºåœ¨è§†é¢‘è½¬éŸ³é¢‘ç”Ÿæˆé¢†åŸŸçš„é¢†å…ˆæ°´å¹³ï¼Œç‰¹åˆ«æ“…é•¿å¤„ç†éå¸¸è§„çš„æµ‹è¯•åœºæ™¯å’Œæƒ…å¢ƒå¤æ‚å¤šå˜çš„è§†é¢‘å†…å®¹ã€‚è¿™åœ¨å½±è§†å’ŒéŸ³ä¹äº§ä¸šä¸­å°†æœ‰å¾ˆå¤§çš„åº”ç”¨æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.21448">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-66dd18fcff033fff71c61daa722e85ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b89de3e30a6b4be0f267e1a3c107e0d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dde4048b0b9c1d5926a7b747883766e1.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="TableMoE-Neuro-Symbolic-Routing-for-Structured-Expert-Reasoning-in-Multimodal-Table-Understanding"><a href="#TableMoE-Neuro-Symbolic-Routing-for-Structured-Expert-Reasoning-in-Multimodal-Table-Understanding" class="headerlink" title="TableMoE: Neuro-Symbolic Routing for Structured Expert Reasoning in   Multimodal Table Understanding"></a>TableMoE: Neuro-Symbolic Routing for Structured Expert Reasoning in   Multimodal Table Understanding</h2><p><strong>Authors:Junwen Zhang, Pu Chen, Yin Zhang</strong></p>
<p>Multimodal understanding of tables in real-world contexts is challenging due to the complexity of structure, symbolic density, and visual degradation (blur, skew, watermarking, incomplete structures or fonts, multi-span or hierarchically nested layouts). Existing multimodal large language models (MLLMs) struggle with such WildStruct conditions, resulting in limited performance and poor generalization. To address these challenges, we propose TableMoE, a neuro-symbolic Mixture-of-Connector-Experts (MoCE) architecture specifically designed for robust, structured reasoning over multimodal table data. TableMoE features an innovative Neuro-Symbolic Routing mechanism, which predicts latent semantic token roles (e.g., header, data cell, axis, formula) and dynamically routes table elements to specialized experts (Table-to-HTML, Table-to-JSON, Table-to-Code) using a confidence-aware gating strategy informed by symbolic reasoning graphs. To facilitate effective alignment-driven pretraining, we introduce the large-scale TableMoE-Align dataset, consisting of 1.2M table-HTML-JSON-code quadruples across finance, science, biomedicine and industry, utilized exclusively for model pretraining. For evaluation, we curate and release four challenging WildStruct benchmarks: WMMFinQA, WMMTatQA, WMMTabDialog, and WMMFinanceMath, designed specifically to stress-test models under real-world multimodal degradation and structural complexity. Experimental results demonstrate that TableMoE significantly surpasses existing state-of-the-art models. Extensive ablation studies validate each core component, emphasizing the critical role of Neuro-Symbolic Routing and structured expert alignment. Through qualitative analyses, we further showcase TableMoEâ€™s interpretability and enhanced robustness, underscoring the effectiveness of integrating neuro-symbolic reasoning for multimodal table understanding. </p>
<blockquote>
<p>åœ¨çœŸå®ä¸–ç•Œç¯å¢ƒä¸‹å¯¹è¡¨æ ¼è¿›è¡Œå¤šæ¨¡æ€ç†è§£æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ï¼Œå…¶å¤æ‚æ€§ä¸»è¦ä½“ç°åœ¨ç»“æ„ã€ç¬¦å·å¯†åº¦ä»¥åŠè§†è§‰é€€åŒ–ï¼ˆæ¨¡ç³Šã€æ­ªæ–œã€æ°´å°ã€ç»“æ„æˆ–å­—ä½“ä¸å®Œæ•´ã€è·¨å±‚æ¬¡åµŒå¥—å¸ƒå±€ï¼‰ç­‰æ–¹é¢ã€‚ç°æœ‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨é¢ä¸´è¿™ç§WildStructæ¡ä»¶æ—¶è¡¨ç°æŒ£æ‰ï¼Œå¯¼è‡´æ€§èƒ½æœ‰é™å’Œæ³›åŒ–èƒ½åŠ›ä¸ä½³ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†TableMoEï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºåœ¨å¤šæ¨¡æ€è¡¨æ ¼æ•°æ®ä¸Šè¿›è¡Œç¨³å¥ç»“æ„åŒ–æ¨ç†è€Œè®¾è®¡çš„ç¥ç»ç¬¦å·æ··åˆè¿æ¥å™¨ä¸“å®¶ï¼ˆMoCEï¼‰æ¶æ„ã€‚TableMoEçš„ç‰¹ç‚¹æ˜¯ä¸€ç§åˆ›æ–°æ€§çš„ç¥ç»ç¬¦å·è·¯ç”±æœºåˆ¶ï¼Œå®ƒé¢„æµ‹æ½œåœ¨è¯­ä¹‰ä»¤ç‰Œè§’è‰²ï¼ˆä¾‹å¦‚ï¼Œè¡¨å¤´ã€æ•°æ®å•å…ƒæ ¼ã€è½´ã€å…¬å¼ç­‰ï¼‰ï¼Œå¹¶ä½¿ç”¨åŸºäºç¬¦å·æ¨ç†å›¾çš„ç½®ä¿¡åº¦æ„ŸçŸ¥é—¨æ§ç­–ç•¥ï¼ŒåŠ¨æ€åœ°å°†è¡¨æ ¼å…ƒç´ è·¯ç”±åˆ°ä¸“é—¨çš„ä¸“å®¶ï¼ˆè¡¨æ ¼åˆ°HTMLã€è¡¨æ ¼åˆ°JSONã€è¡¨æ ¼åˆ°ä»£ç ï¼‰ã€‚ä¸ºäº†ä¿ƒè¿›æœ‰æ•ˆçš„å¯¹é½é©±åŠ¨é¢„è®­ç»ƒï¼Œæˆ‘ä»¬å¼•å…¥äº†å¤§è§„æ¨¡TableMoE-Alignæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«120ä¸‡å¼ è¡¨æ ¼HTMLJSONä»£ç å››å…ƒç»„åœ¨é‡‘èã€ç§‘å­¦ã€ç”Ÿç‰©åŒ»å­¦å’Œå·¥ä¸šé¢†åŸŸçš„åº”ç”¨ï¼Œä»…ç”¨äºæ¨¡å‹é¢„è®­ç»ƒã€‚ä¸ºäº†è¯„ä¼°ï¼Œæˆ‘ä»¬ç­–åˆ’å¹¶å‘å¸ƒäº†å››ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„WildStructåŸºå‡†æµ‹è¯•ï¼šWMMFinQAã€WMMTatQAã€WMMTabDialogå’ŒWMMFinanceMathï¼Œè¿™äº›æµ‹è¯•ä¸“ä¸ºåœ¨çœŸå®ä¸–ç•Œå¤šæ¨¡æ€é€€åŒ–å’Œç»“æ„å¤æ‚æ€§ä¸‹å¯¹æ¨¡å‹è¿›è¡Œå‹åŠ›æµ‹è¯•è€Œè®¾è®¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTableMoEæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›çš„æ¨¡å‹ã€‚å¹¿æ³›çš„æ¶ˆèç ”ç©¶éªŒè¯äº†æ¯ä¸ªæ ¸å¿ƒç»„ä»¶çš„ä½œç”¨ï¼Œå¼ºè°ƒäº†ç¥ç»ç¬¦å·è·¯ç”±å’Œç»“æ„åŒ–ä¸“å®¶å¯¹é½çš„å…³é”®ä½œç”¨ã€‚é€šè¿‡å®šæ€§åˆ†æï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å±•ç¤ºäº†TableMoEçš„å¯è§£é‡Šæ€§å’Œå¢å¼ºçš„ç¨³å¥æ€§ï¼Œå¼ºè°ƒäº†åœ¨å¤šæ¨¡æ€è¡¨æ ¼ç†è§£ä¸­æ•´åˆç¥ç»ç¬¦å·æ¨ç†çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.21393v1">PDF</a> 43 pages and 11 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºTableMoEçš„ç¥ç»ç¬¦å·æ··åˆä¸“å®¶æ¶æ„ï¼Œæ—¨åœ¨è§£å†³çœŸå®ä¸–ç•Œæƒ…å¢ƒä¸‹è¡¨æ ¼çš„å¤šæ¨¡æ€ç†è§£æŒ‘æˆ˜ã€‚è¯¥æ¶æ„é€šè¿‡ç¥ç»ç¬¦å·è·¯ç”±æœºåˆ¶é¢„æµ‹æ½œåœ¨è¯­ä¹‰ä»¤ç‰Œè§’è‰²ï¼Œå¹¶åŠ¨æ€åœ°å°†è¡¨æ ¼å…ƒç´ è·¯ç”±åˆ°ä¸“å®¶å¤„ã€‚ä¸ºè§£å†³æ¨¡å‹é¢„è®­ç»ƒé—®é¢˜ï¼Œå¼•å…¥äº†å¤§è§„æ¨¡TableMoE-Alignæ•°æ®é›†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTableMoEåœ¨å¤æ‚è¡¨æ ¼çš„å¤šæ¨¡æ€ç†è§£æ–¹é¢æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>çœŸå®ä¸–ç•Œä¸­çš„è¡¨æ ¼å¤šæ¨¡æ€ç†è§£é¢ä¸´ç»“æ„å¤æ‚ã€ç¬¦å·å¯†é›†å’Œè§†è§‰é€€åŒ–ç­‰æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨WildStructæ¡ä»¶ä¸‹è¡¨ç°æœ‰é™ï¼Œç¼ºä¹é€šç”¨æ€§ã€‚</li>
<li>TableMoEæ¶æ„é€šè¿‡ç¥ç»ç¬¦å·æ··åˆä¸“å®¶ï¼ˆMoCEï¼‰æ–¹æ³•è§£å†³è¿™äº›é—®é¢˜ï¼Œå…·æœ‰ç¥ç»ç¬¦å·è·¯ç”±æœºåˆ¶ã€‚</li>
<li>TableMoEèƒ½é¢„æµ‹è¡¨æ ¼å…ƒç´ çš„æ½œåœ¨è¯­ä¹‰ä»¤ç‰Œè§’è‰²ï¼Œå¹¶é€šè¿‡ä¿¡å¿ƒæ„ŸçŸ¥é—¨æ§ç­–ç•¥åŠ¨æ€è·¯ç”±åˆ°ä¸“å®¶å¤„ã€‚</li>
<li>å¼•å…¥å¤§è§„æ¨¡TableMoE-Alignæ•°æ®é›†ï¼Œç”¨äºæ¨¡å‹çš„æœ‰æ•ˆå¯¹é½é©±åŠ¨é¢„è®­ç»ƒã€‚</li>
<li>TableMoEåœ¨å¤æ‚è¡¨æ ¼çš„å¤šæ¨¡æ€ç†è§£æ–¹é¢æ˜¾è‘—è¶…è¶Šç°æœ‰æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.21393">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f28f30185beb7439c905f9e6b22313d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d59f126c7260401c3f7085c956297f0b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8a80bce142d2356d01452b208101ca4a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8df855816330e6b8f7548d267856164d.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="ShotBench-Expert-Level-Cinematic-Understanding-in-Vision-Language-Models"><a href="#ShotBench-Expert-Level-Cinematic-Understanding-in-Vision-Language-Models" class="headerlink" title="ShotBench: Expert-Level Cinematic Understanding in Vision-Language   Models"></a>ShotBench: Expert-Level Cinematic Understanding in Vision-Language   Models</h2><p><strong>Authors:Hongbo Liu, Jingwen He, Yi Jin, Dian Zheng, Yuhao Dong, Fan Zhang, Ziqi Huang, Yinan He, Yangguang Li, Weichao Chen, Yu Qiao, Wanli Ouyang, Shengjie Zhao, Ziwei Liu</strong></p>
<p>Cinematography, the fundamental visual language of film, is essential for conveying narrative, emotion, and aesthetic quality. While recent Vision-Language Models (VLMs) demonstrate strong general visual understanding, their proficiency in comprehending the nuanced cinematic grammar embedded within individual shots remains largely unexplored and lacks robust evaluation. This critical gap limits both fine-grained visual comprehension and the precision of AI-assisted video generation. To address this, we introduce \textbf{ShotBench}, a comprehensive benchmark specifically designed for cinematic language understanding. It features over 3.5k expert-annotated QA pairs from images and video clips, meticulously curated from over 200 acclaimed (predominantly Oscar-nominated) films and spanning eight key cinematography dimensions. Our evaluation of 24 leading VLMs on ShotBench reveals their substantial limitations: even the top-performing model achieves less than 60% average accuracy, particularly struggling with fine-grained visual cues and complex spatial reasoning. To catalyze advancement in this domain, we construct \textbf{ShotQA}, a large-scale multimodal dataset comprising approximately 70k cinematic QA pairs. Leveraging ShotQA, we develop \textbf{ShotVL} through supervised fine-tuning and Group Relative Policy Optimization. ShotVL significantly outperforms all existing open-source and proprietary models on ShotBench, establishing new \textbf{state-of-the-art} performance. We open-source our models, data, and code to foster rapid progress in this crucial area of AI-driven cinematic understanding and generation. </p>
<blockquote>
<p>ç”µå½±æ‘„å½±ä½œä¸ºç”µå½±çš„åŸºæœ¬è§†è§‰è¯­è¨€ï¼Œå¯¹äºä¼ è¾¾å™äº‹ã€æƒ…æ„Ÿå’Œå®¡ç¾è´¨é‡è‡³å…³é‡è¦ã€‚å°½ç®¡æœ€è¿‘çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å±•ç¤ºäº†å¼ºå¤§çš„é€šç”¨è§†è§‰ç†è§£èƒ½åŠ›ï¼Œä½†å¯¹äºç†è§£å•ä¸ªé•œå¤´ä¸­å¾®å¦™çš„ç”µå½±è¯­æ³•çš„ç†Ÿç»ƒç¨‹åº¦ä»ç„¶åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæœªè¢«æ¢ç´¢å¹¶ä¸”ç¼ºä¹ç¨³å¥çš„è¯„ä¼°ã€‚è¿™ä¸€å…³é”®å·®è·é™åˆ¶äº†ç²¾ç»†çš„è§†è§‰ç†è§£å’ŒAIè¾…åŠ©è§†é¢‘ç”Ÿæˆçš„ç²¾åº¦ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†<strong>ShotBench</strong>ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºç”µå½±è¯­è¨€ç†è§£è€Œè®¾è®¡çš„ç»¼åˆåŸºå‡†æµ‹è¯•ã€‚å®ƒåŒ…å«æ¥è‡ªè¶…è¿‡200éƒ¨å¤‡å—èµèª‰ï¼ˆä¸»è¦æ˜¯å¥¥æ–¯å¡æåï¼‰çš„ç”µå½±çš„3.5kå¤šä¸ªä¸“å®¶æ³¨é‡Šçš„é—®ç­”å¯¹ï¼Œæ¶µç›–å…«ä¸ªå…³é”®çš„æ‘„å½±ç»´åº¦ã€‚æˆ‘ä»¬å¯¹ShotBenchçš„24æ¬¾é¢†å…ˆVLMçš„è¯„ä»·æ­ç¤ºäº†å®ƒä»¬çš„é‡å¤§å±€é™æ€§ï¼šå³ä½¿è¡¨ç°æœ€ä½³çš„æ¨¡å‹å¹³å‡å‡†ç¡®ç‡ä¹Ÿä½äº60ï¼…ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†ç»†å¾®çš„è§†è§‰çº¿ç´¢å’Œå¤æ‚çš„ç©ºé—´æ¨ç†æ–¹é¢é‡åˆ°å›°éš¾ã€‚ä¸ºäº†ä¿ƒè¿›è¿™ä¸€é¢†åŸŸçš„è¿›æ­¥ï¼Œæˆ‘ä»¬æ„å»ºäº†å¤§è§„æ¨¡å¤šæ¨¡å¼æ•°æ®é›†<strong>ShotQA</strong>ï¼ŒåŒ…å«çº¦70kä¸ªç”µå½±é—®ç­”å¯¹ã€‚åˆ©ç”¨ShotQAï¼Œæˆ‘ä»¬é€šè¿‡æœ‰ç›‘ç£çš„å¾®è°ƒåˆ†ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼Œå¼€å‘äº†<strong>ShotVL</strong>ã€‚ShotVLåœ¨ShotBenchä¸Šæ˜¾è‘—ä¼˜äºæ‰€æœ‰ç°æœ‰çš„å¼€æºå’Œä¸“æœ‰æ¨¡å‹ï¼Œåˆ›é€ äº†æ–°çš„<strong>æœ€æ–°æŠ€æœ¯æ€§èƒ½</strong>ã€‚æˆ‘ä»¬å¼€æºæˆ‘ä»¬çš„æ¨¡å‹ã€æ•°æ®å’Œä»£ç ï¼Œä»¥ä¿ƒè¿›äººå·¥æ™ºèƒ½é©±åŠ¨çš„ç”µå½±ç†è§£åŠç”Ÿæˆè¿™ä¸€å…³é”®é¢†åŸŸçš„å¿«é€Ÿå‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.21356v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>è¯¥æ–‡æœ¬ä»‹ç»äº†ç”µå½±è§†è§‰è¯­è¨€çš„é‡è¦æ€§ï¼Œå¼ºè°ƒäº†ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç†è§£ç”µå½±é•œå¤´ä¸­çš„ç»†å¾®å·®åˆ«æ–¹é¢å­˜åœ¨çš„å±€é™æ€§ã€‚ä¸ºæ­¤ï¼Œå¼•å…¥äº†ShotBenchåŸºå‡†æµ‹è¯•å¹³å°ä»¥åŠShotQAå’ŒShotVLæ•°æ®é›†å’Œæ¨¡å‹ï¼Œä»¥æ¨åŠ¨å¯¹ç”µå½±è¯­è¨€ç†è§£çš„è¿›æ­¥ã€‚è¯¥æ–‡æœ¬å¼ºè°ƒäº†ç°æœ‰æ¨¡å‹çš„ä¸è¶³ï¼Œå¹¶å±•ç¤ºäº†æ–°æ¨¡å‹çš„ä¼˜è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ç”µå½±è§†è§‰è¯­è¨€çš„é‡è¦æ€§ï¼šç”µå½±è§†è§‰è¯­è¨€å¯¹äºä¼ è¾¾å™äº‹ã€æƒ…æ„Ÿå’Œå®¡ç¾è´¨é‡è‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹çš„å±€é™æ€§ï¼šè™½ç„¶è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ä¸€èˆ¬è§†è§‰ç†è§£æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ç†è§£é•œå¤´ä¸­åµŒå…¥çš„å¾®å¦™ç”µå½±è¯­æ³•æ–¹é¢ä»å­˜åœ¨å¾ˆå¤§ä¸è¶³ã€‚è¿™é™åˆ¶äº†ç²¾ç»†çš„è§†è§‰ç†è§£å’ŒAIè¾…åŠ©è§†é¢‘ç”Ÿæˆçš„ç²¾åº¦ã€‚</li>
<li>ShotBenchåŸºå‡†æµ‹è¯•å¹³å°çš„é‡è¦æ€§ï¼šShotBenchæ˜¯ä¸ºäº†ç”µå½±è¯­è¨€ç†è§£è€Œè®¾è®¡çš„ç»¼åˆåŸºå‡†æµ‹è¯•å¹³å°ï¼ŒåŒ…å«æ¥è‡ªè¶…è¿‡200éƒ¨è·å¥–ç”µå½±çš„è¶…è¿‡3.5kä¸ªä¸“å®¶æ³¨é‡Šçš„é—®é¢˜ç­”æ¡ˆå¯¹ï¼Œæ¶‰åŠå…«ä¸ªå…³é”®çš„æ‘„å½±ç»´åº¦ã€‚</li>
<li>ç°æœ‰æ¨¡å‹åœ¨ShotBenchä¸Šçš„è¡¨ç°ï¼šå¯¹ç°æœ‰24ä¸ªé¢†å…ˆçš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ShotBenchä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œå®ƒä»¬çš„å¹³å‡å‡†ç¡®ç‡ä½äº60%ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†ç»†å¾®çš„è§†è§‰çº¿ç´¢å’Œå¤æ‚çš„ç©ºé—´æ¨ç†æ–¹é¢å­˜åœ¨å›°éš¾ã€‚</li>
<li>ShotQAæ•°æ®é›†çš„ä»‹ç»ï¼šä¸ºäº†æ¨åŠ¨è¯¥é¢†åŸŸçš„è¿›æ­¥ï¼Œå¼•å…¥äº†ShotQAæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„å¤šæ¨¡å¼ç”µå½±é—®ç­”å¯¹æ•°æ®é›†ï¼ŒåŒ…å«å¤§çº¦70kä¸ªç”µå½±é—®é¢˜ç­”æ¡ˆå¯¹ã€‚</li>
<li>ShotVLæ¨¡å‹çš„å‘å±•ï¼šé€šè¿‡åˆ©ç”¨ShotQAæ•°æ®é›†è¿›è¡Œæœ‰ç›‘ç£çš„å¾®è°ƒå’ŒGroup Relative Policy Optimizationï¼Œå‘å±•å‡ºäº†ShotVLæ¨¡å‹ã€‚è¯¥æ¨¡å‹åœ¨ShotBenchä¸Šæ˜¾è‘—ä¼˜äºæ‰€æœ‰ç°æœ‰çš„å¼€æºå’Œä¸“æœ‰æ¨¡å‹ï¼Œè¾¾åˆ°äº†æ–°çš„å…ˆè¿›æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.21356">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-1555fa274539ba43114b4e9bc8eb3385.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-568f2aef1f52a64ab8b92ecda762d653.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3d4f5dcb667d98acfd01a263a69c4d01.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-73a79725965f33a75a3fdc1cd26ba5b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-01c6cc23c7c4a5bf59db2f0d1f8e17df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d4c1ab37832c32770d9b96cbeedfe59.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f9f1cdc76727af2b28eb6e9cf2373841.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="HumanOmniV2-From-Understanding-to-Omni-Modal-Reasoning-with-Context"><a href="#HumanOmniV2-From-Understanding-to-Omni-Modal-Reasoning-with-Context" class="headerlink" title="HumanOmniV2: From Understanding to Omni-Modal Reasoning with Context"></a>HumanOmniV2: From Understanding to Omni-Modal Reasoning with Context</h2><p><strong>Authors:Qize Yang, Shimin Yao, Weixuan Chen, Shenghao Fu, Detao Bai, Jiaxing Zhao, Boyuan Sun, Bowen Yin, Xihan Wei, Jingren Zhou</strong></p>
<p>With the rapid evolution of multimodal large language models, the capacity to deeply understand and interpret human intentions has emerged as a critical capability, which demands detailed and thoughtful reasoning. In recent studies, Reinforcement Learning (RL) has demonstrated potential in enhancing the reasoning capabilities of Large Language Models (LLMs). Nonetheless, the challenges associated with adapting RL to multimodal data and formats remain largely unaddressed. In this paper, we identify two issues in existing multimodal reasoning models: insufficient global context understanding and shortcut problems. Insufficient context understanding can happen when a model misinterprets multimodal context, resulting in incorrect answers. The shortcut problem occurs when the model overlooks crucial clues in multimodal inputs, directly addressing the query without considering the multimodal information. To tackle these issues, we emphasize the necessity for the model to reason with a clear understanding of the global context within multimodal inputs. This global context understanding can effectively prevent the model from overlooking key multimodal cues and ensure a thorough reasoning process. To ensure the accurate interpretation of multimodal context information, we implement a context reward judged by a large language model, alongside format and accuracy rewards. Additionally, to improve complex reasoning capability, we employ the LLM to assess the logical reward, determining whether the reasoning process successfully integrates multimodal information with logical methods. We also introduce a reasoning omni-modal benchmark, IntentBench, aimed at evaluating models in understanding complex human intentions and emotions. Our proposed method demonstrates advanced performance across multiple omni-modal benchmarks compared to other open-source omni-modal models. </p>
<blockquote>
<p>éšç€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„å¿«é€Ÿå‘å±•ï¼Œæ·±åº¦ç†è§£å’Œè§£é‡Šäººç±»æ„å›¾çš„èƒ½åŠ›å·²ç»æˆä¸ºäº†ä¸€é¡¹è‡³å…³é‡è¦çš„æŠ€æœ¯ï¼Œè¿™è¦æ±‚è¯¦ç»†çš„æ€è€ƒæ¨ç†ã€‚è¿‘æœŸç ”ç©¶ä¸­ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›æ–¹é¢è¡¨ç°å‡ºäº†æ½œåŠ›ã€‚ç„¶è€Œï¼Œå°†å¼ºåŒ–å­¦ä¹ é€‚åº”äºå¤šæ¨¡æ€æ•°æ®å’Œæ ¼å¼çš„æŒ‘æˆ˜ä»å¤§é‡å­˜åœ¨ä¸”æœªè§£å†³ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æŒ‡å‡ºäº†ç°æœ‰å¤šæ¨¡æ€æ¨ç†æ¨¡å‹çš„ä¸¤ä¸ªé—®é¢˜ï¼šå…¨å±€ä¸Šä¸‹æ–‡ç†è§£ä¸è¶³å’Œæ·å¾„é—®é¢˜ã€‚å½“æ¨¡å‹è¯¯è§£å¤šæ¨¡æ€ä¸Šä¸‹æ–‡æ—¶ï¼Œå¯èƒ½ä¼šå‘ç”Ÿä¸Šä¸‹æ–‡ç†è§£ä¸è¶³çš„æƒ…å†µï¼Œä»è€Œå¯¼è‡´ç­”æ¡ˆé”™è¯¯ã€‚æ·å¾„é—®é¢˜åˆ™å‘ç”Ÿåœ¨æ¨¡å‹å¿½è§†å¤šæ¨¡æ€è¾“å…¥ä¸­çš„é‡è¦çº¿ç´¢ï¼Œç›´æ¥å›ç­”æŸ¥è¯¢è€Œå¿½ç•¥å¤šæ¨¡æ€ä¿¡æ¯çš„æƒ…å†µã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼ºè°ƒæ¨¡å‹éœ€è¦åœ¨æ˜ç¡®ç†è§£å¤šæ¨¡æ€è¾“å…¥çš„å…¨å±€ä¸Šä¸‹æ–‡çš„åŸºç¡€ä¸Šè¿›è¡Œæ¨ç†ã€‚è¿™ç§å…¨å±€ä¸Šä¸‹æ–‡çš„ç†è§£å¯ä»¥æœ‰æ•ˆåœ°é˜²æ­¢æ¨¡å‹å¿½ç•¥å…³é”®çš„å¤šæ¨¡æ€çº¿ç´¢ï¼Œå¹¶ç¡®ä¿ä¸€ä¸ªå½»åº•çš„æ¨ç†è¿‡ç¨‹ã€‚ä¸ºäº†ç¡®ä¿å¤šæ¨¡æ€ä¸Šä¸‹æ–‡ä¿¡æ¯çš„å‡†ç¡®è§£é‡Šï¼Œæˆ‘ä»¬å®æ–½äº†ä¸€ç§ç”±å¤§å‹è¯­è¨€æ¨¡å‹åˆ¤æ–­çš„èƒŒæ™¯å¥–åŠ±æœºåˆ¶ï¼ŒåŒæ—¶è¾…ä»¥æ ¼å¼å’Œå‡†ç¡®åº¦çš„å¥–åŠ±ã€‚æ­¤å¤–ï¼Œä¸ºäº†æé«˜å¤æ‚çš„æ¨ç†èƒ½åŠ›ï¼Œæˆ‘ä»¬è¿˜åˆ©ç”¨LLMæ¥è¯„ä¼°é€»è¾‘å¥–åŠ±ï¼Œç¡®å®šæ¨ç†è¿‡ç¨‹æ˜¯å¦æˆåŠŸåœ°å°†å¤šæ¨¡æ€ä¿¡æ¯ä¸é€»è¾‘æ–¹æ³•ç›¸ç»“åˆã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªè·¨æ¨¡æ€æ¨ç†åŸºå‡†æµ‹è¯•å¹³å°IntentBenchï¼Œæ—¨åœ¨è¯„ä¼°æ¨¡å‹åœ¨ç†è§£å¤æ‚çš„äººç±»æ„å›¾å’Œæƒ…æ„Ÿæ–¹é¢çš„èƒ½åŠ›ã€‚ä¸å…¶ä»–å¼€æºçš„è·¨æ¨¡æ€æ¨¡å‹ç›¸æ¯”ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•åœ¨å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.21277v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>éšç€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„å¿«é€Ÿå‘å±•ï¼Œæ·±åº¦ç†è§£å’Œè§£é‡Šäººç±»æ„å›¾çš„èƒ½åŠ›å·²æˆä¸ºä¸€é¡¹å…³é”®æŠ€èƒ½ï¼Œè¿™è¦æ±‚è¯¦ç»†çš„æ€è€ƒæ¨ç†ã€‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ã€‚ç„¶è€Œï¼Œå°†RLé€‚åº”äºå¤šæ¨¡æ€æ•°æ®å’Œæ ¼å¼çš„æŒ‘æˆ˜å°šæœªå¾—åˆ°å……åˆ†è§£å†³ã€‚æœ¬æ–‡è¯†åˆ«äº†ç°æœ‰å¤šæ¨¡æ€æ¨ç†æ¨¡å‹çš„ä¸¤ä¸ªé—®é¢˜ï¼šå…¨å±€ä¸Šä¸‹æ–‡ç†è§£ä¸è¶³å’Œæ·å¾„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼ºè°ƒæ¨¡å‹éœ€è¦åœ¨å¤šæ¨¡æ€è¾“å…¥ä¸­æ¸…æ™°ç†è§£å…¨å±€ä¸Šä¸‹æ–‡è¿›è¡Œæ¨ç†çš„å¿…è¦æ€§ã€‚ä¸ºäº†ç¡®ä¿å¯¹å¤šæ¨¡æ€ä¸Šä¸‹æ–‡ä¿¡æ¯çš„å‡†ç¡®è§£é‡Šï¼Œæˆ‘ä»¬é‡‡ç”¨ç”±å¤§å‹è¯­è¨€æ¨¡å‹åˆ¤æ–­çš„ä¸Šæ–‡å¥–åŠ±ã€æ ¼å¼å’Œå‡†ç¡®å¥–åŠ±ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†å¤šæ¨¡æ€æ¨ç†åŸºå‡†æµ‹è¯•IntentBenchï¼Œæ—¨åœ¨è¯„ä¼°æ¨¡å‹åœ¨ç†è§£å¤æ‚äººç±»æ„å›¾å’Œæƒ…æ„Ÿæ–¹é¢çš„èƒ½åŠ›ã€‚ç›¸æ¯”å…¶ä»–å¼€æºå¤šæ¨¡æ€æ¨¡å‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹éœ€æ·±åº¦ç†è§£å’Œè§£é‡Šäººç±»æ„å›¾ï¼Œè¦æ±‚è¯¦ç»†çš„æ€è€ƒæ¨ç†ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ åœ¨å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æ–¹é¢å…·æœ‰æ½œåŠ›ã€‚</li>
<li>ç°æœ‰å¤šæ¨¡æ€æ¨ç†æ¨¡å‹é¢ä¸´å…¨å±€ä¸Šä¸‹æ–‡ç†è§£ä¸è¶³å’Œæ·å¾„é—®é¢˜ã€‚</li>
<li>æ¨¡å‹éœ€è¦æ¸…æ™°ç†è§£å…¨å±€ä¸Šä¸‹æ–‡è¿›è¡Œå¤šæ¨¡æ€æ¨ç†ï¼Œé¿å…è¯¯è§£å’Œå¿½ç•¥å…³é”®ä¿¡æ¯ã€‚</li>
<li>å¼•å…¥ä¸Šä¸‹æ–‡å¥–åŠ±ã€æ ¼å¼å’Œå‡†ç¡®å¥–åŠ±ï¼Œç¡®ä¿å¯¹å¤šæ¨¡æ€ä¸Šä¸‹æ–‡ä¿¡æ¯çš„å‡†ç¡®è§£é‡Šã€‚</li>
<li>å¼•å…¥å¤šæ¨¡æ€æ¨ç†åŸºå‡†æµ‹è¯•IntentBenchï¼Œè¯„ä¼°æ¨¡å‹åœ¨ç†è§£å¤æ‚äººç±»æ„å›¾å’Œæƒ…æ„Ÿæ–¹é¢çš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.21277">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-00b44fc554fa9a99207560e734feaf95.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fc1f48ecb0fb54252c06c248c9f63694.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3e1ccbae4155ff2efaa96a6a6900ba19.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45e0b1cce4d103533b95053211afae88.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Unveiling-Causal-Reasoning-in-Large-Language-Models-Reality-or-Mirage"><a href="#Unveiling-Causal-Reasoning-in-Large-Language-Models-Reality-or-Mirage" class="headerlink" title="Unveiling Causal Reasoning in Large Language Models: Reality or Mirage?"></a>Unveiling Causal Reasoning in Large Language Models: Reality or Mirage?</h2><p><strong>Authors:Haoang Chi, He Li, Wenjing Yang, Feng Liu, Long Lan, Xiaoguang Ren, Tongliang Liu, Bo Han</strong></p>
<p>Causal reasoning capability is critical in advancing large language models (LLMs) toward strong artificial intelligence. While versatile LLMs appear to have demonstrated capabilities in understanding contextual causality and providing responses that obey the laws of causality, it remains unclear whether they perform genuine causal reasoning akin to humans. However, current evidence indicates the contrary. Specifically, LLMs are only capable of performing shallow (level-1) causal reasoning, primarily attributed to the causal knowledge embedded in their parameters, but they lack the capacity for genuine human-like (level-2) causal reasoning. To support this hypothesis, methodologically, we delve into the autoregression mechanism of transformer-based LLMs, revealing that it is not inherently causal. Empirically, we introduce a new causal Q&amp;A benchmark called CausalProbe-2024, whose corpora are fresh and nearly unseen for the studied LLMs. The LLMs exhibit a significant performance drop on CausalProbe-2024 compared to earlier benchmarks, indicating the fact that they primarily engage in level-1 causal reasoning. To bridge the gap towards level-2 causal reasoning, we draw inspiration from the fact that human reasoning is usually facilitated by general knowledge and intended goals. We propose G^2-Reasoner, a method that incorporates general knowledge and goal-oriented prompts into LLMsâ€™ causal reasoning processes. Experiments demonstrate that G^2-Reasoner significantly enhances LLMsâ€™ causal reasoning capability, particularly in fresh and counterfactual contexts. This work sheds light on a new path for LLMs to advance towards genuine causal reasoning, going beyond level-1 and making strides towards level-2. </p>
<blockquote>
<p>å› æœæ¨ç†èƒ½åŠ›åœ¨æ¨åŠ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å‘å¼ºå¤§çš„äººå·¥æ™ºèƒ½å‘å±•æ–¹é¢è‡³å…³é‡è¦ã€‚è™½ç„¶é€šç”¨LLMä¼¼ä¹å·²å±•ç°å‡ºç†è§£ä¸Šä¸‹æ–‡å› æœå…³ç³»çš„èƒ½åŠ›ï¼Œå¹¶èƒ½æä¾›éµå¾ªå› æœå¾‹çš„å›åº”ï¼Œä½†å°šä¸æ¸…æ¥šå®ƒä»¬æ˜¯å¦åƒäººç±»ä¸€æ ·è¿›è¡ŒçœŸæ­£çš„å› æœæ¨ç†ã€‚ç„¶è€Œï¼Œç›®å‰çš„è¯æ®è¡¨æ˜æ°æ°ç›¸åã€‚å…·ä½“æ¥è¯´ï¼ŒLLMåªèƒ½è¿›è¡Œæµ…å±‚æ¬¡çš„ï¼ˆä¸€çº§ï¼‰å› æœæ¨ç†ï¼Œè¿™ä¸»è¦å½’å› äºåµŒå…¥å…¶å‚æ•°ä¸­çš„å› æœçŸ¥è¯†ï¼Œä½†å®ƒä»¬ç¼ºä¹ç±»ä¼¼äººç±»ï¼ˆäºŒçº§ï¼‰çš„å› æœæ¨ç†èƒ½åŠ›ã€‚ä¸ºäº†æ”¯æŒè¿™ä¸€å‡è®¾ï¼Œåœ¨æ–¹æ³•ä¸Šï¼Œæˆ‘ä»¬æ·±å…¥ç ”ç©¶äº†åŸºäºå˜å‹å™¨çš„LLMçš„è‡ªå›å½’æœºåˆ¶ï¼Œå‘ç°å®ƒå¹¶éå›ºæœ‰åœ°å…·æœ‰å› æœæ€§ã€‚åœ¨å®è¯æ–¹é¢ï¼Œæˆ‘ä»¬å¼•å…¥äº†åä¸ºCausalProbe-2024çš„æ–°å› æœé—®ç­”åŸºå‡†æµ‹è¯•ï¼Œå…¶è¯­æ–™åº“å¯¹äºæ‰€ç ”ç©¶çš„LLMè€Œè¨€æ˜¯æ–°é²œä¸”å‡ ä¹æœªè¢«è§è¿‡çš„ã€‚ä¸æ—©æœŸåŸºå‡†æµ‹è¯•ç›¸æ¯”ï¼ŒLLMåœ¨CausalProbe-2024ä¸Šçš„æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œè¿™è¡¨æ˜å®ƒä»¬ä¸»è¦è¿›è¡Œä¸€çº§å› æœæ¨ç†ã€‚ä¸ºäº†å¼¥è¡¥å®ç°äºŒçº§å› æœæ¨ç†çš„å·®è·ï¼Œæˆ‘ä»¬ä»äººç±»æ¨ç†é€šå¸¸å€ŸåŠ©é€šç”¨çŸ¥è¯†å’Œç›®æ ‡å¯¼å‘è¿™ä¸€äº‹å®ä¸­è·å¾—çµæ„Ÿã€‚æˆ‘ä»¬æå‡ºäº†G^2-Reasoneræ–¹æ³•ï¼Œå®ƒå°†é€šç”¨çŸ¥è¯†å’Œç›®æ ‡å¯¼å‘æç¤ºèå…¥LLMçš„å› æœæ¨ç†è¿‡ç¨‹ã€‚å®éªŒè¡¨æ˜ï¼ŒG^2-Reasoneræ˜¾è‘—å¢å¼ºäº†LLMçš„å› æœæ¨ç†èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨æ–°é¢–å’Œåäº‹å®æƒ…å¢ƒä¸‹ã€‚è¿™é¡¹å·¥ä½œä¸ºLLMèµ°å‘çœŸæ­£çš„å› æœæ¨ç†æŒ‡æ˜äº†ä¸€æ¡æ–°é€”å¾„ï¼Œè¶…è¶Šäº†ä¸€çº§æ¨ç†ï¼Œæœç€äºŒçº§æ¨ç†è¿ˆè¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.21215v1">PDF</a> 24 pages, accepted at NeurIPS 2024</p>
<p><strong>Summary</strong></p>
<p>å¤§è¯­è¨€æ¨¡å‹çš„å› æœæ¨ç†èƒ½åŠ›å¯¹äºå®ç°å¼ºäººå·¥æ™ºèƒ½è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹å·²ç»å…·å¤‡ç†è§£ä¸Šä¸‹æ–‡å› æœå…³ç³»å’Œéµå®ˆå› æœæ³•åˆ™çš„èƒ½åŠ›ï¼Œä½†å®ƒä»¬ä»åªèƒ½è¿›è¡Œæµ…å±‚æ¬¡çš„å› æœæ¨ç†ï¼ˆLevel-1ï¼‰ï¼Œç¼ºä¹äººç±»èˆ¬çš„æ·±å±‚æ¬¡ï¼ˆLevel-2ï¼‰å› æœæ¨ç†èƒ½åŠ›ã€‚æœ¬æ–‡æ­ç¤ºäº†åŸºäºTransformerçš„å¤§å‹è¯­è¨€æ¨¡å‹çš„è‡ªå›å½’æœºåˆ¶å¹¶éå›ºæœ‰åœ°å…·æœ‰å› æœæ€§ï¼Œå¹¶å¼•å…¥äº†æ–°çš„å› æœé—®ç­”åŸºå‡†æµ‹è¯•CausalProbe-2024ã€‚å®éªŒè¡¨æ˜ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹åœ¨CausalProbe-2024ä¸Šçš„æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œè¡¨æ˜å®ƒä»¬ä¸»è¦è¿›è¡ŒLevel-1å› æœæ¨ç†ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæœ¬æ–‡æå‡ºäº†èå…¥é€šç”¨çŸ¥è¯†å’Œç›®æ ‡å¯¼å‘æç¤ºçš„G^2-Reasoneræ–¹æ³•ã€‚å®éªŒè¯æ˜ï¼ŒG^2-Reasonerèƒ½æ˜¾è‘—æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„å› æœæ¨ç†èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨æ–°é¢–å’Œåäº‹å®æƒ…å¢ƒä¸‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å› æœæ¨ç†æ–¹é¢ä»æœ‰å±€é™ï¼Œåªèƒ½è¿›è¡Œæµ…å±‚æ¬¡çš„å› æœæ¨ç†ï¼ˆLevel-1ï¼‰ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹çš„è‡ªå›å½’æœºåˆ¶å¹¶éå›ºæœ‰åœ°å…·æœ‰å› æœæ€§ã€‚</li>
<li>å¼•å…¥æ–°çš„å› æœé—®ç­”åŸºå‡†æµ‹è¯•CausalProbe-2024ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ­¤æµ‹è¯•ä¸Šçš„æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ç¼ºä¹äººç±»èˆ¬çš„æ·±å±‚æ¬¡ï¼ˆLevel-2ï¼‰å› æœæ¨ç†èƒ½åŠ›ã€‚</li>
<li>ä¸ºäº†æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„å› æœæ¨ç†èƒ½åŠ›ï¼Œæå‡ºäº†G^2-Reasoneræ–¹æ³•ã€‚</li>
<li>G^2-Reasoneré€šè¿‡èå…¥é€šç”¨çŸ¥è¯†å’Œç›®æ ‡å¯¼å‘æç¤ºï¼Œèƒ½æ˜¾è‘—æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„å› æœæ¨ç†èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.21215">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-7dd320d6f3c531829d5d1d380cd29c4c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-24d3bad817072dee02d2d3e3616b0197.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5b50ff158eae464289a66dc20652b5ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87ca9f4ce2fc55e281d395ceef70e0e5.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="GroundFlow-A-Plug-in-Module-for-Temporal-Reasoning-on-3D-Point-Cloud-Sequential-Grounding"><a href="#GroundFlow-A-Plug-in-Module-for-Temporal-Reasoning-on-3D-Point-Cloud-Sequential-Grounding" class="headerlink" title="GroundFlow: A Plug-in Module for Temporal Reasoning on 3D Point Cloud   Sequential Grounding"></a>GroundFlow: A Plug-in Module for Temporal Reasoning on 3D Point Cloud   Sequential Grounding</h2><p><strong>Authors:Zijun Lin, Shuting He, Cheston Tan, Bihan Wen</strong></p>
<p>Sequential grounding in 3D point clouds (SG3D) refers to locating sequences of objects by following text instructions for a daily activity with detailed steps. Current 3D visual grounding (3DVG) methods treat text instructions with multiple steps as a whole, without extracting useful temporal information from each step. However, the instructions in SG3D often contain pronouns such as â€œitâ€, â€œhereâ€ and â€œthe sameâ€ to make language expressions concise. This requires grounding methods to understand the context and retrieve relevant information from previous steps to correctly locate object sequences. Due to the lack of an effective module for collecting related historical information, state-of-the-art 3DVG methods face significant challenges in adapting to the SG3D task. To fill this gap, we propose GroundFlow â€“ a plug-in module for temporal reasoning on 3D point cloud sequential grounding. Firstly, we demonstrate that integrating GroundFlow improves the task accuracy of 3DVG baseline methods by a large margin (+7.5% and +10.2%) in the SG3D benchmark, even outperforming a 3D large language model pre-trained on various datasets. Furthermore, we selectively extract both short-term and long-term step information based on its relevance to the current instruction, enabling GroundFlow to take a comprehensive view of historical information and maintain its temporal understanding advantage as step counts increase. Overall, our work introduces temporal reasoning capabilities to existing 3DVG models and achieves state-of-the-art performance in the SG3D benchmark across five datasets. </p>
<blockquote>
<p>è¿ç»­å®šä½ä¸‰ç»´ç‚¹äº‘ï¼ˆSG3Dï¼‰æ˜¯æŒ‡é€šè¿‡éµå¾ªåŒ…å«è¯¦ç»†æ­¥éª¤çš„æ–‡æœ¬æŒ‡ä»¤æ¥å®šä½æ—¥å¸¸æ´»åŠ¨ä¸­çš„å¯¹è±¡åºåˆ—ã€‚å½“å‰çš„ä¸‰ç»´è§†è§‰å®šä½ï¼ˆ3DVGï¼‰æ–¹æ³•å°†åŒ…å«å¤šä¸ªæ­¥éª¤çš„æ–‡æœ¬æŒ‡ä»¤è§†ä¸ºæ•´ä½“ï¼Œè€Œæ²¡æœ‰ä»æ¯ä¸ªæ­¥éª¤ä¸­æå–æœ‰ç”¨çš„æ—¶é—´ä¿¡æ¯ã€‚ç„¶è€Œï¼ŒSG3Dä¸­çš„æŒ‡ä»¤é€šå¸¸åŒ…å«ä»£è¯ï¼Œå¦‚â€œå®ƒâ€ã€â€œè¿™é‡Œâ€å’Œâ€œåŒä¸€ä¸ªâ€ï¼Œä»¥ä½¿è¯­è¨€è¡¨è¾¾ç®€æ´ã€‚è¿™è¦æ±‚å®šä½æ–¹æ³•ç†è§£ä¸Šä¸‹æ–‡å¹¶æ£€ç´¢ä»å‰ä¸€æ­¥éª¤ä¸­çš„ç›¸å…³ä¿¡æ¯ï¼Œä»¥æ­£ç¡®å®šä½å¯¹è±¡åºåˆ—ã€‚ç”±äºç¼ºä¹æ”¶é›†ç›¸å…³å†å²ä¿¡æ¯çš„æœ‰æ•ˆæ¨¡å—ï¼Œæœ€å…ˆè¿›çš„ä¸‰ç»´è§†è§‰å®šä½æ–¹æ³•é¢ä¸´ç€é€‚åº”SG3Dä»»åŠ¡çš„é‡å¤§æŒ‘æˆ˜ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†GroundFlowâ€”â€”ä¸€ä¸ªç”¨äºä¸‰ç»´ç‚¹äº‘åºåˆ—å®šä½çš„æ—¶é—´æ¨ç†æ’ä»¶æ¨¡å—ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è¯æ˜äº†é›†æˆGroundFlowå¯ä»¥å¤§å¹…åº¦æé«˜ä¸‰ç»´è§†è§‰å®šä½åŸºçº¿æ–¹æ³•çš„ä»»åŠ¡å‡†ç¡®æ€§ï¼ˆ+7.5%å’Œ+10.2%ï¼‰ï¼Œå³ä½¿åœ¨SG3DåŸºå‡†æµ‹è¯•ä¸­ä¹Ÿæ˜¯å¦‚æ­¤ï¼Œç”šè‡³å¯ä»¥è¶…è¶Šåœ¨å„ç§æ•°æ®é›†ä¸Šé¢„å…ˆè®­ç»ƒçš„ä¸‰ç»´å¤§å‹è¯­è¨€æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ ¹æ®å½“å‰æŒ‡ä»¤çš„ç›¸å…³æ€§æœ‰é€‰æ‹©åœ°æå–çŸ­æœŸå’Œé•¿æœŸæ­¥éª¤ä¿¡æ¯ï¼Œä½¿GroundFlowèƒ½å¤Ÿå…¨é¢æŸ¥çœ‹å†å²ä¿¡æ¯å¹¶ä¿æŒå…¶éšæ—¶é—´æ¨ç§»è€Œå¢å¼ºçš„æ—¶é—´ç†è§£ä¼˜åŠ¿ã€‚æ€»ä½“è€Œè¨€ï¼Œæˆ‘ä»¬çš„å·¥ä½œä¸ºç°æœ‰çš„ä¸‰ç»´è§†è§‰å®šä½æ¨¡å‹å¼•å…¥äº†æ—¶é—´æ¨ç†èƒ½åŠ›ï¼Œå¹¶åœ¨SG3DåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†è·¨äº”ä¸ªæ•°æ®é›†çš„æœ€ä½³æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.21188v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é¡ºåºå®šä½ä¸‰ç»´ç‚¹äº‘ï¼ˆSG3Dï¼‰çš„æ¦‚å¿µï¼ŒæŒ‡å‡ºä¼ ç»Ÿä¸‰ç»´è§†è§‰å®šä½æ–¹æ³•åœ¨å¤„ç†åŒ…å«å¤šä¸ªæ­¥éª¤çš„æ–‡æœ¬æŒ‡ä»¤æ—¶ï¼Œæ— æ³•æå–æœ‰ç”¨çš„æ—¶é—´ä¿¡æ¯ã€‚é’ˆå¯¹æ­¤é—®é¢˜ï¼Œæå‡ºäº†GroundFlowæ¨¡å—ï¼Œç”¨äºåœ¨ä¸‰ç»´ç‚¹äº‘ä¸Šè¿›è¡Œæ—¶åºæ¨ç†ã€‚GroundFlowé€šè¿‡æ•´åˆå†å²ä¿¡æ¯ï¼Œæé«˜äº†ç°æœ‰ä¸‰ç»´è§†è§‰å®šä½åŸºçº¿æ–¹æ³•çš„ä»»åŠ¡å‡†ç¡®æ€§ï¼Œå¹¶åœ¨SG3DåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œç”šè‡³è¶…è¶Šäº†é¢„è®­ç»ƒäºå¤šä¸ªæ•°æ®é›†çš„å¤§å‹ä¸‰ç»´è¯­è¨€æ¨¡å‹ã€‚GroundFlowèƒ½å¤Ÿé€‰æ‹©æ€§æå–ä¸å½“å‰æŒ‡ä»¤ç›¸å…³çš„çŸ­æœŸå’Œé•¿æœŸæ­¥éª¤ä¿¡æ¯ï¼Œå…·æœ‰å…¨é¢çš„å†å²è§†è§’å’Œç»´æŒæ—¶é—´ç†è§£çš„ä¼˜åŠ¿ï¼Œå³ä½¿åœ¨æ­¥éª¤æ•°é‡å¢åŠ çš„æƒ…å†µä¸‹ä¾ç„¶æœ‰æ•ˆã€‚ç ”ç©¶å®ç°äº†ç°æœ‰ä¸‰ç»´è§†è§‰å®šä½æ¨¡å‹çš„æ—¶åºæ¨ç†èƒ½åŠ›ï¼Œå¹¶åœ¨äº”ä¸ªæ•°æ®é›†ä¸Šçš„SG3DåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°é¢†å…ˆæ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SG3Dæ¶‰åŠæ ¹æ®æ–‡æœ¬æŒ‡ä»¤æŒ‰é¡ºåºå®šä½å¯¹è±¡ï¼Œä½†ç°æœ‰æ–¹æ³•æ— æ³•æœ‰æ•ˆå¤„ç†åŒ…å«æ—¶é—´ä¿¡æ¯çš„æŒ‡ä»¤ã€‚</li>
<li>GroundFlowæ¨¡å—æ˜¯ä¸€ç§ç”¨äºå¤„ç†ä¸‰ç»´ç‚¹äº‘æ—¶åºæ¨ç†çš„æ’ä»¶ï¼Œèƒ½å¤Ÿæ•´åˆå†å²ä¿¡æ¯æé«˜å®šä½å‡†ç¡®æ€§ã€‚</li>
<li>GroundFlowæé«˜äº†åŸºçº¿æ–¹æ³•çš„ä»»åŠ¡å‡†ç¡®æ€§ï¼Œå¹¶åœ¨SG3DåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</li>
<li>GroundFlowèƒ½å¤Ÿé€‰æ‹©æ€§æå–ä¸å½“å‰æŒ‡ä»¤ç›¸å…³çš„çŸ­æœŸå’Œé•¿æœŸæ­¥éª¤ä¿¡æ¯ï¼Œå…·æœ‰å…¨é¢çš„å†å²è§†è§’ã€‚</li>
<li>GroundFlowåœ¨æ­¥éª¤æ•°é‡å¢åŠ çš„æƒ…å†µä¸‹ä¾ç„¶èƒ½å¤Ÿç»´æŒå…¶æ—¶é—´ç†è§£çš„ä¼˜åŠ¿ã€‚</li>
<li>ä¸é¢„è®­ç»ƒçš„å¤§å‹ä¸‰ç»´è¯­è¨€æ¨¡å‹ç›¸æ¯”ï¼ŒGroundFlowå…·æœ‰ä¼˜è¶Šçš„æ€§èƒ½è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.21188">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7adc96a95ca81d8650ed37c1eaa8772a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e2eaccab61aa16f727352418c62c5e3b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a7e08cce0f8148ff744b606907f7670d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-35acb8e2a178860802d9e1bb1b21ba80.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Maintaining-MTEB-Towards-Long-Term-Usability-and-Reproducibility-of-Embedding-Benchmarks"><a href="#Maintaining-MTEB-Towards-Long-Term-Usability-and-Reproducibility-of-Embedding-Benchmarks" class="headerlink" title="Maintaining MTEB: Towards Long Term Usability and Reproducibility of   Embedding Benchmarks"></a>Maintaining MTEB: Towards Long Term Usability and Reproducibility of   Embedding Benchmarks</h2><p><strong>Authors:Isaac Chung, Imene Kerboua, Marton Kardos, Roman Solomatin, Kenneth Enevoldsen</strong></p>
<p>The Massive Text Embedding Benchmark (MTEB) has become a standard evaluation platform for text embedding models. While previous work has established the core benchmark methodology, this paper focuses on the engineering aspects that ensure MTEBâ€™s continued reproducibility and extensibility. We present our approach to maintaining robust continuous integration pipelines that validate dataset integrity, automate test execution, and assess benchmark resultsâ€™ generalizability. We detail the design choices that collectively enhance reproducibility and usability. Furthermore, we discuss our strategies for handling community contributions and extending the benchmark with new tasks and datasets. These engineering practices have been instrumental in scaling MTEB to become more comprehensive while maintaining quality and, ultimately, relevance to the field. Our experiences offer valuable insights for benchmark maintainers facing similar challenges in ensuring reproducibility and usability in machine learning evaluation frameworks. The MTEB repository is available at: <a target="_blank" rel="noopener" href="https://github.com/embeddings-benchmark/mteb">https://github.com/embeddings-benchmark/mteb</a> </p>
<blockquote>
<p>å¤§è§„æ¨¡æ–‡æœ¬åµŒå…¥åŸºå‡†æµ‹è¯•ï¼ˆMTEBï¼‰å·²æˆä¸ºæ–‡æœ¬åµŒå…¥æ¨¡å‹çš„æ ‡å‡†è¯„ä¼°å¹³å°ã€‚è™½ç„¶ä»¥å‰çš„å·¥ä½œå·²ç»å»ºç«‹äº†æ ¸å¿ƒåŸºå‡†æµ‹è¯•æ–¹æ³•ï¼Œä½†æœ¬æ–‡ä¾§é‡äºå·¥ç¨‹æ–¹é¢ï¼Œä»¥ç¡®ä¿MTEBçš„æŒç»­å¯é‡å¤æ€§å’Œå¯æ‰©å±•æ€§ã€‚æˆ‘ä»¬å±•ç¤ºäº†ç»´æŠ¤ç¨³å¥çš„æŒç»­é›†æˆç®¡é“çš„æ–¹æ³•ï¼Œè¯¥ç®¡é“éªŒè¯æ•°æ®é›†å®Œæ•´æ€§ã€è‡ªåŠ¨åŒ–æµ‹è¯•æ‰§è¡Œå¹¶è¯„ä¼°åŸºå‡†æµ‹è¯•ç»“æœçš„å¯æ¨å¹¿æ€§ã€‚æˆ‘ä»¬è¯¦ç»†è¯´æ˜äº†è®¾è®¡é€‰æ‹©ï¼Œè¿™äº›é€‰æ‹©å…±åŒæé«˜äº†å¯é‡å¤æ€§å’Œæ˜“ç”¨æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¨è®ºäº†å¤„ç†ç¤¾åŒºè´¡çŒ®å’Œæ‰©å±•åŸºå‡†æµ‹è¯•çš„ç­–ç•¥ï¼ŒåŒ…æ‹¬æ·»åŠ æ–°ä»»åŠ¡å’Œæ•°æ®é›†ã€‚è¿™äº›å·¥ç¨‹å®è·µå¯¹äºä½¿MTEBåœ¨ä¿æŒè´¨é‡çš„åŒæ—¶å˜å¾—æ›´åŠ å…¨é¢ï¼Œå¹¶æœ€ç»ˆæˆä¸ºè¯¥é¢†åŸŸçš„æ›´å…¨é¢çš„ç›¸å…³æ€§æ–¹é¢å‘æŒ¥äº†é‡è¦ä½œç”¨ã€‚æˆ‘ä»¬çš„ç»éªŒä¸ºé¢ä¸´ç±»ä¼¼æŒ‘æˆ˜ã€ç¡®ä¿æœºå™¨å­¦ä¹ è¯„ä¼°æ¡†æ¶çš„å¯é‡å¤æ€§å’Œå¯ç”¨æ€§çš„åŸºå‡†æµ‹è¯•ç»´æŠ¤äººå‘˜æä¾›äº†å®è´µçš„è§è§£ã€‚MTEBä»“åº“ä½äºï¼š<a target="_blank" rel="noopener" href="https://github.com/embeddings-benchmark/mteb%E3%80%82">https://github.com/embeddings-benchmark/mteb</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.21182v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†å¤§è§„æ¨¡æ–‡æœ¬åµŒå…¥åŸºå‡†æµ‹è¯•ï¼ˆMTEBï¼‰å¹³å°åœ¨å·¥ç¨‹æ–¹é¢çš„æ”¹è¿›ï¼ŒåŒ…æ‹¬å¦‚ä½•ç¡®ä¿æ•°æ®é›†å®Œæ•´æ€§ã€è‡ªåŠ¨åŒ–æµ‹è¯•æ‰§è¡Œã€è¯„ä¼°åŸºå‡†æµ‹è¯•ç»“æœçš„å¯æ¨å¹¿æ€§ï¼Œä»¥åŠå¦‚ä½•å¤„ç†ç¤¾åŒºè´¡çŒ®å’Œæ‰©å±•åŸºå‡†æµ‹è¯•ä»¥åº”å¯¹æ–°ä»»åŠ¡å’Œæ•°æ®é›†çš„ç­–ç•¥ã€‚è¿™äº›å·¥ç¨‹å®è·µå¯¹ç¡®ä¿MTEBçš„è´¨é‡ã€å…¨é¢æ€§å’Œå¯¹é¢†åŸŸçš„å®ç”¨æ€§è‡³å…³é‡è¦ã€‚æ­¤å¤–ï¼Œè¿˜æä¾›äº†ä¸€äº›æœ‰ä»·å€¼çš„ä¿¡æ¯ï¼Œä»¥ä¾›é¢ä¸´ç±»ä¼¼æŒ‘æˆ˜çš„åŸºå‡†æµ‹è¯•ç»´æŠ¤è€…å‚è€ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MTEBå·²æˆä¸ºæ–‡æœ¬åµŒå…¥æ¨¡å‹çš„æ ‡å‡†è¯„ä¼°å¹³å°ã€‚</li>
<li>è¯¥è®ºæ–‡å…³æ³¨MTEBçš„å·¥ç¨‹æ–¹é¢ï¼Œç¡®ä¿æŒç»­çš„å¯é‡å¤æ€§å’Œå¯æ‰©å±•æ€§ã€‚</li>
<li>é€šè¿‡ç»´æŠ¤ç¨³å¥çš„è¿ç»­é›†æˆç®¡é“æ¥éªŒè¯æ•°æ®é›†å®Œæ•´æ€§ã€è‡ªåŠ¨åŒ–æµ‹è¯•æ‰§è¡Œå’Œè¯„ä¼°åŸºå‡†ç»“æœçš„å¯æ¨å¹¿æ€§ã€‚</li>
<li>è®¾è®¡é€‰æ‹©å…±åŒæé«˜å¯é‡å¤æ€§å’Œå¯ç”¨æ€§ã€‚</li>
<li>å¤„ç†ç¤¾åŒºè´¡çŒ®å’Œæ‰©å±•åŸºå‡†æµ‹è¯•çš„ç­–ç•¥è¢«è¯¦ç»†ä»‹ç»ã€‚</li>
<li>å·¥ç¨‹å®è·µæœ‰åŠ©äºMTEBåœ¨ä¿æŒè´¨é‡çš„åŒæ—¶å®ç°è§„æ¨¡åŒ–ï¼Œå¹¶ä¿æŒå…¶åœ¨é¢†åŸŸä¸­çš„å®ç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.21182">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-889fb2246d9340dbacab1c29e5525894.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-04eaadf67a2099825ffb62fde2684aaf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8033a88f1a8da9e382dea41b1b10bb26.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Interpretable-Hierarchical-Concept-Reasoning-through-Attention-Guided-Graph-Learning"><a href="#Interpretable-Hierarchical-Concept-Reasoning-through-Attention-Guided-Graph-Learning" class="headerlink" title="Interpretable Hierarchical Concept Reasoning through Attention-Guided   Graph Learning"></a>Interpretable Hierarchical Concept Reasoning through Attention-Guided   Graph Learning</h2><p><strong>Authors:David Debot, Pietro Barbiero, Gabriele Dominici, Giuseppe Marra</strong></p>
<p>Concept-Based Models (CBMs) are a class of deep learning models that provide interpretability by explaining predictions through high-level concepts. These models first predict concepts and then use them to perform a downstream task. However, current CBMs offer interpretability only for the final task prediction, while the concept predictions themselves are typically made via black-box neural networks. To address this limitation, we propose Hierarchical Concept Memory Reasoner (H-CMR), a new CBM that provides interpretability for both concept and task predictions. H-CMR models relationships between concepts using a learned directed acyclic graph, where edges represent logic rules that define concepts in terms of other concepts. During inference, H-CMR employs a neural attention mechanism to select a subset of these rules, which are then applied hierarchically to predict all concepts and the final task. Experimental results demonstrate that H-CMR matches state-of-the-art performance while enabling strong human interaction through concept and model interventions. The former can significantly improve accuracy at inference time, while the latter can enhance data efficiency during training when background knowledge is available. </p>
<blockquote>
<p>åŸºäºæ¦‚å¿µæ¨¡å‹ï¼ˆCBMï¼‰æ˜¯ä¸€ç±»æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œé€šè¿‡è§£é‡Šé«˜çº§æ¦‚å¿µæ¥æä¾›é¢„æµ‹çš„å¯è§£é‡Šæ€§ã€‚è¿™äº›æ¨¡å‹é¦–å…ˆé¢„æµ‹æ¦‚å¿µï¼Œç„¶åä½¿ç”¨è¿™äº›æ¦‚å¿µæ¥å®Œæˆä¸‹æ¸¸ä»»åŠ¡ã€‚ç„¶è€Œï¼Œå½“å‰çš„CBMåªä¸ºæœ€ç»ˆçš„ä»»åŠ¡é¢„æµ‹æä¾›å¯è§£é‡Šæ€§ï¼Œè€Œæ¦‚å¿µé¢„æµ‹æœ¬èº«é€šå¸¸æ˜¯é€šè¿‡é»‘ç®±ç¥ç»ç½‘ç»œå®Œæˆçš„ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†åˆ†å±‚æ¦‚å¿µè®°å¿†æ¨ç†å™¨ï¼ˆH-CMRï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„CBMï¼Œå¯ä»¥ä¸ºæ¦‚å¿µå’Œä»»åŠ¡é¢„æµ‹æä¾›å¯è§£é‡Šæ€§ã€‚H-CMRä½¿ç”¨å­¦ä¹ åˆ°çš„æœ‰å‘æ— ç¯å›¾æ¥æ¨¡æ‹Ÿæ¦‚å¿µä¹‹é—´çš„å…³ç³»ï¼Œå…¶ä¸­è¾¹ç¼˜ä»£è¡¨é€»è¾‘è§„åˆ™ï¼Œè¿™äº›è§„åˆ™ä»¥å…¶ä»–æ¦‚å¿µçš„å½¢å¼å®šä¹‰æ¦‚å¿µã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼ŒH-CMRé‡‡ç”¨ç¥ç»æ³¨æ„åŠ›æœºåˆ¶æ¥é€‰æ‹©è¿™äº›è§„åˆ™çš„ä¸€ä¸ªå­é›†ï¼Œç„¶ååˆ†å±‚åº”ç”¨è¿™äº›è§„åˆ™æ¥é¢„æµ‹æ‰€æœ‰æ¦‚å¿µå’Œæœ€ç»ˆä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒH-CMRè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶é€šè¿‡æ¦‚å¿µå’Œæ¨¡å‹å¹²é¢„å®ç°äº†å¼ºçƒˆçš„äººæœºäº¤äº’ã€‚å‰è€…å¯ä»¥åœ¨æ¨ç†æ—¶æ˜¾è‘—æé«˜å‡†ç¡®æ€§ï¼Œè€Œåè€…åœ¨å¯ç”¨èƒŒæ™¯çŸ¥è¯†çš„æƒ…å†µä¸‹ï¼Œå¯ä»¥æé«˜è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ•°æ®æ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.21102v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ¦‚å¿µåŸºç¡€æ¨¡å‹ï¼ˆCBMï¼‰æ˜¯ä¸€ç±»æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œé€šè¿‡é¢„æµ‹é«˜çº§æ¦‚å¿µæ¥è§£é‡Šé¢„æµ‹ç»“æœï¼Œè¿›è€Œæ‰§è¡Œä¸‹æ¸¸ä»»åŠ¡ã€‚ç„¶è€Œï¼Œå½“å‰CBMçš„è§£è¯»æ€§ä»…é™äºæœ€ç»ˆä»»åŠ¡é¢„æµ‹ï¼Œæ¦‚å¿µé¢„æµ‹é€šå¸¸æ˜¯é€šè¿‡é»‘ç®±ç¥ç»ç½‘ç»œå®Œæˆçš„ã€‚ä¸ºè§£å†³æ­¤å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†åˆ†å±‚æ¦‚å¿µè®°å¿†æ¨ç†å™¨ï¼ˆH-CMRï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„CBMï¼Œå¯ä¸ºæ¦‚å¿µå’Œä»»åŠ¡é¢„æµ‹æä¾›è§£è¯»æ€§ã€‚H-CMRé€šè¿‡å­¦åˆ°çš„æœ‰å‘æ— ç¯å›¾å¯¹æ¦‚å¿µä¹‹é—´çš„å…³ç³»è¿›è¡Œå»ºæ¨¡ï¼Œå…¶ä¸­è¾¹ä»£è¡¨é€»è¾‘è§„åˆ™ï¼Œä»¥å…¶ä»–æ¦‚å¿µå®šä¹‰æ¦‚å¿µã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼ŒH-CMRé‡‡ç”¨ç¥ç»æ³¨æ„åŠ›æœºåˆ¶é€‰æ‹©è¿™äº›è§„åˆ™çš„ä¸€ä¸ªå­é›†ï¼Œç„¶åé€å±‚åº”ç”¨ä»¥é¢„æµ‹æ‰€æœ‰æ¦‚å¿µå’Œæœ€ç»ˆä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒH-CMRè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶é€šè¿‡æ¦‚å¿µå’Œæ¨¡å‹å¹²é¢„å®ç°äº†å¼ºå¤§çš„äººç±»äº’åŠ¨ï¼Œå¯ä»¥æé«˜æ¨ç†æ—¶çš„å‡†ç¡®æ€§ï¼Œå¹¶åœ¨æœ‰èƒŒæ™¯çŸ¥è¯†çš„æƒ…å†µä¸‹æé«˜è®­ç»ƒæ—¶çš„æ•°æ®æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¦‚å¿µåŸºç¡€æ¨¡å‹ï¼ˆCBMï¼‰æ˜¯ä¸€ç±»æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œé€šè¿‡è§£é‡Šé«˜çº§æ¦‚å¿µæ¥æä¾›é¢„æµ‹çš„è§£é‡Šæ€§ã€‚</li>
<li>å½“å‰CBMçš„å±€é™æ€§åœ¨äºå…¶è§£è¯»æ€§ä»…é™äºæœ€ç»ˆä»»åŠ¡é¢„æµ‹ï¼Œæ¦‚å¿µé¢„æµ‹æ˜¯é»‘ç®±æ“ä½œã€‚</li>
<li>H-CMRæ˜¯ä¸€ç§æ–°çš„CBMï¼Œèƒ½å¤Ÿå¯¹æ¦‚å¿µå’Œä»»åŠ¡é¢„æµ‹æä¾›è§£è¯»æ€§ã€‚</li>
<li>H-CMRé€šè¿‡å­¦åˆ°çš„æœ‰å‘æ— ç¯å›¾å¯¹æ¦‚å¿µä¹‹é—´çš„å…³ç³»è¿›è¡Œå»ºæ¨¡ï¼Œè¾¹ä»£è¡¨é€»è¾‘è§„åˆ™ã€‚</li>
<li>H-CMRé‡‡ç”¨ç¥ç»æ³¨æ„åŠ›æœºåˆ¶é€‰æ‹©è§„åˆ™å­é›†ï¼Œç”¨äºé¢„æµ‹æ‰€æœ‰æ¦‚å¿µå’Œæœ€ç»ˆä»»åŠ¡ã€‚</li>
<li>å®éªŒè¯æ˜H-CMRæ€§èƒ½å…ˆè¿›ï¼Œé€šè¿‡æ¦‚å¿µå’Œæ¨¡å‹å¹²é¢„å®ç°äº†äººç±»ä¸æ¨¡å‹çš„è‰¯å¥½äº’åŠ¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.21102">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e13ab5f83258354c8d64b760146a39f9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fa9089cd112a02f0ebd0c6671969a998.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fd54881b6bbee97e752adb89ad053d77.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="STEP-Planner-Constructing-cross-hierarchical-subgoal-tree-as-an-embodied-long-horizon-task-planner"><a href="#STEP-Planner-Constructing-cross-hierarchical-subgoal-tree-as-an-embodied-long-horizon-task-planner" class="headerlink" title="STEP Planner: Constructing cross-hierarchical subgoal tree as an   embodied long-horizon task planner"></a>STEP Planner: Constructing cross-hierarchical subgoal tree as an   embodied long-horizon task planner</h2><p><strong>Authors:Zhou Tianxing, Wang Zhirui, Ao Haojia, Chen Guangyan, Xing Boyang, Cheng Jingwen, Yang Yi, Yue Yufeng</strong></p>
<p>The ability to perform reliable long-horizon task planning is crucial for deploying robots in real-world environments. However, directly employing Large Language Models (LLMs) as action sequence generators often results in low success rates due to their limited reasoning ability for long-horizon embodied tasks. In the STEP framework, we construct a subgoal tree through a pair of closed-loop models: a subgoal decomposition model and a leaf node termination model. Within this framework, we develop a hierarchical tree structure that spans from coarse to fine resolutions. The subgoal decomposition model leverages a foundation LLM to break down complex goals into manageable subgoals, thereby spanning the subgoal tree. The leaf node termination model provides real-time feedback based on environmental states, determining when to terminate the tree spanning and ensuring each leaf node can be directly converted into a primitive action. Experiments conducted in both the VirtualHome WAH-NL benchmark and on real robots demonstrate that STEP achieves long-horizon embodied task completion with success rates up to 34% (WAH-NL) and 25% (real robot) outperforming SOTA methods. </p>
<blockquote>
<p>å°†æœºå™¨äººéƒ¨ç½²åœ¨çœŸå®ä¸–ç•Œç¯å¢ƒä¸­æ—¶ï¼Œæ‰§è¡Œå¯é çš„é•¿å‘¨æœŸä»»åŠ¡è§„åˆ’çš„èƒ½åŠ›è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç›´æ¥ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºåŠ¨ä½œåºåˆ—ç”Ÿæˆå™¨ï¼Œå¾€å¾€ä¼šå› ä¸ºå…¶å¯¹é•¿å‘¨æœŸå®ä½“ä»»åŠ¡æ¨ç†èƒ½åŠ›çš„å±€é™è€Œå¯¼è‡´æˆåŠŸç‡è¾ƒä½ã€‚åœ¨STEPæ¡†æ¶ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡ä¸€å¯¹é—­ç¯æ¨¡å‹æ„å»ºäº†ä¸€ä¸ªå­ç›®æ ‡æ ‘ï¼ŒåŒ…æ‹¬å­ç›®æ ‡åˆ†è§£æ¨¡å‹å’Œå¶èŠ‚ç‚¹ç»ˆæ­¢æ¨¡å‹ã€‚åœ¨æ­¤æ¡†æ¶å†…ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§ä»ç²—åˆ°ç»†çš„åˆ†å±‚æ ‘ç»“æ„ã€‚å­ç›®æ ‡åˆ†è§£æ¨¡å‹åˆ©ç”¨åŸºç¡€LLMå°†å¤æ‚ç›®æ ‡åˆ†è§£ä¸ºå¯ç®¡ç†çš„å­ç›®æ ‡ï¼Œä»è€Œæ„å»ºå­ç›®æ ‡æ ‘ã€‚å¶èŠ‚ç‚¹ç»ˆæ­¢æ¨¡å‹æ ¹æ®ç¯å¢ƒçŠ¶æ€æä¾›å®æ—¶åé¦ˆï¼Œç¡®å®šä½•æ—¶ç»ˆæ­¢æ ‘è·¨è¶Šï¼Œå¹¶ç¡®ä¿æ¯ä¸ªå¶èŠ‚ç‚¹éƒ½èƒ½ç›´æ¥è½¬åŒ–ä¸ºåŸå§‹åŠ¨ä½œã€‚åœ¨VirtualHome WAH-NLåŸºå‡†æµ‹è¯•å’ŒçœŸå®æœºå™¨äººä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSTEPå®ç°äº†é•¿å‘¨æœŸå®ä½“ä»»åŠ¡çš„å®Œæˆï¼ŒæˆåŠŸç‡é«˜è¾¾WAH-NLçš„34%å’ŒçœŸå®æœºå™¨äººçš„25%ï¼Œè¶…è¿‡äº†æœ€æ–°æ–¹æ³•çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.21030v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœºå™¨äººè¦åœ¨çœŸå®ç¯å¢ƒä¸­éƒ¨ç½²ï¼Œè¿›è¡Œå¯é çš„é•¿è¿œä»»åŠ¡è§„åˆ’æ˜¯è‡³å…³é‡è¦çš„ã€‚ç„¶è€Œï¼Œç›´æ¥ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºè¡ŒåŠ¨åºåˆ—ç”Ÿæˆå™¨å¾€å¾€æˆåŠŸç‡è¾ƒä½ï¼Œå› ä¸ºå®ƒä»¬å¯¹äºé•¿è¿œä»»åŠ¡çš„æ¨ç†èƒ½åŠ›æœ‰é™ã€‚åœ¨STEPæ¡†æ¶ä¸­ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå­ç›®æ ‡æ ‘ï¼ŒåŒ…æ‹¬ä¸€ä¸ªå­ç›®æ ‡åˆ†è§£æ¨¡å‹å’Œä¸€ä¸ªå¶èŠ‚ç‚¹ç»ˆæ­¢æ¨¡å‹ã€‚å­ç›®æ ‡åˆ†è§£æ¨¡å‹åˆ©ç”¨åŸºç¡€å¤§å‹è¯­è¨€æ¨¡å‹å°†å¤æ‚ç›®æ ‡åˆ†è§£ä¸ºå¯ç®¡ç†çš„å­ç›®æ ‡ï¼Œæ„å»ºä»ç²—åˆ°ç»†çš„å±‚æ¬¡ç»“æ„ã€‚å¶èŠ‚ç‚¹ç»ˆæ­¢æ¨¡å‹æ ¹æ®ç¯å¢ƒçŠ¶æ€æä¾›å®æ—¶åé¦ˆï¼Œç¡®å®šä½•æ—¶ç»ˆæ­¢æ ‘æ‰©å±•ï¼Œç¡®ä¿æ¯ä¸ªå¶èŠ‚ç‚¹èƒ½ç›´æ¥è½¬åŒ–ä¸ºåŸå§‹åŠ¨ä½œã€‚åœ¨VirtualHome WAH-NLåŸºå‡†æµ‹è¯•å’ŒçœŸå®æœºå™¨äººä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSTEPå®ç°äº†é•¿è¿œå®ä½“ä»»åŠ¡å®Œæˆï¼ŒæˆåŠŸç‡é«˜è¾¾WAH-NLçš„34%å’ŒçœŸå®æœºå™¨äººçš„25%ï¼Œè¶…è¿‡äº†ç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¯é çš„é•¿è¿œä»»åŠ¡è§„åˆ’å¯¹äºæœºå™¨äººéƒ¨ç½²è‡³å…³é‡è¦ã€‚</li>
<li>ç›´æ¥ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºè¡ŒåŠ¨åºåˆ—ç”Ÿæˆå™¨çš„æˆåŠŸç‡è¾ƒä½ã€‚</li>
<li>STEPæ¡†æ¶é€šè¿‡æ„å»ºå­ç›®æ ‡æ ‘æ¥è§£å†³é•¿è¿œä»»åŠ¡è§„åˆ’é—®é¢˜ã€‚</li>
<li>å­ç›®æ ‡åˆ†è§£æ¨¡å‹åˆ©ç”¨åŸºç¡€å¤§å‹è¯­è¨€æ¨¡å‹å°†å¤æ‚ç›®æ ‡åˆ†è§£ä¸ºå¯ç®¡ç†çš„å­ç›®æ ‡ã€‚</li>
<li>å¶èŠ‚ç‚¹ç»ˆæ­¢æ¨¡å‹æä¾›å®æ—¶åé¦ˆï¼Œç¡®ä¿ä»»åŠ¡åœ¨çœŸå®ç¯å¢ƒä¸­æœ‰æ•ˆæ‰§è¡Œã€‚</li>
<li>STEPæ¡†æ¶åœ¨WAH-NLåŸºå‡†æµ‹è¯•å’ŒçœŸå®æœºå™¨äººä¸Šçš„å®éªŒè¡¨ç°å‡ºè¾ƒé«˜çš„æˆåŠŸç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.21030">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2b6089e4116ba9929d8be88ba42ae9ec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a872ae29eba860f5afa28ad105aec0e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-27349e466749d702ea8f809655d4a138.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d365399ed5003b81805a48575826472.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f7cf6fd51a0eac516cda4ffa622ecce7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e8f3892d673d334afb1bac13e588c79e.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Evidence-based-diagnostic-reasoning-with-multi-agent-copilot-for-human-pathology"><a href="#Evidence-based-diagnostic-reasoning-with-multi-agent-copilot-for-human-pathology" class="headerlink" title="Evidence-based diagnostic reasoning with multi-agent copilot for human   pathology"></a>Evidence-based diagnostic reasoning with multi-agent copilot for human   pathology</h2><p><strong>Authors:Chengkuan Chen, Luca L. Weishaupt, Drew F. K. Williamson, Richard J. Chen, Tong Ding, Bowen Chen, Anurag Vaidya, Long Phi Le, Guillaume Jaume, Ming Y. Lu, Faisal Mahmood</strong></p>
<p>Pathology is experiencing rapid digital transformation driven by whole-slide imaging and artificial intelligence (AI). While deep learning-based computational pathology has achieved notable success, traditional models primarily focus on image analysis without integrating natural language instruction or rich, text-based context. Current multimodal large language models (MLLMs) in computational pathology face limitations, including insufficient training data, inadequate support and evaluation for multi-image understanding, and a lack of autonomous, diagnostic reasoning capabilities. To address these limitations, we introduce PathChat+, a new MLLM specifically designed for human pathology, trained on over 1 million diverse, pathology-specific instruction samples and nearly 5.5 million question answer turns. Extensive evaluations across diverse pathology benchmarks demonstrated that PathChat+ substantially outperforms the prior PathChat copilot, as well as both state-of-the-art (SOTA) general-purpose and other pathology-specific models. Furthermore, we present SlideSeek, a reasoning-enabled multi-agent AI system leveraging PathChat+ to autonomously evaluate gigapixel whole-slide images (WSIs) through iterative, hierarchical diagnostic reasoning, reaching high accuracy on DDxBench, a challenging open-ended differential diagnosis benchmark, while also capable of generating visually grounded, humanly-interpretable summary reports. </p>
<blockquote>
<p>ç—…ç†å­¦æ­£åœ¨ç»å†ç”±å…¨åˆ‡ç‰‡æˆåƒå’Œäººå·¥æ™ºèƒ½ï¼ˆAIï¼‰é©±åŠ¨çš„å¿«é€Ÿæ•°å­—åŒ–è½¬å‹ã€‚è™½ç„¶åŸºäºæ·±åº¦å­¦ä¹ çš„è®¡ç®—ç—…ç†å­¦å·²ç»å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†ä¼ ç»Ÿæ¨¡å‹ä¸»è¦å…³æ³¨å›¾åƒåˆ†æï¼Œæ²¡æœ‰æ•´åˆè‡ªç„¶è¯­è¨€æŒ‡ä»¤æˆ–ä¸°å¯Œçš„æ–‡æœ¬ä¸Šä¸‹æ–‡ã€‚ç›®å‰è®¡ç®—ç—…ç†å­¦ä¸­çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰é¢ä¸´ä¸€äº›å±€é™æ€§ï¼ŒåŒ…æ‹¬è®­ç»ƒæ•°æ®ä¸è¶³ã€å¯¹å¤šå›¾åƒç†è§£çš„æ”¯æŒå’Œè¯„ä¼°ä¸è¶³ï¼Œä»¥åŠç¼ºä¹è‡ªä¸»è¯Šæ–­æ¨ç†èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æ¨å‡ºäº†PathChat+ï¼Œè¿™æ˜¯ä¸€æ¬¾ä¸“é—¨ä¸ºäººç±»ç—…ç†å­¦è®¾è®¡çš„æ–°å‹MLLMï¼Œåœ¨è¶…è¿‡100ä¸‡ä¸ªå¤šæ ·åŒ–çš„ç—…ç†å­¦ç‰¹å®šæŒ‡ä»¤æ ·æœ¬å’Œè¿‘550ä¸‡ä¸ªé—®ç­”å›åˆä¸­è¿›è¡Œè®­ç»ƒã€‚åœ¨å¤šç§ç—…ç†å­¦åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒPathChat+æ˜¾è‘—ä¼˜äºä¹‹å‰çš„PathChatåŠ©æ‰‹ï¼Œä»¥åŠæœ€æ–°çš„å…ˆè¿›æŠ€æœ¯å’Œå…¶ä»–ç—…ç†å­¦ç‰¹å®šæ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ¨å‡ºäº†SlideSeekï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨PathChat+çš„å¤šæ™ºèƒ½ä½“AIç³»ç»Ÿï¼Œèƒ½å¤Ÿè‡ªä¸»è¯„ä¼°åƒå…†åƒç´ å…¨åˆ‡ç‰‡å›¾åƒï¼ˆWSIsï¼‰ï¼Œé€šè¿‡è¿­ä»£ã€åˆ†å±‚çš„è¯Šæ–­æ¨ç†ï¼Œåœ¨DDxBenchè¿™ä¸€å…·æœ‰æŒ‘æˆ˜æ€§çš„å¼€æ”¾å¼é‰´åˆ«è¯Šæ–­åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†é«˜å‡†ç¡®æ€§ï¼ŒåŒæ—¶èƒ½å¤Ÿç”Ÿæˆè§†è§‰åŒ–ã€å¯äººç±»è§£è¯»çš„æ€»ç»“æŠ¥å‘Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20964v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>ç—…ç†å­¦æ­£åœ¨ç»å†ç”±å…¨åˆ‡ç‰‡æˆåƒå’Œäººå·¥æ™ºèƒ½é©±åŠ¨çš„æ•°å­—åŒ–è½¬å‹ã€‚å°½ç®¡æ·±åº¦å­¦ä¹ åœ¨è®¡ç®—ç—…ç†å­¦æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†ä¼ ç»Ÿæ¨¡å‹ä¸»è¦å…³æ³¨å›¾åƒåˆ†æï¼Œæ²¡æœ‰æ•´åˆè‡ªç„¶è¯­è¨€æŒ‡ä»¤æˆ–ä¸°å¯Œçš„æ–‡æœ¬èƒŒæ™¯ã€‚å½“å‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è®¡ç®—ç—…ç†å­¦æ–¹é¢é¢ä¸´æ•°æ®ä¸è¶³ã€å¤šå›¾åƒç†è§£æ”¯æŒè¯„ä¼°ä¸è¶³ä»¥åŠç¼ºä¹è‡ªä¸»è¯Šæ–­æ¨ç†èƒ½åŠ›ç­‰é—®é¢˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ä¸“ä¸ºäººç±»ç—…ç†å­¦è®¾è®¡çš„PathChat+ã€‚å®ƒåœ¨è¶…è¿‡100ä¸‡å¤šæ ·åŒ–çš„ç—…ç†å­¦ç‰¹å®šæŒ‡ä»¤æ ·æœ¬å’Œè¿‘550ä¸‡é—®ç­”å›åˆä¸­è¿›è¡Œè®­ç»ƒã€‚åœ¨å¤šç§ç—…ç†å­¦åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒPathChat+æ˜¾è‘—ä¼˜äºä¹‹å‰çš„PathChat copilotä»¥åŠå…¶ä»–æœ€å…ˆè¿›çš„å’Œç—…ç†å­¦ç‰¹å®šçš„æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¨å‡ºäº†SlideSeekï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨PathChat+è¿›è¡Œè‡ªä¸»è¯„ä¼°çš„æ¨ç†å‹å¤šæ™ºèƒ½ä½“AIç³»ç»Ÿï¼Œå¯é€å±‚è¯Šæ–­æ¨ç†ï¼Œåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å¼€æ”¾å¼å·®å¼‚è¯Šæ–­åŸºå‡†æµ‹è¯•DDxBenchä¸Šè¾¾åˆ°é«˜å‡†ç¡®ç‡ï¼ŒåŒæ—¶èƒ½å¤Ÿç”Ÿæˆè§†è§‰åŒ–ã€å¯è§£è¯»çš„æ€»ç»“æŠ¥å‘Šã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ç—…ç†å­¦æ­£åœ¨ç»å†æ•°å­—åŒ–è½¬å‹ï¼Œç”±å…¨åˆ‡ç‰‡æˆåƒå’Œäººå·¥æ™ºèƒ½é©±åŠ¨ã€‚</li>
<li>å½“å‰è®¡ç®—ç—…ç†å­¦é¢†åŸŸçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹é¢ä¸´å¤šæ–¹é¢çš„æŒ‘æˆ˜ã€‚</li>
<li>PathChat+æ˜¯ä¸€ç§é’ˆå¯¹äººç±»ç—…ç†å­¦çš„æ–°çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œç»è¿‡å¤§é‡ç—…ç†å­¦ç‰¹å®šæ•°æ®è®­ç»ƒï¼Œè¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚</li>
<li>PathChat+åœ¨å¤šç§ç—…ç†å­¦åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–æ¨¡å‹ã€‚</li>
<li>SlideSeekæ˜¯ä¸€ä¸ªåˆ©ç”¨PathChat+çš„æ¨ç†å‹å¤šæ™ºèƒ½ä½“AIç³»ç»Ÿï¼Œå¯è‡ªä¸»è¯„ä¼°å…¨åˆ‡ç‰‡å›¾åƒï¼ˆWSIsï¼‰ã€‚</li>
<li>SlideSeeké€šè¿‡é€å±‚è¯Šæ–­æ¨ç†è¾¾åˆ°é«˜å‡†ç¡®ç‡ï¼Œå¹¶åœ¨å·®å¼‚è¯Šæ–­åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜ç§€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20964">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3c9772c4bbffff405f228790a5321e72.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dbb2a9dca720db7f53f8b0c4d040987f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-609fee2aae0bdf34158314af033e5db2.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Test-time-Scaling-Techniques-in-Theoretical-Physics-â€“-A-Comparison-of-Methods-on-the-TPBench-Dataset"><a href="#Test-time-Scaling-Techniques-in-Theoretical-Physics-â€“-A-Comparison-of-Methods-on-the-TPBench-Dataset" class="headerlink" title="Test-time Scaling Techniques in Theoretical Physics â€“ A Comparison of   Methods on the TPBench Dataset"></a>Test-time Scaling Techniques in Theoretical Physics â€“ A Comparison of   Methods on the TPBench Dataset</h2><p><strong>Authors:Zhiqi Gao, Tianyi Li, Yurii Kvasiuk, Sai Chaitanya Tadepalli, Maja Rudolph, Daniel J. H. Chung, Frederic Sala, Moritz MÃ¼nchmeyer</strong></p>
<p>Large language models (LLMs) have shown strong capabilities in complex reasoning, and test-time scaling techniques can enhance their performance with comparably low cost. Many of these methods have been developed and evaluated on mathematical reasoning benchmarks such as AIME. This paper investigates whether the lessons learned from these benchmarks generalize to the domain of advanced theoretical physics. We evaluate a range of common test-time scaling methods on the TPBench physics dataset and compare their effectiveness with results on AIME. To better leverage the structure of physics problems, we develop a novel, symbolic weak-verifier framework to improve parallel scaling results. Our empirical results demonstrate that this method significantly outperforms existing test-time scaling approaches on TPBench. We also evaluate our method on AIME, confirming its effectiveness in solving advanced mathematical problems. Our findings highlight the power of step-wise symbolic verification for tackling complex scientific problems. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚æ¨ç†æ–¹é¢æ˜¾ç¤ºå‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œè€Œä¸”æµ‹è¯•æ—¶æ‰©å±•æŠ€æœ¯å¯ä»¥åœ¨æˆæœ¬ç›¸å¯¹è¾ƒä½çš„æƒ…å†µä¸‹æé«˜å®ƒä»¬çš„æ€§èƒ½ã€‚è®¸å¤šè¿™äº›æ–¹æ³•å·²åœ¨æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ï¼ˆå¦‚AIMEï¼‰ä¸Šå¼€å‘å’Œè¯„ä¼°ã€‚æœ¬æ–‡è°ƒæŸ¥ä»è¿™äº›åŸºå‡†æµ‹è¯•ä¸­å¸å–çš„ç»éªŒæ˜¯å¦å¯æ¨å¹¿åˆ°é«˜çº§ç†è®ºç‰©ç†é¢†åŸŸã€‚æˆ‘ä»¬åœ¨TPBenchç‰©ç†æ•°æ®é›†ä¸Šè¯„ä¼°äº†ä¸€ç³»åˆ—å¸¸è§çš„æµ‹è¯•æ—¶æ‰©å±•æ–¹æ³•ï¼Œå¹¶å°†å®ƒä»¬ä¸AIMEä¸Šçš„ç»“æœè¿›è¡Œäº†æœ‰æ•ˆæ€§æ¯”è¾ƒã€‚ä¸ºäº†æ›´å¥½åœ°åˆ©ç”¨ç‰©ç†é—®é¢˜çš„ç»“æ„ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªæ–°å‹çš„ç¬¦å·å¼±éªŒè¯æ¡†æ¶æ¥æé«˜å¹¶è¡Œæ‰©å±•ç»“æœã€‚æˆ‘ä»¬çš„ç»éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨TPBenchä¸Šçš„æ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰çš„æµ‹è¯•æ—¶æ‰©å±•æ–¹æ³•ã€‚æˆ‘ä»¬ä¹Ÿåœ¨AIMEä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œè¯å®äº†å®ƒåœ¨è§£å†³é«˜çº§æ•°å­¦é—®é¢˜æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåˆ†æ­¥ç¬¦å·éªŒè¯åœ¨è§£å†³å¤æ‚ç§‘å­¦é—®é¢˜æ–¹é¢å…·æœ‰å¼ºå¤§çš„èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20729v1">PDF</a> 23 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚æ¨ç†æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œæµ‹è¯•æ—¶æ‰©å±•æŠ€æœ¯èƒ½å¤Ÿä»¥è¾ƒä½çš„æˆæœ¬æé«˜å…¶æ€§èƒ½ã€‚æœ¬æ–‡ä¸ä»…æ¢ç©¶è¿™äº›æˆæœæ˜¯å¦èƒ½å¤Ÿæ¨å¹¿è‡³é«˜çº§ç†è®ºç‰©ç†é¢†åŸŸï¼Œå¹¶åœ¨TPBenchç‰©ç†æ•°æ®é›†ä¸Šè¯„ä¼°å¤šç§å¸¸ç”¨æµ‹è¯•æ—¶æ‰©å±•æ–¹æ³•çš„æ•ˆæœï¼ŒåŒæ—¶ä¸AIMEä¸Šçš„ç»“æœè¿›è¡Œæ¯”è¾ƒã€‚ä¸ºäº†å……åˆ†åˆ©ç”¨ç‰©ç†é—®é¢˜çš„ç»“æ„ç‰¹ç‚¹ï¼Œæœ¬æ–‡å¼€å‘äº†ä¸€ç§æ–°å‹çš„ç¬¦å·å¼±éªŒè¯æ¡†æ¶ï¼Œä»¥æ”¹å–„å¹¶è¡Œæ‰©å±•ç»“æœã€‚å®è¯ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨TPBenchä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„æµ‹è¯•æ—¶æ‰©å±•æ–¹æ³•ã€‚æ­¤å¤–ï¼Œåœ¨AIMEä¸Šè¿›è¡Œçš„è¯„ä¼°ä¹Ÿè¯å®äº†è¯¥æ–¹æ³•è§£å†³é«˜çº§æ•°å­¦é—®é¢˜æ—¶çš„æœ‰æ•ˆæ€§ã€‚æœ¬æ–‡çš„å‘ç°çªæ˜¾äº†åˆ†æ­¥ç¬¦å·éªŒè¯åœ¨è§£å†³å¤æ‚ç§‘å­¦é—®é¢˜æ–¹é¢çš„å¨åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æ¨ç†æ–¹é¢å±•ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ã€‚</li>
<li>æµ‹è¯•æ—¶æ‰©å±•æŠ€æœ¯å¯ä»¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ï¼Œä¸”æˆæœ¬ç›¸å¯¹è¾ƒä½ã€‚</li>
<li>æœ¬æ–‡ç ”ç©¶äº†å°†æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•çš„ç»éªŒæ¨å¹¿åˆ°é«˜çº§ç†è®ºç‰©ç†é¢†åŸŸçš„æ–¹æ³•ã€‚</li>
<li>åœ¨TPBenchç‰©ç†æ•°æ®é›†ä¸Šè¯„ä¼°äº†å¤šç§æµ‹è¯•æ—¶æ‰©å±•æ–¹æ³•çš„æ•ˆæœã€‚</li>
<li>ä¸ºåˆ©ç”¨ç‰©ç†é—®é¢˜çš„ç»“æ„ç‰¹ç‚¹ï¼Œå¼€å‘äº†ç¬¦å·å¼±éªŒè¯æ¡†æ¶ã€‚</li>
<li>ç¬¦å·å¼±éªŒè¯æ¡†æ¶åœ¨TPBenchä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–æµ‹è¯•æ—¶æ‰©å±•æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20729">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2a49ad20248d67b935f4ec7e645e281f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-39febbf87663dbe4fd205b42c93dee69.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ca596c122e2b612458cad6f043f4dd9f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca5269cc83167c6de0098dafafa30b0a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-84a1984fcce907fbf742a3fcde2ffe2f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-152a36328f7fa67ac8b1cebf8a79a6a2.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="DiffuCoder-Understanding-and-Improving-Masked-Diffusion-Models-for-Code-Generation"><a href="#DiffuCoder-Understanding-and-Improving-Masked-Diffusion-Models-for-Code-Generation" class="headerlink" title="DiffuCoder: Understanding and Improving Masked Diffusion Models for Code   Generation"></a>DiffuCoder: Understanding and Improving Masked Diffusion Models for Code   Generation</h2><p><strong>Authors:Shansan Gong, Ruixiang Zhang, Huangjie Zheng, Jiatao Gu, Navdeep Jaitly, Lingpeng Kong, Yizhe Zhang</strong></p>
<p>Diffusion large language models (dLLMs) are compelling alternatives to autoregressive (AR) models because their denoising models operate over the entire sequence. The global planning and iterative refinement features of dLLMs are particularly useful for code generation. However, current training and inference mechanisms for dLLMs in coding are still under-explored. To demystify the decoding behavior of dLLMs and unlock their potential for coding, we systematically investigate their denoising processes and reinforcement learning (RL) methods. We train a 7B dLLM, \textbf{DiffuCoder}, on 130B tokens of code. Using this model as a testbed, we analyze its decoding behavior, revealing how it differs from that of AR models: (1) dLLMs can decide how causal their generation should be without relying on semi-AR decoding, and (2) increasing the sampling temperature diversifies not only token choices but also their generation order. This diversity creates a rich search space for RL rollouts. For RL training, to reduce the variance of token log-likelihood estimates and maintain training efficiency, we propose \textbf{coupled-GRPO}, a novel sampling scheme that constructs complementary mask noise for completions used in training. In our experiments, coupled-GRPO significantly improves DiffuCoderâ€™s performance on code generation benchmarks (+4.4% on EvalPlus) and reduces reliance on AR bias during decoding. Our work provides deeper insight into the machinery of dLLM generation and offers an effective, diffusion-native RL training framework. <a target="_blank" rel="noopener" href="https://github.com/apple/ml-diffucoder">https://github.com/apple/ml-diffucoder</a>. </p>
<blockquote>
<p>æ‰©æ•£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆdLLMsï¼‰æ˜¯è‡ªåŠ¨å›å½’ï¼ˆARï¼‰æ¨¡å‹çš„å¼•äººæ³¨ç›®çš„æ›¿ä»£å“ï¼Œå› ä¸ºå®ƒä»¬çš„é™å™ªæ¨¡å‹åœ¨æ•´ä¸ªåºåˆ—ä¸Šè¿è¡Œã€‚dLLMsçš„å…¨å±€è§„åˆ’å’Œè¿­ä»£ä¼˜åŒ–åŠŸèƒ½å¯¹äºä»£ç ç”Ÿæˆç‰¹åˆ«æœ‰ç”¨ã€‚ç„¶è€Œï¼Œç›®å‰é’ˆå¯¹ç¼–ç ä¸­çš„dLLMsçš„è®­ç»ƒå’Œæ¨ç†æœºåˆ¶ä»è¢«æ¢ç´¢ä¸è¶³ã€‚ä¸ºäº†æ­ç¤ºdLLMsçš„è§£ç è¡Œä¸ºå¹¶è§£é”å…¶åœ¨ç¼–ç æ–¹é¢çš„æ½œåŠ›ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°ç ”ç©¶äº†å®ƒä»¬çš„é™å™ªè¿‡ç¨‹å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•ã€‚æˆ‘ä»¬åœ¨130Bä»£ç æ ‡è®°ä¸Šè®­ç»ƒäº†ä¸€ä¸ª7Bçš„dLLMï¼Œåä¸ºâ€œDiffuCoderâ€ã€‚ä»¥æ­¤æ¨¡å‹ä¸ºæµ‹è¯•å¹³å°ï¼Œæˆ‘ä»¬åˆ†æäº†å…¶è§£ç è¡Œä¸ºï¼Œæ­ç¤ºäº†å®ƒä¸ARæ¨¡å‹çš„ä¸åŒä¹‹å¤„ï¼šï¼ˆ1ï¼‰dLLMå¯ä»¥åœ¨ä¸ä¾èµ–åŠè‡ªåŠ¨å›å½’è§£ç çš„æƒ…å†µä¸‹å†³å®šå…¶ç”Ÿæˆçš„å› æœæ€§ï¼›ï¼ˆ2ï¼‰å¢åŠ é‡‡æ ·æ¸©åº¦ä¸ä»…ä½¿æ ‡è®°é€‰æ‹©å¤šæ ·åŒ–ï¼Œè€Œä¸”ä½¿å…¶ç”Ÿæˆé¡ºåºä¹Ÿå¤šæ ·åŒ–ã€‚è¿™ç§å¤šæ ·æ€§ä¸ºRLå›åˆæä¾›äº†ä¸°å¯Œçš„æœç´¢ç©ºé—´ã€‚å¯¹äºRLè®­ç»ƒï¼Œä¸ºäº†å‡å°‘æ ‡è®°å¯¹æ•°ä¼¼ç„¶ä¼°è®¡çš„æ–¹å·®å¹¶ä¿æŒè®­ç»ƒæ•ˆç‡ï¼Œæˆ‘ä»¬æå‡ºäº†åä¸ºâ€œè€¦åˆGRPOâ€çš„æ–°å‹é‡‡æ ·æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆä¸ºè®­ç»ƒä¸­ä½¿ç”¨çš„å®Œæˆéƒ¨åˆ†æ„å»ºäº†äº’è¡¥çš„æ©ç å™ªå£°ã€‚åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼Œè€¦åˆGRPOæ˜¾ç€æé«˜äº†DiffuCoderåœ¨ä»£ç ç”ŸæˆåŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½ï¼ˆåœ¨EvalPlusä¸Šæé«˜4.4ï¼…ï¼‰ï¼Œå¹¶å‡å°‘äº†è§£ç è¿‡ç¨‹ä¸­å¯¹äºARåè§çš„ä¾èµ–ã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ºdLLMç”Ÿæˆæœºåˆ¶æä¾›äº†æ›´æ·±å…¥çš„äº†è§£ï¼Œå¹¶æä¾›äº†ä¸€ä¸ªæœ‰æ•ˆçš„ã€é’ˆå¯¹æ‰©æ•£çš„RLè®­ç»ƒæ¡†æ¶ã€‚è¯¦æƒ…å‚è§<a target="_blank" rel="noopener" href="https://github.com/apple/ml-diffucoder%E3%80%82">https://github.com/apple/ml-diffucoderã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20639v2">PDF</a> minor update</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æ‰©æ•£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆdLLMsï¼‰åœ¨ç¼–ç¨‹ä»£ç ç”Ÿæˆæ–¹é¢çš„ä¼˜åŠ¿åŠæ½œåŠ›ã€‚æ–‡ç« ä»‹ç»äº†dLLMsçš„å…¨å±€è§„åˆ’å’Œè¿­ä»£ä¼˜åŒ–ç‰¹æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨ä»£ç ç”Ÿæˆä¸­çš„åº”ç”¨ã€‚é€šè¿‡å¯¹dLLMsçš„é™å™ªè¿‡ç¨‹è¿›è¡Œç³»ç»Ÿæ€§ç ”ç©¶ï¼Œä»¥åŠç»“åˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•ï¼Œä½œè€…è®­ç»ƒäº†ä¸€ä¸ªåä¸ºDiffuCoderçš„7B dLLMæ¨¡å‹ï¼Œå¹¶åœ¨ä»£ç ç”Ÿæˆä»»åŠ¡ä¸Šè¿›è¡Œäº†å®éªŒéªŒè¯ã€‚ç ”ç©¶å‘ç°ï¼ŒdLLMsçš„è§£ç è¡Œä¸ºå…·æœ‰ç‹¬ç‰¹æ€§ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„é‡‡æ ·æ–¹æ¡ˆcoupled-GRPOï¼Œä»¥æé«˜è®­ç»ƒæ•ˆç‡å¹¶å‡å°‘å¯¹æ•°ä¼¼ç„¶ä¼°è®¡çš„æ–¹å·®ã€‚è¯¥æ–¹æ¡ˆåœ¨ä»£ç ç”ŸæˆåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†DiffuCoderçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆdLLMsï¼‰æ˜¯ç”Ÿæˆä»£ç çš„æœ‰æ•ˆæ›¿ä»£æ–¹æ¡ˆï¼Œå…¶å…¨å±€è§„åˆ’å’Œè¿­ä»£ä¼˜åŒ–ç‰¹æ€§åœ¨ä»£ç ç”Ÿæˆä¸­ç‰¹åˆ«æœ‰ç”¨ã€‚</li>
<li>dLLMsçš„è§£ç è¡Œä¸ºä¸åŒäºä¼ ç»Ÿçš„è‡ªå›å½’ï¼ˆARï¼‰æ¨¡å‹ï¼Œå¯ä»¥åœ¨ä¸ä¾èµ–åŠè‡ªå›å½’è§£ç çš„æƒ…å†µä¸‹å†³å®šç”Ÿæˆçš„å› æœæ€§ã€‚</li>
<li>å¢åŠ é‡‡æ ·æ¸©åº¦ä¸ä»…ä½¿ä»¤ç‰Œé€‰æ‹©å¤šæ ·åŒ–ï¼Œè¿˜æ”¹å˜äº†ä»¤ç‰Œçš„ç”Ÿæˆé¡ºåºï¼Œä¸ºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„rolloutsæä¾›äº†ä¸°å¯Œçš„æœç´¢ç©ºé—´ã€‚</li>
<li>é’ˆå¯¹RLè®­ç»ƒï¼Œä¸ºäº†å‡å°‘ä»¤ç‰Œå¯¹æ•°ä¼¼ç„¶ä¼°è®¡çš„æ–¹å·®å¹¶ä¿æŒè®­ç»ƒæ•ˆç‡ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„é‡‡æ ·æ–¹æ¡ˆâ€”â€”coupled-GRPOã€‚</li>
<li>coupled-GRPOé€šè¿‡æ„å»ºç”¨äºè®­ç»ƒçš„äº’è¡¥æ©ç å™ªå£°ï¼Œæ˜¾è‘—æé«˜äº†DiffuCoderåœ¨ä»£ç ç”ŸæˆåŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒDiffuCoderåœ¨EvalPlusä¸Šçš„æ€§èƒ½æå‡äº†4.4%ï¼Œå¹¶åœ¨è§£ç è¿‡ç¨‹ä¸­å‡å°‘äº†ARåå€šçš„ä¾èµ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20639">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b661d512447bdbe4fad5845e74cf08cd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1fab6f03b8677a7805ff29cfa9802755.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-894b6e6328ece7c23bc3d00fd9739b61.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cc7edce831c554d0a6d32308f871a1e8.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Multi-Preference-Lambda-weighted-Listwise-DPO-for-Dynamic-Preference-Alignment"><a href="#Multi-Preference-Lambda-weighted-Listwise-DPO-for-Dynamic-Preference-Alignment" class="headerlink" title="Multi-Preference Lambda-weighted Listwise DPO for Dynamic Preference   Alignment"></a>Multi-Preference Lambda-weighted Listwise DPO for Dynamic Preference   Alignment</h2><p><strong>Authors:Yuhui Sun, Xiyao Wang, Zixi Li, Jinman Zhao</strong></p>
<p>While large-scale unsupervised language models (LMs) capture broad world knowledge and reasoning capabilities, steering their behavior toward desired objectives remains challenging due to the lack of explicit supervision. Existing alignment techniques, such as reinforcement learning from human feedback (RLHF), rely on training a reward model and performing reinforcement learning to align with human preferences. However, RLHF is often computationally intensive, unstable, and sensitive to hyperparameters.   To address these limitations, Direct Preference Optimization (DPO) was introduced as a lightweight and stable alternative, enabling direct alignment of language models with pairwise preference data via classification loss. However, DPO and its extensions generally assume a single static preference distribution, limiting flexibility in multi-objective or dynamic alignment settings.   In this paper, we propose a novel framework: Multi-Preference Lambda-weighted Listwise DPO, which extends DPO to incorporate multiple human preference dimensions (e.g., helpfulness, harmlessness, informativeness) and enables dynamic interpolation through a controllable simplex-weighted formulation. Our method supports both listwise preference feedback and flexible alignment across varying user intents without re-training. Empirical and theoretical analysis demonstrates that our method is as effective as traditional DPO on static objectives while offering greater generality and adaptability for real-world deployment. </p>
<blockquote>
<p>å¤§è§„æ¨¡æ— ç›‘ç£è¯­è¨€æ¨¡å‹ï¼ˆLMsï¼‰è™½ç„¶æ•æ‰åˆ°äº†å¹¿æ³›çš„ä¸–ç•ŒçŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ï¼Œä½†ç”±äºç¼ºä¹æ˜ç¡®çš„ç›‘ç£ï¼Œå°†å…¶è¡Œä¸ºå¯¼å‘æ—¢å®šç›®æ ‡ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç°æœ‰çš„å¯¹é½æŠ€æœ¯ï¼Œå¦‚å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰ï¼Œä¾èµ–äºè®­ç»ƒå¥–åŠ±æ¨¡å‹å’Œè¿›è¡Œå¼ºåŒ–å­¦ä¹ ä»¥ç¬¦åˆäººç±»åå¥½ã€‚ç„¶è€Œï¼ŒRLHFé€šå¸¸è®¡ç®—é‡å¤§ã€ä¸ç¨³å®šï¼Œå¯¹è¶…å‚æ•°æ•æ„Ÿã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œå¼•å…¥äº†ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ä½œä¸ºè½»é‡çº§å’Œç¨³å®šçš„æ›¿ä»£æ–¹æ¡ˆï¼Œé€šè¿‡åˆ†ç±»æŸå¤±ç›´æ¥å¯¹é½è¯­è¨€æ¨¡å‹ä¸é…å¯¹åå¥½æ•°æ®ã€‚ç„¶è€Œï¼ŒDPOåŠå…¶æ‰©å±•é€šå¸¸å‡è®¾å•ä¸€é™æ€åå¥½åˆ†å¸ƒï¼Œè¿™åœ¨å¤šç›®æ ‡æˆ–åŠ¨æ€å¯¹é½è®¾ç½®ä¸­é™åˆ¶äº†çµæ´»æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼šå¤šåå¥½LambdaåŠ æƒåˆ—è¡¨å¼DPOï¼Œå®ƒæ‰©å±•äº†DPOä»¥çº³å…¥å¤šä¸ªäººç±»åå¥½ç»´åº¦ï¼ˆä¾‹å¦‚ï¼Œæœ‰ç”¨æ€§ã€æ— å®³æ€§ã€ä¿¡æ¯é‡ï¼‰å¹¶é€šè¿‡å¯æ§çš„å•çº¯åŠ æƒå…¬å¼å®ç°åŠ¨æ€æ’å€¼ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ”¯æŒåˆ—è¡¨å¼åå¥½åé¦ˆå’Œæ— éœ€é‡æ–°è®­ç»ƒå³å¯é€‚åº”ä¸åŒç”¨æˆ·æ„å›¾çš„çµæ´»å¯¹é½ã€‚å®è¯å’Œç†è®ºåˆ†æè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨é™æ€ç›®æ ‡ä¸Šä¸ä¼ ç»ŸDPOåŒæ ·æœ‰æ•ˆï¼ŒåŒæ—¶æä¾›äº†æ›´å¤§çš„é€šç”¨æ€§å’Œé€‚åº”æ€§ï¼Œé€‚ç”¨äºç°å®ä¸–ç•Œéƒ¨ç½²ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.19780v2">PDF</a> 10 pages, 4 figures, appendix included. To appear in Proceedings of   AAAI 2026. Code:   <a target="_blank" rel="noopener" href="https://github.com/yuhui15/Multi-Preference-Lambda-weighted-DPO">https://github.com/yuhui15/Multi-Preference-Lambda-weighted-DPO</a></p>
<p><strong>Summary</strong><br>å¤§è§„æ¨¡æ— ç›‘ç£è¯­è¨€æ¨¡å‹æ•è·äº†ä¸°å¯Œçš„ä¸–ç•ŒçŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ï¼Œä½†å°†å…¶è¡Œä¸ºå¯¼å‘ç‰¹å®šç›®æ ‡ä»å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç°æœ‰å¯¹é½æŠ€æœ¯å¦‚å¼ºåŒ–å­¦ä¹ ä¾èµ–äºè®­ç»ƒå¥–åŠ±æ¨¡å‹å¹¶è¿›è¡Œå¼ºåŒ–å­¦ä¹ ä»¥ç¬¦åˆäººç±»åå¥½ï¼Œä½†è®¡ç®—é‡å¤§ä¸”ä¸ç¨³å®šã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæå‡ºäº†ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ã€‚æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹æ¡†æ¶ï¼šå¤šåå¥½Î»åŠ æƒåˆ—è¡¨å¼DPOï¼Œå®ƒå°†DPOæ‰©å±•åˆ°å¤šä¸ªåå¥½ç»´åº¦å¹¶å®ç°åŠ¨æ€æ’å€¼ã€‚ç»éªŒåˆ†æå’Œç†è®ºè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é™æ€ç›®æ ‡ä¸Šçš„æ•ˆæœä¸ä¼ ç»ŸDPOç›¸å½“ï¼ŒåŒæ—¶åœ¨å®é™…éƒ¨ç½²ä¸­æ›´å…·é€šç”¨æ€§å’Œé€‚åº”æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§è§„æ¨¡æ— ç›‘ç£è¯­è¨€æ¨¡å‹åœ¨æ•è·ä¸–ç•ŒçŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å®ç°ç‰¹å®šç›®æ ‡å¯¹é½ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>ç°æœ‰å¯¹é½æŠ€æœ¯å¦‚å¼ºåŒ–å­¦ä¹ å­˜åœ¨è®¡ç®—é‡å¤§ã€ä¸ç¨³å®šç­‰é—®é¢˜ã€‚</li>
<li>ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ä½œä¸ºä¸€ç§è½»é‡çº§ã€ç¨³å®šçš„æ›¿ä»£æ–¹æ³•è¢«å¼•å…¥ï¼Œé€šè¿‡å¯¹è¯­è¨€æ¨¡å‹è¿›è¡Œç›´æ¥å¯¹é½é…å¯¹åå¥½æ•°æ®ã€‚</li>
<li>æ–°å‹æ¡†æ¶â€”â€”å¤šåå¥½Î»åŠ æƒåˆ—è¡¨å¼DPOæ‰©å±•äº†DPOï¼Œèƒ½å¤„ç†å¤šä¸ªåå¥½ç»´åº¦å¹¶å®ç°åŠ¨æ€æ’å€¼ã€‚</li>
<li>è¯¥æ–¹æ³•æ”¯æŒåˆ—è¡¨å¼åå¥½åé¦ˆï¼Œæ— éœ€é‡æ–°è®­ç»ƒå³å¯é€‚åº”ä¸åŒç”¨æˆ·æ„å›¾çš„å˜åŒ–ã€‚</li>
<li>å®è¯å’Œç†è®ºåˆ†æè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é™æ€ç›®æ ‡ä¸Šçš„æ•ˆæœä¸ä¼ ç»ŸDPOç›¸å½“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.19780">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bb8689920ec2a45714bc15e25568911e.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="From-Web-Search-towards-Agentic-Deep-Research-Incentivizing-Search-with-Reasoning-Agents"><a href="#From-Web-Search-towards-Agentic-Deep-Research-Incentivizing-Search-with-Reasoning-Agents" class="headerlink" title="From Web Search towards Agentic Deep Research: Incentivizing Search with   Reasoning Agents"></a>From Web Search towards Agentic Deep Research: Incentivizing Search with   Reasoning Agents</h2><p><strong>Authors:Weizhi Zhang, Yangning Li, Yuanchen Bei, Junyu Luo, Guancheng Wan, Liangwei Yang, Chenxuan Xie, Yuyao Yang, Wei-Chieh Huang, Chunyu Miao, Henry Peng Zou, Xiao Luo, Yusheng Zhao, Yankai Chen, Chunkit Chan, Peilin Zhou, Xinyang Zhang, Chenwei Zhang, Jingbo Shang, Ming Zhang, Yangqiu Song, Irwin King, Philip S. Yu</strong></p>
<p>Information retrieval is a cornerstone of modern knowledge acquisition, enabling billions of queries each day across diverse domains. However, traditional keyword-based search engines are increasingly inadequate for handling complex, multi-step information needs. Our position is that Large Language Models (LLMs), endowed with reasoning and agentic capabilities, are ushering in a new paradigm termed Agentic Deep Research. These systems transcend conventional information search techniques by tightly integrating autonomous reasoning, iterative retrieval, and information synthesis into a dynamic feedback loop. We trace the evolution from static web search to interactive, agent-based systems that plan, explore, and learn. We also introduce a test-time scaling law to formalize the impact of computational depth on reasoning and search. Supported by benchmark results and the rise of open-source implementations, we demonstrate that Agentic Deep Research not only significantly outperforms existing approaches, but is also poised to become the dominant paradigm for future information seeking. All the related resources, including industry products, research papers, benchmark datasets, and open-source implementations, are collected for the community in <a target="_blank" rel="noopener" href="https://github.com/DavidZWZ/Awesome-Deep-Research">https://github.com/DavidZWZ/Awesome-Deep-Research</a>. </p>
<blockquote>
<p>ä¿¡æ¯æ£€ç´¢æ˜¯ç°ä»£çŸ¥è¯†è·å–çš„æ ¸å¿ƒåŸºçŸ³ï¼Œæ¯å¤©èƒ½åœ¨ä¸åŒé¢†åŸŸå¤„ç†æ•°åäº¿çš„æŸ¥è¯¢ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„åŸºäºå…³é”®è¯çš„æœç´¢å¼•æ“è¶Šæ¥è¶Šä¸èƒ½æ»¡è¶³å¤æ‚çš„ã€å¤šæ­¥éª¤çš„ä¿¡æ¯éœ€æ±‚ã€‚æˆ‘ä»¬çš„è§‚ç‚¹æ˜¯ï¼Œæ‹¥æœ‰æ¨ç†å’Œæ™ºèƒ½èƒ½åŠ›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ­£åœ¨å¼€åˆ›ä¸€ç§åä¸ºæ™ºèƒ½æ·±åº¦ç ”ç©¶çš„æ–°èŒƒå¼ã€‚è¿™äº›ç³»ç»Ÿé€šè¿‡ç´§å¯†é›†æˆè‡ªä¸»æ¨ç†ã€è¿­ä»£æ£€ç´¢å’Œä¿¡æ¯åˆæˆåˆ°ä¸€ä¸ªåŠ¨æ€åé¦ˆå¾ªç¯ä¸­ï¼Œä»è€Œè¶…è¶Šäº†ä¼ ç»Ÿçš„ä¿¡æ¯æœç´¢æŠ€æœ¯ã€‚æˆ‘ä»¬è¿½è¸ªä»é™æ€ç½‘é¡µæœç´¢åˆ°åŸºäºäº¤äº’ã€æ™ºèƒ½ç³»ç»Ÿçš„æ¼”å˜ï¼Œè¿™äº›ç³»ç»Ÿå¯ä»¥è®¡åˆ’ã€æ¢ç´¢å’Œè‡ªä¸»å­¦ä¹ ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªæµ‹è¯•æ—¶ç¼©æ”¾å®šå¾‹ï¼Œä»¥æ­£å¼è®¡ç®—æ·±åº¦å¯¹æ¨ç†å’Œæœç´¢çš„å½±å“ã€‚åœ¨åŸºå‡†æµ‹è¯•ç»“æœå’Œå¼€æºå®ç°å…´èµ·çš„æ”¯æŒä¸‹ï¼Œæˆ‘ä»¬è¯æ˜äº†æ™ºèƒ½æ·±åº¦ç ”ç©¶ä¸ä»…æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè€Œä¸”è¿˜å°†æˆä¸ºæœªæ¥ä¿¡æ¯æœç´¢çš„ä¸»å¯¼èŒƒå¼ã€‚æ‰€æœ‰ç›¸å…³èµ„æºï¼ŒåŒ…æ‹¬å·¥ä¸šäº§å“ã€ç ”ç©¶è®ºæ–‡ã€åŸºå‡†æ•°æ®é›†å’Œå¼€æºå®ç°ï¼Œéƒ½ä¸ºç¤¾åŒºæ”¶é›†åœ¨<a target="_blank" rel="noopener" href="https://github.com/DavidZWZ/Awesome-Deep-Research">https://github.com/DavidZWZ/Awesome-Deep-Research</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18959v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·å¤‡æ¨ç†å’Œæ™ºèƒ½ç‰¹æ€§ï¼Œæ­£åœ¨å¼•é¢†ä¸€ç§æ–°çš„ä¿¡æ¯æ£€ç´¢æ¨¡å¼â€”â€”æ™ºèƒ½æ·±åº¦ç ”ç©¶ï¼Œå¹¶è¶…è¶Šäº†ä¼ ç»Ÿä¿¡æ¯æ£€ç´¢æŠ€æœ¯ã€‚è¯¥æ¨¡å¼æ•´åˆäº†è‡ªä¸»æ¨ç†ã€è¿­ä»£æ£€ç´¢å’Œä¿¡æ¯åˆæˆè¿›å…¥ä¸€ä¸ªåŠ¨æ€åé¦ˆå¾ªç¯ã€‚ä»é™æ€ç½‘é¡µæœç´¢å‘å±•åˆ°äº¤äº’å¼çš„ã€åŸºäºä»£ç†çš„ç³»ç»Ÿï¼Œæ™ºèƒ½æ·±åº¦ç ”ç©¶èƒ½å¤Ÿè¿›è¡Œè§„åˆ’ã€æ¢ç´¢å’Œè‡ªä¸»å­¦ä¹ ã€‚åŒæ—¶ï¼Œæå‡ºæµ‹è¯•æ—¶å°ºåº¦å®šå¾‹æ¥æ­£å¼è®¡ç®—æ·±åº¦å¯¹æ¨ç†å’Œæœç´¢çš„å½±å“ã€‚æ™ºèƒ½æ·±åº¦ç ”ç©¶ä¸ä»…æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè€Œä¸”å°†æˆä¸ºæœªæ¥ä¿¡æ¯æœç´¢çš„ä¸»å¯¼æ¨¡å¼ã€‚ç›¸å…³èµ„æºå‡å·²åœ¨GitHubä¸Šå…±äº«ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹å¼•é¢†æ–°çš„ä¿¡æ¯æ£€ç´¢æ¨¡å¼â€”â€”æ™ºèƒ½æ·±åº¦ç ”ç©¶ã€‚</li>
<li>æ™ºèƒ½æ·±åº¦ç ”ç©¶æ•´åˆè‡ªä¸»æ¨ç†ã€è¿­ä»£æ£€ç´¢å’Œä¿¡æ¯åˆæˆè¿›å…¥åŠ¨æ€åé¦ˆå¾ªç¯ã€‚</li>
<li>æ™ºèƒ½æ·±åº¦ç ”ç©¶ä»é™æ€ç½‘é¡µæœç´¢å‘å±•åˆ°äº¤äº’å¼çš„ã€åŸºäºä»£ç†çš„ç³»ç»Ÿã€‚</li>
<li>æ™ºèƒ½æ·±åº¦ç ”ç©¶å…·å¤‡è§„åˆ’ã€æ¢ç´¢å’Œè‡ªä¸»å­¦ä¹ çš„èƒ½åŠ›ã€‚</li>
<li>æµ‹è¯•æ—¶å°ºåº¦å®šå¾‹æ­£å¼è®¡ç®—æ·±åº¦å¯¹æ¨ç†å’Œæœç´¢çš„å½±å“ã€‚</li>
<li>æ™ºèƒ½æ·±åº¦ç ”ç©¶æ˜¾è‘—ä¼˜äºä¼ ç»Ÿä¿¡æ¯æ£€ç´¢æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18959">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-606272d05a87c0533b51ed6302057f9f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-76a584b347a5f8a48347111db296ccc0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-155280869ad2bd126b54c3da5ebc7cd6.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Taming-the-Untamed-Graph-Based-Knowledge-Retrieval-and-Reasoning-for-MLLMs-to-Conquer-the-Unknown"><a href="#Taming-the-Untamed-Graph-Based-Knowledge-Retrieval-and-Reasoning-for-MLLMs-to-Conquer-the-Unknown" class="headerlink" title="Taming the Untamed: Graph-Based Knowledge Retrieval and Reasoning for   MLLMs to Conquer the Unknown"></a>Taming the Untamed: Graph-Based Knowledge Retrieval and Reasoning for   MLLMs to Conquer the Unknown</h2><p><strong>Authors:Bowen Wang, Zhouqiang Jiang, Yasuaki Susumu, Shotaro Miwa, Tianwei Chen, Yuta Nakashima</strong></p>
<p>The real value of knowledge lies not just in its accumulation, but in its potential to be harnessed effectively to conquer the unknown. Although recent multimodal large language models (MLLMs) exhibit impressing multimodal capabilities, they often fail in rarely encountered domain-specific tasks due to limited relevant knowledge. To explore this, we adopt visual game cognition as a testbed and select Monster Hunter: World as the target to construct a multimodal knowledge graph (MH-MMKG), which incorporates multi-modalities and intricate entity relations. We also design a series of challenging queries based on MH-MMKG to evaluate the modelsâ€™ ability for complex knowledge retrieval and reasoning. Furthermore, we propose a multi-agent retriever that enables a model to autonomously search relevant knowledge without additional training. Experimental results show that our approach significantly enhances the performance of MLLMs, providing a new perspective on multimodal knowledge-augmented reasoning and laying a solid foundation for future research. </p>
<blockquote>
<p>çŸ¥è¯†çš„çœŸæ­£ä»·å€¼ä¸ä»…åœ¨äºå…¶ç§¯ç´¯ï¼Œæ›´åœ¨äºå…¶æœ‰æ•ˆå¾æœæœªçŸ¥é¢†åŸŸçš„æ½œåŠ›ã€‚å°½ç®¡æœ€è¿‘çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„å¤šæ¨¡æ€èƒ½åŠ›ï¼Œä½†ç”±äºç›¸å…³çŸ¥è¯†æœ‰é™ï¼Œå®ƒä»¬åœ¨ç½•è§é¢†åŸŸç‰¹å®šä»»åŠ¡ä¸­å¾€å¾€è¡¨ç°ä¸ä½³ã€‚ä¸ºäº†æ¢ç©¶è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬ä»¥è§†è§‰æ¸¸æˆè®¤çŸ¥ä½œä¸ºæµ‹è¯•å¹³å°ï¼Œé€‰æ‹©ã€Šæ€ªç‰©çŒäººï¼šä¸–ç•Œã€‹ä½œä¸ºç›®æ ‡æ„å»ºå¤šæ¨¡æ€çŸ¥è¯†å›¾è°±ï¼ˆMH-MMKGï¼‰ï¼Œè¯¥å›¾è°±ç»“åˆäº†å¤šç§æ¨¡æ€å’Œå¤æ‚çš„å®ä½“å…³ç³»ã€‚æˆ‘ä»¬è¿˜åŸºäºMH-MMKGè®¾è®¡äº†ä¸€ç³»åˆ—å…·æœ‰æŒ‘æˆ˜æ€§çš„æŸ¥è¯¢ï¼Œä»¥è¯„ä¼°æ¨¡å‹è¿›è¡Œå¤æ‚çŸ¥è¯†æ£€ç´¢å’Œæ¨ç†çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šæ™ºèƒ½ä½“æ£€ç´¢å™¨ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ— éœ€é¢å¤–è®­ç»ƒå³å¯è‡ªä¸»æœç´¢ç›¸å…³çŸ¥è¯†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—æé«˜äº†MLLMsçš„æ€§èƒ½ï¼Œä¸ºå¤šæ¨¡æ€çŸ¥è¯†å¢å¼ºæ¨ç†æä¾›äº†æ–°çš„è§†è§’ï¼Œå¹¶ä¸ºæœªæ¥ç ”ç©¶å¥ å®šäº†åšå®åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17589v2">PDF</a> Accepted by ICCV 2025</p>
<p><strong>Summary</strong><br>æ–‡ç« å¼ºè°ƒçŸ¥è¯†çš„çœŸæ­£ä»·å€¼åœ¨äºæœ‰æ•ˆåœ°åˆ©ç”¨çŸ¥è¯†æ¥å¾æœæœªçŸ¥é¢†åŸŸï¼Œè€Œéå•çº¯ç§¯ç´¯çŸ¥è¯†ã€‚æ–‡ç« é€šè¿‡æ„å»ºå¤šæ¨¡æ€çŸ¥è¯†å›¾è°±ï¼ˆMH-MMKGï¼‰æ¢ç´¢äº†å¤§å‹å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ç‰¹å®šé¢†åŸŸä»»åŠ¡ä¸­çš„å±€é™æ€§ï¼Œå¹¶ä»¥ç”µå­æ¸¸æˆã€Šæ€ªç‰©çŒäººï¼šä¸–ç•Œã€‹ä¸ºå®ä¾‹å±•å¼€ç ”ç©¶ã€‚è®¾è®¡ä¸€ç³»åˆ—æŒ‘æˆ˜æŸ¥è¯¢ä»»åŠ¡ï¼Œå¯¹æ¨¡å‹çš„å¤æ‚çŸ¥è¯†æ£€ç´¢å’Œæ¨ç†èƒ½åŠ›è¿›è¡Œè¯„ä¼°ã€‚æå‡ºå¤šä»£ç†æ£€ç´¢å™¨æ–¹æ³•ä½¿æ¨¡å‹èƒ½è‡ªä¸»æ£€ç´¢ç›¸å…³çŸ¥è¯†ï¼Œæ— éœ€é¢å¤–è®­ç»ƒã€‚å®éªŒç»“æœè¯æ˜äº†è¯¥ç ”ç©¶çš„æ–°è§†è§’åŠæ–°æ–¹æ³•å¯¹æé«˜MLLMsæ€§èƒ½çš„æ˜¾è‘—ä½œç”¨ï¼Œä¸ºæœªæ¥çš„å¤šæ¨¡æ€çŸ¥è¯†å¢å¼ºæ¨ç†ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>çŸ¥è¯†ä»·å€¼ä¸ä»…åœ¨äºç§¯ç´¯ï¼Œæ›´åœ¨äºæœ‰æ•ˆè¿ç”¨æ¥è§£å†³é—®é¢˜ã€‚</li>
<li>å¤§å‹å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹åœ¨å¤šæ¨¡æ€é¢†åŸŸæœ‰æ½œåŠ›ï¼Œä½†åœ¨ç‰¹å®šé¢†åŸŸä»»åŠ¡ä¸­å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>æ„å»ºå¤šæ¨¡æ€çŸ¥è¯†å›¾è°±ï¼ˆMH-MMKGï¼‰ä½œä¸ºè§£å†³æ­¤é—®é¢˜çš„é€”å¾„ä¹‹ä¸€ã€‚</li>
<li>ä»¥ç”µå­æ¸¸æˆã€Šæ€ªç‰©çŒäººï¼šä¸–ç•Œã€‹ä¸ºä¾‹ï¼Œè®¾è®¡æŒ‘æˆ˜æŸ¥è¯¢ä»»åŠ¡è¯„ä¼°æ¨¡å‹èƒ½åŠ›ã€‚</li>
<li>æå‡ºå¤šä»£ç†æ£€ç´¢å™¨æ–¹æ³•ï¼Œä½¿æ¨¡å‹èƒ½è‡ªä¸»æ£€ç´¢ç›¸å…³çŸ¥è¯†ã€‚</li>
<li>å®éªŒç»“æœè¯æ˜äº†æ–°æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17589">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-26bf9ec2bc6ca420557d20077edebc5b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d41186a12fc0edec40528c17a5f53f97.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e355e074ddb74a18a8eaccb7e6fbfe3d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-457c69a5cc524ac49ae9f4f2e3ed9067.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bf5c7bee349aaeefd1422f4315d93a6c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f495804739f2dc2776299cf8956ba981.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Exploring-Big-Five-Personality-and-AI-Capability-Effects-in-LLM-Simulated-Negotiation-Dialogues"><a href="#Exploring-Big-Five-Personality-and-AI-Capability-Effects-in-LLM-Simulated-Negotiation-Dialogues" class="headerlink" title="Exploring Big Five Personality and AI Capability Effects in   LLM-Simulated Negotiation Dialogues"></a>Exploring Big Five Personality and AI Capability Effects in   LLM-Simulated Negotiation Dialogues</h2><p><strong>Authors:Myke C. Cohen, Zhe Su, Hsien-Te Kao, Daniel Nguyen, Spencer Lynch, Maarten Sap, Svitlana Volkova</strong></p>
<p>This paper presents an evaluation framework for agentic AI systems in mission-critical negotiation contexts, addressing the need for AI agents that can adapt to diverse human operators and stakeholders. Using Sotopia as a simulation testbed, we present two experiments that systematically evaluated how personality traits and AI agent characteristics influence LLM-simulated social negotiation outcomesâ€“a capability essential for a variety of applications involving cross-team coordination and civil-military interactions. Experiment 1 employs causal discovery methods to measure how personality traits impact price bargaining negotiations, through which we found that Agreeableness and Extraversion significantly affect believability, goal achievement, and knowledge acquisition outcomes. Sociocognitive lexical measures extracted from team communications detected fine-grained differences in agentsâ€™ empathic communication, moral foundations, and opinion patterns, providing actionable insights for agentic AI systems that must operate reliably in high-stakes operational scenarios. Experiment 2 evaluates human-AI job negotiations by manipulating both simulated human personality and AI system characteristics, specifically transparency, competence, adaptability, demonstrating how AI agent trustworthiness impact mission effectiveness. These findings establish a repeatable evaluation methodology for experimenting with AI agent reliability across diverse operator personalities and human-agent team dynamics, directly supporting operational requirements for reliable AI systems. Our work advances the evaluation of agentic AI workflows by moving beyond standard performance metrics to incorporate social dynamics essential for mission success in complex operations. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç”¨äºè¯„ä¼°å…³é”®ä»»åŠ¡è°ˆåˆ¤æƒ…å¢ƒä¸­ä»£ç†æ™ºèƒ½ç³»ç»Ÿï¼ˆAIç³»ç»Ÿï¼‰çš„è¯„ä»·æ¡†æ¶ï¼Œè§£å†³äº†éœ€è¦é€‚åº”ä¸åŒäººç±»æ“ä½œè€…å’Œåˆ©ç›Šç›¸å…³è€…çš„AIä»£ç†çš„éœ€æ±‚ã€‚ä»¥Sotopiaä½œä¸ºä»¿çœŸæµ‹è¯•å¹³å°ï¼Œæˆ‘ä»¬è¿›è¡Œäº†ä¸¤é¡¹å®éªŒï¼Œç³»ç»Ÿåœ°è¯„ä¼°äº†äººæ ¼ç‰¹è´¨å’ŒAIä»£ç†ç‰¹å¾å¦‚ä½•å½±å“ç”±å¤§å‹è¯­è¨€æ¨¡å‹æ¨¡æ‹Ÿçš„ç¤¾ä¼šè°ˆåˆ¤ç»“æœâ€”â€”è¿™ç§èƒ½åŠ›å¯¹äºæ¶‰åŠè·¨å›¢é˜Ÿåä½œå’Œå†›æ°‘äº’åŠ¨çš„å„ç§åº”ç”¨ç¨‹åºè‡³å…³é‡è¦ã€‚å®éªŒä¸€é‡‡ç”¨å› æœå‘ç°æ–¹æ³•æ¥è¡¡é‡äººæ ¼ç‰¹è´¨å¯¹è®®ä»·è°ˆåˆ¤çš„å½±å“ï¼Œæˆ‘ä»¬å‘ç°å®œäººæ€§åŠå¤–å‘æ€§æ˜¾è‘—å½±å“å¯ä¿¡åº¦ã€ç›®æ ‡å®ç°å’ŒçŸ¥è¯†è·å–ç»“æœã€‚ä»å›¢é˜Ÿé€šä¿¡ä¸­æå–çš„ç¤¾ä¼šè®¤çŸ¥è¯æ±‡è¡¡é‡æ ‡å‡†å¯ä»¥æ£€æµ‹åˆ°ä»£ç†çš„å…±æƒ…æ²Ÿé€šã€é“å¾·åŸºç¡€å’Œè§‚ç‚¹æ¨¡å¼çš„ç»†å¾®å·®å¼‚ï¼Œä¸ºå¿…é¡»åœ¨é«˜é£é™©æ“ä½œåœºæ™¯ä¸­å¯é è¿è¡Œçš„ä»£ç†æ™ºèƒ½ç³»ç»Ÿæä¾›äº†å¯æ“ä½œçš„è§è§£ã€‚å®éªŒäºŒé€šè¿‡æ“çºµæ¨¡æ‹Ÿäººç±»ä¸ªæ€§å’ŒAIç³»ç»Ÿç‰¹æ€§ï¼ˆç‰¹åˆ«æ˜¯é€æ˜åº¦ã€èƒ½åŠ›å’Œé€‚åº”æ€§ï¼‰æ¥è¯„ä¼°äººæœºå·¥ä½œè°ˆåˆ¤ï¼Œå±•ç¤ºäº†AIä»£ç†çš„å¯ä¿¡æ€§å¦‚ä½•å½±å“ä»»åŠ¡çš„æœ‰æ•ˆæ€§ã€‚è¿™äº›å‘ç°å»ºç«‹äº†ä¸€ç§å¯é‡å¤çš„è¯„ä»·æ–¹æ³•ï¼Œç”¨äºåœ¨ä¸åŒæ“ä½œè€…ä¸ªæ€§å’Œäººæœºå›¢é˜ŸåŠ¨æ€ä¸­æµ‹è¯•AIä»£ç†çš„å¯é æ€§ï¼Œç›´æ¥æ”¯æŒå¯¹å¯é AIç³»ç»Ÿçš„æ“ä½œè¦æ±‚ã€‚æˆ‘ä»¬çš„å·¥ä½œé€šè¿‡è¶…è¶Šæ ‡å‡†æ€§èƒ½æŒ‡æ ‡æ¥è¯„ä¼°ä»£ç†æ™ºèƒ½çš„å·¥ä½œæµç¨‹ï¼Œçº³å…¥äº†å¯¹å¤æ‚æ“ä½œä»»åŠ¡æˆåŠŸè‡³å…³é‡è¦çš„ç¤¾ä¼šåŠ¨æ€å› ç´ ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15928v2">PDF</a> Under review for KDD 2025 Workshop on Evaluation and Trustworthiness   of Agentic and Generative AI Models</p>
<p><strong>Summary</strong><br>åœ¨æ¨¡æ‹Ÿæµ‹è¯•åºŠSotopiaä¸­ï¼Œè¯¥è®ºæ–‡å¯¹ä»£ç†å‹äººå·¥æ™ºèƒ½ç³»ç»Ÿåœ¨å…³é”®è°ˆåˆ¤ä»»åŠ¡ä¸­çš„è¯„ä¼°æ¡†æ¶è¿›è¡Œäº†å±•ç¤ºã€‚é€šè¿‡ä¸¤ä¸ªå®éªŒç³»ç»Ÿåœ°æ¢è®¨äº†äººæ ¼ç‰¹è´¨å’Œäººå·¥æ™ºèƒ½ä»£ç†çš„ç‰¹æ€§å¦‚ä½•å½±å“å¤§å‹è¯­è¨€æ¨¡å‹æ¨¡æ‹Ÿçš„ç¤¾ä¼šè°ˆåˆ¤ç»“æœã€‚å®éªŒä¸€é€šè¿‡å› æœå‘ç°æ–¹æ³•æµ‹é‡äººæ ¼ç‰¹è´¨å¯¹è®®ä»·è°ˆåˆ¤çš„å½±å“ï¼Œå‘ç°å®œäººæ€§å’Œå¤–å‘æ€§ä¼šå½±å“å¯ä¿¡åº¦ã€ç›®æ ‡å®ç°å’ŒçŸ¥è¯†è·å–çš„ç»“æœã€‚å®éªŒäºŒè¯„ä¼°äº†äººç±»ä¸äººå·¥æ™ºèƒ½çš„å·¥ä½œè°ˆåˆ¤ï¼Œé€šè¿‡æ“çºµæ¨¡æ‹Ÿäººç±»ä¸ªæ€§å’Œäººå·¥æ™ºèƒ½ç³»ç»Ÿç‰¹æ€§ï¼Œå¦‚é€æ˜åº¦ã€èƒ½åŠ›å’Œé€‚åº”æ€§ï¼Œæ¢è®¨äº†äººå·¥æ™ºèƒ½ä»£ç†çš„å¯ä¿¡åº¦å¯¹ä»»åŠ¡æ•ˆç‡çš„å½±å“ã€‚è¿™äº›å‘ç°å»ºç«‹äº†ä¸€ç§å¯é‡å¤çš„è¯„ä¼°æ–¹æ³•ï¼Œç”¨äºåœ¨ä¸åŒè¿è¥å•†ä¸ªæ€§å’Œäººæœºå›¢é˜ŸåŠ¨æ€ä¸‹å¯¹äººå·¥æ™ºèƒ½ä»£ç†çš„å¯é æ€§è¿›è¡Œå®éªŒã€‚æ€»ç»“æ¥è¯´ï¼Œè¯¥ç ”ç©¶æ¨è¿›äº†ä»£ç†å‹äººå·¥æ™ºèƒ½çš„å·¥ä½œæµç¨‹è¯„ä¼°ï¼Œä¸ä»…å…³æ³¨æ ‡å‡†æ€§èƒ½åº¦é‡æŒ‡æ ‡ï¼Œè¿˜ç»“åˆäº†ç¤¾ä¼šåŠ¨æ€çš„å…³é”®è¦ç´ ã€‚è¿™å¯¹äºå¤æ‚æ“ä½œä»»åŠ¡çš„æˆåŠŸè‡³å…³é‡è¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡æå‡ºè¯„ä¼°æ¡†æ¶é’ˆå¯¹ä»£ç†å‹AIç³»ç»Ÿåœ¨å…³é”®è°ˆåˆ¤ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚</li>
<li>åˆ©ç”¨Sotopiaä½œä¸ºæ¨¡æ‹Ÿæµ‹è¯•åºŠè¿›è¡Œå®éªŒç ”ç©¶ã€‚</li>
<li>å®éªŒä¸€å‘ç°äººæ ¼ç‰¹è´¨ï¼ˆå¦‚å®œäººæ€§ã€å¤–å‘æ€§ï¼‰å½±å“AIåœ¨æ¨¡æ‹Ÿç¤¾ä¼šè°ˆåˆ¤ä¸­çš„è¡¨ç°ã€‚</li>
<li>å®éªŒäºŒæ¢è®¨äº†AIä»£ç†çš„é€æ˜åº¦ã€èƒ½åŠ›å’Œé€‚åº”æ€§ç­‰ç‰¹æ€§åœ¨äººæœºè°ˆåˆ¤ä¸­çš„é‡è¦æ€§ã€‚</li>
<li>ç ”ç©¶é€šè¿‡å®é™…æ•°æ®æµ‹é‡äº†AIä»£ç†åœ¨å¤æ‚æ“ä½œåœºæ™¯ä¸­çš„å¯é æ€§ã€‚</li>
<li>ç ”ç©¶ç»“æœæä¾›äº†ä¸€ç§è¯„ä¼°AIä»£ç†åœ¨ä¸åŒæ“ä½œè€…ä¸ªæ€§å’Œå›¢é˜ŸåŠ¨æ€ä¸‹æ€§èƒ½çš„æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15928">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a8e851ec64ed5a0c98212bde2db8626c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d750f84d49b6cb5bc52730d1d4fb1e6f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1c6277b3b8dde75e0c6a53b2e5832fc7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-80297b907255febd28570f4fa2989b1c.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Metis-RISE-RL-Incentivizes-and-SFT-Enhances-Multimodal-Reasoning-Model-Learning"><a href="#Metis-RISE-RL-Incentivizes-and-SFT-Enhances-Multimodal-Reasoning-Model-Learning" class="headerlink" title="Metis-RISE: RL Incentivizes and SFT Enhances Multimodal Reasoning Model   Learning"></a>Metis-RISE: RL Incentivizes and SFT Enhances Multimodal Reasoning Model   Learning</h2><p><strong>Authors:Haibo Qiu, Xiaohan Lan, Fanfan Liu, Xiaohu Sun, Delian Ruan, Peng Shi, Lin Ma</strong></p>
<p>Recent advancements in large language models (LLMs) have witnessed a surge in the development of advanced reasoning paradigms, which are now being integrated into multimodal large language models (MLLMs). However, existing approaches often fall short: methods solely employing reinforcement learning (RL) can struggle with sample inefficiency and activating entirely absent reasoning capabilities, while conventional pipelines that initiate with a cold-start supervised fine-tuning (SFT) phase before RL may restrict the modelâ€™s exploratory capacity and face suboptimal convergence. In this work, we introduce \textbf{Metis-RISE} (\textbf{R}L \textbf{I}ncentivizes and \textbf{S}FT \textbf{E}nhances) for multimodal reasoning model learning. Unlike conventional approaches, Metis-RISE distinctively omits an initial SFT stage, beginning instead with an RL phase (e.g., using a Group Relative Policy Optimization variant) to incentivize and activate the modelâ€™s latent reasoning capacity. Subsequently, the targeted SFT stage addresses two key challenges identified during RL: (1) \textit{inefficient trajectory sampling} for tasks where the model possesses but inconsistently applies correct reasoning, which we tackle using self-distilled reasoning trajectories from the RL model itself; and (2) \textit{fundamental capability absence}, which we address by injecting expert-augmented knowledge for prompts where the model entirely fails. This strategic application of RL for incentivization followed by SFT for enhancement forms the core of Metis-RISE, leading to two versions of our MLLMs (7B and 72B parameters). Evaluations on the OpenCompass Multimodal Reasoning Leaderboard demonstrate that both models achieve state-of-the-art performance among similar-sized models, with the 72B version ranking fourth overall. Please refer to our project page for open-source information. </p>
<blockquote>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›å±•è§è¯äº†é«˜çº§æ¨ç†æ¨¡å¼çš„è“¬å‹ƒå‘å±•ï¼Œè¿™äº›æ¨¡å¼æ­£è¢«é›†æˆåˆ°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ä¸­ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¾€å¾€å­˜åœ¨ä¸è¶³ï¼šä»…ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ–¹æ³•å¯èƒ½é¢ä¸´æ ·æœ¬æ•ˆç‡ä½ä¸‹å’Œæ— æ³•æ¿€æ´»å®Œå…¨ç¼ºå¤±çš„æ¨ç†èƒ½åŠ›çš„é—®é¢˜ï¼Œè€Œä¼ ç»Ÿæµç¨‹åœ¨RLä¹‹å‰å…ˆè¿›è¡Œå†·å¯åŠ¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰é˜¶æ®µå¯èƒ½ä¼šé™åˆ¶æ¨¡å‹çš„æ¢ç´¢èƒ½åŠ›å¹¶å¯¼è‡´æ¬¡ä¼˜æ”¶æ•›ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†\textbf{Metis-RISEï¼ˆRLæ¿€åŠ±å¹¶å¢å¼ºSFTï¼‰}ç”¨äºå¤šæ¨¡æ€æ¨ç†æ¨¡å‹å­¦ä¹ ã€‚ä¸åŒäºä¼ ç»Ÿæ–¹æ³•ï¼ŒMetis-RISEç‹¬ç‰¹åœ°çœç•¥äº†åˆå§‹SFTé˜¶æ®µï¼Œè€Œæ˜¯é¦–å…ˆè¿›è¡ŒRLé˜¶æ®µï¼ˆä¾‹å¦‚ï¼Œä½¿ç”¨Group Relative Policy Optimizationå˜ä½“ï¼‰æ¥æ¿€åŠ±å’Œæ¿€æ´»æ¨¡å‹çš„æ½œåœ¨æ¨ç†èƒ½åŠ›ã€‚éšåï¼Œæœ‰é’ˆå¯¹æ€§çš„SFTé˜¶æ®µè§£å†³äº†RLæœŸé—´è¯†åˆ«çš„ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼šï¼ˆ1ï¼‰\textit{è½¨è¿¹é‡‡æ ·æ•ˆç‡ä½ä¸‹}ï¼Œé’ˆå¯¹æ¨¡å‹æ‹¥æœ‰ä½†åº”ç”¨ä¸æ­£ç¡®çš„æ¨ç†ä»»åŠ¡ï¼Œæˆ‘ä»¬é€šè¿‡ä½¿ç”¨RLæ¨¡å‹æœ¬èº«çš„è‡ªæˆ‘è’¸é¦æ¨ç†è½¨è¿¹æ¥è§£å†³æ­¤é—®é¢˜ï¼›ï¼ˆ2ï¼‰\textit{åŸºæœ¬èƒ½åŠ›ç¼ºå¤±}ï¼Œæˆ‘ä»¬é€šè¿‡æ³¨å…¥ä¸“å®¶å¢å¼ºçŸ¥è¯†æ¥è§£å†³æ­¤é—®é¢˜ï¼Œé’ˆå¯¹æ¨¡å‹å®Œå…¨å¤±è´¥çš„æƒ…å†µæä¾›æç¤ºã€‚RLçš„æ¿€åŠ±å’Œéšåçš„SFTå¢å¼ºç›¸ç»“åˆå½¢æˆäº†Metis-RISEçš„æ ¸å¿ƒï¼Œå¯¼è‡´æˆ‘ä»¬å¼€å‘äº†ä¸¤ä¸ªç‰ˆæœ¬çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆ7Bå’Œ72Bå‚æ•°ï¼‰ã€‚åœ¨OpenCompasså¤šæ¨¡æ€æ¨ç†æ’è¡Œæ¦œä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œè¿™ä¸¤ä¸ªæ¨¡å‹åœ¨åŒç±»æ¨¡å‹ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå…¶ä¸­72Bç‰ˆæœ¬æ€»ä½“æ’åç¬¬å››ã€‚æœ‰å…³å¼€æºä¿¡æ¯ï¼Œè¯·å‚é˜…æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.13056v2">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://github.com/MM-Thinking/Metis-RISE">https://github.com/MM-Thinking/Metis-RISE</a></p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›å±•æ¨åŠ¨äº†é«˜çº§æ¨ç†èŒƒå¼çš„å¼€å‘ï¼Œå¹¶é›†æˆåˆ°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ä¸­ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å­˜åœ¨ç¼ºé™·ï¼šä»…ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ–¹æ³•å¯èƒ½é¢ä¸´æ ·æœ¬æ•ˆç‡ä½ä¸‹å’Œç¼ºä¹æ¨ç†èƒ½åŠ›çš„é—®é¢˜ï¼Œè€Œä¼ ç»Ÿçš„å…ˆç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰åRLçš„æµç¨‹å¯èƒ½é™åˆ¶æ¨¡å‹çš„æ¢ç´¢èƒ½åŠ›å¹¶å¯¼è‡´æ¬¡ä¼˜æ”¶æ•›ã€‚æœ¬ç ”ç©¶ä»‹ç»äº†ä¸€ç§æ–°çš„å¤šæ¨¡æ€æ¨ç†æ¨¡å‹å­¦ä¹ æ–¹æ³•â€”â€”Metis-RISEã€‚ä¸åŒäºä¼ ç»Ÿæ–¹æ³•ï¼ŒMetis-RISEçœç•¥äº†åˆå§‹çš„SFTé˜¶æ®µï¼Œè€Œæ˜¯é¦–å…ˆé€šè¿‡RLé˜¶æ®µæ¿€åŠ±å’Œæ¿€æ´»æ¨¡å‹çš„æ½œåœ¨æ¨ç†èƒ½åŠ›ã€‚æ¥ç€ï¼Œé’ˆå¯¹RLé˜¶æ®µå‘ç°çš„ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼ˆå³ä»»åŠ¡è½¨è¿¹é‡‡æ ·æ•ˆç‡ä½ä¸‹å’Œç¼ºä¹åŸºæœ¬èƒ½åŠ›ï¼‰ï¼Œä½¿ç”¨è‡ªæˆ‘è’¸é¦çš„æ¨ç†è½¨è¿¹å’Œä¸“å®¶å¢å¼ºçŸ¥è¯†æ¥è§£å†³ã€‚è¿™ç§æˆ˜ç•¥æ€§çš„RLæ¿€åŠ±åŠ SFTå¢å¼ºçš„æ–¹æ³•å½¢æˆäº†Metis-RISEçš„æ ¸å¿ƒï¼Œäº§ç”Ÿäº†ä¸¤ä¸ªç‰ˆæœ¬çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆå‚æ•°åˆ†åˆ«ä¸º7Bå’Œ72Bï¼‰ã€‚åœ¨OpenCompasså¤šæ¨¡æ€æ¨ç†æ’è¡Œæ¦œä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œè¿™ä¸¤ä¸ªæ¨¡å‹åœ¨åŒç±»æ¨¡å‹ä¸­å®ç°äº†æœ€ä½³æ€§èƒ½ï¼Œå…¶ä¸­72Bç‰ˆæœ¬æ’åç¬¬å››ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›å±•æ¨åŠ¨äº†é«˜çº§æ¨ç†èŒƒå¼çš„é›†æˆåˆ°å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ä¸­ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•å­˜åœ¨æ ·æœ¬æ•ˆç‡ä½ä¸‹å’Œæ¨ç†èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>Metis-RISEæ–¹æ³•çœç•¥äº†åˆå§‹çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰é˜¶æ®µï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¿€åŠ±å’Œæ¿€æ´»æ¨¡å‹çš„æ½œåœ¨æ¨ç†èƒ½åŠ›ã€‚</li>
<li>Metis-RISEè§£å†³äº†RLé˜¶æ®µçš„ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼šä»»åŠ¡è½¨è¿¹é‡‡æ ·æ•ˆç‡ä½ä¸‹å’Œç¼ºä¹åŸºæœ¬èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡è‡ªæˆ‘è’¸é¦çš„æ¨ç†è½¨è¿¹å’Œä¸“å®¶å¢å¼ºçŸ¥è¯†æ¥è§£å†³è¿™ä¸¤ä¸ªæŒ‘æˆ˜ã€‚</li>
<li>Metis-RISEäº§ç”Ÿäº†ä¸¤ä¸ªç‰ˆæœ¬çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼Œåˆ†åˆ«åœ¨OpenCompasså¤šæ¨¡æ€æ¨ç†æ’è¡Œæ¦œä¸Šå–å¾—äº†æœ€ä½³æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.13056">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-2ce16cab2eafbec4319107175dcc70db.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ba74c31a8e0adbba98889d02b06436bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-44eecadb7158b7584037a96dea1051f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-269e297af42d36633d37b94f96ac5686.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="OneIG-Bench-Omni-dimensional-Nuanced-Evaluation-for-Image-Generation"><a href="#OneIG-Bench-Omni-dimensional-Nuanced-Evaluation-for-Image-Generation" class="headerlink" title="OneIG-Bench: Omni-dimensional Nuanced Evaluation for Image Generation"></a>OneIG-Bench: Omni-dimensional Nuanced Evaluation for Image Generation</h2><p><strong>Authors:Jingjing Chang, Yixiao Fang, Peng Xing, Shuhan Wu, Wei Cheng, Rui Wang, Xianfang Zeng, Gang Yu, Hai-Bao Chen</strong></p>
<p>Text-to-image (T2I) models have garnered significant attention for generating high-quality images aligned with text prompts. However, rapid T2I model advancements reveal limitations in early benchmarks, lacking comprehensive evaluations, for example, the evaluation on reasoning, text rendering and style. Notably, recent state-of-the-art models, with their rich knowledge modeling capabilities, show promising results on the image generation problems requiring strong reasoning ability, yet existing evaluation systems have not adequately addressed this frontier. To systematically address these gaps, we introduce OneIG-Bench, a meticulously designed comprehensive benchmark framework for fine-grained evaluation of T2I models across multiple dimensions, including prompt-image alignment, text rendering precision, reasoning-generated content, stylization, and diversity. By structuring the evaluation, this benchmark enables in-depth analysis of model performance, helping researchers and practitioners pinpoint strengths and bottlenecks in the full pipeline of image generation. Specifically, OneIG-Bench enables flexible evaluation by allowing users to focus on a particular evaluation subset. Instead of generating images for the entire set of prompts, users can generate images only for the prompts associated with the selected dimension and complete the corresponding evaluation accordingly. Our codebase and dataset are now publicly available to facilitate reproducible evaluation studies and cross-model comparisons within the T2I research community. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ¨¡å‹åœ¨ç”Ÿæˆä¸æ–‡æœ¬æç¤ºå¯¹é½çš„é«˜è´¨é‡å›¾åƒæ–¹é¢å¼•èµ·äº†æå¤§çš„å…³æ³¨ã€‚ç„¶è€Œï¼ŒT2Iæ¨¡å‹çš„å¿«é€Ÿå‘å±•æ˜¾ç¤ºå‡ºæ—©æœŸåŸºå‡†æµ‹è¯•çš„å±€é™æ€§ï¼Œç¼ºä¹å…¨é¢è¯„ä¼°ï¼Œä¾‹å¦‚å¯¹æ¨ç†ã€æ–‡æœ¬æ¸²æŸ“å’Œé£æ ¼çš„è¯„ä¼°ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæœ€è¿‘çš„æœ€å…ˆè¿›æ¨¡å‹å‡­å€Ÿå…¶ä¸°å¯Œçš„çŸ¥è¯†å»ºæ¨¡èƒ½åŠ›ï¼Œåœ¨éœ€è¦å¼ºå¤§æ¨ç†èƒ½åŠ›çš„å›¾åƒç”Ÿæˆé—®é¢˜ä¸Šæ˜¾ç¤ºå‡ºæœ‰å¸Œæœ›çš„ç»“æœï¼Œä½†ç°æœ‰çš„è¯„ä¼°ç³»ç»Ÿå°šæœªå……åˆ†è§£å†³è¿™ä¸€å‰æ²¿é—®é¢˜ã€‚ä¸ºäº†ç³»ç»Ÿåœ°è§£å†³è¿™äº›å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†OneIG-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªç²¾å¿ƒè®¾è®¡çš„ç»¼åˆåŸºå‡†æ¡†æ¶ï¼Œç”¨äºå¯¹T2Iæ¨¡å‹è¿›è¡Œè·¨å¤šä¸ªç»´åº¦çš„ç²¾ç»†è¯„ä¼°ï¼ŒåŒ…æ‹¬æç¤ºå›¾åƒå¯¹é½ã€æ–‡æœ¬æ¸²æŸ“ç²¾åº¦ã€æ¨ç†ç”Ÿæˆå†…å®¹ã€é£æ ¼åŒ–å’Œå¤šæ ·æ€§ã€‚é€šè¿‡ç»“æ„åŒ–è¯„ä¼°ï¼Œæ­¤åŸºå‡†æµ‹è¯•èƒ½å¤Ÿæ·±å…¥åˆ†ææ¨¡å‹æ€§èƒ½ï¼Œå¸®åŠ©ç ”ç©¶äººå‘˜å’Œå®è·µè€…ç¡®å®šå›¾åƒç”Ÿæˆæ•´ä¸ªæµç¨‹ä¸­çš„ä¼˜åŠ¿å’Œç“¶é¢ˆã€‚ç‰¹åˆ«æ˜¯ï¼ŒOneIG-Benché€šè¿‡å…è®¸ç”¨æˆ·å…³æ³¨ç‰¹å®šçš„è¯„ä¼°å­é›†æ¥å®ç°çµæ´»è¯„ä¼°ã€‚ç”¨æˆ·ä¸å¿…ä¸ºæ•´ä¸ªæç¤ºé›†ç”Ÿæˆå›¾åƒï¼Œè€Œå¯ä»¥åªä¸ºä¸æ‰€é€‰ç»´åº¦ç›¸å…³çš„æç¤ºç”Ÿæˆå›¾åƒï¼Œå¹¶ç›¸åº”åœ°å®Œæˆè¯„ä¼°ã€‚æˆ‘ä»¬çš„ä»£ç åº“å’Œæ•°æ®é›†ç°å·²å…¬å¼€å¯ç”¨ï¼Œä»¥ä¿ƒè¿›T2Iç ”ç©¶ç¤¾åŒºå†…çš„å¯é‡å¤è¯„ä¼°ç ”ç©¶å’Œè·¨æ¨¡å‹æ¯”è¾ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07977v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªé’ˆå¯¹æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ¨¡å‹çš„ç»¼åˆæ€§è¯„ä¼°æ¡†æ¶â€”â€”OneIG-Benchã€‚è¯¥æ¡†æ¶æ—¨åœ¨ç²¾ç»†åœ°è¯„ä¼°T2Iæ¨¡å‹åœ¨å¤šä¸ªç»´åº¦ä¸Šçš„æ€§èƒ½ï¼ŒåŒ…æ‹¬æ–‡æœ¬æç¤ºä¸å›¾åƒå¯¹é½ã€æ–‡æœ¬æ¸²æŸ“ç²¾åº¦ã€åŸºäºæ¨ç†çš„å†…å®¹ç”Ÿæˆã€é£æ ¼åŒ–ä»¥åŠå¤šæ ·æ€§ã€‚OneIG-Benchæä¾›äº†çµæ´»çš„è¯„ä»·æ–¹å¼ï¼Œå…è®¸ç”¨æˆ·ä»…é’ˆå¯¹ç‰¹å®šè¯„ä»·å­é›†è¿›è¡Œè¯„ä¼°ï¼Œä»¥æ·±å…¥äº†è§£æ¨¡å‹åœ¨ä¸åŒæ–¹é¢çš„è¡¨ç°ï¼Œæœ‰åŠ©äºç ”ç©¶äººå‘˜å’Œå®è·µè€…å‘ç°å›¾åƒç”Ÿæˆæµç¨‹ä¸­çš„ä¼˜åŠ¿å’Œç“¶é¢ˆã€‚OneIG-Benchçš„ä»£ç åº“å’Œæ•°æ®é›†ç°å·²å…¬å¼€ï¼Œä»¥ä¿ƒè¿›T2Iç ”ç©¶é¢†åŸŸå†…çš„å¯é‡å¤æ€§è¯„ä¼°ç ”ç©¶å’Œè·¨æ¨¡å‹æ¯”è¾ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OneIG-Benchæ˜¯ä¸€ä¸ªé’ˆå¯¹æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ¨¡å‹çš„ç»¼åˆæ€§è¯„ä¼°æ¡†æ¶ã€‚</li>
<li>è¯¥æ¡†æ¶æ—¨åœ¨ç²¾ç»†åœ°è¯„ä¼°T2Iæ¨¡å‹åœ¨å¤šä¸ªç»´åº¦ä¸Šçš„æ€§èƒ½ï¼ŒåŒ…æ‹¬æç¤ºä¸å›¾åƒå¯¹é½ã€æ–‡æœ¬æ¸²æŸ“ç²¾åº¦ç­‰ã€‚</li>
<li>OneIG-Benchæä¾›äº†çµæ´»çš„è¯„ä»·æ–¹å¼ï¼Œå…è®¸ç”¨æˆ·ä»…é’ˆå¯¹ç‰¹å®šè¯„ä»·å­é›†è¿›è¡Œè¯„ä¼°ã€‚</li>
<li>è¯¥æ¡†æ¶æœ‰åŠ©äºæ·±å…¥äº†è§£æ¨¡å‹åœ¨ä¸åŒæ–¹é¢çš„è¡¨ç°ï¼Œå¹¶å‘ç°å›¾åƒç”Ÿæˆæµç¨‹ä¸­çš„ä¼˜åŠ¿å’Œç“¶é¢ˆã€‚</li>
<li>OneIG-Benchæ”¯æŒä¸°å¯Œå¤šæ ·çš„è¯„ä»·ç»´åº¦ï¼ŒåŒ…æ‹¬åŸºäºæ¨ç†çš„å†…å®¹ç”Ÿæˆã€é£æ ¼åŒ–å’Œå¤šæ ·æ€§ç­‰ã€‚</li>
<li>è¯¥æ¡†æ¶çš„ä»£ç åº“å’Œæ•°æ®é›†ç°å·²å…¬å¼€ï¼Œä»¥ä¿ƒè¿›T2Iç ”ç©¶é¢†åŸŸå†…çš„å¯é‡å¤æ€§è¯„ä¼°ç ”ç©¶å’Œè·¨æ¨¡å‹æ¯”è¾ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07977">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2544647e6ab9e8bb2d31cf42b0f334d7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b0113c71ca059e049e83bffe5bfab23d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-91c4725be62f93558a019b4fd26cf42f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-72e10a45bd31d48192069449dd366c0c.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-28/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-28/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-28/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-4c826f5d3ba4bf3338ec4bc057ee9250.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-28  mTSBench Benchmarking Multivariate Time Series Anomaly Detection and   Model Selection at Scale
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-28
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-27/Interactive/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-01104ce69ff908bfbff30f9d42d6026b.jpg" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-27  Why Robots Are Bad at Detecting Their Mistakes Limitations of   Miscommunication Detection in Human-Robot Dialogue
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-27
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">31086.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
