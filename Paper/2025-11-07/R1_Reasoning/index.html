<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-07  Part-Aware Bottom-Up Group Reasoning for Fine-Grained Social Interaction   Detection">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-8e72cc24c8e286580d3bbabb45e922b3')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-07
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-16
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    78 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-07-æ›´æ–°"><a href="#2025-11-07-æ›´æ–°" class="headerlink" title="2025-11-07 æ›´æ–°"></a>2025-11-07 æ›´æ–°</h1><h2 id="Part-Aware-Bottom-Up-Group-Reasoning-for-Fine-Grained-Social-Interaction-Detection"><a href="#Part-Aware-Bottom-Up-Group-Reasoning-for-Fine-Grained-Social-Interaction-Detection" class="headerlink" title="Part-Aware Bottom-Up Group Reasoning for Fine-Grained Social Interaction   Detection"></a>Part-Aware Bottom-Up Group Reasoning for Fine-Grained Social Interaction   Detection</h2><p><strong>Authors:Dongkeun Kim, Minsu Cho, Suha Kwak</strong></p>
<p>Social interactions often emerge from subtle, fine-grained cues such as facial expressions, gaze, and gestures. However, existing methods for social interaction detection overlook such nuanced cues and primarily rely on holistic representations of individuals. Moreover, they directly detect social groups without explicitly modeling the underlying interactions between individuals. These drawbacks limit their ability to capture localized social signals and introduce ambiguity when group configurations should be inferred from social interactions grounded in nuanced cues. In this work, we propose a part-aware bottom-up group reasoning framework for fine-grained social interaction detection. The proposed method infers social groups and their interactions using body part features and their interpersonal relations. Our model first detects individuals and enhances their features using part-aware cues, and then infers group configuration by associating individuals via similarity-based reasoning, which considers not only spatial relations but also subtle social cues that signal interactions, leading to more accurate group inference. Experiments on the NVI dataset demonstrate that our method outperforms prior methods, achieving the new state of the art. </p>
<blockquote>
<p>ç¤¾ä¼šäº’åŠ¨å¾€å¾€æºäºå¾®å¦™çš„ã€ç²¾ç»†çš„çº¿ç´¢ï¼Œå¦‚é¢éƒ¨è¡¨æƒ…ã€ç›®å…‰å’Œæ‰‹åŠ¿ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç¤¾ä¼šäº’åŠ¨æ£€æµ‹æ–¹æ³•å¿½è§†äº†è¿™äº›ç»†å¾®çš„çº¿ç´¢ï¼Œä¸»è¦ä¾èµ–äºä¸ªäººçš„æ•´ä½“è¡¨ç°ã€‚è€Œä¸”ï¼Œå®ƒä»¬ç›´æ¥æ£€æµ‹ç¤¾ä¼šç¾¤ä½“ï¼Œè€Œæ²¡æœ‰æ˜ç¡®å»ºæ¨¡ä¸ªäººä¹‹é—´çš„åŸºæœ¬äº’åŠ¨ã€‚è¿™äº›ç¼ºç‚¹é™åˆ¶äº†å®ƒä»¬æ•æ‰å±€éƒ¨ç¤¾ä¼šä¿¡å·çš„èƒ½åŠ›ï¼Œå¹¶åœ¨åº”è¯¥æ ¹æ®ç»†å¾®çº¿ç´¢æ¨æ–­ç¤¾ä¼šäº’åŠ¨æ¥ç¡®å®šç¾¤ä½“é…ç½®æ—¶å¼•å…¥äº†æ­§ä¹‰ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é¢å‘ç²¾ç»†ç¤¾ä¼šäº’åŠ¨æ£€æµ‹çš„éƒ¨ä»¶æ„ŸçŸ¥è‡ªä¸‹è€Œä¸Šç¾¤ä½“æ¨ç†æ¡†æ¶ã€‚æ‰€æå‡ºçš„æ–¹æ³•åˆ©ç”¨èº«ä½“éƒ¨ä½ç‰¹å¾å’Œäººé™…å…³ç³»æ¥æ¨æ–­ç¤¾ä¼šç¾¤ä½“åŠå…¶äº’åŠ¨ã€‚æˆ‘ä»¬çš„æ¨¡å‹é¦–å…ˆæ£€æµ‹ä¸ªäººï¼Œå¹¶ä½¿ç”¨éƒ¨ä»¶æ„ŸçŸ¥çº¿ç´¢å¢å¼ºå…¶ç‰¹å¾ï¼Œç„¶åé€šè¿‡åŸºäºç›¸ä¼¼æ€§çš„æ¨ç†æ¥å…³è”ä¸ªäººï¼Œä»è€Œæ¨æ–­ç¾¤ä½“é…ç½®ã€‚è¿™ç§æ¨ç†ä¸ä»…è€ƒè™‘ç©ºé—´å…³ç³»ï¼Œè¿˜è€ƒè™‘æš—ç¤ºäº’åŠ¨çš„å¾®å¦™ç¤¾ä¼šçº¿ç´¢ï¼Œå¯¼è‡´æ›´å‡†ç¡®çš„ç¾¤ä½“æ¨æ–­ã€‚åœ¨NVIæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºä»¥å‰çš„æ–¹æ³•ï¼Œè¾¾åˆ°äº†æ–°çš„æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.03666v1">PDF</a> Accepted to NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æŒ‡å‡ºç¤¾ä¼šäº’åŠ¨å¾€å¾€æºäºå¾®å¦™çš„ã€ç²¾ç»†çš„çº¿ç´¢ï¼Œå¦‚é¢éƒ¨è¡¨æƒ…ã€ç›®å…‰å’Œæ‰‹åŠ¿ã€‚ç°æœ‰çš„ç¤¾ä¼šäº’åŠ¨æ£€æµ‹æ–¹æ³•å¿½è§†äº†è¿™äº›ç»†å¾®çš„çº¿ç´¢ï¼Œä¸»è¦ä¾èµ–äºä¸ªä½“çš„æ•´ä½“è¡¨ç°ã€‚æ­¤å¤–ï¼Œå®ƒä»¬ç›´æ¥æ£€æµ‹ç¤¾ä¼šç¾¤ä½“ï¼Œè€Œæ²¡æœ‰æ˜ç¡®æ¨¡æ‹Ÿä¸ªä½“ä¹‹é—´çš„åŸºæœ¬äº’åŠ¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå±€éƒ¨æ„ŸçŸ¥çš„è‡ªä¸‹è€Œä¸Šçš„ç¾¤ä½“æ¨ç†æ¡†æ¶ï¼Œç”¨äºç²¾ç»†çš„ç¤¾ä¼šäº’åŠ¨æ£€æµ‹ã€‚è¯¥æ–¹æ³•åˆ©ç”¨èº«ä½“éƒ¨ä½ç‰¹å¾å’Œäººé™…å…³ç³»æ¥æ¨æ–­ç¤¾ä¼šç¾¤ä½“åŠå…¶äº’åŠ¨ã€‚é¦–å…ˆæ£€æµ‹ä¸ªä½“å¹¶å¢å¼ºä»–ä»¬çš„ç‰¹å¾ï¼Œç„¶åé€šè¿‡åŸºäºç›¸ä¼¼æ€§çš„æ¨ç†å…³è”ä¸ªä½“æ¥æ¨æ–­ç¾¤ä½“é…ç½®ï¼Œè¿™ç§æ–¹æ³•ä¸ä»…è€ƒè™‘ç©ºé—´å…³ç³»ï¼Œè¿˜è€ƒè™‘æš—ç¤ºäº’åŠ¨çš„å¾®å¦™ç¤¾ä¼šçº¿ç´¢ï¼Œä»è€Œå®ç°æ›´å‡†ç¡®çš„ç¾¤ä½“æ¨æ–­ã€‚åœ¨NVIæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºä»¥å‰çš„æ–¹æ³•ï¼Œè¾¾åˆ°äº†æ–°çš„æŠ€æœ¯æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¤¾ä¼šäº’åŠ¨é€šå¸¸æºäºå¾®å¦™çš„çº¿ç´¢ï¼Œå¦‚é¢éƒ¨è¡¨æƒ…ã€ç›®å…‰å’Œæ‰‹åŠ¿ã€‚</li>
<li>ç°æœ‰ç¤¾ä¼šäº’åŠ¨æ£€æµ‹æ–¹æ³•ä¸»è¦ä¾èµ–äºä¸ªä½“çš„æ•´ä½“è¡¨ç°ï¼Œå¿½è§†äº†ç»†å¾®çº¿ç´¢ã€‚</li>
<li>ç›´æ¥æ£€æµ‹ç¤¾ä¼šç¾¤ä½“è€Œå¿½è§†ä¸ªä½“é—´åŸºæœ¬äº’åŠ¨çš„æ–¹æ³•å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºå±€éƒ¨æ„ŸçŸ¥çš„è‡ªä¸‹è€Œä¸Šçš„ç¾¤ä½“æ¨ç†æ¡†æ¶ç”¨äºç²¾ç»†ç¤¾ä¼šäº’åŠ¨æ£€æµ‹ã€‚</li>
<li>è¯¥æ–¹æ³•åˆ©ç”¨èº«ä½“éƒ¨ä½ç‰¹å¾å’Œäººé™…å…³ç³»æ¥æ¨æ–­ç¤¾ä¼šç¾¤ä½“åŠå…¶äº’åŠ¨ã€‚</li>
<li>é€šè¿‡å¢å¼ºä¸ªä½“ç‰¹å¾å¹¶è¿›è¡ŒåŸºäºç›¸ä¼¼æ€§çš„æ¨ç†æ¥æ¨æ–­ç¾¤ä½“é…ç½®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.03666">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5ff36dc85ddb298946b718c056110681" align="middle">
<img src="https://picx.zhimg.com/v2-901e065af4c5e8c02eec1a0e5c6c35c4" align="middle">
<img src="https://picx.zhimg.com/v2-c9669e466cdb559fa1485ebf8d196b6e" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="CareMedEval-dataset-Evaluating-Critical-Appraisal-and-Reasoning-in-the-Biomedical-Field"><a href="#CareMedEval-dataset-Evaluating-Critical-Appraisal-and-Reasoning-in-the-Biomedical-Field" class="headerlink" title="CareMedEval dataset: Evaluating Critical Appraisal and Reasoning in the   Biomedical Field"></a>CareMedEval dataset: Evaluating Critical Appraisal and Reasoning in the   Biomedical Field</h2><p><strong>Authors:Doria Bonzi, Alexandre Guiggi, FrÃ©dÃ©ric BÃ©chet, Carlos Ramisch, Benoit Favre</strong></p>
<p>Critical appraisal of scientific literature is an essential skill in the biomedical field. While large language models (LLMs) can offer promising support in this task, their reliability remains limited, particularly for critical reasoning in specialized domains. We introduce CareMedEval, an original dataset designed to evaluate LLMs on biomedical critical appraisal and reasoning tasks. Derived from authentic exams taken by French medical students, the dataset contains 534 questions based on 37 scientific articles. Unlike existing benchmarks, CareMedEval explicitly evaluates critical reading and reasoning grounded in scientific papers. Benchmarking state-of-the-art generalist and biomedical-specialized LLMs under various context conditions reveals the difficulty of the task: open and commercial models fail to exceed an Exact Match Rate of 0.5 even though generating intermediate reasoning tokens considerably improves the results. Yet, models remain challenged especially on questions about study limitations and statistical analysis. CareMedEval provides a challenging benchmark for grounded reasoning, exposing current LLM limitations and paving the way for future development of automated support for critical appraisal. </p>
<blockquote>
<p>æ–‡çŒ®æ‰¹åˆ¤æ˜¯ç”Ÿç‰©åŒ»å­¦é¢†åŸŸçš„ä¸€é¡¹åŸºæœ¬èƒ½åŠ›ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ­¤ä»»åŠ¡ä¸­æä¾›äº†æœ‰å‰æ™¯çš„æ”¯æŒï¼Œä½†å…¶å¯é æ€§ä»ç„¶æœ‰é™ï¼Œç‰¹åˆ«æ˜¯åœ¨ç‰¹å®šé¢†åŸŸçš„æ‰¹åˆ¤æ€§æ¨ç†æ–¹é¢ã€‚æˆ‘ä»¬ä»‹ç»äº†CareMedEvalï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°LLMåœ¨ç”Ÿç‰©åŒ»å­¦æ‰¹åˆ¤æ€§è¯„ä»·å’Œæ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°çš„æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†æ¥æºäºæ³•å›½åŒ»å­¦ç”ŸçœŸå®è€ƒè¯•ï¼ŒåŒ…å«åŸºäº37ç¯‡ç§‘å­¦æ–‡ç« çš„534ä¸ªé—®é¢˜ã€‚ä¸ç°æœ‰åŸºå‡†æµ‹è¯•ä¸åŒï¼ŒCareMedEvalæ˜ç¡®è¯„ä¼°åŸºäºç§‘å­¦è®ºæ–‡çš„æ‰¹åˆ¤æ€§é˜…è¯»å’Œæ¨ç†èƒ½åŠ›ã€‚åœ¨å¤šç§ä¸Šä¸‹æ–‡æ¡ä»¶ä¸‹å¯¹æœ€æ–°é€šç”¨å’Œç”Ÿç‰©åŒ»å­¦ä¸“ä¸šLLMè¿›è¡ŒåŸºå‡†æµ‹è¯•è¡¨æ˜è¯¥ä»»åŠ¡çš„éš¾åº¦ï¼šå¼€æ”¾å’Œå•†ä¸šæ¨¡å‹æœªèƒ½è¾¾åˆ°ç²¾ç¡®åŒ¹é…ç‡è¶…è¿‡0.5çš„æ°´å¹³ï¼Œå°½ç®¡ç”Ÿæˆä¸­é—´æ¨ç†ä»¤ç‰Œå¯ä»¥æ˜¾è‘—æ”¹å–„ç»“æœã€‚ç„¶è€Œï¼Œåœ¨å…³äºç ”ç©¶å±€é™æ€§å’Œç»Ÿè®¡åˆ†æçš„é—®é¢˜ä¸Šï¼Œæ¨¡å‹ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚CareMedEvalä¸ºåŸºäºç°å®æƒ…å†µçš„æ¨ç†æä¾›äº†ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ï¼Œæ­ç¤ºäº†å½“å‰LLMçš„å±€é™æ€§ï¼Œå¹¶ä¸ºæœªæ¥è‡ªåŠ¨åŒ–æ”¯æŒæ‰¹åˆ¤æ€§è¯„ä»·çš„å‘å±•é“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.03441v1">PDF</a> Preprint submitted to LREC 2026 (under review) To access the dataset,   see <a target="_blank" rel="noopener" href="https://github.com/bonzid/CareMedEval">https://github.com/bonzid/CareMedEval</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†CareMedEvalæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†ç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç”Ÿç‰©åŒ»å­¦æ‰¹è¯„è¯„ä»·å’Œæ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚è¯¥æ•°æ®é›†åŒ…å«åŸºäºçœŸå®è€ƒè¯•çš„534ä¸ªé—®é¢˜ï¼Œè¿™äº›é—®é¢˜åŸºäºåŒ»å­¦å­¦ç”Ÿçš„å®é™…é˜…è¯»ææ–™ç”Ÿæˆã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹é¢ä¸´æŒ‘æˆ˜ï¼Œä½†é€šè¿‡ç”Ÿæˆä¸­é—´æ¨ç†æ ‡è®°å¯ä»¥æ˜¾è‘—æé«˜ç»“æœã€‚ç„¶è€Œï¼Œæ¨¡å‹ä»ç„¶é¢ä¸´å…³äºç ”ç©¶å±€é™æ€§å’Œç»Ÿè®¡åˆ†æçš„é—®é¢˜ã€‚CareMedEvalä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†ä¸€ä¸ªæŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ï¼Œæ­ç¤ºäº†å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹çš„å±€é™æ€§ï¼Œå¹¶ä¸ºæœªæ¥çš„è‡ªåŠ¨åŒ–æ”¯æŒæä¾›äº†å‘å±•æœºä¼šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CareMedEvalæ˜¯ä¸€ä¸ªä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”Ÿç‰©åŒ»å­¦æ‰¹è¯„è¯„ä»·å’Œæ¨ç†ä»»åŠ¡æ€§èƒ½çš„æ•°æ®é›†ã€‚</li>
<li>æ•°æ®é›†åŒ…å«åŸºäºçœŸå®è€ƒè¯•çš„534ä¸ªé—®é¢˜ï¼Œè¿™äº›é—®é¢˜åŸºäºåŒ»å­¦å­¦ç”Ÿçš„é˜…è¯»ææ–™ç”Ÿæˆã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å®Œæˆä»»åŠ¡æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å…³äºç ”ç©¶å±€é™æ€§å’Œç»Ÿè®¡åˆ†æçš„é—®é¢˜æ—¶ã€‚</li>
<li>ç”Ÿæˆä¸­é—´æ¨ç†æ ‡è®°å¯ä»¥æ˜¾è‘—æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„ç»“æœã€‚</li>
<li>CareMedEvalæä¾›äº†ä¸€ä¸ªæŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹çš„æ€§èƒ½å¹¶æ­ç¤ºå½“å‰å¤§å‹è¯­è¨€æ¨¡å‹çš„å±€é™æ€§ã€‚</li>
<li>è¯¥æ•°æ®é›†æœ‰åŠ©äºä¸ºæœªæ¥çš„ç ”ç©¶å’Œå‘å±•æ–¹å‘æä¾›æŒ‡å¯¼ï¼Œç‰¹åˆ«æ˜¯åœ¨è‡ªåŠ¨åŒ–æ”¯æŒç”Ÿç‰©åŒ»å­¦é¢†åŸŸçš„æ‰¹è¯„è¯„ä»·æ–¹é¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.03441">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-795d371c6efb9d09931b3ef09de94bae" align="middle">
<img src="https://picx.zhimg.com/v2-16defadae822eb38b5490339e2d88ec1" align="middle">
<img src="https://picx.zhimg.com/v2-23cb1922783dc844fe22a4ed0dbb98a0" align="middle">
<img src="https://picx.zhimg.com/v2-a9f751040a82aef804a0b195d7df1029" align="middle">
<img src="https://picx.zhimg.com/v2-755a23bfb5bc1fa0ed51494488478add" align="middle">
<img src="https://picx.zhimg.com/v2-dfb9d5a861ee149ce0c8c2276c087f9a" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="LFC-DA-Logical-Formula-Controlled-Data-Augmentation-for-Enhanced-Logical-Reasoning"><a href="#LFC-DA-Logical-Formula-Controlled-Data-Augmentation-for-Enhanced-Logical-Reasoning" class="headerlink" title="LFC-DA: Logical Formula-Controlled Data Augmentation for Enhanced   Logical Reasoning"></a>LFC-DA: Logical Formula-Controlled Data Augmentation for Enhanced   Logical Reasoning</h2><p><strong>Authors:Shenghao Li</strong></p>
<p>For complex logical data augmentation, heavy reliance on human annotation is costly, whereas direct generation with large language models yields uninterpretable and logically homogeneous examples. To address this, we present LFC-DA, a symbolic-logic-controlled pipeline: logical text is first mapped to propositional expressions, a compact rule library is compiled, and a bounded state-space search systematically discovers valid formulas that are then verbalized back into natural-language questions, ensuring both diversity and logical rigor under propositional logic. Experiments on ReClor and LogiQA show significant improvements in the logical-reasoning accuracy of pretrained models, confirming the effectiveness of LFC-DA for LLM-guided logical data augmentation. </p>
<blockquote>
<p>å¯¹äºå¤æ‚é€»è¾‘æ•°æ®å¢å¼ºï¼Œè¿‡åº¦ä¾èµ–äººå·¥æ ‡æ³¨æˆæœ¬é«˜æ˜‚ï¼Œè€Œç›´æ¥ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆåˆ™ä¼šäº§ç”Ÿä¸å¯è§£é‡Šå’Œé€»è¾‘ä¸Šå•ä¸€åŒ–çš„ä¾‹å­ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†LFC-DAï¼Œè¿™æ˜¯ä¸€ä¸ªå—ç¬¦å·é€»è¾‘æ§åˆ¶çš„è¿‡ç¨‹ï¼šé¦–å…ˆï¼Œå°†é€»è¾‘æ–‡æœ¬æ˜ å°„åˆ°å‘½é¢˜è¡¨è¾¾å¼ï¼Œç¼–è¯‘ä¸€ä¸ªç´§å‡‘çš„è§„åˆ™åº“ï¼Œç„¶ååœ¨æœ‰ç•ŒçŠ¶æ€ç©ºé—´æœç´¢ä¸­ç³»ç»Ÿåœ°å‘ç°æœ‰æ•ˆçš„å…¬å¼ï¼Œè¿™äº›å…¬å¼éšåè¢«è½¬åŒ–ä¸ºè‡ªç„¶è¯­è¨€é—®é¢˜ï¼Œç¡®ä¿äº†å‘½é¢˜é€»è¾‘ä¸‹çš„å¤šæ ·æ€§å’Œé€»è¾‘ä¸¥è°¨æ€§ã€‚åœ¨ReClorå’ŒLogiQAä¸Šçš„å®éªŒè¡¨æ˜ï¼Œé¢„è®­ç»ƒæ¨¡å‹çš„é€»è¾‘æ¨ç†å‡†ç¡®æ€§å¾—åˆ°äº†æ˜¾è‘—æé«˜ï¼Œè¯å®äº†LFC-DAåœ¨å¤§å‹è¯­è¨€æ¨¡å‹å¼•å¯¼çš„é€»è¾‘æ•°æ®å¢å¼ºä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.03372v1">PDF</a> 10 pages, 6 figures</p>
<p><strong>Summary</strong>ï¼šé’ˆå¯¹å¤æ‚é€»è¾‘æ•°æ®å¢å¼ºï¼Œå•çº¯ä¾èµ–äººå·¥æ ‡æ³¨æˆæœ¬é«˜æ˜‚ï¼Œè€Œç›´æ¥ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆåˆ™äº§ç”Ÿéš¾ä»¥è§£é‡Šä¸”é€»è¾‘å•ä¸€çš„ä¾‹å­ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºLFC-DAï¼Œä¸€ç§ç¬¦å·é€»è¾‘æ§åˆ¶æµç¨‹ï¼šé¦–å…ˆå°†é€»è¾‘æ–‡æœ¬æ˜ å°„åˆ°å‘½é¢˜è¡¨è¾¾å¼ï¼Œç¼–è¯‘ç®€æ´çš„è§„åˆ™åº“ï¼Œç„¶ååœ¨æœ‰ç•ŒçŠ¶æ€ç©ºé—´æœç´¢ä¸­ç³»ç»Ÿåœ°å‘ç°æœ‰æ•ˆå…¬å¼ï¼Œå†å°†å…¶è½¬åŒ–ä¸ºè‡ªç„¶è¯­è¨€é—®é¢˜ï¼Œç¡®ä¿å¤šæ ·æ€§å’Œé€»è¾‘ä¸¥è°¨æ€§ã€‚åœ¨ReClorå’ŒLogiQAä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒLFC-DAèƒ½æ˜¾è‘—æé«˜é¢„è®­ç»ƒæ¨¡å‹çš„é€»è¾‘æ¨ç†å‡†ç¡®æ€§ï¼Œè¯æ˜å…¶åœ¨å¤§å‹è¯­è¨€æ¨¡å‹å¼•å¯¼çš„é€»è¾‘æ•°æ®å¢å¼ºä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>LFC-DAæ˜¯ä¸€ç§ç¬¦å·é€»è¾‘æ§åˆ¶æµç¨‹ï¼Œç”¨äºé€»è¾‘æ•°æ®å¢å¼ºã€‚</li>
<li>æµç¨‹åŒ…æ‹¬å°†é€»è¾‘æ–‡æœ¬æ˜ å°„åˆ°å‘½é¢˜è¡¨è¾¾å¼ï¼Œç¼–è¯‘è§„åˆ™åº“ï¼Œå¹¶åœ¨æœ‰ç•ŒçŠ¶æ€ç©ºé—´æœç´¢ä¸­å‘ç°æœ‰æ•ˆå…¬å¼ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½ç¡®ä¿ç”Ÿæˆçš„ä¾‹å­æ—¢å¤šæ ·åˆé€»è¾‘ä¸¥è°¨ã€‚</li>
<li>LFC-DAé€šè¿‡å°†å…¬å¼è½¬åŒ–ä¸ºè‡ªç„¶è¯­è¨€é—®é¢˜ï¼Œæé«˜äº†é¢„è®­ç»ƒæ¨¡å‹çš„é€»è¾‘æ¨ç†å‡†ç¡®æ€§ã€‚</li>
<li>å®éªŒåœ¨ReClorå’ŒLogiQAæ•°æ®é›†ä¸Šè¿›è¡Œï¼Œè¯æ˜äº†LFC-DAçš„æœ‰æ•ˆæ€§ã€‚</li>
<li>å•çº¯ä¾èµ–äººå·¥æ ‡æ³¨æˆæœ¬é«˜æ˜‚ï¼Œè€Œç›´æ¥ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆåˆ™å¯èƒ½äº§ç”Ÿéš¾ä»¥è§£é‡Šçš„ç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.03372">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bb1b76b52d53b644b0e91ffa3e954499" align="middle">
<img src="https://picx.zhimg.com/v2-358b77deddab1c60a7b33dfe18e1f81d" align="middle">
<img src="https://picx.zhimg.com/v2-a36174cec7834dc7b34e13fb856fe33b" align="middle">
<img src="https://picx.zhimg.com/v2-a1e9da32e5476eb96a979508c3883ff4" align="middle">
<img src="https://picx.zhimg.com/v2-701281136a758df8c7dde9659904076c" align="middle">
<img src="https://picx.zhimg.com/v2-940371aa3713d6d8ea2ef5030481e793" align="middle">
<img src="https://picx.zhimg.com/v2-2d475b9d566ffe2b4e74d80b80dc690e" align="middle">
<img src="https://picx.zhimg.com/v2-e8d9ddf4e1469b6f6af889083cc9609d" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Auditing-M-LLMs-for-Privacy-Risks-A-Synthetic-Benchmark-and-Evaluation-Framework"><a href="#Auditing-M-LLMs-for-Privacy-Risks-A-Synthetic-Benchmark-and-Evaluation-Framework" class="headerlink" title="Auditing M-LLMs for Privacy Risks: A Synthetic Benchmark and Evaluation   Framework"></a>Auditing M-LLMs for Privacy Risks: A Synthetic Benchmark and Evaluation   Framework</h2><p><strong>Authors:Junhao Li, Jiahao Chen, Zhou Feng, Chunyi Zhou</strong></p>
<p>Recent advances in multi-modal Large Language Models (M-LLMs) have demonstrated a powerful ability to synthesize implicit information from disparate sources, including images and text. These resourceful data from social media also introduce a significant and underexplored privacy risk: the inference of sensitive personal attributes from seemingly daily media content. However, the lack of benchmarks and comprehensive evaluations of state-of-the-art M-LLM capabilities hinders the research of private attribute profiling on social media. Accordingly, we propose (1) PRISM, the first multi-modal, multi-dimensional and fine-grained synthesized dataset incorporating a comprehensive privacy landscape and dynamic user history; (2) an Efficient evaluation framework that measures the cross-modal privacy inference capabilities of advanced M-LLM. Specifically, PRISM is a large-scale synthetic benchmark designed to evaluate cross-modal privacy risks. Its key feature is 12 sensitive attribute labels across a diverse set of multi-modal profiles, which enables targeted privacy analysis. These profiles are generated via a sophisticated LLM agentic workflow, governed by a prior distribution to ensure they realistically mimic social media users. Additionally, we propose a Multi-Agent Inference Framework that leverages a pipeline of specialized LLMs to enhance evaluation capabilities. We evaluate the inference capabilities of six leading M-LLMs (Qwen, Gemini, GPT-4o, GLM, Doubao, and Grok) on PRISM. The comparison with human performance reveals that these MLLMs significantly outperform in accuracy and efficiency, highlighting the threat of potential privacy risks and the urgent need for robust defenses. </p>
<blockquote>
<p>æœ€è¿‘å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆM-LLMï¼‰çš„è¿›å±•è¡¨æ˜ï¼Œå®ƒä»¬å…·æœ‰ä»å„ç§æ¥æºåˆæˆéšå«ä¿¡æ¯ï¼ˆåŒ…æ‹¬å›¾åƒå’Œæ–‡æœ¬ï¼‰çš„å¼ºå¤§èƒ½åŠ›ã€‚è¿™äº›æ¥è‡ªç¤¾äº¤åª’ä½“çš„æœ‰ç”¨æ•°æ®ä¹Ÿå¸¦æ¥äº†ä¸€ä¸ªé‡å¤§ä¸”å°šæœªè¢«å……åˆ†ç ”ç©¶çš„éšç§é—®é¢˜ï¼šä»çœ‹ä¼¼æ—¥å¸¸çš„åª’ä½“å†…å®¹ä¸­æ¨æ–­å‡ºæ•æ„Ÿçš„ä¸ªäººå±æ€§ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹åŸºå‡†æµ‹è¯•å’Œæœ€æ–°M-LLMèƒ½åŠ›çš„å…¨é¢è¯„ä¼°ï¼Œç¤¾äº¤åª’ä½“ä¸Šçš„ä¸ªäººå±æ€§åˆ†æçš„ç ”ç©¶å—åˆ°äº†é˜»ç¢ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ï¼ˆ1ï¼‰æ£±é•œï¼ˆPRISMï¼‰ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå¤šæ¨¡æ€ã€å¤šç»´åº¦ã€ç²¾ç»†åˆæˆçš„æ•°æ®é›†ï¼Œèåˆäº†å…¨é¢çš„éšç§çŠ¶å†µå’ŒåŠ¨æ€ç”¨æˆ·å†å²ï¼›ï¼ˆ2ï¼‰ä¸€ä¸ªé«˜æ•ˆçš„è¯„ä¼°æ¡†æ¶ï¼Œç”¨äºè¡¡é‡å…ˆè¿›M-LLMçš„è·¨æ¨¡æ€éšç§æ¨æ–­èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œæ£±é•œæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡åˆæˆåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°è·¨æ¨¡æ€éšç§é£é™©ã€‚å®ƒçš„å…³é”®ç‰¹ç‚¹æ˜¯åŒ…å«12ä¸ªæ•æ„Ÿå±æ€§æ ‡ç­¾ï¼Œè·¨è¶Šä¸€ç³»åˆ—å¤šæ¨¡æ€é…ç½®æ–‡ä»¶ï¼Œå¯å®ç°æœ‰é’ˆå¯¹æ€§çš„éšç§åˆ†æã€‚è¿™äº›é…ç½®æ–‡ä»¶æ˜¯é€šè¿‡å¤æ‚çš„LLMä»£ç†å·¥ä½œæµç¨‹ç”Ÿæˆçš„ï¼Œéµå¾ªå…ˆéªŒåˆ†å¸ƒï¼Œä»¥ç¡®ä¿å®ƒä»¬çœŸå®åœ°æ¨¡æ‹Ÿç¤¾äº¤åª’ä½“ç”¨æˆ·ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå¤šä»£ç†æ¨æ–­æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨ä¸€ç³»åˆ—ä¸“é—¨çš„LLMsæ¥å¢å¼ºè¯„ä¼°èƒ½åŠ›ã€‚æˆ‘ä»¬åœ¨æ£±é•œä¸Šè¯„ä¼°äº†å…­ä¸ªé¢†å…ˆM-LLMï¼ˆQwenã€Geminiã€GPT-4oã€GLMã€Doubaoå’ŒGrokï¼‰çš„æ¨æ–­èƒ½åŠ›ã€‚ä¸äººç±»æ€§èƒ½çš„æ¯”è¾ƒè¡¨æ˜ï¼Œè¿™äº›MLLMåœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—çš„ä¼˜åŠ¿ï¼Œè¿™çªæ˜¾äº†æ½œåœ¨çš„éšç§é£é™©å¨èƒå’Œå¯¹ç¨³å¥é˜²å¾¡çš„è¿«åˆ‡éœ€æ±‚ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.03248v1">PDF</a> 14 pages, 3 figures; Accepted by MMM 2026; Complete version in   progress</p>
<p><strong>Summary</strong></p>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆM-LLMï¼‰èƒ½å¤Ÿä»å„ç§æ¥æºä¸­ç»¼åˆéšå«ä¿¡æ¯ï¼ŒåŒ…æ‹¬å›¾åƒå’Œæ–‡æœ¬ã€‚ç„¶è€Œï¼Œè¿™äº›ç¤¾äº¤åª’ä½“èµ„æºä¹Ÿå¸¦æ¥äº†é‡å¤§ä¸”å°šæœªè¢«å……åˆ†æ¢ç´¢çš„éšç§é£é™©ï¼Œå³ä»æ—¥å¸¸åª’ä½“å†…å®¹ä¸­æ¨æ–­å‡ºæ•æ„Ÿçš„ä¸ªäººå±æ€§ã€‚é’ˆå¯¹è¿™ä¸€é¢†åŸŸç¼ºä¹åŸºå‡†æµ‹è¯•å’ŒM-LLMçš„æœ€æ–°èƒ½åŠ›ç»¼åˆè¯„ä¼°ï¼Œæˆ‘ä»¬æå‡ºäº†ï¼ˆ1ï¼‰PRISMï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå¤šæ¨¡æ€ã€å¤šç»´åº¦ã€ç²¾ç»†åˆæˆçš„æ•°æ®é›†ï¼Œèå…¥å…¨é¢çš„éšç§æ™¯è§‚å’ŒåŠ¨æ€ç”¨æˆ·å†å²ï¼›ï¼ˆ2ï¼‰æœ‰æ•ˆçš„è¯„ä¼°æ¡†æ¶ï¼Œç”¨ä»¥è¡¡é‡å…ˆè¿›M-LLMçš„è·¨æ¨¡æ€éšç§æ¨æ–­èƒ½åŠ›ã€‚PRISMæ˜¯ä¸€ä¸ªå¤§å‹åˆæˆåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°è·¨æ¨¡æ€éšç§é£é™©ã€‚å…¶å…³é”®åŠŸèƒ½åœ¨äºåŒ…å«12ä¸ªæ•æ„Ÿå±æ€§æ ‡ç­¾ï¼Œè¿™äº›æ ‡ç­¾åº”ç”¨äºä¸€ç³»åˆ—å¤šæ¨¡æ€èµ„æ–™çš„å¤šæ ·åŒ–é›†åˆä¸Šï¼Œä»¥å®ç°æœ‰é’ˆå¯¹æ€§çš„éšç§åˆ†æã€‚è¿™äº›èµ„æ–™æ˜¯é€šè¿‡éµå¾ªå…ˆéªŒåˆ†å¸ƒçš„LLMä»£ç†å·¥ä½œæµç¨‹ç”Ÿæˆçš„ï¼Œä»¥ç¡®ä¿å®ƒä»¬çœŸå®åœ°æ¨¡æ‹Ÿç¤¾äº¤åª’ä½“ç”¨æˆ·ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†å¤šä»£ç†æ¨ç†æ¡†æ¶ï¼Œåˆ©ç”¨ä¸€ç³»åˆ—ä¸“ä¸šLLMå¢å¼ºè¯„ä¼°èƒ½åŠ›ã€‚æˆ‘ä»¬åœ¨PRISMä¸Šè¯„ä¼°äº†å…­ç§é¢†å…ˆçš„M-LLMï¼ˆQwenã€Geminiã€GPT-4oã€GLMã€Doubaoå’ŒGrokï¼‰çš„æ¨æ–­èƒ½åŠ›ã€‚ä¸äººç±»çš„æ€§èƒ½ç›¸æ¯”ï¼Œè¿™äº›MLLMåœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œçªæ˜¾äº†æ½œåœ¨çš„éšç§é£é™©å¨èƒå’ŒäºŸéœ€çš„ç¨³å¥é˜²å¾¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆM-LLMï¼‰èƒ½ä»å›¾åƒå’Œæ–‡æœ¬ç­‰å¤šç§æ¥æºä¸­ç»¼åˆä¿¡æ¯ã€‚</li>
<li>ç¤¾äº¤åª’ä½“èµ„æºå¼•å…¥é‡å¤§ä¸”æœªè¢«å……åˆ†æ¢ç´¢çš„éšç§é£é™©ï¼Œå³ä»æ—¥å¸¸å†…å®¹ä¸­æ¨æ–­æ•æ„Ÿä¸ªäººå±æ€§ã€‚</li>
<li>ç¼ºä¹åŸºå‡†æµ‹è¯•å’ŒM-LLMçš„ç»¼åˆèƒ½åŠ›è¯„ä¼°é˜»ç¢äº†ç¤¾äº¤åª’ä½“çš„éšç§å±æ€§åˆ†æçš„ç ”ç©¶ã€‚</li>
<li>æå‡ºäº†PRISMæ•°æ®é›†ï¼Œä¸€ä¸ªç”¨äºè¯„ä¼°è·¨æ¨¡æ€éšç§é£é™©çš„åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«å¤šæ¨¡æ€ã€å¤šç»´åº¦å’Œç²¾ç»†åˆæˆçš„æ•°æ®ã€‚</li>
<li>PRISMå…·æœ‰12ä¸ªæ•æ„Ÿå±æ€§æ ‡ç­¾ï¼Œç”¨äºæœ‰é’ˆå¯¹æ€§çš„éšç§åˆ†æï¼Œå¹¶ç”Ÿæˆæ¨¡æ‹Ÿç¤¾äº¤åª’ä½“ç”¨æˆ·è¡Œä¸ºçš„åˆæˆèµ„æ–™ã€‚</li>
<li>å¼•å…¥å¤šä»£ç†æ¨ç†æ¡†æ¶ï¼Œåˆ©ç”¨ä¸“ä¸šLLMå¢å¼ºè¯„ä¼°èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.03248">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-035ed9c0f39e2aa241ee0c15c4fb0d77" align="middle">
<img src="https://picx.zhimg.com/v2-00270f96b59486afe0bcca46b5727894" align="middle">
<img src="https://picx.zhimg.com/v2-6e067cc902b0526e1143797faa93a5f6" align="middle">
<img src="https://picx.zhimg.com/v2-7ee0f161ed3d66c193d075498f6b796a" align="middle">
<img src="https://picx.zhimg.com/v2-adc9c7e2c5c9bc00f26ff1b92876d5f8" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="LEGO-Eval-Towards-Fine-Grained-Evaluation-on-Synthesizing-3D-Embodied-Environments-with-Tool-Augmentation"><a href="#LEGO-Eval-Towards-Fine-Grained-Evaluation-on-Synthesizing-3D-Embodied-Environments-with-Tool-Augmentation" class="headerlink" title="LEGO-Eval: Towards Fine-Grained Evaluation on Synthesizing 3D Embodied   Environments with Tool Augmentation"></a>LEGO-Eval: Towards Fine-Grained Evaluation on Synthesizing 3D Embodied   Environments with Tool Augmentation</h2><p><strong>Authors:Gyeom Hwangbo, Hyungjoo Chae, Minseok Kang, Hyeonjong Ju, Soohyun Oh, Jinyoung Yeo</strong></p>
<p>Despite recent progress in using Large Language Models (LLMs) for automatically generating 3D scenes, generated scenes often lack realistic spatial layouts and object attributes found in real-world environments. As this problem stems from insufficiently detailed, coarse-grained instructions, advancing 3D scene synthesis guided by more detailed, fine-grained instructions that reflect real-world environments becomes crucial. Without such realistic scenes, training embodied agents in unrealistic environments can lead them to learn priors that diverge significantly from real-world physics and semantics, degrading their performance when deployed. Thus, verifying the alignment between the fine-grained instruction and the generated scene is essential for effective learning. However, current evaluation methods, such as CLIPScore and vision-language models (VLMs), often fail to reliably assess such alignment. This shortcoming arises primarily from their shallow understanding of 3D scenes, which often leads to improperly grounded scene components. To address this, we introduce LEGO-Eval, an evaluation framework equipped with diverse tools designed to explicitly ground scene components, enabling more accurate alignment assessments. We also present LEGO-Bench, a benchmark of detailed instructions that specify complex layouts and attributes of real-world environments. Experiments demonstrate that LEGO-Eval outperforms VLM-as-a-judge by 0.41 F1 score in assessing scene-instruction alignment. Benchmarking with LEGO-Bench reveals significant limitations in current generation methods. Across all evaluated approaches, success rates reached at most 10% in generating scenes that fully align with fine-grained instructions. </p>
<blockquote>
<p>å°½ç®¡æœ€è¿‘åœ¨åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡ªåŠ¨ç”Ÿæˆ3Dåœºæ™¯æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†ç”Ÿæˆçš„åœºæ™¯é€šå¸¸ç¼ºä¹ç°å®ä¸–ç•Œç¯å¢ƒä¸­å­˜åœ¨çš„çœŸå®ç©ºé—´å¸ƒå±€å’Œç‰©ä½“å±æ€§ã€‚è¿™ä¸ªé—®é¢˜æºäºæŒ‡ä»¤ä¸å¤Ÿè¯¦ç»†ä¸”è¿‡äºç²—ç•¥ï¼Œå› æ­¤ï¼Œé€šè¿‡æ›´è¯¦ç»†ã€æ›´ç²¾ç»†çš„æŒ‡ä»¤æ¥æ¨åŠ¨3Dåœºæ™¯åˆæˆï¼Œä»¥åæ˜ ç°å®ä¸–ç•Œç¯å¢ƒè‡³å…³é‡è¦ã€‚æ²¡æœ‰è¿™æ ·çš„çœŸå®åœºæ™¯ï¼Œåœ¨ä¸çœŸå®çš„ç¯å¢ƒä¸­è®­ç»ƒå®ä½“ä»£ç†ä¼šå¯¼è‡´å®ƒä»¬å­¦åˆ°çš„å…ˆéªŒçŸ¥è¯†ä¸¥é‡åç¦»ç°å®ä¸–ç•Œçš„ç‰©ç†å’Œè¯­ä¹‰ï¼Œåœ¨éƒ¨ç½²æ—¶æ€§èƒ½ä¸‹é™ã€‚å› æ­¤ï¼ŒéªŒè¯ç²¾ç»†æŒ‡ä»¤ä¸ç”Ÿæˆåœºæ™¯ä¹‹é—´çš„å¯¹é½å¯¹äºæœ‰æ•ˆå­¦ä¹ è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è¯„ä¼°æ–¹æ³•ï¼Œå¦‚CLIPScoreå’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œé€šå¸¸æ— æ³•å¯é åœ°è¯„ä¼°è¿™ç§å¯¹é½ã€‚è¿™ä¸€ç¼ºé™·ä¸»è¦æºäºå®ƒä»¬å¯¹3Dåœºæ™¯çš„æµ…è–„ç†è§£ï¼Œè¿™å¾€å¾€å¯¼è‡´åœºæ™¯ç»„ä»¶ä¸å½“å®šä½ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†LEGO-Evalè¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é…å¤‡äº†å„ç§å·¥å…·ï¼Œæ—¨åœ¨æ˜ç¡®å®šä½åœºæ™¯ç»„ä»¶ï¼Œä»è€Œå®ç°æ›´å‡†ç¡®çš„å¯¹é½è¯„ä¼°ã€‚æˆ‘ä»¬è¿˜æ¨å‡ºäº†LEGO-BenchåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«è¯¦ç»†çš„æŒ‡ä»¤ï¼ŒæŒ‡å®šå¤æ‚å¸ƒå±€å’Œç°å®ä¸–ç•Œç¯å¢ƒçš„å±æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨è¯„ä¼°åœºæ™¯æŒ‡ä»¤å¯¹é½æ–¹é¢ï¼ŒLEGO-Evalçš„F1åˆ†æ•°æ¯”ä»¥è§†è§‰è¯­è¨€æ¨¡å‹ä½œä¸ºè¯„åˆ¤é«˜å‡º0.41åˆ†ã€‚ä½¿ç”¨LEGO-BenchåŸºå‡†æµ‹è¯•è¡¨æ˜ï¼Œå½“å‰ç”Ÿæˆæ–¹æ³•çš„å±€é™æ€§æ˜¾è‘—ã€‚åœ¨æ‰€æœ‰è¯„ä¼°æ–¹æ³•ä¸­ï¼ŒæˆåŠŸç”Ÿæˆä¸ç²¾ç»†æŒ‡ä»¤å®Œå…¨å¯¹é½çš„åœºæ™¯çš„æ¯”ä¾‹æœ€é«˜ä»…è¾¾10%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.03001v1">PDF</a> Work in Progress</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æŒ‡å‡ºï¼Œå°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªåŠ¨ç”Ÿæˆ3Dåœºæ™¯æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†ç”Ÿæˆçš„åœºæ™¯é€šå¸¸ç¼ºä¹çœŸå®ä¸–ç•Œçš„ç©ºé—´å¸ƒå±€å’Œç‰©ä½“å±æ€§ã€‚é—®é¢˜æ ¹æºåœ¨äºæŒ‡ä»¤ä¸å¤Ÿè¯¦ç»†å’Œç²¾ç»†ã€‚å› æ­¤ï¼Œéœ€è¦æ›´è¯¦ç»†åœ°æŒ‡å¯¼3Dåœºæ™¯åˆæˆï¼Œä»¥åæ˜ çœŸå®ä¸–ç•Œç¯å¢ƒã€‚åœ¨ä¸çœŸå®çš„ç¯å¢ƒä¸­è®­ç»ƒå®ä½“ä»£ç†ä¼šå¯¼è‡´å…¶å­¦ä¹ åˆ°çš„ä¼˜å…ˆäº‹é¡¹ä¸çœŸå®ä¸–ç•Œçš„ç‰©ç†å’Œè¯­ä¹‰å¤§ç›¸å¾„åº­ï¼Œä»è€Œå½±å“å…¶éƒ¨ç½²åçš„æ€§èƒ½ã€‚å› æ­¤ï¼ŒéªŒè¯ç²¾ç»†æŒ‡ä»¤ä¸ç”Ÿæˆåœºæ™¯ä¹‹é—´çš„å¯¹é½è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è¯„ä¼°æ–¹æ³•å¦‚CLIPScoreå’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æ— æ³•å¯é åœ°è¯„ä¼°è¿™ç§å¯¹é½ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡å¼•å…¥äº†LEGO-Evalè¯„ä¼°æ¡†æ¶å’ŒLEGO-BenchåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨æ›´å‡†ç¡®åœ°è¯„ä¼°åœºæ™¯ä¸æŒ‡ä»¤çš„å¯¹é½æƒ…å†µã€‚å®éªŒè¡¨æ˜ï¼ŒLEGO-Evalåœ¨è¯„ä¼°åœºæ™¯æŒ‡ä»¤å¯¹é½æ–¹é¢ä¼˜äºVLM-as-a-judgeï¼Œè€ŒLEGO-BenchåŸºå‡†æµ‹è¯•æ­ç¤ºäº†å½“å‰ç”Ÿæˆæ–¹æ³•çš„é‡è¦å±€é™æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsåœ¨ç”Ÿæˆ3Dåœºæ™¯æ—¶é¢ä¸´ç¼ºä¹çœŸå®ä¸–ç•Œç©ºé—´å¸ƒå±€å’Œç‰©ä½“å±æ€§çš„é—®é¢˜ã€‚</li>
<li>ç°æœ‰è¯„ä¼°æ–¹æ³•å¦‚CLIPScoreå’ŒVLMæ— æ³•å‡†ç¡®è¯„ä¼°åœºæ™¯ä¸æŒ‡ä»¤çš„å¯¹é½æƒ…å†µã€‚</li>
<li>ä¸ºè§£å†³ä¸Šè¿°é—®é¢˜ï¼Œå¼•å…¥äº†LEGO-Evalè¯„ä¼°æ¡†æ¶å’ŒLEGO-BenchåŸºå‡†æµ‹è¯•ã€‚</li>
<li>LEGO-Evalé€šè¿‡å¤šæ ·çš„å·¥å…·è®¾è®¡èƒ½æ›´å‡†ç¡®åœ°è¯„ä¼°åœºæ™¯ä¸æŒ‡ä»¤çš„å¯¹é½æƒ…å†µã€‚</li>
<li>å®éªŒè¡¨æ˜LEGO-Evalåœ¨è¯„ä¼°åœºæ™¯æŒ‡ä»¤å¯¹é½æ–¹é¢ä¼˜äºVLM-as-a-judgeã€‚</li>
<li>ç›®å‰ç”Ÿæˆæ–¹æ³•çš„æˆåŠŸç‡åœ¨å®Œå…¨å¯¹é½ç²¾ç»†æŒ‡ä»¤ç”Ÿæˆçš„åœºæ™¯ä¸­æœ€å¤šåªè¾¾åˆ°10%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.03001">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-12256274f43fd9d2e4a2afcbba6b0906" align="middle">
<img src="https://picx.zhimg.com/v2-68b234b89282661d7ab2bb708f342bb7" align="middle">
<img src="https://picx.zhimg.com/v2-edc892385aea1d32a9363bd75e93235d" align="middle">
<img src="https://picx.zhimg.com/v2-c082c043a2bd3821a57abe48fd4e2ce5" align="middle">
<img src="https://picx.zhimg.com/v2-7787b180f6e93504a2a7646a028c7c58" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Agent-Omni-Test-Time-Multimodal-Reasoning-via-Model-Coordination-for-Understanding-Anything"><a href="#Agent-Omni-Test-Time-Multimodal-Reasoning-via-Model-Coordination-for-Understanding-Anything" class="headerlink" title="Agent-Omni: Test-Time Multimodal Reasoning via Model Coordination for   Understanding Anything"></a>Agent-Omni: Test-Time Multimodal Reasoning via Model Coordination for   Understanding Anything</h2><p><strong>Authors:Huawei Lin, Yunzhi Shi, Tong Geng, Weijie Zhao, Wei Wang, Ravender Pal Singh</strong></p>
<p>Multimodal large language models (MLLMs) have shown strong capabilities but remain limited to fixed modality pairs and require costly fine-tuning with large aligned datasets. Building fully omni-capable models that can integrate text, images, audio, and video remains impractical and lacks robust reasoning support. In this paper, we propose an Agent-Omni framework that coordinates existing foundation models through a master-agent system, enabling flexible multimodal reasoning without retraining. The master agent interprets user intent, delegates subtasks to modality-specific agents, and integrates their outputs into coherent responses. Extensive experiments across text, image, audio, video, and omni benchmarks show that Agent-Omni consistently achieves state-of-the-art performance, particularly on tasks requiring complex cross-modal reasoning. Its agent-based design enables seamless integration of specialized foundation models, ensuring adaptability to diverse inputs while maintaining transparency and interpretability. In addition, the framework is modular and easily extensible, allowing future improvements as stronger models become available. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å·²ç»æ˜¾ç¤ºå‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†ä»ç„¶å±€é™äºå›ºå®šçš„æ¨¡æ€å¯¹ï¼Œéœ€è¦æ˜‚è´µçš„å¤§å‹å¯¹é½æ•°æ®é›†è¿›è¡Œå¾®è°ƒã€‚æ„å»ºèƒ½å¤Ÿæ•´åˆæ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘çš„å®Œå…¨é€šç”¨æ¨¡å‹ä»ç„¶ä¸åˆ‡å®é™…ï¼Œç¼ºä¹ç¨³å¥çš„æ¨ç†æ”¯æŒã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§Agent-Omniæ¡†æ¶ï¼Œå®ƒé€šè¿‡ä¸»ä»£ç†ç³»ç»Ÿåè°ƒç°æœ‰çš„åŸºç¡€æ¨¡å‹ï¼Œå®ç°äº†çµæ´»çš„å¤šæ¨¡æ€æ¨ç†è€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚ä¸»ä»£ç†è§£é‡Šç”¨æˆ·æ„å›¾ï¼Œå°†å­ä»»åŠ¡å§”æ‰˜ç»™ç‰¹å®šæ¨¡æ€çš„ä»£ç†ï¼Œå¹¶å°†ä»–ä»¬çš„è¾“å‡ºæ•´åˆä¸ºè¿è´¯çš„å“åº”ã€‚åœ¨æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ã€è§†é¢‘å’Œå…¨åŸºå‡†æµ‹è¯•çš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒAgent-OmniæŒç»­å®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦å¤æ‚è·¨æ¨¡æ€æ¨ç†çš„ä»»åŠ¡ä¸Šã€‚å…¶åŸºäºä»£ç†çš„è®¾è®¡èƒ½å¤Ÿæ— ç¼é›†æˆä¸“ä¸šçš„åŸºç¡€æ¨¡å‹ï¼Œç¡®ä¿é€‚åº”å„ç§è¾“å…¥ï¼ŒåŒæ—¶ä¿æŒé€æ˜åº¦å’Œå¯è§£é‡Šæ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶æ˜¯æ¨¡å—åŒ–çš„ï¼Œæ˜“äºæ‰©å±•ï¼Œå…è®¸éšç€æ›´å¼ºå¤§çš„æ¨¡å‹çš„å‡ºç°è€Œè¿›è¡Œæœªæ¥æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.02834v2">PDF</a> 16 pages, 7 figures, 14 tables. Under Review</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºAgent-Omniçš„æ¡†æ¶ï¼Œé€šè¿‡ä¸»ä»£ç†ç³»ç»Ÿåè°ƒç°æœ‰çš„åŸºç¡€æ¨¡å‹ï¼Œå®ç°äº†çµæ´»çš„å¤šæ¨¡æ€æ¨ç†è€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿè§£é‡Šç”¨æˆ·æ„å›¾ï¼Œå°†å­ä»»åŠ¡å§”æ´¾ç»™ç‰¹å®šæ¨¡æ€çš„ä»£ç†ï¼Œå¹¶å°†å®ƒä»¬çš„è¾“å‡ºæ•´åˆä¸ºè¿è´¯çš„å“åº”ã€‚å®éªŒè¡¨æ˜ï¼ŒAgent-Omniåœ¨æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ã€è§†é¢‘å’Œå…¨æ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­å§‹ç»ˆå®ç°æœ€æ–°æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦å¤æ‚è·¨æ¨¡æ€æ¨ç†çš„ä»»åŠ¡ä¸Šã€‚å…¶åŸºäºä»£ç†çš„è®¾è®¡å¯æ— ç¼é›†æˆä¸“ä¸šåŸºç¡€æ¨¡å‹ï¼Œç¡®ä¿é€‚åº”å„ç§è¾“å…¥å¹¶ä¿æŒé€æ˜åº¦å’Œå¯è§£é‡Šæ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶æ˜¯æ¨¡å—åŒ–çš„å¹¶ä¸”æ˜“äºæ‰©å±•ï¼Œå…è®¸éšç€æ›´å¼ºå¤§æ¨¡å‹çš„å¯ç”¨è€Œæ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Agent-Omniæ¡†æ¶é€šè¿‡ä¸»ä»£ç†ç³»ç»Ÿåè°ƒç°æœ‰åŸºç¡€æ¨¡å‹ï¼Œå®ç°çµæ´»å¤šæ¨¡æ€æ¨ç†ã€‚</li>
<li>è¯¥æ¡†æ¶èƒ½å¤Ÿè§£é‡Šç”¨æˆ·æ„å›¾ï¼Œå¹¶å°†å­ä»»åŠ¡å§”æ´¾ç»™ç‰¹å®šæ¨¡æ€çš„ä»£ç†ã€‚</li>
<li>Agent-Omniåœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸­å®ç°æœ€æ–°æ€§èƒ½ï¼Œå°¤å…¶åœ¨éœ€è¦å¤æ‚è·¨æ¨¡æ€æ¨ç†çš„ä»»åŠ¡ä¸Šã€‚</li>
<li>åŸºäºä»£ç†çš„è®¾è®¡ç¡®ä¿é€‚åº”å„ç§è¾“å…¥å¹¶ä¿æŒé€æ˜åº¦å’Œå¯è§£é‡Šæ€§ã€‚</li>
<li>Agent-Omniæ¡†æ¶æ˜¯æ¨¡å—åŒ–çš„ï¼Œæ˜“äºæ‰©å±•ï¼Œå…è®¸æœªæ¥éšç€æ›´å¼ºæ¨¡å‹çš„å¯ç”¨è€Œæ”¹è¿›ã€‚</li>
<li>è¯¥æ¡†æ¶çš„å‡ºç°è§£å†³äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å›ºå®šæ¨¡æ€å¯¹ä¸Šçš„å±€é™æ€§ï¼Œä»¥åŠéœ€è¦å¤§é‡ç²¾ç»†è°ƒæ•´å’Œå¯¹é½æ•°æ®é›†çš„é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.02834">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-900a5e772aa44b777c52ae9dd8e80690" align="middle">
<img src="https://picx.zhimg.com/v2-dce1ac02af7fca26ab12ff5f7408c985" align="middle">
<img src="https://picx.zhimg.com/v2-c3a01543767599167d24ba00c9a0bea9" align="middle">
<img src="https://picx.zhimg.com/v2-91a10ea385811e0f083f0a09ed7c88e0" align="middle">
<img src="https://picx.zhimg.com/v2-703127fa741a33a4a6ff8843b40b6669" align="middle">
<img src="https://picx.zhimg.com/v2-a7e39165e1bc5d8d30214d90cf847b80" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Scalable-Evaluation-and-Neural-Models-for-Compositional-Generalization"><a href="#Scalable-Evaluation-and-Neural-Models-for-Compositional-Generalization" class="headerlink" title="Scalable Evaluation and Neural Models for Compositional Generalization"></a>Scalable Evaluation and Neural Models for Compositional Generalization</h2><p><strong>Authors:Giacomo Camposampiero, Pietro Barbiero, Michael Hersche, Roger Wattenhofer, Abbas Rahimi</strong></p>
<p>Compositional generalization-a key open challenge in modern machine learning-requires models to predict unknown combinations of known concepts. However, assessing compositional generalization remains a fundamental challenge due to the lack of standardized evaluation protocols and the limitations of current benchmarks, which often favor efficiency over rigor. At the same time, general-purpose vision architectures lack the necessary inductive biases, and existing approaches to endow them compromise scalability. As a remedy, this paper introduces: 1) a rigorous evaluation framework that unifies and extends previous approaches while reducing computational requirements from combinatorial to constant; 2) an extensive and modern evaluation on the status of compositional generalization in supervised vision backbones, training more than 5000 models; 3) Attribute Invariant Networks, a class of models establishing a new Pareto frontier in compositional generalization, achieving a 23.43% accuracy improvement over baselines while reducing parameter overhead from 600% to 16% compared to fully disentangled counterparts. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/IBM/scalable-compositional-generalization">https://github.com/IBM/scalable-compositional-generalization</a>. </p>
<blockquote>
<p>ç»„åˆæ³›åŒ–æ˜¯ç°ä»£æœºå™¨å­¦ä¹ ä¸­çš„ä¸€ä¸ªå…³é”®å¼€æ”¾æŒ‘æˆ˜ï¼Œè¦æ±‚æ¨¡å‹èƒ½å¤Ÿé¢„æµ‹å·²çŸ¥æ¦‚å¿µæœªçŸ¥çš„ç»„åˆã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹æ ‡å‡†åŒ–çš„è¯„ä¼°åè®®å’Œå½“å‰åŸºå‡†æµ‹è¯•çš„å±€é™æ€§ï¼ˆé€šå¸¸æ•ˆç‡ä¼˜å…ˆäºä¸¥è°¨æ€§ï¼‰ï¼Œè¯„ä¼°ç»„åˆæ³›åŒ–ä»ç„¶æ˜¯ä¸€ä¸ªåŸºæœ¬æŒ‘æˆ˜ã€‚åŒæ—¶ï¼Œé€šç”¨è§†è§‰æ¶æ„ç¼ºä¹å¿…è¦çš„å½’çº³åè§ï¼Œç°æœ‰çš„èµ‹äºˆå®ƒä»¬çš„æ–¹æ³•ç‰ºç‰²äº†å¯æ‰©å±•æ€§ã€‚ä¸ºäº†è¡¥æ•‘è¿™ä¸€ç‚¹ï¼Œæœ¬æ–‡ä»‹ç»äº†ï¼š1ï¼‰ä¸€ä¸ªä¸¥è°¨çš„è¯„ä»·æ¡†æ¶ï¼Œå®ƒç»Ÿä¸€å¹¶æ‰©å±•äº†ä»¥å‰çš„æ–¹æ³•ï¼ŒåŒæ—¶å°†è®¡ç®—è¦æ±‚ä»ç»„åˆé™ä½åˆ°å¸¸æ•°ï¼›2ï¼‰å¯¹ç›‘ç£è§†è§‰ä¸»å¹²ä¸­ç»„åˆæ³›åŒ–ç°çŠ¶çš„å¹¿æ³›å’Œç°ä»£è¯„ä¼°ï¼Œè®­ç»ƒäº†è¶…è¿‡5000ä¸ªæ¨¡å‹ï¼›3ï¼‰å±æ€§ä¸å˜ç½‘ç»œï¼Œæ˜¯ä¸€ç±»æ¨¡å‹åœ¨ç»„åˆæ³›åŒ–æ–¹é¢å»ºç«‹äº†æ–°çš„å¸•ç´¯æ‰˜è¾¹ç•Œï¼Œåœ¨åŸºçº¿æµ‹è¯•ä¸Šå®ç°äº†23.43%çš„å‡†ç¡®ç‡æå‡ï¼ŒåŒæ—¶ä¸å®Œå…¨è§£è€¦çš„åŒç±»æ¨¡å‹ç›¸æ¯”ï¼Œå‚æ•°å¼€é”€ä»600%å‡å°‘åˆ°16%ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/IBM/scalable-compositional-generalization">https://github.com/IBM/scalable-compositional-generalization</a>å¤„è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.02667v2">PDF</a> Accepted at the Thirty-ninth Annual Conference on Neural Information   Processing Systems (NeurIPS), 2025</p>
<p><strong>Summary</strong></p>
<p>è¯¥è®ºæ–‡ä»‹ç»äº†å½“å‰æœºå™¨å­¦ä¹ é¢ä¸´çš„ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ï¼šç»„åˆæ³›åŒ–ã€‚æ–‡ç« æå‡ºè¯„ä¼°æ¨¡å‹é¢„æµ‹å·²çŸ¥æ¦‚å¿µç»„åˆçš„èƒ½åŠ›æ˜¯è‡³å…³é‡è¦çš„ã€‚ä¸ºäº†è§£å†³å½“å‰åœ¨ç»„åˆæ³›åŒ–è¯„ä¼°ä¸Šå­˜åœ¨çš„æ ‡å‡†åŒ–è¯„ä¼°åè®®ç¼ºå¤±åŠç°æœ‰è¯„ä¼°å·¥å…·çš„å±€é™æ€§ï¼Œè¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€å¹¶æ‰©å±•çš„ä¸¥è°¨è¯„ä»·æ¡†æ¶ï¼Œå¹¶åœ¨è®¡ç®—æœºè§†è§‰æ¨¡å‹ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜æå‡ºäº†ä¸€ç§æ–°çš„æ¨¡å‹æ¶æ„â€”â€”å±æ€§ä¸å˜ç½‘ç»œï¼Œå®ƒåœ¨ç»„åˆæ³›åŒ–æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆæ•ˆï¼Œå¹¶æ”¹å–„äº†å‚æ•°å¼€é”€é—®é¢˜ã€‚è¯¥ç ”ç©¶çš„ä»£ç å·²åœ¨GitHubä¸Šå…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç»„åˆæ³›åŒ–æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªé‡è¦æŒ‘æˆ˜ï¼Œéœ€è¦æ¨¡å‹é¢„æµ‹å·²çŸ¥æ¦‚å¿µçš„æœªçŸ¥ç»„åˆã€‚</li>
<li>ç¼ºä¹æ ‡å‡†åŒ–çš„è¯„ä¼°åè®®å’Œç°æœ‰çš„è¯„ä¼°å·¥å…·é™åˆ¶äº†ç»„åˆæ³›åŒ–çš„ç ”ç©¶è¿›ç¨‹ã€‚</li>
<li>ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªä¸¥è°¨çš„è¯„ä»·æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»Ÿä¸€å¹¶æ‰©å±•äº†ä¹‹å‰çš„è¯„ä¼°æ–¹æ³•ï¼Œå¹¶é™ä½äº†è®¡ç®—è¦æ±‚ã€‚</li>
<li>åœ¨è®¡ç®—æœºè§†è§‰æ¨¡å‹ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒæ¥è¯„ä¼°ç»„åˆæ³›åŒ–çš„ç°çŠ¶ã€‚</li>
<li>å±æ€§ä¸å˜ç½‘ç»œæ˜¯ä¸€ç§æ–°çš„æ¨¡å‹æ¶æ„ï¼Œå®ƒåœ¨ç»„åˆæ³›åŒ–æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—çš„æ•ˆæœã€‚</li>
<li>å±æ€§ä¸å˜ç½‘ç»œç›¸æ¯”åŸºå‡†æ¨¡å‹æœ‰é«˜è¾¾23.43%çš„å‡†ç¡®æ€§æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.02667">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-18496e4c1ecd9a2b1779c0e12918a90c" align="middle">
<img src="https://picx.zhimg.com/v2-2fb3cc7a632cd7ec41838b33bb0b5134" align="middle">
<img src="https://picx.zhimg.com/v2-66481f21f56e8e87522f9169129fc02d" align="middle">
<img src="https://picx.zhimg.com/v2-c316e275370e304e39111b45aed8d316" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="TabDSR-Decompose-Sanitize-and-Reason-for-Complex-Numerical-Reasoning-in-Tabular-Data"><a href="#TabDSR-Decompose-Sanitize-and-Reason-for-Complex-Numerical-Reasoning-in-Tabular-Data" class="headerlink" title="TabDSR: Decompose, Sanitize, and Reason for Complex Numerical Reasoning   in Tabular Data"></a>TabDSR: Decompose, Sanitize, and Reason for Complex Numerical Reasoning   in Tabular Data</h2><p><strong>Authors:Changjiang Jiang, Fengchang Yu, Haihua Chen, Wei Lu, Jin Zeng</strong></p>
<p>Complex reasoning over tabular data is crucial in real-world data analysis, yet large language models (LLMs) often underperform due to complex queries, noisy data, and limited numerical capabilities. To address these issues, we propose TabDSR, a framework consisting of: (1) a query decomposer that breaks down complex questions, (2) a table sanitizer that cleans and filters noisy tables, and (3) a program-of-thoughts (PoT)-based reasoner that generates executable code to derive the final answer from the sanitized table. To ensure unbiased evaluation and mitigate data leakage, we introduce a new dataset, CalTab151, specifically designed for complex numerical reasoning over tables. Experimental results demonstrate that TabDSR consistently outperforms existing methods, achieving state-of-the-art (SOTA) performance with 8.79%, 6.08%, and 19.87% accuracy improvement on TAT-QA, TableBench, and TabDSR, respectively. Moreover, our framework integrates seamlessly with mainstream LLMs, providing a robust solution for complex tabular numerical reasoning. These findings highlight the effectiveness of our framework in enhancing LLM performance for complex tabular numerical reasoning. Data and code are available upon request. </p>
<blockquote>
<p>è¡¨æ ¼æ•°æ®çš„å¤æ‚æ¨ç†åœ¨çœŸå®ä¸–ç•Œæ•°æ®åˆ†æä¸­è‡³å…³é‡è¦ï¼Œç„¶è€Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”±äºå¤æ‚çš„æŸ¥è¯¢ã€å˜ˆæ‚çš„æ•°æ®å’Œæœ‰é™çš„æ•°å€¼èƒ½åŠ›è€Œå¸¸å¸¸è¡¨ç°ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†TabDSRæ¡†æ¶ï¼Œå®ƒåŒ…æ‹¬ï¼šï¼ˆ1ï¼‰æŸ¥è¯¢åˆ†è§£å™¨ï¼Œç”¨äºåˆ†è§£å¤æ‚é—®é¢˜ï¼›ï¼ˆ2ï¼‰è¡¨æ ¼å‡€åŒ–å™¨ï¼Œç”¨äºæ¸…æ´å’Œè¿‡æ»¤å˜ˆæ‚çš„è¡¨æ ¼ï¼›ï¼ˆ3ï¼‰åŸºäºæ€ç»´ç¨‹åºï¼ˆPoTï¼‰çš„æ¨ç†å™¨ï¼Œç”Ÿæˆå¯æ‰§è¡Œä»£ç ï¼Œä»å‡€åŒ–åçš„è¡¨æ ¼ä¸­å¾—å‡ºæœ€ç»ˆç­”æ¡ˆã€‚ä¸ºäº†ç¡®ä¿å…¬å¹³è¯„ä¼°å’Œå‡è½»æ•°æ®æ³„éœ²ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°æ•°æ®é›†CalTab151ï¼Œè¯¥æ•°æ®é›†ä¸“ä¸ºè¡¨æ ¼ä¸Šçš„å¤æ‚æ•°å€¼æ¨ç†è®¾è®¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTabDSRå§‹ç»ˆä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨TAT-QAã€TableBenchå’ŒTabDSRä¸Šåˆ†åˆ«å®ç°äº†å…ˆè¿›æ€§èƒ½ï¼Œå‡†ç¡®ç‡æé«˜äº†8.79%ã€6.08%å’Œ19.87%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ¡†æ¶ä¸ä¸»æµLLMsæ— ç¼é›†æˆï¼Œä¸ºå¤æ‚è¡¨æ ¼æ•°å€¼æ¨ç†æä¾›äº†ç¨³å¥çš„è§£å†³æ–¹æ¡ˆã€‚è¿™äº›å‘ç°å‡¸æ˜¾äº†æˆ‘ä»¬çš„æ¡†æ¶åœ¨å¢å¼ºLLMè¿›è¡Œå¤æ‚è¡¨æ ¼æ•°å€¼æ¨ç†æ€§èƒ½æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æ•°æ®å’Œä»£ç å¯æ ¹æ®è¦æ±‚æä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.02219v2">PDF</a> Accepted to EMNLP 2025 Findings</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æå‡ºä¸€ä¸ªåä¸ºTabDSRçš„æ¡†æ¶ï¼Œç”¨äºè§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†è¡¨æ ¼æ•°æ®æ—¶çš„å¤æ‚æ¨ç†éš¾é¢˜ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬æŸ¥è¯¢åˆ†è§£å™¨ã€è¡¨æ ¼æ¸…ç†å™¨å’ŒåŸºäºç¨‹åºæ€ç»´ï¼ˆPoTï¼‰çš„æ¨ç†å™¨ã€‚æ¡†æ¶å¼•å…¥äº†CalTab151æ•°æ®é›†ä»¥ç¡®ä¿å…¬æ­£è¯„ä¼°å’Œå‡å°‘æ•°æ®æ³„éœ²é—®é¢˜ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTabDSRæ¡†æ¶åœ¨TAT-QAã€TableBenchå’ŒTabDSRä¸Šçš„å‡†ç¡®ç‡åˆ†åˆ«æé«˜äº†8.79%ã€6.08%å’Œ19.87%ï¼Œå±•ç°å‡ºå“è¶Šæ€§èƒ½ã€‚è¯¥æ¡†æ¶å¯æ— ç¼é›†æˆä¸»æµå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä¸ºè§£å†³å¤æ‚è¡¨æ ¼æ•°å€¼æ¨ç†é—®é¢˜æä¾›äº†ç¨³å¥è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†è¡¨æ ¼æ•°æ®æ—¶çš„å¤æ‚æ¨ç†å­˜åœ¨æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å¤æ‚æŸ¥è¯¢ã€å™ªå£°æ•°æ®å’Œæœ‰é™æ•°å€¼å¤„ç†èƒ½åŠ›ã€‚</li>
<li>TabDSRæ¡†æ¶åŒ…æ‹¬æŸ¥è¯¢åˆ†è§£å™¨ã€è¡¨æ ¼æ¸…ç†å™¨å’ŒåŸºäºç¨‹åºæ€ç»´çš„æ¨ç†å™¨ï¼Œä»¥åº”å¯¹è¿™äº›æŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥CalTab151æ•°æ®é›†ï¼Œä¸“ä¸ºå¤æ‚æ•°å€¼è¡¨æ ¼æ¨ç†è®¾è®¡ï¼Œç¡®ä¿å…¬æ­£è¯„ä¼°å¹¶å‡å°‘æ•°æ®æ³„éœ²ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºTabDSRæ¡†æ¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¾ƒç°æœ‰æ–¹æ³•æœ‰æ˜æ˜¾æå‡ã€‚</li>
<li>TabDSRæ¡†æ¶å¯æ— ç¼é›†æˆä¸»æµå¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>TabDSRæ¡†æ¶æé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹å¤„ç†å¤æ‚è¡¨æ ¼æ•°å€¼æ¨ç†çš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.02219">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a4b948957e272f08c2372f07b808355a" align="middle">
<img src="https://picx.zhimg.com/v2-44166456826338049ec8b53758f0ceb0" align="middle">
<img src="https://picx.zhimg.com/v2-1698f4edd342681a39ed6832cd7459e4" align="middle">
<img src="https://picx.zhimg.com/v2-3654fb58861752fa42a3b55ed0a0dbf3" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Actial-Activate-Spatial-Reasoning-Ability-of-Multimodal-Large-Language-Models"><a href="#Actial-Activate-Spatial-Reasoning-Ability-of-Multimodal-Large-Language-Models" class="headerlink" title="Actial: Activate Spatial Reasoning Ability of Multimodal Large Language   Models"></a>Actial: Activate Spatial Reasoning Ability of Multimodal Large Language   Models</h2><p><strong>Authors:Xiaoyu Zhan, Wenxuan Huang, Hao Sun, Xinyu Fu, Changfeng Ma, Shaosheng Cao, Bohan Jia, Shaohui Lin, Zhenfei Yin, Lei Bai, Wanli Ouyang, Yuanqi Li, Jie Guo, Yanwen Guo</strong></p>
<p>Recent advances in Multimodal Large Language Models (MLLMs) have significantly improved 2D visual understanding, prompting interest in their application to complex 3D reasoning tasks. However, it remains unclear whether these models can effectively capture the detailed spatial information required for robust real-world performance, especially cross-view consistency, a key requirement for accurate 3D reasoning. Considering this issue, we introduce Viewpoint Learning, a task designed to evaluate and improve the spatial reasoning capabilities of MLLMs. We present the Viewpoint-100K dataset, consisting of 100K object-centric image pairs with diverse viewpoints and corresponding question-answer pairs. Our approach employs a two-stage fine-tuning strategy: first, foundational knowledge is injected to the baseline MLLM via Supervised Fine-Tuning (SFT) on Viewpoint-100K, resulting in significant improvements across multiple tasks; second, generalization is enhanced through Reinforcement Learning using the Group Relative Policy Optimization (GRPO) algorithm on a broader set of questions. Additionally, we introduce a hybrid cold-start initialization method designed to simultaneously learn viewpoint representations and maintain coherent reasoning thinking. Experimental results show that our approach significantly activates the spatial reasoning ability of MLLM, improving performance on both in-domain and out-of-domain reasoning tasks. Our findings highlight the value of developing foundational spatial skills in MLLMs, supporting future progress in robotics, autonomous systems, and 3D scene understanding. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è¿›å±•åœ¨äºŒç»´è§†è§‰ç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æå‡ï¼Œä¿ƒä½¿äººä»¬å¯¹å…¶åº”ç”¨äºå¤æ‚ä¸‰ç»´æ¨ç†ä»»åŠ¡çš„å…´è¶£ã€‚ç„¶è€Œï¼Œå°šä¸æ¸…æ¥šè¿™äº›æ¨¡å‹æ˜¯å¦èƒ½æœ‰æ•ˆåœ°æ•è·ç”¨äºç¨³å¥ç°å®ä¸–ç•Œæ€§èƒ½æ‰€éœ€çš„è¯¦ç»†ç©ºé—´ä¿¡æ¯ï¼Œç‰¹åˆ«æ˜¯è·¨è§†å›¾ä¸€è‡´æ€§ï¼Œè¿™æ˜¯å‡†ç¡®ä¸‰ç»´æ¨ç†çš„å…³é”®è¦æ±‚ã€‚è€ƒè™‘åˆ°è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†è§†è§’å­¦ä¹ ï¼ˆViewpoint Learningï¼‰ä»»åŠ¡ï¼Œæ—¨åœ¨è¯„ä¼°å’Œæ”¹è¿›MLLMsçš„ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬æ¨å‡ºäº†è§†è§’æ•°æ®é›†Viewpoint-100Kï¼Œå…¶ä¸­åŒ…å«ä»¥ä¸åŒè§†è§’ä¸ºä¸­å¿ƒçš„å¯¹è±¡å›¾åƒå¯¹ä»¥åŠç›¸åº”çš„é—®é¢˜ç­”æ¡ˆå¯¹ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨ä¸¤é˜¶æ®µå¾®è°ƒç­–ç•¥ï¼šé¦–å…ˆé€šè¿‡Viewpoint-100Kçš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å‘åŸºçº¿MLLMæ³¨å…¥åŸºç¡€çŸ¥è¯†ï¼Œè¿™åœ¨å¤šä¸ªä»»åŠ¡ä¸Šå®ç°äº†æ˜¾è‘—çš„æ”¹è¿›ï¼›å…¶æ¬¡ï¼Œé€šè¿‡åœ¨æ›´å¹¿æ³›çš„é—®é¢˜é›†ä¸Šä½¿ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç®—æ³•è¿›è¡Œå¼ºåŒ–å­¦ä¹ æ¥å¢å¼ºæ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§æ··åˆçš„å†·å¯åŠ¨åˆå§‹åŒ–æ–¹æ³•ï¼Œæ—¨åœ¨åŒæ—¶å­¦ä¹ è§†è§’è¡¨ç¤ºå¹¶ä¿æŒè¿è´¯çš„æ¨ç†æ€ç»´ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—æ¿€æ´»äº†MLLMçš„ç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œæé«˜äº†é¢†åŸŸå†…å¤–æ¨ç†ä»»åŠ¡çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¼ºè°ƒäº†åœ¨å¤§è§„æ¨¡æ¨¡å‹ä¸­åŸ¹å…»åŸºç¡€ç©ºé—´æŠ€èƒ½çš„ä»·å€¼ï¼Œæ”¯æŒæœªæ¥åœ¨æœºå™¨äººæŠ€æœ¯ã€è‡ªä¸»ç³»ç»Ÿå’Œä¸‰ç»´åœºæ™¯ç†è§£æ–¹é¢çš„è¿›æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.01618v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ€è¿‘çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è¿›æ­¥æ˜¾è‘—ï¼Œæå‡äº†å¯¹2Dè§†è§‰çš„ç†è§£ï¼Œå¹¶å¼•å‘äº†å°†å…¶åº”ç”¨äºå¤æ‚3Dæ¨ç†ä»»åŠ¡çš„å…´è¶£ã€‚ç„¶è€Œï¼Œå°šä¸æ¸…æ¥šè¿™äº›æ¨¡å‹æ˜¯å¦èƒ½æœ‰æ•ˆæ•æ‰ç”¨äºç¨³å¥ç°å®è¡¨ç°çš„è¯¦ç»†ç©ºé—´ä¿¡æ¯ï¼Œç‰¹åˆ«æ˜¯è·¨è§†å›¾ä¸€è‡´æ€§ï¼Œè¿™æ˜¯å‡†ç¡®3Dæ¨ç†çš„å…³é”®è¦æ±‚ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†è§†ç‚¹å­¦ä¹ ï¼ˆViewpoint Learningï¼‰ä»»åŠ¡æ¥è¯„ä¼°å’Œæ”¹è¿›MLLMsçš„ç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œå¹¶ä»‹ç»äº†Viewpoint-100Kæ•°æ®é›†ã€‚é€šè¿‡ä¸¤é˜¶æ®µå¾®è°ƒç­–ç•¥ï¼Œé¦–å…ˆåœ¨Viewpoint-100Kæ•°æ®é›†ä¸Šé€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ³¨å…¥åŸºç¡€çŸ¥è¯†åˆ°åŸºçº¿MLLMä¸­ï¼Œç„¶ååœ¨æ›´å¹¿æ³›çš„é—®é¢˜é›†ä¸Šé€šè¿‡å¼ºåŒ–å­¦ä¹ ä½¿ç”¨é›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç®—æ³•å¢å¼ºæ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä»‹ç»äº†ä¸€ç§æ··åˆå†·å¯åŠ¨åˆå§‹åŒ–æ–¹æ³•ï¼Œæ—¨åœ¨åŒæ—¶å­¦ä¹ è§†ç‚¹è¡¨ç¤ºå¹¶ä¿æŒè¿è´¯çš„æ¨ç†æ€ç»´ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—æ¿€æ´»äº†MLLMçš„ç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œæé«˜äº†åŸŸå†…å’ŒåŸŸå¤–æ¨ç†ä»»åŠ¡çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLMsåœ¨2Dè§†è§‰ç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œæ¨åŠ¨äº†åœ¨å¤æ‚3Dæ¨ç†ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚</li>
<li>å½“å‰æ¨¡å‹åœ¨æ•æ‰è¯¦ç»†ç©ºé—´ä¿¡æ¯ï¼Œå°¤å…¶æ˜¯è·¨è§†å›¾ä¸€è‡´æ€§æ–¹é¢ï¼Œä»é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥äº†è§†ç‚¹å­¦ä¹ ï¼ˆViewpoint Learningï¼‰ä»»åŠ¡æ¥è¯„ä¼°å’Œæ”¹è¿›MLLMsçš„ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚</li>
<li>Viewpoint-100Kæ•°æ®é›†åŒ…å«10ä¸‡ä¸ªä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„å›¾åƒå¯¹å’Œç›¸åº”çš„é—®é¢˜ç­”æ¡ˆå¯¹ï¼Œç”¨äºæ”¯æŒè§†ç‚¹å­¦ä¹ ä»»åŠ¡ã€‚</li>
<li>é‡‡ç”¨ä¸¤é˜¶æ®µå¾®è°ƒç­–ç•¥ï¼Œé€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œé›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç®—æ³•å¼ºåŒ–æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>å¼•å…¥æ··åˆå†·å¯åŠ¨åˆå§‹åŒ–æ–¹æ³•ï¼ŒåŒæ—¶å­¦ä¹ è§†ç‚¹è¡¨ç¤ºå¹¶ä¿æŒè¿è´¯æ¨ç†æ€ç»´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.01618">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a028633ac439aa22ec16d7d43a451b66" align="middle">
<img src="https://picx.zhimg.com/v2-5911ce09d4c2ef542c774f903bca06e6" align="middle">
<img src="https://picx.zhimg.com/v2-df92b29aa6ebc5b7c8a64230b3dc7470" align="middle">
<img src="https://picx.zhimg.com/v2-d35480f70df56395b339fe5e1016268d" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Learning-to-Seek-Evidence-A-Verifiable-Reasoning-Agent-with-Causal-Faithfulness-Analysis"><a href="#Learning-to-Seek-Evidence-A-Verifiable-Reasoning-Agent-with-Causal-Faithfulness-Analysis" class="headerlink" title="Learning to Seek Evidence: A Verifiable Reasoning Agent with Causal   Faithfulness Analysis"></a>Learning to Seek Evidence: A Verifiable Reasoning Agent with Causal   Faithfulness Analysis</h2><p><strong>Authors:Yuhang Huang, Zekai Lin, Fan Zhong, Lei Liu</strong></p>
<p>Explanations for AI models in high-stakes domains like medicine often lack verifiability, which can hinder trust. To address this, we propose an interactive agent that produces explanations through an auditable sequence of actions. The agent learns a policy to strategically seek external visual evidence to support its diagnostic reasoning. This policy is optimized using reinforcement learning, resulting in a model that is both efficient and generalizable. Our experiments show that this action-based reasoning process significantly improves calibrated accuracy, reducing the Brier score by 18% compared to a non-interactive baseline. To validate the faithfulness of the agentâ€™s explanations, we introduce a causal intervention method. By masking the visual evidence the agent chooses to use, we observe a measurable degradation in its performance ($\Delta$Brier&#x3D;+0.029), confirming that the evidence is integral to its decision-making process. Our work provides a practical framework for building AI systems with verifiable and faithful reasoning capabilities. </p>
<blockquote>
<p>åœ¨åƒåŒ»å­¦è¿™æ ·é«˜é£é™©é¢†åŸŸä¸­ï¼ŒAIæ¨¡å‹çš„è§£é‡Šé€šå¸¸ç¼ºä¹å¯éªŒè¯æ€§ï¼Œè¿™å¯èƒ½ä¼šé˜»ç¢ä¿¡ä»»ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šè¿‡å¯å®¡æ ¸çš„è¡ŒåŠ¨åºåˆ—äº§ç”Ÿè§£é‡Šçš„äº¤äº’ä»£ç†ã€‚è¯¥ä»£ç†å­¦ä¹ ä¸€ç§ç­–ç•¥ï¼Œä»¥æˆ˜ç•¥æ€§åœ°å¯»æ±‚å¤–éƒ¨è§†è§‰è¯æ®æ¥æ”¯æŒå…¶è¯Šæ–­æ¨ç†ã€‚è¯¥ç­–ç•¥é€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›è¡Œä¼˜åŒ–ï¼Œä»è€Œå¾—åˆ°ä¸€ä¸ªæ—¢é«˜æ•ˆåˆé€šç”¨çš„æ¨¡å‹ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œè¿™ç§åŸºäºè¡ŒåŠ¨çš„ç†ç”±è¿‡ç¨‹æ˜¾è‘—æé«˜äº†æ ¡å‡†ç²¾åº¦ï¼Œä¸éäº¤äº’åŸºçº¿ç›¸æ¯”ï¼ŒBrierå¾—åˆ†é™ä½äº†18%ã€‚ä¸ºäº†éªŒè¯ä»£ç†è§£é‡Šçš„çœŸå®æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å› æœå¹²é¢„æ–¹æ³•ã€‚é€šè¿‡æ©ç›–ä»£ç†é€‰æ‹©çš„è§†è§‰è¯æ®ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°å…¶æ€§èƒ½å‡ºç°äº†å¯è¡¡é‡çš„ä¸‹é™ï¼ˆÎ”Brier&#x3D;+0.029ï¼‰ï¼Œè¿™è¯å®äº†è¯æ®æ˜¯å…¶å†³ç­–è¿‡ç¨‹ä¸­çš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚æˆ‘ä»¬çš„å·¥ä½œæä¾›äº†ä¸€ä¸ªå®ç”¨çš„æ¡†æ¶ï¼Œç”¨äºæ„å»ºå…·æœ‰å¯éªŒè¯å’Œå¿ è¯šæ¨ç†èƒ½åŠ›çš„AIç³»ç»Ÿã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.01425v1">PDF</a> 12 pages, 3 figures. Under review at the Conference on Computer   Vision and Pattern Recognition (CVPR) 2026</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åœ¨åŒ»å­¦ç­‰é«˜é£é™©é¢†åŸŸä¸­ï¼ŒAIæ¨¡å‹è§£é‡Šç¼ºä¹å¯éªŒè¯æ€§çš„é—®é¢˜ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿè®¾è®¡äº†ä¸€ç§äº¤äº’å¼æ™ºèƒ½ä½“ï¼Œå®ƒé€šè¿‡ä¸€ç³»åˆ—å¯å®¡æ ¸çš„è¡ŒåŠ¨äº§ç”Ÿè§£é‡Šã€‚æ™ºèƒ½ä½“å­¦ä¹ ä¸€ç§ç­–ç•¥æ€§å¯»æ‰¾å¤–éƒ¨è§†è§‰è¯æ®ä»¥æ”¯æŒå…¶è¯Šæ–­æ¨ç†çš„ç­–ç•¥ï¼Œè¯¥ç­–ç•¥é€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›è¡Œä¼˜åŒ–ï¼Œä½¿å¾—æ¨¡å‹æ—¢é«˜æ•ˆåˆå…·æœ‰æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒåŸºäºè¡ŒåŠ¨æ¨ç†è¿‡ç¨‹æ˜¾è‘—æé«˜äº†æ ¡å‡†ç²¾åº¦ï¼Œä¸éäº¤äº’å¼åŸºå‡†ç›¸æ¯”ï¼ŒBrierå¾—åˆ†é™ä½äº†18%ã€‚ä¸ºéªŒè¯æ™ºèƒ½ä½“è§£é‡Šçš„å¯ä¿¡æ€§ï¼Œç ”ç©¶å›¢é˜Ÿå¼•å…¥äº†ä¸€ç§å› æœå¹²é¢„æ–¹æ³•ã€‚é€šè¿‡å±è”½æ™ºèƒ½ä½“é€‰æ‹©çš„è§†è§‰è¯æ®ï¼Œè§‚å¯Ÿåˆ°å…¶æ€§èƒ½çš„å¯è¡¡é‡ä¸‹é™ï¼ˆÎ”Brier&#x3D;+0.029ï¼‰ï¼Œè¯å®äº†è¯æ®å¯¹å…¶å†³ç­–è¿‡ç¨‹çš„é‡è¦æ€§ã€‚æœ¬ç ”ç©¶ä¸ºæ„å»ºå…·æœ‰å¯éªŒè¯å’Œå¯é æ¨ç†èƒ½åŠ›çš„AIç³»ç»Ÿæä¾›äº†å®ç”¨æ¡†æ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AIæ¨¡å‹åœ¨é«˜é£é™©é¢†åŸŸï¼ˆå¦‚åŒ»å­¦ï¼‰ä¸­çš„è§£é‡Šç¼ºä¹å¯éªŒè¯æ€§ï¼Œå¯èƒ½å½±å“ä¿¡ä»»ã€‚</li>
<li>äº¤äº’å¼æ™ºèƒ½ä½“é€šè¿‡ä¸€ç³»åˆ—å¯å®¡æ ¸çš„è¡ŒåŠ¨äº§ç”Ÿè§£é‡Šã€‚</li>
<li>æ™ºèƒ½ä½“å­¦ä¹ ç­–ç•¥æ€§å¯»æ‰¾å¤–éƒ¨è§†è§‰è¯æ®ä»¥æ”¯æŒå…¶è¯Šæ–­æ¨ç†ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ ç”¨äºä¼˜åŒ–æ™ºèƒ½ä½“çš„ç­–ç•¥ï¼Œä½¿å…¶æ—¢é«˜æ•ˆåˆå…·æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>åŸºäºè¡ŒåŠ¨æ¨ç†è¿‡ç¨‹æ˜¾è‘—æé«˜æ ¡å‡†ç²¾åº¦ï¼ŒBrierå¾—åˆ†é™ä½18%ã€‚</li>
<li>å¼•å…¥å› æœå¹²é¢„æ–¹æ³•æ¥éªŒè¯æ™ºèƒ½ä½“è§£é‡Šçš„å¯ä¿¡æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.01425">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7ea1d937ae06cb7c0a38ffb60ff562a7" align="middle">
<img src="https://picx.zhimg.com/v2-8fc58cc325b7c81440920fcfb3cbe1db" align="middle">
<img src="https://picx.zhimg.com/v2-fa492f60fc005a30d4bff0a6f6aa64dd" align="middle">
<img src="https://picx.zhimg.com/v2-8e72cc24c8e286580d3bbabb45e922b3" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Lares-LLM-driven-Code-Slice-Semantic-Search-for-Patch-Presence-Testing"><a href="#Lares-LLM-driven-Code-Slice-Semantic-Search-for-Patch-Presence-Testing" class="headerlink" title="Lares: LLM-driven Code Slice Semantic Search for Patch Presence Testing"></a>Lares: LLM-driven Code Slice Semantic Search for Patch Presence Testing</h2><p><strong>Authors:Siyuan Li, Yaowen Zheng, Hong Li, Jingdong Guo, Chaopeng Dong, Chunpeng Yan, Weijie Wang, Yimo Ren, Limin Sun, Hongsong Zhu</strong></p>
<p>In modern software ecosystems, 1-day vulnerabilities pose significant security risks due to extensive code reuse. Identifying vulnerable functions in target binaries alone is insufficient; it is also crucial to determine whether these functions have been patched. Existing methods, however, suffer from limited usability and accuracy. They often depend on the compilation process to extract features, requiring substantial manual effort and failing for certain software. Moreover, they cannot reliably differentiate between code changes caused by patches or compilation variations. To overcome these limitations, we propose Lares, a scalable and accurate method for patch presence testing. Lares introduces Code Slice Semantic Search, which directly extracts features from the patch source code and identifies semantically equivalent code slices in the pseudocode of the target binary. By eliminating the need for the compilation process, Lares improves usability, while leveraging large language models (LLMs) for code analysis and SMT solvers for logical reasoning to enhance accuracy. Experimental results show that Lares achieves superior precision, recall, and usability. Furthermore, it is the first work to evaluate patch presence testing across optimization levels, architectures, and compilers. The datasets and source code used in this article are available at <a target="_blank" rel="noopener" href="https://github.com/Siyuan-Li201/Lares">https://github.com/Siyuan-Li201/Lares</a>. </p>
<blockquote>
<p>åœ¨ç°ä»£è½¯ä»¶ç”Ÿæ€ç³»ç»Ÿä¸­ï¼Œç”±äºå¤§é‡ä»£ç çš„é‡å¤ä½¿ç”¨ï¼Œä¸ºæœŸä¸€å¤©çš„æ¼æ´æ„æˆäº†é‡å¤§å®‰å…¨é£é™©ã€‚ä»…è¯†åˆ«ç›®æ ‡äºŒè¿›åˆ¶æ–‡ä»¶ä¸­çš„è„†å¼±åŠŸèƒ½æ˜¯ä¸å¤Ÿçš„ï¼›ç¡®å®šè¿™äº›åŠŸèƒ½æ˜¯å¦å·²è¢«ä¿®å¤ä¹Ÿæ˜¯è‡³å…³é‡è¦çš„ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å­˜åœ¨å¯ç”¨æ€§å’Œå‡†ç¡®æ€§æ–¹é¢çš„å±€é™æ€§ã€‚å®ƒä»¬é€šå¸¸ä¾èµ–äºç¼–è¯‘è¿‡ç¨‹æ¥æå–ç‰¹å¾ï¼Œéœ€è¦å¤§é‡çš„äººå·¥åŠªåŠ›ï¼Œå¹¶ä¸”å¯¹æŸäº›è½¯ä»¶ä¸èµ·ä½œç”¨ã€‚è€Œä¸”ï¼Œå®ƒä»¬æ— æ³•å¯é åœ°åŒºåˆ†ç”±è¡¥ä¸æˆ–ç¼–è¯‘å˜åŒ–å¼•èµ·çš„ä»£ç æ›´æ”¹ã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†Laresï¼Œè¿™æ˜¯ä¸€ç§å¯æ‰©å±•ä¸”å‡†ç¡®çš„è¡¥ä¸å­˜åœ¨æµ‹è¯•æ–¹æ³•ã€‚Lareså¼•å…¥äº†ä»£ç åˆ‡ç‰‡è¯­ä¹‰æœç´¢ï¼Œå®ƒç›´æ¥ä»è¡¥ä¸æºä»£ç ä¸­æå–ç‰¹å¾ï¼Œå¹¶è¯†åˆ«ç›®æ ‡äºŒè¿›åˆ¶æ–‡ä»¶ä¼ªä»£ç ä¸­çš„è¯­ä¹‰ç­‰æ•ˆä»£ç åˆ‡ç‰‡ã€‚é€šè¿‡æ¶ˆé™¤å¯¹ç¼–è¯‘è¿‡ç¨‹çš„éœ€æ±‚ï¼ŒLaresæé«˜äº†å¯ç”¨æ€§ï¼ŒåŒæ—¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œä»£ç åˆ†æå’ŒSMTæ±‚è§£å™¨è¿›è¡Œé€»è¾‘æ¨ç†ä»¥æé«˜å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLaresåœ¨ç²¾åº¦ã€å¬å›ç‡å’Œå¯ç”¨æ€§æ–¹é¢å‡è¡¨ç°ä¼˜è¶Šã€‚æ­¤å¤–ï¼Œå®ƒæ˜¯é¦–é¡¹åœ¨ä¼˜åŒ–çº§åˆ«ã€æ¶æ„å’Œç¼–è¯‘å™¨æ–¹é¢è¯„ä¼°è¡¥ä¸å­˜åœ¨æµ‹è¯•çš„å·¥ä½œã€‚æœ¬æ–‡ä½¿ç”¨çš„æ•°æ®é›†å’Œæºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Siyuan-Li201/Lares%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Siyuan-Li201/Laresè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.01252v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä»‹ç»äº†åœ¨ç°ä»£è½¯ä»¶ç”Ÿæ€ç³»ç»Ÿä¸­ï¼Œä¸€æ—¥æ¼æ´ä¿®å¤çš„é‡è¦æ€§åŠå…¶å¸¦æ¥çš„å®‰å…¨é£é™©ã€‚ç°æœ‰çš„æ–¹æ³•å­˜åœ¨å±€é™æ€§ï¼Œéœ€è¦ç¼–è¯‘è¿‡ç¨‹æå–ç‰¹å¾ä¸”éš¾ä»¥åŒºåˆ†è¡¥ä¸å¼•èµ·çš„ä»£ç å˜åŒ–å’Œç¼–è¯‘å¼•èµ·çš„å˜åŒ–ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºåä¸ºLaresçš„å¯é æµ‹è¯•è¡¥ä¸å­˜åœ¨æ€§çš„æ–¹æ³•ã€‚å®ƒç›´æ¥æå–è¡¥ä¸æºä»£ç çš„ç‰¹å¾ï¼Œå¹¶åœ¨ç›®æ ‡äºŒè¿›åˆ¶æ–‡ä»¶çš„ä¼ªä»£ç ä¸­å¯»æ‰¾è¯­ä¹‰ç­‰æ•ˆçš„ä»£ç ç‰‡æ®µã€‚è¯¥æ–¹æ³•æ— éœ€ç¼–è¯‘è¿‡ç¨‹ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å’ŒSMTæ±‚è§£å™¨æé«˜å‡†ç¡®æ€§å’Œå¯ç”¨æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLaresåœ¨ç²¾åº¦ã€å¬å›ç‡å’Œå¯ç”¨æ€§æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œå¹¶é¦–æ¬¡å¯¹ä¼˜åŒ–çº§åˆ«ã€æ¶æ„å’Œç¼–è¯‘å™¨è¿›è¡Œäº†è¡¥ä¸å­˜åœ¨æµ‹è¯•è¯„ä¼°ã€‚æ•°æ®é›†å’Œæºä»£ç å¯åœ¨æŒ‡å®šé“¾æ¥æ‰¾åˆ°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Laresæ˜¯ä¸€ç§ç”¨äºæµ‹è¯•è¡¥ä¸å­˜åœ¨æ€§çš„å¯æ‰©å±•ä¸”å‡†ç¡®çš„æ–¹æ³•ã€‚</li>
<li>å®ƒç›´æ¥æå–è¡¥ä¸æºä»£ç çš„ç‰¹å¾ï¼Œæé«˜äº†æµ‹è¯•çš„å¯é æ€§å’Œå‡†ç¡®æ€§ã€‚</li>
<li>Laresåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å’ŒSMTæ±‚è§£å™¨è¿›è¡Œä»£ç åˆ†æå’Œé€»è¾‘æ¨ç†ã€‚</li>
<li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒLaresæé«˜äº†æ˜“ç”¨æ€§å¹¶å‡å°‘äº†äººå·¥å·¥ä½œé‡ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜Laresåœ¨å¤šä¸ªåœºæ™¯ä¸‹å…·æœ‰è¾ƒé«˜çš„æ€§èƒ½è¡¨ç°ã€‚</li>
<li>Laresè§£å†³äº†ç°æœ‰æ–¹æ³•æ— æ³•å‡†ç¡®åŒºåˆ†è¡¥ä¸å¼•èµ·çš„ä»£ç å˜åŒ–å’Œç¼–è¯‘å˜åŒ–çš„é—®é¢˜ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.01252">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ac6c8ebb34eef0687adfa14f6885c29e" align="middle">
<img src="https://picx.zhimg.com/v2-ffff3699dffe2d0bba0e1d81d597a164" align="middle">
<img src="https://picx.zhimg.com/v2-84351b0237f5ae7e9e29f27b2f81fa50" align="middle">
<img src="https://picx.zhimg.com/v2-f6b6dcfbdc10b5364a01bee91ccc5aae" align="middle">
<img src="https://picx.zhimg.com/v2-1db14a687ec73351ad2ca16ea3297a41" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Self-Harmony-Learning-to-Harmonize-Self-Supervision-and-Self-Play-in-Test-Time-Reinforcement-Learning"><a href="#Self-Harmony-Learning-to-Harmonize-Self-Supervision-and-Self-Play-in-Test-Time-Reinforcement-Learning" class="headerlink" title="Self-Harmony: Learning to Harmonize Self-Supervision and Self-Play in   Test-Time Reinforcement Learning"></a>Self-Harmony: Learning to Harmonize Self-Supervision and Self-Play in   Test-Time Reinforcement Learning</h2><p><strong>Authors:Ru Wang, Wei Huang, Qi Cao, Yusuke Iwasawa, Yutaka Matsuo, Jiaxian Guo</strong></p>
<p>Test-time reinforcement learning (TTRL) offers a label-free paradigm for adapting models using only synthetic signals at inference, but its success hinges on constructing reliable learning signals. Standard approaches such as majority voting often collapse to spurious yet popular answers. We introduce Self-Harmony, a framework built on a simple intuition: the correct answer should remain stable across both an original question and its paraphrase. Self-Harmony operationalizes this by employing a single model in two complementary roles: a Solver to produce answers and a Reframer to rephrase the input. Based on this, we further propose a pseudo-label method: instead of majority voting, it aggregates answer frequencies across these original and reframed views using the harmonic mean. This is a process that naturally selects for solutions stable under reframing, thereby avoiding the common trap of favoring view-dependent, spurious answers. Crucially, this requires no human supervision or auxiliary models. Across diverse reasoning benchmarks, Self-Harmony achieves state-of-the-art results at the label-free test-time setting, ranking first in 28 of 30 settings across multiple methods. Beyond accuracy, it demonstrates unprecedented robustness, with zero training failures in all experiments, underscoring its stability and reliability. </p>
<blockquote>
<p>æµ‹è¯•æ—¶å¼ºåŒ–å­¦ä¹ ï¼ˆTTRLï¼‰æä¾›äº†ä¸€ç§æ— æ ‡ç­¾èŒƒå¼ï¼Œå¯åœ¨æ¨ç†æ—¶ä»…ä½¿ç”¨åˆæˆä¿¡å·æ¥é€‚åº”æ¨¡å‹ï¼Œä½†å…¶æˆåŠŸå–å†³äºæ„å»ºå¯é çš„å­¦ä¹ ä¿¡å·ã€‚æ ‡å‡†æ–¹æ³•ï¼ˆå¦‚å¤šæ•°æŠ•ç¥¨ï¼‰é€šå¸¸åå‘äºè™šå‡ä½†æµè¡Œçš„ç­”æ¡ˆã€‚æˆ‘ä»¬å¼•å…¥äº†Self-Harmonyæ¡†æ¶ï¼Œè¯¥æ¡†æ¶å»ºç«‹åœ¨ç®€å•ç›´è§‰ä¹‹ä¸Šï¼šæ­£ç¡®ç­”æ¡ˆåº”è¯¥åœ¨åŸå§‹é—®é¢˜å’Œå…¶åŒä¹‰è½¬è¿°ä¹‹é—´ä¿æŒç¨³å®šã€‚Self-Harmonyé€šè¿‡ä¸¤ç§æ–¹å¼å®ç°è¿™ä¸€ç‚¹ï¼šä½¿ç”¨æ¨¡å‹åœ¨ä¸¤ä¸ªäº’è¡¥è§’è‰²ä¸­æ‰®æ¼”æ±‚è§£å™¨å’Œé‡æ–°æ„é€ è€…ã€‚åŸºäºè¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ä¸€ç§ä¼ªæ ‡ç­¾æ–¹æ³•ï¼šå®ƒä¸ä½¿ç”¨å¤šæ•°æŠ•ç¥¨ï¼Œè€Œæ˜¯ä½¿ç”¨è°ƒå’Œå¹³å‡æ•°æ¥æ±‡æ€»åŸå§‹å’Œé‡æ„è§†å›¾ä¸­çš„ç­”æ¡ˆé¢‘ç‡ã€‚è¿™æ˜¯ä¸€ä¸ªè‡ªç„¶é€‰æ‹©é‡æ„ä¸‹ç¨³å®šè§£å†³æ–¹æ¡ˆçš„è¿‡ç¨‹ï¼Œä»è€Œé¿å…äº†å€¾å‘äºä¾èµ–äºè§†å›¾çš„è™šå‡ç­”æ¡ˆçš„å¸¸è§é™·é˜±ã€‚å…³é”®çš„æ˜¯ï¼Œè¿™ä¸éœ€è¦äººå·¥ç›‘ç£æˆ–è¾…åŠ©æ¨¡å‹ã€‚åœ¨å¤šç§æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒSelf-Harmonyåœ¨æ— æ ‡ç­¾æµ‹è¯•æ—¶é—´è®¾ç½®ä¸­å®ç°äº†æœ€æ–°æŠ€æœ¯æˆæœï¼Œåœ¨å¤šä¸ªæ–¹æ³•çš„30ä¸ªè®¾ç½®ä¸­æœ‰28ä¸ªæ’åç¬¬ä¸€ã€‚é™¤äº†å‡†ç¡®æ€§ä¹‹å¤–ï¼Œå®ƒè¿˜è¡¨ç°å‡ºå‰æ‰€æœªæœ‰çš„ç¨³å¥æ€§ï¼Œåœ¨æ‰€æœ‰å®éªŒä¸­å‡æœªå‡ºç°è®­ç»ƒå¤±è´¥çš„æƒ…å†µï¼Œçªæ˜¾äº†å…¶ç¨³å®šæ€§å’Œå¯é æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.01191v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æµ‹è¯•æ—¶é—´å¼ºåŒ–å­¦ä¹ ï¼ˆTTRLï¼‰çš„ä¸€ç§æ–°å‹æ¡†æ¶â€”â€”Self-Harmonyã€‚å®ƒé€šè¿‡é‡‡ç”¨å•ä¸€æ¨¡å‹çš„ä¸¤ä¸ªäº’è¡¥è§’è‰²ï¼ˆSolverå’ŒReframerï¼‰æ¥åœ¨æ— éœ€äººå·¥ç›‘ç£æˆ–è¾…åŠ©æ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œåˆ©ç”¨åŸå§‹é—®é¢˜å’Œå…¶åŒä¹‰å¥ä¹‹é—´çš„æ­£ç¡®ç­”æ¡ˆä¸ºç¨³å®šçš„ç‰¹æ€§ï¼Œå®ç°äº†ä¸€ç§ä¼ªæ ‡ç­¾æ–¹æ³•ï¼Œé€šè¿‡è®¡ç®—åŸå§‹é—®é¢˜å’ŒåŒä¹‰é—®é¢˜ç­”æ¡ˆé¢‘ç‡çš„è°ƒå’Œå¹³å‡æ•°æ¥èšåˆç­”æ¡ˆã€‚è¿™ç§æ–¹æ³•åœ¨å¤šç§æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æ— æ ‡ç­¾æµ‹è¯•ç¯å¢ƒä¸‹çš„æœ€æ–°ç»“æœï¼Œå¹¶åœ¨å¤šä¸ªæ–¹æ³•çš„30ä¸ªè®¾ç½®ä¸­æ’åç¬¬ä¸€ã€‚é™¤äº†å‡†ç¡®æ€§å¤–ï¼Œå®ƒè¿˜å…·æœ‰å‰æ‰€æœªæœ‰çš„ç¨³å¥æ€§ï¼Œæ‰€æœ‰å®éªŒä¸­å‡æœªå‡ºç°è®­ç»ƒå¤±è´¥çš„æƒ…å†µï¼Œçªæ˜¾äº†å…¶ç¨³å®šæ€§å’Œå¯é æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Self-Harmonyæ˜¯ä¸€ç§æ–°å‹çš„æµ‹è¯•æ—¶é—´å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºåœ¨æ— éœ€äººå·¥ç›‘ç£æˆ–è¾…åŠ©æ¨¡å‹çš„æƒ…å†µä¸‹é€‚åº”æ¨¡å‹ã€‚</li>
<li>å®ƒé€šè¿‡é‡‡ç”¨å•ä¸€æ¨¡å‹çš„ä¸¤ä¸ªäº’è¡¥è§’è‰²ï¼ˆSolverå’ŒReframerï¼‰æ¥æ“ä½œï¼Œåˆ†åˆ«äº§ç”Ÿç­”æ¡ˆå’Œé‡æ–°è¡¨è¿°è¾“å…¥ã€‚</li>
<li>Self-Harmonyæå‡ºäº†ä¸€ç§ä¼ªæ ‡ç­¾æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡è®¡ç®—åŸå§‹é—®é¢˜å’ŒåŒä¹‰é—®é¢˜ç­”æ¡ˆé¢‘ç‡çš„è°ƒå’Œå¹³å‡æ•°æ¥èšåˆç­”æ¡ˆã€‚</li>
<li>è¯¥æ–¹æ³•é¿å…äº†å¸¸è§çš„é€‰æ‹©è§†å›¾ç›¸å…³ã€è™šå‡ç­”æ¡ˆçš„é™·é˜±ã€‚</li>
<li>åœ¨å¤šç§æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒSelf-Harmonyåœ¨æ— éœ€æ ‡ç­¾çš„æµ‹è¯•ç¯å¢ƒä¸­å®ç°äº†æœ€æ–°ç»“æœï¼Œå¹¶åœ¨å¤šä¸ªè®¾ç½®ä¸­æ’åç¬¬ä¸€ã€‚</li>
<li>é™¤äº†å‡†ç¡®æ€§å¤–ï¼ŒSelf-Harmonyè¿˜å…·æœ‰å‰æ‰€æœªæœ‰çš„ç¨³å¥æ€§ï¼Œæ‰€æœ‰å®éªŒä¸­å‡æœªå‡ºç°è®­ç»ƒå¤±è´¥çš„æƒ…å†µã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.01191">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2211bc7a3ac52cc56b12f5b9378ee821" align="middle">
<img src="https://picx.zhimg.com/v2-2c430dbcdb5524a40e15cab8509e3ee9" align="middle">
<img src="https://picx.zhimg.com/v2-16c0a6b79994486a4323b61e19daeccd" align="middle">
<img src="https://picx.zhimg.com/v2-a79fc2c34f52b568b7f4dd6d3757a23d" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Prompt-R1-Collaborative-Automatic-Prompting-Framework-via-End-to-end-Reinforcement-Learning"><a href="#Prompt-R1-Collaborative-Automatic-Prompting-Framework-via-End-to-end-Reinforcement-Learning" class="headerlink" title="Prompt-R1: Collaborative Automatic Prompting Framework via End-to-end   Reinforcement Learning"></a>Prompt-R1: Collaborative Automatic Prompting Framework via End-to-end   Reinforcement Learning</h2><p><strong>Authors:Wenjin Liu, Haoran Luo, Xueyuan Lin, Haoming Liu, Tiesunlong Shen, Jiapu Wang, Rui Mao, Erik Cambria</strong></p>
<p>Recently, advanced large language models (LLMs) have emerged at an increasingly rapid pace. However, when faced with complex problems, most users are often unable to provide accurate and effective prompts to interact with LLMs, thus limiting the performance of LLMs. To address this challenge, we propose Prompt-R1, an end-to-end reinforcement learning framework that uses a small-scale LLM to collaborate with large-scale LLMs, replacing user interaction to solve problems better. This collaboration is cast as a multi-turn prompt interaction, where the small-scale LLM thinks and generates prompts, and the large-scale LLM performs complex reasoning. A dual-constrained reward is designed to optimize for correctness, generation quality, and reasoning accuracy. Prompt-R1 provides a plug-and-play framework that supports both inference and training with various large-scale LLMs. Experiments on multiple public datasets show that Prompt-R1 significantly outperforms baseline models across tasks. Our code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/QwenQKing/Prompt-R1">https://github.com/QwenQKing/Prompt-R1</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œå…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‡ºç°é€Ÿåº¦è¶Šæ¥è¶Šå¿«ã€‚ç„¶è€Œï¼Œå½“é¢å¯¹å¤æ‚é—®é¢˜æ—¶ï¼Œå¤§å¤šæ•°ç”¨æˆ·å¾€å¾€æ— æ³•æä¾›ä¸LLMäº¤äº’çš„å‡†ç¡®å’Œæœ‰æ•ˆçš„æç¤ºï¼Œä»è€Œé™åˆ¶äº†LLMçš„æ€§èƒ½ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†Prompt-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå®ƒä½¿ç”¨å°å‹LLMä¸å¤§å‹LLMè¿›è¡Œåä½œï¼Œä»¥æ›´å¥½åœ°è§£å†³é—®é¢˜æ¥ä»£æ›¿ç”¨æˆ·äº¤äº’ã€‚è¿™ç§åä½œè¢«å¡‘é€ ä¸ºä¸€ç§å¤šè½®æç¤ºäº¤äº’ï¼Œå°å‹LLMæ€è€ƒå¹¶ç”Ÿæˆæç¤ºï¼Œè€Œå¤§å‹LLMè¿›è¡Œå¤æ‚æ¨ç†ã€‚è®¾è®¡äº†ä¸€ä¸ªåŒé‡çº¦æŸå¥–åŠ±æ¥ä¼˜åŒ–æ­£ç¡®æ€§ã€ç”Ÿæˆè´¨é‡å’Œæ¨ç†å‡†ç¡®æ€§ã€‚Prompt-R1æä¾›äº†ä¸€ä¸ªå³æ’å³ç”¨çš„æ¡†æ¶ï¼Œæ”¯æŒä½¿ç”¨å„ç§å¤§å‹LLMè¿›è¡Œæ¨ç†å’Œè®­ç»ƒã€‚åœ¨å¤šä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒPrompt-R1åœ¨å„é¡¹ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºåŸºå‡†æ¨¡å‹ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/QwenQKing/Prompt-R1%E4%B8%8A%E5%85%AC%E5%BC%80%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/QwenQKing/Prompt-R1ä¸Šå…¬å¼€è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.01016v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†å¤æ‚é—®é¢˜æ—¶ï¼Œå¤šæ•°ç”¨æˆ·éš¾ä»¥æä¾›å‡†ç¡®çš„æç¤ºä¸ä¹‹äº¤äº’ï¼Œé™åˆ¶äº†LLMsçš„æ€§èƒ½ã€‚ä¸ºæ­¤ï¼Œæå‡ºPrompt-R1ï¼Œä¸€ä¸ªç«¯åˆ°ç«¯çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé‡‡ç”¨å°å‹LLMä¸å¤§å‹LLMåä½œï¼Œæ›¿ä»£ç”¨æˆ·äº¤äº’ï¼Œæ›´å¥½åœ°è§£å†³é—®é¢˜ã€‚æ­¤åä½œè¢«æ„å»ºä¸ºå¤šè½®æç¤ºäº¤äº’ï¼Œå°å‹LLMè´Ÿè´£æ€è€ƒå’Œç”Ÿæˆæç¤ºï¼Œå¤§å‹LLMè¿›è¡Œå¤æ‚æ¨ç†ã€‚è®¾è®¡åŒé‡çº¦æŸå¥–åŠ±ä»¥ä¼˜åŒ–æ­£ç¡®æ€§ã€ç”Ÿæˆè´¨é‡å’Œæ¨ç†å‡†ç¡®æ€§ã€‚Prompt-R1æä¾›äº†ä¸€ä¸ªå³æ’å³ç”¨çš„æ¡†æ¶ï¼Œæ”¯æŒå„ç§å¤§å‹LLMsçš„æ¨ç†å’Œè®­ç»ƒã€‚åœ¨å¤šä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒPrompt-R1æ˜¾è‘—ä¼˜äºåŸºå‡†æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†å¤æ‚é—®é¢˜æ—¶é¢ä¸´ç”¨æˆ·æç¤ºä¸å‡†ç¡®çš„é—®é¢˜ã€‚</li>
<li>Prompt-R1æ˜¯ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>Prompt-R1é‡‡ç”¨å°å‹LLMä¸å¤§å‹LLMåä½œï¼Œè¿›è¡Œå¤šè½®æç¤ºäº¤äº’ã€‚</li>
<li>å°å‹LLMè´Ÿè´£æ€è€ƒå’Œç”Ÿæˆæç¤ºï¼Œå¤§å‹LLMè¿›è¡Œå¤æ‚æ¨ç†ã€‚</li>
<li>è®¾è®¡äº†åŒé‡çº¦æŸå¥–åŠ±ä»¥ä¼˜åŒ–æ­£ç¡®æ€§ã€ç”Ÿæˆè´¨é‡å’Œæ¨ç†å‡†ç¡®æ€§ã€‚</li>
<li>Prompt-R1æä¾›äº†ä¸€ä¸ªå³æ’å³ç”¨çš„æ¡†æ¶ï¼Œé€‚ç”¨äºå„ç§å¤§å‹LLMsçš„æ¨ç†å’Œè®­ç»ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.01016">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a097f134cd85ced915da87e3d91e28db" align="middle">
<img src="https://picx.zhimg.com/v2-e47a62101e8efc75a132663363d78e53" align="middle">
<img src="https://picx.zhimg.com/v2-fb94bfb1c109f6e7610791650d524214" align="middle">
<img src="https://picx.zhimg.com/v2-8f4fdfed6741b3ce5f6e8663db9da9d0" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Fleming-VL-Towards-Universal-Medical-Visual-Reasoning-with-Multimodal-LLMs"><a href="#Fleming-VL-Towards-Universal-Medical-Visual-Reasoning-with-Multimodal-LLMs" class="headerlink" title="Fleming-VL: Towards Universal Medical Visual Reasoning with Multimodal   LLMs"></a>Fleming-VL: Towards Universal Medical Visual Reasoning with Multimodal   LLMs</h2><p><strong>Authors:Yan Shu, Chi Liu, Robin Chen, Derek Li, Bryan Dai</strong></p>
<p>Multimodal Large Language Models (MLLMs) have demonstrated remarkable effectiveness in various general-domain scenarios, such as visual question answering and image captioning. Recently, researchers have increasingly focused on empowering MLLMs with medical conversational abilities, which hold significant promise for clinical applications. However, medical data presents unique challenges due to its heterogeneous nature â€“ encompassing diverse modalities including 2D images, 3D volumetric scans, and temporal video sequences. The substantial domain gap and data format inconsistencies across these modalities have hindered the development of unified medical MLLMs. To address these challenges, we propose Fleming-VL, a unified end-to-end framework for comprehensive medical visual understanding across heterogeneous modalities. Fleming-VL tackles this problem from a data-centric perspective through three key strategies: (1) scaling up pretraining by integrating long-context data from both natural and medical-specific domains; (2) complementing fine-tuning with rare medical data, including holistic video analysis and underrepresented 2D modalities such as ultrasound and dermoscopy images; (3) extending existing evaluation frameworks to incorporate 3D volumetric and video understanding benchmarks. Through supervised fine-tuning (SFT) and group relative policy optimization (GRPO), we develop Fleming-VL in multiple model scales. Extensive experiments demonstrate that Fleming-VL achieves state-of-the-art performance across multiple benchmarks, including medical VQA, video QA, and 3D medical image understanding. We publicly release Fleming-VL to promote transparent, reproducible, and auditable progress in medical AI. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å„ç§é€šç”¨åœºæ™¯ä¸­è¡¨ç°å‡ºäº†æ˜¾è‘—çš„æœ‰æ•ˆæ€§ï¼Œä¾‹å¦‚è§†è§‰é—®ç­”å’Œå›¾åƒæè¿°ã€‚æœ€è¿‘ï¼Œç ”ç©¶è€…è¶Šæ¥è¶Šå…³æ³¨ä¸ºMLLMsèµ‹äºˆåŒ»ç–—å¯¹è¯èƒ½åŠ›ï¼Œè¿™åœ¨ä¸´åºŠåº”ç”¨æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼ŒåŒ»ç–—æ•°æ®ç”±äºå…¶å¼‚è´¨æ€§è€Œå‘ˆç°å‡ºç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼Œæ¶µç›–å¤šç§æ¨¡å¼ï¼ŒåŒ…æ‹¬äºŒç»´å›¾åƒã€ä¸‰ç»´ä½“ç§¯æ‰«æå’Œæ—¶åºè§†é¢‘åºåˆ—ã€‚è¿™äº›æ¨¡å¼ä¹‹é—´çš„å·¨å¤§é¢†åŸŸå·®è·å’Œæ•°æ®æ ¼å¼ä¸ä¸€è‡´æ€§é˜»ç¢äº†ç»Ÿä¸€åŒ»ç–—MLLMsçš„å‘å±•ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†Fleming-VLï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè·¨å¼‚æ„æ¨¡å¼è¿›è¡Œå…¨é¢åŒ»ç–—è§†è§‰ç†è§£çš„ç»Ÿä¸€ç«¯åˆ°ç«¯æ¡†æ¶ã€‚Fleming-VLä»æ•°æ®ä¸­å¿ƒè§†è§’é€šè¿‡ä¸‰ä¸ªå…³é”®ç­–ç•¥æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼šï¼ˆ1ï¼‰é€šè¿‡æ•´åˆè‡ªç„¶å’ŒåŒ»ç–—ç‰¹å®šé¢†åŸŸçš„é•¿ä¸Šä¸‹æ–‡æ•°æ®æ¥æ‰©å¤§é¢„è®­ç»ƒè§„æ¨¡ï¼›ï¼ˆ2ï¼‰ç”¨ç¨€ç¼ºåŒ»ç–—æ•°æ®è¡¥å……å¾®è°ƒï¼ŒåŒ…æ‹¬æ•´ä½“è§†é¢‘åˆ†æå’Œè¶…å£°å’Œçš®è‚¤é•œå›¾åƒç­‰ä»£è¡¨æ€§ä¸è¶³çš„äºŒç»´æ¨¡å¼ï¼›ï¼ˆ3ï¼‰æ‰©å±•ç°æœ‰è¯„ä¼°æ¡†æ¶ï¼Œçº³å…¥ä¸‰ç»´ä½“ç§¯å’Œè§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬é€šè¿‡æœ‰ç›‘ç£çš„å¾®è°ƒï¼ˆSFTï¼‰å’Œç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰åœ¨å¤šä¸ªæ¨¡å‹è§„æ¨¡ä¸Šå¼€å‘Fleming-VLã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒFleming-VLåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€æ–°æ€§èƒ½ï¼ŒåŒ…æ‹¬åŒ»ç–—VQAã€è§†é¢‘QAå’Œä¸‰ç»´åŒ»å­¦å›¾åƒç†è§£ã€‚æˆ‘ä»¬å…¬å¼€å‘å¸ƒFleming-VLï¼Œä»¥ä¿ƒè¿›åŒ»ç–—äººå·¥æ™ºèƒ½çš„é€æ˜ã€å¯å¤åˆ¶å’Œå¯å®¡æ ¸çš„è¿›æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.00916v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨é€šç”¨é¢†åŸŸåœºæ™¯å¦‚è§†è§‰é—®ç­”å’Œå›¾åƒæè¿°ç­‰æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ•ˆæœã€‚è¿‘æœŸï¼Œç ”ç©¶è€…è‡´åŠ›äºèµ‹äºˆMLLMsåŒ»ç–—å¯¹è¯èƒ½åŠ›ï¼Œä¸ºå…¶åœ¨ä¸´åºŠåº”ç”¨æä¾›å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼ŒåŒ»ç–—æ•°æ®å› å…¶å¼‚æ„æ€§å¸¦æ¥ç‹¬ç‰¹æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬2Då›¾åƒã€3Dä½“ç§¯æ‰«æå’Œæ—¶åºè§†é¢‘åºåˆ—ç­‰å¤šç§æ¨¡æ€ã€‚æ˜¾è‘—çš„é¢†åŸŸå·®è·å’Œæ•°æ®æ ¼å¼ä¸ä¸€è‡´æ€§é˜»ç¢äº†ç»Ÿä¸€åŒ»ç–—MLLMsçš„å‘å±•ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæå‡ºFleming-VLï¼Œä¸€ä¸ªç»Ÿä¸€çš„ç«¯åˆ°ç«¯æ¡†æ¶ï¼Œç”¨äºè·¨å¼‚æ„æ¨¡æ€è¿›è¡Œå…¨é¢åŒ»ç–—è§†è§‰ç†è§£ã€‚Fleming-VLä»æ•°æ®ä¸­å¿ƒè§†è§’é€šè¿‡ä¸‰ä¸ªå…³é”®ç­–ç•¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼šï¼ˆ1ï¼‰é€šè¿‡æ¥è‡ªè‡ªç„¶å’ŒåŒ»ç–—ç‰¹å®šé¢†åŸŸçš„é•¿ä¸Šä¸‹æ–‡æ•°æ®æ‰©å¤§é¢„è®­ç»ƒè§„æ¨¡ï¼›ï¼ˆ2ï¼‰ç”¨ç²¾ç»†è°ƒèŠ‚è¡¥å……ç¨€æœ‰åŒ»ç–—æ•°æ®ï¼ŒåŒ…æ‹¬æ•´ä½“è§†é¢‘åˆ†æå’Œä»£è¡¨æ€§ä¸è¶³çš„2Dæ¨¡æ€ï¼Œå¦‚è¶…å£°å’Œçš®è‚¤é•œæ£€æŸ¥å›¾åƒï¼›ï¼ˆ3ï¼‰æ‰©å±•ç°æœ‰è¯„ä¼°æ¡†æ¶ï¼Œä»¥çº³å…¥3Dä½“ç§¯å’Œè§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ã€‚é€šè¿‡ç›‘ç£ç²¾ç»†è°ƒèŠ‚ï¼ˆSFTï¼‰å’Œç¾¤ä½“ç›¸å¯¹æ”¿ç­–ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œæˆ‘ä»¬åœ¨å¤šä¸ªæ¨¡å‹è§„æ¨¡ä¸Šå¼€å‘Fleming-VLã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒFleming-VLåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°æœ€ä½³æ€§èƒ½ï¼ŒåŒ…æ‹¬åŒ»ç–—VQAã€è§†é¢‘QAå’Œ3DåŒ»å­¦å›¾åƒç†è§£ã€‚æˆ‘ä»¬å…¬å¼€å‘å¸ƒFleming-VLï¼Œä»¥ä¿ƒè¿›åŒ»ç–—AIçš„é€æ˜ã€å¯å¤åˆ¶å’Œå¯å®¡æ ¸çš„è¿›æ­¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤šä¸ªé¢†åŸŸè¡¨ç°ä¼˜ç§€ï¼Œç°åœ¨æ­£è¢«èµ‹äºˆåŒ»ç–—å¯¹è¯èƒ½åŠ›ï¼Œä¸ºä¸´åºŠåº”ç”¨å¸¦æ¥å¸Œæœ›ã€‚</li>
<li>åŒ»ç–—æ•°æ®å› å…¶å¼‚æ„æ€§å¸¦æ¥æŒ‘æˆ˜ï¼Œéœ€è¦å¤„ç†å¤šç§æ¨¡æ€å¦‚2Då›¾åƒã€3Dä½“ç§¯æ‰«æå’Œæ—¶åºè§†é¢‘åºåˆ—ç­‰ã€‚</li>
<li>Fleming-VLæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„ç«¯åˆ°ç«¯æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³åŒ»ç–—æ•°æ®å¼‚æ„æ€§é—®é¢˜ï¼Œé€šè¿‡æ‰©å¤§é¢„è®­ç»ƒè§„æ¨¡ã€è¡¥å……ç¨€æœ‰åŒ»ç–—æ•°æ®å’Œæ‰©å±•è¯„ä¼°æ¡†æ¶ç­‰æ–¹æ³•æ¥æå‡åŒ»ç–—è§†è§‰ç†è§£çš„æ•ˆæœã€‚</li>
<li>Fleming-VLé€šè¿‡ç›‘ç£ç²¾ç»†è°ƒèŠ‚ï¼ˆSFTï¼‰å’Œç¾¤ä½“ç›¸å¯¹æ”¿ç­–ä¼˜åŒ–ï¼ˆGRPOï¼‰åœ¨å¤šä¸ªæ¨¡å‹è§„æ¨¡ä¸Šè¿›è¡Œå¼€å‘ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒFleming-VLåœ¨åŒ»ç–—VQAã€è§†é¢‘QAå’Œ3DåŒ»å­¦å›¾åƒç†è§£ç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚</li>
<li>Fleming-VLè¢«å…¬å¼€å‘å¸ƒï¼Œæ—¨åœ¨ä¿ƒè¿›åŒ»ç–—äººå·¥æ™ºèƒ½çš„é€æ˜ã€å¯å¤åˆ¶å’Œå¯å®¡æ ¸çš„è¿›æ­¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.00916">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bab1a9e3f88b60aad3b2ffdee08830f1" align="middle">
<img src="https://picx.zhimg.com/v2-12866f118f117bfa3e91b3b4838fe441" align="middle">
<img src="https://picx.zhimg.com/v2-c4f29d3f228291863635c21cb663d75d" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Do-Math-Reasoning-LLMs-Help-Predict-the-Impact-of-Public-Transit-Events"><a href="#Do-Math-Reasoning-LLMs-Help-Predict-the-Impact-of-Public-Transit-Events" class="headerlink" title="Do Math Reasoning LLMs Help Predict the Impact of Public Transit Events?"></a>Do Math Reasoning LLMs Help Predict the Impact of Public Transit Events?</h2><p><strong>Authors:Bowen Fang, Ruijian Zha, Xuan Di</strong></p>
<p>Predicting public transit incident duration from unstructured text alerts is a critical but challenging task. Addressing the domain sparsity of transit operations with standard Supervised Fine-Tuning (SFT) is difficult, as the task involves noisy, continuous labels and lacks reliable expert demonstrations for reasoning. While Reinforcement Learning from Verifiable Rewards (RLVR) excels at tasks with binary correctness, like mathematics, its applicability to noisy, continuous forecasting is an open question. This work, to our knowledge, is the first to bridge the gap between RLVR LLM training with the critical, real-world forecasting challenges in public transit operations. We adapt RLVR to this task by introducing a tolerance-based, shaped reward function that grants partial credit within a continuous error margin, rather than demanding a single correct answer. We systematically evaluate this framework on a curated dataset of NYC MTA service alerts. Our findings show that general-purpose, instruction-tuned LLMs significantly outperform specialized math-reasoning models, which struggle with the ambiguous, real-world text. We empirically demonstrate that the binary reward is unstable and degrades performance, whereas our shaped reward design is critical and allows our model to dominate on the most challenging metrics. While classical regressors are superior at minimizing overall MAE or MSE, our RLVR approach achieved a 35% relative improvement in 5-minute accuracy (Acc@5) over the strongest baseline. This demonstrates that RLVR can be successfully adapted to real-world, noisy forecasting, but requires a verifier design that reflects the continuous nature of the problem. </p>
<blockquote>
<p>é¢„æµ‹å…¬å…±äº¤é€šäº‹ä»¶æŒç»­æ—¶é—´ä»éç»“æ„åŒ–æ–‡æœ¬è­¦æŠ¥æ˜¯ä¸€ä¸ªè‡³å…³é‡è¦ä½†å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚ä½¿ç”¨æ ‡å‡†çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è§£å†³å…¬å…±äº¤é€šè¿è¥é¢†åŸŸçš„ç¨€ç–æ€§é—®é¢˜æ˜¯å¾ˆå›°éš¾çš„ï¼Œå› ä¸ºè¯¥ä»»åŠ¡æ¶‰åŠå¸¦æœ‰å™ªå£°çš„è¿ç»­æ ‡ç­¾ï¼Œå¹¶ä¸”ç¼ºä¹å¯é çš„ä¸“å®¶æ¼”ç¤ºæ¥è¿›è¡Œæ¨ç†ã€‚è™½ç„¶å¼ºåŒ–å­¦ä¹ ä»å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰åœ¨æ•°å­¦ç­‰äºŒå…ƒæ­£ç¡®æ€§ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†å…¶å¯¹å™ªå£°ã€è¿ç»­é¢„æµ‹çš„é€‚ç”¨æ€§ä»æ˜¯æœªè§£çš„é—®é¢˜ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™é¡¹å·¥ä½œæ˜¯ç¬¬ä¸€æ¬¡æ¶èµ·RLVRå¤§å‹è¯­è¨€æ¨¡å‹è®­ç»ƒå’Œå…¬å…±äº¤é€šè¿è¥ä¸­å…³é”®çš„ç°å®ä¸–ç•Œé¢„æµ‹æŒ‘æˆ˜ä¹‹é—´çš„æ¡¥æ¢ã€‚æˆ‘ä»¬é€šè¿‡å¼•å…¥åŸºäºå®¹å¿åº¦çš„æˆå‹å¥–åŠ±å‡½æ•°æ¥é€‚åº”RLVRé€‚åº”è¿™é¡¹ä»»åŠ¡ï¼Œè¯¥å‡½æ•°åœ¨è¿ç»­çš„è¯¯å·®èŒƒå›´å†…æˆäºˆéƒ¨åˆ†ä¿¡ç”¨ï¼Œè€Œä¸æ˜¯è¦æ±‚ä¸€ä¸ªå•ä¸€çš„æ­£ç¡®ç­”æ¡ˆã€‚æˆ‘ä»¬åœ¨çº½çº¦å¸‚MTAæœåŠ¡è­¦æŠ¥çš„æ‰‹å·¥æ•°æ®é›†ä¸Šç³»ç»Ÿåœ°è¯„ä¼°äº†è¿™ä¸€æ¡†æ¶ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé€šç”¨æŒ‡ä»¤è°ƒæ•´çš„å¤§å‹è¯­è¨€æ¨¡å‹æ˜¾è‘—ä¼˜äºä¸“ä¸šæ•°å­¦æ¨ç†æ¨¡å‹ï¼Œåè€…åœ¨å¤„ç†æ¨¡ç³Šçš„ç°å®ä¸–ç•Œæ–‡æœ¬æ—¶æ„Ÿåˆ°å›°éš¾é‡é‡ã€‚æˆ‘ä»¬å®è¯è¡¨æ˜äºŒå…ƒå¥–åŠ±ä¸ç¨³å®šä¸”æ€§èƒ½ä¸‹é™ï¼Œè€Œæˆ‘ä»¬çš„æˆå‹å¥–åŠ±è®¾è®¡æ˜¯å…³é”®æ‰€åœ¨ï¼Œå¹¶å…è®¸æˆ‘ä»¬çš„æ¨¡å‹åœ¨æœ€å…·æŒ‘æˆ˜æ€§çš„æŒ‡æ ‡ä¸Šå æ®ä¸»å¯¼åœ°ä½ã€‚è™½ç„¶ç»å…¸å›å½’å™¨åœ¨æœ€å°åŒ–æ€»ä½“MAEæˆ–MSEæ–¹é¢æ›´å‡ºè‰²ï¼Œä½†æˆ‘ä»¬çš„RLVRæ–¹æ³•åœ¨æœ€ä¸¥æ ¼çš„åŸºçº¿åŸºç¡€ä¸Šå®ç°äº†5åˆ†é’Ÿå‡†ç¡®ç‡ï¼ˆAcc@5ï¼‰çš„ç›¸å¯¹æ”¹è¿›æé«˜äº†35%ã€‚è¿™è¡¨æ˜RLVRå¯ä»¥æˆåŠŸé€‚åº”ç°å®ä¸–ç•Œçš„å™ªå£°é¢„æµ‹ï¼Œä½†éœ€è¦åæ˜ é—®é¢˜è¿ç»­æ€§çš„éªŒè¯å™¨è®¾è®¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.00808v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬æ¢è®¨äº†å…¬å…±äº¤é€šäº‹ä»¶æŒç»­æ—¶é—´é¢„æµ‹çš„æŒ‘æˆ˜æ€§é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯ä»éç»“æ„åŒ–æ–‡æœ¬è­¦æŠ¥ä¸­è¿›è¡Œé¢„æµ‹ã€‚æ–‡ç« æŒ‡å‡ºï¼Œé’ˆå¯¹å…¬å…±äº¤é€šè¿è¥çš„é¢†åŸŸç¨€ç–æ€§ï¼Œä½¿ç”¨æ ‡å‡†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰é¢ä¸´å›°éš¾ã€‚æ–‡ç« é¦–æ¬¡å°†å¼ºåŒ–å­¦ä¹ ä»å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰ä¸å…¬å…±äº¤é€šè¿è¥ä¸­çš„å…³é”®ç°å®é¢„æµ‹æŒ‘æˆ˜ç›¸ç»“åˆã€‚é€šè¿‡å¼•å…¥åŸºäºå®¹å¿åº¦çš„å¥–åŠ±å‡½æ•°ï¼Œè¯¥æ¨¡å‹å¯åœ¨è¿ç»­è¯¯å·®èŒƒå›´å†…ç»™äºˆéƒ¨åˆ†ä¿¡ç”¨ï¼Œè€Œä¸æ˜¯è¦æ±‚å•ä¸€æ­£ç¡®ç­”æ¡ˆã€‚æ–‡ç« å¯¹çº½çº¦å¸‚äº¤é€šå±€æœåŠ¡è­¦æŠ¥çš„æ•°æ®é›†è¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ã€‚ç ”ç©¶å‘ç°ï¼Œé€šç”¨ã€æŒ‡ä»¤è°ƒä¼˜çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜¾è‘—ä¼˜äºä¸“é—¨ç”¨äºæ•°å­¦æ¨ç†çš„æ¨¡å‹ï¼Œåè€…åœ¨å¤„ç†æ¨¡ç³Šç°å®æ–‡æœ¬æ—¶é‡åˆ°å›°éš¾ã€‚æ–‡ç« è¿˜å®è¯äº†äºŒå…ƒå¥–åŠ±çš„ä¸ç¨³å®šæ€§åŠå…¶å¯¹æ€§èƒ½çš„å½±å“ï¼Œè€Œè®¾è®¡çš„è¿ç»­å¥–åŠ±å¯¹äºæ¨¡å‹æˆåŠŸè‡³å…³é‡è¦ã€‚è™½ç„¶ç»å…¸å›å½’å™¨åœ¨æœ€å°åŒ–æ•´ä½“MAEæˆ–MSEæ–¹é¢è¡¨ç°ä¼˜è¶Šï¼Œä½†RLVRæ–¹æ³•åœ¨5åˆ†é’Ÿå‡†ç¡®ç‡ï¼ˆAcc@5ï¼‰ä¸Šå®ç°äº†ç›¸å¯¹äºæœ€å¼ºåŸºçº¿35%çš„ç›¸å¯¹æ”¹è¿›ã€‚è¿™è¡¨æ˜RLVRå¯ä»¥æˆåŠŸé€‚åº”ç°å®ä¸–ç•Œçš„å˜ˆæ‚é¢„æµ‹ï¼Œä½†éœ€è¦åæ˜ é—®é¢˜çš„è¿ç»­æ€§çš„éªŒè¯å™¨è®¾è®¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢„æµ‹å…¬å…±äº¤é€šäº‹ä»¶æŒç»­æ—¶é—´æ˜¯ä¸€é¡¹æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯ä»éç»“æ„åŒ–æ–‡æœ¬è­¦æŠ¥ä¸­è¿›è¡Œé¢„æµ‹ã€‚</li>
<li>æ ‡å‡†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰åœ¨è§£å†³å…¬å…±äº¤é€šè¿è¥çš„é¢†åŸŸç¨€ç–æ€§æ–¹é¢å­˜åœ¨å›°éš¾ã€‚</li>
<li>è¯¥ç ”ç©¶é¦–æ¬¡ç»“åˆäº†å¼ºåŒ–å­¦ä¹ ä»å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰ä¸ç°å®ä¸–ç•Œçš„å…¬å…±äº¤é€šè¿è¥é¢„æµ‹æŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥åŸºäºå®¹å¿åº¦çš„å¥–åŠ±å‡½æ•°ï¼Œå¯åœ¨è¿ç»­è¯¯å·®èŒƒå›´å†…éƒ¨åˆ†ä¿¡ç”¨ã€‚</li>
<li>é€šç”¨ã€æŒ‡ä»¤è°ƒä¼˜çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†æ¨¡ç³Šç°å®æ–‡æœ¬æ—¶è¡¨ç°ä¼˜äºæ•°å­¦æ¨ç†æ¨¡å‹ã€‚</li>
<li>äºŒå…ƒå¥–åŠ±çš„ä¸ç¨³å®šæ€§åŠå…¶å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“è¢«å®è¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.00808">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-61612212749932281c92d35ae82e46ed" align="middle">
<img src="https://picx.zhimg.com/v2-95ab20af924071b629b628dab23ed51e" align="middle">
<img src="https://picx.zhimg.com/v2-ad94bcee26e65429567c6903784bce99" align="middle">
<img src="https://picx.zhimg.com/v2-4e88b73ac91f4b15c6d97266ab6117ec" align="middle">
<img src="https://picx.zhimg.com/v2-18f5cb3055428b45d2c63b0ad263553e" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="GrowthHacker-Automated-Off-Policy-Evaluation-Optimization-Using-Code-Modifying-LLM-Agents"><a href="#GrowthHacker-Automated-Off-Policy-Evaluation-Optimization-Using-Code-Modifying-LLM-Agents" class="headerlink" title="GrowthHacker: Automated Off-Policy Evaluation Optimization Using   Code-Modifying LLM Agents"></a>GrowthHacker: Automated Off-Policy Evaluation Optimization Using   Code-Modifying LLM Agents</h2><p><strong>Authors:Jie JW Wu, Ayanda Patrick Herlihy, Ahmad Saleem Mirza, Ali Afoud, Fatemeh Fard</strong></p>
<p>With the software industry shifting toward a data-driven culture, online A&#x2F;B testing is a key tool for evaluating new technologies. However, deploying such experiments requires substantial resources, may negatively impact users, and involves long data collection periods. To address this, \textit{off-policy evaluation (OPE)}, or offline A&#x2F;B testing, uses logged data to assess technologies and is fundamental in Reinforcement Learning, making it crucial in domains where online testing is costly or risky, such as healthcare, recommender systems, education, dialog systems, and robotics. Despite advances in coding LLMs and agentic AI, little is known about leveraging them to optimize OPE results. We investigate whether LLMs and LLM-based agents can improve OPE performance via code optimization. We propose \textit{GrowthHacker}, a benchmark with agent and baseline methods on large-scale real-world datasets, which iteratively optimizes code, evaluates results, and begins new optimization cycles. We collected datasets, established protocols, implemented baselines for OPE on the Open Bandit Pipeline (OBP)<del>\cite{saito2021openbanditdatasetpipeline} and Scope-RL</del>\cite{kiyohara2023scope}, and developed the \textit{two_agent} framework, which reduces system complexity while preserving optimization effectiveness. Results show the two_agent framework achieves 100% reliability and the highest average improvement of 106.7% among positive outcomes. Both two_agent and CrewAI reach 45% success rates, outperforming AutoGenâ€™s 34%. These findings demonstrate the feasibility of LLM-based agents as automated â€œgrowth hackersâ€ to enhance OPE systems, with implications for scaling data-driven decision-making in production. </p>
<blockquote>
<p>éšç€è½¯ä»¶è¡Œä¸šå‘æ•°æ®é©±åŠ¨çš„æ–‡åŒ–è½¬å˜ï¼Œåœ¨çº¿A&#x2F;Bæµ‹è¯•æ˜¯è¯„ä¼°æ–°æŠ€æœ¯çš„é‡è¦å·¥å…·ã€‚ç„¶è€Œï¼Œéƒ¨ç½²æ­¤ç±»å®éªŒéœ€è¦å¤§é‡èµ„æºï¼Œå¯èƒ½ä¼šå¯¹ç”¨æˆ·äº§ç”Ÿè´Ÿé¢å½±å“ï¼Œå¹¶ä¸”æ¶‰åŠé•¿æœŸçš„æ•°æ®æ”¶é›†ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œâ€œç¦»ç­–ç•¥è¯„ä¼°ï¼ˆOPEï¼‰â€æˆ–ç¦»çº¿A&#x2F;Bæµ‹è¯•ä½¿ç”¨æ—¥å¿—æ•°æ®æ¥è¯„ä¼°æŠ€æœ¯ï¼Œè¿™åœ¨å¼ºåŒ–å­¦ä¹ ä¸­æ˜¯æ ¹æœ¬æ€§çš„ï¼Œå› æ­¤åœ¨åœ¨çº¿æµ‹è¯•æˆæœ¬é«˜æ˜‚æˆ–å…·æœ‰é£é™©ï¼ˆå¦‚åŒ»ç–—ä¿å¥ã€æ¨èç³»ç»Ÿã€æ•™è‚²ã€å¯¹è¯ç³»ç»Ÿå’Œæœºå™¨äººæŠ€æœ¯ï¼‰çš„é¢†åŸŸå°¤ä¸ºé‡è¦ã€‚å°½ç®¡ç¼–ç å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œæ™ºèƒ½ä½“äººå·¥æ™ºèƒ½æœ‰æ‰€è¿›å±•ï¼Œä½†å¦‚ä½•åˆ©ç”¨å®ƒä»¬ä¼˜åŒ–OPEç»“æœçŸ¥ä¹‹ç”šå°‘ã€‚æˆ‘ä»¬è°ƒæŸ¥LLMså’ŒåŸºäºLLMçš„ä»£ç†æ˜¯å¦å¯ä»¥é€šè¿‡ä»£ç ä¼˜åŒ–æ”¹å–„OPEæ€§èƒ½ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåä¸ºâ€œGrowthHackerâ€çš„åŸºå‡†æµ‹è¯•ï¼Œè¯¥æµ‹è¯•åœ¨å¤§å‹ç°å®ä¸–ç•Œæ•°æ®é›†ä¸Šå¯¹ä»£ç†å’ŒåŸºå‡†æ–¹æ³•è¿›è¡Œäº†è¯„ä¼°ï¼Œå®ƒè¿­ä»£åœ°ä¼˜åŒ–ä»£ç ã€è¯„ä¼°ç»“æœå¹¶å¯åŠ¨æ–°çš„ä¼˜åŒ–å‘¨æœŸã€‚æˆ‘ä»¬åœ¨å¼€æ”¾åŒªå¾’ç®¡é“ï¼ˆOBPï¼‰å’ŒScope-RLä¸Šæ”¶é›†äº†æ•°æ®é›†ï¼Œå»ºç«‹äº†åè®®ï¼Œä¸ºOPEå®æ–½äº†åŸºçº¿ï¼Œå¹¶å¼€å‘äº†â€œtwo_agentâ€æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é™ä½äº†ç³»ç»Ÿå¤æ‚æ€§åŒæ—¶ä¿ç•™äº†ä¼˜åŒ–çš„æœ‰æ•ˆæ€§ã€‚ç»“æœæ˜¾ç¤ºï¼Œtwo_agentæ¡†æ¶å®ç°äº†100%çš„å¯é æ€§ï¼Œå¹¶åœ¨ç§¯ææˆæœä¸­å¹³å‡æé«˜äº†æœ€é«˜çš„106.7%ã€‚two_agentå’ŒCrewAIçš„æˆåŠŸç‡å‡è¾¾åˆ°45%ï¼Œé«˜äºAutoGençš„34%ã€‚è¿™äº›å‘ç°è¯æ˜äº†åŸºäºLLMçš„ä»£ç†ä½œä¸ºè‡ªåŠ¨åŒ–â€œå¢é•¿é»‘å®¢â€å¢å¼ºOPEç³»ç»Ÿçš„å¯è¡Œæ€§ï¼Œä¸ºç”Ÿäº§ä¸­çš„æ•°æ®é©±åŠ¨å†³ç­–æä¾›äº†æ‰©å±•çš„å¯ç¤ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.00802v1">PDF</a> </p>
<p><strong>Summary</strong><br>åœ¨çº¿A&#x2F;Bæµ‹è¯•æ˜¯è¯„ä¼°æ–°æŠ€æœ¯çš„å…³é”®å·¥å…·ï¼Œä½†åœ¨å®é™…æ“ä½œä¸­éœ€è¦æ¶ˆè€—å¤§é‡èµ„æºå¹¶å¯èƒ½å¯¹ç”¨æˆ·äº§ç”Ÿè´Ÿé¢å½±å“ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œç¦»çº¿A&#x2F;Bæµ‹è¯•ï¼ˆå³Off-Policy Evaluationï¼ŒOPEï¼‰åˆ©ç”¨æ—¥å¿—æ•°æ®è¯„ä¼°æŠ€æœ¯ï¼Œåœ¨å¼ºåŒ–å­¦ä¹ ä¸­è‡³å…³é‡è¦ã€‚é’ˆå¯¹åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’ŒåŸºäºLLMçš„ä»£ç†ä¼˜åŒ–OPEç»“æœçš„ç ”ç©¶è¡¨æ˜ï¼ŒLLMsåœ¨ä¼˜åŒ–OPEæ€§èƒ½æ–¹é¢å…·æœ‰æ½œåŠ›ã€‚æœ¬æ–‡æå‡ºGrowthHackeråŸºå‡†æµ‹è¯•ï¼Œä½¿ç”¨å¤§è§„æ¨¡ç°å®ä¸–ç•Œæ•°æ®é›†ï¼Œæ—¨åœ¨é€šè¿‡è¿­ä»£ä¼˜åŒ–ä»£ç å’Œæé«˜è¯„ä¼°ç»“æœï¼Œå¼€å§‹æ–°çš„ä¼˜åŒ–å¾ªç¯ã€‚ç ”ç©¶å‘ç°ä¸¤agentæ¡†æ¶å¯å®ç°é«˜å¯é æ€§å’Œæœ€é«˜å¹³å‡æ”¹å–„ç‡ã€‚æ­¤ç ”ç©¶å±•ç¤ºLLMä»£ç†åœ¨æå‡ç¦»çº¿æµ‹è¯•çš„æ½œåŠ›ã€‚æœ‰åŠ©äºæ¨è¿›ç”Ÿäº§ç¯å¢ƒä¸­çš„æ•°æ®é©±åŠ¨å†³ç­–è§„æ¨¡åŒ–ã€‚ </p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è½¯ä»¶è¡Œä¸šæ­£åœ¨å‘æ•°æ®é©±åŠ¨æ–‡åŒ–è½¬å˜ï¼Œåœ¨çº¿A&#x2F;Bæµ‹è¯•æ˜¯è¯„ä¼°æ–°æŠ€æœ¯çš„å…³é”®å·¥å…·ï¼Œä½†å­˜åœ¨èµ„æºæ¶ˆè€—å¤§ã€ç”¨æˆ·å½±å“å¯èƒ½è´Ÿé¢çš„ç¼ºç‚¹ã€‚</li>
<li>Off-Policy Evaluation (OPE) æˆ–ç¦»çº¿A&#x2F;Bæµ‹è¯•ä½¿ç”¨æ—¥å¿—æ•°æ®è¯„ä¼°æŠ€æœ¯ï¼Œåœ¨å¼ºåŒ–å­¦ä¹ ä¸­è‡³å…³é‡è¦ï¼Œç‰¹åˆ«é€‚ç”¨äºåœ¨çº¿æµ‹è¯•æˆæœ¬é«˜æˆ–é£é™©å¤§çš„é¢†åŸŸã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’ŒåŸºäºLLMçš„ä»£ç†åœ¨ä¼˜åŒ–OPEæ€§èƒ½æ–¹é¢å…·æœ‰æ½œåŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.00802">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-86cf7f8be2fdfd81d160ff5c50d723fd" align="middle">
<img src="https://picx.zhimg.com/v2-5161362fd0a4c3f1aece5f1409dc0e9e" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="CoT-Saliency-Unified-Chain-of-Thought-Reasoning-for-Heterogeneous-Saliency-Tasks"><a href="#CoT-Saliency-Unified-Chain-of-Thought-Reasoning-for-Heterogeneous-Saliency-Tasks" class="headerlink" title="CoT-Saliency: Unified Chain-of-Thought Reasoning for Heterogeneous   Saliency Tasks"></a>CoT-Saliency: Unified Chain-of-Thought Reasoning for Heterogeneous   Saliency Tasks</h2><p><strong>Authors:Long Li, Shuichen Ji, Ziyang Luo, Nian Liu, Dingwen Zhang, Junwei Han</strong></p>
<p>We present the first unified framework that jointly handles three operationally heterogeneous saliency tasks, eg, SOD, CoSOD, and SIS, by casting each as a Chain-of-Thought (CoT) reasoning process in a Vision-Language Model (VLM) to bridge task heterogeneity. CoT training follows a two-stage paradigm: Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL). To enhance CoT quality in RL, we propose Confidence-Guided Policy Optimization (CGPO), a lightweight single-sample algorithm that leverages the discrepancy between reward and model confidence as a per-sample advantage signal. This design naturally focuses updates on informative responses while eliminating group sampling, thereby addressing GRPOâ€™s key limitations: confidence-agnostic learning, signal dilution, and prohibitive computational overhead. We also introduce an â€œoutput-to-reasoningâ€ strategy to construct high-fidelity SFT data that ensures logical consistency with ground-truth masks. Experiments show our model matches or outperforms specialized SOTA methods and strong closed-source VLMs across all tasks, especially achieving an S-measure of 0.899 on CoCA for CoSOD, surpassing the prior best by 8.0 percentage points, despite using far less training data. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ç¬¬ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œé€šè¿‡é‡‡ç”¨æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†è¿‡ç¨‹ï¼Œå°†æ“ä½œå¼‚è´¨çš„æ˜¾è‘—æ€§ä»»åŠ¡ï¼ˆä¾‹å¦‚SODã€CoSODå’ŒSISï¼‰è”åˆå¤„ç†ï¼Œä»¥å¼¥åˆä»»åŠ¡ä¹‹é—´çš„å¼‚è´¨æ€§é¸¿æ²Ÿã€‚æ€ç»´é“¾è®­ç»ƒéµå¾ªä¸¤é˜¶æ®µæ¨¡å¼ï¼šç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ã€‚ä¸ºäº†æé«˜å¼ºåŒ–å­¦ä¹ ä¸­çš„æ€ç»´é“¾è´¨é‡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¿¡å¿ƒå¼•å¯¼ç­–ç•¥ä¼˜åŒ–ï¼ˆCGPOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§è½»é‡çº§çš„å•æ ·æœ¬ç®—æ³•ï¼Œåˆ©ç”¨å¥–åŠ±ä¸æ¨¡å‹ä¿¡å¿ƒä¹‹é—´çš„å·®å¼‚ä½œä¸ºæ¯ä¸ªæ ·æœ¬çš„ä¼˜åŠ¿ä¿¡å·ã€‚è¿™ç§è®¾è®¡è‡ªç„¶åœ°ä¸“æ³¨äºæ›´æ–°ä¿¡æ¯å“åº”ï¼ŒåŒæ—¶æ¶ˆé™¤äº†ç¾¤ä½“é‡‡æ ·ï¼Œä»è€Œè§£å†³äº†å…³é”®é™åˆ¶ï¼šä¿¡å¿ƒæ— å…³çš„å­¦ä¹ ã€ä¿¡å·ç¨€é‡Šå’Œè®¡ç®—å¼€é”€è¿‡é«˜çš„é—®é¢˜ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†â€œè¾“å‡ºåˆ°æ¨ç†â€çš„ç­–ç•¥æ¥æ„å»ºé«˜ä¿çœŸSFTæ•°æ®ï¼Œç¡®ä¿ä¸åœ°é¢çœŸå®æ©ç çš„å†…åœ¨é€»è¾‘ä¸€è‡´æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨æ‰€æœ‰ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æˆ–è¶…è¶Šäº†ä¸“é—¨çš„æœ€æ–°æ–¹æ³•ä»¥åŠå¼ºå¤§çš„å°é—­å¼è§†è§‰è¯­è¨€æ¨¡å‹çš„è¡¨ç°ã€‚ç‰¹åˆ«æ˜¯åœ¨CoCAä¸Šçš„CoSODä»»åŠ¡ä¸Šï¼Œæˆ‘ä»¬å®ç°äº†S-measureä¸º0.899çš„å¾—åˆ†ï¼Œåœ¨ä¹‹å‰æœ€å¥½çš„æˆç»©åŸºç¡€ä¸Šæé«˜äº†8ä¸ªç™¾åˆ†ç‚¹ï¼Œå°½ç®¡ä½¿ç”¨äº†è¾ƒå°‘çš„è®­ç»ƒæ•°æ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.00396v1">PDF</a> 14 pages,10 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†é¦–ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œé€šè¿‡é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†è¿‡ç¨‹åœ¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ä¸­è§£å†³ä¸‰ç§æ“ä½œå¼‚è´¨çš„æ˜¾è‘—æ€§ä»»åŠ¡ï¼Œå¦‚SODã€CoSODå’ŒSISã€‚é‡‡ç”¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„ä¸¤é˜¶æ®µè®­ç»ƒæ¨¡å¼ï¼Œå¹¶æå‡ºä¿¡å¿ƒå¼•å¯¼ç­–ç•¥ä¼˜åŒ–ï¼ˆCGPOï¼‰æå‡CoTè´¨é‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åŒ¹é…æˆ–è¶…è¶Šç‰¹å®šé¢†åŸŸçš„æœ€å…ˆè¿›æ–¹æ³•å’Œå¼ºå¤§çš„å°é—­æºVLMåœ¨æ‰€æœ‰ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†é¦–ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œèƒ½å¤Ÿå¤„ç†ä¸‰ç§æ“ä½œå¼‚è´¨çš„æ˜¾è‘—æ€§ä»»åŠ¡ï¼šSODã€CoSODå’ŒSISã€‚</li>
<li>é€šè¿‡é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†è¿‡ç¨‹åœ¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ä¸­è§£å†³ä»»åŠ¡å¼‚è´¨æ€§ã€‚</li>
<li>é‡‡ç”¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„ä¸¤é˜¶æ®µè®­ç»ƒæ¨¡å¼ã€‚</li>
<li>æå‡ºä¿¡å¿ƒå¼•å¯¼ç­–ç•¥ä¼˜åŒ–ï¼ˆCGPOï¼‰ç®—æ³•ï¼Œåˆ©ç”¨å¥–åŠ±ä¸æ¨¡å‹ä¿¡å¿ƒä¹‹é—´çš„ä¸ä¸€è‡´æ€§ä½œä¸ºæ¯æ ·æœ¬çš„ä¼˜åŠ¿ä¿¡å·ï¼Œæé«˜CoTè´¨é‡ã€‚</li>
<li>å¼•å…¥â€œè¾“å‡ºåˆ°æ¨ç†â€ç­–ç•¥ï¼Œæ„å»ºé«˜ä¿çœŸSFTæ•°æ®ï¼Œç¡®ä¿ä¸åœ°é¢çœŸå®æ©è†œçš„é€»è¾‘ä¸€è‡´æ€§ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨æ‰€æœ‰ä»»åŠ¡ä¸Šçš„è¡¨ç°å‡åŒ¹é…æˆ–ä¼˜äºä¸“ä¸šæœ€å…ˆè¿›æ–¹æ³•å’Œå¼ºå¤§çš„å°é—­æºVLMã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.00396">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4ea39885c9c2b66f776221d8d0bb7107" align="middle">
<img src="https://picx.zhimg.com/v2-2722cbe22f482af1007f8dc5fb69edf1" align="middle">
<img src="https://picx.zhimg.com/v2-9a7ed155ccb2dac9afa76ffafd08388a" align="middle">
<img src="https://picx.zhimg.com/v2-9ac93e612c5e99c92d772ca89a67f0a6" align="middle">
<img src="https://picx.zhimg.com/v2-c6a891116e501c5d244d88a465585b7c" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Token-Regulated-Group-Relative-Policy-Optimization-for-Stable-Reinforcement-Learning-in-Large-Language-Models"><a href="#Token-Regulated-Group-Relative-Policy-Optimization-for-Stable-Reinforcement-Learning-in-Large-Language-Models" class="headerlink" title="Token-Regulated Group Relative Policy Optimization for Stable   Reinforcement Learning in Large Language Models"></a>Token-Regulated Group Relative Policy Optimization for Stable   Reinforcement Learning in Large Language Models</h2><p><strong>Authors:Tue Le, Nghi D. Q. Bui, Linh Ngo Van, Trung Le</strong></p>
<p>Reinforcement learning with verifiable rewards (RLVR) has emerged as a powerful approach for strengthening the reasoning capabilities of large language models (LLMs). Among existing algorithms, Group Relative Policy Optimization (GRPO) has demonstrated strong performance, yet it suffers from a critical issue: low-probability tokens disproportionately dominate gradient updates due to their inherently large gradient magnitudes. This imbalance leads to unstable training and suppresses the contribution of high-probability tokens that are more reliable for learning. In this work, we introduce Token-Regulated Group Relative Policy Optimization (TR-GRPO), a simple yet effective extension of GRPO that assigns token-level weights positively correlated with the modelâ€™s predicted probability. By downweighting low-probability tokens and emphasizing high-probability ones, TR-GRPO mitigates gradient over-amplification while preserving informative learning signals. Extensive experiments demonstrate that TR-GRPO consistently outperforms GRPO across RLVR tasks, including logic, math, and agentic reasoning, highlighting the importance of regulating token contributions during RL training and establishing TR-GRPO as a robust framework for enhancing LLM reasoning. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰å·²æˆä¸ºå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›çš„ä¸€ç§å¼ºå¤§æ–¹æ³•ã€‚åœ¨ç°æœ‰ç®—æ³•ä¸­ï¼Œç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰è¡¨ç°å‡ºäº†å¼ºå¤§çš„æ€§èƒ½ï¼Œä½†å®ƒå­˜åœ¨ä¸€ä¸ªå…³é”®é—®é¢˜ï¼šç”±äºä½æ¦‚ç‡æ ‡è®°çš„æ¢¯åº¦å¹…åº¦è¾ƒå¤§ï¼Œå®ƒä»¬åœ¨æ¢¯åº¦æ›´æ–°ä¸­å ä¸»å¯¼åœ°ä½ã€‚è¿™ç§ä¸å¹³è¡¡å¯¼è‡´è®­ç»ƒä¸ç¨³å®šï¼Œå¹¶æŠ‘åˆ¶äº†é«˜æ¦‚ç‡æ ‡è®°å¯¹å­¦ä¹ çš„æ›´å¯é è´¡çŒ®ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†Tokenè°ƒæ§ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆTR-GRPOï¼‰ï¼Œè¿™æ˜¯GRPOçš„ä¸€ä¸ªç®€å•è€Œæœ‰æ•ˆçš„æ‰©å±•ï¼Œå®ƒé€šè¿‡åˆ†é…ä¸æ¨¡å‹é¢„æµ‹æ¦‚ç‡æ­£ç›¸å…³æ ‡è®°çº§åˆ«çš„æƒé‡æ¥å‘æŒ¥ä½œç”¨ã€‚é€šè¿‡é™ä½ä½æ¦‚ç‡æ ‡è®°çš„æƒé‡å¹¶å¼ºè°ƒé«˜æ¦‚ç‡æ ‡è®°ï¼ŒTR-GRPOç¼“è§£äº†æ¢¯åº¦è¿‡åº¦æ”¾å¤§é—®é¢˜ï¼ŒåŒæ—¶ä¿ç•™äº†ä¿¡æ¯ä¸°å¯Œçš„å­¦ä¹ ä¿¡å·ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œåœ¨RLVRä»»åŠ¡ä¸Šï¼ŒåŒ…æ‹¬é€»è¾‘ã€æ•°å­¦å’Œæ™ºèƒ½ä½“æ¨ç†ï¼ŒTR-GRPOå§‹ç»ˆä¼˜äºGRPOï¼Œè¿™çªå‡ºäº†åœ¨RLè®­ç»ƒè¿‡ç¨‹ä¸­è°ƒèŠ‚æ ‡è®°è´¡çŒ®çš„é‡è¦æ€§ï¼Œå¹¶ç¡®ç«‹äº†TR-GRPOä½œä¸ºä¸€ä¸ªå¢å¼ºLLMæ¨ç†çš„ç¨³å¥æ¡†æ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.00066v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šå¼ºåŒ–å­¦ä¹ å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰æ–¹æ³•å¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚Group Relative Policy Optimizationï¼ˆGRPOï¼‰ç®—æ³•è¡¨ç°ä¼˜å¼‚ï¼Œä½†å­˜åœ¨ä½æ¦‚ç‡æ ‡è®°è¿‡åˆ†ä¸»å¯¼æ¢¯åº¦æ›´æ–°çš„é—®é¢˜ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†Token-Regulated Group Relative Policy Optimizationï¼ˆTR-GRPOï¼‰ï¼Œå®ƒé€šè¿‡æ ¹æ®æ¨¡å‹é¢„æµ‹æ¦‚ç‡åˆ†é…æ ‡è®°çº§æƒé‡ï¼Œæœ‰æ•ˆå¹³è¡¡äº†æ¢¯åº¦æ›´æ–°ã€‚å®éªŒè¯æ˜ï¼ŒTR-GRPOåœ¨RLVRä»»åŠ¡ä¸Šè¡¨ç°ä¼˜äºGRPOï¼Œç‰¹åˆ«æ˜¯åœ¨é€»è¾‘ã€æ•°å­¦å’Œæ™ºèƒ½ä½“æ¨ç†æ–¹é¢ã€‚è¿™è¡¨æ˜åœ¨RLè®­ç»ƒä¸­è°ƒèŠ‚æ ‡è®°è´¡çŒ®çš„é‡è¦æ€§ï¼Œå¹¶ç¡®ç«‹äº†TR-GRPOä½œä¸ºå¢å¼ºLLMæ¨ç†çš„ç¨³å¥æ¡†æ¶ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰æœ‰åŠ©äºå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>Group Relative Policy Optimizationï¼ˆGRPOï¼‰ç®—æ³•åœ¨RLVRä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†å­˜åœ¨æ¢¯åº¦æ›´æ–°ä¸å¹³è¡¡é—®é¢˜ã€‚</li>
<li>TR-GRPOæ˜¯GRPOçš„æœ‰æ•ˆæ‰©å±•ï¼Œé€šè¿‡è°ƒèŠ‚æ ‡è®°çº§æƒé‡è§£å†³æ¢¯åº¦æ›´æ–°ä¸å¹³è¡¡é—®é¢˜ã€‚</li>
<li>TR-GRPOåœ¨é€»è¾‘ã€æ•°å­¦å’Œæ™ºèƒ½ä½“æ¨ç†ç­‰RLVRä»»åŠ¡ä¸Šè¡¨ç°ä¼˜äºGRPOã€‚</li>
<li>TR-GRPOèƒ½ç¼“è§£æ¢¯åº¦è¿‡åº¦æ”¾å¤§é—®é¢˜ï¼ŒåŒæ—¶ä¿ç•™æœ‰ç”¨çš„å­¦ä¹ ä¿¡å·ã€‚</li>
<li>æ ‡è®°è´¡çŒ®çš„è°ƒèŠ‚åœ¨RLè®­ç»ƒä¸­å¾ˆé‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.00066">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-418b939c020cdd1749fc23c1fc164b2c" align="middle">
<img src="https://picx.zhimg.com/v2-aca167d76fd233dc6a39c153f04c5d92" align="middle">
<img src="https://picx.zhimg.com/v2-c1cac3bfe82fb4d6474a97a4c4577527" align="middle">
<img src="https://picx.zhimg.com/v2-0fa79f101b69c2f8af6d77add4fb0f14" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Decomposition-Enhanced-Training-for-Post-Hoc-Attributions-In-Language-Models"><a href="#Decomposition-Enhanced-Training-for-Post-Hoc-Attributions-In-Language-Models" class="headerlink" title="Decomposition-Enhanced Training for Post-Hoc Attributions In Language   Models"></a>Decomposition-Enhanced Training for Post-Hoc Attributions In Language   Models</h2><p><strong>Authors:Sriram Balasubramaniam, Samyadeep Basu, Koustava Goswami, Ryan Rossi, Varun Manjunatha, Roshan Santhosh, Ruiyi Zhang, Soheil Feizi, Nedim Lipka</strong></p>
<p>Large language models (LLMs) are increasingly used for long-document question answering, where reliable attribution to sources is critical for trust. Existing post-hoc attribution methods work well for extractive QA but struggle in multi-hop, abstractive, and semi-extractive settings, where answers synthesize information across passages. To address these challenges, we argue that post-hoc attribution can be reframed as a reasoning problem, where answers are decomposed into constituent units, each tied to specific context. We first show that prompting models to generate such decompositions alongside attributions improves performance. Building on this, we introduce DecompTune, a post-training method that teaches models to produce answer decompositions as intermediate reasoning steps. We curate a diverse dataset of complex QA tasks, annotated with decompositions by a strong LLM, and post-train Qwen-2.5 (7B and 14B) using a two-stage SFT + GRPO pipeline with task-specific curated rewards. Across extensive experiments and ablations, DecompTune substantially improves attribution quality, outperforming prior methods and matching or exceeding state-of-the-art frontier models. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¶Šæ¥è¶Šå¤šåœ°ç”¨äºé•¿æ–‡æ¡£é—®ç­”ï¼Œå…¶ä¸­å¯é çš„æ¥æºå½’å±å¯¹äºä¿¡ä»»è‡³å…³é‡è¦ã€‚ç°æœ‰çš„äº‹åå½’å› æ–¹æ³•åœ¨æå–å¼é—®ç­”ä¸­æ•ˆæœå¾ˆå¥½ï¼Œä½†åœ¨å¤šè·³ã€æŠ½è±¡å’ŒåŠæŠ½è±¡è®¾ç½®ä¸­é¢ä¸´å›°éš¾ï¼Œè¿™äº›è®¾ç½®ä¸­çš„ç­”æ¡ˆä¼šç»¼åˆè·¨æ®µè½çš„ä¿¡æ¯ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬è®¤ä¸ºäº‹åå½’å› å¯ä»¥é‡æ–°æ„å»ºä¸ºæ¨ç†é—®é¢˜ï¼Œå…¶ä¸­ç­”æ¡ˆè¢«åˆ†è§£ä¸ºæ„æˆå•å…ƒï¼Œæ¯ä¸ªå•å…ƒéƒ½ä¸ç‰¹å®šä¸Šä¸‹æ–‡ç›¸å…³è”ã€‚æˆ‘ä»¬é¦–å…ˆè¯æ˜ï¼Œæç¤ºæ¨¡å‹ç”Ÿæˆè¿™ç§åˆ†è§£åŒæ—¶å½’å› å¯ä»¥æé«˜æ€§èƒ½ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å¼•å…¥äº†DecompTuneï¼Œè¿™æ˜¯ä¸€ç§äº‹åè®­ç»ƒæ–¹æ³•ï¼Œå¯ä»¥æ•™å¯¼æ¨¡å‹å°†ç­”æ¡ˆåˆ†è§£ä½œä¸ºä¸­é—´æ¨ç†æ­¥éª¤æ¥äº§ç”Ÿã€‚æˆ‘ä»¬æ•´ç†äº†ä¸€ä¸ªå¤æ‚çš„é—®ç­”ä»»åŠ¡æ•°æ®é›†ï¼Œç”±å¼ºå¤§çš„LLMè¿›è¡Œåˆ†è§£æ³¨é‡Šï¼Œå¹¶ä½¿ç”¨ä»»åŠ¡ç‰¹å®šå¥–åŠ±çš„ä¸¤é˜¶æ®µSFT+GRPOç®¡é“å¯¹Qwen-2.5ï¼ˆ7Bå’Œ14Bï¼‰è¿›è¡Œäº‹åè®­ç»ƒã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒå’Œæ¶ˆèç ”ç©¶ï¼ŒDecompTuneå¤§å¹…æé«˜äº†å½’å› è´¨é‡ï¼Œä¼˜äºä»¥å‰çš„æ–¹æ³•ï¼Œå¹¶åŒ¹é…æˆ–è¶…è¿‡æœ€å‰æ²¿çš„æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.25766v2">PDF</a> Post-hoc attribution</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é•¿ç¯‡æ–‡æ¡£é—®ç­”ä¸­çš„åº”ç”¨æ—¥ç›Šå¹¿æ³›ï¼Œå¯é çš„æ¥æºå½’å±å¯¹ä¿¡ä»»è‡³å…³é‡è¦ã€‚ç°æœ‰çš„äº‹åå½’å±æ–¹æ³•åœ¨æå–å¼é—®ç­”ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å¤šè·³ã€æŠ½è±¡å’ŒåŠæå–å¼è®¾ç½®ä¸­é¢ä¸´æŒ‘æˆ˜ï¼Œè¿™äº›è®¾ç½®ä¸­çš„ç­”æ¡ˆä¼šç»¼åˆå„æ®µè½çš„ä¿¡æ¯ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºå°†äº‹åå½’å±é‡æ–°å®šä½ä¸ºæ¨ç†é—®é¢˜ï¼Œç­”æ¡ˆè¢«åˆ†è§£ä¸ºä¸ç‰¹å®šä¸Šä¸‹æ–‡ç›¸å…³è”çš„ç»„æˆéƒ¨åˆ†ã€‚æˆ‘ä»¬é¦–å…ˆå±•ç¤ºé€šè¿‡æç¤ºæ¨¡å‹ç”Ÿæˆæ­¤ç±»åˆ†è§£å¹¶ä¼´éšå½’å±å¯ä»¥æé«˜æ€§èƒ½ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å¼•å…¥äº†DecompTuneï¼Œè¿™æ˜¯ä¸€ç§äº‹åè®­ç»ƒæ–¹æ³•ï¼Œå¯æ•™å¯¼æ¨¡å‹å°†ç­”æ¡ˆåˆ†è§£ä½œä¸ºä¸­é—´æ¨ç†æ­¥éª¤ã€‚æˆ‘ä»¬ä½¿ç”¨å¼ºå¤§çš„LLMå¯¹å¤æ‚çš„é—®ç­”ä»»åŠ¡è¿›è¡Œäº†å¤šæ ·åŒ–çš„æ•°æ®é›†æ ‡æ³¨ï¼Œå¹¶é€šè¿‡ä¸¤é˜¶æ®µSFT+GRPOç®¡é“ä½¿ç”¨ç‰¹å®šä»»åŠ¡å¥–åŠ±å¯¹Qwen-2.5ï¼ˆ7Bå’Œ14Bï¼‰è¿›è¡Œäº‹åè®­ç»ƒã€‚ç»è¿‡å¹¿æ³›çš„å®éªŒå’Œå‡æŸæµ‹è¯•ï¼ŒDecompTuneå¤§å¹…æé«˜äº†å½’å±è´¨é‡ï¼Œä¼˜äºå…ˆå‰çš„æ–¹æ³•ï¼Œå¹¶è¾¾åˆ°æˆ–è¶…è¶Šäº†å‰æ²¿æ¨¡å‹çš„æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é•¿ç¯‡æ–‡æ¡£é—®ç­”ä¸­çš„å¯é æ¥æºå½’å±å¯¹ä¿¡ä»»è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰äº‹åå½’å±æ–¹æ³•åœ¨å¤æ‚é—®ç­”åœºæ™¯ä¸­è¡¨ç°ä¸è¶³ï¼Œéœ€è¦æ›´ç²¾ç»†çš„æ¨ç†å’Œåˆ†è§£èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡å°†ç­”æ¡ˆåˆ†è§£ä¸ºä¸ç‰¹å®šä¸Šä¸‹æ–‡å…³è”çš„ç»„æˆéƒ¨åˆ†ï¼Œå¯ä»¥æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>DecompTuneæ˜¯ä¸€ç§äº‹åè®­ç»ƒæ–¹æ³•ï¼Œæ—¨åœ¨æ•™å¯¼æ¨¡å‹äº§ç”Ÿä½œä¸ºä¸­é—´æ¨ç†æ­¥éª¤çš„ç­”æ¡ˆåˆ†è§£ã€‚</li>
<li>DecompTuneä½¿ç”¨ç‰¹å®šä»»åŠ¡å¥–åŠ±å¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œå¹¶é€šè¿‡ä¸¤é˜¶æ®µç®¡é“å®ç°é«˜æ•ˆè®­ç»ƒã€‚</li>
<li>DecompTuneå¤§å¹…æé«˜äº†å½’å±è´¨é‡ï¼Œä¼˜äºå…ˆå‰çš„æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.25766">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7d48fcab5076da07efb0a94247758f76" align="middle">
<img src="https://picx.zhimg.com/v2-55e128fb1ec1626eb74c951904e53a8a" align="middle">
<img src="https://picx.zhimg.com/v2-f4399ff55380180bd39a15de943db7fd" align="middle">
<img src="https://picx.zhimg.com/v2-30d52593c3f8151d871644d3c78e3ed2" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-07/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-07/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-07/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-ad829d3c956607fec250519c5d60e24b" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-07  Disentangled Concepts Speak Louder Than WordsExplainable Video Action   Recognition
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-07
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-06/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e4b55bda452811b19995ce3b75da3a95" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-06  Densemarks Learning Canonical Embeddings for Human Heads Images via   Point Tracks
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-06
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32562k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
