<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-07  Disentangled Concepts Speak Louder Than WordsExplainable Video Action   Recognition">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-ad829d3c956607fec250519c5d60e24b')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-07
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    88 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-07-æ›´æ–°"><a href="#2025-11-07-æ›´æ–°" class="headerlink" title="2025-11-07 æ›´æ–°"></a>2025-11-07 æ›´æ–°</h1><h2 id="Disentangled-Concepts-Speak-Louder-Than-Words-Explainable-Video-Action-Recognition"><a href="#Disentangled-Concepts-Speak-Louder-Than-Words-Explainable-Video-Action-Recognition" class="headerlink" title="Disentangled Concepts Speak Louder Than Words:Explainable Video Action   Recognition"></a>Disentangled Concepts Speak Louder Than Words:Explainable Video Action   Recognition</h2><p><strong>Authors:Jongseo Lee, Wooil Lee, Gyeong-Moon Park, Seong Tae Kim, Jinwoo Choi</strong></p>
<p>Effective explanations of video action recognition models should disentangle how movements unfold over time from the surrounding spatial context. However, existing methods based on saliency produce entangled explanations, making it unclear whether predictions rely on motion or spatial context. Language-based approaches offer structure but often fail to explain motions due to their tacit nature â€“ intuitively understood but difficult to verbalize. To address these challenges, we propose Disentangled Action aNd Context concept-based Explainable (DANCE) video action recognition, a framework that predicts actions through disentangled concept types: motion dynamics, objects, and scenes. We define motion dynamics concepts as human pose sequences. We employ a large language model to automatically extract object and scene concepts. Built on an ante-hoc concept bottleneck design, DANCE enforces prediction through these concepts. Experiments on four datasets â€“ KTH, Penn Action, HAA500, and UCF-101 â€“ demonstrate that DANCE significantly improves explanation clarity with competitive performance. We validate the superior interpretability of DANCE through a user study. Experimental results also show that DANCE is beneficial for model debugging, editing, and failure analysis. </p>
<blockquote>
<p>å¯¹äºè§†é¢‘åŠ¨ä½œè¯†åˆ«æ¨¡å‹çš„è§£é‡Šï¼Œåº”å½“è§£å¼€æ—¶é—´è¿›ç¨‹ä¸­åŠ¨ä½œå±•å¼€çš„æ–¹å¼ä¸å‘¨å›´ç©ºé—´ç¯å¢ƒçš„è”ç³»ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºæ˜¾è‘—æ€§æ£€æµ‹çš„æ–¹æ³•äº§ç”Ÿçš„è§£é‡Šæ˜¯çº ç¼ åœ¨ä¸€èµ·çš„ï¼Œä½¿å¾—é¢„æµ‹æ˜¯ä¾èµ–äºåŠ¨ä½œè¿˜æ˜¯ç©ºé—´ç¯å¢ƒå˜å¾—ä¸æ¸…æ¥šã€‚åŸºäºè¯­è¨€çš„æ–¹æ³•æä¾›äº†ç»“æ„ï¼Œä½†ç”±äºå…¶éšå«æ€§ï¼Œå¾€å¾€æ— æ³•è§£é‡ŠåŠ¨ä½œâ€”â€”è™½ç„¶å¯ä»¥ç›´è§‰ä¸Šç†è§£ï¼Œä½†éš¾ä»¥ç”¨è¯­è¨€è¡¨è¾¾ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºè§£è€¦åŠ¨ä½œä¸ä¸Šä¸‹æ–‡æ¦‚å¿µçš„è§£é‡Šï¼ˆDANCEï¼‰è§†é¢‘åŠ¨ä½œè¯†åˆ«æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡è§£è€¦çš„æ¦‚å¿µç±»å‹é¢„æµ‹åŠ¨ä½œï¼šè¿åŠ¨åŠ¨æ€ã€å¯¹è±¡å’Œåœºæ™¯ã€‚æˆ‘ä»¬å°†è¿åŠ¨åŠ¨æ€æ¦‚å¿µå®šä¹‰ä¸ºäººç±»å§¿åŠ¿åºåˆ—ã€‚æˆ‘ä»¬é‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è‡ªåŠ¨æå–å¯¹è±¡å’Œåœºæ™¯çš„æ¦‚å¿µã€‚åŸºäºä¸“ç”¨æ¦‚å¿µç“¶é¢ˆè®¾è®¡ï¼ŒDANCEé€šè¿‡è¿™äº›æ¦‚å¿µè¿›è¡Œé¢„æµ‹ã€‚åœ¨KTHã€Penn Actionã€HAA500å’ŒUCF-101å››ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDANCEåœ¨å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ä¸‹æ˜¾è‘—æé«˜äº†è§£é‡Šçš„æ¸…æ™°åº¦ã€‚æˆ‘ä»¬é€šè¿‡ç”¨æˆ·ç ”ç©¶éªŒè¯äº†DANCEçš„å“è¶Šå¯è§£é‡Šæ€§ã€‚å®éªŒç»“æœè¿˜è¡¨æ˜ï¼ŒDANCEå¯¹æ¨¡å‹è°ƒè¯•ã€ç¼–è¾‘å’Œæ•…éšœåˆ†ææ˜¯æœ‰ç›Šçš„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.03725v1">PDF</a> NeurIPS 2025 Spotlight paper. Project page:   <a target="_blank" rel="noopener" href="https://jong980812.github.io/DANCE/">https://jong980812.github.io/DANCE/</a></p>
<p><strong>Summary</strong><br>å¤§æ¨¡å‹åœ¨è¿›è¡Œè§†é¢‘åŠ¨ä½œè¯†åˆ«æ—¶ï¼Œåº”åˆ†è§£åŠ¨ä½œæ—¶åºä¸å‘¨å›´ç©ºé—´èƒŒæ™¯çš„äº¤äº’ã€‚ç°æœ‰åŸºäºæ˜¾è‘—æ€§æ£€æµ‹çš„æ–¹æ³•æ··æ·†äº†åŠ¨ä½œä¸ç©ºé—´èƒŒæ™¯çš„åŒºåˆ†ï¼Œéš¾ä»¥åˆ¤æ–­é¢„æµ‹ä¾æ®æ˜¯åŠ¨ä½œè¿˜æ˜¯èƒŒæ™¯ã€‚è¯­è¨€å‹æ–¹æ³•è™½æœ‰ç»“æ„ä½†éš¾ä»¥è§£é‡ŠåŠ¨ä½œç»†èŠ‚ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºæ¦‚å¿µåˆ†è§£çš„è§†é¢‘åŠ¨ä½œè¯†åˆ«æ¡†æ¶DANCEï¼Œé€šè¿‡åŠ¨ä½œåŠ¨æ€ã€ç‰©ä½“å’Œåœºæ™¯ç­‰æ¦‚å¿µç±»å‹è¿›è¡Œé¢„æµ‹ã€‚å®éªŒè¯æ˜ï¼ŒDANCEåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šæ˜¾è‘—æé«˜äº†è§£é‡Šæ¸…æ™°åº¦ï¼Œå¹¶å…·å¤‡ç«äº‰åŠ›ã€‚ç”¨æˆ·ç ”ç©¶éªŒè¯äº†å…¶å“è¶Šçš„è§£è¯»æ€§ï¼ŒåŒæ—¶æœ‰åŠ©äºæ¨¡å‹è°ƒè¯•ã€ç¼–è¾‘å’Œæ•…éšœåˆ†æã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†é¢‘åŠ¨ä½œè¯†åˆ«æ¨¡å‹åº”åŒºåˆ†åŠ¨ä½œæ—¶åºä¸ç©ºé—´èƒŒæ™¯çš„äº¤äº’ã€‚</li>
<li>ç°æœ‰æ–¹æ³•éš¾ä»¥åŒºåˆ†åŠ¨ä½œä¸ç©ºé—´èƒŒæ™¯å¯¹é¢„æµ‹çš„å½±å“ã€‚</li>
<li>è¯­è¨€å‹æ–¹æ³•è™½ç„¶å…·æœ‰ç»“æ„ï¼Œä½†éš¾ä»¥è¯¦ç»†è§£é‡ŠåŠ¨ä½œã€‚</li>
<li>DANCEæ¡†æ¶é€šè¿‡æ¦‚å¿µç±»å‹ï¼ˆåŠ¨ä½œåŠ¨æ€ã€ç‰©ä½“å’Œåœºæ™¯ï¼‰è¿›è¡Œé¢„æµ‹ã€‚</li>
<li>DANCEåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æ¸…æ™°ä¸”å…·ç«äº‰åŠ›çš„è§£é‡Šæ€§èƒ½ã€‚</li>
<li>ç”¨æˆ·ç ”ç©¶éªŒè¯äº†DANCEçš„å“è¶Šè§£è¯»æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.03725">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9e4f4cb6cc8a05f09b6c1a8bf4a99851" align="middle">
<img src="https://picx.zhimg.com/v2-013654c7582dde4f6474e06e833b4125" align="middle">
<img src="https://picx.zhimg.com/v2-f8f579c59fe0063c70bd2ae0d0724e36" align="middle">
<img src="https://picx.zhimg.com/v2-8f05785c0c07fb9a435a0d8d04e84276" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Watermarking-Large-Language-Models-in-Europe-Interpreting-the-AI-Act-in-Light-of-Technology"><a href="#Watermarking-Large-Language-Models-in-Europe-Interpreting-the-AI-Act-in-Light-of-Technology" class="headerlink" title="Watermarking Large Language Models in Europe: Interpreting the AI Act in   Light of Technology"></a>Watermarking Large Language Models in Europe: Interpreting the AI Act in   Light of Technology</h2><p><strong>Authors:Thomas Souverain</strong></p>
<p>To foster trustworthy Artificial Intelligence (AI) within the European Union, the AI Act requires providers to mark and detect the outputs of their general-purpose models. The Article 50 and Recital 133 call for marking methods that are â€˜â€™sufficiently reliable, interoperable, effective and robustâ€™â€™. Yet, the rapidly evolving and heterogeneous landscape of watermarks for Large Language Models (LLMs) makes it difficult to determine how these four standards can be translated into concrete and measurable evaluations. Our paper addresses this challenge, anchoring the normativity of European requirements in the multiplicity of watermarking techniques. Introducing clear and distinct concepts on LLM watermarking, our contribution is threefold. (1) Watermarking Categorisation: We propose an accessible taxonomy of watermarking methods according to the stage of the LLM lifecycle at which they are applied - before, during, or after training, and during next-token distribution or sampling. (2) Watermarking Evaluation: We interpret the EU AI Actâ€™s requirements by mapping each criterion with state-of-the-art evaluations on robustness and detectability of the watermark, and of quality of the LLM. Since interoperability remains largely untheorised in LLM watermarking research, we propose three normative dimensions to frame its assessment. (3) Watermarking Comparison: We compare current watermarking methods for LLMs against the operationalised European criteria and show that no approach yet satisfies all four standards. Encouraged by emerging empirical tests, we recommend further research into watermarking directly embedded within the low-level architecture of LLMs. </p>
<blockquote>
<p>ä¸ºäº†åœ¨æ¬§æ´²è”ç›Ÿå†…éƒ¨ä¿ƒè¿›å¯ä¿¡çš„äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰ï¼ŒAIæ³•æ¡ˆè¦æ±‚æä¾›è€…å¯¹å…¶é€šç”¨æ¨¡å‹çš„è¾“å‡ºè¿›è¡Œæ ‡è®°å’Œæ£€æµ‹ã€‚ç¬¬50æ¡å’Œç¬¬133æ¡å‘¼åé‡‡ç”¨â€œè¶³å¤Ÿå¯é ã€å…¼å®¹æ€§å¼ºã€æœ‰æ•ˆä¸”ç¨³å¥â€çš„æ ‡è®°æ–¹æ³•ã€‚ç„¶è€Œï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ°´å°çš„å¿«é€Ÿå‘å±•å’Œå¤šæ ·æ€§ä½¿å¾—è¿™å››é¡¹æ ‡å‡†éš¾ä»¥è½¬åŒ–ä¸ºå…·ä½“å’Œå¯è¡¡é‡çš„è¯„ä¼°ã€‚æˆ‘ä»¬çš„è®ºæ–‡åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œå°†æ¬§æ´²è¦æ±‚çš„è§„èŒƒæ€§æ ¹æ¤äºæ°´å°æŠ€æœ¯çš„å¤šæ ·æ€§ä¸­ã€‚æˆ‘ä»¬æå‡ºäº†å…³äºLLMæ°´å°çš„æ¸…æ™°å’Œæ˜ç¡®çš„æ¦‚å¿µï¼Œæˆ‘ä»¬çš„è´¡çŒ®ä¸»è¦ä½“ç°åœ¨ä¸‰ä¸ªæ–¹é¢ã€‚ï¼ˆ1ï¼‰æ°´å°åˆ†ç±»ï¼šæˆ‘ä»¬æ ¹æ®æ°´å°æ–¹æ³•åº”ç”¨çš„LLMç”Ÿå‘½å‘¨æœŸé˜¶æ®µï¼ˆå³åœ¨è®­ç»ƒä¹‹å‰ã€æœŸé—´æˆ–ä¹‹åï¼Œä»¥åŠåœ¨ä¸‹ä¸€ä¸ªä»¤ç‰Œåˆ†å‘æˆ–é‡‡æ ·æœŸé—´ï¼‰æå‡ºäº†ä¸€ä¸ªæ˜“äºç†è§£çš„æ°´å°åˆ†ç±»æ³•ã€‚ï¼ˆ2ï¼‰æ°´å°è¯„ä¼°ï¼šæˆ‘ä»¬é€šè¿‡å°†æ¯ä¸ªæ ‡å‡†ä¸æœ€æ–°æ°´å°çš„ç¨³å¥æ€§å’Œæ£€æµ‹æ€§è¯„ä¼°ä»¥åŠLLMçš„è´¨é‡è¯„ä¼°ç›¸å¯¹åº”æ¥è§£é‡Šæ¬§ç›ŸAIæ³•æ¡ˆçš„è¦æ±‚ã€‚ç”±äºäº’æ“ä½œæ€§åœ¨LLMæ°´å°ç ”ç©¶ä¸­ä»ç„¶åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæœªè¢«ç†è®ºåŒ–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸‰ä¸ªè§„èŒƒæ€§ç»´åº¦æ¥å¯¹å…¶è¿›è¡Œè¯„ä¼°ã€‚ï¼ˆ3ï¼‰æ°´å°æ¯”è¾ƒï¼šæˆ‘ä»¬å°†å½“å‰çš„LLMæ°´å°æ–¹æ³•ä¸æ¬§æ´²çš„æ“ä½œæ ‡å‡†è¿›è¡Œäº†æ¯”è¾ƒï¼Œå¹¶è¡¨æ˜ç›®å‰æ²¡æœ‰ä»»ä½•æ–¹æ³•èƒ½æ»¡è¶³æ‰€æœ‰å››é¡¹æ ‡å‡†ã€‚æˆ‘ä»¬é¼“åŠ±åŸºäºæ–°å…´çš„å®éªŒæµ‹è¯•è¿›ä¸€æ­¥çš„ç ”ç©¶ï¼Œå»ºè®®åœ¨æ°´å°ä¸­ç›´æ¥åµŒå…¥LLMçš„ä½çº§æ¶æ„ä¸­ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.03641v1">PDF</a> 17 pages, 2 Tables and 2 Pictures</p>
<p><strong>Summary</strong><br>     ä¸ºæ¨åŠ¨æ¬§ç›Ÿå†…å¯ä¿¡äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰çš„å‘å±•ï¼ŒAIæ³•æ¡ˆè¦æ±‚æä¾›è€…å¯¹å…¶é€šç”¨æ¨¡å‹è¾“å‡ºè¿›è¡Œæ ‡è®°å’Œæ£€æµ‹ã€‚ç¬¬50æ¡å’Œç¬¬133æ¡æè®®è¦æ±‚æ ‡è®°æ–¹æ³•å…·å¤‡è¶³å¤Ÿå¯é æ€§ã€å¯äº’æ“ä½œæ€§ã€æœ‰æ•ˆæ€§å’Œç¨³å¥æ€§ã€‚ç„¶è€Œï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ°´å°æŠ€æœ¯çš„è¿…é€Ÿå‘å±•å’Œå¤šæ ·æ€§ä½¿å¾—è¿™å››é¡¹æ ‡å‡†éš¾ä»¥è½¬åŒ–ä¸ºå…·ä½“å¯è¡¡é‡çš„è¯„ä¼°æ ‡å‡†ã€‚æœ¬æ–‡åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œå°†æ¬§æ´²æ°´å°è¦æ±‚ä¸å¤šç§æ°´å°æŠ€æœ¯ç›¸ç»“åˆï¼Œæå‡ºæ˜ç¡®ä¸”ä¸åŒçš„LLMæ°´å°æ¦‚å¿µã€‚æˆ‘ä»¬è´¡çŒ®æœ‰ä¸‰ç‚¹ï¼šï¼ˆ1ï¼‰æ°´å°åˆ†ç±»ï¼šæ ¹æ®LLMç”Ÿå‘½å‘¨æœŸçš„åº”ç”¨é˜¶æ®µï¼ˆè®­ç»ƒå‰ã€è®­ç»ƒä¸­ã€è®­ç»ƒåï¼Œä»¥åŠä»¤ç‰Œåˆ†å‘æˆ–é‡‡æ ·æ—¶ï¼‰ï¼Œæå‡ºæ˜“äºç†è§£çš„æ°´å°æ–¹æ³•åˆ†ç±»ã€‚ï¼ˆ2ï¼‰æ°´å°è¯„ä¼°ï¼šé€šè¿‡æ˜ å°„æ¯ä¸ªæ ‡å‡†ä¸å½“å‰æ°´å°çš„ç¨³å¥æ€§å’Œæ£€æµ‹æ€§çš„è´¨é‡è¯„ä¼°ï¼Œè§£è¯»æ¬§ç›ŸAIæ³•æ¡ˆçš„è¦æ±‚ã€‚ç”±äºäº’æ“ä½œæ€§åœ¨LLMæ°´å°ç ”ç©¶ä¸­å°šæœªè¢«å……åˆ†æ¢è®¨ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸‰ä¸ªè§„èŒƒæ€§ç»´åº¦æ¥è¯„ä¼°å…¶è¯„ä¼°ã€‚ï¼ˆ3ï¼‰æ°´å°æ¯”è¾ƒï¼šæ ¹æ®æ“ä½œåŒ–çš„æ¬§æ´²æ ‡å‡†å¯¹ç°æœ‰çš„LLMæ°´å°æ–¹æ³•è¿›è¡Œå¯¹æ¯”ï¼Œå¹¶å‘ç°å°šæ— æ–¹æ³•æ»¡è¶³æ‰€æœ‰æ ‡å‡†ã€‚æˆ‘ä»¬é¼“åŠ±åŸºäºæ–°å…´å®è¯æµ‹è¯•è¿›è¡Œè¿›ä¸€æ­¥ç ”ç©¶ï¼Œå°†æ°´å°ç›´æ¥åµŒå…¥LLMçš„ä½å±‚æ¬¡æ¶æ„ä¸­ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¬§ç›ŸAIæ³•æ¡ˆè¦æ±‚LLMæä¾›å•†å¯¹å…¶æ¨¡å‹è¾“å‡ºè¿›è¡Œæ ‡è®°å’Œæ£€æµ‹ï¼Œä»¥ç¡®ä¿AIçš„å¯é æ€§ã€å®‰å…¨æ€§å’Œé€æ˜åº¦ã€‚</li>
<li>å¯¹æ°´å°æ–¹æ³•çš„è¯„ä¼°éœ€è¦æ»¡è¶³å››ä¸ªæ ‡å‡†ï¼šå¯é æ€§ã€å¯äº’æ“ä½œæ€§ã€æœ‰æ•ˆæ€§å’Œç¨³å¥æ€§ã€‚</li>
<li>LLMæ°´å°æŠ€æœ¯çš„å¤šæ ·æ€§å’Œå¿«é€Ÿå‘å±•ä½¿å¾—ç¡®å®šå…·ä½“è¯„ä¼°æ ‡å‡†å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªå…³äºæ°´å°æ–¹æ³•çš„åˆ†ç±»æ¡†æ¶ï¼Œæ ¹æ®å…¶åœ¨LLMç”Ÿå‘½å‘¨æœŸçš„ä¸åŒé˜¶æ®µåº”ç”¨è¿›è¡Œåˆ†ç±»ã€‚</li>
<li>å¯¹æ¬§ç›ŸAIæ³•æ¡ˆçš„è¦æ±‚è¿›è¡Œäº†è¯¦ç»†è§£è¯»ï¼Œå¹¶æå‡ºä¸€ä¸ªé’ˆå¯¹æ°´å°è¯„ä¼°çš„äº’æ“ä½œæ€§è¯„ä¼°æ¡†æ¶ã€‚</li>
<li>ç›®å‰æ²¡æœ‰ä¸€ç§æ°´å°æ–¹æ³•èƒ½æ»¡è¶³æ‰€æœ‰çš„æ¬§æ´²æ ‡å‡†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.03641">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a21c18a0e4f5962e5eca27d4d91a6e84" align="middle">
<img src="https://picx.zhimg.com/v2-b66753f53adc5eaa1c9f6a1cac043865" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Towards-Transparent-Stance-Detection-A-Zero-Shot-Approach-Using-Implicit-and-Explicit-Interpretability"><a href="#Towards-Transparent-Stance-Detection-A-Zero-Shot-Approach-Using-Implicit-and-Explicit-Interpretability" class="headerlink" title="Towards Transparent Stance Detection: A Zero-Shot Approach Using   Implicit and Explicit Interpretability"></a>Towards Transparent Stance Detection: A Zero-Shot Approach Using   Implicit and Explicit Interpretability</h2><p><strong>Authors:Apoorva Upadhyaya, Wolfgang Nejdl, Marco Fisichella</strong></p>
<p>Zero-Shot Stance Detection (ZSSD) identifies the attitude of the post toward unseen targets. Existing research using contrastive, meta-learning, or data augmentation suffers from generalizability issues or lack of coherence between text and target. Recent works leveraging large language models (LLMs) for ZSSD focus either on improving unseen target-specific knowledge or generating explanations for stance analysis. However, most of these works are limited by their over-reliance on explicit reasoning, provide coarse explanations that lack nuance, and do not explicitly model the reasoning process, making it difficult to interpret the modelâ€™s predictions. To address these issues, in our study, we develop a novel interpretable ZSSD framework, IRIS. We provide an interpretable understanding of the attitude of the input towards the target implicitly based on sequences within the text (implicit rationales) and explicitly based on linguistic measures (explicit rationales). IRIS considers stance detection as an information retrieval ranking task, understanding the relevance of implicit rationales for different stances to guide the model towards correct predictions without requiring the ground-truth of rationales, thus providing inherent interpretability. In addition, explicit rationales based on communicative features help decode the emotional and cognitive dimensions of stance, offering an interpretable understanding of the authorâ€™s attitude towards the given target. Extensive experiments on the benchmark datasets of VAST, EZ-STANCE, P-Stance, and RFD using 50%, 30%, and even 10% training data prove the generalizability of our model, benefiting from the proposed architecture and interpretable design. </p>
<blockquote>
<p>é›¶æ ·æœ¬ç«‹åœºæ£€æµ‹ï¼ˆZSSDï¼‰èƒ½å¤Ÿè¯†åˆ«å¸–å­å¯¹æœªè§ç›®æ ‡çš„ç«‹åœºã€‚ç°æœ‰ç ”ç©¶é‡‡ç”¨å¯¹æ¯”ã€å…ƒå­¦ä¹ æˆ–æ•°æ®å¢å¼ºçš„æ–¹æ³•å­˜åœ¨æ³›åŒ–é—®é¢˜æˆ–æ–‡æœ¬ä¸ç›®æ ‡ä¹‹é—´ç¼ºä¹è¿è´¯æ€§ã€‚æœ€è¿‘åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡ŒZSSDçš„å·¥ä½œä¸»è¦é›†ä¸­åœ¨æ”¹è¿›å¯¹æœªè§ç›®æ ‡ç‰¹å®šçŸ¥è¯†çš„è·å–æˆ–ä¸ºç«‹åœºåˆ†æç”Ÿæˆè§£é‡Šã€‚ç„¶è€Œï¼Œè¿™äº›å·¥ä½œå¤§å¤šè¿‡äºä¾èµ–æ˜¾æ€§æ¨ç†ï¼Œæä¾›çš„è§£é‡Šè¾ƒä¸ºç²—ç•¥ï¼Œç¼ºä¹ç»†å¾®å·®åˆ«ï¼Œå¹¶ä¸”æ²¡æœ‰æ˜¾å¼åœ°å»ºæ¨¡æ¨ç†è¿‡ç¨‹ï¼Œä½¿å¾—æ¨¡å‹é¢„æµ‹éš¾ä»¥è§£é‡Šã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œåœ¨æˆ‘ä»¬çš„ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªæ–°é¢–çš„å¯è§£é‡Šçš„ZSSDæ¡†æ¶IRISã€‚æˆ‘ä»¬åŸºäºæ–‡æœ¬ä¸­çš„åºåˆ—ï¼ˆéšå¼ç†ç”±ï¼‰å’ŒåŸºäºè¯­è¨€æµ‹é‡çš„ï¼ˆæ˜¾å¼ç†ç”±ï¼‰éšå¼å’Œæ˜¾å¼åœ°ç†è§£è¾“å…¥å¯¹ç›®æ ‡çš„ç«‹åœºã€‚IRISå°†ç«‹åœºæ£€æµ‹è§†ä¸ºä¿¡æ¯æ£€ç´¢æ’åä»»åŠ¡ï¼Œç†è§£ä¸åŒç«‹åœºä¸éšå¼ç†ç”±çš„ç›¸å…³æ€§ï¼Œä»¥æŒ‡å¯¼æ¨¡å‹åšå‡ºæ­£ç¡®é¢„æµ‹ï¼Œæ— éœ€ç†ç”±çš„çœŸç›¸ï¼Œä»è€Œæä¾›äº†å›ºæœ‰çš„å¯è§£é‡Šæ€§ã€‚æ­¤å¤–ï¼ŒåŸºäºæ²Ÿé€šç‰¹å¾çš„æ˜¾å¼ç†ç”±æœ‰åŠ©äºè§£ç ç«‹åœºçš„æƒ…æ„Ÿè®¤çŸ¥ç»´åº¦ï¼Œæä¾›äº†å¯¹ä½œè€…å¯¹æ‰€ç»™ç›®æ ‡çš„æ€åº¦çš„å¯è§£é‡Šç†è§£ã€‚åœ¨VASTã€EZ-STANCEã€P-Stanceå’ŒRFDåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¯æ˜äº†æˆ‘ä»¬çš„æ¨¡å‹åœ¨è®­ç»ƒæ•°æ®ä¸º50%ã€30%ç”šè‡³ä½è‡³10%æ—¶çš„æ³›åŒ–èƒ½åŠ›å¾—ç›Šäºæ‰€æå‡ºçš„æ¶æ„å’Œå¯è§£é‡Šè®¾è®¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.03635v1">PDF</a> Accepted in AAAI CONFERENCE ON WEB AND SOCIAL MEDIA (ICWSM 2026)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é›¶æ ·æœ¬ç«‹åœºæ£€æµ‹ï¼ˆZSSDï¼‰çš„é—®é¢˜å’ŒæŒ‘æˆ˜ï¼Œç°æœ‰ç ”ç©¶åœ¨ä¸€èˆ¬åŒ–èƒ½åŠ›å’Œæ–‡æœ¬ä¸ç›®æ ‡ä¹‹é—´çš„è¿è´¯æ€§æ–¹é¢å­˜åœ¨é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç ”ç©¶è€…å¼€å‘äº†ä¸€ç§æ–°çš„å¯è§£é‡Šçš„ZSSDæ¡†æ¶â€”â€”IRISï¼Œè¯¥æ¡†æ¶èƒ½å¤ŸåŸºäºæ–‡æœ¬ä¸­çš„åºåˆ—ï¼ˆéšå¼ç†ç”±ï¼‰å’Œè¯­è¨€åº¦é‡ï¼ˆæ˜¾å¼ç†ç”±ï¼‰æ¥éšå¼å’Œæ˜¾å¼åœ°ç†è§£è¾“å…¥å¯¹ç›®æ ‡çš„ç«‹åœºã€‚IRISå°†ç«‹åœºæ£€æµ‹è§†ä¸ºä¿¡æ¯æ£€ç´¢æ’åä»»åŠ¡ï¼Œç†è§£ä¸åŒç«‹åœºä¸éšå¼ç†ç”±çš„ç›¸å…³æ€§ï¼Œæ— éœ€å¯¹ç†ç”±çš„åœ°é¢çœŸå®æƒ…å†µè¿›è¡Œè¦æ±‚ï¼Œä»è€Œæä¾›äº†å›ºæœ‰çš„å¯è§£é‡Šæ€§ã€‚æ­¤å¤–ï¼ŒåŸºäºæ²Ÿé€šç‰¹å¾çš„æ˜¾å¼ç†ç”±æœ‰åŠ©äºè§£ç ç«‹åœºçš„æƒ…æ„Ÿè®¤çŸ¥ç»´åº¦ï¼Œæä¾›å¯¹ä½œè€…å¯¹äºç»™å®šç›®æ ‡çš„æ€åº¦çš„å¯è§£é‡Šç†è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ZSSDæ—¨åœ¨è¯†åˆ«å¸–å­å¯¹æœªè§ç›®æ ‡çš„ç«‹åœºã€‚</li>
<li>ç°æœ‰ç ”ç©¶åœ¨ä¸€èˆ¬åŒ–èƒ½åŠ›å’Œæ–‡æœ¬ä¸ç›®æ ‡çš„è¿è´¯æ€§æ–¹é¢å­˜åœ¨é—®é¢˜ã€‚</li>
<li>IRISæ¡†æ¶é€šè¿‡éšå¼å’Œæ˜¾å¼ç†ç”±æä¾›äº†ç«‹åœºçš„å¯è§£é‡Šç†è§£ã€‚</li>
<li>IRISå°†ç«‹åœºæ£€æµ‹è§†ä¸ºä¿¡æ¯æ£€ç´¢æ’åä»»åŠ¡ï¼Œç†è§£éšå¼ç†ç”±ä¸ä¸åŒç«‹åœºçš„ç›¸å…³æ€§ã€‚</li>
<li>æ— éœ€å¯¹ç†ç”±çš„åœ°é¢çœŸå®æƒ…å†µè¿›è¡Œè¦æ±‚ï¼Œæä¾›äº†å›ºæœ‰çš„å¯è§£é‡Šæ€§ã€‚</li>
<li>åŸºäºæ²Ÿé€šç‰¹å¾çš„æ˜¾å¼ç†ç”±æœ‰åŠ©äºè§£ç ç«‹åœºçš„æƒ…æ„Ÿè®¤çŸ¥ç»´åº¦ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.03635">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-32b459ef2675ee81d754471e97848492" align="middle">
<img src="https://picx.zhimg.com/v2-70cea5b0b02aec0f1870c055797c502b" align="middle">
<img src="https://picx.zhimg.com/v2-fdf5e74f42a61749e843fdd1bf56eab3" align="middle">
<img src="https://picx.zhimg.com/v2-a6a51d52395187fa8c8a210b89536923" align="middle">
<img src="https://picx.zhimg.com/v2-b69a57f0cb592ed046aa96a9df19d81a" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="TabGemma-Text-Based-Tabular-ICL-via-LLM-using-Continued-Pretraining-and-Retrieval"><a href="#TabGemma-Text-Based-Tabular-ICL-via-LLM-using-Continued-Pretraining-and-Retrieval" class="headerlink" title="TabGemma: Text-Based Tabular ICL via LLM using Continued Pretraining and   Retrieval"></a>TabGemma: Text-Based Tabular ICL via LLM using Continued Pretraining and   Retrieval</h2><p><strong>Authors:GÃ¼nther Schindler, Maximilian Schambach, Michael Medek, Sam Thelin</strong></p>
<p>We study LLMs for tabular prediction with mixed text, numeric, and categorical fields. We introduce TabGemma, a schema-agnostic in-context learner that treats rows as sequences and tackles two practical hurdles when adapting pretrained LLMs for tabular predictions: unstable numeric tokenization and limited context size. We propose to canonicalize numbers via signed scientific notation and continue pretraining of a 12B Gemma 3 model with a target imputation objective using a large-scale real world dataset. For inference, we use a compact n-gram-based retrieval to select informative exemplars that fit within a 128k-token window.   On semantically rich benchmarks, TabGemma establishes a new state of the art on classification across low- and high-data regimes and improves monotonically with more context rows. For regression, it is competitive at small sample sizes but trails conventional approaches as data grows. Our results show that LLMs can be effective tabular in-context learners on highly semantic tasks when paired with dedicated numeric handling and context retrieval, while motivating further advances in numeric modeling and long-context scaling. </p>
<blockquote>
<p>æˆ‘ä»¬ç ”ç©¶äº†å…·æœ‰æ··åˆæ–‡æœ¬ã€æ•°å­—å’Œåˆ†ç±»å­—æ®µçš„è¡¨æ ¼é¢„æµ‹çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚æˆ‘ä»¬æ¨å‡ºäº†TabGemmaï¼Œè¿™æ˜¯ä¸€ç§ç‹¬ç«‹äºæ¨¡å¼çš„ä¸Šä¸‹æ–‡å­¦ä¹ å™¨ï¼Œå®ƒå°†è¡Œè§†ä¸ºåºåˆ—ï¼Œå¹¶è§£å†³äº†åœ¨å°†é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹é€‚åº”äºè¡¨æ ¼é¢„æµ‹æ—¶æ‰€é¢ä¸´çš„ä¸¤ä¸ªå®é™…éšœç¢ï¼šä¸ç¨³å®šçš„æ•°å­—æ ‡è®°åŒ–å’Œæœ‰é™çš„ä¸Šä¸‹æ–‡å¤§å°ã€‚æˆ‘ä»¬æå‡ºé€šè¿‡æœ‰ç¬¦å·ç§‘å­¦è®¡æ•°æ³•å¯¹æ•°å­—è¿›è¡Œæ ‡å‡†åŒ–ï¼Œå¹¶ä½¿ç”¨å¤§è§„æ¨¡ç°å®ä¸–ç•Œæ•°æ®é›†å¯¹ç›®æ ‡æ’è¡¥ç›®æ ‡è¿›è¡Œ12B Gemma 3æ¨¡å‹çš„é¢„è®­ç»ƒã€‚å¯¹äºæ¨ç†ï¼Œæˆ‘ä»¬ä½¿ç”¨åŸºäºç´§å‡‘nå…ƒç»„çš„æ£€ç´¢æ¥é€‰æ‹©ç¬¦åˆ128kä»¤ç‰Œçª—å£çš„ç¤ºä¾‹ã€‚åœ¨è¯­ä¹‰ä¸°å¯Œçš„åŸºå‡†æµ‹è¯•ä¸­ï¼ŒTabGemmaåœ¨ä½æ•°æ®å’Œé«˜æ•°æ®æƒ…å†µä¸‹å»ºç«‹äº†åˆ†ç±»æ–¹é¢çš„æœ€æ–°æ°´å¹³ï¼Œéšç€ä¸Šä¸‹æ–‡è¡Œçš„å¢åŠ è€Œå‘ˆç°å•è°ƒæ”¹è¿›ã€‚å¯¹äºå›å½’ï¼Œå®ƒåœ¨å°æ ·æœ¬é‡æ—¶å…·æœ‰ç«äº‰åŠ›ï¼Œä½†éšç€æ•°æ®é‡çš„å¢é•¿è€Œè½åäºä¼ ç»Ÿæ–¹æ³•ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå½“ä¸ä¸“é—¨çš„æ•°å€¼å¤„ç†å’Œä¸Šä¸‹æ–‡æ£€ç´¢ç›¸ç»“åˆæ—¶ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹å¯ä»¥åœ¨é«˜åº¦è¯­ä¹‰çš„ä»»åŠ¡ä¸Šæˆä¸ºæœ‰æ•ˆçš„è¡¨æ ¼ä¸Šä¸‹æ–‡å­¦ä¹ è€…ï¼Œè¿™åŒæ—¶ä¹Ÿæ¨åŠ¨äº†æ•°å€¼å»ºæ¨¡å’Œé•¿ä¸Šä¸‹æ–‡æ‰©å±•çš„è¿›ä¸€æ­¥è¿›å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.03570v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LLMsåœ¨è¡¨æ ¼é¢„æµ‹æ–¹é¢è¿›è¡Œäº†æ·±å…¥ç ”ç©¶ï¼Œé’ˆå¯¹æ–‡æœ¬ã€æ•°å­—å’Œåˆ†ç±»å­—æ®µçš„æ··åˆæ•°æ®ï¼Œæå‡ºäº†TabGemmaæ¨¡å‹ã€‚è¯¥æ¨¡å‹æ˜¯ä¸€ç§æ¨¡å¼æ— å…³çš„ä¸Šä¸‹æ–‡å­¦ä¹ è€…ï¼Œå°†è¡Œè§†ä¸ºåºåˆ—ï¼Œè§£å†³äº†é€‚åº”é¢„è®­ç»ƒLLMsè¿›è¡Œè¡¨æ ¼é¢„æµ‹æ—¶çš„ä¸¤ä¸ªå®é™…é—®é¢˜ï¼šä¸ç¨³å®šçš„æ•°å€¼æ ‡è®°åŒ–å’Œæœ‰é™çš„ä¸Šä¸‹æ–‡å¤§å°ã€‚é€šè¿‡ç¬¦å·ç§‘å­¦è®°æ•°æ³•å¯¹æ•°å­—è¿›è¡Œè§„èŒƒåŒ–ï¼Œå¹¶ç»§ç»­ä½¿ç”¨å¤§è§„æ¨¡ç°å®ä¸–ç•Œæ•°æ®é›†å¯¹12B Gemma 3æ¨¡å‹è¿›è¡Œç›®æ ‡æ’å€¼ç›®æ ‡çš„é¢„è®­ç»ƒã€‚å¯¹äºæ¨ç†ï¼Œä½¿ç”¨åŸºäºç´§å‡‘n-gramçš„æ£€ç´¢æ–¹æ³•é€‰æ‹©ä¿¡æ¯æ ·æœ¬ï¼Œä»¥é€‚åº”128kä»¤ç‰Œçª—å£ã€‚TabGemmaåœ¨è¯­ä¹‰ä¸°å¯Œçš„åŸºå‡†æµ‹è¯•ä¸Šå»ºç«‹äº†åˆ†ç±»çš„æ–°å›½å®¶æ ‡å‡†ï¼Œå¹¶åœ¨ä½æ•°æ®å’Œé«˜æ•°æ®çŠ¶æ€ä¸‹è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ã€‚å¯¹äºå›å½’ä»»åŠ¡ï¼Œåœ¨å°æ ·æœ¬æ—¶å…·æœ‰ç«äº‰åŠ›ï¼Œä½†éšç€æ•°æ®å¢é•¿ç¨æ˜¾è½åä¼ ç»Ÿæ–¹æ³•ã€‚ç»“æœè¡¨æ˜ï¼Œå½“ä¸ä¸“é—¨çš„æ•°å€¼å¤„ç†å’Œä¸Šä¸‹æ–‡æ£€ç´¢ç›¸ç»“åˆæ—¶ï¼ŒLLMsåœ¨é«˜åº¦è¯­ä¹‰ä»»åŠ¡ä¸Šå¯ä½œä¸ºæœ‰æ•ˆçš„è¡¨æ ¼ä¸Šä¸‹æ–‡å­¦ä¹ è€…ï¼ŒåŒæ—¶æ¨åŠ¨äº†æ•°å€¼å»ºæ¨¡å’Œé•¿ä¸Šä¸‹æ–‡æ‰©å±•æ–¹é¢çš„è¿›ä¸€æ­¥æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶LLMsåœ¨è¡¨æ ¼é¢„æµ‹æ–¹é¢çš„åº”ç”¨ï¼Œé’ˆå¯¹åŒ…å«æ–‡æœ¬ã€æ•°å­—å’Œåˆ†ç±»å­—æ®µçš„æ··åˆæ•°æ®ã€‚</li>
<li>å¼•å…¥TabGemmaæ¨¡å‹ï¼Œä¸€ç§æ¨¡å¼æ— å…³çš„åœ¨ä¸Šä¸‹æ–‡ä¸­çš„å­¦ä¹ è€…ï¼Œå¯ä»¥è§£å†³é¢„è®­ç»ƒLLMsåœ¨è¿›è¡Œè¡¨æ ¼é¢„æµ‹æ—¶é‡åˆ°çš„ä¸ç¨³å®šçš„æ•°å€¼æ ‡è®°åŒ–å’Œæœ‰é™çš„ä¸Šä¸‹æ–‡å¤§å°é—®é¢˜ã€‚</li>
<li>é€šè¿‡ç¬¦å·ç§‘å­¦è®°æ•°æ³•å¯¹æ•°å­—è¿›è¡Œè§„èŒƒåŒ–å¤„ç†ã€‚</li>
<li>ä½¿ç”¨å¤§è§„æ¨¡ç°å®ä¸–ç•Œæ•°æ®é›†å¯¹æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒã€‚</li>
<li>ä½¿ç”¨åŸºäºç´§å‡‘n-gramçš„æ£€ç´¢æ–¹æ³•é€‰æ‹©ä¿¡æ¯æ ·æœ¬ä»¥é€‚åº”ä¸Šä¸‹æ–‡é™åˆ¶ã€‚</li>
<li>TabGemmaåœ¨è¯­ä¹‰ä¸°å¯Œçš„åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†åˆ†ç±»çš„æ–°å›½å®¶æ ‡å‡†ï¼Œå¹¶åœ¨ä¸åŒæ•°æ®è§„æ¨¡ä¸‹è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>åœ¨å›å½’ä»»åŠ¡ä¸­ï¼ŒTabGemmaå…·æœ‰ç«äº‰åŠ›ä½†ä»éœ€è¿›ä¸€æ­¥æé«˜ä»¥åº”å¯¹å¤§é‡æ•°æ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.03570">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8bbfb91ae75db7eb4efc27107734d674" align="middle">
<img src="https://picx.zhimg.com/v2-f09b44cf941c975035a4cfae585da56f" align="middle">
<img src="https://picx.zhimg.com/v2-b27e1998f59928f5ad5002d8a6991078" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Uncovering-Code-Insights-Leveraging-GitHub-Artifacts-for-Deeper-Code-Understanding"><a href="#Uncovering-Code-Insights-Leveraging-GitHub-Artifacts-for-Deeper-Code-Understanding" class="headerlink" title="Uncovering Code Insights: Leveraging GitHub Artifacts for Deeper Code   Understanding"></a>Uncovering Code Insights: Leveraging GitHub Artifacts for Deeper Code   Understanding</h2><p><strong>Authors:Ziv Nevo, Orna Raz, Karen Yorav</strong></p>
<p>Understanding the purpose of source code is a critical task in software maintenance, onboarding, and modernization. While large language models (LLMs) have shown promise in generating code explanations, they often lack grounding in the broader software engineering context. We propose a novel approach that leverages natural language artifacts from GitHub â€“ such as pull request descriptions, issue descriptions and discussions, and commit messages â€“ to enhance LLM-based code understanding. Our system consists of three components: one that extracts and structures relevant GitHub context, another that uses this context to generate high-level explanations of the codeâ€™s purpose, and a third that validates the explanation. We implemented this as a standalone tool, as well as a server within the Model Context Protocol (MCP), enabling integration with other AI-assisted development tools. Our main use case is that of enhancing a standard LLM-based code explanation with code insights that our system generates. To evaluate explanationsâ€™ quality, we conducted a small scale user study, with developers of several open projects, as well as developers of proprietary projects. Our user study indicates that when insights are generated they often are helpful and non trivial, and are free from hallucinations. </p>
<blockquote>
<p>ç†è§£æºä»£ç çš„ç›®çš„æ˜¯è½¯ä»¶ç»´æŠ¤ã€å…¥èŒå’Œç°ä»£åŒ–è¿‡ç¨‹ä¸­çš„ä¸€é¡¹å…³é”®ä»»åŠ¡ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç”Ÿæˆä»£ç è§£é‡Šæ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å®ƒä»¬å¾€å¾€ç¼ºä¹æ›´å¹¿æ³›çš„è½¯ä»¶å·¥ç¨‹èƒŒæ™¯çŸ¥è¯†ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œåˆ©ç”¨GitHubä¸Šçš„è‡ªç„¶è¯­è¨€åˆ¶å“ï¼ˆå¦‚æ‹‰å–è¯·æ±‚æè¿°ã€é—®é¢˜æè¿°å’Œè®¨è®ºã€æäº¤æ¶ˆæ¯ï¼‰æ¥å¢å¼ºåŸºäºLLMçš„ä»£ç ç†è§£èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿç”±ä¸‰ä¸ªç»„ä»¶æ„æˆï¼šä¸€ä¸ªç”¨äºæå–å’Œç»“æ„åŒ–ç›¸å…³çš„GitHubä¸Šä¸‹æ–‡ï¼Œå¦ä¸€ä¸ªä½¿ç”¨æ­¤ä¸Šä¸‹æ–‡ç”Ÿæˆå¯¹ä»£ç ç›®çš„çš„é«˜çº§è§£é‡Šï¼Œç¬¬ä¸‰ä¸ªç”¨äºéªŒè¯è§£é‡Šã€‚æˆ‘ä»¬å°†å…¶å®ç°ä¸ºä¸€ä¸ªç‹¬ç«‹å·¥å…·ï¼Œä¹Ÿå®ç°ä¸ºæ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼ˆMCPï¼‰å†…çš„æœåŠ¡å™¨ï¼Œä»¥ä¾¿ä¸å…¶ä»–AIè¾…åŠ©å¼€å‘å·¥å…·é›†æˆã€‚æˆ‘ä»¬çš„ä¸»è¦ç”¨ä¾‹æ˜¯åˆ©ç”¨æˆ‘ä»¬çš„ç³»ç»Ÿç”Ÿæˆçš„ä»£ç è§è§£æ¥å¢å¼ºåŸºäºLLMçš„æ ‡å‡†ä»£ç è§£é‡Šã€‚ä¸ºäº†è¯„ä¼°è§£é‡Šçš„è´¨é‡ï¼Œæˆ‘ä»¬å¯¹å‡ ä¸ªå¼€æºé¡¹ç›®çš„å¼€å‘äººå‘˜ä»¥åŠä¸“æœ‰é¡¹ç›®çš„å¼€å‘äººå‘˜è¿›è¡Œäº†å°è§„æ¨¡çš„ç”¨æˆ·ç ”ç©¶ã€‚æˆ‘ä»¬çš„ç”¨æˆ·ç ”ç©¶è¡¨æ˜ï¼Œå½“ç”Ÿæˆè§è§£æ—¶ï¼Œå®ƒä»¬é€šå¸¸æ˜¯æœ‰å¸®åŠ©çš„ä¸”éå¾®ä¸è¶³é“çš„ï¼Œå¹¶ä¸”æ²¡æœ‰å¹»è§‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.03549v1">PDF</a> 7 pages, 6 figures, to be published in AISM 2025, see   <a target="_blank" rel="noopener" href="https://aism25.github.io/aism25/">https://aism25.github.io/aism25/</a></p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç ç†è§£åœ¨è½¯ä»¶ç»´æŠ¤ã€å…¥èŒå’Œç°ä»£åŒ–è¿‡ç¨‹ä¸­å‘æŒ¥ç€å…³é”®ä½œç”¨ã€‚ç„¶è€Œï¼ŒLLMåœ¨ç”Ÿæˆä»£ç è§£é‡Šæ—¶å¾€å¾€ç¼ºä¹å¯¹æ›´å¹¿æ³›çš„è½¯ä»¶å·¥ç¨‹ç¯å¢ƒçš„å…³æ³¨ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œåˆ©ç”¨GitHubä¸Šçš„è‡ªç„¶è¯­è¨€äº§ç‰©ï¼ˆå¦‚åˆå¹¶è¯·æ±‚æè¿°ã€é—®é¢˜æè¿°å’Œè®¨è®ºã€æäº¤ä¿¡æ¯ç­‰ï¼‰å¢å¼ºLLMçš„ä»£ç ç†è§£èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿç”±ä¸‰ä¸ªç»„ä»¶æ„æˆï¼šä¸€ä¸ªæå–å’Œç»“æ„åŒ–GitHubç›¸å…³ç¯å¢ƒçš„ç»„ä»¶ï¼Œä¸€ä¸ªä½¿ç”¨è¯¥ç¯å¢ƒç”Ÿæˆä»£ç é«˜çº§è§£é‡Šçš„ç»„ä»¶ï¼Œä»¥åŠä¸€ä¸ªéªŒè¯è¿™äº›è§£é‡Šçš„ç»„ä»¶ã€‚æˆ‘ä»¬å°†å…¶å®ç°ä¸ºä¸€ä¸ªç‹¬ç«‹å·¥å…·ï¼Œä¹Ÿä½œä¸ºä¸€ä¸ªæœåŠ¡å™¨é›†æˆåˆ°æ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼ˆMCPï¼‰ä¸­ï¼Œä»¥ä¾¿ä¸å…¶ä»–AIè¾…åŠ©å¼€å‘å·¥å…·é›†æˆã€‚æˆ‘ä»¬çš„ä¸»è¦ç”¨ä¾‹æ˜¯åˆ©ç”¨æˆ‘ä»¬çš„ç³»ç»Ÿç”Ÿæˆçš„ä»£ç è§è§£å¢å¼ºæ ‡å‡†çš„LLMä»£ç è§£é‡Šã€‚ä¸ºäº†è¯„ä¼°è§£é‡Šçš„è´¨é‡ï¼Œæˆ‘ä»¬å¯¹å‡ ä¸ªå¼€æºé¡¹ç›®å’Œä¸“æœ‰é¡¹ç›®çš„å¼€å‘äººå‘˜è¿›è¡Œäº†å°è§„æ¨¡çš„ç”¨æˆ·ç ”ç©¶ã€‚ç”¨æˆ·ç ”ç©¶è¡¨æ˜ï¼Œå½“è§è§£è¢«ç”Ÿæˆæ—¶ï¼Œå®ƒä»¬é€šå¸¸æ˜¯æœ‰å¸®åŠ©çš„ã€éå¹³å‡¡çš„ï¼Œå¹¶ä¸”æ²¡æœ‰å¹»è§‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç†è§£æºä»£ç çš„ç”¨é€”åœ¨è½¯ä»¶ç»´æŠ¤ã€å…¥èŒå’Œç°ä»£åŒ–è¿‡ç¨‹ä¸­è‡³å…³é‡è¦ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç”Ÿæˆä»£ç è§£é‡Šæ—¶ç¼ºä¹è½¯ä»¶å·¥ç¨‹çš„æ•´ä½“èƒŒæ™¯ã€‚</li>
<li>å€ŸåŠ©GitHubçš„è‡ªç„¶è¯­è¨€äº§ç‰©å¦‚åˆå¹¶è¯·æ±‚æè¿°ç­‰å¢å¼ºLLMçš„ä»£ç ç†è§£èƒ½åŠ›æ˜¯ä¸€ä¸ªæœ‰æ•ˆæ–¹æ³•ã€‚</li>
<li>ç³»ç»ŸåŒ…æ‹¬æå–å’Œç»“æ„åŒ–GitHubç¯å¢ƒä¿¡æ¯çš„ç»„ä»¶ã€ç”Ÿæˆé«˜çº§ä»£ç è§£é‡Šçš„ç»„ä»¶ä»¥åŠéªŒè¯è¿™äº›è§£é‡Šçš„ç»„ä»¶ã€‚</li>
<li>ç³»ç»Ÿå¯ä»¥ä½œä¸ºç‹¬ç«‹å·¥å…·ä½¿ç”¨ï¼Œä¹Ÿå¯ä»¥ä½œä¸ºæœåŠ¡å™¨é›†æˆåˆ°æ¨¡å‹ä¸Šä¸‹æ–‡åè®®ä¸­ï¼Œä»¥ä¾¿ä¸å…¶ä»–AIè¾…åŠ©å¼€å‘å·¥å…·é›†æˆã€‚</li>
<li>è¯¥ç³»ç»Ÿçš„ä¸»è¦ç”¨ä¾‹æ˜¯å¢å¼ºæ ‡å‡†çš„LLMä»£ç è§£é‡Šèƒ½åŠ›ï¼Œç”Ÿæˆä»£ç è§è§£ä»¥æ”¯æŒå¼€å‘è¿‡ç¨‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.03549">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-782662a7c6d24f14d7228bbb56bcd995" align="middle">
<img src="https://picx.zhimg.com/v2-2cf1a6b8efc209175ed10fdf2505bb74" align="middle">
<img src="https://picx.zhimg.com/v2-9c544377981ad98c8c5a215a26aed868" align="middle">
<img src="https://picx.zhimg.com/v2-ad829d3c956607fec250519c5d60e24b" align="middle">
<img src="https://picx.zhimg.com/v2-7ef2587d3f82495bd0c853757368b3a6" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="HAFixAgent-History-Aware-Automated-Program-Repair-Agent"><a href="#HAFixAgent-History-Aware-Automated-Program-Repair-Agent" class="headerlink" title="HAFixAgent: History-Aware Automated Program Repair Agent"></a>HAFixAgent: History-Aware Automated Program Repair Agent</h2><p><strong>Authors:Yu Shi, Hao Li, Bram Adams, Ahmed E. Hassan</strong></p>
<p>Automated program repair (APR) has recently shifted toward large language models and agent-based systems, yet most systems rely on local snapshot context, overlooking repository history. Prior work shows that repository history helps repair single-line bugs, since the last commit touching the buggy line is often the bug-introducing one. In this paper, we investigate whether repository history can also improve agentic APR systems at scale, especially for complex multi-hunk bugs. We present HAFixAgent, a History-Aware Bug-Fixing Agent that injects blame-derived repository heuristics into its repair loop. A preliminary study of all 854 real-world bugs from Defects4J motivates our design, showing that bug-relevant history is both widely available and highly concentrated. Empirical comparison of HAFixAgent with two state-of-the-art baselines shows: (1) Effectiveness: HAFixAgent significantly improves over the agent-based baseline (by 212.3%) and the multi-hunk baseline (by 29.9%). (2) Efficiency: history does not significantly increase agent steps and keeps token costs comparable, with notably lower median costs for complex multi-file-multi-hunk bugs. (3) Practicality: combining different historical heuristics repairs more bugs, offering a clear cost-benefit trade-off. HAFixAgent offers a practical recipe for history-aware agentic APR: ground the agent in version control history, prioritize diff-based historical context, and integrate complementary heuristics when needed. </p>
<blockquote>
<p>è‡ªåŠ¨åŒ–ç¨‹åºä¿®å¤ï¼ˆAPRï¼‰æœ€è¿‘å·²è½¬å‘å¤§å‹è¯­è¨€æ¨¡å‹å’ŒåŸºäºä»£ç†çš„ç³»ç»Ÿï¼Œç„¶è€Œå¤§å¤šæ•°ç³»ç»Ÿéƒ½ä¾èµ–äºå±€éƒ¨å¿«ç…§ä¸Šä¸‹æ–‡ï¼Œå¿½ç•¥äº†ä»“åº“å†å²ã€‚å…ˆå‰çš„å·¥ä½œè¡¨æ˜ï¼Œä»“åº“å†å²æœ‰åŠ©äºä¿®å¤å•è¡Œé”™è¯¯ï¼Œå› ä¸ºè§¦æ‘¸é”™è¯¯è¡Œçš„æœ€åä¸€æ¬¡æäº¤é€šå¸¸æ˜¯å¼•å…¥é”™è¯¯çš„æäº¤ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è°ƒæŸ¥äº†ä»“åº“å†å²æ˜¯å¦ä¹Ÿå¯ä»¥å¤§è§„æ¨¡æ”¹è¿›åŸºäºä»£ç†çš„APRç³»ç»Ÿï¼Œç‰¹åˆ«æ˜¯å¯¹å¤æ‚çš„å¤šå—é”™è¯¯ã€‚æˆ‘ä»¬æå‡ºäº†HAFixAgentï¼Œä¸€ä¸ªæ„ŸçŸ¥å†å²çš„æ•…éšœä¿®å¤ä»£ç†ï¼Œå®ƒå°†è´£ä»»é©±åŠ¨çš„ä»“åº“å¯å‘å¼æ³¨å…¥åˆ°ä¿®å¤å¾ªç¯ä¸­ã€‚å¯¹Defects4Jä¸­æ‰€æœ‰854ä¸ªçœŸå®ä¸–ç•Œé”™è¯¯çš„åˆæ­¥ç ”ç©¶æ¿€å‘äº†æˆ‘ä»¬çš„è®¾è®¡çµæ„Ÿï¼Œæ˜¾ç¤ºé”™è¯¯ç›¸å…³çš„å†å²ä¸ä»…æ™®éå­˜åœ¨è€Œä¸”é«˜åº¦é›†ä¸­ã€‚HAFixAgentä¸ä¸¤ä¸ªæœ€å…ˆè¿›çš„åŸºå‡†æŠ€æœ¯çš„ç»éªŒæ¯”è¾ƒè¡¨æ˜ï¼šï¼ˆ1ï¼‰æœ‰æ•ˆæ€§ï¼šHAFixAgentæ˜¾è‘—æ”¹è¿›äº†åŸºäºä»£ç†çš„åŸºå‡†æŠ€æœ¯ï¼ˆæé«˜äº†212.3ï¼…ï¼‰å’Œå¤šå—åŸºå‡†æŠ€æœ¯ï¼ˆæé«˜äº†29.9ï¼…ï¼‰ã€‚ï¼ˆ2ï¼‰æ•ˆç‡ï¼šå†å²ä¸ä¼šæ˜¾è‘—å¢åŠ ä»£ç†æ­¥éª¤ï¼Œå¹¶ä¿æŒä»¤ç‰Œæˆæœ¬ç›¸å½“ï¼Œå¯¹äºå¤æ‚çš„å¤šæ–‡ä»¶å¤šå—é”™è¯¯ï¼Œä¸­ä½æ•°æˆæœ¬æ˜¾è‘—é™ä½ã€‚ï¼ˆ3ï¼‰å®ç”¨æ€§ï¼šç»“åˆä¸åŒçš„å†å²å¯å‘å¼å¯ä»¥ä¿®å¤æ›´å¤šçš„é”™è¯¯ï¼Œæä¾›äº†æ˜æ˜¾çš„æˆæœ¬æ•ˆç›Šæƒè¡¡ã€‚HAFixAgentæä¾›äº†æ„ŸçŸ¥å†å²çš„åŸºäºä»£ç†çš„APRçš„å®é™…é…æ–¹ï¼šå°†ä»£ç†ç½®äºç‰ˆæœ¬æ§åˆ¶å†å²ä¸­ï¼Œä¼˜å…ˆåŸºäºå·®å¼‚çš„ä¸Šä¸‹æ–‡ï¼Œå¹¶åœ¨éœ€è¦æ—¶é›†æˆäº’è¡¥çš„å¯å‘å¼æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.01047v2">PDF</a> 31 pages, 6 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>åŸºäºç‰ˆæœ¬æ§åˆ¶å†å²çš„è‡ªåŠ¨åŒ–ç¨‹åºä¿®å¤ï¼ˆAPRï¼‰ç ”ç©¶ã€‚æå‡ºä¸€ä¸ªå†å²æ„ŸçŸ¥çš„ç¼ºé™·ä¿®å¤ä»£ç†HAFixAgentï¼Œå®ƒç»“åˆäº†è´£ä»»è¡ç”Ÿç‰ˆæœ¬æ§åˆ¶å¯å‘å¼æ–¹æ³•ä»¥æé«˜å¤§è§„æ¨¡ç¼ºé™·ä¿®å¤çš„æ•ˆç‡ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼ŒHAFixAgentåœ¨ä¿®å¤å¤šè¡Œç¼ºé™·æ—¶æ˜¾è‘—ä¼˜äºå…¶ä»–ä¸¤ç§å½“å‰ä¸»æµæ–¹æ³•ï¼ŒåŒæ—¶ä¿æŒè¾ƒé«˜çš„æ•ˆç‡ä¸å®ç”¨æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å†å²æ„ŸçŸ¥çš„ç¼ºé™·ä¿®å¤ä»£ç†ï¼ˆHAFixAgentï¼‰ç»“åˆäº†ç‰ˆæœ¬æ§åˆ¶å†å²ä¿¡æ¯ä»¥æé«˜è‡ªåŠ¨åŒ–ç¨‹åºä¿®å¤çš„æ•ˆæœã€‚</li>
<li>ç ”ç©¶å‘ç°ï¼Œç¼ºé™·ç›¸å…³çš„å†å²ä¿¡æ¯å¹¿æ³›å­˜åœ¨ä¸”é«˜åº¦é›†ä¸­ï¼Œä¸ºHAFixAgentçš„è®¾è®¡æä¾›äº†åŠ¨æœºã€‚</li>
<li>ä¸ä¸¤ç§å½“å‰ä¸»æµæ–¹æ³•çš„å®è¯æ¯”è¾ƒæ˜¾ç¤ºï¼ŒHAFixAgentåœ¨ä¿®å¤å¤šè¡Œç¼ºé™·æ—¶å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œå°¤å…¶æ˜¯å¤æ‚çš„å¤šå—ç¼ºé™·ã€‚</li>
<li>å†å²ä¿¡æ¯å¹¶æœªæ˜¾è‘—å¢åŠ ä¿®å¤ä»£ç†çš„æ­¥éª¤å’Œæ ‡è®°æˆæœ¬ï¼Œå¯¹äºå¤æ‚çš„å¤šæ–‡ä»¶å¤šå—ç¼ºé™·ï¼Œå…¶æˆæœ¬ç”šè‡³æ›´ä½ã€‚</li>
<li>ç»“åˆä¸åŒçš„å†å²å¯å‘å¼æ–¹æ³•å¯ä»¥ä¿®å¤æ›´å¤šçš„ç¼ºé™·ï¼Œæ˜¾ç¤ºå‡ºæ˜æ˜¾çš„æˆæœ¬æ•ˆç›Šæƒè¡¡ã€‚</li>
<li>HAFixAgentæä¾›äº†ä¸€ä¸ªå®ç”¨çš„å†å²æ„ŸçŸ¥çš„è‡ªåŠ¨åŒ–ç¨‹åºä¿®å¤çš„é…æ–¹ï¼Œå¼ºè°ƒåœ¨ç‰ˆæœ¬æ§åˆ¶å†å²ä¸­å®šä½ä»£ç†ï¼Œä¼˜å…ˆåŸºäºå·®å¼‚çš„å†å²ä¸Šä¸‹æ–‡ï¼Œå¹¶åœ¨éœ€è¦æ—¶é›†æˆäº’è¡¥çš„å¯å‘å¼æ–¹æ³•ã€‚</li>
<li>ç ”ç©¶ç»“æœå¼ºè°ƒäº†ç‰ˆæœ¬æ§åˆ¶å†å²åœ¨è‡ªåŠ¨åŒ–ç¨‹åºä¿®å¤ä¸­çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.01047">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9fd72b594819b7e40e16f40a32dbbcc3" align="middle">
<img src="https://picx.zhimg.com/v2-72ff6bb5909f5722a08b853c21dabd9c" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Enabling-Robust-In-Context-Memory-and-Rapid-Task-Adaptation-in-Transformers-with-Hebbian-and-Gradient-Based-Plasticity"><a href="#Enabling-Robust-In-Context-Memory-and-Rapid-Task-Adaptation-in-Transformers-with-Hebbian-and-Gradient-Based-Plasticity" class="headerlink" title="Enabling Robust In-Context Memory and Rapid Task Adaptation in   Transformers with Hebbian and Gradient-Based Plasticity"></a>Enabling Robust In-Context Memory and Rapid Task Adaptation in   Transformers with Hebbian and Gradient-Based Plasticity</h2><p><strong>Authors:Siddharth Chaudhary</strong></p>
<p>Large language models display in-context learning as an emergent effect of scale, but they rely on static weights during inference. In contrast, biological systems continually adapt via synaptic plasticity. We investigate whether explicit, biologically inspired plasticity can endow Transformers with faster in-sequence adaptation. To this end, we augment decoder-only Transformers with fast-weight modules updated either by (i) a neuromodulated Hebbian rule or (ii) the gradient-based plasticity mechanism of Duan et al. (2023). Across copying, regression, and few-shot classification tasks (CIFAR-FS, Omniglot), Hebbian plasticity consistently achieves lower loss and stronger few-shot generalization, while gradient-based updates perform best on long-horizon credit assignment. When associations are short and linearly separable, static weights suffice, defining a clear boundary condition for when plasticity helps. Analysis of learned modulatory signals reveals that gradient-based rules maintain large, persistent updates, whereas Hebbian plasticity is sharply gated around salient events. Together, these results show that explicit plasticity complements attention by enabling rapid, task-specific adaptation, and clarify when different plasticity mechanisms are most effective. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹å±•ç°å‡ºä¸Šä¸‹æ–‡å­¦ä¹ ä½œä¸ºè§„æ¨¡çš„ä¸€ç§æ¶Œç°æ•ˆåº”ï¼Œä½†å®ƒä»¬åœ¨æ¨ç†è¿‡ç¨‹ä¸­ä¾èµ–äºé™æ€æƒé‡ã€‚ä¸æ­¤ç›¸åï¼Œç”Ÿç‰©ç³»ç»Ÿé€šè¿‡çªè§¦å¯å¡‘æ€§æŒç»­é€‚åº”ã€‚æˆ‘ä»¬è°ƒæŸ¥æ˜ç¡®çš„ã€å—ç”Ÿç‰©å¯å‘çš„å¯å¡‘æ€§æ˜¯å¦å¯ä»¥èµ‹äºˆTransformeræ›´å¿«çš„åºåˆ—é€‚åº”æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ç»™ä»…è§£ç çš„Transformerå¢åŠ äº†å¿«é€Ÿæƒé‡æ¨¡å—ï¼Œè¿™äº›æ¨¡å—é€šè¿‡ï¼ˆiï¼‰ç¥ç»è°ƒèŠ‚çš„èµ«å¸ƒè§„åˆ™æˆ–ï¼ˆiiï¼‰Duanç­‰äººæå‡ºçš„åŸºäºæ¢¯åº¦çš„å¯å¡‘æ€§æœºåˆ¶ï¼ˆ2023å¹´ï¼‰è¿›è¡Œæ›´æ–°ã€‚åœ¨å¤åˆ¶ã€å›å½’å’Œå°‘é‡æ ·æœ¬åˆ†ç±»ä»»åŠ¡ï¼ˆCIFAR-FSã€Omniglotï¼‰ä¸­ï¼Œèµ«å¸ƒå¯å¡‘æ€§å§‹ç»ˆå®ç°äº†æ›´ä½çš„æŸå¤±å’Œæ›´å¼ºçš„å°‘é‡æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œè€ŒåŸºäºæ¢¯åº¦çš„æ›´æ–°åœ¨é•¿æ—¶é—´èŒƒå›´çš„ä»»åŠ¡åˆ†é…æ–¹é¢è¡¨ç°æœ€ä½³ã€‚å½“å…³è”æ˜¯çŸ­æš‚ä¸”çº¿æ€§å¯åˆ†ç¦»æ—¶ï¼Œé™æ€æƒé‡å°±è¶³å¤Ÿäº†ï¼Œè¿™ä¸ºå¯å¡‘æ€§æœ‰åŠ©äºå®šä¹‰äº†ä¸€ä¸ªæ˜ç¡®çš„è¾¹ç•Œæ¡ä»¶ã€‚å¯¹å­¦åˆ°çš„è°ƒåˆ¶ä¿¡å·çš„åˆ†æè¡¨æ˜ï¼ŒåŸºäºæ¢¯åº¦çš„è§„åˆ™ç»´æŒäº†å¤§è§„æ¨¡ã€æŒä¹…çš„æ›´æ–°ï¼Œè€Œèµ«å¸ƒå¯å¡‘æ€§åˆ™å›´ç»•æ˜¾è‘—äº‹ä»¶è¿›è¡Œå°–é”çš„é—¸é—¨æ§åˆ¶ã€‚æ€»çš„æ¥è¯´ï¼Œè¿™äº›ç»“æœè¡¨æ˜ï¼Œæ˜ç¡®çš„å¯å¡‘æ€§é€šè¿‡å®ç°å¿«é€Ÿã€ç‰¹å®šçš„ä»»åŠ¡é€‚åº”æ€§æ¥è¡¥å……æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¹¶æ˜ç¡®äº†ä¸åŒå¯å¡‘æ€§æœºåˆ¶ä½•æ—¶æœ€ä¸ºæœ‰æ•ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.21908v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹å±•ç°å‡ºä¸Šä¸‹æ–‡å­¦ä¹ çš„èƒ½åŠ›ï¼Œä½†å®ƒä»¬ä¾èµ–äºé™æ€æƒé‡è¿›è¡Œæ¨ç†ã€‚ä¸æ­¤ç›¸åï¼Œç”Ÿç‰©ç³»ç»Ÿé€šè¿‡çªè§¦å¯å¡‘æ€§æŒç»­é€‚åº”ã€‚æœ¬ç ”ç©¶æ—¨åœ¨æ¢ç©¶æ˜ç¡®çš„ã€å—ç”Ÿç‰©å¯å‘çš„å¯å¡‘æ€§æ˜¯å¦èƒ½å¤Ÿèµ‹äºˆTransformeræ›´å¿«çš„åºåˆ—å†…é€‚åº”èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ä¸ºä»…è§£ç çš„Transformerå¢åŠ äº†å¿«é€Ÿæƒé‡æ¨¡å—ï¼Œè¿™äº›æ¨¡å—é€šè¿‡ï¼ˆiï¼‰ç¥ç»è°ƒèŠ‚çš„èµ«å¸ƒè§„åˆ™æˆ–ï¼ˆiiï¼‰æ®µç­‰äººæå‡ºçš„åŸºäºæ¢¯åº¦çš„å¯å¡‘æ€§æœºåˆ¶è¿›è¡Œæ›´æ–°ã€‚åœ¨å¤åˆ¶ã€å›å½’å’Œå°‘é‡æ ·æœ¬åˆ†ç±»ä»»åŠ¡ï¼ˆå¦‚CIFAR-FSå’ŒOmniglotï¼‰ä¸­ï¼Œèµ«å¸ƒå¯å¡‘æ€§å§‹ç»ˆå®ç°äº†æ›´ä½çš„æŸå¤±å’Œæ›´å¼ºçš„å°‘é‡æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œè€ŒåŸºäºæ¢¯åº¦çš„æ›´æ–°åœ¨é•¿æœŸä¿¡ç”¨åˆ†é…ä»»åŠ¡ä¸Šè¡¨ç°æœ€ä½³ã€‚å½“å…³è”çŸ­æš‚ä¸”çº¿æ€§å¯åˆ†æ—¶ï¼Œé™æ€æƒé‡è¶³å¤Ÿï¼Œè¿™ä¸ºå¯å¡‘æ€§å‘æŒ¥ä½œç”¨çš„è¾¹ç•Œæ¡ä»¶æä¾›äº†æ˜ç¡®ç•Œå®šã€‚å¯¹å­¦åˆ°çš„è°ƒåˆ¶ä¿¡å·çš„åˆ†æè¡¨æ˜ï¼ŒåŸºäºæ¢¯åº¦çš„è§„åˆ™ç»´æŒäº†å¤§è§„æ¨¡ã€æŒä¹…çš„æ›´æ–°ï¼Œè€Œèµ«å¸ƒå¯å¡‘æ€§åˆ™å›´ç»•é‡è¦äº‹ä»¶è¿›è¡Œå°–é”çš„é—¨æ§ã€‚æ€»ä½“è€Œè¨€ï¼Œæ˜ç¡®çš„å¯å¡‘æ€§è¡¥å……äº†æ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ç°äº†å¿«é€Ÿã€ç‰¹å®šçš„ä»»åŠ¡é€‚åº”ï¼Œå¹¶æ˜ç¡®äº†ä¸åŒå¯å¡‘æ€§æœºåˆ¶ä½•æ—¶æœ€ä¸ºæœ‰æ•ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹å±•ç°å‡ºä¸Šä¸‹æ–‡å­¦ä¹ çš„èƒ½åŠ›ï¼Œä½†æ¨ç†æ—¶ä¾èµ–äºé™æ€æƒé‡ã€‚</li>
<li>ç”Ÿç‰©ç³»ç»Ÿé€šè¿‡çªè§¦å¯å¡‘æ€§æŒç»­é€‚åº”ï¼Œæœ¬ç ”ç©¶å—æ­¤å¯å‘ï¼Œæ¢ç©¶èµ‹äºˆTransformeræ›´å¿«é€‚åº”èƒ½åŠ›çš„å¯èƒ½æ€§ã€‚</li>
<li>é€šè¿‡å¢åŠ å¿«é€Ÿæƒé‡æ¨¡å—ï¼Œç ”ç©¶äº†èµ«å¸ƒå¯å¡‘æ€§å’ŒåŸºäºæ¢¯åº¦çš„å¯å¡‘æ€§æœºåˆ¶ã€‚</li>
<li>åœ¨å¤šç§ä»»åŠ¡ä¸­ï¼Œèµ«å¸ƒå¯å¡‘æ€§å®ç°äº†æ›´ä½çš„æŸå¤±å’Œæ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ï¼Œè€ŒåŸºäºæ¢¯åº¦çš„æ›´æ–°åœ¨é•¿æœŸä»»åŠ¡ä¸Šè¡¨ç°æœ€ä½³ã€‚</li>
<li>å½“å…³è”çŸ­æš‚ä¸”çº¿æ€§å¯åˆ†æ—¶ï¼Œé™æ€æƒé‡è¶³å¤Ÿï¼Œè¿™å®šä¹‰äº†å¯å¡‘æ€§å‘æŒ¥ä½œç”¨çš„è¾¹ç•Œæ¡ä»¶ã€‚</li>
<li>å¯¹å­¦åˆ°çš„è°ƒåˆ¶ä¿¡å·çš„åˆ†æè¡¨æ˜ï¼ŒåŸºäºæ¢¯åº¦çš„è§„åˆ™ç»´æŒäº†å¤§è§„æ¨¡ã€æŒä¹…çš„æ›´æ–°ï¼Œè€Œèµ«å¸ƒå¯å¡‘æ€§åˆ™å¯¹é‡è¦äº‹ä»¶è¿›è¡Œå°–é”çš„é—¨æ§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.21908">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9c915953067f462fbe1df8cf6f875657" align="middle">
<img src="https://picx.zhimg.com/v2-23e9691191530aa4509a1d49e3c5e0ab" align="middle">
<img src="https://picx.zhimg.com/v2-c6920bbe63807ef9943a847fa13922ed" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Multilingual-Political-Views-of-Large-Language-Models-Identification-and-Steering"><a href="#Multilingual-Political-Views-of-Large-Language-Models-Identification-and-Steering" class="headerlink" title="Multilingual Political Views of Large Language Models: Identification   and Steering"></a>Multilingual Political Views of Large Language Models: Identification   and Steering</h2><p><strong>Authors:Daniil Gurgurov, Katharina Trinley, Ivan Vykopal, Josef van Genabith, Simon Ostermann, Roberto Zamparelli</strong></p>
<p>Large language models (LLMs) are increasingly used in everyday tools and applications, raising concerns about their potential influence on political views. While prior research has shown that LLMs often exhibit measurable political biasesâ€“frequently skewing toward liberal or progressive positionsâ€“key gaps remain. Most existing studies evaluate only a narrow set of models and languages, leaving open questions about the generalizability of political biases across architectures, scales, and multilingual settings. Moreover, few works examine whether these biases can be actively controlled.   In this work, we address these gaps through a large-scale study of political orientation in modern open-source instruction-tuned LLMs. We evaluate seven models, including LLaMA-3.1, Qwen-3, and Aya-Expanse, across 14 languages using the Political Compass Test with 11 semantically equivalent paraphrases per statement to ensure robust measurement. Our results reveal that larger models consistently shift toward libertarian-left positions, with significant variations across languages and model families. To test the manipulability of political stances, we utilize a simple center-of-mass activation intervention technique and show that it reliably steers model responses toward alternative ideological positions across multiple languages. Our code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/d-gurgurov/Political-Ideologies-LLMs">https://github.com/d-gurgurov/Political-Ideologies-LLMs</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ—¥å¸¸å·¥å…·å’Œåº”ç”¨ç¨‹åºä¸­çš„ä½¿ç”¨æ—¥ç›Šæ™®åŠï¼Œå¼•å‘äººä»¬å¯¹å®ƒä»¬å¯¹æ”¿æ²»è§‚ç‚¹æ½œåœ¨å½±å“çš„æ‹…å¿§ã€‚è™½ç„¶ä¹‹å‰çš„ç ”ç©¶è¡¨æ˜ï¼ŒLLMé€šå¸¸è¡¨ç°å‡ºå¯è¡¡é‡çš„æ”¿æ²»åè§ï¼Œå¾€å¾€åå‘è‡ªç”±æˆ–è¿›æ­¥ç«‹åœºï¼Œä½†ä»å­˜åœ¨å…³é”®å·®è·ã€‚å¤§å¤šæ•°ç°æœ‰ç ”ç©¶åªè¯„ä¼°äº†æœ‰é™çš„æ¨¡å‹å’Œè¯­è¨€ï¼Œå¯¹äºè·¨æ¶æ„ã€è§„æ¨¡å’Œè·¨è¯­è¨€ç¯å¢ƒä¸­æ”¿æ²»åè§çš„ä¸€èˆ¬æ€§ä»å­˜åœ¨å¼€æ”¾é—®é¢˜ã€‚æ­¤å¤–ï¼Œå¾ˆå°‘æœ‰å·¥ä½œç ”ç©¶æ˜¯å¦å¯ä»¥ä¸»åŠ¨æ§åˆ¶è¿™äº›åè§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡ä¸€é¡¹é’ˆå¯¹ç°ä»£å¼€æºæŒ‡ä»¤è°ƒæ•´çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸­æ”¿æ²»å€¾å‘çš„å¤§è§„æ¨¡ç ”ç©¶æ¥è§£å†³è¿™äº›å·®è·ã€‚æˆ‘ä»¬è¯„ä¼°äº†ä¸ƒä¸ªæ¨¡å‹ï¼ŒåŒ…æ‹¬LLaMA-3.1ã€Qwen-3å’ŒAya-Expanseç­‰ï¼Œåœ¨åå››ç§è¯­è¨€ä¸Šä½¿ç”¨æ”¿æ²»æµ‹è¯•è¿›è¡Œè¯„æµ‹ï¼Œä»¥ç¡®ä¿æµ‹é‡çš„ç¨³å¥æ€§ã€‚æˆ‘ä»¬çš„ç»“æœæ˜¾ç¤ºï¼Œæ›´å¤§çš„æ¨¡å‹ä¸€è´¯åœ°åå‘è‡ªç”±å·¦ç¿¼ç«‹åœºï¼Œä¸åŒè¯­è¨€å’Œæ¨¡å‹å®¶æ—ä¹‹é—´å­˜åœ¨æ˜¾è‘—å˜åŒ–ã€‚ä¸ºäº†æµ‹è¯•æ”¿æ²»ç«‹åœºçš„å¯æ“æ§æ€§ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§ç®€å•çš„ä¸­å¿ƒè´¨é‡æ¿€æ´»å¹²é¢„æŠ€æœ¯ï¼Œå¹¶è¯æ˜è¯¥æŠ€æœ¯å¯ä»¥åœ¨å¤šç§è¯­è¨€ä¸­å¯é åœ°å¼•å¯¼æ¨¡å‹å“åº”èµ°å‘æ›¿ä»£æ„è¯†å½¢æ€ç«‹åœºã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/d-gurgurov/Political-Ideologies-LLMs%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/d-gurgurov/Political-Ideologies-LLMså…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.22623v2">PDF</a> pre-print</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†ç°ä»£å¼€æºæŒ‡ä»¤è°ƒæ•´çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ”¿æ²»å€¾å‘æ€§ã€‚é€šè¿‡å¯¹ä¸ƒä¸ªæ¨¡å‹åœ¨14ç§è¯­è¨€ä¸­çš„å¤§è§„æ¨¡ç ”ç©¶ï¼Œå‘ç°å¤§å‹æ¨¡å‹å¾€å¾€å€¾å‘äºè‡ªç”±ä¸»ä¹‰å·¦ç¿¼ç«‹åœºï¼Œä½†ä¸åŒæ¨¡å‹å’Œè¯­è¨€é—´å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚åŒæ—¶ï¼Œé€šè¿‡ç®€å•çš„ä¸­å¿ƒæ¿€æ´»å¹²é¢„æŠ€æœ¯ï¼Œå¯ä»¥å¯é åœ°å¼•å¯¼æ¨¡å‹å“åº”ä¸åŒçš„æ„è¯†å½¢æ€ç«‹åœºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ—¥å¸¸å·¥å…·å’Œåº”ç”¨ç¨‹åºä¸­çš„ä½¿ç”¨å¼•å‘äº†äººä»¬å¯¹æ”¿æ²»è§‚ç‚¹æ½œåœ¨å½±å“çš„æ‹…å¿§ã€‚</li>
<li>ç°æœ‰ç ”ç©¶å·²ç»è¡¨æ˜LLMsç»å¸¸è¡¨ç°å‡ºå¯è¡¡é‡çš„æ”¿æ²»åè§ï¼Œé€šå¸¸åå‘è‡ªç”±æˆ–è¿›æ­¥ç«‹åœºã€‚</li>
<li>æœ¬ç ”ç©¶é€šè¿‡å¤§è§„æ¨¡ç ”ç©¶ç°ä»£å¼€æºæŒ‡ä»¤è°ƒæ•´LLMsçš„æ”¿æ²»å€¾å‘æ€§æ¥å¡«è¡¥ç°æœ‰ç ”ç©¶çš„ç©ºç™½ã€‚</li>
<li>ç ”ç©¶è¯„ä¼°äº†ä¸ƒä¸ªæ¨¡å‹åœ¨14ç§è¯­è¨€ä¸­çš„è¡¨ç°ï¼Œå‘ç°å¤§å‹æ¨¡å‹æ™®éå€¾å‘äºè‡ªç”±ä¸»ä¹‰å·¦ç¿¼ç«‹åœºã€‚</li>
<li>ä¸åŒæ¨¡å‹å’Œè¯­è¨€ä¹‹é—´åœ¨æ”¿æ²»å€¾å‘æ€§ä¸Šå­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚</li>
<li>ç ”ç©¶é€šè¿‡ä¸­å¿ƒæ¿€æ´»å¹²é¢„æŠ€æœ¯æµ‹è¯•äº†æ”¿æ²»ç«‹åœºçš„å¯æ“æ§æ€§ï¼Œå¹¶å‘ç°è¯¥æŠ€æœ¯èƒ½å¤Ÿå¯é åœ°å¼•å¯¼æ¨¡å‹å“åº”ä¸åŒçš„æ„è¯†å½¢æ€ç«‹åœºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.22623">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2ea318f80c8674bf2f4f7b4e823c06b3" align="middle">
<img src="https://picx.zhimg.com/v2-b4fa959378c1d273be0cc22b3540054f" align="middle">
<img src="https://picx.zhimg.com/v2-ff0be662c8e6d6f83bd998d0c2493d04" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Echo-State-Transformer-Attention-Over-Finite-Memories"><a href="#Echo-State-Transformer-Attention-Over-Finite-Memories" class="headerlink" title="Echo State Transformer: Attention Over Finite Memories"></a>Echo State Transformer: Attention Over Finite Memories</h2><p><strong>Authors:Yannis Bendi-Ouis, Xavier Hinaut</strong></p>
<p>While Large Language Models and their underlying Transformer architecture are remarkably efficient, they do not reflect how our brain processes and learns a diversity of cognitive tasks such as language and working memory. Furthermore, sequential data processing with Transformers encounters a fundamental barrier: quadratic complexity growth with sequence length. Motivated by these limitations, our ambition is to create more efficient models that are less reliant on intensive computations. We introduce Echo State Transformers (EST), a hybrid architecture that elegantly resolves this challenge while demonstrating exceptional performance in classification and detection tasks. EST integrates the Transformer attention mechanisms with principles from Reservoir Computing to create a fixed-size window distributed memory system. Drawing inspiration from Echo State Networks, the most prominent instance of the Reservoir Computing paradigm, our approach leverages reservoirs (random recurrent networks) as a lightweight and efficient memory. Our architecture integrates a new module called â€˜â€™Working Memoryâ€™â€™ based on several reservoirs working in parallel. These reservoirs work as independent working memory units with distinct internal dynamics. A novelty here is that the classical reservoir hyperparameters, controlling the dynamics, are now trained. Thus, the EST dynamically adapts the reservoir memory&#x2F;non-linearity trade-off. Thanks to these working memory units, EST achieves constant computational complexity at each processing step, effectively breaking the quadratic scaling problem of standard Transformers. We evaluate ESTs on a recent challenging timeseries benchmark: the Time Series Library, which comprises 69 tasks across five categories. Results show that ESTs ranks first overall in two of five categories, outperforming strong state-of-the-art baselines on classification and anomaly detection tasks, while remaining competitive on short-term forecasting. These results position ESTs as a compelling alternative for time-series classification and anomaly detection, and a practical complement to transformer-style models in applications that prioritize robust representations and sensitive event detection. </p>
<blockquote>
<p>å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹åŠå…¶åŸºç¡€Transformeræ¶æ„éå¸¸é«˜æ•ˆï¼Œä½†å®ƒä»¬å¹¶ä¸èƒ½åæ˜ æˆ‘ä»¬çš„å¤§è„‘å¦‚ä½•å¤„ç†å’Œå­¦ä¹ è¯­è¨€å’Œå·¥ä½œè®°å¿†ç­‰å¤šæ ·åŒ–çš„è®¤çŸ¥ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œä½¿ç”¨Transformerè¿›è¡Œåºåˆ—æ•°æ®å¤„ç†ä¼šé‡åˆ°ä¸€ä¸ªæ ¹æœ¬éšœç¢ï¼šéšç€åºåˆ—é•¿åº¦çš„å¢åŠ ï¼Œå¤æ‚æ€§å‘ˆäºŒæ¬¡å¢é•¿ã€‚å—åˆ°è¿™äº›é™åˆ¶çš„å¯å‘ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯åˆ›å»ºæ›´å°‘ä¾èµ–è®¡ç®—å¯†é›†å‹çš„æ›´é«˜æ•ˆæ¨¡å‹ã€‚æˆ‘ä»¬å¼•å…¥äº†å›å£°çŠ¶æ€è½¬æ¢å™¨ï¼ˆEcho State Transformersï¼Œç®€ç§°ESTï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ··åˆæ¶æ„ï¼Œä¼˜é›…åœ°è§£å†³äº†è¿™ä¸€æŒ‘æˆ˜ï¼ŒåŒæ—¶åœ¨åˆ†ç±»å’Œæ£€æµ‹ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚ESTå°†Transformeræ³¨æ„åŠ›æœºåˆ¶ä¸å‚¨å¤‡è®¡ç®—åŸç†ç›¸ç»“åˆï¼Œåˆ›å»ºäº†ä¸€ä¸ªå›ºå®šå¤§å°çš„çª—å£åˆ†å¸ƒå¼å†…å­˜ç³»ç»Ÿã€‚å—åˆ°å›å£°çŠ¶æ€ç½‘ç»œï¼ˆEcho State Networksï¼‰è¿™ä¸€å‚¨å¤‡è®¡ç®—èŒƒä¾‹ä¸­æœ€çªå‡ºå®ä¾‹çš„å¯å‘ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨å‚¨å¤‡ï¼ˆéšæœºé€’å½’ç½‘ç»œï¼‰ä½œä¸ºè½»é‡çº§ä¸”é«˜æ•ˆçš„å†…å­˜ã€‚æˆ‘ä»¬çš„æ¶æ„é›†æˆäº†ä¸€ä¸ªåä¸ºâ€œå·¥ä½œå†…å­˜â€çš„æ–°æ¨¡å—ï¼Œè¯¥æ¨¡å—åŸºäºå¤šä¸ªå¹¶è¡Œå·¥ä½œçš„å‚¨å¤‡ã€‚è¿™äº›å‚¨å¤‡å……å½“ç‹¬ç«‹çš„å·¥ä½œå†…å­˜å•å…ƒï¼Œå…·æœ‰ä¸åŒçš„å†…éƒ¨åŠ¨æ€ã€‚è¿™é‡Œçš„ä¸€ä¸ªæ–°é¢–ä¹‹å¤„åœ¨äºæ§åˆ¶åŠ¨æ€çš„ç»å…¸å‚¨å¤‡è¶…å‚æ•°ç°åœ¨æ˜¯å¯ä»¥è®­ç»ƒçš„ã€‚å› æ­¤ï¼ŒESTèƒ½å¤ŸåŠ¨æ€åœ°è°ƒæ•´å‚¨å¤‡å†…å­˜ä¸éçº¿æ€§çš„æƒè¡¡ã€‚ç”±äºè¿™äº›å·¥ä½œå†…å­˜å•å…ƒçš„å­˜åœ¨ï¼ŒESTåœ¨å¤„ç†æ¯ä¸ªæ­¥éª¤æ—¶å®ç°äº†æ’å®šçš„è®¡ç®—å¤æ‚æ€§ï¼Œæœ‰æ•ˆåœ°è§£å†³äº†æ ‡å‡†Transformerçš„äºŒæ¬¡æ‰©å±•é—®é¢˜ã€‚æˆ‘ä»¬åœ¨æœ€æ–°çš„æ—¶é—´åºåˆ—åŸºå‡†æµ‹è¯•â€”â€”æ—¶é—´åºåˆ—åº“ä¸Šè¯„ä¼°äº†ESTçš„æ€§èƒ½ï¼Œè¯¥åº“åŒ…å«äº”ä¸ªç±»åˆ«ä¸­çš„69ä¸ªä»»åŠ¡ã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨äº”ä¸ªç±»åˆ«ä¸­ï¼ŒESTåœ¨ä¸¤é¡¹ä¸­æ’åç¬¬ä¸€ï¼Œåœ¨åˆ†ç±»å’Œå¼‚å¸¸æ£€æµ‹ä»»åŠ¡ä¸Šä¼˜äºå¼ºå¤§çš„æœ€æ–°åŸºçº¿æŠ€æœ¯ï¼Œå¹¶åœ¨çŸ­æœŸé¢„æµ‹æ–¹é¢ä¿æŒç«äº‰åŠ›ã€‚è¿™äº›ç»“æœä½¿ESTæˆä¸ºæ—¶é—´åºåˆ—åˆ†ç±»å’Œå¼‚å¸¸æ£€æµ‹çš„æœ‰åŠ›æ›¿ä»£æ–¹æ¡ˆï¼Œå¹¶ä¸”æ˜¯ä¼˜å…ˆè€ƒè™‘ç¨³å¥è¡¨ç¤ºå’Œæ•æ„Ÿäº‹ä»¶æ£€æµ‹çš„å˜å‹å™¨é£æ ¼æ¨¡å‹çš„å®é™…è¡¥å……ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02917v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤§å‹è¯­è¨€æ¨¡å‹åŠå…¶åº•å±‚Transformeræ¶æ„çš„å±€é™æ€§ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¤šæ ·è®¤çŸ¥ä»»åŠ¡å¦‚è¯­è¨€å’Œå·¥ä½œè®°å¿†æ–¹é¢çš„ä¸è¶³ã€‚é’ˆå¯¹Transformeråœ¨å¤„ç†åºåˆ—æ•°æ®æ—¶é‡åˆ°çš„äºŒæ¬¡å¤æ‚åº¦å¢é•¿é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ··åˆæ¶æ„â€”â€”Echo State Transformersï¼ˆESTï¼‰ã€‚è¯¥æ¶æ„èåˆäº†Transformeræ³¨æ„åŠ›æœºåˆ¶å’ŒReservoir Computingçš„åŸç†ï¼Œå±•ç¤ºå‡ºåœ¨åˆ†ç±»å’Œæ£€æµ‹ä»»åŠ¡ä¸­çš„å“è¶Šæ€§èƒ½ã€‚ESTé€šè¿‡å¼•å…¥â€œå·¥ä½œè®°å¿†â€æ¨¡å—ï¼Œå®ç°äº†å›ºå®šå¤§å°çš„çª—å£åˆ†å¸ƒå¼å†…å­˜ç³»ç»Ÿã€‚å…¶åˆ©ç”¨å¤šä¸ªå¹¶è¡Œå·¥ä½œçš„å‚¨å¤‡æ± ä½œä¸ºè½»é‡çº§å’Œé«˜æ•ˆçš„å†…å­˜ï¼Œè¿™äº›å‚¨å¤‡æ± ä½œä¸ºç‹¬ç«‹çš„å†…å­˜å•å…ƒå…·æœ‰ä¸åŒçš„å†…éƒ¨åŠ¨æ€ã€‚æ­¤å¤–ï¼ŒESTè¿˜è®­ç»ƒäº†æ§åˆ¶åŠ¨æ€çš„ç»å…¸å‚¨å¤‡æ± è¶…å‚æ•°ï¼Œä»è€ŒåŠ¨æ€è°ƒæ•´å‚¨å¤‡æ± å†…å­˜ä¸éçº¿æ€§çš„æƒè¡¡ã€‚åœ¨æ—¶åºåº“ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒESTåœ¨åˆ†ç±»å’Œå¼‚å¸¸æ£€æµ‹ä»»åŠ¡ä¸Šä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå¹¶åœ¨çŸ­æœŸé¢„æµ‹æ–¹é¢ä¿æŒç«äº‰åŠ›ã€‚è¿™ä½¿å¾—ESTæˆä¸ºæ—¶é—´åºåˆ—åˆ†ç±»å’Œå¼‚å¸¸æ£€æµ‹çš„æœ‰åŠ›æ›¿ä»£æ–¹æ¡ˆï¼Œå¹¶ä¸”åœ¨è¿½æ±‚ç¨³å¥è¡¨å¾å’Œæ•æ„Ÿäº‹ä»¶æ£€æµ‹çš„åº”ç”¨ä¸­æˆä¸ºTransformeræ¨¡å‹çš„å®ç”¨è¡¥å……ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’ŒTransformeræ¶æ„åœ¨å¤„ç†å¤šæ ·è®¤çŸ¥ä»»åŠ¡æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œæ— æ³•å®Œå…¨æ¨¡æ‹Ÿäººè„‘çš„å¤„ç†å’Œå­¦ä¹ æ–¹å¼ã€‚</li>
<li>Echo State Transformersï¼ˆESTï¼‰æ—¨åœ¨åˆ›å»ºæ›´é«˜æ•ˆã€ä¾èµ–è®¡ç®—è¾ƒå°‘çš„æ¨¡å‹ï¼Œè§£å†³Transformeråœ¨å¤„ç†åºåˆ—æ•°æ®æ—¶é¢ä¸´çš„äºŒæ¬¡å¤æ‚åº¦å¢é•¿é—®é¢˜ã€‚</li>
<li>ESTèåˆäº†Transformerçš„æ³¨æ„åŠ›æœºåˆ¶å’ŒReservoir Computingçš„åŸç†ï¼Œé€šè¿‡å¼•å…¥â€œå·¥ä½œè®°å¿†â€æ¨¡å—å®ç°å›ºå®šå¤§å°çš„çª—å£åˆ†å¸ƒå¼å†…å­˜ç³»ç»Ÿã€‚</li>
<li>ESTåˆ©ç”¨å¤šä¸ªå¹¶è¡Œå·¥ä½œçš„å‚¨å¤‡æ± ä½œä¸ºè½»é‡çº§å’Œé«˜æ•ˆçš„å†…å­˜å•å…ƒï¼Œè¿™äº›å‚¨å¤‡æ± å…·æœ‰ä¸åŒçš„å†…éƒ¨åŠ¨æ€ï¼Œå¹¶èƒ½åŠ¨æ€è°ƒæ•´å†…å­˜ä¸éçº¿æ€§çš„æƒè¡¡ã€‚</li>
<li>ESTåœ¨æ—¶åºåº“ä¸Šçš„è¯„ä¼°ç»“æœè¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨åˆ†ç±»å’Œå¼‚å¸¸æ£€æµ‹ä»»åŠ¡ä¸Šä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</li>
<li>ESTä¸ºæ—¶é—´åºåˆ—åˆ†ç±»å’Œå¼‚å¸¸æ£€æµ‹æä¾›äº†æœ‰åŠ›æ›¿ä»£æ–¹æ¡ˆï¼Œå¹¶ä¸”åœ¨è¿½æ±‚ç¨³å¥è¡¨å¾å’Œæ•æ„Ÿäº‹ä»¶æ£€æµ‹çš„åº”ç”¨ä¸­å…·æœ‰å®ç”¨ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02917">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-219e389fd558bfbd55655e41a099f846" align="middle">
<img src="https://picx.zhimg.com/v2-164d13794f2b5e452c3dae40a2ef35a9" align="middle">
<img src="https://picx.zhimg.com/v2-86436e4ea620c25f769b23034d45bb83" align="middle">
<img src="https://picx.zhimg.com/v2-e9fcd3b359ca3c50f5a23efa9d91d9ea" align="middle">
<img src="https://picx.zhimg.com/v2-cb9b8b13a9056715cfda780d3b27e280" align="middle">
<img src="https://picx.zhimg.com/v2-4395b803224a339590fd4e7b3904cd76" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Post-Persona-Alignment-for-Multi-Session-Dialogue-Generation"><a href="#Post-Persona-Alignment-for-Multi-Session-Dialogue-Generation" class="headerlink" title="Post Persona Alignment for Multi-Session Dialogue Generation"></a>Post Persona Alignment for Multi-Session Dialogue Generation</h2><p><strong>Authors:Yi-Pei Chen, Noriki Nishida, Hideki Nakayama, Yuji Matsumoto</strong></p>
<p>Multi-session persona-based dialogue generation presents challenges in maintaining long-term consistency and generating diverse, personalized responses. While large language models (LLMs) excel in single-session dialogues, they struggle to preserve persona fidelity and conversational coherence across extended interactions. Existing methods typically retrieve persona information before response generation, which can constrain diversity and result in generic outputs. We propose Post Persona Alignment (PPA), a novel two-stage framework that reverses this process. PPA first generates a general response based solely on dialogue context, then retrieves relevant persona memories using the response as a query, and finally refines the response to align with the speakerâ€™s persona. This post-hoc alignment strategy promotes naturalness and diversity while preserving consistency and personalization. Experiments on multi-session LLM-generated dialogue data demonstrate that PPA significantly outperforms prior approaches in consistency, diversity, and persona relevance, offering a more flexible and effective paradigm for long-term personalized dialogue generation. </p>
<blockquote>
<p>åŸºäºå¤šä¼šè¯ä¸ªæ€§å¯¹è¯ç”Ÿæˆé¢ä¸´ç€ä¿æŒé•¿æœŸä¸€è‡´æ€§ä»¥åŠç”Ÿæˆå¤šæ ·åŒ–ã€ä¸ªæ€§åŒ–å“åº”çš„æŒ‘æˆ˜ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å•ä¼šè¯å¯¹è¯ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æ‰©å±•äº¤äº’ä¸­ï¼Œå®ƒä»¬éš¾ä»¥ä¿æŒä¸ªæ€§çš„ä¸€è‡´æ€§å’Œå¯¹è¯çš„è¿è´¯æ€§ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸åœ¨ç”Ÿæˆå“åº”ä¹‹å‰æ£€ç´¢ä¸ªäººä¿¡æ¯ï¼Œè¿™å¯èƒ½ä¼šé™åˆ¶å¤šæ ·æ€§å¹¶å¯¼è‡´é€šç”¨è¾“å‡ºã€‚æˆ‘ä»¬æå‡ºä¸€ç§æ–°å‹çš„ä¸¤é˜¶æ®µæ¡†æ¶â€”â€”åä¸ªæ€§å¯¹é½ï¼ˆPPAï¼‰ï¼Œåè½¬äº†è¿™ä¸€è¿‡ç¨‹ã€‚PPAé¦–å…ˆä»…åŸºäºå¯¹è¯ä¸Šä¸‹æ–‡ç”Ÿæˆä¸€èˆ¬å“åº”ï¼Œç„¶åä½¿ç”¨å“åº”ä½œä¸ºæŸ¥è¯¢æ£€ç´¢ç›¸å…³ä¸ªæ€§è®°å¿†ï¼Œæœ€åå¯¹å“åº”è¿›è¡Œå¾®è°ƒï¼Œä»¥ä¸è¯´è¯äººçš„ä¸ªæ€§ä¿æŒä¸€è‡´ã€‚è¿™ç§äº‹åå¯¹é½ç­–ç•¥æ—¢ä¿ƒè¿›äº†è‡ªç„¶æ€§å’Œå¤šæ ·æ€§ï¼Œåˆä¿æŒäº†è¿è´¯æ€§å’Œä¸ªæ€§åŒ–ã€‚åœ¨å¤šä¼šè¯LLMç”Ÿæˆå¯¹è¯æ•°æ®ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œåœ¨ä¸€è‡´æ€§ã€å¤šæ ·æ€§å’Œä¸ªæ€§ç›¸å…³æ€§æ–¹é¢ï¼ŒPPAæ˜¾è‘—ä¼˜äºå…ˆå‰çš„æ–¹æ³•ï¼Œä¸ºé•¿æœŸä¸ªæ€§åŒ–å¯¹è¯ç”Ÿæˆæä¾›äº†æ›´çµæ´»ã€æ›´æœ‰æ•ˆçš„èŒƒå¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.11857v2">PDF</a> EMNLP 2025 Findings</p>
<p><strong>Summary</strong><br>å¤šä¼šè¯ä¸ªæ€§åŒ–å¯¹è¯ç”Ÿæˆåœ¨ç»´æŒé•¿æœŸä¸€è‡´æ€§ã€ç”Ÿæˆå¤šæ ·åŒ–å’Œä¸ªæ€§åŒ–å“åº”æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å•ä¼šè¯å¯¹è¯ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨è·¨æ‰©å±•äº¤äº’ä¸­ä¿æŒäººç‰©ä¿çœŸåº¦å’Œä¼šè¯è¿è´¯æ€§æ–¹é¢å­˜åœ¨å›°éš¾ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸åœ¨ç”Ÿæˆå“åº”ä¹‹å‰æ£€ç´¢äººç‰©ä¿¡æ¯ï¼Œè¿™å¯èƒ½ä¼šé™åˆ¶å¤šæ ·æ€§å¹¶å¯¼è‡´é€šç”¨è¾“å‡ºã€‚æˆ‘ä»¬æå‡ºä¸€ç§åä¸ºâ€œåäººç‰©å¯¹é½â€ï¼ˆPPAï¼‰çš„æ–°å‹ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åè½¬äº†è¿™ä¸€è¿‡ç¨‹ã€‚PPAé¦–å…ˆä»…æ ¹æ®å¯¹è¯ä¸Šä¸‹æ–‡ç”Ÿæˆä¸€èˆ¬å“åº”ï¼Œç„¶åä½¿ç”¨å“åº”ä½œä¸ºæŸ¥è¯¢æ£€ç´¢ç›¸å…³çš„äººç‰©è®°å¿†ï¼Œå¹¶æœ€ç»ˆå¯¹å“åº”è¿›è¡Œå¾®è°ƒä»¥ä¸è¯´è¯è€…çš„äººç‰©å¯¹é½ã€‚è¿™ç§äº‹åå¯¹é½ç­–ç•¥æ—¢ä¿ƒè¿›äº†è‡ªç„¶æ€§å’Œå¤šæ ·æ€§ï¼Œåˆä¿æŒäº†è¿è´¯æ€§å’Œä¸ªæ€§åŒ–ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨å¤šä¼šè¯LLMç”Ÿæˆçš„å¯¹è¯æ•°æ®ä¸­ï¼ŒPPAåœ¨ä¸€è‡´æ€§ã€å¤šæ ·æ€§å’Œäººç‰©ç›¸å…³æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºå…ˆå‰çš„æ–¹æ³•ï¼Œä¸ºé•¿æœŸä¸ªæ€§åŒ–å¯¹è¯ç”Ÿæˆæä¾›äº†æ›´çµæ´»æœ‰æ•ˆçš„èŒƒå¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šä¼šè¯ä¸ªæ€§åŒ–å¯¹è¯ç”Ÿæˆé¢ä¸´é•¿æœŸä¸€è‡´æ€§ã€å“åº”å¤šæ ·åŒ–å’Œä¸ªæ€§åŒ–çš„æŒ‘æˆ˜ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å•ä¼šè¯å¯¹è¯ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å¤šä¼šè¯åœºæ™¯ä¸­ä¿æŒäººç‰©ä¸€è‡´æ€§æ–¹é¢å­˜åœ¨å›°éš¾ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é€šå¸¸åœ¨ç”Ÿæˆå“åº”å‰æ£€ç´¢äººç‰©ä¿¡æ¯ï¼Œè¿™é™åˆ¶äº†å“åº”çš„å¤šæ ·æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„ä¸¤é˜¶æ®µæ¡†æ¶â€”â€”åäººç‰©å¯¹é½ï¼ˆPPAï¼‰æ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>PPAé¦–å…ˆæ ¹æ®å¯¹è¯ä¸Šä¸‹æ–‡ç”Ÿæˆåˆæ­¥å“åº”ï¼Œç„¶åæ£€ç´¢ç›¸å…³äººç‰©ä¿¡æ¯ï¼Œæœ€åå¾®è°ƒå“åº”ä»¥ä¸äººç‰©å¯¹é½ã€‚</li>
<li>PPAç­–ç•¥åœ¨ä¿æŒè‡ªç„¶æ€§å’Œå¤šæ ·æ€§çš„åŒæ—¶ï¼Œæé«˜äº†å“åº”çš„è¿è´¯æ€§å’Œä¸ªæ€§åŒ–ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒPPAåœ¨ä¸€è‡´æ€§ã€å¤šæ ·æ€§å’Œäººç‰©ç›¸å…³æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.11857">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-08e2701b11051491155c1ac3b08bdc7c" align="middle">
<img src="https://picx.zhimg.com/v2-8766829c880a0603ee713952d416108d" align="middle">
<img src="https://picx.zhimg.com/v2-a2c3cff4fcdf7d0393925b4d39c796a4" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Inv-Entropy-A-Fully-Probabilistic-Framework-for-Uncertainty-Quantification-in-Language-Models"><a href="#Inv-Entropy-A-Fully-Probabilistic-Framework-for-Uncertainty-Quantification-in-Language-Models" class="headerlink" title="Inv-Entropy: A Fully Probabilistic Framework for Uncertainty   Quantification in Language Models"></a>Inv-Entropy: A Fully Probabilistic Framework for Uncertainty   Quantification in Language Models</h2><p><strong>Authors:Haoyi Song, Ruihan Ji, Naichen Shi, Fan Lai, Raed Al Kontar</strong></p>
<p>Large language models (LLMs) have transformed natural language processing, but their reliable deployment requires effective uncertainty quantification (UQ). Existing UQ methods are often heuristic and lack a probabilistic interpretation. This paper begins by providing a theoretical justification for the role of perturbations in UQ for LLMs. We then introduce a dual random walk perspective, modeling input-output pairs as two Markov chains with transition probabilities defined by semantic similarity. Building on this, we propose a fully probabilistic framework based on an inverse model, which quantifies uncertainty by evaluating the diversity of the input space conditioned on a given output through systematic perturbations. Within this framework, we define a new uncertainty measure, Inv-Entropy. A key strength of our framework is its flexibility: it supports various definitions of uncertainty measures, embeddings, perturbation strategies, and similarity metrics. We also propose GAAP, a perturbation algorithm based on genetic algorithms, which enhances the diversity of sampled inputs. In addition, we introduce a new evaluation metric, Temperature Sensitivity of Uncertainty (TSU), which directly assesses uncertainty without relying on correctness as a proxy. Extensive experiments demonstrate that Inv-Entropy outperforms existing semantic UQ methods. The code to reproduce the results can be found at <a target="_blank" rel="noopener" href="https://github.com/UMDataScienceLab/Uncertainty-Quantification-for-LLMs">https://github.com/UMDataScienceLab/Uncertainty-Quantification-for-LLMs</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»æ”¹å˜äº†è‡ªç„¶è¯­è¨€å¤„ç†çš„æ ¼å±€ï¼Œä½†å…¶å¯é éƒ¨ç½²éœ€è¦æœ‰æ•ˆçš„ä¸ç¡®å®šæ€§é‡åŒ–ï¼ˆUQï¼‰ã€‚ç°æœ‰çš„UQæ–¹æ³•å¾€å¾€æ˜¯å¯å‘å¼çš„ï¼Œç¼ºä¹æ¦‚ç‡è§£é‡Šã€‚æœ¬æ–‡é¦–å…ˆä¸ºæ‰°åŠ¨åœ¨LLMçš„UQä¸­çš„ä½œç”¨æä¾›äº†ç†è®ºä¾æ®ã€‚ç„¶åï¼Œæˆ‘ä»¬å¼•å…¥äº†åŒéšæœºæ¸¸èµ°è§†è§’ï¼Œå°†è¾“å…¥è¾“å‡ºå¯¹å»ºæ¨¡ä¸ºä¸¤ä¸ªé©¬å°”å¯å¤«é“¾ï¼Œå…¶è½¬ç§»æ¦‚ç‡ç”±è¯­ä¹‰ç›¸ä¼¼æ€§å®šä¹‰ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŸºäºé€†æ¨¡å‹çš„å®Œå…¨æ¦‚ç‡æ¡†æ¶ï¼Œé€šè¿‡ç³»ç»Ÿæ€§æ‰°åŠ¨è¯„ä¼°ç»™å®šè¾“å‡ºæ¡ä»¶ä¸‹è¾“å…¥ç©ºé—´çš„å¤šæ ·æ€§æ¥é‡åŒ–ä¸ç¡®å®šæ€§ã€‚åœ¨è¯¥æ¡†æ¶ä¸‹ï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªæ–°çš„ä¸ç¡®å®šæ€§åº¦é‡æ ‡å‡†â€”â€”Inv-Entropyï¼ˆé€†ç†µï¼‰ã€‚æˆ‘ä»¬æ¡†æ¶çš„å…³é”®ä¼˜åŠ¿åœ¨äºå…¶çµæ´»æ€§ï¼šå®ƒæ”¯æŒå„ç§ä¸ç¡®å®šæ€§åº¦é‡ã€åµŒå…¥ã€æ‰°åŠ¨ç­–ç•¥å’Œç›¸ä¼¼æ€§åº¦é‡çš„å®šä¹‰ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†åŸºäºé—ä¼ ç®—æ³•çš„GAAPæ‰°åŠ¨ç®—æ³•ï¼Œæé«˜äº†é‡‡æ ·è¾“å…¥çš„å¤šæ ·æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°çš„è¯„ä¼°æŒ‡æ ‡â€”â€”ä¸ç¡®å®šæ€§æ¸©åº¦æ•æ„Ÿæ€§ï¼ˆTSUï¼‰ï¼Œå®ƒå¯ä»¥ç›´æ¥è¯„ä¼°ä¸ç¡®å®šæ€§ï¼Œè€Œä¸ä¾èµ–äºæ­£ç¡®æ€§ä½œä¸ºä»£ç†ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒInv-Entropyåœ¨ç°æœ‰çš„è¯­ä¹‰UQæ–¹æ³•ä¸­å…·æœ‰æ›´å¥½çš„è¡¨ç°ã€‚ç›¸å…³ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/UMDataScienceLab/Uncertainty-Quantification-for-LLMs%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/UMDataScienceLab/Uncertainty-Quantification-for-LLMsæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09684v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¯é æ€§éƒ¨ç½²éœ€è¦æœ‰æ•ˆçš„ä¸ç¡®å®šæ€§é‡åŒ–ï¼ˆUQï¼‰ã€‚æœ¬æ–‡æä¾›äº†ä»æ‰°åŠ¨åœ¨UQä¸­å¯¹LLMä½œç”¨çš„ç†è®ºè¯æ˜ï¼Œå¼•å…¥åŒé‡éšæœºæ¸¸èµ°è§†è§’å¹¶å»ºç«‹åŸºäºé€†æ¨¡å‹çš„å…¨æ¦‚ç‡æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡ç³»ç»Ÿæ‰°åŠ¨è¯„ä¼°ç»™å®šè¾“å‡ºçš„è¾“å…¥ç©ºé—´å¤šæ ·æ€§æ¥é‡åŒ–ä¸ç¡®å®šæ€§ã€‚æœ¬æ–‡å®šä¹‰äº†ä¸€ä¸ªæ–°çš„ä¸ç¡®å®šæ€§åº¦é‡æŒ‡æ ‡Inv-Entropyï¼Œå¹¶æå‡ºä¸€ç§åŸºäºé—ä¼ ç®—æ³•çš„æ‰°åŠ¨ç®—æ³•GAAPï¼Œä»¥æé«˜é‡‡æ ·è¾“å…¥çš„å¤šæ ·æ€§ã€‚åŒæ—¶ï¼Œä»‹ç»äº†ä¸€ç§æ–°çš„è¯„ä¼°æŒ‡æ ‡â€”â€”æ¸©åº¦æ•æ„Ÿæ€§ä¸ç¡®å®šæ€§ï¼ˆTSUï¼‰ï¼Œè¯¥æŒ‡æ ‡ç›´æ¥è¯„ä¼°ä¸ç¡®å®šæ€§ï¼Œä¸ä¾èµ–äºæ­£ç¡®æ€§ä½œä¸ºä»£ç†ã€‚å®éªŒè¡¨æ˜ï¼ŒInv-Entropyä¼˜äºç°æœ‰çš„è¯­ä¹‰UQæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMçš„å¯é éƒ¨ç½²éœ€è¦æœ‰æ•ˆçš„ä¸ç¡®å®šæ€§é‡åŒ–ï¼ˆUQï¼‰ã€‚</li>
<li>æœ¬æ–‡ä»ç†è®ºè§’åº¦è¯æ˜äº†æ‰°åŠ¨åœ¨LLMä¸ç¡®å®šæ€§é‡åŒ–ä¸­çš„é‡è¦æ€§ã€‚</li>
<li>å¼•å…¥åŒé‡éšæœºæ¸¸èµ°è§†è§’ï¼Œå»ºç«‹åŸºäºé€†æ¨¡å‹çš„å…¨æ¦‚ç‡æ¡†æ¶è¿›è¡ŒUQã€‚</li>
<li>å®šä¹‰æ–°çš„ä¸ç¡®å®šæ€§åº¦é‡æŒ‡æ ‡Inv-Entropyï¼Œç”¨äºè¯„ä¼°è¾“å…¥ç©ºé—´çš„å¤šæ ·æ€§ã€‚</li>
<li>æå‡ºåŸºäºé—ä¼ ç®—æ³•çš„æ‰°åŠ¨ç®—æ³•GAAPï¼Œæé«˜é‡‡æ ·è¾“å…¥çš„å¤šæ ·æ€§ã€‚</li>
<li>ä»‹ç»æ–°çš„è¯„ä¼°æŒ‡æ ‡TSUï¼Œç›´æ¥è¯„ä¼°ä¸ç¡®å®šæ€§ï¼Œé¿å…ä¾èµ–æ­£ç¡®æ€§ä½œä¸ºä»£ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09684">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a03dffccc5d3fa2fc5f0f04845971a40" align="middle">
<img src="https://picx.zhimg.com/v2-f0e30c0eb5b57a5ad7bf391f88008806" align="middle">
<img src="https://picx.zhimg.com/v2-e520be2b443dc6f2635e4b24eaf576b1" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Instructing-Large-Language-Models-for-Low-Resource-Languages-A-Systematic-Study-for-Basque"><a href="#Instructing-Large-Language-Models-for-Low-Resource-Languages-A-Systematic-Study-for-Basque" class="headerlink" title="Instructing Large Language Models for Low-Resource Languages: A   Systematic Study for Basque"></a>Instructing Large Language Models for Low-Resource Languages: A   Systematic Study for Basque</h2><p><strong>Authors:Oscar Sainz, Naiara Perez, Julen Etxaniz, Joseba Fernandez de Landa, Itziar Aldabe, Iker GarcÃ­a-Ferrero, Aimar Zabala, Ekhi Azurmendi, German Rigau, Eneko Agirre, Mikel Artetxe, Aitor Soroa</strong></p>
<p>Instructing language models with user intent requires large instruction datasets, which are only available for a limited set of languages. In this paper, we explore alternatives to conventional instruction adaptation pipelines in low-resource scenarios. We assume a realistic scenario for low-resource languages, where only the following are available: corpora in the target language, existing open-weight multilingual base and instructed backbone LLMs, and synthetically generated instructions sampled from the instructed backbone. We present a comprehensive set of experiments for Basque that systematically study different combinations of these components evaluated on benchmarks and human preferences from 1,680 participants. Our conclusions show that target language corpora are essential, with synthetic instructions yielding robust models, and, most importantly, that using as backbone an instruction-tuned model outperforms using a base non-instructed model. Scaling up to Llama 3.1 Instruct 70B as backbone, our model comes near frontier models of much larger sizes for Basque, without using any Basque instructions. We release code, models, instruction datasets, and human preferences to support full reproducibility in future research on low-resource language adaptation. <a target="_blank" rel="noopener" href="https://github.com/hitz-zentroa/latxa-instruct">https://github.com/hitz-zentroa/latxa-instruct</a> </p>
<blockquote>
<p>ä½¿ç”¨ç”¨æˆ·æ„å›¾æŒ‡å¯¼è¯­è¨€æ¨¡å‹éœ€è¦å¤§é‡çš„æŒ‡ä»¤æ•°æ®é›†ï¼Œè€Œè¿™äº›æ•°æ®ä»…å¯¹æœ‰é™çš„è¯­è¨€é›†å¯ç”¨ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢åœ¨ä½èµ„æºåœºæ™¯ä¸­ä½¿ç”¨ä¼ ç»ŸæŒ‡ä»¤é€‚åº”æµç¨‹æ›¿ä»£æ–¹æ¡ˆã€‚æˆ‘ä»¬å‡è®¾ä½èµ„æºè¯­è¨€çš„ç°å®åœºæ™¯ï¼Œå…¶ä¸­åªæœ‰ä»¥ä¸‹èµ„æºå¯ç”¨ï¼šç›®æ ‡è¯­è¨€è¯­æ–™åº“ã€ç°æœ‰çš„å¼€æ”¾å¼æƒé‡å¤šè¯­è¨€åŸºç¡€å’ŒæŒ‡ä»¤å‹ä¸»å¹²LLMï¼Œä»¥åŠä»æŒ‡ä»¤å‹ä¸»å¹²ä¸­é‡‡æ ·çš„äººå·¥åˆæˆæŒ‡ä»¤ã€‚æˆ‘ä»¬å¯¹å·´æ–¯å…‹è¯­è¿›è¡Œäº†ä¸€ç³»åˆ—ç»¼åˆå®éªŒï¼Œè¿™äº›å®éªŒç³»ç»Ÿåœ°ç ”ç©¶äº†è¿™äº›ç»„ä»¶çš„ä¸åŒç»„åˆåœ¨åŸºå‡†æµ‹è¯•å’Œäººç±»åå¥½ä¸Šçš„è¯„ä¼°ï¼ˆç”±1680åå‚ä¸è€…è¿›è¡Œï¼‰ã€‚æˆ‘ä»¬çš„ç»“è®ºè¡¨æ˜ï¼Œç›®æ ‡è¯­è¨€è¯­æ–™åº“è‡³å…³é‡è¦ï¼ŒåˆæˆæŒ‡ä»¤äº§ç”Ÿäº†ç¨³å¥æ¨¡å‹ï¼Œæœ€é‡è¦çš„æ˜¯ä½¿ç”¨æŒ‡ä»¤è°ƒæ•´æ¨¡å‹ä½œä¸ºä¸»å¹²ä¼˜äºä½¿ç”¨åŸºç¡€éæŒ‡ä»¤æ¨¡å‹ã€‚æ‰©å¤§è§„æ¨¡è‡³ä½¿ç”¨Llama 3.1ä½œä¸ºä¸»å¹²æ—¶ï¼Œæˆ‘ä»¬çš„æ¨¡å‹æ¥è¿‘å·´æ–¯å…‹è¯­çš„å¤§å‹å‰æ²¿æ¨¡å‹ï¼Œå¹¶ä¸”æœªä½¿ç”¨ä»»ä½•å·´æ–¯å…‹è¯­æŒ‡ä»¤ã€‚æˆ‘ä»¬å‘å¸ƒä»£ç ã€æ¨¡å‹ã€æŒ‡ä»¤æ•°æ®é›†å’Œäººç±»åå¥½ï¼Œä»¥æ”¯æŒæœªæ¥ä½èµ„æºè¯­è¨€é€‚åº”ç ”ç©¶ä¸­çš„å…¨é¢å¯é‡å¤æ€§ã€‚å¯è®¿é—® <a target="_blank" rel="noopener" href="https://github.com/hitz-zentroa/latxa-instruct">https://github.com/hitz-zentroa/latxa-instruct</a> äº†è§£æ›´å¤šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07597v2">PDF</a> Accepted at EMNLP 2025 Main Conference</p>
<p><strong>Summary</strong><br>åœ¨èµ„æºæœ‰é™çš„è¯­è¨€åœºæ™¯ä¸‹ï¼Œä½¿ç”¨ç”¨æˆ·æ„å›¾æŒ‡å¯¼è¯­è¨€æ¨¡å‹éœ€è¦å¤§å‹æŒ‡ä»¤æ•°æ®é›†ï¼Œè€Œè¿™äº›æ•°æ®é›†åªå¯¹æœ‰é™çš„å‡ ç§è¯­è¨€å¯ç”¨ã€‚æœ¬æ–‡æ¢ç´¢åœ¨ä½èµ„æºæƒ…å†µä¸‹æ›¿ä»£ä¼ ç»ŸæŒ‡ä»¤é€‚åº”æµç¨‹çš„æ–¹æ³•ã€‚å‡è®¾å¯¹äºä½èµ„æºè¯­è¨€åªæœ‰ç›®æ ‡è¯­è¨€è¯­æ–™åº“ã€ç°æœ‰çš„å¼€æ”¾æƒé‡å¤šè¯­è¨€åŸºç¡€åŠæŒ‡å¯¼å‹éª¨æ¶LLMså’Œä»æŒ‡å¯¼å‹éª¨æ¶ä¸­æŠ½å–çš„åˆæˆæŒ‡ä»¤å¯ç”¨ã€‚æˆ‘ä»¬å¯¹å·´æ–¯å…‹è¯­è¿›è¡Œäº†ä¸€ç³»åˆ—ç»¼åˆå®éªŒï¼Œç³»ç»Ÿåœ°ç ”ç©¶äº†è¿™äº›ç»„ä»¶çš„ä¸åŒç»„åˆï¼Œå¹¶åœ¨åŸºå‡†æµ‹è¯•å’Œäººç±»åå¥½ï¼ˆæ¥è‡ª1680åå‚ä¸è€…ï¼‰ä¸­è¿›è¡Œäº†è¯„ä¼°ã€‚ç ”ç©¶è¡¨æ˜ç›®æ ‡è¯­è¨€è¯­æ–™åº“è‡³å…³é‡è¦ï¼ŒåˆæˆæŒ‡ä»¤èƒ½äº§ç”Ÿç¨³å¥æ¨¡å‹ï¼Œæœ€é‡è¦çš„æ˜¯ä½¿ç”¨æŒ‡ä»¤è°ƒä¼˜æ¨¡å‹ä½œä¸ºéª¨æ¶ä¼˜äºä½¿ç”¨åŸºç¡€éæŒ‡ä»¤æ¨¡å‹ã€‚æ‰©å¤§è‡³Llama 3.1æŒ‡å¯¼70Bä½œä¸ºéª¨æ¶ï¼Œæˆ‘ä»¬çš„æ¨¡å‹æ¥è¿‘å‰æ²¿çš„å·´æ–¯å…‹è¯­å¤§å‹æ¨¡å‹ï¼Œä¸”æœªä½¿ç”¨ä»»ä½•å·´æ–¯å…‹è¯­æŒ‡ä»¤ã€‚æˆ‘ä»¬å‘å¸ƒä»£ç ã€æ¨¡å‹ã€æŒ‡ä»¤æ•°æ®é›†å’Œäººç±»åå¥½ï¼Œä»¥æ”¯æŒæœªæ¥ä½èµ„æºè¯­è¨€é€‚åº”ç ”ç©¶ä¸­çš„å®Œå…¨å¯é‡å¤æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åœ¨ä½èµ„æºè¯­è¨€åœºæ™¯ä¸‹ï¼Œéœ€è¦ä½¿ç”¨æ›¿ä»£ä¼ ç»ŸæŒ‡ä»¤é€‚åº”æµç¨‹çš„æ–¹æ³•ã€‚</li>
<li>ç›®æ ‡è¯­è¨€è¯­æ–™åº“åœ¨é€‚åº”è¯­è¨€æ¨¡å‹è¿‡ç¨‹ä¸­è‡³å…³é‡è¦ã€‚</li>
<li>åˆæˆæŒ‡ä»¤å¯ä»¥äº§ç”Ÿç¨³å¥çš„è¯­è¨€æ¨¡å‹ã€‚</li>
<li>ä½¿ç”¨æŒ‡ä»¤è°ƒä¼˜æ¨¡å‹ä½œä¸ºéª¨æ¶ä¼˜äºä½¿ç”¨åŸºç¡€éæŒ‡ä»¤æ¨¡å‹ã€‚</li>
<li>é€šè¿‡æ‰©å¤§æ¨¡å‹è§„æ¨¡ï¼Œå¦‚ä½¿ç”¨Llama 3.1æŒ‡å¯¼70Bä½œä¸ºéª¨æ¶ï¼Œå¯ä»¥åœ¨ä¸ä½¿ç”¨ç‰¹å®šè¯­è¨€æŒ‡ä»¤çš„æƒ…å†µä¸‹æ¥è¿‘å‰æ²¿æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>ä»£ç ã€æ¨¡å‹ã€æŒ‡ä»¤æ•°æ®é›†å’Œäººç±»åå¥½ç ”ç©¶ç»“æœçš„å…¬å¼€æœ‰åŠ©äºæœªæ¥ç ”ç©¶çš„å¯é‡å¤æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07597">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-373cce1d8acb56460abecc9b35b4d425" align="middle">
<img src="https://picx.zhimg.com/v2-735e8bcda26cc30efd7362d6b48d2474" align="middle">
<img src="https://picx.zhimg.com/v2-7a8e68a784fe6a5bba92319ead7c23cb" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Struct2D-A-Perception-Guided-Framework-for-Spatial-Reasoning-in-MLLMs"><a href="#Struct2D-A-Perception-Guided-Framework-for-Spatial-Reasoning-in-MLLMs" class="headerlink" title="Struct2D: A Perception-Guided Framework for Spatial Reasoning in MLLMs"></a>Struct2D: A Perception-Guided Framework for Spatial Reasoning in MLLMs</h2><p><strong>Authors:Fangrui Zhu, Hanhui Wang, Yiming Xie, Jing Gu, Tianye Ding, Jianwei Yang, Huaizu Jiang</strong></p>
<p>Unlocking spatial reasoning in Multimodal Large Language Models (MLLMs) is crucial for enabling intelligent interaction with 3D environments. While prior efforts often rely on explicit 3D inputs or specialized model architectures, we ask: can MLLMs reason about 3D space using only structured 2D representations derived from perception? We introduce Struct2D, a perception-guided prompting framework that combines birdâ€™s-eye-view (BEV) images with object marks and object-centric metadata, optionally incorporating egocentric keyframes when needed. Using Struct2D, we conduct an in-depth zero-shot analysis of closed-source MLLMs (e.g., GPT-o3) and find that they exhibit surprisingly strong spatial reasoning abilities when provided with structured 2D inputs, effectively handling tasks such as relative direction estimation and route planning. Building on these insights, we construct Struct2D-Set, a large-scale instruction tuning dataset with 200K fine-grained QA pairs across eight spatial reasoning categories, generated automatically from 3D indoor scenes. We fine-tune an open-source MLLM (Qwen2.5VL) on Struct2D-Set, achieving competitive performance on multiple benchmarks, including 3D question answering, dense captioning, and object grounding. Our approach demonstrates that structured 2D inputs can effectively bridge perception and language reasoning in MLLMs-without requiring explicit 3D representations as input. We will release both our code and dataset to support future research. </p>
<blockquote>
<p>åœ¨Multimodalå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸­è§£é”ç©ºé—´æ¨ç†èƒ½åŠ›å¯¹äºå®ç°ä¸3Dç¯å¢ƒçš„æ™ºèƒ½äº¤äº’è‡³å…³é‡è¦ã€‚è™½ç„¶æ—©æœŸçš„åŠªåŠ›ç»å¸¸ä¾èµ–äºæ˜ç¡®çš„3Dè¾“å…¥æˆ–ä¸“é—¨çš„æ¨¡å‹æ¶æ„ï¼Œä½†æˆ‘ä»¬çš„é—®é¢˜æ˜¯ï¼šMLLMsèƒ½å¦ä»…ä½¿ç”¨æ¥è‡ªæ„ŸçŸ¥çš„ç»“æ„åŒ–2Dè¡¨ç¤ºæ¥æ¨ç†3Dç©ºé—´ï¼Ÿæˆ‘ä»¬å¼•å…¥äº†Struct2Dï¼Œè¿™æ˜¯ä¸€ä¸ªæ„ŸçŸ¥å¼•å¯¼æç¤ºæ¡†æ¶ï¼Œå®ƒå°†é¸Ÿç°å›¾ï¼ˆBEVï¼‰å›¾åƒä¸å¯¹è±¡æ ‡è®°å’Œå¯¹è±¡ä¸­å¿ƒå…ƒæ•°æ®ç›¸ç»“åˆï¼Œåœ¨éœ€è¦æ—¶è¿˜å¯é€‰æ‹©æ€§åœ°èå…¥ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„å…³é”®å¸§ã€‚ä½¿ç”¨Struct2Dï¼Œæˆ‘ä»¬å¯¹å°é—­çš„MLLMsï¼ˆä¾‹å¦‚GPT-o3ï¼‰è¿›è¡Œäº†æ·±å…¥çš„é›¶æ ·æœ¬åˆ†æï¼Œå¹¶å‘ç°å½“æä¾›ç»“æ„åŒ–2Dè¾“å…¥æ—¶ï¼Œå®ƒä»¬è¡¨ç°å‡ºä»¤äººæƒŠè®¶çš„å¼ºå¤§çš„ç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œå¯ä»¥æœ‰æ•ˆåœ°å¤„ç†ç›¸å¯¹æ–¹å‘ä¼°è®¡å’Œè·¯çº¿è§„åˆ’ç­‰ä»»åŠ¡ã€‚åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬æ„å»ºäº†Struct2D-Setï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ï¼ŒåŒ…å«8ä¸ªç©ºé—´æ¨ç†ç±»åˆ«çš„20ä¸‡ä¸ªç²¾ç»†ç²’åº¦é—®ç­”å¯¹ï¼Œè¿™äº›é—®ç­”å¯¹è‡ªåŠ¨ä»3Då®¤å†…åœºæ™¯ä¸­ç”Ÿæˆã€‚æˆ‘ä»¬åœ¨Struct2D-Setä¸Šå¯¹å¼€æºMLLMï¼ˆQwen2.5VLï¼‰è¿›è¡Œäº†å¾®è°ƒï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„è¡¨ç°ï¼ŒåŒ…æ‹¬3Dé—®ç­”ã€å¯†é›†å­—å¹•å’Œå¯¹è±¡å®šä½ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¡¨æ˜ï¼Œç»“æ„åŒ–2Dè¾“å…¥å¯ä»¥æœ‰æ•ˆåœ°åœ¨MLLMsä¸­æ¶èµ·æ„ŸçŸ¥å’Œè¯­è¨€æ¨ç†çš„æ¡¥æ¢ï¼Œè€Œæ— éœ€æ˜ç¡®çš„3Dè¡¨ç¤ºä½œä¸ºè¾“å…¥ã€‚æˆ‘ä»¬å°†å‘å¸ƒæˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®é›†ä»¥æ”¯æŒæœªæ¥çš„ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04220v3">PDF</a> NeurIPS 2025, code link: <a target="_blank" rel="noopener" href="https://github.com/neu-vi/struct2d">https://github.com/neu-vi/struct2d</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢ç´¢äº†åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸­è§£é”ç©ºé—´æ¨ç†èƒ½åŠ›çš„é‡è¦æ€§ï¼Œè¿™å¯¹äºå®ç°ä¸3Dç¯å¢ƒçš„æ™ºèƒ½äº¤äº’è‡³å…³é‡è¦ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºStruct2Dçš„æ„ŸçŸ¥å¼•å¯¼æç¤ºæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†é¸Ÿç°å›¾ã€å¯¹è±¡æ ‡è®°å’Œå¯¹è±¡çº§å…ƒæ•°æ®ï¼Œå¹¶åœ¨éœ€è¦æ—¶çº³å…¥ç¬¬ä¸€äººç§°è§†è§’çš„å…³é”®å¸§ã€‚é€šè¿‡æ·±å…¥çš„æ— æºåˆ†æï¼Œå‘ç°å°é—­çš„MLLMåœ¨æä¾›ç»“æ„åŒ–2Dè¾“å…¥æ—¶å±•ç°å‡ºå¼ºå¤§çš„ç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œèƒ½æœ‰æ•ˆåœ°å¤„ç†ç›¸å¯¹æ–¹å‘ä¼°è®¡å’Œè·¯çº¿è§„åˆ’ç­‰ä»»åŠ¡ã€‚åŸºäºæ­¤ï¼Œç ”ç©¶æ„å»ºäº†Struct2D-Setæ•°æ®é›†ï¼ŒåŒ…å«20ä¸‡ä¸ªç²¾ç»†é—®ç­”å¯¹ï¼Œè·¨è¶Šå…«ä¸ªç©ºé—´æ¨ç†ç±»åˆ«ï¼Œè‡ªåŠ¨ä»3Då®¤å†…åœºæ™¯ä¸­ç”Ÿæˆã€‚åœ¨Struct2D-Setä¸Šå¾®è°ƒå¼€æºMLLMï¼Œå®ç°äº†åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„ç«äº‰åŠ›è¡¨ç°ï¼ŒåŒ…æ‹¬3Dé—®ç­”ã€å¯†é›†å­—å¹•å’Œå¯¹è±¡å®šä½ã€‚è¯¥ç ”ç©¶å±•ç¤ºäº†ç»“æ„åŒ–2Dè¾“å…¥åœ¨MLLMsä¸­æœ‰æ•ˆæ¡¥æ¥æ„ŸçŸ¥å’Œè¯­è¨€æ¨ç†çš„èƒ½åŠ›ï¼Œæ— éœ€æ˜ç¡®çš„3Dè¡¨ç¤ºä½œä¸ºè¾“å…¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„ç©ºé—´æ¨ç†èƒ½åŠ›å¯¹äºå®ç°ä¸3Dç¯å¢ƒçš„æ™ºèƒ½äº¤äº’è‡³å…³é‡è¦ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªåä¸ºStruct2Dçš„æ„ŸçŸ¥å¼•å¯¼æç¤ºæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†é¸Ÿç°å›¾ã€å¯¹è±¡æ ‡è®°å’Œå¯¹è±¡çº§å…ƒæ•°æ®ã€‚</li>
<li>å°é—­å¼çš„MLLMåœ¨æä¾›ç»“æ„åŒ–2Dè¾“å…¥æ—¶å±•ç°å‡ºå¼ºå¤§çš„ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡æ·±åº¦åˆ†æå‘ç°ï¼Œç»“æ„åŒ–2Dè¾“å…¥èƒ½æœ‰æ•ˆå¤„ç†ç›¸å¯¹æ–¹å‘ä¼°è®¡å’Œè·¯çº¿è§„åˆ’ç­‰ä»»åŠ¡ã€‚</li>
<li>æ„å»ºäº†Struct2D-Setæ•°æ®é›†ï¼Œç”¨äºå¾®è°ƒMLLMæ¨¡å‹ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºç«äº‰åŠ›ã€‚</li>
<li>ç ”ç©¶è¯æ˜äº†ç»“æ„åŒ–2Dè¾“å…¥èƒ½å¤Ÿæ¡¥æ¥æ„ŸçŸ¥å’Œè¯­è¨€æ¨ç†ï¼Œæ— éœ€æ˜ç¡®çš„3Dè¡¨ç¤ºä½œä¸ºè¾“å…¥ã€‚</li>
<li>ä»£ç å’Œæ•°æ®é›†å°†å…¬å¼€å‘å¸ƒï¼Œä»¥æ”¯æŒæœªæ¥ç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04220">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-16093911351f4c496ec742d6c642eac4" align="middle">
<img src="https://picx.zhimg.com/v2-f79e274815898482868ee2eaf10faedc" align="middle">
<img src="https://picx.zhimg.com/v2-997d2db009f33c6a9bc2b4208e4a582d" align="middle">
<img src="https://picx.zhimg.com/v2-4dcc0704eb216d900f7059baa77b2c52" align="middle">
<img src="https://picx.zhimg.com/v2-eef3ce0c2681591a7f9ba9a338b6bc20" align="middle">
<img src="https://picx.zhimg.com/v2-77e741e3bc7fcbcc625972835c9b40d3" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="R2R-Efficiently-Navigating-Divergent-Reasoning-Paths-with-Small-Large-Model-Token-Routing"><a href="#R2R-Efficiently-Navigating-Divergent-Reasoning-Paths-with-Small-Large-Model-Token-Routing" class="headerlink" title="R2R: Efficiently Navigating Divergent Reasoning Paths with Small-Large   Model Token Routing"></a>R2R: Efficiently Navigating Divergent Reasoning Paths with Small-Large   Model Token Routing</h2><p><strong>Authors:Tianyu Fu, Yi Ge, Yichen You, Enshu Liu, Zhihang Yuan, Guohao Dai, Shengen Yan, Huazhong Yang, Yu Wang</strong></p>
<p>Large Language Models (LLMs) achieve impressive reasoning capabilities at the cost of substantial inference overhead, posing substantial deployment challenges. Although distilled Small Language Models (SLMs) significantly enhance efficiency, their performance suffers as they fail to follow LLMsâ€™ reasoning paths. Luckily, we reveal that only a small fraction of tokens genuinely diverge reasoning paths between LLMs and SLMs. Most generated tokens are either identical or exhibit neutral differences, such as minor variations in abbreviations or expressions. Leveraging this insight, we introduce <strong>Roads to Rome (R2R)</strong>, a neural token routing method that selectively utilizes LLMs only for these critical, path-divergent tokens, while leaving the majority of token generation to the SLM. We also develop an automatic data generation pipeline that identifies divergent tokens and generates token-level routing labels to train the lightweight router. We apply R2R to combine R1-1.5B and R1-32B models from the DeepSeek family, and evaluate on challenging math, coding, and QA benchmarks. With an average activated parameter size of 5.6B, R2R surpasses the average accuracy of R1-7B by 1.6x, outperforming even the R1-14B model. Compared to R1-32B, it delivers a 2.8x wall-clock speedup with comparable performance, advancing the Pareto frontier of test-time scaling efficiency. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/thu-nics/R2R">https://github.com/thu-nics/R2R</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä»˜å‡ºäº†å·¨å¤§çš„æ¨ç†å¼€é”€åè·å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„æ¨ç†èƒ½åŠ›ï¼Œè¿™å¸¦æ¥äº†éƒ¨ç½²ä¸Šçš„å·¨å¤§æŒ‘æˆ˜ã€‚å°½ç®¡è’¸é¦çš„å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰æ˜¾è‘—æé«˜äº†æ•ˆç‡ï¼Œä½†å…¶æ€§èƒ½å´é­å—äº†æŸå¤±ï¼Œå› ä¸ºå®ƒä»¬æ— æ³•éµå¾ªLLMçš„æ¨ç†è·¯å¾„ã€‚å¹¸è¿çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°åªæœ‰ä¸€å°éƒ¨åˆ†æ ‡è®°åœ¨LLMå’ŒSLMä¹‹é—´çœŸæ­£åç¦»äº†æ¨ç†è·¯å¾„ã€‚å¤§å¤šæ•°ç”Ÿæˆçš„æ ‡è®°éƒ½æ˜¯ç›¸åŒçš„ï¼Œæˆ–è€…è¡¨ç°å‡ºä¸­æ€§çš„å·®å¼‚ï¼Œä¾‹å¦‚ç¼©å†™æˆ–è¡¨è¾¾ä¸Šçš„å¾®å°å˜åŒ–ã€‚åŸºäºè¿™ä¸€è§è§£ï¼Œæˆ‘ä»¬å¼•å…¥äº†<strong>ç½—é©¬ä¹‹è·¯ï¼ˆR2Rï¼‰</strong>ï¼Œè¿™æ˜¯ä¸€ç§ç¥ç»æ ‡è®°è·¯ç”±æ–¹æ³•ï¼Œå®ƒåªé€‰æ‹©æ€§åœ°ä½¿ç”¨LLMæ¥å¤„ç†è¿™äº›å…³é”®çš„ã€è·¯å¾„åˆ†æ­§çš„æ ‡è®°ï¼Œè€Œå°†å¤§å¤šæ•°æ ‡è®°ç”Ÿæˆç•™ç»™SLMã€‚æˆ‘ä»¬è¿˜å¼€å‘äº†ä¸€ä¸ªè‡ªåŠ¨æ•°æ®ç”Ÿæˆç®¡é“ï¼Œç”¨äºè¯†åˆ«åˆ†æ­§çš„æ ‡è®°å¹¶ç”Ÿæˆæ ‡è®°çº§åˆ«çš„è·¯ç”±æ ‡ç­¾æ¥è®­ç»ƒè½»é‡çº§è·¯ç”±å™¨ã€‚æˆ‘ä»¬å°†R2Råº”ç”¨äºDeepSeekç³»åˆ—çš„R1-1.5Bå’ŒR1-32Bæ¨¡å‹ï¼Œå¹¶åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦ã€ç¼–ç å’Œé—®ç­”åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œè¯„ä¼°ã€‚R2Rçš„å¹³å‡æ¿€æ´»å‚æ•°å¤§å°ä¸º5.6Bï¼Œå…¶å¹³å‡å‡†ç¡®ç‡è¶…è¿‡äº†R1-7Bçš„1.6å€ï¼Œç”šè‡³è¶…è¶Šäº†R1-14Bæ¨¡å‹çš„è¡¨ç°ã€‚ä¸R1-32Bç›¸æ¯”ï¼Œå®ƒåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶å®ç°äº†2.8å€çš„å®æ—¶åŠ é€Ÿï¼Œæ¨åŠ¨äº†æµ‹è¯•æ—¶ç¼©æ”¾æ•ˆç‡çš„å‰æ²¿ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/thu-nics/R2R%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/thu-nics/R2Rè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21600v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·å¤‡å‡ºè‰²çš„æ¨ç†èƒ½åŠ›ï¼Œä½†æ¨ç†å¼€é”€è¾ƒå¤§ï¼Œéƒ¨ç½²æŒ‘æˆ˜æ˜¾è‘—ã€‚è’¸é¦çš„å°è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰è™½èƒ½æé«˜æ•ˆç‡ï¼Œä½†æ€§èƒ½ä¸‹é™ï¼Œæ— æ³•è·ŸéšLLMçš„æ¨ç†è·¯å¾„ã€‚ç ”ç©¶å‘ç°ï¼ŒLLMså’ŒSLMsä¹‹é—´çœŸæ­£æ”¹å˜æ¨ç†è·¯å¾„çš„ä»¤ç‰Œåªæœ‰ä¸€å°éƒ¨åˆ†ã€‚å¤§å¤šæ•°ç”Ÿæˆçš„ä»¤ç‰Œéƒ½æ˜¯ç›¸åŒçš„ï¼Œæˆ–è€…åªæ˜¯è¡¨ç°å‡ºä¸­æ€§å·®å¼‚ï¼Œå¦‚ç¼©å†™æˆ–è¡¨è¾¾ä¸Šçš„å¾®å°å˜åŒ–ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºä¸€ç§åä¸ºâ€œç½—é©¬ä¹‹è·¯ï¼ˆR2Rï¼‰â€çš„ç¥ç»ä»¤ç‰Œè·¯ç”±æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä»…é’ˆå¯¹è¿™äº›å…³é”®çš„ã€è·¯å¾„åˆ†æ­§çš„ä»¤ç‰Œä½¿ç”¨LLMsï¼Œè€Œå°†å¤§å¤šæ•°ä»¤ç‰Œç”Ÿæˆç•™ç»™SLMã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªè‡ªåŠ¨æ•°æ®ç”Ÿæˆç®¡é“ï¼Œç”¨äºè¯†åˆ«åˆ†æ­§ä»¤ç‰Œå¹¶ç”Ÿæˆä»¤ç‰Œçº§è·¯ç”±æ ‡ç­¾æ¥è®­ç»ƒè½»é‡çº§è·¯ç”±å™¨ã€‚å°†R2Råº”ç”¨äºDeepSeekç³»åˆ—çš„R1-1.5Bå’ŒR1-32Bæ¨¡å‹ï¼Œå¹¶åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦ã€ç¼–ç å’Œé—®ç­”åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œè¯„ä¼°ã€‚åœ¨å¹³å‡æ¿€æ´»å‚æ•°å¤§å°ä¸º5.6Bçš„æƒ…å†µä¸‹ï¼ŒR2Rè¶…è¶Šäº†R1-7Bçš„å¹³å‡å‡†ç¡®ç‡ï¼Œç”šè‡³è¶…è¿‡äº†R1-14Bæ¨¡å‹ã€‚ä¸R1-32Bç›¸æ¯”ï¼Œå®ƒåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶å®ç°äº†2.8å€çš„å®æ—¶åŠ é€Ÿï¼Œæ¨è¿›äº†æµ‹è¯•æ—¶è§„æ¨¡æ•ˆç‡çš„å‰æ²¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsæ‹¥æœ‰å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ä½†æ¨ç†å¼€é”€å¤§ï¼Œéƒ¨ç½²å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>SLMsæé«˜æ•ˆç‡ä½†æ€§èƒ½ä¸‹é™ï¼Œä¸èƒ½å®Œå…¨è·ŸéšLLMçš„æ¨ç†è·¯å¾„ã€‚</li>
<li>åªæœ‰ä¸€å°éƒ¨åˆ†ä»¤ç‰Œåœ¨LLMså’ŒSLMsä¹‹é—´çœŸæ­£æ”¹å˜æ¨ç†è·¯å¾„ã€‚</li>
<li>å¼•å…¥R2Ræ–¹æ³•ï¼Œé€‰æ‹©æ€§ä½¿ç”¨LLMså¤„ç†å…³é”®ä»¤ç‰Œï¼Œå…¶ä½™ç”±SLMå¤„ç†ã€‚</li>
<li>R2Ré€šè¿‡è‡ªåŠ¨æ•°æ®ç”Ÿæˆç®¡é“è¯†åˆ«åˆ†æ­§ä»¤ç‰Œå¹¶ç”Ÿæˆè·¯ç”±æ ‡ç­¾ã€‚</li>
<li>R2Råœ¨åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå¹³å‡æ¿€æ´»å‚æ•°å°ä¸”æ€§èƒ½è¶…è¿‡å¤§å‹æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21600">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-04f705dd79a9f2106c812ec22ebf475e" align="middle">
<img src="https://picx.zhimg.com/v2-af90f013cfaa0c3cf8562f5853c418f8" align="middle">
<img src="https://picx.zhimg.com/v2-f48be78f655b0e62c85d5e8dc2cf6033" align="middle">
<img src="https://picx.zhimg.com/v2-12edcd2c5935b66fd1a0599fc53700de" align="middle">
<img src="https://picx.zhimg.com/v2-34652b756a9fdd416f72e5084a7ba2fa" align="middle">
<img src="https://picx.zhimg.com/v2-099b38e423ca5db536b5bf94dfcbd5c5" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Scaling-Diffusion-Transformers-Efficiently-via-Î¼-P"><a href="#Scaling-Diffusion-Transformers-Efficiently-via-Î¼-P" class="headerlink" title="Scaling Diffusion Transformers Efficiently via $Î¼$P"></a>Scaling Diffusion Transformers Efficiently via $Î¼$P</h2><p><strong>Authors:Chenyu Zheng, Xinyu Zhang, Rongzhen Wang, Wei Huang, Zhi Tian, Weilin Huang, Jun Zhu, Chongxuan Li</strong></p>
<p>Diffusion Transformers have emerged as the foundation for vision generative models, but their scalability is limited by the high cost of hyperparameter (HP) tuning at large scales. Recently, Maximal Update Parametrization ($\mu$P) was proposed for vanilla Transformers, which enables stable HP transfer from small to large language models, and dramatically reduces tuning costs. However, it remains unclear whether $\mu$P of vanilla Transformers extends to diffusion Transformers, which differ architecturally and objectively. In this work, we generalize standard $\mu$P to diffusion Transformers and validate its effectiveness through large-scale experiments. First, we rigorously prove that $\mu$P of mainstream diffusion Transformers, including U-ViT, DiT, PixArt-$\alpha$, and MMDiT, aligns with that of the vanilla Transformer, enabling the direct application of existing $\mu$P methodologies. Leveraging this result, we systematically demonstrate that DiT-$\mu$P enjoys robust HP transferability. Notably, DiT-XL-2-$\mu$P with transferred learning rate achieves 2.9 times faster convergence than the original DiT-XL-2. Finally, we validate the effectiveness of $\mu$P on text-to-image generation by scaling PixArt-$\alpha$ from 0.04B to 0.61B and MMDiT from 0.18B to 18B. In both cases, models under $\mu$P outperform their respective baselines while requiring small tuning cost, only 5.5% of one training run for PixArt-$\alpha$ and 3% of consumption by human experts for MMDiT-18B. These results establish $\mu$P as a principled and efficient framework for scaling diffusion Transformers. </p>
<blockquote>
<p>æ‰©æ•£Transformerå·²ç»æˆä¸ºè§†è§‰ç”Ÿæˆæ¨¡å‹çš„åŸºç¡€ï¼Œä½†å…¶å¯æ‰©å±•æ€§å—åˆ°å¤§è§„æ¨¡è¶…å‚æ•°ï¼ˆHPï¼‰è°ƒæ•´çš„é«˜æˆæœ¬çš„é™åˆ¶ã€‚æœ€è¿‘ï¼Œé’ˆå¯¹æ™®é€šTransformeræå‡ºäº†æœ€å¤§æ›´æ–°å‚æ•°åŒ–ï¼ˆ$\mu$Pï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿå®ç°ä»å°å‹åˆ°å¤§å‹è¯­è¨€æ¨¡å‹çš„ç¨³å®šHPè¿ç§»ï¼Œå¹¶å¤§å¹…åº¦é™ä½è°ƒæ•´æˆæœ¬ã€‚ç„¶è€Œï¼Œå°šä¸æ¸…æ¥šæ™®é€šTransformerçš„$\mu$Pæ˜¯å¦é€‚ç”¨äºæ¶æ„å’Œå®¢è§‚ä¸Šå­˜åœ¨å·®å¼‚çš„æ‰©æ•£Transformerã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å°†æ ‡å‡†$\mu$Pæ¨å¹¿åˆ°æ‰©æ•£Transformerï¼Œå¹¶é€šè¿‡å¤§è§„æ¨¡å®éªŒéªŒè¯å…¶æœ‰æ•ˆæ€§ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä¸¥æ ¼è¯æ˜äº†ä¸»æµæ‰©æ•£Transformerï¼ŒåŒ…æ‹¬U-ViTã€DiTã€PixArt-$\alpha$å’ŒMMDiTï¼Œä¸æ™®é€šTransformerçš„$\mu$Pä¸€è‡´æ€§ï¼Œä½¿å¾—ç°æœ‰$\mu$Pæ–¹æ³•å¯ä»¥ç›´æ¥åº”ç”¨ã€‚åˆ©ç”¨è¿™ä¸€ç»“æœï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°è¯æ˜äº†DiT-$\mu$På…·æœ‰ç¨³å¥çš„HPå¯è¿ç§»æ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œé‡‡ç”¨è¿ç§»å­¦ä¹ ç‡çš„DiT-XL-2-$\mu$På®ç°äº†æ¯”åŸå§‹DiT-XL-2å¿«2.9å€çš„æ”¶æ•›ã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡å°†PixArt-$\alpha$ä»0.04Bæ‰©å±•åˆ°0.61Bå’Œå°†MMDiTä»0.18Bæ‰©å±•åˆ°18Bæ¥éªŒè¯$\mu$Påœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­çš„æœ‰æ•ˆæ€§ã€‚åœ¨è¿™ä¸¤ç§æƒ…å†µä¸‹ï¼Œ$\mu$Pä¸‹çš„æ¨¡å‹è¡¨ç°å‡ä¼˜äºå„è‡ªåŸºçº¿ï¼ŒåŒæ—¶éœ€è¦è¾ƒå°çš„è°ƒæ•´æˆæœ¬ï¼ŒPixArt-$\alpha$åªéœ€5.5%çš„ä¸€æ¬¡è®­ç»ƒè¿è¡Œæˆæœ¬ï¼Œè€ŒMMDiT-18Båªéœ€3%çš„äººåŠ›ä¸“å®¶æŠ•å…¥ã€‚è¿™äº›ç»“æœç¡®ç«‹äº†$\mu$Pä½œä¸ºä¸€ä¸ªæœ‰åŸåˆ™ã€é«˜æ•ˆç‡çš„æ¡†æ¶ï¼Œç”¨äºæ‰©å±•æ‰©æ•£Transformerã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15270v3">PDF</a> Accepted by NeurIPS 2025, 38 pages, 10 figures, 17 tables</p>
<p><strong>Summary</strong><br>    æ‰©æ•£Transformerå·²æˆä¸ºè§†è§‰ç”Ÿæˆæ¨¡å‹çš„åŸºç¡€ï¼Œä½†å…¶å¯æ‰©å±•æ€§å—åˆ°å¤§è§„æ¨¡è¶…å‚æ•°ï¼ˆHPï¼‰è°ƒæ•´çš„é«˜æˆæœ¬çš„é™åˆ¶ã€‚æœ€è¿‘ï¼Œé’ˆå¯¹æ™®é€šTransformerçš„æœ€å¤§æ›´æ–°å‚æ•°åŒ–ï¼ˆÎ¼Pï¼‰æ–¹æ³•è¢«æå‡ºï¼Œèƒ½å¤Ÿå®ç°ä»å°å‹åˆ°å¤§å‹è¯­è¨€æ¨¡å‹çš„ç¨³å®šHPè¿ç§»ï¼Œå¹¶æ˜¾è‘—é™ä½è°ƒæ•´æˆæœ¬ã€‚æœ¬æ–‡å°†Î¼Pæ¨å¹¿åˆ°æ‰©æ•£Transformerï¼Œå¹¶é€šè¿‡å¤§è§„æ¨¡å®éªŒéªŒè¯å…¶æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬è¯æ˜äº†ä¸»æµæ‰©æ•£Transformerçš„Î¼Pä¸æ™®é€šTransformerçš„ä¸€è‡´æ€§ï¼Œå®ç°äº†ç°æœ‰Î¼Pæ–¹æ³•è®ºçš„ç›´æ¥åº”ç”¨ã€‚åˆ©ç”¨è¿™ä¸€ç»“æœï¼Œæˆ‘ä»¬ç³»ç»Ÿå±•ç¤ºäº†DiT-Î¼Pçš„ç¨³å¥HPè¿ç§»æ€§ã€‚PixArt-Î±å’ŒMMDiTåœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­çš„å®éªŒéªŒè¯äº†Î¼Pçš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£Transformerå·²æˆä¸ºè§†è§‰ç”Ÿæˆæ¨¡å‹çš„é‡è¦åŸºç¡€ï¼Œä½†åœ¨å¤§è§„æ¨¡åº”ç”¨ä¸­é¢ä¸´é«˜æˆæœ¬è¶…å‚æ•°è°ƒæ•´çš„æŒ‘æˆ˜ã€‚</li>
<li>æœ€å¤§æ›´æ–°å‚æ•°åŒ–ï¼ˆÎ¼Pï¼‰æ–¹æ³•è¢«æå‡ºå¹¶æˆåŠŸåº”ç”¨äºæ™®é€šTransformerï¼Œèƒ½å¤Ÿå®ç°ç¨³å®šHPè¿ç§»å¹¶é™ä½è°ƒæ•´æˆæœ¬ã€‚</li>
<li>æœ¬æ–‡å°†Î¼Pæ¨å¹¿åˆ°æ‰©æ•£Transformerï¼Œå¹¶é€šè¿‡å®éªŒè¯æ˜å…¶æœ‰æ•ˆæ€§ã€‚</li>
<li>Î¼På…è®¸ç›´æ¥åº”ç”¨ç°æœ‰æ–¹æ³•åˆ°æ‰©æ•£Transformerï¼Œæé«˜äº†ç¨³å¥æ€§ã€‚</li>
<li>DiT-Î¼Pè¡¨ç°å‡ºå¼ºå¤§çš„HPè¿ç§»æ€§ï¼Œæ˜¾è‘—åŠ é€Ÿæ¨¡å‹æ”¶æ•›ã€‚</li>
<li>åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­ï¼ŒPixArt-Î±å’ŒMMDiTçš„å®éªŒéªŒè¯äº†Î¼Pçš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15270">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0fabbf870cef01bd3f3789524e898684" align="middle">
<img src="https://picx.zhimg.com/v2-ae327bed95e08e1472c8ade960b258a8" align="middle">
<img src="https://picx.zhimg.com/v2-e3de40487b9e9cd280513563e3132038" align="middle">
<img src="https://picx.zhimg.com/v2-2a4d6a4833d7ac657b5b9ac8d989f6cb" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Efficient-Attention-via-Pre-Scoring-Prioritizing-Informative-Keys-in-Transformers"><a href="#Efficient-Attention-via-Pre-Scoring-Prioritizing-Informative-Keys-in-Transformers" class="headerlink" title="Efficient Attention via Pre-Scoring: Prioritizing Informative Keys in   Transformers"></a>Efficient Attention via Pre-Scoring: Prioritizing Informative Keys in   Transformers</h2><p><strong>Authors:Zhexiang Li, Haoyu Wang, Yutong Bao, David Woodruff</strong></p>
<p>Recent advances in transformer architectures deeply enhanced long-context language modeling. Among them, HyperAttention achieves competitive efficiency by combining a single-level LSH-based clustering with uniform residual sampling. However, HyperAttention fails to find all significant keys, which in turn raises the overall perplexity. We propose a pre-scoring mechanism that prioritizes significant keys before applying HyperAttention. We introduce three scoring methods: $k$-means and kernel $k$-means clustering, $k$-median clustering, and leverage score-based ranking (inspired by LevAttention) to filter keys effectively. We further replace HyperAttentionâ€™s original uniform residual sampling, relying exclusively on our pre-scoring mechanism. Experiments on ChatGLM2 (131k token context) reduce perplexity from 12 to 8.3, which outperforms standard HyperAttention. Moreover, when running on the Vision-Transformer (ViT), our method shows that it can guarantee similar accuracy compared with LevAttention, and will surpass LevAttention given specific parameters. Although this method introduces some computational overhead, its combination with HyperAttention achieves up to 20 times faster than FlashAttention, providing a balanced trade-off between speed and modeling accuracy. Our results highlight the effectiveness of integrating pre-scoring into hierarchical attention mechanisms, significantly improving transformer efficiency. </p>
<blockquote>
<p>è¿‘æœŸtransformeræ¶æ„çš„è¿›å±•æå¤§åœ°æå‡äº†é•¿è¯­å¢ƒè¯­è¨€å»ºæ¨¡çš„èƒ½åŠ›ã€‚å…¶ä¸­ï¼ŒHyperAttentioné€šè¿‡ç»“åˆå•çº§LSHï¼ˆå±€éƒ¨æ•æ„Ÿå“ˆå¸Œï¼‰èšç±»ä¸å‡åŒ€å‰©ä½™é‡‡æ ·å®ç°äº†ç«äº‰æ•ˆç‡ã€‚ç„¶è€Œï¼ŒHyperAttentionæ— æ³•æ‰¾åˆ°æ‰€æœ‰é‡è¦çš„é”®ï¼Œè¿™è¿›è€Œæé«˜äº†æ•´ä½“çš„å›°æƒ‘åº¦ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§é¢„è¯„åˆ†æœºåˆ¶ï¼Œåœ¨åº”ç”¨HyperAttentionä¹‹å‰ä¼˜å…ˆè¯„ä¼°é‡è¦é”®ã€‚æˆ‘ä»¬ä»‹ç»äº†ä¸‰ç§è¯„åˆ†æ–¹æ³•ï¼šKå‡å€¼å’Œæ ¸Kå‡å€¼èšç±»ã€Kä¸­å€¼èšç±»ä»¥åŠå—LevAttentionå¯å‘çš„åŸºäºå¾—åˆ†çš„æ’åï¼Œä»¥æœ‰æ•ˆåœ°è¿‡æ»¤é”®ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æ›¿æ¢äº†HyperAttentionçš„åŸå§‹å‡åŒ€å‰©ä½™é‡‡æ ·ï¼Œå®Œå…¨ä¾èµ–äºæˆ‘ä»¬çš„é¢„è¯„åˆ†æœºåˆ¶ã€‚åœ¨ChatGLM2ï¼ˆ13.1ä¸‡ä»¤ç‰Œä¸Šä¸‹æ–‡ï¼‰ä¸Šçš„å®éªŒå°†å›°æƒ‘åº¦ä»12é™ä½åˆ°8.3ï¼Œä¼˜äºæ ‡å‡†HyperAttentionã€‚æ­¤å¤–ï¼Œåœ¨Vision-Transformerï¼ˆViTï¼‰ä¸Šè¿è¡Œæ—¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½ä¿è¯ä¸LevAttentionç›¸ä¼¼çš„ç²¾åº¦ï¼Œå¹¶åœ¨ç»™å®šç‰¹å®šå‚æ•°æ—¶è¶…è¶ŠLevAttentionã€‚å°½ç®¡è¿™ç§æ–¹æ³•å¼•å…¥äº†ä¸€äº›è®¡ç®—å¼€é”€ï¼Œä½†å®ƒä¸HyperAttentionçš„ç»“åˆæœ€é«˜å¯è¾¾åˆ°FlashAttentionçš„20å€é€Ÿåº¦ï¼Œåœ¨é€Ÿåº¦å’Œå»ºæ¨¡ç²¾åº¦ä¹‹é—´å®ç°äº†å¹³è¡¡çš„æƒè¡¡ã€‚æˆ‘ä»¬çš„ç»“æœçªæ˜¾äº†å°†é¢„è¯„åˆ†èå…¥åˆ†å±‚æ³¨æ„åŠ›æœºåˆ¶çš„æœ‰æ•ˆæ€§ï¼Œå¤§å¤§æé«˜äº†transformerçš„æ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11040v3">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    æœ€æ–°å˜å‹å™¨æ¶æ„çš„è¿›æ­¥æå¤§åœ°æé«˜äº†é•¿æ–‡æœ¬è¯­å¢ƒå»ºæ¨¡çš„èƒ½åŠ›ã€‚HyperAttentioné€šè¿‡ç»“åˆå•çº§LSHèšç±»ä¸å‡åŒ€æ®‹å·®é‡‡æ ·å®ç°äº†é«˜æ•ˆçš„æ€§èƒ½ã€‚ç„¶è€Œï¼ŒHyperAttentionæ— æ³•æ‰¾åˆ°æ‰€æœ‰å…³é”®ä¿¡æ¯ï¼Œå¯¼è‡´æ•´ä½“å›°æƒ‘åº¦ä¸Šå‡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºä¸€ç§é¢„è¯„åˆ†æœºåˆ¶ï¼Œåœ¨åº”ç”¨HyperAttentionå‰ä¼˜å…ˆç­›é€‰å…³é”®ä¿¡æ¯ã€‚æˆ‘ä»¬å¼•å…¥ä¸‰ç§è¯„åˆ†æ–¹æ³•ï¼šKå‡å€¼å’Œæ ¸Kå‡å€¼èšç±»ã€Kä¸­ä½æ•°èšç±»å’ŒåŸºäºæ æ†è¯„åˆ†çš„æ’åï¼ˆå—LevAttentionå¯å‘ï¼‰ï¼Œä»¥æœ‰æ•ˆç­›é€‰å…³é”®ä¿¡æ¯ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬æ›¿æ¢HyperAttentionçš„åŸå§‹å‡åŒ€æ®‹å·®é‡‡æ ·ï¼Œä»…ä¾èµ–é¢„è¯„åˆ†æœºåˆ¶ã€‚åœ¨ChatGLM2ï¼ˆ13.1ä¸‡ä»¤ç‰Œè¯­å¢ƒï¼‰ä¸Šçš„å®éªŒå°†å›°æƒ‘åº¦ä»12é™è‡³8.3ï¼Œä¼˜äºæ ‡å‡†HyperAttentionã€‚æ­¤å¤–ï¼Œåœ¨Vision-Transformerï¼ˆViTï¼‰ä¸Šè¿è¡Œï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½ä¿è¯ä¸LevAttentionç›¸ä¼¼çš„ç²¾åº¦ï¼Œå¹¶åœ¨ç»™å®šç‰¹å®šå‚æ•°æ—¶è¶…è¶ŠLevAttentionã€‚è™½ç„¶æ­¤æ–¹æ³•å¼•å…¥äº†ä¸€å®šçš„è®¡ç®—å¼€é”€ï¼Œä½†ä¸HyperAttentionç»“åˆåï¼Œå…¶é€Ÿåº¦å¯è¾¾åˆ°FlashAttentionçš„20å€ï¼Œå®ç°äº†é€Ÿåº¦ä¸å»ºæ¨¡ç²¾åº¦ä¹‹é—´çš„å¹³è¡¡ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¯æ˜äº†é¢„è¯„åˆ†æœºåˆ¶åœ¨åˆ†å±‚æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„æœ‰æ•ˆæ€§ï¼Œå¯æ˜¾è‘—æé«˜å˜å‹å™¨æ•ˆç‡ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>HyperAttentioné€šè¿‡ç»“åˆLSHèšç±»å’Œå‡åŒ€æ®‹å·®é‡‡æ ·å®ç°äº†é«˜æ•ˆé•¿æ–‡æœ¬è¯­å¢ƒå»ºæ¨¡ã€‚</li>
<li>é¢„è¯„åˆ†æœºåˆ¶ä¼˜å…ˆç­›é€‰å…³é”®ä¿¡æ¯ï¼Œä»¥æé«˜HyperAttentionçš„æ€§èƒ½ã€‚</li>
<li>å¼•å…¥ä¸‰ç§è¯„åˆ†æ–¹æ³•è¿›è¡Œé¢„è¯„åˆ†ï¼šKå‡å€¼å’Œæ ¸Kå‡å€¼èšç±»ã€Kä¸­ä½æ•°èšç±»ä»¥åŠåŸºäºæ æ†è¯„åˆ†çš„æ’åã€‚</li>
<li>åœ¨ChatGLM2ä¸Šåº”ç”¨é¢„è¯„åˆ†æœºåˆ¶ä¸HyperAttentionç»“åˆçš„æ–¹æ³•é™ä½äº†å›°æƒ‘åº¦ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨Vision-Transformerä¸Šè¡¨ç°å‡ºä¸LevAttentionç›¸ä¼¼çš„ç²¾åº¦ï¼Œå¹¶åœ¨ç‰¹å®šå‚æ•°ä¸‹æœ‰æœ›è¶…è¶Šã€‚</li>
<li>è™½ç„¶å­˜åœ¨è®¡ç®—å¼€é”€ï¼Œä½†è¯¥æ–¹æ³•ä¸HyperAttentionç»“åˆåé€Ÿåº¦æ˜¾è‘—å¿«äºFlashAttentionï¼Œå®ç°äº†é€Ÿåº¦ä¸ç²¾åº¦çš„å¹³è¡¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11040">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5072d66581347d9a2d0b48090efbb3f2" align="middle">
<img src="https://picx.zhimg.com/v2-579c0689bf959f968999b882388f522f" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Bias-in-Decision-Making-for-AIâ€™s-Ethical-Dilemmas-A-Comparative-Study-of-ChatGPT-and-Claude"><a href="#Bias-in-Decision-Making-for-AIâ€™s-Ethical-Dilemmas-A-Comparative-Study-of-ChatGPT-and-Claude" class="headerlink" title="Bias in Decision-Making for AIâ€™s Ethical Dilemmas: A Comparative Study   of ChatGPT and Claude"></a>Bias in Decision-Making for AIâ€™s Ethical Dilemmas: A Comparative Study   of ChatGPT and Claude</h2><p><strong>Authors:Wentao Xu, Yile Yan, Yuqi Zhu</strong></p>
<p>Recent advances in Large Language Models (LLMs) have enabled human-like responses across various tasks, raising questions about their ethical decision-making capabilities and potential biases. This study systematically evaluates how nine popular LLMs (both open-source and closed-source) respond to ethical dilemmas involving protected attributes. Across 50,400 trials spanning single and intersectional attribute combinations in four dilemma scenarios (protective vs. harmful), we assess modelsâ€™ ethical preferences, sensitivity, stability, and clustering patterns. Results reveal significant biases in protected attributes in all models, with differing preferences depending on model type and dilemma context. Notably, open-source LLMs show stronger preferences for marginalized groups and greater sensitivity in harmful scenarios, while closed-source models are more selective in protective situations and tend to favor mainstream groups. We also find that ethical behavior varies across dilemma types: LLMs maintain consistent patterns in protective scenarios but respond with more diverse and cognitively demanding decisions in harmful ones. Furthermore, models display more pronounced ethical tendencies under intersectional conditions than in single-attribute settings, suggesting that complex inputs reveal deeper biases. These findings highlight the need for multi-dimensional, context-aware evaluation of LLMsâ€™ ethical behavior and offer a systematic evaluation and approach to understanding and addressing fairness in LLM decision-making. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥èƒ½å¤Ÿåœ¨å„ç§ä»»åŠ¡ä¸­ç”Ÿæˆç±»ä¼¼äººç±»çš„å›åº”ï¼Œè¿™å¼•å‘äº†å…³äºå…¶é“å¾·å†³ç­–èƒ½åŠ›å’Œæ½œåœ¨åè§çš„é—®é¢˜ã€‚æœ¬ç ”ç©¶ç³»ç»Ÿåœ°è¯„ä¼°äº†ä¹ç§æµè¡Œçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆåŒ…æ‹¬å¼€æºå’Œé—­æºï¼‰å¦‚ä½•åº”å¯¹æ¶‰åŠå—ä¿æŠ¤å±æ€§çš„é“å¾·å›°å¢ƒã€‚åœ¨è·¨è¶Šå•ä¸€å±æ€§å’Œäº¤å‰å±æ€§ç»„åˆçš„å››ç§å›°å¢ƒåœºæ™¯ï¼ˆä¿æŠ¤æ€§å¯¹æœ‰å®³æ€§ï¼‰çš„50400æ¬¡è¯•éªŒä¸­ï¼Œæˆ‘ä»¬è¯„ä¼°äº†æ¨¡å‹çš„é“å¾·åå¥½ã€æ•æ„Ÿæ€§ã€ç¨³å®šæ€§å’Œèšç±»æ¨¡å¼ã€‚ç»“æœè¡¨æ˜ï¼Œæ‰€æœ‰æ¨¡å‹åœ¨å—ä¿æŠ¤çš„å±æ€§ä¸Šéƒ½å­˜åœ¨æ˜¾è‘—çš„åè§ï¼Œå…¶åå¥½å› æ¨¡å‹ç±»å‹å’Œå›°å¢ƒèƒŒæ™¯è€Œå¼‚ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå¼€æºå¤§å‹è¯­è¨€æ¨¡å‹å¯¹è¾¹ç¼˜ç¾¤ä½“è¡¨ç°å‡ºæ›´å¼ºçš„åå¥½ï¼Œåœ¨æœ‰å®³åœºæ™¯ä¸­æ›´ä¸ºæ•æ„Ÿï¼Œè€Œé—­æºæ¨¡å‹åœ¨ä¿æŠ¤æƒ…å†µä¸‹æ›´ä¸ºæŒ‘å‰”ï¼Œæ›´å€¾å‘äºä¸»æµç¾¤ä½“ã€‚æˆ‘ä»¬è¿˜å‘ç°ï¼Œä¸åŒç±»å‹çš„é“å¾·å›°å¢ƒä¸­é“å¾·è¡Œä¸ºæœ‰æ‰€ä¸åŒï¼šå¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¿æŠ¤åœºæ™¯ä¸­ä¿æŒä¸€è‡´çš„æ¨¡å¼ï¼Œä½†åœ¨æœ‰å®³åœºæ™¯ä¸­åšå‡ºæ›´å¤šæ ·åŒ–ã€è®¤çŸ¥è¦æ±‚æ›´é«˜çš„å†³ç­–ã€‚æ­¤å¤–ï¼Œä¸å•ä¸€å±æ€§è®¾ç½®ç›¸æ¯”ï¼Œæ¨¡å‹åœ¨äº¤å‰æ¡ä»¶ä¸‹çš„é“å¾·å€¾å‘æ›´ä¸ºæ˜æ˜¾ï¼Œè¿™è¡¨æ˜å¤æ‚çš„è¾“å…¥æ­ç¤ºäº†æ›´æ·±çš„åè§ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†å¤šç»´ã€è¯­å¢ƒæ„ŸçŸ¥çš„è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹é“å¾·è¡Œä¸ºçš„å¿…è¦æ€§ï¼Œå¹¶æä¾›äº†ä¸€ç§ç³»ç»Ÿè¯„ä¼°ä»¥åŠç†è§£å’Œè§£å†³å¤§å‹è¯­è¨€æ¨¡å‹å†³ç­–ä¸­å…¬å¹³é—®é¢˜çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.10484v5">PDF</a> This paper has been accepted by The 20th International AAAI   Conference on Web and Social Media (ICWSM 2026), sunny Los Angeles,   California</p>
<p><strong>Summary</strong>ï¼šè¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›å±•å¼•å‘äº†å…³äºå…¶ä¼¦ç†å†³ç­–èƒ½åŠ›å’Œæ½œåœ¨åè§çš„é—®é¢˜ã€‚æœ¬ç ”ç©¶ç³»ç»Ÿè¯„ä¼°äº†ä¹ç§æµè¡Œçš„LLMåœ¨æ¶‰åŠå—ä¿æŠ¤å±æ€§çš„é“å¾·å›°å¢ƒä¸­çš„å›åº”ã€‚ç ”ç©¶è·¨è¶Šå››ä¸ªé“å¾·å›°å¢ƒåœºæ™¯ï¼ˆä¿æŠ¤å¯¹æœ‰å®³ï¼‰ï¼Œé€šè¿‡å¯¹å•å±æ€§å’Œäº¤å‰å±æ€§ç»„åˆè¿›è¡Œè¶…è¿‡äº”ä¸‡æ¬¡è¯•éªŒï¼Œè¯„ä¼°æ¨¡å‹çš„é“å¾·åå¥½ã€æ•æ„Ÿæ€§ã€ç¨³å®šæ€§å’Œèšç±»æ¨¡å¼ã€‚å‘ç°æ‰€æœ‰æ¨¡å‹åœ¨å—ä¿æŠ¤å±æ€§ä¸Šå‡æœ‰æ˜¾è‘—åè§ï¼Œä¸”åå¥½å–å†³äºæ¨¡å‹ç±»å‹å’Œå›°å¢ƒä¸Šä¸‹æ–‡ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå¼€æºLLMå¯¹è¾¹ç¼˜ç¾¤ä½“è¡¨ç°å‡ºæ›´å¼ºçš„åå¥½å’Œå¯¹æœ‰å®³åœºæ™¯æ›´é«˜çš„æ•æ„Ÿæ€§ï¼Œè€Œå°é—­å¼æ¨¡å‹åˆ™åœ¨ä¿æŠ¤æ€§æƒ…å†µä¸‹æ›´åŠ é€‰æ‹©æ€§ä¸”å€¾å‘äºä¸»æµç¾¤ä½“ã€‚æ­¤å¤–ï¼ŒLLMåœ¨ä¸åŒç±»å‹å›°å¢ƒä¸­çš„é“å¾·è¡Œä¸ºå„ä¸ç›¸åŒï¼šåœ¨ä¿æŠ¤æ€§åœºæ™¯ä¸­ä¿æŒä¸€è‡´æ€§æ¨¡å¼ï¼Œä½†åœ¨æœ‰å®³åœºæ™¯ä¸­åšå‡ºæ›´å¤šæ ·åŒ–ä¸”éœ€è¦è®¤çŸ¥èƒ½åŠ›çš„å†³ç­–ã€‚åŒæ—¶ï¼Œåœ¨äº¤å‰æ¡ä»¶ä¸‹æ¨¡å‹çš„é“å¾·å€¾å‘æ¯”åœ¨å•ä¸€å±æ€§è®¾ç½®ä¸­æ›´ä¸ºçªå‡ºï¼Œè¡¨æ˜å¤æ‚è¾“å…¥æ­ç¤ºäº†æ›´æ·±å±‚æ¬¡çš„åè§ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†å¤šç»´ã€æƒ…å¢ƒæ„ŸçŸ¥è¯„ä¼°LLMä¼¦ç†è¡Œä¸ºçš„éœ€è¦ï¼Œå¹¶ä¸ºè§£å†³LLMå†³ç­–ä¸­çš„å…¬å¹³æ€§é—®é¢˜æä¾›äº†ç³»ç»Ÿè¯„ä¼°æ–¹æ³•å’Œé€”å¾„ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>LLMåœ¨æ¶‰åŠå—ä¿æŠ¤å±æ€§çš„é“å¾·å›°å¢ƒä¸­å±•ç°å‡ºä¼¦ç†å†³ç­–èƒ½åŠ›ï¼Œä½†å­˜åœ¨æ˜¾è‘—åè§ã€‚</li>
<li>ä¸åŒç±»å‹çš„LLMï¼ˆå¼€æºä¸å°é—­å¼ï¼‰åœ¨é“å¾·å›°å¢ƒä¸­çš„è¡¨ç°æœ‰æ‰€ä¸åŒï¼Œå¯¹è¾¹ç¼˜ç¾¤ä½“çš„åå¥½å’Œå¯¹æœ‰å®³åœºæ™¯çš„æ•æ„Ÿæ€§å­˜åœ¨å·®åˆ«ã€‚</li>
<li>LLMåœ¨ä¿æŠ¤æ€§åœºæ™¯å’Œæœ‰å®³åœºæ™¯ä¸­çš„é“å¾·è¡Œä¸ºæ¨¡å¼ä¸åŒï¼Œéœ€è¦å¤šæ ·åŒ–è®¤çŸ¥èƒ½åŠ›åº”å¯¹å¤æ‚å†³ç­–ã€‚</li>
<li>äº¤å‰å±æ€§æ¡ä»¶ä¸‹çš„é“å¾·å›°å¢ƒæ­ç¤ºå‡ºLLMæ›´æ·±å±‚æ¬¡çš„åè§ã€‚</li>
<li>éœ€è¦å¤šç»´ã€æƒ…å¢ƒæ„ŸçŸ¥è¯„ä¼°LLMçš„ä¼¦ç†è¡Œä¸ºã€‚</li>
<li>æœ¬ç ”ç©¶ä¸ºç†è§£å¹¶è§£å†³LLMå†³ç­–å…¬å¹³æ€§æä¾›äº†ç³»ç»Ÿè¯„ä¼°æ–¹æ³•å’Œé€”å¾„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.10484">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ffedf431ce55a52b6e447615a215371b" align="middle">
<img src="https://picx.zhimg.com/v2-e6d695dbd6af0c7258ed9a7f172cc4b0" align="middle">
<img src="https://picx.zhimg.com/v2-ba479860b35149d4ebd3a2c0be13ccb9" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="HALO-Hadamard-Assisted-Lower-Precision-Optimization-for-LLMs"><a href="#HALO-Hadamard-Assisted-Lower-Precision-Optimization-for-LLMs" class="headerlink" title="HALO: Hadamard-Assisted Lower-Precision Optimization for LLMs"></a>HALO: Hadamard-Assisted Lower-Precision Optimization for LLMs</h2><p><strong>Authors:Saleh Ashkboos, Mahdi Nikdan, Soroush Tabesh, Roberto L. Castro, Torsten Hoefler, Dan Alistarh</strong></p>
<p>Quantized training of Large Language Models (LLMs) remains an open challenge, as maintaining accuracy while performing all matrix multiplications in low precision has proven difficult. This is particularly the case when fine-tuning pre-trained models, which can have large weight and activation outlier values that make lower-precision optimization difficult. To address this, we present HALO, a novel quantization-aware training approach for Transformers that enables accurate and efficient low-precision training by combining 1) strategic placement of Hadamard rotations in both forward and backward passes, which mitigate outliers, 2) high-performance kernel support, and 3) FSDP integration for low-precision communication. Our approach ensures that all large matrix multiplications during the forward and backward passes are executed in lower precision. Applied to LLAMA-family models, HALO achieves near-full-precision-equivalent results during fine-tuning on various tasks, while delivering up to 1.41x end-to-end speedup for full fine-tuning on RTX 4090 GPUs. HALO efficiently supports both standard and parameterefficient fine-tuning (PEFT). Our results demonstrate the first practical approach to fully quantized LLM fine-tuning that maintains accuracy in 8-bit precision, while delivering performance benefits. Code is available at <a target="_blank" rel="noopener" href="https://github.com/IST-DASLab/HALO">https://github.com/IST-DASLab/HALO</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é‡åŒ–è®­ç»ƒä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾æ€§çš„æŒ‘æˆ˜ï¼Œå› ä¸ºåœ¨ä½ç²¾åº¦ä¸‹æ‰§è¡Œæ‰€æœ‰çŸ©é˜µä¹˜æ³•çš„åŒæ—¶ä¿æŒå‡†ç¡®æ€§è¢«è¯æ˜æ˜¯å¾ˆå›°éš¾çš„ã€‚è¿™åœ¨å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹æ—¶å°¤å…¶å¦‚æ­¤ï¼Œé¢„è®­ç»ƒæ¨¡å‹å¯èƒ½å­˜åœ¨å¤§é‡çš„æƒé‡å’Œæ¿€æ´»å¼‚å¸¸å€¼ï¼Œä½¿å¾—ä½ç²¾åº¦ä¼˜åŒ–å˜å¾—å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†HALOï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºTransformerçš„æ–°å‹é‡åŒ–æ„ŸçŸ¥è®­ç»ƒæ–¹æ³•ã€‚å®ƒé€šè¿‡ç»“åˆä»¥ä¸‹ä¸‰ç‚¹å®ç°å‡†ç¡®é«˜æ•ˆçš„ä½ç²¾åº¦è®­ç»ƒï¼š1ï¼‰åœ¨æ­£å‘å’Œåå‘ä¼ é€’ä¸­æˆ˜ç•¥æ€§åœ°æ”¾ç½®Hadamardæ—‹è½¬ï¼Œä»¥ç¼“è§£å¼‚å¸¸å€¼ï¼›2ï¼‰é«˜æ€§èƒ½å†…æ ¸æ”¯æŒï¼›3ï¼‰ç”¨äºä½ç²¾åº¦é€šä¿¡çš„FSDPé›†æˆã€‚æˆ‘ä»¬çš„æ–¹æ³•ç¡®ä¿æ­£å‘å’Œåå‘ä¼ é€’ä¸­çš„æ‰€æœ‰å¤§å‹çŸ©é˜µä¹˜æ³•å‡ä»¥è¾ƒä½çš„ç²¾åº¦æ‰§è¡Œã€‚åº”ç”¨äºLLAMAç³»åˆ—æ¨¡å‹æ—¶ï¼ŒHALOåœ¨å¤šç§ä»»åŠ¡ä¸Šè¿›è¡Œå¾®è°ƒæ—¶å®ç°äº†æ¥è¿‘å…¨ç²¾åº¦ç­‰æ•ˆçš„ç»“æœï¼ŒåŒæ—¶åœ¨RTX 4090 GPUä¸Šè¿›è¡Œå®Œæ•´çš„å¾®è°ƒæ—¶å®ç°äº†é«˜è¾¾1.41å€çš„ç«¯åˆ°ç«¯åŠ é€Ÿã€‚HALOæœ‰æ•ˆæ”¯æŒæ ‡å‡†å¾®è°ƒä»¥åŠå‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰ã€‚æˆ‘ä»¬çš„ç»“æœè¯æ˜äº†åœ¨8ä½ç²¾åº¦ä¸‹ä¿æŒå‡†ç¡®åº¦çš„å®Œå…¨é‡åŒ–LLMå¾®è°ƒçš„é¦–ä¸ªå®ç”¨æ–¹æ³•ï¼ŒåŒæ—¶å¸¦æ¥äº†æ€§èƒ½ä¼˜åŠ¿ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/IST-DASLab/HALO%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/IST-DASLab/HALOæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.02625v3">PDF</a> 19 pages, 6 figures</p>
<p><strong>Summary</strong><br>     é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é‡åŒ–è®­ç»ƒä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå¦‚ä½•åœ¨ä¿æŒå‡†ç¡®æ€§çš„åŒæ—¶æ‰§è¡Œæ‰€æœ‰ä½ç²¾åº¦çŸ©é˜µä¹˜æ³•è¿ç®—ååˆ†å›°éš¾ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæå‡ºä¸€ç§é’ˆå¯¹Transformerçš„æ–°å‹é‡åŒ–æ„ŸçŸ¥è®­ç»ƒæ–¹å¼HALOã€‚ç»“åˆæˆ˜ç•¥æ€§æ”¾ç½®Hadamardæ—‹è½¬ï¼ˆå‡è½»å¼‚å¸¸å€¼ï¼‰ã€é«˜æ€§èƒ½å†…æ ¸æ”¯æŒå’ŒFSDPé›†æˆå®ç°ä½ç²¾åº¦é€šä¿¡ï¼Œç¡®ä¿å‰åå‘ä¼ æ’­æœŸé—´çš„å¤§å‹çŸ©é˜µä¹˜æ³•è¿ç®—åœ¨ä½ç²¾åº¦ä¸‹æ‰§è¡Œã€‚åº”ç”¨äºLLAMAç³»åˆ—æ¨¡å‹æ—¶ï¼ŒHALOåœ¨å¤šç§ä»»åŠ¡å¾®è°ƒæ–¹é¢å®ç°äº†è¿‘ä¼¼å…¨ç²¾åº¦ç­‰æ•ˆç»“æœï¼ŒåŒæ—¶åœ¨RTX 4090 GPUä¸Šè¿›è¡Œå®Œæ•´çš„å¾®è°ƒæä¾›äº†é«˜è¾¾1.41å€ç«¯åˆ°ç«¯åŠ é€Ÿã€‚HALOæ”¯æŒæ ‡å‡†å¾®è°ƒï¼ˆStandard Fine-tuningï¼‰å’Œå‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆParameterefficient Fine-tuningï¼‰ã€‚ç ”ç©¶å®ç°äº†åœ¨8ä½ç²¾åº¦ä¸‹ä¿æŒå‡†ç¡®æ€§çš„é¦–ä¸ªå®ç”¨é‡åŒ–LLMå¾®è°ƒæ–¹æ³•ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/IST-DASLab/HALO%E5%A4%84%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/IST-DASLab/HALOå¤„è·å–ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>é‡åŒ–è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¿æŒå‡†ç¡®æ€§æ˜¯ä¸€é¡¹æŒ‘æˆ˜ã€‚</li>
<li>HALOæ˜¯ä¸€ç§æ–°å‹é‡åŒ–æ„ŸçŸ¥è®­ç»ƒæ–¹å¼ï¼Œé’ˆå¯¹Transformeræ¨¡å‹è®¾è®¡ã€‚</li>
<li>HALOç»“åˆäº†æˆ˜ç•¥æ€§æ”¾ç½®Hadamardæ—‹è½¬ã€é«˜æ€§èƒ½å†…æ ¸æ”¯æŒå’ŒFSDPé›†æˆå®ç°ä½ç²¾åº¦è®­ç»ƒã€‚</li>
<li>HALOå¯ä»¥åœ¨ä¸åŒä»»åŠ¡ä¸Šå®ç°è¿‘ä¼¼å…¨ç²¾åº¦ç­‰æ•ˆçš„å¾®è°ƒç»“æœã€‚</li>
<li>HALOæä¾›äº†åœ¨RTX 4090 GPUä¸Šçš„ç«¯åˆ°ç«¯åŠ é€Ÿã€‚</li>
<li>HALOæ”¯æŒæ ‡å‡†å¾®è°ƒï¼ˆStandard Fine-tuningï¼‰å’Œå‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆParameterefficient Fine-tuningï¼‰ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.02625">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1e72cbad79c162c197c38e9788ace5a2" align="middle">
<img src="https://picx.zhimg.com/v2-2f5f19e41ba14006fbcef3bdff280e2a" align="middle">
<img src="https://picx.zhimg.com/v2-85d8b0a4905d749348b48a5039dd37e7" align="middle">
<img src="https://picx.zhimg.com/v2-faca548cbae73767e473a39f86aaf2ab" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="VITA-1-5-Towards-GPT-4o-Level-Real-Time-Vision-and-Speech-Interaction"><a href="#VITA-1-5-Towards-GPT-4o-Level-Real-Time-Vision-and-Speech-Interaction" class="headerlink" title="VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction"></a>VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction</h2><p><strong>Authors:Chaoyou Fu, Haojia Lin, Xiong Wang, Yi-Fan Zhang, Yunhang Shen, Xiaoyu Liu, Haoyu Cao, Zuwei Long, Heting Gao, Ke Li, Long Ma, Xiawu Zheng, Rongrong Ji, Xing Sun, Caifeng Shan, Ran He</strong></p>
<p>Recent Multimodal Large Language Models (MLLMs) have typically focused on integrating visual and textual modalities, with less emphasis placed on the role of speech in enhancing interaction. However, speech plays a crucial role in multimodal dialogue systems, and implementing high-performance in both vision and speech tasks remains a significant challenge due to the fundamental modality differences. In this paper, we propose a carefully designed multi-stage training methodology that progressively trains LLM to understand both visual and speech information, ultimately enabling fluent vision and speech interaction. Our approach not only preserves strong vision-language capacity, but also enables efficient speech-to-speech dialogue capabilities without separate ASR and TTS modules, significantly accelerating multimodal end-to-end response speed. By comparing our method against state-of-the-art counterparts across benchmarks for image, video, and speech tasks, we demonstrate that our model is equipped with both strong visual and speech capabilities, making near real-time vision and speech interaction. Code has been released at <a target="_blank" rel="noopener" href="https://github.com/VITA-MLLM/VITA">https://github.com/VITA-MLLM/VITA</a>. </p>
<blockquote>
<p>æœ€è¿‘çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰é€šå¸¸ä¾§é‡äºæ•´åˆè§†è§‰å’Œæ–‡æœ¬æ¨¡æ€ï¼Œè¾ƒå°‘å¼ºè°ƒè¯­éŸ³åœ¨å¢å¼ºäº¤äº’ä¸­çš„ä½œç”¨ã€‚ç„¶è€Œï¼Œè¯­éŸ³åœ¨å¤šæ¨¡æ€å¯¹è¯ç³»ç»Ÿä¸­æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ï¼Œç”±äºåœ¨æ ¹æœ¬ä¸Šçš„æ¨¡æ€å·®å¼‚ï¼Œå®ç°åœ¨è§†è§‰å’Œè¯­éŸ³ä»»åŠ¡ä¸­éƒ½é«˜æ€§èƒ½ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç²¾å¿ƒè®¾è®¡çš„å¤šé˜¶æ®µè®­ç»ƒæ–¹æ³•è®ºï¼Œé€æ­¥è®­ç»ƒLLMä»¥ç†è§£è§†è§‰å’Œè¯­éŸ³ä¿¡æ¯ï¼Œæœ€ç»ˆä½¿æµç•…çš„è§†è§‰å’Œè¯­éŸ³äº¤äº’æˆä¸ºå¯èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…ä¿ç•™äº†å¼ºå¤§çš„è§†è§‰è¯­è¨€åŠŸèƒ½ï¼Œè€Œä¸”èƒ½å¤Ÿæ”¯æŒé«˜æ•ˆçš„è¯­éŸ³å¯¹è¯åŠŸèƒ½ï¼Œæ— éœ€å•ç‹¬çš„ASRå’ŒTTSæ¨¡å—ï¼Œæå¤§åœ°åŠ å¿«äº†å¤šæ¨¡æ€ç«¯åˆ°ç«¯çš„å“åº”é€Ÿåº¦ã€‚é€šè¿‡ä¸å›¾åƒã€è§†é¢‘å’Œè¯­éŸ³ä»»åŠ¡çš„æœ€æ–°å…ˆè¿›æ¨¡å‹è¿›è¡Œæ¯”è¾ƒï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„æ¨¡å‹æ‹¥æœ‰å¼ºå¤§çš„è§†è§‰å’Œè¯­éŸ³åŠŸèƒ½ï¼Œå¯å®ç°è¿‘ä¹å®æ—¶çš„è§†è§‰å’Œè¯­éŸ³äº¤äº’ã€‚ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/VITA-MLLM/VITA%E4%B8%8A%E3%80%82">https://github.com/VITA-MLLM/VITAä¸Šã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01957v4">PDF</a> NeurIPS 2025 Spotlight, Code 2.4K Stars:   <a target="_blank" rel="noopener" href="https://github.com/VITA-MLLM/VITA">https://github.com/VITA-MLLM/VITA</a></p>
<p><strong>Summary</strong><br>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰é€šå¸¸èšç„¦äºè§†è§‰å’Œæ–‡æœ¬æ¨¡æ€çš„èåˆï¼Œä½†å¯¹è¯­éŸ³åœ¨å¢å¼ºäº¤äº’ä¸­çš„ä½œç”¨é‡è§†ä¸è¶³ã€‚æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§ç²¾å¿ƒè®¾è®¡çš„å¤šé˜¶æ®µè®­ç»ƒæ–¹æ³•ï¼Œä½¿LLMé€æ­¥ç†è§£è§†è§‰å’Œè¯­éŸ³ä¿¡æ¯ï¼Œæœ€ç»ˆå®ç°æµç•…çš„è§†è§‰å’Œè¯­éŸ³äº¤äº’ã€‚è¯¥æ–¹æ³•ä¸ä»…ä¿ç•™äº†å¼ºå¤§çš„è§†è§‰è¯­è¨€èƒ½åŠ›ï¼Œè¿˜èƒ½å®ç°é«˜æ•ˆçš„è¯­éŸ³å¯¹è¯èƒ½åŠ›ï¼Œæ— éœ€é¢å¤–çš„è¯­éŸ³è¯†åˆ«å’Œæ–‡æœ¬è½¬è¯­éŸ³æ¨¡å—ï¼Œæ˜¾è‘—åŠ å¿«äº†å¤šæ¨¡æ€ç«¯åˆ°ç«¯çš„å“åº”é€Ÿåº¦ã€‚å¯¹æ¯”å‰æ²¿æ¨¡å‹åœ¨å›¾åƒã€è§†é¢‘å’Œè¯­éŸ³ä»»åŠ¡ä¸Šçš„åŸºå‡†æµ‹è¯•è¡¨æ˜ï¼Œè¯¥æ¨¡å‹å…¼å…·å¼ºå¤§çš„è§†è§‰å’Œè¯­éŸ³èƒ½åŠ›ï¼Œå¯å®ç°è¿‘å®æ—¶çš„è§†è§‰å’Œè¯­éŸ³äº¤äº’ã€‚ä»£ç å·²å‘å¸ƒåœ¨VITAé¡¹ç›®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLMséœ€è¦æ•´åˆè§†è§‰å’Œè¯­éŸ³æ¨¡æ€ä»¥å¢å¼ºäº¤äº’èƒ½åŠ›ã€‚</li>
<li>æå‡ºäº†ä¸€ç§å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥æ¥è®­ç»ƒLLMç†è§£è§†è§‰å’Œè¯­éŸ³ä¿¡æ¯ã€‚</li>
<li>æ¨¡å‹èƒ½å¤Ÿåœ¨ä¿æŒå¼ºå¤§çš„è§†è§‰è¯­è¨€èƒ½åŠ›çš„åŒæ—¶ï¼Œå®ç°é«˜æ•ˆçš„è¯­éŸ³å¯¹è¯åŠŸèƒ½ã€‚</li>
<li>æ¨¡å‹æ— éœ€é¢å¤–çš„ASRå’ŒTTSæ¨¡å—ï¼Œæå‡äº†å“åº”é€Ÿåº¦å’Œå®ç”¨æ€§ã€‚</li>
<li>ä¸æœ€æ–°æ¨¡å‹å¯¹æ¯”å®éªŒæ˜¾ç¤ºå…¶åœ¨å›¾åƒã€è§†é¢‘å’Œè¯­éŸ³ä»»åŠ¡ä¸Šçš„ä¼˜åŠ¿ã€‚</li>
<li>æ¨¡å‹å®ç°äº†è¿‘å®æ—¶çš„è§†è§‰å’Œè¯­éŸ³äº¤äº’èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01957">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a862c6482be13d4067b24007a2c6afae" align="middle">
<img src="https://picx.zhimg.com/v2-dcced7eae39d85540050428837191e16" align="middle">
<img src="https://picx.zhimg.com/v2-af64631cf671fc440e48a2e406545035" align="middle">
<img src="https://picx.zhimg.com/v2-ed82f2ce1ffcecc27be32cc14be801b0" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="RAG-IT-Retrieval-Augmented-Instruction-Tuning-for-Automated-Financial-Analysis"><a href="#RAG-IT-Retrieval-Augmented-Instruction-Tuning-for-Automated-Financial-Analysis" class="headerlink" title="RAG-IT: Retrieval-Augmented Instruction Tuning for Automated Financial   Analysis"></a>RAG-IT: Retrieval-Augmented Instruction Tuning for Automated Financial   Analysis</h2><p><strong>Authors:Van-Duc Le, Hai-Thien To</strong></p>
<p>Financial analysis relies heavily on the interpretation of earnings reports to assess company performance and guide decision-making. Traditional methods for generating such analyses demand significant financial expertise and are often time-consuming. With the rapid advancement of Large Language Models (LLMs), domain-specific adaptations have emerged for financial tasks such as sentiment analysis and entity recognition. This paper introduces RAG-IT (Retrieval-Augmented Instruction Tuning), a novel framework designed to automate the generation of earnings report analyses through an LLM fine-tuned specifically for the financial domain. Our approach integrates retrieval augmentation with instruction-based fine-tuning to enhance factual accuracy, contextual relevance, and domain adaptability. We construct a comprehensive financial instruction dataset derived from extensive financial documents and earnings reports to guide the LLMâ€™s adaptation to specialized financial reasoning. Experimental results demonstrate that RAG-IT outperforms general-purpose open-source models and achieves performance comparable to commercial systems like GPT-3.5 on financial report generation tasks. This research highlights the potential of retrieval-augmented instruction tuning to streamline and elevate financial analysis automation, advancing the broader field of intelligent financial reporting. </p>
<blockquote>
<p>è´¢åŠ¡åˆ†æä¸»è¦ä¾èµ–äºå¯¹æ”¶ç›ŠæŠ¥å‘Šçš„è§£é‡Šæ¥è¯„ä¼°å…¬å¸è¡¨ç°å¹¶æŒ‡å¯¼å†³ç­–ã€‚ä¼ ç»Ÿçš„ç”Ÿæˆæ­¤ç±»åˆ†æçš„æ–¹æ³•éœ€è¦å¤§é‡çš„è´¢åŠ¡ä¸“ä¸šçŸ¥è¯†ï¼Œå¹¶ä¸”å¾€å¾€è€—æ—¶ã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œé’ˆå¯¹è´¢åŠ¡ä»»åŠ¡ï¼ˆå¦‚æƒ…æ„Ÿåˆ†æå’Œå®ä½“è¯†åˆ«ï¼‰çš„ç‰¹å®šé¢†åŸŸé€‚é…å·²ç»å‡ºç°ã€‚æœ¬æ–‡ä»‹ç»äº†RAG-ITï¼ˆæ£€ç´¢å¢å¼ºæŒ‡ä»¤è°ƒæ•´ï¼‰è¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œå®ƒæ—¨åœ¨é€šè¿‡é’ˆå¯¹è´¢åŠ¡é¢†åŸŸè¿›è¡Œç²¾ç»†è°ƒæ•´çš„LLMè‡ªåŠ¨ç”Ÿæˆæ”¶ç›ŠæŠ¥å‘Šåˆ†æã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†æ£€ç´¢å¢å¼ºä¸åŸºäºæŒ‡ä»¤çš„å¾®è°ƒç›¸ç»“åˆï¼Œä»¥æé«˜äº‹å®å‡†ç¡®æ€§ã€ä¸Šä¸‹æ–‡ç›¸å…³æ€§å’Œé¢†åŸŸé€‚åº”æ€§ã€‚æˆ‘ä»¬æ„å»ºäº†åŸºäºå¹¿æ³›è´¢åŠ¡æ–‡ä»¶å’Œæ”¶ç›ŠæŠ¥å‘Šçš„ç»¼åˆè´¢åŠ¡æŒ‡ä»¤æ•°æ®é›†ï¼Œä»¥å¼•å¯¼LLMé€‚åº”ä¸“ä¸šè´¢åŠ¡æ¨ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRAG-ITåœ¨è´¢åŠ¡æŠ¥å‘Šç”Ÿæˆä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºé€šç”¨å¼€æºæ¨¡å‹ï¼Œå¹¶ä¸”ä¸GPT-3.5ç­‰å•†ä¸šç³»ç»Ÿçš„è¡¨ç°ç›¸å½“ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†æ£€ç´¢å¢å¼ºæŒ‡ä»¤è°ƒæ•´åœ¨ç®€åŒ–å’Œæå‡è´¢åŠ¡åˆ†æè‡ªåŠ¨åŒ–æ–¹é¢çš„æ½œåŠ›ï¼Œæ¨åŠ¨äº†æ™ºèƒ½è´¢åŠ¡æŠ¥å‘Šé¢†åŸŸçš„æ›´å¹¿æ³›å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08179v2">PDF</a> 11 pages, 1 figure, 4 tables</p>
<p><strong>æ€»ç»“</strong></p>
<p>é‡‘èåˆ†æä¾èµ–äºæ”¶ç›ŠæŠ¥å‘Šçš„è§£é‡Šæ¥è¯„ä¼°å…¬å¸ç»©æ•ˆå¹¶æŒ‡å¯¼å†³ç­–ã€‚ä¼ ç»Ÿç”Ÿæˆæ­¤ç±»åˆ†æçš„æ–¹æ³•éœ€è¦ä¸°å¯Œçš„é‡‘èçŸ¥è¯†å’Œå¤§é‡æ—¶é—´ã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œé’ˆå¯¹é‡‘èä»»åŠ¡çš„ç‰¹å®šé¢†åŸŸé€‚é…å·²ç»å‡ºç°ï¼Œå¦‚æƒ…æ„Ÿåˆ†æå’Œå®ä½“è¯†åˆ«ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºRAG-ITçš„æ–°å‹æ¡†æ¶ï¼Œå®ƒé€šè¿‡ä¸“é—¨é’ˆå¯¹é‡‘èé¢†åŸŸè¿›è¡Œå¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹è‡ªåŠ¨åŒ–ç”Ÿæˆæ”¶ç›ŠæŠ¥å‘Šåˆ†æã€‚RAG-ITæ–¹æ³•ç»“åˆäº†æ£€ç´¢å¢å¼ºä¸åŸºäºæŒ‡ä»¤çš„å¾®è°ƒï¼Œä»¥æé«˜äº‹å®å‡†ç¡®æ€§ã€ä¸Šä¸‹æ–‡ç›¸å…³æ€§å’Œé¢†åŸŸé€‚åº”æ€§ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå…¨é¢çš„è´¢åŠ¡æŒ‡ä»¤æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†æ¥æºäºå¹¿æ³›çš„è´¢åŠ¡æ–‡ä»¶å’Œæ”¶ç›ŠæŠ¥å‘Šï¼Œä»¥æŒ‡å¯¼å¤§å‹è¯­è¨€æ¨¡å‹é€‚åº”ä¸“ä¸šè´¢åŠ¡æ¨ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRAG-ITåœ¨è´¢åŠ¡æŠ¥å‘Šç”Ÿæˆä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¼˜äºé€šç”¨å¼€æºæ¨¡å‹ï¼Œå¹¶ä¸GPT-3.5ç­‰å•†ä¸šç³»ç»Ÿç›¸å½“ã€‚æœ¬ç ”ç©¶çªæ˜¾äº†æ£€ç´¢å¢å¼ºæŒ‡ä»¤è°ƒæ•´åœ¨ç®€åŒ–å¹¶æå‡è´¢åŠ¡åˆ†æè‡ªåŠ¨åŒ–æ–¹é¢çš„æ½œåŠ›ï¼Œæ¨åŠ¨äº†æ™ºèƒ½è´¢åŠ¡æŠ¥å‘Šé¢†åŸŸçš„è¿›æ­¥ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>é‡‘èåˆ†æçš„æ ¸å¿ƒæ˜¯è§£è¯»æ”¶ç›ŠæŠ¥å‘Šæ¥è¯„ä¼°å…¬å¸è¡¨ç°å’ŒæŒ‡å¯¼å†³ç­–ã€‚</li>
<li>ä¼ ç»Ÿé‡‘èåˆ†ææ–¹æ³•éœ€è¦æ·±åšçš„é‡‘èçŸ¥è¯†å’Œå¤§é‡æ—¶é—´ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªåŠ¨åŒ–é‡‘èåˆ†ææ–¹é¢å±•ç°å‡ºæ½œåŠ›ã€‚</li>
<li>RAG-ITæ˜¯ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œé€šè¿‡ä¸“é—¨é’ˆå¯¹é‡‘èé¢†åŸŸçš„è¯­è¨€æ¨¡å‹è‡ªåŠ¨åŒ–ç”Ÿæˆæ”¶ç›ŠæŠ¥å‘Šåˆ†æã€‚</li>
<li>RAG-ITç»“åˆäº†æ£€ç´¢å¢å¼ºä¸åŸºäºæŒ‡ä»¤çš„å¾®è°ƒæŠ€æœ¯ä»¥æé«˜æ€§èƒ½ã€‚</li>
<li>RAG-ITæ¡†æ¶çš„æ€§èƒ½å·²ç»å¾—åˆ°å®éªŒéªŒè¯ï¼Œæ˜¾ç¤ºå‡ºä¼˜äºæŸäº›é€šç”¨æ¨¡å‹çš„æ€§èƒ½å¹¶å¯ä¸å•†ä¸šç³»ç»Ÿç›¸ç«äº‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.08179">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a8083c84a4994910384e02b3948f7c66" align="middle">
<img src="https://picx.zhimg.com/v2-5a2daf6091db4f5fb78c6f5cf64880d7" align="middle">
<img src="https://picx.zhimg.com/v2-3e10c33c7baf8ae6eb1cee2a5d963b51" align="middle">
<img src="https://picx.zhimg.com/v2-54a084c1e3df02d648bdc02b52640336" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-07/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-07/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-07/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-c0e0da729ae65cd12dcf37a2918d3aa3" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-07  Manifold-constrained Hamilton-Jacobi Reachability Learning for   Decentralized Multi-Agent Motion Planning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-07
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-07/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-8e72cc24c8e286580d3bbabb45e922b3" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-07  Part-Aware Bottom-Up Group Reasoning for Fine-Grained Social Interaction   Detection
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-07
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33297.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
