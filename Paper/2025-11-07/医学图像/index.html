<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-07  Model order reduction via Lie groups">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-4f8e3acc26339a60e4e8f73e4882ab0e')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-07
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-19
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    80 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-07-æ›´æ–°"><a href="#2025-11-07-æ›´æ–°" class="headerlink" title="2025-11-07 æ›´æ–°"></a>2025-11-07 æ›´æ–°</h1><h2 id="Model-order-reduction-via-Lie-groups"><a href="#Model-order-reduction-via-Lie-groups" class="headerlink" title="Model order reduction via Lie groups"></a>Model order reduction via Lie groups</h2><p><strong>Authors:Yannik P. Wotte, Patrick Buchfink, Silke Glas, Federico Califano, Stefano Stramigioli</strong></p>
<p>Lie groups and their actions are ubiquitous in the description of physical systems, and we explore implications in the setting of model order reduction (MOR). We present a novel framework of MOR via Lie groups, called MORLie, in which high-dimensional dynamical systems on manifolds are approximated by low-dimensional dynamical systems on Lie groups. In comparison to other Lie group methods we are able to attack non-equivariant dynamics, which are frequent in practical applications, and we provide new non-intrusive MOR methods based on the presented geometric formulation. We also highlight numerically that MORLie has a lower error bound than the Kolmogorov $N$-width, which limits linear-subspace methods. The method is applied to various examples: 1. MOR of a simplified deforming body modeled by a noisy point cloud data following a sheering motion, where MORLie outperforms a naive POD approach in terms of accuracy and dimensionality reduction. 2. Reconstructing liver motion during respiration with data from edge detection in ultrasound scans, where MORLie reaches performance approaching the state of the art, while reducing the training time from hours on a computing cluster to minutes on a mobile workstation. 3. An analytic example showing that the method of freezing is analytically recovered as a special case, showing the generality of the geometric framework. </p>
<blockquote>
<p>æç¾¤åŠå…¶è¡ŒåŠ¨åœ¨æè¿°ç‰©ç†ç³»ç»Ÿæ—¶æ— å¤„ä¸åœ¨ï¼Œæˆ‘ä»¬æ¢ç´¢äº†å…¶åœ¨æ¨¡å‹é™é˜¶ï¼ˆMORï¼‰è®¾ç½®ä¸­çš„å½±å“ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºæç¾¤çš„MORæ¡†æ¶ï¼Œç§°ä¸ºMORLieï¼Œå…¶ä¸­é«˜ç»´æµå½¢ä¸Šçš„åŠ¨åŠ›ç³»ç»Ÿè¢«è¿‘ä¼¼ä¸ºä½ç»´æç¾¤ä¸Šçš„åŠ¨åŠ›ç³»ç»Ÿã€‚ä¸å…¶ä»–æç¾¤æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬èƒ½å¤Ÿè§£å†³å®é™…åº”ç”¨ä¸­å¸¸è§çš„éç­‰ä»·åŠ¨åŠ›å­¦é—®é¢˜ï¼Œå¹¶åŸºäºæ‰€ç»™å‡ºçš„å‡ ä½•å…¬å¼æä¾›äº†æ–°çš„éä¾µå…¥å¼MORæ–¹æ³•ã€‚æˆ‘ä»¬è¿˜ä»æ•°å€¼ä¸Šå¼ºè°ƒï¼ŒMORLieçš„è¯¯å·®ç•Œé™ä½äºé™åˆ¶å­ç©ºé—´æ–¹æ³•çš„Kolmogorov Nå®½åº¦ã€‚è¯¥æ–¹æ³•åº”ç”¨äºå„ç§ç¤ºä¾‹ï¼š1. MORçš„ä¸€ä¸ªç®€åŒ–å˜å½¢ä½“ï¼Œç”±ä¸€ä¸ªéµå¾ªå‰ªåˆ‡è¿åŠ¨çš„å¸¦å™ªå£°ç‚¹äº‘æ•°æ®æ¨¡æ‹Ÿï¼ŒMORLieåœ¨ç²¾åº¦å’Œé™ç»´æ–¹é¢ä¼˜äºç®€å•çš„PODæ–¹æ³•ã€‚2.ä½¿ç”¨è¶…å£°æ‰«æè¾¹ç¼˜æ£€æµ‹çš„æ•°æ®é‡å»ºå‘¼å¸æ—¶çš„è‚è„è¿åŠ¨ï¼ŒMORLieè¾¾åˆ°äº†æ¥è¿‘æœ€æ–°æŠ€æœ¯æ°´å¹³çš„æ€§èƒ½ï¼ŒåŒæ—¶å°†è®­ç»ƒæ—¶é—´ä»è®¡ç®—æœºé›†ç¾¤ä¸Šçš„æ•°å°æ—¶ç¼©çŸ­åˆ°ç§»åŠ¨å·¥ä½œç«™ä¸Šçš„æ•°åˆ†é’Ÿã€‚3.ä¸€ä¸ªåˆ†æç¤ºä¾‹ï¼Œæ˜¾ç¤ºå†»ç»“æ–¹æ³•ä½œä¸ºç‰¹æ®Šæƒ…å†µè¢«åˆ†ææ¢å¤ï¼Œæ˜¾ç¤ºäº†å‡ ä½•æ¡†æ¶çš„æ™®éæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.03520v1">PDF</a> 22 pages, 21 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åŸºäºæç¾¤ï¼ˆLie groupsï¼‰æ¨¡å‹çš„é™é˜¶æ¨¡å‹åŒ–ï¼ˆMORï¼‰æ¡†æ¶ï¼Œç§°ä¸ºMORLieã€‚è¯¥æ¡†æ¶å¯è¿‘ä¼¼é«˜ç»´æµå½¢ä¸Šçš„åŠ¨åŠ›ç³»ç»Ÿä¸ºä½ç»´æç¾¤ä¸Šçš„åŠ¨åŠ›ç³»ç»Ÿã€‚ç›¸è¾ƒäºå…¶ä»–æç¾¤æ–¹æ³•ï¼ŒMORLieå¯å¤„ç†å®é™…åº”ç”¨ä¸­å¸¸è§çš„éç­‰ä»·åŠ¨åŠ›å­¦ã€‚æœ¬æ–‡è¿˜æä¾›åŸºäºå‡ ä½•å…¬å¼çš„éä¾µå…¥å¼MORæ–¹æ³•ï¼Œå¹¶æŒ‡å‡ºMORLieå…·æœ‰ä½äºKolmogorov Nå®½åº¦çš„è¯¯å·®ç•Œã€‚è¯¥æ–¹æ³•åº”ç”¨äºå¤šä¸ªå®ä¾‹ï¼ŒåŒ…æ‹¬ç®€åŒ–å˜å½¢ä½“çš„é™é˜¶æ¨¡å‹åŒ–ã€è‚è„è¿åŠ¨é‡å»ºå’Œç†è®ºåˆ†æï¼Œæ˜¾ç¤ºå‡ºå…¶å‡†ç¡®æ€§ä¸æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MORLieæ¡†æ¶å¼•å…¥æç¾¤ç”¨äºæ¨¡å‹é™é˜¶ï¼ˆMORï¼‰ï¼Œç”¨ä»¥è¿‘ä¼¼é«˜ç»´æµå½¢ä¸Šçš„åŠ¨åŠ›ç³»ç»Ÿã€‚</li>
<li>ä¸å…¶ä»–æç¾¤æ–¹æ³•ç›¸æ¯”ï¼ŒMORLieèƒ½å¤Ÿå¤„ç†éç­‰ä»·åŠ¨åŠ›å­¦ï¼Œè¿™åœ¨å®è·µä¸­å¾ˆå¸¸è§ã€‚</li>
<li>åŸºäºå‡ ä½•å…¬å¼çš„éä¾µå…¥å¼MORæ–¹æ³•è¢«æå‡ºã€‚</li>
<li>MORLieå…·æœ‰ä½äºKolmogorov Nå®½åº¦çš„è¯¯å·®ç•Œã€‚</li>
<li>åœ¨é™é˜¶æ¨¡å‹åŒ–ç®€åŒ–å˜å½¢ä½“çš„ä¾‹å­ä¸­ï¼ŒMORLieç›¸è¾ƒäºPODæ–¹æ³•è¡¨ç°å‡ºæ›´é«˜çš„å‡†ç¡®æ€§å’Œé™ç»´æ•ˆæœã€‚</li>
<li>åœ¨é‡å»ºè‚è„è¿åŠ¨çš„æ•°æ®ä¸­ï¼ŒMORLieè¾¾åˆ°äº†æ¥è¿‘æœ€ä½³æ€§èƒ½ï¼Œå¹¶å°†è®­ç»ƒæ—¶é—´ä»é›†ç¾¤è®¡ç®—çš„å°æ—¶çº§ç¼©çŸ­åˆ°å·¥ä½œç«™ä¸Šçš„å‡ åˆ†é’Ÿçº§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.03520">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a74508529463c30388908460c157e85d" align="middle">
<img src="https://picx.zhimg.com/v2-83fb72f8a41f01321f41e6c725c6b593" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Measuring-accretion-disc-properties-in-the-transitional-millisecond-pulsar-PSR-J1023-0038-using-XMM-Newton-NuSTAR-NICER-and-Chandra"><a href="#Measuring-accretion-disc-properties-in-the-transitional-millisecond-pulsar-PSR-J1023-0038-using-XMM-Newton-NuSTAR-NICER-and-Chandra" class="headerlink" title="Measuring accretion disc properties in the transitional millisecond   pulsar PSR J1023+0038 using XMM-Newton, NuSTAR, NICER and Chandra"></a>Measuring accretion disc properties in the transitional millisecond   pulsar PSR J1023+0038 using XMM-Newton, NuSTAR, NICER and Chandra</h2><p><strong>Authors:Vishal Jadoliya, Mayukh Pahari, Sudip Bhattacharyya, Shaswat Suresh Nair</strong></p>
<p>Whether the accretion disc in the X-ray high-mode of transitional millisecond pulsars (tMSP) reaches near the neutron star surface by penetrating the magnetosphere is a crucial question with many implications, including for continuous gravitational wave emission from the pulsar. We attempt to answer this question for the tMSP PSR J1023+0038 by segregating high-mode data and performing detailed spectral analysis using the XMM-Newton EPIC-PN+MOS1+MOS2 joint observations, XMM-Newton+NuSTAR joint observations, NICER and Chandra individual observations during different epochs. With the sum of longest exposures ($\sim$202 ksec of high mode data from $\sim$364 ksec of total exposure), we performed a self-consistent spectral analysis and constrain the inner disc radius 16.8 $\pm$ 3.8 km with at least 3$\sigma$ significance. Such a measurement is found consistent with best-fit spectral values of inner disc radius from other observatory like NICER and a joint observations with XMM-Newton and NuSTAR within 3$\sigma$ limits. We also detect a Fe emission line at 6.45 keV, for the first time from a tMSP, in the Chandra spectrum with 99% significance with an upper limit of the inner disc radius of 21 R$_g$, supporting independently the fact that inner disc extends into neutron starsâ€™s magnetosphere during high mode. All results from our analysis imply that the accretion disc is significantly present and extended within the corotation radius of the neutron star in PSR J1023+0038 during the X-ray high-mode of the tMSP PSR J1023+0038. The measured range of inner disc radius is fully consistent with an independent analysis by Bhattacharyya (2020), which suggests continuous gravitational wave emission from this neutron star, and the standard model of X-ray pulsations in accreting MSPs. </p>
<blockquote>
<p>å…³äºè¿‡æ¸¡å‹æ¯«ç§’è„‰å†²æ˜Ÿï¼ˆtMSPï¼‰Xå°„çº¿é«˜æ¨¡å¼ä¸‹çš„å¸ç§¯ç›˜æ˜¯å¦é€šè¿‡ç©¿é€ç£å±‚è€Œæ¥è¿‘ä¸­å­æ˜Ÿè¡¨é¢è¿™ä¸€é—®é¢˜å…·æœ‰è®¸å¤šé‡è¦æ„ä¹‰ï¼ŒåŒ…æ‹¬å¯¹è„‰å†²æ˜Ÿè¿ç»­å¼•åŠ›æ³¢å‘å°„çš„å½±å“ã€‚æˆ‘ä»¬å°è¯•é€šè¿‡å¯¹tMSP PSR J1023+0038çš„é«˜æ¨¡å¼æ•°æ®è¿›è¡Œåˆ†å‰²ï¼Œå¹¶åˆ©ç”¨XMM-Newton EPIC-PN+MOS1+MOS2è”åˆè§‚æµ‹ã€XMM-Newton+NuSTARè”åˆè§‚æµ‹ä»¥åŠåœ¨ä¸åŒæ—¶æœŸNICERå’ŒChandraçš„å•ç‹¬è§‚æµ‹ç»“æœï¼Œè¿›è¡Œè¯¦ç»†çš„å…‰è°±åˆ†ææ¥å›ç­”è¿™ä¸ªé—®é¢˜ã€‚é€šè¿‡å¯¹æœ€é•¿æ›å…‰æ—¶é—´çš„ç´¯åŠ ï¼ˆçº¦202 ksecçš„é«˜æ¨¡å¼æ•°æ®ï¼Œæ€»è®¡çº¦364 ksecçš„æ›å…‰æ—¶é—´ï¼‰ï¼Œæˆ‘ä»¬è¿›è¡Œäº†è‡ªæ´½çš„å…‰è°±åˆ†æï¼Œä»¥è‡³å°‘3Ïƒçš„æ˜¾è‘—æ€§å°†å†…ç›˜åŠå¾„çº¦æŸä¸º16.8Â±3.8å…¬é‡Œã€‚è¿™æ ·çš„æµ‹é‡å€¼ä¸NICERç­‰å…¶ä»–è§‚æµ‹ç«™çš„æœ€ä½³æ‹Ÿåˆå…‰è°±å†…ç›˜åŠå¾„å€¼ç›¸ç¬¦ï¼Œå¹¶ä¸XMM-Newtonå’ŒNuSTARè”åˆè§‚æµ‹çš„3Ïƒé™åˆ¶å†…çš„å€¼ä¸€è‡´ã€‚æˆ‘ä»¬è¿˜é¦–æ¬¡åœ¨æ¥è‡ªtMSPçš„Chandraå…‰è°±ä¸­æ£€æµ‹åˆ°Feå‘å°„çº¿ï¼ˆä»¥99%çš„æ˜¾è‘—æ€§ï¼‰ï¼Œå†…ç›˜åŠå¾„çš„ä¸Šé™ä¸º21 Rgï¼Œç‹¬ç«‹æ”¯æŒåœ¨é«˜æ¨¡å¼ä¸‹å†…ç›˜å»¶ä¼¸è‡³ä¸­å­æ˜Ÿç£å±‚çš„äº‹å®ã€‚æˆ‘ä»¬åˆ†æçš„æ‰€æœ‰ç»“æœéƒ½æš—ç¤ºï¼Œåœ¨PSR J1023+0038çš„Xå°„çº¿é«˜æ¨¡å¼ä¸‹ï¼Œå…¶å¸ç§¯ç›˜æ˜æ˜¾å­˜åœ¨äºä¸­å­æ˜Ÿçš„ååŒè½¬åŠ¨åŠå¾„èŒƒå›´å†…ã€‚æ‰€æµ‹å¾—çš„å†…ç›˜åŠå¾„èŒƒå›´ä¸Bhattacharyyaï¼ˆ2020ï¼‰çš„ç‹¬ç«‹åˆ†æå®Œå…¨ä¸€è‡´ï¼Œè¡¨æ˜è¯¥ä¸­å­æ˜Ÿå­˜åœ¨è¿ç»­çš„å¼•åŠ›æ³¢å‘å°„ï¼Œå¹¶ä¸”ç¬¦åˆå¸ç§¯MSPsçš„Xå°„çº¿è„‰åŠ¨æ ‡å‡†æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.03457v1">PDF</a> 18 pages, 3 tables, 12 figures, Accepted for publication in the   Journal of High Energy Astrophysics</p>
<p><strong>Summary</strong><br>     è¿‡æ¸¡å‹æ¯«ç§’è„‰å†²æ˜ŸPSR J1023+0038åœ¨Xå°„çº¿é«˜æ¨¡å¼ä¸‹çš„å¸ç§¯ç›˜æ˜¯å¦ç©¿é€ç£å±‚æ¥è¿‘ä¸­å­æ˜Ÿè¡¨é¢æ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ï¼Œå¯¹æ­¤é—®é¢˜çš„ç ”ç©¶å¯¹äºäº†è§£æ¥è‡ªè¯¥è„‰å†²æ˜Ÿçš„è¿ç»­å¼•åŠ›æ³¢å‘å°„å…·æœ‰é‡è¦æ„ä¹‰ã€‚é€šè¿‡å¯¹PSR J1023+0038çš„é«˜æ¨¡å¼æ•°æ®è¿›è¡Œåˆ†ç¦»å’Œè¯¦ç»†çš„è°±åˆ†æï¼Œæˆ‘ä»¬çº¦æŸäº†å…¶å†…ç›˜åŠå¾„ä¸º16.8Â±3.8å…¬é‡Œï¼Œä¸NICERç­‰å…¶ä»–è§‚æµ‹ç»“æœä¸€è‡´ã€‚åœ¨Chandraå…‰è°±ä¸­é¦–æ¬¡æ£€æµ‹åˆ°6.45åƒç”µå­ä¼çš„Feå‘å°„çº¿ï¼Œè¿›ä¸€æ­¥æ”¯æŒäº†åœ¨é«˜æ¨¡å¼ä¸‹å†…ç›˜æ‰©å±•åˆ°ä¸­å­æ˜Ÿç£å±‚çš„è§‚ç‚¹ã€‚æ‰€æœ‰ç»“æœå‡è¡¨æ˜ï¼Œåœ¨tMSP PSR J1023+0038çš„Xå°„çº¿é«˜æ¨¡å¼ä¸‹ï¼Œå…¶å¸ç§¯ç›˜æ˜¾è‘—å­˜åœ¨äºä¸­å­æ˜Ÿè‡ªè½¬åŠå¾„å†…ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶å›ç­”äº†å…³äºè¿‡æ¸¡å‹æ¯«ç§’è„‰å†²æ˜ŸPSR J1023+0038åœ¨Xå°„çº¿é«˜æ¨¡å¼ä¸‹çš„å¸ç§¯ç›˜æ˜¯å¦æ¥è¿‘ä¸­å­æ˜Ÿè¡¨é¢çš„é—®é¢˜ã€‚</li>
<li>é€šè¿‡è°±åˆ†æï¼Œçº¦æŸäº†å†…ç›˜åŠå¾„ä¸ºçº¦16.8å…¬é‡Œï¼Œè¿™ä¸€ç»“æœä¸NICERå’Œå…¶ä»–è§‚æµ‹ç»“æœç›¸ç¬¦ã€‚</li>
<li>åœ¨Chandraå…‰è°±ä¸­é¦–æ¬¡æ£€æµ‹åˆ°æ¥è‡ªtMSPçš„Feå‘å°„çº¿ï¼Œè¿›ä¸€æ­¥æ”¯æŒå†…ç›˜åœ¨Xå°„çº¿é«˜æ¨¡å¼ä¸‹æ‰©å±•è‡³ä¸­å­æ˜Ÿç£å±‚çš„è¯´æ³•ã€‚</li>
<li>æ‰€æœ‰ç»“æœè¡¨æ˜ï¼Œåœ¨Xå°„çº¿é«˜æ¨¡å¼ä¸‹ï¼Œå¸ç§¯ç›˜æ˜¾è‘—å­˜åœ¨äºä¸­å­æ˜Ÿçš„è‡ªè½¬åŠå¾„å†…ã€‚è¿™å¯¹ç†è§£è¿ç»­å¼•åŠ›æ³¢å‘å°„å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
<li>ç»“æœä¸Bhattacharyyaï¼ˆ2020ï¼‰çš„ç‹¬ç«‹åˆ†æä¸€è‡´ï¼Œè¯¥åˆ†ææš—ç¤ºè¯¥ä¸­å­æ˜Ÿå¯èƒ½äº§ç”Ÿè¿ç»­å¼•åŠ›æ³¢å‘å°„ã€‚</li>
<li>ç ”ç©¶ç»“æœç¬¦åˆXå°„çº¿è„‰å†²å‹æ¯«ç§’è„‰å†²æ˜Ÿçš„æ ‡å‡†æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.03457">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-97dbe863998d5e325fd368d9d662d077" align="middle">
<img src="https://picx.zhimg.com/v2-42359ddadcca8d46f91bd0bcdee0a76a" align="middle">
<img src="https://picx.zhimg.com/v2-a4550d9155600b412438f6116da90a91" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Robust-Alignment-of-the-Human-Embryo-in-3D-Ultrasound-using-PCA-and-an-Ensemble-of-Heuristic-Atlas-based-and-Learning-based-Classifiers-Evaluated-on-the-Rotterdam-Periconceptional-Cohort"><a href="#Robust-Alignment-of-the-Human-Embryo-in-3D-Ultrasound-using-PCA-and-an-Ensemble-of-Heuristic-Atlas-based-and-Learning-based-Classifiers-Evaluated-on-the-Rotterdam-Periconceptional-Cohort" class="headerlink" title="Robust Alignment of the Human Embryo in 3D Ultrasound using PCA and an   Ensemble of Heuristic, Atlas-based and Learning-based Classifiers Evaluated   on the Rotterdam Periconceptional Cohort"></a>Robust Alignment of the Human Embryo in 3D Ultrasound using PCA and an   Ensemble of Heuristic, Atlas-based and Learning-based Classifiers Evaluated   on the Rotterdam Periconceptional Cohort</h2><p><strong>Authors:Nikolai Herrmann, Marcella C. Zijta, Stefan Klein, RÃ©gine P. M. Steegers-Theunissen, Rene M. H. Wijnen, Bernadette S. de Bakker, Melek Rousian, Wietske A. P. Bastiaansen</strong></p>
<p>Standardized alignment of the embryo in three-dimensional (3D) ultrasound images aids prenatal growth monitoring by facilitating standard plane detection, improving visualization of landmarks and accentuating differences between different scans. In this work, we propose an automated method for standardizing this alignment. Given a segmentation mask of the embryo, Principal Component Analysis (PCA) is applied to the mask extracting the embryoâ€™s principal axes, from which four candidate orientations are derived. The candidate in standard orientation is selected using one of three strategies: a heuristic based on Pearsonâ€™s correlation assessing shape, image matching to an atlas through normalized cross-correlation, and a Random Forest classifier. We tested our method on 2166 images longitudinally acquired 3D ultrasound scans from 1043 pregnancies from the Rotterdam Periconceptional Cohort, ranging from 7+0 to 12+6 weeks of gestational age. In 99.0% of images, PCA correctly extracted the principal axes of the embryo. The correct candidate was selected by the Pearson Heuristic, Atlas-based and Random Forest in 97.4%, 95.8%, and 98.4% of images, respectively. A Majority Vote of these selection methods resulted in an accuracy of 98.5%. The high accuracy of this pipeline enables consistent embryonic alignment in the first trimester, enabling scalable analysis in both clinical and research settings. The code is publicly available at: <a target="_blank" rel="noopener" href="https://gitlab.com/radiology/prenatal-image-analysis/pca-3d-alignment">https://gitlab.com/radiology/prenatal-image-analysis/pca-3d-alignment</a>. </p>
<blockquote>
<p>èƒšèƒåœ¨ä¸‰ç»´ï¼ˆ3Dï¼‰è¶…å£°å›¾åƒä¸­çš„æ ‡å‡†åŒ–å¯¹é½ï¼Œé€šè¿‡ä¿ƒè¿›æ ‡å‡†å¹³é¢çš„æ£€æµ‹ã€æé«˜åœ°æ ‡çš„å¯è§†åŒ–å’Œçªå‡ºä¸åŒæ‰«æä¹‹é—´çš„å·®å¼‚ï¼Œæœ‰åŠ©äºè¿›è¡Œäº§å‰ç”Ÿé•¿ç›‘æµ‹ã€‚åœ¨æ­¤å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è‡ªåŠ¨åŒ–çš„æ–¹æ³•è¿›è¡Œè¿™ç§å¯¹é½çš„æ ‡å‡†åŒ–ã€‚ç»™å®šèƒšèƒçš„åˆ†å‰²æ©è†œï¼Œåº”ç”¨ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰å¯¹æ©è†œæå–èƒšèƒçš„ä¸»è½´ï¼Œä»ä¸»è½´ä¸­æ¨å¯¼å‡ºå››ä¸ªå€™é€‰æ–¹å‘ã€‚é€šè¿‡ä¸‰ç§ç­–ç•¥ä¹‹ä¸€é€‰æ‹©å¤„äºæ ‡å‡†æ–¹å‘çš„å€™é€‰ç‰©ï¼šä¸€ç§åŸºäºçš®å°”é€Šç›¸å…³æ€§çš„å¯å‘å¼è¯„ä¼°å½¢çŠ¶ï¼Œé€šè¿‡å½’ä¸€åŒ–äº¤å‰ç›¸å…³æ€§ä¸å›¾è°±çš„å›¾åƒåŒ¹é…ï¼Œä»¥åŠéšæœºæ£®æ—åˆ†ç±»å™¨ã€‚æˆ‘ä»¬åœ¨Rotterdam Periconceptional Cohortçš„1043ä¸ªå¦Šå¨ çš„2166å¼ çºµå‘é‡‡é›†çš„3Dè¶…å£°æ‰«æå›¾åƒä¸Šæµ‹è¯•äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œè¿™äº›å›¾åƒçš„èƒé¾„ä»7+0åˆ°12+6å‘¨ä¸ç­‰ã€‚åœ¨99.0%çš„å›¾åƒä¸­ï¼ŒPCAæ­£ç¡®æå–äº†èƒšèƒçš„ä¸»è½´ã€‚çš®å°”é€Šå¯å‘å¼æ–¹æ³•ã€åŸºäºå›¾è°±çš„æ–¹æ³•å’Œéšæœºæ£®æ—åˆ†åˆ«æ­£ç¡®é€‰æ‹©äº†å€™é€‰ç‰©çš„97.4%ã€95.8%å’Œ98.4%ã€‚è¿™äº›é€‰æ‹©æ–¹æ³•çš„å¤§å¤šæ•°æŠ•ç¥¨äº§ç”Ÿäº†é«˜è¾¾98.5%çš„å‡†ç¡®æ€§ã€‚è¯¥æµç¨‹çš„é«˜å‡†ç¡®æ€§å¯å®ç°å­•æ—©æœŸèƒšèƒçš„ä¸€è‡´æ€§å¯¹é½ï¼Œåœ¨ä¸´åºŠå’Œç ”ç©¶ç¯å¢ƒä¸­å¯å®ç°å¯é‡åŒ–çš„åˆ†æã€‚ä»£ç å…¬å¼€åœ¨ï¼š<a target="_blank" rel="noopener" href="https://gitlab.com/radiology/prenatal-image-analysis/pca-3d-alignment%E3%80%82">https://gitlab.com/radiology/prenatal-image-analysis/pca-3d-alignmentã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.03416v1">PDF</a> Submitted version of paper accepted at International Workshop on   Preterm, Perinatal and Paediatric Image Analysis 2025</p>
<p><strong>Summary</strong></p>
<p>åœ¨ä¸‰ç»´è¶…å£°æ³¢å›¾åƒä¸­æ ‡å‡†åŒ–èƒšèƒå¯¹é½ï¼Œæœ‰åŠ©äºæé«˜äº§å‰ç”Ÿé•¿ç›‘æµ‹ä¸­çš„æ ‡å‡†å¹³é¢æ£€æµ‹ã€åœ°æ ‡å¯è§†åŒ–å’Œä¸åŒæ‰«æé—´çš„å·®å¼‚è¯†åˆ«ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§è‡ªåŠ¨åŒ–æ–¹æ³•è¿›è¡Œæ ‡å‡†åŒ–å¯¹é½ã€‚é€šè¿‡åº”ç”¨ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰äºèƒšèƒåˆ†å‰²æ©è†œï¼Œæå–èƒšèƒçš„ä¸»è½´ï¼Œæ¨å¯¼å‡ºå››ä¸ªå€™é€‰æ–¹å‘ã€‚é€‰æ‹©æ ‡å‡†æ–¹å‘å€™é€‰è€…é‡‡ç”¨ä¸‰ç§ç­–ç•¥ä¹‹ä¸€ï¼šåŸºäºçš®å°”é€Šç›¸å…³æ€§çš„å¯å‘å¼è¯„ä¼°å½¢çŠ¶ã€é€šè¿‡å½’ä¸€åŒ–äº’ç›¸å…³è¿›è¡Œå›¾åƒä¸å›¾è°±çš„åŒ¹é…ä»¥åŠéšæœºæ£®æ—åˆ†ç±»å™¨ã€‚åœ¨Rotterdam Periconceptional Cohortçš„1043ä¾‹å¦Šå¨ çš„2166å¼ çºµå‘é‡‡é›†çš„ä¸‰ç»´è¶…å£°æ‰«æå›¾åƒä¸Šè¿›è¡Œæµ‹è¯•ï¼Œç»“æœè¯å®PCAåœ¨99.0%çš„å›¾åƒä¸­æ­£ç¡®æå–äº†èƒšèƒçš„ä¸»è½´ã€‚çš®å°”é€Šå¯å‘å¼ã€å›¾è°±åŒ¹é…å’Œéšæœºæ£®æ—åˆ†ç±»å™¨åˆ†åˆ«åœ¨97.4%ã€95.8%å’Œ98.4%çš„å›¾åƒä¸­é€‰æ‹©äº†æ­£ç¡®çš„å€™é€‰è€…ã€‚è¿™äº›æ–¹æ³•çš„å¤§å¤šæ•°æŠ•ç¥¨è¡¨å†³å‡†ç¡®ç‡ä¸º98.5%ã€‚è¯¥æµç¨‹çš„é«˜å‡†ç¡®æ€§å®ç°äº†å­•æ—©æœŸèƒšèƒçš„ä¸€è‡´æ€§å¯¹é½ï¼Œåœ¨ä¸´åºŠå’Œç ”ç©¶ç¯å¢ƒä¸­å¯å®ç°å¤§è§„æ¨¡åˆ†æã€‚ä»£ç å…¬å¼€å¯è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://gitlab.com/radiology/prenatal-image-analysis/pca-3d-alignment%E3%80%82">https://gitlab.com/radiology/prenatal-image-analysis/pca-3d-alignmentã€‚</a> </p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¸‰ç»´è¶…å£°æ³¢å›¾åƒçš„èƒšèƒæ ‡å‡†åŒ–å¯¹é½æœ‰åŠ©äºäº§å‰ç”Ÿé•¿ç›‘æµ‹ã€‚</li>
<li>æå‡ºäº†ä¸€ç§è‡ªåŠ¨åŒ–æ–¹æ³•è¿›è¡Œèƒšèƒå¯¹é½çš„æ ‡å‡†åŒ–ã€‚</li>
<li>é€šè¿‡PCAæå–èƒšèƒä¸»è½´ï¼Œå¹¶æ®æ­¤æ¨å¯¼å‡ºå››ä¸ªå€™é€‰æ–¹å‘ã€‚</li>
<li>ä¸‰ç§é€‰æ‹©æ ‡å‡†æ–¹å‘çš„æ–¹æ³•åŒ…æ‹¬åŸºäºçš®å°”é€Šç›¸å…³æ€§çš„å¯å‘å¼è¯„ä¼°ã€å›¾åƒä¸å›¾è°±çš„åŒ¹é…ä»¥åŠéšæœºæ£®æ—åˆ†ç±»å™¨ã€‚</li>
<li>åœ¨å¤§é‡å›¾åƒæµ‹è¯•ä¸­ï¼ŒPCAå’Œå…¶ä»–æ–¹æ³•è¡¨ç°å‡ºé«˜å‡†ç¡®æ€§ã€‚</li>
<li>è¯¥æµç¨‹çš„é«˜å‡†ç¡®æ€§å®ç°äº†å­•æ—©æœŸèƒšèƒçš„ä¸€è‡´æ€§å¯¹é½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.03416">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6c2acc4d73afb98004572469d316e067" align="middle">
<img src="https://picx.zhimg.com/v2-4fb8a78f57f25794f1e858f1724209ea" align="middle">
<img src="https://picx.zhimg.com/v2-4a9638c3d5b62dea8cbea3ebda6a6abf" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Computational-Imaging-Meets-LLMs-Zero-Shot-IDH-Mutation-Prediction-in-Brain-Gliomas"><a href="#Computational-Imaging-Meets-LLMs-Zero-Shot-IDH-Mutation-Prediction-in-Brain-Gliomas" class="headerlink" title="Computational Imaging Meets LLMs: Zero-Shot IDH Mutation Prediction in   Brain Gliomas"></a>Computational Imaging Meets LLMs: Zero-Shot IDH Mutation Prediction in   Brain Gliomas</h2><p><strong>Authors:Syed Muqeem Mahmood, Hassan Mohy-ud-Din</strong></p>
<p>We present a framework that combines Large Language Models with computational image analytics for non-invasive, zero-shot prediction of IDH mutation status in brain gliomas. For each subject, coregistered multi-parametric MRI scans and multi-class tumor segmentation maps were processed to extract interpretable semantic (visual) attributes and quantitative features, serialized in a standardized JSON file, and used to query GPT 4o and GPT 5 without fine-tuning. We evaluated this framework on six publicly available datasets (N &#x3D; 1427) and results showcased high accuracy and balanced classification performance across heterogeneous cohorts, even in the absence of manual annotations. GPT 5 outperformed GPT 4o in context-driven phenotype interpretation. Volumetric features emerged as the most important predictors, supplemented by subtype-specific imaging markers and clinical information. Our results demonstrate the potential of integrating LLM-based reasoning with computational image analytics for precise, non-invasive tumor genotyping, advancing diagnostic strategies in neuro-oncology. The code is available at <a target="_blank" rel="noopener" href="https://github.com/ATPLab-LUMS/CIM-LLM">https://github.com/ATPLab-LUMS/CIM-LLM</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºä¸€ä¸ªç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹å’Œè®¡ç®—å›¾åƒåˆ†æçš„éä¾µå…¥æ€§é›¶æ ·æœ¬é¢„æµ‹è„‘èƒ¶è´¨ç˜¤IDHçªå˜çŠ¶æ€çš„æ¡†æ¶ã€‚é’ˆå¯¹æ¯ä¸ªå—è¯•è€…ï¼Œæˆ‘ä»¬å¯¹æ ¸å¿ƒå¤šå‚æ•°MRIæ‰«æå’Œå¤šå…ƒè‚¿ç˜¤åˆ†å‰²å›¾è¿›è¡Œå¤„ç†ï¼Œæå–å¯è§£é‡Šçš„è¯­ä¹‰ï¼ˆè§†è§‰ï¼‰å±æ€§å’Œå®šé‡ç‰¹å¾ï¼Œå°†å…¶åºåˆ—åŒ–åœ¨æ ‡å‡†åŒ–çš„JSONæ–‡ä»¶ä¸­ï¼Œç”¨äºæŸ¥è¯¢GPT 4oå’ŒGPT 5è€Œæ— éœ€å¾®è°ƒã€‚æˆ‘ä»¬åœ¨å…­ä¸ªå…¬å¼€æ•°æ®é›†ï¼ˆN &#x3D; 1427ï¼‰ä¸Šè¯„ä¼°äº†è¿™ä¸€æ¡†æ¶ï¼Œç»“æœæ˜¾ç¤ºï¼Œå³ä½¿åœ¨æ— éœ€æ‰‹åŠ¨æ³¨é‡Šçš„æƒ…å†µä¸‹ï¼Œè¯¥æ¡†æ¶åœ¨å¼‚è´¨ç¾¤ä½“ä¸­ä¹Ÿå…·æœ‰è¾ƒé«˜çš„å‡†ç¡®æ€§å’Œå¹³è¡¡çš„åˆ†ç±»æ€§èƒ½ã€‚GPT 5åœ¨ä¸Šä¸‹æ–‡é©±åŠ¨çš„è¡¨å‹è§£é‡Šæ–¹é¢è¡¨ç°ä¼˜äºGPT 4oã€‚ä½“ç§¯ç‰¹å¾æ˜¯æœ€é‡è¦é¢„æµ‹å› ç´ ï¼Œè¾…ä»¥äºšå‹ç‰¹å¼‚æ€§æˆåƒæ ‡è®°å’Œä¸´åºŠä¿¡æ¯ã€‚æˆ‘ä»¬çš„ç»“æœè¯æ˜äº†å°†LLMæ¨ç†ä¸è®¡ç®—å›¾åƒåˆ†æç›¸ç»“åˆåœ¨ç²¾ç¡®éä¾µå…¥æ€§è‚¿ç˜¤åŸºå› åˆ†å‹æ–¹é¢çš„æ½œåŠ›ï¼Œä¸ºç¥ç»è‚¿ç˜¤å­¦çš„è¯Šæ–­ç­–ç•¥æä¾›äº†å…ˆè¿›çš„æ‰‹æ®µã€‚ä»£ç å¯åœ¨ <a target="_blank" rel="noopener" href="https://github.com/ATPLab-LUMS/CIM-LLM">https://github.com/ATPLab-LUMS/CIM-LLM</a> è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.03376v1">PDF</a> 5 pages, 1 figure, 3 tables</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹å’Œè®¡ç®—å›¾åƒåˆ†æçš„éä¾µå…¥æ€§é›¶æ ·æœ¬é¢„æµ‹è„‘èƒ¶è´¨ç˜¤IDHçªå˜çŠ¶æ€çš„æ–¹æ³•ã€‚é€šè¿‡å¯¹æ¯ä½æ‚£è€…çš„å¤šå‚æ•°MRIæ‰«æå’Œå¤šç±»è‚¿ç˜¤åˆ†å‰²å›¾è¿›è¡Œæ ¸å¿ƒæ³¨å†Œå¤„ç†ï¼Œæå–å¯è§£é‡Šçš„è¯­ä¹‰ï¼ˆè§†è§‰ï¼‰ç‰¹å¾å’Œå®šé‡ç‰¹å¾ï¼Œå¹¶åºåˆ—åŒ–å­˜å‚¨åœ¨æ ‡å‡†åŒ–JSONæ–‡ä»¶ä¸­ï¼Œç”¨äºæŸ¥è¯¢GPT 4oå’ŒGPT 5æ¨¡å‹è€Œæ— éœ€å¾®è°ƒã€‚åœ¨å…­ä¸ªå…¬å¼€æ•°æ®é›†ï¼ˆN &#x3D; 1427ï¼‰ä¸Šè¯„ä¼°è¯¥æ¡†æ¶ï¼Œç»“æœæ˜¾ç¤ºå…¶å…·æœ‰è¾ƒé«˜çš„å‡†ç¡®æ€§å’Œè·¨ä¸åŒç¾¤ä½“çš„å¹³è¡¡åˆ†ç±»æ€§èƒ½ï¼Œå³ä½¿åœ¨æ— éœ€æ‰‹åŠ¨æ³¨é‡Šçš„æƒ…å†µä¸‹ä¹Ÿæ˜¯å¦‚æ­¤ã€‚GPT 5åœ¨ä¸Šä¸‹æ–‡é©±åŠ¨çš„è¡¨å‹è§£é‡Šæ–¹é¢ä¼˜äºGPT 4oã€‚ä½“ç§¯ç‰¹å¾æ˜¯æœ€é‡è¦é¢„æµ‹å› ç´ ï¼Œè¾…ä»¥ç‰¹å®šäºšå‹çš„æˆåƒæ ‡è®°å’Œä¸´åºŠä¿¡æ¯ã€‚æœ¬ç ”ç©¶ç»“æœå±•ç¤ºäº†å°†å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†ä¸è®¡ç®—å›¾åƒåˆ†æç›¸ç»“åˆåœ¨ç²¾ç¡®éä¾µå…¥æ€§è‚¿ç˜¤åŸºå› åˆ†å‹æ–¹é¢çš„æ½œåŠ›ï¼Œä¸ºç¥ç»è‚¿ç˜¤å­¦çš„è¯Šæ–­ç­–ç•¥æä¾›äº†è¿›æ­¥ã€‚ç›¸å…³ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/ATPLab-LUMS/CIM-LLM%E4%B8%8A%E3%80%82">https://github.com/ATPLab-LUMS/CIM-LLMä¸Šã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç ”ç©¶æå‡ºäº†ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹å’Œè®¡ç®—å›¾åƒåˆ†æçš„æ¡†æ¶ï¼Œç”¨äºéä¾µå…¥æ€§åœ°é¢„æµ‹è„‘èƒ¶è´¨ç˜¤çš„IDHçªå˜çŠ¶æ€ã€‚</li>
<li>åˆ©ç”¨å¤šå‚æ•°MRIæ‰«æå’Œè‚¿ç˜¤åˆ†å‰²å›¾æå–ç‰¹å¾ï¼Œå¹¶æ ‡å‡†åŒ–å¤„ç†ç”¨äºæŸ¥è¯¢è¯­è¨€æ¨¡å‹ã€‚</li>
<li>åœ¨å¤šä¸ªå…¬å¼€æ•°æ®é›†ä¸Šè¯„ä¼°ï¼Œæ˜¾ç¤ºé«˜å‡†ç¡®æ€§å’Œè·¨ç¾¤ä½“å¹³è¡¡åˆ†ç±»æ€§èƒ½ï¼Œæ— éœ€æ‰‹åŠ¨æ³¨é‡Šã€‚</li>
<li>GPT 5æ¨¡å‹åœ¨ä¸Šä¸‹æ–‡é©±åŠ¨çš„è¡¨å‹è§£é‡Šæ–¹é¢ä¼˜äºGPT 4oã€‚</li>
<li>ä½“ç§¯ç‰¹å¾æ˜¯é¢„æµ‹IDHçªå˜çŠ¶æ€æœ€é‡è¦çš„å› ç´ ã€‚</li>
<li>ç»“åˆç‰¹å®šäºšå‹çš„æˆåƒæ ‡è®°å’Œä¸´åºŠä¿¡æ¯æé«˜äº†é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.03376">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6c94b6d55cb2ba9bc0cdb3e2cae599bd" align="middle">
<img src="https://picx.zhimg.com/v2-7c2247d4f916a529e0bc7b0e0544cb17" align="middle">
<img src="https://picx.zhimg.com/v2-90bfff8d614d81dd7710a4070d0246e2" align="middle">
<img src="https://picx.zhimg.com/v2-e80892dd18b9ec23e2bbf693a7648b8a" align="middle">
<img src="https://picx.zhimg.com/v2-51675003ced1a4bd2b46925e192c7826" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Morpho-Genomic-Deep-Learning-for-Ovarian-Cancer-Subtype-and-Gene-Mutation-Prediction-from-Histopathology"><a href="#Morpho-Genomic-Deep-Learning-for-Ovarian-Cancer-Subtype-and-Gene-Mutation-Prediction-from-Histopathology" class="headerlink" title="Morpho-Genomic Deep Learning for Ovarian Cancer Subtype and Gene   Mutation Prediction from Histopathology"></a>Morpho-Genomic Deep Learning for Ovarian Cancer Subtype and Gene   Mutation Prediction from Histopathology</h2><p><strong>Authors:Gabriela Fernandes</strong></p>
<p>Ovarian cancer remains one of the most lethal gynecological malignancies, largely due to late diagnosis and extensive heterogeneity across subtypes. Current diagnostic methods are limited in their ability to reveal underlying genomic variations essential for precision oncology. This study introduces a novel hybrid deep learning pipeline that integrates quantitative nuclear morphometry with deep convolutional image features to perform ovarian cancer subtype classification and gene mutation inference directly from Hematoxylin and Eosin (H&amp;E) histopathological images. Using $\sim45,000$ image patches sourced from The Cancer Genome Atlas (TCGA) and public datasets, a fusion model combining a ResNet-50 Convolutional Neural Network (CNN) encoder and a Vision Transformer (ViT) was developed. This model successfully captured both local morphological texture and global tissue context. The pipeline achieved a robust overall subtype classification accuracy of $84.2%$ (Macro AUC of $0.87 \pm 0.03$). Crucially, the model demonstrated the capacity for gene mutation inference with moderate-to-high accuracy: $AUC_{TP53} &#x3D; 0.82 \pm 0.02$, $AUC_{BRCA1} &#x3D; 0.76 \pm 0.04$, and $AUC_{ARID1A} &#x3D; 0.73 \pm 0.05$. Feature importance analysis established direct quantitative links, revealing that nuclear solidity and eccentricity were the dominant predictors for TP53 mutation. These findings validate that quantifiable histological phenotypes encode measurable genomic signals, paving the way for cost-effective, precision histopathology in ovarian cancer triage and diagnosis. </p>
<blockquote>
<p>åµå·¢ç™Œä»æ˜¯è‡´æ­»ç‡æœ€é«˜çš„å¦‡ç§‘æ¶æ€§è‚¿ç˜¤ä¹‹ä¸€ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºè¯Šæ–­æ—¶é—´æ™šä»¥åŠå„äºšå‹ä¹‹é—´çš„å¹¿æ³›å¼‚è´¨æ€§ã€‚å½“å‰è¯Šæ–­æ–¹æ³•åœ¨æ­ç¤ºåµå·¢ç™Œç²¾å‡†åŒ»ç–—æ‰€éœ€æ½œåœ¨åŸºå› ç»„å˜å¼‚æ–¹é¢çš„èƒ½åŠ›å—é™ã€‚æœ¬ç ”ç©¶ä»‹ç»äº†ä¸€ç§æ–°å‹æ··åˆæ·±åº¦å­¦ä¹ ç®¡é“ï¼Œå®ƒå°†å®šé‡æ ¸å½¢æ€æµ‹é‡ä¸æ·±åº¦å·ç§¯å›¾åƒç‰¹å¾ç›¸ç»“åˆï¼Œç›´æ¥ä»è‹æœ¨ç²¾å’Œä¼Šçº¢ï¼ˆH&amp;Eï¼‰ç»„ç»‡ç—…ç†å›¾åƒä¸­æ‰§è¡Œåµå·¢ç™Œäºšå‹åˆ†ç±»å’ŒåŸºå› çªå˜æ¨æ–­ã€‚è¯¥ç ”ç©¶ä½¿ç”¨æ¥è‡ªç™Œç—‡åŸºå› ç»„å›¾è°±ï¼ˆTCGAï¼‰å’Œå…¬å…±æ•°æ®é›†çš„çº¦45,000ä¸ªå›¾åƒå—ï¼Œå¼€å‘äº†ä¸€ä¸ªç»“åˆResNet-50å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ç¼–ç å™¨å’Œè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰çš„èåˆæ¨¡å‹ã€‚è¯¥æ¨¡å‹æˆåŠŸæ•è·äº†å±€éƒ¨å½¢æ€çº¹ç†å’Œå…¨å±€ç»„ç»‡ä¸Šä¸‹æ–‡ã€‚ç®¡é“å®ç°äº†ç¨³å¥çš„æ€»ä½“äºšå‹åˆ†ç±»å‡†ç¡®ç‡84.2%ï¼ˆå®è§‚AUCä¸º0.87Â±0.03ï¼‰ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œè¯¥æ¨¡å‹åœ¨åŸºå› çªå˜æ¨æ–­æ–¹é¢è¡¨ç°å‡ºä¸­ç­‰åˆ°è¾ƒé«˜çš„å‡†ç¡®æ€§ï¼šAUC_TP53&#x3D;0.82Â±0.02ï¼ŒAUC_BRCA1&#x3D;0.76Â±0.04å’ŒAUC_ARID1A&#x3D;0.73Â±0.05ã€‚ç‰¹å¾é‡è¦æ€§åˆ†æå»ºç«‹äº†ç›´æ¥çš„å®šé‡è”ç³»ï¼Œè¡¨æ˜æ ¸åšå›ºæ€§å’Œç¦»å¿ƒç‡æ˜¯TP53çªå˜çš„ä¸»è¦é¢„æµ‹å› å­ã€‚è¿™äº›å‘ç°è¯æ˜äº†å¯é‡åŒ–çš„ç»„ç»‡ç—…ç†è¡¨å‹èƒ½å¤Ÿç¼–ç å¯æµ‹é‡çš„åŸºå› ç»„ä¿¡å·ï¼Œä¸ºåµå·¢ç™Œç­›æŸ¥å’Œè¯Šæ–­ä¸­çš„ç»æµé«˜æ•ˆã€ç²¾å‡†ç»„ç»‡ç—…ç†å­¦é“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.03365v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬ç ”ç©¶ä»‹ç»äº†ä¸€ç§æ–°å‹æ··åˆæ·±åº¦å­¦ä¹ ç®¡é“ï¼Œè¯¥ç®¡é“ç»“åˆäº†å®šé‡æ ¸å½¢æ€æµ‹é‡å’Œæ·±åº¦å·ç§¯å›¾åƒç‰¹å¾ï¼Œå¯ä»è‹æœ¨ç²¾å’Œä¼Šçº¢ï¼ˆH&amp;Eï¼‰ç»„ç»‡ç—…ç†å­¦å›¾åƒç›´æ¥è¿›è¡Œåµå·¢ç™Œäºšå‹åˆ†ç±»å’ŒåŸºå› çªå˜æ¨æ–­ã€‚è¯¥æ¨¡å‹æˆåŠŸæ•è·äº†å±€éƒ¨å½¢æ€çº¹ç†å’Œå…¨å±€ç»„ç»‡ä¸Šä¸‹æ–‡ï¼Œå®ç°äº†ç¨³å¥çš„äºšå‹åˆ†ç±»å‡†ç¡®ç‡ï¼ˆå®è§‚AUCä¸º0.87Â±0.03ï¼‰ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹è¿˜å…·æœ‰ä¸­ç­‰è‡³é«˜åº¦çš„åŸºå› çªå˜æ¨æ–­å‡†ç¡®æ€§ã€‚ç‰¹å¾é‡è¦æ€§åˆ†ææ­ç¤ºäº†æ ¸å®ä½“æ€§å’Œåå¿ƒç‡æ˜¯TP53çªå˜çš„ä¸»è¦é¢„æµ‹å› å­ã€‚è¿™ä¸ºç²¾ç¡®ç—…ç†ç»„ç»‡å­¦åœ¨åµå·¢ç™Œç­›æŸ¥å’Œè¯Šæ–­ä¸­çš„æˆæœ¬æ•ˆç›Šé“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>å…³é”®è¦ç‚¹</strong></p>
<ul>
<li>åµå·¢ç™Œæ˜¯ä¸€ç§è‡´æ­»ç‡é«˜çš„å¦‡ç§‘æ¶æ€§è‚¿ç˜¤ï¼Œå½“å‰è¯Šæ–­æ–¹æ³•éš¾ä»¥æ­ç¤ºåŸºå› ç»„å˜å¼‚ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹æ··åˆæ·±åº¦å­¦ä¹ ç®¡é“ï¼Œæ•´åˆäº†å®šé‡æ ¸å½¢æ€æµ‹é‡ä¸æ·±åº¦å·ç§¯å›¾åƒç‰¹å¾ã€‚</li>
<li>è¯¥æ¨¡å‹ä»H&amp;Eç»„ç»‡ç—…ç†å­¦å›¾åƒä¸­ç›´æ¥è¿›è¡Œåµå·¢ç™Œäºšå‹åˆ†ç±»å’ŒåŸºå› çªå˜æ¨æ–­ã€‚</li>
<li>æ¨¡å‹å®ç°äº†ç¨³å¥çš„äºšå‹åˆ†ç±»å‡†ç¡®ç‡ï¼ˆå®è§‚AUCä¸º0.87Â±0.03ï¼‰ï¼Œå¹¶å±•ç¤ºäº†ä¸­ç­‰è‡³é«˜åº¦çš„åŸºå› çªå˜æ¨æ–­å‡†ç¡®æ€§ã€‚</li>
<li>ç‰¹å¾é‡è¦æ€§åˆ†æå‘ç°æ ¸å®ä½“æ€§å’Œåå¿ƒç‡æ˜¯é¢„æµ‹åŸºå› å˜å¼‚çš„å…³é”®æŒ‡æ ‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.03365">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e20854d2c1b3999051d667e2c93f4d0e" align="middle">
<img src="https://picx.zhimg.com/v2-5ed552e57f3758829d1efb2f8ebc2a78" align="middle">
<img src="https://picx.zhimg.com/v2-c11b9f483d1949225219c1da8bcfecb6" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="The-nexus-between-negative-charge-transfer-and-reduced-on-site-Coulomb-energy-in-correlated-topological-metals"><a href="#The-nexus-between-negative-charge-transfer-and-reduced-on-site-Coulomb-energy-in-correlated-topological-metals" class="headerlink" title="The nexus between negative charge-transfer and reduced on-site Coulomb   energy in correlated topological metals"></a>The nexus between negative charge-transfer and reduced on-site Coulomb   energy in correlated topological metals</h2><p><strong>Authors:A. R. Shelke, C. -W. Chuang, S. Hamamoto, M. Oura, M. Yoshimura, N. Hiraoka, C. -N. Kuo, C. -S. Lue, A. Fujimori, A. Chainani</strong></p>
<p>The layered $3d$ transition metal dichalcogenides (TMDs) CoTe$<em>2$ and NiTe$<em>2$ are topological Dirac Type-II metals. Their $d$-bands do not exhibit the expected correlation-induced band narrowing seen in CoO and NiO. We address this conundrum by quantifying the on-site Coulomb energy $U</em>{dd}$ via single-particle partial density of states and the two-hole correlation satellite using valence band resonant photoemission spectroscopy (PES), and obtain $U</em>{dd}$ &#x3D; 3.0 eV&#x2F;3.7 eV for CoTe$_2$&#x2F;NiTe$<em>2$. Charge-transfer (CT) cluster model simulations of the measured core-level PES and x-ray absorption spectra of CoTe$<em>2$ and CoO validate their contrasting electronic parameters:$U</em>{dd}$ and CT energy $\Delta$ are (3.0 eV, -2.0 eV) for CoTe$<em>2$, and (5.0 eV, 4.0 eV) for CoO, respectively. The $d$-$p$ hybridization strength $T</em>{eg}$ for CoTe$<em>2$$&lt;$CoO, and indicates that the reduced $U_{dd}$ in CoTe$_2$ is not due to $T_{eg}$. The increase in $d^n$-count$\sim$1 by CT from ligand to Co site in CoTe$_2$ is due to a negative-$\Delta$ and reduced $U_{dd}$. Yet, only because $U_{dd}$$&gt;$$\big|\Delta\big|$, CoTe$</em>{2}$ becomes a topological metal with $p$$\rightarrow$${p}$ type lowest energy excitations. Similarly, we obtain a negative-$\Delta$ and reduced $U</em>{dd}$ in NiTe$<em>2$ compared to NiO. The study reveals the nexus between negative-$\Delta$ and reduced $U</em>{dd}$ required for setting up the electronic structure framework for achieving topological behavior via band inversion in correlated metals. </p>
<blockquote>
<p>åˆ†å±‚ä¸‰ç»´è¿‡æ¸¡é‡‘å±äºŒå¤åŒ–ç‰©ï¼ˆTMDsï¼‰ä¸­çš„CoTe2å’ŒNiTe2æ˜¯æ‹“æ‰‘ç‹„æ‹‰å…‹ç¬¬äºŒç±»é‡‘å±ã€‚å®ƒä»¬çš„då¸¦æ²¡æœ‰è¡¨ç°å‡ºåœ¨CoOå’ŒNiOä¸­é¢„æœŸçš„ç”±ç›¸å…³æ€§å¼•èµ·çš„å¸¦çª„åŒ–ç°è±¡ã€‚æˆ‘ä»¬é€šè¿‡é‡åŒ–ç°åœºçš„åº“ä»‘èƒ½Uddï¼Œä½¿ç”¨å•ç²’å­éƒ¨åˆ†æ€å¯†åº¦å’Œé€šè¿‡ä»·å¸¦å…±æŒ¯å…‰ç”µå­å…‰è°±æ³•ï¼ˆPESï¼‰çš„ä¸¤å­”ç›¸å…³æ€§å«æ˜Ÿï¼Œè§£å†³äº†è¿™ä¸€éš¾é¢˜ï¼Œå¹¶å¾—å‡ºCoTe2&#x2F;NiTe2çš„Uddä¸º3.0 eV&#x2F;3.7 eVã€‚å¯¹CoTe2å’ŒCoOçš„æµ‹é‡æ ¸å¿ƒèƒ½çº§PESå’ŒXå°„çº¿å¸æ”¶å…‰è°±çš„ç”µè·è½¬ç§»ï¼ˆCTï¼‰é›†ç¾¤æ¨¡å‹æ¨¡æ‹ŸéªŒè¯äº†å…¶å¯¹æ¯”ç”µå­å‚æ•°ï¼šå¯¹äºCoTe2ï¼ŒUddå’ŒCTèƒ½é‡Î”åˆ†åˆ«ä¸ºï¼ˆ3.0 eVï¼Œ-2.0 eVï¼‰ï¼Œè€Œå¯¹äºCoOåˆ†åˆ«ä¸ºï¼ˆ5.0 eVï¼Œ4.0 eVï¼‰ã€‚CoTe2çš„d-pæ‚åŒ–å¼ºåº¦Tegå°äºCoOï¼Œè¡¨æ˜CoTe2ä¸­Uddçš„å‡å°‘å¹¶éç”±äºTegã€‚CoTe2ä¸­ä»é…ä½“åˆ°Coä½ç‚¹çš„ç”µè·è½¬ç§»å¯¼è‡´çš„d nè®¡æ•°å¢åŠ çº¦ä¸º1æ˜¯ç”±äºè´ŸÎ”å’Œå‡å°‘çš„Uddã€‚ç„¶è€Œï¼Œåªæœ‰Udd&gt; |Î”|æ—¶ï¼ŒCoTe2æ‰èƒ½æˆä¸ºå…·æœ‰pâ†’på‹æœ€ä½èƒ½é‡æ¿€å‘çš„æ‹“æ‰‘é‡‘å±ã€‚ç±»ä¼¼åœ°ï¼Œä¸NiOç›¸æ¯”ï¼Œæˆ‘ä»¬åœ¨NiTe2ä¸­ä¹Ÿè·å¾—äº†è´ŸÎ”å’Œå‡å°‘çš„Uddã€‚è¯¥ç ”ç©¶æ­ç¤ºäº†è´ŸÎ”å’Œå‡å°‘çš„Uddä¹‹é—´çš„å…³è”ï¼Œè¿™æ˜¯é€šè¿‡åœ¨ç›¸å…³é‡‘å±ä¸­å®ç°èƒ½å¸¦åè½¬æ¥å®ç°æ‹“æ‰‘è¡Œä¸ºæ‰€éœ€çš„ç”µå­ç»“æ„æ¡†æ¶çš„å…³é”®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.03299v1">PDF</a> 8 pages + 5 figures(main) and 10 pages + 9 figures (SM) (submitted to   PRB)</p>
<p><strong>Summary</strong></p>
<p>é’´å’Œé•çš„äºŒå…ƒè¿‡æ¸¡é‡‘å±ç¡«åŒ–ç‰©ï¼ˆTMDsï¼‰CoTe2å’ŒNiTe2æ˜¯æ‹“æ‰‘ç‹„æ‹‰å…‹ç¬¬äºŒç±»é‡‘å±ã€‚å®ƒä»¬å‘ˆç°å‡ºä¸åŒäºæ°§åŒ–é’´å’Œæ°§åŒ–é•çš„då¸¦ç‰¹æ€§ã€‚é€šè¿‡æµ‹é‡éƒ¨åˆ†æ€å¯†åº¦å’Œä¸¤å­”å…³è”å«æ˜Ÿçš„åº“ä»‘èƒ½Udï¼Œå‘ç°CoTe2å’ŒNiTe2çš„Udåˆ†åˆ«ä¸º3.0 eVå’Œ3.7 eVã€‚åŸºäºç”µè·è½¬ç§»æ¨¡å‹å¯¹CoTe2å’ŒCoOçš„æ ¸å¿ƒèƒ½çº§å…‰ç”µå­å‘å°„å…‰è°±å’ŒXå°„çº¿å¸æ”¶å…‰è°±çš„æ¨¡æ‹Ÿï¼ŒéªŒè¯äº†å…¶ä¸åŒçš„ç”µå­å‚æ•°ï¼Œæ­ç¤ºäº†å¯¹æ¯”é²œæ˜çš„ç”µå­ç»“æ„å’Œæ‹“æ‰‘è¡Œä¸ºçš„å†…åœ¨å…³è”ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œè´Ÿç”µè·è½¬ç§»èƒ½é‡Î”å’Œé™ä½çš„Udæ˜¯å®ç°æ‹“æ‰‘é‡‘å±çš„å…³é”®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CoTe2å’ŒNiTe2æ˜¯æ‹“æ‰‘ç‹„æ‹‰å…‹ç¬¬äºŒç±»é‡‘å±ï¼Œå…¶då¸¦ç‰¹æ€§ä¸åŒäºæ°§åŒ–é’´å’Œæ°§åŒ–é•ã€‚</li>
<li>é€šè¿‡æµ‹é‡éƒ¨åˆ†æ€å¯†åº¦å’Œä¸¤å­”å…³è”å«æ˜Ÿï¼Œå‘ç°CoTe2å’ŒNiTe2çš„åº“ä»‘èƒ½Udåˆ†åˆ«ä¸º3.0 eVå’Œ3.7 eVã€‚</li>
<li>åŸºäºç”µè·è½¬ç§»æ¨¡å‹å¯¹CoTe2å’ŒCoOçš„æ ¸å¿ƒèƒ½çº§å…‰ç”µå­å‘å°„å…‰è°±å’ŒXå°„çº¿å¸æ”¶å…‰è°±æ¨¡æ‹ŸéªŒè¯äº†å…¶ä¸åŒçš„ç”µå­å‚æ•°ã€‚</li>
<li>è´Ÿç”µè·è½¬ç§»èƒ½é‡Î”å’Œé™ä½çš„Udå¯¹äºå®ç°æ‹“æ‰‘é‡‘å±è¡Œä¸ºè‡³å…³é‡è¦ã€‚</li>
<li>CoTe2ä¸­çš„d-pæ‚åŒ–å¼ºåº¦ä½äºCoOï¼Œè¡¨æ˜å…¶è¾ƒä½çš„Udå¹¶éç”±d-pæ‚åŒ–å¼•èµ·ã€‚</li>
<li>ä»é…ä½“åˆ°é’´ä½çš„ç”µè·è½¬ç§»å¯¼è‡´d^nè®¡æ•°å¢åŠ çº¦1ï¼Œè¿™æ˜¯ç”±è´ŸÎ”å’Œé™ä½çš„Udå¼•èµ·çš„ã€‚ç„¶è€Œï¼Œåªæœ‰Udå¤§äºÎ”æ—¶ï¼ŒCoTe2æ‰èƒ½æˆä¸ºæ‹“æ‰‘é‡‘å±ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.03299">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-13b66a6e8f3917908189fffa4daad473" align="middle">
<img src="https://picx.zhimg.com/v2-705775a9fde703855acebb460a412a76" align="middle">
<img src="https://picx.zhimg.com/v2-dab18afc11e9bece69cbf21397407e6b" align="middle">
<img src="https://picx.zhimg.com/v2-1b1859b04f413f06f9552afa90024179" align="middle">
<img src="https://picx.zhimg.com/v2-f4fd181fe6d20d7864c326a049563444" align="middle">
<img src="https://picx.zhimg.com/v2-50a94c30e91525a39862576d09e8fb10" align="middle">
<img src="https://picx.zhimg.com/v2-178922d3d63a936d99b568e4d346d092" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Enhancing-Medical-Image-Segmentation-via-Heat-Conduction-Equation"><a href="#Enhancing-Medical-Image-Segmentation-via-Heat-Conduction-Equation" class="headerlink" title="Enhancing Medical Image Segmentation via Heat Conduction Equation"></a>Enhancing Medical Image Segmentation via Heat Conduction Equation</h2><p><strong>Authors:Rong Wu, Yim-Sang Yu</strong></p>
<p>Medical image segmentation has been significantly advanced by deep learning architectures, notably U-Net variants. However, existing models struggle to achieve efficient global context modeling and long-range dependency reasoning under practical computational budgets simultaneously. In this work, we propose a novel hybrid architecture utilizing U-Mamba with Heat Conduction Equation. Our model combines Mamba-based state-space modules for efficient long-range reasoning with Heat Conduction Operators (HCOs) in the bottleneck layers, simulating frequency-domain thermal diffusion for enhanced semantic abstraction. Experimental results on multimodal abdominal CT and MRI datasets demonstrate that the proposed model consistently outperforms strong baselines, validating its effectiveness and generalizability. It suggest that blending state-space dynamics with heat-based global diffusion offers a scalable and interpretable solution for medical segmentation tasks. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²é¢†åŸŸåœ¨æ·±åº¦å­¦ä¹ æ¶æ„çš„æ¨åŠ¨ä¸‹å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œå°¤å…¶æ˜¯U-Netå˜ä½“ã€‚ç„¶è€Œï¼Œç°æœ‰æ¨¡å‹åœ¨å®ç”¨è®¡ç®—é¢„ç®—ä¸‹éš¾ä»¥åŒæ—¶å®ç°æœ‰æ•ˆçš„å…¨å±€ä¸Šä¸‹æ–‡å»ºæ¨¡å’Œé•¿ç¨‹ä¾èµ–æ¨ç†ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ··åˆæ¶æ„ï¼Œåˆ©ç”¨U-Mambaå’Œå¯¼çƒ­æ–¹ç¨‹ã€‚æˆ‘ä»¬çš„æ¨¡å‹ç»“åˆäº†åŸºäºMambaçš„çŠ¶æ€ç©ºé—´æ¨¡å—ï¼Œç”¨äºé«˜æ•ˆçš„é•¿ç¨‹æ¨ç†ï¼Œå¹¶åœ¨ç“¶é¢ˆå±‚å¼•å…¥äº†å¯¼çƒ­ç®—å­ï¼ˆHCOsï¼‰ï¼Œæ¨¡æ‹Ÿé¢‘åŸŸçƒ­æ‰©æ•£ä»¥å¢å¼ºè¯­ä¹‰æŠ½è±¡ã€‚åœ¨å¤šæ¨¡æ€è…¹éƒ¨CTå’ŒMRIæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ¨¡å‹å§‹ç»ˆä¼˜äºå¼ºåŸºçº¿ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚è¿™è¯æ˜å°†çŠ¶æ€ç©ºé—´åŠ¨åŠ›å­¦ä¸åŸºäºçƒ­çš„å…¨å±€æ‰©æ•£ç›¸ç»“åˆï¼Œä¸ºåŒ»å­¦åˆ†å‰²ä»»åŠ¡æä¾›äº†å¯æ‰©å±•å’Œå¯è§£é‡Šçš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.03260v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒåˆ†å‰²é¢†åŸŸï¼Œæ·±åº¦å­¦ä¹ æ¶æ„ç‰¹åˆ«æ˜¯U-Netå˜ä½“å·²å–å¾—æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œç°æœ‰æ¨¡å‹åœ¨æœ‰é™çš„è®¡ç®—é¢„ç®—ä¸‹éš¾ä»¥å®ç°é«˜æ•ˆçš„å…¨å±€ä¸Šä¸‹æ–‡å»ºæ¨¡å’Œé•¿è·ç¦»ä¾èµ–æ¨ç†ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹æ··åˆæ¶æ„U-Mambaä¸çƒ­ä¼ å¯¼æ–¹ç¨‹ç»“åˆçš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•ç»“åˆäº†åŸºäºMambaçš„çŠ¶æ€ç©ºé—´æ¨¡å—è¿›è¡Œé«˜æ•ˆçš„é•¿è·ç¦»æ¨ç†ï¼Œå¹¶åœ¨ç“¶é¢ˆå±‚å¼•å…¥çƒ­ä¼ å¯¼ç®—å­ï¼ˆHCOsï¼‰ï¼Œæ¨¡æ‹Ÿé¢‘åŸŸçƒ­æ‰©æ•£ä»¥å¢å¼ºè¯­ä¹‰æŠ½è±¡ã€‚åœ¨å¤šæ¨¡æ€è…¹éƒ¨CTå’ŒMRIæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¼ºåŸºçº¿åŸºç¡€ä¸Šè¡¨ç°ä¼˜å¼‚ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ã€‚è¿™è¡¨æ˜ç»“åˆçŠ¶æ€ç©ºé—´åŠ¨åŠ›å­¦ä¸åŸºäºçƒ­çš„å…¨å±€æ‰©æ•£ä¸ºåŒ»å­¦åˆ†å‰²ä»»åŠ¡æä¾›äº†å¯æ‰©å±•ä¸”å¯è§£é‡Šçš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ æ¶æ„åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²é¢†åŸŸå–å¾—æ˜¾è‘—è¿›å±•ï¼Œå°¤å…¶æ˜¯U-Netå˜ä½“ã€‚</li>
<li>ç°æœ‰æ¨¡å‹åœ¨è®¡ç®—é¢„ç®—æœ‰é™çš„æƒ…å†µä¸‹éš¾ä»¥å®ç°å…¨å±€ä¸Šä¸‹æ–‡å»ºæ¨¡å’Œé•¿è·ç¦»ä¾èµ–æ¨ç†çš„å¹³è¡¡ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹æ··åˆæ¶æ„U-Mambaä¸çƒ­ä¼ å¯¼æ–¹ç¨‹ç»“åˆçš„æ–¹æ³•ï¼Œç»“åˆäº†çŠ¶æ€ç©ºé—´æ¨¡å—å’Œçƒ­ä¼ å¯¼ç®—å­ã€‚</li>
<li>çŠ¶æ€ç©ºé—´æ¨¡å—ç”¨äºé«˜æ•ˆé•¿è·ç¦»æ¨ç†ï¼Œè€Œçƒ­ä¼ å¯¼ç®—å­æ¨¡æ‹Ÿé¢‘åŸŸçƒ­æ‰©æ•£ä»¥å¢å¼ºè¯­ä¹‰æŠ½è±¡ã€‚</li>
<li>åœ¨å¤šæ¨¡æ€è…¹éƒ¨CTå’ŒMRIæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>è¯¥æ–¹æ³•å…·æœ‰æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ï¼Œç›¸è¾ƒäºå¼ºåŸºçº¿æœ‰æ˜¾è‘—æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.03260">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-239184bad713be23aaed4a7ca5d49763" align="middle">
<img src="https://picx.zhimg.com/v2-4331fd1a6658fb125e5f0cbfb7104320" align="middle">
<img src="https://picx.zhimg.com/v2-fbd94b47c360aafb0abf30bdcd538f61" align="middle">
<img src="https://picx.zhimg.com/v2-5b9286e846b0540d7feaaf0f8dde6df7" align="middle">
<img src="https://picx.zhimg.com/v2-773cba1446a753c7aa40387d672c6fe9" align="middle">
<img src="https://picx.zhimg.com/v2-053a2f44a22d0ce9c83b0002a79deb2a" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Diffusion-Guided-Mask-Consistent-Paired-Mixing-for-Endoscopic-Image-Segmentation"><a href="#Diffusion-Guided-Mask-Consistent-Paired-Mixing-for-Endoscopic-Image-Segmentation" class="headerlink" title="Diffusion-Guided Mask-Consistent Paired Mixing for Endoscopic Image   Segmentation"></a>Diffusion-Guided Mask-Consistent Paired Mixing for Endoscopic Image   Segmentation</h2><p><strong>Authors:Pengyu Jie, Wanquan Liu, Rui He, Yihui Wen, Deyu Meng, Chenqiang Gao</strong></p>
<p>Augmentation for dense prediction typically relies on either sample mixing or generative synthesis. Mixing improves robustness but misaligned masks yield soft label ambiguity. Diffusion synthesis increases apparent diversity but, when trained as common samples, overlooks the structural benefit of mask conditioning and introduces synthetic-real domain shift. We propose a paired, diffusion-guided paradigm that fuses the strengths of both. For each real image, a synthetic counterpart is generated under the same mask and the pair is used as a controllable input for Mask-Consistent Paired Mixing (MCPMix), which mixes only image appearance while supervision always uses the original hard mask. This produces a continuous family of intermediate samples that smoothly bridges synthetic and real appearances under shared geometry, enlarging diversity without compromising pixel-level semantics. To keep learning aligned with real data, Real-Anchored Learnable Annealing (RLA) adaptively adjusts the mixing strength and the loss weight of mixed samples over training, gradually re-anchoring optimization to real data and mitigating distributional bias. Across Kvasir-SEG, PICCOLO, CVC-ClinicDB, a private NPC-LES cohort, and ISIC 2017, the approach achieves state-of-the-art segmentation performance and consistent gains over baselines. The results show that combining label-preserving mixing with diffusion-driven diversity, together with adaptive re-anchoring, yields robust and generalizable endoscopic segmentation. </p>
<blockquote>
<p>æ•°æ®å¢å¼ºå¯¹äºå¯†é›†é¢„æµ‹é€šå¸¸ä¾èµ–äºæ ·æœ¬æ··åˆæˆ–ç”Ÿæˆåˆæˆã€‚æ··åˆæé«˜äº†ç¨³å¥æ€§ï¼Œä½†é”™ä½æ©è†œä¼šå¯¼è‡´è½¯æ ‡ç­¾æ­§ä¹‰ã€‚æ‰©æ•£åˆæˆå¢åŠ äº†æ˜æ˜¾çš„å¤šæ ·æ€§ï¼Œä½†å½“ä½œä¸ºå¸¸è§„æ ·æœ¬è¿›è¡Œè®­ç»ƒæ—¶ï¼Œå®ƒå¿½ç•¥äº†æ©è†œæ¡ä»¶çš„ç»“æ„æ€§ä¼˜åŠ¿ï¼Œå¹¶å¼•å…¥äº†åˆæˆ-çœŸå®åŸŸåç§»ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§é…å¯¹ã€æ‰©æ•£å¼•å¯¼çš„èŒƒå¼ï¼Œèåˆäº†ä¸¤è€…çš„ä¼˜ç‚¹ã€‚å¯¹äºæ¯ä¸ªçœŸå®å›¾åƒï¼Œåœ¨ç›¸åŒæ©è†œä¸‹ç”Ÿæˆä¸€ä¸ªåˆæˆå¯¹åº”ç‰©ï¼Œå¹¶ä¸”è¯¥å¯¹ç”¨ä½œå¯æ§è¾“å…¥ï¼Œç”¨äºæ©è†œä¸€è‡´é…å¯¹æ··åˆï¼ˆMCPMixï¼‰ï¼Œå…¶ä¸­ä»…æ··åˆå›¾åƒå¤–è§‚ï¼Œè€Œç›‘ç£å§‹ç»ˆä½¿ç”¨åŸå§‹çš„ç¡¬æ©è†œã€‚è¿™äº§ç”Ÿäº†ä¸€ç³»åˆ—ä¸­é—´æ ·æœ¬ï¼Œè¿™äº›æ ·æœ¬åœ¨å…±äº«å‡ ä½•ç»“æ„ä¸‹å¹³æ»‘åœ°æ¡¥æ¥äº†åˆæˆå’ŒçœŸå®å¤–è§‚ï¼Œæ‰©å¤§äº†å¤šæ ·æ€§ï¼ŒåŒæ—¶ä¸æŸå®³åƒç´ çº§è¯­ä¹‰ã€‚ä¸ºäº†ä¿æŒå­¦ä¹ ä¸çœŸå®æ•°æ®å¯¹é½ï¼ŒReal-Anchored Learnable Annealingï¼ˆRLAï¼‰è‡ªé€‚åº”åœ°è°ƒæ•´æ··åˆå¼ºåº¦å’Œæ··åˆæ ·æœ¬çš„æŸå¤±æƒé‡ï¼Œé€æ­¥å°†ä¼˜åŒ–é‡æ–°é”šå®šåˆ°çœŸå®æ•°æ®ï¼Œå¹¶å‡è½»åˆ†å¸ƒåè§ã€‚åœ¨Kvasir-SEGã€PICCOLOã€CVC-ClinicDBã€ç§äººNPC-LESé˜Ÿåˆ—å’ŒISIC 2017ä¸­ï¼Œè¯¥æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„åˆ†å‰²æ€§èƒ½ï¼Œå¹¶åœ¨åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†ä¸€è‡´çš„æ”¶ç›Šã€‚ç»“æœè¡¨æ˜ï¼Œå°†æ ‡ç­¾ä¿ç•™æ··åˆä¸æ‰©æ•£é©±åŠ¨å¤šæ ·æ€§ç›¸ç»“åˆï¼Œå†åŠ ä¸Šè‡ªé€‚åº”é‡æ–°é”šå®šï¼Œå¯å®ç°ç¨³å¥ä¸”å¯æ¨å¹¿çš„å†…é•œåˆ†å‰²ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.03219v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§ç»“åˆæ ·æœ¬æ··åˆå’Œæ‰©æ•£åˆæˆçš„æ–¹æ³•ï¼Œç”¨äºå¢å¼ºå¯†é›†é¢„æµ‹ä»»åŠ¡çš„æ•ˆæœã€‚è¯¥æ–¹æ³•é€šè¿‡ç”Ÿæˆä¸çœŸå®å›¾åƒé…å¯¹çš„äººå·¥å›¾åƒï¼Œæå‡ºMask-Consistent Paired Mixingï¼ˆMCPMixï¼‰æŠ€æœ¯ï¼Œåœ¨ä¿æŒç›‘ç£æ—¶ä½¿ç”¨åŸå§‹ç¡¬æ©è†œçš„åŒæ—¶ï¼Œåªæ··åˆå›¾åƒå¤–è§‚ã€‚æ­¤å¤–ï¼Œå¼•å…¥Real-Anchored Learnable Annealingï¼ˆRLAï¼‰æŠ€æœ¯ï¼Œè‡ªé€‚åº”è°ƒæ•´æ··åˆå¼ºåº¦å’ŒæŸå¤±æƒé‡ï¼Œä½¿å­¦ä¹ å§‹ç»ˆä¸çœŸå®æ•°æ®å¯¹é½ã€‚è¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„åˆ†å‰²æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºä¸€ç§ç»“åˆæ ·æœ¬æ··åˆå’Œæ‰©æ•£åˆæˆçš„æ–¹æ³•ï¼Œå¢å¼ºå¯†é›†é¢„æµ‹ä»»åŠ¡çš„æ€§èƒ½ã€‚</li>
<li>å¼•å…¥Mask-Consistent Paired Mixingï¼ˆMCPMixï¼‰æŠ€æœ¯ï¼Œç”Ÿæˆä¸çœŸå®å›¾åƒé…å¯¹çš„äººå·¥å›¾åƒï¼Œå¹¶ç”¨äºå¯æ§çš„è¾“å…¥ã€‚</li>
<li>MCPMixåªåœ¨å›¾åƒå¤–è§‚ä¸Šè¿›è¡Œæ··åˆï¼ŒåŒæ—¶å§‹ç»ˆä¿æŒä½¿ç”¨åŸå§‹ç¡¬æ©è†œè¿›è¡Œç›‘ç£ã€‚</li>
<li>æå‡ºReal-Anchored Learnable Annealingï¼ˆRLAï¼‰æŠ€æœ¯ï¼Œè‡ªé€‚åº”è°ƒæ•´æ··åˆå¼ºåº¦å’ŒæŸå¤±æƒé‡ã€‚</li>
<li>æ–¹æ³•å®ç°äº†åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å…ˆè¿›åˆ†å‰²æ€§èƒ½ã€‚</li>
<li>ç»“åˆæ ‡ç­¾ä¿ç•™æ··åˆå’Œæ‰©æ•£é©±åŠ¨çš„å¤šæ ·æ€§ï¼Œä»¥åŠè‡ªé€‚åº”é‡æ–°é”šå®šï¼Œå¾—åˆ°ç¨³å¥ä¸”å¯æ¨å¹¿çš„å†…é•œåˆ†å‰²ç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.03219">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-339db67600dad0bf4fc4cf09b7c43c5e" align="middle">
<img src="https://picx.zhimg.com/v2-13ad12a5a21bdf083ae4fe04b482e77c" align="middle">
<img src="https://picx.zhimg.com/v2-ca340550871924f441cc7859d0126029" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="A-Plug-and-Play-Framework-for-Volumetric-Light-Sheet-Image-Reconstruction"><a href="#A-Plug-and-Play-Framework-for-Volumetric-Light-Sheet-Image-Reconstruction" class="headerlink" title="A Plug-and-Play Framework for Volumetric Light-Sheet Image   Reconstruction"></a>A Plug-and-Play Framework for Volumetric Light-Sheet Image   Reconstruction</h2><p><strong>Authors:Yi Gong, Xinyuan Zhang, Jichen Chai, Yichen Ding, Yifei Lou</strong></p>
<p>Cardiac contraction is a rapid, coordinated process that unfolds across three-dimensional tissue on millisecond timescales. Traditional optical imaging is often inadequate for capturing dynamic cellular structure in the beating heart because of a fundamental trade-off between spatial and temporal resolution. To overcome these limitations, we propose a high-performance computational imaging framework that integrates Compressive Sensing (CS) with Light-Sheet Microscopy (LSM) for efficient, low-phototoxic cardiac imaging. The system performs compressed acquisition of fluorescence signals via random binary mask coding using a Digital Micromirror Device (DMD). We propose a Plug-and-Play (PnP) framework, solved using the alternating direction method of multipliers (ADMM), which flexibly incorporates advanced denoisers, including Tikhonov, Total Variation (TV), and BM3D. To preserve structural continuity in dynamic imaging, we further introduce temporal regularization enforcing smoothness between adjacent z-slices. Experimental results on zebrafish heart imaging under high compression ratios demonstrate that the proposed method successfully reconstructs cellular structures with excellent denoising performance and image clarity, validating the effectiveness and robustness of our algorithm in real-world high-speed, low-light biological imaging scenarios. </p>
<blockquote>
<p>å¿ƒè„æ”¶ç¼©æ˜¯ä¸€ä¸ªå¿«é€Ÿã€åè°ƒçš„è¿‡ç¨‹ï¼Œè¿™ä¸€è¿‡ç¨‹åœ¨ä¸‰ç»´ç»„ç»‡ä¸­ä»¥æ¯«ç§’ä¸ºå•ä½å±•å¼€ã€‚ç”±äºç©ºé—´å’Œæ—¶é—´åˆ†è¾¨ç‡ä¹‹é—´çš„æ ¹æœ¬æƒè¡¡ï¼Œä¼ ç»Ÿå…‰å­¦æˆåƒåœ¨æ•æ‰è·³åŠ¨å¿ƒè„çš„åŠ¨æ€ç»†èƒç»“æ„æ—¶å¸¸å¸¸æ˜¾å¾—ä¸è¶³ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é«˜æ€§èƒ½çš„è®¡ç®—æˆåƒæ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†å‹ç¼©æ„ŸçŸ¥ï¼ˆCSï¼‰ä¸å…‰ç‰‡æ˜¾å¾®é•œï¼ˆLSMï¼‰ç›¸ç»“åˆï¼Œç”¨äºé«˜æ•ˆã€ä½å…‰æ¯’æ€§å¿ƒè„æˆåƒã€‚è¯¥ç³»ç»Ÿé€šè¿‡æ•°å­—å¾®é•œè®¾å¤‡ï¼ˆDMDï¼‰ä½¿ç”¨éšæœºäºŒè¿›åˆ¶æ©æ¨¡ç¼–ç å¯¹è§å…‰ä¿¡å·è¿›è¡Œå‹ç¼©é‡‡é›†ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå³æ’å³ç”¨ï¼ˆPnPï¼‰æ¡†æ¶ï¼Œé€šè¿‡äº¤æ›¿æ–¹å‘ä¹˜æ•°æ³•ï¼ˆADMMï¼‰è§£å†³ï¼Œçµæ´»åœ°ç»“åˆäº†å…ˆè¿›çš„å»å™ªå™¨ï¼ŒåŒ…æ‹¬Tikhonovã€æ€»å˜å¼‚ï¼ˆTVï¼‰å’ŒBM3Dã€‚ä¸ºäº†ä¿æŒåŠ¨æ€æˆåƒçš„ç»“æ„è¿ç»­æ€§ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥äº†æ—¶é—´æ­£åˆ™åŒ–ï¼Œä»¥åŠ å¼ºç›¸é‚»zåˆ‡ç‰‡çš„å¹³æ»‘åº¦ã€‚åœ¨é«˜å‹ç¼©ç‡ä¸‹çš„æ–‘é©¬é±¼å¿ƒè„æˆåƒå®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•æˆåŠŸåœ°é‡å»ºäº†ç»†èƒç»“æ„ï¼Œå…·æœ‰å‡ºè‰²çš„å»å™ªæ€§èƒ½å’Œå›¾åƒæ¸…æ™°åº¦ï¼ŒéªŒè¯äº†æˆ‘ä»¬çš„ç®—æ³•åœ¨ç°å®ä¸–ç•Œä¸­é«˜é€Ÿã€ä½å…‰ç”Ÿç‰©æˆåƒåœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§å’Œç¨³å¥æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.03093v1">PDF</a> </p>
<p><strong>Summary</strong><br>å…‰å­¦æˆåƒæŠ€æœ¯å› ç©ºé—´å’Œæ—¶é—´çš„åˆ†è¾¨ç‡çš„æƒè¡¡é€šå¸¸éš¾ä»¥æ•æ‰åŠ¨æ€çš„å¿ƒè„ç»†èƒç»“æ„ï¼Œå°¤å…¶æ˜¯å¿«é€Ÿå¿ƒè·³æƒ…å†µä¸‹çš„æ•æ‰æ›´æ˜¯éš¾ä»¥å®ç°ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é«˜æ€§èƒ½çš„è®¡ç®—æˆåƒæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†å‹ç¼©æ„ŸçŸ¥ï¼ˆCSï¼‰å’Œå…‰ç‰‡æ˜¾å¾®é•œï¼ˆLSMï¼‰ï¼Œå®ç°äº†é«˜æ•ˆã€ä½å…‰æ¯’æ€§å¿ƒè„æˆåƒã€‚é€šè¿‡æ•°å­—å¾®é•œå™¨ä»¶ï¼ˆDMDï¼‰è¿›è¡ŒéšæœºäºŒè¿›åˆ¶æ©æ¨¡ç¼–ç å®ç°è§å…‰ä¿¡å·çš„å‹ç¼©é‡‡é›†ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå³æ’å³ç”¨ï¼ˆPnPï¼‰æ¡†æ¶ï¼Œé‡‡ç”¨äº¤æ›¿æ–¹å‘ä¹˜å­æ³•ï¼ˆADMMï¼‰è§£å†³è¯¥é—®é¢˜ï¼Œå¹¶çµæ´»å¼•å…¥äº†Tikhonovã€æ€»å˜å·®ï¼ˆTVï¼‰å’ŒBM3Dç­‰é«˜çº§å»å™ªå™¨ã€‚ä¸ºåœ¨åŠ¨æ€æˆåƒä¸­ä¿æŒç»“æ„è¿ç»­æ€§ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥äº†æ—¶é—´æ­£åˆ™åŒ–ï¼Œå¼ºåˆ¶ç›¸é‚»zåˆ‡ç‰‡ä¹‹é—´çš„å¹³æ»‘æ€§ã€‚åœ¨é«˜å‹ç¼©æ¯”æ¡ä»¶ä¸‹å¯¹æ–‘é©¬é±¼å¿ƒè„æˆåƒçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤ŸæˆåŠŸåœ°é‡å»ºç»†èƒç»“æ„ï¼Œå…·æœ‰å‡ºè‰²çš„é™å™ªæ€§èƒ½å’Œå›¾åƒæ¸…æ™°åº¦ï¼ŒéªŒè¯äº†è¯¥ç®—æ³•åœ¨é«˜é€Ÿã€ä½å…‰ç…§ç”Ÿç‰©æˆåƒåœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§å’Œç¨³å¥æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¼ ç»Ÿå…‰å­¦æˆåƒéš¾ä»¥æ•æ‰å¿«é€ŸåŠ¨æ€å¿ƒè„ç»†èƒç»“æ„ï¼Œå­˜åœ¨ç©ºé—´å’Œæ—¶é—´çš„åˆ†è¾¨ç‡æƒè¡¡é—®é¢˜ã€‚</li>
<li>æå‡ºä¸€ç§ç»“åˆå‹ç¼©æ„ŸçŸ¥å’Œå…‰ç‰‡æ˜¾å¾®é•œçš„è®¡ç®—æˆåƒæ¡†æ¶ï¼Œå®ç°é«˜æ•ˆä½å…‰æ¯’æ€§å¿ƒè„æˆåƒã€‚</li>
<li>é‡‡ç”¨æ•°å­—å¾®é•œå™¨ä»¶è¿›è¡Œè§å…‰ä¿¡å·å‹ç¼©é‡‡é›†ã€‚</li>
<li>æå‡ºå³æ’å³ç”¨æ¡†æ¶ï¼Œç”¨äº¤æ›¿æ–¹å‘ä¹˜å­æ³•è§£å†³ï¼Œå¹¶çµæ´»å¼•å…¥é«˜çº§å»å™ªå™¨ã€‚</li>
<li>ä¸ºä¿æŒç»“æ„è¿ç»­æ€§ï¼Œå¼•å…¥æ—¶é—´æ­£åˆ™åŒ–æŠ€æœ¯ã€‚</li>
<li>åœ¨æ–‘é©¬é±¼å¿ƒè„æˆåƒå®éªŒä¸­çš„é«˜å‹ç¼©æ¯”æ¡ä»¶ä¸‹æˆåŠŸé‡å»ºç»†èƒç»“æ„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.03093">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1c62e338cb42d2a2df4361d7770de5b6" align="middle">
<img src="https://picx.zhimg.com/v2-8270ec822045d559f8614d01f5c92d6e" align="middle">
<img src="https://picx.zhimg.com/v2-662c23317090afabec2fa200ab87bdac" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="A-Foundation-Model-for-Brain-MRI-with-Dynamic-Modality-Integration"><a href="#A-Foundation-Model-for-Brain-MRI-with-Dynamic-Modality-Integration" class="headerlink" title="A Foundation Model for Brain MRI with Dynamic Modality Integration"></a>A Foundation Model for Brain MRI with Dynamic Modality Integration</h2><p><strong>Authors:Minh Sao Khue Luu, Bair N. Tuchinov</strong></p>
<p>We present a foundation model for brain MRI that can work with different combinations of imaging sequences. The model uses one encoder with learnable modality embeddings, conditional layer normalization, and a masked autoencoding objective that accounts for missing modalities. A variance-covariance regularizer is applied to stabilize feature learning and improve representation diversity. This design removes the need for separate models for each modality and allows the network to adapt when some sequences are missing or unseen. It is trained on about 60,000 multi-center MRIs using self-supervised reconstruction and modality imputation to learn flexible representations. A learnable modality embedding guides feature extraction so the encoder can adjust to different inputs. We describe our planned evaluation on brain tumor and multiple sclerosis segmentation, as well as lesion classification, under various modality settings. Preliminary results show that the method works feasibly, and further experiments are planned to study its performance in more detail. All code and pretrained models are available at <a target="_blank" rel="noopener" href="https://github.com/BrainFM/brainfm">https://github.com/BrainFM/brainfm</a> </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§é€‚ç”¨äºä¸åŒæˆåƒåºåˆ—ç»„åˆçš„è„‘MRIåŸºç¡€æ¨¡å‹ã€‚è¯¥æ¨¡å‹ä½¿ç”¨ä¸€ä¸ªç¼–ç å™¨ï¼Œå¸¦æœ‰å¯å­¦ä¹ çš„æ¨¡æ€åµŒå…¥ã€æ¡ä»¶å±‚å½’ä¸€åŒ–ï¼Œå¹¶è€ƒè™‘äº†ç¼ºå¤±æ¨¡æ€çš„æ©ç è‡ªåŠ¨ç¼–ç ç›®æ ‡ã€‚åº”ç”¨æ–¹å·®åæ–¹å·®æ­£åˆ™åŒ–å™¨ä»¥ç¨³å®šç‰¹å¾å­¦ä¹ å¹¶æé«˜è¡¨ç¤ºå¤šæ ·æ€§ã€‚è¿™ç§è®¾è®¡æ¶ˆé™¤äº†ä¸ºæ¯ä¸ªæ¨¡æ€ä½¿ç”¨å•ç‹¬æ¨¡å‹çš„éœ€è¦ï¼Œå¹¶å…è®¸ç½‘ç»œåœ¨æŸäº›åºåˆ—ç¼ºå¤±æˆ–ä¸å¯è§çš„æƒ…å†µä¸‹è¿›è¡Œé€‚åº”ã€‚è¯¥æ¨¡å‹ä½¿ç”¨è‡ªç›‘ç£é‡å»ºå’Œæ¨¡æ€æ’è¡¥æŠ€æœ¯åœ¨çº¦6ä¸‡å¼ å¤šä¸­å¿ƒMRIä¸Šè¿›è¡Œè®­ç»ƒï¼Œä»¥å­¦ä¹ çµæ´»çš„è¡¨ç°ã€‚å¯å­¦ä¹ çš„æ¨¡æ€åµŒå…¥å¼•å¯¼ç‰¹å¾æå–ï¼Œä½¿ç¼–ç å™¨èƒ½å¤Ÿé€‚åº”ä¸åŒçš„è¾“å…¥ã€‚æˆ‘ä»¬ä»‹ç»äº†åœ¨è„‘è‚¿ç˜¤å’Œå¤šé‡ç¡¬åŒ–ç—‡åˆ†å‰²ä»¥åŠä¸åŒæ¨¡æ€è®¾ç½®ä¸‹çš„ç—…å˜åˆ†ç±»ç­‰è®¡åˆ’è¿›è¡Œçš„è¯„ä¼°ã€‚åˆæ­¥ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å…·æœ‰è‰¯å¥½çš„å¯è¡Œæ€§ï¼Œå¹¶è®¡åˆ’è¿›ä¸€æ­¥å®éªŒä»¥æ›´è¯¦ç»†åœ°ç ”ç©¶å…¶æ€§èƒ½ã€‚æ‰€æœ‰ä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹å¯åœ¨ <a target="_blank" rel="noopener" href="https://github.com/BrainFM/brainfm">https://github.com/BrainFM/brainfm</a> æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.03014v1">PDF</a> Preliminary work; results ongoing</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é€‚ç”¨äºä¸åŒç»„åˆæˆåƒåºåˆ—çš„è„‘MRIåŸºç¡€æ¨¡å‹ã€‚è¯¥æ¨¡å‹é‡‡ç”¨å¸¦æœ‰å¯å­¦ä¹ æ¨¡æ€åµŒå…¥ã€æ¡ä»¶å±‚å½’ä¸€åŒ–å’Œæ©ç è‡ªåŠ¨ç¼–ç ç›®æ ‡çš„ç¼–ç å™¨ï¼Œè§£å†³äº†ç¼ºå¤±æ¨¡æ€çš„é—®é¢˜ã€‚é€šè¿‡åº”ç”¨æ–¹å·®åæ–¹å·®æ­£åˆ™åŒ–å™¨ï¼Œç¨³å®šç‰¹å¾å­¦ä¹ å¹¶æ”¹å–„è¡¨ç¤ºå¤šæ ·æ€§ã€‚è¯¥è®¾è®¡æ— éœ€ä¸ºæ¯ä¸ªæ¨¡æ€å»ºç«‹å•ç‹¬æ¨¡å‹ï¼Œå¯åœ¨æŸäº›åºåˆ—ç¼ºå¤±æˆ–æœªè§æ—¶è‡ªé€‚åº”ç½‘ç»œã€‚è¯¥æ¨¡å‹åœ¨çº¦6ä¸‡ä¸ªå¤šä¸­å¿ƒMRIä¸Šè¿›è¡Œäº†è‡ªæˆ‘ç›‘ç£é‡å»ºå’Œæ¨¡æ€æ’å€¼è®­ç»ƒï¼Œä»¥å­¦ä¹ çµæ´»è¡¨ç¤ºã€‚åˆæ­¥ç»“æœè¯æ˜äº†è¯¥æ–¹æ³•çš„å¯è¡Œæ€§ï¼Œå¹¶è®¡åˆ’è¿›è¡Œæ›´å¤šå®éªŒä»¥ç ”ç©¶å…¶æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¨¡å‹é€‚ç”¨äºå¤šç§æˆåƒåºåˆ—çš„è„‘MRIåˆ†æã€‚</li>
<li>æ¨¡å‹é‡‡ç”¨å¸¦æœ‰å¯å­¦ä¹ æ¨¡æ€åµŒå…¥çš„ç¼–ç å™¨ï¼Œé€‚åº”ä¸åŒè¾“å…¥ã€‚</li>
<li>æ¡ä»¶å±‚å½’ä¸€åŒ–å’Œæ©ç è‡ªåŠ¨ç¼–ç ç›®æ ‡å¤„ç†ç¼ºå¤±æ¨¡æ€é—®é¢˜ã€‚</li>
<li>åº”ç”¨æ–¹å·®-åæ–¹å·®æ­£åˆ™åŒ–å™¨ç¨³å®šç‰¹å¾å­¦ä¹ å¹¶æ”¹å–„è¡¨ç¤ºå¤šæ ·æ€§ã€‚</li>
<li>æ¨¡å‹åœ¨å¤§é‡è„‘MRIæ•°æ®ä¸Šè¿›è¡Œè‡ªæˆ‘ç›‘ç£é‡å»ºå’Œæ¨¡æ€æ’å€¼è®­ç»ƒã€‚</li>
<li>åˆæ­¥ç»“æœè¯æ˜äº†æ¨¡å‹çš„å¯è¡Œæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.03014">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a3b29ecf15069c1c263dac6e013126c1" align="middle">
<img src="https://picx.zhimg.com/v2-58a1eaf22192a4c1bfd1ac96f748c876" align="middle">
<img src="https://picx.zhimg.com/v2-2eb0d2eb72370aecfc5d2c98945de2ae" align="middle">
<img src="https://picx.zhimg.com/v2-e434e9e4b7a512e2f18ecabe31435555" align="middle">
<img src="https://picx.zhimg.com/v2-68373a288a86f86263a08d152d14fe85" align="middle">
<img src="https://picx.zhimg.com/v2-c33b7af0c5ed0beb939f860d1c55a1d5" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="SCALE-VLP-Soft-Weighted-Contrastive-Volumetric-Vision-Language-Pre-training-with-Spatial-Knowledge-Semantics"><a href="#SCALE-VLP-Soft-Weighted-Contrastive-Volumetric-Vision-Language-Pre-training-with-Spatial-Knowledge-Semantics" class="headerlink" title="SCALE-VLP: Soft-Weighted Contrastive Volumetric Vision-Language   Pre-training with Spatial-Knowledge Semantics"></a>SCALE-VLP: Soft-Weighted Contrastive Volumetric Vision-Language   Pre-training with Spatial-Knowledge Semantics</h2><p><strong>Authors:Ailar Mahdizadeh, Puria Azadi Moghadam, Xiangteng He, Shahriar Mirabbasi, Panos Nasiopoulos, Leonid Sigal</strong></p>
<p>Vision-language models (VLMs) have demonstrated strong cross-modal capabilities, yet most work remains limited to 2D data and assumes binary supervision (i.e., positive vs. negative pairs), overlooking the continuous and structured dependencies present in volumetric data such as CT. Existing approaches often treat volumetric scans as independent 2D slices, compromising spatial coherence and underutilizing rich clinical semantics. We propose SCALE-VLP, a soft-weighted contrastive vision-language pre-training framework that integrates (i) volumetric spatial semantics to preserve anatomical structure and (ii) domain-aware, knowledge-infused semantics (e.g., radiological ontologies) to guide alignment. This yields structurally consistent and semantically grounded representations under limited supervision, demonstrating strong cross-task transferability (retrieval, report generation, and classification), and cross-domain generalizability with consistent gains without further fine-tuning. In particular, compared to the previous state of the art, SCALE-VLP achieves up to 4.3x higher top-1 CT-report retrieval, improves abnormality classification by 10 points, and reaches ROUGE-L 0.44 and BERT-F1 0.89 for report generation. Further, in zero-shot evaluation on an out-of-domain external dataset, we observe consistent gains, indicating the cross-task and cross-domain generalization ability of SCALE-VLP. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å·²ç»å±•ç°å‡ºå¼ºå¤§çš„è·¨æ¨¡æ€èƒ½åŠ›ï¼Œä½†å¤§å¤šæ•°å·¥ä½œä»ç„¶å±€é™äº2Dæ•°æ®ï¼Œå¹¶å‡è®¾äºŒå…ƒç›‘ç£ï¼ˆå³æ­£é¢ä¸è´Ÿé¢é…å¯¹ï¼‰ï¼Œå¿½è§†äº†å­˜åœ¨äºCTç­‰ä½“ç§¯æ•°æ®ä¸­çš„è¿ç»­å’Œç»“æ„åŒ–ä¾èµ–å…³ç³»ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸å°†ä½“ç§¯æ‰«æè§†ä¸ºç‹¬ç«‹çš„2Dåˆ‡ç‰‡ï¼Œè¿™æŸå®³äº†ç©ºé—´è¿è´¯æ€§å¹¶æœªèƒ½å……åˆ†åˆ©ç”¨ä¸°å¯Œçš„ä¸´åºŠè¯­ä¹‰ã€‚æˆ‘ä»¬æå‡ºäº†SCALE-VLPï¼Œè¿™æ˜¯ä¸€ç§è½¯åŠ æƒå¯¹æ¯”è§†è§‰è¯­è¨€é¢„è®­ç»ƒæ¡†æ¶ï¼Œå®ƒé›†æˆäº†ï¼ˆiï¼‰ä½“ç§¯ç©ºé—´è¯­ä¹‰ä»¥ä¿ç•™è§£å‰–ç»“æ„ï¼Œä»¥åŠï¼ˆiiï¼‰é¢†åŸŸæ„ŸçŸ¥ã€çŸ¥è¯†æ³¨å…¥è¯­ä¹‰ï¼ˆä¾‹å¦‚ï¼Œæ”¾å°„å­¦æœ¬ä½“ï¼‰ä»¥æŒ‡å¯¼å¯¹é½ã€‚è¿™åœ¨æœ‰é™ç›‘ç£ä¸‹äº§ç”Ÿäº†ç»“æ„ä¸€è‡´ä¸”è¯­ä¹‰ä¸°å¯Œçš„è¡¨ç¤ºï¼Œæ˜¾ç¤ºå‡ºå¼ºå¤§çš„è·¨ä»»åŠ¡å¯è¿ç§»æ€§ï¼ˆæ£€ç´¢ã€æŠ¥å‘Šç”Ÿæˆå’Œåˆ†ç±»ï¼‰ï¼Œä»¥åŠè·¨åŸŸæ³›åŒ–èƒ½åŠ›ï¼Œæ— éœ€è¿›ä¸€æ­¥å¾®è°ƒå³å¯å¸¦æ¥ä¸€è‡´æ€§çš„æ”¶ç›Šã€‚ç‰¹åˆ«æ˜¯ä¸ä»¥å‰çš„æœ€å…ˆè¿›æ°´å¹³ç›¸æ¯”ï¼ŒSCALE-VLPåœ¨CTæŠ¥å‘Šæ£€ç´¢æ–¹é¢è¾¾åˆ°äº†é«˜è¾¾4.3å€çš„top-1å‡†ç¡®ç‡ï¼Œå¼‚å¸¸åˆ†ç±»æé«˜äº†10ä¸ªç‚¹ï¼ŒæŠ¥å‘Šç”Ÿæˆçš„ROUGE-Lè¾¾åˆ°0.44ï¼ŒBERT-F1è¾¾åˆ°0.89ã€‚æ­¤å¤–ï¼Œåœ¨åŸŸå¤–å¤–éƒ¨æ•°æ®é›†ä¸Šè¿›è¡Œé›¶æ ·æœ¬è¯„ä¼°æ—¶ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°äº†ä¸€è‡´çš„æ”¶ç›Šï¼Œè¿™è¡¨æ˜SCALE-VLPçš„è·¨ä»»åŠ¡å’Œè·¨åŸŸæ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.02996v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†SCALE-VLPæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†ä½“ç§¯ç©ºé—´è¯­ä¹‰å’Œé¢†åŸŸçŸ¥è¯†è¯­ä¹‰ï¼Œç”¨äºåœ¨æœ‰é™çš„ç›‘ç£ä¸‹å®ç°ç»“æ„ä¸€è‡´ä¸”è¯­ä¹‰ä¸°å¯Œçš„è¡¨ç¤ºã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿä¿ç•™è§£å‰–ç»“æ„å¹¶å¼•å¯¼å¯¹é½ï¼Œå®ç°äº†è·¨ä»»åŠ¡å’Œè·¨é¢†åŸŸçš„è‰¯å¥½è¡¨ç°ã€‚ç›¸è¾ƒäºå…ˆå‰çš„ç ”ç©¶ï¼ŒSCALE-VLPåœ¨CTæŠ¥å‘Šæ£€ç´¢ã€å¼‚å¸¸åˆ†ç±»ç­‰æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLMsè™½ç„¶å…·æœ‰å¼ºå¤§çš„è·¨æ¨¡æ€èƒ½åŠ›ï¼Œä½†å¤§å¤šæ•°å·¥ä½œä»ç„¶å±€é™äº2Dæ•°æ®ï¼Œå¹¶å‡è®¾äºŒå…ƒç›‘ç£ï¼Œå¿½ç•¥äº†å­˜åœ¨äºä½“ç§¯æ•°æ®ï¼ˆå¦‚CTï¼‰ä¸­çš„è¿ç»­å’Œç»“æ„åŒ–ä¾èµ–å…³ç³»ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¾€å¾€å°†ä½“ç§¯æ‰«æè§†ä¸ºç‹¬ç«‹çš„2Dåˆ‡ç‰‡ï¼Œè¿™æŸå®³äº†ç©ºé—´è¿è´¯æ€§å¹¶å¯¼è‡´ä¸°å¯Œçš„ä¸´åºŠè¯­ä¹‰æœªèƒ½å……åˆ†åˆ©ç”¨ã€‚</li>
<li>SCALE-VLPæ˜¯ä¸€ä¸ªè½¯åŠ æƒå¯¹æ¯”è§†è§‰è¯­è¨€é¢„è®­ç»ƒæ¡†æ¶ï¼Œå®ƒæ•´åˆäº†ä½“ç§¯ç©ºé—´è¯­ä¹‰å’Œé¢†åŸŸçŸ¥è¯†è¯­ä¹‰ã€‚</li>
<li>SCALE-VLPèƒ½å¤Ÿä¿ç•™è§£å‰–ç»“æ„å¹¶å¼•å¯¼å¯¹é½ï¼Œä»è€Œå®ç°ç»“æ„ä¸€è‡´ä¸”è¯­ä¹‰ä¸°å¯Œçš„è¡¨ç¤ºã€‚</li>
<li>SCALE-VLPåœ¨è·¨ä»»åŠ¡å’Œè·¨é¢†åŸŸçš„è¡¨ç°ä¸­å‡è¡¨ç°å‡ºè‰²ï¼Œç‰¹åˆ«æ˜¯åœ¨CTæŠ¥å‘Šæ£€ç´¢å’Œå¼‚å¸¸åˆ†ç±»æ–¹é¢ã€‚</li>
<li>SCALE-VLPç›¸è¾ƒäºå…ˆå‰çš„ç ”ç©¶å–å¾—äº†æ˜¾è‘—çš„æå‡ï¼Œå¦‚åœ¨CTæŠ¥å‘Šæ£€ç´¢ä¸Šè¾¾åˆ°äº†4.3å€çš„æå‡ï¼Œå¼‚å¸¸åˆ†ç±»æé«˜äº†10ä¸ªç‚¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.02996">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-71982e0ca6d4b13201d5cd19e381421e" align="middle">
<img src="https://picx.zhimg.com/v2-bcc0bdd8762abf73b64c632f916769d3" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Domain-Adaptive-Transformer-for-Data-Efficient-Glioma-Segmentation-in-Sub-Saharan-MRI"><a href="#Domain-Adaptive-Transformer-for-Data-Efficient-Glioma-Segmentation-in-Sub-Saharan-MRI" class="headerlink" title="Domain-Adaptive Transformer for Data-Efficient Glioma Segmentation in   Sub-Saharan MRI"></a>Domain-Adaptive Transformer for Data-Efficient Glioma Segmentation in   Sub-Saharan MRI</h2><p><strong>Authors:Ilerioluwakiiye Abolade, Aniekan Udo, Augustine Ojo, Abdulbasit Oyetunji, Hammed Ajigbotosho, Aondana Iorumbur, Confidence Raymond, Maruf Adewole</strong></p>
<p>Glioma segmentation is critical for diagnosis and treatment planning, yet remains challenging in Sub-Saharan Africa due to limited MRI infrastructure and heterogeneous acquisition protocols that induce severe domain shift. We propose SegFormer3D-plus, a radiomics-guided transformer architecture designed for robust segmentation under domain variability. Our method combines: (1) histogram matching for intensity harmonization across scanners, (2) radiomic feature extraction with PCA-reduced k-means for domain-aware stratified sampling, (3) a dual-pathway encoder with frequency-aware feature extraction and spatial-channel attention, and (4) composite Dice-Cross-Entropy loss for boundary refinement. Pretrained on BraTS 2023 and fine-tuned on BraTS-Africa data, SegFormer3D-plus demonstrates improved tumor subregion delineation and boundary localization across heterogeneous African clinical scans, highlighting the value of radiomics-guided domain adaptation for resource-limited settings. </p>
<blockquote>
<p>èƒ¶è´¨ç˜¤åˆ†å‰²å¯¹äºè¯Šæ–­å’Œæ²»ç–—è®¡åˆ’è‡³å…³é‡è¦ï¼Œç„¶è€Œåœ¨æ’’å“ˆæ‹‰ä»¥å—éæ´²ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå› ä¸ºæœ‰é™çš„MRIåŸºç¡€è®¾æ–½å’Œä¸åŒçš„é‡‡é›†åè®®å¯¼è‡´äº†ä¸¥é‡çš„é¢†åŸŸåç§»ã€‚æˆ‘ä»¬æå‡ºäº†SegFormer3D-plusï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ”¾å°„å­¦æŒ‡å¯¼çš„è½¬æ¢å™¨æ¶æ„ï¼Œæ—¨åœ¨åœ¨é¢†åŸŸå˜åŒ–ä¸‹è¿›è¡Œç¨³å¥çš„åˆ†å‰²ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†ï¼šï¼ˆ1ï¼‰ç›´æ–¹å›¾åŒ¹é…ï¼Œç”¨äºæ‰«æä»ªä¹‹é—´çš„å¼ºåº¦åè°ƒï¼Œï¼ˆ2ï¼‰ä½¿ç”¨PCAé™ç»´çš„kå‡å€¼è¿›è¡Œæ”¾å°„å­¦ç‰¹å¾æå–ï¼Œä»¥å®ç°é¢†åŸŸæ„ŸçŸ¥åˆ†å±‚é‡‡æ ·ï¼Œï¼ˆ3ï¼‰å…·æœ‰é¢‘ç‡æ„ŸçŸ¥ç‰¹å¾æå–å’Œç©ºé—´é€šé“æ³¨æ„åŠ›çš„åŒè·¯å¾„ç¼–ç å™¨ï¼Œï¼ˆ4ï¼‰å¤åˆDice-Cross-EntropyæŸå¤±ä»¥ä¼˜åŒ–è¾¹ç•Œã€‚åœ¨BraTS 2023ä¸Šè¿›è¡Œé¢„è®­ç»ƒå¹¶åœ¨BraTS-Africaæ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒåï¼ŒSegFormer3D-pluså±•ç¤ºäº†åœ¨éæ´²çš„ä¸´åºŠæ‰«æå›¾åƒä¸­æ”¹å–„è‚¿ç˜¤äºšåŒºåŸŸåˆ†å‰²å’Œè¾¹ç•Œå®šä½çš„èƒ½åŠ›ï¼Œè¿™çªå‡ºäº†åœ¨èµ„æºå—é™ç¯å¢ƒä¸­æ”¾å°„å­¦æŒ‡å¯¼çš„é¢†åŸŸé€‚åº”çš„ä»·å€¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.02928v1">PDF</a> 4 pages, 2 figures. Accepted as an abstract at the Women in Machine   Learning (WiML) Workshop at NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºSegFormer3D-plusçš„æ”¾å°„ç»„å­¦æŒ‡å¯¼çš„å˜æ¢å™¨æ¶æ„ï¼Œç”¨äºåœ¨åŸŸå˜åŒ–ä¸‹è¿›è¡Œç¨³å¥çš„èƒ¶è´¨ç˜¤åˆ†å‰²ã€‚è¯¥æ–¹æ³•ç»“åˆäº†å¼ºåº¦ç›´æ–¹å›¾åŒ¹é…ã€åŸºäºPCAé™ç»´çš„k-meansæ”¾å°„ç‰¹å¾æå–ã€åŒé€šè·¯ç¼–ç å™¨ä¸å¤åˆDice-Cross-EntropyæŸå¤±å‡½æ•°ç­‰æ–¹æ³•ï¼Œä»¥æ”¹å–„è‚¿ç˜¤äºšåŒºåŸŸåˆ†å‰²å’Œè¾¹ç•Œå®šä½ã€‚SegFormer3D-plusåœ¨éæ´²å¼‚è´¨ä¸´åºŠæ‰«ææ•°æ®ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå‡¸æ˜¾äº†æ”¾å°„ç»„å­¦æŒ‡å¯¼çš„åŸŸé€‚åº”åœ¨èµ„æºæœ‰é™ç¯å¢ƒä¸­çš„ä»·å€¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>èƒ¶è´¨ç˜¤åˆ†å‰²å¯¹äºè¯Šæ–­å’Œæ²»ç–—è®¡åˆ’è‡³å…³é‡è¦ï¼Œä½†åœ¨æ’’å“ˆæ‹‰ä»¥å—éæ´²åœ°åŒºä»å…·æŒ‘æˆ˜ï¼Œå—é™äºMRIåŸºç¡€è®¾æ–½å’Œå¼‚è´¨é‡‡é›†åè®®å¯¼è‡´çš„åŸŸåç§»ã€‚</li>
<li>SegFormer3D-plusæ˜¯ä¸€ç§é’ˆå¯¹åŸŸå˜åŒ–è®¾è®¡çš„ç¨³å¥åˆ†å‰²æ–¹æ³•ï¼Œé‡‡ç”¨æ”¾å°„ç»„å­¦æŒ‡å¯¼çš„å˜æ¢å™¨æ¶æ„ã€‚</li>
<li>SegFormer3D-plusç»“åˆäº†å¼ºåº¦ç›´æ–¹å›¾åŒ¹é…ä»¥åè°ƒä¸åŒæ‰«æä»ªé—´çš„å·®å¼‚ã€‚</li>
<li>ä½¿ç”¨PCAé™ç»´çš„k-meansæ–¹æ³•è¿›è¡Œæ”¾å°„ç‰¹å¾æå–ï¼Œå®ç°åŸŸæ„ŸçŸ¥åˆ†å±‚é‡‡æ ·ã€‚</li>
<li>åŒé€šè·¯ç¼–ç å™¨å…·å¤‡é¢‘ç‡æ„ŸçŸ¥ç‰¹å¾æå–å’Œæ—¶ç©ºé€šé“æ³¨æ„åŠ›æœºåˆ¶ã€‚</li>
<li>é‡‡ç”¨å¤åˆDice-CrossEntropyæŸå¤±å‡½æ•°ä»¥æ”¹å–„è¾¹ç•Œç»†åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.02928">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f3c09a4b66420b1d8ea57065c8132948" align="middle">
<img src="https://picx.zhimg.com/v2-cd91be848bef948de30a58bda73af12d" align="middle">
<img src="https://picx.zhimg.com/v2-aee844dc2db762cef7e1c6910d1c59fb" align="middle">
<img src="https://picx.zhimg.com/v2-f3c338ec148d22c8a5a63bfcc47d075b" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Med-Banana-50K-A-Cross-modality-Large-Scale-Dataset-for-Text-guided-Medical-Image-Editing"><a href="#Med-Banana-50K-A-Cross-modality-Large-Scale-Dataset-for-Text-guided-Medical-Image-Editing" class="headerlink" title="Med-Banana-50K: A Cross-modality Large-Scale Dataset for Text-guided   Medical Image Editing"></a>Med-Banana-50K: A Cross-modality Large-Scale Dataset for Text-guided   Medical Image Editing</h2><p><strong>Authors:Zhihui Chen, Mengling Feng</strong></p>
<p>Recent advances in multimodal large language models have enabled remarkable medical image editing capabilities. However, the research communityâ€™s progress remains constrained by the absence of large-scale, high-quality, and openly accessible datasets built specifically for medical image editing with strict anatomical and clinical constraints. We introduce Med-Banana-50K, a comprehensive 50K-image dataset for instruction-based medical image editing spanning three modalities (chest X-ray, brain MRI, fundus photography) and 23 disease types. Our dataset is constructed by leveraging Gemini-2.5-Flash-Image to generate bidirectional edits (lesion addition and removal) from real medical images. What distinguishes Med-Banana-50K from general-domain editing datasets is our systematic approach to medical quality control: we employ LLM-as-Judge with a medically grounded rubric (instruction compliance, structural plausibility, realism, and fidelity preservation) and history-aware iterative refinement up to five rounds. Beyond single-turn editing, Med-Banana-50K includes 37K failed attempts with full conversation logs for preference learning and alignment research. By providing this large-scale, medically validated, and fully documented resource, Med-Banana-50K establishes a foundation for training and evaluating the next generation of medical image editing models.Our dataset and code are publicly available at [<a target="_blank" rel="noopener" href="https://github.com/richardChenzhihui/med-banana-50k]">https://github.com/richardChenzhihui/med-banana-50k]</a>. </p>
<blockquote>
<p>è¿‘æœŸå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„è¿›å±•ä¸ºåŒ»å­¦å›¾åƒç¼–è¾‘æä¾›äº†æ˜¾è‘—çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç ”ç©¶ç¤¾åŒºçš„è¿›å±•ä»ç„¶å—åˆ°ç¼ºä¹å¤§è§„æ¨¡ã€é«˜è´¨é‡ã€å…¬å¼€å¯è®¿é—®çš„ä¸“é—¨ç”¨äºåŒ»å­¦å›¾åƒç¼–è¾‘æ•°æ®é›†çš„åˆ¶çº¦ï¼Œè¿™äº›æ•°æ®é›†éœ€è¦ä¸¥æ ¼çš„è§£å‰–å’Œä¸´åºŠçº¦æŸã€‚æˆ‘ä»¬ä»‹ç»äº†Med-Banana-50Kï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºæŒ‡ä»¤çš„åŒ»å­¦å›¾åƒç¼–è¾‘çš„ç»¼åˆæ€§5ä¸‡å¼ å›¾åƒæ•°æ®é›†ï¼Œæ¶µç›–ä¸‰ç§æ¨¡æ€ï¼ˆèƒ¸éƒ¨Xå…‰ã€è„‘éƒ¨MRIã€çœ¼åº•æ‘„å½±ï¼‰å’Œ23ç§ç–¾ç—…ç±»å‹ã€‚æˆ‘ä»¬çš„æ•°æ®é›†é€šè¿‡åˆ©ç”¨Gemini-2.5-Flash-Imageç”ŸæˆçœŸå®åŒ»å­¦å›¾åƒçš„åŒå‘ç¼–è¾‘ï¼ˆç—…ç¶å¢åŠ å’Œå»é™¤ï¼‰æ¥æ„å»ºã€‚Med-Banana-50Kä¸ä¸€èˆ¬é¢†åŸŸç¼–è¾‘æ•°æ®é›†çš„åŒºåˆ«åœ¨äºæˆ‘ä»¬çš„åŒ»ç–—è´¨é‡æ§åˆ¶ç³»ç»Ÿæ–¹æ³•ï¼šæˆ‘ä»¬é‡‡ç”¨LLM-as-Judgeï¼Œä½¿ç”¨ä»¥åŒ»å­¦ä¸ºåŸºç¡€çš„è¯„åˆ†ç³»ç»Ÿï¼ˆæŒ‡ä»¤åˆè§„æ€§ã€ç»“æ„å¯è¡Œæ€§ã€ç°å®æ€§å’Œä¿çœŸæ€§ä¿ç•™ï¼‰ï¼Œå¹¶è¿›è¡Œæœ€å¤šäº”è½®çš„å†å²æ„ŸçŸ¥è¿­ä»£æ”¹è¿›ã€‚é™¤äº†å•å›åˆç¼–è¾‘å¤–ï¼ŒMed-Banana-50Kè¿˜åŒ…æ‹¬åŒ…å«å®Œæ•´å¯¹è¯æ—¥å¿—çš„3.7ä¸‡æ¬¡å¤±è´¥å°è¯•è®°å½•ï¼Œç”¨äºåå¥½å­¦ä¹ å’Œå¯¹é½ç ”ç©¶ã€‚é€šè¿‡æä¾›å¤§è§„æ¨¡ã€åŒ»å­¦éªŒè¯å’Œå®Œæ•´è®°å½•çš„è¿™ä¸€èµ„æºï¼ŒMed-Banana-50Kä¸ºåŸ¹è®­å’Œè¯„ä¼°ä¸‹ä¸€ä»£åŒ»å­¦å›¾åƒç¼–è¾‘æ¨¡å‹å¥ å®šäº†åŸºç¡€ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œä»£ç å¯åœ¨[<a target="_blank" rel="noopener" href="https://github.com/richardChenzhihui/med-banana-50k]%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/richardChenzhihui/med-banana-50k]å…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.00801v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŒ»å­¦å›¾åƒç¼–è¾‘æ•°æ®é›†Med-Banana-50Kè¢«æ¨å‡ºï¼ŒåŒ…å«5ä¸‡å¼ è·¨ä¸‰ç§æ¨¡æ€ï¼ˆèƒ¸éƒ¨Xå…‰ã€è„‘éƒ¨MRIã€çœ¼åº•æ‘„å½±ï¼‰å’Œ23ç§ç–¾ç—…ç±»å‹çš„æŒ‡ä»¤å‹åŒ»å­¦å›¾åƒç¼–è¾‘æ•°æ®ã€‚è¯¥æ•°æ®é›†é€šè¿‡Gemini-2.5-Flash-Imageç”ŸæˆåŒå‘ç¼–è¾‘ï¼ˆç—…å˜å¢åŠ å’Œç§»é™¤ï¼‰çš„çœŸå®åŒ»å­¦å›¾åƒï¼Œå¹¶é‡‡ç”¨LLM-as-Judgeè¿›è¡ŒåŒ»å­¦è´¨é‡æ§åˆ¶ã€‚æ­¤å¤–ï¼Œè¿˜åŒ…æ‹¬3.7ä¸‡æ¬¡å¤±è´¥çš„å°è¯•åŠå®Œæ•´å¯¹è¯æ—¥å¿—ï¼Œæœ‰åˆ©äºåå¥½å­¦ä¹ å’Œå¯¹é½ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Med-Banana-50Kæ˜¯ä¸€ä¸ªå¤§å‹åŒ»å­¦å›¾åƒç¼–è¾‘æ•°æ®é›†ï¼ŒåŒ…å«5ä¸‡å¼ å›¾åƒã€‚</li>
<li>æ•°æ®é›†æ¶µç›–ä¸‰ç§æ¨¡æ€å’Œ23ç§ç–¾ç—…ç±»å‹ã€‚</li>
<li>é€šè¿‡Gemini-2.5-Flash-Imageç”ŸæˆçœŸå®åŒ»å­¦å›¾åƒçš„åŒå‘ç¼–è¾‘ã€‚</li>
<li>é‡‡ç”¨LLM-as-Judgeè¿›è¡ŒåŒ»å­¦è´¨é‡æ§åˆ¶ï¼ŒåŒ…æ‹¬æŒ‡ä»¤éµå®ˆã€ç»“æ„åˆç†æ€§ã€çœŸå®æ€§å’Œä¿çœŸåº¦ä¿ç•™ã€‚</li>
<li>æ•°æ®é›†åŒ…å«3.7ä¸‡æ¬¡çš„å¤±è´¥å°è¯•åŠå®Œæ•´å¯¹è¯æ—¥å¿—ã€‚</li>
<li>Med-Banana-50Kä¸ºè®­ç»ƒå’Œè¯„ä»·ä¸‹ä¸€ä»£åŒ»å­¦å›¾åƒç¼–è¾‘æ¨¡å‹æä¾›äº†åŸºç¡€ã€‚</li>
<li>æ•°æ®é›†å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/richardChenzhihui/med-banana-50k%E5%85%AC%E5%BC%80%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/richardChenzhihui/med-banana-50kå…¬å¼€è·å–ã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.00801">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ada39152dbe44ef7eb11114f036cf0fe" align="middle">
<img src="https://picx.zhimg.com/v2-41b02bfe2fad8e6ee46ce588d0d48b60" align="middle">
<img src="https://picx.zhimg.com/v2-e9a1fa0899be83216ffa88d661660a64" align="middle">
<img src="https://picx.zhimg.com/v2-9a8f62ea18bb7d0f2ab4b261dbda832a" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="BALR-SAM-Boundary-Aware-Low-Rank-Adaptation-of-SAM-for-Resource-Efficient-Medical-Image-Segmentation"><a href="#BALR-SAM-Boundary-Aware-Low-Rank-Adaptation-of-SAM-for-Resource-Efficient-Medical-Image-Segmentation" class="headerlink" title="BALR-SAM: Boundary-Aware Low-Rank Adaptation of SAM for   Resource-Efficient Medical Image Segmentation"></a>BALR-SAM: Boundary-Aware Low-Rank Adaptation of SAM for   Resource-Efficient Medical Image Segmentation</h2><p><strong>Authors:Zelin Liu, Sicheng Dong, Bocheng Li, Yixuan Yang, Jiacheng Ruan, Chenxu Zhou, Suncheng Xiang</strong></p>
<p>Vision foundation models like the Segment Anything Model (SAM), pretrained on large-scale natural image datasets, often struggle in medical image segmentation due to a lack of domain-specific adaptation. In clinical practice, fine-tuning such models efficiently for medical downstream tasks with minimal resource demands, while maintaining strong performance, is challenging. To address these issues, we propose BALR-SAM, a boundary-aware low-rank adaptation framework that enhances SAM for medical imaging. It combines three tailored components: (1) a Complementary Detail Enhancement Network (CDEN) using depthwise separable convolutions and multi-scale fusion to capture boundary-sensitive features essential for accurate segmentation; (2) low-rank adapters integrated into SAMâ€™s Vision Transformer blocks to optimize feature representation and attention for medical contexts, while simultaneously significantly reducing the parameter space; and (3) a low-rank tensor attention mechanism in the mask decoder, cutting memory usage by 75% and boosting inference speed. Experiments on standard medical segmentation datasets show that BALR-SAM, without requiring prompts, outperforms several state-of-the-art (SOTA) methods, including fully fine-tuned MedSAM, while updating just 1.8% (11.7M) of its parameters. </p>
<blockquote>
<p>åƒSegment Anything Modelï¼ˆSAMï¼‰è¿™æ ·çš„è§†è§‰åŸºç¡€æ¨¡å‹ï¼Œåœ¨å¤§è§„æ¨¡è‡ªç„¶å›¾åƒæ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå¾€å¾€ç”±äºç¼ºå°‘ç‰¹å®šé¢†åŸŸçš„é€‚åº”æ€§è€Œåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­è¡¨ç°æŒ£æ‰ã€‚åœ¨ä¸´åºŠå®è·µä¸­ï¼Œä»¥æœ€å°çš„èµ„æºéœ€æ±‚é«˜æ•ˆåœ°å¯¹è¿™ç±»æ¨¡å‹è¿›è¡ŒåŒ»å­¦ä¸‹æ¸¸ä»»åŠ¡çš„å¾®è°ƒï¼ŒåŒæ—¶ä¿æŒå¼ºå¤§çš„æ€§èƒ½ï¼Œæ˜¯ä¸€é¡¹æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†BALR-SAMï¼Œè¿™æ˜¯ä¸€ä¸ªè¾¹ç•Œæ„ŸçŸ¥çš„ä½ç§©é€‚åº”æ¡†æ¶ï¼Œç”¨äºå¢å¼ºSAMåœ¨åŒ»å­¦æˆåƒä¸­çš„åº”ç”¨ã€‚å®ƒç»“åˆäº†ä¸‰ä¸ªå®šåˆ¶ç»„ä»¶ï¼šï¼ˆ1ï¼‰ä½¿ç”¨æ·±åº¦å¯åˆ†ç¦»å·ç§¯å’Œå¤šå°ºåº¦èåˆçš„äº’è¡¥ç»†èŠ‚å¢å¼ºç½‘ç»œï¼ˆCDENï¼‰ï¼Œä»¥æ•è·å¯¹å‡†ç¡®åˆ†å‰²è‡³å…³é‡è¦çš„è¾¹ç•Œæ•æ„Ÿç‰¹å¾ï¼›ï¼ˆ2ï¼‰å°†ä½ç§©é€‚é…å™¨é›†æˆåˆ°SAMçš„è§†è§‰è½¬æ¢å™¨å—ä¸­ï¼Œä»¥ä¼˜åŒ–åŒ»ç–—ç¯å¢ƒä¸‹çš„ç‰¹å¾è¡¨ç¤ºå’Œæ³¨æ„åŠ›ï¼ŒåŒæ—¶æ˜¾è‘—å‡å°‘å‚æ•°ç©ºé—´ï¼›ï¼ˆ3ï¼‰åœ¨æ©è†œè§£ç å™¨ä¸­ä½¿ç”¨ä½ç§©å¼ é‡æ³¨æ„åŠ›æœºåˆ¶ï¼Œå‡å°‘75%çš„å†…å­˜ä½¿ç”¨ï¼Œå¹¶æé«˜æ¨ç†é€Ÿåº¦ã€‚åœ¨æ ‡å‡†åŒ»å­¦åˆ†å‰²æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒBALR-SAMæ— éœ€æç¤ºå³å¯è¶…è¶Šå¤šç§æœ€æ–°æŠ€æœ¯æ–¹æ³•ï¼ŒåŒ…æ‹¬å®Œå…¨å¾®è°ƒè¿‡çš„MedSAMï¼ŒåŒæ—¶ä»…æ›´æ–°å…¶1.8%ï¼ˆ11.7Mï¼‰çš„å‚æ•°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24204v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è‡ªç„¶å›¾åƒæ•°æ®é›†é¢„è®­ç»ƒçš„Segment Anything Modelï¼ˆSAMï¼‰åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹é¢å­˜åœ¨é¢†åŸŸç‰¹å®šé€‚åº”æ€§é—®é¢˜ã€‚ä¸ºè§£å†³æ­¤æŒ‘æˆ˜ï¼Œæå‡ºBALR-SAMï¼Œä¸€ä¸ªè¾¹ç•Œæ„ŸçŸ¥çš„ä½ç§©é€‚åº”æ¡†æ¶ï¼Œå¢å¼ºSAMåœ¨åŒ»å­¦æˆåƒä¸­çš„æ€§èƒ½ã€‚åŒ…å«ä¸‰ä¸ªå®šåˆ¶ç»„ä»¶ï¼šCDENæ•æ‰è¾¹ç•Œæ•æ„Ÿç‰¹å¾ï¼Œä½ç§©é€‚é…å™¨ä¼˜åŒ–ç‰¹å¾è¡¨ç¤ºå’Œæ³¨æ„åŠ›ï¼Œä½ç§©å¼ é‡æ³¨æ„åŠ›æœºåˆ¶å‡å°‘å†…å­˜ä½¿ç”¨å¹¶åŠ å¿«æ¨ç†é€Ÿåº¦ã€‚åœ¨æ ‡å‡†åŒ»å­¦åˆ†å‰²æ•°æ®é›†ä¸Šçš„å®éªŒæ˜¾ç¤ºï¼ŒBALR-SAMä»…æ›´æ–°å°‘é‡å‚æ•°å³å¯è¶…è¶Šå…ˆè¿›æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>é¢„è®­ç»ƒçš„åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹é¢ä¸´é¢†åŸŸç‰¹å®šé€‚åº”æ€§æŒ‘æˆ˜ã€‚</li>
<li>BALR-SAMæ˜¯ä¸€ä¸ªå¢å¼ºåŒ»å­¦æˆåƒæ€§èƒ½çš„è¾¹ç•Œæ„ŸçŸ¥ä½ç§©é€‚åº”æ¡†æ¶ã€‚</li>
<li>CDENä½¿ç”¨æ·±åº¦å¯åˆ†ç¦»å·ç§¯å’Œå¤šå°ºåº¦èåˆæ¥æ•æ‰è¾¹ç•Œæ•æ„Ÿç‰¹å¾ã€‚</li>
<li>ä½ç§©é€‚é…å™¨ä¼˜åŒ–ç‰¹å¾è¡¨ç¤ºå’Œæ³¨æ„åŠ›ï¼ŒåŒæ—¶æ˜¾è‘—å‡å°‘å‚æ•°ç©ºé—´ã€‚</li>
<li>ä½ç§©å¼ é‡æ³¨æ„åŠ›æœºåˆ¶å‡å°‘å†…å­˜ä½¿ç”¨å¹¶åŠ é€Ÿæ¨ç†é€Ÿåº¦ã€‚</li>
<li>BALR-SAMåœ¨æ ‡å‡†åŒ»å­¦åˆ†å‰²æ•°æ®é›†ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œè¶…è¶Šäº†ä¸€äº›æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24204">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5749b5d9e86043615e77cc35f31caf5b" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Joint-Lossless-Compression-and-Steganography-for-Medical-Images-via-Large-Language-Models"><a href="#Joint-Lossless-Compression-and-Steganography-for-Medical-Images-via-Large-Language-Models" class="headerlink" title="Joint Lossless Compression and Steganography for Medical Images via   Large Language Models"></a>Joint Lossless Compression and Steganography for Medical Images via   Large Language Models</h2><p><strong>Authors:Pengcheng Zheng, Xiaorong Pu, Kecheng Chen, Jiaxin Huang, Meng Yang, Bai Feng, Yazhou Ren, Jianan Jiang, Chaoning Zhang, Yang Yang, Heng Tao Shen</strong></p>
<p>Recently, large language models (LLMs) have driven promising progress in lossless image compression. However, directly adopting existing paradigms for medical images suffers from an unsatisfactory trade-off between compression performance and efficiency. Moreover, existing LLM-based compressors often overlook the security of the compression process, which is critical in modern medical scenarios. To this end, we propose a novel joint lossless compression and steganography framework. Inspired by bit plane slicing (BPS), we find it feasible to securely embed privacy messages into medical images in an invisible manner. Based on this insight, an adaptive modalities decomposition strategy is first devised to partition the entire image into two segments, providing global and local modalities for subsequent dual-path lossless compression. During this dual-path stage, we innovatively propose a segmented message steganography algorithm within the local modality path to ensure the security of the compression process. Coupled with the proposed anatomical priors-based low-rank adaptation (A-LoRA) fine-tuning strategy, extensive experimental results demonstrate the superiority of our proposed method in terms of compression ratios, efficiency, and security. The source code will be made publicly available. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ— æŸå›¾åƒå‹ç¼©æ–¹é¢å–å¾—äº†ä»¤äººç©ç›®çš„è¿›å±•ã€‚ç„¶è€Œï¼Œç›´æ¥é‡‡ç”¨ç°æœ‰çš„åŒ»å­¦å›¾åƒèŒƒä¾‹åœ¨å‹ç¼©æ€§èƒ½å’Œæ•ˆç‡ä¹‹é—´å¾€å¾€é¢ä¸´ä»¤äººä¸æ»¡æ„çš„æƒè¡¡ã€‚æ­¤å¤–ï¼Œç°æœ‰çš„åŸºäºLLMçš„å‹ç¼©å™¨å¾€å¾€å¿½è§†äº†å‹ç¼©è¿‡ç¨‹çš„å®‰å…¨æ€§ï¼Œè¿™åœ¨ç°ä»£åŒ»ç–—åœºæ™¯ä¸­è‡³å…³é‡è¦ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ— æŸå‹ç¼©ä¸éšå†™æœ¯è”åˆæ¡†æ¶ã€‚å—ä½å¹³é¢åˆ‡ç‰‡ï¼ˆBPSï¼‰çš„å¯å‘ï¼Œæˆ‘ä»¬å‘ç°å¯ä»¥å°†éšç§ä¿¡æ¯ä»¥ä¸å¯è§çš„æ–¹å¼åµŒå…¥åŒ»å­¦å›¾åƒä¸­ã€‚åŸºäºæ­¤è§è§£ï¼Œæˆ‘ä»¬é¦–å…ˆè®¾è®¡äº†ä¸€ç§è‡ªé€‚åº”æ¨¡æ€åˆ†è§£ç­–ç•¥ï¼Œå°†æ•´ä¸ªå›¾åƒåˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼Œä¸ºéšåçš„åŒè·¯å¾„æ— æŸå‹ç¼©æä¾›å…¨å±€å’Œå±€éƒ¨æ¨¡æ€ã€‚åœ¨è¿™ä¸€åŒè·¯å¾„é˜¶æ®µï¼Œæˆ‘ä»¬åœ¨å±€éƒ¨æ¨¡æ€è·¯å¾„ä¸­åˆ›æ–°åœ°æå‡ºäº†ä¸€ç§åˆ†æ®µæ¶ˆæ¯éšå†™ç®—æ³•ï¼Œä»¥ç¡®ä¿å‹ç¼©è¿‡ç¨‹çš„å®‰å…¨æ€§ã€‚ç»“åˆæå‡ºçš„åŸºäºè§£å‰–å…ˆéªŒçš„ä½ç§©è‡ªé€‚åº”ï¼ˆA-LoRAï¼‰å¾®è°ƒç­–ç•¥ï¼Œå¤§é‡å®éªŒç»“æœè¯æ˜äº†æˆ‘ä»¬æ–¹æ³•åœ¨å‹ç¼©æ¯”ã€æ•ˆç‡å’Œå®‰å…¨æ€§æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚æºä»£ç å°†å…¬å¼€æä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01782v3">PDF</a> </p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒæ— æŸå‹ç¼©é¢†åŸŸå­˜åœ¨æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥æ»¡è¶³å‹ç¼©æ€§èƒ½ä¸æ•ˆç‡ä¹‹é—´çš„å¹³è¡¡ï¼Œä¸”å¿½è§†å‹ç¼©è¿‡ç¨‹çš„å®‰å…¨æ€§ã€‚ä¸ºæ­¤ï¼Œæå‡ºä¸€ç§æ–°å‹è”åˆæ— æŸå‹ç¼©ä¸éšå†™æ¡†æ¶ï¼Œé‡‡ç”¨ä½å¹³é¢åˆ‡ç‰‡ï¼ˆBPSï¼‰æŠ€æœ¯å®ç°åŒ»å­¦å›¾åƒçš„å®‰å…¨ä¿¡æ¯åµŒå…¥ã€‚é€šè¿‡è‡ªé€‚åº”æ¨¡æ€åˆ†è§£ç­–ç•¥å°†å›¾åƒåˆ†ä¸ºå…¨å±€å’Œå±€éƒ¨æ¨¡æ€ï¼Œè¿›è¡ŒåŒè·¯å¾„æ— æŸå‹ç¼©ï¼Œå¹¶åœ¨å±€éƒ¨æ¨¡æ€è·¯å¾„é‡‡ç”¨åˆ†æ®µæ¶ˆæ¯éšå†™ç®—æ³•ä¿éšœå‹ç¼©å®‰å…¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åŒ»å­¦å›¾åƒæ— æŸå‹ç¼©ä¸­çš„åº”ç”¨é¢ä¸´æ€§èƒ½ä¸æ•ˆç‡çš„æƒè¡¡é—®é¢˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¿½è§†åŒ»å­¦å›¾åƒå‹ç¼©è¿‡ç¨‹çš„å®‰å…¨æ€§ï¼Œè¿™åœ¨ç°ä»£åŒ»ç–—åœºæ™¯ä¸­è‡³å…³é‡è¦ã€‚</li>
<li>æå‡ºçš„è”åˆæ— æŸå‹ç¼©å’Œéšå†™æ¡†æ¶åŸºäºä½å¹³é¢åˆ‡ç‰‡ï¼ˆBPSï¼‰æŠ€æœ¯ï¼Œå¯å®ç°åŒ»å­¦å›¾åƒçš„å®‰å…¨ä¿¡æ¯åµŒå…¥ã€‚</li>
<li>é€šè¿‡è‡ªé€‚åº”æ¨¡æ€åˆ†è§£ç­–ç•¥ï¼Œå°†æ•´ä¸ªåŒ»å­¦å›¾åƒåˆ†ä¸ºå…¨å±€å’Œå±€éƒ¨æ¨¡æ€ï¼Œè¿›è¡ŒåŒè·¯å¾„æ— æŸå‹ç¼©ã€‚</li>
<li>åœ¨å±€éƒ¨æ¨¡æ€è·¯å¾„é‡‡ç”¨åˆ†æ®µæ¶ˆæ¯éšå†™ç®—æ³•ï¼Œç¡®ä¿å‹ç¼©è¿‡ç¨‹çš„å®‰å…¨æ€§ã€‚</li>
<li>æ¡†æ¶ç»“åˆäº†åŸºäºè§£å‰–å…ˆéªŒçš„ä½ç§©é€‚åº”ï¼ˆA-LoRAï¼‰å¾®è°ƒç­–ç•¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01782">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e072c3597a2ab35fb22294eb4a371121" align="middle">
<img src="https://picx.zhimg.com/v2-4f8e3acc26339a60e4e8f73e4882ab0e" align="middle">
<img src="https://picx.zhimg.com/v2-458a59caa7a4ada4e497269e1aefc174" align="middle">
<img src="https://picx.zhimg.com/v2-afd8d268953a21a9cd232f532d3fc026" align="middle">
<img src="https://picx.zhimg.com/v2-93a5e721295e5009edbf5b29ce50b67f" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Style-Aware-Blending-and-Prototype-Based-Cross-Contrast-Consistency-for-Semi-Supervised-Medical-Image-Segmentation"><a href="#Style-Aware-Blending-and-Prototype-Based-Cross-Contrast-Consistency-for-Semi-Supervised-Medical-Image-Segmentation" class="headerlink" title="Style-Aware Blending and Prototype-Based Cross-Contrast Consistency for   Semi-Supervised Medical Image Segmentation"></a>Style-Aware Blending and Prototype-Based Cross-Contrast Consistency for   Semi-Supervised Medical Image Segmentation</h2><p><strong>Authors:Chaowei Chen, Xiang Zhang, Honglie Guo, Shunfang Wang</strong></p>
<p>Weak-strong consistency learning strategies are widely employed in semi-supervised medical image segmentation to train models by leveraging limited labeled data and enforcing weak-to-strong consistency. However, existing methods primarily focus on designing and combining various perturbation schemes, overlooking the inherent potential and limitations within the framework itself. In this paper, we first identify two critical deficiencies: (1) separated training data streams, which lead to confirmation bias dominated by the labeled stream; and (2) incomplete utilization of supervisory information, which limits exploration of strong-to-weak consistency. To tackle these challenges, we propose a style-aware blending and prototype-based cross-contrast consistency learning framework. Specifically, inspired by the empirical observation that the distribution mismatch between labeled and unlabeled data can be characterized by statistical moments, we design a style-guided distribution blending module to break the independent training data streams. Meanwhile, considering the potential noise in strong pseudo-labels, we introduce a prototype-based cross-contrast strategy to encourage the model to learn informative supervisory signals from both weak-to-strong and strong-to-weak predictions, while mitigating the adverse effects of noise. Experimental results demonstrate the effectiveness and superiority of our framework across multiple medical segmentation benchmarks under various semi-supervised settings. </p>
<blockquote>
<p>å¼±å¼ºä¸€è‡´æ€§å­¦ä¹ ç­–ç•¥åœ¨åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ï¼Œé€šè¿‡åˆ©ç”¨æœ‰é™çš„æ ‡è®°æ•°æ®å¹¶å¼ºåˆ¶æ‰§è¡Œå¼±åˆ°å¼ºçš„ä¸€è‡´æ€§æ¥è®­ç»ƒæ¨¡å‹ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨è®¾è®¡å’Œç»„åˆå„ç§æ‰°åŠ¨æ–¹æ¡ˆä¸Šï¼Œå¿½è§†äº†æ¡†æ¶æœ¬èº«çš„å†…åœ¨æ½œåŠ›å’Œå±€é™æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆè¯†åˆ«å‡ºä¸¤ä¸ªå…³é”®ç¼ºé™·ï¼šä¸€æ˜¯è®­ç»ƒæ•°æ®æµåˆ†ç¦»ï¼Œè¿™å¯¼è‡´ä»¥æ ‡è®°æµä¸ºä¸»çš„ç¡®è®¤åè§ï¼›äºŒæ˜¯ç›‘ç£ä¿¡æ¯åˆ©ç”¨ä¸å®Œå…¨ï¼Œè¿™é™åˆ¶äº†ä»å¼ºåˆ°å¼±çš„ä¸€è‡´æ€§çš„æ¢ç´¢ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªé£æ ¼æ„ŸçŸ¥çš„æ··åˆå’ŒåŸºäºåŸå‹çš„äº¤å‰å¯¹æ¯”ä¸€è‡´æ€§å­¦ä¹ æ¡†æ¶ã€‚å…·ä½“è€Œè¨€ï¼Œå—ç»éªŒè§‚å¯Ÿå¯å‘ï¼Œå³æ ‡è®°å’Œæ— æ ‡è®°æ•°æ®ä¹‹é—´çš„åˆ†å¸ƒä¸åŒ¹é…å¯ä»¥é€šè¿‡ç»Ÿè®¡çŸ©æ¥è¡¨å¾ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªé£æ ¼å¼•å¯¼çš„åˆ†å¸ƒæ··åˆæ¨¡å—æ¥æ‰“ç ´ç‹¬ç«‹çš„è®­ç»ƒæ•°æ®æµã€‚åŒæ—¶ï¼Œè€ƒè™‘åˆ°å¼ºä¼ªæ ‡ç­¾ä¸­çš„æ½œåœ¨å™ªå£°ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºåŸå‹çš„äº¤å‰å¯¹æ¯”ç­–ç•¥ï¼Œä»¥é¼“åŠ±æ¨¡å‹ä»å¼±åˆ°å¼ºå’Œå¼ºåˆ°å¼±çš„é¢„æµ‹ä¸­å­¦ä¹ æœ‰ç”¨çš„ç›‘ç£ä¿¡å·ï¼ŒåŒæ—¶å‡è½»å™ªå£°çš„ä¸åˆ©å½±å“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨å¤šä¸ªåŒ»å­¦åˆ†å‰²åŸºå‡†æµ‹è¯•ä¸‹å…·æœ‰æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.20729v2">PDF</a> </p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒåŠç›‘ç£åˆ†å‰²ä¸­å¸¸é‡‡ç”¨å¼±å¼ºä¸€è‡´æ€§å­¦ä¹ ç­–ç•¥ï¼Œåˆ©ç”¨æœ‰é™æ ‡æ³¨æ•°æ®è¿›è¡Œè®­ç»ƒå¹¶å¼ºåˆ¶å®æ–½å¼±åˆ°å¼ºçš„ä¸€è‡´æ€§ã€‚ä½†ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨è®¾è®¡å’Œç»„åˆå„ç§æ‰°åŠ¨æ–¹æ¡ˆï¼Œå¿½ç•¥äº†æ¡†æ¶æœ¬èº«çš„æ½œåœ¨èƒ½åŠ›å’Œå±€é™æ€§ã€‚æœ¬æ–‡é¦–å…ˆè¯†åˆ«äº†ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼šä¸€æ˜¯è®­ç»ƒæ•°æ®æµåˆ†ç¦»ï¼Œå¯¼è‡´ä»¥æ ‡æ³¨æµä¸ºä¸»çš„ç¡®è®¤åè§ï¼›äºŒæ˜¯ç›‘ç£ä¿¡æ¯åˆ©ç”¨ä¸è¶³ï¼Œé™åˆ¶äº†å¼ºåˆ°å¼±ä¸€è‡´æ€§çš„æ¢ç´¢ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªé£æ ¼æ„ŸçŸ¥çš„æ··åˆå’ŒåŸºäºåŸå‹çš„äº¤å‰å¯¹æ¯”ä¸€è‡´æ€§å­¦ä¹ æ¡†æ¶ã€‚é€šè¿‡è®¾è®¡é£æ ¼å¼•å¯¼çš„åˆ†å¸ƒæ··åˆæ¨¡å—æ‰“ç ´ç‹¬ç«‹è®­ç»ƒæ•°æ®æµï¼ŒåŒæ—¶è€ƒè™‘å¼ºä¼ªæ ‡ç­¾ä¸­çš„æ½œåœ¨å™ªå£°ï¼Œå¼•å…¥åŸºäºåŸå‹çš„äº¤å‰å¯¹æ¯”ç­–ç•¥ï¼Œé¼“åŠ±æ¨¡å‹ä»å¼±åˆ°å¼ºå’Œå¼ºåˆ°å¼±çš„é¢„æµ‹ä¸­å­¦ä¹ æœ‰ç”¨çš„ç›‘ç£ä¿¡å·ï¼ŒåŒæ—¶å‡è½»å™ªå£°çš„ä¸åˆ©å½±å“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåŠç›‘ç£åˆ†å‰²ä¸­é‡‡ç”¨å¼±å¼ºä¸€è‡´æ€§å­¦ä¹ ç­–ç•¥ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨æ‰°åŠ¨æ–¹æ¡ˆçš„è®¾è®¡ï¼Œå¿½ç•¥äº†æ¡†æ¶çš„æ½œåœ¨èƒ½åŠ›å’Œå±€é™æ€§ã€‚</li>
<li>è®­ç»ƒæ•°æ®æµåˆ†ç¦»å’Œç›‘ç£ä¿¡æ¯åˆ©ç”¨ä¸è¶³æ˜¯ä¸¤å¤§å…³é”®é—®é¢˜ã€‚</li>
<li>æå‡ºé£æ ¼æ„ŸçŸ¥çš„æ··åˆå’ŒåŸºäºåŸå‹çš„äº¤å‰å¯¹æ¯”ä¸€è‡´æ€§å­¦ä¹ æ¡†æ¶ã€‚</li>
<li>é€šè¿‡è®¾è®¡é£æ ¼å¼•å¯¼çš„åˆ†å¸ƒæ··åˆæ¨¡å—è§£å†³æ•°æ®æµåˆ†ç¦»é—®é¢˜ã€‚</li>
<li>å¼•å…¥åŸºäºåŸå‹çš„äº¤å‰å¯¹æ¯”ç­–ç•¥ï¼Œä»¥å¤„ç†å¼ºä¼ªæ ‡ç­¾ä¸­çš„æ½œåœ¨å™ªå£°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.20729">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-27d95456f3267c95a1f6a15ee6ffe323" align="middle">
<img src="https://picx.zhimg.com/v2-806fca0e263f0dbc429c40c81c646715" align="middle">
<img src="https://picx.zhimg.com/v2-abb9bb92ba1fb693724a16a417297c79" align="middle">
<img src="https://picx.zhimg.com/v2-083d16bf92e9b63d057e3a5da2486d91" align="middle">
<img src="https://picx.zhimg.com/v2-fe3e6c56c86c6c837741892c706c4d2a" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Label-tree-semantic-losses-for-rich-multi-class-medical-image-segmentation"><a href="#Label-tree-semantic-losses-for-rich-multi-class-medical-image-segmentation" class="headerlink" title="Label tree semantic losses for rich multi-class medical image   segmentation"></a>Label tree semantic losses for rich multi-class medical image   segmentation</h2><p><strong>Authors:Junwen Wang, Oscar MacCormac, William Rochford, Aaron Kujawa, Jonathan Shapey, Tom Vercauteren</strong></p>
<p>Rich and accurate medical image segmentation is poised to underpin the next generation of AI-defined clinical practice by delineating critical anatomy for pre-operative planning, guiding real-time intra-operative navigation, and supporting precise post-operative assessment. However, commonly used learning methods for medical and surgical imaging segmentation tasks penalise all errors equivalently and thus fail to exploit any inter-class semantics in the labels space. This becomes particularly problematic as the cardinality and richness of labels increases to include subtly different classes. In this work, we propose two tree-based semantic loss functions which take advantage of a hierarchical organisation of the labels. We further incorporate our losses in a recently proposed approach for training with sparse, background-free annotations to extend the applicability of our proposed losses. Extensive experiments are reported on two medical and surgical image segmentation tasks, namely head MRI for whole brain parcellation (WBP) with full supervision and neurosurgical hyperspectral imaging (HSI) for scene understanding with sparse annotations. Results demonstrate that our proposed method reaches state-of-the-art performance in both cases. </p>
<blockquote>
<p>ä¸°å¯Œè€Œå‡†ç¡®çš„åŒ»å­¦å›¾åƒåˆ†å‰²æœ‰æœ›ä¸ºä¸‹ä¸€ä»£äººå·¥æ™ºèƒ½å®šä¹‰çš„ä¸´åºŠå®è·µæä¾›æ”¯æŒï¼Œé€šè¿‡æç»˜å…³é”®è§£å‰–ç»“æ„æ¥è¿›è¡Œæœ¯å‰è§„åˆ’ã€æŒ‡å¯¼å®æ—¶æœ¯ä¸­å¯¼èˆªï¼Œå¹¶æ”¯æŒç²¾ç¡®çš„æœ¯åè¯„ä¼°ã€‚ç„¶è€Œï¼ŒåŒ»å­¦å’Œæ‰‹æœ¯å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­å¸¸ç”¨çš„å­¦ä¹ æ–¹æ³•å¹³ç­‰åœ°æƒ©ç½šæ‰€æœ‰é”™è¯¯ï¼Œå› æ­¤æœªèƒ½åˆ©ç”¨æ ‡ç­¾ç©ºé—´ä¸­çš„ä»»ä½•ç±»é—´è¯­ä¹‰ã€‚å½“æ ‡ç­¾çš„åŸºæ•°å’Œä¸°å¯Œæ€§å¢åŠ ä»¥åŒ…æ‹¬ç»†å¾®ä¸åŒçš„ç±»åˆ«æ—¶ï¼Œè¿™å˜å¾—ç‰¹åˆ«æˆé—®é¢˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ç§åŸºäºæ ‘çš„è¯­ä¹‰æŸå¤±å‡½æ•°ï¼Œå®ƒä»¬åˆ©ç”¨äº†æ ‡ç­¾çš„å±‚æ¬¡ç»„ç»‡ã€‚æˆ‘ä»¬è¿˜å°†æˆ‘ä»¬çš„æŸå¤±çº³å…¥æœ€è¿‘æå‡ºçš„ç”¨äºç¨€ç–ã€æ— èƒŒæ™¯æ³¨é‡Šçš„è®­ç»ƒæ–¹æ³•ä¸­ï¼Œä»¥æ‰©å±•æˆ‘ä»¬æå‡ºçš„æŸå¤±é€‚ç”¨æ€§ã€‚åœ¨ä¸¤é¡¹åŒ»å­¦å’Œæ‰‹æœ¯å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œå³åœ¨å…¨ç›‘ç£ä¸‹çš„å¤´éƒ¨MRIå…¨è„‘ç»†åˆ†ï¼ˆWBPï¼‰å’Œåœ¨ç¨€ç–æ³¨é‡Šä¸‹çš„ç¥ç»å¤–ç§‘é«˜å…‰è°±æˆåƒï¼ˆHSIï¼‰åœºæ™¯ç†è§£ã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸¤ç§æƒ…å†µä¸‹å‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.15777v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ‘˜è¦ä»‹ç»äº†ä¸€ç§æ–°æ–¹æ³•ï¼Œé’ˆå¯¹åŒ»ç–—å’Œæ‰‹æœ¯å›¾åƒåˆ†å‰²ä»»åŠ¡è®¾è®¡äº†ä¸¤æ¬¾åŸºäºæ ‘ç»“æ„çš„è¯­ä¹‰æŸå¤±å‡½æ•°ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†å¤æ‚çš„åŒ»å­¦å›¾åƒåˆ†å‰²é—®é¢˜ã€‚è¯¥æ–¹æ³•å……åˆ†åˆ©ç”¨æ ‡ç­¾çš„å±‚æ¬¡ç»“æ„ç»„ç»‡ä¿¡æ¯ï¼Œé€šè¿‡æ„å»ºä¸¤ç§æ ‘å½¢ç»“æ„æŸå¤±å‡½æ•°æ¥æå‡åˆ†å‰²ç²¾åº¦ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨è„‘éƒ¨MRIå…¨ç›‘ç£åˆ†å‰²å’Œç¥ç»å¤–ç§‘åœºæ™¯ç†è§£çš„ç¨€ç–æ³¨é‡Šä¸­å‡è¾¾åˆ°äº†æœ€ä½³æ€§èƒ½ã€‚è¯¥æ–¹æ³•æœ‰åŠ©äºæå‡AIåœ¨ä¸´åºŠå®è·µä¸­çš„åº”ç”¨æ•ˆæœï¼Œå¦‚æœ¯å‰è§„åˆ’ã€æœ¯ä¸­å¯¼èˆªå’Œæœ¯åè¯„ä¼°ç­‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»ç–—å’Œæ‰‹æœ¯å›¾åƒåˆ†å‰²ä»»åŠ¡çš„å¸¸è§„å­¦ä¹ æ–¹æ³•å¯¹é”™è¯¯é‡‡å–ä¸€è§†åŒä»çš„å¤„ç†æ–¹å¼ï¼Œå¿½ç•¥æ ‡ç­¾ç©ºé—´ä¸­ä¸åŒç±»åˆ«ä¹‹é—´çš„è¯­ä¹‰å·®å¼‚ã€‚è¿™ç§å¤„ç†æ–¹æ³•å¯¹äºç±»åˆ«å¤šä¸”ä¸°å¯Œçš„æ ‡ç­¾ä¼šäº§ç”Ÿé—®é¢˜ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºæ ‘ç»“æ„çš„è¯­ä¹‰æŸå¤±å‡½æ•°ï¼Œåˆ©ç”¨æ ‡ç­¾çš„å±‚æ¬¡ç»“æ„æ¥æå‡åˆ†å‰²ç²¾åº¦ã€‚è¿™ç§æ–¹æ³•é€‚ç”¨äºå¤æ‚çš„åŒ»å­¦å›¾åƒåˆ†å‰²é—®é¢˜ã€‚</li>
<li>æ–¹æ³•åŒ…æ‹¬ä¸¤ç§æ ‘å½¢ç»“æ„æŸå¤±å‡½æ•°ï¼Œæ—¨åœ¨æé«˜åˆ†å‰²æ€§èƒ½å¹¶å¤„ç†å¤æ‚çš„åŒ»å­¦å›¾åƒæ•°æ®ã€‚</li>
<li>æ–¹æ³•é€šè¿‡ç»“åˆç¨€ç–ã€æ— èƒŒæ™¯æ³¨é‡Šçš„è®­ç»ƒæ–¹æ³•ï¼Œæ‰©å¤§äº†å…¶é€‚ç”¨æ€§ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨è„‘éƒ¨MRIå…¨ç›‘ç£åˆ†å‰²å’Œç¥ç»å¤–ç§‘åœºæ™¯ç†è§£çš„ç¨€ç–æ³¨é‡Šä¸­å‡è¾¾åˆ°äº†æœ€ä½³æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.15777">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-48d11c5bf4e7dbd2a07c0107e499e9c3" align="middle">
<img src="https://picx.zhimg.com/v2-6e51e084877a4c2187bcfc22bf0999c6" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Autoadaptive-Medical-Segment-Anything-Model"><a href="#Autoadaptive-Medical-Segment-Anything-Model" class="headerlink" title="Autoadaptive Medical Segment Anything Model"></a>Autoadaptive Medical Segment Anything Model</h2><p><strong>Authors:Tyler Ward, Meredith K. Owen, Oâ€™Kira Coleman, Brian Noehren, Abdullah-Al-Zubaer Imran</strong></p>
<p>Medical image segmentation is a key task in the imaging workflow, influencing many image-based decisions. Traditional, fully-supervised segmentation models rely on large amounts of labeled training data, typically obtained through manual annotation, which can be an expensive, time-consuming, and error-prone process. This signals a need for accurate, automatic, and annotation-efficient methods of training these models. We propose ADA-SAM (automated, domain-specific, and adaptive segment anything model), a novel multitask learning framework for medical image segmentation that leverages class activation maps from an auxiliary classifier to guide the predictions of the semi-supervised segmentation branch, which is based on the Segment Anything (SAM) framework. Additionally, our ADA-SAM model employs a novel gradient feedback mechanism to create a learnable connection between the segmentation and classification branches by using the segmentation gradients to guide and improve the classification predictions. We validate ADA-SAM on real-world clinical data collected during rehabilitation trials, and demonstrate that our proposed method outperforms both fully-supervised and semi-supervised baselines by double digits in limited label settings. Our code is available at: <a target="_blank" rel="noopener" href="https://github.com/tbwa233/ADA-SAM">https://github.com/tbwa233/ADA-SAM</a>. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²æ˜¯æˆåƒå·¥ä½œæµç¨‹ä¸­çš„ä¸€é¡¹å…³é”®ä»»åŠ¡ï¼Œå½±å“ç€è®¸å¤šåŸºäºå›¾åƒçš„å†³å®šã€‚ä¼ ç»Ÿçš„å…¨ç›‘ç£åˆ†å‰²æ¨¡å‹ä¾èµ–äºå¤§é‡çš„æ ‡è®°è®­ç»ƒæ•°æ®ï¼Œè¿™äº›æ•°æ®é€šå¸¸é€šè¿‡æ‰‹åŠ¨æ³¨é‡Šè·å¾—ï¼Œè¿™ä¸€è¿‡ç¨‹æ—¢æ˜‚è´µåˆè€—æ—¶ï¼Œè¿˜å®¹æ˜“å‡ºé”™ã€‚è¿™å‡¸æ˜¾äº†å¯¹å‡†ç¡®ã€è‡ªåŠ¨å’Œæ ‡æ³¨æ•ˆç‡é«˜çš„æ¨¡å‹è®­ç»ƒæ–¹æ³•çš„éœ€è¦ã€‚æˆ‘ä»¬æå‡ºäº†ADA-SAMï¼ˆè‡ªåŠ¨ã€ç‰¹å®šé¢†åŸŸå’Œè‡ªé€‚åº”åˆ†å‰²ä»»ä½•äº‹ç‰©æ¨¡å‹ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºåŒ»å­¦å›¾åƒåˆ†å‰²çš„æ–°å‹å¤šä»»åŠ¡å­¦ä¹ æ¡†æ¶ã€‚å®ƒåˆ©ç”¨è¾…åŠ©åˆ†ç±»å™¨çš„ç±»æ¿€æ´»å›¾æ¥æŒ‡å¯¼åŠç›‘ç£åˆ†å‰²åˆ†æ”¯çš„é¢„æµ‹ï¼Œè¯¥åˆ†æ”¯åŸºäºåˆ†å‰²ä»»ä½•äº‹ç‰©ï¼ˆSAMï¼‰æ¡†æ¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ADA-SAMæ¨¡å‹é‡‡ç”¨äº†ä¸€ç§æ–°å‹æ¢¯åº¦åé¦ˆæœºåˆ¶ï¼Œé€šè¿‡åœ¨åˆ†å‰²å’Œåˆ†ç±»åˆ†æ”¯ä¹‹é—´å»ºç«‹å¯å­¦ä¹ çš„è¿æ¥ï¼Œåˆ©ç”¨åˆ†å‰²æ¢¯åº¦æ¥æŒ‡å¯¼å¹¶æ”¹è¿›åˆ†ç±»é¢„æµ‹ã€‚æˆ‘ä»¬åœ¨åº·å¤è¯•éªŒæœŸé—´æ”¶é›†çš„çœŸå®ä¸–ç•Œä¸´åºŠæ•°æ®ä¸ŠéªŒè¯äº†ADA-SAMï¼Œå¹¶è¯æ˜åœ¨æœ‰é™æ ‡ç­¾è®¾ç½®ä¸‹ï¼Œæˆ‘ä»¬æ‰€æå‡ºçš„æ–¹æ³•åœ¨å…¨ç›‘ç£å’ŒåŠç›‘ç£åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°å‡è¶…å‡ºä¸¤ä½æ•°ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/tbwa233/ADA-SAM%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/tbwa233/ADA-SAMæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01828v2">PDF</a> 11 pages, 2 figures, 3 tables</p>
<p><strong>Summary</strong></p>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²æ˜¯æˆåƒå·¥ä½œæµç¨‹ä¸­çš„å…³é”®ä»»åŠ¡ï¼Œå½±å“è®¸å¤šåŸºäºå›¾åƒçš„å†³å®šã€‚ä¼ ç»Ÿå…¨ç›‘ç£åˆ†å‰²æ¨¡å‹ä¾èµ–äºå¤§é‡æ‰‹åŠ¨æ ‡æ³¨çš„è®­ç»ƒæ•°æ®ï¼Œè¿™ä¸€è¿‡ç¨‹æ—¢æ˜‚è´µåˆè€—æ—¶ï¼Œä¸”å®¹æ˜“å‡ºé”™ã€‚å› æ­¤ï¼Œéœ€è¦å‡†ç¡®ã€è‡ªåŠ¨ã€æ ‡æ³¨æ•ˆç‡é«˜çš„æ¨¡å‹è®­ç»ƒæ–¹æ³•ã€‚æˆ‘ä»¬æå‡ºADA-SAMï¼ˆè‡ªåŠ¨ã€ç‰¹å®šé¢†åŸŸã€è‡ªé€‚åº”åˆ†å‰²ä»»ä½•æ¨¡å‹ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºSegment Anythingï¼ˆSAMï¼‰æ¡†æ¶çš„æ–°å‹å¤šä»»åŠ¡å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²ã€‚å®ƒåˆ©ç”¨è¾…åŠ©åˆ†ç±»å™¨çš„ç±»æ¿€æ´»å›¾æ¥æŒ‡å¯¼åŠç›‘ç£åˆ†å‰²åˆ†æ”¯çš„é¢„æµ‹ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„æ¢¯åº¦åé¦ˆæœºåˆ¶ï¼Œé€šè¿‡åˆ†å‰²æ¢¯åº¦æ¥æŒ‡å¯¼å’Œæ”¹è¿›åˆ†ç±»é¢„æµ‹ï¼Œä»è€Œåœ¨åˆ†å‰²å’Œåˆ†ç±»åˆ†æ”¯ä¹‹é—´å»ºç«‹å¯å­¦ä¹ çš„è¿æ¥ã€‚æˆ‘ä»¬åœ¨åº·å¤è¯•éªŒæœŸé—´æ”¶é›†çš„çœŸå®ä¸–ç•Œä¸´åºŠæ•°æ®ä¸ŠéªŒè¯äº†ADA-SAMï¼Œç»“æœè¡¨æ˜ï¼Œåœ¨æœ‰é™æ ‡ç­¾æ¡ä»¶ä¸‹ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨å…¨ç›‘ç£å’ŒåŠç›‘ç£åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°å‡è¶…å‡ºä¸¤ä½æ•°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²åœ¨æˆåƒå·¥ä½œæµç¨‹ä¸­èµ·å…³é”®ä½œç”¨ï¼Œå½±å“åŸºäºå›¾åƒçš„å¤šé¡¹å†³ç­–ã€‚</li>
<li>ä¼ ç»Ÿå…¨ç›‘ç£åˆ†å‰²æ¨¡å‹ä¾èµ–å¤§é‡æ‰‹åŠ¨æ ‡æ³¨çš„è®­ç»ƒæ•°æ®ï¼Œè¿™æ—¢è€—æ—¶åˆæˆæœ¬é«˜æ˜‚ã€‚</li>
<li>éœ€è¦æ›´å‡†ç¡®ã€è‡ªåŠ¨ã€æ ‡æ³¨æ•ˆç‡æ›´é«˜çš„åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹è®­ç»ƒæ–¹æ³•ã€‚</li>
<li>æå‡ºäº†ADA-SAMæ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹å¤šä»»åŠ¡å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²ã€‚<br>5.ADA-SAMåˆ©ç”¨è¾…åŠ©åˆ†ç±»å™¨çš„ç±»æ¿€æ´»å›¾æŒ‡å¯¼åŠç›‘ç£åˆ†å‰²åˆ†æ”¯çš„é¢„æµ‹ã€‚<br>6.ADA-SAMé‡‡ç”¨æ–°é¢–çš„æ¢¯åº¦åé¦ˆæœºåˆ¶ï¼Œé€šè¿‡åˆ†å‰²æ¢¯åº¦æ”¹è¿›åˆ†ç±»é¢„æµ‹ï¼Œå»ºç«‹åˆ†å‰²å’Œåˆ†ç±»åˆ†æ”¯ä¹‹é—´çš„å¯å­¦ä¹ è¿æ¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01828">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c899b7975c46f8fe88d80168f12598fd" align="middle">
<img src="https://picx.zhimg.com/v2-d68178e7e51e72fd3adf490b0306107a" align="middle">
<img src="https://picx.zhimg.com/v2-a8971aa4c8e2a4793226874d0c99e1c1" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="MediQ-GAN-Quantum-Inspired-GAN-for-High-Resolution-Medical-Image-Generation"><a href="#MediQ-GAN-Quantum-Inspired-GAN-for-High-Resolution-Medical-Image-Generation" class="headerlink" title="MediQ-GAN: Quantum-Inspired GAN for High Resolution Medical Image   Generation"></a>MediQ-GAN: Quantum-Inspired GAN for High Resolution Medical Image   Generation</h2><p><strong>Authors:Qingyue Jiao, Yongcan Tang, Jun Zhuang, Jason Cong, Yiyu Shi</strong></p>
<p>Machine learning-assisted diagnosis shows promise, yet medical imaging datasets are often scarce, imbalanced, and constrained by privacy, making data augmentation essential. Classical generative models typically demand extensive computational and sample resources. Quantum computing offers a promising alternative, but existing quantum-based image generation methods remain limited in scale and often face barren plateaus. We present MediQ-GAN, a quantum-inspired GAN with prototype-guided skip connections and a dual-stream generator that fuses classical and quantum-inspired branches. Its variational quantum circuits inherently preserve full-rank mappings, avoid rank collapse, and are theory-guided to balance expressivity with trainability. Beyond generation quality, we provide the first latent-geometry and rank-based analysis of quantum-inspired GANs, offering theoretical insight into their performance. Across three medical imaging datasets, MediQ-GAN outperforms state-of-the-art GANs and diffusion models. While validated on IBM hardware for robustness, our contribution is hardware-agnostic, offering a scalable and data-efficient framework for medical image generation and augmentation. </p>
<blockquote>
<p>æœºå™¨å­¦ä¹ è¾…åŠ©è¯Šæ–­å…·æœ‰å¹¿é˜”å‰æ™¯ï¼Œç„¶è€ŒåŒ»å­¦å›¾åƒæ•°æ®é›†å¾€å¾€ç¨€ç¼ºã€ä¸å‡è¡¡ä¸”å—éšç§é™åˆ¶ï¼Œä½¿å¾—æ•°æ®å¢å¼ºå˜å¾—è‡³å…³é‡è¦ã€‚ä¼ ç»Ÿçš„ç”Ÿæˆæ¨¡å‹é€šå¸¸éœ€è¦å¤§é‡çš„è®¡ç®—å’Œæ ·æœ¬èµ„æºã€‚é‡å­è®¡ç®—æä¾›äº†ä¸€ä¸ªæœ‰å‰é€”çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†ç°æœ‰çš„åŸºäºé‡å­å›¾åƒç”Ÿæˆçš„æ–¹æ³•åœ¨è§„æ¨¡ä¸Šä»ç„¶æœ‰é™ï¼Œå¹¶ç»å¸¸é¢ä¸´è’èŠœçš„é«˜åŸé—®é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†MediQ-GANï¼Œè¿™æ˜¯ä¸€ä¸ªå—é‡å­å¯å‘çš„GANï¼Œå…·æœ‰åŸå‹å¼•å¯¼è·³è·ƒè¿æ¥å’ŒåŒæµç”Ÿæˆå™¨ï¼Œèåˆäº†ç»å…¸å’Œé‡å­å¯å‘åˆ†æ”¯ã€‚å…¶å˜åˆ†é‡å­ç”µè·¯å›ºæœ‰çš„ä¿æŒäº†å…¨ç§©æ˜ å°„ï¼Œé¿å…äº†ç§©å´©æºƒï¼Œå¹¶åœ¨ç†è®ºçš„æŒ‡å¯¼ä¸‹åœ¨è¡¨ç°åŠ›å’Œå¯è®­ç»ƒæ€§ä¹‹é—´å–å¾—å¹³è¡¡ã€‚é™¤äº†ç”Ÿæˆè´¨é‡ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜å¯¹é‡å­å¯å‘çš„GANsè¿›è¡Œäº†é¦–æ¬¡åŸºäºæ½œå‡ ä½•å’Œç§©çš„åˆ†æï¼Œä¸ºå…¶æ€§èƒ½æä¾›äº†ç†è®ºä¸Šçš„è§è§£ã€‚åœ¨ä¸‰ä¸ªåŒ»å­¦å›¾åƒæ•°æ®é›†ä¸Šï¼ŒMediQ-GANä¼˜äºæœ€å…ˆè¿›GANså’Œæ‰©æ•£æ¨¡å‹ã€‚å°½ç®¡åœ¨IBMç¡¬ä»¶ä¸Šè¿›è¡Œäº†ç¨³å¥æ€§éªŒè¯ï¼Œä½†æˆ‘ä»¬çš„è´¡çŒ®æ˜¯ç¡¬ä»¶æ— å…³çš„ï¼Œæä¾›äº†ä¸€ä¸ªå¯æ‰©å±•å’Œé«˜æ•ˆçš„æ•°æ®æ¡†æ¶ç”¨äºåŒ»å­¦å›¾åƒç”Ÿæˆå’Œå¢å¼ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.21015v2">PDF</a> </p>
<p><strong>Summary</strong><br>     é‡å­å¯å‘å¼çš„MediQ-GANåœ¨åŒ»å­¦å›¾åƒç”Ÿæˆå’Œå¢å¼ºæ–¹é¢å±•ç°å‡ºä¼˜åŠ¿ï¼Œå…¶ç»“åˆäº†ç»å…¸å’Œé‡å­åˆ†æ”¯ï¼Œæœ‰æ•ˆè§£å†³äº†åŒ»å­¦å›¾åƒæ•°æ®é›†ç¨€ç¼ºã€ä¸å‡è¡¡å’Œéšç§å—é™çš„é—®é¢˜ã€‚è¯¥æ¨¡å‹é€šè¿‡åŸå‹å¼•å¯¼è·³è¿‡è¿æ¥å’ŒåŒæµç”Ÿæˆå™¨ï¼Œæé«˜äº†ç”Ÿæˆè´¨é‡å’Œæ•ˆç‡ã€‚æ­¤å¤–ï¼Œè¿˜æä¾›äº†å¯¹é‡å­å¯å‘å¼çš„GANçš„æ½œåœ¨å‡ ä½•å’Œç§©çš„åˆ†æã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒæ•°æ®é›†å¸¸å¸¸å­˜åœ¨ç¨€ç¼ºã€ä¸å‡è¡¡å’Œéšç§å—é™çš„é—®é¢˜ï¼Œæ•°æ®å¢å¼ºæ˜¾å¾—å°¤ä¸ºé‡è¦ã€‚</li>
<li>é‡å­è®¡ç®—ä¸ºè§£å†³åŒ»å­¦å›¾åƒç”Ÿæˆæä¾›äº†æ–°çš„å‰æ™¯ï¼Œä½†ç°æœ‰é‡å­å›¾åƒç”Ÿæˆæ–¹æ³•å­˜åœ¨è§„æ¨¡é™åˆ¶å’Œæ€§èƒ½ç“¶é¢ˆã€‚</li>
<li>MediQ-GANç»“åˆäº†ç»å…¸å’Œé‡å­åˆ†æ”¯ï¼Œé€šè¿‡åŸå‹å¼•å¯¼è·³è¿‡è¿æ¥å’ŒåŒæµç”Ÿæˆå™¨å®ç°é«˜æ•ˆåŒ»å­¦å›¾åƒç”Ÿæˆã€‚</li>
<li>å˜åˆ†é‡å­ç”µè·¯åœ¨MediQ-GANä¸­ä¿æŒäº†å…¨ç§©æ˜ å°„ï¼Œé¿å…äº†ç§©å´©æºƒï¼Œå®ç°äº†è¡¨è¾¾åŠ›å’Œè®­ç»ƒæ€§çš„å¹³è¡¡ã€‚</li>
<li>é™¤äº†ç”Ÿæˆè´¨é‡å¤–ï¼Œç ”ç©¶è¿˜æä¾›äº†å¯¹é‡å­å¯å‘å¼çš„GANçš„æ½œåœ¨å‡ ä½•å’Œç§©çš„åˆ†æï¼Œä¸ºç†è§£å…¶æ€§èƒ½æä¾›äº†ç†è®ºè§†è§’ã€‚</li>
<li>åœ¨ä¸‰ä¸ªåŒ»å­¦å›¾åƒæ•°æ®é›†ä¸Šï¼ŒMediQ-GANä¼˜äºæœ€å…ˆè¿›çš„å…¶ä»–GANså’Œæ‰©æ•£æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.21015">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d851a33ebf2b3fd494091250aa67bf27" align="middle">
<img src="https://picx.zhimg.com/v2-0ed5eebdc9768da734fd0496729bdd36" align="middle">
<img src="https://picx.zhimg.com/v2-ab8fdd2d8d6af91df074f4605e1b8a90" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="BRISC-Annotated-Dataset-for-Brain-Tumor-Segmentation-and-Classification"><a href="#BRISC-Annotated-Dataset-for-Brain-Tumor-Segmentation-and-Classification" class="headerlink" title="BRISC: Annotated Dataset for Brain Tumor Segmentation and Classification"></a>BRISC: Annotated Dataset for Brain Tumor Segmentation and Classification</h2><p><strong>Authors:Amirreza Fateh, Yasin Rezvani, Sara Moayedi, Sadjad Rezvani, Fatemeh Fateh, Mansoor Fateh, Vahid Abolghasemi</strong></p>
<p>Accurate segmentation and classification of brain tumors from Magnetic Resonance Imaging (MRI) remain key challenges in medical image analysis, primarily due to the lack of high-quality, balanced, and diverse datasets with expert annotations. In this work, we address this gap by introducing BRISC, a dataset designed for brain tumor segmentation and classification tasks, featuring high-resolution segmentation masks. The dataset comprises 6,000 contrast-enhanced T1-weighted MRI scans, which were collated from multiple public datasets that lacked segmentation labels. Our primary contribution is the subsequent expert annotation of these images, performed by certified radiologists and physicians. It includes three major tumor types, namely glioma, meningioma, and pituitary, as well as non-tumorous cases. Each sample includes high-resolution labels and is categorized across axial, sagittal, and coronal imaging planes to facilitate robust model development and cross-view generalization. To demonstrate the utility of the dataset, we provide benchmark results for both tasks using standard deep learning models. The BRISC dataset is made publicly available. datasetlink: Kaggle (<a target="_blank" rel="noopener" href="https://www.kaggle.com/datasets/briscdataset/brisc2025/">https://www.kaggle.com/datasets/briscdataset/brisc2025/</a>), Figshare (<a target="_blank" rel="noopener" href="https://doi.org/10.6084/m9.figshare.30533120">https://doi.org/10.6084/m9.figshare.30533120</a>), Zenodo (<a target="_blank" rel="noopener" href="https://doi.org/10.5281/zenodo.17524350">https://doi.org/10.5281/zenodo.17524350</a>) </p>
<blockquote>
<p>ä»ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰å¯¹è„‘è‚¿ç˜¤è¿›è¡Œç²¾ç¡®åˆ†å‰²å’Œåˆ†ç±»ï¼Œåœ¨åŒ»å­¦å›¾åƒåˆ†æé¢†åŸŸä»æ˜¯å…³é”®æŒ‘æˆ˜ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºç¼ºä¹å¸¦æœ‰ä¸“å®¶æ³¨é‡Šçš„é«˜è´¨é‡ã€å‡è¡¡å’Œå¤šæ ·åŒ–çš„æ•°æ®é›†ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å¼•å…¥BRISCæ•°æ®é›†æ¥è§£å†³è¿™ä¸€å·®è·ï¼Œè¯¥æ•°æ®é›†ä¸“ä¸ºè„‘è‚¿ç˜¤åˆ†å‰²å’Œåˆ†ç±»ä»»åŠ¡è®¾è®¡ï¼Œå…·æœ‰é«˜è´¨é‡åˆ†è¾¨ç‡åˆ†å‰²æ©è†œã€‚è¯¥æ•°æ®é›†åŒ…å«6000å¼ å¢å¼ºå‹T1åŠ æƒMRIæ‰«æå›¾åƒï¼Œè¿™äº›å›¾åƒæ˜¯ä»å¤šä¸ªç¼ºå°‘åˆ†å‰²æ ‡ç­¾çš„å…¬å¼€æ•°æ®é›†ä¸­æ•´ç†çš„ã€‚æˆ‘ä»¬çš„ä¸»è¦è´¡çŒ®æ˜¯å¯¹è¿™äº›å›¾åƒè¿›è¡Œäº†éšåçš„ä¸“å®¶æ³¨é‡Šï¼Œè¿™äº›æ³¨é‡Šç”±è®¤è¯è¿‡çš„æ”¾å°„å­¦å®¶å’ŒåŒ»ç”Ÿå®Œæˆã€‚å®ƒåŒ…å«ä¸‰ç§ä¸»è¦çš„è‚¿ç˜¤ç±»å‹ï¼Œå³èƒ¶è´¨ç˜¤ã€è„‘è†œç˜¤å’Œå‚ä½“ç˜¤ï¼Œä»¥åŠéè‚¿ç˜¤ç—…ä¾‹ã€‚æ¯ä¸ªæ ·æœ¬éƒ½åŒ…å«é«˜åˆ†è¾¨ç‡çš„æ ‡ç­¾ï¼Œå¹¶æŒ‰è½´å‘ã€çŸ¢çŠ¶å’Œå† çŠ¶æˆåƒå¹³é¢è¿›è¡Œåˆ†ç±»ï¼Œä»¥ä¿ƒè¿›ç¨³å¥çš„æ¨¡å‹å¼€å‘å’Œè·¨è§†å›¾æ¨å¹¿ã€‚ä¸ºäº†è¯æ˜æ•°æ®é›†çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬ä½¿ç”¨æ ‡å‡†æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸ºä¸¤ä¸ªä»»åŠ¡æä¾›äº†åŸºå‡†æµ‹è¯•ç»“æœã€‚BRISCæ•°æ®é›†å·²å…¬å¼€å‘å¸ƒã€‚æ•°æ®é›†é“¾æ¥ï¼šKaggleï¼ˆ<a target="_blank" rel="noopener" href="https://www.kaggle.com/datasets/briscdataset/brisc2025%EF%BC%89%E3%80%81Figshare%EF%BC%88https://doi.org/10.6084/m9.figshare.30533120%EF%BC%89%E3%80%81Zenodo%EF%BC%88https://doi.org/10.5281/zenodo.17524350%EF%BC%89">https://www.kaggle.com/datasets/briscdataset/brisc2025/ï¼‰ã€Figshareï¼ˆhttps://doi.org/10.6084/m9.figshare.30533120ï¼‰ã€Zenodoï¼ˆhttps://doi.org/10.5281/zenodo.17524350ï¼‰</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.14318v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªä¸“ä¸ºè„‘è‚¿ç˜¤åˆ†å‰²å’Œåˆ†ç±»ä»»åŠ¡è®¾è®¡çš„æ•°æ®é›†BRISCï¼ŒåŒ…å«äº†ç»è¿‡ä¸“å®¶æ ‡æ³¨çš„é«˜åˆ†è¾¨ç‡åˆ†å‰²æ©è†œã€‚è¯¥æ•°æ®é›†ç”±6000ä¸ªå¯¹æ¯”å¢å¼ºçš„T1åŠ æƒMRIæ‰«æç»„æˆï¼Œæ¶µç›–äº†ä¸‰ç§ä¸»è¦è‚¿ç˜¤ç±»å‹ï¼ˆèƒ¶è´¨ç˜¤ã€è„‘è†œç˜¤å’Œå‚ä½“ç˜¤ï¼‰ä»¥åŠéè‚¿ç˜¤ç—…ä¾‹ã€‚æ¯ä¸ªæ ·æœ¬éƒ½æŒ‰è½´å‘ã€çŸ¢çŠ¶é¢å’Œå† çŠ¶é¢è¿›è¡Œåˆ†ç±»ï¼Œä»¥ä¿ƒè¿›ç¨³å¥çš„æ¨¡å‹å¼€å‘å’Œè·¨è§†å›¾æ³›åŒ–ã€‚ä¸ºå±•ç¤ºè¯¥æ•°æ®é›†å®ç”¨æ€§ï¼Œæœ¬æ–‡æä¾›äº†ä½¿ç”¨æ ‡å‡†æ·±åº¦å­¦ä¹ æ¨¡å‹çš„åŸºå‡†æµ‹è¯•ç»“æœï¼Œå¹¶å…¬å¼€æä¾›BRISCæ•°æ®é›†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>BRISCæ•°æ®é›†ä¸“ä¸ºè„‘è‚¿ç˜¤åˆ†å‰²å’Œåˆ†ç±»ä»»åŠ¡è®¾è®¡ï¼Œè§£å†³äº†å½“å‰åŒ»å­¦å›¾åƒåˆ†æä¸­ç¼ºä¹é«˜è´¨é‡ã€å¹³è¡¡å’Œå¤šæ ·åŒ–æ•°æ®é›†çš„é—®é¢˜ã€‚</li>
<li>æ•°æ®é›†åŒ…å«ç»è¿‡ä¸“å®¶æ ‡æ³¨çš„é«˜åˆ†è¾¨ç‡åˆ†å‰²æ©è†œï¼Œæœ‰åŠ©äºæé«˜æ¨¡å‹çš„å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>æ•°æ®é›†åŒ…å«ä¸‰ç§ä¸»è¦è‚¿ç˜¤ç±»å‹ï¼ˆèƒ¶è´¨ç˜¤ã€è„‘è†œç˜¤å’Œå‚ä½“ç˜¤ï¼‰ä»¥åŠéè‚¿ç˜¤ç—…ä¾‹ï¼Œä¸ºæ¨¡å‹è®­ç»ƒæä¾›äº†ä¸°å¯Œçš„æ•°æ®æ ·æœ¬ã€‚</li>
<li>æ¯ä¸ªæ ·æœ¬éƒ½æŒ‰è½´å‘ã€çŸ¢çŠ¶é¢å’Œå† çŠ¶é¢è¿›è¡Œåˆ†ç±»ï¼Œä¿ƒè¿›äº†æ¨¡å‹åœ¨è·¨è§†å›¾æƒ…å¢ƒä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨æ ‡å‡†æ·±åº¦å­¦ä¹ æ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œè¯æ˜äº†BRISCæ•°æ®é›†çš„å®ç”¨æ€§ã€‚</li>
<li>BRISCæ•°æ®é›†å·²å…¬å¼€æä¾›ï¼Œæ–¹ä¾¿ç ”ç©¶äººå‘˜ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.14318">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-92982bd21a206f065d114fd821f303b2" align="middle">
<img src="https://picx.zhimg.com/v2-d7dac924626876a2bc5494b096c0a744" align="middle">
<img src="https://picx.zhimg.com/v2-0ee52baa7eb792c54e1299eed9b89bea" align="middle">
<img src="https://picx.zhimg.com/v2-25f07b14413c3f087c85b0063e34dae3" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-07/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-07/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-07/Interactive/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-84e599f1945d1c2a0e6d71e1947520ee" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-07  ENDF/B-VIII.1 Updated Nuclear Reaction Data Library for Science and   Applications
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-07
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-07/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e3ccccc165fac198a90c7e166c5c0c8f" class="responsive-img" alt="ç‰™é½¿ä¿®å¤">
                        
                        <span class="card-title">ç‰™é½¿ä¿®å¤</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            ç‰™é½¿ä¿®å¤ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-07  DentalSplat Dental Occlusion Novel View Synthesis from Sparse   Intra-Oral Photographs
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-07
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/" class="post-category">
                                    ç‰™é½¿ä¿®å¤
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                        <span class="chip bg-color">ç‰™é½¿ä¿®å¤</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33125.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
