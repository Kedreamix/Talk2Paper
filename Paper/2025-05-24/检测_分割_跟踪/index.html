<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
    <meta name="description" content="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-24  Temporal Object Captioning for Street Scene Videos from LiDAR Tracks">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-04c7a152ae50387e4ccd60a96fce9ab4.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-24
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    8.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    34 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-24-æ›´æ–°"><a href="#2025-05-24-æ›´æ–°" class="headerlink" title="2025-05-24 æ›´æ–°"></a>2025-05-24 æ›´æ–°</h1><h2 id="Temporal-Object-Captioning-for-Street-Scene-Videos-from-LiDAR-Tracks"><a href="#Temporal-Object-Captioning-for-Street-Scene-Videos-from-LiDAR-Tracks" class="headerlink" title="Temporal Object Captioning for Street Scene Videos from LiDAR Tracks"></a>Temporal Object Captioning for Street Scene Videos from LiDAR Tracks</h2><p><strong>Authors:Vignesh Gopinathan, Urs Zimmermann, Michael Arnold, Matthias Rottmann</strong></p>
<p>Video captioning models have seen notable advancements in recent years, especially with regard to their ability to capture temporal information. While many research efforts have focused on architectural advancements, such as temporal attention mechanisms, there remains a notable gap in understanding how models capture and utilize temporal semantics for effective temporal feature extraction, especially in the context of Advanced Driver Assistance Systems. We propose an automated LiDAR-based captioning procedure that focuses on the temporal dynamics of traffic participants. Our approach uses a rule-based system to extract essential details such as lane position and relative motion from object tracks, followed by a template-based caption generation. Our findings show that training SwinBERT, a video captioning model, using only front camera images and supervised with our template-based captions, specifically designed to encapsulate fine-grained temporal behavior, leads to improved temporal understanding consistently across three datasets. In conclusion, our results clearly demonstrate that integrating LiDAR-based caption supervision significantly enhances temporal understanding, effectively addressing and reducing the inherent visual&#x2F;static biases prevalent in current state-of-the-art model architectures. </p>
<blockquote>
<p>è§†é¢‘å­—å¹•ç”Ÿæˆæ¨¡å‹åœ¨è¿‘å¹´æ¥å–å¾—äº†æ˜¾è‘—çš„è¿›å±•ï¼Œå°¤å…¶æ˜¯åœ¨æ•æ‰æ—¶åºä¿¡æ¯æ–¹é¢ã€‚è™½ç„¶è®¸å¤šç ”ç©¶å·¥ä½œéƒ½é›†ä¸­åœ¨æ¶æ„çš„æ”¹è¿›ä¸Šï¼Œå¦‚æ—¶åºæ³¨æ„åŠ›æœºåˆ¶ï¼Œä½†å¯¹äºæ¨¡å‹å¦‚ä½•æ•æ‰å’Œåˆ©ç”¨æ—¶åºè¯­ä¹‰è¿›è¡Œæœ‰æ•ˆæ—¶åºç‰¹å¾æå–çš„ç†è§£ä»å­˜åœ¨æ˜æ˜¾å·®è·ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜çº§é©¾é©¶è¾…åŠ©ç³»ç»ŸèƒŒæ™¯ä¸‹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ¿€å…‰é›·è¾¾çš„è‡ªåŠ¨å­—å¹•ç”Ÿæˆç¨‹åºï¼Œå®ƒä¸“æ³¨äºäº¤é€šå‚ä¸è€…çš„æ—¶åºåŠ¨æ€ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨åŸºäºè§„åˆ™çš„ç³»ç»Ÿä»ç‰©ä½“è½¨è¿¹ä¸­æå–é‡è¦ç»†èŠ‚ï¼Œå¦‚è½¦é“ä½ç½®å’Œç›¸å¯¹è¿åŠ¨ï¼Œç„¶åè¿›è¡ŒåŸºäºæ¨¡æ¿çš„å­—å¹•ç”Ÿæˆã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œä»…ä½¿ç”¨å‰æ‘„åƒå¤´å›¾åƒè®­ç»ƒSwinBERTè§†é¢‘å­—å¹•ç”Ÿæˆæ¨¡å‹ï¼Œå¹¶ä½¿ç”¨æˆ‘ä»¬ä¸“é—¨è®¾è®¡ç”¨äºåŒ…å«ç²¾ç»†æ—¶åºè¡Œä¸ºçš„åŸºäºæ¨¡æ¿çš„å­—å¹•è¿›è¡Œç›‘ç£ï¼Œå¯åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸ŠæŒç»­æé«˜æ—¶åºç†è§£èƒ½åŠ›ã€‚æ€»ä¹‹ï¼Œæˆ‘ä»¬çš„ç»“æœæ¸…æ¥šåœ°è¡¨æ˜ï¼Œç»“åˆåŸºäºæ¿€å…‰é›·è¾¾çš„å­—å¹•ç›‘ç£æ˜¾è‘—æé«˜äº†æ—¶åºç†è§£èƒ½åŠ›ï¼Œæœ‰æ•ˆè§£å†³äº†å½“å‰æœ€å…ˆè¿›æ¨¡å‹æ¶æ„ä¸­æ™®éå­˜åœ¨çš„è§†è§‰&#x2F;é™æ€åè§é—®é¢˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16594v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šè¿‘å¹´æ¥ï¼Œè§†é¢‘æè¿°æ¨¡å‹åœ¨æ•æ‰æ—¶åºä¿¡æ¯æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚å°½ç®¡è®¸å¤šç ”ç©¶é›†ä¸­åœ¨æ¶æ„æ”¹è¿›ä¸Šï¼Œå¦‚æ—¶åºæ³¨æ„æœºåˆ¶ï¼Œä½†åœ¨ç†è§£æ¨¡å‹å¦‚ä½•æ•è·å’Œåˆ©ç”¨æ—¶åºè¯­ä¹‰è¿›è¡Œæœ‰æ•ˆæ—¶åºç‰¹å¾æå–æ–¹é¢ä»å­˜åœ¨å·®è·ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜çº§é©¾é©¶è¾…åŠ©ç³»ç»Ÿä¸Šä¸‹æ–‡ä¸­ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ¿€å…‰é›·è¾¾çš„è‡ªåŠ¨æè¿°ç¨‹åºï¼Œä¾§é‡äºäº¤é€šå‚ä¸è€…çš„æ—¶åºåŠ¨æ€ã€‚è¯¥ç ”ç©¶ä½¿ç”¨åŸºäºè§„åˆ™çš„ç³»ç»Ÿæå–è½¦é“ä½ç½®å’Œç›¸å¯¹è¿åŠ¨ç­‰å…³é”®ç»†èŠ‚ï¼Œç„¶åé€šè¿‡åŸºäºæ¨¡æ¿çš„æè¿°ç”Ÿæˆã€‚ç ”ç©¶å‘ç°ï¼Œä½¿ç”¨ä»…æ¥è‡ªå‰æ‘„åƒå¤´çš„å›¾åƒè®­ç»ƒSwinBERTè§†é¢‘æè¿°æ¨¡å‹ï¼Œå¹¶ç»“åˆä¸“é—¨é’ˆå¯¹ç²¾ç»†æ—¶é—´è¡Œä¸ºçš„æ¨¡æ¿æè¿°è¿›è¡Œç›‘ç£ï¼Œå¯ä»¥æŒç»­æ”¹å–„æ—¶é—´ç†è§£ã€‚æ€»çš„æ¥è¯´ï¼Œç»“æœæ¸…æ™°åœ°è¯æ˜ï¼Œæ•´åˆæ¿€å…‰é›·è¾¾æè¿°çš„ç›‘ç£æ˜¾ç€æé«˜äº†æ—¶é—´ç†è§£èƒ½åŠ›ï¼Œæœ‰æ•ˆè§£å†³å¹¶å‡å°‘äº†å½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹æ¶æ„ä¸­æ™®éå­˜åœ¨çš„è§†è§‰&#x2F;é™æ€åè§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è§†é¢‘æè¿°æ¨¡å‹åœ¨æ•æ‰æ—¶åºä¿¡æ¯æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ã€‚</li>
<li>å°½ç®¡å­˜åœ¨è®¸å¤šå…³äºæ¶æ„æ”¹è¿›çš„ç ”ç©¶ï¼Œä½†å¯¹æ¨¡å‹å¦‚ä½•æ•è·å’Œåˆ©ç”¨æ—¶åºè¯­ä¹‰çš„ç†è§£ä»å­˜åœ¨å·®è·ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºæ¿€å…‰é›·è¾¾çš„è‡ªåŠ¨æè¿°ç¨‹åºï¼Œä¾§é‡äºäº¤é€šå‚ä¸è€…çš„æ—¶åºåŠ¨æ€ã€‚</li>
<li>ä½¿ç”¨åŸºäºè§„åˆ™çš„ç³»ç»Ÿæå–å…³é”®ä¿¡æ¯ï¼Œå¦‚è½¦é“ä½ç½®å’Œç›¸å¯¹è¿åŠ¨ã€‚</li>
<li>é€šè¿‡åŸºäºæ¨¡æ¿çš„æè¿°ç”Ÿæˆè¿›è¡Œæè¿°ã€‚</li>
<li>ä½¿ç”¨ä»…å‰æ‘„åƒå¤´å›¾åƒè®­ç»ƒçš„SwinBERTæ¨¡å‹ç»“åˆæ¨¡æ¿æè¿°ç›‘ç£å¯ä»¥æé«˜æ—¶é—´ç†è§£èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16594">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-04c7a152ae50387e4ccd60a96fce9ab4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1e68a52632e092e6061a0b791a21b1cb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1aeefbe25c8ddb80cd25dd5071f2676.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a8932e21193dcfdcdaf01f1a606ee1a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4bab0dc5d587550ddbacd1eefa2f48e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e9aed4e9ca1f7280b61ce00d59b5fd41.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="VP-Lab-a-PEFT-Enabled-Visual-Prompting-Laboratory-for-Semantic-Segmentation"><a href="#VP-Lab-a-PEFT-Enabled-Visual-Prompting-Laboratory-for-Semantic-Segmentation" class="headerlink" title="VP Lab: a PEFT-Enabled Visual Prompting Laboratory for Semantic   Segmentation"></a>VP Lab: a PEFT-Enabled Visual Prompting Laboratory for Semantic   Segmentation</h2><p><strong>Authors:Niccolo Avogaro, Thomas Frick, Yagmur G. Cinar, Daniel Caraballo, Cezary Skura, Filip M. Janicki, Piotr Kluska, Brown Ebouky, Nicola Farronato, Florian Scheidegger, Cristiano Malossi, Konrad Schindler, Andrea Bartezzaghi, Roy Assaf, Mattia Rigotti</strong></p>
<p>Large-scale pretrained vision backbones have transformed computer vision by providing powerful feature extractors that enable various downstream tasks, including training-free approaches like visual prompting for semantic segmentation. Despite their success in generic scenarios, these models often fall short when applied to specialized technical domains where the visual features differ significantly from their training distribution. To bridge this gap, we introduce VP Lab, a comprehensive iterative framework that enhances visual prompting for robust segmentation model development. At the core of VP Lab lies E-PEFT, a novel ensemble of parameter-efficient fine-tuning techniques specifically designed to adapt our visual prompting pipeline to specific domains in a manner that is both parameter- and data-efficient. Our approach not only surpasses the state-of-the-art in parameter-efficient fine-tuning for the Segment Anything Model (SAM), but also facilitates an interactive, near-real-time loop, allowing users to observe progressively improving results as they experiment within the framework. By integrating E-PEFT with visual prompting, we demonstrate a remarkable 50% increase in semantic segmentation mIoU performance across various technical datasets using only 5 validated images, establishing a new paradigm for fast, efficient, and interactive model deployment in new, challenging domains. This work comes in the form of a demonstration. </p>
<blockquote>
<p>å¤§è§„æ¨¡é¢„è®­ç»ƒçš„è§†è§‰ä¸»å¹²ç½‘ç»œé€šè¿‡æä¾›å¼ºå¤§çš„ç‰¹å¾æå–å™¨ï¼Œä¸ºåŒ…æ‹¬æ— è®­ç»ƒæ–¹æ³•ï¼ˆå¦‚è¯­ä¹‰åˆ†å‰²çš„è§†è§‰æç¤ºï¼‰åœ¨å†…çš„å„ç§ä¸‹æ¸¸ä»»åŠ¡æä¾›äº†å¼ºå¤§çš„æ”¯æŒï¼Œä»è€Œå½»åº•æ”¹å˜äº†è®¡ç®—æœºè§†è§‰é¢†åŸŸã€‚å°½ç®¡è¿™äº›æ¨¡å‹åœ¨é€šç”¨åœºæ™¯ä¸­çš„è¡¨ç°éå¸¸å‡ºè‰²ï¼Œä½†å½“åº”ç”¨äºç‰¹å®šæŠ€æœ¯åŸŸæ—¶ï¼Œç”±äºè§†è§‰ç‰¹å¾ä¸è®­ç»ƒåˆ†å¸ƒå­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œè¿™äº›æ¨¡å‹å¾€å¾€æ— æ³•è¾¾åˆ°é¢„æœŸæ•ˆæœã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†VP Labè¿™ä¸€å…¨é¢çš„è¿­ä»£æ¡†æ¶ï¼Œç”¨äºå¢å¼ºè§†è§‰æç¤ºçš„ç¨³å¥æ€§åˆ†å‰²æ¨¡å‹å¼€å‘ã€‚VP Labçš„æ ¸å¿ƒæ˜¯E-PEFTï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯é›†åˆï¼Œæ—¨åœ¨ä»¥å‚æ•°å’Œæ•°æ®æ•ˆç‡é«˜çš„æ–¹å¼è°ƒæ•´æˆ‘ä»¬çš„è§†è§‰æç¤ºç®¡é“ï¼Œä»¥é€‚åº”ç‰¹å®šé¢†åŸŸã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…è¶…è¶Šäº†é’ˆå¯¹åˆ†å‰²ä»»ä½•æ¨¡å‹ï¼ˆSAMï¼‰çš„å‚æ•°é«˜æ•ˆå¾®è°ƒçš„æœ€å…ˆè¿›æ°´å¹³ï¼Œè¿˜ä¿ƒè¿›äº†äº¤äº’å¼è¿‘å®æ—¶å¾ªç¯ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿåœ¨æ¡†æ¶å†…å®éªŒæ—¶è§‚å¯Ÿåˆ°é€æ­¥æ”¹è¿›çš„ç»“æœã€‚é€šè¿‡å°†E-PEFTä¸è§†è§‰æç¤ºç›¸ç»“åˆï¼Œæˆ‘ä»¬åœ¨å„ç§æŠ€æœ¯æ•°æ®é›†ä¸Šå®ç°äº†è¯­ä¹‰åˆ†å‰²mIoUæ€§èƒ½çš„æ˜¾è‘—æå‡ã€‚ä»…ä½¿ç”¨5å¼ ç»è¿‡éªŒè¯çš„å›¾åƒï¼Œä¾¿å®ç°äº†ä»¤äººå°è±¡æ·±åˆ»çš„50%æå‡ï¼Œä¸ºå¿«é€Ÿã€é«˜æ•ˆã€äº¤äº’å¼æ¨¡å‹åœ¨æ–°æŒ‘æˆ˜é¢†åŸŸéƒ¨ç½²å»ºç«‹äº†æ–°çš„èŒƒä¾‹ã€‚æœ¬å·¥ä½œä»¥æ¼”ç¤ºçš„å½¢å¼å‘ˆç°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15592v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>å¤§è§„æ¨¡é¢„è®­ç»ƒè§†è§‰æ¨¡å‹ä¸ºè®¡ç®—æœºè§†è§‰æä¾›äº†å¼ºå¤§çš„ç‰¹å¾æå–å™¨ï¼Œå¯åº”ç”¨äºå¤šç§ä¸‹æ¸¸ä»»åŠ¡ï¼ŒåŒ…æ‹¬æ— éœ€è®­ç»ƒçš„è§†è§‰æç¤ºè¯­ä¹‰åˆ†å‰²æ–¹æ³•ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨åº”ç”¨äºç‰¹å®šæŠ€æœ¯åŸŸæ—¶å¸¸å¸¸è¡¨ç°ä¸ä½³ï¼Œå› ä¸ºå…¶ä¸è®­ç»ƒæ•°æ®åˆ†å¸ƒå·®å¼‚è¾ƒå¤§ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºVP Labï¼Œä¸€ä¸ªå…¨é¢çš„è¿­ä»£æ¡†æ¶ï¼Œç”¨äºå¢å¼ºè§†è§‰æç¤ºçš„ç¨³å¥åˆ†å‰²æ¨¡å‹å¼€å‘ã€‚å…¶æ ¸å¿ƒæ˜¯E-PEFTï¼Œä¸€ç§å‚æ•°æ•ˆç‡é«˜çš„å¾®è°ƒæŠ€æœ¯é›†åˆï¼Œä¸“é—¨è®¾è®¡ä»¥é€‚åº”ç‰¹å®šçš„è§†è§‰æç¤ºç®¡é“ï¼Œæ—¢å‚æ•°é«˜æ•ˆåˆæ•°æ®é«˜æ•ˆã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…è¶…è¶Šäº†é’ˆå¯¹Segment Anything Modelï¼ˆSAMï¼‰çš„å‚æ•°é«˜æ•ˆå¾®è°ƒçš„æœ€å…ˆè¿›æ°´å¹³ï¼Œè€Œä¸”æä¾›äº†ä¸€ä¸ªäº¤äº’å¼ã€è¿‘å®æ—¶çš„å¾ªç¯ï¼Œä½¿ç”¨æˆ·å¯ä»¥åœ¨æ¡†æ¶å†…å®éªŒæ—¶è§‚å¯Ÿåˆ°é€æ­¥æ”¹è¿›çš„ç»“æœã€‚é€šè¿‡æ•´åˆE-PEFTä¸è§†è§‰æç¤ºï¼Œæˆ‘ä»¬åœ¨å„ç§æŠ€æœ¯æ•°æ®é›†ä¸Šå®ç°äº†è¯­ä¹‰åˆ†å‰²mIoUæ€§èƒ½çš„50%æå‡ï¼Œä»…ä½¿ç”¨5å¼ éªŒè¯å›¾åƒï¼Œä¸ºå¿«é€Ÿã€é«˜æ•ˆã€äº¤äº’å¼æ¨¡å‹åœ¨æ–°æŒ‘æˆ˜åŸŸä¸­çš„éƒ¨ç½²å»ºç«‹äº†æ–°èŒƒä¾‹ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§è§„æ¨¡é¢„è®­ç»ƒè§†è§‰æ¨¡å‹å·²æˆä¸ºè®¡ç®—æœºè§†è§‰é¢†åŸŸçš„å¼ºå¤§å·¥å…·ï¼Œå¹¿æ³›åº”ç”¨äºå¤šç§ä¸‹æ¸¸ä»»åŠ¡ã€‚</li>
<li>è¿™äº›æ¨¡å‹åœ¨ç‰¹å®šæŠ€æœ¯åŸŸçš„åº”ç”¨ä¸­å¸¸é¢ä¸´æŒ‘æˆ˜ï¼Œå› ä¸ºå…¶ä¸è®­ç»ƒæ•°æ®åˆ†å¸ƒå­˜åœ¨å·®å¼‚ã€‚</li>
<li>VP Labæ˜¯ä¸€ä¸ªå…¨é¢çš„è¿­ä»£æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºè§†è§‰æç¤ºçš„ç¨³å¥åˆ†å‰²æ¨¡å‹å¼€å‘ã€‚</li>
<li>E-PEFTæ˜¯ä¸“ä¸ºé€‚åº”ç‰¹å®šåŸŸçš„è§†è§‰æç¤ºç®¡é“è€Œè®¾è®¡çš„å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯é›†åˆã€‚</li>
<li>E-PEFTä¸ä»…æé«˜äº†å‚æ•°æ•ˆç‡ï¼Œè€Œä¸”æé«˜äº†æ•°æ®æ•ˆç‡ã€‚</li>
<li>é€šè¿‡æ•´åˆE-PEFTä¸è§†è§‰æç¤ºï¼Œæˆ‘ä»¬åœ¨å¤šä¸ªæŠ€æœ¯æ•°æ®é›†ä¸Šå®ç°äº†æ˜¾è‘—çš„è¯­ä¹‰åˆ†å‰²æ€§èƒ½æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15592">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-be1fef1f89e81b021c67081b5dd5a80a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c5a8f6b3b41942c4920aed7af3933616.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a5778dbdd6cd1036735499555b2a529.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f7889f7edacf35f24317fa41817e8de4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2604aa03291183c1b7c3bd23bba80d45.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="UWSAM-Segment-Anything-Model-Guided-Underwater-Instance-Segmentation-and-A-Large-scale-Benchmark-Dataset"><a href="#UWSAM-Segment-Anything-Model-Guided-Underwater-Instance-Segmentation-and-A-Large-scale-Benchmark-Dataset" class="headerlink" title="UWSAM: Segment Anything Model Guided Underwater Instance Segmentation   and A Large-scale Benchmark Dataset"></a>UWSAM: Segment Anything Model Guided Underwater Instance Segmentation   and A Large-scale Benchmark Dataset</h2><p><strong>Authors:Hua Li, Shijie Lian, Zhiyuan Li, Runmin Cong, Sam Kwong</strong></p>
<p>With recent breakthroughs in large-scale modeling, the Segment Anything Model (SAM) has demonstrated significant potential in a variety of visual applications. However, due to the lack of underwater domain expertise, SAM and its variants face performance limitations in end-to-end underwater instance segmentation tasks, while their higher computational requirements further hinder their application in underwater scenarios. To address this challenge, we propose a large-scale underwater instance segmentation dataset, UIIS10K, which includes 10,048 images with pixel-level annotations for 10 categories. Then, we introduce UWSAM, an efficient model designed for automatic and accurate segmentation of underwater instances. UWSAM efficiently distills knowledge from the SAM ViT-Huge image encoder into the smaller ViT-Small image encoder via the Mask GAT-based Underwater Knowledge Distillation (MG-UKD) method for effective visual representation learning. Furthermore, we design an End-to-end Underwater Prompt Generator (EUPG) for UWSAM, which automatically generates underwater prompts instead of explicitly providing foreground points or boxes as prompts, thus enabling the network to locate underwater instances accurately for efficient segmentation. Comprehensive experimental results show that our model is effective, achieving significant performance improvements over state-of-the-art methods on multiple underwater instance datasets. Datasets and codes are available at <a target="_blank" rel="noopener" href="https://github.com/LiamLian0727/UIIS10K">https://github.com/LiamLian0727/UIIS10K</a>. </p>
<blockquote>
<p>éšç€å¤§è§„æ¨¡å»ºæ¨¡é¢†åŸŸçš„æœ€æ–°çªç ´ï¼ŒSegment Anything Modelï¼ˆSAMï¼‰åœ¨å„ç§è§†è§‰åº”ç”¨ä¸­è¡¨ç°å‡ºäº†å·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹æ°´ä¸‹é¢†åŸŸçš„ä¸“ä¸šçŸ¥è¯†ï¼ŒSAMåŠå…¶å˜ä½“åœ¨ç«¯åˆ°ç«¯çš„æ°´ä¸‹å®ä¾‹åˆ†å‰²ä»»åŠ¡ä¸­é¢ä¸´æ€§èƒ½é™åˆ¶ï¼Œè€Œå®ƒä»¬è¾ƒé«˜çš„è®¡ç®—è¦æ±‚è¿›ä¸€æ­¥é˜»ç¢äº†åœ¨æ°´ä¸‹åœºæ™¯ä¸­çš„åº”ç”¨ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†å¤§è§„æ¨¡æ°´ä¸‹å®ä¾‹åˆ†å‰²æ•°æ®é›†UIIS10Kï¼Œå…¶ä¸­åŒ…å«10,048å¼ å…·æœ‰10ç±»åƒç´ çº§æ³¨é‡Šçš„å›¾åƒã€‚æ¥ç€ï¼Œæˆ‘ä»¬ä»‹ç»äº†UWSAMï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºæ°´ä¸‹å®ä¾‹è‡ªåŠ¨å‡†ç¡®åˆ†å‰²è€Œè®¾è®¡çš„é«˜æ•ˆæ¨¡å‹ã€‚UWSAMé€šè¿‡åŸºäºMask GATçš„æ°´ä¸‹çŸ¥è¯†è’¸é¦ï¼ˆMG-UKDï¼‰æ–¹æ³•ï¼Œæœ‰æ•ˆåœ°ä»SAMçš„ViT-Hugeå›¾åƒç¼–ç å™¨æç‚¼çŸ¥è¯†ï¼Œå¹¶å°†å…¶è¿ç”¨åˆ°è¾ƒå°çš„ViT-Smallå›¾åƒç¼–ç å™¨ä¸Šï¼Œä»è€Œå®ç°æœ‰æ•ˆçš„è§†è§‰è¡¨å¾å­¦ä¹ ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä¸ºUWSAMè®¾è®¡äº†ç«¯åˆ°ç«¯æ°´ä¸‹æç¤ºç”Ÿæˆå™¨ï¼ˆEUPGï¼‰ï¼Œå®ƒä¼šè‡ªåŠ¨ç”Ÿæˆæ°´ä¸‹æç¤ºï¼Œè€Œä¸æ˜¯æ˜¾å¼æä¾›å‰æ™¯ç‚¹æˆ–æ¡†ä½œä¸ºæç¤ºï¼Œä»è€Œèƒ½å¤Ÿå‡†ç¡®å®šä½æ°´ä¸‹å®ä¾‹ï¼Œå®ç°é«˜æ•ˆçš„åˆ†å‰²ã€‚ç»¼åˆå®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹æ˜¯æœ‰æ•ˆçš„ï¼Œåœ¨å¤šä¸ªæ°´ä¸‹å®ä¾‹æ•°æ®é›†ä¸Šè¾ƒæœ€æ–°æ–¹æ³•å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚æ•°æ®é›†å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/LiamLian0727/UIIS10K%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/LiamLian0727/UIIS10Kè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15581v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šéšç€å¤§è§„æ¨¡å»ºæ¨¡çš„çªç ´ï¼ŒSegment Anything Modelï¼ˆSAMï¼‰åœ¨å¤šç§è§†è§‰åº”ç”¨ä¸­å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹æ°´ä¸‹é¢†åŸŸä¸“ä¸šçŸ¥è¯†ï¼ŒSAMåŠå…¶å˜ä½“åœ¨ç«¯åˆ°ç«¯æ°´ä¸‹å®ä¾‹åˆ†å‰²ä»»åŠ¡ä¸­çš„æ€§èƒ½å—åˆ°é™åˆ¶ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†å¤§è§„æ¨¡æ°´ä¸‹å®ä¾‹åˆ†å‰²æ•°æ®é›†UIIS10Kï¼ŒåŒ…å«10,048å¼ å¸¦æœ‰åƒç´ çº§æ³¨é‡Šçš„10ç±»å›¾åƒã€‚åŒæ—¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸“ä¸ºæ°´ä¸‹è‡ªåŠ¨ç²¾ç¡®å®ä¾‹åˆ†å‰²è®¾è®¡çš„UWSAMæ¨¡å‹ã€‚UWSAMé€šè¿‡åŸºäºMask GATçš„æ°´ä¸‹çŸ¥è¯†è’¸é¦ï¼ˆMG-UKDï¼‰æ–¹æ³•ï¼Œæœ‰æ•ˆåœ°ä»SAMçš„ViT-Hugeå›¾åƒç¼–ç å™¨è’¸é¦çŸ¥è¯†åˆ°è¾ƒå°çš„ViT-Smallå›¾åƒç¼–ç å™¨ï¼Œå®ç°æœ‰æ•ˆçš„è§†è§‰è¡¨ç¤ºå­¦ä¹ ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä¸ºUWSAMè®¾è®¡äº†ç«¯åˆ°ç«¯æ°´ä¸‹æç¤ºç”Ÿæˆå™¨ï¼ˆEUPGï¼‰ï¼Œå¯è‡ªåŠ¨ç”Ÿæˆæ°´ä¸‹æç¤ºï¼Œè€Œæ— éœ€æ˜ç¡®æä¾›å‰æ™¯ç‚¹æˆ–æ¡†ä½œä¸ºæç¤ºï¼Œä»è€Œä½¿ç½‘ç»œèƒ½å¤Ÿå‡†ç¡®å®šä½æ°´ä¸‹å®ä¾‹ï¼Œå®ç°é«˜æ•ˆåˆ†å‰²ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>SAMæ¨¡å‹åœ¨è§†è§‰åº”ç”¨ä¸­æœ‰å·¨å¤§æ½œåŠ›ï¼Œä½†åœ¨æ°´ä¸‹å®ä¾‹åˆ†å‰²æ–¹é¢å­˜åœ¨æ€§èƒ½é™åˆ¶ã€‚</li>
<li>ç¼ºä¹æ°´ä¸‹é¢†åŸŸä¸“ä¸šçŸ¥è¯†æ˜¯å½±å“SAMåœ¨æ°´ä¸‹ä»»åŠ¡ä¸­æ€§èƒ½çš„ä¸»è¦åŸå› ä¹‹ä¸€ã€‚</li>
<li>UIIS10Kæ•°æ®é›†åŒ…å«å¤§é‡æ°´ä¸‹å®ä¾‹åˆ†å‰²çš„å›¾åƒå’Œåƒç´ çº§æ³¨é‡Šã€‚</li>
<li>UWSAMæ¨¡å‹ä¸“ä¸ºæ°´ä¸‹è‡ªåŠ¨ç²¾ç¡®å®ä¾‹åˆ†å‰²è®¾è®¡ã€‚</li>
<li>UWSAMé€šè¿‡Mask GATçš„æ°´ä¸‹çŸ¥è¯†è’¸é¦æ–¹æ³•æå‡è§†è§‰è¡¨ç¤ºå­¦ä¹ ã€‚</li>
<li>ç«¯åˆ°ç«¯æ°´ä¸‹æç¤ºç”Ÿæˆå™¨ï¼ˆEUPGï¼‰èƒ½è‡ªåŠ¨ç”Ÿæˆæ°´ä¸‹æç¤ºï¼Œæé«˜ç½‘ç»œå®šä½æ°´ä¸‹å®ä¾‹çš„å‡†ç¡®æ€§ã€‚</li>
<li>UWSAMæ¨¡å‹åœ¨å¤šä¸ªæ°´ä¸‹å®ä¾‹æ•°æ®é›†ä¸Šçš„æ€§èƒ½æ˜¾è‘—ï¼Œè¶…è¶Šç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15581">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-441b667b86a33fc92d23a041a8a08c93.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7cfcae1f57425fd2c7796fb24446890d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-97b69e92a6d1df639f352876f8816377.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-417104f48ac35af205379c25eeebe73f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f7eb65a293f4df3f6e76989761167ba.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-63adc646e52d8a12a7a38d4be641ea53.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-81a618ddb8d199d20ce84e093b867ae6.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Spectral-Aware-Global-Fusion-for-RGB-Thermal-Semantic-Segmentation"><a href="#Spectral-Aware-Global-Fusion-for-RGB-Thermal-Semantic-Segmentation" class="headerlink" title="Spectral-Aware Global Fusion for RGB-Thermal Semantic Segmentation"></a>Spectral-Aware Global Fusion for RGB-Thermal Semantic Segmentation</h2><p><strong>Authors:Ce Zhang, Zifu Wan, Simon Stepputtis, Katia Sycara, Yaqi Xie</strong></p>
<p>Semantic segmentation relying solely on RGB data often struggles in challenging conditions such as low illumination and obscured views, limiting its reliability in critical applications like autonomous driving. To address this, integrating additional thermal radiation data with RGB images demonstrates enhanced performance and robustness. However, how to effectively reconcile the modality discrepancies and fuse the RGB and thermal features remains a well-known challenge. In this work, we address this challenge from a novel spectral perspective. We observe that the multi-modal features can be categorized into two spectral components: low-frequency features that provide broad scene context, including color variations and smooth areas, and high-frequency features that capture modality-specific details such as edges and textures. Inspired by this, we propose the Spectral-aware Global Fusion Network (SGFNet) to effectively enhance and fuse the multi-modal features by explicitly modeling the interactions between the high-frequency, modality-specific features. Our experimental results demonstrate that SGFNet outperforms the state-of-the-art methods on the MFNet and PST900 datasets. </p>
<blockquote>
<p>ä»…ä¾èµ–RGBæ•°æ®è¿›è¡Œè¯­ä¹‰åˆ†å‰²åœ¨æŒ‘æˆ˜æ¡ä»¶ä¸‹ï¼ˆä¾‹å¦‚ä½å…‰ç…§å’Œé®æŒ¡è§†å›¾ï¼‰å¾€å¾€è¡¨ç°ä¸ä½³ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨è‡ªåŠ¨é©¾é©¶ç­‰å…³é”®åº”ç”¨ä¸­çš„å¯é æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå°†é¢å¤–çš„çƒ­è¾å°„æ•°æ®ä¸RGBå›¾åƒç›¸ç»“åˆï¼Œæ˜¾ç¤ºå‡ºæ›´é«˜çš„æ€§èƒ½å’Œç¨³å¥æ€§ã€‚ç„¶è€Œï¼Œå¦‚ä½•æœ‰æ•ˆåœ°åè°ƒä¸åŒæ¨¡æ€ä¹‹é—´çš„å·®å¼‚å¹¶èåˆRGBå’Œçƒ­ç‰¹å¾ä»ç„¶æ˜¯ä¸€ä¸ªå…¬è®¤çš„æŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»å…¨æ–°çš„å…‰è°±è§’åº¦æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚æˆ‘ä»¬å‘ç°å¤šæ¨¡æ€ç‰¹å¾å¯ä»¥åˆ’åˆ†ä¸ºä¸¤ä¸ªå…‰è°±æˆåˆ†ï¼šæä¾›å¹¿æ³›åœºæ™¯ä¸Šä¸‹æ–‡çš„ä½é¢‘ç‰¹å¾ï¼ŒåŒ…æ‹¬é¢œè‰²å˜åŒ–å’Œå¹³æ»‘åŒºåŸŸï¼›ä»¥åŠæ•æ‰ç‰¹å®šæ¨¡æ€ç»†èŠ‚ï¼ˆå¦‚è¾¹ç¼˜å’Œçº¹ç†ï¼‰çš„é«˜é¢‘ç‰¹å¾ã€‚å—æ­¤å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†Spectral-aware Global Fusion Networkï¼ˆSGFNetï¼‰ç½‘ç»œï¼Œé€šè¿‡æ˜¾å¼å»ºæ¨¡é«˜é¢‘å’Œç‰¹å®šæ¨¡æ€ç‰¹å¾ä¹‹é—´çš„äº¤äº’ï¼Œæœ‰æ•ˆåœ°å¢å¼ºå’Œèåˆå¤šæ¨¡æ€ç‰¹å¾ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨MFNetå’ŒPST900æ•°æ®é›†ä¸Šï¼ŒSGFNetçš„æ€§èƒ½è¶…è¿‡äº†æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15491v1">PDF</a> Accepted by ICIP 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æŒ‡å‡ºï¼Œåœ¨æŒ‘æˆ˜æ¡ä»¶ä¸‹ï¼Œå¦‚ä½å…‰ç…§å’Œé®æŒ¡è§†å›¾ä¸­ï¼Œä»…ä¾èµ–RGBæ•°æ®çš„è¯­ä¹‰åˆ†å‰²å­˜åœ¨å±€é™æ€§ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæ•´åˆé¢å¤–çš„çƒ­è¾å°„æ•°æ®ä¸RGBå›¾åƒèƒ½æé«˜æ€§èƒ½å’Œç¨³å¥æ€§ã€‚ç„¶è€Œï¼Œå¦‚ä½•æœ‰æ•ˆåè°ƒä¸åŒæ¨¡æ€ä¹‹é—´çš„å·®å¼‚å¹¶èåˆRGBå’Œçƒ­ç‰¹å¾ä»æ˜¯ä¸€å¤§æŒ‘æˆ˜ã€‚æœ¬æ–‡ä»æ–°çš„å…‰è°±è§†è§’å‡ºå‘ï¼Œè§‚å¯Ÿåˆ°å¤šæ¨¡æ€ç‰¹å¾å¯åˆ†ä¸ºä¸¤ç§å…‰è°±æˆåˆ†ï¼šæä¾›åœºæ™¯å¹¿æ³›èƒŒæ™¯çš„ä½é¢‘ç‰¹å¾ï¼Œä»¥åŠæ•æ‰è¾¹ç¼˜å’Œçº¹ç†ç­‰ç‰¹å®šæ¨¡æ€ç»†èŠ‚çš„é«˜é¢‘ç‰¹å¾ã€‚å—æ­¤å¯å‘ï¼Œæœ¬æ–‡æå‡ºè°±æ„ŸçŸ¥å…¨å±€èåˆç½‘ç»œï¼ˆSGFNetï¼‰ï¼Œé€šè¿‡æ˜¾å¼å»ºæ¨¡é«˜é¢‘å’Œç‰¹å®šæ¨¡æ€ç‰¹å¾ä¹‹é—´çš„ç›¸äº’ä½œç”¨ï¼Œæœ‰æ•ˆå¢å¼ºå’Œèåˆå¤šæ¨¡æ€ç‰¹å¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSGFNetåœ¨MFNetå’ŒPST900æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºæœ€æ–°æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åœ¨ä½å…‰ç…§å’Œé®æŒ¡ç­‰æŒ‘æˆ˜æ¡ä»¶ä¸‹ï¼Œä»…ä¾èµ–RGBæ•°æ®çš„è¯­ä¹‰åˆ†å‰²å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>æ•´åˆRGBä¸çƒ­è¾å°„æ•°æ®å¯ä»¥æé«˜è¯­ä¹‰åˆ†å‰²çš„æ€§èƒ½å’Œç¨³å¥æ€§ã€‚</li>
<li>å­˜åœ¨åè°ƒä¸åŒæ¨¡æ€ä¹‹é—´çš„å·®å¼‚å¹¶èåˆRGBå’Œçƒ­ç‰¹å¾çš„æŠ€æœ¯æŒ‘æˆ˜ã€‚</li>
<li>æœ¬æ–‡ä»æ–°çš„å…‰è°±è§†è§’åˆ†æå¤šæ¨¡æ€ç‰¹å¾ï¼Œåˆ†ä¸ºä½é¢‘å’Œé«˜é¢‘ç‰¹å¾ã€‚</li>
<li>ä½é¢‘ç‰¹å¾æä¾›åœºæ™¯èƒŒæ™¯ä¿¡æ¯ï¼Œé«˜é¢‘ç‰¹å¾æ•æ‰ç‰¹å®šæ¨¡æ€çš„ç»†èŠ‚ï¼ˆå¦‚è¾¹ç¼˜å’Œçº¹ç†ï¼‰ã€‚</li>
<li>æå‡ºè°±æ„ŸçŸ¥å…¨å±€èåˆç½‘ç»œï¼ˆSGFNetï¼‰ï¼Œé€šè¿‡å»ºæ¨¡é«˜é¢‘å’Œç‰¹å®šæ¨¡æ€ç‰¹å¾ä¹‹é—´çš„ç›¸äº’ä½œç”¨æ¥å¢å¼ºå’Œèåˆå¤šæ¨¡æ€ç‰¹å¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15491">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-292bf88ddba26ba93d18f94686f69be6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df646c7441256732337f4015e0c6ad5d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac0fab27fc32fb4cf19309838c6fb6f4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-45ab689911d610e02939b621e5b6e354.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-49a948cbc2d20394c76cd188f18cfc3e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dda62e85afb06f1ee6a4e32df317b3de.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="gen2seg-Generative-Models-Enable-Generalizable-Instance-Segmentation"><a href="#gen2seg-Generative-Models-Enable-Generalizable-Instance-Segmentation" class="headerlink" title="gen2seg: Generative Models Enable Generalizable Instance Segmentation"></a>gen2seg: Generative Models Enable Generalizable Instance Segmentation</h2><p><strong>Authors:Om Khangaonkar, Hamed Pirsiavash</strong></p>
<p>By pretraining to synthesize coherent images from perturbed inputs, generative models inherently learn to understand object boundaries and scene compositions. How can we repurpose these generative representations for general-purpose perceptual organization? We finetune Stable Diffusion and MAE (encoder+decoder) for category-agnostic instance segmentation using our instance coloring loss exclusively on a narrow set of object types (indoor furnishings and cars). Surprisingly, our models exhibit strong zero-shot generalization, accurately segmenting objects of types and styles unseen in finetuning (and in many cases, MAEâ€™s ImageNet-1K pretraining too). Our best-performing models closely approach the heavily supervised SAM when evaluated on unseen object types and styles, and outperform it when segmenting fine structures and ambiguous boundaries. In contrast, existing promptable segmentation architectures or discriminatively pretrained models fail to generalize. This suggests that generative models learn an inherent grouping mechanism that transfers across categories and domains, even without internet-scale pretraining. Code, pretrained models, and demos are available on our website. </p>
<blockquote>
<p>é€šè¿‡é¢„è®­ç»ƒä»å—æ‰°è¾“å…¥ä¸­åˆæˆè¿è´¯çš„å›¾åƒï¼Œç”Ÿæˆæ¨¡å‹æœ¬è´¨ä¸Šå­¦ä¼šäº†ç†è§£å¯¹è±¡è¾¹ç•Œå’Œåœºæ™¯ç»„æˆã€‚æˆ‘ä»¬å¦‚ä½•å°†è¿™äº›ç”Ÿæˆè¡¨ç¤ºé‡æ–°ç”¨äºé€šç”¨æ„ŸçŸ¥ç»„ç»‡å‘¢ï¼Ÿæˆ‘ä»¬å¯¹å®¤å†…å®¶å…·å’Œæ±½è½¦ç­‰å°‘æ•°å¯¹è±¡ç±»å‹ï¼Œä½¿ç”¨å®ä¾‹ç€è‰²æŸå¤±å¯¹Stable Diffusionå’ŒMAEï¼ˆç¼–ç å™¨+è§£ç å™¨ï¼‰è¿›è¡Œå¾®è°ƒï¼Œç”¨äºç±»åˆ«æ— å…³çš„å®ä¾‹åˆ†å‰²ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ¨¡å‹è¡¨ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿå‡†ç¡®åˆ†å‰²å¾®è°ƒä¸­æœªè§è¿‡çš„å¯¹è±¡ç±»å‹å’Œé£æ ¼ï¼ˆå¹¶ä¸”åœ¨è®¸å¤šæƒ…å†µä¸‹ï¼ŒMAEçš„ImageNet-1Ké¢„è®­ç»ƒä¹Ÿæ˜¯å¦‚æ­¤ï¼‰ã€‚åœ¨æœªè§è¿‡çš„å¯¹è±¡ç±»å‹å’Œé£æ ¼è¿›è¡Œè¯„ä¼°æ—¶ï¼Œæˆ‘ä»¬è¡¨ç°æœ€ä½³çš„æ¨¡å‹æ¥è¿‘é«˜åº¦ç›‘ç£çš„SAMï¼Œåœ¨åˆ†å‰²ç²¾ç»†ç»“æ„å’Œæ¨¡ç³Šè¾¹ç•Œæ—¶åˆ™è¡¨ç°ä¼˜äºå®ƒã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œç°æœ‰çš„å¯æç¤ºåˆ†å‰²æ¶æ„æˆ–åˆ¤åˆ«é¢„è®­ç»ƒæ¨¡å‹æ— æ³•æ³›åŒ–ã€‚è¿™è¡¨æ˜ç”Ÿæˆæ¨¡å‹å­¦ä¹ äº†ä¸€ç§å›ºæœ‰çš„åˆ†ç»„æœºåˆ¶ï¼Œå¯ä»¥åœ¨ç±»åˆ«å’Œé¢†åŸŸä¹‹é—´è½¬ç§»ï¼Œå³ä½¿ä¸ä½¿ç”¨äº’è”ç½‘è§„æ¨¡çš„é¢„è®­ç»ƒä¹Ÿæ˜¯å¦‚æ­¤ã€‚ä»£ç ã€é¢„è®­ç»ƒæ¨¡å‹å’Œæ¼”ç¤ºå¯åœ¨æˆ‘ä»¬çš„ç½‘ç«™ä¸Šæ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15263v1">PDF</a> Website: <a target="_blank" rel="noopener" href="https://reachomk.github.io/gen2seg/">https://reachomk.github.io/gen2seg/</a></p>
<p><strong>Summary</strong></p>
<p>é¢„è®­ç»ƒç”Ÿæˆæ¨¡å‹ä»æ‰°åŠ¨è¾“å…¥ä¸­åˆæˆè¿è´¯å›¾åƒï¼Œä»è€Œå­¦ä¹ ç‰©ä½“è¾¹ç•Œå’Œåœºæ™¯ç»„åˆã€‚å¦‚ä½•é€šè¿‡å¾®è°ƒç”¨äºé€šç”¨æ„ŸçŸ¥ç»„ç»‡ï¼Ÿæˆ‘ä»¬å¯¹Stable Diffusionå’ŒMAEï¼ˆç¼–ç å™¨+è§£ç å™¨ï¼‰è¿›è¡Œç±»åˆ«æ— å…³çš„å®ä¾‹åˆ†å‰²ï¼Œä½¿ç”¨å®ä¾‹æŸ“è‰²æŸå¤±ä»…é’ˆå¯¹å°‘é‡ç‰©ä½“ç±»å‹ï¼ˆå®¤å†…å®¶å…·å’Œæ±½è½¦ï¼‰ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæ¨¡å‹è¡¨ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œå‡†ç¡®åˆ†å‰²åœ¨å¾®è°ƒä¸­æœªè§è¿‡çš„ç‰©ä½“ç±»å‹å’Œé£æ ¼ï¼ˆå¹¶ä¸”åœ¨è®¸å¤šæƒ…å†µä¸‹ï¼ŒMAEçš„ImageNet-1Ké¢„è®­ç»ƒäº¦æ˜¯å¦‚æ­¤ï¼‰ã€‚æœ€ä½³æ€§èƒ½çš„æ¨¡å‹åœ¨æœªè§è¿‡çš„ç‰©ä½“ç±»å‹å’Œé£æ ¼ä¸Šçš„è¯„ä¼°æ¥è¿‘é«˜åº¦ç›‘ç£çš„SAMï¼Œåœ¨åˆ†å‰²ç²¾ç»†ç»“æ„å’Œæ¨¡ç³Šè¾¹ç•Œæ—¶è¡¨ç°æ›´ä¼˜ã€‚è¿™è¡¨æ˜ç”Ÿæˆæ¨¡å‹å­¦ä¹ äº†ä¸€ç§å›ºæœ‰çš„åˆ†ç»„æœºåˆ¶ï¼Œå¯è·¨ç±»åˆ«å’Œé¢†åŸŸè¿ç§»ï¼Œå³ä½¿æ— éœ€äº’è”ç½‘è§„æ¨¡çš„é¢„è®­ç»ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆæ¨¡å‹é€šè¿‡é¢„è®­ç»ƒå­¦ä¹ åˆæˆè¿è´¯å›¾åƒï¼Œä»è€ŒæŒæ¡ç‰©ä½“è¾¹ç•Œå’Œåœºæ™¯ç»„åˆçš„ç†è§£ã€‚</li>
<li>é€šè¿‡å®ä¾‹æŸ“è‰²æŸå¤±å¯¹ç‰¹å®šç‰©ä½“ç±»å‹ï¼ˆå¦‚å®¤å†…å®¶å…·å’Œæ±½è½¦ï¼‰è¿›è¡Œå¾®è°ƒï¼Œæ¨¡å‹è¡¨ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>æœ€ä½³æ€§èƒ½çš„æ¨¡å‹åœ¨æœªè§è¿‡çš„ç‰©ä½“ç±»å‹å’Œé£æ ¼ä¸Šæ¥è¿‘æˆ–ä¼˜äºé«˜åº¦ç›‘ç£çš„SAMã€‚</li>
<li>ç”Ÿæˆæ¨¡å‹åœ¨åˆ†å‰²ç²¾ç»†ç»“æ„å’Œæ¨¡ç³Šè¾¹ç•Œæ—¶è¡¨ç°ä¼˜è¶Šã€‚</li>
<li>ç›¸æ¯”å…¶ä»–å¯æç¤ºçš„åˆ†å‰²æ¶æ„æˆ–åˆ¤åˆ«é¢„è®­ç»ƒæ¨¡å‹ï¼Œç”Ÿæˆæ¨¡å‹å…·æœ‰æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ç”Ÿæˆæ¨¡å‹å­¦ä¹ åˆ°ä¸€ç§å›ºæœ‰çš„åˆ†ç»„æœºåˆ¶ï¼Œå¯è·¨ç±»åˆ«å’Œé¢†åŸŸè¿ç§»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15263">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-aae70db10f98d1c089ec9d785d0789f7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-787cc8612050ec65929ae41bdfd3975c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9c7c27048a643306379938d80ee05771.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c93982fc4655d156731bc830acc6bfc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-46c5b981ad03540f1a7b1f0737c09051.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="TransMedSeg-A-Transferable-Semantic-Framework-for-Semi-Supervised-Medical-Image-Segmentation"><a href="#TransMedSeg-A-Transferable-Semantic-Framework-for-Semi-Supervised-Medical-Image-Segmentation" class="headerlink" title="TransMedSeg: A Transferable Semantic Framework for Semi-Supervised   Medical Image Segmentation"></a>TransMedSeg: A Transferable Semantic Framework for Semi-Supervised   Medical Image Segmentation</h2><p><strong>Authors:Mengzhu Wang, Jiao Li, Shanshan Wang, Long Lan, Huibin Tan, Liang Yang, Guoli Yang</strong></p>
<p>Semi-supervised learning (SSL) has achieved significant progress in medical image segmentation (SSMIS) through effective utilization of limited labeled data. While current SSL methods for medical images predominantly rely on consistency regularization and pseudo-labeling, they often overlook transferable semantic relationships across different clinical domains and imaging modalities. To address this, we propose TransMedSeg, a novel transferable semantic framework for semi-supervised medical image segmentation. Our approach introduces a Transferable Semantic Augmentation (TSA) module, which implicitly enhances feature representations by aligning domain-invariant semantics through cross-domain distribution matching and intra-domain structural preservation. Specifically, TransMedSeg constructs a unified feature space where teacher network features are adaptively augmented towards student network semantics via a lightweight memory module, enabling implicit semantic transformation without explicit data generation. Interestingly, this augmentation is implicitly realized through an expected transferable cross-entropy loss computed over the augmented teacher distribution. An upper bound of the expected loss is theoretically derived and minimized during training, incurring negligible computational overhead. Extensive experiments on medical image datasets demonstrate that TransMedSeg outperforms existing semi-supervised methods, establishing a new direction for transferable representation learning in medical image analysis. </p>
<blockquote>
<p>åŠç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ï¼ˆSSMISï¼‰ä¸­é€šè¿‡æœ‰æ•ˆåˆ©ç”¨æœ‰é™æ ‡è®°æ•°æ®å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚è™½ç„¶å½“å‰çš„åŒ»å­¦å›¾åƒSSLæ–¹æ³•ä¸»è¦ä¾èµ–äºä¸€è‡´æ€§æ­£åˆ™åŒ–å’Œä¼ªæ ‡ç­¾ï¼Œä½†å®ƒä»¬å¾€å¾€å¿½è§†äº†ä¸åŒä¸´åºŠåŸŸå’Œæˆåƒæ¨¡å¼ä¹‹é—´çš„å¯è½¬ç§»è¯­ä¹‰å…³ç³»ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†TransMedSegï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºåŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²çš„æ–°å‹å¯è½¬ç§»è¯­ä¹‰æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸€ä¸ªå¯è½¬ç§»è¯­ä¹‰å¢å¼ºï¼ˆTSAï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—é€šè¿‡è·¨åŸŸåˆ†å¸ƒåŒ¹é…å’ŒåŸŸå†…ç»“æ„ä¿ç•™æ¥å¯¹é½åŸŸä¸å˜è¯­ä¹‰ï¼Œä»è€Œéšå¼åœ°å¢å¼ºç‰¹å¾è¡¨ç¤ºã€‚å…·ä½“æ¥è¯´ï¼ŒTransMedSegæ„å»ºäº†ä¸€ä¸ªç»Ÿä¸€çš„ç‰¹å¾ç©ºé—´ï¼Œå…¶ä¸­æ•™å¸ˆç½‘ç»œç‰¹å¾é€šè¿‡è½»é‡çº§å†…å­˜æ¨¡å—å‘å­¦ç”Ÿç½‘ç»œè¯­ä¹‰è¿›è¡Œè‡ªé€‚åº”å¢å¼ºï¼Œå®ç°éšå¼è¯­ä¹‰è½¬æ¢è€Œæ— éœ€æ˜¾å¼æ•°æ®ç”Ÿæˆã€‚æœ‰è¶£çš„æ˜¯ï¼Œè¿™ç§å¢å¼ºæ˜¯é€šè¿‡åœ¨æ•™å¸ˆåˆ†å¸ƒä¸Šè®¡ç®—çš„é¢„æœŸå¯è½¬ç§»äº¤å‰ç†µæŸå¤±æ¥å®ç°çš„ã€‚ç†è®ºä¸Šæ¨å¯¼äº†é¢„æœŸæŸå¤±çš„ä¸Šç•Œï¼Œå¹¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¿›è¡Œæœ€å°åŒ–ï¼Œå‡ ä¹ä¸ä¼šå¢åŠ è®¡ç®—å¼€é”€ã€‚åœ¨åŒ»å­¦å›¾åƒæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒTransMedSegä¼˜äºç°æœ‰çš„åŠç›‘ç£æ–¹æ³•ï¼Œä¸ºåŒ»å­¦å›¾åƒåˆ†æä¸­çš„å¯è½¬ç§»è¡¨ç¤ºå­¦ä¹ æŒ‡æ˜äº†æ–°çš„æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14753v1">PDF</a> </p>
<p><strong>Summary</strong><br>åœ¨åŠç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ï¼ˆSSMISï¼‰ä¸­åˆ©ç”¨æœ‰é™æ ‡è®°æ•°æ®å–å¾—æ˜¾è‘—è¿›å±•çš„èƒŒæ™¯ä¸‹ï¼Œå½“å‰SSLæ–¹æ³•ä¸»è¦ä¾èµ–ä¸€è‡´æ€§æ­£åˆ™åŒ–å’Œä¼ªæ ‡ç­¾ï¼Œå¿½ç•¥äº†ä¸åŒä¸´åºŠåŸŸå’Œæˆåƒæ¨¡å¼ä¹‹é—´çš„å¯è½¬ç§»è¯­ä¹‰å…³ç³»ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºTransMedSegï¼Œä¸€ç§æ–°å‹å¯è½¬ç§»è¯­ä¹‰æ¡†æ¶ï¼Œç”¨äºåŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²ã€‚å®ƒå¼•å…¥å¯è½¬ç§»è¯­ä¹‰å¢å¼ºï¼ˆTSAï¼‰æ¨¡å—ï¼Œé€šè¿‡è·¨åŸŸåˆ†å¸ƒåŒ¹é…å’ŒåŸŸå†…ç»“æ„ä¿ç•™ï¼Œéšå¼åœ°å¢å¼ºç‰¹å¾è¡¨ç¤ºå¹¶å¯¹é½åŸŸä¸å˜è¯­ä¹‰ã€‚TransMedSegæ„å»ºç»Ÿä¸€ç‰¹å¾ç©ºé—´ï¼Œé€šè¿‡è½»é‡çº§å†…å­˜æ¨¡å—è‡ªé€‚åº”å¢å¼ºæ•™å¸ˆç½‘ç»œç‰¹å¾å‘å­¦ç”Ÿç½‘ç»œè¯­ä¹‰ï¼Œå®ç°éšå¼è¯­ä¹‰è½¬æ¢ï¼Œæ— éœ€æ˜¾å¼æ•°æ®ç”Ÿæˆã€‚é€šè¿‡æœ€å°åŒ–ç†è®ºæ¨å¯¼çš„é¢„æœŸæŸå¤±ä¸Šé™ï¼Œåœ¨è®­ç»ƒä¸­å¼•å…¥çš„è®¡ç®—å¼€é”€å¾®ä¹å…¶å¾®ã€‚åœ¨åŒ»å­¦å›¾åƒæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒTransMedSegä¼˜äºç°æœ‰åŠç›‘ç£æ–¹æ³•ï¼Œä¸ºåŒ»å­¦å›¾åƒåˆ†æä¸­çš„å¯è½¬ç§»è¡¨ç¤ºå­¦ä¹ æŒ‡æ˜äº†æ–°æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰SSLæ–¹æ³•åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­ä¸»è¦ä¾èµ–ä¸€è‡´æ€§æ­£åˆ™åŒ–å’Œä¼ªæ ‡ç­¾ï¼Œä½†å¿½ç•¥äº†ä¸åŒä¸´åºŠåŸŸå’Œæˆåƒæ¨¡å¼é—´çš„å¯è½¬ç§»è¯­ä¹‰å…³ç³»ã€‚</li>
<li>TransMedSegæ˜¯ä¸€ç§æ–°å‹å¯è½¬ç§»è¯­ä¹‰æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>TransMedSegå¼•å…¥TSAæ¨¡å—ï¼Œéšå¼åœ°å¢å¼ºç‰¹å¾è¡¨ç¤ºå¹¶é€šè¿‡å¯¹é½åŸŸä¸å˜è¯­ä¹‰æ¥æ”¹å–„SSLåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„æ€§èƒ½ã€‚</li>
<li>TransMedSegæ„å»ºç»Ÿä¸€ç‰¹å¾ç©ºé—´ï¼Œé€šè¿‡è½»é‡çº§å†…å­˜æ¨¡å—å®ç°æ•™å¸ˆç½‘ç»œå’Œå­¦ç”Ÿç½‘ç»œä¹‹é—´çš„éšå¼è¯­ä¹‰è½¬æ¢ã€‚</li>
<li>TransMedSegåˆ©ç”¨é¢„æœŸçš„è·¨ç†µæŸå¤±æ¥å®ç°å¯è½¬ç§»è¯­ä¹‰å¢å¼ºï¼Œè¯¥æŸå¤±æ˜¯é€šè¿‡æ•™å¸ˆåˆ†å¸ƒå¢å¼ºæ¥è®¡ç®—çš„ã€‚</li>
<li>TransMedSegé€šè¿‡æœ€å°åŒ–ç†è®ºæ¨å¯¼çš„é¢„æœŸæŸå¤±ä¸Šé™æ¥è®­ç»ƒæ¨¡å‹ï¼ŒåŒæ—¶ä¿æŒè®¡ç®—å¼€é”€å¾®å°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14753">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-adffa5109807bdaae993bfcfd718eb8a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c76a86ca68174d42b2164afc5bcc6107.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9ffc18aa4d9c4fcc8c6cfc92565914c6.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="DINOv2-powered-Few-Shot-Semantic-Segmentation-A-Unified-Framework-via-Cross-Model-Distillation-and-4D-Correlation-Mining"><a href="#DINOv2-powered-Few-Shot-Semantic-Segmentation-A-Unified-Framework-via-Cross-Model-Distillation-and-4D-Correlation-Mining" class="headerlink" title="DINOv2-powered Few-Shot Semantic Segmentation: A Unified Framework via   Cross-Model Distillation and 4D Correlation Mining"></a>DINOv2-powered Few-Shot Semantic Segmentation: A Unified Framework via   Cross-Model Distillation and 4D Correlation Mining</h2><p><strong>Authors:Wei Zhuo, Zhiyue Tang, Wufeng Xue, Hao Ding, Linlin Shen</strong></p>
<p>Few-shot semantic segmentation has gained increasing interest due to its generalization capability, i.e., segmenting pixels of novel classes requiring only a few annotated images. Prior work has focused on meta-learning for support-query matching, with extensive development in both prototype-based and aggregation-based methods. To address data scarcity, recent approaches have turned to foundation models to enhance representation transferability for novel class segmentation. Among them, a hybrid dual-modal framework including both DINOv2 and SAM has garnered attention due to their complementary capabilities. We wonder â€œcan we build a unified model with knowledge from both foundation models?â€ To this end, we propose FS-DINO, with only DINOv2â€™s encoder and a lightweight segmenter. The segmenter features a bottleneck adapter, a meta-visual prompt generator based on dense similarities and semantic embeddings, and a decoder. Through coarse-to-fine cross-model distillation, we effectively integrate SAMâ€™s knowledge into our lightweight segmenter, which can be further enhanced by 4D correlation mining on support-query pairs. Extensive experiments on COCO-20i, PASCAL-5i, and FSS-1000 demonstrate the effectiveness and superiority of our method. </p>
<blockquote>
<p>å°‘æ ·æœ¬è¯­ä¹‰åˆ†å‰²å› å…¶æ³›åŒ–èƒ½åŠ›è€Œå—åˆ°è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ï¼Œå³å¯¹æ–°å‹ç±»åˆ«è¿›è¡Œåƒç´ åˆ†å‰²æ—¶åªéœ€è¦å¾ˆå°‘æ ‡æ³¨çš„å›¾åƒå³å¯ã€‚å…ˆå‰çš„ç ”ç©¶ä¸»è¦èšç„¦äºåŸºäºæ”¯æŒé›†æŸ¥è¯¢åŒ¹é…çš„å…ƒå­¦ä¹ ï¼Œå¹¶åœ¨åŸºäºåŸå‹å’ŒåŸºäºèšåˆçš„æ–¹æ³•ä¸Šè¿›è¡Œäº†å¤§é‡å¼€å‘ã€‚ä¸ºäº†è§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œè¿‘æœŸçš„æ–¹æ³•è½¬å‘ä½¿ç”¨åŸºç¡€æ¨¡å‹ä»¥å¢å¼ºæ–°å‹ç±»åˆ«åˆ†å‰²çš„è¡¨ç¤ºè¿ç§»èƒ½åŠ›ã€‚å…¶ä¸­ï¼Œä¸€ç§æ··åˆåŒæ¨¡æ€æ¡†æ¶ï¼ŒåŒ…æ‹¬DINOv2å’ŒSAMåœ¨å†…ï¼Œå› å…¶äº’è¡¥èƒ½åŠ›è€Œå—åˆ°å…³æ³¨ã€‚æˆ‘ä»¬æƒ³çŸ¥é“â€œæˆ‘ä»¬èƒ½å¦å»ºç«‹ä¸€ä¸ªèåˆä¸¤ç§åŸºç¡€æ¨¡å‹çŸ¥è¯†çš„ç»Ÿä¸€æ¨¡å‹ï¼Ÿâ€ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†FS-DINOï¼Œä»…é‡‡ç”¨DINOv2çš„ç¼–ç å™¨å’Œè½»é‡çº§åˆ†å‰²å™¨ã€‚åˆ†å‰²å™¨ç‰¹ç‚¹åŒ…æ‹¬ç“¶é¢ˆé€‚é…å™¨ã€åŸºäºå¯†é›†ç›¸ä¼¼æ€§å’Œè¯­ä¹‰åµŒå…¥çš„å…ƒè§†è§‰æç¤ºç”Ÿæˆå™¨ä»¥åŠè§£ç å™¨ã€‚é€šè¿‡ç²—åˆ°ç»†çš„è·¨æ¨¡å‹è’¸é¦ï¼Œæˆ‘ä»¬æœ‰æ•ˆåœ°å°†SAMçš„çŸ¥è¯†é›†æˆåˆ°æˆ‘ä»¬çš„è½»é‡çº§åˆ†å‰²å™¨ä¸­ï¼Œé€šè¿‡æ”¯æŒæŸ¥è¯¢å¯¹ä¸Šçš„4Dç›¸å…³æ€§æŒ–æ˜å¯ä»¥è¿›ä¸€æ­¥å¢å¼ºå…¶æ€§èƒ½ã€‚åœ¨COCO-20iã€PASCAL-5iå’ŒFSS-1000ä¸Šçš„å¤§é‡å®éªŒè¯æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15669v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿™æ˜¯ä¸€ç¯‡å…³äºå°‘æ ·æœ¬è¯­ä¹‰åˆ†å‰²æŠ€æœ¯çš„æ–‡ç« ï¼Œä¸»è¦ä»‹ç»äº†å¦‚ä½•å€ŸåŠ©åŸºç¡€æ¨¡å‹æ¥æå‡æ¨¡å‹å¯¹æ–°å‹ç±»åˆ«çš„æ³›åŒ–èƒ½åŠ›ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§åä¸ºFS-DINOçš„ç»Ÿä¸€æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä»…ä½¿ç”¨DINOv2çš„ç¼–ç å™¨å’Œä¸€ä¸ªè½»é‡çº§åˆ†å‰²å™¨ã€‚é€šè¿‡ç²—åˆ°ç»†çš„è·¨æ¨¡å‹è’¸é¦å’Œ4Då…³è”æŒ–æ˜æŠ€æœ¯ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåœ°æ•´åˆSAMçš„çŸ¥è¯†ï¼Œè¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚åœ¨COCO-20iã€PASCAL-5iå’ŒFSS-1000ç­‰æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å°‘æ ·æœ¬è¯­ä¹‰åˆ†å‰²æŠ€æœ¯å› å…¶åœ¨æ–°å‹ç±»åˆ«ä¸Šçš„æ³›åŒ–èƒ½åŠ›è€Œå—åˆ°å…³æ³¨ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨åŸºäºå…ƒå­¦ä¹ çš„æ”¯æŒæŸ¥è¯¢åŒ¹é…æŠ€æœ¯ã€‚</li>
<li>åŸºç¡€æ¨¡å‹è¢«ç”¨æ¥å¢å¼ºå¯¹æ–°å‹ç±»åˆ«åˆ†å‰²çš„è¡¨ç¤ºè¿ç§»èƒ½åŠ›ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªåä¸ºFS-DINOçš„ç»Ÿä¸€æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç»“åˆäº†DINOv2çš„ç¼–ç å™¨å’Œè½»é‡çº§åˆ†å‰²å™¨ã€‚</li>
<li>é€šè¿‡ç²—åˆ°ç»†çš„è·¨æ¨¡å‹è’¸é¦æŠ€æœ¯æ•´åˆäº†SAMçš„çŸ¥è¯†ã€‚</li>
<li>4Då…³è”æŒ–æ˜æŠ€æœ¯ç”¨äºå¢å¼ºæ¨¡å‹æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15669">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-3f6890d91ebe1960e5599200e3039c30.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f1b3d4db6d74d97dc00881c9c258be31.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-705f4bf8ce37863dbc19816784df5723.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="HV-BEV-Decoupling-Horizontal-and-Vertical-Feature-Sampling-for-Multi-View-3D-Object-Detection"><a href="#HV-BEV-Decoupling-Horizontal-and-Vertical-Feature-Sampling-for-Multi-View-3D-Object-Detection" class="headerlink" title="HV-BEV: Decoupling Horizontal and Vertical Feature Sampling for   Multi-View 3D Object Detection"></a>HV-BEV: Decoupling Horizontal and Vertical Feature Sampling for   Multi-View 3D Object Detection</h2><p><strong>Authors:Di Wu, Feng Yang, Benlian Xu, Pan Liao, Wenhui Zhao, Dingwen Zhang</strong></p>
<p>The application of vision-based multi-view environmental perception system has been increasingly recognized in autonomous driving technology, especially the BEV-based models. Current state-of-the-art solutions primarily encode image features from each camera view into the BEV space through explicit or implicit depth prediction. However, these methods often overlook the structured correlations among different parts of objects in 3D space and the fact that different categories of objects often occupy distinct local height ranges. For example, trucks appear at higher elevations, whereas traffic cones are near the ground. In this work, we propose a novel approach that decouples feature sampling in the \textbf{BEV} grid queries paradigm into \textbf{H}orizontal feature aggregation and \textbf{V}ertical adaptive height-aware reference point sampling (HV-BEV), aiming to improve both the aggregation of objectsâ€™ complete information and awareness of diverse objectsâ€™ height distribution. Specifically, a set of relevant neighboring points is dynamically constructed for each 3D reference point on the ground-aligned horizontal plane, enhancing the association of the same instance across different BEV grids, especially when the instance spans multiple image views around the vehicle. Additionally, instead of relying on uniform sampling within a fixed height range, we introduce a height-aware module that incorporates historical information, enabling the reference points to adaptively focus on the varying heights at which objects appear in different scenes. Extensive experiments validate the effectiveness of our proposed method, demonstrating its superior performance over the baseline across the nuScenes dataset. Moreover, our best-performing model achieves a remarkable 50.5% mAP and 59.8% NDS on the nuScenes testing set. The code is available at <a target="_blank" rel="noopener" href="https://github.com/Uddd821/HV-BEV">https://github.com/Uddd821/HV-BEV</a>. </p>
<blockquote>
<p>åŸºäºè§†è§‰çš„å¤šè§†è§’ç¯å¢ƒæ„ŸçŸ¥ç³»ç»Ÿåœ¨è‡ªåŠ¨é©¾é©¶æŠ€æœ¯ä¸­çš„åº”ç”¨æ—¥ç›Šå—åˆ°é‡è§†ï¼Œå°¤å…¶æ˜¯åŸºäºé¸Ÿç°è§†å›¾ï¼ˆBEVï¼‰çš„æ¨¡å‹ã€‚ç›®å‰æœ€å…ˆè¿›çš„è§£å†³æ–¹æ¡ˆä¸»è¦é€šè¿‡æ˜¾å¼æˆ–éšå¼çš„æ·±åº¦é¢„æµ‹ï¼Œå°†æ¯ä¸ªç›¸æœºè§†è§’çš„å›¾åƒç‰¹å¾ç¼–ç åˆ°é¸Ÿç°è§†å›¾ç©ºé—´ä¸­ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¾€å¾€å¿½è§†äº†ç‰©ä½“ä¸åŒéƒ¨åˆ†åœ¨ä¸‰ç»´ç©ºé—´ä¸­çš„ç»“æ„åŒ–å…³è”ä»¥åŠä¸åŒç±»åˆ«çš„ç‰©ä½“é€šå¸¸å æ®ä¸åŒçš„å±€éƒ¨é«˜åº¦èŒƒå›´è¿™ä¸€äº‹å®ã€‚ä¾‹å¦‚ï¼Œå¡è½¦å‡ºç°åœ¨è¾ƒé«˜çš„åœ°æ–¹ï¼Œè€Œäº¤é€šé”¥åˆ™æ¥è¿‘åœ°é¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œå°†é¸Ÿç°è§†å›¾ç½‘æ ¼æŸ¥è¯¢èŒƒå¼ä¸­çš„ç‰¹å¾é‡‡æ ·è§£è€¦ä¸ºæ°´å¹³ç‰¹å¾èšåˆå’Œå‚ç›´è‡ªé€‚åº”é«˜åº¦æ„ŸçŸ¥å‚è€ƒç‚¹é‡‡æ ·ï¼ˆHV-BEVï¼‰ï¼Œæ—¨åœ¨æ”¹è¿›ç‰©ä½“å®Œæ•´ä¿¡æ¯çš„èšåˆå’Œä¸åŒç‰©ä½“é«˜åº¦åˆ†å¸ƒçš„æ„ŸçŸ¥ã€‚å…·ä½“æ¥è¯´ï¼Œå¯¹äºåœ°é¢ä¸Šå¯¹é½çš„æ°´å¹³å¹³é¢ä¸Šçš„æ¯ä¸ªä¸‰ç»´å‚è€ƒç‚¹ï¼ŒåŠ¨æ€æ„å»ºäº†ä¸€ç»„ç›¸å…³çš„é‚»è¿‘ç‚¹ï¼Œå¢å¼ºäº†åŒä¸€å®ä¾‹åœ¨ä¸åŒé¸Ÿç°è§†å›¾ç½‘æ ¼ä¸­çš„å…³è”ï¼Œç‰¹åˆ«æ˜¯åœ¨å®ä¾‹è·¨è¶Šè½¦è¾†å‘¨å›´å¤šä¸ªå›¾åƒè§†å›¾æ—¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†é«˜åº¦æ„ŸçŸ¥æ¨¡å—ï¼Œè¯¥æ¨¡å—åˆ©ç”¨å†å²ä¿¡æ¯ï¼Œè€Œä¸æ˜¯åœ¨å›ºå®šé«˜åº¦èŒƒå›´å†…è¿›è¡Œå‡åŒ€é‡‡æ ·ï¼Œä½¿å‚è€ƒç‚¹èƒ½å¤Ÿè‡ªé€‚åº”åœ°å…³æ³¨ä¸åŒåœºæ™¯ä¸­ç‰©ä½“å‡ºç°çš„é«˜åº¦å˜åŒ–ã€‚å¤§é‡å®éªŒéªŒè¯äº†æˆ‘ä»¬æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œåœ¨nuScenesæ•°æ®é›†ä¸Šç›¸å¯¹äºåŸºçº¿æ–¹æ³•è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚è€Œä¸”ï¼Œæˆ‘ä»¬è¡¨ç°æœ€ä½³çš„æ¨¡å‹åœ¨nuScenesæµ‹è¯•é›†ä¸Šå–å¾—äº†50.5%çš„mAPå’Œ59.8%çš„NDSã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Uddd821/HV-BEV%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Uddd821/HV-BEVä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18884v3">PDF</a> 13 pages, 7 figures, submitted to T-ITS</p>
<p><strong>Summary</strong></p>
<p>åŸºäºè§†è§‰çš„å¤šè§†è§’ç¯å¢ƒæ„ŸçŸ¥ç³»ç»Ÿåœ¨è‡ªåŠ¨é©¾é©¶æŠ€æœ¯ä¸­å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åŸºäºBEVçš„æ¨¡å‹ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå°†ç‰¹å¾é‡‡æ ·åœ¨BEVç½‘æ ¼æŸ¥è¯¢èŒƒå¼ä¸­è§£è€¦ä¸ºæ°´å¹³ç‰¹å¾èšåˆå’Œå‚ç›´è‡ªé€‚åº”é«˜åº¦æ„ŸçŸ¥å‚è€ƒç‚¹é‡‡æ ·ï¼ˆHV-BEVï¼‰ï¼Œæ—¨åœ¨æ”¹è¿›å¯¹è±¡å®Œæ•´ä¿¡æ¯çš„èšåˆå’Œä¸åŒå¯¹è±¡é«˜åº¦åˆ†å¸ƒçš„æ„ŸçŸ¥ã€‚è¯¥æ–¹æ³•åœ¨nuScenesæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åŸºçº¿æ–¹æ³•ä¸Šå…·æœ‰ä¼˜å¼‚æ€§èƒ½ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬æœ€å¥½çš„æ¨¡å‹åœ¨nuScenesæµ‹è¯•é›†ä¸Šè¾¾åˆ°äº†ä»¤äººç©ç›®çš„50.5%çš„mAPå’Œ59.8%çš„NDSã€‚ä»£ç å¯åœ¨GitHubä¸Šæ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/Uddd821/HV-BEV">https://github.com/Uddd821/HV-BEV</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è‡ªåŠ¨é©¾é©¶æŠ€æœ¯ä¸­ï¼ŒåŸºäºè§†è§‰çš„å¤šè§†è§’ç¯å¢ƒæ„ŸçŸ¥ç³»ç»Ÿå¾—åˆ°å¹¿æ³›åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯BEVæ¨¡å‹ã€‚</li>
<li>å½“å‰ä¸»æµæ–¹æ³•ä¸»è¦é€šè¿‡æ˜¾å¼æˆ–éšå¼æ·±åº¦é¢„æµ‹å°†å„ç›¸æœºè§†è§’çš„å›¾åƒç‰¹å¾ç¼–ç åˆ°BEVç©ºé—´ã€‚</li>
<li>æœ¬æ–‡æå‡ºçš„æ–°æ–¹æ³•HV-BEVï¼Œå°†ç‰¹å¾é‡‡æ ·è§£è€¦ä¸ºæ°´å¹³ç‰¹å¾èšåˆå’Œå‚ç›´è‡ªé€‚åº”é«˜åº¦æ„ŸçŸ¥å‚è€ƒç‚¹é‡‡æ ·ï¼Œä»¥æ”¹è¿›å¯¹è±¡çš„å®Œæ•´ä¿¡æ¯èšåˆå’Œä¸åŒå¯¹è±¡çš„é«˜åº¦åˆ†å¸ƒæ„ŸçŸ¥ã€‚</li>
<li>HV-BEVæ–¹æ³•é€šè¿‡åŠ¨æ€æ„å»ºç›¸å…³é‚»è¿‘ç‚¹ï¼Œå¢å¼ºäº†åŒä¸€å®ä¾‹åœ¨ä¸åŒBEVç½‘æ ¼ä¹‹é—´çš„å…³è”ï¼Œç‰¹åˆ«æ˜¯å½“å®ä¾‹è·¨è¶Šè½¦è¾†å‘¨å›´çš„å¤šä¸ªå›¾åƒè§†å›¾æ—¶ã€‚</li>
<li>è¯¥æ–¹æ³•å¼•å…¥äº†ä¸€ä¸ªè‡ªé€‚åº”é«˜åº¦æ„ŸçŸ¥æ¨¡å—ï¼Œè¯¥æ¨¡å—ç»“åˆäº†å†å²ä¿¡æ¯ï¼Œä½¿å‚è€ƒç‚¹èƒ½å¤Ÿè‡ªé€‚åº”åœ°å…³æ³¨ä¸åŒåœºæ™¯ä¸­å¯¹è±¡å‡ºç°çš„é«˜åº¦å˜åŒ–ã€‚</li>
<li>åœ¨nuScenesæ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå…¶æ€§èƒ½ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18884">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6c10440a8f0cbd2e313240008098e059.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc32fe222fcab80715bba75851039cb3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cacf0f6da2d6f05ba83b0dab643eaf24.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5b21b462c585d81e1eae90c65191c3ef.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-24/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-24/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                    <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-24/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-cbfb56a451dc1566f2cfe5b952c5034d.jpg" class="responsive-img" alt="äººè„¸ç›¸å…³">
                        
                        <span class="card-title">äººè„¸ç›¸å…³</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            äººè„¸ç›¸å…³ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-24  Pose-invariant face recognition via feature-space pose frontalization
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-24
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/" class="post-category">
                                    äººè„¸ç›¸å…³
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                        <span class="chip bg-color">äººè„¸ç›¸å…³</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-24/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-0bafbf63fc12cfe8e4081236d48c3bc0.jpg" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-24  Self-Rewarding Large Vision-Language Models for Optimizing Prompts in   Text-to-Image Generation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-24
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">27927k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
