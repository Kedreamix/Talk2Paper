<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-24  Let Androids Dream of Electric Sheep A Human-like Image Implication   Understanding and Reasoning Framework">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-612c734a99dd1fec3a0ba7edf415c57a.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-24
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    88 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-24-æ›´æ–°"><a href="#2025-05-24-æ›´æ–°" class="headerlink" title="2025-05-24 æ›´æ–°"></a>2025-05-24 æ›´æ–°</h1><h2 id="Let-Androids-Dream-of-Electric-Sheep-A-Human-like-Image-Implication-Understanding-and-Reasoning-Framework"><a href="#Let-Androids-Dream-of-Electric-Sheep-A-Human-like-Image-Implication-Understanding-and-Reasoning-Framework" class="headerlink" title="Let Androids Dream of Electric Sheep: A Human-like Image Implication   Understanding and Reasoning Framework"></a>Let Androids Dream of Electric Sheep: A Human-like Image Implication   Understanding and Reasoning Framework</h2><p><strong>Authors:Chenhao Zhang, Yazhe Niu</strong></p>
<p>Metaphorical comprehension in images remains a critical challenge for AI systems, as existing models struggle to grasp the nuanced cultural, emotional, and contextual implications embedded in visual content. While multimodal large language models (MLLMs) excel in basic Visual Question Answer (VQA) tasks, they struggle with a fundamental limitation on image implication tasks: contextual gaps that obscure the relationships between different visual elements and their abstract meanings. Inspired by the human cognitive process, we propose Let Androids Dream (LAD), a novel framework for image implication understanding and reasoning. LAD addresses contextual missing through the three-stage framework: (1) Perception: converting visual information into rich and multi-level textual representations, (2) Search: iteratively searching and integrating cross-domain knowledge to resolve ambiguity, and (3) Reasoning: generating context-alignment image implication via explicit reasoning. Our framework with the lightweight GPT-4o-mini model achieves SOTA performance compared to 15+ MLLMs on English image implication benchmark and a huge improvement on Chinese benchmark, performing comparable with the GPT-4o model on Multiple-Choice Question (MCQ) and outperforms 36.7% on Open-Style Question (OSQ). Additionally, our work provides new insights into how AI can more effectively interpret image implications, advancing the field of vision-language reasoning and human-AI interaction. Our project is publicly available at <a target="_blank" rel="noopener" href="https://github.com/MING-ZCH/Let-Androids-Dream-of-Electric-Sheep">https://github.com/MING-ZCH/Let-Androids-Dream-of-Electric-Sheep</a>. </p>
<blockquote>
<p>å›¾åƒä¸­çš„éšå–»ç†è§£ä»ç„¶æ˜¯äººå·¥æ™ºèƒ½ç³»ç»Ÿé¢ä¸´çš„ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ï¼Œå› ä¸ºç°æœ‰æ¨¡å‹éš¾ä»¥æŠŠæ¡è§†è§‰å†…å®¹ä¸­åµŒå…¥çš„å¾®å¦™æ–‡åŒ–ã€æƒ…æ„Ÿå’Œä¸Šä¸‹æ–‡å«ä¹‰ã€‚å°½ç®¡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰åœ¨åŸºæœ¬çš„è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å›¾åƒå«ä¹‰ä»»åŠ¡ä¸Šå´å­˜åœ¨åŸºæœ¬å±€é™ï¼šä¸Šä¸‹æ–‡ç¼ºå¤±å¯¼è‡´ä¸åŒè§†è§‰å…ƒç´ åŠå…¶æŠ½è±¡æ„ä¹‰ä¹‹é—´çš„å…³ç³»æ¨¡ç³Šä¸æ¸…ã€‚å—äººç±»è®¤çŸ¥è¿‡ç¨‹çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†â€œè®©æœºå™¨äººåšæ¢¦â€ï¼ˆLADï¼‰è¿™ä¸€å…¨æ–°çš„å›¾åƒå«ä¹‰ç†è§£å’Œæ¨ç†æ¡†æ¶ã€‚LADé€šè¿‡ä¸‰ä¸ªé˜¶æ®µè§£å†³ä¸Šä¸‹æ–‡ç¼ºå¤±é—®é¢˜ï¼šï¼ˆ1ï¼‰æ„ŸçŸ¥ï¼šå°†è§†è§‰ä¿¡æ¯è½¬æ¢ä¸ºä¸°å¯Œä¸”å¤šå±‚æ¬¡çš„æ–‡æœ¬è¡¨ç¤ºï¼›ï¼ˆ2ï¼‰æœç´¢ï¼šè¿­ä»£æœç´¢å¹¶æ•´åˆè·¨åŸŸçŸ¥è¯†ä»¥è§£å†³æ­§ä¹‰ï¼›ï¼ˆ3ï¼‰æ¨ç†ï¼šé€šè¿‡æ˜ç¡®æ¨ç†ç”Ÿæˆä¸ä¸Šä¸‹æ–‡å¯¹é½çš„å›¾åƒå«ä¹‰ã€‚æˆ‘ä»¬çš„æ¡†æ¶ç»“åˆäº†è½»é‡çº§GPT-4o-miniæ¨¡å‹ï¼Œåœ¨è‹±è¯­å›¾åƒå«ä¹‰åŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½è¶…è¿‡äº†15ä¸ªä»¥ä¸Šçš„MLLMï¼Œåœ¨ä¸­æ–‡åŸºå‡†æµ‹è¯•ä¸Šä¹Ÿæœ‰å¾ˆå¤§æ”¹è¿›ã€‚åœ¨å¤šé€‰é¢˜ï¼ˆMCQï¼‰æ–¹é¢ï¼Œæˆ‘ä»¬çš„è¡¨ç°ä¸GPT-4oæ¨¡å‹ç›¸å½“ï¼Œåœ¨å¼€æ”¾æ€§é—®é¢˜ï¼ˆOSQï¼‰ä¸Šè¶…å‡ºäº†36.7%çš„æ°´å¹³ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„å·¥ä½œæä¾›äº†å…³äºAIå¦‚ä½•æ›´æœ‰æ•ˆåœ°è§£é‡Šå›¾åƒå«ä¹‰çš„æ–°è§è§£ï¼Œæ¨åŠ¨äº†è§†è§‰è¯­è¨€æ¨ç†å’Œäººæœºäº¤äº’é¢†åŸŸçš„å‘å±•ã€‚æˆ‘ä»¬çš„é¡¹ç›®å…¬å¼€å¯è®¿é—®äº<a target="_blank" rel="noopener" href="https://github.com/MING-ZCH/Let-Androids-Dream-of-Electric-Sheep%E3%80%82">https://github.com/MING-ZCH/Let-Androids-Dream-of-Electric-Sheepã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17019v1">PDF</a> 16 pages, 9 figures. Code &amp; Dataset:   <a target="_blank" rel="noopener" href="https://github.com/MING-ZCH/Let-Androids-Dream-of-Electric-Sheep">https://github.com/MING-ZCH/Let-Androids-Dream-of-Electric-Sheep</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦æ¢è®¨äººå·¥æ™ºèƒ½åœ¨ç†è§£å›¾åƒéšå–»æ–¹é¢çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ç°æœ‰æ¨¡å‹åœ¨æ•æ‰è§†è§‰å†…å®¹çš„æ–‡åŒ–ã€æƒ…æ„Ÿå’Œè¯­å¢ƒå†…æ¶µæ–¹é¢çš„å±€é™æ€§ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLet Androids Dreamï¼Œç®€ç§°LADï¼‰ï¼Œç”¨äºå›¾åƒéšå–»ç†è§£å’Œæ¨ç†ã€‚LADé€šè¿‡æ„ŸçŸ¥ã€æœç´¢å’Œæ¨ç†ä¸‰ä¸ªé˜¶æ®µæ¥å¡«è¡¥è¯­å¢ƒç¼ºå¤±ï¼Œå®ç°å›¾åƒéšå–»çš„ç†è§£ã€‚è¯¥æ¡†æ¶åœ¨è‹±æ–‡å›¾åƒéšå–»åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°è¾¾åˆ°æœ€æ–°æ°´å¹³ï¼Œå¹¶åœ¨ä¸­æ–‡åŸºå‡†æµ‹è¯•ä¸Šæœ‰æ˜¾è‘—æé«˜ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶ä¸ºAIæ›´æœ‰æ•ˆåœ°è§£é‡Šå›¾åƒå«ä¹‰æä¾›äº†æ–°è§†è§’ï¼Œæ¨åŠ¨äº†è§†è§‰è¯­è¨€æ¨ç†å’Œäººæœºäº¤äº’é¢†åŸŸçš„å‘å±•ã€‚ç›¸å…³é¡¹ç›®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/MING-ZCH/Let-Androids-Dream-of-Electric-Sheep%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/MING-ZCH/Let-Androids-Dream-of-Electric-Sheepæ‰¾åˆ°ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AIåœ¨ç†è§£å›¾åƒéšå–»æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œæ— æ³•æŠŠæ¡è§†è§‰å†…å®¹çš„æ–‡åŒ–ã€æƒ…æ„Ÿå’Œè¯­å¢ƒå†…æ¶µã€‚</li>
<li>ç°æœ‰æ¨¡å‹åœ¨å›¾åƒéšå–»ä»»åŠ¡ä¸Šå­˜åœ¨ä¸Šä¸‹æ–‡ç¼ºå¤±çš„é—®é¢˜ã€‚</li>
<li>Let Androids Dreamï¼ˆLADï¼‰æ¡†æ¶é€šè¿‡æ„ŸçŸ¥ã€æœç´¢å’Œæ¨ç†ä¸‰ä¸ªé˜¶æ®µè§£å†³ä¸Šä¸‹æ–‡ç¼ºå¤±é—®é¢˜ã€‚</li>
<li>LADæ¡†æ¶åœ¨è‹±æ–‡å›¾åƒéšå–»åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¾¾åˆ°æœ€æ–°æ°´å¹³ã€‚</li>
<li>LADæ¡†æ¶åœ¨ä¸­æ–‡å›¾åƒéšå–»ä»»åŠ¡ä¸Šæœ‰æ˜¾è‘—æé«˜ï¼Œä¸GPT-4oæ¨¡å‹è¡¨ç°ç›¸å½“æˆ–æ›´ä¼˜ã€‚</li>
<li>LADæ¡†æ¶ä¸ºAIæ›´æœ‰æ•ˆåœ°è§£é‡Šå›¾åƒå«ä¹‰æä¾›äº†æ–°è§†è§’ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17019">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-a4974cfddf43e33a51124be9b7f934ac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b3b22f11c1a73d19d5166f8faf4b7e1a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-612c734a99dd1fec3a0ba7edf415c57a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b5473767b43f0eab8eb0005d6a4d19c1.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="SophiaVL-R1-Reinforcing-MLLMs-Reasoning-with-Thinking-Reward"><a href="#SophiaVL-R1-Reinforcing-MLLMs-Reasoning-with-Thinking-Reward" class="headerlink" title="SophiaVL-R1: Reinforcing MLLMs Reasoning with Thinking Reward"></a>SophiaVL-R1: Reinforcing MLLMs Reasoning with Thinking Reward</h2><p><strong>Authors:Kaixuan Fan, Kaituo Feng, Haoming Lyu, Dongzhan Zhou, Xiangyu Yue</strong></p>
<p>Recent advances have shown success in eliciting strong reasoning abilities in multimodal large language models (MLLMs) through rule-based reinforcement learning (RL) with outcome rewards. However, this paradigm typically lacks supervision over the thinking process leading to the final outcome.As a result, the model may learn sub-optimal reasoning strategies, which can hinder its generalization ability. In light of this, we propose SophiaVL-R1, as an attempt to add reward signals for the thinking process in this paradigm. To achieve this, we first train a thinking reward model that evaluates the quality of the entire thinking process. Given that the thinking reward may be unreliable for certain samples due to reward hacking, we propose the Trust-GRPO method, which assigns a trustworthiness weight to the thinking reward during training. This weight is computed based on the thinking reward comparison of responses leading to correct answers versus incorrect answers, helping to mitigate the impact of potentially unreliable thinking rewards. Moreover, we design an annealing training strategy that gradually reduces the thinking reward over time, allowing the model to rely more on the accurate rule-based outcome reward in later training stages. Experiments show that our SophiaVL-R1 surpasses a series of reasoning MLLMs on various benchmarks (e.g., MathVisita, MMMU), demonstrating strong reasoning and generalization capabilities. Notably, our SophiaVL-R1-7B even outperforms LLaVA-OneVision-72B on most benchmarks, despite the latter having 10 times more parameters. All code, models, and datasets are made publicly available at <a target="_blank" rel="noopener" href="https://github.com/kxfan2002/SophiaVL-R1">https://github.com/kxfan2002/SophiaVL-R1</a>. </p>
<blockquote>
<p>æœ€è¿‘çš„è¿›å±•è¡¨æ˜ï¼Œé€šè¿‡åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸ç»“æœå¥–åŠ±ç›¸ç»“åˆï¼Œåœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸­æ¿€å‘å¼ºå¤§çš„æ¨ç†èƒ½åŠ›å·²ç»å–å¾—äº†æˆåŠŸã€‚ç„¶è€Œï¼Œè¿™ç§èŒƒå¼é€šå¸¸ç¼ºä¹å¯¹æœ€ç»ˆç»“æœçš„æ€è€ƒè¿‡ç¨‹çš„ç›‘ç£ã€‚å› æ­¤ï¼Œæ¨¡å‹å¯èƒ½ä¼šå­¦ä¹ æ¬¡ä¼˜çš„æ¨ç†ç­–ç•¥ï¼Œè¿™å¯èƒ½ä¼šé˜»ç¢å…¶æ³›åŒ–èƒ½åŠ›ã€‚é‰´äºæ­¤ï¼Œæˆ‘ä»¬æå‡ºSophiaVL-R1ï¼Œè¯•å›¾ä¸ºè¿™ä¸€èŒƒå¼ä¸­çš„æ€è€ƒè¿‡ç¨‹å¢åŠ å¥–åŠ±ä¿¡å·ã€‚ä¸ºå®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬é¦–å…ˆè®­ç»ƒä¸€ä¸ªæ€è€ƒå¥–åŠ±æ¨¡å‹ï¼Œè¯¥æ¨¡å‹è¯„ä¼°æ•´ä¸ªæ€è€ƒè¿‡ç¨‹çš„è´¨é‡ã€‚é‰´äºåœ¨æŸäº›æ ·æœ¬ä¸Šæ€è€ƒå¥–åŠ±å¯èƒ½å› å¥–åŠ±ç ´è§£è€Œä¸å¯é ï¼Œæˆ‘ä»¬æå‡ºäº†Trust-GRPOæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸ºæ€è€ƒå¥–åŠ±åˆ†é…å¯ä¿¡åº¦æƒé‡ã€‚è¯¥æƒé‡æ˜¯åŸºäºæ­£ç¡®ç­”æ¡ˆä¸é”™è¯¯ç­”æ¡ˆæ‰€å¯¼è‡´çš„æ€è€ƒå¥–åŠ±çš„å¯¹æ¯”æ¥è®¡ç®—çš„ï¼Œæœ‰åŠ©äºå‡è½»æ½œåœ¨ä¸å¯é çš„æ€è€ƒå¥–åŠ±çš„å½±å“ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§é€€ç«è®­ç»ƒç­–ç•¥ï¼Œéšç€æ—¶é—´çš„æ¨ç§»é€æ¸é™ä½æ€è€ƒå¥–åŠ±ï¼Œä½¿æ¨¡å‹åœ¨åæœŸè®­ç»ƒé˜¶æ®µæ›´å¤šåœ°ä¾èµ–äºå‡†ç¡®çš„åŸºäºè§„åˆ™çš„æˆæœå¥–åŠ±ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„SophiaVL-R1åœ¨å„ç§åŸºå‡†æµ‹è¯•ï¼ˆä¾‹å¦‚MathVisitaã€MMMUï¼‰ä¸Šè¶…è¶Šäº†ä¸€ç³»åˆ—æ¨ç†MLLMsï¼Œè¡¨ç°å‡ºå¼ºå¤§çš„æ¨ç†å’Œæ³›åŒ–èƒ½åŠ›ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„SophiaVL-R1-7Bç”šè‡³åœ¨å¤§å¤šæ•°åŸºå‡†æµ‹è¯•ä¸Šè¶…è¶Šäº†å‚æ•°æ›´å¤šçš„LLaVA-OneVision-72Bã€‚æ‰€æœ‰ä»£ç ã€æ¨¡å‹å’Œæ•°æ®é›†å‡å…¬å¼€æä¾›åœ¨<a target="_blank" rel="noopener" href="https://github.com/kxfan2002/SophiaVL-R1%E3%80%82">https://github.com/kxfan2002/SophiaVL-R1ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17018v1">PDF</a> Project page:<a target="_blank" rel="noopener" href="https://github.com/kxfan2002/SophiaVL-R1">https://github.com/kxfan2002/SophiaVL-R1</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶é€šè¿‡ç»“åˆè§„åˆ™å¼ºåŒ–å­¦ä¹ ä¸å¥–åŠ±ä¿¡å·ï¼Œæå‡ºäº†SophiaVL-R1æ¨¡å‹ï¼Œæ—¨åœ¨å¢å¼ºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚é’ˆå¯¹ç°æœ‰æ¨¡å‹åœ¨æ€è€ƒè¿‡ç¨‹ç›‘ç£ä¸è¶³ã€å¯èƒ½å­¦ä¹ æ¬¡ä¼˜æ¨ç†ç­–ç•¥çš„é—®é¢˜ï¼ŒSophiaVL-R1é€šè¿‡è®­ç»ƒæ€è€ƒå¥–åŠ±æ¨¡å‹æ¥è¯„ä»·æ•´ä¸ªæ€è€ƒè¿‡ç¨‹çš„è´¨é‡ã€‚åŒæ—¶ï¼Œä¸ºåº”å¯¹æ€è€ƒå¥–åŠ±åœ¨æŸäº›æ ·æœ¬ä¸Šå¯èƒ½å­˜åœ¨çš„ä¸å¯é æ€§ï¼Œç ”ç©¶æå‡ºäº†Trust-GRPOæ–¹æ³•ï¼Œé€šè¿‡è®¡ç®—æ­£ç¡®ä¸é”™è¯¯ç­”æ¡ˆå¯¹åº”çš„æ€è€ƒå¥–åŠ±çš„ä¿¡ä»»åº¦æƒé‡æ¥ä¼˜åŒ–ã€‚æ­¤å¤–ï¼Œè¿˜è®¾è®¡äº†é€€ç«è®­ç»ƒç­–ç•¥ï¼Œé€æ­¥å‡å°‘æ€è€ƒå¥–åŠ±çš„ä¾èµ–ï¼Œä½¿æ¨¡å‹åœ¨åæœŸæ›´å¤šåœ°ä¾èµ–äºå‡†ç¡®çš„è§„åˆ™ç»“æœå¥–åŠ±ã€‚å®éªŒè¡¨æ˜ï¼ŒSophiaVL-R1åœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜äºä¸€ç³»åˆ—æ¨ç†MLLMsï¼ŒåŒ…æ‹¬MathVisitaå’ŒMMMUç­‰ã€‚å…¶å‚æ•°è§„æ¨¡è¾ƒå°çš„SophiaVL-R1-7Bç”šè‡³èƒ½åœ¨å¤šæ•°åŸºå‡†æµ‹è¯•ä¸Šè¶…è¶Šå‚æ•°è§„æ¨¡æ›´å¤§çš„LLaVA-OneVision-72Bã€‚è¯¥ç ”ç©¶çš„ä»£ç ã€æ¨¡å‹å’Œæ•°æ®é›†å·²å…¬å¼€å‘å¸ƒåœ¨GitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶é‡‡ç”¨è§„åˆ™å¼ºåŒ–å­¦ä¹ ä¸å¥–åŠ±ä¿¡å·ç»“åˆçš„æ–¹æ³•ï¼ŒæˆåŠŸå¢å¼ºMLLMsçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>SophiaVL-R1æ¨¡å‹é€šè¿‡è®­ç»ƒæ€è€ƒå¥–åŠ±æ¨¡å‹è¯„ä»·æ€è€ƒè¿‡ç¨‹è´¨é‡ï¼Œæ”¹å–„æ¨¡å‹å¯èƒ½å­¦ä¹ çš„æ¬¡ä¼˜æ¨ç†ç­–ç•¥é—®é¢˜ã€‚</li>
<li>Trust-GRPOæ–¹æ³•ç”¨äºåº”å¯¹æ€è€ƒå¥–åŠ±åœ¨æŸäº›æ ·æœ¬ä¸Šçš„ä¸å¯é æ€§ï¼Œé€šè¿‡è®¡ç®—ä¿¡ä»»åº¦æƒé‡ä¼˜åŒ–å¥–åŠ±åˆ†é…ã€‚</li>
<li>é‡‡ç”¨é€€ç«è®­ç»ƒç­–ç•¥é€æ­¥å‡å°‘å¯¹æ€è€ƒå¥–åŠ±çš„ä¾èµ–ï¼Œå¢å¼ºæ¨¡å‹å¯¹å‡†ç¡®è§„åˆ™ç»“æœå¥–åŠ±çš„ä¾èµ–ã€‚</li>
<li>SophiaVL-R1åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜è¶Šï¼ŒåŒ…æ‹¬MathVisitaå’ŒMMMUç­‰ã€‚</li>
<li>SophiaVL-R1-7Bæ¨¡å‹å³ä½¿å‚æ•°è§„æ¨¡è¾ƒå°ï¼Œä¹Ÿèƒ½åœ¨å¤šæ•°åŸºå‡†æµ‹è¯•ä¸Šè¶…è¶Šæ›´å¤§è§„æ¨¡çš„LLaVA-OneVision-72Bæ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17018">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4287b1bfc30ece86e036dbb8fde1ef33.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7e665fbbf39162732e4348f1dcd9579d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-72f4342affed04709864e9b7390db021.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1bcbf40ee948fed70ec37f9ade0c441c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8584e75a82a1202b3a02313748e43f30.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Delving-into-RL-for-Image-Generation-with-CoT-A-Study-on-DPO-vs-GRPO"><a href="#Delving-into-RL-for-Image-Generation-with-CoT-A-Study-on-DPO-vs-GRPO" class="headerlink" title="Delving into RL for Image Generation with CoT: A Study on DPO vs. GRPO"></a>Delving into RL for Image Generation with CoT: A Study on DPO vs. GRPO</h2><p><strong>Authors:Chengzhuo Tong, Ziyu Guo, Renrui Zhang, Wenyu Shan, Xinyu Wei, Zhenghao Xing, Hongsheng Li, Pheng-Ann Heng</strong></p>
<p>Recent advancements underscore the significant role of Reinforcement Learning (RL) in enhancing the Chain-of-Thought (CoT) reasoning capabilities of large language models (LLMs). Two prominent RL algorithms, Direct Preference Optimization (DPO) and Group Relative Policy Optimization (GRPO), are central to these developments, showcasing different pros and cons. Autoregressive image generation, also interpretable as a sequential CoT reasoning process, presents unique challenges distinct from LLM-based CoT reasoning. These encompass ensuring text-image consistency, improving image aesthetic quality, and designing sophisticated reward models, rather than relying on simpler rule-based rewards. While recent efforts have extended RL to this domain, these explorations typically lack an in-depth analysis of the domain-specific challenges and the characteristics of different RL strategies. To bridge this gap, we provide the first comprehensive investigation of the GRPO and DPO algorithms in autoregressive image generation, evaluating their in-domain performance and out-of-domain generalization, while scrutinizing the impact of different reward models on their respective capabilities. Our findings reveal that GRPO and DPO exhibit distinct advantages, and crucially, that reward models possessing stronger intrinsic generalization capabilities potentially enhance the generalization potential of the applied RL algorithms. Furthermore, we systematically explore three prevalent scaling strategies to enhance both their in-domain and out-of-domain proficiency, deriving unique insights into efficiently scaling performance for each paradigm. We hope our study paves a new path for inspiring future work on developing more effective RL algorithms to achieve robust CoT reasoning in the realm of autoregressive image generation. Code is released at <a target="_blank" rel="noopener" href="https://github.com/ZiyuGuo99/Image-Generation-CoT">https://github.com/ZiyuGuo99/Image-Generation-CoT</a> </p>
<blockquote>
<p>æœ€è¿‘çš„è¿›å±•å¼ºè°ƒäº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†èƒ½åŠ›ä¸­çš„é‡è¦ä½œç”¨ã€‚ä¸¤ä¸ªçªå‡ºçš„å¼ºåŒ–å­¦ä¹ ç®—æ³•â€”â€”ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å’Œç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰â€”â€”æ˜¯è¿™äº›å‘å±•çš„æ ¸å¿ƒï¼Œå±•ç¤ºäº†å„è‡ªçš„ä¼˜åŠ¿å’ŒåŠ£åŠ¿ã€‚è‡ªåŠ¨å›å½’å›¾åƒç”Ÿæˆï¼Œä¹Ÿå¯è§£é‡Šä¸ºä¸€ç§è¿ç»­çš„CoTæ¨ç†è¿‡ç¨‹ï¼Œå‘ˆç°å‡ºä¸åŸºäºLLMçš„CoTæ¨ç†ä¸åŒçš„ç‹¬ç‰¹æŒ‘æˆ˜ã€‚è¿™äº›æŒ‘æˆ˜åŒ…æ‹¬ç¡®ä¿æ–‡æœ¬ä¸å›¾åƒçš„ä¸€è‡´æ€§ã€æé«˜å›¾åƒçš„ç¾å­¦è´¨é‡ï¼Œä»¥åŠè®¾è®¡å¤æ‚å¥–åŠ±æ¨¡å‹ï¼Œè€Œä¸æ˜¯ä¾èµ–æ›´ç®€å•çš„åŸºäºè§„åˆ™çš„å¥–åŠ±ã€‚å°½ç®¡æœ€è¿‘çš„åŠªåŠ›å·²å°†å¼ºåŒ–å­¦ä¹ æ‰©å±•åˆ°äº†è¿™ä¸ªé¢†åŸŸï¼Œä½†è¿™äº›æ¢ç´¢é€šå¸¸ç¼ºä¹å¯¹ç‰¹å®šé¢†åŸŸæŒ‘æˆ˜çš„æ·±å…¥åˆ†æä»¥åŠå¯¹ä¸åŒå¼ºåŒ–å­¦ä¹ ç­–ç•¥çš„ç‰¹æ€§çš„ç ”ç©¶ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¯¹è‡ªåŠ¨å›å½’å›¾åƒç”Ÿæˆä¸­çš„GRPOå’ŒDPOç®—æ³•è¿›è¡Œäº†é¦–æ¬¡å…¨é¢çš„è°ƒæŸ¥ï¼Œè¯„ä¼°äº†å®ƒä»¬çš„åŸŸå†…æ€§èƒ½å’ŒåŸŸå¤–æ³›åŒ–èƒ½åŠ›ï¼ŒåŒæ—¶ä»”ç»†ç ”ç©¶äº†ä¸åŒå¥–åŠ±æ¨¡å‹å¯¹å…¶å„è‡ªèƒ½åŠ›çš„å½±å“ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼ŒGRPOå’ŒDPOè¡¨ç°å‡ºå„è‡ªç‹¬ç‰¹çš„ä¼˜åŠ¿ï¼Œè€Œä¸”é‡è¦çš„æ˜¯ï¼Œå…·æœ‰æ›´å¼ºå†…åœ¨æ³›åŒ–èƒ½åŠ›çš„å¥–åŠ±æ¨¡å‹å¯èƒ½å¢å¼ºäº†æ‰€åº”ç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„æ³›åŒ–æ½œåŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°æ¢ç´¢äº†ä¸‰ç§æµè¡Œçš„æ‰©å±•ç­–ç•¥ï¼Œä»¥æé«˜å®ƒä»¬çš„åŸŸå†…å’ŒåŸŸå¤–ç†Ÿç»ƒç¨‹åº¦ï¼Œä¸ºæ¯ç§æ–¹æ³•çš„æ€§èƒ½æ‰©å±•æä¾›äº†ç‹¬ç‰¹çš„è§è§£ã€‚æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„ç ”ç©¶èƒ½ä¸ºæœªæ¥å¼€å‘æ›´æœ‰æ•ˆçš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ä»¥å®ç°è‡ªåŠ¨å›å½’å›¾åƒç”Ÿæˆçš„ç¨³å¥CoTæ¨ç†çš„å·¥ä½œå¼€è¾Ÿæ–°çš„é“è·¯ã€‚ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/ZiyuGuo99/Image-Generation-CoT">https://github.com/ZiyuGuo99/Image-Generation-CoT</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17017v1">PDF</a> Code is released at <a target="_blank" rel="noopener" href="https://github.com/ZiyuGuo99/Image-Generation-CoT">https://github.com/ZiyuGuo99/Image-Generation-CoT</a></p>
<p><strong>Summary</strong></p>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†èƒ½åŠ›æ–¹é¢æ‰®æ¼”äº†é‡è¦è§’è‰²ã€‚æœ€è¿‘çš„ç ”ç©¶é›†ä¸­æ¢è®¨äº†Direct Preference Optimizationï¼ˆDPOï¼‰å’ŒGroup Relative Policy Optimizationï¼ˆGRPOï¼‰ä¸¤ç§ä¸»è¦RLç®—æ³•çš„åº”ç”¨å’ŒæŒ‘æˆ˜ã€‚åœ¨è‡ªåŠ¨å›¾åƒç”Ÿæˆé¢†åŸŸï¼Œå°½ç®¡å­˜åœ¨ä¸€äº›å°†RLåº”ç”¨äºè¯¥é¢†åŸŸçš„å°è¯•ï¼Œä½†å¯¹äºç‰¹å®šé¢†åŸŸçš„æŒ‘æˆ˜å’Œä¸åŒRLç­–ç•¥çš„ç‰¹ç‚¹çš„æ·±å…¥åˆ†æä»ç„¶ç¼ºä¹ã€‚æœ¬ç ”ç©¶é¦–æ¬¡å…¨é¢æ¢è®¨äº†GRPOå’ŒDPOåœ¨è‡ªåŠ¨å›¾åƒç”Ÿæˆé¢†åŸŸçš„åº”ç”¨ï¼Œè¯„ä¼°äº†å®ƒä»¬çš„åŸŸå†…æ€§èƒ½å’ŒåŸŸå¤–æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶æ·±å…¥æ¢è®¨äº†ä¸åŒå¥–åŠ±æ¨¡å‹å¯¹å…¶èƒ½åŠ›çš„å½±å“ã€‚ç ”ç©¶å‘ç°ï¼ŒGRPOå’ŒDPOå…·æœ‰å„è‡ªçš„ä¼˜åŠ¿ï¼Œå…·æœ‰æ›´å¼ºå†…åœ¨æ³›åŒ–èƒ½åŠ›çš„å¥–åŠ±æ¨¡å‹å¯èƒ½ä¼šæé«˜æ‰€åº”ç”¨RLç®—æ³•çš„æ³›åŒ–æ½œåŠ›ã€‚æ­¤å¤–ï¼Œæœ¬ç ”ç©¶è¿˜ç³»ç»Ÿåœ°æ¢ç´¢äº†ä¸‰ç§æµè¡Œçš„æ‰©å±•ç­–ç•¥ï¼Œä»¥æé«˜è¿™ä¸¤ç§ç®—æ³•çš„åŸŸå†…å’ŒåŸŸå¤–æ€§èƒ½ã€‚å¸Œæœ›æœ¬ç ”ç©¶ä¸ºæœªæ¥å¼€å‘æ›´æœ‰æ•ˆçš„RLç®—æ³•ä»¥å®ç°åœ¨è‡ªåŠ¨å›¾åƒç”Ÿæˆé¢†åŸŸä¸­çš„ç¨³å¥CoTæ¨ç†æä¾›æ–°çš„æ€è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ åœ¨å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„é“¾å¼æ€ç»´æ¨ç†èƒ½åŠ›æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—ä½œç”¨ã€‚</li>
<li>Direct Preference Optimization (DPO) å’Œ Group Relative Policy Optimization (GRPO) æ˜¯æ¨åŠ¨è¿™ä¸€è¿›å±•çš„å…³é”®RLç®—æ³•ã€‚</li>
<li>è‡ªåŠ¨å›¾åƒç”Ÿæˆé¢†åŸŸä¸­çš„é“¾å¼æ€ç»´æ¨ç†å…·æœ‰ç‹¬ç‰¹æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ç¡®ä¿æ–‡æœ¬ä¸å›¾åƒçš„ä¸€è‡´æ€§ã€æé«˜å›¾åƒçš„ç¾å­¦è´¨é‡ä»¥åŠè®¾è®¡å¤æ‚çš„å¥–åŠ±æ¨¡å‹ã€‚</li>
<li>GRPOå’ŒDPOåœ¨è‡ªåŠ¨å›¾åƒç”Ÿæˆä¸­çš„æ€§èƒ½è¯„ä¼°æ˜¾ç¤ºå®ƒä»¬å„è‡ªçš„ä¼˜åŠ¿ã€‚</li>
<li>å…·æœ‰æ›´å¼ºå†…åœ¨æ³›åŒ–èƒ½åŠ›çš„å¥–åŠ±æ¨¡å‹å¯èƒ½æé«˜RLç®—æ³•çš„æ³›åŒ–æ½œåŠ›ã€‚</li>
<li>ç³»ç»Ÿåœ°æ¢ç´¢äº†ä¸‰ç§æ‰©å±•ç­–ç•¥ï¼Œæ—¨åœ¨æé«˜RLç®—æ³•åœ¨åŸŸå†…å’ŒåŸŸå¤–çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17017">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0875b114c3da56ff50622e225ff53df7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d2078dc50658837cf001220cb50c6cad.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0b2510c4e7adf79e2ddf6cf6ce5accac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9e4128c368775092031356c9fe7b433.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-de52380d3e6be37cc816cc81b1cd5991.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Multi-SpatialMLLM-Multi-Frame-Spatial-Understanding-with-Multi-Modal-Large-Language-Models"><a href="#Multi-SpatialMLLM-Multi-Frame-Spatial-Understanding-with-Multi-Modal-Large-Language-Models" class="headerlink" title="Multi-SpatialMLLM: Multi-Frame Spatial Understanding with Multi-Modal   Large Language Models"></a>Multi-SpatialMLLM: Multi-Frame Spatial Understanding with Multi-Modal   Large Language Models</h2><p><strong>Authors:Runsen Xu, Weiyao Wang, Hao Tang, Xingyu Chen, Xiaodong Wang, Fu-Jen Chu, Dahua Lin, Matt Feiszli, Kevin J. Liang</strong></p>
<p>Multi-modal large language models (MLLMs) have rapidly advanced in visual tasks, yet their spatial understanding remains limited to single images, leaving them ill-suited for robotics and other real-world applications that require multi-frame reasoning. In this paper, we propose a framework to equip MLLMs with robust multi-frame spatial understanding by integrating depth perception, visual correspondence, and dynamic perception. Central to our approach is the MultiSPA dataset, a novel, large-scale collection of more than 27 million samples spanning diverse 3D and 4D scenes. Alongside MultiSPA, we introduce a comprehensive benchmark that tests a wide spectrum of spatial tasks under uniform metrics. Our resulting model, Multi-SpatialMLLM, achieves significant gains over baselines and proprietary systems, demonstrating scalable, generalizable multi-frame reasoning. We further observe multi-task benefits and early indications of emergent capabilities in challenging scenarios, and showcase how our model can serve as a multi-frame reward annotator for robotics. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰ä»»åŠ¡ä¸Šå–å¾—äº†å¿«é€Ÿè¿›å±•ï¼Œä½†å®ƒä»¬çš„ç©ºé—´ç†è§£ä»ç„¶ä»…é™äºå•å¹…å›¾åƒï¼Œè¿™ä½¿å¾—å®ƒä»¬ä¸é€‚åˆéœ€è¦å¤šå¸§æ¨ç†çš„æœºå™¨äººå’Œå…¶ä»–ç°å®ä¸–ç•Œåº”ç”¨ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸ºMLLMsé…å¤‡ç¨³å¥çš„å¤šå¸§ç©ºé—´ç†è§£èƒ½åŠ›çš„æ¡†æ¶ï¼Œé€šè¿‡é›†æˆæ·±åº¦æ„ŸçŸ¥ã€è§†è§‰å¯¹åº”å’ŒåŠ¨æ€æ„ŸçŸ¥æ¥å®ç°ã€‚æˆ‘ä»¬æ–¹æ³•çš„æ ¸å¿ƒæ˜¯MultiSPAæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡2700ä¸‡ä¸ªæ ·æœ¬ï¼Œæ¶µç›–å„ç§3Då’Œ4Dåœºæ™¯ã€‚é™¤MultiSPAå¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œè¯¥æµ‹è¯•åœ¨ç»Ÿä¸€æŒ‡æ ‡ä¸‹æµ‹è¯•äº†å¹¿æ³›çš„ç©ºé—´ä»»åŠ¡ã€‚æˆ‘ä»¬å¾—åˆ°çš„æ¨¡å‹â€”â€”Multi-SpatialMLLMï¼Œç›¸è¾ƒäºåŸºå‡†çº¿å’Œä¸“æœ‰ç³»ç»Ÿå–å¾—äº†æ˜¾è‘—çš„æå‡ï¼Œå±•ç¤ºäº†å¯æ‰©å±•ã€å¯é€šç”¨çš„å¤šå¸§æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬è¿˜è§‚å¯Ÿåˆ°äº†å¤šä»»åŠ¡çš„ç›Šå¤„ä»¥åŠå…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ä¸­æ–°å…´èƒ½åŠ›çš„æ—©æœŸè¿¹è±¡ï¼Œå¹¶å±•ç¤ºäº†æˆ‘ä»¬çš„æ¨¡å‹å¦‚ä½•ä½œä¸ºå¤šå¸§å¥–åŠ±æ ‡æ³¨å™¨ä¸ºæœºå™¨äººæœåŠ¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17015v1">PDF</a> 24 pages. An MLLM, dataset, and benchmark for multi-frame spatial   understanding. Project page: <a target="_blank" rel="noopener" href="https://runsenxu.com/projects/Multi-SpatialMLLM">https://runsenxu.com/projects/Multi-SpatialMLLM</a></p>
<p><strong>Summary</strong></p>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰ä»»åŠ¡ä¸Šå–å¾—äº†å¿«é€Ÿè¿›å±•ï¼Œä½†åœ¨ç©ºé—´ç†è§£æ–¹é¢ä»ç„¶å±€é™äºå•å¹…å›¾åƒï¼Œè¿™ä½¿å¾—å®ƒä»¬å¯¹äºéœ€è¦å¤šå¸§æ¨ç†çš„æœºå™¨äººå’Œå…¶ä»–ç°å®ä¸–ç•Œåº”ç”¨å­˜åœ¨ä¸é€‚ç”¨çš„é—®é¢˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ¡†æ¶ï¼Œé€šè¿‡æ•´åˆæ·±åº¦æ„ŸçŸ¥ã€è§†è§‰å¯¹åº”å’ŒåŠ¨æ€æ„ŸçŸ¥ï¼Œä¸ºMLLMsæä¾›ç¨³å¥çš„å¤šå¸§ç©ºé—´ç†è§£èƒ½åŠ›ã€‚å…³é”®æ–¹æ³•åŒ…æ‹¬æ„å»ºMultiSPAæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«è¶…è¿‡27ä¸‡ä¸ªæ ·æœ¬çš„å¤§å‹å¤šæ ·3Då’Œ4Dåœºæ™¯æ•°æ®é›†ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸€ä¸ªç»¼åˆåŸºå‡†æµ‹è¯•ï¼Œä»¥ç»Ÿä¸€çš„æŒ‡æ ‡æµ‹è¯•ä¸€ç³»åˆ—ç©ºé—´ä»»åŠ¡ã€‚æ‰€å¾—åˆ°çš„æ¨¡å‹Multi-SpatialMLLMåœ¨åŸºå‡†æµ‹è¯•å’Œä¸“æœ‰ç³»ç»Ÿæ–¹é¢å–å¾—äº†æ˜¾è‘—çš„ä¼˜åŠ¿ï¼Œå±•ç¤ºäº†å¯æ‰©å±•çš„ã€å¯é€šç”¨çš„å¤šå¸§æ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¿˜è§‚å¯Ÿåˆ°äº†å¤šä»»åŠ¡çš„ä¼˜åŠ¿ä»¥åŠåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ä¸­çš„æ½œåœ¨èƒ½åŠ›ï¼Œå¹¶å±•ç¤ºäº†è¯¥æ¨¡å‹å¯ä»¥ä½œä¸ºæœºå™¨äººçš„å¤šå¸§å¥–åŠ±æ³¨é‡Šå™¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLMsåœ¨è§†è§‰ä»»åŠ¡ä¸Šå–å¾—äº†è¿›å±•ï¼Œä½†åœ¨éœ€è¦å¤šå¸§æ¨ç†çš„æœºå™¨äººç­‰åº”ç”¨ä¸­ä»å—é™ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶å’ŒMultiSPAæ•°æ®é›†ï¼Œä»¥æé«˜MLLMsçš„å¤šå¸§ç©ºé—´ç†è§£èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡æ•´åˆæ·±åº¦æ„ŸçŸ¥ã€è§†è§‰å¯¹åº”å’ŒåŠ¨æ€æ„ŸçŸ¥æ¥å®ç°ç¨³å¥çš„å¤šå¸§ç©ºé—´ç†è§£ã€‚</li>
<li>MultiSPAæ•°æ®é›†åŒ…å«å¤§é‡å¤šæ ·çš„3Då’Œ4Dåœºæ™¯æ ·æœ¬ã€‚</li>
<li>å¼•å…¥äº†ç»¼åˆåŸºå‡†æµ‹è¯•ï¼Œä»¥è¯„ä¼°æ¨¡å‹åœ¨å„ç§ç©ºé—´ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>Multi-SpatialMLLMæ¨¡å‹åœ¨åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå…·æœ‰å¯æ‰©å±•çš„å¤šå¸§æ¨ç†èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17015">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-931f79dc85a53ebac09115c3d5fe162e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2e879038295c2c2d8157264ad148950d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-27f87fc5f42bb2baa2c1ec36a5cb4973.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7117ed38b1517a45cce915d9b4d86672.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-40544f011cd55d3497dc6bab4ed46076.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="R1-Searcher-Incentivizing-the-Dynamic-Knowledge-Acquisition-of-LLMs-via-Reinforcement-Learning"><a href="#R1-Searcher-Incentivizing-the-Dynamic-Knowledge-Acquisition-of-LLMs-via-Reinforcement-Learning" class="headerlink" title="R1-Searcher++: Incentivizing the Dynamic Knowledge Acquisition of LLMs   via Reinforcement Learning"></a>R1-Searcher++: Incentivizing the Dynamic Knowledge Acquisition of LLMs   via Reinforcement Learning</h2><p><strong>Authors:Huatong Song, Jinhao Jiang, Wenqing Tian, Zhipeng Chen, Yuhuan Wu, Jiahao Zhao, Yingqian Min, Wayne Xin Zhao, Lei Fang, Ji-Rong Wen</strong></p>
<p>Large Language Models (LLMs) are powerful but prone to hallucinations due to static knowledge. Retrieval-Augmented Generation (RAG) helps by injecting external information, but current methods often are costly, generalize poorly, or ignore the internal knowledge of the model. In this paper, we introduce R1-Searcher++, a novel framework designed to train LLMs to adaptively leverage both internal and external knowledge sources. R1-Searcher++ employs a two-stage training strategy: an initial SFT Cold-start phase for preliminary format learning, followed by RL for Dynamic Knowledge Acquisition. The RL stage uses outcome-supervision to encourage exploration, incorporates a reward mechanism for internal knowledge utilization, and integrates a memorization mechanism to continuously assimilate retrieved information, thereby enriching the modelâ€™s internal knowledge. By leveraging internal knowledge and external search engine, the model continuously improves its capabilities, enabling efficient retrieval-augmented reasoning. Our experiments demonstrate that R1-Searcher++ outperforms previous RAG and reasoning methods and achieves efficient retrieval. The code is available at <a target="_blank" rel="noopener" href="https://github.com/RUCAIBox/R1-Searcher-plus">https://github.com/RUCAIBox/R1-Searcher-plus</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è™½ç„¶å¼ºå¤§ï¼Œä½†ç”±äºé™æ€çŸ¥è¯†è€Œå®¹æ˜“å‡ºç°å¹»è§‰ã€‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰é€šè¿‡æ³¨å…¥å¤–éƒ¨ä¿¡æ¯æ¥å¸®åŠ©è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†å½“å‰çš„æ–¹æ³•å¾€å¾€æˆæœ¬é«˜æ˜‚ã€é€šç”¨æ€§è¾ƒå·®æˆ–å¿½ç•¥äº†æ¨¡å‹çš„å†…éƒ¨çŸ¥è¯†ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†R1-Searcher++ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨è®­ç»ƒLLMä»¥è‡ªé€‚åº”åœ°åˆ©ç”¨å†…éƒ¨å’Œå¤–éƒ¨çŸ¥è¯†æºã€‚R1-Searcher++é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼šåˆå§‹çš„SFTå†·å¯åŠ¨é˜¶æ®µç”¨äºåˆæ­¥æ ¼å¼å­¦ä¹ ï¼Œå…¶æ¬¡æ˜¯ç”¨äºåŠ¨æ€çŸ¥è¯†è·å–çš„å¼ºåŒ–å­¦ä¹ ã€‚å¼ºåŒ–å­¦ä¹ é˜¶æ®µä½¿ç”¨ç»“æœç›‘ç£æ¥é¼“åŠ±æ¢ç´¢ï¼Œèå…¥å†…éƒ¨çŸ¥è¯†åˆ©ç”¨çš„å¥–åŠ±æœºåˆ¶ï¼Œå¹¶æ•´åˆè®°å¿†æœºåˆ¶ä»¥æŒç»­åŒåŒ–æ£€ç´¢çš„ä¿¡æ¯ï¼Œä»è€Œä¸°å¯Œæ¨¡å‹çš„å†…éƒ¨çŸ¥è¯†ã€‚é€šè¿‡åˆ©ç”¨å†…éƒ¨çŸ¥è¯†å’Œå¤–éƒ¨æœç´¢å¼•æ“ï¼Œæ¨¡å‹çš„èƒ½åŠ›å¾—åˆ°æŒç»­æé«˜ï¼Œå®ç°äº†é«˜æ•ˆçš„æ£€ç´¢å¢å¼ºæ¨ç†ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒR1-Searcher++ä¼˜äºå…ˆå‰çš„RAGå’Œæ¨ç†æ–¹æ³•ï¼Œå®ç°äº†é«˜æ•ˆçš„æ£€ç´¢ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/RUCAIBox/R1-Searcher-plus%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/RUCAIBox/R1-Searcher-plusæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17005v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LLMsé€šè¿‡é™æ€çŸ¥è¯†å®¹æ˜“å‡ºç°å¹»è§‰ï¼Œè€Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰é€šè¿‡æ³¨å…¥å¤–éƒ¨ä¿¡æ¯æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚å½“å‰æ–¹æ³•æˆæœ¬é«˜ã€æ¨å¹¿æ€§å·®æˆ–å¿½ç•¥æ¨¡å‹å†…éƒ¨çŸ¥è¯†ã€‚æœ¬æ–‡ä»‹ç»R1-Searcher++æ¡†æ¶ï¼Œé‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œåˆæ­¥æ ¼å¼å­¦ä¹ é˜¶æ®µçš„SFT Cold-startä¸åŠ¨æ€çŸ¥è¯†è·å–çš„RLé˜¶æ®µã€‚RLé˜¶æ®µé‡‡ç”¨ç»“æœç›‘ç£é¼“åŠ±æ¢ç´¢ï¼Œèå…¥å†…éƒ¨çŸ¥è¯†åˆ©ç”¨çš„å¥–åŠ±æœºåˆ¶ï¼Œå¹¶æ•´åˆè®°å¿†æœºåˆ¶æŒç»­åŒåŒ–æ£€ç´¢ä¿¡æ¯ï¼Œä¸°å¯Œæ¨¡å‹å†…éƒ¨çŸ¥è¯†ã€‚ç»“åˆå†…éƒ¨çŸ¥è¯†ä¸å¤–éƒ¨æœç´¢å¼•æ“ï¼Œæ¨¡å‹èƒ½åŠ›æŒç»­æå‡ï¼Œå®ç°é«˜æ•ˆæ£€ç´¢å¢å¼ºæ¨ç†ã€‚å®éªŒæ˜¾ç¤ºR1-Searcher++ä¼˜äºå…ˆå‰çš„RAGå’Œæ¨ç†æ–¹æ³•ï¼Œå®ç°é«˜æ•ˆæ£€ç´¢ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsç”±äºä¾èµ–é™æ€çŸ¥è¯†å®¹æ˜“å‡ºç°å¹»è§‰ï¼Œéœ€è¦æ³¨å…¥å¤–éƒ¨ä¿¡æ¯æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>å½“å‰è§£å†³æ­¤é—®é¢˜çš„æ–¹æ³•å­˜åœ¨æˆæœ¬é«˜ã€æ¨å¹¿æ€§å·®æˆ–å¿½ç•¥æ¨¡å‹å†…éƒ¨çŸ¥è¯†çš„é—®é¢˜ã€‚</li>
<li>R1-Searcher++æ¡†æ¶é€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œç»“åˆå†…éƒ¨å’Œå¤–éƒ¨çŸ¥è¯†æºæ¥æå‡æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>R1-Searcher++é‡‡ç”¨åˆå§‹çš„SFT Cold-starté˜¶æ®µè¿›è¡Œåˆæ­¥æ ¼å¼å­¦ä¹ ï¼Œç„¶åæ˜¯RLé˜¶æ®µçš„åŠ¨æ€çŸ¥è¯†è·å–ã€‚</li>
<li>RLé˜¶æ®µé‡‡ç”¨ç»“æœç›‘ç£é¼“åŠ±æ¢ç´¢ï¼Œå¹¶èå…¥å†…éƒ¨çŸ¥è¯†åˆ©ç”¨çš„å¥–åŠ±æœºåˆ¶å’Œè®°å¿†æœºåˆ¶ã€‚</li>
<li>R1-Searcher++èƒ½ç»“åˆå†…éƒ¨çŸ¥è¯†å’Œå¤–éƒ¨æœç´¢å¼•æ“ï¼ŒæŒç»­æå‡æ¨¡å‹èƒ½åŠ›ï¼Œå®ç°é«˜æ•ˆæ£€ç´¢å¢å¼ºæ¨ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17005">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2732a6aa7ef87d646c961d16a4ae9f1f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48d8c000f3f83bff59d7c9375e186045.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Do-Large-Language-Models-Excel-in-Complex-Logical-Reasoning-with-Formal-Language"><a href="#Do-Large-Language-Models-Excel-in-Complex-Logical-Reasoning-with-Formal-Language" class="headerlink" title="Do Large Language Models Excel in Complex Logical Reasoning with Formal   Language?"></a>Do Large Language Models Excel in Complex Logical Reasoning with Formal   Language?</h2><p><strong>Authors:Jin Jiang, Jianing Wang, Yuchen Yan, Yang Liu, Jianhua Zhu, Mengdi Zhang, Xunliang Cai, Liangcai Gao</strong></p>
<p>Large Language Models (LLMs) have been shown to achieve breakthrough performance on complex logical reasoning tasks. Nevertheless, most existing research focuses on employing formal language to guide LLMs to derive reliable reasoning paths, while systematic evaluations of these capabilities are still limited. In this paper, we aim to conduct a comprehensive evaluation of LLMs across various logical reasoning problems utilizing formal languages. From the perspective of three dimensions, i.e., spectrum of LLMs, taxonomy of tasks, and format of trajectories, our key findings are: 1) Thinking models significantly outperform Instruct models, especially when formal language is employed; 2) All LLMs exhibit limitations in inductive reasoning capability, irrespective of whether they use a formal language; 3) Data with PoT format achieves the best generalization performance across other languages. Additionally, we also curate the formal-relative training data to further enhance the small language models, and the experimental results indicate that a simple rejected fine-tuning method can better enable LLMs to generalize across formal languages and achieve the best overall performance. Our codes and reports are available at <a target="_blank" rel="noopener" href="https://github.com/jiangjin1999/FormalEval">https://github.com/jiangjin1999/FormalEval</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²åœ¨å¤æ‚çš„é€»è¾‘æ¨ç†ä»»åŠ¡ä¸­å–å¾—äº†çªç ´æ€§çš„è¡¨ç°ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤§å¤šæ•°ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ä½¿ç”¨æ­£å¼è¯­è¨€æ¥æŒ‡å¯¼LLMæ¨å¯¼å¯é çš„æ¨ç†è·¯å¾„ï¼Œè€Œå¯¹è¿™äº›èƒ½åŠ›çš„ç³»ç»Ÿè¯„ä¼°ä»ç„¶æœ‰é™ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ—¨åœ¨åˆ©ç”¨æ­£å¼è¯­è¨€ï¼Œå¯¹å„ç§é€»è¾‘æ¨ç†é—®é¢˜å¯¹LLMè¿›è¡Œå…¨é¢è¯„ä¼°ã€‚ä»ä¸‰ä¸ªç»´åº¦ï¼ˆå³LLMçš„é¢‘è°±ã€ä»»åŠ¡åˆ†ç±»å’Œè½¨è¿¹æ ¼å¼ï¼‰æ¥çœ‹ï¼Œæˆ‘ä»¬çš„ä¸»è¦å‘ç°å¦‚ä¸‹ï¼š1ï¼‰æ€è€ƒæ¨¡å‹åœ¨é‡‡ç”¨æ­£å¼è¯­è¨€æ—¶æ˜¾è‘—ä¼˜äºæŒ‡ä»¤æ¨¡å‹ï¼›2ï¼‰æ— è®ºæ˜¯å¦ä½¿ç”¨æ­£å¼è¯­è¨€ï¼Œæ‰€æœ‰LLMåœ¨å½’çº³æ¨ç†èƒ½åŠ›æ–¹é¢éƒ½è¡¨ç°å‡ºå±€é™æ€§ï¼›3ï¼‰é‡‡ç”¨PoTæ ¼å¼çš„æ•°æ®åœ¨å…¶ä»–è¯­è¨€ä¸­å®ç°äº†æœ€ä½³æ³›åŒ–æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ•´ç†äº†ä¸å½¢å¼ç›¸å…³çš„è®­ç»ƒæ•°æ®ï¼Œä»¥è¿›ä¸€æ­¥å¢å¼ºå°å‹è¯­è¨€æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç®€å•çš„æ‹’ç»å¾®è°ƒæ–¹æ³•èƒ½æ›´å¥½åœ°ä½¿LLMåœ¨æ­£å¼è¯­è¨€ä¹‹é—´æ³›åŒ–ï¼Œå¹¶è¾¾åˆ°æœ€ä½³çš„æ•´ä½“æ€§èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç å’ŒæŠ¥å‘Šå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/jiangjin1999/FormalEval">https://github.com/jiangjin1999/FormalEval</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16998v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å…¨é¢è¯„ä¼°äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¸åŒé€»è¾‘æ¨ç†é—®é¢˜ä¸Šçš„è¡¨ç°ï¼Œä¸»è¦å‘ç°åŒ…æ‹¬ï¼š1ï¼‰é‡‡ç”¨æ­£å¼è¯­è¨€çš„Thinkingæ¨¡å‹åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºInstructæ¨¡å‹ï¼›2ï¼‰æ‰€æœ‰LLMsåœ¨å½’çº³æ¨ç†èƒ½åŠ›ä¸Šå‡å­˜åœ¨å±€é™æ€§ï¼Œæ— è®ºæ˜¯å¦ä½¿ç”¨æ­£å¼è¯­è¨€ï¼›3ï¼‰é‡‡ç”¨PoTæ ¼å¼çš„æ•°æ®åœ¨å…¶ä»–è¯­è¨€ä¸Šå®ç°äº†æœ€ä½³æ³›åŒ–æ€§èƒ½ï¼›4ï¼‰é€šè¿‡æ”¶é›†æ­£å¼ç›¸å…³çš„è®­ç»ƒæ•°æ®è¿›ä¸€æ­¥æ”¹è¿›å°å‹è¯­è¨€æ¨¡å‹ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œç®€å•çš„æ‹’ç»å¾®è°ƒæ–¹æ³•èƒ½å¤Ÿæ›´å¥½åœ°ä½¿LLMsåœ¨æ­£å¼è¯­è¨€ä¸Šæ³›åŒ–å¹¶å–å¾—æœ€ä½³æ€»ä½“æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Thinkingæ¨¡å‹åœ¨è¿ç”¨æ­£å¼è¯­è¨€æ—¶æ˜¾è‘—ä¼˜äºInstructæ¨¡å‹ã€‚</li>
<li>æ‰€æœ‰LLMsåœ¨å½’çº³æ¨ç†æ–¹é¢éƒ½å­˜åœ¨å±€é™æ€§ï¼Œæ— è®ºæ˜¯å¦ä½¿ç”¨æ­£å¼è¯­è¨€ã€‚</li>
<li>PoTæ ¼å¼çš„æ•°æ®åœ¨è·¨è¯­è¨€æ³›åŒ–æ–¹é¢è¡¨ç°æœ€ä½³ã€‚</li>
<li>æ”¶é›†æ­£å¼ç›¸å…³çš„è®­ç»ƒæ•°æ®å¯æ”¹è¿›å°å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>ç®€å•çš„æ‹’ç»å¾®è°ƒæ–¹æ³•æœ‰åŠ©äºæé«˜LLMsåœ¨æ­£å¼è¯­è¨€ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ç ”ç©¶æˆæœä»£ç å’ŒæŠ¥å‘Šå¯åœ¨çº¿è·å–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16998">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d3b6f7152f8566f1397b5e3609f61697.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ebc3f63e78d1e736de670f9896c1d4ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ecf7db8d3c6f1cf109663f117a4b2cea.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="DecoupledESC-Enhancing-Emotional-Support-Generation-via-Strategy-Response-Decoupled-Preference-Optimization"><a href="#DecoupledESC-Enhancing-Emotional-Support-Generation-via-Strategy-Response-Decoupled-Preference-Optimization" class="headerlink" title="DecoupledESC: Enhancing Emotional Support Generation via   Strategy-Response Decoupled Preference Optimization"></a>DecoupledESC: Enhancing Emotional Support Generation via   Strategy-Response Decoupled Preference Optimization</h2><p><strong>Authors:Chao Zhang, Xin Shi, Xueqiao Zhang, Yifan Zhu, Yi Yang, Yawei Luo</strong></p>
<p>Recent advances in Emotional Support Conversation (ESC) have improved emotional support generation by fine-tuning Large Language Models (LLMs) via Supervised Fine-Tuning (SFT). However, common psychological errors still persist. While Direct Preference Optimization (DPO) shows promise in reducing such errors through pairwise preference learning, its effectiveness in ESC tasks is limited by two key challenges: (1) Entangled data structure: Existing ESC data inherently entangles psychological strategies and response content, making it difficult to construct high-quality preference pairs; and (2) Optimization ambiguity: Applying vanilla DPO to such entangled pairwise data leads to ambiguous training objectives. To address these issues, we introduce Inferential Preference Mining (IPM) to construct high-quality preference data, forming the IPM-PrefDial dataset. Building upon this data, we propose a Decoupled ESC framework inspired by Grossâ€™s Extended Process Model of Emotion Regulation, which decomposes the ESC task into two sequential subtasks: strategy planning and empathic response generation. Each was trained via SFT and subsequently enhanced by DPO to align with the psychological preference. Extensive experiments demonstrate that our Decoupled ESC framework outperforms joint optimization baselines, reducing preference bias and improving response quality. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œæƒ…æ„Ÿæ”¯æŒå¯¹è¯ï¼ˆESCï¼‰çš„è¿›å±•é€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œå¾®è°ƒï¼Œæé«˜äº†æƒ…æ„Ÿæ”¯æŒç”Ÿæˆçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå¸¸è§çš„å¿ƒç†é”™è¯¯ä»ç„¶æŒç»­å­˜åœ¨ã€‚è™½ç„¶ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰é€šè¿‡é…å¯¹åå¥½å­¦ä¹ æ˜¾ç¤ºå‡ºå‡å°‘è¿™äº›é”™è¯¯çš„æ½œåŠ›ï¼Œä½†åœ¨ESCä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§å—åˆ°ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜çš„é™åˆ¶ï¼šï¼ˆ1ï¼‰çº ç¼ çš„æ•°æ®ç»“æ„ï¼šç°æœ‰çš„ESCæ•°æ®å¤©ç”Ÿåœ°å°†å¿ƒç†ç­–ç•¥å’Œå“åº”å†…å®¹çº ç¼ åœ¨ä¸€èµ·ï¼Œéš¾ä»¥æ„å»ºé«˜è´¨é‡çš„åå¥½å¯¹ï¼›ï¼ˆ2ï¼‰ä¼˜åŒ–æ¨¡ç³Šæ€§ï¼šå°†æ™®é€šçš„DPOåº”ç”¨äºè¿™ç§çº ç¼ çš„é…å¯¹æ•°æ®ä¼šå¯¼è‡´è®­ç»ƒç›®æ ‡æ¨¡ç³Šã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥æ¨ç†åå¥½æŒ–æ˜ï¼ˆIPMï¼‰æ¥æ„å»ºé«˜è´¨é‡çš„åå¥½æ•°æ®ï¼Œå½¢æˆIPM-PrefDialæ•°æ®é›†ã€‚åŸºäºè¿™äº›æ•°æ®ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå—Grossæ‰©å±•çš„æƒ…ç»ªè°ƒèŠ‚è¿‡ç¨‹æ¨¡å‹å¯å‘çš„è§£è€¦ESCæ¡†æ¶ï¼Œå®ƒå°†ESCä»»åŠ¡åˆ†è§£ä¸ºä¸¤ä¸ªé¡ºåºçš„å­ä»»åŠ¡ï¼šç­–ç•¥è§„åˆ’å’Œå…±æƒ…å“åº”ç”Ÿæˆã€‚æ¯ä¸ªå­ä»»åŠ¡éƒ½é€šè¿‡SFTè¿›è¡Œè®­ç»ƒï¼Œéšåé€šè¿‡DPOå¢å¼ºä»¥ç¬¦åˆå¿ƒç†åå¥½ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„è§£è€¦ESCæ¡†æ¶ä¼˜äºè”åˆä¼˜åŒ–åŸºçº¿ï¼Œå‡å°‘äº†åå¥½åè§ï¼Œæé«˜äº†å“åº”è´¨é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16995v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>æœ€æ–°ç ”ç©¶é€šè¿‡å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆæƒ…æ„Ÿæ”¯æŒå¯¹è¯ï¼ˆESCï¼‰ï¼Œå‡å°‘äº†å¿ƒç†é”™è¯¯ã€‚è™½ç„¶ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰åœ¨å‡å°‘æ­¤ç±»é”™è¯¯æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†åœ¨ESCä»»åŠ¡ä¸­é¢ä¸´ä¸¤ä¸ªæŒ‘æˆ˜ï¼šçº ç¼ çš„æ•°æ®ç»“æ„å’Œä¼˜åŒ–æ¨¡ç³Šæ€§ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œå¼•å…¥æ¨ç†åå¥½æŒ–æ˜ï¼ˆIPMï¼‰æ„å»ºé«˜è´¨é‡åå¥½æ•°æ®ï¼Œå½¢æˆIPM-PrefDialæ•°æ®é›†ã€‚åŸºäºè¯¥æ•°æ®é›†ï¼Œæå‡ºäº†å—Grossæƒ…ç»ªè°ƒèŠ‚æ‰©å±•è¿‡ç¨‹æ¨¡å‹å¯å‘çš„è§£è€¦ESCæ¡†æ¶ï¼Œå°†ESCä»»åŠ¡åˆ†è§£ä¸ºç­–ç•¥è§„åˆ’å’Œå…±æƒ…å“åº”ç”Ÿæˆä¸¤ä¸ªè¿ç»­å­ä»»åŠ¡ã€‚é€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è¿›è¡Œè®­ç»ƒï¼Œå¹¶é€šè¿‡DPOä¸å¿ƒç†åå¥½å¯¹é½ã€‚å®éªŒè¡¨æ˜ï¼Œè§£è€¦ESCæ¡†æ¶ä¼˜äºè”åˆä¼˜åŒ–åŸºçº¿ï¼Œé™ä½äº†åå¥½åè§ï¼Œæé«˜äº†å“åº”è´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ul>
<li>LLMsé€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ç”¨äºç”Ÿæˆæƒ…æ„Ÿæ”¯æŒå¯¹è¯ï¼ˆESCï¼‰ï¼Œä½†å­˜åœ¨å¿ƒç†é”™è¯¯ã€‚</li>
<li>ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰åœ¨å‡å°‘è¿™äº›é”™è¯¯æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†é¢ä¸´æ•°æ®çº ç¼ å’Œä¼˜åŒ–æ¨¡ç³Šæ€§ä¸¤å¤§æŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥æ¨ç†åå¥½æŒ–æ˜ï¼ˆIPMï¼‰æ„å»ºé«˜è´¨é‡åå¥½æ•°æ®ï¼Œåˆ›å»ºIPM-PrefDialæ•°æ®é›†ã€‚</li>
<li>æå‡ºåŸºäºGrossæƒ…ç»ªè°ƒèŠ‚æ‰©å±•è¿‡ç¨‹æ¨¡å‹çš„è§£è€¦ESCæ¡†æ¶ï¼Œå°†ESCä»»åŠ¡åˆ†ä¸ºç­–ç•¥è§„åˆ’å’Œå…±æƒ…å“åº”ä¸¤ä¸ªå­ä»»åŠ¡ã€‚</li>
<li>é€šè¿‡SFTè®­ç»ƒå­ä»»åŠ¡ï¼Œå¹¶ç”¨DPOä¸å¿ƒç†åå¥½å¯¹é½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16995">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-87a1d686e6a2423bf652c0e8b35a8650.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fda15160a30dcc6a946c77498ab83130.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-628b540dfd383b87f9e22674a61578e5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1ef02a4701657750ffaa0be3d48e903b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd9ddc3824074f8cfb5f2bb1374f302d.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Dimple-Discrete-Diffusion-Multimodal-Large-Language-Model-with-Parallel-Decoding"><a href="#Dimple-Discrete-Diffusion-Multimodal-Large-Language-Model-with-Parallel-Decoding" class="headerlink" title="Dimple: Discrete Diffusion Multimodal Large Language Model with Parallel   Decoding"></a>Dimple: Discrete Diffusion Multimodal Large Language Model with Parallel   Decoding</h2><p><strong>Authors:Runpeng Yu, Xinyin Ma, Xinchao Wang</strong></p>
<p>In this work, we propose Dimple, the first Discrete Diffusion Multimodal Large Language Model (DMLLM). We observe that training with a purely discrete diffusion approach leads to significant training instability, suboptimal performance, and severe length bias issues. To address these challenges, we design a novel training paradigm that combines an initial autoregressive phase with a subsequent diffusion phase. This approach yields the Dimple-7B model, trained on the same dataset and using a similar training pipeline as LLaVA-NEXT. Dimple-7B ultimately surpasses LLaVA-NEXT in performance by 3.9%, demonstrating that DMLLM can achieve performance comparable to that of autoregressive models. To improve inference efficiency, we propose a decoding strategy termed confident decoding, which dynamically adjusts the number of tokens generated at each step, significantly reducing the number of generation iterations. In autoregressive models, the number of forward iterations during generation equals the response length. With confident decoding, however, the number of iterations needed by Dimple is even only $\frac{\text{response length}}{3}$. We also re-implement the prefilling technique in autoregressive models and demonstrate that it does not significantly impact performance on most benchmark evaluations, while offering a speedup of 1.5x to 7x. Additionally, we explore Dimpleâ€™s capability to precisely control its response using structure priors. These priors enable structured responses in a manner distinct from instruction-based or chain-of-thought prompting, and allow fine-grained control over response format and length, which is difficult to achieve in autoregressive models. Overall, this work validates the feasibility and advantages of DMLLM and enhances its inference efficiency and controllability. Code and models are available at <a target="_blank" rel="noopener" href="https://github.com/yu-rp/Dimple">https://github.com/yu-rp/Dimple</a>. </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Dimpleï¼Œå³é¦–ä¸ªç¦»æ•£æ‰©æ•£å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆDMLLMï¼‰ã€‚æˆ‘ä»¬å‘ç°ï¼Œä»…ä½¿ç”¨çº¯ç²¹çš„ç¦»æ•£æ‰©æ•£æ–¹æ³•è¿›è¡Œè®­ç»ƒä¼šå¯¼è‡´æ˜¾è‘—çš„è®­ç»ƒä¸ç¨³å®šã€æ€§èƒ½ä¸ä½³å’Œä¸¥é‡çš„é•¿åº¦åå·®é—®é¢˜ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§ç»“åˆåˆå§‹è‡ªå›å½’é˜¶æ®µå’Œåç»­æ‰©æ•£é˜¶æ®µçš„æ–°å‹è®­ç»ƒèŒƒå¼ã€‚è¿™ç§æ–¹æ³•äº§ç”Ÿäº†Dimple-7Bæ¨¡å‹ï¼Œå®ƒåœ¨ç›¸åŒçš„æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶ä½¿ç”¨ä¸LLaVA-NEXTç±»ä¼¼çš„è®­ç»ƒæµç¨‹ã€‚Dimple-7Båœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†LLaVA-NEXTï¼Œæé«˜äº†3.9%ï¼Œè¯æ˜äº†DMLLMå¯ä»¥è¾¾åˆ°ä¸è‡ªå›å½’æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚ä¸ºäº†æé«˜æ¨ç†æ•ˆç‡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç§°ä¸ºç¡®å®šæ€§è§£ç çš„è§£ç ç­–ç•¥ï¼Œè¯¥ç­–ç•¥åŠ¨æ€è°ƒæ•´æ¯ä¸€æ­¥ç”Ÿæˆçš„ä»¤ç‰Œæ•°é‡ï¼Œæ˜¾è‘—å‡å°‘äº†ç”Ÿæˆè¿­ä»£æ¬¡æ•°ã€‚åœ¨è‡ªå›å½’æ¨¡å‹ä¸­ï¼Œç”Ÿæˆè¿‡ç¨‹ä¸­çš„æ­£å‘è¿­ä»£æ¬¡æ•°ç­‰äºå“åº”é•¿åº¦ã€‚ç„¶è€Œï¼Œä½¿ç”¨ç¡®å®šæ€§è§£ç ï¼ŒDimpleæ‰€éœ€çš„è¿­ä»£æ¬¡æ•°ç”šè‡³åªæœ‰$\frac{\text{å“åº”é•¿åº¦}}{3}$ã€‚æˆ‘ä»¬è¿˜é‡æ–°å®ç°äº†è‡ªå›å½’æ¨¡å‹ä¸­çš„é¢„å¡«å……æŠ€æœ¯ï¼Œå¹¶è¯æ˜å®ƒåœ¨å¤§å¤šæ•°åŸºå‡†è¯„ä¼°ä¸­ä¸ä¼šå¯¹æ€§èƒ½äº§ç”Ÿé‡å¤§å½±å“ï¼ŒåŒæ—¶æä¾›äº†1.5xåˆ°7xçš„åŠ é€Ÿã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ¢ç´¢äº†Dimpleé€šè¿‡ç»“æ„å…ˆéªŒç²¾ç¡®æ§åˆ¶å…¶å“åº”çš„èƒ½åŠ›ã€‚è¿™äº›å…ˆéªŒçŸ¥è¯†èƒ½å¤Ÿä»¥åŒºåˆ«äºæŒ‡ä»¤å¼æˆ–æ€ç»´é“¾æç¤ºçš„æ–¹å¼ï¼Œå®ç°ç»“æ„åŒ–å“åº”ï¼Œå¹¶å¯¹å“åº”æ ¼å¼å’Œé•¿åº¦è¿›è¡Œç²¾ç»†æ§åˆ¶ï¼Œè¿™åœ¨è‡ªå›å½’æ¨¡å‹ä¸­éš¾ä»¥å®ç°ã€‚æ€»ä½“è€Œè¨€ï¼Œè¿™é¡¹å·¥ä½œéªŒè¯äº†DMLLMçš„å¯è¡Œæ€§å’Œä¼˜åŠ¿ï¼Œå¹¶æé«˜äº†å…¶æ¨ç†æ•ˆç‡å’Œå¯æ§æ€§ã€‚ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/yu-rp/Dimple%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/yu-rp/Dimpleæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16990v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>Dimpleæ˜¯é¦–ä¸ªç¦»æ•£æ‰©æ•£å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆDMLLMï¼‰ã€‚æœ¬ç ”ç©¶å‘ç°ï¼Œçº¯ç¦»æ•£æ‰©æ•£è®­ç»ƒæ–¹æ³•ä¼šå¯¼è‡´è®­ç»ƒä¸ç¨³å®šã€æ€§èƒ½ä¸ä½³ä»¥åŠä¸¥é‡çš„é•¿åº¦åè§é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºäº†ä¸€ç§ç»“åˆåˆå§‹è‡ªå›å½’é˜¶æ®µå’Œåç»­æ‰©æ•£é˜¶æ®µçš„æ–°å‹è®­ç»ƒèŒƒå¼ï¼ŒæˆåŠŸè®­ç»ƒå‡ºDimple-7Bæ¨¡å‹ã€‚è¯¥æ¨¡å‹åœ¨ç›¸åŒæ•°æ®é›†ä¸Šä½¿ç”¨ç±»ä¼¼è®­ç»ƒæµæ°´çº¿ï¼Œæ€§èƒ½è¾ƒLLaVA-NEXTæå‡3.9%ã€‚ä¸ºæå‡æ¨ç†æ•ˆç‡ï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºäº†åä¸ºâ€œè‡ªä¿¡è§£ç â€çš„è§£ç ç­–ç•¥ï¼Œè¯¥ç­–ç•¥èƒ½åŠ¨æ€è°ƒæ•´æ¯ä¸€æ­¥ç”Ÿæˆçš„ä»¤ç‰Œæ•°é‡ï¼Œæ˜¾è‘—å‡å°‘ç”Ÿæˆè¿­ä»£æ¬¡æ•°ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜é‡æ–°å®ç°äº†è‡ªå›å½’æ¨¡å‹ä¸­çš„é¢„å¡«å……æŠ€æœ¯ï¼Œå¹¶å¯¹ç»“æ„å…ˆéªŒåœ¨Dimpleä¸­çš„èƒ½åŠ›è¿›è¡Œäº†æ¢ç´¢ã€‚è¿™äº›å…ˆéªŒçŸ¥è¯†èƒ½å¤Ÿä»¥ç‹¬ç‰¹çš„æ–¹å¼å®ç°ç»“æ„åŒ–å“åº”ï¼Œå¯¹å“åº”æ ¼å¼å’Œé•¿åº¦è¿›è¡Œç²¾ç»†æ§åˆ¶ï¼Œè¿™åœ¨è‡ªå›å½’æ¨¡å‹ä¸­éš¾ä»¥å®ç°ã€‚æ€»ä½“è€Œè¨€ï¼Œæœ¬ç ”ç©¶éªŒè¯äº†DMLLMçš„å¯è¡Œæ€§å’Œä¼˜åŠ¿ï¼Œæé«˜äº†æ¨ç†æ•ˆç‡å’Œå¯æ§æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Dimpleæ˜¯é¦–ä¸ªç¦»æ•£æ‰©æ•£å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆDMLLMï¼‰ã€‚</li>
<li>çº¯ç¦»æ•£æ‰©æ•£è®­ç»ƒä¼šå¯¼è‡´è®­ç»ƒä¸ç¨³å®šã€æ€§èƒ½ä¸ä½³å’Œé•¿åº¦åè§é—®é¢˜ã€‚</li>
<li>Dimpleé€šè¿‡ç»“åˆè‡ªå›å½’å’Œæ‰©æ•£é˜¶æ®µçš„æ–°å‹è®­ç»ƒèŒƒå¼æå‡æ€§èƒ½ã€‚</li>
<li>Dimple-7Bæ¨¡å‹æ€§èƒ½è¾ƒLLaVA-NEXTæå‡3.9%ã€‚</li>
<li>â€œè‡ªä¿¡è§£ç â€ç­–ç•¥æé«˜äº†Dimpleçš„æ¨ç†æ•ˆç‡ã€‚</li>
<li>é¢„å¡«å……æŠ€æœ¯åœ¨è‡ªå›å½’æ¨¡å‹ä¸­çš„å®ç°åŠå…¶å¯¹æ€§èƒ½çš„å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16990">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-da7c892340577775fe659dcbe9453d47.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6f442ed7150a3501600077560efd3e5e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-15914635590877bfdc79e628b429693b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d888185ea4a9296c9c6fc034e62db886.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="UFT-Unifying-Supervised-and-Reinforcement-Fine-Tuning"><a href="#UFT-Unifying-Supervised-and-Reinforcement-Fine-Tuning" class="headerlink" title="UFT: Unifying Supervised and Reinforcement Fine-Tuning"></a>UFT: Unifying Supervised and Reinforcement Fine-Tuning</h2><p><strong>Authors:Mingyang Liu, Gabriele Farina, Asuman Ozdaglar</strong></p>
<p>Post-training has demonstrated its importance in enhancing the reasoning capabilities of large language models (LLMs). The primary post-training methods can be categorized into supervised fine-tuning (SFT) and reinforcement fine-tuning (RFT). SFT is efficient and well-suited for small language models, but it may lead to overfitting and limit the reasoning abilities of larger models. In contrast, RFT generally yields better generalization but depends heavily on the strength of the base model. To address the limitations of SFT and RFT, we propose Unified Fine-Tuning (UFT), a novel post-training paradigm that unifies SFT and RFT into a single, integrated process. UFT enables the model to effectively explore solutions while incorporating informative supervision signals, bridging the gap between memorizing and thinking underlying existing methods. Notably, UFT outperforms both SFT and RFT in general, regardless of model sizes. Furthermore, we theoretically prove that UFT breaks RFTâ€™s inherent exponential sample complexity bottleneck, showing for the first time that unified training can exponentially accelerate convergence on long-horizon reasoning tasks. </p>
<blockquote>
<p>è®­ç»ƒåçš„ä¼˜åŒ–å¯¹äºæé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›è¡¨ç°å‡ºäº†å…¶é‡è¦æ€§ã€‚ä¸»è¦çš„è®­ç»ƒåä¼˜åŒ–æ–¹æ³•å¯ä»¥åˆ†ä¸ºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰ã€‚SFTæ•ˆç‡é«˜ï¼Œéå¸¸é€‚åˆå°å‹è¯­è¨€æ¨¡å‹ï¼Œä½†å¯èƒ½ä¼šå¯¼è‡´è¿‡æ‹Ÿåˆç°è±¡ï¼Œå¹¶é™åˆ¶å¤§å‹æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒRFTé€šå¸¸å¯ä»¥äº§ç”Ÿæ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½†å®ƒéå¸¸ä¾èµ–äºåŸºç¡€æ¨¡å‹çš„å¼ºåº¦ã€‚ä¸ºäº†è§£å†³SFTå’ŒRFTçš„å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ç»Ÿä¸€å¾®è°ƒï¼ˆUFTï¼‰è¿™ä¸€æ–°å‹è®­ç»ƒåä¼˜åŒ–èŒƒå¼ï¼Œå®ƒå°†SFTå’ŒRFTç»Ÿä¸€åˆ°ä¸€ä¸ªå•ä¸€çš„ç»¼åˆè¿‡ç¨‹ä¸­ã€‚UFTä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨å¼•å…¥ä¿¡æ¯ç›‘ç£ä¿¡å·çš„åŒæ—¶æœ‰æ•ˆåœ°æ¢ç´¢è§£å†³æ–¹æ¡ˆï¼Œä»è€Œå¼¥åˆäº†ç°æœ‰æ–¹æ³•è®°å¿†ä¸æ€ç»´ä¹‹é—´çš„é¸¿æ²Ÿã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ— è®ºæ¨¡å‹è§„æ¨¡å¦‚ä½•ï¼ŒUFTé€šå¸¸éƒ½ä¼˜äºSFTå’ŒRFTã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä»ç†è®ºä¸Šè¯æ˜äº†UFTæ‰“ç ´äº†RFTå›ºæœ‰çš„æŒ‡æ•°æ ·æœ¬å¤æ‚æ€§ç“¶é¢ˆï¼Œé¦–æ¬¡å±•ç¤ºäº†ç»Ÿä¸€è®­ç»ƒå¯ä»¥åŠ é€Ÿé•¿æœŸæ¨ç†ä»»åŠ¡çš„æ”¶æ•›é€Ÿåº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16984v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šè®­ç»ƒåå¤„ç†åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›æ–¹é¢è¡¨ç°å‡ºé‡è¦æ€§ã€‚ä¸»è¦è®­ç»ƒåå¤„ç†æ–¹æ³•å¯åˆ†ä¸ºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰ã€‚SFTæ•ˆç‡é«˜ï¼Œé€‚åˆå°å‹è¯­è¨€æ¨¡å‹ï¼Œä½†å¯èƒ½å¯¼è‡´å¤§å‹æ¨¡å‹çš„è¿‡åº¦æ‹Ÿåˆã€‚ä¸ä¹‹ç›¸æ¯”ï¼ŒRFTå…·æœ‰æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½†å¯¹åŸºç¡€æ¨¡å‹çš„å¼ºåº¦æœ‰è¾ƒé«˜ä¾èµ–æ€§ã€‚ä¸ºäº†è§£å†³SFTå’ŒRFTçš„å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹è®­ç»ƒåå¤„ç†æ–¹æ³•â€”â€”ç»Ÿä¸€å¾®è°ƒï¼ˆUFTï¼‰ã€‚UFTç»Ÿä¸€äº†SFTå’ŒRFTåœ¨ä¸€ä¸ªå•ä¸€çš„é›†æˆè¿‡ç¨‹ä¸­ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æ¢ç´¢è§£å†³æ–¹æ¡ˆå¹¶èå…¥ä¿¡æ¯ç›‘ç£ä¿¡å·ï¼Œç¼©å°äº†ç°æœ‰æ–¹æ³•ä¸­çš„è®°å¿†ä¸æ€ç»´ä¹‹é—´çš„é¸¿æ²Ÿã€‚UFTé€šå¸¸ä¼˜äºSFTå’ŒRFTï¼Œæ— è®ºæ¨¡å‹è§„æ¨¡å¦‚ä½•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä»ç†è®ºä¸Šè¯æ˜äº†UFTæ‰“ç ´äº†RFTå›ºæœ‰çš„æŒ‡æ•°æ ·æœ¬å¤æ‚æ€§ç“¶é¢ˆï¼Œè¡¨æ˜ç»Ÿä¸€è®­ç»ƒå¯ä»¥åŠ å¿«è§£å†³é•¿æœŸæ¨ç†ä»»åŠ¡çš„æ”¶æ•›é€Ÿåº¦ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è®­ç»ƒåå¤„ç†å¯¹äºå¢å¼ºLLMçš„æ¨ç†èƒ½åŠ›è‡³å…³é‡è¦ã€‚</li>
<li>ä¸»è¦çš„è®­ç»ƒåå¤„ç†æ–¹æ³•åŒ…æ‹¬ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰ã€‚</li>
<li>SFTé€‚åˆå°å‹è¯­è¨€æ¨¡å‹ï¼Œä½†å¯èƒ½å¯¼è‡´å¤§å‹æ¨¡å‹çš„è¿‡åº¦æ‹Ÿåˆã€‚</li>
<li>RFTå…·æœ‰è¾ƒå¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½†å¯¹åŸºç¡€æ¨¡å‹çš„å¼ºåº¦ä¾èµ–è¾ƒå¤§ã€‚</li>
<li>ä¸ºè§£å†³SFTå’ŒRFTçš„å±€é™æ€§ï¼Œæå‡ºäº†ç»Ÿä¸€å¾®è°ƒï¼ˆUFTï¼‰æ–¹æ³•ã€‚</li>
<li>UFTç»“åˆäº†SFTå’ŒRFTçš„ä¼˜ç‚¹ï¼Œé€šå¸¸ä¼˜äºä¸¤è€…ï¼Œå¹¶é€‚ç”¨äºå„ç§æ¨¡å‹è§„æ¨¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16984">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4379c5fa8eb7e004538fae226f4e95be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a106eac80f07dce52fbf5765893a115.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d05654ae47aa06b8cb1eaeaca2523d74.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3b9e44acebac4a9b4c61ade8cead9f19.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6aa6d18cc54677c8a4db559a564f130b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2c6e430c3af43aaf1df7d6f91e8fc7b4.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="LLM-as-Effective-Streaming-Processor-Bridging-Streaming-Batch-Mismatches-with-Group-Position-Encoding"><a href="#LLM-as-Effective-Streaming-Processor-Bridging-Streaming-Batch-Mismatches-with-Group-Position-Encoding" class="headerlink" title="LLM as Effective Streaming Processor: Bridging Streaming-Batch   Mismatches with Group Position Encoding"></a>LLM as Effective Streaming Processor: Bridging Streaming-Batch   Mismatches with Group Position Encoding</h2><p><strong>Authors:Junlong Tong, Jinlan Fu, Zixuan Lin, Yingqi Fan, Anhao Zhao, Hui Su, Xiaoyu Shen</strong></p>
<p>Large Language Models (LLMs) are primarily designed for batch processing. Existing methods for adapting LLMs to streaming rely either on expensive re-encoding or specialized architectures with limited scalability. This work identifies three key mismatches in adapting batch-oriented LLMs to streaming: (1) input-attention, (2) output-attention, and (3) position-ID mismatches. While it is commonly assumed that the latter two mismatches require frequent re-encoding, our analysis reveals that only the input-attention mismatch significantly impacts performance, indicating re-encoding outputs is largely unnecessary. To better understand this discrepancy with the common assumption, we provide the first comprehensive analysis of the impact of position encoding on LLMs in streaming, showing that preserving relative positions within source and target contexts is more critical than maintaining absolute order. Motivated by the above analysis, we introduce a group position encoding paradigm built on batch architectures to enhance consistency between streaming and batch modes. Extensive experiments on cross-lingual and cross-modal tasks demonstrate that our method outperforms existing approaches. Our method requires no architectural modifications, exhibits strong generalization in both streaming and batch modes. The code is available at repository <a target="_blank" rel="noopener" href="https://github.com/EIT-NLP/StreamingLLM">https://github.com/EIT-NLP/StreamingLLM</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸»è¦è®¾è®¡ç”¨äºæ‰¹å¤„ç†ã€‚ç°æœ‰å°†LLMé€‚åº”æµå¼å¤„ç†çš„æ–¹æ³•ä¾èµ–äºæ˜‚è´µçš„é‡æ–°ç¼–ç æˆ–å…·æœ‰æœ‰é™å¯æ‰©å±•æ€§çš„ä¸“ç”¨æ¶æ„ã€‚è¿™é¡¹å·¥ä½œç¡®å®šäº†å°†é¢å‘æ‰¹å¤„ç†çš„LLMé€‚åº”æµå¼å¤„ç†çš„ä¸‰ä¸ªå…³é”®ä¸åŒ¹é…ä¹‹å¤„ï¼šï¼ˆ1ï¼‰è¾“å…¥å…³æ³¨ï¼Œï¼ˆ2ï¼‰è¾“å‡ºå…³æ³¨ï¼Œä»¥åŠï¼ˆ3ï¼‰ä½ç½®IDä¸åŒ¹é…ã€‚è™½ç„¶é€šå¸¸è®¤ä¸ºåä¸¤ç§ä¸åŒ¹é…éœ€è¦é¢‘ç¹é‡æ–°ç¼–ç ï¼Œä½†æˆ‘ä»¬çš„åˆ†ææ­ç¤ºåªæœ‰è¾“å…¥å…³æ³¨ä¸åŒ¹é…æ‰ä¼šå¯¹æ€§èƒ½äº§ç”Ÿé‡å¤§å½±å“ï¼Œè¿™è¡¨æ˜é‡æ–°ç¼–ç è¾“å‡ºå¤§å¤šæ˜¯ä¸å¿…è¦çš„ã€‚ä¸ºäº†æ›´å¥½åœ°ç†è§£è¿™ä¸€ä¸å¸¸è§å‡è®¾çš„åå·®ï¼Œæˆ‘ä»¬å¯¹ä½ç½®ç¼–ç å¯¹æµå¼LLMçš„å½±å“è¿›è¡Œäº†é¦–æ¬¡ç»¼åˆåˆ†æï¼Œè¡¨æ˜åœ¨æºå’Œç›®æ ‡è¯­å¢ƒä¸­ä¿æŒç›¸å¯¹ä½ç½®æ¯”ä¿æŒç»å¯¹é¡ºåºæ›´ä¸ºé‡è¦ã€‚å—ä¸Šè¿°åˆ†æçš„å¯å‘ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºæ‰¹å¤„ç†æ¶æ„çš„åˆ†ç»„ä½ç½®ç¼–ç èŒƒå¼ï¼Œä»¥å¢å¼ºæµå¼å’Œæ‰¹å¤„ç†æ¨¡å¼ä¹‹é—´çš„ä¸€è‡´æ€§ã€‚åœ¨è·¨è¯­è¨€å’Œè·¨æ¨¡æ€ä»»åŠ¡ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ— éœ€è¿›è¡Œä»»ä½•æ¶æ„ä¿®æ”¹ï¼Œåœ¨æµå¼å’Œæ‰¹å¤„ç†æ¨¡å¼ä¸‹å‡è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/EIT-NLP/StreamingLLM%E4%BB%93%E5%BA%93%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/EIT-NLP/StreamingLLMä»“åº“ä¸­æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16983v1">PDF</a> ACL 2025 Findings</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸»è¦ä¸ºæ‰¹å¤„ç†è®¾è®¡ã€‚ç°æœ‰é€‚åº”æµå¼æ•°æ®çš„LLMæ–¹æ³•ä¾èµ–äºæ˜‚è´µçš„é‡æ–°ç¼–ç æˆ–å…·æœ‰æœ‰é™å¯æ‰©å±•æ€§çš„ä¸“ç”¨æ¶æ„ã€‚æœ¬ç ”ç©¶ç¡®å®šäº†å°†æ‰¹å¤„ç†å¯¼å‘çš„LLMé€‚åº”æµå¼æ•°æ®çš„ä¸‰ä¸ªå…³é”®ä¸åŒ¹é…ç‚¹ï¼šï¼ˆ1ï¼‰è¾“å…¥å…³æ³¨ã€ï¼ˆ2ï¼‰è¾“å‡ºå…³æ³¨å’Œï¼ˆ3ï¼‰ä½ç½®IDä¸åŒ¹é…ã€‚å°½ç®¡äººä»¬æ™®éè®¤ä¸ºåä¸¤ä¸ªä¸åŒ¹é…éœ€è¦é¢‘ç¹é‡æ–°ç¼–ç ï¼Œä½†æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œåªæœ‰è¾“å…¥å…³æ³¨ä¸åŒ¹é…æ‰ä¼šå¯¹æ€§èƒ½äº§ç”Ÿé‡å¤§å½±å“ï¼Œè¿™æ„å‘³ç€é‡æ–°ç¼–ç è¾“å‡ºå¤§å¤šæ˜¯ä¸å¿…è¦çš„ã€‚ä¸ºäº†æ›´å¥½åœ°ç†è§£è¿™ä¸€ä¸å¸¸è§å‡è®¾çš„åå·®ï¼Œæˆ‘ä»¬å¯¹ä½ç½®ç¼–ç å¯¹æµå¼LLMçš„å½±å“è¿›è¡Œäº†é¦–æ¬¡ç»¼åˆåˆ†æï¼Œè¡¨æ˜åœ¨æºå’Œç›®æ ‡è¯­å¢ƒä¸­ä¿æŒç›¸å¯¹ä½ç½®æ¯”ç»´æŒç»å¯¹é¡ºåºæ›´ä¸ºé‡è¦ã€‚å—ä¸Šè¿°åˆ†æå¯å‘ï¼Œæˆ‘ä»¬åœ¨æ‰¹å¤„ç†æ¶æ„çš„åŸºç¡€ä¸Šå¼•å…¥äº†ä¸€ç§ç»„ä½ç½®ç¼–ç èŒƒå¼ï¼Œä»¥æé«˜æµå¼æ¨¡å¼å’Œæ‰¹æ¨¡å¼ä¹‹é—´çš„ä¸€è‡´æ€§ã€‚åœ¨è·¨è¯­è¨€å’Œè·¨æ¨¡æ€ä»»åŠ¡ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸éœ€è¦ä¿®æ”¹æ¶æ„ï¼Œåœ¨æµå¼å’Œæ‰¹æ¨¡å¼ä¸‹å‡è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚ç›¸å…³ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/EIT-NLP/StreamingLLM%E4%BB%93%E5%BA%93%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/EIT-NLP/StreamingLLMä»“åº“ä¸­æ‰¾åˆ°ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsä¸»è¦ä¸ºæ‰¹å¤„ç†è®¾è®¡ï¼Œç°æœ‰é€‚åº”æµå¼çš„ç­–ç•¥å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>è¾“å…¥å…³æ³¨ã€è¾“å‡ºå…³æ³¨å’Œä½ç½®IDä¸åŒ¹é…æ˜¯é€‚åº”æµå¼LLMçš„ä¸»è¦æŒ‘æˆ˜ã€‚</li>
<li>åªæœ‰è¾“å…¥å…³æ³¨ä¸åŒ¹é…æ˜¾è‘—å½±å“æ€§èƒ½ï¼Œé‡æ–°ç¼–ç è¾“å‡ºä¸å¿…è¦ã€‚</li>
<li>ä¿æŒæºå’Œç›®æ ‡è¯­å¢ƒä¸­çš„ç›¸å¯¹ä½ç½®æ¯”ç»´æŒç»å¯¹é¡ºåºæ›´é‡è¦ã€‚</li>
<li>å¼•å…¥ç»„ä½ç½®ç¼–ç èŒƒå¼ï¼Œæé«˜æµå¼ä¸æ‰¹æ¨¡å¼é—´çš„ä¸€è‡´æ€§ã€‚</li>
<li>æ‰€ææ–¹æ³•ä¼˜äºç°æœ‰ç­–ç•¥ï¼Œæ— éœ€ä¿®æ”¹æ¶æ„ï¼Œæ³›åŒ–èƒ½åŠ›å¼ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16983">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-6d6450f69c13d8efd6fc874f91e549cd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6551390badc1e17844310210c69c3bf8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c48b926750575f337e117e76c452d5c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-97b0db90d1a24a8c9ca16a2906f6eb64.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55db6a45a45ccf5a41c3162d40d3934d.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="SWE-Dev-Evaluating-and-Training-Autonomous-Feature-Driven-Software-Development"><a href="#SWE-Dev-Evaluating-and-Training-Autonomous-Feature-Driven-Software-Development" class="headerlink" title="SWE-Dev: Evaluating and Training Autonomous Feature-Driven Software   Development"></a>SWE-Dev: Evaluating and Training Autonomous Feature-Driven Software   Development</h2><p><strong>Authors:Yaxin Du, Yuzhu Cai, Yifan Zhou, Cheng Wang, Yu Qian, Xianghe Pang, Qian Liu, Yue Hu, Siheng Chen</strong></p>
<p>Large Language Models (LLMs) have shown strong capability in diverse software engineering tasks, e.g. code completion, bug fixing, and document generation. However, feature-driven development (FDD), a highly prevalent real-world task that involves developing new functionalities for large, existing codebases, remains underexplored. We therefore introduce SWE-Dev, the first large-scale dataset (with 14,000 training and 500 test samples) designed to evaluate and train autonomous coding systems on real-world feature development tasks. To ensure verifiable and diverse training, SWE-Dev uniquely provides all instances with a runnable environment and its developer-authored executable unit tests. This collection not only provides high-quality data for Supervised Fine-Tuning (SFT), but also enables Reinforcement Learning (RL) by delivering accurate reward signals from executable unit tests. Our extensive evaluations on SWE-Dev, covering 17 chatbot LLMs, 10 reasoning models, and 10 Multi-Agent Systems (MAS), reveal that FDD is a profoundly challenging frontier for current AI (e.g., Claude-3.7-Sonnet achieves only 22.45% Pass@3 on the hard test split). Crucially, we demonstrate that SWE-Dev serves as an effective platform for model improvement: fine-tuning on training set enabled a 7B model comparable to GPT-4o on \textit{hard} split, underscoring the value of its high-quality training data. Code is available here \href{<a target="_blank" rel="noopener" href="https://github.com/justLittleWhite/SWE-Dev%7D%7Bhttps://github.com/justLittleWhite/SWE-Dev%7D">https://github.com/justLittleWhite/SWE-Dev}{https://github.com/justLittleWhite/SWE-Dev}</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šç§è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä¾‹å¦‚ä»£ç è¡¥å…¨ã€é”™è¯¯ä¿®å¤å’Œæ–‡æ¡£ç”Ÿæˆã€‚ç„¶è€Œï¼Œå…³äºç‰¹å¾é©±åŠ¨å¼€å‘ï¼ˆFDDï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ¶‰åŠä¸ºå¤§å‹ç°æœ‰ä»£ç åº“å¼€å‘æ–°åŠŸèƒ½çš„é«˜åº¦æ™®éçš„ç°å®ä»»åŠ¡ï¼Œä»ç„¶ç¼ºä¹è¶³å¤Ÿçš„æ¢ç´¢ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†SWE-Devï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå¤§è§„æ¨¡æ•°æ®é›†ï¼ˆåŒ…å«14,000ä¸ªè®­ç»ƒæ ·æœ¬å’Œ500ä¸ªæµ‹è¯•æ ·æœ¬ï¼‰ï¼Œæ—¨åœ¨è¯„ä¼°å¹¶è®­ç»ƒè‡ªä¸»ç¼–ç ç³»ç»Ÿä»¥æ‰§è¡Œç°å®ä¸–ç•Œçš„ç‰¹å¾å¼€å‘ä»»åŠ¡ã€‚ä¸ºäº†ç¡®ä¿å¯éªŒè¯å’Œå¤šæ ·åŒ–çš„è®­ç»ƒï¼ŒSWE-Devç‹¬ç‰¹åœ°æä¾›äº†å¯è¿è¡Œçš„ç¯å¢ƒå’Œå¼€å‘äººå‘˜ç¼–å†™çš„å¯æ‰§è¡Œå•å…ƒæµ‹è¯•ã€‚è¿™ä¸€é›†åˆä¸ä»…ä¸ºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æä¾›äº†é«˜è´¨é‡çš„æ•°æ®ï¼Œè€Œä¸”è¿˜é€šè¿‡æä¾›å¯æ‰§è¡Œå•å…ƒæµ‹è¯•çš„å‡†ç¡®å¥–åŠ±ä¿¡å·ï¼Œä½¿å¾—å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æˆä¸ºå¯èƒ½ã€‚æˆ‘ä»¬å¯¹SWE-Devè¿›è¡Œçš„å¹¿æ³›è¯„ä¼°æ¶µç›–äº†17ä¸ªèŠå¤©æœºå™¨äººLLMã€10ä¸ªæ¨ç†æ¨¡å‹å’Œ10ä¸ªå¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰ï¼Œè¡¨æ˜FDDæ˜¯å½“å‰äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªæå…·æŒ‘æˆ˜æ€§çš„å‰æ²¿é¢†åŸŸï¼ˆä¾‹å¦‚ï¼ŒClaude-3.7-Sonnetåœ¨å›°éš¾æµ‹è¯•é›†ä¸Šä»…è¾¾åˆ°22.45ï¼…çš„Pass@3ï¼‰ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬è¯æ˜äº†SWE-Devæ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„æ¨¡å‹æ”¹è¿›å¹³å°ï¼šåœ¨è®­ç»ƒé›†ä¸Šè¿›è¡Œå¾®è°ƒä½¿å¾—ä¸€ä¸ªä¸GPT-4ç›¸å½“çš„è§„æ¨¡ä¸º7Bçš„æ¨¡å‹åœ¨å›°éš¾åˆ†å‰²ä¸Šå–å¾—äº†è¿›å±•ï¼Œçªæ˜¾äº†å…¶é«˜è´¨é‡è®­ç»ƒæ•°æ®çš„ä»·å€¼ã€‚ä»£ç å¯ä»<a target="_blank" rel="noopener" href="https://github.com/justLittleWhite/SWE-Dev%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/justLittleWhite/SWE-Devè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16975v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œå¦‚ä»£ç è¡¥å…¨ã€é”™è¯¯ä¿®å¤å’Œæ–‡æ¡£ç”Ÿæˆç­‰ã€‚ç„¶è€Œï¼Œé’ˆå¯¹å¤§å‹ç°æœ‰ä»£ç åº“è¿›è¡Œæ–°åŠŸèƒ½å¼€å‘çš„ç‰¹å¾é©±åŠ¨å¼€å‘ï¼ˆFDDï¼‰ä»»åŠ¡å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†SWE-Devæ•°æ®é›†ï¼Œå®ƒæ˜¯é¦–ä¸ªé’ˆå¯¹çœŸå®ä¸–ç•Œç‰¹å¾å¼€å‘ä»»åŠ¡è¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°çš„å¤§å‹æ•°æ®é›†ï¼ŒåŒ…å«14,000ä¸ªè®­ç»ƒæ ·æœ¬å’Œ500ä¸ªæµ‹è¯•æ ·æœ¬ã€‚SWE-Devçš„ç‹¬ç‰¹ä¹‹å¤„åœ¨äºä¸ºæ‰€æœ‰å®ä¾‹æä¾›äº†ä¸€ä¸ªå¯è¿è¡Œçš„ç¯å¢ƒå’Œå¼€å‘è€…ç¼–å†™çš„å¯æ‰§è¡Œå•å…ƒæµ‹è¯•ï¼Œç¡®ä¿äº†å¯éªŒè¯å’Œå¤šæ ·åŒ–çš„è®­ç»ƒã€‚æ­¤å¤–ï¼ŒSWE-Devä¸ä»…ä¸ºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æä¾›äº†é«˜è´¨é‡çš„æ•°æ®ï¼Œè€Œä¸”è¿˜é€šè¿‡æä¾›æ¥è‡ªå¯æ‰§è¡Œå•å…ƒæµ‹è¯•çš„å‡†ç¡®å¥–åŠ±ä¿¡å·ï¼Œä½¿å¾—å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¾—ä»¥å¯èƒ½ã€‚é€šè¿‡å¯¹æ¶µç›–17æ¬¾èŠå¤©æœºå™¨äººå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€10ä¸ªæ¨ç†æ¨¡å‹å’Œ10ä¸ªå¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰çš„å¹¿æ³›è¯„ä¼°ï¼Œæˆ‘ä»¬å‘ç°FDDæ˜¯å½“å‰äººå·¥æ™ºèƒ½é¢ä¸´çš„ä¸€ä¸ªæå…·æŒ‘æˆ˜æ€§çš„å‰æ²¿é¢†åŸŸã€‚é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬è¯æ˜äº†SWE-Devä½œä¸ºä¸€ä¸ªæœ‰æ•ˆçš„å¹³å°ï¼Œèƒ½å¤Ÿæ”¹è¿›æ¨¡å‹æ€§èƒ½ï¼šåœ¨è®­ç»ƒé›†ä¸Šè¿›è¡Œå¾®è°ƒåï¼Œæ€§èƒ½æ¥è¿‘GPT-4çš„æ¨¡å‹å¾—åˆ†æå‡æ˜¾è‘—ã€‚ç›¸å…³ä»£ç å¯é€šè¿‡é“¾æ¥è®¿é—®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ã€‚</li>
<li>ç‰¹å¾é©±åŠ¨å¼€å‘ï¼ˆFDDï¼‰æ˜¯ä¸€ä¸ªå°šæœªå……åˆ†æ¢ç´¢çš„é¢†åŸŸï¼Œå¯¹äºç°æœ‰å¤§å‹ä»£ç åº“çš„æ–°åŠŸèƒ½å¼€å‘è‡³å…³é‡è¦ã€‚</li>
<li>SWE-Devæ•°æ®é›†æ˜¯é¦–ä¸ªé’ˆå¯¹FDDä»»åŠ¡è¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°çš„å¤§å‹æ•°æ®é›†ã€‚</li>
<li>SWE-Devæä¾›äº†å¯è¿è¡Œçš„ç¯å¢ƒå’Œå¼€å‘è€…ç¼–å†™çš„å•å…ƒæµ‹è¯•ï¼Œç¡®ä¿è®­ç»ƒå’Œè¯„ä¼°çš„å‡†ç¡®æ€§å’Œå¤šæ ·æ€§ã€‚</li>
<li>è¯¥æ•°æ®é›†ä¸ä»…æ”¯æŒç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œè¿˜ä¸ºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æä¾›äº†å¥–åŠ±ä¿¡å·ã€‚</li>
<li>FDDæ˜¯å½“å‰äººå·¥æ™ºèƒ½é¢ä¸´çš„ä¸€ä¸ªæå…·æŒ‘æˆ˜æ€§çš„å‰æ²¿é¢†åŸŸã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16975">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d76cc3c400d975b56cd8543e91547ee8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c98b6e58ef1ef24a3fe9676088b01589.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-51f5f922ff3920ef9b3be573ab26c22b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b6072b88a0691901156fdc4987a21f4d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6b49a1218b75ef16434a7543070c20c9.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="A-Comprehensive-Evaluation-of-Contemporary-ML-Based-Solvers-for-Combinatorial-Optimization"><a href="#A-Comprehensive-Evaluation-of-Contemporary-ML-Based-Solvers-for-Combinatorial-Optimization" class="headerlink" title="A Comprehensive Evaluation of Contemporary ML-Based Solvers for   Combinatorial Optimization"></a>A Comprehensive Evaluation of Contemporary ML-Based Solvers for   Combinatorial Optimization</h2><p><strong>Authors:Shengyu Feng, Weiwei Sun, Shanda Li, Ameet Talwalkar, Yiming Yang</strong></p>
<p>Machine learning (ML) has demonstrated considerable potential in supporting model design and optimization for combinatorial optimization (CO) problems. However, much of the progress to date has been evaluated on small-scale, synthetic datasets, raising concerns about the practical effectiveness of ML-based solvers in real-world, large-scale CO scenarios. Additionally, many existing CO benchmarks lack sufficient training data, limiting their utility for evaluating data-driven approaches. To address these limitations, we introduce FrontierCO, a comprehensive benchmark that covers eight canonical CO problem types and evaluates 16 representative ML-based solversâ€“including graph neural networks and large language model (LLM) agents. FrontierCO features challenging instances drawn from industrial applications and frontier CO research, offering both realistic problem difficulty and abundant training data. Our empirical results provide critical insights into the strengths and limitations of current ML methods, helping to guide more robust and practically relevant advances at the intersection of machine learning and combinatorial optimization. Our data is available at <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/CO-Bench/FrontierCO">https://huggingface.co/datasets/CO-Bench/FrontierCO</a>. </p>
<blockquote>
<p>æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰åœ¨æ”¯æŒç»„åˆä¼˜åŒ–ï¼ˆCOï¼‰é—®é¢˜çš„æ¨¡å‹è®¾è®¡å’Œä¼˜åŒ–æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œè‡³ä»Šå¤§éƒ¨åˆ†çš„è¿›å±•éƒ½æ˜¯åœ¨å°å‹åˆæˆæ•°æ®é›†ä¸Šè¯„ä¼°çš„ï¼Œè¿™å¼•å‘äº†äººä»¬å¯¹åŸºäºMLçš„æ±‚è§£å™¨åœ¨çœŸå®ä¸–ç•Œå¤§è§„æ¨¡COåœºæ™¯ä¸­çš„å®é™…æ•ˆæœçš„æ‹…å¿§ã€‚æ­¤å¤–ï¼Œè®¸å¤šç°æœ‰çš„COåŸºå‡†æµ‹è¯•ç¼ºä¹è¶³å¤Ÿçš„è®­ç»ƒæ•°æ®ï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨è¯„ä¼°æ•°æ®é©±åŠ¨æ–¹æ³•æ–¹é¢çš„æ•ˆç”¨ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æ¨å‡ºäº†FrontierCOï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–äº†å…«ç§å…¸å‹çš„COé—®é¢˜ç±»å‹ï¼Œå¹¶è¯„ä¼°äº†16ç§å…·æœ‰ä»£è¡¨æ€§çš„åŸºäºMLçš„æ±‚è§£å™¨ï¼ŒåŒ…æ‹¬å›¾ç¥ç»ç½‘ç»œå’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†ã€‚FrontierCOä»¥å·¥ä¸šåº”ç”¨å’Œå‰æ²¿COç ”ç©¶ä¸­çš„æŒ‘æˆ˜å®ä¾‹ä¸ºç‰¹è‰²ï¼Œæä¾›äº†ç°å®çš„éš¾é¢˜å’Œä¸°å¯Œçš„è®­ç»ƒæ•°æ®ã€‚æˆ‘ä»¬çš„å®è¯ç»“æœä¸ºå½“å‰MLæ–¹æ³•çš„åŠ›é‡å’Œå±€é™æ€§æä¾›äº†å…³é”®è§è§£ï¼Œæœ‰åŠ©äºæŒ‡å¯¼æœºå™¨å­¦ä¹ å’Œç»„åˆä¼˜åŒ–äº¤å‰ç‚¹ä¸Šæ›´ç¨³å¥å’Œå®é™…ç›¸å…³çš„è¿›æ­¥ã€‚æˆ‘ä»¬çš„æ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/CO-Bench/FrontierCO%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://huggingface.co/datasets/CO-Bench/FrontierCOä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16952v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœºå™¨å­¦ä¹ åœ¨ç»„åˆä¼˜åŒ–é—®é¢˜çš„æ¨¡å‹è®¾è®¡å’Œä¼˜åŒ–æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†åœ¨çœŸå®ä¸–ç•Œçš„å¤§è§„æ¨¡ç»„åˆä¼˜åŒ–åœºæ™¯ä¸­å…¶å®è·µæ•ˆæœå°šå­˜ç–‘è™‘ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæå‡ºFrontierCOåŸºå‡†æµ‹è¯•å¹³å°ï¼Œæ¶µç›–å…«ç§å…¸å‹ç»„åˆä¼˜åŒ–é—®é¢˜ç±»å‹ï¼Œè¯„ä¼°äº†16ç§ä»£è¡¨æ€§åŸºäºæœºå™¨å­¦ä¹ çš„æ±‚è§£å™¨ï¼ŒåŒ…æ‹¬å›¾ç¥ç»ç½‘ç»œå’Œå¤§è¯­è¨€æ¨¡å‹ä»£ç†ã€‚è¯¥å¹³å°ä»¥å·¥ä¸šåº”ç”¨å’Œå‰æ²¿ç»„åˆä¼˜åŒ–ç ”ç©¶çš„é—®é¢˜å®ä¾‹ä¸ºç‰¹è‰²ï¼Œæä¾›äº†ç°å®é—®é¢˜çš„éš¾åº¦å’Œä¸°å¯Œçš„è®­ç»ƒæ•°æ®ã€‚å®è¯ç»“æœä¸ºå½“å‰æœºå™¨å­¦ä¹ æ–¹æ³•çš„åŠ›é‡å’Œå±€é™æ€§æä¾›äº†å…³é”®è§è§£ï¼Œæœ‰åŠ©äºæŒ‡å¯¼æœºå™¨å­¦ä¹ å’Œç»„åˆä¼˜åŒ–äº¤å‰é¢†åŸŸçš„æ›´ç¨³å¥å’Œå®é™…ç›¸å…³çš„è¿›æ­¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœºå™¨å­¦ä¹ åœ¨ç»„åˆä¼˜åŒ–é—®é¢˜ä¸­å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚</li>
<li>å½“å‰è¯„ä¼°ä¸»è¦é›†ä¸­åœ¨å°å‹åˆæˆæ•°æ®é›†ä¸Šï¼Œå¯¹å¤§è§„æ¨¡çœŸå®åœºæ™¯çš„ç»„åˆä¼˜åŒ–é—®é¢˜å®è·µæ•ˆæœå°šå­˜ç–‘è™‘ã€‚</li>
<li>æå‡ºFrontierCOåŸºå‡†æµ‹è¯•å¹³å°ä»¥è§£å†³æ­¤é—®é¢˜ï¼Œæ¶µç›–å¤šç§ç»„åˆä¼˜åŒ–é—®é¢˜ç±»å‹å’Œä»£è¡¨æ€§æœºå™¨å­¦ä¹ æ±‚è§£å™¨ã€‚</li>
<li>FrontierCOä»¥å·¥ä¸šåº”ç”¨å’Œå‰æ²¿ç ”ç©¶çš„é—®é¢˜å®ä¾‹ä¸ºç‰¹è‰²ï¼Œæä¾›ç°å®é—®é¢˜çš„éš¾åº¦å’Œä¸°å¯Œçš„è®­ç»ƒæ•°æ®ã€‚</li>
<li>å®è¯ç»“æœæ­ç¤ºäº†å½“å‰æœºå™¨å­¦ä¹ æ–¹æ³•åœ¨ç»„åˆä¼˜åŒ–ä¸­çš„åŠ›é‡å’Œå±€é™æ€§ã€‚</li>
<li>è¯¥å¹³å°æœ‰åŠ©äºæŒ‡å¯¼æœºå™¨å­¦ä¹ ä¸ç»„åˆä¼˜åŒ–äº¤å‰é¢†åŸŸçš„è¿›ä¸€æ­¥ç ”ç©¶å’Œè¿›æ­¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16952">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4ab12bfe97c6c077e0c31309d8216c3d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5f1b1b66f05cd74ffcbcc0a9d46846de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d5d489b8b15e76bef262bd844f98d7a.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="MixAT-Combining-Continuous-and-Discrete-Adversarial-Training-for-LLMs"><a href="#MixAT-Combining-Continuous-and-Discrete-Adversarial-Training-for-LLMs" class="headerlink" title="MixAT: Combining Continuous and Discrete Adversarial Training for LLMs"></a>MixAT: Combining Continuous and Discrete Adversarial Training for LLMs</h2><p><strong>Authors:Csaba DÃ©kÃ¡ny, Stefan Balauca, Robin Staab, Dimitar I. Dimitrov, Martin Vechev</strong></p>
<p>Despite recent efforts in Large Language Models (LLMs) safety and alignment, current adversarial attacks on frontier LLMs are still able to force harmful generations consistently. Although adversarial training has been widely studied and shown to significantly improve the robustness of traditional machine learning models, its strengths and weaknesses in the context of LLMs are less understood. Specifically, while existing discrete adversarial attacks are effective at producing harmful content, training LLMs with concrete adversarial prompts is often computationally expensive, leading to reliance on continuous relaxations. As these relaxations do not correspond to discrete input tokens, such latent training methods often leave models vulnerable to a diverse set of discrete attacks. In this work, we aim to bridge this gap by introducing MixAT, a novel method that combines stronger discrete and faster continuous attacks during training. We rigorously evaluate MixAT across a wide spectrum of state-of-the-art attacks, proposing the At Least One Attack Success Rate (ALO-ASR) metric to capture the worst-case vulnerability of models. We show MixAT achieves substantially better robustness (ALO-ASR &lt; 20%) compared to prior defenses (ALO-ASR &gt; 50%), while maintaining a runtime comparable to methods based on continuous relaxations. We further analyze MixAT in realistic deployment settings, exploring how chat templates, quantization, low-rank adapters, and temperature affect both adversarial training and evaluation, revealing additional blind spots in current methodologies. Our results demonstrate that MixATâ€™s discrete-continuous defense offers a principled and superior robustness-accuracy tradeoff with minimal computational overhead, highlighting its promise for building safer LLMs. We provide our code and models at <a target="_blank" rel="noopener" href="https://github.com/insait-institute/MixAT">https://github.com/insait-institute/MixAT</a>. </p>
<blockquote>
<p>å°½ç®¡æœ€è¿‘åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å®‰å…¨æ€§å’Œå¯¹é½æ–¹é¢ä»˜å‡ºäº†åŠªåŠ›ï¼Œä½†å½“å‰é’ˆå¯¹å‰æ²¿LLMçš„å¯¹æŠ—æ€§æ”»å‡»ä»ç„¶èƒ½å¤ŸæŒç»­å¼ºåˆ¶äº§ç”Ÿæœ‰å®³ç”Ÿæˆã€‚è™½ç„¶å¯¹æŠ—æ€§è®­ç»ƒå·²ç»å¾—åˆ°äº†å¹¿æ³›çš„ç ”ç©¶ï¼Œå¹¶å·²è¯æ˜å¯ä»¥æ˜¾è‘—æé«˜ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹çš„ç¨³å¥æ€§ï¼Œä½†å…¶åœ¨LLMé¢†åŸŸçš„ä¼˜åŠ¿å’ŒåŠ£åŠ¿å´é²œä¸ºäººçŸ¥ã€‚å…·ä½“æ¥è¯´ï¼Œå°½ç®¡ç°æœ‰çš„ç¦»æ•£å¯¹æŠ—æ€§æ”»å‡»åœ¨äº§ç”Ÿæœ‰å®³å†…å®¹æ–¹é¢éå¸¸æœ‰æ•ˆï¼Œä½†ä½¿ç”¨å…·ä½“çš„å¯¹æŠ—æ€§æç¤ºè®­ç»ƒLLMé€šå¸¸è®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œå¯¼è‡´äººä»¬ä¾èµ–è¿ç»­æ”¾æ¾æ–¹æ³•ã€‚ç”±äºè¿™äº›æ”¾æ¾æ–¹æ³•å¹¶ä¸å¯¹åº”äºç¦»æ•£è¾“å…¥ä»¤ç‰Œï¼Œå› æ­¤è¿™ç§æ½œåœ¨çš„è®­ç»ƒæ–¹æ³•å¾€å¾€ä½¿æ¨¡å‹å®¹æ˜“å—åˆ°ä¸€ç³»åˆ—ç¦»æ•£æ”»å‡»çš„å½±å“ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ—¨åœ¨é€šè¿‡å¼•å…¥MixATï¼ˆä¸€ç§ç»“åˆè®­ç»ƒè¿‡ç¨‹ä¸­æ›´å¼ºçš„ç¦»æ•£å’Œæ›´å¿«çš„è¿ç»­æ”»å‡»çš„æ–°æ–¹æ³•ï¼‰æ¥å¼¥è¡¥è¿™ä¸€å·®è·ã€‚æˆ‘ä»¬ä¸¥æ ¼è¯„ä¼°äº†MixATåœ¨å¤šç§æœ€å…ˆè¿›çš„æ”»å‡»æ–¹æ³•ä¸­çš„è¡¨ç°ï¼Œå¹¶æå‡ºäº†æ•è·æ¨¡å‹æœ€åæƒ…å†µä¸‹æ¼æ´çš„è‡³å°‘ä¸€ç§æ”»å‡»æˆåŠŸç‡ï¼ˆALO-ASRï¼‰æŒ‡æ ‡ã€‚æˆ‘ä»¬å±•ç¤ºäº†MixATç›¸æ¯”å…ˆå‰çš„é˜²å¾¡æ‰‹æ®µå®ç°äº†æ˜¾è‘—æ›´å¥½çš„ç¨³å¥æ€§ï¼ˆALO-ASR&lt;20%ï¼‰ï¼ŒåŒæ—¶ä¿æŒäº†ä¸åŸºäºè¿ç»­æ”¾æ¾çš„æ–¹æ³•ç›¸å½“çš„è¿è¡Œæ—¶é—´ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥åœ¨çœŸå®éƒ¨ç½²ç¯å¢ƒä¸­åˆ†æMixATï¼Œæ¢ç´¢èŠå¤©æ¨¡æ¿ã€é‡åŒ–ã€ä½é˜¶é€‚é…å™¨å’Œæ¸©åº¦å¦‚ä½•å½±å“å¯¹æŠ—æ€§è®­ç»ƒå’Œè¯„ä¼°ï¼Œæ­ç¤ºäº†å½“å‰æ–¹æ³•ä¸­çš„é¢å¤–ç›²ç‚¹ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒMixATçš„ç¦»æ•£è¿ç»­é˜²å¾¡æä¾›äº†ä¸€ç§æœ‰åŸåˆ™çš„ã€ä¼˜è¶Šçš„ç¨³å¥æ€§å‡†ç¡®æ€§æƒè¡¡ï¼Œå…·æœ‰æœ€å°çš„è®¡ç®—å¼€é”€ï¼Œå‡¸æ˜¾å…¶åœ¨æ„å»ºæ›´å®‰å…¨LLMæ–¹é¢çš„æ½œåŠ›ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/insait-institute/MixAT">https://github.com/insait-institute/MixAT</a>æä¾›äº†æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16947v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>è¿‘æœŸé’ˆå¯¹å‰æ²¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¯¹æŠ—æ€§æ”»å‡»ä»èƒ½å¤ŸæŒç»­äº§ç”Ÿæœ‰å®³ç”Ÿæˆã€‚å°½ç®¡å¯¹æŠ—æ€§è®­ç»ƒå·²å¹¿æ³›ç ”ç©¶å¹¶æ˜¾è‘—æé«˜äº†ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹çš„ç¨³å¥æ€§ï¼Œä½†åœ¨LLMèƒŒæ™¯ä¸‹å…¶ä¼˜ç¼ºç‚¹çŸ¥ä¹‹ç”šå°‘ã€‚ç°æœ‰ç¦»æ•£å¯¹æŠ—æ€§æ”»å‡»èƒ½æœ‰æ•ˆäº§ç”Ÿæœ‰å®³å†…å®¹ï¼Œä½†å¯¹LLMè¿›è¡Œå…·ä½“å¯¹æŠ—æ€§æç¤ºçš„è®­ç»ƒå¾€å¾€è®¡ç®—æˆæœ¬é«˜ï¼Œå¯¼è‡´å¯¹è¿ç»­æ¾å¼›æ–¹æ³•çš„ä¾èµ–ã€‚è¿™äº›æ–¹æ³•ä¸ç¦»æ•£è¾“å…¥æ ‡è®°å¹¶ä¸å¯¹åº”ï¼Œå› æ­¤åŸºäºæ½œä¼çš„è®­ç»ƒæ–¹æ³•å¸¸å¸¸ä½¿æ¨¡å‹é¢ä¸´å¤šç§ç¦»æ•£æ”»å‡»ã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡å¼•å…¥MixATï¼ˆä¸€ç§ç»“åˆæ›´å¼ºç¦»æ•£å’Œæ›´å¿«è¿ç»­æ”»å‡»çš„è®­ç»ƒæ–¹æ³•ï¼‰æ¥å¼¥è¡¥è¿™ä¸€å·®è·ã€‚æˆ‘ä»¬ä¸¥æ ¼è¯„ä¼°äº†MixATåœ¨å¤šç§æœ€å…ˆè¿›çš„æ”»å‡»ä¸‹çš„è¡¨ç°ï¼Œå¹¶æå‡ºALOS-ASRæŒ‡æ ‡æ¥æ•æ‰æ¨¡å‹çš„æœ€åæƒ…å†µè„†å¼±æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒMixATå®ç°äº†æ˜¾è‘—çš„ç¨³å¥æ€§ï¼ˆALOS-ASR&lt;20%ï¼‰ï¼Œç›¸æ¯”ä»¥å‰çš„é˜²å¾¡æ–¹æ³•ï¼ˆALOS-ASR&gt;50%ï¼‰ï¼ŒåŒæ—¶ä¿æŒä¸åŸºäºè¿ç»­æ¾å¼›çš„æ–¹æ³•ç›¸å½“çš„è¿è¡Œæ—¶é—´ã€‚æˆ‘ä»¬è¿˜åˆ†æäº†MixATåœ¨çœŸå®éƒ¨ç½²è®¾ç½®ä¸­çš„è¡¨ç°ï¼Œæ¢è®¨äº†èŠå¤©æ¨¡æ¿ã€é‡åŒ–ã€ä½ç§©é€‚é…å™¨å’Œæ¸©åº¦å¯¹å¯¹æŠ—æ€§è®­ç»ƒå’Œè¯„ä¼°çš„å½±å“ï¼Œæ­ç¤ºäº†å½“å‰æ–¹æ³•ä¸­çš„é¢å¤–ç›²ç‚¹ã€‚MixATçš„ç¦»æ•£è¿ç»­é˜²å¾¡æä¾›äº†æœ‰åŸåˆ™çš„ç¨³å¥æ€§å’Œå‡†ç¡®æ€§ä¹‹é—´çš„æƒè¡¡ï¼Œå…·æœ‰æœ€å°çš„è®¡ç®—å¼€é”€ï¼Œå‡¸æ˜¾å…¶åœ¨æ„å»ºæ›´å®‰å…¨LLMæ–¹é¢çš„æ½œåŠ›ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹æä¾›åœ¨<a target="_blank" rel="noopener" href="https://github.com/insait-institute/MixAT%E3%80%82">https://github.com/insait-institute/MixATã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å½“å‰å¯¹æŠ—æ€§æ”»å‡»ä»èƒ½å¯¹å‰æ²¿çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰äº§ç”Ÿæœ‰å®³ç”Ÿæˆã€‚</li>
<li>å¯¹æŠ—æ€§è®­ç»ƒåœ¨LLMä¸­çš„åº”ç”¨åŠå…¶ä¼˜ç¼ºç‚¹å°šä¸å®Œå…¨æ¸…æ¥šã€‚</li>
<li>ç°æœ‰ç¦»æ•£å¯¹æŠ—æ€§æ”»å‡»æœ‰æ•ˆï¼Œä½†è®­ç»ƒLLMçš„è®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œå¯¼è‡´å¯¹è¿ç»­æ¾å¼›æ–¹æ³•çš„ä¾èµ–ã€‚</li>
<li>MixATç»“åˆäº†ç¦»æ•£å’Œè¿ç»­æ”»å‡»ï¼Œå¡«è¡¥äº†è®­ç»ƒè¿‡ç¨‹ä¸­çš„å·®è·ã€‚</li>
<li>MixATåœ¨å¤šç§æ”»å‡»ä¸‹è¡¨ç°å‡ºæ›´é«˜çš„ç¨³å¥æ€§ï¼Œä¸å…ˆå‰çš„æ–¹æ³•ç›¸æ¯”å…·æœ‰æ›´ä½çš„ALOS-ASRå€¼ã€‚</li>
<li>MixATåœ¨çœŸå®éƒ¨ç½²ç¯å¢ƒä¸­çš„è¡¨ç°åˆ†ææ­ç¤ºäº†ç°æœ‰æ–¹æ³•ä¸­çš„ç›²ç‚¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16947">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9ce6baadde7125345eade7f6126bcf98.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-16649fc80ea9ed3ec6e39679de3f5623.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-49815cb15d8819859caeb45cc8417492.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-798b743c373a87d8bcc0ef03c45e2742.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="LLaDA-V-Large-Language-Diffusion-Models-with-Visual-Instruction-Tuning"><a href="#LLaDA-V-Large-Language-Diffusion-Models-with-Visual-Instruction-Tuning" class="headerlink" title="LLaDA-V: Large Language Diffusion Models with Visual Instruction Tuning"></a>LLaDA-V: Large Language Diffusion Models with Visual Instruction Tuning</h2><p><strong>Authors:Zebin You, Shen Nie, Xiaolu Zhang, Jun Hu, Jun Zhou, Zhiwu Lu, Ji-Rong Wen, Chongxuan Li</strong></p>
<p>In this work, we introduce LLaDA-V, a purely diffusion-based Multimodal Large Language Model (MLLM) that integrates visual instruction tuning with masked diffusion models, representing a departure from the autoregressive paradigms dominant in current multimodal approaches. Built upon LLaDA, a representative large language diffusion model, LLaDA-V incorporates a vision encoder and MLP connector that projects visual features into the language embedding space, enabling effective multimodal alignment. Our empirical investigation reveals several intriguing results: First, LLaDA-V demonstrates promising multimodal performance despite its language model being weaker on purely textual tasks than counterparts like LLaMA3-8B and Qwen2-7B. When trained on the same instruction data, LLaDA-V is highly competitive to LLaMA3-V across multimodal tasks with better data scalability. It also narrows the performance gap to Qwen2-VL, suggesting the effectiveness of its architecture for multimodal tasks. Second, LLaDA-V achieves state-of-the-art performance in multimodal understanding compared to existing hybrid autoregressive-diffusion and purely diffusion-based MLLMs. Our findings suggest that large language diffusion models show promise in multimodal contexts and warrant further investigation in future research. Project page and codes: <a target="_blank" rel="noopener" href="https://ml-gsai.github.io/LLaDA-V-demo/">https://ml-gsai.github.io/LLaDA-V-demo/</a>. </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†LLaDA-Vï¼Œè¿™æ˜¯ä¸€ä¸ªå®Œå…¨åŸºäºæ‰©æ•£çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œå®ƒå°†è§†è§‰æŒ‡ä»¤è°ƒæ•´ä¸æ©ç æ‰©æ•£æ¨¡å‹ç›¸ç»“åˆï¼Œè¿™æ ‡å¿—ç€å®ƒä¸å½“å‰å¤šæ¨¡æ€æ–¹æ³•ä¸­å ä¸»å¯¼åœ°ä½çš„è‡ªå›å½’èŒƒå¼çš„åç¦»ã€‚LLaDA-Vå»ºç«‹åœ¨ä»£è¡¨æ€§çš„å¤§å‹è¯­è¨€æ‰©æ•£æ¨¡å‹LLaDAä¹‹ä¸Šï¼Œèå…¥äº†è§†è§‰ç¼–ç å™¨å’ŒMLPè¿æ¥å™¨ï¼Œå°†è§†è§‰ç‰¹å¾æŠ•å°„åˆ°è¯­è¨€åµŒå…¥ç©ºé—´ï¼Œä»è€Œå®ç°æœ‰æ•ˆçš„å¤šæ¨¡æ€å¯¹é½ã€‚æˆ‘ä»¬çš„å®è¯ç ”ç©¶æ­ç¤ºäº†å‡ ä¸ªæœ‰è¶£çš„ç»“æœï¼šé¦–å…ˆï¼Œå°½ç®¡LLaDA-Våœ¨è¯­è¨€æ¨¡å‹æ‰§è¡Œçº¯æ–‡æœ¬ä»»åŠ¡æ–¹é¢è¾ƒLLaMA3-8Bå’ŒQwen2-7Bç­‰åŒè¡Œè¡¨ç°è¾ƒå¼±ï¼Œä½†å®ƒä»å±•ç°å‡ºæœ‰å‰æ™¯çš„å¤šæ¨¡æ€æ€§èƒ½ã€‚åœ¨ç›¸åŒçš„æŒ‡ä»¤æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒæ—¶ï¼ŒLLaDA-Våœ¨å¤šæ¨¡æ€ä»»åŠ¡æ–¹é¢ä¸LLaMA3-Væå…·ç«äº‰åŠ›ï¼Œå¹¶ä¸”å…·æœ‰æ›´å¥½çš„æ•°æ®å¯æ‰©å±•æ€§ã€‚å®ƒè¿˜ç¼©å°äº†ä¸Qwen2-VLçš„æ€§èƒ½å·®è·ï¼Œè¿™è¡¨æ˜å…¶æ¶æ„å¯¹äºå¤šæ¨¡æ€ä»»åŠ¡çš„æœ‰æ•ˆæ€§ã€‚å…¶æ¬¡ï¼Œä¸ç°æœ‰çš„æ··åˆè‡ªå›å½’-æ‰©æ•£å’Œçº¯æ‰©æ•£çš„MLLMç›¸æ¯”ï¼ŒLLaDA-Våœ¨å¤šæ¨¡æ€ç†è§£æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¤§å‹è¯­è¨€æ‰©æ•£æ¨¡å‹åœ¨å¤šæ¨¡æ€ä¸Šä¸‹æ–‡ä¸­æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œå€¼å¾—åœ¨æœªæ¥çš„ç ”ç©¶ä¸­è¿›ä¸€æ­¥æ¢è®¨ã€‚é¡¹ç›®é¡µé¢å’Œä»£ç ï¼š<a target="_blank" rel="noopener" href="https://ml-gsai.github.io/LLaDA-V-demo/">https://ml-gsai.github.io/LLaDA-V-demo/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16933v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LLaDA-Væ˜¯ä¸€æ¬¾åŸºäºæ‰©æ•£çš„çº¯å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œç»“åˆäº†è§†è§‰æŒ‡ä»¤å¾®è°ƒä¸æ©ç æ‰©æ•£æ¨¡å‹ï¼Œçªç ´äº†å½“å‰å¤šæ¨¡æ€æ–¹æ³•ä¸­çš„è‡ªå›å½’èŒƒå¼ã€‚è¯¥æ¨¡å‹åœ¨LLaDAåŸºç¡€ä¸Šï¼ŒåŠ å…¥è§†è§‰ç¼–ç å™¨å’ŒMLPè¿æ¥å™¨ï¼Œå°†è§†è§‰ç‰¹å¾æŠ•å°„åˆ°è¯­è¨€åµŒå…¥ç©ºé—´ï¼Œå®ç°æœ‰æ•ˆå¤šæ¨¡æ€å¯¹é½ã€‚åœ¨ç›¸åŒæŒ‡ä»¤æ•°æ®è®­ç»ƒä¸‹ï¼ŒLLaDA-Våœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸Šå…·æœ‰å‡ºè‰²è¡¨ç°ï¼Œå³ä½¿å…¶çº¯æ–‡æœ¬ä»»åŠ¡è¡¨ç°ç›¸å¯¹è¾ƒå¼±ã€‚åŒæ—¶ï¼ŒLLaDA-Vç¼©å°äº†ä¸é¡¶å°–æ¨¡å‹çš„æ€§èƒ½å·®è·ï¼Œå±•ç°äº†å…¶æ¶æ„åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸Šçš„æœ‰æ•ˆæ€§ã€‚å®ƒæ˜¯å½“å‰æœ€å…ˆè¿›çš„çº¯æ‰©æ•£å’Œæ··åˆè‡ªå›å½’æ‰©æ•£å¤šæ¨¡æ€ç†è§£æ¨¡å‹ä¹‹ä¸€ã€‚æœªæ¥ç ”ç©¶éœ€è¦è¿›ä¸€æ­¥æ¢ç´¢å¤§è¯­è¨€æ‰©æ•£æ¨¡å‹åœ¨å¤šæ¨¡æ€é¢†åŸŸçš„æ½œåŠ›ã€‚æ›´å¤šä¿¡æ¯å¯é€šè¿‡é“¾æ¥è®¿é—®ï¼š[é“¾æ¥åœ°å€]ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLaDA-Væ˜¯åŸºäºæ‰©æ•£çš„çº¯å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œä¸åŒäºç°æœ‰çš„è‡ªå›å½’èŒƒå¼æ–¹æ³•ã€‚</li>
<li>LLaDA-Vç»“åˆäº†è§†è§‰æŒ‡ä»¤å¾®è°ƒä¸æ©ç æ‰©æ•£æ¨¡å‹ï¼Œå®ç°äº†å¤šæ¨¡æ€å¯¹é½ã€‚</li>
<li>åœ¨ç›¸åŒæŒ‡ä»¤æ•°æ®è®­ç»ƒä¸‹ï¼ŒLLaDA-Våœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜ç§€ï¼Œå°½ç®¡å…¶çº¯æ–‡æœ¬ä»»åŠ¡æ€§èƒ½ç›¸å¯¹è¾ƒå¼±ã€‚</li>
<li>LLaDA-Vç¼©å°äº†ä¸é¡¶å°–æ¨¡å‹åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸Šçš„æ€§èƒ½å·®è·ã€‚</li>
<li>LLaDA-Vå±•ç°äº†å…¶æ¶æ„åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸Šçš„æœ‰æ•ˆæ€§ï¼Œç‰¹åˆ«æ˜¯è§†è§‰ç¼–ç å™¨å’ŒMLPè¿æ¥å™¨çš„ä½¿ç”¨ã€‚</li>
<li>LLaDA-Vè¾¾åˆ°äº†å½“å‰æœ€å…ˆè¿›çš„çº¯æ‰©æ•£å’Œæ··åˆè‡ªå›å½’æ‰©æ•£å¤šæ¨¡æ€ç†è§£æ¨¡å‹çš„æ€§èƒ½æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16933">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d9181cfbc2d145507a43e2ad4c69d9b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d39ba6cda7ca7adcb1c2dc7aad10ad18.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed659bfa20dbe4dba03864406d86a419.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5fdf1cba7ea415778817b386220d68cc.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Locate-then-Merge-Neuron-Level-Parameter-Fusion-for-Mitigating-Catastrophic-Forgetting-in-Multimodal-LLMs"><a href="#Locate-then-Merge-Neuron-Level-Parameter-Fusion-for-Mitigating-Catastrophic-Forgetting-in-Multimodal-LLMs" class="headerlink" title="Locate-then-Merge: Neuron-Level Parameter Fusion for Mitigating   Catastrophic Forgetting in Multimodal LLMs"></a>Locate-then-Merge: Neuron-Level Parameter Fusion for Mitigating   Catastrophic Forgetting in Multimodal LLMs</h2><p><strong>Authors:Zeping Yu, Sophia Ananiadou</strong></p>
<p>Although multimodal large language models (MLLMs) have achieved impressive performance, the multimodal instruction tuning stage often causes catastrophic forgetting of the base LLMâ€™s language ability, even in strong models like Llama3. To address this, we propose Locate-then-Merge, a training-free parameter fusion framework that first locates important parameters and then selectively merges them. We further introduce Neuron-Fusion, a neuron-level strategy that preserves the influence of neurons with large parameter shiftsâ€“neurons likely responsible for newly acquired visual capabilitiesâ€“while attenuating the influence of neurons with smaller changes that likely encode general-purpose language skills. This design enables better retention of visual adaptation while mitigating language degradation. Experiments on 13 benchmarks across both language and visual tasks show that Neuron-Fusion consistently outperforms existing model merging methods. Further analysis reveals that our method effectively reduces context hallucination in generation. </p>
<blockquote>
<p>è™½ç„¶å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å·²ç»å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ï¼Œä½†å¤šæ¨¡æ€æŒ‡ä»¤è°ƒæ•´é˜¶æ®µå¾€å¾€ä¼šå¯¼è‡´åŸºç¡€LLMè¯­è¨€èƒ½åŠ›å‘ç”Ÿç¾éš¾æ€§é—å¿˜ï¼Œç”šè‡³åœ¨Llama3ç­‰å¼ºå¤§æ¨¡å‹ä¸­ä¹Ÿæ˜¯å¦‚æ­¤ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Locate-then-Mergeç­–ç•¥ï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒå³å¯è¿›è¡Œå‚æ•°èåˆçš„æ–¹æ³•ï¼Œèƒ½å¤Ÿé¦–å…ˆå®šä½é‡è¦å‚æ•°ï¼Œç„¶åæœ‰é€‰æ‹©åœ°è¿›è¡Œèåˆã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ç¥ç»å…ƒçº§èåˆç­–ç•¥Neuron-Fusionï¼Œèƒ½å¤Ÿåœ¨ä¿ç•™è´Ÿè´£æ–°è·å¾—è§†è§‰åŠŸèƒ½çš„ç¥ç»å…ƒå½±å“çš„åŒæ—¶ï¼Œå‰Šå¼±è´Ÿè´£é€šç”¨è¯­è¨€æŠ€èƒ½çš„ç¥ç»å…ƒå½±å“ã€‚è¿™ç§è®¾è®¡èƒ½å¤Ÿåœ¨ä¿ç•™è§†è§‰é€‚åº”çš„åŒæ—¶ï¼Œå‡è½»è¯­è¨€é€€åŒ–çš„é—®é¢˜ã€‚åœ¨æ¶µç›–è¯­è¨€å’Œè§†è§‰ä»»åŠ¡çš„13ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒNeuron-Fusionæ–¹æ³•å§‹ç»ˆä¼˜äºç°æœ‰æ¨¡å‹èåˆæ–¹æ³•ã€‚è¿›ä¸€æ­¥çš„åˆ†æè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æœ‰æ•ˆåœ°å‡å°‘äº†ç”Ÿæˆè¿‡ç¨‹ä¸­çš„ä¸Šä¸‹æ–‡å¹»è§‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16703v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨åŸºç¡€è¯­è¨€æ¨¡å‹èƒ½åŠ›ä¸Šå–å¾—äº†æ˜¾è‘—æˆæ•ˆï¼Œä½†åœ¨å¤šæ¨¡æ€æŒ‡ä»¤è°ƒæ•´é˜¶æ®µï¼Œå³ä½¿æ˜¯åƒLlama3è¿™æ ·çš„å¼ºå¤§æ¨¡å‹ä¹Ÿä¼šå‡ºç°åŸºç¡€LLMè¯­è¨€èƒ½åŠ›çš„ç¾éš¾æ€§é—å¿˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†æ— è®­ç»ƒå‚æ•°èåˆæ¡†æ¶Locate-then-Mergeï¼Œè¯¥æ¡†æ¶é¦–å…ˆå®šä½é‡è¦å‚æ•°ï¼Œç„¶åæœ‰é€‰æ‹©åœ°è¿›è¡Œåˆå¹¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ç¥ç»å…ƒå±‚é¢çš„ç­–ç•¥â€”â€”Neuron-Fusionï¼Œè¯¥ç­–ç•¥ä¿ç•™å…·æœ‰è¾ƒå¤§å‚æ•°å˜åŒ–çš„ç¥ç»å…ƒçš„å½±å“ï¼Œè¿™äº›ç¥ç»å…ƒå¯èƒ½è´Ÿè´£æ–°è·å¾—çš„è§†è§‰èƒ½åŠ›ï¼ŒåŒæ—¶å‡å¼±å…·æœ‰è¾ƒå°å˜åŒ–çš„ç¥ç»å…ƒçš„å½±å“ï¼Œè¿™äº›ç¥ç»å…ƒå¯èƒ½ç¼–ç é€šç”¨è¯­è¨€æŠ€èƒ½ã€‚è¿™ç§è®¾è®¡èƒ½å¤Ÿåœ¨ä¿ç•™è§†è§‰é€‚åº”æ€§çš„åŒæ—¶ï¼Œå‡è½»è¯­è¨€é€€åŒ–çš„é—®é¢˜ã€‚åœ¨è¯­è¨€å’Œè§†è§‰ä»»åŠ¡ä¸Šçš„13ä¸ªåŸºå‡†æµ‹è¯•æ˜¾ç¤ºï¼ŒNeuron-Fusionåœ¨æ¨¡å‹åˆå¹¶æ–¹æ³•ä¸Šè¡¨ç°å“è¶Šã€‚è¿›ä¸€æ­¥çš„åˆ†æè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æœ‰æ•ˆåœ°å‡å°‘äº†ç”Ÿæˆè¿‡ç¨‹ä¸­çš„ä¸Šä¸‹æ–‡å¹»è§‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æŒ‡ä»¤è°ƒæ•´é˜¶æ®µå¯èƒ½ä¼šå‡ºç°åŸºç¡€è¯­è¨€èƒ½åŠ›çš„é—å¿˜é—®é¢˜ã€‚</li>
<li>Locate-then-Mergeæ¡†æ¶é€šè¿‡å®šä½å¹¶åˆå¹¶é‡è¦å‚æ•°æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚</li>
<li>Neuron-Fusionæ˜¯ç¥ç»å…ƒå±‚é¢çš„ç­–ç•¥ï¼Œä¿ç•™è´Ÿè´£æ–°è§†è§‰èƒ½åŠ›çš„ç¥ç»å…ƒå½±å“ï¼ŒåŒæ—¶å‡å¼±é€šç”¨è¯­è¨€æŠ€èƒ½çš„ç¥ç»å…ƒå½±å“ã€‚</li>
<li>è¯¥è®¾è®¡èƒ½åœ¨ä¿ç•™è§†è§‰é€‚åº”æ€§çš„åŒæ—¶å‡è½»è¯­è¨€é€€åŒ–é—®é¢˜ã€‚</li>
<li>åœ¨è¯­è¨€å’Œè§†è§‰ä»»åŠ¡ä¸Šçš„å¤šä¸ªåŸºå‡†æµ‹è¯•æ˜¾ç¤ºNeuron-Fusionä¼˜äºç°æœ‰æ¨¡å‹åˆå¹¶æ–¹æ³•ã€‚</li>
<li>Neuron-Fusionèƒ½æœ‰æ•ˆå‡å°‘ç”Ÿæˆè¿‡ç¨‹ä¸­çš„ä¸Šä¸‹æ–‡å¹»è§‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16703">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-03e6b6b4e0c6e0462380c4cdd2105651.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-71583f9cf5d47763a2501a6f93b1e2e6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a8b05c9df8836c2220637a1d9a593bf8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-24ed381d0158fda10785056980b4dbd2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-71f49d8eb7a00a98b22075b5e2da87db.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Point-Detect-Count-Multi-Task-Medical-Image-Understanding-with-Instruction-Tuned-Vision-Language-Models"><a href="#Point-Detect-Count-Multi-Task-Medical-Image-Understanding-with-Instruction-Tuned-Vision-Language-Models" class="headerlink" title="Point, Detect, Count: Multi-Task Medical Image Understanding with   Instruction-Tuned Vision-Language Models"></a>Point, Detect, Count: Multi-Task Medical Image Understanding with   Instruction-Tuned Vision-Language Models</h2><p><strong>Authors:Sushant Gautam, Michael A. Riegler, PÃ¥l Halvorsen</strong></p>
<p>We investigate fine-tuning Vision-Language Models (VLMs) for multi-task medical image understanding, focusing on detection, localization, and counting of findings in medical images. Our objective is to evaluate whether instruction-tuned VLMs can simultaneously improve these tasks, with the goal of enhancing diagnostic accuracy and efficiency. Using MedMultiPoints, a multimodal dataset with annotations from endoscopy (polyps and instruments) and microscopy (sperm cells), we reformulate each task into instruction-based prompts suitable for vision-language reasoning. We fine-tune Qwen2.5-VL-7B-Instruct using Low-Rank Adaptation (LoRA) across multiple task combinations. Results show that multi-task training improves robustness and accuracy. For example, it reduces the Count Mean Absolute Error (MAE) and increases Matching Accuracy in the Counting + Pointing task. However, trade-offs emerge, such as more zero-case point predictions, indicating reduced reliability in edge cases despite overall performance gains. Our study highlights the potential of adapting general-purpose VLMs to specialized medical tasks via prompt-driven fine-tuning. This approach mirrors clinical workflows, where radiologists simultaneously localize, count, and describe findings - demonstrating how VLMs can learn composite diagnostic reasoning patterns. The model produces interpretable, structured outputs, offering a promising step toward explainable and versatile medical AI. Code, model weights, and scripts will be released for reproducibility at <a target="_blank" rel="noopener" href="https://github.com/simula/PointDetectCount">https://github.com/simula/PointDetectCount</a>. </p>
<blockquote>
<p>æˆ‘ä»¬ç ”ç©¶äº†é’ˆå¯¹å¤šä»»åŠ¡åŒ»å­¦å½±åƒç†è§£çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å¾®è°ƒï¼Œé‡ç‚¹ç ”ç©¶åŒ»å­¦å½±åƒä¸­çš„æ£€æµ‹ç»“æœæ£€æµ‹ã€å®šä½å’Œè®¡æ•°ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯è¯„ä¼°æŒ‡ä»¤è°ƒæ•´åçš„VLMsæ˜¯å¦èƒ½åŒæ—¶æ”¹è¿›è¿™äº›ä»»åŠ¡ï¼Œä»¥æé«˜è¯Šæ–­å’Œæ•ˆç‡ã€‚æˆ‘ä»¬ä½¿ç”¨MedMultiPointsæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«æ¥è‡ªå†…çª¥é•œï¼ˆæ¯è‚‰å’Œä»ªå™¨ï¼‰å’Œæ˜¾å¾®é•œï¼ˆç²¾å­ç»†èƒï¼‰çš„æ³¨é‡Šï¼Œæˆ‘ä»¬å°†æ¯ä¸ªä»»åŠ¡é‡æ–°åˆ¶å®šä¸ºé€‚åˆè§†è§‰è¯­è¨€æ¨ç†çš„æŒ‡ä»¤æç¤ºã€‚æˆ‘ä»¬ä½¿ç”¨ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰å¯¹Qwen2.5-VL-7B-Instructè¿›è¡Œå¾®è°ƒï¼Œè¿›è¡Œå¤šç§ä»»åŠ¡ç»„åˆã€‚ç»“æœè¡¨æ˜ï¼Œå¤šä»»åŠ¡è®­ç»ƒæé«˜äº†ç¨³å¥æ€§å’Œå‡†ç¡®æ€§ã€‚ä¾‹å¦‚ï¼Œå®ƒå‡å°‘äº†è®¡æ•°å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰ï¼Œå¹¶æé«˜äº†è®¡æ•°+å®šä½ä»»åŠ¡çš„åŒ¹é…å‡†ç¡®ç‡ã€‚ç„¶è€Œï¼Œä¹Ÿå‡ºç°äº†ä¸€äº›æƒè¡¡ï¼Œä¾‹å¦‚æ›´å¤šçš„é›¶æ¡ˆä¾‹ç‚¹é¢„æµ‹ï¼Œè¿™è¡¨æ˜å°½ç®¡æ€»ä½“æ€§èƒ½æœ‰æ‰€æå‡ï¼Œä½†åœ¨è¾¹ç¼˜æƒ…å†µä¸‹å¯é æ€§æœ‰æ‰€é™ä½ã€‚æˆ‘ä»¬çš„ç ”ç©¶çªå‡ºäº†é€šè¿‡æç¤ºé©±åŠ¨çš„å¾®è°ƒå°†é€šç”¨VLMsé€‚åº”ä¸“ä¸šåŒ»ç–—ä»»åŠ¡çš„æ½œåŠ›ã€‚è¿™ç§æ–¹æ³•åæ˜ äº†ä¸´åºŠå·¥ä½œæµç¨‹ï¼Œæ”¾å°„ç§‘åŒ»ç”Ÿå¯ä»¥åŒæ—¶å®šä½ã€è®¡æ•°å’Œæè¿°å‘ç°ç»“æœï¼Œè¯æ˜äº†VLMså¦‚ä½•å­¦ä¹ å¤åˆè¯Šæ–­æ¨ç†æ¨¡å¼ã€‚è¯¥æ¨¡å‹äº§ç”Ÿå¯è§£é‡Šçš„ç»“æ„åŒ–è¾“å‡ºï¼Œä¸ºåŒ»ç–—äººå·¥æ™ºèƒ½çš„å¯è§£é‡Šæ€§å’Œå¤šåŠŸèƒ½æ€§æä¾›äº†æœ‰å¸Œæœ›çš„è¿›æ­¥ã€‚ä»£ç ã€æ¨¡å‹æƒé‡å’Œè„šæœ¬å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/simula/PointDetectCount%E4%B8%8A%E7%BD%AE%E5%B9%BF%EF%BC%8C%E4%BB%A5%E4%BE%9D%E6%9C%AC%E7%A0%94%E7%A9%B6%E7%BB%93%E6%9E%9C%E3%80%82">https://github.com/simula/PointDetectCountä¸Šå‘å¸ƒï¼Œä»¥ä¾›é‡ç°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16647v1">PDF</a> Accepted as a full paper at the 38th IEEE International Symposium on   Computer-Based Medical Systems (CBMS) 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬ç ”ç©¶è°ƒæŸ¥äº†é€šè¿‡å¾®è°ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ç”¨äºå¤šä»»åŠ¡åŒ»ç–—å›¾åƒç†è§£ï¼Œä¸»è¦å…³æ³¨åŒ»ç–—å›¾åƒä¸­çš„æ£€æµ‹ã€å®šä½å’Œè®¡æ•°ä»»åŠ¡ã€‚ç›®çš„æ˜¯è¯„ä¼°æŒ‡ä»¤è°ƒä¼˜çš„VLMsæ˜¯å¦èƒ½åŒæ—¶æ”¹è¿›è¿™äº›ä»»åŠ¡ï¼Œä»è€Œæé«˜è¯Šæ–­å’Œæ•ˆç‡ã€‚ä½¿ç”¨MedMultiPointå¤šæ¨¡æ€æ•°æ®é›†è¿›è¡Œå¾®è°ƒï¼Œè¯¥æ•°æ®é›†åŒ…å«å†…çª¥é•œï¼ˆæ¯è‚‰å’Œä»ªå™¨ï¼‰å’Œæ˜¾å¾®é•œï¼ˆç²¾å­ç»†èƒï¼‰çš„æ³¨é‡Šï¼Œé€‚åˆè§†è§‰è¯­è¨€æ¨ç†çš„æŒ‡ä»¤åŸºç¡€æç¤ºã€‚é€šè¿‡LoRAè·¨å¤šä¸ªä»»åŠ¡ç»„åˆå¯¹Qwen2.5-VL-7B-Instructè¿›è¡Œå¾®è°ƒã€‚ç»“æœæ˜¾ç¤ºå¤šä»»åŠ¡è®­ç»ƒèƒ½æé«˜ç¨³å¥æ€§å’Œå‡†ç¡®æ€§ã€‚ä¾‹å¦‚ï¼Œå®ƒé™ä½äº†è®¡æ•°å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰ï¼Œå¹¶æé«˜äº†è®¡æ•°åŠ å®šä½ä»»åŠ¡çš„åŒ¹é…å‡†ç¡®æ€§ã€‚ä½†å‡ºç°æƒè¡¡ï¼Œä¾‹å¦‚æ›´å¤šé›¶æ¡ˆä¾‹ç‚¹é¢„æµ‹ï¼Œè¡¨æ˜å°½ç®¡æ€»ä½“æ€§èƒ½æœ‰æ‰€æå‡ï¼Œä½†åœ¨è¾¹ç¼˜æƒ…å†µä¸‹å¯é æ€§é™ä½ã€‚æœ¬ç ”ç©¶çªæ˜¾äº†é€šè¿‡æç¤ºé©±åŠ¨å¾®è°ƒå°†é€šç”¨VLMsé€‚åº”ä¸“ä¸šåŒ»ç–—ä»»åŠ¡çš„æ½œåŠ›ã€‚è¿™ç§æ–¹æ³•æ¨¡æ‹Ÿäº†æ”¾å°„ç§‘åŒ»ç”ŸåŒæ—¶å®šä½ã€è®¡æ•°å’Œæè¿°å‘ç°çš„ä¸´åºŠå·¥ä½œæµç¨‹ï¼Œå±•ç¤ºäº†VLMså¦‚ä½•å­¦ä¹ å¤åˆè¯Šæ–­æ¨ç†æ¨¡å¼ã€‚è¯¥æ¨¡å‹äº§ç”Ÿå¯è§£é‡Šçš„ç»“æ„åŒ–è¾“å‡ºï¼Œä¸ºåŒ»ç–—äººå·¥æ™ºèƒ½çš„å¯é å’Œå¤šåŠŸèƒ½æ€§æä¾›äº†æœ‰å¸Œæœ›çš„æ­¥éª¤ã€‚ç›¸å…³ä»£ç ã€æ¨¡å‹æƒé‡å’Œè„šæœ¬å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/simula/PointDetectCount%E5%8F%91%E5%B8%83%EF%BC%8C%E4%BB%A5%E4%BF%83%E8%BF%9B%E5%8F%AF%E9%87%8D%E5%A4%8D%E6%80%A7%E3%80%82">https://github.com/simula/PointDetectCountå‘å¸ƒï¼Œä»¥ä¿ƒè¿›å¯é‡å¤æ€§ã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç ”ç©¶æ¢ç´¢äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤šä»»åŠ¡åŒ»ç–—å›¾åƒç†è§£ä¸­çš„åº”ç”¨ï¼ŒåŒ…æ‹¬æ£€æµ‹ã€å®šä½å’Œè®¡æ•°ä»»åŠ¡ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨MedMultiPointså¤šæ¨¡æ€æ•°æ®é›†è¿›è¡Œå¾®è°ƒï¼Œå®éªŒè¯„ä¼°äº†æŒ‡ä»¤ä¼˜åŒ–çš„VLMsåœ¨æ”¹è¿›è¿™äº›ä»»åŠ¡æ–¹é¢çš„æ½œåŠ›ã€‚</li>
<li>LoRAæ–¹æ³•ç”¨äºå¾®è°ƒæ¨¡å‹ï¼Œå¹¶åœ¨å¤šä¸ªä»»åŠ¡ç»„åˆä¸Šå–å¾—è‰¯å¥½æ•ˆæœã€‚</li>
<li>å¤šä»»åŠ¡è®­ç»ƒæé«˜äº†æ¨¡å‹çš„ç¨³å¥æ€§å’Œå‡†ç¡®æ€§ï¼Œåœ¨è®¡æ•°ä»»åŠ¡ä¸­è¡¨ç°å°¤ä¸ºæ˜¾è‘—ã€‚</li>
<li>è™½ç„¶æ€»ä½“æ€§èƒ½æå‡ï¼Œä½†åœ¨è¾¹ç¼˜æƒ…å†µä¸‹å‡ºç°å¯é æ€§é™ä½çš„æƒè¡¡é—®é¢˜ã€‚</li>
<li>ç ”ç©¶å¼ºè°ƒäº†é€šè¿‡æç¤ºé©±åŠ¨å¾®è°ƒå°†é€šç”¨VLMsé€‚åº”ä¸“ä¸šåŒ»ç–—ä»»åŠ¡çš„é‡è¦æ€§ï¼Œè¿™ä¸ä¸´åºŠå·¥ä½œæµç¨‹ç›¸ç¬¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16647">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-bb2c99815c192704a3f055f58582fd18.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3aec9cd89c3d9eff3027bd6f9c0b4113.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4147ce1c6eedaad57f24d207c7349a2f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5d18b3daedcd78a2d97fcf57b8b6107a.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="PMPO-Probabilistic-Metric-Prompt-Optimization-for-Small-and-Large-Language-Models"><a href="#PMPO-Probabilistic-Metric-Prompt-Optimization-for-Small-and-Large-Language-Models" class="headerlink" title="PMPO: Probabilistic Metric Prompt Optimization for Small and Large   Language Models"></a>PMPO: Probabilistic Metric Prompt Optimization for Small and Large   Language Models</h2><p><strong>Authors:Chenzhuo Zhao, Ziqian Liu, Xingda Wang, Junting Lu, Chaoyi Ruan</strong></p>
<p>Prompt optimization offers a practical and broadly applicable alternative to fine-tuning for improving large language model (LLM) performance. However, existing methods often rely on costly output generation, self-critiquing abilities, or human-annotated preferences, which limit their scalability, especially for smaller or non-instruction-tuned models. We introduce PMPO (Probabilistic Metric Prompt Optimization), a unified framework that refines prompts using token-level cross-entropy loss as a direct, lightweight evaluation signal. PMPO identifies low-quality prompt segments by masking and measuring their impact on loss, then rewrites and selects improved variants by minimizing loss over positive and negative examples. Unlike prior methods, it requires no output sampling or human evaluation during optimization, relying only on forward passes and log-likelihoods. PMPO supports both supervised and preference-based tasks through a closely aligned loss-based evaluation strategy. Experiments show that PMPO consistently outperforms prior methods across model sizes and tasks: it achieves the highest average accuracy on BBH, performs strongly on GSM8K and AQUA-RAT, and improves AlpacaEval 2.0 win rates by over 19 points. These results highlight PMPOâ€™s effectiveness, efficiency, and broad applicability. </p>
<blockquote>
<p>æç¤ºä¼˜åŒ–ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ€§èƒ½çš„æå‡æä¾›äº†ä¸€ç§å®ç”¨ä¸”å¹¿æ³›åº”ç”¨æ›¿ä»£å¾®è°ƒçš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºæ˜‚è´µçš„è¾“å‡ºç”Ÿæˆã€è‡ªæˆ‘æ‰¹åˆ¤èƒ½åŠ›æˆ–äººå·¥æ ‡æ³¨çš„åå¥½ï¼Œè¿™é™åˆ¶äº†å…¶å¯æ‰©å±•æ€§ï¼Œå°¤å…¶æ˜¯å¯¹äºè¾ƒå°æˆ–æœªç»æŒ‡ä»¤è°ƒæ•´è¿‡çš„æ¨¡å‹ã€‚æˆ‘ä»¬å¼•å…¥äº†PMPOï¼ˆæ¦‚ç‡åº¦é‡æç¤ºä¼˜åŒ–ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œä½¿ç”¨ä»¤ç‰Œçº§åˆ«çš„äº¤å‰ç†µæŸå¤±ä½œä¸ºç›´æ¥ã€è½»é‡çº§çš„è¯„ä¼°ä¿¡å·æ¥ä¼˜åŒ–æç¤ºã€‚PMPOé€šè¿‡æ©è”½å¹¶æµ‹é‡ä½è´¨é‡æç¤ºæ®µå¯¹æŸå¤±çš„å½±å“æ¥è¯†åˆ«å®ƒä»¬ï¼Œç„¶åé€šè¿‡æœ€å°åŒ–æ­£è´Ÿä¾‹çš„æŸå¤±æ¥é‡å†™å’Œé€‰æ‹©æ”¹è¿›åçš„å˜ä½“ã€‚ä¸åŒäºä»¥å¾€çš„æ–¹æ³•ï¼Œå®ƒåœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­ä¸éœ€è¦è¾“å‡ºé‡‡æ ·æˆ–äººå·¥è¯„ä¼°ï¼Œä»…ä¾èµ–äºå‰å‘ä¼ é€’å’Œæ—¥å¿—ä¼¼ç„¶ã€‚PMPOé€šè¿‡ç´§å¯†å¯¹é½çš„æŸå¤±è¯„ä¼°ç­–ç•¥æ”¯æŒç›‘ç£å­¦ä¹ å’ŒåŸºäºåå¥½çš„ä»»åŠ¡ã€‚å®éªŒè¡¨æ˜ï¼ŒPMPOåœ¨æ¨¡å‹å¤§å°å’Œä»»åŠ¡æ–¹é¢å‡ä¼˜äºä»¥å‰çš„æ–¹æ³•ï¼šå®ƒåœ¨BBHä¸Šè¾¾åˆ°äº†æœ€é«˜çš„å¹³å‡å‡†ç¡®ç‡ï¼Œåœ¨GSM8Kå’ŒAQUA-RATä¸Šè¡¨ç°å¼ºåŠ²ï¼Œå¹¶å°†AlpacaEval 2.0çš„èƒœç‡æé«˜äº†è¶…è¿‡19ä¸ªç™¾åˆ†ç‚¹ã€‚è¿™äº›ç»“æœçªå‡ºäº†PMPOçš„æœ‰æ•ˆæ€§ã€æ•ˆç‡å’Œå¹¿æ³›çš„é€‚ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16307v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¼˜åŒ–ä¸­ï¼Œç°æœ‰æ–¹æ³•å¤šä¾èµ–æ˜‚è´µä¸”å¤æ‚çš„è¾“å‡ºç”Ÿæˆå’Œè‡ªæˆ‘æ‰¹åˆ¤èƒ½åŠ›æˆ–äººç±»æ ‡æ³¨åå¥½ï¼Œéš¾ä»¥è¿›è¡Œè§„æ¨¡åŒ–åº”ç”¨ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ¦‚ç‡åº¦é‡çš„æç¤ºä¼˜åŒ–ï¼ˆPMPOï¼‰æ¡†æ¶ï¼Œé€šè¿‡ä»¤ç‰Œçº§åˆ«çš„äº¤å‰ç†µæŸå¤±ä½œä¸ºç›´æ¥ã€è½»é‡çº§çš„è¯„ä¼°ä¿¡å·æ¥ä¼˜åŒ–æç¤ºã€‚è¯¥æ–¹æ³•æ— éœ€è¾“å‡ºé‡‡æ ·æˆ–äººä¸ºè¯„ä¼°ï¼Œä»…ä¾èµ–å‰å‘ä¼ æ’­å’Œæ—¥å¿—ä¼¼ç„¶ç‡å³å¯å®Œæˆä¼˜åŒ–è¿‡ç¨‹ã€‚å®éªŒç»“æœè¯æ˜äº†PMPOçš„æœ‰æ•ˆæ€§å’Œé«˜æ•ˆæ€§ï¼Œä¸”åœ¨è·¨æ¨¡å‹å’Œè·¨ä»»åŠ¡åœºæ™¯ä¸‹è¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>PMPOæ˜¯ä¸€ç§å®ç”¨çš„LLMæ€§èƒ½æå‡æ–¹æ³•ï¼Œé€‚ç”¨äºå¤šç§æ¨¡å‹å’Œä»»åŠ¡ã€‚</li>
<li>ç°æœ‰ä¼˜åŒ–æ–¹æ³•å­˜åœ¨æˆæœ¬é«˜ã€éš¾ä»¥è§„æ¨¡åŒ–çš„é—®é¢˜ï¼Œè€ŒPMPOé€šè¿‡æ¦‚ç‡åº¦é‡æç¤ºä¼˜åŒ–è§£å†³äº†è¿™äº›é—®é¢˜ã€‚</li>
<li>PMPOåˆ©ç”¨äº¤å‰ç†µæŸå¤±ä½œä¸ºè¯„ä¼°ä¿¡å·æ¥è¯†åˆ«ä½è´¨é‡æç¤ºç‰‡æ®µå¹¶è¿›è¡Œé‡å†™é€‰æ‹©ã€‚</li>
<li>PMPOæ— éœ€è¾“å‡ºé‡‡æ ·å’Œäººä¸ºè¯„ä¼°ï¼Œæé«˜äº†ä¼˜åŒ–çš„æ•ˆç‡å’Œä¾¿æ·æ€§ã€‚</li>
<li>PMPOæ”¯æŒç›‘ç£å­¦ä¹ å’Œåå¥½å‹ä»»åŠ¡ï¼Œå…·å¤‡ç»Ÿä¸€çš„è¯„ä»·ç­–ç•¥ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16307">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fc4667422696d425155d4a9186bad080.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ceaf571b9d53b9b3c72f276d1b707a0d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-71631bfaa04f1745b66709121fe4ae41.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-56728d346e34f4607a0c4bc64e16dfea.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Pixel-Reasoner-Incentivizing-Pixel-Space-Reasoning-with-Curiosity-Driven-Reinforcement-Learning"><a href="#Pixel-Reasoner-Incentivizing-Pixel-Space-Reasoning-with-Curiosity-Driven-Reinforcement-Learning" class="headerlink" title="Pixel Reasoner: Incentivizing Pixel-Space Reasoning with   Curiosity-Driven Reinforcement Learning"></a>Pixel Reasoner: Incentivizing Pixel-Space Reasoning with   Curiosity-Driven Reinforcement Learning</h2><p><strong>Authors:Alex Su, Haozhe Wang, Weimin Ren, Fangzhen Lin, Wenhu Chen</strong></p>
<p>Chain-of-thought reasoning has significantly improved the performance of Large Language Models (LLMs) across various domains. However, this reasoning process has been confined exclusively to textual space, limiting its effectiveness in visually intensive tasks. To address this limitation, we introduce the concept of reasoning in the pixel-space. Within this novel framework, Vision-Language Models (VLMs) are equipped with a suite of visual reasoning operations, such as zoom-in and select-frame. These operations enable VLMs to directly inspect, interrogate, and infer from visual evidences, thereby enhancing reasoning fidelity for visual tasks. Cultivating such pixel-space reasoning capabilities in VLMs presents notable challenges, including the modelâ€™s initially imbalanced competence and its reluctance to adopt the newly introduced pixel-space operations. We address these challenges through a two-phase training approach. The first phase employs instruction tuning on synthesized reasoning traces to familiarize the model with the novel visual operations. Following this, a reinforcement learning (RL) phase leverages a curiosity-driven reward scheme to balance exploration between pixel-space reasoning and textual reasoning. With these visual operations, VLMs can interact with complex visual inputs, such as information-rich images or videos to proactively gather necessary information. We demonstrate that this approach significantly improves VLM performance across diverse visual reasoning benchmarks. Our 7B model, \model, achieves 84% on V* bench, 74% on TallyQA-Complex, and 84% on InfographicsVQA, marking the highest accuracy achieved by any open-source model to date. These results highlight the importance of pixel-space reasoning and the effectiveness of our framework. </p>
<blockquote>
<p>æ€ç»´é“¾æ¨ç†å·²æ˜¾è‘—æé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ç§é¢†åŸŸçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™ç§æ¨ç†è¿‡ç¨‹ä¸€ç›´è¢«é™åˆ¶åœ¨æ–‡æœ¬ç©ºé—´å†…ï¼Œä½¿å…¶åœ¨è§†è§‰å¯†é›†å‹ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§å—åˆ°é™åˆ¶ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†åƒç´ ç©ºé—´æ¨ç†çš„æ¦‚å¿µã€‚åœ¨è¿™ä¸€æ–°é¢–æ¡†æ¶ä¸‹ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰é…å¤‡äº†ä¸€ç³»åˆ—è§†è§‰æ¨ç†æ“ä½œï¼Œä¾‹å¦‚æ”¾å¤§å’Œé€‰æ‹©å¸§ã€‚è¿™äº›æ“ä½œä½¿VLMèƒ½å¤Ÿç›´æ¥æ£€æŸ¥ã€è´¨è¯¢å’Œæ¨æ–­è§†è§‰è¯æ®ï¼Œä»è€Œæé«˜è§†è§‰ä»»åŠ¡çš„æ¨ç†ä¿çœŸåº¦ã€‚åœ¨VLMä¸­åŸ¹å…»è¿™ç§åƒç´ ç©ºé—´æ¨ç†èƒ½åŠ›é¢ä¸´æ˜¾è‘—æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ¨¡å‹çš„åˆå§‹èƒ½åŠ›ä¸å¹³è¡¡åŠå…¶å¯¹æ–°å¼•å…¥çš„åƒç´ ç©ºé—´æ“ä½œçš„æŠµè§¦ã€‚æˆ‘ä»¬é€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•æ¥åº”å¯¹è¿™äº›æŒ‘æˆ˜ã€‚ç¬¬ä¸€é˜¶æ®µé€šè¿‡åˆæˆæ¨ç†è½¨è¿¹è¿›è¡ŒæŒ‡ä»¤è°ƒæ•´ï¼Œä½¿æ¨¡å‹ç†Ÿæ‚‰æ–°å‹è§†è§‰æ“ä½œã€‚ä¹‹åï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é˜¶æ®µåˆ©ç”¨åŸºäºå¥½å¥‡å¿ƒçš„å¥–åŠ±æ–¹æ¡ˆæ¥å¹³è¡¡åƒç´ ç©ºé—´æ¨ç†å’Œæ–‡æœ¬æ¨ç†ä¹‹é—´çš„æ¢ç´¢ã€‚é€šè¿‡è¿™äº›è§†è§‰æ“ä½œï¼ŒVLMå¯ä»¥ä¸å¤æ‚çš„è§†è§‰è¾“å…¥ï¼ˆå¦‚ä¿¡æ¯ä¸°å¯Œçš„å›¾åƒæˆ–è§†é¢‘ï¼‰è¿›è¡Œäº¤äº’ï¼Œä»¥ä¸»åŠ¨æ”¶é›†å¿…è¦ä¿¡æ¯ã€‚æˆ‘ä»¬è¯æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†VLMåœ¨å¤šç§è§†è§‰æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„7Bæ¨¡å‹è¾¾åˆ°V*æµ‹è¯•å°84%ã€TallyQA-Complexæµ‹è¯•å°74%ã€å›¾è¡¨å¯è§†é—®ç­”ï¼ˆInfographicsVQAï¼‰æµ‹è¯•å°85%ï¼Œè¿™æ˜¯è¿„ä»Šä¸ºæ­¢ä»»ä½•å¼€æºæ¨¡å‹æ‰€å–å¾—çš„æœ€é«˜çš„å‡†ç¡®åº¦ã€‚è¿™äº›ç»“æœçªæ˜¾äº†åƒç´ ç©ºé—´æ¨ç†çš„é‡è¦æ€§ä»¥åŠæˆ‘ä»¬æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15966v1">PDF</a> Haozhe Wang and Alex Su contributed equally and listed alphabetically</p>
<p><strong>æ‘˜è¦</strong></p>
<p>é“¾å¼æ€ç»´æ¨ç†å·²æ˜¾è‘—æå‡äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„é¢†åŸŸçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™ç§æ¨ç†è¿‡ç¨‹ä»…é™äºæ–‡æœ¬ç©ºé—´ï¼Œä½¿å…¶åœ¨è§†è§‰å¯†é›†å‹ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§å—é™ã€‚ä¸ºè§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥åƒç´ ç©ºé—´æ¨ç†çš„æ¦‚å¿µã€‚åœ¨æ­¤æ–°æ¡†æ¶ä¸‹ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰é…å¤‡äº†ä¸€ç³»åˆ—è§†è§‰æ¨ç†æ“ä½œï¼Œå¦‚æ”¾å¤§å’Œé€‰æ‹©å¸§ã€‚è¿™äº›æ“ä½œä½¿VLMèƒ½å¤Ÿç›´æ¥æ£€æŸ¥ã€è¯¢é—®å’Œæ¨æ–­è§†è§‰è¯æ®ï¼Œä»è€Œæé«˜è§†è§‰ä»»åŠ¡çš„æ¨ç†ä¿çœŸåº¦ã€‚åŸ¹å…»VLMä¸­çš„åƒç´ ç©ºé—´æ¨ç†èƒ½åŠ›å¸¦æ¥äº†æ˜¾è‘—æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ¨¡å‹çš„åˆå§‹èƒ½åŠ›ä¸å‡è¡¡å’Œå¯¹æ–°å¼•å…¥çš„åƒç´ ç©ºé—´æ“ä½œçš„æ¥å—åº¦ä½ã€‚æˆ‘ä»¬é€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•æ¥åº”å¯¹è¿™äº›æŒ‘æˆ˜ã€‚ç¬¬ä¸€é˜¶æ®µé€šè¿‡åˆæˆæ¨ç†è½¨è¿¹è¿›è¡ŒæŒ‡ä»¤è°ƒæ•´ï¼Œä½¿æ¨¡å‹ç†Ÿæ‚‰æ–°çš„è§†è§‰æ“ä½œã€‚æ¥ä¸‹æ¥ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é˜¶æ®µåˆ©ç”¨åŸºäºå¥½å¥‡å¿ƒçš„å¥–åŠ±æ–¹æ¡ˆæ¥å¹³è¡¡åƒç´ ç©ºé—´æ¨ç†å’Œæ–‡æœ¬æ¨ç†ä¹‹é—´çš„æ¢ç´¢ã€‚è¿™äº›è§†è§‰æ“ä½œä½¿VLMèƒ½å¤Ÿä¸å¤æ‚è§†è§‰è¾“å…¥ï¼ˆå¦‚ä¿¡æ¯ä¸°å¯Œçš„å›¾åƒæˆ–è§†é¢‘ï¼‰è¿›è¡Œäº¤äº’ï¼Œä¸»åŠ¨æ”¶é›†å¿…è¦ä¿¡æ¯ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—æé«˜äº†VLMåœ¨å¤šç§è§†è§‰æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„7Bæ¨¡å‹åœ¨V*åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°84%ã€TallyQA-Complexä¸Šè¾¾åˆ°74%ã€InfographicsVQAä¸Šè¾¾åˆ°84%ï¼Œæˆä¸ºè¿„ä»Šä¸ºæ­¢ä»»ä½•å¼€æºæ¨¡å‹ä¸­æœ€é«˜ç²¾åº¦çš„æ¨¡å‹ã€‚è¿™äº›ç»“æœçªæ˜¾äº†åƒç´ ç©ºé—´æ¨ç†çš„é‡è¦æ€§åŠæˆ‘ä»¬æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>é“¾å¼æ€ç»´æ¨ç†å·²æå‡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šä¸ªé¢†åŸŸçš„æ€§èƒ½ã€‚</li>
<li>åƒç´ ç©ºé—´æ¨ç†çš„å¼•å…¥è§£å†³äº†è¯­è¨€æ¨¡å‹åœ¨è§†è§‰ä»»åŠ¡ä¸­çš„å±€é™æ€§ã€‚</li>
<li>è§†è§‰è¯­è¨€æ¨¡å‹é…å¤‡äº†å¦‚æ”¾å¤§å’Œé€‰æ‹©å¸§ç­‰è§†è§‰æ¨ç†æ“ä½œï¼Œå¯ç›´æ¥å¤„ç†è§†è§‰è¯æ®ã€‚</li>
<li>å®æ–½åƒç´ ç©ºé—´æ¨ç†èƒ½åŠ›é¢ä¸´æ¨¡å‹èƒ½åŠ›ä¸å‡è¡¡å’Œæ¥å—æ–°æ“ä½œçš„æŒ‘æˆ˜ã€‚</li>
<li>é€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒæ³•åº”å¯¹ä¸Šè¿°æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æŒ‡ä»¤è°ƒæ•´å’Œå¼ºåŒ–å­¦ä¹ é˜¶æ®µã€‚</li>
<li>è§†è§‰æ“ä½œä½¿æ¨¡å‹èƒ½å¤„ç†å¤æ‚è§†è§‰è¾“å…¥ï¼Œå¦‚ä¿¡æ¯ä¸°å¯Œçš„å›¾åƒæˆ–è§†é¢‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15966">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8b8d8d06241b54139e6626fd289e3358.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21b36704bed0c7db46bd8427702f400f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d57b310f88595197e546e7e4c5563795.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-007f33c81c0e4837970c1723b22f4966.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Meta-Learning-an-In-Context-Transformer-Model-of-Human-Higher-Visual-Cortex"><a href="#Meta-Learning-an-In-Context-Transformer-Model-of-Human-Higher-Visual-Cortex" class="headerlink" title="Meta-Learning an In-Context Transformer Model of Human Higher Visual   Cortex"></a>Meta-Learning an In-Context Transformer Model of Human Higher Visual   Cortex</h2><p><strong>Authors:Muquan Yu, Mu Nan, Hossein Adeli, Jacob S. Prince, John A. Pyles, Leila Wehbe, Margaret M. Henderson, Michael J. Tarr, Andrew F. Luo</strong></p>
<p>Understanding functional representations within higher visual cortex is a fundamental question in computational neuroscience. While artificial neural networks pretrained on large-scale datasets exhibit striking representational alignment with human neural responses, learning image-computable models of visual cortex relies on individual-level, large-scale fMRI datasets. The necessity for expensive, time-intensive, and often impractical data acquisition limits the generalizability of encoders to new subjects and stimuli. BraInCoRL uses in-context learning to predict voxelwise neural responses from few-shot examples without any additional finetuning for novel subjects and stimuli. We leverage a transformer architecture that can flexibly condition on a variable number of in-context image stimuli, learning an inductive bias over multiple subjects. During training, we explicitly optimize the model for in-context learning. By jointly conditioning on image features and voxel activations, our model learns to directly generate better performing voxelwise models of higher visual cortex. We demonstrate that BraInCoRL consistently outperforms existing voxelwise encoder designs in a low-data regime when evaluated on entirely novel images, while also exhibiting strong test-time scaling behavior. The model also generalizes to an entirely new visual fMRI dataset, which uses different subjects and fMRI data acquisition parameters. Further, BraInCoRL facilitates better interpretability of neural signals in higher visual cortex by attending to semantically relevant stimuli. Finally, we show that our framework enables interpretable mappings from natural language queries to voxel selectivity. </p>
<blockquote>
<p>ç†è§£é«˜çº§è§†è§‰çš®å±‚ä¸­çš„åŠŸèƒ½æ€§è¡¨å¾æ˜¯è®¡ç®—ç¥ç»ç§‘å­¦ä¸­çš„ä¸€ä¸ªåŸºæœ¬é—®é¢˜ã€‚è™½ç„¶ç»è¿‡å¤§è§„æ¨¡æ•°æ®é›†é¢„è®­ç»ƒçš„äººå·¥ç¥ç»ç½‘ç»œä¸äººç±»ç¥ç»ååº”è¡¨ç°å‡ºæƒŠäººçš„è¡¨å¾å¯¹é½ï¼Œä½†å­¦ä¹ è§†è§‰çš®å±‚çš„å›¾åƒè®¡ç®—æ¨¡å‹ä¾èµ–äºä¸ªä½“å±‚é¢çš„å¤§è§„æ¨¡åŠŸèƒ½æ€§ç£å…±æŒ¯æˆåƒï¼ˆfMRIï¼‰æ•°æ®é›†ã€‚æ˜‚è´µã€è€—æ—¶ä¸”é€šå¸¸ä¸åˆ‡å®é™…çš„æ•°æ®é‡‡é›†éœ€æ±‚é™åˆ¶äº†ç¼–ç å™¨å¯¹æ–°å¯¹è±¡å’Œåˆºæ¿€ç‰©çš„é€šç”¨æ€§ã€‚BraInCoRLä½¿ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ æ¥é¢„æµ‹åŸºäºå°‘é‡æ ·æœ¬çš„é€åƒç´ ç¥ç»ååº”ï¼Œæ— éœ€å¯¹æ–°å¯¹è±¡å’Œåˆºæ¿€è¿›è¡Œä»»ä½•é¢å¤–çš„å¾®è°ƒã€‚æˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§å˜å‹å™¨æ¶æ„ï¼Œå¯ä»¥çµæ´»åœ°é€‚åº”ä¸åŒæ•°é‡çš„ä¸Šä¸‹æ–‡å›¾åƒåˆºæ¿€ï¼Œå¹¶åœ¨å¤šä¸ªä¸»ä½“ä¸Šå­¦ä¹ å½’çº³åè§ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬æ˜ç¡®åœ°å¯¹æ¨¡å‹è¿›è¡Œä¸Šä¸‹æ–‡å­¦ä¹ ä¼˜åŒ–ã€‚é€šè¿‡è”åˆå›¾åƒç‰¹å¾å’Œä½“ç´ æ¿€æ´»ä½œä¸ºæ¡ä»¶ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å­¦ä¼šäº†ç›´æ¥ç”Ÿæˆé«˜çº§è§†è§‰çš®å±‚çš„æ€§èƒ½æ›´ä½³çš„é€åƒç´ æ¨¡å‹ã€‚æˆ‘ä»¬è¯æ˜ï¼Œåœ¨å…¨æ–°å›¾åƒè¯„ä¼°æ—¶ï¼ŒBraInCoRLåœ¨ä½æ•°æ®çŠ¶æ€ä¸‹å§‹ç»ˆä¼˜äºç°æœ‰çš„é€åƒç´ ç¼–ç å™¨è®¾è®¡ï¼ŒåŒæ—¶åœ¨æµ‹è¯•æ—¶è¡¨ç°å‡ºå¼ºå¤§çš„å¯æ‰©å±•æ€§ã€‚è¯¥æ¨¡å‹ä¹Ÿé€‚ç”¨äºå…¨æ–°çš„è§†è§‰åŠŸèƒ½æ€§ç£å…±æŒ¯æˆåƒæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†ä½¿ç”¨ä¸åŒçš„ä¸»ä½“å’ŒåŠŸèƒ½æ€§ç£å…±æŒ¯æˆåƒæ•°æ®é‡‡å‚æ•°é›†ã€‚æ­¤å¤–ï¼ŒBraInCoRLé€šè¿‡å…³æ³¨è¯­ä¹‰ç›¸å…³çš„åˆºæ¿€ï¼Œæé«˜äº†é«˜çº§è§†è§‰çš®å±‚ç¥ç»ä¿¡å·çš„è§£è¯»æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬å±•ç¤ºäº†æˆ‘ä»¬çš„æ¡†æ¶èƒ½å¤Ÿå®ç°ä»è‡ªç„¶è¯­è¨€æŸ¥è¯¢åˆ°ä½“ç´ é€‰æ‹©æ€§çš„å¯è§£é‡Šæ˜ å°„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15813v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†è§£è§†è§‰çš®å±‚ä¸­åŠŸèƒ½æ€§è¡¨å¾çš„é—®é¢˜ï¼ŒæŒ‡å‡ºäººå·¥ç¥ç»ç½‘ç»œåœ¨å¤§å‹æ•°æ®é›†ä¸Šçš„é¢„è®­ç»ƒä¸äººç±»ç¥ç»ååº”å­˜åœ¨æ˜¾è‘—çš„å¯¹é½ã€‚ç„¶è€Œï¼Œå­¦ä¹ è§†è§‰çš®å±‚çš„å›¾åƒè®¡ç®—æ¨¡å‹ä¾èµ–äºä¸ªä½“å±‚é¢çš„å¤§è§„æ¨¡fMRIæ•°æ®é›†ï¼Œæ•°æ®è·å–æˆæœ¬é«˜æ˜‚ã€è€—æ—¶é•¿ä¸”éš¾ä»¥å®ç°ï¼Œé™åˆ¶äº†ç¼–ç å™¨å¯¹æ–°ä¸»ä½“å’Œåˆºæ¿€çš„æ¨å¹¿èƒ½åŠ›ã€‚BraInCoRLä½¿ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œä»å°‘é‡æ ·æœ¬é¢„æµ‹ç¥ç»å…ƒå“åº”ï¼Œæ— éœ€å¯¹æ–°ä¸»ä½“å’Œåˆºæ¿€è¿›è¡Œä»»ä½•é¢å¤–å¾®è°ƒã€‚åˆ©ç”¨å˜å‹å™¨æ¶æ„ï¼Œå¯çµæ´»é€‚åº”ä¸åŒæ•°é‡çš„ä¸Šä¸‹æ–‡å›¾åƒåˆºæ¿€ï¼Œå­¦ä¹ è·¨å¤šä¸ªä¸»ä½“çš„å½’çº³åè§ã€‚é€šè¿‡è”åˆå›¾åƒç‰¹å¾å’Œä½“ç´ æ¿€æ´»ï¼Œæ¨¡å‹ç›´æ¥ç”Ÿæˆæ›´é«˜è§†è§‰çš®å±‚çš„ä½“ç´ æ¨¡å‹ï¼Œè¡¨ç°æ›´ä½³ã€‚åœ¨å…¨æ–°å›¾åƒçš„ä½æ•°æ®çŠ¶æ€ä¸‹ï¼ŒBraInCoRLè¡¨ç°ä¼˜äºç°æœ‰ä½“ç´ ç¼–ç å™¨è®¾è®¡ï¼›æµ‹è¯•æ—¶ï¼Œå…¶æ‰©å±•è¡Œä¸ºè¡¨ç°å¼ºåŠ²ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹å¯æ¨å¹¿åˆ°ä½¿ç”¨ä¸åŒä¸»ä½“å’ŒfMRIæ•°æ®è·å–å‚æ•°çš„æ–°è§†è§‰fMRIæ•°æ®é›†ä¸Šï¼Œå¹¶èƒ½æ›´å¥½åœ°è§£é‡Šé«˜çº§è§†è§‰çš®å±‚çš„ç¥ç»ä¿¡å·ã€‚æœ€åï¼Œç ”ç©¶å±•ç¤ºäº†è¯¥æ¡†æ¶å¯å®ç°ä»è‡ªç„¶è¯­è¨€æŸ¥è¯¢åˆ°ä½“ç´ é€‰æ‹©æ€§çš„å¯è§£é‡Šæ˜ å°„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç†è§£è§†è§‰çš®å±‚ä¸­çš„åŠŸèƒ½æ€§è¡¨å¾æ˜¯è®¡ç®—ç¥ç»ç§‘å­¦çš„åŸºæœ¬é—®é¢˜ã€‚</li>
<li>äººå·¥ç¥ç»ç½‘ç»œé¢„è®­ç»ƒä¸å¤§å‹æ•°æ®é›†å±•ç°ä¸äººç±»ç¥ç»ååº”çš„æ˜¾è‘—å¯¹é½ã€‚</li>
<li>å­¦ä¹ å›¾åƒè®¡ç®—æ¨¡å‹ä¾èµ–å¤§è§„æ¨¡fMRIæ•°æ®é›†ï¼Œä½†æ•°æ®è·å–æˆæœ¬é«˜ã€æ—¶é—´é•¿ä¸”ä¸åˆ‡å®é™…ã€‚</li>
<li>BraInCoRLä½¿ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ é¢„æµ‹ç¥ç»å…ƒå“åº”ï¼Œæ— éœ€å¯¹æ–°æ•°æ®é¢å¤–å¾®è°ƒã€‚</li>
<li>BraInCoRLåˆ©ç”¨å˜å‹å™¨æ¶æ„çµæ´»é€‚åº”å¤šç§ä¸Šä¸‹æ–‡å›¾åƒåˆºæ¿€ï¼Œå¹¶å­¦ä¹ è·¨ä¸»ä½“å½’çº³åè§ã€‚</li>
<li>æ¨¡å‹è”åˆå›¾åƒç‰¹å¾å’Œä½“ç´ æ¿€æ´»ï¼Œç”Ÿæˆè¡¨ç°æ›´ä½³çš„é«˜çº§è§†è§‰çš®å±‚ä½“ç´ æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15813">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ec85e517c1b2214b51a18d1f5c4aebbd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d43a9a3308789e8c9ed8606c7dfdf214.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4a4181be70cd10df12b22966a968ac31.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bf3cc5b298464d8e9109448a1b009881.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca0bd825b858430e6c086f4dda6628e6.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="STAR-R1-Spacial-TrAnsformation-Reasoning-by-Reinforcing-Multimodal-LLMs"><a href="#STAR-R1-Spacial-TrAnsformation-Reasoning-by-Reinforcing-Multimodal-LLMs" class="headerlink" title="STAR-R1: Spacial TrAnsformation Reasoning by Reinforcing Multimodal LLMs"></a>STAR-R1: Spacial TrAnsformation Reasoning by Reinforcing Multimodal LLMs</h2><p><strong>Authors:Zongzhao Li, Zongyang Ma, Mingze Li, Songyou Li, Yu Rong, Tingyang Xu, Ziqi Zhang, Deli Zhao, Wenbing Huang</strong></p>
<p>Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities across diverse tasks, yet they lag significantly behind humans in spatial reasoning. We investigate this gap through Transformation-Driven Visual Reasoning (TVR), a challenging task requiring identification of object transformations across images under varying viewpoints. While traditional Supervised Fine-Tuning (SFT) fails to generate coherent reasoning paths in cross-view settings, sparse-reward Reinforcement Learning (RL) suffers from inefficient exploration and slow convergence. To address these limitations, we propose STAR-R1, a novel framework that integrates a single-stage RL paradigm with a fine-grained reward mechanism tailored for TVR. Specifically, STAR-R1 rewards partial correctness while penalizing excessive enumeration and passive inaction, enabling efficient exploration and precise reasoning. Comprehensive evaluations demonstrate that STAR-R1 achieves state-of-the-art performance across all 11 metrics, outperforming SFT by 23% in cross-view scenarios. Further analysis reveals STAR-R1â€™s anthropomorphic behavior and highlights its unique ability to compare all objects for improving spatial reasoning. Our work provides critical insights in advancing the research of MLLMs and reasoning models. The codes, model weights, and data will be publicly available at <a target="_blank" rel="noopener" href="https://github.com/zongzhao23/STAR-R1">https://github.com/zongzhao23/STAR-R1</a>. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å„é¡¹ä»»åŠ¡ä¸­å±•ç°äº†å“è¶Šçš„èƒ½åŠ›ï¼Œä½†åœ¨ç©ºé—´æ¨ç†æ–¹é¢ä»æ˜¾è‘—è½åäºäººç±»ã€‚æˆ‘ä»¬é€šè¿‡è½¬æ¢é©±åŠ¨è§†è§‰æ¨ç†ï¼ˆTVRï¼‰æ¥ç ”ç©¶è¿™ä¸€å·®è·ï¼Œè¿™æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œè¦æ±‚åœ¨ä¸åŒè§†è§’çš„å›¾åƒä¸­è¯†åˆ«å¯¹è±¡çš„è½¬æ¢ã€‚ä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ— æ³•åœ¨è·¨è§†å›¾è®¾ç½®ä¸­ç”Ÿæˆè¿è´¯çš„æ¨ç†è·¯å¾„ï¼Œè€Œç¨€ç–å¥–åŠ±å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åˆ™é¢ä¸´æ•ˆç‡ä½ä¸‹å’Œæ¢ç´¢ç¼“æ…¢çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†STAR-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªå°†å•é˜¶æ®µRLèŒƒå¼ä¸é’ˆå¯¹TVRçš„ç²¾ç»†å¥–åŠ±æœºåˆ¶ç›¸ç»“åˆçš„æ–°å‹æ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼ŒSTAR-R1å¥–åŠ±éƒ¨åˆ†æ­£ç¡®æ€§ï¼ŒåŒæ—¶æƒ©ç½šè¿‡åº¦æšä¸¾å’Œè¢«åŠ¨æ— è¡ŒåŠ¨ï¼Œä»è€Œå®ç°æœ‰æ•ˆçš„æ¢ç´¢å’Œç²¾ç¡®æ¨ç†ã€‚ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼ŒSTAR-R1åœ¨æ‰€æœ‰1:metricä¸Šè¾¾åˆ°äº†æœ€æ–°æ€§èƒ½æ°´å¹³ï¼Œåœ¨è·¨è§†å›¾åœºæ™¯ä¸­æ¯”SFTé«˜å‡º23%ã€‚è¿›ä¸€æ­¥çš„åˆ†ææ­ç¤ºäº†STAR-R1çš„äººç±»è¡Œä¸ºç‰¹å¾ï¼Œå¹¶çªå‡ºäº†å…¶åœ¨æ¯”è¾ƒæ‰€æœ‰å¯¹è±¡æ–¹é¢æé«˜ç©ºé—´æ¨ç†çš„ç‹¬ç‰¹èƒ½åŠ›ã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ºå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹å’Œæ¨ç†æ¨¡å‹çš„ç ”ç©¶æä¾›äº†å…³é”®è§è§£ã€‚ä»£ç ã€æ¨¡å‹æƒé‡å’Œæ•°æ®å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/zongzhao23/STAR-R1%E4%B8%8A%E5%85%AC%E5%BC%80%E5%8F%AF%E7%94%A8%E3%80%82">https://github.com/zongzhao23/STAR-R1ä¸Šå…¬å¼€å¯ç”¨ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15804v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>MLLMåœ¨ç©ºé—´æ¨ç†æ–¹é¢å­˜åœ¨ä¸äººç±»æ˜¾è‘—å·®è·ï¼Œæœ¬ç ”ç©¶é€šè¿‡Transformation-Drivenè§†è§‰æ¨ç†ï¼ˆTVRï¼‰ä»»åŠ¡æ¢ç©¶æ­¤å·®è·ã€‚æå‡ºSTAR-R1æ¡†æ¶ï¼Œç»“åˆå•é˜¶æ®µå¼ºåŒ–å­¦ä¹ ä¸ç»†ç²’åº¦å¥–åŠ±æœºåˆ¶ï¼Œè§£å†³ä¼ ç»Ÿç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„å±€é™æ€§ã€‚STAR-R1æ¡†æ¶å®ç°é«˜æ•ˆæ¢ç´¢ä¸ç²¾ç¡®æ¨ç†ï¼Œåœ¨è·¨è§†è§’åœºæ™¯ä¸­å®ç°ä¸šç•Œæœ€ä½³æ€§èƒ½ã€‚ç ”ç©¶æä¾›å¯¹MLLMå’Œæ¨ç†æ¨¡å‹å‘å±•çš„æ·±åˆ»è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLMåœ¨å¤šä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨ç©ºé—´æ¨ç†æ–¹é¢å­˜åœ¨ä¸äººç±»æ˜¾è‘—å·®è·ã€‚</li>
<li>æå‡ºTransformation-Drivenè§†è§‰æ¨ç†ï¼ˆTVRï¼‰ä»»åŠ¡æ¥æ¢ç©¶è¿™ä¸€å·®è·ã€‚</li>
<li>ä¼ ç»Ÿç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰åœ¨è·¨è§†è§’è®¾ç½®ä¸‹æ— æ³•ç”Ÿæˆè¿è´¯çš„æ¨ç†è·¯å¾„ã€‚</li>
<li>ç¨€ç–å¥–åŠ±å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å­˜åœ¨æ¢ç´¢æ•ˆç‡ä½ä¸‹å’Œæ”¶æ•›é€Ÿåº¦æ…¢çš„é—®é¢˜ã€‚</li>
<li>æå‡ºSTAR-R1æ¡†æ¶ï¼Œç»“åˆå•é˜¶æ®µRLèŒƒå¼å’Œé’ˆå¯¹TVRçš„ç²¾ç»†å¥–åŠ±æœºåˆ¶ã€‚</li>
<li>STAR-R1æ¡†æ¶é€šè¿‡å¥–åŠ±éƒ¨åˆ†æ­£ç¡®æ€§å’Œæƒ©ç½šè¿‡åº¦æšä¸¾ä¸è¢«åŠ¨è¡Œä¸ºï¼Œå®ç°äº†é«˜æ•ˆæ¢ç´¢å’Œç²¾ç¡®æ¨ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15804">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8deeb17b04d5f6763e2015bbd68feba8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3175477c62e1924b40260c938fd5bb51.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-64d507ff00417d60ec92ecb7b99014a1.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-24/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-24/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-24/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-057aeeb66cc0433f578069a2a100f587.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-24  From EduVisBench to EduVisAgent A Benchmark and Multi-Agent Framework   for Pedagogical Visualization
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-24
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-24/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-a182ffdb78954744b9948c3d72830132.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-24  GoT-R1 Unleashing Reasoning Capability of MLLM for Visual Generation   with Reinforcement Learning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-24
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">23542.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
