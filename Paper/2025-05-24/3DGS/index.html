<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="3DGS">
    <meta name="description" content="3DGS 方向最新论文已更新，请持续关注 Update in 2025-05-24  SHaDe Compact and Consistent Dynamic 3D Reconstruction via Tri-Plane   Deformation and Latent Diffusion">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>3DGS | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-5d2078e5afc415869e09090a1b2beb05.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">3DGS</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/3DGS/">
                                <span class="chip bg-color">3DGS</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                3DGS
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-05-24
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    15k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    60 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-05-24-更新"><a href="#2025-05-24-更新" class="headerlink" title="2025-05-24 更新"></a>2025-05-24 更新</h1><h2 id="SHaDe-Compact-and-Consistent-Dynamic-3D-Reconstruction-via-Tri-Plane-Deformation-and-Latent-Diffusion"><a href="#SHaDe-Compact-and-Consistent-Dynamic-3D-Reconstruction-via-Tri-Plane-Deformation-and-Latent-Diffusion" class="headerlink" title="SHaDe: Compact and Consistent Dynamic 3D Reconstruction via Tri-Plane   Deformation and Latent Diffusion"></a>SHaDe: Compact and Consistent Dynamic 3D Reconstruction via Tri-Plane   Deformation and Latent Diffusion</h2><p><strong>Authors:Asrar Alruwayqi</strong></p>
<p>We present a novel framework for dynamic 3D scene reconstruction that integrates three key components: an explicit tri-plane deformation field, a view-conditioned canonical radiance field with spherical harmonics (SH) attention, and a temporally-aware latent diffusion prior. Our method encodes 4D scenes using three orthogonal 2D feature planes that evolve over time, enabling efficient and compact spatiotemporal representation. These features are explicitly warped into a canonical space via a deformation offset field, eliminating the need for MLP-based motion modeling.   In canonical space, we replace traditional MLP decoders with a structured SH-based rendering head that synthesizes view-dependent color via attention over learned frequency bands improving both interpretability and rendering efficiency. To further enhance fidelity and temporal consistency, we introduce a transformer-guided latent diffusion module that refines the tri-plane and deformation features in a compressed latent space. This generative module denoises scene representations under ambiguous or out-of-distribution (OOD) motion, improving generalization.   Our model is trained in two stages: the diffusion module is first pre-trained independently, and then fine-tuned jointly with the full pipeline using a combination of image reconstruction, diffusion denoising, and temporal consistency losses. We demonstrate state-of-the-art results on synthetic benchmarks, surpassing recent methods such as HexPlane and 4D Gaussian Splatting in visual quality, temporal coherence, and robustness to sparse-view dynamic inputs. </p>
<blockquote>
<p>我们提出了一种新的动态3D场景重建框架，它集成了三个关键组件：显式三平面变形场、带有球面谐波（SH）注意力的视场条件规范辐射场以及时间感知潜在扩散先验。我们的方法使用随时间变化的三个正交2D特征平面对4D场景进行编码，从而实现高效且紧凑的时空表示。这些特征通过变形偏移场明确变形到规范空间，无需基于MLP的运动建模。在规范空间中，我们用结构化的SH基于渲染头替换传统的MLP解码器，通过关注学习频率波段来合成视觉相关的颜色，从而提高可解释性和渲染效率。为了进一步提高保真度和时间一致性，我们引入了一个受变压器引导的潜在扩散模块，该模块在压缩的潜在空间中细化了三平面和变形特征。这个生成模块减少了模糊或超出分布（OOD）运动下的场景表示噪声，提高了泛化能力。我们的模型分为两个阶段进行训练：首先独立地预训练扩散模块，然后使用图像重建、扩散去噪和时间一致性损失的组合与全管道联合微调。我们在合成基准测试上展示了卓越的结果，在视觉质量、时间连贯性和对稀疏视图动态输入的鲁棒性方面超越了最近的HexPlane和4D高斯拼贴等方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16535v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种新的动态三维场景重建框架，包含三个关键组件：显式三平面变形场、基于视图的典型辐射场与球面谐波（SH）注意力机制，以及时间感知潜在扩散先验。该方法采用随时间变化的三正交二维特征平面编码四维场景，通过变形偏移场将这些特征显式地变换到典型空间，无需使用MLP基运动建模。典型空间中，使用结构化SH渲染头合成视相关色彩，通过关注学习频率带提高解释性和渲染效率。为进一步提高保真度和时间一致性，引入了基于变压器的潜在扩散模块，在压缩潜在空间中优化三平面和变形特征。该生成模块在模糊或分布外（OOD）运动情况下降低场景表示的噪声，提高泛化能力。该模型分两个阶段训练：首先独立预训练扩散模块，然后使用图像重建、扩散去噪和时间一致性损失联合微调整个管道。在合成基准测试上表现卓越，超越HexPlane和4D高斯贴片等方法在视觉质量、时间连贯性和对稀疏动态输入鲁棒性方面的表现。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了一种新型动态三维场景重建框架，集成了显式三平面变形场、基于视图的典型辐射场与SH注意力机制以及时间感知潜在扩散先验。</li>
<li>通过变形偏移场将特征显式变换到典型空间，提高了效率和准确性。</li>
<li>采用结构化SH渲染头合成视相关色彩，提升了渲染效率和解释性。</li>
<li>引入基于变压器的潜在扩散模块，优化了场景表示，提高了泛化能力和时间一致性。</li>
<li>模型采用两阶段训练法，先独立预训练扩散模块，再联合微调整个管道。</li>
<li>在图像重建、扩散去噪和时间一致性方面使用损失函数进行优化。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16535">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5d2078e5afc415869e09090a1b2beb05.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fd5696dd58fff4cd2fbdf20329e853a3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-53ca7679e0829cfeabc3d94ab477210b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0408e7eafc56b86ef5e3e641eea9d11e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-810bd2dee7512748b03541103c339eff.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Motion-Matters-Compact-Gaussian-Streaming-for-Free-Viewpoint-Video-Reconstruction"><a href="#Motion-Matters-Compact-Gaussian-Streaming-for-Free-Viewpoint-Video-Reconstruction" class="headerlink" title="Motion Matters: Compact Gaussian Streaming for Free-Viewpoint Video   Reconstruction"></a>Motion Matters: Compact Gaussian Streaming for Free-Viewpoint Video   Reconstruction</h2><p><strong>Authors:Jiacong Chen, Qingyu Mao, Youneng Bao, Xiandong Meng, Fanyang Meng, Ronggang Wang, Yongsheng Liang</strong></p>
<p>3D Gaussian Splatting (3DGS) has emerged as a high-fidelity and efficient paradigm for online free-viewpoint video (FVV) reconstruction, offering viewers rapid responsiveness and immersive experiences. However, existing online methods face challenge in prohibitive storage requirements primarily due to point-wise modeling that fails to exploit the motion properties. To address this limitation, we propose a novel Compact Gaussian Streaming (ComGS) framework, leveraging the locality and consistency of motion in dynamic scene, that models object-consistent Gaussian point motion through keypoint-driven motion representation. By transmitting only the keypoint attributes, this framework provides a more storage-efficient solution. Specifically, we first identify a sparse set of motion-sensitive keypoints localized within motion regions using a viewspace gradient difference strategy. Equipped with these keypoints, we propose an adaptive motion-driven mechanism that predicts a spatial influence field for propagating keypoint motion to neighboring Gaussian points with similar motion. Moreover, ComGS adopts an error-aware correction strategy for key frame reconstruction that selectively refines erroneous regions and mitigates error accumulation without unnecessary overhead. Overall, ComGS achieves a remarkable storage reduction of over 159 X compared to 3DGStream and 14 X compared to the SOTA method QUEEN, while maintaining competitive visual fidelity and rendering speed. Our code will be released. </p>
<blockquote>
<p>3D高斯映射（3DGS）已经成为一种用于在线自由视点视频（FVV）重建的高保真和高效范式，为观众提供快速响应和沉浸式体验。然而，现有的在线方法面临存储要求过高的挑战，这主要是由于点对建模未能充分利用运动属性。为了解决这一局限性，我们提出了一种新型的紧凑高斯流（ComGS）框架，它利用动态场景中的运动局部性和一致性，通过关键帧驱动的运动表示对物体一致的高斯点运动进行建模。通过仅传输关键点的属性，该框架提供了更高效的存储解决方案。具体来说，我们首先使用视图空间梯度差异策略，在运动区域内识别一组稀疏的运动敏感关键点。有了这些关键点，我们提出了一种自适应的运动驱动机制，预测一个空间影响场，将关键点的运动传播到具有相似运动的相邻高斯点。此外，ComGS采用了一种错误感知校正策略，对关键帧重建进行选择性细化，缓解错误积累，同时避免不必要的开销。总体而言，ComGS与3DGStream相比实现了高达159倍的存储缩减，与当前最佳方法QUEEN相比实现了14倍的存储缩减，同时保持了有竞争力的视觉保真度和渲染速度。我们的代码将会发布。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16533v1">PDF</a> 17 pages, 9 figures</p>
<p><strong>Summary</strong></p>
<p>3DGS方法以其高保真度和高效率成为了在线自由视点视频重建领域的流行方法，为用户提供快速响应和沉浸式体验。但是现有在线方法存储要求很高，主要是因了点模型没有充分利用运动特性。为解决此问题，我们提出了Compact Gaussian Streaming（ComGS）框架，利用动态场景的运动局部性和一致性，通过关键点的运动表示来模拟对象一致的Gaussian点运动。它仅传输关键点的属性，从而更节省存储空间。通过运动敏感的关键点自适应预测空间影响场来将运动传播到相邻的Gaussian点。此外，ComGS采用了一种错误感知校正策略进行关键帧重建，选择性优化错误区域并减轻错误累积负担。相较于其他方法，ComGS实现了显著的存储降低，同时保持了竞争性的视觉保真度和渲染速度。</p>
<p><strong>Key Takeaways</strong></p>
<p>以下是关于文本的关键见解：</p>
<ul>
<li>现有在线方法的存储要求较高是因为未充分利用运动特性，尤其是在点对模型处理方面存在问题。对此我们提出了一种新颖的ComGS框架来应对这一挑战。该框架通过利用动态场景的运动局部性和一致性来实现高效存储。</li>
<li>ComGS框架利用关键点的运动表示来模拟对象一致的Gaussian点运动，仅传输关键点的属性，从而显著减少存储需求。这是通过识别运动敏感的关键点并预测空间影响场实现的。通过这种方式，关键点的运动能够传播到具有相似运动的相邻Gaussian点。这是一种结合关键点和运动预测的策略，使得存储效率大大提高。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16533">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-c3eb68f7037794b08679271c3b4ec12b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8702188b9dec2c0cc286562015c741ea.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="MAGIC-Motion-Aware-Generative-Inference-via-Confidence-Guided-LLM"><a href="#MAGIC-Motion-Aware-Generative-Inference-via-Confidence-Guided-LLM" class="headerlink" title="MAGIC: Motion-Aware Generative Inference via Confidence-Guided LLM"></a>MAGIC: Motion-Aware Generative Inference via Confidence-Guided LLM</h2><p><strong>Authors:Siwei Meng, Yawei Luo, Ping Liu</strong></p>
<p>Recent advances in static 3D generation have intensified the demand for physically consistent dynamic 3D content. However, existing video generation models, including diffusion-based methods, often prioritize visual realism while neglecting physical plausibility, resulting in implausible object dynamics. Prior approaches for physics-aware dynamic generation typically rely on large-scale annotated datasets or extensive model fine-tuning, which imposes significant computational and data collection burdens and limits scalability across scenarios. To address these challenges, we present MAGIC, a training-free framework for single-image physical property inference and dynamic generation, integrating pretrained image-to-video diffusion models with iterative LLM-based reasoning. Our framework generates motion-rich videos from a static image and closes the visual-to-physical gap through a confidence-driven LLM feedback loop that adaptively steers the diffusion model toward physics-relevant motion. To translate visual dynamics into controllable physical behavior, we further introduce a differentiable MPM simulator operating directly on 3D Gaussians reconstructed from the single image, enabling physically grounded, simulation-ready outputs without any supervision or model tuning. Experiments show that MAGIC outperforms existing physics-aware generative methods in inference accuracy and achieves greater temporal coherence than state-of-the-art video diffusion models. </p>
<blockquote>
<p>近年来静态三维生成技术的进展加剧了对物理一致性动态三维内容的需求。然而，现有的视频生成模型，包括基于扩散的方法，通常优先追求视觉真实性，而忽视物理合理性，导致物体动态不真实。以前的方法对于物理感知的动态生成通常依赖于大规模标注数据集或模型精细调整，这带来了显著的计算和数据收集负担，并限制了跨场景的扩展性。为了应对这些挑战，我们提出了MAGIC，这是一个无需训练的单图像物理属性推断和动态生成框架，它结合了预训练的图像到视频的扩散模型与基于迭代的大型语言模型（LLM）的推理。我们的框架从静态图像生成运动丰富的视频，并通过信心驱动的LLM反馈循环来缩小视觉到物理的差距，该循环自适应地引导扩散模型向物理相关的运动方向进行。为了将视觉动态转化为可控的物理行为，我们进一步引入了一个可微分的MPM模拟器，该模拟器直接在由单图像重建的3D高斯分布上运行，无需监督或模型调整即可实现物理基础、模拟就绪的输出。实验表明，MAGIC在推理准确性方面优于现有的物理感知生成方法，并且在时间连贯性方面优于最先进的视频扩散模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16456v1">PDF</a> </p>
<p><strong>Summary</strong><br>本文介绍了MAGIC框架，一个无需训练的单图像物理属性推断和动态生成框架。它结合了预训练的图像到视频的扩散模型和基于迭代的大型语言模型（LLM）推理，能够生成运动丰富的视频并弥合视觉到物理之间的差距。框架通过置信度驱动的LLM反馈循环自适应地引导扩散模型进行物理相关的运动。此外，引入了可微分的MPM模拟器，直接在从单张图像重建的3D高斯上操作，实现物理基础且模拟就绪的输出，无需监督或模型调整。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MAGIC框架是首个无需训练就能进行物理属性推断和动态生成的方法。</li>
<li>结合了图像到视频的扩散模型和LLM推理，生成运动丰富的视频。</li>
<li>通过置信度驱动的LLM反馈循环，使模型能自适应地模拟物理相关的运动。</li>
<li>引入了可微分的MPM模拟器，直接在3D高斯上进行操作，实现物理基础的模拟。</li>
<li>该方法无需额外的监督或模型调整，具有更好的通用性。</li>
<li>实验显示，MAGIC在推理准确性和时间连贯性方面超越了现有的物理感知生成方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16456">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-dca90b978eb12c54278e945f8392df79.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e05670f0400652cee38bd0d733e0648d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-20e9e24d2e0a0db7dcb33948ed816fa1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-488a64befa1216189a3cae36eb973c80.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-43eded7d51517ad257d1da2db9033f82.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="RUSplatting-Robust-3D-Gaussian-Splatting-for-Sparse-View-Underwater-Scene-Reconstruction"><a href="#RUSplatting-Robust-3D-Gaussian-Splatting-for-Sparse-View-Underwater-Scene-Reconstruction" class="headerlink" title="RUSplatting: Robust 3D Gaussian Splatting for Sparse-View Underwater   Scene Reconstruction"></a>RUSplatting: Robust 3D Gaussian Splatting for Sparse-View Underwater   Scene Reconstruction</h2><p><strong>Authors:Zhuodong Jiang, Haoran Wang, Guoxi Huang, Brett Seymour, Nantheera Anantrasirichai</strong></p>
<p>Reconstructing high-fidelity underwater scenes remains a challenging task due to light absorption, scattering, and limited visibility inherent in aquatic environments. This paper presents an enhanced Gaussian Splatting-based framework that improves both the visual quality and geometric accuracy of deep underwater rendering. We propose decoupled learning for RGB channels, guided by the physics of underwater attenuation, to enable more accurate colour restoration. To address sparse-view limitations and improve view consistency, we introduce a frame interpolation strategy with a novel adaptive weighting scheme. Additionally, we introduce a new loss function aimed at reducing noise while preserving edges, which is essential for deep-sea content. We also release a newly collected dataset, Submerged3D, captured specifically in deep-sea environments. Experimental results demonstrate that our framework consistently outperforms state-of-the-art methods with PSNR gains up to 1.90dB, delivering superior perceptual quality and robustness, and offering promising directions for marine robotics and underwater visual analytics. </p>
<blockquote>
<p>重建高保真水下场景仍然是一个具有挑战性的任务，由于水下环境中的光吸收、散射和有限的可见度。本文提出了一种基于增强高斯Splatting的框架，提高了深海渲染的视觉质量和几何精度。我们提出了基于水下衰减物理原理的RGB通道解耦学习，以实现更准确的颜色恢复。为了解决稀疏视角的限制并提高视角一致性，我们引入了一种带有新型自适应加权方案的新帧插值策略。此外，我们引入了一种新的损失函数，旨在减少噪声的同时保留边缘，这对于深海内容至关重要。我们还发布了一个新收集的特定在深海环境中捕获的数据集Submerged3D。实验结果表明，我们的框架始终优于最新技术的方法，PSNR增益高达1.90dB，具有出色的感知质量和稳健性，并为海洋机器人技术和水下视觉分析提供了有前景的方向。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15737v1">PDF</a> 10 pages, 3 figures. Submitted to BMVC 2025</p>
<p><strong>Summary</strong><br>本文提出了一种基于高斯Splatting的增强框架，用于提高水下场景的视觉质量和几何渲染精度。该研究通过解耦RGB通道学习、引入帧插值策略和自适应权重方案，以及设计新的损失函数，有效应对水下环境中的光吸收、散射和有限可见度等挑战，实现了更准确的颜色恢复、视图一致性和边缘保护。同时，研究团队还公开了一个专门在深海环境中采集的新数据集Submerged3D。研究成果在实验中表现优异，较现有方法平均峰值信噪比提高达1.90dB，为海洋机器人和水下视觉分析提供了有力支持。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>采用增强高斯Splatting框架，提高水下场景视觉质量和几何渲染精度。</li>
<li>解耦RGB通道学习，根据水下衰减物理特性指导更准确的颜色恢复。</li>
<li>引入帧插值策略及自适应权重方案，解决稀疏视图问题并改善视图一致性。</li>
<li>设计新损失函数，旨在降低噪声同时保留边缘，对深海内容至关重要。</li>
<li>公开新数据集Submerged3D，专注于深海环境采集。</li>
<li>实验结果表明，该框架较现有技术有显著提升，峰值信噪比提高达1.90dB。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15737">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-578f5643f1b2cdd35ae063cecf7dcd2c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-684dcc6928c25e245169e45e01e6abd6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6528c2d6dd13fa6985f9bc72715fe025.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="PlantDreamer-Achieving-Realistic-3D-Plant-Models-with-Diffusion-Guided-Gaussian-Splatting"><a href="#PlantDreamer-Achieving-Realistic-3D-Plant-Models-with-Diffusion-Guided-Gaussian-Splatting" class="headerlink" title="PlantDreamer: Achieving Realistic 3D Plant Models with Diffusion-Guided   Gaussian Splatting"></a>PlantDreamer: Achieving Realistic 3D Plant Models with Diffusion-Guided   Gaussian Splatting</h2><p><strong>Authors:Zane K J Hartley, Lewis A G Stuart, Andrew P French, Michael P Pound</strong></p>
<p>Recent years have seen substantial improvements in the ability to generate synthetic 3D objects using AI. However, generating complex 3D objects, such as plants, remains a considerable challenge. Current generative 3D models struggle with plant generation compared to general objects, limiting their usability in plant analysis tools, which require fine detail and accurate geometry. We introduce PlantDreamer, a novel approach to 3D synthetic plant generation, which can achieve greater levels of realism for complex plant geometry and textures than available text-to-3D models. To achieve this, our new generation pipeline leverages a depth ControlNet, fine-tuned Low-Rank Adaptation and an adaptable Gaussian culling algorithm, which directly improve textural realism and geometric integrity of generated 3D plant models. Additionally, PlantDreamer enables both purely synthetic plant generation, by leveraging L-System-generated meshes, and the enhancement of real-world plant point clouds by converting them into 3D Gaussian Splats. We evaluate our approach by comparing its outputs with state-of-the-art text-to-3D models, demonstrating that PlantDreamer outperforms existing methods in producing high-fidelity synthetic plants. Our results indicate that our approach not only advances synthetic plant generation, but also facilitates the upgrading of legacy point cloud datasets, making it a valuable tool for 3D phenotyping applications. </p>
<blockquote>
<p>近年来，利用人工智能生成合成3D物体的能力得到了显著提高。然而，生成复杂的3D物体，如植物，仍然是一项巨大的挑战。与通用物体相比，当前的3D生成模型在植物生成方面存在困难，这限制了它们在需要精细细节和精确几何形状的植物分析工具中的可用性。我们引入了PlantDreamer，这是一种新型的3D合成植物生成方法，与现有的文本到3D模型相比，它可以在复杂的植物几何和纹理方面实现更高水平的逼真度。为实现这一目标，我们的新一代流水线利用深度ControlNet，精细调整的低秩适应性和可适应的高斯剔除算法，直接提高了生成3D植物模型的真实感和几何完整性。此外，PlantDreamer能够通过利用L系统生成的网格进行纯粹的合成植物生成，并且能够通过将现实世界中的植物点云转换为3D高斯斑块来增强它们。我们通过将我们的方法与最先进文本到3D模型输出进行比较来评估我们的方法，证明PlantDreamer在生成高保真合成植物方面优于现有方法。我们的结果表明，我们的方法不仅推动了合成植物的生成，而且还促进了旧有点云数据集升级，使其成为3D表型应用的有价值的工具。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15528v1">PDF</a> 13 pages, 5 figures, 4 tables</p>
<p><strong>Summary</strong><br>     近年来AI生成合成3D物体的能力显著提高，但在生成复杂的植物方面仍存在挑战。为此，研究者推出PlantDreamer，一种新型的3D合成植物生成方法，可实现更高级的逼真度。此方法利用深度控制网络、精细的低阶适应性和灵活的高斯剔除算法等技术提升纹理真实感和几何完整性。此外，它不仅能够生成纯合成的植物，还可以提高现实世界植物点云的转换为高质量的植物模型的能力。实验表明，与目前主流的文本到3D模型相比，PlantDreamer在生成高质量合成植物方面表现更优。它不仅推动了合成植物的生成技术，还有助于提升现有的点云数据集，对植物表型分析应用具有重要意义。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AI在生成复杂植物方面仍存在挑战。目前主流的生成模型在这方面难以与一般的物体相提并论，导致它们在精细细节和精确几何上的应用受到限制。这影响植物分析工具的实用性和可靠性。尽管如此，近期的技术发展展现了人工智能在此方面的进步空间和发展潜力。</li>
<li>PlantDreamer作为一种新型的合成植物生成方法引入了一系列技术改进以提升植物生成的逼真度。它利用深度控制网络、低阶适应性调整和高斯剔除算法等技术来优化生成的植物模型的纹理和几何完整性。这一方法在优化模拟植物上展示了优势，并能够同时增强现实植物的点云数据质量。这为植物科学研究和可视化提供了有力的工具。</li>
<li>PlantDreamer不仅能生成纯合成的植物模型，还能将现实世界中的植物点云转换为高质量的模型，这对于升级现有的点云数据集具有关键作用。这对于植物表型分析应用来说至关重要，有助于提高农业和生态研究的准确性和效率。这有助于填补现实世界中植物数据的空白，提高模型的多样性和准确性。这一技术为构建更全面的植物数据库提供了可能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15528">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3e8fe810ba9a3f6e7b242a3a53df5835.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e5adad00bf3e14febb16db304b5ddc65.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8e16947612c822874d6652ccefc199dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-27108270eac8905588576faf7f7ab84c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-13d6a42597f1a008cce29441ae7ef3b4.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="EVA-Expressive-Virtual-Avatars-from-Multi-view-Videos"><a href="#EVA-Expressive-Virtual-Avatars-from-Multi-view-Videos" class="headerlink" title="EVA: Expressive Virtual Avatars from Multi-view Videos"></a>EVA: Expressive Virtual Avatars from Multi-view Videos</h2><p><strong>Authors:Hendrik Junkawitsch, Guoxing Sun, Heming Zhu, Christian Theobalt, Marc Habermann</strong></p>
<p>With recent advancements in neural rendering and motion capture algorithms, remarkable progress has been made in photorealistic human avatar modeling, unlocking immense potential for applications in virtual reality, augmented reality, remote communication, and industries such as gaming, film, and medicine. However, existing methods fail to provide complete, faithful, and expressive control over human avatars due to their entangled representation of facial expressions and body movements. In this work, we introduce Expressive Virtual Avatars (EVA), an actor-specific, fully controllable, and expressive human avatar framework that achieves high-fidelity, lifelike renderings in real time while enabling independent control of facial expressions, body movements, and hand gestures. Specifically, our approach designs the human avatar as a two-layer model: an expressive template geometry layer and a 3D Gaussian appearance layer. First, we present an expressive template tracking algorithm that leverages coarse-to-fine optimization to accurately recover body motions, facial expressions, and non-rigid deformation parameters from multi-view videos. Next, we propose a novel decoupled 3D Gaussian appearance model designed to effectively disentangle body and facial appearance. Unlike unified Gaussian estimation approaches, our method employs two specialized and independent modules to model the body and face separately. Experimental results demonstrate that EVA surpasses state-of-the-art methods in terms of rendering quality and expressiveness, validating its effectiveness in creating full-body avatars. This work represents a significant advancement towards fully drivable digital human models, enabling the creation of lifelike digital avatars that faithfully replicate human geometry and appearance. </p>
<blockquote>
<p>随着神经网络渲染和运动捕捉算法的最新进展，光真人化的人类化身建模取得了显著进展，为虚拟现实、增强现实、远程通信以及游戏、电影和医学等行业的应用解锁了巨大的潜力。然而，现有方法由于面部表情和身体运动的纠缠表示，无法提供完整、忠实和富有表现力的化身控制。在这项工作中，我们介绍了“表情虚拟化身（EVA）”，这是一个针对特定演员、可完全控制和富有表现力的化身框架，可在实时实现高保真、逼真的渲染，同时实现对面部表情、身体运动和手势的独立控制。具体来说，我们的方法将人类化身设计为两层模型：一个表情模板几何层和一个3D高斯外观层。首先，我们提出了一种表情模板跟踪算法，该算法利用从粗到细的优化，可以从多角度视频中准确恢复身体运动、面部表情和非刚性变形参数。接下来，我们提出了一种新颖的解耦3D高斯外观模型，该模型有效地分离了身体和面部外观。不同于统一的高斯估计方法，我们的方法采用两个专门独立的模块来分别建模身体和面部。实验结果表明，EVA在渲染质量和表现力方面超过了最先进的方法，验证了其在创建全身化身方面的有效性。这项工作在朝着完全可驱动的数字化人物模型方面取得了重大进展，能够实现逼真复制人类几何和外观的数字化身。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15385v1">PDF</a> Accepted at SIGGRAPH 2025 Conference Track, Project page:   <a target="_blank" rel="noopener" href="https://vcai.mpi-inf.mpg.de/projects/EVA/">https://vcai.mpi-inf.mpg.de/projects/EVA/</a></p>
<p><strong>摘要</strong><br>     随着神经网络渲染和运动捕捉算法的最新进展，在光栅化人类化身建模方面取得了显著进展，为虚拟现实、增强现实、远程通信以及游戏、电影和医学等行业带来了巨大的潜力。然而，现有方法由于面部表情和躯体运动的纠缠表示，无法提供完整、忠诚和富有表现力的化身控制。在此工作中，我们引入了表现性虚拟化身（EVA），这是一个特定演员、可完全控制和富有表现力的化身框架，可在实时情况下实现高保真、逼真的渲染，同时实现对面部表情、躯体运动和手势的独立控制。具体来说，我们的方法将人类化身设计为一个两层模型：表现模板几何层和3D高斯外观层。首先，我们提出了一种表现模板跟踪算法，该算法利用从粗到细的优化技术，从多角度视频中准确恢复身体运动、面部表情和非刚性变形参数。接下来，我们提出了一种新颖的解耦3D高斯外观模型，旨在有效地分离身体和面部外观。我们的方法采用两个专业和独立的模块来分别建模身体和面部，不同于统一的高斯估计方法。实验结果表明，EVA在渲染质量和表现力方面超越了最先进的方法，验证了其在创建全身化身方面的有效性。这项工作代表着通向完全可驱动的数字化人类模型的重大进展，能够创建出逼真、忠诚地复制人类几何和外观的数字化身。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>最近的神经过程和运动捕捉算法进展在光栅化人类化身建模方面取得了显著进展。</li>
<li>现有方法在控制人类化身方面存在缺陷，无法提供完整、忠诚和富有表现力的控制。</li>
<li>Expressive Virtual Avatars (EVA)框架被引入，实现高保真、逼真的实时渲染，同时独立控制面部表情、身体运动和手势。</li>
<li>EVA采用两层模型设计：表现模板几何层和3D高斯外观层。</li>
<li>提出了表现模板跟踪算法，利用从粗到细的优化技术从多角度视频中恢复身体运动和面部表情。</li>
<li>创新的解耦3D高斯外观模型有效地分离了身体和面部外观。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15385">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-fcc274326a03eed48b70734a3a70bc11.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e90425be9385ad443d7e07844a98ace7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb5a5652c328b29c78cda766f10c6ca5.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="R3GS-Gaussian-Splatting-for-Robust-Reconstruction-and-Relocalization-in-Unconstrained-Image-Collections"><a href="#R3GS-Gaussian-Splatting-for-Robust-Reconstruction-and-Relocalization-in-Unconstrained-Image-Collections" class="headerlink" title="R3GS: Gaussian Splatting for Robust Reconstruction and Relocalization in   Unconstrained Image Collections"></a>R3GS: Gaussian Splatting for Robust Reconstruction and Relocalization in   Unconstrained Image Collections</h2><p><strong>Authors:Xu yan, Zhaohui Wang, Rong Wei, Jingbo Yu, Dong Li, Xiangde Liu</strong></p>
<p>We propose R3GS, a robust reconstruction and relocalization framework tailored for unconstrained datasets. Our method uses a hybrid representation during training. Each anchor combines a global feature from a convolutional neural network (CNN) with a local feature encoded by the multiresolution hash grids [2]. Subsequently, several shallow multi-layer perceptrons (MLPs) predict the attributes of each Gaussians, including color, opacity, and covariance. To mitigate the adverse effects of transient objects on the reconstruction process, we ffne-tune a lightweight human detection network. Once ffne-tuned, this network generates a visibility map that efffciently generalizes to other transient objects (such as posters, banners, and cars) with minimal need for further adaptation. Additionally, to address the challenges posed by sky regions in outdoor scenes, we propose an effective sky-handling technique that incorporates a depth prior as a constraint. This allows the inffnitely distant sky to be represented on the surface of a large-radius sky sphere, signiffcantly reducing ffoaters caused by errors in sky reconstruction. Furthermore, we introduce a novel relocalization method that remains robust to changes in lighting conditions while estimating the camera pose of a given image within the reconstructed 3DGS scene. As a result, R3GS significantly enhances rendering ffdelity, improves both training and rendering efffciency, and reduces storage requirements. Our method achieves state-of-the-art performance compared to baseline methods on in-the-wild datasets. The code will be made open-source following the acceptance of the paper. </p>
<blockquote>
<p>我们提出了R3GS，这是一个针对无约束数据集的稳健重建和重新定位框架。我们的方法采用混合表示进行训练。每个锚点结合了卷积神经网络（CNN）的全局特征和多分辨率哈希网格[2]编码的局部特征。随后，几个浅层的多层感知器（MLP）预测每个高斯属性的值，包括颜色、不透明度和协方差。为了减轻瞬态物体对重建过程的不利影响，我们对轻量级的人脸检测网络进行了微调。一旦微调完成，该网络可以生成一个可见性映射，该映射能够高效推广到其它瞬态对象（如海报、横幅和汽车等），并且几乎不需要进一步的适应。此外，为了解决户外场景中的天空区域带来的挑战，我们提出了一种有效的天空处理技术，该技术采用深度先验作为约束。这使得无限远的天空可以在大半径天空球面上表示出来，从而显著减少因天空重建错误引起的浮沫现象。此外，我们提出了一种新的重新定位方法，它在估计给定图像在重建的3DGS场景中的相机姿态时保持对光照变化的稳健性。因此，R3GS显著提高了渲染的保真度，提高了训练和渲染的效率，并降低了存储需求。我们的方法与基准数据集上的基线方法相比达到了最先进的性能。论文被接受后，代码将作为开源发布。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15294v1">PDF</a> 7 pages, 4 figures</p>
<p><strong>Summary</strong></p>
<p>本文提出一种针对无约束数据集的稳健重建与重新定位框架R3GS。它采用混合表示进行训练，结合卷积神经网络的全局特征与多分辨率哈希网格编码的局部特征。通过浅层多层感知器预测高斯属性的属性，包括颜色、不透明度和协方差。通过微调轻量级人体检测网络来减轻瞬时对象对重建过程的不利影响，生成可视化地图，有效地泛化到其他瞬时对象。针对户外场景的天空区域挑战，提出了一种有效的天空处理技术，采用深度先验作为约束。此外，还引入了一种新的重新定位方法，在光照条件变化时保持稳健，估计给定图像在重建的3DGS场景中的相机姿态。R3GS提高了渲染的保真度，提高了训练和渲染的效率，并降低了存储需求，在野生数据集上的性能达到了最新水平。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>R3GS是一个针对无约束数据集的重建和重新定位框架。</li>
<li>融合卷积神经网络的全局特征与多分辨率哈希网格的局部特征进行训练。</li>
<li>通过浅层多层感知器预测高斯属性的属性，包括颜色、不透明度和协方差。</li>
<li>通过微调轻量级人体检测网络来应对瞬时对象的影响，生成可视化地图。</li>
<li>引入天空处理技术，采用深度先验作为约束，处理天空区域的挑战。</li>
<li>提出一种新型的重新定位方法，能在光照条件变化时保持稳健。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15294">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-571ae2932bd6b3d75a5e82d2b40daf82.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a5061485433530f68434669d4779b602.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8624831b6e61a5b74e1d510472a41284.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6b6e0fdb12ead36eed8f46551b279099.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-94185f5e4d39add9ecb231640703842d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-93fd5d3eadb24d0dd1630c2f88ccfa82.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-96ddd3c6fb75de0d0c2de9d2d8a92577.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="X-GRM-Large-Gaussian-Reconstruction-Model-for-Sparse-view-X-rays-to-Computed-Tomography"><a href="#X-GRM-Large-Gaussian-Reconstruction-Model-for-Sparse-view-X-rays-to-Computed-Tomography" class="headerlink" title="X-GRM: Large Gaussian Reconstruction Model for Sparse-view X-rays to   Computed Tomography"></a>X-GRM: Large Gaussian Reconstruction Model for Sparse-view X-rays to   Computed Tomography</h2><p><strong>Authors:Yifan Liu, Wuyang Li, Weihao Yu, Chenxin Li, Alexandre Alahi, Max Meng, Yixuan Yuan</strong></p>
<p>Computed Tomography serves as an indispensable tool in clinical workflows, providing non-invasive visualization of internal anatomical structures. Existing CT reconstruction works are limited to small-capacity model architecture, inflexible volume representation, and small-scale training data. In this paper, we present X-GRM (X-ray Gaussian Reconstruction Model), a large feedforward model for reconstructing 3D CT from sparse-view 2D X-ray projections. X-GRM employs a scalable transformer-based architecture to encode an arbitrary number of sparse X-ray inputs, where tokens from different views are integrated efficiently. Then, tokens are decoded into a new volume representation, named Voxel-based Gaussian Splatting (VoxGS), which enables efficient CT volume extraction and differentiable X-ray rendering. To support the training of X-GRM, we collect ReconX-15K, a large-scale CT reconstruction dataset containing around 15,000 CT&#x2F;X-ray pairs across diverse organs, including the chest, abdomen, pelvis, and tooth etc. This combination of a high-capacity model, flexible volume representation, and large-scale training data empowers our model to produce high-quality reconstructions from various testing inputs, including in-domain and out-domain X-ray projections. Project Page: <a target="_blank" rel="noopener" href="https://github.com/CUHK-AIM-Group/X-GRM">https://github.com/CUHK-AIM-Group/X-GRM</a>. </p>
<blockquote>
<p>计算机断层扫描在临床工作流程中扮演着不可或缺的角色，能够非侵入性地呈现内部结构。现有的CT重建工作受限于小容量模型结构、不灵活的体积表示和小规模训练数据。在本文中，我们介绍了X-GRM（X射线高斯重建模型），这是一个用于从稀疏二维X射线投影重建三维CT的大型前馈模型。X-GRM采用可扩展的基于transformer的架构来编码任意数量的稀疏X射线输入，其中不同视角的标记被高效集成。然后，标记被解码成一种新的体积表示，称为基于体素的高斯溅射（VoxGS），这能够实现高效的CT体积提取和可微分X射线渲染。为了支持X-GRM的训练，我们收集了ReconX-15K大规模CT重建数据集，其中包含大约15000个涵盖多个器官的CT&#x2F;X射线对，包括胸部、腹部、骨盆和牙齿等。大容量模型、灵活的体积表示和大规模训练数据的结合使我们的模型能够从各种测试输入中产生高质量的重构，包括领域内的和跨领域的X射线投影。项目页面：<a target="_blank" rel="noopener" href="https://github.com/CUHK-AIM-Group/X-GRM%E3%80%82">https://github.com/CUHK-AIM-Group/X-GRM。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15235v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了计算机断层扫描（CT）在临床工作流中的重要性，以及现有的CT重建技术的局限性。为解决这些问题，研究者提出了X-GRM模型，这是一个用于从稀疏视角的二维X射线投影重建三维CT的大型前馈模型。X-GRM采用可扩展的基于变压器的架构，能够编码任意数量的稀疏X射线输入，并有效地整合不同视角的标记。然后，这些标记被解码成一种新的体积表示——基于体素的高斯喷溅（VoxGS），这有助于高效地进行CT体积提取和可微分X射线渲染。为了支持X-GRM的训练，研究者还收集了一个大规模的CT重建数据集ReconX-15K，包含大约1.5万个CT&#x2F;X射线配对，涵盖了多个器官类型。模型能够从多种测试输入中产生高质量的重构结果，包括领域内的和跨领域的X射线投影。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>计算机断层扫描（CT）在临床工作流中扮演重要角色，但现有的CT重建技术存在局限性。</li>
<li>X-GRM模型是一个用于从稀疏视角的二维X射线投影重建三维CT的大型前馈模型。</li>
<li>X-GRM采用基于变压器的可扩展架构进行编码和整合不同视角的标记。</li>
<li>X-GRM采用新的体积表示方法——基于体素的高斯喷溅（VoxGS）。</li>
<li>VoxGS有助于高效CT体积提取和可微分X射线渲染。</li>
<li>支持X-GRM训练的大型CT重建数据集ReconX-15K包含多种器官类型的1.5万个CT&#x2F;X射线配对。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15235">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-db094be52d587e70669e1fbcd90e6900.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e3b6bcf6250d03a7172ed10c9447d681.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-03db5e24ab0081795990b96e9d95ec5a.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="GT-2-GS-Geometry-aware-Texture-Transfer-for-Gaussian-Splatting"><a href="#GT-2-GS-Geometry-aware-Texture-Transfer-for-Gaussian-Splatting" class="headerlink" title="GT^2-GS: Geometry-aware Texture Transfer for Gaussian Splatting"></a>GT^2-GS: Geometry-aware Texture Transfer for Gaussian Splatting</h2><p><strong>Authors:Wenjie Liu, Zhongliang Liu, Junwei Shu, Changbo Wang, Yang Li</strong></p>
<p>Transferring 2D textures to 3D modalities is of great significance for improving the efficiency of multimedia content creation. Existing approaches have rarely focused on transferring image textures onto 3D representations. 3D style transfer methods are capable of transferring abstract artistic styles to 3D scenes. However, these methods often overlook the geometric information of the scene, which makes it challenging to achieve high-quality 3D texture transfer results. In this paper, we present GT^2-GS, a geometry-aware texture transfer framework for gaussian splitting. From the perspective of matching texture features with geometric information in rendered views, we identify the issue of insufficient texture features and propose a geometry-aware texture augmentation module to expand the texture feature set. Moreover, a geometry-consistent texture loss is proposed to optimize texture features into the scene representation. This loss function incorporates both camera pose and 3D geometric information of the scene, enabling controllable texture-oriented appearance editing. Finally, a geometry preservation strategy is introduced. By alternating between the texture transfer and geometry correction stages over multiple iterations, this strategy achieves a balance between learning texture features and preserving geometric integrity. Extensive experiments demonstrate the effectiveness and controllability of our method. Through geometric awareness, our approach achieves texture transfer results that better align with human visual perception. Our homepage is available at <a target="_blank" rel="noopener" href="https://vpx-ecnu.github.io/GT2-GS-website">https://vpx-ecnu.github.io/GT2-GS-website</a>. </p>
<blockquote>
<p>将二维纹理转移到三维模态对于提高多媒体内容创建效率具有重要意义。现有方法很少关注将图像纹理转移到三维表示上。三维风格转移方法能够将抽象的艺术风格转移到三维场景。然而，这些方法往往忽略了场景的三维几何信息，使得实现高质量的三维纹理转移结果具有挑战性。在本文中，我们提出了GT^2-GS，一个用于高斯分割的几何感知纹理转移框架。从匹配渲染视图中的纹理特征和几何信息的角度，我们发现了纹理特征不足的问题，并提出了一个几何感知纹理增强模块来扩展纹理特征集。此外，还提出了一种几何一致的纹理损失函数，以优化场景表示中的纹理特征。该损失函数结合了场景的相机姿态和三维几何信息，实现了可控的纹理导向的外观编辑。最后，引入了一种几何保留策略。通过多次迭代在纹理转移和几何校正阶段之间交替进行，该策略实现了学习纹理特征和保持几何完整性之间的平衡。大量实验证明了我们方法的有效性和可控性。通过几何感知，我们的方法实现了更符合人类视觉感知的纹理转移结果。我们的网站可在<a target="_blank" rel="noopener" href="https://vpx-ecnu.github.io/GT2-GS-website">网站链接</a>访问。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15208v1">PDF</a> 15 pages, 16 figures</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种几何感知的纹理转移框架GT^2-GS，用于高斯分割的纹理转移。该框架从匹配渲染视图中的纹理特征和几何信息出发，通过几何感知纹理增强模块扩展纹理特征集，并提出几何一致性纹理损失函数优化场景表示的纹理特征。此外，还介绍了几何保留策略，通过迭代纹理转移和几何校正阶段，实现纹理特征和几何完整性的平衡。实验证明，该方法有效且可控，通过几何感知，实现了更符合人类视觉感知的纹理转移结果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了一种新的几何感知纹理转移框架GT^2-GS，旨在提高多媒体内容创建的效率。</li>
<li>框架通过匹配渲染视图中的纹理特征和几何信息，解决了现有纹理转移方法忽略几何信息的问题。</li>
<li>引入了几何感知纹理增强模块，以扩展纹理特征集，提高纹理转移的质感。</li>
<li>提出了几何一致性纹理损失函数，结合相机姿态和场景的三维几何信息，实现可控的纹理导向外观编辑。</li>
<li>介绍了几何保留策略，通过迭代纹理转移和几何校正阶段，平衡了学习纹理特征和保持几何完整性的需求。</li>
<li>实验证明，该方法有效且可控，能够实现高质量的3D纹理转移结果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15208">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f3bfc0945b49dc3bdaa1a96590c7bc01.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-401834dfe70fd0817666a5aa77d1a3a1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-10bbd9c0064fed1a506c349cff08c8f2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-18285079b410f788adf627f89838ebb6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-54f3d7f42807bb2c9d857f3565a87b05.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="MonoSplat-Generalizable-3D-Gaussian-Splatting-from-Monocular-Depth-Foundation-Models"><a href="#MonoSplat-Generalizable-3D-Gaussian-Splatting-from-Monocular-Depth-Foundation-Models" class="headerlink" title="MonoSplat: Generalizable 3D Gaussian Splatting from Monocular Depth   Foundation Models"></a>MonoSplat: Generalizable 3D Gaussian Splatting from Monocular Depth   Foundation Models</h2><p><strong>Authors:Yifan Liu, Keyu Fan, Weihao Yu, Chenxin Li, Hao Lu, Yixuan Yuan</strong></p>
<p>Recent advances in generalizable 3D Gaussian Splatting have demonstrated promising results in real-time high-fidelity rendering without per-scene optimization, yet existing approaches still struggle to handle unfamiliar visual content during inference on novel scenes due to limited generalizability. To address this challenge, we introduce MonoSplat, a novel framework that leverages rich visual priors from pre-trained monocular depth foundation models for robust Gaussian reconstruction. Our approach consists of two key components: a Mono-Multi Feature Adapter that transforms monocular features into multi-view representations, coupled with an Integrated Gaussian Prediction module that effectively fuses both feature types for precise Gaussian generation. Through the Adapter’s lightweight attention mechanism, features are seamlessly aligned and aggregated across views while preserving valuable monocular priors, enabling the Prediction module to generate Gaussian primitives with accurate geometry and appearance. Through extensive experiments on diverse real-world datasets, we convincingly demonstrate that MonoSplat achieves superior reconstruction quality and generalization capability compared to existing methods while maintaining computational efficiency with minimal trainable parameters. Codes are available at <a target="_blank" rel="noopener" href="https://github.com/CUHK-AIM-Group/MonoSplat">https://github.com/CUHK-AIM-Group/MonoSplat</a>. </p>
<blockquote>
<p>近期通用3D高斯融合技术的进展，在无需针对场景优化的情况下，已经显示出在实时高保真渲染方面的前景。然而，现有方法仍然难以在新型场景推理时处理不熟悉的视觉内容，其泛化能力有限。为了应对这一挑战，我们引入了MonoSplat这一新型框架，它利用预训练的单目深度基础模型中的丰富视觉先验来进行稳健的高斯重建。我们的方法包括两个关键组成部分：Mono-Multi特征适配器，将单目特征转换为多视角表示，以及与集成高斯预测模块相结合，有效地融合这两种特征类型，以实现精确的高斯生成。通过适配器的轻量级注意力机制，特征在不同视角之间无缝对齐和聚合，同时保留有价值的单目先验，使预测模块能够生成具有精确几何形状和外观的高斯基本体。在多种真实世界数据集上的广泛实验表明，MonoSplat与现有方法相比，在重建质量和泛化能力方面表现出优越性，同时保持计算效率并具有最少的可训练参数。代码可从<a target="_blank" rel="noopener" href="https://github.com/CUHK-AIM-Group/MonoSplat%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/CUHK-AIM-Group/MonoSplat获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15185v1">PDF</a> </p>
<p><strong>摘要</strong><br>    新一代通用化3D高斯拼贴技术无需针对每个场景进行优化，即可实现实时高保真渲染，展现出良好前景。然而，现有方法在面对推理阶段遇到的新场景中的未知视觉内容时，仍面临泛化性有限的问题。为解决这一挑战，我们推出MonoSplat框架，利用预训练单目深度基础模型中的丰富视觉先验信息，进行稳健的高斯重建。我们的方法包括两个关键组件：Mono-Multi特征适配器，将单目特征转换为多视角表示，以及与集成高斯预测模块相结合，有效融合这两种特征类型，进行精确的高斯生成。通过适配器的轻量化注意力机制，不同视角的特征能够无缝对齐和聚合，同时保留宝贵的单目先验信息，使预测模块能够生成具有精确几何形状和外观的高斯基本体。在多种真实世界数据集上的广泛实验表明，MonoSplat相较于现有方法实现了更高的重建质量和泛化能力，同时保持计算效率且可训练参数最少。相关代码可访问：<a target="_blank" rel="noopener" href="https://github.com/CUHK-AIM-Group/MonoSplat">链接地址</a>。</p>
<p><strong>要点</strong></p>
<ol>
<li>引入MonoSplat框架，利用预训练的单目深度基础模型中的视觉先验信息，增强高斯重建的稳健性。</li>
<li>方法包括两个关键组件：Mono-Multi特征适配器与集成高斯预测模块，实现特征的类型转换和精确高斯生成。</li>
<li>适配器通过轻量化注意力机制无缝对齐和聚合不同视角的特征，同时保留单目先验信息。</li>
<li>MonoSplat在多种真实世界数据集上实现了较高的重建质量和泛化能力。</li>
<li>该方法保持计算效率且可训练参数最少。</li>
<li>相关代码已公开，方便研究者和开发者进一步探索和改进。</li>
<li>MonoSplat为处理新场景中的未知视觉内容提供了一个有效的解决方案。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15185">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-dc12ffecb6cdaac593875b159d8990a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7e9dca4a15b577499031d9413764bf72.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d8388c6b583cb7a5ac5cb1a2ca0ded7c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8a4ccb001821fc97e67391527141e989.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Consistent-Quantity-Quality-Control-across-Scenes-for-Deployment-Aware-Gaussian-Splatting"><a href="#Consistent-Quantity-Quality-Control-across-Scenes-for-Deployment-Aware-Gaussian-Splatting" class="headerlink" title="Consistent Quantity-Quality Control across Scenes for Deployment-Aware   Gaussian Splatting"></a>Consistent Quantity-Quality Control across Scenes for Deployment-Aware   Gaussian Splatting</h2><p><strong>Authors:Fengdi Zhang, Hongkun Cao, Ruqi Huang</strong></p>
<p>To reduce storage and computational costs, 3D Gaussian splatting (3DGS) seeks to minimize the number of Gaussians used while preserving high rendering quality, introducing an inherent trade-off between Gaussian quantity and rendering quality. Existing methods strive for better quantity-quality performance, but lack the ability for users to intuitively adjust this trade-off to suit practical needs such as model deployment under diverse hardware and communication constraints. Here, we present ControlGS, a 3DGS optimization method that achieves semantically meaningful and cross-scene consistent quantity-quality control. Through a single training run using a fixed setup and a user-specified hyperparameter reflecting quantity-quality preference, ControlGS can automatically find desirable quantity-quality trade-off points across diverse scenes, from compact objects to large outdoor scenes. It also outperforms baselines by achieving higher rendering quality with fewer Gaussians, and supports a broad adjustment range with stepless control over the trade-off. Project page: <a target="_blank" rel="noopener" href="https://zhang-fengdi.github.io/ControlGS/">https://zhang-fengdi.github.io/ControlGS/</a> </p>
<blockquote>
<p>为了减少存储和计算成本，3D高斯贴图（3DGS）旨在减少使用的高斯数量，同时保持高渲染质量，从而在高斯数量和渲染质量之间引入固有的权衡。现有方法努力提升数量与质量的性能，但缺乏让用户能够直观地调整这种权衡以适应实际需求的能力，例如在各种硬件和通信约束下进行模型部署。在这里，我们提出了ControlGS，这是一种3DGS优化方法，实现了语义上有意义和跨场景一致的数量与质量控制。通过一次固定的设置和用户指定的反映数量与质量偏好的超参数训练运行，ControlGS可以在各种场景中自动找到理想的质量与数量权衡点，从紧凑的对象到大型室外场景。它还通过用更少的高斯实现更高的渲染质量超越了基线，并支持广泛的调整范围和无缝控制权衡。项目页面：<a target="_blank" rel="noopener" href="https://zhang-fengdi.github.io/ControlGS/">https://zhang-fengdi.github.io/ControlGS/</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.10473v2">PDF</a> 16 pages, 7 figures, 7 tables. Project page available at   <a target="_blank" rel="noopener" href="https://zhang-fengdi.github.io/ControlGS/">https://zhang-fengdi.github.io/ControlGS/</a></p>
<p><strong>Summary</strong></p>
<p>该文介绍了控制3D高斯拼接（ControlGS）方法，此方法旨在优化渲染质量和使用的高斯数量之间的权衡。ControlGS能够在一次训练运行中自动找到场景间的最佳平衡，并提供用户可调节的偏好参数，以实现灵活的高斯数量与渲染质量之间的平衡。同时，ControlGS实现了更高质量的渲染，使用了更少的高斯数量，并且提供了广泛的调整范围和连续的控制权衡能力。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ControlGS是一种优化方法，用于控制三维高斯拼接中的高斯数量和渲染质量之间的权衡。</li>
<li>用户可以通过指定的超参数反映其对数量与质量的偏好。</li>
<li>ControlGS能够在不同场景之间自动找到最佳数量与质量之间的平衡。</li>
<li>该方法通过单次训练即可实现这一目标，并且可以在紧凑物体和大型室外场景等不同场景下应用。</li>
<li>ControlGS在渲染质量方面优于基准方法，同时使用了更少的高斯数量。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.10473">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-830b666bd569e79498eb1e0f782cb1ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-34a8e0c630fbc54a0eb59f7488aa85d9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4b8aca5f8cdf0656dea72f0d3243bce9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cce0b8b69f3ee09ba3ca812cca8f8e92.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="OpenFly-A-Comprehensive-Platform-for-Aerial-Vision-Language-Navigation"><a href="#OpenFly-A-Comprehensive-Platform-for-Aerial-Vision-Language-Navigation" class="headerlink" title="OpenFly: A Comprehensive Platform for Aerial Vision-Language Navigation"></a>OpenFly: A Comprehensive Platform for Aerial Vision-Language Navigation</h2><p><strong>Authors:Yunpeng Gao, Chenhui Li, Zhongrui You, Junli Liu, Zhen Li, Pengan Chen, Qizhi Chen, Zhonghan Tang, Liansheng Wang, Penghui Yang, Yiwen Tang, Yuhang Tang, Shuai Liang, Songyi Zhu, Ziqin Xiong, Yifei Su, Xinyi Ye, Jianan Li, Yan Ding, Dong Wang, Zhigang Wang, Bin Zhao, Xuelong Li</strong></p>
<p>Vision-Language Navigation (VLN) aims to guide agents by leveraging language instructions and visual cues, playing a pivotal role in embodied AI. Indoor VLN has been extensively studied, whereas outdoor aerial VLN remains underexplored. The potential reason is that outdoor aerial view encompasses vast areas, making data collection more challenging, which results in a lack of benchmarks. To address this problem, we propose OpenFly, a platform comprising various rendering engines, a versatile toolchain, and a large-scale benchmark for aerial VLN. Firstly, we integrate diverse rendering engines and advanced techniques for environment simulation, including Unreal Engine, GTA V, Google Earth, and 3D Gaussian Splatting (3D GS). Particularly, 3D GS supports real-to-sim rendering, further enhancing the realism of our environments. Secondly, we develop a highly automated toolchain for aerial VLN data collection, streamlining point cloud acquisition, scene semantic segmentation, flight trajectory creation, and instruction generation. Thirdly, based on the toolchain, we construct a large-scale aerial VLN dataset with 100k trajectories, covering diverse heights and lengths across 18 scenes. Moreover, we propose OpenFly-Agent, a keyframe-aware VLN model emphasizing key observations during flight. For benchmarking, extensive experiments and analyses are conducted, evaluating several recent VLN methods and showcasing the superiority of our OpenFly platform and agent. The toolchain, dataset, and codes will be open-sourced. </p>
<blockquote>
<p>视觉语言导航（VLN）旨在通过语言指令和视觉线索来引导代理，在嵌入式人工智能中扮演着至关重要的角色。室内VLN已经得到了广泛的研究，而户外空中VLN仍然被较少探索。可能的原因是户外空中视角涉及大片区域，使得数据收集更具挑战性，从而导致缺乏基准测试。为了解决这个问题，我们提出了OpenFly平台，该平台包括各种渲染引擎、通用工具链和用于空中VLN的大规模基准测试。首先，我们集成了多种渲染引擎和先进的环境模拟技术，包括Unreal Engine、GTA V、Google Earth和3D高斯平板印刷术（3D GS）。特别是，3D GS支持真实到模拟渲染，进一步增强了我们的环境真实性。其次，我们开发了一个高度自动化的空中VLN数据收集工具链，用于点云采集、场景语义分割、飞行轨迹创建和指令生成。第三，基于工具链，我们构建了一个大规模的空中VLN数据集，包含10万条轨迹，覆盖18个场景的多种高度和长度。此外，我们提出了OpenFly-Agent，一个关键帧感知的VLN模型，强调飞行过程中的关键观察。为了进行基准测试，我们进行了广泛的实验和分析，评估了几种最新的VLN方法，并展示了我们的OpenFly平台和代理的优势。工具链、数据集和代码将开源。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.18041v5">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该文介绍了视觉语言导航（VLN）的研究，特别强调了户外高空视角导航的潜在价值及其挑战。文章提出一个名为OpenFly的平台，集成多种渲染引擎、先进仿真技术和一个大规模的空中VLN数据集，为数据收集和环境模拟提供便捷的工具链。该平台包含一个针对高空视角导航设计的导航模型，并通过实验验证了其优越性。文章旨在推动空中VLN的研究进展。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLN结合了语言指令和视觉线索，对于实际机器人的行为具有重要意义。室内研究充分，而高空视角下的研究不足是由于高空数据收集的复杂性所致。提出一个新的OpenFly平台，解决数据收集的问题。</li>
<li>OpenFly平台包含多种渲染引擎和仿真技术，包括Unreal Engine、GTA V等先进引擎以及专为高空视角设计的实时模拟渲染技术（如3D GS）。这些技术增强了环境的真实感。</li>
<li>OpenFly开发了一个高度自动化的工具链，用于高空VLN数据收集，包括点云获取、场景语义分割、飞行轨迹创建和指令生成等环节。这使得构建大规模高空VLN数据集变得高效且可能。OpenFly基于这个工具链创建了一个大规模的高空VLN数据集，包含覆盖不同高度和长度的场景达到1万条轨迹。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.18041">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-86c28bef348c9cbd2db269c3e00b881d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-18eba59ba848bcf8eb8794610cb96533.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-15860a1fe81b72557546445e8b331ba0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c923fc96bb4894bcbdbfe34600689443.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Volumetrically-Consistent-3D-Gaussian-Rasterization"><a href="#Volumetrically-Consistent-3D-Gaussian-Rasterization" class="headerlink" title="Volumetrically Consistent 3D Gaussian Rasterization"></a>Volumetrically Consistent 3D Gaussian Rasterization</h2><p><strong>Authors:Chinmay Talegaonkar, Yash Belhe, Ravi Ramamoorthi, Nicholas Antipa</strong></p>
<p>Recently, 3D Gaussian Splatting (3DGS) has enabled photorealistic view synthesis at high inference speeds. However, its splatting-based rendering model makes several approximations to the rendering equation, reducing physical accuracy. We show that the core approximations in splatting are unnecessary, even within a rasterizer; We instead volumetrically integrate 3D Gaussians directly to compute the transmittance across them analytically. We use this analytic transmittance to derive more physically-accurate alpha values than 3DGS, which can directly be used within their framework. The result is a method that more closely follows the volume rendering equation (similar to ray-tracing) while enjoying the speed benefits of rasterization. Our method represents opaque surfaces with higher accuracy and fewer points than 3DGS. This enables it to outperform 3DGS for view synthesis (measured in SSIM and LPIPS). Being volumetrically consistent also enables our method to work out of the box for tomography. We match the state-of-the-art 3DGS-based tomography method with fewer points. Our code is publicly available at: <a target="_blank" rel="noopener" href="https://github.com/chinmay0301ucsd/Vol3DGS">https://github.com/chinmay0301ucsd/Vol3DGS</a> </p>
<blockquote>
<p>最近，3D高斯Splatting（3DGS）已经实现了快速推理下的逼真视图合成。然而，其基于Splatting的渲染模型对渲染方程进行了多个近似处理，降低了物理准确性。我们显示，Splatting中的核心近似值在光栅化器中是不必要的；相反，我们直接在体积内积分3D高斯值，以分析计算其透射率。我们使用这种分析透射率来得出比3DGS更准确的alpha值，并可在其框架内直接使用。结果是一种更紧密地遵循体积渲染方程（类似于光线追踪）的方法，同时享受光栅化的速度优势。我们的方法能够以更高的精度和更少的点数代表不透明表面，超过了用于视图合成的3DGS（在SSIM和LPIPS中测量）。其体积一致性还使我们的方法能够轻松应用于断层扫描。我们使用较少的点与最先进的基于3DGS的断层扫描方法相匹配。我们的代码可在以下网址公开访问：<a target="_blank" rel="noopener" href="https://github.com/chinmay0301ucsd/Vol3DGS">https://github.com/chinmay0301ucsd/Vol3DGS</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.03378v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于高斯的3D渲染方法近期被提出，虽然能够实现逼真的场景合成并拥有高效的推理速度，但在渲染方程的处理过程中存在一定程度的近似处理，从而影响其物理准确性。本研究直接对三维高斯进行体积积分，解析计算其透射率，旨在提供更精确的物理模型。该方法的提出结合了高准确性的光线追踪技术同时拥有高效渲染的速度优势。与传统的基于点近似的方法相比，我们的方法对于不透明表面的表示更为准确且使用更少的点。此外，其体积一致性还使其能应用于层析成像技术并达到优秀的性能表现。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>3DGS能够实现逼真的场景合成并拥有高效的推理速度，但在物理准确性方面存在不足。</li>
<li>研究通过直接对三维高斯进行体积积分计算透射率来提高物理准确性。</li>
<li>方法结合了光线追踪的高准确性以及高效渲染的速度优势。</li>
<li>对于不透明表面的表示相比传统方法更为准确且使用更少的点。</li>
<li>该方法的体积一致性使其能适用于层析成像技术并展现出卓越性能。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.03378">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-523363565277eccadd0024128844845f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-025e3fd668ea9795946199c03591fac2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8246de84fb8bb02305e59c3604e6ce5d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e05f14fe6836dd798b9db0c9802ee2a.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="VisionPAD-A-Vision-Centric-Pre-training-Paradigm-for-Autonomous-Driving"><a href="#VisionPAD-A-Vision-Centric-Pre-training-Paradigm-for-Autonomous-Driving" class="headerlink" title="VisionPAD: A Vision-Centric Pre-training Paradigm for Autonomous Driving"></a>VisionPAD: A Vision-Centric Pre-training Paradigm for Autonomous Driving</h2><p><strong>Authors:Haiming Zhang, Wending Zhou, Yiyao Zhu, Xu Yan, Jiantao Gao, Dongfeng Bai, Yingjie Cai, Bingbing Liu, Shuguang Cui, Zhen Li</strong></p>
<p>This paper introduces VisionPAD, a novel self-supervised pre-training paradigm designed for vision-centric algorithms in autonomous driving. In contrast to previous approaches that employ neural rendering with explicit depth supervision, VisionPAD utilizes more efficient 3D Gaussian Splatting to reconstruct multi-view representations using only images as supervision. Specifically, we introduce a self-supervised method for voxel velocity estimation. By warping voxels to adjacent frames and supervising the rendered outputs, the model effectively learns motion cues in the sequential data. Furthermore, we adopt a multi-frame photometric consistency approach to enhance geometric perception. It projects adjacent frames to the current frame based on rendered depths and relative poses, boosting the 3D geometric representation through pure image supervision. Extensive experiments on autonomous driving datasets demonstrate that VisionPAD significantly improves performance in 3D object detection, occupancy prediction and map segmentation, surpassing state-of-the-art pre-training strategies by a considerable margin. </p>
<blockquote>
<p>本文介绍了VisionPAD，这是一种为自动驾驶中的以视觉为中心的算法设计的新型自监督预训练范式。与之前采用具有明确深度监督的神经渲染方法不同，VisionPAD利用更有效的3D高斯Splatting技术，仅使用图像作为监督来重建多视图表示。具体来说，我们为体素速度估计提出了一种自监督方法。通过将体素变形到相邻帧并对渲染输出进行监督，模型有效地学习了序列数据中的运动线索。此外，我们采用多帧光度一致性方法来增强几何感知。它根据渲染深度和相对姿态将相邻帧投影到当前帧，通过纯图像监督增强3D几何表示。在自动驾驶数据集上的广泛实验表明，VisionPAD在3D目标检测、占用预测和地图分割方面的性能得到了显著提高，显著超越了最先进的预训练策略。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.14716v2">PDF</a> Accepted at CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>本文提出了VisionPAD，这是一种针对自动驾驶中视觉算法的新型自监督预训练范式。与以往采用明确深度监督的神经渲染方法不同，VisionPAD利用更有效的3D高斯Splatting技术，仅使用图像作为监督来重建多视角表示。具体地，我们引入了一种自监督的体素速度估计方法。通过邻帧变形和监督渲染输出，模型有效地学习了序列数据中的运动线索。此外，我们采用多帧光度一致性方法，以提高几何感知能力。它将相邻帧投影到当前帧上，基于渲染深度和相对姿态，通过纯图像监督增强3D几何表示。在自动驾驶数据集上的大量实验表明，VisionPAD在3D目标检测、占用预测和地图分割方面的性能得到显著提升，优于其他先进的预训练策略。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VisionPAD是一种针对自动驾驶视觉算法的自监督预训练范式。</li>
<li>与传统方法不同，VisionPAD采用3D高斯Splatting技术重建多视角表示。</li>
<li>VisionPAD利用自监督方法估计体素速度，学习序列数据中的运动线索。</li>
<li>通过多帧光度一致性方法增强几何感知能力。</li>
<li>VisionPAD通过投影相邻帧到当前帧来提高3D几何表示。</li>
<li>在自动驾驶数据集上，VisionPAD显著提高了3D目标检测、占用预测和地图分割的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.14716">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-cd1b646db3b9ceb6dd96470313d69f72.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac354524907b734ddec680a60fd5efcb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c019efddfbad4b028125325e3d6e14c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2985ac443c7217d98c42ce9b4d57a16c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aa1dc94103d0c6f9aef97d79de06a1e7.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-24/3DGS/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-24/3DGS/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/3DGS/">
                                    <span class="chip bg-color">3DGS</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-24/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-d3126b4d9896e174493289f273f89f37.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2025-05-24  Seeing through Satellite Images at Street Views
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-24
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-24/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-fcc274326a03eed48b70734a3a70bc11.jpg" class="responsive-img" alt="元宇宙/虚拟人">
                        
                        <span class="card-title">元宇宙/虚拟人</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            元宇宙/虚拟人 方向最新论文已更新，请持续关注 Update in 2025-05-24  EVA Expressive Virtual Avatars from Multi-view Videos
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-24
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/" class="post-category">
                                    元宇宙/虚拟人
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                        <span class="chip bg-color">元宇宙/虚拟人</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">24801.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
