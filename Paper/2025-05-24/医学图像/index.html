<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-24  Tracking the Flight Exploring a Computational Framework for Analyzing   Escape Responses in Plains Zebra (Equus quagga)">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-4686fad44d4ed9b29375e021d84df535.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-24
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-27
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    84 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-24-æ›´æ–°"><a href="#2025-05-24-æ›´æ–°" class="headerlink" title="2025-05-24 æ›´æ–°"></a>2025-05-24 æ›´æ–°</h1><h2 id="Tracking-the-Flight-Exploring-a-Computational-Framework-for-Analyzing-Escape-Responses-in-Plains-Zebra-Equus-quagga"><a href="#Tracking-the-Flight-Exploring-a-Computational-Framework-for-Analyzing-Escape-Responses-in-Plains-Zebra-Equus-quagga" class="headerlink" title="Tracking the Flight: Exploring a Computational Framework for Analyzing   Escape Responses in Plains Zebra (Equus quagga)"></a>Tracking the Flight: Exploring a Computational Framework for Analyzing   Escape Responses in Plains Zebra (Equus quagga)</h2><p><strong>Authors:Isla Duporge, Sofia Minano, Nikoloz Sirmpilatze, Igor Tatarnikov, Scott Wolf, Adam L. Tyson, Daniel Rubenstein</strong></p>
<p>Ethological research increasingly benefits from the growing affordability and accessibility of drones, which enable the capture of high-resolution footage of animal movement at fine spatial and temporal scales. However, analyzing such footage presents the technical challenge of separating animal movement from drone motion. While non-trivial, computer vision techniques such as image registration and Structure-from-Motion (SfM) offer practical solutions. For conservationists, open-source tools that are user-friendly, require minimal setup, and deliver timely results are especially valuable for efficient data interpretation. This study evaluates three approaches: a bioimaging-based registration technique, an SfM pipeline, and a hybrid interpolation method. We apply these to a recorded escape event involving 44 plains zebras, captured in a single drone video. Using the best-performing method, we extract individual trajectories and identify key behavioral patterns: increased alignment (polarization) during escape, a brief widening of spacing just before stopping, and tighter coordination near the groupâ€™s center. These insights highlight the methodâ€™s effectiveness and its potential to scale to larger datasets, contributing to broader investigations of collective animal behavior. </p>
<blockquote>
<p>ç”Ÿæ€å­¦ç ”ç©¶æ—¥ç›Šå—ç›Šäºæ— äººæœºæ—¥ç›Šå¯è´Ÿæ‹…çš„æ€§ä»·æ¯”å’Œæ™®åŠæ€§ï¼Œæ— äººæœºèƒ½å¤Ÿæ•æ‰åˆ°åŠ¨ç‰©è¿åŠ¨çš„é«˜åˆ†è¾¨ç‡å½±åƒï¼Œç²¾ç»†çš„ç©ºé—´å’Œæ—¶é—´å°ºåº¦ä¸Šéƒ½èƒ½è§‚å¯Ÿåˆ°ã€‚ç„¶è€Œï¼Œåˆ†æè¿™æ ·çš„å½±åƒé¢ä¸´æŠ€æœ¯æŒ‘æˆ˜ï¼Œå³å¦‚ä½•å°†åŠ¨ç‰©è¿åŠ¨ä¸æ— äººæœºè¿åŠ¨åŒºåˆ†å¼€æ¥ã€‚è™½ç„¶æœ‰ä¸€å®šçš„éš¾åº¦ï¼Œä½†è®¡ç®—æœºè§†è§‰æŠ€æœ¯ï¼Œå¦‚å›¾åƒé…å‡†å’Œä»è¿åŠ¨ç»“æ„ï¼ˆSfMï¼‰ï¼Œæä¾›äº†å®ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚å¯¹äºä¿æŠ¤ä¸»ä¹‰è€…æ¥è¯´ï¼Œç”¨æˆ·å‹å¥½çš„å¼€æºå·¥å…·ã€éœ€è¦æœ€å°‘çš„è®¾ç½®å¹¶ä¸”èƒ½åŠæ—¶åé¦ˆç»“æœï¼Œå¯¹äºé«˜æ•ˆçš„æ•°æ®è§£è¯»å°¤å…¶æœ‰ä»·å€¼ã€‚æœ¬ç ”ç©¶è¯„ä¼°äº†ä¸‰ç§æ–¹æ³•ï¼šä¸€ç§åŸºäºç”Ÿç‰©æˆåƒçš„é…å‡†æŠ€æœ¯ã€ä¸€ä¸ªSfMç®¡é“å’Œä¸€ä¸ªæ··åˆæ’å€¼æ–¹æ³•ã€‚æˆ‘ä»¬å°†è¿™äº›æ–¹æ³•åº”ç”¨åœ¨ä¸€ä¸ªæ¶‰åŠ44åªæ–‘é©¬é€ƒè·‘äº‹ä»¶çš„è®°å½•ä¸Šï¼Œè¯¥è§†é¢‘ç”±å•æ¶æ— äººæœºæ‹æ‘„ã€‚ä½¿ç”¨è¡¨ç°æœ€å¥½çš„æ–¹æ³•ï¼Œæˆ‘ä»¬æå–äº†å•ä¸ªè½¨è¿¹å¹¶è¯†åˆ«å‡ºäº†å…³é”®çš„è¡Œä¸ºæ¨¡å¼ï¼šé€ƒè·‘æ—¶å¢åŠ å¯¹é½ï¼ˆæåŒ–ï¼‰ã€é€ƒè·‘å‰çŸ­æš‚æ‰©å¤§é—´è·ä»¥åŠåœ¨ç¾¤ä½“ä¸­å¿ƒé™„è¿‘æ›´ç´§å¯†çš„åè°ƒã€‚è¿™äº›è§è§£å‡¸æ˜¾äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ä»¥åŠå…¶æ‰©å±•åˆ°æ›´å¤§æ•°æ®é›†çš„æ½œåŠ›ï¼Œä¸ºæ›´å¹¿æ³›çš„ç¾¤ä½“åŠ¨ç‰©è¡Œä¸ºç ”ç©¶åšå‡ºäº†è´¡çŒ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16882v1">PDF</a> Accepted to the CV4Animals workshop at CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>éšç€æ— äººæœºæˆæœ¬å’ŒæŠ€æœ¯çš„æ™®åŠï¼Œå…¶è¢«å¹¿æ³›åº”ç”¨äºåŠ¨ç‰©è¡Œä¸ºå­¦ç ”ç©¶ä¸­ï¼Œèƒ½å¤Ÿæ•æ‰åŠ¨ç‰©è¡Œä¸ºçš„é«˜åˆ†è¾¨ç‡å½±åƒã€‚ç„¶è€Œï¼Œå¦‚ä½•åŒºåˆ†åŠ¨ç‰©è¡Œä¸ºå’Œæ— äººæœºè¿åŠ¨çš„åˆ†ææ˜¯ä¸€å¤§æŠ€æœ¯æŒ‘æˆ˜ã€‚è®¡ç®—æœºè§†è§‰æŠ€æœ¯å¦‚å›¾åƒé…å‡†å’Œè¿åŠ¨ç»“æ„æŠ€æœ¯ä¸ºæ­¤æä¾›äº†è§£å†³æ–¹æ¡ˆã€‚æœ¬ç ”ç©¶è¯„ä¼°äº†ä¸‰ç§æ–¹æ³•ï¼Œå¹¶åº”ç”¨äºè®°å½•çš„ä¸€æ¬¡æ¶‰åŠ44åªæ–‘é©¬é€ƒç¦»äº‹ä»¶çš„åˆ†æã€‚é€šè¿‡æœ€ä½³æ–¹æ³•çš„åº”ç”¨ï¼Œæå–äº†ä¸ªä½“è½¨è¿¹å¹¶è¯†åˆ«äº†å…³é”®è¡Œä¸ºæ¨¡å¼ï¼Œå¦‚é€ƒè·‘æ—¶çš„å¢åŠ å¯¹é½ã€é€ƒè·‘å‰çš„çŸ­æš‚é—´è·æ‰©å¤§ä»¥åŠé è¿‘ç¾¤ä½“ä¸­å¿ƒçš„ç´§å¯†åè°ƒã€‚è¿™ä¸ºé›†ä½“åŠ¨ç‰©è¡Œä¸ºç ”ç©¶æä¾›äº†æœ‰æ•ˆæ–¹æ³•å’Œæ½œåœ¨çš„å¤§è§„æ¨¡æ•°æ®é›†åº”ç”¨å‰æ™¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ— äººæœºåœ¨åŠ¨ç‰©è¡Œä¸ºå­¦ç ”ç©¶ä¸­å¾—åˆ°å¹¿æ³›åº”ç”¨ï¼Œå¯æ•æ‰é«˜æ¸…æ™°åº¦å½±åƒã€‚</li>
<li>åˆ†ææ— äººæœºå½±åƒæ—¶ï¼ŒåŒºåˆ†åŠ¨ç‰©è¡Œä¸ºä¸æ— äººæœºè¿åŠ¨æ˜¯ä¸€å¤§æŠ€æœ¯æŒ‘æˆ˜ã€‚</li>
<li>è®¡ç®—æœºè§†è§‰æŠ€æœ¯å¦‚å›¾åƒé…å‡†å’Œè¿åŠ¨ç»“æ„æŠ€æœ¯ä¸ºè§£å†³æ­¤æŒ‘æˆ˜æä¾›äº†æ–¹æ³•ã€‚</li>
<li>æœ¬ç ”ç©¶è¯„ä¼°äº†ä¸‰ç§æ–¹æ³•ï¼Œå¹¶æˆåŠŸåº”ç”¨äºæ–‘é©¬é€ƒç¦»äº‹ä»¶çš„åˆ†æã€‚</li>
<li>é€šè¿‡æœ€ä½³æ–¹æ³•çš„åº”ç”¨ï¼Œå¯ä»¥æå–ä¸ªä½“è½¨è¿¹å¹¶è¯†åˆ«å…³é”®è¡Œä¸ºæ¨¡å¼ã€‚</li>
<li>è¿™äº›æ–¹æ³•å¯¹äºç ”ç©¶é›†ä½“åŠ¨ç‰©è¡Œä¸ºæœ‰æ•ˆï¼Œå¹¶æœ‰æœ›åº”ç”¨äºå¤§è§„æ¨¡æ•°æ®é›†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16882">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cf4ec80c070dd74fb48586a5b6f2df12.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d31d92427b4a610969321c0672cb4d40.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-26bb7b5bdc083d233a0dcbf1b021cef1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-22aa3ac37ca35b495a3a31062e07e973.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e255c637afd388132bc5a96cfbea60dc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ae7eb6a2e35511293f41512f696aa5f2.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Improvement-of-H-2-O-2-electrogeneration-using-a-Vulcan-XC72-carbon-based-electrocatalyst-modified-with-Ce-doped-Nb-2-O-5"><a href="#Improvement-of-H-2-O-2-electrogeneration-using-a-Vulcan-XC72-carbon-based-electrocatalyst-modified-with-Ce-doped-Nb-2-O-5" class="headerlink" title="Improvement of H$_2$O$_2$ electrogeneration using a Vulcan XC72   carbon-based electrocatalyst modified with Ce-doped Nb$_2$O$_5$"></a>Improvement of H$_2$O$_2$ electrogeneration using a Vulcan XC72   carbon-based electrocatalyst modified with Ce-doped Nb$_2$O$_5$</h2><p><strong>Authors:Aline B. Trench, JoÃ£o Paulo C. Moura, Vanessa S. Antonin, Caio Machado Fernandes, Liying Liu, Mauro C. Santos</strong></p>
<p>The use of the oxygen reduction reaction (ORR) for in-situ production of H$_2$O$_2$ is an attractive alternative to replace the methods based on anthraquinone oxidation. This study investigates the modification of Vulcan XC72 carbon with Ce-doped Nb$_2$O$_5$ in different molar proportions and its application as electrocatalysts in the ORR. One performed the characterization of the electrocatalysts using X-ray diffraction, Raman spectroscopy, scanning electron microscopy, transmission electron microscopy, contact angle measurements, and X-ray photoelectron spectroscopy. Subsequently, the electrocatalysts were analyzed for the ORR and the Nb$_2$O$_5$ doped with 0.5% Ce showing the highest electrocatalytic response. This electrocatalyst was also employed as a gas diffusion electrode and exhibited more significant H$_2$O$_2$ production at all potentials than the Vulcan XC72 carbon modified solely with Nb$_2$O$_5$. At the applied potentials of -1.3 V and -1.9 V, it produced 105% and 86% more H$_2$O$_2$, respectively, than the Vulcan XC72 carbon modified only with Nb$_2$O$_5$. These results can be attributed to the doping of Nb$_2$O$_5$ with 0.5% Ce, which induces local distortions in the crystal lattice of Nb$_2$O$_5$ due to the difference in ionic radius between Nb$^{5+}$ and Ce$^{3+}$, which combined with increased hydrophilicity and wetting properties, may have facilitated electron transfer and O$_2$ transport, favoring the ORR. </p>
<blockquote>
<p>ä½¿ç”¨æ°§è¿˜åŸååº”ï¼ˆORRï¼‰è¿›è¡ŒH<sub>2</sub>O<sub>2</sub>çš„åŸä½ç”Ÿäº§æ˜¯ä¸€ç§å¸å¼•äººçš„æ›¿ä»£æ–¹æ³•ï¼Œå–ä»£äº†åŸºäºè’½é†Œæ°§åŒ–çš„æ–¹æ³•ã€‚æœ¬ç ”ç©¶å¯¹Vulcan XC72ç¢³è¿›è¡Œäº†ä¸åŒæ‘©å°”æ¯”ä¾‹çš„é“ˆæºæ‚çš„Nb<sub>2</sub>O<sub>5</sub>æ”¹æ€§ï¼Œå¹¶æ¢è®¨äº†å…¶åœ¨ORRä¸­çš„ç”µå‚¬åŒ–ä½œç”¨ã€‚åˆ©ç”¨Xå°„çº¿è¡å°„ã€æ‹‰æ›¼å…‰è°±ã€æ‰«æç”µå­æ˜¾å¾®é•œã€é€å°„ç”µå­æ˜¾å¾®é•œã€æ¥è§¦è§’æµ‹é‡å’ŒXå°„çº¿å…‰ç”µå­èƒ½è°±å¯¹ç”µå‚¬åŒ–å‰‚è¿›è¡Œäº†è¡¨å¾ã€‚éšåï¼Œå¯¹ç”µå‚¬åŒ–å‰‚è¿›è¡Œäº†ORRåˆ†æï¼Œå‘ç°æºæ‚äº†0.5% Ceçš„Nb<sub>2</sub>O<sub>5</sub>è¡¨ç°å‡ºæœ€é«˜çš„ç”µå‚¬åŒ–å“åº”ã€‚è¯¥ç”µå‚¬åŒ–å‰‚è¿˜è¢«ç”¨ä½œæ°”ä½“æ‰©æ•£ç”µæï¼Œåœ¨æ‰€æœ‰çš„ç”µä½ä¸‹éƒ½è¡¨ç°å‡ºæ¯”ä»…ç”¨Nb<sub>2</sub>O<sub>5</sub>æ”¹æ€§çš„Vulcan XC72ç¢³æ›´å¤§çš„H<sub>2</sub>O<sub>2</sub>ç”Ÿäº§æ•ˆç‡ã€‚åœ¨-1.3 Vå’Œ-1.9 Vçš„æ–½åŠ ç”µä½ä¸‹ï¼Œå®ƒäº§ç”Ÿçš„H<sub>2</sub>O<sub>2</sub>åˆ†åˆ«æ¯”ä»…ç”¨Nb<sub>2</sub>O<sub>5</sub>æ”¹æ€§çš„Vulcan XC72ç¢³å¤š105%å’Œ86%ã€‚è¿™äº›ç»“æœå¯å½’å› äºNb<sub>2</sub>O<sub>5</sub>çš„æºæ‚ï¼Œå…¶ä¸­æºæ‚äº†0.5%çš„Ceï¼Œç”±äºNb<sup>5+</sup>å’ŒCe<sup>3+</sup>ä¹‹é—´çš„ç¦»å­åŠå¾„å·®å¼‚ï¼Œä¼šåœ¨Nb<sub>2</sub>O<sub>5</sub>çš„æ™¶æ ¼ä¸­äº§ç”Ÿå±€éƒ¨ç•¸å˜ã€‚è¿™ç§ç•¸å˜ä¸å¢åŠ çš„äº²æ°´æ€§å’Œæ¶¦æ¹¿æ€§èƒ½ç›¸ç»“åˆï¼Œå¯èƒ½ä¿ƒè¿›äº†ç”µå­è½¬ç§»å’Œæ°§æ°”çš„ä¼ è¾“ï¼Œæœ‰åˆ©äºæ°§è¿˜åŸååº”ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16871v1">PDF</a> </p>
<p><strong>Summary</strong><br>     åŸºäºæ°§è¿˜åŸååº”ï¼ˆORRï¼‰çš„ç°åœºç”Ÿäº§è¿‡æ°§åŒ–æ°¢ï¼ˆH$_2$O$_2$ï¼‰æ–¹æ³•å¯¹æ›¿ä»£åŸºäºè’½é†Œæ°§åŒ–æ³•å…·æœ‰å¸å¼•åŠ›ã€‚æœ¬ç ”ç©¶é€šè¿‡ä¸åŒæ‘©å°”æ¯”ä¾‹çš„Ceæºæ‚Nb$_2$O$_5$æ”¹æ€§Vulcan XC72ç¢³ï¼Œå¹¶å°†å…¶ç”¨ä½œORRä¸­çš„ç”µå‚¬åŒ–å‰‚ã€‚ç»è¿‡ä¸€ç³»åˆ—è¡¨å¾æ–¹æ³•åï¼Œå‘ç°æºæ‚æœ‰0.5% Ceçš„Nb$_2$O$_5$è¡¨ç°æœ€ä½³ï¼Œå…¶åœ¨æ‰€æœ‰ç”µä½ä¸‹çš„è¿‡æ°§åŒ–æ°¢äº§é‡æ˜¾è‘—é«˜äºä»…ä½¿ç”¨Nb$_2$O$_5$æ”¹æ€§çš„Vulcan XC72ç¢³ã€‚æ­¤æˆæœå½’å› äºæºæ‚Ceå¯¼è‡´Nb$_2$O$_5$æ™¶ä½“æ™¶æ ¼æ‰­æ›²ç­‰æ”¹å˜ï¼Œä¿ƒè¿›ç”µå­è½¬ç§»å’Œæ°§è¾“é€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶é‡‡ç”¨æ°§è¿˜åŸååº”ï¼ˆORRï¼‰ç”Ÿäº§è¿‡æ°§åŒ–æ°¢ï¼ˆH$_2$O$_2$ï¼‰ä½œä¸ºæ›¿ä»£åŸºäºè’½é†Œæ°§åŒ–æ³•çš„æ–°æ–¹æ³•ã€‚</li>
<li>é€šè¿‡æºæ‚ä¸åŒæ¯”ä¾‹çš„Ceåˆ°Nb$_2$O$_5$æ¥æ”¹æ€§Vulcan XC72ç¢³ï¼Œç”¨ä½œç”µå‚¬åŒ–å‰‚ã€‚</li>
<li>æºæ‚æœ‰0.5% Ceçš„Nb$_2$O$_5$åœ¨ORRä¸­è¡¨ç°æœ€ä½³ï¼Œå…¶ç”µå‚¬åŒ–å“åº”æœ€é«˜ã€‚</li>
<li>è¯¥ç”µå‚¬åŒ–å‰‚åœ¨ä½œä¸ºæ°”ä½“æ‰©æ•£ç”µæåº”ç”¨æ—¶ï¼Œåœ¨æ‰€æœ‰ç”µä½ä¸‹çš„è¿‡æ°§åŒ–æ°¢äº§é‡æ˜¾è‘—é«˜äºä»…ä½¿ç”¨Nb$_2$O$_5$æ”¹æ€§çš„Vulcan XC72ç¢³ã€‚</li>
<li>åœ¨-1.3Vå’Œ-1.9Vçš„åº”ç”¨ç”µä½ä¸‹ï¼Œå…¶åˆ†åˆ«äº§ç”Ÿäº†105%å’Œ86%æ›´å¤šçš„è¿‡æ°§åŒ–æ°¢ã€‚</li>
<li>æºæ‚Ceå¯¼è‡´Nb$_2$O$_5$æ™¶ä½“æ™¶æ ¼æ‰­æ›²ï¼Œå¯èƒ½ä¿ƒè¿›äº†ç”µå­è½¬ç§»å’Œæ°§è¾“é€ï¼Œä»è€Œæœ‰åˆ©äºORRã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16871">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f7c222ccc5bcfb382d5dff1e0341827a.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Hypergraph-Tversky-Aware-Domain-Incremental-Learning-for-Brain-Tumor-Segmentation-with-Missing-Modalities"><a href="#Hypergraph-Tversky-Aware-Domain-Incremental-Learning-for-Brain-Tumor-Segmentation-with-Missing-Modalities" class="headerlink" title="Hypergraph Tversky-Aware Domain Incremental Learning for Brain Tumor   Segmentation with Missing Modalities"></a>Hypergraph Tversky-Aware Domain Incremental Learning for Brain Tumor   Segmentation with Missing Modalities</h2><p><strong>Authors:Junze Wang, Lei Fan, Weipeng Jing, Donglin Di, Yang Song, Sidong Liu, Cong Cong</strong></p>
<p>Existing methods for multimodal MRI segmentation with missing modalities typically assume that all MRI modalities are available during training. However, in clinical practice, some modalities may be missing due to the sequential nature of MRI acquisition, leading to performance degradation. Furthermore, retraining models to accommodate newly available modalities can be inefficient and may cause overfitting, potentially compromising previously learned knowledge. To address these challenges, we propose Replay-based Hypergraph Domain Incremental Learning (ReHyDIL) for brain tumor segmentation with missing modalities. ReHyDIL leverages Domain Incremental Learning (DIL) to enable the segmentation model to learn from newly acquired MRI modalities without forgetting previously learned information. To enhance segmentation performance across diverse patient scenarios, we introduce the Cross-Patient Hypergraph Segmentation Network (CHSNet), which utilizes hypergraphs to capture high-order associations between patients. Additionally, we incorporate Tversky-Aware Contrastive (TAC) loss to effectively mitigate information imbalance both across and within different modalities. Extensive experiments on the BraTS2019 dataset demonstrate that ReHyDIL outperforms state-of-the-art methods, achieving an improvement of over 2% in the Dice Similarity Coefficient across various tumor regions. Our code is available at ReHyDIL. </p>
<blockquote>
<p>ç°æœ‰é’ˆå¯¹ç¼ºå¤±æ¨¡æ€çš„å¤šæ¨¡æ€MRIåˆ†å‰²æ–¹æ³•é€šå¸¸å‡è®¾åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ‰€æœ‰MRIæ¨¡æ€éƒ½æ˜¯å¯ç”¨çš„ã€‚ç„¶è€Œï¼Œåœ¨ä¸´åºŠå®è·µä¸­ï¼Œç”±äºMRIé‡‡é›†çš„åºåˆ—æ€§ï¼ŒæŸäº›æ¨¡æ€å¯èƒ½ä¼šç¼ºå¤±ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚æ­¤å¤–ï¼Œä¸ºäº†å®¹çº³æ–°å¯ç”¨çš„æ¨¡æ€è€Œé‡æ–°è®­ç»ƒæ¨¡å‹å¯èƒ½æ•ˆç‡ä½ä¸‹ï¼Œå¹¶å¯èƒ½å¯¼è‡´è¿‡åº¦æ‹Ÿåˆï¼Œä»è€Œå¯èƒ½æŸå®³ä¹‹å‰å­¦åˆ°çš„çŸ¥è¯†ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºå›æ”¾å’Œè¶…å›¾åŸŸå¢é‡å­¦ä¹ ï¼ˆReHyDILï¼‰çš„è„‘è‚¿ç˜¤åˆ†å‰²æ–¹æ³•ï¼Œç”¨äºå¤„ç†ç¼ºå¤±æ¨¡æ€çš„æƒ…å†µã€‚ReHyDILåˆ©ç”¨åŸŸå¢é‡å­¦ä¹ ï¼ˆDILï¼‰ä½¿åˆ†å‰²æ¨¡å‹èƒ½å¤Ÿä»æ–°è·å–çš„MRIæ¨¡æ€ä¸­å­¦ä¹ ï¼Œè€Œä¸ä¼šå¿˜è®°ä¹‹å‰å­¦åˆ°çš„ä¿¡æ¯ã€‚ä¸ºäº†æé«˜ä¸åŒæ‚£è€…åœºæ™¯ä¸‹çš„åˆ†å‰²æ€§èƒ½ï¼Œæˆ‘ä»¬å¼•å…¥äº†è·¨æ‚£è€…è¶…å›¾åˆ†å‰²ç½‘ç»œï¼ˆCHSNetï¼‰ï¼Œè¯¥ç½‘ç»œåˆ©ç”¨è¶…å›¾æ¥æ•æ‰æ‚£è€…ä¹‹é—´çš„é«˜é˜¶å…³è”ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç»“åˆäº†Tverskyæ„ŸçŸ¥å¯¹æ¯”ï¼ˆTACï¼‰æŸå¤±ï¼Œä»¥æœ‰æ•ˆç¼“è§£ä¸åŒæ¨¡æ€ä¹‹é—´å’Œå†…éƒ¨çš„ä¿¡æ¯ä¸å¹³è¡¡é—®é¢˜ã€‚åœ¨BraTS2019æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒReHyDILä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œåœ¨å„ç§è‚¿ç˜¤åŒºåŸŸçš„Diceç›¸ä¼¼ç³»æ•°ä¸Šæé«˜äº†2%ä»¥ä¸Šã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨ReHyDILç½‘ç«™ä¸Šæ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16809v1">PDF</a> </p>
<p><strong>Summary</strong><br>    æœ¬æ–‡æå‡ºä¸€ç§åŸºäºå›æ”¾å’Œè¶…å›¾åŸŸçš„å¢é‡å­¦ä¹ ï¼ˆReHyDILï¼‰æ–¹æ³•ï¼Œç”¨äºå¤„ç†MRIå›¾åƒä¸­ç¼ºå¤±æ¨¡æ€çš„æƒ…å†µä¸‹çš„è„‘è‚¿ç˜¤åˆ†å‰²é—®é¢˜ã€‚è¯¥æ–¹æ³•ç»“åˆäº†é¢†åŸŸå¢é‡å­¦ä¹ ï¼ˆDILï¼‰å’Œè·¨æ‚£è€…è¶…å›¾åˆ†å‰²ç½‘ç»œï¼ˆCHSNetï¼‰ï¼Œä»¥åº”å¯¹ç¼ºå¤±æ¨¡æ€å¯¼è‡´çš„è®­ç»ƒé—®é¢˜ã€‚åŒæ—¶ï¼Œé€šè¿‡å¼•å…¥Tverskyæ„è¯†å¯¹æ¯”æŸå¤±ï¼ˆTACæŸå¤±ï¼‰ï¼Œæé«˜è·¨ä¸åŒæ¨¡æ€çš„ä¿¡æ¯å¹³è¡¡æ€§ã€‚åœ¨BraTS2019æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒReHyDILè¾ƒå…¶ä»–å‰æ²¿æ–¹æ³•æœ‰æ›´å‡ºè‰²çš„è¡¨ç°ï¼Œåœ¨Diceç›¸ä¼¼ç³»æ•°ä¸Šæé«˜äº†è¶…è¿‡2%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ReHyDILæ–¹æ³•ç»“åˆäº†é¢†åŸŸå¢é‡å­¦ä¹ ï¼ˆDILï¼‰æ¥å¤„ç†MRIå›¾åƒä¸­ç¼ºå¤±æ¨¡æ€çš„é—®é¢˜ã€‚</li>
<li>CHSNetç½‘ç»œç”¨äºå¢å¼ºåˆ†å‰²æ€§èƒ½ï¼Œé€‚åº”ä¸åŒçš„æ‚£è€…åœºæ™¯ã€‚</li>
<li>ReHyDILå¼•å…¥TACæŸå¤±ä»¥ç¼“è§£ä¸åŒæ¨¡æ€ä¹‹é—´çš„ä¿¡æ¯ä¸å¹³è¡¡é—®é¢˜ã€‚</li>
<li>ç›¸æ¯”ç°æœ‰æ–¹æ³•ï¼ŒReHyDILåœ¨BraTS2019æ•°æ®é›†ä¸Šçš„è¡¨ç°æ›´ä¼˜ï¼ŒDiceç›¸ä¼¼ç³»æ•°æé«˜äº†è¶…è¿‡2%ã€‚</li>
<li>ReHyDILèƒ½å¤Ÿé€‚åº”æ–°è·å–çš„MRIæ¨¡æ€ï¼ŒåŒæ—¶ä¿ç•™ä¹‹å‰å­¦ä¹ çš„çŸ¥è¯†ã€‚</li>
<li>ReHyDILé€šè¿‡åˆ©ç”¨è¶…å›¾æ•æ‰æ‚£è€…ä¹‹é—´çš„é«˜é˜¶å…³è”ï¼Œæé«˜äº†åˆ†å‰²å‡†ç¡®æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•çš„ä»£ç å·²å…¬å¼€å¯ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16809">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9c8c0b8ff88aafcbc359dfd117fe56fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c63fdee1662522d21c25650965323e3f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e12711a8c69205b0682e3484e6940b2.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Point-Detect-Count-Multi-Task-Medical-Image-Understanding-with-Instruction-Tuned-Vision-Language-Models"><a href="#Point-Detect-Count-Multi-Task-Medical-Image-Understanding-with-Instruction-Tuned-Vision-Language-Models" class="headerlink" title="Point, Detect, Count: Multi-Task Medical Image Understanding with   Instruction-Tuned Vision-Language Models"></a>Point, Detect, Count: Multi-Task Medical Image Understanding with   Instruction-Tuned Vision-Language Models</h2><p><strong>Authors:Sushant Gautam, Michael A. Riegler, PÃ¥l Halvorsen</strong></p>
<p>We investigate fine-tuning Vision-Language Models (VLMs) for multi-task medical image understanding, focusing on detection, localization, and counting of findings in medical images. Our objective is to evaluate whether instruction-tuned VLMs can simultaneously improve these tasks, with the goal of enhancing diagnostic accuracy and efficiency. Using MedMultiPoints, a multimodal dataset with annotations from endoscopy (polyps and instruments) and microscopy (sperm cells), we reformulate each task into instruction-based prompts suitable for vision-language reasoning. We fine-tune Qwen2.5-VL-7B-Instruct using Low-Rank Adaptation (LoRA) across multiple task combinations. Results show that multi-task training improves robustness and accuracy. For example, it reduces the Count Mean Absolute Error (MAE) and increases Matching Accuracy in the Counting + Pointing task. However, trade-offs emerge, such as more zero-case point predictions, indicating reduced reliability in edge cases despite overall performance gains. Our study highlights the potential of adapting general-purpose VLMs to specialized medical tasks via prompt-driven fine-tuning. This approach mirrors clinical workflows, where radiologists simultaneously localize, count, and describe findings - demonstrating how VLMs can learn composite diagnostic reasoning patterns. The model produces interpretable, structured outputs, offering a promising step toward explainable and versatile medical AI. Code, model weights, and scripts will be released for reproducibility at <a target="_blank" rel="noopener" href="https://github.com/simula/PointDetectCount">https://github.com/simula/PointDetectCount</a>. </p>
<blockquote>
<p>æˆ‘ä»¬ç ”ç©¶äº†é’ˆå¯¹å¤šä»»åŠ¡åŒ»å­¦å½±åƒç†è§£çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å¾®è°ƒæŠ€æœ¯ï¼Œé‡ç‚¹èšç„¦äºåŒ»å­¦å½±åƒä¸­çš„æ£€æµ‹ç»“æœè¯†åˆ«ã€å®šä½ä»¥åŠè®¡æ•°ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯è¯„ä¼°æŒ‡ä»¤è°ƒæ•´åçš„VLMsæ˜¯å¦èƒ½å¤ŸåŒæ—¶æ”¹è¿›è¿™äº›ä»»åŠ¡ï¼Œæ—¨åœ¨æé«˜è¯Šæ–­å’Œæ•ˆç‡çš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬ä½¿ç”¨MedMultiPointsè¿™ä¸€åŒ…å«å†…çª¥é•œï¼ˆæ¯è‚‰å’Œä»ªå™¨ï¼‰å’Œæ˜¾å¾®é•œï¼ˆç²¾å­ç»†èƒï¼‰æ³¨é‡Šçš„å¤šæ¨¡å¼æ•°æ®é›†ï¼Œå°†æ¯ä¸ªä»»åŠ¡é‡æ–°åˆ¶å®šä¸ºé€‚åº”è§†è§‰è¯­è¨€æ¨ç†çš„æŒ‡ä»¤æç¤ºã€‚æˆ‘ä»¬åˆ©ç”¨ä½ç§©è‡ªé€‚åº”ï¼ˆLoRAï¼‰æŠ€æœ¯å¾®è°ƒQwen2.5-VL-7B-Instructæ¨¡å‹ï¼Œé€‚åº”å¤šç§ä»»åŠ¡ç»„åˆã€‚ç»“æœæ˜¾ç¤ºï¼Œå¤šä»»åŠ¡è®­ç»ƒæé«˜äº†ç¨³å¥æ€§å’Œå‡†ç¡®æ€§ã€‚ä¾‹å¦‚ï¼Œå®ƒå‡å°‘äº†è®¡æ•°å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰ï¼Œå¹¶æé«˜äº†è®¡æ•°+å®šä½ä»»åŠ¡çš„åŒ¹é…å‡†ç¡®ç‡ã€‚ç„¶è€Œï¼Œä¹Ÿå‡ºç°äº†ä¸€äº›æƒè¡¡æƒ…å†µï¼Œæ¯”å¦‚æ›´å¤šçš„é›¶æ¡ˆä¾‹ç‚¹é¢„æµ‹ï¼Œè¿™è¡¨æ˜åœ¨æ•´ä½“æ€§èƒ½æå‡çš„åŒæ—¶ï¼Œè¾¹ç¼˜æ¡ˆä¾‹çš„å¯é æ€§æœ‰æ‰€é™ä½ã€‚æˆ‘ä»¬çš„ç ”ç©¶çªå‡ºäº†é€šè¿‡æç¤ºé©±åŠ¨å¾®è°ƒå°†é€šç”¨VLMsé€‚åº”ä¸“ä¸šåŒ»ç–—ä»»åŠ¡çš„æ½œåŠ›ã€‚è¿™ç§æ–¹æ³•åæ˜ äº†ä¸´åºŠå·¥ä½œæµç¨‹ï¼Œæ”¾å°„ç§‘åŒ»ç”ŸåŒæ—¶å®šä½ã€è®¡æ•°å’Œæè¿°å‘ç°ç»“æœï¼Œå±•ç¤ºäº†VLMså¦‚ä½•å­¦ä¹ å¤åˆè¯Šæ–­æ¨ç†æ¨¡å¼ã€‚è¯¥æ¨¡å‹äº§ç”Ÿå¯è§£é‡Šçš„ç»“æ„åŒ–è¾“å‡ºï¼Œä¸ºå‘å¯è§£é‡Šå’Œé€šç”¨åŒ»ç–—äººå·¥æ™ºèƒ½è¿ˆå‡ºäº†ä¸€æ­¥ï¼Œå…·æœ‰å¹¿é˜”çš„å‘å±•å‰æ™¯ã€‚ç›¸å…³ä»£ç ã€æ¨¡å‹æƒé‡å’Œè„šæœ¬å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/simula/PointDetectCount%E4%B8%8A%E5%8F%91%E5%B8%83%EF%BC%8C%E4%BB%A5%E4%BF%83%E8%BF%9B%E7%A0%94%E7%A9%B6%E7%9A%84%E5%8F%AF%E9%87%8D%E5%A4%8D%E6%80%A7%E3%80%82">https://github.com/simula/PointDetectCountä¸Šå‘å¸ƒï¼Œä»¥ä¿ƒè¿›ç ”ç©¶çš„å¯é‡å¤æ€§ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16647v1">PDF</a> Accepted as a full paper at the 38th IEEE International Symposium on   Computer-Based Medical Systems (CBMS) 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢ç´¢äº†é’ˆå¯¹å¤šä»»åŠ¡åŒ»å­¦å›¾åƒç†è§£çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„å¾®è°ƒç ”ç©¶ã€‚æ–‡ç« é‡ç‚¹ä»‹ç»äº†å¦‚ä½•ä½¿ç”¨MedMultiPointsæ•°æ®é›†å¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œæ—¨åœ¨é€šè¿‡å¤šä»»åŠ¡è®­ç»ƒæé«˜æ¨¡å‹çš„ç¨³å¥æ€§å’Œå‡†ç¡®æ€§ï¼Œæ—¨åœ¨æé«˜è¯Šæ–­çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚é€šè¿‡æŒ‡ä»¤å¾®è°ƒVLMsï¼Œæ–‡ç« å±•ç¤ºäº†ä¸€ç§å¯èƒ½çš„æ–¹æ³•æ¥é€‚åº”ç‰¹æ®ŠåŒ»å­¦ä»»åŠ¡ï¼Œè¿™ç§æ–¹æ³•èƒ½å¤Ÿæ¨¡æ‹ŸåŒ»ç”ŸåŒæ—¶å®šä½ã€è®¡æ•°å’Œæè¿°ç—…ç¶ç‚¹çš„ä¸´åºŠå·¥ä½œæµç¨‹ã€‚æ­¤ç ”ç©¶å±•ç¤ºäº†åˆ©ç”¨ç»“æ„åŒ–è¾“å‡ºè¿›è¡Œè§£é‡Šçš„æ½œåŠ›ï¼Œå¯¹äºæ¨åŠ¨åŒ»å­¦äººå·¥æ™ºèƒ½çš„é€æ˜æ€§å’Œé€šç”¨æ€§å…·æœ‰é‡è¦æ„ä¹‰ã€‚æ›´å¤šç»†èŠ‚å’Œæ•°æ®å°†åœ¨ç›¸å…³ä»£ç å’Œæ¨¡å‹æƒé‡ä¸­å…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç ”ç©¶å…³æ³¨å¤šä»»åŠ¡åŒ»å­¦å›¾åƒç†è§£ï¼Œç‰¹åˆ«æ˜¯æ£€æµ‹ã€å®šä½å’Œè®¡æ•°ä»»åŠ¡ã€‚</li>
<li>ä½¿ç”¨MedMultiPointsæ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œæ—¨åœ¨æé«˜è¯Šæ–­å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚</li>
<li>é€šè¿‡æŒ‡ä»¤å¾®è°ƒVLMsä»¥é€‚åº”ç‰¹æ®ŠåŒ»å­¦ä»»åŠ¡ï¼Œå±•ç¤ºäº†ä¸€ç§å¯èƒ½çš„æ–¹æ³•ã€‚</li>
<li>å¤šä»»åŠ¡è®­ç»ƒèƒ½æé«˜æ¨¡å‹çš„ç¨³å¥æ€§å’Œå‡†ç¡®æ€§ã€‚ä¾‹å¦‚ï¼Œåœ¨è®¡æ•°å’ŒæŒ‡å‘ä»»åŠ¡ä¸­å‡å°‘äº†å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰ï¼Œæé«˜äº†åŒ¹é…å‡†ç¡®ç‡ã€‚</li>
<li>ç ”ç©¶å‘ç°å­˜åœ¨æƒè¡¡é—®é¢˜ï¼Œå¦‚é›¶æ¡ˆä¾‹ç‚¹é¢„æµ‹å¢å¤šï¼Œè¾¹ç¼˜æƒ…å†µå¯é æ€§é™ä½ã€‚</li>
<li>æ¨¡å‹æ¨¡æ‹ŸåŒ»ç”Ÿä¸´åºŠå·¥ä½œæµç¨‹ï¼Œå±•ç¤ºå­¦ä¹ å¤åˆè¯Šæ–­æ¨ç†æ¨¡å¼çš„æ½œåŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16647">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-bb2c99815c192704a3f055f58582fd18.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3aec9cd89c3d9eff3027bd6f9c0b4113.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4147ce1c6eedaad57f24d207c7349a2f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5d18b3daedcd78a2d97fcf57b8b6107a.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Background-Matters-A-Cross-view-Bidirectional-Modeling-Framework-for-Semi-supervised-Medical-Image-Segmentation"><a href="#Background-Matters-A-Cross-view-Bidirectional-Modeling-Framework-for-Semi-supervised-Medical-Image-Segmentation" class="headerlink" title="Background Matters: A Cross-view Bidirectional Modeling Framework for   Semi-supervised Medical Image Segmentation"></a>Background Matters: A Cross-view Bidirectional Modeling Framework for   Semi-supervised Medical Image Segmentation</h2><p><strong>Authors:Luyang Cao, Jianwei Li, Yinghuan Shi</strong></p>
<p>Semi-supervised medical image segmentation (SSMIS) leverages unlabeled data to reduce reliance on manually annotated images. However, current SOTA approaches predominantly focus on foreground-oriented modeling (i.e., segmenting only the foreground region) and have largely overlooked the potential benefits of explicitly modeling the background region. Our study theoretically and empirically demonstrates that highly certain predictions in background modeling enhance the confidence of corresponding foreground modeling. Building on this insight, we propose the Cross-view Bidirectional Modeling (CVBM) framework, which introduces a novel perspective by incorporating background modeling to improve foreground modeling performance. Within CVBM, background modeling serves as an auxiliary perspective, providing complementary supervisory signals to enhance the confidence of the foreground model. Additionally, CVBM introduces an innovative bidirectional consistency mechanism, which ensures mutual alignment between foreground predictions and background-guided predictions. Extensive experiments demonstrate that our approach achieves SOTA performance on the LA, Pancreas, ACDC, and HRF datasets. Notably, on the Pancreas dataset, CVBM outperforms fully supervised methods (i.e., DSC: 84.57% vs. 83.89%) while utilizing only 20% of the labeled data. Our code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/caoluyang0830/CVBM.git">https://github.com/caoluyang0830/CVBM.git</a>. </p>
<blockquote>
<p>åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²ï¼ˆSSMISï¼‰åˆ©ç”¨æ— æ ‡ç­¾æ•°æ®å‡å°‘äº†å¯¹æ‰‹åŠ¨æ ‡æ³¨å›¾åƒçš„ä¾èµ–ã€‚ç„¶è€Œï¼Œå½“å‰çš„æœ€å…ˆè¿›æ–¹æ³•ä¸»è¦å…³æ³¨å‰æ™¯å¯¼å‘å»ºæ¨¡ï¼ˆå³åªåˆ†å‰²å‰æ™¯åŒºåŸŸï¼‰ï¼Œå¹¶å¤§å¤šå¿½ç•¥äº†æ˜¾å¼å»ºæ¨¡èƒŒæ™¯åŒºåŸŸçš„æ½œåœ¨ä¼˜åŠ¿ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä»ç†è®ºå’Œå®è¯ä¸Šè¯æ˜ï¼ŒèƒŒæ™¯å»ºæ¨¡ä¸­çš„é«˜ç¡®å®šæ€§é¢„æµ‹å¢å¼ºäº†å¯¹åº”å‰æ™¯å»ºæ¨¡çš„ç½®ä¿¡åº¦ã€‚åŸºäºè¿™ä¸€è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†è·¨è§†å›¾åŒå‘å»ºæ¨¡ï¼ˆCVBMï¼‰æ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥èƒŒæ™¯å»ºæ¨¡æ¥æ”¹å–„å‰æ™¯å»ºæ¨¡æ€§èƒ½ï¼Œæä¾›äº†ä¸€ç§æ–°é¢–çš„è§†è§’ã€‚åœ¨CVBMä¸­ï¼ŒèƒŒæ™¯å»ºæ¨¡ä½œä¸ºä¸€ä¸ªè¾…åŠ©è§†è§’ï¼Œæä¾›è¡¥å……ç›‘ç£ä¿¡å·ï¼Œå¢å¼ºå‰æ™¯æ¨¡å‹çš„ç½®ä¿¡åº¦ã€‚æ­¤å¤–ï¼ŒCVBMå¼•å…¥äº†ä¸€ç§åˆ›æ–°çš„åŒå‘ä¸€è‡´æ€§æœºåˆ¶ï¼Œç¡®ä¿å‰æ™¯é¢„æµ‹å’ŒèƒŒæ™¯å¼•å¯¼é¢„æµ‹ä¹‹é—´çš„ç›¸äº’å¯¹é½ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨LAã€èƒ°è…ºã€ACDCå’ŒHRFæ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å°¤å…¶åœ¨èƒ°è…ºæ•°æ®é›†ä¸Šï¼ŒCVBMåœ¨ä»…ä½¿ç”¨20%æ ‡è®°æ•°æ®çš„æƒ…å†µä¸‹ï¼Œè¶…è¶Šäº†å…¨ç›‘ç£æ–¹æ³•ï¼ˆDSCï¼š84.57%å¯¹83.89%ï¼‰ã€‚æˆ‘ä»¬çš„ä»£ç å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/caoluyang0830/CVBM.git%E3%80%82">https://github.com/caoluyang0830/CVBM.gitã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16625v1">PDF</a> Accepted by IEEE Transactions on Image Processing</p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„åŠç›‘ç£å­¦ä¹ åˆ©ç”¨æœªæ ‡è®°æ•°æ®å‡å°‘å¯¹æ‰‹åŠ¨æ ‡æ³¨å›¾åƒçš„ä¾èµ–ã€‚ç„¶è€Œï¼Œç°æœ‰æŠ€æœ¯ä¸»è¦é›†ä¸­åœ¨å‰æ™¯å»ºæ¨¡ä¸Šï¼Œå¿½è§†äº†èƒŒæ™¯å»ºæ¨¡çš„æ½œåœ¨ä¼˜åŠ¿ã€‚ç ”ç©¶è¯æ˜èƒŒæ™¯å»ºæ¨¡çš„ç¡®å®šæ€§é¢„æµ‹å¯ä»¥æé«˜å‰æ™¯å»ºæ¨¡çš„ç½®ä¿¡åº¦ã€‚åŸºäºæ­¤ï¼Œæå‡ºCross-view Bidirectional Modelingï¼ˆCVBMï¼‰æ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥èƒŒæ™¯å»ºæ¨¡æ”¹å–„å‰æ™¯å»ºæ¨¡æ€§èƒ½ã€‚CVBMåˆ©ç”¨èƒŒæ™¯å»ºæ¨¡ä½œä¸ºè¾…åŠ©è§†è§’ï¼Œæä¾›é¢å¤–çš„ç›‘ç£ä¿¡å·ï¼ŒåŒæ—¶å¼•å…¥åŒå‘ä¸€è‡´æ€§æœºåˆ¶ç¡®ä¿é¢„æµ‹é—´çš„ç›¸äº’å¯¹é½ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°æœ€ä½³æ€§èƒ½ï¼Œå°¤å…¶åœ¨èƒ°è…ºæ•°æ®é›†ä¸Šï¼ŒCVBMåœ¨ä»…ä½¿ç”¨20%æ ‡è®°æ•°æ®çš„æƒ…å†µä¸‹ä¾¿è¶…è¶Šå…¨ç›‘ç£æ–¹æ³•ï¼ˆDSCï¼š84.57% vs. 83.89%ï¼‰ã€‚ä»£ç å·²å…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>SSMISåˆ©ç”¨æœªæ ‡è®°æ•°æ®å‡å°‘æ ‡æ³¨å›¾åƒçš„ä¾èµ–ã€‚</li>
<li>å½“å‰æ–¹æ³•ä¸»è¦å…³æ³¨å‰æ™¯å»ºæ¨¡ï¼Œå¿½ç•¥èƒŒæ™¯å»ºæ¨¡çš„æ½œåœ¨ä¼˜åŠ¿ã€‚</li>
<li>èƒŒæ™¯å»ºæ¨¡çš„ç¡®å®šæ€§é¢„æµ‹èƒ½æé«˜å‰æ™¯å»ºæ¨¡çš„ç½®ä¿¡åº¦ã€‚</li>
<li>æå‡ºCVBMæ¡†æ¶ç»“åˆèƒŒæ™¯å»ºæ¨¡æ”¹å–„å‰æ™¯å»ºæ¨¡æ€§èƒ½ã€‚</li>
<li>CVBMåˆ©ç”¨èƒŒæ™¯å»ºæ¨¡ä½œä¸ºè¾…åŠ©è§†è§’ï¼Œæä¾›é¢å¤–çš„ç›‘ç£ä¿¡å·ã€‚</li>
<li>åŒå‘ä¸€è‡´æ€§æœºåˆ¶ç¡®ä¿å‰æ™¯å’ŒèƒŒæ™¯é¢„æµ‹é—´çš„ç›¸äº’å¯¹é½ã€‚</li>
<li>å®éªŒè¡¨æ˜CVBMåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°æœ€ä½³æ€§èƒ½ï¼Œå°¤å…¶åœ¨èƒ°è…ºæ•°æ®é›†ä¸Šè¡¨ç°çªå‡ºã€‚</li>
<li>ä»…ä½¿ç”¨å°‘é‡æ ‡è®°æ•°æ®ä¾¿è¶…è¶Šå…¨ç›‘ç£æ–¹æ³•ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16625">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-abf5142b412c3b4d3e3763f0d2e7e219.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b2bb50e06a7abdb8d7479bfcebacc24b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-410a8cdf9b796aef65878f1b65268930.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b6268f111e5e12f8499d4f452b63cbf6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b6f4567e404187ab38de7bc9e5d7246f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-932b097f5b71af40efba7f6b750fa1fd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fb889e687e541fd2e26c32b9751ec1fc.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Grounding-Chest-X-Ray-Visual-Question-Answering-with-Generated-Radiology-Reports"><a href="#Grounding-Chest-X-Ray-Visual-Question-Answering-with-Generated-Radiology-Reports" class="headerlink" title="Grounding Chest X-Ray Visual Question Answering with Generated Radiology   Reports"></a>Grounding Chest X-Ray Visual Question Answering with Generated Radiology   Reports</h2><p><strong>Authors:Francesco Dalla Serra, Patrick Schrempf, Chaoyang Wang, Zaiqiao Meng, Fani Deligianni, Alison Q. Oâ€™Neil</strong></p>
<p>We present a novel approach to Chest X-ray (CXR) Visual Question Answering (VQA), addressing both single-image image-difference questions. Single-image questions focus on abnormalities within a specific CXR (â€œWhat abnormalities are seen in image X?â€), while image-difference questions compare two longitudinal CXRs acquired at different time points (â€œWhat are the differences between image X and Y?â€). We further explore how the integration of radiology reports can enhance the performance of VQA models. While previous approaches have demonstrated the utility of radiology reports during the pre-training phase, we extend this idea by showing that the reports can also be leveraged as additional input to improve the VQA modelâ€™s predicted answers. First, we propose a unified method that handles both types of questions and auto-regressively generates the answers. For single-image questions, the model is provided with a single CXR. For image-difference questions, the model is provided with two CXRs from the same patient, captured at different time points, enabling the model to detect and describe temporal changes. Taking inspiration from â€˜Chain-of-Thought reasoningâ€™, we demonstrate that performance on the CXR VQA task can be improved by grounding the answer generator module with a radiology report predicted for the same CXR. In our approach, the VQA model is divided into two steps: i) Report Generation (RG) and ii) Answer Generation (AG). Our results demonstrate that incorporating predicted radiology reports as evidence to the AG model enhances performance on both single-image and image-difference questions, achieving state-of-the-art results on the Medical-Diff-VQA dataset. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§é’ˆå¯¹èƒ¸éƒ¨Xå…‰ï¼ˆCXRï¼‰è§†è§‰é—®ç­”ï¼ˆVQAï¼‰çš„æ–°å‹æ–¹æ³•ï¼Œè¯¥æ–¹æ³•è§£å†³äº†å•å›¾åƒå›¾åƒå·®å¼‚é—®é¢˜ã€‚å•å›¾åƒé—®é¢˜ä¾§é‡äºCXRå†…çš„å¼‚å¸¸ç°è±¡ï¼ˆâ€œå›¾åƒXä¸­çœ‹åˆ°äº†å“ªäº›å¼‚å¸¸ï¼Ÿâ€ï¼‰ï¼Œè€Œå›¾åƒå·®å¼‚é—®é¢˜åˆ™æ¯”è¾ƒåœ¨ä¸åŒæ—¶é—´ç‚¹é‡‡é›†çš„ä¸¤ä¸ªçºµå‘CXRï¼ˆâ€œå›¾åƒXå’ŒYä¹‹é—´çš„å·®å¼‚æ˜¯ä»€ä¹ˆï¼Ÿâ€ï¼‰ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æ¢è®¨äº†é›†æˆæ”¾å°„å­¦æŠ¥å‘Šå¦‚ä½•å¢å¼ºVQAæ¨¡å‹çš„è¡¨ç°ã€‚è™½ç„¶ä»¥å‰çš„æ–¹æ³•åœ¨é¢„è®­ç»ƒé˜¶æ®µå±•ç¤ºäº†æ”¾å°„å­¦æŠ¥å‘Šçš„æœ‰ç”¨æ€§ï¼Œä½†æˆ‘ä»¬é€šè¿‡å±•ç¤ºæŠ¥å‘Šè¿˜å¯ä»¥ä½œä¸ºé¢å¤–è¾“å…¥æ¥æ”¹å–„VQAæ¨¡å‹çš„é¢„æµ‹ç­”æ¡ˆï¼Œä»è€Œæ‰©å±•äº†è¿™ä¸€æƒ³æ³•ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥å¤„ç†è¿™ä¸¤ç§ç±»å‹çš„é—®é¢˜ï¼Œå¹¶è‡ªåŠ¨ç”Ÿæˆç­”æ¡ˆã€‚å¯¹äºå•å›¾åƒé—®é¢˜ï¼Œæ¨¡å‹ä¼šæ¥æ”¶ä¸€å¼ CXRã€‚å¯¹äºå›¾åƒå·®å¼‚é—®é¢˜ï¼Œæ¨¡å‹ä¼šæ¥æ”¶æ¥è‡ªåŒä¸€æ‚£è€…åœ¨ä¸åŒæ—¶é—´ç‚¹æ•è·çš„ä¸¤å¼ CXRï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ£€æµ‹å’Œæè¿°æ—¶é—´å˜åŒ–ã€‚æˆ‘ä»¬ä»â€œæ€ç»´é“¾æ¨ç†â€ä¸­æ±²å–çµæ„Ÿï¼Œè¯æ˜é€šè¿‡æŠŠç­”æ¡ˆç”Ÿæˆæ¨¡å—å»ºç«‹åœ¨ä¸ºåŒä¸€CXRé¢„æµ‹çš„æ”¾å°„å­¦æŠ¥å‘Šä¸Šï¼Œå¯ä»¥æ”¹å–„CXR VQAä»»åŠ¡çš„è¡¨ç°ã€‚åœ¨æˆ‘ä»¬çš„æ–¹æ³•ä¸­ï¼ŒVQAæ¨¡å‹åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šiï¼‰æŠ¥å‘Šç”Ÿæˆï¼ˆRGï¼‰å’Œiiï¼‰ç­”æ¡ˆç”Ÿæˆï¼ˆAGï¼‰ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå°†é¢„æµ‹çš„æ”¾å°„å­¦æŠ¥å‘Šä½œä¸ºè¯æ®èå…¥AGæ¨¡å‹ï¼Œæé«˜äº†åœ¨å•å›¾åƒå’Œå›¾åƒå·®å¼‚é—®é¢˜ä¸Šçš„æ€§èƒ½ï¼Œåœ¨Medical-Diff-VQAæ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16624v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é’ˆå¯¹èƒ¸éƒ¨Xå…‰ï¼ˆCXRï¼‰è§†è§‰é—®ç­”ï¼ˆVQAï¼‰çš„æ–°å‹æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿå¤„ç†å•å›¾åƒå·®å¼‚é—®é¢˜ã€‚å•å›¾åƒé—®é¢˜å…³æ³¨ç‰¹å®šCXRä¸­çš„å¼‚å¸¸ï¼Œè€Œå›¾åƒå·®å¼‚é—®é¢˜åˆ™æ¯”è¾ƒåœ¨ä¸åŒæ—¶é—´ç‚¹é‡‡é›†çš„ä¸¤ä¸ªçºµå‘CXRsä¹‹é—´çš„å·®å¼‚ã€‚æ–‡ç« è¿˜æ¢è®¨äº†å°†æ”¾å°„å­¦æŠ¥å‘Šèå…¥VQAæ¨¡å‹çš„æ–¹å¼ï¼Œä¸ä»…å¯ä»¥ç”¨äºé¢„è®­ç»ƒé˜¶æ®µï¼Œè¿˜å¯ä»¥ä½œä¸ºé¢å¤–è¾“å…¥æ¥æ”¹å–„æ¨¡å‹çš„é¢„æµ‹ç­”æ¡ˆã€‚é€šè¿‡ç»Ÿä¸€çš„å¤„ç†æ–¹æ³•ï¼Œæ¨¡å‹å¯ä»¥å¤„ç†ä¸¤ç§ç±»å‹çš„é—®é¢˜ï¼Œå¹¶è‡ªåŠ¨ç”Ÿæˆç­”æ¡ˆã€‚è¯¥æ–¹æ³•åˆ†ä¸ºæŠ¥å‘Šç”Ÿæˆå’Œç­”æ¡ˆç”Ÿæˆä¸¤ä¸ªæ­¥éª¤ï¼Œé€šè¿‡å°†é¢„æµ‹çš„æ”¾å°„å­¦æŠ¥å‘Šä½œä¸ºè¯æ®èå…¥ç­”æ¡ˆç”Ÿæˆæ¨¡å‹ï¼Œæé«˜äº†åœ¨å•å›¾åƒå’Œå›¾åƒå·®å¼‚é—®é¢˜ä¸Šçš„æ€§èƒ½ï¼Œå®ç°äº†åœ¨Medical-Diff-VQAæ•°æ®é›†ä¸Šçš„æœ€æ–°ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†å¤„ç†èƒ¸éƒ¨Xå…‰è§†è§‰é—®ç­”çš„æ–°å‹æ–¹æ³•ï¼Œæ¶µç›–å•å›¾åƒé—®é¢˜å’Œå›¾åƒå·®å¼‚é—®é¢˜ã€‚</li>
<li>æ¢è®¨äº†å°†æ”¾å°„å­¦æŠ¥å‘Šèå…¥VQAæ¨¡å‹çš„å¤šç§æ–¹å¼ï¼ŒåŒ…æ‹¬é¢„è®­ç»ƒå’Œä½œä¸ºé¢å¤–è¾“å…¥æ”¹å–„é¢„æµ‹ç­”æ¡ˆã€‚</li>
<li>é‡‡ç”¨ç»Ÿä¸€æ–¹æ³•å¤„ç†ä¸¤ç§ç±»å‹çš„é—®é¢˜ï¼Œå¹¶è‡ªåŠ¨ç”Ÿæˆç­”æ¡ˆã€‚</li>
<li>æ¨¡å‹åˆ†ä¸ºæŠ¥å‘Šç”Ÿæˆå’Œç­”æ¡ˆç”Ÿæˆä¸¤ä¸ªæ­¥éª¤ã€‚</li>
<li>é¢„æµ‹çš„æ”¾å°„å­¦æŠ¥å‘Šä½œä¸ºè¯æ®èå…¥ç­”æ¡ˆç”Ÿæˆæ¨¡å‹ï¼Œæé«˜åœ¨å•å›¾åƒå’Œå›¾åƒå·®å¼‚é—®é¢˜ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨Medical-Diff-VQAæ•°æ®é›†ä¸Šå®ç°äº†æœ€æ–°ç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16624">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4686fad44d4ed9b29375e021d84df535.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fce83f41e45d732ef807e385ecafa2d3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a627dcba64b5d2b3e88b6318a38e7c76.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-84797dce37a2943581b89e29b5142faf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-54ce69e9ca5762c245682d7fd06ee568.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b3079d48cfa8f1616be649063c93d69f.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Auto-nnU-Net-Towards-Automated-Medical-Image-Segmentation"><a href="#Auto-nnU-Net-Towards-Automated-Medical-Image-Segmentation" class="headerlink" title="Auto-nnU-Net: Towards Automated Medical Image Segmentation"></a>Auto-nnU-Net: Towards Automated Medical Image Segmentation</h2><p><strong>Authors:Jannis Becktepe, Leona Hennig, Steffen Oeltze-Jafra, Marius Lindauer</strong></p>
<p>Medical Image Segmentation (MIS) includes diverse tasks, from bone to organ segmentation, each with its own challenges in finding the best segmentation model. The state-of-the-art AutoML-related MIS-framework nnU-Net automates many aspects of model configuration but remains constrained by fixed hyperparameters and heuristic design choices. As a full-AutoML framework for MIS, we propose Auto-nnU-Net, a novel nnU-Net variant enabling hyperparameter optimization (HPO), neural architecture search (NAS), and hierarchical NAS (HNAS). Additionally, we propose Regularized PriorBand to balance model accuracy with the computational resources required for training, addressing the resource constraints often faced in real-world medical settings that limit the feasibility of extensive training procedures. We evaluate our approach across diverse MIS datasets from the well-established Medical Segmentation Decathlon, analyzing the impact of AutoML techniques on segmentation performance, computational efficiency, and model design choices. The results demonstrate that our AutoML approach substantially improves the segmentation performance of nnU-Net on 6 out of 10 datasets and is on par on the other datasets while maintaining practical resource requirements. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/LUH-AI/AutonnUNet">https://github.com/LUH-AI/AutonnUNet</a>. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²ï¼ˆMISï¼‰æ¶µç›–äº†ä»éª¨éª¼åˆ°å™¨å®˜åˆ†å‰²ç­‰å¤šæ ·åŒ–ä»»åŠ¡ï¼Œåœ¨å¯»æ‰¾æœ€ä½³åˆ†å‰²æ¨¡å‹æ—¶æ¯ä¸ªä»»åŠ¡éƒ½é¢ä¸´è‡ªå·±çš„æŒ‘æˆ˜ã€‚æœ€å…ˆè¿›çš„ä¸AutoMLç›¸å…³çš„MISæ¡†æ¶nnU-Netèƒ½å¤Ÿè‡ªåŠ¨åŒ–æ¨¡å‹é…ç½®çš„è®¸å¤šæ–¹é¢ï¼Œä½†ä»å—åˆ°å›ºå®šè¶…å‚æ•°å’Œå¯å‘å¼è®¾è®¡é€‰æ‹©çš„é™åˆ¶ã€‚ä½œä¸ºMISçš„å…¨è‡ªåŠ¨MLæ¡†æ¶ï¼Œæˆ‘ä»¬æå‡ºäº†Auto-nnU-Netï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„nnU-Netå˜ä½“ï¼Œèƒ½å¤Ÿå®ç°è¶…å‚æ•°ä¼˜åŒ–ï¼ˆHPOï¼‰ã€ç¥ç»ç½‘ç»œæ¶æ„æœç´¢ï¼ˆNASï¼‰å’Œåˆ†å±‚NASï¼ˆHNASï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†æ­£åˆ™åŒ–PriorBandï¼Œä»¥å¹³è¡¡æ¨¡å‹ç²¾åº¦ä¸è®­ç»ƒæ‰€éœ€çš„è®¡ç®—èµ„æºï¼Œè§£å†³ç°å®åŒ»ç–—ç¯å¢ƒä¸­ç»å¸¸é¢ä¸´çš„èµ„æºçº¦æŸé—®é¢˜ï¼Œè¿™äº›èµ„æºçº¦æŸé™åˆ¶äº†å¹¿æ³›è®­ç»ƒç¨‹åºçš„å¯è¡Œæ€§ã€‚æˆ‘ä»¬åœ¨ç»è¿‡è‰¯å¥½éªŒè¯çš„åŒ»å­¦åˆ†å‰²åé¡¹å…¨èƒ½èµ›ä¸­çš„å¤šæ ·åŒ–MISæ•°æ®é›†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œåˆ†æäº†AutoMLæŠ€æœ¯å¯¹åˆ†å‰²æ€§èƒ½ã€è®¡ç®—æ•ˆç‡å’Œæ¨¡å‹è®¾è®¡é€‰æ‹©çš„å½±å“ã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„AutoMLæ–¹æ³•åœ¨6ä¸ªæ•°æ®é›†ä¸Šæ˜¾è‘—æé«˜äº†nnU-Netçš„åˆ†å‰²æ€§èƒ½ï¼Œåœ¨å…¶ä»–æ•°æ®é›†ä¸Šè¡¨ç°ç›¸å½“ï¼ŒåŒæ—¶ä¿æŒäº†å®é™…çš„èµ„æºéœ€æ±‚ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨[<a target="_blank" rel="noopener" href="https://github.com/LUH-AI/AutonnUNet%E8%8E%B7%E5%8F%96%E3%80%82]">https://github.com/LUH-AI/AutonnUNetè·å–ã€‚]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16561v1">PDF</a> 31 pages, 19 figures. Accepted for publication at AutoML 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬ç ”ç©¶é’ˆå¯¹åŒ»ç–—å›¾åƒåˆ†å‰²ï¼ˆMISï¼‰ä¸­çš„å¤šæ ·ä»»åŠ¡æŒ‘æˆ˜ï¼Œæå‡ºäº†Auto-nnU-Netï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨è‡ªåŠ¨çš„MISæ¡†æ¶ï¼Œæ”¹è¿›äº†ç°æœ‰çš„nnU-Netæ¨¡å‹ã€‚æ–°æ¡†æ¶å¼•å…¥è¶…å‚æ•°ä¼˜åŒ–ï¼ˆHPOï¼‰ã€ç¥ç»ç½‘ç»œæ¶æ„æœç´¢ï¼ˆNASï¼‰å’Œåˆ†å±‚NASï¼ˆHNASï¼‰ï¼Œæœ‰æ•ˆè§£å†³äº†å›ºå®šè¶…å‚æ•°å’Œå¯å‘å¼è®¾è®¡é€‰æ‹©å¸¦æ¥çš„é™åˆ¶ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶æå‡ºäº†Regularized PriorBandï¼Œæ—¨åœ¨å¹³è¡¡æ¨¡å‹ç²¾åº¦å’Œè®¡ç®—èµ„æºéœ€æ±‚ï¼Œä»¥è§£å†³ç°å®åŒ»ç–—ç¯å¢ƒä¸­èµ„æºçº¦æŸçš„é—®é¢˜ã€‚ç ”ç©¶åœ¨Medical Segmentation Decathlonçš„ä¸åŒMISæ•°æ®é›†ä¸Šè¯„ä¼°äº†æ–°æ–¹æ³•çš„å½±å“ï¼Œè¯æ˜äº†AutoMLæŠ€æœ¯åœ¨åˆ†å‰²æ€§èƒ½ã€è®¡ç®—æ•ˆç‡å’Œæ¨¡å‹è®¾è®¡é€‰æ‹©ä¸Šçš„ä¼˜åŠ¿ã€‚Auto-nnU-Netåœ¨10ä¸ªæ•°æ®é›†ä¸­çš„6ä¸ªä¸Šæ˜¾è‘—æé«˜äº†nnU-Netçš„åˆ†å‰²æ€§èƒ½ï¼Œå…¶ä½™æ•°æ®é›†è¡¨ç°ç›¸å½“ï¼ŒåŒæ—¶æ»¡è¶³äº†å®é™…èµ„æºéœ€æ±‚ã€‚ç›¸å…³ä»£ç å·²å…¬å¼€ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>Auto-nnU-Netæ˜¯ä¸€ä¸ªå…¨è‡ªåŠ¨çš„åŒ»ç–—å›¾åƒåˆ†å‰²æ¡†æ¶ï¼Œæ‰©å±•äº†ç°æœ‰çš„nnU-Netæ¨¡å‹ï¼Œæ”¯æŒè¶…å‚æ•°ä¼˜åŒ–ã€ç¥ç»ç½‘ç»œæ¶æ„æœç´¢å’Œåˆ†å±‚NASã€‚</li>
<li>Regularized PriorBandè§£å†³äº†æ¨¡å‹ç²¾åº¦å’Œè®¡ç®—èµ„æºä¹‹é—´çš„å¹³è¡¡é—®é¢˜ï¼Œå°¤å…¶é€‚ç”¨äºèµ„æºå—é™çš„ç°å®åŒ»ç–—ç¯å¢ƒã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒAuto-nnU-Netæ˜¾è‘—æé«˜äº†åˆ†å‰²æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨6ä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºnnU-Netã€‚</li>
<li>Auto-nnU-Netåœ¨ä¿æŒå®é™…èµ„æºéœ€æ±‚çš„åŒæ—¶ï¼Œå…¶ä½™æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¸nnU-Netç›¸å½“ã€‚</li>
<li>è¯¥ç ”ç©¶æä¾›äº†ä¸€ä¸ªæœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œä»¥è§£å†³åŒ»ç–—å›¾åƒåˆ†å‰²ä¸­é¢ä¸´çš„å¤šæ ·ä»»åŠ¡æŒ‘æˆ˜å’Œè®¡ç®—èµ„æºé™åˆ¶é—®é¢˜ã€‚</li>
<li>AutoMLæŠ€æœ¯åœ¨åŒ»ç–—å›¾åƒåˆ†å‰²é¢†åŸŸå…·æœ‰å·¨å¤§çš„æ½œåŠ›ï¼Œæœ‰æœ›æ¨åŠ¨è¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16561">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ff4a75cb58d9d2de42699b1c1f163a84.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-113f352dd289a97cd261c9c75b9d8d7c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-95c23babfb91541184b1a3629f20407c.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="SHaDe-Compact-and-Consistent-Dynamic-3D-Reconstruction-via-Tri-Plane-Deformation-and-Latent-Diffusion"><a href="#SHaDe-Compact-and-Consistent-Dynamic-3D-Reconstruction-via-Tri-Plane-Deformation-and-Latent-Diffusion" class="headerlink" title="SHaDe: Compact and Consistent Dynamic 3D Reconstruction via Tri-Plane   Deformation and Latent Diffusion"></a>SHaDe: Compact and Consistent Dynamic 3D Reconstruction via Tri-Plane   Deformation and Latent Diffusion</h2><p><strong>Authors:Asrar Alruwayqi</strong></p>
<p>We present a novel framework for dynamic 3D scene reconstruction that integrates three key components: an explicit tri-plane deformation field, a view-conditioned canonical radiance field with spherical harmonics (SH) attention, and a temporally-aware latent diffusion prior. Our method encodes 4D scenes using three orthogonal 2D feature planes that evolve over time, enabling efficient and compact spatiotemporal representation. These features are explicitly warped into a canonical space via a deformation offset field, eliminating the need for MLP-based motion modeling.   In canonical space, we replace traditional MLP decoders with a structured SH-based rendering head that synthesizes view-dependent color via attention over learned frequency bands improving both interpretability and rendering efficiency. To further enhance fidelity and temporal consistency, we introduce a transformer-guided latent diffusion module that refines the tri-plane and deformation features in a compressed latent space. This generative module denoises scene representations under ambiguous or out-of-distribution (OOD) motion, improving generalization.   Our model is trained in two stages: the diffusion module is first pre-trained independently, and then fine-tuned jointly with the full pipeline using a combination of image reconstruction, diffusion denoising, and temporal consistency losses. We demonstrate state-of-the-art results on synthetic benchmarks, surpassing recent methods such as HexPlane and 4D Gaussian Splatting in visual quality, temporal coherence, and robustness to sparse-view dynamic inputs. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„åŠ¨æ€3Dåœºæ™¯é‡å»ºæ¡†æ¶ï¼Œå®ƒé›†æˆäº†ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šæ˜¾å¼ä¸‰å¹³é¢å˜å½¢åœºã€å—è§†å›¾æ¡ä»¶çº¦æŸçš„å¸¦æœ‰çƒé¢è°æ³¢ï¼ˆSHï¼‰æ³¨æ„åŠ›çš„è§„èŒƒè¾å°„åœºï¼Œä»¥åŠæ—¶é—´æ„ŸçŸ¥æ½œåœ¨æ‰©æ•£å…ˆéªŒã€‚æˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨éšæ—¶é—´å˜åŒ–çš„ä¸‰ä¸ªæ­£äº¤2Dç‰¹å¾å¹³é¢å¯¹4Dåœºæ™¯è¿›è¡Œç¼–ç ï¼Œä»è€Œå®ç°é«˜æ•ˆä¸”ç´§å‡‘çš„æ—¶ç©ºè¡¨ç¤ºã€‚è¿™äº›ç‰¹å¾é€šè¿‡ä¸€ä¸ªå˜å½¢åç§»åœºæ˜¾å¼åœ°æ˜ å°„åˆ°è§„èŒƒç©ºé—´ï¼Œä»è€Œæ— éœ€åŸºäºMLPçš„è¿åŠ¨å»ºæ¨¡ã€‚åœ¨è§„èŒƒç©ºé—´ä¸­ï¼Œæˆ‘ä»¬ç”¨ç»“æ„åŒ–çš„SHæ¸²æŸ“å¤´æ›¿æ¢ä¼ ç»Ÿçš„MLPè§£ç å™¨ï¼Œé€šè¿‡å…³æ³¨å­¦ä¹ åˆ°çš„é¢‘ç‡å¸¦ï¼Œåˆæˆä¸è§†å›¾ç›¸å…³çš„é¢œè‰²ï¼Œæé«˜äº†å¯è§£é‡Šæ€§å’Œæ¸²æŸ“æ•ˆç‡ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜ä¿çœŸåº¦å’Œæ—¶é—´ä¸€è‡´æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåŸºäºå˜å‹å™¨çš„æ½œåœ¨æ‰©æ•£æ¨¡å—ï¼Œè¯¥æ¨¡å—åœ¨å‹ç¼©çš„æ½œåœ¨ç©ºé—´ä¸­ç»†åŒ–ä¸‰å¹³é¢å’Œå˜å½¢ç‰¹å¾ã€‚è¿™ä¸€ç”Ÿæˆæ¨¡å—åœ¨æ¨¡ç³Šæˆ–ç¦»ç¾¤åˆ†å¸ƒï¼ˆOODï¼‰è¿åŠ¨çš„æƒ…å†µä¸‹å¯¹åœºæ™¯è¡¨ç¤ºè¿›è¡Œå»å™ªï¼Œæé«˜äº†æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ¨¡å‹åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µè¿›è¡Œè®­ç»ƒï¼šé¦–å…ˆç‹¬ç«‹åœ°é¢„è®­ç»ƒæ‰©æ•£æ¨¡å—ï¼Œç„¶åä½¿ç”¨å›¾åƒé‡å»ºã€æ‰©æ•£å»å™ªå’Œæ—¶é—´ä¸€è‡´æ€§æŸå¤±çš„ç»„åˆå¯¹å®Œæ•´ç®¡é“è¿›è¡Œå¾®è°ƒã€‚æˆ‘ä»¬åœ¨åˆæˆåŸºå‡†æµ‹è¯•ä¸Šå±•ç¤ºäº†å“è¶Šçš„ç»“æœï¼Œåœ¨è§†è§‰è´¨é‡ã€æ—¶é—´è¿è´¯æ€§å’Œå¯¹ç¨€ç–è§†å›¾åŠ¨æ€è¾“å…¥çš„é²æ£’æ€§æ–¹é¢è¶…è¶Šäº†æœ€è¿‘çš„HexPlaneå’Œ4Dé«˜æ–¯æ‹¼è´´æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16535v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åŠ¨æ€3Dåœºæ™¯é‡å»ºæ¡†æ¶ï¼Œé›†æˆäº†ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šæ˜¾å¼ä¸‰å¹³é¢å˜å½¢åœºã€è§†æ¡ä»¶è§„èŒƒè¾å°„åœºå’Œå…·æœ‰çƒé¢è°æ³¢ï¼ˆSHï¼‰æ³¨æ„åŠ›çš„æ—¶é—´æ„ŸçŸ¥æ½œåœ¨æ‰©æ•£å…ˆéªŒã€‚è¯¥æ–¹æ³•é€šè¿‡ä¸‰ä¸ªéšæ—¶é—´å˜åŒ–çš„æ­£äº¤äºŒç»´ç‰¹å¾å¹³é¢ç¼–ç å››ç»´åœºæ™¯ï¼Œç„¶åé€šè¿‡å˜å½¢åç§»åœºå°†è¿™äº›ç‰¹å¾æ˜¾å¼åœ°å˜æ¢åˆ°è§„èŒƒç©ºé—´ï¼Œä»è€Œæ— éœ€ä½¿ç”¨MLPåŸºè¿åŠ¨å»ºæ¨¡ã€‚åœ¨è§„èŒƒç©ºé—´ä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨ç»“æ„åŒ–SHæ¸²æŸ“å¤´ä»£æ›¿ä¼ ç»ŸMLPè§£ç å™¨ï¼Œé€šè¿‡å…³æ³¨å­¦ä¹ é¢‘ç‡æ³¢æ®µæ¥åˆæˆè§†å›¾ç›¸å…³çš„é¢œè‰²ï¼Œæé«˜äº†å¯è§£é‡Šæ€§å’Œæ¸²æŸ“æ•ˆç‡ã€‚ä¸ºäº†è¿›ä¸€æ­¥æ”¹å–„ä¿çœŸåº¦å’Œæ—¶é—´ä¸€è‡´æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåŸºäºå˜å‹å™¨çš„æ½œåœ¨æ‰©æ•£æ¨¡å—ï¼Œè¯¥æ¨¡å—åœ¨å‹ç¼©çš„æ½œåœ¨ç©ºé—´ä¸­ç»†åŒ–ä¸‰å¹³é¢å’Œå˜å½¢ç‰¹å¾ã€‚è¯¥ç”Ÿæˆæ¨¡å—å‡å°‘äº†æ¨¡ç³Šæˆ–ç¦»ç¾¤å€¼è¿åŠ¨ä¸‹çš„åœºæ™¯è¡¨ç¤ºçš„å™ªå£°ï¼Œæé«˜äº†æ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ¨¡å‹åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µè¿›è¡Œè®­ç»ƒï¼šé¦–å…ˆç‹¬ç«‹åœ°é¢„è®­ç»ƒæ‰©æ•£æ¨¡å—ï¼Œç„¶åä½¿ç”¨å›¾åƒé‡å»ºã€æ‰©æ•£å»å™ªå’Œæ—¶é—´ä¸€è‡´æ€§æŸå¤±çš„ç»„åˆè”åˆå¾®è°ƒæ•´ä¸ªç®¡é“ã€‚æˆ‘ä»¬åœ¨åˆæˆåŸºå‡†æµ‹è¯•ä¸Šå±•ç¤ºäº†å“è¶Šçš„ç»“æœï¼Œåœ¨è§†è§‰è´¨é‡ã€æ—¶é—´è¿è´¯æ€§å’Œå¯¹ç¨€ç–åŠ¨æ€è¾“å…¥çš„é²æ£’æ€§æ–¹é¢è¶…è¶Šäº†æœ€è¿‘çš„HexPlaneå’Œå››ç»´é«˜æ–¯å¹³ç‰ˆå°åˆ·ç­‰æ–¹æ³•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<p>ä¸€ã€æå‡ºäº†ä¸€ä¸ªé›†æˆä¸‰å…³é”®ç»„ä»¶çš„æ–°å‹åŠ¨æ€3Dåœºæ™¯é‡å»ºæ¡†æ¶ï¼ŒåŒ…æ‹¬æ˜¾å¼ä¸‰å¹³é¢å˜å½¢åœºã€è§†æ¡ä»¶è§„èŒƒè¾å°„åœºå’Œå…·æœ‰çƒé¢è°æ³¢ï¼ˆSHï¼‰æ³¨æ„åŠ›çš„æ—¶é—´æ„ŸçŸ¥æ½œåœ¨æ‰©æ•£å…ˆéªŒã€‚</p>
<p>äºŒã€é‡‡ç”¨ä¸‰ä¸ªéšæ—¶é—´å˜åŒ–çš„æ­£äº¤äºŒç»´ç‰¹å¾å¹³é¢ç¼–ç å››ç»´åœºæ™¯ï¼Œå¹¶å€ŸåŠ©å˜å½¢åç§»åœºå°†è¿™äº›ç‰¹å¾æ˜ å°„åˆ°è§„èŒƒç©ºé—´ã€‚</p>
<p>ä¸‰ã€ä½¿ç”¨ç»“æ„åŒ–SHæ¸²æŸ“å¤´ä»£æ›¿ä¼ ç»ŸMLPè§£ç å™¨ï¼Œå®ç°é«˜æ•ˆæ¸²æŸ“å¹¶æé«˜æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚</p>
<p>å››ã€å¼•å…¥äº†åŸºäºå˜å‹å™¨çš„æ½œåœ¨æ‰©æ•£æ¨¡å—ï¼Œæé«˜äº†åœºæ™¯è¡¨ç¤ºçš„ä¿çœŸåº¦å’Œæ—¶é—´ä¸€è‡´æ€§ï¼Œå¢å¼ºäº†æ¨¡å‹å¯¹æ¨¡ç³Šæˆ–ç¦»ç¾¤å€¼è¿åŠ¨çš„å¤„ç†èƒ½åŠ›ã€‚</p>
<p>äº”ã€æ¨¡å‹è®­ç»ƒåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šé¢„è®­ç»ƒæ‰©æ•£æ¨¡å—ï¼Œç„¶åç»“åˆå¤šç§æŸå¤±å¯¹æ•´ä¸ªç®¡é“è¿›è¡Œå¾®è°ƒã€‚</p>
<p>å…­ã€åœ¨åˆæˆåŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†å“è¶Šçš„ç»“æœï¼Œè¶…è¶Šäº†ç°æœ‰çš„åŠ¨æ€é‡å»ºæ–¹æ³•ã€‚æœ¬æ–¹æ³•åœ¨è§†è§‰è´¨é‡ã€æ—¶é—´è¿è´¯æ€§å’Œå¯¹ç¨€ç–è¾“å…¥ä¿¡å·çš„é²æ£’æ€§æ–¹é¢å…·æœ‰ä¼˜åŠ¿ã€‚</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16535">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5d2078e5afc415869e09090a1b2beb05.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd5696dd58fff4cd2fbdf20329e853a3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-53ca7679e0829cfeabc3d94ab477210b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0408e7eafc56b86ef5e3e641eea9d11e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-810bd2dee7512748b03541103c339eff.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Observational-Properties-of-Thermal-Emission-from-Relativistic-Jets-Embedded-in-AGN-Disks"><a href="#Observational-Properties-of-Thermal-Emission-from-Relativistic-Jets-Embedded-in-AGN-Disks" class="headerlink" title="Observational Properties of Thermal Emission from Relativistic Jets   Embedded in AGN Disks"></a>Observational Properties of Thermal Emission from Relativistic Jets   Embedded in AGN Disks</h2><p><strong>Authors:Ken Chen, Zi-Gao Dai</strong></p>
<p>Relativistic jets can be produced within the accretion disk of an active galactic nucleus (AGN), leading to distinct thermal emission as they propagate through a dense disk environment. In this paper, we present a comprehensive study of dynamical evolution of jets embedded in an AGN disk and their associated observational properties, focusing on scenarios in which jets either successfully break out of the disk or become choked. By modeling the jet-cocoon system propagation, we calculate the thermal emission contributions from the jet-head shock breakout, disk cocoon, and jet cocoon components. Our results reveal that soft X-ray flares are the most prominent observable signatures, with duration ranging from O(10^2) s to O(10^5) s, occasionally exhibiting double-peaked light curves, whereas UV&#x2F;optical flares are detectable only for powerful jets, persisting for several days to tens of days. This thermal emission serves as a critical electromagnetic counterpart to jet-producing events and provide insights into jet dynamics and AGN disk properties. Our findings highlight the importance of multi-wavelength follow-up observations to establish a diagnostic paradigm for candidate electromagnetic counterpart identification to AGN-embedded events and to distinguish thermal flares from AGN background variability. </p>
<blockquote>
<p>åœ¨æ´»åŠ¨æ˜Ÿç³»æ ¸ï¼ˆAGNï¼‰çš„å¸ç§¯ç›˜ä¸­å¯ä»¥äº§ç”Ÿç›¸å¯¹è®ºæ€§å–·æµï¼Œå®ƒä»¬åœ¨ç¨ å¯†çš„ç›˜ç¯å¢ƒä¸­ä¼ æ’­æ—¶ä¼šäº§ç”Ÿç‹¬ç‰¹çš„çƒ­è¾å°„ã€‚æœ¬æ–‡å…¨é¢ç ”ç©¶äº†åµŒå…¥åœ¨AGNç›˜ä¸­çš„å–·æµçš„åŠ¨æ€æ¼”åŒ–åŠå…¶ç›¸å…³çš„è§‚æµ‹ç‰¹æ€§ï¼Œé‡ç‚¹å…³æ³¨å–·æµæˆåŠŸçªç ´ç£ç›˜æˆ–å—é˜»çš„åœºæ™¯ã€‚é€šè¿‡æ¨¡æ‹Ÿå–·æµ-èŒ§çŠ¶ç³»ç»Ÿä¼ æ’­ï¼Œæˆ‘ä»¬è®¡ç®—äº†å–·æµå¤´éƒ¨å†²å‡»çªç ´ã€ç›˜èŒ§å’Œå–·æµèŒ§ç»„ä»¶çš„çƒ­è¾å°„è´¡çŒ®ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œè½¯Xå°„çº¿è€€æ–‘æ˜¯æœ€çªå‡ºçš„å¯è§‚æµ‹ç‰¹å¾ï¼ŒæŒç»­æ—¶é—´ä»O(10^2)ç§’åˆ°O(10^5)ç§’ä¸ç­‰ï¼Œå¶å°”è¡¨ç°å‡ºåŒå³°çŠ¶å…‰æ›²çº¿ï¼Œè€Œç´«å¤–&#x2F;å…‰å­¦è€€æ–‘ä»…å¯¹å¼ºå¤§çš„å–·æµå¯æ£€æµ‹ï¼ŒæŒç»­æ•°å¤©è‡³æ•°åå¤©ã€‚è¿™ç§çƒ­è¾å°„æ˜¯å–·æµäº§ç”Ÿäº‹ä»¶çš„é‡è¦ç”µç£å¯¹åº”ç‰©ï¼Œä¸ºç†è§£å–·æµåŠ¨åŠ›å’ŒAGNç›˜ç‰¹æ€§æä¾›äº†è§è§£ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¼ºè°ƒäº†å¤šæ³¢é•¿åç»­è§‚æµ‹çš„é‡è¦æ€§ï¼Œä»¥å»ºç«‹å¯¹å€™é€‰ç”µç£å¯¹åº”ç‰©çš„è¯Šæ–­èŒƒå¼ï¼ŒåŒºåˆ†çƒ­é—ªä¸AGNèƒŒæ™¯å˜åŒ–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16390v1">PDF</a> 29 pages, 12 figures, 2 tables, accepted for publication in ApJ</p>
<p><strong>Summary</strong><br>     æ´»åŠ¨æ˜Ÿç³»æ ¸ï¼ˆAGNï¼‰å–·æµåœ¨è‡´å¯†ç›˜ç¯å¢ƒä¸­ä¼ æ’­æ—¶ï¼Œä¼šäº§ç”Ÿç‰¹æ®Šçš„çƒ­è¾å°„ã€‚æœ¬æ–‡ç ”ç©¶äº†å–·æµåœ¨AGNç›˜ä¸­çš„åŠ¨æ€æ¼”åŒ–åŠå…¶è§‚æµ‹ç‰¹æ€§ï¼Œé‡ç‚¹ç ”ç©¶å–·æµæˆåŠŸçªç ´åœ†ç›˜æˆ–è¢«é˜»æ­¢çš„æƒ…å¢ƒã€‚æ¨¡æ‹Ÿå–·æµ-èŒ§çŠ¶ç‰©ç³»ç»Ÿçš„ä¼ æ’­è¿‡ç¨‹ï¼Œè®¡ç®—äº†å–·æµå¤´éƒ¨å†²å‡»çªç ´ã€ç›˜èŒ§çŠ¶ç‰©å’Œå–·æµèŒ§çŠ¶ç‰©ç»„ä»¶çš„çƒ­è¾å°„è´¡çŒ®ã€‚ç»“æœæ˜¾ç¤ºè½¯Xå°„çº¿è€€æ–‘æ˜¯æœ€æ˜¾è‘—çš„å¯è§‚æµ‹ç‰¹å¾ï¼ŒæŒç»­æ—¶é—´ä»å‡ ç§’åˆ°æ•°ä¸‡ç§’ä¸ç­‰ï¼Œå¶å°”ä¼šå‡ºç°åŒå³°å…‰å˜æ›²çº¿ï¼Œè€Œç´«å¤–&#x2F;å…‰å­¦è€€æ–‘åªå¯¹äºå¼ºå¤§çš„å–·æµå¯æ£€æµ‹åˆ°ï¼ŒæŒç»­æ•°å¤©è‡³æ•°åå¤©ã€‚è¿™ç§çƒ­è¾å°„ä½œä¸ºå–·æµäº§ç”Ÿäº‹ä»¶çš„å…³é”®ç”µç£å¯¹åº”ä½“ï¼Œä¸ºç†è§£å–·æµåŠ¨åŠ›å­¦å’ŒAGNç›˜å±æ€§æä¾›äº†è§è§£ã€‚å¼ºè°ƒå¤šæ³¢é•¿åç»­è§‚æµ‹çš„é‡è¦æ€§ï¼Œä¸ºè¯†åˆ«ä¸åµŒå…¥åœ¨AGNä¸­çš„äº‹ä»¶çš„ç”µç£å¯¹åº”ä½“å¹¶å»ºç«‹è¯Šæ–­èŒƒå¼ä»¥åŠåŒºåˆ†çƒ­è€€æ–‘ä¸èƒŒæ™¯æ´»åŠ¨æ˜Ÿç³»æ ¸çš„å˜å¼‚æ€§è‡³å…³é‡è¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ´»åŠ¨æ˜Ÿç³»æ ¸ï¼ˆAGNï¼‰å–·æµåœ¨è‡´å¯†ç›˜ç¯å¢ƒä¸­ä¼ æ’­æ—¶ä¼šäº§ç”Ÿç‹¬ç‰¹çš„çƒ­è¾å°„ã€‚</li>
<li>å–·æµçš„åŠ¨æ€æ¼”åŒ–åŠå…¶è§‚æµ‹ç‰¹æ€§æ˜¯ç ”ç©¶é‡ç‚¹ï¼ŒåŒ…æ‹¬æˆåŠŸçªç ´åœ†ç›˜æˆ–è¢«é˜»æ­¢çš„æƒ…å¢ƒã€‚</li>
<li>æ¨¡æ‹Ÿå–·æµç³»ç»Ÿçš„ä¼ æ’­ä»¥è®¡ç®—ä¸åŒç»„ä»¶çš„çƒ­è¾å°„è´¡çŒ®ã€‚</li>
<li>è½¯Xå°„çº¿è€€æ–‘æ˜¯æœ€æ˜¾è‘—çš„å¯è§‚æµ‹ç‰¹å¾ï¼ŒæŒç»­æ—¶é—´ä»å‡ ç§’åˆ°æ•°ä¸‡ç§’ä¸ç­‰ï¼Œå¶å°”å‡ºç°åŒå³°å…‰å˜æ›²çº¿ã€‚</li>
<li>ç´«å¤–&#x2F;å…‰å­¦è€€æ–‘ä»…å¯¹å¼ºå¤§çš„å–·æµå¯æ£€æµ‹åˆ°ï¼ŒæŒç»­æ•°å¤©è‡³æ•°åå¤©ã€‚</li>
<li>çƒ­è¾å°„ä¸ºç†è§£å–·æµåŠ¨åŠ›å­¦å’ŒAGNç›˜å±æ€§æä¾›å…³é”®ä¿¡æ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16390">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-90ae1ff37ee76e00d92b93779c20b8e4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e479a5b300b4132709002020777b9cfc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-03d07efeca994396fc5f6ee3adb48d2a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2fbf577ef6953a7536341fe67ac3fecb.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_åŒ»å­¦å›¾åƒ/2505.16390v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Efficient-Prototype-Consistency-Learning-in-Medical-Image-Segmentation-via-Joint-Uncertainty-and-Data-Augmentation"><a href="#Efficient-Prototype-Consistency-Learning-in-Medical-Image-Segmentation-via-Joint-Uncertainty-and-Data-Augmentation" class="headerlink" title="Efficient Prototype Consistency Learning in Medical Image Segmentation   via Joint Uncertainty and Data Augmentation"></a>Efficient Prototype Consistency Learning in Medical Image Segmentation   via Joint Uncertainty and Data Augmentation</h2><p><strong>Authors:Lijian Li, Yuanpeng He, Chi-Man Pun</strong></p>
<p>Recently, prototype learning has emerged in semi-supervised medical image segmentation and achieved remarkable performance. However, the scarcity of labeled data limits the expressiveness of prototypes in previous methods, potentially hindering the complete representation of prototypes for class embedding. To overcome this issue, we propose an efficient prototype consistency learning via joint uncertainty quantification and data augmentation (EPCL-JUDA) to enhance the semantic expression of prototypes based on the framework of Mean-Teacher. The concatenation of original and augmented labeled data is fed into student network to generate expressive prototypes. Then, a joint uncertainty quantification method is devised to optimize pseudo-labels and generate reliable prototypes for original and augmented unlabeled data separately. High-quality global prototypes for each class are formed by fusing labeled and unlabeled prototypes, which are utilized to generate prototype-to-features to conduct consistency learning. Notably, a prototype network is proposed to reduce high memory requirements brought by the introduction of augmented data. Extensive experiments on Left Atrium, Pancreas-NIH, Type B Aortic Dissection datasets demonstrate EPCL-JUDAâ€™s superiority over previous state-of-the-art approaches, confirming the effectiveness of our framework. The code will be released soon. </p>
<blockquote>
<p>è¿‘æœŸï¼ŒåŸå‹å­¦ä¹ åœ¨åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­å´­éœ²å¤´è§’ï¼Œå¹¶è·å¾—äº†æ˜¾è‘—çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œæ ‡æ³¨æ•°æ®çš„ç¨€ç¼ºæ€§é™åˆ¶äº†ä¹‹å‰æ–¹æ³•ä¸­åŸå‹çš„è¡¨è¾¾åŠ›ï¼Œè¿™å¯èƒ½é˜»ç¢äº†åŸå‹çš„å®Œå…¨è¡¨ç¤ºä¸ºç±»åµŒå…¥ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†é€šè¿‡è”åˆä¸ç¡®å®šæ€§é‡åŒ–å’Œæ•°æ®å¢å¼ºï¼ˆEPCL-JUDAï¼‰çš„æœ‰æ•ˆåŸå‹ä¸€è‡´æ€§å­¦ä¹ æ–¹æ³•ï¼Œä»¥å¢å¼ºåŸºäºMean-Teacheræ¡†æ¶çš„åŸå‹è¯­ä¹‰è¡¨è¾¾ã€‚åŸå§‹å’Œå¢å¼ºåçš„æ ‡æ³¨æ•°æ®çš„ç»„åˆè¢«è¾“å…¥åˆ°å­¦ç”Ÿç½‘ç»œä¸­ä»¥ç”Ÿæˆè¡¨è¾¾æ€§åŸå‹ã€‚ç„¶åï¼Œè®¾è®¡äº†ä¸€ç§è”åˆä¸ç¡®å®šæ€§é‡åŒ–æ–¹æ³•æ¥ä¼˜åŒ–ä¼ªæ ‡ç­¾å¹¶ä¸ºåŸå§‹å’Œå¢å¼ºåçš„æ— æ ‡ç­¾æ•°æ®åˆ†åˆ«ç”Ÿæˆå¯é åŸå‹ã€‚é€šè¿‡èåˆæœ‰æ ‡ç­¾å’Œæ— æ ‡ç­¾çš„åŸå‹ï¼Œå½¢æˆæ¯ä¸ªç±»çš„é«˜è´¨é‡å…¨å±€åŸå‹ï¼Œç”¨äºç”ŸæˆåŸå‹åˆ°ç‰¹å¾è¿›è¡Œä¸€è‡´æ€§å­¦ä¹ ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä¸ºäº†å‡å°‘ç”±å¼•å…¥å¢å¼ºæ•°æ®å¸¦æ¥çš„é«˜å†…å­˜è¦æ±‚ï¼Œæå‡ºäº†ä¸€ä¸ªåŸå‹ç½‘ç»œã€‚åœ¨å·¦å¿ƒæˆ¿ã€èƒ°è…ºNIHã€Bå‹ä¸»åŠ¨è„‰å¤¹å±‚æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒEPCL-JUDAä¼˜äºä¹‹å‰çš„æœ€æ–°æ–¹æ³•ï¼Œè¯å®äº†æˆ‘ä»¬çš„æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚ä»£ç å°†å¾ˆå¿«å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16283v1">PDF</a> arXiv admin note: substantial text overlap with arXiv:2404.10717</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸï¼ŒåŸå‹å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²çš„åŠç›‘ç£å­¦ä¹ ä¸­å–å¾—äº†æ˜¾è‘—æˆæ•ˆã€‚ä½†ä¹‹å‰çš„æ–¹æ³•å—é™äºæ ‡è®°æ•°æ®çš„ç¨€ç¼ºæ€§ï¼Œå½±å“äº†åŸå‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºMean-Teacheræ¡†æ¶çš„æœ‰æ•ˆåŸå‹ä¸€è‡´æ€§å­¦ä¹ æ–¹æ³•EPCL-JUDAã€‚é€šè¿‡è”åˆä¸ç¡®å®šæ€§é‡åŒ–è¿›è¡Œæ•°æ®å¢å¼ºï¼Œå¢å¼ºäº†åŸå‹çš„è¯­ä¹‰è¡¨è¾¾ã€‚å°†åŸå§‹å’Œå¢å¼ºåçš„æ ‡è®°æ•°æ®åˆå¹¶è¾“å…¥å­¦ç”Ÿç½‘ç»œä»¥ç”Ÿæˆè¡¨è¾¾æ€§åŸå‹ã€‚éšåï¼Œè®¾è®¡äº†ä¸€ç§è”åˆä¸ç¡®å®šæ€§é‡åŒ–æ–¹æ³•ï¼Œä»¥ä¼˜åŒ–ä¼ªæ ‡ç­¾å¹¶ä¸ºåŸå§‹å’Œå¢å¼ºåçš„æœªæ ‡è®°æ•°æ®åˆ†åˆ«ç”Ÿæˆå¯é çš„åŸå‹ã€‚é€šè¿‡èåˆæ ‡è®°å’Œæ— æ ‡è®°åŸå‹å½¢æˆé«˜è´¨é‡çš„å…¨å±€åŸå‹ï¼Œç”¨äºç”Ÿæˆç‰¹å¾å¹¶è¿›è¡Œä¸€è‡´æ€§å­¦ä¹ ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†åŸå‹ç½‘ç»œä»¥é™ä½å¢å¼ºæ•°æ®å¸¦æ¥çš„é«˜å†…å­˜éœ€æ±‚ã€‚åœ¨Left Atriumã€Pancreas-NIHå’ŒType B Aortic Dissectionæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†EPCL-JUDAçš„ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŸå‹å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²çš„åŠç›‘ç£å­¦ä¹ ä¸­è¡¨ç°å‡ºæ˜¾è‘—æˆæ•ˆã€‚</li>
<li>ç°æœ‰æ–¹æ³•å—é™äºæ ‡è®°æ•°æ®çš„ç¨€ç¼ºæ€§ï¼Œå½±å“äº†åŸå‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚</li>
<li>æå‡ºäº†åŸºäºMean-Teacheræ¡†æ¶çš„æœ‰æ•ˆåŸå‹ä¸€è‡´æ€§å­¦ä¹ æ–¹æ³•EPCL-JUDAï¼Œå¢å¼ºåŸå‹çš„è¯­ä¹‰è¡¨è¾¾ã€‚</li>
<li>é€šè¿‡è”åˆä¸ç¡®å®šæ€§é‡åŒ–ä¼˜åŒ–ä¼ªæ ‡ç­¾ï¼Œä¸ºåŸå§‹å’Œå¢å¼ºåçš„æœªæ ‡è®°æ•°æ®ç”Ÿæˆå¯é åŸå‹ã€‚</li>
<li>é€šè¿‡èåˆæ ‡è®°å’Œæ— æ ‡è®°åŸå‹å½¢æˆé«˜è´¨é‡å…¨å±€åŸå‹ï¼Œç”¨äºç‰¹å¾ç”Ÿæˆå’Œä¸€è‡´æ€§å­¦ä¹ ã€‚</li>
<li>å¼•å…¥åŸå‹ç½‘ç»œé™ä½é«˜å†…å­˜éœ€æ±‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16283">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_åŒ»å­¦å›¾åƒ/2505.16283v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_åŒ»å­¦å›¾åƒ/2505.16283v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_åŒ»å­¦å›¾åƒ/2505.16283v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Benchmarking-Chest-X-ray-Diagnosis-Models-Across-Multinational-Datasets"><a href="#Benchmarking-Chest-X-ray-Diagnosis-Models-Across-Multinational-Datasets" class="headerlink" title="Benchmarking Chest X-ray Diagnosis Models Across Multinational Datasets"></a>Benchmarking Chest X-ray Diagnosis Models Across Multinational Datasets</h2><p><strong>Authors:Qinmei Xu, Yiheng Li, Xianghao Zhan, Ahmet Gorkem Er, Brittany Dashevsky, Chuanjun Xu, Mohammed Alawad, Mengya Yang, Liu Ya, Changsheng Zhou, Xiao Li, Haruka Itakura, Olivier Gevaert</strong></p>
<p>Foundation models leveraging vision-language pretraining have shown promise in chest X-ray (CXR) interpretation, yet their real-world performance across diverse populations and diagnostic tasks remains insufficiently evaluated. This study benchmarks the diagnostic performance and generalizability of foundation models versus traditional convolutional neural networks (CNNs) on multinational CXR datasets. We evaluated eight CXR diagnostic models - five vision-language foundation models and three CNN-based architectures - across 37 standardized classification tasks using six public datasets from the USA, Spain, India, and Vietnam, and three private datasets from hospitals in China. Performance was assessed using AUROC, AUPRC, and other metrics across both shared and dataset-specific tasks. Foundation models outperformed CNNs in both accuracy and task coverage. MAVL, a model incorporating knowledge-enhanced prompts and structured supervision, achieved the highest performance on public (mean AUROC: 0.82; AUPRC: 0.32) and private (mean AUROC: 0.95; AUPRC: 0.89) datasets, ranking first in 14 of 37 public and 3 of 4 private tasks. All models showed reduced performance on pediatric cases, with average AUROC dropping from 0.88 +&#x2F;- 0.18 in adults to 0.57 +&#x2F;- 0.29 in children (p &#x3D; 0.0202). These findings highlight the value of structured supervision and prompt design in radiologic AI and suggest future directions including geographic expansion and ensemble modeling for clinical deployment. Code for all evaluated models is available at <a target="_blank" rel="noopener" href="https://drive.google.com/drive/folders/1B99yMQm7bB4h1sVMIBja0RfUu8gLktCE">https://drive.google.com/drive/folders/1B99yMQm7bB4h1sVMIBja0RfUu8gLktCE</a> </p>
<blockquote>
<p>åˆ©ç”¨è§†è§‰è¯­è¨€é¢„è®­ç»ƒçš„åŸºçŸ³æ¨¡å‹åœ¨èƒ¸éƒ¨Xå°„çº¿ï¼ˆCXRï¼‰è§£è¯»æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œç„¶è€Œå®ƒä»¬åœ¨å¤šæ ·äººç¾¤å’Œè¯Šæ–­ä»»åŠ¡ä¸­çš„ç°å®ä¸–ç•Œæ€§èƒ½ä»è¯„ä»·ä¸è¶³ã€‚æœ¬ç ”ç©¶åœ¨å¤šå›½CXRæ•°æ®é›†ä¸Šï¼Œå¯¹åŸºçŸ³æ¨¡å‹ä¸ä¼ ç»Ÿå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰çš„è¯Šæ–­æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›è¿›è¡Œäº†è¯„ä¼°ã€‚æˆ‘ä»¬è¯„ä¼°äº†8ç§CXRè¯Šæ–­æ¨¡å‹ï¼ˆ5ç§è§†è§‰è¯­è¨€åŸºçŸ³æ¨¡å‹å’Œ3ç§åŸºäºCNNçš„æ¶æ„ï¼‰åœ¨37ä¸ªæ ‡å‡†åŒ–åˆ†ç±»ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œè¿™äº›ä»»åŠ¡ä½¿ç”¨äº†æ¥è‡ªç¾å›½ã€è¥¿ç­ç‰™ã€å°åº¦å’Œè¶Šå—çš„å…­ä¸ªå…¬å¼€æ•°æ®é›†ä»¥åŠæ¥è‡ªä¸­å›½ä¸‰å®¶åŒ»é™¢çš„ä¸‰ä¸ªç§æœ‰æ•°æ®é›†ã€‚æ€§èƒ½è¯„ä¼°é‡‡ç”¨AUROCã€AUPRCå’Œå…¶ä»–æŒ‡æ ‡ï¼ŒåŒ…æ‹¬å…±äº«ä»»åŠ¡å’Œç‰¹å®šæ•°æ®é›†çš„ä»»åŠ¡ã€‚åŸºçŸ³æ¨¡å‹åœ¨å‡†ç¡®åº¦å’Œä»»åŠ¡è¦†ç›–æ–¹é¢éƒ½ä¼˜äºCNNã€‚MAVLæ¨¡å‹ç»“åˆäº†çŸ¥è¯†å¢å¼ºæç¤ºå’Œç»“æ„åŒ–ç›‘ç£ï¼Œåœ¨å…¬å¼€æ•°æ®é›†ï¼ˆå¹³å‡AUROCï¼š0.82ï¼›AUPRCï¼š0.32ï¼‰å’Œç§æœ‰æ•°æ®é›†ï¼ˆå¹³å‡AUROCï¼š0.95ï¼›AUPRCï¼š0.89ï¼‰ä¸Šå–å¾—äº†æœ€é«˜æ€§èƒ½ï¼Œåœ¨37ä¸ªå…¬å¼€ä»»åŠ¡ä¸­æ’åç¬¬ä¸€14ä¸ªï¼Œåœ¨4ä¸ªç§æœ‰ä»»åŠ¡ä¸­æ’åç¬¬ä¸€3ä¸ªã€‚æ‰€æœ‰æ¨¡å‹åœ¨å„¿ç§‘ç—…ä¾‹ä¸­çš„è¡¨ç°éƒ½æœ‰æ‰€ä¸‹é™ï¼Œæˆå¹´äººå’Œå„¿ç«¥çš„å¹³å‡AUROCåˆ†åˆ«ä»0.88 +&#x2F;- 0.18ä¸‹é™åˆ°0.57 +&#x2F;- 0.29ï¼ˆp &#x3D; 0.0202ï¼‰ã€‚è¿™äº›å‘ç°çªæ˜¾äº†ç»“æ„åŒ–ç›‘ç£å’Œæç¤ºè®¾è®¡åœ¨æ”¾å°„å­¦äººå·¥æ™ºèƒ½ä¸­çš„ä»·å€¼ï¼Œå¹¶æç¤ºæœªæ¥çš„æ–¹å‘åŒ…æ‹¬åœ°ç†æ‰©å¼ å’Œé›†æˆå»ºæ¨¡ä»¥ç”¨äºä¸´åºŠéƒ¨ç½²ã€‚æ‰€æœ‰è¯„ä¼°æ¨¡å‹çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://drive.google.com/drive/folders/1B99yMQm7bB4h1sVMIBja0RfUu8gLktCE">https://drive.google.com/drive/folders/1B99yMQm7bB4h1sVMIBja0RfUu8gLktCE</a>å¤„è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16027v1">PDF</a> 78 pages, 7 figures, 2 tabeles</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†åŸºäºè§†è§‰è¯­è¨€é¢„è®­ç»ƒçš„æ¨¡å‹åœ¨è·¨å›½èƒ¸éƒ¨Xå°„çº¿ï¼ˆCXRï¼‰æ•°æ®é›†ä¸Šçš„è¯Šæ–­æ€§èƒ½ä¸æ³›åŒ–èƒ½åŠ›ã€‚å¯¹æ¯”äº†äº”ç§è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹å’Œä¸‰ç§å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ¶æ„çš„å…«ä¸ªCXRè¯Šæ–­æ¨¡å‹ï¼Œå‘ç°åŸºç¡€æ¨¡å‹åœ¨å‡†ç¡®åº¦å’Œä»»åŠ¡è¦†ç›–æ–¹é¢éƒ½ä¼˜äºCNNã€‚å…¶ä¸­ï¼Œç»“åˆäº†çŸ¥è¯†å¢å¼ºæç¤ºå’Œç»“æ„åŒ–ç›‘ç£çš„MAVLæ¨¡å‹åœ¨å…¬å…±æ•°æ®é›†å’Œç§æœ‰æ•°æ®é›†ä¸Šè¡¨ç°æœ€ä½³ã€‚æ‰€æœ‰æ¨¡å‹åœ¨å„¿ç§‘ç—…ä¾‹ä¸­çš„è¡¨ç°éƒ½æœ‰æ‰€ä¸‹é™ï¼Œè¿™è¡¨æ˜æœªæ¥éœ€è¦è¿›ä¸€æ­¥ç ”ç©¶å¦‚ä½•æ”¹è¿›æ¨¡å‹å¤„ç†å„¿ç§‘ç—…ä¾‹çš„èƒ½åŠ›ã€‚ä»£ç å¯åœ¨Google Driveæ‰¾åˆ°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŸºäºè§†è§‰è¯­è¨€é¢„è®­ç»ƒçš„æ¨¡å‹åœ¨èƒ¸éƒ¨Xå°„çº¿ï¼ˆCXRï¼‰è§£è¯»ä¸­å±•ç°å‡ºæ½œåŠ›ã€‚</li>
<li>ç ”ç©¶å¯¹æ¯”äº†å…«ç§CXRè¯Šæ–­æ¨¡å‹ï¼ˆäº”ç§åŸºç¡€æ¨¡å‹å’Œä¸‰ç§CNNæ¶æ„ï¼‰åœ¨è·¨å›½æ•°æ®é›†ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>åŸºç¡€æ¨¡å‹åœ¨å‡†ç¡®åº¦å’Œä»»åŠ¡è¦†ç›–æ–¹é¢ä¼˜äºCNNã€‚</li>
<li>MAVLæ¨¡å‹ç»“åˆäº†çŸ¥è¯†å¢å¼ºæç¤ºå’Œç»“æ„åŒ–ç›‘ç£ï¼Œåœ¨å¤šæ•°ä»»åŠ¡ä¸­è¡¨ç°æœ€ä½³ã€‚</li>
<li>æ‰€æœ‰æ¨¡å‹åœ¨å„¿ç§‘ç—…ä¾‹ä¸­çš„è¡¨ç°ä¸‹é™ï¼Œæç¤ºéœ€è¦æ”¹è¿›æ¨¡å‹å¤„ç†å„¿ç§‘ç—…ä¾‹çš„èƒ½åŠ›ã€‚</li>
<li>ç ”ç©¶å¼ºè°ƒäº†ç»“æ„åŒ–ç›‘ç£å’Œæç¤ºè®¾è®¡åœ¨æ”¾å°„å­¦äººå·¥æ™ºèƒ½ä¸­çš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16027">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_åŒ»å­¦å›¾åƒ/2505.16027v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_åŒ»å­¦å›¾åƒ/2505.16027v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="P3Net-Progressive-and-Periodic-Perturbation-for-Semi-Supervised-Medical-Image-Segmentation"><a href="#P3Net-Progressive-and-Periodic-Perturbation-for-Semi-Supervised-Medical-Image-Segmentation" class="headerlink" title="P3Net: Progressive and Periodic Perturbation for Semi-Supervised Medical   Image Segmentation"></a>P3Net: Progressive and Periodic Perturbation for Semi-Supervised Medical   Image Segmentation</h2><p><strong>Authors:Zhenyan Yao, Miao Zhang, Lanhu Wu, Yongri Piao, Feng Tian, Weibing Sun, Huchuan Lu</strong></p>
<p>Perturbation with diverse unlabeled data has proven beneficial for semi-supervised medical image segmentation (SSMIS). While many works have successfully used various perturbation techniques, a deeper understanding of learning perturbations is needed. Excessive or inappropriate perturbation can have negative effects, so we aim to address two challenges: how to use perturbation mechanisms to guide the learning of unlabeled data through labeled data, and how to ensure accurate predictions in boundary regions. Inspired by human progressive and periodic learning, we propose a progressive and periodic perturbation mechanism (P3M) and a boundary-focused loss. P3M enables dynamic adjustment of perturbations, allowing the model to gradually learn them. Our boundary-focused loss encourages the model to concentrate on boundary regions, enhancing sensitivity to intricate details and ensuring accurate predictions. Experimental results demonstrate that our method achieves state-of-the-art performance on two 2D and 3D datasets. Moreover, P3M is extendable to other methods, and the proposed loss serves as a universal tool for improving existing methods, highlighting the scalability and applicability of our approach. </p>
<blockquote>
<p>æ‰°åŠ¨æŠ€æœ¯åœ¨åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²ï¼ˆSSMISï¼‰ä¸­ï¼Œåº”ç”¨å¤šæ ·åŒ–çš„æ— æ ‡ç­¾æ•°æ®å·²ç»è¯æ˜æ˜¯æœ‰ç›Šçš„ã€‚è™½ç„¶è®¸å¤šå·¥ä½œå·²ç»æˆåŠŸåœ°ä½¿ç”¨äº†å„ç§æ‰°åŠ¨æŠ€æœ¯ï¼Œä½†å¯¹å­¦ä¹ æ‰°åŠ¨çš„ç†è§£ä»ç„¶ä¸å¤Ÿæ·±å…¥ã€‚è¿‡åº¦çš„æˆ–ä¸é€‚å½“çš„æ‰°åŠ¨ä¼šäº§ç”Ÿè´Ÿé¢å½±å“ï¼Œå› æ­¤ï¼Œæˆ‘ä»¬æ—¨åœ¨è§£å†³ä¸¤ä¸ªæŒ‘æˆ˜ï¼šä¸€æ˜¯å¦‚ä½•ä½¿ç”¨æ‰°åŠ¨æœºåˆ¶é€šè¿‡æœ‰æ ‡ç­¾æ•°æ®å¼•å¯¼æ— æ ‡ç­¾æ•°æ®çš„å­¦ä¹ ï¼ŒäºŒæ˜¯å¦‚ä½•ç¡®ä¿è¾¹ç•ŒåŒºåŸŸçš„å‡†ç¡®é¢„æµ‹ã€‚å—äººç±»æ¸è¿›å¼å’Œå‘¨æœŸæ€§å­¦ä¹ è¿‡ç¨‹çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ¸è¿›å¼å’Œå‘¨æœŸæ€§æ‰°åŠ¨æœºåˆ¶ï¼ˆP3Mï¼‰å’Œä¸€ç§è¾¹ç•Œèšç„¦æŸå¤±ã€‚P3Mèƒ½å¤ŸåŠ¨æ€è°ƒæ•´æ‰°åŠ¨ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿé€æ­¥å­¦ä¹ å®ƒä»¬ã€‚æˆ‘ä»¬çš„è¾¹ç•Œèšç„¦æŸå¤±é¼“åŠ±æ¨¡å‹ä¸“æ³¨äºè¾¹ç•ŒåŒºåŸŸï¼Œæé«˜å¯¹å¤æ‚ç»†èŠ‚çš„æ•æ„Ÿæ€§ï¼Œå¹¶ç¡®ä¿å‡†ç¡®é¢„æµ‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨äºŒç»´å’Œä¸‰ç»´æ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒP3Må¯æ‰©å±•åˆ°å…¶ä»–æ–¹æ³•ï¼Œæ‰€æå‡ºçš„æŸå¤±å¯ä½œä¸ºæ”¹è¿›ç°æœ‰æ–¹æ³•çš„é€šç”¨å·¥å…·ï¼Œçªå‡ºäº†æˆ‘ä»¬æ–¹æ³•çš„å¯æ‰©å±•æ€§å’Œé€‚ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15861v1">PDF</a> </p>
<p><strong>Summary</strong><br>     åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­åˆ©ç”¨å¤šæ ·æ— æ ‡ç­¾æ•°æ®çš„æ‰°åŠ¨å·²è¯æ˜æœ‰ç›Šã€‚æœ¬æ–‡æå‡ºä¸€ç§æ¸è¿›å¼å‘¨æœŸæ‰°åŠ¨æœºåˆ¶ï¼ˆP3Mï¼‰å’Œè¾¹ç•Œèšç„¦æŸå¤±ï¼Œæ—¨åœ¨è§£å†³å¦‚ä½•åˆ©ç”¨æ‰°åŠ¨æœºåˆ¶é€šè¿‡æœ‰æ ‡ç­¾æ•°æ®å¼•å¯¼æ— æ ‡ç­¾æ•°æ®å­¦ä¹ ï¼Œä»¥åŠå¦‚ä½•åœ¨è¾¹ç•ŒåŒºåŸŸç¡®ä¿å‡†ç¡®é¢„æµ‹çš„æŒ‘æˆ˜ã€‚è¯¥æœºåˆ¶å¯åŠ¨æ€è°ƒæ•´æ‰°åŠ¨ï¼Œä½¿æ¨¡å‹é€æ­¥å­¦ä¹ ã€‚è¾¹ç•Œèšç„¦æŸå¤±é¼“åŠ±æ¨¡å‹å…³æ³¨è¾¹ç•ŒåŒºåŸŸï¼Œæé«˜æ•æ„Ÿåº¦å’Œé¢„æµ‹å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨äºŒç»´å’Œä¸‰ç»´æ•°æ®é›†ä¸Šå‡è¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚æ­¤å¤–ï¼ŒP3Må¯æ‰©å±•åˆ°å…¶ä»–æ–¹æ³•ï¼Œæ‰€æå‡ºçš„æŸå¤±å¯ä½œä¸ºæ”¹è¿›ç°æœ‰æ–¹æ³•çš„é€šç”¨å·¥å…·ï¼Œå‡¸æ˜¾äº†æ–¹æ³•çš„å¯æ‰©å±•æ€§å’Œé€‚ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ ·æ— æ ‡ç­¾æ•°æ®çš„æ‰°åŠ¨å¯¹åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²æœ‰ç›Šã€‚</li>
<li>æå‡ºæ¸è¿›å¼å‘¨æœŸæ‰°åŠ¨æœºåˆ¶ï¼ˆP3Mï¼‰ä»¥åŠ¨æ€è°ƒæ•´æ‰°åŠ¨ï¼Œä½¿æ¨¡å‹é€æ­¥å­¦ä¹ ã€‚</li>
<li>å¼•å…¥è¾¹ç•Œèšç„¦æŸå¤±ï¼Œä»¥æé«˜æ¨¡å‹å¯¹è¾¹ç•ŒåŒºåŸŸçš„æ•æ„Ÿåº¦å’Œé¢„æµ‹å‡†ç¡®æ€§ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•åœ¨äºŒç»´å’Œä¸‰ç»´æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>P3Må¯æ‰©å±•åˆ°å…¶ä»–æ–¹æ³•ï¼Œæ˜¾ç¤ºå‡ºå…¶é€‚ç”¨æ€§ã€‚</li>
<li>æ‰€æå‡ºçš„æŸå¤±å‡½æ•°å¯ä½œä¸ºæ”¹è¿›ç°æœ‰æ–¹æ³•çš„é€šç”¨å·¥å…·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15861">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_åŒ»å­¦å›¾åƒ/2505.15861v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_åŒ»å­¦å›¾åƒ/2505.15861v1/page_2_0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Deep-Learning-Enabled-Segmentation-Classification-and-Risk-Assessment-of-Cervical-Cancer"><a href="#Deep-Learning-Enabled-Segmentation-Classification-and-Risk-Assessment-of-Cervical-Cancer" class="headerlink" title="Deep Learning Enabled Segmentation, Classification and Risk Assessment   of Cervical Cancer"></a>Deep Learning Enabled Segmentation, Classification and Risk Assessment   of Cervical Cancer</h2><p><strong>Authors:Abdul Samad Shaik, Shashaank Mattur Aswatha, Rahul Jashvantbhai Pandya</strong></p>
<p>Cervical cancer, the fourth leading cause of cancer in women globally, requires early detection through Pap smear tests to identify precancerous changes and prevent disease progression. In this study, we performed a focused analysis by segmenting the cellular boundaries and drawing bounding boxes to isolate the cancer cells. A novel Deep Learning (DL) architecture, the &#96;&#96;Multi-Resolution Fusion Deep Convolutional Networkâ€, was proposed to effectively handle images with varying resolutions and aspect ratios, with its efficacy showcased using the SIPaKMeD dataset. The performance of this DL model was observed to be similar to the state-of-the-art models, with accuracy variations of a mere 2% to 3%, achieved using just 1.7 million learnable parameters, which is approximately 85 times less than the VGG-19 model. Furthermore, we introduced a multi-task learning technique that simultaneously performs segmentation and classification tasks and begets an Intersection over Union score of 0.83 and a classification accuracy of 90%. The final stage of the workflow employs a probabilistic approach for risk assessment, extracting feature vectors to predict the likelihood of normal cells progressing to malignant states, which can be utilized for the prognosis of cervical cancer. </p>
<blockquote>
<p>å®«é¢ˆç™Œæ˜¯å…¨çƒå¥³æ€§ç¬¬å››å¤§å¸¸è§ç™Œç—‡ï¼Œéœ€è¦é€šè¿‡æ¶‚ç‰‡æ£€æŸ¥è¿›è¡Œæ—©æœŸæ£€æµ‹ï¼Œä»¥è¯†åˆ«ç™Œå‰ç—…å˜å¹¶é˜²æ­¢ç–¾ç—…è¿›å±•ã€‚æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡åˆ†å‰²ç»†èƒè¾¹ç•Œå’Œç»˜åˆ¶è¾¹ç•Œæ¡†æ¥è¿›è¡Œæœ‰é’ˆå¯¹æ€§çš„åˆ†æï¼Œä»¥éš”ç¦»ç™Œç»†èƒã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰æ¶æ„â€”â€”â€œå¤šåˆ†è¾¨ç‡èåˆæ·±åº¦å·ç§¯ç½‘ç»œâ€ï¼Œè¯¥æ¶æ„èƒ½å¤Ÿæœ‰æ•ˆå¤„ç†ä¸åŒåˆ†è¾¨ç‡å’Œé•¿å®½æ¯”çš„å›¾åƒï¼Œå…¶æ•ˆèƒ½é€šè¿‡ä½¿ç”¨SIPaKMeDæ•°æ®é›†å¾—åˆ°äº†å±•ç¤ºã€‚æ­¤DLæ¨¡å‹çš„è¡¨ç°ä¸æœ€å…ˆè¿›çš„æ¨¡å‹ç›¸ä¼¼ï¼Œä»…ä½¿ç”¨170ä¸‡å¯å­¦ä¹ å‚æ•°å°±è¾¾åˆ°äº†ä»…ç›¸å·®2%è‡³3%çš„å‡†ç¡®ç‡ï¼Œå¤§çº¦æ˜¯VGG-19æ¨¡å‹çš„85åˆ†ä¹‹ä¸€ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¤šä»»åŠ¡å­¦ä¹ æŠ€æœ¯ï¼Œå¯ä»¥åŒæ—¶æ‰§è¡Œåˆ†å‰²å’Œåˆ†ç±»ä»»åŠ¡ï¼Œè¾¾åˆ°0.83çš„äº¤å¹¶æ¯”å’Œ90%çš„åˆ†ç±»å‡†ç¡®ç‡ã€‚å·¥ä½œæµç¨‹çš„æœ€åé˜¶æ®µé‡‡ç”¨æ¦‚ç‡æ–¹æ³•è¿›è¡Œé£é™©è¯„ä¼°ï¼Œé€šè¿‡æå–ç‰¹å¾å‘é‡æ¥é¢„æµ‹æ­£å¸¸ç»†èƒå‘å±•ä¸ºæ¶æ€§çŠ¶æ€çš„å¯èƒ½æ€§ï¼Œè¿™å¯ç”¨äºå®«é¢ˆç™Œçš„é¢„åã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15505v1">PDF</a> 11 pages, 10 figures</p>
<p><strong>Summary</strong><br>     è¯¥ç ”ç©¶åˆ©ç”¨æ·±åº¦å­¦ä¹ æŠ€æœ¯ï¼Œé€šè¿‡ç»†èƒè¾¹ç•Œåˆ†å‰²å’ŒåŒ…å›´ç›’ç»˜åˆ¶æ¥è¯†åˆ«å®«é¢ˆç™Œç»†èƒã€‚æå‡ºä¸€ç§æ–°é¢–çš„å¤šåˆ†è¾¨ç‡èåˆæ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œæ¶æ„ï¼Œæœ‰æ•ˆå¤„ç†ä¸åŒåˆ†è¾¨ç‡å’Œé•¿å®½æ¯”çš„å›¾åƒï¼Œå¹¶ä½¿ç”¨SIPaKMeDæ•°æ®é›†éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚è¯¥æ¨¡å‹æ€§èƒ½ä¸æœ€æ–°æ¨¡å‹ç›¸ä¼¼ï¼Œä»…ä½¿ç”¨çº¦170ä¸‡ä¸ªå¯å­¦ä¹ å‚æ•°ä¾¿å®ç°äº†ä»…ç›¸å·®ç™¾åˆ†ä¹‹äºŒè‡³ä¸‰çš„ç²¾åº¦ï¼Œçº¦ä¸ºVGG-19æ¨¡å‹çš„85å€ã€‚æ­¤å¤–ï¼Œå¼•å…¥å¤šä»»åŠ¡å­¦ä¹ æŠ€æœ¯åŒæ—¶æ‰§è¡Œåˆ†å‰²å’Œåˆ†ç±»ä»»åŠ¡ï¼Œè·å¾—äº¤å¹¶æ¯”åˆ†æ•°ä¸º0.83å’Œåˆ†ç±»å‡†ç¡®åº¦ä¸ºç™¾åˆ†ä¹‹ä¹åã€‚æœ€åé˜¶æ®µé‡‡ç”¨æ¦‚ç‡æ–¹æ³•è¿›è¡Œé£é™©è¯„ä¼°ï¼Œé€šè¿‡æå–ç‰¹å¾å‘é‡é¢„æµ‹æ­£å¸¸ç»†èƒå‘å±•ä¸ºæ¶æ€§çŠ¶æ€çš„å¯èƒ½æ€§ï¼Œå¯ç”¨äºé¢„æµ‹å®«é¢ˆç™Œçš„é¢„åã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å®«é¢ˆç™Œæ˜¯å…¨çƒå¥³æ€§ç¬¬å››å¤§å¸¸è§ç™Œç—‡ï¼Œæ—©æœŸæ£€æµ‹è‡³å…³é‡è¦ï¼Œå¯é€šè¿‡æ¶‚ç‰‡æ£€æŸ¥å‘ç°ç™Œå‰ç—…å˜ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨æ·±åº¦å­¦ä¹ æŠ€æœ¯è¯†åˆ«å®«é¢ˆç™Œç»†èƒï¼Œé€šè¿‡ç»†èƒè¾¹ç•Œåˆ†å‰²å’ŒåŒ…å›´ç›’ç»˜åˆ¶è¿›è¡Œç»†è‡´åˆ†æã€‚</li>
<li>æå‡ºä¸€ç§æ–°é¢–çš„å¤šåˆ†è¾¨ç‡èåˆæ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œæ¶æ„ï¼Œèƒ½æœ‰æ•ˆå¤„ç†ä¸åŒåˆ†è¾¨ç‡å’Œé•¿å®½æ¯”çš„å›¾åƒã€‚</li>
<li>è¯¥æ·±åº¦å­¦ä¹ æ¨¡å‹æ€§èƒ½ä¸æœ€æ–°æŠ€æœ¯ç›¸å½“ï¼Œä½¿ç”¨è¾ƒå°‘çš„å¯å­¦ä¹ å‚æ•°å®ç°äº†é«˜å‡†ç¡®ç‡ã€‚</li>
<li>å¤šä»»åŠ¡å­¦ä¹ æŠ€æœ¯åŒæ—¶æ‰§è¡Œåˆ†å‰²å’Œåˆ†ç±»ä»»åŠ¡ï¼Œè·å¾—è‰¯å¥½çš„äº¤å¹¶æ¯”åˆ†æ•°å’Œåˆ†ç±»å‡†ç¡®åº¦ã€‚</li>
<li>æ¦‚ç‡æ–¹æ³•ç”¨äºé£é™©è¯„ä¼°ï¼Œå¯é¢„æµ‹æ­£å¸¸ç»†èƒè½¬å˜ä¸ºæ¶æ€§çŠ¶æ€çš„å¯èƒ½æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15505">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_åŒ»å­¦å›¾åƒ/2505.15505v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_åŒ»å­¦å›¾åƒ/2505.15505v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_åŒ»å­¦å›¾åƒ/2505.15505v1/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_åŒ»å­¦å›¾åƒ/2505.15505v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_åŒ»å­¦å›¾åƒ/2505.15505v1/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_åŒ»å­¦å›¾åƒ/2505.15505v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_åŒ»å­¦å›¾åƒ/2505.15505v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_åŒ»å­¦å›¾åƒ/2505.15505v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="WISP-Image-Segmentation-Based-Whitespace-Diagnosis-for-Optimal-Rectilinear-Floorplanning"><a href="#WISP-Image-Segmentation-Based-Whitespace-Diagnosis-for-Optimal-Rectilinear-Floorplanning" class="headerlink" title="WISP: Image Segmentation-Based Whitespace Diagnosis for Optimal   Rectilinear Floorplanning"></a>WISP: Image Segmentation-Based Whitespace Diagnosis for Optimal   Rectilinear Floorplanning</h2><p><strong>Authors:Xiaotian Zhao, Zixuan Li, Yichen Cai, Xinfei Guo</strong></p>
<p>The increasing number of rectilinear floorplans in modern chip designs presents significant challenges for traditional macro placers due to the additional complexity introduced by blocked corners. Particularly, the widely adopted wirelength model Half-Perimeter Wirelength (HPWL) struggles to accurately handle rectilinear boundaries, highlighting the need for additional objectives tailored to rectilinear floorplan optimization. In this paper, we identify the necessity for whitespace diagnosis in rectilinear floorplanning, an aspect often overlooked in past research. We introduce WISP, a novel framework that analyzes and scores whitespace regions to guide placement optimization. WISP leverages image segmentation techniques for whitespace parsing, a lightweight probabilistic model to score whitespace regions based on macro distribution, a Gaussian Mixture Model (GMM) for whitespace density scoring and direction-aware macro relocation to iteratively refine macro placement, reduce wasted whitespace, and enhance design quality. The proposed diagnostic technique also enables the reclamation of block-level unused area and its return to the top level, maximizing overall area utilization. When compared against state-of-the-art academia placer DREAMPlace 4.1, our method achieves an average improvement of 5.4% in routing wirelength, with a maximum of 11.4% across widely-used benchmarks. This yields an average of 41.5% and 43.7% improvement in Worst Negative Slack (WNS) and Total Negative Slack (TNS), respectively. Additionally, WISP recycles an average of 16.2% area at the block level, contributing to more efficient top-level area distribution. </p>
<blockquote>
<p>åœ¨ç°ä»£èŠ¯ç‰‡è®¾è®¡ä¸­ï¼ŒçŸ©å½¢å¹³é¢è®¾è®¡æ•°é‡çš„å¢åŠ ç»™ä¼ ç»Ÿå®æ”¾ç½®å™¨å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ï¼Œå› ä¸ºç”±äºé˜»å¡è§’è½çš„å¼•å…¥è€Œå¢åŠ äº†å¤æ‚æ€§ã€‚å°¤å…¶æ˜¯å¹¿æ³›é‡‡ç”¨çš„çº¿é•¿æ¨¡å‹åŠå‘¨é•¿çº¿é•¿ï¼ˆHPWLï¼‰å¾ˆéš¾å‡†ç¡®å¤„ç†çŸ©å½¢è¾¹ç•Œï¼Œè¿™çªæ˜¾äº†éœ€è¦é’ˆå¯¹çŸ©å½¢å¹³é¢è®¾è®¡ä¼˜åŒ–çš„é™„åŠ ç›®æ ‡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç¡®å®šäº†çŸ©å½¢å¹³é¢è§„åˆ’ä¸­ç©ºç™½åŒºåŸŸè¯Šæ–­çš„å¿…è¦æ€§ï¼Œè¿™æ˜¯è¿‡å»ç ”ç©¶ä¸­ç»å¸¸è¢«å¿½è§†çš„ä¸€ä¸ªæ–¹é¢ã€‚æˆ‘ä»¬ä»‹ç»äº†WISPï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„æ¡†æ¶ï¼Œå®ƒåˆ†æå’Œè¯„ä¼°ç©ºç™½åŒºåŸŸä»¥å¼•å¯¼æ”¾ç½®ä¼˜åŒ–ã€‚WISPåˆ©ç”¨å›¾åƒåˆ†å‰²æŠ€æœ¯è¿›è¡Œç©ºç™½åŒºåŸŸè§£æï¼Œé‡‡ç”¨è½»é‡çº§æ¦‚ç‡æ¨¡å‹æ ¹æ®å®åˆ†å¸ƒå¯¹ç©ºç™½åŒºåŸŸè¿›è¡Œè¯„åˆ†ï¼Œä½¿ç”¨é«˜æ–¯æ··åˆæ¨¡å‹ï¼ˆGMMï¼‰è¿›è¡Œç©ºç™½åŒºåŸŸå¯†åº¦è¯„åˆ†ï¼Œå¹¶æä¾›æ–¹å‘æ„ŸçŸ¥å®é‡æ–°å®šä½ï¼Œä»¥è¿­ä»£åœ°ä¼˜åŒ–å®æ”¾ç½®ã€å‡å°‘æµªè´¹çš„ç©ºç™½åŒºåŸŸå¹¶æé«˜è®¾è®¡è´¨é‡ã€‚æ‰€æå‡ºçš„è¯Šæ–­æŠ€æœ¯è¿˜å¯ç”¨äº†å—çº§æœªä½¿ç”¨åŒºåŸŸçš„å›æ”¶å¹¶å°†å…¶è¿”å›åˆ°é¡¶å±‚ï¼Œä»è€Œæœ€å¤§é™åº¦åœ°æé«˜äº†æ•´ä½“åŒºåŸŸåˆ©ç”¨ç‡ã€‚ä¸æœ€æ–°çš„å­¦æœ¯æ”¾ç½®å™¨DREAMPlace 4.1ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è·¯ç”±çº¿é•¿ä¸Šå¹³å‡æé«˜äº†5.4%ï¼Œåœ¨å¹¿æ³›ä½¿ç”¨çš„åŸºå‡†æµ‹è¯•ä¸­æœ€é«˜æé«˜äº†11.4%ã€‚è¿™å¯¼è‡´æœ€åè´Ÿæ—¶å·®ï¼ˆWNSï¼‰å’Œæ€»è´Ÿæ—¶å·®ï¼ˆTNSï¼‰å¹³å‡åˆ†åˆ«æé«˜äº†41.5%å’Œ43.7%ã€‚æ­¤å¤–ï¼ŒWISPåœ¨å—çº§åˆ«å¹³å‡å›æ”¶äº†16.2%çš„åŒºåŸŸï¼Œæœ‰åŠ©äºæ›´æœ‰æ•ˆåœ°åˆ†é…é¡¶å±‚åŒºåŸŸã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15271v1">PDF</a> </p>
<p><strong>Summary</strong><br>     é’ˆå¯¹ç°ä»£èŠ¯ç‰‡è®¾è®¡ä¸­çŸ©å½¢å¸ƒå±€ä¸æ–­å¢åŠ æ‰€å¸¦æ¥çš„å¤æ‚æ€§æŒ‘æˆ˜ï¼Œä¼ ç»Ÿå®æ”¾ç½®å™¨é¢ä¸´å›°å¢ƒã€‚æœ¬æ–‡æå‡ºä¸€ç§åä¸ºWISPçš„æ–°å‹æ¡†æ¶ï¼Œç”¨äºåˆ†æå’Œè¯„åˆ†ç©ºç™½åŒºåŸŸï¼Œä»¥æŒ‡å¯¼æ”¾ç½®ä¼˜åŒ–ã€‚WISPåˆ©ç”¨å›¾åƒåˆ†å‰²æŠ€æœ¯è¿›è¡Œç©ºç™½åŒºåŸŸè§£æï¼Œé‡‡ç”¨è½»é‡çº§æ¦‚ç‡æ¨¡å‹å¯¹ç©ºç™½åŒºåŸŸè¿›è¡Œè¯„åˆ†ï¼Œå¹¶åˆ©ç”¨é«˜æ–¯æ··åˆæ¨¡å‹è¿›è¡Œç©ºç™½åŒºåŸŸå¯†åº¦è¯„åˆ†å’Œæ–¹å‘æ„ŸçŸ¥å®ç§»ä½ï¼Œä»¥è¿­ä»£ä¼˜åŒ–å®æ”¾ç½®ã€å‡å°‘æµªè´¹çš„ç©ºç™½åŒºåŸŸå¹¶æé«˜è®¾è®¡è´¨é‡ã€‚æ­¤å¤–ï¼Œè¯¥æŠ€æœ¯è¿˜èƒ½å›æ”¶å—çº§æœªä½¿ç”¨çš„åŒºåŸŸå¹¶å°†å…¶è¿”å›ç»™é¡¶å±‚ï¼Œä»¥å®ç°æœ€å¤§é¢ç§¯åˆ©ç”¨ç‡ã€‚ä¸ç°æœ‰çš„æœ€å…ˆè¿›çš„å­¦æœ¯ç•Œæ”¾ç½®å™¨DREAMPlace 4.1ç›¸æ¯”ï¼ŒWISPåœ¨è·¯ç”±çº¿é•¿ä¸Šå¹³å‡æé«˜äº†5.4%ï¼Œåœ¨æœ€å¸¸ç”¨çš„åŸºå‡†æµ‹è¯•ä¸­æœ€é«˜æé«˜äº†11.4%ã€‚æ­¤å¤–ï¼ŒWISPåœ¨å—çº§åˆ«å¹³å‡å›æ”¶äº†16.2%çš„é¢ç§¯ï¼Œæœ‰åŠ©äºæé«˜é¡¶å±‚é¢ç§¯åˆ†å¸ƒçš„åˆ©ç”¨ç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°ä»£èŠ¯ç‰‡è®¾è®¡çš„çŸ©å½¢å¸ƒå±€ç»™ä¼ ç»Ÿå®æ”¾ç½®å™¨å¸¦æ¥äº†æŒ‘æˆ˜ï¼Œéœ€è¦é’ˆå¯¹çŸ©å½¢å¸ƒå±€ä¼˜åŒ–çš„æ–°ç­–ç•¥ã€‚</li>
<li>WISPæ¡†æ¶é€šè¿‡åˆ†æå¹¶è¯„åˆ†ç©ºç™½åŒºåŸŸæ¥ä¼˜åŒ–å®æ”¾ç½®ã€‚</li>
<li>WISPåˆ©ç”¨å›¾åƒåˆ†å‰²æŠ€æœ¯ã€æ¦‚ç‡æ¨¡å‹å’Œé«˜æ–¯æ··åˆæ¨¡å‹è¿›è¡Œç©ºç™½åŒºåŸŸè§£æã€è¯„åˆ†å’Œå¯†åº¦è¯„ä¼°ã€‚</li>
<li>WISPé€šè¿‡è¿­ä»£ä¼˜åŒ–å®æ”¾ç½®ï¼Œå‡å°‘äº†ç©ºç™½åŒºåŸŸçš„æµªè´¹ï¼Œæé«˜äº†è®¾è®¡è´¨é‡ã€‚</li>
<li>WISPèƒ½å¤Ÿå®ç°å—çº§æœªä½¿ç”¨åŒºåŸŸçš„å›æ”¶ï¼Œå¹¶æœ€å¤§åŒ–æ•´ä½“é¢ç§¯åˆ©ç”¨ç‡ã€‚</li>
<li>ä¸DREAMPlace 4.1ç›¸æ¯”ï¼ŒWISPåœ¨è·¯ç”±çº¿é•¿ä¸Šæœ‰æ‰€æ”¹è¿›ï¼Œå¹³å‡æé«˜äº†5.4%ï¼Œåœ¨æŸäº›åŸºå‡†æµ‹è¯•ä¸­æœ€é«˜è¾¾åˆ°äº†11.4%çš„æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15271">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_åŒ»å­¦å›¾åƒ/2505.15271v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_åŒ»å­¦å›¾åƒ/2505.15271v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_åŒ»å­¦å›¾åƒ/2505.15271v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_åŒ»å­¦å›¾åƒ/2505.15271v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_åŒ»å­¦å›¾åƒ/2505.15271v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_åŒ»å­¦å›¾åƒ/2505.15271v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Zero-Shot-Gaze-based-Volumetric-Medical-Image-Segmentation"><a href="#Zero-Shot-Gaze-based-Volumetric-Medical-Image-Segmentation" class="headerlink" title="Zero-Shot Gaze-based Volumetric Medical Image Segmentation"></a>Zero-Shot Gaze-based Volumetric Medical Image Segmentation</h2><p><strong>Authors:Tatyana Shmykova, Leila Khaertdinova, Ilya Pershin</strong></p>
<p>Accurate segmentation of anatomical structures in volumetric medical images is crucial for clinical applications, including disease monitoring and cancer treatment planning. Contemporary interactive segmentation models, such as Segment Anything Model 2 (SAM-2) and its medical variant (MedSAM-2), rely on manually provided prompts like bounding boxes and mouse clicks. In this study, we introduce eye gaze as a novel informational modality for interactive segmentation, marking the application of eye-tracking for 3D medical image segmentation. We evaluate the performance of using gaze-based prompts with SAM-2 and MedSAM-2 using both synthetic and real gaze data. Compared to bounding boxes, gaze-based prompts offer a time-efficient interaction approach with slightly lower segmentation quality. Our findings highlight the potential of using gaze as a complementary input modality for interactive 3D medical image segmentation. </p>
<blockquote>
<p>åœ¨ä¸‰ç»´åŒ»å­¦å›¾åƒä¸­å¯¹è§£å‰–ç»“æ„è¿›è¡Œç²¾ç¡®åˆ†å‰²å¯¹äºä¸´åºŠåº”ç”¨è‡³å…³é‡è¦ï¼ŒåŒ…æ‹¬ç–¾ç—…ç›‘æµ‹å’Œç™Œç—‡æ²»ç–—è®¡åˆ’ã€‚å½“ä»£çš„äº¤äº’å¼åˆ†å‰²æ¨¡å‹ï¼Œå¦‚Segment Anything Model 2ï¼ˆSAM-2ï¼‰åŠå…¶åŒ»å­¦å˜ä½“ï¼ˆMedSAM-2ï¼‰ï¼Œä¾èµ–äºæ‰‹åŠ¨æä¾›çš„æç¤ºä¿¡æ¯ï¼Œå¦‚è¾¹ç•Œæ¡†å’Œé¼ æ ‡ç‚¹å‡»ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥çœ¼åŠ¨è¿½è¸ªä½œä¸ºä¸€ç§æ–°å‹ä¿¡æ¯äº¤äº’æ¨¡å¼ï¼Œç”¨äºäº¤äº’å¼åˆ†å‰²ï¼Œæ ‡å¿—ç€çœ¼åŠ¨è¿½è¸ªåœ¨ä¸‰ç»´åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„åº”ç”¨ã€‚æˆ‘ä»¬ä½¿ç”¨åˆæˆå’ŒçœŸå®çœ¼åŠ¨æ•°æ®è¯„ä¼°äº†åŸºäºçœ¼åŠ¨æç¤ºçš„SAM-2å’ŒMedSAM-2çš„æ€§èƒ½ã€‚ä¸è¾¹ç•Œæ¡†ç›¸æ¯”ï¼ŒåŸºäºçœ¼åŠ¨æç¤ºçš„æ–¹æ³•æä¾›äº†ä¸€ç§æ—¶é—´æ•ˆç‡æ›´é«˜çš„äº¤äº’æ–¹å¼ï¼Œä½†åˆ†å‰²è´¨é‡ç•¥æœ‰ä¸‹é™ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œçœ¼åŠ¨ä½œä¸ºä¸€ç§è¡¥å……è¾“å…¥æ¨¡å¼åœ¨äº¤äº’å¼ä¸‰ç»´åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­å…·æœ‰æ½œåœ¨åº”ç”¨ä»·å€¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15256v1">PDF</a> Accepted to MMFM-BIOMED Workshop @ CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨åŒ»å­¦å›¾åƒé¢†åŸŸä¸­ä½¿ç”¨çœ¼åŠ¨è¿½è¸ªæŠ€æœ¯è¿›è¡Œäº¤äº’å¼åˆ†å‰²ç ”ç©¶çš„é‡è¦æ€§åŠå…¶åº”ç”¨åœºæ™¯ã€‚é€šè¿‡å¯¹SAM-2å’ŒMedSAM-2ç­‰æ¨¡å‹çš„ç ”ç©¶ï¼Œå°†çœ¼åŠ¨ä½œä¸ºæ–°çš„ä¿¡æ¯æ¨¡å¼ç”¨äºäº¤äº’å¼åˆ†å‰²ï¼Œä½¿ç”¨åˆæˆå’ŒçœŸå®çœ¼åŠ¨æ•°æ®å¯¹åŸºäºçœ¼åŠ¨çš„æç¤ºè¿›è¡Œæ€§èƒ½è¯„ä¼°ã€‚ç»“æœæ˜¾ç¤ºï¼Œç›¸è¾ƒäºè¾¹ç•Œæ¡†æç¤ºï¼ŒåŸºäºçœ¼åŠ¨çš„æç¤ºèƒ½æé«˜äº¤äº’æ•ˆç‡å¹¶ç¨å¾®é™ä½åˆ†å‰²è´¨é‡ï¼Œè¯æ˜äº†å…¶åœ¨äº¤äº’å¼ä¸‰ç»´åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„æ½œåœ¨åº”ç”¨ä»·å€¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²åœ¨ä¸´åºŠåº”ç”¨å¦‚ç–¾ç—…ç›‘æµ‹å’Œç™Œç—‡æ²»ç–—è§„åˆ’ä¸­éå¸¸é‡è¦ã€‚</li>
<li>ç°æœ‰çš„äº¤äº’å¼åˆ†å‰²æ¨¡å‹ä¾èµ–äºæ‰‹åŠ¨æç¤ºå¦‚è¾¹ç•Œæ¡†å’Œé¼ æ ‡ç‚¹å‡»ã€‚</li>
<li>ç ”ç©¶å¼•å…¥äº†çœ¼åŠ¨è¿½è¸ªä½œä¸ºä¸€ç§æ–°çš„äº¤äº’å¼åˆ†å‰²çš„ä¿¡æ¯æ¨¡å¼ã€‚</li>
<li>é€šè¿‡åˆæˆå’ŒçœŸå®çœ¼åŠ¨æ•°æ®å¯¹çœ¼åŠ¨æç¤ºæ€§èƒ½è¿›è¡Œè¯„ä¼°ã€‚</li>
<li>åŸºäºçœ¼åŠ¨çš„æç¤ºèƒ½æé«˜äº¤äº’æ•ˆç‡ã€‚</li>
<li>åŸºäºçœ¼åŠ¨çš„æç¤ºå¯¹åˆ†å‰²è´¨é‡çš„å½±å“ç•¥ä½äºä¼ ç»Ÿæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15256">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_åŒ»å­¦å›¾åƒ/2505.15256v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_åŒ»å­¦å›¾åƒ/2505.15256v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_åŒ»å­¦å›¾åƒ/2505.15256v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_åŒ»å­¦å›¾åƒ/2505.15256v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_åŒ»å­¦å›¾åƒ/2505.15256v1/page_3_1.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="CAD-A-General-Multimodal-Framework-for-Video-Deepfake-Detection-via-Cross-Modal-Alignment-and-Distillation"><a href="#CAD-A-General-Multimodal-Framework-for-Video-Deepfake-Detection-via-Cross-Modal-Alignment-and-Distillation" class="headerlink" title="CAD: A General Multimodal Framework for Video Deepfake Detection via   Cross-Modal Alignment and Distillation"></a>CAD: A General Multimodal Framework for Video Deepfake Detection via   Cross-Modal Alignment and Distillation</h2><p><strong>Authors:Yuxuan Du, Zhendong Wang, Yuhao Luo, Caiyong Piao, Zhiyuan Yan, Hao Li, Li Yuan</strong></p>
<p>The rapid emergence of multimodal deepfakes (visual and auditory content are manipulated in concert) undermines the reliability of existing detectors that rely solely on modality-specific artifacts or cross-modal inconsistencies. In this work, we first demonstrate that modality-specific forensic traces (e.g., face-swap artifacts or spectral distortions) and modality-shared semantic misalignments (e.g., lip-speech asynchrony) offer complementary evidence, and that neglecting either aspect limits detection performance. Existing approaches either naively fuse modality-specific features without reconciling their conflicting characteristics or focus predominantly on semantic misalignment at the expense of modality-specific fine-grained artifact cues. To address these shortcomings, we propose a general multimodal framework for video deepfake detection via Cross-Modal Alignment and Distillation (CAD). CAD comprises two core components: 1) Cross-modal alignment that identifies inconsistencies in high-level semantic synchronization (e.g., lip-speech mismatches); 2) Cross-modal distillation that mitigates feature conflicts during fusion while preserving modality-specific forensic traces (e.g., spectral distortions in synthetic audio). Extensive experiments on both multimodal and unimodal (e.g., image-only&#x2F;video-only)deepfake benchmarks demonstrate that CAD significantly outperforms previous methods, validating the necessity of harmonious integration of multimodal complementary information. </p>
<blockquote>
<p>å¤šåª’ä½“æ·±åº¦ä¼ªé€ ï¼ˆè§†è§‰å’Œå¬è§‰å†…å®¹åŒæ­¥æ“çºµï¼‰çš„è¿…é€Ÿå‡ºç°ç ´åäº†ç°æœ‰æ£€æµ‹å™¨çš„å¯é æ€§ï¼Œè¿™äº›æ£€æµ‹å™¨è¦ä¹ˆä¾èµ–äºç‰¹å®šæ¨¡æ€çš„ä¼ªè¿¹ï¼Œè¦ä¹ˆä¾èµ–äºè·¨æ¨¡æ€çš„ä¸ä¸€è‡´æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆè¯æ˜ç‰¹å®šæ¨¡æ€çš„æ³•åŒ»ç—•è¿¹ï¼ˆä¾‹å¦‚é¢éƒ¨æ›¿æ¢ä¼ªè¿¹æˆ–å…‰è°±å¤±çœŸï¼‰å’Œæ¨¡æ€å…±äº«çš„è¯­ä¹‰ä¸åŒ¹é…ï¼ˆä¾‹å¦‚å”‡è¯­ä¸åŒæ­¥ï¼‰æä¾›äº†äº’è¡¥è¯æ®ï¼Œå¿½ç•¥ä»»ä½•ä¸€æ–¹é¢éƒ½ä¼šé™åˆ¶æ£€æµ‹æ€§èƒ½ã€‚ç°æœ‰æ–¹æ³•è¦ä¹ˆç®€å•åœ°èåˆç‰¹å®šæ¨¡æ€çš„ç‰¹å¾ï¼Œè€Œæ²¡æœ‰è§£å†³å…¶å†²çªç‰¹å¾ï¼Œè¦ä¹ˆä¸»è¦å…³æ³¨è¯­ä¹‰ä¸åŒ¹é…è€Œå¿½ç•¥äº†ç‰¹å®šæ¨¡æ€çš„ç²¾ç»†ä¼ªè¿¹çº¿ç´¢ã€‚ä¸ºäº†è§£å†³è¿™äº›ä¸è¶³ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè·¨æ¨¡æ€å¯¹é½å’Œè’¸é¦ï¼ˆCADï¼‰çš„è§†é¢‘æ·±åº¦ä¼ªé€ æ£€æµ‹é€šç”¨å¤šæ¨¡æ€æ¡†æ¶ã€‚CADåŒ…å«ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼š1ï¼‰è·¨æ¨¡æ€å¯¹é½ï¼Œç”¨äºè¯†åˆ«é«˜çº§è¯­ä¹‰åŒæ­¥ä¸­çš„ä¸ä¸€è‡´ï¼ˆä¾‹å¦‚å”‡è¯­ä¸åŒ¹é…ï¼‰ï¼›2ï¼‰è·¨æ¨¡æ€è’¸é¦ï¼Œåœ¨èåˆè¿‡ç¨‹ä¸­ç¼“è§£ç‰¹å¾å†²çªï¼ŒåŒæ—¶ä¿ç•™ç‰¹å®šæ¨¡æ€çš„æ³•åŒ»ç—•è¿¹ï¼ˆä¾‹å¦‚åˆæˆéŸ³é¢‘ä¸­çš„å…‰è°±å¤±çœŸï¼‰ã€‚åœ¨å¤šåª’ä½“å’Œå•æ¨¡æ€ï¼ˆä¾‹å¦‚ä»…å›¾åƒ&#x2F;ä»…è§†é¢‘ï¼‰æ·±åº¦ä¼ªé€ åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒCADæ˜¾è‘—ä¼˜äºä»¥å‰çš„æ–¹æ³•ï¼ŒéªŒè¯äº†å’Œè°æ•´åˆå¤šæ¨¡æ€äº’è¡¥ä¿¡æ¯çš„å¿…è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15233v1">PDF</a> </p>
<p><strong>Summary</strong><br>    å¤šåª’ä½“æ·±åº¦ä¼ªé€ æŠ€æœ¯å¿«é€Ÿå‘å±•ï¼Œå¯¹ç°æœ‰æ£€æµ‹å™¨æå‡ºæŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§åŸºäºè·¨æ¨¡æ€å¯¹é½å’Œè’¸é¦ï¼ˆCADï¼‰çš„é€šç”¨å¤šåª’ä½“æ¡†æ¶ï¼Œé€šè¿‡ç»“åˆæ¨¡æ€ç‰¹å®šç—•è¿¹å’Œæ¨¡æ€å…±äº«è¯­ä¹‰ä¸ä¸€è‡´ï¼Œå®ç°è§†é¢‘æ·±åº¦ä¼ªé€ æ£€æµ‹ã€‚CADåŒ…å«ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šè·¨æ¨¡æ€å¯¹é½å’Œè·¨æ¨¡æ€è’¸é¦ã€‚å‰è€…è¯†åˆ«é«˜çº§è¯­ä¹‰åŒæ­¥çš„ä¸ä¸€è‡´æ€§ï¼Œåè€…åœ¨èåˆæ—¶ç¼“è§£ç‰¹å¾å†²çªå¹¶ä¿ç•™æ¨¡æ€ç‰¹å®šç—•è¿¹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€æ·±åº¦ä¼ªé€ æŠ€æœ¯è¿…é€Ÿå´›èµ·ï¼Œå¯¹ç°æœ‰æ£€æµ‹å™¨æ„æˆæŒ‘æˆ˜ã€‚</li>
<li>æ¨¡æ€ç‰¹å®šç—•è¿¹ï¼ˆä¾‹å¦‚é¢éƒ¨æ›¿æ¢ç—•è¿¹æˆ–å…‰è°±å¤±çœŸï¼‰å’Œæ¨¡æ€å…±äº«è¯­ä¹‰ä¸ä¸€è‡´ï¼ˆä¾‹å¦‚å”‡éŸ³ä¸åŒæ­¥ï¼‰æä¾›äº’è¡¥è¯æ®ã€‚</li>
<li>è·¨æ¨¡æ€å¯¹é½æ˜¯è¯†åˆ«é«˜çº§è¯­ä¹‰åŒæ­¥ä¸ä¸€è‡´æ€§çš„å…³é”®ã€‚</li>
<li>è·¨æ¨¡æ€è’¸é¦æœ‰åŠ©äºåœ¨èåˆæ—¶ç¼“è§£ç‰¹å¾å†²çªå¹¶ä¿ç•™æ¨¡æ€ç‰¹å®šç»†èŠ‚ã€‚</li>
<li>æå‡ºçš„CADæ¡†æ¶åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šè·¨æ¨¡æ€å¯¹é½å’Œè·¨æ¨¡æ€è’¸é¦ã€‚</li>
<li>CADæ¡†æ¶æ˜¾è‘—ä¼˜äºä»¥å‰çš„æ–¹æ³•ï¼ŒéªŒè¯äº†å’Œè°æ•´åˆå¤šåª’ä½“äº’è¡¥ä¿¡æ¯çš„å¿…è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15233">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_åŒ»å­¦å›¾åƒ/2505.15233v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_åŒ»å­¦å›¾åƒ/2505.15233v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_åŒ»å­¦å›¾åƒ/2505.15233v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Pathobiological-Dictionary-Defining-Pathomics-and-Texture-Features-Addressing-Understandable-AI-Issues-in-Personalized-Liver-Cancer-Dictionary-Version-LCP1-0"><a href="#Pathobiological-Dictionary-Defining-Pathomics-and-Texture-Features-Addressing-Understandable-AI-Issues-in-Personalized-Liver-Cancer-Dictionary-Version-LCP1-0" class="headerlink" title="Pathobiological Dictionary Defining Pathomics and Texture Features:   Addressing Understandable AI Issues in Personalized Liver Cancer; Dictionary   Version LCP1.0"></a>Pathobiological Dictionary Defining Pathomics and Texture Features:   Addressing Understandable AI Issues in Personalized Liver Cancer; Dictionary   Version LCP1.0</h2><p><strong>Authors:Mohammad R. Salmanpour, Seyed Mohammad Piri, Somayeh Sadat Mehrnia, Ahmad Shariftabrizi, Masume Allahmoradi, Venkata SK. Manem, Arman Rahmim, Ilker Hacihaliloglu</strong></p>
<p>Artificial intelligence (AI) holds strong potential for medical diagnostics, yet its clinical adoption is limited by a lack of interpretability and generalizability. This study introduces the Pathobiological Dictionary for Liver Cancer (LCP1.0), a practical framework designed to translate complex Pathomics and Radiomics Features (PF and RF) into clinically meaningful insights aligned with existing diagnostic workflows. QuPath and PyRadiomics, standardized according to IBSI guidelines, were used to extract 333 imaging features from hepatocellular carcinoma (HCC) tissue samples, including 240 PF-based-cell detection&#x2F;intensity, 74 RF-based texture, and 19 RF-based first-order features. Expert-defined ROIs from the public dataset excluded artifact-prone areas, and features were aggregated at the case level. Their relevance to the WHO grading system was assessed using multiple classifiers linked with feature selectors. The resulting dictionary was validated by 8 experts in oncology and pathology. In collaboration with 10 domain experts, we developed a Pathobiological dictionary of imaging features such as PFs and RF. In our study, the Variable Threshold feature selection algorithm combined with the SVM model achieved the highest accuracy (0.80, P-value less than 0.05), selecting 20 key features, primarily clinical and pathomics traits such as Centroid, Cell Nucleus, and Cytoplasmic characteristics. These features, particularly nuclear and cytoplasmic, were strongly associated with tumor grading and prognosis, reflecting atypia indicators like pleomorphism, hyperchromasia, and cellular orientation.The LCP1.0 provides a clinically validated bridge between AI outputs and expert interpretation, enhancing model transparency and usability. Aligning AI-derived features with clinical semantics supports the development of interpretable, trustworthy diagnostic tools for liver cancer pathology. </p>
<blockquote>
<p>äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰åœ¨åŒ»å­¦è¯Šæ–­æ–¹é¢æ‹¥æœ‰å·¨å¤§æ½œåŠ›ï¼Œä½†å…¶ä¸´åºŠåº”ç”¨å—é™äºç¼ºä¹å¯è§£é‡Šæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†è‚ç™Œç”Ÿç‰©ç—…ç†å­¦è¯å…¸ï¼ˆLCP1.0ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå®ç”¨æ¡†æ¶ï¼Œæ—¨åœ¨å°†å¤æ‚çš„ç—…ç†å­¦å’Œæ”¾å°„å­¦ç‰¹å¾ï¼ˆPFå’ŒRFï¼‰è½¬åŒ–ä¸ºä¸ç°æœ‰è¯Šæ–­å·¥ä½œæµç›¸ä¸€è‡´çš„ã€å…·æœ‰ä¸´åºŠæ„ä¹‰çš„è§è§£ã€‚æ ¹æ®IBSIæŒ‡å—è¿›è¡Œæ ‡å‡†åŒ–çš„QuPathå’ŒPyRadiomicsè¢«ç”¨æ¥ä»è‚ç»†èƒç™Œï¼ˆHCCï¼‰ç»„ç»‡æ ·æœ¬ä¸­æå–333ä¸ªæˆåƒç‰¹å¾ï¼ŒåŒ…æ‹¬åŸºäºPFçš„ç»†èƒæ£€æµ‹&#x2F;å¼ºåº¦ç‰¹å¾240ä¸ªï¼ŒåŸºäºRFçš„çº¹ç†ç‰¹å¾74ä¸ªï¼ŒåŸºäºRFçš„ä¸€é˜¶ç‰¹å¾19ä¸ªã€‚æ¥è‡ªå…¬å…±æ•°æ®é›†çš„ä¸“å®¶å®šä¹‰ROIæ’é™¤äº†æ˜“äº§ç”Ÿä¼ªå½±çš„åŒºåŸŸï¼Œç‰¹å¾åœ¨ç—…ä¾‹å±‚é¢è¿›è¡Œäº†æ±‡æ€»ã€‚å®ƒä»¬ä¸WHOåˆ†çº§ç³»ç»Ÿçš„ç›¸å…³æ€§é€šè¿‡ä½¿ç”¨ä¸ç‰¹å¾é€‰æ‹©å™¨ç›¸å…³è”çš„å¤šé‡åˆ†ç±»å™¨è¿›è¡Œè¯„ä¼°ã€‚è¯¥è¯å…¸å¾—åˆ°äº†8åè‚¿ç˜¤å­¦å’Œç—…ç†å­¦ä¸“å®¶çš„éªŒè¯ã€‚æˆ‘ä»¬ä¸10åé¢†åŸŸä¸“å®¶åˆä½œï¼Œå¼€å‘äº†ç—…ç†ç”Ÿç‰©å­¦è¯å…¸ï¼ŒåŒ…æ‹¬æˆåƒç‰¹å¾ï¼Œå¦‚PFså’ŒRFsã€‚åœ¨æˆ‘ä»¬çš„ç ”ç©¶ä¸­ï¼Œç»“åˆæ”¯æŒå‘é‡æœºï¼ˆSVMï¼‰æ¨¡å‹çš„å˜é‡é˜ˆå€¼ç‰¹å¾é€‰æ‹©ç®—æ³•è¾¾åˆ°äº†æœ€é«˜çš„å‡†ç¡®æ€§ï¼ˆ0.80ï¼ŒPå€¼å°äº0.05ï¼‰ï¼Œé€‰æ‹©äº†20ä¸ªå…³é”®ç‰¹å¾ï¼Œä¸»è¦æ˜¯ä¸´åºŠå’Œç—…ç†ç‰¹å¾ï¼Œå¦‚è´¨å¿ƒã€ç»†èƒæ ¸å’Œç»†èƒè´¨ç‰¹æ€§ã€‚è¿™äº›ç‰¹å¾ï¼Œå°¤å…¶æ˜¯ç»†èƒæ ¸å’Œç»†èƒè´¨ç‰¹å¾ä¸è‚¿ç˜¤åˆ†çº§å’Œé¢„åå¯†åˆ‡ç›¸å…³ï¼Œåæ˜ äº†è¯¸å¦‚å¼‚å‹æ€§ã€è¶…æŸ“è‰²å’Œç»†èƒæ–¹å‘ç­‰ä¸å…¸å‹çš„æŒ‡æ ‡ã€‚LCP1.0ä¸ºAIè¾“å‡ºå’Œä¸“å®¶è§£é‡Šä¹‹é—´æä¾›äº†ä¸´åºŠéªŒè¯çš„æ¡¥æ¢ï¼Œæé«˜äº†æ¨¡å‹çš„é€æ˜åº¦å’Œå¯ç”¨æ€§ã€‚å°†AIè¡ç”Ÿçš„ç‰¹å¾ä¸ä¸´åºŠè¯­ä¹‰ç›¸ç»“åˆï¼Œä¸ºè‚ç™Œç—…ç†å¼€å‘å¯è§£é‡Šã€å¯ä¿¡èµ–çš„è¯Šæ–­å·¥å…·æä¾›äº†æ”¯æŒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14926v1">PDF</a> 29 pages, 4 figures and 1 table</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†äººå·¥æ™ºèƒ½åœ¨åŒ»å­¦è¯Šæ–­ä¸­çš„æ½œåŠ›åŠé™åˆ¶ï¼Œå¹¶æå‡ºäº†ä¸€ç§åä¸ºLCP1.0çš„å®ç”¨æ¡†æ¶ã€‚è¯¥æ¡†æ¶èƒ½å°†å¤æ‚çš„Pathmicså’ŒRadiomicsç‰¹å¾è½¬åŒ–ä¸ºå…·æœ‰ä¸´åºŠæ„ä¹‰çš„è§è§£ï¼Œä¸ç°æœ‰è¯Šæ–­æµç¨‹ç›¸ç»“åˆã€‚ç ”ç©¶ä½¿ç”¨QuPathå’ŒPyRadiomicsæå–äº†è‚ç™Œç»„ç»‡æ ·æœ¬çš„333ä¸ªæˆåƒç‰¹å¾ï¼Œå¹¶é€šè¿‡å¤šåˆ†ç±»å™¨è¯„ä¼°å…¶ä¸WHOåˆ†çº§ç³»ç»Ÿçš„ç›¸å…³æ€§ã€‚æœ€ç»ˆå¼€å‘å‡ºPathobiological Dictionaryï¼ˆLCP1.0ï¼‰ï¼Œå®ç°äº†AIè¾“å‡ºä¸ä¸“å®¶è§£è¯»ä¹‹é—´çš„æ¡¥æ¢ï¼Œå¢å¼ºäº†æ¨¡å‹çš„é€æ˜åº¦å’Œå¯ç”¨æ€§ã€‚æ­¤æ¡†æ¶æœ‰åˆ©äºå¼€å‘å¯è§£é‡Šã€å¯ä¿¡èµ–çš„è‚ç™Œç—…ç†è¯Šæ–­å·¥å…·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººå·¥æ™ºèƒ½åœ¨åŒ»å­¦è¯Šæ–­ä¸­å…·æœ‰æ½œåŠ›ï¼Œä½†ç¼ºä¹è§£é‡Šæ€§å’Œé€šç”¨æ€§é™åˆ¶äº†å…¶ä¸´åºŠåº”ç”¨ã€‚</li>
<li>LCP1.0æ¡†æ¶ç”¨äºå°†å¤æ‚çš„Pathmicså’ŒRadiomicsç‰¹å¾è½¬åŒ–ä¸ºä¸´åºŠæ„ä¹‰çš„è§è§£ã€‚</li>
<li>ç ”ç©¶ä½¿ç”¨QuPathå’ŒPyRadiomicsæå–è‚ç™Œæ ·æœ¬çš„æˆåƒç‰¹å¾ï¼ŒåŒ…æ‹¬åŸºäºPFçš„ç»†èƒæ£€æµ‹å’Œå¼ºåº¦ã€åŸºäºRFçš„çº¹ç†å’ŒåŸºäºRFçš„ä¸€é˜¶ç‰¹å¾ã€‚</li>
<li>ç‰¹å¾é€‰æ‹©ä¸åˆ†ç±»å™¨ç»“åˆè¯„ä¼°ç‰¹å¾ä¸WHOåˆ†çº§ç³»ç»Ÿçš„ç›¸å…³æ€§ã€‚</li>
<li>é€šè¿‡8ä½è‚¿ç˜¤å­¦å’Œç—…ç†å­¦ä¸“å®¶éªŒè¯Pathobiological Dictionaryï¼ˆLCP1.0ï¼‰ã€‚</li>
<li>Variable Thresholdç‰¹å¾é€‰æ‹©ç®—æ³•ç»“åˆSVMæ¨¡å‹å–å¾—æœ€é«˜å‡†ç¡®åº¦ï¼ˆ0.80ï¼‰ï¼Œé€‰å‡ºä¸è‚¿ç˜¤åˆ†çº§å’Œé¢„åå¯†åˆ‡ç›¸å…³çš„å…³é”®ç‰¹å¾ã€‚è¿™äº›ç‰¹å¾ä¸»è¦åŒ…æ‹¬ä¸´åºŠå’Œç—…ç†ç‰¹å¾ï¼Œå¦‚è´¨å¿ƒã€ç»†èƒæ ¸å’Œç»†èƒè´¨ç‰¹æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14926">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_åŒ»å­¦å›¾åƒ/2505.14926v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_åŒ»å­¦å›¾åƒ/2505.14926v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_åŒ»å­¦å›¾åƒ/2505.14926v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_åŒ»å­¦å›¾åƒ/2505.14926v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="TransMedSeg-A-Transferable-Semantic-Framework-for-Semi-Supervised-Medical-Image-Segmentation"><a href="#TransMedSeg-A-Transferable-Semantic-Framework-for-Semi-Supervised-Medical-Image-Segmentation" class="headerlink" title="TransMedSeg: A Transferable Semantic Framework for Semi-Supervised   Medical Image Segmentation"></a>TransMedSeg: A Transferable Semantic Framework for Semi-Supervised   Medical Image Segmentation</h2><p><strong>Authors:Mengzhu Wang, Jiao Li, Shanshan Wang, Long Lan, Huibin Tan, Liang Yang, Guoli Yang</strong></p>
<p>Semi-supervised learning (SSL) has achieved significant progress in medical image segmentation (SSMIS) through effective utilization of limited labeled data. While current SSL methods for medical images predominantly rely on consistency regularization and pseudo-labeling, they often overlook transferable semantic relationships across different clinical domains and imaging modalities. To address this, we propose TransMedSeg, a novel transferable semantic framework for semi-supervised medical image segmentation. Our approach introduces a Transferable Semantic Augmentation (TSA) module, which implicitly enhances feature representations by aligning domain-invariant semantics through cross-domain distribution matching and intra-domain structural preservation. Specifically, TransMedSeg constructs a unified feature space where teacher network features are adaptively augmented towards student network semantics via a lightweight memory module, enabling implicit semantic transformation without explicit data generation. Interestingly, this augmentation is implicitly realized through an expected transferable cross-entropy loss computed over the augmented teacher distribution. An upper bound of the expected loss is theoretically derived and minimized during training, incurring negligible computational overhead. Extensive experiments on medical image datasets demonstrate that TransMedSeg outperforms existing semi-supervised methods, establishing a new direction for transferable representation learning in medical image analysis. </p>
<blockquote>
<p>åŠç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ï¼ˆSSMISï¼‰ä¸­é€šè¿‡æœ‰æ•ˆåˆ©ç”¨æœ‰é™çš„æ ‡è®°æ•°æ®å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚è™½ç„¶å½“å‰ç”¨äºåŒ»å­¦å›¾åƒçš„SSLæ–¹æ³•ä¸»è¦ä¾èµ–äºä¸€è‡´æ€§æ­£åˆ™åŒ–å’Œä¼ªæ ‡ç­¾ï¼Œä½†å®ƒä»¬å¸¸å¸¸å¿½è§†äº†ä¸åŒä¸´åºŠåŸŸå’Œæˆåƒæ¨¡å¼ä¹‹é—´çš„å¯è½¬ç§»è¯­ä¹‰å…³ç³»ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†TransMedSegï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºåŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²çš„å¯è½¬ç§»è¯­ä¹‰æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸€ä¸ªå¯è½¬ç§»è¯­ä¹‰å¢å¼ºï¼ˆTSAï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—é€šè¿‡è·¨åŸŸåˆ†å¸ƒåŒ¹é…å’ŒåŸŸå†…ç»“æ„ä¿ç•™æ¥å¯¹é½åŸŸä¸å˜è¯­ä¹‰ï¼Œä»è€Œéšå¼åœ°å¢å¼ºç‰¹å¾è¡¨ç¤ºã€‚å…·ä½“æ¥è¯´ï¼ŒTransMedSegæ„å»ºäº†ä¸€ä¸ªç»Ÿä¸€çš„ç‰¹å¾ç©ºé—´ï¼Œå…¶ä¸­æ•™å¸ˆç½‘ç»œç‰¹å¾é€šè¿‡è½»é‡çº§å†…å­˜æ¨¡å—è‡ªé€‚åº”åœ°å¢å¼ºå‘å­¦ç”Ÿç½‘ç»œè¯­ä¹‰ï¼Œå®ç°äº†éšå¼è¯­ä¹‰è½¬æ¢ï¼Œæ— éœ€æ˜¾å¼æ•°æ®ç”Ÿæˆã€‚æœ‰è¶£çš„æ˜¯ï¼Œè¿™ç§å¢å¼ºæ˜¯é€šè¿‡åœ¨å¢å¼ºæ•™å¸ˆåˆ†å¸ƒä¸Šè®¡ç®—çš„é¢„æœŸå¯è½¬ç§»äº¤å‰ç†µæŸå¤±æ¥éšå¼å®ç°çš„ã€‚ç†è®ºä¸Šæ¨å¯¼äº†é¢„æœŸæŸå¤±çš„ä¸Šç•Œï¼Œå¹¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¿›è¡Œæœ€å°åŒ–ï¼Œå‡ ä¹ä¸ä¼šå¼•èµ·è®¡ç®—å¼€é”€çš„å¢åŠ ã€‚åœ¨åŒ»å­¦å›¾åƒæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒTransMedSegä¼˜äºç°æœ‰çš„åŠç›‘ç£æ–¹æ³•ï¼Œä¸ºåŒ»å­¦å›¾åƒåˆ†æä¸­çš„å¯è½¬ç§»è¡¨ç¤ºå­¦ä¹ æä¾›äº†æ–°çš„æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14753v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒåŠç›‘ç£å­¦ä¹ ï¼ˆSSMISï¼‰å·²ç»å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†ä»å­˜åœ¨å¿½è§†è·¨ä¸åŒä¸´åºŠé¢†åŸŸå’Œæˆåƒæ¨¡æ€çš„å¯è½¬ç§»è¯­ä¹‰å…³ç³»çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†TransMedSegï¼Œä¸€ç§ç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²çš„æ–°å‹å¯è½¬ç§»è¯­ä¹‰æ¡†æ¶ã€‚å®ƒå¼•å…¥äº†ä¸€ä¸ªå¯è½¬ç§»è¯­ä¹‰å¢å¼ºï¼ˆTSAï¼‰æ¨¡å—ï¼Œé€šè¿‡è·¨åŸŸåˆ†å¸ƒåŒ¹é…å’ŒåŸŸå†…ç»“æ„ä¿ç•™éšå¼å¢å¼ºç‰¹å¾è¡¨ç¤ºã€‚TransMedSegæ„å»ºäº†ä¸€ä¸ªç»Ÿä¸€ç‰¹å¾ç©ºé—´ï¼Œé€šè¿‡è½»é‡çº§å†…å­˜æ¨¡å—è‡ªé€‚åº”å¢å¼ºæ•™å¸ˆç½‘ç»œç‰¹å¾å‘å­¦ç”Ÿç½‘ç»œè¯­ä¹‰å¯¹é½ï¼Œå®ç°éšå¼è¯­ä¹‰è½¬æ¢ã€‚æ­¤å¤–ï¼Œé€šè¿‡é¢„æœŸçš„è·¨ç†µæŸå¤±è®¡ç®—æ•™å¸ˆåˆ†å¸ƒçš„å¢å¼ºç‰ˆï¼Œå®ç°éšæ€§å¢å¼ºã€‚ç†è®ºæ¨å¯¼é¢„æœŸæŸå¤±çš„ä¸Šç•Œï¼Œå¹¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æœ€å°åŒ–ï¼Œè®¡ç®—å¼€é”€å°ã€‚åœ¨åŒ»å­¦å›¾åƒæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒTransMedSegä¼˜äºç°æœ‰åŠç›‘ç£æ–¹æ³•ï¼Œä¸ºåŒ»å­¦å›¾åƒåˆ†æä¸­çš„å¯è½¬ç§»è¡¨ç¤ºå­¦ä¹ æŒ‡æ˜äº†æ–°æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TransMedSegæ˜¯ä¸€ç§é’ˆå¯¹åŒ»å­¦å›¾åƒåˆ†å‰²çš„åŠç›‘ç£å­¦ä¹ æ–°æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•å¿½è§†è·¨ä¸åŒä¸´åºŠé¢†åŸŸå’Œæˆåƒæ¨¡æ€çš„å¯è½¬ç§»è¯­ä¹‰å…³ç³»çš„é—®é¢˜ã€‚</li>
<li>TransMedSegå¼•å…¥TSAæ¨¡å—ï¼Œé€šè¿‡è·¨åŸŸåˆ†å¸ƒåŒ¹é…å’ŒåŸŸå†…ç»“æ„ä¿ç•™éšå¼å¢å¼ºç‰¹å¾è¡¨ç¤ºï¼Œæ„å»ºç»Ÿä¸€ç‰¹å¾ç©ºé—´ã€‚</li>
<li>æ•™å¸ˆç½‘ç»œç‰¹å¾é€šè¿‡è½»é‡çº§å†…å­˜æ¨¡å—è‡ªé€‚åº”åœ°å‘å­¦ç”Ÿç½‘ç»œè¯­ä¹‰å¯¹é½ï¼Œå®ç°éšå¼è¯­ä¹‰è½¬æ¢ã€‚</li>
<li>TransMedSegåˆ©ç”¨é¢„æœŸçš„è·¨ç†µæŸå¤±è®¡ç®—æ•™å¸ˆåˆ†å¸ƒçš„å¢å¼ºç‰ˆï¼Œå®ç°éšæ€§å¢å¼ºï¼Œæœ€å°åŒ–ç†è®ºæ¨å¯¼çš„é¢„æœŸæŸå¤±ä¸Šç•Œã€‚</li>
<li>TransMedSegç›¸å¯¹äºç°æœ‰åŠç›‘ç£æ–¹æ³•è¡¨ç°å‡ºä¼˜è¶Šæ€§ï¼Œä¸ºåŒ»å­¦å›¾åƒåˆ†æçš„å¯è½¬ç§»è¡¨ç¤ºå­¦ä¹ æä¾›æ–°çš„æ–¹å‘ã€‚</li>
<li>TransMedSegåœ¨å¤„ç†åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡æ—¶èƒ½æœ‰æ•ˆåˆ©ç”¨æœ‰é™çš„æ ‡æ³¨æ•°æ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14753">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_åŒ»å­¦å›¾åƒ/2505.14753v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_åŒ»å­¦å›¾åƒ/2505.14753v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_åŒ»å­¦å›¾åƒ/2505.14753v1/page_2_0.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="MedBLIP-Fine-tuning-BLIP-for-Medical-Image-Captioning"><a href="#MedBLIP-Fine-tuning-BLIP-for-Medical-Image-Captioning" class="headerlink" title="MedBLIP: Fine-tuning BLIP for Medical Image Captioning"></a>MedBLIP: Fine-tuning BLIP for Medical Image Captioning</h2><p><strong>Authors:Manshi Limbu, Diwita Banerjee</strong></p>
<p>Medical image captioning is a challenging task that requires generating clinically accurate and semantically meaningful descriptions of radiology images. While recent vision-language models (VLMs) such as BLIP, BLIP2, Gemini and ViT-GPT2 show strong performance on natural image datasets, they often produce generic or imprecise captions when applied to specialized medical domains. In this project, we explore the effectiveness of fine-tuning the BLIP model on the ROCO dataset for improved radiology captioning. We compare the fine-tuned BLIP against its zero-shot version, BLIP-2 base, BLIP-2 Instruct and a ViT-GPT2 transformer baseline. Our results demonstrate that domain-specific fine-tuning on BLIP significantly improves performance across both quantitative and qualitative evaluation metrics. We also visualize decoder cross-attention maps to assess interpretability and conduct an ablation study to evaluate the contributions of encoder-only and decoder-only fine-tuning. Our findings highlight the importance of targeted adaptation for medical applications and suggest that decoder-only fine-tuning (encoder-frozen) offers a strong performance baseline with 5% lower training time than full fine-tuning, while full model fine-tuning still yields the best results overall. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒæ ‡æ³¨æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œéœ€è¦ç”Ÿæˆä¸´åºŠå‡†ç¡®ä¸”è¯­ä¹‰æ˜ç¡®çš„æ”¾å°„å­¦å›¾åƒæè¿°ã€‚è™½ç„¶æœ€è¿‘çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ï¼Œå¦‚BLIPã€BLIP2ã€Geminiå’ŒViT-GPT2åœ¨è‡ªç„¶å›¾åƒæ•°æ®é›†ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†å®ƒä»¬åº”ç”¨äºä¸“ä¸šåŒ»å­¦é¢†åŸŸæ—¶ï¼Œé€šå¸¸ä¼šäº§ç”Ÿé€šç”¨æˆ–ä¸å¤Ÿç²¾ç¡®çš„æ ‡æ³¨ã€‚åœ¨æœ¬é¡¹ç›®ä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†ä½¿ç”¨ROCOæ•°æ®é›†å¯¹BLIPæ¨¡å‹è¿›è¡Œå¾®è°ƒä»¥æ”¹è¿›æ”¾å°„å­¦æ ‡æ³¨çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬å°†å¾®è°ƒåçš„BLIPä¸å…¶é›¶æ ·æœ¬ç‰ˆæœ¬ã€BLIP-2åŸºç¡€ç‰ˆã€BLIP-2æŒ‡ä»¤ç‰ˆå’ŒViT-GPT2è½¬æ¢å™¨åŸºçº¿è¿›è¡Œäº†æ¯”è¾ƒã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œåœ¨BLIPä¸Šè¿›è¡Œç‰¹å®šé¢†åŸŸçš„å¾®è°ƒæ˜¾è‘—æé«˜äº†å®šé‡å’Œå®šæ€§è¯„ä¼°æŒ‡æ ‡çš„æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜é€šè¿‡å¯è§†åŒ–è§£ç å™¨äº¤å‰æ³¨æ„åŠ›å›¾æ¥è¯„ä¼°å¯è§£é‡Šæ€§ï¼Œå¹¶è¿›è¡Œäº†ä¸€é¡¹æ¶ˆèç ”ç©¶ï¼Œä»¥è¯„ä¼°ä»…ç¼–ç å™¨å¾®è°ƒä¸ä»…è§£ç å™¨å¾®è°ƒå„è‡ªçš„è´¡çŒ®ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¼ºè°ƒäº†é’ˆå¯¹åŒ»å­¦åº”ç”¨è¿›è¡Œæœ‰é’ˆå¯¹æ€§çš„é€‚åº”æ€§çš„é‡è¦æ€§ï¼Œå¹¶è¡¨æ˜ä»…å¯¹è§£ç å™¨è¿›è¡Œå¾®è°ƒï¼ˆå†»ç»“ç¼–ç å™¨ï¼‰æä¾›äº†ä¸€ä¸ªå¼ºå¤§çš„æ€§èƒ½åŸºå‡†ï¼Œå…¶è®­ç»ƒæ—¶é—´æ¯”å®Œå…¨å¾®è°ƒä½5%ï¼Œè€Œå…¨æ¨¡å‹å¾®è°ƒä»ç„¶æ€»ä½“ä¸Šè¡¨ç°æœ€ä½³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14726v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŒ»å­¦å›¾åƒæè¿°æ˜¯ä¸€é¡¹æŒ‘æˆ˜ä»»åŠ¡ï¼Œéœ€è¦ä¸ºæ”¾å°„å­¦å›¾åƒç”Ÿæˆä¸´åºŠå‡†ç¡®ä¸”è¯­ä¹‰æœ‰æ„ä¹‰çš„æè¿°ã€‚æœ¬ç ”ç©¶é€šè¿‡å¾®è°ƒBLIPæ¨¡å‹åœ¨ROCOæ•°æ®é›†ä¸Šï¼Œæé«˜äº†åŒ»å­¦å½±åƒæè¿°çš„æ•ˆæœã€‚å¯¹æ¯”äº†å¾®è°ƒåçš„BLIPä¸é›¶æ ·æœ¬ç‰ˆæœ¬çš„BLIP-2åŸºç¡€ç‰ˆã€BLIP-2æŒ‡å¯¼ç‰ˆä»¥åŠViT-GPT2è½¬æ¢å™¨åŸºçº¿ï¼Œå‘ç°å¾®è°ƒBLIPæ¨¡å‹åœ¨å®šé‡å’Œå®šæ€§è¯„ä¼°æŒ‡æ ‡ä¸Šéƒ½æœ‰æ˜¾è‘—æé«˜ã€‚åŒæ—¶ï¼Œæœ¬ç ”ç©¶è¿˜é€šè¿‡å¯è§†åŒ–è§£ç å™¨äº¤å‰æ³¨æ„åŠ›å›¾æ¥è¯„ä¼°æ¨¡å‹çš„å¯è§£é‡Šæ€§ï¼Œå¹¶é€šè¿‡æ¶ˆèç ”ç©¶è¯„ä¼°äº†ä»…ç¼–ç å™¨æˆ–ä»…è§£ç å™¨å¾®è°ƒçš„å½±å“ã€‚ç»“æœè¡¨æ˜ï¼Œæœ‰é’ˆå¯¹æ€§çš„é€‚åº”å¯¹äºåŒ»å­¦åº”ç”¨è‡³å…³é‡è¦ï¼Œè€Œä»…è§£ç å™¨å¾®è°ƒåœ¨è®­ç»ƒæ—¶é—´ä¸Šæ¯”å…¨æ¨¡å‹å¾®è°ƒä½5%ï¼Œä½†ä»è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½åŸºçº¿ã€‚å…¨æ¨¡å‹å¾®è°ƒä»ç„¶æ˜¯ç›®å‰æœ€ä½³é€‰æ‹©ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒæè¿°æ˜¯ä¸€é¡¹éœ€è¦ç”Ÿæˆä¸´åºŠå‡†ç¡®å’Œè¯­ä¹‰æœ‰æ„ä¹‰çš„æè¿°çš„æŒ‘æˆ˜æ€§ä»»åŠ¡ã€‚</li>
<li>ç ”ç©¶é€šè¿‡å¯¹BLIPæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œæé«˜äº†åœ¨ROCOæ•°æ®é›†ä¸Šçš„åŒ»å­¦å½±åƒæè¿°æ•ˆæœã€‚</li>
<li>ç›¸æ¯”é›¶æ ·æœ¬ç‰ˆæœ¬çš„BLIPå’Œå…¶ä»–æ¨¡å‹ï¼Œå¾®è°ƒåçš„BLIPæ¨¡å‹åœ¨å®šé‡å’Œå®šæ€§è¯„ä¼°æŒ‡æ ‡ä¸Šéƒ½æœ‰æ˜¾è‘—æé«˜ã€‚</li>
<li>å¯è§†åŒ–è§£ç å™¨äº¤å‰æ³¨æ„åŠ›å›¾æœ‰åŠ©äºè¯„ä¼°æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚</li>
<li>æ¶ˆèç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œä»…è§£ç å™¨å¾®è°ƒåœ¨è®­ç»ƒæ—¶é—´ä¸Šå…·æœ‰ä¼˜åŠ¿ï¼Œè€Œå…¨æ¨¡å‹å¾®è°ƒåˆ™è¡¨ç°æœ€ä½³ã€‚</li>
<li>é’ˆå¯¹æ€§çš„æ¨¡å‹é€‚åº”å¯¹äºåŒ»å­¦åº”ç”¨è‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14726">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_åŒ»å­¦å›¾åƒ/2505.14726v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_åŒ»å­¦å›¾åƒ/2505.14726v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_åŒ»å­¦å›¾åƒ/2505.14726v1/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_åŒ»å­¦å›¾åƒ/2505.14726v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="PRS-Med-Position-Reasoning-Segmentation-with-Vision-Language-Model-in-Medical-Imaging"><a href="#PRS-Med-Position-Reasoning-Segmentation-with-Vision-Language-Model-in-Medical-Imaging" class="headerlink" title="PRS-Med: Position Reasoning Segmentation with Vision-Language Model in   Medical Imaging"></a>PRS-Med: Position Reasoning Segmentation with Vision-Language Model in   Medical Imaging</h2><p><strong>Authors:Quoc-Huy Trinh, Minh-Van Nguyen, Jung Peng, Ulas Bagci, Debesh Jha</strong></p>
<p>Recent advancements in prompt-based medical image segmentation have enabled clinicians to identify tumors using simple input like bounding boxes or text prompts. However, existing methods face challenges when doctors need to interact through natural language or when position reasoning is required - understanding spatial relationships between anatomical structures and pathologies. We present PRS-Med, a framework that integrates vision-language models with segmentation capabilities to generate both accurate segmentation masks and corresponding spatial reasoning outputs. Additionally, we introduce the MMRS dataset (Multimodal Medical in Positional Reasoning Segmentation), which provides diverse, spatially-grounded question-answer pairs to address the lack of position reasoning data in medical imaging. PRS-Med demonstrates superior performance across six imaging modalities (CT, MRI, X-ray, ultrasound, endoscopy, RGB), significantly outperforming state-of-the-art methods in both segmentation accuracy and position reasoning. Our approach enables intuitive doctor-system interaction through natural language, facilitating more efficient diagnoses. Our dataset pipeline, model, and codebase will be released to foster further research in spatially-aware multimodal reasoning for medical applications. </p>
<blockquote>
<p>åœ¨åŸºäºæç¤ºçš„åŒ»ç–—å›¾åƒåˆ†å‰²æ–¹é¢çš„æœ€æ–°è¿›å±•ï¼Œå·²ç»ä½¿ä¸´åºŠåŒ»ç”Ÿèƒ½å¤Ÿä½¿ç”¨ç®€å•çš„è¾“å…¥ï¼Œå¦‚è¾¹ç•Œæ¡†æˆ–æ–‡æœ¬æç¤ºæ¥è¯†åˆ«è‚¿ç˜¤ã€‚ç„¶è€Œï¼Œå½“åŒ»ç”Ÿéœ€è¦é€šè¿‡è‡ªç„¶è¯­è¨€è¿›è¡Œäº¤äº’æˆ–éœ€è¦ä½ç½®æ¨ç†ï¼ˆå³ç†è§£è§£å‰–ç»“æ„å’Œç—…ç†ä¹‹é—´çš„ç©ºé—´å…³ç³»ï¼‰æ—¶ï¼Œç°æœ‰æ–¹æ³•é¢ä¸´æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†PRS-Medæ¡†æ¶ï¼Œå®ƒèåˆäº†è§†è§‰è¯­è¨€æ¨¡å‹ä¸åˆ†å‰²èƒ½åŠ›ï¼Œèƒ½å¤Ÿç”Ÿæˆå‡†ç¡®çš„åˆ†å‰²æ©è†œå’Œç›¸åº”çš„ç©ºé—´æ¨ç†è¾“å‡ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä»‹ç»äº†MMRSæ•°æ®é›†ï¼ˆå®šä½æ¨ç†åˆ†å‰²ä¸­çš„å¤šæ¨¡æ€åŒ»ç–—æ•°æ®ï¼‰ï¼Œè¯¥æ•°æ®é›†æä¾›äº†å¤šæ ·ä¸”åŸºäºç©ºé—´çš„é—®é¢˜ç­”æ¡ˆå¯¹ï¼Œä»¥è§£å†³åŒ»ç–—æˆåƒä¸­ä½ç½®æ¨ç†æ•°æ®çš„ç¼ºä¹ã€‚PRS-Medåœ¨å…­ç§æˆåƒæ¨¡å¼ï¼ˆCTã€MRIã€Xå…‰ã€è¶…å£°ã€å†…çª¥é•œã€RGBï¼‰ä¸‹è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œåœ¨åˆ†å‰²ç²¾åº¦å’Œä½ç½®æ¨ç†æ–¹é¢éƒ½å¤§å¤§ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿé€šè¿‡è‡ªç„¶è¯­è¨€å®ç°ç›´è§‚çš„åŒ»ç”Ÿç³»ç»Ÿäº¤äº’ï¼Œä¿ƒè¿›æ›´é«˜æ•ˆçš„è¯Šæ–­ã€‚æˆ‘ä»¬çš„æ•°æ®é›†æµç¨‹ã€æ¨¡å‹å’Œä»£ç åº“å°†äºˆä»¥å‘å¸ƒï¼Œä»¥ä¿ƒè¿›åœ¨ç©ºé—´æ„ŸçŸ¥å¤šæ¨¡å¼æ¨ç†åŒ»ç–—åº”ç”¨æ–¹é¢çš„è¿›ä¸€æ­¥ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11872v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ€è¿‘ï¼ŒåŸºäºæç¤ºçš„åŒ»ç–—å›¾åƒåˆ†å‰²æŠ€æœ¯çš„è¿›å±•ï¼Œä½¿å¾—ä¸´åºŠåŒ»ç”Ÿå¯ä»¥é€šè¿‡ç®€å•çš„è¾“å…¥ï¼Œå¦‚è¾¹ç•Œæ¡†æˆ–æ–‡æœ¬æç¤ºï¼Œæ¥è¯†åˆ«è‚¿ç˜¤ã€‚ç„¶è€Œï¼Œå½“åŒ»ç”Ÿéœ€è¦é€šè¿‡è‡ªç„¶è¯­è¨€è¿›è¡Œäº¤äº’æˆ–éœ€è¦ä½ç½®æ¨ç†ï¼ˆå³ç†è§£è§£å‰–ç»“æ„å’Œç—…ç†ä¹‹é—´çš„ç©ºé—´å…³ç³»ï¼‰æ—¶ï¼Œç°æœ‰æ–¹æ³•é¢ä¸´æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†PRS-Medæ¡†æ¶ï¼Œå®ƒç»“åˆäº†è§†è§‰è¯­è¨€æ¨¡å‹å’Œåˆ†å‰²èƒ½åŠ›ï¼Œèƒ½å¤Ÿç”Ÿæˆå‡†ç¡®çš„åˆ†å‰²æ©è†œå’Œç›¸åº”çš„ç©ºé—´æ¨ç†è¾“å‡ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä»‹ç»äº†MMRSæ•°æ®é›†ï¼ˆä½ç½®æ¨ç†åˆ†å‰²ä¸­çš„å¤šæ¨¡æ€åŒ»ç–—å›¾åƒï¼‰ï¼Œè¯¥æ•°æ®é›†æä¾›äº†å¤šæ ·ä¸”ç©ºé—´å®šä½çš„é—®ç­”å¯¹ï¼Œä»¥è§£å†³åŒ»ç–—å½±åƒä¸­ä½ç½®æ¨ç†æ•°æ®çš„ç¼ºä¹ã€‚PRS-Medåœ¨å…­ç§æˆåƒæ¨¡å¼ï¼ˆCTã€MRIã€Xå…‰ã€è¶…å£°ã€å†…çª¥é•œã€RGBï¼‰ä¸‹è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œåœ¨åˆ†å‰²å‡†ç¡®æ€§å’Œä½ç½®æ¨ç†æ–¹é¢éƒ½å¤§å¤§ä¼˜äºæœ€æ–°æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡è‡ªç„¶è¯­è¨€å®ç°äº†åŒ»ç”Ÿä¸ç³»ç»Ÿä¹‹é—´çš„ç›´è§‚äº¤äº’ï¼Œæœ‰åŠ©äºæ›´é«˜æ•ˆçš„è¯Šæ–­ã€‚æˆ‘ä»¬çš„æ•°æ®é›†ç®¡é“ã€æ¨¡å‹å’Œæºä»£ç å°†äºˆä»¥å…¬å¼€ï¼Œä»¥ä¿ƒè¿›ç©ºé—´æ„ŸçŸ¥å¤šæ¨¡æ€æ¨ç†åœ¨åŒ»ç–—åº”ç”¨ä¸­çš„è¿›ä¸€æ­¥ç ”ç©¶ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>PRS-Medæ¡†æ¶ç»“åˆäº†è§†è§‰è¯­è¨€æ¨¡å‹å’Œåˆ†å‰²èƒ½åŠ›ï¼Œèƒ½å¤Ÿç”Ÿæˆå‡†ç¡®çš„åˆ†å‰²æ©è†œå’Œç›¸åº”çš„ç©ºé—´æ¨ç†è¾“å‡ºã€‚</li>
<li>å¼•å…¥MMRSæ•°æ®é›†ï¼Œæä¾›å¤šæ ·ä¸”ç©ºé—´å®šä½çš„é—®ç­”å¯¹ï¼Œè§£å†³åŒ»ç–—å½±åƒä¸­ä½ç½®æ¨ç†æ•°æ®çš„ç¼ºä¹é—®é¢˜ã€‚</li>
<li>PRS-Medåœ¨å…­ç§ä¸åŒçš„æˆåƒæ¨¡å¼ä¸‹è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå¤§å¤§ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡è‡ªç„¶è¯­è¨€å®ç°äº†åŒ»ç”Ÿä¸ç³»ç»Ÿä¹‹é—´çš„ç›´è§‚äº¤äº’ã€‚</li>
<li>PRS-Medæ¡†æ¶æœ‰åŠ©äºæ›´é«˜æ•ˆçš„è¯Šæ–­ã€‚</li>
<li>æ•°æ®é›†ç®¡é“ã€æ¨¡å‹å’Œæºä»£ç å°†å…¬å¼€ï¼Œä»¥ä¿ƒè¿›è¿›ä¸€æ­¥ç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11872">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_åŒ»å­¦å›¾åƒ/2505.11872v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_åŒ»å­¦å›¾åƒ/2505.11872v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_åŒ»å­¦å›¾åƒ/2505.11872v2/page_3_1.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-24/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-24/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-24/TTS/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-05-24\./crop_TTS/2505.14871v1/page_1_0.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-24  Audio Jailbreak An Open Comprehensive Benchmark for Jailbreaking Large   Audio-Language Models
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-24
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-24/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-9ea8fda246e471d018ae69deae8b7be8.jpg" class="responsive-img" alt="ç‰™é½¿ä¿®å¤">
                        
                        <span class="card-title">ç‰™é½¿ä¿®å¤</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            ç‰™é½¿ä¿®å¤ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-24  Oral Imaging for Malocclusion Issues Assessments OMNI Dataset, Deep   Learning Baselines and Benchmarking
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-24
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/" class="post-category">
                                    ç‰™é½¿ä¿®å¤
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                        <span class="chip bg-color">ç‰™é½¿ä¿®å¤</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">19017.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
