<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Agent">
    <meta name="description" content="Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-24  From EduVisBench to EduVisAgent A Benchmark and Multi-Agent Framework   for Pedagogical Visualization">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Agent | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-057aeeb66cc0433f578069a2a100f587.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Agent</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Agent/">
                                <span class="chip bg-color">Agent</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                Agent
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-24
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    16.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    67 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-24-æ›´æ–°"><a href="#2025-05-24-æ›´æ–°" class="headerlink" title="2025-05-24 æ›´æ–°"></a>2025-05-24 æ›´æ–°</h1><h2 id="From-EduVisBench-to-EduVisAgent-A-Benchmark-and-Multi-Agent-Framework-for-Pedagogical-Visualization"><a href="#From-EduVisBench-to-EduVisAgent-A-Benchmark-and-Multi-Agent-Framework-for-Pedagogical-Visualization" class="headerlink" title="From EduVisBench to EduVisAgent: A Benchmark and Multi-Agent Framework   for Pedagogical Visualization"></a>From EduVisBench to EduVisAgent: A Benchmark and Multi-Agent Framework   for Pedagogical Visualization</h2><p><strong>Authors:Haonian Ji, Shi Qiu, Siyang Xin, Siwei Han, Zhaorun Chen, Hongyi Wang, Dake Zhang, Huaxiu Yao</strong></p>
<p>While foundation models (FMs), such as diffusion models and large vision-language models (LVLMs), have been widely applied in educational contexts, their ability to generate pedagogically effective visual explanations remains limited. Most existing approaches focus primarily on textual reasoning, overlooking the critical role of structured and interpretable visualizations in supporting conceptual understanding. To better assess the visual reasoning capabilities of FMs in educational settings, we introduce EduVisBench, a multi-domain, multi-level benchmark. EduVisBench features diverse STEM problem sets requiring visually grounded solutions, along with a fine-grained evaluation rubric informed by pedagogical theory. Our empirical analysis reveals that existing models frequently struggle with the inherent challenge of decomposing complex reasoning and translating it into visual representations aligned with human cognitive processes. To address these limitations, we propose EduVisAgent, a multi-agent collaborative framework that coordinates specialized agents for instructional planning, reasoning decomposition, metacognitive prompting, and visualization design. Experimental results show that EduVisAgent substantially outperforms all baselines, achieving a 40.2% improvement and delivering more educationally aligned visualizations. EduVisBench and EduVisAgent are available at <a target="_blank" rel="noopener" href="https://github.com/aiming-lab/EduVisBench">https://github.com/aiming-lab/EduVisBench</a> and <a target="_blank" rel="noopener" href="https://github.com/aiming-lab/EduVisAgent">https://github.com/aiming-lab/EduVisAgent</a>. </p>
<blockquote>
<p>å°½ç®¡æ‰©æ•£æ¨¡å‹ç­‰å¤§å‹åŸºç¡€æ¨¡å‹ï¼ˆFMsï¼‰å’Œå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨æ•™è‚²ç¯å¢ƒä¸­å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ï¼Œå®ƒä»¬åœ¨ç”Ÿæˆæ•™å­¦æœ‰æ•ˆçš„è§†è§‰è§£é‡Šæ–¹é¢çš„èƒ½åŠ›ä»ç„¶æœ‰é™ã€‚ç›®å‰å¤§å¤šæ•°æ–¹æ³•ä¸»è¦å…³æ³¨æ–‡æœ¬æ¨ç†ï¼Œå¿½è§†äº†ç»“æ„åŒ–ã€å¯è§£é‡Šçš„è§†è§‰æ”¯æŒåœ¨ä¿ƒè¿›æ¦‚å¿µç†è§£æ–¹é¢çš„å…³é”®ä½œç”¨ã€‚ä¸ºäº†æ›´å¥½åœ°è¯„ä¼°æ•™è‚²ç¯å¢ƒä¸­åŸºç¡€æ¨¡å‹çš„è§†è§‰æ¨ç†èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼•å…¥äº†EduVisBenchï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šé¢†åŸŸã€å¤šå±‚æ¬¡çš„åŸºå‡†æµ‹è¯•ã€‚EduVisBenchåŒ…å«å¤šæ ·åŒ–çš„STEMé—®é¢˜é›†ï¼Œéœ€è¦è§†è§‰è§£å†³æ–¹æ¡ˆï¼Œä»¥åŠç”±æ•™å­¦ç†è®ºæ”¯æŒçš„ç²¾ç»†è¯„ä¼°æ ‡å‡†ã€‚æˆ‘ä»¬çš„å®è¯åˆ†æè¡¨æ˜ï¼Œç°æœ‰æ¨¡å‹åœ¨åˆ†è§£å¤æ‚æ¨ç†å¹¶å°†å…¶è½¬åŒ–ä¸ºä¸äººç±»è®¤çŸ¥è¿‡ç¨‹ç›¸ç¬¦çš„è§†è§‰è¡¨ç¤ºæ–¹é¢å­˜åœ¨å›ºæœ‰çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†EduVisAgentï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šæ™ºèƒ½ä½“åä½œæ¡†æ¶ï¼Œåè°ƒä¸“é—¨ç”¨äºæ•™å­¦è§„åˆ’çš„æ™ºèƒ½ä½“ï¼Œè¿›è¡Œæ¨ç†åˆ†è§£ã€å…ƒè®¤çŸ¥æç¤ºå’Œå¯è§†åŒ–è®¾è®¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEduVisAgentæ˜¾è‘—ä¼˜äºæ‰€æœ‰åŸºçº¿æ¨¡å‹ï¼Œå®ç°äº†40.2%çš„æ”¹è¿›ï¼Œå¹¶æä¾›äº†æ›´ç¬¦åˆæ•™è‚²è¦æ±‚çš„å¯è§†åŒ–ã€‚EduVisBenchå’ŒEduVisAgentå¯åœ¨ä»¥ä¸‹ç½‘ç«™è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/aiming-lab/EduVisBench">https://github.com/aiming-lab/EduVisBench</a> å’Œ <a target="_blank" rel="noopener" href="https://github.com/aiming-lab/EduVisAgent">https://github.com/aiming-lab/EduVisAgent</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16832v1">PDF</a> 16 pages; 7 figures</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æ¢è®¨äº†åŸºç¡€æ¨¡å‹ï¼ˆå¦‚æ‰©æ•£æ¨¡å‹å’Œå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼‰åœ¨æ•™è‚²èƒŒæ™¯ä¸‹çš„åº”ç”¨åŠå…¶ç”Ÿæˆæœ‰æ•ˆè§†è§‰è§£é‡Šçš„å±€é™æ€§ã€‚ä¸ºæ­¤ï¼Œæ–‡ç« å¼•å…¥äº†ä¸€ä¸ªå¤šé¢†åŸŸã€å¤šå±‚æ¬¡çš„åŸºå‡†æµ‹è¯•EduVisBenchï¼Œé€šè¿‡å®è¯åˆ†æå’Œç²¾ç»†åŒ–è¯„ä¼°æ ‡å‡†æ¥è¯„ä¼°æ¨¡å‹åœ¨è§†è§‰æ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚é’ˆå¯¹ç°æœ‰æ¨¡å‹çš„ä¸è¶³ï¼Œæ–‡ç« æå‡ºäº†EduVisAgentå¤šä»£ç†åä½œæ¡†æ¶ï¼Œé€šè¿‡ä¸“ä¸šåŒ–çš„ä»£ç†æ¥åè°ƒæ•™å­¦è§„åˆ’ã€æ¨ç†åˆ†è§£ã€å…ƒè®¤çŸ¥æç¤ºå’Œå¯è§†åŒ–è®¾è®¡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒEduVisAgentæ˜¾è‘—ä¼˜äºæ‰€æœ‰åŸºçº¿æ¨¡å‹ï¼Œå®ç°äº†40.2%çš„æ”¹è¿›ï¼Œå¹¶æä¾›äº†æ›´ç¬¦åˆæ•™è‚²éœ€æ±‚çš„æ•°æ®å¯è§†åŒ–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŸºç¡€æ¨¡å‹åœ¨æ•™è‚²é¢†åŸŸåº”ç”¨å¹¿æ³›ï¼Œä½†åœ¨ç”Ÿæˆæœ‰æ•ˆçš„è§†è§‰è§£é‡Šæ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>ç°æœ‰æ¨¡å‹ä¸»è¦å…³æ³¨æ–‡æœ¬æ¨ç†ï¼Œå¿½è§†äº†ç»“æ„åŒ–ã€å¯è§£é‡Šçš„è§†è§‰æ”¯æŒåœ¨æ¦‚å¿µç†è§£ä¸­çš„é‡è¦æ€§ã€‚</li>
<li>å¼•å…¥EduVisBenchåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°æ¨¡å‹åœ¨æ•™è‚²ç¯å¢ƒä¸­çš„è§†è§‰æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å®è¯åˆ†ææ˜¾ç¤ºç°æœ‰æ¨¡å‹åœ¨åˆ†è§£å¤æ‚æ¨ç†å’Œè½¬åŒ–ä¸ºä¸äººç±»è®¤çŸ¥è¿‡ç¨‹å¯¹é½çš„è§†è§‰è¡¨ç¤ºæ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºEduVisAgentå¤šä»£ç†åä½œæ¡†æ¶ï¼ŒåŒ…å«æ•™å­¦è§„åˆ’ã€æ¨ç†åˆ†è§£ã€å…ƒè®¤çŸ¥æç¤ºå’Œå¯è§†åŒ–è®¾è®¡ç­‰åŠŸèƒ½ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºEduVisAgentæ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œå®ç°äº†æ•™è‚²è§†è§‰åŒ–çš„æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16832">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-257fdc584862c8504099526384973fc8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-88d32afee7d33df3fe3957ab76eeb5b5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d199b348347b7272917911acd269691e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d307f6d9b7d9ff21c5993b7d005ccc30.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8a06327db499ffbb7a55953f0d10fb9d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-abdd914104220633265fd78c361fa0e8.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="GUI-explorer-Autonomous-Exploration-and-Mining-of-Transition-aware-Knowledge-for-GUI-Agent"><a href="#GUI-explorer-Autonomous-Exploration-and-Mining-of-Transition-aware-Knowledge-for-GUI-Agent" class="headerlink" title="GUI-explorer: Autonomous Exploration and Mining of Transition-aware   Knowledge for GUI Agent"></a>GUI-explorer: Autonomous Exploration and Mining of Transition-aware   Knowledge for GUI Agent</h2><p><strong>Authors:Bin Xie, Rui Shao, Gongwei Chen, Kaiwen Zhou, Yinchuan Li, Jie Liu, Min Zhang, Liqiang Nie</strong></p>
<p>GUI automation faces critical challenges in dynamic environments. MLLMs suffer from two key issues: misinterpreting UI components and outdated knowledge. Traditional fine-tuning methods are costly for app-specific knowledge updates. We propose GUI-explorer, a training-free GUI agent that incorporates two fundamental mechanisms: (1) Autonomous Exploration of Function-aware Trajectory. To comprehensively cover all application functionalities, we design a Function-aware Task Goal Generator that automatically constructs exploration goals by analyzing GUI structural information (e.g., screenshots and activity hierarchies). This enables systematic exploration to collect diverse trajectories. (2) Unsupervised Mining of Transition-aware Knowledge. To establish precise screen-operation logic, we develop a Transition-aware Knowledge Extractor that extracts effective screen-operation logic through unsupervised analysis the state transition of structured interaction triples (observation, action, outcome). This eliminates the need for human involvement in knowledge extraction. With a task success rate of 53.7% on SPA-Bench and 47.4% on AndroidWorld, GUI-explorer shows significant improvements over SOTA agents. It requires no parameter updates for new apps. GUI-explorer is open-sourced and publicly available at <a target="_blank" rel="noopener" href="https://github.com/JiuTian-VL/GUI-explorer">https://github.com/JiuTian-VL/GUI-explorer</a>. </p>
<blockquote>
<p>å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰è‡ªåŠ¨åŒ–åœ¨åŠ¨æ€ç¯å¢ƒä¸­é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚MLLMsé¢ä¸´ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼šè¯¯è§£UIç»„ä»¶å’ŒçŸ¥è¯†å‚¨å¤‡è¿‡æ—¶ã€‚ä¼ ç»Ÿçš„å¾®è°ƒæ–¹æ³•å¯¹äºç‰¹å®šåº”ç”¨ç¨‹åºçš„çŸ¥è¯†æ›´æ–°æˆæœ¬é«˜æ˜‚ã€‚æˆ‘ä»¬æå‡ºäº†GUI-explorerï¼Œè¿™æ˜¯ä¸€æ¬¾æ— éœ€è®­ç»ƒçš„GUIä»£ç†ï¼Œå®ƒåŒ…å«ä¸¤ç§åŸºæœ¬æœºåˆ¶ï¼šï¼ˆ1ï¼‰åŠŸèƒ½æ„ŸçŸ¥è½¨è¿¹çš„è‡ªä¸»æ¢ç´¢ã€‚ä¸ºäº†å…¨é¢è¦†ç›–æ‰€æœ‰åº”ç”¨ç¨‹åºåŠŸèƒ½ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªåŠŸèƒ½æ„ŸçŸ¥ä»»åŠ¡ç›®æ ‡ç”Ÿæˆå™¨ï¼Œå®ƒé€šè¿‡è‡ªåŠ¨åˆ†æGUIç»“æ„ä¿¡æ¯ï¼ˆä¾‹å¦‚æˆªå›¾å’Œæ´»åŠ¨å±‚æ¬¡ç»“æ„ï¼‰æ¥æ„å»ºæ¢ç´¢ç›®æ ‡ã€‚è¿™èƒ½å¤Ÿå®ç°ç³»ç»Ÿçš„æ¢ç´¢å¹¶æ”¶é›†å„ç§è½¨è¿¹ã€‚ï¼ˆ2ï¼‰è¿‡æ¸¡æ„ŸçŸ¥çŸ¥è¯†çš„æ— ç›‘ç£æŒ–æ˜ã€‚ä¸ºäº†å»ºç«‹ç²¾ç¡®çš„å±å¹•æ“ä½œé€»è¾‘ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªè¿‡æ¸¡æ„ŸçŸ¥çŸ¥è¯†æå–å™¨ï¼Œå®ƒé€šè¿‡æ— ç›‘ç£åˆ†æç»“æ„åŒ–äº¤äº’ä¸‰å…ƒç»„ï¼ˆè§‚å¯Ÿã€è¡ŒåŠ¨ã€ç»“æœï¼‰çš„çŠ¶æ€è¿‡æ¸¡æ¥æå–æœ‰æ•ˆçš„å±å¹•æ“ä½œé€»è¾‘ã€‚è¿™æ¶ˆé™¤äº†äººç±»åœ¨çŸ¥è¯†æå–ä¸­çš„å‚ä¸ã€‚åœ¨SPA-Benchä¸Šï¼ŒGUI-explorerçš„ä»»åŠ¡æˆåŠŸç‡ä¸º53.7%ï¼Œåœ¨AndroidWorldä¸Šä¸º47.4%ï¼Œè¾ƒå…¶ä»–æœ€æ–°ä»£ç†æœ‰æ˜æ˜¾æ”¹è¿›ã€‚GUI-exploreræ— éœ€æ›´æ–°å‚æ•°å³å¯é€‚åº”æ–°åº”ç”¨ç¨‹åºã€‚GUI-explorerå·²å¼€æºå¹¶å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/JiuTian-VL/GUI-explorer%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/JiuTian-VL/GUI-explorerå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16827v1">PDF</a> ACL 2025. Github: <a target="_blank" rel="noopener" href="https://github.com/JiuTian-VL/GUI-explorer">https://github.com/JiuTian-VL/GUI-explorer</a></p>
<p><strong>æ‘˜è¦</strong><br>è‡ªåŠ¨åŒ–å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰é¢ä¸´åŠ¨æ€ç¯å¢ƒä¸­çš„å…³é”®æŒ‘æˆ˜ã€‚MLLMé¢ä¸´ä¸¤å¤§é—®é¢˜ï¼šè¯¯è§£UIç»„ä»¶å’ŒçŸ¥è¯†è¿‡æ—¶ã€‚ä¼ ç»Ÿçš„å¾®è°ƒæ–¹æ³•å¯¹äºåº”ç”¨ç¨‹åºç‰¹å®šçŸ¥è¯†çš„æ›´æ–°æˆæœ¬é«˜æ˜‚ã€‚æˆ‘ä»¬æå‡ºäº†GUIæ¢ç´¢å™¨ï¼Œä¸€ç§æ— éœ€è®­ç»ƒçš„GUIä»£ç†ï¼Œå®ƒç»“åˆäº†ä¸¤ç§åŸºæœ¬æœºåˆ¶ï¼šï¼ˆ1ï¼‰åŠŸèƒ½æ„ŸçŸ¥è½¨è¿¹çš„è‡ªä¸»æ¢ç´¢ã€‚ä¸ºäº†å…¨é¢è¦†ç›–æ‰€æœ‰åº”ç”¨ç¨‹åºåŠŸèƒ½ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åŠŸèƒ½æ„ŸçŸ¥ä»»åŠ¡ç›®æ ‡ç”Ÿæˆå™¨ï¼Œå®ƒé€šè¿‡è‡ªåŠ¨åˆ†æGUIç»“æ„ä¿¡æ¯ï¼ˆå¦‚æˆªå›¾å’Œæ´»åŠ¨å±‚æ¬¡ç»“æ„ï¼‰æ¥æ„å»ºæ¢ç´¢ç›®æ ‡ã€‚è¿™å¯ä»¥é€šè¿‡ç³»ç»Ÿæ¢ç´¢æ”¶é›†å¤šæ ·åŒ–çš„è½¨è¿¹ã€‚ï¼ˆ2ï¼‰è¿‡æ¸¡æ„ŸçŸ¥çŸ¥è¯†çš„æ— ç›‘ç£æŒ–æ˜ã€‚ä¸ºäº†å»ºç«‹ç²¾ç¡®çš„å±å¹•æ“ä½œé€»è¾‘ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§è¿‡æ¸¡æ„ŸçŸ¥çŸ¥è¯†æå–å™¨ï¼Œå®ƒé€šè¿‡æ— ç›‘ç£åˆ†æç»“æ„åŒ–äº¤äº’ä¸‰å…ƒç»„ï¼ˆè§‚å¯Ÿã€è¡ŒåŠ¨ã€ç»“æœï¼‰çš„çŠ¶æ€è½¬æ¢æ¥æå–æœ‰æ•ˆçš„å±å¹•æ“ä½œé€»è¾‘ã€‚è¿™æ¶ˆé™¤äº†çŸ¥è¯†æå–ä¸­äººç±»å‚ä¸çš„éœ€è¦ã€‚åœ¨SPA-Benchä¸ŠGUIæ¢ç´¢å™¨çš„ä»»åŠ¡æˆåŠŸç‡ä¸º53.7%ï¼Œåœ¨AndroidWorldä¸Šä¸º47.4%ï¼Œç›¸è¾ƒäºå…¶ä»–æœ€å…ˆè¿›ä»£ç†æœ‰æ˜æ˜¾æ”¹è¿›ã€‚GUIæ¢ç´¢å™¨æ— éœ€é’ˆå¯¹æ–°åº”ç”¨ç¨‹åºæ›´æ–°å‚æ•°ã€‚GUIæ¢ç´¢å™¨å·²å¼€æºå¹¶å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/JiuTian-VL/GUI-explorer%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/JiuTian-VL/GUI-explorerè·å–ã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>GUIè‡ªåŠ¨åŒ–åœ¨åŠ¨æ€ç¯å¢ƒä¸­é¢ä¸´æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬è¯¯è§£UIç»„ä»¶å’ŒçŸ¥è¯†è¿‡æ—¶é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºGUI-explorerçš„æ— éœ€è®­ç»ƒçš„æ–°GUIä»£ç†ï¼Œè§£å†³ä¼ ç»Ÿæ–¹æ³•çš„å±€é™æ€§ã€‚</li>
<li>GUI-exploreré€šè¿‡è‡ªä¸»æ¢ç´¢å’Œè¿‡æ¸¡æ„ŸçŸ¥çŸ¥è¯†æŒ–æ˜ä¸¤ç§åŸºæœ¬æœºåˆ¶å·¥ä½œã€‚</li>
<li>åŠŸèƒ½æ„ŸçŸ¥ä»»åŠ¡ç›®æ ‡ç”Ÿæˆå™¨é€šè¿‡åˆ†æGUIç»“æ„ä¿¡æ¯è‡ªåŠ¨æ„å»ºæ¢ç´¢ç›®æ ‡ï¼Œå®ç°å…¨é¢çš„åº”ç”¨åŠŸèƒ½è¦†ç›–ã€‚</li>
<li>è¿‡æ¸¡æ„ŸçŸ¥çŸ¥è¯†æå–å™¨é€šè¿‡æ— ç›‘ç£åˆ†æç»“æ„åŒ–äº¤äº’ä¸‰å…ƒç»„çš„çŠ¶æ€è½¬æ¢æ¥æå–å±å¹•æ“ä½œé€»è¾‘ï¼Œæ— éœ€äººå·¥å‚ä¸ã€‚</li>
<li>GUI-exploreråœ¨SPA-Benchå’ŒAndroidWorldä¸Šçš„ä»»åŠ¡æˆåŠŸç‡è¶…è¿‡å…¶ä»–æœ€å…ˆè¿›ä»£ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16827">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4bf39af22c6095505a2720733091358f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-926afe9b889a733bcf5e09d06949526b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a4630743c059f9fcc6fa284f22232b41.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fa279616d8737bd9e815cf5082450b0c.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="A-modular-framework-for-automated-evaluation-of-procedural-content-generation-in-serious-games-with-deep-reinforcement-learning-agents"><a href="#A-modular-framework-for-automated-evaluation-of-procedural-content-generation-in-serious-games-with-deep-reinforcement-learning-agents" class="headerlink" title="A modular framework for automated evaluation of procedural content   generation in serious games with deep reinforcement learning agents"></a>A modular framework for automated evaluation of procedural content   generation in serious games with deep reinforcement learning agents</h2><p><strong>Authors:Eleftherios Kalafatis, Konstantinos Mitsis, Konstantia Zarkogianni, Maria Athanasiou, Konstantina Nikita</strong></p>
<p>Serious Games (SGs) are nowadays shifting focus to include procedural content generation (PCG) in the development process as a means of offering personalized and enhanced player experience. However, the development of a framework to assess the impact of PCG techniques when integrated into SGs remains particularly challenging. This study proposes a methodology for automated evaluation of PCG integration in SGs, incorporating deep reinforcement learning (DRL) game testing agents. To validate the proposed framework, a previously introduced SG featuring card game mechanics and incorporating three different versions of PCG for nonplayer character (NPC) creation has been deployed. Version 1 features random NPC creation, while versions 2 and 3 utilize a genetic algorithm approach. These versions are used to test the impact of different dynamic SG environments on the proposed frameworkâ€™s agents. The obtained results highlight the superiority of the DRL game testing agents trained on Versions 2 and 3 over those trained on Version 1 in terms of win rate (i.e. number of wins per played games) and training time. More specifically, within the execution of a test emulating regular gameplay, both Versions 2 and 3 peaked at a 97% win rate and achieved statistically significant higher (p&#x3D;0009) win rates compared to those achieved in Version 1 that peaked at 94%. Overall, results advocate towards the proposed frameworkâ€™s capability to produce meaningful data for the evaluation of procedurally generated content in SGs. </p>
<blockquote>
<p>å¦‚ä»Šï¼Œä¸¥è‚ƒæ¸¸æˆï¼ˆSGsï¼‰æ­£å°†ç„¦ç‚¹è½¬å‘åœ¨å¼€å‘è¿‡ç¨‹ä¸­åŒ…å«ç¨‹åºå†…å®¹ç”Ÿæˆï¼ˆPCGï¼‰ï¼Œä»¥æä¾›ä¸ªæ€§åŒ–çš„å¢å¼ºç©å®¶ä½“éªŒã€‚ç„¶è€Œï¼Œå»ºç«‹ä¸€ä¸ªæ¡†æ¶æ¥è¯„ä¼°PCGæŠ€æœ¯åœ¨SGsä¸­çš„å½±å“ä»ç„¶æ˜¯ä¸€ä¸ªç‰¹åˆ«å¤§çš„æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§è‡ªåŠ¨åŒ–è¯„ä¼°SGä¸­PCGé›†æˆçš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰æ¸¸æˆæµ‹è¯•ä»£ç†ã€‚ä¸ºäº†éªŒè¯æ‰€æå‡ºçš„æ¡†æ¶ï¼Œå·²ç»éƒ¨ç½²äº†ä¸€æ¬¾ä¹‹å‰å¼•å…¥çš„é‡‡ç”¨å¡ç‰Œæ¸¸æˆæœºåˆ¶çš„SGï¼Œå¹¶èå…¥äº†ä¸‰ç§ä¸åŒç‰ˆæœ¬çš„PCGæ¥è¿›è¡Œéç©å®¶è§’è‰²ï¼ˆNPCï¼‰çš„åˆ›å»ºã€‚ç‰ˆæœ¬1å…·æœ‰éšæœºNPCåˆ›å»ºåŠŸèƒ½ï¼Œè€Œç‰ˆæœ¬2å’Œç‰ˆæœ¬3åˆ™é‡‡ç”¨é—ä¼ ç®—æ³•æ–¹æ³•ã€‚è¿™äº›ç‰ˆæœ¬è¢«ç”¨æ¥æµ‹è¯•ä¸åŒçš„åŠ¨æ€SGç¯å¢ƒå¯¹æ‰€æå‡ºæ¡†æ¶çš„ä»£ç†çš„å½±å“ã€‚è·å¾—çš„ç»“æœçªæ˜¾äº†ç‰ˆæœ¬2å’Œç‰ˆæœ¬3è®­ç»ƒçš„DRLæ¸¸æˆæµ‹è¯•ä»£ç†åœ¨èƒœç‡ï¼ˆå³æ¯åœºæ¸¸æˆçš„è·èƒœæ¬¡æ•°ï¼‰å’Œè®­ç»ƒæ—¶é—´ä¸Šä¼˜äºç‰ˆæœ¬1è®­ç»ƒçš„ä»£ç†ã€‚æ›´å…·ä½“åœ°è¯´ï¼Œåœ¨ä¸€ä¸ªæ¨¡æ‹Ÿå¸¸è§„æ¸¸æˆç©æ³•çš„æµ‹è¯•ä¸­ï¼Œç‰ˆæœ¬2å’Œç‰ˆæœ¬3çš„èƒœç‡è¾¾åˆ°äº†å³°å€¼ï¼Œä¸º97%ï¼Œå¹¶ä¸”å…¶èƒœç‡åœ¨ç»Ÿè®¡å­¦ä¸Šæ˜¾è‘—é«˜äºç‰ˆæœ¬1ï¼ˆp&#x3D;0.0009ï¼‰ï¼Œç‰ˆæœ¬1çš„å³°å€¼èƒœç‡ä¸º94%ã€‚æ€»ä½“è€Œè¨€ï¼Œç»“æœæ”¯æŒæ‰€æå‡ºæ¡†æ¶åœ¨è¯„ä¼°SGä¸­ç¨‹åºç”Ÿæˆå†…å®¹æ–¹é¢çš„èƒ½åŠ›ï¼Œèƒ½å¤Ÿäº§ç”Ÿæœ‰æ„ä¹‰çš„æ•°æ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16801v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å°†ç¨‹åºåŒ–å†…å®¹ç”Ÿæˆï¼ˆPCGï¼‰æŠ€æœ¯é›†æˆåˆ°ä¸¥è‚ƒæ¸¸æˆï¼ˆSGsï¼‰ä¸­ï¼Œå¹¶åº”ç”¨æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰æ¸¸æˆæµ‹è¯•ä»£ç†è¿›è¡Œè‡ªåŠ¨åŒ–è¯„ä¼°çš„æ–¹æ³•è®ºã€‚é€šè¿‡éƒ¨ç½²ä¸€æ¬¾é‡‡ç”¨å¡ç‰Œæ¸¸æˆæœºåˆ¶çš„ä¸¥è‚ƒæ¸¸æˆï¼Œå¹¶è®¾è®¡ä¸‰ç§ä¸åŒç‰ˆæœ¬çš„PCG NPCåˆ›å»ºæŠ€æœ¯è¿›è¡Œæµ‹è¯•éªŒè¯ï¼Œå‘ç°ç‰ˆæœ¬2å’Œç‰ˆæœ¬3ä¸­åº”ç”¨çš„é—ä¼ ç®—æ³•åœ¨è®­ç»ƒå‡ºçš„DRLæ¸¸æˆæµ‹è¯•ä»£ç†æ€§èƒ½ä¸Šè¡¨ç°å‡ºä¼˜è¶Šæ€§ï¼Œå…·ä½“ä½“ç°åœ¨èƒœç‡å’Œè®­ç»ƒæ—¶é—´ä¸Šã€‚ç»“æœæ”¯æŒè¯¥æ¡†æ¶å¯¹ç¨‹åºåŒ–ç”Ÿæˆå†…å®¹è¿›è¡Œæœ‰æ„ä¹‰è¯„ä¼°çš„èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ä¸¥è‚ƒæ¸¸æˆç°åœ¨å°†ç„¦ç‚¹è½¬å‘åœ¨å¼€å‘è¿‡ç¨‹ä¸­åŠ å…¥ç¨‹åºåŒ–å†…å®¹ç”Ÿæˆï¼ˆPCGï¼‰ï¼Œä»¥æä¾›ä¸ªæ€§åŒ–çš„å¢å¼ºæ¸¸æˆä½“éªŒã€‚</li>
<li>æå‡ºäº†ä¸€ç§è‡ªåŠ¨åŒ–è¯„ä¼°PCGé›†æˆåœ¨SGsä¸­çš„æ–¹æ³•è®ºï¼Œåˆ©ç”¨æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰æ¸¸æˆæµ‹è¯•ä»£ç†ã€‚</li>
<li>é€šè¿‡éƒ¨ç½²ä¸€æ¬¾ä¸¥è‚ƒæ¸¸æˆå¹¶è®¾è®¡ä¸‰ä¸ªç‰ˆæœ¬çš„PCG NPCåˆ›å»ºæŠ€æœ¯è¿›è¡Œæµ‹è¯•éªŒè¯ï¼Œå‘ç°ç‰ˆæœ¬2å’Œç‰ˆæœ¬3çš„é—ä¼ ç®—æ³•è¡¨ç°æ›´ä¼˜ç§€ã€‚</li>
<li>DRLæ¸¸æˆæµ‹è¯•ä»£ç†åœ¨ç‰ˆæœ¬2å’Œç‰ˆæœ¬3ä¸­çš„èƒœç‡å’Œè®­ç»ƒæ—¶é—´å‡ä¼˜äºç‰ˆæœ¬1ã€‚</li>
<li>ç»“æœæ”¯æŒè¯¥æ¡†æ¶å¯¹ç¨‹åºåŒ–ç”Ÿæˆå†…å®¹è¿›è¡Œæœ‰æ„ä¹‰è¯„ä¼°çš„èƒ½åŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16801">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5334162e4dadb8f6a0cb3deb97595497.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5b8b874dba097381d05cf78ae8e1d0bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3171f0f36f8f9b885bffc0d69e345571.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-69a7a559179f00fc018f85fc5ed93c3c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8dc8efa905340f9ffde36a0b6be880b6.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="O-2-Searcher-A-Searching-based-Agent-Model-for-Open-Domain-Open-Ended-Question-Answering"><a href="#O-2-Searcher-A-Searching-based-Agent-Model-for-Open-Domain-Open-Ended-Question-Answering" class="headerlink" title="O$^2$-Searcher: A Searching-based Agent Model for Open-Domain Open-Ended   Question Answering"></a>O$^2$-Searcher: A Searching-based Agent Model for Open-Domain Open-Ended   Question Answering</h2><p><strong>Authors:Jianbiao Mei, Tao Hu, Daocheng Fu, Licheng Wen, Xuemeng Yang, Rong Wu, Pinlong Cai, Xing Gao, Yu Yang, Chengjun Xie, Botian Shi, Yong Liu, Yu Qiao</strong></p>
<p>Large Language Models (LLMs), despite their advancements, are fundamentally limited by their static parametric knowledge, hindering performance on tasks requiring open-domain up-to-date information. While enabling LLMs to interact with external knowledge environments is a promising solution, current efforts primarily address closed-end problems. Open-ended questions, which characterized by lacking a standard answer or providing non-unique and diverse answers, remain underexplored. To bridge this gap, we present O$^2$-Searcher, a novel search agent leveraging reinforcement learning to effectively tackle both open-ended and closed-ended questions in the open domain. O$^2$-Searcher leverages an efficient, locally simulated search environment for dynamic knowledge acquisition, effectively decoupling the external world knowledge from modelâ€™s sophisticated reasoning processes. It employs a unified training mechanism with meticulously designed reward functions, enabling the agent to identify problem types and adapt different answer generation strategies. Furthermore, to evaluate performance on complex open-ended tasks, we construct O$^2$-QA, a high-quality benchmark featuring 300 manually curated, multi-domain open-ended questions with associated web page caches. Extensive experiments show that O$^2$-Searcher, using only a 3B model, significantly surpasses leading LLM agents on O$^2$-QA. It also achieves SOTA results on various closed-ended QA benchmarks against similarly-sized models, while performing on par with much larger ones. </p>
<blockquote>
<p>å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å–å¾—äº†è¿›å±•ï¼Œä½†å®ƒä»¬ä»å—åˆ°é™æ€å‚æ•°çŸ¥è¯†çš„æ ¹æœ¬é™åˆ¶ï¼Œè¿™é˜»ç¢äº†å®ƒä»¬åœ¨éœ€è¦å¼€æ”¾é¢†åŸŸæœ€æ–°ä¿¡æ¯çš„ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚è™½ç„¶è®©LLMsä¸å¤–éƒ¨ç¯å¢ƒè¿›è¡Œäº¤äº’æ˜¯ä¸€ä¸ªæœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆï¼Œä½†ç›®å‰çš„åŠªåŠ›ä¸»è¦ä¾§é‡äºè§£å†³å°é—­å¼é—®é¢˜ã€‚å¼€æ”¾å¼é—®é¢˜ç¼ºä¹æ ‡å‡†ç­”æ¡ˆæˆ–æä¾›éå”¯ä¸€å’Œå¤šæ ·åŒ–çš„ç­”æ¡ˆï¼Œä»ç„¶è¢«æ¢ç´¢å¾—ä¸å¤Ÿã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†O$^2$-Searcherï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„æœç´¢ä»£ç†ï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ æœ‰æ•ˆåœ°è§£å†³å¼€æ”¾å¼å’Œå°é—­å¼é—®é¢˜åœ¨å¼€æ”¾é¢†åŸŸçš„é—®é¢˜ã€‚O$^2$-Searcheråˆ©ç”¨é«˜æ•ˆã€å±€éƒ¨æ¨¡æ‹Ÿçš„æœç´¢ç¯å¢ƒè¿›è¡ŒåŠ¨æ€çŸ¥è¯†è·å–ï¼Œæœ‰æ•ˆåœ°å°†å¤–éƒ¨ä¸–ç•Œçš„çŸ¥è¯†ä»æ¨¡å‹çš„å¤æ‚æ¨ç†è¿‡ç¨‹ä¸­è§£è€¦å‡ºæ¥ã€‚å®ƒé‡‡ç”¨ç»Ÿä¸€çš„è®­ç»ƒæœºåˆ¶å’Œç²¾å¿ƒè®¾è®¡çš„å¥–åŠ±å‡½æ•°ï¼Œä½¿ä»£ç†èƒ½å¤Ÿè¯†åˆ«é—®é¢˜ç±»å‹å¹¶é€‚åº”ä¸åŒçš„ç­”æ¡ˆç”Ÿæˆç­–ç•¥ã€‚æ­¤å¤–ï¼Œä¸ºäº†è¯„ä¼°åœ¨å¤æ‚å¼€æ”¾å¼ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œæˆ‘ä»¬æ„å»ºäº†O$^2$-QAï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜è´¨é‡çš„æ ‡å‡†ï¼ŒåŒ…å«300ä¸ªæ‰‹åŠ¨æ•´ç†ã€å¤šé¢†åŸŸçš„å¼€æ”¾å¼é—®é¢˜ä»¥åŠä¸ç›¸å…³ç½‘é¡µç¼“å­˜ç›¸å…³çš„å†…å®¹ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œä»…ä½¿ç”¨3Bæ¨¡å‹çš„O$^2$-Searcheråœ¨O$^2$-QAä¸Šæ˜¾è‘—è¶…è¶Šäº†é¢†å…ˆçš„å¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†ã€‚å®ƒåœ¨å„ç§å°é—­å¼é—®ç­”åŸºå‡†æµ‹è¯•ä¸Šä¹Ÿå–å¾—äº†æœ€æ–°ç»“æœï¼Œåœ¨ä¸åŒç±»è§„æ¨¡çš„æ¨¡å‹ä¸­è¡¨ç°çªå‡ºï¼ŒåŒæ—¶åœ¨ä¸æ›´å¤§çš„æ¨¡å‹çš„è¡¨ç°ä¸ŠæŒå¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16582v1">PDF</a> 25 pages, 9 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¼€æ”¾é¢†åŸŸã€å®æ—¶ä¿¡æ¯çš„ä»»åŠ¡æ—¶çš„å±€é™æ€§ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¼€æ”¾æ€§é—®é¢˜æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæ–‡ç« æå‡ºäº†ä¸€ç§åä¸ºO$^2$-Searcherçš„æ–°å‹æœç´¢ä»£ç†ï¼Œè¯¥ä»£ç†åˆ©ç”¨å¼ºåŒ–å­¦ä¹ æœ‰æ•ˆåº”å¯¹å¼€æ”¾æ€§å’Œå°é—­æ€§é—®é¢˜ã€‚å®ƒé€šè¿‡é«˜æ•ˆçš„æœ¬åœ°æ¨¡æ‹Ÿæœç´¢ç¯å¢ƒå®ç°åŠ¨æ€çŸ¥è¯†è·å–ï¼Œå°†å¤–éƒ¨ä¸–ç•ŒçŸ¥è¯†ä¸æ¨¡å‹çš„å¤æ‚æ¨ç†è¿‡ç¨‹è§£è€¦ã€‚æ­¤å¤–ï¼Œä¸ºè¯„ä¼°åœ¨å¤æ‚å¼€æ”¾æ€§ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œæ–‡ç« æ„å»ºäº†ä¸€ä¸ªé«˜è´¨é‡åŸºå‡†æµ‹è¯•O$^2$-QAã€‚å®éªŒè¡¨æ˜ï¼ŒO$^2$-Searcheråœ¨O$^2$-QAä¸Šçš„è¡¨ç°æ˜¾è‘—è¶…è¶Šäº†é¢†å…ˆçš„å¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†ï¼ŒåŒæ—¶åœ¨å„ç§å°é—­æ€§é—®ç­”åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†é¡¶å°–æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†éœ€è¦å¼€æ”¾é¢†åŸŸå®æ—¶ä¿¡æ¯çš„ä»»åŠ¡æ—¶å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>O$^2$-Searcheræ˜¯ä¸€ç§æ–°å‹æœç´¢ä»£ç†ï¼Œèƒ½å¤„ç†å¼€æ”¾æ€§å’Œå°é—­æ€§é—®é¢˜ã€‚</li>
<li>O$^2$-Searcheråˆ©ç”¨å¼ºåŒ–å­¦ä¹ ï¼Œåœ¨æœ¬åœ°æ¨¡æ‹Ÿæœç´¢ç¯å¢ƒå®ç°åŠ¨æ€çŸ¥è¯†è·å–ã€‚</li>
<li>O$^2$-Searcherå°†å¤–éƒ¨ä¸–ç•ŒçŸ¥è¯†ä¸æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹è§£è€¦ã€‚</li>
<li>ä¸ºè¯„ä¼°åœ¨å¼€æ”¾æ€§ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œæ„å»ºäº†åŸºå‡†æµ‹è¯•O$^2$-QAã€‚</li>
<li>O$^2$-Searcheråœ¨O$^2$-QAä¸Šçš„è¡¨ç°è¶…è¶Šäº†å…¶ä»–å¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16582">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a79122ab1a26aa98808275a60d0d0a34.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b8a2a3bec0c619d0bd283c919a14b9d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d14ba5cf04f2b87a3c074368134e999.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b8577460e43a987c0b9ce0ecd0bcf8e4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-36c9a2ec9ca7826a16f52102bac095ff.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Psychology-driven-LLM-Agents-for-Explainable-Panic-Prediction-on-Social-Media-during-Sudden-Disaster-Events"><a href="#Psychology-driven-LLM-Agents-for-Explainable-Panic-Prediction-on-Social-Media-during-Sudden-Disaster-Events" class="headerlink" title="Psychology-driven LLM Agents for Explainable Panic Prediction on Social   Media during Sudden Disaster Events"></a>Psychology-driven LLM Agents for Explainable Panic Prediction on Social   Media during Sudden Disaster Events</h2><p><strong>Authors:Mengzhu Liu, Zhengqiu Zhu, Chuan Ai, Chen Gao, Xinghong Li, Lingnan He, Kaisheng Lai, Yingfeng Chen, Xin Lu, Yong Li, Quanjun Yin</strong></p>
<p>During sudden disaster events, accurately predicting public panic sentiment on social media is crucial for proactive governance and crisis management. Current efforts on this problem face three main challenges: lack of finely annotated data hinders emotion prediction studies, unmodeled risk perception causes prediction inaccuracies, and insufficient interpretability of panic formation mechanisms. We address these issues by proposing a Psychology-driven generative Agent framework (PsychoAgent) for explainable panic prediction based on emotion arousal theory. Specifically, we first construct a fine-grained open panic emotion dataset (namely COPE) via human-large language models (LLMs) collaboration to mitigate semantic bias. Then, we develop a framework integrating cross-domain heterogeneous data grounded in psychological mechanisms to model risk perception and cognitive differences in emotion generation. To enhance interpretability, we design an LLM-based role-playing agent that simulates individual psychological chains through dedicatedly designed prompts. Experimental results on our annotated dataset show that PsychoAgent improves panic emotion prediction performance by 12.6% to 21.7% compared to baseline models. Furthermore, the explainability and generalization of our approach is validated. Crucially, this represents a paradigm shift from opaque â€œdata-driven fittingâ€ to transparent â€œrole-based simulation with mechanistic interpretationâ€ for panic emotion prediction during emergencies. Our implementation is publicly available at: <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/PsychoAgent-19DD">https://anonymous.4open.science/r/PsychoAgent-19DD</a>. </p>
<blockquote>
<p>åœ¨çªå‘ç¾éš¾äº‹ä»¶ä¸­ï¼Œå‡†ç¡®é¢„æµ‹ç¤¾äº¤åª’ä½“ä¸Šçš„å…¬ä¼—ææ…Œæƒ…ç»ªå¯¹äºç§¯ææ²»ç†å’Œå±æœºç®¡ç†è‡³å…³é‡è¦ã€‚ç›®å‰ï¼Œé’ˆå¯¹è¿™ä¸ªé—®é¢˜çš„ç ”ç©¶é¢ä¸´ä¸‰å¤§æŒ‘æˆ˜ï¼šç¼ºä¹ç²¾ç»†æ ‡æ³¨çš„æ•°æ®é˜»ç¢äº†æƒ…ç»ªé¢„æµ‹ç ”ç©¶ï¼Œæœªå»ºæ¨¡çš„é£é™©æ„ŸçŸ¥å¯¼è‡´é¢„æµ‹ä¸å‡†ç¡®ï¼Œä»¥åŠææ…Œå½¢æˆæœºåˆ¶çš„è§£é‡Šæ€§ä¸è¶³ã€‚æˆ‘ä»¬é€šè¿‡æå‡ºåŸºäºæƒ…ç»ªæ¿€å‘ç†è®ºçš„å¿ƒç†å­¦é©±åŠ¨ç”Ÿæˆä»£ç†æ¡†æ¶ï¼ˆPsychoAgentï¼‰æ¥è§£å†³è¿™äº›é—®é¢˜ï¼Œä»¥å®ç°å¯è§£é‡Šçš„ææ…Œé¢„æµ‹ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬é¦–å…ˆé€šè¿‡äººç±»ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åˆä½œï¼Œæ„å»ºäº†ä¸€ä¸ªç²¾ç»†çš„å¼€æ”¾ææ…Œæƒ…ç»ªæ•°æ®é›†ï¼ˆåä¸ºCOPEï¼‰ï¼Œä»¥å‡è½»è¯­ä¹‰åè§ã€‚ç„¶åï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªæ•´åˆè·¨åŸŸå¼‚æ„æ•°æ®çš„æ¡†æ¶ï¼ŒåŸºäºå¿ƒç†æœºåˆ¶æ¥æ¨¡æ‹Ÿé£é™©æ„ŸçŸ¥å’Œæƒ…ç»ªäº§ç”Ÿçš„è®¤çŸ¥å·®å¼‚ã€‚ä¸ºäº†æé«˜è§£é‡Šæ€§ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªåŸºäºLLMçš„è§’è‰²æ‰®æ¼”ä»£ç†ï¼Œé€šè¿‡ä¸“é—¨è®¾è®¡çš„æç¤ºæ¥æ¨¡æ‹Ÿä¸ªä½“å¿ƒç†é“¾ã€‚åœ¨æˆ‘ä»¬æ ‡æ³¨çš„æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œä¸åŸºå‡†æ¨¡å‹ç›¸æ¯”ï¼ŒPsychoAgentæé«˜äº†12.6%è‡³21.7%çš„ææ…Œæƒ…ç»ªé¢„æµ‹æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•çš„å¯è§£é‡Šæ€§å’Œé€šç”¨æ€§å¾—åˆ°äº†éªŒè¯ã€‚é‡è¦çš„æ˜¯ï¼Œè¿™ä»£è¡¨äº†ä»éšè”½çš„â€œæ•°æ®é©±åŠ¨æ‹Ÿåˆâ€åˆ°é€æ˜çš„â€œåŸºäºè§’è‰²çš„æ¨¡æ‹Ÿä¸æœºåˆ¶è§£é‡Šâ€çš„èŒƒå¼è½¬å˜ï¼Œä¸ºç´§æ€¥æƒ…å†µä¸‹çš„ææ…Œæƒ…ç»ªé¢„æµ‹æä¾›äº†æ–°çš„è§†è§’ã€‚æˆ‘ä»¬çš„å®ç°å…¬å¼€å¯ç”¨ï¼š<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/PsychoAgent-19DD%E3%80%82">https://anonymous.4open.science/r/PsychoAgent-19DDã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16455v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºæƒ…ç»ªæ¿€å‘ç†è®ºçš„å¿ƒç†é©±åŠ¨ç”ŸæˆAgentæ¡†æ¶ï¼ˆPsychoAgentï¼‰å¯å®ç°è§£é‡Šæ€§çš„ææ…Œé¢„æµ‹ã€‚è¯¥ç ”ç©¶æ„å»ºäº†ç²¾ç»†å¼€æ”¾çš„ææ…Œæƒ…ç»ªæ•°æ®é›†COPEï¼Œå¼€å‘äº†ä¸€ä¸ªæ•´åˆè·¨åŸŸå¼‚æ„æ•°æ®çš„æ¡†æ¶ï¼Œæ¨¡æ‹Ÿä¸ªä½“å¿ƒç†é“¾ï¼Œæé«˜é¢„æµ‹çš„å¯è§£é‡Šæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPsychoAgentç›¸è¾ƒäºåŸºå‡†æ¨¡å‹ï¼Œåœ¨ææ…Œæƒ…ç»ªé¢„æµ‹æ€§èƒ½ä¸Šæé«˜äº†12.6%è‡³21.7%ã€‚è¯¥ç ”ç©¶çš„å®æ–½å·²å…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åœ¨çªå‘ç¾éš¾äº‹ä»¶ä¸­ï¼Œå‡†ç¡®é¢„æµ‹ç¤¾äº¤åª’ä½“ä¸Šçš„å…¬ä¼—ææ…Œæƒ…ç»ªå¯¹ç§¯ææ²»ç†å’Œå±æœºç®¡ç†è‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰ç ”ç©¶é¢ä¸´ä¸‰å¤§æŒ‘æˆ˜ï¼šç¼ºä¹ç²¾ç»†æ ‡æ³¨çš„æ•°æ®ã€æœªå»ºæ¨¡çš„é£é™©æ„ŸçŸ¥å¯¼è‡´é¢„æµ‹ä¸å‡†ç¡®ä»¥åŠææ…Œå½¢æˆæœºåˆ¶çš„è§£é‡Šæ€§ä¸è¶³ã€‚</li>
<li>æå‡ºå¿ƒç†é©±åŠ¨ç”ŸæˆAgentæ¡†æ¶ï¼ˆPsychoAgentï¼‰è¿›è¡Œè§£é‡Šæ€§ææ…Œé¢„æµ‹ï¼ŒåŸºäºæƒ…ç»ªæ¿€å‘ç†è®ºã€‚</li>
<li>æ„å»ºäº†ä¸€ä¸ªç²¾ç»†å¼€æ”¾çš„ææ…Œæƒ…ç»ªæ•°æ®é›†ï¼ˆCOPEï¼‰ï¼Œé€šè¿‡äººä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„åˆä½œå‡å°‘è¯­ä¹‰åè§ã€‚</li>
<li>é›†æˆè·¨åŸŸå¼‚æ„æ•°æ®ï¼ŒåŸºäºå¿ƒç†æœºåˆ¶å»ºæ¨¡é£é™©æ„ŸçŸ¥å’Œæƒ…ç»ªäº§ç”Ÿçš„è®¤çŸ¥å·®å¼‚ã€‚</li>
<li>è®¾è®¡äº†åŸºäºLLMçš„è§’è‰²æ‰®æ¼”agentï¼Œæ¨¡æ‹Ÿä¸ªä½“å¿ƒç†é“¾ï¼Œæé«˜é¢„æµ‹çš„å¯è§£é‡Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16455">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ebea3e5e37f8e81e071ebbf1e40f42de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-33c573d0da3e2daf11d35ff165b09c2a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2801b3e83c85d651716d8236a56783b0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Beyond-Static-Testbeds-An-Interaction-Centric-Agent-Simulation-Platform-for-Dynamic-Recommender-Systems"><a href="#Beyond-Static-Testbeds-An-Interaction-Centric-Agent-Simulation-Platform-for-Dynamic-Recommender-Systems" class="headerlink" title="Beyond Static Testbeds: An Interaction-Centric Agent Simulation Platform   for Dynamic Recommender Systems"></a>Beyond Static Testbeds: An Interaction-Centric Agent Simulation Platform   for Dynamic Recommender Systems</h2><p><strong>Authors:Song Jin, Juntian Zhang, Yuhan Liu, Xun Zhang, Yufei Zhang, Guojun Yin, Fei Jiang, Wei Lin, Rui Yan</strong></p>
<p>Evaluating and iterating upon recommender systems is crucial, yet traditional A&#x2F;B testing is resource-intensive, and offline methods struggle with dynamic user-platform interactions. While agent-based simulation is promising, existing platforms often lack a mechanism for user actions to dynamically reshape the environment. To bridge this gap, we introduce RecInter, a novel agent-based simulation platform for recommender systems featuring a robust interaction mechanism. In RecInter platform, simulated user actions (e.g., likes, reviews, purchases) dynamically update item attributes in real-time, and introduced Merchant Agents can reply, fostering a more realistic and evolving ecosystem. High-fidelity simulation is ensured through Multidimensional User Profiling module, Advanced Agent Architecture, and LLM fine-tuned on Chain-of-Thought (CoT) enriched interaction data. Our platform achieves significantly improved simulation credibility and successfully replicates emergent phenomena like Brand Loyalty and the Matthew Effect. Experiments demonstrate that this interaction mechanism is pivotal for simulating realistic system evolution, establishing our platform as a credible testbed for recommender systems research. </p>
<blockquote>
<p>è¯„ä¼°å¹¶è¿­ä»£æ¨èç³»ç»Ÿè‡³å…³é‡è¦ï¼Œç„¶è€Œä¼ ç»Ÿçš„A&#x2F;Bæµ‹è¯•èµ„æºæ¶ˆè€—å¤§ï¼Œä¸”ç¦»çº¿æ–¹æ³•åœ¨åŠ¨æ€ç”¨æˆ·å¹³å°äº¤äº’æ–¹é¢è¡¨ç°æŒ£æ‰ã€‚è™½ç„¶åŸºäºä»£ç†çš„æ¨¡æ‹Ÿå¾ˆæœ‰å‰æ™¯ï¼Œä½†ç°æœ‰å¹³å°é€šå¸¸ç¼ºä¹ä¸€ç§æœºåˆ¶ï¼Œä½¿ç”¨æˆ·è¡Œä¸ºèƒ½å¤ŸåŠ¨æ€åœ°é‡å¡‘ç¯å¢ƒã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†RecInterï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹çš„åŸºäºä»£ç†çš„æ¨èç³»ç»Ÿæ¨¡æ‹Ÿå¹³å°ï¼Œå…·æœ‰ç¨³å¥çš„äº¤äº’æœºåˆ¶ã€‚åœ¨RecInterå¹³å°ä¸Šï¼Œæ¨¡æ‹Ÿçš„ç”¨æˆ·è¡Œä¸ºï¼ˆä¾‹å¦‚å–œæ¬¢ã€è¯„è®ºã€è´­ä¹°ï¼‰ä¼šå®æ—¶åŠ¨æ€æ›´æ–°ç‰©å“å±æ€§ï¼Œå¼•å…¥çš„å•†æˆ·ä»£ç†å¯ä»¥åšå‡ºå›åº”ï¼Œè¥é€ æ›´åŠ çœŸå®å’Œä¸æ–­æ¼”åŒ–çš„ç”Ÿæ€ç³»ç»Ÿã€‚é€šè¿‡å¤šç»´åº¦ç”¨æˆ·åˆ†ææ¨¡å—ã€é«˜çº§ä»£ç†æ¶æ„ä»¥åŠåœ¨å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹ä¸Šå¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œç¡®ä¿äº†é«˜ä¿çœŸæ¨¡æ‹Ÿã€‚æˆ‘ä»¬çš„å¹³å°å¤§å¤§æé«˜äº†æ¨¡æ‹Ÿçš„å¯ä¿¡åº¦ï¼Œå¹¶æˆåŠŸå¤åˆ¶äº†å“ç‰Œå¿ è¯šåº¦å’Œé©¬å¤ªæ•ˆåº”ç­‰çªå‘ç°è±¡ã€‚å®éªŒè¡¨æ˜ï¼Œè¿™ç§äº¤äº’æœºåˆ¶å¯¹äºæ¨¡æ‹ŸçœŸå®ç³»ç»Ÿæ¼”å˜è‡³å…³é‡è¦ï¼Œä½¿æˆ‘ä»¬çš„å¹³å°æˆä¸ºæ¨èç³»ç»Ÿç ”ç©¶çš„å¯ä¿¡æµ‹è¯•å¹³å°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16429v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ¨¡æ‹Ÿç”¨æˆ·è¡Œä¸ºåœ¨æ¨èç³»ç»Ÿä¸­çš„äº¤äº’ï¼Œç‰¹åˆ«æ˜¯å®æ—¶åŠ¨æ€æ›´æ–°çš„åŠŸèƒ½æ­£åœ¨è·å¾—å…³æ³¨ã€‚ç„¶è€Œä¼ ç»Ÿçš„ABæµ‹è¯•æ–¹å¼æˆæœ¬é«˜æ˜‚ï¼Œç°æœ‰ä»¿çœŸå¹³å°çš„äº’åŠ¨èƒ½åŠ›ä¸å¼ºã€‚ä¸ºäº†è§£å†³æ­¤é—®é¢˜ï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸€ä¸ªæ–°å‹ä»¿çœŸå¹³å°RecInterï¼Œå®ƒé€šè¿‡ç”¨æˆ·æ¨¡æ‹Ÿè¡Œä¸ºåŠ¨æ€æ›´æ–°å•†å“å±æ€§ï¼Œå¹¶å¼•å…¥å•†æˆ·ä»£ç†è¿›è¡Œå“åº”ï¼Œä»è€Œæ„å»ºæ›´çœŸå®å’ŒåŠ¨æ€çš„ç”Ÿæ€ç³»ç»Ÿã€‚RecInterå¹³å°é€šè¿‡å¤šç»´åº¦ç”¨æˆ·ç”»åƒæ¨¡å—ã€å…ˆè¿›çš„ä»£ç†æ¶æ„å’Œå¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ä¿è¯é«˜ä¿çœŸæ¨¡æ‹Ÿï¼Œå¹¶æˆåŠŸå¤åˆ¶äº†å“ç‰Œå¿ è¯šå’Œé©¬å¤ªæ•ˆåº”ç­‰ç°å®ç°è±¡ã€‚å¹³å°å»ºç«‹çš„äº’åŠ¨æœºåˆ¶ä¸ºæ¨¡æ‹Ÿç³»ç»Ÿæä¾›äº†çœŸå®å¯é çš„æµ‹è¯•ç¯å¢ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æ¨èç³»ç»Ÿçš„è¯„ä¼°å’Œè¿­ä»£è‡³å…³é‡è¦ï¼Œä½†ä¼ ç»ŸABæµ‹è¯•æˆæœ¬é«˜ä¸”ç¦»çº¿æ–¹æ³•éš¾ä»¥åº”å¯¹åŠ¨æ€çš„ç”¨æˆ·-å¹³å°äº¤äº’ã€‚</li>
<li>ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œå¼•å…¥äº†ä¸€ç§æ–°å‹çš„åŸºäºä»£ç†çš„ä»¿çœŸå¹³å°RecInterã€‚</li>
<li>RecInterå¹³å°é€šè¿‡æ¨¡æ‹Ÿç”¨æˆ·è¡Œä¸ºå®æ—¶æ›´æ–°å•†å“å±æ€§ï¼Œå¹¶å¼•å…¥å•†æˆ·ä»£ç†è¿›è¡Œå“åº”ï¼Œæ„å»ºæ›´çœŸå®å’ŒåŠ¨æ€çš„ç”Ÿæ€ç³»ç»Ÿã€‚</li>
<li>å¹³å°é‡‡ç”¨äº†å¤šç»´åº¦ç”¨æˆ·ç”»åƒæ¨¡å—ã€å…ˆè¿›çš„ä»£ç†æ¶æ„å’Œå¤§å‹è¯­è¨€æ¨¡å‹ä¿è¯é«˜ä¿çœŸæ¨¡æ‹Ÿã€‚</li>
<li>å¹³å°æˆåŠŸå¤åˆ¶äº†å“ç‰Œå¿ è¯šå’Œé©¬å¤ªæ•ˆåº”ç­‰ç°å®ç°è±¡ï¼Œè¯æ˜äº†å…¶æ¨¡æ‹Ÿç³»ç»Ÿçš„çœŸå®æ€§å’Œå¯é æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16429">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-45ae30b2cf691b4f44e30d19dee851ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa756b599220f87c54359b8a78b29c43.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c523463fc1ac4d2cfbc5970fecdc7240.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="WebAgent-R1-Training-Web-Agents-via-End-to-End-Multi-Turn-Reinforcement-Learning"><a href="#WebAgent-R1-Training-Web-Agents-via-End-to-End-Multi-Turn-Reinforcement-Learning" class="headerlink" title="WebAgent-R1: Training Web Agents via End-to-End Multi-Turn Reinforcement   Learning"></a>WebAgent-R1: Training Web Agents via End-to-End Multi-Turn Reinforcement   Learning</h2><p><strong>Authors:Zhepei Wei, Wenlin Yao, Yao Liu, Weizhi Zhang, Qin Lu, Liang Qiu, Changlong Yu, Puyang Xu, Chao Zhang, Bing Yin, Hyokun Yun, Lihong Li</strong></p>
<p>While reinforcement learning (RL) has demonstrated remarkable success in enhancing large language models (LLMs), it has primarily focused on single-turn tasks such as solving math problems. Training effective web agents for multi-turn interactions remains challenging due to the complexity of long-horizon decision-making across dynamic web interfaces. In this work, we present WebAgent-R1, a simple yet effective end-to-end multi-turn RL framework for training web agents. It learns directly from online interactions with web environments by asynchronously generating diverse trajectories, entirely guided by binary rewards depending on task success. Experiments on the WebArena-Lite benchmark demonstrate the effectiveness of WebAgent-R1, boosting the task success rate of Qwen-2.5-3B from 6.1% to 33.9% and Llama-3.1-8B from 8.5% to 44.8%, significantly outperforming existing state-of-the-art methods and strong proprietary models such as OpenAI o3. In-depth analyses reveal the effectiveness of the thinking-based prompting strategy and test-time scaling through increased interactions for web tasks. We further investigate different RL initialization policies by introducing two variants, namely WebAgent-R1-Zero and WebAgent-R1-CoT, which highlight the importance of the warm-up training stage (i.e., behavior cloning) and provide insights on incorporating long chain-of-thought (CoT) reasoning in web agents. </p>
<blockquote>
<p>è™½ç„¶å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆå°±ï¼Œä½†å®ƒä¸»è¦å…³æ³¨äºå¦‚è§£å†³æ•°å­¦é—®é¢˜ç­‰å•è½®ä»»åŠ¡ã€‚ç”±äºè·¨åŠ¨æ€ç½‘é¡µç•Œé¢çš„é•¿å‘¨æœŸå†³ç­–å¤æ‚æ€§ï¼Œè®­ç»ƒç”¨äºå¤šè½®äº¤äº’çš„æœ‰æ•ˆç½‘ç»œä»£ç†ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†WebAgent-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•ä½†æœ‰æ•ˆçš„ç«¯åˆ°ç«¯å¤šè½®å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºè®­ç»ƒç½‘ç»œä»£ç†ã€‚å®ƒç›´æ¥ä»ä¸åœ¨çº¿ç½‘ç»œç¯å¢ƒçš„äº¤äº’ä¸­å­¦ä¹ ï¼Œé€šè¿‡å¼‚æ­¥ç”Ÿæˆå¤šæ ·çš„è½¨è¿¹ï¼Œå®Œå…¨ç”±ä»»åŠ¡æˆåŠŸä¸å¦å†³å®šçš„äºŒå…ƒå¥–åŠ±æ¥å¼•å¯¼ã€‚åœ¨WebArena-LiteåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¯æ˜äº†WebAgent-R1çš„æœ‰æ•ˆæ€§ï¼Œå®ƒå°†Qwen-2.5-3Bçš„ä»»åŠ¡æˆåŠŸç‡ä»6.1%æé«˜åˆ°33.9%ï¼Œå°†Llama-3.1-8Bçš„ä»»åŠ¡æˆåŠŸç‡ä»8.5%æé«˜åˆ°44.8%ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›çš„æ–¹æ³•å’Œå¼ºå¤§çš„ä¸“æœ‰æ¨¡å‹ï¼Œå¦‚OpenAI o3ã€‚æ·±å…¥åˆ†ææ­ç¤ºäº†åŸºäºæ€è€ƒçš„æç¤ºç­–ç•¥å’Œé€šè¿‡å¢åŠ äº¤äº’æ¥è¿›è¡Œæµ‹è¯•æ—¶é—´ç¼©æ”¾å¯¹äºç½‘ç»œä»»åŠ¡çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥é€šè¿‡å¼•å…¥ä¸¤ç§å˜ä½“ï¼Œå³WebAgent-R1-Zeroå’ŒWebAgent-R1-CoTï¼Œæ¥æ¢è®¨ä¸åŒçš„RLåˆå§‹åŒ–ç­–ç•¥ï¼Œè¿™çªå‡ºäº†çƒ­èº«è®­ç»ƒé˜¶æ®µï¼ˆå³è¡Œä¸ºå…‹éš†ï¼‰çš„é‡è¦æ€§ï¼Œå¹¶æä¾›äº†å°†é•¿é“¾æ€ç»´ï¼ˆCoTï¼‰æ¨ç†èå…¥ç½‘ç»œä»£ç†çš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16421v1">PDF</a> Preprint</p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹æ–¹é¢å·²å±•ç°æ˜¾è‘—æˆæ•ˆï¼Œå°¤å…¶åœ¨å•å›åˆä»»åŠ¡å¦‚æ•°å­¦é—®é¢˜æ±‚è§£ä¸­ã€‚ç„¶è€Œï¼Œåœ¨è®­ç»ƒç”¨äºå¤šå›åˆäº¤äº’çš„ç½‘é¡µä»£ç†æ—¶ä»é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æå‡ºWebAgent-R1ï¼Œä¸€ä¸ªç®€æ´æœ‰æ•ˆçš„å¤šå›åˆå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œèƒ½ç›´æ¥ä»ä¸ç½‘é¡µç¯å¢ƒçš„åœ¨çº¿äº¤äº’ä¸­å­¦ä¹ ï¼Œé€šè¿‡å¼‚æ­¥ç”Ÿæˆå¤šæ ·åŒ–è½¨è¿¹ï¼Œä»…æ ¹æ®ä»»åŠ¡æˆåŠŸä¸å¦æä¾›äºŒå…ƒå¥–åŠ±ã€‚åœ¨WebArena-LiteåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒæ˜¾ç¤ºï¼ŒWebAgent-R1æ˜¾è‘—æå‡ä»»åŠ¡æˆåŠŸç‡ï¼Œç›¸è¾ƒäºQwen-2.5-3Bæ¨¡å‹æå‡ä»6.1%è‡³33.9%ï¼ŒLlama-3.1-8Bæ¨¡å‹ä»8.5%æå‡è‡³44.8%ï¼Œæ˜¾è‘—è¶…è¶Šç°æœ‰å…ˆè¿›æ–¹æ³•ä¸å¼ºå¤§ä¸“æœ‰æ¨¡å‹å¦‚OpenAI o3ã€‚æ·±å…¥åˆ†ææ˜¾ç¤ºåŸºäºæ€è€ƒæç¤ºç­–ç•¥çš„æœ‰æ•ˆæ€§ä»¥åŠé€šè¿‡å¢åŠ äº¤äº’çš„æµ‹è¯•æ—¶é—´ç¼©æ”¾å¯¹ç½‘é¡µä»»åŠ¡çš„æ•ˆæœã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥ä¸¤ç§å¼ºåŒ–å­¦ä¹ åˆå§‹åŒ–ç­–ç•¥å˜ä½“ï¼Œçªæ˜¾çƒ­èº«è®­ç»ƒé˜¶æ®µï¼ˆè¡Œä¸ºå…‹éš†ï¼‰çš„é‡è¦æ€§ï¼Œå¹¶æä¾›å°†é•¿é“¾æ€ç»´èå…¥ç½‘é¡µä»£ç†çš„è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>WebAgent-R1æ˜¯ä¸€ä¸ªç”¨äºè®­ç»ƒç½‘é¡µä»£ç†çš„å¤šå›åˆå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå¯ä»ä¸ç½‘é¡µç¯å¢ƒçš„åœ¨çº¿äº¤äº’ä¸­ç›´æ¥å­¦ä¹ ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡å¼‚æ­¥ç”Ÿæˆå¤šæ ·åŒ–è½¨è¿¹ï¼Œä»…ä¾æ®ä»»åŠ¡æˆåŠŸä¸å¦æä¾›äºŒå…ƒå¥–åŠ±ã€‚</li>
<li>åœ¨WebArena-LiteåŸºå‡†æµ‹è¯•ä¸Šï¼ŒWebAgent-R1æ˜¾è‘—æå‡ä»»åŠ¡æˆåŠŸç‡ï¼Œç›¸è¾ƒå…¶ä»–æ¨¡å‹æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚</li>
<li>åŸºäºæ€è€ƒæç¤ºç­–ç•¥åœ¨ç½‘é¡µä»»åŠ¡ä¸­å±•ç°å‡ºæœ‰æ•ˆæ€§ã€‚</li>
<li>å¢åŠ äº¤äº’çš„æµ‹è¯•æ—¶é—´ç¼©æ”¾èƒ½å¤Ÿæé«˜ç½‘é¡µä»£ç†çš„æ€§èƒ½ã€‚</li>
<li>å¼•å…¥çš„ä¸¤ç§RLåˆå§‹åŒ–ç­–ç•¥å˜ä½“çªæ˜¾çƒ­èº«è®­ç»ƒé˜¶æ®µçš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16421">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5b644a27521daa3e125cf52c31b8ad4b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c4742907be9c02c3f5eddd2c23676c50.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-980744fa5f8b8bd8f97c52ebdbe2ff96.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a2f96938cbbf52620e1896460df54ea3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0ddb9172526b07885c64909614a637b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f85ab7cab094195f36a92b0579a0b449.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Embodied-Agents-Meet-Personalization-Exploring-Memory-Utilization-for-Personalized-Assistance"><a href="#Embodied-Agents-Meet-Personalization-Exploring-Memory-Utilization-for-Personalized-Assistance" class="headerlink" title="Embodied Agents Meet Personalization: Exploring Memory Utilization for   Personalized Assistance"></a>Embodied Agents Meet Personalization: Exploring Memory Utilization for   Personalized Assistance</h2><p><strong>Authors:Taeyoon Kwon, Dongwook Choi, Sunghwan Kim, Hyojun Kim, Seungjun Moon, Beong-woo Kwak, Kuan-Hao Huang, Jinyoung Yeo</strong></p>
<p>Embodied agents empowered by large language models (LLMs) have shown strong performance in household object rearrangement tasks. However, these tasks primarily focus on single-turn interactions with simplified instructions, which do not truly reflect the challenges of providing meaningful assistance to users. To provide personalized assistance, embodied agents must understand the unique semantics that users assign to the physical world (e.g., favorite cup, breakfast routine) by leveraging prior interaction history to interpret dynamic, real-world instructions. Yet, the effectiveness of embodied agents in utilizing memory for personalized assistance remains largely underexplored. To address this gap, we present MEMENTO, a personalized embodied agent evaluation framework designed to comprehensively assess memory utilization capabilities to provide personalized assistance. Our framework consists of a two-stage memory evaluation process design that enables quantifying the impact of memory utilization on task performance. This process enables the evaluation of agentsâ€™ understanding of personalized knowledge in object rearrangement tasks by focusing on its role in goal interpretation: (1) the ability to identify target objects based on personal meaning (object semantics), and (2) the ability to infer object-location configurations from consistent user patterns, such as routines (user patterns). Our experiments across various LLMs reveal significant limitations in memory utilization, with even frontier models like GPT-4o experiencing a 30.5% performance drop when required to reference multiple memories, particularly in tasks involving user patterns. These findings, along with our detailed analyses and case studies, provide valuable insights for future research in developing more effective personalized embodied agents. Project website: <a target="_blank" rel="noopener" href="https://connoriginal.github.io/MEMENTO">https://connoriginal.github.io/MEMENTO</a> </p>
<blockquote>
<p>ç”±å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èµ‹èƒ½çš„å®ä½“ä»£ç†äººåœ¨å®¶åº­ç‰©å“é‡æ–°å¸ƒç½®ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™äº›ä»»åŠ¡ä¸»è¦ä¾§é‡äºä¸ç®€åŒ–æŒ‡ä»¤çš„å•è½®äº¤äº’ï¼Œå¹¶ä¸èƒ½çœŸæ­£åæ˜ ä¸ºç”¨æˆ·æä¾›æœ‰æ„ä¹‰å¸®åŠ©çš„æŒ‘æˆ˜ã€‚ä¸ºäº†æä¾›ä¸ªæ€§åŒ–çš„å¸®åŠ©ï¼Œå®ä½“ä»£ç†äººå¿…é¡»åˆ©ç”¨å…ˆå‰çš„äº¤äº’å†å²æ¥ç†è§£ç”¨æˆ·èµ‹äºˆç‰©ç†ä¸–ç•Œçš„ç‹¬ç‰¹è¯­ä¹‰ï¼ˆä¾‹å¦‚ï¼Œæœ€å–œæ¬¢çš„æ¯å­ã€æ—©é¤ä¾‹è¡Œç¨‹åºï¼‰ï¼Œä»¥è§£é‡ŠåŠ¨æ€ã€ç°å®ä¸–ç•Œçš„æŒ‡ä»¤ã€‚ç„¶è€Œï¼Œå®ä½“ä»£ç†äººåœ¨åˆ©ç”¨è®°å¿†æä¾›ä¸ªæ€§åŒ–å¸®åŠ©æ–¹é¢çš„æœ‰æ•ˆæ€§åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå°šæœªè¢«æ¢ç´¢ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æ¨å‡ºäº†MEMENTOï¼Œè¿™æ˜¯ä¸€ä¸ªä¸ªæ€§åŒ–çš„å®ä½“ä»£ç†äººè¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨å…¨é¢è¯„ä¼°åˆ©ç”¨è®°å¿†æä¾›ä¸ªæ€§åŒ–å¸®åŠ©çš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ¡†æ¶ç”±ä¸¤é˜¶æ®µè®°å¿†è¯„ä¼°è¿‡ç¨‹è®¾è®¡ç»„æˆï¼Œèƒ½å¤Ÿé‡åŒ–è®°å¿†åˆ©ç”¨å¯¹ä»»åŠ¡æ€§èƒ½çš„å½±å“ã€‚è¿™ä¸ªè¿‡ç¨‹é€šè¿‡å…³æ³¨è®°å¿†åœ¨ç›®æ ‡è§£é‡Šä¸­çš„é‡è¦ä½œç”¨ï¼Œæ¥è¯„ä¼°ä»£ç†äººåœ¨ç‰©å“é‡æ–°å¸ƒç½®ä»»åŠ¡ä¸­å¯¹ä¸ªæ€§åŒ–çŸ¥è¯†çš„ç†è§£èƒ½åŠ›ï¼šï¼ˆ1ï¼‰åŸºäºä¸ªäººæ„ä¹‰ï¼ˆå¯¹è±¡è¯­ä¹‰ï¼‰è¯†åˆ«ç›®æ ‡å¯¹è±¡çš„èƒ½åŠ›ï¼›ï¼ˆ2ï¼‰ä»ä¸€è‡´çš„ç”¨æˆ·æ¨¡å¼ä¸­æ¨æ–­å¯¹è±¡ä½ç½®é…ç½®çš„èƒ½åŠ›ï¼Œä¾‹å¦‚ä¾‹è¡Œç¨‹åºï¼ˆç”¨æˆ·æ¨¡å¼ï¼‰ã€‚æˆ‘ä»¬åœ¨å„ç§LLMsä¸Šçš„å®éªŒæ­ç¤ºäº†è®°å¿†åˆ©ç”¨çš„æ˜¾è‘—å±€é™æ€§ï¼Œç”šè‡³åƒGPT-4oè¿™æ ·çš„å‰æ²¿æ¨¡å‹åœ¨éœ€è¦å‚è€ƒå¤šæ¡è®°å¿†æ—¶æ€§èƒ½ä¸‹é™äº†30.5%ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¶‰åŠç”¨æˆ·æ¨¡å¼çš„ä»»åŠ¡ä¸­ã€‚è¿™äº›å‘ç°ï¼Œä»¥åŠæˆ‘ä»¬çš„è¯¦ç»†åˆ†æå’Œæ¡ˆä¾‹ç ”ç©¶ï¼Œä¸ºæœªæ¥ç ”ç©¶å¼€å‘æ›´æœ‰æ•ˆçš„ä¸ªæ€§åŒ–å®ä½“ä»£ç†äººæä¾›äº†å®è´µçš„è§è§£ã€‚é¡¹ç›®ç½‘ç«™ï¼š<a target="_blank" rel="noopener" href="https://connoriginal.github.io/MEMENTO">https://connoriginal.github.io/MEMENTO</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16348v1">PDF</a> Work in progress</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹èµ‹èƒ½çš„å®ä½“æ™ºèƒ½ä»£ç†åœ¨å®¶åº­ç‰©ä½“é‡æ–°å¸ƒç½®ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§æ€§èƒ½ï¼Œä½†ç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ç®€å•çš„å•å›åˆäº¤äº’ä»»åŠ¡ä¸Šï¼Œæ— æ³•çœŸæ­£åæ˜ ä¸ºç”¨æˆ·æä¾›çš„ä¸ªæ€§åŒ–æœåŠ¡çš„æŒ‘æˆ˜ã€‚ä¸ºç”¨æˆ·æä¾›ä¸ªæ€§åŒ–æœåŠ¡ï¼Œå®ä½“æ™ºèƒ½ä»£ç†éœ€è¦ç†è§£ç”¨æˆ·èµ‹äºˆç‰©ç†ä¸–ç•Œçš„ç‹¬ç‰¹è¯­ä¹‰ï¼Œå¹¶åˆ©ç”¨è¿‡å»çš„äº¤äº’å†å²æ¥è§£è¯»åŠ¨æ€ã€çœŸå®ä¸–ç•Œçš„æŒ‡ä»¤ã€‚ç„¶è€Œï¼Œå®ä½“æ™ºèƒ½ä»£ç†åœ¨åˆ©ç”¨è®°å¿†æä¾›ä¸ªæ€§åŒ–æœåŠ¡æ–¹é¢çš„æœ‰æ•ˆæ€§å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ã€‚ä¸ºè§£å†³è¿™ä¸€ç©ºç™½ï¼Œæœ¬æ–‡æå‡ºäº†MEMENTOä¸ªæ€§åŒ–å®ä½“æ™ºèƒ½ä»£ç†è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨å…¨é¢è¯„ä¼°è®°å¿†åˆ©ç”¨èƒ½åŠ›ä»¥æä¾›ä¸ªæ€§åŒ–æœåŠ¡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¤§å‹è¯­è¨€æ¨¡å‹åœ¨è®°å¿†åˆ©ç”¨æ–¹é¢å­˜åœ¨æ˜¾è‘—å±€é™ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¨¡å‹å¦‚GPT-4oåœ¨éœ€è¦å‚è€ƒå¤šä¸ªè®°å¿†çš„ä»»åŠ¡ä¸­æ€§èƒ½ä¸‹é™30.5%ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¶‰åŠç”¨æˆ·æ¨¡å¼çš„ä»»åŠ¡ä¸­ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹èµ‹èƒ½çš„å®ä½“æ™ºèƒ½ä»£ç†åœ¨å®¶åº­ç‰©ä½“é‡æ–°å¸ƒç½®ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚</li>
<li>å½“å‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ç®€å•çš„å•å›åˆäº¤äº’ä»»åŠ¡ä¸Šï¼Œå¿½ç•¥äº†ä¸ºç”¨æˆ·æä¾›ä¸ªæ€§åŒ–æœåŠ¡çš„æŒ‘æˆ˜ã€‚</li>
<li>ä¸ºæä¾›ä¸ªæ€§åŒ–æœåŠ¡ï¼Œå®ä½“æ™ºèƒ½ä»£ç†éœ€è¦ç†è§£ç”¨æˆ·èµ‹äºˆçš„ç‰©ç†ä¸–ç•Œçš„ç‹¬ç‰¹è¯­ä¹‰ï¼Œå¹¶åˆ©ç”¨è¿‡å»çš„äº¤äº’å†å²è§£è¯»çœŸå®ä¸–ç•Œçš„æŒ‡ä»¤ã€‚</li>
<li>MEMENTOæ¡†æ¶ç”¨äºè¯„ä¼°å®ä½“æ™ºèƒ½ä»£ç†çš„è®°å¿†åˆ©ç”¨èƒ½åŠ›ä»¥æä¾›ä¸ªæ€§åŒ–æœåŠ¡ã€‚</li>
<li>å®éªŒå‘ç°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è®°å¿†åˆ©ç”¨æ–¹é¢å­˜åœ¨å±€é™ï¼Œå°¤å…¶åœ¨æ¶‰åŠç”¨æˆ·æ¨¡å¼çš„ä»»åŠ¡ä¸­æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚</li>
<li>å¤šä¸ªè®°å¿†å‚è€ƒçš„ä»»åŠ¡ä¸­ï¼Œå³ä½¿æ˜¯å…ˆè¿›çš„æ¨¡å‹å¦‚GPT-4oä¹Ÿé¢ä¸´æ€§èƒ½æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16348">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-595e052b9d053cd433f64a1c60b8dbea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4af9c0e019ec8eeb68c49ceb235fa58a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-834ed23c654082c86b0f628e48681b1b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6b9428e2d3e9374c2f1b0aeea4bc13ea.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="GUI-G1-Understanding-R1-Zero-Like-Training-for-Visual-Grounding-in-GUI-Agents"><a href="#GUI-G1-Understanding-R1-Zero-Like-Training-for-Visual-Grounding-in-GUI-Agents" class="headerlink" title="GUI-G1: Understanding R1-Zero-Like Training for Visual Grounding in GUI   Agents"></a>GUI-G1: Understanding R1-Zero-Like Training for Visual Grounding in GUI   Agents</h2><p><strong>Authors:Yuqi Zhou, Sunhao Dai, Shuai Wang, Kaiwen Zhou, Qinglin Jia, Jun Xu</strong></p>
<p>Recent Graphical User Interface (GUI) agents replicate the R1-Zero paradigm, coupling online Reinforcement Learning (RL) with explicit chain-of-thought reasoning prior to object grounding and thereby achieving substantial performance gains. In this paper, we first conduct extensive analysis experiments of three key components of that training pipeline: input design, output evaluation, and policy update-each revealing distinct challenges arising from blindly applying general-purpose RL without adapting to GUI grounding tasks. Input design: Current templates encourage the model to generate chain-of-thought reasoning, but longer chains unexpectedly lead to worse grounding performance. Output evaluation: Reward functions based on hit signals or box area allow models to exploit box size, leading to reward hacking and poor localization quality. Policy update: Online RL tends to overfit easy examples due to biases in length and sample difficulty, leading to under-optimization on harder cases. To address these issues, we propose three targeted solutions. First, we adopt a Fast Thinking Template that encourages direct answer generation, reducing excessive reasoning during training. Second, we incorporate a box size constraint into the reward function to mitigate reward hacking. Third, we revise the RL objective by adjusting length normalization and adding a difficulty-aware scaling factor, enabling better optimization on hard samples. Our GUI-G1-3B, trained on 17K public samples with Qwen2.5-VL-3B-Instruct, achieves 90.3% accuracy on ScreenSpot and 37.1% on ScreenSpot-Pro. This surpasses all prior models of similar size and even outperforms the larger UI-TARS-7B, establishing a new state-of-the-art in GUI agent grounding. The project repository is available at <a target="_blank" rel="noopener" href="https://github.com/Yuqi-Zhou/GUI-G1">https://github.com/Yuqi-Zhou/GUI-G1</a>. </p>
<blockquote>
<p>æœ€è¿‘çš„å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†å¤åˆ¶äº†R1-ZeroèŒƒå¼ï¼Œå°†åœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸å¯¹è±¡æ¥åœ°ä¹‹å‰çš„æ˜¾å¼æ€ç»´é“¾æ¨ç†ç›¸ç»“åˆï¼Œä»è€Œå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆå¯¹è¯¥è®­ç»ƒç®¡é“çš„ä¸‰ä¸ªå…³é”®ç»„ä»¶è¿›è¡Œäº†å¹¿æ³›çš„åˆ†æå®éªŒï¼šè¾“å…¥è®¾è®¡ã€è¾“å‡ºè¯„ä¼°å’Œç­–ç•¥æ›´æ–°â€”â€”æ¯ä¸€ä¸ªéƒ½æ­ç¤ºäº†ç›²ç›®åº”ç”¨é€šç”¨RLè€Œä¸é€‚åº”GUIæ¥åœ°ä»»åŠ¡æ‰€äº§ç”Ÿçš„ç‹¬ç‰¹æŒ‘æˆ˜ã€‚è¾“å…¥è®¾è®¡ï¼šå½“å‰æ¨¡æ¿é¼“åŠ±æ¨¡å‹ç”Ÿæˆæ€ç»´é“¾æ¨ç†ï¼Œä½†æ›´é•¿çš„é“¾æ¡å‡ºä¹æ„æ–™åœ°å¯¼è‡´æ›´å·®çš„æ¥åœ°æ€§èƒ½ã€‚è¾“å‡ºè¯„ä¼°ï¼šåŸºäºå‘½ä¸­ä¿¡å·æˆ–æ¡†åŒºåŸŸçš„å¥–åŠ±å‡½æ•°å…è®¸æ¨¡å‹åˆ©ç”¨æ¡†å¤§å°ï¼Œä»è€Œå¯¼è‡´å¥–åŠ±ä½œå¼Šå’Œå®šä½è´¨é‡å·®ã€‚ç­–ç•¥æ›´æ–°ï¼šç”±äºé•¿åº¦å’Œæ ·æœ¬éš¾åº¦çš„åè§ï¼Œåœ¨çº¿RLå¾€å¾€ä¼šå¯¹ç®€å•ç¤ºä¾‹è¿›è¡Œè¿‡åº¦æ‹Ÿåˆï¼Œå¯¼è‡´åœ¨æ›´å¤æ‚æƒ…å†µä¸‹çš„ä¼˜åŒ–ä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸‰ç§æœ‰é’ˆå¯¹æ€§çš„è§£å†³æ–¹æ¡ˆã€‚é¦–å…ˆï¼Œæˆ‘ä»¬é‡‡ç”¨å¿«é€Ÿæ€è€ƒæ¨¡æ¿ï¼Œé¼“åŠ±ç›´æ¥ç­”æ¡ˆç”Ÿæˆï¼Œå‡å°‘è®­ç»ƒè¿‡ç¨‹ä¸­çš„è¿‡åº¦æ¨ç†ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å°†æ¡†å¤§å°çº¦æŸçº³å…¥å¥–åŠ±å‡½æ•°ï¼Œä»¥å‡è½»å¥–åŠ±ä½œå¼Šã€‚ç¬¬ä¸‰ï¼Œæˆ‘ä»¬é€šè¿‡è°ƒæ•´é•¿åº¦å½’ä¸€åŒ–å¹¶æ·»åŠ ä¸€ä¸ªéš¾åº¦æ„ŸçŸ¥ç¼©æ”¾å› å­æ¥ä¿®è®¢RLç›®æ ‡ï¼Œä»è€Œåœ¨å›°éš¾æ ·æœ¬ä¸Šå®ç°æ›´å¥½çš„ä¼˜åŒ–ã€‚æˆ‘ä»¬çš„GUI-G1-3Båœ¨17Kå…¬å…±æ ·æœ¬ä¸Šä½¿ç”¨Qwen2.5-VL-3B-Instructè¿›è¡Œè®­ç»ƒï¼Œåœ¨ScreenSpotä¸Šè¾¾åˆ°90.3%çš„å‡†ç¡®ç‡ï¼Œåœ¨ScreenSpot-Proä¸Šè¾¾åˆ°37.1%çš„å‡†ç¡®ç‡ã€‚è¿™è¶…è¶Šäº†ç±»ä¼¼å¤§å°çš„æ‰€æœ‰å…ˆå‰æ¨¡å‹ï¼Œç”šè‡³è¶…è¶Šäº†æ›´å¤§çš„UI-TARS-7Bï¼Œåœ¨GUIä»£ç†æ¥åœ°é¢†åŸŸåˆ›é€ äº†æ–°çš„æœ€å…ˆè¿›çš„æ°´å¹³ã€‚é¡¹ç›®ä»“åº“å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Yuqi-Zhou/GUI-G1%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Yuqi-Zhou/GUI-G1æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15810v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡åˆ†æäº†å½“å‰å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ‰€é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†é’ˆå¯¹æ€§çš„è§£å†³æ–¹æ¡ˆã€‚ç ”ç©¶å‘ç°åœ¨è¾“å…¥è®¾è®¡ã€è¾“å‡ºè¯„ä»·å’Œç­–ç•¥æ›´æ–°ä¸‰ä¸ªæ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†å¿«é€Ÿæ€è€ƒæ¨¡æ¿ã€å¼•å…¥ç›’å°ºå¯¸çº¦æŸå’Œè°ƒæ•´RLç›®æ ‡ç­‰æªæ–½ã€‚ç»è¿‡æ”¹è¿›ï¼ŒGUI-G1-3Bæ¨¡å‹åœ¨ScreenSpotå’ŒScreenSpot-Proä¸Šå–å¾—äº†æ˜¾è‘—çš„æˆç»©ï¼Œè¾¾åˆ°äº†åŒç±»æ¨¡å‹ä¸­çš„æœ€ä½³æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>GUIä»£ç†åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é¢ä¸´è¾“å…¥è®¾è®¡ã€è¾“å‡ºè¯„ä»·å’Œç­–ç•¥æ›´æ–°çš„æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ¨¡æ¿é¼“åŠ±ç”Ÿæˆé“¾å¼æ€è€ƒæ¨ç†ï¼Œä½†è¿‡é•¿çš„é“¾ä¼šå¯¼è‡´å®šä½æ€§èƒ½ä¸‹é™ã€‚</li>
<li>åŸºäºå‘½ä¸­ä¿¡å·æˆ–æ¡†é¢ç§¯çš„å¥–åŠ±å‡½æ•°å¯èƒ½å¯¼è‡´æ¨¡å‹åˆ©ç”¨æ¡†å¤§å°ï¼Œä»è€Œäº§ç”Ÿå¥–åŠ±é»‘å®¢è¡Œä¸ºå’Œä½å®šä½è´¨é‡ã€‚</li>
<li>åœ¨çº¿RLå€¾å‘äºå¯¹ç®€å•æ ·æœ¬è¿‡åº¦æ‹Ÿåˆï¼Œå¯¼è‡´åœ¨æ›´å¤æ‚æ¡ˆä¾‹ä¸Šçš„ä¼˜åŒ–ä¸è¶³ã€‚</li>
<li>ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œé‡‡å–äº†å¿«é€Ÿæ€è€ƒæ¨¡æ¿ã€å¼•å…¥ç›’å°ºå¯¸çº¦æŸå’Œè°ƒæ•´RLç›®æ ‡ç­‰é’ˆå¯¹æ€§æªæ–½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15810">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-93e4aa858e84fbb59a5d1f3ef62ecec9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5fd4ff352e6c6e52c3811808acbab2c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a4634f1c6008752c48af1f39095efa3d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-057aeeb66cc0433f578069a2a100f587.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48bc256426467f534ecbb8e61d18a007.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3aa0d59216827c0371e9e8af5d495cc1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0b807a67082931ee47779fc1beb7ec4.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Multiple-Weaks-Win-Single-Strong-Large-Language-Models-Ensemble-Weak-Reinforcement-Learning-Agents-into-a-Supreme-One"><a href="#Multiple-Weaks-Win-Single-Strong-Large-Language-Models-Ensemble-Weak-Reinforcement-Learning-Agents-into-a-Supreme-One" class="headerlink" title="Multiple Weaks Win Single Strong: Large Language Models Ensemble Weak   Reinforcement Learning Agents into a Supreme One"></a>Multiple Weaks Win Single Strong: Large Language Models Ensemble Weak   Reinforcement Learning Agents into a Supreme One</h2><p><strong>Authors:Yiwen Song, Qianyue Hao, Qingmin Liao, Jian Yuan, Yong Li</strong></p>
<p>Model ensemble is a useful approach in reinforcement learning (RL) for training effective agents. Despite wide success of RL, training effective agents remains difficult due to the multitude of factors requiring careful tuning, such as algorithm selection, hyperparameter settings, and even random seed choices, all of which can significantly influence an agentâ€™s performance. Model ensemble helps overcome this challenge by combining multiple weak agents into a single, more powerful one, enhancing overall performance. However, existing ensemble methods, such as majority voting and Boltzmann addition, are designed as fixed strategies and lack a semantic understanding of specific tasks, limiting their adaptability and effectiveness. To address this, we propose LLM-Ens, a novel approach that enhances RL model ensemble with task-specific semantic understandings driven by large language models (LLMs). Given a task, we first design an LLM to categorize states in this task into distinct â€˜situationsâ€™, incorporating high-level descriptions of the task conditions. Then, we statistically analyze the strengths and weaknesses of each individual agent to be used in the ensemble in each situation. During the inference time, LLM-Ens dynamically identifies the changing task situation and switches to the agent that performs best in the current situation, ensuring dynamic model selection in the evolving task condition. Our approach is designed to be compatible with agents trained with different random seeds, hyperparameter settings, and various RL algorithms. Extensive experiments on the Atari benchmark show that LLM-Ens significantly improves the RL model ensemble, surpassing well-known baselines by up to 20.9%. For reproducibility, our code is open-source at <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/LLM4RLensemble-F7EE">https://anonymous.4open.science/r/LLM4RLensemble-F7EE</a>. </p>
<blockquote>
<p>æ¨¡å‹é›†æˆæ˜¯ä¸€ç§åœ¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸­è®­ç»ƒæœ‰æ•ˆä»£ç†çš„æœ‰ç”¨æ–¹æ³•ã€‚å°½ç®¡å¼ºåŒ–å­¦ä¹ å–å¾—äº†å¹¿æ³›çš„æˆåŠŸï¼Œä½†ç”±äºç®—æ³•é€‰æ‹©ã€è¶…å‚æ•°è®¾ç½®ç”šè‡³éšæœºç§å­é€‰æ‹©ç­‰éœ€è¦ä»”ç»†è°ƒæ•´çš„å¤šé‡å› ç´ ï¼Œè®­ç»ƒæœ‰æ•ˆä»£ç†ä»ç„¶å¾ˆå›°éš¾ï¼Œæ‰€æœ‰è¿™äº›å› ç´ éƒ½å¯èƒ½æ˜¾è‘—å½±å“ä»£ç†çš„æ€§èƒ½ã€‚æ¨¡å‹é›†æˆé€šè¿‡æŠŠå¤šä¸ªå¼±ä»£ç†åˆå¹¶æˆä¸€ä¸ªæ›´å¼ºå¤§çš„å•ä¸€ä»£ç†ï¼Œæé«˜æ•´ä½“æ€§èƒ½ï¼Œä»è€Œå…‹æœè¿™ä¸€æŒ‘æˆ˜ã€‚ç„¶è€Œï¼Œç°æœ‰çš„é›†æˆæ–¹æ³•ï¼Œå¦‚å¤šæ•°æŠ•ç¥¨å’Œç»å°”å…¹æ›¼åŠ æ³•ï¼Œè¢«è®¾è®¡ä¸ºå›ºå®šç­–ç•¥ï¼Œç¼ºä¹å¯¹ç‰¹å®šä»»åŠ¡çš„è¯­ä¹‰ç†è§£ï¼Œé™åˆ¶äº†å®ƒä»¬çš„é€‚åº”æ€§å’Œæœ‰æ•ˆæ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†LLM-Ensï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„ä»»åŠ¡ç‰¹å®šè¯­ä¹‰ç†è§£ï¼Œå¢å¼ºRLæ¨¡å‹é›†æˆçš„æ–°å‹æ–¹æ³•ã€‚ç»™å®šä¸€ä¸ªä»»åŠ¡ï¼Œæˆ‘ä»¬é¦–å…ˆå¯¹LLMè¿›è¡Œè®¾è®¡ï¼Œå°†è¿™ä¸ªä»»åŠ¡çš„çŠ¶æ€åˆ†ç±»ä¸ºä¸åŒçš„â€œæƒ…å¢ƒâ€ï¼Œå¹¶èå…¥ä»»åŠ¡æ¡ä»¶çš„é«˜çº§æè¿°ã€‚ç„¶åï¼Œæˆ‘ä»¬å¯¹é›†æˆä¸­æ¯ç§æƒ…å¢ƒä¸‹æ¯ä¸ªä¸ªä½“ä»£ç†çš„ä¼˜åŠ¿å’ŒåŠ£åŠ¿è¿›è¡Œç»Ÿè®¡åˆ†æã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼ŒLLM-EnsåŠ¨æ€è¯†åˆ«ä»»åŠ¡æƒ…å¢ƒçš„å˜åŒ–ï¼Œå¹¶åˆ‡æ¢åˆ°å½“å‰æƒ…å¢ƒä¸‹è¡¨ç°æœ€ä½³çš„ä»£ç†ï¼Œç¡®ä¿åœ¨å˜åŒ–çš„ä»»åŠ¡æ¡ä»¶ä¸‹è¿›è¡ŒåŠ¨æ€æ¨¡å‹é€‰æ‹©ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ—¨åœ¨ä¸ç”¨ä¸åŒéšæœºç§å­ã€è¶…å‚æ•°è®¾ç½®å’Œå„ç§RLç®—æ³•è®­ç»ƒçš„ä»£ç†å…¼å®¹ã€‚åœ¨AtariåŸºå‡†æµ‹è¯•çš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒLLM-Ensæ˜¾è‘—æ”¹è¿›äº†RLæ¨¡å‹é›†æˆï¼Œè¶…è¶Šäº†çŸ¥ååŸºå‡†æµ‹è¯•è¾¾20.9%ã€‚ä¸ºäº†å¯é‡å¤æ€§ï¼Œæˆ‘ä»¬çš„ä»£ç å·²å¼€æºåœ¨<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/LLM4RLensemble-F7EE%E3%80%82">https://anonymous.4open.science/r/LLM4RLensemble-F7EEã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15306v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ ä¸­çš„æ¨¡å‹é›†æˆæ˜¯ä¸€ç§è®­ç»ƒæœ‰æ•ˆä»£ç†çš„æœ‰ç”¨æ–¹æ³•ã€‚è®­ç»ƒæœ‰æ•ˆçš„ä»£ç†å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå®ƒæ¶‰åŠåˆ°å¤šä¸ªå› ç´ çš„å¾®è°ƒï¼Œå¦‚ç®—æ³•é€‰æ‹©ã€è¶…å‚æ•°è®¾ç½®ç”šè‡³éšæœºç§å­é€‰æ‹©ã€‚æ¨¡å‹é›†æˆé€šè¿‡ç»“åˆå¤šä¸ªå¼±ä»£ç†ä¸ºä¸€ä¸ªå¼ºå¤§çš„å•ä¸€ä»£ç†æ¥å…‹æœè¿™ä¸€æŒ‘æˆ˜ï¼Œä»è€Œæé«˜æ•´ä½“æ€§èƒ½ã€‚ç„¶è€Œï¼Œç°æœ‰çš„é›†æˆæ–¹æ³•å¦‚å¤šæ•°æŠ•ç¥¨å’Œæ³¢å°”å…¹æ›¼åŠ æ³•è¢«è®¾è®¡ä¸ºå›ºå®šç­–ç•¥ï¼Œç¼ºä¹ç‰¹å®šä»»åŠ¡çš„è¯­ä¹‰ç†è§£ï¼Œé™åˆ¶äº†å…¶é€‚åº”æ€§å’Œæœ‰æ•ˆæ€§ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºLLM-Ensï¼Œä¸€ç§ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„ç‰¹å®šä»»åŠ¡è¯­ä¹‰ç†è§£çš„å¼ºåŒ–å­¦ä¹ æ¨¡å‹é›†æˆæ–°æ–¹æ³•ã€‚ç»™å®šä»»åŠ¡æ—¶ï¼Œæˆ‘ä»¬é¦–å…ˆè®¾è®¡LLMå°†ä»»åŠ¡çŠ¶æ€åˆ†ç±»ä¸ºä¸åŒçš„â€œæƒ…å¢ƒâ€ï¼Œå¹¶èå…¥ä»»åŠ¡æ¡ä»¶çš„é«˜çº§æè¿°ã€‚ç„¶åï¼Œæˆ‘ä»¬ç»Ÿè®¡åˆ†æäº†é›†æˆä¸­æ¯ä¸ªä¸ªä½“ä»£ç†åœ¨æ¯ä¸ªæƒ…å¢ƒä¸­çš„ä¼˜åŠ¿å’ŒåŠ£åŠ¿ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼ŒLLM-EnsåŠ¨æ€è¯†åˆ«ä»»åŠ¡æƒ…å¢ƒçš„å˜åŒ–ï¼Œå¹¶åˆ‡æ¢åˆ°å½“å‰æƒ…å¢ƒä¸‹è¡¨ç°æœ€ä½³çš„ä»£ç†ï¼Œç¡®ä¿åœ¨å˜åŒ–çš„ä»»åŠ¡æ¡ä»¶ä¸‹åŠ¨æ€é€‰æ‹©æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ä½¿ç”¨ä¸åŒéšæœºç§å­ã€è¶…å‚æ•°è®¾ç½®å’Œå„ç§RLç®—æ³•è®­ç»ƒçš„ä»£ç†å…¼å®¹ã€‚åœ¨AtariåŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒLLM-Ensæ˜¾è‘—æé«˜äº†å¼ºåŒ–å­¦ä¹ æ¨¡å‹é›†æˆæ•ˆæœï¼Œè¶…è¿‡çŸ¥ååŸºçº¿é«˜è¾¾20.9%ã€‚ç›¸å…³ä»£ç å·²å¼€æºäº<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/LLM4RLensemble-F7EE%E3%80%82">https://anonymous.4open.science/r/LLM4RLensemble-F7EEã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¨¡å‹é›†æˆåœ¨å¼ºåŒ–å­¦ä¹ ä¸­æ˜¯ä¸€ç§æœ‰æ•ˆçš„è®­ç»ƒä»£ç†æ–¹æ³•ï¼Œé€šè¿‡ç»“åˆå¤šä¸ªä»£ç†æ¥æé«˜æ•´ä½“æ€§èƒ½ã€‚</li>
<li>ç°æœ‰é›†æˆæ–¹æ³•ç¼ºä¹ç‰¹å®šä»»åŠ¡çš„è¯­ä¹‰ç†è§£ï¼Œé™åˆ¶äº†å…¶é€‚åº”æ€§å’Œæœ‰æ•ˆæ€§ã€‚</li>
<li>LLM-Ensæ–¹æ³•åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥é©±åŠ¨ä»»åŠ¡ç‰¹å®šè¯­ä¹‰ç†è§£ï¼Œå°†ä»»åŠ¡çŠ¶æ€åˆ†ç±»ä¸ºä¸åŒçš„æƒ…å¢ƒã€‚</li>
<li>LLM-Ensé€šè¿‡ç»Ÿè®¡åˆ†ææ¯ä¸ªä»£ç†åœ¨æ¯ç§æƒ…å¢ƒä¸­çš„è¡¨ç°æ¥ä¼˜åŒ–é›†æˆã€‚</li>
<li>åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼ŒLLM-Ensèƒ½åŠ¨æ€è¯†åˆ«ä»»åŠ¡æƒ…å¢ƒå˜åŒ–ï¼Œå¹¶é€‰æ‹©æœ€ä½³ä»£ç†åº”å¯¹ã€‚</li>
<li>LLM-Ensä¸ä¸åŒè®¾ç½®å’Œç®—æ³•è®­ç»ƒçš„ä»£ç†å…¼å®¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15306">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1008f4fb39bde997fdb1ca2ba8c8fbf3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9321e4ea17f6894a71b132c22e3ae6d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d92b972ecff087e709e5950622ed3a54.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Adaptive-Thinking-via-Mode-Policy-Optimization-for-Social-Language-Agents"><a href="#Adaptive-Thinking-via-Mode-Policy-Optimization-for-Social-Language-Agents" class="headerlink" title="Adaptive Thinking via Mode Policy Optimization for Social Language   Agents"></a>Adaptive Thinking via Mode Policy Optimization for Social Language   Agents</h2><p><strong>Authors:Minzheng Wang, Yongbin Li, Haobo Wang, Xinghua Zhang, Nan Xu, Bingli Wu, Fei Huang, Haiyang Yu, Wenji Mao</strong></p>
<p>Effective social intelligence simulation requires language agents to dynamically adjust reasoning depth, a capability notably absent in current studies. Existing methods either lack this kind of reasoning capability or enforce Long Chain-of-Thought reasoning uniformly across all scenarios, resulting in excessive token usage and inflexible social simulation. To address this, we propose an $\textbf{A}$daptive $\textbf{M}$ode $\textbf{L}$earning ($\textbf{AML}$) framework in this paper, aiming to improve the adaptive thinking ability of language agents in dynamic social interactions. To this end, we first identify hierarchical thinking modes ranging from intuitive response to deep deliberation based on the cognitive control theory. We then develop the $\textbf{A}$daptive $\textbf{M}$ode $\textbf{P}$olicy $\textbf{O}$ptimization ($\textbf{AMPO}$) algorithm to optimize the context-aware mode switching and reasoning. Our framework advances existing research in three key aspects: (1) Multi-granular thinking mode design, (2) Context-aware mode switching across social interaction, and (3) Token-efficient reasoning via depth-adaptive processing. Extensive experiments on social intelligence benchmarks verify that AML achieves 15.6% higher task performance than GPT-4o. Notably, our AMPO outperforms GRPO by 7.0% with 32.8% shorter reasoning chains, demonstrating the advantage of adaptive thinking mode selection and optimization mechanism in AMPO over GRPOâ€™s fixed-depth solution. </p>
<blockquote>
<p>æœ‰æ•ˆçš„ç¤¾ä¼šæ™ºèƒ½æ¨¡æ‹Ÿéœ€è¦è¯­è¨€ä»£ç†åŠ¨æ€è°ƒæ•´æ¨ç†æ·±åº¦ï¼Œè€Œå½“å‰çš„ç ”ç©¶ä¸­æ™®éç¼ºä¹è¿™ç§èƒ½åŠ›ã€‚ç°æœ‰çš„æ–¹æ³•è¦ä¹ˆä¸å…·å¤‡è¿™ç§æ¨ç†èƒ½åŠ›ï¼Œè¦ä¹ˆåœ¨æ‰€æœ‰åœºæ™¯ä¸­å¼ºåˆ¶å®æ–½é•¿é“¾æ€ç»´æ¨ç†ï¼Œå¯¼è‡´è¿‡åº¦ä½¿ç”¨æ ‡è®°ï¼ˆtokensï¼‰å’Œçµæ´»åº¦ä¸è¶³çš„ç¤¾ä¼šæ¨¡æ‹Ÿã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†è‡ªé€‚åº”æ¨¡å¼å­¦ä¹ ï¼ˆAMLï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜è¯­è¨€ä»£ç†åœ¨åŠ¨æ€ç¤¾ä¼šäº¤äº’ä¸­çš„è‡ªé€‚åº”æ€ç»´èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é¦–å…ˆåŸºäºè®¤çŸ¥æ§åˆ¶ç†è®ºï¼Œè¯†åˆ«äº†ä»ç›´è§‰ååº”åˆ°æ·±æ€ç†Ÿè™‘çš„ä¸åŒå±‚æ¬¡æ€ç»´æ¨¡å¼ã€‚ç„¶åï¼Œæˆ‘ä»¬å¼€å‘äº†è‡ªé€‚åº”æ¨¡å¼ç­–ç•¥ä¼˜åŒ–ï¼ˆAMPOï¼‰ç®—æ³•ï¼Œä»¥ä¼˜åŒ–åŸºäºä¸Šä¸‹æ–‡çš„æ¨¡å¼åˆ‡æ¢å’Œæ¨ç†ã€‚æˆ‘ä»¬çš„æ¡†æ¶åœ¨ä¸‰ä¸ªæ–¹é¢æ¨åŠ¨äº†ç°æœ‰ç ”ç©¶ï¼šï¼ˆ1ï¼‰å¤šç²’åº¦æ€ç»´æ¨¡å¼è®¾è®¡ï¼Œï¼ˆ2ï¼‰ç¤¾ä¼šäº¤äº’ä¸­çš„åŸºäºä¸Šä¸‹æ–‡æ¨¡å¼åˆ‡æ¢ï¼Œï¼ˆ3ï¼‰é€šè¿‡æ·±åº¦è‡ªé€‚åº”å¤„ç†å®ç°é«˜æ•ˆçš„æ ‡è®°æ¨ç†ã€‚åœ¨ç¤¾ä¼šæ™ºèƒ½åŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¯æ˜ï¼ŒAMLçš„ä»»åŠ¡æ€§èƒ½æ¯”GPT-4oé«˜å‡º15.6%ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„AMPOåœ¨æ€§èƒ½ä¸Šä¼˜äºGRPOï¼Œä»»åŠ¡å®Œæˆåº¦é«˜å‡º7.0%ï¼Œä¸”æ¨ç†é“¾ç¼©çŸ­32.8%ï¼Œè¿™æ˜¾ç¤ºäº†AMPOä¸­è‡ªé€‚åº”æ€ç»´æ¨¡å¼é€‰æ‹©å’Œä¼˜åŒ–æœºåˆ¶çš„ä¼˜åŠ¿ï¼Œè¶…è¶Šäº†GRPOçš„å›ºå®šæ·±åº¦è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.02156v4">PDF</a> Work in Progress. The code and data are available, see   <a target="_blank" rel="noopener" href="https://github.com/MozerWang/AMPO">https://github.com/MozerWang/AMPO</a></p>
<p><strong>Summary</strong><br>åŸºäºç°æœ‰ç ”ç©¶ä¸­è¯­è¨€ä»£ç†åœ¨åŠ¨æ€ç¤¾ä¼šäº¤äº’ä¸­ç¼ºä¹é€‚åº”æ€§æ€è€ƒèƒ½åŠ›çš„ç°çŠ¶ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºAMLçš„é€‚åº”æ¨¡å¼å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜è¯­è¨€ä»£ç†åœ¨åŠ¨æ€ç¤¾ä¼šäº¤äº’ä¸­çš„é€‚åº”æ€§æ€è€ƒèƒ½åŠ›ã€‚è¯¥æ¡†æ¶é€šè¿‡è®¾è®¡å¤šå±‚æ¬¡æ€è€ƒæ¨¡å¼ã€å®ç°è¯­å¢ƒæ„ŸçŸ¥æ¨¡å¼åˆ‡æ¢å’Œæ·±åº¦è‡ªé€‚åº”å¤„ç†ï¼Œä¼˜åŒ–äº†è¯­è¨€ä»£ç†çš„æ¨ç†è¿‡ç¨‹ã€‚å®éªŒè¯æ˜ï¼ŒAMLåœ¨ç¤¾äº¤æ™ºèƒ½åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†ä¼˜äºGPT-4oçš„ä»»åŠ¡æ€§èƒ½è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è¯­è¨€ä»£ç†åœ¨åŠ¨æ€ç¤¾ä¼šäº¤äº’ä¸­éœ€è¦åŠ¨æ€è°ƒæ•´æ¨ç†æ·±åº¦ã€‚</li>
<li>ç°æœ‰ç ”ç©¶ç¼ºä¹é€‚åº”æ€§æ€è€ƒèƒ½åŠ›æˆ–ç»Ÿä¸€é‡‡ç”¨é•¿é“¾æ€ç»´ï¼Œå¯¼è‡´è¿‡åº¦ä½¿ç”¨æ ‡è®°å’Œä¸çµæ´»çš„ç¤¾ä¼šæ¨¡æ‹Ÿã€‚</li>
<li>AMLæ¡†æ¶æ—¨åœ¨æé«˜è¯­è¨€ä»£ç†åœ¨åŠ¨æ€ç¤¾ä¼šäº¤äº’ä¸­çš„é€‚åº”æ€§æ€è€ƒèƒ½åŠ›ã€‚</li>
<li>AMLæ¡†æ¶åŒ…æ‹¬è®¾è®¡å¤šå±‚æ¬¡æ€è€ƒæ¨¡å¼ã€å®ç°è¯­å¢ƒæ„ŸçŸ¥æ¨¡å¼åˆ‡æ¢å’Œä¼˜åŒ–æ·±åº¦è‡ªé€‚åº”å¤„ç†ã€‚</li>
<li>AMPOç®—æ³•ç”¨äºä¼˜åŒ–è¯­å¢ƒæ„ŸçŸ¥æ¨¡å¼åˆ‡æ¢å’Œæ¨ç†è¿‡ç¨‹ã€‚</li>
<li>å®éªŒè¯æ˜AMLåœ¨ç¤¾äº¤æ™ºèƒ½åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºGPT-4oå’ŒGRPOã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.02156">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-077873a343d7d07be868e8e57fda7fe5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-291ddb362bcda3f5a0abf41c6a238b40.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca1dc801b044cbe3ad0e1ecdb6ed5d81.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0977122f20978879832f4d7f7136cd47.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Agentic-AI-Software-Engineers-Programming-with-Trust"><a href="#Agentic-AI-Software-Engineers-Programming-with-Trust" class="headerlink" title="Agentic AI Software Engineers: Programming with Trust"></a>Agentic AI Software Engineers: Programming with Trust</h2><p><strong>Authors:Abhik Roychoudhury, Corina Pasareanu, Michael Pradel, Baishakhi Ray</strong></p>
<p>Large Language Models (LLMs) have shown surprising proficiency in generating code snippets, promising to automate large parts of software engineering via artificial intelligence (AI). We argue that successfully deploying AI software engineers requires a level of trust equal to or even greater than the trust established by human-driven software engineering practices. The recent trend toward LLM agents offers a path toward integrating the power of LLMs to create new code with the power of analysis tools to increase trust in the code. This opinion piece comments on whether LLM agents could dominate software engineering workflows in the future and whether the focus of programming will shift from programming at scale to programming with trust. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç”Ÿæˆä»£ç ç‰‡æ®µæ–¹é¢è¡¨ç°å‡ºäº†æƒŠäººçš„ç†Ÿç»ƒç¨‹åº¦ï¼Œæœ‰æœ›é€šè¿‡äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰è‡ªåŠ¨åŒ–è½¯ä»¶å·¥ç¨‹çš„å¾ˆå¤§ä¸€éƒ¨åˆ†å·¥ä½œã€‚æˆ‘ä»¬ä¸»å¼ ï¼ŒæˆåŠŸéƒ¨ç½²AIè½¯ä»¶å·¥ç¨‹å¸ˆéœ€è¦å»ºç«‹ä¸äººç±»é©±åŠ¨çš„è½¯ä»¶å·¥ç¨‹å®è·µç›¸å½“çš„ç”šè‡³æ›´é«˜çš„ä¿¡ä»»åº¦ã€‚æœ€è¿‘å¯¹LLMä»£ç†çš„è¶‹åŠ¿æä¾›äº†ä¸€ä¸ªå°†LLMsçš„åŠ›é‡ä¸åˆ†æå·¥å…·çš„åŠ›é‡ç›¸ç»“åˆæ¥åˆ›å»ºæ–°ä»£ç çš„è·¯å¾„ï¼Œä»¥å¢åŠ å¯¹ä»£ç çš„ä¿¡ä»»ã€‚æœ¬æ–‡ä½œè€…è®¤ä¸ºï¼Œæœªæ¥çš„è½¯ä»¶å·¥ä½œæµç¨‹ä¸­LLMä»£ç†æ˜¯å¦ä¼šå æ®ä¸»å¯¼åœ°ä½ï¼Œç¼–ç¨‹çš„é‡ç‚¹æ˜¯å¦ä¼šä»è§„æ¨¡åŒ–ç¼–ç¨‹è½¬å‘å¯ä¿¡ç¼–ç¨‹ï¼Œéƒ½æ˜¯å€¼å¾—æ¢è®¨çš„é—®é¢˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13767v3">PDF</a> 5 pages</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç”Ÿæˆä»£ç ç‰‡æ®µæ–¹é¢è¡¨ç°å‡ºæƒŠäººçš„èƒ½åŠ›ï¼Œé¢„ç¤ºç€äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰å¯èƒ½è‡ªåŠ¨åŒ–è½¯ä»¶å·¥ç¨‹çš„å¾ˆå¤§ä¸€éƒ¨åˆ†å·¥ä½œã€‚æˆåŠŸéƒ¨ç½²AIè½¯ä»¶å·¥ç¨‹å¸ˆéœ€è¦å»ºç«‹ä¸äººç±»é©±åŠ¨çš„è½¯ä»¶å·¥ç¨‹å®è·µç›¸å½“çš„ä¿¡ä»»æ°´å¹³ï¼Œç”šè‡³æ›´é«˜ã€‚æœ€è¿‘å‡ºç°çš„LLMä»£ç†è¶‹åŠ¿ä¸ºæ•´åˆLLMsç”Ÿæˆæ–°ä»£ç çš„èƒ½åŠ›ä¸åˆ†æå·¥å…·å¢åŠ ä»£ç ä¿¡ä»»çš„èƒ½åŠ›æä¾›äº†é€”å¾„ã€‚æœ¬è¯„è®ºæ–‡ç« æ¢è®¨äº†LLMä»£ç†æ˜¯å¦å¯èƒ½åœ¨æœªæ¥ä¸»å¯¼è½¯ä»¶å·¥ç¨‹å·¥ä½œæµç¨‹ï¼Œä»¥åŠç¼–ç¨‹çš„ç„¦ç‚¹æ˜¯å¦ä¼šä»å¤§è§„æ¨¡ç¼–ç¨‹è½¬å‘å…·æœ‰ä¿¡ä»»çš„ç¼–ç¨‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½ç”Ÿæˆä»£ç ç‰‡æ®µï¼Œé¢„ç¤ºAIåœ¨è½¯ä»¶å·¥ç¨‹ä¸­çš„æ½œåœ¨ä½œç”¨ã€‚</li>
<li>æˆåŠŸéƒ¨ç½²AIè½¯ä»¶å·¥ç¨‹å¸ˆéœ€è¦å»ºç«‹é«˜ä¿¡ä»»åº¦ï¼Œä¸äººç±»é©±åŠ¨çš„è½¯ä»¶å·¥ç¨‹å®è·µç›¸å½“ã€‚</li>
<li>LLMä»£ç†çš„å‡ºç°ä¸ºæ•´åˆLLMsçš„èƒ½åŠ›å’Œåˆ†æå·¥å…·å¢åŠ äº†å¯¹ä»£ç çš„ä¿¡å¿ƒã€‚</li>
<li>LLMä»£ç†å¯èƒ½åœ¨æœªæ¥ä¸»å¯¼è½¯ä»¶å·¥ç¨‹å·¥ä½œæµç¨‹ã€‚</li>
<li>ç¼–ç¨‹çš„ç„¦ç‚¹å¯èƒ½ä¼šä»å¤§è§„æ¨¡ç¼–ç¨‹è½¬å‘å…·æœ‰ä¿¡ä»»çš„ç¼–ç¨‹ã€‚</li>
<li>éœ€è¦è¿›ä¸€æ­¥ç ”ç©¶å’Œæ¢ç´¢å¦‚ä½•åœ¨è½¯ä»¶å·¥ç¨‹ä¸­å¹³è¡¡äººå·¥æ™ºèƒ½ä¸äººç±»è§’è‰²çš„åä½œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13767">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9e2f6486df327aa2ed0c9f02762b960e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c26067435299e2aa6d438ebe39d58f94.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="InSTA-Towards-Internet-Scale-Training-For-Agents"><a href="#InSTA-Towards-Internet-Scale-Training-For-Agents" class="headerlink" title="InSTA: Towards Internet-Scale Training For Agents"></a>InSTA: Towards Internet-Scale Training For Agents</h2><p><strong>Authors:Brandon Trabucco, Gunnar Sigurdsson, Robinson Piramuthu, Ruslan Salakhutdinov</strong></p>
<p>The predominant approach for training web navigation agents is to gather human demonstrations for a set of popular websites and hand-written tasks, but it is becoming clear that human data is an inefficient resource. We develop a pipeline to facilitate internet-scale training for agents without laborious human annotations. In the first stage, an LLM annotates 150k sites with agentic tasks. In the next stage, LLM agents complete tasks and produce trajectories. In the final stage, an LLM filters trajectories by judging their success. Language models are powerful data curation tools, identifying harmful content with an accuracy of 97%, judging successful trajectories with an accuracy of 82.6%, and producing effective data. We train agents based on Qwen 3 1.7B that are competitive with frontier LLMs as web agents, while being smaller and faster. Our top agent reaches a success rate of 56.9%, outperforming the data collection policy Qwen 3 235B, a 235 times larger Llama 4 Maverick, and reaching 94.7% of the performance of Gemini 2.5 Flash. We are releasing code, models and data at: <a target="_blank" rel="noopener" href="https://data-for-agents.github.io/">https://data-for-agents.github.io</a>. </p>
<blockquote>
<p>è®­ç»ƒç½‘ç»œå¯¼èˆªä»£ç†çš„ä¸»æµæ–¹æ³•æ˜¯ä¸ºæµè¡Œç½‘ç«™å’Œæ‰‹å†™ä»»åŠ¡æ”¶é›†äººç±»æ¼”ç¤ºï¼Œä½†è¶Šæ¥è¶Šæ˜æ˜¾çš„æ˜¯ï¼Œäººç±»æ•°æ®æ˜¯ä¸€ç§ä½æ•ˆçš„èµ„æºã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªæµç¨‹ï¼Œä»¥åœ¨æ²¡æœ‰ç¹ççš„äººç±»æ³¨é‡Šçš„æƒ…å†µä¸‹ä¿ƒè¿›ä»£ç†çš„äº’è”ç½‘è§„æ¨¡è®­ç»ƒã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸º15ä¸‡ä¸ªç½‘ç«™æ³¨é‡Šäº†ä»»åŠ¡ã€‚åœ¨ä¸‹ä¸€é˜¶æ®µï¼ŒLLMä»£ç†å®Œæˆä»»åŠ¡å¹¶äº§ç”Ÿè½¨è¿¹ã€‚åœ¨æœ€åä¸€ä¸ªé˜¶æ®µï¼ŒLLMé€šè¿‡åˆ¤æ–­å…¶æˆåŠŸä¸å¦æ¥è¿‡æ»¤è½¨è¿¹ã€‚è¯­è¨€æ¨¡å‹æ˜¯å¼ºå¤§çš„æ•°æ®æ•´ç†å·¥å…·ï¼Œå®ƒä»¬èƒ½å‡†ç¡®åœ°è¯†åˆ«æœ‰å®³å†…å®¹ï¼ˆå‡†ç¡®ç‡ä¸º97%ï¼‰ï¼Œå‡†ç¡®åœ°åˆ¤æ–­æˆåŠŸçš„è½¨è¿¹ï¼ˆå‡†ç¡®ç‡ä¸º82.6%ï¼‰ï¼Œå¹¶èƒ½äº§ç”Ÿæœ‰æ•ˆçš„æ•°æ®ã€‚æˆ‘ä»¬ä»¥Qwen 3 1.7Bä¸ºåŸºç¡€è®­ç»ƒä»£ç†ï¼Œä½œä¸ºç½‘ç»œä»£ç†ï¼Œæˆ‘ä»¬çš„ä»£ç†ä¸å‰æ²¿LLMå…·æœ‰ç«äº‰åŠ›ï¼Œè€Œä¸”æ›´å°ã€æ›´å¿«ã€‚æˆ‘ä»¬çš„é¡¶çº§ä»£ç†æˆåŠŸç‡è¾¾åˆ°56.9%ï¼Œè¶…è¿‡äº†æ•°æ®é‡‡é›†ç­–ç•¥Qwen 3 235Bï¼Œä¸€ä¸ªä½“ç§¯å¤§å¾—å¤šçš„Llama 4 Maverickï¼Œå¹¶è¾¾åˆ°äº†Gemini 2.5 Flashæ€§èƒ½çš„94.7%ã€‚æˆ‘ä»¬åœ¨ä»¥ä¸‹ç½‘å€å‘å¸ƒä»£ç ã€æ¨¡å‹å’Œæ•°ï¼š<a target="_blank" rel="noopener" href="https://data-for-agents.github.io./">https://data-for-agents.github.ioã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.06776v2">PDF</a> Improved results, zero-shot transfer to Web Voyager</p>
<p><strong>Summary</strong></p>
<p>åŸºäºLLMçš„ç®¡é“è®­ç»ƒç½‘ç»œå¯¼èˆªä»£ç†ï¼Œæ— éœ€ç¹ççš„äººå·¥æ³¨é‡Šã€‚é€šè¿‡LLMæ ‡æ³¨ç½‘ç«™ã€å®Œæˆä»£ç†ä»»åŠ¡å’Œè¿‡æ»¤è½¨è¿¹ï¼ŒæˆåŠŸè®­ç»ƒå‡ºç«äº‰å‰æ²¿çš„å°å‹å¿«é€Ÿç½‘ç»œå¯¼èˆªä»£ç†ã€‚è¯¥ä»£ç†æ€§èƒ½ä¼˜å¼‚ï¼ŒæˆåŠŸç‡é«˜ï¼Œä¸”å¯è¯†åˆ«æœ‰å®³å†…å®¹ã€‚ç›¸å…³ä»£ç ã€æ¨¡å‹å’Œæ•°æ®å·²å‘å¸ƒåœ¨ç›¸å…³ç½‘ç«™ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä½¿ç”¨LLMï¼ˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼‰æ„å»ºç®¡é“è®­ç»ƒç½‘ç»œå¯¼èˆªä»£ç†ï¼Œæé«˜äº†æ•ˆç‡å¹¶é™ä½äº†äººå·¥å‚ä¸æˆæœ¬ã€‚</li>
<li>LLMç”¨äºæ ‡æ³¨ç½‘ç«™ã€å®Œæˆä»£ç†ä»»åŠ¡å’Œè¿‡æ»¤è½¨è¿¹ï¼Œç®€åŒ–äº†è®­ç»ƒè¿‡ç¨‹ã€‚</li>
<li>æˆåŠŸè®­ç»ƒå‡ºæ€§èƒ½ä¼˜å¼‚ã€æˆåŠŸç‡é«˜çš„ç½‘ç»œå¯¼èˆªä»£ç†ï¼Œè¾¾åˆ°äº†å‰æ²¿æ°´å¹³ã€‚</li>
<li>LLMèƒ½å‡†ç¡®è¯†åˆ«æœ‰å®³å†…å®¹ï¼Œå±•ç°äº†å…¶åœ¨æ•°æ®ç­›é€‰æ–¹é¢çš„å¼ºå¤§èƒ½åŠ›ã€‚</li>
<li>è®­ç»ƒå‡ºçš„ä»£ç†åŸºäºè¾ƒå°çš„æ¨¡å‹ï¼ˆQwen 3 1.7Bï¼‰ï¼Œç›¸è¾ƒäºå¤§å‹æ¨¡å‹ï¼ˆå¦‚Qwen 3 235Bå’ŒLlama 4 Maverickï¼‰æ›´ä¸ºé«˜æ•ˆã€‚</li>
<li>è®­ç»ƒå‡ºçš„æœ€ä½³ä»£ç†æ€§èƒ½è¾¾åˆ°äº†é«˜æˆåŠŸç‡ï¼ˆ56.9%ï¼‰ï¼Œå¹¶æ¥è¿‘Gemini 2.5 Flashçš„æ€§èƒ½ï¼ˆè¾¾åˆ°å…¶94.7%ï¼‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.06776">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4f7d6b29e2f69812ea760cf2b2d90c49.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-08a8e1b475f9e0ea5b03c8b288beaf2b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8d1a2fb0609b56d6343a67ffccdd2f31.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-56d7b200d950159537a71c7ef454765e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ad08cdde230176a3106b4608e1facfa.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b9e41d2e928ec2e8f27ddec0fd22c46b.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="My-Words-Imply-Your-Opinion-Reader-Agent-based-Propagation-Enhancement-for-Personalized-Implicit-Emotion-Analysis"><a href="#My-Words-Imply-Your-Opinion-Reader-Agent-based-Propagation-Enhancement-for-Personalized-Implicit-Emotion-Analysis" class="headerlink" title="My Words Imply Your Opinion: Reader Agent-based Propagation Enhancement   for Personalized Implicit Emotion Analysis"></a>My Words Imply Your Opinion: Reader Agent-based Propagation Enhancement   for Personalized Implicit Emotion Analysis</h2><p><strong>Authors:Jian Liao, Yu Feng, Yujin Zheng, Jun Zhao, Suge Wang, Jianxing Zheng</strong></p>
<p>The subtlety of emotional expressions makes implicit emotion analysis (IEA) particularly sensitive to user-specific characteristics. Current studies personalize emotion analysis by focusing on the author but neglect the impact of the intended reader on implicit emotional feedback. In this paper, we introduce Personalized IEA (PIEA) and present the RAPPIE model, which addresses subjective variability by incorporating reader feedback. In particular, (1) we create reader agents based on large language models to simulate reader feedback, overcoming the issue of &#96;&#96;spiral of silence effectâ€™â€™ and data incompleteness of real reader reaction. (2) We develop a role-aware multi-view graph learning to model the emotion interactive propagation process in scenarios with sparse reader information. (3) We construct two new PIEA datasets covering English and Chinese social media with detailed user metadata, addressing the text-centric limitation of existing datasets. Extensive experiments show that RAPPIE significantly outperforms state-of-the-art baselines, demonstrating the value of incorporating reader feedback in PIEA. </p>
<blockquote>
<p>æƒ…æ„Ÿè¡¨è¾¾çš„ç»†å¾®ä¹‹å¤„ä½¿å¾—éšå¼æƒ…æ„Ÿåˆ†æï¼ˆIEAï¼‰ç‰¹åˆ«å…³æ³¨ç”¨æˆ·ç‰¹å®šç‰¹å¾ã€‚å½“å‰çš„ç ”ç©¶é€šè¿‡å…³æ³¨ä½œè€…æ¥å®ç°æƒ…æ„Ÿåˆ†æçš„ä¸ªæ€§åŒ–ï¼Œä½†å¿½è§†äº†ç›®æ ‡è¯»è€…å¯¹éšå¼æƒ…æ„Ÿåé¦ˆçš„å½±å“ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸ªæ€§åŒ–IEAï¼ˆPIEAï¼‰å¹¶æå‡ºäº†RAPPIEæ¨¡å‹ï¼Œè¯¥æ¨¡å‹é€šè¿‡èå…¥è¯»è€…åé¦ˆæ¥è§£å†³ä¸»è§‚å˜åŒ–æ€§çš„é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œï¼ˆ1ï¼‰æˆ‘ä»¬åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹åˆ›å»ºè¯»è€…ä»£ç†æ¥æ¨¡æ‹Ÿè¯»è€…åé¦ˆï¼Œä»è€Œå…‹æœâ€œæ²‰é»˜èºæ—‹æ•ˆåº”â€å’ŒçœŸå®è¯»è€…ååº”æ•°æ®ä¸å®Œæ•´çš„é—®é¢˜ã€‚ï¼ˆ2ï¼‰æˆ‘ä»¬å¼€å‘äº†ä¸€ç§è§’è‰²æ„ŸçŸ¥å¤šè§†å›¾å›¾å­¦ä¹ ï¼Œä»¥åœ¨è¯»è€…ä¿¡æ¯ç¨€ç–çš„åœºæ™¯ä¸­å»ºç«‹æƒ…æ„Ÿäº¤äº’ä¼ æ’­è¿‡ç¨‹ã€‚ ï¼ˆ3ï¼‰æˆ‘ä»¬æ„å»ºäº†ä¸¤ä¸ªæ–°çš„PIEAæ•°æ®é›†ï¼Œæ¶µç›–äº†è‹±è¯­å’Œä¸­æ–‡ç¤¾äº¤åª’ä½“ï¼Œå¹¶åŒ…å«è¯¦ç»†çš„ç”¨æˆ·å…ƒæ•°æ®ï¼Œè§£å†³äº†ç°æœ‰æ•°æ®é›†ä»¥æ–‡æœ¬ä¸ºä¸­å¿ƒçš„å±€é™æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒRAPPIEåœ¨èå…¥è¯»è€…åé¦ˆåæ˜¾è‘—è¶…è¶Šäº†æœ€æ–°åŸºçº¿ï¼Œè¡¨ç°å‡ºå…¶ä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07367v3">PDF</a> </p>
<p><strong>Summary</strong><br>æ–‡æœ¬æå‡ºä¸ªæ€§åŒ–éšå¼æƒ…æ„Ÿåˆ†æï¼ˆPIEAï¼‰çš„æ–°æ¦‚å¿µï¼Œå¹¶ä»‹ç»RAPPIEæ¨¡å‹ã€‚è¯¥æ¨¡å‹é€šè¿‡æ¨¡æ‹Ÿè¯»è€…åé¦ˆå…‹æœçœŸå®è¯»è€…ååº”çš„â€œæ²‰é»˜èºæ—‹æ•ˆåº”â€å’Œæ•°æ®ä¸å®Œæ•´æ€§é—®é¢˜ï¼Œå¹¶æ„å»ºè¯»è€…ä»£ç†æ¥æ•æ‰è¯»è€…ä¿¡æ¯å¯¹æƒ…ç»ªè¡¨è¾¾çš„å½±å“ã€‚é€šè¿‡è§’è‰²æ„ŸçŸ¥çš„å¤šè§†å›¾å›¾å­¦ä¹ ï¼Œå»ºæ¨¡æƒ…æ„Ÿäº¤äº’ä¼ æ’­è¿‡ç¨‹ã€‚åŒæ—¶ï¼Œåˆ›å»ºè¦†ç›–è‹±æ–‡å’Œä¸­æ–‡ç¤¾äº¤åª’ä½“çš„æ–°æ•°æ®é›†ï¼ŒåŒ…å«è¯¦ç»†çš„ç”¨æˆ·å…ƒæ•°æ®ã€‚å®éªŒè¯æ˜RAPPIEæ¨¡å‹æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿æ¨¡å‹ï¼Œè¯æ˜äº†åœ¨PIEAä¸­èå…¥è¯»è€…åé¦ˆçš„ä»·å€¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬å¼ºè°ƒæƒ…æ„Ÿè¡¨è¾¾çš„å¾®å¦™æ€§ä½¿å¾—éšå¼æƒ…æ„Ÿåˆ†æï¼ˆIEAï¼‰å¯¹ç”¨æˆ·ç‰¹å®šç‰¹æ€§ç‰¹åˆ«æ•æ„Ÿã€‚</li>
<li>å½“å‰ç ”ç©¶ä¸ªæ€§åŒ–æƒ…æ„Ÿåˆ†æä¸»è¦é›†ä¸­åœ¨ä½œè€…ä¸Šï¼Œä½†å¿½ç•¥äº†é¢„æœŸè¯»è€…å¯¹éšå¼æƒ…æ„Ÿåé¦ˆçš„å½±å“ã€‚</li>
<li>å¼•å…¥ä¸ªæ€§åŒ–éšå¼æƒ…æ„Ÿåˆ†æï¼ˆPIEAï¼‰å’ŒRAPPIEæ¨¡å‹ï¼Œé€šè¿‡æ¨¡æ‹Ÿè¯»è€…åé¦ˆè§£å†³ä¸»è§‚å˜å¼‚æ€§é—®é¢˜ã€‚</li>
<li>åˆ›å»ºåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„è¯»è€…ä»£ç†æ¥æ¨¡æ‹Ÿè¯»è€…åé¦ˆï¼Œå…‹æœçœŸå®è¯»è€…ååº”çš„â€œæ²‰é»˜èºæ—‹æ•ˆåº”â€å’Œæ•°æ®ä¸å®Œæ•´æ€§é—®é¢˜ã€‚</li>
<li>å‘å±•è§’è‰²æ„ŸçŸ¥çš„å¤šè§†å›¾å›¾å­¦ä¹ ï¼Œå»ºæ¨¡æƒ…æ„Ÿäº¤äº’ä¼ æ’­è¿‡ç¨‹ï¼Œä»¥å¤„ç†ç¨€ç–çš„è¯»è€…ä¿¡æ¯åœºæ™¯ã€‚</li>
<li>æ„å»ºæ¶µç›–è‹±æ–‡å’Œä¸­æ–‡ç¤¾äº¤åª’ä½“çš„æ–°PIEAæ•°æ®é›†ï¼ŒåŒ…å«è¯¦ç»†çš„ç”¨æˆ·å…ƒæ•°æ®ï¼Œè§£å†³ç°æœ‰æ•°æ®é›†æ–‡æœ¬ä¸ºä¸­å¿ƒçš„é™åˆ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.07367">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-c0fd07cdd7245cfc67c17c2af7aa26fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-109b6945ce470aeaf0ad1fd771202255.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="HyperMARL-Adaptive-Hypernetworks-for-Multi-Agent-RL"><a href="#HyperMARL-Adaptive-Hypernetworks-for-Multi-Agent-RL" class="headerlink" title="HyperMARL: Adaptive Hypernetworks for Multi-Agent RL"></a>HyperMARL: Adaptive Hypernetworks for Multi-Agent RL</h2><p><strong>Authors:Kale-ab Abebe Tessera, Arrasy Rahman, Amos Storkey, Stefano V. Albrecht</strong></p>
<p>Adaptability to specialised or homogeneous behaviours is critical in cooperative multi-agent reinforcement learning (MARL). Parameter sharing (PS) techniques, common for efficient adaptation, often limit behavioural diversity due to cross-agent gradient interference, which we show can be exacerbated by the coupling of observations and agent IDs. Current remedies typically add complexity through altered objectives, manual preset diversity levels, or sequential updates. We ask: can shared policies adapt without these complexities? We propose HyperMARL, a PS approach using hypernetworks for dynamic agent-specific parameters, without altering the RL objective or requiring preset diversity levels. HyperMARLâ€™s explicit decoupling of observation- and agent-conditioned gradients empirically reduces policy gradient variance, facilitates shared-policy adaptation (including specialisation), and helps mitigate cross-agent interference. Across diverse MARL benchmarks (up to 20 agents), requiring homogeneous, heterogeneous, or mixed behaviours, HyperMARL achieves competitive performance against key baselines â€“ fully shared, non-parameter sharing, and three diversity-promoting methods â€“ while preserving behavioural diversity comparable to non-parameter sharing. These findings establish HyperMARL as a versatile approach for adaptive MARL. The code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/KaleabTessera/HyperMARL">https://github.com/KaleabTessera/HyperMARL</a>. </p>
<blockquote>
<p>é€‚åº”ç‰¹æ®Šæˆ–åŒè´¨è¡Œä¸ºåœ¨åˆä½œå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰ä¸­è‡³å…³é‡è¦ã€‚å‚æ•°å…±äº«ï¼ˆPSï¼‰æŠ€æœ¯ï¼Œå¸¸ç”¨äºé«˜æ•ˆé€‚åº”ï¼Œä½†ç”±äºè·¨æ™ºèƒ½ä½“æ¢¯åº¦å¹²æ‰°ï¼Œé€šå¸¸ä¼šé™åˆ¶è¡Œä¸ºå¤šæ ·æ€§ï¼Œæˆ‘ä»¬è¡¨æ˜è§‚å¯Ÿå’Œæ™ºèƒ½ä½“IDçš„è€¦åˆä¼šåŠ å‰§è¿™ä¸€é—®é¢˜ã€‚å½“å‰çš„è¡¥æ•‘æªæ–½é€šå¸¸é€šè¿‡æ”¹å˜ç›®æ ‡ã€æ‰‹åŠ¨é¢„è®¾å¤šæ ·æ€§æ°´å¹³æˆ–é¡ºåºæ›´æ–°æ¥å¢åŠ å¤æ‚æ€§ã€‚æˆ‘ä»¬çš„é—®é¢˜æ˜¯ï¼šå…±äº«æ”¿ç­–èƒ½å¦åœ¨æ²¡æœ‰è¿™äº›å¤æ‚æ€§çš„æƒ…å†µä¸‹é€‚åº”ï¼Ÿæˆ‘ä»¬æå‡ºäº†HyperMARLï¼Œè¿™æ˜¯ä¸€ç§ä½¿ç”¨è¶…ç½‘ç»œè¿›è¡ŒåŠ¨æ€ç‰¹å®šäºæ™ºèƒ½ä½“çš„å‚æ•°å…±äº«æ–¹æ³•ï¼Œæ— éœ€æ”¹å˜RLç›®æ ‡æˆ–è¦æ±‚é¢„è®¾å¤šæ ·æ€§æ°´å¹³ã€‚HyperMARLå¯¹è§‚å¯Ÿå’Œæ™ºèƒ½ä½“æ¡ä»¶ä¸‹çš„æ¢¯åº¦è¿›è¡Œæ˜¾å¼è§£è€¦ï¼Œè¿™å®é™…ä¸Šå‡å°‘äº†æ”¿ç­–æ¢¯åº¦æ–¹å·®ï¼Œä¿ƒè¿›äº†å…±äº«æ”¿ç­–çš„é€‚åº”ï¼ˆåŒ…æ‹¬ä¸“ä¸šåŒ–ï¼‰ï¼Œå¹¶æœ‰åŠ©äºç¼“è§£è·¨æ™ºèƒ½ä½“å¹²æ‰°ã€‚åœ¨å¤šç§MARLåŸºå‡†æµ‹è¯•ï¼ˆæœ€å¤š20ä¸ªæ™ºèƒ½ä½“ï¼‰ä¸­ï¼Œéœ€è¦åŒè´¨ã€å¼‚è´¨æˆ–æ··åˆè¡Œä¸ºï¼ŒHyperMARLåœ¨å…³é”®åŸºå‡†çº¿ä¸Šå®ç°äº†ç«äº‰æ€§èƒ½â€”â€”å®Œå…¨å…±äº«ã€éå‚æ•°å…±äº«å’Œä¸‰ç§ä¿ƒè¿›å¤šæ ·æ€§çš„æ–¹æ³•ï¼ŒåŒæ—¶ä¿æŒä¸éå‚æ•°å…±äº«ç›¸å½“çš„è¡Œä¸ºå¤šæ ·æ€§ã€‚è¿™äº›å‘ç°è¯æ˜HyperMARLæ˜¯é€‚åº”æ€§MARLçš„é€šç”¨æ–¹æ³•ã€‚ä»£ç å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/KaleabTessera/HyperMARL">https://github.com/KaleabTessera/HyperMARL</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04233v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>é€‚åº”ç‰¹å®šæˆ–åŒè´¨åŒ–è¡Œä¸ºåœ¨åˆä½œå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰ä¸­è‡³å…³é‡è¦ã€‚å…±äº«å‚æ•°ï¼ˆPSï¼‰æŠ€æœ¯å› å¯¹è‡ªé€‚åº”çš„é€‚åº”æ•ˆç‡è€Œæ™®éåº”ç”¨ï¼Œä½†å¾€å¾€é™åˆ¶äº†è¡Œä¸ºå¤šæ ·æ€§ï¼Œå› ä¸ºå­˜åœ¨è·¨æ™ºèƒ½ä½“æ¢¯åº¦å¹²æ‰°é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºHyperMARLçš„æ–°æ–¹æ³•ï¼Œä½¿ç”¨è¶…ç½‘ç»œè¿›è¡ŒåŠ¨æ€æ™ºèƒ½ä½“ç‰¹å®šå‚æ•°åŒ–ï¼Œæ— éœ€æ”¹å˜å¼ºåŒ–å­¦ä¹ ç›®æ ‡æˆ–é¢„è®¾å¤šæ ·æ€§çº§åˆ«ã€‚HyperMARLèƒ½å¤Ÿé™ä½ç­–ç•¥æ¢¯åº¦æ–¹å·®ã€é€‚åº”å…±äº«ç­–ç•¥ã€å‡å°‘è·¨æ™ºèƒ½ä½“å¹²æ‰°ç­‰ã€‚åœ¨å¤šç§MARLåŸºå‡†æµ‹è¯•ä¸Šï¼ŒHyperMARLå®ç°äº†ä¸å…³é”®åŸºçº¿ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒä¸æ— å‚æ•°å…±äº«ç›¸è¿‘çš„è¡Œä¸ºå¤šæ ·æ€§ã€‚è¿™è¡¨æ˜HyperMARLæ˜¯é€‚åº”æ€§MARLçš„ä¸€ç§é€šç”¨æ–¹æ³•ã€‚ä»£ç å…¬å¼€äºï¼š<a target="_blank" rel="noopener" href="https://github.com/KaleabTessera/HyperMARL%E3%80%82">https://github.com/KaleabTessera/HyperMARLã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é€‚åº”ç‰¹å®šæˆ–åŒè´¨åŒ–è¡Œä¸ºåœ¨å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ä¸­çš„é‡è¦æ€§ã€‚</li>
<li>å‚æ•°å…±äº«æŠ€æœ¯åœ¨å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ä¸­å¯èƒ½é™åˆ¶è¡Œä¸ºå¤šæ ·æ€§ï¼Œå¯¼è‡´è·¨æ™ºèƒ½ä½“æ¢¯åº¦å¹²æ‰°é—®é¢˜ã€‚</li>
<li>æå‡ºHyperMARLæ–¹æ³•ï¼Œä½¿ç”¨è¶…ç½‘ç»œè¿›è¡ŒåŠ¨æ€æ™ºèƒ½ä½“ç‰¹å®šå‚æ•°åŒ–ï¼Œæ— éœ€æ”¹å˜å¼ºåŒ–å­¦ä¹ ç›®æ ‡æˆ–é¢„è®¾å¤šæ ·æ€§çº§åˆ«ã€‚</li>
<li>HyperMARLèƒ½å¤Ÿé™ä½ç­–ç•¥æ¢¯åº¦æ–¹å·®ï¼Œä¿ƒè¿›å…±äº«ç­–ç•¥é€‚åº”ï¼Œå¹¶æœ‰åŠ©äºå‡å°‘è·¨æ™ºèƒ½ä½“å¹²æ‰°ã€‚</li>
<li>åœ¨å¤šç§MARLåŸºå‡†æµ‹è¯•ä¸­ï¼ŒHyperMARLè¡¨ç°å‡ºä¸å…³é”®åŸºçº¿ç›¸æ¯”çš„ç«äº‰åŠ›ï¼ŒåŒæ—¶ä¿æŒè¡Œä¸ºå¤šæ ·æ€§ã€‚</li>
<li>HyperMARLåœ¨ä¿æŒè¡Œä¸ºå¤šæ ·æ€§çš„åŒæ—¶ï¼Œå®ç°äº†è¾ƒé«˜çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶ä½œä¸ºé€‚åº”æ€§MARLé€šç”¨æ–¹æ³•çš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.04233">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8af7f6ca60b858bb92cba3dc14b5ff64.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b07aa129879ebdb8f07347ff23a1d768.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e1096ac781d0c00cd0fe40e1990ab84d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aef7bc7421006bd79b09ee58beeeb73d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a822e2a9e9ff28516561415e77521bb8.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="MLLM-as-Retriever-Interactively-Learning-Multimodal-Retrieval-for-Embodied-Agents"><a href="#MLLM-as-Retriever-Interactively-Learning-Multimodal-Retrieval-for-Embodied-Agents" class="headerlink" title="MLLM as Retriever: Interactively Learning Multimodal Retrieval for   Embodied Agents"></a>MLLM as Retriever: Interactively Learning Multimodal Retrieval for   Embodied Agents</h2><p><strong>Authors:Junpeng Yue, Xinrun Xu, BÃ¶rje F. Karlsson, Zongqing Lu</strong></p>
<p>MLLM agents demonstrate potential for complex embodied tasks by retrieving multimodal task-relevant trajectory data. However, current retrieval methods primarily focus on surface-level similarities of textual or visual cues in trajectories, neglecting their effectiveness for the specific task at hand. To address this issue, we propose a novel method, MLLM As ReTriever (MART), which enhances the performance of embodied agents by utilizing interaction data to fine-tune an MLLM retriever based on preference learning, such that the retriever fully considers the effectiveness of trajectories and prioritizes them for unseen tasks. We also introduce Trajectory Abstraction, a mechanism that leverages MLLMsâ€™ summarization capabilities to represent trajectories with fewer tokens while preserving key information, enabling agents to better comprehend milestones in the trajectory. Experimental results across various environments demonstrate our method significantly improves task success rates in unseen scenes compared to baseline methods. This work presents a new paradigm for multimodal retrieval in embodied agents, by fine-tuning a general-purpose MLLM as the retriever to assess trajectory effectiveness. All the code for benchmark tasks, simulator modifications, and the MLLM retriever is available at <a target="_blank" rel="noopener" href="https://github.com/PKU-RL/MART">https://github.com/PKU-RL/MART</a>. </p>
<blockquote>
<p>MLLMä»£ç†é€šè¿‡æ£€ç´¢å¤šæ¨¡å¼ä»»åŠ¡ç›¸å…³è½¨è¿¹æ•°æ®ï¼Œå±•ç°å‡ºæ‰§è¡Œå¤æ‚å®ä½“ä»»åŠ¡æ½œåŠ›ã€‚ç„¶è€Œï¼Œå½“å‰æ£€ç´¢æ–¹æ³•ä¸»è¦å…³æ³¨è½¨è¿¹ä¸­æ–‡å­—æˆ–è§†è§‰çº¿ç´¢çš„è¡¨é¢ç›¸ä¼¼æ€§ï¼Œå¿½è§†äº†å®ƒä»¬åœ¨ç‰¹å®šä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œåä¸ºMLLMä½œä¸ºæ£€ç´¢å™¨ï¼ˆMARTï¼‰ï¼Œé€šè¿‡åˆ©ç”¨äº¤äº’æ•°æ®å¯¹åŸºäºåå¥½å­¦ä¹ çš„MLLMæ£€ç´¢å™¨è¿›è¡Œå¾®è°ƒï¼Œæé«˜å®ä½“ä»£ç†çš„æ€§èƒ½ï¼Œä½¿æ£€ç´¢å™¨å…¨é¢è€ƒè™‘è½¨è¿¹çš„æœ‰æ•ˆæ€§å¹¶ä¼˜å…ˆç”¨äºæœªè§ä»»åŠ¡ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†è½¨è¿¹æŠ½è±¡æœºåˆ¶ï¼Œè¯¥æœºåˆ¶åˆ©ç”¨MLLMçš„æ€»ç»“èƒ½åŠ›ä»¥è¾ƒå°‘çš„æ ‡è®°è¡¨ç¤ºè½¨è¿¹ï¼ŒåŒæ—¶ä¿ç•™å…³é”®ä¿¡æ¯ï¼Œä½¿ä»£ç†èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£è½¨è¿¹ä¸­çš„é‡Œç¨‹ç¢‘ã€‚åœ¨å¤šç§ç¯å¢ƒä¸‹çš„å®éªŒç»“æœè¡¨æ˜ï¼Œä¸åŸºå‡†æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æœªè§åœºæ™¯ä¸­çš„ä»»åŠ¡æˆåŠŸç‡æ˜¾è‘—æé«˜ã€‚è¿™é¡¹å·¥ä½œé€šè¿‡å¾®è°ƒé€šç”¨MLLMä½œä¸ºæ£€ç´¢å™¨æ¥è¯„ä¼°è½¨è¿¹çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå®ä½“ä»£ç†ä¸­çš„å¤šæ¨¡å¼æ£€ç´¢æä¾›äº†æ–°çš„èŒƒä¾‹ã€‚æ‰€æœ‰åŸºå‡†ä»»åŠ¡ã€æ¨¡æ‹Ÿå™¨ä¿®æ”¹å’ŒMLLMæ£€ç´¢å™¨çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/PKU-RL/">https://github.com/PKU-RL/</a> MARTæ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.03450v2">PDF</a> ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>MLLMä»£ç†åœ¨å¤æ‚çš„å®ä½“ä»»åŠ¡ä¸­å±•ç°å‡ºæ½œåŠ›ï¼Œé€šè¿‡æ£€ç´¢å¤šæ¨¡æ€ä»»åŠ¡ç›¸å…³è½¨è¿¹æ•°æ®æ¥å®Œæˆä»»åŠ¡ã€‚ç„¶è€Œï¼Œå½“å‰æ£€ç´¢æ–¹æ³•ä¸»è¦å…³æ³¨è½¨è¿¹çš„æ–‡æœ¬æˆ–è§†è§‰çº¿ç´¢çš„è¡¨é¢ç›¸ä¼¼æ€§ï¼Œå¿½è§†äº†å®ƒä»¬å¯¹ç‰¹å®šä»»åŠ¡çš„æœ‰æ•ˆæ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•MLLM As ReTrieverï¼ˆMARTï¼‰ï¼Œé€šè¿‡åˆ©ç”¨äº¤äº’æ•°æ®å¯¹åŸºäºåå¥½å­¦ä¹ çš„MLLMæ£€ç´¢å™¨è¿›è¡Œå¾®è°ƒï¼Œæé«˜å®ä½“ä»£ç†çš„æ€§èƒ½ï¼Œä½¿æ£€ç´¢å™¨å……åˆ†è€ƒè™‘è½¨è¿¹çš„æœ‰æ•ˆæ€§å¹¶ä¼˜å…ˆå¤„ç†æœªè§ä»»åŠ¡ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†è½¨è¿¹æŠ½è±¡æœºåˆ¶ï¼Œåˆ©ç”¨MLLMçš„æ€»ç»“èƒ½åŠ›ä»¥æ›´å°‘çš„ä»¤ç‰Œè¡¨ç¤ºè½¨è¿¹ï¼ŒåŒæ—¶ä¿ç•™å…³é”®ä¿¡æ¯ï¼Œä½¿ä»£ç†æ›´å¥½åœ°ç†è§£è½¨è¿¹ä¸­çš„é‡Œç¨‹ç¢‘ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸åŸºå‡†æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æœªè§åœºæ™¯ä¸­çš„ä»»åŠ¡æˆåŠŸç‡æ˜¾è‘—æé«˜ã€‚æœ¬ç ”ç©¶ä¸ºå®ä½“ä»£ç†ä¸­çš„å¤šæ¨¡æ€æ£€ç´¢æä¾›äº†æ–°çš„èŒƒä¾‹ï¼Œå³é€šè¿‡å¾®è°ƒé€šç”¨MLLMä½œä¸ºæ£€ç´¢å™¨æ¥è¯„ä¼°è½¨è¿¹çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLMä»£ç†åœ¨å¤æ‚å®ä½“ä»»åŠ¡ä¸­å±•ç°æ½œåŠ›ï¼Œå¯é€šè¿‡æ£€ç´¢å¤šæ¨¡æ€è½¨è¿¹æ•°æ®å®Œæˆä»»åŠ¡ã€‚</li>
<li>å½“å‰æ£€ç´¢æ–¹æ³•ä¸»è¦å…³æ³¨è¡¨é¢ç›¸ä¼¼æ€§ï¼Œå¿½è§†è½¨è¿¹å¯¹ç‰¹å®šä»»åŠ¡çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>æå‡ºæ–°æ–¹æ³•MLLM As ReTrieverï¼ˆMARTï¼‰ï¼Œåˆ©ç”¨äº¤äº’æ•°æ®å¾®è°ƒMLLMæ£€ç´¢å™¨ï¼Œä½¿å…¶å……åˆ†è€ƒè™‘è½¨è¿¹æœ‰æ•ˆæ€§å¹¶ä¼˜å…ˆå¤„ç†æœªè§ä»»åŠ¡ã€‚</li>
<li>å¼•å…¥è½¨è¿¹æŠ½è±¡æœºåˆ¶ï¼Œç”¨æ›´å°‘ä»¤ç‰Œè¡¨ç¤ºè½¨è¿¹åŒæ—¶ä¿ç•™å…³é”®ä¿¡æ¯ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¸åŸºå‡†æ–¹æ³•ç›¸æ¯”ï¼ŒMARTåœ¨æœªè§åœºæ™¯çš„ä»»åŠ¡æˆåŠŸç‡ä¸Šæ˜¾è‘—æé«˜ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºå®ä½“ä»£ç†ä¸­çš„å¤šæ¨¡æ€æ£€ç´¢æä¾›äº†æ–°çš„èŒƒä¾‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.03450">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-bbbbad6b7764c8356caf6ff41a929024.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0c2dc96dff9b3dfe4e0a0db54745d55c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-87be4308ec094e70d8e89a59bd665c44.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6451cc204fc87ec59161ad833ebfa467.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-24/Agent/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-24/Agent/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Agent/">
                                    <span class="chip bg-color">Agent</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-24/MMT/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-cbbd765fafcd09f21f4b2375d4b1e5ce.jpg" class="responsive-img" alt="MMT">
                        
                        <span class="card-title">MMT</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            MMT æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-24  Property Enhanced Instruction Tuning for Multi-task Molecule Generation   with Large Language Models
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-24
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/MMT/" class="post-category">
                                    MMT
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/MMT/">
                        <span class="chip bg-color">MMT</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-24/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-612c734a99dd1fec3a0ba7edf415c57a.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-24  Let Androids Dream of Electric Sheep A Human-like Image Implication   Understanding and Reasoning Framework
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-24
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33297.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
