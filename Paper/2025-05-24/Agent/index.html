<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Agent">
    <meta name="description" content="Agent 方向最新论文已更新，请持续关注 Update in 2025-05-24  From EduVisBench to EduVisAgent A Benchmark and Multi-Agent Framework   for Pedagogical Visualization">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Agent | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-057aeeb66cc0433f578069a2a100f587.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Agent</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Agent/">
                                <span class="chip bg-color">Agent</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                Agent
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-05-24
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    16.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    67 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-05-24-更新"><a href="#2025-05-24-更新" class="headerlink" title="2025-05-24 更新"></a>2025-05-24 更新</h1><h2 id="From-EduVisBench-to-EduVisAgent-A-Benchmark-and-Multi-Agent-Framework-for-Pedagogical-Visualization"><a href="#From-EduVisBench-to-EduVisAgent-A-Benchmark-and-Multi-Agent-Framework-for-Pedagogical-Visualization" class="headerlink" title="From EduVisBench to EduVisAgent: A Benchmark and Multi-Agent Framework   for Pedagogical Visualization"></a>From EduVisBench to EduVisAgent: A Benchmark and Multi-Agent Framework   for Pedagogical Visualization</h2><p><strong>Authors:Haonian Ji, Shi Qiu, Siyang Xin, Siwei Han, Zhaorun Chen, Hongyi Wang, Dake Zhang, Huaxiu Yao</strong></p>
<p>While foundation models (FMs), such as diffusion models and large vision-language models (LVLMs), have been widely applied in educational contexts, their ability to generate pedagogically effective visual explanations remains limited. Most existing approaches focus primarily on textual reasoning, overlooking the critical role of structured and interpretable visualizations in supporting conceptual understanding. To better assess the visual reasoning capabilities of FMs in educational settings, we introduce EduVisBench, a multi-domain, multi-level benchmark. EduVisBench features diverse STEM problem sets requiring visually grounded solutions, along with a fine-grained evaluation rubric informed by pedagogical theory. Our empirical analysis reveals that existing models frequently struggle with the inherent challenge of decomposing complex reasoning and translating it into visual representations aligned with human cognitive processes. To address these limitations, we propose EduVisAgent, a multi-agent collaborative framework that coordinates specialized agents for instructional planning, reasoning decomposition, metacognitive prompting, and visualization design. Experimental results show that EduVisAgent substantially outperforms all baselines, achieving a 40.2% improvement and delivering more educationally aligned visualizations. EduVisBench and EduVisAgent are available at <a target="_blank" rel="noopener" href="https://github.com/aiming-lab/EduVisBench">https://github.com/aiming-lab/EduVisBench</a> and <a target="_blank" rel="noopener" href="https://github.com/aiming-lab/EduVisAgent">https://github.com/aiming-lab/EduVisAgent</a>. </p>
<blockquote>
<p>尽管扩散模型等大型基础模型（FMs）和大型视觉语言模型（LVLMs）在教育环境中得到了广泛应用，它们在生成教学有效的视觉解释方面的能力仍然有限。目前大多数方法主要关注文本推理，忽视了结构化、可解释的视觉支持在促进概念理解方面的关键作用。为了更好地评估教育环境中基础模型的视觉推理能力，我们引入了EduVisBench，这是一个多领域、多层次的基准测试。EduVisBench包含多样化的STEM问题集，需要视觉解决方案，以及由教学理论支持的精细评估标准。我们的实证分析表明，现有模型在分解复杂推理并将其转化为与人类认知过程相符的视觉表示方面存在固有的挑战。为了解决这些局限性，我们提出了EduVisAgent，这是一个多智能体协作框架，协调专门用于教学规划的智能体，进行推理分解、元认知提示和可视化设计。实验结果表明，EduVisAgent显著优于所有基线模型，实现了40.2%的改进，并提供了更符合教育要求的可视化。EduVisBench和EduVisAgent可在以下网站访问：<a target="_blank" rel="noopener" href="https://github.com/aiming-lab/EduVisBench">https://github.com/aiming-lab/EduVisBench</a> 和 <a target="_blank" rel="noopener" href="https://github.com/aiming-lab/EduVisAgent">https://github.com/aiming-lab/EduVisAgent</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16832v1">PDF</a> 16 pages; 7 figures</p>
<p><strong>Summary</strong></p>
<p>该文探讨了基础模型（如扩散模型和大型视觉语言模型）在教育背景下的应用及其生成有效视觉解释的局限性。为此，文章引入了一个多领域、多层次的基准测试EduVisBench，通过实证分析和精细化评估标准来评估模型在视觉推理方面的能力。针对现有模型的不足，文章提出了EduVisAgent多代理协作框架，通过专业化的代理来协调教学规划、推理分解、元认知提示和可视化设计。实验结果显示，EduVisAgent显著优于所有基线模型，实现了40.2%的改进，并提供了更符合教育需求的数据可视化。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>基础模型在教育领域应用广泛，但在生成有效的视觉解释方面存在局限性。</li>
<li>现有模型主要关注文本推理，忽视了结构化、可解释的视觉支持在概念理解中的重要性。</li>
<li>引入EduVisBench基准测试，旨在评估模型在教育环境中的视觉推理能力。</li>
<li>实证分析显示现有模型在分解复杂推理和转化为与人类认知过程对齐的视觉表示方面存在挑战。</li>
<li>提出EduVisAgent多代理协作框架，包含教学规划、推理分解、元认知提示和可视化设计等功能。</li>
<li>实验结果显示EduVisAgent显著优于基线模型，实现了教育视觉化的改进。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16832">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-257fdc584862c8504099526384973fc8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-88d32afee7d33df3fe3957ab76eeb5b5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d199b348347b7272917911acd269691e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d307f6d9b7d9ff21c5993b7d005ccc30.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8a06327db499ffbb7a55953f0d10fb9d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-abdd914104220633265fd78c361fa0e8.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="GUI-explorer-Autonomous-Exploration-and-Mining-of-Transition-aware-Knowledge-for-GUI-Agent"><a href="#GUI-explorer-Autonomous-Exploration-and-Mining-of-Transition-aware-Knowledge-for-GUI-Agent" class="headerlink" title="GUI-explorer: Autonomous Exploration and Mining of Transition-aware   Knowledge for GUI Agent"></a>GUI-explorer: Autonomous Exploration and Mining of Transition-aware   Knowledge for GUI Agent</h2><p><strong>Authors:Bin Xie, Rui Shao, Gongwei Chen, Kaiwen Zhou, Yinchuan Li, Jie Liu, Min Zhang, Liqiang Nie</strong></p>
<p>GUI automation faces critical challenges in dynamic environments. MLLMs suffer from two key issues: misinterpreting UI components and outdated knowledge. Traditional fine-tuning methods are costly for app-specific knowledge updates. We propose GUI-explorer, a training-free GUI agent that incorporates two fundamental mechanisms: (1) Autonomous Exploration of Function-aware Trajectory. To comprehensively cover all application functionalities, we design a Function-aware Task Goal Generator that automatically constructs exploration goals by analyzing GUI structural information (e.g., screenshots and activity hierarchies). This enables systematic exploration to collect diverse trajectories. (2) Unsupervised Mining of Transition-aware Knowledge. To establish precise screen-operation logic, we develop a Transition-aware Knowledge Extractor that extracts effective screen-operation logic through unsupervised analysis the state transition of structured interaction triples (observation, action, outcome). This eliminates the need for human involvement in knowledge extraction. With a task success rate of 53.7% on SPA-Bench and 47.4% on AndroidWorld, GUI-explorer shows significant improvements over SOTA agents. It requires no parameter updates for new apps. GUI-explorer is open-sourced and publicly available at <a target="_blank" rel="noopener" href="https://github.com/JiuTian-VL/GUI-explorer">https://github.com/JiuTian-VL/GUI-explorer</a>. </p>
<blockquote>
<p>图形用户界面（GUI）自动化在动态环境中面临重大挑战。MLLMs面临两个关键问题：误解UI组件和知识储备过时。传统的微调方法对于特定应用程序的知识更新成本高昂。我们提出了GUI-explorer，这是一款无需训练的GUI代理，它包含两种基本机制：（1）功能感知轨迹的自主探索。为了全面覆盖所有应用程序功能，我们设计了一个功能感知任务目标生成器，它通过自动分析GUI结构信息（例如截图和活动层次结构）来构建探索目标。这能够实现系统的探索并收集各种轨迹。（2）过渡感知知识的无监督挖掘。为了建立精确的屏幕操作逻辑，我们开发了一个过渡感知知识提取器，它通过无监督分析结构化交互三元组（观察、行动、结果）的状态过渡来提取有效的屏幕操作逻辑。这消除了人类在知识提取中的参与。在SPA-Bench上，GUI-explorer的任务成功率为53.7%，在AndroidWorld上为47.4%，较其他最新代理有明显改进。GUI-explorer无需更新参数即可适应新应用程序。GUI-explorer已开源并可在<a target="_blank" rel="noopener" href="https://github.com/JiuTian-VL/GUI-explorer%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/JiuTian-VL/GUI-explorer公开访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16827v1">PDF</a> ACL 2025. Github: <a target="_blank" rel="noopener" href="https://github.com/JiuTian-VL/GUI-explorer">https://github.com/JiuTian-VL/GUI-explorer</a></p>
<p><strong>摘要</strong><br>自动化图形用户界面（GUI）面临动态环境中的关键挑战。MLLM面临两大问题：误解UI组件和知识过时。传统的微调方法对于应用程序特定知识的更新成本高昂。我们提出了GUI探索器，一种无需训练的GUI代理，它结合了两种基本机制：（1）功能感知轨迹的自主探索。为了全面覆盖所有应用程序功能，我们设计了一种功能感知任务目标生成器，它通过自动分析GUI结构信息（如截图和活动层次结构）来构建探索目标。这可以通过系统探索收集多样化的轨迹。（2）过渡感知知识的无监督挖掘。为了建立精确的屏幕操作逻辑，我们开发了一种过渡感知知识提取器，它通过无监督分析结构化交互三元组（观察、行动、结果）的状态转换来提取有效的屏幕操作逻辑。这消除了知识提取中人类参与的需要。在SPA-Bench上GUI探索器的任务成功率为53.7%，在AndroidWorld上为47.4%，相较于其他最先进代理有明显改进。GUI探索器无需针对新应用程序更新参数。GUI探索器已开源并可在<a target="_blank" rel="noopener" href="https://github.com/JiuTian-VL/GUI-explorer%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/JiuTian-VL/GUI-explorer获取。</a></p>
<p><strong>关键见解</strong></p>
<ol>
<li>GUI自动化在动态环境中面临挑战，包括误解UI组件和知识过时问题。</li>
<li>提出了一种名为GUI-explorer的无需训练的新GUI代理，解决传统方法的局限性。</li>
<li>GUI-explorer通过自主探索和过渡感知知识挖掘两种基本机制工作。</li>
<li>功能感知任务目标生成器通过分析GUI结构信息自动构建探索目标，实现全面的应用功能覆盖。</li>
<li>过渡感知知识提取器通过无监督分析结构化交互三元组的状态转换来提取屏幕操作逻辑，无需人工参与。</li>
<li>GUI-explorer在SPA-Bench和AndroidWorld上的任务成功率超过其他最先进代理。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16827">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-4bf39af22c6095505a2720733091358f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-926afe9b889a733bcf5e09d06949526b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a4630743c059f9fcc6fa284f22232b41.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fa279616d8737bd9e815cf5082450b0c.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="A-modular-framework-for-automated-evaluation-of-procedural-content-generation-in-serious-games-with-deep-reinforcement-learning-agents"><a href="#A-modular-framework-for-automated-evaluation-of-procedural-content-generation-in-serious-games-with-deep-reinforcement-learning-agents" class="headerlink" title="A modular framework for automated evaluation of procedural content   generation in serious games with deep reinforcement learning agents"></a>A modular framework for automated evaluation of procedural content   generation in serious games with deep reinforcement learning agents</h2><p><strong>Authors:Eleftherios Kalafatis, Konstantinos Mitsis, Konstantia Zarkogianni, Maria Athanasiou, Konstantina Nikita</strong></p>
<p>Serious Games (SGs) are nowadays shifting focus to include procedural content generation (PCG) in the development process as a means of offering personalized and enhanced player experience. However, the development of a framework to assess the impact of PCG techniques when integrated into SGs remains particularly challenging. This study proposes a methodology for automated evaluation of PCG integration in SGs, incorporating deep reinforcement learning (DRL) game testing agents. To validate the proposed framework, a previously introduced SG featuring card game mechanics and incorporating three different versions of PCG for nonplayer character (NPC) creation has been deployed. Version 1 features random NPC creation, while versions 2 and 3 utilize a genetic algorithm approach. These versions are used to test the impact of different dynamic SG environments on the proposed framework’s agents. The obtained results highlight the superiority of the DRL game testing agents trained on Versions 2 and 3 over those trained on Version 1 in terms of win rate (i.e. number of wins per played games) and training time. More specifically, within the execution of a test emulating regular gameplay, both Versions 2 and 3 peaked at a 97% win rate and achieved statistically significant higher (p&#x3D;0009) win rates compared to those achieved in Version 1 that peaked at 94%. Overall, results advocate towards the proposed framework’s capability to produce meaningful data for the evaluation of procedurally generated content in SGs. </p>
<blockquote>
<p>如今，严肃游戏（SGs）正将焦点转向在开发过程中包含程序内容生成（PCG），以提供个性化的增强玩家体验。然而，建立一个框架来评估PCG技术在SGs中的影响仍然是一个特别大的挑战。本研究提出了一种自动化评估SG中PCG集成的方法，该方法结合了深度强化学习（DRL）游戏测试代理。为了验证所提出的框架，已经部署了一款之前引入的采用卡牌游戏机制的SG，并融入了三种不同版本的PCG来进行非玩家角色（NPC）的创建。版本1具有随机NPC创建功能，而版本2和版本3则采用遗传算法方法。这些版本被用来测试不同的动态SG环境对所提出框架的代理的影响。获得的结果突显了版本2和版本3训练的DRL游戏测试代理在胜率（即每场游戏的获胜次数）和训练时间上优于版本1训练的代理。更具体地说，在一个模拟常规游戏玩法的测试中，版本2和版本3的胜率达到了峰值，为97%，并且其胜率在统计学上显著高于版本1（p&#x3D;0.0009），版本1的峰值胜率为94%。总体而言，结果支持所提出框架在评估SG中程序生成内容方面的能力，能够产生有意义的数据。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16801v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探讨了将程序化内容生成（PCG）技术集成到严肃游戏（SGs）中，并应用深度强化学习（DRL）游戏测试代理进行自动化评估的方法论。通过部署一款采用卡牌游戏机制的严肃游戏，并设计三种不同版本的PCG NPC创建技术进行测试验证，发现版本2和版本3中应用的遗传算法在训练出的DRL游戏测试代理性能上表现出优越性，具体体现在胜率和训练时间上。结果支持该框架对程序化生成内容进行有意义评估的能力。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>严肃游戏现在将焦点转向在开发过程中加入程序化内容生成（PCG），以提供个性化的增强游戏体验。</li>
<li>提出了一种自动化评估PCG集成在SGs中的方法论，利用深度强化学习（DRL）游戏测试代理。</li>
<li>通过部署一款严肃游戏并设计三个版本的PCG NPC创建技术进行测试验证，发现版本2和版本3的遗传算法表现更优秀。</li>
<li>DRL游戏测试代理在版本2和版本3中的胜率和训练时间均优于版本1。</li>
<li>结果支持该框架对程序化生成内容进行有意义评估的能力。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16801">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5334162e4dadb8f6a0cb3deb97595497.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5b8b874dba097381d05cf78ae8e1d0bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3171f0f36f8f9b885bffc0d69e345571.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-69a7a559179f00fc018f85fc5ed93c3c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8dc8efa905340f9ffde36a0b6be880b6.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="O-2-Searcher-A-Searching-based-Agent-Model-for-Open-Domain-Open-Ended-Question-Answering"><a href="#O-2-Searcher-A-Searching-based-Agent-Model-for-Open-Domain-Open-Ended-Question-Answering" class="headerlink" title="O$^2$-Searcher: A Searching-based Agent Model for Open-Domain Open-Ended   Question Answering"></a>O$^2$-Searcher: A Searching-based Agent Model for Open-Domain Open-Ended   Question Answering</h2><p><strong>Authors:Jianbiao Mei, Tao Hu, Daocheng Fu, Licheng Wen, Xuemeng Yang, Rong Wu, Pinlong Cai, Xing Gao, Yu Yang, Chengjun Xie, Botian Shi, Yong Liu, Yu Qiao</strong></p>
<p>Large Language Models (LLMs), despite their advancements, are fundamentally limited by their static parametric knowledge, hindering performance on tasks requiring open-domain up-to-date information. While enabling LLMs to interact with external knowledge environments is a promising solution, current efforts primarily address closed-end problems. Open-ended questions, which characterized by lacking a standard answer or providing non-unique and diverse answers, remain underexplored. To bridge this gap, we present O$^2$-Searcher, a novel search agent leveraging reinforcement learning to effectively tackle both open-ended and closed-ended questions in the open domain. O$^2$-Searcher leverages an efficient, locally simulated search environment for dynamic knowledge acquisition, effectively decoupling the external world knowledge from model’s sophisticated reasoning processes. It employs a unified training mechanism with meticulously designed reward functions, enabling the agent to identify problem types and adapt different answer generation strategies. Furthermore, to evaluate performance on complex open-ended tasks, we construct O$^2$-QA, a high-quality benchmark featuring 300 manually curated, multi-domain open-ended questions with associated web page caches. Extensive experiments show that O$^2$-Searcher, using only a 3B model, significantly surpasses leading LLM agents on O$^2$-QA. It also achieves SOTA results on various closed-ended QA benchmarks against similarly-sized models, while performing on par with much larger ones. </p>
<blockquote>
<p>尽管大型语言模型（LLMs）取得了进展，但它们仍受到静态参数知识的根本限制，这阻碍了它们在需要开放领域最新信息的任务上的表现。虽然让LLMs与外部环境进行交互是一个有前景的解决方案，但目前的努力主要侧重于解决封闭式问题。开放式问题缺乏标准答案或提供非唯一和多样化的答案，仍然被探索得不够。为了弥补这一差距，我们提出了O$^2$-Searcher，这是一个新的搜索代理，利用强化学习有效地解决开放式和封闭式问题在开放领域的问题。O$^2$-Searcher利用高效、局部模拟的搜索环境进行动态知识获取，有效地将外部世界的知识从模型的复杂推理过程中解耦出来。它采用统一的训练机制和精心设计的奖励函数，使代理能够识别问题类型并适应不同的答案生成策略。此外，为了评估在复杂开放式任务上的性能，我们构建了O$^2$-QA，这是一个高质量的标准，包含300个手动整理、多领域的开放式问题以及与相关网页缓存相关的内容。大量实验表明，仅使用3B模型的O$^2$-Searcher在O$^2$-QA上显著超越了领先的大型语言模型代理。它在各种封闭式问答基准测试上也取得了最新结果，在与同类规模的模型中表现突出，同时在与更大的模型的表现上持平。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16582v1">PDF</a> 25 pages, 9 figures</p>
<p><strong>Summary</strong></p>
<p>本文探讨了大型语言模型在处理开放领域、实时信息的任务时的局限性，尤其是在处理开放性问题方面存在不足。为解决这一问题，文章提出了一种名为O$^2$-Searcher的新型搜索代理，该代理利用强化学习有效应对开放性和封闭性问题。它通过高效的本地模拟搜索环境实现动态知识获取，将外部世界知识与模型的复杂推理过程解耦。此外，为评估在复杂开放性任务上的性能，文章构建了一个高质量基准测试O$^2$-QA。实验表明，O$^2$-Searcher在O$^2$-QA上的表现显著超越了领先的大型语言模型代理，同时在各种封闭性问答基准测试上达到了顶尖水平。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型在处理需要开放领域实时信息的任务时存在局限性。</li>
<li>O$^2$-Searcher是一种新型搜索代理，能处理开放性和封闭性问题。</li>
<li>O$^2$-Searcher利用强化学习，在本地模拟搜索环境实现动态知识获取。</li>
<li>O$^2$-Searcher将外部世界知识与模型的推理过程解耦。</li>
<li>为评估在开放性任务上的性能，构建了基准测试O$^2$-QA。</li>
<li>O$^2$-Searcher在O$^2$-QA上的表现超越了其他大型语言模型代理。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16582">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a79122ab1a26aa98808275a60d0d0a34.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b8a2a3bec0c619d0bd283c919a14b9d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d14ba5cf04f2b87a3c074368134e999.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b8577460e43a987c0b9ce0ecd0bcf8e4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-36c9a2ec9ca7826a16f52102bac095ff.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Psychology-driven-LLM-Agents-for-Explainable-Panic-Prediction-on-Social-Media-during-Sudden-Disaster-Events"><a href="#Psychology-driven-LLM-Agents-for-Explainable-Panic-Prediction-on-Social-Media-during-Sudden-Disaster-Events" class="headerlink" title="Psychology-driven LLM Agents for Explainable Panic Prediction on Social   Media during Sudden Disaster Events"></a>Psychology-driven LLM Agents for Explainable Panic Prediction on Social   Media during Sudden Disaster Events</h2><p><strong>Authors:Mengzhu Liu, Zhengqiu Zhu, Chuan Ai, Chen Gao, Xinghong Li, Lingnan He, Kaisheng Lai, Yingfeng Chen, Xin Lu, Yong Li, Quanjun Yin</strong></p>
<p>During sudden disaster events, accurately predicting public panic sentiment on social media is crucial for proactive governance and crisis management. Current efforts on this problem face three main challenges: lack of finely annotated data hinders emotion prediction studies, unmodeled risk perception causes prediction inaccuracies, and insufficient interpretability of panic formation mechanisms. We address these issues by proposing a Psychology-driven generative Agent framework (PsychoAgent) for explainable panic prediction based on emotion arousal theory. Specifically, we first construct a fine-grained open panic emotion dataset (namely COPE) via human-large language models (LLMs) collaboration to mitigate semantic bias. Then, we develop a framework integrating cross-domain heterogeneous data grounded in psychological mechanisms to model risk perception and cognitive differences in emotion generation. To enhance interpretability, we design an LLM-based role-playing agent that simulates individual psychological chains through dedicatedly designed prompts. Experimental results on our annotated dataset show that PsychoAgent improves panic emotion prediction performance by 12.6% to 21.7% compared to baseline models. Furthermore, the explainability and generalization of our approach is validated. Crucially, this represents a paradigm shift from opaque “data-driven fitting” to transparent “role-based simulation with mechanistic interpretation” for panic emotion prediction during emergencies. Our implementation is publicly available at: <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/PsychoAgent-19DD">https://anonymous.4open.science/r/PsychoAgent-19DD</a>. </p>
<blockquote>
<p>在突发灾难事件中，准确预测社交媒体上的公众恐慌情绪对于积极治理和危机管理至关重要。目前，针对这个问题的研究面临三大挑战：缺乏精细标注的数据阻碍了情绪预测研究，未建模的风险感知导致预测不准确，以及恐慌形成机制的解释性不足。我们通过提出基于情绪激发理论的心理学驱动生成代理框架（PsychoAgent）来解决这些问题，以实现可解释的恐慌预测。具体而言，我们首先通过人类与大型语言模型（LLM）的合作，构建了一个精细的开放恐慌情绪数据集（名为COPE），以减轻语义偏见。然后，我们开发了一个整合跨域异构数据的框架，基于心理机制来模拟风险感知和情绪产生的认知差异。为了提高解释性，我们设计了一个基于LLM的角色扮演代理，通过专门设计的提示来模拟个体心理链。在我们标注的数据集上的实验结果表明，与基准模型相比，PsychoAgent提高了12.6%至21.7%的恐慌情绪预测性能。此外，我们的方法的可解释性和通用性得到了验证。重要的是，这代表了从隐蔽的“数据驱动拟合”到透明的“基于角色的模拟与机制解释”的范式转变，为紧急情况下的恐慌情绪预测提供了新的视角。我们的实现公开可用：<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/PsychoAgent-19DD%E3%80%82">https://anonymous.4open.science/r/PsychoAgent-19DD。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16455v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于情绪激发理论的心理驱动生成Agent框架（PsychoAgent）可实现解释性的恐慌预测。该研究构建了精细开放的恐慌情绪数据集COPE，开发了一个整合跨域异构数据的框架，模拟个体心理链，提高预测的可解释性。实验结果表明，PsychoAgent相较于基准模型，在恐慌情绪预测性能上提高了12.6%至21.7%。该研究的实施已公开。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>在突发灾难事件中，准确预测社交媒体上的公众恐慌情绪对积极治理和危机管理至关重要。</li>
<li>当前研究面临三大挑战：缺乏精细标注的数据、未建模的风险感知导致预测不准确以及恐慌形成机制的解释性不足。</li>
<li>提出心理驱动生成Agent框架（PsychoAgent）进行解释性恐慌预测，基于情绪激发理论。</li>
<li>构建了一个精细开放的恐慌情绪数据集（COPE），通过人与大型语言模型（LLMs）的合作减少语义偏见。</li>
<li>集成跨域异构数据，基于心理机制建模风险感知和情绪产生的认知差异。</li>
<li>设计了基于LLM的角色扮演agent，模拟个体心理链，提高预测的可解释性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16455">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-ebea3e5e37f8e81e071ebbf1e40f42de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-33c573d0da3e2daf11d35ff165b09c2a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2801b3e83c85d651716d8236a56783b0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Beyond-Static-Testbeds-An-Interaction-Centric-Agent-Simulation-Platform-for-Dynamic-Recommender-Systems"><a href="#Beyond-Static-Testbeds-An-Interaction-Centric-Agent-Simulation-Platform-for-Dynamic-Recommender-Systems" class="headerlink" title="Beyond Static Testbeds: An Interaction-Centric Agent Simulation Platform   for Dynamic Recommender Systems"></a>Beyond Static Testbeds: An Interaction-Centric Agent Simulation Platform   for Dynamic Recommender Systems</h2><p><strong>Authors:Song Jin, Juntian Zhang, Yuhan Liu, Xun Zhang, Yufei Zhang, Guojun Yin, Fei Jiang, Wei Lin, Rui Yan</strong></p>
<p>Evaluating and iterating upon recommender systems is crucial, yet traditional A&#x2F;B testing is resource-intensive, and offline methods struggle with dynamic user-platform interactions. While agent-based simulation is promising, existing platforms often lack a mechanism for user actions to dynamically reshape the environment. To bridge this gap, we introduce RecInter, a novel agent-based simulation platform for recommender systems featuring a robust interaction mechanism. In RecInter platform, simulated user actions (e.g., likes, reviews, purchases) dynamically update item attributes in real-time, and introduced Merchant Agents can reply, fostering a more realistic and evolving ecosystem. High-fidelity simulation is ensured through Multidimensional User Profiling module, Advanced Agent Architecture, and LLM fine-tuned on Chain-of-Thought (CoT) enriched interaction data. Our platform achieves significantly improved simulation credibility and successfully replicates emergent phenomena like Brand Loyalty and the Matthew Effect. Experiments demonstrate that this interaction mechanism is pivotal for simulating realistic system evolution, establishing our platform as a credible testbed for recommender systems research. </p>
<blockquote>
<p>评估并迭代推荐系统至关重要，然而传统的A&#x2F;B测试资源消耗大，且离线方法在动态用户平台交互方面表现挣扎。虽然基于代理的模拟很有前景，但现有平台通常缺乏一种机制，使用户行为能够动态地重塑环境。为了弥补这一差距，我们引入了RecInter，这是一个新型的基于代理的推荐系统模拟平台，具有稳健的交互机制。在RecInter平台上，模拟的用户行为（例如喜欢、评论、购买）会实时动态更新物品属性，引入的商户代理可以做出回应，营造更加真实和不断演化的生态系统。通过多维度用户分析模块、高级代理架构以及在大规模预训练模型上微调的大型语言模型（LLM），确保了高保真模拟。我们的平台大大提高了模拟的可信度，并成功复制了品牌忠诚度和马太效应等突发现象。实验表明，这种交互机制对于模拟真实系统演变至关重要，使我们的平台成为推荐系统研究的可信测试平台。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16429v1">PDF</a> </p>
<p><strong>Summary</strong><br>模拟用户行为在推荐系统中的交互，特别是实时动态更新的功能正在获得关注。然而传统的AB测试方式成本高昂，现有仿真平台的互动能力不强。为了解决此问题，本文引入了一个新型仿真平台RecInter，它通过用户模拟行为动态更新商品属性，并引入商户代理进行响应，从而构建更真实和动态的生态系统。RecInter平台通过多维度用户画像模块、先进的代理架构和微调的大型语言模型保证高保真模拟，并成功复制了品牌忠诚和马太效应等现实现象。平台建立的互动机制为模拟系统提供了真实可靠的测试环境。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>推荐系统的评估和迭代至关重要，但传统AB测试成本高且离线方法难以应对动态的用户-平台交互。</li>
<li>为解决这些问题，引入了一种新型的基于代理的仿真平台RecInter。</li>
<li>RecInter平台通过模拟用户行为实时更新商品属性，并引入商户代理进行响应，构建更真实和动态的生态系统。</li>
<li>平台采用了多维度用户画像模块、先进的代理架构和大型语言模型保证高保真模拟。</li>
<li>平台成功复制了品牌忠诚和马太效应等现实现象，证明了其模拟系统的真实性和可靠性。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16429">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-45ae30b2cf691b4f44e30d19dee851ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa756b599220f87c54359b8a78b29c43.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c523463fc1ac4d2cfbc5970fecdc7240.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="WebAgent-R1-Training-Web-Agents-via-End-to-End-Multi-Turn-Reinforcement-Learning"><a href="#WebAgent-R1-Training-Web-Agents-via-End-to-End-Multi-Turn-Reinforcement-Learning" class="headerlink" title="WebAgent-R1: Training Web Agents via End-to-End Multi-Turn Reinforcement   Learning"></a>WebAgent-R1: Training Web Agents via End-to-End Multi-Turn Reinforcement   Learning</h2><p><strong>Authors:Zhepei Wei, Wenlin Yao, Yao Liu, Weizhi Zhang, Qin Lu, Liang Qiu, Changlong Yu, Puyang Xu, Chao Zhang, Bing Yin, Hyokun Yun, Lihong Li</strong></p>
<p>While reinforcement learning (RL) has demonstrated remarkable success in enhancing large language models (LLMs), it has primarily focused on single-turn tasks such as solving math problems. Training effective web agents for multi-turn interactions remains challenging due to the complexity of long-horizon decision-making across dynamic web interfaces. In this work, we present WebAgent-R1, a simple yet effective end-to-end multi-turn RL framework for training web agents. It learns directly from online interactions with web environments by asynchronously generating diverse trajectories, entirely guided by binary rewards depending on task success. Experiments on the WebArena-Lite benchmark demonstrate the effectiveness of WebAgent-R1, boosting the task success rate of Qwen-2.5-3B from 6.1% to 33.9% and Llama-3.1-8B from 8.5% to 44.8%, significantly outperforming existing state-of-the-art methods and strong proprietary models such as OpenAI o3. In-depth analyses reveal the effectiveness of the thinking-based prompting strategy and test-time scaling through increased interactions for web tasks. We further investigate different RL initialization policies by introducing two variants, namely WebAgent-R1-Zero and WebAgent-R1-CoT, which highlight the importance of the warm-up training stage (i.e., behavior cloning) and provide insights on incorporating long chain-of-thought (CoT) reasoning in web agents. </p>
<blockquote>
<p>虽然强化学习（RL）在提升大型语言模型（LLM）方面取得了显著的成就，但它主要关注于如解决数学问题等单轮任务。由于跨动态网页界面的长周期决策复杂性，训练用于多轮交互的有效网络代理仍然具有挑战性。在这项工作中，我们提出了WebAgent-R1，这是一个简单但有效的端到端多轮强化学习框架，用于训练网络代理。它直接从与在线网络环境的交互中学习，通过异步生成多样的轨迹，完全由任务成功与否决定的二元奖励来引导。在WebArena-Lite基准测试上的实验证明了WebAgent-R1的有效性，它将Qwen-2.5-3B的任务成功率从6.1%提高到33.9%，将Llama-3.1-8B的任务成功率从8.5%提高到44.8%，显著优于现有的最先进的方法和强大的专有模型，如OpenAI o3。深入分析揭示了基于思考的提示策略和通过增加交互来进行测试时间缩放对于网络任务的有效性。我们进一步通过引入两种变体，即WebAgent-R1-Zero和WebAgent-R1-CoT，来探讨不同的RL初始化策略，这突出了热身训练阶段（即行为克隆）的重要性，并提供了将长链思维（CoT）推理融入网络代理的见解。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16421v1">PDF</a> Preprint</p>
<p><strong>Summary</strong><br>强化学习在提升大型语言模型方面已展现显著成效，尤其在单回合任务如数学问题求解中。然而，在训练用于多回合交互的网页代理时仍面临挑战。本研究提出WebAgent-R1，一个简洁有效的多回合强化学习框架，能直接从与网页环境的在线交互中学习，通过异步生成多样化轨迹，仅根据任务成功与否提供二元奖励。在WebArena-Lite基准测试上的实验显示，WebAgent-R1显著提升任务成功率，相较于Qwen-2.5-3B模型提升从6.1%至33.9%，Llama-3.1-8B模型从8.5%提升至44.8%，显著超越现有先进方法与强大专有模型如OpenAI o3。深入分析显示基于思考提示策略的有效性以及通过增加交互的测试时间缩放对网页任务的效果。此外，研究引入两种强化学习初始化策略变体，突显热身训练阶段（行为克隆）的重要性，并提供将长链思维融入网页代理的见解。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>WebAgent-R1是一个用于训练网页代理的多回合强化学习框架，可从与网页环境的在线交互中直接学习。</li>
<li>该框架通过异步生成多样化轨迹，仅依据任务成功与否提供二元奖励。</li>
<li>在WebArena-Lite基准测试上，WebAgent-R1显著提升任务成功率，相较其他模型有显著优势。</li>
<li>基于思考提示策略在网页任务中展现出有效性。</li>
<li>增加交互的测试时间缩放能够提高网页代理的性能。</li>
<li>引入的两种RL初始化策略变体突显热身训练阶段的重要性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16421">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5b644a27521daa3e125cf52c31b8ad4b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c4742907be9c02c3f5eddd2c23676c50.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-980744fa5f8b8bd8f97c52ebdbe2ff96.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a2f96938cbbf52620e1896460df54ea3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0ddb9172526b07885c64909614a637b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f85ab7cab094195f36a92b0579a0b449.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Embodied-Agents-Meet-Personalization-Exploring-Memory-Utilization-for-Personalized-Assistance"><a href="#Embodied-Agents-Meet-Personalization-Exploring-Memory-Utilization-for-Personalized-Assistance" class="headerlink" title="Embodied Agents Meet Personalization: Exploring Memory Utilization for   Personalized Assistance"></a>Embodied Agents Meet Personalization: Exploring Memory Utilization for   Personalized Assistance</h2><p><strong>Authors:Taeyoon Kwon, Dongwook Choi, Sunghwan Kim, Hyojun Kim, Seungjun Moon, Beong-woo Kwak, Kuan-Hao Huang, Jinyoung Yeo</strong></p>
<p>Embodied agents empowered by large language models (LLMs) have shown strong performance in household object rearrangement tasks. However, these tasks primarily focus on single-turn interactions with simplified instructions, which do not truly reflect the challenges of providing meaningful assistance to users. To provide personalized assistance, embodied agents must understand the unique semantics that users assign to the physical world (e.g., favorite cup, breakfast routine) by leveraging prior interaction history to interpret dynamic, real-world instructions. Yet, the effectiveness of embodied agents in utilizing memory for personalized assistance remains largely underexplored. To address this gap, we present MEMENTO, a personalized embodied agent evaluation framework designed to comprehensively assess memory utilization capabilities to provide personalized assistance. Our framework consists of a two-stage memory evaluation process design that enables quantifying the impact of memory utilization on task performance. This process enables the evaluation of agents’ understanding of personalized knowledge in object rearrangement tasks by focusing on its role in goal interpretation: (1) the ability to identify target objects based on personal meaning (object semantics), and (2) the ability to infer object-location configurations from consistent user patterns, such as routines (user patterns). Our experiments across various LLMs reveal significant limitations in memory utilization, with even frontier models like GPT-4o experiencing a 30.5% performance drop when required to reference multiple memories, particularly in tasks involving user patterns. These findings, along with our detailed analyses and case studies, provide valuable insights for future research in developing more effective personalized embodied agents. Project website: <a target="_blank" rel="noopener" href="https://connoriginal.github.io/MEMENTO">https://connoriginal.github.io/MEMENTO</a> </p>
<blockquote>
<p>由大型语言模型（LLMs）赋能的实体代理人在家庭物品重新布置任务中表现出强大的性能。然而，这些任务主要侧重于与简化指令的单轮交互，并不能真正反映为用户提供有意义帮助的挑战。为了提供个性化的帮助，实体代理人必须利用先前的交互历史来理解用户赋予物理世界的独特语义（例如，最喜欢的杯子、早餐例行程序），以解释动态、现实世界的指令。然而，实体代理人在利用记忆提供个性化帮助方面的有效性在很大程度上尚未被探索。为了弥补这一空白，我们推出了MEMENTO，这是一个个性化的实体代理人评估框架，旨在全面评估利用记忆提供个性化帮助的能力。我们的框架由两阶段记忆评估过程设计组成，能够量化记忆利用对任务性能的影响。这个过程通过关注记忆在目标解释中的重要作用，来评估代理人在物品重新布置任务中对个性化知识的理解能力：（1）基于个人意义（对象语义）识别目标对象的能力；（2）从一致的用户模式中推断对象位置配置的能力，例如例行程序（用户模式）。我们在各种LLMs上的实验揭示了记忆利用的显著局限性，甚至像GPT-4o这样的前沿模型在需要参考多条记忆时性能下降了30.5%，特别是在涉及用户模式的任务中。这些发现，以及我们的详细分析和案例研究，为未来研究开发更有效的个性化实体代理人提供了宝贵的见解。项目网站：<a target="_blank" rel="noopener" href="https://connoriginal.github.io/MEMENTO">https://connoriginal.github.io/MEMENTO</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16348v1">PDF</a> Work in progress</p>
<p><strong>Summary</strong></p>
<p>大型语言模型赋能的实体智能代理在家庭物体重新布置任务中展现出强大性能，但现有研究主要集中在简单的单回合交互任务上，无法真正反映为用户提供的个性化服务的挑战。为用户提供个性化服务，实体智能代理需要理解用户赋予物理世界的独特语义，并利用过去的交互历史来解读动态、真实世界的指令。然而，实体智能代理在利用记忆提供个性化服务方面的有效性尚未得到充分研究。为解决这一空白，本文提出了MEMENTO个性化实体智能代理评估框架，旨在全面评估记忆利用能力以提供个性化服务。实验结果显示，大型语言模型在记忆利用方面存在显著局限，即使是最先进的模型如GPT-4o在需要参考多个记忆的任务中性能下降30.5%，特别是在涉及用户模式的任务中。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型赋能的实体智能代理在家庭物体重新布置任务中表现出强大的性能。</li>
<li>当前研究主要集中在简单的单回合交互任务上，忽略了为用户提供个性化服务的挑战。</li>
<li>为提供个性化服务，实体智能代理需要理解用户赋予的物理世界的独特语义，并利用过去的交互历史解读真实世界的指令。</li>
<li>MEMENTO框架用于评估实体智能代理的记忆利用能力以提供个性化服务。</li>
<li>实验发现大型语言模型在记忆利用方面存在局限，尤其在涉及用户模式的任务中性能显著下降。</li>
<li>多个记忆参考的任务中，即使是先进的模型如GPT-4o也面临性能挑战。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16348">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-595e052b9d053cd433f64a1c60b8dbea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4af9c0e019ec8eeb68c49ceb235fa58a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-834ed23c654082c86b0f628e48681b1b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6b9428e2d3e9374c2f1b0aeea4bc13ea.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="GUI-G1-Understanding-R1-Zero-Like-Training-for-Visual-Grounding-in-GUI-Agents"><a href="#GUI-G1-Understanding-R1-Zero-Like-Training-for-Visual-Grounding-in-GUI-Agents" class="headerlink" title="GUI-G1: Understanding R1-Zero-Like Training for Visual Grounding in GUI   Agents"></a>GUI-G1: Understanding R1-Zero-Like Training for Visual Grounding in GUI   Agents</h2><p><strong>Authors:Yuqi Zhou, Sunhao Dai, Shuai Wang, Kaiwen Zhou, Qinglin Jia, Jun Xu</strong></p>
<p>Recent Graphical User Interface (GUI) agents replicate the R1-Zero paradigm, coupling online Reinforcement Learning (RL) with explicit chain-of-thought reasoning prior to object grounding and thereby achieving substantial performance gains. In this paper, we first conduct extensive analysis experiments of three key components of that training pipeline: input design, output evaluation, and policy update-each revealing distinct challenges arising from blindly applying general-purpose RL without adapting to GUI grounding tasks. Input design: Current templates encourage the model to generate chain-of-thought reasoning, but longer chains unexpectedly lead to worse grounding performance. Output evaluation: Reward functions based on hit signals or box area allow models to exploit box size, leading to reward hacking and poor localization quality. Policy update: Online RL tends to overfit easy examples due to biases in length and sample difficulty, leading to under-optimization on harder cases. To address these issues, we propose three targeted solutions. First, we adopt a Fast Thinking Template that encourages direct answer generation, reducing excessive reasoning during training. Second, we incorporate a box size constraint into the reward function to mitigate reward hacking. Third, we revise the RL objective by adjusting length normalization and adding a difficulty-aware scaling factor, enabling better optimization on hard samples. Our GUI-G1-3B, trained on 17K public samples with Qwen2.5-VL-3B-Instruct, achieves 90.3% accuracy on ScreenSpot and 37.1% on ScreenSpot-Pro. This surpasses all prior models of similar size and even outperforms the larger UI-TARS-7B, establishing a new state-of-the-art in GUI agent grounding. The project repository is available at <a target="_blank" rel="noopener" href="https://github.com/Yuqi-Zhou/GUI-G1">https://github.com/Yuqi-Zhou/GUI-G1</a>. </p>
<blockquote>
<p>最近的图形用户界面（GUI）代理复制了R1-Zero范式，将在线强化学习（RL）与对象接地之前的显式思维链推理相结合，从而实现了显著的性能提升。在本文中，我们首先对该训练管道的三个关键组件进行了广泛的分析实验：输入设计、输出评估和策略更新——每一个都揭示了盲目应用通用RL而不适应GUI接地任务所产生的独特挑战。输入设计：当前模板鼓励模型生成思维链推理，但更长的链条出乎意料地导致更差的接地性能。输出评估：基于命中信号或框区域的奖励函数允许模型利用框大小，从而导致奖励作弊和定位质量差。策略更新：由于长度和样本难度的偏见，在线RL往往会对简单示例进行过度拟合，导致在更复杂情况下的优化不足。为了解决这些问题，我们提出了三种有针对性的解决方案。首先，我们采用快速思考模板，鼓励直接答案生成，减少训练过程中的过度推理。其次，我们将框大小约束纳入奖励函数，以减轻奖励作弊。第三，我们通过调整长度归一化并添加一个难度感知缩放因子来修订RL目标，从而在困难样本上实现更好的优化。我们的GUI-G1-3B在17K公共样本上使用Qwen2.5-VL-3B-Instruct进行训练，在ScreenSpot上达到90.3%的准确率，在ScreenSpot-Pro上达到37.1%的准确率。这超越了类似大小的所有先前模型，甚至超越了更大的UI-TARS-7B，在GUI代理接地领域创造了新的最先进的水平。项目仓库可在<a target="_blank" rel="noopener" href="https://github.com/Yuqi-Zhou/GUI-G1%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Yuqi-Zhou/GUI-G1找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15810v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文分析了当前图形用户界面（GUI）代理在训练过程中所面临的挑战，并提出了针对性的解决方案。研究发现在输入设计、输出评价和策略更新三个方面存在挑战。为解决这些问题，提出了快速思考模板、引入盒尺寸约束和调整RL目标等措施。经过改进，GUI-G1-3B模型在ScreenSpot和ScreenSpot-Pro上取得了显著的成绩，达到了同类模型中的最佳水平。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>GUI代理在训练过程中面临输入设计、输出评价和策略更新的挑战。</li>
<li>现有模板鼓励生成链式思考推理，但过长的链会导致定位性能下降。</li>
<li>基于命中信号或框面积的奖励函数可能导致模型利用框大小，从而产生奖励黑客行为和低定位质量。</li>
<li>在线RL倾向于对简单样本过度拟合，导致在更复杂案例上的优化不足。</li>
<li>为解决这些问题，采取了快速思考模板、引入盒尺寸约束和调整RL目标等针对性措施。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15810">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-93e4aa858e84fbb59a5d1f3ef62ecec9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5fd4ff352e6c6e52c3811808acbab2c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a4634f1c6008752c48af1f39095efa3d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-057aeeb66cc0433f578069a2a100f587.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48bc256426467f534ecbb8e61d18a007.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3aa0d59216827c0371e9e8af5d495cc1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0b807a67082931ee47779fc1beb7ec4.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Multiple-Weaks-Win-Single-Strong-Large-Language-Models-Ensemble-Weak-Reinforcement-Learning-Agents-into-a-Supreme-One"><a href="#Multiple-Weaks-Win-Single-Strong-Large-Language-Models-Ensemble-Weak-Reinforcement-Learning-Agents-into-a-Supreme-One" class="headerlink" title="Multiple Weaks Win Single Strong: Large Language Models Ensemble Weak   Reinforcement Learning Agents into a Supreme One"></a>Multiple Weaks Win Single Strong: Large Language Models Ensemble Weak   Reinforcement Learning Agents into a Supreme One</h2><p><strong>Authors:Yiwen Song, Qianyue Hao, Qingmin Liao, Jian Yuan, Yong Li</strong></p>
<p>Model ensemble is a useful approach in reinforcement learning (RL) for training effective agents. Despite wide success of RL, training effective agents remains difficult due to the multitude of factors requiring careful tuning, such as algorithm selection, hyperparameter settings, and even random seed choices, all of which can significantly influence an agent’s performance. Model ensemble helps overcome this challenge by combining multiple weak agents into a single, more powerful one, enhancing overall performance. However, existing ensemble methods, such as majority voting and Boltzmann addition, are designed as fixed strategies and lack a semantic understanding of specific tasks, limiting their adaptability and effectiveness. To address this, we propose LLM-Ens, a novel approach that enhances RL model ensemble with task-specific semantic understandings driven by large language models (LLMs). Given a task, we first design an LLM to categorize states in this task into distinct ‘situations’, incorporating high-level descriptions of the task conditions. Then, we statistically analyze the strengths and weaknesses of each individual agent to be used in the ensemble in each situation. During the inference time, LLM-Ens dynamically identifies the changing task situation and switches to the agent that performs best in the current situation, ensuring dynamic model selection in the evolving task condition. Our approach is designed to be compatible with agents trained with different random seeds, hyperparameter settings, and various RL algorithms. Extensive experiments on the Atari benchmark show that LLM-Ens significantly improves the RL model ensemble, surpassing well-known baselines by up to 20.9%. For reproducibility, our code is open-source at <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/LLM4RLensemble-F7EE">https://anonymous.4open.science/r/LLM4RLensemble-F7EE</a>. </p>
<blockquote>
<p>模型集成是一种在强化学习（RL）中训练有效代理的有用方法。尽管强化学习取得了广泛的成功，但由于算法选择、超参数设置甚至随机种子选择等需要仔细调整的多重因素，训练有效代理仍然很困难，所有这些因素都可能显著影响代理的性能。模型集成通过把多个弱代理合并成一个更强大的单一代理，提高整体性能，从而克服这一挑战。然而，现有的集成方法，如多数投票和玻尔兹曼加法，被设计为固定策略，缺乏对特定任务的语义理解，限制了它们的适应性和有效性。为了解决这一问题，我们提出了LLM-Ens，这是一种结合大型语言模型（LLM）驱动的任务特定语义理解，增强RL模型集成的新型方法。给定一个任务，我们首先对LLM进行设计，将这个任务的状态分类为不同的“情境”，并融入任务条件的高级描述。然后，我们对集成中每种情境下每个个体代理的优势和劣势进行统计分析。在推理过程中，LLM-Ens动态识别任务情境的变化，并切换到当前情境下表现最佳的代理，确保在变化的任务条件下进行动态模型选择。我们的方法旨在与用不同随机种子、超参数设置和各种RL算法训练的代理兼容。在Atari基准测试的大量实验表明，LLM-Ens显著改进了RL模型集成，超越了知名基准测试达20.9%。为了可重复性，我们的代码已开源在<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/LLM4RLensemble-F7EE%E3%80%82">https://anonymous.4open.science/r/LLM4RLensemble-F7EE。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15306v1">PDF</a> </p>
<p><strong>Summary</strong><br>强化学习中的模型集成是一种训练有效代理的有用方法。训练有效的代理具有挑战性，因为它涉及到多个因素的微调，如算法选择、超参数设置甚至随机种子选择。模型集成通过结合多个弱代理为一个强大的单一代理来克服这一挑战，从而提高整体性能。然而，现有的集成方法如多数投票和波尔兹曼加法被设计为固定策略，缺乏特定任务的语义理解，限制了其适应性和有效性。为解决此问题，我们提出LLM-Ens，一种结合大型语言模型（LLM）驱动的特定任务语义理解的强化学习模型集成新方法。给定任务时，我们首先设计LLM将任务状态分类为不同的“情境”，并融入任务条件的高级描述。然后，我们统计分析了集成中每个个体代理在每个情境中的优势和劣势。在推理过程中，LLM-Ens动态识别任务情境的变化，并切换到当前情境下表现最佳的代理，确保在变化的任务条件下动态选择模型。我们的方法与使用不同随机种子、超参数设置和各种RL算法训练的代理兼容。在Atari基准测试上的大量实验表明，LLM-Ens显著提高了强化学习模型集成效果，超过知名基线高达20.9%。相关代码已开源于<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/LLM4RLensemble-F7EE%E3%80%82">https://anonymous.4open.science/r/LLM4RLensemble-F7EE。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>模型集成在强化学习中是一种有效的训练代理方法，通过结合多个代理来提高整体性能。</li>
<li>现有集成方法缺乏特定任务的语义理解，限制了其适应性和有效性。</li>
<li>LLM-Ens方法利用大型语言模型（LLM）来驱动任务特定语义理解，将任务状态分类为不同的情境。</li>
<li>LLM-Ens通过统计分析每个代理在每种情境中的表现来优化集成。</li>
<li>在推理过程中，LLM-Ens能动态识别任务情境变化，并选择最佳代理应对。</li>
<li>LLM-Ens与不同设置和算法训练的代理兼容。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15306">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-1008f4fb39bde997fdb1ca2ba8c8fbf3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9321e4ea17f6894a71b132c22e3ae6d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d92b972ecff087e709e5950622ed3a54.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Adaptive-Thinking-via-Mode-Policy-Optimization-for-Social-Language-Agents"><a href="#Adaptive-Thinking-via-Mode-Policy-Optimization-for-Social-Language-Agents" class="headerlink" title="Adaptive Thinking via Mode Policy Optimization for Social Language   Agents"></a>Adaptive Thinking via Mode Policy Optimization for Social Language   Agents</h2><p><strong>Authors:Minzheng Wang, Yongbin Li, Haobo Wang, Xinghua Zhang, Nan Xu, Bingli Wu, Fei Huang, Haiyang Yu, Wenji Mao</strong></p>
<p>Effective social intelligence simulation requires language agents to dynamically adjust reasoning depth, a capability notably absent in current studies. Existing methods either lack this kind of reasoning capability or enforce Long Chain-of-Thought reasoning uniformly across all scenarios, resulting in excessive token usage and inflexible social simulation. To address this, we propose an $\textbf{A}$daptive $\textbf{M}$ode $\textbf{L}$earning ($\textbf{AML}$) framework in this paper, aiming to improve the adaptive thinking ability of language agents in dynamic social interactions. To this end, we first identify hierarchical thinking modes ranging from intuitive response to deep deliberation based on the cognitive control theory. We then develop the $\textbf{A}$daptive $\textbf{M}$ode $\textbf{P}$olicy $\textbf{O}$ptimization ($\textbf{AMPO}$) algorithm to optimize the context-aware mode switching and reasoning. Our framework advances existing research in three key aspects: (1) Multi-granular thinking mode design, (2) Context-aware mode switching across social interaction, and (3) Token-efficient reasoning via depth-adaptive processing. Extensive experiments on social intelligence benchmarks verify that AML achieves 15.6% higher task performance than GPT-4o. Notably, our AMPO outperforms GRPO by 7.0% with 32.8% shorter reasoning chains, demonstrating the advantage of adaptive thinking mode selection and optimization mechanism in AMPO over GRPO’s fixed-depth solution. </p>
<blockquote>
<p>有效的社会智能模拟需要语言代理动态调整推理深度，而当前的研究中普遍缺乏这种能力。现有的方法要么不具备这种推理能力，要么在所有场景中强制实施长链思维推理，导致过度使用标记（tokens）和灵活度不足的社会模拟。针对这一问题，本文提出了自适应模式学习（AML）框架，旨在提高语言代理在动态社会交互中的自适应思维能力。为此，我们首先基于认知控制理论，识别了从直觉反应到深思熟虑的不同层次思维模式。然后，我们开发了自适应模式策略优化（AMPO）算法，以优化基于上下文的模式切换和推理。我们的框架在三个方面推动了现有研究：（1）多粒度思维模式设计，（2）社会交互中的基于上下文模式切换，（3）通过深度自适应处理实现高效的标记推理。在社会智能基准测试上的大量实验证明，AML的任务性能比GPT-4o高出15.6%。值得注意的是，我们的AMPO在性能上优于GRPO，任务完成度高出7.0%，且推理链缩短32.8%，这显示了AMPO中自适应思维模式选择和优化机制的优势，超越了GRPO的固定深度解决方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.02156v4">PDF</a> Work in Progress. The code and data are available, see   <a target="_blank" rel="noopener" href="https://github.com/MozerWang/AMPO">https://github.com/MozerWang/AMPO</a></p>
<p><strong>Summary</strong><br>基于现有研究中语言代理在动态社会交互中缺乏适应性思考能力的现状，本文提出了一个名为AML的适应模式学习框架，旨在提高语言代理在动态社会交互中的适应性思考能力。该框架通过设计多层次思考模式、实现语境感知模式切换和深度自适应处理，优化了语言代理的推理过程。实验证明，AML在社交智能基准测试中实现了优于GPT-4o的任务性能表现。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>语言代理在动态社会交互中需要动态调整推理深度。</li>
<li>现有研究缺乏适应性思考能力或统一采用长链思维，导致过度使用标记和不灵活的社会模拟。</li>
<li>AML框架旨在提高语言代理在动态社会交互中的适应性思考能力。</li>
<li>AML框架包括设计多层次思考模式、实现语境感知模式切换和优化深度自适应处理。</li>
<li>AMPO算法用于优化语境感知模式切换和推理过程。</li>
<li>实验证明AML在社交智能基准测试中表现优于GPT-4o和GRPO。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.02156">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-077873a343d7d07be868e8e57fda7fe5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-291ddb362bcda3f5a0abf41c6a238b40.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca1dc801b044cbe3ad0e1ecdb6ed5d81.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0977122f20978879832f4d7f7136cd47.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Agentic-AI-Software-Engineers-Programming-with-Trust"><a href="#Agentic-AI-Software-Engineers-Programming-with-Trust" class="headerlink" title="Agentic AI Software Engineers: Programming with Trust"></a>Agentic AI Software Engineers: Programming with Trust</h2><p><strong>Authors:Abhik Roychoudhury, Corina Pasareanu, Michael Pradel, Baishakhi Ray</strong></p>
<p>Large Language Models (LLMs) have shown surprising proficiency in generating code snippets, promising to automate large parts of software engineering via artificial intelligence (AI). We argue that successfully deploying AI software engineers requires a level of trust equal to or even greater than the trust established by human-driven software engineering practices. The recent trend toward LLM agents offers a path toward integrating the power of LLMs to create new code with the power of analysis tools to increase trust in the code. This opinion piece comments on whether LLM agents could dominate software engineering workflows in the future and whether the focus of programming will shift from programming at scale to programming with trust. </p>
<blockquote>
<p>大型语言模型（LLMs）在生成代码片段方面表现出了惊人的熟练程度，有望通过人工智能（AI）自动化软件工程的很大一部分工作。我们主张，成功部署AI软件工程师需要建立与人类驱动的软件工程实践相当的甚至更高的信任度。最近对LLM代理的趋势提供了一个将LLMs的力量与分析工具的力量相结合来创建新代码的路径，以增加对代码的信任。本文作者认为，未来的软件工作流程中LLM代理是否会占据主导地位，编程的重点是否会从规模化编程转向可信编程，都是值得探讨的问题。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13767v3">PDF</a> 5 pages</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLMs）在生成代码片段方面表现出惊人的能力，预示着人工智能（AI）可能自动化软件工程的很大一部分工作。成功部署AI软件工程师需要建立与人类驱动的软件工程实践相当的信任水平，甚至更高。最近出现的LLM代理趋势为整合LLMs生成新代码的能力与分析工具增加代码信任的能力提供了途径。本评论文章探讨了LLM代理是否可能在未来主导软件工程工作流程，以及编程的焦点是否会从大规模编程转向具有信任的编程。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）能生成代码片段，预示AI在软件工程中的潜在作用。</li>
<li>成功部署AI软件工程师需要建立高信任度，与人类驱动的软件工程实践相当。</li>
<li>LLM代理的出现为整合LLMs的能力和分析工具增加了对代码的信心。</li>
<li>LLM代理可能在未来主导软件工程工作流程。</li>
<li>编程的焦点可能会从大规模编程转向具有信任的编程。</li>
<li>需要进一步研究和探索如何在软件工程中平衡人工智能与人类角色的协作。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13767">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9e2f6486df327aa2ed0c9f02762b960e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c26067435299e2aa6d438ebe39d58f94.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="InSTA-Towards-Internet-Scale-Training-For-Agents"><a href="#InSTA-Towards-Internet-Scale-Training-For-Agents" class="headerlink" title="InSTA: Towards Internet-Scale Training For Agents"></a>InSTA: Towards Internet-Scale Training For Agents</h2><p><strong>Authors:Brandon Trabucco, Gunnar Sigurdsson, Robinson Piramuthu, Ruslan Salakhutdinov</strong></p>
<p>The predominant approach for training web navigation agents is to gather human demonstrations for a set of popular websites and hand-written tasks, but it is becoming clear that human data is an inefficient resource. We develop a pipeline to facilitate internet-scale training for agents without laborious human annotations. In the first stage, an LLM annotates 150k sites with agentic tasks. In the next stage, LLM agents complete tasks and produce trajectories. In the final stage, an LLM filters trajectories by judging their success. Language models are powerful data curation tools, identifying harmful content with an accuracy of 97%, judging successful trajectories with an accuracy of 82.6%, and producing effective data. We train agents based on Qwen 3 1.7B that are competitive with frontier LLMs as web agents, while being smaller and faster. Our top agent reaches a success rate of 56.9%, outperforming the data collection policy Qwen 3 235B, a 235 times larger Llama 4 Maverick, and reaching 94.7% of the performance of Gemini 2.5 Flash. We are releasing code, models and data at: <a target="_blank" rel="noopener" href="https://data-for-agents.github.io/">https://data-for-agents.github.io</a>. </p>
<blockquote>
<p>训练网络导航代理的主流方法是为流行网站和手写任务收集人类演示，但越来越明显的是，人类数据是一种低效的资源。我们开发了一个流程，以在没有繁琐的人类注释的情况下促进代理的互联网规模训练。在第一阶段，大型语言模型（LLM）为15万个网站注释了任务。在下一阶段，LLM代理完成任务并产生轨迹。在最后一个阶段，LLM通过判断其成功与否来过滤轨迹。语言模型是强大的数据整理工具，它们能准确地识别有害内容（准确率为97%），准确地判断成功的轨迹（准确率为82.6%），并能产生有效的数据。我们以Qwen 3 1.7B为基础训练代理，作为网络代理，我们的代理与前沿LLM具有竞争力，而且更小、更快。我们的顶级代理成功率达到56.9%，超过了数据采集策略Qwen 3 235B，一个体积大得多的Llama 4 Maverick，并达到了Gemini 2.5 Flash性能的94.7%。我们在以下网址发布代码、模型和数：<a target="_blank" rel="noopener" href="https://data-for-agents.github.io./">https://data-for-agents.github.io。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.06776v2">PDF</a> Improved results, zero-shot transfer to Web Voyager</p>
<p><strong>Summary</strong></p>
<p>基于LLM的管道训练网络导航代理，无需繁琐的人工注释。通过LLM标注网站、完成代理任务和过滤轨迹，成功训练出竞争前沿的小型快速网络导航代理。该代理性能优异，成功率高，且可识别有害内容。相关代码、模型和数据已发布在相关网站。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>使用LLM（大型语言模型）构建管道训练网络导航代理，提高了效率并降低了人工参与成本。</li>
<li>LLM用于标注网站、完成代理任务和过滤轨迹，简化了训练过程。</li>
<li>成功训练出性能优异、成功率高的网络导航代理，达到了前沿水平。</li>
<li>LLM能准确识别有害内容，展现了其在数据筛选方面的强大能力。</li>
<li>训练出的代理基于较小的模型（Qwen 3 1.7B），相较于大型模型（如Qwen 3 235B和Llama 4 Maverick）更为高效。</li>
<li>训练出的最佳代理性能达到了高成功率（56.9%），并接近Gemini 2.5 Flash的性能（达到其94.7%）。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.06776">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4f7d6b29e2f69812ea760cf2b2d90c49.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-08a8e1b475f9e0ea5b03c8b288beaf2b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8d1a2fb0609b56d6343a67ffccdd2f31.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-56d7b200d950159537a71c7ef454765e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ad08cdde230176a3106b4608e1facfa.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b9e41d2e928ec2e8f27ddec0fd22c46b.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="My-Words-Imply-Your-Opinion-Reader-Agent-based-Propagation-Enhancement-for-Personalized-Implicit-Emotion-Analysis"><a href="#My-Words-Imply-Your-Opinion-Reader-Agent-based-Propagation-Enhancement-for-Personalized-Implicit-Emotion-Analysis" class="headerlink" title="My Words Imply Your Opinion: Reader Agent-based Propagation Enhancement   for Personalized Implicit Emotion Analysis"></a>My Words Imply Your Opinion: Reader Agent-based Propagation Enhancement   for Personalized Implicit Emotion Analysis</h2><p><strong>Authors:Jian Liao, Yu Feng, Yujin Zheng, Jun Zhao, Suge Wang, Jianxing Zheng</strong></p>
<p>The subtlety of emotional expressions makes implicit emotion analysis (IEA) particularly sensitive to user-specific characteristics. Current studies personalize emotion analysis by focusing on the author but neglect the impact of the intended reader on implicit emotional feedback. In this paper, we introduce Personalized IEA (PIEA) and present the RAPPIE model, which addresses subjective variability by incorporating reader feedback. In particular, (1) we create reader agents based on large language models to simulate reader feedback, overcoming the issue of &#96;&#96;spiral of silence effect’’ and data incompleteness of real reader reaction. (2) We develop a role-aware multi-view graph learning to model the emotion interactive propagation process in scenarios with sparse reader information. (3) We construct two new PIEA datasets covering English and Chinese social media with detailed user metadata, addressing the text-centric limitation of existing datasets. Extensive experiments show that RAPPIE significantly outperforms state-of-the-art baselines, demonstrating the value of incorporating reader feedback in PIEA. </p>
<blockquote>
<p>情感表达的细微之处使得隐式情感分析（IEA）特别关注用户特定特征。当前的研究通过关注作者来实现情感分析的个性化，但忽视了目标读者对隐式情感反馈的影响。在本文中，我们介绍了个性化IEA（PIEA）并提出了RAPPIE模型，该模型通过融入读者反馈来解决主观变化性的问题。具体来说，（1）我们基于大型语言模型创建读者代理来模拟读者反馈，从而克服“沉默螺旋效应”和真实读者反应数据不完整的问题。（2）我们开发了一种角色感知多视图图学习，以在读者信息稀疏的场景中建立情感交互传播过程。 （3）我们构建了两个新的PIEA数据集，涵盖了英语和中文社交媒体，并包含详细的用户元数据，解决了现有数据集以文本为中心的局限性。大量实验表明，RAPPIE在融入读者反馈后显著超越了最新基线，表现出其优越性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07367v3">PDF</a> </p>
<p><strong>Summary</strong><br>文本提出个性化隐式情感分析（PIEA）的新概念，并介绍RAPPIE模型。该模型通过模拟读者反馈克服真实读者反应的“沉默螺旋效应”和数据不完整性问题，并构建读者代理来捕捉读者信息对情绪表达的影响。通过角色感知的多视图图学习，建模情感交互传播过程。同时，创建覆盖英文和中文社交媒体的新数据集，包含详细的用户元数据。实验证明RAPPIE模型显著优于现有基线模型，证明了在PIEA中融入读者反馈的价值。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文本强调情感表达的微妙性使得隐式情感分析（IEA）对用户特定特性特别敏感。</li>
<li>当前研究个性化情感分析主要集中在作者上，但忽略了预期读者对隐式情感反馈的影响。</li>
<li>引入个性化隐式情感分析（PIEA）和RAPPIE模型，通过模拟读者反馈解决主观变异性问题。</li>
<li>创建基于大型语言模型的读者代理来模拟读者反馈，克服真实读者反应的“沉默螺旋效应”和数据不完整性问题。</li>
<li>发展角色感知的多视图图学习，建模情感交互传播过程，以处理稀疏的读者信息场景。</li>
<li>构建涵盖英文和中文社交媒体的新PIEA数据集，包含详细的用户元数据，解决现有数据集文本为中心的限制。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.07367">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-c0fd07cdd7245cfc67c17c2af7aa26fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-109b6945ce470aeaf0ad1fd771202255.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="HyperMARL-Adaptive-Hypernetworks-for-Multi-Agent-RL"><a href="#HyperMARL-Adaptive-Hypernetworks-for-Multi-Agent-RL" class="headerlink" title="HyperMARL: Adaptive Hypernetworks for Multi-Agent RL"></a>HyperMARL: Adaptive Hypernetworks for Multi-Agent RL</h2><p><strong>Authors:Kale-ab Abebe Tessera, Arrasy Rahman, Amos Storkey, Stefano V. Albrecht</strong></p>
<p>Adaptability to specialised or homogeneous behaviours is critical in cooperative multi-agent reinforcement learning (MARL). Parameter sharing (PS) techniques, common for efficient adaptation, often limit behavioural diversity due to cross-agent gradient interference, which we show can be exacerbated by the coupling of observations and agent IDs. Current remedies typically add complexity through altered objectives, manual preset diversity levels, or sequential updates. We ask: can shared policies adapt without these complexities? We propose HyperMARL, a PS approach using hypernetworks for dynamic agent-specific parameters, without altering the RL objective or requiring preset diversity levels. HyperMARL’s explicit decoupling of observation- and agent-conditioned gradients empirically reduces policy gradient variance, facilitates shared-policy adaptation (including specialisation), and helps mitigate cross-agent interference. Across diverse MARL benchmarks (up to 20 agents), requiring homogeneous, heterogeneous, or mixed behaviours, HyperMARL achieves competitive performance against key baselines – fully shared, non-parameter sharing, and three diversity-promoting methods – while preserving behavioural diversity comparable to non-parameter sharing. These findings establish HyperMARL as a versatile approach for adaptive MARL. The code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/KaleabTessera/HyperMARL">https://github.com/KaleabTessera/HyperMARL</a>. </p>
<blockquote>
<p>适应特殊或同质行为在合作多智能体强化学习（MARL）中至关重要。参数共享（PS）技术，常用于高效适应，但由于跨智能体梯度干扰，通常会限制行为多样性，我们表明观察和智能体ID的耦合会加剧这一问题。当前的补救措施通常通过改变目标、手动预设多样性水平或顺序更新来增加复杂性。我们的问题是：共享政策能否在没有这些复杂性的情况下适应？我们提出了HyperMARL，这是一种使用超网络进行动态特定于智能体的参数共享方法，无需改变RL目标或要求预设多样性水平。HyperMARL对观察和智能体条件下的梯度进行显式解耦，这实际上减少了政策梯度方差，促进了共享政策的适应（包括专业化），并有助于缓解跨智能体干扰。在多种MARL基准测试（最多20个智能体）中，需要同质、异质或混合行为，HyperMARL在关键基准线上实现了竞争性能——完全共享、非参数共享和三种促进多样性的方法，同时保持与非参数共享相当的行为多样性。这些发现证明HyperMARL是适应性MARL的通用方法。代码公开在<a target="_blank" rel="noopener" href="https://github.com/KaleabTessera/HyperMARL">https://github.com/KaleabTessera/HyperMARL</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04233v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>适应特定或同质化行为在合作多智能体强化学习（MARL）中至关重要。共享参数（PS）技术因对自适应的适应效率而普遍应用，但往往限制了行为多样性，因为存在跨智能体梯度干扰问题。我们提出了一种名为HyperMARL的新方法，使用超网络进行动态智能体特定参数化，无需改变强化学习目标或预设多样性级别。HyperMARL能够降低策略梯度方差、适应共享策略、减少跨智能体干扰等。在多种MARL基准测试上，HyperMARL实现了与关键基线相比具有竞争力的性能，同时保持与无参数共享相近的行为多样性。这表明HyperMARL是适应性MARL的一种通用方法。代码公开于：<a target="_blank" rel="noopener" href="https://github.com/KaleabTessera/HyperMARL%E3%80%82">https://github.com/KaleabTessera/HyperMARL。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>适应特定或同质化行为在多智能体强化学习中的重要性。</li>
<li>参数共享技术在多智能体强化学习中可能限制行为多样性，导致跨智能体梯度干扰问题。</li>
<li>提出HyperMARL方法，使用超网络进行动态智能体特定参数化，无需改变强化学习目标或预设多样性级别。</li>
<li>HyperMARL能够降低策略梯度方差，促进共享策略适应，并有助于减少跨智能体干扰。</li>
<li>在多种MARL基准测试中，HyperMARL表现出与关键基线相比的竞争力，同时保持行为多样性。</li>
<li>HyperMARL在保持行为多样性的同时，实现了较高的性能，证明了其作为适应性MARL通用方法的潜力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.04233">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-8af7f6ca60b858bb92cba3dc14b5ff64.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b07aa129879ebdb8f07347ff23a1d768.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e1096ac781d0c00cd0fe40e1990ab84d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aef7bc7421006bd79b09ee58beeeb73d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a822e2a9e9ff28516561415e77521bb8.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="MLLM-as-Retriever-Interactively-Learning-Multimodal-Retrieval-for-Embodied-Agents"><a href="#MLLM-as-Retriever-Interactively-Learning-Multimodal-Retrieval-for-Embodied-Agents" class="headerlink" title="MLLM as Retriever: Interactively Learning Multimodal Retrieval for   Embodied Agents"></a>MLLM as Retriever: Interactively Learning Multimodal Retrieval for   Embodied Agents</h2><p><strong>Authors:Junpeng Yue, Xinrun Xu, Börje F. Karlsson, Zongqing Lu</strong></p>
<p>MLLM agents demonstrate potential for complex embodied tasks by retrieving multimodal task-relevant trajectory data. However, current retrieval methods primarily focus on surface-level similarities of textual or visual cues in trajectories, neglecting their effectiveness for the specific task at hand. To address this issue, we propose a novel method, MLLM As ReTriever (MART), which enhances the performance of embodied agents by utilizing interaction data to fine-tune an MLLM retriever based on preference learning, such that the retriever fully considers the effectiveness of trajectories and prioritizes them for unseen tasks. We also introduce Trajectory Abstraction, a mechanism that leverages MLLMs’ summarization capabilities to represent trajectories with fewer tokens while preserving key information, enabling agents to better comprehend milestones in the trajectory. Experimental results across various environments demonstrate our method significantly improves task success rates in unseen scenes compared to baseline methods. This work presents a new paradigm for multimodal retrieval in embodied agents, by fine-tuning a general-purpose MLLM as the retriever to assess trajectory effectiveness. All the code for benchmark tasks, simulator modifications, and the MLLM retriever is available at <a target="_blank" rel="noopener" href="https://github.com/PKU-RL/MART">https://github.com/PKU-RL/MART</a>. </p>
<blockquote>
<p>MLLM代理通过检索多模式任务相关轨迹数据，展现出执行复杂实体任务潜力。然而，当前检索方法主要关注轨迹中文字或视觉线索的表面相似性，忽视了它们在特定任务中的有效性。为了解决这一问题，我们提出了一种新方法，名为MLLM作为检索器（MART），通过利用交互数据对基于偏好学习的MLLM检索器进行微调，提高实体代理的性能，使检索器全面考虑轨迹的有效性并优先用于未见任务。我们还引入了轨迹抽象机制，该机制利用MLLM的总结能力以较少的标记表示轨迹，同时保留关键信息，使代理能够更好地理解轨迹中的里程碑。在多种环境下的实验结果表明，与基准方法相比，我们的方法在未见场景中的任务成功率显著提高。这项工作通过微调通用MLLM作为检索器来评估轨迹的有效性，为实体代理中的多模式检索提供了新的范例。所有基准任务、模拟器修改和MLLM检索器的代码可在<a target="_blank" rel="noopener" href="https://github.com/PKU-RL/">https://github.com/PKU-RL/</a> MART找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.03450v2">PDF</a> ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>MLLM代理在复杂的实体任务中展现出潜力，通过检索多模态任务相关轨迹数据来完成任务。然而，当前检索方法主要关注轨迹的文本或视觉线索的表面相似性，忽视了它们对特定任务的有效性。为此，我们提出了一种新方法MLLM As ReTriever（MART），通过利用交互数据对基于偏好学习的MLLM检索器进行微调，提高实体代理的性能，使检索器充分考虑轨迹的有效性并优先处理未见任务。我们还引入了轨迹抽象机制，利用MLLM的总结能力以更少的令牌表示轨迹，同时保留关键信息，使代理更好地理解轨迹中的里程碑。实验结果表明，与基准方法相比，我们的方法在未见场景中的任务成功率显著提高。本研究为实体代理中的多模态检索提供了新的范例，即通过微调通用MLLM作为检索器来评估轨迹的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLM代理在复杂实体任务中展现潜力，可通过检索多模态轨迹数据完成任务。</li>
<li>当前检索方法主要关注表面相似性，忽视轨迹对特定任务的有效性。</li>
<li>提出新方法MLLM As ReTriever（MART），利用交互数据微调MLLM检索器，使其充分考虑轨迹有效性并优先处理未见任务。</li>
<li>引入轨迹抽象机制，用更少令牌表示轨迹同时保留关键信息。</li>
<li>实验结果显示，与基准方法相比，MART在未见场景的任务成功率上显著提高。</li>
<li>该研究为实体代理中的多模态检索提供了新的范例。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.03450">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-bbbbad6b7764c8356caf6ff41a929024.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0c2dc96dff9b3dfe4e0a0db54745d55c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-87be4308ec094e70d8e89a59bd665c44.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6451cc204fc87ec59161ad833ebfa467.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-24/Agent/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-24/Agent/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Agent/">
                                    <span class="chip bg-color">Agent</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-24/MMT/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-cbbd765fafcd09f21f4b2375d4b1e5ce.jpg" class="responsive-img" alt="MMT">
                        
                        <span class="card-title">MMT</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            MMT 方向最新论文已更新，请持续关注 Update in 2025-05-24  Property Enhanced Instruction Tuning for Multi-task Molecule Generation   with Large Language Models
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-24
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/MMT/" class="post-category">
                                    MMT
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/MMT/">
                        <span class="chip bg-color">MMT</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-24/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-612c734a99dd1fec3a0ba7edf415c57a.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM 方向最新论文已更新，请持续关注 Update in 2025-05-24  Let Androids Dream of Electric Sheep A Human-like Image Implication   Understanding and Reasoning Framework
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-24
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">25243.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
