<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Vision Transformer">
    <meta name="description" content="Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-24  Self-Rewarding Large Vision-Language Models for Optimizing Prompts in   Text-to-Image Generation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Vision Transformer | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-0bafbf63fc12cfe8e4081236d48c3bc0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Vision Transformer</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Vision-Transformer/">
                                <span class="chip bg-color">Vision Transformer</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                Vision Transformer
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-24
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-04
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    6.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    28 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-24-æ›´æ–°"><a href="#2025-05-24-æ›´æ–°" class="headerlink" title="2025-05-24 æ›´æ–°"></a>2025-05-24 æ›´æ–°</h1><h2 id="Self-Rewarding-Large-Vision-Language-Models-for-Optimizing-Prompts-in-Text-to-Image-Generation"><a href="#Self-Rewarding-Large-Vision-Language-Models-for-Optimizing-Prompts-in-Text-to-Image-Generation" class="headerlink" title="Self-Rewarding Large Vision-Language Models for Optimizing Prompts in   Text-to-Image Generation"></a>Self-Rewarding Large Vision-Language Models for Optimizing Prompts in   Text-to-Image Generation</h2><p><strong>Authors:Hongji Yang, Yucheng Zhou, Wencheng Han, Jianbing Shen</strong></p>
<p>Text-to-image models are powerful for producing high-quality images based on given text prompts, but crafting these prompts often requires specialized vocabulary. To address this, existing methods train rewriting models with supervision from large amounts of manually annotated data and trained aesthetic assessment models. To alleviate the dependence on data scale for model training and the biases introduced by trained models, we propose a novel prompt optimization framework, designed to rephrase a simple user prompt into a sophisticated prompt to a text-to-image model. Specifically, we employ the large vision language models (LVLMs) as the solver to rewrite the user prompt, and concurrently, employ LVLMs as a reward model to score the aesthetics and alignment of the images generated by the optimized prompt. Instead of laborious human feedback, we exploit the prior knowledge of the LVLM to provide rewards, i.e., AI feedback. Simultaneously, the solver and the reward model are unified into one model and iterated in reinforcement learning to achieve self-improvement by giving a solution and judging itself. Results on two popular datasets demonstrate that our method outperforms other strong competitors. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹èƒ½å¤Ÿæ ¹æ®ç»™å®šçš„æ–‡æœ¬æç¤ºç”Ÿæˆé«˜è´¨é‡å›¾åƒï¼Œä½†åˆ¶ä½œè¿™äº›æç¤ºé€šå¸¸éœ€è¦ä¸“ä¸šè¯æ±‡ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•é€šè¿‡å¤§é‡æ‰‹åŠ¨æ³¨é‡Šæ•°æ®çš„ç›‘ç£æ¥è®­ç»ƒé‡å†™æ¨¡å‹ï¼Œå¹¶è®­ç»ƒå®¡ç¾è¯„ä¼°æ¨¡å‹ã€‚ä¸ºå‡è½»æ¨¡å‹è®­ç»ƒå¯¹æ•°æ®è§„æ¨¡çš„ä¾èµ–ä»¥åŠè®­ç»ƒæ¨¡å‹æ‰€å¸¦æ¥çš„åè§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æç¤ºä¼˜åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨å°†ç®€å•ç”¨æˆ·æç¤ºé‡è¿°ä¸ºå¤æ‚æç¤ºç»™æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é‡‡ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰ä½œä¸ºæ±‚è§£å™¨æ¥é‡å†™ç”¨æˆ·æç¤ºï¼ŒåŒæ—¶ï¼Œåˆ©ç”¨LVLMä½œä¸ºå¥–åŠ±æ¨¡å‹æ¥è¯„ä¼°ä¼˜åŒ–æç¤ºç”Ÿæˆçš„å›¾åƒçš„ç¾å­¦å’Œä¸€è‡´æ€§ã€‚æˆ‘ä»¬ä¸éœ€è¦ç¹ççš„äººå·¥åé¦ˆï¼Œè€Œæ˜¯åˆ©ç”¨LVLMçš„å…ˆéªŒçŸ¥è¯†æ¥æä¾›å¥–åŠ±ï¼Œå³AIåé¦ˆã€‚åŒæ—¶ï¼Œæ±‚è§£å™¨å’Œå¥–åŠ±æ¨¡å‹è¢«åˆå¹¶åˆ°ä¸€ä¸ªæ¨¡å‹ä¸­ï¼Œå¹¶åœ¨å¼ºåŒ–å­¦ä¹ ä¸­è¿›è¡Œè¿­ä»£ï¼Œé€šè¿‡ç»™å‡ºè§£å†³æ–¹æ¡ˆå¹¶è‡ªæˆ‘åˆ¤æ–­æ¥å®ç°è‡ªæˆ‘å®Œå–„ã€‚åœ¨ä¸¤ä¸ªæµè¡Œæ•°æ®é›†ä¸Šçš„ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºå…¶ä»–å¼ºå¤§çš„ç«äº‰å¯¹æ‰‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16763v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ–°é¢–çš„æç¤ºä¼˜åŒ–æ¡†æ¶ï¼Œç”¨äºå°†ç”¨æˆ·ç®€å•æç¤ºæ”¹å†™ä¸ºé’ˆå¯¹æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„å¤æ‚æç¤ºã€‚åˆ©ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰ä½œä¸ºæ±‚è§£å™¨è¿›è¡Œæç¤ºé‡å†™ï¼Œå¹¶åŒæ—¶ä½œä¸ºå¥–åŠ±æ¨¡å‹æ¥è¯„ä¼°ä¼˜åŒ–æç¤ºç”Ÿæˆçš„å›¾åƒçš„ç¾å­¦ç¨‹åº¦å’Œä¸€è‡´æ€§ã€‚è¯¥æ–¹æ³•åˆ©ç”¨LVLMçš„å…ˆéªŒçŸ¥è¯†æä¾›å¥–åŠ±ï¼Œå³AIåé¦ˆï¼Œå®ç°è‡ªæˆ‘æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹éœ€è¦ç‰¹å®šçš„è¯æ±‡æç¤ºæ¥ç”Ÿæˆé«˜è´¨é‡å›¾åƒã€‚</li>
<li>ç°æœ‰æ–¹æ³•é€šè¿‡å¤§é‡æ‰‹åŠ¨æ³¨é‡Šæ•°æ®è¿›è¡Œæ¨¡å‹è®­ç»ƒå’Œå¯¹ç¾å­¦è¯„ä¼°æ¨¡å‹çš„è®­ç»ƒæ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>æå‡ºä¸€ç§æ–°é¢–çš„æç¤ºä¼˜åŒ–æ¡†æ¶ï¼Œé€šè¿‡é‡å†™ç»™å®šçš„ç”¨æˆ·æç¤ºæ¥æ”¹è¿›æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>åˆ©ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰ä½œä¸ºæ±‚è§£å™¨å’Œå¥–åŠ±æ¨¡å‹ã€‚</li>
<li>å¥–åŠ±æ¨¡å‹åˆ©ç”¨LVLMçš„å…ˆéªŒçŸ¥è¯†æä¾›AIåé¦ˆï¼Œæ— éœ€ç¹ççš„äººå·¥åé¦ˆã€‚</li>
<li>æ±‚è§£å™¨å’Œå¥–åŠ±æ¨¡å‹è¢«é›†æˆåˆ°ä¸€ä¸ªæ¨¡å‹ä¸­ï¼Œå¹¶é€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›è¡Œè¿­ä»£å’Œè‡ªæˆ‘æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16763">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9efb676bb0531ba520e6c3b2b5dee81d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1fd2565597ade8502d99d485181906ec.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a40e699a8f7879ba258cbfd3a804a660.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Zero-Shot-Anomaly-Detection-in-Battery-Thermal-Images-Using-Visual-Question-Answering-with-Prior-Knowledge"><a href="#Zero-Shot-Anomaly-Detection-in-Battery-Thermal-Images-Using-Visual-Question-Answering-with-Prior-Knowledge" class="headerlink" title="Zero-Shot Anomaly Detection in Battery Thermal Images Using Visual   Question Answering with Prior Knowledge"></a>Zero-Shot Anomaly Detection in Battery Thermal Images Using Visual   Question Answering with Prior Knowledge</h2><p><strong>Authors:Marcella Astrid, Abdelrahman Shabayek, Djamila Aouada</strong></p>
<p>Batteries are essential for various applications, including electric vehicles and renewable energy storage, making safety and efficiency critical concerns. Anomaly detection in battery thermal images helps identify failures early, but traditional deep learning methods require extensive labeled data, which is difficult to obtain, especially for anomalies due to safety risks and high data collection costs. To overcome this, we explore zero-shot anomaly detection using Visual Question Answering (VQA) models, which leverage pretrained knowledge and textbased prompts to generalize across vision tasks. By incorporating prior knowledge of normal battery thermal behavior, we design prompts to detect anomalies without battery-specific training data. We evaluate three VQA models (ChatGPT-4o, LLaVa-13b, and BLIP-2) analyzing their robustness to prompt variations, repeated trials, and qualitative outputs. Despite the lack of finetuning on battery data, our approach demonstrates competitive performance compared to state-of-the-art models that are trained with the battery data. Our findings highlight the potential of VQA-based zero-shot learning for battery anomaly detection and suggest future directions for improving its effectiveness. </p>
<blockquote>
<p>ç”µæ± åœ¨å„ç§åº”ç”¨ä¸­éƒ½æ˜¯è‡³å…³é‡è¦çš„ï¼ŒåŒ…æ‹¬ç”µåŠ¨æ±½è½¦å’Œå¯å†ç”Ÿèƒ½æºå­˜å‚¨ï¼Œå› æ­¤å…¶å®‰å…¨æ€§å’Œæ•ˆç‡éƒ½è‡³å…³é‡è¦ã€‚ç”µæ± çƒ­å›¾åƒçš„å¼‚å¸¸æ£€æµ‹æœ‰åŠ©äºæ—©æœŸå‘ç°æ•…éšœï¼Œä½†ä¼ ç»Ÿçš„æ·±åº¦å­¦ä¹ æ–¹æ³•éœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®ï¼Œè¿™å¾ˆéš¾è·å¾—ï¼Œå°¤å…¶æ˜¯å¯¹äºå› å®‰å…¨é£é™©å’Œé«˜æ˜‚çš„æ•°æ®æ”¶é›†æˆæœ¬è€Œå¯¼è‡´çš„å¼‚å¸¸ã€‚ä¸ºäº†å…‹æœè¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¢ç´¢ä½¿ç”¨åŸºäºè§†è§‰é—®ç­”ï¼ˆVQAï¼‰æ¨¡å‹çš„é›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹ï¼Œè¯¥æ¨¡å‹åˆ©ç”¨é¢„è®­ç»ƒçŸ¥è¯†å’Œæ–‡æœ¬æç¤ºæ¥åœ¨è§†è§‰ä»»åŠ¡ä¸­è¿›è¡Œæ¨å¹¿ã€‚é€šè¿‡ç»“åˆç”µæ± æ­£å¸¸çƒ­è¡Œä¸ºçš„å…ˆéªŒçŸ¥è¯†ï¼Œæˆ‘ä»¬è®¾è®¡æç¤ºæ¥æ£€æµ‹å¼‚å¸¸ï¼Œè€Œæ— éœ€ç‰¹å®šçš„ç”µæ± è®­ç»ƒæ•°æ®ã€‚æˆ‘ä»¬è¯„ä¼°äº†ä¸‰ä¸ªVQAæ¨¡å‹ï¼ˆChatGPT-4oã€LLaVa-13bå’ŒBLIP-2ï¼‰ï¼Œåˆ†æå®ƒä»¬å¯¹æç¤ºå˜åŒ–ã€é‡å¤è¯•éªŒå’Œå®šæ€§è¾“å‡ºçš„ç¨³å¥æ€§ã€‚å°½ç®¡æ²¡æœ‰åœ¨ç”µæ± æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ä½¿ç”¨ç”µæ± æ•°æ®è®­ç»ƒçš„æœ€æ–°æ¨¡å‹ç›¸æ¯”ï¼Œè¡¨ç°å…·æœ‰ç«äº‰åŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶çªå‡ºäº†åŸºäºVQAçš„é›¶æ ·æœ¬å­¦ä¹ åœ¨ç”µæ± å¼‚å¸¸æ£€æµ‹ä¸­çš„æ½œåŠ›ï¼Œå¹¶ä¸ºæé«˜å…¶æœ‰æ•ˆæ€§æä¾›äº†æœªæ¥ç ”ç©¶æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16674v1">PDF</a> Accepted in EUSIPCO 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢ç´¢äº†åˆ©ç”¨è§†è§‰é—®ç­”ï¼ˆVQAï¼‰æ¨¡å‹è¿›è¡Œé›¶æ ·æœ¬ç”µæ± å¼‚å¸¸æ£€æµ‹çš„æ–¹æ³•ã€‚é€šè¿‡ç»“åˆç”µæ± æ­£å¸¸çƒ­è¡Œä¸ºçš„å…ˆéªŒçŸ¥è¯†ï¼Œè®¾è®¡æ–‡æœ¬æç¤ºæ¥æ£€æµ‹å¼‚å¸¸ï¼Œæ— éœ€ç‰¹å®šçš„ç”µæ± è®­ç»ƒæ•°æ®ã€‚è¯„ä¼°äº†ä¸‰ç§VQAæ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶å±•ç¤ºäº†å…¶ç›¸è¾ƒäºä½¿ç”¨ç”µæ± æ•°æ®è®­ç»ƒçš„å…ˆè¿›æ¨¡å‹ä»å…·æœ‰ç«äº‰åŠ›çš„è¡¨ç°ã€‚æ­¤ç ”ç©¶çªæ˜¾äº†VQAæ¨¡å‹åœ¨ç”µæ± å¼‚å¸¸æ£€æµ‹ä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”µæ± åœ¨ç”µåŠ¨æ±½è½¦å’Œå¯å†ç”Ÿèƒ½æºå­˜å‚¨ç­‰é¢†åŸŸæœ‰å¹¿æ³›åº”ç”¨ï¼Œå› æ­¤å…¶å®‰å…¨æ€§å’Œæ•ˆç‡è‡³å…³é‡è¦ã€‚</li>
<li>ä¼ ç»Ÿæ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨ç”µæ± çƒ­å›¾åƒå¼‚å¸¸æ£€æµ‹ä¸­éœ€å¤§é‡æ ‡æ³¨æ•°æ®ï¼Œä½†è·å–è¿™äº›æ•°æ®å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºè§†è§‰é—®ç­”ï¼ˆVQAï¼‰æ¨¡å‹çš„é›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹æ–¹æ³•ï¼Œæ— éœ€ç‰¹å®šçš„ç”µæ± è®­ç»ƒæ•°æ®ã€‚</li>
<li>é€šè¿‡ç»“åˆç”µæ± æ­£å¸¸çƒ­è¡Œä¸ºçš„å…ˆéªŒçŸ¥è¯†ï¼Œè®¾è®¡æ–‡æœ¬æç¤ºæ¥æ£€æµ‹å¼‚å¸¸ã€‚</li>
<li>è¯„ä¼°äº†ä¸‰ç§VQAæ¨¡å‹çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬ChatGPT-4oã€LLaVa-13bå’ŒBLIP-2ã€‚</li>
<li>è¿™äº›æ¨¡å‹åœ¨ç¼ºä¹ç”µæ± æ•°æ®å¾®è°ƒçš„æƒ…å†µä¸‹ä»è¡¨ç°å‡ºç«äº‰åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16674">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3f9ae25460b516da99771bdfc378f5c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b342965b50d33a15f89f3ad0b8587ca.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4ae92f7c2ef430e378f1452659a56537.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-35616d0288e11a286bc6fc7809350614.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Point-Detect-Count-Multi-Task-Medical-Image-Understanding-with-Instruction-Tuned-Vision-Language-Models"><a href="#Point-Detect-Count-Multi-Task-Medical-Image-Understanding-with-Instruction-Tuned-Vision-Language-Models" class="headerlink" title="Point, Detect, Count: Multi-Task Medical Image Understanding with   Instruction-Tuned Vision-Language Models"></a>Point, Detect, Count: Multi-Task Medical Image Understanding with   Instruction-Tuned Vision-Language Models</h2><p><strong>Authors:Sushant Gautam, Michael A. Riegler, PÃ¥l Halvorsen</strong></p>
<p>We investigate fine-tuning Vision-Language Models (VLMs) for multi-task medical image understanding, focusing on detection, localization, and counting of findings in medical images. Our objective is to evaluate whether instruction-tuned VLMs can simultaneously improve these tasks, with the goal of enhancing diagnostic accuracy and efficiency. Using MedMultiPoints, a multimodal dataset with annotations from endoscopy (polyps and instruments) and microscopy (sperm cells), we reformulate each task into instruction-based prompts suitable for vision-language reasoning. We fine-tune Qwen2.5-VL-7B-Instruct using Low-Rank Adaptation (LoRA) across multiple task combinations. Results show that multi-task training improves robustness and accuracy. For example, it reduces the Count Mean Absolute Error (MAE) and increases Matching Accuracy in the Counting + Pointing task. However, trade-offs emerge, such as more zero-case point predictions, indicating reduced reliability in edge cases despite overall performance gains. Our study highlights the potential of adapting general-purpose VLMs to specialized medical tasks via prompt-driven fine-tuning. This approach mirrors clinical workflows, where radiologists simultaneously localize, count, and describe findings - demonstrating how VLMs can learn composite diagnostic reasoning patterns. The model produces interpretable, structured outputs, offering a promising step toward explainable and versatile medical AI. Code, model weights, and scripts will be released for reproducibility at <a target="_blank" rel="noopener" href="https://github.com/simula/PointDetectCount">https://github.com/simula/PointDetectCount</a>. </p>
<blockquote>
<p>æˆ‘ä»¬ç ”ç©¶äº†é’ˆå¯¹å¤šä»»åŠ¡åŒ»å­¦å½±åƒç†è§£çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å¾®è°ƒæŠ€æœ¯ï¼Œé‡ç‚¹å…³æ³¨åŒ»å­¦å½±åƒä¸­çš„æ£€æµ‹ç»“æœè¯†åˆ«ã€å®šä½ä»¥åŠè®¡æ•°ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯è¯„ä¼°æŒ‡ä»¤è°ƒæ•´å‹VLMæ˜¯å¦èƒ½åŒæ—¶æ”¹å–„è¿™äº›ä»»åŠ¡ï¼Œä»¥æé«˜è¯Šæ–­çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚æˆ‘ä»¬ä½¿ç”¨MedMultiPointsæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«å†…çª¥é•œï¼ˆæ¯è‚‰å’Œä»ªå™¨ï¼‰å’Œæ˜¾å¾®é•œï¼ˆç²¾å­ç»†èƒï¼‰çš„æ³¨é‡Šï¼Œæˆ‘ä»¬å°†æ¯ä¸ªä»»åŠ¡é‡æ–°åˆ¶å®šä¸ºé€‚åˆè§†è§‰è¯­è¨€æ¨ç†çš„æŒ‡ä»¤æç¤ºã€‚æˆ‘ä»¬ä½¿ç”¨ä½ç§©è‡ªé€‚åº”ï¼ˆLoRAï¼‰å¯¹Qwen2.5-VL-7B-Instructè¿›è¡Œå¤šä»»åŠ¡ç»„åˆçš„å¾®è°ƒã€‚ç»“æœè¡¨æ˜ï¼Œå¤šä»»åŠ¡è®­ç»ƒæé«˜äº†ç¨³å¥æ€§å’Œå‡†ç¡®æ€§ã€‚ä¾‹å¦‚ï¼Œå®ƒå‡å°‘äº†è®¡æ•°å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰ï¼Œå¹¶æé«˜äº†è®¡æ•°+å®šä½ä»»åŠ¡çš„åŒ¹é…å‡†ç¡®ç‡ã€‚ç„¶è€Œï¼Œä¹Ÿå‡ºç°äº†æƒè¡¡æƒ…å†µï¼Œä¾‹å¦‚æ›´å¤šçš„é›¶å€¼ç‚¹é¢„æµ‹ï¼Œè¿™è¡¨æ˜åœ¨æ•´ä½“æ€§èƒ½æå‡çš„åŒæ—¶ï¼Œç‰¹å®šæƒ…å†µä¸‹çš„å¯é æ€§æœ‰æ‰€é™ä½ã€‚æˆ‘ä»¬çš„ç ”ç©¶çªå‡ºäº†é€šè¿‡æç¤ºé©±åŠ¨å¾®è°ƒå°†é€šç”¨VLMsé€‚åº”ä¸“ä¸šåŒ»ç–—ä»»åŠ¡çš„æ½œåŠ›ã€‚è¿™ç§æ–¹æ³•åæ˜ äº†ä¸´åºŠå·¥ä½œæµç¨‹ï¼Œæ”¾å°„ç§‘åŒ»ç”ŸåŒæ—¶å®šä½ã€è®¡æ•°å’Œæè¿°å‘ç°ç»“æœï¼Œå±•ç¤ºäº†VLMså¦‚ä½•å­¦ä¹ å¤åˆè¯Šæ–­æ¨ç†æ¨¡å¼ã€‚è¯¥æ¨¡å‹äº§ç”Ÿå¯è§£é‡Šçš„ç»“æ„åŒ–è¾“å‡ºï¼Œæœç€å¯è§£é‡Šå’Œå¤šåŠŸèƒ½åŒ»ç–—AIè¿ˆå‡ºäº†æœ‰å‰æ™¯çš„ä¸€æ­¥ã€‚ä»£ç ã€æ¨¡å‹æƒé‡å’Œè„šæœ¬å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/simula/PointDetectCount%E4%B8%8A%E5%8F%91%E5%B8%83%EF%BC%8C%E4%BB%A5%E4%BE%9B%E5%A4%8D%E5%88%B6%E5%92%8C%E9%87%8D%E7%8E%B0%E3%80%82">https://github.com/simula/PointDetectCountä¸Šå‘å¸ƒï¼Œä»¥ä¾›å¤åˆ¶å’Œé‡ç°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16647v1">PDF</a> Accepted as a full paper at the 38th IEEE International Symposium on   Computer-Based Medical Systems (CBMS) 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†é’ˆå¯¹å¤šä»»åŠ¡åŒ»ç–—å›¾åƒç†è§£çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„å¾®è°ƒæŠ€æœ¯ã€‚æ–‡ç« æ—¨åœ¨è¯„ä¼°æŒ‡ä»¤å¾®è°ƒVLMsæ˜¯å¦èƒ½åŒæ—¶æ”¹è¿›æ£€æµ‹ã€å®šä½å’Œè®¡æ•°ä»»åŠ¡ï¼Œä»¥æé«˜è¯Šæ–­å’Œæ•ˆç‡çš„å‡†ç¡®æ€§ã€‚ä½¿ç”¨MedMultiPointså¤šæ¨¡æ€æ•°æ®é›†ï¼Œå°†å„é¡¹ä»»åŠ¡é‡æ–°åˆ¶å®šä¸ºé€‚åº”è§†è§‰è¯­è¨€æ¨ç†çš„æŒ‡ä»¤åŸºç¡€æç¤ºã€‚é€šè¿‡ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰æŠ€æœ¯å¾®è°ƒQwen2.5-VL-7B-Instructæ¨¡å‹è¿›è¡Œå¤šä»»åŠ¡ç»„åˆè®­ç»ƒã€‚ç»“æœæ˜¾ç¤ºï¼Œå¤šä»»åŠ¡è®­ç»ƒèƒ½æé«˜ç¨³å¥æ€§å’Œå‡†ç¡®æ€§ï¼Œå¦‚å‡å°‘è®¡æ•°å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰å¹¶æé«˜åŒ¹é…å‡†ç¡®ç‡ã€‚ç„¶è€Œï¼Œä»å­˜åœ¨æƒè¡¡ï¼Œä¾‹å¦‚æ›´å¤šçš„é›¶æ¡ˆä¾‹ç‚¹é¢„æµ‹ï¼Œè¡¨æ˜åœ¨è¾¹ç¼˜æƒ…å†µä¸‹å¯é æ€§é™ä½ã€‚ç ”ç©¶çªå‡ºäº†é€šè¿‡æç¤ºé©±åŠ¨å¾®è°ƒé€šç”¨VLMsåˆ°ä¸“ä¸šåŒ»ç–—ä»»åŠ¡çš„æ½œåŠ›ã€‚è¯¥æ¨¡å‹äº§ç”Ÿå¯è§£é‡Šçš„ç»“æ„åŒ–è¾“å‡ºï¼Œä¸ºåŒ»ç–—äººå·¥æ™ºèƒ½çš„æ™®åŠå’Œè§£é‡Šæ€§è¿ˆå‡ºäº†æœ‰å‰æ™¯çš„ä¸€æ­¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤šä»»åŠ¡åŒ»ç–—å›¾åƒç†è§£ä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯æ£€æµ‹ã€å®šä½å’Œè®¡æ•°ä»»åŠ¡ã€‚</li>
<li>è¯„ä¼°äº†æŒ‡ä»¤å¾®è°ƒæ˜¯å¦èƒ½æ”¹å–„è¿™äº›ä»»åŠ¡ï¼Œæ—¨åœ¨æé«˜è¯Šæ–­å’Œæ•ˆç‡çš„å‡†ç¡®æ€§ã€‚</li>
<li>ä½¿ç”¨MedMultiPointså¤šæ¨¡æ€æ•°æ®é›†è¿›è¡Œå®éªŒç ”ç©¶ï¼Œè¯¥æ•°æ®é›†åŒ…å«å†…é•œå’Œæ˜¾å¾®é•œå›¾åƒçš„æ³¨è§£ã€‚</li>
<li>é‡‡ç”¨ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰æŠ€æœ¯å¯¹Qwen2.5-VL-7B-Instructæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå®ç°å¤šä»»åŠ¡è®­ç»ƒã€‚</li>
<li>å¤šä»»åŠ¡è®­ç»ƒæé«˜äº†æ¨¡å‹çš„ç¨³å¥æ€§å’Œå‡†ç¡®æ€§ï¼Œè¡¨ç°åœ¨è®¡æ•°ä»»åŠ¡çš„å¹³å‡ç»å¯¹è¯¯å·®é™ä½å’ŒåŒ¹é…å‡†ç¡®ç‡çš„æé«˜ã€‚</li>
<li>å­˜åœ¨æƒè¡¡é—®é¢˜ï¼Œå¦‚è¾¹ç¼˜æƒ…å†µä¸‹çš„å¯é æ€§æœ‰å¾…æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16647">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bb2c99815c192704a3f055f58582fd18.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3aec9cd89c3d9eff3027bd6f9c0b4113.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4147ce1c6eedaad57f24d207c7349a2f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5d18b3daedcd78a2d97fcf57b8b6107a.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="UWSAM-Segment-Anything-Model-Guided-Underwater-Instance-Segmentation-and-A-Large-scale-Benchmark-Dataset"><a href="#UWSAM-Segment-Anything-Model-Guided-Underwater-Instance-Segmentation-and-A-Large-scale-Benchmark-Dataset" class="headerlink" title="UWSAM: Segment Anything Model Guided Underwater Instance Segmentation   and A Large-scale Benchmark Dataset"></a>UWSAM: Segment Anything Model Guided Underwater Instance Segmentation   and A Large-scale Benchmark Dataset</h2><p><strong>Authors:Hua Li, Shijie Lian, Zhiyuan Li, Runmin Cong, Sam Kwong</strong></p>
<p>With recent breakthroughs in large-scale modeling, the Segment Anything Model (SAM) has demonstrated significant potential in a variety of visual applications. However, due to the lack of underwater domain expertise, SAM and its variants face performance limitations in end-to-end underwater instance segmentation tasks, while their higher computational requirements further hinder their application in underwater scenarios. To address this challenge, we propose a large-scale underwater instance segmentation dataset, UIIS10K, which includes 10,048 images with pixel-level annotations for 10 categories. Then, we introduce UWSAM, an efficient model designed for automatic and accurate segmentation of underwater instances. UWSAM efficiently distills knowledge from the SAM ViT-Huge image encoder into the smaller ViT-Small image encoder via the Mask GAT-based Underwater Knowledge Distillation (MG-UKD) method for effective visual representation learning. Furthermore, we design an End-to-end Underwater Prompt Generator (EUPG) for UWSAM, which automatically generates underwater prompts instead of explicitly providing foreground points or boxes as prompts, thus enabling the network to locate underwater instances accurately for efficient segmentation. Comprehensive experimental results show that our model is effective, achieving significant performance improvements over state-of-the-art methods on multiple underwater instance datasets. Datasets and codes are available at <a target="_blank" rel="noopener" href="https://github.com/LiamLian0727/UIIS10K">https://github.com/LiamLian0727/UIIS10K</a>. </p>
<blockquote>
<p>éšç€å¤§è§„æ¨¡å»ºæ¨¡æŠ€æœ¯çš„æœ€æ–°çªç ´ï¼ŒSegment Anything Modelï¼ˆSAMï¼‰åœ¨å„ç§è§†è§‰åº”ç”¨ä¸­è¡¨ç°å‡ºäº†å·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹æ°´ä¸‹é¢†åŸŸçš„ä¸“ä¸šçŸ¥è¯†ï¼ŒSAMåŠå…¶å˜ä½“åœ¨ç«¯åˆ°ç«¯çš„æ°´ä¸‹å®ä¾‹åˆ†å‰²ä»»åŠ¡ä¸­é¢ä¸´æ€§èƒ½å±€é™ï¼Œè€Œå®ƒä»¬è¾ƒé«˜çš„è®¡ç®—è¦æ±‚è¿›ä¸€æ­¥é˜»ç¢äº†åœ¨æ°´ä¸‹åœºæ™¯ä¸­çš„åº”ç”¨ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„æ°´ä¸‹å®ä¾‹åˆ†å‰²æ•°æ®é›†UIIS10Kï¼Œå…¶ä¸­åŒ…æ‹¬10,048å¼ å¸¦æœ‰10ç±»åƒç´ çº§æ³¨é‡Šçš„å›¾åƒã€‚æ¥ç€ï¼Œæˆ‘ä»¬ä»‹ç»äº†UWSAMï¼Œè¿™æ˜¯ä¸€ä¸ªä¸ºæ°´ä¸‹å®ä¾‹è‡ªåŠ¨å‡†ç¡®åˆ†å‰²è€Œè®¾è®¡çš„é«˜æ•ˆæ¨¡å‹ã€‚UWSAMé€šè¿‡åŸºäºMask GATçš„æ°´ä¸‹çŸ¥è¯†è’¸é¦ï¼ˆMG-UKDï¼‰æ–¹æ³•ï¼Œæœ‰æ•ˆåœ°ä»SAMçš„ViT-Hugeå›¾åƒç¼–ç å™¨è’¸é¦çŸ¥è¯†åˆ°è¾ƒå°çš„ViT-Smallå›¾åƒç¼–ç å™¨ï¼Œä»è€Œå®ç°æœ‰æ•ˆçš„è§†è§‰è¡¨ç¤ºå­¦ä¹ ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä¸ºUWSAMè®¾è®¡äº†ç«¯åˆ°ç«¯çš„æ°´ä¸‹æç¤ºç”Ÿæˆå™¨ï¼ˆEUPGï¼‰ï¼Œå®ƒä¼šè‡ªåŠ¨ç”Ÿæˆæ°´ä¸‹æç¤ºï¼Œè€Œä¸æ˜¯æ˜¾å¼æä¾›å‰æ™¯ç‚¹æˆ–æ¡†ä½œä¸ºæç¤ºï¼Œä»è€Œä½¿ç½‘ç»œèƒ½å¤Ÿå‡†ç¡®å®šä½æ°´ä¸‹å®ä¾‹ï¼Œå®ç°é«˜æ•ˆåˆ†å‰²ã€‚ç»¼åˆå®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹æ˜¯æœ‰æ•ˆçš„ï¼Œåœ¨å¤šä¸ªæ°´ä¸‹å®ä¾‹æ•°æ®é›†ä¸Šå®ç°äº†å¯¹æœ€å…ˆè¿›æ–¹æ³•çš„æ˜¾è‘—æ€§èƒ½æ”¹è¿›ã€‚æ•°æ®é›†å’Œä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/LiamLian0727/UIIS10K%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/LiamLian0727/UIIS10Kè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15581v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>é’ˆå¯¹å¤§å‹æ¨¡å‹çš„æ–°çªç ´ï¼ŒSegment Anything Modelï¼ˆSAMï¼‰åœ¨å„ç§è§†è§‰åº”ç”¨ä¸­å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹æ°´ä¸‹é¢†åŸŸçš„ä¸“ä¸šçŸ¥è¯†ï¼ŒSAMåŠå…¶å˜ä½“åœ¨ç«¯åˆ°ç«¯çš„æ°´ä¸‹å®ä¾‹åˆ†å‰²ä»»åŠ¡ä¸­æ€§èƒ½å—é™ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†å¤§è§„æ¨¡æ°´ä¸‹å®ä¾‹åˆ†å‰²æ•°æ®é›†UIIS10Kï¼ŒåŒ…å«10,048å¼ å¸¦æœ‰åƒç´ çº§æ³¨é‡Šçš„10ç±»å›¾åƒã€‚åŒæ—¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸“ä¸ºæ°´ä¸‹å®ä¾‹è‡ªåŠ¨ç²¾ç¡®åˆ†å‰²è€Œè®¾è®¡çš„UWSAMæ¨¡å‹ã€‚UWSAMé€šè¿‡Mask GATåŸºç¡€ä¸Šçš„æ°´ä¸‹çŸ¥è¯†è’¸é¦ï¼ˆMG-UKDï¼‰æ–¹æ³•ï¼Œæœ‰æ•ˆåœ°ä»SAMçš„ViT-Hugeå›¾åƒç¼–ç å™¨æç‚¼çŸ¥è¯†ï¼Œåº”ç”¨äºæ›´å°çš„ViT-Smallå›¾åƒç¼–ç å™¨ï¼Œå®ç°æœ‰æ•ˆçš„è§†è§‰è¡¨å¾å­¦ä¹ ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä¸ºUWSAMè®¾è®¡äº†ç«¯åˆ°ç«¯çš„æ°´ä¸‹æç¤ºç”Ÿæˆå™¨ï¼ˆEUPGï¼‰ï¼Œèƒ½å¤Ÿè‡ªåŠ¨ç”Ÿæˆæ°´ä¸‹æç¤ºï¼Œè€Œæ— éœ€æ˜ç¡®æä¾›å‰æ™¯ç‚¹æˆ–æ¡†ä½œä¸ºæç¤ºï¼Œä½¿ç½‘ç»œèƒ½å¤Ÿå‡†ç¡®å®šä½æ°´ä¸‹å®ä¾‹ï¼Œå®ç°é«˜æ•ˆåˆ†å‰²ã€‚å®éªŒç»“æœè¯æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å¤šä¸ªæ°´ä¸‹å®ä¾‹æ•°æ®é›†ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Segment Anything Modelï¼ˆSAMï¼‰åœ¨è§†è§‰åº”ç”¨ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨æ°´ä¸‹å®ä¾‹åˆ†å‰²ä»»åŠ¡ä¸­å› ç¼ºä¹ä¸“ä¸šé¢†åŸŸçŸ¥è¯†è€Œå—é™ã€‚</li>
<li>æå‡ºäº†å¤§è§„æ¨¡æ°´ä¸‹å®ä¾‹åˆ†å‰²æ•°æ®é›†UIIS10Kï¼ŒåŒ…å«å¸¦åƒç´ çº§æ³¨é‡Šçš„10,048å¼ å›¾åƒã€‚</li>
<li>å¼•å…¥äº†UWSAMæ¨¡å‹ï¼Œä¸“ä¸ºæ°´ä¸‹å®ä¾‹è‡ªåŠ¨ç²¾ç¡®åˆ†å‰²è®¾è®¡ã€‚</li>
<li>UWSAMé€šè¿‡Mask GATåŸºç¡€ä¸Šçš„æ°´ä¸‹çŸ¥è¯†è’¸é¦ï¼ˆMG-UKDï¼‰æå‡è§†è§‰è¡¨å¾å­¦ä¹ æ•ˆæœã€‚</li>
<li>æå‡ºäº†ç«¯åˆ°ç«¯çš„æ°´ä¸‹æç¤ºç”Ÿæˆå™¨ï¼ˆEUPGï¼‰ï¼Œèƒ½å¤Ÿè‡ªåŠ¨ç”Ÿæˆæ°´ä¸‹æç¤ºï¼Œæé«˜ç½‘ç»œå®šä½æ°´ä¸‹å®ä¾‹çš„å‡†ç¡®æ€§ã€‚</li>
<li>UWSAMåœ¨å¤šä¸ªæ°´ä¸‹å®ä¾‹æ•°æ®é›†ä¸Šçš„æ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>æ•°æ®é›†å’Œä»£ç å·²å…¬å¼€ï¼Œä¾¿äºç ”ç©¶ä½¿ç”¨å’Œè¿›ä¸€æ­¥å¼€å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15581">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-441b667b86a33fc92d23a041a8a08c93.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7cfcae1f57425fd2c7796fb24446890d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-97b69e92a6d1df639f352876f8816377.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-417104f48ac35af205379c25eeebe73f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f7eb65a293f4df3f6e76989761167ba.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-63adc646e52d8a12a7a38d4be641ea53.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-81a618ddb8d199d20ce84e093b867ae6.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="MedBLIP-Fine-tuning-BLIP-for-Medical-Image-Captioning"><a href="#MedBLIP-Fine-tuning-BLIP-for-Medical-Image-Captioning" class="headerlink" title="MedBLIP: Fine-tuning BLIP for Medical Image Captioning"></a>MedBLIP: Fine-tuning BLIP for Medical Image Captioning</h2><p><strong>Authors:Manshi Limbu, Diwita Banerjee</strong></p>
<p>Medical image captioning is a challenging task that requires generating clinically accurate and semantically meaningful descriptions of radiology images. While recent vision-language models (VLMs) such as BLIP, BLIP2, Gemini and ViT-GPT2 show strong performance on natural image datasets, they often produce generic or imprecise captions when applied to specialized medical domains. In this project, we explore the effectiveness of fine-tuning the BLIP model on the ROCO dataset for improved radiology captioning. We compare the fine-tuned BLIP against its zero-shot version, BLIP-2 base, BLIP-2 Instruct and a ViT-GPT2 transformer baseline. Our results demonstrate that domain-specific fine-tuning on BLIP significantly improves performance across both quantitative and qualitative evaluation metrics. We also visualize decoder cross-attention maps to assess interpretability and conduct an ablation study to evaluate the contributions of encoder-only and decoder-only fine-tuning. Our findings highlight the importance of targeted adaptation for medical applications and suggest that decoder-only fine-tuning (encoder-frozen) offers a strong performance baseline with 5% lower training time than full fine-tuning, while full model fine-tuning still yields the best results overall. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒæè¿°æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œéœ€è¦ç”Ÿæˆä¸´åºŠå‡†ç¡®ä¸”è¯­ä¹‰æ˜ç¡®çš„æ”¾å°„å­¦å›¾åƒæè¿°ã€‚è™½ç„¶æœ€è¿‘çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ï¼Œå¦‚BLIPã€BLIP2ã€Geminiå’ŒViT-GPT2åœ¨è‡ªç„¶å›¾åƒæ•°æ®é›†ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†å®ƒä»¬åœ¨åº”ç”¨äºä¸“ä¸šåŒ»å­¦é¢†åŸŸæ—¶å¾€å¾€ä¼šäº§ç”Ÿé€šç”¨æˆ–ä¸ç²¾ç¡®çš„æ ‡é¢˜ã€‚åœ¨è¿™ä¸ªé¡¹ç›®ä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†å¯¹BLIPæ¨¡å‹è¿›è¡Œå¾®è°ƒä»¥æ”¹è¿›æ”¾å°„å­¦æè¿°çš„ROCOæ•°æ®é›†çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬å°†å¾®è°ƒåçš„BLIPä¸å…¶é›¶æ ·æœ¬ç‰ˆæœ¬ã€BLIP-2åŸºç¡€ç‰ˆã€BLIP-2æŒ‡ä»¤ç‰ˆä»¥åŠViT-GPT2è½¬æ¢å™¨åŸºçº¿è¿›è¡Œäº†æ¯”è¾ƒã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œåœ¨BLIPä¸Šè¿›è¡Œç‰¹å®šé¢†åŸŸçš„å¾®è°ƒæ˜¾è‘—æé«˜äº†å®šé‡å’Œå®šæ€§è¯„ä¼°æŒ‡æ ‡çš„æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜å¯è§†åŒ–äº†è§£ç å™¨äº¤å‰æ³¨æ„åŠ›å›¾æ¥è¯„ä¼°å¯è§£é‡Šæ€§ï¼Œå¹¶è¿›è¡Œäº†ä¸€é¡¹æ¶ˆèç ”ç©¶æ¥è¯„ä¼°ä»…ç¼–ç å™¨å¾®è°ƒä¸ä»…è§£ç å™¨å¾®è°ƒçš„è´¡çŒ®ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¼ºè°ƒäº†é’ˆå¯¹åŒ»å­¦åº”ç”¨è¿›è¡Œé’ˆå¯¹æ€§é€‚åº”çš„é‡è¦æ€§ï¼Œå¹¶è¡¨æ˜ä»…è§£ç å™¨å¾®è°ƒï¼ˆå†»ç»“ç¼–ç å™¨ï¼‰åœ¨æ¯”å®Œå…¨å¾®è°ƒå°‘5%çš„è®­ç»ƒæ—¶é—´å†…æä¾›äº†ä¸€ä¸ªå¼ºå¤§çš„æ€§èƒ½åŸºçº¿ï¼Œè€Œå…¨æ¨¡å‹å¾®è°ƒä»ç„¶æ€»ä½“ä¸Šè·å¾—äº†æœ€ä½³ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14726v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>åœ¨åŒ»å­¦å›¾åƒæè¿°ä»»åŠ¡ä¸­ï¼Œç”Ÿæˆå‡†ç¡®ä¸”è¯­ä¹‰æ˜ç¡®çš„æ”¾å°„å­¦å›¾åƒæè¿°æ˜¯ä¸€é¡¹æŒ‘æˆ˜ã€‚è™½ç„¶æœ€æ–°çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å¦‚BLIPã€BLIP2ã€Geminiå’ŒViT-GPT2åœ¨è‡ªç„¶å›¾åƒæ•°æ®é›†ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†å®ƒä»¬åº”ç”¨äºä¸“ä¸šåŒ»å­¦é¢†åŸŸæ—¶ï¼Œå¾€å¾€ä¼šäº§ç”Ÿé€šç”¨æˆ–ä¸ç²¾ç¡®çš„æè¿°ã€‚æœ¬é¡¹ç›®æ¢ç´¢äº†ä½¿ç”¨ROCOæ•°æ®é›†å¯¹BLIPæ¨¡å‹è¿›è¡Œå¾®è°ƒä»¥æé«˜æ”¾å°„å­¦æè¿°æ•ˆæœçš„æ–¹æ³•ã€‚æˆ‘ä»¬å°†å¾®è°ƒåçš„BLIPä¸é›¶æ ·æœ¬ç‰ˆæœ¬çš„BLIP-2åŸºç¡€ç‰ˆã€BLIP-2æŒ‡ä»¤ç‰ˆå’ŒViT-GPT2è½¬æ¢å™¨åŸºçº¿è¿›è¡Œäº†æ¯”è¾ƒã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨BLIPä¸Šè¿›è¡Œç‰¹å®šé¢†åŸŸçš„å¾®è°ƒå¯ä»¥æ˜¾è‘—æé«˜å®šé‡å’Œå®šæ€§è¯„ä¼°æŒ‡æ ‡çš„æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜é€šè¿‡å¯è§†åŒ–è§£ç å™¨äº¤å‰æ³¨æ„åŠ›å›¾æ¥è¯„ä¼°è§£é‡Šæ€§ï¼Œå¹¶è¿›è¡Œä¸€é¡¹æ¶ˆèç ”ç©¶ä»¥è¯„ä¼°ä»…ç¼–ç å™¨æˆ–ä»…è§£ç å™¨å¾®è°ƒçš„å½±å“ã€‚ç ”ç©¶ç»“æœè¡¨æ˜æœ‰é’ˆå¯¹æ€§çš„é€‚åº”å¯¹äºåŒ»å­¦åº”ç”¨è‡³å…³é‡è¦ï¼Œå¹¶è¡¨æ˜ä»…è§£ç å™¨å¾®è°ƒï¼ˆå†»ç»“ç¼–ç å™¨ï¼‰æä¾›äº†ä¸€ä¸ªå¼ºå¤§çš„æ€§èƒ½åŸºå‡†ï¼Œä¸å…¨æ¨¡å‹å¾®è°ƒç›¸æ¯”è®­ç»ƒæ—¶é—´ç¼©çŸ­5%ï¼Œè€Œå…¨æ¨¡å‹å¾®è°ƒä»ç„¶æ€»ä½“æ•ˆæœæœ€ä½³ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒæè¿°æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œéœ€è¦ä¸ºæ”¾å°„å­¦å›¾åƒç”Ÿæˆå‡†ç¡®ä¸”è¯­ä¹‰æ˜ç¡®çš„æè¿°ã€‚</li>
<li>å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶å›¾åƒä¸Šçš„è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨åŒ»å­¦é¢†åŸŸå¯èƒ½äº§ç”Ÿé€šç”¨æˆ–ä¸ç²¾ç¡®çš„æè¿°ã€‚</li>
<li>ä½¿ç”¨ROCOæ•°æ®é›†å¯¹BLIPæ¨¡å‹è¿›è¡Œå¾®è°ƒå¯ä»¥æé«˜æ”¾å°„å­¦æè¿°çš„å‡†ç¡®æ€§ã€‚</li>
<li>ä¸å…¶ä»–æ¨¡å‹ç›¸æ¯”ï¼Œå¾®è°ƒåçš„BLIPæ¨¡å‹æ€§èƒ½æ˜¾è‘—æé«˜ã€‚</li>
<li>é€šè¿‡å¯è§†åŒ–è§£ç å™¨äº¤å‰æ³¨æ„åŠ›å›¾æ¥è¯„ä¼°æ¨¡å‹çš„è§£é‡Šæ€§ã€‚</li>
<li>æ¶ˆèç ”ç©¶è¡¨æ˜æœ‰é’ˆå¯¹æ€§çš„é€‚åº”å¯¹äºåŒ»å­¦åº”ç”¨è‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14726">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-985cbc19dc6738cbb298e5af3012f22a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8b8fc12356270b2415a0f583cbd5e7f4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dc911de73254ff17c60b22946ad9012b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fed69dd26122a095bb3cac79817b6ac3.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Benchmarking-Ophthalmology-Foundation-Models-for-Clinically-Significant-Age-Macular-Degeneration-Detection"><a href="#Benchmarking-Ophthalmology-Foundation-Models-for-Clinically-Significant-Age-Macular-Degeneration-Detection" class="headerlink" title="Benchmarking Ophthalmology Foundation Models for Clinically Significant   Age Macular Degeneration Detection"></a>Benchmarking Ophthalmology Foundation Models for Clinically Significant   Age Macular Degeneration Detection</h2><p><strong>Authors:Benjamin A. Cohen, Jonathan Fhima, Meishar Meisel, Baskin Meital, Luis Filipe Nakayama, Eran Berkowitz, Joachim A. Behar</strong></p>
<p>Self-supervised learning (SSL) has enabled Vision Transformers (ViTs) to learn robust representations from large-scale natural image datasets, enhancing their generalization across domains. In retinal imaging, foundation models pretrained on either natural or ophthalmic data have shown promise, but the benefits of in-domain pretraining remain uncertain. To investigate this, we benchmark six SSL-pretrained ViTs on seven digital fundus image (DFI) datasets totaling 70,000 expert-annotated images for the task of moderate-to-late age-related macular degeneration (AMD) identification. Our results show that iBOT pretrained on natural images achieves the highest out-of-distribution generalization, with AUROCs of 0.80-0.97, outperforming domain-specific models, which achieved AUROCs of 0.78-0.96 and a baseline ViT-L with no pretraining, which achieved AUROCs of 0.68-0.91. These findings highlight the value of foundation models in improving AMD identification and challenge the assumption that in-domain pretraining is necessary. Furthermore, we release BRAMD, an open-access dataset (n&#x3D;587) of DFIs with AMD labels from Brazil. </p>
<blockquote>
<p>è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰ä½¿å¾—è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰èƒ½å¤Ÿä»å¤§è§„æ¨¡çš„è‡ªç„¶å›¾åƒæ•°æ®é›†ä¸­å­¦ä¹ é²æ£’æ€§è¡¨ç¤ºï¼Œä»è€Œæé«˜äº†å…¶åœ¨ä¸åŒé¢†åŸŸçš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨è§†ç½‘è†œæˆåƒä¸­ï¼Œé¢„è®­ç»ƒåœ¨è‡ªç„¶æˆ–çœ¼ç§‘æ•°æ®ä¸Šçš„åŸºç¡€æ¨¡å‹å·²ç»æ˜¾ç¤ºå‡ºäº†ä¸€å®šçš„æ½œåŠ›ï¼Œä½†æ˜¯é¢†åŸŸå†…é¢„è®­ç»ƒçš„å¥½å¤„ä»ç„¶ä¸ç¡®å®šã€‚ä¸ºäº†ç ”ç©¶è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬åœ¨åŒ…å«70,000å¼ ä¸“å®¶æ ‡æ³¨çš„å›¾åƒçš„æ€»å…±ä¸ƒä¸ªæ•°å­—çœ¼åº•å›¾åƒï¼ˆDFIï¼‰æ•°æ®é›†ä¸Šï¼Œå¯¹å…­ä¸ªSSLé¢„è®­ç»ƒçš„ViTè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œç”¨äºä¸­åº¦è‡³æ™šæœŸå¹´é¾„ç›¸å…³æ€§é»„æ–‘å˜æ€§ï¼ˆAMDï¼‰çš„è¯†åˆ«ä»»åŠ¡ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œåœ¨è‡ªç„¶å›¾åƒä¸Šè¿›è¡Œé¢„è®­ç»ƒçš„iBOTå…·æœ‰æœ€ä½³çš„éåˆ†å¸ƒå¤–æ³›åŒ–èƒ½åŠ›ï¼Œå…¶AUROCï¼ˆæ›²çº¿ä¸‹é¢ç§¯ï¼‰åœ¨0.80è‡³0.97ä¹‹é—´ï¼Œè¶…è¿‡äº†é¢†åŸŸç‰¹å®šæ¨¡å‹çš„AUROCï¼ˆåœ¨0.78è‡³0.96ä¹‹é—´ï¼‰å’Œæœªç»é¢„è®­ç»ƒçš„åŸºçº¿ViT-Lï¼ˆAUROCåœ¨0.68è‡³0.91ä¹‹é—´ï¼‰ã€‚è¿™äº›å‘ç°å‡¸æ˜¾äº†åŸºç¡€æ¨¡å‹åœ¨æ”¹è¿›AMDè¯†åˆ«æ–¹é¢çš„ä»·å€¼ï¼Œå¹¶æŒ‘æˆ˜äº†è®¤ä¸ºé¢†åŸŸå†…é¢„è®­ç»ƒæ˜¯å¿…éœ€çš„å‡è®¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘å¸ƒäº†BRAMDï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€æ”¾è®¿é—®çš„çœ¼åº•å›¾åƒæ•°æ®é›†ï¼ˆn&#x3D;587ï¼‰ï¼Œå¸¦æœ‰AMDæ ‡ç­¾ï¼Œæ¥è‡ªå·´è¥¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.05291v2">PDF</a> 10 pages, 3 figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§è§„æ¨¡è‡ªç„¶å›¾åƒæ•°æ®é›†ï¼Œè‡ªç›‘ç£å­¦ä¹ ä½¿Vision Transformersï¼ˆViTsï¼‰å­¦ä¹ ç¨³å¥çš„ç‰¹å¾è¡¨ç¤ºï¼Œå¹¶æå‡äº†è·¨åŸŸçš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨è§†ç½‘è†œæˆåƒä¸­ï¼Œè™½ç„¶åŸºäºè‡ªç„¶æˆ–çœ¼ç§‘æ•°æ®é¢„è®­ç»ƒçš„åŸºå‡†æ¨¡å‹å±•ç°å‡ºæ½œåŠ›ï¼Œä½†é¢†åŸŸå†…é¢„è®­ç»ƒçš„ä¼˜åŠ¿å°šä¸ç¡®å®šã€‚æœ¬ç ”ç©¶å¯¹å…­ç§SSLé¢„è®­ç»ƒçš„ViTsåœ¨ç”¨äºå¹´é¾„ç›¸å…³æ€§é»„æ–‘ç—…å˜ï¼ˆAMDï¼‰è¯†åˆ«çš„70,000å¼ ä¸“ä¸šæ ‡æ³¨çš„çœ¼åº•å›¾åƒæ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ã€‚ç»“æœæ˜¾ç¤ºï¼Œåœ¨è‡ªç„¶å›¾åƒä¸Šé¢„è®­ç»ƒçš„iBOTå…·æœ‰æœ€ä½³çš„è·¨åˆ†å¸ƒæ³›åŒ–èƒ½åŠ›ï¼ŒAUROCå€¼ä¸º0.80-0.97ï¼Œä¼˜äºé¢†åŸŸç‰¹å®šæ¨¡å‹å’Œæœªè¿›è¡Œé¢„è®­ç»ƒçš„åŸºçº¿ViT-Lï¼ˆAUROCå€¼ä¸º0.68-0.91ï¼‰ã€‚è¿™çªæ˜¾äº†åŸºå‡†æ¨¡å‹åœ¨æå‡AMDè¯†åˆ«ä¸­çš„ä»·å€¼ï¼Œå¹¶è´¨ç–‘é¢†åŸŸå†…é¢„è®­ç»ƒçš„å¿…è¦å‡è®¾ã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘å¸ƒäº†BRAMDè¿™ä¸€å¼€æ”¾è®¿é—®çš„å·´è¥¿AMDæ ‡è®°çœ¼åº•å›¾åƒæ•°æ®é›†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Self-supervised learning enables Vision Transformers to learn robust representations from large-scale natural image datasets.</li>
<li>In retinal imaging, the potential of foundation models pretrained on either natural or ophthalmic data has been demonstrated.</li>
<li>The benefits of in-domain pretraining remain uncertain in retinal imaging.</li>
<li>iBOT pretrained on natural images achieves high out-of-distribution generalization for AMD identification.</li>
<li>Domain-specific models and a baseline ViT-L with no pretraining are also evaluated, providing a comparative perspective.</li>
<li>The study highlights the value of foundation models in improving AMD identification.</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.05291">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a87bb049ce1324eabd4e22c9bfefdd03.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-68741de23d21e9d4dad004d22b15d06b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8192d69baa4700648077845d9e99c26f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-98b7b5da06fd1dab1de5b626d64d7ec3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cef667cf42e596a1a3f0470e5b1f79b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-32395660dddbc26b0aa920f47f6271d7.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Leveraging-Habitat-Information-for-Fine-grained-Bird-Identification"><a href="#Leveraging-Habitat-Information-for-Fine-grained-Bird-Identification" class="headerlink" title="Leveraging Habitat Information for Fine-grained Bird Identification"></a>Leveraging Habitat Information for Fine-grained Bird Identification</h2><p><strong>Authors:Tin Nguyen, Peijie Chen, Anh Totti Nguyen</strong></p>
<p>Traditional bird classifiers mostly rely on the visual characteristics of birds. Some prior works even train classifiers to be invariant to the background, completely discarding the living environment of birds. Instead, we are the first to explore integrating habitat information, one of the four major cues for identifying birds by ornithologists, into modern bird classifiers. We focus on two leading model types: (1) CNNs and ViTs trained on the downstream bird datasets; and (2) original, multi-modal CLIP. Training CNNs and ViTs with habitat-augmented data results in an improvement of up to +0.83 and +0.23 points on NABirds and CUB-200, respectively. Similarly, adding habitat descriptors to the prompts for CLIP yields a substantial accuracy boost of up to +0.99 and +1.1 points on NABirds and CUB-200, respectively. We find consistent accuracy improvement after integrating habitat features into the image augmentation process and into the textual descriptors of vision-language CLIP classifiers. Code is available at: <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/reasoning-8B7E/">https://anonymous.4open.science/r/reasoning-8B7E/</a>. </p>
<blockquote>
<p>ä¼ ç»Ÿçš„é¸Ÿç±»åˆ†ç±»å™¨å¤§å¤šä¾èµ–äºé¸Ÿç±»çš„è§†è§‰ç‰¹å¾ã€‚ä¸€äº›æ—©æœŸä½œå“ç”šè‡³è®­ç»ƒåˆ†ç±»å™¨å¯¹èƒŒæ™¯ä¿æŒä¸å˜ï¼Œå®Œå…¨å¿½ç•¥äº†é¸Ÿç±»çš„ç”Ÿæ´»ç¯å¢ƒã€‚ç›¸åï¼Œæˆ‘ä»¬æ˜¯é¦–æ‰¹æ¢ç´¢å°†æ –æ¯åœ°ä¿¡æ¯ï¼ˆé¸Ÿç±»å­¦å®¶è¯†åˆ«é¸Ÿç±»å››å¤§çº¿ç´¢ä¹‹ä¸€ï¼‰èå…¥ç°ä»£é¸Ÿç±»åˆ†ç±»å™¨çš„å›¢é˜Ÿã€‚æˆ‘ä»¬ä¸»è¦å…³æ³¨ä¸¤ç§é¢†å…ˆçš„æ¨¡å‹ç±»å‹ï¼šï¼ˆ1ï¼‰åœ¨ä¸‹æ¸¸é¸Ÿç±»æ•°æ®é›†ä¸Šè®­ç»ƒçš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’Œè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰ï¼›ï¼ˆ2ï¼‰åŸå§‹çš„è·¨æ¨¡æ€CLIPæ¨¡å‹ã€‚ä½¿ç”¨æ –æ¯åœ°å¢å¼ºæ•°æ®è¿›è¡Œè®­ç»ƒçš„CNNå’ŒViTæ¨¡å‹åœ¨NABirdså’ŒCUB-200ä¸Šçš„å‡†ç¡®ç‡åˆ†åˆ«æé«˜äº†+0.83å’Œ+0.23ä¸ªç‚¹ã€‚åŒæ ·ï¼Œåœ¨CLIPçš„æç¤ºä¸­åŠ å…¥æ –æ¯åœ°æè¿°ç¬¦å¯¼è‡´NABirdså’ŒCUB-200ä¸Šçš„å‡†ç¡®ç‡åˆ†åˆ«å¤§å¹…æé«˜äº†+0.99å’Œ+1.1ä¸ªç‚¹ã€‚æˆ‘ä»¬å‘ç°ï¼Œåœ¨å›¾åƒå¢å¼ºè¿‡ç¨‹å’Œè§†è§‰è¯­è¨€CLIPåˆ†ç±»å™¨çš„æ–‡æœ¬æè¿°ç¬¦ä¸­èå…¥æ –æ¯åœ°ç‰¹å¾åï¼Œå‡†ç¡®ç‡å¾—åˆ°äº†æŒç»­çš„æå‡ã€‚ä»£ç å·²å‘å¸ƒåœ¨ï¼š[<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/reasoning-8B7E/]">https://anonymous.4open.science/r/reasoning-8B7E/]</a> ä¸Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.14999v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºç°ä»£é¸Ÿç±»çš„åˆ†ç±»å™¨ï¼Œä¼ ç»Ÿé¸Ÿç±»åˆ†ç±»å™¨ä¸»è¦ä¾èµ–é¸Ÿç±»çš„è§†è§‰ç‰¹å¾ã€‚æœ¬ç ”ç©¶é¦–æ¬¡æ¢ç´¢å°†æ –æ¯åœ°ä¿¡æ¯â€”â€”é¸Ÿç±»å­¦å®¶è¯†åˆ«é¸Ÿç±»çš„å››å¤§çº¿ç´¢ä¹‹ä¸€â€”â€”èå…¥ç°ä»£é¸Ÿç±»åˆ†ç±»å™¨ã€‚ç ”ç©¶é›†ä¸­åœ¨ä¸¤ç§é¢†å…ˆçš„æ¨¡å‹ç±»å‹ä¸Šï¼šåœ¨ä¸‹æ¸¸é¸Ÿç±»æ•°æ®é›†ä¸Šè®­ç»ƒçš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰å’Œè§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰ï¼Œä»¥åŠåŸå§‹çš„è·¨æ¨¡æ€CLIPæ¨¡å‹ã€‚é€šè¿‡åŠ å…¥æ –æ¯åœ°ä¿¡æ¯åï¼Œæ— è®ºæ˜¯åœ¨CNNså’ŒViTsæ¨¡å‹ä¸­ï¼Œè¿˜æ˜¯åœ¨CLIPæ¨¡å‹ä¸­ï¼Œè®­ç»ƒæ•°æ®çš„å¢å¼ºå‡æå‡äº†æ¨¡å‹åœ¨NABirdså’ŒCUB-200æ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡ï¼Œæœ€é«˜æå‡å¹…åº¦åˆ†åˆ«ä¸º+0.83ç‚¹å’Œ+0.23ç‚¹ã€‚å°†æ –æ¯åœ°ç‰¹å¾èå…¥å›¾åƒå¢å¼ºè¿‡ç¨‹å’Œè·¨è§†è§‰è¯­è¨€çš„CLIPåˆ†ç±»å™¨çš„æ–‡æœ¬æè¿°ç¬¦ä¸­ï¼Œä¹Ÿèƒ½å®ç°å‡†ç¡®ç‡çš„æŒç»­æå‡ã€‚ä»£ç å·²å…¬å¼€æä¾›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¼ ç»Ÿé¸Ÿç±»åˆ†ç±»å™¨ä¸»è¦ä¾èµ–é¸Ÿç±»çš„è§†è§‰ç‰¹å¾è¿›è¡Œåˆ†ç±»ã€‚</li>
<li>æœ¬ç ”ç©¶é¦–æ¬¡å°è¯•å°†æ –æ¯åœ°ä¿¡æ¯èå…¥ç°ä»£é¸Ÿç±»åˆ†ç±»å™¨è®¾è®¡ä¸­ã€‚</li>
<li>åœ¨CNNå’ŒViTæ¨¡å‹ä¸Šèå…¥æ –æ¯åœ°ä¿¡æ¯åï¼Œåœ¨NABirdså’ŒCUB-200æ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡æœ‰æ‰€æå‡ã€‚</li>
<li>CLIPæ¨¡å‹é€šè¿‡ç»“åˆæ –æ¯åœ°æè¿°ï¼Œå‡†ç¡®ç‡å¾—åˆ°æ˜¾è‘—æé«˜ã€‚</li>
<li>å°†æ –æ¯åœ°ç‰¹å¾èå…¥å›¾åƒå¢å¼ºè¿‡ç¨‹å’Œæ–‡æœ¬æè¿°ç¬¦ä¸­ï¼Œèƒ½è¿›ä¸€æ­¥æå‡æ¨¡å‹çš„å‡†ç¡®ç‡ã€‚</li>
<li>è¯¥ç ”ç©¶æä¾›çš„ä»£ç å·²å…¬å¼€ä¾›å…¬ä¼—æŸ¥é˜…å’Œä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2312.14999">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-484883b6180d4cfe0c144dcb65f35331.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c8edc27aabf7834941721ceb8a5bf472.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0bafbf63fc12cfe8e4081236d48c3bc0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eab15d61ada084727e37e5011ca54d23.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c13a7c66c91082465f072fb7ad8ea0fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d41832642ee733f6773c691c2f6c17f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1b2ad4a4c6d8d2a89363df8ef35e6012.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f1c559c8a348c20c413ca5108440254.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-285c11c1712e40189b2169bc5113ca19.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d978695a38326c1a282fe67076f800d.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-24/Vision%20Transformer/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-24/Vision%20Transformer/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Vision-Transformer/">
                                    <span class="chip bg-color">Vision Transformer</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-24/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-04c7a152ae50387e4ccd60a96fce9ab4.jpg" class="responsive-img" alt="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
                        
                        <span class="card-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-24  Temporal Object Captioning for Street Scene Videos from LiDAR Tracks
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-24
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-24/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e5f7e0b979aba75fcc5cbfca07feb3df.jpg" class="responsive-img" alt="è§†é¢‘ç†è§£">
                        
                        <span class="card-title">è§†é¢‘ç†è§£</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-24  LiveVLM Efficient Online Video Understanding via Streaming-Oriented KV   Cache and Retrieval
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-24
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    è§†é¢‘ç†è§£
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">20064.6k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
