<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-24  An Effective Training Framework for Light-Weight Automatic Speech   Recognition Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-14a4e3300ae37e46159b13792cc36ef8.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-24
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    17k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    68 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-24-æ›´æ–°"><a href="#2025-05-24-æ›´æ–°" class="headerlink" title="2025-05-24 æ›´æ–°"></a>2025-05-24 æ›´æ–°</h1><h2 id="An-Effective-Training-Framework-for-Light-Weight-Automatic-Speech-Recognition-Models"><a href="#An-Effective-Training-Framework-for-Light-Weight-Automatic-Speech-Recognition-Models" class="headerlink" title="An Effective Training Framework for Light-Weight Automatic Speech   Recognition Models"></a>An Effective Training Framework for Light-Weight Automatic Speech   Recognition Models</h2><p><strong>Authors:Abdul Hannan, Alessio Brutti, Shah Nawaz, Mubashir Noman</strong></p>
<p>Recent advancement in deep learning encouraged developing large automatic speech recognition (ASR) models that achieve promising results while ignoring computational and memory constraints. However, deploying such models on low resource devices is impractical despite of their favorable performance. Existing approaches (pruning, distillation, layer skip etc.) transform the large models into smaller ones at the cost of significant performance degradation or require prolonged training of smaller models for better performance. To address these issues, we introduce an efficacious two-step representation learning based approach capable of producing several small sized models from a single large model ensuring considerably better performance in limited number of epochs. Comprehensive experimentation on ASR benchmarks reveals the efficacy of our approach, achieving three-fold training speed-up and up to 12.54% word error rate improvement. </p>
<blockquote>
<p>è¿‘æœŸæ·±åº¦å­¦ä¹ çš„å‘å±•ä¿ƒè¿›äº†å¤§å‹è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹çš„å¼€å‘ï¼Œè¿™äº›æ¨¡å‹åœ¨å¿½ç•¥è®¡ç®—å’Œå†…å­˜é™åˆ¶çš„æƒ…å†µä¸‹å–å¾—äº†ä»¤äººé¼“èˆçš„ç»“æœã€‚ç„¶è€Œï¼Œå°†è¿™ç§æ¨¡å‹éƒ¨ç½²åœ¨èµ„æºæœ‰é™çš„è®¾å¤‡ä¸Šå¹¶ä¸å®ç”¨ï¼Œå°½ç®¡å®ƒä»¬çš„æ€§èƒ½è¡¨ç°è‰¯å¥½ã€‚ç°æœ‰æ–¹æ³•ï¼ˆå¦‚ä¿®å‰ªã€è’¸é¦ã€å±‚è·³è¿‡ç­‰ï¼‰å°†å¤§å‹æ¨¡å‹è½¬åŒ–ä¸ºå°å‹æ¨¡å‹ï¼Œä½†è¦ä»¥æ€§èƒ½æ˜¾è‘—ä¸‹é™ä¸ºä»£ä»·ï¼Œæˆ–è€…éœ€è¦å¯¹å°å‹æ¨¡å‹è¿›è¡Œæ›´é•¿æ—¶é—´çš„è®­ç»ƒä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æœ‰æ•ˆçš„ä¸¤æ­¥è¡¨ç¤ºå­¦ä¹ çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿä»å•ä¸€çš„å¤§å‹æ¨¡å‹ä¸­ç”Ÿæˆå¤šä¸ªå°å‹æ¨¡å‹ï¼Œå¹¶åœ¨æœ‰é™çš„è¿­ä»£æ¬¡æ•°å†…ç¡®ä¿æ€§èƒ½æ˜¾è‘—æå‡ã€‚åœ¨ASRåŸºå‡†æµ‹è¯•ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå®ç°äº†ä¸‰å€çš„è®­ç»ƒé€Ÿåº¦æå‡å’Œæœ€é«˜è¾¾åˆ°12.54%çš„è¯é”™è¯¯ç‡æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16991v1">PDF</a> Accepted at InterSpeech 2025</p>
<p><strong>Summary</strong>ï¼š<br>æ·±åº¦å­¦ä¹ é¢†åŸŸçš„æœ€æ–°è¿›å±•æ¨åŠ¨äº†å¤§å‹è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹çš„å‘å±•ï¼Œè¿™äº›æ¨¡å‹åœ¨å¿½ç•¥è®¡ç®—å’Œå†…å­˜çº¦æŸçš„æƒ…å†µä¸‹å–å¾—äº†ä»¤äººé¼“èˆçš„ç»“æœã€‚ç„¶è€Œï¼Œå°†è¿™äº›æ¨¡å‹éƒ¨ç½²åœ¨èµ„æºæœ‰é™çš„è®¾å¤‡ä¸Šå¹¶ä¸å®ç”¨ã€‚ç°æœ‰æ–¹æ³•ï¼ˆå¦‚å‰ªæã€è’¸é¦ã€å±‚è·³è¿‡ç­‰ï¼‰å°†å¤§å‹æ¨¡å‹è½¬æ¢ä¸ºå°å‹æ¨¡å‹ï¼Œä½†ç‰ºç‰²äº†æ€§èƒ½æˆ–éœ€è¦å»¶é•¿å°å‹æ¨¡å‹çš„è®­ç»ƒæ—¶é—´ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„ä¸¤æ­¥è¡¨ç¤ºå­¦ä¹ æ³•ï¼Œèƒ½ä»å•ä¸€å¤§å‹æ¨¡å‹ä¸­ç”Ÿæˆå¤šä¸ªå°å‹æ¨¡å‹ï¼Œå¹¶åœ¨æœ‰é™çš„è®­ç»ƒå‘¨æœŸå†…å®ç°æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†ä¸‰å€çš„è®­ç»ƒé€Ÿåº¦æå‡å’Œæœ€é«˜è¾¾12.54%çš„è¯é”™è¯¯ç‡æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ·±åº¦å­¦ä¹ æ¨åŠ¨å¤§å‹ASRæ¨¡å‹å‘å±•ï¼Œä½†èµ„æºæœ‰é™è®¾å¤‡çš„éƒ¨ç½²ä¸å®ç”¨ã€‚</li>
<li>ç°æœ‰æ–¹æ³•è½¬æ¢å¤§å‹æ¨¡å‹åˆ°å°å‹æ¨¡å‹ä¼šç‰ºç‰²æ€§èƒ½æˆ–éœ€è¦å»¶é•¿è®­ç»ƒæ—¶é—´ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªåŸºäºè¡¨ç¤ºå­¦ä¹ çš„ä¸¤æ­¥æ–¹æ³•ï¼Œä»å•ä¸€å¤§å‹æ¨¡å‹ç”Ÿæˆå¤šä¸ªå°å‹æ¨¡å‹ã€‚</li>
<li>æ–¹æ³•å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡å’Œè®­ç»ƒé€Ÿåº¦æå‡ã€‚</li>
<li>å®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨ASRåŸºå‡†æµ‹è¯•ä¸Šæœ‰æ•ˆã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨ä¿è¯æ€§èƒ½çš„åŒæ—¶ï¼Œå®ç°äº†æ¨¡å‹çš„å¿«é€Ÿè®­ç»ƒå’Œéƒ¨ç½²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16991">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-07235e6076f9518fb58558fda5f92ab1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-590dbc913b7fe93104a2b1f5dc09f49f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b99f9410a26d61b994237b03b603002c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d13242ab52e2f9263624f0bd2296ea1.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="On-Multilingual-Encoder-Language-Model-Compression-for-Low-Resource-Languages"><a href="#On-Multilingual-Encoder-Language-Model-Compression-for-Low-Resource-Languages" class="headerlink" title="On Multilingual Encoder Language Model Compression for Low-Resource   Languages"></a>On Multilingual Encoder Language Model Compression for Low-Resource   Languages</h2><p><strong>Authors:Daniil Gurgurov, Michal Gregor, Josef van Genabith, Simon Ostermann</strong></p>
<p>In this paper, we combine two-step knowledge distillation, structured pruning, truncation, and vocabulary trimming for extremely compressing multilingual encoder-only language models for low-resource languages. Our novel approach systematically combines existing techniques and takes them to the extreme, reducing layer depth, feed-forward hidden size, and intermediate layer embedding size to create significantly smaller monolingual models while retaining essential language-specific knowledge. We achieve compression rates of up to 92% with only a marginal performance drop of 2-10% in four downstream tasks, including sentiment analysis, topic classification, named entity recognition, and part-of-speech tagging, across three low-resource languages. Notably, the performance degradation correlates with the amount of language-specific data in the teacher model, with larger datasets resulting in smaller performance losses. Additionally, we conduct extensive ablation studies to identify best practices for multilingual model compression using these techniques. </p>
<blockquote>
<p>åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬ç»“åˆäº†ä¸¤æ­¥çŸ¥è¯†è’¸é¦ã€ç»“æ„åŒ–å‰ªæã€æˆªæ–­å’Œè¯æ±‡ç¼©å‡ç­‰æŠ€æœ¯ï¼Œå¯¹å¤šè¯­è¨€ç¼–ç å™¨ä»…æœ‰è¯­è¨€æ¨¡å‹è¿›è¡Œäº†æè‡´å‹ç¼©ï¼Œé€‚ç”¨äºèµ„æºæœ‰é™çš„è¯­è¨€ã€‚æˆ‘ä»¬çš„æ–°æ–¹æ³•ç³»ç»Ÿåœ°ç»“åˆäº†ç°æœ‰æŠ€æœ¯å¹¶å°†å…¶æ¨å‘æè‡´ï¼Œé€šè¿‡å‡å°‘å±‚æ·±åº¦ã€å‰é¦ˆéšè—å¤§å°å’Œä¸­é—´å±‚åµŒå…¥å¤§å°ï¼Œåœ¨ä¿ç•™å…³é”®è¯­è¨€ç‰¹å®šçŸ¥è¯†çš„åŒæ—¶ï¼Œåˆ›å»ºäº†æ˜¾è‘—æ›´å°çš„å•è¯­ç§æ¨¡å‹ã€‚æˆ‘ä»¬åœ¨å››ç§ä¸‹æ¸¸ä»»åŠ¡ä¸­å®ç°äº†é«˜è¾¾92%çš„å‹ç¼©ç‡ï¼Œæ€§èƒ½ä»…ä¸‹é™äº†2-10%ï¼ŒåŒ…æ‹¬æƒ…æ„Ÿåˆ†æã€ä¸»é¢˜åˆ†ç±»ã€å‘½åå®ä½“è¯†åˆ«å’Œè¯æ€§æ ‡æ³¨ï¼Œæ¶‰åŠä¸‰ç§èµ„æºæœ‰é™çš„è¯­è¨€ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ€§èƒ½ä¸‹é™ä¸æ•™å¸ˆæ¨¡å‹ä¸­çš„è¯­è¨€ç‰¹å®šæ•°æ®é‡ç›¸å…³ï¼Œæ›´å¤§çš„æ•°æ®é›†å¯¼è‡´æ›´å°çš„æ€§èƒ½æŸå¤±ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¿›è¡Œäº†å¹¿æ³›çš„æ¶ˆèç ”ç©¶ï¼Œä»¥è¯†åˆ«ä½¿ç”¨è¿™äº›æŠ€æœ¯è¿›è¡Œå¤šè¯­ç§æ¨¡å‹å‹ç¼©çš„æœ€ä½³å®è·µã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16956v1">PDF</a> Pre-print</p>
<p><strong>Summary</strong>:<br>æœ¬ç ”ç©¶é€šè¿‡ç»“åˆä¸¤æ­¥çŸ¥è¯†è’¸é¦ã€ç»“æ„åŒ–å‰ªæã€æˆªæ–­å’Œè¯æ±‡ç¼©å‡ç­‰æŠ€æœ¯ï¼Œå¯¹å¤šè¯­è¨€ç¼–ç å™¨ä»…è¯­è¨€æ¨¡å‹è¿›è¡Œæç«¯å‹ç¼©ï¼Œé€‚ç”¨äºä½èµ„æºè¯­è¨€ã€‚é€šè¿‡å‡å°‘å±‚æ·±åº¦ã€å‰é¦ˆéšè—å¤§å°å’Œä¸­é—´å±‚åµŒå…¥å¤§å°ï¼Œåˆ›å»ºæ˜¾è‘—æ›´å°çš„å•è¯­è¨€æ¨¡å‹ï¼ŒåŒæ—¶ä¿ç•™å…³é”®çš„è¯­è¨€ç‰¹å®šçŸ¥è¯†ã€‚åœ¨æƒ…æ„Ÿåˆ†æã€ä¸»é¢˜åˆ†ç±»ã€å‘½åå®ä½“è¯†åˆ«å’Œè¯æ€§æ ‡æ³¨ç­‰å››ä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸­ï¼Œä»…åœ¨ä¸‰ç§ä½èµ„æºè¯­è¨€ä¸Šæœ‰2-10%çš„æ€§èƒ½ä¸‹é™ï¼Œå®ç°äº†é«˜è¾¾92%çš„å‹ç¼©ç‡ã€‚æ€§èƒ½ä¸‹é™ä¸æ•™å¸ˆæ¨¡å‹ä¸­çš„è¯­è¨€ç‰¹å®šæ•°æ®é‡æœ‰å…³ï¼Œè¾ƒå¤§æ•°æ®é›†å¯¼è‡´çš„æ€§èƒ½æŸå¤±è¾ƒå°ã€‚æ­¤å¤–ï¼Œè¿˜è¿›è¡Œäº†å¹¿æ³›çš„ç ”ç©¶ä»¥ç¡®å®šä½¿ç”¨è¿™äº›æŠ€æœ¯çš„å¤šè¯­è¨€æ¨¡å‹å‹ç¼©çš„æœ€ä½³å®è·µã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>æœ¬ç ”ç©¶é€šè¿‡ç»“åˆå¤šç§æŠ€æœ¯ï¼Œå®ç°äº†å¯¹å¤šè¯­è¨€ç¼–ç å™¨ä»…è¯­è¨€æ¨¡å‹çš„æç«¯å‹ç¼©ï¼Œé€‚ç”¨äºä½èµ„æºè¯­è¨€ã€‚</li>
<li>é€šè¿‡å‡å°‘æ¨¡å‹å±‚æ·±åº¦ã€å‰é¦ˆéšè—å¤§å°å’Œä¸­é—´å±‚åµŒå…¥å¤§å°ï¼Œåˆ›å»ºäº†æ˜¾è‘—æ›´å°çš„å•è¯­è¨€æ¨¡å‹ã€‚</li>
<li>åœ¨å››ä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸­å®ç°äº†é«˜è¾¾92%çš„å‹ç¼©ç‡ï¼ŒåŒæ—¶æ€§èƒ½ä¸‹é™ä»…ä¸º2-10%ã€‚</li>
<li>æ€§èƒ½ä¸‹é™ä¸æ•™å¸ˆæ¨¡å‹ä¸­çš„è¯­è¨€ç‰¹å®šæ•°æ®é‡æœ‰å…³ã€‚</li>
<li>åœ¨ä½èµ„æºè¯­è¨€ä¸Šï¼Œè¾ƒå¤§æ•°æ®é›†èƒ½å‡å°æ€§èƒ½æŸå¤±ã€‚</li>
<li>ç ”ç©¶é€šè¿‡å¹¿æ³›çš„ç ”ç©¶ç¡®å®šäº†ä½¿ç”¨å¤šç§æŠ€æœ¯å‹ç¼©å¤šè¯­è¨€æ¨¡å‹çš„æœ€ä½³å®è·µã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16956">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-62919fd5f24566ce19c3bc07a5b55b24.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9ec361b61f35ea519a52615a44bdfdb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f1250c8c1cb142d14846d4c7d860f87a.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Active-Speech-Enhancement-Active-Speech-Denoising-Decliping-and-Deveraberation"><a href="#Active-Speech-Enhancement-Active-Speech-Denoising-Decliping-and-Deveraberation" class="headerlink" title="Active Speech Enhancement: Active Speech Denoising Decliping and   Deveraberation"></a>Active Speech Enhancement: Active Speech Denoising Decliping and   Deveraberation</h2><p><strong>Authors:Ofir Yaish, Yehuda Mishaly, Eliya Nachmani</strong></p>
<p>We introduce a new paradigm for active sound modification: Active Speech Enhancement (ASE). While Active Noise Cancellation (ANC) algorithms focus on suppressing external interference, ASE goes further by actively shaping the speech signal â€“ both attenuating unwanted noise components and amplifying speech-relevant frequencies â€“ to improve intelligibility and perceptual quality. To enable this, we propose a novel Transformer-Mamba-based architecture, along with a task-specific loss function designed to jointly optimize interference suppression and signal enrichment. Our method outperforms existing baselines across multiple speech processing tasks â€“ including denoising, dereverberation, and declipping â€“ demonstrating the effectiveness of active, targeted modulation in challenging acoustic environments. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†ä¸€ç§ä¸»åŠ¨å£°éŸ³ä¿®æ”¹çš„æ–°èŒƒå¼ï¼šä¸»åŠ¨è¯­éŸ³å¢å¼ºï¼ˆASEï¼‰ã€‚è€Œä¸»åŠ¨é™å™ªï¼ˆANCï¼‰ç®—æ³•ä¸»è¦ä¾§é‡äºæŠ‘åˆ¶å¤–éƒ¨å¹²æ‰°ï¼ŒASEæ›´è¿›ä¸€æ­¥ï¼Œé€šè¿‡ç§¯æå¡‘é€ è¯­éŸ³ä¿¡å·â€”â€”æ—¢å‡å¼±ä¸éœ€è¦çš„å™ªå£°æˆåˆ†ï¼Œåˆæ”¾å¤§è¯­éŸ³ç›¸å…³çš„é¢‘ç‡â€”â€”æ¥æé«˜å¯æ‡‚åº¦å’Œæ„ŸçŸ¥è´¨é‡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºTransformer-Mambaçš„æ¶æ„ï¼Œä»¥åŠä¸€ç§é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„æŸå¤±å‡½æ•°ï¼Œæ—¨åœ¨è”åˆä¼˜åŒ–å¹²æ‰°æŠ‘åˆ¶å’Œä¿¡å·å¢å¼ºã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªè¯­éŸ³å¤„ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°è¶…è¿‡äº†ç°æœ‰åŸºçº¿ï¼ŒåŒ…æ‹¬å»å™ªã€å»æ··å“å’Œæ¶ˆæŠ–åŠ¨ï¼Œè¯æ˜äº†åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å£°å­¦ç¯å¢ƒä¸­ä¸»åŠ¨ã€é’ˆå¯¹æ€§è°ƒåˆ¶çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16911v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„ä¸»åŠ¨å£°éŸ³ä¿®æ”¹èŒƒå¼ï¼šä¸»åŠ¨è¯­éŸ³å¢å¼ºï¼ˆASEï¼‰ã€‚ä¸ä¸»åŠ¨å™ªå£°æ¶ˆé™¤ï¼ˆANCï¼‰ç®—æ³•ä¾§é‡äºæŠ‘åˆ¶å¤–éƒ¨å¹²æ‰°ä¸åŒï¼ŒASEé€šè¿‡ç§¯æå¡‘é€ è¯­éŸ³ä¿¡å·æ¥æé«˜è¯­éŸ³çš„æ¸…æ™°åº¦å’Œæ„ŸçŸ¥è´¨é‡ï¼Œæ—¢å‡å¼±äº†ä¸éœ€è¦çš„å™ªå£°æˆåˆ†ï¼Œåˆæ”¾å¤§äº†è¯­éŸ³ç›¸å…³çš„é¢‘ç‡ã€‚ä¸ºå®ç°è¿™ä¸€ç›®æ ‡ï¼Œæå‡ºäº†ä¸€ç§åŸºäºTransformer-Mambaçš„æ–°å‹æ¶æ„ï¼Œä»¥åŠä¸€ç§é’ˆå¯¹å¹²æ‰°æŠ‘åˆ¶å’Œä¿¡å·ä¸°å¯Œè”åˆä¼˜åŒ–çš„ç‰¹å®šä»»åŠ¡æŸå¤±å‡½æ•°ã€‚è¯¥æ–¹æ³•åœ¨å¤šä¸ªè¯­éŸ³å¤„ç†ä»»åŠ¡ä¸Šä¼˜äºç°æœ‰åŸºçº¿ï¼ŒåŒ…æ‹¬å»å™ªã€å»æ··å“å’Œå»å‰ªè¾‘ï¼Œè¯æ˜äº†åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å£°å­¦ç¯å¢ƒä¸­è¿›è¡Œä¸»åŠ¨ã€é’ˆå¯¹æ€§è°ƒåˆ¶çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†ä¸»åŠ¨è¯­éŸ³å¢å¼ºï¼ˆASEï¼‰è¿™ä¸€æ–°çš„å£°éŸ³ä¿®æ”¹èŒƒå¼ã€‚</li>
<li>ä¸ä¸»åŠ¨å™ªå£°æ¶ˆé™¤ï¼ˆANCï¼‰ä¸åŒï¼ŒASEæ—¨åœ¨ç§¯æå¡‘é€ è¯­éŸ³ä¿¡å·ã€‚</li>
<li>ASEèƒ½åŒæ—¶å‡å¼±ä¸éœ€è¦çš„å™ªå£°æˆåˆ†å¹¶æ”¾å¤§è¯­éŸ³ç›¸å…³çš„é¢‘ç‡ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºTransformer-Mambaçš„æ–°å‹æ¶æ„æ¥å®ç°ASEã€‚</li>
<li>æŸå¤±å‡½æ•°çš„è®¾è®¡èƒ½è”åˆä¼˜åŒ–å¹²æ‰°æŠ‘åˆ¶å’Œä¿¡å·ä¸°å¯Œã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å¤šä¸ªè¯­éŸ³å¤„ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰åŸºçº¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16911">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-11fd90a4a6597832a13bd692d5789996.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a09f095b6413342f788c91eda228206c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-570be815f846e66a308fac4b3fc37fe9.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="SEED-Speaker-Embedding-Enhancement-Diffusion-Model"><a href="#SEED-Speaker-Embedding-Enhancement-Diffusion-Model" class="headerlink" title="SEED: Speaker Embedding Enhancement Diffusion Model"></a>SEED: Speaker Embedding Enhancement Diffusion Model</h2><p><strong>Authors:KiHyun Nam, Jungwoo Heo, Jee-weon Jung, Gangin Park, Chaeyoung Jung, Ha-Jin Yu, Joon Son Chung</strong></p>
<p>A primary challenge when deploying speaker recognition systems in real-world applications is performance degradation caused by environmental mismatch. We propose a diffusion-based method that takes speaker embeddings extracted from a pre-trained speaker recognition model and generates refined embeddings. For training, our approach progressively adds Gaussian noise to both clean and noisy speaker embeddings extracted from clean and noisy speech, respectively, via forward process of a diffusion model, and then reconstructs them to clean embeddings in the reverse process. While inferencing, all embeddings are regenerated via diffusion process. Our method needs neither speaker label nor any modification to the existing speaker recognition pipeline. Experiments on evaluation sets simulating environment mismatch scenarios show that our method can improve recognition accuracy by up to 19.6% over baseline models while retaining performance on conventional scenarios. We publish our code here <a target="_blank" rel="noopener" href="https://github.com/kaistmm/seed-pytorch">https://github.com/kaistmm/seed-pytorch</a> </p>
<blockquote>
<p>åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­éƒ¨ç½²è¯­éŸ³è¯†åˆ«ç³»ç»Ÿæ—¶é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜æ˜¯ç¯å¢ƒä¸åŒ¹é…å¯¼è‡´çš„æ€§èƒ½ä¸‹é™ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é‡‡ç”¨ä»é¢„è®­ç»ƒçš„è¯­éŸ³è¯†åˆ«æ¨¡å‹ä¸­æå–çš„è¯´è¯äººåµŒå…¥ï¼Œå¹¶ç”Ÿæˆç²¾ç»†çš„åµŒå…¥ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡æ‰©æ•£æ¨¡å‹çš„æ­£å‘è¿‡ç¨‹ï¼Œé€æ­¥å‘å¹²å‡€å’Œå¸¦æœ‰å™ªéŸ³çš„è¯­éŸ³ä¸­æå–çš„å¹²å‡€å’Œå¸¦å™ªéŸ³çš„è¯´è¯äººåµŒå…¥æ·»åŠ é«˜æ–¯å™ªéŸ³ï¼Œç„¶ååœ¨åå‘è¿‡ç¨‹ä¸­å°†å®ƒä»¬é‡å»ºä¸ºå¹²å‡€çš„åµŒå…¥ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œæ‰€æœ‰åµŒå…¥éƒ½æ˜¯é€šè¿‡æ‰©æ•£è¿‡ç¨‹é‡æ–°ç”Ÿæˆçš„ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ—¢ä¸éœ€è¦è¯´è¯äººæ ‡ç­¾ï¼Œä¹Ÿä¸éœ€è¦ä¿®æ”¹ç°æœ‰çš„è¯­éŸ³è¯†åˆ«ç®¡é“ã€‚åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸åŒ¹é…åœºæ™¯çš„è¯„ä»·é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥åœ¨ä¿æŒä¼ ç»Ÿåœºæ™¯æ€§èƒ½çš„åŒæ—¶ï¼Œå°†åŸºçº¿æ¨¡å‹çš„è¯†åˆ«å‡†ç¡®ç‡æé«˜é«˜è¾¾19.6%ã€‚æˆ‘ä»¬å·²å°†ä»£ç å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/kaistmm/seed-pytorch%E4%B8%8A%E3%80%82">https://github.com/kaistmm/seed-pytorchä¸Šã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16798v1">PDF</a> Accepted to Interspeech 2025. The official code can be found at   <a target="_blank" rel="noopener" href="https://github.com/kaistmm/seed-pytorch">https://github.com/kaistmm/seed-pytorch</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„ä¼˜åŒ–æ–¹æ³•ï¼Œç”¨äºæ”¹å–„å®é™…éƒ¨ç½²ä¸­çš„è¯´è¯äººè¯†åˆ«ç³»ç»Ÿå› ç¯å¢ƒä¸åŒ¹é…å¯¼è‡´çš„æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚è¯¥æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒçš„è¯´è¯äººè¯†åˆ«æ¨¡å‹æå–è¯´è¯äººåµŒå…¥ï¼Œå¹¶é€šè¿‡æ‰©æ•£è¿‡ç¨‹ç”Ÿæˆç²¾ç»†åŒ–çš„åµŒå…¥ã€‚è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå¯¹å¹²å‡€å’Œå¸¦å™ªå£°çš„è¯´è¯äººåµŒå…¥é€æ­¥æ·»åŠ é«˜æ–¯å™ªå£°ï¼Œå†é€šè¿‡æ‰©æ•£æ¨¡å‹çš„é€†å‘è¿‡ç¨‹é‡å»ºä¸ºå¹²å‡€åµŒå…¥ã€‚åœ¨æ¨æ–­é˜¶æ®µï¼Œæ‰€æœ‰åµŒå…¥éƒ½é€šè¿‡æ‰©æ•£è¿‡ç¨‹é‡æ–°ç”Ÿæˆã€‚è¯¥æ–¹æ³•æ—¢ä¸éœ€è¦è¯´è¯äººæ ‡ç­¾ï¼Œä¹Ÿä¸éœ€è¦å¯¹ç°æœ‰çš„è¯´è¯äººè¯†åˆ«æµç¨‹è¿›è¡Œä¿®æ”¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸åŒ¹é…åœºæ™¯çš„è¯„ä¼°é›†ä¸Šï¼Œç›¸è¾ƒäºåŸºå‡†æ¨¡å‹ï¼Œè¯†åˆ«å‡†ç¡®ç‡æé«˜äº†é«˜è¾¾19.6%ï¼ŒåŒæ—¶åœ¨å¸¸è§„åœºæ™¯ä¸Šä¿æŒäº†æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯´è¯äººè¯†åˆ«ç³»ç»Ÿåœ¨ç°å®åº”ç”¨ä¸­çš„ä¸»è¦æŒ‘æˆ˜ä¹‹ä¸€æ˜¯ç¯å¢ƒä¸åŒ¹é…å¯¼è‡´çš„æ€§èƒ½ä¸‹é™ã€‚</li>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„ä¼˜åŒ–æ–¹æ³•ï¼Œç”¨äºæ”¹å–„å› ç¯å¢ƒä¸åŒ¹é…å¯¼è‡´çš„æ€§èƒ½é—®é¢˜ã€‚</li>
<li>è¯¥æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒçš„è¯´è¯äººè¯†åˆ«æ¨¡å‹æå–åµŒå…¥ï¼Œå¹¶é€šè¿‡æ‰©æ•£è¿‡ç¨‹ç”Ÿæˆç²¾ç»†åŒ–çš„åµŒå…¥ã€‚</li>
<li>è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå¹²å‡€å’Œå¸¦å™ªå£°çš„è¯´è¯äººåµŒå…¥ä¼šå—åˆ°é«˜æ–¯å™ªå£°çš„å¹²æ‰°ï¼Œç„¶åé€šè¿‡é€†å‘è¿‡ç¨‹é‡å»ºä¸ºå¹²å‡€åµŒå…¥ã€‚</li>
<li>åœ¨æ¨æ–­é˜¶æ®µï¼Œæ‰€æœ‰åµŒå…¥éƒ½é€šè¿‡æ‰©æ•£è¿‡ç¨‹é‡æ–°ç”Ÿæˆï¼Œæ— éœ€é¢å¤–çš„æ ‡ç­¾æˆ–ä¿®æ”¹ç°æœ‰æµç¨‹ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸åŒ¹é…åœºæ™¯çš„è¯„ä¼°é›†ä¸Šæ˜¾è‘—æé«˜äº†è¯†åˆ«å‡†ç¡®ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16798">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9002670bdc78141a234edfbbe21264ed.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dde69cbb9de5ffb77c895713798dd530.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c080e9a4d5e260b2c0f5268e42da8f8a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d6498b2c243ad9efdfd451991642bde3.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="SoccerChat-Integrating-Multimodal-Data-for-Enhanced-Soccer-Game-Understanding"><a href="#SoccerChat-Integrating-Multimodal-Data-for-Enhanced-Soccer-Game-Understanding" class="headerlink" title="SoccerChat: Integrating Multimodal Data for Enhanced Soccer Game   Understanding"></a>SoccerChat: Integrating Multimodal Data for Enhanced Soccer Game   Understanding</h2><p><strong>Authors:Sushant Gautam, Cise Midoglu, Vajira Thambawita, Michael A. Riegler, PÃ¥l Halvorsen, Mubarak Shah</strong></p>
<p>The integration of artificial intelligence in sports analytics has transformed soccer video understanding, enabling real-time, automated insights into complex game dynamics. Traditional approaches rely on isolated data streams, limiting their effectiveness in capturing the full context of a match. To address this, we introduce SoccerChat, a multimodal conversational AI framework that integrates visual and textual data for enhanced soccer video comprehension. Leveraging the extensive SoccerNet dataset, enriched with jersey color annotations and automatic speech recognition (ASR) transcripts, SoccerChat is fine-tuned on a structured video instruction dataset to facilitate accurate game understanding, event classification, and referee decision making. We benchmark SoccerChat on action classification and referee decision-making tasks, demonstrating its performance in general soccer event comprehension while maintaining competitive accuracy in referee decision making. Our findings highlight the importance of multimodal integration in advancing soccer analytics, paving the way for more interactive and explainable AI-driven sports analysis. <a target="_blank" rel="noopener" href="https://github.com/simula/SoccerChat">https://github.com/simula/SoccerChat</a> </p>
<blockquote>
<p>å°†äººå·¥æ™ºèƒ½èå…¥ä½“è‚²åˆ†æå·²ç»æ”¹å˜äº†å¯¹è¶³çƒè§†é¢‘çš„ç†è§£æ–¹å¼ï¼Œèƒ½å¤Ÿå®ç°é’ˆå¯¹å¤æ‚æ¯”èµ›åŠ¨æ€çš„å®æ—¶è‡ªåŠ¨åŒ–æ´å¯Ÿã€‚ä¼ ç»Ÿæ–¹æ³•ä¾èµ–äºå­¤ç«‹çš„æ•°æ®æµï¼Œåœ¨æ•æ‰æ¯”èµ›çš„å®Œæ•´ä¸Šä¸‹æ–‡æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†SoccerChatï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šæ¨¡å¼å¯¹è¯äººå·¥æ™ºèƒ½æ¡†æ¶ï¼Œèåˆäº†è§†è§‰å’Œæ–‡æœ¬æ•°æ®ï¼Œä»¥æé«˜è¶³çƒè§†é¢‘çš„ç†è§£èƒ½åŠ›ã€‚å€ŸåŠ©ä¸°å¯Œçš„SoccerNetæ•°æ®é›†ï¼Œé€šè¿‡å¢åŠ çƒè¡£é¢œè‰²æ³¨é‡Šå’Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰è½¬å½•æœ¬è¿›è¡Œä¸°å¯Œï¼ŒSoccerChatåœ¨ç»“æ„åŒ–è§†é¢‘æŒ‡ä»¤æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒï¼Œä»¥ä¿ƒè¿›å‡†ç¡®çš„æ¯”èµ›ç†è§£ã€äº‹ä»¶åˆ†ç±»å’Œè£åˆ¤å†³ç­–åˆ¶å®šã€‚æˆ‘ä»¬åœ¨åŠ¨ä½œåˆ†ç±»å’Œè£åˆ¤å†³ç­–åˆ¶å®šä»»åŠ¡ä¸Šå¯¹SoccerChatè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œè¯æ˜äº†å…¶åœ¨ä¸€èˆ¬è¶³çƒäº‹ä»¶ç†è§£æ–¹é¢çš„æ€§èƒ½ï¼ŒåŒæ—¶åœ¨è£åˆ¤å†³ç­–åˆ¶å®šæ–¹é¢ä¿æŒäº†ç«äº‰æ€§çš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¼ºè°ƒäº†å¤šæ¨¡å¼èåˆåœ¨æ¨è¿›è¶³çƒåˆ†ææ–¹é¢çš„é‡è¦æ€§ï¼Œä¸ºæ›´äº¤äº’å’Œå¯è§£é‡Šçš„AIé©±åŠ¨çš„ä½“è‚²åˆ†æé“ºå¹³äº†é“è·¯ã€‚ç›¸å…³é“¾æ¥ï¼š<a target="_blank" rel="noopener" href="https://github.com/simula/SoccerChat%E3%80%82">https://github.com/simula/SoccerChatã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16630v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>äººå·¥æ™ºèƒ½åœ¨ä½“è‚²åˆ†æé¢†åŸŸçš„åº”ç”¨å·²ç»è½¬å˜äº†äººä»¬å¯¹è¶³çƒæ¯”èµ›è§†é¢‘çš„ç†è§£æ–¹å¼ï¼Œé€šè¿‡å®æ—¶ã€è‡ªåŠ¨åŒ–çš„å¤æ‚æ¯”èµ›åŠ¨æ€æ´å¯Ÿã€‚ä¼ ç»Ÿæ–¹æ³•ä¾èµ–äºå­¤ç«‹çš„æ•°æ®æµï¼Œæ— æ³•å…¨é¢æ•æ‰æ¯”èµ›ä¸Šä¸‹æ–‡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºSoccerChatâ€”â€”ä¸€ä¸ªå¤šæ¨¡å¼å¯¹è¯å¼äººå·¥æ™ºèƒ½æ¡†æ¶ï¼Œèåˆäº†è§†è§‰å’Œæ–‡å­—æ•°æ®ï¼Œä»¥æé«˜è¶³çƒè§†é¢‘çš„ç†è§£èƒ½åŠ›ã€‚å€ŸåŠ©ä¸°å¯Œçš„SoccerNetæ•°æ®é›†ï¼Œé€šè¿‡çƒè¡£é¢œè‰²æ ‡æ³¨å’Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰è½¬å½•è¿›è¡Œä¸°å¯Œï¼ŒSoccerChatåœ¨ç»“æ„åŒ–è§†é¢‘æŒ‡ä»¤æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒï¼Œä»¥ä¿ƒè¿›å‡†ç¡®çš„æ¸¸æˆç†è§£ã€äº‹ä»¶åˆ†ç±»å’Œè£åˆ¤å†³ç­–ã€‚æˆ‘ä»¬åœ¨åŠ¨ä½œåˆ†ç±»å’Œè£åˆ¤å†³ç­–ä»»åŠ¡ä¸Šå¯¹SoccerChatè¿›è¡Œäº†è¯„ä¼°ï¼Œè¯æ˜äº†å®ƒåœ¨ä¸€èˆ¬è¶³çƒäº‹ä»¶ç†è§£æ–¹é¢çš„è¡¨ç°ï¼ŒåŒæ—¶åœ¨è£åˆ¤å†³ç­–åˆ¶å®šæ–¹é¢ä¿æŒç«äº‰æ€§çš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶çªå‡ºäº†å¤šæ¨¡å¼èåˆåœ¨æ¨åŠ¨è¶³çƒåˆ†ææ–¹é¢çš„é‡è¦æ€§ï¼Œä¸ºæ›´åŠ äº¤äº’å’Œå¯è§£é‡Šçš„AIé©±åŠ¨çš„ä½“è‚²åˆ†æé“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººå·¥æ™ºèƒ½åœ¨ä½“è‚²åˆ†æé¢†åŸŸçš„åº”ç”¨å·²ç»æ”¹å˜äº†å¯¹è¶³çƒè§†é¢‘çš„ç†è§£æ–¹å¼ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•éš¾ä»¥å…¨é¢æ•æ‰è¶³çƒæ¯”èµ›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚</li>
<li>SoccerChatæ˜¯ä¸€ä¸ªå¤šæ¨¡å¼å¯¹è¯å¼AIæ¡†æ¶ï¼Œèåˆäº†è§†è§‰å’Œæ–‡å­—æ•°æ®æ¥æé«˜è¶³çƒè§†é¢‘ç†è§£ã€‚</li>
<li>SoccerChatåˆ©ç”¨SoccerNetæ•°æ®é›†å’Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«æŠ€æœ¯è¿›è¡Œä¼˜åŒ–ã€‚</li>
<li>SoccerChatåœ¨åŠ¨ä½œåˆ†ç±»å’Œè£åˆ¤å†³ç­–ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚</li>
<li>å¤šæ¨¡å¼èåˆå¯¹äºæé«˜è¶³çƒåˆ†æçš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16630">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fcb2f7c733ecb583a8146b0e40bdf4b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cf52d0cbde2b135c0e3831e8ddb68829.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-461a7d479e89ca40e932eca9a2d00286.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Attractor-Based-Speech-Separation-of-Multiple-Utterances-by-Unknown-Number-of-Speakers"><a href="#Attractor-Based-Speech-Separation-of-Multiple-Utterances-by-Unknown-Number-of-Speakers" class="headerlink" title="Attractor-Based Speech Separation of Multiple Utterances by Unknown   Number of Speakers"></a>Attractor-Based Speech Separation of Multiple Utterances by Unknown   Number of Speakers</h2><p><strong>Authors:Yuzhu Wang, Archontis Politis, Konstantinos Drossos, Tuomas Virtanen</strong></p>
<p>This paper addresses the problem of single-channel speech separation, where the number of speakers is unknown, and each speaker may speak multiple utterances. We propose a speech separation model that simultaneously performs separation, dynamically estimates the number of speakers, and detects individual speaker activities by integrating an attractor module. The proposed system outperforms existing methods by introducing an attractor-based architecture that effectively combines local and global temporal modeling for multi-utterance scenarios. To evaluate the method in reverberant and noisy conditions, a multi-speaker multi-utterance dataset was synthesized by combining Librispeech speech signals with WHAM! noise signals. The results demonstrate that the proposed system accurately estimates the number of sources. The system effectively detects source activities and separates the corresponding utterances into correct outputs in both known and unknown source count scenarios. </p>
<blockquote>
<p>æœ¬æ–‡è§£å†³äº†å•é€šé“è¯­éŸ³åˆ†ç¦»çš„é—®é¢˜ï¼Œå…¶ä¸­æœªçŸ¥è¯´è¯äººæ•°é‡ï¼Œæ¯ä¸ªè¯´è¯äººå¯èƒ½ä¼šå‘å‡ºå¤šæ¬¡è¯è¯­ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§è¯­éŸ³åˆ†ç¦»æ¨¡å‹ï¼Œè¯¥æ¨¡å‹é€šè¿‡é›†æˆå¸å¼•æ¨¡å—ï¼ŒåŒæ—¶æ‰§è¡Œåˆ†ç¦»ã€åŠ¨æ€ä¼°è®¡è¯´è¯äººæ•°ä»¥åŠæ£€æµ‹å•ä¸ªè¯´è¯äººçš„æ´»åŠ¨ã€‚é€šè¿‡å¼•å…¥åŸºäºå¸å¼•å™¨çš„æ¶æ„ï¼Œæœ‰æ•ˆç»“åˆå±€éƒ¨å’Œå…¨å±€æ—¶é—´å»ºæ¨¡ï¼Œä¸ºå¤šé‡è¯è¯­åœºæ™¯æä¾›å¤šè¯è¯­åœºæ™¯æ•°æ®é›†ï¼Œæ‰€æå‡ºç³»ç»Ÿåœ¨æ€§èƒ½ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ã€‚ä¸ºäº†è¯„ä¼°åœ¨æ··å“å’Œå™ªéŸ³æ¡ä»¶ä¸‹çš„æ–¹æ³•ï¼Œæˆ‘ä»¬é€šè¿‡ç»“åˆLibrispeechè¯­éŸ³ä¿¡å·å’ŒWHAMï¼å™ªéŸ³ä¿¡å·åˆæˆå¤šè¯´è¯äººå¤šè¯è¯­æ•°æ®é›†ã€‚ç»“æœè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿå‡†ç¡®ä¼°è®¡äº†æºæ•°é‡ã€‚ç³»ç»Ÿæœ‰æ•ˆåœ°æ£€æµ‹æºæ´»åŠ¨ï¼Œå¹¶å°†ç›¸åº”çš„è®²è¯å†…å®¹åˆ†ç¦»ä¸ºå·²çŸ¥å’ŒæœªçŸ¥æºæ•°çš„æ­£ç¡®è¾“å‡ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16607v1">PDF</a> 5 pages, 4 figures, accepted by Interspeech 2025</p>
<p><strong>æ€»ç»“</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§è§£å†³å•é€šé“è¯­éŸ³åˆ†ç¦»é—®é¢˜çš„æ–¹æ³•ï¼Œå…¶ä¸­æœªçŸ¥è¯´è¯äººæ•°é‡ï¼Œä¸”æ¯ä¸ªè¯´è¯äººå¯èƒ½å‘å‡ºå¤šæ¬¡è¿ç»­è¯´è¯ã€‚é€šè¿‡é›†æˆå¸å¼•æ¨¡å—ï¼Œè¯¥è¯­éŸ³åˆ†ç¦»æ¨¡å‹å¯åŒæ—¶å®ç°åˆ†ç¦»ã€åŠ¨æ€ä¼°è®¡è¯´è¯äººæ•°å’Œæ£€æµ‹å•ä¸ªè¯´è¯äººçš„æ´»åŠ¨ã€‚è¯¥æ¨¡å‹å¼•å…¥åŸºäºå¸å¼•å™¨çš„æ¶æ„ï¼Œæœ‰æ•ˆç»“åˆæœ¬åœ°å’Œå…¨å±€æ—¶é—´å»ºæ¨¡ï¼Œä¸ºå¤šè¿ç»­å‘è¨€åœºæ™¯æä¾›ä¼˜åŠ¿ï¼Œä»è€Œè¶…è¶Šç°æœ‰æ–¹æ³•ã€‚ä¸ºè¯„ä¼°è¯¥æ¨¡å‹åœ¨æ··å“å’Œå™ªéŸ³æ¡ä»¶ä¸‹çš„æ€§èƒ½ï¼Œåˆæˆäº†ä¸€ä¸ªå¤šå‘è¨€äººè¿ç»­å‘è¨€æ•°æ®é›†ï¼Œç»“åˆäº†Librispeechè¯­éŸ³ä¿¡å·ä¸WHAM!å™ªéŸ³ä¿¡å·ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹èƒ½å‡†ç¡®ä¼°è®¡æºçš„æ•°é‡ï¼Œå¹¶åœ¨å·²çŸ¥å’ŒæœªçŸ¥æºè®¡æ•°åœºæ™¯ä¸‹æœ‰æ•ˆæ£€æµ‹æºæ´»åŠ¨å’Œå°†ç›¸åº”çš„è¿ç»­å‘è¨€æ­£ç¡®åˆ†ç¦»ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è®ºæ–‡è§£å†³çš„æ˜¯å•é€šé“è¯­éŸ³åˆ†ç¦»é—®é¢˜ï¼Œå…¶ä¸­è¯´è¯äººçš„æ•°é‡æœªçŸ¥ï¼Œä¸”æ¯ä¸ªè¯´è¯äººå¯èƒ½è¿ç»­å‘è¨€ã€‚</li>
<li>æå‡ºäº†ä¸€ç§è¯­éŸ³åˆ†ç¦»æ¨¡å‹ï¼Œè¯¥æ¨¡å‹é€šè¿‡é›†æˆå¸å¼•æ¨¡å—å®ç°å¤šç§åŠŸèƒ½ï¼ŒåŒ…æ‹¬è¯­éŸ³åˆ†ç¦»ã€åŠ¨æ€ä¼°è®¡è¯´è¯äººæ•°å’Œæ£€æµ‹å•ä¸ªè¯´è¯äººçš„æ´»åŠ¨ã€‚</li>
<li>æ¨¡å‹é‡‡ç”¨åŸºäºå¸å¼•å™¨çš„æ¶æ„ï¼Œæœ‰æ•ˆç»“åˆæœ¬åœ°å’Œå…¨å±€æ—¶é—´å»ºæ¨¡ï¼Œä»¥å¤„ç†å¤šè¿ç»­å‘è¨€åœºæ™¯ã€‚</li>
<li>æ¨¡å‹åœ¨åˆæˆæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œè¯¥æ•°æ®é›†ç»“åˆäº†Librispeechå’ŒWHAM!çš„è¯­éŸ³å’Œå™ªéŸ³ä¿¡å·ï¼Œä»¥æ¨¡æ‹ŸçœŸå®ç¯å¢ƒä¸­çš„æ··å“å’Œå™ªéŸ³å½±å“ã€‚</li>
<li>æ¨¡å‹èƒ½å‡†ç¡®ä¼°è®¡æºçš„æ•°é‡ï¼Œå³ä½¿åœ¨æºæ•°é‡æœªçŸ¥çš„æƒ…å†µä¸‹ä¹Ÿèƒ½æœ‰æ•ˆå·¥ä½œã€‚</li>
<li>æ¨¡å‹å¯ä»¥æœ‰æ•ˆåœ°æ£€æµ‹æºæ´»åŠ¨å¹¶å°†è¿ç»­å‘è¨€æ­£ç¡®åˆ†ç¦»ä¸ºä¸åŒçš„è¾“å‡ºã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨è¯­éŸ³åˆ†ç¦»é¢†åŸŸå±•ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œå¯èƒ½ä¸ºæœªæ¥ç›¸å…³ç ”ç©¶æä¾›æ–°çš„æ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16607">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c50261827bf4536a88d804800a2b8dd2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-053f4cca045407e7ecc7385aa2282a4d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1ec373a563e057fb0b3113c28cc9d513.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-05ff389ae8a39e261e585e03e0a51107.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c1c4613feb7ef41023e1373396bbc26a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-edd504665cde1d67a0947bd4c80d4957.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="X-ARES-A-Comprehensive-Framework-for-Assessing-Audio-Encoder-Performance"><a href="#X-ARES-A-Comprehensive-Framework-for-Assessing-Audio-Encoder-Performance" class="headerlink" title="X-ARES: A Comprehensive Framework for Assessing Audio Encoder   Performance"></a>X-ARES: A Comprehensive Framework for Assessing Audio Encoder   Performance</h2><p><strong>Authors:Junbo Zhang, Heinrich Dinkel, Yadong Niu, Chenyu Liu, Si Cheng, Anbei Zhao, Jian Luan</strong></p>
<p>We introduces X-ARES (eXtensive Audio Representation and Evaluation Suite), a novel open-source benchmark designed to systematically assess audio encoder performance across diverse domains. By encompassing tasks spanning speech, environmental sounds, and music, X-ARES provides two evaluation approaches for evaluating audio representations: linear fine-tuning and unparameterized evaluation. The framework includes 22 distinct tasks that cover essential aspects of audio processing, from speech recognition and emotion detection to sound event classification and music genre identification. Our extensive evaluation of state-of-the-art audio encoders reveals significant performance variations across different tasks and domains, highlighting the complexity of general audio representation learning. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†X-ARESï¼ˆæ‰©å±•éŸ³é¢‘è¡¨ç¤ºå’Œè¯„ä¼°å¥—ä»¶ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹å¼€æºåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨ç³»ç»Ÿåœ°è¯„ä¼°ä¸åŒé¢†åŸŸéŸ³é¢‘ç¼–ç å™¨çš„æ€§èƒ½ã€‚X-ARESæ¶µç›–äº†è¯­éŸ³ã€ç¯å¢ƒå£°éŸ³å’ŒéŸ³ä¹ç­‰ä»»åŠ¡ï¼Œä¸ºè¯„ä¼°éŸ³é¢‘è¡¨ç¤ºæä¾›äº†ä¸¤ç§è¯„ä¼°æ–¹æ³•ï¼šçº¿æ€§å¾®è°ƒå’Œæ— å‚æ•°è¯„ä¼°ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬æ¶µç›–éŸ³é¢‘å¤„ç†å„ä¸ªæ–¹é¢çš„22ä¸ªä¸åŒä»»åŠ¡ï¼Œä»è¯­éŸ³è¯†åˆ«å’Œæƒ…æ„Ÿæ£€æµ‹åˆ°å£°éŸ³äº‹ä»¶åˆ†ç±»å’ŒéŸ³ä¹é£æ ¼è¯†åˆ«ã€‚æˆ‘ä»¬å¯¹æœ€å…ˆè¿›çš„éŸ³é¢‘ç¼–ç å™¨çš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œä¸åŒä»»åŠ¡å’Œé¢†åŸŸä¹‹é—´çš„æ€§èƒ½å­˜åœ¨å¾ˆå¤§å·®å¼‚ï¼Œè¿™çªå‡ºäº†é€šç”¨éŸ³é¢‘è¡¨ç¤ºå­¦ä¹ çš„å¤æ‚æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16369v1">PDF</a> Accepted by Interspeech 2025</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä»‹ç»äº†X-ARESï¼ˆå¹¿æ³›éŸ³é¢‘è¡¨ç¤ºå’Œè¯„ä¼°å¥—ä»¶ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹çš„å¼€æºåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨ç³»ç»Ÿåœ°è¯„ä¼°ä¸åŒé¢†åŸŸéŸ³é¢‘ç¼–ç å™¨çš„æ€§èƒ½ã€‚X-ARESæ¶µç›–äº†æ¶µç›–è¯­éŸ³ã€ç¯å¢ƒå£°éŸ³å’ŒéŸ³ä¹çš„ä»»åŠ¡ï¼Œæä¾›äº†ä¸¤ç§è¯„ä¼°éŸ³é¢‘è¡¨ç¤ºçš„æ–¹æ³•ï¼šçº¿æ€§å¾®è°ƒå’Œæ— å‚æ•°è¯„ä¼°ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬22ä¸ªä¸åŒçš„ä»»åŠ¡ï¼Œæ¶µç›–éŸ³é¢‘å¤„ç†çš„é‡è¦æ–¹é¢ï¼Œå¦‚è¯­éŸ³è¯†åˆ«ã€æƒ…æ„Ÿæ£€æµ‹ã€å£°éŸ³äº‹ä»¶åˆ†ç±»å’ŒéŸ³ä¹é£æ ¼è¯†åˆ«ã€‚å¯¹æœ€å…ˆè¿›çš„éŸ³é¢‘ç¼–ç å™¨çš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œä¸åŒä»»åŠ¡å’Œé¢†åŸŸä¹‹é—´çš„æ€§èƒ½å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œçªå‡ºäº†é€šç”¨éŸ³é¢‘è¡¨ç¤ºå­¦ä¹ å¤æ‚æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>X-ARESæ˜¯ä¸€ä¸ªæ–°å‹çš„å¼€æºåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°éŸ³é¢‘ç¼–ç å™¨çš„æ€§èƒ½ã€‚</li>
<li>X-ARESæ¶µç›–äº†è¯­éŸ³ã€ç¯å¢ƒå£°éŸ³å’ŒéŸ³ä¹ç­‰å¤šç§é¢†åŸŸçš„ä»»åŠ¡ã€‚</li>
<li>X-ARESæä¾›äº†ä¸¤ç§è¯„ä¼°éŸ³é¢‘è¡¨ç¤ºçš„æ–¹æ³•ï¼šçº¿æ€§å¾®è°ƒå’Œæ— å‚æ•°è¯„ä¼°ã€‚</li>
<li>æ¡†æ¶åŒ…å«22ä¸ªä¸åŒçš„ä»»åŠ¡ï¼Œæ¶µç›–éŸ³é¢‘å¤„ç†çš„é‡è¦æ–¹é¢ï¼Œå¦‚è¯­éŸ³è¯†åˆ«ã€æƒ…æ„Ÿæ£€æµ‹ã€å£°éŸ³äº‹ä»¶åˆ†ç±»å’ŒéŸ³ä¹é£æ ¼è¯†åˆ«ã€‚</li>
<li>å¹¿æ³›è¯„ä¼°æ˜¾ç¤ºï¼Œä¸åŒä»»åŠ¡å’Œé¢†åŸŸä¹‹é—´éŸ³é¢‘ç¼–ç å™¨çš„æ€§èƒ½å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚</li>
<li>è¯„ä¼°ç»“æœå¼ºè°ƒäº†é€šç”¨éŸ³é¢‘è¡¨ç¤ºå­¦ä¹ çš„å¤æ‚æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16369">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2738696b4fb17ec5cc5c99a44ba56f3c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f487d64a504192774a4ba644053fa077.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-701069cc0f0b87a96d423c67478de3a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-35d2479d211a526218f3d83bb87f4ce2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4c13c9fb47b5d8934925001fb3ea299b.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Dysfluent-WFST-A-Framework-for-Zero-Shot-Speech-Dysfluency-Transcription-and-Detection"><a href="#Dysfluent-WFST-A-Framework-for-Zero-Shot-Speech-Dysfluency-Transcription-and-Detection" class="headerlink" title="Dysfluent WFST: A Framework for Zero-Shot Speech Dysfluency   Transcription and Detection"></a>Dysfluent WFST: A Framework for Zero-Shot Speech Dysfluency   Transcription and Detection</h2><p><strong>Authors:Chenxu Guo, Jiachen Lian, Xuanru Zhou, Jinming Zhang, Shuhe Li, Zongli Ye, Hwi Joo Park, Anaisha Das, Zoe Ezzes, Jet Vonk, Brittany Morin, Rian Bogley, Lisa Wauters, Zachary Miller, Maria Gorno-Tempini, Gopala Anumanchipalli</strong></p>
<p>Automatic detection of speech dysfluency aids speech-language pathologists in efficient transcription of disordered speech, enhancing diagnostics and treatment planning. Traditional methods, often limited to classification, provide insufficient clinical insight, and text-independent models misclassify dysfluency, especially in context-dependent cases. This work introduces Dysfluent-WFST, a zero-shot decoder that simultaneously transcribes phonemes and detects dysfluency. Unlike previous models, Dysfluent-WFST operates with upstream encoders like WavLM and requires no additional training. It achieves state-of-the-art performance in both phonetic error rate and dysfluency detection on simulated and real speech data. Our approach is lightweight, interpretable, and effective, demonstrating that explicit modeling of pronunciation behavior in decoding, rather than complex architectures, is key to improving dysfluency processing systems. </p>
<blockquote>
<p>è‡ªåŠ¨æ£€æµ‹è¯­è¨€æµç•…æ€§éšœç¢æœ‰åŠ©äºè¯­è¨€ç—…ç†å­¦å®¶æœ‰æ•ˆåœ°è½¬å½•éšœç¢æ€§è¯­è¨€ï¼Œæé«˜è¯Šæ–­å’Œåˆ¶å®šæ²»ç–—æ–¹æ¡ˆã€‚ä¼ ç»Ÿçš„æ–¹æ³•é€šå¸¸ä»…é™äºåˆ†ç±»ï¼Œæä¾›çš„ä¸´åºŠè§è§£ä¸è¶³ï¼Œå¹¶ä¸”æ–‡æœ¬ç‹¬ç«‹æ¨¡å‹å®¹æ˜“è¯¯åˆ¤è¯­è¨€æµç•…æ€§éšœç¢ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¾èµ–è¯­å¢ƒçš„æ¡ˆä¾‹ä¸­ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†Dysfluent-WFSTï¼Œè¿™æ˜¯ä¸€ç§é›¶æ ·æœ¬è§£ç å™¨ï¼Œå¯ä»¥åŒæ—¶è½¬å½•éŸ³ç´ å¹¶æ£€æµ‹è¯­è¨€æµç•…æ€§éšœç¢ã€‚ä¸åŒäºä¹‹å‰çš„æ¨¡å‹ï¼ŒDysfluent-WFSTä¸ä¸Šæ¸¸ç¼–ç å™¨ï¼ˆå¦‚WavLMï¼‰ä¸€èµ·è¿è¡Œï¼Œæ— éœ€é¢å¤–è®­ç»ƒã€‚å®ƒåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®è¯­éŸ³æ•°æ®ä¸Šè¾¾åˆ°äº†è¯­éŸ³é”™è¯¯ç‡å’Œè¯­è¨€æµç•…æ€§æ£€æµ‹æ–¹é¢çš„æœ€ä½³æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•è½»ä¾¿ã€å¯è§£é‡Šæ€§å¼ºä¸”æœ‰æ•ˆï¼Œè¡¨æ˜åœ¨è§£ç è¿‡ç¨‹ä¸­æ˜ç¡®å»ºæ¨¡å‘éŸ³è¡Œä¸ºï¼Œè€Œä¸æ˜¯ä½¿ç”¨å¤æ‚çš„æ¶æ„ï¼Œæ˜¯æé«˜è¯­è¨€æµç•…æ€§å¤„ç†ç³»ç»Ÿçš„å…³é”®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16351v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è‡ªåŠ¨æ£€æµ‹è¯­è¨€æµç•…æ€§éšœç¢æœ‰åŠ©äºè¯­è¨€ç—…ç†å­¦å®¶é«˜æ•ˆåœ°è½¬å½•éšœç¢æ€§è¯­è¨€ï¼Œä»è€Œæé«˜è¯Šæ–­å’Œåˆ¶å®šæ²»ç–—æ–¹æ¡ˆã€‚ä¼ ç»Ÿæ–¹æ³•å¾€å¾€å±€é™äºåˆ†ç±»ï¼Œæä¾›ä¸è¶³çš„ä¸´åºŠè§è§£ï¼Œè€Œç‹¬ç«‹äºæ–‡æœ¬çš„æ¨¡å‹åœ¨ä¸Šä¸‹æ–‡ç›¸å…³çš„æ¡ˆä¾‹ä¸­è¯¯åˆ¤è¯­è¨€æµç•…æ€§éšœç¢ã€‚æœ¬ç ”ç©¶å¼•å…¥Dysfluent-WFSTæ¨¡å‹ï¼Œä¸€ç§é›¶å°„å‡»è§£ç å™¨å¯åŒæ—¶è½¬å½•éŸ³ç´ å¹¶æ£€æµ‹è¯­è¨€æµç•…æ€§éšœç¢ã€‚ä¸å…¶ä»–æ¨¡å‹ä¸åŒï¼ŒDysfluent-WFSTä¸ä¸Šæ¸¸ç¼–ç å™¨å¦‚WavLMååŒå·¥ä½œï¼Œæ— éœ€é¢å¤–è®­ç»ƒã€‚å®ƒåœ¨æ¨¡æ‹Ÿå’Œå®é™…è¯­éŸ³æ•°æ®ä¸Šå‡å®ç°äº†è¯­éŸ³é”™è¯¯ç‡å’Œè¯­è¨€æµç•…æ€§éšœç¢æ£€æµ‹çš„å“è¶Šæ€§èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•å…·æœ‰è½»ä¾¿æ€§ã€è§£é‡Šæ€§å’Œæœ‰æ•ˆæ€§ï¼Œè¯æ˜äº†è§£ç è¿‡ç¨‹ä¸­æ˜¾å¼å»ºæ¨¡å‘éŸ³è¡Œä¸ºè€Œéå¤æ‚æ¶æ„æ˜¯æé«˜è¯­è¨€æµç•…æ€§å¤„ç†ç³»ç»Ÿçš„å…³é”®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨æ£€æµ‹è¯­è¨€æµç•…æ€§éšœç¢å¯æé«˜è¯Šæ–­ä¸æ²»ç–—æ•ˆç‡ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•åœ¨åˆ†ç±»ä¸Šå­˜åœ¨å±€é™æ€§ï¼Œéœ€è¦æ–°æ–¹æ³•æä¾›æ›´æ·±å…¥çš„è§è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16351">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-a89107df145af9ba17ac75f8df28a5ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-72dbb715284d0f9e467b80eaea1fb515.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c698578373eddacab7d8c2ec84c99f22.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-db83b56946c95c5fc3097276cb396e02.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7fc99bdb2d2621beb13e9ce8a2babca7.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="MM-MovieDubber-Towards-Multi-Modal-Learning-for-Multi-Modal-Movie-Dubbing"><a href="#MM-MovieDubber-Towards-Multi-Modal-Learning-for-Multi-Modal-Movie-Dubbing" class="headerlink" title="MM-MovieDubber: Towards Multi-Modal Learning for Multi-Modal Movie   Dubbing"></a>MM-MovieDubber: Towards Multi-Modal Learning for Multi-Modal Movie   Dubbing</h2><p><strong>Authors:Junjie Zheng, Zihao Chen, Chaofan Ding, Yunming Liang, Yihan Fan, Huan Yang, Lei Xie, Xinhan Di</strong></p>
<p>Current movie dubbing technology can produce the desired speech using a reference voice and input video, maintaining perfect synchronization with the visuals while effectively conveying the intended emotions. However, crucial aspects of movie dubbing, including adaptation to various dubbing styles, effective handling of dialogue, narration, and monologues, as well as consideration of subtle details such as speaker age and gender, remain insufficiently explored. To tackle these challenges, we introduce a multi-modal generative framework. First, it utilizes a multi-modal large vision-language model (VLM) to analyze visual inputs, enabling the recognition of dubbing types and fine-grained attributes. Second, it produces high-quality dubbing using large speech generation models, guided by multi-modal inputs. Additionally, a movie dubbing dataset with annotations for dubbing types and subtle details is constructed to enhance movie understanding and improve dubbing quality for the proposed multi-modal framework. Experimental results across multiple benchmark datasets show superior performance compared to state-of-the-art (SOTA) methods. In details, the LSE-D, SPK-SIM, EMO-SIM, and MCD exhibit improvements of up to 1.09%, 8.80%, 19.08%, and 18.74%, respectively. </p>
<blockquote>
<p>å½“å‰çš„ç”µå½±é…éŸ³æŠ€æœ¯å¯ä»¥ä½¿ç”¨å‚è€ƒå£°éŸ³å’Œè¾“å…¥è§†é¢‘æ¥ç”Ÿæˆæ‰€éœ€çš„è¯­éŸ³ï¼Œåœ¨è§†è§‰æ•ˆæœä¸Šä¿æŒå®Œç¾çš„åŒæ­¥ï¼ŒåŒæ—¶æœ‰æ•ˆåœ°ä¼ è¾¾é¢„æœŸçš„æƒ…æ„Ÿã€‚ç„¶è€Œï¼Œç”µå½±é…éŸ³çš„å…³é”®æ–¹é¢ï¼ŒåŒ…æ‹¬é€‚åº”å„ç§é…éŸ³é£æ ¼ã€æœ‰æ•ˆå¤„ç†å¯¹è¯ã€æ—ç™½å’Œç‹¬ç™½ï¼Œä»¥åŠè€ƒè™‘è¯¸å¦‚æ¼”è®²è€…å¹´é¾„å’Œæ€§åˆ«ç­‰ç»†å¾®ç»†èŠ‚ï¼Œä»ç„¶æ²¡æœ‰å¾—åˆ°è¶³å¤Ÿçš„æ¢ç´¢ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¤šæ¨¡æ€ç”Ÿæˆæ¡†æ¶ã€‚é¦–å…ˆï¼Œå®ƒåˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æ¥åˆ†æè§†è§‰è¾“å…¥ï¼Œèƒ½å¤Ÿè¯†åˆ«é…éŸ³ç±»å‹å’Œç²¾ç»†å±æ€§ã€‚å…¶æ¬¡ï¼Œå®ƒä½¿ç”¨å¤§å‹è¯­éŸ³ç”Ÿæˆæ¨¡å‹ç”Ÿæˆé«˜è´¨é‡çš„é…éŸ³ï¼Œè¿™äº›æ¨¡å‹å—åˆ°å¤šæ¨¡æ€è¾“å…¥çš„æŒ‡å¯¼ã€‚æ­¤å¤–ï¼Œè¿˜æ„å»ºäº†ä¸€ä¸ªåŒ…å«é…éŸ³ç±»å‹å’Œç»†å¾®ç»†èŠ‚æ³¨é‡Šçš„ç”µå½±é…éŸ³æ•°æ®é›†ï¼Œä»¥æé«˜ç”µå½±ç†è§£å’Œæé«˜æ‰€æå‡ºçš„å¤šæ¨¡æ€æ¡†æ¶çš„é…éŸ³è´¨é‡ã€‚åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œä¸æœ€æ–°æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ¡†æ¶å…·æœ‰å“è¶Šçš„æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼ŒLSE-Dã€SPK-SIMã€EMO-SIMå’ŒMCDåˆ†åˆ«æé«˜äº†é«˜è¾¾1.09%ã€8.80%ã€19.08%å’Œ18.74%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16279v1">PDF</a> 5 pages, 4 figures, accepted by Interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>å½“å‰ç”µå½±é…éŸ³æŠ€æœ¯å·²èƒ½é€šè¿‡å‚è€ƒå£°éŸ³å’Œè¾“å…¥è§†é¢‘äº§ç”Ÿæ‰€éœ€çš„è¯­éŸ³ï¼Œä¿æŒä¸è§†è§‰ç”»é¢çš„å®Œç¾åŒæ­¥ï¼Œå¹¶æœ‰æ•ˆä¼ è¾¾é¢„æœŸçš„æƒ…ç»ªã€‚ç„¶è€Œï¼Œç”µå½±é…éŸ³çš„å…³é”®æ–¹é¢ï¼Œå¦‚é€‚åº”ä¸åŒçš„é…éŸ³é£æ ¼ã€æœ‰æ•ˆå¤„ç†å¯¹è¯ã€æ—ç™½å’Œç‹¬ç™½ï¼Œä»¥åŠè€ƒè™‘æ¼”è®²è€…çš„å¹´é¾„å’Œæ€§åˆ«ç­‰ç»†å¾®ä¹‹å¤„ï¼Œä»æ¢ç´¢ä¸è¶³ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€ç”Ÿæˆæ¡†æ¶ï¼Œå®ƒé¦–å…ˆåˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åˆ†æè§†è§‰è¾“å…¥ï¼Œè¯†åˆ«é…éŸ³ç±»å‹å’Œç²¾ç»†å±æ€§ã€‚å…¶æ¬¡ï¼Œåˆ©ç”¨å¤§å‹è¯­éŸ³ç”Ÿæˆæ¨¡å‹äº§ç”Ÿé«˜è´¨é‡é…éŸ³ï¼Œå—å¤šæ¨¡æ€è¾“å…¥çš„æŒ‡å¯¼ã€‚æ­¤å¤–ï¼Œè¿˜æ„å»ºäº†ä¸€ä¸ªåŒ…å«é…éŸ³ç±»å‹å’Œç»†å¾®ä¹‹å¤„æ³¨é‡Šçš„ç”µå½±é…éŸ³æ•°æ®é›†ï¼Œä»¥æé«˜ç”µå½±ç†è§£å’Œæå‡é…éŸ³è´¨é‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¸æœ€æ–°æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ¡†æ¶åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¡¨ç°å“è¶Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰ç”µå½±é…éŸ³æŠ€æœ¯èƒ½å¤ŸåŒæ­¥è§†é¢‘å¹¶ä¼ è¾¾æƒ…ç»ªã€‚</li>
<li>ç”µå½±é…éŸ³ä»é¢ä¸´é€‚åº”ä¸åŒé…éŸ³é£æ ¼ã€å¤„ç†å¯¹è¯ã€æ—ç™½å’Œç‹¬ç™½çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºçš„å¤šæ¨¡æ€ç”Ÿæˆæ¡†æ¶åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹è¯†åˆ«é…éŸ³ç±»å‹å’Œç²¾ç»†å±æ€§ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡å¤§å‹è¯­éŸ³ç”Ÿæˆæ¨¡å‹äº§ç”Ÿé«˜è´¨é‡é…éŸ³ã€‚</li>
<li>æ„å»ºäº†ä¸€ä¸ªåŒ…å«é…éŸ³ç±»å‹å’Œç»†å¾®ä¹‹å¤„çš„ç”µå½±é…éŸ³æ•°æ®é›†ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºè¯¥æ¡†æ¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºæœ€æ–°æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16279">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0348abc1ca837d6933c3848018c0dcef.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1fee04bf9e4415ee981543265f126cc6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-900bb5a66835af50bf131061bd35a924.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c385297ffc4f8603bd92aa04121f968.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f46db5dd8aca4a724b562249c7e9e284.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Meta-PerSER-Few-Shot-Listener-Personalized-Speech-Emotion-Recognition-via-Meta-learning"><a href="#Meta-PerSER-Few-Shot-Listener-Personalized-Speech-Emotion-Recognition-via-Meta-learning" class="headerlink" title="Meta-PerSER: Few-Shot Listener Personalized Speech Emotion Recognition   via Meta-learning"></a>Meta-PerSER: Few-Shot Listener Personalized Speech Emotion Recognition   via Meta-learning</h2><p><strong>Authors:Liang-Yeh Shen, Shi-Xin Fang, Yi-Cheng Lin, Huang-Cheng Chou, Hung-yi Lee</strong></p>
<p>This paper introduces Meta-PerSER, a novel meta-learning framework that personalizes Speech Emotion Recognition (SER) by adapting to each listenerâ€™s unique way of interpreting emotion. Conventional SER systems rely on aggregated annotations, which often overlook individual subtleties and lead to inconsistent predictions. In contrast, Meta-PerSER leverages a Model-Agnostic Meta-Learning (MAML) approach enhanced with Combined-Set Meta-Training, Derivative Annealing, and per-layer per-step learning rates, enabling rapid adaptation with only a few labeled examples. By integrating robust representations from pre-trained self-supervised models, our framework first captures general emotional cues and then fine-tunes itself to personal annotation styles. Experiments on the IEMOCAP corpus demonstrate that Meta-PerSER significantly outperforms baseline methods in both seen and unseen data scenarios, highlighting its promise for personalized emotion recognition. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†Meta-PerSERï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å…ƒå­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡é€‚åº”æ¯ä¸ªå¬ä¼—ç‹¬ç‰¹çš„æƒ…æ„Ÿè§£è¯»æ–¹å¼ï¼Œå¯¹è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰è¿›è¡Œä¸ªæ€§åŒ–å¤„ç†ã€‚ä¼ ç»Ÿçš„SERç³»ç»Ÿä¾èµ–äºèšåˆæ³¨é‡Šï¼Œè¿™å¾€å¾€å¿½ç•¥äº†ä¸ªäººç»†å¾®å·®åˆ«ï¼Œå¹¶å¯¼è‡´é¢„æµ‹ç»“æœä¸ä¸€è‡´ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒMeta-PerSERåˆ©ç”¨æ¨¡å‹æ— å…³çš„å…ƒå­¦ä¹ ï¼ˆMAMLï¼‰æ–¹æ³•ï¼Œå¹¶ç»“åˆé›†åˆå…ƒè®­ç»ƒã€å¯¼æ•°é€€ç«å’Œé€å±‚æ¯æ­¥å­¦ä¹ ç‡ï¼Œä»…é€šè¿‡å°‘é‡æœ‰æ ‡ç­¾çš„æ ·æœ¬å³å¯å®ç°å¿«é€Ÿé€‚åº”ã€‚é€šè¿‡æ•´åˆé¢„è®­ç»ƒè‡ªç›‘ç£æ¨¡å‹çš„ç¨³å¥è¡¨ç¤ºï¼Œæˆ‘ä»¬çš„æ¡†æ¶é¦–å…ˆæ•æ‰ä¸€èˆ¬çš„æƒ…æ„Ÿçº¿ç´¢ï¼Œç„¶åå¯¹è‡ªå·±è¿›è¡Œå¾®è°ƒä»¥é€‚åº”ä¸ªäººæ³¨é‡Šé£æ ¼ã€‚åœ¨IEMOCAPè¯­æ–™åº“ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMeta-PerSERåœ¨å¯è§å’Œæœªè§æ•°æ®åœºæ™¯ä¸­å‡æ˜¾è‘—ä¼˜äºåŸºå‡†æ–¹æ³•ï¼Œå‡¸æ˜¾å…¶åœ¨ä¸ªæ€§åŒ–æƒ…æ„Ÿè¯†åˆ«æ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16220v1">PDF</a> Accepted by INTERSPEECH 2025. 7 pages, including 2 pages of appendix</p>
<p><strong>æ€»ç»“</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Meta-PerSERï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„å…ƒå­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡é€‚åº”æ¯ä¸ªå¬ä¼—å¯¹æƒ…ç»ªè§£è¯»çš„ç‹¬ç‰¹æ–¹å¼ï¼Œä¸ªæ€§åŒ–åœ°åº”ç”¨äºè¯­éŸ³æƒ…ç»ªè¯†åˆ«ï¼ˆSERï¼‰ã€‚ä¸ä¼ ç»Ÿçš„SERç³»ç»Ÿä¾èµ–äºèšåˆæ³¨é‡Šä¸åŒï¼Œè¿™å¸¸å¸¸å¿½ç•¥äº†ä¸ªäººç»†å¾®å·®åˆ«å¹¶å¯¼è‡´é¢„æµ‹ä¸ä¸€è‡´ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒMeta-PerSERé‡‡ç”¨æ¨¡å‹æ— å…³çš„å…ƒå­¦ä¹ æ–¹æ³•ï¼Œå¹¶ç»“åˆç»„åˆé›†å…ƒè®­ç»ƒã€å¯¼æ•°é€€ç«å’Œé€å±‚é€æ­¥çš„å­¦ä¹ ç‡ï¼Œä»…é€šè¿‡å°‘é‡æ ‡æ³¨æ ·æœ¬å³å¯å®ç°å¿«é€Ÿé€‚åº”ã€‚é€šè¿‡æ•´åˆé¢„è®­ç»ƒè‡ªç›‘ç£æ¨¡å‹çš„ç¨³å¥è¡¨ç¤ºï¼Œæˆ‘ä»¬çš„æ¡†æ¶é¦–å…ˆæ•æ‰ä¸€èˆ¬æƒ…ç»ªçº¿ç´¢ï¼Œç„¶åç»†åŒ–è‡ªèº«ä»¥é€‚åº”ä¸ªäººæ³¨é‡Šé£æ ¼ã€‚åœ¨IEMOCAPè¯­æ–™åº“ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMeta-PerSERåœ¨å·²è§å’Œæœªè§çš„æ•°æ®åœºæ™¯ä¸­å‡æ˜¾è‘—ä¼˜äºåŸºå‡†æ–¹æ³•ï¼Œå‡¸æ˜¾å…¶åœ¨ä¸ªæ€§åŒ–æƒ…ç»ªè¯†åˆ«æ–¹é¢çš„æ½œåŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>Meta-PerSERæ˜¯ä¸€ä¸ªæ–°çš„å…ƒå­¦ä¹ æ¡†æ¶ï¼Œç”¨äºä¸ªæ€§åŒ–è¯­éŸ³æƒ…ç»ªè¯†åˆ«ï¼ˆSERï¼‰ã€‚</li>
<li>ä¼ ç»Ÿçš„SERç³»ç»Ÿä¾èµ–äºèšåˆæ³¨é‡Šï¼Œè¿™å¯èƒ½å¯¼è‡´é¢„æµ‹çš„ä¸ä¸€è‡´ã€‚</li>
<li>Meta-PerSERé‡‡ç”¨æ¨¡å‹æ— å…³çš„å…ƒå­¦ä¹ æ–¹æ³•ï¼Œå¹¶ç»“åˆå¤šç§æŠ€æœ¯å®ç°å¿«é€Ÿé€‚åº”ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡æ•´åˆé¢„è®­ç»ƒè‡ªç›‘ç£æ¨¡å‹çš„ç¨³å¥è¡¨ç¤ºï¼Œæ•æ‰ä¸€èˆ¬æƒ…ç»ªçº¿ç´¢å¹¶é€‚åº”ä¸ªäººæ³¨é‡Šé£æ ¼ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒMeta-PerSERåœ¨å·²è§å’Œæœªè§çš„æ•°æ®åœºæ™¯ä¸­å‡ä¼˜äºåŸºå‡†æ–¹æ³•ã€‚</li>
<li>Meta-PerSERé€šè¿‡é€‚åº”æ¯ä¸ªå¬ä¼—å¯¹æƒ…ç»ªè§£è¯»çš„ç‹¬ç‰¹æ–¹å¼ï¼Œæé«˜äº†æƒ…ç»ªè¯†åˆ«çš„ä¸ªæ€§åŒ–ç¨‹åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16220">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-98396fab6f4c5d3e136d37e8bc0160f7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7d7977f75f5b3fc31a274d7accffb250.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-17d15c4264b428f552e5e09d9325260c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fca87e70dfa41f2df87ed81868b3973f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-80dcacaf3b3ba3f547674cc3ae5cd2ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3b2ed53ab1212067f259231368920b9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-33ff0db73f47b59732e05cecc4af9651.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Differentiable-K-means-for-Fully-optimized-Discrete-Token-based-ASR"><a href="#Differentiable-K-means-for-Fully-optimized-Discrete-Token-based-ASR" class="headerlink" title="Differentiable K-means for Fully-optimized Discrete Token-based ASR"></a>Differentiable K-means for Fully-optimized Discrete Token-based ASR</h2><p><strong>Authors:Kentaro Onda, Yosuke Kashiwagi, Emiru Tsunoo, Hayato Futami, Shinji Watanabe</strong></p>
<p>Recent studies have highlighted the potential of discrete tokens derived from self-supervised learning (SSL) models for various speech-related tasks. These tokens serve not only as substitutes for text in language modeling but also as intermediate representations for tasks such as automatic speech recognition (ASR). However, discrete tokens are typically obtained via k-means clustering of SSL features independently of downstream tasks, making them suboptimal for specific applications. This paper proposes the use of differentiable k-means, enabling the joint optimization of tokenization and downstream tasks. This approach enables the fine-tuning of the SSL parameters and learning weights for outputs from multiple SSL layers. Experiments were conducted with ASR as a downstream task. ASR accuracy successfully improved owing to the optimized tokens. The acquired tokens also exhibited greater purity of phonetic information, which were found to be useful even in speech resynthesis. </p>
<blockquote>
<p>æœ€è¿‘çš„ç ”ç©¶å·²ç»çªå‡ºäº†è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰æ¨¡å‹ç”Ÿæˆçš„ç¦»æ•£æ ‡è®°åœ¨å¤šç§è¯­éŸ³ç›¸å…³ä»»åŠ¡ä¸­çš„æ½œåŠ›ã€‚è¿™äº›æ ‡è®°ä¸ä»…ç”¨ä½œè¯­è¨€å»ºæ¨¡ä¸­çš„æ–‡æœ¬æ›¿ä»£ï¼Œè¿˜ç”¨ä½œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç­‰ä»»åŠ¡çš„ä¸­é—´è¡¨ç¤ºã€‚ç„¶è€Œï¼Œç¦»æ•£æ ‡è®°é€šå¸¸æ˜¯é€šè¿‡ç‹¬ç«‹äºä¸‹æ¸¸ä»»åŠ¡çš„SSLç‰¹å¾çš„k-å‡å€¼èšç±»è·å¾—çš„ï¼Œè¿™ä½¿å¾—å®ƒä»¬å¯¹äºç‰¹å®šåº”ç”¨è€Œè¨€å¹¶ä¸ç†æƒ³ã€‚æœ¬æ–‡å»ºè®®ä½¿ç”¨å¯å¾®åˆ†çš„k-å‡å€¼æ–¹æ³•ï¼Œå®ç°å¯¹æ ‡è®°åŒ–å’Œä¸‹æ¸¸ä»»åŠ¡çš„è”åˆä¼˜åŒ–ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿå¾®è°ƒSSLå‚æ•°å¹¶å­¦ä¹ æ¥è‡ªå¤šä¸ªSSLå±‚çš„è¾“å‡ºçš„æƒé‡ã€‚ä»¥ASRä½œä¸ºä¸‹æ¸¸ä»»åŠ¡è¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œç”±äºä¼˜åŒ–çš„æ ‡è®°ï¼ŒASRå‡†ç¡®ç‡æˆåŠŸæé«˜ã€‚æ‰€è·å¾—çš„æ ‡è®°è¿˜è¡¨ç°å‡ºæ›´é«˜çš„è¯­éŸ³ä¿¡æ¯çº¯åº¦ï¼Œå³ä½¿åœ¨è¯­éŸ³é‡å»ºä¸­ä¹Ÿè¢«å‘ç°éå¸¸æœ‰ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16207v1">PDF</a> Accepted by Interspeech2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åŸºäºè‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰æ¨¡å‹çš„ç¦»æ•£æ ‡è®°åœ¨è¯­éŸ³ç›¸å…³ä»»åŠ¡ä¸­çš„æ½œåŠ›ã€‚è¿™äº›æ ‡è®°ä¸ä»…å¯ä½œä¸ºè¯­è¨€å»ºæ¨¡ä¸­çš„æ–‡æœ¬æ›¿ä»£å“ï¼Œè¿˜å¯ä½œä¸ºè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç­‰ä»»åŠ¡çš„ä¸­é—´è¡¨ç¤ºå½¢å¼ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„ç¦»æ•£æ ‡è®°æ˜¯é€šè¿‡ç‹¬ç«‹äºä¸‹æ¸¸ä»»åŠ¡çš„SSLç‰¹å¾è¿›è¡Œk-å‡å€¼èšç±»è·å¾—çš„ï¼Œè¿™ä½¿å¾—å®ƒä»¬å¯¹äºç‰¹å®šåº”ç”¨ç¨‹åºè€Œè¨€å¹¶ä¸ç†æƒ³ã€‚æœ¬æ–‡æå‡ºä½¿ç”¨å¯å¾®åˆ†çš„k-å‡å€¼æ–¹æ³•ï¼Œå®ç°æ ‡è®°åŒ–ä¸ä¸‹æ¸¸ä»»åŠ¡çš„è”åˆä¼˜åŒ–ã€‚è¿™ç§æ–¹æ³•å¯ä»¥å¾®è°ƒSSLå‚æ•°å¹¶å­¦ä¹ æ¥è‡ªå¤šä¸ªSSLå±‚çš„è¾“å‡ºæƒé‡ã€‚åœ¨ä¸‹æ¸¸ä»»åŠ¡ï¼ˆå¦‚ASRï¼‰ä¸Šè¿›è¡Œå®éªŒå‘ç°ï¼Œä¼˜åŒ–çš„æ ‡è®°æ˜¾è‘—æé«˜äº†ASRçš„å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼Œè·å¾—çš„æ ‡è®°å…·æœ‰æ›´é«˜çš„è¯­éŸ³ä¿¡æ¯çº¯åº¦ï¼Œåœ¨è¯­éŸ³åˆæˆä¸­åŒæ ·å±•ç°å‡ºè‰¯å¥½çš„è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è‡ªç›‘ç£å­¦ä¹ æ¨¡å‹çš„ç¦»æ•£æ ‡è®°åœ¨è¯­éŸ³ä»»åŠ¡ä¸­æœ‰å¹¿æ³›åº”ç”¨æ½œåŠ›ã€‚</li>
<li>ä¼ ç»Ÿç¦»æ•£æ ‡è®°çš„è·å–æ–¹å¼ç‹¬ç«‹äºä¸‹æ¸¸ä»»åŠ¡ï¼Œå¯èƒ½ä¸é€‚ç”¨äºç‰¹å®šåº”ç”¨ã€‚</li>
<li>å¯å¾®åˆ†çš„k-å‡å€¼æ–¹æ³•ç”¨äºè”åˆä¼˜åŒ–æ ‡è®°åŒ–ä¸ä¸‹æ¸¸ä»»åŠ¡ï¼Œæé«˜æ€§èƒ½ã€‚</li>
<li>ä¼˜åŒ–åçš„æ ‡è®°èƒ½æå‡è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰çš„å‡†ç¡®ç‡ã€‚</li>
<li>è·å¾—çš„æ ‡è®°å…·æœ‰æ›´é«˜çš„è¯­éŸ³ä¿¡æ¯çº¯åº¦ï¼Œå¯ç”¨äºè¯­éŸ³åˆæˆã€‚</li>
<li>SSLå‚æ•°å¯ä»¥é€šè¿‡è¯¥æ–¹æ³•è¿›è¡Œå¾®è°ƒï¼ŒåŒæ—¶å­¦ä¹ å¤šä¸ªSSLå±‚çš„è¾“å‡ºæƒé‡ã€‚</li>
<li>è¯¥æ–¹æ³•å¯¹äºæ”¹è¿›åŸºäºè‡ªç›‘ç£å­¦ä¹ çš„è¯­éŸ³ä»»åŠ¡å…·æœ‰ç§¯ææ„ä¹‰ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16207">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-25e153d7701d62f408668d26be1e00c3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c16b28d6a15a3f770a7e51e8e827d63.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c3deef409de87d8b18f5d51ac2db789f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9063742dd7ddae4e3aced54ca4dd9268.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-724f28ff392c5d9c1dc71e88fcd3f143.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Prosodically-Enhanced-Foreign-Accent-Simulation-by-Discrete-Token-based-Resynthesis-Only-with-Native-Speech-Corpora"><a href="#Prosodically-Enhanced-Foreign-Accent-Simulation-by-Discrete-Token-based-Resynthesis-Only-with-Native-Speech-Corpora" class="headerlink" title="Prosodically Enhanced Foreign Accent Simulation by Discrete Token-based   Resynthesis Only with Native Speech Corpora"></a>Prosodically Enhanced Foreign Accent Simulation by Discrete Token-based   Resynthesis Only with Native Speech Corpora</h2><p><strong>Authors:Kentaro Onda, Keisuke Imoto, Satoru Fukayama, Daisuke Saito, Nobuaki Minematsu</strong></p>
<p>Recently, a method for synthesizing foreign-accented speech only with native speech data using discrete tokens obtained from self-supervised learning (SSL) models was proposed. Considering limited availability of accented speech data, this method is expected to make it much easier to simulate foreign accents. By using the synthesized accented speech as listening materials for humans or training data for automatic speech recognition (ASR), both of them will acquire higher robustness against foreign accents. However, the previous method has a fatal flaw that it cannot reproduce duration-related accents. Durational accents are commonly seen when L2 speakers, whose native language has syllable-timed or mora-timed rhythm, speak stress-timed languages, such as English. In this paper, we integrate duration modification to the previous method to simulate foreign accents more accurately. Experiments show that the proposed method successfully replicates durational accents seen in real L2 speech. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œæå‡ºä¸€ç§ä»…ä½¿ç”¨ä»è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰æ¨¡å‹ä¸­è·å¾—çš„ç¦»æ•£ä»¤ç‰Œåˆæˆå¸¦æœ‰å¤–å›½å£éŸ³çš„è¯­éŸ³çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä»¥åŸç”Ÿè¯­éŸ³æ•°æ®ä¸ºåŸºç¡€ã€‚è€ƒè™‘åˆ°å£éŸ³è¯­éŸ³æ•°æ®çš„å¯ç”¨æ€§æœ‰é™ï¼Œæ­¤æ–¹æ³•å¯ä»¥æå¤§åœ°ç®€åŒ–æ¨¡æ‹Ÿå¤–æ¥å£éŸ³çš„è¿‡ç¨‹ã€‚é€šè¿‡ä½¿ç”¨åˆæˆçš„å£éŸ³è¯­éŸ³ä½œä¸ºäººç±»çš„å¬åŠ›ææ–™æˆ–è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰çš„è®­ç»ƒæ•°æ®ï¼Œä¸¤è€…éƒ½å¯ä»¥æé«˜å¯¹å¤–æ¥å£éŸ³çš„é²æ£’æ€§ã€‚ç„¶è€Œï¼Œä¹‹å‰çš„æ–¹æ³•å­˜åœ¨ä¸€ä¸ªé‡å¤§ç¼ºé™·ï¼Œå³æ— æ³•é‡ç°ä¸æŒç»­æ—¶é—´ç›¸å…³çš„å£éŸ³ã€‚å½“äºŒè¯­ï¼ˆL2ï¼‰è¯´è¯è€…ï¼ˆå…¶æ¯è¯­å…·æœ‰éŸ³èŠ‚å®šæ—¶æˆ–æ‘©æ‹‰å®šæ—¶èŠ‚å¥ï¼‰è¯´ä»¥éŸ³èŠ‚é—´éš”å®šæ—¶ä¸ºä¸»çš„å¦‚è‹±è¯­è¿™æ ·çš„è¯­è¨€æ—¶ï¼Œé€šå¸¸ä¼šé‡åˆ°æŒç»­æ€§å£éŸ³ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†æŒç»­æ—¶é—´ä¿®æ”¹æ•´åˆåˆ°ä¹‹å‰çš„æ–¹æ³•ä¸­ï¼Œä»¥æ›´å‡†ç¡®åœ°æ¨¡æ‹Ÿå¤–æ¥å£éŸ³ã€‚å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•æˆåŠŸåœ°å¤åˆ¶äº†ç°å®ä¸­äºŒè¯­è¯­éŸ³ä¸­æ‰€è§çš„æŒç»­æ€§å£éŸ³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16191v1">PDF</a> Accepted by Interspeech2025</p>
<p><strong>Summary</strong><br>è¯­éŸ³åˆæˆæŠ€æœ¯å–å¾—æ–°è¿›å±•ï¼Œèƒ½å¤Ÿé€šè¿‡è‡ªç›‘ç£å­¦ä¹ æ¨¡å‹è·å–çš„ç¦»æ•£æ ‡è®°ï¼Œä»…ä½¿ç”¨åŸç”Ÿè¯­éŸ³æ•°æ®åˆæˆå¸¦å£éŸ³çš„è¯­éŸ³ã€‚æ–°æ–¹æ³•è§£å†³äº†ä»¥å¾€ä¸èƒ½é‡ç°ä¸æŒç»­æ—¶é—´ç›¸å…³çš„å£éŸ³çš„é—®é¢˜ï¼Œèƒ½å¤Ÿæ›´å‡†ç¡®åœ°æ¨¡æ‹Ÿå¤–è¯­å£éŸ³ï¼Œæé«˜äººç±»å¬ä¼—æˆ–è¯­éŸ³è¯†åˆ«ç³»ç»Ÿçš„ç¨³å¥æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–°çš„è¯­éŸ³åˆæˆæ–¹æ³•åˆ©ç”¨è‡ªç›‘ç£å­¦ä¹ æ¨¡å‹ç”Ÿæˆçš„ç¦»æ•£æ ‡è®°ä»…ä½¿ç”¨åŸç”Ÿè¯­éŸ³æ•°æ®ã€‚</li>
<li>æ–°æ–¹æ³•è§£å†³äº†ä¸èƒ½å‡†ç¡®æ¨¡æ‹Ÿå¤–è¯­å£éŸ³çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯æŒç»­æ—¶é—´ç›¸å…³çš„å£éŸ³ã€‚</li>
<li>æ–°æ–¹æ³•å¯åº”ç”¨äºä¸ºå¬åŠ›è®­ç»ƒææ–™æˆ–è¯­éŸ³è¯†åˆ«è®­ç»ƒæä¾›æ›´é«˜ç¨³å¥æ€§çš„å¸¦å£éŸ³è¯­éŸ³æ¨¡æ‹Ÿã€‚</li>
<li>L2è¯­è¨€ï¼ˆéæ¯è¯­ï¼‰å­¦ä¹ è€…åœ¨å­¦ä¹ ä»¥è‹±è¯­ä¸ºä»£è¡¨çš„å‹åŠ›å®šæ—¶è¯­è¨€æ—¶å¯èƒ½é‡åˆ°çš„å£éŸ³é—®é¢˜å¾—åˆ°å…³æ³¨ã€‚</li>
<li>å®éªŒè¡¨æ˜æ–°æ–¹æ³•èƒ½å¤ŸæˆåŠŸæ¨¡æ‹ŸçœŸå®L2è¯­éŸ³ä¸­çš„æŒç»­å£éŸ³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16191">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-77b61769ff7788cfb9b9f38177ec72c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-09888464577831f235bbd7d57deec21a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a4c0705e5b4ad6730af037a99fc0d07e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8d00bdca2562f52b31ba2fa9364aa24d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d4b23374cbbd84a0db213ebecffbb56b.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Selective-Invocation-for-Multilingual-ASR-A-Cost-effective-Approach-Adapting-to-Speech-Recognition-Difficulty"><a href="#Selective-Invocation-for-Multilingual-ASR-A-Cost-effective-Approach-Adapting-to-Speech-Recognition-Difficulty" class="headerlink" title="Selective Invocation for Multilingual ASR: A Cost-effective Approach   Adapting to Speech Recognition Difficulty"></a>Selective Invocation for Multilingual ASR: A Cost-effective Approach   Adapting to Speech Recognition Difficulty</h2><p><strong>Authors:Hongfei Xue, Yufeng Tang, Jun Zhang, Xuelong Geng, Lei Xie</strong></p>
<p>Although multilingual automatic speech recognition (ASR) systems have significantly advanced, enabling a single model to handle multiple languages, inherent linguistic differences and data imbalances challenge SOTA performance across all languages. While language identification (LID) models can route speech to the appropriate ASR model, they incur high costs from invoking SOTA commercial models and suffer from inaccuracies due to misclassification. To overcome these, we propose SIMA, a selective invocation for multilingual ASR that adapts to the difficulty level of the input speech. Built on a spoken large language model (SLLM), SIMA evaluates whether the input is simple enough for direct transcription or requires the invocation of a SOTA ASR model. Our approach reduces word error rates by 18.7% compared to the SLLM and halves invocation costs compared to LID-based methods. Tests on three datasets show that SIMA is a scalable, cost-effective solution for multilingual ASR applications. </p>
<blockquote>
<p>è™½ç„¶å¤šè¯­è¨€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿå·²ç»å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½¿å¾—å•ä¸€æ¨¡å‹èƒ½å¤Ÿå¤„ç†å¤šç§è¯­è¨€ï¼Œä½†å›ºæœ‰çš„è¯­è¨€å·®å¼‚å’Œæ•°æ®ä¸å¹³è¡¡ä»ç„¶å¯¹æ‰€æœ‰è¯­è¨€çš„æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼ˆSOTAï¼‰æ€§èƒ½æ„æˆäº†æŒ‘æˆ˜ã€‚è™½ç„¶è¯­è¨€è¯†åˆ«ï¼ˆLIDï¼‰æ¨¡å‹å¯ä»¥å°†è¯­éŸ³è·¯ç”±åˆ°é€‚å½“çš„ASRæ¨¡å‹ï¼Œä½†å®ƒä»¬ä¼šå› è°ƒç”¨æœ€æ–°æŠ€æœ¯çš„å•†ä¸šæ¨¡å‹è€Œäº§ç”Ÿé«˜æ˜‚çš„æˆæœ¬ï¼Œå¹¶ä¸”ç”±äºè¯¯åˆ†ç±»è€Œé¢ä¸´ä¸ç²¾ç¡®çš„é—®é¢˜ã€‚ä¸ºäº†å…‹æœè¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SIMAï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºå¤šè¯­è¨€ASRçš„é€‰æ‹©æ€§è°ƒç”¨æ–¹æ³•ï¼Œå®ƒèƒ½å¤Ÿé€‚åº”è¾“å…¥è¯­éŸ³çš„éš¾åº¦æ°´å¹³ã€‚åŸºäºå£è¯­å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆSLLMï¼‰ï¼ŒSIMAè¯„ä¼°è¾“å…¥æ˜¯å¦è¶³å¤Ÿç®€å•ä»¥è¿›è¡Œç›´æ¥è½¬å½•ï¼Œæˆ–è€…æ˜¯å¦éœ€è¦è°ƒç”¨æœ€æ–°æŠ€æœ¯çš„ASRæ¨¡å‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸SLLMç›¸æ¯”ï¼Œå°†å•è¯é”™è¯¯ç‡é™ä½äº†18.7%ï¼Œå¹¶å°†åŸºäºLIDçš„æ–¹æ³•çš„è°ƒç”¨æˆæœ¬å‡åŠã€‚åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šçš„æµ‹è¯•è¡¨æ˜ï¼ŒSIMAæ˜¯ä¸€ç§å¯ç”¨äºå¤šè¯­è¨€ASRåº”ç”¨çš„å¯æ‰©å±•ä¸”ç»æµé«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16168v1">PDF</a> Accepted by INTERSPEECH 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤šè¯­è¨€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿçš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬è¯­è¨€å·®å¼‚å’Œæ•°æ®ä¸å¹³è¡¡é—®é¢˜ã€‚è™½ç„¶è¯­è¨€è¯†åˆ«ï¼ˆLIDï¼‰æ¨¡å‹å¯ä»¥å°†è¯­éŸ³è·¯ç”±åˆ°é€‚å½“çš„ASRæ¨¡å‹ï¼Œä½†å®ƒä»¬è°ƒç”¨å…ˆè¿›å•†ä¸šæ¨¡å‹çš„æˆæœ¬é«˜æ˜‚ï¼Œå¹¶ä¸”ç”±äºè¯¯åˆ†ç±»è€Œå­˜åœ¨ä¸å‡†ç¡®çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†SIMAï¼Œä¸€ç§é€‚åº”è¾“å…¥è¯­éŸ³éš¾åº¦çº§åˆ«çš„é€‰æ‹©æ€§è°ƒç”¨å¤šè¯­è¨€ASRçš„æ–¹æ³•ã€‚SIMAå»ºç«‹åœ¨å£è¯­å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆSLLMï¼‰ä¹‹ä¸Šï¼Œè¯„ä¼°è¾“å…¥æ˜¯å¦ç®€å•åˆ°å¯ä»¥ç›´æ¥è½¬å½•ï¼Œè¿˜æ˜¯éœ€è¦è°ƒç”¨é«˜çº§ASRæ¨¡å‹ã€‚è¯¥æ–¹æ³•å°†å­—è¯é”™è¯¯ç‡é™ä½äº†18.7%ï¼Œä¸SLLMç›¸æ¯”ï¼Œå¹¶å°†è°ƒç”¨æˆæœ¬é™ä½äº†ä¸€åŠã€‚åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šçš„æµ‹è¯•è¡¨æ˜ï¼ŒSIMAæ˜¯ä¸€ç§å¯æ‰©å±•ä¸”ç»æµå®æƒ çš„å¤šè¯­è¨€ASRåº”ç”¨ç¨‹åºè§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šè¯­è¨€ASRç³»ç»Ÿè™½ç„¶æœ‰æ‰€å‘å±•ï¼Œä½†ä»é¢ä¸´è¯­è¨€å·®å¼‚å’Œæ•°æ®ä¸å¹³è¡¡çš„æŒ‘æˆ˜ã€‚</li>
<li>LIDæ¨¡å‹åœ¨è·¯ç”±è¯­éŸ³åˆ°é€‚å½“çš„ASRæ¨¡å‹æ—¶ï¼Œå­˜åœ¨é«˜æˆæœ¬å’Œè¯¯åˆ†ç±»çš„é—®é¢˜ã€‚</li>
<li>SIMAæ˜¯ä¸€ç§åŸºäºå£è¯­å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆSLLMï¼‰çš„é€‰æ‹©æ€§è°ƒç”¨æ–¹æ³•ï¼Œé€‚åº”è¾“å…¥è¯­éŸ³çš„éš¾åº¦çº§åˆ«ã€‚</li>
<li>SIMAèƒ½å¤Ÿè¯„ä¼°è¾“å…¥æ˜¯å¦ç®€å•åˆ°å¯ä»¥ç›´æ¥è½¬å½•ï¼Œæˆ–éœ€è¦è°ƒç”¨é«˜çº§ASRæ¨¡å‹ã€‚</li>
<li>SIMAå°†å­—è¯é”™è¯¯ç‡é™ä½äº†18.7%ï¼Œä¸SLLMç›¸æ¯”å…·æœ‰ä¼˜è¶Šæ€§ã€‚</li>
<li>SIMAå°†è°ƒç”¨æˆæœ¬é™ä½äº†ä¸€åŠï¼Œç›¸æ¯”LID-basedæ–¹æ³•æ›´å…·æˆæœ¬æ•ˆç›Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16168">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-961365af1a6e5d68f765081828777394.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-05946d78f30447ee15527160b58d8592.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-183099b8b25d0ffe599ae8e6312c9b82.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21d9ece0f55ea572778acacd4a4fe8b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d0d1a055c452691cb63a26667d67736.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b2a32df295be90c57ba2a418d0267a8e.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Efficient-and-Direct-Duplex-Modeling-for-Speech-to-Speech-Language-Model"><a href="#Efficient-and-Direct-Duplex-Modeling-for-Speech-to-Speech-Language-Model" class="headerlink" title="Efficient and Direct Duplex Modeling for Speech-to-Speech Language Model"></a>Efficient and Direct Duplex Modeling for Speech-to-Speech Language Model</h2><p><strong>Authors:Ke Hu, Ehsan Hosseini-Asl, Chen Chen, Edresson Casanova, Subhankar Ghosh, Piotr Å»elasko, Zhehuai Chen, Jason Li, Jagadeesh Balam, Boris Ginsburg</strong></p>
<p>Spoken dialogue is an intuitive form of human-computer interaction, yet current speech language models often remain constrained to turn-based exchanges, lacking real-time adaptability such as user barge-in. We propose a novel duplex speech to speech (S2S) architecture featuring continuous user inputs and codec agent outputs with channel fusion that directly models simultaneous user and agent streams. Using a pretrained streaming encoder for user input enables the first duplex S2S model without requiring speech pretrain. Separate architectures for agent and user modeling facilitate codec fine-tuning for better agent voices and halve the bitrate (0.6 kbps) compared to previous works. Experimental results show that the proposed model outperforms previous duplex models in reasoning, turn-taking, and barge-in abilities. The model requires significantly less speech data, as speech pretrain is skipped, which markedly simplifies the process of building a duplex S2S model from any LLMs. Finally, it is the first openly available duplex S2S model with training and inference code to foster reproducibility. </p>
<blockquote>
<p>å£è¯­å¯¹è¯æ˜¯äººç±»ä¸è®¡ç®—æœºäº¤äº’çš„ä¸€ç§ç›´è§‚å½¢å¼ã€‚ç„¶è€Œï¼Œå½“å‰çš„è¯­éŸ³è¯­è¨€æ¨¡å‹é€šå¸¸ä»…é™äºåŸºäºå›åˆçš„äº¤äº’ï¼Œç¼ºä¹å®æ—¶é€‚åº”æ€§ï¼Œå¦‚ç”¨æˆ·æŠ¢è¯ç­‰ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„åŒè¯­è¯­éŸ³åˆ°è¯­éŸ³ï¼ˆS2Sï¼‰æ¶æ„ï¼Œå…·æœ‰è¿ç»­ç”¨æˆ·è¾“å…¥å’Œç¼–è§£ç å™¨ä»£ç†è¾“å‡ºï¼Œé€šè¿‡é€šé“èåˆç›´æ¥æ¨¡æ‹Ÿç”¨æˆ·å’Œä»£ç†çš„åŒæ—¶æµã€‚ä½¿ç”¨é¢„è®­ç»ƒçš„æµå¼ç¼–ç å™¨è¿›è¡Œç”¨æˆ·è¾“å…¥ï¼Œä½¿å¾—ç¬¬ä¸€ä¸ªåŒè¯­S2Sæ¨¡å‹æ— éœ€è¯­éŸ³é¢„è®­ç»ƒã€‚ä¸ºä»£ç†å’Œç”¨æˆ·å»ºæ¨¡æä¾›å•ç‹¬çš„ç»“æ„ï¼Œä¾¿äºç¼–è§£ç å™¨å¾®è°ƒä»¥è·å–æ›´å¥½çš„ä»£ç†è¯­éŸ³ï¼Œä¸ä»¥å‰çš„å·¥ä½œç›¸æ¯”ï¼Œæ¯”ç‰¹ç‡å‡åŠï¼ˆ0.6kbpsï¼‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨æ¨ç†ã€è¯è½®è½¬æ¢å’ŒæŠ¢è¯èƒ½åŠ›æ–¹é¢ä¼˜äºä¹‹å‰çš„åŒè¯­æ¨¡å‹ã€‚è¯¥æ¨¡å‹éœ€è¦çš„è¯­éŸ³æ•°æ®å¤§å¤§å‡å°‘ï¼Œå› ä¸ºè·³è¿‡äº†è¯­éŸ³é¢„è®­ç»ƒï¼Œè¿™æå¤§åœ°ç®€åŒ–äº†ä»ä»»ä½•å¤§å‹è¯­è¨€æ¨¡å‹æ„å»ºåŒè¯­S2Sæ¨¡å‹çš„è¿‡ç¨‹ã€‚æœ€åï¼Œå®ƒæ˜¯ç¬¬ä¸€ä¸ªå…¬å¼€å¯ç”¨çš„å¸¦æœ‰è®­ç»ƒå’Œæ¨ç†ä»£ç çš„åŒè¯­S2Sæ¨¡å‹ï¼Œæœ‰åˆ©äºä¿ƒè¿›å¯é‡å¤æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15670v1">PDF</a> Accepted to Interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>æ–‡ç« æå‡ºä¸€ç§æ–°å‹åŒå·¥è¯­éŸ³åˆ°è¯­éŸ³ï¼ˆS2Sï¼‰æ¶æ„ï¼Œæ”¯æŒè¿ç»­ç”¨æˆ·è¾“å…¥å’Œç¼–ç è§£ç å™¨è¾“å‡ºï¼Œé€šè¿‡ä¿¡é“èåˆç›´æ¥æ¨¡æ‹Ÿç”¨æˆ·å’Œä»£ç†çš„å®æ—¶äº¤äº’æµã€‚è¯¥æ¶æ„ä½¿ç”¨é¢„è®­ç»ƒçš„æµå¼ç¼–ç å™¨å®ç°ç”¨æˆ·è¾“å…¥ï¼Œæ— éœ€è¯­éŸ³é¢„è®­ç»ƒå³å¯æ„å»ºé¦–ä¸ªåŒå·¥S2Sæ¨¡å‹ã€‚é€šè¿‡åˆ†åˆ«æ„å»ºä»£ç†å’Œç”¨æˆ·æ¨¡å‹ï¼Œä¼˜åŒ–ç¼–ç è§£ç å™¨ä»¥æ”¹å–„ä»£ç†è¯­éŸ³è´¨é‡ï¼ŒåŒæ—¶ä¸ä»¥å¾€å·¥ä½œç›¸æ¯”å°†æ¯”ç‰¹ç‡å‡åŠï¼ˆé™è‡³0.6kbpsï¼‰ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨æ¨ç†ã€è½®æ›¿å’Œæ‰“æ–­èƒ½åŠ›ä¸Šä¼˜äºä¹‹å‰çš„åŒå·¥æ¨¡å‹ã€‚æ­¤å¤–ï¼Œç”±äºçœç•¥äº†è¯­éŸ³é¢„è®­ç»ƒï¼Œè¯¥æ¨¡å‹æ‰€éœ€è¯­éŸ³æ•°æ®é‡å¤§å¤§å‡å°‘ï¼Œæ˜¾è‘—ç®€åŒ–äº†ä»ä»»ä½•å¤§å‹è¯­è¨€æ¨¡å‹æ„å»ºåŒå·¥S2Sæ¨¡å‹çš„è¿‡ç¨‹ã€‚è¿™æ˜¯é¦–ä¸ªå…¬å¼€å¯ç”¨çš„åŒå·¥S2Sæ¨¡å‹ï¼Œé™„æœ‰è®­ç»ƒå’Œæ¨ç†ä»£ç ï¼Œä¾¿äºé‡å¤å®éªŒå’Œè¿›ä¸€æ­¥å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºä¸€ç§æ–°å‹åŒå·¥è¯­éŸ³åˆ°è¯­éŸ³ï¼ˆS2Sï¼‰æ¶æ„ï¼Œæ”¯æŒè¿ç»­ç”¨æˆ·è¾“å…¥å’Œç¼–ç è§£ç å™¨è¾“å‡ºã€‚</li>
<li>é€šè¿‡ä¿¡é“èåˆå®ç°ç”¨æˆ·å’Œä»£ç†çš„å®æ—¶äº¤äº’æµæ¨¡æ‹Ÿã€‚</li>
<li>ä½¿ç”¨é¢„è®­ç»ƒçš„æµå¼ç¼–ç å™¨ï¼Œæ— éœ€è¯­éŸ³é¢„è®­ç»ƒå³å¯æ„å»ºåŒå·¥S2Sæ¨¡å‹ã€‚</li>
<li>åˆ†ç¦»ä»£ç†å’Œç”¨æˆ·å»ºæ¨¡ä¼˜åŒ–ç¼–ç è§£ç å™¨ï¼Œæ”¹å–„ä»£ç†è¯­éŸ³è´¨é‡å¹¶é™ä½æ¯”ç‰¹ç‡ã€‚</li>
<li>å®éªŒè¯æ˜è¯¥æ¨¡å‹åœ¨æ¨ç†ã€è½®æ›¿å’Œæ‰“æ–­èƒ½åŠ›ä¸Šè¶…è¶Šä¹‹å‰çš„åŒå·¥æ¨¡å‹ã€‚</li>
<li>æ‰€éœ€è¯­éŸ³æ•°æ®é‡å‡å°‘ï¼Œç®€åŒ–ä»å¤§å‹è¯­è¨€æ¨¡å‹æ„å»ºåŒå·¥S2Sæ¨¡å‹çš„è¿‡ç¨‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15670">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-48ced667046baab251e5e9e436d652f3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-29e57d7b58e499b2319ef470f5dc7386.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-141ffcba3e23d56eeb8f42246523a56e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d134d6e2425d0c178372442ddd484c9d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce72e53da69534c9b7cba405760c607b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-79e6ccc0956dc107cff583a134db8993.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Prosody-Adaptable-Audio-Codecs-for-Zero-Shot-Voice-Conversion-via-In-Context-Learning"><a href="#Prosody-Adaptable-Audio-Codecs-for-Zero-Shot-Voice-Conversion-via-In-Context-Learning" class="headerlink" title="Prosody-Adaptable Audio Codecs for Zero-Shot Voice Conversion via   In-Context Learning"></a>Prosody-Adaptable Audio Codecs for Zero-Shot Voice Conversion via   In-Context Learning</h2><p><strong>Authors:Junchuan Zhao, Xintong Wang, Ye Wang</strong></p>
<p>Recent advances in discrete audio codecs have significantly improved speech representation modeling, while codec language models have enabled in-context learning for zero-shot speech synthesis. Inspired by this, we propose a voice conversion (VC) model within the VALLE-X framework, leveraging its strong in-context learning capabilities for speaker adaptation. To enhance prosody control, we introduce a prosody-aware audio codec encoder (PACE) module, which isolates and refines prosody from other sources, improving expressiveness and control. By integrating PACE into our VC model, we achieve greater flexibility in prosody manipulation while preserving speaker timbre. Experimental evaluation results demonstrate that our approach outperforms baseline VC systems in prosody preservation, timbre consistency, and overall naturalness, surpassing baseline VC systems. </p>
<blockquote>
<p>è¿‘æœŸç¦»æ•£éŸ³é¢‘ç¼–è§£ç å™¨çš„è¿›å±•æå¤§åœ°æ”¹è¿›äº†è¯­éŸ³è¡¨ç¤ºå»ºæ¨¡ï¼Œè€Œç¼–è§£ç å™¨è¯­è¨€æ¨¡å‹å·²ç»å®ç°äº†é›¶å°„å‡»è¯­éŸ³åˆæˆçš„ä¸Šä¸‹æ–‡å­¦ä¹ ã€‚å—æ­¤å¯å‘ï¼Œæˆ‘ä»¬åœ¨VALLE-Xæ¡†æ¶å†…æå‡ºäº†ä¸€ä¸ªè¯­éŸ³è½¬æ¢ï¼ˆVCï¼‰æ¨¡å‹ï¼Œåˆ©ç”¨å…¶å¼ºå¤§çš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›è¿›è¡Œè¯´è¯äººé€‚é…ã€‚ä¸ºäº†å¢å¼ºéŸµå¾‹æ§åˆ¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªéŸµå¾‹æ„ŸçŸ¥éŸ³é¢‘ç¼–è§£ç å™¨ç¼–ç å™¨ï¼ˆPACEï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—å¯ä»¥éš”ç¦»å¹¶ä¼˜åŒ–éŸµå¾‹çš„æ¥æºï¼Œä»è€Œæé«˜è¡¨è¾¾åŠ›å’Œæ§åˆ¶åŠ›ã€‚é€šè¿‡å°†PACEé›†æˆåˆ°æˆ‘ä»¬çš„VCæ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬åœ¨éŸµå¾‹æ“çºµæ–¹é¢å®ç°äº†æ›´å¤§çš„çµæ´»æ€§ï¼ŒåŒæ—¶ä¿ç•™äº†è¯´è¯äººçš„éŸ³è‰²ã€‚å®éªŒè¯„ä¼°ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨éŸµå¾‹ä¿æŒã€éŸ³è‰²ä¸€è‡´æ€§å’Œæ•´ä½“è‡ªç„¶åº¦æ–¹é¢è¶…è¿‡äº†åŸºçº¿VCç³»ç»Ÿã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15402v1">PDF</a> 5 pages, 3 figures</p>
<p><strong>Summary</strong></p>
<p>æœ€æ–°çš„ç¦»æ•£éŸ³é¢‘ç¼–è§£ç å™¨æŠ€æœ¯çš„è¿›å±•ä¸ºè¯­éŸ³è¡¨ç°å»ºæ¨¡å¸¦æ¥äº†é‡å¤§æ”¹è¿›ï¼Œç¼–è§£ç å™¨è¯­è¨€æ¨¡å‹å®ç°äº†é›¶æ ·æœ¬è¯­éŸ³åˆæˆçš„ä¸Šä¸‹æ–‡å­¦ä¹ ã€‚å—æ­¤å¯å‘ï¼Œæˆ‘ä»¬åœ¨VALLE-Xæ¡†æ¶å†…æå‡ºäº†ä¸€ç§è¯­éŸ³è½¬æ¢ï¼ˆVCï¼‰æ¨¡å‹ï¼Œåˆ©ç”¨å…¶å¼ºå¤§çš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›è¿›è¡Œè‡ªé€‚åº”è¯´è¯äººé€‚é…ã€‚ä¸ºäº†å¢å¼ºéŸµå¾‹æ§åˆ¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†éŸµå¾‹æ„ŸçŸ¥éŸ³é¢‘ç¼–è§£ç å™¨ç¼–ç å™¨ï¼ˆPACEï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—èƒ½éš”ç¦»å¹¶ç²¾ç‚¼éŸµå¾‹æ¥æºï¼Œæé«˜äº†è¡¨è¾¾åŠ›å’Œæ§åˆ¶åŠ›ã€‚å°†PACEé›†æˆåˆ°æˆ‘ä»¬çš„VCæ¨¡å‹ä¸­ï¼Œå®ç°äº†åœ¨ä¿æŒè¯´è¯äººéŸ³è´¨çš„åŒæ—¶ï¼ŒéŸµå¾‹æ“æ§æ›´åŠ çµæ´»ã€‚å®éªŒè¯„ä¼°ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¿æŒéŸµå¾‹ã€éŸ³è´¨ä¸€è‡´æ€§å’Œæ•´ä½“è‡ªç„¶åº¦æ–¹é¢è¶…è¶Šäº†åŸºçº¿VCç³»ç»Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¦»æ•£éŸ³é¢‘ç¼–è§£ç å™¨çš„æœ€æ–°è¿›å±•æ˜¾è‘—æ”¹è¿›äº†è¯­éŸ³è¡¨ç°å»ºæ¨¡ã€‚</li>
<li>ç¼–è§£ç å™¨è¯­è¨€æ¨¡å‹å®ç°äº†é›¶æ ·æœ¬è¯­éŸ³åˆæˆçš„ä¸Šä¸‹æ–‡å­¦ä¹ ã€‚</li>
<li>æå‡ºäº†åœ¨VALLE-Xæ¡†æ¶å†…çš„è¯­éŸ³è½¬æ¢ï¼ˆVCï¼‰æ¨¡å‹ï¼Œå…·æœ‰å¼ºå¤§çš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ï¼Œç”¨äºè‡ªé€‚åº”è¯´è¯äººé€‚é…ã€‚</li>
<li>å¼•å…¥äº†éŸµå¾‹æ„ŸçŸ¥éŸ³é¢‘ç¼–è§£ç å™¨ç¼–ç å™¨ï¼ˆPACEï¼‰æ¨¡å—ï¼Œä»¥æé«˜éŸµå¾‹æ§åˆ¶å’Œè¡¨è¾¾åŠ›ã€‚</li>
<li>PACEé›†æˆåˆ°VCæ¨¡å‹ä¸­ï¼Œå®ç°äº†æ›´çµæ´»çš„éŸµå¾‹æ“æ§ï¼ŒåŒæ—¶ä¿æŒè¯´è¯äººéŸ³è´¨ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•çš„éŸµå¾‹ä¿æŒã€éŸ³è´¨ä¸€è‡´æ€§å’Œè‡ªç„¶åº¦å‡è¶…è¶ŠåŸºçº¿VCç³»ç»Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15402">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ba1cba21ed3de6bc44389948aa5b00e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1e4201e7de303229190d1eea635ba23.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e3646ee87f32d11323a55446a1552483.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-32184b8a627ad4363f6d6f2252a3a6df.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1880b7f97a55df6b9a2c14ad26fa717f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f38d0b65c8dfaeb9c81bb1e530eac4c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5c2b129d197257c131aeb99eebbd2fd2.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Analysis-of-ABC-Frontend-Audio-Systems-for-the-NIST-SRE24"><a href="#Analysis-of-ABC-Frontend-Audio-Systems-for-the-NIST-SRE24" class="headerlink" title="Analysis of ABC Frontend Audio Systems for the NIST-SRE24"></a>Analysis of ABC Frontend Audio Systems for the NIST-SRE24</h2><p><strong>Authors:Sara Barahona, Anna Silnova, Ladislav MoÅ¡ner, Junyi Peng, OldÅ™ich Plchot, Johan Rohdin, Lin Zhang, Jiangyu Han, Petr Palka, Federico Landini, LukÃ¡Å¡ Burget, Themos Stafylakis, Sandro Cumani, Dominik BoboÅ¡, Miroslav HlavaÄek, Martin Kodovsky, TomÃ¡Å¡ PavlÃ­Äek</strong></p>
<p>We present a comprehensive analysis of the embedding extractors (frontends) developed by the ABC team for the audio track of NIST SRE 2024. We follow the two scenarios imposed by NIST: using only a provided set of telephone recordings for training (fixed) or adding publicly available data (open condition). Under these constraints, we develop the best possible speaker embedding extractors for the pre-dominant conversational telephone speech (CTS) domain. We explored architectures based on ResNet with different pooling mechanisms, recently introduced ReDimNet architecture, as well as a system based on the XLS-R model, which represents the family of large pre-trained self-supervised models. In open condition, we train on VoxBlink2 dataset, containing 110 thousand speakers across multiple languages. We observed a good performance and robustness of VoxBlink-trained models, and our experiments show practical recipes for developing state-of-the-art frontends for speaker recognition. </p>
<blockquote>
<p>æˆ‘ä»¬å¯¹ABCå›¢é˜Ÿä¸ºNIST SRE 2024éŸ³é¢‘è½¨é“å¼€å‘åµŒå…¥æå–å™¨ï¼ˆå‰ç«¯ï¼‰è¿›è¡Œäº†ç»¼åˆåˆ†æã€‚æˆ‘ä»¬éµå¾ªNISTè§„å®šçš„ä¸¤ç§åœºæ™¯ï¼šä»…ä½¿ç”¨æä¾›çš„ç”µè¯å½•éŸ³é›†è¿›è¡Œè®­ç»ƒï¼ˆå›ºå®šï¼‰æˆ–æ·»åŠ å…¬å¼€æ•°æ®ï¼ˆå¼€æ”¾æ¡ä»¶ï¼‰ã€‚åœ¨è¿™äº›çº¦æŸä¸‹ï¼Œæˆ‘ä»¬ä¸ºä¸»è¦çš„å¯¹è¯ç”µè¯è¯­éŸ³ï¼ˆCTSï¼‰é¢†åŸŸå¼€å‘æœ€ä½³çš„è¯­éŸ³åµŒå…¥æå–å™¨ã€‚æˆ‘ä»¬æ¢ç´¢äº†åŸºäºResNetçš„ä¸åŒæ± åŒ–æœºåˆ¶æ¶æ„ã€æœ€è¿‘æ¨å‡ºçš„ReDimNetæ¶æ„ï¼Œä»¥åŠåŸºäºXLS-Ræ¨¡å‹çš„ç³»ç»Ÿï¼Œå®ƒä»£è¡¨äº†ä¸€ç±»å¤§å‹é¢„è®­ç»ƒè‡ªç›‘ç£æ¨¡å‹ã€‚åœ¨å¼€æ”¾æ¡ä»¶ä¸‹ï¼Œæˆ‘ä»¬åœ¨åŒ…å«å¤šç§è¯­è¨€çš„VoxBlink2æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°VoxBlinkè®­ç»ƒæ¨¡å‹çš„è‰¯å¥½æ€§èƒ½å’Œç¨³å¥æ€§ï¼Œå¹¶ä¸”æˆ‘ä»¬çš„å®éªŒä¸ºå¼€å‘å…ˆè¿›çš„å‰ç«¯è¯­éŸ³è¯†åˆ«æä¾›äº†å®ç”¨æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15320v1">PDF</a> Accepted at Interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ABCå›¢é˜Ÿé’ˆå¯¹NIST SRE 2024éŸ³é¢‘è½¨é“å¼€å‘çš„åµŒå…¥æå–å™¨ï¼ˆå‰ç«¯ï¼‰çš„ç»¼åˆåˆ†æã€‚æ–‡ç« éµå¾ªNISTè®¾å®šçš„ä¸¤ç§åœºæ™¯ï¼Œä¸€ç§æ˜¯ä»…ä½¿ç”¨æä¾›çš„ç”µè¯å½•éŸ³é›†è¿›è¡Œè®­ç»ƒï¼ˆå›ºå®šï¼‰ï¼Œå¦ä¸€ç§æ˜¯åŠ å…¥å…¬å¼€æ•°æ®ï¼ˆå¼€æ”¾æ¡ä»¶ï¼‰ã€‚åœ¨è¿™äº›é™åˆ¶ä¸‹ï¼Œå›¢é˜Ÿä¸ºä¸»è¦çš„å¯¹è¯ç”µè¯è¯­éŸ³ï¼ˆCTSï¼‰é¢†åŸŸå¼€å‘äº†æœ€ä½³å¯èƒ½çš„è¯´è¯äººåµŒå…¥æå–å™¨ã€‚æ–‡ç« æ¢ç´¢äº†åŸºäºResNetçš„ä¸åŒæ± åŒ–æœºåˆ¶æ¶æ„ã€æ–°æ¨å‡ºçš„ReDimNetæ¶æ„ï¼Œä»¥åŠåŸºäºé¢„è®­ç»ƒè‡ªç›‘ç£æ¨¡å‹å®¶æ—çš„XLS-Ræ¨¡å‹çš„ä½“ç³»ã€‚åœ¨å¼€æ”¾æ¡ä»¶ä¸‹ï¼Œå›¢é˜Ÿå¯¹åŒ…å«å¤šç§è¯­è¨€çš„VoxBlink2æ•°æ®é›†è¿›è¡Œäº†è®­ç»ƒï¼Œè§‚å¯Ÿåˆ°è‰¯å¥½æ€§èƒ½å’Œç¨³å¥æ€§ï¼Œå®éªŒå±•ç¤ºäº†å¼€å‘å‰æ²¿è¯´è¯äººè¯†åˆ«æŠ€æœ¯çš„å®ç”¨æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†ABCå›¢é˜Ÿé’ˆå¯¹NIST SRE 2024éŸ³é¢‘è½¨é“çš„åµŒå…¥æå–å™¨å¼€å‘ã€‚</li>
<li>éµå¾ªNISTè®¾å®šçš„ä¸¤ç§åœºæ™¯ï¼šå›ºå®šåœºæ™¯å’Œå¼€æ”¾æ¡ä»¶åœºæ™¯ã€‚</li>
<li>åœ¨å¯¹è¯ç”µè¯è¯­éŸ³ï¼ˆCTSï¼‰é¢†åŸŸå¼€å‘æœ€ä½³è¯´è¯äººåµŒå…¥æå–å™¨ã€‚</li>
<li>æ¢ç´¢äº†å¤šç§æ¶æ„ï¼ŒåŒ…æ‹¬åŸºäºResNetçš„æ± åŒ–æœºåˆ¶ã€ReDimNetæ¶æ„å’ŒXLS-Ræ¨¡å‹ã€‚</li>
<li>åœ¨å¼€æ”¾æ¡ä»¶ä¸‹ä½¿ç”¨VoxBlink2æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œè¯¥æ•°æ®é›†åŒ…å«å¤šç§è¯­è¨€çš„110åƒåè¯´è¯äººã€‚</li>
<li>VoxBlinkè®­ç»ƒæ¨¡å‹è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½å’Œç¨³å¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15320">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c0c49e43b2febb2629d77a1e832e44cd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-70ce0158d813890e8771801946ce400a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5bf488a35fd15b15bf10b24026cfc33a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9816c1864300f51e90065e20697e16c6.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="MentalMAC-Enhancing-Large-Language-Models-for-Detecting-Mental-Manipulation-via-Multi-Task-Anti-Curriculum-Distillation"><a href="#MentalMAC-Enhancing-Large-Language-Models-for-Detecting-Mental-Manipulation-via-Multi-Task-Anti-Curriculum-Distillation" class="headerlink" title="MentalMAC: Enhancing Large Language Models for Detecting Mental   Manipulation via Multi-Task Anti-Curriculum Distillation"></a>MentalMAC: Enhancing Large Language Models for Detecting Mental   Manipulation via Multi-Task Anti-Curriculum Distillation</h2><p><strong>Authors:Yuansheng Gao, Han Bao, Tong Zhang, Bin Li, Zonghui Wang, Wenzhi Chen</strong></p>
<p>Mental manipulation is a subtle yet pervasive form of psychological abuse that poses serious threats to mental health. Its covert nature and the complexity of manipulation strategies make it challenging to detect, even for state-of-the-art large language models (LLMs). This concealment also hinders the manual collection of large-scale, high-quality annotations essential for training effective models. Although recent efforts have sought to improve LLMsâ€™ performance on this task, progress remains limited due to the scarcity of real-world annotated datasets. To address these challenges, we propose MentalMAC, a multi-task anti-curriculum distillation method that enhances LLMsâ€™ ability to detect mental manipulation in multi-turn dialogue. Our approach includes: (i) EvoSA, an unsupervised data expansion method based on evolutionary operations and speech act theory; (ii) teacher model-generated multi-task supervision; and (iii) progressive knowledge distillation from complex to simpler tasks. We then constructed the ReaMent dataset with 5,000 real-world dialogue samples, using a MentalMAC-distilled model to assist human annotation. Vast experiments demonstrate that our method significantly narrows the gap between student and teacher models and outperforms competitive LLMs across key evaluation metrics. All code, datasets, and checkpoints will be released upon paper acceptance. Warning: This paper contains content that may be offensive to readers. </p>
<blockquote>
<p>å¿ƒç†æ“æ§æ˜¯ä¸€ç§å¾®å¦™è€Œæ™®éçš„å¿ƒç†è™å¾…å½¢å¼ï¼Œå¯¹å¿ƒç†å¥åº·æ„æˆä¸¥é‡å¨èƒã€‚å…¶éšè”½æ€§å’Œç­–ç•¥å¤æ‚æ€§ä½¿å¾—æ£€æµ‹å˜å¾—å…·æœ‰æŒ‘æˆ˜ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¹Ÿéš¾ä»¥è¯†åˆ«ã€‚è¿™ç§éšè”½æ€§ä¹Ÿé˜»ç¢äº†æ‰‹åŠ¨æ”¶é›†å¤§è§„æ¨¡é«˜è´¨é‡æ³¨é‡Šï¼Œè¿™å¯¹è®­ç»ƒæœ‰æ•ˆæ¨¡å‹è‡³å…³é‡è¦ã€‚å°½ç®¡è¿‘æœŸåŠªåŠ›æ—¨åœ¨æé«˜LLMåœ¨æ­¤ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œä½†ç”±äºç¼ºä¹çœŸå®ä¸–ç•Œçš„æ³¨é‡Šæ•°æ®é›†ï¼Œè¿›å±•ä»ç„¶æœ‰é™ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†MentalMACï¼Œè¿™æ˜¯ä¸€ç§å¤šä»»åŠ¡åå‘è¯¾ç¨‹è’¸é¦æ–¹æ³•ï¼Œå¯å¢å¼ºLLMåœ¨å¤šè½®å¯¹è¯ä¸­æ£€æµ‹å¿ƒç†æ“æ§çš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬ï¼š(i) EvoSAï¼Œä¸€ç§åŸºäºè¿›åŒ–æ“ä½œå’Œè¨€è¯­è¡Œä¸ºç†è®ºçš„æ— ç›‘ç£æ•°æ®æ‰©å±•æ–¹æ³•ï¼›(ii)æ•™å¸ˆæ¨¡å‹ç”Ÿæˆçš„å¤šä»»åŠ¡ç›‘ç£ï¼›(iii)ä»å¤æ‚åˆ°ç®€å•ä»»åŠ¡çš„çŸ¥è¯†æ¸è¿›è’¸é¦ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨MentalMACè’¸é¦çš„æ¨¡å‹ååŠ©äººå·¥æ³¨é‡Šï¼Œæ„å»ºäº†åŒ…å«5000ä¸ªçœŸå®ä¸–ç•Œå¯¹è¯æ ·æœ¬çš„ReaMentæ•°æ®é›†ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ç¼©å°äº†å­¦ç”Ÿæ¨¡å‹ä¸æ•™å¸ˆæ¨¡å‹ä¹‹é—´çš„å·®è·ï¼Œå¹¶åœ¨å…³é”®è¯„ä¼°æŒ‡æ ‡ä¸Šä¼˜äºå…¶ä»–LLMã€‚è®ºæ–‡æ¥å—åï¼Œæˆ‘ä»¬å°†å…¬å¼€æ‰€æœ‰ä»£ç ã€æ•°æ®é›†å’Œæ£€æŸ¥ç‚¹ã€‚è­¦å‘Šï¼šæœ¬è®ºæ–‡å«æœ‰å¯èƒ½å¯¹è¯»è€…é€ æˆä¸é€‚çš„å†…å®¹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15255v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡æ¢è®¨äº†å¿ƒç†æ“çºµå¯¹å¿ƒç†å¥åº·çš„ä¸¥é‡å¨èƒï¼Œä»¥åŠç°æœ‰æŠ€æœ¯ä¸‹æ£€æµ‹å¿ƒç†æ“çºµçš„éš¾åº¦ã€‚é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å¤šä»»åŠ¡åè¯¾ç¨‹è’¸é¦æ–¹æ³•MentalMACï¼Œç”¨äºæé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šè½®å¯¹è¯ä¸­æ£€æµ‹å¿ƒç†æ“çºµçš„èƒ½åŠ›ã€‚è¯¥æ–¹æ³•åŒ…æ‹¬æ•°æ®æ‰©å±•æ–¹æ³•EvoSAã€æ•™å¸ˆæ¨¡å‹ç”Ÿæˆçš„å¤šä»»åŠ¡ç›‘ç£ä»¥åŠä»å¤æ‚åˆ°ç®€å•ä»»åŠ¡çš„çŸ¥è¯†è’¸é¦ã€‚åŒæ—¶æ„å»ºäº†RealMentæ•°æ®é›†ç”¨äºè®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ç¼©å°äº†å­¦ç”Ÿä¸æ•™å¸ˆæ¨¡å‹ä¹‹é—´çš„å·®è·ï¼Œå¹¶åœ¨å…³é”®è¯„ä¼°æŒ‡æ ‡ä¸Šä¼˜äºå…¶ä»–LLMã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¿ƒç†æ“çºµæ˜¯ä¸€ç§éšè”½ä¸”æ™®éçš„å¿ƒç†è™å¾…å½¢å¼ï¼Œå¯¹å¿ƒç†å¥åº·æ„æˆä¸¥é‡å¨èƒã€‚</li>
<li>æ£€æµ‹å¿ƒç†æ“çºµæ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªå¤šä»»åŠ¡åè¯¾ç¨‹è’¸é¦æ–¹æ³•MentalMACï¼Œæ—¨åœ¨æé«˜LLMæ£€æµ‹å¿ƒç†æ“çºµçš„èƒ½åŠ›ã€‚</li>
<li>MentalMACåŒ…æ‹¬æ•°æ®æ‰©å±•æ–¹æ³•EvoSAã€æ•™å¸ˆæ¨¡å‹ç”Ÿæˆçš„å¤šä»»åŠ¡ç›‘ç£ä»¥åŠæ¸è¿›çš„çŸ¥è¯†è’¸é¦æŠ€æœ¯ã€‚</li>
<li>æ„å»ºäº†RealMentæ•°æ®é›†ç”¨äºè®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹ï¼Œå…¶ä¸­åŒ…å«5000ä¸ªçœŸå®å¯¹è¯æ ·æœ¬ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒMentalMACæ–¹æ³•æ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶åœ¨å…³é”®è¯„ä¼°æŒ‡æ ‡ä¸Šä¼˜äºå…¶ä»–LLMã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15255">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-8adfbeeff5ddded7131e823ba4a5daad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3e615b4b416ab5e7623e8ffe75af36c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac6ee570f763b9787f7830ab4e30f60b.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Voice-ENHANCE-Speech-Restoration-using-a-Diffusion-based-Voice-Conversion-Framework"><a href="#Voice-ENHANCE-Speech-Restoration-using-a-Diffusion-based-Voice-Conversion-Framework" class="headerlink" title="Voice-ENHANCE: Speech Restoration using a Diffusion-based Voice   Conversion Framework"></a>Voice-ENHANCE: Speech Restoration using a Diffusion-based Voice   Conversion Framework</h2><p><strong>Authors:Kyungguen Byun, Jason Filos, Erik Visser, Sunkuk Moon</strong></p>
<p>We propose a speech enhancement system that combines speaker-agnostic speech restoration with voice conversion (VC) to obtain a studio-level quality speech signal. While voice conversion models are typically used to change speaker characteristics, they can also serve as a means of speech restoration when the target speaker is the same as the source speaker. However, since VC models are vulnerable to noisy conditions, we have included a generative speech restoration (GSR) model at the front end of our proposed system. The GSR model performs noise suppression and restores speech damage incurred during that process without knowledge about the target speaker. The VC stage then uses guidance from clean speaker embeddings to further restore the output speech. By employing this two-stage approach, we have achieved speech quality objective metric scores comparable to state-of-the-art (SOTA) methods across multiple datasets. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»“åˆè¯´è¯è€…æ— å…³çš„è¯­éŸ³æ¢å¤å’Œè¯­éŸ³è½¬æ¢ï¼ˆVCï¼‰çš„è¯­éŸ³å¢å¼ºç³»ç»Ÿï¼Œä»¥è·å¾—å·¥ä½œå®¤çº§åˆ«çš„è¯­éŸ³ä¿¡å·ã€‚è™½ç„¶è¯­éŸ³è½¬æ¢æ¨¡å‹é€šå¸¸ç”¨äºæ”¹å˜è¯´è¯äººçš„ç‰¹å¾ï¼Œä½†å½“ç›®æ ‡è¯´è¯äººä¸æºè¯´è¯äººç›¸åŒæ—¶ï¼Œå®ƒä»¬ä¹Ÿå¯ä»¥ä½œä¸ºè¯­éŸ³æ¢å¤çš„ä¸€ç§æ‰‹æ®µã€‚ç„¶è€Œï¼Œç”±äºVCæ¨¡å‹å¯¹å™ªå£°æ¡ä»¶å¾ˆæ•æ„Ÿï¼Œæˆ‘ä»¬åœ¨æ‰€æå‡ºç³»ç»Ÿçš„å‰ç«¯åŠ å…¥äº†ä¸€ä¸ªç”Ÿæˆå¼è¯­éŸ³æ¢å¤ï¼ˆGSRï¼‰æ¨¡å‹ã€‚GSRæ¨¡å‹æ‰§è¡Œå™ªå£°æŠ‘åˆ¶ï¼Œå¹¶åœ¨æ— éœ€äº†è§£ç›®æ ‡è¯´è¯äººçš„æƒ…å†µä¸‹æ¢å¤åœ¨è¿‡ç¨‹ä¸­å‡ºç°çš„è¯­éŸ³æŸä¼¤ã€‚VCé˜¶æ®µç„¶åä½¿ç”¨å¹²å‡€çš„è¯´è¯äººåµŒå…¥ä½œä¸ºæŒ‡å¯¼ï¼Œè¿›ä¸€æ­¥æ¢å¤è¾“å‡ºè¯­éŸ³ã€‚é€šè¿‡é‡‡ç”¨è¿™ç§ä¸¤é˜¶æ®µçš„æ–¹æ³•ï¼Œæˆ‘ä»¬åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¾¾åˆ°äº†ä¸æœ€æ–°æ–¹æ³•ç›¸å½“çš„è¯­éŸ³è´¨é‡å®¢è§‚æŒ‡æ ‡å¾—åˆ†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15254v1">PDF</a> 5 pages, 3 figures, Accepted to INTERSPEECH 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§ç»“åˆæ— ç‰¹å®šäººè¯­éŸ³æ¢å¤ä¸è¯­éŸ³è½¬æ¢ï¼ˆVCï¼‰çš„è¯­éŸ³å¢å¼ºç³»ç»Ÿï¼Œæ—¨åœ¨è·å¾—å·¥ä½œå®¤çº§åˆ«çš„è¯­éŸ³ä¿¡å·è´¨é‡ã€‚ç³»ç»Ÿå‰ç«¯é‡‡ç”¨ç”Ÿæˆå¼è¯­éŸ³æ¢å¤ï¼ˆGSRï¼‰æ¨¡å‹è¿›è¡Œé™å™ªå’Œæ¢å¤è¯­éŸ³æŸä¼¤ï¼Œæ— éœ€äº†è§£ç›®æ ‡è¯´è¯äººçš„ä¿¡æ¯ã€‚æ¥ç€ï¼ŒVCé˜¶æ®µåˆ©ç”¨å¹²å‡€è¯´è¯äººçš„åµŒå…¥å¼•å¯¼è¿›ä¸€æ­¥æ¢å¤è¾“å‡ºè¯­éŸ³ã€‚é€šè¿‡é‡‡ç”¨ä¸¤é˜¶æ®µæ–¹æ³•ï¼Œå®ç°äº†ä¸å¤šä¸ªæ•°æ®é›†ä¸Šçš„æœ€æ–°æ–¹æ³•ç›¸å½“çš„è¯­éŸ³è´¨é‡å®¢è§‚åº¦é‡åˆ†æ•°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºç»“åˆæ— ç‰¹å®šäººè¯­éŸ³æ¢å¤ä¸è¯­éŸ³è½¬æ¢çš„è¯­éŸ³å¢å¼ºç³»ç»Ÿã€‚</li>
<li>é‡‡ç”¨ç”Ÿæˆå¼è¯­éŸ³æ¢å¤ï¼ˆGSRï¼‰æ¨¡å‹è¿›è¡Œé™å™ªå’Œæ¢å¤è¯­éŸ³æŸä¼¤ã€‚</li>
<li>GSRæ¨¡å‹åœ¨ä¸çŸ¥é“ç›®æ ‡è¯´è¯äººçš„æƒ…å†µä¸‹ï¼Œå¯¹è¯­éŸ³è¿›è¡Œæ¢å¤ã€‚</li>
<li>è¯­éŸ³è½¬æ¢ï¼ˆVCï¼‰é˜¶æ®µä½¿ç”¨å¹²å‡€è¯´è¯äººçš„åµŒå…¥è¿›è¡Œè¿›ä¸€æ­¥çš„è¯­éŸ³æ¢å¤ã€‚</li>
<li>é€šè¿‡ä¸¤é˜¶æ®µæ–¹æ³•å®ç°äº†é«˜è´¨é‡çš„è¯­éŸ³æ¢å¤æ•ˆæœã€‚</li>
<li>ç³»ç»Ÿæ€§èƒ½åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¾¾åˆ°æˆ–è¶…è¿‡ç°æœ‰æŠ€æœ¯æ°´å¹³çš„å®¢è§‚åº¦é‡åˆ†æ•°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15254">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d59c2d34501db73243b4a7f1443b2afa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bbb9efac088abc70343b60180fcf0f01.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f9f073ba1ec4f19b6b566e916ef45a4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-69c2c9e6e193ad061192d732ee552287.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3591141137db54a7b7fe3aa6a89fc08b.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="EASY-Emotion-aware-Speaker-Anonymization-via-Factorized-Distillation"><a href="#EASY-Emotion-aware-Speaker-Anonymization-via-Factorized-Distillation" class="headerlink" title="EASY: Emotion-aware Speaker Anonymization via Factorized Distillation"></a>EASY: Emotion-aware Speaker Anonymization via Factorized Distillation</h2><p><strong>Authors:Jixun Yao, Hexin Liu, Eng Siong Chng, Lei Xie</strong></p>
<p>Emotion plays a significant role in speech interaction, conveyed through tone, pitch, and rhythm, enabling the expression of feelings and intentions beyond words to create a more personalized experience. However, most existing speaker anonymization systems employ parallel disentanglement methods, which only separate speech into linguistic content and speaker identity, often neglecting the preservation of the original emotional state. In this study, we introduce EASY, an emotion-aware speaker anonymization framework. EASY employs a novel sequential disentanglement process to disentangle speaker identity, linguistic content, and emotional representation, modeling each speech attribute in distinct subspaces through a factorized distillation approach. By independently constraining speaker identity and emotional representation, EASY minimizes information leakage, enhancing privacy protection while preserving original linguistic content and emotional state. Experimental results on the VoicePrivacy Challenge official datasets demonstrate that our proposed approach outperforms all baseline systems, effectively protecting speaker privacy while maintaining linguistic content and emotional state. </p>
<blockquote>
<p>æƒ…æ„Ÿåœ¨è¯­éŸ³äº¤äº’ä¸­æ‰®æ¼”ç€é‡è¦è§’è‰²ï¼Œé€šè¿‡è¯­è°ƒã€éŸ³é«˜å’ŒèŠ‚å¥æ¥ä¼ è¾¾ï¼Œä½¿äººä»¬åœ¨è¨€è¯­ä¹‹å¤–èƒ½å¤Ÿè¡¨è¾¾æ„Ÿå—å’Œæ„å›¾ï¼Œä»è€Œåˆ›é€ æ›´åŠ ä¸ªæ€§åŒ–çš„ä½“éªŒã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„è¯´è¯äººåŒ¿ååŒ–ç³»ç»Ÿé‡‡ç”¨å¹¶è¡Œè§£è€¦æ–¹æ³•ï¼Œä»…å°†è¯­éŸ³åˆ†ç¦»ä¸ºè¯­è¨€å†…å®¹å’Œè¯´è¯äººèº«ä»½ï¼Œå¾€å¾€å¿½è§†äº†åŸå§‹æƒ…æ„ŸçŠ¶æ€çš„ä¿æŒã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†EASYï¼Œä¸€ä¸ªæƒ…æ„Ÿæ„ŸçŸ¥çš„è¯´è¯äººåŒ¿ååŒ–æ¡†æ¶ã€‚EASYé‡‡ç”¨æ–°é¢–çš„é¡ºåºè§£è€¦è¿‡ç¨‹æ¥è§£è€¦è¯´è¯äººèº«ä»½ã€è¯­è¨€å†…å®¹å’Œæƒ…æ„Ÿè¡¨ç¤ºï¼Œé€šè¿‡å› å­è’¸é¦æ³•å°†æ¯ç§è¯­éŸ³å±æ€§å»ºæ¨¡åœ¨ä¸åŒçš„å­ç©ºé—´ä¸­ã€‚é€šè¿‡ç‹¬ç«‹çº¦æŸè¯´è¯äººèº«ä»½å’Œæƒ…æ„Ÿè¡¨ç¤ºï¼ŒEASYå‡å°‘äº†ä¿¡æ¯æ³„éœ²ï¼Œåœ¨ä¿æŒåŸå§‹è¯­è¨€å†…å®¹å’Œæƒ…æ„ŸçŠ¶æ€çš„åŒæ—¶å¢å¼ºäº†éšç§ä¿æŠ¤ã€‚åœ¨VoicePrivacy Challengeå®˜æ–¹æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•ä¼˜äºæ‰€æœ‰åŸºçº¿ç³»ç»Ÿï¼Œæœ‰æ•ˆä¿æŠ¤è¯´è¯äººéšç§çš„åŒæ—¶ä¿æŒäº†è¯­è¨€å†…å®¹å’Œæƒ…æ„ŸçŠ¶æ€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15004v1">PDF</a> Accepted by INTERSPEECH 2025</p>
<p><strong>Summary</strong></p>
<p>æƒ…ç»ªåœ¨è¯­éŸ³äº¤äº’ä¸­æ‰®æ¼”ç€é‡è¦çš„è§’è‰²ï¼Œé€šè¿‡è¯­è°ƒã€éŸ³é«˜å’ŒèŠ‚å¥æ¥è¡¨è¾¾æƒ…æ„Ÿå’Œæ„å›¾ï¼Œä½¿ä½“éªŒæ›´åŠ ä¸ªæ€§åŒ–ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤§å¤šæ•°è¯´è¯äººåŒ¿ååŒ–ç³»ç»Ÿé‡‡ç”¨å¹¶è¡Œåˆ†ç¦»æ–¹æ³•ï¼Œä»…å°†è¯­éŸ³åˆ†ç¦»ä¸ºè¯­è¨€å†…å®¹å’Œè¯´è¯äººèº«ä»½ï¼Œå¿½è§†äº†åŸå§‹æƒ…ç»ªçŠ¶æ€çš„ä¿ç•™ã€‚æœ¬ç ”ç©¶ä»‹ç»äº†EASYï¼Œä¸€ç§æƒ…æ„Ÿæ„ŸçŸ¥çš„è¯´è¯äººåŒ¿ååŒ–æ¡†æ¶ã€‚EASYé‡‡ç”¨æ–°é¢–çš„é¡ºåºåˆ†ç¦»è¿‡ç¨‹ï¼Œå°†è¯´è¯äººèº«ä»½ã€è¯­è¨€å†…å®¹å’Œæƒ…æ„Ÿè¡¨ç¤ºè¿›è¡Œåˆ†ç¦»ï¼Œé€šè¿‡å› å­è’¸é¦æ³•å°†æ¯ç§è¯­éŸ³å±æ€§å»ºæ¨¡åœ¨ä¸åŒçš„å­ç©ºé—´ä¸­ã€‚é€šè¿‡ç‹¬ç«‹çº¦æŸè¯´è¯äººèº«ä»½å’Œæƒ…æ„Ÿè¡¨ç¤ºï¼ŒEASYå‡å°‘äº†ä¿¡æ¯æ³„éœ²ï¼Œå¢å¼ºäº†éšç§ä¿æŠ¤ï¼ŒåŒæ—¶ä¿ç•™äº†åŸå§‹çš„è¯­è¨€å†…å®¹å’Œæƒ…æ„ŸçŠ¶æ€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨VoicePrivacy Challengeå®˜æ–¹æ•°æ®é›†ä¸Šï¼Œæœ¬ç ”ç©¶æå‡ºçš„æ–¹æ³•ä¼˜äºæ‰€æœ‰åŸºçº¿ç³»ç»Ÿï¼Œæœ‰æ•ˆåœ°ä¿æŠ¤äº†è¯´è¯äººçš„éšç§ï¼ŒåŒæ—¶ä¿æŒäº†è¯­è¨€å†…å®¹å’Œæƒ…æ„ŸçŠ¶æ€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æƒ…æ„Ÿåœ¨è¯­éŸ³äº¤äº’ä¸­èµ·åˆ°é‡è¦ä½œç”¨ï¼Œé€šè¿‡è¯­è°ƒã€éŸ³é«˜å’ŒèŠ‚å¥è¡¨è¾¾æƒ…æ„Ÿå’Œæ„å›¾ã€‚</li>
<li>ç°æœ‰çš„è¯´è¯äººåŒ¿ååŒ–ç³»ç»Ÿä¸»è¦å…³æ³¨äºåˆ†ç¦»è¯­è¨€å†…å®¹å’Œè¯´è¯äººèº«ä»½ï¼Œå®¹æ˜“å¿½è§†åŸå§‹æƒ…æ„ŸçŠ¶æ€çš„ä¿ç•™ã€‚</li>
<li>EASYæ¡†æ¶æ˜¯ä¸€ç§æƒ…æ„Ÿæ„ŸçŸ¥çš„è¯´è¯äººåŒ¿ååŒ–æ–¹æ³•ï¼Œèƒ½å¤Ÿåˆ†ç¦»è¯´è¯äººèº«ä»½ã€è¯­è¨€å†…å®¹å’Œæƒ…æ„Ÿè¡¨ç¤ºã€‚</li>
<li>EASYé‡‡ç”¨å› å­è’¸é¦æ³•ï¼Œå°†æ¯ç§è¯­éŸ³å±æ€§å»ºæ¨¡åœ¨ç‹¬ç«‹çš„å­ç©ºé—´ä¸­ã€‚</li>
<li>EASYé€šè¿‡ç‹¬ç«‹çº¦æŸè¯´è¯äººèº«ä»½å’Œæƒ…æ„Ÿè¡¨ç¤ºï¼Œå‡å°‘äº†ä¿¡æ¯æ³„éœ²ï¼Œå¢å¼ºäº†éšç§ä¿æŠ¤ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒEASYåœ¨ä¿æŠ¤è¯´è¯äººéšç§çš„åŒæ—¶ï¼Œèƒ½å¤Ÿä¿æŒè¯­è¨€å†…å®¹å’Œæƒ…æ„ŸçŠ¶æ€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15004">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7ee9a1493bb5bea6f5fec67ffa6c3d5e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-15a36b4b664f73dd2bbcea98739052c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-14a4e3300ae37e46159b13792cc36ef8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-339c3ee39b7532bd9777afad6ec54210.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a8fd3e4275693a3456e39bb4fb461e6d.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="TCSinger-2-Customizable-Multilingual-Zero-shot-Singing-Voice-Synthesis"><a href="#TCSinger-2-Customizable-Multilingual-Zero-shot-Singing-Voice-Synthesis" class="headerlink" title="TCSinger 2: Customizable Multilingual Zero-shot Singing Voice Synthesis"></a>TCSinger 2: Customizable Multilingual Zero-shot Singing Voice Synthesis</h2><p><strong>Authors:Yu Zhang, Wenxiang Guo, Changhao Pan, Dongyu Yao, Zhiyuan Zhu, Ziyue Jiang, Yuhan Wang, Tao Jin, Zhou Zhao</strong></p>
<p>Customizable multilingual zero-shot singing voice synthesis (SVS) has various potential applications in music composition and short video dubbing. However, existing SVS models overly depend on phoneme and note boundary annotations, limiting their robustness in zero-shot scenarios and producing poor transitions between phonemes and notes. Moreover, they also lack effective multi-level style control via diverse prompts. To overcome these challenges, we introduce TCSinger 2, a multi-task multilingual zero-shot SVS model with style transfer and style control based on various prompts. TCSinger 2 mainly includes three key modules: 1) Blurred Boundary Content (BBC) Encoder, predicts duration, extends content embedding, and applies masking to the boundaries to enable smooth transitions. 2) Custom Audio Encoder, uses contrastive learning to extract aligned representations from singing, speech, and textual prompts. 3) Flow-based Custom Transformer, leverages Cus-MOE, with F0 supervision, enhancing both the synthesis quality and style modeling of the generated singing voice. Experimental results show that TCSinger 2 outperforms baseline models in both subjective and objective metrics across multiple related tasks. </p>
<blockquote>
<p>å¯å®šåˆ¶çš„å¤šè¯­ç§é›¶æ ·æœ¬æ­Œå”±å£°éŸ³åˆæˆï¼ˆSVSï¼‰åœ¨éŸ³ä¹åˆ›ä½œå’ŒçŸ­è§†é¢‘é…éŸ³ç­‰é¢†åŸŸå…·æœ‰å¤šç§æ½œåœ¨åº”ç”¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„SVSæ¨¡å‹è¿‡äºä¾èµ–éŸ³ç´ å’ŒéŸ³ç¬¦è¾¹ç•Œæ³¨é‡Šï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨é›¶æ ·æœ¬åœºæ™¯ä¸­çš„ç¨³å¥æ€§ï¼Œå¹¶å¯¼è‡´éŸ³ç´ å’ŒéŸ³ç¬¦ä¹‹é—´çš„è¿‡æ¸¡ä¸è‡ªç„¶ã€‚æ­¤å¤–ï¼Œå®ƒä»¬è¿˜ç¼ºä¹é€šè¿‡ä¸åŒæç¤ºè¿›è¡Œæœ‰æ•ˆçš„å¤šçº§é£æ ¼æ§åˆ¶ã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†TCSinger 2ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºä¸åŒæç¤ºè¿›è¡Œé£æ ¼è¿ç§»å’Œé£æ ¼æ§åˆ¶çš„å¤šä»»åŠ¡å¤šè¯­ç§é›¶æ ·æœ¬SVSæ¨¡å‹ã€‚TCSinger 2ä¸»è¦åŒ…æ‹¬ä¸‰ä¸ªå…³é”®æ¨¡å—ï¼š1ï¼‰æ¨¡ç³Šè¾¹ç•Œå†…å®¹ï¼ˆBBCï¼‰ç¼–ç å™¨ï¼Œé¢„æµ‹æŒç»­æ—¶é—´ï¼Œæ‰©å±•å†…å®¹åµŒå…¥ï¼Œå¹¶å¯¹è¾¹ç•Œåº”ç”¨æ©ç ä»¥å®ç°å¹³æ»‘è¿‡æ¸¡ã€‚2ï¼‰è‡ªå®šä¹‰éŸ³é¢‘ç¼–ç å™¨ï¼Œåˆ©ç”¨å¯¹æ¯”å­¦ä¹ ä»æ­Œå”±ã€è¯­éŸ³å’Œæ–‡æœ¬æç¤ºä¸­æå–å¯¹é½è¡¨ç¤ºã€‚3ï¼‰åŸºäºæµçš„è‡ªå®šä¹‰Transformerï¼Œåˆ©ç”¨Cus-MOEå’ŒF0ç›‘ç£ï¼Œæé«˜ç”Ÿæˆæ­Œå”±å£°éŸ³çš„åˆæˆè´¨é‡å’Œé£æ ¼å»ºæ¨¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTCSinger 2åœ¨å¤šä¸ªç›¸å…³ä»»åŠ¡ä¸­çš„ä¸»è§‚å’Œå®¢è§‚æŒ‡æ ‡ä¸Šå‡ä¼˜äºåŸºå‡†æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14910v1">PDF</a> Accepted by ACL 2025</p>
<p><strong>Summary</strong></p>
<p>TCSinger 2æ˜¯ä¸€æ¬¾å¤šä»»åŠ¡å¤šè¯­è¨€é›¶æ ·æœ¬æ­Œå”±å£°éŸ³åˆæˆæ¨¡å‹ï¼Œå…·æœ‰é£æ ¼è¿ç§»å’ŒåŸºäºä¸åŒæç¤ºçš„é£æ ¼æ§åˆ¶åŠŸèƒ½ã€‚è¯¥æ¨¡å‹é€šè¿‡å¼•å…¥BBCç¼–ç å™¨ã€è‡ªå®šä¹‰éŸ³é¢‘ç¼–ç å™¨å’ŒåŸºäºæµçš„è‡ªå®šä¹‰è½¬æ¢å™¨ç­‰æŠ€æœ¯ï¼Œæé«˜äº†è¯­éŸ³åˆæˆçš„è´¨é‡å’Œçµæ´»æ€§ï¼Œå¯åº”ç”¨äºéŸ³ä¹åˆ›ä½œå’ŒçŸ­è§†é¢‘é…éŸ³ç­‰é¢†åŸŸã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TCSinger 2æ˜¯ä¸€æ¬¾å¤šè¯­è¨€é›¶æ ·æœ¬æ­Œå”±å£°éŸ³åˆæˆæ¨¡å‹ï¼Œé€‚ç”¨äºéŸ³ä¹åˆ›ä½œå’ŒçŸ­è§†é¢‘é…éŸ³ã€‚</li>
<li>ç°æœ‰SVSæ¨¡å‹è¿‡äºä¾èµ–éŸ³ç´ å’ŒéŸ³ç¬¦è¾¹ç•Œæ³¨é‡Šï¼Œé™åˆ¶äº†å…¶åœ¨é›¶æ ·æœ¬åœºæ™¯ä¸­çš„ç¨³å¥æ€§ã€‚</li>
<li>TCSinger 2é€šè¿‡å¼•å…¥BBCç¼–ç å™¨å®ç°å¹³æ»‘è¿‡æ¸¡ï¼Œå»¶é•¿å†…å®¹åµŒå…¥å¹¶åº”ç”¨è¾¹ç•Œæ©ç ã€‚</li>
<li>è‡ªå®šä¹‰éŸ³é¢‘ç¼–ç å™¨ä½¿ç”¨å¯¹æ¯”å­¦ä¹ ä»æ­Œå”±ã€è¯­éŸ³å’Œæ–‡æœ¬æç¤ºä¸­æå–å¯¹é½è¡¨ç¤ºã€‚</li>
<li>æµå¼è‡ªå®šä¹‰è½¬æ¢å™¨åˆ©ç”¨Cus-MOEå’ŒF0ç›‘ç£ï¼Œæé«˜åˆæˆè´¨é‡å’Œé£æ ¼å»ºæ¨¡ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒTCSinger 2åœ¨å¤šé¡¹ä»»åŠ¡ä¸­ä¸»è§‚å’Œå®¢è§‚æŒ‡æ ‡ä¸Šå‡ä¼˜äºåŸºå‡†æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14910">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f676ddf43e1dd0499c7ecca871178ea3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-27a4eb1027100c021e210d447c2a5f1f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-99887b38acacc46968bc74cb19d1cd6e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e608cb554b61d11570a3fefe79d414fd.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-24/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-24/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-24/Face%20Swapping/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-6bda9ec0ecfd02e19feb65e875b7f40a.jpg" class="responsive-img" alt="Face Swapping">
                        
                        <span class="card-title">Face Swapping</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Face Swapping æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-24  Beyond Face Swapping A Diffusion-Based Digital Human Benchmark for   Multimodal Deepfake Detection
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-24
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Face-Swapping/" class="post-category">
                                    Face Swapping
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Face-Swapping/">
                        <span class="chip bg-color">Face Swapping</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-24/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-a302749a96d55713eb0ccb2cc9352df0.jpg" class="responsive-img" alt="æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ ">
                        
                        <span class="card-title">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹  æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-24  Visual Perturbation and Adaptive Hard Negative Contrastive Learning for   Compositional Reasoning in Vision-Language Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-24
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ 
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">29885.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
