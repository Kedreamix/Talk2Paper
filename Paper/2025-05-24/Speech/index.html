<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech 方向最新论文已更新，请持续关注 Update in 2025-05-24  An Effective Training Framework for Light-Weight Automatic Speech   Recognition Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-14a4e3300ae37e46159b13792cc36ef8.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-05-24
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    17k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    68 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-05-24-更新"><a href="#2025-05-24-更新" class="headerlink" title="2025-05-24 更新"></a>2025-05-24 更新</h1><h2 id="An-Effective-Training-Framework-for-Light-Weight-Automatic-Speech-Recognition-Models"><a href="#An-Effective-Training-Framework-for-Light-Weight-Automatic-Speech-Recognition-Models" class="headerlink" title="An Effective Training Framework for Light-Weight Automatic Speech   Recognition Models"></a>An Effective Training Framework for Light-Weight Automatic Speech   Recognition Models</h2><p><strong>Authors:Abdul Hannan, Alessio Brutti, Shah Nawaz, Mubashir Noman</strong></p>
<p>Recent advancement in deep learning encouraged developing large automatic speech recognition (ASR) models that achieve promising results while ignoring computational and memory constraints. However, deploying such models on low resource devices is impractical despite of their favorable performance. Existing approaches (pruning, distillation, layer skip etc.) transform the large models into smaller ones at the cost of significant performance degradation or require prolonged training of smaller models for better performance. To address these issues, we introduce an efficacious two-step representation learning based approach capable of producing several small sized models from a single large model ensuring considerably better performance in limited number of epochs. Comprehensive experimentation on ASR benchmarks reveals the efficacy of our approach, achieving three-fold training speed-up and up to 12.54% word error rate improvement. </p>
<blockquote>
<p>近期深度学习的发展促进了大型自动语音识别（ASR）模型的开发，这些模型在忽略计算和内存限制的情况下取得了令人鼓舞的结果。然而，将这种模型部署在资源有限的设备上并不实用，尽管它们的性能表现良好。现有方法（如修剪、蒸馏、层跳过等）将大型模型转化为小型模型，但要以性能显著下降为代价，或者需要对小型模型进行更长时间的训练以获得更好的性能。为了解决这些问题，我们引入了一种有效的两步表示学习的方法，能够从单一的大型模型中生成多个小型模型，并在有限的迭代次数内确保性能显著提升。在ASR基准测试上的综合实验表明了我们方法的有效性，实现了三倍的训练速度提升和最高达到12.54%的词错误率改进。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16991v1">PDF</a> Accepted at InterSpeech 2025</p>
<p><strong>Summary</strong>：<br>深度学习领域的最新进展推动了大型自动语音识别（ASR）模型的发展，这些模型在忽略计算和内存约束的情况下取得了令人鼓舞的结果。然而，将这些模型部署在资源有限的设备上并不实用。现有方法（如剪枝、蒸馏、层跳过等）将大型模型转换为小型模型，但牺牲了性能或需要延长小型模型的训练时间。为解决这些问题，我们提出了一种有效的两步表示学习法，能从单一大型模型中生成多个小型模型，并在有限的训练周期内实现显著的性能提升。实验证明，我们的方法实现了三倍的训练速度提升和最高达12.54%的词错误率改进。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>深度学习推动大型ASR模型发展，但资源有限设备的部署不实用。</li>
<li>现有方法转换大型模型到小型模型会牺牲性能或需要延长训练时间。</li>
<li>提出了一个基于表示学习的两步方法，从单一大型模型生成多个小型模型。</li>
<li>方法实现了显著的性能提升和训练速度提升。</li>
<li>实验证明该方法在ASR基准测试上有效。</li>
<li>该方法在保证性能的同时，实现了模型的快速训练和部署。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16991">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-07235e6076f9518fb58558fda5f92ab1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-590dbc913b7fe93104a2b1f5dc09f49f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b99f9410a26d61b994237b03b603002c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d13242ab52e2f9263624f0bd2296ea1.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="On-Multilingual-Encoder-Language-Model-Compression-for-Low-Resource-Languages"><a href="#On-Multilingual-Encoder-Language-Model-Compression-for-Low-Resource-Languages" class="headerlink" title="On Multilingual Encoder Language Model Compression for Low-Resource   Languages"></a>On Multilingual Encoder Language Model Compression for Low-Resource   Languages</h2><p><strong>Authors:Daniil Gurgurov, Michal Gregor, Josef van Genabith, Simon Ostermann</strong></p>
<p>In this paper, we combine two-step knowledge distillation, structured pruning, truncation, and vocabulary trimming for extremely compressing multilingual encoder-only language models for low-resource languages. Our novel approach systematically combines existing techniques and takes them to the extreme, reducing layer depth, feed-forward hidden size, and intermediate layer embedding size to create significantly smaller monolingual models while retaining essential language-specific knowledge. We achieve compression rates of up to 92% with only a marginal performance drop of 2-10% in four downstream tasks, including sentiment analysis, topic classification, named entity recognition, and part-of-speech tagging, across three low-resource languages. Notably, the performance degradation correlates with the amount of language-specific data in the teacher model, with larger datasets resulting in smaller performance losses. Additionally, we conduct extensive ablation studies to identify best practices for multilingual model compression using these techniques. </p>
<blockquote>
<p>在这篇论文中，我们结合了两步知识蒸馏、结构化剪枝、截断和词汇缩减等技术，对多语言编码器仅有语言模型进行了极致压缩，适用于资源有限的语言。我们的新方法系统地结合了现有技术并将其推向极致，通过减少层深度、前馈隐藏大小和中间层嵌入大小，在保留关键语言特定知识的同时，创建了显著更小的单语种模型。我们在四种下游任务中实现了高达92%的压缩率，性能仅下降了2-10%，包括情感分析、主题分类、命名实体识别和词性标注，涉及三种资源有限的语言。值得注意的是，性能下降与教师模型中的语言特定数据量相关，更大的数据集导致更小的性能损失。此外，我们还进行了广泛的消融研究，以识别使用这些技术进行多语种模型压缩的最佳实践。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16956v1">PDF</a> Pre-print</p>
<p><strong>Summary</strong>:<br>本研究通过结合两步知识蒸馏、结构化剪枝、截断和词汇缩减等技术，对多语言编码器仅语言模型进行极端压缩，适用于低资源语言。通过减少层深度、前馈隐藏大小和中间层嵌入大小，创建显著更小的单语言模型，同时保留关键的语言特定知识。在情感分析、主题分类、命名实体识别和词性标注等四个下游任务中，仅在三种低资源语言上有2-10%的性能下降，实现了高达92%的压缩率。性能下降与教师模型中的语言特定数据量有关，较大数据集导致的性能损失较小。此外，还进行了广泛的研究以确定使用这些技术的多语言模型压缩的最佳实践。</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>本研究通过结合多种技术，实现了对多语言编码器仅语言模型的极端压缩，适用于低资源语言。</li>
<li>通过减少模型层深度、前馈隐藏大小和中间层嵌入大小，创建了显著更小的单语言模型。</li>
<li>在四个下游任务中实现了高达92%的压缩率，同时性能下降仅为2-10%。</li>
<li>性能下降与教师模型中的语言特定数据量有关。</li>
<li>在低资源语言上，较大数据集能减小性能损失。</li>
<li>研究通过广泛的研究确定了使用多种技术压缩多语言模型的最佳实践。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16956">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-62919fd5f24566ce19c3bc07a5b55b24.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9ec361b61f35ea519a52615a44bdfdb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f1250c8c1cb142d14846d4c7d860f87a.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Active-Speech-Enhancement-Active-Speech-Denoising-Decliping-and-Deveraberation"><a href="#Active-Speech-Enhancement-Active-Speech-Denoising-Decliping-and-Deveraberation" class="headerlink" title="Active Speech Enhancement: Active Speech Denoising Decliping and   Deveraberation"></a>Active Speech Enhancement: Active Speech Denoising Decliping and   Deveraberation</h2><p><strong>Authors:Ofir Yaish, Yehuda Mishaly, Eliya Nachmani</strong></p>
<p>We introduce a new paradigm for active sound modification: Active Speech Enhancement (ASE). While Active Noise Cancellation (ANC) algorithms focus on suppressing external interference, ASE goes further by actively shaping the speech signal – both attenuating unwanted noise components and amplifying speech-relevant frequencies – to improve intelligibility and perceptual quality. To enable this, we propose a novel Transformer-Mamba-based architecture, along with a task-specific loss function designed to jointly optimize interference suppression and signal enrichment. Our method outperforms existing baselines across multiple speech processing tasks – including denoising, dereverberation, and declipping – demonstrating the effectiveness of active, targeted modulation in challenging acoustic environments. </p>
<blockquote>
<p>我们介绍了一种主动声音修改的新范式：主动语音增强（ASE）。而主动降噪（ANC）算法主要侧重于抑制外部干扰，ASE更进一步，通过积极塑造语音信号——既减弱不需要的噪声成分，又放大语音相关的频率——来提高可懂度和感知质量。为此，我们提出了一种基于Transformer-Mamba的架构，以及一种针对特定任务的损失函数，旨在联合优化干扰抑制和信号增强。我们的方法在多个语音处理任务上的表现超过了现有基线，包括去噪、去混响和消抖动，证明了在具有挑战性的声学环境中主动、针对性调制的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16911v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种新的主动声音修改范式：主动语音增强（ASE）。与主动噪声消除（ANC）算法侧重于抑制外部干扰不同，ASE通过积极塑造语音信号来提高语音的清晰度和感知质量，既减弱了不需要的噪声成分，又放大了语音相关的频率。为实现这一目标，提出了一种基于Transformer-Mamba的新型架构，以及一种针对干扰抑制和信号丰富联合优化的特定任务损失函数。该方法在多个语音处理任务上优于现有基线，包括去噪、去混响和去剪辑，证明了在具有挑战性的声学环境中进行主动、针对性调制的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>介绍了主动语音增强（ASE）这一新的声音修改范式。</li>
<li>与主动噪声消除（ANC）不同，ASE旨在积极塑造语音信号。</li>
<li>ASE能同时减弱不需要的噪声成分并放大语音相关的频率。</li>
<li>提出了一种基于Transformer-Mamba的新型架构来实现ASE。</li>
<li>损失函数的设计能联合优化干扰抑制和信号丰富。</li>
<li>该方法在多个语音处理任务上的表现优于现有基线。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16911">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-11fd90a4a6597832a13bd692d5789996.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a09f095b6413342f788c91eda228206c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-570be815f846e66a308fac4b3fc37fe9.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="SEED-Speaker-Embedding-Enhancement-Diffusion-Model"><a href="#SEED-Speaker-Embedding-Enhancement-Diffusion-Model" class="headerlink" title="SEED: Speaker Embedding Enhancement Diffusion Model"></a>SEED: Speaker Embedding Enhancement Diffusion Model</h2><p><strong>Authors:KiHyun Nam, Jungwoo Heo, Jee-weon Jung, Gangin Park, Chaeyoung Jung, Ha-Jin Yu, Joon Son Chung</strong></p>
<p>A primary challenge when deploying speaker recognition systems in real-world applications is performance degradation caused by environmental mismatch. We propose a diffusion-based method that takes speaker embeddings extracted from a pre-trained speaker recognition model and generates refined embeddings. For training, our approach progressively adds Gaussian noise to both clean and noisy speaker embeddings extracted from clean and noisy speech, respectively, via forward process of a diffusion model, and then reconstructs them to clean embeddings in the reverse process. While inferencing, all embeddings are regenerated via diffusion process. Our method needs neither speaker label nor any modification to the existing speaker recognition pipeline. Experiments on evaluation sets simulating environment mismatch scenarios show that our method can improve recognition accuracy by up to 19.6% over baseline models while retaining performance on conventional scenarios. We publish our code here <a target="_blank" rel="noopener" href="https://github.com/kaistmm/seed-pytorch">https://github.com/kaistmm/seed-pytorch</a> </p>
<blockquote>
<p>在现实世界应用中部署语音识别系统时面临的主要挑战是环境不匹配导致的性能下降。我们提出了一种基于扩散的方法，该方法采用从预训练的语音识别模型中提取的说话人嵌入，并生成精细的嵌入。在训练过程中，我们的方法通过扩散模型的正向过程，逐步向干净和带有噪音的语音中提取的干净和带噪音的说话人嵌入添加高斯噪音，然后在反向过程中将它们重建为干净的嵌入。在推理过程中，所有嵌入都是通过扩散过程重新生成的。我们的方法既不需要说话人标签，也不需要修改现有的语音识别管道。在模拟环境不匹配场景的评价集上进行的实验表明，我们的方法可以在保持传统场景性能的同时，将基线模型的识别准确率提高高达19.6%。我们已将代码发布在<a target="_blank" rel="noopener" href="https://github.com/kaistmm/seed-pytorch%E4%B8%8A%E3%80%82">https://github.com/kaistmm/seed-pytorch上。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16798v1">PDF</a> Accepted to Interspeech 2025. The official code can be found at   <a target="_blank" rel="noopener" href="https://github.com/kaistmm/seed-pytorch">https://github.com/kaistmm/seed-pytorch</a></p>
<p><strong>Summary</strong></p>
<p>本论文提出了一种基于扩散模型的优化方法，用于改善实际部署中的说话人识别系统因环境不匹配导致的性能下降问题。该方法利用预训练的说话人识别模型提取说话人嵌入，并通过扩散过程生成精细化的嵌入。训练过程中，对干净和带噪声的说话人嵌入逐步添加高斯噪声，再通过扩散模型的逆向过程重建为干净嵌入。在推断阶段，所有嵌入都通过扩散过程重新生成。该方法既不需要说话人标签，也不需要对现有的说话人识别流程进行修改。实验结果表明，该方法在模拟环境不匹配场景的评估集上，相较于基准模型，识别准确率提高了高达19.6%，同时在常规场景上保持了性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>说话人识别系统在现实应用中的主要挑战之一是环境不匹配导致的性能下降。</li>
<li>论文提出了一种基于扩散模型的优化方法，用于改善因环境不匹配导致的性能问题。</li>
<li>该方法利用预训练的说话人识别模型提取嵌入，并通过扩散过程生成精细化的嵌入。</li>
<li>训练过程中，干净和带噪声的说话人嵌入会受到高斯噪声的干扰，然后通过逆向过程重建为干净嵌入。</li>
<li>在推断阶段，所有嵌入都通过扩散过程重新生成，无需额外的标签或修改现有流程。</li>
<li>实验结果表明，该方法在模拟环境不匹配场景的评估集上显著提高了识别准确率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16798">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9002670bdc78141a234edfbbe21264ed.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dde69cbb9de5ffb77c895713798dd530.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c080e9a4d5e260b2c0f5268e42da8f8a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d6498b2c243ad9efdfd451991642bde3.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="SoccerChat-Integrating-Multimodal-Data-for-Enhanced-Soccer-Game-Understanding"><a href="#SoccerChat-Integrating-Multimodal-Data-for-Enhanced-Soccer-Game-Understanding" class="headerlink" title="SoccerChat: Integrating Multimodal Data for Enhanced Soccer Game   Understanding"></a>SoccerChat: Integrating Multimodal Data for Enhanced Soccer Game   Understanding</h2><p><strong>Authors:Sushant Gautam, Cise Midoglu, Vajira Thambawita, Michael A. Riegler, Pål Halvorsen, Mubarak Shah</strong></p>
<p>The integration of artificial intelligence in sports analytics has transformed soccer video understanding, enabling real-time, automated insights into complex game dynamics. Traditional approaches rely on isolated data streams, limiting their effectiveness in capturing the full context of a match. To address this, we introduce SoccerChat, a multimodal conversational AI framework that integrates visual and textual data for enhanced soccer video comprehension. Leveraging the extensive SoccerNet dataset, enriched with jersey color annotations and automatic speech recognition (ASR) transcripts, SoccerChat is fine-tuned on a structured video instruction dataset to facilitate accurate game understanding, event classification, and referee decision making. We benchmark SoccerChat on action classification and referee decision-making tasks, demonstrating its performance in general soccer event comprehension while maintaining competitive accuracy in referee decision making. Our findings highlight the importance of multimodal integration in advancing soccer analytics, paving the way for more interactive and explainable AI-driven sports analysis. <a target="_blank" rel="noopener" href="https://github.com/simula/SoccerChat">https://github.com/simula/SoccerChat</a> </p>
<blockquote>
<p>将人工智能融入体育分析已经改变了对足球视频的理解方式，能够实现针对复杂比赛动态的实时自动化洞察。传统方法依赖于孤立的数据流，在捕捉比赛的完整上下文方面存在局限性。为了解决这一问题，我们推出了SoccerChat，这是一个多模式对话人工智能框架，融合了视觉和文本数据，以提高足球视频的理解能力。借助丰富的SoccerNet数据集，通过增加球衣颜色注释和自动语音识别（ASR）转录本进行丰富，SoccerChat在结构化视频指令数据集上进行微调，以促进准确的比赛理解、事件分类和裁判决策制定。我们在动作分类和裁判决策制定任务上对SoccerChat进行了基准测试，证明了其在一般足球事件理解方面的性能，同时在裁判决策制定方面保持了竞争性的准确性。我们的研究强调了多模式融合在推进足球分析方面的重要性，为更交互和可解释的AI驱动的体育分析铺平了道路。相关链接：<a target="_blank" rel="noopener" href="https://github.com/simula/SoccerChat%E3%80%82">https://github.com/simula/SoccerChat。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16630v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>人工智能在体育分析领域的应用已经转变了人们对足球比赛视频的理解方式，通过实时、自动化的复杂比赛动态洞察。传统方法依赖于孤立的数据流，无法全面捕捉比赛上下文。为此，我们推出SoccerChat——一个多模式对话式人工智能框架，融合了视觉和文字数据，以提高足球视频的理解能力。借助丰富的SoccerNet数据集，通过球衣颜色标注和自动语音识别（ASR）转录进行丰富，SoccerChat在结构化视频指令数据集上进行微调，以促进准确的游戏理解、事件分类和裁判决策。我们在动作分类和裁判决策任务上对SoccerChat进行了评估，证明了它在一般足球事件理解方面的表现，同时在裁判决策制定方面保持竞争性的准确性。我们的研究突出了多模式融合在推动足球分析方面的重要性，为更加交互和可解释的AI驱动的体育分析铺平了道路。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>人工智能在体育分析领域的应用已经改变了对足球视频的理解方式。</li>
<li>传统方法难以全面捕捉足球比赛的上下文信息。</li>
<li>SoccerChat是一个多模式对话式AI框架，融合了视觉和文字数据来提高足球视频理解。</li>
<li>SoccerChat利用SoccerNet数据集和自动语音识别技术进行优化。</li>
<li>SoccerChat在动作分类和裁判决策任务上表现出良好的性能。</li>
<li>多模式融合对于提高足球分析的重要性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16630">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-fcb2f7c733ecb583a8146b0e40bdf4b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cf52d0cbde2b135c0e3831e8ddb68829.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-461a7d479e89ca40e932eca9a2d00286.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Attractor-Based-Speech-Separation-of-Multiple-Utterances-by-Unknown-Number-of-Speakers"><a href="#Attractor-Based-Speech-Separation-of-Multiple-Utterances-by-Unknown-Number-of-Speakers" class="headerlink" title="Attractor-Based Speech Separation of Multiple Utterances by Unknown   Number of Speakers"></a>Attractor-Based Speech Separation of Multiple Utterances by Unknown   Number of Speakers</h2><p><strong>Authors:Yuzhu Wang, Archontis Politis, Konstantinos Drossos, Tuomas Virtanen</strong></p>
<p>This paper addresses the problem of single-channel speech separation, where the number of speakers is unknown, and each speaker may speak multiple utterances. We propose a speech separation model that simultaneously performs separation, dynamically estimates the number of speakers, and detects individual speaker activities by integrating an attractor module. The proposed system outperforms existing methods by introducing an attractor-based architecture that effectively combines local and global temporal modeling for multi-utterance scenarios. To evaluate the method in reverberant and noisy conditions, a multi-speaker multi-utterance dataset was synthesized by combining Librispeech speech signals with WHAM! noise signals. The results demonstrate that the proposed system accurately estimates the number of sources. The system effectively detects source activities and separates the corresponding utterances into correct outputs in both known and unknown source count scenarios. </p>
<blockquote>
<p>本文解决了单通道语音分离的问题，其中未知说话人数量，每个说话人可能会发出多次话语。我们提出了一种语音分离模型，该模型通过集成吸引模块，同时执行分离、动态估计说话人数以及检测单个说话人的活动。通过引入基于吸引器的架构，有效结合局部和全局时间建模，为多重话语场景提供多话语场景数据集，所提出系统在性能上优于现有方法。为了评估在混响和噪音条件下的方法，我们通过结合Librispeech语音信号和WHAM！噪音信号合成多说话人多话语数据集。结果表明，该系统准确估计了源数量。系统有效地检测源活动，并将相应的讲话内容分离为已知和未知源数的正确输出。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16607v1">PDF</a> 5 pages, 4 figures, accepted by Interspeech 2025</p>
<p><strong>总结</strong></p>
<p>本文提出了一种解决单通道语音分离问题的方法，其中未知说话人数量，且每个说话人可能发出多次连续说话。通过集成吸引模块，该语音分离模型可同时实现分离、动态估计说话人数和检测单个说话人的活动。该模型引入基于吸引器的架构，有效结合本地和全局时间建模，为多连续发言场景提供优势，从而超越现有方法。为评估该模型在混响和噪音条件下的性能，合成了一个多发言人连续发言数据集，结合了Librispeech语音信号与WHAM!噪音信号。实验结果表明，该模型能准确估计源的数量，并在已知和未知源计数场景下有效检测源活动和将相应的连续发言正确分离。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>论文解决的是单通道语音分离问题，其中说话人的数量未知，且每个说话人可能连续发言。</li>
<li>提出了一种语音分离模型，该模型通过集成吸引模块实现多种功能，包括语音分离、动态估计说话人数和检测单个说话人的活动。</li>
<li>模型采用基于吸引器的架构，有效结合本地和全局时间建模，以处理多连续发言场景。</li>
<li>模型在合成数据集上进行了评估，该数据集结合了Librispeech和WHAM!的语音和噪音信号，以模拟真实环境中的混响和噪音影响。</li>
<li>模型能准确估计源的数量，即使在源数量未知的情况下也能有效工作。</li>
<li>模型可以有效地检测源活动并将连续发言正确分离为不同的输出。</li>
<li>该模型在语音分离领域展现出优异的性能，可能为未来相关研究提供新的方向。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16607">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-c50261827bf4536a88d804800a2b8dd2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-053f4cca045407e7ecc7385aa2282a4d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1ec373a563e057fb0b3113c28cc9d513.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-05ff389ae8a39e261e585e03e0a51107.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c1c4613feb7ef41023e1373396bbc26a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-edd504665cde1d67a0947bd4c80d4957.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="X-ARES-A-Comprehensive-Framework-for-Assessing-Audio-Encoder-Performance"><a href="#X-ARES-A-Comprehensive-Framework-for-Assessing-Audio-Encoder-Performance" class="headerlink" title="X-ARES: A Comprehensive Framework for Assessing Audio Encoder   Performance"></a>X-ARES: A Comprehensive Framework for Assessing Audio Encoder   Performance</h2><p><strong>Authors:Junbo Zhang, Heinrich Dinkel, Yadong Niu, Chenyu Liu, Si Cheng, Anbei Zhao, Jian Luan</strong></p>
<p>We introduces X-ARES (eXtensive Audio Representation and Evaluation Suite), a novel open-source benchmark designed to systematically assess audio encoder performance across diverse domains. By encompassing tasks spanning speech, environmental sounds, and music, X-ARES provides two evaluation approaches for evaluating audio representations: linear fine-tuning and unparameterized evaluation. The framework includes 22 distinct tasks that cover essential aspects of audio processing, from speech recognition and emotion detection to sound event classification and music genre identification. Our extensive evaluation of state-of-the-art audio encoders reveals significant performance variations across different tasks and domains, highlighting the complexity of general audio representation learning. </p>
<blockquote>
<p>我们介绍了X-ARES（扩展音频表示和评估套件），这是一个新型开源基准测试，旨在系统地评估不同领域音频编码器的性能。X-ARES涵盖了语音、环境声音和音乐等任务，为评估音频表示提供了两种评估方法：线性微调和无参数评估。该框架包括涵盖音频处理各个方面的22个不同任务，从语音识别和情感检测到声音事件分类和音乐风格识别。我们对最先进的音频编码器的广泛评估表明，不同任务和领域之间的性能存在很大差异，这突出了通用音频表示学习的复杂性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16369v1">PDF</a> Accepted by Interspeech 2025</p>
<p><strong>Summary</strong><br>     本文介绍了X-ARES（广泛音频表示和评估套件），这是一个新型的开源基准测试，旨在系统地评估不同领域音频编码器的性能。X-ARES涵盖了涵盖语音、环境声音和音乐的任务，提供了两种评估音频表示的方法：线性微调和无参数评估。该框架包括22个不同的任务，涵盖音频处理的重要方面，如语音识别、情感检测、声音事件分类和音乐风格识别。对最先进的音频编码器的广泛评估表明，不同任务和领域之间的性能存在显著差异，突出了通用音频表示学习复杂性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>X-ARES是一个新型的开源基准测试，旨在评估音频编码器的性能。</li>
<li>X-ARES涵盖了语音、环境声音和音乐等多种领域的任务。</li>
<li>X-ARES提供了两种评估音频表示的方法：线性微调和无参数评估。</li>
<li>框架包含22个不同的任务，涵盖音频处理的重要方面，如语音识别、情感检测、声音事件分类和音乐风格识别。</li>
<li>广泛评估显示，不同任务和领域之间音频编码器的性能存在显著差异。</li>
<li>评估结果强调了通用音频表示学习的复杂性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16369">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2738696b4fb17ec5cc5c99a44ba56f3c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f487d64a504192774a4ba644053fa077.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-701069cc0f0b87a96d423c67478de3a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-35d2479d211a526218f3d83bb87f4ce2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4c13c9fb47b5d8934925001fb3ea299b.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Dysfluent-WFST-A-Framework-for-Zero-Shot-Speech-Dysfluency-Transcription-and-Detection"><a href="#Dysfluent-WFST-A-Framework-for-Zero-Shot-Speech-Dysfluency-Transcription-and-Detection" class="headerlink" title="Dysfluent WFST: A Framework for Zero-Shot Speech Dysfluency   Transcription and Detection"></a>Dysfluent WFST: A Framework for Zero-Shot Speech Dysfluency   Transcription and Detection</h2><p><strong>Authors:Chenxu Guo, Jiachen Lian, Xuanru Zhou, Jinming Zhang, Shuhe Li, Zongli Ye, Hwi Joo Park, Anaisha Das, Zoe Ezzes, Jet Vonk, Brittany Morin, Rian Bogley, Lisa Wauters, Zachary Miller, Maria Gorno-Tempini, Gopala Anumanchipalli</strong></p>
<p>Automatic detection of speech dysfluency aids speech-language pathologists in efficient transcription of disordered speech, enhancing diagnostics and treatment planning. Traditional methods, often limited to classification, provide insufficient clinical insight, and text-independent models misclassify dysfluency, especially in context-dependent cases. This work introduces Dysfluent-WFST, a zero-shot decoder that simultaneously transcribes phonemes and detects dysfluency. Unlike previous models, Dysfluent-WFST operates with upstream encoders like WavLM and requires no additional training. It achieves state-of-the-art performance in both phonetic error rate and dysfluency detection on simulated and real speech data. Our approach is lightweight, interpretable, and effective, demonstrating that explicit modeling of pronunciation behavior in decoding, rather than complex architectures, is key to improving dysfluency processing systems. </p>
<blockquote>
<p>自动检测语言流畅性障碍有助于语言病理学家有效地转录障碍性语言，提高诊断和制定治疗方案。传统的方法通常仅限于分类，提供的临床见解不足，并且文本独立模型容易误判语言流畅性障碍，特别是在依赖语境的案例中。本研究引入了Dysfluent-WFST，这是一种零样本解码器，可以同时转录音素并检测语言流畅性障碍。不同于之前的模型，Dysfluent-WFST与上游编码器（如WavLM）一起运行，无需额外训练。它在模拟和真实语音数据上达到了语音错误率和语言流畅性检测方面的最佳性能。我们的方法轻便、可解释性强且有效，表明在解码过程中明确建模发音行为，而不是使用复杂的架构，是提高语言流畅性处理系统的关键。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16351v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>自动检测语言流畅性障碍有助于语言病理学家高效地转录障碍性语言，从而提高诊断和制定治疗方案。传统方法往往局限于分类，提供不足的临床见解，而独立于文本的模型在上下文相关的案例中误判语言流畅性障碍。本研究引入Dysfluent-WFST模型，一种零射击解码器可同时转录音素并检测语言流畅性障碍。与其他模型不同，Dysfluent-WFST与上游编码器如WavLM协同工作，无需额外训练。它在模拟和实际语音数据上均实现了语音错误率和语言流畅性障碍检测的卓越性能。我们的方法具有轻便性、解释性和有效性，证明了解码过程中显式建模发音行为而非复杂架构是提高语言流畅性处理系统的关键。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自动检测语言流畅性障碍可提高诊断与治疗效率。</li>
<li>传统方法在分类上存在局限性，需要新方法提供更深入的见解。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16351">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-a89107df145af9ba17ac75f8df28a5ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-72dbb715284d0f9e467b80eaea1fb515.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c698578373eddacab7d8c2ec84c99f22.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-db83b56946c95c5fc3097276cb396e02.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7fc99bdb2d2621beb13e9ce8a2babca7.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="MM-MovieDubber-Towards-Multi-Modal-Learning-for-Multi-Modal-Movie-Dubbing"><a href="#MM-MovieDubber-Towards-Multi-Modal-Learning-for-Multi-Modal-Movie-Dubbing" class="headerlink" title="MM-MovieDubber: Towards Multi-Modal Learning for Multi-Modal Movie   Dubbing"></a>MM-MovieDubber: Towards Multi-Modal Learning for Multi-Modal Movie   Dubbing</h2><p><strong>Authors:Junjie Zheng, Zihao Chen, Chaofan Ding, Yunming Liang, Yihan Fan, Huan Yang, Lei Xie, Xinhan Di</strong></p>
<p>Current movie dubbing technology can produce the desired speech using a reference voice and input video, maintaining perfect synchronization with the visuals while effectively conveying the intended emotions. However, crucial aspects of movie dubbing, including adaptation to various dubbing styles, effective handling of dialogue, narration, and monologues, as well as consideration of subtle details such as speaker age and gender, remain insufficiently explored. To tackle these challenges, we introduce a multi-modal generative framework. First, it utilizes a multi-modal large vision-language model (VLM) to analyze visual inputs, enabling the recognition of dubbing types and fine-grained attributes. Second, it produces high-quality dubbing using large speech generation models, guided by multi-modal inputs. Additionally, a movie dubbing dataset with annotations for dubbing types and subtle details is constructed to enhance movie understanding and improve dubbing quality for the proposed multi-modal framework. Experimental results across multiple benchmark datasets show superior performance compared to state-of-the-art (SOTA) methods. In details, the LSE-D, SPK-SIM, EMO-SIM, and MCD exhibit improvements of up to 1.09%, 8.80%, 19.08%, and 18.74%, respectively. </p>
<blockquote>
<p>当前的电影配音技术可以使用参考声音和输入视频来生成所需的语音，在视觉效果上保持完美的同步，同时有效地传达预期的情感。然而，电影配音的关键方面，包括适应各种配音风格、有效处理对话、旁白和独白，以及考虑诸如演讲者年龄和性别等细微细节，仍然没有得到足够的探索。为了应对这些挑战，我们引入了一种多模态生成框架。首先，它利用多模态大型视觉语言模型（VLM）来分析视觉输入，能够识别配音类型和精细属性。其次，它使用大型语音生成模型生成高质量的配音，这些模型受到多模态输入的指导。此外，还构建了一个包含配音类型和细微细节注释的电影配音数据集，以提高电影理解和提高所提出的多模态框架的配音质量。在多个基准数据集上的实验结果表明，与最新方法相比，该框架具有卓越的性能。具体来说，LSE-D、SPK-SIM、EMO-SIM和MCD分别提高了高达1.09%、8.80%、19.08%和18.74%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16279v1">PDF</a> 5 pages, 4 figures, accepted by Interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>当前电影配音技术已能通过参考声音和输入视频产生所需的语音，保持与视觉画面的完美同步，并有效传达预期的情绪。然而，电影配音的关键方面，如适应不同的配音风格、有效处理对话、旁白和独白，以及考虑演讲者的年龄和性别等细微之处，仍探索不足。为解决这些挑战，我们提出了一种多模态生成框架，它首先利用多模态大型视觉语言模型分析视觉输入，识别配音类型和精细属性。其次，利用大型语音生成模型产生高质量配音，受多模态输入的指导。此外，还构建了一个包含配音类型和细微之处注释的电影配音数据集，以提高电影理解和提升配音质量。实验结果显示，与最新方法相比，该框架在多个基准数据集上表现卓越。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前电影配音技术能够同步视频并传达情绪。</li>
<li>电影配音仍面临适应不同配音风格、处理对话、旁白和独白的挑战。</li>
<li>提出的多模态生成框架利用视觉语言模型识别配音类型和精细属性。</li>
<li>该框架通过大型语音生成模型产生高质量配音。</li>
<li>构建了一个包含配音类型和细微之处的电影配音数据集。</li>
<li>实验结果显示该框架在多个数据集上表现优于最新方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16279">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0348abc1ca837d6933c3848018c0dcef.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1fee04bf9e4415ee981543265f126cc6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-900bb5a66835af50bf131061bd35a924.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c385297ffc4f8603bd92aa04121f968.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f46db5dd8aca4a724b562249c7e9e284.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Meta-PerSER-Few-Shot-Listener-Personalized-Speech-Emotion-Recognition-via-Meta-learning"><a href="#Meta-PerSER-Few-Shot-Listener-Personalized-Speech-Emotion-Recognition-via-Meta-learning" class="headerlink" title="Meta-PerSER: Few-Shot Listener Personalized Speech Emotion Recognition   via Meta-learning"></a>Meta-PerSER: Few-Shot Listener Personalized Speech Emotion Recognition   via Meta-learning</h2><p><strong>Authors:Liang-Yeh Shen, Shi-Xin Fang, Yi-Cheng Lin, Huang-Cheng Chou, Hung-yi Lee</strong></p>
<p>This paper introduces Meta-PerSER, a novel meta-learning framework that personalizes Speech Emotion Recognition (SER) by adapting to each listener’s unique way of interpreting emotion. Conventional SER systems rely on aggregated annotations, which often overlook individual subtleties and lead to inconsistent predictions. In contrast, Meta-PerSER leverages a Model-Agnostic Meta-Learning (MAML) approach enhanced with Combined-Set Meta-Training, Derivative Annealing, and per-layer per-step learning rates, enabling rapid adaptation with only a few labeled examples. By integrating robust representations from pre-trained self-supervised models, our framework first captures general emotional cues and then fine-tunes itself to personal annotation styles. Experiments on the IEMOCAP corpus demonstrate that Meta-PerSER significantly outperforms baseline methods in both seen and unseen data scenarios, highlighting its promise for personalized emotion recognition. </p>
<blockquote>
<p>本文介绍了Meta-PerSER，这是一种新型的元学习框架，通过适应每个听众独特的情感解读方式，对语音情感识别（SER）进行个性化处理。传统的SER系统依赖于聚合注释，这往往忽略了个人细微差别，并导致预测结果不一致。相比之下，Meta-PerSER利用模型无关的元学习（MAML）方法，并结合集合元训练、导数退火和逐层每步学习率，仅通过少量有标签的样本即可实现快速适应。通过整合预训练自监督模型的稳健表示，我们的框架首先捕捉一般的情感线索，然后对自己进行微调以适应个人注释风格。在IEMOCAP语料库上的实验表明，Meta-PerSER在可见和未见数据场景中均显著优于基准方法，凸显其在个性化情感识别方面的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16220v1">PDF</a> Accepted by INTERSPEECH 2025. 7 pages, including 2 pages of appendix</p>
<p><strong>总结</strong></p>
<p>本文介绍了Meta-PerSER，这是一个新的元学习框架，通过适应每个听众对情绪解读的独特方式，个性化地应用于语音情绪识别（SER）。与传统的SER系统依赖于聚合注释不同，这常常忽略了个人细微差别并导致预测不一致。相比之下，Meta-PerSER采用模型无关的元学习方法，并结合组合集元训练、导数退火和逐层逐步的学习率，仅通过少量标注样本即可实现快速适应。通过整合预训练自监督模型的稳健表示，我们的框架首先捕捉一般情绪线索，然后细化自身以适应个人注释风格。在IEMOCAP语料库上的实验表明，Meta-PerSER在已见和未见的数据场景中均显著优于基准方法，凸显其在个性化情绪识别方面的潜力。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>Meta-PerSER是一个新的元学习框架，用于个性化语音情绪识别（SER）。</li>
<li>传统的SER系统依赖于聚合注释，这可能导致预测的不一致。</li>
<li>Meta-PerSER采用模型无关的元学习方法，并结合多种技术实现快速适应。</li>
<li>该框架通过整合预训练自监督模型的稳健表示，捕捉一般情绪线索并适应个人注释风格。</li>
<li>实验表明，Meta-PerSER在已见和未见的数据场景中均优于基准方法。</li>
<li>Meta-PerSER通过适应每个听众对情绪解读的独特方式，提高了情绪识别的个性化程度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16220">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-98396fab6f4c5d3e136d37e8bc0160f7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7d7977f75f5b3fc31a274d7accffb250.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-17d15c4264b428f552e5e09d9325260c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fca87e70dfa41f2df87ed81868b3973f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-80dcacaf3b3ba3f547674cc3ae5cd2ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3b2ed53ab1212067f259231368920b9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-33ff0db73f47b59732e05cecc4af9651.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Differentiable-K-means-for-Fully-optimized-Discrete-Token-based-ASR"><a href="#Differentiable-K-means-for-Fully-optimized-Discrete-Token-based-ASR" class="headerlink" title="Differentiable K-means for Fully-optimized Discrete Token-based ASR"></a>Differentiable K-means for Fully-optimized Discrete Token-based ASR</h2><p><strong>Authors:Kentaro Onda, Yosuke Kashiwagi, Emiru Tsunoo, Hayato Futami, Shinji Watanabe</strong></p>
<p>Recent studies have highlighted the potential of discrete tokens derived from self-supervised learning (SSL) models for various speech-related tasks. These tokens serve not only as substitutes for text in language modeling but also as intermediate representations for tasks such as automatic speech recognition (ASR). However, discrete tokens are typically obtained via k-means clustering of SSL features independently of downstream tasks, making them suboptimal for specific applications. This paper proposes the use of differentiable k-means, enabling the joint optimization of tokenization and downstream tasks. This approach enables the fine-tuning of the SSL parameters and learning weights for outputs from multiple SSL layers. Experiments were conducted with ASR as a downstream task. ASR accuracy successfully improved owing to the optimized tokens. The acquired tokens also exhibited greater purity of phonetic information, which were found to be useful even in speech resynthesis. </p>
<blockquote>
<p>最近的研究已经突出了自监督学习（SSL）模型生成的离散标记在多种语音相关任务中的潜力。这些标记不仅用作语言建模中的文本替代，还用作自动语音识别（ASR）等任务的中间表示。然而，离散标记通常是通过独立于下游任务的SSL特征的k-均值聚类获得的，这使得它们对于特定应用而言并不理想。本文建议使用可微分的k-均值方法，实现对标记化和下游任务的联合优化。这种方法能够微调SSL参数并学习来自多个SSL层的输出的权重。以ASR作为下游任务进行的实验表明，由于优化的标记，ASR准确率成功提高。所获得的标记还表现出更高的语音信息纯度，即使在语音重建中也被发现非常有用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16207v1">PDF</a> Accepted by Interspeech2025</p>
<p><strong>Summary</strong></p>
<p>本文探讨了基于自监督学习（SSL）模型的离散标记在语音相关任务中的潜力。这些标记不仅可作为语言建模中的文本替代品，还可作为自动语音识别（ASR）等任务的中间表示形式。然而，传统的离散标记是通过独立于下游任务的SSL特征进行k-均值聚类获得的，这使得它们对于特定应用程序而言并不理想。本文提出使用可微分的k-均值方法，实现标记化与下游任务的联合优化。这种方法可以微调SSL参数并学习来自多个SSL层的输出权重。在下游任务（如ASR）上进行实验发现，优化的标记显著提高了ASR的准确率。此外，获得的标记具有更高的语音信息纯度，在语音合成中同样展现出良好的表现。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>自监督学习模型的离散标记在语音任务中有广泛应用潜力。</li>
<li>传统离散标记的获取方式独立于下游任务，可能不适用于特定应用。</li>
<li>可微分的k-均值方法用于联合优化标记化与下游任务，提高性能。</li>
<li>优化后的标记能提升自动语音识别（ASR）的准确率。</li>
<li>获得的标记具有更高的语音信息纯度，可用于语音合成。</li>
<li>SSL参数可以通过该方法进行微调，同时学习多个SSL层的输出权重。</li>
<li>该方法对于改进基于自监督学习的语音任务具有积极意义。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16207">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-25e153d7701d62f408668d26be1e00c3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c16b28d6a15a3f770a7e51e8e827d63.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c3deef409de87d8b18f5d51ac2db789f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9063742dd7ddae4e3aced54ca4dd9268.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-724f28ff392c5d9c1dc71e88fcd3f143.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Prosodically-Enhanced-Foreign-Accent-Simulation-by-Discrete-Token-based-Resynthesis-Only-with-Native-Speech-Corpora"><a href="#Prosodically-Enhanced-Foreign-Accent-Simulation-by-Discrete-Token-based-Resynthesis-Only-with-Native-Speech-Corpora" class="headerlink" title="Prosodically Enhanced Foreign Accent Simulation by Discrete Token-based   Resynthesis Only with Native Speech Corpora"></a>Prosodically Enhanced Foreign Accent Simulation by Discrete Token-based   Resynthesis Only with Native Speech Corpora</h2><p><strong>Authors:Kentaro Onda, Keisuke Imoto, Satoru Fukayama, Daisuke Saito, Nobuaki Minematsu</strong></p>
<p>Recently, a method for synthesizing foreign-accented speech only with native speech data using discrete tokens obtained from self-supervised learning (SSL) models was proposed. Considering limited availability of accented speech data, this method is expected to make it much easier to simulate foreign accents. By using the synthesized accented speech as listening materials for humans or training data for automatic speech recognition (ASR), both of them will acquire higher robustness against foreign accents. However, the previous method has a fatal flaw that it cannot reproduce duration-related accents. Durational accents are commonly seen when L2 speakers, whose native language has syllable-timed or mora-timed rhythm, speak stress-timed languages, such as English. In this paper, we integrate duration modification to the previous method to simulate foreign accents more accurately. Experiments show that the proposed method successfully replicates durational accents seen in real L2 speech. </p>
<blockquote>
<p>最近，提出一种仅使用从自监督学习（SSL）模型中获得的离散令牌合成带有外国口音的语音的方法，该方法以原生语音数据为基础。考虑到口音语音数据的可用性有限，此方法可以极大地简化模拟外来口音的过程。通过使用合成的口音语音作为人类的听力材料或自动语音识别（ASR）的训练数据，两者都可以提高对外来口音的鲁棒性。然而，之前的方法存在一个重大缺陷，即无法重现与持续时间相关的口音。当二语（L2）说话者（其母语具有音节定时或摩拉定时节奏）说以音节间隔定时为主的如英语这样的语言时，通常会遇到持续性口音。在本文中，我们将持续时间修改整合到之前的方法中，以更准确地模拟外来口音。实验表明，所提出的方法成功地复制了现实中二语语音中所见的持续性口音。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16191v1">PDF</a> Accepted by Interspeech2025</p>
<p><strong>Summary</strong><br>语音合成技术取得新进展，能够通过自监督学习模型获取的离散标记，仅使用原生语音数据合成带口音的语音。新方法解决了以往不能重现与持续时间相关的口音的问题，能够更准确地模拟外语口音，提高人类听众或语音识别系统的稳健性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>新的语音合成方法利用自监督学习模型生成的离散标记仅使用原生语音数据。</li>
<li>新方法解决了不能准确模拟外语口音的问题，特别是持续时间相关的口音。</li>
<li>新方法可应用于为听力训练材料或语音识别训练提供更高稳健性的带口音语音模拟。</li>
<li>L2语言（非母语）学习者在学习以英语为代表的压力定时语言时可能遇到的口音问题得到关注。</li>
<li>实验表明新方法能够成功模拟真实L2语音中的持续口音。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16191">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-77b61769ff7788cfb9b9f38177ec72c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-09888464577831f235bbd7d57deec21a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a4c0705e5b4ad6730af037a99fc0d07e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8d00bdca2562f52b31ba2fa9364aa24d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d4b23374cbbd84a0db213ebecffbb56b.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Selective-Invocation-for-Multilingual-ASR-A-Cost-effective-Approach-Adapting-to-Speech-Recognition-Difficulty"><a href="#Selective-Invocation-for-Multilingual-ASR-A-Cost-effective-Approach-Adapting-to-Speech-Recognition-Difficulty" class="headerlink" title="Selective Invocation for Multilingual ASR: A Cost-effective Approach   Adapting to Speech Recognition Difficulty"></a>Selective Invocation for Multilingual ASR: A Cost-effective Approach   Adapting to Speech Recognition Difficulty</h2><p><strong>Authors:Hongfei Xue, Yufeng Tang, Jun Zhang, Xuelong Geng, Lei Xie</strong></p>
<p>Although multilingual automatic speech recognition (ASR) systems have significantly advanced, enabling a single model to handle multiple languages, inherent linguistic differences and data imbalances challenge SOTA performance across all languages. While language identification (LID) models can route speech to the appropriate ASR model, they incur high costs from invoking SOTA commercial models and suffer from inaccuracies due to misclassification. To overcome these, we propose SIMA, a selective invocation for multilingual ASR that adapts to the difficulty level of the input speech. Built on a spoken large language model (SLLM), SIMA evaluates whether the input is simple enough for direct transcription or requires the invocation of a SOTA ASR model. Our approach reduces word error rates by 18.7% compared to the SLLM and halves invocation costs compared to LID-based methods. Tests on three datasets show that SIMA is a scalable, cost-effective solution for multilingual ASR applications. </p>
<blockquote>
<p>虽然多语言自动语音识别（ASR）系统已经取得了重大进展，使得单一模型能够处理多种语言，但固有的语言差异和数据不平衡仍然对所有语言的最新技术水平（SOTA）性能构成了挑战。虽然语言识别（LID）模型可以将语音路由到适当的ASR模型，但它们会因调用最新技术的商业模型而产生高昂的成本，并且由于误分类而面临不精确的问题。为了克服这些问题，我们提出了SIMA，这是一种用于多语言ASR的选择性调用方法，它能够适应输入语音的难度水平。基于口语大型语言模型（SLLM），SIMA评估输入是否足够简单以进行直接转录，或者是否需要调用最新技术的ASR模型。我们的方法与SLLM相比，将单词错误率降低了18.7%，并将基于LID的方法的调用成本减半。在三个数据集上的测试表明，SIMA是一种可用于多语言ASR应用的可扩展且经济高效的解决方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16168v1">PDF</a> Accepted by INTERSPEECH 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了多语言自动语音识别（ASR）系统的挑战，包括语言差异和数据不平衡问题。虽然语言识别（LID）模型可以将语音路由到适当的ASR模型，但它们调用先进商业模型的成本高昂，并且由于误分类而存在不准确的问题。为此，本文提出了SIMA，一种适应输入语音难度级别的选择性调用多语言ASR的方法。SIMA建立在口语大型语言模型（SLLM）之上，评估输入是否简单到可以直接转录，还是需要调用高级ASR模型。该方法将字词错误率降低了18.7%，与SLLM相比，并将调用成本降低了一半。在三个数据集上的测试表明，SIMA是一种可扩展且经济实惠的多语言ASR应用程序解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多语言ASR系统虽然有所发展，但仍面临语言差异和数据不平衡的挑战。</li>
<li>LID模型在路由语音到适当的ASR模型时，存在高成本和误分类的问题。</li>
<li>SIMA是一种基于口语大型语言模型（SLLM）的选择性调用方法，适应输入语音的难度级别。</li>
<li>SIMA能够评估输入是否简单到可以直接转录，或需要调用高级ASR模型。</li>
<li>SIMA将字词错误率降低了18.7%，与SLLM相比具有优越性。</li>
<li>SIMA将调用成本降低了一半，相比LID-based方法更具成本效益。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16168">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-961365af1a6e5d68f765081828777394.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-05946d78f30447ee15527160b58d8592.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-183099b8b25d0ffe599ae8e6312c9b82.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21d9ece0f55ea572778acacd4a4fe8b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d0d1a055c452691cb63a26667d67736.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b2a32df295be90c57ba2a418d0267a8e.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Efficient-and-Direct-Duplex-Modeling-for-Speech-to-Speech-Language-Model"><a href="#Efficient-and-Direct-Duplex-Modeling-for-Speech-to-Speech-Language-Model" class="headerlink" title="Efficient and Direct Duplex Modeling for Speech-to-Speech Language Model"></a>Efficient and Direct Duplex Modeling for Speech-to-Speech Language Model</h2><p><strong>Authors:Ke Hu, Ehsan Hosseini-Asl, Chen Chen, Edresson Casanova, Subhankar Ghosh, Piotr Żelasko, Zhehuai Chen, Jason Li, Jagadeesh Balam, Boris Ginsburg</strong></p>
<p>Spoken dialogue is an intuitive form of human-computer interaction, yet current speech language models often remain constrained to turn-based exchanges, lacking real-time adaptability such as user barge-in. We propose a novel duplex speech to speech (S2S) architecture featuring continuous user inputs and codec agent outputs with channel fusion that directly models simultaneous user and agent streams. Using a pretrained streaming encoder for user input enables the first duplex S2S model without requiring speech pretrain. Separate architectures for agent and user modeling facilitate codec fine-tuning for better agent voices and halve the bitrate (0.6 kbps) compared to previous works. Experimental results show that the proposed model outperforms previous duplex models in reasoning, turn-taking, and barge-in abilities. The model requires significantly less speech data, as speech pretrain is skipped, which markedly simplifies the process of building a duplex S2S model from any LLMs. Finally, it is the first openly available duplex S2S model with training and inference code to foster reproducibility. </p>
<blockquote>
<p>口语对话是人类与计算机交互的一种直观形式。然而，当前的语音语言模型通常仅限于基于回合的交互，缺乏实时适应性，如用户抢话等。我们提出了一种新型的双语语音到语音（S2S）架构，具有连续用户输入和编解码器代理输出，通过通道融合直接模拟用户和代理的同时流。使用预训练的流式编码器进行用户输入，使得第一个双语S2S模型无需语音预训练。为代理和用户建模提供单独的结构，便于编解码器微调以获取更好的代理语音，与以前的工作相比，比特率减半（0.6kbps）。实验结果表明，该模型在推理、话轮转换和抢话能力方面优于之前的双语模型。该模型需要的语音数据大大减少，因为跳过了语音预训练，这极大地简化了从任何大型语言模型构建双语S2S模型的过程。最后，它是第一个公开可用的带有训练和推理代码的双语S2S模型，有利于促进可重复性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15670v1">PDF</a> Accepted to Interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>文章提出一种新型双工语音到语音（S2S）架构，支持连续用户输入和编码解码器输出，通过信道融合直接模拟用户和代理的实时交互流。该架构使用预训练的流式编码器实现用户输入，无需语音预训练即可构建首个双工S2S模型。通过分别构建代理和用户模型，优化编码解码器以改善代理语音质量，同时与以往工作相比将比特率减半（降至0.6kbps）。实验结果显示，该模型在推理、轮替和打断能力上优于之前的双工模型。此外，由于省略了语音预训练，该模型所需语音数据量大大减少，显著简化了从任何大型语言模型构建双工S2S模型的过程。这是首个公开可用的双工S2S模型，附有训练和推理代码，便于重复实验和进一步发展。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出一种新型双工语音到语音（S2S）架构，支持连续用户输入和编码解码器输出。</li>
<li>通过信道融合实现用户和代理的实时交互流模拟。</li>
<li>使用预训练的流式编码器，无需语音预训练即可构建双工S2S模型。</li>
<li>分离代理和用户建模优化编码解码器，改善代理语音质量并降低比特率。</li>
<li>实验证明该模型在推理、轮替和打断能力上超越之前的双工模型。</li>
<li>所需语音数据量减少，简化从大型语言模型构建双工S2S模型的过程。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15670">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-48ced667046baab251e5e9e436d652f3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-29e57d7b58e499b2319ef470f5dc7386.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-141ffcba3e23d56eeb8f42246523a56e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d134d6e2425d0c178372442ddd484c9d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce72e53da69534c9b7cba405760c607b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-79e6ccc0956dc107cff583a134db8993.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Prosody-Adaptable-Audio-Codecs-for-Zero-Shot-Voice-Conversion-via-In-Context-Learning"><a href="#Prosody-Adaptable-Audio-Codecs-for-Zero-Shot-Voice-Conversion-via-In-Context-Learning" class="headerlink" title="Prosody-Adaptable Audio Codecs for Zero-Shot Voice Conversion via   In-Context Learning"></a>Prosody-Adaptable Audio Codecs for Zero-Shot Voice Conversion via   In-Context Learning</h2><p><strong>Authors:Junchuan Zhao, Xintong Wang, Ye Wang</strong></p>
<p>Recent advances in discrete audio codecs have significantly improved speech representation modeling, while codec language models have enabled in-context learning for zero-shot speech synthesis. Inspired by this, we propose a voice conversion (VC) model within the VALLE-X framework, leveraging its strong in-context learning capabilities for speaker adaptation. To enhance prosody control, we introduce a prosody-aware audio codec encoder (PACE) module, which isolates and refines prosody from other sources, improving expressiveness and control. By integrating PACE into our VC model, we achieve greater flexibility in prosody manipulation while preserving speaker timbre. Experimental evaluation results demonstrate that our approach outperforms baseline VC systems in prosody preservation, timbre consistency, and overall naturalness, surpassing baseline VC systems. </p>
<blockquote>
<p>近期离散音频编解码器的进展极大地改进了语音表示建模，而编解码器语言模型已经实现了零射击语音合成的上下文学习。受此启发，我们在VALLE-X框架内提出了一个语音转换（VC）模型，利用其强大的上下文学习能力进行说话人适配。为了增强韵律控制，我们引入了一个韵律感知音频编解码器编码器（PACE）模块，该模块可以隔离并优化韵律的来源，从而提高表达力和控制力。通过将PACE集成到我们的VC模型中，我们在韵律操纵方面实现了更大的灵活性，同时保留了说话人的音色。实验评估结果表明，我们的方法在韵律保持、音色一致性和整体自然度方面超过了基线VC系统。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15402v1">PDF</a> 5 pages, 3 figures</p>
<p><strong>Summary</strong></p>
<p>最新的离散音频编解码器技术的进展为语音表现建模带来了重大改进，编解码器语言模型实现了零样本语音合成的上下文学习。受此启发，我们在VALLE-X框架内提出了一种语音转换（VC）模型，利用其强大的上下文学习能力进行自适应说话人适配。为了增强韵律控制，我们引入了韵律感知音频编解码器编码器（PACE）模块，该模块能隔离并精炼韵律来源，提高了表达力和控制力。将PACE集成到我们的VC模型中，实现了在保持说话人音质的同时，韵律操控更加灵活。实验评估结果表明，我们的方法在保持韵律、音质一致性和整体自然度方面超越了基线VC系统。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>离散音频编解码器的最新进展显著改进了语音表现建模。</li>
<li>编解码器语言模型实现了零样本语音合成的上下文学习。</li>
<li>提出了在VALLE-X框架内的语音转换（VC）模型，具有强大的上下文学习能力，用于自适应说话人适配。</li>
<li>引入了韵律感知音频编解码器编码器（PACE）模块，以提高韵律控制和表达力。</li>
<li>PACE集成到VC模型中，实现了更灵活的韵律操控，同时保持说话人音质。</li>
<li>实验结果表明，该方法的韵律保持、音质一致性和自然度均超越基线VC系统。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15402">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ba1cba21ed3de6bc44389948aa5b00e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1e4201e7de303229190d1eea635ba23.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e3646ee87f32d11323a55446a1552483.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-32184b8a627ad4363f6d6f2252a3a6df.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1880b7f97a55df6b9a2c14ad26fa717f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f38d0b65c8dfaeb9c81bb1e530eac4c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5c2b129d197257c131aeb99eebbd2fd2.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Analysis-of-ABC-Frontend-Audio-Systems-for-the-NIST-SRE24"><a href="#Analysis-of-ABC-Frontend-Audio-Systems-for-the-NIST-SRE24" class="headerlink" title="Analysis of ABC Frontend Audio Systems for the NIST-SRE24"></a>Analysis of ABC Frontend Audio Systems for the NIST-SRE24</h2><p><strong>Authors:Sara Barahona, Anna Silnova, Ladislav Mošner, Junyi Peng, Oldřich Plchot, Johan Rohdin, Lin Zhang, Jiangyu Han, Petr Palka, Federico Landini, Lukáš Burget, Themos Stafylakis, Sandro Cumani, Dominik Boboš, Miroslav Hlavaček, Martin Kodovsky, Tomáš Pavlíček</strong></p>
<p>We present a comprehensive analysis of the embedding extractors (frontends) developed by the ABC team for the audio track of NIST SRE 2024. We follow the two scenarios imposed by NIST: using only a provided set of telephone recordings for training (fixed) or adding publicly available data (open condition). Under these constraints, we develop the best possible speaker embedding extractors for the pre-dominant conversational telephone speech (CTS) domain. We explored architectures based on ResNet with different pooling mechanisms, recently introduced ReDimNet architecture, as well as a system based on the XLS-R model, which represents the family of large pre-trained self-supervised models. In open condition, we train on VoxBlink2 dataset, containing 110 thousand speakers across multiple languages. We observed a good performance and robustness of VoxBlink-trained models, and our experiments show practical recipes for developing state-of-the-art frontends for speaker recognition. </p>
<blockquote>
<p>我们对ABC团队为NIST SRE 2024音频轨道开发嵌入提取器（前端）进行了综合分析。我们遵循NIST规定的两种场景：仅使用提供的电话录音集进行训练（固定）或添加公开数据（开放条件）。在这些约束下，我们为主要的对话电话语音（CTS）领域开发最佳的语音嵌入提取器。我们探索了基于ResNet的不同池化机制架构、最近推出的ReDimNet架构，以及基于XLS-R模型的系统，它代表了一类大型预训练自监督模型。在开放条件下，我们在包含多种语言的VoxBlink2数据集上进行训练。我们观察到VoxBlink训练模型的良好性能和稳健性，并且我们的实验为开发先进的前端语音识别提供了实用方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15320v1">PDF</a> Accepted at Interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了ABC团队针对NIST SRE 2024音频轨道开发的嵌入提取器（前端）的综合分析。文章遵循NIST设定的两种场景，一种是仅使用提供的电话录音集进行训练（固定），另一种是加入公开数据（开放条件）。在这些限制下，团队为主要的对话电话语音（CTS）领域开发了最佳可能的说话人嵌入提取器。文章探索了基于ResNet的不同池化机制架构、新推出的ReDimNet架构，以及基于预训练自监督模型家族的XLS-R模型的体系。在开放条件下，团队对包含多种语言的VoxBlink2数据集进行了训练，观察到良好性能和稳健性，实验展示了开发前沿说话人识别技术的实用方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>介绍了ABC团队针对NIST SRE 2024音频轨道的嵌入提取器开发。</li>
<li>遵循NIST设定的两种场景：固定场景和开放条件场景。</li>
<li>在对话电话语音（CTS）领域开发最佳说话人嵌入提取器。</li>
<li>探索了多种架构，包括基于ResNet的池化机制、ReDimNet架构和XLS-R模型。</li>
<li>在开放条件下使用VoxBlink2数据集进行训练，该数据集包含多种语言的110千名说话人。</li>
<li>VoxBlink训练模型表现出良好的性能和稳健性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15320">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c0c49e43b2febb2629d77a1e832e44cd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-70ce0158d813890e8771801946ce400a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5bf488a35fd15b15bf10b24026cfc33a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9816c1864300f51e90065e20697e16c6.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="MentalMAC-Enhancing-Large-Language-Models-for-Detecting-Mental-Manipulation-via-Multi-Task-Anti-Curriculum-Distillation"><a href="#MentalMAC-Enhancing-Large-Language-Models-for-Detecting-Mental-Manipulation-via-Multi-Task-Anti-Curriculum-Distillation" class="headerlink" title="MentalMAC: Enhancing Large Language Models for Detecting Mental   Manipulation via Multi-Task Anti-Curriculum Distillation"></a>MentalMAC: Enhancing Large Language Models for Detecting Mental   Manipulation via Multi-Task Anti-Curriculum Distillation</h2><p><strong>Authors:Yuansheng Gao, Han Bao, Tong Zhang, Bin Li, Zonghui Wang, Wenzhi Chen</strong></p>
<p>Mental manipulation is a subtle yet pervasive form of psychological abuse that poses serious threats to mental health. Its covert nature and the complexity of manipulation strategies make it challenging to detect, even for state-of-the-art large language models (LLMs). This concealment also hinders the manual collection of large-scale, high-quality annotations essential for training effective models. Although recent efforts have sought to improve LLMs’ performance on this task, progress remains limited due to the scarcity of real-world annotated datasets. To address these challenges, we propose MentalMAC, a multi-task anti-curriculum distillation method that enhances LLMs’ ability to detect mental manipulation in multi-turn dialogue. Our approach includes: (i) EvoSA, an unsupervised data expansion method based on evolutionary operations and speech act theory; (ii) teacher model-generated multi-task supervision; and (iii) progressive knowledge distillation from complex to simpler tasks. We then constructed the ReaMent dataset with 5,000 real-world dialogue samples, using a MentalMAC-distilled model to assist human annotation. Vast experiments demonstrate that our method significantly narrows the gap between student and teacher models and outperforms competitive LLMs across key evaluation metrics. All code, datasets, and checkpoints will be released upon paper acceptance. Warning: This paper contains content that may be offensive to readers. </p>
<blockquote>
<p>心理操控是一种微妙而普遍的心理虐待形式，对心理健康构成严重威胁。其隐蔽性和策略复杂性使得检测变得具有挑战，即使是最先进的大型语言模型（LLM）也难以识别。这种隐蔽性也阻碍了手动收集大规模高质量注释，这对训练有效模型至关重要。尽管近期努力旨在提高LLM在此任务上的性能，但由于缺乏真实世界的注释数据集，进展仍然有限。为了解决这些挑战，我们提出了MentalMAC，这是一种多任务反向课程蒸馏方法，可增强LLM在多轮对话中检测心理操控的能力。我们的方法包括：(i) EvoSA，一种基于进化操作和言语行为理论的无监督数据扩展方法；(ii)教师模型生成的多任务监督；(iii)从复杂到简单任务的知识渐进蒸馏。然后，我们使用MentalMAC蒸馏的模型协助人工注释，构建了包含5000个真实世界对话样本的ReaMent数据集。大量实验表明，我们的方法显著缩小了学生模型与教师模型之间的差距，并在关键评估指标上优于其他LLM。论文接受后，我们将公开所有代码、数据集和检查点。警告：本论文含有可能对读者造成不适的内容。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15255v2">PDF</a> </p>
<p><strong>Summary</strong>：</p>
<p>本文探讨了心理操纵对心理健康的严重威胁，以及现有技术下检测心理操纵的难度。针对这些问题，提出了一种多任务反课程蒸馏方法MentalMAC，用于提高大型语言模型（LLM）在多轮对话中检测心理操纵的能力。该方法包括数据扩展方法EvoSA、教师模型生成的多任务监督以及从复杂到简单任务的知识蒸馏。同时构建了RealMent数据集用于训练和评估模型。实验证明，该方法显著缩小了学生与教师模型之间的差距，并在关键评估指标上优于其他LLM。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>心理操纵是一种隐蔽且普遍的心理虐待形式，对心理健康构成严重威胁。</li>
<li>检测心理操纵是一个挑战，特别是对于大型语言模型（LLM）。</li>
<li>提出了一个多任务反课程蒸馏方法MentalMAC，旨在提高LLM检测心理操纵的能力。</li>
<li>MentalMAC包括数据扩展方法EvoSA、教师模型生成的多任务监督以及渐进的知识蒸馏技术。</li>
<li>构建了RealMent数据集用于训练和评估模型，其中包含5000个真实对话样本。</li>
<li>实验证明，MentalMAC方法显著提高了模型的性能，并在关键评估指标上优于其他LLM。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15255">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-8adfbeeff5ddded7131e823ba4a5daad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3e615b4b416ab5e7623e8ffe75af36c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac6ee570f763b9787f7830ab4e30f60b.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Voice-ENHANCE-Speech-Restoration-using-a-Diffusion-based-Voice-Conversion-Framework"><a href="#Voice-ENHANCE-Speech-Restoration-using-a-Diffusion-based-Voice-Conversion-Framework" class="headerlink" title="Voice-ENHANCE: Speech Restoration using a Diffusion-based Voice   Conversion Framework"></a>Voice-ENHANCE: Speech Restoration using a Diffusion-based Voice   Conversion Framework</h2><p><strong>Authors:Kyungguen Byun, Jason Filos, Erik Visser, Sunkuk Moon</strong></p>
<p>We propose a speech enhancement system that combines speaker-agnostic speech restoration with voice conversion (VC) to obtain a studio-level quality speech signal. While voice conversion models are typically used to change speaker characteristics, they can also serve as a means of speech restoration when the target speaker is the same as the source speaker. However, since VC models are vulnerable to noisy conditions, we have included a generative speech restoration (GSR) model at the front end of our proposed system. The GSR model performs noise suppression and restores speech damage incurred during that process without knowledge about the target speaker. The VC stage then uses guidance from clean speaker embeddings to further restore the output speech. By employing this two-stage approach, we have achieved speech quality objective metric scores comparable to state-of-the-art (SOTA) methods across multiple datasets. </p>
<blockquote>
<p>我们提出了一种结合说话者无关的语音恢复和语音转换（VC）的语音增强系统，以获得工作室级别的语音信号。虽然语音转换模型通常用于改变说话人的特征，但当目标说话人与源说话人相同时，它们也可以作为语音恢复的一种手段。然而，由于VC模型对噪声条件很敏感，我们在所提出系统的前端加入了一个生成式语音恢复（GSR）模型。GSR模型执行噪声抑制，并在无需了解目标说话人的情况下恢复在过程中出现的语音损伤。VC阶段然后使用干净的说话人嵌入作为指导，进一步恢复输出语音。通过采用这种两阶段的方法，我们在多个数据集上达到了与最新方法相当的语音质量客观指标得分。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15254v1">PDF</a> 5 pages, 3 figures, Accepted to INTERSPEECH 2025</p>
<p><strong>Summary</strong></p>
<p>本文提出一种结合无特定人语音恢复与语音转换（VC）的语音增强系统，旨在获得工作室级别的语音信号质量。系统前端采用生成式语音恢复（GSR）模型进行降噪和恢复语音损伤，无需了解目标说话人的信息。接着，VC阶段利用干净说话人的嵌入引导进一步恢复输出语音。通过采用两阶段方法，实现了与多个数据集上的最新方法相当的语音质量客观度量分数。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出结合无特定人语音恢复与语音转换的语音增强系统。</li>
<li>采用生成式语音恢复（GSR）模型进行降噪和恢复语音损伤。</li>
<li>GSR模型在不知道目标说话人的情况下，对语音进行恢复。</li>
<li>语音转换（VC）阶段使用干净说话人的嵌入进行进一步的语音恢复。</li>
<li>通过两阶段方法实现了高质量的语音恢复效果。</li>
<li>系统性能在多个数据集上达到或超过现有技术水平的客观度量分数。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15254">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d59c2d34501db73243b4a7f1443b2afa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bbb9efac088abc70343b60180fcf0f01.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f9f073ba1ec4f19b6b566e916ef45a4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-69c2c9e6e193ad061192d732ee552287.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3591141137db54a7b7fe3aa6a89fc08b.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="EASY-Emotion-aware-Speaker-Anonymization-via-Factorized-Distillation"><a href="#EASY-Emotion-aware-Speaker-Anonymization-via-Factorized-Distillation" class="headerlink" title="EASY: Emotion-aware Speaker Anonymization via Factorized Distillation"></a>EASY: Emotion-aware Speaker Anonymization via Factorized Distillation</h2><p><strong>Authors:Jixun Yao, Hexin Liu, Eng Siong Chng, Lei Xie</strong></p>
<p>Emotion plays a significant role in speech interaction, conveyed through tone, pitch, and rhythm, enabling the expression of feelings and intentions beyond words to create a more personalized experience. However, most existing speaker anonymization systems employ parallel disentanglement methods, which only separate speech into linguistic content and speaker identity, often neglecting the preservation of the original emotional state. In this study, we introduce EASY, an emotion-aware speaker anonymization framework. EASY employs a novel sequential disentanglement process to disentangle speaker identity, linguistic content, and emotional representation, modeling each speech attribute in distinct subspaces through a factorized distillation approach. By independently constraining speaker identity and emotional representation, EASY minimizes information leakage, enhancing privacy protection while preserving original linguistic content and emotional state. Experimental results on the VoicePrivacy Challenge official datasets demonstrate that our proposed approach outperforms all baseline systems, effectively protecting speaker privacy while maintaining linguistic content and emotional state. </p>
<blockquote>
<p>情感在语音交互中扮演着重要角色，通过语调、音高和节奏来传达，使人们在言语之外能够表达感受和意图，从而创造更加个性化的体验。然而，大多数现有的说话人匿名化系统采用并行解耦方法，仅将语音分离为语言内容和说话人身份，往往忽视了原始情感状态的保持。在本研究中，我们引入了EASY，一个情感感知的说话人匿名化框架。EASY采用新颖的顺序解耦过程来解耦说话人身份、语言内容和情感表示，通过因子蒸馏法将每种语音属性建模在不同的子空间中。通过独立约束说话人身份和情感表示，EASY减少了信息泄露，在保持原始语言内容和情感状态的同时增强了隐私保护。在VoicePrivacy Challenge官方数据集上的实验结果表明，我们提出的方法优于所有基线系统，有效保护说话人隐私的同时保持了语言内容和情感状态。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15004v1">PDF</a> Accepted by INTERSPEECH 2025</p>
<p><strong>Summary</strong></p>
<p>情绪在语音交互中扮演着重要的角色，通过语调、音高和节奏来表达情感和意图，使体验更加个性化。然而，现有的大多数说话人匿名化系统采用并行分离方法，仅将语音分离为语言内容和说话人身份，忽视了原始情绪状态的保留。本研究介绍了EASY，一种情感感知的说话人匿名化框架。EASY采用新颖的顺序分离过程，将说话人身份、语言内容和情感表示进行分离，通过因子蒸馏法将每种语音属性建模在不同的子空间中。通过独立约束说话人身份和情感表示，EASY减少了信息泄露，增强了隐私保护，同时保留了原始的语言内容和情感状态。实验结果表明，在VoicePrivacy Challenge官方数据集上，本研究提出的方法优于所有基线系统，有效地保护了说话人的隐私，同时保持了语言内容和情感状态。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>情感在语音交互中起到重要作用，通过语调、音高和节奏表达情感和意图。</li>
<li>现有的说话人匿名化系统主要关注于分离语言内容和说话人身份，容易忽视原始情感状态的保留。</li>
<li>EASY框架是一种情感感知的说话人匿名化方法，能够分离说话人身份、语言内容和情感表示。</li>
<li>EASY采用因子蒸馏法，将每种语音属性建模在独立的子空间中。</li>
<li>EASY通过独立约束说话人身份和情感表示，减少了信息泄露，增强了隐私保护。</li>
<li>实验结果表明，EASY在保护说话人隐私的同时，能够保持语言内容和情感状态。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15004">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-7ee9a1493bb5bea6f5fec67ffa6c3d5e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-15a36b4b664f73dd2bbcea98739052c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-14a4e3300ae37e46159b13792cc36ef8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-339c3ee39b7532bd9777afad6ec54210.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a8fd3e4275693a3456e39bb4fb461e6d.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="TCSinger-2-Customizable-Multilingual-Zero-shot-Singing-Voice-Synthesis"><a href="#TCSinger-2-Customizable-Multilingual-Zero-shot-Singing-Voice-Synthesis" class="headerlink" title="TCSinger 2: Customizable Multilingual Zero-shot Singing Voice Synthesis"></a>TCSinger 2: Customizable Multilingual Zero-shot Singing Voice Synthesis</h2><p><strong>Authors:Yu Zhang, Wenxiang Guo, Changhao Pan, Dongyu Yao, Zhiyuan Zhu, Ziyue Jiang, Yuhan Wang, Tao Jin, Zhou Zhao</strong></p>
<p>Customizable multilingual zero-shot singing voice synthesis (SVS) has various potential applications in music composition and short video dubbing. However, existing SVS models overly depend on phoneme and note boundary annotations, limiting their robustness in zero-shot scenarios and producing poor transitions between phonemes and notes. Moreover, they also lack effective multi-level style control via diverse prompts. To overcome these challenges, we introduce TCSinger 2, a multi-task multilingual zero-shot SVS model with style transfer and style control based on various prompts. TCSinger 2 mainly includes three key modules: 1) Blurred Boundary Content (BBC) Encoder, predicts duration, extends content embedding, and applies masking to the boundaries to enable smooth transitions. 2) Custom Audio Encoder, uses contrastive learning to extract aligned representations from singing, speech, and textual prompts. 3) Flow-based Custom Transformer, leverages Cus-MOE, with F0 supervision, enhancing both the synthesis quality and style modeling of the generated singing voice. Experimental results show that TCSinger 2 outperforms baseline models in both subjective and objective metrics across multiple related tasks. </p>
<blockquote>
<p>可定制的多语种零样本歌唱声音合成（SVS）在音乐创作和短视频配音等领域具有多种潜在应用。然而，现有的SVS模型过于依赖音素和音符边界注释，这限制了它们在零样本场景中的稳健性，并导致音素和音符之间的过渡不自然。此外，它们还缺乏通过不同提示进行有效的多级风格控制。为了克服这些挑战，我们引入了TCSinger 2，这是一个基于不同提示进行风格迁移和风格控制的多任务多语种零样本SVS模型。TCSinger 2主要包括三个关键模块：1）模糊边界内容（BBC）编码器，预测持续时间，扩展内容嵌入，并对边界应用掩码以实现平滑过渡。2）自定义音频编码器，利用对比学习从歌唱、语音和文本提示中提取对齐表示。3）基于流的自定义Transformer，利用Cus-MOE和F0监督，提高生成歌唱声音的合成质量和风格建模。实验结果表明，TCSinger 2在多个相关任务中的主观和客观指标上均优于基准模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14910v1">PDF</a> Accepted by ACL 2025</p>
<p><strong>Summary</strong></p>
<p>TCSinger 2是一款多任务多语言零样本歌唱声音合成模型，具有风格迁移和基于不同提示的风格控制功能。该模型通过引入BBC编码器、自定义音频编码器和基于流的自定义转换器等技术，提高了语音合成的质量和灵活性，可应用于音乐创作和短视频配音等领域。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TCSinger 2是一款多语言零样本歌唱声音合成模型，适用于音乐创作和短视频配音。</li>
<li>现有SVS模型过于依赖音素和音符边界注释，限制了其在零样本场景中的稳健性。</li>
<li>TCSinger 2通过引入BBC编码器实现平滑过渡，延长内容嵌入并应用边界掩码。</li>
<li>自定义音频编码器使用对比学习从歌唱、语音和文本提示中提取对齐表示。</li>
<li>流式自定义转换器利用Cus-MOE和F0监督，提高合成质量和风格建模。</li>
<li>实验结果表明，TCSinger 2在多项任务中主观和客观指标上均优于基准模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14910">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-f676ddf43e1dd0499c7ecca871178ea3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-27a4eb1027100c021e210d447c2a5f1f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-99887b38acacc46968bc74cb19d1cd6e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e608cb554b61d11570a3fefe79d414fd.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-24/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-24/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-24/Face%20Swapping/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-6bda9ec0ecfd02e19feb65e875b7f40a.jpg" class="responsive-img" alt="Face Swapping">
                        
                        <span class="card-title">Face Swapping</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Face Swapping 方向最新论文已更新，请持续关注 Update in 2025-05-24  Beyond Face Swapping A Diffusion-Based Digital Human Benchmark for   Multimodal Deepfake Detection
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-24
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Face-Swapping/" class="post-category">
                                    Face Swapping
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Face-Swapping/">
                        <span class="chip bg-color">Face Swapping</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-24/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-a302749a96d55713eb0ccb2cc9352df0.jpg" class="responsive-img" alt="无监督/半监督/对比学习">
                        
                        <span class="card-title">无监督/半监督/对比学习</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            无监督/半监督/对比学习 方向最新论文已更新，请持续关注 Update in 2025-05-24  Visual Perturbation and Adaptive Hard Negative Contrastive Learning for   Compositional Reasoning in Vision-Language Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-24
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    无监督/半监督/对比学习
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">无监督/半监督/对比学习</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">29885.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
