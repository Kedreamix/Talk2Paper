<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-24  Four Eyes Are Better Than Two Harnessing the Collaborative Potential of   Large Models via Differentiated Thinking and Complementary Ensembles">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-3b8b98615086f88163b093fb821de12a.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-24
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    80 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-24-æ›´æ–°"><a href="#2025-05-24-æ›´æ–°" class="headerlink" title="2025-05-24 æ›´æ–°"></a>2025-05-24 æ›´æ–°</h1><h2 id="Four-Eyes-Are-Better-Than-Two-Harnessing-the-Collaborative-Potential-of-Large-Models-via-Differentiated-Thinking-and-Complementary-Ensembles"><a href="#Four-Eyes-Are-Better-Than-Two-Harnessing-the-Collaborative-Potential-of-Large-Models-via-Differentiated-Thinking-and-Complementary-Ensembles" class="headerlink" title="Four Eyes Are Better Than Two: Harnessing the Collaborative Potential of   Large Models via Differentiated Thinking and Complementary Ensembles"></a>Four Eyes Are Better Than Two: Harnessing the Collaborative Potential of   Large Models via Differentiated Thinking and Complementary Ensembles</h2><p><strong>Authors:Jun Xie, Xiongjun Guan, Yingjian Zhu, Zhaoran Zhao, Xinming Wang, Feng Chen, Zhepeng Wang</strong></p>
<p>In this paper, we present the runner-up solution for the Ego4D EgoSchema Challenge at CVPR 2025 (Confirmed on May 20, 2025). Inspired by the success of large models, we evaluate and leverage leading accessible multimodal large models and adapt them to video understanding tasks via few-shot learning and model ensemble strategies. Specifically, diversified prompt styles and process paradigms are systematically explored and evaluated to effectively guide the attention of large models, fully unleashing their powerful generalization and adaptability abilities. Experimental results demonstrate that, with our carefully designed approach, directly utilizing an individual multimodal model already outperforms the previous state-of-the-art (SOTA) method which includes several additional processes. Besides, an additional stage is further introduced that facilitates the cooperation and ensemble of periodic results, which achieves impressive performance improvements. We hope this work serves as a valuable reference for the practical application of large models and inspires future research in the field. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†æˆ‘ä»¬åœ¨CVPR 2025å¹´ä¼šï¼ˆç¡®è®¤äº2025å¹´5æœˆ20æ—¥ä¸¾è¡Œï¼‰ä¸Šæå‡ºçš„Ego4D EgoSchemaæŒ‘æˆ˜èµ›çš„ç¬¬äºŒåè§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬å—åˆ°å¤§å‹æ¨¡å‹æˆåŠŸçš„å¯å‘ï¼Œè¯„ä¼°å¹¶é‡‡ç”¨äº†å‰æ²¿çš„æ˜“äºè®¿é—®çš„å¤šæ¨¡æ€å¤§å‹æ¨¡å‹ï¼Œå¹¶é€šè¿‡å°æ ·æœ¬å­¦ä¹ å’Œæ¨¡å‹é›†æˆç­–ç•¥å°†å®ƒä»¬é€‚åº”åˆ°è§†é¢‘ç†è§£ä»»åŠ¡ä¸­ã€‚å…·ä½“æ¥è¯´ï¼Œç³»ç»Ÿåœ°æ¢ç´¢å¹¶è¯„ä¼°å¤šæ ·åŒ–çš„æç¤ºé£æ ¼å’Œæµç¨‹èŒƒå¼ï¼Œä»¥æœ‰æ•ˆåœ°å¼•å¯¼å¤§å‹æ¨¡å‹çš„æ³¨æ„åŠ›ï¼Œå……åˆ†å‘æŒ¥å®ƒä»¬å¼ºå¤§çš„æ³›åŒ–å’Œé€‚åº”æ€§èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé€šè¿‡æˆ‘ä»¬ç²¾å¿ƒè®¾è®¡çš„æ–¹æ³•ï¼Œç›´æ¥ä½¿ç”¨å•ä¸ªå¤šæ¨¡æ€æ¨¡å‹å·²ç»è¶…è¶Šäº†ä¹‹å‰æœ€å…ˆè¿›çš„ï¼ˆSOTAï¼‰æ–¹æ³•ï¼Œåè€…åŒ…æ‹¬å‡ ä¸ªé¢å¤–çš„æµç¨‹ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†é¢å¤–çš„é˜¶æ®µï¼Œä¿ƒè¿›äº†å‘¨æœŸæ€§ç»“æœçš„åˆä½œå’Œé›†æˆï¼Œå–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½æå‡ã€‚æˆ‘ä»¬å¸Œæœ›è¿™é¡¹å·¥ä½œèƒ½å¤Ÿä¸ºå¤§å‹æ¨¡å‹çš„å®é™…åº”ç”¨æä¾›æœ‰ä»·å€¼çš„å‚è€ƒï¼Œå¹¶æ¿€å‘è¯¥é¢†åŸŸçš„æœªæ¥ç ”ç©¶çµæ„Ÿã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16784v1">PDF</a> </p>
<p><strong>Summary</strong><br>è¯¥è®ºæ–‡ä»‹ç»äº†åœ¨CVPR 2025çš„Ego4D EgoSchemaæŒ‘æˆ˜ä¸­è£è·ç¬¬äºŒåçš„è§£å†³æ–¹æ¡ˆã€‚è¯¥æ–¹æ¡ˆé€šè¿‡è¯„ä¼°å’Œå……åˆ†åˆ©ç”¨å‰æ²¿çš„å¤šæ¨¡æ€å¤§å‹æ¨¡å‹ï¼Œå¹¶å€ŸåŠ©å°‘æ ·æœ¬å­¦ä¹ å’Œæ¨¡å‹é›†æˆç­–ç•¥ï¼Œé€‚åº”è§†é¢‘ç†è§£ä»»åŠ¡ã€‚é€šè¿‡å®éªŒè¯æ˜ï¼Œåˆ©ç”¨ä¸ªåˆ«å•ä¸€çš„å¤šæ¨¡æ€æ¨¡å‹çš„è¡¨ç°å·²è¶…è¶Šå…ˆå‰çš„å…ˆè¿›æ–¹æ³•ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†ä¿ƒè¿›å‘¨æœŸç»“æœçš„åˆä½œå’Œé›†æˆçš„é¢å¤–é˜¶æ®µï¼Œå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚è¿™é¡¹å·¥ä½œå°†ä¸ºå¤§å‹æ¨¡å‹çš„å®é™…åº”ç”¨æä¾›æœ‰ä»·å€¼çš„å‚è€ƒï¼Œå¹¶æ¿€å‘è¯¥é¢†åŸŸçš„æœªæ¥ç ”ç©¶çµæ„Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥è®ºæ–‡æ˜¯CVPR 2025çš„Ego4D EgoSchemaæŒ‘æˆ˜ä¸­çš„ç¬¬äºŒåè§£å†³æ–¹æ¡ˆã€‚</li>
<li>åˆ©ç”¨å‰æ²¿çš„å¤šæ¨¡æ€å¤§å‹æ¨¡å‹å¹¶å€ŸåŠ©å°‘æ ·æœ¬å­¦ä¹ å’Œæ¨¡å‹é›†æˆç­–ç•¥æ¥é€‚åº”è§†é¢‘ç†è§£ä»»åŠ¡ã€‚</li>
<li>é€šè¿‡å®éªŒè¯æ˜åˆ©ç”¨å•ä¸€å¤šæ¨¡æ€æ¨¡å‹å·²è¶…è¶Šå…ˆå‰çš„æ–¹æ³•ã€‚</li>
<li>å¼•å…¥é¢å¤–çš„é˜¶æ®µä¿ƒè¿›å‘¨æœŸç»“æœçš„åˆä½œå’Œé›†æˆï¼Œä»¥æé«˜æ€§èƒ½ã€‚</li>
<li>è¿™é¡¹å·¥ä½œæ—¨åœ¨ä¸ºå¤§å‹æ¨¡å‹çš„å®é™…åº”ç”¨æä¾›å‚è€ƒä»·å€¼ã€‚</li>
<li>è¯¥è®ºæ–‡å±•ç¤ºäº†å¤šæ ·åŒ–çš„æç¤ºé£æ ¼å’Œæµç¨‹èŒƒå¼ï¼Œä»¥æœ‰æ•ˆåœ°å¼•å¯¼å¤§å‹æ¨¡å‹çš„æ³¨æ„åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16784">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ebe90a8dba0e2daa2d5f6470a8a16a42.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7aa684bc4699e3a962ee2dc834ac32fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-664becf7cd08bb900b3090d6f46463b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-811c0f9c9b4bd74c9c04d42f73bbc4b8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a9b9118bad4343419ba8f2f4b2d1f519.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cf2aa5fa774159bb7a5bddd2d58ff987.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Single-Domain-Generalization-for-Few-Shot-Counting-via-Universal-Representation-Matching"><a href="#Single-Domain-Generalization-for-Few-Shot-Counting-via-Universal-Representation-Matching" class="headerlink" title="Single Domain Generalization for Few-Shot Counting via Universal   Representation Matching"></a>Single Domain Generalization for Few-Shot Counting via Universal   Representation Matching</h2><p><strong>Authors:Xianing Chen, Si Huo, Borui Jiang, Hailin Hu, Xinghao Chen</strong></p>
<p>Few-shot counting estimates the number of target objects in an image using only a few annotated exemplars. However, domain shift severely hinders existing methods to generalize to unseen scenarios. This falls into the realm of single domain generalization that remains unexplored in few-shot counting. To solve this problem, we begin by analyzing the main limitations of current methods, which typically follow a standard pipeline that extract the object prototypes from exemplars and then match them with image feature to construct the correlation map. We argue that existing methods overlook the significance of learning highly generalized prototypes. Building on this insight, we propose the first single domain generalization few-shot counting model, Universal Representation Matching, termed URM. Our primary contribution is the discovery that incorporating universal vision-language representations distilled from a large scale pretrained vision-language model into the correlation construction process substantially improves robustness to domain shifts without compromising in domain performance. As a result, URM achieves state-of-the-art performance on both in domain and the newly introduced domain generalization setting. </p>
<blockquote>
<p>å°‘é‡æ ·æœ¬è®¡æ•°ä»…ä½¿ç”¨å°‘é‡å·²æ ‡æ³¨æ ·æœ¬ä¼°è®¡å›¾åƒä¸­ç›®æ ‡å¯¹è±¡çš„æ•°é‡ã€‚ç„¶è€Œï¼Œé¢†åŸŸåç§»ä¸¥é‡é˜»ç¢äº†ç°æœ‰æ–¹æ³•æ¨å¹¿åˆ°æœªè§åœºæ™¯çš„èƒ½åŠ›ã€‚è¿™å±äºå°‘é‡æ ·æœ¬è®¡æ•°ä¸­å°šæœªæ¢ç´¢çš„å•åŸŸæ³›åŒ–é¢†åŸŸã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬é¦–å…ˆåˆ†æäº†å½“å‰æ–¹æ³•çš„ä¸»è¦å±€é™æ€§ï¼Œè¿™äº›æ–¹æ³•é€šå¸¸éµå¾ªæ ‡å‡†æµç¨‹ï¼Œä»æ ·æœ¬ä¸­æå–å¯¹è±¡åŸå‹ï¼Œç„¶åä¸å›¾åƒç‰¹å¾åŒ¹é…ä»¥æ„å»ºå…³è”æ˜ å°„å›¾ã€‚æˆ‘ä»¬è®¤ä¸ºç°æœ‰æ–¹æ³•å¿½è§†äº†å­¦ä¹ é«˜åº¦é€šç”¨åŸå‹çš„é‡è¦æ€§ã€‚åŸºäºè¿™ä¸€è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†ç¬¬ä¸€ä¸ªç”¨äºå•åŸŸæ³›åŒ–çš„å°‘é‡æ ·æœ¬è®¡æ•°æ¨¡å‹â€”â€”é€šç”¨è¡¨ç¤ºåŒ¹é…ï¼ˆURMï¼‰ã€‚æˆ‘ä»¬çš„ä¸»è¦è´¡çŒ®æ˜¯å‘ç°å°†æ¥è‡ªå¤§è§„æ¨¡é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹çš„é€šç”¨è§†è§‰è¯­è¨€è¡¨ç¤ºèå…¥å…³è”æ„å»ºè¿‡ç¨‹ï¼Œå¯ä»¥æ˜¾è‘—æé«˜å¯¹é¢†åŸŸåç§»çš„é²æ£’æ€§ï¼ŒåŒæ—¶ä¸æŸå®³é¢†åŸŸæ€§èƒ½ã€‚å› æ­¤ï¼ŒURMåœ¨ç°æœ‰çš„é¢†åŸŸå†…ä»¥åŠæ–°å¼•å…¥çš„é¢†åŸŸæ³›åŒ–è®¾ç½®ä¸­éƒ½è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16778v1">PDF</a> CVPR 2025</p>
<p><strong>Summary</strong><br>å°‘é‡æ ·æœ¬è®¡æ•°é€šè¿‡ä»…ä½¿ç”¨å°‘é‡æ ‡æ³¨æ ·æœ¬ä¼°è®¡å›¾åƒä¸­çš„ç›®æ ‡å¯¹è±¡æ•°é‡ã€‚ç„¶è€Œï¼Œé¢†åŸŸå·®å¼‚ä¸¥é‡é˜»ç¢äº†ç°æœ‰æ–¹æ³•æ¨å¹¿åˆ°æœªè§åœºæ™¯çš„èƒ½åŠ›ï¼Œè¿™å½’å±äºæœªè§é¢†åŸŸçš„ä¸€èˆ¬åŒ–é—®é¢˜ï¼Œåœ¨å°‘é‡æ ·æœ¬è®¡æ•°ä¸­å°šæœªè¢«æ¢ç´¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡åˆ†æäº†å½“å‰æ–¹æ³•çš„ä¸»è¦å±€é™æ€§ï¼Œå®ƒä»¬é€šå¸¸éµå¾ªä»æ ·æœ¬ä¸­æå–å¯¹è±¡åŸå‹å¹¶ä¸å›¾åƒç‰¹å¾åŒ¹é…ä»¥æ„å»ºå…³è”å›¾çš„ç®¡é“ã€‚æœ¬æ–‡è®¤ä¸ºç°æœ‰æ–¹æ³•å¿½è§†äº†å­¦ä¹ é«˜åº¦é€šç”¨åŸå‹çš„é‡è¦æ€§ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†é¦–ä¸ªç”¨äºå•é¢†åŸŸæ³›åŒ–çš„å°‘é‡æ ·æœ¬è®¡æ•°æ¨¡å‹â€”â€”é€šç”¨è¡¨ç¤ºåŒ¹é…ï¼ˆURMï¼‰ã€‚æˆ‘ä»¬çš„ä¸»è¦è´¡çŒ®æ˜¯å‘ç°å°†æ¥è‡ªå¤§è§„æ¨¡é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹çš„é€šç”¨è§†è§‰è¯­è¨€è¡¨ç¤ºèå…¥å…³è”æ„å»ºè¿‡ç¨‹ï¼Œå¯å¤§å¹…æé«˜å¯¹ä¸åŒé¢†åŸŸçš„é€‚åº”æ€§ï¼ŒåŒæ—¶ä¸æŸå¤±åŸæœ‰é¢†åŸŸçš„æ€§èƒ½ã€‚å› æ­¤ï¼ŒURMåœ¨åŸæœ‰é¢†åŸŸå’Œå…¨æ–°å¼•å…¥çš„é¢†åŸŸæ³›åŒ–è®¾ç½®ä¸Šéƒ½è¾¾åˆ°äº†æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å°‘é‡æ ·æœ¬è®¡æ•°æ—¨åœ¨åˆ©ç”¨å°‘é‡æ ‡æ³¨æ ·æœ¬ä¼°è®¡å›¾åƒä¸­çš„ç›®æ ‡æ•°é‡ã€‚</li>
<li>é¢†åŸŸå·®å¼‚æ˜¯å½“å‰æ–¹æ³•çš„æ¨å¹¿ç“¶é¢ˆï¼Œå°¤å…¶æ˜¯åœ¨æœªè§é¢†åŸŸçš„ä¸€èˆ¬åŒ–é—®é¢˜ä¸Šã€‚</li>
<li>å½“å‰æ–¹æ³•ä¸»è¦å±€é™äºæ ‡å‡†ç®¡é“ï¼Œå³ä»æ ·æœ¬ä¸­æå–å¯¹è±¡åŸå‹å¹¶ä¸å›¾åƒç‰¹å¾åŒ¹é…ä»¥æ„å»ºå…³è”å›¾ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¿½è§†äº†å­¦ä¹ é«˜åº¦é€šç”¨çš„åŸå‹çš„é‡è¦æ€§ã€‚</li>
<li>æå‡ºé¦–ä¸ªç”¨äºå•é¢†åŸŸæ³›åŒ–çš„å°‘é‡æ ·æœ¬è®¡æ•°æ¨¡å‹â€”â€”URMã€‚</li>
<li>URMé€šè¿‡å°†é€šç”¨è§†è§‰è¯­è¨€è¡¨ç¤ºèå…¥å…³è”æ„å»ºè¿‡ç¨‹ï¼Œæé«˜äº†å¯¹ä¸åŒé¢†åŸŸçš„é€‚åº”æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16778">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b7028f24b26e96231f618498243dec77.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-69b94eab584b7cba9ba84bc565416601.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1be13f683e93913f8b1b0378482f3b4f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21b48b19dd24a5d465546a4c5de022d7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8e6817fb2f197b73cc22726bc7e6c1a1.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Meta-reinforcement-learning-with-minimum-attention"><a href="#Meta-reinforcement-learning-with-minimum-attention" class="headerlink" title="Meta-reinforcement learning with minimum attention"></a>Meta-reinforcement learning with minimum attention</h2><p><strong>Authors:Pilhwa Lee, Shashank Gupta</strong></p>
<p>Minimum attention applies the least action principle in the changes of control concerning state and time, first proposed by Brockett. The involved regularization is highly relevant in emulating biological control, such as motor learning. We apply minimum attention in reinforcement learning (RL) as part of the rewards and investigate its connection to meta-learning and stabilization. Specifically, model-based meta-learning with minimum attention is explored in high-dimensional nonlinear dynamics. Ensemble-based model learning and gradient-based meta-policy learning are alternately performed. Empirically, we show that the minimum attention does show outperforming competence in comparison to the state-of-the-art algorithms in model-free and model-based RL, i.e., fast adaptation in few shots and variance reduction from the perturbations of the model and environment. Furthermore, the minimum attention demonstrates the improvement in energy efficiency. </p>
<blockquote>
<p>æœ€å°æ³¨æ„åŠ›éµå¾ªå¸ƒç½—å…‹ç‰¹ï¼ˆBrockettï¼‰æå‡ºçš„å…³äºçŠ¶æ€å’Œæ—¶é—´çš„æ§åˆ¶å˜åŒ–ä¸­çš„æœ€å°ä½œç”¨åŸåˆ™ã€‚è¿™ç§æ¶‰åŠçš„è§„åˆ™åŒ–ä¸æ¨¡æ‹Ÿç”Ÿç‰©æ§åˆ¶ï¼ˆå¦‚è¿åŠ¨å­¦ä¹ ï¼‰å¯†åˆ‡ç›¸å…³ã€‚æˆ‘ä»¬å°†æœ€å°æ³¨æ„åŠ›åº”ç”¨äºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä½œä¸ºå¥–åŠ±çš„ä¸€éƒ¨åˆ†ï¼Œå¹¶ç ”ç©¶å…¶ä¸å…ƒå­¦ä¹ å’Œç¨³å®šåŒ–çš„è”ç³»ã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨é«˜ç»´éçº¿æ€§åŠ¨åŠ›å­¦ä¸­æ¢ç´¢äº†åŸºäºæ¨¡å‹çš„å…ƒå­¦ä¹ ä¸æœ€å°æ³¨æ„åŠ›çš„ç»“åˆã€‚åŸºäºé›†åˆçš„æ¨¡å‹å­¦ä¹ å’ŒåŸºäºæ¢¯åº¦çš„å…ƒç­–ç•¥å­¦ä¹ äº¤æ›¿è¿›è¡Œã€‚ä»å®è¯ä¸Šçœ‹ï¼Œæˆ‘ä»¬è¯æ˜äº†æœ€å°æ³¨æ„åŠ›åœ¨ä¸æ— æ¨¡å‹å’ŒåŸºäºæ¨¡å‹çš„æœ€æ–°RLç®—æ³•çš„æ¯”è¾ƒä¸­è¡¨ç°å‡ºä¼˜è¶Šçš„ç«äº‰åŠ›ï¼Œä¾‹å¦‚å¿«é€Ÿé€‚åº”å°‘æ•°æ ·æœ¬å’Œå‡å°‘æ¨¡å‹å’Œç¯å¢ƒçš„æ‰°åŠ¨å¸¦æ¥çš„æ–¹å·®ã€‚æ­¤å¤–ï¼Œæœ€å°æ³¨æ„åŠ›è¿˜è¯æ˜äº†å…¶åœ¨èƒ½æ•ˆæ–¹é¢çš„æå‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16741v1">PDF</a> 10 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>æœ€å°æ³¨æ„åŠ›åŸç†å°†æœ€å°‘è¡ŒåŠ¨åŸåˆ™åº”ç”¨äºçŠ¶æ€å’Œæ—¶é—´æ§åˆ¶çš„å˜åŒ–ä¸­ï¼Œæœ€æ—©ç”±Brockettæå‡ºã€‚è¯¥åŸç†åœ¨æ¨¡æ‹Ÿç”Ÿç‰©æ§åˆ¶ï¼Œå¦‚è¿åŠ¨å­¦ä¹ æ–¹é¢å…·æœ‰å¾ˆé«˜çš„ç›¸å…³æ€§ã€‚æˆ‘ä»¬åœ¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸­åº”ç”¨æœ€å°æ³¨æ„åŠ›ä½œä¸ºå¥–åŠ±çš„ä¸€éƒ¨åˆ†ï¼Œå¹¶ç ”ç©¶å…¶ä¸å…ƒå­¦ä¹ å’Œç¨³å®šåŒ–çš„è”ç³»ã€‚ç‰¹åˆ«æ˜¯åœ¨é«˜ç»´éçº¿æ€§åŠ¨åŠ›å­¦ä¸­ï¼Œæ¢ç´¢äº†åŸºäºæ¨¡å‹çš„å…ƒå­¦ä¹ ä¸æœ€å°æ³¨æ„åŠ›çš„ç»“åˆã€‚æœ¬æ–‡äº¤æ›¿è¿›è¡ŒåŸºäºé›†åˆçš„æ¨¡å‹å­¦ä¹ å’ŒåŸºäºæ¢¯åº¦çš„å…ƒç­–ç•¥å­¦ä¹ ã€‚ç»éªŒè¡¨æ˜ï¼Œä¸æœ€æ–°çš„æ¨¡å‹æ— å…³å’ŒåŸºäºæ¨¡å‹çš„RLç®—æ³•ç›¸æ¯”ï¼Œæœ€å°æ³¨æ„åŠ›åœ¨å¿«é€Ÿé€‚åº”å’Œå‡å°‘æ¨¡å‹å’Œç¯å¢ƒçš„æ‰°åŠ¨å¼•èµ·çš„æ–¹å·®æ–¹é¢è¡¨ç°å‡ºæ›´é«˜çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæœ€å°æ³¨æ„åŠ›åœ¨æé«˜èƒ½æºæ•ˆç‡æ–¹é¢ä¹Ÿæ˜¾ç¤ºå‡ºæ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ€å°æ³¨æ„åŠ›åŸç†æ¶‰åŠæœ€å°‘è¡ŒåŠ¨åŸåˆ™ï¼Œåº”ç”¨äºçŠ¶æ€å’Œæ—¶é—´æ§åˆ¶çš„æ”¹å˜ã€‚</li>
<li>æœ€å°æ³¨æ„åŠ›åœ¨å¼ºåŒ–å­¦ä¹ ä¸­ä½œä¸ºå¥–åŠ±çš„ä¸€éƒ¨åˆ†è¢«ç ”ç©¶ï¼Œå¹¶ä¸å…ƒå­¦ä¹ å’Œç¨³å®šåŒ–æœ‰å…³è”ã€‚</li>
<li>åœ¨é«˜ç»´éçº¿æ€§åŠ¨åŠ›å­¦ä¸­ï¼ŒåŸºäºæ¨¡å‹çš„å…ƒå­¦ä¹ ä¸æœ€å°æ³¨æ„åŠ›ç›¸ç»“åˆè¿›è¡Œäº†æ¢ç´¢ã€‚</li>
<li>ç ”ç©¶ä¸­äº¤æ›¿è¿›è¡Œäº†åŸºäºé›†åˆçš„æ¨¡å‹å­¦ä¹ å’ŒåŸºäºæ¢¯åº¦çš„å…ƒç­–ç•¥å­¦ä¹ ã€‚</li>
<li>æœ€å°æ³¨æ„åŠ›åœ¨å¿«é€Ÿé€‚åº”å’Œå‡å°‘æ¨¡å‹åŠç¯å¢ƒæ‰°åŠ¨å¼•èµ·çš„æ–¹å·®æ–¹é¢è¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ã€‚</li>
<li>æœ€å°æ³¨æ„åŠ›åœ¨æé«˜èƒ½æºæ•ˆç‡æ–¹é¢æœ‰æ‰€æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16741">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-86925ac42f6f37aa6b8336429fb71b62.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b7237ccdab8a3311b76ffb4b03a885a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8b2a0f9703fa402b512612c626aaaf9b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b1b2a6e9e02df9b659bfa7c5bd11a523.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7cb8908d0668f32cea9fd944ad98b182.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="MAPLE-Many-Shot-Adaptive-Pseudo-Labeling-for-In-Context-Learning"><a href="#MAPLE-Many-Shot-Adaptive-Pseudo-Labeling-for-In-Context-Learning" class="headerlink" title="MAPLE: Many-Shot Adaptive Pseudo-Labeling for In-Context Learning"></a>MAPLE: Many-Shot Adaptive Pseudo-Labeling for In-Context Learning</h2><p><strong>Authors:Zihan Chen, Song Wang, Zhen Tan, Jundong Li, Cong Shen</strong></p>
<p>In-Context Learning (ICL) empowers Large Language Models (LLMs) to tackle diverse tasks by incorporating multiple input-output examples, known as demonstrations, into the input of LLMs. More recently, advancements in the expanded context windows of LLMs have led to many-shot ICL, which uses hundreds of demonstrations and outperforms few-shot ICL, which relies on fewer examples. However, this approach is often hindered by the high cost of obtaining large amounts of labeled data. To address this challenge, we propose Many-Shot Adaptive Pseudo-LabEling, namely MAPLE, a novel influence-based many-shot ICL framework that utilizes pseudo-labeled samples to compensate for the lack of label information. We first identify a subset of impactful unlabeled samples and perform pseudo-labeling on them by querying LLMs. These pseudo-labeled samples are then adaptively selected and tailored to each test query as input to improve the performance of many-shot ICL, without significant labeling costs. Extensive experiments on real-world datasets demonstrate the effectiveness of our framework, showcasing its ability to enhance LLM adaptability and performance with limited labeled data. </p>
<blockquote>
<p>ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰é€šè¿‡èå…¥å¤šä¸ªè¾“å…¥è¾“å‡ºç¤ºä¾‹ï¼ˆä¹Ÿç§°ä¸ºæ¼”ç¤ºï¼‰åˆ°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¾“å…¥ä¸­ï¼Œä½¿LLMèƒ½å¤Ÿå¤„ç†å¤šæ ·åŒ–ä»»åŠ¡ã€‚æœ€è¿‘ï¼ŒLLMæ‰©å±•çš„ä¸Šä¸‹æ–‡çª—å£çš„è¿›æ­¥å¯¼è‡´äº†å¤šç¤ºä¾‹ICLçš„å‡ºç°ï¼Œå®ƒä½¿ç”¨æ•°ç™¾ä¸ªæ¼”ç¤ºï¼Œå¹¶ä¼˜äºå°‘ç¤ºä¾‹ICLï¼Œåè€…ä¾èµ–äºè¾ƒå°‘çš„ä¾‹å­ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•å¸¸å¸¸å—åˆ°è·å–å¤§é‡æ ‡è®°æ•°æ®çš„é«˜æˆæœ¬çš„é˜»ç¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºå½±å“çš„å¤šç¤ºä¾‹è‡ªé€‚åº”ä¼ªæ ‡ç­¾æŠ€æœ¯ï¼ˆMAPLEï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å¤šç¤ºä¾‹ICLæ¡†æ¶ï¼Œå®ƒåˆ©ç”¨ä¼ªæ ‡ç­¾æ ·æœ¬æ¥å¼¥è¡¥æ ‡ç­¾ä¿¡æ¯çš„ç¼ºä¹ã€‚æˆ‘ä»¬é¦–å…ˆè¯†åˆ«å‡ºæœ‰å½±å“åŠ›çš„æœªæ ‡è®°æ ·æœ¬å­é›†ï¼Œå¹¶é€šè¿‡æŸ¥è¯¢LLMå¯¹å®ƒä»¬è¿›è¡Œä¼ªæ ‡è®°ã€‚è¿™äº›ä¼ªæ ‡è®°çš„æ ·æœ¬éšåè¢«è‡ªé€‚åº”åœ°é€‰æ‹©å’Œé’ˆå¯¹æ¯ä¸ªæµ‹è¯•æŸ¥è¯¢ä½œä¸ºè¾“å…¥ï¼Œä»¥æé«˜å¤šç¤ºä¾‹ICLçš„æ€§èƒ½ï¼Œè€Œæ— éœ€æ‰¿æ‹…å¤§é‡çš„æ ‡è®°æˆæœ¬ã€‚åœ¨çœŸå®æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜äº†æˆ‘ä»¬æ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œå±•ç¤ºäº†å…¶åœ¨æœ‰é™æ ‡è®°æ•°æ®ä¸‹æé«˜LLMé€‚åº”æ€§å’Œæ€§èƒ½çš„èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16225v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡ç»“åˆå¤šä¸ªè¾“å…¥è¾“å‡ºèŒƒä¾‹ï¼ˆå³æ¼”ç¤ºï¼‰æ¥æ‰§è¡Œå¤šç§ä»»åŠ¡ã€‚æœ€è¿‘ï¼ŒLLMæ‰©å±•çš„ä¸Šä¸‹æ–‡çª—å£å¯¼è‡´å‡ºç°å¤šç¤ºä¾‹ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ï¼Œå®ƒä½¿ç”¨æ•°ç™¾ä¸ªæ¼”ç¤ºï¼Œå¹¶ä¼˜äºå°‘ç¤ºä¾‹ICLã€‚ç„¶è€Œï¼Œè·å–å¤§é‡æ ‡è®°æ•°æ®çš„æˆæœ¬å¾ˆé«˜ï¼Œæˆä¸ºä¸€å¤§æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åä¸ºMAPLEçš„æ–°å‹åŸºäºå½±å“çš„å¤šç¤ºä¾‹ICLæ¡†æ¶ï¼Œåˆ©ç”¨ä¼ªæ ‡è®°æ ·æœ¬æ¥å¼¥è¡¥æ ‡ç­¾ä¿¡æ¯çš„ç¼ºä¹ã€‚æˆ‘ä»¬é¦–å…ˆè¯†åˆ«å‡ºæœ‰å½±å“åŠ›çš„æœªæ ‡è®°æ ·æœ¬å­é›†å¹¶å¯¹å®ƒä»¬è¿›è¡Œä¼ªæ ‡è®°æŸ¥è¯¢LLMã€‚è¿™äº›ä¼ªæ ‡è®°æ ·æœ¬éšåè¢«è‡ªé€‚åº”é€‰æ‹©å¹¶é’ˆå¯¹æ¯ä¸ªæµ‹è¯•æŸ¥è¯¢è¿›è¡Œè¾“å…¥ï¼Œä»¥æé«˜å¤šç¤ºä¾‹ICLçš„æ€§èƒ½ï¼ŒåŒæ—¶æ— éœ€å¤§é‡æ ‡è®°æˆæœ¬ã€‚åœ¨çœŸå®æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜äº†æ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œå±•ç¤ºäº†å…¶åœ¨æœ‰é™æ ‡è®°æ•°æ®ä¸‹æé«˜LLMé€‚åº”æ€§å’Œæ€§èƒ½çš„èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>In-Context Learning (ICL)å…è®¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç»“åˆå¤šä¸ªè¾“å…¥è¾“å‡ºèŒƒä¾‹æ¥æ‰§è¡Œå¤šæ ·åŒ–ä»»åŠ¡ã€‚</li>
<li>å¤šç¤ºä¾‹ICLä½¿ç”¨æ•°ç™¾ä¸ªæ¼”ç¤ºæ ·æœ¬ï¼Œå¹¶è¡¨ç°å‡ºä¼˜äºå°‘ç¤ºä¾‹ICLçš„æ€§èƒ½ã€‚</li>
<li>è·å–å¤§é‡æ ‡è®°æ•°æ®çš„æˆæœ¬é«˜æ˜‚ï¼Œæ˜¯ICLçš„ä¸»è¦æŒ‘æˆ˜ä¹‹ä¸€ã€‚</li>
<li>MAPLEæ¡†æ¶åˆ©ç”¨ä¼ªæ ‡è®°æ ·æœ¬æ¥å¼¥è¡¥æ ‡ç­¾ä¿¡æ¯çš„ç¼ºä¹ï¼Œæé«˜å¤šç¤ºä¾‹ICLçš„æ€§èƒ½ã€‚</li>
<li>MAPLEé€šè¿‡è¯†åˆ«æœ‰å½±å“åŠ›çš„æœªæ ‡è®°æ ·æœ¬å¹¶è¿›è¡Œä¼ªæ ‡è®°æŸ¥è¯¢LLMï¼Œæ¥é€‚åº”æ¯ä¸ªæµ‹è¯•æŸ¥è¯¢ã€‚</li>
<li>å¹¿æ³›å®éªŒè¯æ˜MAPLEæ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œåœ¨çœŸå®æ•°æ®é›†ä¸Šæé«˜äº†LLMçš„é€‚åº”æ€§å’Œæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16225">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a8487a96efe3787bdfddd505cb7056a5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4f6e7af72d97dfd66cc8ebe302a94527.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f4e07756865fc8648ea5f8b6817b2541.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Meta-PerSER-Few-Shot-Listener-Personalized-Speech-Emotion-Recognition-via-Meta-learning"><a href="#Meta-PerSER-Few-Shot-Listener-Personalized-Speech-Emotion-Recognition-via-Meta-learning" class="headerlink" title="Meta-PerSER: Few-Shot Listener Personalized Speech Emotion Recognition   via Meta-learning"></a>Meta-PerSER: Few-Shot Listener Personalized Speech Emotion Recognition   via Meta-learning</h2><p><strong>Authors:Liang-Yeh Shen, Shi-Xin Fang, Yi-Cheng Lin, Huang-Cheng Chou, Hung-yi Lee</strong></p>
<p>This paper introduces Meta-PerSER, a novel meta-learning framework that personalizes Speech Emotion Recognition (SER) by adapting to each listenerâ€™s unique way of interpreting emotion. Conventional SER systems rely on aggregated annotations, which often overlook individual subtleties and lead to inconsistent predictions. In contrast, Meta-PerSER leverages a Model-Agnostic Meta-Learning (MAML) approach enhanced with Combined-Set Meta-Training, Derivative Annealing, and per-layer per-step learning rates, enabling rapid adaptation with only a few labeled examples. By integrating robust representations from pre-trained self-supervised models, our framework first captures general emotional cues and then fine-tunes itself to personal annotation styles. Experiments on the IEMOCAP corpus demonstrate that Meta-PerSER significantly outperforms baseline methods in both seen and unseen data scenarios, highlighting its promise for personalized emotion recognition. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†Meta-PerSERï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹å…ƒå­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡é€‚åº”æ¯ä¸ªå¬ä¼—ç‹¬ç‰¹çš„æƒ…æ„Ÿè§£è¯»æ–¹å¼æ¥ä¸ªæ€§åŒ–è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰ã€‚ä¼ ç»Ÿçš„SERç³»ç»Ÿä¾èµ–äºèšåˆæ³¨é‡Šï¼Œè¿™å¾€å¾€ä¼šå¿½ç•¥ä¸ªä½“ç»†å¾®å·®åˆ«å¹¶å¯¼è‡´é¢„æµ‹ç»“æœä¸ä¸€è‡´ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒMeta-PerSERåˆ©ç”¨ç»“åˆé›†åˆå…ƒè®­ç»ƒã€å¯¼æ•°é€€ç«å’Œé€å±‚åˆ†æ­¥å­¦ä¹ ç‡çš„æ¨¡å‹æ— å…³å…ƒå­¦ä¹ æ–¹æ³•ï¼ˆMAMLï¼‰ï¼Œä»…é€šè¿‡å°‘é‡æ ‡æ³¨æ ·æœ¬å³å¯å®ç°å¿«é€Ÿé€‚åº”ã€‚é€šè¿‡æ•´åˆé¢„è®­ç»ƒè‡ªç›‘ç£æ¨¡å‹çš„ç¨³å¥è¡¨ç¤ºï¼Œæˆ‘ä»¬çš„æ¡†æ¶é¦–å…ˆæ•è·ä¸€èˆ¬æƒ…æ„Ÿçº¿ç´¢ï¼Œç„¶åå¯¹è‡ªèº«è¿›è¡Œå¾®è°ƒä»¥é€‚åº”ä¸ªäººæ³¨é‡Šé£æ ¼ã€‚åœ¨IEMOCAPè¯­æ–™åº“ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMeta-PerSERåœ¨å·²è§å’Œæœªè§æ•°æ®åœºæ™¯ä¸­å‡æ˜¾è‘—ä¼˜äºåŸºå‡†æ–¹æ³•ï¼Œå‡¸æ˜¾å…¶åœ¨ä¸ªæ€§åŒ–æƒ…æ„Ÿè¯†åˆ«æ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16220v1">PDF</a> Accepted by INTERSPEECH 2025. 7 pages, including 2 pages of appendix</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†ä¸€ç§æ–°å‹çš„å…ƒå­¦ä¹ æ¡†æ¶Meta-PerSERï¼Œå®ƒé€šè¿‡ä¸ªæ€§åŒ–è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰æ¥é€‚åº”æ¯ä¸ªå¬ä¼—ç‹¬ç‰¹çš„æƒ…æ„Ÿè§£è¯»æ–¹å¼ã€‚è¯¥æ¡†æ¶é‡‡ç”¨æ¨¡å‹æ— å…³çš„å…ƒå­¦ä¹ æ–¹æ³•ï¼Œå¹¶ç»“åˆé›†åˆå…ƒè®­ç»ƒã€å¯¼æ•°é€€ç«ä»¥åŠåˆ†å±‚åˆ†æ­¥éª¤çš„å­¦ä¹ ç‡è°ƒæ•´æŠ€æœ¯ï¼Œèƒ½åœ¨ä»…ä½¿ç”¨å°‘é‡æ ‡æ³¨æ ·æœ¬çš„æƒ…å†µä¸‹å¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¼•å…¥é¢„è®­ç»ƒè‡ªç›‘ç£æ¨¡å‹çš„ç¨³å¥è¡¨ç¤ºï¼ŒMeta-PerSERä¸ä»…èƒ½æ•æ‰ä¸€èˆ¬æƒ…æ„Ÿçº¿ç´¢ï¼Œè¿˜èƒ½é’ˆå¯¹ä¸ªäººæ ‡æ³¨é£æ ¼è¿›è¡Œå¾®è°ƒã€‚åœ¨IEMOCAPè¯­æ–™åº“ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMeta-PerSERåœ¨å¯è§å’Œæœªè§æ•°æ®åœºæ™¯ä¸‹å‡æ˜¾è‘—ä¼˜äºåŸºå‡†æ–¹æ³•ï¼Œå±•ç°å‡ºä¸ªæ€§åŒ–æƒ…æ„Ÿè¯†åˆ«çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Meta-PerSERæ˜¯ä¸€ç§æ–°é¢–çš„å…ƒå­¦ä¹ æ¡†æ¶ï¼Œç”¨äºä¸ªæ€§åŒ–è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡é€‚åº”æ¯ä¸ªå¬ä¼—ç‹¬ç‰¹çš„æƒ…æ„Ÿè§£è¯»æ–¹å¼ï¼Œæé«˜äº†SERç³»ç»Ÿçš„æ€§èƒ½ã€‚</li>
<li>Meta-PerSERé‡‡ç”¨æ¨¡å‹æ— å…³çš„å…ƒå­¦ä¹ æ–¹æ³•ï¼Œå¹¶ç»“åˆå¤šç§æŠ€æœ¯ï¼Œå¦‚é›†åˆå…ƒè®­ç»ƒã€å¯¼æ•°é€€ç«å’Œåˆ†å±‚åˆ†æ­¥éª¤çš„å­¦ä¹ ç‡è°ƒæ•´ï¼Œå®ç°å¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡çš„èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡å¼•å…¥é¢„è®­ç»ƒè‡ªç›‘ç£æ¨¡å‹çš„ç¨³å¥è¡¨ç¤ºï¼ŒMeta-PerSERèƒ½æ•æ‰ä¸€èˆ¬æƒ…æ„Ÿçº¿ç´¢å¹¶é€‚åº”ä¸ªäººæ ‡æ³¨é£æ ¼ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒMeta-PerSERåœ¨å¯è§å’Œæœªè§æ•°æ®åœºæ™¯ä¸‹å‡æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚</li>
<li>Meta-PerSERåœ¨ä¸ªæ€§åŒ–æƒ…æ„Ÿè¯†åˆ«é¢†åŸŸå±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16220">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-98396fab6f4c5d3e136d37e8bc0160f7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7d7977f75f5b3fc31a274d7accffb250.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-17d15c4264b428f552e5e09d9325260c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fca87e70dfa41f2df87ed81868b3973f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-80dcacaf3b3ba3f547674cc3ae5cd2ff.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f3b2ed53ab1212067f259231368920b9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-33ff0db73f47b59732e05cecc4af9651.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="VLM-R-3-Region-Recognition-Reasoning-and-Refinement-for-Enhanced-Multimodal-Chain-of-Thought"><a href="#VLM-R-3-Region-Recognition-Reasoning-and-Refinement-for-Enhanced-Multimodal-Chain-of-Thought" class="headerlink" title="VLM-R$^3$: Region Recognition, Reasoning, and Refinement for Enhanced   Multimodal Chain-of-Thought"></a>VLM-R$^3$: Region Recognition, Reasoning, and Refinement for Enhanced   Multimodal Chain-of-Thought</h2><p><strong>Authors:Chaoya Jiang, Yongrui Heng, Wei Ye, Han Yang, Haiyang Xu, Ming Yan, Ji Zhang, Fei Huang, Shikun Zhang</strong></p>
<p>Recently, reasoning-based MLLMs have achieved a degree of success in generating long-form textual reasoning chains. However, they still struggle with complex tasks that necessitate dynamic and iterative focusing on and revisiting of visual regions to achieve precise grounding of textual reasoning in visual evidence. We introduce \textbf{VLM-R$^3$} (\textbf{V}isual \textbf{L}anguage \textbf{M}odel with \textbf{R}egion \textbf{R}ecognition and \textbf{R}easoning), a framework that equips an MLLM with the ability to (i) decide \emph{when} additional visual evidence is needed, (ii) determine \emph{where} to ground within the image, and (iii) seamlessly weave the relevant sub-image content back into an interleaved chain-of-thought. The core of our method is \textbf{Region-Conditioned Reinforcement Policy Optimization (R-GRPO)}, a training paradigm that rewards the model for selecting informative regions, formulating appropriate transformations (e.g.\ crop, zoom), and integrating the resulting visual context into subsequent reasoning steps. To bootstrap this policy, we compile a modest but carefully curated Visuo-Lingual Interleaved Rationale (VLIR) corpus that provides step-level supervision on region selection and textual justification. Extensive experiments on MathVista, ScienceQA, and other benchmarks show that VLM-R$^3$ sets a new state of the art in zero-shot and few-shot settings, with the largest gains appearing on questions demanding subtle spatial reasoning or fine-grained visual cue extraction. </p>
<blockquote>
<p>æœ€è¿‘ï¼ŒåŸºäºæ¨ç†çš„MLLMsï¼ˆMasked Language Modelingï¼‰åœ¨ç”Ÿæˆé•¿æ–‡æœ¬æ¨ç†é“¾æ–¹é¢å–å¾—äº†ä¸€å®šçš„æˆåŠŸã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨å¤„ç†å¤æ‚çš„ä»»åŠ¡æ—¶ä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼Œè¿™äº›ä»»åŠ¡éœ€è¦åŠ¨æ€å’Œè¿­ä»£åœ°å…³æ³¨å¹¶å›é¡¾è§†è§‰åŒºåŸŸï¼Œä»¥å®ç°æ–‡æœ¬æ¨ç†åœ¨è§†è§‰è¯æ®ä¸­çš„ç²¾ç¡®å®šä½ã€‚æˆ‘ä»¬å¼•å…¥äº†<strong>VLM-R$^3$<strong>ï¼ˆå¸¦åŒºåŸŸè¯†åˆ«å’Œæ¨ç†çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼‰ï¼Œè¿™ä¸€æ¡†æ¶ä½¿MLLMå…·å¤‡ä»¥ä¸‹èƒ½åŠ›ï¼šï¼ˆiï¼‰å†³å®šä½•æ—¶éœ€è¦é¢å¤–çš„è§†è§‰è¯æ®ï¼›ï¼ˆiiï¼‰ç¡®å®šåœ¨å›¾åƒä¸­çš„å®šä½ä½ç½®ï¼›ï¼ˆiiiï¼‰æ— ç¼åœ°å°†ç›¸å…³çš„å­å›¾åƒå†…å®¹é‡æ–°ç¼–ç»‡æˆè¿è´¯çš„æ¨ç†é“¾ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„æ ¸å¿ƒæ˜¯</strong>åŒºåŸŸæ¡ä»¶å¼ºåŒ–ç­–ç•¥ä¼˜åŒ–ï¼ˆR-GRPOï¼‰</strong>ï¼Œè¿™æ˜¯ä¸€ç§è®­ç»ƒèŒƒå¼ï¼Œå¥–åŠ±æ¨¡å‹é€‰æ‹©ä¿¡æ¯åŒºåŸŸã€åˆ¶å®šé€‚å½“çš„è½¬æ¢ï¼ˆä¾‹å¦‚è£å‰ªã€æ”¾å¤§ï¼‰å¹¶å°†æ‰€å¾—çš„è§†è§‰ä¸Šä¸‹æ–‡é›†æˆåˆ°éšåçš„æ¨ç†æ­¥éª¤ä¸­ã€‚ä¸ºäº†å¼•å¯¼ç­–ç•¥ï¼Œæˆ‘ä»¬æ•´ç†äº†ä¸€ä¸ªè§„æ¨¡ä¸å¤§ä½†ç²¾å¿ƒç­–åˆ’çš„Visuo-Lingual Interleaved Rationaleï¼ˆVLIRï¼‰è¯­æ–™åº“ï¼Œè¯¥è¯­æ–™åº“ä¸ºåŒºåŸŸé€‰æ‹©å’Œæ–‡æœ¬è®ºè¯æä¾›äº†æ­¥éª¤çº§çš„ç›‘ç£ã€‚åœ¨MathVistaã€ScienceQAå’Œå…¶ä»–åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒVLM-R$^3$åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬è®¾ç½®ä¸Šåˆ›ä¸‹äº†æ–°çš„æŠ€æœ¯çºªå½•ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦å¾®å¦™çš„ç©ºé—´æ¨ç†æˆ–ç²¾ç»†çš„è§†è§‰çº¿ç´¢æå–çš„é—®é¢˜ä¸Šï¼Œå…¶å¢ç›Šæœ€ä¸ºæ˜¾è‘—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16192v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨åŸºäºæ–‡æœ¬çš„å¤§å‹è¯­è¨€æ¨¡å‹å–å¾—é•¿æœŸæ¨ç†é“¾ç”Ÿæˆçš„æˆåŠŸä¹‹åï¼Œé¢å¯¹éœ€è¦åœ¨è§†è§‰è¯æ®ä¸­è¿›è¡Œç²¾ç¡®æ–‡æœ¬æ¨ç†çš„å¤æ‚ä»»åŠ¡ï¼Œå®ƒä»¬ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†VLM-RÂ³æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹é…å¤‡äº†ä¸‰é¡¹èƒ½åŠ›ï¼šå†³å®šä½•æ—¶éœ€è¦é¢å¤–çš„è§†è§‰è¯æ®ã€ç¡®å®šåœ¨å›¾åƒä¸­çš„å®šä½ï¼Œä»¥åŠæ— ç¼åœ°å°†ç›¸å…³å­å›¾åƒå†…å®¹èå…¥æ€ç»´é“¾ä¸­ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒæ˜¯åŒºåŸŸæ¡ä»¶å¼ºåŒ–æ”¿ç­–ä¼˜åŒ–ï¼ˆR-GRPOï¼‰è®­ç»ƒèŒƒå¼ï¼Œå¥–åŠ±æ¨¡å‹é€‰æ‹©ä¿¡æ¯åŒºåŸŸã€åˆ¶å®šé€‚å½“çš„è½¬æ¢ï¼ˆå¦‚è£å‰ªã€ç¼©æ”¾ï¼‰ï¼Œå¹¶å°†æ‰€å¾—çš„è§†è§‰ä¸Šä¸‹æ–‡èå…¥åç»­æ¨ç†æ­¥éª¤ã€‚ä¸ºäº†å¼•å¯¼æ”¿ç­–ï¼Œæˆ‘ä»¬ç²¾å¿ƒç¼–åˆ¶äº†Visuo-Lingualäº¤äº’å¼è®ºè¯è¯­æ–™åº“ï¼Œä¸ºåŒºåŸŸé€‰æ‹©å’Œæ–‡æœ¬è®ºè¯æä¾›æ­¥éª¤å±‚é¢çš„ç›‘ç£ã€‚åœ¨æ•°å­¦æ™¯è§‚ã€ç§‘å­¦é—®ç­”ç­‰åŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒVLM-RÂ³åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬è®¾ç½®ä¸­æ ‘ç«‹äº†æ–°çš„æŠ€æœ¯æ ‡æ†ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦ç²¾ç»†ç©ºé—´æ¨ç†æˆ–ç²¾ç»†è§†è§‰çº¿ç´¢æå–çš„é—®é¢˜ä¸Šï¼Œå…¶å¢ç›Šæœ€ä¸ºæ˜¾è‘—ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>VLM-RÂ³æ¡†æ¶ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹é…å¤‡äº†åŠ¨æ€è¿­ä»£èšç„¦å’Œå›è®¿è§†è§‰åŒºåŸŸçš„èƒ½åŠ›ï¼Œä»¥å®ç°æ–‡æœ¬æ¨ç†åœ¨è§†è§‰è¯æ®ä¸­çš„ç²¾ç¡®å®šä½ã€‚</li>
<li>æ ¸å¿ƒæ–¹æ³•Region-Conditioned Reinforcement Policy Optimization (R-GRPO)é¼“åŠ±æ¨¡å‹é€‰æ‹©ä¿¡æ¯åŒºåŸŸã€è¿›è¡Œè½¬æ¢å¹¶æ•´åˆè§†è§‰ä¸Šä¸‹æ–‡ã€‚</li>
<li>Visuo-Lingual Interleaved Rationale (VLIR)è¯­æ–™åº“çš„åˆ›å»ºä¸ºåŒºåŸŸé€‰æ‹©å’Œæ–‡æœ¬è®ºè¯æä¾›äº†æ­¥éª¤å±‚é¢çš„ç›‘ç£ã€‚</li>
<li>VLM-RÂ³åœ¨æ•°å­¦æ™¯è§‚ã€ç§‘å­¦é—®ç­”ç­‰åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å“è¶Šï¼Œå°¤å…¶åœ¨éœ€è¦ç²¾ç»†ç©ºé—´æ¨ç†æˆ–è§†è§‰çº¿ç´¢æå–çš„é—®é¢˜ä¸Šå¢ç›Šæ˜¾è‘—ã€‚</li>
<li>è¯¥æ¡†æ¶çš„å¼•å…¥è§£å†³äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¤æ‚è§†è§‰ä»»åŠ¡æ—¶çš„å±€é™æ€§ã€‚</li>
<li>VLM-RÂ³èƒ½å¤Ÿæ— ç¼åœ°å°†å­å›¾åƒå†…å®¹èå…¥æ€ç»´é“¾ä¸­ï¼Œå¢å¼ºäº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>R-GRPOè®­ç»ƒèŒƒå¼é€šè¿‡å¥–åŠ±æœºåˆ¶ä¼˜åŒ–æ¨¡å‹åœ¨è§†è§‰ä»»åŠ¡ä¸­çš„å†³ç­–è¿‡ç¨‹ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16192">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2e58c89e9347a05f0805fa5c6ec2b876.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-422aef326d89b2525c634234f6fae7df.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dcae11ec4f5131ec71033ae813583463.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Meta-Learning-an-In-Context-Transformer-Model-of-Human-Higher-Visual-Cortex"><a href="#Meta-Learning-an-In-Context-Transformer-Model-of-Human-Higher-Visual-Cortex" class="headerlink" title="Meta-Learning an In-Context Transformer Model of Human Higher Visual   Cortex"></a>Meta-Learning an In-Context Transformer Model of Human Higher Visual   Cortex</h2><p><strong>Authors:Muquan Yu, Mu Nan, Hossein Adeli, Jacob S. Prince, John A. Pyles, Leila Wehbe, Margaret M. Henderson, Michael J. Tarr, Andrew F. Luo</strong></p>
<p>Understanding functional representations within higher visual cortex is a fundamental question in computational neuroscience. While artificial neural networks pretrained on large-scale datasets exhibit striking representational alignment with human neural responses, learning image-computable models of visual cortex relies on individual-level, large-scale fMRI datasets. The necessity for expensive, time-intensive, and often impractical data acquisition limits the generalizability of encoders to new subjects and stimuli. BraInCoRL uses in-context learning to predict voxelwise neural responses from few-shot examples without any additional finetuning for novel subjects and stimuli. We leverage a transformer architecture that can flexibly condition on a variable number of in-context image stimuli, learning an inductive bias over multiple subjects. During training, we explicitly optimize the model for in-context learning. By jointly conditioning on image features and voxel activations, our model learns to directly generate better performing voxelwise models of higher visual cortex. We demonstrate that BraInCoRL consistently outperforms existing voxelwise encoder designs in a low-data regime when evaluated on entirely novel images, while also exhibiting strong test-time scaling behavior. The model also generalizes to an entirely new visual fMRI dataset, which uses different subjects and fMRI data acquisition parameters. Further, BraInCoRL facilitates better interpretability of neural signals in higher visual cortex by attending to semantically relevant stimuli. Finally, we show that our framework enables interpretable mappings from natural language queries to voxel selectivity. </p>
<blockquote>
<p>ç†è§£é«˜çº§è§†è§‰çš®å±‚ä¸­çš„åŠŸèƒ½è¡¨å¾æ˜¯è®¡ç®—ç¥ç»ç§‘å­¦ä¸­çš„ä¸€ä¸ªåŸºæœ¬é—®é¢˜ã€‚è™½ç„¶åŸºäºå¤§è§„æ¨¡æ•°æ®é›†è¿›è¡Œé¢„è®­ç»ƒçš„äººå·¥ç¥ç»ç½‘ç»œä¸äººçš„ç¥ç»ååº”è¡¨ç°å‡ºäº†æƒŠäººçš„è¡¨å¾ä¸€è‡´æ€§ï¼Œä½†å­¦ä¹ è§†è§‰çš®å±‚çš„å›¾åƒè®¡ç®—æ¨¡å‹ä¾èµ–äºä¸ªä½“å±‚é¢çš„å¤§è§„æ¨¡åŠŸèƒ½æ€§ç£å…±æŒ¯æˆåƒï¼ˆfMRIï¼‰æ•°æ®é›†ã€‚æ˜‚è´µã€è€—æ—¶ä¸”é€šå¸¸ä¸åˆ‡å®é™…çš„æ•°æ®é‡‡é›†éœ€æ±‚é™åˆ¶äº†ç¼–ç å™¨å¯¹æ–°æ‰‹å’Œåˆºæ¿€çš„æ¨å¹¿èƒ½åŠ›ã€‚BraInCoRLä½¿ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ æ¥é¢„æµ‹å°‘æ•°æ ·æœ¬çš„ä½“ç´ ç¥ç»ååº”ï¼Œæ— éœ€å¯¹æ–°ä¸»ä½“å’Œåˆºæ¿€è¿›è¡Œä»»ä½•é¢å¤–çš„å¾®è°ƒã€‚æˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§å˜å‹å™¨æ¶æ„ï¼Œå¯ä»¥çµæ´»åœ°é€‚åº”ä¸åŒæ•°é‡çš„ä¸Šä¸‹æ–‡å›¾åƒåˆºæ¿€ï¼Œå¹¶åœ¨å¤šä¸ªä¸»ä½“ä¸Šå½¢æˆå½’çº³åè§ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬æ˜ç¡®åœ°å¯¹æ¨¡å‹è¿›è¡Œäº†ä¸Šä¸‹æ–‡å­¦ä¹ ä¼˜åŒ–ã€‚é€šè¿‡è”åˆå›¾åƒç‰¹å¾å’Œä½“ç´ æ¿€æ´»ç‰©ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å­¦ä¼šäº†ç›´æ¥ç”Ÿæˆé«˜çº§è§†è§‰çš®å±‚çš„æ€§èƒ½æ›´å¥½çš„ä½“ç´ æ¨¡å‹ã€‚æˆ‘ä»¬åœ¨å…¨æ–°å›¾åƒä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒBraInCoRLåœ¨æ•°æ®ç¼ºä¹çš„æƒ…å†µä¸‹å§‹ç»ˆä¼˜äºç°æœ‰çš„ä½“ç´ ç¼–ç å™¨è®¾è®¡ï¼ŒåŒæ—¶åœ¨æµ‹è¯•æ—¶è¡¨ç°å‡ºå¼ºå¤§çš„æ‰©å±•è¡Œä¸ºã€‚è¯¥æ¨¡å‹è¿˜å¯ä»¥æ¨å¹¿åˆ°ä¸€ä¸ªå…¨æ–°çš„è§†è§‰fMRIæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†ä½¿ç”¨ä¸åŒçš„ä¸»ä½“å’ŒfMRIæ•°æ®é‡‡é›†å‚æ•°ã€‚æ­¤å¤–ï¼ŒBraInCoRLé€šè¿‡å…³æ³¨è¯­ä¹‰ç›¸å…³çš„åˆºæ¿€ï¼Œæé«˜äº†é«˜çº§è§†è§‰çš®å±‚ç¥ç»ä¿¡å·çš„è§£è¯»æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬å±•ç¤ºäº†æˆ‘ä»¬çš„æ¡†æ¶èƒ½å¤Ÿå®ç°ä»è‡ªç„¶è¯­è¨€æŸ¥è¯¢åˆ°ä½“ç´ é€‰æ‹©æ€§çš„å¯è§£é‡Šæ˜ å°„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15813v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†è®¡ç®—ç¥ç»ç§‘å­¦ä¸­çš„åŸºæœ¬é—®é¢˜ï¼Œå³ç†è§£é«˜çº§è§†è§‰çš®å±‚ä¸­çš„åŠŸèƒ½è¡¨å¾ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œé¢„è®­ç»ƒäºå¤§è§„æ¨¡æ•°æ®é›†çš„äººå·¥ç¥ç»ç½‘ç»œä¸äººè„‘ç¥ç»å“åº”å…·æœ‰æ˜¾è‘—çš„ä¸€è‡´æ€§ã€‚ç„¶è€Œï¼Œå»ºç«‹å¯è§†çš®å±‚çš„å›¾åƒè®¡ç®—æ¨¡å‹ä¾èµ–äºä¸ªä½“å±‚é¢çš„å¤§è§„æ¨¡fMRIæ•°æ®é›†ï¼Œè¿™é™åˆ¶äº†ç¼–ç å™¨å¯¹æ–°ä¸»ä½“å’Œåˆºæ¿€çš„æ¨å¹¿èƒ½åŠ›ã€‚BraInCoRLä½¿ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ æŠ€æœ¯ï¼Œä»…é€šè¿‡å°‘é‡æ ·æœ¬å°±èƒ½é¢„æµ‹ç¥ç»å“åº”ï¼Œæ— éœ€å¯¹æ–°ä¸»ä½“å’Œåˆºæ¿€è¿›è¡Œå¾®è°ƒã€‚åˆ©ç”¨å˜å‹å™¨æ¶æ„ï¼Œè¯¥æ¨¡å‹å¯çµæ´»é€‚åº”ä¸åŒæ•°é‡çš„ä¸Šä¸‹æ–‡å›¾åƒåˆºæ¿€ï¼Œå¹¶åœ¨å¤šä¸ªä¸»ä½“ä¹‹é—´å»ºç«‹å½’çº³åè§ã€‚é€šè¿‡è”åˆå›¾åƒç‰¹å¾å’Œä½“ç´ æ¿€æ´»ï¼Œæ¨¡å‹å¯ç›´æ¥ç”Ÿæˆé«˜çº§è§†è§‰çš®å±‚çš„ä½“ç´ æ¨¡å‹ã€‚å®éªŒè¡¨æ˜ï¼ŒBraInCoRLåœ¨ä½æ•°æ®ç¯å¢ƒä¸­æŒç»­ä¼˜äºç°æœ‰ä½“ç´ ç¼–ç å™¨è®¾è®¡ï¼Œåœ¨å…¨æ–°å›¾åƒä¸Šçš„è¡¨ç°å°¤ä¸ºå‡ºè‰²ï¼Œå¹¶å…·æœ‰å¼ºå¤§çš„æµ‹è¯•è§„æ¨¡è¡Œä¸ºã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹å¯æ¨å¹¿åˆ°ä½¿ç”¨ä¸åŒä¸»ä½“å’ŒfMRIæ•°æ®é‡‡é›†å‚æ•°çš„æ–°è§†è§‰fMRIæ•°æ®é›†ï¼Œå¹¶æœ‰åŠ©äºæ›´å¥½åœ°è§£é‡Šé«˜çº§è§†è§‰çš®å±‚çš„ç¥ç»ä¿¡å·ã€‚æœ€åï¼Œç ”ç©¶å±•ç¤ºäº†è¯¥æ¡†æ¶å¯å®ç°è‡ªç„¶è¯­è¨€æŸ¥è¯¢åˆ°ä½“ç´ é€‰æ‹©æ€§çš„å¯è§£é‡Šæ˜ å°„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç†è§£é«˜çº§è§†è§‰çš®å±‚ä¸­çš„åŠŸèƒ½è¡¨å¾æ˜¯è®¡ç®—ç¥ç»ç§‘å­¦çš„åŸºæœ¬é—®é¢˜ã€‚</li>
<li>äººå·¥ç¥ç»ç½‘ç»œé¢„è®­ç»ƒäºå¤§è§„æ¨¡æ•°æ®é›†ä¸äººè„‘ç¥ç»å“åº”å…·æœ‰ä¸€è‡´æ€§ã€‚</li>
<li>å­¦ä¹ å›¾åƒè®¡ç®—æ¨¡å‹ä¾èµ–äºä¸ªä½“å±‚é¢çš„å¤§è§„æ¨¡fMRIæ•°æ®é›†ï¼Œè¿™é™åˆ¶äº†ç¼–ç å™¨çš„æ¨å¹¿èƒ½åŠ›ã€‚</li>
<li>BraInCoRLä½¿ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ æŠ€æœ¯ï¼Œé€šè¿‡å°‘é‡æ ·æœ¬é¢„æµ‹ç¥ç»å“åº”ï¼Œæ— éœ€å¯¹æ–°ä¸»ä½“å’Œåˆºæ¿€è¿›è¡Œå¾®è°ƒã€‚</li>
<li>BraInCoRLåˆ©ç”¨å˜å‹å™¨æ¶æ„ï¼Œå¯çµæ´»é€‚åº”ä¸åŒæ•°é‡çš„ä¸Šä¸‹æ–‡å›¾åƒåˆºæ¿€ï¼Œå¹¶åœ¨å¤šä¸ªä¸»ä½“é—´å»ºç«‹å½’çº³åè§ã€‚</li>
<li>BraInCoRLåœ¨ä½æ•°æ®ç¯å¢ƒä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¹¶èƒ½åœ¨å…¨æ–°å›¾åƒä¸Šå±•ç¤ºå¼ºå¤§çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15813">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-ec85e517c1b2214b51a18d1f5c4aebbd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d43a9a3308789e8c9ed8606c7dfdf214.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a4181be70cd10df12b22966a968ac31.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bf3cc5b298464d8e9109448a1b009881.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca0bd825b858430e6c086f4dda6628e6.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Prompt-Tuning-Vision-Language-Models-with-Margin-Regularizer-for-Few-Shot-Learning-under-Distribution-Shifts"><a href="#Prompt-Tuning-Vision-Language-Models-with-Margin-Regularizer-for-Few-Shot-Learning-under-Distribution-Shifts" class="headerlink" title="Prompt Tuning Vision Language Models with Margin Regularizer for   Few-Shot Learning under Distribution Shifts"></a>Prompt Tuning Vision Language Models with Margin Regularizer for   Few-Shot Learning under Distribution Shifts</h2><p><strong>Authors:Debarshi Brahma, Anuska Roy, Soma Biswas</strong></p>
<p>Recently, Vision-Language foundation models like CLIP and ALIGN, which are pre-trained on large-scale data have shown remarkable zero-shot generalization to diverse datasets with different classes and even domains. In this work, we take a step further and analyze whether these models can be adapted to target datasets having very different distributions and classes compared to what these models have been trained on, using only a few labeled examples from the target dataset. In such scenarios, finetuning large pretrained models is challenging due to problems of overfitting as well as loss of generalization, and has not been well explored in prior literature. Since, the pre-training data of such models are unavailable, it is difficult to comprehend the performance on various downstream datasets. First, we try to answer the question: Given a target dataset with a few labelled examples, can we estimate whether further fine-tuning can enhance the performance compared to zero-shot evaluation? by analyzing the common vision-language embedding space. Based on the analysis, we propose a novel prompt-tuning method, PromptMargin for adapting such large-scale VLMs directly on the few target samples. PromptMargin effectively tunes the text as well as visual prompts for this task, and has two main modules: 1) Firstly, we use a selective augmentation strategy to complement the few training samples in each task; 2) Additionally, to ensure robust training in the presence of unfamiliar class names, we increase the inter-class margin for improved class discrimination using a novel Multimodal Margin Regularizer. Extensive experiments and analysis across fifteen target benchmark datasets, with varying degrees of distribution shifts from natural images, shows the effectiveness of the proposed framework over the existing state-of-the-art approaches applied to this setting. github.com&#x2F;debarshigit&#x2F;PromptMargin. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œè¯¸å¦‚CLIPå’ŒALIGNä¹‹ç±»çš„è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹åœ¨å¤§è§„æ¨¡æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒåï¼Œåœ¨å¯¹å…·æœ‰ä¸åŒç±»åˆ«ç”šè‡³åŸŸçš„å„ç§æ•°æ®é›†ä¸Šè¿›è¡Œé›¶æ ·æœ¬æ³›åŒ–æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ›´è¿›ä¸€æ­¥åœ°åˆ†æï¼Œä½¿ç”¨ç›®æ ‡æ•°æ®é›†ä¸­ä»…æœ‰çš„å°‘æ•°æ ‡è®°æ ·æœ¬ï¼Œè¿™äº›æ¨¡å‹æ˜¯å¦å¯ä»¥è¢«é€‚åº”äºä¸ç›®æ ‡æ•°æ®é›†å…·æœ‰ä¸åŒåˆ†å¸ƒå’Œç±»åˆ«çš„æ•°æ®é›†ã€‚åœ¨è¿™æ ·çš„åœºæ™¯ä¸­ï¼Œå¾®è°ƒå¤§è§„æ¨¡çš„é¢„è®­ç»ƒæ¨¡å‹å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå­˜åœ¨è¿‡æ‹Ÿåˆå’Œæ³›åŒ–æŸå¤±çš„é—®é¢˜ï¼Œå¹¶ä¸”åœ¨å…ˆå‰çš„æ–‡çŒ®ä¸­å°šæœªå¾—åˆ°å¾ˆå¥½çš„æ¢ç´¢ã€‚ç”±äºæ­¤ç±»æ¨¡å‹çš„é¢„è®­ç»ƒæ•°æ®ä¸å¯ç”¨ï¼Œå› æ­¤éš¾ä»¥åœ¨å„ç§ä¸‹æ¸¸æ•°æ®é›†ä¸Šç†è§£å…¶æ€§èƒ½ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è¯•å›¾å›ç­”ä»¥ä¸‹é—®é¢˜ï¼šç»™å®šä¸€ä¸ªå…·æœ‰å°‘é‡æ ‡è®°æ ·æœ¬çš„ç›®æ ‡æ•°æ®é›†ï¼Œæˆ‘ä»¬èƒ½å¦é€šè¿‡åˆ†æå¸¸è§çš„è§†è§‰è¯­è¨€åµŒå…¥ç©ºé—´æ¥ä¼°è®¡ä¸é›¶æ ·æœ¬è¯„ä¼°ç›¸æ¯”ï¼Œè¿›ä¸€æ­¥çš„å¾®è°ƒæ˜¯å¦èƒ½æé«˜æ€§èƒ½ï¼ŸåŸºäºæ­¤åˆ†æï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„æç¤ºè°ƒæ•´æ–¹æ³•PromptMarginï¼Œç”¨äºç›´æ¥åœ¨å°‘é‡ç›®æ ‡æ ·æœ¬ä¸Šé€‚åº”å¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹ã€‚PromptMarginæœ‰æ•ˆåœ°é’ˆå¯¹æ­¤ä»»åŠ¡è°ƒæ•´äº†æ–‡æœ¬å’Œè§†è§‰æç¤ºï¼Œå¹¶æœ‰ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼š1ï¼‰é¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨é€‰æ‹©æ€§å¢å¼ºç­–ç•¥æ¥è¡¥å……æ¯ä¸ªä»»åŠ¡ä¸­çš„å°‘é‡è®­ç»ƒæ ·æœ¬ï¼›2ï¼‰æ­¤å¤–ï¼Œä¸ºäº†ç¡®ä¿åœ¨ä¸ç†Ÿæ‚‰çš„ç±»åå­˜åœ¨çš„æƒ…å†µä¸‹è¿›è¡Œç¨³å¥è®­ç»ƒï¼Œæˆ‘ä»¬é€šè¿‡ä½¿ç”¨æ–°å‹çš„å¤šæ¨¡æ€è¾¹è·è°ƒèŠ‚å™¨æ¥å¢åŠ ç±»é—´é—´è·ï¼Œä»¥å®ç°æ”¹è¿›åçš„ç±»é—´é‰´åˆ«ã€‚åœ¨æ¥è‡ªè‡ªç„¶å›¾åƒçš„åäº”ä¸ªç›®æ ‡åŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒå’Œåˆ†æï¼Œæ˜¾ç¤ºå‡ºæ‰€æå‡ºçš„æ¡†æ¶åœ¨ç°æœ‰æœ€å…ˆè¿›çš„é€‚ç”¨äºæ­¤åœºæ™¯çš„æ–¹æ³•ä¸­çš„æœ‰æ•ˆæ€§ã€‚github.com&#x2F;debarshigit&#x2F;PromptMarginã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15506v1">PDF</a> Published in TMLR (2025)</p>
<p><strong>Summary</strong><br>     è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹å¦‚CLIPå’ŒALIGNåœ¨å¤§å‹æ•°æ®ä¸Šçš„é¢„è®­ç»ƒè¡¨ç°å‡ºå¯¹å¤šç§æ•°æ®é›†çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚æœ¬ç ”ç©¶è¿›ä¸€æ­¥æ¢è®¨è¿™äº›æ¨¡å‹åœ¨ç›®æ ‡æ•°æ®é›†ä¸Šçš„é€‚åº”æ€§ï¼Œç›®æ ‡æ•°æ®é›†ä¸æ¨¡å‹è®­ç»ƒæ—¶çš„æ•°æ®åˆ†å¸ƒå’Œç±»åˆ«å·®å¼‚æ˜¾è‘—ï¼Œä»…ä½¿ç”¨ç›®æ ‡æ•°æ®é›†çš„å°‘é‡æ ‡è®°æ ·æœ¬ã€‚æå‡ºä¸€ç§æ–°é¢–çš„æç¤ºè°ƒæ•´æ–¹æ³•PromptMarginï¼Œç›´æ¥åœ¨å°‘é‡ç›®æ ‡æ ·æœ¬ä¸Šé€‚åº”å¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹ã€‚è¯¥æ–¹æ³•é€šè¿‡é€‰æ‹©æ€§å¢å¼ºç­–ç•¥è¡¥å……æ¯ä¸ªä»»åŠ¡çš„å°‘é‡è®­ç»ƒæ ·æœ¬ï¼Œå¹¶é€šè¿‡å¤šæ¨¡æ€è¾¹è·æ­£åˆ™åŒ–å™¨æé«˜ç±»é—´é—´è·ï¼Œä»è€Œæé«˜ç±»ä¹‹é—´çš„é‰´åˆ«èƒ½åŠ›ã€‚åœ¨åäº”ä¸ªç›®æ ‡åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒå’Œåˆ†æè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é¢ä¸´ä¸åŸå§‹æ•°æ®é›†ä¸åŒçš„åˆ†å¸ƒè½¬ç§»æ—¶ï¼Œæ¯”ç°æœ‰æ–¹æ³•æ›´æœ‰æ•ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹å¦‚CLIPå’ŒALIGNå…·æœ‰é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½åº”å¯¹ä¸åŒç±»åˆ«å’Œé¢†åŸŸçš„æ•°æ®é›†ã€‚</li>
<li>åœ¨ç›®æ ‡æ•°æ®é›†ä¸é¢„è®­ç»ƒæ•°æ®åˆ†å¸ƒå’Œç±»åˆ«å·®å¼‚æ˜¾è‘—çš„æƒ…å†µä¸‹ï¼Œè¿™äº›æ¨¡å‹çš„é€‚åº”æ€§å—åˆ°æŒ‘æˆ˜ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°é¢–çš„æç¤ºè°ƒæ•´æ–¹æ³•PromptMarginï¼Œç”¨äºåœ¨å°‘é‡ç›®æ ‡æ ·æœ¬ä¸Šç›´æ¥é€‚åº”å¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹ã€‚</li>
<li>PromptMarginé€šè¿‡é€‰æ‹©æ€§å¢å¼ºç­–ç•¥è¡¥å……å°‘é‡è®­ç»ƒæ ·æœ¬ï¼Œæé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>å¤šæ¨¡æ€è¾¹è·æ­£åˆ™åŒ–å™¨ç”¨äºæé«˜ç±»é—´é‰´åˆ«èƒ½åŠ›ï¼Œç¡®ä¿åœ¨å­˜åœ¨ä¸ç†Ÿæ‚‰çš„ç±»åæ—¶ä¹Ÿèƒ½è¿›è¡Œç¨³å¥è®­ç»ƒã€‚</li>
<li>åœ¨å¤šä¸ªç›®æ ‡æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒPromptMarginæ¯”ç°æœ‰æ–¹æ³•æ›´æœ‰æ•ˆã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºå¤„ç†åˆ†å¸ƒè½¬ç§»é—®é¢˜æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15506">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8fbf8d44fd876b334d5e33e7d2965243.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-15517e1511a784473b40ad861127b9b4.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="On-the-Robustness-of-Medical-Vision-Language-Models-Are-they-Truly-Generalizable"><a href="#On-the-Robustness-of-Medical-Vision-Language-Models-Are-they-Truly-Generalizable" class="headerlink" title="On the Robustness of Medical Vision-Language Models: Are they Truly   Generalizable?"></a>On the Robustness of Medical Vision-Language Models: Are they Truly   Generalizable?</h2><p><strong>Authors:Raza Imam, Rufael Marew, Mohammad Yaqub</strong></p>
<p>Medical Vision-Language Models (MVLMs) have achieved par excellence generalization in medical image analysis, yet their performance under noisy, corrupted conditions remains largely untested. Clinical imaging is inherently susceptible to acquisition artifacts and noise; however, existing evaluations predominantly assess generally clean datasets, overlooking robustness â€“ i.e., the modelâ€™s ability to perform under real-world distortions. To address this gap, we first introduce MediMeta-C, a corruption benchmark that systematically applies several perturbations across multiple medical imaging datasets. Combined with MedMNIST-C, this establishes a comprehensive robustness evaluation framework for MVLMs. We further propose RobustMedCLIP, a visual encoder adaptation of a pretrained MVLM that incorporates few-shot tuning to enhance resilience against corruptions. Through extensive experiments, we benchmark 5 major MVLMs across 5 medical imaging modalities, revealing that existing models exhibit severe degradation under corruption and struggle with domain-modality tradeoffs. Our findings highlight the necessity of diverse training and robust adaptation strategies, demonstrating that efficient low-rank adaptation when paired with few-shot tuning, improves robustness while preserving generalization across modalities. </p>
<blockquote>
<p>åŒ»ç–—è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆMVLMï¼‰åœ¨åŒ»å­¦å›¾åƒåˆ†ææ–¹é¢å·²ç»è¾¾åˆ°äº†å“è¶Šçš„æ³›åŒ–æ€§èƒ½ï¼Œä½†åœ¨å™ªå£°å’ŒæŸåæ¡ä»¶ä¸‹çš„æ€§èƒ½ä»ç„¶æ²¡æœ‰å¾—åˆ°å……åˆ†çš„æµ‹è¯•ã€‚ä¸´åºŠæˆåƒæœ¬è´¨ä¸Šå®¹æ˜“å—åˆ°é‡‡é›†ä¼ªå½±å’Œå™ªå£°çš„å½±å“ï¼›ç„¶è€Œï¼Œç°æœ‰çš„è¯„ä¼°ä¸»è¦ä¾§é‡äºå¯¹é€šå¸¸å¹²å‡€æ•°æ®é›†çš„è¯„ä»·ï¼Œå¿½ç•¥äº†æ¨¡å‹çš„ç¨³å¥æ€§ï¼Œå³æ¨¡å‹åœ¨ç°å®ä¸–ç•Œå¤±çœŸæƒ…å†µä¸‹çš„æ€§èƒ½ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬é¦–å…ˆå¼•å…¥äº†MediMeta-Cï¼Œè¿™æ˜¯ä¸€ä¸ªè…è´¥åŸºå‡†æµ‹è¯•ï¼Œå®ƒç³»ç»Ÿåœ°åº”ç”¨äºå¤šä¸ªåŒ»å­¦æˆåƒæ•°æ®é›†çš„å¤šé‡æ‰°åŠ¨ã€‚ç»“åˆMedMNIST-Cï¼Œè¿™ä¸ºMVLMå»ºç«‹äº†ä¸€ä¸ªå…¨é¢çš„ç¨³å¥æ€§è¯„ä¼°æ¡†æ¶ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†RobustMedCLIPï¼Œè¿™æ˜¯ä¸€ä¸ªè§†è§‰ç¼–ç å™¨é€‚åº”çš„é¢„è®­ç»ƒMVLMï¼Œå®ƒç»“åˆäº†å°‘é‡æ ·æœ¬å¾®è°ƒæŠ€æœ¯ï¼Œä»¥æé«˜å¯¹æŸåçš„æŠµæŠ—åŠ›ã€‚é€šè¿‡å¤§é‡å®éªŒï¼Œæˆ‘ä»¬å¯¹5ç§ä¸»è¦çš„MVLMåœ¨5ç§åŒ»å­¦æˆåƒæ¨¡æ€ä¸Šè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œç»“æœæ˜¾ç¤ºç°æœ‰æ¨¡å‹åœ¨æŸåæƒ…å†µä¸‹å­˜åœ¨ä¸¥é‡é€€åŒ–ï¼Œå¹¶ä¸”åœ¨é¢†åŸŸæ¨¡æ€æƒè¡¡æ–¹é¢é¢ä¸´å›°éš¾ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¤šæ ·åŒ–çš„è®­ç»ƒç­–ç•¥å’Œç¨³å¥çš„é€‚åº”ç­–ç•¥æ˜¯å¿…è¦çš„ï¼ŒåŒæ—¶è¯æ˜æœ‰æ•ˆçš„ä½ç§©é€‚åº”æŠ€æœ¯ä¸å°‘é‡æ ·æœ¬å¾®è°ƒç›¸ç»“åˆï¼Œå¯ä»¥åœ¨æé«˜ç¨³å¥æ€§çš„åŒæ—¶ï¼Œä¿æŒè·¨æ¨¡æ€çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15425v1">PDF</a> Dataset and Code is available at   <a target="_blank" rel="noopener" href="https://github.com/BioMedIA-MBZUAI/RobustMedCLIP">https://github.com/BioMedIA-MBZUAI/RobustMedCLIP</a> Accepted at: Medical Image   Understanding and Analysis (MIUA) 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å…³æ³¨åŒ»ç–—è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆMVLMsï¼‰åœ¨å™ªå£°ã€å¤±çœŸç¯å¢ƒä¸‹çš„æ€§èƒ½ã€‚ä¸ºè¯„ä¼°æ¨¡å‹åœ¨å®é™…æ‰­æ›²æƒ…å†µä¸‹çš„ç¨³å¥æ€§ï¼Œå¼•å…¥MediMeta-Cè…èš€åŸºå‡†ï¼Œç»“åˆMedMNIST-Cï¼Œå»ºç«‹å…¨é¢çš„MVLMsç¨³å¥æ€§è¯„ä¼°æ¡†æ¶ã€‚åŒæ—¶æå‡ºRobustMedCLIPï¼Œé€šè¿‡å¾®è°ƒé¢„è®­ç»ƒçš„MVLMçš„è§†è§‰ç¼–ç å™¨ï¼Œå¢å¼ºå…¶å¯¹æŠ—è…èš€çš„éŸ§æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œç°æœ‰æ¨¡å‹åœ¨è…èš€ç¯å¢ƒä¸‹æ€§èƒ½ä¸¥é‡ä¸‹é™ï¼Œä¸”å­˜åœ¨åŸŸæ¨¡æ€æƒè¡¡é—®é¢˜ã€‚ç ”ç©¶å¼ºè°ƒå¤šæ ·è®­ç»ƒå’Œç¨³å¥é€‚åº”ç­–ç•¥çš„å¿…è¦æ€§ï¼Œæœ‰æ•ˆä½ç§©é€‚åº”ç»“åˆå°‘é‡å¾®è°ƒï¼Œåœ¨æé«˜ç¨³å¥æ€§çš„åŒæ—¶ï¼Œä¿æŒè·¨æ¨¡æ€çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»ç–—è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆMVLMsï¼‰åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­å…·æœ‰å‡ºè‰²çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½†åœ¨å™ªå£°ã€å¤±çœŸç¯å¢ƒä¸‹çš„æ€§èƒ½å°šæœªå¾—åˆ°å……åˆ†æµ‹è¯•ã€‚</li>
<li>å¼•å…¥MediMeta-Cè…èš€åŸºå‡†å’ŒMedMNIST-Cï¼Œå»ºç«‹MVLMsçš„ç¨³å¥æ€§è¯„ä¼°æ¡†æ¶ã€‚</li>
<li>æå‡ºRobustMedCLIPï¼Œé€šè¿‡å¾®è°ƒé¢„è®­ç»ƒMVLMçš„è§†è§‰ç¼–ç å™¨ï¼Œå¢å¼ºæ¨¡å‹å¯¹æŠ—è…èš€çš„éŸ§æ€§ã€‚</li>
<li>å®éªŒè¡¨æ˜ç°æœ‰æ¨¡å‹åœ¨è…èš€ç¯å¢ƒä¸‹æ€§èƒ½ä¸¥é‡ä¸‹é™ï¼Œéœ€è¦å¤šæ ·è®­ç»ƒå’Œç¨³å¥é€‚åº”ç­–ç•¥ã€‚</li>
<li>åŸŸæ¨¡æ€æƒè¡¡é—®é¢˜å­˜åœ¨ï¼Œéœ€è¦è§£å†³ä¸åŒåŒ»å­¦æˆåƒæ¨¡æ€ä¹‹é—´çš„å·®å¼‚ã€‚</li>
<li>æœ‰æ•ˆä½ç§©é€‚åº”ç»“åˆå°‘é‡å¾®è°ƒå¯ä»¥æé«˜æ¨¡å‹çš„ç¨³å¥æ€§å¹¶ä¿ç•™è·¨æ¨¡æ€æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15425">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-286f6d23ccbffa6bcbe0493f53c42c3e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df4a2c72be71d24bc620c49371c89ebf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-37de556265d7b3bb6796091b74386055.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a95e421eac30c1766409599c54665b94.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3b8b98615086f88163b093fb821de12a.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="GAMA-Disentangled-Geometric-Alignment-with-Adaptive-Contrastive-Perturbation-for-Reliable-Domain-Transfer"><a href="#GAMA-Disentangled-Geometric-Alignment-with-Adaptive-Contrastive-Perturbation-for-Reliable-Domain-Transfer" class="headerlink" title="GAMA++: Disentangled Geometric Alignment with Adaptive Contrastive   Perturbation for Reliable Domain Transfer"></a>GAMA++: Disentangled Geometric Alignment with Adaptive Contrastive   Perturbation for Reliable Domain Transfer</h2><p><strong>Authors:Kim Yun, Hana Satou, F Monkey</strong></p>
<p>Despite progress in geometry-aware domain adaptation, current methods such as GAMA still suffer from two unresolved issues: (1) insufficient disentanglement of task-relevant and task-irrelevant manifold dimensions, and (2) rigid perturbation schemes that ignore per-class alignment asymmetries. To address this, we propose GAMA++, a novel framework that introduces (i) latent space disentanglement to isolate label-consistent manifold directions from nuisance factors, and (ii) an adaptive contrastive perturbation strategy that tailors both on- and off-manifold exploration to class-specific manifold curvature and alignment discrepancy. We further propose a cross-domain contrastive consistency loss that encourages local semantic clusters to align while preserving intra-domain diversity. Our method achieves state-of-the-art results on DomainNet, Office-Home, and VisDA benchmarks under both standard and few-shot settings, with notable improvements in class-level alignment fidelity and boundary robustness. GAMA++ sets a new standard for semantic geometry alignment in transfer learning. </p>
<blockquote>
<p>å°½ç®¡å‡ ä½•æ„ŸçŸ¥é¢†åŸŸè‡ªé€‚åº”æ–¹é¢å·²æœ‰æ‰€è¿›å±•ï¼Œä½†å½“å‰çš„æ–¹æ³•ï¼Œå¦‚GAMAï¼Œä»ç„¶é¢ä¸´ä¸¤ä¸ªæœªè§£å†³çš„é—®é¢˜ï¼š(1)ä»»åŠ¡ç›¸å…³å’Œä»»åŠ¡ä¸ç›¸å…³çš„æµå½¢ç»´åº¦è§£è€¦ä¸è¶³ï¼›(2)å¿½ç•¥äº†æ¯ç±»å¯¹é½ä¸å¯¹ç§°æ€§çš„åˆšæ€§æ‰°åŠ¨æ–¹æ¡ˆã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†GAMA++ï¼Œä¸€ä¸ªå¼•å…¥(i)æ½œåœ¨ç©ºé—´è§£è€¦çš„æ–°æ¡†æ¶ï¼Œä»¥ä»å¹²æ‰°å› ç´ ä¸­éš”ç¦»å‡ºæ ‡ç­¾ä¸€è‡´çš„æµå½¢æ–¹å‘ï¼›(ii)è‡ªé€‚åº”å¯¹æ¯”æ‰°åŠ¨ç­–ç•¥ï¼Œæ ¹æ®ç±»ç‰¹å®šçš„æµå½¢æ›²ç‡å’Œå¯¹é½å·®å¼‚ï¼Œå®šåˆ¶æµå½¢å†…å’Œæµå½¢å¤–çš„æ¢ç´¢ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§è·¨åŸŸå¯¹æ¯”ä¸€è‡´æ€§æŸå¤±ï¼Œé¼“åŠ±å±€éƒ¨è¯­ä¹‰é›†ç¾¤å¯¹é½ï¼ŒåŒæ—¶ä¿ç•™åŸŸå†…å¤šæ ·æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨DomainNetã€Office-Homeå’ŒVisDAåŸºå‡†æµ‹è¯•ä¸‹ï¼Œæ— è®ºæ˜¯åœ¨æ ‡å‡†è¿˜æ˜¯å°æ ·æœ¬è®¾ç½®ä¸‹ï¼Œéƒ½å®ç°äº†æœ€æ–°ç»“æœï¼Œåœ¨ç±»çº§å¯¹é½ä¿çœŸæ€§å’Œè¾¹ç•Œç¨³å¥æ€§æ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚GAMA++ä¸ºè¿ç§»å­¦ä¹ ä¸­çš„è¯­ä¹‰å‡ ä½•å¯¹é½è®¾å®šäº†æ–°çš„æ ‡å‡†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15241v1">PDF</a> </p>
<p><strong>Summary</strong><br>å‡ ä½•æ„ŸçŸ¥é¢†åŸŸè‡ªé€‚åº”è™½ç„¶æœ‰æ‰€è¿›å±•ï¼Œä½†å½“å‰æ–¹æ³•å¦‚GAMAä»å­˜åœ¨ä¸¤å¤§é—®é¢˜ï¼šä¸€æ˜¯ä»»åŠ¡ç›¸å…³å’Œä»»åŠ¡ä¸ç›¸å…³çš„æµå½¢ç»´åº¦è§£è€¦ä¸è¶³ï¼ŒäºŒæ˜¯å¿½ç•¥äº†æ¯ç±»å¯¹é½ä¸å¯¹ç§°æ€§çš„åˆšæ€§æ‰°åŠ¨æ–¹æ¡ˆã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†GAMA++æ¡†æ¶ï¼Œå¼•å…¥æ½œåœ¨ç©ºé—´è§£è€¦æ¥éš”ç¦»æ ‡ç­¾ä¸€è‡´çš„æµå½¢æ–¹å‘å¹¶æ¶ˆé™¤å¹²æ‰°å› ç´ ï¼›åŒæ—¶é‡‡ç”¨è‡ªé€‚åº”å¯¹æ¯”æ‰°åŠ¨ç­–ç•¥ï¼Œæ ¹æ®ç±»ç‰¹å®šçš„æµå½¢æ›²ç‡å’Œå¯¹é½å·®å¼‚è¿›è¡Œæµå½¢å†…å’Œæµå½¢å¤–çš„æ¢ç´¢ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†è·¨åŸŸå¯¹æ¯”ä¸€è‡´æ€§æŸå¤±ï¼Œé¼“åŠ±å±€éƒ¨è¯­ä¹‰é›†ç¾¤å¯¹é½ï¼ŒåŒæ—¶ä¿æŒåŸŸå†…å¤šæ ·æ€§ã€‚GAMA++åœ¨DomainNetã€Office-Homeå’ŒVisDAç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­éƒ½è¾¾åˆ°äº†å‰æ‰€æœªæœ‰çš„ç»“æœï¼Œå°¤å…¶æ˜¯åœ¨ç±»çº§åˆ«å¯¹é½ä¿çœŸæ€§å’Œè¾¹ç•Œç¨³å¥æ€§æ–¹é¢æœ‰äº†æ˜¾è‘—æé«˜ã€‚GAMA++ä¸ºè¿ç§»å­¦ä¹ ä¸­çš„è¯­ä¹‰å‡ ä½•å¯¹é½è®¾å®šäº†æ–°çš„æ ‡å‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰å‡ ä½•æ„ŸçŸ¥é¢†åŸŸè‡ªé€‚åº”æ–¹æ³•å­˜åœ¨ä¸¤å¤§é—®é¢˜ï¼šä»»åŠ¡ç›¸å…³å’Œä»»åŠ¡ä¸ç›¸å…³çš„æµå½¢ç»´åº¦è§£è€¦ä¸è¶³ï¼Œä»¥åŠå¿½ç•¥ç±»å¯¹é½ä¸å¯¹ç§°æ€§çš„åˆšæ€§æ‰°åŠ¨æ–¹æ¡ˆã€‚</li>
<li>GAMA++æ¡†æ¶é€šè¿‡å¼•å…¥æ½œåœ¨ç©ºé—´è§£è€¦å’Œè‡ªé€‚åº”å¯¹æ¯”æ‰°åŠ¨ç­–ç•¥æ¥è§£å†³è¿™ä¸¤ä¸ªé—®é¢˜ã€‚</li>
<li>æ½œåœ¨ç©ºé—´è§£è€¦ç”¨äºéš”ç¦»æ ‡ç­¾ä¸€è‡´çš„æµå½¢æ–¹å‘å¹¶æ¶ˆé™¤å¹²æ‰°å› ç´ ã€‚</li>
<li>è‡ªé€‚åº”å¯¹æ¯”æ‰°åŠ¨ç­–ç•¥æ ¹æ®ç±»ç‰¹å®šçš„æµå½¢æ›²ç‡å’Œå¯¹é½å·®å¼‚è¿›è¡Œæµå½¢å†…å’Œæµå½¢å¤–çš„æ¢ç´¢ã€‚</li>
<li>è·¨åŸŸå¯¹æ¯”ä¸€è‡´æ€§æŸå¤±é¼“åŠ±å±€éƒ¨è¯­ä¹‰é›†ç¾¤å¯¹é½ï¼ŒåŒæ—¶ä¿æŒåŸŸå†…å¤šæ ·æ€§ã€‚</li>
<li>GAMA++åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°å‰æ‰€æœªæœ‰çš„ç»“æœï¼Œç‰¹åˆ«æ˜¯åœ¨ç±»çº§åˆ«å¯¹é½å’Œè¾¹ç•Œç¨³å¥æ€§æ–¹é¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15241">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0ccdb97bee82a3df352d3aa8799711cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-58037c5520401f45f7b4a761cc30916d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3a99b83337fab29b5bb56329cfc81237.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Unlocking-the-Power-of-SAM-2-for-Few-Shot-Segmentation"><a href="#Unlocking-the-Power-of-SAM-2-for-Few-Shot-Segmentation" class="headerlink" title="Unlocking the Power of SAM 2 for Few-Shot Segmentation"></a>Unlocking the Power of SAM 2 for Few-Shot Segmentation</h2><p><strong>Authors:Qianxiong Xu, Lanyun Zhu, Xuanyi Liu, Guosheng Lin, Cheng Long, Ziyue Li, Rui Zhao</strong></p>
<p>Few-Shot Segmentation (FSS) aims to learn class-agnostic segmentation on few classes to segment arbitrary classes, but at the risk of overfitting. To address this, some methods use the well-learned knowledge of foundation models (e.g., SAM) to simplify the learning process. Recently, SAM 2 has extended SAM by supporting video segmentation, whose class-agnostic matching ability is useful to FSS. A simple idea is to encode support foreground (FG) features as memory, with which query FG features are matched and fused. Unfortunately, the FG objects in different frames of SAM 2â€™s video data are always the same identity, while those in FSS are different identities, i.e., the matching step is incompatible. Therefore, we design Pseudo Prompt Generator to encode pseudo query memory, matching with query features in a compatible way. However, the memories can never be as accurate as the real ones, i.e., they are likely to contain incomplete query FG, and some unexpected query background (BG) features, leading to wrong segmentation. Hence, we further design Iterative Memory Refinement to fuse more query FG features into the memory, and devise a Support-Calibrated Memory Attention to suppress the unexpected query BG features in memory. Extensive experiments have been conducted on PASCAL-5$^i$ and COCO-20$^i$ to validate the effectiveness of our design, e.g., the 1-shot mIoU can be 4.2% better than the best baseline. </p>
<blockquote>
<p>å°‘æ•°é•œå¤´åˆ†å‰²ï¼ˆFSSï¼‰çš„ç›®æ ‡æ˜¯å­¦ä¼šå¯¹å°‘æ•°ç±»åˆ«è¿›è¡Œç±»åˆ«æ— å…³çš„åˆ†å‰²ï¼Œä»¥å®ç°å¯¹ä»»æ„ç±»åˆ«çš„åˆ†å‰²ï¼Œä½†å­˜åœ¨è¿‡æ‹Ÿåˆçš„é£é™©ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œä¸€äº›æ–¹æ³•ä½¿ç”¨åŸºç¡€æ¨¡å‹çš„è‰¯å¥½çŸ¥è¯†æ¥ç®€åŒ–å­¦ä¹ è¿‡ç¨‹ï¼ˆä¾‹å¦‚SAMï¼‰ã€‚æœ€è¿‘ï¼ŒSAM 2é€šè¿‡æ”¯æŒè§†é¢‘åˆ†å‰²æ‰©å±•äº†SAMçš„åº”ç”¨ï¼Œå…¶ç±»åˆ«æ— å…³çš„åŒ¹é…èƒ½åŠ›å¯¹FSSå¾ˆæœ‰ç”¨ã€‚ä¸€ä¸ªç®€å•çš„æƒ³æ³•æ˜¯å°†æ”¯æŒå‰æ™¯ï¼ˆFGï¼‰ç‰¹å¾ç¼–ç ä¸ºå†…å­˜ï¼Œé€šè¿‡æŸ¥è¯¢å‰æ™¯ç‰¹å¾ä¸å†…å­˜è¿›è¡ŒåŒ¹é…å’Œèåˆã€‚ç„¶è€Œï¼Œä¸å¹¸çš„æ˜¯ï¼ŒSAM 2çš„è§†é¢‘æ•°æ®ä¸åŒå¸§ä¸­çš„å‰æ™¯å¯¹è±¡æ€»æ˜¯ç›¸åŒçš„èº«ä»½ï¼Œè€ŒFSSä¸­çš„åˆ™æ˜¯ä¸åŒçš„èº«ä»½ï¼Œå³åŒ¹é…æ­¥éª¤æ˜¯ä¸å…¼å®¹çš„ã€‚å› æ­¤ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¼ªæç¤ºç”Ÿæˆå™¨æ¥ç¼–ç ä¼ªæŸ¥è¯¢å†…å­˜ï¼Œä»¥ä¸æŸ¥è¯¢ç‰¹å¾è¿›è¡Œå…¼å®¹çš„åŒ¹é…ã€‚ç„¶è€Œï¼Œè¿™äº›è®°å¿†æ°¸è¿œæ— æ³•åƒçœŸå®çš„é‚£æ ·å‡†ç¡®ï¼Œå³å®ƒä»¬å¯èƒ½åŒ…å«ä¸å®Œæ•´æŸ¥è¯¢å‰æ™¯å’Œä¸€äº›æ„å¤–çš„æŸ¥è¯¢èƒŒæ™¯ï¼ˆBGï¼‰ç‰¹å¾ï¼Œä»è€Œå¯¼è‡´é”™è¯¯çš„åˆ†å‰²ã€‚å› æ­¤ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥è®¾è®¡äº†è¿­ä»£å†…å­˜ä¼˜åŒ–æŠ€æœ¯ï¼Œå°†æ›´å¤šæŸ¥è¯¢å‰æ™¯ç‰¹å¾èåˆåˆ°å†…å­˜ä¸­ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§æ”¯æŒæ ¡å‡†å†…å­˜æ³¨æ„åŠ›æœºåˆ¶æ¥æŠ‘åˆ¶å†…å­˜ä¸­æ„å¤–çš„æŸ¥è¯¢èƒŒæ™¯ç‰¹å¾ã€‚åœ¨PASCAL-5iå’ŒCOCO-20iä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼ŒéªŒè¯äº†æˆ‘ä»¬çš„è®¾è®¡æœ‰æ•ˆæ€§ï¼Œä¾‹å¦‚ï¼Œä¸æœ€ä½³åŸºçº¿ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å•é•œå¤´ä¸‹çš„å¹³å‡äº¤å¹¶æ¯”ï¼ˆmIoUï¼‰å¯ä»¥æé«˜4.2%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14100v2">PDF</a> This paper is accepted by ICMLâ€™25</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡è®¨è®ºäº†Few-Shot Segmentationï¼ˆFSSï¼‰çš„ç›®æ ‡å’Œæ–¹æ³•ã€‚è™½ç„¶æ—¨åœ¨å­¦ä¹ å°‘ç±»åˆ«ç±»ä¸ç›¸å…³çš„åˆ†å‰²æ¥åˆ†å‰²ä»»æ„ç±»åˆ«ï¼Œä½†å­˜åœ¨è¿‡æ‹Ÿåˆçš„é£é™©ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä¸€äº›æ–¹æ³•ä½¿ç”¨åŸºç¡€æ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†ç®€åŒ–å­¦ä¹ è¿‡ç¨‹ã€‚æœ€è¿‘SAM 2æ‰©å±•äº†SAMï¼Œæ”¯æŒè§†é¢‘åˆ†å‰²ï¼Œä½†å…¶ç±»ä¸ç›¸å…³çš„åŒ¹é…èƒ½åŠ›ä¸FSSä¸å…¼å®¹ã€‚å› æ­¤ï¼Œè®¾è®¡ä¼ªæç¤ºç”Ÿæˆå™¨æ¥ç¼–ç ä¼ªæŸ¥è¯¢å†…å­˜ï¼Œä¸æŸ¥è¯¢ç‰¹å¾è¿›è¡Œå…¼å®¹åŒ¹é…ã€‚ç„¶è€Œï¼Œè®°å¿†æ— æ³•åƒçœŸå®æ•°æ®é‚£æ ·å‡†ç¡®ï¼Œå¯èƒ½åŒ…å«ä¸å®Œæ•´çš„æŸ¥è¯¢å‰æ™¯å’Œä¸€äº›æ„å¤–çš„æŸ¥è¯¢èƒŒæ™¯ç‰¹å¾ï¼Œå¯¼è‡´é”™è¯¯åˆ†å‰²ã€‚å› æ­¤ï¼Œè¿›ä¸€æ­¥è®¾è®¡äº†è¿­ä»£å†…å­˜ä¼˜åŒ–å’Œæ ¡å‡†è®°å¿†æ³¨æ„åŠ›æ¥å®Œå–„è®¾è®¡å’Œä¼˜åŒ–æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Few-Shot Segmentationï¼ˆFSSï¼‰çš„ç›®æ ‡æ˜¯å­¦ä¹ å¯¹ä»»æ„ç±»åˆ«çš„ç±»ä¸ç›¸å…³åˆ†å‰²ï¼Œä½†å­˜åœ¨è¿‡æ‹Ÿåˆé£é™©ã€‚</li>
<li>ä½¿ç”¨åŸºç¡€æ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†å¯ä»¥è§£å†³æ­¤é—®é¢˜ã€‚</li>
<li>SAM 2æ‰©å±•äº†SAMæ”¯æŒè§†é¢‘åˆ†å‰²ï¼Œä½†å…¶åœ¨è§†é¢‘æ•°æ®ä¸FSSä¸­çš„åŒ¹é…æ­¥éª¤ä¸å…¼å®¹ã€‚</li>
<li>è®¾è®¡äº†ä¼ªæç¤ºç”Ÿæˆå™¨ä»¥ç¼–ç ä¼ªæŸ¥è¯¢å†…å­˜æ¥è¿›è¡ŒåŒ¹é…ã€‚ä½†è®°å¿†å¯èƒ½åŒ…å«ä¸å®Œæ•´çš„æŸ¥è¯¢å‰æ™¯å’Œä¸€äº›æ„å¤–çš„æŸ¥è¯¢èƒŒæ™¯ç‰¹å¾ã€‚</li>
<li>ä¸ºè§£å†³è®°å¿†çš„ä¸å‡†ç¡®æ€§é—®é¢˜ï¼Œè®¾è®¡äº†è¿­ä»£å†…å­˜ä¼˜åŒ–å’Œæ ¡å‡†è®°å¿†æ³¨æ„åŠ›æœºåˆ¶ã€‚</li>
<li>åœ¨PASCAL-5$^i$å’ŒCOCO-20$^i$ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒéªŒè¯äº†è®¾è®¡çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14100">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-02fefc42c3853bd620de4d6c50ed6ad5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55b5349ebeac0a93d3e773c4a1498977.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-20cc4d224ada50805ed8aec4e78bd194.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ad2cd2d49b136b45c8885f9fb3e6224f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7676cfabb099634e3a7f26118c0fec09.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Data-Whisperer-Efficient-Data-Selection-for-Task-Specific-LLM-Fine-Tuning-via-Few-Shot-In-Context-Learning"><a href="#Data-Whisperer-Efficient-Data-Selection-for-Task-Specific-LLM-Fine-Tuning-via-Few-Shot-In-Context-Learning" class="headerlink" title="Data Whisperer: Efficient Data Selection for Task-Specific LLM   Fine-Tuning via Few-Shot In-Context Learning"></a>Data Whisperer: Efficient Data Selection for Task-Specific LLM   Fine-Tuning via Few-Shot In-Context Learning</h2><p><strong>Authors:Shaobo Wang, Xiangqi Jin, Ziming Wang, Jize Wang, Jiajun Zhang, Kaixin Li, Zichen Wen, Zhong Li, Conghui He, Xuming Hu, Linfeng Zhang</strong></p>
<p>Fine-tuning large language models (LLMs) on task-specific data is essential for their effective deployment. As dataset sizes grow, efficiently selecting optimal subsets for training becomes crucial to balancing performance and computational costs. Traditional data selection methods often require fine-tuning a scoring model on the target dataset, which is time-consuming and resource-intensive, or rely on heuristics that fail to fully leverage the modelâ€™s predictive capabilities. To address these challenges, we propose Data Whisperer, an efficient, training-free, attention-based method that leverages few-shot in-context learning with the model to be fine-tuned. Comprehensive evaluations were conducted on both raw and synthetic datasets across diverse tasks and models. Notably, Data Whisperer achieves superior performance compared to the full GSM8K dataset on the Llama-3-8B-Instruct model, using just 10% of the data, and outperforms existing methods with a 3.1-point improvement and a 7.4$\times$ speedup. </p>
<blockquote>
<p>å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œé’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„å¾®è°ƒæ˜¯å…¶æœ‰æ•ˆéƒ¨ç½²çš„å…³é”®ã€‚éšç€æ•°æ®é›†è§„æ¨¡çš„å¢é•¿ï¼Œæœ‰æ•ˆé€‰æ‹©æœ€ä½³å­é›†è¿›è¡Œè®­ç»ƒå¯¹äºå¹³è¡¡æ€§èƒ½å’Œè®¡ç®—æˆæœ¬è‡³å…³é‡è¦ã€‚ä¼ ç»Ÿæ•°æ®é€‰æ‹©æ–¹æ³•é€šå¸¸éœ€è¦é’ˆå¯¹ç›®æ ‡æ•°æ®é›†å¯¹è¯„åˆ†æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œè¿™ä¸ä»…è€—æ—¶è€Œä¸”èµ„æºå¯†é›†ï¼Œæˆ–è€…ä¾èµ–äºæœªèƒ½å……åˆ†åˆ©ç”¨æ¨¡å‹é¢„æµ‹èƒ½åŠ›çš„å¯å‘å¼æ–¹æ³•ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†Data Whispererï¼Œè¿™æ˜¯ä¸€ç§é«˜æ•ˆã€æ— éœ€è®­ç»ƒã€åŸºäºæ³¨æ„åŠ›çš„æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨å°‘é‡ä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œä¸å¾…å¾®è°ƒæ¨¡å‹ç›¸ç»“åˆã€‚æˆ‘ä»¬åœ¨åŸå§‹å’Œåˆæˆæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤šæ ·åŒ–çš„ä»»åŠ¡å’Œæ¨¡å‹çš„å…¨é¢è¯„ä¼°ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒData Whispereråœ¨åªä½¿ç”¨10%æ•°æ®çš„æƒ…å†µä¸‹ï¼Œåœ¨Llama-3-8B-Instructæ¨¡å‹ä¸Šå®ç°äº†å¯¹GSM8Kæ•°æ®é›†çš„ä¼˜è¶Šæ€§èƒ½ï¼Œå¹¶ä¸”ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œå®ç°äº†3.1ç‚¹çš„æ€§èƒ½æå‡å’Œ7.4å€çš„åŠ é€Ÿã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12212v2">PDF</a> Accepted by ACL 2025 main, 18 pages, 8 figures, 6 tables</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç‰¹å®šä»»åŠ¡æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒå¯¹äºå…¶æœ‰æ•ˆéƒ¨ç½²è‡³å…³é‡è¦ã€‚éšç€æ•°æ®é›†è§„æ¨¡çš„å¢é•¿ï¼Œå¦‚ä½•æœ‰æ•ˆåœ°é€‰æ‹©æœ€ä¼˜å­é›†è¿›è¡Œè®­ç»ƒå·²æˆä¸ºå¹³è¡¡æ€§èƒ½å’Œè®¡ç®—æˆæœ¬çš„å…³é”®ã€‚ä¸ºè§£å†³ä¼ ç»Ÿæ•°æ®é€‰æ‹©æ–¹æ³•è€—æ—¶ã€èµ„æºå¯†é›†æˆ–æ— æ³•å……åˆ†åˆ©ç”¨æ¨¡å‹é¢„æµ‹èƒ½åŠ›çš„é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Data Whispererï¼Œè¿™æ˜¯ä¸€ç§é«˜æ•ˆã€æ— éœ€è®­ç»ƒã€åŸºäºæ³¨æ„åŠ›çš„æ–¹æ³•ï¼Œåˆ©ç”¨å°‘æ ·æœ¬ä¸Šä¸‹æ–‡å­¦ä¹ ä¸å¾…å¾®è°ƒæ¨¡å‹ç›¸ç»“åˆã€‚åœ¨åŸå§‹å’Œåˆæˆæ•°æ®é›†ä¸Šçš„å¤šæ ·åŒ–ä»»åŠ¡å’Œæ¨¡å‹çš„ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼ŒData Whispereråœ¨ä»…ä½¿ç”¨10%æ•°æ®çš„æƒ…å†µä¸‹ï¼Œåœ¨Llama-3-8B-Instructæ¨¡å‹ä¸Šå®ç°äº†å¯¹GSM8Kæ•°æ®é›†çš„å“è¶Šæ€§èƒ½ï¼Œå¹¶ç›¸å¯¹äºç°æœ‰æ–¹æ³•å®ç°äº†3.1ä¸ªç‚¹çš„æ”¹è¿›å’Œ7.4å€çš„åŠ é€Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¾®è°ƒå¯¹äºå…¶åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„æœ‰æ•ˆéƒ¨ç½²éå¸¸é‡è¦ã€‚</li>
<li>æ•°æ®é›†è§„æ¨¡çš„å¢åŠ ä½¿å¾—é€‰æ‹©æœ€ä½³å­é›†è¿›è¡Œè®­ç»ƒå˜å¾—è‡³å…³é‡è¦ï¼Œä»¥å¹³è¡¡æ€§èƒ½å’Œè®¡ç®—æˆæœ¬ã€‚</li>
<li>ä¼ ç»Ÿæ•°æ®é€‰æ‹©æ–¹æ³•å­˜åœ¨è€—æ—¶ã€èµ„æºå¯†é›†æˆ–æ— æ³•å……åˆ†åˆ©ç”¨æ¨¡å‹é¢„æµ‹èƒ½åŠ›çš„é—®é¢˜ã€‚</li>
<li>Data Whispereræ˜¯ä¸€ç§é«˜æ•ˆã€æ— éœ€è®­ç»ƒã€åŸºäºæ³¨æ„åŠ›çš„æ–¹æ³•ï¼Œåˆ©ç”¨å°‘æ ·æœ¬ä¸Šä¸‹æ–‡å­¦ä¹ ã€‚</li>
<li>Data Whispereråœ¨å¤šæ ·åŒ–ä»»åŠ¡å’Œæ¨¡å‹çš„ç»¼åˆè¯„ä¼°ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>Data Whispereråœ¨Llama-3-8B-Instructæ¨¡å‹ä¸Šçš„æ€§èƒ½è¶…è¶Šäº†GSM8Kæ•°æ®é›†ï¼Œä»…ä½¿ç”¨10%çš„æ•°æ®ã€‚</li>
<li>Data Whispererç›¸å¯¹äºç°æœ‰æ–¹æ³•å®ç°äº†æ˜¾è‘—çš„æ”¹è¿›å’ŒåŠ é€Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12212">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ca5d31639a7760ec44a2f44b27dedd31.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b45dc95a439d1d199d01bbda0ae96514.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d50cfb0cae1e151984528fcd02d7c52.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2ec03c60ec22094a18bcbeca4944c4a5.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Do-different-prompting-methods-yield-a-common-task-representation-in-language-models"><a href="#Do-different-prompting-methods-yield-a-common-task-representation-in-language-models" class="headerlink" title="Do different prompting methods yield a common task representation in   language models?"></a>Do different prompting methods yield a common task representation in   language models?</h2><p><strong>Authors:Guy Davidson, Todd M. Gureckis, Brenden M. Lake, Adina Williams</strong></p>
<p>Demonstrations and instructions are two primary approaches for prompting language models to perform in-context learning (ICL) tasks. Do identical tasks elicited in different ways result in similar representations of the task? An improved understanding of task representation mechanisms would offer interpretability insights and may aid in steering models. We study this through \textit{function vectors} (FVs), recently proposed as a mechanism to extract few-shot ICL task representations. We generalize FVs to alternative task presentations, focusing on short textual instruction prompts, and successfully extract instruction function vectors that promote zero-shot task accuracy. We find evidence that demonstration- and instruction-based function vectors leverage different model components, and offer several controls to dissociate their contributions to task performance. Our results suggest that different task promptings forms do not induce a common task representation through FVs but elicit different, partly overlapping mechanisms. Our findings offer principled support to the practice of combining instructions and task demonstrations, imply challenges in universally monitoring task inference across presentation forms, and encourage further examinations of LLM task inference mechanisms. </p>
<blockquote>
<p>æ¼”ç¤ºå’ŒæŒ‡ä»¤æ˜¯æç¤ºè¯­è¨€æ¨¡å‹æ‰§è¡Œä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ä»»åŠ¡çš„ä¸»è¦ä¸¤ç§æ–¹æ³•ã€‚ä»¥ä¸åŒæ–¹å¼å‘ˆç°ç›¸åŒä»»åŠ¡æ˜¯å¦ä¼šå¯¼è‡´å¯¹ä»»åŠ¡çš„ç›¸ä¼¼è¡¨ç¤ºï¼Ÿå¯¹ä»»åŠ¡è¡¨ç¤ºæœºåˆ¶çš„ç†è§£å¯ä»¥æä¾›è§£é‡Šæ€§æ´å¯Ÿå¹¶å¯èƒ½æœ‰åŠ©äºå¼•å¯¼æ¨¡å‹ã€‚æˆ‘ä»¬é€šè¿‡æœ€è¿‘æå‡ºçš„ç”¨äºæå–å°‘æ ·æœ¬ICLä»»åŠ¡è¡¨ç¤ºçš„â€œåŠŸèƒ½å‘é‡â€ï¼ˆFVï¼‰æ¥ç ”ç©¶è¿™ä¸€ç‚¹ã€‚æˆ‘ä»¬å°†FVæ¨å¹¿åˆ°æ›¿ä»£ä»»åŠ¡å±•ç¤ºä¸­ï¼Œé‡ç‚¹å…³æ³¨ç®€çŸ­çš„æ–‡æœ¬æŒ‡ä»¤æç¤ºï¼Œå¹¶æˆåŠŸæå–å‡ºèƒ½æå‡é›¶æ ·æœ¬ä»»åŠ¡å‡†ç¡®åº¦çš„æŒ‡ä»¤åŠŸèƒ½å‘é‡ã€‚æˆ‘ä»¬å‘ç°åŸºäºæ¼”ç¤ºå’ŒæŒ‡ä»¤çš„åŠŸèƒ½å‘é‡åˆ©ç”¨ä¸åŒçš„æ¨¡å‹ç»„ä»¶ï¼Œå¹¶æä¾›äº†ä¸€äº›æ§åˆ¶æ–¹æ³•æ¥åˆ†ç¦»å®ƒä»¬å¯¹ä»»åŠ¡æ€§èƒ½çš„è´¡çŒ®ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œä¸åŒçš„ä»»åŠ¡æç¤ºå½¢å¼ä¸ä¼šé€šè¿‡FVäº§ç”Ÿå…±åŒçš„ä»»åŠ¡è¡¨ç¤ºï¼Œè€Œæ˜¯å¼•å‘éƒ¨åˆ†é‡å çš„ä¸åŒæœºåˆ¶ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºç»“åˆæŒ‡ä»¤å’Œä»»åŠ¡æ¼”ç¤ºçš„å®è·µæä¾›äº†åŸåˆ™æ”¯æŒï¼Œæš—ç¤ºäº†åœ¨å„ç§å‘ˆç°å½¢å¼ä¸‹æ™®éç›‘æµ‹ä»»åŠ¡æ¨ç†çš„æŒ‘æˆ˜æ€§ï¼Œå¹¶é¼“åŠ±è¿›ä¸€æ­¥è€ƒå¯ŸLLMçš„ä»»åŠ¡æ¨ç†æœºåˆ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12075v2">PDF</a> 9 pages, 4 figures; under review</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†é€šè¿‡æ¼”ç¤ºå’ŒæŒ‡ä»¤ä¸¤ç§ä¸åŒæ–¹å¼å‘ˆç°ä»»åŠ¡æ—¶ï¼Œè¯­è¨€æ¨¡å‹è¿›è¡Œä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰çš„è¡¨ç°ã€‚é€šè¿‡åŠŸèƒ½å‘é‡ï¼ˆFVsï¼‰è¿™ä¸€æœºåˆ¶ï¼Œç ”ç©¶å‘ç°åœ¨ä¸åŒä»»åŠ¡å‘ˆç°æ–¹å¼ä¸‹ï¼Œæ¼”ç¤ºå’ŒæŒ‡ä»¤å¯¹è¯­è¨€æ¨¡å‹çš„ä»»åŠ¡è¡¨ç°æœ‰ç€ä¸åŒçš„å½±å“ã€‚ç ”ç©¶å‘ç°æŒ‡ä»¤åŠŸèƒ½å‘é‡æœ‰åŠ©äºæé«˜é›¶æ ·æœ¬ä»»åŠ¡å‡†ç¡®ç‡ï¼Œè€Œæ¼”ç¤ºå’ŒæŒ‡ä»¤åŠŸèƒ½å‘é‡åˆ©ç”¨æ¨¡å‹çš„ä¸åŒç»„æˆéƒ¨åˆ†ã€‚è¿™è¡¨æ˜ä¸åŒä»»åŠ¡å‘ˆç°æ–¹å¼ä¸ä¼šå½¢æˆé€šç”¨çš„ä»»åŠ¡è¡¨ç¤ºï¼Œè€Œæ˜¯æ¿€å‘éƒ¨åˆ†é‡å çš„æœºåˆ¶ã€‚è¿™ä¸ºç»“åˆæŒ‡ä»¤å’Œä»»åŠ¡æ¼”ç¤ºçš„å®è·µæä¾›äº†ç†è®ºæ”¯æŒï¼ŒåŒæ—¶ä¹Ÿå¯¹è·¨ä¸åŒå‘ˆç°å½¢å¼çš„ä»»åŠ¡æ¨ç†æå‡ºäº†æŒ‘æˆ˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¼”ç¤ºå’ŒæŒ‡ä»¤æ˜¯å¼•å¯¼è¯­è¨€æ¨¡å‹è¿›è¡Œä¸Šä¸‹æ–‡å­¦ä¹ çš„ä¸¤ç§ä¸»è¦æ–¹å¼ã€‚</li>
<li>åŠŸèƒ½å‘é‡ï¼ˆFVsï¼‰è¢«ç”¨æ¥æå–ä»»åŠ¡è¡¨ç¤ºã€‚</li>
<li>æŒ‡ä»¤åŠŸèƒ½å‘é‡å¯ä»¥æé«˜é›¶æ ·æœ¬ä»»åŠ¡çš„å‡†ç¡®ç‡ã€‚</li>
<li>æ¼”ç¤ºå’ŒæŒ‡ä»¤åŠŸèƒ½å‘é‡åˆ©ç”¨æ¨¡å‹çš„ä¸åŒéƒ¨åˆ†ã€‚</li>
<li>ä¸åŒä»»åŠ¡å‘ˆç°æ–¹å¼ä¸ä¼šå½¢æˆé€šç”¨çš„ä»»åŠ¡è¡¨ç¤ºï¼Œè€Œæ˜¯æ¿€å‘éƒ¨åˆ†é‡å çš„æœºåˆ¶ã€‚</li>
<li>ç»“åˆæŒ‡ä»¤å’Œä»»åŠ¡æ¼”ç¤ºçš„å®è·µå¯ä»¥å¾—åˆ°æ›´å¥½çš„æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12075">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-79332502a0e4a52811d78b9a23d3f3d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b2589291a5b80e73c363ebdf5aeefb81.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-18046737cb7dfa4f8cea85ff3ed197b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb56b643a3e3f4e708136c42728b6463.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Identifying-Legal-Holdings-with-LLMs-A-Systematic-Study-of-Performance-Scale-and-Memorization"><a href="#Identifying-Legal-Holdings-with-LLMs-A-Systematic-Study-of-Performance-Scale-and-Memorization" class="headerlink" title="Identifying Legal Holdings with LLMs: A Systematic Study of Performance,   Scale, and Memorization"></a>Identifying Legal Holdings with LLMs: A Systematic Study of Performance,   Scale, and Memorization</h2><p><strong>Authors:Chuck Arvin</strong></p>
<p>As large language models (LLMs) continue to advance in capabilities, it is essential to assess how they perform on established benchmarks. In this study, we present a suite of experiments to assess the performance of modern LLMs (ranging from 3B to 90B+ parameters) on CaseHOLD, a legal benchmark dataset for identifying case holdings. Our experiments demonstrate &#96;&#96;scaling effectsâ€™â€™ - performance on this task improves with model size, with more capable models like GPT4o and AmazonNovaPro achieving macro F1 scores of 0.744 and 0.720 respectively. These scores are competitive with the best published results on this dataset, and do not require any technically sophisticated model training, fine-tuning or few-shot prompting. To ensure that these strong results are not due to memorization of judicial opinions contained in the training data, we develop and utilize a novel citation anonymization test that preserves semantic meaning while ensuring case names and citations are fictitious. Models maintain strong performance under these conditions (macro F1 of 0.728), suggesting the performance is not due to rote memorization. These findings demonstrate both the promise and current limitations of LLMs for legal tasks with important implications for the development and measurement of automated legal analytics and legal benchmarks. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„èƒ½åŠ›ä¸æ–­æå‡ï¼Œè¯„ä¼°å®ƒä»¬åœ¨ç°æœ‰åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°å˜å¾—è‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶å‘ˆç°äº†ä¸€ç³»åˆ—å®éªŒï¼Œæ—¨åœ¨è¯„ä¼°ç°ä»£LLMï¼ˆå‚æ•°èŒƒå›´ä»3Båˆ°90B+ï¼‰åœ¨CaseHOLDè¿™ä¸€æ³•å¾‹åŸºå‡†æ•°æ®é›†ä¸Šçš„è¡¨ç°ï¼Œè¯¥æ•°æ®é›†ç”¨äºè¯†åˆ«åˆ¤ä¾‹æ³•æ‘˜è¦ã€‚æˆ‘ä»¬çš„å®éªŒå±•ç¤ºäº†â€œè§„æ¨¡æ•ˆåº”â€â€”â€”åœ¨æ­¤ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼šéšç€æ¨¡å‹è§„æ¨¡çš„æ‰©å¤§è€Œæé«˜ï¼Œæ›´å¼ºå¤§çš„æ¨¡å‹å¦‚GPT4oå’ŒAmazonNovaProåˆ†åˆ«å–å¾—äº†å®è§‚F1åˆ†æ•°ä¸º0.744å’Œ0.720ã€‚è¿™äº›åˆ†æ•°ä¸è¯¥æ•°æ®é›†ä¸Šå·²å‘å¸ƒçš„æœ€ä½³ç»“æœå…·æœ‰ç«äº‰åŠ›ï¼Œå¹¶ä¸”ä¸éœ€è¦ä»»ä½•æŠ€æœ¯å¤æ‚æ¨¡å‹çš„è®­ç»ƒã€å¾®è°ƒæˆ–å°‘æ ·æœ¬æç¤ºã€‚ä¸ºç¡®ä¿è¿™äº›å¼ºæœ‰åŠ›çš„ç»“æœå¹¶éç”±äºè®­ç»ƒæ•°æ®ä¸­å¸æ³•æ„è§çš„è®°å¿†ï¼Œæˆ‘ä»¬å¼€å‘å¹¶åˆ©ç”¨äº†ä¸€ç§æ–°å‹çš„å¼•ç”¨åŒ¿åæµ‹è¯•ï¼Œè¯¥æµ‹è¯•ä¿ç•™äº†è¯­ä¹‰å«ä¹‰ï¼ŒåŒæ—¶ç¡®ä¿æ¡ˆä¾‹åç§°å’Œå¼•ç”¨æ˜¯è™šæ„çš„ã€‚åœ¨è¿™äº›æ¡ä»¶ä¸‹ï¼Œæ¨¡å‹ä»ä¿æŒè‰¯å¥½çš„è¡¨ç°ï¼ˆå®è§‚F1åˆ†æ•°ä¸º0.728ï¼‰ï¼Œè¿™è¡¨æ˜è¡¨ç°å¹¶éç”±äºæ­»è®°ç¡¬èƒŒã€‚è¿™äº›å‘ç°å±•ç¤ºäº†LLMåœ¨æ³•å¾‹ä»»åŠ¡ä¸Šçš„æ½œåŠ›å’Œå½“å‰å±€é™æ€§ï¼Œå¯¹äºè‡ªåŠ¨åŒ–æ³•å¾‹åˆ†æå’Œæ³•å¾‹åŸºå‡†çš„å¼€å‘å’Œè¡¡é‡å…·æœ‰é‡è¦æ„ä¹‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.02172v2">PDF</a> Presented as a short paper at International Conference on Artificial   Intelligence and Law 2025 (Chicago, IL)</p>
<p><strong>Summary</strong></p>
<p>ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ³•å¾‹ä»»åŠ¡ä¸Šçš„æ€§èƒ½è¯„ä¼°è‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶é€šè¿‡ä¸€ç³»åˆ—å®éªŒï¼Œè¯„ä¼°äº†ä»3Båˆ°90B+å‚æ•°çš„ç°ä»£LLMåœ¨CaseHOLDæ³•å¾‹åŸºå‡†æ•°æ®é›†ä¸Šçš„è¡¨ç°ã€‚å®éªŒè¡¨æ˜ï¼Œæ¨¡å‹æ€§èƒ½éšè§„æ¨¡æ‰©å¤§è€Œæå‡ï¼ŒGPT4oå’ŒAmazonNovaProç­‰æ›´å…ˆè¿›çš„æ¨¡å‹å–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„å®è§‚F1åˆ†æ•°ã€‚æ¨¡å‹è®­ç»ƒæ— éœ€å¤æ‚æŠ€æœ¯ï¼Œä¸”é€šè¿‡æ–°å‹å¼•ç”¨åŒ¿åæµ‹è¯•è¯æ˜è¡¨ç°ç¨³å¥ï¼Œæš—ç¤ºæ¨¡å‹è¡¨ç°å¹¶éä»…ä¾é å¯¹è®­ç»ƒæ•°æ®çš„è®°å¿†ã€‚æ­¤ç ”ç©¶å¯¹è‡ªåŠ¨åŒ–æ³•å¾‹åˆ†æå’Œæ³•å¾‹åŸºå‡†çš„å‘å±•å…·æœ‰é‡è¦æ„ä¹‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ³•å¾‹ä»»åŠ¡ä¸Šçš„æ€§èƒ½è¯„ä¼°æ˜¯å…³é”®çš„ã€‚</li>
<li>å®éªŒè¯„ä¼°äº†ä¸åŒè§„æ¨¡çš„ç°ä»£LLMåœ¨CaseHOLDæ³•å¾‹åŸºå‡†æ•°æ®é›†ä¸Šçš„è¡¨ç°ã€‚</li>
<li>æ¨¡å‹æ€§èƒ½éšè§„æ¨¡æ‰©å¤§è€Œæå‡ï¼ŒGPT4oå’ŒAmazonNovaProç­‰æ¨¡å‹å–å¾—å®è§‚F1åˆ†æ•°å±•ç°å‡ºç«äº‰åŠ›ã€‚</li>
<li>æœ€ä½³æ¨¡å‹çš„è¡¨ç°ä¸éœ€è¦å¤æ‚çš„æŠ€æœ¯è®­ç»ƒï¼Œä¸”æ— éœ€æ±‚åŠ©äºç²¾ç»†è°ƒæ•´æˆ–å°‘æ ·æœ¬æç¤ºã€‚</li>
<li>é€šè¿‡æ–°å‹å¼•ç”¨åŒ¿åæµ‹è¯•è¯æ˜æ¨¡å‹è¡¨ç°ç¨³å¥ï¼Œæš—ç¤ºå…¶å¹¶éä»…ä¾é è®°å¿†è®­ç»ƒæ•°æ®æ¥å–å¾—è¡¨ç°ã€‚</li>
<li>ç ”ç©¶ç»“æœæ­ç¤ºäº†LLMåœ¨æ³•å¾‹ä»»åŠ¡ä¸Šçš„æ½œåŠ›å’Œå½“å‰é™åˆ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.02172">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6bb5df790f9a9b546add69605930edb3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-79f626b5f7c483edab76796e2ff988c1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6f1d77beddfbb2842423c5f5c740a469.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9adc73211c22c0b0ecb6cc0984cd1fe5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7b51ceda711f76de4442f49ea93c0e28.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="DINOv2-powered-Few-Shot-Semantic-Segmentation-A-Unified-Framework-via-Cross-Model-Distillation-and-4D-Correlation-Mining"><a href="#DINOv2-powered-Few-Shot-Semantic-Segmentation-A-Unified-Framework-via-Cross-Model-Distillation-and-4D-Correlation-Mining" class="headerlink" title="DINOv2-powered Few-Shot Semantic Segmentation: A Unified Framework via   Cross-Model Distillation and 4D Correlation Mining"></a>DINOv2-powered Few-Shot Semantic Segmentation: A Unified Framework via   Cross-Model Distillation and 4D Correlation Mining</h2><p><strong>Authors:Wei Zhuo, Zhiyue Tang, Wufeng Xue, Hao Ding, Linlin Shen</strong></p>
<p>Few-shot semantic segmentation has gained increasing interest due to its generalization capability, i.e., segmenting pixels of novel classes requiring only a few annotated images. Prior work has focused on meta-learning for support-query matching, with extensive development in both prototype-based and aggregation-based methods. To address data scarcity, recent approaches have turned to foundation models to enhance representation transferability for novel class segmentation. Among them, a hybrid dual-modal framework including both DINOv2 and SAM has garnered attention due to their complementary capabilities. We wonder â€œcan we build a unified model with knowledge from both foundation models?â€ To this end, we propose FS-DINO, with only DINOv2â€™s encoder and a lightweight segmenter. The segmenter features a bottleneck adapter, a meta-visual prompt generator based on dense similarities and semantic embeddings, and a decoder. Through coarse-to-fine cross-model distillation, we effectively integrate SAMâ€™s knowledge into our lightweight segmenter, which can be further enhanced by 4D correlation mining on support-query pairs. Extensive experiments on COCO-20i, PASCAL-5i, and FSS-1000 demonstrate the effectiveness and superiority of our method. </p>
<blockquote>
<p>å°‘æ ·æœ¬è¯­ä¹‰åˆ†å‰²å› å…¶æ³›åŒ–èƒ½åŠ›è€Œå—åˆ°è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ï¼Œå³åªéœ€è¦å°‘é‡æ ‡æ³¨å›¾åƒå°±èƒ½å¯¹æ–°å‹ç±»åˆ«çš„åƒç´ è¿›è¡Œåˆ†å‰²ã€‚æ—©æœŸçš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨æ”¯æŒæŸ¥è¯¢åŒ¹é…çš„å…ƒå­¦ä¹ ä¸Šï¼Œå¹¶åœ¨åŸºäºåŸå‹å’ŒåŸºäºèšåˆçš„æ–¹æ³•ä¸Šè¿›è¡Œäº†å¤§é‡å¼€å‘ã€‚ä¸ºäº†è§£å†³æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼Œæœ€è¿‘çš„æ–¹æ³•å·²ç»è½¬å‘åŸºç¡€æ¨¡å‹ï¼Œä»¥å¢å¼ºæ–°å‹ç±»åˆ«åˆ†å‰²çš„è¡¨ç¤ºå¯è¿ç§»æ€§ã€‚å…¶ä¸­ï¼Œä¸€ä¸ªåŒ…å«DINOv2å’ŒSAMçš„æ··åˆåŒæ¨¡æ€æ¡†æ¶å› å…¶äº’è¡¥èƒ½åŠ›è€Œå—åˆ°å…³æ³¨ã€‚æˆ‘ä»¬æƒ³çŸ¥é“â€œæˆ‘ä»¬èƒ½å¦å»ºç«‹ä¸€ä¸ªç»Ÿä¸€çš„æ¨¡å‹ï¼ŒåŒæ—¶æ‹¥æœ‰è¿™ä¸¤ç§åŸºç¡€æ¨¡å‹çš„çŸ¥è¯†ï¼Ÿâ€ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†FS-DINOæ¨¡å‹ï¼Œå®ƒä»…é‡‡ç”¨DINOv2çš„ç¼–ç å™¨å’Œè½»é‡çº§åˆ†å‰²å™¨ã€‚åˆ†å‰²å™¨å…·æœ‰ç“¶é¢ˆé€‚é…å™¨ã€åŸºäºå¯†é›†ç›¸ä¼¼æ€§å’Œè¯­ä¹‰åµŒå…¥çš„å…ƒè§†è§‰æç¤ºç”Ÿæˆå™¨ä»¥åŠè§£ç å™¨ã€‚é€šè¿‡ç²—åˆ°ç»†çš„è·¨æ¨¡å‹è’¸é¦ï¼Œæˆ‘ä»¬æœ‰æ•ˆåœ°å°†SAMçš„çŸ¥è¯†é›†æˆåˆ°æˆ‘ä»¬çš„è½»é‡çº§åˆ†å‰²å™¨ä¸­ï¼Œé€šè¿‡æ”¯æŒæŸ¥è¯¢å¯¹ä¸Šçš„4Då…³è”æŒ–æ˜å¯ä»¥è¿›ä¸€æ­¥å¢å¼ºå…¶æ€§èƒ½ã€‚åœ¨COCO-20iã€PASCAL-5iå’ŒFSS-1000ä¸Šçš„å¤§é‡å®éªŒè¯æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15669v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å°‘æ ·æœ¬è¯­ä¹‰åˆ†å‰²æ¨¡å‹çš„ç ”ç©¶å·²ç»å–å¾—äº†å¾ˆå¤§è¿›å±•ï¼Œå°¤å…¶åœ¨åˆ©ç”¨åŸºç¡€æ¨¡å‹æé«˜å¯¹æ–°ç±»åˆ«æ•°æ®çš„æ³›åŒ–èƒ½åŠ›æ–¹é¢ã€‚æå‡ºäº†FS-DINOæ¨¡å‹ï¼Œé€šè¿‡ç»“åˆDINOv2ç¼–ç å™¨å’Œè½»é‡çº§åˆ†å‰²å™¨æ¥å®ç°ã€‚è¯¥æ¨¡å‹é€šè¿‡è·¨æ¨¡å‹è’¸é¦æŠ€æœ¯ï¼Œèåˆäº†SAMæ¨¡å‹çš„çŸ¥è¯†ï¼Œè¿›ä¸€æ­¥æé«˜æ€§èƒ½ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨COCO-20iã€PASCAL-5iå’ŒFSS-1000æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å°‘æ ·æœ¬è¯­ä¹‰åˆ†å‰²æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å¾—åˆ°å…³æ³¨ã€‚</li>
<li>å‰ç½®å·¥ä½œä¸»è¦é›†ä¸­åœ¨æ”¯æŒæŸ¥è¯¢åŒ¹é…ã€åŸå‹åŸºç¡€å’ŒèšåˆåŸºç¡€æ–¹æ³•ä¸Šã€‚</li>
<li>åŸºç¡€æ¨¡å‹è¢«ç”¨äºè§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œæé«˜æ–°ç±»åˆ«æ•°æ®çš„è¡¨ç¤ºè¿ç§»èƒ½åŠ›ã€‚</li>
<li>æå‡ºäº†FS-DINOæ¨¡å‹ï¼Œç»“åˆDINOv2ç¼–ç å™¨å’Œè½»é‡çº§åˆ†å‰²å™¨ã€‚</li>
<li>æ¨¡å‹åˆ©ç”¨è·¨æ¨¡å‹è’¸é¦æŠ€æœ¯èåˆäº†SAMæ¨¡å‹çš„çŸ¥è¯†ã€‚</li>
<li>é€šè¿‡ç²—åˆ°ç»†çš„è·¨æ¨¡å‹è’¸é¦å’Œ4Då…³è”æŒ–æ˜æŠ€æœ¯å¢å¼ºäº†æ¨¡å‹æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15669">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3f6890d91ebe1960e5599200e3039c30.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f1b3d4db6d74d97dc00881c9c258be31.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-705f4bf8ce37863dbc19816784df5723.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="SQL-o1-A-Self-Reward-Heuristic-Dynamic-Search-Method-for-Text-to-SQL"><a href="#SQL-o1-A-Self-Reward-Heuristic-Dynamic-Search-Method-for-Text-to-SQL" class="headerlink" title="SQL-o1: A Self-Reward Heuristic Dynamic Search Method for Text-to-SQL"></a>SQL-o1: A Self-Reward Heuristic Dynamic Search Method for Text-to-SQL</h2><p><strong>Authors:Shuai Lyu, Haoran Luo, Ripeng Li, Zhonghong Ou, Jiangfeng Sun, Yang Qin, Xiaoran Shang, Meina Song, Yifan Zhu</strong></p>
<p>Text-to-SQL (Text2SQL) aims to map natural language questions to executable SQL queries. Although large language models (LLMs) have driven significant progress, current approaches struggle with poor transferability to open-source LLMs, limited robustness against logic and function errors in complex queries, and inefficiencies in structured search. We introduce SQL-o1, a self-reward-driven heuristic search framework built on an agent-based architecture to enhance model reasoning capabilities. SQL-o1 leverages Monte Carlo Tree Search (MCTS) for structured, multi-step exploration, and incorporates a dynamic pruning strategy to accelerate inference without sacrificing accuracy. On the Spider and Bird benchmarks, SQL-o1 achieves a +10.8 execution accuracy improvement on the complex Bird dataset, surpassing even GPT-4-based models. Notably, it exhibits strong few-shot generalization and robust cross-model transferability across open-source LLMs. Our code is available at:<a target="_blank" rel="noopener" href="https://github.com/ShuaiLyu0110/SQL-o1">https://github.com/ShuaiLyu0110/SQL-o1</a>. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°SQLï¼ˆText2SQLï¼‰æ—¨åœ¨å°†è‡ªç„¶è¯­è¨€é—®é¢˜æ˜ å°„ä¸ºå¯æ‰§è¡Œçš„SQLæŸ¥è¯¢ã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å·²ç»å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å½“å‰çš„æ–¹æ³•åœ¨å¼€æºLLMsçš„è¿ç§»æ€§æ–¹é¢è¡¨ç°è¾ƒå·®ï¼Œåœ¨å¤æ‚æŸ¥è¯¢ä¸­çš„é€»è¾‘å’ŒåŠŸèƒ½é”™è¯¯æ–¹é¢çš„ç¨³å¥æ€§æœ‰é™ï¼Œä»¥åŠç»“æ„åŒ–æœç´¢çš„æ•ˆç‡ä½ä¸‹ã€‚æˆ‘ä»¬å¼•å…¥äº†SQL-o1ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºä»£ç†æ¶æ„çš„è‡ªæˆ‘å¥–åŠ±é©±åŠ¨å¯å‘å¼æœç´¢æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚SQL-o1åˆ©ç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰è¿›è¡Œç»“æ„åŒ–ã€å¤šæ­¥éª¤çš„æ¢ç´¢ï¼Œå¹¶é‡‡ç”¨äº†åŠ¨æ€å‰ªæç­–ç•¥æ¥åŠ é€Ÿæ¨ç†ï¼ŒåŒæ—¶ä¸ç‰ºç‰²å‡†ç¡®æ€§ã€‚åœ¨Spiderå’ŒBirdåŸºå‡†æµ‹è¯•ä¸­ï¼ŒSQL-o1åœ¨å¤æ‚çš„Birdæ•°æ®é›†ä¸Šå®ç°äº†+10.8çš„æ‰§è¡Œå‡†ç¡®æ€§æ”¹è¿›ï¼Œç”šè‡³è¶…è¶Šäº†GPT-4æ¨¡å‹ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå®ƒè¡¨ç°å‡ºå¼ºå¤§çš„å°‘æ ·æœ¬æ³›åŒ–èƒ½åŠ›å’Œè·¨æ¨¡å‹çš„ç¨³å¥è¿ç§»æ€§ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨ï¼š[<a target="_blank" rel="noopener" href="https://github.com/ShuaiLyu011">https://github.com/ShuaiLyu011</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.11741v3">PDF</a> 28 pages,12 figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ–‡æœ¬åˆ°SQLï¼ˆText2SQLï¼‰çš„ç›®æ ‡æ˜¯å°†è‡ªç„¶è¯­è¨€é—®é¢˜æ˜ å°„åˆ°å¯æ‰§è¡Œçš„SQLæŸ¥è¯¢ã€‚å½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è™½ç„¶æœ‰æ‰€è¿›å±•ï¼Œä½†åœ¨å¼€æºLLMsçš„è¿ç§»æ€§ã€å¤æ‚æŸ¥è¯¢ä¸­çš„é€»è¾‘å’ŒåŠŸèƒ½é”™è¯¯æŠ—æ€§ï¼Œä»¥åŠç»“æ„åŒ–æœç´¢çš„æ•ˆç‡æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚æˆ‘ä»¬å¼•å…¥äº†SQL-o1ï¼Œä¸€ä¸ªåŸºäºä»£ç†æ¶æ„çš„è‡ªå¥–åŠ±é©±åŠ¨å¯å‘å¼æœç´¢æ¡†æ¶ï¼Œä»¥æé«˜æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚SQL-o1åˆ©ç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰è¿›è¡Œç»“æ„åŒ–ã€å¤šæ­¥éª¤çš„æ¢ç´¢ï¼Œå¹¶é‡‡ç”¨åŠ¨æ€å‰ªæç­–ç•¥ä»¥åŠ é€Ÿæ¨ç†è€Œä¸æŸå¤±ç²¾åº¦ã€‚åœ¨Spiderå’ŒBirdåŸºå‡†æµ‹è¯•ä¸­ï¼ŒSQL-o1åœ¨å¤æ‚çš„Birdæ•°æ®é›†ä¸Šå®ç°äº†+10.8çš„æ‰§è¡Œç²¾åº¦æå‡ï¼Œè¶…è¶Šäº†GPT-4æ¨¡å‹ã€‚æ­¤å¤–ï¼Œå®ƒå±•ç°å‡ºå¼ºå¤§çš„å°æ ·æœ¬æ³›åŒ–èƒ½åŠ›å’Œè·¨æ¨¡å‹çš„å¼€æ”¾æ€§è¿ç§»èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Text-to-SQLæ—¨åœ¨å°†è‡ªç„¶è¯­è¨€é—®é¢˜è½¬æ¢ä¸ºSQLæŸ¥è¯¢ã€‚</li>
<li>å½“å‰LLMsåœ¨Text-to-SQLé¢†åŸŸé¢ä¸´è¿ç§»æ€§ã€å¤æ‚æŸ¥è¯¢ä¸­çš„é€»è¾‘å’ŒåŠŸèƒ½é”™è¯¯æŠ—æ€§ä»¥åŠæœç´¢æ•ˆç‡çš„æŒ‘æˆ˜ã€‚</li>
<li>SQL-o1æ˜¯ä¸€ä¸ªè‡ªå¥–åŠ±é©±åŠ¨å¯å‘å¼æœç´¢æ¡†æ¶ï¼Œç”¨äºå¢å¼ºæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>SQL-o1åˆ©ç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰è¿›è¡Œç»“æ„åŒ–ã€å¤šæ­¥éª¤æ¢ç´¢ã€‚</li>
<li>åŠ¨æ€å‰ªæç­–ç•¥è¢«ç”¨äºåŠ é€Ÿæ¨ç†è¿‡ç¨‹è€Œä¸æŸå¤±ç²¾åº¦ã€‚</li>
<li>åœ¨åŸºå‡†æµ‹è¯•ä¸­ï¼ŒSQL-o1åœ¨å¤æ‚æ•°æ®é›†ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¶…è¶Šäº†GPT-4æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.11741">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ad7dc37bbb5b2dedf995417b6283f79b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-af44bc7be2bd9a3fa44d7d3a77468a7e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e284892eae3cc743d5bee9506bf9a911.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-682e540e6a8aa8efa6811ae891cbed93.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ecc1e035666faf21a74636f516f5a84e.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="BARE-Leveraging-Base-Language-Models-for-Few-Shot-Synthetic-Data-Generation"><a href="#BARE-Leveraging-Base-Language-Models-for-Few-Shot-Synthetic-Data-Generation" class="headerlink" title="BARE: Leveraging Base Language Models for Few-Shot Synthetic Data   Generation"></a>BARE: Leveraging Base Language Models for Few-Shot Synthetic Data   Generation</h2><p><strong>Authors:Alan Zhu, Parth Asawa, Jared Quincy Davis, Lingjiao Chen, Boris Hanin, Ion Stoica, Joseph E. Gonzalez, Matei Zaharia</strong></p>
<p>As the demand for high-quality data in model training grows, researchers and developers are increasingly generating synthetic data to tune and train LLMs. However, current data generation methods rely on seed sets containing tens of thousands of examples to prompt instruction-tuned models. This reliance can be especially problematic when the curation of high-quality examples is expensive or difficult. In this paper we explore the novel few-shot synthetic data generation setting â€“ generating a high-quality dataset from a few examples. We show that when working with only a few seed examples, instruction-tuned models used in current synthetic data methods produce insufficient diversity for downstream tasks. In contrast, we show that base models without post-training, largely untapped for synthetic data generation, offer substantially greater output diversity, albeit with lower instruction following abilities. Leveraging this insight, we propose Base-Refine (BARE), a novel two-stage method that combines the diversity of base models with the quality assurance of instruction-tuned models. BARE excels in few-shot synthetic data generation: using only 3 seed examples it generates diverse, high-quality datasets that significantly improve downstream task performance. We show that fine-tuning Llama 3.1 8B with 1,000 BARE-generated samples achieves performance comparable to state-of-the-art similarly sized models on LiveCodeBench tasks. Furthermore, data generated with BARE enables a 101% improvement for a fine-tuned Llama 3.2 1B on GSM8K over data generated by only instruction-models, and an 18.4% improvement for a fine-tuned Llama 3.1 8B over the state-of-the-art RAFT method for RAG data generation. </p>
<blockquote>
<p>éšç€æ¨¡å‹è®­ç»ƒä¸­å¯¹é«˜è´¨é‡æ•°æ®çš„éœ€æ±‚ä¸æ–­å¢é•¿ï¼Œç ”ç©¶äººå‘˜å’Œå¼€å‘äººå‘˜è¶Šæ¥è¶Šå¤šåœ°ç”Ÿæˆåˆæˆæ•°æ®æ¥è°ƒæ•´å’ŒåŸ¹è®­å¤§å‹è¯­è¨€æ¨¡å‹ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ•°æ®ç”Ÿæˆæ–¹æ³•ä¾èµ–äºåŒ…å«æ•°åä¸‡ä¸ªç¤ºä¾‹çš„ç§å­é›†æ¥æç¤ºæŒ‡ä»¤è°ƒæ•´æ¨¡å‹ã€‚å½“é«˜è´¨é‡ç¤ºä¾‹çš„æ”¶é›†æ—¢æ˜‚è´µåˆå›°éš¾æ—¶ï¼Œè¿™ç§ä¾èµ–å¯èƒ½ä¼šç‰¹åˆ«æˆé—®é¢˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†æ–°å‹å°‘æ ·æœ¬åˆæˆæ•°æ®ç”Ÿæˆè®¾ç½®â€”â€”ä»å°‘é‡ç¤ºä¾‹ç”Ÿæˆé«˜è´¨é‡æ•°æ®é›†ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œåœ¨ä¸ä»…å‡ ä¸ªç§å­ç¤ºä¾‹ä¸€èµ·å·¥ä½œæ—¶ï¼Œå½“å‰åˆæˆæ•°æ®æ–¹æ³•ä¸­ä½¿ç”¨æŒ‡ä»¤è°ƒæ•´æ¨¡å‹ä¼šäº§ç”Ÿä¸‹æ¸¸ä»»åŠ¡ä¸è¶³çš„å¤šæ ·æ€§ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘ä»¬è¯æ˜ï¼Œæœªç»è®­ç»ƒçš„åŸºç¡€æ¨¡å‹åœ¨åˆæˆæ•°æ®ç”Ÿæˆæ–¹é¢å°šæœªå¾—åˆ°å……åˆ†åˆ©ç”¨ï¼Œå¯ä»¥æä¾›æ›´å¤§çš„è¾“å‡ºå¤šæ ·æ€§ï¼Œå°½ç®¡å…¶æŒ‡ä»¤æ‰§è¡Œèƒ½åŠ›è¾ƒä½ã€‚åˆ©ç”¨è¿™ä¸€è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†Base-Refineï¼ˆBAREï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„ä¸¤é˜¶æ®µæ–¹æ³•ï¼Œç»“åˆäº†åŸºç¡€æ¨¡å‹çš„å¤šæ ·æ€§å’ŒæŒ‡ä»¤è°ƒæ•´æ¨¡å‹çš„è´¨é‡ä¿è¯ã€‚BAREåœ¨å°‘æ ·æœ¬åˆæˆæ•°æ®ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼šä»…ä½¿ç”¨ä¸‰ä¸ªç§å­ç¤ºä¾‹å³å¯ç”Ÿæˆå¤šæ ·åŒ–ä¸”é«˜è´¨é‡çš„æ•°æ®é›†ï¼Œè¿™äº›æ•°æ®é›†å¯ä»¥æ˜¾è‘—æé«˜ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚æˆ‘ä»¬å±•ç¤ºäº†ä»…ä½¿ç”¨ç»è¿‡BAREç”Ÿæˆçš„æ ·æœ¬å¾®è°ƒè¿‡çš„Llama 3.1 8Bçš„æ€§èƒ½å¯ä»¥ä¸LiveCodeBenchä»»åŠ¡ä¸Šè¡¨ç°å“è¶Šçš„å¤§å‹ç›¸ä¼¼æ¨¡å‹ç›¸åª²ç¾ã€‚æ­¤å¤–ï¼Œä½¿ç”¨BAREç”Ÿæˆçš„æ•°æ®å¯ä»¥ä½¿ç»è¿‡å¾®è°ƒåçš„Llama 3.2 1Båœ¨GSM8Kä¸Šçš„æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°101%ï¼Œé«˜äºä»…ç”±æŒ‡ä»¤æ¨¡å‹ç”Ÿæˆçš„æ•°æ®ç”Ÿæˆçš„æ ·æœ¬ï¼Œè€Œå¯¹äºç»è¿‡å¾®è°ƒåçš„Llama 3.1 8Båœ¨RAGæ•°æ®ç”Ÿæˆæ–¹é¢çš„æ€§èƒ½æå‡å¹…åº¦ä¸º18.4%ï¼Œè¶…è¿‡äº†å½“å‰æœ€å…ˆè¿›çš„RAFTæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01697v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢ç´¢äº†åŸºäºå°‘æ•°æ ·æœ¬çš„åˆæˆæ•°æ®ç”Ÿæˆæ–¹æ³•ã€‚ç ”ç©¶å‘ç°ï¼Œå½“å‰åˆæˆæ•°æ®ç”Ÿæˆæ–¹æ³•ä¾èµ–äºå¤§é‡é«˜è´¨é‡æ ·æœ¬è¿›è¡Œè®­ç»ƒï¼Œæˆæœ¬é«˜æ˜‚æˆ–éš¾ä»¥è·å–é«˜è´¨é‡æ ·æœ¬æ—¶å­˜åœ¨é—®é¢˜ã€‚æœ¬æ–‡æå‡ºäº†ç»“åˆåŸºç¡€æ¨¡å‹çš„å¤šæ ·æ€§å’ŒæŒ‡ä»¤å¾®è°ƒæ¨¡å‹çš„è´¨é‡ä¿è¯çš„Base-Refineï¼ˆBAREï¼‰æ–¹æ³•ï¼Œèƒ½åœ¨ä»…ä½¿ç”¨å°‘æ•°æ ·æœ¬çš„æƒ…å†µä¸‹ç”Ÿæˆé«˜è´¨é‡ã€å¤šæ ·åŒ–çš„æ•°æ®é›†ã€‚å®éªŒè¡¨æ˜ï¼ŒBAREåœ¨ç”Ÿæˆå°‘é‡åˆæˆæ•°æ®æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä»…ä½¿ç”¨ä¸‰ä¸ªæ ·æœ¬å³å¯ç”Ÿæˆé«˜è´¨é‡çš„æ•°æ®é›†ï¼Œæ˜¾è‘—æé«˜ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å½“å‰åˆæˆæ•°æ®ç”Ÿæˆæ–¹æ³•ä¾èµ–äºå¤§é‡é«˜è´¨é‡æ ·æœ¬è¿›è¡Œè®­ç»ƒï¼Œæˆæœ¬é«˜æ˜‚æˆ–éš¾ä»¥è·å–æ—¶å­˜åœ¨é—®é¢˜ã€‚</li>
<li>åŸºäºå°‘æ•°æ ·æœ¬çš„åˆæˆæ•°æ®ç”Ÿæˆæ–¹æ³•å—åˆ°å…³æ³¨ï¼Œæˆä¸ºç ”ç©¶çƒ­ç‚¹ã€‚</li>
<li>åŸºç¡€æ¨¡å‹åœ¨æœªç»è¿‡æŒ‡ä»¤å¾®è°ƒæ—¶å±•ç°å‡ºæ›´é«˜çš„è¾“å‡ºå¤šæ ·æ€§ã€‚</li>
<li>BAREç»“åˆäº†åŸºç¡€æ¨¡å‹çš„å¤šæ ·æ€§å’ŒæŒ‡ä»¤å¾®è°ƒæ¨¡å‹çš„è´¨é‡ä¿è¯ï¼Œèƒ½åœ¨ä»…ä½¿ç”¨å°‘æ•°æ ·æœ¬çš„æƒ…å†µä¸‹ç”Ÿæˆé«˜è´¨é‡çš„æ•°æ®é›†ã€‚</li>
<li>BAREæ–¹æ³•åœ¨å¤šä¸ªå®éªŒä»»åŠ¡ä¸­è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œå¦‚LiveCodeBenchä»»åŠ¡å’ŒGSM8Kä»»åŠ¡ç­‰ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01697">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-732eabf3cca18f43125c1a1133e4979a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-489d9e2aca63db868c028afa7cc4688b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a05c40204bac0d3a17b8b400573df7a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce53d8eff2ade9b6ac5ba91bff8be431.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d01e1697a47b259e1829f35caba44291.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc89248735488e2af125e4416f350632.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="P3P-Pseudo-3D-Pre-training-for-Scaling-3D-Voxel-based-Masked-Autoencoders"><a href="#P3P-Pseudo-3D-Pre-training-for-Scaling-3D-Voxel-based-Masked-Autoencoders" class="headerlink" title="P3P: Pseudo-3D Pre-training for Scaling 3D Voxel-based Masked   Autoencoders"></a>P3P: Pseudo-3D Pre-training for Scaling 3D Voxel-based Masked   Autoencoders</h2><p><strong>Authors:Xuechao Chen, Ying Chen, Jialin Li, Qiang Nie, Hanqiu Deng, Yong Liu, Qixing Huang, Yang Li</strong></p>
<p>3D pre-training is crucial to 3D perception tasks. Nevertheless, limited by the difficulties in collecting clean and complete 3D data, 3D pre-training has persistently faced data scaling challenges. In this work, we introduce a novel self-supervised pre-training framework that incorporates millions of images into 3D pre-training corpora by leveraging a large depth estimation model. New pre-training corpora encounter new challenges in representation ability and embedding efficiency of models. Previous pre-training methods rely on farthest point sampling and k-nearest neighbors to embed a fixed number of 3D tokens. However, these approaches prove inadequate when it comes to embedding millions of samples that feature a diverse range of point numbers, spanning from 1,000 to 100,000. In contrast, we propose a tokenizer with linear-time complexity, which enables the efficient embedding of a flexible number of tokens. Accordingly, a new 3D reconstruction target is proposed to cooperate with our 3D tokenizer. Our method achieves state-of-the-art performance in 3D classification, few-shot learning, and 3D segmentation. Code is available at <a target="_blank" rel="noopener" href="https://github.com/XuechaoChen/P3P-MAE">https://github.com/XuechaoChen/P3P-MAE</a>. </p>
<blockquote>
<p>3Dé¢„è®­ç»ƒå¯¹3Dæ„ŸçŸ¥ä»»åŠ¡è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå—é™äºæ”¶é›†å¹²å‡€ä¸”å®Œæ•´çš„3Dæ•°æ®çš„å›°éš¾ï¼Œ3Dé¢„è®­ç»ƒä¸€ç›´é¢ä¸´ç€æ•°æ®è§„æ¨¡æ‰©å¤§çš„æŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹è‡ªç›‘ç£é¢„è®­ç»ƒæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨æ·±åº¦ä¼°è®¡æ¨¡å‹å°†æ•°ç™¾ä¸‡å¼ å›¾åƒçº³å…¥3Dé¢„è®­ç»ƒè¯­æ–™åº“ä¸­ã€‚æ–°çš„é¢„è®­ç»ƒè¯­æ–™åº“åœ¨æ¨¡å‹çš„è¡¨ç¤ºèƒ½åŠ›å’ŒåµŒå…¥æ•ˆç‡æ–¹é¢é‡åˆ°äº†æ–°çš„æŒ‘æˆ˜ã€‚ä¹‹å‰çš„é¢„è®­ç»ƒæ–¹æ³•ä¾èµ–äºæœ€è¿œç‚¹é‡‡æ ·å’Œkè¿‘é‚»æ¥åµŒå…¥å›ºå®šæ•°é‡çš„3Dä»¤ç‰Œã€‚ç„¶è€Œï¼Œå½“éœ€è¦åµŒå…¥æ•°é‡ä»1000åˆ°100000ä¸ç­‰çš„æ•°ç™¾ä¸‡æ ·æœ¬æ—¶ï¼Œè¿™äº›æ–¹æ³•è¯æ˜æ˜¯æ— æ•ˆçš„ã€‚ç›¸åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å…·æœ‰çº¿æ€§æ—¶é—´å¤æ‚åº¦çš„æ ‡è®°å™¨ï¼Œå¯ä»¥å®ç°çµæ´»æ•°é‡çš„ä»¤ç‰Œçš„é«˜æ•ˆåµŒå…¥ã€‚å› æ­¤ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„3Dé‡å»ºç›®æ ‡ï¼Œä»¥é…åˆæˆ‘ä»¬çš„3Dä»¤ç‰ŒåŒ–å™¨ä½¿ç”¨ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨3Dåˆ†ç±»ã€å°æ ·æœ¬å­¦ä¹ å’Œ3Dåˆ†å‰²æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/XuechaoChen/P3P-MAE%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/XuechaoChen/P3P-MAEæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.10007v3">PDF</a> Under review. Pre-print</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åˆ©ç”¨å¤§è§„æ¨¡æ·±åº¦ä¼°è®¡æ¨¡å‹å°†æ•°ç™¾ä¸‡å›¾åƒçº³å…¥3Dé¢„è®­ç»ƒè¯­æ–™åº“çš„æ–°å‹è‡ªç›‘ç£é¢„è®­ç»ƒæ¡†æ¶ã€‚é’ˆå¯¹æ–°é¢„è®­ç»ƒè¯­æ–™åº“æ‰€å¸¦æ¥çš„æ¨¡å‹è¡¨å¾èƒ½åŠ›å’ŒåµŒå…¥æ•ˆç‡é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å…·æœ‰çº¿æ€§æ—¶é—´å¤æ‚åº¦çš„åˆ†è¯å™¨ï¼Œå®ç°äº†çµæ´»æ•°é‡çš„æ ‡è®°çš„é«˜æ•ˆåµŒå…¥ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„3Dé‡å»ºç›®æ ‡ä¸ä¹‹é…åˆã€‚è¯¥æ–¹æ³•åœ¨3Dåˆ†ç±»ã€å°æ ·æœ¬å­¦ä¹ å’Œ3Dåˆ†å‰²æ–¹é¢å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>3Dé¢„è®­ç»ƒå¯¹3Dæ„ŸçŸ¥ä»»åŠ¡è‡³å…³é‡è¦ï¼Œä½†å—é™äºç¼ºä¹å¹²å‡€å®Œæ•´çš„3Dæ•°æ®ï¼Œé¢ä¸´æ•°æ®è§„æ¨¡æ‰©å±•çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹è‡ªç›‘ç£é¢„è®­ç»ƒæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å¤§è§„æ¨¡æ·±åº¦ä¼°è®¡æ¨¡å‹å°†æ•°ç™¾ä¸‡å›¾åƒçº³å…¥3Dé¢„è®­ç»ƒè¯­æ–™åº“ã€‚</li>
<li>æ–°é¢„è®­ç»ƒè¯­æ–™åº“å¸¦æ¥äº†æ¨¡å‹è¡¨å¾èƒ½åŠ›å’ŒåµŒå…¥æ•ˆç‡çš„æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰é¢„è®­ç»ƒæ–¹æ³•ä¾èµ–æœ€è¿œç‚¹é‡‡æ ·å’Œkè¿‘é‚»æ¥åµŒå…¥å›ºå®šæ•°é‡çš„3Dæ ‡è®°ï¼Œä½†å¯¹äºåµŒå…¥æ•°é‡èŒƒå›´å¹¿æ³›çš„æ•°ç™¾ä¸‡æ ·æœ¬æ˜¾å¾—ä¸è¶³ã€‚</li>
<li>æå‡ºäº†ä¸€ç§å…·æœ‰çº¿æ€§æ—¶é—´å¤æ‚åº¦çš„åˆ†è¯å™¨ï¼Œå®ç°äº†é«˜æ•ˆåµŒå…¥çµæ´»æ•°é‡çš„æ ‡è®°ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„3Dé‡å»ºç›®æ ‡ï¼Œä¸3Dåˆ†è¯å™¨é…åˆä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.10007">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d91140d27a27efdb303e9528ee58ad67.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a3c201f2f97d9eaef83bb78aec2b549.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f02b9aa61de4505b144d0a08df6cf210.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a011e09d1b1d56950b115074be3738d2.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="VITAL-Interactive-Few-Shot-Imitation-Learning-via-Visual-Human-in-the-Loop-Corrections"><a href="#VITAL-Interactive-Few-Shot-Imitation-Learning-via-Visual-Human-in-the-Loop-Corrections" class="headerlink" title="VITAL: Interactive Few-Shot Imitation Learning via Visual   Human-in-the-Loop Corrections"></a>VITAL: Interactive Few-Shot Imitation Learning via Visual   Human-in-the-Loop Corrections</h2><p><strong>Authors:Hamidreza Kasaei, Mohammadreza Kasaei</strong></p>
<p>Imitation Learning (IL) has emerged as a powerful approach in robotics, allowing robots to acquire new skills by mimicking human actions. Despite its potential, the data collection process for IL remains a significant challenge due to the logistical difficulties and high costs associated with obtaining high-quality demonstrations. To address these issues, we propose a large-scale data generation from a handful of demonstrations through data augmentation in simulation. Our approach leverages affordable hardware and visual processing techniques to collect demonstrations, which are then augmented to create extensive training datasets for imitation learning. By utilizing both real and simulated environments, along with human-in-the-loop corrections, we enhance the generalizability and robustness of the learned policies. We evaluated our method through several rounds of experiments in both simulated and real-robot settings, focusing on tasks of varying complexity, including bottle collecting, stacking objects, and hammering. Our experimental results validate the effectiveness of our approach in learning robust robot policies from simulated data, significantly improved by human-in-the-loop corrections and real-world data integration. Additionally, we demonstrate the frameworkâ€™s capability to generalize to new tasks, such as setting a drink tray, showcasing its adaptability and potential for handling a wide range of real-world manipulation tasks. A video of the experiments can be found at: <a target="_blank" rel="noopener" href="https://youtu.be/YeVAMRqRe64?si=R179xDlEGc7nPu8i">https://youtu.be/YeVAMRqRe64?si=R179xDlEGc7nPu8i</a> </p>
<blockquote>
<p>æ¨¡ä»¿å­¦ä¹ ï¼ˆILï¼‰ä½œä¸ºæœºå™¨äººæŠ€æœ¯ä¸­çš„ä¸€ç§å¼ºå¤§æ–¹æ³•å·²ç»å‡ºç°ï¼Œå®ƒå…è®¸æœºå™¨äººé€šè¿‡æ¨¡ä»¿äººç±»è¡Œä¸ºæ¥ä¹ å¾—æ–°æŠ€èƒ½ã€‚å°½ç®¡æ½œåŠ›å·¨å¤§ï¼Œä½†ILçš„æ•°æ®æ”¶é›†è¿‡ç¨‹ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºè·å–é«˜è´¨é‡æ¼”ç¤ºçš„ç‰©æµå›°éš¾å’Œæˆæœ¬é«˜æ˜‚ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šè¿‡æ¨¡æ‹Ÿä¸­çš„æ•°æ®å¢å¼ºä»å°‘é‡æ¼”ç¤ºä¸­ç”Ÿæˆå¤§è§„æ¨¡æ•°æ®çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨å»‰ä»·çš„ç¡¬ä»¶å’Œè§†è§‰å¤„ç†æŠ€æœ¯æ¥æ”¶é›†æ¼”ç¤ºï¼Œç„¶åé€šè¿‡å¢å¼ºè¿™äº›æ¼”ç¤ºæ¥åˆ›å»ºç”¨äºæ¨¡ä»¿å­¦ä¹ çš„å¹¿æ³›è®­ç»ƒæ•°æ®é›†ã€‚é€šè¿‡ç»“åˆçœŸå®å’Œæ¨¡æ‹Ÿç¯å¢ƒä»¥åŠäººä¸ºçš„å®æ—¶æ ¡æ­£ï¼Œæˆ‘ä»¬æé«˜äº†å­¦ä¹ ç­–ç•¥çš„é€šç”¨æ€§å’Œç¨³å¥æ€§ã€‚æˆ‘ä»¬åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®æœºå™¨äººç¯å¢ƒä¸­è¿›è¡Œäº†å¤šè½®å®éªŒï¼Œè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œé‡ç‚¹ç ”ç©¶äº†ä¸åŒå¤æ‚åº¦çš„ä»»åŠ¡ï¼ŒåŒ…æ‹¬æ¡ç“¶å­ã€å †å ç‰©ä½“å’Œé”¤å‡»ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ä»æ¨¡æ‹Ÿæ•°æ®ä¸­å­¦ä¹ ç¨³å¥çš„æœºå™¨äººç­–ç•¥çš„æœ‰æ•ˆæ€§ï¼Œäººä¸ºçš„å®æ—¶æ ¡æ­£å’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†æˆå¯¹æ­¤æœ‰å¾ˆå¤§çš„æå‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å±•ç¤ºäº†è¯¥æ¡†æ¶èƒ½å¤Ÿæ¨å¹¿åˆ°æ–°çš„ä»»åŠ¡ï¼Œå¦‚è®¾ç½®é¥®æ–™æ‰˜ç›˜ï¼Œå±•ç¤ºäº†å…¶é€‚åº”æ€§å’Œå¤„ç†å„ç§ç°å®ä¸–ç•Œæ“ä½œä»»åŠ¡çš„æ½œåŠ›ã€‚å®éªŒè§†é¢‘å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://youtu.be/YeVAMRqRe64?si=R179xDlEGc7nPu8i">https://youtu.be/YeVAMRqRe64?si=R179xDlEGc7nPu8i</a> æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.21244v2">PDF</a> </p>
<p><strong>Summary</strong><br>     æœºå™¨äººé€šè¿‡æ¨¡ä»¿äººç±»åŠ¨ä½œè¿›è¡ŒæŠ€èƒ½å­¦ä¹ çš„æ–¹æ³•è¢«ç§°ä¸ºæ¨¡ä»¿å­¦ä¹ ï¼ˆILï¼‰ã€‚ç„¶è€Œï¼Œæ•°æ®æ”¶é›†æ˜¯ILçš„ä¸€å¤§æŒ‘æˆ˜ï¼Œå› ä¸ºè·å–é«˜è´¨é‡æ¼”ç¤ºçš„ç‰©æµå›°éš¾å’Œæˆæœ¬é«˜æ˜‚ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºé€šè¿‡æ¨¡æ‹Ÿä¸­çš„æ•°æ®å¢å¼ºä»å°‘é‡æ¼”ç¤ºä¸­ç”Ÿæˆå¤§è§„æ¨¡æ•°æ®çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å»‰ä»·ç¡¬ä»¶å’Œè§†è§‰å¤„ç†æŠ€æœ¯æ”¶é›†æ¼”ç¤ºï¼Œå¹¶é€šè¿‡æ¨¡æ‹Ÿç¯å¢ƒå’Œäººä¸ºä¿®æ­£å¢å¼ºè®­ç»ƒæ•°æ®é›†ã€‚åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®æœºå™¨äººç¯å¢ƒä¸­è¿›è¡Œçš„å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•å­¦ä¹ åˆ°çš„ç­–ç•¥å…·æœ‰å¹¿æ³›çš„é€‚ç”¨æ€§å’Œé²æ£’æ€§ã€‚å®éªŒç»“æœè¯å®ï¼Œäººä¸ºä¿®æ­£å’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†æˆèƒ½æé«˜ç­–ç•¥è´¨é‡ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿé€‚åº”å„ç§ç°å®ä¸–ç•Œæ“æ§ä»»åŠ¡å¹¶æ³›åŒ–åˆ°æ–°ä»»åŠ¡ã€‚è§‚çœ‹å®éªŒçš„è§†é¢‘è¯·ç‚¹å‡»é“¾æ¥ã€‚</p>
<p><strong>Key Takeaways</strong><br>     1. æ¨¡ä»¿å­¦ä¹ ï¼ˆILï¼‰æ˜¯æœºå™¨äººå­¦ä¹ çš„ä¸€ç§å¼ºå¤§æ–¹æ³•ï¼Œå…è®¸æœºå™¨äººé€šè¿‡æ¨¡ä»¿äººç±»åŠ¨ä½œè·å–æ–°æŠ€èƒ½ã€‚<br>     2. æ•°æ®æ”¶é›†æ˜¯ILé¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜ä¹‹ä¸€ï¼Œå› ä¸ºè·å–é«˜è´¨é‡æ¼”ç¤ºçš„æˆæœ¬è¾ƒé«˜ä¸”å­˜åœ¨ç‰©æµå›°éš¾ã€‚<br>     3. æå‡ºäº†ä¸€ç§é€šè¿‡æ¨¡æ‹Ÿä¸­çš„æ•°æ®å¢å¼ºä»å°‘é‡æ¼”ç¤ºç”Ÿæˆå¤§è§„æ¨¡æ•°æ®çš„æ–¹æ³•æ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚<br>     4. è¯¥æ–¹æ³•åˆ©ç”¨å»‰ä»·ç¡¬ä»¶å’Œè§†è§‰å¤„ç†æŠ€æœ¯æ”¶é›†æ¼”ç¤ºï¼Œå¹¶é€šè¿‡æ¨¡æ‹Ÿç¯å¢ƒå’Œäººä¸ºä¿®æ­£å¢å¼ºè®­ç»ƒæ•°æ®é›†ã€‚<br>     5. å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿå­¦ä¹ é€‚åº”æ€§å¼ºã€é²æ£’æ€§é«˜çš„ç­–ç•¥ã€‚<br>     6. äººä¸ºä¿®æ­£å’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†æˆæœ‰åŠ©äºæé«˜ç­–ç•¥è´¨é‡ã€‚</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.21244">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e103aea637a0b281da3c8cf11fac3085.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3b9896c527839778caea7c8c1ddd20af.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f7b2632cbc9a073e77ce0bba676c8aa0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-559379b0cf28d9a0dfd5dd211ad14857.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Make-An-Agent-A-Generalizable-Policy-Network-Generator-with-Behavior-Prompted-Diffusion"><a href="#Make-An-Agent-A-Generalizable-Policy-Network-Generator-with-Behavior-Prompted-Diffusion" class="headerlink" title="Make-An-Agent: A Generalizable Policy Network Generator with   Behavior-Prompted Diffusion"></a>Make-An-Agent: A Generalizable Policy Network Generator with   Behavior-Prompted Diffusion</h2><p><strong>Authors:Yongyuan Liang, Tingqiang Xu, Kaizhe Hu, Guangqi Jiang, Furong Huang, Huazhe Xu</strong></p>
<p>Can we generate a control policy for an agent using just one demonstration of desired behaviors as a prompt, as effortlessly as creating an image from a textual description? In this paper, we present Make-An-Agent, a novel policy parameter generator that leverages the power of conditional diffusion models for behavior-to-policy generation. Guided by behavior embeddings that encode trajectory information, our policy generator synthesizes latent parameter representations, which can then be decoded into policy networks. Trained on policy network checkpoints and their corresponding trajectories, our generation model demonstrates remarkable versatility and scalability on multiple tasks and has a strong generalization ability on unseen tasks to output well-performed policies with only few-shot demonstrations as inputs. We showcase its efficacy and efficiency on various domains and tasks, including varying objectives, behaviors, and even across different robot manipulators. Beyond simulation, we directly deploy policies generated by Make-An-Agent onto real-world robots on locomotion tasks. Project page: <a target="_blank" rel="noopener" href="https://cheryyunl.github.io/make-an-agent/">https://cheryyunl.github.io/make-an-agent/</a> </p>
<blockquote>
<p>æˆ‘ä»¬å¯ä»¥åƒä»æ–‡æœ¬æè¿°ä¸­åˆ›å»ºå›¾åƒä¸€æ ·ï¼Œä»…ä½¿ç”¨ä¸€ä¸ªæœŸæœ›è¡Œä¸ºçš„æ¼”ç¤ºä½œä¸ºæç¤ºæ¥ä¸ºä»£ç†ç”Ÿæˆä¸€ä¸ªæ§åˆ¶ç­–ç•¥å—ï¼Ÿåœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Make-An-Agentï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹çš„ç­–ç•¥å‚æ•°ç”Ÿæˆå™¨ï¼Œå®ƒåˆ©ç”¨æ¡ä»¶æ‰©æ•£æ¨¡å‹çš„åŠ›é‡æ¥è¿›è¡Œè¡Œä¸ºåˆ°ç­–ç•¥çš„ç”Ÿæˆã€‚åœ¨è¡Œä¸ºåµŒå…¥ï¼ˆç¼–ç è½¨è¿¹ä¿¡æ¯ï¼‰çš„æŒ‡å¯¼ä¸‹ï¼Œæˆ‘ä»¬çš„ç­–ç•¥ç”Ÿæˆå™¨èƒ½å¤Ÿåˆæˆæ½œåœ¨çš„å‚æ•°è¡¨ç¤ºï¼Œç„¶åå¯ä»¥å°†å…¶è§£ç ä¸ºç­–ç•¥ç½‘ç»œã€‚æˆ‘ä»¬çš„ç”Ÿæˆæ¨¡å‹åœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šçš„å¤šåŠŸèƒ½æ€§å’Œå¯æ‰©å±•æ€§ï¼Œå¹¶ä¸”åœ¨ä»…ä½¿ç”¨å°‘é‡æ¼”ç¤ºä½œä¸ºè¾“å…¥çš„æƒ…å†µä¸‹ï¼Œåœ¨æœªè§è¿‡çš„ä»»åŠ¡ä¸Šå…·æœ‰å¾ˆå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬åœ¨å„ç§é¢†åŸŸå’Œä»»åŠ¡ä¸­å±•ç¤ºäº†å®ƒçš„åŠŸæ•ˆå’Œæ•ˆç‡ï¼ŒåŒ…æ‹¬ä¸åŒçš„ç›®æ ‡ã€è¡Œä¸ºå’Œä¸åŒçš„æœºå™¨äººæ“çºµå™¨ã€‚é™¤äº†æ¨¡æ‹Ÿç¯å¢ƒå¤–ï¼Œæˆ‘ä»¬è¿˜ç›´æ¥å°†Make-An-Agentç”Ÿæˆçš„ç­–ç•¥éƒ¨ç½²åˆ°çœŸå®ä¸–ç•Œçš„æœºå™¨äººä¸Šè¿›è¡Œè¿åŠ¨ä»»åŠ¡ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://cheryyunl.github.io/make-an-agent/">https://cheryyunl.github.io/make-an-agent/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.10973v4">PDF</a> Annual Conference on Neural Information Processing Systems 38</p>
<p><strong>Summary</strong><br>åŸºäºæ¡ä»¶æ‰©æ•£æ¨¡å‹çš„ç­–ç•¥å‚æ•°ç”Ÿæˆå™¨Make-An-Agentï¼Œé€šè¿‡åˆ©ç”¨è¡Œä¸ºåµŒå…¥è½¨è¿¹ä¿¡æ¯æ¥åˆæˆæ½œåœ¨å‚æ•°è¡¨ç¤ºï¼Œè¿›è€Œè§£ç ä¸ºç­–ç•¥ç½‘ç»œã€‚è¯¥ç”Ÿæˆæ¨¡å‹åœ¨å¤šä»»åŠ¡ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„é€šç”¨æ€§å’Œå¯æ‰©å±•æ€§ï¼Œä»…éœ€å°‘é‡æ¼”ç¤ºå³å¯è¾“å‡ºè¡¨ç°è‰¯å¥½çš„ç­–ç•¥ï¼Œå¹¶å¯¹æœªè§ä»»åŠ¡å…·æœ‰è¾ƒå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Make-An-Agentåˆ©ç”¨æ¡ä»¶æ‰©æ•£æ¨¡å‹è¿›è¡Œè¡Œä¸ºåˆ°ç­–ç•¥ç”Ÿæˆã€‚</li>
<li>é€šè¿‡è¡Œä¸ºåµŒå…¥è½¨è¿¹ä¿¡æ¯æ¥åˆæˆæ½œåœ¨å‚æ•°è¡¨ç¤ºã€‚</li>
<li>æ¨¡å‹èƒ½å¤Ÿè§£ç æ½œåœ¨å‚æ•°è¡¨ç¤ºå½¢æˆç­–ç•¥ç½‘ç»œã€‚</li>
<li>ç”Ÿæˆæ¨¡å‹åœ¨å¤šä»»åŠ¡ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„é€šç”¨æ€§å’Œå¯æ‰©å±•æ€§ã€‚</li>
<li>ä»…éœ€å°‘é‡æ¼”ç¤ºå³å¯è¾“å‡ºè¡¨ç°è‰¯å¥½çš„ç­–ç•¥ã€‚</li>
<li>æ¨¡å‹å¯¹æœªè§ä»»åŠ¡å…·æœ‰è¾ƒå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.10973">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-aa45f4f001fb029de675ad9f1ebd0c1c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-423c4964e23d50f08558fb68b2c1ce0a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-32bf11efff904191e99cde375638abca.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fbf5570a59fc0a488dc9783f95ddf5d0.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-24/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-24/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-24/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-c8c36f47c381d93c7567615f0704074e.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-24  Efficient Correlation Volume Sampling for Ultra-High-Resolution Optical   Flow Estimation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-24
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-24/MMT/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-cbbd765fafcd09f21f4b2375d4b1e5ce.jpg" class="responsive-img" alt="MMT">
                        
                        <span class="card-title">MMT</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            MMT æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-24  Property Enhanced Instruction Tuning for Multi-task Molecule Generation   with Large Language Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-24
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/MMT/" class="post-category">
                                    MMT
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/MMT/">
                        <span class="chip bg-color">MMT</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">25156.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
