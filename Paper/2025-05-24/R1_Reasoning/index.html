<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-24  GoT-R1 Unleashing Reasoning Capability of MLLM for Visual Generation   with Reinforcement Learning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-a182ffdb78954744b9948c3d72830132.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-24
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    81 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-24-æ›´æ–°"><a href="#2025-05-24-æ›´æ–°" class="headerlink" title="2025-05-24 æ›´æ–°"></a>2025-05-24 æ›´æ–°</h1><h2 id="GoT-R1-Unleashing-Reasoning-Capability-of-MLLM-for-Visual-Generation-with-Reinforcement-Learning"><a href="#GoT-R1-Unleashing-Reasoning-Capability-of-MLLM-for-Visual-Generation-with-Reinforcement-Learning" class="headerlink" title="GoT-R1: Unleashing Reasoning Capability of MLLM for Visual Generation   with Reinforcement Learning"></a>GoT-R1: Unleashing Reasoning Capability of MLLM for Visual Generation   with Reinforcement Learning</h2><p><strong>Authors:Chengqi Duan, Rongyao Fang, Yuqing Wang, Kun Wang, Linjiang Huang, Xingyu Zeng, Hongsheng Li, Xihui Liu</strong></p>
<p>Visual generation models have made remarkable progress in creating realistic images from text prompts, yet struggle with complex prompts that specify multiple objects with precise spatial relationships and attributes. Effective handling of such prompts requires explicit reasoning about the semantic content and spatial layout. We present GoT-R1, a framework that applies reinforcement learning to enhance semantic-spatial reasoning in visual generation. Building upon the Generation Chain-of-Thought approach, GoT-R1 enables models to autonomously discover effective reasoning strategies beyond predefined templates through carefully designed reinforcement learning. To achieve this, we propose a dual-stage multi-dimensional reward framework that leverages MLLMs to evaluate both the reasoning process and final output, enabling effective supervision across the entire generation pipeline. The reward system assesses semantic alignment, spatial accuracy, and visual quality in a unified approach. Experimental results demonstrate significant improvements on T2I-CompBench benchmark, particularly in compositional tasks involving precise spatial relationships and attribute binding. GoT-R1 advances the state-of-the-art in image generation by successfully transferring sophisticated reasoning capabilities to the visual generation domain. To facilitate future research, we make our code and pretrained models publicly available at <a target="_blank" rel="noopener" href="https://github.com/gogoduan/GoT-R1">https://github.com/gogoduan/GoT-R1</a>. </p>
<blockquote>
<p>è§†è§‰ç”Ÿæˆæ¨¡å‹åœ¨æ ¹æ®æ–‡æœ¬æç¤ºåˆ›å»ºé€¼çœŸçš„å›¾åƒæ–¹é¢å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä½†åœ¨å¤„ç†å¤æ‚çš„æç¤ºæ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ï¼Œè¿™äº›æç¤ºæŒ‡å®šäº†å…·æœ‰ç²¾ç¡®ç©ºé—´å…³ç³»å’Œå±æ€§çš„å¤šä¸ªå¯¹è±¡ã€‚æœ‰æ•ˆå¤„ç†è¿™äº›æç¤ºéœ€è¦å¯¹è¯­ä¹‰å†…å®¹å’Œç©ºé—´å¸ƒå±€è¿›è¡Œæ˜ç¡®çš„æ¨ç†ã€‚æˆ‘ä»¬æå‡ºäº†GoT-R1æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åº”ç”¨å¼ºåŒ–å­¦ä¹ æ¥å¢å¼ºè§†è§‰ç”Ÿæˆä¸­çš„è¯­ä¹‰ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚åŸºäºæ€ç»´ç”Ÿæˆé“¾çš„æ–¹æ³•ï¼ŒGoT-R1ä½¿æ¨¡å‹èƒ½å¤Ÿé€šè¿‡ç²¾å¿ƒè®¾è®¡çš„å¼ºåŒ–å­¦ä¹ è‡ªä¸»å‘ç°è¶…è¶Šé¢„å®šæ¨¡æ¿çš„æœ‰æ•ˆæ¨ç†ç­–ç•¥ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŒé˜¶æ®µå¤šç»´å¥–åŠ±æ¡†æ¶ï¼Œåˆ©ç”¨MLLMsæ¥è¯„ä¼°æ¨ç†è¿‡ç¨‹å’Œæœ€ç»ˆè¾“å‡ºï¼Œä»è€Œåœ¨æ•´ä¸ªç”Ÿæˆç®¡é“ä¸­å®ç°å¯¹æœ‰æ•ˆç›‘ç£çš„ç»Ÿä¸€æ–¹æ³•ã€‚å¥–åŠ±ç³»ç»Ÿè¯„ä¼°è¯­ä¹‰å¯¹é½ã€ç©ºé—´ç²¾åº¦å’Œè§†è§‰è´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨T2I-CompBenchåŸºå‡†æµ‹è¯•ä¸­ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¶‰åŠç²¾ç¡®ç©ºé—´å…³ç³»å’Œå±æ€§ç»‘å®šçš„ç»„åˆä»»åŠ¡ä¸­ï¼ŒGoT-R1å–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚GoT-R1é€šè¿‡å°†å¤æ‚çš„æ¨ç†èƒ½åŠ›æˆåŠŸè½¬ç§»åˆ°è§†è§‰ç”Ÿæˆé¢†åŸŸï¼Œä»è€Œæ¨åŠ¨äº†å›¾åƒç”Ÿæˆçš„æœ€æ–°æŠ€æœ¯ã€‚ä¸ºäº†æ–¹ä¾¿æœªæ¥çš„ç ”ç©¶ï¼Œæˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/gogoduan/GoT-R1%E5%85%AC%E5%BC%80%E6%88%91%E4%BB%AC%E7%9A%84%E4%BB%A3%E7%A0%81%E5%92%8C%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E3%80%82">https://github.com/gogoduan/GoT-R1å…¬å¼€æˆ‘ä»¬çš„ä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17022v1">PDF</a> Github page refer to: <a target="_blank" rel="noopener" href="https://github.com/gogoduan/GoT-R1">https://github.com/gogoduan/GoT-R1</a></p>
<p><strong>Summary</strong><br>æ–‡æœ¬æè¿°äº†ä¸€ç§åä¸ºGoT-R1çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨å¼ºåŒ–å­¦ä¹ æ¥æé«˜è§†è§‰ç”Ÿæˆä¸­çš„è¯­ä¹‰ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚å®ƒåœ¨Generation Chain-of-Thoughtæ–¹æ³•çš„åŸºç¡€ä¸Šï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿè‡ªä¸»å‘ç°æœ‰æ•ˆçš„æ¨ç†ç­–ç•¥ï¼Œè¶…è¶Šé¢„è®¾æ¨¡æ¿ã€‚é€šè¿‡ç²¾å¿ƒè®¾è®¡å¼ºåŒ–å­¦ä¹ ï¼Œæå‡ºäº†ä¸€ä¸ªåŒé˜¶æ®µå¤šç»´åº¦å¥–åŠ±æ¡†æ¶ï¼Œåˆ©ç”¨MLLMsè¯„ä¼°æ¨ç†è¿‡ç¨‹å’Œæœ€ç»ˆè¾“å‡ºï¼Œå®ç°æ•´ä¸ªç”Ÿæˆç®¡é“çš„æœ‰æ•ˆç›‘ç£ã€‚å¥–åŠ±ç³»ç»Ÿä»¥ç»Ÿä¸€çš„æ–¹å¼è¯„ä¼°è¯­ä¹‰å¯¹é½ã€ç©ºé—´å‡†ç¡®æ€§å’Œè§†è§‰è´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGoT-R1åœ¨T2I-CompBenchåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°æ˜¾è‘—æ”¹è¿›ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¶‰åŠç²¾ç¡®ç©ºé—´å…³ç³»å’Œå±æ€§ç»‘å®šçš„ç»„åˆä»»åŠ¡ä¸­ã€‚å®ƒä¸ºæ¨åŠ¨å›¾åƒç”Ÿæˆé¢†åŸŸçš„å…ˆè¿›æŠ€æœ¯åšå‡ºäº†è´¡çŒ®ï¼ŒæˆåŠŸåœ°å°†å¤æ‚çš„æ¨ç†èƒ½åŠ›è½¬ç§»åˆ°è§†è§‰ç”Ÿæˆé¢†åŸŸã€‚ç›¸å…³çš„ä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹å¯ä»¥åœ¨<a target="_blank" rel="noopener" href="https://github.com/gogoduan/GoT-R1%E5%85%AC%E5%BC%80%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/gogoduan/GoT-R1å…¬å¼€è·å–ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GoT-R1æ¡†æ¶ä½¿ç”¨å¼ºåŒ–å­¦ä¹ æå‡è§†è§‰ç”Ÿæˆä¸­çš„è¯­ä¹‰ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚</li>
<li>è¯¥æ¡†æ¶åŸºäºGeneration Chain-of-Thoughtæ–¹æ³•ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿè‡ªä¸»å‘ç°è¶…è¶Šé¢„è®¾æ¨¡æ¿çš„æœ‰æ•ˆæ¨ç†ç­–ç•¥ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªåŒé˜¶æ®µå¤šç»´åº¦å¥–åŠ±æ¡†æ¶æ¥è¯„ä¼°æ¨ç†è¿‡ç¨‹å’Œæœ€ç»ˆè¾“å‡ºã€‚</li>
<li>å¥–åŠ±ç³»ç»ŸåŒæ—¶è€ƒè™‘è¯­ä¹‰å¯¹é½ã€ç©ºé—´å‡†ç¡®æ€§å’Œè§†è§‰è´¨é‡ã€‚</li>
<li>GoT-R1åœ¨T2I-CompBenchåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶åœ¨æ¶‰åŠç²¾ç¡®ç©ºé—´å…³ç³»å’Œå±æ€§ç»‘å®šçš„ä»»åŠ¡ä¸Šã€‚</li>
<li>GoT-R1ä¸ºå›¾åƒç”Ÿæˆé¢†åŸŸåšå‡ºäº†è´¡çŒ®ï¼ŒæˆåŠŸè½¬ç§»å¤æ‚çš„æ¨ç†èƒ½åŠ›è‡³è§†è§‰ç”Ÿæˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17022">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e4f86e756ea103ba33ef85276fa499cc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ac3e533746199a71b6311cf6f8e569a3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-72a7943da5a5b50fcc00365153cbcaed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d02cea7366d973db12152c62a36e4e0e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-82ab1004f60eb92e5a1ada95bf60cd8a.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="ARB-A-Comprehensive-Arabic-Multimodal-Reasoning-Benchmark"><a href="#ARB-A-Comprehensive-Arabic-Multimodal-Reasoning-Benchmark" class="headerlink" title="ARB: A Comprehensive Arabic Multimodal Reasoning Benchmark"></a>ARB: A Comprehensive Arabic Multimodal Reasoning Benchmark</h2><p><strong>Authors:Sara Ghaboura, Ketan More, Wafa Alghallabi, Omkar Thawakar, Jorma Laaksonen, Hisham Cholakkal, Salman Khan, Rao Muhammad Anwer</strong></p>
<p>As Large Multimodal Models (LMMs) become more capable, there is growing interest in evaluating their reasoning processes alongside their final outputs. However, most benchmarks remain focused on English, overlooking languages with rich linguistic and cultural contexts, such as Arabic. To address this gap, we introduce the Comprehensive Arabic Multimodal Reasoning Benchmark (ARB), the first benchmark designed to evaluate step-by-step reasoning in Arabic across both textual and visual modalities. ARB spans 11 diverse domains, including visual reasoning, document understanding, OCR, scientific analysis, and cultural interpretation. It comprises 1,356 multimodal samples paired with 5,119 human-curated reasoning steps and corresponding actions. We evaluated 12 state-of-the-art open- and closed-source LMMs and found persistent challenges in coherence, faithfulness, and cultural grounding. ARB offers a structured framework for diagnosing multimodal reasoning in underrepresented languages and marks a critical step toward inclusive, transparent, and culturally aware AI systems. We release the benchmark, rubric, and evaluation suit to support future research and reproducibility. Code available at: <a target="_blank" rel="noopener" href="https://github.com/mbzuai-oryx/ARB">https://github.com/mbzuai-oryx/ARB</a> </p>
<blockquote>
<p>éšç€å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰çš„åŠŸèƒ½è¶Šæ¥è¶Šå¼ºå¤§ï¼Œäººä»¬å¯¹å…¶æ¨ç†è¿‡ç¨‹åŠå…¶æœ€ç»ˆè¾“å‡ºçš„è¯„ä¼°å…´è¶£æ—¥ç›Šæµ“åšã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°åŸºå‡†æµ‹è¯•ä»ç„¶ä¸“æ³¨äºè‹±è¯­ï¼Œå¿½ç•¥äº†å…·æœ‰ä¸°å¯Œè¯­è¨€å’Œæ–‡åŒ–èƒŒæ™¯çš„è¯­è¨€ï¼Œå¦‚é˜¿æ‹‰ä¼¯è¯­ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æ¨å‡ºäº†å…¨é¢çš„é˜¿æ‹‰ä¼¯è¯­å¤šæ¨¡æ€æ¨ç†åŸºå‡†æµ‹è¯•ï¼ˆARBï¼‰ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°é˜¿æ‹‰ä¼¯è¯­ä¸­é€æ­¥æ¨ç†çš„åŸºå‡†æµ‹è¯•ï¼Œæ¶‰åŠæ–‡æœ¬å’Œè§†è§‰ä¸¤ç§æ¨¡å¼ã€‚ARBæ¶µç›–11ä¸ªä¸åŒé¢†åŸŸï¼ŒåŒ…æ‹¬è§†è§‰æ¨ç†ã€æ–‡æ¡£ç†è§£ã€OCRã€ç§‘å­¦åˆ†æå’Œæ–‡åŒ–è§£è¯»ã€‚å®ƒç”±1356ä¸ªå¤šæ¨¡æ€æ ·æœ¬ç»„æˆï¼Œé…æœ‰5119ä¸ªäººå·¥ç­–åˆ’çš„æ¨ç†æ­¥éª¤å’Œç›¸åº”è¡ŒåŠ¨ã€‚æˆ‘ä»¬è¯„ä¼°äº†12ä¸ªæœ€å…ˆè¿›çš„å¼€æºå’Œé—­æºLMMsï¼Œå‘ç°åœ¨è¿è´¯æ€§ã€å¿ è¯šæ€§å’Œæ–‡åŒ–ä¾æ®æ–¹é¢å­˜åœ¨æŒç»­æŒ‘æˆ˜ã€‚ARBä¸ºè¯Šæ–­ä»£è¡¨æ€§ä¸è¶³çš„è¯­è¨€ä¸­çš„å¤šæ¨¡æ€æ¨ç†æä¾›äº†ç»“æ„åŒ–æ¡†æ¶ï¼Œæœç€åŒ…å®¹ã€é€æ˜å’Œæ–‡åŒ–æ„ŸçŸ¥çš„AIç³»ç»Ÿè¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚æˆ‘ä»¬å‘å¸ƒè¿™ä¸ªåŸºå‡†æµ‹è¯•ã€ç»†åˆ™å’Œè¯„ä¼°å¥—ä»¶ï¼Œä»¥æ”¯æŒæœªæ¥çš„ç ”ç©¶å’Œå¯é‡å¤æ€§ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/mbzuai-oryx/ARB%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/mbzuai-oryx/ARBæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17021v1">PDF</a> Github : <a target="_blank" rel="noopener" href="https://github.com/mbzuai-oryx/ARB">https://github.com/mbzuai-oryx/ARB</a>, Huggingface:   <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/MBZUAI/ARB">https://huggingface.co/datasets/MBZUAI/ARB</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç»¼åˆæ€§é˜¿æ‹‰ä¼¯è¯­å¤šæ¨¡æ€æ¨ç†åŸºå‡†æµ‹è¯•ï¼ˆARBï¼‰ï¼Œè¯¥åŸºå‡†æµ‹è¯•æ—¨åœ¨è¯„ä¼°é˜¿æ‹‰ä¼¯è¯­ä¸­çš„é€æ­¥æ¨ç†è¿‡ç¨‹ï¼Œæ¶‰åŠæ–‡æœ¬å’Œè§†è§‰ä¸¤ç§æ¨¡å¼ã€‚ARBæ¶µç›–äº†11ä¸ªä¸åŒé¢†åŸŸï¼ŒåŒ…æ‹¬è§†è§‰æ¨ç†ã€æ–‡æ¡£ç†è§£ã€OCRã€ç§‘å­¦åˆ†æå’Œæ–‡åŒ–è§£è¯»ç­‰ã€‚å®ƒåŒ…å«äº†ä¸äººå·¥æ¨ç†æ­¥éª¤ç›¸å¯¹åº”çš„å¤šç§æ¨¡å¼æ ·æœ¬ï¼Œä»¥åŠå¤§è§„æ¨¡æ¨¡å‹é¢ä¸´çš„å¤šæ¨¡æ€æ¨ç†æŒ‘æˆ˜ã€‚è¯¥åŸºå‡†æµ‹è¯•ä¸ºæœªæ¥çš„ç ”ç©¶å’Œæ–‡åŒ–æ„è¯†çš„AIç³»ç»Ÿæä¾›äº†ä¸€ä¸ªæ¡†æ¶æ”¯æŒã€‚ARBå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/mbzuai-oryx/ARB">https://github.com/mbzuai-oryx/ARB</a>ä¸Šè®¿é—®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç»¼åˆé˜¿æ‹‰ä¼¯è¯­å¤šæ¨¡æ€æ¨ç†åŸºå‡†æµ‹è¯•ï¼ˆARBï¼‰æ˜¯é¦–ä¸ªæ—¨åœ¨è¯„ä¼°é˜¿æ‹‰ä¼¯è¯­ä¸­é€æ­¥æ¨ç†è¿‡ç¨‹çš„åŸºå‡†æµ‹è¯•ã€‚</li>
<li>è¯¥åŸºå‡†æµ‹è¯•æ¶µç›–äº†æ–‡æœ¬å’Œè§†è§‰ä¸¤ç§æ¨¡å¼çš„å¤šæ¨¡æ€æ ·æœ¬ã€‚</li>
<li>å®ƒåŒ…æ‹¬å¤šç§é¢†åŸŸï¼Œå¦‚è§†è§‰æ¨ç†ã€æ–‡æ¡£ç†è§£ã€OCRã€ç§‘å­¦åˆ†æå’Œæ–‡åŒ–è§£è¯»ç­‰ã€‚</li>
<li>è¯¥åŸºå‡†æµ‹è¯•åŒ…å«äººå·¥æ¨ç†æ­¥éª¤å’Œå¯¹åº”çš„è¡ŒåŠ¨ï¼Œç”¨äºè¯„ä¼°å¤§è§„æ¨¡æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ã€‚</li>
<li>å¯¹å¤šä¸ªå…ˆè¿›çš„å¼€æ”¾å’Œå°é—­çš„å¤§è§„æ¨¡æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œå‘ç°å®ƒä»¬åœ¨è¿è´¯æ€§ã€å¿ è¯šæ€§å’Œæ–‡åŒ–ä¾æ®æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>è¯¥åŸºå‡†æµ‹è¯•ä¸ºæœªæ¥çš„ç ”ç©¶å’Œæ–‡åŒ–æ„è¯†çš„AIç³»ç»Ÿæä¾›äº†æ¡†æ¶æ”¯æŒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17021">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3e9caecfe9f79f46e463c19dba71ebaf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b4e382e5b8b710b352785fa68562cb5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fcf4b8ee673a3e071a7c1b5cdb35298f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-499d2deb0ccb05c134a04d81e12d7545.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-05e4e0d7b2bb52834b9c179f6d498097.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Let-Androids-Dream-of-Electric-Sheep-A-Human-like-Image-Implication-Understanding-and-Reasoning-Framework"><a href="#Let-Androids-Dream-of-Electric-Sheep-A-Human-like-Image-Implication-Understanding-and-Reasoning-Framework" class="headerlink" title="Let Androids Dream of Electric Sheep: A Human-like Image Implication   Understanding and Reasoning Framework"></a>Let Androids Dream of Electric Sheep: A Human-like Image Implication   Understanding and Reasoning Framework</h2><p><strong>Authors:Chenhao Zhang, Yazhe Niu</strong></p>
<p>Metaphorical comprehension in images remains a critical challenge for AI systems, as existing models struggle to grasp the nuanced cultural, emotional, and contextual implications embedded in visual content. While multimodal large language models (MLLMs) excel in basic Visual Question Answer (VQA) tasks, they struggle with a fundamental limitation on image implication tasks: contextual gaps that obscure the relationships between different visual elements and their abstract meanings. Inspired by the human cognitive process, we propose Let Androids Dream (LAD), a novel framework for image implication understanding and reasoning. LAD addresses contextual missing through the three-stage framework: (1) Perception: converting visual information into rich and multi-level textual representations, (2) Search: iteratively searching and integrating cross-domain knowledge to resolve ambiguity, and (3) Reasoning: generating context-alignment image implication via explicit reasoning. Our framework with the lightweight GPT-4o-mini model achieves SOTA performance compared to 15+ MLLMs on English image implication benchmark and a huge improvement on Chinese benchmark, performing comparable with the GPT-4o model on Multiple-Choice Question (MCQ) and outperforms 36.7% on Open-Style Question (OSQ). Additionally, our work provides new insights into how AI can more effectively interpret image implications, advancing the field of vision-language reasoning and human-AI interaction. Our project is publicly available at <a target="_blank" rel="noopener" href="https://github.com/MING-ZCH/Let-Androids-Dream-of-Electric-Sheep">https://github.com/MING-ZCH/Let-Androids-Dream-of-Electric-Sheep</a>. </p>
<blockquote>
<p>å›¾åƒä¸­çš„éšå–»ç†è§£å¯¹äººå·¥æ™ºèƒ½ç³»ç»Ÿæ¥è¯´ä»ç„¶æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ï¼Œå› ä¸ºç°æœ‰æ¨¡å‹å¾ˆéš¾æŠŠæ¡è§†è§‰å†…å®¹ä¸­åµŒå…¥çš„å¾®å¦™æ–‡åŒ–ã€æƒ…æ„Ÿå’Œä¸Šä¸‹æ–‡å«ä¹‰ã€‚å°½ç®¡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰åœ¨åŸºæœ¬çš„è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å›¾åƒå«ä¹‰ä»»åŠ¡ä¸Šå´å­˜åœ¨åŸºæœ¬å±€é™ï¼šä¸Šä¸‹æ–‡ç¼ºå¤±å¯¼è‡´ä¸åŒè§†è§‰å…ƒç´ åŠå…¶æŠ½è±¡æ„ä¹‰ä¹‹é—´çš„å…³ç³»æ¨¡ç³Šä¸æ¸…ã€‚æˆ‘ä»¬å—åˆ°äººç±»è®¤çŸ¥è¿‡ç¨‹çš„å¯å‘ï¼Œæå‡ºäº†ä¸€ç§åä¸ºâ€œè®©å®‰å“åšæ¢¦â€ï¼ˆLADï¼‰çš„æ–°å‹å›¾åƒå«ä¹‰ç†è§£å’Œæ¨ç†æ¡†æ¶ã€‚LADé€šè¿‡ä¸‰ä¸ªé˜¶æ®µè§£å†³ä¸Šä¸‹æ–‡ç¼ºå¤±é—®é¢˜ï¼šï¼ˆ1ï¼‰æ„ŸçŸ¥ï¼šå°†è§†è§‰ä¿¡æ¯è½¬æ¢ä¸ºä¸°å¯Œä¸”å¤šå±‚æ¬¡çš„æ–‡æœ¬è¡¨ç¤ºï¼›ï¼ˆ2ï¼‰æœç´¢ï¼šè¿­ä»£æœç´¢å¹¶æ•´åˆè·¨åŸŸçŸ¥è¯†ä»¥è§£å†³æ­§ä¹‰ï¼›ï¼ˆ3ï¼‰æ¨ç†ï¼šé€šè¿‡æ˜ç¡®çš„æ¨ç†ç”Ÿæˆä¸ä¸Šä¸‹æ–‡å¯¹é½çš„å›¾åƒå«ä¹‰ã€‚æˆ‘ä»¬çš„æ¡†æ¶ä½¿ç”¨è½»é‡çº§çš„GPT-4o-miniæ¨¡å‹ï¼Œåœ¨è‹±è¯­å›¾åƒå«ä¹‰åŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½è¾¾åˆ°æœ€ä½³æ°´å¹³ï¼Œå¹¶åœ¨ä¸­æ–‡åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†å·¨å¤§æ”¹è¿›ã€‚åœ¨å¤šé€‰é¢˜æ–¹é¢ï¼Œå…¶è¡¨ç°ä¸GPT-4oæ¨¡å‹ç›¸å½“ï¼›åœ¨å¼€æ”¾æ€§é—®é¢˜æ–¹é¢çš„å¾—åˆ†é«˜å‡º36.7%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„å·¥ä½œæä¾›äº†æ–°çš„è§è§£ï¼Œæ¢è®¨äººå·¥æ™ºèƒ½å¦‚ä½•æ›´æœ‰æ•ˆåœ°è§£é‡Šå›¾åƒå«ä¹‰ï¼Œæ¨åŠ¨äº†è§†è§‰è¯­è¨€æ¨ç†å’Œäººæœºäº¤äº’é¢†åŸŸçš„å‘å±•ã€‚æˆ‘ä»¬çš„é¡¹ç›®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/MING-ZCH/Let-Androids-Dream-of-Electric-Sheep%E4%B8%8A%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/MING-ZCH/Let-Androids-Dream-of-Electric-Sheepä¸Šå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17019v1">PDF</a> 16 pages, 9 figures. Code &amp; Dataset:   <a target="_blank" rel="noopener" href="https://github.com/MING-ZCH/Let-Androids-Dream-of-Electric-Sheep">https://github.com/MING-ZCH/Let-Androids-Dream-of-Electric-Sheep</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æŒ‡å‡ºäººå·¥æ™ºèƒ½ç³»ç»Ÿåœ¨ç†è§£å›¾åƒéšå–»æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œç°æœ‰æ¨¡å‹éš¾ä»¥æŠŠæ¡è§†è§‰å†…å®¹ä¸­çš„æ–‡åŒ–ã€æƒ…æ„Ÿå’Œä¸Šä¸‹æ–‡å«ä¹‰ã€‚å—äººç±»è®¤çŸ¥è¿‡ç¨‹çš„å¯å‘ï¼Œæå‡ºäº†ä¸€ç§åä¸ºLet Androids Dreamï¼ˆLADï¼‰çš„æ–°å‹å›¾åƒéšå«ç†è§£å’Œæ¨ç†æ¡†æ¶ã€‚LADé€šè¿‡ä¸‰ä¸ªé˜¶æ®µè§£å†³ä¸Šä¸‹æ–‡ç¼ºå¤±é—®é¢˜ï¼šæ„ŸçŸ¥é˜¶æ®µå°†è§†è§‰ä¿¡æ¯è½¬æ¢ä¸ºä¸°å¯Œä¸”å¤šå±‚æ¬¡çš„æ–‡æœ¬è¡¨ç¤ºï¼›æœç´¢é˜¶æ®µè¿­ä»£åœ°æœç´¢å’Œæ•´åˆè·¨åŸŸçŸ¥è¯†ä»¥è§£å†³æ­§ä¹‰ï¼›æ¨ç†é˜¶æ®µé€šè¿‡æ˜ç¡®çš„æ¨ç†ç”Ÿæˆä¸ä¸Šä¸‹æ–‡å¯¹é½çš„å›¾åƒå«ä¹‰ã€‚LADæ¡†æ¶ä¸è½»é‡çº§GPT-4o-miniæ¨¡å‹ç›¸ç»“åˆï¼Œåœ¨è‹±è¯­å›¾åƒéšå«åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€æ–°æ€§èƒ½ï¼Œå¹¶åœ¨ä¸­æ–‡åŸºå‡†æµ‹è¯•ä¸Šæœ‰æ˜¾è‘—æ”¹å–„ï¼Œåœ¨å¤šé€‰å’Œå¼€æ”¾é£æ ¼é—®é¢˜ä¸Šçš„è¡¨ç°ä»¤äººå°è±¡æ·±åˆ»ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶ä¸ºAIæ›´æœ‰æ•ˆåœ°è§£é‡Šå›¾åƒå«ä¹‰æä¾›äº†æ–°è§è§£ï¼Œæ¨åŠ¨äº†è§†è§‰è¯­è¨€æ¨ç†å’Œäººæœºäº¤äº’é¢†åŸŸçš„å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AIç³»ç»Ÿåœ¨ç†è§£å›¾åƒéšå–»æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå°¤å…¶æ˜¯åœ¨æ•æ‰æ–‡åŒ–ã€æƒ…æ„Ÿå’Œä¸Šä¸‹æ–‡å«ä¹‰æ–¹é¢ã€‚</li>
<li>Let Androids Dreamï¼ˆLADï¼‰æ¡†æ¶æ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ï¼Œé€šè¿‡ä¸‰ä¸ªé˜¶æ®µå¤„ç†å›¾åƒéšå«ç†è§£å’Œæ¨ç†ã€‚</li>
<li>LADæ¡†æ¶åŒ…æ‹¬æ„ŸçŸ¥é˜¶æ®µã€æœç´¢é˜¶æ®µå’Œæ¨ç†é˜¶æ®µï¼Œåˆ†åˆ«è´Ÿè´£å°†è§†è§‰ä¿¡æ¯è½¬æ¢ä¸ºæ–‡æœ¬è¡¨ç¤ºã€æœç´¢å’Œæ•´åˆè·¨åŸŸçŸ¥è¯†ä»¥åŠç”Ÿæˆä¸Šä¸‹æ–‡å¯¹é½çš„å›¾åƒå«ä¹‰ã€‚</li>
<li>LADæ¡†æ¶ä¸è½»é‡çº§GPT-4o-miniæ¨¡å‹ç»“åˆï¼Œåœ¨è‹±è¯­å›¾åƒéšå«åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå¹¶åœ¨ä¸­æ–‡åŸºå‡†æµ‹è¯•ä¸Šæœ‰æ˜¾è‘—æ”¹å–„ã€‚</li>
<li>LADåœ¨å¤šé€‰å’Œå¼€æ”¾é£æ ¼é—®é¢˜ä¸Šçš„è¡¨ç°çªå‡ºã€‚<br>6.è¯¥ç ”ç©¶ä¸ºAIæ›´æœ‰æ•ˆåœ°è§£é‡Šå›¾åƒå«ä¹‰æä¾›äº†æ–°è§è§£ï¼Œæœ‰åŠ©äºæ¨åŠ¨è§†è§‰è¯­è¨€æ¨ç†é¢†åŸŸçš„å‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17019">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-a4974cfddf43e33a51124be9b7f934ac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b3b22f11c1a73d19d5166f8faf4b7e1a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-612c734a99dd1fec3a0ba7edf415c57a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b5473767b43f0eab8eb0005d6a4d19c1.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="SophiaVL-R1-Reinforcing-MLLMs-Reasoning-with-Thinking-Reward"><a href="#SophiaVL-R1-Reinforcing-MLLMs-Reasoning-with-Thinking-Reward" class="headerlink" title="SophiaVL-R1: Reinforcing MLLMs Reasoning with Thinking Reward"></a>SophiaVL-R1: Reinforcing MLLMs Reasoning with Thinking Reward</h2><p><strong>Authors:Kaixuan Fan, Kaituo Feng, Haoming Lyu, Dongzhan Zhou, Xiangyu Yue</strong></p>
<p>Recent advances have shown success in eliciting strong reasoning abilities in multimodal large language models (MLLMs) through rule-based reinforcement learning (RL) with outcome rewards. However, this paradigm typically lacks supervision over the thinking process leading to the final outcome.As a result, the model may learn sub-optimal reasoning strategies, which can hinder its generalization ability. In light of this, we propose SophiaVL-R1, as an attempt to add reward signals for the thinking process in this paradigm. To achieve this, we first train a thinking reward model that evaluates the quality of the entire thinking process. Given that the thinking reward may be unreliable for certain samples due to reward hacking, we propose the Trust-GRPO method, which assigns a trustworthiness weight to the thinking reward during training. This weight is computed based on the thinking reward comparison of responses leading to correct answers versus incorrect answers, helping to mitigate the impact of potentially unreliable thinking rewards. Moreover, we design an annealing training strategy that gradually reduces the thinking reward over time, allowing the model to rely more on the accurate rule-based outcome reward in later training stages. Experiments show that our SophiaVL-R1 surpasses a series of reasoning MLLMs on various benchmarks (e.g., MathVisita, MMMU), demonstrating strong reasoning and generalization capabilities. Notably, our SophiaVL-R1-7B even outperforms LLaVA-OneVision-72B on most benchmarks, despite the latter having 10 times more parameters. All code, models, and datasets are made publicly available at <a target="_blank" rel="noopener" href="https://github.com/kxfan2002/SophiaVL-R1">https://github.com/kxfan2002/SophiaVL-R1</a>. </p>
<blockquote>
<p>æœ€è¿‘çš„ç ”ç©¶è¿›å±•è¡¨æ˜ï¼Œé€šè¿‡åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸ç»“æœå¥–åŠ±ç›¸ç»“åˆï¼Œå¯ä»¥åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸­æ¿€å‘å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™ç§èŒƒå¼é€šå¸¸ç¼ºä¹å¯¹æœ€ç»ˆç»“æœçš„æ€è€ƒè¿‡ç¨‹çš„ç›‘ç£ã€‚å› æ­¤ï¼Œæ¨¡å‹å¯èƒ½ä¼šå­¦ä¹ æ¬¡ä¼˜çš„æ¨ç†ç­–ç•¥ï¼Œè¿™å¯èƒ½ä¼šé˜»ç¢å…¶æ³›åŒ–èƒ½åŠ›ã€‚é‰´äºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†SophiaVL-R1ï¼Œè¯•å›¾åœ¨æ­¤èŒƒå¼ä¸­ä¸ºæ€è€ƒè¿‡ç¨‹å¢åŠ å¥–åŠ±ä¿¡å·ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬é¦–å…ˆè®­ç»ƒäº†ä¸€ä¸ªæ€è€ƒå¥–åŠ±æ¨¡å‹ï¼Œè¯¥æ¨¡å‹è¯„ä¼°æ•´ä¸ªæ€è€ƒè¿‡ç¨‹çš„è´¨é‡ã€‚ç”±äºæŸäº›æ ·æœ¬çš„æ€è€ƒå¥–åŠ±å¯èƒ½å­˜åœ¨å› å¥–åŠ±ä½œå¼Šè€Œä¸å¯é çš„æƒ…å†µï¼Œæˆ‘ä»¬æå‡ºäº†Trust-GRPOæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸ºæ€è€ƒå¥–åŠ±åˆ†é…å¯ä¿¡åº¦æƒé‡ã€‚æ­¤æƒé‡æ˜¯åŸºäºæ­£ç¡®ç­”æ¡ˆä¸é”™è¯¯ç­”æ¡ˆå¯¼è‡´çš„æ€è€ƒå¥–åŠ±çš„å¯¹æ¯”è®¡ç®—å¾—å‡ºçš„ï¼Œæœ‰åŠ©äºç¼“è§£æ½œåœ¨çš„ä¸å¯é æ€è€ƒå¥–åŠ±çš„å½±å“ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§é€€ç«è®­ç»ƒç­–ç•¥ï¼Œéšç€æ—¶é—´çš„æ¨ç§»é€æ¸é™ä½æ€è€ƒå¥–åŠ±ï¼Œä½¿æ¨¡å‹åœ¨åæœŸè®­ç»ƒé˜¶æ®µæ›´å¤šåœ°ä¾èµ–äºå‡†ç¡®çš„åŸºäºè§„åˆ™çš„æˆæœå¥–åŠ±ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„SophiaVL-R1åœ¨å„ç§åŸºå‡†æµ‹è¯•ï¼ˆä¾‹å¦‚MathVisitaã€MMMUï¼‰ä¸Šçš„è¡¨ç°è¶…è¶Šäº†ä¸€ç³»åˆ—æ¨ç†MLLMsï¼Œæ˜¾ç¤ºå‡ºå¼ºå¤§çš„æ¨ç†å’Œæ³›åŒ–èƒ½åŠ›ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„SophiaVL-R1-7Bç”šè‡³åœ¨å¤§å¤šæ•°åŸºå‡†æµ‹è¯•ä¸Šè¶…è¿‡äº†å‚æ•°å¤šåå€çš„LLaVA-OneVision-72Bã€‚æ‰€æœ‰ä»£ç ã€æ¨¡å‹å’Œæ•°æ®é›†å‡å…¬å¼€å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/kxfan2002/SophiaVL-R1%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/kxfan2002/SophiaVL-R1è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17018v1">PDF</a> Project page:<a target="_blank" rel="noopener" href="https://github.com/kxfan2002/SophiaVL-R1">https://github.com/kxfan2002/SophiaVL-R1</a></p>
<p><strong>Summary</strong></p>
<p>åŸºäºè§„åˆ™å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸­å¼•å…¥æ¨ç†èƒ½åŠ›çš„æ–¹æ³•å·²å–å¾—æˆåŠŸï¼Œä½†è¿™ç§æ–¹æ³•ç¼ºä¹å¯¹æ€è€ƒè¿‡ç¨‹çš„ç›‘ç£ï¼Œå¯èƒ½å¯¼è‡´æ¨¡å‹å­¦ä¹ æ¬¡ä¼˜æ¨ç†ç­–ç•¥ï¼Œä»è€Œå½±å“å…¶æ³›åŒ–èƒ½åŠ›ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SophiaVL-R1æ–¹æ³•ï¼Œä¸ºè¿™ä¸€èŒƒå¼å¢åŠ æ€è€ƒè¿‡ç¨‹çš„å¥–åŠ±ä¿¡å·ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªæ€è€ƒå¥–åŠ±æ¨¡å‹ï¼Œç”¨äºè¯„ä¼°æ•´ä¸ªæ€è€ƒè¿‡ç¨‹çš„è´¨é‡ã€‚é’ˆå¯¹æŸäº›æ ·æœ¬ä¸­æ€è€ƒå¥–åŠ±å¯èƒ½å­˜åœ¨çš„ä¸å¯é é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Trust-GRPOæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸ºæ€è€ƒå¥–åŠ±åˆ†é…å¯ä¿¡åº¦æƒé‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è®¾è®¡äº†é€€ç«è®­ç»ƒç­–ç•¥ï¼Œéšç€æ—¶é—´çš„æ¨ç§»é€æ¸é™ä½æ€è€ƒå¥–åŠ±ï¼Œä½¿æ¨¡å‹åœ¨åæœŸæ›´å¤šåœ°ä¾èµ–äºå‡†ç¡®çš„è§„åˆ™ç»“æœå¥–åŠ±ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„SophiaVL-R1åœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸Šè¶…è¶Šäº†å¤šä¸ªæ¨ç†MLLMsï¼Œå±•ç°å‡ºå¼ºå¤§çš„æ¨ç†å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é€šè¿‡è§„åˆ™å¼ºåŒ–å­¦ä¹ å¼•å…¥æ¨ç†èƒ½åŠ›åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸­å–å¾—è¿›å±•ã€‚</li>
<li>ç¼ºä¹æ€è€ƒè¿‡ç¨‹ç›‘ç£å¯èƒ½å¯¼è‡´æ¨¡å‹å­¦ä¹ æ¬¡ä¼˜æ¨ç†ç­–ç•¥ã€‚</li>
<li>SophiaVL-R1æ—¨åœ¨æ·»åŠ æ€è€ƒè¿‡ç¨‹çš„å¥–åŠ±ä¿¡å·ä»¥æ”¹è¿›æ­¤é—®é¢˜ã€‚</li>
<li>è®¾è®¡äº†æ€è€ƒå¥–åŠ±æ¨¡å‹ä»¥è¯„ä¼°æ€è€ƒè¿‡ç¨‹è´¨é‡ã€‚</li>
<li>é’ˆå¯¹æ€è€ƒå¥–åŠ±å¯èƒ½å­˜åœ¨çš„ä¸å¯é æ€§ï¼Œæå‡ºäº†Trust-GRPOæ–¹æ³•æ¥åˆ†é…å¯ä¿¡åº¦æƒé‡ã€‚</li>
<li>é‡‡ç”¨é€€ç«è®­ç»ƒç­–ç•¥æ¥å¹³è¡¡æ€è€ƒå’Œç»“æœå¥–åŠ±çš„ä¾èµ–åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17018">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-4287b1bfc30ece86e036dbb8fde1ef33.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7e665fbbf39162732e4348f1dcd9579d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-72f4342affed04709864e9b7390db021.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1bcbf40ee948fed70ec37f9ade0c441c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8584e75a82a1202b3a02313748e43f30.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Delving-into-RL-for-Image-Generation-with-CoT-A-Study-on-DPO-vs-GRPO"><a href="#Delving-into-RL-for-Image-Generation-with-CoT-A-Study-on-DPO-vs-GRPO" class="headerlink" title="Delving into RL for Image Generation with CoT: A Study on DPO vs. GRPO"></a>Delving into RL for Image Generation with CoT: A Study on DPO vs. GRPO</h2><p><strong>Authors:Chengzhuo Tong, Ziyu Guo, Renrui Zhang, Wenyu Shan, Xinyu Wei, Zhenghao Xing, Hongsheng Li, Pheng-Ann Heng</strong></p>
<p>Recent advancements underscore the significant role of Reinforcement Learning (RL) in enhancing the Chain-of-Thought (CoT) reasoning capabilities of large language models (LLMs). Two prominent RL algorithms, Direct Preference Optimization (DPO) and Group Relative Policy Optimization (GRPO), are central to these developments, showcasing different pros and cons. Autoregressive image generation, also interpretable as a sequential CoT reasoning process, presents unique challenges distinct from LLM-based CoT reasoning. These encompass ensuring text-image consistency, improving image aesthetic quality, and designing sophisticated reward models, rather than relying on simpler rule-based rewards. While recent efforts have extended RL to this domain, these explorations typically lack an in-depth analysis of the domain-specific challenges and the characteristics of different RL strategies. To bridge this gap, we provide the first comprehensive investigation of the GRPO and DPO algorithms in autoregressive image generation, evaluating their in-domain performance and out-of-domain generalization, while scrutinizing the impact of different reward models on their respective capabilities. Our findings reveal that GRPO and DPO exhibit distinct advantages, and crucially, that reward models possessing stronger intrinsic generalization capabilities potentially enhance the generalization potential of the applied RL algorithms. Furthermore, we systematically explore three prevalent scaling strategies to enhance both their in-domain and out-of-domain proficiency, deriving unique insights into efficiently scaling performance for each paradigm. We hope our study paves a new path for inspiring future work on developing more effective RL algorithms to achieve robust CoT reasoning in the realm of autoregressive image generation. Code is released at <a target="_blank" rel="noopener" href="https://github.com/ZiyuGuo99/Image-Generation-CoT">https://github.com/ZiyuGuo99/Image-Generation-CoT</a> </p>
<blockquote>
<p>è¿‘æœŸè¿›å±•çªæ˜¾å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†èƒ½åŠ›æ–¹é¢çš„é‡è¦ä½œç”¨ã€‚ä¸¤ç§ä¸»è¦çš„RLç®—æ³•â€”â€”ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å’Œç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰â€”â€”æ˜¯è¿™äº›å‘å±•çš„æ ¸å¿ƒï¼Œå±•ç¤ºäº†å„è‡ªçš„ä¼˜åŠ¿å’ŒåŠ£åŠ¿ã€‚è‡ªåŠ¨å›å½’å›¾åƒç”Ÿæˆï¼Œä¹Ÿå¯è§£é‡Šä¸ºä¸€ç§è¿ç»­çš„CoTæ¨ç†è¿‡ç¨‹ï¼Œå‘ˆç°å‡ºä¸åŸºäºLLMçš„CoTæ¨ç†æˆªç„¶ä¸åŒçš„ç‹¬ç‰¹æŒ‘æˆ˜ã€‚è¿™äº›æŒ‘æˆ˜åŒ…æ‹¬ç¡®ä¿æ–‡æœ¬ä¸å›¾åƒçš„ä¸€è‡´æ€§ã€æé«˜å›¾åƒçš„å®¡ç¾è´¨é‡ï¼Œä»¥åŠè®¾è®¡ç²¾å¯†çš„å¥–åŠ±æ¨¡å‹ï¼Œè€Œä¸æ˜¯ä¾èµ–æ›´ç®€å•çš„åŸºäºè§„åˆ™çš„å¥–åŠ±ã€‚å°½ç®¡æœ€è¿‘çš„åŠªåŠ›å·²å°†RLæ‰©å±•åˆ°æ­¤é¢†åŸŸï¼Œä½†è¿™äº›æ¢ç´¢é€šå¸¸ç¼ºä¹å¯¹ç‰¹å®šé¢†åŸŸæŒ‘æˆ˜çš„æ·±å…¥åˆ†æä»¥åŠå¯¹ä¸åŒRLç­–ç•¥çš„ç‰¹ç‚¹çš„ç ”ç©¶ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¯¹è‡ªåŠ¨å›å½’å›¾åƒç”Ÿæˆä¸­çš„GRPOå’ŒDPOç®—æ³•è¿›è¡Œäº†é¦–æ¬¡å…¨é¢è°ƒæŸ¥ï¼Œè¯„ä¼°äº†å®ƒä»¬çš„é¢†åŸŸå†…æ€§èƒ½å’Œè·¨é¢†åŸŸæ³›åŒ–èƒ½åŠ›ï¼ŒåŒæ—¶ä»”ç»†ç ”ç©¶äº†ä¸åŒå¥–åŠ±æ¨¡å‹å¯¹å…¶å„è‡ªèƒ½åŠ›çš„å½±å“ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼ŒGRPOå’ŒDPOå…·æœ‰å„è‡ªç‹¬ç‰¹çš„ä¼˜åŠ¿ï¼Œå¹¶ä¸”é‡è¦çš„æ˜¯ï¼Œå…·æœ‰æ›´å¼ºå†…åœ¨æ³›åŒ–èƒ½åŠ›çš„å¥–åŠ±æ¨¡å‹å¯èƒ½æé«˜äº†æ‰€åº”ç”¨RLç®—æ³•çš„æ³›åŒ–æ½œåŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°æ¢ç´¢äº†ä¸‰ç§æµè¡Œçš„æ‰©å±•ç­–ç•¥ï¼Œä»¥æé«˜å®ƒä»¬çš„é¢†åŸŸå†…å’Œè·¨é¢†åŸŸç†Ÿç»ƒç¨‹åº¦ï¼Œä¸ºæ¯ç§èŒƒå¼æœ‰æ•ˆåœ°æ‰©å±•æ€§èƒ½æä¾›äº†ç‹¬ç‰¹çš„è§è§£ã€‚æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„ç ”ç©¶èƒ½ä¸ºæœªæ¥å¼€å‘æ›´æœ‰æ•ˆçš„RLç®—æ³•ä»¥å®ç°è‡ªåŠ¨å›å½’å›¾åƒç”Ÿæˆé¢†åŸŸçš„ç¨³å¥CoTæ¨ç†é“ºå¹³é“è·¯ã€‚ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/ZiyuGuo99/Image-Generation-CoT%E3%80%82">https://github.com/ZiyuGuo99/Image-Generation-CoTã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17017v1">PDF</a> Code is released at <a target="_blank" rel="noopener" href="https://github.com/ZiyuGuo99/Image-Generation-CoT">https://github.com/ZiyuGuo99/Image-Generation-CoT</a></p>
<p><strong>Summary</strong><br>åœ¨æœ€æ–°è¿›å±•ä¸­ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†èƒ½åŠ›æ–¹é¢æ‰®æ¼”ç€é‡è¦è§’è‰²ã€‚æœ¬æ–‡ä¸»è¦æ¢è®¨äº†Direct Preference Optimizationï¼ˆDPOï¼‰å’ŒGroup Relative Policy Optimizationï¼ˆGRPOï¼‰ä¸¤ç§ä¸»è¦çš„RLç®—æ³•åœ¨å¢å¼ºå›¾åƒç”Ÿæˆä¸­çš„è¡¨ç°å’Œåº”ç”¨å‰æ™¯ã€‚åˆ†æè¡¨æ˜ï¼ŒGRPOå’ŒDPOå„å…·ä¼˜åŠ¿ï¼Œè€Œå…·å¤‡æ›´å¼ºå†…åœ¨æ³›åŒ–èƒ½åŠ›çš„å¥–åŠ±æ¨¡å‹æœ‰åŠ©äºæé«˜åº”ç”¨RLç®—æ³•çš„æ³›åŒ–æ½œåŠ›ã€‚æœ¬æ–‡è¿˜ç³»ç»Ÿåœ°æ¢è®¨äº†ä¸‰ç§å¸¸è§çš„æ‰©å±•ç­–ç•¥ï¼Œä»¥æé«˜å®ƒä»¬çš„é¢†åŸŸå†…å¤–è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†èƒ½åŠ›æ–¹é¢æ‰®æ¼”é‡è¦è§’è‰²ã€‚</li>
<li>Direct Preference Optimizationï¼ˆDPOï¼‰å’ŒGroup Relative Policy Optimizationï¼ˆGRPOï¼‰æ˜¯æå‡é“¾å¼æ€ç»´èƒ½åŠ›çš„å…³é”®RLç®—æ³•ã€‚</li>
<li>GRPOå’ŒDPOåœ¨å›¾åƒç”Ÿæˆé¢†åŸŸå…·æœ‰ä¸åŒçš„ä¼˜åŠ¿ã€‚</li>
<li>å…·å¤‡æ›´å¼ºå†…åœ¨æ³›åŒ–èƒ½åŠ›çš„å¥–åŠ±æ¨¡å‹æœ‰åŠ©äºæé«˜RLç®—æ³•çš„æ³›åŒ–æ½œåŠ›ã€‚</li>
<li>ä¸‰ç§å¸¸è§çš„æ‰©å±•ç­–ç•¥å¯ä»¥æé«˜ç®—æ³•åœ¨é¢†åŸŸå†…çš„è¡¨ç°ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17017">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0875b114c3da56ff50622e225ff53df7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d2078dc50658837cf001220cb50c6cad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b2510c4e7adf79e2ddf6cf6ce5accac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9e4128c368775092031356c9fe7b433.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de52380d3e6be37cc816cc81b1cd5991.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="R1-Searcher-Incentivizing-the-Dynamic-Knowledge-Acquisition-of-LLMs-via-Reinforcement-Learning"><a href="#R1-Searcher-Incentivizing-the-Dynamic-Knowledge-Acquisition-of-LLMs-via-Reinforcement-Learning" class="headerlink" title="R1-Searcher++: Incentivizing the Dynamic Knowledge Acquisition of LLMs   via Reinforcement Learning"></a>R1-Searcher++: Incentivizing the Dynamic Knowledge Acquisition of LLMs   via Reinforcement Learning</h2><p><strong>Authors:Huatong Song, Jinhao Jiang, Wenqing Tian, Zhipeng Chen, Yuhuan Wu, Jiahao Zhao, Yingqian Min, Wayne Xin Zhao, Lei Fang, Ji-Rong Wen</strong></p>
<p>Large Language Models (LLMs) are powerful but prone to hallucinations due to static knowledge. Retrieval-Augmented Generation (RAG) helps by injecting external information, but current methods often are costly, generalize poorly, or ignore the internal knowledge of the model. In this paper, we introduce R1-Searcher++, a novel framework designed to train LLMs to adaptively leverage both internal and external knowledge sources. R1-Searcher++ employs a two-stage training strategy: an initial SFT Cold-start phase for preliminary format learning, followed by RL for Dynamic Knowledge Acquisition. The RL stage uses outcome-supervision to encourage exploration, incorporates a reward mechanism for internal knowledge utilization, and integrates a memorization mechanism to continuously assimilate retrieved information, thereby enriching the modelâ€™s internal knowledge. By leveraging internal knowledge and external search engine, the model continuously improves its capabilities, enabling efficient retrieval-augmented reasoning. Our experiments demonstrate that R1-Searcher++ outperforms previous RAG and reasoning methods and achieves efficient retrieval. The code is available at <a target="_blank" rel="noopener" href="https://github.com/RUCAIBox/R1-Searcher-plus">https://github.com/RUCAIBox/R1-Searcher-plus</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è™½ç„¶å¼ºå¤§ï¼Œä½†ç”±äºé™æ€çŸ¥è¯†å®¹æ˜“å‡ºç°å¹»è§‰ã€‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰é€šè¿‡æ³¨å…¥å¤–éƒ¨ä¿¡æ¯æ¥å¸®åŠ©è§£å†³è¿™ä¸€é—®é¢˜ï¼Œä½†å½“å‰çš„æ–¹æ³•å¾€å¾€æˆæœ¬é«˜æ˜‚ã€æ¨å¹¿æ€§å·®æˆ–å¿½ç•¥äº†æ¨¡å‹çš„å†…éƒ¨çŸ¥è¯†ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†R1-Searcher++ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ä»¥è‡ªé€‚åº”åœ°åˆ©ç”¨å†…éƒ¨å’Œå¤–éƒ¨çŸ¥è¯†æºã€‚R1-Searcher++é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼šåˆå§‹SFTå†·å¯åŠ¨é˜¶æ®µç”¨äºåˆæ­¥æ ¼å¼å­¦ä¹ ï¼Œå…¶æ¬¡æ˜¯ç”¨äºåŠ¨æ€çŸ¥è¯†è·å–çš„å¼ºåŒ–å­¦ä¹ ã€‚å¼ºåŒ–å­¦ä¹ é˜¶æ®µä½¿ç”¨ç»“æœç›‘ç£æ¥é¼“åŠ±æ¢ç´¢ï¼Œç»“åˆå†…éƒ¨çŸ¥è¯†åˆ©ç”¨çš„å¥–åŠ±æœºåˆ¶ï¼Œå¹¶æ•´åˆè®°å¿†æœºåˆ¶ä»¥æŒç»­åŒåŒ–æ£€ç´¢ä¿¡æ¯ï¼Œä»è€Œä¸°å¯Œæ¨¡å‹çš„å†…éƒ¨çŸ¥è¯†ã€‚é€šè¿‡åˆ©ç”¨å†…éƒ¨çŸ¥è¯†å’Œå¤–éƒ¨æœç´¢å¼•æ“ï¼Œæ¨¡å‹ä¸æ–­æ”¹å–„å…¶èƒ½åŠ›ï¼Œå®ç°é«˜æ•ˆçš„æ£€ç´¢å¢å¼ºæ¨ç†ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒR1-Searcher++ä¼˜äºå…ˆå‰çš„RAGå’Œæ¨ç†æ–¹æ³•ï¼Œå®ç°äº†é«˜æ•ˆçš„æ£€ç´¢ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/RUCAIBox/R1-Searcher-plus%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/RUCAIBox/R1-Searcher-plusæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17005v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å…·æœ‰å¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†æ˜“å—åˆ°é™æ€çŸ¥è¯†å¯¼è‡´çš„å¹»è§‰å½±å“ã€‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰é€šè¿‡æ³¨å…¥å¤–éƒ¨ä¿¡æ¯æ¥å¸®åŠ©è§£å†³è¿™ä¸€é—®é¢˜ï¼Œä½†ç°æœ‰æ–¹æ³•å¾€å¾€æˆæœ¬é«˜æ˜‚ã€é€šç”¨æ€§å·®æˆ–å¿½ç•¥æ¨¡å‹å†…éƒ¨çŸ¥è¯†ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹æ¡†æ¶R1-Searcher++ï¼Œæ—¨åœ¨è®­ç»ƒLLMsè‡ªé€‚åº”åœ°åˆ©ç”¨å†…éƒ¨å’Œå¤–éƒ¨çŸ¥è¯†æºã€‚R1-Searcher++é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼šåˆå§‹SFT Cold-starté˜¶æ®µç”¨äºåˆæ­¥æ ¼å¼å­¦ä¹ ï¼Œç„¶åæ˜¯ç”¨äºåŠ¨æ€çŸ¥è¯†è·å–çš„RLé˜¶æ®µã€‚RLé˜¶æ®µåˆ©ç”¨ç»“æœç›‘ç£æ¥é¼“åŠ±æ¢ç´¢ï¼Œå¼•å…¥å†…éƒ¨çŸ¥è¯†åˆ©ç”¨çš„å¥–åŠ±æœºåˆ¶ï¼Œå¹¶é›†æˆè®°å¿†æœºåˆ¶ä»¥æŒç»­åŒåŒ–æ£€ç´¢çš„ä¿¡æ¯ï¼Œä»è€Œä¸°å¯Œæ¨¡å‹çš„å†…éƒ¨çŸ¥è¯†ã€‚é€šè¿‡åˆ©ç”¨å†…éƒ¨çŸ¥è¯†å’Œå¤–éƒ¨æœç´¢å¼•æ“ï¼Œæ¨¡å‹ä¸æ–­æå‡å…¶èƒ½åŠ›ï¼Œå®ç°é«˜æ•ˆçš„æ£€ç´¢å¢å¼ºæ¨ç†ã€‚å®éªŒè¡¨æ˜ï¼ŒR1-Searcher++ä¼˜äºå…ˆå‰çš„RAGå’Œæ¨ç†æ–¹æ³•ï¼Œå®ç°äº†é«˜æ•ˆçš„æ£€ç´¢ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å­˜åœ¨ç”±äºé™æ€çŸ¥è¯†å¯¼è‡´çš„å¹»è§‰é—®é¢˜ã€‚</li>
<li>æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰é€šè¿‡æ³¨å…¥å¤–éƒ¨ä¿¡æ¯æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>R1-Searcher++æ˜¯ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨è®­ç»ƒLLMsè‡ªé€‚åº”åˆ©ç”¨å†…éƒ¨å’Œå¤–éƒ¨çŸ¥è¯†æºã€‚</li>
<li>R1-Searcher++é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼šåˆå§‹é˜¶æ®µè¿›è¡Œæ ¼å¼å­¦ä¹ ï¼Œç„¶åæ˜¯åŠ¨æ€çŸ¥è¯†è·å–çš„å¼ºåŒ–å­¦ä¹ é˜¶æ®µã€‚</li>
<li>RLé˜¶æ®µé‡‡ç”¨ç»“æœç›‘ç£é¼“åŠ±æ¢ç´¢ï¼Œå¹¶å¼•å…¥å†…éƒ¨çŸ¥è¯†åˆ©ç”¨çš„å¥–åŠ±æœºåˆ¶å’Œè®°å¿†æœºåˆ¶ã€‚</li>
<li>R1-Searcher++é€šè¿‡ç»“åˆå†…éƒ¨çŸ¥è¯†å’Œå¤–éƒ¨æœç´¢å¼•æ“ï¼Œå®ç°äº†é«˜æ•ˆçš„æ£€ç´¢å¢å¼ºæ¨ç†ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒR1-Searcher++åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†å…ˆå‰çš„RAGå’Œæ¨ç†æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17005">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2732a6aa7ef87d646c961d16a4ae9f1f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48d8c000f3f83bff59d7c9375e186045.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Do-Large-Language-Models-Excel-in-Complex-Logical-Reasoning-with-Formal-Language"><a href="#Do-Large-Language-Models-Excel-in-Complex-Logical-Reasoning-with-Formal-Language" class="headerlink" title="Do Large Language Models Excel in Complex Logical Reasoning with Formal   Language?"></a>Do Large Language Models Excel in Complex Logical Reasoning with Formal   Language?</h2><p><strong>Authors:Jin Jiang, Jianing Wang, Yuchen Yan, Yang Liu, Jianhua Zhu, Mengdi Zhang, Xunliang Cai, Liangcai Gao</strong></p>
<p>Large Language Models (LLMs) have been shown to achieve breakthrough performance on complex logical reasoning tasks. Nevertheless, most existing research focuses on employing formal language to guide LLMs to derive reliable reasoning paths, while systematic evaluations of these capabilities are still limited. In this paper, we aim to conduct a comprehensive evaluation of LLMs across various logical reasoning problems utilizing formal languages. From the perspective of three dimensions, i.e., spectrum of LLMs, taxonomy of tasks, and format of trajectories, our key findings are: 1) Thinking models significantly outperform Instruct models, especially when formal language is employed; 2) All LLMs exhibit limitations in inductive reasoning capability, irrespective of whether they use a formal language; 3) Data with PoT format achieves the best generalization performance across other languages. Additionally, we also curate the formal-relative training data to further enhance the small language models, and the experimental results indicate that a simple rejected fine-tuning method can better enable LLMs to generalize across formal languages and achieve the best overall performance. Our codes and reports are available at <a target="_blank" rel="noopener" href="https://github.com/jiangjin1999/FormalEval">https://github.com/jiangjin1999/FormalEval</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚çš„é€»è¾‘æ¨ç†ä»»åŠ¡ä¸­å–å¾—äº†çªç ´æ€§çš„è¡¨ç°ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤§å¤šæ•°ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ä½¿ç”¨æ­£å¼è¯­è¨€æ¥æŒ‡å¯¼LLMsæ¨å¯¼å¯é çš„æ¨ç†è·¯å¾„ï¼Œè€Œå¯¹äºè¿™äº›èƒ½åŠ›çš„ç³»ç»Ÿè¯„ä»·ä»ç„¶æœ‰é™ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ—¨åœ¨åˆ©ç”¨æ­£å¼è¯­è¨€ï¼Œå¯¹å„ç§é€»è¾‘æ¨ç†é—®é¢˜å¯¹LLMsè¿›è¡Œå…¨é¢è¯„ä¼°ã€‚ä»ä¸‰ä¸ªç»´åº¦æ¥çœ‹ï¼Œå³LLMçš„é¢‘è°±ã€ä»»åŠ¡åˆ†ç±»å’Œè½¨è¿¹æ ¼å¼ï¼Œæˆ‘ä»¬çš„ä¸»è¦å‘ç°å¦‚ä¸‹ï¼š1ï¼‰æ€ç»´æ¨¡å‹åœ¨é‡‡ç”¨æ­£å¼è¯­è¨€æ—¶æ˜¾è‘—ä¼˜äºæŒ‡ä»¤æ¨¡å‹ï¼›2ï¼‰æ— è®ºæ˜¯å¦ä½¿ç”¨æ­£å¼è¯­è¨€ï¼Œæ‰€æœ‰LLMåœ¨å½’çº³æ¨ç†èƒ½åŠ›æ–¹é¢éƒ½è¡¨ç°å‡ºå±€é™æ€§ï¼›3ï¼‰é‡‡ç”¨PoTæ ¼å¼çš„æ•°æ®åœ¨å…¶ä»–è¯­è¨€ä¸­å®ç°äº†æœ€ä½³æ³›åŒ–æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ•´ç†äº†ä¸å½¢å¼ç›¸å…³çš„è®­ç»ƒæ•°æ®ï¼Œä»¥è¿›ä¸€æ­¥å¢å¼ºå°å‹è¯­è¨€æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç®€å•çš„æ‹’ç»å¾®è°ƒæ–¹æ³•èƒ½æ›´å¥½åœ°ä½¿LLMåœ¨æ­£å¼è¯­è¨€ä¸Šæ³›åŒ–ï¼Œå¹¶è¾¾åˆ°æœ€ä½³çš„æ•´ä½“æ€§èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç å’ŒæŠ¥å‘Šå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/jiangjin1999/FormalEval">https://github.com/jiangjin1999/FormalEval</a> ä¸­æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16998v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚çš„é€»è¾‘æ¨ç†ä»»åŠ¡ä¸­å–å¾—äº†çªç ´æ€§è¿›å±•ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰ç ”ç©¶ä¾§é‡äºä½¿ç”¨æ­£å¼è¯­è¨€æ¥æŒ‡å¯¼LLMså¾—å‡ºå¯é çš„æ¨ç†è·¯å¾„ï¼Œè€Œå¯¹è¿™äº›èƒ½åŠ›çš„ç³»ç»Ÿè¯„ä¼°ä»ç„¶æœ‰é™ã€‚æœ¬æ–‡æ—¨åœ¨ä»ä½¿ç”¨æ­£å¼è¯­è¨€çš„å¤šä¸ªç»´åº¦å…¨é¢è¯„ä¼°LLMsåœ¨å„ç§é€»è¾‘æ¨ç†é—®é¢˜ä¸Šçš„è¡¨ç°ã€‚æˆ‘ä»¬çš„å…³é”®å‘ç°åŒ…æ‹¬ï¼š1ï¼‰Thinkingæ¨¡å‹åœ¨é‡‡ç”¨æ­£å¼è¯­è¨€æ—¶æ˜¾è‘—ä¼˜äºInstructæ¨¡å‹ï¼›2ï¼‰æ‰€æœ‰LLMsåœ¨å½’çº³æ¨ç†èƒ½åŠ›ä¸Šå‡å­˜åœ¨å±€é™æ€§ï¼Œæ— è®ºæ˜¯å¦ä½¿ç”¨æ­£å¼è¯­è¨€ï¼›3ï¼‰é‡‡ç”¨PoTæ ¼å¼çš„æ•°æ®åœ¨å…¶ä»–è¯­è¨€ä¸Šå–å¾—äº†æœ€ä½³æ³›åŒ–æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ•´ç†äº†ä¸å½¢å¼ç›¸å…³çš„è®­ç»ƒæ•°æ®ï¼Œä»¥è¿›ä¸€æ­¥æé«˜å°å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç®€å•çš„æ‹’ç»å¾®è°ƒæ–¹æ³•èƒ½å¤Ÿæ›´å¥½åœ°ä½¿LLMsåœ¨æ­£å¼è¯­è¨€ä¸Šå®ç°æ³›åŒ–ï¼Œå¹¶è¾¾åˆ°æœ€ä½³çš„æ•´ä½“æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsåœ¨å¤æ‚çš„é€»è¾‘æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚</li>
<li>ç°æœ‰ç ”ç©¶ä¸»è¦å…³æ³¨ä½¿ç”¨æ­£å¼è¯­è¨€æ¥æŒ‡å¯¼LLMsè¿›è¡Œæ¨ç†ã€‚</li>
<li>Thinkingæ¨¡å‹åœ¨é‡‡ç”¨æ­£å¼è¯­è¨€æ—¶çš„æ€§èƒ½ä¼˜äºInstructæ¨¡å‹ã€‚</li>
<li>æ‰€æœ‰LLMsåœ¨å½’çº³æ¨ç†æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œæ— è®ºæ˜¯å¦ä½¿ç”¨æ­£å¼è¯­è¨€ã€‚</li>
<li>é‡‡ç”¨PoTæ ¼å¼çš„æ•°æ®åœ¨è·¨è¯­è¨€æ³›åŒ–æ–¹é¢è¡¨ç°æœ€ä½³ã€‚</li>
<li>æ•´ç†ä¸å½¢å¼ç›¸å…³çš„è®­ç»ƒæ•°æ®å¯è¿›ä¸€æ­¥æé«˜å°å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16998">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d3b6f7152f8566f1397b5e3609f61697.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ebc3f63e78d1e736de670f9896c1d4ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ecf7db8d3c6f1cf109663f117a4b2cea.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="UFT-Unifying-Supervised-and-Reinforcement-Fine-Tuning"><a href="#UFT-Unifying-Supervised-and-Reinforcement-Fine-Tuning" class="headerlink" title="UFT: Unifying Supervised and Reinforcement Fine-Tuning"></a>UFT: Unifying Supervised and Reinforcement Fine-Tuning</h2><p><strong>Authors:Mingyang Liu, Gabriele Farina, Asuman Ozdaglar</strong></p>
<p>Post-training has demonstrated its importance in enhancing the reasoning capabilities of large language models (LLMs). The primary post-training methods can be categorized into supervised fine-tuning (SFT) and reinforcement fine-tuning (RFT). SFT is efficient and well-suited for small language models, but it may lead to overfitting and limit the reasoning abilities of larger models. In contrast, RFT generally yields better generalization but depends heavily on the strength of the base model. To address the limitations of SFT and RFT, we propose Unified Fine-Tuning (UFT), a novel post-training paradigm that unifies SFT and RFT into a single, integrated process. UFT enables the model to effectively explore solutions while incorporating informative supervision signals, bridging the gap between memorizing and thinking underlying existing methods. Notably, UFT outperforms both SFT and RFT in general, regardless of model sizes. Furthermore, we theoretically prove that UFT breaks RFTâ€™s inherent exponential sample complexity bottleneck, showing for the first time that unified training can exponentially accelerate convergence on long-horizon reasoning tasks. </p>
<blockquote>
<p>è®­ç»ƒåçš„è¿‡ç¨‹åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›å¢å¼ºä¸­è¡¨ç°å‡ºäº†å…¶é‡è¦æ€§ã€‚ä¸»è¦çš„è®­ç»ƒåæ–¹æ³•å¯ä»¥åˆ†ä¸ºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰ã€‚SFTæ•ˆç‡é«˜ï¼Œéå¸¸é€‚åˆå°å‹è¯­è¨€æ¨¡å‹ï¼Œä½†å¯èƒ½ä¼šå¯¼è‡´è¿‡æ‹Ÿåˆï¼Œå¹¶é™åˆ¶å¤§å‹æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒRFTé€šå¸¸ä¼šäº§ç”Ÿæ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½†ä¸¥é‡ä¾èµ–äºåŸºç¡€æ¨¡å‹çš„å¼ºåº¦ã€‚ä¸ºäº†è§£å†³SFTå’ŒRFTçš„å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ç»Ÿä¸€å¾®è°ƒï¼ˆUFTï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„è®­ç»ƒåèŒƒå¼ï¼Œå®ƒå°†SFTå’ŒRFTç»Ÿä¸€åˆ°ä¸€ä¸ªå•ä¸€çš„é›†æˆè¿‡ç¨‹ä¸­ã€‚UFTä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨å¼•å…¥ä¿¡æ¯ç›‘ç£ä¿¡å·çš„åŒæ—¶æœ‰æ•ˆåœ°æ¢ç´¢è§£å†³æ–¹æ¡ˆï¼Œä»è€Œå¼¥è¡¥äº†ç°æœ‰æ–¹æ³•åœ¨è®°å¿†å’Œæ€è€ƒæ–¹é¢çš„å·®è·ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ— è®ºæ¨¡å‹è§„æ¨¡å¦‚ä½•ï¼ŒUFTé€šå¸¸éƒ½ä¼˜äºSFTå’ŒRFTã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä»ç†è®ºä¸Šè¯æ˜äº†UFTæ‰“ç ´äº†RFTå›ºæœ‰çš„æŒ‡æ•°æ ·æœ¬å¤æ‚æ€§ç“¶é¢ˆï¼Œé¦–æ¬¡å±•ç¤ºäº†ç»Ÿä¸€è®­ç»ƒå¯ä»¥åœ¨é•¿å‘¨æœŸæ¨ç†ä»»åŠ¡ä¸Šå®ç°æŒ‡æ•°çº§åŠ é€Ÿæ”¶æ•›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16984v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹çš„è®­ç»ƒåè¿‡ç¨‹å¯¹äºæå‡å…¶æ¨ç†èƒ½åŠ›è‡³å…³é‡è¦ï¼Œä¸»è¦æ–¹æ³•åŒ…æ‹¬ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰ã€‚SFTæ•ˆç‡é«˜ä¸”é€‚ç”¨äºå°å‹è¯­è¨€æ¨¡å‹ï¼Œä½†å¯èƒ½å¯¼è‡´è¿‡æ‹Ÿåˆå¹¶é™åˆ¶å¤§å‹æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚RFTåˆ™é€šå¸¸å…·æœ‰æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½†é«˜åº¦ä¾èµ–äºåŸºç¡€æ¨¡å‹çš„å¼ºåº¦ã€‚ä¸ºè§£å†³SFTå’ŒRFTçš„å±€é™æ€§ï¼Œæå‡ºäº†ç»Ÿä¸€å¾®è°ƒï¼ˆUFTï¼‰è¿™ä¸€æ–°å‹è®­ç»ƒåèŒƒå¼ï¼Œå®ƒå°†SFTå’ŒRFTç»Ÿä¸€åˆ°ä¸€ä¸ªé›†æˆè¿‡ç¨‹ä¸­ã€‚UFTä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨æ¢ç´¢è§£å†³æ–¹æ¡ˆçš„åŒæ—¶èå…¥ä¿¡æ¯ä¸°å¯Œçš„ç›‘ç£ä¿¡å·ï¼Œä»è€Œç¼©å°ç°æœ‰æ–¹æ³•è®°å¿†ä¸æ€ç»´ä¹‹é—´çš„é¸¿æ²Ÿã€‚UFTåœ¨æ¨¡å‹å¤§å°æ–¹é¢è¡¨ç°å‡ºä¼˜äºSFTå’ŒRFTçš„é€šç”¨æ€§ï¼Œå¹¶åœ¨ç†è®ºä¸Šçªç ´äº†RFTå›ºæœ‰çš„æŒ‡æ•°æ ·æœ¬å¤æ‚åº¦ç“¶é¢ˆï¼Œé¦–æ¬¡å±•ç¤ºäº†ç»Ÿä¸€è®­ç»ƒå¯ä»¥åŠ é€Ÿé•¿æœŸæ¨ç†ä»»åŠ¡çš„æ”¶æ•›é€Ÿåº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®­ç»ƒåè¿‡ç¨‹å¯¹äºæå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰æ–¹æ³•å„æœ‰ä¼˜ç¼ºç‚¹ã€‚</li>
<li>SFTæ•ˆç‡é«˜ä¸”é€‚ç”¨äºå°å‹è¯­è¨€æ¨¡å‹ï¼Œä½†å¯èƒ½è¿‡æ‹Ÿåˆå¤§å‹æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>RFTå…·æœ‰æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½†ä¾èµ–åŸºç¡€æ¨¡å‹çš„å¼ºåº¦ã€‚</li>
<li>ç»Ÿä¸€å¾®è°ƒï¼ˆUFTï¼‰æ˜¯ä¸€ç§æ–°å‹è®­ç»ƒåèŒƒå¼ï¼Œç»“åˆäº†SFTå’ŒRFTçš„ä¼˜ç‚¹ã€‚</li>
<li>UFTèƒ½å¤Ÿåœ¨æ¢ç´¢è§£å†³æ–¹æ¡ˆçš„åŒæ—¶èå…¥ç›‘ç£ä¿¡å·ï¼Œæé«˜æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16984">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4379c5fa8eb7e004538fae226f4e95be.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9a106eac80f07dce52fbf5765893a115.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d05654ae47aa06b8cb1eaeaca2523d74.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3b9e44acebac4a9b4c61ade8cead9f19.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6aa6d18cc54677c8a4db559a564f130b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c6e430c3af43aaf1df7d6f91e8fc7b4.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="SWE-Dev-Evaluating-and-Training-Autonomous-Feature-Driven-Software-Development"><a href="#SWE-Dev-Evaluating-and-Training-Autonomous-Feature-Driven-Software-Development" class="headerlink" title="SWE-Dev: Evaluating and Training Autonomous Feature-Driven Software   Development"></a>SWE-Dev: Evaluating and Training Autonomous Feature-Driven Software   Development</h2><p><strong>Authors:Yaxin Du, Yuzhu Cai, Yifan Zhou, Cheng Wang, Yu Qian, Xianghe Pang, Qian Liu, Yue Hu, Siheng Chen</strong></p>
<p>Large Language Models (LLMs) have shown strong capability in diverse software engineering tasks, e.g. code completion, bug fixing, and document generation. However, feature-driven development (FDD), a highly prevalent real-world task that involves developing new functionalities for large, existing codebases, remains underexplored. We therefore introduce SWE-Dev, the first large-scale dataset (with 14,000 training and 500 test samples) designed to evaluate and train autonomous coding systems on real-world feature development tasks. To ensure verifiable and diverse training, SWE-Dev uniquely provides all instances with a runnable environment and its developer-authored executable unit tests. This collection not only provides high-quality data for Supervised Fine-Tuning (SFT), but also enables Reinforcement Learning (RL) by delivering accurate reward signals from executable unit tests. Our extensive evaluations on SWE-Dev, covering 17 chatbot LLMs, 10 reasoning models, and 10 Multi-Agent Systems (MAS), reveal that FDD is a profoundly challenging frontier for current AI (e.g., Claude-3.7-Sonnet achieves only 22.45% Pass@3 on the hard test split). Crucially, we demonstrate that SWE-Dev serves as an effective platform for model improvement: fine-tuning on training set enabled a 7B model comparable to GPT-4o on \textit{hard} split, underscoring the value of its high-quality training data. Code is available here \href{<a target="_blank" rel="noopener" href="https://github.com/justLittleWhite/SWE-Dev%7D%7Bhttps://github.com/justLittleWhite/SWE-Dev%7D">https://github.com/justLittleWhite/SWE-Dev}{https://github.com/justLittleWhite/SWE-Dev}</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šç§è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä¾‹å¦‚ä»£ç è¡¥å…¨ã€æ•…éšœä¿®å¤å’Œæ–‡æ¡£ç”Ÿæˆã€‚ç„¶è€Œï¼ŒåŠŸèƒ½é©±åŠ¨å¼€å‘ï¼ˆFDDï¼‰æ˜¯ä¸€ä¸ªåœ¨ç°å®ä¸–ç•Œä»»åŠ¡ä¸­éå¸¸æ™®éçš„å­˜åœ¨ï¼Œå®ƒä¸ºå¤§å‹ç°æœ‰ä»£ç åº“å¼€å‘æ–°åŠŸèƒ½ï¼Œä»ç„¶è¢«ç ”ç©¶å¾—ä¸å¤Ÿæ·±å…¥ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†SWE-Devï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå¤§è§„æ¨¡æ•°æ®é›†ï¼ˆåŒ…å«14,000ä¸ªè®­ç»ƒæ ·æœ¬å’Œ500ä¸ªæµ‹è¯•æ ·æœ¬ï¼‰ï¼Œæ—¨åœ¨è¯„ä¼°å’ŒåŸ¹è®­ç”¨äºç°å®ä¸–ç•ŒåŠŸèƒ½å¼€å‘ä»»åŠ¡çš„è‡ªä¸»ç¼–ç ç³»ç»Ÿã€‚ä¸ºäº†ç¡®ä¿å¯éªŒè¯å’Œå¤šæ ·åŒ–çš„è®­ç»ƒï¼ŒSWE-Devç‹¬ç‰¹åœ°æä¾›äº†å¯è¿è¡Œçš„ç¯å¢ƒå’Œå¼€å‘äººå‘˜ç¼–å†™çš„å¯æ‰§è¡Œå•å…ƒæµ‹è¯•çš„æ‰€æœ‰å®ä¾‹ã€‚è¿™ä¸€é›†åˆä¸ä»…ä¸ºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æä¾›äº†é«˜è´¨é‡çš„æ•°æ®ï¼Œè€Œä¸”é€šè¿‡æä¾›å¯æ‰§è¡Œå•å…ƒæµ‹è¯•çš„ç²¾ç¡®å¥–åŠ±ä¿¡å·ï¼Œä½¿å¾—å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æˆä¸ºå¯èƒ½ã€‚æˆ‘ä»¬å¯¹SWE-Devè¿›è¡Œäº†å¹¿æ³›çš„è¯„ä¼°ï¼Œæ¶µç›–äº†17ä¸ªèŠå¤©æœºå™¨äººLLMã€10ä¸ªæ¨ç†æ¨¡å‹å’Œ10ä¸ªå¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰ï¼Œç»“æœè¡¨æ˜FDDæ˜¯å½“å‰AIçš„ä¸€ä¸ªæå…·æŒ‘æˆ˜æ€§çš„å‰æ²¿é¢†åŸŸï¼ˆä¾‹å¦‚ï¼ŒClaude-3.7-Sonnetåœ¨å›°éš¾æµ‹è¯•é›†ä¸Šä»…è¾¾åˆ°22.45%çš„é€šè¿‡ç‡ï¼‰ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬è¯æ˜äº†SWE-Devæ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„æ¨¡å‹æ”¹è¿›å¹³å°ï¼šåœ¨è®­ç»ƒé›†ä¸Šè¿›è¡Œå¾®è°ƒä½¿ä¸€ä¸ªä¸GPT-4oç›¸å½“çš„7Bæ¨¡å‹èƒ½å¤Ÿåœ¨å›°éš¾åˆ†å‰²ä¸Šå–å¾—è¿›å±•ï¼Œè¿™å‡¸æ˜¾äº†å…¶é«˜è´¨é‡è®­ç»ƒæ•°æ®çš„ä»·å€¼ã€‚ä»£ç å¯åœ¨ä»¥ä¸‹é“¾æ¥ä¸­æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/justLittleWhite/SWE-Dev">https://github.com/justLittleWhite/SWE-Dev</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16975v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è½¯ä»¶å·¥ç¨‹æŠ€æœ¯ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œå¦‚ä»£ç è¡¥å…¨ã€æ•…éšœä¿®å¤å’Œæ–‡æ¡£ç”Ÿæˆã€‚ä½†å¯¹äºç°å®ä¸–ç•Œä¸­æ¶‰åŠä¸ºå¤§å‹ç°æœ‰ä»£ç åº“å¼€å‘æ–°åŠŸèƒ½çš„åŠŸèƒ½é©±åŠ¨å¼€å‘ï¼ˆFDDï¼‰ä»»åŠ¡ä»ç„¶ç¼ºä¹æ¢ç´¢ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†SWE-Devï¼Œè¿™æ˜¯é¦–ä¸ªé’ˆå¯¹è‡ªä¸»ç¼–ç ç³»ç»Ÿåœ¨çœŸå®ä¸–ç•ŒåŠŸèƒ½å¼€å‘ä»»åŠ¡ä¸Šè¿›è¡Œè¯„ä¼°å’Œè®­ç»ƒçš„å¤§è§„æ¨¡æ•°æ®é›†ï¼ŒåŒ…å«14,000ä¸ªè®­ç»ƒæ ·æœ¬å’Œ500ä¸ªæµ‹è¯•æ ·æœ¬ã€‚SWE-Devçš„ç‹¬ç‰¹ä¹‹å¤„åœ¨äºä¸ºæ‰€æœ‰å®ä¾‹æä¾›äº†ä¸€ä¸ªå¯è¿è¡Œçš„ç¯å¢ƒå’Œå¼€å‘è€…ç¼–å†™çš„å¯æ‰§è¡Œå•å…ƒæµ‹è¯•ï¼Œç¡®ä¿äº†å¯éªŒè¯å’Œå¤šæ ·åŒ–çš„è®­ç»ƒã€‚æ­¤å¤–ï¼ŒSWE-Devä¸ä»…ä¸ºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æä¾›äº†é«˜è´¨é‡çš„æ•°æ®ï¼Œè€Œä¸”è¿˜é€šè¿‡æ¥è‡ªå¯æ‰§è¡Œå•å…ƒæµ‹è¯•çš„å‡†ç¡®å¥–åŠ±ä¿¡å·å®ç°äº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„å¯èƒ½ã€‚å¯¹ç°æœ‰æ¨¡å‹åœ¨SWE-Devä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒFDDæ˜¯å½“å‰AIçš„ä¸€ä¸ªæå…·æŒ‘æˆ˜æ€§çš„å‰æ²¿é¢†åŸŸã€‚åŒæ—¶ï¼Œæˆ‘ä»¬è¯æ˜äº†SWE-Devä½œä¸ºä¸€ä¸ªæœ‰æ•ˆå¹³å°çš„ä»·å€¼ï¼Œåœ¨è®­ç»ƒé›†ä¸Šè¿›è¡Œå¾®è°ƒä½¿ä¸€ä¸ªä¸GPT-4ç›¸å½“çš„7Bæ¨¡å‹åœ¨ç¡¬åˆ†å‰²ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è½¯ä»¶å·¥ç¨‹æŠ€æœ¯ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ã€‚</li>
<li>åŠŸèƒ½é©±åŠ¨å¼€å‘ï¼ˆFDDï¼‰æ˜¯ä¸€ä¸ªç°å®ä¸–ç•Œä¸­æ¶‰åŠå¤§å‹ä»£ç åº“çš„æ–°åŠŸèƒ½å¼€å‘ä»»åŠ¡ï¼Œç›®å‰è¢«å¿½è§†ã€‚</li>
<li>å¼•å…¥SWE-Devæ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°å’Œè®­ç»ƒè‡ªä¸»ç¼–ç ç³»ç»Ÿåœ¨çœŸå®ä¸–ç•ŒåŠŸèƒ½å¼€å‘ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚</li>
<li>SWE-Devæä¾›å¯è¿è¡Œç¯å¢ƒåŠå¼€å‘è€…ç¼–å†™çš„å¯æ‰§è¡Œå•å…ƒæµ‹è¯•ï¼Œç¡®ä¿è®­ç»ƒå’ŒéªŒè¯çš„å¤šæ ·æ€§å’Œå‡†ç¡®æ€§ã€‚</li>
<li>è¯¥æ•°æ®é›†æ”¯æŒç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œæä¾›é«˜è´¨é‡æ•°æ®å’Œå‡†ç¡®å¥–åŠ±ä¿¡å·ã€‚</li>
<li>FDDæ˜¯ä¸€ä¸ªå¯¹å½“å‰AIæå…·æŒ‘æˆ˜æ€§çš„é¢†åŸŸï¼Œç°æœ‰æ¨¡å‹è¡¨ç°æœ‰å¾…æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16975">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d76cc3c400d975b56cd8543e91547ee8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c98b6e58ef1ef24a3fe9676088b01589.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-51f5f922ff3920ef9b3be573ab26c22b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b6072b88a0691901156fdc4987a21f4d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6b49a1218b75ef16434a7543070c20c9.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="OpenSeg-R-Improving-Open-Vocabulary-Segmentation-via-Step-by-Step-Visual-Reasoning"><a href="#OpenSeg-R-Improving-Open-Vocabulary-Segmentation-via-Step-by-Step-Visual-Reasoning" class="headerlink" title="OpenSeg-R: Improving Open-Vocabulary Segmentation via Step-by-Step   Visual Reasoning"></a>OpenSeg-R: Improving Open-Vocabulary Segmentation via Step-by-Step   Visual Reasoning</h2><p><strong>Authors:Zongyan Han, Jiale Cao, Shuo Chen, Tong Wang, Jorma Laaksonen, Rao Muhammad Anwer</strong></p>
<p>Open-Vocabulary Segmentation (OVS) has drawn increasing attention for its capacity to generalize segmentation beyond predefined categories. However, existing methods typically predict segmentation masks with simple forward inference, lacking explicit reasoning and interpretability. This makes it challenging for OVS model to distinguish similar categories in open-world settings due to the lack of contextual understanding and discriminative visual cues. To address this limitation, we propose a step-by-step visual reasoning framework for open-vocabulary segmentation, named OpenSeg-R. The proposed OpenSeg-R leverages Large Multimodal Models (LMMs) to perform hierarchical visual reasoning before segmentation. Specifically, we generate both generic and image-specific reasoning for each image, forming structured triplets that explain the visual reason for objects in a coarse-to-fine manner. Based on these reasoning steps, we can compose detailed description prompts, and feed them to the segmentor to produce more accurate segmentation masks. To the best of our knowledge, OpenSeg-R is the first framework to introduce explicit step-by-step visual reasoning into OVS. Experimental results demonstrate that OpenSeg-R significantly outperforms state-of-the-art methods on open-vocabulary semantic segmentation across five benchmark datasets. Moreover, it achieves consistent gains across all metrics on open-vocabulary panoptic segmentation. Qualitative results further highlight the effectiveness of our reasoning-guided framework in improving both segmentation precision and interpretability. Our code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/Hanzy1996/OpenSeg-R">https://github.com/Hanzy1996/OpenSeg-R</a>. </p>
<blockquote>
<p>å¼€æ”¾è¯æ±‡åˆ†å‰²ï¼ˆOVSï¼‰ç”±äºå…¶è¶…è¶Šé¢„å®šç±»åˆ«çš„æ³›åŒ–åˆ†å‰²èƒ½åŠ›è€Œè¶Šæ¥è¶Šå—åˆ°å…³æ³¨ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸é€šè¿‡ç®€å•çš„å‰å‘æ¨ç†æ¥é¢„æµ‹åˆ†å‰²æ©è†œï¼Œç¼ºä¹æ˜ç¡®çš„æ¨ç†å’Œè§£é‡Šæ€§ã€‚è¿™ä½¿å¾—OVSæ¨¡å‹åœ¨å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸‹åŒºåˆ†ç›¸ä¼¼ç±»åˆ«æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå› ä¸ºç¼ºä¹ä¸Šä¸‹æ–‡ç†è§£å’Œåˆ¤åˆ«æ€§è§†è§‰çº¿ç´¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç”¨äºå¼€æ”¾è¯æ±‡åˆ†å‰²çš„é€æ­¥è§†è§‰æ¨ç†æ¡†æ¶ï¼Œåä¸ºOpenSeg-Rã€‚æ‰€æå‡ºçš„OpenSeg-Råˆ©ç”¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨åˆ†å‰²ä¹‹å‰è¿›è¡Œåˆ†å±‚è§†è§‰æ¨ç†ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä¸ºæ¯å¼ å›¾åƒç”Ÿæˆé€šç”¨å’Œå›¾åƒç‰¹å®šçš„æ¨ç†ï¼Œå½¢æˆç»“æ„åŒ–ä¸‰å…ƒç»„ï¼Œä»¥ä»ç²—ç•¥åˆ°ç²¾ç»†çš„æ–¹å¼è§£é‡Šå¯¹è±¡å‡ºç°çš„è§†è§‰åŸå› ã€‚åŸºäºè¿™äº›æ¨ç†æ­¥éª¤ï¼Œæˆ‘ä»¬å¯ä»¥ç»„æˆè¯¦ç»†çš„æè¿°æç¤ºï¼Œå¹¶å°†å…¶è¾“å…¥åˆ°åˆ†å‰²å™¨ä¸­ä»¥äº§ç”Ÿæ›´å‡†ç¡®çš„åˆ†å‰²æ©è†œã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒOpenSeg-Ræ˜¯ç¬¬ä¸€ä¸ªå°†æ˜ç¡®çš„é€æ­¥è§†è§‰æ¨ç†å¼•å…¥åˆ°OVSä¸­çš„æ¡†æ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOpenSeg-Råœ¨äº”ä¸ªåŸºå‡†æ•°æ®é›†çš„å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²ä¸Šæ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨å¼€æ”¾è¯æ±‡å…¨æ™¯åˆ†å‰²çš„æ‰€æœ‰æŒ‡æ ‡ä¸Šéƒ½å®ç°äº†æŒç»­çš„æ”¶ç›Šã€‚å®šæ€§ç»“æœè¿›ä¸€æ­¥çªå‡ºäº†æˆ‘ä»¬çš„æ¨ç†å¼•å¯¼æ¡†æ¶åœ¨æé«˜åˆ†å‰²ç²¾åº¦å’Œè§£é‡Šæ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ä»£ç å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/Hanzy1996/OpenSeg-R%E3%80%82">https://github.com/Hanzy1996/OpenSeg-Rã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16974v1">PDF</a> </p>
<p><strong>Summary</strong><br>     å¼€æ”¾è¯æ±‡åˆ†å‰²ï¼ˆOVSï¼‰èƒ½å¤Ÿæ¨å¹¿è¶…è¶Šé¢„è®¾ç±»åˆ«çš„åˆ†å‰²èƒ½åŠ›è€Œå—åˆ°è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸é€šè¿‡ç®€å•çš„æ­£å‘æ¨ç†é¢„æµ‹åˆ†å‰²æ©ç ï¼Œç¼ºä¹æ˜ç¡®çš„æ¨ç†å’Œè§£é‡Šæ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåä¸ºOpenSeg-Rçš„é€æ­¥è§†è§‰æ¨ç†æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰è¿›è¡Œåˆ†å±‚è§†è§‰æ¨ç†ï¼Œç„¶åè¿›è¡Œåˆ†å‰²ã€‚æˆ‘ä»¬ä¸ºæ¯å¼ å›¾ç‰‡ç”Ÿæˆé€šç”¨å’Œç‰¹å®šçš„è§†è§‰æ¨ç†ï¼Œå½¢æˆè§£é‡Šå›¾åƒä¸­ç‰©ä½“çš„ç»“æ„åŒ–ä¸‰å…ƒç»„ï¼Œä»¥ä»ç²—åˆ°ç»†çš„æ–¹å¼è§£é‡Šè§†è§‰åŸå› ã€‚åŸºäºè¿™äº›æ¨ç†æ­¥éª¤ï¼Œæˆ‘ä»¬å¯ä»¥ç»„æˆè¯¦ç»†çš„æè¿°æç¤ºï¼Œå¹¶å°†å…¶è¾“å…¥åˆ°åˆ†å‰²å™¨ä¸­ï¼Œä»¥äº§ç”Ÿæ›´å‡†ç¡®çš„åˆ†å‰²æ©ç ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒOpenSeg-Ræ˜¯é¦–ä¸ªå°†æ˜ç¡®çš„é€æ­¥è§†è§‰æ¨ç†å¼•å…¥å¼€æ”¾è¯æ±‡åˆ†å‰²çš„æ¡†æ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOpenSeg-Råœ¨äº”ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²ä»¥åŠå¼€æ”¾è¯æ±‡å…¨æ™¯åˆ†å‰²æ–¹é¢éƒ½æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æˆ‘ä»¬çš„ä»£ç å·²å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/Hanzy1996/OpenSeg-R">https://github.com/Hanzy1996/OpenSeg-R</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼€æ”¾è¯æ±‡åˆ†å‰²ï¼ˆOVSï¼‰èƒ½å¤Ÿæ¨å¹¿åˆ†å‰²èƒ½åŠ›è¶…è¶Šé¢„è®¾ç±»åˆ«ã€‚</li>
<li>ç°æœ‰OVSæ–¹æ³•é€šå¸¸ç¼ºä¹æ˜ç¡®çš„æ¨ç†å’Œè§£é‡Šæ€§ï¼Œåœ¨å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­åŒºåˆ†ç›¸ä¼¼ç±»åˆ«å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>OpenSeg-Ræ¡†æ¶åˆ©ç”¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰è¿›è¡Œåˆ†å±‚è§†è§‰æ¨ç†ï¼Œç„¶åè¿›è¡Œåˆ†å‰²ã€‚</li>
<li>OpenSeg-Rç”Ÿæˆç»“æ„åŒ–ä¸‰å…ƒç»„ï¼Œè§£é‡Šå›¾åƒä¸­ç‰©ä½“çš„è§†è§‰åŸå› ï¼Œä»¥ä»ç²—åˆ°ç»†çš„æ–¹å¼æé«˜åˆ†å‰²ç²¾åº¦å’Œè§£é‡Šæ€§ã€‚</li>
<li>OpenSeg-Ræ˜¯é¦–ä¸ªå°†æ˜ç¡®çš„é€æ­¥è§†è§‰æ¨ç†å¼•å…¥å¼€æ”¾è¯æ±‡åˆ†å‰²çš„æ¡†æ¶ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒOpenSeg-Råœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²å’Œå¼€æ”¾è¯æ±‡å…¨æ™¯åˆ†å‰²æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16974">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-12c404ca46c7c4ae639602ab628271e4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-430f7ff550fbc60a97350fd33202efd9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-12b6aef9dc77cdb5e7132e0369e80fcc.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Donâ€™t-â€œOverthinkâ€-Passage-Reranking-Is-Reasoning-Truly-Necessary"><a href="#Donâ€™t-â€œOverthinkâ€-Passage-Reranking-Is-Reasoning-Truly-Necessary" class="headerlink" title="Donâ€™t â€œOverthinkâ€ Passage Reranking: Is Reasoning Truly Necessary?"></a>Donâ€™t â€œOverthinkâ€ Passage Reranking: Is Reasoning Truly Necessary?</h2><p><strong>Authors:Nour Jedidi, Yung-Sung Chuang, James Glass, Jimmy Lin</strong></p>
<p>With the growing success of reasoning models across complex natural language tasks, researchers in the Information Retrieval (IR) community have begun exploring how similar reasoning capabilities can be integrated into passage rerankers built on Large Language Models (LLMs). These methods typically employ an LLM to produce an explicit, step-by-step reasoning process before arriving at a final relevance prediction. But, does reasoning actually improve reranking accuracy? In this paper, we dive deeper into this question, studying the impact of the reasoning process by comparing reasoning-based pointwise rerankers (ReasonRR) to standard, non-reasoning pointwise rerankers (StandardRR) under identical training conditions, and observe that StandardRR generally outperforms ReasonRR. Building on this observation, we then study the importance of reasoning to ReasonRR by disabling its reasoning process (ReasonRR-NoReason), and find that ReasonRR-NoReason is surprisingly more effective than ReasonRR. Examining the cause of this result, our findings reveal that reasoning-based rerankers are limited by the LLMâ€™s reasoning process, which pushes it toward polarized relevance scores and thus fails to consider the partial relevance of passages, a key factor for the accuracy of pointwise rerankers. </p>
<blockquote>
<p>éšç€æ¨ç†æ¨¡å‹åœ¨å¤æ‚çš„è‡ªç„¶è¯­è¨€ä»»åŠ¡ä¸­è¶Šæ¥è¶ŠæˆåŠŸï¼Œä¿¡æ¯æ£€ç´¢ï¼ˆIRï¼‰é¢†åŸŸçš„ç ”ç©¶äººå‘˜å¼€å§‹æ¢ç´¢å¦‚ä½•å°†ç±»ä¼¼çš„æ¨ç†èƒ½åŠ›é›†æˆåˆ°åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ®µè½é‡æ’å™¨ä¸­ã€‚è¿™äº›æ–¹æ³•é€šå¸¸ä½¿ç”¨LLMæ¥äº§ç”Ÿä¸€ä¸ªæ˜ç¡®çš„ã€é€æ­¥çš„æ¨ç†è¿‡ç¨‹ï¼Œç„¶åå†åšå‡ºæœ€ç»ˆçš„å…³è”é¢„æµ‹ã€‚ä½†æ˜¯ï¼Œæ¨ç†å®é™…ä¸Šä¼šæé«˜é‡æ’å‡†ç¡®ç‡å—ï¼Ÿåœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬å¯¹è¿™ä¸ªé—®é¢˜è¿›è¡Œäº†æ·±å…¥ç ”ç©¶ï¼Œé€šè¿‡æ¯”è¾ƒåŸºäºæ¨ç†çš„ç‚¹æ€é‡æ’å™¨ï¼ˆReasonRRï¼‰å’Œç›¸åŒè®­ç»ƒæ¡ä»¶ä¸‹çš„æ ‡å‡†éæ¨ç†ç‚¹æ€é‡æ’å™¨ï¼ˆStandardRRï¼‰ï¼Œç ”ç©¶æ¨ç†è¿‡ç¨‹çš„å½±å“ã€‚æˆ‘ä»¬å‘ç°StandardRRé€šå¸¸ä¼˜äºReasonRRã€‚åŸºäºè¿™ä¸€è§‚å¯Ÿï¼Œæˆ‘ä»¬é€šè¿‡ç¦ç”¨ReasonRRçš„æ¨ç†è¿‡ç¨‹æ¥ç ”ç©¶æ¨ç†å¯¹ReasonRRçš„é‡è¦æ€§ï¼Œå‘ç°ReasonRR-NoReasonå‡ºäººæ„æ–™åœ°æ¯”ReasonRRæ›´æœ‰æ•ˆã€‚åˆ†æé€ æˆè¿™ä¸€ç»“æœçš„åŸå› ï¼Œæˆ‘ä»¬å‘ç°åŸºäºæ¨ç†çš„é‡æ’å™¨å—åˆ°LLMæ¨ç†è¿‡ç¨‹çš„é™åˆ¶ï¼Œè¿™å¯¼è‡´å®ƒå€¾å‘äºæç«¯çš„å…³è”åˆ†æ•°ï¼Œä»è€Œæœªèƒ½è€ƒè™‘åˆ°æ®µè½çš„å±€éƒ¨å…³è”æ€§ï¼Œè¿™æ˜¯ç‚¹æ€é‡æ’å™¨å‡†ç¡®æ€§çš„å…³é”®å› ç´ ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16886v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å°†æ¨ç†èƒ½åŠ›èå…¥åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ®µè½é‡æ’ç³»ç»Ÿï¼ˆReasonRRï¼‰çš„å®é™…æ•ˆæœã€‚è®ºæ–‡æ¯”è¾ƒäº†å¸¦æœ‰æ¨ç†åŠŸèƒ½çš„ç‚¹å¼é‡æ’å™¨ï¼ˆReasonRRï¼‰ä¸ä¸å¸¦æ¨ç†åŠŸèƒ½çš„æ ‡å‡†ç‚¹å¼é‡æ’å™¨ï¼ˆStandardRRï¼‰ï¼Œå‘ç°åœ¨ç›¸åŒè®­ç»ƒæ¡ä»¶ä¸‹ï¼ŒStandardRRé€šå¸¸è¡¨ç°æ›´å¥½ã€‚è¿›ä¸€æ­¥çš„ç ”ç©¶å‘ç°ï¼Œç§»é™¤ReasonRRçš„æ¨ç†è¿‡ç¨‹ï¼ˆReasonRR-NoReasonï¼‰åè€Œæå‡äº†å…¶æ€§èƒ½ï¼Œè¿™è¡¨æ˜LLMçš„æ¨ç†è¿‡ç¨‹é™åˆ¶äº†ReasonRRçš„æ€§èƒ½ï¼Œä½¿å…¶å€¾å‘äºäº§ç”Ÿä¸¤æåŒ–çš„ç›¸å…³æ€§è¯„åˆ†ï¼Œå¿½ç•¥äº†æ®µè½çš„å±€éƒ¨ç›¸å…³æ€§ï¼Œè¿™æ˜¯ç‚¹å¼é‡æ’å™¨å‡†ç¡®æ€§çš„å…³é”®å› ç´ ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶è€…å¼€å§‹æ¢ç´¢å¦‚ä½•å°†æ¨ç†èƒ½åŠ›èå…¥åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ®µè½é‡æ’ç³»ç»Ÿã€‚</li>
<li>å¯¹æ¯”äº†å¸¦æœ‰æ¨ç†åŠŸèƒ½çš„é‡æ’å™¨ï¼ˆReasonRRï¼‰ä¸ä¸å¸¦æ¨ç†åŠŸèƒ½çš„æ ‡å‡†é‡æ’å™¨ï¼ˆStandardRRï¼‰ï¼Œå‘ç°StandardRRåœ¨ç›¸åŒè®­ç»ƒæ¡ä»¶ä¸‹è¡¨ç°æ›´ä½³ã€‚</li>
<li>ç§»é™¤ReasonRRçš„æ¨ç†è¿‡ç¨‹ï¼ˆReasonRR-NoReasonï¼‰åè€Œæå‡äº†å…¶æ€§èƒ½ï¼Œè¿™è¡¨æ˜æ¨ç†è¿‡ç¨‹å¯èƒ½æ˜¯é™åˆ¶å› ç´ ã€‚</li>
<li>LLMçš„æ¨ç†è¿‡ç¨‹å¯èƒ½å¯¼è‡´è¿‡åº¦ä¸¤æåŒ–çš„ç›¸å…³æ€§è¯„åˆ†ã€‚</li>
<li>æ¨ç†è¿‡ç¨‹å¿½ç•¥äº†æ®µè½çš„å±€éƒ¨ç›¸å…³æ€§ï¼Œè¿™æ˜¯ç‚¹å¼é‡æ’å™¨å‡†ç¡®æ€§çš„å…³é”®å› ç´ ã€‚</li>
<li>ç ”ç©¶æŒ‡å‡ºèåˆæ¨ç†å’Œé‡æ’ç³»ç»Ÿæ—¶éœ€è¦ä»”ç»†æƒè¡¡å’Œä¼˜åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16886">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a5600b93acd56430f230aab131aa664e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b61db29d8b96afec77408d6af951acb5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b083d5827a79ada7064f31c33d24caf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fdf13d1898a003b7570f217b5590e17b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b5ea7f7482da05d93beaf16ae1cbd6e5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a5b1f705aa6bcae5da7f9bb8a402044d.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Think-or-Not-Selective-Reasoning-via-Reinforcement-Learning-for-Vision-Language-Models"><a href="#Think-or-Not-Selective-Reasoning-via-Reinforcement-Learning-for-Vision-Language-Models" class="headerlink" title="Think or Not? Selective Reasoning via Reinforcement Learning for   Vision-Language Models"></a>Think or Not? Selective Reasoning via Reinforcement Learning for   Vision-Language Models</h2><p><strong>Authors:Jiaqi Wang, Kevin Qinghong Lin, James Cheng, Mike Zheng Shou</strong></p>
<p>Reinforcement Learning (RL) has proven to be an effective post-training strategy for enhancing reasoning in vision-language models (VLMs). Group Relative Policy Optimization (GRPO) is a recent prominent method that encourages models to generate complete reasoning traces before answering, leading to increased token usage and computational cost. Inspired by the human-like thinking process-where people skip reasoning for easy questions but think carefully when needed-we explore how to enable VLMs to first decide when reasoning is necessary. To realize this, we propose TON, a two-stage training strategy: (i) a supervised fine-tuning (SFT) stage with a simple yet effective â€˜thought dropoutâ€™ operation, where reasoning traces are randomly replaced with empty thoughts. This introduces a think-or-not format that serves as a cold start for selective reasoning; (ii) a GRPO stage that enables the model to freely explore when to think or not, while maximizing task-aware outcome rewards. Experimental results show that TON can reduce the completion length by up to 90% compared to vanilla GRPO, without sacrificing performance or even improving it. Further evaluations across diverse vision-language tasks-covering a range of reasoning difficulties under both 3B and 7B models-consistently reveal that the model progressively learns to bypass unnecessary reasoning steps as training advances. These findings shed light on the path toward human-like reasoning patterns in reinforcement learning approaches. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/kokolerk/TON">https://github.com/kokolerk/TON</a>. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·²è¢«è¯æ˜æ˜¯æå‡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æ¨ç†èƒ½åŠ›çš„æœ‰æ•ˆåè®­ç»ƒç­–ç•¥ã€‚è¿‘æœŸå¤‡å—ç©ç›®çš„ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰æ–¹æ³•é¼“åŠ±æ¨¡å‹åœ¨å›ç­”é—®é¢˜å‰ç”Ÿæˆå®Œæ•´çš„æ¨ç†è¿‡ç¨‹ï¼Œä»è€Œå¯¼è‡´ä»¤ç‰Œä½¿ç”¨é‡å’Œè®¡ç®—æˆæœ¬çš„å¢åŠ ã€‚äººç±»æ€è€ƒè¿‡ç¨‹é€šå¸¸æ˜¯åœ¨é¢å¯¹ç®€å•é—®é¢˜æ—¶ä¼šè·³è¿‡æ¨ç†ï¼Œåœ¨å¿…è¦æ—¶æ‰ä¼šä»”ç»†æ€è€ƒã€‚å—æ­¤å¯å‘ï¼Œæˆ‘ä»¬æ¢ç´¢å¦‚ä½•ä½¿VLMé¦–å…ˆå†³å®šä½•æ—¶è¿›è¡Œæ¨ç†ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†TONï¼Œè¿™æ˜¯ä¸€ç§ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼šï¼ˆiï¼‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰é˜¶æ®µï¼Œé‡‡ç”¨ç®€å•æœ‰æ•ˆçš„â€œæ€æƒ³ä¸¢å¤±â€æ“ä½œï¼Œå…¶ä¸­æ¨ç†è½¨è¿¹è¢«éšæœºæ›¿æ¢ä¸ºç©ºæ´æ€æƒ³ã€‚è¿™å¼•å…¥äº†â€œæ€è€ƒä¸å¦â€çš„æ ¼å¼ï¼Œä½œä¸ºé€‰æ‹©æ€§æ¨ç†çš„å†·å¯åŠ¨ï¼›ï¼ˆiiï¼‰GRPOé˜¶æ®µä½¿æ¨¡å‹èƒ½å¤Ÿè‡ªç”±æ¢ç´¢ä½•æ—¶æ€è€ƒæˆ–ä¸æ€è€ƒï¼ŒåŒæ—¶æœ€å¤§åŒ–ä»»åŠ¡æ„ŸçŸ¥ç»“æœå¥–åŠ±ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸å¸¸è§„GRPOç›¸æ¯”ï¼ŒTONå¯ä»¥å‡å°‘é«˜è¾¾90%çš„å®Œæˆé•¿åº¦ï¼ŒåŒæ—¶ä¸ç‰ºç‰²æ€§èƒ½ç”šè‡³å¯èƒ½æœ‰æ‰€æé«˜ã€‚åœ¨æ¶µç›–ä¸åŒéš¾åº¦æ¨ç†ä»»åŠ¡çš„è§†è§‰è¯­è¨€ä»»åŠ¡ä¸Šè¿›è¡Œçš„è¿›ä¸€æ­¥è¯„ä¼°ï¼Œæ— è®ºæ˜¯ä½¿ç”¨3Bè¿˜æ˜¯7Bæ¨¡å‹ï¼Œéƒ½ä¸€è‡´è¡¨æ˜éšç€è®­ç»ƒçš„è¿›è¡Œï¼Œæ¨¡å‹é€æ¸å­¦ä¼šç»•è¿‡ä¸å¿…è¦çš„æ¨ç†æ­¥éª¤ã€‚è¿™äº›å‘ç°æ­ç¤ºäº†æœç€å¼ºåŒ–å­¦ä¹ ä¸­çš„äººç±»å¼æ¨ç†æ¨¡å¼å‘å±•çš„é“è·¯ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/kokolerk/TON%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/kokolerk/TONè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16854v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¼ºåŒ–å­¦ä¹ åœ¨æå‡è§†è§‰è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›æ–¹é¢çš„åº”ç”¨ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•å¦‚GRPOåœ¨è®¡ç®—æˆæœ¬å’Œtokenä½¿ç”¨ä¸Šçš„ä¸è¶³ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥TONã€‚é¦–å…ˆé€šè¿‡å¸¦æœ‰â€œæ€ç»´ä¸¢å¤±â€æ“ä½œçš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è¿›è¡Œå†·å¯åŠ¨é€‰æ‹©æ€§æ¨ç†ï¼›ç„¶åé‡‡ç”¨GRPOï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿè‡ªç”±æ¢ç´¢ä½•æ—¶è¿›è¡Œæ¨ç†ï¼ŒåŒæ—¶æœ€å¤§åŒ–ä»»åŠ¡æ„ŸçŸ¥ç»“æœå¥–åŠ±ã€‚å®éªŒè¡¨æ˜ï¼ŒTONèƒ½æœ‰æ•ˆå‡å°‘é«˜è¾¾90%çš„å®Œæˆé•¿åº¦ï¼ŒåŒæ—¶ä¸ç‰ºç‰²æ€§èƒ½ç”šè‡³å¯èƒ½æœ‰æ‰€æå‡ã€‚è¯¥ç­–ç•¥åœ¨ä¸åŒéš¾åº¦å’Œç±»å‹çš„è§†è§‰è¯­è¨€ä»»åŠ¡ä¸Šå‡è¡¨ç°å‡ºæ¸è¿›å­¦ä¹ ç»•è¿‡ä¸å¿…è¦æ¨ç†æ­¥éª¤çš„èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ è¢«è¯æ˜æ˜¯æå‡è§†è§‰è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„æœ‰æ•ˆç­–ç•¥ã€‚</li>
<li>Group Relative Policy Optimization (GRPO)é¼“åŠ±æ¨¡å‹åœ¨å›ç­”é—®é¢˜å‰ç”Ÿæˆå®Œæ•´çš„æ¨ç†è¿‡ç¨‹ï¼Œä½†å¯¼è‡´æ›´é«˜çš„è®¡ç®—æˆæœ¬å’Œtokenä½¿ç”¨ã€‚</li>
<li>TONæ˜¯ä¸€ç§æ–°çš„ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œæ—¨åœ¨ä½¿è§†è§‰è¯­è¨€æ¨¡å‹å­¦ä¼šä½•æ—¶è¿›è¡Œæ¨ç†ã€‚</li>
<li>TONé€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰é˜¶æ®µå¼•å…¥â€œæ€ç»´ä¸¢å¤±â€æ“ä½œï¼Œä½œä¸ºé€‰æ‹©æ€§æ¨ç†çš„å†·å¯åŠ¨ã€‚</li>
<li>TONçš„ç¬¬äºŒé˜¶æ®µæ˜¯GRPOï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿè‡ªç”±æ¢ç´¢ä½•æ—¶è¿›è¡Œæ¨ç†ï¼ŒåŒæ—¶æœ€å¤§åŒ–ä»»åŠ¡æ„ŸçŸ¥ç»“æœå¥–åŠ±ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒTONèƒ½æ˜¾è‘—å‡å°‘å®Œæˆé•¿åº¦ï¼ŒåŒæ—¶ä¿æŒæˆ–æé«˜æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16854">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1733e005973f10e82a4016d616b00356.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3e34e795e04eeee473414f39499c0932.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dd0d32f5bfdb9629e496c5bbe0654361.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc0145ae9cda694ae3ea3b6e160422f4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8b00446e8ddb906ebdf792da68e878f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-90d108734774c4163e554f433072ac32.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="SimpleDeepSearcher-Deep-Information-Seeking-via-Web-Powered-Reasoning-Trajectory-Synthesis"><a href="#SimpleDeepSearcher-Deep-Information-Seeking-via-Web-Powered-Reasoning-Trajectory-Synthesis" class="headerlink" title="SimpleDeepSearcher: Deep Information Seeking via Web-Powered Reasoning   Trajectory Synthesis"></a>SimpleDeepSearcher: Deep Information Seeking via Web-Powered Reasoning   Trajectory Synthesis</h2><p><strong>Authors:Shuang Sun, Huatong Song, Yuhao Wang, Ruiyang Ren, Jinhao Jiang, Junjie Zhang, Fei Bai, Jia Deng, Wayne Xin Zhao, Zheng Liu, Lei Fang, Zhongyuan Wang, Ji-Rong Wen</strong></p>
<p>Retrieval-augmented generation (RAG) systems have advanced large language models (LLMs) in complex deep search scenarios requiring multi-step reasoning and iterative information retrieval. However, existing approaches face critical limitations that lack high-quality training trajectories or suffer from the distributional mismatches in simulated environments and prohibitive computational costs for real-world deployment. This paper introduces SimpleDeepSearcher, a lightweight yet effective framework that bridges this gap through strategic data engineering rather than complex training paradigms. Our approach synthesizes high-quality training data by simulating realistic user interactions in live web search environments, coupled with a multi-criteria curation strategy that optimizes the diversity and quality of input and output side. Experiments on five benchmarks across diverse domains demonstrate that SFT on only 871 curated samples yields significant improvements over RL-based baselines. Our work establishes SFT as a viable pathway by systematically addressing the data-scarce bottleneck, offering practical insights for efficient deep search systems. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/RUCAIBox/SimpleDeepSearcher">https://github.com/RUCAIBox/SimpleDeepSearcher</a>. </p>
<blockquote>
<p>å¢å¼ºæ£€ç´¢ç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿåœ¨éœ€è¦å¤šæ­¥éª¤æ¨ç†å’Œè¿­ä»£ä¿¡æ¯æ£€ç´¢çš„å¤æ‚æ·±åº¦æœç´¢åœºæ™¯ä¸­æ¨åŠ¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‘å±•ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é¢ä¸´ç¼ºä¹é«˜è´¨é‡è®­ç»ƒè½¨è¿¹çš„å…³é”®å±€é™ï¼Œæˆ–åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸çœŸå®ä¸–ç•Œéƒ¨ç½²ä¸­çš„åˆ†å¸ƒä¸åŒ¹é…ï¼Œä»¥åŠè®¡ç®—æˆæœ¬é«˜æ˜‚çš„é—®é¢˜ã€‚æœ¬æ–‡ä»‹ç»äº†SimpleDeepSearcherï¼Œè¿™æ˜¯ä¸€ä¸ªè½»ä¾¿æœ‰æ•ˆçš„æ¡†æ¶ï¼Œå®ƒé€šè¿‡æˆ˜ç•¥æ•°æ®å·¥ç¨‹è€Œä¸æ˜¯å¤æ‚çš„è®­ç»ƒèŒƒå¼æ¥å¼¥åˆè¿™ä¸€é¸¿æ²Ÿã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡æ¨¡æ‹ŸçœŸå®ç”¨æˆ·äº¤äº’æ¥åˆæˆé«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ï¼Œè¿™åœ¨å®æ—¶ç½‘ç»œæœç´¢ç¯å¢ƒä¸­è¿›è¡Œï¼ŒåŒæ—¶é‡‡ç”¨å¤šæ ‡å‡†ç­›é€‰ç­–ç•¥ï¼Œä»¥ä¼˜åŒ–è¾“å…¥å’Œè¾“å‡ºç«¯çš„å¤šæ ·æ€§å’Œè´¨é‡ã€‚åœ¨å¤šä¸ªé¢†åŸŸäº”ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä»…åœ¨871ä¸ªç­›é€‰æ ·æœ¬ä¸Šè¿›è¡ŒSFTå³å¯è·å¾—å¯¹åŸºäºRLçš„åŸºå‡†æµ‹è¯•çš„é‡å¤§æ”¹è¿›ã€‚æˆ‘ä»¬çš„å·¥ä½œé€šè¿‡ç³»ç»Ÿåœ°è§£å†³æ•°æ®ç¨€ç¼ºç“¶é¢ˆï¼Œå»ºç«‹äº†SFTä½œä¸ºä¸€ä¸ªå¯è¡Œçš„é€”å¾„ï¼Œä¸ºé«˜æ•ˆçš„æ·±åº¦æœç´¢ç³»ç»Ÿæä¾›äº†å®é™…è§è§£ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/RUCAIBox/SimpleDeepSearcher">https://github.com/RUCAIBox/SimpleDeepSearcher</a>å¤„è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16834v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>SimpleDeepSearcheræ¡†æ¶é€šè¿‡æˆ˜ç•¥æ•°æ®å·¥ç¨‹è§£å†³äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æ·±åº¦æœç´¢åœºæ™¯ä¸­é¢ä¸´çš„é—®é¢˜ï¼Œå¦‚é«˜è´¨é‡è®­ç»ƒè½¨è¿¹ç¼ºä¹ã€æ¨¡æ‹Ÿç¯å¢ƒä¸çœŸå®ç¯å¢ƒåˆ†å¸ƒä¸åŒ¹é…ä»¥åŠè®¡ç®—æˆæœ¬é«˜æ˜‚ç­‰ã€‚è¯¥æ¡†æ¶é€šè¿‡æ¨¡æ‹ŸçœŸå®ç”¨æˆ·äº¤äº’å’Œé‡‡ç”¨å¤šæ ‡å‡†ç­›é€‰ç­–ç•¥æ¥åˆæˆé«˜è´¨é‡è®­ç»ƒæ•°æ®ï¼Œä¼˜åŒ–è¾“å…¥å’Œè¾“å‡ºçš„å¤šæ ·æ€§å’Œè´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä»…åœ¨871ä¸ªç²¾é€‰æ ·æœ¬ä¸Šè¿›è¡Œè½¯å¯åŠ¨è®­ç»ƒå³å¯åœ¨äº”ä¸ªåŸºå‡†æµ‹è¯•ä¸Šå®ç°æ˜¾è‘—æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SimpleDeepSearcheræ˜¯ä¸€ä¸ªç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤æ‚æ·±åº¦æœç´¢åœºæ™¯ä¸­çš„å¤šæ­¥æ¨ç†å’Œè¿­ä»£ä¿¡æ¯æ£€ç´¢é—®é¢˜ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡æˆ˜ç•¥æ•°æ®å·¥ç¨‹æ¥å¼¥è¡¥ç°æœ‰æ–¹æ³•çš„ä¸è¶³ï¼Œè€Œä¸æ˜¯é‡‡ç”¨å¤æ‚çš„è®­ç»ƒæ¨¡å¼ã€‚</li>
<li>SimpleDeepSearcheré€šè¿‡æ¨¡æ‹ŸçœŸå®ç”¨æˆ·äº¤äº’å’Œé‡‡ç”¨å¤šæ ‡å‡†ç­›é€‰ç­–ç•¥æ¥åˆæˆé«˜è´¨é‡è®­ç»ƒæ•°æ®ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œä»…åœ¨å°‘é‡ç²¾é€‰æ ·æœ¬ä¸Šè¿›è¡Œè½¯å¯åŠ¨è®­ç»ƒå³å¯å®ç°æ˜¾è‘—æ€§èƒ½æå‡ã€‚</li>
<li>è¯¥æ¡†æ¶åœ¨äº”ä¸ªä¸åŒé¢†åŸŸçš„åŸºå‡†æµ‹è¯•ä¸­å‡è¡¨ç°å‡ºè‰²ã€‚</li>
<li>SimpleDeepSearcherä¸ºé«˜æ•ˆæ·±åº¦æœç´¢ç³»ç»Ÿæä¾›äº†å®ç”¨è§è§£ï¼Œä¸ºè§£å†³æ•°æ®ç¨€ç¼ºç“¶é¢ˆæä¾›äº†ä¸€ç§å¯è¡Œé€”å¾„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16834">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8525d5d2d0a3573ea0d323deeeef2ce2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-31a9533e4ec6822b1972f35403eb10ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a1436883a5dc2d7b1db1bd84cbfc387c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a5e31ca32b57bfdd86ecfc97c56d2bc0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0218ea7e3273aa15121e4abc81333d34.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Unlearning-Isnâ€™t-Deletion-Investigating-Reversibility-of-Machine-Unlearning-in-LLMs"><a href="#Unlearning-Isnâ€™t-Deletion-Investigating-Reversibility-of-Machine-Unlearning-in-LLMs" class="headerlink" title="Unlearning Isnâ€™t Deletion: Investigating Reversibility of Machine   Unlearning in LLMs"></a>Unlearning Isnâ€™t Deletion: Investigating Reversibility of Machine   Unlearning in LLMs</h2><p><strong>Authors:Xiaoyu Xu, Xiang Yue, Yang Liu, Qingqing Ye, Haibo Hu, Minxin Du</strong></p>
<p>Unlearning in large language models (LLMs) is intended to remove the influence of specific data, yet current evaluations rely heavily on token-level metrics such as accuracy and perplexity. We show that these metrics can be misleading: models often appear to forget, but their original behavior can be rapidly restored with minimal fine-tuning, revealing that unlearning may obscure information rather than erase it. To diagnose this phenomenon, we introduce a representation-level evaluation framework using PCA-based similarity and shift, centered kernel alignment, and Fisher information. Applying this toolkit across six unlearning methods, three domains (text, code, math), and two open-source LLMs, we uncover a critical distinction between reversible and irreversible forgetting. In reversible cases, models suffer token-level collapse yet retain latent features; in irreversible cases, deeper representational damage occurs. We further provide a theoretical account linking shallow weight perturbations near output layers to misleading unlearning signals, and show that reversibility is modulated by task type and hyperparameters. Our findings reveal a fundamental gap in current evaluation practices and establish a new diagnostic foundation for trustworthy unlearning in LLMs. We provide a unified toolkit for analyzing LLM representation changes under unlearning and relearning: <a target="_blank" rel="noopener" href="https://github.com/XiaoyuXU1/Representational_Analysis_Tools.git">https://github.com/XiaoyuXU1/Representational_Analysis_Tools.git</a>. </p>
<blockquote>
<p>åœ¨å¤§è¯­è¨€æ¨¡å‹ä¸­ï¼Œâ€è§£å­¦ä¹ â€ï¼ˆunlearningï¼‰æ—¨åœ¨æ¶ˆé™¤ç‰¹å®šæ•°æ®çš„å½±å“ï¼Œç„¶è€Œå½“å‰çš„è¯„ä¼°ä¸»è¦ä¾èµ–äºè¯¸å¦‚å‡†ç¡®ç‡å’Œå›°æƒ‘åº¦ä¹‹ç±»çš„æ ‡è®°çº§æŒ‡æ ‡ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œè¿™äº›æŒ‡æ ‡å¯èƒ½ä¼šè¯¯å¯¼äººï¼šæ¨¡å‹ä¼¼ä¹å¿˜è®°äº†æŸäº›ä¿¡æ¯ï¼Œä½†é€šè¿‡å¾®è°ƒå¾ˆå°‘çš„å‚æ•°ï¼Œå…¶åŸå§‹è¡Œä¸ºå¯ä»¥è¿…é€Ÿæ¢å¤ï¼Œè¿™è¡¨æ˜è§£å­¦ä¹ å¯èƒ½åªæ˜¯æ©ç›–äº†ä¿¡æ¯è€Œä¸æ˜¯çœŸæ­£åˆ é™¤å®ƒã€‚ä¸ºäº†è¯Šæ–­è¿™ç§ç°è±¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåŸºäºPCAçš„ç›¸ä¼¼æ€§å˜åŒ–å’Œä¸­å¿ƒå†…æ ¸å¯¹é½ä»¥åŠFisherä¿¡æ¯çš„è¡¨ç¤ºçº§è¯„ä¼°æ¡†æ¶ã€‚æˆ‘ä»¬åœ¨å…­ç§è§£å­¦ä¹ æ–¹æ³•ã€ä¸‰ä¸ªé¢†åŸŸï¼ˆæ–‡æœ¬ã€ä»£ç ã€æ•°å­¦ï¼‰å’Œä¸¤ä¸ªå¼€æºçš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸­åº”ç”¨äº†è¿™ä¸€å·¥å…·åŒ…ï¼Œå‘ç°äº†å¯é€†å’Œä¸å¯é€†é—å¿˜ä¹‹é—´çš„å…³é”®åŒºåˆ«ã€‚åœ¨å¯é€†æƒ…å†µä¸‹ï¼Œæ¨¡å‹åœ¨æ ‡è®°å±‚é¢å´©æºƒä½†ä»ä¿ç•™æ½œåœ¨ç‰¹å¾ï¼›åœ¨ä¸å¯é€†æƒ…å†µä¸‹ï¼Œæ›´æ·±å±‚æ¬¡çš„è¡¨ç¤ºæ€§æŸå®³ä¼šå‘ç”Ÿã€‚æˆ‘ä»¬è¿˜ä»ç†è®ºä¸Šè§£é‡Šäº†è¾“å‡ºå±‚é™„è¿‘æµ…å±‚æƒé‡æ‰°åŠ¨ä¸è¯¯å¯¼è§£å­¦ä¹ ä¿¡å·ä¹‹é—´çš„è”ç³»ï¼Œå¹¶è¡¨æ˜å¯é€†æ€§å—ä»»åŠ¡ç±»å‹å’Œè¶…å‚æ•°çš„å½±å“ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ­ç¤ºäº†å½“å‰è¯„ä¼°å®è·µä¸­çš„åŸºæœ¬å·®è·ï¼Œå¹¶ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ä¸­å¯é çš„è§£å­¦ä¹ å»ºç«‹äº†æ–°çš„è¯Šæ–­åŸºç¡€ã€‚æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªç»Ÿä¸€å·¥å…·åŒ…ï¼Œç”¨äºåˆ†æå¤§å‹è¯­è¨€æ¨¡å‹åœ¨è§£å­¦ä¹ å’Œå†å­¦ä¹ è¿‡ç¨‹ä¸­çš„è¡¨ç¤ºå˜åŒ–ï¼š<a target="_blank" rel="noopener" href="https://github.com/XiaoyuXU1/Representational_Analysis_Tools.git">https://github.com/XiaoyuXU1/Representational_Analysis_Tools.git</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16831v1">PDF</a> 44 pages</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„â€œé—å¿˜å­¦ä¹ â€ï¼ˆUnlearningï¼‰æ—¨åœ¨æ¶ˆé™¤ç‰¹å®šæ•°æ®çš„å½±å“ï¼Œä½†ç°æœ‰è¯„ä¼°ä¸»è¦ä¾èµ–å‡†ç¡®æ€§ç­‰è¯çº§æŒ‡æ ‡ã€‚ç ”ç©¶æŒ‡å‡ºè¿™äº›æŒ‡æ ‡å…·æœ‰è¯¯å¯¼æ€§ï¼Œå› ä¸ºæ¨¡å‹çœ‹ä¼¼é—å¿˜ï¼Œå´å¯é€šè¿‡å¾®è°ƒè¿…é€Ÿæ¢å¤åŸæœ‰è¡¨ç°ï¼Œè¡¨æ˜é—å¿˜å¯èƒ½åªæ˜¯æ©ç›–ä¿¡æ¯è€ŒéçœŸæ­£æ“¦é™¤ã€‚ä¸ºè¯Šæ–­æ­¤ç°è±¡ï¼Œç ”ç©¶å¼•å…¥åŸºäºPCAçš„ç›¸ä¼¼æ€§ä¸å˜åŒ–ã€ä¸­å¿ƒæ ¸å¯¹é½å’ŒFisherä¿¡æ¯çš„è¡¨å¾çº§è¯„ä¼°æ¡†æ¶ã€‚åº”ç”¨æ­¤å·¥å…·åŒ…äºå…­ç§é—å¿˜æ–¹æ³•ã€ä¸‰ä¸ªé¢†åŸŸï¼ˆæ–‡æœ¬ã€ä»£ç ã€æ•°å­¦ï¼‰å’Œä¸¤ä¸ªå¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå‘ç°å¯é€†ä¸ä¸å¯é€†é—å¿˜é—´å­˜åœ¨å…³é”®å·®å¼‚ã€‚å¯é€†æ¡ˆä¾‹ä¸­ï¼Œæ¨¡å‹è¯çº§è¡¨ç°å´©æºƒä½†ä¿ç•™æ½œåœ¨ç‰¹å¾ï¼›ä¸å¯é€†æ¡ˆä¾‹ä¸­ï¼Œæ›´æ·±å±‚æ¬¡çš„è¡¨å¾å—æŸã€‚ç ”ç©¶è¿˜ä»ç†è®ºå±‚é¢è”ç³»è¾“å‡ºå±‚é™„è¿‘çš„æµ…å±‚æƒé‡æ‰°åŠ¨ä¸è¯¯å¯¼æ€§çš„é—å¿˜ä¿¡å·ï¼Œå¹¶è¡¨æ˜å¯é€†æ€§å—ä»»åŠ¡ç±»å‹å’Œè¶…å‚æ•°è°ƒåˆ¶ã€‚ç ”ç©¶æ­ç¤ºäº†å½“å‰è¯„ä¼°å®è·µçš„é‡å¤§ç¼ºé™·ï¼Œå¹¶ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ä¸­å¯ä¿¡çš„é—å¿˜å­¦ä¹ æä¾›äº†æ–°çš„è¯Šæ–­åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹çš„â€œé—å¿˜å­¦ä¹ â€æ—¨åœ¨æ¶ˆé™¤ç‰¹å®šæ•°æ®å½±å“ã€‚</li>
<li>å½“å‰è¯„ä¼°ä¸»è¦ä¾èµ–è¯çº§æŒ‡æ ‡ï¼Œå¦‚å‡†ç¡®æ€§å’Œå›°æƒ‘åº¦ï¼Œè¿™å¯èƒ½å…·æœ‰è¯¯å¯¼æ€§ã€‚</li>
<li>æ¨¡å‹çœ‹ä¼¼é—å¿˜ï¼Œä½†å¯èƒ½åªæ˜¯é€šè¿‡å¾®è°ƒè¿…é€Ÿæ¢å¤åŸæœ‰è¡¨ç°ï¼Œè¡¨æ˜é—å¿˜å¯èƒ½åªæ˜¯æ©ç›–ä¿¡æ¯ã€‚</li>
<li>å¼•å…¥è¡¨å¾çº§è¯„ä¼°æ¡†æ¶ä»¥æ›´å‡†ç¡®åœ°è¯Šæ–­é—å¿˜ç°è±¡ã€‚</li>
<li>å¯é€†ä¸ä¸å¯é€†é—å¿˜ä¹‹é—´å­˜åœ¨å…³é”®å·®å¼‚ï¼šå¯é€†æ¡ˆä¾‹ä¸­æ¨¡å‹è¯çº§è¡¨ç°å´©æºƒä½†ä¿ç•™æ½œåœ¨ç‰¹å¾ï¼›ä¸å¯é€†æ¡ˆä¾‹ä¸­æ›´æ·±å±‚æ¬¡çš„è¡¨å¾å—æŸã€‚</li>
<li>æµ…å±‚æƒé‡æ‰°åŠ¨å¯èƒ½å¯¼è‡´è¯¯å¯¼æ€§çš„é—å¿˜ä¿¡å·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16831">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6f51dcf980a500a1cd19fbd410a9052d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f4012dbe8a9befe17f6be40907129d5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-04941cbde46c9ffebefaa362db63a695.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2ea9595d666df508ea667c33b66fde5c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-363395fc7e14c7cedaff047079a24fcc.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="DeepRec-Towards-a-Deep-Dive-Into-the-Item-Space-with-Large-Language-Model-Based-Recommendation"><a href="#DeepRec-Towards-a-Deep-Dive-Into-the-Item-Space-with-Large-Language-Model-Based-Recommendation" class="headerlink" title="DeepRec: Towards a Deep Dive Into the Item Space with Large Language   Model Based Recommendation"></a>DeepRec: Towards a Deep Dive Into the Item Space with Large Language   Model Based Recommendation</h2><p><strong>Authors:Bowen Zheng, Xiaolei Wang, Enze Liu, Xi Wang, Lu Hongyu, Yu Chen, Wayne Xin Zhao, Ji-Rong Wen</strong></p>
<p>Recently, large language models (LLMs) have been introduced into recommender systems (RSs), either to enhance traditional recommendation models (TRMs) or serve as recommendation backbones. However, existing LLM-based RSs often do not fully exploit the complementary advantages of LLMs (e.g., world knowledge and reasoning) and TRMs (e.g., recommendation-specific knowledge and efficiency) to fully explore the item space. To address this, we propose DeepRec, a novel LLM-based RS that enables autonomous multi-turn interactions between LLMs and TRMs for deep exploration of the item space. In each interaction turn, LLMs reason over user preferences and interact with TRMs to retrieve candidate items. After multi-turn interactions, LLMs rank the retrieved items to generate the final recommendations. We adopt reinforcement learning(RL) based optimization and propose novel designs from three aspects: recommendation model based data rollout, recommendation-oriented hierarchical rewards, and a two-stage RL training strategy. For data rollout, we introduce a preference-aware TRM, with which LLMs interact to construct trajectory data. For rewards, we design a hierarchical reward function that involves both process-level and outcome-level rewards to optimize the interaction process and recommendation performance, respectively. For RL training, we develop a two-stage training strategy, where the first stage aims to guide LLMs to interact with TRMs and the second stage focuses on performance improvement. Experiments on public datasets demonstrate that DeepRec significantly outperforms both traditional and LLM-based baselines, offering a new paradigm for deep exploration in recommendation systems. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²è¢«å¼•å…¥æ¨èç³»ç»Ÿï¼ˆRSï¼‰ï¼Œæ— è®ºæ˜¯ä¸ºäº†å¢å¼ºä¼ ç»Ÿæ¨èæ¨¡å‹ï¼ˆTRMï¼‰è¿˜æ˜¯ä½œä¸ºæ¨èç³»ç»Ÿçš„ä¸»å¹²ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºLLMçš„æ¨èç³»ç»Ÿå¾€å¾€æ²¡æœ‰å……åˆ†åˆ©ç”¨LLMï¼ˆå¦‚ä¸–ç•ŒçŸ¥è¯†å’Œæ¨ç†ï¼‰å’ŒTRMï¼ˆå¦‚ç‰¹å®šäºæ¨èçš„çŸ¥è¯†å’Œæ•ˆç‡ï¼‰çš„äº’è¡¥ä¼˜åŠ¿ï¼Œä»¥å……åˆ†æ¢ç´¢ç‰©å“ç©ºé—´ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†DeepRecï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„åŸºäºLLMçš„æ¨èç³»ç»Ÿï¼Œå®ƒèƒ½å¤Ÿåœ¨LLMå’ŒTRMä¹‹é—´è¿›è¡Œè‡ªä¸»çš„å¤šæ¬¡äº¤äº’ï¼Œä»¥æ·±å…¥æ¢ç´¢ç‰©å“ç©ºé—´ã€‚åœ¨æ¯ä¸ªäº¤äº’å›åˆä¸­ï¼ŒLLMå¯¹ç”¨æˆ·çš„åå¥½è¿›è¡Œæ¨ç†ï¼Œå¹¶ä¸TRMè¿›è¡Œäº¤äº’ä»¥æ£€ç´¢å€™é€‰ç‰©å“ã€‚åœ¨ç»è¿‡å¤šæ¬¡äº¤äº’åï¼ŒLLMå¯¹æ£€ç´¢åˆ°çš„ç‰©å“è¿›è¡Œæ’åä»¥ç”Ÿæˆæœ€ç»ˆæ¨èã€‚æˆ‘ä»¬é‡‡ç”¨åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„ä¼˜åŒ–æ–¹æ³•ï¼Œå¹¶ä»ä¸‰ä¸ªæ–¹é¢æå‡ºæ–°çš„è®¾è®¡ï¼šåŸºäºæ¨èæ¨¡å‹çš„æ•°æ®æ»šåŠ¨ã€é¢å‘æ¨èçš„åˆ†å±‚å¥–åŠ±å’Œä¸¤é˜¶æ®µRLè®­ç»ƒç­–ç•¥ã€‚å¯¹äºæ•°æ®æ»šåŠ¨ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ„ŸçŸ¥åå¥½çš„TRMï¼ŒLLMä¸ä¹‹äº¤äº’ä»¥æ„å»ºè½¨è¿¹æ•°æ®ã€‚å¯¹äºå¥–åŠ±ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªåˆ†å±‚å¥–åŠ±å‡½æ•°ï¼Œæ¶‰åŠè¿‡ç¨‹çº§å’Œç»“æœçº§å¥–åŠ±ï¼Œä»¥åˆ†åˆ«ä¼˜åŒ–äº¤äº’è¿‡ç¨‹å’Œæ¨èæ€§èƒ½ã€‚å¯¹äºRLè®­ç»ƒï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œç¬¬ä¸€é˜¶æ®µæ—¨åœ¨æŒ‡å¯¼LLMä¸TRMè¿›è¡Œäº¤äº’ï¼Œç¬¬äºŒé˜¶æ®µä¾§é‡äºæ€§èƒ½æå‡ã€‚åœ¨å…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDeepRecæ˜¾è‘—ä¼˜äºä¼ ç»Ÿå’ŒåŸºäºLLMçš„åŸºçº¿æ–¹æ³•ï¼Œä¸ºæ¨èç³»ç»Ÿçš„æ·±å…¥æ¢ç´¢æä¾›äº†æ–°çš„èŒƒå¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16810v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨èç³»ç»Ÿï¼ˆRSï¼‰ä¸­çš„åº”ç”¨å·²å¼•èµ·å…³æ³¨ï¼Œä½†ç°æœ‰LLM-based RSæœªèƒ½å……åˆ†åˆ©ç”¨LLMså’Œä¼ ç»Ÿçš„æ¨èæ¨¡å‹ï¼ˆTRMsï¼‰çš„äº’è¡¥ä¼˜åŠ¿æ¥å…¨é¢æ¢ç´¢ç‰©å“ç©ºé—´ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºDeepRecï¼Œä¸€ç§æ–°å‹LLM-based RSï¼Œå®ç°LLMså’ŒTRMsä¹‹é—´çš„è‡ªä¸»å¤šè½®äº¤äº’ï¼Œä»¥æ·±åº¦æ¢ç´¢ç‰©å“ç©ºé—´ã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¼˜åŒ–ï¼ŒDeepRecä»ä¸‰ä¸ªæ–¹é¢è¿›è¡Œåˆ›æ–°è®¾è®¡ï¼šåŸºäºæ¨èæ¨¡å‹çš„æ•°æ®æ»šåŠ¨ã€é¢å‘æ¨èçš„åˆ†å±‚å¥–åŠ±å’Œä¸¤é˜¶æ®µRLè®­ç»ƒç­–ç•¥ã€‚å®éªŒè¯æ˜ï¼ŒDeepRecæ˜¾è‘—ä¼˜äºä¼ ç»Ÿå’ŒLLM-basedåŸºçº¿æ–¹æ³•ï¼Œä¸ºæ¨èç³»ç»Ÿçš„æ·±åº¦æ¢ç´¢æä¾›äº†æ–°çš„èŒƒå¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså·²åº”ç”¨äºæ¨èç³»ç»Ÿï¼Œä½†æœªèƒ½å……åˆ†åˆ©ç”¨å…¶ä¸TRMsçš„äº’è¡¥ä¼˜åŠ¿ã€‚</li>
<li>DeepRecæ˜¯ä¸€ç§æ–°å‹LLM-based RSï¼Œå®ç°LLMså’ŒTRMsä¹‹é—´çš„å¤šè½®äº¤äº’ï¼Œæ·±åº¦æ¢ç´¢ç‰©å“ç©ºé—´ã€‚</li>
<li>DeepRecé‡‡ç”¨å¼ºåŒ–å­¦ä¹ è¿›è¡Œä¼˜åŒ–ï¼ŒåŒ…æ‹¬æ•°æ®æ»šåŠ¨ã€åˆ†å±‚å¥–åŠ±å’Œä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ã€‚</li>
<li>æ•°æ®æ»šåŠ¨é€šè¿‡LLMsä¸TRMsçš„äº¤äº’æ„å»ºè½¨è¿¹æ•°æ®ã€‚</li>
<li>åˆ†å±‚å¥–åŠ±å‡½æ•°åŒæ—¶ä¼˜åŒ–äº¤äº’è¿‡ç¨‹å’Œæ¨èæ€§èƒ½ã€‚</li>
<li>ä¸¤é˜¶æ®µRLè®­ç»ƒç­–ç•¥æ—¨åœ¨æŒ‡å¯¼LLMsä¸TRMsçš„äº¤äº’å¹¶æé«˜æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16810">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-78ba180caf19c41447269a5740634cf4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-51ded13f905531ce9013bff01846fd1e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c68db8f369d99fad02b9f0953f98b361.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-68ab6712f602693d2946d5d68e15d7b4.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Two-way-Evidence-self-Alignment-based-Dual-Gated-Reasoning-Enhancement"><a href="#Two-way-Evidence-self-Alignment-based-Dual-Gated-Reasoning-Enhancement" class="headerlink" title="Two-way Evidence self-Alignment based Dual-Gated Reasoning Enhancement"></a>Two-way Evidence self-Alignment based Dual-Gated Reasoning Enhancement</h2><p><strong>Authors:Kexin Zhang, Junlan Chen, Daifeng Li, Yuxuan Zhang, Yangyang Feng, Bowen Deng, Weixu Chen</strong></p>
<p>Large language models (LLMs) encounter difficulties in knowledge-intensive multi-step reasoning (KIMSR) tasks. One challenge is how to effectively extract and represent rationale evidence. The current methods often extract semantically relevant but logically irrelevant evidence, resulting in flawed reasoning and inaccurate responses. We propose a two-way evidence self-alignment (TW-ESA) module, which utilizes the mutual alignment between strict reasoning and LLM reasoning to enhance its understanding of the causal logic of evidence, thereby addressing the first challenge. Another challenge is how to utilize the rationale evidence and LLMâ€™s intrinsic knowledge for accurate reasoning when the evidence contains uncertainty. We propose a dual-gated reasoning enhancement (DGR) module to gradually fuse useful knowledge of LLM within strict reasoning, which can enable the model to perform accurate reasoning by focusing on causal elements in the evidence and exhibit greater robustness. The two modules are collaboratively trained in a unified framework ESA-DGR. Extensive experiments on three diverse and challenging KIMSR datasets reveal that ESA-DGR significantly surpasses state-of-the-art LLM-based fine-tuning methods, with remarkable average improvements of 4% in exact match (EM) and 5% in F1 score. The implementation code is available at <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/ESA-DGR-2BF8">https://anonymous.4open.science/r/ESA-DGR-2BF8</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨çŸ¥è¯†å¯†é›†å‹å¤šæ­¥æ¨ç†ï¼ˆKIMSRï¼‰ä»»åŠ¡ä¸­é‡åˆ°äº†å›°éš¾ã€‚å…¶ä¸­ä¸€ä¸ªæŒ‘æˆ˜æ˜¯å¦‚ä½•æœ‰æ•ˆåœ°æå–å’Œè¡¨ç¤ºç†æ€§è¯æ®ã€‚å½“å‰çš„æ–¹æ³•ç»å¸¸æå–è¯­ä¹‰ä¸Šç›¸å…³ä½†é€»è¾‘ä¸Šä¸ç›¸å…³çš„è¯æ®ï¼Œå¯¼è‡´æ¨ç†ç¼ºé™·å’Œå“åº”ä¸å‡†ç¡®ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŒå‘è¯æ®è‡ªæˆ‘å¯¹é½ï¼ˆTW-ESAï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—åˆ©ç”¨ä¸¥æ ¼æ¨ç†å’ŒLLMæ¨ç†ä¹‹é—´çš„ç›¸äº’å¯¹é½ï¼Œä»¥å¢å¼ºå…¶å¯¹è¯æ®å› æœé€»è¾‘çš„ç†è§£ï¼Œä»è€Œè§£å†³ç¬¬ä¸€ä¸ªæŒ‘æˆ˜ã€‚å¦ä¸€ä¸ªæŒ‘æˆ˜æ˜¯ï¼Œå½“è¯æ®åŒ…å«ä¸ç¡®å®šæ€§æ—¶ï¼Œå¦‚ä½•åˆ©ç”¨ç†æ€§è¯æ®å’ŒLLMçš„å†…åœ¨çŸ¥è¯†è¿›è¡Œå‡†ç¡®æ¨ç†ã€‚æˆ‘ä»¬æå‡ºäº†åŒé—¨æ§æ¨ç†å¢å¼ºï¼ˆDGRï¼‰æ¨¡å—ï¼Œä»¥é€æ­¥èåˆLLMçš„æœ‰ç”¨çŸ¥è¯†äºä¸¥æ ¼æ¨ç†ä¹‹ä¸­ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿé€šè¿‡å…³æ³¨è¯æ®ä¸­çš„å› æœå…ƒç´ è¿›è¡Œå‡†ç¡®æ¨ç†ï¼Œå¹¶è¡¨ç°å‡ºæ›´å¤§çš„ç¨³å¥æ€§ã€‚ä¸¤ä¸ªæ¨¡å—åœ¨ç»Ÿä¸€æ¡†æ¶ESA-DGRä¸­è¿›è¡ŒååŒè®­ç»ƒã€‚åœ¨ä¸‰ä¸ªå¤šæ ·ä¸”å…·æŒ‘æˆ˜æ€§çš„KIMSRæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒESA-DGRæ˜¾è‘—è¶…è¶Šäº†åŸºäºLLMçš„æœ€æ–°å¾®è°ƒæ–¹æ³•ï¼Œåœ¨ç²¾ç¡®åŒ¹é…ï¼ˆEMï¼‰æ–¹é¢å¹³å‡æé«˜äº†4%ï¼Œåœ¨F1åˆ†æ•°æ–¹é¢æé«˜äº†5%ã€‚å®ç°ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/ESA-DGR-2BF8%E6%89%BE%E5%88%B0%E3%80%82">https://anonymous.4open.science/r/ESA-DGR-2BF8æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16806v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šå¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šæ­¥çŸ¥è¯†æ¨ç†ä»»åŠ¡ä¸­é¢ä¸´æŒ‘æˆ˜ï¼Œå­˜åœ¨è¯æ®ç†æ€§æå–å’Œè¡¨ç¤ºçš„é—®é¢˜ã€‚æå‡ºä¸€ç§åŒå‘è¯æ®è‡ªæˆ‘å¯¹é½æ¨¡å—ï¼ˆTW-ESAï¼‰ï¼Œåˆ©ç”¨ä¸¥æ ¼æ¨ç†ä¸LLMæ¨ç†ä¹‹é—´çš„ç›¸äº’å¯¹é½ï¼Œæé«˜å…¶å¯¹å› æœé€»è¾‘çš„ç†è§£ã€‚å¦ä¸€æŒ‘æˆ˜æ˜¯è¯æ®ä¸­çš„ä¸ç¡®å®šæ€§å¦‚ä½•åˆ©ç”¨LLMçš„å†…åœ¨çŸ¥è¯†è¿›è¡Œå‡†ç¡®æ¨ç†ã€‚æå‡ºä¸€ç§åŒé—¨æ¨ç†å¢å¼ºï¼ˆDGRï¼‰æ¨¡å—ï¼Œä½¿æ¨¡å‹é€šè¿‡å…³æ³¨è¯æ®ä¸­çš„å› æœå…ƒç´ è¿›è¡Œå‡†ç¡®æ¨ç†ï¼Œå¹¶å±•ç°æ›´å¼ºçš„ç¨³å¥æ€§ã€‚ä¸¤ä¸ªæ¨¡å—åœ¨ç»Ÿä¸€æ¡†æ¶ESA-DGRä¸­ååŒè®­ç»ƒï¼Œåœ¨ä¸‰ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„KIMSRæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒESA-DGRæ˜¾è‘—ä¼˜äºæœ€æ–°çš„LLMå¾®è°ƒæ–¹æ³•ï¼Œåœ¨ç²¾ç¡®åŒ¹é…å’ŒF1åˆ†æ•°ä¸Šå¹³å‡æé«˜4%å’Œ5%ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨çŸ¥è¯†å¯†é›†å‹å¤šæ­¥æ¨ç†ï¼ˆKIMSRï¼‰ä»»åŠ¡ä¸­é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¸¸å¸¸æå–è¯­ä¹‰ä¸Šç›¸å…³ä½†é€»è¾‘ä¸Šæ— å…³çš„è¯æ®ï¼Œå¯¼è‡´æ¨ç†é”™è¯¯å’Œå“åº”ä¸å‡†ç¡®ã€‚</li>
<li>æå‡ºä¸€ç§åŒå‘è¯æ®è‡ªæˆ‘å¯¹é½ï¼ˆTW-ESAï¼‰æ¨¡å—ï¼Œé€šè¿‡ä¸¥æ ¼æ¨ç†ä¸LLMæ¨ç†çš„ç›¸äº’å¯¹é½ï¼Œæé«˜å› æœé€»è¾‘çš„ç†è§£ã€‚</li>
<li>å¦ä¸€æŒ‘æˆ˜æ˜¯å¦‚ä½•å¤„ç†è¯æ®ä¸­çš„ä¸ç¡®å®šæ€§ï¼Œå¹¶åˆ©ç”¨LLMçš„å†…åœ¨çŸ¥è¯†è¿›è¡Œå‡†ç¡®æ¨ç†ã€‚</li>
<li>æå‡ºä¸€ç§åŒé—¨æ¨ç†å¢å¼ºï¼ˆDGRï¼‰æ¨¡å—ï¼Œèƒ½é€æ­¥èåˆLLMçš„æœ‰ç”¨çŸ¥è¯†è¿›è¡Œå‡†ç¡®æ¨ç†ï¼Œå¹¶å±•ç°æ›´å¼ºçš„ç¨³å¥æ€§ã€‚</li>
<li>ESA-DGRæ¡†æ¶èåˆäº†TW-ESAå’ŒDGRä¸¤ä¸ªæ¨¡å—ï¼Œå¹¶åœ¨å¤šä¸ªKIMSRæ•°æ®é›†ä¸Šè¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16806">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-89bf19341603fe75672cafced523b0aa.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6f9b7f492e515d16192e3091c64355c4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c883246145aa35c0317336ff1038fcca.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Reasoning-Beyond-Language-A-Comprehensive-Survey-on-Latent-Chain-of-Thought-Reasoning"><a href="#Reasoning-Beyond-Language-A-Comprehensive-Survey-on-Latent-Chain-of-Thought-Reasoning" class="headerlink" title="Reasoning Beyond Language: A Comprehensive Survey on Latent   Chain-of-Thought Reasoning"></a>Reasoning Beyond Language: A Comprehensive Survey on Latent   Chain-of-Thought Reasoning</h2><p><strong>Authors:Xinghao Chen, Anhao Zhao, Heming Xia, Xuan Lu, Hanlin Wang, Yanjun Chen, Wei Zhang, Jian Wang, Wenjie Li, Xiaoyu Shen</strong></p>
<p>Large Language Models (LLMs) have achieved impressive performance on complex reasoning tasks with Chain-of-Thought (CoT) prompting. However, conventional CoT relies on reasoning steps explicitly verbalized in natural language, introducing inefficiencies and limiting its applicability to abstract reasoning. To address this, there has been growing research interest in latent CoT reasoning, where inference occurs within latent spaces. By decoupling reasoning from language, latent reasoning promises richer cognitive representations and more flexible, faster inference. Researchers have explored various directions in this promising field, including training methodologies, structural innovations, and internal reasoning mechanisms. This paper presents a comprehensive overview and analysis of this reasoning paradigm. We begin by proposing a unified taxonomy from four perspectives: token-wise strategies, internal mechanisms, analysis, and applications. We then provide in-depth discussions and comparative analyses of representative methods, highlighting their design patterns, strengths, and open challenges. We aim to provide a structured foundation for advancing this emerging direction in LLM reasoning. The relevant papers will be regularly updated at <a target="_blank" rel="noopener" href="https://github.com/EIT-NLP/Awesome-Latent-CoT">https://github.com/EIT-NLP/Awesome-Latent-CoT</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡æ€ç»´é“¾ï¼ˆCoTï¼‰æç¤ºï¼Œåœ¨å¤æ‚çš„æ¨ç†ä»»åŠ¡ä¸Šå–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„CoTä¾èµ–äºè‡ªç„¶è¯­è¨€æ˜ç¡®è¡¨è¾¾çš„æ¨ç†æ­¥éª¤ï¼Œè¿™å¼•å…¥äº†ä½æ•ˆæ€§å¹¶é™åˆ¶äº†å…¶åœ¨æŠ½è±¡æ¨ç†ä¸­çš„åº”ç”¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œäººä»¬å¯¹æ½œåœ¨CoTæ¨ç†çš„ç ”ç©¶å…´è¶£æ—¥ç›Šæµ“åšï¼Œæ¨ç†è¿‡ç¨‹å‘ç”Ÿåœ¨æ½œåœ¨ç©ºé—´å†…ã€‚é€šè¿‡å°†æ¨ç†ä¸è¯­è¨€è§£è€¦ï¼Œæ½œåœ¨æ¨ç†æœ‰æœ›å®ç°æ›´ä¸°å¯Œçš„è®¤çŸ¥è¡¨ç¤ºå’Œæ›´çµæ´»ã€æ›´å¿«çš„æ¨ç†ã€‚ç ”ç©¶äººå‘˜åœ¨è¿™ä¸ªå……æ»¡å¸Œæœ›çš„é¢†åŸŸæ¢ç´¢äº†å„ä¸ªæ–¹å‘ï¼ŒåŒ…æ‹¬è®­ç»ƒæ–¹æ³•è®ºã€ç»“æ„åˆ›æ–°ä»¥åŠå†…éƒ¨æ¨ç†æœºåˆ¶ã€‚æœ¬æ–‡å¯¹è¿™ä¸€æ¨ç†èŒƒå¼è¿›è¡Œäº†å…¨é¢æ¦‚è¿°å’Œåˆ†æã€‚æˆ‘ä»¬é¦–å…ˆæå‡ºä»å››ä¸ªæ–¹é¢ï¼šåŸºäºä»£å¸çš„ç­–ç•¥ã€å†…éƒ¨æœºåˆ¶ã€åˆ†æä¸åº”ç”¨æ¥è¿›è¡Œç»Ÿä¸€åˆ†ç±»ã€‚ç„¶åæ·±å…¥è®¨è®ºå’Œæ¯”è¾ƒåˆ†æä»£è¡¨æ€§çš„æ–¹æ³•ï¼Œçªå‡ºå…¶è®¾è®¡æ¨¡å¼ã€ä¼˜ç‚¹å’Œå¼€æ”¾æŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯ä¸ºæ¨åŠ¨LLMæ¨ç†çš„è¿™ä¸€æ–°å…´æ–¹å‘æä¾›ä¸€ä¸ªç»“æ„åŒ–çš„åŸºç¡€ã€‚ç›¸å…³è®ºæ–‡å°†å®šæœŸæ›´æ–°åœ¨<a target="_blank" rel="noopener" href="https://github.com/EIT-NLP/Awesome-Latent-CoT%E4%B8%8A%E3%80%82">https://github.com/EIT-NLP/Awesome-Latent-CoTä¸Šã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16782v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šè¿‡é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºæˆåŠŸå®Œæˆäº†å¤æ‚çš„æ¨ç†ä»»åŠ¡ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„CoTä¾èµ–äºè‡ªç„¶è¯­è¨€ä¸­çš„æ˜ç¡®è¡¨è¿°æ­¥éª¤ï¼Œå¼•å…¥æ•ˆç‡ä½ä¸‹ï¼Œé™åˆ¶äº†å…¶åœ¨æŠ½è±¡æ¨ç†ä¸­çš„åº”ç”¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç ”ç©¶è€…å¯¹éšæ€§CoTæ¨ç†äº§ç”Ÿäº†æµ“åšçš„å…´è¶£ï¼Œæ¨ç†è¿‡ç¨‹å‘ç”Ÿåœ¨æ½œåœ¨ç©ºé—´å†…ã€‚é€šè¿‡å°†æ¨ç†ä¸è¯­è¨€è§£è€¦ï¼Œæ½œåœ¨æ¨ç†æä¾›äº†æ›´ä¸°å¯Œè®¤çŸ¥è¡¨ç¤ºå’Œæ›´çµæ´»ã€å¿«é€Ÿçš„æ¨ç†è¿‡ç¨‹ã€‚æœ¬æ–‡ä»å››ä¸ªæ–¹é¢å¯¹éšæ€§æ¨ç†æ¨¡å¼è¿›è¡Œäº†å…¨é¢æ¦‚è¿°å’Œåˆ†æï¼šåŸºäºæ ‡è®°çš„ç­–ç•¥ã€å†…éƒ¨æœºåˆ¶ã€åˆ†æå’Œåº”ç”¨ã€‚å¹¶å¯¹ä»£è¡¨æ€§æ–¹æ³•è¿›è¡Œäº†æ·±å…¥è®¨è®ºå’Œæ¯”è¾ƒåˆ†æï¼Œçªå‡ºäº†å…¶è®¾è®¡æ¨¡å¼ã€ä¼˜åŠ¿å’Œå¼€æ”¾æŒ‘æˆ˜ã€‚æˆ‘ä»¬æ—¨åœ¨ä¸ºæ¨è¿›è¿™ä¸€æ–°å…´çš„è¯­è¨€æ¨¡å‹æ¨ç†æ–¹å‘æä¾›ç»“æ„åŒ–åŸºç¡€ã€‚ç›¸å…³è®ºæ–‡å°†å®šæœŸåœ¨<a target="_blank" rel="noopener" href="https://github.com/EIT-NLP/Awesome-Latent-CoT%E6%9B%B4%E6%96%B0%E3%80%82">https://github.com/EIT-NLP/Awesome-Latent-CoTæ›´æ–°ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šè¿‡é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºå±•ç°å‡ºå¼ºå¤§çš„å¤æ‚æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ä¼ ç»ŸCoTä¾èµ–è‡ªç„¶è¯­è¨€ä¸­çš„æ˜ç¡®è¡¨è¿°æ­¥éª¤ï¼Œå­˜åœ¨æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚</li>
<li>éšæ€§CoTæ¨ç†æ˜¯æ–°å…´ç ”ç©¶æ–¹å‘ï¼Œå…è®¸åœ¨æ½œåœ¨ç©ºé—´å†…å®Œæˆæ¨ç†è¿‡ç¨‹ã€‚</li>
<li>éšæ€§æ¨ç†è§£è€¦äº†æ¨ç†ä¸è¯­è¨€ï¼Œä½¿è®¤çŸ¥è¡¨ç¤ºæ›´ä¸°å¯Œï¼Œæ¨ç†æ›´çµæ´»å’Œå¿«é€Ÿã€‚</li>
<li>è®ºæ–‡ä»å››ä¸ªè§’åº¦å¯¹éšæ€§æ¨ç†æ¨¡å¼è¿›è¡Œäº†å…¨é¢æ¦‚è¿°å’Œåˆ†æï¼šåŸºäºæ ‡è®°çš„ç­–ç•¥ã€å†…éƒ¨æœºåˆ¶ã€åˆ†æå’Œåº”ç”¨ã€‚</li>
<li>è®ºæ–‡æ·±å…¥è®¨è®ºå¹¶æ¯”è¾ƒäº†ä»£è¡¨æ€§æ–¹æ³•çš„è®¾è®¡æ¨¡å¼ã€ä¼˜åŠ¿å’ŒæŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16782">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f74c2ce84f0963d0bb3b2f4a9cf684eb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6c3120eae739e8488d9bacc949f742e5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-865673c06eb7e8ac9c5dbfc74b789ee6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-642d02308bfffb4eacf3f879740c80bf.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="RBench-V-A-Primary-Assessment-for-Visual-Reasoning-Models-with-Multi-modal-Outputs"><a href="#RBench-V-A-Primary-Assessment-for-Visual-Reasoning-Models-with-Multi-modal-Outputs" class="headerlink" title="RBench-V: A Primary Assessment for Visual Reasoning Models with   Multi-modal Outputs"></a>RBench-V: A Primary Assessment for Visual Reasoning Models with   Multi-modal Outputs</h2><p><strong>Authors:Meng-Hao Guo, Xuanyu Chu, Qianrui Yang, Zhe-Han Mo, Yiqing Shen, Pei-lin Li, Xinjie Lin, Jinnian Zhang, Xin-Sheng Chen, Yi Zhang, Kiyohiro Nakayama, Zhengyang Geng, Houwen Peng, Han Hu, Shi-Nin Hu</strong></p>
<p>The rapid advancement of native multi-modal models and omni-models, exemplified by GPT-4o, Gemini, and o3, with their capability to process and generate content across modalities such as text and images, marks a significant milestone in the evolution of intelligence. Systematic evaluation of their multi-modal output capabilities in visual thinking processes (also known as multi-modal chain of thought, M-CoT) becomes critically important. However, existing benchmarks for evaluating multi-modal models primarily focus on assessing multi-modal inputs and text-only reasoning while neglecting the importance of reasoning through multi-modal outputs. In this paper, we present a benchmark, dubbed RBench-V, designed to assess modelsâ€™ vision-indispensable reasoning abilities. To construct RBench-V, we carefully hand-pick 803 questions covering math, physics, counting, and games. Unlike previous benchmarks that typically specify certain input modalities, RBench-V presents problems centered on multi-modal outputs, which require image manipulation such as generating novel images and constructing auxiliary lines to support the reasoning process. We evaluate numerous open- and closed-source models on RBench-V, including o3, Gemini 2.5 Pro, Qwen2.5-VL, etc. Even the best-performing model, o3, achieves only 25.8% accuracy on RBench-V, far below the human score of 82.3%, highlighting that current models struggle to leverage multi-modal reasoning. Data and code are available at <a target="_blank" rel="noopener" href="https://evalmodels.github.io/rbenchv">https://evalmodels.github.io/rbenchv</a> </p>
<blockquote>
<p>éšç€æœ¬åœ°å¤šæ¨¡æ€æ¨¡å‹å’Œå…¨èƒ½æ¨¡å‹ï¼ˆå¦‚GPT-4oã€åŒå­åº§å’Œo3ï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œå®ƒä»¬å…·å¤‡å¤„ç†ç”Ÿæˆæ–‡æœ¬å’Œå›¾åƒç­‰å¤šæ¨¡æ€å†…å®¹çš„èƒ½åŠ›ï¼Œæ ‡å¿—ç€æ™ºèƒ½é¢†åŸŸçš„ä¸€ä¸ªé‡è¦é‡Œç¨‹ç¢‘ã€‚å¯¹ä»–ä»¬åœ¨è§†è§‰æ€ç»´è¿‡ç¨‹ä¸­çš„å¤šæ¨¡æ€è¾“å‡ºèƒ½åŠ›ï¼ˆä¹Ÿç§°ä¸ºå¤šæ¨¡æ€æ€ç»´é“¾ï¼ŒM-CoTï¼‰è¿›è¡Œç³»ç»Ÿæ€§è¯„ä»·å˜å¾—è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤šæ¨¡æ€æ¨¡å‹è¯„ä¼°åŸºå‡†ä¸»è¦å…³æ³¨å¤šæ¨¡æ€è¾“å…¥å’Œçº¯æ–‡æœ¬æ¨ç†ï¼Œå¿½è§†äº†é€šè¿‡å¤šæ¨¡æ€è¾“å‡ºæ¥è¿›è¡Œæ¨ç†çš„é‡è¦æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªè¯„ä¼°åŸºå‡†ï¼Œåä¸ºRBench-Vï¼Œæ—¨åœ¨è¯„ä¼°æ¨¡å‹çš„è§†è§‰ä¸å¯æˆ–ç¼ºæ¨ç†èƒ½åŠ›ã€‚ä¸ºäº†æ„å»ºRBench-Vï¼Œæˆ‘ä»¬ç²¾å¿ƒæŒ‘é€‰äº†803ä¸ªæ¶µç›–æ•°å­¦ã€ç‰©ç†ã€è®¡æ•°å’Œæ¸¸æˆçš„é—®é¢˜ã€‚ä¸é€šå¸¸æŒ‡å®šç‰¹å®šè¾“å…¥æ¨¡æ€çš„å…ˆå‰åŸºå‡†ä¸åŒï¼ŒRBench-Vä»¥å¤šæ¨¡æ€è¾“å‡ºä¸ºä¸­å¿ƒå‘ˆç°é—®é¢˜ï¼Œéœ€è¦è¿›è¡Œå›¾åƒæ“ä½œï¼Œå¦‚ç”Ÿæˆæ–°å›¾åƒå’Œæ„å»ºè¾…åŠ©çº¿ä»¥æ”¯æŒæ¨ç†è¿‡ç¨‹ã€‚æˆ‘ä»¬åœ¨RBench-Vä¸Šè¯„ä¼°äº†å¤šä¸ªå¼€æºå’Œé—­æºæ¨¡å‹ï¼ŒåŒ…æ‹¬o3ã€åŒå­åº§2.5 Proã€Qwen2.5-VLç­‰ã€‚å³ä½¿åœ¨è¡¨ç°æœ€ä½³çš„o3æ¨¡å‹ä¸Šï¼Œå…¶åœ¨RBench-Vä¸Šçš„å‡†ç¡®ç‡ä¹Ÿåªæœ‰25.8%ï¼Œè¿œä½äºäººç±»çš„82.3%ï¼Œè¿™è¡¨æ˜å½“å‰æ¨¡å‹åœ¨åˆ©ç”¨å¤šæ¨¡æ€æ¨ç†æ–¹é¢å­˜åœ¨å›°éš¾ã€‚æ•°æ®å’Œä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://evalmodels.github.io/rbenchv%E8%8E%B7%E5%8F%96%E3%80%82">https://evalmodels.github.io/rbenchvè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16770v1">PDF</a> 12 pages</p>
<p><strong>Summary</strong></p>
<p>éšç€æœ¬åœŸå¤šæ¨¡æ€æ¨¡å‹å’Œé€šæ‰æ¨¡å‹ï¼ˆå¦‚GPT-4oã€Geminiå’Œo3ï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œå®ƒä»¬å¤„ç†å¹¶ç”Ÿæˆæ–‡æœ¬å’Œå›¾åƒç­‰å¤šæ¨¡æ€å†…å®¹çš„èƒ½åŠ›ï¼Œæ ‡å¿—ç€äººå·¥æ™ºèƒ½é¢†åŸŸçš„é‡è¦é‡Œç¨‹ç¢‘ã€‚ç³»ç»Ÿè¯„ä¼°è¿™äº›æ¨¡å‹åœ¨è§†è§‰æ€ç»´è¿‡ç¨‹ï¼ˆä¹Ÿç§°ä¸ºå¤šæ¨¡æ€æ€ç»´é“¾ï¼ŒM-CoTï¼‰ä¸­çš„å¤šæ¨¡æ€è¾“å‡ºèƒ½åŠ›å˜å¾—è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è¯„ä¼°åŸºå‡†ä¸»è¦å…³æ³¨å¤šæ¨¡æ€è¾“å…¥å’Œçº¯æ–‡æœ¬æ¨ç†ï¼Œå¿½è§†äº†é€šè¿‡å¤šæ¨¡æ€è¾“å‡ºæ¥è¿›è¡Œæ¨ç†çš„é‡è¦æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºRBench-Vçš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°æ¨¡å‹çš„è§†è§‰ä¸å¯æˆ–ç¼ºæ¨ç†èƒ½åŠ›ã€‚è¯¥æµ‹è¯•é€šè¿‡ç²¾å¿ƒæŒ‘é€‰çš„803ä¸ªæ¶‰åŠæ•°å­¦ã€ç‰©ç†ã€è®¡æ•°å’Œæ¸¸æˆçš„é—®é¢˜æ¥æ„å»ºï¼Œè¦æ±‚æ¨¡å‹è¿›è¡Œå›¾åƒæ“ä½œä»¥æ”¯æŒæ¨ç†è¿‡ç¨‹ã€‚å³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨RBench-Vä¸Šçš„å‡†ç¡®ç‡ä¹Ÿåªæœ‰25.8%ï¼Œè¿œä½äºäººç±»çš„82.3%ï¼Œçªæ˜¾å‡ºç°æœ‰æ¨¡å‹åœ¨å¤šæ¨¡æ€æ¨ç†æ–¹é¢çš„ä¸è¶³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€æ¨¡å‹å’Œé€šæ‰æ¨¡å‹çš„å‘å±•ä¸ºäººå·¥æ™ºèƒ½é¢†åŸŸå¸¦æ¥é‡è¦é‡Œç¨‹ç¢‘ã€‚</li>
<li>ç³»ç»Ÿè¯„ä¼°æ¨¡å‹åœ¨è§†è§‰æ€ç»´è¿‡ç¨‹ï¼ˆå¤šæ¨¡æ€æ€ç»´é“¾ï¼‰ä¸­çš„å¤šæ¨¡æ€è¾“å‡ºèƒ½åŠ›è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰è¯„ä¼°åŸºå‡†ä¸»è¦å…³æ³¨å¤šæ¨¡æ€è¾“å…¥å’Œæ–‡æœ¬æ¨ç†ï¼Œå¿½è§†å¤šæ¨¡æ€è¾“å‡ºçš„é‡è¦æ€§ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†RBench-VåŸºå‡†æµ‹è¯•ï¼Œä¸“æ³¨äºè¯„ä¼°æ¨¡å‹çš„è§†è§‰ä¸å¯æˆ–ç¼ºæ¨ç†èƒ½åŠ›ã€‚</li>
<li>RBench-VåŒ…å«803ä¸ªæ¶‰åŠæ•°å­¦ã€ç‰©ç†ã€è®¡æ•°å’Œæ¸¸æˆçš„é—®é¢˜ã€‚</li>
<li>æ¨¡å‹éœ€è¦æ‰§è¡Œå›¾åƒæ“ä½œä»¥æ”¯æŒæ¨ç†è¿‡ç¨‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16770">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-003990ed86416dc235075597113c4bc6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ac7c9dfe1dffe6a12145c8eb59d34511.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-713fa61f2e0163bb99036364f017b028.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-78afb633fef56dc3200a87c32a6e1245.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-77a3e3cdcc0baee7cf65c2d379fb9845.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6da8c50bc28e93af7a6dab55fed4d9b8.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="SPaRC-A-Spatial-Pathfinding-Reasoning-Challenge"><a href="#SPaRC-A-Spatial-Pathfinding-Reasoning-Challenge" class="headerlink" title="SPaRC: A Spatial Pathfinding Reasoning Challenge"></a>SPaRC: A Spatial Pathfinding Reasoning Challenge</h2><p><strong>Authors:Lars Benedikt Kaesberg, Jan Philip Wahle, Terry Ruas, Bela Gipp</strong></p>
<p>Existing reasoning datasets saturate and fail to test abstract, multi-step problems, especially pathfinding and complex rule constraint satisfaction. We introduce SPaRC (Spatial Pathfinding Reasoning Challenge), a dataset of 1,000 2D grid pathfinding puzzles to evaluate spatial and symbolic reasoning, requiring step-by-step planning with arithmetic and geometric rules. Humans achieve near-perfect accuracy (98.0%; 94.5% on hard puzzles), while the best reasoning models, such as o4-mini, struggle (15.8%; 1.1% on hard puzzles). Models often generate invalid paths (&gt;50% of puzzles for o4-mini), and reasoning tokens reveal they make errors in navigation and spatial logic. Unlike humans, who take longer on hard puzzles, models fail to scale test-time compute with difficulty. Allowing models to make multiple solution attempts improves accuracy, suggesting potential for better spatial reasoning with improved training and efficient test-time scaling methods. SPaRC can be used as a window into modelsâ€™ spatial reasoning limitations and drive research toward new methods that excel in abstract, multi-step problem-solving. </p>
<blockquote>
<p>ç°æœ‰çš„æ¨ç†æ•°æ®é›†è¶‹äºé¥±å’Œï¼Œå¹¶ä¸”åœ¨æµ‹è¯•æŠ½è±¡ã€å¤šæ­¥éª¤é—®é¢˜æ—¶è¡¨ç°ä¸ä½³ï¼Œå°¤å…¶æ˜¯åœ¨è·¯å¾„æŸ¥æ‰¾å’Œå¤æ‚è§„åˆ™çº¦æŸæ»¡è¶³æ–¹é¢ã€‚æˆ‘ä»¬æ¨å‡ºäº†SPaRCï¼ˆç©ºé—´è·¯å¾„æŸ¥æ‰¾æ¨ç†æŒ‘æˆ˜ï¼‰æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«1000ä¸ªäºŒç»´ç½‘æ ¼è·¯å¾„æŸ¥æ‰¾è°œé¢˜ï¼Œç”¨äºè¯„ä¼°ç©ºé—´ä¸ç¬¦å·æ¨ç†çš„èƒ½åŠ›ï¼Œéœ€è¦æŒ‰æ­¥éª¤è§„åˆ’ï¼Œæ¶‰åŠç®—æœ¯å’Œå‡ ä½•è§„åˆ™ã€‚äººç±»åœ¨è°œé¢˜ä¸Šçš„å‡†ç¡®ç‡æ¥è¿‘å®Œç¾ï¼ˆ98.0%ï¼Œåœ¨éš¾é¢˜ä¸Šä¸º94.5%ï¼‰ï¼Œè€Œæœ€ä½³çš„æ¨ç†æ¨¡å‹ï¼ˆå¦‚o4-miniï¼‰è¡¨ç°æŒ£æ‰ï¼ˆå‡†ç¡®ç‡ä¸º15.8%ï¼Œåœ¨éš¾é¢˜ä¸Šä»…ä¸º1.1%ï¼‰ã€‚æ¨¡å‹ç»å¸¸ç”Ÿæˆæ— æ•ˆè·¯å¾„ï¼ˆå¯¹äºo4-miniè€Œè¨€è¶…è¿‡50%ï¼‰ï¼Œæ¨ç†ä»¤ç‰Œæ˜¾ç¤ºå®ƒä»¬åœ¨å¯¼èˆªå’Œç©ºé—´é€»è¾‘æ–¹é¢çŠ¯äº†é”™è¯¯ã€‚ä¸åŒäºäººç±»åœ¨éš¾é¢˜ä¸ŠèŠ±è´¹æ›´å¤šæ—¶é—´ï¼Œæ¨¡å‹éš¾ä»¥éšç€éš¾åº¦çš„å¢åŠ è°ƒæ•´æµ‹è¯•æ—¶é—´è®¡ç®—ã€‚å…è®¸æ¨¡å‹è¿›è¡Œå¤šæ¬¡è§£é¢˜å°è¯•å¯æé«˜å‡†ç¡®æ€§ï¼Œè¿™è¡¨æ˜é€šè¿‡æ”¹è¿›è®­ç»ƒå’Œé«˜æ•ˆçš„æµ‹è¯•æ—¶é—´ç¼©æ”¾æ–¹æ³•ï¼Œæœ‰å¯èƒ½å®ç°æ›´å¥½çš„ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚SPaRCå¯ä»¥ä½œä¸ºäº†è§£æ¨¡å‹ç©ºé—´æ¨ç†å±€é™æ€§çš„çª—å£ï¼Œå¹¶æ¨åŠ¨ç ”ç©¶å¼€å‘å‡ºæ“…é•¿æŠ½è±¡ã€å¤šæ­¥éª¤é—®é¢˜è§£å†³çš„æ–°æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16686v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†SPaRCæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«1000ä¸ªäºŒç»´ç½‘æ ¼è·¯å¾„å¯»æ‰¾è°œé¢˜ï¼Œæ—¨åœ¨è¯„ä¼°ç©ºé—´ä¸ç¬¦å·æ¨ç†èƒ½åŠ›ï¼Œéœ€è¦é€æ­¥è§„åˆ’å¹¶éµå¾ªç®—æœ¯å’Œå‡ ä½•è§„åˆ™ã€‚äººç±»åœ¨æ­¤æ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡æ¥è¿‘å®Œç¾ï¼ˆ98.0%ï¼Œéš¾é¢˜ä¸Š94.5%ï¼‰ï¼Œè€Œæœ€ä½³æ¨ç†æ¨¡å‹ï¼ˆå¦‚o4-miniï¼‰è¡¨ç°è¾ƒå·®ï¼ˆ15.8%ï¼Œéš¾é¢˜ä¸Š1.1%ï¼‰ã€‚æ¨¡å‹å¸¸å¸¸ç”Ÿæˆæ— æ•ˆè·¯å¾„ï¼Œä¸”åœ¨å¯¼èˆªå’Œç©ºé—´é€»è¾‘æ–¹é¢å‡ºç°é”™è¯¯ã€‚æ¨¡å‹æœªèƒ½éšéš¾åº¦å¢åŠ è°ƒæ•´æµ‹è¯•æ—¶é—´è®¡ç®—ï¼Œä¸äººç±»ä¸åŒï¼Œåè€…åœ¨éš¾é¢˜ä¸ŠèŠ±è´¹æ›´å¤šæ—¶é—´ã€‚å…è®¸æ¨¡å‹å¤šæ¬¡å°è¯•è§£ç­”å¯æé«˜å‡†ç¡®ç‡ï¼Œè¡¨æ˜é€šè¿‡æ”¹è¿›è®­ç»ƒå’Œé«˜æ•ˆæµ‹è¯•æ—¶é—´ç¼©æ”¾æ–¹æ³•ï¼Œæ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›æœ‰æœ›æé«˜ã€‚SPaRCå¯ä»¥ä½œä¸ºäº†è§£æ¨¡å‹ç©ºé—´æ¨ç†å±€é™æ€§çš„çª—å£ï¼Œå¹¶æ¨åŠ¨ç ”ç©¶ä»¥å¼€å‘æ“…é•¿æŠ½è±¡å¤šæ­¥éª¤é—®é¢˜è§£å†³çš„æ–°æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SPaRCæ•°æ®é›†ç”¨äºè¯„ä¼°ç©ºé—´å’Œç¬¦å·æ¨ç†èƒ½åŠ›ï¼ŒåŒ…å«1000ä¸ªäºŒç»´ç½‘æ ¼è·¯å¾„å¯»æ‰¾è°œé¢˜ã€‚</li>
<li>äººç±»åœ¨SPaRCæ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡å¾ˆé«˜ï¼ˆ98.0%ï¼Œéš¾é¢˜ä¸Šä¸º94.5%ï¼‰ã€‚</li>
<li>å½“å‰æœ€ä½³æ¨ç†æ¨¡å‹ï¼ˆå¦‚o4-miniï¼‰åœ¨SPaRCä¸Šçš„è¡¨ç°è¾ƒå·®ï¼Œå¸¸å¸¸ç”Ÿæˆæ— æ•ˆè·¯å¾„ã€‚</li>
<li>æ¨¡å‹åœ¨å¯¼èˆªå’Œç©ºé—´é€»è¾‘æ–¹é¢å­˜åœ¨é”™è¯¯ã€‚</li>
<li>æ¨¡å‹æœªèƒ½éšç€é—®é¢˜éš¾åº¦çš„å¢åŠ è°ƒæ•´å…¶æµ‹è¯•æ—¶é—´çš„è®¡ç®—ã€‚</li>
<li>å…è®¸æ¨¡å‹å¤šæ¬¡å°è¯•è§£ç­”å¯ä»¥æé«˜å…¶å‡†ç¡®ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16686">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f3432cb542ad5fc5fa12de804e3613ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a182ffdb78954744b9948c3d72830132.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-608a5f5ca2ade863ac156419e6434911.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4709eafc8efbab8a2796f7811b81f4db.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a2b2024a9efe6f71f37086cb77b9fc09.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-37b85c8844ffa86970345e73ab723d2e.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-24/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-24/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-24/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-612c734a99dd1fec3a0ba7edf415c57a.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-24  Let Androids Dream of Electric Sheep A Human-like Image Implication   Understanding and Reasoning Framework
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-24
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-22/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-d889979738b094133ac240d75f0a4394.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-22  ClapFM-EVC High-Fidelity and Flexible Emotional Voice Conversion with   Dual Control from Natural Language and Speech
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-22
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33125.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
