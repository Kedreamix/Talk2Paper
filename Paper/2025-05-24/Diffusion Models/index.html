<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-05-24  Pursuing Temporal-Consistent Video Virtual Try-On via Dynamic Pose   Interaction">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-11a3eaa79b0d03650067200e1b8c02ab.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-05-24
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    20.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    82 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-05-24-更新"><a href="#2025-05-24-更新" class="headerlink" title="2025-05-24 更新"></a>2025-05-24 更新</h1><h2 id="Pursuing-Temporal-Consistent-Video-Virtual-Try-On-via-Dynamic-Pose-Interaction"><a href="#Pursuing-Temporal-Consistent-Video-Virtual-Try-On-via-Dynamic-Pose-Interaction" class="headerlink" title="Pursuing Temporal-Consistent Video Virtual Try-On via Dynamic Pose   Interaction"></a>Pursuing Temporal-Consistent Video Virtual Try-On via Dynamic Pose   Interaction</h2><p><strong>Authors:Dong Li, Wenqi Zhong, Wei Yu, Yingwei Pan, Dingwen Zhang, Ting Yao, Junwei Han, Tao Mei</strong></p>
<p>Video virtual try-on aims to seamlessly dress a subject in a video with a specific garment. The primary challenge involves preserving the visual authenticity of the garment while dynamically adapting to the pose and physique of the subject. While existing methods have predominantly focused on image-based virtual try-on, extending these techniques directly to videos often results in temporal inconsistencies. Most current video virtual try-on approaches alleviate this challenge by incorporating temporal modules, yet still overlook the critical spatiotemporal pose interactions between human and garment. Effective pose interactions in videos should not only consider spatial alignment between human and garment poses in each frame but also account for the temporal dynamics of human poses throughout the entire video. With such motivation, we propose a new framework, namely Dynamic Pose Interaction Diffusion Models (DPIDM), to leverage diffusion models to delve into dynamic pose interactions for video virtual try-on. Technically, DPIDM introduces a skeleton-based pose adapter to integrate synchronized human and garment poses into the denoising network. A hierarchical attention module is then exquisitely designed to model intra-frame human-garment pose interactions and long-term human pose dynamics across frames through pose-aware spatial and temporal attention mechanisms. Moreover, DPIDM capitalizes on a temporal regularized attention loss between consecutive frames to enhance temporal consistency. Extensive experiments conducted on VITON-HD, VVT and ViViD datasets demonstrate the superiority of our DPIDM against the baseline methods. Notably, DPIDM achieves VFID score of 0.506 on VVT dataset, leading to 60.5% improvement over the state-of-the-art GPD-VVTO approach. </p>
<blockquote>
<p>视频虚拟试穿旨在无缝地将特定服装添加到视频中的主体上。主要挑战在于在动态适应主体的姿势和体型的同时，保持服装的视觉真实性。虽然现有方法主要集中在基于图像的虚拟试穿上，但直接将这些技术扩展到视频通常会导致时间上的不一致。当前大多数视频虚拟试穿方法通过引入时间模块来缓解这一挑战，但仍然忽略了人与服装之间关键的空间时间姿势交互。视频中的有效姿势交互不仅要考虑每一帧中人与服装姿势的空间对齐，还要考虑整个视频中人的姿势的动态变化。基于这样的动机，我们提出了一种新的框架，即动态姿势交互扩散模型（DPIDM），利用扩散模型深入研究视频虚拟试穿中的动态姿势交互。技术上，DPIDM引入了一个基于骨架的姿势适配器，将同步的人类和服装姿势集成到去噪网络中。然后精心设计了一个分层注意力模块，以通过姿势感知的空间和时间注意力机制对帧内的人与服装姿势交互以及跨帧的长期人类姿势动态进行建模。此外，DPIDM利用相邻帧之间的时间正则化注意力损失来增强时间一致性。在VITON-HD、VVT和ViViD数据集上进行的广泛实验证明了我们的DPIDM相对于基准方法的优越性。值得注意的是，DPIDM在VVT数据集上的VFID得分达到0.506，相较于目前最先进的GPD-VVTO方法，提升了60.5%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16980v1">PDF</a> CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种新的视频虚拟试穿框架——动态姿态交互扩散模型（DPIDM），该模型利用扩散模型深入探究视频中的动态姿态交互。DPIDM通过引入基于骨架的姿态适配器和层次注意力模块，实现了对帧内人与衣物姿态的精细建模，并考虑了跨帧的长期人体姿态动态。此外，DPIDM采用临时正则化注意力损失来增强时序一致性，在多个数据集上的实验表明其优越性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视频虚拟试穿旨在无缝地将特定服装添加到视频中的主体上。</li>
<li>主要挑战在于在动态适应主体姿势和体型的同时保持服装的视觉真实性。</li>
<li>尽管图像虚拟试穿方法很受欢迎，但直接应用于视频会导致时序不一致性。</li>
<li>当前视频虚拟试穿方法虽然加入时序模块来缓解这个问题，但仍然忽略了关键的空间时间姿态交互。</li>
<li>DPIDM框架利用扩散模型处理动态姿态交互进行视频虚拟试穿。</li>
<li>DPIDM通过引入骨架基于的姿态适配器和层次注意力模块进行精细建模。</li>
<li>DPIDM采用临时正则化注意力损失以增强时序一致性，并在多个数据集上实现了优越的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16980">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-87f9b7ed6326278cca0db76b862880a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a55243a8ded48ece41b92d31fb861b8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48953bcc2e805a7eb1b7ead8db0bf5f7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1462633c78a27d60bb79a906ce5a62f6.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Incorporating-Visual-Correspondence-into-Diffusion-Model-for-Virtual-Try-On"><a href="#Incorporating-Visual-Correspondence-into-Diffusion-Model-for-Virtual-Try-On" class="headerlink" title="Incorporating Visual Correspondence into Diffusion Model for Virtual   Try-On"></a>Incorporating Visual Correspondence into Diffusion Model for Virtual   Try-On</h2><p><strong>Authors:Siqi Wan, Jingwen Chen, Yingwei Pan, Ting Yao, Tao Mei</strong></p>
<p>Diffusion models have shown preliminary success in virtual try-on (VTON) task. The typical dual-branch architecture comprises two UNets for implicit garment deformation and synthesized image generation respectively, and has emerged as the recipe for VTON task. Nevertheless, the problem remains challenging to preserve the shape and every detail of the given garment due to the intrinsic stochasticity of diffusion model. To alleviate this issue, we novelly propose to explicitly capitalize on visual correspondence as the prior to tame diffusion process instead of simply feeding the whole garment into UNet as the appearance reference. Specifically, we interpret the fine-grained appearance and texture details as a set of structured semantic points, and match the semantic points rooted in garment to the ones over target person through local flow warping. Such 2D points are then augmented into 3D-aware cues with depth&#x2F;normal map of target person. The correspondence mimics the way of putting clothing on human body and the 3D-aware cues act as semantic point matching to supervise diffusion model training. A point-focused diffusion loss is further devised to fully take the advantage of semantic point matching. Extensive experiments demonstrate strong garment detail preservation of our approach, evidenced by state-of-the-art VTON performances on both VITON-HD and DressCode datasets. Code is publicly available at: <a target="_blank" rel="noopener" href="https://github.com/HiDream-ai/SPM-Diff">https://github.com/HiDream-ai/SPM-Diff</a>. </p>
<blockquote>
<p>扩散模型在虚拟试穿（VTON）任务中初步取得了成功。典型的双分支架构包含两个U网，分别用于隐式服装变形和合成图像生成，已成为VTON任务的配方。然而，由于扩散模型的固有随机性，保持给定服装的形状和每一个细节仍然是一个挑战。为了缓解这个问题，我们创新地提出明确利用视觉对应作为先验来驯服扩散过程，而不是简单地将整个服装输入U网作为外观参考。具体来说，我们将细致的外观和纹理细节解释为一组结构化语义点，并通过局部流扭曲将服装中的语义点与目标人物上的语义点相匹配。这些二维点然后通过目标人物的深度&#x2F;法线图增强为三维感知线索。这种对应关系模仿了将衣服穿在人体上的方式，三维感知线索作为语义点匹配来监督扩散模型的训练。进一步设计了一种点聚焦扩散损失，以充分利用语义点匹配的优势。大量实验表明，我们的方法在服装细节保护方面具有优势，在VITON-HD和DressCode数据集上的VTON性能均达到了最新水平。代码公开在：<a target="_blank" rel="noopener" href="https://github.com/HiDream-ai/SPM-Diff">HiDream-ai&#x2F;SPM-Diff</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16977v1">PDF</a> ICLR 2025. Code is publicly available at:   <a target="_blank" rel="noopener" href="https://github.com/HiDream-ai/SPM-Diff">https://github.com/HiDream-ai/SPM-Diff</a></p>
<p><strong>Summary</strong><br>扩散模型在虚拟试穿（VTON）任务中取得初步成功。文章提出一种新型的双分支架构，使用两个UNet分别处理衣物变形和图像生成。为解决扩散模型内在随机性导致的衣物形状和细节保留问题，文章创新地提出利用视觉对应性作为扩散过程的先验信息，而非简单地将整个衣物输入UNet作为外观参考。通过匹配衣物中的语义点与目标人物上的点，实现精细外观和纹理细节的解读。此外，将这些2D点增强为具有目标人物深度&#x2F;法线图的3D感知线索。视觉对应模仿了衣物穿在人体上的方式，而3D感知线索则作为语义点匹配来监督扩散模型的训练。实验证明，该方法在保持衣物细节方面表现出色，在VITON-HD和DressCode数据集上的虚拟试穿性能达到最新水平。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型在虚拟试穿任务中取得初步成功。</li>
<li>传统的双分支架构包括两个UNet，分别用于隐式衣物变形和合成图像生成。</li>
<li>提出的新方法利用视觉对应性作为扩散过程的先验信息，以提高衣物形状和细节的保留效果。</li>
<li>通过匹配衣物中的语义点与目标人物上的点，实现精细外观和纹理细节的解读。</li>
<li>2D点被增强为具有目标人物深度&#x2F;法线图的3D感知线索。</li>
<li>视觉对应模仿衣物穿在人体上的方式。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16977">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-73514832929c45dcf7ef1b454cd3ec57.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ee74394f860bc4758c5d65d3923b5099.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa66913b8ffb9ad465b38ad47d565d84.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Creatively-Upscaling-Images-with-Global-Regional-Priors"><a href="#Creatively-Upscaling-Images-with-Global-Regional-Priors" class="headerlink" title="Creatively Upscaling Images with Global-Regional Priors"></a>Creatively Upscaling Images with Global-Regional Priors</h2><p><strong>Authors:Yurui Qian, Qi Cai, Yingwei Pan, Ting Yao, Tao Mei</strong></p>
<p>Contemporary diffusion models show remarkable capability in text-to-image generation, while still being limited to restricted resolutions (e.g., 1,024 X 1,024). Recent advances enable tuning-free higher-resolution image generation by recycling pre-trained diffusion models and extending them via regional denoising or dilated sampling&#x2F;convolutions. However, these models struggle to simultaneously preserve global semantic structure and produce creative regional details in higher-resolution images. To address this, we present C-Upscale, a new recipe of tuning-free image upscaling that pivots on global-regional priors derived from given global prompt and estimated regional prompts via Multimodal LLM. Technically, the low-frequency component of low-resolution image is recognized as global structure prior to encourage global semantic consistency in high-resolution generation. Next, we perform regional attention control to screen cross-attention between global prompt and each region during regional denoising, leading to regional attention prior that alleviates object repetition issue. The estimated regional prompts containing rich descriptive details further act as regional semantic prior to fuel the creativity of regional detail generation. Both quantitative and qualitative evaluations demonstrate that our C-Upscale manages to generate ultra-high-resolution images (e.g., 4,096 X 4,096 and 8,192 X 8,192) with higher visual fidelity and more creative regional details. </p>
<blockquote>
<p>当前扩散模型在文本到图像生成方面显示出显著的能力，但受限于较低分辨率（例如，1,024 X 1,024）。最近的进展通过回收预训练的扩散模型并通过区域去噪或膨胀采样&#x2F;卷积对其进行扩展，实现了无需调整的高分辨率图像生成。然而，这些模型在保持全局语义结构的同时，在高分辨率图像中产生创造性区域细节方面遇到困难。为了解决这一问题，我们提出了C-Upscale，这是一种无需调整的图像超分辨率新方法，它依赖于基于给定全局提示和通过多模态大型语言模型估计的区域提示所派生的全局-区域先验知识。技术上，我们识别低分辨率图像的低频成分作为全局结构先验，以鼓励高分辨率生成中的全局语义一致性。接下来，我们对区域去噪过程中的全局提示和每个区域之间的交叉注意力进行筛选，形成区域注意力先验，缓解了对象重复问题。估计的区域提示富含描述性细节，进一步作为区域语义先验，为区域细节生成提供创造力。定量和定性评估均表明，我们的C-Upscale能够生成超高分辨率图像（例如，4,096 X 4,096和8,192 X 8,192），具有更高的视觉保真度和更富有创造力的区域细节。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16976v1">PDF</a> International Journal of Computer Vision (IJCV) 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了当代扩散模型在文本转图像生成中的显著能力，但仍受限于低分辨率。近期发展通过再利用预训练扩散模型并借助区域去噪或膨胀采样&#x2F;卷积进行扩展，实现了无调整高分辨率图像生成。然而，这些模型在保持全局语义结构和生成创意区域细节方面存在挑战。为此，本文提出了C-Upscale，一种无需调整的新图像超分辨率方案，基于全局和区域先验进行工作。该方案可从给定的全局提示和通过多模态大型语言模型估计的区域提示中派生出来。通过识别低分辨率图像的低频成分作为全局结构先验，鼓励在生成高分辨率图像时保持全局语义一致性。接着，对区域去噪过程中的全局提示和每个区域之间的交叉注意力进行筛选，形成区域注意力先验，减轻了对象重复问题。包含丰富描述性细节的区域提示估计进一步作为区域语义先验，促进区域细节生成的创造力。评估证明，C-Upscale成功生成了超高分辨率图像，具有更高的视觉保真度和更富有创造力的区域细节。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当代扩散模型虽擅长文本转图像生成，但分辨率受限。</li>
<li>近期发展通过扩展预训练扩散模型实现无调整高分辨率图像生成。</li>
<li>扩散模型在保持全局语义结构和生成创意区域细节方面存在挑战。</li>
<li>C-Upscale是一种无需调整的新图像超分辨率方案，基于全局和区域先验。</li>
<li>C-Upscale通过识别低分辨率图像的低频成分作为全局结构先验，促进全局语义一致性。</li>
<li>C-Upscale采用区域注意力控制，减轻对象重复问题，并包含丰富细节的区域语义先验，促进区域细节创造力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16976">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-27820cb26a2b0de7d1539f24c5d1b101.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f7724161b87a57d114dbd5e2cfbd0b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b743cde9875070c5ecf90aa1b7fd9d77.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-818e30c5801c3edf3104a73baf2a269f.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Bigger-Isn’t-Always-Memorizing-Early-Stopping-Overparameterized-Diffusion-Models"><a href="#Bigger-Isn’t-Always-Memorizing-Early-Stopping-Overparameterized-Diffusion-Models" class="headerlink" title="Bigger Isn’t Always Memorizing: Early Stopping Overparameterized   Diffusion Models"></a>Bigger Isn’t Always Memorizing: Early Stopping Overparameterized   Diffusion Models</h2><p><strong>Authors:Alessandro Favero, Antonio Sclocchi, Matthieu Wyart</strong></p>
<p>Diffusion probabilistic models have become a cornerstone of modern generative AI, yet the mechanisms underlying their generalization remain poorly understood. In fact, if these models were perfectly minimizing their training loss, they would just generate data belonging to their training set, i.e., memorize, as empirically found in the overparameterized regime. We revisit this view by showing that, in highly overparameterized diffusion models, generalization in natural data domains is progressively achieved during training before the onset of memorization. Our results, ranging from image to language diffusion models, systematically support the empirical law that memorization time is proportional to the dataset size. Generalization vs. memorization is then best understood as a competition between time scales. We show that this phenomenology is recovered in diffusion models learning a simple probabilistic context-free grammar with random rules, where generalization corresponds to the hierarchical acquisition of deeper grammar rules as training time grows, and the generalization cost of early stopping can be characterized. We summarize these results in a phase diagram. Overall, our results support that a principled early-stopping criterion - scaling with dataset size - can effectively optimize generalization while avoiding memorization, with direct implications for hyperparameter transfer and privacy-sensitive applications. </p>
<blockquote>
<p>扩散概率模型已成为现代生成式人工智能的基石，但其背后的泛化机制仍知之甚少。事实上，如果这些模型能够完美地最小化其训练损失，它们只会生成属于其训练集的数据，即进行记忆，正如在超参数范围内所发现的那样。我们通过展示高度超参数的扩散模型在记忆开始前，在自然数据领域泛化是在训练过程中逐步实现的，重新审视了这一观点。我们的结果涵盖了图像到语言扩散模型，系统地支持了经验法则，即记忆时间与数据集大小成正比。泛化与记忆之间的最佳理解是时间尺度的竞争。我们展示了在扩散模型学习具有随机规则的简单无上下文概率语法时恢复了这种现象，其中泛化对应于随着训练时间的增长层次地获取更深的语法规则，早期停止的泛化成本可以得到表征。我们在相图中总结了这些结果。总的来说，我们的结果支持一个原则性的早期停止准则——与数据集大小成比例——可以有效地优化泛化同时避免记忆，对超参数转移和隐私敏感应用有直接的影响。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16959v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>扩散概率模型是现代生成人工智能的核心，但其泛化机制尚不清楚。事实上，如果这些模型完美最小化其训练损失，它们只会生成属于其训练集的数据，即存在记忆现象，这在过度参数化状态下已被经验发现。我们重新审视这一观点，发现高度过度参数化的扩散模型中，在自然数据领域的泛化是在训练过程中逐渐实现的，且在记忆出现之前。我们的结果从图像到语言扩散模型都有系统支持，实证规律是记忆时间与数据集大小成正比。泛化与记忆之间的最佳理解是时间尺度的竞争。我们展示了这一现象在扩散模型学习简单的无上下文概率语法与随机规则时得以恢复，其中泛化对应于随着训练时间的增长层次地获取更深的语法规则，早期停止的泛化成本可以得到特征描述。总体而言，我们的结果支持一个原则性的早期停止准则——与数据集大小成比例——可以有效地优化泛化同时避免记忆现象，对超参数转移和隐私敏感应用有直接意义。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散概率模型是现代生成AI的核心，但其泛化机制尚未完全理解。</li>
<li>在过度参数化的扩散模型中，泛化过程是在训练过程中逐渐实现的，而非一开始就能达到。</li>
<li>记忆现象在扩散模型中也存在，尤其是在过度参数化状态下。</li>
<li>记忆时间与数据集大小成正比，这是一个实证规律。</li>
<li>泛化与记忆之间的平衡可以理解为时间尺度的竞争。</li>
<li>在学习简单的无上下文概率语法时，扩散模型的泛化过程与获取更深层次的语法规则相关。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16959">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-c4d6fbfa6b4bb33b7751c1961485380d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eaf72d07e407191573008e2ed9ef3a8a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-197aff0591f7a96f35c7f838ebe1b455.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eeb001027ae3b6b82ab6fbce4ec34bff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9467d7179362564865a235a8163f61b8.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Forward-only-Diffusion-Probabilistic-Models"><a href="#Forward-only-Diffusion-Probabilistic-Models" class="headerlink" title="Forward-only Diffusion Probabilistic Models"></a>Forward-only Diffusion Probabilistic Models</h2><p><strong>Authors:Ziwei Luo, Fredrik K. Gustafsson, Jens Sjölund, Thomas B. Schön</strong></p>
<p>This work presents a forward-only diffusion (FoD) approach for generative modelling. In contrast to traditional diffusion models that rely on a coupled forward-backward diffusion scheme, FoD directly learns data generation through a single forward diffusion process, yielding a simple yet efficient generative framework. The core of FoD is a state-dependent linear stochastic differential equation that involves a mean-reverting term in both the drift and diffusion functions. This mean-reversion property guarantees the convergence to clean data, naturally simulating a stochastic interpolation between source and target distributions. More importantly, FoD is analytically tractable and is trained using a simple stochastic flow matching objective, enabling a few-step non-Markov chain sampling during inference. The proposed FoD model, despite its simplicity, achieves competitive performance on various image-conditioned (e.g., image restoration) and unconditional generation tasks, demonstrating its effectiveness in generative modelling. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/Algolzw/FoD">https://github.com/Algolzw/FoD</a>. </p>
<blockquote>
<p>本文提出了一种只前向扩散（FoD）的方法，用于生成建模。与传统的依赖于耦合的前向-后向扩散方案的扩散模型不同，FoD通过单一的前向扩散过程直接学习数据生成，从而构建了一个简单而高效的生成框架。FoD的核心是一个与状态相关的线性随机微分方程，该方程在漂移和扩散函数中涉及均值回复项。这种均值回归属性保证了向干净数据的收敛，自然地模拟了源分布和目标分布之间的随机插值。更重要的是，FoD分析上是可行的，并且使用简单的随机流匹配目标进行训练，在推理期间实现几步非马尔可夫链采样。尽管FoD模型简单，但在各种图像条件（例如图像恢复）和无条件生成任务上实现了具有竞争力的性能，证明了其在生成建模中的有效性。我们的代码位于<a target="_blank" rel="noopener" href="https://github.com/Algolzw/FoD%E3%80%82">https://github.com/Algolzw/FoD。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16733v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://algolzw.github.io/fod">https://algolzw.github.io/fod</a></p>
<p><strong>Summary</strong></p>
<p>本文提出了一种仅前向扩散（FoD）的方法用于生成建模。与传统的依赖于耦合前向-后向扩散方案的扩散模型不同，FoD通过单一的前向扩散过程直接学习数据生成，提供了一个简洁而高效的生成框架。FoD的核心是一个涉及均值反转项的依赖于状态的非线性随机微分方程，这一特性保证了向清洁数据的收敛，自然地模拟了源分布和目标分布之间的随机插值。更重要的是，FoD具有分析上的可追踪性，并使用简单的随机流匹配目标进行训练，在推理期间实现少数非马尔可夫链采样步骤。尽管其简单性，提出的FoD模型在图像条件（如图像恢复）和无条件生成任务上取得了具有竞争力的表现，证明了其在生成建模中的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了一种仅前向扩散（FoD）的生成建模方法。</li>
<li>与传统扩散模型不同，FoD通过单一的前向扩散过程进行学习。</li>
<li>FoD的核心是一个涉及均值反转的依赖于状态的非线性随机微分方程。</li>
<li>均值反转特性保证了向清洁数据的收敛，模拟了源分布与目标分布之间的随机插值。</li>
<li>FoD具有分析上的可追踪性，并使用简单的随机流匹配目标进行训练。</li>
<li>FoD模型在图像条件和无条件生成任务上取得了具有竞争力的表现。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16733">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-500104a85ae2895290af5871857a908a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9ece37e8b928279052176b7dc1c75ccb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8295672b87fa1a9e83d8e9da885259d1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1fb188f309760a1e6248228d5f8f5aaa.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MAGIC-Motion-Aware-Generative-Inference-via-Confidence-Guided-LLM"><a href="#MAGIC-Motion-Aware-Generative-Inference-via-Confidence-Guided-LLM" class="headerlink" title="MAGIC: Motion-Aware Generative Inference via Confidence-Guided LLM"></a>MAGIC: Motion-Aware Generative Inference via Confidence-Guided LLM</h2><p><strong>Authors:Siwei Meng, Yawei Luo, Ping Liu</strong></p>
<p>Recent advances in static 3D generation have intensified the demand for physically consistent dynamic 3D content. However, existing video generation models, including diffusion-based methods, often prioritize visual realism while neglecting physical plausibility, resulting in implausible object dynamics. Prior approaches for physics-aware dynamic generation typically rely on large-scale annotated datasets or extensive model fine-tuning, which imposes significant computational and data collection burdens and limits scalability across scenarios. To address these challenges, we present MAGIC, a training-free framework for single-image physical property inference and dynamic generation, integrating pretrained image-to-video diffusion models with iterative LLM-based reasoning. Our framework generates motion-rich videos from a static image and closes the visual-to-physical gap through a confidence-driven LLM feedback loop that adaptively steers the diffusion model toward physics-relevant motion. To translate visual dynamics into controllable physical behavior, we further introduce a differentiable MPM simulator operating directly on 3D Gaussians reconstructed from the single image, enabling physically grounded, simulation-ready outputs without any supervision or model tuning. Experiments show that MAGIC outperforms existing physics-aware generative methods in inference accuracy and achieves greater temporal coherence than state-of-the-art video diffusion models. </p>
<blockquote>
<p>近期静态3D生成的进展加剧了对物理一致性动态3D内容的需求。然而，现有的视频生成模型，包括基于扩散的方法，通常优先考虑视觉真实性而忽视物理可行性，导致物体动态不可信。之前针对物理感知动态生成的方法通常依赖于大规模标注数据集或模型精细调整，这带来了显著的计算和数据收集负担，并限制了跨场景的扩展性。为了应对这些挑战，我们提出了MAGIC，这是一个无需训练的单图像物理属性推断和动态生成框架，它整合了预训练的图像到视频的扩散模型与基于迭代的大型语言模型（LLM）推理。我们的框架从静态图像生成运动丰富的视频，并通过信心驱动的LLM反馈循环缩小视觉到物理的差距，该循环自适应地引导扩散模型朝向物理相关的运动。为了将视觉动态转化为可控的物理行为，我们进一步引入了一个可直接在单图像重建的3D高斯上运行的可微分MPM模拟器，无需监督或模型调整即可实现物理基础、模拟就绪的输出。实验表明，MAGIC在推理准确性方面超越了现有的物理感知生成方法，并且在时间连贯性方面优于最先进的视频扩散模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16456v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于静态图像生成动态内容的物理一致性挑战。现有视频生成模型忽视物理合理性，导致物体动态不合理。MAGIC框架结合预训练的图像到视频的扩散模型与基于LLM的推理，无需训练即可进行单图像物理属性推断和动态生成。通过置信度驱动的LLM反馈循环和可微分的MPM模拟器，MAGIC实现了从静态图像生成运动丰富视频，并关闭了视觉到物理的差距。实验表明，MAGIC在推理准确性方面优于现有的物理感知生成方法，并且在时间连贯性方面达到了最先进的视频扩散模型的水平。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>现有视频生成模型在生成动态内容时忽视了物理一致性，导致物体动态不合理。</li>
<li>MAGIC框架是一个无需训练的框架，用于单图像物理属性推断和动态生成。</li>
<li>MAGIC结合了预训练的图像到视频的扩散模型和基于LLM的推理。</li>
<li>通过置信度驱动的LLM反馈循环，MAGIC实现了自适应引导扩散模型向物理相关运动方向。</li>
<li>MAGIC引入了一个可微分的MPM模拟器，直接在3D高斯上操作，生成物理上合理且模拟就绪的输出。</li>
<li>无需监督或模型调整，MAGIC能够从单个静态图像生成物理上丰富的视频内容。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16456">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-dca90b978eb12c54278e945f8392df79.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e05670f0400652cee38bd0d733e0648d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-20e9e24d2e0a0db7dcb33948ed816fa1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-488a64befa1216189a3cae36eb973c80.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-43eded7d51517ad257d1da2db9033f82.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Style-Transfer-with-Diffusion-Models-for-Synthetic-to-Real-Domain-Adaptation"><a href="#Style-Transfer-with-Diffusion-Models-for-Synthetic-to-Real-Domain-Adaptation" class="headerlink" title="Style Transfer with Diffusion Models for Synthetic-to-Real Domain   Adaptation"></a>Style Transfer with Diffusion Models for Synthetic-to-Real Domain   Adaptation</h2><p><strong>Authors:Estelle Chigot, Dennis G. Wilson, Meriem Ghrib, Thomas Oberlin</strong></p>
<p>Semantic segmentation models trained on synthetic data often perform poorly on real-world images due to domain gaps, particularly in adverse conditions where labeled data is scarce. Yet, recent foundation models enable to generate realistic images without any training. This paper proposes to leverage such diffusion models to improve the performance of vision models when learned on synthetic data. We introduce two novel techniques for semantically consistent style transfer using diffusion models: Class-wise Adaptive Instance Normalization and Cross-Attention (CACTI) and its extension with selective attention Filtering (CACTIF). CACTI applies statistical normalization selectively based on semantic classes, while CACTIF further filters cross-attention maps based on feature similarity, preventing artifacts in regions with weak cross-attention correspondences. Our methods transfer style characteristics while preserving semantic boundaries and structural coherence, unlike approaches that apply global transformations or generate content without constraints. Experiments using GTA5 as source and Cityscapes&#x2F;ACDC as target domains show that our approach produces higher quality images with lower FID scores and better content preservation. Our work demonstrates that class-aware diffusion-based style transfer effectively bridges the synthetic-to-real domain gap even with minimal target domain data, advancing robust perception systems for challenging real-world applications. The source code is available at: <a target="_blank" rel="noopener" href="https://github.com/echigot/cactif">https://github.com/echigot/cactif</a>. </p>
<blockquote>
<p>利用合成数据训练的语义分割模型在真实世界图像上的表现往往较差，这主要是由于领域差距造成的，特别是在标记数据稀缺的不利条件下。然而，最近的基础模型能够在无需任何训练的情况下生成逼真的图像。本文提出利用这种扩散模型来改善在合成数据上学习的视觉模型的性能。我们介绍两种利用扩散模型进行语义一致风格转移的新技术：基于类别的自适应实例归一化和交叉注意力（CACTI），以及基于选择性注意力过滤的扩展（CACTIF）。CACTI根据语义类别有选择地应用统计归一化，而CACTIF则进一步根据特征相似性过滤交叉注意力图，防止在交叉注意力对应较弱的区域出现伪影。我们的方法在转移风格特征的同时，保留了语义边界和结构一致性，不同于那些应用全局变换或无约束生成内容的方法。使用GTA5作为源域，Cityscapes&#x2F;ACDC作为目标域的实验表明，我们的方法生成了质量更高、FID得分更低、内容保存更好的图像。我们的工作证明，类感知扩散风格转移有效地缩短了合成到真实的领域差距，即使目标领域的数据最少，也为具有挑战性的真实世界应用提供了稳健的感知系统。源代码可在<a target="_blank" rel="noopener" href="https://github.com/echigot/cactif%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/echigot/cactif获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16360v1">PDF</a> Under review</p>
<p><strong>Summary</strong></p>
<p>基于扩散模型利用生成图像的方法能有效缩减合成图像与现实图像之间的差距，提高语义分割模型在真实世界图像上的性能。文中提出了两种新颖的语义一致性风格转移技术——基于分类自适应实例归一化和交叉注意力机制（CACTI）和其选择性注意力滤波扩展（CACTIF）。CACTI能够针对语义类别选择性应用统计归一化，而CACTIF则进一步根据特征相似性过滤交叉注意力映射，避免了弱交叉注意力对应区域的伪影。实验表明，该方法在GTA5作为源域和Cityscapes&#x2F;ACDC作为目标域的场景下，生成图像质量更高、FID得分更低且内容保留更好。该研究证明了基于类别感知的扩散风格转移即使在目标域数据很少的情况下也能有效地弥合合成域与现实域之间的鸿沟，为应对挑战性的真实世界应用提供了先进的感知系统。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语义分割模型在真实世界图像上的性能可以通过利用扩散模型生成的图像来提高。</li>
<li>提出了两种新的语义一致性风格转移技术：CACTI和CACTIF。</li>
<li>CACTI能够针对语义类别进行选择性统计归一化。</li>
<li>CACTIF通过过滤交叉注意力映射来避免伪影，特别是在弱交叉注意力的区域。</li>
<li>实验结果显示，该方法在生成图像质量、FID得分和内容保留方面表现优异。</li>
<li>该方法有效地缩小了合成域与现实域之间的差距，尤其是使用较少的真实世界数据。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16360">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-4eccc3398cbfdd856b0f999f699f917d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7588621449cab3136b0e2f33ade893db.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="FPQVAR-Floating-Point-Quantization-for-Visual-Autoregressive-Model-with-FPGA-Hardware-Co-design"><a href="#FPQVAR-Floating-Point-Quantization-for-Visual-Autoregressive-Model-with-FPGA-Hardware-Co-design" class="headerlink" title="FPQVAR: Floating Point Quantization for Visual Autoregressive Model with   FPGA Hardware Co-design"></a>FPQVAR: Floating Point Quantization for Visual Autoregressive Model with   FPGA Hardware Co-design</h2><p><strong>Authors:Renjie Wei, Songqiang Xu, Qingyu Guo, Meng Li</strong></p>
<p>Visual autoregressive (VAR) modeling has marked a paradigm shift in image generation from next-token prediction to next-scale prediction. VAR predicts a set of tokens at each step from coarse to fine scale, leading to better image quality and faster inference speed compared to existing diffusion models. However, the large parameter size and computation cost hinder its deployment on edge devices. To reduce the memory and computation cost, we propose FPQVAR, an efficient post-training floating-point (FP) quantization framework for VAR featuring algorithm and hardware co-design. At the algorithm level, we first identify the challenges of quantizing VAR. To address them, we propose Dual Format Quantization for the highly imbalanced input activation. We further propose Group-wise Hadamard Transformation and GHT-Aware Learnable Transformation to address the time-varying outlier channels. At the hardware level, we design the first low-bit FP quantizer and multiplier with lookup tables on FPGA and propose the first FPGA-based VAR accelerator featuring low-bit FP computation and an elaborate two-level pipeline. Extensive experiments show that compared to the state-of-the-art quantization method, our proposed FPQVAR significantly improves Fr&#39;echet Inception Distance (FID) from 10.83 to 3.58, Inception Score (IS) from 175.9 to 241.5 under 4-bit quantization. FPQVAR also significantly improves the performance of 6-bit quantized VAR, bringing it on par with the FP16 model. Our accelerator on AMD-Xilinx VCK190 FPGA achieves a throughput of 1.1 image&#x2F;s, which is 3.1x higher than the integer-based accelerator. It also demonstrates 3.6x and 2.8x higher energy efficiency compared to the integer-based accelerator and GPU baseline, respectively. </p>
<blockquote>
<p>视觉自回归（VAR）建模标志着图像生成从下一个词预测到下一个尺度预测的范式转变。VAR通过从粗略到精细尺度的每一步预测一系列标记（tokens），与现有的扩散模型相比，产生了更高质量的图像和更快的推理速度。然而，其庞大的参数大小和计算成本阻碍了其在边缘设备上的部署。为了降低内存和计算成本，我们提出了FPQVAR，这是一种用于VAR的后训练浮点（FP）量化框架，融合了算法和硬件协同设计。在算法层面，我们首先确定了量化VAR的挑战。为了解决这些问题，我们提出了用于高度不平衡输入激活的双重格式量化。我们还提出了分组哈达玛变换和GHT感知可学习变换，以解决时变异常通道的问题。在硬件层面，我们设计了基于FPGA的首个低位浮点量化和乘法器查找表，并提出了基于FPGA的VAR加速器，该加速器具有低位浮点计算和精心设计的两级流水线。大量实验表明，与最新的量化方法相比，我们提出的FPQVAR在4位量化下将Fréchet Inception Distance (FID)从10.83显著改进到3.58，Inception Score (IS)从175.9提高到241.5。FPQVAR还显著提高了6位量化VAR的性能，使其与FP16模型相当。我们的加速器在AMD-Xilinx VCK190 FPGA上的吞吐量达到1.1图像&#x2F;秒，比整数加速器高出3.1倍。它还表现出比整数加速器和GPU基准测试高3.6倍和2.8倍的能效。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16335v1">PDF</a> </p>
<p><strong>Summary</strong><br>     视觉自回归（VAR）模型在图像生成领域实现了从下一个标记预测到下一个尺度预测的范式转变。VAR通过从粗到细的尺度预测一系列标记，提高了图像质量和推理速度。为降低内存和计算成本，便于在边缘设备部署，本文提出了FPQVAR，这是一个针对VAR的后训练浮点（FP）量化框架，融合了算法和硬件协同设计。在算法层面，提出双格式量化解决输入激活的不平衡问题，以及分组哈达玛变换和GHT感知可学习变换应对时变异常通道。在硬件层面，设计了低比特FP量化和乘法器，采用FPGA查找表，并推出基于FPGA的VAR加速器，实现低比特FP计算和两级精细管道。实验表明，与最新量化方法相比，FPQVAR在4位量化下将Fréchet Inception Distance (FID)从10.83大幅改善至3.58，Inception Score (IS)从175.9提升至241.5。在6位量化方面，FPQVAR的表现与FP16模型相当。在AMD-Xilinx VCK190 FPGA上，加速器吞吐量达1.1图像&#x2F;秒，是整数加速器的3.1倍。此外，与整数加速器和GPU基准测试相比，其能效分别提高了3.6倍和2.8倍。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视觉自回归（VAR）模型实现了图像生成中的尺度预测转变，从粗到细提高图像质量和推理速度。</li>
<li>FPQVAR是首个针对VAR的后训练浮点（FP）量化框架，包含算法和硬件协同设计。</li>
<li>在算法层面，FPQVAR通过双格式量化解决输入激活不平衡问题，通过分组哈达玛变换和GHT感知可学习变换应对异常通道。</li>
<li>在硬件层面，设计了低比特FP量化和乘法器，并推出基于FPGA的VAR加速器。</li>
<li>FPQVAR显著提高了图像生成质量，同时在4位和6位量化方面表现出优异性能。</li>
<li>实验结果显示，FPQVAR在图像质量指标Fréchet Inception Distance (FID)和Inception Score (IS)上有显著改善。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16335">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9b2ef185e4d29a1e4925cd1b4ba9a8ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ebc388be20c9c4800a45be8f3e68291.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e5a68494d2087937e18b5fac1355b35.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-01c15aa00c2857b4334e27d666006001.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1ff93b107768bc8167668ddd22bf89cd.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="TRAIL-Transferable-Robust-Adversarial-Images-via-Latent-diffusion"><a href="#TRAIL-Transferable-Robust-Adversarial-Images-via-Latent-diffusion" class="headerlink" title="TRAIL: Transferable Robust Adversarial Images via Latent diffusion"></a>TRAIL: Transferable Robust Adversarial Images via Latent diffusion</h2><p><strong>Authors:Yuhao Xue, Zhifei Zhang, Xinyang Jiang, Yifei Shen, Junyao Gao, Wentao Gu, Jiale Zhao, Miaojing Shi, Cairong Zhao</strong></p>
<p>Adversarial attacks exploiting unrestricted natural perturbations present severe security risks to deep learning systems, yet their transferability across models remains limited due to distribution mismatches between generated adversarial features and real-world data. While recent works utilize pre-trained diffusion models as adversarial priors, they still encounter challenges due to the distribution shift between the distribution of ideal adversarial samples and the natural image distribution learned by the diffusion model. To address the challenge, we propose Transferable Robust Adversarial Images via Latent Diffusion (TRAIL), a test-time adaptation framework that enables the model to generate images from a distribution of images with adversarial features and closely resembles the target images. To mitigate the distribution shift, during attacks, TRAIL updates the diffusion U-Net’s weights by combining adversarial objectives (to mislead victim models) and perceptual constraints (to preserve image realism). The adapted model then generates adversarial samples through iterative noise injection and denoising guided by these objectives. Experiments demonstrate that TRAIL significantly outperforms state-of-the-art methods in cross-model attack transferability, validating that distribution-aligned adversarial feature synthesis is critical for practical black-box attacks. </p>
<blockquote>
<p>利用不受限制的自然扰动进行的对抗性攻击对深度学习系统构成了严重的安全风险。然而，由于生成的对抗性特征与现实世界数据之间的分布不匹配，这些攻击在不同模型之间的可转移性仍然有限。虽然最近的研究利用预训练的扩散模型作为对抗性先验，但由于理想对抗样本的分布与扩散模型学到的自然图像分布之间存在分布偏移，它们仍然面临挑战。为了应对这一挑战，我们提出了通过潜在扩散生成可转移稳健对抗图像（TRAIL）的方法，这是一种测试时间适应框架，使模型能够从具有对抗特征的图像分布中生成图像，并紧密模拟目标图像。为了缓解分布偏移，在攻击过程中，TRAIL通过结合对抗目标（以误导目标模型）和感知约束（以保持图像的真实性）来更新扩散U-Net的权重。适应后的模型然后通过迭代噪声注入和去噪，在这些目标的指导下生成对抗样本。实验表明，与传统的先进方法相比，TRAIL在跨模型攻击转移方面表现出显著的优势，验证了与实际黑箱攻击中对齐分布的对抗特征合成的重要性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16166v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为TRAIL的测试时适应性框架，利用预训练的扩散模型作为对抗性先验，生成具有对抗性特征的图像分布，并紧密模拟目标图像。该框架通过结合对抗性目标和感知约束来减轻分布转移问题，从而更新扩散U-Net的权重。实验证明，TRAIL在跨模型攻击转移方面显著优于现有方法，验证了分布对齐对抗性特征合成对于实际黑盒攻击的重要性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>对抗攻击对深度学习系统构成严重安全风险，但其在模型间的传输性因生成对抗特征与真实世界数据分布不匹配而受限。</li>
<li>近期工作利用预训练的扩散模型作为对抗性先验，但仍面临挑战，即理想对抗样本分布与扩散模型学到的自然图像分布之间的分布转移问题。</li>
<li>TRAIL框架通过测试时适应性，使模型能够从具有对抗性特征的图像分布中生成图像，并紧密模拟目标图像。</li>
<li>TRAIL结合对抗性目标和感知约束来更新扩散U-Net的权重，以减轻分布转移问题。</li>
<li>对抗性目标旨在误导受害者模型，而感知约束则旨在保持图像的真实性。</li>
<li>通过迭代噪声注入和去噪过程，生成受目标引导的对抗样本。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16166">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-a76653627e13f715c22b2f570a0783b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-288492c75d198da3796dffe18f5d214d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f7cd05949d2a122ab3bc3c980323f5a9.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="OSCAR-One-Step-Diffusion-Codec-Across-Multiple-Bit-rates"><a href="#OSCAR-One-Step-Diffusion-Codec-Across-Multiple-Bit-rates" class="headerlink" title="OSCAR: One-Step Diffusion Codec Across Multiple Bit-rates"></a>OSCAR: One-Step Diffusion Codec Across Multiple Bit-rates</h2><p><strong>Authors:Jinpei Guo, Yifei Ji, Zheng Chen, Kai Liu, Min Liu, Wang Rao, Wenbo Li, Yong Guo, Yulun Zhang</strong></p>
<p>Pretrained latent diffusion models have shown strong potential for lossy image compression, owing to their powerful generative priors. Most existing diffusion-based methods reconstruct images by iteratively denoising from random noise, guided by compressed latent representations. While these approaches have achieved high reconstruction quality, their multi-step sampling process incurs substantial computational overhead. Moreover, they typically require training separate models for different compression bit-rates, leading to significant training and storage costs. To address these challenges, we propose a one-step diffusion codec across multiple bit-rates. termed OSCAR. Specifically, our method views compressed latents as noisy variants of the original latents, where the level of distortion depends on the bit-rate. This perspective allows them to be modeled as intermediate states along a diffusion trajectory. By establishing a mapping from the compression bit-rate to a pseudo diffusion timestep, we condition a single generative model to support reconstructions at multiple bit-rates. Meanwhile, we argue that the compressed latents retain rich structural information, thereby making one-step denoising feasible. Thus, OSCAR replaces iterative sampling with a single denoising pass, significantly improving inference efficiency. Extensive experiments demonstrate that OSCAR achieves superior performance in both quantitative and visual quality metrics. The code and models will be released at <a target="_blank" rel="noopener" href="https://github.com/jp-guo/OSCAR">https://github.com/jp-guo/OSCAR</a>. </p>
<blockquote>
<p>预训练的潜在扩散模型在有损图像压缩方面显示出强大的潜力，这得益于其强大的生成先验。大多数现有的基于扩散的方法通过迭代去噪从随机噪声中重建图像，由压缩的潜在表示引导。虽然这些方法达到了较高的重建质量，但它们的多步采样过程产生了大量的计算开销。此外，它们通常需要针对不同的压缩比特率训练不同的模型，从而导致显著的训练和存储成本。为了解决这些挑战，我们提出了一种跨多个比特率的扩散编解码器（OSCAR）。具体来说，我们的方法将压缩潜在表示视为原始潜在表示的噪声版本，其中失真程度取决于比特率。这个视角允许将它们建模为沿扩散轨迹的中间状态。通过建立从压缩比特率到伪扩散时间步长的映射，我们将单一生成模型调整为支持多个比特率的重建。同时，我们认为压缩的潜在表示保留了丰富的结构信息，从而使得一步去噪成为可能。因此，OSCAR用单个去噪过程取代了迭代采样，大大提高了推理效率。大量实验表明，OSCAR在定量和视觉质量指标上均取得了卓越的性能。代码和模型将在<a target="_blank" rel="noopener" href="https://github.com/jp-guo/OSCAR%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/jp-guo/OSCAR上发布。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16091v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>预训练潜在扩散模型在无损图像压缩领域展现出强大潜力，其生成先验具有显著优势。现有扩散方法主要通过迭代去噪从随机噪声中重建图像，这一过程由压缩潜在表示引导。虽然这些方法重建质量高，但多步采样过程带来较大计算开销，且通常需要针对不同压缩比特率训练不同模型，导致训练和存储成本增加。针对这些挑战，我们提出一种跨多比特率的单步扩散编解码器OSCAR。我们将压缩潜在视作原始潜在的噪声版本，其失真程度取决于比特率，将其作为扩散轨迹的中间状态建模。通过建立从压缩比特率到伪扩散时间步长的映射，我们使单一生成模型支持多比特率的重建。同时，我们认为压缩潜在保留了丰富的结构信息，使得一步去噪成为可能。因此，OSCAR用一步去噪过程替代了迭代采样，显著提高了推理效率。实验表明，OSCAR在定量和视觉质量指标上均实现优越性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>预训练潜在扩散模型在无损图像压缩领域具有强大潜力。</li>
<li>现有扩散方法存在计算开销大、需要针对多种比特率训练不同模型的问题。</li>
<li>OSCAR提出一种跨多比特率的单步扩散编解码器方法。</li>
<li>OSCAR将压缩潜在视作原始潜在的噪声版本，并建立其与扩散轨迹中间状态的映射。</li>
<li>OSCAR利用单一生成模型支持多比特率的图像重建。</li>
<li>OSCAR通过一步去噪过程提高了推理效率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16091">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-27d05c0c3c13fed805a463ff1eba9dfe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-30fced3344ddec95fcdb6f16cc547286.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e3a6a289238a5a219cc6292763810bc7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-003e472d813eadd16c17e71dd2e022d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5aebc4ce4acc0e0bb70f1e5729ef3800.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="MoRE-Brain-Routed-Mixture-of-Experts-for-Interpretable-and-Generalizable-Cross-Subject-fMRI-Visual-Decoding"><a href="#MoRE-Brain-Routed-Mixture-of-Experts-for-Interpretable-and-Generalizable-Cross-Subject-fMRI-Visual-Decoding" class="headerlink" title="MoRE-Brain: Routed Mixture of Experts for Interpretable and   Generalizable Cross-Subject fMRI Visual Decoding"></a>MoRE-Brain: Routed Mixture of Experts for Interpretable and   Generalizable Cross-Subject fMRI Visual Decoding</h2><p><strong>Authors:Yuxiang Wei, Yanteng Zhang, Xi Xiao, Tianyang Wang, Xiao Wang, Vince D. Calhoun</strong></p>
<p>Decoding visual experiences from fMRI offers a powerful avenue to understand human perception and develop advanced brain-computer interfaces. However, current progress often prioritizes maximizing reconstruction fidelity while overlooking interpretability, an essential aspect for deriving neuroscientific insight. To address this gap, we propose MoRE-Brain, a neuro-inspired framework designed for high-fidelity, adaptable, and interpretable visual reconstruction. MoRE-Brain uniquely employs a hierarchical Mixture-of-Experts architecture where distinct experts process fMRI signals from functionally related voxel groups, mimicking specialized brain networks. The experts are first trained to encode fMRI into the frozen CLIP space. A finetuned diffusion model then synthesizes images, guided by expert outputs through a novel dual-stage routing mechanism that dynamically weighs expert contributions across the diffusion process. MoRE-Brain offers three main advancements: First, it introduces a novel Mixture-of-Experts architecture grounded in brain network principles for neuro-decoding. Second, it achieves efficient cross-subject generalization by sharing core expert networks while adapting only subject-specific routers. Third, it provides enhanced mechanistic insight, as the explicit routing reveals precisely how different modeled brain regions shape the semantic and spatial attributes of the reconstructed image. Extensive experiments validate MoRE-Brain’s high reconstruction fidelity, with bottleneck analyses further demonstrating its effective utilization of fMRI signals, distinguishing genuine neural decoding from over-reliance on generative priors. Consequently, MoRE-Brain marks a substantial advance towards more generalizable and interpretable fMRI-based visual decoding. Code will be publicly available soon: <a target="_blank" rel="noopener" href="https://github.com/yuxiangwei0808/MoRE-Brain">https://github.com/yuxiangwei0808/MoRE-Brain</a>. </p>
<blockquote>
<p>从功能性磁共振成像（fMRI）中解码视觉经验提供了一种强大的方法来理解人类感知并开发先进的脑机接口。然而，目前的进展往往优先最大化重建保真度，而忽视了解释性这一对于获取神经科学洞察力的关键方面。为了解决这一差距，我们提出了MoRE-Brain，这是一个神经启发的框架，旨在实现高保真、可适应和可解释的视觉重建。MoRE-Brain独特地采用了一种层次化的混合专家架构，其中不同的专家处理来自功能相关体素组的fMRI信号，模仿专门的脑网络。专家首先被训练将fMRI编码到固定的CLIP空间中。然后，经过微调扩散模型由专家输出通过一种新型的双阶段路由机制合成图像，该机制在扩散过程中动态权衡专家的贡献。MoRE-Brain提供了三个主要进步：首先，它引入了一种基于脑网络原理的新型混合专家架构来进行神经解码。其次，通过共享核心专家网络并仅适应特定于主题的路由器，它实现了跨主题的有效泛化。第三，它提供了增强的机械洞察力，因为明确的路由机制揭示了不同建模的脑区域如何塑造重建图像的语义和空间属性。大量实验验证了MoRE-Brain的高重建保真度，瓶颈分析进一步证明了它有效利用fMRI信号的能力，区分了真正的神经解码和过度依赖生成先验知识。因此，MoRE-Brain标志着朝着更具通用性和可解释的基于fMRI的视觉解码迈出了重大的一步。代码将于近期在<a target="_blank" rel="noopener" href="https://github.com/yuxiangwei0808/MoRE-Brain%E5%85%AC%E5%BC%80%E3%80%82">https://github.com/yuxiangwei0808/MoRE-Brain公开。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15946v1">PDF</a> </p>
<p><strong>Summary</strong><br>     利用功能性磁共振成像解码视觉体验是理解人类感知和发展先进脑机接口的有力途径。当前方法往往重视重建保真度的最大化而忽视了解释性，这对于获取神经科学洞察至关重要。为解决这一差距，我们提出MoRE-Brain框架，旨在实现高保真、可适应和可解释的视觉重建。该框架采用神经启发的层次化混合专家架构，处理功能相关体素组的fMRI信号，模仿专门化的脑网络。专家首先被训练将fMRI编码到固定的CLIP空间。随后通过微调扩散模型并借助新颖的双重阶段路由机制合成图像，该机制在扩散过程中动态权衡专家的贡献。MoRE-Brain具有三项主要优势：一是引入基于脑网络原理的混合专家架构用于神经解码；二是通过共享核心专家网络并仅适应特定主体路由器来实现跨主体高效概括；三是提供增强机制洞察力，显式路由揭示了不同模拟脑区域如何精确塑造重建图像的语义和空间属性。实验验证MoRE-Brain高重建保真度，瓶颈分析进一步证明其有效利用fMRI信号，区分真正的神经解码与过度依赖生成先验。因此，MoRE-Brain标志着在更具通用性和可解释的fMRI视觉解码方面取得重大进展。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>利用解码视觉体验的fMRI是理解人类感知和开发先进脑机接口的有效方法。</li>
<li>当前方法偏重于重建保真度而忽视了解释性，MoRE-Brain框架旨在解决这一问题。</li>
<li>MoRE-Brain采用混合专家架构处理fMRI信号，模仿脑网络工作方式。</li>
<li>该框架实现了高保真、可适应和可解释的视觉重建。</li>
<li>MoRE-Brain具有跨主体概括能力，通过共享核心专家网络并适应特定主体路由器实现。</li>
<li>框架提供了增强的机制洞察力，展示了不同模拟脑区域如何影响重建图像的语义和空间属性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15946">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-11a3eaa79b0d03650067200e1b8c02ab.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bc92da07951ccfe9eb89d2a0344d7a5d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-645e01c92190ccd6f4f3b22d1c901e02.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8de6a61ae5f5f1b4e04a184b09a34118.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-669d4398a30ac8aa316d07615f14e37b.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Leveraging-the-Powerful-Attention-of-a-Pre-trained-Diffusion-Model-for-Exemplar-based-Image-Colorization"><a href="#Leveraging-the-Powerful-Attention-of-a-Pre-trained-Diffusion-Model-for-Exemplar-based-Image-Colorization" class="headerlink" title="Leveraging the Powerful Attention of a Pre-trained Diffusion Model for   Exemplar-based Image Colorization"></a>Leveraging the Powerful Attention of a Pre-trained Diffusion Model for   Exemplar-based Image Colorization</h2><p><strong>Authors:Satoshi Kosugi</strong></p>
<p>Exemplar-based image colorization aims to colorize a grayscale image using a reference color image, ensuring that reference colors are applied to corresponding input regions based on their semantic similarity. To achieve accurate semantic matching between regions, we leverage the self-attention module of a pre-trained diffusion model, which is trained on a large dataset and exhibits powerful attention capabilities. To harness this power, we propose a novel, fine-tuning-free approach based on a pre-trained diffusion model, making two key contributions. First, we introduce dual attention-guided color transfer. We utilize the self-attention module to compute an attention map between the input and reference images, effectively capturing semantic correspondences. The color features from the reference image is then transferred to the semantically matching regions of the input image, guided by this attention map, and finally, the grayscale features are replaced with the corresponding color features. Notably, we utilize dual attention to calculate attention maps separately for the grayscale and color images, achieving more precise semantic alignment. Second, we propose classifier-free colorization guidance, which enhances the transferred colors by combining color-transferred and non-color-transferred outputs. This process improves the quality of colorization. Our experimental results demonstrate that our method outperforms existing techniques in terms of image quality and fidelity to the reference. Specifically, we use 335 input-reference pairs from previous research, achieving an FID of 95.27 (image quality) and an SI-FID of 5.51 (fidelity to the reference). Our source code is available at <a target="_blank" rel="noopener" href="https://github.com/satoshi-kosugi/powerful-attention">https://github.com/satoshi-kosugi/powerful-attention</a>. </p>
<blockquote>
<p>基于范例的图像彩色化旨在使用参考彩色图像对灰度图像进行彩色化，确保参考颜色根据语义相似性应用于相应的输入区域。为了实现区域之间的准确语义匹配，我们利用预训练扩散模型的自注意力模块，该模块在大规模数据集上进行训练，展现出强大的注意力功能。为了利用这种能力，我们提出了一种基于预训练扩散模型的新型无需微调的方法，做出两个主要贡献。首先，我们引入双重注意力引导色彩转移。我们利用自注意力模块计算输入图像和参考图像之间的注意力图，有效地捕捉语义对应关系。然后，来自参考图像的颜色特征被转移到输入图像的语义匹配区域，由注意力图引导，最后，灰度特征被相应的颜色特征所替代。值得注意的是，我们使用双重注意力分别为灰度图像和彩色图像计算注意力图，实现更精确的语义对齐。其次，我们提出了无分类色彩化指导方法，通过结合色彩转移和非色彩转移的输出，增强转移的色彩。这一过程提高了彩色化的质量。我们的实验结果证明，我们的方法在图像质量和参考忠实度方面超越了现有技术。具体来说，我们使用335个输入-参考对来自之前的研究，达到FID（图像质量）为95.27，SI-FID（对参考的忠实度）为5.51。我们的源代码可在<a target="_blank" rel="noopener" href="https://github.com/satoshi-kosugi/powerful-attention%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/satoshi-kosugi/powerful-attention找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15812v1">PDF</a> Accepted to IEEE Transactions on Circuits and Systems for Video   Technology (TCSVT)</p>
<p><strong>Summary</strong><br>彩色化图像的目标是利用参考彩色图像对灰度图像进行着色，确保参考颜色根据语义相似性应用于对应的输入区域。为了实现区域之间的准确语义匹配，我们利用预训练扩散模型的自注意力模块，该模块在大规模数据集上训练，展现出强大的注意力功能。我们提出了一种基于预训练扩散模型的无需微调的新方法，该方法做出了两个关键贡献。首先，我们引入了双注意力引导色彩转移。利用自注意力模块计算输入图像和参考图像之间的注意力图，有效地捕捉语义对应关系。参考图像的颜色特征被转移到输入图像的语义匹配区域，由注意力图引导，最后灰度特征被相应的颜色特征所替代。其次，我们提出了无分类色彩化指导方法，通过结合色彩转移和非色彩转移的输出，提高了转移颜色的质量。实验结果表明，我们的方法在图像质量和保持对参考图像的忠实度方面均优于现有技术。具体地，我们从之前的研究中使用了335个输入-参考对，实现了95.27的FID（图像质量）和5.51的SI-FID（对参考的忠实度）。我们的源代码可在<a target="_blank" rel="noopener" href="https://github.com/satoshi-kosugi/powerful-attention%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/satoshi-kosugi/powerful-attention上找到。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>方法使用参考彩色图像来为灰度图像着色，并基于语义相似性匹配输入区域和参考颜色。</li>
<li>利用预训练扩散模型的自注意力模块实现精确语义匹配。</li>
<li>提出无需微调的新方法，利用双注意力引导色彩转移，通过注意力图捕捉语义对应关系并实现颜色转移。</li>
<li>引入无分类色彩化指导方法，结合色彩转移和非色彩转移的输出提高颜色质量。</li>
<li>方法在图像质量和忠实于参考方面优于现有技术。</li>
<li>使用335个输入-参考对进行实验验证方法的有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15812">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-385ccfc26b349eb4a8991786c8e57cd6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d58f2391042a0ab431dc65dbbfb8167.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fbe000b67859beaf5b789df013601915.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e4686b4d3a5d66eb28bf880cf8711083.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3509b13a9a3602040e8215af07ba2b04.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="FragFake-A-Dataset-for-Fine-Grained-Detection-of-Edited-Images-with-Vision-Language-Models"><a href="#FragFake-A-Dataset-for-Fine-Grained-Detection-of-Edited-Images-with-Vision-Language-Models" class="headerlink" title="FragFake: A Dataset for Fine-Grained Detection of Edited Images with   Vision Language Models"></a>FragFake: A Dataset for Fine-Grained Detection of Edited Images with   Vision Language Models</h2><p><strong>Authors:Zhen Sun, Ziyi Zhang, Zeren Luo, Zeyang Sha, Tianshuo Cong, Zheng Li, Shiwen Cui, Weiqiang Wang, Jiaheng Wei, Xinlei He, Qi Li, Qian Wang</strong></p>
<p>Fine-grained edited image detection of localized edits in images is crucial for assessing content authenticity, especially given that modern diffusion models and image editing methods can produce highly realistic manipulations. However, this domain faces three challenges: (1) Binary classifiers yield only a global real-or-fake label without providing localization; (2) Traditional computer vision methods often rely on costly pixel-level annotations; and (3) No large-scale, high-quality dataset exists for modern image-editing detection techniques. To address these gaps, we develop an automated data-generation pipeline to create FragFake, the first dedicated benchmark dataset for edited image detection, which includes high-quality images from diverse editing models and a wide variety of edited objects. Based on FragFake, we utilize Vision Language Models (VLMs) for the first time in the task of edited image classification and edited region localization. Experimental results show that fine-tuned VLMs achieve higher average Object Precision across all datasets, significantly outperforming pretrained models. We further conduct ablation and transferability analyses to evaluate the detectors across various configurations and editing scenarios. To the best of our knowledge, this work is the first to reformulate localized image edit detection as a vision-language understanding task, establishing a new paradigm for the field. We anticipate that this work will establish a solid foundation to facilitate and inspire subsequent research endeavors in the domain of multimodal content authenticity. </p>
<blockquote>
<p>精细编辑图像的图像检测对于评估内容真实性至关重要，尤其是考虑到现代扩散模型和图像编辑方法可以产生高度逼真的操纵。然而，此领域面临三个挑战：（1）二元分类器只产生全局真实或虚假的标签，无法提供定位；（2）传统计算机视觉方法通常依赖于昂贵的像素级注释；（3）对于现代图像编辑检测技术，尚不存在大规模高质量的数据集。为解决这些空白，我们开发了一个自动化的数据生成管道来创建FragFake，这是专门用于编辑图像检测的第一个基准数据集，包括来自各种编辑模型的高质量图像和多种编辑对象。基于FragFake，我们首次在编辑图像分类和编辑区域定位任务中使用视觉语言模型（VLMs）。实验结果表明，经过微调的VLMs在所有数据集上达到了更高的对象精度平均值，显著优于预训练模型。我们进一步进行了消融和可转移性分析，以评估检测器在各种配置和编辑场景中的表现。据我们所知，这项工作首次将局部图像编辑检测重新定位为视觉语言理解任务，为该领域建立了新的范式。我们预计这项工作将为多模式内容真实性的后续研究工作和提供灵感。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15644v1">PDF</a> 14pages,15 figures</p>
<p><strong>Summary</strong></p>
<p>本文介绍了图像编辑检测的重要性及其所面临的挑战，包括分类器无法提供局部化信息、依赖高成本像素级标注以及缺乏针对现代图像编辑检测技术的大规模高质量数据集。为解决这些问题，研究团队开发了一个自动化数据生成管道，创建了FragFake数据集，用于编辑图像检测任务。基于该数据集，研究首次利用视觉语言模型（VLMs）进行图像分类和编辑区域定位任务，并取得了优异的结果。此研究开创性地将其改革为视觉语言理解任务，为领域中的后续研究提供了一个新方向。同时强调了其对多媒体内容真实性的重要性。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>图像编辑检测对于评估内容真实性至关重要，尤其是考虑到现代扩散模型和图像编辑方法可以产生高度逼真的操作。</li>
<li>此领域面临三大挑战：分类器无法定位编辑区域、依赖昂贵的像素级标注以及缺乏针对现代图像编辑检测的大规模高质量数据集。</li>
<li>提出并创建FragFake数据集，这是一个专门针对编辑图像检测任务的高质量数据集，包含从不同编辑模型生成的各种高质量图像和多种编辑对象。</li>
<li>研究首次使用视觉语言模型（VLMs）进行图像分类和编辑区域定位任务，实验结果显示其性能优于预训练模型。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15644">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-14c0ddc86670c7fe40557d6bbf4dc343.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b49e6db7e3eb8119d7dbc26277c15976.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8b23a308e67ef6ef1eac44bac15c2e27.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5accf227b98694b62e15ab50f2bfeeee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c38c19aeab800a32aab717466b7010dd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8edb6243ab1bfa227035f00c5b33d83e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-39956ae9bf413249bcd9d01843bea9b1.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Learning-Joint-ID-Textual-Representation-for-ID-Preserving-Image-Synthesis"><a href="#Learning-Joint-ID-Textual-Representation-for-ID-Preserving-Image-Synthesis" class="headerlink" title="Learning Joint ID-Textual Representation for ID-Preserving Image   Synthesis"></a>Learning Joint ID-Textual Representation for ID-Preserving Image   Synthesis</h2><p><strong>Authors:Zichuan Liu, Liming Jiang, Qing Yan, Yumin Jia, Hao Kang, Xin Lu</strong></p>
<p>We propose a novel framework for ID-preserving generation using a multi-modal encoding strategy rather than injecting identity features via adapters into pre-trained models. Our method treats identity and text as a unified conditioning input. To achieve this, we introduce FaceCLIP, a multi-modal encoder that learns a joint embedding space for both identity and textual semantics. Given a reference face and a text prompt, FaceCLIP produces a unified representation that encodes both identity and text, which conditions a base diffusion model to generate images that are identity-consistent and text-aligned. We also present a multi-modal alignment algorithm to train FaceCLIP, using a loss that aligns its joint representation with face, text, and image embedding spaces. We then build FaceCLIP-SDXL, an ID-preserving image synthesis pipeline by integrating FaceCLIP with Stable Diffusion XL (SDXL). Compared to prior methods, FaceCLIP-SDXL enables photorealistic portrait generation with better identity preservation and textual relevance. Extensive experiments demonstrate its quantitative and qualitative superiority. </p>
<blockquote>
<p>我们提出了一种新的ID保留生成框架，采用多模态编码策略，而不是通过适配器向预训练模型注入身份特征。我们的方法将身份和文本视为统一的条件输入。为了实现这一点，我们引入了FaceCLIP，这是一个多模态编码器，它学习身份和文本语义的联合嵌入空间。给定参考人脸和文本提示，FaceCLIP生成一个统一表示，该表示编码身份和文本，使基础扩散模型生成身份一致且文本对齐的图像。我们还提出了一种使用损失函数对齐其联合表示与面部、文本和图像嵌入空间的多模态对齐算法来训练FaceCLIP。然后，我们将FaceCLIP与Stable Diffusion XL（SDXL）相结合，构建了FaceCLIP-SDXL，一个ID保留图像合成管道。与之前的方法相比，FaceCLIP-SDXL能够实现具有更好身份保留和文本相关性的照片级肖像生成。大量实验证明了其在数量和质量上的优越性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14202v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种新颖的ID保留生成框架，采用多模态编码策略，而非通过适配器注入身份特征到预训练模型中。该方法将身份和文本视为统一的条件输入。为此，引入了FaceCLIP多模态编码器，学习身份和文本语义的联合嵌入空间。给定参考人脸和文本提示，FaceCLIP产生统一表示，编码身份和文本，以条件基础扩散模型生成一致身份和文本对齐的图像。还提出了一种多模态对齐算法来训练FaceCLIP，使用损失函数将其联合表示与面部、文本和图像嵌入空间对齐。通过与Stable Diffusion XL（SDXL）集成，构建了FaceCLIP-SDXL身份保留图像合成管道，实现了具有更好身份保留和文本相关性的逼真肖像生成。实验证明其定量和定性优势。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>引入了一种新颖的ID保留生成框架，利用多模态编码策略。</li>
<li>提出了FaceCLIP多模态编码器，可以学习身份和文本语义的联合嵌入空间。</li>
<li>FaceCLIP能够产生统一表示，编码身份和文本，为扩散模型提供条件。</li>
<li>介绍了多模态对齐算法来训练FaceCLIP，通过损失函数实现面部、文本和图像嵌入空间的对齐。</li>
<li>构建了FaceCLIP-SDXL身份保留图像合成管道，集成了FaceCLIP与Stable Diffusion XL（SDXL）。</li>
<li>该方法能够实现逼真肖像生成，具有更好的身份保留和文本相关性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14202">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3020f3125643b578602c03193dac6380.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f013279aa724a6863bd25a4f96d2b176.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Split-Gibbs-Discrete-Diffusion-Posterior-Sampling"><a href="#Split-Gibbs-Discrete-Diffusion-Posterior-Sampling" class="headerlink" title="Split Gibbs Discrete Diffusion Posterior Sampling"></a>Split Gibbs Discrete Diffusion Posterior Sampling</h2><p><strong>Authors:Wenda Chu, Zihui Wu, Yifan Chen, Yang Song, Yisong Yue</strong></p>
<p>We study the problem of posterior sampling in discrete-state spaces using discrete diffusion models. While posterior sampling methods for continuous diffusion models have achieved remarkable progress, analogous methods for discrete diffusion models remain challenging. In this work, we introduce a principled plug-and-play discrete diffusion posterior sampling algorithm based on split Gibbs sampling, which we call SGDD. Our algorithm enables reward-guided generation and solving inverse problems in discrete-state spaces. We demonstrate the convergence of SGDD to the target posterior distribution and verify this through controlled experiments on synthetic benchmarks. Our method enjoys state-of-the-art posterior sampling performance on a range of benchmarks for discrete data, including DNA sequence design, discrete image inverse problems, and music infilling, achieving more than 30% improved performance compared to existing baselines. </p>
<blockquote>
<p>我们研究了离散状态空间中后采样的问题，使用了离散扩散模型。虽然连续扩散模型的后采样方法已经取得了显著的进步，但离散扩散模型的后采样方法仍然具有挑战性。在这项工作中，我们基于分割Gibbs采样引入了一种原则性的即插即用离散扩散后采样算法，我们称之为SGDD。我们的算法能够实现奖励引导生成和离散状态空间中的反问题求解。我们证明了SGDD能够收敛到目标后验分布，并通过合成基准的有控制实验验证了这一点。我们的方法在离散数据的一系列基准测试中享有最先进的后采样性能，包括DNA序列设计、离散图像反问题和音乐填充，与现有基线相比，性能提高了30%以上。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01161v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文研究了离散状态空间中的后采样问题，采用离散扩散模型进行研究。尽管连续扩散模型的后采样方法已经取得了显著进展，但离散扩散模型的后采样方法仍然具有挑战性。本文提出了一种基于分裂Gibbs采样的离散扩散后采样算法SGDD，该算法可实现奖励引导生成和解决离散状态空间中的逆问题。实验证明SGDD能收敛到目标后验分布，并在合成基准测试上得到验证。该方法在离散数据的一系列基准测试中表现出卓越的后采样性能，包括DNA序列设计、离散图像逆问题和音乐填充，相较于现有基线性能提升超过30%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究了离散状态空间中的后采样问题，这是离散扩散模型中的一个挑战。</li>
<li>提出了一种基于分裂Gibbs采样的离散扩散后采样算法SGDD。</li>
<li>SGDD算法能够执行奖励引导生成和解决离散状态空间中的逆问题。</li>
<li>实验证明了SGDD能收敛到目标后验分布。</li>
<li>SGDD在合成基准测试上表现出卓越性能。</li>
<li>在一系列基准测试中，SGDD在DNA序列设计、离散图像逆问题和音乐填充等方面表现出比现有基线更好的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01161">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-9c8edd6e0443f8d088d56747bc18d693.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-858c68b7e99cd15f114a194f226b10c1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c834eda0b84748656d489c3d30f17e7a.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Remasking-Discrete-Diffusion-Models-with-Inference-Time-Scaling"><a href="#Remasking-Discrete-Diffusion-Models-with-Inference-Time-Scaling" class="headerlink" title="Remasking Discrete Diffusion Models with Inference-Time Scaling"></a>Remasking Discrete Diffusion Models with Inference-Time Scaling</h2><p><strong>Authors:Guanghan Wang, Yair Schiff, Subham Sekhar Sahoo, Volodymyr Kuleshov</strong></p>
<p>Part of the success of diffusion models stems from their ability to perform iterative refinement, i.e., repeatedly correcting outputs during generation. However, modern masked discrete diffusion lacks this capability: when a token is generated, it cannot be updated again, even when it introduces an error. Here, we address this limitation by introducing the remasking diffusion model (ReMDM) sampler, a method that can be applied to pretrained masked diffusion models in a principled way and that is derived from a discrete diffusion model with a custom remasking backward process. Most interestingly, ReMDM endows discrete diffusion with a form of inference-time compute scaling. By increasing the number of sampling steps, ReMDM generates natural language outputs that approach the quality of autoregressive models, whereas when the computation budget is limited, ReMDM better maintains quality. ReMDM also improves sample quality of masked diffusion models for discretized images, and in scientific domains such as molecule design, ReMDM facilitates diffusion guidance and pushes the Pareto frontier of controllability relative to classical masking and uniform noise diffusion. We provide the code along with a blog post on the project page: <a target="_blank" rel="noopener" href="https://remdm.github.io/">https://remdm.github.io</a> </p>
<blockquote>
<p>扩散模型的部分成功源于其进行迭代优化的能力，即生成过程中反复修正输出。然而，现代掩码离散扩散缺乏这种能力：一旦生成标记，即使出现错误，也无法再次更新。在这里，我们通过引入重掩码扩散模型（ReMDM）采样器来解决这一限制，这是一种可以原则性地应用于预训练掩码扩散模型的方法，它来源于具有自定义反向重掩码过程的离散扩散模型。最有趣的是，ReMDM为离散扩散赋予了推理时间计算缩放的形式。通过增加采样步骤的数量，ReMDM生成的自然语言输出接近自回归模型的质量，而当计算预算有限时，ReMDM能更好地保持质量。ReMDM还提高了离散图像、分子设计等科学领域的掩码扩散模型的样本质量，并推动了相对于传统掩码和均匀噪声扩散的可控性的帕累托前沿。我们已提供项目页面上的代码及博客文章：<a target="_blank" rel="noopener" href="https://remdm.github.io./">https://remdm.github.io。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.00307v2">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://remdm.github.io/">https://remdm.github.io</a></p>
<p><strong>Summary</strong></p>
<p>扩散模型的一部分成功源于其能够进行迭代优化的能力，即生成过程中不断修正输出。然而，现代掩码离散扩散缺乏这种能力：一旦生成令牌，即使出现错误，也不能再次更新。我们通过引入重掩码扩散模型（ReMDM）采样器来解决这一问题，该方法可以原则性地应用于预训练的掩码扩散模型，源于离散扩散模型，具有自定义的重掩码逆向过程。ReMDM使离散扩散具有一种推理时间计算缩放形式。通过增加采样步骤的数量，ReMDM生成的自然语言输出接近自回归模型的质量，而在计算预算有限时，ReMDM能更好地保持质量。此外，ReMDM还提高了掩码扩散模型在离散图像以及科学领域（如分子设计）的样本质量，推动了可控性的帕累托前沿，相对于传统的掩码和均匀噪声扩散有所突破。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型能够通过迭代优化生成结果。</li>
<li>现代掩码离散扩散无法更新已生成的令牌。</li>
<li>ReMDM采样器解决了这一限制，提高了离散扩散模型的性能。</li>
<li>ReMDM能够在有限的计算预算内生成高质量的自然语言输出。</li>
<li>ReMDM提高了掩码扩散模型在图像科学领域的样本质量。</li>
<li>ReMDM在分子设计等领域推动了扩散指导的帕累托前沿。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.00307">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-75a85e7090232ae136abaf7738419196.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-113cbc60f770cf08f0914f121e086943.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f06f006b3bff6a186ba420767a39e76.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Dress-1-to-3-Single-Image-to-Simulation-Ready-3D-Outfit-with-Diffusion-Prior-and-Differentiable-Physics"><a href="#Dress-1-to-3-Single-Image-to-Simulation-Ready-3D-Outfit-with-Diffusion-Prior-and-Differentiable-Physics" class="headerlink" title="Dress-1-to-3: Single Image to Simulation-Ready 3D Outfit with Diffusion   Prior and Differentiable Physics"></a>Dress-1-to-3: Single Image to Simulation-Ready 3D Outfit with Diffusion   Prior and Differentiable Physics</h2><p><strong>Authors:Xuan Li, Chang Yu, Wenxin Du, Ying Jiang, Tianyi Xie, Yunuo Chen, Yin Yang, Chenfanfu Jiang</strong></p>
<p>Recent advances in large models have significantly advanced image-to-3D reconstruction. However, the generated models are often fused into a single piece, limiting their applicability in downstream tasks. This paper focuses on 3D garment generation, a key area for applications like virtual try-on with dynamic garment animations, which require garments to be separable and simulation-ready. We introduce Dress-1-to-3, a novel pipeline that reconstructs physics-plausible, simulation-ready separated garments with sewing patterns and humans from an in-the-wild image. Starting with the image, our approach combines a pre-trained image-to-sewing pattern generation model for creating coarse sewing patterns with a pre-trained multi-view diffusion model to produce multi-view images. The sewing pattern is further refined using a differentiable garment simulator based on the generated multi-view images. Versatile experiments demonstrate that our optimization approach substantially enhances the geometric alignment of the reconstructed 3D garments and humans with the input image. Furthermore, by integrating a texture generation module and a human motion generation module, we produce customized physics-plausible and realistic dynamic garment demonstrations. Project page: <a target="_blank" rel="noopener" href="https://dress-1-to-3.github.io/">https://dress-1-to-3.github.io/</a> </p>
<blockquote>
<p>近期大型模型的进展极大地推动了图像到3D重建的技术。然而，生成的模型通常被融合成单个部分，这在下游任务中的应用受到局限。本文重点关注3D服装生成，这是虚拟试穿等应用的关键领域，要求服装可分离且可模拟动态服装动画。我们引入了Dress-1-to-3，这是一个新型管道，可以从野生图像中重建物理可行的、可模拟的分离服装，包括缝纫图案和人类。从图像开始，我们的方法结合预训练的图像到缝纫图案生成模型来创建粗略的缝纫图案，以及与预训练的多视角扩散模型来生成多视角图像。缝纫图案进一步使用基于生成的多视角图像的可微分服装模拟器进行细化。多种实验表明，我们的优化方法极大地提高了重建的3D服装和人体与输入图像的几何对齐。此外，通过集成纹理生成模块和人体运动生成模块，我们生成了定制的、物理可行的和逼真的动态服装演示。项目页面：<a target="_blank" rel="noopener" href="https://dress-1-to-3.github.io/">https://dress-1-to-3.github.io/</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03449v2">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://dress-1-to-3.github.io/">https://dress-1-to-3.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了Dress-1-to-3项目，该项目专注于从图像重建物理仿真可用的分离衣物模型。通过使用预训练图像到缝纫图案生成模型和多视角扩散模型，结合可微分的衣物模拟器，实现从单张图像重建出具有物理仿真效果的分离衣物。此方法提高了重建的3D衣物与输入图像几何对齐的精确度，并可通过集成纹理生成模块和运动生成模块，产生逼真的动态衣物演示。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Dress-1-to-3项目实现从单张图像重建物理仿真可用的分离衣物模型。</li>
<li>使用预训练图像到缝纫图案生成模型创建粗缝纫图案。</li>
<li>利用预训练的多视角扩散模型生成多视角图像。</li>
<li>缝纫图案基于生成的多视角图像使用可微分的衣物模拟器进行精细调整。</li>
<li>优化方法提高了重建的3D衣物与输入图像的几何对齐精度。</li>
<li>通过集成纹理生成模块，产生定制的、物理仿真的动态衣物演示。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03449">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-24412ef34860fcd55abb854cee3d2aaf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3a3d193962d7f79ff5c601cbc5452695.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="SAeUron-Interpretable-Concept-Unlearning-in-Diffusion-Models-with-Sparse-Autoencoders"><a href="#SAeUron-Interpretable-Concept-Unlearning-in-Diffusion-Models-with-Sparse-Autoencoders" class="headerlink" title="SAeUron: Interpretable Concept Unlearning in Diffusion Models with   Sparse Autoencoders"></a>SAeUron: Interpretable Concept Unlearning in Diffusion Models with   Sparse Autoencoders</h2><p><strong>Authors:Bartosz Cywiński, Kamil Deja</strong></p>
<p>Diffusion models, while powerful, can inadvertently generate harmful or undesirable content, raising significant ethical and safety concerns. Recent machine unlearning approaches offer potential solutions but often lack transparency, making it difficult to understand the changes they introduce to the base model. In this work, we introduce SAeUron, a novel method leveraging features learned by sparse autoencoders (SAEs) to remove unwanted concepts in text-to-image diffusion models. First, we demonstrate that SAEs, trained in an unsupervised manner on activations from multiple denoising timesteps of the diffusion model, capture sparse and interpretable features corresponding to specific concepts. Building on this, we propose a feature selection method that enables precise interventions on model activations to block targeted content while preserving overall performance. Our evaluation shows that SAeUron outperforms existing approaches on the UnlearnCanvas benchmark for concepts and style unlearning, and effectively eliminates nudity when evaluated with I2P. Moreover, we show that with a single SAE, we can remove multiple concepts simultaneously and that in contrast to other methods, SAeUron mitigates the possibility of generating unwanted content under adversarial attack. Code and checkpoints are available at <a target="_blank" rel="noopener" href="https://github.com/cywinski/SAeUron">https://github.com/cywinski/SAeUron</a>. </p>
<blockquote>
<p>扩散模型虽然强大，但可能无意中生成有害或不良内容，引发严重的伦理和安全担忧。最近的机器非学习技术提供了新的解决方案，但往往缺乏透明度，使得难以了解它们对基础模型所做的改变。在这项研究中，我们介绍了SAeUron，这是一种利用稀疏自动编码器（SAE）学习到的特性来消除文本到图像扩散模型中不需要概念的新方法。首先，我们证明了在扩散模型的多个去噪时间步长的激活上采用无监督方式训练的SAE可以捕获对应于特定概念的稀疏和可解释的特性。在此基础上，我们提出了一种特征选择方法，能够对模型激活进行精确干预，以阻止目标内容，同时保留整体性能。我们的评估显示，在概念和风格非学习的UnlearnCanvas基准测试中，SAeUron优于现有方法，在评估裸体内容时与I2P结合使用可以有效消除其存在。此外，我们还表明，使用一个单一的SAE可以同时消除多个概念，而且与其他方法相比，SAeUron减轻了生成对抗攻击下可能产生的不必要内容的可能性。代码和检查点可在<a target="_blank" rel="noopener" href="https://github.com/cywinski/SAeUron%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/cywinski/SAeUron找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.18052v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了Diffusion模型可能产生的伦理和安全问题，即可能生成有害或不受欢迎的内容。尽管现有的机器遗忘法提供了一种可能的解决方案，但它们往往缺乏透明度，难以了解对基础模型所做的改变。本文提出了一种新方法SAeUron，它利用稀疏自编码器（SAE）的特性来消除文本到图像扩散模型中的不需要的概念。实验表明，SAeUron在概念与风格遗忘的基准测试中表现优于现有方法，能够有效消除图像中的裸体内容。此外，我们还展示了单个SAE可以同时消除多个概念的能力，并且与其他方法相比，SAeUron减轻了生成对抗性攻击下产生的不必要内容的可能性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Diffusion模型可能会生成有害或不受欢迎的内容，引发伦理和安全担忧。</li>
<li>现有机器遗忘法解决此问题但缺乏透明度。</li>
<li>SAeUron方法利用稀疏自编码器（SAE）的特性消除扩散模型中的不需要的概念。</li>
<li>SAeUron在概念与风格遗忘的测试中表现优异，有效消除特定内容。</li>
<li>SAeUron能同时消除多个概念。</li>
<li>SAeUron降低了生成对抗性攻击下产生不必要内容的可能性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.18052">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ce83f2ad29caa2d78af9fdd0e75db81f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-570edce8b07c3b8ecb334a2dc89391a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-093b0ce4bf8810190a0dc9edb12810df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-22d3835115ae6c44a1080ba3db2c97f5.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Make-An-Agent-A-Generalizable-Policy-Network-Generator-with-Behavior-Prompted-Diffusion"><a href="#Make-An-Agent-A-Generalizable-Policy-Network-Generator-with-Behavior-Prompted-Diffusion" class="headerlink" title="Make-An-Agent: A Generalizable Policy Network Generator with   Behavior-Prompted Diffusion"></a>Make-An-Agent: A Generalizable Policy Network Generator with   Behavior-Prompted Diffusion</h2><p><strong>Authors:Yongyuan Liang, Tingqiang Xu, Kaizhe Hu, Guangqi Jiang, Furong Huang, Huazhe Xu</strong></p>
<p>Can we generate a control policy for an agent using just one demonstration of desired behaviors as a prompt, as effortlessly as creating an image from a textual description? In this paper, we present Make-An-Agent, a novel policy parameter generator that leverages the power of conditional diffusion models for behavior-to-policy generation. Guided by behavior embeddings that encode trajectory information, our policy generator synthesizes latent parameter representations, which can then be decoded into policy networks. Trained on policy network checkpoints and their corresponding trajectories, our generation model demonstrates remarkable versatility and scalability on multiple tasks and has a strong generalization ability on unseen tasks to output well-performed policies with only few-shot demonstrations as inputs. We showcase its efficacy and efficiency on various domains and tasks, including varying objectives, behaviors, and even across different robot manipulators. Beyond simulation, we directly deploy policies generated by Make-An-Agent onto real-world robots on locomotion tasks. Project page: <a target="_blank" rel="noopener" href="https://cheryyunl.github.io/make-an-agent/">https://cheryyunl.github.io/make-an-agent/</a> </p>
<blockquote>
<p>我们可以像从文本描述中创建图像一样，仅使用一个期望行为的演示来提示为智能体生成控制策略吗？在本文中，我们介绍了Make-An-Agent，这是一种新型的策略参数生成器，它利用条件扩散模型的力量进行行为到策略的生成。在行为嵌入的指导下（这些嵌入编码了轨迹信息），我们的策略生成器合成潜在参数表示，然后可以将其解码为策略网络。我们的生成模型在多个任务上表现出卓越的通用性和可扩展性，在未见过的新任务上具有很强的泛化能力，仅使用少数演示作为输入即可输出表现良好的策略。我们在各种领域和任务中展示了它的功效和效率，包括不同的目标、行为和甚至不同的机器人操纵器。除了模拟之外，我们还直接将Make-An-Agent生成的策略部署到现实世界中的机器人上进行运动任务。项目页面：<a target="_blank" rel="noopener" href="https://cheryyunl.github.io/make-an-agent/">https://cheryyunl.github.io/make-an-agent/</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.10973v4">PDF</a> Annual Conference on Neural Information Processing Systems 38</p>
<p><strong>Summary</strong></p>
<p>使用单一演示行为生成代理控制策略，如同根据文字描述生成图像一样简单便捷。本研究提出了Make-An-Agent，这是一种新型策略参数生成器，利用条件扩散模型实现行为到策略的生成。该策略生成器通过行为嵌入编码轨迹信息，合成潜在参数表示，然后解码为策略网络。在策略网络检查点及其对应轨迹的训练下，该生成模型在多个任务上表现出卓越的多功能性和可扩展性，并且对未见任务的策略输出具有较强的泛化能力，仅使用少量演示作为输入。它在各种领域和任务中展示了有效性和高效性，包括不同的目标、行为和机器人操作器。此外，我们将Make-An-Agent生成的策略直接部署到真实世界机器人上执行运动任务。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Make-An-Agent利用条件扩散模型实现了从行为到策略的生成。</li>
<li>该策略生成器通过行为嵌入编码轨迹信息，进而合成潜在参数表示。</li>
<li>策略生成器可以解码为策略网络，具备卓越的多功能性和可扩展性。</li>
<li>生成模型在少量演示作为输入的情况下，对未见任务具有强泛化能力。</li>
<li>Make-An-Agent在各种领域和任务中表现出有效性和高效性，包括不同的目标、行为和机器人操作器。</li>
<li>Make-An-Agent可以直接将生成的策略部署到真实世界的机器人上。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.10973">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-aa45f4f001fb029de675ad9f1ebd0c1c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-423c4964e23d50f08558fb68b2c1ce0a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-32bf11efff904191e99cde375638abca.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fbf5570a59fc0a488dc9783f95ddf5d0.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-24/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-24/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-24/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-9ea8fda246e471d018ae69deae8b7be8.jpg" class="responsive-img" alt="牙齿修复">
                        
                        <span class="card-title">牙齿修复</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            牙齿修复 方向最新论文已更新，请持续关注 Update in 2025-05-24  Oral Imaging for Malocclusion Issues Assessments OMNI Dataset, Deep   Learning Baselines and Benchmarking
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-24
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/" class="post-category">
                                    牙齿修复
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                        <span class="chip bg-color">牙齿修复</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-24/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-d3126b4d9896e174493289f273f89f37.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2025-05-24  Seeing through Satellite Images at Street Views
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-24
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">25691.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
